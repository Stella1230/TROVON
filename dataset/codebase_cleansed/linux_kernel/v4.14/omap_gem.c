static uint64_t mmap_offset(struct drm_gem_object *obj)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nint ret;\r\nsize_t size;\r\nWARN_ON(!mutex_is_locked(&dev->struct_mutex));\r\nsize = omap_gem_mmap_size(obj);\r\nret = drm_gem_create_mmap_offset_size(obj, size);\r\nif (ret) {\r\ndev_err(dev->dev, "could not allocate mmap offset\n");\r\nreturn 0;\r\n}\r\nreturn drm_vma_node_offset_addr(&obj->vma_node);\r\n}\r\nstatic bool is_contiguous(struct omap_gem_object *omap_obj)\r\n{\r\nif (omap_obj->flags & OMAP_BO_MEM_DMA_API)\r\nreturn true;\r\nif ((omap_obj->flags & OMAP_BO_MEM_DMABUF) && omap_obj->sgt->nents == 1)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void evict_entry(struct drm_gem_object *obj,\r\nenum tiler_fmt fmt, struct omap_drm_usergart_entry *entry)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nstruct omap_drm_private *priv = obj->dev->dev_private;\r\nint n = priv->usergart[fmt].height;\r\nsize_t size = PAGE_SIZE * n;\r\nloff_t off = mmap_offset(obj) +\r\n(entry->obj_pgoff << PAGE_SHIFT);\r\nconst int m = DIV_ROUND_UP(omap_obj->width << fmt, PAGE_SIZE);\r\nif (m > 1) {\r\nint i;\r\nfor (i = n; i > 0; i--) {\r\nunmap_mapping_range(obj->dev->anon_inode->i_mapping,\r\noff, PAGE_SIZE, 1);\r\noff += PAGE_SIZE * m;\r\n}\r\n} else {\r\nunmap_mapping_range(obj->dev->anon_inode->i_mapping,\r\noff, size, 1);\r\n}\r\nentry->obj = NULL;\r\n}\r\nstatic void evict(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nstruct omap_drm_private *priv = obj->dev->dev_private;\r\nif (omap_obj->flags & OMAP_BO_TILED) {\r\nenum tiler_fmt fmt = gem2fmt(omap_obj->flags);\r\nint i;\r\nfor (i = 0; i < NUM_USERGART_ENTRIES; i++) {\r\nstruct omap_drm_usergart_entry *entry =\r\n&priv->usergart[fmt].entry[i];\r\nif (entry->obj == obj)\r\nevict_entry(obj, fmt, entry);\r\n}\r\n}\r\n}\r\nstatic int omap_gem_attach_pages(struct drm_gem_object *obj)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nstruct page **pages;\r\nint npages = obj->size >> PAGE_SHIFT;\r\nint i, ret;\r\ndma_addr_t *addrs;\r\nWARN_ON(omap_obj->pages);\r\npages = drm_gem_get_pages(obj);\r\nif (IS_ERR(pages)) {\r\ndev_err(obj->dev->dev, "could not get pages: %ld\n", PTR_ERR(pages));\r\nreturn PTR_ERR(pages);\r\n}\r\nif (omap_obj->flags & (OMAP_BO_WC|OMAP_BO_UNCACHED)) {\r\naddrs = kmalloc(npages * sizeof(*addrs), GFP_KERNEL);\r\nif (!addrs) {\r\nret = -ENOMEM;\r\ngoto free_pages;\r\n}\r\nfor (i = 0; i < npages; i++) {\r\naddrs[i] = dma_map_page(dev->dev, pages[i],\r\n0, PAGE_SIZE, DMA_TO_DEVICE);\r\nif (dma_mapping_error(dev->dev, addrs[i])) {\r\ndev_warn(dev->dev,\r\n"%s: failed to map page\n", __func__);\r\nfor (i = i - 1; i >= 0; --i) {\r\ndma_unmap_page(dev->dev, addrs[i],\r\nPAGE_SIZE, DMA_TO_DEVICE);\r\n}\r\nret = -ENOMEM;\r\ngoto free_addrs;\r\n}\r\n}\r\n} else {\r\naddrs = kzalloc(npages * sizeof(*addrs), GFP_KERNEL);\r\nif (!addrs) {\r\nret = -ENOMEM;\r\ngoto free_pages;\r\n}\r\n}\r\nomap_obj->dma_addrs = addrs;\r\nomap_obj->pages = pages;\r\nreturn 0;\r\nfree_addrs:\r\nkfree(addrs);\r\nfree_pages:\r\ndrm_gem_put_pages(obj, pages, true, false);\r\nreturn ret;\r\n}\r\nstatic int get_pages(struct drm_gem_object *obj, struct page ***pages)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = 0;\r\nif ((omap_obj->flags & OMAP_BO_MEM_SHMEM) && !omap_obj->pages) {\r\nret = omap_gem_attach_pages(obj);\r\nif (ret) {\r\ndev_err(obj->dev->dev, "could not attach pages\n");\r\nreturn ret;\r\n}\r\n}\r\n*pages = omap_obj->pages;\r\nreturn 0;\r\n}\r\nstatic void omap_gem_detach_pages(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nunsigned int npages = obj->size >> PAGE_SHIFT;\r\nunsigned int i;\r\nfor (i = 0; i < npages; i++) {\r\nif (omap_obj->dma_addrs[i])\r\ndma_unmap_page(obj->dev->dev, omap_obj->dma_addrs[i],\r\nPAGE_SIZE, DMA_TO_DEVICE);\r\n}\r\nkfree(omap_obj->dma_addrs);\r\nomap_obj->dma_addrs = NULL;\r\ndrm_gem_put_pages(obj, omap_obj->pages, true, false);\r\nomap_obj->pages = NULL;\r\n}\r\nuint32_t omap_gem_flags(struct drm_gem_object *obj)\r\n{\r\nreturn to_omap_bo(obj)->flags;\r\n}\r\nuint64_t omap_gem_mmap_offset(struct drm_gem_object *obj)\r\n{\r\nuint64_t offset;\r\nmutex_lock(&obj->dev->struct_mutex);\r\noffset = mmap_offset(obj);\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn offset;\r\n}\r\nsize_t omap_gem_mmap_size(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nsize_t size = obj->size;\r\nif (omap_obj->flags & OMAP_BO_TILED) {\r\nsize = tiler_vsize(gem2fmt(omap_obj->flags),\r\nomap_obj->width, omap_obj->height);\r\n}\r\nreturn size;\r\n}\r\nstatic int fault_1d(struct drm_gem_object *obj,\r\nstruct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nunsigned long pfn;\r\npgoff_t pgoff;\r\npgoff = (vmf->address - vma->vm_start) >> PAGE_SHIFT;\r\nif (omap_obj->pages) {\r\nomap_gem_cpu_sync_page(obj, pgoff);\r\npfn = page_to_pfn(omap_obj->pages[pgoff]);\r\n} else {\r\nBUG_ON(!is_contiguous(omap_obj));\r\npfn = (omap_obj->dma_addr >> PAGE_SHIFT) + pgoff;\r\n}\r\nVERB("Inserting %p pfn %lx, pa %lx", (void *)vmf->address,\r\npfn, pfn << PAGE_SHIFT);\r\nreturn vm_insert_mixed(vma, vmf->address, __pfn_to_pfn_t(pfn, PFN_DEV));\r\n}\r\nstatic int fault_2d(struct drm_gem_object *obj,\r\nstruct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nstruct omap_drm_private *priv = obj->dev->dev_private;\r\nstruct omap_drm_usergart_entry *entry;\r\nenum tiler_fmt fmt = gem2fmt(omap_obj->flags);\r\nstruct page *pages[64];\r\nunsigned long pfn;\r\npgoff_t pgoff, base_pgoff;\r\nunsigned long vaddr;\r\nint i, ret, slots;\r\nconst int n = priv->usergart[fmt].height;\r\nconst int n_shift = priv->usergart[fmt].height_shift;\r\nconst int m = DIV_ROUND_UP(omap_obj->width << fmt, PAGE_SIZE);\r\npgoff = (vmf->address - vma->vm_start) >> PAGE_SHIFT;\r\nbase_pgoff = round_down(pgoff, m << n_shift);\r\nslots = omap_obj->width >> priv->usergart[fmt].slot_shift;\r\nvaddr = vmf->address - ((pgoff - base_pgoff) << PAGE_SHIFT);\r\nentry = &priv->usergart[fmt].entry[priv->usergart[fmt].last];\r\nif (entry->obj)\r\nevict_entry(entry->obj, fmt, entry);\r\nentry->obj = obj;\r\nentry->obj_pgoff = base_pgoff;\r\nbase_pgoff = (base_pgoff >> n_shift) * slots;\r\nif (m > 1) {\r\nint off = pgoff % m;\r\nentry->obj_pgoff += off;\r\nbase_pgoff /= m;\r\nslots = min(slots - (off << n_shift), n);\r\nbase_pgoff += off << n_shift;\r\nvaddr += off << PAGE_SHIFT;\r\n}\r\nmemcpy(pages, &omap_obj->pages[base_pgoff],\r\nsizeof(struct page *) * slots);\r\nmemset(pages + slots, 0,\r\nsizeof(struct page *) * (n - slots));\r\nret = tiler_pin(entry->block, pages, ARRAY_SIZE(pages), 0, true);\r\nif (ret) {\r\ndev_err(obj->dev->dev, "failed to pin: %d\n", ret);\r\nreturn ret;\r\n}\r\npfn = entry->dma_addr >> PAGE_SHIFT;\r\nVERB("Inserting %p pfn %lx, pa %lx", (void *)vmf->address,\r\npfn, pfn << PAGE_SHIFT);\r\nfor (i = n; i > 0; i--) {\r\nvm_insert_mixed(vma, vaddr, __pfn_to_pfn_t(pfn, PFN_DEV));\r\npfn += priv->usergart[fmt].stride_pfn;\r\nvaddr += PAGE_SIZE * m;\r\n}\r\npriv->usergart[fmt].last = (priv->usergart[fmt].last + 1)\r\n% NUM_USERGART_ENTRIES;\r\nreturn 0;\r\n}\r\nint omap_gem_fault(struct vm_fault *vmf)\r\n{\r\nstruct vm_area_struct *vma = vmf->vma;\r\nstruct drm_gem_object *obj = vma->vm_private_data;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nstruct drm_device *dev = obj->dev;\r\nstruct page **pages;\r\nint ret;\r\nmutex_lock(&dev->struct_mutex);\r\nret = get_pages(obj, &pages);\r\nif (ret)\r\ngoto fail;\r\nif (omap_obj->flags & OMAP_BO_TILED)\r\nret = fault_2d(obj, vma, vmf);\r\nelse\r\nret = fault_1d(obj, vma, vmf);\r\nfail:\r\nmutex_unlock(&dev->struct_mutex);\r\nswitch (ret) {\r\ncase 0:\r\ncase -ERESTARTSYS:\r\ncase -EINTR:\r\ncase -EBUSY:\r\nreturn VM_FAULT_NOPAGE;\r\ncase -ENOMEM:\r\nreturn VM_FAULT_OOM;\r\ndefault:\r\nreturn VM_FAULT_SIGBUS;\r\n}\r\n}\r\nint omap_gem_mmap(struct file *filp, struct vm_area_struct *vma)\r\n{\r\nint ret;\r\nret = drm_gem_mmap(filp, vma);\r\nif (ret) {\r\nDBG("mmap failed: %d", ret);\r\nreturn ret;\r\n}\r\nreturn omap_gem_mmap_obj(vma->vm_private_data, vma);\r\n}\r\nint omap_gem_mmap_obj(struct drm_gem_object *obj,\r\nstruct vm_area_struct *vma)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nvma->vm_flags &= ~VM_PFNMAP;\r\nvma->vm_flags |= VM_MIXEDMAP;\r\nif (omap_obj->flags & OMAP_BO_WC) {\r\nvma->vm_page_prot = pgprot_writecombine(vm_get_page_prot(vma->vm_flags));\r\n} else if (omap_obj->flags & OMAP_BO_UNCACHED) {\r\nvma->vm_page_prot = pgprot_noncached(vm_get_page_prot(vma->vm_flags));\r\n} else {\r\nif (WARN_ON(!obj->filp))\r\nreturn -EINVAL;\r\nfput(vma->vm_file);\r\nvma->vm_pgoff = 0;\r\nvma->vm_file = get_file(obj->filp);\r\nvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\r\n}\r\nreturn 0;\r\n}\r\nint omap_gem_dumb_create(struct drm_file *file, struct drm_device *dev,\r\nstruct drm_mode_create_dumb *args)\r\n{\r\nunion omap_gem_size gsize;\r\nargs->pitch = DIV_ROUND_UP(args->width * args->bpp, 8);\r\nargs->size = PAGE_ALIGN(args->pitch * args->height);\r\ngsize = (union omap_gem_size){\r\n.bytes = args->size,\r\n};\r\nreturn omap_gem_new_handle(dev, file, gsize,\r\nOMAP_BO_SCANOUT | OMAP_BO_WC, &args->handle);\r\n}\r\nint omap_gem_dumb_map_offset(struct drm_file *file, struct drm_device *dev,\r\nuint32_t handle, uint64_t *offset)\r\n{\r\nstruct drm_gem_object *obj;\r\nint ret = 0;\r\nobj = drm_gem_object_lookup(file, handle);\r\nif (obj == NULL) {\r\nret = -ENOENT;\r\ngoto fail;\r\n}\r\n*offset = omap_gem_mmap_offset(obj);\r\ndrm_gem_object_unreference_unlocked(obj);\r\nfail:\r\nreturn ret;\r\n}\r\nint omap_gem_roll(struct drm_gem_object *obj, uint32_t roll)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nuint32_t npages = obj->size >> PAGE_SHIFT;\r\nint ret = 0;\r\nif (roll > npages) {\r\ndev_err(obj->dev->dev, "invalid roll: %d\n", roll);\r\nreturn -EINVAL;\r\n}\r\nomap_obj->roll = roll;\r\nmutex_lock(&obj->dev->struct_mutex);\r\nif (omap_obj->block) {\r\nstruct page **pages;\r\nret = get_pages(obj, &pages);\r\nif (ret)\r\ngoto fail;\r\nret = tiler_pin(omap_obj->block, pages, npages, roll, true);\r\nif (ret)\r\ndev_err(obj->dev->dev, "could not repin: %d\n", ret);\r\n}\r\nfail:\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn ret;\r\n}\r\nstatic inline bool is_cached_coherent(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nreturn !((omap_obj->flags & OMAP_BO_MEM_SHMEM) &&\r\n((omap_obj->flags & OMAP_BO_CACHE_MASK) == OMAP_BO_CACHED));\r\n}\r\nvoid omap_gem_cpu_sync_page(struct drm_gem_object *obj, int pgoff)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nif (is_cached_coherent(obj))\r\nreturn;\r\nif (omap_obj->dma_addrs[pgoff]) {\r\ndma_unmap_page(dev->dev, omap_obj->dma_addrs[pgoff],\r\nPAGE_SIZE, DMA_TO_DEVICE);\r\nomap_obj->dma_addrs[pgoff] = 0;\r\n}\r\n}\r\nvoid omap_gem_dma_sync_buffer(struct drm_gem_object *obj,\r\nenum dma_data_direction dir)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint i, npages = obj->size >> PAGE_SHIFT;\r\nstruct page **pages = omap_obj->pages;\r\nbool dirty = false;\r\nif (is_cached_coherent(obj))\r\nreturn;\r\nfor (i = 0; i < npages; i++) {\r\nif (!omap_obj->dma_addrs[i]) {\r\ndma_addr_t addr;\r\naddr = dma_map_page(dev->dev, pages[i], 0,\r\nPAGE_SIZE, dir);\r\nif (dma_mapping_error(dev->dev, addr)) {\r\ndev_warn(dev->dev, "%s: failed to map page\n",\r\n__func__);\r\nbreak;\r\n}\r\ndirty = true;\r\nomap_obj->dma_addrs[i] = addr;\r\n}\r\n}\r\nif (dirty) {\r\nunmap_mapping_range(obj->filp->f_mapping, 0,\r\nomap_gem_mmap_size(obj), 1);\r\n}\r\n}\r\nint omap_gem_pin(struct drm_gem_object *obj, dma_addr_t *dma_addr)\r\n{\r\nstruct omap_drm_private *priv = obj->dev->dev_private;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = 0;\r\nmutex_lock(&obj->dev->struct_mutex);\r\nif (!is_contiguous(omap_obj) && priv->has_dmm) {\r\nif (omap_obj->dma_addr_cnt == 0) {\r\nstruct page **pages;\r\nuint32_t npages = obj->size >> PAGE_SHIFT;\r\nenum tiler_fmt fmt = gem2fmt(omap_obj->flags);\r\nstruct tiler_block *block;\r\nBUG_ON(omap_obj->block);\r\nret = get_pages(obj, &pages);\r\nif (ret)\r\ngoto fail;\r\nif (omap_obj->flags & OMAP_BO_TILED) {\r\nblock = tiler_reserve_2d(fmt,\r\nomap_obj->width,\r\nomap_obj->height, 0);\r\n} else {\r\nblock = tiler_reserve_1d(obj->size);\r\n}\r\nif (IS_ERR(block)) {\r\nret = PTR_ERR(block);\r\ndev_err(obj->dev->dev,\r\n"could not remap: %d (%d)\n", ret, fmt);\r\ngoto fail;\r\n}\r\nret = tiler_pin(block, pages, npages,\r\nomap_obj->roll, true);\r\nif (ret) {\r\ntiler_release(block);\r\ndev_err(obj->dev->dev,\r\n"could not pin: %d\n", ret);\r\ngoto fail;\r\n}\r\nomap_obj->dma_addr = tiler_ssptr(block);\r\nomap_obj->block = block;\r\nDBG("got dma address: %pad", &omap_obj->dma_addr);\r\n}\r\nomap_obj->dma_addr_cnt++;\r\n*dma_addr = omap_obj->dma_addr;\r\n} else if (is_contiguous(omap_obj)) {\r\n*dma_addr = omap_obj->dma_addr;\r\n} else {\r\nret = -EINVAL;\r\ngoto fail;\r\n}\r\nfail:\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn ret;\r\n}\r\nvoid omap_gem_unpin(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret;\r\nmutex_lock(&obj->dev->struct_mutex);\r\nif (omap_obj->dma_addr_cnt > 0) {\r\nomap_obj->dma_addr_cnt--;\r\nif (omap_obj->dma_addr_cnt == 0) {\r\nret = tiler_unpin(omap_obj->block);\r\nif (ret) {\r\ndev_err(obj->dev->dev,\r\n"could not unpin pages: %d\n", ret);\r\n}\r\nret = tiler_release(omap_obj->block);\r\nif (ret) {\r\ndev_err(obj->dev->dev,\r\n"could not release unmap: %d\n", ret);\r\n}\r\nomap_obj->dma_addr = 0;\r\nomap_obj->block = NULL;\r\n}\r\n}\r\nmutex_unlock(&obj->dev->struct_mutex);\r\n}\r\nint omap_gem_rotated_dma_addr(struct drm_gem_object *obj, uint32_t orient,\r\nint x, int y, dma_addr_t *dma_addr)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = -EINVAL;\r\nmutex_lock(&obj->dev->struct_mutex);\r\nif ((omap_obj->dma_addr_cnt > 0) && omap_obj->block &&\r\n(omap_obj->flags & OMAP_BO_TILED)) {\r\n*dma_addr = tiler_tsptr(omap_obj->block, orient, x, y);\r\nret = 0;\r\n}\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn ret;\r\n}\r\nint omap_gem_tiled_stride(struct drm_gem_object *obj, uint32_t orient)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = -EINVAL;\r\nif (omap_obj->flags & OMAP_BO_TILED)\r\nret = tiler_stride(gem2fmt(omap_obj->flags), orient);\r\nreturn ret;\r\n}\r\nint omap_gem_get_pages(struct drm_gem_object *obj, struct page ***pages,\r\nbool remap)\r\n{\r\nint ret;\r\nif (!remap) {\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nif (!omap_obj->pages)\r\nreturn -ENOMEM;\r\n*pages = omap_obj->pages;\r\nreturn 0;\r\n}\r\nmutex_lock(&obj->dev->struct_mutex);\r\nret = get_pages(obj, pages);\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn ret;\r\n}\r\nint omap_gem_put_pages(struct drm_gem_object *obj)\r\n{\r\nreturn 0;\r\n}\r\nvoid *omap_gem_vaddr(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nWARN_ON(!mutex_is_locked(&obj->dev->struct_mutex));\r\nif (!omap_obj->vaddr) {\r\nstruct page **pages;\r\nint ret = get_pages(obj, &pages);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nomap_obj->vaddr = vmap(pages, obj->size >> PAGE_SHIFT,\r\nVM_MAP, pgprot_writecombine(PAGE_KERNEL));\r\n}\r\nreturn omap_obj->vaddr;\r\n}\r\nint omap_gem_resume(struct device *dev)\r\n{\r\nstruct drm_device *drm_dev = dev_get_drvdata(dev);\r\nstruct omap_drm_private *priv = drm_dev->dev_private;\r\nstruct omap_gem_object *omap_obj;\r\nint ret = 0;\r\nlist_for_each_entry(omap_obj, &priv->obj_list, mm_list) {\r\nif (omap_obj->block) {\r\nstruct drm_gem_object *obj = &omap_obj->base;\r\nuint32_t npages = obj->size >> PAGE_SHIFT;\r\nWARN_ON(!omap_obj->pages);\r\nret = tiler_pin(omap_obj->block,\r\nomap_obj->pages, npages,\r\nomap_obj->roll, true);\r\nif (ret) {\r\ndev_err(dev, "could not repin: %d\n", ret);\r\nreturn ret;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid omap_gem_describe(struct drm_gem_object *obj, struct seq_file *m)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nuint64_t off;\r\noff = drm_vma_node_start(&obj->vma_node);\r\nseq_printf(m, "%08x: %2d (%2d) %08llx %pad (%2d) %p %4d",\r\nomap_obj->flags, obj->name, kref_read(&obj->refcount),\r\noff, &omap_obj->dma_addr, omap_obj->dma_addr_cnt,\r\nomap_obj->vaddr, omap_obj->roll);\r\nif (omap_obj->flags & OMAP_BO_TILED) {\r\nseq_printf(m, " %dx%d", omap_obj->width, omap_obj->height);\r\nif (omap_obj->block) {\r\nstruct tcm_area *area = &omap_obj->block->area;\r\nseq_printf(m, " (%dx%d, %dx%d)",\r\narea->p0.x, area->p0.y,\r\narea->p1.x, area->p1.y);\r\n}\r\n} else {\r\nseq_printf(m, " %zu", obj->size);\r\n}\r\nseq_printf(m, "\n");\r\n}\r\nvoid omap_gem_describe_objects(struct list_head *list, struct seq_file *m)\r\n{\r\nstruct omap_gem_object *omap_obj;\r\nint count = 0;\r\nsize_t size = 0;\r\nlist_for_each_entry(omap_obj, list, mm_list) {\r\nstruct drm_gem_object *obj = &omap_obj->base;\r\nseq_printf(m, " ");\r\nomap_gem_describe(obj, m);\r\ncount++;\r\nsize += obj->size;\r\n}\r\nseq_printf(m, "Total %d objects, %zu bytes\n", count, size);\r\n}\r\nvoid omap_gem_free_object(struct drm_gem_object *obj)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nstruct omap_drm_private *priv = dev->dev_private;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nevict(obj);\r\nWARN_ON(!mutex_is_locked(&dev->struct_mutex));\r\nspin_lock(&priv->list_lock);\r\nlist_del(&omap_obj->mm_list);\r\nspin_unlock(&priv->list_lock);\r\nWARN_ON(omap_obj->dma_addr_cnt > 0);\r\nif (omap_obj->pages) {\r\nif (omap_obj->flags & OMAP_BO_MEM_DMABUF)\r\nkfree(omap_obj->pages);\r\nelse\r\nomap_gem_detach_pages(obj);\r\n}\r\nif (omap_obj->flags & OMAP_BO_MEM_DMA_API) {\r\ndma_free_wc(dev->dev, obj->size, omap_obj->vaddr,\r\nomap_obj->dma_addr);\r\n} else if (omap_obj->vaddr) {\r\nvunmap(omap_obj->vaddr);\r\n} else if (obj->import_attach) {\r\ndrm_prime_gem_destroy(obj, omap_obj->sgt);\r\n}\r\ndrm_gem_object_release(obj);\r\nkfree(omap_obj);\r\n}\r\nstruct drm_gem_object *omap_gem_new(struct drm_device *dev,\r\nunion omap_gem_size gsize, uint32_t flags)\r\n{\r\nstruct omap_drm_private *priv = dev->dev_private;\r\nstruct omap_gem_object *omap_obj;\r\nstruct drm_gem_object *obj;\r\nstruct address_space *mapping;\r\nsize_t size;\r\nint ret;\r\nif (flags & OMAP_BO_TILED) {\r\nif (!priv->usergart) {\r\ndev_err(dev->dev, "Tiled buffers require DMM\n");\r\nreturn NULL;\r\n}\r\nflags &= ~OMAP_BO_SCANOUT;\r\nflags |= OMAP_BO_MEM_SHMEM;\r\nflags &= ~(OMAP_BO_CACHED|OMAP_BO_WC|OMAP_BO_UNCACHED);\r\nflags |= tiler_get_cpu_cache_flags();\r\n} else if ((flags & OMAP_BO_SCANOUT) && !priv->has_dmm) {\r\nflags |= OMAP_BO_MEM_DMA_API;\r\n} else if (!(flags & OMAP_BO_MEM_DMABUF)) {\r\nflags |= OMAP_BO_MEM_SHMEM;\r\n}\r\nomap_obj = kzalloc(sizeof(*omap_obj), GFP_KERNEL);\r\nif (!omap_obj)\r\nreturn NULL;\r\nobj = &omap_obj->base;\r\nomap_obj->flags = flags;\r\nif (flags & OMAP_BO_TILED) {\r\ntiler_align(gem2fmt(flags), &gsize.tiled.width,\r\n&gsize.tiled.height);\r\nsize = tiler_size(gem2fmt(flags), gsize.tiled.width,\r\ngsize.tiled.height);\r\nomap_obj->width = gsize.tiled.width;\r\nomap_obj->height = gsize.tiled.height;\r\n} else {\r\nsize = PAGE_ALIGN(gsize.bytes);\r\n}\r\nif (!(flags & OMAP_BO_MEM_SHMEM)) {\r\ndrm_gem_private_object_init(dev, obj, size);\r\n} else {\r\nret = drm_gem_object_init(dev, obj, size);\r\nif (ret)\r\ngoto err_free;\r\nmapping = obj->filp->f_mapping;\r\nmapping_set_gfp_mask(mapping, GFP_USER | __GFP_DMA32);\r\n}\r\nif (flags & OMAP_BO_MEM_DMA_API) {\r\nomap_obj->vaddr = dma_alloc_wc(dev->dev, size,\r\n&omap_obj->dma_addr,\r\nGFP_KERNEL);\r\nif (!omap_obj->vaddr)\r\ngoto err_release;\r\n}\r\nspin_lock(&priv->list_lock);\r\nlist_add(&omap_obj->mm_list, &priv->obj_list);\r\nspin_unlock(&priv->list_lock);\r\nreturn obj;\r\nerr_release:\r\ndrm_gem_object_release(obj);\r\nerr_free:\r\nkfree(omap_obj);\r\nreturn NULL;\r\n}\r\nstruct drm_gem_object *omap_gem_new_dmabuf(struct drm_device *dev, size_t size,\r\nstruct sg_table *sgt)\r\n{\r\nstruct omap_drm_private *priv = dev->dev_private;\r\nstruct omap_gem_object *omap_obj;\r\nstruct drm_gem_object *obj;\r\nunion omap_gem_size gsize;\r\nif (sgt->orig_nents != 1 && !priv->has_dmm)\r\nreturn ERR_PTR(-EINVAL);\r\nmutex_lock(&dev->struct_mutex);\r\ngsize.bytes = PAGE_ALIGN(size);\r\nobj = omap_gem_new(dev, gsize, OMAP_BO_MEM_DMABUF | OMAP_BO_WC);\r\nif (!obj) {\r\nobj = ERR_PTR(-ENOMEM);\r\ngoto done;\r\n}\r\nomap_obj = to_omap_bo(obj);\r\nomap_obj->sgt = sgt;\r\nif (sgt->orig_nents == 1) {\r\nomap_obj->dma_addr = sg_dma_address(sgt->sgl);\r\n} else {\r\nstruct sg_page_iter iter;\r\nstruct page **pages;\r\nunsigned int npages;\r\nunsigned int i = 0;\r\nnpages = DIV_ROUND_UP(size, PAGE_SIZE);\r\npages = kcalloc(npages, sizeof(*pages), GFP_KERNEL);\r\nif (!pages) {\r\nomap_gem_free_object(obj);\r\nobj = ERR_PTR(-ENOMEM);\r\ngoto done;\r\n}\r\nomap_obj->pages = pages;\r\nfor_each_sg_page(sgt->sgl, &iter, sgt->orig_nents, 0) {\r\npages[i++] = sg_page_iter_page(&iter);\r\nif (i > npages)\r\nbreak;\r\n}\r\nif (WARN_ON(i != npages)) {\r\nomap_gem_free_object(obj);\r\nobj = ERR_PTR(-ENOMEM);\r\ngoto done;\r\n}\r\n}\r\ndone:\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn obj;\r\n}\r\nint omap_gem_new_handle(struct drm_device *dev, struct drm_file *file,\r\nunion omap_gem_size gsize, uint32_t flags, uint32_t *handle)\r\n{\r\nstruct drm_gem_object *obj;\r\nint ret;\r\nobj = omap_gem_new(dev, gsize, flags);\r\nif (!obj)\r\nreturn -ENOMEM;\r\nret = drm_gem_handle_create(file, obj, handle);\r\nif (ret) {\r\nomap_gem_free_object(obj);\r\nreturn ret;\r\n}\r\ndrm_gem_object_unreference_unlocked(obj);\r\nreturn 0;\r\n}\r\nvoid omap_gem_init(struct drm_device *dev)\r\n{\r\nstruct omap_drm_private *priv = dev->dev_private;\r\nstruct omap_drm_usergart *usergart;\r\nconst enum tiler_fmt fmts[] = {\r\nTILFMT_8BIT, TILFMT_16BIT, TILFMT_32BIT\r\n};\r\nint i, j;\r\nif (!dmm_is_available()) {\r\ndev_warn(dev->dev, "DMM not available, disable DMM support\n");\r\nreturn;\r\n}\r\nusergart = kcalloc(3, sizeof(*usergart), GFP_KERNEL);\r\nif (!usergart)\r\nreturn;\r\nfor (i = 0; i < ARRAY_SIZE(fmts); i++) {\r\nuint16_t h = 1, w = PAGE_SIZE >> i;\r\ntiler_align(fmts[i], &w, &h);\r\nusergart[i].height = h;\r\nusergart[i].height_shift = ilog2(h);\r\nusergart[i].stride_pfn = tiler_stride(fmts[i], 0) >> PAGE_SHIFT;\r\nusergart[i].slot_shift = ilog2((PAGE_SIZE / h) >> i);\r\nfor (j = 0; j < NUM_USERGART_ENTRIES; j++) {\r\nstruct omap_drm_usergart_entry *entry;\r\nstruct tiler_block *block;\r\nentry = &usergart[i].entry[j];\r\nblock = tiler_reserve_2d(fmts[i], w, h, PAGE_SIZE);\r\nif (IS_ERR(block)) {\r\ndev_err(dev->dev,\r\n"reserve failed: %d, %d, %ld\n",\r\ni, j, PTR_ERR(block));\r\nreturn;\r\n}\r\nentry->dma_addr = tiler_ssptr(block);\r\nentry->block = block;\r\nDBG("%d:%d: %dx%d: dma_addr=%pad stride=%d", i, j, w, h,\r\n&entry->dma_addr,\r\nusergart[i].stride_pfn << PAGE_SHIFT);\r\n}\r\n}\r\npriv->usergart = usergart;\r\npriv->has_dmm = true;\r\n}\r\nvoid omap_gem_deinit(struct drm_device *dev)\r\n{\r\nstruct omap_drm_private *priv = dev->dev_private;\r\nkfree(priv->usergart);\r\n}
