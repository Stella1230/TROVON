static unsigned int vm_node_end(unsigned int start, unsigned int pgnr)\r\n{\r\nreturn start + pgnr_to_size(pgnr);\r\n}\r\nstatic int addr_in_vm_node(unsigned int addr,\r\nstruct hmm_vm_node *node)\r\n{\r\nreturn (addr >= node->start) && (addr < (node->start + node->size));\r\n}\r\nint hmm_vm_init(struct hmm_vm *vm, unsigned int start,\r\nunsigned int size)\r\n{\r\nif (!vm)\r\nreturn -1;\r\nvm->start = start;\r\nvm->pgnr = size_to_pgnr_ceil(size);\r\nvm->size = pgnr_to_size(vm->pgnr);\r\nINIT_LIST_HEAD(&vm->vm_node_list);\r\nspin_lock_init(&vm->lock);\r\nvm->cache = kmem_cache_create("atomisp_vm", sizeof(struct hmm_vm_node),\r\n0, 0, NULL);\r\nreturn vm->cache != NULL ? 0 : -ENOMEM;\r\n}\r\nvoid hmm_vm_clean(struct hmm_vm *vm)\r\n{\r\nstruct hmm_vm_node *node, *tmp;\r\nstruct list_head new_head;\r\nif (!vm)\r\nreturn;\r\nspin_lock(&vm->lock);\r\nlist_replace_init(&vm->vm_node_list, &new_head);\r\nspin_unlock(&vm->lock);\r\nlist_for_each_entry_safe(node, tmp, &new_head, list) {\r\nlist_del(&node->list);\r\nkmem_cache_free(vm->cache, node);\r\n}\r\nkmem_cache_destroy(vm->cache);\r\n}\r\nstatic struct hmm_vm_node *alloc_hmm_vm_node(unsigned int pgnr,\r\nstruct hmm_vm *vm)\r\n{\r\nstruct hmm_vm_node *node;\r\nnode = kmem_cache_alloc(vm->cache, GFP_KERNEL);\r\nif (!node) {\r\ndev_err(atomisp_dev, "out of memory.\n");\r\nreturn NULL;\r\n}\r\nINIT_LIST_HEAD(&node->list);\r\nnode->pgnr = pgnr;\r\nnode->size = pgnr_to_size(pgnr);\r\nnode->vm = vm;\r\nreturn node;\r\n}\r\nstruct hmm_vm_node *hmm_vm_alloc_node(struct hmm_vm *vm, unsigned int pgnr)\r\n{\r\nstruct list_head *head;\r\nstruct hmm_vm_node *node, *cur, *next;\r\nunsigned int vm_start, vm_end;\r\nunsigned int addr;\r\nunsigned int size;\r\nif (!vm)\r\nreturn NULL;\r\nvm_start = vm->start;\r\nvm_end = vm_node_end(vm->start, vm->pgnr);\r\nsize = pgnr_to_size(pgnr);\r\naddr = vm_start;\r\nhead = &vm->vm_node_list;\r\nnode = alloc_hmm_vm_node(pgnr, vm);\r\nif (!node) {\r\ndev_err(atomisp_dev, "no memory to allocate hmm vm node.\n");\r\nreturn NULL;\r\n}\r\nspin_lock(&vm->lock);\r\nlist_for_each_entry(cur, head, list) {\r\naddr = PAGE_ALIGN(vm_node_end(cur->start, cur->pgnr) + 1);\r\nif (list_is_last(&cur->list, head)) {\r\nif (addr + size > vm_end) {\r\nspin_unlock(&vm->lock);\r\nkmem_cache_free(vm->cache, node);\r\ndev_err(atomisp_dev,\r\n"no enough virtual address space.\n");\r\nreturn NULL;\r\n}\r\nbreak;\r\n}\r\nnext = list_entry(cur->list.next, struct hmm_vm_node, list);\r\nif ((next->start - addr) > size)\r\nbreak;\r\n}\r\nnode->start = addr;\r\nnode->vm = vm;\r\nlist_add(&node->list, &cur->list);\r\nspin_unlock(&vm->lock);\r\nreturn node;\r\n}\r\nvoid hmm_vm_free_node(struct hmm_vm_node *node)\r\n{\r\nstruct hmm_vm *vm;\r\nif (!node)\r\nreturn;\r\nvm = node->vm;\r\nspin_lock(&vm->lock);\r\nlist_del(&node->list);\r\nspin_unlock(&vm->lock);\r\nkmem_cache_free(vm->cache, node);\r\n}\r\nstruct hmm_vm_node *hmm_vm_find_node_start(struct hmm_vm *vm, unsigned int addr)\r\n{\r\nstruct hmm_vm_node *node;\r\nif (!vm)\r\nreturn NULL;\r\nspin_lock(&vm->lock);\r\nlist_for_each_entry(node, &vm->vm_node_list, list) {\r\nif (node->start == addr) {\r\nspin_unlock(&vm->lock);\r\nreturn node;\r\n}\r\n}\r\nspin_unlock(&vm->lock);\r\nreturn NULL;\r\n}\r\nstruct hmm_vm_node *hmm_vm_find_node_in_range(struct hmm_vm *vm,\r\nunsigned int addr)\r\n{\r\nstruct hmm_vm_node *node;\r\nif (!vm)\r\nreturn NULL;\r\nspin_lock(&vm->lock);\r\nlist_for_each_entry(node, &vm->vm_node_list, list) {\r\nif (addr_in_vm_node(addr, node)) {\r\nspin_unlock(&vm->lock);\r\nreturn node;\r\n}\r\n}\r\nspin_unlock(&vm->lock);\r\nreturn NULL;\r\n}
