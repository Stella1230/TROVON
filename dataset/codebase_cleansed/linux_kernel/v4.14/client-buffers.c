int ishtp_cl_alloc_rx_ring(struct ishtp_cl *cl)\r\n{\r\nsize_t len = cl->device->fw_client->props.max_msg_length;\r\nint j;\r\nstruct ishtp_cl_rb *rb;\r\nint ret = 0;\r\nunsigned long flags;\r\nfor (j = 0; j < cl->rx_ring_size; ++j) {\r\nrb = ishtp_io_rb_init(cl);\r\nif (!rb) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = ishtp_io_rb_alloc_buf(rb, len);\r\nif (ret)\r\ngoto out;\r\nspin_lock_irqsave(&cl->free_list_spinlock, flags);\r\nlist_add_tail(&rb->list, &cl->free_rb_list.list);\r\nspin_unlock_irqrestore(&cl->free_list_spinlock, flags);\r\n}\r\nreturn 0;\r\nout:\r\ndev_err(&cl->device->dev, "error in allocating Rx buffers\n");\r\nishtp_cl_free_rx_ring(cl);\r\nreturn ret;\r\n}\r\nint ishtp_cl_alloc_tx_ring(struct ishtp_cl *cl)\r\n{\r\nsize_t len = cl->device->fw_client->props.max_msg_length;\r\nint j;\r\nunsigned long flags;\r\nfor (j = 0; j < cl->tx_ring_size; ++j) {\r\nstruct ishtp_cl_tx_ring *tx_buf;\r\ntx_buf = kzalloc(sizeof(struct ishtp_cl_tx_ring), GFP_KERNEL);\r\nif (!tx_buf)\r\ngoto out;\r\ntx_buf->send_buf.data = kmalloc(len, GFP_KERNEL);\r\nif (!tx_buf->send_buf.data) {\r\nkfree(tx_buf);\r\ngoto out;\r\n}\r\nspin_lock_irqsave(&cl->tx_free_list_spinlock, flags);\r\nlist_add_tail(&tx_buf->list, &cl->tx_free_list.list);\r\nspin_unlock_irqrestore(&cl->tx_free_list_spinlock, flags);\r\n}\r\nreturn 0;\r\nout:\r\ndev_err(&cl->device->dev, "error in allocating Tx pool\n");\r\nishtp_cl_free_rx_ring(cl);\r\nreturn -ENOMEM;\r\n}\r\nvoid ishtp_cl_free_rx_ring(struct ishtp_cl *cl)\r\n{\r\nstruct ishtp_cl_rb *rb;\r\nunsigned long flags;\r\nspin_lock_irqsave(&cl->free_list_spinlock, flags);\r\nwhile (!list_empty(&cl->free_rb_list.list)) {\r\nrb = list_entry(cl->free_rb_list.list.next, struct ishtp_cl_rb,\r\nlist);\r\nlist_del(&rb->list);\r\nkfree(rb->buffer.data);\r\nkfree(rb);\r\n}\r\nspin_unlock_irqrestore(&cl->free_list_spinlock, flags);\r\nspin_lock_irqsave(&cl->in_process_spinlock, flags);\r\nwhile (!list_empty(&cl->in_process_list.list)) {\r\nrb = list_entry(cl->in_process_list.list.next,\r\nstruct ishtp_cl_rb, list);\r\nlist_del(&rb->list);\r\nkfree(rb->buffer.data);\r\nkfree(rb);\r\n}\r\nspin_unlock_irqrestore(&cl->in_process_spinlock, flags);\r\n}\r\nvoid ishtp_cl_free_tx_ring(struct ishtp_cl *cl)\r\n{\r\nstruct ishtp_cl_tx_ring *tx_buf;\r\nunsigned long flags;\r\nspin_lock_irqsave(&cl->tx_free_list_spinlock, flags);\r\nwhile (!list_empty(&cl->tx_free_list.list)) {\r\ntx_buf = list_entry(cl->tx_free_list.list.next,\r\nstruct ishtp_cl_tx_ring, list);\r\nlist_del(&tx_buf->list);\r\nkfree(tx_buf->send_buf.data);\r\nkfree(tx_buf);\r\n}\r\nspin_unlock_irqrestore(&cl->tx_free_list_spinlock, flags);\r\nspin_lock_irqsave(&cl->tx_list_spinlock, flags);\r\nwhile (!list_empty(&cl->tx_list.list)) {\r\ntx_buf = list_entry(cl->tx_list.list.next,\r\nstruct ishtp_cl_tx_ring, list);\r\nlist_del(&tx_buf->list);\r\nkfree(tx_buf->send_buf.data);\r\nkfree(tx_buf);\r\n}\r\nspin_unlock_irqrestore(&cl->tx_list_spinlock, flags);\r\n}\r\nvoid ishtp_io_rb_free(struct ishtp_cl_rb *rb)\r\n{\r\nif (rb == NULL)\r\nreturn;\r\nkfree(rb->buffer.data);\r\nkfree(rb);\r\n}\r\nstruct ishtp_cl_rb *ishtp_io_rb_init(struct ishtp_cl *cl)\r\n{\r\nstruct ishtp_cl_rb *rb;\r\nrb = kzalloc(sizeof(struct ishtp_cl_rb), GFP_KERNEL);\r\nif (!rb)\r\nreturn NULL;\r\nINIT_LIST_HEAD(&rb->list);\r\nrb->cl = cl;\r\nrb->buf_idx = 0;\r\nreturn rb;\r\n}\r\nint ishtp_io_rb_alloc_buf(struct ishtp_cl_rb *rb, size_t length)\r\n{\r\nif (!rb)\r\nreturn -EINVAL;\r\nif (length == 0)\r\nreturn 0;\r\nrb->buffer.data = kmalloc(length, GFP_KERNEL);\r\nif (!rb->buffer.data)\r\nreturn -ENOMEM;\r\nrb->buffer.size = length;\r\nreturn 0;\r\n}\r\nint ishtp_cl_io_rb_recycle(struct ishtp_cl_rb *rb)\r\n{\r\nstruct ishtp_cl *cl;\r\nint rets = 0;\r\nunsigned long flags;\r\nif (!rb || !rb->cl)\r\nreturn -EFAULT;\r\ncl = rb->cl;\r\nspin_lock_irqsave(&cl->free_list_spinlock, flags);\r\nlist_add_tail(&rb->list, &cl->free_rb_list.list);\r\nspin_unlock_irqrestore(&cl->free_list_spinlock, flags);\r\nif (!cl->out_flow_ctrl_creds)\r\nrets = ishtp_cl_read_start(cl);\r\nreturn rets;\r\n}
