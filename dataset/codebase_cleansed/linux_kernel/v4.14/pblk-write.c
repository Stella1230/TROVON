static unsigned long pblk_end_w_bio(struct pblk *pblk, struct nvm_rq *rqd,\r\nstruct pblk_c_ctx *c_ctx)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct bio *original_bio;\r\nunsigned long ret;\r\nint i;\r\nfor (i = 0; i < c_ctx->nr_valid; i++) {\r\nstruct pblk_w_ctx *w_ctx;\r\nw_ctx = pblk_rb_w_ctx(&pblk->rwb, c_ctx->sentry + i);\r\nwhile ((original_bio = bio_list_pop(&w_ctx->bios)))\r\nbio_endio(original_bio);\r\n}\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_add(c_ctx->nr_valid, &pblk->sync_writes);\r\n#endif\r\nret = pblk_rb_sync_advance(&pblk->rwb, c_ctx->nr_valid);\r\nnvm_dev_dma_free(dev->parent, rqd->meta_list, rqd->dma_meta_list);\r\nbio_put(rqd->bio);\r\npblk_free_rqd(pblk, rqd, WRITE);\r\nreturn ret;\r\n}\r\nstatic unsigned long pblk_end_queued_w_bio(struct pblk *pblk,\r\nstruct nvm_rq *rqd,\r\nstruct pblk_c_ctx *c_ctx)\r\n{\r\nlist_del(&c_ctx->list);\r\nreturn pblk_end_w_bio(pblk, rqd, c_ctx);\r\n}\r\nstatic void pblk_complete_write(struct pblk *pblk, struct nvm_rq *rqd,\r\nstruct pblk_c_ctx *c_ctx)\r\n{\r\nstruct pblk_c_ctx *c, *r;\r\nunsigned long flags;\r\nunsigned long pos;\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_sub(c_ctx->nr_valid, &pblk->inflight_writes);\r\n#endif\r\npblk_up_rq(pblk, rqd->ppa_list, rqd->nr_ppas, c_ctx->lun_bitmap);\r\npos = pblk_rb_sync_init(&pblk->rwb, &flags);\r\nif (pos == c_ctx->sentry) {\r\npos = pblk_end_w_bio(pblk, rqd, c_ctx);\r\nretry:\r\nlist_for_each_entry_safe(c, r, &pblk->compl_list, list) {\r\nrqd = nvm_rq_from_c_ctx(c);\r\nif (c->sentry == pos) {\r\npos = pblk_end_queued_w_bio(pblk, rqd, c);\r\ngoto retry;\r\n}\r\n}\r\n} else {\r\nWARN_ON(nvm_rq_from_c_ctx(c_ctx) != rqd);\r\nlist_add_tail(&c_ctx->list, &pblk->compl_list);\r\n}\r\npblk_rb_sync_end(&pblk->rwb, &flags);\r\n}\r\nstatic void pblk_end_w_fail(struct pblk *pblk, struct nvm_rq *rqd)\r\n{\r\nvoid *comp_bits = &rqd->ppa_status;\r\nstruct pblk_c_ctx *c_ctx = nvm_rq_to_pdu(rqd);\r\nstruct pblk_rec_ctx *recovery;\r\nstruct ppa_addr *ppa_list = rqd->ppa_list;\r\nint nr_ppas = rqd->nr_ppas;\r\nunsigned int c_entries;\r\nint bit, ret;\r\nif (unlikely(nr_ppas == 1))\r\nppa_list = &rqd->ppa_addr;\r\nrecovery = mempool_alloc(pblk->rec_pool, GFP_ATOMIC);\r\nif (!recovery) {\r\npr_err("pblk: could not allocate recovery context\n");\r\nreturn;\r\n}\r\nINIT_LIST_HEAD(&recovery->failed);\r\nbit = -1;\r\nwhile ((bit = find_next_bit(comp_bits, nr_ppas, bit + 1)) < nr_ppas) {\r\nstruct pblk_rb_entry *entry;\r\nstruct ppa_addr ppa;\r\nif (bit > c_ctx->nr_valid) {\r\nWARN_ONCE(1, "pblk: corrupted write request\n");\r\nmempool_free(recovery, pblk->rec_pool);\r\ngoto out;\r\n}\r\nppa = ppa_list[bit];\r\nentry = pblk_rb_sync_scan_entry(&pblk->rwb, &ppa);\r\nif (!entry) {\r\npr_err("pblk: could not scan entry on write failure\n");\r\nmempool_free(recovery, pblk->rec_pool);\r\ngoto out;\r\n}\r\nlist_add_tail(&entry->index, &recovery->failed);\r\n}\r\nc_entries = find_first_bit(comp_bits, nr_ppas);\r\nret = pblk_recov_setup_rq(pblk, c_ctx, recovery, comp_bits, c_entries);\r\nif (ret) {\r\npr_err("pblk: could not recover from write failure\n");\r\nmempool_free(recovery, pblk->rec_pool);\r\ngoto out;\r\n}\r\nINIT_WORK(&recovery->ws_rec, pblk_submit_rec);\r\nqueue_work(pblk->close_wq, &recovery->ws_rec);\r\nout:\r\npblk_complete_write(pblk, rqd, c_ctx);\r\n}\r\nstatic void pblk_end_io_write(struct nvm_rq *rqd)\r\n{\r\nstruct pblk *pblk = rqd->private;\r\nstruct pblk_c_ctx *c_ctx = nvm_rq_to_pdu(rqd);\r\nif (rqd->error) {\r\npblk_log_write_err(pblk, rqd);\r\nreturn pblk_end_w_fail(pblk, rqd);\r\n}\r\n#ifdef CONFIG_NVM_DEBUG\r\nelse\r\nWARN_ONCE(rqd->bio->bi_status, "pblk: corrupted write error\n");\r\n#endif\r\npblk_complete_write(pblk, rqd, c_ctx);\r\natomic_dec(&pblk->inflight_io);\r\n}\r\nstatic void pblk_end_io_write_meta(struct nvm_rq *rqd)\r\n{\r\nstruct pblk *pblk = rqd->private;\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct pblk_g_ctx *m_ctx = nvm_rq_to_pdu(rqd);\r\nstruct pblk_line *line = m_ctx->private;\r\nstruct pblk_emeta *emeta = line->emeta;\r\nint sync;\r\npblk_up_page(pblk, rqd->ppa_list, rqd->nr_ppas);\r\nif (rqd->error) {\r\npblk_log_write_err(pblk, rqd);\r\npr_err("pblk: metadata I/O failed. Line %d\n", line->id);\r\n}\r\n#ifdef CONFIG_NVM_DEBUG\r\nelse\r\nWARN_ONCE(rqd->bio->bi_status, "pblk: corrupted write error\n");\r\n#endif\r\nsync = atomic_add_return(rqd->nr_ppas, &emeta->sync);\r\nif (sync == emeta->nr_entries)\r\npblk_line_run_ws(pblk, line, NULL, pblk_line_close_ws,\r\npblk->close_wq);\r\nbio_put(rqd->bio);\r\nnvm_dev_dma_free(dev->parent, rqd->meta_list, rqd->dma_meta_list);\r\npblk_free_rqd(pblk, rqd, READ);\r\natomic_dec(&pblk->inflight_io);\r\n}\r\nstatic int pblk_setup_w_rq(struct pblk *pblk, struct nvm_rq *rqd,\r\nstruct pblk_c_ctx *c_ctx, struct ppa_addr *erase_ppa)\r\n{\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct pblk_line *e_line = pblk_line_get_erase(pblk);\r\nunsigned int valid = c_ctx->nr_valid;\r\nunsigned int padded = c_ctx->nr_padded;\r\nunsigned int nr_secs = valid + padded;\r\nunsigned long *lun_bitmap;\r\nint ret = 0;\r\nlun_bitmap = kzalloc(lm->lun_bitmap_len, GFP_KERNEL);\r\nif (!lun_bitmap)\r\nreturn -ENOMEM;\r\nc_ctx->lun_bitmap = lun_bitmap;\r\nret = pblk_alloc_w_rq(pblk, rqd, nr_secs, pblk_end_io_write);\r\nif (ret) {\r\nkfree(lun_bitmap);\r\nreturn ret;\r\n}\r\nif (likely(!e_line || !atomic_read(&e_line->left_eblks)))\r\npblk_map_rq(pblk, rqd, c_ctx->sentry, lun_bitmap, valid, 0);\r\nelse\r\npblk_map_erase_rq(pblk, rqd, c_ctx->sentry, lun_bitmap,\r\nvalid, erase_ppa);\r\nreturn 0;\r\n}\r\nint pblk_setup_w_rec_rq(struct pblk *pblk, struct nvm_rq *rqd,\r\nstruct pblk_c_ctx *c_ctx)\r\n{\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nunsigned long *lun_bitmap;\r\nint ret;\r\nlun_bitmap = kzalloc(lm->lun_bitmap_len, GFP_KERNEL);\r\nif (!lun_bitmap)\r\nreturn -ENOMEM;\r\nc_ctx->lun_bitmap = lun_bitmap;\r\nret = pblk_alloc_w_rq(pblk, rqd, rqd->nr_ppas, pblk_end_io_write);\r\nif (ret)\r\nreturn ret;\r\npblk_map_rq(pblk, rqd, c_ctx->sentry, lun_bitmap, c_ctx->nr_valid, 0);\r\nrqd->ppa_status = (u64)0;\r\nrqd->flags = pblk_set_progr_mode(pblk, WRITE);\r\nreturn ret;\r\n}\r\nstatic int pblk_calc_secs_to_sync(struct pblk *pblk, unsigned int secs_avail,\r\nunsigned int secs_to_flush)\r\n{\r\nint secs_to_sync;\r\nsecs_to_sync = pblk_calc_secs(pblk, secs_avail, secs_to_flush);\r\n#ifdef CONFIG_NVM_DEBUG\r\nif ((!secs_to_sync && secs_to_flush)\r\n|| (secs_to_sync < 0)\r\n|| (secs_to_sync > secs_avail && !secs_to_flush)) {\r\npr_err("pblk: bad sector calculation (a:%d,s:%d,f:%d)\n",\r\nsecs_avail, secs_to_sync, secs_to_flush);\r\n}\r\n#endif\r\nreturn secs_to_sync;\r\n}\r\nstatic inline int pblk_valid_meta_ppa(struct pblk *pblk,\r\nstruct pblk_line *meta_line,\r\nstruct ppa_addr *ppa_list, int nr_ppas)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct pblk_line *data_line;\r\nstruct ppa_addr ppa, ppa_opt;\r\nu64 paddr;\r\nint i;\r\ndata_line = &pblk->lines[pblk_dev_ppa_to_line(ppa_list[0])];\r\npaddr = pblk_lookup_page(pblk, meta_line);\r\nppa = addr_to_gen_ppa(pblk, paddr, 0);\r\nif (test_bit(pblk_ppa_to_pos(geo, ppa), data_line->blk_bitmap))\r\nreturn 1;\r\nppa_opt = addr_to_gen_ppa(pblk, paddr + data_line->meta_distance, 0);\r\nif (unlikely(ppa_opt.ppa == ppa.ppa)) {\r\ndata_line->meta_distance--;\r\nreturn 0;\r\n}\r\nfor (i = 0; i < nr_ppas; i += pblk->min_write_pgs)\r\nif (ppa_list[i].g.ch == ppa_opt.g.ch &&\r\nppa_list[i].g.lun == ppa_opt.g.lun)\r\nreturn 1;\r\nif (test_bit(pblk_ppa_to_pos(geo, ppa_opt), data_line->blk_bitmap)) {\r\nfor (i = 0; i < nr_ppas; i += pblk->min_write_pgs)\r\nif (ppa_list[i].g.ch == ppa.g.ch &&\r\nppa_list[i].g.lun == ppa.g.lun)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nint pblk_submit_meta_io(struct pblk *pblk, struct pblk_line *meta_line)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct pblk_emeta *emeta = meta_line->emeta;\r\nstruct pblk_g_ctx *m_ctx;\r\nstruct bio *bio;\r\nstruct nvm_rq *rqd;\r\nvoid *data;\r\nu64 paddr;\r\nint rq_ppas = pblk->min_write_pgs;\r\nint id = meta_line->id;\r\nint rq_len;\r\nint i, j;\r\nint ret;\r\nrqd = pblk_alloc_rqd(pblk, READ);\r\nif (IS_ERR(rqd)) {\r\npr_err("pblk: cannot allocate write req.\n");\r\nreturn PTR_ERR(rqd);\r\n}\r\nm_ctx = nvm_rq_to_pdu(rqd);\r\nm_ctx->private = meta_line;\r\nrq_len = rq_ppas * geo->sec_size;\r\ndata = ((void *)emeta->buf) + emeta->mem;\r\nbio = pblk_bio_map_addr(pblk, data, rq_ppas, rq_len,\r\nl_mg->emeta_alloc_type, GFP_KERNEL);\r\nif (IS_ERR(bio)) {\r\nret = PTR_ERR(bio);\r\ngoto fail_free_rqd;\r\n}\r\nbio->bi_iter.bi_sector = 0;\r\nbio_set_op_attrs(bio, REQ_OP_WRITE, 0);\r\nrqd->bio = bio;\r\nret = pblk_alloc_w_rq(pblk, rqd, rq_ppas, pblk_end_io_write_meta);\r\nif (ret)\r\ngoto fail_free_bio;\r\nfor (i = 0; i < rqd->nr_ppas; ) {\r\nspin_lock(&meta_line->lock);\r\npaddr = __pblk_alloc_page(pblk, meta_line, rq_ppas);\r\nspin_unlock(&meta_line->lock);\r\nfor (j = 0; j < rq_ppas; j++, i++, paddr++)\r\nrqd->ppa_list[i] = addr_to_gen_ppa(pblk, paddr, id);\r\n}\r\nemeta->mem += rq_len;\r\nif (emeta->mem >= lm->emeta_len[0]) {\r\nspin_lock(&l_mg->close_lock);\r\nlist_del(&meta_line->list);\r\nWARN(!bitmap_full(meta_line->map_bitmap, lm->sec_per_line),\r\n"pblk: corrupt meta line %d\n", meta_line->id);\r\nspin_unlock(&l_mg->close_lock);\r\n}\r\npblk_down_page(pblk, rqd->ppa_list, rqd->nr_ppas);\r\nret = pblk_submit_io(pblk, rqd);\r\nif (ret) {\r\npr_err("pblk: emeta I/O submission failed: %d\n", ret);\r\ngoto fail_rollback;\r\n}\r\nreturn NVM_IO_OK;\r\nfail_rollback:\r\npblk_up_page(pblk, rqd->ppa_list, rqd->nr_ppas);\r\nspin_lock(&l_mg->close_lock);\r\npblk_dealloc_page(pblk, meta_line, rq_ppas);\r\nlist_add(&meta_line->list, &meta_line->list);\r\nspin_unlock(&l_mg->close_lock);\r\nnvm_dev_dma_free(dev->parent, rqd->meta_list, rqd->dma_meta_list);\r\nfail_free_bio:\r\nif (likely(l_mg->emeta_alloc_type == PBLK_VMALLOC_META))\r\nbio_put(bio);\r\nfail_free_rqd:\r\npblk_free_rqd(pblk, rqd, READ);\r\nreturn ret;\r\n}\r\nstatic int pblk_sched_meta_io(struct pblk *pblk, struct ppa_addr *prev_list,\r\nint prev_n)\r\n{\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_line *meta_line;\r\nspin_lock(&l_mg->close_lock);\r\nretry:\r\nif (list_empty(&l_mg->emeta_list)) {\r\nspin_unlock(&l_mg->close_lock);\r\nreturn 0;\r\n}\r\nmeta_line = list_first_entry(&l_mg->emeta_list, struct pblk_line, list);\r\nif (bitmap_full(meta_line->map_bitmap, lm->sec_per_line))\r\ngoto retry;\r\nspin_unlock(&l_mg->close_lock);\r\nif (!pblk_valid_meta_ppa(pblk, meta_line, prev_list, prev_n))\r\nreturn 0;\r\nreturn pblk_submit_meta_io(pblk, meta_line);\r\n}\r\nstatic int pblk_submit_io_set(struct pblk *pblk, struct nvm_rq *rqd)\r\n{\r\nstruct pblk_c_ctx *c_ctx = nvm_rq_to_pdu(rqd);\r\nstruct ppa_addr erase_ppa;\r\nint err;\r\nppa_set_empty(&erase_ppa);\r\nerr = pblk_setup_w_rq(pblk, rqd, c_ctx, &erase_ppa);\r\nif (err) {\r\npr_err("pblk: could not setup write request: %d\n", err);\r\nreturn NVM_IO_ERR;\r\n}\r\nif (likely(ppa_empty(erase_ppa))) {\r\nerr = pblk_sched_meta_io(pblk, rqd->ppa_list, rqd->nr_ppas);\r\nif (err) {\r\npr_err("pblk: metadata I/O submission failed: %d", err);\r\nreturn NVM_IO_ERR;\r\n}\r\nerr = pblk_submit_io(pblk, rqd);\r\nif (err) {\r\npr_err("pblk: data I/O submission failed: %d\n", err);\r\nreturn NVM_IO_ERR;\r\n}\r\n} else {\r\nerr = pblk_submit_io(pblk, rqd);\r\nif (err) {\r\npr_err("pblk: data I/O submission failed: %d\n", err);\r\nreturn NVM_IO_ERR;\r\n}\r\nif (pblk_blk_erase_async(pblk, erase_ppa)) {\r\nstruct pblk_line *e_line = pblk_line_get_erase(pblk);\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nint bit;\r\natomic_inc(&e_line->left_eblks);\r\nbit = pblk_ppa_to_pos(geo, erase_ppa);\r\nWARN_ON(!test_and_clear_bit(bit, e_line->erase_bitmap));\r\n}\r\n}\r\nreturn NVM_IO_OK;\r\n}\r\nstatic void pblk_free_write_rqd(struct pblk *pblk, struct nvm_rq *rqd)\r\n{\r\nstruct pblk_c_ctx *c_ctx = nvm_rq_to_pdu(rqd);\r\nstruct bio *bio = rqd->bio;\r\nif (c_ctx->nr_padded)\r\npblk_bio_free_pages(pblk, bio, rqd->nr_ppas, c_ctx->nr_padded);\r\n}\r\nstatic int pblk_submit_write(struct pblk *pblk)\r\n{\r\nstruct bio *bio;\r\nstruct nvm_rq *rqd;\r\nunsigned int secs_avail, secs_to_sync, secs_to_com;\r\nunsigned int secs_to_flush;\r\nunsigned long pos;\r\nsecs_avail = pblk_rb_read_count(&pblk->rwb);\r\nif (!secs_avail)\r\nreturn 1;\r\nsecs_to_flush = pblk_rb_sync_point_count(&pblk->rwb);\r\nif (!secs_to_flush && secs_avail < pblk->min_write_pgs)\r\nreturn 1;\r\nrqd = pblk_alloc_rqd(pblk, WRITE);\r\nif (IS_ERR(rqd)) {\r\npr_err("pblk: cannot allocate write req.\n");\r\nreturn 1;\r\n}\r\nbio = bio_alloc(GFP_KERNEL, pblk->max_write_pgs);\r\nif (!bio) {\r\npr_err("pblk: cannot allocate write bio\n");\r\ngoto fail_free_rqd;\r\n}\r\nbio->bi_iter.bi_sector = 0;\r\nbio_set_op_attrs(bio, REQ_OP_WRITE, 0);\r\nrqd->bio = bio;\r\nsecs_to_sync = pblk_calc_secs_to_sync(pblk, secs_avail, secs_to_flush);\r\nif (secs_to_sync > pblk->max_write_pgs) {\r\npr_err("pblk: bad buffer sync calculation\n");\r\ngoto fail_put_bio;\r\n}\r\nsecs_to_com = (secs_to_sync > secs_avail) ? secs_avail : secs_to_sync;\r\npos = pblk_rb_read_commit(&pblk->rwb, secs_to_com);\r\nif (pblk_rb_read_to_bio(&pblk->rwb, rqd, bio, pos, secs_to_sync,\r\nsecs_avail)) {\r\npr_err("pblk: corrupted write bio\n");\r\ngoto fail_put_bio;\r\n}\r\nif (pblk_submit_io_set(pblk, rqd))\r\ngoto fail_free_bio;\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_add(secs_to_sync, &pblk->sub_writes);\r\n#endif\r\nreturn 0;\r\nfail_free_bio:\r\npblk_free_write_rqd(pblk, rqd);\r\nfail_put_bio:\r\nbio_put(bio);\r\nfail_free_rqd:\r\npblk_free_rqd(pblk, rqd, WRITE);\r\nreturn 1;\r\n}\r\nint pblk_write_ts(void *data)\r\n{\r\nstruct pblk *pblk = data;\r\nwhile (!kthread_should_stop()) {\r\nif (!pblk_submit_write(pblk))\r\ncontinue;\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nio_schedule();\r\n}\r\nreturn 0;\r\n}
