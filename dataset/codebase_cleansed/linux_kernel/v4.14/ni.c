u32 tn_smc_rreg(struct radeon_device *rdev, u32 reg)\r\n{\r\nunsigned long flags;\r\nu32 r;\r\nspin_lock_irqsave(&rdev->smc_idx_lock, flags);\r\nWREG32(TN_SMC_IND_INDEX_0, (reg));\r\nr = RREG32(TN_SMC_IND_DATA_0);\r\nspin_unlock_irqrestore(&rdev->smc_idx_lock, flags);\r\nreturn r;\r\n}\r\nvoid tn_smc_wreg(struct radeon_device *rdev, u32 reg, u32 v)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&rdev->smc_idx_lock, flags);\r\nWREG32(TN_SMC_IND_INDEX_0, (reg));\r\nWREG32(TN_SMC_IND_DATA_0, (v));\r\nspin_unlock_irqrestore(&rdev->smc_idx_lock, flags);\r\n}\r\nstatic void ni_init_golden_registers(struct radeon_device *rdev)\r\n{\r\nswitch (rdev->family) {\r\ncase CHIP_CAYMAN:\r\nradeon_program_register_sequence(rdev,\r\ncayman_golden_registers,\r\n(const u32)ARRAY_SIZE(cayman_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\ncayman_golden_registers2,\r\n(const u32)ARRAY_SIZE(cayman_golden_registers2));\r\nbreak;\r\ncase CHIP_ARUBA:\r\nif ((rdev->pdev->device == 0x9900) ||\r\n(rdev->pdev->device == 0x9901) ||\r\n(rdev->pdev->device == 0x9903) ||\r\n(rdev->pdev->device == 0x9904) ||\r\n(rdev->pdev->device == 0x9905) ||\r\n(rdev->pdev->device == 0x9906) ||\r\n(rdev->pdev->device == 0x9907) ||\r\n(rdev->pdev->device == 0x9908) ||\r\n(rdev->pdev->device == 0x9909) ||\r\n(rdev->pdev->device == 0x990A) ||\r\n(rdev->pdev->device == 0x990B) ||\r\n(rdev->pdev->device == 0x990C) ||\r\n(rdev->pdev->device == 0x990D) ||\r\n(rdev->pdev->device == 0x990E) ||\r\n(rdev->pdev->device == 0x990F) ||\r\n(rdev->pdev->device == 0x9910) ||\r\n(rdev->pdev->device == 0x9913) ||\r\n(rdev->pdev->device == 0x9917) ||\r\n(rdev->pdev->device == 0x9918)) {\r\nradeon_program_register_sequence(rdev,\r\ndvst_golden_registers,\r\n(const u32)ARRAY_SIZE(dvst_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\ndvst_golden_registers2,\r\n(const u32)ARRAY_SIZE(dvst_golden_registers2));\r\n} else {\r\nradeon_program_register_sequence(rdev,\r\nscrapper_golden_registers,\r\n(const u32)ARRAY_SIZE(scrapper_golden_registers));\r\nradeon_program_register_sequence(rdev,\r\ndvst_golden_registers2,\r\n(const u32)ARRAY_SIZE(dvst_golden_registers2));\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nint ni_mc_load_microcode(struct radeon_device *rdev)\r\n{\r\nconst __be32 *fw_data;\r\nu32 mem_type, running, blackout = 0;\r\nu32 *io_mc_regs;\r\nint i, ucode_size, regs_size;\r\nif (!rdev->mc_fw)\r\nreturn -EINVAL;\r\nswitch (rdev->family) {\r\ncase CHIP_BARTS:\r\nio_mc_regs = (u32 *)&barts_io_mc_regs;\r\nucode_size = BTC_MC_UCODE_SIZE;\r\nregs_size = BTC_IO_MC_REGS_SIZE;\r\nbreak;\r\ncase CHIP_TURKS:\r\nio_mc_regs = (u32 *)&turks_io_mc_regs;\r\nucode_size = BTC_MC_UCODE_SIZE;\r\nregs_size = BTC_IO_MC_REGS_SIZE;\r\nbreak;\r\ncase CHIP_CAICOS:\r\ndefault:\r\nio_mc_regs = (u32 *)&caicos_io_mc_regs;\r\nucode_size = BTC_MC_UCODE_SIZE;\r\nregs_size = BTC_IO_MC_REGS_SIZE;\r\nbreak;\r\ncase CHIP_CAYMAN:\r\nio_mc_regs = (u32 *)&cayman_io_mc_regs;\r\nucode_size = CAYMAN_MC_UCODE_SIZE;\r\nregs_size = BTC_IO_MC_REGS_SIZE;\r\nbreak;\r\n}\r\nmem_type = (RREG32(MC_SEQ_MISC0) & MC_SEQ_MISC0_GDDR5_MASK) >> MC_SEQ_MISC0_GDDR5_SHIFT;\r\nrunning = RREG32(MC_SEQ_SUP_CNTL) & RUN_MASK;\r\nif ((mem_type == MC_SEQ_MISC0_GDDR5_VALUE) && (running == 0)) {\r\nif (running) {\r\nblackout = RREG32(MC_SHARED_BLACKOUT_CNTL);\r\nWREG32(MC_SHARED_BLACKOUT_CNTL, 1);\r\n}\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000008);\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000010);\r\nfor (i = 0; i < regs_size; i++) {\r\nWREG32(MC_SEQ_IO_DEBUG_INDEX, io_mc_regs[(i << 1)]);\r\nWREG32(MC_SEQ_IO_DEBUG_DATA, io_mc_regs[(i << 1) + 1]);\r\n}\r\nfw_data = (const __be32 *)rdev->mc_fw->data;\r\nfor (i = 0; i < ucode_size; i++)\r\nWREG32(MC_SEQ_SUP_PGM, be32_to_cpup(fw_data++));\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000008);\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000004);\r\nWREG32(MC_SEQ_SUP_CNTL, 0x00000001);\r\nfor (i = 0; i < rdev->usec_timeout; i++) {\r\nif (RREG32(MC_IO_PAD_CNTL_D0) & MEM_FALL_OUT_CMD)\r\nbreak;\r\nudelay(1);\r\n}\r\nif (running)\r\nWREG32(MC_SHARED_BLACKOUT_CNTL, blackout);\r\n}\r\nreturn 0;\r\n}\r\nint ni_init_microcode(struct radeon_device *rdev)\r\n{\r\nconst char *chip_name;\r\nconst char *rlc_chip_name;\r\nsize_t pfp_req_size, me_req_size, rlc_req_size, mc_req_size;\r\nsize_t smc_req_size = 0;\r\nchar fw_name[30];\r\nint err;\r\nDRM_DEBUG("\n");\r\nswitch (rdev->family) {\r\ncase CHIP_BARTS:\r\nchip_name = "BARTS";\r\nrlc_chip_name = "BTC";\r\npfp_req_size = EVERGREEN_PFP_UCODE_SIZE * 4;\r\nme_req_size = EVERGREEN_PM4_UCODE_SIZE * 4;\r\nrlc_req_size = EVERGREEN_RLC_UCODE_SIZE * 4;\r\nmc_req_size = BTC_MC_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(BARTS_SMC_UCODE_SIZE, 4);\r\nbreak;\r\ncase CHIP_TURKS:\r\nchip_name = "TURKS";\r\nrlc_chip_name = "BTC";\r\npfp_req_size = EVERGREEN_PFP_UCODE_SIZE * 4;\r\nme_req_size = EVERGREEN_PM4_UCODE_SIZE * 4;\r\nrlc_req_size = EVERGREEN_RLC_UCODE_SIZE * 4;\r\nmc_req_size = BTC_MC_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(TURKS_SMC_UCODE_SIZE, 4);\r\nbreak;\r\ncase CHIP_CAICOS:\r\nchip_name = "CAICOS";\r\nrlc_chip_name = "BTC";\r\npfp_req_size = EVERGREEN_PFP_UCODE_SIZE * 4;\r\nme_req_size = EVERGREEN_PM4_UCODE_SIZE * 4;\r\nrlc_req_size = EVERGREEN_RLC_UCODE_SIZE * 4;\r\nmc_req_size = BTC_MC_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(CAICOS_SMC_UCODE_SIZE, 4);\r\nbreak;\r\ncase CHIP_CAYMAN:\r\nchip_name = "CAYMAN";\r\nrlc_chip_name = "CAYMAN";\r\npfp_req_size = CAYMAN_PFP_UCODE_SIZE * 4;\r\nme_req_size = CAYMAN_PM4_UCODE_SIZE * 4;\r\nrlc_req_size = CAYMAN_RLC_UCODE_SIZE * 4;\r\nmc_req_size = CAYMAN_MC_UCODE_SIZE * 4;\r\nsmc_req_size = ALIGN(CAYMAN_SMC_UCODE_SIZE, 4);\r\nbreak;\r\ncase CHIP_ARUBA:\r\nchip_name = "ARUBA";\r\nrlc_chip_name = "ARUBA";\r\npfp_req_size = CAYMAN_PFP_UCODE_SIZE * 4;\r\nme_req_size = CAYMAN_PM4_UCODE_SIZE * 4;\r\nrlc_req_size = ARUBA_RLC_UCODE_SIZE * 4;\r\nmc_req_size = 0;\r\nbreak;\r\ndefault: BUG();\r\n}\r\nDRM_INFO("Loading %s Microcode\n", chip_name);\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_pfp.bin", chip_name);\r\nerr = request_firmware(&rdev->pfp_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->pfp_fw->size != pfp_req_size) {\r\npr_err("ni_cp: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->pfp_fw->size, fw_name);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_me.bin", chip_name);\r\nerr = request_firmware(&rdev->me_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->me_fw->size != me_req_size) {\r\npr_err("ni_cp: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->me_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_rlc.bin", rlc_chip_name);\r\nerr = request_firmware(&rdev->rlc_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->rlc_fw->size != rlc_req_size) {\r\npr_err("ni_rlc: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->rlc_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\nif (!(rdev->flags & RADEON_IS_IGP)) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_mc.bin", chip_name);\r\nerr = request_firmware(&rdev->mc_fw, fw_name, rdev->dev);\r\nif (err)\r\ngoto out;\r\nif (rdev->mc_fw->size != mc_req_size) {\r\npr_err("ni_mc: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->mc_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n}\r\nif ((rdev->family >= CHIP_BARTS) && (rdev->family <= CHIP_CAYMAN)) {\r\nsnprintf(fw_name, sizeof(fw_name), "radeon/%s_smc.bin", chip_name);\r\nerr = request_firmware(&rdev->smc_fw, fw_name, rdev->dev);\r\nif (err) {\r\npr_err("smc: error loading firmware \"%s\"\n", fw_name);\r\nrelease_firmware(rdev->smc_fw);\r\nrdev->smc_fw = NULL;\r\nerr = 0;\r\n} else if (rdev->smc_fw->size != smc_req_size) {\r\npr_err("ni_mc: Bogus length %zu in firmware \"%s\"\n",\r\nrdev->mc_fw->size, fw_name);\r\nerr = -EINVAL;\r\n}\r\n}\r\nout:\r\nif (err) {\r\nif (err != -EINVAL)\r\npr_err("ni_cp: Failed to load firmware \"%s\"\n",\r\nfw_name);\r\nrelease_firmware(rdev->pfp_fw);\r\nrdev->pfp_fw = NULL;\r\nrelease_firmware(rdev->me_fw);\r\nrdev->me_fw = NULL;\r\nrelease_firmware(rdev->rlc_fw);\r\nrdev->rlc_fw = NULL;\r\nrelease_firmware(rdev->mc_fw);\r\nrdev->mc_fw = NULL;\r\n}\r\nreturn err;\r\n}\r\nint cayman_get_allowed_info_register(struct radeon_device *rdev,\r\nu32 reg, u32 *val)\r\n{\r\nswitch (reg) {\r\ncase GRBM_STATUS:\r\ncase GRBM_STATUS_SE0:\r\ncase GRBM_STATUS_SE1:\r\ncase SRBM_STATUS:\r\ncase SRBM_STATUS2:\r\ncase (DMA_STATUS_REG + DMA0_REGISTER_OFFSET):\r\ncase (DMA_STATUS_REG + DMA1_REGISTER_OFFSET):\r\ncase UVD_STATUS:\r\n*val = RREG32(reg);\r\nreturn 0;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nint tn_get_temp(struct radeon_device *rdev)\r\n{\r\nu32 temp = RREG32_SMC(TN_CURRENT_GNB_TEMP) & 0x7ff;\r\nint actual_temp = (temp / 8) - 49;\r\nreturn actual_temp * 1000;\r\n}\r\nstatic void cayman_gpu_init(struct radeon_device *rdev)\r\n{\r\nu32 gb_addr_config = 0;\r\nu32 mc_shared_chmap, mc_arb_ramcfg;\r\nu32 cgts_tcc_disable;\r\nu32 sx_debug_1;\r\nu32 smx_dc_ctl0;\r\nu32 cgts_sm_ctrl_reg;\r\nu32 hdp_host_path_cntl;\r\nu32 tmp;\r\nu32 disabled_rb_mask;\r\nint i, j;\r\nswitch (rdev->family) {\r\ncase CHIP_CAYMAN:\r\nrdev->config.cayman.max_shader_engines = 2;\r\nrdev->config.cayman.max_pipes_per_simd = 4;\r\nrdev->config.cayman.max_tile_pipes = 8;\r\nrdev->config.cayman.max_simds_per_se = 12;\r\nrdev->config.cayman.max_backends_per_se = 4;\r\nrdev->config.cayman.max_texture_channel_caches = 8;\r\nrdev->config.cayman.max_gprs = 256;\r\nrdev->config.cayman.max_threads = 256;\r\nrdev->config.cayman.max_gs_threads = 32;\r\nrdev->config.cayman.max_stack_entries = 512;\r\nrdev->config.cayman.sx_num_of_sets = 8;\r\nrdev->config.cayman.sx_max_export_size = 256;\r\nrdev->config.cayman.sx_max_export_pos_size = 64;\r\nrdev->config.cayman.sx_max_export_smx_size = 192;\r\nrdev->config.cayman.max_hw_contexts = 8;\r\nrdev->config.cayman.sq_num_cf_insts = 2;\r\nrdev->config.cayman.sc_prim_fifo_size = 0x100;\r\nrdev->config.cayman.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.cayman.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = CAYMAN_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\ncase CHIP_ARUBA:\r\ndefault:\r\nrdev->config.cayman.max_shader_engines = 1;\r\nrdev->config.cayman.max_pipes_per_simd = 4;\r\nrdev->config.cayman.max_tile_pipes = 2;\r\nif ((rdev->pdev->device == 0x9900) ||\r\n(rdev->pdev->device == 0x9901) ||\r\n(rdev->pdev->device == 0x9905) ||\r\n(rdev->pdev->device == 0x9906) ||\r\n(rdev->pdev->device == 0x9907) ||\r\n(rdev->pdev->device == 0x9908) ||\r\n(rdev->pdev->device == 0x9909) ||\r\n(rdev->pdev->device == 0x990B) ||\r\n(rdev->pdev->device == 0x990C) ||\r\n(rdev->pdev->device == 0x990F) ||\r\n(rdev->pdev->device == 0x9910) ||\r\n(rdev->pdev->device == 0x9917) ||\r\n(rdev->pdev->device == 0x9999) ||\r\n(rdev->pdev->device == 0x999C)) {\r\nrdev->config.cayman.max_simds_per_se = 6;\r\nrdev->config.cayman.max_backends_per_se = 2;\r\nrdev->config.cayman.max_hw_contexts = 8;\r\nrdev->config.cayman.sx_max_export_size = 256;\r\nrdev->config.cayman.sx_max_export_pos_size = 64;\r\nrdev->config.cayman.sx_max_export_smx_size = 192;\r\n} else if ((rdev->pdev->device == 0x9903) ||\r\n(rdev->pdev->device == 0x9904) ||\r\n(rdev->pdev->device == 0x990A) ||\r\n(rdev->pdev->device == 0x990D) ||\r\n(rdev->pdev->device == 0x990E) ||\r\n(rdev->pdev->device == 0x9913) ||\r\n(rdev->pdev->device == 0x9918) ||\r\n(rdev->pdev->device == 0x999D)) {\r\nrdev->config.cayman.max_simds_per_se = 4;\r\nrdev->config.cayman.max_backends_per_se = 2;\r\nrdev->config.cayman.max_hw_contexts = 8;\r\nrdev->config.cayman.sx_max_export_size = 256;\r\nrdev->config.cayman.sx_max_export_pos_size = 64;\r\nrdev->config.cayman.sx_max_export_smx_size = 192;\r\n} else if ((rdev->pdev->device == 0x9919) ||\r\n(rdev->pdev->device == 0x9990) ||\r\n(rdev->pdev->device == 0x9991) ||\r\n(rdev->pdev->device == 0x9994) ||\r\n(rdev->pdev->device == 0x9995) ||\r\n(rdev->pdev->device == 0x9996) ||\r\n(rdev->pdev->device == 0x999A) ||\r\n(rdev->pdev->device == 0x99A0)) {\r\nrdev->config.cayman.max_simds_per_se = 3;\r\nrdev->config.cayman.max_backends_per_se = 1;\r\nrdev->config.cayman.max_hw_contexts = 4;\r\nrdev->config.cayman.sx_max_export_size = 128;\r\nrdev->config.cayman.sx_max_export_pos_size = 32;\r\nrdev->config.cayman.sx_max_export_smx_size = 96;\r\n} else {\r\nrdev->config.cayman.max_simds_per_se = 2;\r\nrdev->config.cayman.max_backends_per_se = 1;\r\nrdev->config.cayman.max_hw_contexts = 4;\r\nrdev->config.cayman.sx_max_export_size = 128;\r\nrdev->config.cayman.sx_max_export_pos_size = 32;\r\nrdev->config.cayman.sx_max_export_smx_size = 96;\r\n}\r\nrdev->config.cayman.max_texture_channel_caches = 2;\r\nrdev->config.cayman.max_gprs = 256;\r\nrdev->config.cayman.max_threads = 256;\r\nrdev->config.cayman.max_gs_threads = 32;\r\nrdev->config.cayman.max_stack_entries = 512;\r\nrdev->config.cayman.sx_num_of_sets = 8;\r\nrdev->config.cayman.sq_num_cf_insts = 2;\r\nrdev->config.cayman.sc_prim_fifo_size = 0x40;\r\nrdev->config.cayman.sc_hiz_tile_fifo_size = 0x30;\r\nrdev->config.cayman.sc_earlyz_tile_fifo_size = 0x130;\r\ngb_addr_config = ARUBA_GB_ADDR_CONFIG_GOLDEN;\r\nbreak;\r\n}\r\nfor (i = 0, j = 0; i < 32; i++, j += 0x18) {\r\nWREG32((0x2c14 + j), 0x00000000);\r\nWREG32((0x2c18 + j), 0x00000000);\r\nWREG32((0x2c1c + j), 0x00000000);\r\nWREG32((0x2c20 + j), 0x00000000);\r\nWREG32((0x2c24 + j), 0x00000000);\r\n}\r\nWREG32(GRBM_CNTL, GRBM_READ_TIMEOUT(0xff));\r\nWREG32(SRBM_INT_CNTL, 0x1);\r\nWREG32(SRBM_INT_ACK, 0x1);\r\nevergreen_fix_pci_max_read_req_size(rdev);\r\nmc_shared_chmap = RREG32(MC_SHARED_CHMAP);\r\nmc_arb_ramcfg = RREG32(MC_ARB_RAMCFG);\r\ntmp = (mc_arb_ramcfg & NOOFCOLS_MASK) >> NOOFCOLS_SHIFT;\r\nrdev->config.cayman.mem_row_size_in_kb = (4 * (1 << (8 + tmp))) / 1024;\r\nif (rdev->config.cayman.mem_row_size_in_kb > 4)\r\nrdev->config.cayman.mem_row_size_in_kb = 4;\r\nrdev->config.cayman.shader_engine_tile_size = 32;\r\nrdev->config.cayman.num_gpus = 1;\r\nrdev->config.cayman.multi_gpu_tile_size = 64;\r\ntmp = (gb_addr_config & NUM_PIPES_MASK) >> NUM_PIPES_SHIFT;\r\nrdev->config.cayman.num_tile_pipes = (1 << tmp);\r\ntmp = (gb_addr_config & PIPE_INTERLEAVE_SIZE_MASK) >> PIPE_INTERLEAVE_SIZE_SHIFT;\r\nrdev->config.cayman.mem_max_burst_length_bytes = (tmp + 1) * 256;\r\ntmp = (gb_addr_config & NUM_SHADER_ENGINES_MASK) >> NUM_SHADER_ENGINES_SHIFT;\r\nrdev->config.cayman.num_shader_engines = tmp + 1;\r\ntmp = (gb_addr_config & NUM_GPUS_MASK) >> NUM_GPUS_SHIFT;\r\nrdev->config.cayman.num_gpus = tmp + 1;\r\ntmp = (gb_addr_config & MULTI_GPU_TILE_SIZE_MASK) >> MULTI_GPU_TILE_SIZE_SHIFT;\r\nrdev->config.cayman.multi_gpu_tile_size = 1 << tmp;\r\ntmp = (gb_addr_config & ROW_SIZE_MASK) >> ROW_SIZE_SHIFT;\r\nrdev->config.cayman.mem_row_size_in_kb = 1 << tmp;\r\nrdev->config.cayman.tile_config = 0;\r\nswitch (rdev->config.cayman.num_tile_pipes) {\r\ncase 1:\r\ndefault:\r\nrdev->config.cayman.tile_config |= (0 << 0);\r\nbreak;\r\ncase 2:\r\nrdev->config.cayman.tile_config |= (1 << 0);\r\nbreak;\r\ncase 4:\r\nrdev->config.cayman.tile_config |= (2 << 0);\r\nbreak;\r\ncase 8:\r\nrdev->config.cayman.tile_config |= (3 << 0);\r\nbreak;\r\n}\r\nif (rdev->flags & RADEON_IS_IGP)\r\nrdev->config.cayman.tile_config |= 1 << 4;\r\nelse {\r\nswitch ((mc_arb_ramcfg & NOOFBANK_MASK) >> NOOFBANK_SHIFT) {\r\ncase 0:\r\nrdev->config.cayman.tile_config |= 0 << 4;\r\nbreak;\r\ncase 1:\r\nrdev->config.cayman.tile_config |= 1 << 4;\r\nbreak;\r\ncase 2:\r\ndefault:\r\nrdev->config.cayman.tile_config |= 2 << 4;\r\nbreak;\r\n}\r\n}\r\nrdev->config.cayman.tile_config |=\r\n((gb_addr_config & PIPE_INTERLEAVE_SIZE_MASK) >> PIPE_INTERLEAVE_SIZE_SHIFT) << 8;\r\nrdev->config.cayman.tile_config |=\r\n((gb_addr_config & ROW_SIZE_MASK) >> ROW_SIZE_SHIFT) << 12;\r\ntmp = 0;\r\nfor (i = (rdev->config.cayman.max_shader_engines - 1); i >= 0; i--) {\r\nu32 rb_disable_bitmap;\r\nWREG32(GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES | SE_INDEX(i));\r\nWREG32(RLC_GFX_INDEX, INSTANCE_BROADCAST_WRITES | SE_INDEX(i));\r\nrb_disable_bitmap = (RREG32(CC_RB_BACKEND_DISABLE) & 0x00ff0000) >> 16;\r\ntmp <<= 4;\r\ntmp |= rb_disable_bitmap;\r\n}\r\ndisabled_rb_mask = tmp;\r\ntmp = 0;\r\nfor (i = 0; i < (rdev->config.cayman.max_backends_per_se * rdev->config.cayman.max_shader_engines); i++)\r\ntmp |= (1 << i);\r\nif ((disabled_rb_mask & tmp) == tmp) {\r\nfor (i = 0; i < (rdev->config.cayman.max_backends_per_se * rdev->config.cayman.max_shader_engines); i++)\r\ndisabled_rb_mask &= ~(1 << i);\r\n}\r\nfor (i = 0; i < rdev->config.cayman.max_shader_engines; i++) {\r\nu32 simd_disable_bitmap;\r\nWREG32(GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES | SE_INDEX(i));\r\nWREG32(RLC_GFX_INDEX, INSTANCE_BROADCAST_WRITES | SE_INDEX(i));\r\nsimd_disable_bitmap = (RREG32(CC_GC_SHADER_PIPE_CONFIG) & 0xffff0000) >> 16;\r\nsimd_disable_bitmap |= 0xffffffff << rdev->config.cayman.max_simds_per_se;\r\ntmp <<= 16;\r\ntmp |= simd_disable_bitmap;\r\n}\r\nrdev->config.cayman.active_simds = hweight32(~tmp);\r\nWREG32(GRBM_GFX_INDEX, INSTANCE_BROADCAST_WRITES | SE_BROADCAST_WRITES);\r\nWREG32(RLC_GFX_INDEX, INSTANCE_BROADCAST_WRITES | SE_BROADCAST_WRITES);\r\nWREG32(GB_ADDR_CONFIG, gb_addr_config);\r\nWREG32(DMIF_ADDR_CONFIG, gb_addr_config);\r\nif (ASIC_IS_DCE6(rdev))\r\nWREG32(DMIF_ADDR_CALC, gb_addr_config);\r\nWREG32(HDP_ADDR_CONFIG, gb_addr_config);\r\nWREG32(DMA_TILING_CONFIG + DMA0_REGISTER_OFFSET, gb_addr_config);\r\nWREG32(DMA_TILING_CONFIG + DMA1_REGISTER_OFFSET, gb_addr_config);\r\nWREG32(UVD_UDEC_ADDR_CONFIG, gb_addr_config);\r\nWREG32(UVD_UDEC_DB_ADDR_CONFIG, gb_addr_config);\r\nWREG32(UVD_UDEC_DBW_ADDR_CONFIG, gb_addr_config);\r\nif ((rdev->config.cayman.max_backends_per_se == 1) &&\r\n(rdev->flags & RADEON_IS_IGP)) {\r\nif ((disabled_rb_mask & 3) == 2) {\r\ntmp = 0x00000000;\r\n} else {\r\ntmp = 0x11111111;\r\n}\r\n} else {\r\ntmp = gb_addr_config & NUM_PIPES_MASK;\r\ntmp = r6xx_remap_render_backend(rdev, tmp,\r\nrdev->config.cayman.max_backends_per_se *\r\nrdev->config.cayman.max_shader_engines,\r\nCAYMAN_MAX_BACKENDS, disabled_rb_mask);\r\n}\r\nWREG32(GB_BACKEND_MAP, tmp);\r\ncgts_tcc_disable = 0xffff0000;\r\nfor (i = 0; i < rdev->config.cayman.max_texture_channel_caches; i++)\r\ncgts_tcc_disable &= ~(1 << (16 + i));\r\nWREG32(CGTS_TCC_DISABLE, cgts_tcc_disable);\r\nWREG32(CGTS_SYS_TCC_DISABLE, cgts_tcc_disable);\r\nWREG32(CGTS_USER_SYS_TCC_DISABLE, cgts_tcc_disable);\r\nWREG32(CGTS_USER_TCC_DISABLE, cgts_tcc_disable);\r\ncgts_sm_ctrl_reg = RREG32(CGTS_SM_CTRL_REG);\r\nfor (i = 0; i < 16; i++)\r\nWREG32(CGTS_SM_CTRL_REG, OVERRIDE);\r\nWREG32(CGTS_SM_CTRL_REG, cgts_sm_ctrl_reg);\r\nWREG32(CP_MEQ_THRESHOLDS, MEQ1_START(0x30) | MEQ2_START(0x60));\r\nsx_debug_1 = RREG32(SX_DEBUG_1);\r\nsx_debug_1 |= ENABLE_NEW_SMX_ADDRESS;\r\nWREG32(SX_DEBUG_1, sx_debug_1);\r\nsmx_dc_ctl0 = RREG32(SMX_DC_CTL0);\r\nsmx_dc_ctl0 &= ~NUMBER_OF_SETS(0x1ff);\r\nsmx_dc_ctl0 |= NUMBER_OF_SETS(rdev->config.cayman.sx_num_of_sets);\r\nWREG32(SMX_DC_CTL0, smx_dc_ctl0);\r\nWREG32(SPI_CONFIG_CNTL_1, VTX_DONE_DELAY(4) | CRC_SIMD_ID_WADDR_DISABLE);\r\nWREG32(VGT_OFFCHIP_LDS_BASE, 0);\r\nWREG32(SQ_LSTMP_RING_BASE, 0);\r\nWREG32(SQ_HSTMP_RING_BASE, 0);\r\nWREG32(SQ_ESTMP_RING_BASE, 0);\r\nWREG32(SQ_GSTMP_RING_BASE, 0);\r\nWREG32(SQ_VSTMP_RING_BASE, 0);\r\nWREG32(SQ_PSTMP_RING_BASE, 0);\r\nWREG32(TA_CNTL_AUX, DISABLE_CUBE_ANISO);\r\nWREG32(SX_EXPORT_BUFFER_SIZES, (COLOR_BUFFER_SIZE((rdev->config.cayman.sx_max_export_size / 4) - 1) |\r\nPOSITION_BUFFER_SIZE((rdev->config.cayman.sx_max_export_pos_size / 4) - 1) |\r\nSMX_BUFFER_SIZE((rdev->config.cayman.sx_max_export_smx_size / 4) - 1)));\r\nWREG32(PA_SC_FIFO_SIZE, (SC_PRIM_FIFO_SIZE(rdev->config.cayman.sc_prim_fifo_size) |\r\nSC_HIZ_TILE_FIFO_SIZE(rdev->config.cayman.sc_hiz_tile_fifo_size) |\r\nSC_EARLYZ_TILE_FIFO_SIZE(rdev->config.cayman.sc_earlyz_tile_fifo_size)));\r\nWREG32(VGT_NUM_INSTANCES, 1);\r\nWREG32(CP_PERFMON_CNTL, 0);\r\nWREG32(SQ_MS_FIFO_SIZES, (CACHE_FIFO_SIZE(16 * rdev->config.cayman.sq_num_cf_insts) |\r\nFETCH_FIFO_HIWATER(0x4) |\r\nDONE_FIFO_HIWATER(0xe0) |\r\nALU_UPDATE_FIFO_HIWATER(0x8)));\r\nWREG32(SQ_GPR_RESOURCE_MGMT_1, NUM_CLAUSE_TEMP_GPRS(4));\r\nWREG32(SQ_CONFIG, (VC_ENABLE |\r\nEXPORT_SRC_C |\r\nGFX_PRIO(0) |\r\nCS1_PRIO(0) |\r\nCS2_PRIO(1)));\r\nWREG32(SQ_DYN_GPR_CNTL_PS_FLUSH_REQ, DYN_GPR_ENABLE);\r\nWREG32(PA_SC_FORCE_EOV_MAX_CNTS, (FORCE_EOV_MAX_CLK_CNT(4095) |\r\nFORCE_EOV_MAX_REZ_CNT(255)));\r\nWREG32(VGT_CACHE_INVALIDATION, CACHE_INVALIDATION(VC_AND_TC) |\r\nAUTO_INVLD_EN(ES_AND_GS_AUTO));\r\nWREG32(VGT_GS_VERTEX_REUSE, 16);\r\nWREG32(PA_SC_LINE_STIPPLE_STATE, 0);\r\nWREG32(CB_PERF_CTR0_SEL_0, 0);\r\nWREG32(CB_PERF_CTR0_SEL_1, 0);\r\nWREG32(CB_PERF_CTR1_SEL_0, 0);\r\nWREG32(CB_PERF_CTR1_SEL_1, 0);\r\nWREG32(CB_PERF_CTR2_SEL_0, 0);\r\nWREG32(CB_PERF_CTR2_SEL_1, 0);\r\nWREG32(CB_PERF_CTR3_SEL_0, 0);\r\nWREG32(CB_PERF_CTR3_SEL_1, 0);\r\ntmp = RREG32(HDP_MISC_CNTL);\r\ntmp |= HDP_FLUSH_INVALIDATE_CACHE;\r\nWREG32(HDP_MISC_CNTL, tmp);\r\nhdp_host_path_cntl = RREG32(HDP_HOST_PATH_CNTL);\r\nWREG32(HDP_HOST_PATH_CNTL, hdp_host_path_cntl);\r\nWREG32(PA_CL_ENHANCE, CLIP_VTX_REORDER_ENA | NUM_CLIP_SEQ(3));\r\nudelay(50);\r\nif (rdev->family == CHIP_ARUBA) {\r\ntmp = RREG32_CG(CG_CGTT_LOCAL_0);\r\ntmp &= ~0x00380000;\r\nWREG32_CG(CG_CGTT_LOCAL_0, tmp);\r\ntmp = RREG32_CG(CG_CGTT_LOCAL_1);\r\ntmp &= ~0x0e000000;\r\nWREG32_CG(CG_CGTT_LOCAL_1, tmp);\r\n}\r\n}\r\nvoid cayman_pcie_gart_tlb_flush(struct radeon_device *rdev)\r\n{\r\nWREG32(HDP_MEM_COHERENCY_FLUSH_CNTL, 0x1);\r\nWREG32(VM_INVALIDATE_REQUEST, 1);\r\n}\r\nstatic int cayman_pcie_gart_enable(struct radeon_device *rdev)\r\n{\r\nint i, r;\r\nif (rdev->gart.robj == NULL) {\r\ndev_err(rdev->dev, "No VRAM object for PCIE GART.\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_gart_table_vram_pin(rdev);\r\nif (r)\r\nreturn r;\r\nWREG32(MC_VM_MX_L1_TLB_CNTL,\r\n(0xA << 7) |\r\nENABLE_L1_TLB |\r\nENABLE_L1_FRAGMENT_PROCESSING |\r\nSYSTEM_ACCESS_MODE_NOT_IN_SYS |\r\nENABLE_ADVANCED_DRIVER_MODEL |\r\nSYSTEM_APERTURE_UNMAPPED_ACCESS_PASS_THRU);\r\nWREG32(VM_L2_CNTL, ENABLE_L2_CACHE |\r\nENABLE_L2_FRAGMENT_PROCESSING |\r\nENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE |\r\nENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE |\r\nEFFECTIVE_L2_QUEUE_SIZE(7) |\r\nCONTEXT1_IDENTITY_ACCESS_MODE(1));\r\nWREG32(VM_L2_CNTL2, INVALIDATE_ALL_L1_TLBS | INVALIDATE_L2_CACHE);\r\nWREG32(VM_L2_CNTL3, L2_CACHE_BIGK_ASSOCIATIVITY |\r\nBANK_SELECT(6) |\r\nL2_CACHE_BIGK_FRAGMENT_SIZE(6));\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_START_ADDR, rdev->mc.gtt_start >> 12);\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_END_ADDR, rdev->mc.gtt_end >> 12);\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_BASE_ADDR, rdev->gart.table_addr >> 12);\r\nWREG32(VM_CONTEXT0_PROTECTION_FAULT_DEFAULT_ADDR,\r\n(u32)(rdev->dummy_page.addr >> 12));\r\nWREG32(VM_CONTEXT0_CNTL2, 0);\r\nWREG32(VM_CONTEXT0_CNTL, ENABLE_CONTEXT | PAGE_TABLE_DEPTH(0) |\r\nRANGE_PROTECTION_FAULT_ENABLE_DEFAULT);\r\nWREG32(0x15D4, 0);\r\nWREG32(0x15D8, 0);\r\nWREG32(0x15DC, 0);\r\nfor (i = 1; i < 8; i++) {\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_START_ADDR + (i << 2), 0);\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_END_ADDR + (i << 2),\r\nrdev->vm_manager.max_pfn - 1);\r\nWREG32(VM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (i << 2),\r\nrdev->vm_manager.saved_table_addr[i]);\r\n}\r\nWREG32(VM_CONTEXT1_PROTECTION_FAULT_DEFAULT_ADDR,\r\n(u32)(rdev->dummy_page.addr >> 12));\r\nWREG32(VM_CONTEXT1_CNTL2, 4);\r\nWREG32(VM_CONTEXT1_CNTL, ENABLE_CONTEXT | PAGE_TABLE_DEPTH(1) |\r\nPAGE_TABLE_BLOCK_SIZE(radeon_vm_block_size - 9) |\r\nRANGE_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nRANGE_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nDUMMY_PAGE_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nDUMMY_PAGE_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nPDE0_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nPDE0_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nVALID_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nVALID_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nREAD_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nREAD_PROTECTION_FAULT_ENABLE_DEFAULT |\r\nWRITE_PROTECTION_FAULT_ENABLE_INTERRUPT |\r\nWRITE_PROTECTION_FAULT_ENABLE_DEFAULT);\r\ncayman_pcie_gart_tlb_flush(rdev);\r\nDRM_INFO("PCIE GART of %uM enabled (table at 0x%016llX).\n",\r\n(unsigned)(rdev->mc.gtt_size >> 20),\r\n(unsigned long long)rdev->gart.table_addr);\r\nrdev->gart.ready = true;\r\nreturn 0;\r\n}\r\nstatic void cayman_pcie_gart_disable(struct radeon_device *rdev)\r\n{\r\nunsigned i;\r\nfor (i = 1; i < 8; ++i) {\r\nrdev->vm_manager.saved_table_addr[i] = RREG32(\r\nVM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (i << 2));\r\n}\r\nWREG32(VM_CONTEXT0_CNTL, 0);\r\nWREG32(VM_CONTEXT1_CNTL, 0);\r\nWREG32(MC_VM_MX_L1_TLB_CNTL, ENABLE_L1_FRAGMENT_PROCESSING |\r\nSYSTEM_ACCESS_MODE_NOT_IN_SYS |\r\nSYSTEM_APERTURE_UNMAPPED_ACCESS_PASS_THRU);\r\nWREG32(VM_L2_CNTL, ENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE |\r\nENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE |\r\nEFFECTIVE_L2_QUEUE_SIZE(7) |\r\nCONTEXT1_IDENTITY_ACCESS_MODE(1));\r\nWREG32(VM_L2_CNTL2, 0);\r\nWREG32(VM_L2_CNTL3, L2_CACHE_BIGK_ASSOCIATIVITY |\r\nL2_CACHE_BIGK_FRAGMENT_SIZE(6));\r\nradeon_gart_table_vram_unpin(rdev);\r\n}\r\nstatic void cayman_pcie_gart_fini(struct radeon_device *rdev)\r\n{\r\ncayman_pcie_gart_disable(rdev);\r\nradeon_gart_table_vram_free(rdev);\r\nradeon_gart_fini(rdev);\r\n}\r\nvoid cayman_cp_int_cntl_setup(struct radeon_device *rdev,\r\nint ring, u32 cp_int_cntl)\r\n{\r\nWREG32(SRBM_GFX_CNTL, RINGID(ring));\r\nWREG32(CP_INT_CNTL, cp_int_cntl);\r\n}\r\nvoid cayman_fence_ring_emit(struct radeon_device *rdev,\r\nstruct radeon_fence *fence)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[fence->ring];\r\nu64 addr = rdev->fence_drv[fence->ring].gpu_addr;\r\nu32 cp_coher_cntl = PACKET3_FULL_CACHE_ENA | PACKET3_TC_ACTION_ENA |\r\nPACKET3_SH_ACTION_ENA;\r\nradeon_ring_write(ring, PACKET3(PACKET3_SURFACE_SYNC, 3));\r\nradeon_ring_write(ring, PACKET3_ENGINE_ME | cp_coher_cntl);\r\nradeon_ring_write(ring, 0xFFFFFFFF);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 10);\r\nradeon_ring_write(ring, PACKET3(PACKET3_EVENT_WRITE_EOP, 4));\r\nradeon_ring_write(ring, EVENT_TYPE(CACHE_FLUSH_AND_INV_EVENT_TS) | EVENT_INDEX(5));\r\nradeon_ring_write(ring, lower_32_bits(addr));\r\nradeon_ring_write(ring, (upper_32_bits(addr) & 0xff) | DATA_SEL(1) | INT_SEL(2));\r\nradeon_ring_write(ring, fence->seq);\r\nradeon_ring_write(ring, 0);\r\n}\r\nvoid cayman_ring_ib_execute(struct radeon_device *rdev, struct radeon_ib *ib)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[ib->ring];\r\nunsigned vm_id = ib->vm ? ib->vm->ids[ib->ring].id : 0;\r\nu32 cp_coher_cntl = PACKET3_FULL_CACHE_ENA | PACKET3_TC_ACTION_ENA |\r\nPACKET3_SH_ACTION_ENA;\r\nradeon_ring_write(ring, PACKET3(PACKET3_MODE_CONTROL, 0));\r\nradeon_ring_write(ring, 1);\r\nif (ring->rptr_save_reg) {\r\nuint32_t next_rptr = ring->wptr + 3 + 4 + 8;\r\nradeon_ring_write(ring, PACKET3(PACKET3_SET_CONFIG_REG, 1));\r\nradeon_ring_write(ring, ((ring->rptr_save_reg -\r\nPACKET3_SET_CONFIG_REG_START) >> 2));\r\nradeon_ring_write(ring, next_rptr);\r\n}\r\nradeon_ring_write(ring, PACKET3(PACKET3_INDIRECT_BUFFER, 2));\r\nradeon_ring_write(ring,\r\n#ifdef __BIG_ENDIAN\r\n(2 << 0) |\r\n#endif\r\n(ib->gpu_addr & 0xFFFFFFFC));\r\nradeon_ring_write(ring, upper_32_bits(ib->gpu_addr) & 0xFF);\r\nradeon_ring_write(ring, ib->length_dw | (vm_id << 24));\r\nradeon_ring_write(ring, PACKET3(PACKET3_SURFACE_SYNC, 3));\r\nradeon_ring_write(ring, PACKET3_ENGINE_ME | cp_coher_cntl);\r\nradeon_ring_write(ring, 0xFFFFFFFF);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, (vm_id << 24) | 10);\r\n}\r\nstatic void cayman_cp_enable(struct radeon_device *rdev, bool enable)\r\n{\r\nif (enable)\r\nWREG32(CP_ME_CNTL, 0);\r\nelse {\r\nif (rdev->asic->copy.copy_ring_index == RADEON_RING_TYPE_GFX_INDEX)\r\nradeon_ttm_set_active_vram_size(rdev, rdev->mc.visible_vram_size);\r\nWREG32(CP_ME_CNTL, (CP_ME_HALT | CP_PFP_HALT));\r\nWREG32(SCRATCH_UMSK, 0);\r\nrdev->ring[RADEON_RING_TYPE_GFX_INDEX].ready = false;\r\n}\r\n}\r\nu32 cayman_gfx_get_rptr(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nu32 rptr;\r\nif (rdev->wb.enabled)\r\nrptr = rdev->wb.wb[ring->rptr_offs/4];\r\nelse {\r\nif (ring->idx == RADEON_RING_TYPE_GFX_INDEX)\r\nrptr = RREG32(CP_RB0_RPTR);\r\nelse if (ring->idx == CAYMAN_RING_TYPE_CP1_INDEX)\r\nrptr = RREG32(CP_RB1_RPTR);\r\nelse\r\nrptr = RREG32(CP_RB2_RPTR);\r\n}\r\nreturn rptr;\r\n}\r\nu32 cayman_gfx_get_wptr(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nu32 wptr;\r\nif (ring->idx == RADEON_RING_TYPE_GFX_INDEX)\r\nwptr = RREG32(CP_RB0_WPTR);\r\nelse if (ring->idx == CAYMAN_RING_TYPE_CP1_INDEX)\r\nwptr = RREG32(CP_RB1_WPTR);\r\nelse\r\nwptr = RREG32(CP_RB2_WPTR);\r\nreturn wptr;\r\n}\r\nvoid cayman_gfx_set_wptr(struct radeon_device *rdev,\r\nstruct radeon_ring *ring)\r\n{\r\nif (ring->idx == RADEON_RING_TYPE_GFX_INDEX) {\r\nWREG32(CP_RB0_WPTR, ring->wptr);\r\n(void)RREG32(CP_RB0_WPTR);\r\n} else if (ring->idx == CAYMAN_RING_TYPE_CP1_INDEX) {\r\nWREG32(CP_RB1_WPTR, ring->wptr);\r\n(void)RREG32(CP_RB1_WPTR);\r\n} else {\r\nWREG32(CP_RB2_WPTR, ring->wptr);\r\n(void)RREG32(CP_RB2_WPTR);\r\n}\r\n}\r\nstatic int cayman_cp_load_microcode(struct radeon_device *rdev)\r\n{\r\nconst __be32 *fw_data;\r\nint i;\r\nif (!rdev->me_fw || !rdev->pfp_fw)\r\nreturn -EINVAL;\r\ncayman_cp_enable(rdev, false);\r\nfw_data = (const __be32 *)rdev->pfp_fw->data;\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nfor (i = 0; i < CAYMAN_PFP_UCODE_SIZE; i++)\r\nWREG32(CP_PFP_UCODE_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nfw_data = (const __be32 *)rdev->me_fw->data;\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\nfor (i = 0; i < CAYMAN_PM4_UCODE_SIZE; i++)\r\nWREG32(CP_ME_RAM_DATA, be32_to_cpup(fw_data++));\r\nWREG32(CP_PFP_UCODE_ADDR, 0);\r\nWREG32(CP_ME_RAM_WADDR, 0);\r\nWREG32(CP_ME_RAM_RADDR, 0);\r\nreturn 0;\r\n}\r\nstatic int cayman_cp_start(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nint r, i;\r\nr = radeon_ring_lock(rdev, ring, 7);\r\nif (r) {\r\nDRM_ERROR("radeon: cp failed to lock ring (%d).\n", r);\r\nreturn r;\r\n}\r\nradeon_ring_write(ring, PACKET3(PACKET3_ME_INITIALIZE, 5));\r\nradeon_ring_write(ring, 0x1);\r\nradeon_ring_write(ring, 0x0);\r\nradeon_ring_write(ring, rdev->config.cayman.max_hw_contexts - 1);\r\nradeon_ring_write(ring, PACKET3_ME_INITIALIZE_DEVICE_ID(1));\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_unlock_commit(rdev, ring, false);\r\ncayman_cp_enable(rdev, true);\r\nr = radeon_ring_lock(rdev, ring, cayman_default_size + 19);\r\nif (r) {\r\nDRM_ERROR("radeon: cp failed to lock ring (%d).\n", r);\r\nreturn r;\r\n}\r\nradeon_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nradeon_ring_write(ring, PACKET3_PREAMBLE_BEGIN_CLEAR_STATE);\r\nfor (i = 0; i < cayman_default_size; i++)\r\nradeon_ring_write(ring, cayman_default_state[i]);\r\nradeon_ring_write(ring, PACKET3(PACKET3_PREAMBLE_CNTL, 0));\r\nradeon_ring_write(ring, PACKET3_PREAMBLE_END_CLEAR_STATE);\r\nradeon_ring_write(ring, PACKET3(PACKET3_CLEAR_STATE, 0));\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0xc0026f00);\r\nradeon_ring_write(ring, 0x00000000);\r\nradeon_ring_write(ring, 0x00000000);\r\nradeon_ring_write(ring, 0x00000000);\r\nradeon_ring_write(ring, 0xc0036f00);\r\nradeon_ring_write(ring, 0x00000bc4);\r\nradeon_ring_write(ring, 0xffffffff);\r\nradeon_ring_write(ring, 0xffffffff);\r\nradeon_ring_write(ring, 0xffffffff);\r\nradeon_ring_write(ring, 0xc0026900);\r\nradeon_ring_write(ring, 0x00000316);\r\nradeon_ring_write(ring, 0x0000000e);\r\nradeon_ring_write(ring, 0x00000010);\r\nradeon_ring_unlock_commit(rdev, ring, false);\r\nreturn 0;\r\n}\r\nstatic void cayman_cp_fini(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\ncayman_cp_enable(rdev, false);\r\nradeon_ring_fini(rdev, ring);\r\nradeon_scratch_free(rdev, ring->rptr_save_reg);\r\n}\r\nstatic int cayman_cp_resume(struct radeon_device *rdev)\r\n{\r\nstatic const int ridx[] = {\r\nRADEON_RING_TYPE_GFX_INDEX,\r\nCAYMAN_RING_TYPE_CP1_INDEX,\r\nCAYMAN_RING_TYPE_CP2_INDEX\r\n};\r\nstatic const unsigned cp_rb_cntl[] = {\r\nCP_RB0_CNTL,\r\nCP_RB1_CNTL,\r\nCP_RB2_CNTL,\r\n};\r\nstatic const unsigned cp_rb_rptr_addr[] = {\r\nCP_RB0_RPTR_ADDR,\r\nCP_RB1_RPTR_ADDR,\r\nCP_RB2_RPTR_ADDR\r\n};\r\nstatic const unsigned cp_rb_rptr_addr_hi[] = {\r\nCP_RB0_RPTR_ADDR_HI,\r\nCP_RB1_RPTR_ADDR_HI,\r\nCP_RB2_RPTR_ADDR_HI\r\n};\r\nstatic const unsigned cp_rb_base[] = {\r\nCP_RB0_BASE,\r\nCP_RB1_BASE,\r\nCP_RB2_BASE\r\n};\r\nstatic const unsigned cp_rb_rptr[] = {\r\nCP_RB0_RPTR,\r\nCP_RB1_RPTR,\r\nCP_RB2_RPTR\r\n};\r\nstatic const unsigned cp_rb_wptr[] = {\r\nCP_RB0_WPTR,\r\nCP_RB1_WPTR,\r\nCP_RB2_WPTR\r\n};\r\nstruct radeon_ring *ring;\r\nint i, r;\r\nWREG32(GRBM_SOFT_RESET, (SOFT_RESET_CP |\r\nSOFT_RESET_PA |\r\nSOFT_RESET_SH |\r\nSOFT_RESET_VGT |\r\nSOFT_RESET_SPI |\r\nSOFT_RESET_SX));\r\nRREG32(GRBM_SOFT_RESET);\r\nmdelay(15);\r\nWREG32(GRBM_SOFT_RESET, 0);\r\nRREG32(GRBM_SOFT_RESET);\r\nWREG32(CP_SEM_WAIT_TIMER, 0x0);\r\nWREG32(CP_SEM_INCOMPLETE_TIMER_CNTL, 0x0);\r\nWREG32(CP_RB_WPTR_DELAY, 0);\r\nWREG32(CP_DEBUG, (1 << 27));\r\nWREG32(SCRATCH_ADDR, ((rdev->wb.gpu_addr + RADEON_WB_SCRATCH_OFFSET) >> 8) & 0xFFFFFFFF);\r\nWREG32(SCRATCH_UMSK, 0xff);\r\nfor (i = 0; i < 3; ++i) {\r\nuint32_t rb_cntl;\r\nuint64_t addr;\r\nring = &rdev->ring[ridx[i]];\r\nrb_cntl = order_base_2(ring->ring_size / 8);\r\nrb_cntl |= order_base_2(RADEON_GPU_PAGE_SIZE/8) << 8;\r\n#ifdef __BIG_ENDIAN\r\nrb_cntl |= BUF_SWAP_32BIT;\r\n#endif\r\nWREG32(cp_rb_cntl[i], rb_cntl);\r\naddr = rdev->wb.gpu_addr + RADEON_WB_CP_RPTR_OFFSET;\r\nWREG32(cp_rb_rptr_addr[i], addr & 0xFFFFFFFC);\r\nWREG32(cp_rb_rptr_addr_hi[i], upper_32_bits(addr) & 0xFF);\r\n}\r\nfor (i = 0; i < 3; ++i) {\r\nring = &rdev->ring[ridx[i]];\r\nWREG32(cp_rb_base[i], ring->gpu_addr >> 8);\r\n}\r\nfor (i = 0; i < 3; ++i) {\r\nring = &rdev->ring[ridx[i]];\r\nWREG32_P(cp_rb_cntl[i], RB_RPTR_WR_ENA, ~RB_RPTR_WR_ENA);\r\nring->wptr = 0;\r\nWREG32(cp_rb_rptr[i], 0);\r\nWREG32(cp_rb_wptr[i], ring->wptr);\r\nmdelay(1);\r\nWREG32_P(cp_rb_cntl[i], 0, ~RB_RPTR_WR_ENA);\r\n}\r\ncayman_cp_start(rdev);\r\nrdev->ring[RADEON_RING_TYPE_GFX_INDEX].ready = true;\r\nrdev->ring[CAYMAN_RING_TYPE_CP1_INDEX].ready = false;\r\nrdev->ring[CAYMAN_RING_TYPE_CP2_INDEX].ready = false;\r\nr = radeon_ring_test(rdev, RADEON_RING_TYPE_GFX_INDEX, &rdev->ring[RADEON_RING_TYPE_GFX_INDEX]);\r\nif (r) {\r\nrdev->ring[RADEON_RING_TYPE_GFX_INDEX].ready = false;\r\nrdev->ring[CAYMAN_RING_TYPE_CP1_INDEX].ready = false;\r\nrdev->ring[CAYMAN_RING_TYPE_CP2_INDEX].ready = false;\r\nreturn r;\r\n}\r\nif (rdev->asic->copy.copy_ring_index == RADEON_RING_TYPE_GFX_INDEX)\r\nradeon_ttm_set_active_vram_size(rdev, rdev->mc.real_vram_size);\r\nreturn 0;\r\n}\r\nu32 cayman_gpu_check_soft_reset(struct radeon_device *rdev)\r\n{\r\nu32 reset_mask = 0;\r\nu32 tmp;\r\ntmp = RREG32(GRBM_STATUS);\r\nif (tmp & (PA_BUSY | SC_BUSY |\r\nSH_BUSY | SX_BUSY |\r\nTA_BUSY | VGT_BUSY |\r\nDB_BUSY | CB_BUSY |\r\nGDS_BUSY | SPI_BUSY |\r\nIA_BUSY | IA_BUSY_NO_DMA))\r\nreset_mask |= RADEON_RESET_GFX;\r\nif (tmp & (CF_RQ_PENDING | PF_RQ_PENDING |\r\nCP_BUSY | CP_COHERENCY_BUSY))\r\nreset_mask |= RADEON_RESET_CP;\r\nif (tmp & GRBM_EE_BUSY)\r\nreset_mask |= RADEON_RESET_GRBM | RADEON_RESET_GFX | RADEON_RESET_CP;\r\ntmp = RREG32(DMA_STATUS_REG + DMA0_REGISTER_OFFSET);\r\nif (!(tmp & DMA_IDLE))\r\nreset_mask |= RADEON_RESET_DMA;\r\ntmp = RREG32(DMA_STATUS_REG + DMA1_REGISTER_OFFSET);\r\nif (!(tmp & DMA_IDLE))\r\nreset_mask |= RADEON_RESET_DMA1;\r\ntmp = RREG32(SRBM_STATUS2);\r\nif (tmp & DMA_BUSY)\r\nreset_mask |= RADEON_RESET_DMA;\r\nif (tmp & DMA1_BUSY)\r\nreset_mask |= RADEON_RESET_DMA1;\r\ntmp = RREG32(SRBM_STATUS);\r\nif (tmp & (RLC_RQ_PENDING | RLC_BUSY))\r\nreset_mask |= RADEON_RESET_RLC;\r\nif (tmp & IH_BUSY)\r\nreset_mask |= RADEON_RESET_IH;\r\nif (tmp & SEM_BUSY)\r\nreset_mask |= RADEON_RESET_SEM;\r\nif (tmp & GRBM_RQ_PENDING)\r\nreset_mask |= RADEON_RESET_GRBM;\r\nif (tmp & VMC_BUSY)\r\nreset_mask |= RADEON_RESET_VMC;\r\nif (tmp & (MCB_BUSY | MCB_NON_DISPLAY_BUSY |\r\nMCC_BUSY | MCD_BUSY))\r\nreset_mask |= RADEON_RESET_MC;\r\nif (evergreen_is_display_hung(rdev))\r\nreset_mask |= RADEON_RESET_DISPLAY;\r\ntmp = RREG32(VM_L2_STATUS);\r\nif (tmp & L2_BUSY)\r\nreset_mask |= RADEON_RESET_VMC;\r\nif (reset_mask & RADEON_RESET_MC) {\r\nDRM_DEBUG("MC busy: 0x%08X, clearing.\n", reset_mask);\r\nreset_mask &= ~RADEON_RESET_MC;\r\n}\r\nreturn reset_mask;\r\n}\r\nstatic void cayman_gpu_soft_reset(struct radeon_device *rdev, u32 reset_mask)\r\n{\r\nstruct evergreen_mc_save save;\r\nu32 grbm_soft_reset = 0, srbm_soft_reset = 0;\r\nu32 tmp;\r\nif (reset_mask == 0)\r\nreturn;\r\ndev_info(rdev->dev, "GPU softreset: 0x%08X\n", reset_mask);\r\nevergreen_print_gpu_status_regs(rdev);\r\ndev_info(rdev->dev, " VM_CONTEXT0_PROTECTION_FAULT_ADDR 0x%08X\n",\r\nRREG32(0x14F8));\r\ndev_info(rdev->dev, " VM_CONTEXT0_PROTECTION_FAULT_STATUS 0x%08X\n",\r\nRREG32(0x14D8));\r\ndev_info(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_ADDR 0x%08X\n",\r\nRREG32(0x14FC));\r\ndev_info(rdev->dev, " VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",\r\nRREG32(0x14DC));\r\nWREG32(CP_ME_CNTL, CP_ME_HALT | CP_PFP_HALT);\r\nif (reset_mask & RADEON_RESET_DMA) {\r\ntmp = RREG32(DMA_RB_CNTL + DMA0_REGISTER_OFFSET);\r\ntmp &= ~DMA_RB_ENABLE;\r\nWREG32(DMA_RB_CNTL + DMA0_REGISTER_OFFSET, tmp);\r\n}\r\nif (reset_mask & RADEON_RESET_DMA1) {\r\ntmp = RREG32(DMA_RB_CNTL + DMA1_REGISTER_OFFSET);\r\ntmp &= ~DMA_RB_ENABLE;\r\nWREG32(DMA_RB_CNTL + DMA1_REGISTER_OFFSET, tmp);\r\n}\r\nudelay(50);\r\nevergreen_mc_stop(rdev, &save);\r\nif (evergreen_mc_wait_for_idle(rdev)) {\r\ndev_warn(rdev->dev, "Wait for MC idle timedout !\n");\r\n}\r\nif (reset_mask & (RADEON_RESET_GFX | RADEON_RESET_COMPUTE)) {\r\ngrbm_soft_reset = SOFT_RESET_CB |\r\nSOFT_RESET_DB |\r\nSOFT_RESET_GDS |\r\nSOFT_RESET_PA |\r\nSOFT_RESET_SC |\r\nSOFT_RESET_SPI |\r\nSOFT_RESET_SH |\r\nSOFT_RESET_SX |\r\nSOFT_RESET_TC |\r\nSOFT_RESET_TA |\r\nSOFT_RESET_VGT |\r\nSOFT_RESET_IA;\r\n}\r\nif (reset_mask & RADEON_RESET_CP) {\r\ngrbm_soft_reset |= SOFT_RESET_CP | SOFT_RESET_VGT;\r\nsrbm_soft_reset |= SOFT_RESET_GRBM;\r\n}\r\nif (reset_mask & RADEON_RESET_DMA)\r\nsrbm_soft_reset |= SOFT_RESET_DMA;\r\nif (reset_mask & RADEON_RESET_DMA1)\r\nsrbm_soft_reset |= SOFT_RESET_DMA1;\r\nif (reset_mask & RADEON_RESET_DISPLAY)\r\nsrbm_soft_reset |= SOFT_RESET_DC;\r\nif (reset_mask & RADEON_RESET_RLC)\r\nsrbm_soft_reset |= SOFT_RESET_RLC;\r\nif (reset_mask & RADEON_RESET_SEM)\r\nsrbm_soft_reset |= SOFT_RESET_SEM;\r\nif (reset_mask & RADEON_RESET_IH)\r\nsrbm_soft_reset |= SOFT_RESET_IH;\r\nif (reset_mask & RADEON_RESET_GRBM)\r\nsrbm_soft_reset |= SOFT_RESET_GRBM;\r\nif (reset_mask & RADEON_RESET_VMC)\r\nsrbm_soft_reset |= SOFT_RESET_VMC;\r\nif (!(rdev->flags & RADEON_IS_IGP)) {\r\nif (reset_mask & RADEON_RESET_MC)\r\nsrbm_soft_reset |= SOFT_RESET_MC;\r\n}\r\nif (grbm_soft_reset) {\r\ntmp = RREG32(GRBM_SOFT_RESET);\r\ntmp |= grbm_soft_reset;\r\ndev_info(rdev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);\r\nWREG32(GRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(GRBM_SOFT_RESET);\r\nudelay(50);\r\ntmp &= ~grbm_soft_reset;\r\nWREG32(GRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(GRBM_SOFT_RESET);\r\n}\r\nif (srbm_soft_reset) {\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\ntmp |= srbm_soft_reset;\r\ndev_info(rdev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);\r\nWREG32(SRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\nudelay(50);\r\ntmp &= ~srbm_soft_reset;\r\nWREG32(SRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(SRBM_SOFT_RESET);\r\n}\r\nudelay(50);\r\nevergreen_mc_resume(rdev, &save);\r\nudelay(50);\r\nevergreen_print_gpu_status_regs(rdev);\r\n}\r\nint cayman_asic_reset(struct radeon_device *rdev, bool hard)\r\n{\r\nu32 reset_mask;\r\nif (hard) {\r\nevergreen_gpu_pci_config_reset(rdev);\r\nreturn 0;\r\n}\r\nreset_mask = cayman_gpu_check_soft_reset(rdev);\r\nif (reset_mask)\r\nr600_set_bios_scratch_engine_hung(rdev, true);\r\ncayman_gpu_soft_reset(rdev, reset_mask);\r\nreset_mask = cayman_gpu_check_soft_reset(rdev);\r\nif (reset_mask)\r\nevergreen_gpu_pci_config_reset(rdev);\r\nr600_set_bios_scratch_engine_hung(rdev, false);\r\nreturn 0;\r\n}\r\nbool cayman_gfx_is_lockup(struct radeon_device *rdev, struct radeon_ring *ring)\r\n{\r\nu32 reset_mask = cayman_gpu_check_soft_reset(rdev);\r\nif (!(reset_mask & (RADEON_RESET_GFX |\r\nRADEON_RESET_COMPUTE |\r\nRADEON_RESET_CP))) {\r\nradeon_ring_lockup_update(rdev, ring);\r\nreturn false;\r\n}\r\nreturn radeon_ring_test_lockup(rdev, ring);\r\n}\r\nstatic void cayman_uvd_init(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_uvd)\r\nreturn;\r\nr = radeon_uvd_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed UVD (%d) init.\n", r);\r\nrdev->has_uvd = 0;\r\nreturn;\r\n}\r\nrdev->ring[R600_RING_TYPE_UVD_INDEX].ring_obj = NULL;\r\nr600_ring_init(rdev, &rdev->ring[R600_RING_TYPE_UVD_INDEX], 4096);\r\n}\r\nstatic void cayman_uvd_start(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_uvd)\r\nreturn;\r\nr = uvd_v2_2_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed UVD resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, R600_RING_TYPE_UVD_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing UVD fences (%d).\n", r);\r\ngoto error;\r\n}\r\nreturn;\r\nerror:\r\nrdev->ring[R600_RING_TYPE_UVD_INDEX].ring_size = 0;\r\n}\r\nstatic void cayman_uvd_resume(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nint r;\r\nif (!rdev->has_uvd || !rdev->ring[R600_RING_TYPE_UVD_INDEX].ring_size)\r\nreturn;\r\nring = &rdev->ring[R600_RING_TYPE_UVD_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, 0, PACKET0(UVD_NO_OP, 0));\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing UVD ring (%d).\n", r);\r\nreturn;\r\n}\r\nr = uvd_v1_0_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing UVD (%d).\n", r);\r\nreturn;\r\n}\r\n}\r\nstatic void cayman_vce_init(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_vce)\r\nreturn;\r\nr = radeon_vce_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed VCE (%d) init.\n", r);\r\nrdev->has_vce = 0;\r\nreturn;\r\n}\r\nrdev->ring[TN_RING_TYPE_VCE1_INDEX].ring_obj = NULL;\r\nr600_ring_init(rdev, &rdev->ring[TN_RING_TYPE_VCE1_INDEX], 4096);\r\nrdev->ring[TN_RING_TYPE_VCE2_INDEX].ring_obj = NULL;\r\nr600_ring_init(rdev, &rdev->ring[TN_RING_TYPE_VCE2_INDEX], 4096);\r\n}\r\nstatic void cayman_vce_start(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (!rdev->has_vce)\r\nreturn;\r\nr = radeon_vce_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed VCE resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = vce_v1_0_resume(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed VCE resume (%d).\n", r);\r\ngoto error;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, TN_RING_TYPE_VCE1_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE1 fences (%d).\n", r);\r\ngoto error;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, TN_RING_TYPE_VCE2_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE2 fences (%d).\n", r);\r\ngoto error;\r\n}\r\nreturn;\r\nerror:\r\nrdev->ring[TN_RING_TYPE_VCE1_INDEX].ring_size = 0;\r\nrdev->ring[TN_RING_TYPE_VCE2_INDEX].ring_size = 0;\r\n}\r\nstatic void cayman_vce_resume(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring;\r\nint r;\r\nif (!rdev->has_vce || !rdev->ring[TN_RING_TYPE_VCE1_INDEX].ring_size)\r\nreturn;\r\nring = &rdev->ring[TN_RING_TYPE_VCE1_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, 0, 0x0);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE1 ring (%d).\n", r);\r\nreturn;\r\n}\r\nring = &rdev->ring[TN_RING_TYPE_VCE2_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, 0, 0x0);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE1 ring (%d).\n", r);\r\nreturn;\r\n}\r\nr = vce_v1_0_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing VCE (%d).\n", r);\r\nreturn;\r\n}\r\n}\r\nstatic int cayman_startup(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nint r;\r\nevergreen_pcie_gen2_enable(rdev);\r\nevergreen_program_aspm(rdev);\r\nr = r600_vram_scratch_init(rdev);\r\nif (r)\r\nreturn r;\r\nevergreen_mc_program(rdev);\r\nif (!(rdev->flags & RADEON_IS_IGP) && !rdev->pm.dpm_enabled) {\r\nr = ni_mc_load_microcode(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to load MC firmware!\n");\r\nreturn r;\r\n}\r\n}\r\nr = cayman_pcie_gart_enable(rdev);\r\nif (r)\r\nreturn r;\r\ncayman_gpu_init(rdev);\r\nif (rdev->flags & RADEON_IS_IGP) {\r\nrdev->rlc.reg_list = tn_rlc_save_restore_register_list;\r\nrdev->rlc.reg_list_size =\r\n(u32)ARRAY_SIZE(tn_rlc_save_restore_register_list);\r\nrdev->rlc.cs_data = cayman_cs_data;\r\nr = sumo_rlc_init(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to init rlc BOs!\n");\r\nreturn r;\r\n}\r\n}\r\nr = radeon_wb_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = radeon_fence_driver_start_ring(rdev, RADEON_RING_TYPE_GFX_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing CP fences (%d).\n", r);\r\nreturn r;\r\n}\r\ncayman_uvd_start(rdev);\r\ncayman_vce_start(rdev);\r\nr = radeon_fence_driver_start_ring(rdev, CAYMAN_RING_TYPE_CP1_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing CP fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, CAYMAN_RING_TYPE_CP2_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing CP fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, R600_RING_TYPE_DMA_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing DMA fences (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_fence_driver_start_ring(rdev, CAYMAN_RING_TYPE_DMA1_INDEX);\r\nif (r) {\r\ndev_err(rdev->dev, "failed initializing DMA fences (%d).\n", r);\r\nreturn r;\r\n}\r\nif (!rdev->irq.installed) {\r\nr = radeon_irq_kms_init(rdev);\r\nif (r)\r\nreturn r;\r\n}\r\nr = r600_irq_init(rdev);\r\nif (r) {\r\nDRM_ERROR("radeon: IH init failed (%d).\n", r);\r\nradeon_irq_kms_fini(rdev);\r\nreturn r;\r\n}\r\nevergreen_irq_set(rdev);\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, RADEON_WB_CP_RPTR_OFFSET,\r\nRADEON_CP_PACKET2);\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[R600_RING_TYPE_DMA_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, R600_WB_DMA_RPTR_OFFSET,\r\nDMA_PACKET(DMA_PACKET_NOP, 0, 0, 0));\r\nif (r)\r\nreturn r;\r\nring = &rdev->ring[CAYMAN_RING_TYPE_DMA1_INDEX];\r\nr = radeon_ring_init(rdev, ring, ring->ring_size, CAYMAN_WB_DMA1_RPTR_OFFSET,\r\nDMA_PACKET(DMA_PACKET_NOP, 0, 0, 0));\r\nif (r)\r\nreturn r;\r\nr = cayman_cp_load_microcode(rdev);\r\nif (r)\r\nreturn r;\r\nr = cayman_cp_resume(rdev);\r\nif (r)\r\nreturn r;\r\nr = cayman_dma_resume(rdev);\r\nif (r)\r\nreturn r;\r\ncayman_uvd_resume(rdev);\r\ncayman_vce_resume(rdev);\r\nr = radeon_ib_pool_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "IB initialization failed (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_vm_manager_init(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "vm manager initialization failed (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_audio_init(rdev);\r\nif (r)\r\nreturn r;\r\nreturn 0;\r\n}\r\nint cayman_resume(struct radeon_device *rdev)\r\n{\r\nint r;\r\natom_asic_init(rdev->mode_info.atom_context);\r\nni_init_golden_registers(rdev);\r\nif (rdev->pm.pm_method == PM_METHOD_DPM)\r\nradeon_pm_resume(rdev);\r\nrdev->accel_working = true;\r\nr = cayman_startup(rdev);\r\nif (r) {\r\nDRM_ERROR("cayman startup failed on resume\n");\r\nrdev->accel_working = false;\r\nreturn r;\r\n}\r\nreturn r;\r\n}\r\nint cayman_suspend(struct radeon_device *rdev)\r\n{\r\nradeon_pm_suspend(rdev);\r\nradeon_audio_fini(rdev);\r\nradeon_vm_manager_fini(rdev);\r\ncayman_cp_enable(rdev, false);\r\ncayman_dma_stop(rdev);\r\nif (rdev->has_uvd) {\r\nuvd_v1_0_fini(rdev);\r\nradeon_uvd_suspend(rdev);\r\n}\r\nevergreen_irq_suspend(rdev);\r\nradeon_wb_disable(rdev);\r\ncayman_pcie_gart_disable(rdev);\r\nreturn 0;\r\n}\r\nint cayman_init(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[RADEON_RING_TYPE_GFX_INDEX];\r\nint r;\r\nif (!radeon_get_bios(rdev)) {\r\nif (ASIC_IS_AVIVO(rdev))\r\nreturn -EINVAL;\r\n}\r\nif (!rdev->is_atom_bios) {\r\ndev_err(rdev->dev, "Expecting atombios for cayman GPU\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_atombios_init(rdev);\r\nif (r)\r\nreturn r;\r\nif (!radeon_card_posted(rdev)) {\r\nif (!rdev->bios) {\r\ndev_err(rdev->dev, "Card not posted and no BIOS - ignoring\n");\r\nreturn -EINVAL;\r\n}\r\nDRM_INFO("GPU not posted. posting now...\n");\r\natom_asic_init(rdev->mode_info.atom_context);\r\n}\r\nni_init_golden_registers(rdev);\r\nr600_scratch_init(rdev);\r\nradeon_surface_init(rdev);\r\nradeon_get_clock_info(rdev->ddev);\r\nr = radeon_fence_driver_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = evergreen_mc_init(rdev);\r\nif (r)\r\nreturn r;\r\nr = radeon_bo_init(rdev);\r\nif (r)\r\nreturn r;\r\nif (rdev->flags & RADEON_IS_IGP) {\r\nif (!rdev->me_fw || !rdev->pfp_fw || !rdev->rlc_fw) {\r\nr = ni_init_microcode(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to load firmware!\n");\r\nreturn r;\r\n}\r\n}\r\n} else {\r\nif (!rdev->me_fw || !rdev->pfp_fw || !rdev->rlc_fw || !rdev->mc_fw) {\r\nr = ni_init_microcode(rdev);\r\nif (r) {\r\nDRM_ERROR("Failed to load firmware!\n");\r\nreturn r;\r\n}\r\n}\r\n}\r\nradeon_pm_init(rdev);\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 1024 * 1024);\r\nring = &rdev->ring[R600_RING_TYPE_DMA_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 64 * 1024);\r\nring = &rdev->ring[CAYMAN_RING_TYPE_DMA1_INDEX];\r\nring->ring_obj = NULL;\r\nr600_ring_init(rdev, ring, 64 * 1024);\r\ncayman_uvd_init(rdev);\r\ncayman_vce_init(rdev);\r\nrdev->ih.ring_obj = NULL;\r\nr600_ih_ring_init(rdev, 64 * 1024);\r\nr = r600_pcie_gart_init(rdev);\r\nif (r)\r\nreturn r;\r\nrdev->accel_working = true;\r\nr = cayman_startup(rdev);\r\nif (r) {\r\ndev_err(rdev->dev, "disabling GPU acceleration\n");\r\ncayman_cp_fini(rdev);\r\ncayman_dma_fini(rdev);\r\nr600_irq_fini(rdev);\r\nif (rdev->flags & RADEON_IS_IGP)\r\nsumo_rlc_fini(rdev);\r\nradeon_wb_fini(rdev);\r\nradeon_ib_pool_fini(rdev);\r\nradeon_vm_manager_fini(rdev);\r\nradeon_irq_kms_fini(rdev);\r\ncayman_pcie_gart_fini(rdev);\r\nrdev->accel_working = false;\r\n}\r\nif (!rdev->mc_fw && !(rdev->flags & RADEON_IS_IGP)) {\r\nDRM_ERROR("radeon: MC ucode required for NI+.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nvoid cayman_fini(struct radeon_device *rdev)\r\n{\r\nradeon_pm_fini(rdev);\r\ncayman_cp_fini(rdev);\r\ncayman_dma_fini(rdev);\r\nr600_irq_fini(rdev);\r\nif (rdev->flags & RADEON_IS_IGP)\r\nsumo_rlc_fini(rdev);\r\nradeon_wb_fini(rdev);\r\nradeon_vm_manager_fini(rdev);\r\nradeon_ib_pool_fini(rdev);\r\nradeon_irq_kms_fini(rdev);\r\nuvd_v1_0_fini(rdev);\r\nradeon_uvd_fini(rdev);\r\nif (rdev->has_vce)\r\nradeon_vce_fini(rdev);\r\ncayman_pcie_gart_fini(rdev);\r\nr600_vram_scratch_fini(rdev);\r\nradeon_gem_fini(rdev);\r\nradeon_fence_driver_fini(rdev);\r\nradeon_bo_fini(rdev);\r\nradeon_atombios_fini(rdev);\r\nkfree(rdev->bios);\r\nrdev->bios = NULL;\r\n}\r\nint cayman_vm_init(struct radeon_device *rdev)\r\n{\r\nrdev->vm_manager.nvm = 8;\r\nif (rdev->flags & RADEON_IS_IGP) {\r\nu64 tmp = RREG32(FUS_MC_VM_FB_OFFSET);\r\ntmp <<= 22;\r\nrdev->vm_manager.vram_base_offset = tmp;\r\n} else\r\nrdev->vm_manager.vram_base_offset = 0;\r\nreturn 0;\r\n}\r\nvoid cayman_vm_fini(struct radeon_device *rdev)\r\n{\r\n}\r\nvoid cayman_vm_decode_fault(struct radeon_device *rdev,\r\nu32 status, u32 addr)\r\n{\r\nu32 mc_id = (status & MEMORY_CLIENT_ID_MASK) >> MEMORY_CLIENT_ID_SHIFT;\r\nu32 vmid = (status & FAULT_VMID_MASK) >> FAULT_VMID_SHIFT;\r\nu32 protections = (status & PROTECTIONS_MASK) >> PROTECTIONS_SHIFT;\r\nchar *block;\r\nswitch (mc_id) {\r\ncase 32:\r\ncase 16:\r\ncase 96:\r\ncase 80:\r\ncase 160:\r\ncase 144:\r\ncase 224:\r\ncase 208:\r\nblock = "CB";\r\nbreak;\r\ncase 33:\r\ncase 17:\r\ncase 97:\r\ncase 81:\r\ncase 161:\r\ncase 145:\r\ncase 225:\r\ncase 209:\r\nblock = "CB_FMASK";\r\nbreak;\r\ncase 34:\r\ncase 18:\r\ncase 98:\r\ncase 82:\r\ncase 162:\r\ncase 146:\r\ncase 226:\r\ncase 210:\r\nblock = "CB_CMASK";\r\nbreak;\r\ncase 35:\r\ncase 19:\r\ncase 99:\r\ncase 83:\r\ncase 163:\r\ncase 147:\r\ncase 227:\r\ncase 211:\r\nblock = "CB_IMMED";\r\nbreak;\r\ncase 36:\r\ncase 20:\r\ncase 100:\r\ncase 84:\r\ncase 164:\r\ncase 148:\r\ncase 228:\r\ncase 212:\r\nblock = "DB";\r\nbreak;\r\ncase 37:\r\ncase 21:\r\ncase 101:\r\ncase 85:\r\ncase 165:\r\ncase 149:\r\ncase 229:\r\ncase 213:\r\nblock = "DB_HTILE";\r\nbreak;\r\ncase 38:\r\ncase 22:\r\ncase 102:\r\ncase 86:\r\ncase 166:\r\ncase 150:\r\ncase 230:\r\ncase 214:\r\nblock = "SX";\r\nbreak;\r\ncase 39:\r\ncase 23:\r\ncase 103:\r\ncase 87:\r\ncase 167:\r\ncase 151:\r\ncase 231:\r\ncase 215:\r\nblock = "DB_STEN";\r\nbreak;\r\ncase 40:\r\ncase 24:\r\ncase 104:\r\ncase 88:\r\ncase 232:\r\ncase 216:\r\ncase 168:\r\ncase 152:\r\nblock = "TC_TFETCH";\r\nbreak;\r\ncase 41:\r\ncase 25:\r\ncase 105:\r\ncase 89:\r\ncase 233:\r\ncase 217:\r\ncase 169:\r\ncase 153:\r\nblock = "TC_VFETCH";\r\nbreak;\r\ncase 42:\r\ncase 26:\r\ncase 106:\r\ncase 90:\r\ncase 234:\r\ncase 218:\r\ncase 170:\r\ncase 154:\r\nblock = "VC";\r\nbreak;\r\ncase 112:\r\nblock = "CP";\r\nbreak;\r\ncase 113:\r\ncase 114:\r\nblock = "SH";\r\nbreak;\r\ncase 115:\r\nblock = "VGT";\r\nbreak;\r\ncase 178:\r\nblock = "IH";\r\nbreak;\r\ncase 51:\r\nblock = "RLC";\r\nbreak;\r\ncase 55:\r\nblock = "DMA";\r\nbreak;\r\ncase 56:\r\nblock = "HDP";\r\nbreak;\r\ndefault:\r\nblock = "unknown";\r\nbreak;\r\n}\r\nprintk("VM fault (0x%02x, vmid %d) at page %u, %s from %s (%d)\n",\r\nprotections, vmid, addr,\r\n(status & MEMORY_CLIENT_RW_MASK) ? "write" : "read",\r\nblock, mc_id);\r\n}\r\nvoid cayman_vm_flush(struct radeon_device *rdev, struct radeon_ring *ring,\r\nunsigned vm_id, uint64_t pd_addr)\r\n{\r\nradeon_ring_write(ring, PACKET0(VM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (vm_id << 2), 0));\r\nradeon_ring_write(ring, pd_addr >> 12);\r\nradeon_ring_write(ring, PACKET0(HDP_MEM_COHERENCY_FLUSH_CNTL, 0));\r\nradeon_ring_write(ring, 0x1);\r\nradeon_ring_write(ring, PACKET0(VM_INVALIDATE_REQUEST, 0));\r\nradeon_ring_write(ring, 1 << vm_id);\r\nradeon_ring_write(ring, PACKET3(PACKET3_WAIT_REG_MEM, 5));\r\nradeon_ring_write(ring, (WAIT_REG_MEM_FUNCTION(0) |\r\nWAIT_REG_MEM_ENGINE(0)));\r\nradeon_ring_write(ring, VM_INVALIDATE_REQUEST >> 2);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0);\r\nradeon_ring_write(ring, 0x20);\r\nradeon_ring_write(ring, PACKET3(PACKET3_PFP_SYNC_ME, 0));\r\nradeon_ring_write(ring, 0x0);\r\n}\r\nint tn_set_vce_clocks(struct radeon_device *rdev, u32 evclk, u32 ecclk)\r\n{\r\nstruct atom_clock_dividers dividers;\r\nint r, i;\r\nr = radeon_atom_get_clock_dividers(rdev, COMPUTE_ENGINE_PLL_PARAM,\r\necclk, false, &dividers);\r\nif (r)\r\nreturn r;\r\nfor (i = 0; i < 100; i++) {\r\nif (RREG32(CG_ECLK_STATUS) & ECLK_STATUS)\r\nbreak;\r\nmdelay(10);\r\n}\r\nif (i == 100)\r\nreturn -ETIMEDOUT;\r\nWREG32_P(CG_ECLK_CNTL, dividers.post_div, ~(ECLK_DIR_CNTL_EN|ECLK_DIVIDER_MASK));\r\nfor (i = 0; i < 100; i++) {\r\nif (RREG32(CG_ECLK_STATUS) & ECLK_STATUS)\r\nbreak;\r\nmdelay(10);\r\n}\r\nif (i == 100)\r\nreturn -ETIMEDOUT;\r\nreturn 0;\r\n}
