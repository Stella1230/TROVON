static bool bbr_full_bw_reached(const struct sock *sk)\r\n{\r\nconst struct bbr *bbr = inet_csk_ca(sk);\r\nreturn bbr->full_bw_cnt >= bbr_full_bw_cnt;\r\n}\r\nstatic u32 bbr_max_bw(const struct sock *sk)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nreturn minmax_get(&bbr->bw);\r\n}\r\nstatic u32 bbr_bw(const struct sock *sk)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nreturn bbr->lt_use_bw ? bbr->lt_bw : bbr_max_bw(sk);\r\n}\r\nstatic u64 bbr_rate_bytes_per_sec(struct sock *sk, u64 rate, int gain)\r\n{\r\nrate *= tcp_mss_to_mtu(sk, tcp_sk(sk)->mss_cache);\r\nrate *= gain;\r\nrate >>= BBR_SCALE;\r\nrate *= USEC_PER_SEC;\r\nreturn rate >> BW_SCALE;\r\n}\r\nstatic u32 bbr_bw_to_pacing_rate(struct sock *sk, u32 bw, int gain)\r\n{\r\nu64 rate = bw;\r\nrate = bbr_rate_bytes_per_sec(sk, rate, gain);\r\nrate = min_t(u64, rate, sk->sk_max_pacing_rate);\r\nreturn rate;\r\n}\r\nstatic void bbr_init_pacing_rate_from_rtt(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu64 bw;\r\nu32 rtt_us;\r\nif (tp->srtt_us) {\r\nrtt_us = max(tp->srtt_us >> 3, 1U);\r\nbbr->has_seen_rtt = 1;\r\n} else {\r\nrtt_us = USEC_PER_MSEC;\r\n}\r\nbw = (u64)tp->snd_cwnd * BW_UNIT;\r\ndo_div(bw, rtt_us);\r\nsk->sk_pacing_rate = bbr_bw_to_pacing_rate(sk, bw, bbr_high_gain);\r\n}\r\nstatic void bbr_set_pacing_rate(struct sock *sk, u32 bw, int gain)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu32 rate = bbr_bw_to_pacing_rate(sk, bw, gain);\r\nif (unlikely(!bbr->has_seen_rtt && tp->srtt_us))\r\nbbr_init_pacing_rate_from_rtt(sk);\r\nif (bbr_full_bw_reached(sk) || rate > sk->sk_pacing_rate)\r\nsk->sk_pacing_rate = rate;\r\n}\r\nstatic u32 bbr_tso_segs_goal(struct sock *sk)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nreturn bbr->tso_segs_goal;\r\n}\r\nstatic void bbr_set_tso_segs_goal(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu32 min_segs;\r\nmin_segs = sk->sk_pacing_rate < (bbr_min_tso_rate >> 3) ? 1 : 2;\r\nbbr->tso_segs_goal = min(tcp_tso_autosize(sk, tp->mss_cache, min_segs),\r\n0x7FU);\r\n}\r\nstatic void bbr_save_cwnd(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nif (bbr->prev_ca_state < TCP_CA_Recovery && bbr->mode != BBR_PROBE_RTT)\r\nbbr->prior_cwnd = tp->snd_cwnd;\r\nelse\r\nbbr->prior_cwnd = max(bbr->prior_cwnd, tp->snd_cwnd);\r\n}\r\nstatic void bbr_cwnd_event(struct sock *sk, enum tcp_ca_event event)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nif (event == CA_EVENT_TX_START && tp->app_limited) {\r\nbbr->idle_restart = 1;\r\nif (bbr->mode == BBR_PROBE_BW)\r\nbbr_set_pacing_rate(sk, bbr_bw(sk), BBR_UNIT);\r\n}\r\n}\r\nstatic u32 bbr_target_cwnd(struct sock *sk, u32 bw, int gain)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu32 cwnd;\r\nu64 w;\r\nif (unlikely(bbr->min_rtt_us == ~0U))\r\nreturn TCP_INIT_CWND;\r\nw = (u64)bw * bbr->min_rtt_us;\r\ncwnd = (((w * gain) >> BBR_SCALE) + BW_UNIT - 1) / BW_UNIT;\r\ncwnd += 3 * bbr->tso_segs_goal;\r\ncwnd = (cwnd + 1) & ~1U;\r\nreturn cwnd;\r\n}\r\nstatic bool bbr_set_cwnd_to_recover_or_restore(\r\nstruct sock *sk, const struct rate_sample *rs, u32 acked, u32 *new_cwnd)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu8 prev_state = bbr->prev_ca_state, state = inet_csk(sk)->icsk_ca_state;\r\nu32 cwnd = tp->snd_cwnd;\r\nif (rs->losses > 0)\r\ncwnd = max_t(s32, cwnd - rs->losses, 1);\r\nif (state == TCP_CA_Recovery && prev_state != TCP_CA_Recovery) {\r\nbbr->packet_conservation = 1;\r\nbbr->next_rtt_delivered = tp->delivered;\r\ncwnd = tcp_packets_in_flight(tp) + acked;\r\n} else if (prev_state >= TCP_CA_Recovery && state < TCP_CA_Recovery) {\r\nbbr->restore_cwnd = 1;\r\nbbr->packet_conservation = 0;\r\n}\r\nbbr->prev_ca_state = state;\r\nif (bbr->restore_cwnd) {\r\ncwnd = max(cwnd, bbr->prior_cwnd);\r\nbbr->restore_cwnd = 0;\r\n}\r\nif (bbr->packet_conservation) {\r\n*new_cwnd = max(cwnd, tcp_packets_in_flight(tp) + acked);\r\nreturn true;\r\n}\r\n*new_cwnd = cwnd;\r\nreturn false;\r\n}\r\nstatic void bbr_set_cwnd(struct sock *sk, const struct rate_sample *rs,\r\nu32 acked, u32 bw, int gain)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu32 cwnd = 0, target_cwnd = 0;\r\nif (!acked)\r\nreturn;\r\nif (bbr_set_cwnd_to_recover_or_restore(sk, rs, acked, &cwnd))\r\ngoto done;\r\ntarget_cwnd = bbr_target_cwnd(sk, bw, gain);\r\nif (bbr_full_bw_reached(sk))\r\ncwnd = min(cwnd + acked, target_cwnd);\r\nelse if (cwnd < target_cwnd || tp->delivered < TCP_INIT_CWND)\r\ncwnd = cwnd + acked;\r\ncwnd = max(cwnd, bbr_cwnd_min_target);\r\ndone:\r\ntp->snd_cwnd = min(cwnd, tp->snd_cwnd_clamp);\r\nif (bbr->mode == BBR_PROBE_RTT)\r\ntp->snd_cwnd = min(tp->snd_cwnd, bbr_cwnd_min_target);\r\n}\r\nstatic bool bbr_is_next_cycle_phase(struct sock *sk,\r\nconst struct rate_sample *rs)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nbool is_full_length =\r\ntcp_stamp_us_delta(tp->delivered_mstamp, bbr->cycle_mstamp) >\r\nbbr->min_rtt_us;\r\nu32 inflight, bw;\r\nif (bbr->pacing_gain == BBR_UNIT)\r\nreturn is_full_length;\r\ninflight = rs->prior_in_flight;\r\nbw = bbr_max_bw(sk);\r\nif (bbr->pacing_gain > BBR_UNIT)\r\nreturn is_full_length &&\r\n(rs->losses ||\r\ninflight >= bbr_target_cwnd(sk, bw, bbr->pacing_gain));\r\nreturn is_full_length ||\r\ninflight <= bbr_target_cwnd(sk, bw, BBR_UNIT);\r\n}\r\nstatic void bbr_advance_cycle_phase(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nbbr->cycle_idx = (bbr->cycle_idx + 1) & (CYCLE_LEN - 1);\r\nbbr->cycle_mstamp = tp->delivered_mstamp;\r\nbbr->pacing_gain = bbr_pacing_gain[bbr->cycle_idx];\r\n}\r\nstatic void bbr_update_cycle_phase(struct sock *sk,\r\nconst struct rate_sample *rs)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nif ((bbr->mode == BBR_PROBE_BW) && !bbr->lt_use_bw &&\r\nbbr_is_next_cycle_phase(sk, rs))\r\nbbr_advance_cycle_phase(sk);\r\n}\r\nstatic void bbr_reset_startup_mode(struct sock *sk)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nbbr->mode = BBR_STARTUP;\r\nbbr->pacing_gain = bbr_high_gain;\r\nbbr->cwnd_gain = bbr_high_gain;\r\n}\r\nstatic void bbr_reset_probe_bw_mode(struct sock *sk)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nbbr->mode = BBR_PROBE_BW;\r\nbbr->pacing_gain = BBR_UNIT;\r\nbbr->cwnd_gain = bbr_cwnd_gain;\r\nbbr->cycle_idx = CYCLE_LEN - 1 - prandom_u32_max(bbr_cycle_rand);\r\nbbr_advance_cycle_phase(sk);\r\n}\r\nstatic void bbr_reset_mode(struct sock *sk)\r\n{\r\nif (!bbr_full_bw_reached(sk))\r\nbbr_reset_startup_mode(sk);\r\nelse\r\nbbr_reset_probe_bw_mode(sk);\r\n}\r\nstatic void bbr_reset_lt_bw_sampling_interval(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nbbr->lt_last_stamp = div_u64(tp->delivered_mstamp, USEC_PER_MSEC);\r\nbbr->lt_last_delivered = tp->delivered;\r\nbbr->lt_last_lost = tp->lost;\r\nbbr->lt_rtt_cnt = 0;\r\n}\r\nstatic void bbr_reset_lt_bw_sampling(struct sock *sk)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nbbr->lt_bw = 0;\r\nbbr->lt_use_bw = 0;\r\nbbr->lt_is_sampling = false;\r\nbbr_reset_lt_bw_sampling_interval(sk);\r\n}\r\nstatic void bbr_lt_bw_interval_done(struct sock *sk, u32 bw)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu32 diff;\r\nif (bbr->lt_bw) {\r\ndiff = abs(bw - bbr->lt_bw);\r\nif ((diff * BBR_UNIT <= bbr_lt_bw_ratio * bbr->lt_bw) ||\r\n(bbr_rate_bytes_per_sec(sk, diff, BBR_UNIT) <=\r\nbbr_lt_bw_diff)) {\r\nbbr->lt_bw = (bw + bbr->lt_bw) >> 1;\r\nbbr->lt_use_bw = 1;\r\nbbr->pacing_gain = BBR_UNIT;\r\nbbr->lt_rtt_cnt = 0;\r\nreturn;\r\n}\r\n}\r\nbbr->lt_bw = bw;\r\nbbr_reset_lt_bw_sampling_interval(sk);\r\n}\r\nstatic void bbr_lt_bw_sampling(struct sock *sk, const struct rate_sample *rs)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu32 lost, delivered;\r\nu64 bw;\r\nu32 t;\r\nif (bbr->lt_use_bw) {\r\nif (bbr->mode == BBR_PROBE_BW && bbr->round_start &&\r\n++bbr->lt_rtt_cnt >= bbr_lt_bw_max_rtts) {\r\nbbr_reset_lt_bw_sampling(sk);\r\nbbr_reset_probe_bw_mode(sk);\r\n}\r\nreturn;\r\n}\r\nif (!bbr->lt_is_sampling) {\r\nif (!rs->losses)\r\nreturn;\r\nbbr_reset_lt_bw_sampling_interval(sk);\r\nbbr->lt_is_sampling = true;\r\n}\r\nif (rs->is_app_limited) {\r\nbbr_reset_lt_bw_sampling(sk);\r\nreturn;\r\n}\r\nif (bbr->round_start)\r\nbbr->lt_rtt_cnt++;\r\nif (bbr->lt_rtt_cnt < bbr_lt_intvl_min_rtts)\r\nreturn;\r\nif (bbr->lt_rtt_cnt > 4 * bbr_lt_intvl_min_rtts) {\r\nbbr_reset_lt_bw_sampling(sk);\r\nreturn;\r\n}\r\nif (!rs->losses)\r\nreturn;\r\nlost = tp->lost - bbr->lt_last_lost;\r\ndelivered = tp->delivered - bbr->lt_last_delivered;\r\nif (!delivered || (lost << BBR_SCALE) < bbr_lt_loss_thresh * delivered)\r\nreturn;\r\nt = div_u64(tp->delivered_mstamp, USEC_PER_MSEC) - bbr->lt_last_stamp;\r\nif ((s32)t < 1)\r\nreturn;\r\nif (t >= ~0U / USEC_PER_MSEC) {\r\nbbr_reset_lt_bw_sampling(sk);\r\nreturn;\r\n}\r\nt *= USEC_PER_MSEC;\r\nbw = (u64)delivered * BW_UNIT;\r\ndo_div(bw, t);\r\nbbr_lt_bw_interval_done(sk, bw);\r\n}\r\nstatic void bbr_update_bw(struct sock *sk, const struct rate_sample *rs)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu64 bw;\r\nbbr->round_start = 0;\r\nif (rs->delivered < 0 || rs->interval_us <= 0)\r\nreturn;\r\nif (!before(rs->prior_delivered, bbr->next_rtt_delivered)) {\r\nbbr->next_rtt_delivered = tp->delivered;\r\nbbr->rtt_cnt++;\r\nbbr->round_start = 1;\r\nbbr->packet_conservation = 0;\r\n}\r\nbbr_lt_bw_sampling(sk, rs);\r\nbw = (u64)rs->delivered * BW_UNIT;\r\ndo_div(bw, rs->interval_us);\r\nif (!rs->is_app_limited || bw >= bbr_max_bw(sk)) {\r\nminmax_running_max(&bbr->bw, bbr_bw_rtts, bbr->rtt_cnt, bw);\r\n}\r\n}\r\nstatic void bbr_check_full_bw_reached(struct sock *sk,\r\nconst struct rate_sample *rs)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu32 bw_thresh;\r\nif (bbr_full_bw_reached(sk) || !bbr->round_start || rs->is_app_limited)\r\nreturn;\r\nbw_thresh = (u64)bbr->full_bw * bbr_full_bw_thresh >> BBR_SCALE;\r\nif (bbr_max_bw(sk) >= bw_thresh) {\r\nbbr->full_bw = bbr_max_bw(sk);\r\nbbr->full_bw_cnt = 0;\r\nreturn;\r\n}\r\n++bbr->full_bw_cnt;\r\n}\r\nstatic void bbr_check_drain(struct sock *sk, const struct rate_sample *rs)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nif (bbr->mode == BBR_STARTUP && bbr_full_bw_reached(sk)) {\r\nbbr->mode = BBR_DRAIN;\r\nbbr->pacing_gain = bbr_drain_gain;\r\nbbr->cwnd_gain = bbr_high_gain;\r\n}\r\nif (bbr->mode == BBR_DRAIN &&\r\ntcp_packets_in_flight(tcp_sk(sk)) <=\r\nbbr_target_cwnd(sk, bbr_max_bw(sk), BBR_UNIT))\r\nbbr_reset_probe_bw_mode(sk);\r\n}\r\nstatic void bbr_update_min_rtt(struct sock *sk, const struct rate_sample *rs)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nbool filter_expired;\r\nfilter_expired = after(tcp_jiffies32,\r\nbbr->min_rtt_stamp + bbr_min_rtt_win_sec * HZ);\r\nif (rs->rtt_us >= 0 &&\r\n(rs->rtt_us <= bbr->min_rtt_us || filter_expired)) {\r\nbbr->min_rtt_us = rs->rtt_us;\r\nbbr->min_rtt_stamp = tcp_jiffies32;\r\n}\r\nif (bbr_probe_rtt_mode_ms > 0 && filter_expired &&\r\n!bbr->idle_restart && bbr->mode != BBR_PROBE_RTT) {\r\nbbr->mode = BBR_PROBE_RTT;\r\nbbr->pacing_gain = BBR_UNIT;\r\nbbr->cwnd_gain = BBR_UNIT;\r\nbbr_save_cwnd(sk);\r\nbbr->probe_rtt_done_stamp = 0;\r\n}\r\nif (bbr->mode == BBR_PROBE_RTT) {\r\ntp->app_limited =\r\n(tp->delivered + tcp_packets_in_flight(tp)) ? : 1;\r\nif (!bbr->probe_rtt_done_stamp &&\r\ntcp_packets_in_flight(tp) <= bbr_cwnd_min_target) {\r\nbbr->probe_rtt_done_stamp = tcp_jiffies32 +\r\nmsecs_to_jiffies(bbr_probe_rtt_mode_ms);\r\nbbr->probe_rtt_round_done = 0;\r\nbbr->next_rtt_delivered = tp->delivered;\r\n} else if (bbr->probe_rtt_done_stamp) {\r\nif (bbr->round_start)\r\nbbr->probe_rtt_round_done = 1;\r\nif (bbr->probe_rtt_round_done &&\r\nafter(tcp_jiffies32, bbr->probe_rtt_done_stamp)) {\r\nbbr->min_rtt_stamp = tcp_jiffies32;\r\nbbr->restore_cwnd = 1;\r\nbbr_reset_mode(sk);\r\n}\r\n}\r\n}\r\nbbr->idle_restart = 0;\r\n}\r\nstatic void bbr_update_model(struct sock *sk, const struct rate_sample *rs)\r\n{\r\nbbr_update_bw(sk, rs);\r\nbbr_update_cycle_phase(sk, rs);\r\nbbr_check_full_bw_reached(sk, rs);\r\nbbr_check_drain(sk, rs);\r\nbbr_update_min_rtt(sk, rs);\r\n}\r\nstatic void bbr_main(struct sock *sk, const struct rate_sample *rs)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu32 bw;\r\nbbr_update_model(sk, rs);\r\nbw = bbr_bw(sk);\r\nbbr_set_pacing_rate(sk, bw, bbr->pacing_gain);\r\nbbr_set_tso_segs_goal(sk);\r\nbbr_set_cwnd(sk, rs, rs->acked_sacked, bw, bbr->cwnd_gain);\r\n}\r\nstatic void bbr_init(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nbbr->prior_cwnd = 0;\r\nbbr->tso_segs_goal = 0;\r\nbbr->rtt_cnt = 0;\r\nbbr->next_rtt_delivered = 0;\r\nbbr->prev_ca_state = TCP_CA_Open;\r\nbbr->packet_conservation = 0;\r\nbbr->probe_rtt_done_stamp = 0;\r\nbbr->probe_rtt_round_done = 0;\r\nbbr->min_rtt_us = tcp_min_rtt(tp);\r\nbbr->min_rtt_stamp = tcp_jiffies32;\r\nminmax_reset(&bbr->bw, bbr->rtt_cnt, 0);\r\nbbr->has_seen_rtt = 0;\r\nbbr_init_pacing_rate_from_rtt(sk);\r\nbbr->restore_cwnd = 0;\r\nbbr->round_start = 0;\r\nbbr->idle_restart = 0;\r\nbbr->full_bw = 0;\r\nbbr->full_bw_cnt = 0;\r\nbbr->cycle_mstamp = 0;\r\nbbr->cycle_idx = 0;\r\nbbr_reset_lt_bw_sampling(sk);\r\nbbr_reset_startup_mode(sk);\r\ncmpxchg(&sk->sk_pacing_status, SK_PACING_NONE, SK_PACING_NEEDED);\r\n}\r\nstatic u32 bbr_sndbuf_expand(struct sock *sk)\r\n{\r\nreturn 3;\r\n}\r\nstatic u32 bbr_undo_cwnd(struct sock *sk)\r\n{\r\nreturn tcp_sk(sk)->snd_cwnd;\r\n}\r\nstatic u32 bbr_ssthresh(struct sock *sk)\r\n{\r\nbbr_save_cwnd(sk);\r\nreturn TCP_INFINITE_SSTHRESH;\r\n}\r\nstatic size_t bbr_get_info(struct sock *sk, u32 ext, int *attr,\r\nunion tcp_cc_info *info)\r\n{\r\nif (ext & (1 << (INET_DIAG_BBRINFO - 1)) ||\r\next & (1 << (INET_DIAG_VEGASINFO - 1))) {\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nu64 bw = bbr_bw(sk);\r\nbw = bw * tp->mss_cache * USEC_PER_SEC >> BW_SCALE;\r\nmemset(&info->bbr, 0, sizeof(info->bbr));\r\ninfo->bbr.bbr_bw_lo = (u32)bw;\r\ninfo->bbr.bbr_bw_hi = (u32)(bw >> 32);\r\ninfo->bbr.bbr_min_rtt = bbr->min_rtt_us;\r\ninfo->bbr.bbr_pacing_gain = bbr->pacing_gain;\r\ninfo->bbr.bbr_cwnd_gain = bbr->cwnd_gain;\r\n*attr = INET_DIAG_BBRINFO;\r\nreturn sizeof(info->bbr);\r\n}\r\nreturn 0;\r\n}\r\nstatic void bbr_set_state(struct sock *sk, u8 new_state)\r\n{\r\nstruct bbr *bbr = inet_csk_ca(sk);\r\nif (new_state == TCP_CA_Loss) {\r\nstruct rate_sample rs = { .losses = 1 };\r\nbbr->prev_ca_state = TCP_CA_Loss;\r\nbbr->full_bw = 0;\r\nbbr->round_start = 1;\r\nbbr_lt_bw_sampling(sk, &rs);\r\n}\r\n}\r\nstatic int __init bbr_register(void)\r\n{\r\nBUILD_BUG_ON(sizeof(struct bbr) > ICSK_CA_PRIV_SIZE);\r\nreturn tcp_register_congestion_control(&tcp_bbr_cong_ops);\r\n}\r\nstatic void __exit bbr_unregister(void)\r\n{\r\ntcp_unregister_congestion_control(&tcp_bbr_cong_ops);\r\n}
