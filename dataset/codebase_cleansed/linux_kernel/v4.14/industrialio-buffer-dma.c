static void iio_buffer_block_release(struct kref *kref)\r\n{\r\nstruct iio_dma_buffer_block *block = container_of(kref,\r\nstruct iio_dma_buffer_block, kref);\r\nWARN_ON(block->state != IIO_BLOCK_STATE_DEAD);\r\ndma_free_coherent(block->queue->dev, PAGE_ALIGN(block->size),\r\nblock->vaddr, block->phys_addr);\r\niio_buffer_put(&block->queue->buffer);\r\nkfree(block);\r\n}\r\nstatic void iio_buffer_block_get(struct iio_dma_buffer_block *block)\r\n{\r\nkref_get(&block->kref);\r\n}\r\nstatic void iio_buffer_block_put(struct iio_dma_buffer_block *block)\r\n{\r\nkref_put(&block->kref, iio_buffer_block_release);\r\n}\r\nstatic void iio_dma_buffer_cleanup_worker(struct work_struct *work)\r\n{\r\nstruct iio_dma_buffer_block *block, *_block;\r\nLIST_HEAD(block_list);\r\nspin_lock_irq(&iio_dma_buffer_dead_blocks_lock);\r\nlist_splice_tail_init(&iio_dma_buffer_dead_blocks, &block_list);\r\nspin_unlock_irq(&iio_dma_buffer_dead_blocks_lock);\r\nlist_for_each_entry_safe(block, _block, &block_list, head)\r\niio_buffer_block_release(&block->kref);\r\n}\r\nstatic void iio_buffer_block_release_atomic(struct kref *kref)\r\n{\r\nstruct iio_dma_buffer_block *block;\r\nunsigned long flags;\r\nblock = container_of(kref, struct iio_dma_buffer_block, kref);\r\nspin_lock_irqsave(&iio_dma_buffer_dead_blocks_lock, flags);\r\nlist_add_tail(&block->head, &iio_dma_buffer_dead_blocks);\r\nspin_unlock_irqrestore(&iio_dma_buffer_dead_blocks_lock, flags);\r\nschedule_work(&iio_dma_buffer_cleanup_work);\r\n}\r\nstatic void iio_buffer_block_put_atomic(struct iio_dma_buffer_block *block)\r\n{\r\nkref_put(&block->kref, iio_buffer_block_release_atomic);\r\n}\r\nstatic struct iio_dma_buffer_queue *iio_buffer_to_queue(struct iio_buffer *buf)\r\n{\r\nreturn container_of(buf, struct iio_dma_buffer_queue, buffer);\r\n}\r\nstatic struct iio_dma_buffer_block *iio_dma_buffer_alloc_block(\r\nstruct iio_dma_buffer_queue *queue, size_t size)\r\n{\r\nstruct iio_dma_buffer_block *block;\r\nblock = kzalloc(sizeof(*block), GFP_KERNEL);\r\nif (!block)\r\nreturn NULL;\r\nblock->vaddr = dma_alloc_coherent(queue->dev, PAGE_ALIGN(size),\r\n&block->phys_addr, GFP_KERNEL);\r\nif (!block->vaddr) {\r\nkfree(block);\r\nreturn NULL;\r\n}\r\nblock->size = size;\r\nblock->state = IIO_BLOCK_STATE_DEQUEUED;\r\nblock->queue = queue;\r\nINIT_LIST_HEAD(&block->head);\r\nkref_init(&block->kref);\r\niio_buffer_get(&queue->buffer);\r\nreturn block;\r\n}\r\nstatic void _iio_dma_buffer_block_done(struct iio_dma_buffer_block *block)\r\n{\r\nstruct iio_dma_buffer_queue *queue = block->queue;\r\nif (block->state != IIO_BLOCK_STATE_DEAD) {\r\nblock->state = IIO_BLOCK_STATE_DONE;\r\nlist_add_tail(&block->head, &queue->outgoing);\r\n}\r\n}\r\nvoid iio_dma_buffer_block_done(struct iio_dma_buffer_block *block)\r\n{\r\nstruct iio_dma_buffer_queue *queue = block->queue;\r\nunsigned long flags;\r\nspin_lock_irqsave(&queue->list_lock, flags);\r\n_iio_dma_buffer_block_done(block);\r\nspin_unlock_irqrestore(&queue->list_lock, flags);\r\niio_buffer_block_put_atomic(block);\r\nwake_up_interruptible_poll(&queue->buffer.pollq, POLLIN | POLLRDNORM);\r\n}\r\nvoid iio_dma_buffer_block_list_abort(struct iio_dma_buffer_queue *queue,\r\nstruct list_head *list)\r\n{\r\nstruct iio_dma_buffer_block *block, *_block;\r\nunsigned long flags;\r\nspin_lock_irqsave(&queue->list_lock, flags);\r\nlist_for_each_entry_safe(block, _block, list, head) {\r\nlist_del(&block->head);\r\nblock->bytes_used = 0;\r\n_iio_dma_buffer_block_done(block);\r\niio_buffer_block_put_atomic(block);\r\n}\r\nspin_unlock_irqrestore(&queue->list_lock, flags);\r\nwake_up_interruptible_poll(&queue->buffer.pollq, POLLIN | POLLRDNORM);\r\n}\r\nstatic bool iio_dma_block_reusable(struct iio_dma_buffer_block *block)\r\n{\r\nswitch (block->state) {\r\ncase IIO_BLOCK_STATE_DEQUEUED:\r\ncase IIO_BLOCK_STATE_QUEUED:\r\ncase IIO_BLOCK_STATE_DONE:\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nint iio_dma_buffer_request_update(struct iio_buffer *buffer)\r\n{\r\nstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buffer);\r\nstruct iio_dma_buffer_block *block;\r\nbool try_reuse = false;\r\nsize_t size;\r\nint ret = 0;\r\nint i;\r\nsize = DIV_ROUND_UP(queue->buffer.bytes_per_datum *\r\nqueue->buffer.length, 2);\r\nmutex_lock(&queue->lock);\r\nif (PAGE_ALIGN(queue->fileio.block_size) == PAGE_ALIGN(size))\r\ntry_reuse = true;\r\nqueue->fileio.block_size = size;\r\nqueue->fileio.active_block = NULL;\r\nspin_lock_irq(&queue->list_lock);\r\nfor (i = 0; i < ARRAY_SIZE(queue->fileio.blocks); i++) {\r\nblock = queue->fileio.blocks[i];\r\nif (block && (!iio_dma_block_reusable(block) || !try_reuse))\r\nblock->state = IIO_BLOCK_STATE_DEAD;\r\n}\r\nINIT_LIST_HEAD(&queue->outgoing);\r\nspin_unlock_irq(&queue->list_lock);\r\nINIT_LIST_HEAD(&queue->incoming);\r\nfor (i = 0; i < ARRAY_SIZE(queue->fileio.blocks); i++) {\r\nif (queue->fileio.blocks[i]) {\r\nblock = queue->fileio.blocks[i];\r\nif (block->state == IIO_BLOCK_STATE_DEAD) {\r\niio_buffer_block_put(block);\r\nblock = NULL;\r\n} else {\r\nblock->size = size;\r\n}\r\n} else {\r\nblock = NULL;\r\n}\r\nif (!block) {\r\nblock = iio_dma_buffer_alloc_block(queue, size);\r\nif (!block) {\r\nret = -ENOMEM;\r\ngoto out_unlock;\r\n}\r\nqueue->fileio.blocks[i] = block;\r\n}\r\nblock->state = IIO_BLOCK_STATE_QUEUED;\r\nlist_add_tail(&block->head, &queue->incoming);\r\n}\r\nout_unlock:\r\nmutex_unlock(&queue->lock);\r\nreturn ret;\r\n}\r\nstatic void iio_dma_buffer_submit_block(struct iio_dma_buffer_queue *queue,\r\nstruct iio_dma_buffer_block *block)\r\n{\r\nint ret;\r\nif (!queue->ops)\r\nreturn;\r\nblock->state = IIO_BLOCK_STATE_ACTIVE;\r\niio_buffer_block_get(block);\r\nret = queue->ops->submit(queue, block);\r\nif (ret) {\r\niio_buffer_block_put(block);\r\n}\r\n}\r\nint iio_dma_buffer_enable(struct iio_buffer *buffer,\r\nstruct iio_dev *indio_dev)\r\n{\r\nstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buffer);\r\nstruct iio_dma_buffer_block *block, *_block;\r\nmutex_lock(&queue->lock);\r\nqueue->active = true;\r\nlist_for_each_entry_safe(block, _block, &queue->incoming, head) {\r\nlist_del(&block->head);\r\niio_dma_buffer_submit_block(queue, block);\r\n}\r\nmutex_unlock(&queue->lock);\r\nreturn 0;\r\n}\r\nint iio_dma_buffer_disable(struct iio_buffer *buffer,\r\nstruct iio_dev *indio_dev)\r\n{\r\nstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buffer);\r\nmutex_lock(&queue->lock);\r\nqueue->active = false;\r\nif (queue->ops && queue->ops->abort)\r\nqueue->ops->abort(queue);\r\nmutex_unlock(&queue->lock);\r\nreturn 0;\r\n}\r\nstatic void iio_dma_buffer_enqueue(struct iio_dma_buffer_queue *queue,\r\nstruct iio_dma_buffer_block *block)\r\n{\r\nif (block->state == IIO_BLOCK_STATE_DEAD) {\r\niio_buffer_block_put(block);\r\n} else if (queue->active) {\r\niio_dma_buffer_submit_block(queue, block);\r\n} else {\r\nblock->state = IIO_BLOCK_STATE_QUEUED;\r\nlist_add_tail(&block->head, &queue->incoming);\r\n}\r\n}\r\nstatic struct iio_dma_buffer_block *iio_dma_buffer_dequeue(\r\nstruct iio_dma_buffer_queue *queue)\r\n{\r\nstruct iio_dma_buffer_block *block;\r\nspin_lock_irq(&queue->list_lock);\r\nblock = list_first_entry_or_null(&queue->outgoing, struct\r\niio_dma_buffer_block, head);\r\nif (block != NULL) {\r\nlist_del(&block->head);\r\nblock->state = IIO_BLOCK_STATE_DEQUEUED;\r\n}\r\nspin_unlock_irq(&queue->list_lock);\r\nreturn block;\r\n}\r\nint iio_dma_buffer_read(struct iio_buffer *buffer, size_t n,\r\nchar __user *user_buffer)\r\n{\r\nstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buffer);\r\nstruct iio_dma_buffer_block *block;\r\nint ret;\r\nif (n < buffer->bytes_per_datum)\r\nreturn -EINVAL;\r\nmutex_lock(&queue->lock);\r\nif (!queue->fileio.active_block) {\r\nblock = iio_dma_buffer_dequeue(queue);\r\nif (block == NULL) {\r\nret = 0;\r\ngoto out_unlock;\r\n}\r\nqueue->fileio.pos = 0;\r\nqueue->fileio.active_block = block;\r\n} else {\r\nblock = queue->fileio.active_block;\r\n}\r\nn = rounddown(n, buffer->bytes_per_datum);\r\nif (n > block->bytes_used - queue->fileio.pos)\r\nn = block->bytes_used - queue->fileio.pos;\r\nif (copy_to_user(user_buffer, block->vaddr + queue->fileio.pos, n)) {\r\nret = -EFAULT;\r\ngoto out_unlock;\r\n}\r\nqueue->fileio.pos += n;\r\nif (queue->fileio.pos == block->bytes_used) {\r\nqueue->fileio.active_block = NULL;\r\niio_dma_buffer_enqueue(queue, block);\r\n}\r\nret = n;\r\nout_unlock:\r\nmutex_unlock(&queue->lock);\r\nreturn ret;\r\n}\r\nsize_t iio_dma_buffer_data_available(struct iio_buffer *buf)\r\n{\r\nstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buf);\r\nstruct iio_dma_buffer_block *block;\r\nsize_t data_available = 0;\r\nmutex_lock(&queue->lock);\r\nif (queue->fileio.active_block)\r\ndata_available += queue->fileio.active_block->size;\r\nspin_lock_irq(&queue->list_lock);\r\nlist_for_each_entry(block, &queue->outgoing, head)\r\ndata_available += block->size;\r\nspin_unlock_irq(&queue->list_lock);\r\nmutex_unlock(&queue->lock);\r\nreturn data_available;\r\n}\r\nint iio_dma_buffer_set_bytes_per_datum(struct iio_buffer *buffer, size_t bpd)\r\n{\r\nbuffer->bytes_per_datum = bpd;\r\nreturn 0;\r\n}\r\nint iio_dma_buffer_set_length(struct iio_buffer *buffer, int length)\r\n{\r\nif (length < 2)\r\nlength = 2;\r\nbuffer->length = length;\r\nbuffer->watermark = length / 2;\r\nreturn 0;\r\n}\r\nint iio_dma_buffer_init(struct iio_dma_buffer_queue *queue,\r\nstruct device *dev, const struct iio_dma_buffer_ops *ops)\r\n{\r\niio_buffer_init(&queue->buffer);\r\nqueue->buffer.length = PAGE_SIZE;\r\nqueue->buffer.watermark = queue->buffer.length / 2;\r\nqueue->dev = dev;\r\nqueue->ops = ops;\r\nINIT_LIST_HEAD(&queue->incoming);\r\nINIT_LIST_HEAD(&queue->outgoing);\r\nmutex_init(&queue->lock);\r\nspin_lock_init(&queue->list_lock);\r\nreturn 0;\r\n}\r\nvoid iio_dma_buffer_exit(struct iio_dma_buffer_queue *queue)\r\n{\r\nunsigned int i;\r\nmutex_lock(&queue->lock);\r\nspin_lock_irq(&queue->list_lock);\r\nfor (i = 0; i < ARRAY_SIZE(queue->fileio.blocks); i++) {\r\nif (!queue->fileio.blocks[i])\r\ncontinue;\r\nqueue->fileio.blocks[i]->state = IIO_BLOCK_STATE_DEAD;\r\n}\r\nINIT_LIST_HEAD(&queue->outgoing);\r\nspin_unlock_irq(&queue->list_lock);\r\nINIT_LIST_HEAD(&queue->incoming);\r\nfor (i = 0; i < ARRAY_SIZE(queue->fileio.blocks); i++) {\r\nif (!queue->fileio.blocks[i])\r\ncontinue;\r\niio_buffer_block_put(queue->fileio.blocks[i]);\r\nqueue->fileio.blocks[i] = NULL;\r\n}\r\nqueue->fileio.active_block = NULL;\r\nqueue->ops = NULL;\r\nmutex_unlock(&queue->lock);\r\n}\r\nvoid iio_dma_buffer_release(struct iio_dma_buffer_queue *queue)\r\n{\r\nmutex_destroy(&queue->lock);\r\n}
