static void ipr_trc_hook(struct ipr_cmnd *ipr_cmd,\r\nu8 type, u32 add_data)\r\n{\r\nstruct ipr_trace_entry *trace_entry;\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nunsigned int trace_index;\r\ntrace_index = atomic_add_return(1, &ioa_cfg->trace_index) & IPR_TRACE_INDEX_MASK;\r\ntrace_entry = &ioa_cfg->trace[trace_index];\r\ntrace_entry->time = jiffies;\r\ntrace_entry->op_code = ipr_cmd->ioarcb.cmd_pkt.cdb[0];\r\ntrace_entry->type = type;\r\nif (ipr_cmd->ioa_cfg->sis64)\r\ntrace_entry->ata_op_code = ipr_cmd->i.ata_ioadl.regs.command;\r\nelse\r\ntrace_entry->ata_op_code = ipr_cmd->ioarcb.u.add_data.u.regs.command;\r\ntrace_entry->cmd_index = ipr_cmd->cmd_index & 0xff;\r\ntrace_entry->res_handle = ipr_cmd->ioarcb.res_handle;\r\ntrace_entry->u.add_data = add_data;\r\nwmb();\r\n}\r\nstatic void ipr_lock_and_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nunsigned long lock_flags;\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nipr_cmd->done(ipr_cmd);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\nstatic void ipr_reinit_ipr_cmnd(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\r\nstruct ipr_ioasa64 *ioasa64 = &ipr_cmd->s.ioasa64;\r\ndma_addr_t dma_addr = ipr_cmd->dma_addr;\r\nint hrrq_id;\r\nhrrq_id = ioarcb->cmd_pkt.hrrq_id;\r\nmemset(&ioarcb->cmd_pkt, 0, sizeof(struct ipr_cmd_pkt));\r\nioarcb->cmd_pkt.hrrq_id = hrrq_id;\r\nioarcb->data_transfer_length = 0;\r\nioarcb->read_data_transfer_length = 0;\r\nioarcb->ioadl_len = 0;\r\nioarcb->read_ioadl_len = 0;\r\nif (ipr_cmd->ioa_cfg->sis64) {\r\nioarcb->u.sis64_addr_data.data_ioadl_addr =\r\ncpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, i.ioadl64));\r\nioasa64->u.gata.status = 0;\r\n} else {\r\nioarcb->write_ioadl_addr =\r\ncpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, i.ioadl));\r\nioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;\r\nioasa->u.gata.status = 0;\r\n}\r\nioasa->hdr.ioasc = 0;\r\nioasa->hdr.residual_data_len = 0;\r\nipr_cmd->scsi_cmd = NULL;\r\nipr_cmd->qc = NULL;\r\nipr_cmd->sense_buffer[0] = 0;\r\nipr_cmd->dma_use_sg = 0;\r\n}\r\nstatic void ipr_init_ipr_cmnd(struct ipr_cmnd *ipr_cmd,\r\nvoid (*fast_done) (struct ipr_cmnd *))\r\n{\r\nipr_reinit_ipr_cmnd(ipr_cmd);\r\nipr_cmd->u.scratch = 0;\r\nipr_cmd->sibling = NULL;\r\nipr_cmd->eh_comp = NULL;\r\nipr_cmd->fast_done = fast_done;\r\ninit_timer(&ipr_cmd->timer);\r\n}\r\nstatic\r\nstruct ipr_cmnd *__ipr_get_free_ipr_cmnd(struct ipr_hrr_queue *hrrq)\r\n{\r\nstruct ipr_cmnd *ipr_cmd = NULL;\r\nif (likely(!list_empty(&hrrq->hrrq_free_q))) {\r\nipr_cmd = list_entry(hrrq->hrrq_free_q.next,\r\nstruct ipr_cmnd, queue);\r\nlist_del(&ipr_cmd->queue);\r\n}\r\nreturn ipr_cmd;\r\n}\r\nstatic\r\nstruct ipr_cmnd *ipr_get_free_ipr_cmnd(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct ipr_cmnd *ipr_cmd =\r\n__ipr_get_free_ipr_cmnd(&ioa_cfg->hrrq[IPR_INIT_HRRQ]);\r\nipr_init_ipr_cmnd(ipr_cmd, ipr_lock_and_done);\r\nreturn ipr_cmd;\r\n}\r\nstatic void ipr_mask_and_clear_interrupts(struct ipr_ioa_cfg *ioa_cfg,\r\nu32 clr_ints)\r\n{\r\nvolatile u32 int_reg;\r\nint i;\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nspin_lock(&ioa_cfg->hrrq[i]._lock);\r\nioa_cfg->hrrq[i].allow_interrupts = 0;\r\nspin_unlock(&ioa_cfg->hrrq[i]._lock);\r\n}\r\nwmb();\r\nif (ioa_cfg->sis64)\r\nwriteq(~0, ioa_cfg->regs.set_interrupt_mask_reg);\r\nelse\r\nwritel(~0, ioa_cfg->regs.set_interrupt_mask_reg);\r\nif (ioa_cfg->sis64)\r\nwritel(~0, ioa_cfg->regs.clr_interrupt_reg);\r\nwritel(clr_ints, ioa_cfg->regs.clr_interrupt_reg32);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg);\r\n}\r\nstatic int ipr_save_pcix_cmd_reg(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nint pcix_cmd_reg = pci_find_capability(ioa_cfg->pdev, PCI_CAP_ID_PCIX);\r\nif (pcix_cmd_reg == 0)\r\nreturn 0;\r\nif (pci_read_config_word(ioa_cfg->pdev, pcix_cmd_reg + PCI_X_CMD,\r\n&ioa_cfg->saved_pcix_cmd_reg) != PCIBIOS_SUCCESSFUL) {\r\ndev_err(&ioa_cfg->pdev->dev, "Failed to save PCI-X command register\n");\r\nreturn -EIO;\r\n}\r\nioa_cfg->saved_pcix_cmd_reg |= PCI_X_CMD_DPERR_E | PCI_X_CMD_ERO;\r\nreturn 0;\r\n}\r\nstatic int ipr_set_pcix_cmd_reg(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nint pcix_cmd_reg = pci_find_capability(ioa_cfg->pdev, PCI_CAP_ID_PCIX);\r\nif (pcix_cmd_reg) {\r\nif (pci_write_config_word(ioa_cfg->pdev, pcix_cmd_reg + PCI_X_CMD,\r\nioa_cfg->saved_pcix_cmd_reg) != PCIBIOS_SUCCESSFUL) {\r\ndev_err(&ioa_cfg->pdev->dev, "Failed to setup PCI-X command register\n");\r\nreturn -EIO;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void __ipr_sata_eh_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ata_queued_cmd *qc = ipr_cmd->qc;\r\nstruct ipr_sata_port *sata_port = qc->ap->private_data;\r\nqc->err_mask |= AC_ERR_OTHER;\r\nsata_port->ioasa.status |= ATA_BUSY;\r\nata_qc_complete(qc);\r\nif (ipr_cmd->eh_comp)\r\ncomplete(ipr_cmd->eh_comp);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\n}\r\nstatic void ipr_sata_eh_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_hrr_queue *hrrq = ipr_cmd->hrrq;\r\nunsigned long hrrq_flags;\r\nspin_lock_irqsave(&hrrq->_lock, hrrq_flags);\r\n__ipr_sata_eh_done(ipr_cmd);\r\nspin_unlock_irqrestore(&hrrq->_lock, hrrq_flags);\r\n}\r\nstatic void __ipr_scsi_eh_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\r\nscsi_cmd->result |= (DID_ERROR << 16);\r\nscsi_dma_unmap(ipr_cmd->scsi_cmd);\r\nscsi_cmd->scsi_done(scsi_cmd);\r\nif (ipr_cmd->eh_comp)\r\ncomplete(ipr_cmd->eh_comp);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\n}\r\nstatic void ipr_scsi_eh_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nunsigned long hrrq_flags;\r\nstruct ipr_hrr_queue *hrrq = ipr_cmd->hrrq;\r\nspin_lock_irqsave(&hrrq->_lock, hrrq_flags);\r\n__ipr_scsi_eh_done(ipr_cmd);\r\nspin_unlock_irqrestore(&hrrq->_lock, hrrq_flags);\r\n}\r\nstatic void ipr_fail_all_ops(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct ipr_cmnd *ipr_cmd, *temp;\r\nstruct ipr_hrr_queue *hrrq;\r\nENTER;\r\nfor_each_hrrq(hrrq, ioa_cfg) {\r\nspin_lock(&hrrq->_lock);\r\nlist_for_each_entry_safe(ipr_cmd,\r\ntemp, &hrrq->hrrq_pending_q, queue) {\r\nlist_del(&ipr_cmd->queue);\r\nipr_cmd->s.ioasa.hdr.ioasc =\r\ncpu_to_be32(IPR_IOASC_IOA_WAS_RESET);\r\nipr_cmd->s.ioasa.hdr.ilid =\r\ncpu_to_be32(IPR_DRIVER_ILID);\r\nif (ipr_cmd->scsi_cmd)\r\nipr_cmd->done = __ipr_scsi_eh_done;\r\nelse if (ipr_cmd->qc)\r\nipr_cmd->done = __ipr_sata_eh_done;\r\nipr_trc_hook(ipr_cmd, IPR_TRACE_FINISH,\r\nIPR_IOASC_IOA_WAS_RESET);\r\ndel_timer(&ipr_cmd->timer);\r\nipr_cmd->done(ipr_cmd);\r\n}\r\nspin_unlock(&hrrq->_lock);\r\n}\r\nLEAVE;\r\n}\r\nstatic void ipr_send_command(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\ndma_addr_t send_dma_addr = ipr_cmd->dma_addr;\r\nif (ioa_cfg->sis64) {\r\nsend_dma_addr |= 0x1;\r\nif (ipr_cmd->dma_use_sg * sizeof(struct ipr_ioadl64_desc) > 128 )\r\nsend_dma_addr |= 0x4;\r\nwriteq(send_dma_addr, ioa_cfg->regs.ioarrin_reg);\r\n} else\r\nwritel(send_dma_addr, ioa_cfg->regs.ioarrin_reg);\r\n}\r\nstatic void ipr_do_req(struct ipr_cmnd *ipr_cmd,\r\nvoid (*done) (struct ipr_cmnd *),\r\nvoid (*timeout_func) (struct ipr_cmnd *), u32 timeout)\r\n{\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\r\nipr_cmd->done = done;\r\nipr_cmd->timer.data = (unsigned long) ipr_cmd;\r\nipr_cmd->timer.expires = jiffies + timeout;\r\nipr_cmd->timer.function = (void (*)(unsigned long))timeout_func;\r\nadd_timer(&ipr_cmd->timer);\r\nipr_trc_hook(ipr_cmd, IPR_TRACE_START, 0);\r\nipr_send_command(ipr_cmd);\r\n}\r\nstatic void ipr_internal_cmd_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nif (ipr_cmd->sibling)\r\nipr_cmd->sibling = NULL;\r\nelse\r\ncomplete(&ipr_cmd->completion);\r\n}\r\nstatic void ipr_init_ioadl(struct ipr_cmnd *ipr_cmd, dma_addr_t dma_addr,\r\nu32 len, int flags)\r\n{\r\nstruct ipr_ioadl_desc *ioadl = ipr_cmd->i.ioadl;\r\nstruct ipr_ioadl64_desc *ioadl64 = ipr_cmd->i.ioadl64;\r\nipr_cmd->dma_use_sg = 1;\r\nif (ipr_cmd->ioa_cfg->sis64) {\r\nioadl64->flags = cpu_to_be32(flags);\r\nioadl64->data_len = cpu_to_be32(len);\r\nioadl64->address = cpu_to_be64(dma_addr);\r\nipr_cmd->ioarcb.ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl64_desc));\r\nipr_cmd->ioarcb.data_transfer_length = cpu_to_be32(len);\r\n} else {\r\nioadl->flags_and_data_len = cpu_to_be32(flags | len);\r\nioadl->address = cpu_to_be32(dma_addr);\r\nif (flags == IPR_IOADL_FLAGS_READ_LAST) {\r\nipr_cmd->ioarcb.read_ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl_desc));\r\nipr_cmd->ioarcb.read_data_transfer_length = cpu_to_be32(len);\r\n} else {\r\nipr_cmd->ioarcb.ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl_desc));\r\nipr_cmd->ioarcb.data_transfer_length = cpu_to_be32(len);\r\n}\r\n}\r\n}\r\nstatic void ipr_send_blocking_cmd(struct ipr_cmnd *ipr_cmd,\r\nvoid (*timeout_func) (struct ipr_cmnd *ipr_cmd),\r\nu32 timeout)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\ninit_completion(&ipr_cmd->completion);\r\nipr_do_req(ipr_cmd, ipr_internal_cmd_done, timeout_func, timeout);\r\nspin_unlock_irq(ioa_cfg->host->host_lock);\r\nwait_for_completion(&ipr_cmd->completion);\r\nspin_lock_irq(ioa_cfg->host->host_lock);\r\n}\r\nstatic int ipr_get_hrrq_index(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nunsigned int hrrq;\r\nif (ioa_cfg->hrrq_num == 1)\r\nhrrq = 0;\r\nelse {\r\nhrrq = atomic_add_return(1, &ioa_cfg->hrrq_index);\r\nhrrq = (hrrq % (ioa_cfg->hrrq_num - 1)) + 1;\r\n}\r\nreturn hrrq;\r\n}\r\nstatic void ipr_send_hcam(struct ipr_ioa_cfg *ioa_cfg, u8 type,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_ioarcb *ioarcb;\r\nif (ioa_cfg->hrrq[IPR_INIT_HRRQ].allow_cmds) {\r\nipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\r\nlist_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_pending_q);\r\nipr_cmd->u.hostrcb = hostrcb;\r\nioarcb = &ipr_cmd->ioarcb;\r\nioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_HCAM;\r\nioarcb->cmd_pkt.cdb[0] = IPR_HOST_CONTROLLED_ASYNC;\r\nioarcb->cmd_pkt.cdb[1] = type;\r\nioarcb->cmd_pkt.cdb[7] = (sizeof(hostrcb->hcam) >> 8) & 0xff;\r\nioarcb->cmd_pkt.cdb[8] = sizeof(hostrcb->hcam) & 0xff;\r\nipr_init_ioadl(ipr_cmd, hostrcb->hostrcb_dma,\r\nsizeof(hostrcb->hcam), IPR_IOADL_FLAGS_READ_LAST);\r\nif (type == IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE)\r\nipr_cmd->done = ipr_process_ccn;\r\nelse\r\nipr_cmd->done = ipr_process_error;\r\nipr_trc_hook(ipr_cmd, IPR_TRACE_START, IPR_IOA_RES_ADDR);\r\nipr_send_command(ipr_cmd);\r\n} else {\r\nlist_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);\r\n}\r\n}\r\nstatic void ipr_update_ata_class(struct ipr_resource_entry *res, unsigned int proto)\r\n{\r\nswitch (proto) {\r\ncase IPR_PROTO_SATA:\r\ncase IPR_PROTO_SAS_STP:\r\nres->ata_class = ATA_DEV_ATA;\r\nbreak;\r\ncase IPR_PROTO_SATA_ATAPI:\r\ncase IPR_PROTO_SAS_STP_ATAPI:\r\nres->ata_class = ATA_DEV_ATAPI;\r\nbreak;\r\ndefault:\r\nres->ata_class = ATA_DEV_UNKNOWN;\r\nbreak;\r\n};\r\n}\r\nstatic void ipr_init_res_entry(struct ipr_resource_entry *res,\r\nstruct ipr_config_table_entry_wrapper *cfgtew)\r\n{\r\nint found = 0;\r\nunsigned int proto;\r\nstruct ipr_ioa_cfg *ioa_cfg = res->ioa_cfg;\r\nstruct ipr_resource_entry *gscsi_res = NULL;\r\nres->needs_sync_complete = 0;\r\nres->in_erp = 0;\r\nres->add_to_ml = 0;\r\nres->del_from_ml = 0;\r\nres->resetting_device = 0;\r\nres->reset_occurred = 0;\r\nres->sdev = NULL;\r\nres->sata_port = NULL;\r\nif (ioa_cfg->sis64) {\r\nproto = cfgtew->u.cfgte64->proto;\r\nres->flags = be16_to_cpu(cfgtew->u.cfgte64->flags);\r\nres->res_flags = be16_to_cpu(cfgtew->u.cfgte64->res_flags);\r\nres->qmodel = IPR_QUEUEING_MODEL64(res);\r\nres->type = cfgtew->u.cfgte64->res_type;\r\nmemcpy(res->res_path, &cfgtew->u.cfgte64->res_path,\r\nsizeof(res->res_path));\r\nres->bus = 0;\r\nmemcpy(&res->dev_lun.scsi_lun, &cfgtew->u.cfgte64->lun,\r\nsizeof(res->dev_lun.scsi_lun));\r\nres->lun = scsilun_to_int(&res->dev_lun);\r\nif (res->type == IPR_RES_TYPE_GENERIC_SCSI) {\r\nlist_for_each_entry(gscsi_res, &ioa_cfg->used_res_q, queue) {\r\nif (gscsi_res->dev_id == cfgtew->u.cfgte64->dev_id) {\r\nfound = 1;\r\nres->target = gscsi_res->target;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nres->target = find_first_zero_bit(ioa_cfg->target_ids,\r\nioa_cfg->max_devs_supported);\r\nset_bit(res->target, ioa_cfg->target_ids);\r\n}\r\n} else if (res->type == IPR_RES_TYPE_IOAFP) {\r\nres->bus = IPR_IOAFP_VIRTUAL_BUS;\r\nres->target = 0;\r\n} else if (res->type == IPR_RES_TYPE_ARRAY) {\r\nres->bus = IPR_ARRAY_VIRTUAL_BUS;\r\nres->target = find_first_zero_bit(ioa_cfg->array_ids,\r\nioa_cfg->max_devs_supported);\r\nset_bit(res->target, ioa_cfg->array_ids);\r\n} else if (res->type == IPR_RES_TYPE_VOLUME_SET) {\r\nres->bus = IPR_VSET_VIRTUAL_BUS;\r\nres->target = find_first_zero_bit(ioa_cfg->vset_ids,\r\nioa_cfg->max_devs_supported);\r\nset_bit(res->target, ioa_cfg->vset_ids);\r\n} else {\r\nres->target = find_first_zero_bit(ioa_cfg->target_ids,\r\nioa_cfg->max_devs_supported);\r\nset_bit(res->target, ioa_cfg->target_ids);\r\n}\r\n} else {\r\nproto = cfgtew->u.cfgte->proto;\r\nres->qmodel = IPR_QUEUEING_MODEL(res);\r\nres->flags = cfgtew->u.cfgte->flags;\r\nif (res->flags & IPR_IS_IOA_RESOURCE)\r\nres->type = IPR_RES_TYPE_IOAFP;\r\nelse\r\nres->type = cfgtew->u.cfgte->rsvd_subtype & 0x0f;\r\nres->bus = cfgtew->u.cfgte->res_addr.bus;\r\nres->target = cfgtew->u.cfgte->res_addr.target;\r\nres->lun = cfgtew->u.cfgte->res_addr.lun;\r\nres->lun_wwn = get_unaligned_be64(cfgtew->u.cfgte->lun_wwn);\r\n}\r\nipr_update_ata_class(res, proto);\r\n}\r\nstatic int ipr_is_same_device(struct ipr_resource_entry *res,\r\nstruct ipr_config_table_entry_wrapper *cfgtew)\r\n{\r\nif (res->ioa_cfg->sis64) {\r\nif (!memcmp(&res->dev_id, &cfgtew->u.cfgte64->dev_id,\r\nsizeof(cfgtew->u.cfgte64->dev_id)) &&\r\n!memcmp(&res->dev_lun.scsi_lun, &cfgtew->u.cfgte64->lun,\r\nsizeof(cfgtew->u.cfgte64->lun))) {\r\nreturn 1;\r\n}\r\n} else {\r\nif (res->bus == cfgtew->u.cfgte->res_addr.bus &&\r\nres->target == cfgtew->u.cfgte->res_addr.target &&\r\nres->lun == cfgtew->u.cfgte->res_addr.lun)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic char *__ipr_format_res_path(u8 *res_path, char *buffer, int len)\r\n{\r\nint i;\r\nchar *p = buffer;\r\n*p = '\0';\r\np += snprintf(p, buffer + len - p, "%02X", res_path[0]);\r\nfor (i = 1; res_path[i] != 0xff && ((i * 3) < len); i++)\r\np += snprintf(p, buffer + len - p, "-%02X", res_path[i]);\r\nreturn buffer;\r\n}\r\nstatic char *ipr_format_res_path(struct ipr_ioa_cfg *ioa_cfg,\r\nu8 *res_path, char *buffer, int len)\r\n{\r\nchar *p = buffer;\r\n*p = '\0';\r\np += snprintf(p, buffer + len - p, "%d/", ioa_cfg->host->host_no);\r\n__ipr_format_res_path(res_path, p, len - (buffer - p));\r\nreturn buffer;\r\n}\r\nstatic void ipr_update_res_entry(struct ipr_resource_entry *res,\r\nstruct ipr_config_table_entry_wrapper *cfgtew)\r\n{\r\nchar buffer[IPR_MAX_RES_PATH_LENGTH];\r\nunsigned int proto;\r\nint new_path = 0;\r\nif (res->ioa_cfg->sis64) {\r\nres->flags = be16_to_cpu(cfgtew->u.cfgte64->flags);\r\nres->res_flags = be16_to_cpu(cfgtew->u.cfgte64->res_flags);\r\nres->type = cfgtew->u.cfgte64->res_type;\r\nmemcpy(&res->std_inq_data, &cfgtew->u.cfgte64->std_inq_data,\r\nsizeof(struct ipr_std_inq_data));\r\nres->qmodel = IPR_QUEUEING_MODEL64(res);\r\nproto = cfgtew->u.cfgte64->proto;\r\nres->res_handle = cfgtew->u.cfgte64->res_handle;\r\nres->dev_id = cfgtew->u.cfgte64->dev_id;\r\nmemcpy(&res->dev_lun.scsi_lun, &cfgtew->u.cfgte64->lun,\r\nsizeof(res->dev_lun.scsi_lun));\r\nif (memcmp(res->res_path, &cfgtew->u.cfgte64->res_path,\r\nsizeof(res->res_path))) {\r\nmemcpy(res->res_path, &cfgtew->u.cfgte64->res_path,\r\nsizeof(res->res_path));\r\nnew_path = 1;\r\n}\r\nif (res->sdev && new_path)\r\nsdev_printk(KERN_INFO, res->sdev, "Resource path: %s\n",\r\nipr_format_res_path(res->ioa_cfg,\r\nres->res_path, buffer, sizeof(buffer)));\r\n} else {\r\nres->flags = cfgtew->u.cfgte->flags;\r\nif (res->flags & IPR_IS_IOA_RESOURCE)\r\nres->type = IPR_RES_TYPE_IOAFP;\r\nelse\r\nres->type = cfgtew->u.cfgte->rsvd_subtype & 0x0f;\r\nmemcpy(&res->std_inq_data, &cfgtew->u.cfgte->std_inq_data,\r\nsizeof(struct ipr_std_inq_data));\r\nres->qmodel = IPR_QUEUEING_MODEL(res);\r\nproto = cfgtew->u.cfgte->proto;\r\nres->res_handle = cfgtew->u.cfgte->res_handle;\r\n}\r\nipr_update_ata_class(res, proto);\r\n}\r\nstatic void ipr_clear_res_target(struct ipr_resource_entry *res)\r\n{\r\nstruct ipr_resource_entry *gscsi_res = NULL;\r\nstruct ipr_ioa_cfg *ioa_cfg = res->ioa_cfg;\r\nif (!ioa_cfg->sis64)\r\nreturn;\r\nif (res->bus == IPR_ARRAY_VIRTUAL_BUS)\r\nclear_bit(res->target, ioa_cfg->array_ids);\r\nelse if (res->bus == IPR_VSET_VIRTUAL_BUS)\r\nclear_bit(res->target, ioa_cfg->vset_ids);\r\nelse if (res->bus == 0 && res->type == IPR_RES_TYPE_GENERIC_SCSI) {\r\nlist_for_each_entry(gscsi_res, &ioa_cfg->used_res_q, queue)\r\nif (gscsi_res->dev_id == res->dev_id && gscsi_res != res)\r\nreturn;\r\nclear_bit(res->target, ioa_cfg->target_ids);\r\n} else if (res->bus == 0)\r\nclear_bit(res->target, ioa_cfg->target_ids);\r\n}\r\nstatic void ipr_handle_config_change(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nstruct ipr_resource_entry *res = NULL;\r\nstruct ipr_config_table_entry_wrapper cfgtew;\r\n__be32 cc_res_handle;\r\nu32 is_ndn = 1;\r\nif (ioa_cfg->sis64) {\r\ncfgtew.u.cfgte64 = &hostrcb->hcam.u.ccn.u.cfgte64;\r\ncc_res_handle = cfgtew.u.cfgte64->res_handle;\r\n} else {\r\ncfgtew.u.cfgte = &hostrcb->hcam.u.ccn.u.cfgte;\r\ncc_res_handle = cfgtew.u.cfgte->res_handle;\r\n}\r\nlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\r\nif (res->res_handle == cc_res_handle) {\r\nis_ndn = 0;\r\nbreak;\r\n}\r\n}\r\nif (is_ndn) {\r\nif (list_empty(&ioa_cfg->free_res_q)) {\r\nipr_send_hcam(ioa_cfg,\r\nIPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE,\r\nhostrcb);\r\nreturn;\r\n}\r\nres = list_entry(ioa_cfg->free_res_q.next,\r\nstruct ipr_resource_entry, queue);\r\nlist_del(&res->queue);\r\nipr_init_res_entry(res, &cfgtew);\r\nlist_add_tail(&res->queue, &ioa_cfg->used_res_q);\r\n}\r\nipr_update_res_entry(res, &cfgtew);\r\nif (hostrcb->hcam.notify_type == IPR_HOST_RCB_NOTIF_TYPE_REM_ENTRY) {\r\nif (res->sdev) {\r\nres->del_from_ml = 1;\r\nres->res_handle = IPR_INVALID_RES_HANDLE;\r\nschedule_work(&ioa_cfg->work_q);\r\n} else {\r\nipr_clear_res_target(res);\r\nlist_move_tail(&res->queue, &ioa_cfg->free_res_q);\r\n}\r\n} else if (!res->sdev || res->del_from_ml) {\r\nres->add_to_ml = 1;\r\nschedule_work(&ioa_cfg->work_q);\r\n}\r\nipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);\r\n}\r\nstatic void ipr_process_ccn(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_hostrcb *hostrcb = ipr_cmd->u.hostrcb;\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nlist_del_init(&hostrcb->queue);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nif (ioasc) {\r\nif (ioasc != IPR_IOASC_IOA_WAS_RESET &&\r\nioasc != IPR_IOASC_ABORTED_CMD_TERM_BY_HOST)\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Host RCB failed with IOASC: 0x%08X\n", ioasc);\r\nipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);\r\n} else {\r\nipr_handle_config_change(ioa_cfg, hostrcb);\r\n}\r\n}\r\nstatic int strip_and_pad_whitespace(int i, char *buf)\r\n{\r\nwhile (i && buf[i] == ' ')\r\ni--;\r\nbuf[i+1] = ' ';\r\nbuf[i+2] = '\0';\r\nreturn i + 2;\r\n}\r\nstatic void ipr_log_vpd_compact(char *prefix, struct ipr_hostrcb *hostrcb,\r\nstruct ipr_vpd *vpd)\r\n{\r\nchar buffer[IPR_VENDOR_ID_LEN + IPR_PROD_ID_LEN + IPR_SERIAL_NUM_LEN + 3];\r\nint i = 0;\r\nmemcpy(buffer, vpd->vpids.vendor_id, IPR_VENDOR_ID_LEN);\r\ni = strip_and_pad_whitespace(IPR_VENDOR_ID_LEN - 1, buffer);\r\nmemcpy(&buffer[i], vpd->vpids.product_id, IPR_PROD_ID_LEN);\r\ni = strip_and_pad_whitespace(i + IPR_PROD_ID_LEN - 1, buffer);\r\nmemcpy(&buffer[i], vpd->sn, IPR_SERIAL_NUM_LEN);\r\nbuffer[IPR_SERIAL_NUM_LEN + i] = '\0';\r\nipr_hcam_err(hostrcb, "%s VPID/SN: %s\n", prefix, buffer);\r\n}\r\nstatic void ipr_log_vpd(struct ipr_vpd *vpd)\r\n{\r\nchar buffer[IPR_VENDOR_ID_LEN + IPR_PROD_ID_LEN\r\n+ IPR_SERIAL_NUM_LEN];\r\nmemcpy(buffer, vpd->vpids.vendor_id, IPR_VENDOR_ID_LEN);\r\nmemcpy(buffer + IPR_VENDOR_ID_LEN, vpd->vpids.product_id,\r\nIPR_PROD_ID_LEN);\r\nbuffer[IPR_VENDOR_ID_LEN + IPR_PROD_ID_LEN] = '\0';\r\nipr_err("Vendor/Product ID: %s\n", buffer);\r\nmemcpy(buffer, vpd->sn, IPR_SERIAL_NUM_LEN);\r\nbuffer[IPR_SERIAL_NUM_LEN] = '\0';\r\nipr_err(" Serial Number: %s\n", buffer);\r\n}\r\nstatic void ipr_log_ext_vpd_compact(char *prefix, struct ipr_hostrcb *hostrcb,\r\nstruct ipr_ext_vpd *vpd)\r\n{\r\nipr_log_vpd_compact(prefix, hostrcb, &vpd->vpd);\r\nipr_hcam_err(hostrcb, "%s WWN: %08X%08X\n", prefix,\r\nbe32_to_cpu(vpd->wwid[0]), be32_to_cpu(vpd->wwid[1]));\r\n}\r\nstatic void ipr_log_ext_vpd(struct ipr_ext_vpd *vpd)\r\n{\r\nipr_log_vpd(&vpd->vpd);\r\nipr_err(" WWN: %08X%08X\n", be32_to_cpu(vpd->wwid[0]),\r\nbe32_to_cpu(vpd->wwid[1]));\r\n}\r\nstatic void ipr_log_enhanced_cache_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nstruct ipr_hostrcb_type_12_error *error;\r\nif (ioa_cfg->sis64)\r\nerror = &hostrcb->hcam.u.error64.u.type_12_error;\r\nelse\r\nerror = &hostrcb->hcam.u.error.u.type_12_error;\r\nipr_err("-----Current Configuration-----\n");\r\nipr_err("Cache Directory Card Information:\n");\r\nipr_log_ext_vpd(&error->ioa_vpd);\r\nipr_err("Adapter Card Information:\n");\r\nipr_log_ext_vpd(&error->cfc_vpd);\r\nipr_err("-----Expected Configuration-----\n");\r\nipr_err("Cache Directory Card Information:\n");\r\nipr_log_ext_vpd(&error->ioa_last_attached_to_cfc_vpd);\r\nipr_err("Adapter Card Information:\n");\r\nipr_log_ext_vpd(&error->cfc_last_attached_to_ioa_vpd);\r\nipr_err("Additional IOA Data: %08X %08X %08X\n",\r\nbe32_to_cpu(error->ioa_data[0]),\r\nbe32_to_cpu(error->ioa_data[1]),\r\nbe32_to_cpu(error->ioa_data[2]));\r\n}\r\nstatic void ipr_log_cache_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nstruct ipr_hostrcb_type_02_error *error =\r\n&hostrcb->hcam.u.error.u.type_02_error;\r\nipr_err("-----Current Configuration-----\n");\r\nipr_err("Cache Directory Card Information:\n");\r\nipr_log_vpd(&error->ioa_vpd);\r\nipr_err("Adapter Card Information:\n");\r\nipr_log_vpd(&error->cfc_vpd);\r\nipr_err("-----Expected Configuration-----\n");\r\nipr_err("Cache Directory Card Information:\n");\r\nipr_log_vpd(&error->ioa_last_attached_to_cfc_vpd);\r\nipr_err("Adapter Card Information:\n");\r\nipr_log_vpd(&error->cfc_last_attached_to_ioa_vpd);\r\nipr_err("Additional IOA Data: %08X %08X %08X\n",\r\nbe32_to_cpu(error->ioa_data[0]),\r\nbe32_to_cpu(error->ioa_data[1]),\r\nbe32_to_cpu(error->ioa_data[2]));\r\n}\r\nstatic void ipr_log_enhanced_config_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nint errors_logged, i;\r\nstruct ipr_hostrcb_device_data_entry_enhanced *dev_entry;\r\nstruct ipr_hostrcb_type_13_error *error;\r\nerror = &hostrcb->hcam.u.error.u.type_13_error;\r\nerrors_logged = be32_to_cpu(error->errors_logged);\r\nipr_err("Device Errors Detected/Logged: %d/%d\n",\r\nbe32_to_cpu(error->errors_detected), errors_logged);\r\ndev_entry = error->dev;\r\nfor (i = 0; i < errors_logged; i++, dev_entry++) {\r\nipr_err_separator;\r\nipr_phys_res_err(ioa_cfg, dev_entry->dev_res_addr, "Device %d", i + 1);\r\nipr_log_ext_vpd(&dev_entry->vpd);\r\nipr_err("-----New Device Information-----\n");\r\nipr_log_ext_vpd(&dev_entry->new_vpd);\r\nipr_err("Cache Directory Card Information:\n");\r\nipr_log_ext_vpd(&dev_entry->ioa_last_with_dev_vpd);\r\nipr_err("Adapter Card Information:\n");\r\nipr_log_ext_vpd(&dev_entry->cfc_last_with_dev_vpd);\r\n}\r\n}\r\nstatic void ipr_log_sis64_config_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nint errors_logged, i;\r\nstruct ipr_hostrcb64_device_data_entry_enhanced *dev_entry;\r\nstruct ipr_hostrcb_type_23_error *error;\r\nchar buffer[IPR_MAX_RES_PATH_LENGTH];\r\nerror = &hostrcb->hcam.u.error64.u.type_23_error;\r\nerrors_logged = be32_to_cpu(error->errors_logged);\r\nipr_err("Device Errors Detected/Logged: %d/%d\n",\r\nbe32_to_cpu(error->errors_detected), errors_logged);\r\ndev_entry = error->dev;\r\nfor (i = 0; i < errors_logged; i++, dev_entry++) {\r\nipr_err_separator;\r\nipr_err("Device %d : %s", i + 1,\r\n__ipr_format_res_path(dev_entry->res_path,\r\nbuffer, sizeof(buffer)));\r\nipr_log_ext_vpd(&dev_entry->vpd);\r\nipr_err("-----New Device Information-----\n");\r\nipr_log_ext_vpd(&dev_entry->new_vpd);\r\nipr_err("Cache Directory Card Information:\n");\r\nipr_log_ext_vpd(&dev_entry->ioa_last_with_dev_vpd);\r\nipr_err("Adapter Card Information:\n");\r\nipr_log_ext_vpd(&dev_entry->cfc_last_with_dev_vpd);\r\n}\r\n}\r\nstatic void ipr_log_config_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nint errors_logged, i;\r\nstruct ipr_hostrcb_device_data_entry *dev_entry;\r\nstruct ipr_hostrcb_type_03_error *error;\r\nerror = &hostrcb->hcam.u.error.u.type_03_error;\r\nerrors_logged = be32_to_cpu(error->errors_logged);\r\nipr_err("Device Errors Detected/Logged: %d/%d\n",\r\nbe32_to_cpu(error->errors_detected), errors_logged);\r\ndev_entry = error->dev;\r\nfor (i = 0; i < errors_logged; i++, dev_entry++) {\r\nipr_err_separator;\r\nipr_phys_res_err(ioa_cfg, dev_entry->dev_res_addr, "Device %d", i + 1);\r\nipr_log_vpd(&dev_entry->vpd);\r\nipr_err("-----New Device Information-----\n");\r\nipr_log_vpd(&dev_entry->new_vpd);\r\nipr_err("Cache Directory Card Information:\n");\r\nipr_log_vpd(&dev_entry->ioa_last_with_dev_vpd);\r\nipr_err("Adapter Card Information:\n");\r\nipr_log_vpd(&dev_entry->cfc_last_with_dev_vpd);\r\nipr_err("Additional IOA Data: %08X %08X %08X %08X %08X\n",\r\nbe32_to_cpu(dev_entry->ioa_data[0]),\r\nbe32_to_cpu(dev_entry->ioa_data[1]),\r\nbe32_to_cpu(dev_entry->ioa_data[2]),\r\nbe32_to_cpu(dev_entry->ioa_data[3]),\r\nbe32_to_cpu(dev_entry->ioa_data[4]));\r\n}\r\n}\r\nstatic void ipr_log_enhanced_array_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nint i, num_entries;\r\nstruct ipr_hostrcb_type_14_error *error;\r\nstruct ipr_hostrcb_array_data_entry_enhanced *array_entry;\r\nconst u8 zero_sn[IPR_SERIAL_NUM_LEN] = { [0 ... IPR_SERIAL_NUM_LEN-1] = '0' };\r\nerror = &hostrcb->hcam.u.error.u.type_14_error;\r\nipr_err_separator;\r\nipr_err("RAID %s Array Configuration: %d:%d:%d:%d\n",\r\nerror->protection_level,\r\nioa_cfg->host->host_no,\r\nerror->last_func_vset_res_addr.bus,\r\nerror->last_func_vset_res_addr.target,\r\nerror->last_func_vset_res_addr.lun);\r\nipr_err_separator;\r\narray_entry = error->array_member;\r\nnum_entries = min_t(u32, be32_to_cpu(error->num_entries),\r\nARRAY_SIZE(error->array_member));\r\nfor (i = 0; i < num_entries; i++, array_entry++) {\r\nif (!memcmp(array_entry->vpd.vpd.sn, zero_sn, IPR_SERIAL_NUM_LEN))\r\ncontinue;\r\nif (be32_to_cpu(error->exposed_mode_adn) == i)\r\nipr_err("Exposed Array Member %d:\n", i);\r\nelse\r\nipr_err("Array Member %d:\n", i);\r\nipr_log_ext_vpd(&array_entry->vpd);\r\nipr_phys_res_err(ioa_cfg, array_entry->dev_res_addr, "Current Location");\r\nipr_phys_res_err(ioa_cfg, array_entry->expected_dev_res_addr,\r\n"Expected Location");\r\nipr_err_separator;\r\n}\r\n}\r\nstatic void ipr_log_array_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nint i;\r\nstruct ipr_hostrcb_type_04_error *error;\r\nstruct ipr_hostrcb_array_data_entry *array_entry;\r\nconst u8 zero_sn[IPR_SERIAL_NUM_LEN] = { [0 ... IPR_SERIAL_NUM_LEN-1] = '0' };\r\nerror = &hostrcb->hcam.u.error.u.type_04_error;\r\nipr_err_separator;\r\nipr_err("RAID %s Array Configuration: %d:%d:%d:%d\n",\r\nerror->protection_level,\r\nioa_cfg->host->host_no,\r\nerror->last_func_vset_res_addr.bus,\r\nerror->last_func_vset_res_addr.target,\r\nerror->last_func_vset_res_addr.lun);\r\nipr_err_separator;\r\narray_entry = error->array_member;\r\nfor (i = 0; i < 18; i++) {\r\nif (!memcmp(array_entry->vpd.sn, zero_sn, IPR_SERIAL_NUM_LEN))\r\ncontinue;\r\nif (be32_to_cpu(error->exposed_mode_adn) == i)\r\nipr_err("Exposed Array Member %d:\n", i);\r\nelse\r\nipr_err("Array Member %d:\n", i);\r\nipr_log_vpd(&array_entry->vpd);\r\nipr_phys_res_err(ioa_cfg, array_entry->dev_res_addr, "Current Location");\r\nipr_phys_res_err(ioa_cfg, array_entry->expected_dev_res_addr,\r\n"Expected Location");\r\nipr_err_separator;\r\nif (i == 9)\r\narray_entry = error->array_member2;\r\nelse\r\narray_entry++;\r\n}\r\n}\r\nstatic void ipr_log_hex_data(struct ipr_ioa_cfg *ioa_cfg, __be32 *data, int len)\r\n{\r\nint i;\r\nif (len == 0)\r\nreturn;\r\nif (ioa_cfg->log_level <= IPR_DEFAULT_LOG_LEVEL)\r\nlen = min_t(int, len, IPR_DEFAULT_MAX_ERROR_DUMP);\r\nfor (i = 0; i < len / 4; i += 4) {\r\nipr_err("%08X: %08X %08X %08X %08X\n", i*4,\r\nbe32_to_cpu(data[i]),\r\nbe32_to_cpu(data[i+1]),\r\nbe32_to_cpu(data[i+2]),\r\nbe32_to_cpu(data[i+3]));\r\n}\r\n}\r\nstatic void ipr_log_enhanced_dual_ioa_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nstruct ipr_hostrcb_type_17_error *error;\r\nif (ioa_cfg->sis64)\r\nerror = &hostrcb->hcam.u.error64.u.type_17_error;\r\nelse\r\nerror = &hostrcb->hcam.u.error.u.type_17_error;\r\nerror->failure_reason[sizeof(error->failure_reason) - 1] = '\0';\r\nstrim(error->failure_reason);\r\nipr_hcam_err(hostrcb, "%s [PRC: %08X]\n", error->failure_reason,\r\nbe32_to_cpu(hostrcb->hcam.u.error.prc));\r\nipr_log_ext_vpd_compact("Remote IOA", hostrcb, &error->vpd);\r\nipr_log_hex_data(ioa_cfg, error->data,\r\nbe32_to_cpu(hostrcb->hcam.length) -\r\n(offsetof(struct ipr_hostrcb_error, u) +\r\noffsetof(struct ipr_hostrcb_type_17_error, data)));\r\n}\r\nstatic void ipr_log_dual_ioa_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nstruct ipr_hostrcb_type_07_error *error;\r\nerror = &hostrcb->hcam.u.error.u.type_07_error;\r\nerror->failure_reason[sizeof(error->failure_reason) - 1] = '\0';\r\nstrim(error->failure_reason);\r\nipr_hcam_err(hostrcb, "%s [PRC: %08X]\n", error->failure_reason,\r\nbe32_to_cpu(hostrcb->hcam.u.error.prc));\r\nipr_log_vpd_compact("Remote IOA", hostrcb, &error->vpd);\r\nipr_log_hex_data(ioa_cfg, error->data,\r\nbe32_to_cpu(hostrcb->hcam.length) -\r\n(offsetof(struct ipr_hostrcb_error, u) +\r\noffsetof(struct ipr_hostrcb_type_07_error, data)));\r\n}\r\nstatic void ipr_log_fabric_path(struct ipr_hostrcb *hostrcb,\r\nstruct ipr_hostrcb_fabric_desc *fabric)\r\n{\r\nint i, j;\r\nu8 path_state = fabric->path_state;\r\nu8 active = path_state & IPR_PATH_ACTIVE_MASK;\r\nu8 state = path_state & IPR_PATH_STATE_MASK;\r\nfor (i = 0; i < ARRAY_SIZE(path_active_desc); i++) {\r\nif (path_active_desc[i].active != active)\r\ncontinue;\r\nfor (j = 0; j < ARRAY_SIZE(path_state_desc); j++) {\r\nif (path_state_desc[j].state != state)\r\ncontinue;\r\nif (fabric->cascaded_expander == 0xff && fabric->phy == 0xff) {\r\nipr_hcam_err(hostrcb, "%s %s: IOA Port=%d\n",\r\npath_active_desc[i].desc, path_state_desc[j].desc,\r\nfabric->ioa_port);\r\n} else if (fabric->cascaded_expander == 0xff) {\r\nipr_hcam_err(hostrcb, "%s %s: IOA Port=%d, Phy=%d\n",\r\npath_active_desc[i].desc, path_state_desc[j].desc,\r\nfabric->ioa_port, fabric->phy);\r\n} else if (fabric->phy == 0xff) {\r\nipr_hcam_err(hostrcb, "%s %s: IOA Port=%d, Cascade=%d\n",\r\npath_active_desc[i].desc, path_state_desc[j].desc,\r\nfabric->ioa_port, fabric->cascaded_expander);\r\n} else {\r\nipr_hcam_err(hostrcb, "%s %s: IOA Port=%d, Cascade=%d, Phy=%d\n",\r\npath_active_desc[i].desc, path_state_desc[j].desc,\r\nfabric->ioa_port, fabric->cascaded_expander, fabric->phy);\r\n}\r\nreturn;\r\n}\r\n}\r\nipr_err("Path state=%02X IOA Port=%d Cascade=%d Phy=%d\n", path_state,\r\nfabric->ioa_port, fabric->cascaded_expander, fabric->phy);\r\n}\r\nstatic void ipr_log64_fabric_path(struct ipr_hostrcb *hostrcb,\r\nstruct ipr_hostrcb64_fabric_desc *fabric)\r\n{\r\nint i, j;\r\nu8 path_state = fabric->path_state;\r\nu8 active = path_state & IPR_PATH_ACTIVE_MASK;\r\nu8 state = path_state & IPR_PATH_STATE_MASK;\r\nchar buffer[IPR_MAX_RES_PATH_LENGTH];\r\nfor (i = 0; i < ARRAY_SIZE(path_active_desc); i++) {\r\nif (path_active_desc[i].active != active)\r\ncontinue;\r\nfor (j = 0; j < ARRAY_SIZE(path_state_desc); j++) {\r\nif (path_state_desc[j].state != state)\r\ncontinue;\r\nipr_hcam_err(hostrcb, "%s %s: Resource Path=%s\n",\r\npath_active_desc[i].desc, path_state_desc[j].desc,\r\nipr_format_res_path(hostrcb->ioa_cfg,\r\nfabric->res_path,\r\nbuffer, sizeof(buffer)));\r\nreturn;\r\n}\r\n}\r\nipr_err("Path state=%02X Resource Path=%s\n", path_state,\r\nipr_format_res_path(hostrcb->ioa_cfg, fabric->res_path,\r\nbuffer, sizeof(buffer)));\r\n}\r\nstatic void ipr_log_path_elem(struct ipr_hostrcb *hostrcb,\r\nstruct ipr_hostrcb_config_element *cfg)\r\n{\r\nint i, j;\r\nu8 type = cfg->type_status & IPR_PATH_CFG_TYPE_MASK;\r\nu8 status = cfg->type_status & IPR_PATH_CFG_STATUS_MASK;\r\nif (type == IPR_PATH_CFG_NOT_EXIST)\r\nreturn;\r\nfor (i = 0; i < ARRAY_SIZE(path_type_desc); i++) {\r\nif (path_type_desc[i].type != type)\r\ncontinue;\r\nfor (j = 0; j < ARRAY_SIZE(path_status_desc); j++) {\r\nif (path_status_desc[j].status != status)\r\ncontinue;\r\nif (type == IPR_PATH_CFG_IOA_PORT) {\r\nipr_hcam_err(hostrcb, "%s %s: Phy=%d, Link rate=%s, WWN=%08X%08X\n",\r\npath_status_desc[j].desc, path_type_desc[i].desc,\r\ncfg->phy, link_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\r\nbe32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\r\n} else {\r\nif (cfg->cascaded_expander == 0xff && cfg->phy == 0xff) {\r\nipr_hcam_err(hostrcb, "%s %s: Link rate=%s, WWN=%08X%08X\n",\r\npath_status_desc[j].desc, path_type_desc[i].desc,\r\nlink_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\r\nbe32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\r\n} else if (cfg->cascaded_expander == 0xff) {\r\nipr_hcam_err(hostrcb, "%s %s: Phy=%d, Link rate=%s, "\r\n"WWN=%08X%08X\n", path_status_desc[j].desc,\r\npath_type_desc[i].desc, cfg->phy,\r\nlink_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\r\nbe32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\r\n} else if (cfg->phy == 0xff) {\r\nipr_hcam_err(hostrcb, "%s %s: Cascade=%d, Link rate=%s, "\r\n"WWN=%08X%08X\n", path_status_desc[j].desc,\r\npath_type_desc[i].desc, cfg->cascaded_expander,\r\nlink_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\r\nbe32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\r\n} else {\r\nipr_hcam_err(hostrcb, "%s %s: Cascade=%d, Phy=%d, Link rate=%s "\r\n"WWN=%08X%08X\n", path_status_desc[j].desc,\r\npath_type_desc[i].desc, cfg->cascaded_expander, cfg->phy,\r\nlink_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\r\nbe32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\r\n}\r\n}\r\nreturn;\r\n}\r\n}\r\nipr_hcam_err(hostrcb, "Path element=%02X: Cascade=%d Phy=%d Link rate=%s "\r\n"WWN=%08X%08X\n", cfg->type_status, cfg->cascaded_expander, cfg->phy,\r\nlink_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\r\nbe32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\r\n}\r\nstatic void ipr_log64_path_elem(struct ipr_hostrcb *hostrcb,\r\nstruct ipr_hostrcb64_config_element *cfg)\r\n{\r\nint i, j;\r\nu8 desc_id = cfg->descriptor_id & IPR_DESCRIPTOR_MASK;\r\nu8 type = cfg->type_status & IPR_PATH_CFG_TYPE_MASK;\r\nu8 status = cfg->type_status & IPR_PATH_CFG_STATUS_MASK;\r\nchar buffer[IPR_MAX_RES_PATH_LENGTH];\r\nif (type == IPR_PATH_CFG_NOT_EXIST || desc_id != IPR_DESCRIPTOR_SIS64)\r\nreturn;\r\nfor (i = 0; i < ARRAY_SIZE(path_type_desc); i++) {\r\nif (path_type_desc[i].type != type)\r\ncontinue;\r\nfor (j = 0; j < ARRAY_SIZE(path_status_desc); j++) {\r\nif (path_status_desc[j].status != status)\r\ncontinue;\r\nipr_hcam_err(hostrcb, "%s %s: Resource Path=%s, Link rate=%s, WWN=%08X%08X\n",\r\npath_status_desc[j].desc, path_type_desc[i].desc,\r\nipr_format_res_path(hostrcb->ioa_cfg,\r\ncfg->res_path, buffer, sizeof(buffer)),\r\nlink_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\r\nbe32_to_cpu(cfg->wwid[0]),\r\nbe32_to_cpu(cfg->wwid[1]));\r\nreturn;\r\n}\r\n}\r\nipr_hcam_err(hostrcb, "Path element=%02X: Resource Path=%s, Link rate=%s "\r\n"WWN=%08X%08X\n", cfg->type_status,\r\nipr_format_res_path(hostrcb->ioa_cfg,\r\ncfg->res_path, buffer, sizeof(buffer)),\r\nlink_rate[cfg->link_rate & IPR_PHY_LINK_RATE_MASK],\r\nbe32_to_cpu(cfg->wwid[0]), be32_to_cpu(cfg->wwid[1]));\r\n}\r\nstatic void ipr_log_fabric_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nstruct ipr_hostrcb_type_20_error *error;\r\nstruct ipr_hostrcb_fabric_desc *fabric;\r\nstruct ipr_hostrcb_config_element *cfg;\r\nint i, add_len;\r\nerror = &hostrcb->hcam.u.error.u.type_20_error;\r\nerror->failure_reason[sizeof(error->failure_reason) - 1] = '\0';\r\nipr_hcam_err(hostrcb, "%s\n", error->failure_reason);\r\nadd_len = be32_to_cpu(hostrcb->hcam.length) -\r\n(offsetof(struct ipr_hostrcb_error, u) +\r\noffsetof(struct ipr_hostrcb_type_20_error, desc));\r\nfor (i = 0, fabric = error->desc; i < error->num_entries; i++) {\r\nipr_log_fabric_path(hostrcb, fabric);\r\nfor_each_fabric_cfg(fabric, cfg)\r\nipr_log_path_elem(hostrcb, cfg);\r\nadd_len -= be16_to_cpu(fabric->length);\r\nfabric = (struct ipr_hostrcb_fabric_desc *)\r\n((unsigned long)fabric + be16_to_cpu(fabric->length));\r\n}\r\nipr_log_hex_data(ioa_cfg, (__be32 *)fabric, add_len);\r\n}\r\nstatic void ipr_log_sis64_array_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nint i, num_entries;\r\nstruct ipr_hostrcb_type_24_error *error;\r\nstruct ipr_hostrcb64_array_data_entry *array_entry;\r\nchar buffer[IPR_MAX_RES_PATH_LENGTH];\r\nconst u8 zero_sn[IPR_SERIAL_NUM_LEN] = { [0 ... IPR_SERIAL_NUM_LEN-1] = '0' };\r\nerror = &hostrcb->hcam.u.error64.u.type_24_error;\r\nipr_err_separator;\r\nipr_err("RAID %s Array Configuration: %s\n",\r\nerror->protection_level,\r\nipr_format_res_path(ioa_cfg, error->last_res_path,\r\nbuffer, sizeof(buffer)));\r\nipr_err_separator;\r\narray_entry = error->array_member;\r\nnum_entries = min_t(u32, error->num_entries,\r\nARRAY_SIZE(error->array_member));\r\nfor (i = 0; i < num_entries; i++, array_entry++) {\r\nif (!memcmp(array_entry->vpd.vpd.sn, zero_sn, IPR_SERIAL_NUM_LEN))\r\ncontinue;\r\nif (error->exposed_mode_adn == i)\r\nipr_err("Exposed Array Member %d:\n", i);\r\nelse\r\nipr_err("Array Member %d:\n", i);\r\nipr_err("Array Member %d:\n", i);\r\nipr_log_ext_vpd(&array_entry->vpd);\r\nipr_err("Current Location: %s\n",\r\nipr_format_res_path(ioa_cfg, array_entry->res_path,\r\nbuffer, sizeof(buffer)));\r\nipr_err("Expected Location: %s\n",\r\nipr_format_res_path(ioa_cfg,\r\narray_entry->expected_res_path,\r\nbuffer, sizeof(buffer)));\r\nipr_err_separator;\r\n}\r\n}\r\nstatic void ipr_log_sis64_fabric_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nstruct ipr_hostrcb_type_30_error *error;\r\nstruct ipr_hostrcb64_fabric_desc *fabric;\r\nstruct ipr_hostrcb64_config_element *cfg;\r\nint i, add_len;\r\nerror = &hostrcb->hcam.u.error64.u.type_30_error;\r\nerror->failure_reason[sizeof(error->failure_reason) - 1] = '\0';\r\nipr_hcam_err(hostrcb, "%s\n", error->failure_reason);\r\nadd_len = be32_to_cpu(hostrcb->hcam.length) -\r\n(offsetof(struct ipr_hostrcb64_error, u) +\r\noffsetof(struct ipr_hostrcb_type_30_error, desc));\r\nfor (i = 0, fabric = error->desc; i < error->num_entries; i++) {\r\nipr_log64_fabric_path(hostrcb, fabric);\r\nfor_each_fabric_cfg(fabric, cfg)\r\nipr_log64_path_elem(hostrcb, cfg);\r\nadd_len -= be16_to_cpu(fabric->length);\r\nfabric = (struct ipr_hostrcb64_fabric_desc *)\r\n((unsigned long)fabric + be16_to_cpu(fabric->length));\r\n}\r\nipr_log_hex_data(ioa_cfg, (__be32 *)fabric, add_len);\r\n}\r\nstatic void ipr_log_generic_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nipr_log_hex_data(ioa_cfg, hostrcb->hcam.u.raw.data,\r\nbe32_to_cpu(hostrcb->hcam.length));\r\n}\r\nstatic void ipr_log_sis64_device_error(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nstruct ipr_hostrcb_type_21_error *error;\r\nchar buffer[IPR_MAX_RES_PATH_LENGTH];\r\nerror = &hostrcb->hcam.u.error64.u.type_21_error;\r\nipr_err("-----Failing Device Information-----\n");\r\nipr_err("World Wide Unique ID: %08X%08X%08X%08X\n",\r\nbe32_to_cpu(error->wwn[0]), be32_to_cpu(error->wwn[1]),\r\nbe32_to_cpu(error->wwn[2]), be32_to_cpu(error->wwn[3]));\r\nipr_err("Device Resource Path: %s\n",\r\n__ipr_format_res_path(error->res_path,\r\nbuffer, sizeof(buffer)));\r\nerror->primary_problem_desc[sizeof(error->primary_problem_desc) - 1] = '\0';\r\nerror->second_problem_desc[sizeof(error->second_problem_desc) - 1] = '\0';\r\nipr_err("Primary Problem Description: %s\n", error->primary_problem_desc);\r\nipr_err("Secondary Problem Description: %s\n", error->second_problem_desc);\r\nipr_err("SCSI Sense Data:\n");\r\nipr_log_hex_data(ioa_cfg, error->sense_data, sizeof(error->sense_data));\r\nipr_err("SCSI Command Descriptor Block: \n");\r\nipr_log_hex_data(ioa_cfg, error->cdb, sizeof(error->cdb));\r\nipr_err("Additional IOA Data:\n");\r\nipr_log_hex_data(ioa_cfg, error->ioa_data, be32_to_cpu(error->length_of_error));\r\n}\r\nstatic u32 ipr_get_error(u32 ioasc)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(ipr_error_table); i++)\r\nif (ipr_error_table[i].ioasc == (ioasc & IPR_IOASC_IOASC_MASK))\r\nreturn i;\r\nreturn 0;\r\n}\r\nstatic void ipr_handle_log_data(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_hostrcb *hostrcb)\r\n{\r\nu32 ioasc;\r\nint error_index;\r\nstruct ipr_hostrcb_type_21_error *error;\r\nif (hostrcb->hcam.notify_type != IPR_HOST_RCB_NOTIF_TYPE_ERROR_LOG_ENTRY)\r\nreturn;\r\nif (hostrcb->hcam.notifications_lost == IPR_HOST_RCB_NOTIFICATIONS_LOST)\r\ndev_err(&ioa_cfg->pdev->dev, "Error notifications lost\n");\r\nif (ioa_cfg->sis64)\r\nioasc = be32_to_cpu(hostrcb->hcam.u.error64.fd_ioasc);\r\nelse\r\nioasc = be32_to_cpu(hostrcb->hcam.u.error.fd_ioasc);\r\nif (!ioa_cfg->sis64 && (ioasc == IPR_IOASC_BUS_WAS_RESET ||\r\nioasc == IPR_IOASC_BUS_WAS_RESET_BY_OTHER)) {\r\nscsi_report_bus_reset(ioa_cfg->host,\r\nhostrcb->hcam.u.error.fd_res_addr.bus);\r\n}\r\nerror_index = ipr_get_error(ioasc);\r\nif (!ipr_error_table[error_index].log_hcam)\r\nreturn;\r\nif (ioasc == IPR_IOASC_HW_CMD_FAILED &&\r\nhostrcb->hcam.overlay_id == IPR_HOST_RCB_OVERLAY_ID_21) {\r\nerror = &hostrcb->hcam.u.error64.u.type_21_error;\r\nif (((be32_to_cpu(error->sense_data[0]) & 0x0000ff00) >> 8) == ILLEGAL_REQUEST &&\r\nioa_cfg->log_level <= IPR_DEFAULT_LOG_LEVEL)\r\nreturn;\r\n}\r\nipr_hcam_err(hostrcb, "%s\n", ipr_error_table[error_index].error);\r\nioa_cfg->errors_logged++;\r\nif (ioa_cfg->log_level < ipr_error_table[error_index].log_hcam)\r\nreturn;\r\nif (be32_to_cpu(hostrcb->hcam.length) > sizeof(hostrcb->hcam.u.raw))\r\nhostrcb->hcam.length = cpu_to_be32(sizeof(hostrcb->hcam.u.raw));\r\nswitch (hostrcb->hcam.overlay_id) {\r\ncase IPR_HOST_RCB_OVERLAY_ID_2:\r\nipr_log_cache_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_3:\r\nipr_log_config_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_4:\r\ncase IPR_HOST_RCB_OVERLAY_ID_6:\r\nipr_log_array_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_7:\r\nipr_log_dual_ioa_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_12:\r\nipr_log_enhanced_cache_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_13:\r\nipr_log_enhanced_config_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_14:\r\ncase IPR_HOST_RCB_OVERLAY_ID_16:\r\nipr_log_enhanced_array_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_17:\r\nipr_log_enhanced_dual_ioa_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_20:\r\nipr_log_fabric_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_21:\r\nipr_log_sis64_device_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_23:\r\nipr_log_sis64_config_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_24:\r\ncase IPR_HOST_RCB_OVERLAY_ID_26:\r\nipr_log_sis64_array_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_30:\r\nipr_log_sis64_fabric_error(ioa_cfg, hostrcb);\r\nbreak;\r\ncase IPR_HOST_RCB_OVERLAY_ID_1:\r\ncase IPR_HOST_RCB_OVERLAY_ID_DEFAULT:\r\ndefault:\r\nipr_log_generic_error(ioa_cfg, hostrcb);\r\nbreak;\r\n}\r\n}\r\nstatic struct ipr_hostrcb *ipr_get_free_hostrcb(struct ipr_ioa_cfg *ioa)\r\n{\r\nstruct ipr_hostrcb *hostrcb;\r\nhostrcb = list_first_entry_or_null(&ioa->hostrcb_free_q,\r\nstruct ipr_hostrcb, queue);\r\nif (unlikely(!hostrcb)) {\r\ndev_info(&ioa->pdev->dev, "Reclaiming async error buffers.");\r\nhostrcb = list_first_entry_or_null(&ioa->hostrcb_report_q,\r\nstruct ipr_hostrcb, queue);\r\n}\r\nlist_del_init(&hostrcb->queue);\r\nreturn hostrcb;\r\n}\r\nstatic void ipr_process_error(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_hostrcb *hostrcb = ipr_cmd->u.hostrcb;\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nu32 fd_ioasc;\r\nif (ioa_cfg->sis64)\r\nfd_ioasc = be32_to_cpu(hostrcb->hcam.u.error64.fd_ioasc);\r\nelse\r\nfd_ioasc = be32_to_cpu(hostrcb->hcam.u.error.fd_ioasc);\r\nlist_del_init(&hostrcb->queue);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nif (!ioasc) {\r\nipr_handle_log_data(ioa_cfg, hostrcb);\r\nif (fd_ioasc == IPR_IOASC_NR_IOA_RESET_REQUIRED)\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_ABBREV);\r\n} else if (ioasc != IPR_IOASC_IOA_WAS_RESET &&\r\nioasc != IPR_IOASC_ABORTED_CMD_TERM_BY_HOST) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Host RCB failed with IOASC: 0x%08X\n", ioasc);\r\n}\r\nlist_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_report_q);\r\nschedule_work(&ioa_cfg->work_q);\r\nhostrcb = ipr_get_free_hostrcb(ioa_cfg);\r\nipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_LOG_DATA, hostrcb);\r\n}\r\nstatic void ipr_timeout(struct ipr_cmnd *ipr_cmd)\r\n{\r\nunsigned long lock_flags = 0;\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nioa_cfg->errors_logged++;\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Adapter being reset due to command timeout.\n");\r\nif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\r\nioa_cfg->sdt_state = GET_DUMP;\r\nif (!ioa_cfg->in_reset_reload || ioa_cfg->reset_cmd == ipr_cmd)\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nLEAVE;\r\n}\r\nstatic void ipr_oper_timeout(struct ipr_cmnd *ipr_cmd)\r\n{\r\nunsigned long lock_flags = 0;\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nioa_cfg->errors_logged++;\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Adapter timed out transitioning to operational.\n");\r\nif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\r\nioa_cfg->sdt_state = GET_DUMP;\r\nif (!ioa_cfg->in_reset_reload || ioa_cfg->reset_cmd == ipr_cmd) {\r\nif (ipr_fastfail)\r\nioa_cfg->reset_retries += IPR_NUM_RESET_RELOAD_RETRIES;\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nLEAVE;\r\n}\r\nstatic const struct ipr_ses_table_entry *\r\nipr_find_ses_entry(struct ipr_resource_entry *res)\r\n{\r\nint i, j, matches;\r\nstruct ipr_std_inq_vpids *vpids;\r\nconst struct ipr_ses_table_entry *ste = ipr_ses_table;\r\nfor (i = 0; i < ARRAY_SIZE(ipr_ses_table); i++, ste++) {\r\nfor (j = 0, matches = 0; j < IPR_PROD_ID_LEN; j++) {\r\nif (ste->compare_product_id_byte[j] == 'X') {\r\nvpids = &res->std_inq_data.vpids;\r\nif (vpids->product_id[j] == ste->product_id[j])\r\nmatches++;\r\nelse\r\nbreak;\r\n} else\r\nmatches++;\r\n}\r\nif (matches == IPR_PROD_ID_LEN)\r\nreturn ste;\r\n}\r\nreturn NULL;\r\n}\r\nstatic u32 ipr_get_max_scsi_speed(struct ipr_ioa_cfg *ioa_cfg, u8 bus, u8 bus_width)\r\n{\r\nstruct ipr_resource_entry *res;\r\nconst struct ipr_ses_table_entry *ste;\r\nu32 max_xfer_rate = IPR_MAX_SCSI_RATE(bus_width);\r\nlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\r\nif (!(IPR_IS_SES_DEVICE(res->std_inq_data)))\r\ncontinue;\r\nif (bus != res->bus)\r\ncontinue;\r\nif (!(ste = ipr_find_ses_entry(res)))\r\ncontinue;\r\nmax_xfer_rate = (ste->max_bus_speed_limit * 10) / (bus_width / 8);\r\n}\r\nreturn max_xfer_rate;\r\n}\r\nstatic int ipr_wait_iodbg_ack(struct ipr_ioa_cfg *ioa_cfg, int max_delay)\r\n{\r\nvolatile u32 pcii_reg;\r\nint delay = 1;\r\nwhile (delay < max_delay) {\r\npcii_reg = readl(ioa_cfg->regs.sense_interrupt_reg);\r\nif (pcii_reg & IPR_PCII_IO_DEBUG_ACKNOWLEDGE)\r\nreturn 0;\r\nif ((delay / 1000) > MAX_UDELAY_MS)\r\nmdelay(delay / 1000);\r\nelse\r\nudelay(delay);\r\ndelay += delay;\r\n}\r\nreturn -EIO;\r\n}\r\nstatic int ipr_get_sis64_dump_data_section(struct ipr_ioa_cfg *ioa_cfg,\r\nu32 start_addr,\r\n__be32 *dest, u32 length_in_words)\r\n{\r\nint i;\r\nfor (i = 0; i < length_in_words; i++) {\r\nwritel(start_addr+(i*4), ioa_cfg->regs.dump_addr_reg);\r\n*dest = cpu_to_be32(readl(ioa_cfg->regs.dump_data_reg));\r\ndest++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ipr_get_ldump_data_section(struct ipr_ioa_cfg *ioa_cfg,\r\nu32 start_addr,\r\n__be32 *dest, u32 length_in_words)\r\n{\r\nvolatile u32 temp_pcii_reg;\r\nint i, delay = 0;\r\nif (ioa_cfg->sis64)\r\nreturn ipr_get_sis64_dump_data_section(ioa_cfg, start_addr,\r\ndest, length_in_words);\r\nwritel((IPR_UPROCI_RESET_ALERT | IPR_UPROCI_IO_DEBUG_ALERT),\r\nioa_cfg->regs.set_uproc_interrupt_reg32);\r\nif (ipr_wait_iodbg_ack(ioa_cfg,\r\nIPR_LDUMP_MAX_LONG_ACK_DELAY_IN_USEC)) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"IOA dump long data transfer timeout\n");\r\nreturn -EIO;\r\n}\r\nwritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,\r\nioa_cfg->regs.clr_interrupt_reg);\r\nwritel(start_addr, ioa_cfg->ioa_mailbox);\r\nwritel(IPR_UPROCI_RESET_ALERT,\r\nioa_cfg->regs.clr_uproc_interrupt_reg32);\r\nfor (i = 0; i < length_in_words; i++) {\r\nif (ipr_wait_iodbg_ack(ioa_cfg,\r\nIPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC)) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"IOA dump short data transfer timeout\n");\r\nreturn -EIO;\r\n}\r\n*dest = cpu_to_be32(readl(ioa_cfg->ioa_mailbox));\r\ndest++;\r\nif (i < (length_in_words - 1)) {\r\nwritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,\r\nioa_cfg->regs.clr_interrupt_reg);\r\n}\r\n}\r\nwritel(IPR_UPROCI_RESET_ALERT,\r\nioa_cfg->regs.set_uproc_interrupt_reg32);\r\nwritel(IPR_UPROCI_IO_DEBUG_ALERT,\r\nioa_cfg->regs.clr_uproc_interrupt_reg32);\r\nwritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,\r\nioa_cfg->regs.clr_interrupt_reg);\r\nwhile (delay < IPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC) {\r\ntemp_pcii_reg =\r\nreadl(ioa_cfg->regs.sense_uproc_interrupt_reg32);\r\nif (!(temp_pcii_reg & IPR_UPROCI_RESET_ALERT))\r\nreturn 0;\r\nudelay(10);\r\ndelay += 10;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ipr_sdt_copy(struct ipr_ioa_cfg *ioa_cfg,\r\nunsigned long pci_address, u32 length)\r\n{\r\nint bytes_copied = 0;\r\nint cur_len, rc, rem_len, rem_page_len, max_dump_size;\r\n__be32 *page;\r\nunsigned long lock_flags = 0;\r\nstruct ipr_ioa_dump *ioa_dump = &ioa_cfg->dump->ioa_dump;\r\nif (ioa_cfg->sis64)\r\nmax_dump_size = IPR_FMT3_MAX_IOA_DUMP_SIZE;\r\nelse\r\nmax_dump_size = IPR_FMT2_MAX_IOA_DUMP_SIZE;\r\nwhile (bytes_copied < length &&\r\n(ioa_dump->hdr.len + bytes_copied) < max_dump_size) {\r\nif (ioa_dump->page_offset >= PAGE_SIZE ||\r\nioa_dump->page_offset == 0) {\r\npage = (__be32 *)__get_free_page(GFP_ATOMIC);\r\nif (!page) {\r\nipr_trace;\r\nreturn bytes_copied;\r\n}\r\nioa_dump->page_offset = 0;\r\nioa_dump->ioa_data[ioa_dump->next_page_index] = page;\r\nioa_dump->next_page_index++;\r\n} else\r\npage = ioa_dump->ioa_data[ioa_dump->next_page_index - 1];\r\nrem_len = length - bytes_copied;\r\nrem_page_len = PAGE_SIZE - ioa_dump->page_offset;\r\ncur_len = min(rem_len, rem_page_len);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->sdt_state == ABORT_DUMP) {\r\nrc = -EIO;\r\n} else {\r\nrc = ipr_get_ldump_data_section(ioa_cfg,\r\npci_address + bytes_copied,\r\n&page[ioa_dump->page_offset / 4],\r\n(cur_len / sizeof(u32)));\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nif (!rc) {\r\nioa_dump->page_offset += cur_len;\r\nbytes_copied += cur_len;\r\n} else {\r\nipr_trace;\r\nbreak;\r\n}\r\nschedule();\r\n}\r\nreturn bytes_copied;\r\n}\r\nstatic void ipr_init_dump_entry_hdr(struct ipr_dump_entry_header *hdr)\r\n{\r\nhdr->eye_catcher = IPR_DUMP_EYE_CATCHER;\r\nhdr->num_elems = 1;\r\nhdr->offset = sizeof(*hdr);\r\nhdr->status = IPR_DUMP_STATUS_SUCCESS;\r\n}\r\nstatic void ipr_dump_ioa_type_data(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_driver_dump *driver_dump)\r\n{\r\nstruct ipr_inquiry_page3 *ucode_vpd = &ioa_cfg->vpd_cbs->page3_data;\r\nipr_init_dump_entry_hdr(&driver_dump->ioa_type_entry.hdr);\r\ndriver_dump->ioa_type_entry.hdr.len =\r\nsizeof(struct ipr_dump_ioa_type_entry) -\r\nsizeof(struct ipr_dump_entry_header);\r\ndriver_dump->ioa_type_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;\r\ndriver_dump->ioa_type_entry.hdr.id = IPR_DUMP_DRIVER_TYPE_ID;\r\ndriver_dump->ioa_type_entry.type = ioa_cfg->type;\r\ndriver_dump->ioa_type_entry.fw_version = (ucode_vpd->major_release << 24) |\r\n(ucode_vpd->card_type << 16) | (ucode_vpd->minor_release[0] << 8) |\r\nucode_vpd->minor_release[1];\r\ndriver_dump->hdr.num_entries++;\r\n}\r\nstatic void ipr_dump_version_data(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_driver_dump *driver_dump)\r\n{\r\nipr_init_dump_entry_hdr(&driver_dump->version_entry.hdr);\r\ndriver_dump->version_entry.hdr.len =\r\nsizeof(struct ipr_dump_version_entry) -\r\nsizeof(struct ipr_dump_entry_header);\r\ndriver_dump->version_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_ASCII;\r\ndriver_dump->version_entry.hdr.id = IPR_DUMP_DRIVER_VERSION_ID;\r\nstrcpy(driver_dump->version_entry.version, IPR_DRIVER_VERSION);\r\ndriver_dump->hdr.num_entries++;\r\n}\r\nstatic void ipr_dump_trace_data(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_driver_dump *driver_dump)\r\n{\r\nipr_init_dump_entry_hdr(&driver_dump->trace_entry.hdr);\r\ndriver_dump->trace_entry.hdr.len =\r\nsizeof(struct ipr_dump_trace_entry) -\r\nsizeof(struct ipr_dump_entry_header);\r\ndriver_dump->trace_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;\r\ndriver_dump->trace_entry.hdr.id = IPR_DUMP_TRACE_ID;\r\nmemcpy(driver_dump->trace_entry.trace, ioa_cfg->trace, IPR_TRACE_SIZE);\r\ndriver_dump->hdr.num_entries++;\r\n}\r\nstatic void ipr_dump_location_data(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_driver_dump *driver_dump)\r\n{\r\nipr_init_dump_entry_hdr(&driver_dump->location_entry.hdr);\r\ndriver_dump->location_entry.hdr.len =\r\nsizeof(struct ipr_dump_location_entry) -\r\nsizeof(struct ipr_dump_entry_header);\r\ndriver_dump->location_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_ASCII;\r\ndriver_dump->location_entry.hdr.id = IPR_DUMP_LOCATION_ID;\r\nstrcpy(driver_dump->location_entry.location, dev_name(&ioa_cfg->pdev->dev));\r\ndriver_dump->hdr.num_entries++;\r\n}\r\nstatic void ipr_get_ioa_dump(struct ipr_ioa_cfg *ioa_cfg, struct ipr_dump *dump)\r\n{\r\nunsigned long start_addr, sdt_word;\r\nunsigned long lock_flags = 0;\r\nstruct ipr_driver_dump *driver_dump = &dump->driver_dump;\r\nstruct ipr_ioa_dump *ioa_dump = &dump->ioa_dump;\r\nu32 num_entries, max_num_entries, start_off, end_off;\r\nu32 max_dump_size, bytes_to_copy, bytes_copied, rc;\r\nstruct ipr_sdt *sdt;\r\nint valid = 1;\r\nint i;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->sdt_state != READ_DUMP) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn;\r\n}\r\nif (ioa_cfg->sis64) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nssleep(IPR_DUMP_DELAY_SECONDS);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\nstart_addr = readl(ioa_cfg->ioa_mailbox);\r\nif (!ioa_cfg->sis64 && !ipr_sdt_is_fmt2(start_addr)) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Invalid dump table format: %lx\n", start_addr);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn;\r\n}\r\ndev_err(&ioa_cfg->pdev->dev, "Dump of IOA initiated\n");\r\ndriver_dump->hdr.eye_catcher = IPR_DUMP_EYE_CATCHER;\r\ndriver_dump->hdr.len = sizeof(struct ipr_driver_dump);\r\ndriver_dump->hdr.num_entries = 1;\r\ndriver_dump->hdr.first_entry_offset = sizeof(struct ipr_dump_header);\r\ndriver_dump->hdr.status = IPR_DUMP_STATUS_SUCCESS;\r\ndriver_dump->hdr.os = IPR_DUMP_OS_LINUX;\r\ndriver_dump->hdr.driver_name = IPR_DUMP_DRIVER_NAME;\r\nipr_dump_version_data(ioa_cfg, driver_dump);\r\nipr_dump_location_data(ioa_cfg, driver_dump);\r\nipr_dump_ioa_type_data(ioa_cfg, driver_dump);\r\nipr_dump_trace_data(ioa_cfg, driver_dump);\r\ndriver_dump->hdr.len += sizeof(struct ipr_dump_entry_header);\r\nipr_init_dump_entry_hdr(&ioa_dump->hdr);\r\nioa_dump->hdr.len = 0;\r\nioa_dump->hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;\r\nioa_dump->hdr.id = IPR_DUMP_IOA_DUMP_ID;\r\nsdt = &ioa_dump->sdt;\r\nif (ioa_cfg->sis64) {\r\nmax_num_entries = IPR_FMT3_NUM_SDT_ENTRIES;\r\nmax_dump_size = IPR_FMT3_MAX_IOA_DUMP_SIZE;\r\n} else {\r\nmax_num_entries = IPR_FMT2_NUM_SDT_ENTRIES;\r\nmax_dump_size = IPR_FMT2_MAX_IOA_DUMP_SIZE;\r\n}\r\nbytes_to_copy = offsetof(struct ipr_sdt, entry) +\r\n(max_num_entries * sizeof(struct ipr_sdt_entry));\r\nrc = ipr_get_ldump_data_section(ioa_cfg, start_addr, (__be32 *)sdt,\r\nbytes_to_copy / sizeof(__be32));\r\nif (rc || ((be32_to_cpu(sdt->hdr.state) != IPR_FMT3_SDT_READY_TO_USE) &&\r\n(be32_to_cpu(sdt->hdr.state) != IPR_FMT2_SDT_READY_TO_USE))) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Dump of IOA failed. Dump table not valid: %d, %X.\n",\r\nrc, be32_to_cpu(sdt->hdr.state));\r\ndriver_dump->hdr.status = IPR_DUMP_STATUS_FAILED;\r\nioa_cfg->sdt_state = DUMP_OBTAINED;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn;\r\n}\r\nnum_entries = be32_to_cpu(sdt->hdr.num_entries_used);\r\nif (num_entries > max_num_entries)\r\nnum_entries = max_num_entries;\r\ndump->driver_dump.hdr.len += sizeof(struct ipr_sdt_header);\r\nif (ioa_cfg->sis64)\r\ndump->driver_dump.hdr.len += num_entries * sizeof(struct ipr_sdt_entry);\r\nelse\r\ndump->driver_dump.hdr.len += max_num_entries * sizeof(struct ipr_sdt_entry);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nfor (i = 0; i < num_entries; i++) {\r\nif (ioa_dump->hdr.len > max_dump_size) {\r\ndriver_dump->hdr.status = IPR_DUMP_STATUS_QUAL_SUCCESS;\r\nbreak;\r\n}\r\nif (sdt->entry[i].flags & IPR_SDT_VALID_ENTRY) {\r\nsdt_word = be32_to_cpu(sdt->entry[i].start_token);\r\nif (ioa_cfg->sis64)\r\nbytes_to_copy = be32_to_cpu(sdt->entry[i].end_token);\r\nelse {\r\nstart_off = sdt_word & IPR_FMT2_MBX_ADDR_MASK;\r\nend_off = be32_to_cpu(sdt->entry[i].end_token);\r\nif (ipr_sdt_is_fmt2(sdt_word) && sdt_word)\r\nbytes_to_copy = end_off - start_off;\r\nelse\r\nvalid = 0;\r\n}\r\nif (valid) {\r\nif (bytes_to_copy > max_dump_size) {\r\nsdt->entry[i].flags &= ~IPR_SDT_VALID_ENTRY;\r\ncontinue;\r\n}\r\nbytes_copied = ipr_sdt_copy(ioa_cfg, sdt_word,\r\nbytes_to_copy);\r\nioa_dump->hdr.len += bytes_copied;\r\nif (bytes_copied != bytes_to_copy) {\r\ndriver_dump->hdr.status = IPR_DUMP_STATUS_QUAL_SUCCESS;\r\nbreak;\r\n}\r\n}\r\n}\r\n}\r\ndev_err(&ioa_cfg->pdev->dev, "Dump of IOA completed.\n");\r\ndriver_dump->hdr.len += ioa_dump->hdr.len;\r\nwmb();\r\nioa_cfg->sdt_state = DUMP_OBTAINED;\r\nLEAVE;\r\n}\r\nstatic void ipr_release_dump(struct kref *kref)\r\n{\r\nstruct ipr_dump *dump = container_of(kref, struct ipr_dump, kref);\r\nstruct ipr_ioa_cfg *ioa_cfg = dump->ioa_cfg;\r\nunsigned long lock_flags = 0;\r\nint i;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nioa_cfg->dump = NULL;\r\nioa_cfg->sdt_state = INACTIVE;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nfor (i = 0; i < dump->ioa_dump.next_page_index; i++)\r\nfree_page((unsigned long) dump->ioa_dump.ioa_data[i]);\r\nvfree(dump->ioa_dump.ioa_data);\r\nkfree(dump);\r\nLEAVE;\r\n}\r\nstatic void ipr_worker_thread(struct work_struct *work)\r\n{\r\nunsigned long lock_flags;\r\nstruct ipr_resource_entry *res;\r\nstruct scsi_device *sdev;\r\nstruct ipr_dump *dump;\r\nstruct ipr_ioa_cfg *ioa_cfg =\r\ncontainer_of(work, struct ipr_ioa_cfg, work_q);\r\nu8 bus, target, lun;\r\nint did_work;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->sdt_state == READ_DUMP) {\r\ndump = ioa_cfg->dump;\r\nif (!dump) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn;\r\n}\r\nkref_get(&dump->kref);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nipr_get_ioa_dump(ioa_cfg, dump);\r\nkref_put(&dump->kref, ipr_release_dump);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->sdt_state == DUMP_OBTAINED && !ioa_cfg->dump_timeout)\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn;\r\n}\r\nif (ioa_cfg->scsi_unblock) {\r\nioa_cfg->scsi_unblock = 0;\r\nioa_cfg->scsi_blocked = 0;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nscsi_unblock_requests(ioa_cfg->host);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->scsi_blocked)\r\nscsi_block_requests(ioa_cfg->host);\r\n}\r\nif (!ioa_cfg->scan_enabled) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn;\r\n}\r\nrestart:\r\ndo {\r\ndid_work = 0;\r\nif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].allow_cmds) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn;\r\n}\r\nlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\r\nif (res->del_from_ml && res->sdev) {\r\ndid_work = 1;\r\nsdev = res->sdev;\r\nif (!scsi_device_get(sdev)) {\r\nif (!res->add_to_ml)\r\nlist_move_tail(&res->queue, &ioa_cfg->free_res_q);\r\nelse\r\nres->del_from_ml = 0;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nscsi_remove_device(sdev);\r\nscsi_device_put(sdev);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\nbreak;\r\n}\r\n}\r\n} while (did_work);\r\nlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\r\nif (res->add_to_ml) {\r\nbus = res->bus;\r\ntarget = res->target;\r\nlun = res->lun;\r\nres->add_to_ml = 0;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nscsi_add_device(ioa_cfg->host, bus, target, lun);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\ngoto restart;\r\n}\r\n}\r\nioa_cfg->scan_done = 1;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nkobject_uevent(&ioa_cfg->host->shost_dev.kobj, KOBJ_CHANGE);\r\nLEAVE;\r\n}\r\nstatic ssize_t ipr_read_trace(struct file *filp, struct kobject *kobj,\r\nstruct bin_attribute *bin_attr,\r\nchar *buf, loff_t off, size_t count)\r\n{\r\nstruct device *dev = container_of(kobj, struct device, kobj);\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long lock_flags = 0;\r\nssize_t ret;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nret = memory_read_from_buffer(buf, count, &off, ioa_cfg->trace,\r\nIPR_TRACE_SIZE);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn ret;\r\n}\r\nstatic ssize_t ipr_show_fw_version(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nstruct ipr_inquiry_page3 *ucode_vpd = &ioa_cfg->vpd_cbs->page3_data;\r\nunsigned long lock_flags = 0;\r\nint len;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nlen = snprintf(buf, PAGE_SIZE, "%02X%02X%02X%02X\n",\r\nucode_vpd->major_release, ucode_vpd->card_type,\r\nucode_vpd->minor_release[0],\r\nucode_vpd->minor_release[1]);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_show_log_level(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long lock_flags = 0;\r\nint len;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nlen = snprintf(buf, PAGE_SIZE, "%d\n", ioa_cfg->log_level);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_store_log_level(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long lock_flags = 0;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nioa_cfg->log_level = simple_strtoul(buf, NULL, 10);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn strlen(buf);\r\n}\r\nstatic ssize_t ipr_store_diagnostics(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long lock_flags = 0;\r\nint rc = count;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nwhile (ioa_cfg->in_reset_reload) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\nioa_cfg->errors_logged = 0;\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);\r\nif (ioa_cfg->in_reset_reload) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nmsleep(1000);\r\n} else {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn -EIO;\r\n}\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->in_reset_reload || ioa_cfg->errors_logged)\r\nrc = -EIO;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn rc;\r\n}\r\nstatic ssize_t ipr_show_adapter_state(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long lock_flags = 0;\r\nint len;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead)\r\nlen = snprintf(buf, PAGE_SIZE, "offline\n");\r\nelse\r\nlen = snprintf(buf, PAGE_SIZE, "online\n");\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_store_adapter_state(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long lock_flags;\r\nint result = count, i;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead &&\r\n!strncmp(buf, "online", 6)) {\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nspin_lock(&ioa_cfg->hrrq[i]._lock);\r\nioa_cfg->hrrq[i].ioa_is_dead = 0;\r\nspin_unlock(&ioa_cfg->hrrq[i]._lock);\r\n}\r\nwmb();\r\nioa_cfg->reset_retries = 0;\r\nioa_cfg->in_ioa_bringdown = 0;\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nreturn result;\r\n}\r\nstatic ssize_t ipr_store_reset_adapter(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long lock_flags;\r\nint result = count;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (!ioa_cfg->in_reset_reload)\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nreturn result;\r\n}\r\nstatic ssize_t ipr_show_iopoll_weight(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long lock_flags = 0;\r\nint len;\r\nspin_lock_irqsave(shost->host_lock, lock_flags);\r\nlen = snprintf(buf, PAGE_SIZE, "%d\n", ioa_cfg->iopoll_weight);\r\nspin_unlock_irqrestore(shost->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_store_iopoll_weight(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long user_iopoll_weight;\r\nunsigned long lock_flags = 0;\r\nint i;\r\nif (!ioa_cfg->sis64) {\r\ndev_info(&ioa_cfg->pdev->dev, "irq_poll not supported on this adapter\n");\r\nreturn -EINVAL;\r\n}\r\nif (kstrtoul(buf, 10, &user_iopoll_weight))\r\nreturn -EINVAL;\r\nif (user_iopoll_weight > 256) {\r\ndev_info(&ioa_cfg->pdev->dev, "Invalid irq_poll weight. It must be less than 256\n");\r\nreturn -EINVAL;\r\n}\r\nif (user_iopoll_weight == ioa_cfg->iopoll_weight) {\r\ndev_info(&ioa_cfg->pdev->dev, "Current irq_poll weight has the same weight\n");\r\nreturn strlen(buf);\r\n}\r\nif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\r\nfor (i = 1; i < ioa_cfg->hrrq_num; i++)\r\nirq_poll_disable(&ioa_cfg->hrrq[i].iopoll);\r\n}\r\nspin_lock_irqsave(shost->host_lock, lock_flags);\r\nioa_cfg->iopoll_weight = user_iopoll_weight;\r\nif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\r\nfor (i = 1; i < ioa_cfg->hrrq_num; i++) {\r\nirq_poll_init(&ioa_cfg->hrrq[i].iopoll,\r\nioa_cfg->iopoll_weight, ipr_iopoll);\r\n}\r\n}\r\nspin_unlock_irqrestore(shost->host_lock, lock_flags);\r\nreturn strlen(buf);\r\n}\r\nstatic struct ipr_sglist *ipr_alloc_ucode_buffer(int buf_len)\r\n{\r\nint sg_size, order, bsize_elem, num_elem, i, j;\r\nstruct ipr_sglist *sglist;\r\nstruct scatterlist *scatterlist;\r\nstruct page *page;\r\nsg_size = buf_len / (IPR_MAX_SGLIST - 1);\r\norder = get_order(sg_size);\r\nbsize_elem = PAGE_SIZE * (1 << order);\r\nif (buf_len % bsize_elem)\r\nnum_elem = (buf_len / bsize_elem) + 1;\r\nelse\r\nnum_elem = buf_len / bsize_elem;\r\nsglist = kzalloc(sizeof(struct ipr_sglist) +\r\n(sizeof(struct scatterlist) * (num_elem - 1)),\r\nGFP_KERNEL);\r\nif (sglist == NULL) {\r\nipr_trace;\r\nreturn NULL;\r\n}\r\nscatterlist = sglist->scatterlist;\r\nsg_init_table(scatterlist, num_elem);\r\nsglist->order = order;\r\nsglist->num_sg = num_elem;\r\nfor (i = 0; i < num_elem; i++) {\r\npage = alloc_pages(GFP_KERNEL, order);\r\nif (!page) {\r\nipr_trace;\r\nfor (j = i - 1; j >= 0; j--)\r\n__free_pages(sg_page(&scatterlist[j]), order);\r\nkfree(sglist);\r\nreturn NULL;\r\n}\r\nsg_set_page(&scatterlist[i], page, 0, 0);\r\n}\r\nreturn sglist;\r\n}\r\nstatic void ipr_free_ucode_buffer(struct ipr_sglist *sglist)\r\n{\r\nint i;\r\nfor (i = 0; i < sglist->num_sg; i++)\r\n__free_pages(sg_page(&sglist->scatterlist[i]), sglist->order);\r\nkfree(sglist);\r\n}\r\nstatic int ipr_copy_ucode_buffer(struct ipr_sglist *sglist,\r\nu8 *buffer, u32 len)\r\n{\r\nint bsize_elem, i, result = 0;\r\nstruct scatterlist *scatterlist;\r\nvoid *kaddr;\r\nbsize_elem = PAGE_SIZE * (1 << sglist->order);\r\nscatterlist = sglist->scatterlist;\r\nfor (i = 0; i < (len / bsize_elem); i++, buffer += bsize_elem) {\r\nstruct page *page = sg_page(&scatterlist[i]);\r\nkaddr = kmap(page);\r\nmemcpy(kaddr, buffer, bsize_elem);\r\nkunmap(page);\r\nscatterlist[i].length = bsize_elem;\r\nif (result != 0) {\r\nipr_trace;\r\nreturn result;\r\n}\r\n}\r\nif (len % bsize_elem) {\r\nstruct page *page = sg_page(&scatterlist[i]);\r\nkaddr = kmap(page);\r\nmemcpy(kaddr, buffer, len % bsize_elem);\r\nkunmap(page);\r\nscatterlist[i].length = len % bsize_elem;\r\n}\r\nsglist->buffer_len = len;\r\nreturn result;\r\n}\r\nstatic void ipr_build_ucode_ioadl64(struct ipr_cmnd *ipr_cmd,\r\nstruct ipr_sglist *sglist)\r\n{\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_ioadl64_desc *ioadl64 = ipr_cmd->i.ioadl64;\r\nstruct scatterlist *scatterlist = sglist->scatterlist;\r\nint i;\r\nipr_cmd->dma_use_sg = sglist->num_dma_sg;\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\r\nioarcb->data_transfer_length = cpu_to_be32(sglist->buffer_len);\r\nioarcb->ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl64_desc) * ipr_cmd->dma_use_sg);\r\nfor (i = 0; i < ipr_cmd->dma_use_sg; i++) {\r\nioadl64[i].flags = cpu_to_be32(IPR_IOADL_FLAGS_WRITE);\r\nioadl64[i].data_len = cpu_to_be32(sg_dma_len(&scatterlist[i]));\r\nioadl64[i].address = cpu_to_be64(sg_dma_address(&scatterlist[i]));\r\n}\r\nioadl64[i-1].flags |= cpu_to_be32(IPR_IOADL_FLAGS_LAST);\r\n}\r\nstatic void ipr_build_ucode_ioadl(struct ipr_cmnd *ipr_cmd,\r\nstruct ipr_sglist *sglist)\r\n{\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_ioadl_desc *ioadl = ipr_cmd->i.ioadl;\r\nstruct scatterlist *scatterlist = sglist->scatterlist;\r\nint i;\r\nipr_cmd->dma_use_sg = sglist->num_dma_sg;\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\r\nioarcb->data_transfer_length = cpu_to_be32(sglist->buffer_len);\r\nioarcb->ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);\r\nfor (i = 0; i < ipr_cmd->dma_use_sg; i++) {\r\nioadl[i].flags_and_data_len =\r\ncpu_to_be32(IPR_IOADL_FLAGS_WRITE | sg_dma_len(&scatterlist[i]));\r\nioadl[i].address =\r\ncpu_to_be32(sg_dma_address(&scatterlist[i]));\r\n}\r\nioadl[i-1].flags_and_data_len |=\r\ncpu_to_be32(IPR_IOADL_FLAGS_LAST);\r\n}\r\nstatic int ipr_update_ioa_ucode(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_sglist *sglist)\r\n{\r\nunsigned long lock_flags;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nwhile (ioa_cfg->in_reset_reload) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\nif (ioa_cfg->ucode_sglist) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Microcode download already in progress\n");\r\nreturn -EIO;\r\n}\r\nsglist->num_dma_sg = dma_map_sg(&ioa_cfg->pdev->dev,\r\nsglist->scatterlist, sglist->num_sg,\r\nDMA_TO_DEVICE);\r\nif (!sglist->num_dma_sg) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Failed to map microcode download buffer!\n");\r\nreturn -EIO;\r\n}\r\nioa_cfg->ucode_sglist = sglist;\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nioa_cfg->ucode_sglist = NULL;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn 0;\r\n}\r\nstatic ssize_t ipr_store_update_fw(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nstruct ipr_ucode_image_header *image_hdr;\r\nconst struct firmware *fw_entry;\r\nstruct ipr_sglist *sglist;\r\nchar fname[100];\r\nchar *src;\r\nchar *endline;\r\nint result, dnld_size;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nsnprintf(fname, sizeof(fname), "%s", buf);\r\nendline = strchr(fname, '\n');\r\nif (endline)\r\n*endline = '\0';\r\nif (request_firmware(&fw_entry, fname, &ioa_cfg->pdev->dev)) {\r\ndev_err(&ioa_cfg->pdev->dev, "Firmware file %s not found\n", fname);\r\nreturn -EIO;\r\n}\r\nimage_hdr = (struct ipr_ucode_image_header *)fw_entry->data;\r\nsrc = (u8 *)image_hdr + be32_to_cpu(image_hdr->header_length);\r\ndnld_size = fw_entry->size - be32_to_cpu(image_hdr->header_length);\r\nsglist = ipr_alloc_ucode_buffer(dnld_size);\r\nif (!sglist) {\r\ndev_err(&ioa_cfg->pdev->dev, "Microcode buffer allocation failed\n");\r\nrelease_firmware(fw_entry);\r\nreturn -ENOMEM;\r\n}\r\nresult = ipr_copy_ucode_buffer(sglist, src, dnld_size);\r\nif (result) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Microcode buffer copy to DMA buffer failed\n");\r\ngoto out;\r\n}\r\nipr_info("Updating microcode, please be patient. This may take up to 30 minutes.\n");\r\nresult = ipr_update_ioa_ucode(ioa_cfg, sglist);\r\nif (!result)\r\nresult = count;\r\nout:\r\nipr_free_ucode_buffer(sglist);\r\nrelease_firmware(fw_entry);\r\nreturn result;\r\n}\r\nstatic ssize_t ipr_show_fw_type(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nunsigned long lock_flags = 0;\r\nint len;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nlen = snprintf(buf, PAGE_SIZE, "%d\n", ioa_cfg->sis64);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_read_async_err_log(struct file *filep, struct kobject *kobj,\r\nstruct bin_attribute *bin_attr, char *buf,\r\nloff_t off, size_t count)\r\n{\r\nstruct device *cdev = container_of(kobj, struct device, kobj);\r\nstruct Scsi_Host *shost = class_to_shost(cdev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nstruct ipr_hostrcb *hostrcb;\r\nunsigned long lock_flags = 0;\r\nint ret;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nhostrcb = list_first_entry_or_null(&ioa_cfg->hostrcb_report_q,\r\nstruct ipr_hostrcb, queue);\r\nif (!hostrcb) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn 0;\r\n}\r\nret = memory_read_from_buffer(buf, count, &off, &hostrcb->hcam,\r\nsizeof(hostrcb->hcam));\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn ret;\r\n}\r\nstatic ssize_t ipr_next_async_err_log(struct file *filep, struct kobject *kobj,\r\nstruct bin_attribute *bin_attr, char *buf,\r\nloff_t off, size_t count)\r\n{\r\nstruct device *cdev = container_of(kobj, struct device, kobj);\r\nstruct Scsi_Host *shost = class_to_shost(cdev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nstruct ipr_hostrcb *hostrcb;\r\nunsigned long lock_flags = 0;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nhostrcb = list_first_entry_or_null(&ioa_cfg->hostrcb_report_q,\r\nstruct ipr_hostrcb, queue);\r\nif (!hostrcb) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn count;\r\n}\r\nlist_move_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn count;\r\n}\r\nstatic ssize_t ipr_read_dump(struct file *filp, struct kobject *kobj,\r\nstruct bin_attribute *bin_attr,\r\nchar *buf, loff_t off, size_t count)\r\n{\r\nstruct device *cdev = container_of(kobj, struct device, kobj);\r\nstruct Scsi_Host *shost = class_to_shost(cdev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nstruct ipr_dump *dump;\r\nunsigned long lock_flags = 0;\r\nchar *src;\r\nint len, sdt_end;\r\nsize_t rc = count;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\ndump = ioa_cfg->dump;\r\nif (ioa_cfg->sdt_state != DUMP_OBTAINED || !dump) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn 0;\r\n}\r\nkref_get(&dump->kref);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nif (off > dump->driver_dump.hdr.len) {\r\nkref_put(&dump->kref, ipr_release_dump);\r\nreturn 0;\r\n}\r\nif (off + count > dump->driver_dump.hdr.len) {\r\ncount = dump->driver_dump.hdr.len - off;\r\nrc = count;\r\n}\r\nif (count && off < sizeof(dump->driver_dump)) {\r\nif (off + count > sizeof(dump->driver_dump))\r\nlen = sizeof(dump->driver_dump) - off;\r\nelse\r\nlen = count;\r\nsrc = (u8 *)&dump->driver_dump + off;\r\nmemcpy(buf, src, len);\r\nbuf += len;\r\noff += len;\r\ncount -= len;\r\n}\r\noff -= sizeof(dump->driver_dump);\r\nif (ioa_cfg->sis64)\r\nsdt_end = offsetof(struct ipr_ioa_dump, sdt.entry) +\r\n(be32_to_cpu(dump->ioa_dump.sdt.hdr.num_entries_used) *\r\nsizeof(struct ipr_sdt_entry));\r\nelse\r\nsdt_end = offsetof(struct ipr_ioa_dump, sdt.entry) +\r\n(IPR_FMT2_NUM_SDT_ENTRIES * sizeof(struct ipr_sdt_entry));\r\nif (count && off < sdt_end) {\r\nif (off + count > sdt_end)\r\nlen = sdt_end - off;\r\nelse\r\nlen = count;\r\nsrc = (u8 *)&dump->ioa_dump + off;\r\nmemcpy(buf, src, len);\r\nbuf += len;\r\noff += len;\r\ncount -= len;\r\n}\r\noff -= sdt_end;\r\nwhile (count) {\r\nif ((off & PAGE_MASK) != ((off + count) & PAGE_MASK))\r\nlen = PAGE_ALIGN(off) - off;\r\nelse\r\nlen = count;\r\nsrc = (u8 *)dump->ioa_dump.ioa_data[(off & PAGE_MASK) >> PAGE_SHIFT];\r\nsrc += off & ~PAGE_MASK;\r\nmemcpy(buf, src, len);\r\nbuf += len;\r\noff += len;\r\ncount -= len;\r\n}\r\nkref_put(&dump->kref, ipr_release_dump);\r\nreturn rc;\r\n}\r\nstatic int ipr_alloc_dump(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct ipr_dump *dump;\r\n__be32 **ioa_data;\r\nunsigned long lock_flags = 0;\r\ndump = kzalloc(sizeof(struct ipr_dump), GFP_KERNEL);\r\nif (!dump) {\r\nipr_err("Dump memory allocation failed\n");\r\nreturn -ENOMEM;\r\n}\r\nif (ioa_cfg->sis64)\r\nioa_data = vmalloc(IPR_FMT3_MAX_NUM_DUMP_PAGES * sizeof(__be32 *));\r\nelse\r\nioa_data = vmalloc(IPR_FMT2_MAX_NUM_DUMP_PAGES * sizeof(__be32 *));\r\nif (!ioa_data) {\r\nipr_err("Dump memory allocation failed\n");\r\nkfree(dump);\r\nreturn -ENOMEM;\r\n}\r\ndump->ioa_dump.ioa_data = ioa_data;\r\nkref_init(&dump->kref);\r\ndump->ioa_cfg = ioa_cfg;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (INACTIVE != ioa_cfg->sdt_state) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nvfree(dump->ioa_dump.ioa_data);\r\nkfree(dump);\r\nreturn 0;\r\n}\r\nioa_cfg->dump = dump;\r\nioa_cfg->sdt_state = WAIT_FOR_DUMP;\r\nif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead && !ioa_cfg->dump_taken) {\r\nioa_cfg->dump_taken = 1;\r\nschedule_work(&ioa_cfg->work_q);\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn 0;\r\n}\r\nstatic int ipr_free_dump(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct ipr_dump *dump;\r\nunsigned long lock_flags = 0;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\ndump = ioa_cfg->dump;\r\nif (!dump) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn 0;\r\n}\r\nioa_cfg->dump = NULL;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nkref_put(&dump->kref, ipr_release_dump);\r\nLEAVE;\r\nreturn 0;\r\n}\r\nstatic ssize_t ipr_write_dump(struct file *filp, struct kobject *kobj,\r\nstruct bin_attribute *bin_attr,\r\nchar *buf, loff_t off, size_t count)\r\n{\r\nstruct device *cdev = container_of(kobj, struct device, kobj);\r\nstruct Scsi_Host *shost = class_to_shost(cdev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nint rc;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nif (buf[0] == '1')\r\nrc = ipr_alloc_dump(ioa_cfg);\r\nelse if (buf[0] == '0')\r\nrc = ipr_free_dump(ioa_cfg);\r\nelse\r\nreturn -EINVAL;\r\nif (rc)\r\nreturn rc;\r\nelse\r\nreturn count;\r\n}\r\nstatic int ipr_free_dump(struct ipr_ioa_cfg *ioa_cfg) { return 0; }\r\nstatic int ipr_change_queue_depth(struct scsi_device *sdev, int qdepth)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags = 0;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = (struct ipr_resource_entry *)sdev->hostdata;\r\nif (res && ipr_is_gata(res) && qdepth > IPR_MAX_CMD_PER_ATA_LUN)\r\nqdepth = IPR_MAX_CMD_PER_ATA_LUN;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nscsi_change_queue_depth(sdev, qdepth);\r\nreturn sdev->queue_depth;\r\n}\r\nstatic ssize_t ipr_show_adapter_handle(struct device *dev, struct device_attribute *attr, char *buf)\r\n{\r\nstruct scsi_device *sdev = to_scsi_device(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags = 0;\r\nssize_t len = -ENXIO;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = (struct ipr_resource_entry *)sdev->hostdata;\r\nif (res)\r\nlen = snprintf(buf, PAGE_SIZE, "%08X\n", res->res_handle);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_show_resource_path(struct device *dev, struct device_attribute *attr, char *buf)\r\n{\r\nstruct scsi_device *sdev = to_scsi_device(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags = 0;\r\nssize_t len = -ENXIO;\r\nchar buffer[IPR_MAX_RES_PATH_LENGTH];\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = (struct ipr_resource_entry *)sdev->hostdata;\r\nif (res && ioa_cfg->sis64)\r\nlen = snprintf(buf, PAGE_SIZE, "%s\n",\r\n__ipr_format_res_path(res->res_path, buffer,\r\nsizeof(buffer)));\r\nelse if (res)\r\nlen = snprintf(buf, PAGE_SIZE, "%d:%d:%d:%d\n", ioa_cfg->host->host_no,\r\nres->bus, res->target, res->lun);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_show_device_id(struct device *dev, struct device_attribute *attr, char *buf)\r\n{\r\nstruct scsi_device *sdev = to_scsi_device(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags = 0;\r\nssize_t len = -ENXIO;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = (struct ipr_resource_entry *)sdev->hostdata;\r\nif (res && ioa_cfg->sis64)\r\nlen = snprintf(buf, PAGE_SIZE, "0x%llx\n", be64_to_cpu(res->dev_id));\r\nelse if (res)\r\nlen = snprintf(buf, PAGE_SIZE, "0x%llx\n", res->lun_wwn);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_show_resource_type(struct device *dev, struct device_attribute *attr, char *buf)\r\n{\r\nstruct scsi_device *sdev = to_scsi_device(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags = 0;\r\nssize_t len = -ENXIO;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = (struct ipr_resource_entry *)sdev->hostdata;\r\nif (res)\r\nlen = snprintf(buf, PAGE_SIZE, "%x\n", res->type);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_show_raw_mode(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct scsi_device *sdev = to_scsi_device(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags = 0;\r\nssize_t len;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = (struct ipr_resource_entry *)sdev->hostdata;\r\nif (res)\r\nlen = snprintf(buf, PAGE_SIZE, "%d\n", res->raw_mode);\r\nelse\r\nlen = -ENXIO;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic ssize_t ipr_store_raw_mode(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct scsi_device *sdev = to_scsi_device(dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags = 0;\r\nssize_t len;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = (struct ipr_resource_entry *)sdev->hostdata;\r\nif (res) {\r\nif (ipr_is_af_dasd_device(res)) {\r\nres->raw_mode = simple_strtoul(buf, NULL, 10);\r\nlen = strlen(buf);\r\nif (res->sdev)\r\nsdev_printk(KERN_INFO, res->sdev, "raw mode is %s\n",\r\nres->raw_mode ? "enabled" : "disabled");\r\n} else\r\nlen = -EINVAL;\r\n} else\r\nlen = -ENXIO;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn len;\r\n}\r\nstatic int ipr_biosparam(struct scsi_device *sdev,\r\nstruct block_device *block_device,\r\nsector_t capacity, int *parm)\r\n{\r\nint heads, sectors;\r\nsector_t cylinders;\r\nheads = 128;\r\nsectors = 32;\r\ncylinders = capacity;\r\nsector_div(cylinders, (128 * 32));\r\nparm[0] = heads;\r\nparm[1] = sectors;\r\nparm[2] = cylinders;\r\nreturn 0;\r\n}\r\nstatic struct ipr_resource_entry *ipr_find_starget(struct scsi_target *starget)\r\n{\r\nstruct Scsi_Host *shost = dev_to_shost(&starget->dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) shost->hostdata;\r\nstruct ipr_resource_entry *res;\r\nlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\r\nif ((res->bus == starget->channel) &&\r\n(res->target == starget->id)) {\r\nreturn res;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic int ipr_target_alloc(struct scsi_target *starget)\r\n{\r\nstruct Scsi_Host *shost = dev_to_shost(&starget->dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) shost->hostdata;\r\nstruct ipr_sata_port *sata_port;\r\nstruct ata_port *ap;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = ipr_find_starget(starget);\r\nstarget->hostdata = NULL;\r\nif (res && ipr_is_gata(res)) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nsata_port = kzalloc(sizeof(*sata_port), GFP_KERNEL);\r\nif (!sata_port)\r\nreturn -ENOMEM;\r\nap = ata_sas_port_alloc(&ioa_cfg->ata_host, &sata_port_info, shost);\r\nif (ap) {\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nsata_port->ioa_cfg = ioa_cfg;\r\nsata_port->ap = ap;\r\nsata_port->res = res;\r\nres->sata_port = sata_port;\r\nap->private_data = sata_port;\r\nstarget->hostdata = sata_port;\r\n} else {\r\nkfree(sata_port);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn 0;\r\n}\r\nstatic void ipr_target_destroy(struct scsi_target *starget)\r\n{\r\nstruct ipr_sata_port *sata_port = starget->hostdata;\r\nstruct Scsi_Host *shost = dev_to_shost(&starget->dev);\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) shost->hostdata;\r\nif (ioa_cfg->sis64) {\r\nif (!ipr_find_starget(starget)) {\r\nif (starget->channel == IPR_ARRAY_VIRTUAL_BUS)\r\nclear_bit(starget->id, ioa_cfg->array_ids);\r\nelse if (starget->channel == IPR_VSET_VIRTUAL_BUS)\r\nclear_bit(starget->id, ioa_cfg->vset_ids);\r\nelse if (starget->channel == 0)\r\nclear_bit(starget->id, ioa_cfg->target_ids);\r\n}\r\n}\r\nif (sata_port) {\r\nstarget->hostdata = NULL;\r\nata_sas_port_destroy(sata_port->ap);\r\nkfree(sata_port);\r\n}\r\n}\r\nstatic struct ipr_resource_entry *ipr_find_sdev(struct scsi_device *sdev)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\r\nif ((res->bus == sdev->channel) &&\r\n(res->target == sdev->id) &&\r\n(res->lun == sdev->lun))\r\nreturn res;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void ipr_slave_destroy(struct scsi_device *sdev)\r\n{\r\nstruct ipr_resource_entry *res;\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nunsigned long lock_flags = 0;\r\nioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = (struct ipr_resource_entry *) sdev->hostdata;\r\nif (res) {\r\nif (res->sata_port)\r\nres->sata_port->ap->link.device[0].class = ATA_DEV_NONE;\r\nsdev->hostdata = NULL;\r\nres->sdev = NULL;\r\nres->sata_port = NULL;\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\nstatic int ipr_slave_configure(struct scsi_device *sdev)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nstruct ata_port *ap = NULL;\r\nunsigned long lock_flags = 0;\r\nchar buffer[IPR_MAX_RES_PATH_LENGTH];\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = sdev->hostdata;\r\nif (res) {\r\nif (ipr_is_af_dasd_device(res))\r\nsdev->type = TYPE_RAID;\r\nif (ipr_is_af_dasd_device(res) || ipr_is_ioa_resource(res)) {\r\nsdev->scsi_level = 4;\r\nsdev->no_uld_attach = 1;\r\n}\r\nif (ipr_is_vset_device(res)) {\r\nsdev->scsi_level = SCSI_SPC_3;\r\nsdev->no_report_opcodes = 1;\r\nblk_queue_rq_timeout(sdev->request_queue,\r\nIPR_VSET_RW_TIMEOUT);\r\nblk_queue_max_hw_sectors(sdev->request_queue, IPR_VSET_MAX_SECTORS);\r\n}\r\nif (ipr_is_gata(res) && res->sata_port)\r\nap = res->sata_port->ap;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nif (ap) {\r\nscsi_change_queue_depth(sdev, IPR_MAX_CMD_PER_ATA_LUN);\r\nata_sas_slave_configure(sdev, ap);\r\n}\r\nif (ioa_cfg->sis64)\r\nsdev_printk(KERN_INFO, sdev, "Resource path: %s\n",\r\nipr_format_res_path(ioa_cfg,\r\nres->res_path, buffer, sizeof(buffer)));\r\nreturn 0;\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn 0;\r\n}\r\nstatic int ipr_ata_slave_alloc(struct scsi_device *sdev)\r\n{\r\nstruct ipr_sata_port *sata_port = NULL;\r\nint rc = -ENXIO;\r\nENTER;\r\nif (sdev->sdev_target)\r\nsata_port = sdev->sdev_target->hostdata;\r\nif (sata_port) {\r\nrc = ata_sas_port_init(sata_port->ap);\r\nif (rc == 0)\r\nrc = ata_sas_sync_probe(sata_port->ap);\r\n}\r\nif (rc)\r\nipr_slave_destroy(sdev);\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic int ipr_slave_alloc(struct scsi_device *sdev)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags;\r\nint rc = -ENXIO;\r\nsdev->hostdata = NULL;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nres = ipr_find_sdev(sdev);\r\nif (res) {\r\nres->sdev = sdev;\r\nres->add_to_ml = 0;\r\nres->in_erp = 0;\r\nsdev->hostdata = res;\r\nif (!ipr_is_naca_model(res))\r\nres->needs_sync_complete = 1;\r\nrc = 0;\r\nif (ipr_is_gata(res)) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn ipr_ata_slave_alloc(sdev);\r\n}\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn rc;\r\n}\r\nstatic int ipr_match_lun(struct ipr_cmnd *ipr_cmd, void *device)\r\n{\r\nif (ipr_cmd->scsi_cmd && ipr_cmd->scsi_cmd->device == device)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic bool ipr_cmnd_is_free(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_cmnd *loop_cmd;\r\nlist_for_each_entry(loop_cmd, &ipr_cmd->hrrq->hrrq_free_q, queue) {\r\nif (loop_cmd == ipr_cmd)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int ipr_match_res(struct ipr_cmnd *ipr_cmd, void *resource)\r\n{\r\nstruct ipr_resource_entry *res = resource;\r\nif (res && ipr_cmd->ioarcb.res_handle == res->res_handle)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int ipr_wait_for_ops(struct ipr_ioa_cfg *ioa_cfg, void *device,\r\nint (*match)(struct ipr_cmnd *, void *))\r\n{\r\nstruct ipr_cmnd *ipr_cmd;\r\nint wait, i;\r\nunsigned long flags;\r\nstruct ipr_hrr_queue *hrrq;\r\nsigned long timeout = IPR_ABORT_TASK_TIMEOUT;\r\nDECLARE_COMPLETION_ONSTACK(comp);\r\nENTER;\r\ndo {\r\nwait = 0;\r\nfor_each_hrrq(hrrq, ioa_cfg) {\r\nspin_lock_irqsave(hrrq->lock, flags);\r\nfor (i = hrrq->min_cmd_id; i <= hrrq->max_cmd_id; i++) {\r\nipr_cmd = ioa_cfg->ipr_cmnd_list[i];\r\nif (!ipr_cmnd_is_free(ipr_cmd)) {\r\nif (match(ipr_cmd, device)) {\r\nipr_cmd->eh_comp = &comp;\r\nwait++;\r\n}\r\n}\r\n}\r\nspin_unlock_irqrestore(hrrq->lock, flags);\r\n}\r\nif (wait) {\r\ntimeout = wait_for_completion_timeout(&comp, timeout);\r\nif (!timeout) {\r\nwait = 0;\r\nfor_each_hrrq(hrrq, ioa_cfg) {\r\nspin_lock_irqsave(hrrq->lock, flags);\r\nfor (i = hrrq->min_cmd_id; i <= hrrq->max_cmd_id; i++) {\r\nipr_cmd = ioa_cfg->ipr_cmnd_list[i];\r\nif (!ipr_cmnd_is_free(ipr_cmd)) {\r\nif (match(ipr_cmd, device)) {\r\nipr_cmd->eh_comp = NULL;\r\nwait++;\r\n}\r\n}\r\n}\r\nspin_unlock_irqrestore(hrrq->lock, flags);\r\n}\r\nif (wait)\r\ndev_err(&ioa_cfg->pdev->dev, "Timed out waiting for aborted commands\n");\r\nLEAVE;\r\nreturn wait ? FAILED : SUCCESS;\r\n}\r\n}\r\n} while (wait);\r\nLEAVE;\r\nreturn SUCCESS;\r\n}\r\nstatic int ipr_eh_host_reset(struct scsi_cmnd *cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nunsigned long lock_flags = 0;\r\nint rc = SUCCESS;\r\nENTER;\r\nioa_cfg = (struct ipr_ioa_cfg *) cmd->device->host->hostdata;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (!ioa_cfg->in_reset_reload && !ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead) {\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_ABBREV);\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Adapter being reset as a result of error recovery.\n");\r\nif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\r\nioa_cfg->sdt_state = GET_DUMP;\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead) {\r\nipr_trace;\r\nrc = FAILED;\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic int ipr_device_reset(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_resource_entry *res)\r\n{\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_ioarcb *ioarcb;\r\nstruct ipr_cmd_pkt *cmd_pkt;\r\nstruct ipr_ioarcb_ata_regs *regs;\r\nu32 ioasc;\r\nENTER;\r\nipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\r\nioarcb = &ipr_cmd->ioarcb;\r\ncmd_pkt = &ioarcb->cmd_pkt;\r\nif (ipr_cmd->ioa_cfg->sis64) {\r\nregs = &ipr_cmd->i.ata_ioadl.regs;\r\nioarcb->add_cmd_parms_offset = cpu_to_be16(sizeof(*ioarcb));\r\n} else\r\nregs = &ioarcb->u.add_data.u.regs;\r\nioarcb->res_handle = res->res_handle;\r\ncmd_pkt->request_type = IPR_RQTYPE_IOACMD;\r\ncmd_pkt->cdb[0] = IPR_RESET_DEVICE;\r\nif (ipr_is_gata(res)) {\r\ncmd_pkt->cdb[2] = IPR_ATA_PHY_RESET;\r\nioarcb->add_cmd_parms_len = cpu_to_be16(sizeof(regs->flags));\r\nregs->flags |= IPR_ATA_FLAG_STATUS_ON_GOOD_COMPLETION;\r\n}\r\nipr_send_blocking_cmd(ipr_cmd, ipr_timeout, IPR_DEVICE_RESET_TIMEOUT);\r\nioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nif (ipr_is_gata(res) && res->sata_port && ioasc != IPR_IOASC_IOA_WAS_RESET) {\r\nif (ipr_cmd->ioa_cfg->sis64)\r\nmemcpy(&res->sata_port->ioasa, &ipr_cmd->s.ioasa64.u.gata,\r\nsizeof(struct ipr_ioasa_gata));\r\nelse\r\nmemcpy(&res->sata_port->ioasa, &ipr_cmd->s.ioasa.u.gata,\r\nsizeof(struct ipr_ioasa_gata));\r\n}\r\nLEAVE;\r\nreturn IPR_IOASC_SENSE_KEY(ioasc) ? -EIO : 0;\r\n}\r\nstatic int ipr_sata_reset(struct ata_link *link, unsigned int *classes,\r\nunsigned long deadline)\r\n{\r\nstruct ipr_sata_port *sata_port = link->ap->private_data;\r\nstruct ipr_ioa_cfg *ioa_cfg = sata_port->ioa_cfg;\r\nstruct ipr_resource_entry *res;\r\nunsigned long lock_flags = 0;\r\nint rc = -ENXIO, ret;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nwhile (ioa_cfg->in_reset_reload) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\nres = sata_port->res;\r\nif (res) {\r\nrc = ipr_device_reset(ioa_cfg, res);\r\n*classes = res->ata_class;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nret = ipr_wait_for_ops(ioa_cfg, res, ipr_match_res);\r\nif (ret != SUCCESS) {\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_ABBREV);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\n}\r\n} else\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic int __ipr_eh_dev_reset(struct scsi_cmnd *scsi_cmd)\r\n{\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nstruct ipr_resource_entry *res;\r\nstruct ata_port *ap;\r\nint rc = 0, i;\r\nstruct ipr_hrr_queue *hrrq;\r\nENTER;\r\nioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;\r\nres = scsi_cmd->device->hostdata;\r\nif (ioa_cfg->in_reset_reload)\r\nreturn FAILED;\r\nif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead)\r\nreturn FAILED;\r\nfor_each_hrrq(hrrq, ioa_cfg) {\r\nspin_lock(&hrrq->_lock);\r\nfor (i = hrrq->min_cmd_id; i <= hrrq->max_cmd_id; i++) {\r\nipr_cmd = ioa_cfg->ipr_cmnd_list[i];\r\nif (ipr_cmd->ioarcb.res_handle == res->res_handle) {\r\nif (!ipr_cmd->qc)\r\ncontinue;\r\nif (ipr_cmnd_is_free(ipr_cmd))\r\ncontinue;\r\nipr_cmd->done = ipr_sata_eh_done;\r\nif (!(ipr_cmd->qc->flags & ATA_QCFLAG_FAILED)) {\r\nipr_cmd->qc->err_mask |= AC_ERR_TIMEOUT;\r\nipr_cmd->qc->flags |= ATA_QCFLAG_FAILED;\r\n}\r\n}\r\n}\r\nspin_unlock(&hrrq->_lock);\r\n}\r\nres->resetting_device = 1;\r\nscmd_printk(KERN_ERR, scsi_cmd, "Resetting device\n");\r\nif (ipr_is_gata(res) && res->sata_port) {\r\nap = res->sata_port->ap;\r\nspin_unlock_irq(scsi_cmd->device->host->host_lock);\r\nata_std_error_handler(ap);\r\nspin_lock_irq(scsi_cmd->device->host->host_lock);\r\n} else\r\nrc = ipr_device_reset(ioa_cfg, res);\r\nres->resetting_device = 0;\r\nres->reset_occurred = 1;\r\nLEAVE;\r\nreturn rc ? FAILED : SUCCESS;\r\n}\r\nstatic int ipr_eh_dev_reset(struct scsi_cmnd *cmd)\r\n{\r\nint rc;\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nstruct ipr_resource_entry *res;\r\nioa_cfg = (struct ipr_ioa_cfg *) cmd->device->host->hostdata;\r\nres = cmd->device->hostdata;\r\nif (!res)\r\nreturn FAILED;\r\nspin_lock_irq(cmd->device->host->host_lock);\r\nrc = __ipr_eh_dev_reset(cmd);\r\nspin_unlock_irq(cmd->device->host->host_lock);\r\nif (rc == SUCCESS) {\r\nif (ipr_is_gata(res) && res->sata_port)\r\nrc = ipr_wait_for_ops(ioa_cfg, res, ipr_match_res);\r\nelse\r\nrc = ipr_wait_for_ops(ioa_cfg, cmd->device, ipr_match_lun);\r\n}\r\nreturn rc;\r\n}\r\nstatic void ipr_bus_reset_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_resource_entry *res;\r\nENTER;\r\nif (!ioa_cfg->sis64)\r\nlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\r\nif (res->res_handle == ipr_cmd->ioarcb.res_handle) {\r\nscsi_report_bus_reset(ioa_cfg->host, res->bus);\r\nbreak;\r\n}\r\n}\r\nif (ipr_cmd->sibling->sibling)\r\nipr_cmd->sibling->sibling = NULL;\r\nelse\r\nipr_cmd->sibling->done(ipr_cmd->sibling);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nLEAVE;\r\n}\r\nstatic void ipr_abort_timeout(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_cmnd *reset_cmd;\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_cmd_pkt *cmd_pkt;\r\nunsigned long lock_flags = 0;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ipr_cmd->completion.done || ioa_cfg->in_reset_reload) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn;\r\n}\r\nsdev_printk(KERN_ERR, ipr_cmd->u.sdev, "Abort timed out. Resetting bus.\n");\r\nreset_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\r\nipr_cmd->sibling = reset_cmd;\r\nreset_cmd->sibling = ipr_cmd;\r\nreset_cmd->ioarcb.res_handle = ipr_cmd->ioarcb.res_handle;\r\ncmd_pkt = &reset_cmd->ioarcb.cmd_pkt;\r\ncmd_pkt->request_type = IPR_RQTYPE_IOACMD;\r\ncmd_pkt->cdb[0] = IPR_RESET_DEVICE;\r\ncmd_pkt->cdb[2] = IPR_RESET_TYPE_SELECT | IPR_BUS_RESET;\r\nipr_do_req(reset_cmd, ipr_bus_reset_done, ipr_timeout, IPR_DEVICE_RESET_TIMEOUT);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nLEAVE;\r\n}\r\nstatic int ipr_cancel_op(struct scsi_cmnd *scsi_cmd)\r\n{\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nstruct ipr_resource_entry *res;\r\nstruct ipr_cmd_pkt *cmd_pkt;\r\nu32 ioasc, int_reg;\r\nint i, op_found = 0;\r\nstruct ipr_hrr_queue *hrrq;\r\nENTER;\r\nioa_cfg = (struct ipr_ioa_cfg *)scsi_cmd->device->host->hostdata;\r\nres = scsi_cmd->device->hostdata;\r\nif (ioa_cfg->in_reset_reload ||\r\nioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead)\r\nreturn FAILED;\r\nif (!res)\r\nreturn FAILED;\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg);\r\nif (!ipr_is_gscsi(res))\r\nreturn FAILED;\r\nfor_each_hrrq(hrrq, ioa_cfg) {\r\nspin_lock(&hrrq->_lock);\r\nfor (i = hrrq->min_cmd_id; i <= hrrq->max_cmd_id; i++) {\r\nif (ioa_cfg->ipr_cmnd_list[i]->scsi_cmd == scsi_cmd) {\r\nif (!ipr_cmnd_is_free(ioa_cfg->ipr_cmnd_list[i])) {\r\nop_found = 1;\r\nbreak;\r\n}\r\n}\r\n}\r\nspin_unlock(&hrrq->_lock);\r\n}\r\nif (!op_found)\r\nreturn SUCCESS;\r\nipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\r\nipr_cmd->ioarcb.res_handle = res->res_handle;\r\ncmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;\r\ncmd_pkt->request_type = IPR_RQTYPE_IOACMD;\r\ncmd_pkt->cdb[0] = IPR_CANCEL_ALL_REQUESTS;\r\nipr_cmd->u.sdev = scsi_cmd->device;\r\nscmd_printk(KERN_ERR, scsi_cmd, "Aborting command: %02X\n",\r\nscsi_cmd->cmnd[0]);\r\nipr_send_blocking_cmd(ipr_cmd, ipr_abort_timeout, IPR_CANCEL_ALL_TIMEOUT);\r\nioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nif (ioasc == IPR_IOASC_BUS_WAS_RESET || ioasc == IPR_IOASC_SYNC_REQUIRED) {\r\nioasc = 0;\r\nipr_trace;\r\n}\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nif (!ipr_is_naca_model(res))\r\nres->needs_sync_complete = 1;\r\nLEAVE;\r\nreturn IPR_IOASC_SENSE_KEY(ioasc) ? FAILED : SUCCESS;\r\n}\r\nstatic int ipr_scan_finished(struct Scsi_Host *shost, unsigned long elapsed_time)\r\n{\r\nunsigned long lock_flags;\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) shost->hostdata;\r\nint rc = 0;\r\nspin_lock_irqsave(shost->host_lock, lock_flags);\r\nif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead || ioa_cfg->scan_done)\r\nrc = 1;\r\nif ((elapsed_time/HZ) > (ioa_cfg->transop_timeout * 2))\r\nrc = 1;\r\nspin_unlock_irqrestore(shost->host_lock, lock_flags);\r\nreturn rc;\r\n}\r\nstatic int ipr_eh_abort(struct scsi_cmnd *scsi_cmd)\r\n{\r\nunsigned long flags;\r\nint rc;\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nENTER;\r\nioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;\r\nspin_lock_irqsave(scsi_cmd->device->host->host_lock, flags);\r\nrc = ipr_cancel_op(scsi_cmd);\r\nspin_unlock_irqrestore(scsi_cmd->device->host->host_lock, flags);\r\nif (rc == SUCCESS)\r\nrc = ipr_wait_for_ops(ioa_cfg, scsi_cmd->device, ipr_match_lun);\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic irqreturn_t ipr_handle_other_interrupt(struct ipr_ioa_cfg *ioa_cfg,\r\nu32 int_reg)\r\n{\r\nirqreturn_t rc = IRQ_HANDLED;\r\nu32 int_mask_reg;\r\nint_mask_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg32);\r\nint_reg &= ~int_mask_reg;\r\nif ((int_reg & IPR_PCII_OPER_INTERRUPTS) == 0) {\r\nif (ioa_cfg->sis64) {\r\nint_mask_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg) & ~int_mask_reg;\r\nif (int_reg & IPR_PCII_IPL_STAGE_CHANGE) {\r\nwritel(IPR_PCII_IPL_STAGE_CHANGE, ioa_cfg->regs.clr_interrupt_reg);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg) & ~int_mask_reg;\r\nlist_del(&ioa_cfg->reset_cmd->queue);\r\ndel_timer(&ioa_cfg->reset_cmd->timer);\r\nipr_reset_ioa_job(ioa_cfg->reset_cmd);\r\nreturn IRQ_HANDLED;\r\n}\r\n}\r\nreturn IRQ_NONE;\r\n}\r\nif (int_reg & IPR_PCII_IOA_TRANS_TO_OPER) {\r\nwritel(IPR_PCII_IOA_TRANS_TO_OPER, ioa_cfg->regs.set_interrupt_mask_reg);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg);\r\nlist_del(&ioa_cfg->reset_cmd->queue);\r\ndel_timer(&ioa_cfg->reset_cmd->timer);\r\nipr_reset_ioa_job(ioa_cfg->reset_cmd);\r\n} else if ((int_reg & IPR_PCII_HRRQ_UPDATED) == int_reg) {\r\nif (ioa_cfg->clear_isr) {\r\nif (ipr_debug && printk_ratelimit())\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Spurious interrupt detected. 0x%08X\n", int_reg);\r\nwritel(IPR_PCII_HRRQ_UPDATED, ioa_cfg->regs.clr_interrupt_reg32);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\r\nreturn IRQ_NONE;\r\n}\r\n} else {\r\nif (int_reg & IPR_PCII_IOA_UNIT_CHECKED)\r\nioa_cfg->ioa_unit_checked = 1;\r\nelse if (int_reg & IPR_PCII_NO_HOST_RRQ)\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"No Host RRQ. 0x%08X\n", int_reg);\r\nelse\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Permanent IOA failure. 0x%08X\n", int_reg);\r\nif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\r\nioa_cfg->sdt_state = GET_DUMP;\r\nipr_mask_and_clear_interrupts(ioa_cfg, ~0);\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\n}\r\nreturn rc;\r\n}\r\nstatic void ipr_isr_eh(struct ipr_ioa_cfg *ioa_cfg, char *msg, u16 number)\r\n{\r\nioa_cfg->errors_logged++;\r\ndev_err(&ioa_cfg->pdev->dev, "%s %d\n", msg, number);\r\nif (WAIT_FOR_DUMP == ioa_cfg->sdt_state)\r\nioa_cfg->sdt_state = GET_DUMP;\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\n}\r\nstatic int ipr_process_hrrq(struct ipr_hrr_queue *hrr_queue, int budget,\r\nstruct list_head *doneq)\r\n{\r\nu32 ioasc;\r\nu16 cmd_index;\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_ioa_cfg *ioa_cfg = hrr_queue->ioa_cfg;\r\nint num_hrrq = 0;\r\nif (!hrr_queue->allow_interrupts)\r\nreturn 0;\r\nwhile ((be32_to_cpu(*hrr_queue->hrrq_curr) & IPR_HRRQ_TOGGLE_BIT) ==\r\nhrr_queue->toggle_bit) {\r\ncmd_index = (be32_to_cpu(*hrr_queue->hrrq_curr) &\r\nIPR_HRRQ_REQ_RESP_HANDLE_MASK) >>\r\nIPR_HRRQ_REQ_RESP_HANDLE_SHIFT;\r\nif (unlikely(cmd_index > hrr_queue->max_cmd_id ||\r\ncmd_index < hrr_queue->min_cmd_id)) {\r\nipr_isr_eh(ioa_cfg,\r\n"Invalid response handle from IOA: ",\r\ncmd_index);\r\nbreak;\r\n}\r\nipr_cmd = ioa_cfg->ipr_cmnd_list[cmd_index];\r\nioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nipr_trc_hook(ipr_cmd, IPR_TRACE_FINISH, ioasc);\r\nlist_move_tail(&ipr_cmd->queue, doneq);\r\nif (hrr_queue->hrrq_curr < hrr_queue->hrrq_end) {\r\nhrr_queue->hrrq_curr++;\r\n} else {\r\nhrr_queue->hrrq_curr = hrr_queue->hrrq_start;\r\nhrr_queue->toggle_bit ^= 1u;\r\n}\r\nnum_hrrq++;\r\nif (budget > 0 && num_hrrq >= budget)\r\nbreak;\r\n}\r\nreturn num_hrrq;\r\n}\r\nstatic int ipr_iopoll(struct irq_poll *iop, int budget)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nstruct ipr_hrr_queue *hrrq;\r\nstruct ipr_cmnd *ipr_cmd, *temp;\r\nunsigned long hrrq_flags;\r\nint completed_ops;\r\nLIST_HEAD(doneq);\r\nhrrq = container_of(iop, struct ipr_hrr_queue, iopoll);\r\nioa_cfg = hrrq->ioa_cfg;\r\nspin_lock_irqsave(hrrq->lock, hrrq_flags);\r\ncompleted_ops = ipr_process_hrrq(hrrq, budget, &doneq);\r\nif (completed_ops < budget)\r\nirq_poll_complete(iop);\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nlist_for_each_entry_safe(ipr_cmd, temp, &doneq, queue) {\r\nlist_del(&ipr_cmd->queue);\r\ndel_timer(&ipr_cmd->timer);\r\nipr_cmd->fast_done(ipr_cmd);\r\n}\r\nreturn completed_ops;\r\n}\r\nstatic irqreturn_t ipr_isr(int irq, void *devp)\r\n{\r\nstruct ipr_hrr_queue *hrrq = (struct ipr_hrr_queue *)devp;\r\nstruct ipr_ioa_cfg *ioa_cfg = hrrq->ioa_cfg;\r\nunsigned long hrrq_flags = 0;\r\nu32 int_reg = 0;\r\nint num_hrrq = 0;\r\nint irq_none = 0;\r\nstruct ipr_cmnd *ipr_cmd, *temp;\r\nirqreturn_t rc = IRQ_NONE;\r\nLIST_HEAD(doneq);\r\nspin_lock_irqsave(hrrq->lock, hrrq_flags);\r\nif (!hrrq->allow_interrupts) {\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nreturn IRQ_NONE;\r\n}\r\nwhile (1) {\r\nif (ipr_process_hrrq(hrrq, -1, &doneq)) {\r\nrc = IRQ_HANDLED;\r\nif (!ioa_cfg->clear_isr)\r\nbreak;\r\nnum_hrrq = 0;\r\ndo {\r\nwritel(IPR_PCII_HRRQ_UPDATED,\r\nioa_cfg->regs.clr_interrupt_reg32);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\r\n} while (int_reg & IPR_PCII_HRRQ_UPDATED &&\r\nnum_hrrq++ < IPR_MAX_HRRQ_RETRIES);\r\n} else if (rc == IRQ_NONE && irq_none == 0) {\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\r\nirq_none++;\r\n} else if (num_hrrq == IPR_MAX_HRRQ_RETRIES &&\r\nint_reg & IPR_PCII_HRRQ_UPDATED) {\r\nipr_isr_eh(ioa_cfg,\r\n"Error clearing HRRQ: ", num_hrrq);\r\nrc = IRQ_HANDLED;\r\nbreak;\r\n} else\r\nbreak;\r\n}\r\nif (unlikely(rc == IRQ_NONE))\r\nrc = ipr_handle_other_interrupt(ioa_cfg, int_reg);\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nlist_for_each_entry_safe(ipr_cmd, temp, &doneq, queue) {\r\nlist_del(&ipr_cmd->queue);\r\ndel_timer(&ipr_cmd->timer);\r\nipr_cmd->fast_done(ipr_cmd);\r\n}\r\nreturn rc;\r\n}\r\nstatic irqreturn_t ipr_isr_mhrrq(int irq, void *devp)\r\n{\r\nstruct ipr_hrr_queue *hrrq = (struct ipr_hrr_queue *)devp;\r\nstruct ipr_ioa_cfg *ioa_cfg = hrrq->ioa_cfg;\r\nunsigned long hrrq_flags = 0;\r\nstruct ipr_cmnd *ipr_cmd, *temp;\r\nirqreturn_t rc = IRQ_NONE;\r\nLIST_HEAD(doneq);\r\nspin_lock_irqsave(hrrq->lock, hrrq_flags);\r\nif (!hrrq->allow_interrupts) {\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nreturn IRQ_NONE;\r\n}\r\nif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\r\nif ((be32_to_cpu(*hrrq->hrrq_curr) & IPR_HRRQ_TOGGLE_BIT) ==\r\nhrrq->toggle_bit) {\r\nirq_poll_sched(&hrrq->iopoll);\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nreturn IRQ_HANDLED;\r\n}\r\n} else {\r\nif ((be32_to_cpu(*hrrq->hrrq_curr) & IPR_HRRQ_TOGGLE_BIT) ==\r\nhrrq->toggle_bit)\r\nif (ipr_process_hrrq(hrrq, -1, &doneq))\r\nrc = IRQ_HANDLED;\r\n}\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nlist_for_each_entry_safe(ipr_cmd, temp, &doneq, queue) {\r\nlist_del(&ipr_cmd->queue);\r\ndel_timer(&ipr_cmd->timer);\r\nipr_cmd->fast_done(ipr_cmd);\r\n}\r\nreturn rc;\r\n}\r\nstatic int ipr_build_ioadl64(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_cmnd *ipr_cmd)\r\n{\r\nint i, nseg;\r\nstruct scatterlist *sg;\r\nu32 length;\r\nu32 ioadl_flags = 0;\r\nstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_ioadl64_desc *ioadl64 = ipr_cmd->i.ioadl64;\r\nlength = scsi_bufflen(scsi_cmd);\r\nif (!length)\r\nreturn 0;\r\nnseg = scsi_dma_map(scsi_cmd);\r\nif (nseg < 0) {\r\nif (printk_ratelimit())\r\ndev_err(&ioa_cfg->pdev->dev, "scsi_dma_map failed!\n");\r\nreturn -1;\r\n}\r\nipr_cmd->dma_use_sg = nseg;\r\nioarcb->data_transfer_length = cpu_to_be32(length);\r\nioarcb->ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl64_desc) * ipr_cmd->dma_use_sg);\r\nif (scsi_cmd->sc_data_direction == DMA_TO_DEVICE) {\r\nioadl_flags = IPR_IOADL_FLAGS_WRITE;\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\r\n} else if (scsi_cmd->sc_data_direction == DMA_FROM_DEVICE)\r\nioadl_flags = IPR_IOADL_FLAGS_READ;\r\nscsi_for_each_sg(scsi_cmd, sg, ipr_cmd->dma_use_sg, i) {\r\nioadl64[i].flags = cpu_to_be32(ioadl_flags);\r\nioadl64[i].data_len = cpu_to_be32(sg_dma_len(sg));\r\nioadl64[i].address = cpu_to_be64(sg_dma_address(sg));\r\n}\r\nioadl64[i-1].flags |= cpu_to_be32(IPR_IOADL_FLAGS_LAST);\r\nreturn 0;\r\n}\r\nstatic int ipr_build_ioadl(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_cmnd *ipr_cmd)\r\n{\r\nint i, nseg;\r\nstruct scatterlist *sg;\r\nu32 length;\r\nu32 ioadl_flags = 0;\r\nstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_ioadl_desc *ioadl = ipr_cmd->i.ioadl;\r\nlength = scsi_bufflen(scsi_cmd);\r\nif (!length)\r\nreturn 0;\r\nnseg = scsi_dma_map(scsi_cmd);\r\nif (nseg < 0) {\r\ndev_err(&ioa_cfg->pdev->dev, "scsi_dma_map failed!\n");\r\nreturn -1;\r\n}\r\nipr_cmd->dma_use_sg = nseg;\r\nif (scsi_cmd->sc_data_direction == DMA_TO_DEVICE) {\r\nioadl_flags = IPR_IOADL_FLAGS_WRITE;\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\r\nioarcb->data_transfer_length = cpu_to_be32(length);\r\nioarcb->ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);\r\n} else if (scsi_cmd->sc_data_direction == DMA_FROM_DEVICE) {\r\nioadl_flags = IPR_IOADL_FLAGS_READ;\r\nioarcb->read_data_transfer_length = cpu_to_be32(length);\r\nioarcb->read_ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);\r\n}\r\nif (ipr_cmd->dma_use_sg <= ARRAY_SIZE(ioarcb->u.add_data.u.ioadl)) {\r\nioadl = ioarcb->u.add_data.u.ioadl;\r\nioarcb->write_ioadl_addr = cpu_to_be32((ipr_cmd->dma_addr) +\r\noffsetof(struct ipr_ioarcb, u.add_data));\r\nioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;\r\n}\r\nscsi_for_each_sg(scsi_cmd, sg, ipr_cmd->dma_use_sg, i) {\r\nioadl[i].flags_and_data_len =\r\ncpu_to_be32(ioadl_flags | sg_dma_len(sg));\r\nioadl[i].address = cpu_to_be32(sg_dma_address(sg));\r\n}\r\nioadl[i-1].flags_and_data_len |= cpu_to_be32(IPR_IOADL_FLAGS_LAST);\r\nreturn 0;\r\n}\r\nstatic void __ipr_erp_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\r\nstruct ipr_resource_entry *res = scsi_cmd->device->hostdata;\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nif (IPR_IOASC_SENSE_KEY(ioasc) > 0) {\r\nscsi_cmd->result |= (DID_ERROR << 16);\r\nscmd_printk(KERN_ERR, scsi_cmd,\r\n"Request Sense failed with IOASC: 0x%08X\n", ioasc);\r\n} else {\r\nmemcpy(scsi_cmd->sense_buffer, ipr_cmd->sense_buffer,\r\nSCSI_SENSE_BUFFERSIZE);\r\n}\r\nif (res) {\r\nif (!ipr_is_naca_model(res))\r\nres->needs_sync_complete = 1;\r\nres->in_erp = 0;\r\n}\r\nscsi_dma_unmap(ipr_cmd->scsi_cmd);\r\nscsi_cmd->scsi_done(scsi_cmd);\r\nif (ipr_cmd->eh_comp)\r\ncomplete(ipr_cmd->eh_comp);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\n}\r\nstatic void ipr_erp_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_hrr_queue *hrrq = ipr_cmd->hrrq;\r\nunsigned long hrrq_flags;\r\nspin_lock_irqsave(&hrrq->_lock, hrrq_flags);\r\n__ipr_erp_done(ipr_cmd);\r\nspin_unlock_irqrestore(&hrrq->_lock, hrrq_flags);\r\n}\r\nstatic void ipr_reinit_ipr_cmnd_for_erp(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\r\ndma_addr_t dma_addr = ipr_cmd->dma_addr;\r\nmemset(&ioarcb->cmd_pkt, 0, sizeof(struct ipr_cmd_pkt));\r\nioarcb->data_transfer_length = 0;\r\nioarcb->read_data_transfer_length = 0;\r\nioarcb->ioadl_len = 0;\r\nioarcb->read_ioadl_len = 0;\r\nioasa->hdr.ioasc = 0;\r\nioasa->hdr.residual_data_len = 0;\r\nif (ipr_cmd->ioa_cfg->sis64)\r\nioarcb->u.sis64_addr_data.data_ioadl_addr =\r\ncpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, i.ioadl64));\r\nelse {\r\nioarcb->write_ioadl_addr =\r\ncpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, i.ioadl));\r\nioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;\r\n}\r\n}\r\nstatic void __ipr_erp_request_sense(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_cmd_pkt *cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nif (IPR_IOASC_SENSE_KEY(ioasc) > 0) {\r\n__ipr_erp_done(ipr_cmd);\r\nreturn;\r\n}\r\nipr_reinit_ipr_cmnd_for_erp(ipr_cmd);\r\ncmd_pkt->request_type = IPR_RQTYPE_SCSICDB;\r\ncmd_pkt->cdb[0] = REQUEST_SENSE;\r\ncmd_pkt->cdb[4] = SCSI_SENSE_BUFFERSIZE;\r\ncmd_pkt->flags_hi |= IPR_FLAGS_HI_SYNC_OVERRIDE;\r\ncmd_pkt->flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;\r\ncmd_pkt->timeout = cpu_to_be16(IPR_REQUEST_SENSE_TIMEOUT / HZ);\r\nipr_init_ioadl(ipr_cmd, ipr_cmd->sense_buffer_dma,\r\nSCSI_SENSE_BUFFERSIZE, IPR_IOADL_FLAGS_READ_LAST);\r\nipr_do_req(ipr_cmd, ipr_erp_done, ipr_timeout,\r\nIPR_REQUEST_SENSE_TIMEOUT * 2);\r\n}\r\nstatic void ipr_erp_request_sense(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_hrr_queue *hrrq = ipr_cmd->hrrq;\r\nunsigned long hrrq_flags;\r\nspin_lock_irqsave(&hrrq->_lock, hrrq_flags);\r\n__ipr_erp_request_sense(ipr_cmd);\r\nspin_unlock_irqrestore(&hrrq->_lock, hrrq_flags);\r\n}\r\nstatic void ipr_erp_cancel_all(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\r\nstruct ipr_resource_entry *res = scsi_cmd->device->hostdata;\r\nstruct ipr_cmd_pkt *cmd_pkt;\r\nres->in_erp = 1;\r\nipr_reinit_ipr_cmnd_for_erp(ipr_cmd);\r\nif (!scsi_cmd->device->simple_tags) {\r\n__ipr_erp_request_sense(ipr_cmd);\r\nreturn;\r\n}\r\ncmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;\r\ncmd_pkt->request_type = IPR_RQTYPE_IOACMD;\r\ncmd_pkt->cdb[0] = IPR_CANCEL_ALL_REQUESTS;\r\nipr_do_req(ipr_cmd, ipr_erp_request_sense, ipr_timeout,\r\nIPR_CANCEL_ALL_TIMEOUT);\r\n}\r\nstatic void ipr_dump_ioasa(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_cmnd *ipr_cmd, struct ipr_resource_entry *res)\r\n{\r\nint i;\r\nu16 data_len;\r\nu32 ioasc, fd_ioasc;\r\nstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\r\n__be32 *ioasa_data = (__be32 *)ioasa;\r\nint error_index;\r\nioasc = be32_to_cpu(ioasa->hdr.ioasc) & IPR_IOASC_IOASC_MASK;\r\nfd_ioasc = be32_to_cpu(ioasa->hdr.fd_ioasc) & IPR_IOASC_IOASC_MASK;\r\nif (0 == ioasc)\r\nreturn;\r\nif (ioa_cfg->log_level < IPR_DEFAULT_LOG_LEVEL)\r\nreturn;\r\nif (ioasc == IPR_IOASC_BUS_WAS_RESET && fd_ioasc)\r\nerror_index = ipr_get_error(fd_ioasc);\r\nelse\r\nerror_index = ipr_get_error(ioasc);\r\nif (ioa_cfg->log_level < IPR_MAX_LOG_LEVEL) {\r\nif (ioasa->hdr.ilid != 0)\r\nreturn;\r\nif (!ipr_is_gscsi(res))\r\nreturn;\r\nif (ipr_error_table[error_index].log_ioasa == 0)\r\nreturn;\r\n}\r\nipr_res_err(ioa_cfg, res, "%s\n", ipr_error_table[error_index].error);\r\ndata_len = be16_to_cpu(ioasa->hdr.ret_stat_len);\r\nif (ioa_cfg->sis64 && sizeof(struct ipr_ioasa64) < data_len)\r\ndata_len = sizeof(struct ipr_ioasa64);\r\nelse if (!ioa_cfg->sis64 && sizeof(struct ipr_ioasa) < data_len)\r\ndata_len = sizeof(struct ipr_ioasa);\r\nipr_err("IOASA Dump:\n");\r\nfor (i = 0; i < data_len / 4; i += 4) {\r\nipr_err("%08X: %08X %08X %08X %08X\n", i*4,\r\nbe32_to_cpu(ioasa_data[i]),\r\nbe32_to_cpu(ioasa_data[i+1]),\r\nbe32_to_cpu(ioasa_data[i+2]),\r\nbe32_to_cpu(ioasa_data[i+3]));\r\n}\r\n}\r\nstatic void ipr_gen_sense(struct ipr_cmnd *ipr_cmd)\r\n{\r\nu32 failing_lba;\r\nu8 *sense_buf = ipr_cmd->scsi_cmd->sense_buffer;\r\nstruct ipr_resource_entry *res = ipr_cmd->scsi_cmd->device->hostdata;\r\nstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\r\nu32 ioasc = be32_to_cpu(ioasa->hdr.ioasc);\r\nmemset(sense_buf, 0, SCSI_SENSE_BUFFERSIZE);\r\nif (ioasc >= IPR_FIRST_DRIVER_IOASC)\r\nreturn;\r\nipr_cmd->scsi_cmd->result = SAM_STAT_CHECK_CONDITION;\r\nif (ipr_is_vset_device(res) &&\r\nioasc == IPR_IOASC_MED_DO_NOT_REALLOC &&\r\nioasa->u.vset.failing_lba_hi != 0) {\r\nsense_buf[0] = 0x72;\r\nsense_buf[1] = IPR_IOASC_SENSE_KEY(ioasc);\r\nsense_buf[2] = IPR_IOASC_SENSE_CODE(ioasc);\r\nsense_buf[3] = IPR_IOASC_SENSE_QUAL(ioasc);\r\nsense_buf[7] = 12;\r\nsense_buf[8] = 0;\r\nsense_buf[9] = 0x0A;\r\nsense_buf[10] = 0x80;\r\nfailing_lba = be32_to_cpu(ioasa->u.vset.failing_lba_hi);\r\nsense_buf[12] = (failing_lba & 0xff000000) >> 24;\r\nsense_buf[13] = (failing_lba & 0x00ff0000) >> 16;\r\nsense_buf[14] = (failing_lba & 0x0000ff00) >> 8;\r\nsense_buf[15] = failing_lba & 0x000000ff;\r\nfailing_lba = be32_to_cpu(ioasa->u.vset.failing_lba_lo);\r\nsense_buf[16] = (failing_lba & 0xff000000) >> 24;\r\nsense_buf[17] = (failing_lba & 0x00ff0000) >> 16;\r\nsense_buf[18] = (failing_lba & 0x0000ff00) >> 8;\r\nsense_buf[19] = failing_lba & 0x000000ff;\r\n} else {\r\nsense_buf[0] = 0x70;\r\nsense_buf[2] = IPR_IOASC_SENSE_KEY(ioasc);\r\nsense_buf[12] = IPR_IOASC_SENSE_CODE(ioasc);\r\nsense_buf[13] = IPR_IOASC_SENSE_QUAL(ioasc);\r\nif ((IPR_IOASC_SENSE_KEY(ioasc) == 0x05) &&\r\n(be32_to_cpu(ioasa->hdr.ioasc_specific) & IPR_FIELD_POINTER_VALID)) {\r\nsense_buf[7] = 10;\r\nif (IPR_IOASC_SENSE_CODE(ioasc) == 0x24)\r\nsense_buf[15] = 0xC0;\r\nelse\r\nsense_buf[15] = 0x80;\r\nsense_buf[16] =\r\n((IPR_FIELD_POINTER_MASK &\r\nbe32_to_cpu(ioasa->hdr.ioasc_specific)) >> 8) & 0xff;\r\nsense_buf[17] =\r\n(IPR_FIELD_POINTER_MASK &\r\nbe32_to_cpu(ioasa->hdr.ioasc_specific)) & 0xff;\r\n} else {\r\nif (ioasc == IPR_IOASC_MED_DO_NOT_REALLOC) {\r\nif (ipr_is_vset_device(res))\r\nfailing_lba = be32_to_cpu(ioasa->u.vset.failing_lba_lo);\r\nelse\r\nfailing_lba = be32_to_cpu(ioasa->u.dasd.failing_lba);\r\nsense_buf[0] |= 0x80;\r\nsense_buf[3] = (failing_lba & 0xff000000) >> 24;\r\nsense_buf[4] = (failing_lba & 0x00ff0000) >> 16;\r\nsense_buf[5] = (failing_lba & 0x0000ff00) >> 8;\r\nsense_buf[6] = failing_lba & 0x000000ff;\r\n}\r\nsense_buf[7] = 6;\r\n}\r\n}\r\n}\r\nstatic int ipr_get_autosense(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioasa *ioasa = &ipr_cmd->s.ioasa;\r\nstruct ipr_ioasa64 *ioasa64 = &ipr_cmd->s.ioasa64;\r\nif ((be32_to_cpu(ioasa->hdr.ioasc_specific) & IPR_AUTOSENSE_VALID) == 0)\r\nreturn 0;\r\nif (ipr_cmd->ioa_cfg->sis64)\r\nmemcpy(ipr_cmd->scsi_cmd->sense_buffer, ioasa64->auto_sense.data,\r\nmin_t(u16, be16_to_cpu(ioasa64->auto_sense.auto_sense_len),\r\nSCSI_SENSE_BUFFERSIZE));\r\nelse\r\nmemcpy(ipr_cmd->scsi_cmd->sense_buffer, ioasa->auto_sense.data,\r\nmin_t(u16, be16_to_cpu(ioasa->auto_sense.auto_sense_len),\r\nSCSI_SENSE_BUFFERSIZE));\r\nreturn 1;\r\n}\r\nstatic void ipr_erp_start(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\r\nstruct ipr_resource_entry *res = scsi_cmd->device->hostdata;\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nu32 masked_ioasc = ioasc & IPR_IOASC_IOASC_MASK;\r\nif (!res) {\r\n__ipr_scsi_eh_done(ipr_cmd);\r\nreturn;\r\n}\r\nif (!ipr_is_gscsi(res) && masked_ioasc != IPR_IOASC_HW_DEV_BUS_STATUS)\r\nipr_gen_sense(ipr_cmd);\r\nipr_dump_ioasa(ioa_cfg, ipr_cmd, res);\r\nswitch (masked_ioasc) {\r\ncase IPR_IOASC_ABORTED_CMD_TERM_BY_HOST:\r\nif (ipr_is_naca_model(res))\r\nscsi_cmd->result |= (DID_ABORT << 16);\r\nelse\r\nscsi_cmd->result |= (DID_IMM_RETRY << 16);\r\nbreak;\r\ncase IPR_IOASC_IR_RESOURCE_HANDLE:\r\ncase IPR_IOASC_IR_NO_CMDS_TO_2ND_IOA:\r\nscsi_cmd->result |= (DID_NO_CONNECT << 16);\r\nbreak;\r\ncase IPR_IOASC_HW_SEL_TIMEOUT:\r\nscsi_cmd->result |= (DID_NO_CONNECT << 16);\r\nif (!ipr_is_naca_model(res))\r\nres->needs_sync_complete = 1;\r\nbreak;\r\ncase IPR_IOASC_SYNC_REQUIRED:\r\nif (!res->in_erp)\r\nres->needs_sync_complete = 1;\r\nscsi_cmd->result |= (DID_IMM_RETRY << 16);\r\nbreak;\r\ncase IPR_IOASC_MED_DO_NOT_REALLOC:\r\ncase IPR_IOASA_IR_DUAL_IOA_DISABLED:\r\nif (scsi_cmd->result != SAM_STAT_CHECK_CONDITION)\r\nscsi_cmd->result |= (DID_PASSTHROUGH << 16);\r\nbreak;\r\ncase IPR_IOASC_BUS_WAS_RESET:\r\ncase IPR_IOASC_BUS_WAS_RESET_BY_OTHER:\r\nif (!res->resetting_device)\r\nscsi_report_bus_reset(ioa_cfg->host, scsi_cmd->device->channel);\r\nscsi_cmd->result |= (DID_ERROR << 16);\r\nif (!ipr_is_naca_model(res))\r\nres->needs_sync_complete = 1;\r\nbreak;\r\ncase IPR_IOASC_HW_DEV_BUS_STATUS:\r\nscsi_cmd->result |= IPR_IOASC_SENSE_STATUS(ioasc);\r\nif (IPR_IOASC_SENSE_STATUS(ioasc) == SAM_STAT_CHECK_CONDITION) {\r\nif (!ipr_get_autosense(ipr_cmd)) {\r\nif (!ipr_is_naca_model(res)) {\r\nipr_erp_cancel_all(ipr_cmd);\r\nreturn;\r\n}\r\n}\r\n}\r\nif (!ipr_is_naca_model(res))\r\nres->needs_sync_complete = 1;\r\nbreak;\r\ncase IPR_IOASC_NR_INIT_CMD_REQUIRED:\r\nbreak;\r\ncase IPR_IOASC_IR_NON_OPTIMIZED:\r\nif (res->raw_mode) {\r\nres->raw_mode = 0;\r\nscsi_cmd->result |= (DID_IMM_RETRY << 16);\r\n} else\r\nscsi_cmd->result |= (DID_ERROR << 16);\r\nbreak;\r\ndefault:\r\nif (IPR_IOASC_SENSE_KEY(ioasc) > RECOVERED_ERROR)\r\nscsi_cmd->result |= (DID_ERROR << 16);\r\nif (!ipr_is_vset_device(res) && !ipr_is_naca_model(res))\r\nres->needs_sync_complete = 1;\r\nbreak;\r\n}\r\nscsi_dma_unmap(ipr_cmd->scsi_cmd);\r\nscsi_cmd->scsi_done(scsi_cmd);\r\nif (ipr_cmd->eh_comp)\r\ncomplete(ipr_cmd->eh_comp);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\n}\r\nstatic void ipr_scsi_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nunsigned long lock_flags;\r\nscsi_set_resid(scsi_cmd, be32_to_cpu(ipr_cmd->s.ioasa.hdr.residual_data_len));\r\nif (likely(IPR_IOASC_SENSE_KEY(ioasc) == 0)) {\r\nscsi_dma_unmap(scsi_cmd);\r\nspin_lock_irqsave(ipr_cmd->hrrq->lock, lock_flags);\r\nscsi_cmd->scsi_done(scsi_cmd);\r\nif (ipr_cmd->eh_comp)\r\ncomplete(ipr_cmd->eh_comp);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nspin_unlock_irqrestore(ipr_cmd->hrrq->lock, lock_flags);\r\n} else {\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nspin_lock(&ipr_cmd->hrrq->_lock);\r\nipr_erp_start(ioa_cfg, ipr_cmd);\r\nspin_unlock(&ipr_cmd->hrrq->_lock);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\n}\r\nstatic int ipr_queuecommand(struct Scsi_Host *shost,\r\nstruct scsi_cmnd *scsi_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nstruct ipr_resource_entry *res;\r\nstruct ipr_ioarcb *ioarcb;\r\nstruct ipr_cmnd *ipr_cmd;\r\nunsigned long hrrq_flags, lock_flags;\r\nint rc;\r\nstruct ipr_hrr_queue *hrrq;\r\nint hrrq_id;\r\nioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;\r\nscsi_cmd->result = (DID_OK << 16);\r\nres = scsi_cmd->device->hostdata;\r\nif (ipr_is_gata(res) && res->sata_port) {\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nrc = ata_sas_queuecmd(scsi_cmd, res->sata_port->ap);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn rc;\r\n}\r\nhrrq_id = ipr_get_hrrq_index(ioa_cfg);\r\nhrrq = &ioa_cfg->hrrq[hrrq_id];\r\nspin_lock_irqsave(hrrq->lock, hrrq_flags);\r\nif (unlikely(!hrrq->allow_cmds && !hrrq->ioa_is_dead && !hrrq->removing_ioa)) {\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\nif (unlikely(hrrq->ioa_is_dead || hrrq->removing_ioa || !res)) {\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\ngoto err_nodev;\r\n}\r\nipr_cmd = __ipr_get_free_ipr_cmnd(hrrq);\r\nif (ipr_cmd == NULL) {\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nipr_init_ipr_cmnd(ipr_cmd, ipr_scsi_done);\r\nioarcb = &ipr_cmd->ioarcb;\r\nmemcpy(ioarcb->cmd_pkt.cdb, scsi_cmd->cmnd, scsi_cmd->cmd_len);\r\nipr_cmd->scsi_cmd = scsi_cmd;\r\nipr_cmd->done = ipr_scsi_eh_done;\r\nif (ipr_is_gscsi(res)) {\r\nif (scsi_cmd->underflow == 0)\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;\r\nif (res->reset_occurred) {\r\nres->reset_occurred = 0;\r\nioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_DELAY_AFTER_RST;\r\n}\r\n}\r\nif (ipr_is_gscsi(res) || ipr_is_vset_device(res)) {\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_LINK_DESC;\r\nioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_ALIGNED_BFR;\r\nif (scsi_cmd->flags & SCMD_TAGGED)\r\nioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_SIMPLE_TASK;\r\nelse\r\nioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_UNTAGGED_TASK;\r\n}\r\nif (scsi_cmd->cmnd[0] >= 0xC0 &&\r\n(!ipr_is_gscsi(res) || scsi_cmd->cmnd[0] == IPR_QUERY_RSRC_STATE)) {\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\r\n}\r\nif (res->raw_mode && ipr_is_af_dasd_device(res)) {\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_PIPE;\r\nif (scsi_cmd->underflow == 0)\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;\r\n}\r\nif (ioa_cfg->sis64)\r\nrc = ipr_build_ioadl64(ioa_cfg, ipr_cmd);\r\nelse\r\nrc = ipr_build_ioadl(ioa_cfg, ipr_cmd);\r\nspin_lock_irqsave(hrrq->lock, hrrq_flags);\r\nif (unlikely(rc || (!hrrq->allow_cmds && !hrrq->ioa_is_dead))) {\r\nlist_add_tail(&ipr_cmd->queue, &hrrq->hrrq_free_q);\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nif (!rc)\r\nscsi_dma_unmap(scsi_cmd);\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\nif (unlikely(hrrq->ioa_is_dead)) {\r\nlist_add_tail(&ipr_cmd->queue, &hrrq->hrrq_free_q);\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nscsi_dma_unmap(scsi_cmd);\r\ngoto err_nodev;\r\n}\r\nioarcb->res_handle = res->res_handle;\r\nif (res->needs_sync_complete) {\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_SYNC_COMPLETE;\r\nres->needs_sync_complete = 0;\r\n}\r\nlist_add_tail(&ipr_cmd->queue, &hrrq->hrrq_pending_q);\r\nipr_trc_hook(ipr_cmd, IPR_TRACE_START, IPR_GET_RES_PHYS_LOC(res));\r\nipr_send_command(ipr_cmd);\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nreturn 0;\r\nerr_nodev:\r\nspin_lock_irqsave(hrrq->lock, hrrq_flags);\r\nmemset(scsi_cmd->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);\r\nscsi_cmd->result = (DID_NO_CONNECT << 16);\r\nscsi_cmd->scsi_done(scsi_cmd);\r\nspin_unlock_irqrestore(hrrq->lock, hrrq_flags);\r\nreturn 0;\r\n}\r\nstatic int ipr_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\r\n{\r\nstruct ipr_resource_entry *res;\r\nres = (struct ipr_resource_entry *)sdev->hostdata;\r\nif (res && ipr_is_gata(res)) {\r\nif (cmd == HDIO_GET_IDENTITY)\r\nreturn -ENOTTY;\r\nreturn ata_sas_scsi_ioctl(res->sata_port->ap, sdev, cmd, arg);\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic const char *ipr_ioa_info(struct Scsi_Host *host)\r\n{\r\nstatic char buffer[512];\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nunsigned long lock_flags = 0;\r\nioa_cfg = (struct ipr_ioa_cfg *) host->hostdata;\r\nspin_lock_irqsave(host->host_lock, lock_flags);\r\nsprintf(buffer, "IBM %X Storage Adapter", ioa_cfg->type);\r\nspin_unlock_irqrestore(host->host_lock, lock_flags);\r\nreturn buffer;\r\n}\r\nstatic void ipr_ata_phy_reset(struct ata_port *ap)\r\n{\r\nunsigned long flags;\r\nstruct ipr_sata_port *sata_port = ap->private_data;\r\nstruct ipr_resource_entry *res = sata_port->res;\r\nstruct ipr_ioa_cfg *ioa_cfg = sata_port->ioa_cfg;\r\nint rc;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\nwhile (ioa_cfg->in_reset_reload) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\n}\r\nif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].allow_cmds)\r\ngoto out_unlock;\r\nrc = ipr_device_reset(ioa_cfg, res);\r\nif (rc) {\r\nap->link.device[0].class = ATA_DEV_NONE;\r\ngoto out_unlock;\r\n}\r\nap->link.device[0].class = res->ata_class;\r\nif (ap->link.device[0].class == ATA_DEV_UNKNOWN)\r\nap->link.device[0].class = ATA_DEV_NONE;\r\nout_unlock:\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\nLEAVE;\r\n}\r\nstatic void ipr_ata_post_internal(struct ata_queued_cmd *qc)\r\n{\r\nstruct ipr_sata_port *sata_port = qc->ap->private_data;\r\nstruct ipr_ioa_cfg *ioa_cfg = sata_port->ioa_cfg;\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_hrr_queue *hrrq;\r\nunsigned long flags;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\nwhile (ioa_cfg->in_reset_reload) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\n}\r\nfor_each_hrrq(hrrq, ioa_cfg) {\r\nspin_lock(&hrrq->_lock);\r\nlist_for_each_entry(ipr_cmd, &hrrq->hrrq_pending_q, queue) {\r\nif (ipr_cmd->qc == qc) {\r\nipr_device_reset(ioa_cfg, sata_port->res);\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&hrrq->_lock);\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\n}\r\nstatic void ipr_copy_sata_tf(struct ipr_ioarcb_ata_regs *regs,\r\nstruct ata_taskfile *tf)\r\n{\r\nregs->feature = tf->feature;\r\nregs->nsect = tf->nsect;\r\nregs->lbal = tf->lbal;\r\nregs->lbam = tf->lbam;\r\nregs->lbah = tf->lbah;\r\nregs->device = tf->device;\r\nregs->command = tf->command;\r\nregs->hob_feature = tf->hob_feature;\r\nregs->hob_nsect = tf->hob_nsect;\r\nregs->hob_lbal = tf->hob_lbal;\r\nregs->hob_lbam = tf->hob_lbam;\r\nregs->hob_lbah = tf->hob_lbah;\r\nregs->ctl = tf->ctl;\r\n}\r\nstatic void ipr_sata_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ata_queued_cmd *qc = ipr_cmd->qc;\r\nstruct ipr_sata_port *sata_port = qc->ap->private_data;\r\nstruct ipr_resource_entry *res = sata_port->res;\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nspin_lock(&ipr_cmd->hrrq->_lock);\r\nif (ipr_cmd->ioa_cfg->sis64)\r\nmemcpy(&sata_port->ioasa, &ipr_cmd->s.ioasa64.u.gata,\r\nsizeof(struct ipr_ioasa_gata));\r\nelse\r\nmemcpy(&sata_port->ioasa, &ipr_cmd->s.ioasa.u.gata,\r\nsizeof(struct ipr_ioasa_gata));\r\nipr_dump_ioasa(ioa_cfg, ipr_cmd, res);\r\nif (be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc_specific) & IPR_ATA_DEVICE_WAS_RESET)\r\nscsi_report_device_reset(ioa_cfg->host, res->bus, res->target);\r\nif (IPR_IOASC_SENSE_KEY(ioasc) > RECOVERED_ERROR)\r\nqc->err_mask |= __ac_err_mask(sata_port->ioasa.status);\r\nelse\r\nqc->err_mask |= ac_err_mask(sata_port->ioasa.status);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nspin_unlock(&ipr_cmd->hrrq->_lock);\r\nata_qc_complete(qc);\r\n}\r\nstatic void ipr_build_ata_ioadl64(struct ipr_cmnd *ipr_cmd,\r\nstruct ata_queued_cmd *qc)\r\n{\r\nu32 ioadl_flags = 0;\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_ioadl64_desc *ioadl64 = ipr_cmd->i.ata_ioadl.ioadl64;\r\nstruct ipr_ioadl64_desc *last_ioadl64 = NULL;\r\nint len = qc->nbytes;\r\nstruct scatterlist *sg;\r\nunsigned int si;\r\ndma_addr_t dma_addr = ipr_cmd->dma_addr;\r\nif (len == 0)\r\nreturn;\r\nif (qc->dma_dir == DMA_TO_DEVICE) {\r\nioadl_flags = IPR_IOADL_FLAGS_WRITE;\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\r\n} else if (qc->dma_dir == DMA_FROM_DEVICE)\r\nioadl_flags = IPR_IOADL_FLAGS_READ;\r\nioarcb->data_transfer_length = cpu_to_be32(len);\r\nioarcb->ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl64_desc) * ipr_cmd->dma_use_sg);\r\nioarcb->u.sis64_addr_data.data_ioadl_addr =\r\ncpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, i.ata_ioadl.ioadl64));\r\nfor_each_sg(qc->sg, sg, qc->n_elem, si) {\r\nioadl64->flags = cpu_to_be32(ioadl_flags);\r\nioadl64->data_len = cpu_to_be32(sg_dma_len(sg));\r\nioadl64->address = cpu_to_be64(sg_dma_address(sg));\r\nlast_ioadl64 = ioadl64;\r\nioadl64++;\r\n}\r\nif (likely(last_ioadl64))\r\nlast_ioadl64->flags |= cpu_to_be32(IPR_IOADL_FLAGS_LAST);\r\n}\r\nstatic void ipr_build_ata_ioadl(struct ipr_cmnd *ipr_cmd,\r\nstruct ata_queued_cmd *qc)\r\n{\r\nu32 ioadl_flags = 0;\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_ioadl_desc *ioadl = ipr_cmd->i.ioadl;\r\nstruct ipr_ioadl_desc *last_ioadl = NULL;\r\nint len = qc->nbytes;\r\nstruct scatterlist *sg;\r\nunsigned int si;\r\nif (len == 0)\r\nreturn;\r\nif (qc->dma_dir == DMA_TO_DEVICE) {\r\nioadl_flags = IPR_IOADL_FLAGS_WRITE;\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\r\nioarcb->data_transfer_length = cpu_to_be32(len);\r\nioarcb->ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);\r\n} else if (qc->dma_dir == DMA_FROM_DEVICE) {\r\nioadl_flags = IPR_IOADL_FLAGS_READ;\r\nioarcb->read_data_transfer_length = cpu_to_be32(len);\r\nioarcb->read_ioadl_len =\r\ncpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);\r\n}\r\nfor_each_sg(qc->sg, sg, qc->n_elem, si) {\r\nioadl->flags_and_data_len = cpu_to_be32(ioadl_flags | sg_dma_len(sg));\r\nioadl->address = cpu_to_be32(sg_dma_address(sg));\r\nlast_ioadl = ioadl;\r\nioadl++;\r\n}\r\nif (likely(last_ioadl))\r\nlast_ioadl->flags_and_data_len |= cpu_to_be32(IPR_IOADL_FLAGS_LAST);\r\n}\r\nstatic int ipr_qc_defer(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct ipr_sata_port *sata_port = ap->private_data;\r\nstruct ipr_ioa_cfg *ioa_cfg = sata_port->ioa_cfg;\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_hrr_queue *hrrq;\r\nint hrrq_id;\r\nhrrq_id = ipr_get_hrrq_index(ioa_cfg);\r\nhrrq = &ioa_cfg->hrrq[hrrq_id];\r\nqc->lldd_task = NULL;\r\nspin_lock(&hrrq->_lock);\r\nif (unlikely(hrrq->ioa_is_dead)) {\r\nspin_unlock(&hrrq->_lock);\r\nreturn 0;\r\n}\r\nif (unlikely(!hrrq->allow_cmds)) {\r\nspin_unlock(&hrrq->_lock);\r\nreturn ATA_DEFER_LINK;\r\n}\r\nipr_cmd = __ipr_get_free_ipr_cmnd(hrrq);\r\nif (ipr_cmd == NULL) {\r\nspin_unlock(&hrrq->_lock);\r\nreturn ATA_DEFER_LINK;\r\n}\r\nqc->lldd_task = ipr_cmd;\r\nspin_unlock(&hrrq->_lock);\r\nreturn 0;\r\n}\r\nstatic unsigned int ipr_qc_issue(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct ipr_sata_port *sata_port = ap->private_data;\r\nstruct ipr_resource_entry *res = sata_port->res;\r\nstruct ipr_ioa_cfg *ioa_cfg = sata_port->ioa_cfg;\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_ioarcb *ioarcb;\r\nstruct ipr_ioarcb_ata_regs *regs;\r\nif (qc->lldd_task == NULL)\r\nipr_qc_defer(qc);\r\nipr_cmd = qc->lldd_task;\r\nif (ipr_cmd == NULL)\r\nreturn AC_ERR_SYSTEM;\r\nqc->lldd_task = NULL;\r\nspin_lock(&ipr_cmd->hrrq->_lock);\r\nif (unlikely(!ipr_cmd->hrrq->allow_cmds ||\r\nipr_cmd->hrrq->ioa_is_dead)) {\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nspin_unlock(&ipr_cmd->hrrq->_lock);\r\nreturn AC_ERR_SYSTEM;\r\n}\r\nipr_init_ipr_cmnd(ipr_cmd, ipr_lock_and_done);\r\nioarcb = &ipr_cmd->ioarcb;\r\nif (ioa_cfg->sis64) {\r\nregs = &ipr_cmd->i.ata_ioadl.regs;\r\nioarcb->add_cmd_parms_offset = cpu_to_be16(sizeof(*ioarcb));\r\n} else\r\nregs = &ioarcb->u.add_data.u.regs;\r\nmemset(regs, 0, sizeof(*regs));\r\nioarcb->add_cmd_parms_len = cpu_to_be16(sizeof(*regs));\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\r\nipr_cmd->qc = qc;\r\nipr_cmd->done = ipr_sata_done;\r\nipr_cmd->ioarcb.res_handle = res->res_handle;\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_ATA_PASSTHRU;\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_LINK_DESC;\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;\r\nipr_cmd->dma_use_sg = qc->n_elem;\r\nif (ioa_cfg->sis64)\r\nipr_build_ata_ioadl64(ipr_cmd, qc);\r\nelse\r\nipr_build_ata_ioadl(ipr_cmd, qc);\r\nregs->flags |= IPR_ATA_FLAG_STATUS_ON_GOOD_COMPLETION;\r\nipr_copy_sata_tf(regs, &qc->tf);\r\nmemcpy(ioarcb->cmd_pkt.cdb, qc->cdb, IPR_MAX_CDB_LEN);\r\nipr_trc_hook(ipr_cmd, IPR_TRACE_START, IPR_GET_RES_PHYS_LOC(res));\r\nswitch (qc->tf.protocol) {\r\ncase ATA_PROT_NODATA:\r\ncase ATA_PROT_PIO:\r\nbreak;\r\ncase ATA_PROT_DMA:\r\nregs->flags |= IPR_ATA_FLAG_XFER_TYPE_DMA;\r\nbreak;\r\ncase ATAPI_PROT_PIO:\r\ncase ATAPI_PROT_NODATA:\r\nregs->flags |= IPR_ATA_FLAG_PACKET_CMD;\r\nbreak;\r\ncase ATAPI_PROT_DMA:\r\nregs->flags |= IPR_ATA_FLAG_PACKET_CMD;\r\nregs->flags |= IPR_ATA_FLAG_XFER_TYPE_DMA;\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\nspin_unlock(&ipr_cmd->hrrq->_lock);\r\nreturn AC_ERR_INVALID;\r\n}\r\nipr_send_command(ipr_cmd);\r\nspin_unlock(&ipr_cmd->hrrq->_lock);\r\nreturn 0;\r\n}\r\nstatic bool ipr_qc_fill_rtf(struct ata_queued_cmd *qc)\r\n{\r\nstruct ipr_sata_port *sata_port = qc->ap->private_data;\r\nstruct ipr_ioasa_gata *g = &sata_port->ioasa;\r\nstruct ata_taskfile *tf = &qc->result_tf;\r\ntf->feature = g->error;\r\ntf->nsect = g->nsect;\r\ntf->lbal = g->lbal;\r\ntf->lbam = g->lbam;\r\ntf->lbah = g->lbah;\r\ntf->device = g->device;\r\ntf->command = g->status;\r\ntf->hob_nsect = g->hob_nsect;\r\ntf->hob_lbal = g->hob_lbal;\r\ntf->hob_lbam = g->hob_lbam;\r\ntf->hob_lbah = g->hob_lbah;\r\nreturn true;\r\n}\r\nstatic int ipr_invalid_adapter(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nint i;\r\nif ((ioa_cfg->type == 0x5702) && (ioa_cfg->pdev->revision < 4)) {\r\nfor (i = 0; i < ARRAY_SIZE(ipr_blocked_processors); i++) {\r\nif (pvr_version_is(ipr_blocked_processors[i]))\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int ipr_ioa_bringdown_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nint i;\r\nENTER;\r\nif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].removing_ioa) {\r\nipr_trace;\r\nioa_cfg->scsi_unblock = 1;\r\nschedule_work(&ioa_cfg->work_q);\r\n}\r\nioa_cfg->in_reset_reload = 0;\r\nioa_cfg->reset_retries = 0;\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nspin_lock(&ioa_cfg->hrrq[i]._lock);\r\nioa_cfg->hrrq[i].ioa_is_dead = 1;\r\nspin_unlock(&ioa_cfg->hrrq[i]._lock);\r\n}\r\nwmb();\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nwake_up_all(&ioa_cfg->reset_wait_q);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_ioa_reset_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_resource_entry *res;\r\nint j;\r\nENTER;\r\nioa_cfg->in_reset_reload = 0;\r\nfor (j = 0; j < ioa_cfg->hrrq_num; j++) {\r\nspin_lock(&ioa_cfg->hrrq[j]._lock);\r\nioa_cfg->hrrq[j].allow_cmds = 1;\r\nspin_unlock(&ioa_cfg->hrrq[j]._lock);\r\n}\r\nwmb();\r\nioa_cfg->reset_cmd = NULL;\r\nioa_cfg->doorbell |= IPR_RUNTIME_RESET;\r\nlist_for_each_entry(res, &ioa_cfg->used_res_q, queue) {\r\nif (res->add_to_ml || res->del_from_ml) {\r\nipr_trace;\r\nbreak;\r\n}\r\n}\r\nschedule_work(&ioa_cfg->work_q);\r\nfor (j = 0; j < IPR_NUM_HCAMS; j++) {\r\nlist_del_init(&ioa_cfg->hostrcb[j]->queue);\r\nif (j < IPR_NUM_LOG_HCAMS)\r\nipr_send_hcam(ioa_cfg,\r\nIPR_HCAM_CDB_OP_CODE_LOG_DATA,\r\nioa_cfg->hostrcb[j]);\r\nelse\r\nipr_send_hcam(ioa_cfg,\r\nIPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE,\r\nioa_cfg->hostrcb[j]);\r\n}\r\nscsi_report_bus_reset(ioa_cfg->host, IPR_VSET_BUS);\r\ndev_info(&ioa_cfg->pdev->dev, "IOA initialized.\n");\r\nioa_cfg->reset_retries = 0;\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nwake_up_all(&ioa_cfg->reset_wait_q);\r\nioa_cfg->scsi_unblock = 1;\r\nschedule_work(&ioa_cfg->work_q);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic void ipr_set_sup_dev_dflt(struct ipr_supported_device *supported_dev,\r\nstruct ipr_std_inq_vpids *vpids)\r\n{\r\nmemset(supported_dev, 0, sizeof(struct ipr_supported_device));\r\nmemcpy(&supported_dev->vpids, vpids, sizeof(struct ipr_std_inq_vpids));\r\nsupported_dev->num_records = 1;\r\nsupported_dev->data_length =\r\ncpu_to_be16(sizeof(struct ipr_supported_device));\r\nsupported_dev->reserved = 0;\r\n}\r\nstatic int ipr_set_supported_devs(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_supported_device *supp_dev = &ioa_cfg->vpd_cbs->supp_dev;\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_resource_entry *res = ipr_cmd->u.res;\r\nipr_cmd->job_step = ipr_ioa_reset_done;\r\nlist_for_each_entry_continue(res, &ioa_cfg->used_res_q, queue) {\r\nif (!ipr_is_scsi_disk(res))\r\ncontinue;\r\nipr_cmd->u.res = res;\r\nipr_set_sup_dev_dflt(supp_dev, &res->std_inq_data.vpids);\r\nioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\r\nioarcb->cmd_pkt.cdb[0] = IPR_SET_SUPPORTED_DEVICES;\r\nioarcb->cmd_pkt.cdb[1] = IPR_SET_ALL_SUPPORTED_DEVICES;\r\nioarcb->cmd_pkt.cdb[7] = (sizeof(struct ipr_supported_device) >> 8) & 0xff;\r\nioarcb->cmd_pkt.cdb[8] = sizeof(struct ipr_supported_device) & 0xff;\r\nipr_init_ioadl(ipr_cmd,\r\nioa_cfg->vpd_cbs_dma +\r\noffsetof(struct ipr_misc_cbs, supp_dev),\r\nsizeof(struct ipr_supported_device),\r\nIPR_IOADL_FLAGS_WRITE_LAST);\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\r\nIPR_SET_SUP_DEVICE_TIMEOUT);\r\nif (!ioa_cfg->sis64)\r\nipr_cmd->job_step = ipr_set_supported_devs;\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nLEAVE;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic void *ipr_get_mode_page(struct ipr_mode_pages *mode_pages,\r\nu32 page_code, u32 len)\r\n{\r\nstruct ipr_mode_page_hdr *mode_hdr;\r\nu32 page_length;\r\nu32 length;\r\nif (!mode_pages || (mode_pages->hdr.length == 0))\r\nreturn NULL;\r\nlength = (mode_pages->hdr.length + 1) - 4 - mode_pages->hdr.block_desc_len;\r\nmode_hdr = (struct ipr_mode_page_hdr *)\r\n(mode_pages->data + mode_pages->hdr.block_desc_len);\r\nwhile (length) {\r\nif (IPR_GET_MODE_PAGE_CODE(mode_hdr) == page_code) {\r\nif (mode_hdr->page_length >= (len - sizeof(struct ipr_mode_page_hdr)))\r\nreturn mode_hdr;\r\nbreak;\r\n} else {\r\npage_length = (sizeof(struct ipr_mode_page_hdr) +\r\nmode_hdr->page_length);\r\nlength -= page_length;\r\nmode_hdr = (struct ipr_mode_page_hdr *)\r\n((unsigned long)mode_hdr + page_length);\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic void ipr_check_term_power(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_mode_pages *mode_pages)\r\n{\r\nint i;\r\nint entry_length;\r\nstruct ipr_dev_bus_entry *bus;\r\nstruct ipr_mode_page28 *mode_page;\r\nmode_page = ipr_get_mode_page(mode_pages, 0x28,\r\nsizeof(struct ipr_mode_page28));\r\nentry_length = mode_page->entry_length;\r\nbus = mode_page->bus;\r\nfor (i = 0; i < mode_page->num_entries; i++) {\r\nif (bus->flags & IPR_SCSI_ATTR_NO_TERM_PWR) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Term power is absent on scsi bus %d\n",\r\nbus->res_addr.bus);\r\n}\r\nbus = (struct ipr_dev_bus_entry *)((char *)bus + entry_length);\r\n}\r\n}\r\nstatic void ipr_scsi_bus_speed_limit(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nu32 max_xfer_rate;\r\nint i;\r\nfor (i = 0; i < IPR_MAX_NUM_BUSES; i++) {\r\nmax_xfer_rate = ipr_get_max_scsi_speed(ioa_cfg, i,\r\nioa_cfg->bus_attr[i].bus_width);\r\nif (max_xfer_rate < ioa_cfg->bus_attr[i].max_xfer_rate)\r\nioa_cfg->bus_attr[i].max_xfer_rate = max_xfer_rate;\r\n}\r\n}\r\nstatic void ipr_modify_ioafp_mode_page_28(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct ipr_mode_pages *mode_pages)\r\n{\r\nint i, entry_length;\r\nstruct ipr_dev_bus_entry *bus;\r\nstruct ipr_bus_attributes *bus_attr;\r\nstruct ipr_mode_page28 *mode_page;\r\nmode_page = ipr_get_mode_page(mode_pages, 0x28,\r\nsizeof(struct ipr_mode_page28));\r\nentry_length = mode_page->entry_length;\r\nfor (i = 0, bus = mode_page->bus;\r\ni < mode_page->num_entries;\r\ni++, bus = (struct ipr_dev_bus_entry *)((u8 *)bus + entry_length)) {\r\nif (bus->res_addr.bus > IPR_MAX_NUM_BUSES) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Invalid resource address reported: 0x%08X\n",\r\nIPR_GET_PHYS_LOC(bus->res_addr));\r\ncontinue;\r\n}\r\nbus_attr = &ioa_cfg->bus_attr[i];\r\nbus->extended_reset_delay = IPR_EXTENDED_RESET_DELAY;\r\nbus->bus_width = bus_attr->bus_width;\r\nbus->max_xfer_rate = cpu_to_be32(bus_attr->max_xfer_rate);\r\nbus->flags &= ~IPR_SCSI_ATTR_QAS_MASK;\r\nif (bus_attr->qas_enabled)\r\nbus->flags |= IPR_SCSI_ATTR_ENABLE_QAS;\r\nelse\r\nbus->flags |= IPR_SCSI_ATTR_DISABLE_QAS;\r\n}\r\n}\r\nstatic void ipr_build_mode_select(struct ipr_cmnd *ipr_cmd,\r\n__be32 res_handle, u8 parm,\r\ndma_addr_t dma_addr, u8 xfer_len)\r\n{\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nioarcb->res_handle = res_handle;\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;\r\nioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;\r\nioarcb->cmd_pkt.cdb[0] = MODE_SELECT;\r\nioarcb->cmd_pkt.cdb[1] = parm;\r\nioarcb->cmd_pkt.cdb[4] = xfer_len;\r\nipr_init_ioadl(ipr_cmd, dma_addr, xfer_len, IPR_IOADL_FLAGS_WRITE_LAST);\r\n}\r\nstatic int ipr_ioafp_mode_select_page28(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_mode_pages *mode_pages = &ioa_cfg->vpd_cbs->mode_pages;\r\nint length;\r\nENTER;\r\nipr_scsi_bus_speed_limit(ioa_cfg);\r\nipr_check_term_power(ioa_cfg, mode_pages);\r\nipr_modify_ioafp_mode_page_28(ioa_cfg, mode_pages);\r\nlength = mode_pages->hdr.length + 1;\r\nmode_pages->hdr.length = 0;\r\nipr_build_mode_select(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE), 0x11,\r\nioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, mode_pages),\r\nlength);\r\nipr_cmd->job_step = ipr_set_supported_devs;\r\nipr_cmd->u.res = list_entry(ioa_cfg->used_res_q.next,\r\nstruct ipr_resource_entry, queue);\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic void ipr_build_mode_sense(struct ipr_cmnd *ipr_cmd,\r\n__be32 res_handle,\r\nu8 parm, dma_addr_t dma_addr, u8 xfer_len)\r\n{\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nioarcb->res_handle = res_handle;\r\nioarcb->cmd_pkt.cdb[0] = MODE_SENSE;\r\nioarcb->cmd_pkt.cdb[2] = parm;\r\nioarcb->cmd_pkt.cdb[4] = xfer_len;\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;\r\nipr_init_ioadl(ipr_cmd, dma_addr, xfer_len, IPR_IOADL_FLAGS_READ_LAST);\r\n}\r\nstatic int ipr_reset_cmd_failed(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"0x%02X failed with IOASC: 0x%08X\n",\r\nipr_cmd->ioarcb.cmd_pkt.cdb[0], ioasc);\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_reset_mode_sense_failed(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nif (ioasc == IPR_IOASC_IR_INVALID_REQ_TYPE_OR_PKT) {\r\nipr_cmd->job_step = ipr_set_supported_devs;\r\nipr_cmd->u.res = list_entry(ioa_cfg->used_res_q.next,\r\nstruct ipr_resource_entry, queue);\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nreturn ipr_reset_cmd_failed(ipr_cmd);\r\n}\r\nstatic int ipr_ioafp_mode_sense_page28(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nipr_build_mode_sense(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE),\r\n0x28, ioa_cfg->vpd_cbs_dma +\r\noffsetof(struct ipr_misc_cbs, mode_pages),\r\nsizeof(struct ipr_mode_pages));\r\nipr_cmd->job_step = ipr_ioafp_mode_select_page28;\r\nipr_cmd->job_step_failed = ipr_reset_mode_sense_failed;\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_ioafp_mode_select_page24(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_mode_pages *mode_pages = &ioa_cfg->vpd_cbs->mode_pages;\r\nstruct ipr_mode_page24 *mode_page;\r\nint length;\r\nENTER;\r\nmode_page = ipr_get_mode_page(mode_pages, 0x24,\r\nsizeof(struct ipr_mode_page24));\r\nif (mode_page)\r\nmode_page->flags |= IPR_ENABLE_DUAL_IOA_AF;\r\nlength = mode_pages->hdr.length + 1;\r\nmode_pages->hdr.length = 0;\r\nipr_build_mode_select(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE), 0x11,\r\nioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, mode_pages),\r\nlength);\r\nipr_cmd->job_step = ipr_ioafp_mode_sense_page28;\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_reset_mode_sense_page24_failed(struct ipr_cmnd *ipr_cmd)\r\n{\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nif (ioasc == IPR_IOASC_IR_INVALID_REQ_TYPE_OR_PKT) {\r\nipr_cmd->job_step = ipr_ioafp_mode_sense_page28;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nreturn ipr_reset_cmd_failed(ipr_cmd);\r\n}\r\nstatic int ipr_ioafp_mode_sense_page24(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nipr_build_mode_sense(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE),\r\n0x24, ioa_cfg->vpd_cbs_dma +\r\noffsetof(struct ipr_misc_cbs, mode_pages),\r\nsizeof(struct ipr_mode_pages));\r\nipr_cmd->job_step = ipr_ioafp_mode_select_page24;\r\nipr_cmd->job_step_failed = ipr_reset_mode_sense_page24_failed;\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_init_res_table(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_resource_entry *res, *temp;\r\nstruct ipr_config_table_entry_wrapper cfgtew;\r\nint entries, found, flag, i;\r\nLIST_HEAD(old_res);\r\nENTER;\r\nif (ioa_cfg->sis64)\r\nflag = ioa_cfg->u.cfg_table64->hdr64.flags;\r\nelse\r\nflag = ioa_cfg->u.cfg_table->hdr.flags;\r\nif (flag & IPR_UCODE_DOWNLOAD_REQ)\r\ndev_err(&ioa_cfg->pdev->dev, "Microcode download required\n");\r\nlist_for_each_entry_safe(res, temp, &ioa_cfg->used_res_q, queue)\r\nlist_move_tail(&res->queue, &old_res);\r\nif (ioa_cfg->sis64)\r\nentries = be16_to_cpu(ioa_cfg->u.cfg_table64->hdr64.num_entries);\r\nelse\r\nentries = ioa_cfg->u.cfg_table->hdr.num_entries;\r\nfor (i = 0; i < entries; i++) {\r\nif (ioa_cfg->sis64)\r\ncfgtew.u.cfgte64 = &ioa_cfg->u.cfg_table64->dev[i];\r\nelse\r\ncfgtew.u.cfgte = &ioa_cfg->u.cfg_table->dev[i];\r\nfound = 0;\r\nlist_for_each_entry_safe(res, temp, &old_res, queue) {\r\nif (ipr_is_same_device(res, &cfgtew)) {\r\nlist_move_tail(&res->queue, &ioa_cfg->used_res_q);\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nif (list_empty(&ioa_cfg->free_res_q)) {\r\ndev_err(&ioa_cfg->pdev->dev, "Too many devices attached\n");\r\nbreak;\r\n}\r\nfound = 1;\r\nres = list_entry(ioa_cfg->free_res_q.next,\r\nstruct ipr_resource_entry, queue);\r\nlist_move_tail(&res->queue, &ioa_cfg->used_res_q);\r\nipr_init_res_entry(res, &cfgtew);\r\nres->add_to_ml = 1;\r\n} else if (res->sdev && (ipr_is_vset_device(res) || ipr_is_scsi_disk(res)))\r\nres->sdev->allow_restart = 1;\r\nif (found)\r\nipr_update_res_entry(res, &cfgtew);\r\n}\r\nlist_for_each_entry_safe(res, temp, &old_res, queue) {\r\nif (res->sdev) {\r\nres->del_from_ml = 1;\r\nres->res_handle = IPR_INVALID_RES_HANDLE;\r\nlist_move_tail(&res->queue, &ioa_cfg->used_res_q);\r\n}\r\n}\r\nlist_for_each_entry_safe(res, temp, &old_res, queue) {\r\nipr_clear_res_target(res);\r\nlist_move_tail(&res->queue, &ioa_cfg->free_res_q);\r\n}\r\nif (ioa_cfg->dual_raid && ipr_dual_ioa_raid)\r\nipr_cmd->job_step = ipr_ioafp_mode_sense_page24;\r\nelse\r\nipr_cmd->job_step = ipr_ioafp_mode_sense_page28;\r\nLEAVE;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic int ipr_ioafp_query_ioa_cfg(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_inquiry_page3 *ucode_vpd = &ioa_cfg->vpd_cbs->page3_data;\r\nstruct ipr_inquiry_cap *cap = &ioa_cfg->vpd_cbs->cap;\r\nENTER;\r\nif (cap->cap & IPR_CAP_DUAL_IOA_RAID)\r\nioa_cfg->dual_raid = 1;\r\ndev_info(&ioa_cfg->pdev->dev, "Adapter firmware version: %02X%02X%02X%02X\n",\r\nucode_vpd->major_release, ucode_vpd->card_type,\r\nucode_vpd->minor_release[0], ucode_vpd->minor_release[1]);\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\r\nioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\r\nioarcb->cmd_pkt.cdb[0] = IPR_QUERY_IOA_CONFIG;\r\nioarcb->cmd_pkt.cdb[6] = (ioa_cfg->cfg_table_size >> 16) & 0xff;\r\nioarcb->cmd_pkt.cdb[7] = (ioa_cfg->cfg_table_size >> 8) & 0xff;\r\nioarcb->cmd_pkt.cdb[8] = ioa_cfg->cfg_table_size & 0xff;\r\nipr_init_ioadl(ipr_cmd, ioa_cfg->cfg_table_dma, ioa_cfg->cfg_table_size,\r\nIPR_IOADL_FLAGS_READ_LAST);\r\nipr_cmd->job_step = ipr_init_res_table;\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_ioa_service_action_failed(struct ipr_cmnd *ipr_cmd)\r\n{\r\nu32 ioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nif (ioasc == IPR_IOASC_IR_INVALID_REQ_TYPE_OR_PKT)\r\nreturn IPR_RC_JOB_CONTINUE;\r\nreturn ipr_reset_cmd_failed(ipr_cmd);\r\n}\r\nstatic void ipr_build_ioa_service_action(struct ipr_cmnd *ipr_cmd,\r\n__be32 res_handle, u8 sa_code)\r\n{\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nioarcb->res_handle = res_handle;\r\nioarcb->cmd_pkt.cdb[0] = IPR_IOA_SERVICE_ACTION;\r\nioarcb->cmd_pkt.cdb[1] = sa_code;\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\r\n}\r\nstatic int ipr_ioafp_set_caching_parameters(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_inquiry_pageC4 *pageC4 = &ioa_cfg->vpd_cbs->pageC4_data;\r\nENTER;\r\nipr_cmd->job_step = ipr_ioafp_query_ioa_cfg;\r\nif (pageC4->cache_cap[0] & IPR_CAP_SYNC_CACHE) {\r\nipr_build_ioa_service_action(ipr_cmd,\r\ncpu_to_be32(IPR_IOA_RES_HANDLE),\r\nIPR_IOA_SA_CHANGE_CACHE_PARAMS);\r\nioarcb->cmd_pkt.cdb[2] = 0x40;\r\nipr_cmd->job_step_failed = ipr_ioa_service_action_failed;\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\r\nIPR_SET_SUP_DEVICE_TIMEOUT);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nLEAVE;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic void ipr_ioafp_inquiry(struct ipr_cmnd *ipr_cmd, u8 flags, u8 page,\r\ndma_addr_t dma_addr, u8 xfer_len)\r\n{\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nENTER;\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;\r\nioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\r\nioarcb->cmd_pkt.cdb[0] = INQUIRY;\r\nioarcb->cmd_pkt.cdb[1] = flags;\r\nioarcb->cmd_pkt.cdb[2] = page;\r\nioarcb->cmd_pkt.cdb[4] = xfer_len;\r\nipr_init_ioadl(ipr_cmd, dma_addr, xfer_len, IPR_IOADL_FLAGS_READ_LAST);\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);\r\nLEAVE;\r\n}\r\nstatic int ipr_inquiry_page_supported(struct ipr_inquiry_page0 *page0, u8 page)\r\n{\r\nint i;\r\nfor (i = 0; i < min_t(u8, page0->len, IPR_INQUIRY_PAGE0_ENTRIES); i++)\r\nif (page0->page[i] == page)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int ipr_ioafp_pageC4_inquiry(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_inquiry_page0 *page0 = &ioa_cfg->vpd_cbs->page0_data;\r\nstruct ipr_inquiry_pageC4 *pageC4 = &ioa_cfg->vpd_cbs->pageC4_data;\r\nENTER;\r\nipr_cmd->job_step = ipr_ioafp_set_caching_parameters;\r\nmemset(pageC4, 0, sizeof(*pageC4));\r\nif (ipr_inquiry_page_supported(page0, 0xC4)) {\r\nipr_ioafp_inquiry(ipr_cmd, 1, 0xC4,\r\n(ioa_cfg->vpd_cbs_dma\r\n+ offsetof(struct ipr_misc_cbs,\r\npageC4_data)),\r\nsizeof(struct ipr_inquiry_pageC4));\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nLEAVE;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic int ipr_ioafp_cap_inquiry(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_inquiry_page0 *page0 = &ioa_cfg->vpd_cbs->page0_data;\r\nstruct ipr_inquiry_cap *cap = &ioa_cfg->vpd_cbs->cap;\r\nENTER;\r\nipr_cmd->job_step = ipr_ioafp_pageC4_inquiry;\r\nmemset(cap, 0, sizeof(*cap));\r\nif (ipr_inquiry_page_supported(page0, 0xD0)) {\r\nipr_ioafp_inquiry(ipr_cmd, 1, 0xD0,\r\nioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, cap),\r\nsizeof(struct ipr_inquiry_cap));\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nLEAVE;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic int ipr_ioafp_page3_inquiry(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nipr_cmd->job_step = ipr_ioafp_cap_inquiry;\r\nipr_ioafp_inquiry(ipr_cmd, 1, 3,\r\nioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, page3_data),\r\nsizeof(struct ipr_inquiry_page3));\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_ioafp_page0_inquiry(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nchar type[5];\r\nENTER;\r\nmemcpy(type, ioa_cfg->vpd_cbs->ioa_vpd.std_inq_data.vpids.product_id, 4);\r\ntype[4] = '\0';\r\nioa_cfg->type = simple_strtoul((char *)type, NULL, 16);\r\nif (ipr_invalid_adapter(ioa_cfg)) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Adapter not supported in this hardware configuration.\n");\r\nif (!ipr_testmode) {\r\nioa_cfg->reset_retries += IPR_NUM_RESET_RELOAD_RETRIES;\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\nlist_add_tail(&ipr_cmd->queue,\r\n&ioa_cfg->hrrq->hrrq_free_q);\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\n}\r\nipr_cmd->job_step = ipr_ioafp_page3_inquiry;\r\nipr_ioafp_inquiry(ipr_cmd, 1, 0,\r\nioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, page0_data),\r\nsizeof(struct ipr_inquiry_page0));\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_ioafp_std_inquiry(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nipr_cmd->job_step = ipr_ioafp_page0_inquiry;\r\nipr_ioafp_inquiry(ipr_cmd, 0, 0,\r\nioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, ioa_vpd),\r\nsizeof(struct ipr_ioa_vpd));\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_ioafp_identify_hrrq(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;\r\nstruct ipr_hrr_queue *hrrq;\r\nENTER;\r\nipr_cmd->job_step = ipr_ioafp_std_inquiry;\r\nif (ioa_cfg->identify_hrrq_index == 0)\r\ndev_info(&ioa_cfg->pdev->dev, "Starting IOA initialization sequence.\n");\r\nif (ioa_cfg->identify_hrrq_index < ioa_cfg->hrrq_num) {\r\nhrrq = &ioa_cfg->hrrq[ioa_cfg->identify_hrrq_index];\r\nioarcb->cmd_pkt.cdb[0] = IPR_ID_HOST_RR_Q;\r\nioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\r\nioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\r\nif (ioa_cfg->sis64)\r\nioarcb->cmd_pkt.cdb[1] = 0x1;\r\nif (ioa_cfg->nvectors == 1)\r\nioarcb->cmd_pkt.cdb[1] &= ~IPR_ID_HRRQ_SELE_ENABLE;\r\nelse\r\nioarcb->cmd_pkt.cdb[1] |= IPR_ID_HRRQ_SELE_ENABLE;\r\nioarcb->cmd_pkt.cdb[2] =\r\n((u64) hrrq->host_rrq_dma >> 24) & 0xff;\r\nioarcb->cmd_pkt.cdb[3] =\r\n((u64) hrrq->host_rrq_dma >> 16) & 0xff;\r\nioarcb->cmd_pkt.cdb[4] =\r\n((u64) hrrq->host_rrq_dma >> 8) & 0xff;\r\nioarcb->cmd_pkt.cdb[5] =\r\n((u64) hrrq->host_rrq_dma) & 0xff;\r\nioarcb->cmd_pkt.cdb[7] =\r\n((sizeof(u32) * hrrq->size) >> 8) & 0xff;\r\nioarcb->cmd_pkt.cdb[8] =\r\n(sizeof(u32) * hrrq->size) & 0xff;\r\nif (ioarcb->cmd_pkt.cdb[1] & IPR_ID_HRRQ_SELE_ENABLE)\r\nioarcb->cmd_pkt.cdb[9] =\r\nioa_cfg->identify_hrrq_index;\r\nif (ioa_cfg->sis64) {\r\nioarcb->cmd_pkt.cdb[10] =\r\n((u64) hrrq->host_rrq_dma >> 56) & 0xff;\r\nioarcb->cmd_pkt.cdb[11] =\r\n((u64) hrrq->host_rrq_dma >> 48) & 0xff;\r\nioarcb->cmd_pkt.cdb[12] =\r\n((u64) hrrq->host_rrq_dma >> 40) & 0xff;\r\nioarcb->cmd_pkt.cdb[13] =\r\n((u64) hrrq->host_rrq_dma >> 32) & 0xff;\r\n}\r\nif (ioarcb->cmd_pkt.cdb[1] & IPR_ID_HRRQ_SELE_ENABLE)\r\nioarcb->cmd_pkt.cdb[14] =\r\nioa_cfg->identify_hrrq_index;\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\r\nIPR_INTERNAL_TIMEOUT);\r\nif (++ioa_cfg->identify_hrrq_index < ioa_cfg->hrrq_num)\r\nipr_cmd->job_step = ipr_ioafp_identify_hrrq;\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nLEAVE;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic void ipr_reset_timer_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nunsigned long lock_flags = 0;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->reset_cmd == ipr_cmd) {\r\nlist_del(&ipr_cmd->queue);\r\nipr_cmd->done(ipr_cmd);\r\n}\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\nstatic void ipr_reset_start_timer(struct ipr_cmnd *ipr_cmd,\r\nunsigned long timeout)\r\n{\r\nENTER;\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\r\nipr_cmd->done = ipr_reset_ioa_job;\r\nipr_cmd->timer.data = (unsigned long) ipr_cmd;\r\nipr_cmd->timer.expires = jiffies + timeout;\r\nipr_cmd->timer.function = (void (*)(unsigned long))ipr_reset_timer_done;\r\nadd_timer(&ipr_cmd->timer);\r\n}\r\nstatic void ipr_init_ioa_mem(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct ipr_hrr_queue *hrrq;\r\nfor_each_hrrq(hrrq, ioa_cfg) {\r\nspin_lock(&hrrq->_lock);\r\nmemset(hrrq->host_rrq, 0, sizeof(u32) * hrrq->size);\r\nhrrq->hrrq_start = hrrq->host_rrq;\r\nhrrq->hrrq_end = &hrrq->host_rrq[hrrq->size - 1];\r\nhrrq->hrrq_curr = hrrq->hrrq_start;\r\nhrrq->toggle_bit = 1;\r\nspin_unlock(&hrrq->_lock);\r\n}\r\nwmb();\r\nioa_cfg->identify_hrrq_index = 0;\r\nif (ioa_cfg->hrrq_num == 1)\r\natomic_set(&ioa_cfg->hrrq_index, 0);\r\nelse\r\natomic_set(&ioa_cfg->hrrq_index, 1);\r\nmemset(ioa_cfg->u.cfg_table, 0, ioa_cfg->cfg_table_size);\r\n}\r\nstatic int ipr_reset_next_stage(struct ipr_cmnd *ipr_cmd)\r\n{\r\nunsigned long stage, stage_time;\r\nu32 feedback;\r\nvolatile u32 int_reg;\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nu64 maskval = 0;\r\nfeedback = readl(ioa_cfg->regs.init_feedback_reg);\r\nstage = feedback & IPR_IPL_INIT_STAGE_MASK;\r\nstage_time = feedback & IPR_IPL_INIT_STAGE_TIME_MASK;\r\nipr_dbg("IPL stage = 0x%lx, IPL stage time = %ld\n", stage, stage_time);\r\nif (stage_time == 0)\r\nstage_time = IPR_IPL_INIT_DEFAULT_STAGE_TIME;\r\nelse if (stage_time < IPR_IPL_INIT_MIN_STAGE_TIME)\r\nstage_time = IPR_IPL_INIT_MIN_STAGE_TIME;\r\nelse if (stage_time > IPR_LONG_OPERATIONAL_TIMEOUT)\r\nstage_time = IPR_LONG_OPERATIONAL_TIMEOUT;\r\nif (stage == IPR_IPL_INIT_STAGE_UNKNOWN) {\r\nwritel(IPR_PCII_IPL_STAGE_CHANGE, ioa_cfg->regs.set_interrupt_mask_reg);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\r\nstage_time = ioa_cfg->transop_timeout;\r\nipr_cmd->job_step = ipr_ioafp_identify_hrrq;\r\n} else if (stage == IPR_IPL_INIT_STAGE_TRANSOP) {\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\r\nif (int_reg & IPR_PCII_IOA_TRANS_TO_OPER) {\r\nipr_cmd->job_step = ipr_ioafp_identify_hrrq;\r\nmaskval = IPR_PCII_IPL_STAGE_CHANGE;\r\nmaskval = (maskval << 32) | IPR_PCII_IOA_TRANS_TO_OPER;\r\nwriteq(maskval, ioa_cfg->regs.set_interrupt_mask_reg);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\n}\r\nipr_cmd->timer.data = (unsigned long) ipr_cmd;\r\nipr_cmd->timer.expires = jiffies + stage_time * HZ;\r\nipr_cmd->timer.function = (void (*)(unsigned long))ipr_oper_timeout;\r\nipr_cmd->done = ipr_reset_ioa_job;\r\nadd_timer(&ipr_cmd->timer);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_reset_enable_ioa(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nvolatile u32 int_reg;\r\nvolatile u64 maskval;\r\nint i;\r\nENTER;\r\nipr_cmd->job_step = ipr_ioafp_identify_hrrq;\r\nipr_init_ioa_mem(ioa_cfg);\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nspin_lock(&ioa_cfg->hrrq[i]._lock);\r\nioa_cfg->hrrq[i].allow_interrupts = 1;\r\nspin_unlock(&ioa_cfg->hrrq[i]._lock);\r\n}\r\nwmb();\r\nif (ioa_cfg->sis64) {\r\nwritel(IPR_ENDIAN_SWAP_KEY, ioa_cfg->regs.endian_swap_reg);\r\nint_reg = readl(ioa_cfg->regs.endian_swap_reg);\r\n}\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg32);\r\nif (int_reg & IPR_PCII_IOA_TRANS_TO_OPER) {\r\nwritel((IPR_PCII_ERROR_INTERRUPTS | IPR_PCII_HRRQ_UPDATED),\r\nioa_cfg->regs.clr_interrupt_mask_reg32);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nwritel(ioa_cfg->doorbell, ioa_cfg->regs.set_uproc_interrupt_reg32);\r\nif (ioa_cfg->sis64) {\r\nmaskval = IPR_PCII_IPL_STAGE_CHANGE;\r\nmaskval = (maskval << 32) | IPR_PCII_OPER_INTERRUPTS;\r\nwriteq(maskval, ioa_cfg->regs.clr_interrupt_mask_reg);\r\n} else\r\nwritel(IPR_PCII_OPER_INTERRUPTS, ioa_cfg->regs.clr_interrupt_mask_reg32);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\r\ndev_info(&ioa_cfg->pdev->dev, "Initializing IOA.\n");\r\nif (ioa_cfg->sis64) {\r\nipr_cmd->job_step = ipr_reset_next_stage;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nipr_cmd->timer.data = (unsigned long) ipr_cmd;\r\nipr_cmd->timer.expires = jiffies + (ioa_cfg->transop_timeout * HZ);\r\nipr_cmd->timer.function = (void (*)(unsigned long))ipr_oper_timeout;\r\nipr_cmd->done = ipr_reset_ioa_job;\r\nadd_timer(&ipr_cmd->timer);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_reset_wait_for_dump(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nif (ioa_cfg->sdt_state == GET_DUMP)\r\nioa_cfg->sdt_state = WAIT_FOR_DUMP;\r\nelse if (ioa_cfg->sdt_state == READ_DUMP)\r\nioa_cfg->sdt_state = ABORT_DUMP;\r\nioa_cfg->dump_timeout = 1;\r\nipr_cmd->job_step = ipr_reset_alert;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic void ipr_unit_check_no_data(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nioa_cfg->errors_logged++;\r\ndev_err(&ioa_cfg->pdev->dev, "IOA unit check with no data\n");\r\n}\r\nstatic void ipr_get_unit_check_buffer(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nunsigned long mailbox;\r\nstruct ipr_hostrcb *hostrcb;\r\nstruct ipr_uc_sdt sdt;\r\nint rc, length;\r\nu32 ioasc;\r\nmailbox = readl(ioa_cfg->ioa_mailbox);\r\nif (!ioa_cfg->sis64 && !ipr_sdt_is_fmt2(mailbox)) {\r\nipr_unit_check_no_data(ioa_cfg);\r\nreturn;\r\n}\r\nmemset(&sdt, 0, sizeof(struct ipr_uc_sdt));\r\nrc = ipr_get_ldump_data_section(ioa_cfg, mailbox, (__be32 *) &sdt,\r\n(sizeof(struct ipr_uc_sdt)) / sizeof(__be32));\r\nif (rc || !(sdt.entry[0].flags & IPR_SDT_VALID_ENTRY) ||\r\n((be32_to_cpu(sdt.hdr.state) != IPR_FMT3_SDT_READY_TO_USE) &&\r\n(be32_to_cpu(sdt.hdr.state) != IPR_FMT2_SDT_READY_TO_USE))) {\r\nipr_unit_check_no_data(ioa_cfg);\r\nreturn;\r\n}\r\nif (be32_to_cpu(sdt.hdr.state) == IPR_FMT3_SDT_READY_TO_USE)\r\nlength = be32_to_cpu(sdt.entry[0].end_token);\r\nelse\r\nlength = (be32_to_cpu(sdt.entry[0].end_token) -\r\nbe32_to_cpu(sdt.entry[0].start_token)) &\r\nIPR_FMT2_MBX_ADDR_MASK;\r\nhostrcb = list_entry(ioa_cfg->hostrcb_free_q.next,\r\nstruct ipr_hostrcb, queue);\r\nlist_del_init(&hostrcb->queue);\r\nmemset(&hostrcb->hcam, 0, sizeof(hostrcb->hcam));\r\nrc = ipr_get_ldump_data_section(ioa_cfg,\r\nbe32_to_cpu(sdt.entry[0].start_token),\r\n(__be32 *)&hostrcb->hcam,\r\nmin(length, (int)sizeof(hostrcb->hcam)) / sizeof(__be32));\r\nif (!rc) {\r\nipr_handle_log_data(ioa_cfg, hostrcb);\r\nioasc = be32_to_cpu(hostrcb->hcam.u.error.fd_ioasc);\r\nif (ioasc == IPR_IOASC_NR_IOA_RESET_REQUIRED &&\r\nioa_cfg->sdt_state == GET_DUMP)\r\nioa_cfg->sdt_state = WAIT_FOR_DUMP;\r\n} else\r\nipr_unit_check_no_data(ioa_cfg);\r\nlist_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);\r\n}\r\nstatic int ipr_reset_get_unit_check_job(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nioa_cfg->ioa_unit_checked = 0;\r\nipr_get_unit_check_buffer(ioa_cfg);\r\nipr_cmd->job_step = ipr_reset_alert;\r\nipr_reset_start_timer(ipr_cmd, 0);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_dump_mailbox_wait(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nif (ioa_cfg->sdt_state != GET_DUMP)\r\nreturn IPR_RC_JOB_RETURN;\r\nif (!ioa_cfg->sis64 || !ipr_cmd->u.time_left ||\r\n(readl(ioa_cfg->regs.sense_interrupt_reg) &\r\nIPR_PCII_MAILBOX_STABLE)) {\r\nif (!ipr_cmd->u.time_left)\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Timed out waiting for Mailbox register.\n");\r\nioa_cfg->sdt_state = READ_DUMP;\r\nioa_cfg->dump_timeout = 0;\r\nif (ioa_cfg->sis64)\r\nipr_reset_start_timer(ipr_cmd, IPR_SIS64_DUMP_TIMEOUT);\r\nelse\r\nipr_reset_start_timer(ipr_cmd, IPR_SIS32_DUMP_TIMEOUT);\r\nipr_cmd->job_step = ipr_reset_wait_for_dump;\r\nschedule_work(&ioa_cfg->work_q);\r\n} else {\r\nipr_cmd->u.time_left -= IPR_CHECK_FOR_RESET_TIMEOUT;\r\nipr_reset_start_timer(ipr_cmd,\r\nIPR_CHECK_FOR_RESET_TIMEOUT);\r\n}\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_reset_restore_cfg_space(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nu32 int_reg;\r\nENTER;\r\nioa_cfg->pdev->state_saved = true;\r\npci_restore_state(ioa_cfg->pdev);\r\nif (ipr_set_pcix_cmd_reg(ioa_cfg)) {\r\nipr_cmd->s.ioasa.hdr.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nipr_fail_all_ops(ioa_cfg);\r\nif (ioa_cfg->sis64) {\r\nwritel(IPR_ENDIAN_SWAP_KEY, ioa_cfg->regs.endian_swap_reg);\r\nint_reg = readl(ioa_cfg->regs.endian_swap_reg);\r\n}\r\nif (ioa_cfg->ioa_unit_checked) {\r\nif (ioa_cfg->sis64) {\r\nipr_cmd->job_step = ipr_reset_get_unit_check_job;\r\nipr_reset_start_timer(ipr_cmd, IPR_DUMP_DELAY_TIMEOUT);\r\nreturn IPR_RC_JOB_RETURN;\r\n} else {\r\nioa_cfg->ioa_unit_checked = 0;\r\nipr_get_unit_check_buffer(ioa_cfg);\r\nipr_cmd->job_step = ipr_reset_alert;\r\nipr_reset_start_timer(ipr_cmd, 0);\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\n}\r\nif (ioa_cfg->in_ioa_bringdown) {\r\nipr_cmd->job_step = ipr_ioa_bringdown_done;\r\n} else if (ioa_cfg->sdt_state == GET_DUMP) {\r\nipr_cmd->job_step = ipr_dump_mailbox_wait;\r\nipr_cmd->u.time_left = IPR_WAIT_FOR_MAILBOX;\r\n} else {\r\nipr_cmd->job_step = ipr_reset_enable_ioa;\r\n}\r\nLEAVE;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic int ipr_reset_bist_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nif (ioa_cfg->cfg_locked)\r\npci_cfg_access_unlock(ioa_cfg->pdev);\r\nioa_cfg->cfg_locked = 0;\r\nipr_cmd->job_step = ipr_reset_restore_cfg_space;\r\nLEAVE;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic int ipr_reset_start_bist(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nint rc = PCIBIOS_SUCCESSFUL;\r\nENTER;\r\nif (ioa_cfg->ipr_chip->bist_method == IPR_MMIO)\r\nwritel(IPR_UPROCI_SIS64_START_BIST,\r\nioa_cfg->regs.set_uproc_interrupt_reg32);\r\nelse\r\nrc = pci_write_config_byte(ioa_cfg->pdev, PCI_BIST, PCI_BIST_START);\r\nif (rc == PCIBIOS_SUCCESSFUL) {\r\nipr_cmd->job_step = ipr_reset_bist_done;\r\nipr_reset_start_timer(ipr_cmd, IPR_WAIT_FOR_BIST_TIMEOUT);\r\nrc = IPR_RC_JOB_RETURN;\r\n} else {\r\nif (ioa_cfg->cfg_locked)\r\npci_cfg_access_unlock(ipr_cmd->ioa_cfg->pdev);\r\nioa_cfg->cfg_locked = 0;\r\nipr_cmd->s.ioasa.hdr.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);\r\nrc = IPR_RC_JOB_CONTINUE;\r\n}\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic int ipr_reset_slot_reset_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nENTER;\r\nipr_cmd->job_step = ipr_reset_bist_done;\r\nipr_reset_start_timer(ipr_cmd, IPR_WAIT_FOR_BIST_TIMEOUT);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic void ipr_reset_reset_work(struct work_struct *work)\r\n{\r\nstruct ipr_cmnd *ipr_cmd = container_of(work, struct ipr_cmnd, work);\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct pci_dev *pdev = ioa_cfg->pdev;\r\nunsigned long lock_flags = 0;\r\nENTER;\r\npci_set_pcie_reset_state(pdev, pcie_warm_reset);\r\nmsleep(jiffies_to_msecs(IPR_PCI_RESET_TIMEOUT));\r\npci_set_pcie_reset_state(pdev, pcie_deassert_reset);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->reset_cmd == ipr_cmd)\r\nipr_reset_ioa_job(ipr_cmd);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nLEAVE;\r\n}\r\nstatic int ipr_reset_slot_reset(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nINIT_WORK(&ipr_cmd->work, ipr_reset_reset_work);\r\nqueue_work(ioa_cfg->reset_work_q, &ipr_cmd->work);\r\nipr_cmd->job_step = ipr_reset_slot_reset_done;\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_reset_block_config_access_wait(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nint rc = IPR_RC_JOB_CONTINUE;\r\nif (pci_cfg_access_trylock(ioa_cfg->pdev)) {\r\nioa_cfg->cfg_locked = 1;\r\nipr_cmd->job_step = ioa_cfg->reset;\r\n} else {\r\nif (ipr_cmd->u.time_left) {\r\nrc = IPR_RC_JOB_RETURN;\r\nipr_cmd->u.time_left -= IPR_CHECK_FOR_RESET_TIMEOUT;\r\nipr_reset_start_timer(ipr_cmd,\r\nIPR_CHECK_FOR_RESET_TIMEOUT);\r\n} else {\r\nipr_cmd->job_step = ioa_cfg->reset;\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"Timed out waiting to lock config access. Resetting anyway.\n");\r\n}\r\n}\r\nreturn rc;\r\n}\r\nstatic int ipr_reset_block_config_access(struct ipr_cmnd *ipr_cmd)\r\n{\r\nipr_cmd->ioa_cfg->cfg_locked = 0;\r\nipr_cmd->job_step = ipr_reset_block_config_access_wait;\r\nipr_cmd->u.time_left = IPR_WAIT_FOR_RESET_TIMEOUT;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic int ipr_reset_allowed(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nvolatile u32 temp_reg;\r\ntemp_reg = readl(ioa_cfg->regs.sense_interrupt_reg);\r\nreturn ((temp_reg & IPR_PCII_CRITICAL_OPERATION) == 0);\r\n}\r\nstatic int ipr_reset_wait_to_start_bist(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nint rc = IPR_RC_JOB_RETURN;\r\nif (!ipr_reset_allowed(ioa_cfg) && ipr_cmd->u.time_left) {\r\nipr_cmd->u.time_left -= IPR_CHECK_FOR_RESET_TIMEOUT;\r\nipr_reset_start_timer(ipr_cmd, IPR_CHECK_FOR_RESET_TIMEOUT);\r\n} else {\r\nipr_cmd->job_step = ipr_reset_block_config_access;\r\nrc = IPR_RC_JOB_CONTINUE;\r\n}\r\nreturn rc;\r\n}\r\nstatic int ipr_reset_alert(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nu16 cmd_reg;\r\nint rc;\r\nENTER;\r\nrc = pci_read_config_word(ioa_cfg->pdev, PCI_COMMAND, &cmd_reg);\r\nif ((rc == PCIBIOS_SUCCESSFUL) && (cmd_reg & PCI_COMMAND_MEMORY)) {\r\nipr_mask_and_clear_interrupts(ioa_cfg, ~0);\r\nwritel(IPR_UPROCI_RESET_ALERT, ioa_cfg->regs.set_uproc_interrupt_reg32);\r\nipr_cmd->job_step = ipr_reset_wait_to_start_bist;\r\n} else {\r\nipr_cmd->job_step = ipr_reset_block_config_access;\r\n}\r\nipr_cmd->u.time_left = IPR_WAIT_FOR_RESET_TIMEOUT;\r\nipr_reset_start_timer(ipr_cmd, IPR_CHECK_FOR_RESET_TIMEOUT);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_reset_quiesce_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nENTER;\r\nipr_cmd->job_step = ipr_ioa_bringdown_done;\r\nipr_mask_and_clear_interrupts(ioa_cfg, ~IPR_PCII_IOA_TRANS_TO_OPER);\r\nLEAVE;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic int ipr_reset_cancel_hcam_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_cmnd *loop_cmd;\r\nstruct ipr_hrr_queue *hrrq;\r\nint rc = IPR_RC_JOB_CONTINUE;\r\nint count = 0;\r\nENTER;\r\nipr_cmd->job_step = ipr_reset_quiesce_done;\r\nfor_each_hrrq(hrrq, ioa_cfg) {\r\nspin_lock(&hrrq->_lock);\r\nlist_for_each_entry(loop_cmd, &hrrq->hrrq_pending_q, queue) {\r\ncount++;\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nrc = IPR_RC_JOB_RETURN;\r\nbreak;\r\n}\r\nspin_unlock(&hrrq->_lock);\r\nif (count)\r\nbreak;\r\n}\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic int ipr_reset_cancel_hcam(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nint rc = IPR_RC_JOB_CONTINUE;\r\nstruct ipr_cmd_pkt *cmd_pkt;\r\nstruct ipr_cmnd *hcam_cmd;\r\nstruct ipr_hrr_queue *hrrq = &ioa_cfg->hrrq[IPR_INIT_HRRQ];\r\nENTER;\r\nipr_cmd->job_step = ipr_reset_cancel_hcam_done;\r\nif (!hrrq->ioa_is_dead) {\r\nif (!list_empty(&ioa_cfg->hostrcb_pending_q)) {\r\nlist_for_each_entry(hcam_cmd, &hrrq->hrrq_pending_q, queue) {\r\nif (hcam_cmd->ioarcb.cmd_pkt.cdb[0] != IPR_HOST_CONTROLLED_ASYNC)\r\ncontinue;\r\nipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\r\nipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\r\ncmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;\r\ncmd_pkt->request_type = IPR_RQTYPE_IOACMD;\r\ncmd_pkt->cdb[0] = IPR_CANCEL_REQUEST;\r\ncmd_pkt->cdb[1] = IPR_CANCEL_64BIT_IOARCB;\r\ncmd_pkt->cdb[10] = ((u64) hcam_cmd->dma_addr >> 56) & 0xff;\r\ncmd_pkt->cdb[11] = ((u64) hcam_cmd->dma_addr >> 48) & 0xff;\r\ncmd_pkt->cdb[12] = ((u64) hcam_cmd->dma_addr >> 40) & 0xff;\r\ncmd_pkt->cdb[13] = ((u64) hcam_cmd->dma_addr >> 32) & 0xff;\r\ncmd_pkt->cdb[2] = ((u64) hcam_cmd->dma_addr >> 24) & 0xff;\r\ncmd_pkt->cdb[3] = ((u64) hcam_cmd->dma_addr >> 16) & 0xff;\r\ncmd_pkt->cdb[4] = ((u64) hcam_cmd->dma_addr >> 8) & 0xff;\r\ncmd_pkt->cdb[5] = ((u64) hcam_cmd->dma_addr) & 0xff;\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\r\nIPR_CANCEL_TIMEOUT);\r\nrc = IPR_RC_JOB_RETURN;\r\nipr_cmd->job_step = ipr_reset_cancel_hcam;\r\nbreak;\r\n}\r\n}\r\n} else\r\nipr_cmd->job_step = ipr_reset_alert;\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic int ipr_reset_ucode_download_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_sglist *sglist = ioa_cfg->ucode_sglist;\r\ndma_unmap_sg(&ioa_cfg->pdev->dev, sglist->scatterlist,\r\nsglist->num_sg, DMA_TO_DEVICE);\r\nipr_cmd->job_step = ipr_reset_alert;\r\nreturn IPR_RC_JOB_CONTINUE;\r\n}\r\nstatic int ipr_reset_ucode_download(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nstruct ipr_sglist *sglist = ioa_cfg->ucode_sglist;\r\nENTER;\r\nipr_cmd->job_step = ipr_reset_alert;\r\nif (!sglist)\r\nreturn IPR_RC_JOB_CONTINUE;\r\nipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\r\nipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;\r\nipr_cmd->ioarcb.cmd_pkt.cdb[0] = WRITE_BUFFER;\r\nipr_cmd->ioarcb.cmd_pkt.cdb[1] = IPR_WR_BUF_DOWNLOAD_AND_SAVE;\r\nipr_cmd->ioarcb.cmd_pkt.cdb[6] = (sglist->buffer_len & 0xff0000) >> 16;\r\nipr_cmd->ioarcb.cmd_pkt.cdb[7] = (sglist->buffer_len & 0x00ff00) >> 8;\r\nipr_cmd->ioarcb.cmd_pkt.cdb[8] = sglist->buffer_len & 0x0000ff;\r\nif (ioa_cfg->sis64)\r\nipr_build_ucode_ioadl64(ipr_cmd, sglist);\r\nelse\r\nipr_build_ucode_ioadl(ipr_cmd, sglist);\r\nipr_cmd->job_step = ipr_reset_ucode_download_done;\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,\r\nIPR_WRITE_BUFFER_TIMEOUT);\r\nLEAVE;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic int ipr_reset_shutdown_ioa(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nenum ipr_shutdown_type shutdown_type = ipr_cmd->u.shutdown_type;\r\nunsigned long timeout;\r\nint rc = IPR_RC_JOB_CONTINUE;\r\nENTER;\r\nif (shutdown_type == IPR_SHUTDOWN_QUIESCE)\r\nipr_cmd->job_step = ipr_reset_cancel_hcam;\r\nelse if (shutdown_type != IPR_SHUTDOWN_NONE &&\r\n!ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead) {\r\nipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\r\nipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\r\nipr_cmd->ioarcb.cmd_pkt.cdb[0] = IPR_IOA_SHUTDOWN;\r\nipr_cmd->ioarcb.cmd_pkt.cdb[1] = shutdown_type;\r\nif (shutdown_type == IPR_SHUTDOWN_NORMAL)\r\ntimeout = IPR_SHUTDOWN_TIMEOUT;\r\nelse if (shutdown_type == IPR_SHUTDOWN_PREPARE_FOR_NORMAL)\r\ntimeout = IPR_INTERNAL_TIMEOUT;\r\nelse if (ioa_cfg->dual_raid && ipr_dual_ioa_raid)\r\ntimeout = IPR_DUAL_IOA_ABBR_SHUTDOWN_TO;\r\nelse\r\ntimeout = IPR_ABBREV_SHUTDOWN_TIMEOUT;\r\nipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, timeout);\r\nrc = IPR_RC_JOB_RETURN;\r\nipr_cmd->job_step = ipr_reset_ucode_download;\r\n} else\r\nipr_cmd->job_step = ipr_reset_alert;\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic void ipr_reset_ioa_job(struct ipr_cmnd *ipr_cmd)\r\n{\r\nu32 rc, ioasc;\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\ndo {\r\nioasc = be32_to_cpu(ipr_cmd->s.ioasa.hdr.ioasc);\r\nif (ioa_cfg->reset_cmd != ipr_cmd) {\r\nlist_add_tail(&ipr_cmd->queue,\r\n&ipr_cmd->hrrq->hrrq_free_q);\r\nreturn;\r\n}\r\nif (IPR_IOASC_SENSE_KEY(ioasc)) {\r\nrc = ipr_cmd->job_step_failed(ipr_cmd);\r\nif (rc == IPR_RC_JOB_RETURN)\r\nreturn;\r\n}\r\nipr_reinit_ipr_cmnd(ipr_cmd);\r\nipr_cmd->job_step_failed = ipr_reset_cmd_failed;\r\nrc = ipr_cmd->job_step(ipr_cmd);\r\n} while (rc == IPR_RC_JOB_CONTINUE);\r\n}\r\nstatic void _ipr_initiate_ioa_reset(struct ipr_ioa_cfg *ioa_cfg,\r\nint (*job_step) (struct ipr_cmnd *),\r\nenum ipr_shutdown_type shutdown_type)\r\n{\r\nstruct ipr_cmnd *ipr_cmd;\r\nint i;\r\nioa_cfg->in_reset_reload = 1;\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nspin_lock(&ioa_cfg->hrrq[i]._lock);\r\nioa_cfg->hrrq[i].allow_cmds = 0;\r\nspin_unlock(&ioa_cfg->hrrq[i]._lock);\r\n}\r\nwmb();\r\nif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].removing_ioa) {\r\nioa_cfg->scsi_unblock = 0;\r\nioa_cfg->scsi_blocked = 1;\r\nscsi_block_requests(ioa_cfg->host);\r\n}\r\nipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\r\nioa_cfg->reset_cmd = ipr_cmd;\r\nipr_cmd->job_step = job_step;\r\nipr_cmd->u.shutdown_type = shutdown_type;\r\nipr_reset_ioa_job(ipr_cmd);\r\n}\r\nstatic void ipr_initiate_ioa_reset(struct ipr_ioa_cfg *ioa_cfg,\r\nenum ipr_shutdown_type shutdown_type)\r\n{\r\nint i;\r\nif (ioa_cfg->hrrq[IPR_INIT_HRRQ].ioa_is_dead)\r\nreturn;\r\nif (ioa_cfg->in_reset_reload) {\r\nif (ioa_cfg->sdt_state == GET_DUMP)\r\nioa_cfg->sdt_state = WAIT_FOR_DUMP;\r\nelse if (ioa_cfg->sdt_state == READ_DUMP)\r\nioa_cfg->sdt_state = ABORT_DUMP;\r\n}\r\nif (ioa_cfg->reset_retries++ >= IPR_NUM_RESET_RELOAD_RETRIES) {\r\ndev_err(&ioa_cfg->pdev->dev,\r\n"IOA taken offline - error recovery failed\n");\r\nioa_cfg->reset_retries = 0;\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nspin_lock(&ioa_cfg->hrrq[i]._lock);\r\nioa_cfg->hrrq[i].ioa_is_dead = 1;\r\nspin_unlock(&ioa_cfg->hrrq[i]._lock);\r\n}\r\nwmb();\r\nif (ioa_cfg->in_ioa_bringdown) {\r\nioa_cfg->reset_cmd = NULL;\r\nioa_cfg->in_reset_reload = 0;\r\nipr_fail_all_ops(ioa_cfg);\r\nwake_up_all(&ioa_cfg->reset_wait_q);\r\nif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].removing_ioa) {\r\nioa_cfg->scsi_unblock = 1;\r\nschedule_work(&ioa_cfg->work_q);\r\n}\r\nreturn;\r\n} else {\r\nioa_cfg->in_ioa_bringdown = 1;\r\nshutdown_type = IPR_SHUTDOWN_NONE;\r\n}\r\n}\r\n_ipr_initiate_ioa_reset(ioa_cfg, ipr_reset_shutdown_ioa,\r\nshutdown_type);\r\n}\r\nstatic int ipr_reset_freeze(struct ipr_cmnd *ipr_cmd)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;\r\nint i;\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nspin_lock(&ioa_cfg->hrrq[i]._lock);\r\nioa_cfg->hrrq[i].allow_interrupts = 0;\r\nspin_unlock(&ioa_cfg->hrrq[i]._lock);\r\n}\r\nwmb();\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_pending_q);\r\nipr_cmd->done = ipr_reset_ioa_job;\r\nreturn IPR_RC_JOB_RETURN;\r\n}\r\nstatic pci_ers_result_t ipr_pci_mmio_enabled(struct pci_dev *pdev)\r\n{\r\nunsigned long flags = 0;\r\nstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\nif (!ioa_cfg->probe_done)\r\npci_save_state(pdev);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\nreturn PCI_ERS_RESULT_NEED_RESET;\r\n}\r\nstatic void ipr_pci_frozen(struct pci_dev *pdev)\r\n{\r\nunsigned long flags = 0;\r\nstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\nif (ioa_cfg->probe_done)\r\n_ipr_initiate_ioa_reset(ioa_cfg, ipr_reset_freeze, IPR_SHUTDOWN_NONE);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\n}\r\nstatic pci_ers_result_t ipr_pci_slot_reset(struct pci_dev *pdev)\r\n{\r\nunsigned long flags = 0;\r\nstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\nif (ioa_cfg->probe_done) {\r\nif (ioa_cfg->needs_warm_reset)\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\nelse\r\n_ipr_initiate_ioa_reset(ioa_cfg, ipr_reset_restore_cfg_space,\r\nIPR_SHUTDOWN_NONE);\r\n} else\r\nwake_up_all(&ioa_cfg->eeh_wait_q);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\nreturn PCI_ERS_RESULT_RECOVERED;\r\n}\r\nstatic void ipr_pci_perm_failure(struct pci_dev *pdev)\r\n{\r\nunsigned long flags = 0;\r\nstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\r\nint i;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\nif (ioa_cfg->probe_done) {\r\nif (ioa_cfg->sdt_state == WAIT_FOR_DUMP)\r\nioa_cfg->sdt_state = ABORT_DUMP;\r\nioa_cfg->reset_retries = IPR_NUM_RESET_RELOAD_RETRIES - 1;\r\nioa_cfg->in_ioa_bringdown = 1;\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nspin_lock(&ioa_cfg->hrrq[i]._lock);\r\nioa_cfg->hrrq[i].allow_cmds = 0;\r\nspin_unlock(&ioa_cfg->hrrq[i]._lock);\r\n}\r\nwmb();\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\n} else\r\nwake_up_all(&ioa_cfg->eeh_wait_q);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\n}\r\nstatic pci_ers_result_t ipr_pci_error_detected(struct pci_dev *pdev,\r\npci_channel_state_t state)\r\n{\r\nswitch (state) {\r\ncase pci_channel_io_frozen:\r\nipr_pci_frozen(pdev);\r\nreturn PCI_ERS_RESULT_CAN_RECOVER;\r\ncase pci_channel_io_perm_failure:\r\nipr_pci_perm_failure(pdev);\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn PCI_ERS_RESULT_NEED_RESET;\r\n}\r\nstatic int ipr_probe_ioa_part2(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nint rc = 0;\r\nunsigned long host_lock_flags = 0;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);\r\ndev_dbg(&ioa_cfg->pdev->dev, "ioa_cfg adx: 0x%p\n", ioa_cfg);\r\nioa_cfg->probe_done = 1;\r\nif (ioa_cfg->needs_hard_reset) {\r\nioa_cfg->needs_hard_reset = 0;\r\nipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);\r\n} else\r\n_ipr_initiate_ioa_reset(ioa_cfg, ipr_reset_enable_ioa,\r\nIPR_SHUTDOWN_NONE);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic void ipr_free_cmd_blks(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nint i;\r\nif (ioa_cfg->ipr_cmnd_list) {\r\nfor (i = 0; i < IPR_NUM_CMD_BLKS; i++) {\r\nif (ioa_cfg->ipr_cmnd_list[i])\r\ndma_pool_free(ioa_cfg->ipr_cmd_pool,\r\nioa_cfg->ipr_cmnd_list[i],\r\nioa_cfg->ipr_cmnd_list_dma[i]);\r\nioa_cfg->ipr_cmnd_list[i] = NULL;\r\n}\r\n}\r\nif (ioa_cfg->ipr_cmd_pool)\r\ndma_pool_destroy(ioa_cfg->ipr_cmd_pool);\r\nkfree(ioa_cfg->ipr_cmnd_list);\r\nkfree(ioa_cfg->ipr_cmnd_list_dma);\r\nioa_cfg->ipr_cmnd_list = NULL;\r\nioa_cfg->ipr_cmnd_list_dma = NULL;\r\nioa_cfg->ipr_cmd_pool = NULL;\r\n}\r\nstatic void ipr_free_mem(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nint i;\r\nkfree(ioa_cfg->res_entries);\r\ndma_free_coherent(&ioa_cfg->pdev->dev, sizeof(struct ipr_misc_cbs),\r\nioa_cfg->vpd_cbs, ioa_cfg->vpd_cbs_dma);\r\nipr_free_cmd_blks(ioa_cfg);\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++)\r\ndma_free_coherent(&ioa_cfg->pdev->dev,\r\nsizeof(u32) * ioa_cfg->hrrq[i].size,\r\nioa_cfg->hrrq[i].host_rrq,\r\nioa_cfg->hrrq[i].host_rrq_dma);\r\ndma_free_coherent(&ioa_cfg->pdev->dev, ioa_cfg->cfg_table_size,\r\nioa_cfg->u.cfg_table, ioa_cfg->cfg_table_dma);\r\nfor (i = 0; i < IPR_MAX_HCAMS; i++) {\r\ndma_free_coherent(&ioa_cfg->pdev->dev,\r\nsizeof(struct ipr_hostrcb),\r\nioa_cfg->hostrcb[i],\r\nioa_cfg->hostrcb_dma[i]);\r\n}\r\nipr_free_dump(ioa_cfg);\r\nkfree(ioa_cfg->trace);\r\n}\r\nstatic void ipr_free_irqs(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct pci_dev *pdev = ioa_cfg->pdev;\r\nint i;\r\nfor (i = 0; i < ioa_cfg->nvectors; i++)\r\nfree_irq(pci_irq_vector(pdev, i), &ioa_cfg->hrrq[i]);\r\npci_free_irq_vectors(pdev);\r\n}\r\nstatic void ipr_free_all_resources(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct pci_dev *pdev = ioa_cfg->pdev;\r\nENTER;\r\nipr_free_irqs(ioa_cfg);\r\nif (ioa_cfg->reset_work_q)\r\ndestroy_workqueue(ioa_cfg->reset_work_q);\r\niounmap(ioa_cfg->hdw_dma_regs);\r\npci_release_regions(pdev);\r\nipr_free_mem(ioa_cfg);\r\nscsi_host_put(ioa_cfg->host);\r\npci_disable_device(pdev);\r\nLEAVE;\r\n}\r\nstatic int ipr_alloc_cmd_blks(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_ioarcb *ioarcb;\r\ndma_addr_t dma_addr;\r\nint i, entries_each_hrrq, hrrq_id = 0;\r\nioa_cfg->ipr_cmd_pool = dma_pool_create(IPR_NAME, &ioa_cfg->pdev->dev,\r\nsizeof(struct ipr_cmnd), 512, 0);\r\nif (!ioa_cfg->ipr_cmd_pool)\r\nreturn -ENOMEM;\r\nioa_cfg->ipr_cmnd_list = kcalloc(IPR_NUM_CMD_BLKS, sizeof(struct ipr_cmnd *), GFP_KERNEL);\r\nioa_cfg->ipr_cmnd_list_dma = kcalloc(IPR_NUM_CMD_BLKS, sizeof(dma_addr_t), GFP_KERNEL);\r\nif (!ioa_cfg->ipr_cmnd_list || !ioa_cfg->ipr_cmnd_list_dma) {\r\nipr_free_cmd_blks(ioa_cfg);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nif (ioa_cfg->hrrq_num > 1) {\r\nif (i == 0) {\r\nentries_each_hrrq = IPR_NUM_INTERNAL_CMD_BLKS;\r\nioa_cfg->hrrq[i].min_cmd_id = 0;\r\nioa_cfg->hrrq[i].max_cmd_id =\r\n(entries_each_hrrq - 1);\r\n} else {\r\nentries_each_hrrq =\r\nIPR_NUM_BASE_CMD_BLKS/\r\n(ioa_cfg->hrrq_num - 1);\r\nioa_cfg->hrrq[i].min_cmd_id =\r\nIPR_NUM_INTERNAL_CMD_BLKS +\r\n(i - 1) * entries_each_hrrq;\r\nioa_cfg->hrrq[i].max_cmd_id =\r\n(IPR_NUM_INTERNAL_CMD_BLKS +\r\ni * entries_each_hrrq - 1);\r\n}\r\n} else {\r\nentries_each_hrrq = IPR_NUM_CMD_BLKS;\r\nioa_cfg->hrrq[i].min_cmd_id = 0;\r\nioa_cfg->hrrq[i].max_cmd_id = (entries_each_hrrq - 1);\r\n}\r\nioa_cfg->hrrq[i].size = entries_each_hrrq;\r\n}\r\nBUG_ON(ioa_cfg->hrrq_num == 0);\r\ni = IPR_NUM_CMD_BLKS -\r\nioa_cfg->hrrq[ioa_cfg->hrrq_num - 1].max_cmd_id - 1;\r\nif (i > 0) {\r\nioa_cfg->hrrq[ioa_cfg->hrrq_num - 1].size += i;\r\nioa_cfg->hrrq[ioa_cfg->hrrq_num - 1].max_cmd_id += i;\r\n}\r\nfor (i = 0; i < IPR_NUM_CMD_BLKS; i++) {\r\nipr_cmd = dma_pool_alloc(ioa_cfg->ipr_cmd_pool, GFP_KERNEL, &dma_addr);\r\nif (!ipr_cmd) {\r\nipr_free_cmd_blks(ioa_cfg);\r\nreturn -ENOMEM;\r\n}\r\nmemset(ipr_cmd, 0, sizeof(*ipr_cmd));\r\nioa_cfg->ipr_cmnd_list[i] = ipr_cmd;\r\nioa_cfg->ipr_cmnd_list_dma[i] = dma_addr;\r\nioarcb = &ipr_cmd->ioarcb;\r\nipr_cmd->dma_addr = dma_addr;\r\nif (ioa_cfg->sis64)\r\nioarcb->a.ioarcb_host_pci_addr64 = cpu_to_be64(dma_addr);\r\nelse\r\nioarcb->a.ioarcb_host_pci_addr = cpu_to_be32(dma_addr);\r\nioarcb->host_response_handle = cpu_to_be32(i << 2);\r\nif (ioa_cfg->sis64) {\r\nioarcb->u.sis64_addr_data.data_ioadl_addr =\r\ncpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, i.ioadl64));\r\nioarcb->u.sis64_addr_data.ioasa_host_pci_addr =\r\ncpu_to_be64(dma_addr + offsetof(struct ipr_cmnd, s.ioasa64));\r\n} else {\r\nioarcb->write_ioadl_addr =\r\ncpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, i.ioadl));\r\nioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;\r\nioarcb->ioasa_host_pci_addr =\r\ncpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, s.ioasa));\r\n}\r\nioarcb->ioasa_len = cpu_to_be16(sizeof(struct ipr_ioasa));\r\nipr_cmd->cmd_index = i;\r\nipr_cmd->ioa_cfg = ioa_cfg;\r\nipr_cmd->sense_buffer_dma = dma_addr +\r\noffsetof(struct ipr_cmnd, sense_buffer);\r\nipr_cmd->ioarcb.cmd_pkt.hrrq_id = hrrq_id;\r\nipr_cmd->hrrq = &ioa_cfg->hrrq[hrrq_id];\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\nif (i >= ioa_cfg->hrrq[hrrq_id].max_cmd_id)\r\nhrrq_id++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ipr_alloc_mem(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct pci_dev *pdev = ioa_cfg->pdev;\r\nint i, rc = -ENOMEM;\r\nENTER;\r\nioa_cfg->res_entries = kzalloc(sizeof(struct ipr_resource_entry) *\r\nioa_cfg->max_devs_supported, GFP_KERNEL);\r\nif (!ioa_cfg->res_entries)\r\ngoto out;\r\nfor (i = 0; i < ioa_cfg->max_devs_supported; i++) {\r\nlist_add_tail(&ioa_cfg->res_entries[i].queue, &ioa_cfg->free_res_q);\r\nioa_cfg->res_entries[i].ioa_cfg = ioa_cfg;\r\n}\r\nioa_cfg->vpd_cbs = dma_alloc_coherent(&pdev->dev,\r\nsizeof(struct ipr_misc_cbs),\r\n&ioa_cfg->vpd_cbs_dma,\r\nGFP_KERNEL);\r\nif (!ioa_cfg->vpd_cbs)\r\ngoto out_free_res_entries;\r\nif (ipr_alloc_cmd_blks(ioa_cfg))\r\ngoto out_free_vpd_cbs;\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nioa_cfg->hrrq[i].host_rrq = dma_alloc_coherent(&pdev->dev,\r\nsizeof(u32) * ioa_cfg->hrrq[i].size,\r\n&ioa_cfg->hrrq[i].host_rrq_dma,\r\nGFP_KERNEL);\r\nif (!ioa_cfg->hrrq[i].host_rrq) {\r\nwhile (--i > 0)\r\ndma_free_coherent(&pdev->dev,\r\nsizeof(u32) * ioa_cfg->hrrq[i].size,\r\nioa_cfg->hrrq[i].host_rrq,\r\nioa_cfg->hrrq[i].host_rrq_dma);\r\ngoto out_ipr_free_cmd_blocks;\r\n}\r\nioa_cfg->hrrq[i].ioa_cfg = ioa_cfg;\r\n}\r\nioa_cfg->u.cfg_table = dma_alloc_coherent(&pdev->dev,\r\nioa_cfg->cfg_table_size,\r\n&ioa_cfg->cfg_table_dma,\r\nGFP_KERNEL);\r\nif (!ioa_cfg->u.cfg_table)\r\ngoto out_free_host_rrq;\r\nfor (i = 0; i < IPR_MAX_HCAMS; i++) {\r\nioa_cfg->hostrcb[i] = dma_alloc_coherent(&pdev->dev,\r\nsizeof(struct ipr_hostrcb),\r\n&ioa_cfg->hostrcb_dma[i],\r\nGFP_KERNEL);\r\nif (!ioa_cfg->hostrcb[i])\r\ngoto out_free_hostrcb_dma;\r\nioa_cfg->hostrcb[i]->hostrcb_dma =\r\nioa_cfg->hostrcb_dma[i] + offsetof(struct ipr_hostrcb, hcam);\r\nioa_cfg->hostrcb[i]->ioa_cfg = ioa_cfg;\r\nlist_add_tail(&ioa_cfg->hostrcb[i]->queue, &ioa_cfg->hostrcb_free_q);\r\n}\r\nioa_cfg->trace = kzalloc(sizeof(struct ipr_trace_entry) *\r\nIPR_NUM_TRACE_ENTRIES, GFP_KERNEL);\r\nif (!ioa_cfg->trace)\r\ngoto out_free_hostrcb_dma;\r\nrc = 0;\r\nout:\r\nLEAVE;\r\nreturn rc;\r\nout_free_hostrcb_dma:\r\nwhile (i-- > 0) {\r\ndma_free_coherent(&pdev->dev, sizeof(struct ipr_hostrcb),\r\nioa_cfg->hostrcb[i],\r\nioa_cfg->hostrcb_dma[i]);\r\n}\r\ndma_free_coherent(&pdev->dev, ioa_cfg->cfg_table_size,\r\nioa_cfg->u.cfg_table, ioa_cfg->cfg_table_dma);\r\nout_free_host_rrq:\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\ndma_free_coherent(&pdev->dev,\r\nsizeof(u32) * ioa_cfg->hrrq[i].size,\r\nioa_cfg->hrrq[i].host_rrq,\r\nioa_cfg->hrrq[i].host_rrq_dma);\r\n}\r\nout_ipr_free_cmd_blocks:\r\nipr_free_cmd_blks(ioa_cfg);\r\nout_free_vpd_cbs:\r\ndma_free_coherent(&pdev->dev, sizeof(struct ipr_misc_cbs),\r\nioa_cfg->vpd_cbs, ioa_cfg->vpd_cbs_dma);\r\nout_free_res_entries:\r\nkfree(ioa_cfg->res_entries);\r\ngoto out;\r\n}\r\nstatic void ipr_initialize_bus_attr(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nint i;\r\nfor (i = 0; i < IPR_MAX_NUM_BUSES; i++) {\r\nioa_cfg->bus_attr[i].bus = i;\r\nioa_cfg->bus_attr[i].qas_enabled = 0;\r\nioa_cfg->bus_attr[i].bus_width = IPR_DEFAULT_BUS_WIDTH;\r\nif (ipr_max_speed < ARRAY_SIZE(ipr_max_bus_speeds))\r\nioa_cfg->bus_attr[i].max_xfer_rate = ipr_max_bus_speeds[ipr_max_speed];\r\nelse\r\nioa_cfg->bus_attr[i].max_xfer_rate = IPR_U160_SCSI_RATE;\r\n}\r\n}\r\nstatic void ipr_init_regs(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nconst struct ipr_interrupt_offsets *p;\r\nstruct ipr_interrupts *t;\r\nvoid __iomem *base;\r\np = &ioa_cfg->chip_cfg->regs;\r\nt = &ioa_cfg->regs;\r\nbase = ioa_cfg->hdw_dma_regs;\r\nt->set_interrupt_mask_reg = base + p->set_interrupt_mask_reg;\r\nt->clr_interrupt_mask_reg = base + p->clr_interrupt_mask_reg;\r\nt->clr_interrupt_mask_reg32 = base + p->clr_interrupt_mask_reg32;\r\nt->sense_interrupt_mask_reg = base + p->sense_interrupt_mask_reg;\r\nt->sense_interrupt_mask_reg32 = base + p->sense_interrupt_mask_reg32;\r\nt->clr_interrupt_reg = base + p->clr_interrupt_reg;\r\nt->clr_interrupt_reg32 = base + p->clr_interrupt_reg32;\r\nt->sense_interrupt_reg = base + p->sense_interrupt_reg;\r\nt->sense_interrupt_reg32 = base + p->sense_interrupt_reg32;\r\nt->ioarrin_reg = base + p->ioarrin_reg;\r\nt->sense_uproc_interrupt_reg = base + p->sense_uproc_interrupt_reg;\r\nt->sense_uproc_interrupt_reg32 = base + p->sense_uproc_interrupt_reg32;\r\nt->set_uproc_interrupt_reg = base + p->set_uproc_interrupt_reg;\r\nt->set_uproc_interrupt_reg32 = base + p->set_uproc_interrupt_reg32;\r\nt->clr_uproc_interrupt_reg = base + p->clr_uproc_interrupt_reg;\r\nt->clr_uproc_interrupt_reg32 = base + p->clr_uproc_interrupt_reg32;\r\nif (ioa_cfg->sis64) {\r\nt->init_feedback_reg = base + p->init_feedback_reg;\r\nt->dump_addr_reg = base + p->dump_addr_reg;\r\nt->dump_data_reg = base + p->dump_data_reg;\r\nt->endian_swap_reg = base + p->endian_swap_reg;\r\n}\r\n}\r\nstatic void ipr_init_ioa_cfg(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct Scsi_Host *host, struct pci_dev *pdev)\r\n{\r\nint i;\r\nioa_cfg->host = host;\r\nioa_cfg->pdev = pdev;\r\nioa_cfg->log_level = ipr_log_level;\r\nioa_cfg->doorbell = IPR_DOORBELL;\r\nsprintf(ioa_cfg->eye_catcher, IPR_EYECATCHER);\r\nsprintf(ioa_cfg->trace_start, IPR_TRACE_START_LABEL);\r\nsprintf(ioa_cfg->cfg_table_start, IPR_CFG_TBL_START);\r\nsprintf(ioa_cfg->resource_table_label, IPR_RES_TABLE_LABEL);\r\nsprintf(ioa_cfg->ipr_hcam_label, IPR_HCAM_LABEL);\r\nsprintf(ioa_cfg->ipr_cmd_label, IPR_CMD_LABEL);\r\nINIT_LIST_HEAD(&ioa_cfg->hostrcb_free_q);\r\nINIT_LIST_HEAD(&ioa_cfg->hostrcb_pending_q);\r\nINIT_LIST_HEAD(&ioa_cfg->hostrcb_report_q);\r\nINIT_LIST_HEAD(&ioa_cfg->free_res_q);\r\nINIT_LIST_HEAD(&ioa_cfg->used_res_q);\r\nINIT_WORK(&ioa_cfg->work_q, ipr_worker_thread);\r\ninit_waitqueue_head(&ioa_cfg->reset_wait_q);\r\ninit_waitqueue_head(&ioa_cfg->msi_wait_q);\r\ninit_waitqueue_head(&ioa_cfg->eeh_wait_q);\r\nioa_cfg->sdt_state = INACTIVE;\r\nipr_initialize_bus_attr(ioa_cfg);\r\nioa_cfg->max_devs_supported = ipr_max_devs;\r\nif (ioa_cfg->sis64) {\r\nhost->max_id = IPR_MAX_SIS64_TARGETS_PER_BUS;\r\nhost->max_lun = IPR_MAX_SIS64_LUNS_PER_TARGET;\r\nif (ipr_max_devs > IPR_MAX_SIS64_DEVS)\r\nioa_cfg->max_devs_supported = IPR_MAX_SIS64_DEVS;\r\nioa_cfg->cfg_table_size = (sizeof(struct ipr_config_table_hdr64)\r\n+ ((sizeof(struct ipr_config_table_entry64)\r\n* ioa_cfg->max_devs_supported)));\r\n} else {\r\nhost->max_id = IPR_MAX_NUM_TARGETS_PER_BUS;\r\nhost->max_lun = IPR_MAX_NUM_LUNS_PER_TARGET;\r\nif (ipr_max_devs > IPR_MAX_PHYSICAL_DEVS)\r\nioa_cfg->max_devs_supported = IPR_MAX_PHYSICAL_DEVS;\r\nioa_cfg->cfg_table_size = (sizeof(struct ipr_config_table_hdr)\r\n+ ((sizeof(struct ipr_config_table_entry)\r\n* ioa_cfg->max_devs_supported)));\r\n}\r\nhost->max_channel = IPR_VSET_BUS;\r\nhost->unique_id = host->host_no;\r\nhost->max_cmd_len = IPR_MAX_CDB_LEN;\r\nhost->can_queue = ioa_cfg->max_cmds;\r\npci_set_drvdata(pdev, ioa_cfg);\r\nfor (i = 0; i < ARRAY_SIZE(ioa_cfg->hrrq); i++) {\r\nINIT_LIST_HEAD(&ioa_cfg->hrrq[i].hrrq_free_q);\r\nINIT_LIST_HEAD(&ioa_cfg->hrrq[i].hrrq_pending_q);\r\nspin_lock_init(&ioa_cfg->hrrq[i]._lock);\r\nif (i == 0)\r\nioa_cfg->hrrq[i].lock = ioa_cfg->host->host_lock;\r\nelse\r\nioa_cfg->hrrq[i].lock = &ioa_cfg->hrrq[i]._lock;\r\n}\r\n}\r\nstatic const struct ipr_chip_t *\r\nipr_get_chip_info(const struct pci_device_id *dev_id)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(ipr_chip); i++)\r\nif (ipr_chip[i].vendor == dev_id->vendor &&\r\nipr_chip[i].device == dev_id->device)\r\nreturn &ipr_chip[i];\r\nreturn NULL;\r\n}\r\nstatic void ipr_wait_for_pci_err_recovery(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nstruct pci_dev *pdev = ioa_cfg->pdev;\r\nif (pci_channel_offline(pdev)) {\r\nwait_event_timeout(ioa_cfg->eeh_wait_q,\r\n!pci_channel_offline(pdev),\r\nIPR_PCI_ERROR_RECOVERY_TIMEOUT);\r\npci_restore_state(pdev);\r\n}\r\n}\r\nstatic void name_msi_vectors(struct ipr_ioa_cfg *ioa_cfg)\r\n{\r\nint vec_idx, n = sizeof(ioa_cfg->vectors_info[0].desc) - 1;\r\nfor (vec_idx = 0; vec_idx < ioa_cfg->nvectors; vec_idx++) {\r\nsnprintf(ioa_cfg->vectors_info[vec_idx].desc, n,\r\n"host%d-%d", ioa_cfg->host->host_no, vec_idx);\r\nioa_cfg->vectors_info[vec_idx].\r\ndesc[strlen(ioa_cfg->vectors_info[vec_idx].desc)] = 0;\r\n}\r\n}\r\nstatic int ipr_request_other_msi_irqs(struct ipr_ioa_cfg *ioa_cfg,\r\nstruct pci_dev *pdev)\r\n{\r\nint i, rc;\r\nfor (i = 1; i < ioa_cfg->nvectors; i++) {\r\nrc = request_irq(pci_irq_vector(pdev, i),\r\nipr_isr_mhrrq,\r\n0,\r\nioa_cfg->vectors_info[i].desc,\r\n&ioa_cfg->hrrq[i]);\r\nif (rc) {\r\nwhile (--i >= 0)\r\nfree_irq(pci_irq_vector(pdev, i),\r\n&ioa_cfg->hrrq[i]);\r\nreturn rc;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic irqreturn_t ipr_test_intr(int irq, void *devp)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)devp;\r\nunsigned long lock_flags = 0;\r\nirqreturn_t rc = IRQ_HANDLED;\r\ndev_info(&ioa_cfg->pdev->dev, "Received IRQ : %d\n", irq);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nioa_cfg->msi_received = 1;\r\nwake_up(&ioa_cfg->msi_wait_q);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nreturn rc;\r\n}\r\nstatic int ipr_test_msi(struct ipr_ioa_cfg *ioa_cfg, struct pci_dev *pdev)\r\n{\r\nint rc;\r\nvolatile u32 int_reg;\r\nunsigned long lock_flags = 0;\r\nint irq = pci_irq_vector(pdev, 0);\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\ninit_waitqueue_head(&ioa_cfg->msi_wait_q);\r\nioa_cfg->msi_received = 0;\r\nipr_mask_and_clear_interrupts(ioa_cfg, ~IPR_PCII_IOA_TRANS_TO_OPER);\r\nwritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE, ioa_cfg->regs.clr_interrupt_mask_reg32);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nrc = request_irq(irq, ipr_test_intr, 0, IPR_NAME, ioa_cfg);\r\nif (rc) {\r\ndev_err(&pdev->dev, "Can not assign irq %d\n", irq);\r\nreturn rc;\r\n} else if (ipr_debug)\r\ndev_info(&pdev->dev, "IRQ assigned: %d\n", irq);\r\nwritel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE, ioa_cfg->regs.sense_interrupt_reg32);\r\nint_reg = readl(ioa_cfg->regs.sense_interrupt_reg);\r\nwait_event_timeout(ioa_cfg->msi_wait_q, ioa_cfg->msi_received, HZ);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nipr_mask_and_clear_interrupts(ioa_cfg, ~IPR_PCII_IOA_TRANS_TO_OPER);\r\nif (!ioa_cfg->msi_received) {\r\ndev_info(&pdev->dev, "MSI test failed. Falling back to LSI.\n");\r\nrc = -EOPNOTSUPP;\r\n} else if (ipr_debug)\r\ndev_info(&pdev->dev, "MSI test succeeded.\n");\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nfree_irq(irq, ioa_cfg);\r\nLEAVE;\r\nreturn rc;\r\n}\r\nstatic int ipr_probe_ioa(struct pci_dev *pdev,\r\nconst struct pci_device_id *dev_id)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nstruct Scsi_Host *host;\r\nunsigned long ipr_regs_pci;\r\nvoid __iomem *ipr_regs;\r\nint rc = PCIBIOS_SUCCESSFUL;\r\nvolatile u32 mask, uproc, interrupts;\r\nunsigned long lock_flags, driver_lock_flags;\r\nunsigned int irq_flag;\r\nENTER;\r\ndev_info(&pdev->dev, "Found IOA with IRQ: %d\n", pdev->irq);\r\nhost = scsi_host_alloc(&driver_template, sizeof(*ioa_cfg));\r\nif (!host) {\r\ndev_err(&pdev->dev, "call to scsi_host_alloc failed!\n");\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nioa_cfg = (struct ipr_ioa_cfg *)host->hostdata;\r\nmemset(ioa_cfg, 0, sizeof(struct ipr_ioa_cfg));\r\nata_host_init(&ioa_cfg->ata_host, &pdev->dev, &ipr_sata_ops);\r\nioa_cfg->ipr_chip = ipr_get_chip_info(dev_id);\r\nif (!ioa_cfg->ipr_chip) {\r\ndev_err(&pdev->dev, "Unknown adapter chipset 0x%04X 0x%04X\n",\r\ndev_id->vendor, dev_id->device);\r\ngoto out_scsi_host_put;\r\n}\r\nioa_cfg->sis64 = ioa_cfg->ipr_chip->sis_type == IPR_SIS64 ? 1 : 0;\r\nioa_cfg->chip_cfg = ioa_cfg->ipr_chip->cfg;\r\nioa_cfg->clear_isr = ioa_cfg->chip_cfg->clear_isr;\r\nioa_cfg->max_cmds = ioa_cfg->chip_cfg->max_cmds;\r\nif (ipr_transop_timeout)\r\nioa_cfg->transop_timeout = ipr_transop_timeout;\r\nelse if (dev_id->driver_data & IPR_USE_LONG_TRANSOP_TIMEOUT)\r\nioa_cfg->transop_timeout = IPR_LONG_OPERATIONAL_TIMEOUT;\r\nelse\r\nioa_cfg->transop_timeout = IPR_OPERATIONAL_TIMEOUT;\r\nioa_cfg->revid = pdev->revision;\r\nipr_init_ioa_cfg(ioa_cfg, host, pdev);\r\nipr_regs_pci = pci_resource_start(pdev, 0);\r\nrc = pci_request_regions(pdev, IPR_NAME);\r\nif (rc < 0) {\r\ndev_err(&pdev->dev,\r\n"Couldn't register memory range of registers\n");\r\ngoto out_scsi_host_put;\r\n}\r\nrc = pci_enable_device(pdev);\r\nif (rc || pci_channel_offline(pdev)) {\r\nif (pci_channel_offline(pdev)) {\r\nipr_wait_for_pci_err_recovery(ioa_cfg);\r\nrc = pci_enable_device(pdev);\r\n}\r\nif (rc) {\r\ndev_err(&pdev->dev, "Cannot enable adapter\n");\r\nipr_wait_for_pci_err_recovery(ioa_cfg);\r\ngoto out_release_regions;\r\n}\r\n}\r\nipr_regs = pci_ioremap_bar(pdev, 0);\r\nif (!ipr_regs) {\r\ndev_err(&pdev->dev,\r\n"Couldn't map memory range of registers\n");\r\nrc = -ENOMEM;\r\ngoto out_disable;\r\n}\r\nioa_cfg->hdw_dma_regs = ipr_regs;\r\nioa_cfg->hdw_dma_regs_pci = ipr_regs_pci;\r\nioa_cfg->ioa_mailbox = ioa_cfg->chip_cfg->mailbox + ipr_regs;\r\nipr_init_regs(ioa_cfg);\r\nif (ioa_cfg->sis64) {\r\nrc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\r\nif (rc < 0) {\r\ndev_dbg(&pdev->dev, "Failed to set 64 bit DMA mask\n");\r\nrc = dma_set_mask_and_coherent(&pdev->dev,\r\nDMA_BIT_MASK(32));\r\n}\r\n} else\r\nrc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\r\nif (rc < 0) {\r\ndev_err(&pdev->dev, "Failed to set DMA mask\n");\r\ngoto cleanup_nomem;\r\n}\r\nrc = pci_write_config_byte(pdev, PCI_CACHE_LINE_SIZE,\r\nioa_cfg->chip_cfg->cache_line_size);\r\nif (rc != PCIBIOS_SUCCESSFUL) {\r\ndev_err(&pdev->dev, "Write of cache line size failed\n");\r\nipr_wait_for_pci_err_recovery(ioa_cfg);\r\nrc = -EIO;\r\ngoto cleanup_nomem;\r\n}\r\ninterrupts = readl(ioa_cfg->regs.sense_interrupt_reg);\r\nipr_wait_for_pci_err_recovery(ioa_cfg);\r\nif (ipr_number_of_msix > IPR_MAX_MSIX_VECTORS) {\r\ndev_err(&pdev->dev, "The max number of MSIX is %d\n",\r\nIPR_MAX_MSIX_VECTORS);\r\nipr_number_of_msix = IPR_MAX_MSIX_VECTORS;\r\n}\r\nirq_flag = PCI_IRQ_LEGACY;\r\nif (ioa_cfg->ipr_chip->has_msi)\r\nirq_flag |= PCI_IRQ_MSI | PCI_IRQ_MSIX;\r\nrc = pci_alloc_irq_vectors(pdev, 1, ipr_number_of_msix, irq_flag);\r\nif (rc < 0) {\r\nipr_wait_for_pci_err_recovery(ioa_cfg);\r\ngoto cleanup_nomem;\r\n}\r\nioa_cfg->nvectors = rc;\r\nif (!pdev->msi_enabled && !pdev->msix_enabled)\r\nioa_cfg->clear_isr = 1;\r\npci_set_master(pdev);\r\nif (pci_channel_offline(pdev)) {\r\nipr_wait_for_pci_err_recovery(ioa_cfg);\r\npci_set_master(pdev);\r\nif (pci_channel_offline(pdev)) {\r\nrc = -EIO;\r\ngoto out_msi_disable;\r\n}\r\n}\r\nif (pdev->msi_enabled || pdev->msix_enabled) {\r\nrc = ipr_test_msi(ioa_cfg, pdev);\r\nswitch (rc) {\r\ncase 0:\r\ndev_info(&pdev->dev,\r\n"Request for %d MSI%ss succeeded.", ioa_cfg->nvectors,\r\npdev->msix_enabled ? "-X" : "");\r\nbreak;\r\ncase -EOPNOTSUPP:\r\nipr_wait_for_pci_err_recovery(ioa_cfg);\r\npci_free_irq_vectors(pdev);\r\nioa_cfg->nvectors = 1;\r\nioa_cfg->clear_isr = 1;\r\nbreak;\r\ndefault:\r\ngoto out_msi_disable;\r\n}\r\n}\r\nioa_cfg->hrrq_num = min3(ioa_cfg->nvectors,\r\n(unsigned int)num_online_cpus(),\r\n(unsigned int)IPR_MAX_HRRQ_NUM);\r\nif ((rc = ipr_save_pcix_cmd_reg(ioa_cfg)))\r\ngoto out_msi_disable;\r\nif ((rc = ipr_set_pcix_cmd_reg(ioa_cfg)))\r\ngoto out_msi_disable;\r\nrc = ipr_alloc_mem(ioa_cfg);\r\nif (rc < 0) {\r\ndev_err(&pdev->dev,\r\n"Couldn't allocate enough memory for device driver!\n");\r\ngoto out_msi_disable;\r\n}\r\nrc = pci_save_state(pdev);\r\nif (rc != PCIBIOS_SUCCESSFUL) {\r\ndev_err(&pdev->dev, "Failed to save PCI config space\n");\r\nrc = -EIO;\r\ngoto cleanup_nolog;\r\n}\r\nmask = readl(ioa_cfg->regs.sense_interrupt_mask_reg32);\r\ninterrupts = readl(ioa_cfg->regs.sense_interrupt_reg32);\r\nuproc = readl(ioa_cfg->regs.sense_uproc_interrupt_reg32);\r\nif ((mask & IPR_PCII_HRRQ_UPDATED) == 0 || (uproc & IPR_UPROCI_RESET_ALERT))\r\nioa_cfg->needs_hard_reset = 1;\r\nif ((interrupts & IPR_PCII_ERROR_INTERRUPTS) || reset_devices)\r\nioa_cfg->needs_hard_reset = 1;\r\nif (interrupts & IPR_PCII_IOA_UNIT_CHECKED)\r\nioa_cfg->ioa_unit_checked = 1;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nipr_mask_and_clear_interrupts(ioa_cfg, ~IPR_PCII_IOA_TRANS_TO_OPER);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nif (pdev->msi_enabled || pdev->msix_enabled) {\r\nname_msi_vectors(ioa_cfg);\r\nrc = request_irq(pci_irq_vector(pdev, 0), ipr_isr, 0,\r\nioa_cfg->vectors_info[0].desc,\r\n&ioa_cfg->hrrq[0]);\r\nif (!rc)\r\nrc = ipr_request_other_msi_irqs(ioa_cfg, pdev);\r\n} else {\r\nrc = request_irq(pdev->irq, ipr_isr,\r\nIRQF_SHARED,\r\nIPR_NAME, &ioa_cfg->hrrq[0]);\r\n}\r\nif (rc) {\r\ndev_err(&pdev->dev, "Couldn't register IRQ %d! rc=%d\n",\r\npdev->irq, rc);\r\ngoto cleanup_nolog;\r\n}\r\nif ((dev_id->driver_data & IPR_USE_PCI_WARM_RESET) ||\r\n(dev_id->device == PCI_DEVICE_ID_IBM_OBSIDIAN_E && !ioa_cfg->revid)) {\r\nioa_cfg->needs_warm_reset = 1;\r\nioa_cfg->reset = ipr_reset_slot_reset;\r\nioa_cfg->reset_work_q = alloc_ordered_workqueue("ipr_reset_%d",\r\nWQ_MEM_RECLAIM, host->host_no);\r\nif (!ioa_cfg->reset_work_q) {\r\ndev_err(&pdev->dev, "Couldn't register reset workqueue\n");\r\nrc = -ENOMEM;\r\ngoto out_free_irq;\r\n}\r\n} else\r\nioa_cfg->reset = ipr_reset_start_bist;\r\nspin_lock_irqsave(&ipr_driver_lock, driver_lock_flags);\r\nlist_add_tail(&ioa_cfg->queue, &ipr_ioa_head);\r\nspin_unlock_irqrestore(&ipr_driver_lock, driver_lock_flags);\r\nLEAVE;\r\nout:\r\nreturn rc;\r\nout_free_irq:\r\nipr_free_irqs(ioa_cfg);\r\ncleanup_nolog:\r\nipr_free_mem(ioa_cfg);\r\nout_msi_disable:\r\nipr_wait_for_pci_err_recovery(ioa_cfg);\r\npci_free_irq_vectors(pdev);\r\ncleanup_nomem:\r\niounmap(ipr_regs);\r\nout_disable:\r\npci_disable_device(pdev);\r\nout_release_regions:\r\npci_release_regions(pdev);\r\nout_scsi_host_put:\r\nscsi_host_put(host);\r\ngoto out;\r\n}\r\nstatic void ipr_initiate_ioa_bringdown(struct ipr_ioa_cfg *ioa_cfg,\r\nenum ipr_shutdown_type shutdown_type)\r\n{\r\nENTER;\r\nif (ioa_cfg->sdt_state == WAIT_FOR_DUMP)\r\nioa_cfg->sdt_state = ABORT_DUMP;\r\nioa_cfg->reset_retries = 0;\r\nioa_cfg->in_ioa_bringdown = 1;\r\nipr_initiate_ioa_reset(ioa_cfg, shutdown_type);\r\nLEAVE;\r\n}\r\nstatic void __ipr_remove(struct pci_dev *pdev)\r\n{\r\nunsigned long host_lock_flags = 0;\r\nstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\r\nint i;\r\nunsigned long driver_lock_flags;\r\nENTER;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);\r\nwhile (ioa_cfg->in_reset_reload) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);\r\n}\r\nfor (i = 0; i < ioa_cfg->hrrq_num; i++) {\r\nspin_lock(&ioa_cfg->hrrq[i]._lock);\r\nioa_cfg->hrrq[i].removing_ioa = 1;\r\nspin_unlock(&ioa_cfg->hrrq[i]._lock);\r\n}\r\nwmb();\r\nipr_initiate_ioa_bringdown(ioa_cfg, IPR_SHUTDOWN_NORMAL);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nflush_work(&ioa_cfg->work_q);\r\nif (ioa_cfg->reset_work_q)\r\nflush_workqueue(ioa_cfg->reset_work_q);\r\nINIT_LIST_HEAD(&ioa_cfg->used_res_q);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);\r\nspin_lock_irqsave(&ipr_driver_lock, driver_lock_flags);\r\nlist_del(&ioa_cfg->queue);\r\nspin_unlock_irqrestore(&ipr_driver_lock, driver_lock_flags);\r\nif (ioa_cfg->sdt_state == ABORT_DUMP)\r\nioa_cfg->sdt_state = WAIT_FOR_DUMP;\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);\r\nipr_free_all_resources(ioa_cfg);\r\nLEAVE;\r\n}\r\nstatic void ipr_remove(struct pci_dev *pdev)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\r\nENTER;\r\nipr_remove_trace_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_trace_attr);\r\nipr_remove_dump_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_dump_attr);\r\nsysfs_remove_bin_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_ioa_async_err_log);\r\nscsi_remove_host(ioa_cfg->host);\r\n__ipr_remove(pdev);\r\nLEAVE;\r\n}\r\nstatic int ipr_probe(struct pci_dev *pdev, const struct pci_device_id *dev_id)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nunsigned long flags;\r\nint rc, i;\r\nrc = ipr_probe_ioa(pdev, dev_id);\r\nif (rc)\r\nreturn rc;\r\nioa_cfg = pci_get_drvdata(pdev);\r\nrc = ipr_probe_ioa_part2(ioa_cfg);\r\nif (rc) {\r\n__ipr_remove(pdev);\r\nreturn rc;\r\n}\r\nrc = scsi_add_host(ioa_cfg->host, &pdev->dev);\r\nif (rc) {\r\n__ipr_remove(pdev);\r\nreturn rc;\r\n}\r\nrc = ipr_create_trace_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_trace_attr);\r\nif (rc) {\r\nscsi_remove_host(ioa_cfg->host);\r\n__ipr_remove(pdev);\r\nreturn rc;\r\n}\r\nrc = sysfs_create_bin_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_ioa_async_err_log);\r\nif (rc) {\r\nipr_remove_dump_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_dump_attr);\r\nipr_remove_trace_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_trace_attr);\r\nscsi_remove_host(ioa_cfg->host);\r\n__ipr_remove(pdev);\r\nreturn rc;\r\n}\r\nrc = ipr_create_dump_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_dump_attr);\r\nif (rc) {\r\nsysfs_remove_bin_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_ioa_async_err_log);\r\nipr_remove_trace_file(&ioa_cfg->host->shost_dev.kobj,\r\n&ipr_trace_attr);\r\nscsi_remove_host(ioa_cfg->host);\r\n__ipr_remove(pdev);\r\nreturn rc;\r\n}\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\nioa_cfg->scan_enabled = 1;\r\nschedule_work(&ioa_cfg->work_q);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\nioa_cfg->iopoll_weight = ioa_cfg->chip_cfg->iopoll_weight;\r\nif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\r\nfor (i = 1; i < ioa_cfg->hrrq_num; i++) {\r\nirq_poll_init(&ioa_cfg->hrrq[i].iopoll,\r\nioa_cfg->iopoll_weight, ipr_iopoll);\r\n}\r\n}\r\nscsi_scan_host(ioa_cfg->host);\r\nreturn 0;\r\n}\r\nstatic void ipr_shutdown(struct pci_dev *pdev)\r\n{\r\nstruct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);\r\nunsigned long lock_flags = 0;\r\nenum ipr_shutdown_type shutdown_type = IPR_SHUTDOWN_NORMAL;\r\nint i;\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\nif (ioa_cfg->iopoll_weight && ioa_cfg->sis64 && ioa_cfg->nvectors > 1) {\r\nioa_cfg->iopoll_weight = 0;\r\nfor (i = 1; i < ioa_cfg->hrrq_num; i++)\r\nirq_poll_disable(&ioa_cfg->hrrq[i].iopoll);\r\n}\r\nwhile (ioa_cfg->in_reset_reload) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);\r\n}\r\nif (ipr_fast_reboot && system_state == SYSTEM_RESTART && ioa_cfg->sis64)\r\nshutdown_type = IPR_SHUTDOWN_QUIESCE;\r\nipr_initiate_ioa_bringdown(ioa_cfg, shutdown_type);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);\r\nwait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);\r\nif (ipr_fast_reboot && system_state == SYSTEM_RESTART && ioa_cfg->sis64) {\r\nipr_free_irqs(ioa_cfg);\r\npci_disable_device(ioa_cfg->pdev);\r\n}\r\n}\r\nstatic void ipr_halt_done(struct ipr_cmnd *ipr_cmd)\r\n{\r\nlist_add_tail(&ipr_cmd->queue, &ipr_cmd->hrrq->hrrq_free_q);\r\n}\r\nstatic int ipr_halt(struct notifier_block *nb, ulong event, void *buf)\r\n{\r\nstruct ipr_cmnd *ipr_cmd;\r\nstruct ipr_ioa_cfg *ioa_cfg;\r\nunsigned long flags = 0, driver_lock_flags;\r\nif (event != SYS_RESTART && event != SYS_HALT && event != SYS_POWER_OFF)\r\nreturn NOTIFY_DONE;\r\nspin_lock_irqsave(&ipr_driver_lock, driver_lock_flags);\r\nlist_for_each_entry(ioa_cfg, &ipr_ioa_head, queue) {\r\nspin_lock_irqsave(ioa_cfg->host->host_lock, flags);\r\nif (!ioa_cfg->hrrq[IPR_INIT_HRRQ].allow_cmds ||\r\n(ipr_fast_reboot && event == SYS_RESTART && ioa_cfg->sis64)) {\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\ncontinue;\r\n}\r\nipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);\r\nipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);\r\nipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_IOACMD;\r\nipr_cmd->ioarcb.cmd_pkt.cdb[0] = IPR_IOA_SHUTDOWN;\r\nipr_cmd->ioarcb.cmd_pkt.cdb[1] = IPR_SHUTDOWN_PREPARE_FOR_NORMAL;\r\nipr_do_req(ipr_cmd, ipr_halt_done, ipr_timeout, IPR_DEVICE_RESET_TIMEOUT);\r\nspin_unlock_irqrestore(ioa_cfg->host->host_lock, flags);\r\n}\r\nspin_unlock_irqrestore(&ipr_driver_lock, driver_lock_flags);\r\nreturn NOTIFY_OK;\r\n}\r\nstatic int __init ipr_init(void)\r\n{\r\nipr_info("IBM Power RAID SCSI Device Driver version: %s %s\n",\r\nIPR_DRIVER_VERSION, IPR_DRIVER_DATE);\r\nregister_reboot_notifier(&ipr_notifier);\r\nreturn pci_register_driver(&ipr_driver);\r\n}\r\nstatic void __exit ipr_exit(void)\r\n{\r\nunregister_reboot_notifier(&ipr_notifier);\r\npci_unregister_driver(&ipr_driver);\r\n}
