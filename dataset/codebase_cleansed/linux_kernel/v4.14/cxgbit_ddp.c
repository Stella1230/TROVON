static void\r\ncxgbit_set_one_ppod(struct cxgbi_pagepod *ppod,\r\nstruct cxgbi_task_tag_info *ttinfo,\r\nstruct scatterlist **sg_pp, unsigned int *sg_off)\r\n{\r\nstruct scatterlist *sg = sg_pp ? *sg_pp : NULL;\r\nunsigned int offset = sg_off ? *sg_off : 0;\r\ndma_addr_t addr = 0UL;\r\nunsigned int len = 0;\r\nint i;\r\nmemcpy(ppod, &ttinfo->hdr, sizeof(struct cxgbi_pagepod_hdr));\r\nif (sg) {\r\naddr = sg_dma_address(sg);\r\nlen = sg_dma_len(sg);\r\n}\r\nfor (i = 0; i < PPOD_PAGES_MAX; i++) {\r\nif (sg) {\r\nppod->addr[i] = cpu_to_be64(addr + offset);\r\noffset += PAGE_SIZE;\r\nif (offset == (len + sg->offset)) {\r\noffset = 0;\r\nsg = sg_next(sg);\r\nif (sg) {\r\naddr = sg_dma_address(sg);\r\nlen = sg_dma_len(sg);\r\n}\r\n}\r\n} else {\r\nppod->addr[i] = 0ULL;\r\n}\r\n}\r\nif (sg_pp) {\r\n*sg_pp = sg;\r\n*sg_off = offset;\r\n}\r\nif (offset == len) {\r\noffset = 0;\r\nif (sg) {\r\nsg = sg_next(sg);\r\nif (sg)\r\naddr = sg_dma_address(sg);\r\n}\r\n}\r\nppod->addr[i] = sg ? cpu_to_be64(addr + offset) : 0ULL;\r\n}\r\nstatic struct sk_buff *\r\ncxgbit_ppod_init_idata(struct cxgbit_device *cdev, struct cxgbi_ppm *ppm,\r\nunsigned int idx, unsigned int npods, unsigned int tid)\r\n{\r\nstruct ulp_mem_io *req;\r\nstruct ulptx_idata *idata;\r\nunsigned int pm_addr = (idx << PPOD_SIZE_SHIFT) + ppm->llimit;\r\nunsigned int dlen = npods << PPOD_SIZE_SHIFT;\r\nunsigned int wr_len = roundup(sizeof(struct ulp_mem_io) +\r\nsizeof(struct ulptx_idata) + dlen, 16);\r\nstruct sk_buff *skb;\r\nskb = alloc_skb(wr_len, GFP_KERNEL);\r\nif (!skb)\r\nreturn NULL;\r\nreq = __skb_put(skb, wr_len);\r\nINIT_ULPTX_WR(req, wr_len, 0, tid);\r\nreq->wr.wr_hi = htonl(FW_WR_OP_V(FW_ULPTX_WR) |\r\nFW_WR_ATOMIC_V(0));\r\nreq->cmd = htonl(ULPTX_CMD_V(ULP_TX_MEM_WRITE) |\r\nULP_MEMIO_ORDER_V(0) |\r\nT5_ULP_MEMIO_IMM_V(1));\r\nreq->dlen = htonl(ULP_MEMIO_DATA_LEN_V(dlen >> 5));\r\nreq->lock_addr = htonl(ULP_MEMIO_ADDR_V(pm_addr >> 5));\r\nreq->len16 = htonl(DIV_ROUND_UP(wr_len - sizeof(req->wr), 16));\r\nidata = (struct ulptx_idata *)(req + 1);\r\nidata->cmd_more = htonl(ULPTX_CMD_V(ULP_TX_SC_IMM));\r\nidata->len = htonl(dlen);\r\nreturn skb;\r\n}\r\nstatic int\r\ncxgbit_ppod_write_idata(struct cxgbi_ppm *ppm, struct cxgbit_sock *csk,\r\nstruct cxgbi_task_tag_info *ttinfo, unsigned int idx,\r\nunsigned int npods, struct scatterlist **sg_pp,\r\nunsigned int *sg_off)\r\n{\r\nstruct cxgbit_device *cdev = csk->com.cdev;\r\nstruct sk_buff *skb;\r\nstruct ulp_mem_io *req;\r\nstruct ulptx_idata *idata;\r\nstruct cxgbi_pagepod *ppod;\r\nunsigned int i;\r\nskb = cxgbit_ppod_init_idata(cdev, ppm, idx, npods, csk->tid);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nreq = (struct ulp_mem_io *)skb->data;\r\nidata = (struct ulptx_idata *)(req + 1);\r\nppod = (struct cxgbi_pagepod *)(idata + 1);\r\nfor (i = 0; i < npods; i++, ppod++)\r\ncxgbit_set_one_ppod(ppod, ttinfo, sg_pp, sg_off);\r\n__skb_queue_tail(&csk->ppodq, skb);\r\nreturn 0;\r\n}\r\nstatic int\r\ncxgbit_ddp_set_map(struct cxgbi_ppm *ppm, struct cxgbit_sock *csk,\r\nstruct cxgbi_task_tag_info *ttinfo)\r\n{\r\nunsigned int pidx = ttinfo->idx;\r\nunsigned int npods = ttinfo->npods;\r\nunsigned int i, cnt;\r\nstruct scatterlist *sg = ttinfo->sgl;\r\nunsigned int offset = 0;\r\nint ret = 0;\r\nfor (i = 0; i < npods; i += cnt, pidx += cnt) {\r\ncnt = npods - i;\r\nif (cnt > ULPMEM_IDATA_MAX_NPPODS)\r\ncnt = ULPMEM_IDATA_MAX_NPPODS;\r\nret = cxgbit_ppod_write_idata(ppm, csk, ttinfo, pidx, cnt,\r\n&sg, &offset);\r\nif (ret < 0)\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int cxgbit_ddp_sgl_check(struct scatterlist *sg,\r\nunsigned int nents)\r\n{\r\nunsigned int last_sgidx = nents - 1;\r\nunsigned int i;\r\nfor (i = 0; i < nents; i++, sg = sg_next(sg)) {\r\nunsigned int len = sg->length + sg->offset;\r\nif ((sg->offset & 0x3) || (i && sg->offset) ||\r\n((i != last_sgidx) && (len != PAGE_SIZE))) {\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\ncxgbit_ddp_reserve(struct cxgbit_sock *csk, struct cxgbi_task_tag_info *ttinfo,\r\nunsigned int xferlen)\r\n{\r\nstruct cxgbit_device *cdev = csk->com.cdev;\r\nstruct cxgbi_ppm *ppm = cdev2ppm(cdev);\r\nstruct scatterlist *sgl = ttinfo->sgl;\r\nunsigned int sgcnt = ttinfo->nents;\r\nunsigned int sg_offset = sgl->offset;\r\nint ret;\r\nif ((xferlen < DDP_THRESHOLD) || (!sgcnt)) {\r\npr_debug("ppm 0x%p, pgidx %u, xfer %u, sgcnt %u, NO ddp.\n",\r\nppm, ppm->tformat.pgsz_idx_dflt,\r\nxferlen, ttinfo->nents);\r\nreturn -EINVAL;\r\n}\r\nif (cxgbit_ddp_sgl_check(sgl, sgcnt) < 0)\r\nreturn -EINVAL;\r\nttinfo->nr_pages = (xferlen + sgl->offset +\r\n(1 << PAGE_SHIFT) - 1) >> PAGE_SHIFT;\r\nret = cxgbi_ppm_ppods_reserve(ppm, ttinfo->nr_pages, 0, &ttinfo->idx,\r\n&ttinfo->tag, 0);\r\nif (ret < 0)\r\nreturn ret;\r\nttinfo->npods = ret;\r\nsgl->offset = 0;\r\nret = dma_map_sg(&ppm->pdev->dev, sgl, sgcnt, DMA_FROM_DEVICE);\r\nsgl->offset = sg_offset;\r\nif (!ret) {\r\npr_info("%s: 0x%x, xfer %u, sgl %u dma mapping err.\n",\r\n__func__, 0, xferlen, sgcnt);\r\ngoto rel_ppods;\r\n}\r\ncxgbi_ppm_make_ppod_hdr(ppm, ttinfo->tag, csk->tid, sgl->offset,\r\nxferlen, &ttinfo->hdr);\r\nret = cxgbit_ddp_set_map(ppm, csk, ttinfo);\r\nif (ret < 0) {\r\n__skb_queue_purge(&csk->ppodq);\r\ndma_unmap_sg(&ppm->pdev->dev, sgl, sgcnt, DMA_FROM_DEVICE);\r\ngoto rel_ppods;\r\n}\r\nreturn 0;\r\nrel_ppods:\r\ncxgbi_ppm_ppod_release(ppm, ttinfo->idx);\r\nreturn -EINVAL;\r\n}\r\nvoid\r\ncxgbit_get_r2t_ttt(struct iscsi_conn *conn, struct iscsi_cmd *cmd,\r\nstruct iscsi_r2t *r2t)\r\n{\r\nstruct cxgbit_sock *csk = conn->context;\r\nstruct cxgbit_device *cdev = csk->com.cdev;\r\nstruct cxgbit_cmd *ccmd = iscsit_priv_cmd(cmd);\r\nstruct cxgbi_task_tag_info *ttinfo = &ccmd->ttinfo;\r\nint ret = -EINVAL;\r\nif ((!ccmd->setup_ddp) ||\r\n(!test_bit(CSK_DDP_ENABLE, &csk->com.flags)))\r\ngoto out;\r\nccmd->setup_ddp = false;\r\nttinfo->sgl = cmd->se_cmd.t_data_sg;\r\nttinfo->nents = cmd->se_cmd.t_data_nents;\r\nret = cxgbit_ddp_reserve(csk, ttinfo, cmd->se_cmd.data_length);\r\nif (ret < 0) {\r\npr_info("csk 0x%p, cmd 0x%p, xfer len %u, sgcnt %u no ddp.\n",\r\ncsk, cmd, cmd->se_cmd.data_length, ttinfo->nents);\r\nttinfo->sgl = NULL;\r\nttinfo->nents = 0;\r\n} else {\r\nccmd->release = true;\r\n}\r\nout:\r\npr_debug("cdev 0x%p, cmd 0x%p, tag 0x%x\n", cdev, cmd, ttinfo->tag);\r\nr2t->targ_xfer_tag = ttinfo->tag;\r\n}\r\nvoid cxgbit_release_cmd(struct iscsi_conn *conn, struct iscsi_cmd *cmd)\r\n{\r\nstruct cxgbit_cmd *ccmd = iscsit_priv_cmd(cmd);\r\nif (ccmd->release) {\r\nstruct cxgbi_task_tag_info *ttinfo = &ccmd->ttinfo;\r\nif (ttinfo->sgl) {\r\nstruct cxgbit_sock *csk = conn->context;\r\nstruct cxgbit_device *cdev = csk->com.cdev;\r\nstruct cxgbi_ppm *ppm = cdev2ppm(cdev);\r\ncxgbi_ppm_ppod_release(ppm, ttinfo->idx);\r\ndma_unmap_sg(&ppm->pdev->dev, ttinfo->sgl,\r\nttinfo->nents, DMA_FROM_DEVICE);\r\n} else {\r\nput_page(sg_page(&ccmd->sg));\r\n}\r\nccmd->release = false;\r\n}\r\n}\r\nint cxgbit_ddp_init(struct cxgbit_device *cdev)\r\n{\r\nstruct cxgb4_lld_info *lldi = &cdev->lldi;\r\nstruct net_device *ndev = cdev->lldi.ports[0];\r\nstruct cxgbi_tag_format tformat;\r\nunsigned int ppmax;\r\nint ret, i;\r\nif (!lldi->vr->iscsi.size) {\r\npr_warn("%s, iscsi NOT enabled, check config!\n", ndev->name);\r\nreturn -EACCES;\r\n}\r\nppmax = lldi->vr->iscsi.size >> PPOD_SIZE_SHIFT;\r\nmemset(&tformat, 0, sizeof(struct cxgbi_tag_format));\r\nfor (i = 0; i < 4; i++)\r\ntformat.pgsz_order[i] = (lldi->iscsi_pgsz_order >> (i << 3))\r\n& 0xF;\r\ncxgbi_tagmask_check(lldi->iscsi_tagmask, &tformat);\r\nret = cxgbi_ppm_init(lldi->iscsi_ppm, cdev->lldi.ports[0],\r\ncdev->lldi.pdev, &cdev->lldi, &tformat,\r\nppmax, lldi->iscsi_llimit,\r\nlldi->vr->iscsi.start, 2);\r\nif (ret >= 0) {\r\nstruct cxgbi_ppm *ppm = (struct cxgbi_ppm *)(*lldi->iscsi_ppm);\r\nif ((ppm->tformat.pgsz_idx_dflt < DDP_PGIDX_MAX) &&\r\n(ppm->ppmax >= 1024))\r\nset_bit(CDEV_DDP_ENABLE, &cdev->flags);\r\nret = 0;\r\n}\r\nreturn ret;\r\n}
