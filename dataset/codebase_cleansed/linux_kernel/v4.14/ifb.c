static void ifb_ri_tasklet(unsigned long _txp)\r\n{\r\nstruct ifb_q_private *txp = (struct ifb_q_private *)_txp;\r\nstruct netdev_queue *txq;\r\nstruct sk_buff *skb;\r\ntxq = netdev_get_tx_queue(txp->dev, txp->txqnum);\r\nskb = skb_peek(&txp->tq);\r\nif (!skb) {\r\nif (!__netif_tx_trylock(txq))\r\ngoto resched;\r\nskb_queue_splice_tail_init(&txp->rq, &txp->tq);\r\n__netif_tx_unlock(txq);\r\n}\r\nwhile ((skb = __skb_dequeue(&txp->tq)) != NULL) {\r\nskb->tc_redirected = 0;\r\nskb->tc_skip_classify = 1;\r\nu64_stats_update_begin(&txp->tsync);\r\ntxp->tx_packets++;\r\ntxp->tx_bytes += skb->len;\r\nu64_stats_update_end(&txp->tsync);\r\nrcu_read_lock();\r\nskb->dev = dev_get_by_index_rcu(dev_net(txp->dev), skb->skb_iif);\r\nif (!skb->dev) {\r\nrcu_read_unlock();\r\ndev_kfree_skb(skb);\r\ntxp->dev->stats.tx_dropped++;\r\nif (skb_queue_len(&txp->tq) != 0)\r\ngoto resched;\r\nbreak;\r\n}\r\nrcu_read_unlock();\r\nskb->skb_iif = txp->dev->ifindex;\r\nif (!skb->tc_from_ingress) {\r\ndev_queue_xmit(skb);\r\n} else {\r\nskb_pull(skb, skb->mac_len);\r\nnetif_receive_skb(skb);\r\n}\r\n}\r\nif (__netif_tx_trylock(txq)) {\r\nskb = skb_peek(&txp->rq);\r\nif (!skb) {\r\ntxp->tasklet_pending = 0;\r\nif (netif_tx_queue_stopped(txq))\r\nnetif_tx_wake_queue(txq);\r\n} else {\r\n__netif_tx_unlock(txq);\r\ngoto resched;\r\n}\r\n__netif_tx_unlock(txq);\r\n} else {\r\nresched:\r\ntxp->tasklet_pending = 1;\r\ntasklet_schedule(&txp->ifb_tasklet);\r\n}\r\n}\r\nstatic void ifb_stats64(struct net_device *dev,\r\nstruct rtnl_link_stats64 *stats)\r\n{\r\nstruct ifb_dev_private *dp = netdev_priv(dev);\r\nstruct ifb_q_private *txp = dp->tx_private;\r\nunsigned int start;\r\nu64 packets, bytes;\r\nint i;\r\nfor (i = 0; i < dev->num_tx_queues; i++,txp++) {\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&txp->rsync);\r\npackets = txp->rx_packets;\r\nbytes = txp->rx_bytes;\r\n} while (u64_stats_fetch_retry_irq(&txp->rsync, start));\r\nstats->rx_packets += packets;\r\nstats->rx_bytes += bytes;\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&txp->tsync);\r\npackets = txp->tx_packets;\r\nbytes = txp->tx_bytes;\r\n} while (u64_stats_fetch_retry_irq(&txp->tsync, start));\r\nstats->tx_packets += packets;\r\nstats->tx_bytes += bytes;\r\n}\r\nstats->rx_dropped = dev->stats.rx_dropped;\r\nstats->tx_dropped = dev->stats.tx_dropped;\r\n}\r\nstatic int ifb_dev_init(struct net_device *dev)\r\n{\r\nstruct ifb_dev_private *dp = netdev_priv(dev);\r\nstruct ifb_q_private *txp;\r\nint i;\r\ntxp = kcalloc(dev->num_tx_queues, sizeof(*txp), GFP_KERNEL);\r\nif (!txp)\r\nreturn -ENOMEM;\r\ndp->tx_private = txp;\r\nfor (i = 0; i < dev->num_tx_queues; i++,txp++) {\r\ntxp->txqnum = i;\r\ntxp->dev = dev;\r\n__skb_queue_head_init(&txp->rq);\r\n__skb_queue_head_init(&txp->tq);\r\nu64_stats_init(&txp->rsync);\r\nu64_stats_init(&txp->tsync);\r\ntasklet_init(&txp->ifb_tasklet, ifb_ri_tasklet,\r\n(unsigned long)txp);\r\nnetif_tx_start_queue(netdev_get_tx_queue(dev, i));\r\n}\r\nreturn 0;\r\n}\r\nstatic void ifb_dev_free(struct net_device *dev)\r\n{\r\nstruct ifb_dev_private *dp = netdev_priv(dev);\r\nstruct ifb_q_private *txp = dp->tx_private;\r\nint i;\r\nfor (i = 0; i < dev->num_tx_queues; i++,txp++) {\r\ntasklet_kill(&txp->ifb_tasklet);\r\n__skb_queue_purge(&txp->rq);\r\n__skb_queue_purge(&txp->tq);\r\n}\r\nkfree(dp->tx_private);\r\n}\r\nstatic void ifb_setup(struct net_device *dev)\r\n{\r\ndev->netdev_ops = &ifb_netdev_ops;\r\nether_setup(dev);\r\ndev->tx_queue_len = TX_Q_LIMIT;\r\ndev->features |= IFB_FEATURES;\r\ndev->hw_features |= dev->features;\r\ndev->hw_enc_features |= dev->features;\r\ndev->vlan_features |= IFB_FEATURES & ~(NETIF_F_HW_VLAN_CTAG_TX |\r\nNETIF_F_HW_VLAN_STAG_TX);\r\ndev->flags |= IFF_NOARP;\r\ndev->flags &= ~IFF_MULTICAST;\r\ndev->priv_flags &= ~IFF_TX_SKB_SHARING;\r\nnetif_keep_dst(dev);\r\neth_hw_addr_random(dev);\r\ndev->needs_free_netdev = true;\r\ndev->priv_destructor = ifb_dev_free;\r\n}\r\nstatic netdev_tx_t ifb_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct ifb_dev_private *dp = netdev_priv(dev);\r\nstruct ifb_q_private *txp = dp->tx_private + skb_get_queue_mapping(skb);\r\nu64_stats_update_begin(&txp->rsync);\r\ntxp->rx_packets++;\r\ntxp->rx_bytes += skb->len;\r\nu64_stats_update_end(&txp->rsync);\r\nif (!skb->tc_redirected || !skb->skb_iif) {\r\ndev_kfree_skb(skb);\r\ndev->stats.rx_dropped++;\r\nreturn NETDEV_TX_OK;\r\n}\r\nif (skb_queue_len(&txp->rq) >= dev->tx_queue_len)\r\nnetif_tx_stop_queue(netdev_get_tx_queue(dev, txp->txqnum));\r\n__skb_queue_tail(&txp->rq, skb);\r\nif (!txp->tasklet_pending) {\r\ntxp->tasklet_pending = 1;\r\ntasklet_schedule(&txp->ifb_tasklet);\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int ifb_close(struct net_device *dev)\r\n{\r\nnetif_tx_stop_all_queues(dev);\r\nreturn 0;\r\n}\r\nstatic int ifb_open(struct net_device *dev)\r\n{\r\nnetif_tx_start_all_queues(dev);\r\nreturn 0;\r\n}\r\nstatic int ifb_validate(struct nlattr *tb[], struct nlattr *data[],\r\nstruct netlink_ext_ack *extack)\r\n{\r\nif (tb[IFLA_ADDRESS]) {\r\nif (nla_len(tb[IFLA_ADDRESS]) != ETH_ALEN)\r\nreturn -EINVAL;\r\nif (!is_valid_ether_addr(nla_data(tb[IFLA_ADDRESS])))\r\nreturn -EADDRNOTAVAIL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init ifb_init_one(int index)\r\n{\r\nstruct net_device *dev_ifb;\r\nint err;\r\ndev_ifb = alloc_netdev(sizeof(struct ifb_dev_private), "ifb%d",\r\nNET_NAME_UNKNOWN, ifb_setup);\r\nif (!dev_ifb)\r\nreturn -ENOMEM;\r\ndev_ifb->rtnl_link_ops = &ifb_link_ops;\r\nerr = register_netdevice(dev_ifb);\r\nif (err < 0)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\nfree_netdev(dev_ifb);\r\nreturn err;\r\n}\r\nstatic int __init ifb_init_module(void)\r\n{\r\nint i, err;\r\nrtnl_lock();\r\nerr = __rtnl_link_register(&ifb_link_ops);\r\nif (err < 0)\r\ngoto out;\r\nfor (i = 0; i < numifbs && !err; i++) {\r\nerr = ifb_init_one(i);\r\ncond_resched();\r\n}\r\nif (err)\r\n__rtnl_link_unregister(&ifb_link_ops);\r\nout:\r\nrtnl_unlock();\r\nreturn err;\r\n}\r\nstatic void __exit ifb_cleanup_module(void)\r\n{\r\nrtnl_link_unregister(&ifb_link_ops);\r\n}
