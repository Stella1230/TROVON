static void pblk_mark_bb(struct pblk *pblk, struct pblk_line *line,\r\nstruct ppa_addr *ppa)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nint pos = pblk_dev_ppa_to_pos(geo, *ppa);\r\npr_debug("pblk: erase failed: line:%d, pos:%d\n", line->id, pos);\r\natomic_long_inc(&pblk->erase_failed);\r\natomic_dec(&line->blk_in_line);\r\nif (test_and_set_bit(pos, line->blk_bitmap))\r\npr_err("pblk: attempted to erase bb: line:%d, pos:%d\n",\r\nline->id, pos);\r\npblk_line_run_ws(pblk, NULL, ppa, pblk_line_mark_bb, pblk->bb_wq);\r\n}\r\nstatic void __pblk_end_io_erase(struct pblk *pblk, struct nvm_rq *rqd)\r\n{\r\nstruct pblk_line *line;\r\nline = &pblk->lines[pblk_dev_ppa_to_line(rqd->ppa_addr)];\r\natomic_dec(&line->left_seblks);\r\nif (rqd->error) {\r\nstruct ppa_addr *ppa;\r\nppa = kmalloc(sizeof(struct ppa_addr), GFP_ATOMIC);\r\nif (!ppa)\r\nreturn;\r\n*ppa = rqd->ppa_addr;\r\npblk_mark_bb(pblk, line, ppa);\r\n}\r\natomic_dec(&pblk->inflight_io);\r\n}\r\nstatic void pblk_end_io_erase(struct nvm_rq *rqd)\r\n{\r\nstruct pblk *pblk = rqd->private;\r\n__pblk_end_io_erase(pblk, rqd);\r\nmempool_free(rqd, pblk->g_rq_pool);\r\n}\r\nvoid __pblk_map_invalidate(struct pblk *pblk, struct pblk_line *line,\r\nu64 paddr)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct list_head *move_list = NULL;\r\nspin_lock(&line->lock);\r\nif (line->state == PBLK_LINESTATE_GC ||\r\nline->state == PBLK_LINESTATE_FREE) {\r\nspin_unlock(&line->lock);\r\nreturn;\r\n}\r\nif (test_and_set_bit(paddr, line->invalid_bitmap)) {\r\nWARN_ONCE(1, "pblk: double invalidate\n");\r\nspin_unlock(&line->lock);\r\nreturn;\r\n}\r\nle32_add_cpu(line->vsc, -1);\r\nif (line->state == PBLK_LINESTATE_CLOSED)\r\nmove_list = pblk_line_gc_list(pblk, line);\r\nspin_unlock(&line->lock);\r\nif (move_list) {\r\nspin_lock(&l_mg->gc_lock);\r\nspin_lock(&line->lock);\r\nif (line->state == PBLK_LINESTATE_GC ||\r\nline->state == PBLK_LINESTATE_FREE) {\r\nspin_unlock(&line->lock);\r\nspin_unlock(&l_mg->gc_lock);\r\nreturn;\r\n}\r\nspin_unlock(&line->lock);\r\nlist_move_tail(&line->list, move_list);\r\nspin_unlock(&l_mg->gc_lock);\r\n}\r\n}\r\nvoid pblk_map_invalidate(struct pblk *pblk, struct ppa_addr ppa)\r\n{\r\nstruct pblk_line *line;\r\nu64 paddr;\r\nint line_id;\r\n#ifdef CONFIG_NVM_DEBUG\r\nBUG_ON(pblk_addr_in_cache(ppa));\r\nBUG_ON(pblk_ppa_empty(ppa));\r\n#endif\r\nline_id = pblk_tgt_ppa_to_line(ppa);\r\nline = &pblk->lines[line_id];\r\npaddr = pblk_dev_ppa_to_line_addr(pblk, ppa);\r\n__pblk_map_invalidate(pblk, line, paddr);\r\n}\r\nstatic void pblk_invalidate_range(struct pblk *pblk, sector_t slba,\r\nunsigned int nr_secs)\r\n{\r\nsector_t lba;\r\nspin_lock(&pblk->trans_lock);\r\nfor (lba = slba; lba < slba + nr_secs; lba++) {\r\nstruct ppa_addr ppa;\r\nppa = pblk_trans_map_get(pblk, lba);\r\nif (!pblk_addr_in_cache(ppa) && !pblk_ppa_empty(ppa))\r\npblk_map_invalidate(pblk, ppa);\r\npblk_ppa_set_empty(&ppa);\r\npblk_trans_map_set(pblk, lba, ppa);\r\n}\r\nspin_unlock(&pblk->trans_lock);\r\n}\r\nstruct nvm_rq *pblk_alloc_rqd(struct pblk *pblk, int rw)\r\n{\r\nmempool_t *pool;\r\nstruct nvm_rq *rqd;\r\nint rq_size;\r\nif (rw == WRITE) {\r\npool = pblk->w_rq_pool;\r\nrq_size = pblk_w_rq_size;\r\n} else {\r\npool = pblk->g_rq_pool;\r\nrq_size = pblk_g_rq_size;\r\n}\r\nrqd = mempool_alloc(pool, GFP_KERNEL);\r\nmemset(rqd, 0, rq_size);\r\nreturn rqd;\r\n}\r\nvoid pblk_free_rqd(struct pblk *pblk, struct nvm_rq *rqd, int rw)\r\n{\r\nmempool_t *pool;\r\nif (rw == WRITE)\r\npool = pblk->w_rq_pool;\r\nelse\r\npool = pblk->g_rq_pool;\r\nmempool_free(rqd, pool);\r\n}\r\nvoid pblk_bio_free_pages(struct pblk *pblk, struct bio *bio, int off,\r\nint nr_pages)\r\n{\r\nstruct bio_vec bv;\r\nint i;\r\nWARN_ON(off + nr_pages != bio->bi_vcnt);\r\nbio_advance(bio, off * PBLK_EXPOSED_PAGE_SIZE);\r\nfor (i = off; i < nr_pages + off; i++) {\r\nbv = bio->bi_io_vec[i];\r\nmempool_free(bv.bv_page, pblk->page_pool);\r\n}\r\n}\r\nint pblk_bio_add_pages(struct pblk *pblk, struct bio *bio, gfp_t flags,\r\nint nr_pages)\r\n{\r\nstruct request_queue *q = pblk->dev->q;\r\nstruct page *page;\r\nint i, ret;\r\nfor (i = 0; i < nr_pages; i++) {\r\npage = mempool_alloc(pblk->page_pool, flags);\r\nif (!page)\r\ngoto err;\r\nret = bio_add_pc_page(q, bio, page, PBLK_EXPOSED_PAGE_SIZE, 0);\r\nif (ret != PBLK_EXPOSED_PAGE_SIZE) {\r\npr_err("pblk: could not add page to bio\n");\r\nmempool_free(page, pblk->page_pool);\r\ngoto err;\r\n}\r\n}\r\nreturn 0;\r\nerr:\r\npblk_bio_free_pages(pblk, bio, 0, i - 1);\r\nreturn -1;\r\n}\r\nstatic void pblk_write_kick(struct pblk *pblk)\r\n{\r\nwake_up_process(pblk->writer_ts);\r\nmod_timer(&pblk->wtimer, jiffies + msecs_to_jiffies(1000));\r\n}\r\nvoid pblk_write_timer_fn(unsigned long data)\r\n{\r\nstruct pblk *pblk = (struct pblk *)data;\r\npblk_write_kick(pblk);\r\n}\r\nvoid pblk_write_should_kick(struct pblk *pblk)\r\n{\r\nunsigned int secs_avail = pblk_rb_read_count(&pblk->rwb);\r\nif (secs_avail >= pblk->min_write_pgs)\r\npblk_write_kick(pblk);\r\n}\r\nvoid pblk_end_bio_sync(struct bio *bio)\r\n{\r\nstruct completion *waiting = bio->bi_private;\r\ncomplete(waiting);\r\n}\r\nvoid pblk_end_io_sync(struct nvm_rq *rqd)\r\n{\r\nstruct completion *waiting = rqd->private;\r\ncomplete(waiting);\r\n}\r\nvoid pblk_wait_for_meta(struct pblk *pblk)\r\n{\r\ndo {\r\nif (!atomic_read(&pblk->inflight_io))\r\nbreak;\r\nschedule();\r\n} while (1);\r\n}\r\nstatic void pblk_flush_writer(struct pblk *pblk)\r\n{\r\npblk_rb_flush(&pblk->rwb);\r\ndo {\r\nif (!pblk_rb_sync_count(&pblk->rwb))\r\nbreak;\r\npblk_write_kick(pblk);\r\nschedule();\r\n} while (1);\r\n}\r\nstruct list_head *pblk_line_gc_list(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct list_head *move_list = NULL;\r\nint vsc = le32_to_cpu(*line->vsc);\r\nlockdep_assert_held(&line->lock);\r\nif (!vsc) {\r\nif (line->gc_group != PBLK_LINEGC_FULL) {\r\nline->gc_group = PBLK_LINEGC_FULL;\r\nmove_list = &l_mg->gc_full_list;\r\n}\r\n} else if (vsc < lm->high_thrs) {\r\nif (line->gc_group != PBLK_LINEGC_HIGH) {\r\nline->gc_group = PBLK_LINEGC_HIGH;\r\nmove_list = &l_mg->gc_high_list;\r\n}\r\n} else if (vsc < lm->mid_thrs) {\r\nif (line->gc_group != PBLK_LINEGC_MID) {\r\nline->gc_group = PBLK_LINEGC_MID;\r\nmove_list = &l_mg->gc_mid_list;\r\n}\r\n} else if (vsc < line->sec_in_line) {\r\nif (line->gc_group != PBLK_LINEGC_LOW) {\r\nline->gc_group = PBLK_LINEGC_LOW;\r\nmove_list = &l_mg->gc_low_list;\r\n}\r\n} else if (vsc == line->sec_in_line) {\r\nif (line->gc_group != PBLK_LINEGC_EMPTY) {\r\nline->gc_group = PBLK_LINEGC_EMPTY;\r\nmove_list = &l_mg->gc_empty_list;\r\n}\r\n} else {\r\nline->state = PBLK_LINESTATE_CORRUPT;\r\nline->gc_group = PBLK_LINEGC_NONE;\r\nmove_list = &l_mg->corrupt_list;\r\npr_err("pblk: corrupted vsc for line %d, vsc:%d (%d/%d/%d)\n",\r\nline->id, vsc,\r\nline->sec_in_line,\r\nlm->high_thrs, lm->mid_thrs);\r\n}\r\nreturn move_list;\r\n}\r\nvoid pblk_discard(struct pblk *pblk, struct bio *bio)\r\n{\r\nsector_t slba = pblk_get_lba(bio);\r\nsector_t nr_secs = pblk_get_secs(bio);\r\npblk_invalidate_range(pblk, slba, nr_secs);\r\n}\r\nstruct ppa_addr pblk_get_lba_map(struct pblk *pblk, sector_t lba)\r\n{\r\nstruct ppa_addr ppa;\r\nspin_lock(&pblk->trans_lock);\r\nppa = pblk_trans_map_get(pblk, lba);\r\nspin_unlock(&pblk->trans_lock);\r\nreturn ppa;\r\n}\r\nvoid pblk_log_write_err(struct pblk *pblk, struct nvm_rq *rqd)\r\n{\r\natomic_long_inc(&pblk->write_failed);\r\n#ifdef CONFIG_NVM_DEBUG\r\npblk_print_failed_rqd(pblk, rqd, rqd->error);\r\n#endif\r\n}\r\nvoid pblk_log_read_err(struct pblk *pblk, struct nvm_rq *rqd)\r\n{\r\nif (rqd->error == NVM_RSP_ERR_EMPTYPAGE) {\r\natomic_long_inc(&pblk->read_empty);\r\nreturn;\r\n}\r\nswitch (rqd->error) {\r\ncase NVM_RSP_WARN_HIGHECC:\r\natomic_long_inc(&pblk->read_high_ecc);\r\nbreak;\r\ncase NVM_RSP_ERR_FAILECC:\r\ncase NVM_RSP_ERR_FAILCRC:\r\natomic_long_inc(&pblk->read_failed);\r\nbreak;\r\ndefault:\r\npr_err("pblk: unknown read error:%d\n", rqd->error);\r\n}\r\n#ifdef CONFIG_NVM_DEBUG\r\npblk_print_failed_rqd(pblk, rqd, rqd->error);\r\n#endif\r\n}\r\nvoid pblk_set_sec_per_write(struct pblk *pblk, int sec_per_write)\r\n{\r\npblk->sec_per_write = sec_per_write;\r\n}\r\nint pblk_submit_io(struct pblk *pblk, struct nvm_rq *rqd)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\n#ifdef CONFIG_NVM_DEBUG\r\nstruct ppa_addr *ppa_list;\r\nppa_list = (rqd->nr_ppas > 1) ? rqd->ppa_list : &rqd->ppa_addr;\r\nif (pblk_boundary_ppa_checks(dev, ppa_list, rqd->nr_ppas)) {\r\nWARN_ON(1);\r\nreturn -EINVAL;\r\n}\r\nif (rqd->opcode == NVM_OP_PWRITE) {\r\nstruct pblk_line *line;\r\nstruct ppa_addr ppa;\r\nint i;\r\nfor (i = 0; i < rqd->nr_ppas; i++) {\r\nppa = ppa_list[i];\r\nline = &pblk->lines[pblk_dev_ppa_to_line(ppa)];\r\nspin_lock(&line->lock);\r\nif (line->state != PBLK_LINESTATE_OPEN) {\r\npr_err("pblk: bad ppa: line:%d,state:%d\n",\r\nline->id, line->state);\r\nWARN_ON(1);\r\nspin_unlock(&line->lock);\r\nreturn -EINVAL;\r\n}\r\nspin_unlock(&line->lock);\r\n}\r\n}\r\n#endif\r\natomic_inc(&pblk->inflight_io);\r\nreturn nvm_submit_io(dev, rqd);\r\n}\r\nstruct bio *pblk_bio_map_addr(struct pblk *pblk, void *data,\r\nunsigned int nr_secs, unsigned int len,\r\nint alloc_type, gfp_t gfp_mask)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nvoid *kaddr = data;\r\nstruct page *page;\r\nstruct bio *bio;\r\nint i, ret;\r\nif (alloc_type == PBLK_KMALLOC_META)\r\nreturn bio_map_kern(dev->q, kaddr, len, gfp_mask);\r\nbio = bio_kmalloc(gfp_mask, nr_secs);\r\nif (!bio)\r\nreturn ERR_PTR(-ENOMEM);\r\nfor (i = 0; i < nr_secs; i++) {\r\npage = vmalloc_to_page(kaddr);\r\nif (!page) {\r\npr_err("pblk: could not map vmalloc bio\n");\r\nbio_put(bio);\r\nbio = ERR_PTR(-ENOMEM);\r\ngoto out;\r\n}\r\nret = bio_add_pc_page(dev->q, bio, page, PAGE_SIZE, 0);\r\nif (ret != PAGE_SIZE) {\r\npr_err("pblk: could not add page to bio\n");\r\nbio_put(bio);\r\nbio = ERR_PTR(-ENOMEM);\r\ngoto out;\r\n}\r\nkaddr += PAGE_SIZE;\r\n}\r\nout:\r\nreturn bio;\r\n}\r\nint pblk_calc_secs(struct pblk *pblk, unsigned long secs_avail,\r\nunsigned long secs_to_flush)\r\n{\r\nint max = pblk->sec_per_write;\r\nint min = pblk->min_write_pgs;\r\nint secs_to_sync = 0;\r\nif (secs_avail >= max)\r\nsecs_to_sync = max;\r\nelse if (secs_avail >= min)\r\nsecs_to_sync = min * (secs_avail / min);\r\nelse if (secs_to_flush)\r\nsecs_to_sync = min;\r\nreturn secs_to_sync;\r\n}\r\nvoid pblk_dealloc_page(struct pblk *pblk, struct pblk_line *line, int nr_secs)\r\n{\r\nu64 addr;\r\nint i;\r\naddr = find_next_zero_bit(line->map_bitmap,\r\npblk->lm.sec_per_line, line->cur_sec);\r\nline->cur_sec = addr - nr_secs;\r\nfor (i = 0; i < nr_secs; i++, line->cur_sec--)\r\nWARN_ON(!test_and_clear_bit(line->cur_sec, line->map_bitmap));\r\n}\r\nu64 __pblk_alloc_page(struct pblk *pblk, struct pblk_line *line, int nr_secs)\r\n{\r\nu64 addr;\r\nint i;\r\nlockdep_assert_held(&line->lock);\r\nif (line->cur_sec + nr_secs > pblk->lm.sec_per_line) {\r\nWARN(1, "pblk: page allocation out of bounds\n");\r\nnr_secs = pblk->lm.sec_per_line - line->cur_sec;\r\n}\r\nline->cur_sec = addr = find_next_zero_bit(line->map_bitmap,\r\npblk->lm.sec_per_line, line->cur_sec);\r\nfor (i = 0; i < nr_secs; i++, line->cur_sec++)\r\nWARN_ON(test_and_set_bit(line->cur_sec, line->map_bitmap));\r\nreturn addr;\r\n}\r\nu64 pblk_alloc_page(struct pblk *pblk, struct pblk_line *line, int nr_secs)\r\n{\r\nu64 addr;\r\nspin_lock(&line->lock);\r\naddr = __pblk_alloc_page(pblk, line, nr_secs);\r\nline->left_msecs -= nr_secs;\r\nWARN(line->left_msecs < 0, "pblk: page allocation out of bounds\n");\r\nspin_unlock(&line->lock);\r\nreturn addr;\r\n}\r\nu64 pblk_lookup_page(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nu64 paddr;\r\nspin_lock(&line->lock);\r\npaddr = find_next_zero_bit(line->map_bitmap,\r\npblk->lm.sec_per_line, line->cur_sec);\r\nspin_unlock(&line->lock);\r\nreturn paddr;\r\n}\r\nstatic int pblk_line_submit_emeta_io(struct pblk *pblk, struct pblk_line *line,\r\nvoid *emeta_buf, u64 paddr, int dir)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nvoid *ppa_list, *meta_list;\r\nstruct bio *bio;\r\nstruct nvm_rq rqd;\r\ndma_addr_t dma_ppa_list, dma_meta_list;\r\nint min = pblk->min_write_pgs;\r\nint left_ppas = lm->emeta_sec[0];\r\nint id = line->id;\r\nint rq_ppas, rq_len;\r\nint cmd_op, bio_op;\r\nint i, j;\r\nint ret;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nif (dir == WRITE) {\r\nbio_op = REQ_OP_WRITE;\r\ncmd_op = NVM_OP_PWRITE;\r\n} else if (dir == READ) {\r\nbio_op = REQ_OP_READ;\r\ncmd_op = NVM_OP_PREAD;\r\n} else\r\nreturn -EINVAL;\r\nmeta_list = nvm_dev_dma_alloc(dev->parent, GFP_KERNEL,\r\n&dma_meta_list);\r\nif (!meta_list)\r\nreturn -ENOMEM;\r\nppa_list = meta_list + pblk_dma_meta_size;\r\ndma_ppa_list = dma_meta_list + pblk_dma_meta_size;\r\nnext_rq:\r\nmemset(&rqd, 0, sizeof(struct nvm_rq));\r\nrq_ppas = pblk_calc_secs(pblk, left_ppas, 0);\r\nrq_len = rq_ppas * geo->sec_size;\r\nbio = pblk_bio_map_addr(pblk, emeta_buf, rq_ppas, rq_len,\r\nl_mg->emeta_alloc_type, GFP_KERNEL);\r\nif (IS_ERR(bio)) {\r\nret = PTR_ERR(bio);\r\ngoto free_rqd_dma;\r\n}\r\nbio->bi_iter.bi_sector = 0;\r\nbio_set_op_attrs(bio, bio_op, 0);\r\nrqd.bio = bio;\r\nrqd.meta_list = meta_list;\r\nrqd.ppa_list = ppa_list;\r\nrqd.dma_meta_list = dma_meta_list;\r\nrqd.dma_ppa_list = dma_ppa_list;\r\nrqd.opcode = cmd_op;\r\nrqd.nr_ppas = rq_ppas;\r\nrqd.end_io = pblk_end_io_sync;\r\nrqd.private = &wait;\r\nif (dir == WRITE) {\r\nstruct pblk_sec_meta *meta_list = rqd.meta_list;\r\nrqd.flags = pblk_set_progr_mode(pblk, WRITE);\r\nfor (i = 0; i < rqd.nr_ppas; ) {\r\nspin_lock(&line->lock);\r\npaddr = __pblk_alloc_page(pblk, line, min);\r\nspin_unlock(&line->lock);\r\nfor (j = 0; j < min; j++, i++, paddr++) {\r\nmeta_list[i].lba = cpu_to_le64(ADDR_EMPTY);\r\nrqd.ppa_list[i] =\r\naddr_to_gen_ppa(pblk, paddr, id);\r\n}\r\n}\r\n} else {\r\nfor (i = 0; i < rqd.nr_ppas; ) {\r\nstruct ppa_addr ppa = addr_to_gen_ppa(pblk, paddr, id);\r\nint pos = pblk_dev_ppa_to_pos(geo, ppa);\r\nint read_type = PBLK_READ_RANDOM;\r\nif (pblk_io_aligned(pblk, rq_ppas))\r\nread_type = PBLK_READ_SEQUENTIAL;\r\nrqd.flags = pblk_set_read_mode(pblk, read_type);\r\nwhile (test_bit(pos, line->blk_bitmap)) {\r\npaddr += min;\r\nif (pblk_boundary_paddr_checks(pblk, paddr)) {\r\npr_err("pblk: corrupt emeta line:%d\n",\r\nline->id);\r\nbio_put(bio);\r\nret = -EINTR;\r\ngoto free_rqd_dma;\r\n}\r\nppa = addr_to_gen_ppa(pblk, paddr, id);\r\npos = pblk_dev_ppa_to_pos(geo, ppa);\r\n}\r\nif (pblk_boundary_paddr_checks(pblk, paddr + min)) {\r\npr_err("pblk: corrupt emeta line:%d\n",\r\nline->id);\r\nbio_put(bio);\r\nret = -EINTR;\r\ngoto free_rqd_dma;\r\n}\r\nfor (j = 0; j < min; j++, i++, paddr++)\r\nrqd.ppa_list[i] =\r\naddr_to_gen_ppa(pblk, paddr, line->id);\r\n}\r\n}\r\nret = pblk_submit_io(pblk, &rqd);\r\nif (ret) {\r\npr_err("pblk: emeta I/O submission failed: %d\n", ret);\r\nbio_put(bio);\r\ngoto free_rqd_dma;\r\n}\r\nif (!wait_for_completion_io_timeout(&wait,\r\nmsecs_to_jiffies(PBLK_COMMAND_TIMEOUT_MS))) {\r\npr_err("pblk: emeta I/O timed out\n");\r\n}\r\natomic_dec(&pblk->inflight_io);\r\nreinit_completion(&wait);\r\nif (likely(pblk->l_mg.emeta_alloc_type == PBLK_VMALLOC_META))\r\nbio_put(bio);\r\nif (rqd.error) {\r\nif (dir == WRITE)\r\npblk_log_write_err(pblk, &rqd);\r\nelse\r\npblk_log_read_err(pblk, &rqd);\r\n}\r\nemeta_buf += rq_len;\r\nleft_ppas -= rq_ppas;\r\nif (left_ppas)\r\ngoto next_rq;\r\nfree_rqd_dma:\r\nnvm_dev_dma_free(dev->parent, rqd.meta_list, rqd.dma_meta_list);\r\nreturn ret;\r\n}\r\nu64 pblk_line_smeta_start(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nint bit;\r\nbit = find_first_zero_bit(line->blk_bitmap, lm->blk_per_line);\r\nif (bit >= lm->blk_per_line)\r\nreturn -1;\r\nreturn bit * geo->sec_per_pl;\r\n}\r\nstatic int pblk_line_submit_smeta_io(struct pblk *pblk, struct pblk_line *line,\r\nu64 paddr, int dir)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct bio *bio;\r\nstruct nvm_rq rqd;\r\n__le64 *lba_list = NULL;\r\nint i, ret;\r\nint cmd_op, bio_op;\r\nint flags;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nif (dir == WRITE) {\r\nbio_op = REQ_OP_WRITE;\r\ncmd_op = NVM_OP_PWRITE;\r\nflags = pblk_set_progr_mode(pblk, WRITE);\r\nlba_list = emeta_to_lbas(pblk, line->emeta->buf);\r\n} else if (dir == READ) {\r\nbio_op = REQ_OP_READ;\r\ncmd_op = NVM_OP_PREAD;\r\nflags = pblk_set_read_mode(pblk, PBLK_READ_SEQUENTIAL);\r\n} else\r\nreturn -EINVAL;\r\nmemset(&rqd, 0, sizeof(struct nvm_rq));\r\nrqd.meta_list = nvm_dev_dma_alloc(dev->parent, GFP_KERNEL,\r\n&rqd.dma_meta_list);\r\nif (!rqd.meta_list)\r\nreturn -ENOMEM;\r\nrqd.ppa_list = rqd.meta_list + pblk_dma_meta_size;\r\nrqd.dma_ppa_list = rqd.dma_meta_list + pblk_dma_meta_size;\r\nbio = bio_map_kern(dev->q, line->smeta, lm->smeta_len, GFP_KERNEL);\r\nif (IS_ERR(bio)) {\r\nret = PTR_ERR(bio);\r\ngoto free_ppa_list;\r\n}\r\nbio->bi_iter.bi_sector = 0;\r\nbio_set_op_attrs(bio, bio_op, 0);\r\nrqd.bio = bio;\r\nrqd.opcode = cmd_op;\r\nrqd.flags = flags;\r\nrqd.nr_ppas = lm->smeta_sec;\r\nrqd.end_io = pblk_end_io_sync;\r\nrqd.private = &wait;\r\nfor (i = 0; i < lm->smeta_sec; i++, paddr++) {\r\nstruct pblk_sec_meta *meta_list = rqd.meta_list;\r\nrqd.ppa_list[i] = addr_to_gen_ppa(pblk, paddr, line->id);\r\nif (dir == WRITE) {\r\n__le64 addr_empty = cpu_to_le64(ADDR_EMPTY);\r\nmeta_list[i].lba = lba_list[paddr] = addr_empty;\r\n}\r\n}\r\nret = pblk_submit_io(pblk, &rqd);\r\nif (ret) {\r\npr_err("pblk: smeta I/O submission failed: %d\n", ret);\r\nbio_put(bio);\r\ngoto free_ppa_list;\r\n}\r\nif (!wait_for_completion_io_timeout(&wait,\r\nmsecs_to_jiffies(PBLK_COMMAND_TIMEOUT_MS))) {\r\npr_err("pblk: smeta I/O timed out\n");\r\n}\r\natomic_dec(&pblk->inflight_io);\r\nif (rqd.error) {\r\nif (dir == WRITE)\r\npblk_log_write_err(pblk, &rqd);\r\nelse\r\npblk_log_read_err(pblk, &rqd);\r\n}\r\nfree_ppa_list:\r\nnvm_dev_dma_free(dev->parent, rqd.meta_list, rqd.dma_meta_list);\r\nreturn ret;\r\n}\r\nint pblk_line_read_smeta(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nu64 bpaddr = pblk_line_smeta_start(pblk, line);\r\nreturn pblk_line_submit_smeta_io(pblk, line, bpaddr, READ);\r\n}\r\nint pblk_line_read_emeta(struct pblk *pblk, struct pblk_line *line,\r\nvoid *emeta_buf)\r\n{\r\nreturn pblk_line_submit_emeta_io(pblk, line, emeta_buf,\r\nline->emeta_ssec, READ);\r\n}\r\nstatic void pblk_setup_e_rq(struct pblk *pblk, struct nvm_rq *rqd,\r\nstruct ppa_addr ppa)\r\n{\r\nrqd->opcode = NVM_OP_ERASE;\r\nrqd->ppa_addr = ppa;\r\nrqd->nr_ppas = 1;\r\nrqd->flags = pblk_set_progr_mode(pblk, ERASE);\r\nrqd->bio = NULL;\r\n}\r\nstatic int pblk_blk_erase_sync(struct pblk *pblk, struct ppa_addr ppa)\r\n{\r\nstruct nvm_rq rqd;\r\nint ret = 0;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nmemset(&rqd, 0, sizeof(struct nvm_rq));\r\npblk_setup_e_rq(pblk, &rqd, ppa);\r\nrqd.end_io = pblk_end_io_sync;\r\nrqd.private = &wait;\r\nret = pblk_submit_io(pblk, &rqd);\r\nif (ret) {\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\npr_err("pblk: could not sync erase line:%d,blk:%d\n",\r\npblk_dev_ppa_to_line(ppa),\r\npblk_dev_ppa_to_pos(geo, ppa));\r\nrqd.error = ret;\r\ngoto out;\r\n}\r\nif (!wait_for_completion_io_timeout(&wait,\r\nmsecs_to_jiffies(PBLK_COMMAND_TIMEOUT_MS))) {\r\npr_err("pblk: sync erase timed out\n");\r\n}\r\nout:\r\nrqd.private = pblk;\r\n__pblk_end_io_erase(pblk, &rqd);\r\nreturn ret;\r\n}\r\nint pblk_line_erase(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct ppa_addr ppa;\r\nint ret, bit = -1;\r\ndo {\r\nspin_lock(&line->lock);\r\nbit = find_next_zero_bit(line->erase_bitmap, lm->blk_per_line,\r\nbit + 1);\r\nif (bit >= lm->blk_per_line) {\r\nspin_unlock(&line->lock);\r\nbreak;\r\n}\r\nppa = pblk->luns[bit].bppa;\r\nppa.g.blk = line->id;\r\natomic_dec(&line->left_eblks);\r\nWARN_ON(test_and_set_bit(bit, line->erase_bitmap));\r\nspin_unlock(&line->lock);\r\nret = pblk_blk_erase_sync(pblk, ppa);\r\nif (ret) {\r\npr_err("pblk: failed to erase line %d\n", line->id);\r\nreturn ret;\r\n}\r\n} while (1);\r\nreturn 0;\r\n}\r\nstatic void pblk_line_setup_metadata(struct pblk_line *line,\r\nstruct pblk_line_mgmt *l_mg,\r\nstruct pblk_line_meta *lm)\r\n{\r\nint meta_line;\r\nlockdep_assert_held(&l_mg->free_lock);\r\nretry_meta:\r\nmeta_line = find_first_zero_bit(&l_mg->meta_bitmap, PBLK_DATA_LINES);\r\nif (meta_line == PBLK_DATA_LINES) {\r\nspin_unlock(&l_mg->free_lock);\r\nio_schedule();\r\nspin_lock(&l_mg->free_lock);\r\ngoto retry_meta;\r\n}\r\nset_bit(meta_line, &l_mg->meta_bitmap);\r\nline->meta_line = meta_line;\r\nline->smeta = l_mg->sline_meta[meta_line];\r\nline->emeta = l_mg->eline_meta[meta_line];\r\nmemset(line->smeta, 0, lm->smeta_len);\r\nmemset(line->emeta->buf, 0, lm->emeta_len[0]);\r\nline->emeta->mem = 0;\r\natomic_set(&line->emeta->sync, 0);\r\n}\r\nstatic int pblk_line_init_metadata(struct pblk *pblk, struct pblk_line *line,\r\nstruct pblk_line *cur)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_emeta *emeta = line->emeta;\r\nstruct line_emeta *emeta_buf = emeta->buf;\r\nstruct line_smeta *smeta_buf = (struct line_smeta *)line->smeta;\r\nint nr_blk_line;\r\nnr_blk_line = lm->blk_per_line -\r\nbitmap_weight(line->blk_bitmap, lm->blk_per_line);\r\nif (nr_blk_line < lm->min_blk_line) {\r\nspin_lock(&l_mg->free_lock);\r\nspin_lock(&line->lock);\r\nline->state = PBLK_LINESTATE_BAD;\r\nspin_unlock(&line->lock);\r\nlist_add_tail(&line->list, &l_mg->bad_list);\r\nspin_unlock(&l_mg->free_lock);\r\npr_debug("pblk: line %d is bad\n", line->id);\r\nreturn 0;\r\n}\r\nline->lun_bitmap = ((void *)(smeta_buf)) + sizeof(struct line_smeta);\r\nbitmap_set(line->lun_bitmap, 0, lm->lun_bitmap_len);\r\nsmeta_buf->header.identifier = cpu_to_le32(PBLK_MAGIC);\r\nmemcpy(smeta_buf->header.uuid, pblk->instance_uuid, 16);\r\nsmeta_buf->header.id = cpu_to_le32(line->id);\r\nsmeta_buf->header.type = cpu_to_le16(line->type);\r\nsmeta_buf->header.version = cpu_to_le16(1);\r\nsmeta_buf->seq_nr = cpu_to_le64(line->seq_nr);\r\nsmeta_buf->window_wr_lun = cpu_to_le32(geo->nr_luns);\r\nif (cur) {\r\nmemcpy(line->lun_bitmap, cur->lun_bitmap, lm->lun_bitmap_len);\r\nsmeta_buf->prev_id = cpu_to_le32(cur->id);\r\ncur->emeta->buf->next_id = cpu_to_le32(line->id);\r\n} else {\r\nsmeta_buf->prev_id = cpu_to_le32(PBLK_LINE_EMPTY);\r\n}\r\nsmeta_buf->header.crc = cpu_to_le32(\r\npblk_calc_meta_header_crc(pblk, &smeta_buf->header));\r\nsmeta_buf->crc = cpu_to_le32(pblk_calc_smeta_crc(pblk, smeta_buf));\r\nmemcpy(&emeta_buf->header, &smeta_buf->header,\r\nsizeof(struct line_header));\r\nemeta_buf->seq_nr = cpu_to_le64(line->seq_nr);\r\nemeta_buf->nr_lbas = cpu_to_le64(line->sec_in_line);\r\nemeta_buf->nr_valid_lbas = cpu_to_le64(0);\r\nemeta_buf->next_id = cpu_to_le32(PBLK_LINE_EMPTY);\r\nemeta_buf->crc = cpu_to_le32(0);\r\nemeta_buf->prev_id = smeta_buf->prev_id;\r\nreturn 1;\r\n}\r\nstatic int pblk_line_init_bb(struct pblk *pblk, struct pblk_line *line,\r\nint init)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nint nr_bb = 0;\r\nu64 off;\r\nint bit = -1;\r\nline->sec_in_line = lm->sec_per_line;\r\nwhile ((bit = find_next_bit(line->blk_bitmap, lm->blk_per_line,\r\nbit + 1)) < lm->blk_per_line) {\r\noff = bit * geo->sec_per_pl;\r\nbitmap_shift_left(l_mg->bb_aux, l_mg->bb_template, off,\r\nlm->sec_per_line);\r\nbitmap_or(line->map_bitmap, line->map_bitmap, l_mg->bb_aux,\r\nlm->sec_per_line);\r\nline->sec_in_line -= geo->sec_per_blk;\r\nif (bit >= lm->emeta_bb)\r\nnr_bb++;\r\n}\r\nbit = find_first_zero_bit(line->blk_bitmap, lm->blk_per_line);\r\noff = bit * geo->sec_per_pl;\r\nbitmap_set(line->map_bitmap, off, lm->smeta_sec);\r\nline->sec_in_line -= lm->smeta_sec;\r\nline->smeta_ssec = off;\r\nline->cur_sec = off + lm->smeta_sec;\r\nif (init && pblk_line_submit_smeta_io(pblk, line, off, WRITE)) {\r\npr_debug("pblk: line smeta I/O failed. Retry\n");\r\nreturn 1;\r\n}\r\nbitmap_copy(line->invalid_bitmap, line->map_bitmap, lm->sec_per_line);\r\nbit = lm->sec_per_line;\r\noff = lm->sec_per_line - lm->emeta_sec[0];\r\nbitmap_set(line->invalid_bitmap, off, lm->emeta_sec[0]);\r\nwhile (nr_bb) {\r\noff -= geo->sec_per_pl;\r\nif (!test_bit(off, line->invalid_bitmap)) {\r\nbitmap_set(line->invalid_bitmap, off, geo->sec_per_pl);\r\nnr_bb--;\r\n}\r\n}\r\nline->sec_in_line -= lm->emeta_sec[0];\r\nline->emeta_ssec = off;\r\nline->nr_valid_lbas = 0;\r\nline->left_msecs = line->sec_in_line;\r\n*line->vsc = cpu_to_le32(line->sec_in_line);\r\nif (lm->sec_per_line - line->sec_in_line !=\r\nbitmap_weight(line->invalid_bitmap, lm->sec_per_line)) {\r\nspin_lock(&line->lock);\r\nline->state = PBLK_LINESTATE_BAD;\r\nspin_unlock(&line->lock);\r\nlist_add_tail(&line->list, &l_mg->bad_list);\r\npr_err("pblk: unexpected line %d is bad\n", line->id);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int pblk_line_prepare(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nint blk_in_line = atomic_read(&line->blk_in_line);\r\nline->map_bitmap = mempool_alloc(pblk->line_meta_pool, GFP_ATOMIC);\r\nif (!line->map_bitmap)\r\nreturn -ENOMEM;\r\nmemset(line->map_bitmap, 0, lm->sec_bitmap_len);\r\nline->invalid_bitmap = mempool_alloc(pblk->line_meta_pool, GFP_ATOMIC);\r\nif (!line->invalid_bitmap) {\r\nmempool_free(line->map_bitmap, pblk->line_meta_pool);\r\nreturn -ENOMEM;\r\n}\r\nspin_lock(&line->lock);\r\nif (line->state != PBLK_LINESTATE_FREE) {\r\nmempool_free(line->invalid_bitmap, pblk->line_meta_pool);\r\nmempool_free(line->map_bitmap, pblk->line_meta_pool);\r\nspin_unlock(&line->lock);\r\nWARN(1, "pblk: corrupted line %d, state %d\n",\r\nline->id, line->state);\r\nreturn -EAGAIN;\r\n}\r\nline->state = PBLK_LINESTATE_OPEN;\r\natomic_set(&line->left_eblks, blk_in_line);\r\natomic_set(&line->left_seblks, blk_in_line);\r\nline->meta_distance = lm->meta_distance;\r\nspin_unlock(&line->lock);\r\nbitmap_copy(line->erase_bitmap, line->blk_bitmap, lm->blk_per_line);\r\nkref_init(&line->ref);\r\nreturn 0;\r\n}\r\nint pblk_line_recov_alloc(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nint ret;\r\nspin_lock(&l_mg->free_lock);\r\nl_mg->data_line = line;\r\nlist_del(&line->list);\r\nret = pblk_line_prepare(pblk, line);\r\nif (ret) {\r\nlist_add(&line->list, &l_mg->free_list);\r\nspin_unlock(&l_mg->free_lock);\r\nreturn ret;\r\n}\r\nspin_unlock(&l_mg->free_lock);\r\npblk_rl_free_lines_dec(&pblk->rl, line);\r\nif (!pblk_line_init_bb(pblk, line, 0)) {\r\nlist_add(&line->list, &l_mg->free_list);\r\nreturn -EINTR;\r\n}\r\nreturn 0;\r\n}\r\nvoid pblk_line_recov_close(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nmempool_free(line->map_bitmap, pblk->line_meta_pool);\r\nline->map_bitmap = NULL;\r\nline->smeta = NULL;\r\nline->emeta = NULL;\r\n}\r\nstruct pblk_line *pblk_line_get(struct pblk *pblk)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct pblk_line *line;\r\nint ret, bit;\r\nlockdep_assert_held(&l_mg->free_lock);\r\nretry:\r\nif (list_empty(&l_mg->free_list)) {\r\npr_err("pblk: no free lines\n");\r\nreturn NULL;\r\n}\r\nline = list_first_entry(&l_mg->free_list, struct pblk_line, list);\r\nlist_del(&line->list);\r\nl_mg->nr_free_lines--;\r\nbit = find_first_zero_bit(line->blk_bitmap, lm->blk_per_line);\r\nif (unlikely(bit >= lm->blk_per_line)) {\r\nspin_lock(&line->lock);\r\nline->state = PBLK_LINESTATE_BAD;\r\nspin_unlock(&line->lock);\r\nlist_add_tail(&line->list, &l_mg->bad_list);\r\npr_debug("pblk: line %d is bad\n", line->id);\r\ngoto retry;\r\n}\r\nret = pblk_line_prepare(pblk, line);\r\nif (ret) {\r\nif (ret == -EAGAIN) {\r\nlist_add(&line->list, &l_mg->corrupt_list);\r\ngoto retry;\r\n} else {\r\npr_err("pblk: failed to prepare line %d\n", line->id);\r\nlist_add(&line->list, &l_mg->free_list);\r\nl_mg->nr_free_lines++;\r\nreturn NULL;\r\n}\r\n}\r\nreturn line;\r\n}\r\nstatic struct pblk_line *pblk_line_retry(struct pblk *pblk,\r\nstruct pblk_line *line)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_line *retry_line;\r\nretry:\r\nspin_lock(&l_mg->free_lock);\r\nretry_line = pblk_line_get(pblk);\r\nif (!retry_line) {\r\nl_mg->data_line = NULL;\r\nspin_unlock(&l_mg->free_lock);\r\nreturn NULL;\r\n}\r\nretry_line->smeta = line->smeta;\r\nretry_line->emeta = line->emeta;\r\nretry_line->meta_line = line->meta_line;\r\npblk_line_free(pblk, line);\r\nl_mg->data_line = retry_line;\r\nspin_unlock(&l_mg->free_lock);\r\npblk_rl_free_lines_dec(&pblk->rl, retry_line);\r\nif (pblk_line_erase(pblk, retry_line))\r\ngoto retry;\r\nreturn retry_line;\r\n}\r\nstatic void pblk_set_space_limit(struct pblk *pblk)\r\n{\r\nstruct pblk_rl *rl = &pblk->rl;\r\natomic_set(&rl->rb_space, 0);\r\n}\r\nstruct pblk_line *pblk_line_get_first_data(struct pblk *pblk)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_line *line;\r\nint is_next = 0;\r\nspin_lock(&l_mg->free_lock);\r\nline = pblk_line_get(pblk);\r\nif (!line) {\r\nspin_unlock(&l_mg->free_lock);\r\nreturn NULL;\r\n}\r\nline->seq_nr = l_mg->d_seq_nr++;\r\nline->type = PBLK_LINETYPE_DATA;\r\nl_mg->data_line = line;\r\npblk_line_setup_metadata(line, l_mg, &pblk->lm);\r\nl_mg->data_next = pblk_line_get(pblk);\r\nif (!l_mg->data_next) {\r\npblk_set_space_limit(pblk);\r\nl_mg->data_next = NULL;\r\n} else {\r\nl_mg->data_next->seq_nr = l_mg->d_seq_nr++;\r\nl_mg->data_next->type = PBLK_LINETYPE_DATA;\r\nis_next = 1;\r\n}\r\nspin_unlock(&l_mg->free_lock);\r\nif (pblk_line_erase(pblk, line)) {\r\nline = pblk_line_retry(pblk, line);\r\nif (!line)\r\nreturn NULL;\r\n}\r\npblk_rl_free_lines_dec(&pblk->rl, line);\r\nif (is_next)\r\npblk_rl_free_lines_dec(&pblk->rl, l_mg->data_next);\r\nretry_setup:\r\nif (!pblk_line_init_metadata(pblk, line, NULL)) {\r\nline = pblk_line_retry(pblk, line);\r\nif (!line)\r\nreturn NULL;\r\ngoto retry_setup;\r\n}\r\nif (!pblk_line_init_bb(pblk, line, 1)) {\r\nline = pblk_line_retry(pblk, line);\r\nif (!line)\r\nreturn NULL;\r\ngoto retry_setup;\r\n}\r\nreturn line;\r\n}\r\nstatic void pblk_stop_writes(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nlockdep_assert_held(&pblk->l_mg.free_lock);\r\npblk_set_space_limit(pblk);\r\npblk->state = PBLK_STATE_STOPPING;\r\n}\r\nvoid pblk_pipeline_stop(struct pblk *pblk)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nint ret;\r\nspin_lock(&l_mg->free_lock);\r\nif (pblk->state == PBLK_STATE_RECOVERING ||\r\npblk->state == PBLK_STATE_STOPPED) {\r\nspin_unlock(&l_mg->free_lock);\r\nreturn;\r\n}\r\npblk->state = PBLK_STATE_RECOVERING;\r\nspin_unlock(&l_mg->free_lock);\r\npblk_flush_writer(pblk);\r\npblk_wait_for_meta(pblk);\r\nret = pblk_recov_pad(pblk);\r\nif (ret) {\r\npr_err("pblk: could not close data on teardown(%d)\n", ret);\r\nreturn;\r\n}\r\nflush_workqueue(pblk->bb_wq);\r\npblk_line_close_meta_sync(pblk);\r\nspin_lock(&l_mg->free_lock);\r\npblk->state = PBLK_STATE_STOPPED;\r\nl_mg->data_line = NULL;\r\nl_mg->data_next = NULL;\r\nspin_unlock(&l_mg->free_lock);\r\n}\r\nvoid pblk_line_replace_data(struct pblk *pblk)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_line *cur, *new;\r\nunsigned int left_seblks;\r\nint is_next = 0;\r\ncur = l_mg->data_line;\r\nnew = l_mg->data_next;\r\nif (!new)\r\nreturn;\r\nl_mg->data_line = new;\r\nspin_lock(&l_mg->free_lock);\r\nif (pblk->state != PBLK_STATE_RUNNING) {\r\nl_mg->data_line = NULL;\r\nl_mg->data_next = NULL;\r\nspin_unlock(&l_mg->free_lock);\r\nreturn;\r\n}\r\npblk_line_setup_metadata(new, l_mg, &pblk->lm);\r\nspin_unlock(&l_mg->free_lock);\r\nretry_erase:\r\nleft_seblks = atomic_read(&new->left_seblks);\r\nif (left_seblks) {\r\nif (atomic_read(&new->left_eblks)) {\r\nif (pblk_line_erase(pblk, new))\r\nreturn;\r\n} else {\r\nio_schedule();\r\n}\r\ngoto retry_erase;\r\n}\r\nretry_setup:\r\nif (!pblk_line_init_metadata(pblk, new, cur)) {\r\nnew = pblk_line_retry(pblk, new);\r\nif (!new)\r\nreturn;\r\ngoto retry_setup;\r\n}\r\nif (!pblk_line_init_bb(pblk, new, 1)) {\r\nnew = pblk_line_retry(pblk, new);\r\nif (!new)\r\nreturn;\r\ngoto retry_setup;\r\n}\r\nspin_lock(&l_mg->free_lock);\r\nl_mg->data_next = pblk_line_get(pblk);\r\nif (!l_mg->data_next) {\r\npblk_stop_writes(pblk, new);\r\nl_mg->data_next = NULL;\r\n} else {\r\nl_mg->data_next->seq_nr = l_mg->d_seq_nr++;\r\nl_mg->data_next->type = PBLK_LINETYPE_DATA;\r\nis_next = 1;\r\n}\r\nspin_unlock(&l_mg->free_lock);\r\nif (is_next)\r\npblk_rl_free_lines_dec(&pblk->rl, l_mg->data_next);\r\n}\r\nvoid pblk_line_free(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nif (line->map_bitmap)\r\nmempool_free(line->map_bitmap, pblk->line_meta_pool);\r\nif (line->invalid_bitmap)\r\nmempool_free(line->invalid_bitmap, pblk->line_meta_pool);\r\n*line->vsc = cpu_to_le32(EMPTY_ENTRY);\r\nline->map_bitmap = NULL;\r\nline->invalid_bitmap = NULL;\r\nline->smeta = NULL;\r\nline->emeta = NULL;\r\n}\r\nvoid pblk_line_put(struct kref *ref)\r\n{\r\nstruct pblk_line *line = container_of(ref, struct pblk_line, ref);\r\nstruct pblk *pblk = line->pblk;\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nspin_lock(&line->lock);\r\nWARN_ON(line->state != PBLK_LINESTATE_GC);\r\nline->state = PBLK_LINESTATE_FREE;\r\nline->gc_group = PBLK_LINEGC_NONE;\r\npblk_line_free(pblk, line);\r\nspin_unlock(&line->lock);\r\nspin_lock(&l_mg->free_lock);\r\nlist_add_tail(&line->list, &l_mg->free_list);\r\nl_mg->nr_free_lines++;\r\nspin_unlock(&l_mg->free_lock);\r\npblk_rl_free_lines_inc(&pblk->rl, line);\r\n}\r\nint pblk_blk_erase_async(struct pblk *pblk, struct ppa_addr ppa)\r\n{\r\nstruct nvm_rq *rqd;\r\nint err;\r\nrqd = mempool_alloc(pblk->g_rq_pool, GFP_KERNEL);\r\nmemset(rqd, 0, pblk_g_rq_size);\r\npblk_setup_e_rq(pblk, rqd, ppa);\r\nrqd->end_io = pblk_end_io_erase;\r\nrqd->private = pblk;\r\nerr = pblk_submit_io(pblk, rqd);\r\nif (err) {\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\npr_err("pblk: could not async erase line:%d,blk:%d\n",\r\npblk_dev_ppa_to_line(ppa),\r\npblk_dev_ppa_to_pos(geo, ppa));\r\n}\r\nreturn err;\r\n}\r\nstruct pblk_line *pblk_line_get_data(struct pblk *pblk)\r\n{\r\nreturn pblk->l_mg.data_line;\r\n}\r\nstruct pblk_line *pblk_line_get_erase(struct pblk *pblk)\r\n{\r\nreturn pblk->l_mg.data_next;\r\n}\r\nint pblk_line_is_full(struct pblk_line *line)\r\n{\r\nreturn (line->left_msecs == 0);\r\n}\r\nvoid pblk_line_close_meta_sync(struct pblk *pblk)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct pblk_line *line, *tline;\r\nLIST_HEAD(list);\r\nspin_lock(&l_mg->close_lock);\r\nif (list_empty(&l_mg->emeta_list)) {\r\nspin_unlock(&l_mg->close_lock);\r\nreturn;\r\n}\r\nlist_cut_position(&list, &l_mg->emeta_list, l_mg->emeta_list.prev);\r\nspin_unlock(&l_mg->close_lock);\r\nlist_for_each_entry_safe(line, tline, &list, list) {\r\nstruct pblk_emeta *emeta = line->emeta;\r\nwhile (emeta->mem < lm->emeta_len[0]) {\r\nint ret;\r\nret = pblk_submit_meta_io(pblk, line);\r\nif (ret) {\r\npr_err("pblk: sync meta line %d failed (%d)\n",\r\nline->id, ret);\r\nreturn;\r\n}\r\n}\r\n}\r\npblk_wait_for_meta(pblk);\r\nflush_workqueue(pblk->close_wq);\r\n}\r\nstatic void pblk_line_should_sync_meta(struct pblk *pblk)\r\n{\r\nif (pblk_rl_is_limit(&pblk->rl))\r\npblk_line_close_meta_sync(pblk);\r\n}\r\nvoid pblk_line_close(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct list_head *move_list;\r\n#ifdef CONFIG_NVM_DEBUG\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nWARN(!bitmap_full(line->map_bitmap, lm->sec_per_line),\r\n"pblk: corrupt closed line %d\n", line->id);\r\n#endif\r\nspin_lock(&l_mg->free_lock);\r\nWARN_ON(!test_and_clear_bit(line->meta_line, &l_mg->meta_bitmap));\r\nspin_unlock(&l_mg->free_lock);\r\nspin_lock(&l_mg->gc_lock);\r\nspin_lock(&line->lock);\r\nWARN_ON(line->state != PBLK_LINESTATE_OPEN);\r\nline->state = PBLK_LINESTATE_CLOSED;\r\nmove_list = pblk_line_gc_list(pblk, line);\r\nlist_add_tail(&line->list, move_list);\r\nmempool_free(line->map_bitmap, pblk->line_meta_pool);\r\nline->map_bitmap = NULL;\r\nline->smeta = NULL;\r\nline->emeta = NULL;\r\nspin_unlock(&line->lock);\r\nspin_unlock(&l_mg->gc_lock);\r\npblk_gc_should_kick(pblk);\r\n}\r\nvoid pblk_line_close_meta(struct pblk *pblk, struct pblk_line *line)\r\n{\r\nstruct pblk_line_mgmt *l_mg = &pblk->l_mg;\r\nstruct pblk_line_meta *lm = &pblk->lm;\r\nstruct pblk_emeta *emeta = line->emeta;\r\nstruct line_emeta *emeta_buf = emeta->buf;\r\nmemcpy(emeta_to_vsc(pblk, emeta_buf), l_mg->vsc_list, lm->vsc_list_len);\r\nmemcpy(emeta_to_bb(emeta_buf), line->blk_bitmap, lm->blk_bitmap_len);\r\nemeta_buf->nr_valid_lbas = cpu_to_le64(line->nr_valid_lbas);\r\nemeta_buf->crc = cpu_to_le32(pblk_calc_emeta_crc(pblk, emeta_buf));\r\nspin_lock(&l_mg->close_lock);\r\nspin_lock(&line->lock);\r\nlist_add_tail(&line->list, &l_mg->emeta_list);\r\nspin_unlock(&line->lock);\r\nspin_unlock(&l_mg->close_lock);\r\npblk_line_should_sync_meta(pblk);\r\n}\r\nvoid pblk_line_close_ws(struct work_struct *work)\r\n{\r\nstruct pblk_line_ws *line_ws = container_of(work, struct pblk_line_ws,\r\nws);\r\nstruct pblk *pblk = line_ws->pblk;\r\nstruct pblk_line *line = line_ws->line;\r\npblk_line_close(pblk, line);\r\nmempool_free(line_ws, pblk->line_ws_pool);\r\n}\r\nvoid pblk_line_mark_bb(struct work_struct *work)\r\n{\r\nstruct pblk_line_ws *line_ws = container_of(work, struct pblk_line_ws,\r\nws);\r\nstruct pblk *pblk = line_ws->pblk;\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct ppa_addr *ppa = line_ws->priv;\r\nint ret;\r\nret = nvm_set_tgt_bb_tbl(dev, ppa, 1, NVM_BLK_T_GRWN_BAD);\r\nif (ret) {\r\nstruct pblk_line *line;\r\nint pos;\r\nline = &pblk->lines[pblk_dev_ppa_to_line(*ppa)];\r\npos = pblk_dev_ppa_to_pos(&dev->geo, *ppa);\r\npr_err("pblk: failed to mark bb, line:%d, pos:%d\n",\r\nline->id, pos);\r\n}\r\nkfree(ppa);\r\nmempool_free(line_ws, pblk->line_ws_pool);\r\n}\r\nvoid pblk_line_run_ws(struct pblk *pblk, struct pblk_line *line, void *priv,\r\nvoid (*work)(struct work_struct *),\r\nstruct workqueue_struct *wq)\r\n{\r\nstruct pblk_line_ws *line_ws;\r\nline_ws = mempool_alloc(pblk->line_ws_pool, GFP_ATOMIC);\r\nif (!line_ws)\r\nreturn;\r\nline_ws->pblk = pblk;\r\nline_ws->line = line;\r\nline_ws->priv = priv;\r\nINIT_WORK(&line_ws->ws, work);\r\nqueue_work(wq, &line_ws->ws);\r\n}\r\nstatic void __pblk_down_page(struct pblk *pblk, struct ppa_addr *ppa_list,\r\nint nr_ppas, int pos)\r\n{\r\nstruct pblk_lun *rlun = &pblk->luns[pos];\r\nint ret;\r\n#ifdef CONFIG_NVM_DEBUG\r\nint i;\r\nfor (i = 1; i < nr_ppas; i++)\r\nWARN_ON(ppa_list[0].g.lun != ppa_list[i].g.lun ||\r\nppa_list[0].g.ch != ppa_list[i].g.ch);\r\n#endif\r\nret = down_timeout(&rlun->wr_sem, msecs_to_jiffies(30000));\r\nif (ret) {\r\nswitch (ret) {\r\ncase -ETIME:\r\npr_err("pblk: lun semaphore timed out\n");\r\nbreak;\r\ncase -EINTR:\r\npr_err("pblk: lun semaphore timed out\n");\r\nbreak;\r\n}\r\n}\r\n}\r\nvoid pblk_down_page(struct pblk *pblk, struct ppa_addr *ppa_list, int nr_ppas)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nint pos = pblk_ppa_to_pos(geo, ppa_list[0]);\r\n__pblk_down_page(pblk, ppa_list, nr_ppas, pos);\r\n}\r\nvoid pblk_down_rq(struct pblk *pblk, struct ppa_addr *ppa_list, int nr_ppas,\r\nunsigned long *lun_bitmap)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nint pos = pblk_ppa_to_pos(geo, ppa_list[0]);\r\nif (test_and_set_bit(pos, lun_bitmap))\r\nreturn;\r\n__pblk_down_page(pblk, ppa_list, nr_ppas, pos);\r\n}\r\nvoid pblk_up_page(struct pblk *pblk, struct ppa_addr *ppa_list, int nr_ppas)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct pblk_lun *rlun;\r\nint pos = pblk_ppa_to_pos(geo, ppa_list[0]);\r\n#ifdef CONFIG_NVM_DEBUG\r\nint i;\r\nfor (i = 1; i < nr_ppas; i++)\r\nWARN_ON(ppa_list[0].g.lun != ppa_list[i].g.lun ||\r\nppa_list[0].g.ch != ppa_list[i].g.ch);\r\n#endif\r\nrlun = &pblk->luns[pos];\r\nup(&rlun->wr_sem);\r\n}\r\nvoid pblk_up_rq(struct pblk *pblk, struct ppa_addr *ppa_list, int nr_ppas,\r\nunsigned long *lun_bitmap)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct pblk_lun *rlun;\r\nint nr_luns = geo->nr_luns;\r\nint bit = -1;\r\nwhile ((bit = find_next_bit(lun_bitmap, nr_luns, bit + 1)) < nr_luns) {\r\nrlun = &pblk->luns[bit];\r\nup(&rlun->wr_sem);\r\n}\r\nkfree(lun_bitmap);\r\n}\r\nvoid pblk_update_map(struct pblk *pblk, sector_t lba, struct ppa_addr ppa)\r\n{\r\nstruct ppa_addr l2p_ppa;\r\nif (!(lba < pblk->rl.nr_secs)) {\r\nWARN(1, "pblk: corrupted L2P map request\n");\r\nreturn;\r\n}\r\nspin_lock(&pblk->trans_lock);\r\nl2p_ppa = pblk_trans_map_get(pblk, lba);\r\nif (!pblk_addr_in_cache(l2p_ppa) && !pblk_ppa_empty(l2p_ppa))\r\npblk_map_invalidate(pblk, l2p_ppa);\r\npblk_trans_map_set(pblk, lba, ppa);\r\nspin_unlock(&pblk->trans_lock);\r\n}\r\nvoid pblk_update_map_cache(struct pblk *pblk, sector_t lba, struct ppa_addr ppa)\r\n{\r\n#ifdef CONFIG_NVM_DEBUG\r\nBUG_ON(!pblk_addr_in_cache(ppa));\r\nBUG_ON(pblk_rb_pos_oob(&pblk->rwb, pblk_addr_to_cacheline(ppa)));\r\n#endif\r\npblk_update_map(pblk, lba, ppa);\r\n}\r\nint pblk_update_map_gc(struct pblk *pblk, sector_t lba, struct ppa_addr ppa,\r\nstruct pblk_line *gc_line)\r\n{\r\nstruct ppa_addr l2p_ppa;\r\nint ret = 1;\r\n#ifdef CONFIG_NVM_DEBUG\r\nBUG_ON(!pblk_addr_in_cache(ppa));\r\nBUG_ON(pblk_rb_pos_oob(&pblk->rwb, pblk_addr_to_cacheline(ppa)));\r\n#endif\r\nif (!(lba < pblk->rl.nr_secs)) {\r\nWARN(1, "pblk: corrupted L2P map request\n");\r\nreturn 0;\r\n}\r\nspin_lock(&pblk->trans_lock);\r\nl2p_ppa = pblk_trans_map_get(pblk, lba);\r\nif (pblk_addr_in_cache(l2p_ppa) || pblk_ppa_empty(l2p_ppa) ||\r\npblk_tgt_ppa_to_line(l2p_ppa) != gc_line->id) {\r\nret = 0;\r\ngoto out;\r\n}\r\npblk_trans_map_set(pblk, lba, ppa);\r\nout:\r\nspin_unlock(&pblk->trans_lock);\r\nreturn ret;\r\n}\r\nvoid pblk_update_map_dev(struct pblk *pblk, sector_t lba, struct ppa_addr ppa,\r\nstruct ppa_addr entry_line)\r\n{\r\nstruct ppa_addr l2p_line;\r\n#ifdef CONFIG_NVM_DEBUG\r\nBUG_ON(pblk_addr_in_cache(ppa));\r\n#endif\r\nif (lba == ADDR_EMPTY) {\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_inc(&pblk->padded_wb);\r\n#endif\r\npblk_map_invalidate(pblk, ppa);\r\nreturn;\r\n}\r\nif (!(lba < pblk->rl.nr_secs)) {\r\nWARN(1, "pblk: corrupted L2P map request\n");\r\nreturn;\r\n}\r\nspin_lock(&pblk->trans_lock);\r\nl2p_line = pblk_trans_map_get(pblk, lba);\r\nif (l2p_line.ppa != entry_line.ppa) {\r\nif (!pblk_ppa_empty(ppa))\r\npblk_map_invalidate(pblk, ppa);\r\ngoto out;\r\n}\r\n#ifdef CONFIG_NVM_DEBUG\r\nWARN_ON(!pblk_addr_in_cache(l2p_line) && !pblk_ppa_empty(l2p_line));\r\n#endif\r\npblk_trans_map_set(pblk, lba, ppa);\r\nout:\r\nspin_unlock(&pblk->trans_lock);\r\n}\r\nvoid pblk_lookup_l2p_seq(struct pblk *pblk, struct ppa_addr *ppas,\r\nsector_t blba, int nr_secs)\r\n{\r\nint i;\r\nspin_lock(&pblk->trans_lock);\r\nfor (i = 0; i < nr_secs; i++)\r\nppas[i] = pblk_trans_map_get(pblk, blba + i);\r\nspin_unlock(&pblk->trans_lock);\r\n}\r\nvoid pblk_lookup_l2p_rand(struct pblk *pblk, struct ppa_addr *ppas,\r\nu64 *lba_list, int nr_secs)\r\n{\r\nsector_t lba;\r\nint i;\r\nspin_lock(&pblk->trans_lock);\r\nfor (i = 0; i < nr_secs; i++) {\r\nlba = lba_list[i];\r\nif (lba == ADDR_EMPTY) {\r\nppas[i].ppa = ADDR_EMPTY;\r\n} else {\r\nif (!(lba < pblk->rl.nr_secs)) {\r\nWARN(1, "pblk: corrupted L2P map request\n");\r\ncontinue;\r\n}\r\nppas[i] = pblk_trans_map_get(pblk, lba);\r\n}\r\n}\r\nspin_unlock(&pblk->trans_lock);\r\n}
