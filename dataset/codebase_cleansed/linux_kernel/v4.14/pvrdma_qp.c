static inline void get_cqs(struct pvrdma_qp *qp, struct pvrdma_cq **send_cq,\r\nstruct pvrdma_cq **recv_cq)\r\n{\r\n*send_cq = to_vcq(qp->ibqp.send_cq);\r\n*recv_cq = to_vcq(qp->ibqp.recv_cq);\r\n}\r\nstatic void pvrdma_lock_cqs(struct pvrdma_cq *scq, struct pvrdma_cq *rcq,\r\nunsigned long *scq_flags,\r\nunsigned long *rcq_flags)\r\n__acquires(scq->cq_lock) __acquires(rcq->cq_lock)\r\n{\r\nif (scq == rcq) {\r\nspin_lock_irqsave(&scq->cq_lock, *scq_flags);\r\n__acquire(rcq->cq_lock);\r\n} else if (scq->cq_handle < rcq->cq_handle) {\r\nspin_lock_irqsave(&scq->cq_lock, *scq_flags);\r\nspin_lock_irqsave_nested(&rcq->cq_lock, *rcq_flags,\r\nSINGLE_DEPTH_NESTING);\r\n} else {\r\nspin_lock_irqsave(&rcq->cq_lock, *rcq_flags);\r\nspin_lock_irqsave_nested(&scq->cq_lock, *scq_flags,\r\nSINGLE_DEPTH_NESTING);\r\n}\r\n}\r\nstatic void pvrdma_unlock_cqs(struct pvrdma_cq *scq, struct pvrdma_cq *rcq,\r\nunsigned long *scq_flags,\r\nunsigned long *rcq_flags)\r\n__releases(scq->cq_lock) __releases(rcq->cq_lock)\r\n{\r\nif (scq == rcq) {\r\n__release(rcq->cq_lock);\r\nspin_unlock_irqrestore(&scq->cq_lock, *scq_flags);\r\n} else if (scq->cq_handle < rcq->cq_handle) {\r\nspin_unlock_irqrestore(&rcq->cq_lock, *rcq_flags);\r\nspin_unlock_irqrestore(&scq->cq_lock, *scq_flags);\r\n} else {\r\nspin_unlock_irqrestore(&scq->cq_lock, *scq_flags);\r\nspin_unlock_irqrestore(&rcq->cq_lock, *rcq_flags);\r\n}\r\n}\r\nstatic void pvrdma_reset_qp(struct pvrdma_qp *qp)\r\n{\r\nstruct pvrdma_cq *scq, *rcq;\r\nunsigned long scq_flags, rcq_flags;\r\nget_cqs(qp, &scq, &rcq);\r\npvrdma_lock_cqs(scq, rcq, &scq_flags, &rcq_flags);\r\n_pvrdma_flush_cqe(qp, scq);\r\nif (scq != rcq)\r\n_pvrdma_flush_cqe(qp, rcq);\r\npvrdma_unlock_cqs(scq, rcq, &scq_flags, &rcq_flags);\r\nif (qp->rq.ring) {\r\natomic_set(&qp->rq.ring->cons_head, 0);\r\natomic_set(&qp->rq.ring->prod_tail, 0);\r\n}\r\nif (qp->sq.ring) {\r\natomic_set(&qp->sq.ring->cons_head, 0);\r\natomic_set(&qp->sq.ring->prod_tail, 0);\r\n}\r\n}\r\nstatic int pvrdma_set_rq_size(struct pvrdma_dev *dev,\r\nstruct ib_qp_cap *req_cap,\r\nstruct pvrdma_qp *qp)\r\n{\r\nif (req_cap->max_recv_wr > dev->dsr->caps.max_qp_wr ||\r\nreq_cap->max_recv_sge > dev->dsr->caps.max_sge) {\r\ndev_warn(&dev->pdev->dev, "recv queue size invalid\n");\r\nreturn -EINVAL;\r\n}\r\nqp->rq.wqe_cnt = roundup_pow_of_two(max(1U, req_cap->max_recv_wr));\r\nqp->rq.max_sg = roundup_pow_of_two(max(1U, req_cap->max_recv_sge));\r\nreq_cap->max_recv_wr = qp->rq.wqe_cnt;\r\nreq_cap->max_recv_sge = qp->rq.max_sg;\r\nqp->rq.wqe_size = roundup_pow_of_two(sizeof(struct pvrdma_rq_wqe_hdr) +\r\nsizeof(struct pvrdma_sge) *\r\nqp->rq.max_sg);\r\nqp->npages_recv = (qp->rq.wqe_cnt * qp->rq.wqe_size + PAGE_SIZE - 1) /\r\nPAGE_SIZE;\r\nreturn 0;\r\n}\r\nstatic int pvrdma_set_sq_size(struct pvrdma_dev *dev, struct ib_qp_cap *req_cap,\r\nstruct pvrdma_qp *qp)\r\n{\r\nif (req_cap->max_send_wr > dev->dsr->caps.max_qp_wr ||\r\nreq_cap->max_send_sge > dev->dsr->caps.max_sge) {\r\ndev_warn(&dev->pdev->dev, "send queue size invalid\n");\r\nreturn -EINVAL;\r\n}\r\nqp->sq.wqe_cnt = roundup_pow_of_two(max(1U, req_cap->max_send_wr));\r\nqp->sq.max_sg = roundup_pow_of_two(max(1U, req_cap->max_send_sge));\r\nreq_cap->max_send_wr = qp->sq.wqe_cnt;\r\nreq_cap->max_send_sge = qp->sq.max_sg;\r\nqp->sq.wqe_size = roundup_pow_of_two(sizeof(struct pvrdma_sq_wqe_hdr) +\r\nsizeof(struct pvrdma_sge) *\r\nqp->sq.max_sg);\r\nqp->npages_send = PVRDMA_QP_NUM_HEADER_PAGES +\r\n(qp->sq.wqe_cnt * qp->sq.wqe_size + PAGE_SIZE - 1) /\r\nPAGE_SIZE;\r\nreturn 0;\r\n}\r\nstruct ib_qp *pvrdma_create_qp(struct ib_pd *pd,\r\nstruct ib_qp_init_attr *init_attr,\r\nstruct ib_udata *udata)\r\n{\r\nstruct pvrdma_qp *qp = NULL;\r\nstruct pvrdma_dev *dev = to_vdev(pd->device);\r\nunion pvrdma_cmd_req req;\r\nunion pvrdma_cmd_resp rsp;\r\nstruct pvrdma_cmd_create_qp *cmd = &req.create_qp;\r\nstruct pvrdma_cmd_create_qp_resp *resp = &rsp.create_qp_resp;\r\nstruct pvrdma_create_qp ucmd;\r\nunsigned long flags;\r\nint ret;\r\nif (init_attr->create_flags) {\r\ndev_warn(&dev->pdev->dev,\r\n"invalid create queuepair flags %#x\n",\r\ninit_attr->create_flags);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nif (init_attr->qp_type != IB_QPT_RC &&\r\ninit_attr->qp_type != IB_QPT_UD &&\r\ninit_attr->qp_type != IB_QPT_GSI) {\r\ndev_warn(&dev->pdev->dev, "queuepair type %d not supported\n",\r\ninit_attr->qp_type);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nif (!atomic_add_unless(&dev->num_qps, 1, dev->dsr->caps.max_qp))\r\nreturn ERR_PTR(-ENOMEM);\r\nswitch (init_attr->qp_type) {\r\ncase IB_QPT_GSI:\r\nif (init_attr->port_num == 0 ||\r\ninit_attr->port_num > pd->device->phys_port_cnt ||\r\nudata) {\r\ndev_warn(&dev->pdev->dev, "invalid queuepair attrs\n");\r\nret = -EINVAL;\r\ngoto err_qp;\r\n}\r\ncase IB_QPT_RC:\r\ncase IB_QPT_UD:\r\nqp = kzalloc(sizeof(*qp), GFP_KERNEL);\r\nif (!qp) {\r\nret = -ENOMEM;\r\ngoto err_qp;\r\n}\r\nspin_lock_init(&qp->sq.lock);\r\nspin_lock_init(&qp->rq.lock);\r\nmutex_init(&qp->mutex);\r\natomic_set(&qp->refcnt, 1);\r\ninit_waitqueue_head(&qp->wait);\r\nqp->state = IB_QPS_RESET;\r\nif (pd->uobject && udata) {\r\ndev_dbg(&dev->pdev->dev,\r\n"create queuepair from user space\n");\r\nif (ib_copy_from_udata(&ucmd, udata, sizeof(ucmd))) {\r\nret = -EFAULT;\r\ngoto err_qp;\r\n}\r\nqp->rumem = ib_umem_get(pd->uobject->context,\r\nucmd.rbuf_addr,\r\nucmd.rbuf_size, 0, 0);\r\nif (IS_ERR(qp->rumem)) {\r\nret = PTR_ERR(qp->rumem);\r\ngoto err_qp;\r\n}\r\nqp->sumem = ib_umem_get(pd->uobject->context,\r\nucmd.sbuf_addr,\r\nucmd.sbuf_size, 0, 0);\r\nif (IS_ERR(qp->sumem)) {\r\nib_umem_release(qp->rumem);\r\nret = PTR_ERR(qp->sumem);\r\ngoto err_qp;\r\n}\r\nqp->npages_send = ib_umem_page_count(qp->sumem);\r\nqp->npages_recv = ib_umem_page_count(qp->rumem);\r\nqp->npages = qp->npages_send + qp->npages_recv;\r\n} else {\r\nqp->is_kernel = true;\r\nret = pvrdma_set_sq_size(to_vdev(pd->device),\r\n&init_attr->cap, qp);\r\nif (ret)\r\ngoto err_qp;\r\nret = pvrdma_set_rq_size(to_vdev(pd->device),\r\n&init_attr->cap, qp);\r\nif (ret)\r\ngoto err_qp;\r\nqp->npages = qp->npages_send + qp->npages_recv;\r\nqp->sq.offset = PVRDMA_QP_NUM_HEADER_PAGES * PAGE_SIZE;\r\nqp->rq.offset = qp->npages_send * PAGE_SIZE;\r\n}\r\nif (qp->npages < 0 || qp->npages > PVRDMA_PAGE_DIR_MAX_PAGES) {\r\ndev_warn(&dev->pdev->dev,\r\n"overflow pages in queuepair\n");\r\nret = -EINVAL;\r\ngoto err_umem;\r\n}\r\nret = pvrdma_page_dir_init(dev, &qp->pdir, qp->npages,\r\nqp->is_kernel);\r\nif (ret) {\r\ndev_warn(&dev->pdev->dev,\r\n"could not allocate page directory\n");\r\ngoto err_umem;\r\n}\r\nif (!qp->is_kernel) {\r\npvrdma_page_dir_insert_umem(&qp->pdir, qp->sumem, 0);\r\npvrdma_page_dir_insert_umem(&qp->pdir, qp->rumem,\r\nqp->npages_send);\r\n} else {\r\nqp->sq.ring = qp->pdir.pages[0];\r\nqp->rq.ring = &qp->sq.ring[1];\r\n}\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\ngoto err_qp;\r\n}\r\ninit_attr->cap.max_inline_data = 0;\r\nmemset(cmd, 0, sizeof(*cmd));\r\ncmd->hdr.cmd = PVRDMA_CMD_CREATE_QP;\r\ncmd->pd_handle = to_vpd(pd)->pd_handle;\r\ncmd->send_cq_handle = to_vcq(init_attr->send_cq)->cq_handle;\r\ncmd->recv_cq_handle = to_vcq(init_attr->recv_cq)->cq_handle;\r\ncmd->max_send_wr = init_attr->cap.max_send_wr;\r\ncmd->max_recv_wr = init_attr->cap.max_recv_wr;\r\ncmd->max_send_sge = init_attr->cap.max_send_sge;\r\ncmd->max_recv_sge = init_attr->cap.max_recv_sge;\r\ncmd->max_inline_data = init_attr->cap.max_inline_data;\r\ncmd->sq_sig_all = (init_attr->sq_sig_type == IB_SIGNAL_ALL_WR) ? 1 : 0;\r\ncmd->qp_type = ib_qp_type_to_pvrdma(init_attr->qp_type);\r\ncmd->access_flags = IB_ACCESS_LOCAL_WRITE;\r\ncmd->total_chunks = qp->npages;\r\ncmd->send_chunks = qp->npages_send - PVRDMA_QP_NUM_HEADER_PAGES;\r\ncmd->pdir_dma = qp->pdir.dir_dma;\r\ndev_dbg(&dev->pdev->dev, "create queuepair with %d, %d, %d, %d\n",\r\ncmd->max_send_wr, cmd->max_recv_wr, cmd->max_send_sge,\r\ncmd->max_recv_sge);\r\nret = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_CREATE_QP_RESP);\r\nif (ret < 0) {\r\ndev_warn(&dev->pdev->dev,\r\n"could not create queuepair, error: %d\n", ret);\r\ngoto err_pdir;\r\n}\r\nqp->qp_handle = resp->qpn;\r\nqp->port = init_attr->port_num;\r\nqp->ibqp.qp_num = resp->qpn;\r\nspin_lock_irqsave(&dev->qp_tbl_lock, flags);\r\ndev->qp_tbl[qp->qp_handle % dev->dsr->caps.max_qp] = qp;\r\nspin_unlock_irqrestore(&dev->qp_tbl_lock, flags);\r\nreturn &qp->ibqp;\r\nerr_pdir:\r\npvrdma_page_dir_cleanup(dev, &qp->pdir);\r\nerr_umem:\r\nif (pd->uobject && udata) {\r\nif (qp->rumem)\r\nib_umem_release(qp->rumem);\r\nif (qp->sumem)\r\nib_umem_release(qp->sumem);\r\n}\r\nerr_qp:\r\nkfree(qp);\r\natomic_dec(&dev->num_qps);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void pvrdma_free_qp(struct pvrdma_qp *qp)\r\n{\r\nstruct pvrdma_dev *dev = to_vdev(qp->ibqp.device);\r\nstruct pvrdma_cq *scq;\r\nstruct pvrdma_cq *rcq;\r\nunsigned long flags, scq_flags, rcq_flags;\r\nget_cqs(qp, &scq, &rcq);\r\npvrdma_lock_cqs(scq, rcq, &scq_flags, &rcq_flags);\r\n_pvrdma_flush_cqe(qp, scq);\r\nif (scq != rcq)\r\n_pvrdma_flush_cqe(qp, rcq);\r\nspin_lock_irqsave(&dev->qp_tbl_lock, flags);\r\ndev->qp_tbl[qp->qp_handle] = NULL;\r\nspin_unlock_irqrestore(&dev->qp_tbl_lock, flags);\r\npvrdma_unlock_cqs(scq, rcq, &scq_flags, &rcq_flags);\r\natomic_dec(&qp->refcnt);\r\nwait_event(qp->wait, !atomic_read(&qp->refcnt));\r\npvrdma_page_dir_cleanup(dev, &qp->pdir);\r\nkfree(qp);\r\natomic_dec(&dev->num_qps);\r\n}\r\nint pvrdma_destroy_qp(struct ib_qp *qp)\r\n{\r\nstruct pvrdma_qp *vqp = to_vqp(qp);\r\nunion pvrdma_cmd_req req;\r\nstruct pvrdma_cmd_destroy_qp *cmd = &req.destroy_qp;\r\nint ret;\r\nmemset(cmd, 0, sizeof(*cmd));\r\ncmd->hdr.cmd = PVRDMA_CMD_DESTROY_QP;\r\ncmd->qp_handle = vqp->qp_handle;\r\nret = pvrdma_cmd_post(to_vdev(qp->device), &req, NULL, 0);\r\nif (ret < 0)\r\ndev_warn(&to_vdev(qp->device)->pdev->dev,\r\n"destroy queuepair failed, error: %d\n", ret);\r\npvrdma_free_qp(vqp);\r\nreturn 0;\r\n}\r\nint pvrdma_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\r\nint attr_mask, struct ib_udata *udata)\r\n{\r\nstruct pvrdma_dev *dev = to_vdev(ibqp->device);\r\nstruct pvrdma_qp *qp = to_vqp(ibqp);\r\nunion pvrdma_cmd_req req;\r\nunion pvrdma_cmd_resp rsp;\r\nstruct pvrdma_cmd_modify_qp *cmd = &req.modify_qp;\r\nint cur_state, next_state;\r\nint ret;\r\nmutex_lock(&qp->mutex);\r\ncur_state = (attr_mask & IB_QP_CUR_STATE) ? attr->cur_qp_state :\r\nqp->state;\r\nnext_state = (attr_mask & IB_QP_STATE) ? attr->qp_state : cur_state;\r\nif (!ib_modify_qp_is_ok(cur_state, next_state, ibqp->qp_type,\r\nattr_mask, IB_LINK_LAYER_ETHERNET)) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (attr_mask & IB_QP_PORT) {\r\nif (attr->port_num == 0 ||\r\nattr->port_num > ibqp->device->phys_port_cnt) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\nif (attr_mask & IB_QP_MIN_RNR_TIMER) {\r\nif (attr->min_rnr_timer > 31) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\nif (attr_mask & IB_QP_PKEY_INDEX) {\r\nif (attr->pkey_index >= dev->dsr->caps.max_pkeys) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\nif (attr_mask & IB_QP_QKEY)\r\nqp->qkey = attr->qkey;\r\nif (cur_state == next_state && cur_state == IB_QPS_RESET) {\r\nret = 0;\r\ngoto out;\r\n}\r\nqp->state = next_state;\r\nmemset(cmd, 0, sizeof(*cmd));\r\ncmd->hdr.cmd = PVRDMA_CMD_MODIFY_QP;\r\ncmd->qp_handle = qp->qp_handle;\r\ncmd->attr_mask = ib_qp_attr_mask_to_pvrdma(attr_mask);\r\ncmd->attrs.qp_state = ib_qp_state_to_pvrdma(attr->qp_state);\r\ncmd->attrs.cur_qp_state =\r\nib_qp_state_to_pvrdma(attr->cur_qp_state);\r\ncmd->attrs.path_mtu = ib_mtu_to_pvrdma(attr->path_mtu);\r\ncmd->attrs.path_mig_state =\r\nib_mig_state_to_pvrdma(attr->path_mig_state);\r\ncmd->attrs.qkey = attr->qkey;\r\ncmd->attrs.rq_psn = attr->rq_psn;\r\ncmd->attrs.sq_psn = attr->sq_psn;\r\ncmd->attrs.dest_qp_num = attr->dest_qp_num;\r\ncmd->attrs.qp_access_flags =\r\nib_access_flags_to_pvrdma(attr->qp_access_flags);\r\ncmd->attrs.pkey_index = attr->pkey_index;\r\ncmd->attrs.alt_pkey_index = attr->alt_pkey_index;\r\ncmd->attrs.en_sqd_async_notify = attr->en_sqd_async_notify;\r\ncmd->attrs.sq_draining = attr->sq_draining;\r\ncmd->attrs.max_rd_atomic = attr->max_rd_atomic;\r\ncmd->attrs.max_dest_rd_atomic = attr->max_dest_rd_atomic;\r\ncmd->attrs.min_rnr_timer = attr->min_rnr_timer;\r\ncmd->attrs.port_num = attr->port_num;\r\ncmd->attrs.timeout = attr->timeout;\r\ncmd->attrs.retry_cnt = attr->retry_cnt;\r\ncmd->attrs.rnr_retry = attr->rnr_retry;\r\ncmd->attrs.alt_port_num = attr->alt_port_num;\r\ncmd->attrs.alt_timeout = attr->alt_timeout;\r\nib_qp_cap_to_pvrdma(&cmd->attrs.cap, &attr->cap);\r\nrdma_ah_attr_to_pvrdma(&cmd->attrs.ah_attr, &attr->ah_attr);\r\nrdma_ah_attr_to_pvrdma(&cmd->attrs.alt_ah_attr, &attr->alt_ah_attr);\r\nret = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_MODIFY_QP_RESP);\r\nif (ret < 0) {\r\ndev_warn(&dev->pdev->dev,\r\n"could not modify queuepair, error: %d\n", ret);\r\n} else if (rsp.hdr.err > 0) {\r\ndev_warn(&dev->pdev->dev,\r\n"cannot modify queuepair, error: %d\n", rsp.hdr.err);\r\nret = -EINVAL;\r\n}\r\nif (ret == 0 && next_state == IB_QPS_RESET)\r\npvrdma_reset_qp(qp);\r\nout:\r\nmutex_unlock(&qp->mutex);\r\nreturn ret;\r\n}\r\nstatic inline void *get_sq_wqe(struct pvrdma_qp *qp, unsigned int n)\r\n{\r\nreturn pvrdma_page_dir_get_ptr(&qp->pdir,\r\nqp->sq.offset + n * qp->sq.wqe_size);\r\n}\r\nstatic inline void *get_rq_wqe(struct pvrdma_qp *qp, unsigned int n)\r\n{\r\nreturn pvrdma_page_dir_get_ptr(&qp->pdir,\r\nqp->rq.offset + n * qp->rq.wqe_size);\r\n}\r\nstatic int set_reg_seg(struct pvrdma_sq_wqe_hdr *wqe_hdr, struct ib_reg_wr *wr)\r\n{\r\nstruct pvrdma_user_mr *mr = to_vmr(wr->mr);\r\nwqe_hdr->wr.fast_reg.iova_start = mr->ibmr.iova;\r\nwqe_hdr->wr.fast_reg.pl_pdir_dma = mr->pdir.dir_dma;\r\nwqe_hdr->wr.fast_reg.page_shift = mr->page_shift;\r\nwqe_hdr->wr.fast_reg.page_list_len = mr->npages;\r\nwqe_hdr->wr.fast_reg.length = mr->ibmr.length;\r\nwqe_hdr->wr.fast_reg.access_flags = wr->access;\r\nwqe_hdr->wr.fast_reg.rkey = wr->key;\r\nreturn pvrdma_page_dir_insert_page_list(&mr->pdir, mr->pages,\r\nmr->npages);\r\n}\r\nint pvrdma_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,\r\nstruct ib_send_wr **bad_wr)\r\n{\r\nstruct pvrdma_qp *qp = to_vqp(ibqp);\r\nstruct pvrdma_dev *dev = to_vdev(ibqp->device);\r\nunsigned long flags;\r\nstruct pvrdma_sq_wqe_hdr *wqe_hdr;\r\nstruct pvrdma_sge *sge;\r\nint i, ret;\r\nif (qp->state < IB_QPS_RTS) {\r\n*bad_wr = wr;\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&qp->sq.lock, flags);\r\nwhile (wr) {\r\nunsigned int tail = 0;\r\nif (unlikely(!pvrdma_idx_ring_has_space(\r\nqp->sq.ring, qp->sq.wqe_cnt, &tail))) {\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"send queue is full\n");\r\n*bad_wr = wr;\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (unlikely(wr->num_sge > qp->sq.max_sg || wr->num_sge < 0)) {\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"send SGE overflow\n");\r\n*bad_wr = wr;\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (unlikely(wr->opcode < 0)) {\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"invalid send opcode\n");\r\n*bad_wr = wr;\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (qp->ibqp.qp_type != IB_QPT_UD &&\r\nqp->ibqp.qp_type != IB_QPT_RC &&\r\nwr->opcode != IB_WR_SEND) {\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"unsupported queuepair type\n");\r\n*bad_wr = wr;\r\nret = -EINVAL;\r\ngoto out;\r\n} else if (qp->ibqp.qp_type == IB_QPT_UD ||\r\nqp->ibqp.qp_type == IB_QPT_GSI) {\r\nif (wr->opcode != IB_WR_SEND &&\r\nwr->opcode != IB_WR_SEND_WITH_IMM) {\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"invalid send opcode\n");\r\n*bad_wr = wr;\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\nwqe_hdr = (struct pvrdma_sq_wqe_hdr *)get_sq_wqe(qp, tail);\r\nmemset(wqe_hdr, 0, sizeof(*wqe_hdr));\r\nwqe_hdr->wr_id = wr->wr_id;\r\nwqe_hdr->num_sge = wr->num_sge;\r\nwqe_hdr->opcode = ib_wr_opcode_to_pvrdma(wr->opcode);\r\nwqe_hdr->send_flags = ib_send_flags_to_pvrdma(wr->send_flags);\r\nif (wr->opcode == IB_WR_SEND_WITH_IMM ||\r\nwr->opcode == IB_WR_RDMA_WRITE_WITH_IMM)\r\nwqe_hdr->ex.imm_data = wr->ex.imm_data;\r\nswitch (qp->ibqp.qp_type) {\r\ncase IB_QPT_GSI:\r\ncase IB_QPT_UD:\r\nif (unlikely(!ud_wr(wr)->ah)) {\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"invalid address handle\n");\r\n*bad_wr = wr;\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nwqe_hdr->wr.ud.remote_qpn = ud_wr(wr)->remote_qpn;\r\nwqe_hdr->wr.ud.remote_qkey =\r\nud_wr(wr)->remote_qkey & 0x80000000 ?\r\nqp->qkey : ud_wr(wr)->remote_qkey;\r\nwqe_hdr->wr.ud.av = to_vah(ud_wr(wr)->ah)->av;\r\nbreak;\r\ncase IB_QPT_RC:\r\nswitch (wr->opcode) {\r\ncase IB_WR_RDMA_READ:\r\ncase IB_WR_RDMA_WRITE:\r\ncase IB_WR_RDMA_WRITE_WITH_IMM:\r\nwqe_hdr->wr.rdma.remote_addr =\r\nrdma_wr(wr)->remote_addr;\r\nwqe_hdr->wr.rdma.rkey = rdma_wr(wr)->rkey;\r\nbreak;\r\ncase IB_WR_LOCAL_INV:\r\ncase IB_WR_SEND_WITH_INV:\r\nwqe_hdr->ex.invalidate_rkey =\r\nwr->ex.invalidate_rkey;\r\nbreak;\r\ncase IB_WR_ATOMIC_CMP_AND_SWP:\r\ncase IB_WR_ATOMIC_FETCH_AND_ADD:\r\nwqe_hdr->wr.atomic.remote_addr =\r\natomic_wr(wr)->remote_addr;\r\nwqe_hdr->wr.atomic.rkey = atomic_wr(wr)->rkey;\r\nwqe_hdr->wr.atomic.compare_add =\r\natomic_wr(wr)->compare_add;\r\nif (wr->opcode == IB_WR_ATOMIC_CMP_AND_SWP)\r\nwqe_hdr->wr.atomic.swap =\r\natomic_wr(wr)->swap;\r\nbreak;\r\ncase IB_WR_REG_MR:\r\nret = set_reg_seg(wqe_hdr, reg_wr(wr));\r\nif (ret < 0) {\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"Failed to set fast register work request\n");\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ndefault:\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"invalid queuepair type\n");\r\nret = -EINVAL;\r\n*bad_wr = wr;\r\ngoto out;\r\n}\r\nsge = (struct pvrdma_sge *)(wqe_hdr + 1);\r\nfor (i = 0; i < wr->num_sge; i++) {\r\nsge->addr = wr->sg_list[i].addr;\r\nsge->length = wr->sg_list[i].length;\r\nsge->lkey = wr->sg_list[i].lkey;\r\nsge++;\r\n}\r\nsmp_wmb();\r\npvrdma_idx_ring_inc(&qp->sq.ring->prod_tail,\r\nqp->sq.wqe_cnt);\r\nwr = wr->next;\r\n}\r\nret = 0;\r\nout:\r\nspin_unlock_irqrestore(&qp->sq.lock, flags);\r\nif (!ret)\r\npvrdma_write_uar_qp(dev, PVRDMA_UAR_QP_SEND | qp->qp_handle);\r\nreturn ret;\r\n}\r\nint pvrdma_post_recv(struct ib_qp *ibqp, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nstruct pvrdma_dev *dev = to_vdev(ibqp->device);\r\nunsigned long flags;\r\nstruct pvrdma_qp *qp = to_vqp(ibqp);\r\nstruct pvrdma_rq_wqe_hdr *wqe_hdr;\r\nstruct pvrdma_sge *sge;\r\nint ret = 0;\r\nint i;\r\nif (qp->state == IB_QPS_RESET) {\r\n*bad_wr = wr;\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&qp->rq.lock, flags);\r\nwhile (wr) {\r\nunsigned int tail = 0;\r\nif (unlikely(wr->num_sge > qp->rq.max_sg ||\r\nwr->num_sge < 0)) {\r\nret = -EINVAL;\r\n*bad_wr = wr;\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"recv SGE overflow\n");\r\ngoto out;\r\n}\r\nif (unlikely(!pvrdma_idx_ring_has_space(\r\nqp->rq.ring, qp->rq.wqe_cnt, &tail))) {\r\nret = -ENOMEM;\r\n*bad_wr = wr;\r\ndev_warn_ratelimited(&dev->pdev->dev,\r\n"recv queue full\n");\r\ngoto out;\r\n}\r\nwqe_hdr = (struct pvrdma_rq_wqe_hdr *)get_rq_wqe(qp, tail);\r\nwqe_hdr->wr_id = wr->wr_id;\r\nwqe_hdr->num_sge = wr->num_sge;\r\nwqe_hdr->total_len = 0;\r\nsge = (struct pvrdma_sge *)(wqe_hdr + 1);\r\nfor (i = 0; i < wr->num_sge; i++) {\r\nsge->addr = wr->sg_list[i].addr;\r\nsge->length = wr->sg_list[i].length;\r\nsge->lkey = wr->sg_list[i].lkey;\r\nsge++;\r\n}\r\nsmp_wmb();\r\npvrdma_idx_ring_inc(&qp->rq.ring->prod_tail,\r\nqp->rq.wqe_cnt);\r\nwr = wr->next;\r\n}\r\nspin_unlock_irqrestore(&qp->rq.lock, flags);\r\npvrdma_write_uar_qp(dev, PVRDMA_UAR_QP_RECV | qp->qp_handle);\r\nreturn ret;\r\nout:\r\nspin_unlock_irqrestore(&qp->rq.lock, flags);\r\nreturn ret;\r\n}\r\nint pvrdma_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\r\nint attr_mask, struct ib_qp_init_attr *init_attr)\r\n{\r\nstruct pvrdma_dev *dev = to_vdev(ibqp->device);\r\nstruct pvrdma_qp *qp = to_vqp(ibqp);\r\nunion pvrdma_cmd_req req;\r\nunion pvrdma_cmd_resp rsp;\r\nstruct pvrdma_cmd_query_qp *cmd = &req.query_qp;\r\nstruct pvrdma_cmd_query_qp_resp *resp = &rsp.query_qp_resp;\r\nint ret = 0;\r\nmutex_lock(&qp->mutex);\r\nif (qp->state == IB_QPS_RESET) {\r\nattr->qp_state = IB_QPS_RESET;\r\ngoto out;\r\n}\r\nmemset(cmd, 0, sizeof(*cmd));\r\ncmd->hdr.cmd = PVRDMA_CMD_QUERY_QP;\r\ncmd->qp_handle = qp->qp_handle;\r\ncmd->attr_mask = ib_qp_attr_mask_to_pvrdma(attr_mask);\r\nret = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_QUERY_QP_RESP);\r\nif (ret < 0) {\r\ndev_warn(&dev->pdev->dev,\r\n"could not query queuepair, error: %d\n", ret);\r\ngoto out;\r\n}\r\nattr->qp_state = pvrdma_qp_state_to_ib(resp->attrs.qp_state);\r\nattr->cur_qp_state =\r\npvrdma_qp_state_to_ib(resp->attrs.cur_qp_state);\r\nattr->path_mtu = pvrdma_mtu_to_ib(resp->attrs.path_mtu);\r\nattr->path_mig_state =\r\npvrdma_mig_state_to_ib(resp->attrs.path_mig_state);\r\nattr->qkey = resp->attrs.qkey;\r\nattr->rq_psn = resp->attrs.rq_psn;\r\nattr->sq_psn = resp->attrs.sq_psn;\r\nattr->dest_qp_num = resp->attrs.dest_qp_num;\r\nattr->qp_access_flags =\r\npvrdma_access_flags_to_ib(resp->attrs.qp_access_flags);\r\nattr->pkey_index = resp->attrs.pkey_index;\r\nattr->alt_pkey_index = resp->attrs.alt_pkey_index;\r\nattr->en_sqd_async_notify = resp->attrs.en_sqd_async_notify;\r\nattr->sq_draining = resp->attrs.sq_draining;\r\nattr->max_rd_atomic = resp->attrs.max_rd_atomic;\r\nattr->max_dest_rd_atomic = resp->attrs.max_dest_rd_atomic;\r\nattr->min_rnr_timer = resp->attrs.min_rnr_timer;\r\nattr->port_num = resp->attrs.port_num;\r\nattr->timeout = resp->attrs.timeout;\r\nattr->retry_cnt = resp->attrs.retry_cnt;\r\nattr->rnr_retry = resp->attrs.rnr_retry;\r\nattr->alt_port_num = resp->attrs.alt_port_num;\r\nattr->alt_timeout = resp->attrs.alt_timeout;\r\npvrdma_qp_cap_to_ib(&attr->cap, &resp->attrs.cap);\r\npvrdma_ah_attr_to_rdma(&attr->ah_attr, &resp->attrs.ah_attr);\r\npvrdma_ah_attr_to_rdma(&attr->alt_ah_attr, &resp->attrs.alt_ah_attr);\r\nqp->state = attr->qp_state;\r\nret = 0;\r\nout:\r\nattr->cur_qp_state = attr->qp_state;\r\ninit_attr->event_handler = qp->ibqp.event_handler;\r\ninit_attr->qp_context = qp->ibqp.qp_context;\r\ninit_attr->send_cq = qp->ibqp.send_cq;\r\ninit_attr->recv_cq = qp->ibqp.recv_cq;\r\ninit_attr->srq = qp->ibqp.srq;\r\ninit_attr->xrcd = NULL;\r\ninit_attr->cap = attr->cap;\r\ninit_attr->sq_sig_type = 0;\r\ninit_attr->qp_type = qp->ibqp.qp_type;\r\ninit_attr->create_flags = 0;\r\ninit_attr->port_num = qp->port;\r\nmutex_unlock(&qp->mutex);\r\nreturn ret;\r\n}
