static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)\r\n{\r\nmemset(done, 0, sizeof(*done));\r\natomic_set(&done->nr_todo, nr_todo);\r\ninit_completion(&done->completion);\r\n}\r\nstatic void cpu_stop_signal_done(struct cpu_stop_done *done)\r\n{\r\nif (atomic_dec_and_test(&done->nr_todo))\r\ncomplete(&done->completion);\r\n}\r\nstatic void __cpu_stop_queue_work(struct cpu_stopper *stopper,\r\nstruct cpu_stop_work *work)\r\n{\r\nlist_add_tail(&work->list, &stopper->works);\r\nwake_up_process(stopper->thread);\r\n}\r\nstatic bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nunsigned long flags;\r\nbool enabled;\r\nspin_lock_irqsave(&stopper->lock, flags);\r\nenabled = stopper->enabled;\r\nif (enabled)\r\n__cpu_stop_queue_work(stopper, work);\r\nelse if (work->done)\r\ncpu_stop_signal_done(work->done);\r\nspin_unlock_irqrestore(&stopper->lock, flags);\r\nreturn enabled;\r\n}\r\nint stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)\r\n{\r\nstruct cpu_stop_done done;\r\nstruct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done };\r\ncpu_stop_init_done(&done, 1);\r\nif (!cpu_stop_queue_work(cpu, &work))\r\nreturn -ENOENT;\r\ncond_resched();\r\nwait_for_completion(&done.completion);\r\nreturn done.ret;\r\n}\r\nstatic void set_state(struct multi_stop_data *msdata,\r\nenum multi_stop_state newstate)\r\n{\r\natomic_set(&msdata->thread_ack, msdata->num_threads);\r\nsmp_wmb();\r\nmsdata->state = newstate;\r\n}\r\nstatic void ack_state(struct multi_stop_data *msdata)\r\n{\r\nif (atomic_dec_and_test(&msdata->thread_ack))\r\nset_state(msdata, msdata->state + 1);\r\n}\r\nstatic int multi_cpu_stop(void *data)\r\n{\r\nstruct multi_stop_data *msdata = data;\r\nenum multi_stop_state curstate = MULTI_STOP_NONE;\r\nint cpu = smp_processor_id(), err = 0;\r\nunsigned long flags;\r\nbool is_active;\r\nlocal_save_flags(flags);\r\nif (!msdata->active_cpus)\r\nis_active = cpu == cpumask_first(cpu_online_mask);\r\nelse\r\nis_active = cpumask_test_cpu(cpu, msdata->active_cpus);\r\ndo {\r\ncpu_relax_yield();\r\nif (msdata->state != curstate) {\r\ncurstate = msdata->state;\r\nswitch (curstate) {\r\ncase MULTI_STOP_DISABLE_IRQ:\r\nlocal_irq_disable();\r\nhard_irq_disable();\r\nbreak;\r\ncase MULTI_STOP_RUN:\r\nif (is_active)\r\nerr = msdata->fn(msdata->data);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nack_state(msdata);\r\n} else if (curstate > MULTI_STOP_PREPARE) {\r\ntouch_nmi_watchdog();\r\n}\r\n} while (curstate != MULTI_STOP_EXIT);\r\nlocal_irq_restore(flags);\r\nreturn err;\r\n}\r\nstatic int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,\r\nint cpu2, struct cpu_stop_work *work2)\r\n{\r\nstruct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);\r\nstruct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);\r\nint err;\r\nretry:\r\nspin_lock_irq(&stopper1->lock);\r\nspin_lock_nested(&stopper2->lock, SINGLE_DEPTH_NESTING);\r\nerr = -ENOENT;\r\nif (!stopper1->enabled || !stopper2->enabled)\r\ngoto unlock;\r\nerr = -EDEADLK;\r\nif (unlikely(stop_cpus_in_progress))\r\ngoto unlock;\r\nerr = 0;\r\n__cpu_stop_queue_work(stopper1, work1);\r\n__cpu_stop_queue_work(stopper2, work2);\r\nunlock:\r\nspin_unlock(&stopper2->lock);\r\nspin_unlock_irq(&stopper1->lock);\r\nif (unlikely(err == -EDEADLK)) {\r\nwhile (stop_cpus_in_progress)\r\ncpu_relax();\r\ngoto retry;\r\n}\r\nreturn err;\r\n}\r\nint stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)\r\n{\r\nstruct cpu_stop_done done;\r\nstruct cpu_stop_work work1, work2;\r\nstruct multi_stop_data msdata;\r\nmsdata = (struct multi_stop_data){\r\n.fn = fn,\r\n.data = arg,\r\n.num_threads = 2,\r\n.active_cpus = cpumask_of(cpu1),\r\n};\r\nwork1 = work2 = (struct cpu_stop_work){\r\n.fn = multi_cpu_stop,\r\n.arg = &msdata,\r\n.done = &done\r\n};\r\ncpu_stop_init_done(&done, 2);\r\nset_state(&msdata, MULTI_STOP_PREPARE);\r\nif (cpu1 > cpu2)\r\nswap(cpu1, cpu2);\r\nif (cpu_stop_queue_two_works(cpu1, &work1, cpu2, &work2))\r\nreturn -ENOENT;\r\nwait_for_completion(&done.completion);\r\nreturn done.ret;\r\n}\r\nbool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,\r\nstruct cpu_stop_work *work_buf)\r\n{\r\n*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, };\r\nreturn cpu_stop_queue_work(cpu, work_buf);\r\n}\r\nstatic bool queue_stop_cpus_work(const struct cpumask *cpumask,\r\ncpu_stop_fn_t fn, void *arg,\r\nstruct cpu_stop_done *done)\r\n{\r\nstruct cpu_stop_work *work;\r\nunsigned int cpu;\r\nbool queued = false;\r\npreempt_disable();\r\nstop_cpus_in_progress = true;\r\nfor_each_cpu(cpu, cpumask) {\r\nwork = &per_cpu(cpu_stopper.stop_work, cpu);\r\nwork->fn = fn;\r\nwork->arg = arg;\r\nwork->done = done;\r\nif (cpu_stop_queue_work(cpu, work))\r\nqueued = true;\r\n}\r\nstop_cpus_in_progress = false;\r\npreempt_enable();\r\nreturn queued;\r\n}\r\nstatic int __stop_cpus(const struct cpumask *cpumask,\r\ncpu_stop_fn_t fn, void *arg)\r\n{\r\nstruct cpu_stop_done done;\r\ncpu_stop_init_done(&done, cpumask_weight(cpumask));\r\nif (!queue_stop_cpus_work(cpumask, fn, arg, &done))\r\nreturn -ENOENT;\r\nwait_for_completion(&done.completion);\r\nreturn done.ret;\r\n}\r\nint stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\r\n{\r\nint ret;\r\nmutex_lock(&stop_cpus_mutex);\r\nret = __stop_cpus(cpumask, fn, arg);\r\nmutex_unlock(&stop_cpus_mutex);\r\nreturn ret;\r\n}\r\nint try_stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\r\n{\r\nint ret;\r\nif (!mutex_trylock(&stop_cpus_mutex))\r\nreturn -EAGAIN;\r\nret = __stop_cpus(cpumask, fn, arg);\r\nmutex_unlock(&stop_cpus_mutex);\r\nreturn ret;\r\n}\r\nstatic int cpu_stop_should_run(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nunsigned long flags;\r\nint run;\r\nspin_lock_irqsave(&stopper->lock, flags);\r\nrun = !list_empty(&stopper->works);\r\nspin_unlock_irqrestore(&stopper->lock, flags);\r\nreturn run;\r\n}\r\nstatic void cpu_stopper_thread(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nstruct cpu_stop_work *work;\r\nrepeat:\r\nwork = NULL;\r\nspin_lock_irq(&stopper->lock);\r\nif (!list_empty(&stopper->works)) {\r\nwork = list_first_entry(&stopper->works,\r\nstruct cpu_stop_work, list);\r\nlist_del_init(&work->list);\r\n}\r\nspin_unlock_irq(&stopper->lock);\r\nif (work) {\r\ncpu_stop_fn_t fn = work->fn;\r\nvoid *arg = work->arg;\r\nstruct cpu_stop_done *done = work->done;\r\nint ret;\r\npreempt_count_inc();\r\nret = fn(arg);\r\nif (done) {\r\nif (ret)\r\ndone->ret = ret;\r\ncpu_stop_signal_done(done);\r\n}\r\npreempt_count_dec();\r\nWARN_ONCE(preempt_count(),\r\n"cpu_stop: %pf(%p) leaked preempt count\n", fn, arg);\r\ngoto repeat;\r\n}\r\n}\r\nvoid stop_machine_park(int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nstopper->enabled = false;\r\nkthread_park(stopper->thread);\r\n}\r\nstatic void cpu_stop_create(unsigned int cpu)\r\n{\r\nsched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));\r\n}\r\nstatic void cpu_stop_park(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nWARN_ON(!list_empty(&stopper->works));\r\n}\r\nvoid stop_machine_unpark(int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nstopper->enabled = true;\r\nkthread_unpark(stopper->thread);\r\n}\r\nstatic int __init cpu_stop_init(void)\r\n{\r\nunsigned int cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nspin_lock_init(&stopper->lock);\r\nINIT_LIST_HEAD(&stopper->works);\r\n}\r\nBUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));\r\nstop_machine_unpark(raw_smp_processor_id());\r\nstop_machine_initialized = true;\r\nreturn 0;\r\n}\r\nint stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,\r\nconst struct cpumask *cpus)\r\n{\r\nstruct multi_stop_data msdata = {\r\n.fn = fn,\r\n.data = data,\r\n.num_threads = num_online_cpus(),\r\n.active_cpus = cpus,\r\n};\r\nlockdep_assert_cpus_held();\r\nif (!stop_machine_initialized) {\r\nunsigned long flags;\r\nint ret;\r\nWARN_ON_ONCE(msdata.num_threads != 1);\r\nlocal_irq_save(flags);\r\nhard_irq_disable();\r\nret = (*fn)(data);\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nset_state(&msdata, MULTI_STOP_PREPARE);\r\nreturn stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);\r\n}\r\nint stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)\r\n{\r\nint ret;\r\ncpus_read_lock();\r\nret = stop_machine_cpuslocked(fn, data, cpus);\r\ncpus_read_unlock();\r\nreturn ret;\r\n}\r\nint stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,\r\nconst struct cpumask *cpus)\r\n{\r\nstruct multi_stop_data msdata = { .fn = fn, .data = data,\r\n.active_cpus = cpus };\r\nstruct cpu_stop_done done;\r\nint ret;\r\nBUG_ON(cpu_active(raw_smp_processor_id()));\r\nmsdata.num_threads = num_active_cpus() + 1;\r\nwhile (!mutex_trylock(&stop_cpus_mutex))\r\ncpu_relax();\r\nset_state(&msdata, MULTI_STOP_PREPARE);\r\ncpu_stop_init_done(&done, num_active_cpus());\r\nqueue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,\r\n&done);\r\nret = multi_cpu_stop(&msdata);\r\nwhile (!completion_done(&done.completion))\r\ncpu_relax();\r\nmutex_unlock(&stop_cpus_mutex);\r\nreturn ret ?: done.ret;\r\n}
