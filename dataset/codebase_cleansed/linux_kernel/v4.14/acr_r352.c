static void\r\nacr_r352_generate_flcn_bl_desc(const struct nvkm_acr *acr,\r\nconst struct ls_ucode_img *img, u64 wpr_addr,\r\nvoid *_desc)\r\n{\r\nstruct acr_r352_flcn_bl_desc *desc = _desc;\r\nconst struct ls_ucode_img_desc *pdesc = &img->ucode_desc;\r\nu64 base, addr_code, addr_data;\r\nbase = wpr_addr + img->ucode_off + pdesc->app_start_offset;\r\naddr_code = (base + pdesc->app_resident_code_offset) >> 8;\r\naddr_data = (base + pdesc->app_resident_data_offset) >> 8;\r\ndesc->ctx_dma = FALCON_DMAIDX_UCODE;\r\ndesc->code_dma_base = lower_32_bits(addr_code);\r\ndesc->code_dma_base1 = upper_32_bits(addr_code);\r\ndesc->non_sec_code_off = pdesc->app_resident_code_offset;\r\ndesc->non_sec_code_size = pdesc->app_resident_code_size;\r\ndesc->code_entry_point = pdesc->app_imem_entry;\r\ndesc->data_dma_base = lower_32_bits(addr_data);\r\ndesc->data_dma_base1 = upper_32_bits(addr_data);\r\ndesc->data_size = pdesc->app_resident_data_size;\r\n}\r\nstruct ls_ucode_img *\r\nacr_r352_ls_ucode_img_load(const struct acr_r352 *acr,\r\nconst struct nvkm_secboot *sb,\r\nenum nvkm_secboot_falcon falcon_id)\r\n{\r\nconst struct nvkm_subdev *subdev = acr->base.subdev;\r\nstruct ls_ucode_img_r352 *img;\r\nint ret;\r\nimg = kzalloc(sizeof(*img), GFP_KERNEL);\r\nif (!img)\r\nreturn ERR_PTR(-ENOMEM);\r\nimg->base.falcon_id = falcon_id;\r\nret = acr->func->ls_func[falcon_id]->load(sb, &img->base);\r\nif (ret) {\r\nkfree(img->base.ucode_data);\r\nkfree(img->base.sig);\r\nkfree(img);\r\nreturn ERR_PTR(ret);\r\n}\r\nif (img->base.sig_size != sizeof(img->lsb_header.signature)) {\r\nnvkm_error(subdev, "invalid signature size for %s falcon!\n",\r\nnvkm_secboot_falcon_name[falcon_id]);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nmemcpy(&img->lsb_header.signature, img->base.sig, img->base.sig_size);\r\nimg->lsb_header.signature.falcon_id = falcon_id;\r\nreturn &img->base;\r\n}\r\nstatic u32\r\nacr_r352_ls_img_fill_headers(struct acr_r352 *acr,\r\nstruct ls_ucode_img_r352 *img, u32 offset)\r\n{\r\nstruct ls_ucode_img *_img = &img->base;\r\nstruct acr_r352_lsf_wpr_header *whdr = &img->wpr_header;\r\nstruct acr_r352_lsf_lsb_header *lhdr = &img->lsb_header;\r\nstruct ls_ucode_img_desc *desc = &_img->ucode_desc;\r\nconst struct acr_r352_ls_func *func =\r\nacr->func->ls_func[_img->falcon_id];\r\nwhdr->falcon_id = _img->falcon_id;\r\nwhdr->bootstrap_owner = acr->base.boot_falcon;\r\nwhdr->status = LSF_IMAGE_STATUS_COPY;\r\nif (acr->lazy_bootstrap & BIT(_img->falcon_id))\r\nwhdr->lazy_bootstrap = 1;\r\noffset = ALIGN(offset, LSF_LSB_HEADER_ALIGN);\r\nwhdr->lsb_offset = offset;\r\noffset += sizeof(*lhdr);\r\noffset = ALIGN(offset, LSF_UCODE_DATA_ALIGN);\r\n_img->ucode_off = lhdr->ucode_off = offset;\r\noffset += _img->ucode_size;\r\nlhdr->bl_code_size = ALIGN(desc->bootloader_size,\r\nLSF_BL_CODE_SIZE_ALIGN);\r\nlhdr->ucode_size = ALIGN(desc->app_resident_data_offset,\r\nLSF_BL_CODE_SIZE_ALIGN) + lhdr->bl_code_size;\r\nlhdr->data_size = ALIGN(desc->app_size, LSF_BL_CODE_SIZE_ALIGN) +\r\nlhdr->bl_code_size - lhdr->ucode_size;\r\nlhdr->bl_imem_off = desc->bootloader_imem_offset;\r\nlhdr->app_code_off = desc->app_start_offset +\r\ndesc->app_resident_code_offset;\r\nlhdr->app_code_size = desc->app_resident_code_size;\r\nlhdr->app_data_off = desc->app_start_offset +\r\ndesc->app_resident_data_offset;\r\nlhdr->app_data_size = desc->app_resident_data_size;\r\nlhdr->flags = func->lhdr_flags;\r\nif (_img->falcon_id == acr->base.boot_falcon)\r\nlhdr->flags |= LSF_FLAG_DMACTL_REQ_CTX;\r\nlhdr->bl_data_size = ALIGN(func->bl_desc_size, LSF_BL_DATA_SIZE_ALIGN);\r\noffset = ALIGN(offset, LSF_BL_DATA_ALIGN);\r\nlhdr->bl_data_off = offset;\r\noffset += lhdr->bl_data_size;\r\nreturn offset;\r\n}\r\nint\r\nacr_r352_ls_fill_headers(struct acr_r352 *acr, struct list_head *imgs)\r\n{\r\nstruct ls_ucode_img_r352 *img;\r\nstruct list_head *l;\r\nu32 count = 0;\r\nu32 offset;\r\nlist_for_each(l, imgs)\r\ncount++;\r\noffset = sizeof(img->wpr_header) * (count + 1);\r\nlist_for_each_entry(img, imgs, base.node) {\r\noffset = acr_r352_ls_img_fill_headers(acr, img, offset);\r\n}\r\nreturn offset;\r\n}\r\nint\r\nacr_r352_ls_write_wpr(struct acr_r352 *acr, struct list_head *imgs,\r\nstruct nvkm_gpuobj *wpr_blob, u64 wpr_addr)\r\n{\r\nstruct ls_ucode_img *_img;\r\nu32 pos = 0;\r\nnvkm_kmap(wpr_blob);\r\nlist_for_each_entry(_img, imgs, node) {\r\nstruct ls_ucode_img_r352 *img = ls_ucode_img_r352(_img);\r\nconst struct acr_r352_ls_func *ls_func =\r\nacr->func->ls_func[_img->falcon_id];\r\nu8 gdesc[ls_func->bl_desc_size];\r\nnvkm_gpuobj_memcpy_to(wpr_blob, pos, &img->wpr_header,\r\nsizeof(img->wpr_header));\r\nnvkm_gpuobj_memcpy_to(wpr_blob, img->wpr_header.lsb_offset,\r\n&img->lsb_header, sizeof(img->lsb_header));\r\nmemset(gdesc, 0, ls_func->bl_desc_size);\r\nls_func->generate_bl_desc(&acr->base, _img, wpr_addr, gdesc);\r\nnvkm_gpuobj_memcpy_to(wpr_blob, img->lsb_header.bl_data_off,\r\ngdesc, ls_func->bl_desc_size);\r\nnvkm_gpuobj_memcpy_to(wpr_blob, img->lsb_header.ucode_off,\r\n_img->ucode_data, _img->ucode_size);\r\npos += sizeof(img->wpr_header);\r\n}\r\nnvkm_wo32(wpr_blob, pos, NVKM_SECBOOT_FALCON_INVALID);\r\nnvkm_done(wpr_blob);\r\nreturn 0;\r\n}\r\nstatic int\r\nacr_r352_prepare_ls_blob(struct acr_r352 *acr, struct nvkm_secboot *sb)\r\n{\r\nconst struct nvkm_subdev *subdev = acr->base.subdev;\r\nstruct list_head imgs;\r\nstruct ls_ucode_img *img, *t;\r\nunsigned long managed_falcons = acr->base.managed_falcons;\r\nu64 wpr_addr = sb->wpr_addr;\r\nu32 wpr_size = sb->wpr_size;\r\nint managed_count = 0;\r\nu32 image_wpr_size, ls_blob_size;\r\nint falcon_id;\r\nint ret;\r\nINIT_LIST_HEAD(&imgs);\r\nfor_each_set_bit(falcon_id, &managed_falcons, NVKM_SECBOOT_FALCON_END) {\r\nstruct ls_ucode_img *img;\r\nimg = acr->func->ls_ucode_img_load(acr, sb, falcon_id);\r\nif (IS_ERR(img)) {\r\nif (acr->base.optional_falcons & BIT(falcon_id)) {\r\nmanaged_falcons &= ~BIT(falcon_id);\r\nnvkm_info(subdev, "skipping %s falcon...\n",\r\nnvkm_secboot_falcon_name[falcon_id]);\r\ncontinue;\r\n}\r\nret = PTR_ERR(img);\r\ngoto cleanup;\r\n}\r\nlist_add_tail(&img->node, &imgs);\r\nmanaged_count++;\r\n}\r\nacr->base.managed_falcons = managed_falcons;\r\nif (acr->func->ls_func[acr->base.boot_falcon] &&\r\n(managed_falcons & BIT(acr->base.boot_falcon))) {\r\nfor_each_set_bit(falcon_id, &managed_falcons,\r\nNVKM_SECBOOT_FALCON_END) {\r\nif (falcon_id == acr->base.boot_falcon)\r\ncontinue;\r\nacr->lazy_bootstrap |= BIT(falcon_id);\r\n}\r\n}\r\nimage_wpr_size = acr->func->ls_fill_headers(acr, &imgs);\r\nimage_wpr_size = ALIGN(image_wpr_size, WPR_ALIGNMENT);\r\nls_blob_size = image_wpr_size;\r\nif (wpr_size == 0 && acr->func->shadow_blob)\r\nls_blob_size *= 2;\r\nret = nvkm_gpuobj_new(subdev->device, ls_blob_size, WPR_ALIGNMENT,\r\nfalse, NULL, &acr->ls_blob);\r\nif (ret)\r\ngoto cleanup;\r\nnvkm_debug(subdev, "%d managed LS falcons, WPR size is %d bytes\n",\r\nmanaged_count, image_wpr_size);\r\nif (wpr_size == 0) {\r\nwpr_addr = acr->ls_blob->addr;\r\nif (acr->func->shadow_blob)\r\nwpr_addr += acr->ls_blob->size / 2;\r\nwpr_size = image_wpr_size;\r\n} else if (image_wpr_size > wpr_size) {\r\nnvkm_error(subdev, "WPR region too small for FW blob!\n");\r\nnvkm_error(subdev, "required: %dB\n", image_wpr_size);\r\nnvkm_error(subdev, "available: %dB\n", wpr_size);\r\nret = -ENOSPC;\r\ngoto cleanup;\r\n}\r\nret = acr->func->ls_write_wpr(acr, &imgs, acr->ls_blob, wpr_addr);\r\nif (ret)\r\nnvkm_gpuobj_del(&acr->ls_blob);\r\ncleanup:\r\nlist_for_each_entry_safe(img, t, &imgs, node) {\r\nkfree(img->ucode_data);\r\nkfree(img->sig);\r\nkfree(img);\r\n}\r\nreturn ret;\r\n}\r\nvoid\r\nacr_r352_fixup_hs_desc(struct acr_r352 *acr, struct nvkm_secboot *sb,\r\nvoid *_desc)\r\n{\r\nstruct hsflcn_acr_desc *desc = _desc;\r\nstruct nvkm_gpuobj *ls_blob = acr->ls_blob;\r\nif (sb->wpr_size == 0) {\r\nu64 wpr_start = ls_blob->addr;\r\nu64 wpr_end = wpr_start + ls_blob->size;\r\ndesc->wpr_region_id = 1;\r\ndesc->regions.no_regions = 2;\r\ndesc->regions.region_props[0].start_addr = wpr_start >> 8;\r\ndesc->regions.region_props[0].end_addr = wpr_end >> 8;\r\ndesc->regions.region_props[0].region_id = 1;\r\ndesc->regions.region_props[0].read_mask = 0xf;\r\ndesc->regions.region_props[0].write_mask = 0xc;\r\ndesc->regions.region_props[0].client_mask = 0x2;\r\n} else {\r\ndesc->ucode_blob_base = ls_blob->addr;\r\ndesc->ucode_blob_size = ls_blob->size;\r\n}\r\n}\r\nstatic void\r\nacr_r352_generate_hs_bl_desc(const struct hsf_load_header *hdr, void *_bl_desc,\r\nu64 offset)\r\n{\r\nstruct acr_r352_flcn_bl_desc *bl_desc = _bl_desc;\r\nu64 addr_code, addr_data;\r\naddr_code = offset >> 8;\r\naddr_data = (offset + hdr->data_dma_base) >> 8;\r\nbl_desc->ctx_dma = FALCON_DMAIDX_VIRT;\r\nbl_desc->code_dma_base = lower_32_bits(addr_code);\r\nbl_desc->non_sec_code_off = hdr->non_sec_code_off;\r\nbl_desc->non_sec_code_size = hdr->non_sec_code_size;\r\nbl_desc->sec_code_off = hsf_load_header_app_off(hdr, 0);\r\nbl_desc->sec_code_size = hsf_load_header_app_size(hdr, 0);\r\nbl_desc->code_entry_point = 0;\r\nbl_desc->data_dma_base = lower_32_bits(addr_data);\r\nbl_desc->data_size = hdr->data_size;\r\n}\r\nstatic int\r\nacr_r352_prepare_hs_blob(struct acr_r352 *acr, struct nvkm_secboot *sb,\r\nconst char *fw, struct nvkm_gpuobj **blob,\r\nstruct hsf_load_header *load_header, bool patch)\r\n{\r\nstruct nvkm_subdev *subdev = &sb->subdev;\r\nvoid *acr_image;\r\nstruct fw_bin_header *hsbin_hdr;\r\nstruct hsf_fw_header *fw_hdr;\r\nstruct hsf_load_header *load_hdr;\r\nvoid *acr_data;\r\nint ret;\r\nacr_image = hs_ucode_load_blob(subdev, sb->boot_falcon, fw);\r\nif (IS_ERR(acr_image))\r\nreturn PTR_ERR(acr_image);\r\nhsbin_hdr = acr_image;\r\nfw_hdr = acr_image + hsbin_hdr->header_offset;\r\nload_hdr = acr_image + fw_hdr->hdr_offset;\r\nacr_data = acr_image + hsbin_hdr->data_offset;\r\nif (patch) {\r\nstruct hsflcn_acr_desc *desc;\r\ndesc = acr_data + load_hdr->data_dma_base;\r\nacr->func->fixup_hs_desc(acr, sb, desc);\r\n}\r\nif (load_hdr->num_apps > ACR_R352_MAX_APPS) {\r\nnvkm_error(subdev, "more apps (%d) than supported (%d)!",\r\nload_hdr->num_apps, ACR_R352_MAX_APPS);\r\nret = -EINVAL;\r\ngoto cleanup;\r\n}\r\nmemcpy(load_header, load_hdr, sizeof(*load_header) +\r\n(sizeof(load_hdr->apps[0]) * 2 * load_hdr->num_apps));\r\nret = nvkm_gpuobj_new(subdev->device, ALIGN(hsbin_hdr->data_size, 256),\r\n0x1000, false, NULL, blob);\r\nif (ret)\r\ngoto cleanup;\r\nnvkm_kmap(*blob);\r\nnvkm_gpuobj_memcpy_to(*blob, 0, acr_data, hsbin_hdr->data_size);\r\nnvkm_done(*blob);\r\ncleanup:\r\nkfree(acr_image);\r\nreturn ret;\r\n}\r\nint\r\nacr_r352_load_blobs(struct acr_r352 *acr, struct nvkm_secboot *sb)\r\n{\r\nstruct nvkm_subdev *subdev = &sb->subdev;\r\nint ret;\r\nif (acr->firmware_ok)\r\nreturn 0;\r\nret = acr_r352_prepare_ls_blob(acr, sb);\r\nif (ret)\r\nreturn ret;\r\nif (!acr->load_blob) {\r\nret = acr_r352_prepare_hs_blob(acr, sb, "acr/ucode_load",\r\n&acr->load_blob,\r\n&acr->load_bl_header, true);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (sb->wpr_size == 0) {\r\nret = acr_r352_prepare_hs_blob(acr, sb, "acr/ucode_unload",\r\n&acr->unload_blob,\r\n&acr->unload_bl_header, false);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (!acr->hsbl_blob) {\r\nacr->hsbl_blob = nvkm_acr_load_firmware(subdev, "acr/bl", 0);\r\nif (IS_ERR(acr->hsbl_blob)) {\r\nret = PTR_ERR(acr->hsbl_blob);\r\nacr->hsbl_blob = NULL;\r\nreturn ret;\r\n}\r\nif (acr->base.boot_falcon != NVKM_SECBOOT_FALCON_PMU) {\r\nacr->hsbl_unload_blob = nvkm_acr_load_firmware(subdev,\r\n"acr/unload_bl", 0);\r\nif (IS_ERR(acr->hsbl_unload_blob)) {\r\nret = PTR_ERR(acr->hsbl_unload_blob);\r\nacr->hsbl_unload_blob = NULL;\r\nreturn ret;\r\n}\r\n} else {\r\nacr->hsbl_unload_blob = acr->hsbl_blob;\r\n}\r\n}\r\nacr->firmware_ok = true;\r\nnvkm_debug(&sb->subdev, "LS blob successfully created\n");\r\nreturn 0;\r\n}\r\nstatic int\r\nacr_r352_load(struct nvkm_acr *_acr, struct nvkm_falcon *falcon,\r\nstruct nvkm_gpuobj *blob, u64 offset)\r\n{\r\nstruct acr_r352 *acr = acr_r352(_acr);\r\nconst u32 bl_desc_size = acr->func->hs_bl_desc_size;\r\nconst struct hsf_load_header *load_hdr;\r\nstruct fw_bin_header *bl_hdr;\r\nstruct fw_bl_desc *hsbl_desc;\r\nvoid *bl, *blob_data, *hsbl_code, *hsbl_data;\r\nu32 code_size;\r\nu8 bl_desc[bl_desc_size];\r\nif (blob == acr->load_blob) {\r\nload_hdr = &acr->load_bl_header;\r\nbl = acr->hsbl_blob;\r\n} else if (blob == acr->unload_blob) {\r\nload_hdr = &acr->unload_bl_header;\r\nbl = acr->hsbl_unload_blob;\r\n} else {\r\nnvkm_error(_acr->subdev, "invalid secure boot blob!\n");\r\nreturn -EINVAL;\r\n}\r\nbl_hdr = bl;\r\nhsbl_desc = bl + bl_hdr->header_offset;\r\nblob_data = bl + bl_hdr->data_offset;\r\nhsbl_code = blob_data + hsbl_desc->code_off;\r\nhsbl_data = blob_data + hsbl_desc->data_off;\r\ncode_size = ALIGN(hsbl_desc->code_size, 256);\r\nnvkm_falcon_load_dmem(falcon, hsbl_data, 0x0, hsbl_desc->data_size, 0);\r\nnvkm_falcon_load_imem(falcon, hsbl_code, falcon->code.limit - code_size,\r\ncode_size, hsbl_desc->start_tag, 0, false);\r\nmemset(bl_desc, 0, bl_desc_size);\r\nacr->func->generate_hs_bl_desc(load_hdr, bl_desc, offset);\r\nnvkm_falcon_load_dmem(falcon, bl_desc, hsbl_desc->dmem_load_off,\r\nbl_desc_size, 0);\r\nreturn hsbl_desc->start_tag << 8;\r\n}\r\nstatic int\r\nacr_r352_shutdown(struct acr_r352 *acr, struct nvkm_secboot *sb)\r\n{\r\nstruct nvkm_subdev *subdev = &sb->subdev;\r\nint i;\r\nif (acr->unload_blob && sb->wpr_set) {\r\nint ret;\r\nnvkm_debug(subdev, "running HS unload blob\n");\r\nret = sb->func->run_blob(sb, acr->unload_blob, sb->halt_falcon);\r\nif (ret < 0)\r\nreturn ret;\r\nif (ret && ret != 0x1d) {\r\nnvkm_error(subdev, "HS unload failed, ret 0x%08x", ret);\r\nreturn -EINVAL;\r\n}\r\nnvkm_debug(subdev, "HS unload blob completed\n");\r\n}\r\nfor (i = 0; i < NVKM_SECBOOT_FALCON_END; i++)\r\nacr->falcon_state[i] = NON_SECURE;\r\nsb->wpr_set = false;\r\nreturn 0;\r\n}\r\nstatic bool\r\nacr_r352_wpr_is_set(const struct acr_r352 *acr, const struct nvkm_secboot *sb)\r\n{\r\nconst struct nvkm_subdev *subdev = &sb->subdev;\r\nconst struct nvkm_device *device = subdev->device;\r\nu64 wpr_lo, wpr_hi;\r\nu64 wpr_range_lo, wpr_range_hi;\r\nnvkm_wr32(device, 0x100cd4, 0x2);\r\nwpr_lo = (nvkm_rd32(device, 0x100cd4) & ~0xff);\r\nwpr_lo <<= 8;\r\nnvkm_wr32(device, 0x100cd4, 0x3);\r\nwpr_hi = (nvkm_rd32(device, 0x100cd4) & ~0xff);\r\nwpr_hi <<= 8;\r\nif (sb->wpr_size != 0) {\r\nwpr_range_lo = sb->wpr_addr;\r\nwpr_range_hi = wpr_range_lo + sb->wpr_size;\r\n} else {\r\nwpr_range_lo = acr->ls_blob->addr;\r\nwpr_range_hi = wpr_range_lo + acr->ls_blob->size;\r\n}\r\nreturn (wpr_lo >= wpr_range_lo && wpr_lo < wpr_range_hi &&\r\nwpr_hi > wpr_range_lo && wpr_hi <= wpr_range_hi);\r\n}\r\nstatic int\r\nacr_r352_bootstrap(struct acr_r352 *acr, struct nvkm_secboot *sb)\r\n{\r\nconst struct nvkm_subdev *subdev = &sb->subdev;\r\nunsigned long managed_falcons = acr->base.managed_falcons;\r\nint falcon_id;\r\nint ret;\r\nif (sb->wpr_set)\r\nreturn 0;\r\nret = acr_r352_load_blobs(acr, sb);\r\nif (ret)\r\nreturn ret;\r\nnvkm_debug(subdev, "running HS load blob\n");\r\nret = sb->func->run_blob(sb, acr->load_blob, sb->boot_falcon);\r\nnvkm_falcon_clear_interrupt(sb->boot_falcon, 0x10);\r\nsb->wpr_set = acr_r352_wpr_is_set(acr, sb);\r\nif (ret < 0) {\r\nreturn ret;\r\n} else if (ret > 0) {\r\nnvkm_error(subdev, "HS load failed, ret 0x%08x", ret);\r\nreturn -EINVAL;\r\n}\r\nnvkm_debug(subdev, "HS load blob completed\n");\r\nif (!sb->wpr_set) {\r\nnvkm_error(subdev, "ACR blob completed but WPR not set!\n");\r\nreturn -EINVAL;\r\n}\r\nfor_each_set_bit(falcon_id, &managed_falcons, NVKM_SECBOOT_FALCON_END) {\r\nconst struct acr_r352_ls_func *func =\r\nacr->func->ls_func[falcon_id];\r\nif (func->post_run) {\r\nret = func->post_run(&acr->base, sb);\r\nif (ret)\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nacr_r352_reset_nopmu(struct acr_r352 *acr, struct nvkm_secboot *sb,\r\nunsigned long falcon_mask)\r\n{\r\nint falcon;\r\nint ret;\r\nif (!(falcon_mask & BIT(NVKM_SECBOOT_FALCON_FECS)))\r\ngoto end;\r\nret = acr_r352_shutdown(acr, sb);\r\nif (ret)\r\nreturn ret;\r\nret = acr_r352_bootstrap(acr, sb);\r\nif (ret)\r\nreturn ret;\r\nend:\r\nfor_each_set_bit(falcon, &falcon_mask, NVKM_SECBOOT_FALCON_END) {\r\nacr->falcon_state[falcon] = RESET;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nacr_r352_reset(struct nvkm_acr *_acr, struct nvkm_secboot *sb,\r\nunsigned long falcon_mask)\r\n{\r\nstruct acr_r352 *acr = acr_r352(_acr);\r\nstruct nvkm_msgqueue *queue;\r\nint falcon;\r\nbool wpr_already_set = sb->wpr_set;\r\nint ret;\r\nret = acr_r352_bootstrap(acr, sb);\r\nif (ret)\r\nreturn ret;\r\nif (!nvkm_secboot_is_managed(sb, _acr->boot_falcon)) {\r\nif (wpr_already_set)\r\nreturn acr_r352_reset_nopmu(acr, sb, falcon_mask);\r\nelse\r\nreturn ret;\r\n}\r\nswitch (_acr->boot_falcon) {\r\ncase NVKM_SECBOOT_FALCON_PMU:\r\nqueue = sb->subdev.device->pmu->queue;\r\nbreak;\r\ncase NVKM_SECBOOT_FALCON_SEC2:\r\nqueue = sb->subdev.device->sec2->queue;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nfor_each_set_bit(falcon, &falcon_mask, NVKM_SECBOOT_FALCON_END)\r\nnvkm_debug(&sb->subdev, "resetting %s falcon\n",\r\nnvkm_secboot_falcon_name[falcon]);\r\nret = nvkm_msgqueue_acr_boot_falcons(queue, falcon_mask);\r\nif (ret) {\r\nnvkm_error(&sb->subdev, "error during falcon reset: %d\n", ret);\r\nreturn ret;\r\n}\r\nnvkm_debug(&sb->subdev, "falcon reset done\n");\r\nreturn 0;\r\n}\r\nstatic int\r\nacr_r352_fini(struct nvkm_acr *_acr, struct nvkm_secboot *sb, bool suspend)\r\n{\r\nstruct acr_r352 *acr = acr_r352(_acr);\r\nreturn acr_r352_shutdown(acr, sb);\r\n}\r\nstatic void\r\nacr_r352_dtor(struct nvkm_acr *_acr)\r\n{\r\nstruct acr_r352 *acr = acr_r352(_acr);\r\nnvkm_gpuobj_del(&acr->unload_blob);\r\nif (_acr->boot_falcon != NVKM_SECBOOT_FALCON_PMU)\r\nkfree(acr->hsbl_unload_blob);\r\nkfree(acr->hsbl_blob);\r\nnvkm_gpuobj_del(&acr->load_blob);\r\nnvkm_gpuobj_del(&acr->ls_blob);\r\nkfree(acr);\r\n}\r\nstatic void\r\nacr_r352_generate_pmu_bl_desc(const struct nvkm_acr *acr,\r\nconst struct ls_ucode_img *img, u64 wpr_addr,\r\nvoid *_desc)\r\n{\r\nconst struct ls_ucode_img_desc *pdesc = &img->ucode_desc;\r\nconst struct nvkm_pmu *pmu = acr->subdev->device->pmu;\r\nstruct acr_r352_pmu_bl_desc *desc = _desc;\r\nu64 base;\r\nu64 addr_code;\r\nu64 addr_data;\r\nu32 addr_args;\r\nbase = wpr_addr + img->ucode_off + pdesc->app_start_offset;\r\naddr_code = (base + pdesc->app_resident_code_offset) >> 8;\r\naddr_data = (base + pdesc->app_resident_data_offset) >> 8;\r\naddr_args = pmu->falcon->data.limit;\r\naddr_args -= NVKM_MSGQUEUE_CMDLINE_SIZE;\r\ndesc->dma_idx = FALCON_DMAIDX_UCODE;\r\ndesc->code_dma_base = lower_32_bits(addr_code);\r\ndesc->code_dma_base1 = upper_32_bits(addr_code);\r\ndesc->code_size_total = pdesc->app_size;\r\ndesc->code_size_to_load = pdesc->app_resident_code_size;\r\ndesc->code_entry_point = pdesc->app_imem_entry;\r\ndesc->data_dma_base = lower_32_bits(addr_data);\r\ndesc->data_dma_base1 = upper_32_bits(addr_data);\r\ndesc->data_size = pdesc->app_resident_data_size;\r\ndesc->overlay_dma_base = lower_32_bits(addr_code);\r\ndesc->overlay_dma_base1 = upper_32_bits(addr_code);\r\ndesc->argc = 1;\r\ndesc->argv = addr_args;\r\n}\r\nstruct nvkm_acr *\r\nacr_r352_new_(const struct acr_r352_func *func,\r\nenum nvkm_secboot_falcon boot_falcon,\r\nunsigned long managed_falcons)\r\n{\r\nstruct acr_r352 *acr;\r\nint i;\r\nfor_each_set_bit(i, &managed_falcons, NVKM_SECBOOT_FALCON_END) {\r\nif (!func->ls_func[i])\r\nreturn ERR_PTR(-ENOTSUPP);\r\n}\r\nacr = kzalloc(sizeof(*acr), GFP_KERNEL);\r\nif (!acr)\r\nreturn ERR_PTR(-ENOMEM);\r\nacr->base.boot_falcon = boot_falcon;\r\nacr->base.managed_falcons = managed_falcons;\r\nacr->base.func = &acr_r352_base_func;\r\nacr->func = func;\r\nreturn &acr->base;\r\n}\r\nstruct nvkm_acr *\r\nacr_r352_new(unsigned long managed_falcons)\r\n{\r\nreturn acr_r352_new_(&acr_r352_func, NVKM_SECBOOT_FALCON_PMU,\r\nmanaged_falcons);\r\n}
