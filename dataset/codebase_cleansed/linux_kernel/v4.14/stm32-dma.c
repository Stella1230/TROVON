static struct stm32_dma_device *stm32_dma_get_dev(struct stm32_dma_chan *chan)\r\n{\r\nreturn container_of(chan->vchan.chan.device, struct stm32_dma_device,\r\nddev);\r\n}\r\nstatic struct stm32_dma_chan *to_stm32_dma_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct stm32_dma_chan, vchan.chan);\r\n}\r\nstatic struct stm32_dma_desc *to_stm32_dma_desc(struct virt_dma_desc *vdesc)\r\n{\r\nreturn container_of(vdesc, struct stm32_dma_desc, vdesc);\r\n}\r\nstatic struct device *chan2dev(struct stm32_dma_chan *chan)\r\n{\r\nreturn &chan->vchan.chan.dev->device;\r\n}\r\nstatic u32 stm32_dma_read(struct stm32_dma_device *dmadev, u32 reg)\r\n{\r\nreturn readl_relaxed(dmadev->base + reg);\r\n}\r\nstatic void stm32_dma_write(struct stm32_dma_device *dmadev, u32 reg, u32 val)\r\n{\r\nwritel_relaxed(val, dmadev->base + reg);\r\n}\r\nstatic struct stm32_dma_desc *stm32_dma_alloc_desc(u32 num_sgs)\r\n{\r\nreturn kzalloc(sizeof(struct stm32_dma_desc) +\r\nsizeof(struct stm32_dma_sg_req) * num_sgs, GFP_NOWAIT);\r\n}\r\nstatic int stm32_dma_get_width(struct stm32_dma_chan *chan,\r\nenum dma_slave_buswidth width)\r\n{\r\nswitch (width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nreturn STM32_DMA_BYTE;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nreturn STM32_DMA_HALF_WORD;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nreturn STM32_DMA_WORD;\r\ndefault:\r\ndev_err(chan2dev(chan), "Dma bus width not supported\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic int stm32_dma_get_burst(struct stm32_dma_chan *chan, u32 maxburst)\r\n{\r\nswitch (maxburst) {\r\ncase 0:\r\ncase 1:\r\nreturn STM32_DMA_BURST_SINGLE;\r\ncase 4:\r\nreturn STM32_DMA_BURST_INCR4;\r\ncase 8:\r\nreturn STM32_DMA_BURST_INCR8;\r\ncase 16:\r\nreturn STM32_DMA_BURST_INCR16;\r\ndefault:\r\ndev_err(chan2dev(chan), "Dma burst size not supported\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic void stm32_dma_set_fifo_config(struct stm32_dma_chan *chan,\r\nu32 src_maxburst, u32 dst_maxburst)\r\n{\r\nchan->chan_reg.dma_sfcr &= ~STM32_DMA_SFCR_MASK;\r\nchan->chan_reg.dma_scr &= ~STM32_DMA_SCR_DMEIE;\r\nif ((!src_maxburst) && (!dst_maxburst)) {\r\nchan->chan_reg.dma_scr |= STM32_DMA_SCR_DMEIE;\r\n} else {\r\nchan->chan_reg.dma_sfcr |= STM32_DMA_SFCR_MASK;\r\n}\r\n}\r\nstatic int stm32_dma_slave_config(struct dma_chan *c,\r\nstruct dma_slave_config *config)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nmemcpy(&chan->dma_sconfig, config, sizeof(*config));\r\nchan->config_init = true;\r\nreturn 0;\r\n}\r\nstatic u32 stm32_dma_irq_status(struct stm32_dma_chan *chan)\r\n{\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nu32 flags, dma_isr;\r\nif (chan->id & 4)\r\ndma_isr = stm32_dma_read(dmadev, STM32_DMA_HISR);\r\nelse\r\ndma_isr = stm32_dma_read(dmadev, STM32_DMA_LISR);\r\nflags = dma_isr >> (((chan->id & 2) << 3) | ((chan->id & 1) * 6));\r\nreturn flags;\r\n}\r\nstatic void stm32_dma_irq_clear(struct stm32_dma_chan *chan, u32 flags)\r\n{\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nu32 dma_ifcr;\r\ndma_ifcr = flags << (((chan->id & 2) << 3) | ((chan->id & 1) * 6));\r\nif (chan->id & 4)\r\nstm32_dma_write(dmadev, STM32_DMA_HIFCR, dma_ifcr);\r\nelse\r\nstm32_dma_write(dmadev, STM32_DMA_LIFCR, dma_ifcr);\r\n}\r\nstatic int stm32_dma_disable_chan(struct stm32_dma_chan *chan)\r\n{\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nunsigned long timeout = jiffies + msecs_to_jiffies(5000);\r\nu32 dma_scr, id;\r\nid = chan->id;\r\ndma_scr = stm32_dma_read(dmadev, STM32_DMA_SCR(id));\r\nif (dma_scr & STM32_DMA_SCR_EN) {\r\ndma_scr &= ~STM32_DMA_SCR_EN;\r\nstm32_dma_write(dmadev, STM32_DMA_SCR(id), dma_scr);\r\ndo {\r\ndma_scr = stm32_dma_read(dmadev, STM32_DMA_SCR(id));\r\ndma_scr &= STM32_DMA_SCR_EN;\r\nif (!dma_scr)\r\nbreak;\r\nif (time_after_eq(jiffies, timeout)) {\r\ndev_err(chan2dev(chan), "%s: timeout!\n",\r\n__func__);\r\nreturn -EBUSY;\r\n}\r\ncond_resched();\r\n} while (1);\r\n}\r\nreturn 0;\r\n}\r\nstatic void stm32_dma_stop(struct stm32_dma_chan *chan)\r\n{\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nu32 dma_scr, dma_sfcr, status;\r\nint ret;\r\ndma_scr = stm32_dma_read(dmadev, STM32_DMA_SCR(chan->id));\r\ndma_scr &= ~STM32_DMA_SCR_IRQ_MASK;\r\nstm32_dma_write(dmadev, STM32_DMA_SCR(chan->id), dma_scr);\r\ndma_sfcr = stm32_dma_read(dmadev, STM32_DMA_SFCR(chan->id));\r\ndma_sfcr &= ~STM32_DMA_SFCR_FEIE;\r\nstm32_dma_write(dmadev, STM32_DMA_SFCR(chan->id), dma_sfcr);\r\nret = stm32_dma_disable_chan(chan);\r\nif (ret < 0)\r\nreturn;\r\nstatus = stm32_dma_irq_status(chan);\r\nif (status) {\r\ndev_dbg(chan2dev(chan), "%s(): clearing interrupt: 0x%08x\n",\r\n__func__, status);\r\nstm32_dma_irq_clear(chan, status);\r\n}\r\nchan->busy = false;\r\n}\r\nstatic int stm32_dma_terminate_all(struct dma_chan *c)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&chan->vchan.lock, flags);\r\nif (chan->busy) {\r\nstm32_dma_stop(chan);\r\nchan->desc = NULL;\r\n}\r\nvchan_get_all_descriptors(&chan->vchan, &head);\r\nspin_unlock_irqrestore(&chan->vchan.lock, flags);\r\nvchan_dma_desc_free_list(&chan->vchan, &head);\r\nreturn 0;\r\n}\r\nstatic void stm32_dma_synchronize(struct dma_chan *c)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nvchan_synchronize(&chan->vchan);\r\n}\r\nstatic void stm32_dma_dump_reg(struct stm32_dma_chan *chan)\r\n{\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nu32 scr = stm32_dma_read(dmadev, STM32_DMA_SCR(chan->id));\r\nu32 ndtr = stm32_dma_read(dmadev, STM32_DMA_SNDTR(chan->id));\r\nu32 spar = stm32_dma_read(dmadev, STM32_DMA_SPAR(chan->id));\r\nu32 sm0ar = stm32_dma_read(dmadev, STM32_DMA_SM0AR(chan->id));\r\nu32 sm1ar = stm32_dma_read(dmadev, STM32_DMA_SM1AR(chan->id));\r\nu32 sfcr = stm32_dma_read(dmadev, STM32_DMA_SFCR(chan->id));\r\ndev_dbg(chan2dev(chan), "SCR: 0x%08x\n", scr);\r\ndev_dbg(chan2dev(chan), "NDTR: 0x%08x\n", ndtr);\r\ndev_dbg(chan2dev(chan), "SPAR: 0x%08x\n", spar);\r\ndev_dbg(chan2dev(chan), "SM0AR: 0x%08x\n", sm0ar);\r\ndev_dbg(chan2dev(chan), "SM1AR: 0x%08x\n", sm1ar);\r\ndev_dbg(chan2dev(chan), "SFCR: 0x%08x\n", sfcr);\r\n}\r\nstatic void stm32_dma_start_transfer(struct stm32_dma_chan *chan)\r\n{\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nstruct virt_dma_desc *vdesc;\r\nstruct stm32_dma_sg_req *sg_req;\r\nstruct stm32_dma_chan_reg *reg;\r\nu32 status;\r\nint ret;\r\nret = stm32_dma_disable_chan(chan);\r\nif (ret < 0)\r\nreturn;\r\nif (!chan->desc) {\r\nvdesc = vchan_next_desc(&chan->vchan);\r\nif (!vdesc)\r\nreturn;\r\nchan->desc = to_stm32_dma_desc(vdesc);\r\nchan->next_sg = 0;\r\n}\r\nif (chan->next_sg == chan->desc->num_sgs)\r\nchan->next_sg = 0;\r\nsg_req = &chan->desc->sg_req[chan->next_sg];\r\nreg = &sg_req->chan_reg;\r\nstm32_dma_write(dmadev, STM32_DMA_SCR(chan->id), reg->dma_scr);\r\nstm32_dma_write(dmadev, STM32_DMA_SPAR(chan->id), reg->dma_spar);\r\nstm32_dma_write(dmadev, STM32_DMA_SM0AR(chan->id), reg->dma_sm0ar);\r\nstm32_dma_write(dmadev, STM32_DMA_SFCR(chan->id), reg->dma_sfcr);\r\nstm32_dma_write(dmadev, STM32_DMA_SM1AR(chan->id), reg->dma_sm1ar);\r\nstm32_dma_write(dmadev, STM32_DMA_SNDTR(chan->id), reg->dma_sndtr);\r\nchan->next_sg++;\r\nstatus = stm32_dma_irq_status(chan);\r\nif (status)\r\nstm32_dma_irq_clear(chan, status);\r\nstm32_dma_dump_reg(chan);\r\nreg->dma_scr |= STM32_DMA_SCR_EN;\r\nstm32_dma_write(dmadev, STM32_DMA_SCR(chan->id), reg->dma_scr);\r\nchan->busy = true;\r\ndev_dbg(chan2dev(chan), "vchan %p: started\n", &chan->vchan);\r\n}\r\nstatic void stm32_dma_configure_next_sg(struct stm32_dma_chan *chan)\r\n{\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nstruct stm32_dma_sg_req *sg_req;\r\nu32 dma_scr, dma_sm0ar, dma_sm1ar, id;\r\nid = chan->id;\r\ndma_scr = stm32_dma_read(dmadev, STM32_DMA_SCR(id));\r\nif (dma_scr & STM32_DMA_SCR_DBM) {\r\nif (chan->next_sg == chan->desc->num_sgs)\r\nchan->next_sg = 0;\r\nsg_req = &chan->desc->sg_req[chan->next_sg];\r\nif (dma_scr & STM32_DMA_SCR_CT) {\r\ndma_sm0ar = sg_req->chan_reg.dma_sm0ar;\r\nstm32_dma_write(dmadev, STM32_DMA_SM0AR(id), dma_sm0ar);\r\ndev_dbg(chan2dev(chan), "CT=1 <=> SM0AR: 0x%08x\n",\r\nstm32_dma_read(dmadev, STM32_DMA_SM0AR(id)));\r\n} else {\r\ndma_sm1ar = sg_req->chan_reg.dma_sm1ar;\r\nstm32_dma_write(dmadev, STM32_DMA_SM1AR(id), dma_sm1ar);\r\ndev_dbg(chan2dev(chan), "CT=0 <=> SM1AR: 0x%08x\n",\r\nstm32_dma_read(dmadev, STM32_DMA_SM1AR(id)));\r\n}\r\n}\r\n}\r\nstatic void stm32_dma_handle_chan_done(struct stm32_dma_chan *chan)\r\n{\r\nif (chan->desc) {\r\nif (chan->desc->cyclic) {\r\nvchan_cyclic_callback(&chan->desc->vdesc);\r\nchan->next_sg++;\r\nstm32_dma_configure_next_sg(chan);\r\n} else {\r\nchan->busy = false;\r\nif (chan->next_sg == chan->desc->num_sgs) {\r\nlist_del(&chan->desc->vdesc.node);\r\nvchan_cookie_complete(&chan->desc->vdesc);\r\nchan->desc = NULL;\r\n}\r\nstm32_dma_start_transfer(chan);\r\n}\r\n}\r\n}\r\nstatic irqreturn_t stm32_dma_chan_irq(int irq, void *devid)\r\n{\r\nstruct stm32_dma_chan *chan = devid;\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nu32 status, scr;\r\nspin_lock(&chan->vchan.lock);\r\nstatus = stm32_dma_irq_status(chan);\r\nscr = stm32_dma_read(dmadev, STM32_DMA_SCR(chan->id));\r\nif ((status & STM32_DMA_TCI) && (scr & STM32_DMA_SCR_TCIE)) {\r\nstm32_dma_irq_clear(chan, STM32_DMA_TCI);\r\nstm32_dma_handle_chan_done(chan);\r\n} else {\r\nstm32_dma_irq_clear(chan, status);\r\ndev_err(chan2dev(chan), "DMA error: status=0x%08x\n", status);\r\n}\r\nspin_unlock(&chan->vchan.lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void stm32_dma_issue_pending(struct dma_chan *c)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->vchan.lock, flags);\r\nif (vchan_issue_pending(&chan->vchan) && !chan->desc && !chan->busy) {\r\ndev_dbg(chan2dev(chan), "vchan %p: issued\n", &chan->vchan);\r\nstm32_dma_start_transfer(chan);\r\nif (chan->desc->cyclic)\r\nstm32_dma_configure_next_sg(chan);\r\n}\r\nspin_unlock_irqrestore(&chan->vchan.lock, flags);\r\n}\r\nstatic int stm32_dma_set_xfer_param(struct stm32_dma_chan *chan,\r\nenum dma_transfer_direction direction,\r\nenum dma_slave_buswidth *buswidth)\r\n{\r\nenum dma_slave_buswidth src_addr_width, dst_addr_width;\r\nint src_bus_width, dst_bus_width;\r\nint src_burst_size, dst_burst_size;\r\nu32 src_maxburst, dst_maxburst;\r\nu32 dma_scr = 0;\r\nsrc_addr_width = chan->dma_sconfig.src_addr_width;\r\ndst_addr_width = chan->dma_sconfig.dst_addr_width;\r\nsrc_maxburst = chan->dma_sconfig.src_maxburst;\r\ndst_maxburst = chan->dma_sconfig.dst_maxburst;\r\nswitch (direction) {\r\ncase DMA_MEM_TO_DEV:\r\ndst_bus_width = stm32_dma_get_width(chan, dst_addr_width);\r\nif (dst_bus_width < 0)\r\nreturn dst_bus_width;\r\ndst_burst_size = stm32_dma_get_burst(chan, dst_maxburst);\r\nif (dst_burst_size < 0)\r\nreturn dst_burst_size;\r\nif (!src_addr_width)\r\nsrc_addr_width = dst_addr_width;\r\nsrc_bus_width = stm32_dma_get_width(chan, src_addr_width);\r\nif (src_bus_width < 0)\r\nreturn src_bus_width;\r\nsrc_burst_size = stm32_dma_get_burst(chan, src_maxburst);\r\nif (src_burst_size < 0)\r\nreturn src_burst_size;\r\ndma_scr = STM32_DMA_SCR_DIR(STM32_DMA_MEM_TO_DEV) |\r\nSTM32_DMA_SCR_PSIZE(dst_bus_width) |\r\nSTM32_DMA_SCR_MSIZE(src_bus_width) |\r\nSTM32_DMA_SCR_PBURST(dst_burst_size) |\r\nSTM32_DMA_SCR_MBURST(src_burst_size);\r\nchan->chan_reg.dma_spar = chan->dma_sconfig.dst_addr;\r\n*buswidth = dst_addr_width;\r\nbreak;\r\ncase DMA_DEV_TO_MEM:\r\nsrc_bus_width = stm32_dma_get_width(chan, src_addr_width);\r\nif (src_bus_width < 0)\r\nreturn src_bus_width;\r\nsrc_burst_size = stm32_dma_get_burst(chan, src_maxburst);\r\nif (src_burst_size < 0)\r\nreturn src_burst_size;\r\nif (!dst_addr_width)\r\ndst_addr_width = src_addr_width;\r\ndst_bus_width = stm32_dma_get_width(chan, dst_addr_width);\r\nif (dst_bus_width < 0)\r\nreturn dst_bus_width;\r\ndst_burst_size = stm32_dma_get_burst(chan, dst_maxburst);\r\nif (dst_burst_size < 0)\r\nreturn dst_burst_size;\r\ndma_scr = STM32_DMA_SCR_DIR(STM32_DMA_DEV_TO_MEM) |\r\nSTM32_DMA_SCR_PSIZE(src_bus_width) |\r\nSTM32_DMA_SCR_MSIZE(dst_bus_width) |\r\nSTM32_DMA_SCR_PBURST(src_burst_size) |\r\nSTM32_DMA_SCR_MBURST(dst_burst_size);\r\nchan->chan_reg.dma_spar = chan->dma_sconfig.src_addr;\r\n*buswidth = chan->dma_sconfig.src_addr_width;\r\nbreak;\r\ndefault:\r\ndev_err(chan2dev(chan), "Dma direction is not supported\n");\r\nreturn -EINVAL;\r\n}\r\nstm32_dma_set_fifo_config(chan, src_maxburst, dst_maxburst);\r\nchan->chan_reg.dma_scr &= ~(STM32_DMA_SCR_DIR_MASK |\r\nSTM32_DMA_SCR_PSIZE_MASK | STM32_DMA_SCR_MSIZE_MASK |\r\nSTM32_DMA_SCR_PBURST_MASK | STM32_DMA_SCR_MBURST_MASK);\r\nchan->chan_reg.dma_scr |= dma_scr;\r\nreturn 0;\r\n}\r\nstatic void stm32_dma_clear_reg(struct stm32_dma_chan_reg *regs)\r\n{\r\nmemset(regs, 0, sizeof(struct stm32_dma_chan_reg));\r\n}\r\nstatic struct dma_async_tx_descriptor *stm32_dma_prep_slave_sg(\r\nstruct dma_chan *c, struct scatterlist *sgl,\r\nu32 sg_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nstruct stm32_dma_desc *desc;\r\nstruct scatterlist *sg;\r\nenum dma_slave_buswidth buswidth;\r\nu32 nb_data_items;\r\nint i, ret;\r\nif (!chan->config_init) {\r\ndev_err(chan2dev(chan), "dma channel is not configured\n");\r\nreturn NULL;\r\n}\r\nif (sg_len < 1) {\r\ndev_err(chan2dev(chan), "Invalid segment length %d\n", sg_len);\r\nreturn NULL;\r\n}\r\ndesc = stm32_dma_alloc_desc(sg_len);\r\nif (!desc)\r\nreturn NULL;\r\nret = stm32_dma_set_xfer_param(chan, direction, &buswidth);\r\nif (ret < 0)\r\ngoto err;\r\nif (chan->dma_sconfig.device_fc)\r\nchan->chan_reg.dma_scr |= STM32_DMA_SCR_PFCTRL;\r\nelse\r\nchan->chan_reg.dma_scr &= ~STM32_DMA_SCR_PFCTRL;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\ndesc->sg_req[i].len = sg_dma_len(sg);\r\nnb_data_items = desc->sg_req[i].len / buswidth;\r\nif (nb_data_items > STM32_DMA_MAX_DATA_ITEMS) {\r\ndev_err(chan2dev(chan), "nb items not supported\n");\r\ngoto err;\r\n}\r\nstm32_dma_clear_reg(&desc->sg_req[i].chan_reg);\r\ndesc->sg_req[i].chan_reg.dma_scr = chan->chan_reg.dma_scr;\r\ndesc->sg_req[i].chan_reg.dma_sfcr = chan->chan_reg.dma_sfcr;\r\ndesc->sg_req[i].chan_reg.dma_spar = chan->chan_reg.dma_spar;\r\ndesc->sg_req[i].chan_reg.dma_sm0ar = sg_dma_address(sg);\r\ndesc->sg_req[i].chan_reg.dma_sm1ar = sg_dma_address(sg);\r\ndesc->sg_req[i].chan_reg.dma_sndtr = nb_data_items;\r\n}\r\ndesc->num_sgs = sg_len;\r\ndesc->cyclic = false;\r\nreturn vchan_tx_prep(&chan->vchan, &desc->vdesc, flags);\r\nerr:\r\nkfree(desc);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *stm32_dma_prep_dma_cyclic(\r\nstruct dma_chan *c, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nstruct stm32_dma_desc *desc;\r\nenum dma_slave_buswidth buswidth;\r\nu32 num_periods, nb_data_items;\r\nint i, ret;\r\nif (!buf_len || !period_len) {\r\ndev_err(chan2dev(chan), "Invalid buffer/period len\n");\r\nreturn NULL;\r\n}\r\nif (!chan->config_init) {\r\ndev_err(chan2dev(chan), "dma channel is not configured\n");\r\nreturn NULL;\r\n}\r\nif (buf_len % period_len) {\r\ndev_err(chan2dev(chan), "buf_len not multiple of period_len\n");\r\nreturn NULL;\r\n}\r\nif (chan->busy) {\r\ndev_err(chan2dev(chan), "Request not allowed when dma busy\n");\r\nreturn NULL;\r\n}\r\nret = stm32_dma_set_xfer_param(chan, direction, &buswidth);\r\nif (ret < 0)\r\nreturn NULL;\r\nnb_data_items = period_len / buswidth;\r\nif (nb_data_items > STM32_DMA_MAX_DATA_ITEMS) {\r\ndev_err(chan2dev(chan), "number of items not supported\n");\r\nreturn NULL;\r\n}\r\nif (buf_len == period_len)\r\nchan->chan_reg.dma_scr |= STM32_DMA_SCR_CIRC;\r\nelse\r\nchan->chan_reg.dma_scr |= STM32_DMA_SCR_DBM;\r\nchan->chan_reg.dma_scr &= ~STM32_DMA_SCR_PFCTRL;\r\nnum_periods = buf_len / period_len;\r\ndesc = stm32_dma_alloc_desc(num_periods);\r\nif (!desc)\r\nreturn NULL;\r\nfor (i = 0; i < num_periods; i++) {\r\ndesc->sg_req[i].len = period_len;\r\nstm32_dma_clear_reg(&desc->sg_req[i].chan_reg);\r\ndesc->sg_req[i].chan_reg.dma_scr = chan->chan_reg.dma_scr;\r\ndesc->sg_req[i].chan_reg.dma_sfcr = chan->chan_reg.dma_sfcr;\r\ndesc->sg_req[i].chan_reg.dma_spar = chan->chan_reg.dma_spar;\r\ndesc->sg_req[i].chan_reg.dma_sm0ar = buf_addr;\r\ndesc->sg_req[i].chan_reg.dma_sm1ar = buf_addr;\r\ndesc->sg_req[i].chan_reg.dma_sndtr = nb_data_items;\r\nbuf_addr += period_len;\r\n}\r\ndesc->num_sgs = num_periods;\r\ndesc->cyclic = true;\r\nreturn vchan_tx_prep(&chan->vchan, &desc->vdesc, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *stm32_dma_prep_dma_memcpy(\r\nstruct dma_chan *c, dma_addr_t dest,\r\ndma_addr_t src, size_t len, unsigned long flags)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nu32 num_sgs;\r\nstruct stm32_dma_desc *desc;\r\nsize_t xfer_count, offset;\r\nint i;\r\nnum_sgs = DIV_ROUND_UP(len, STM32_DMA_MAX_DATA_ITEMS);\r\ndesc = stm32_dma_alloc_desc(num_sgs);\r\nif (!desc)\r\nreturn NULL;\r\nfor (offset = 0, i = 0; offset < len; offset += xfer_count, i++) {\r\nxfer_count = min_t(size_t, len - offset,\r\nSTM32_DMA_MAX_DATA_ITEMS);\r\ndesc->sg_req[i].len = xfer_count;\r\nstm32_dma_clear_reg(&desc->sg_req[i].chan_reg);\r\ndesc->sg_req[i].chan_reg.dma_scr =\r\nSTM32_DMA_SCR_DIR(STM32_DMA_MEM_TO_MEM) |\r\nSTM32_DMA_SCR_MINC |\r\nSTM32_DMA_SCR_PINC |\r\nSTM32_DMA_SCR_TCIE |\r\nSTM32_DMA_SCR_TEIE;\r\ndesc->sg_req[i].chan_reg.dma_sfcr = STM32_DMA_SFCR_DMDIS |\r\nSTM32_DMA_SFCR_FTH(STM32_DMA_FIFO_THRESHOLD_FULL) |\r\nSTM32_DMA_SFCR_FEIE;\r\ndesc->sg_req[i].chan_reg.dma_spar = src + offset;\r\ndesc->sg_req[i].chan_reg.dma_sm0ar = dest + offset;\r\ndesc->sg_req[i].chan_reg.dma_sndtr = xfer_count;\r\n}\r\ndesc->num_sgs = num_sgs;\r\ndesc->cyclic = false;\r\nreturn vchan_tx_prep(&chan->vchan, &desc->vdesc, flags);\r\n}\r\nstatic u32 stm32_dma_get_remaining_bytes(struct stm32_dma_chan *chan)\r\n{\r\nu32 dma_scr, width, ndtr;\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\ndma_scr = stm32_dma_read(dmadev, STM32_DMA_SCR(chan->id));\r\nwidth = STM32_DMA_SCR_PSIZE_GET(dma_scr);\r\nndtr = stm32_dma_read(dmadev, STM32_DMA_SNDTR(chan->id));\r\nreturn ndtr << width;\r\n}\r\nstatic size_t stm32_dma_desc_residue(struct stm32_dma_chan *chan,\r\nstruct stm32_dma_desc *desc,\r\nu32 next_sg)\r\n{\r\nu32 residue = 0;\r\nint i;\r\nif (chan->desc->cyclic && next_sg == 0)\r\nreturn stm32_dma_get_remaining_bytes(chan);\r\nfor (i = next_sg; i < desc->num_sgs; i++)\r\nresidue += desc->sg_req[i].len;\r\nresidue += stm32_dma_get_remaining_bytes(chan);\r\nreturn residue;\r\n}\r\nstatic enum dma_status stm32_dma_tx_status(struct dma_chan *c,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *state)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nstruct virt_dma_desc *vdesc;\r\nenum dma_status status;\r\nunsigned long flags;\r\nu32 residue = 0;\r\nstatus = dma_cookie_status(c, cookie, state);\r\nif ((status == DMA_COMPLETE) || (!state))\r\nreturn status;\r\nspin_lock_irqsave(&chan->vchan.lock, flags);\r\nvdesc = vchan_find_desc(&chan->vchan, cookie);\r\nif (chan->desc && cookie == chan->desc->vdesc.tx.cookie)\r\nresidue = stm32_dma_desc_residue(chan, chan->desc,\r\nchan->next_sg);\r\nelse if (vdesc)\r\nresidue = stm32_dma_desc_residue(chan,\r\nto_stm32_dma_desc(vdesc), 0);\r\ndma_set_residue(state, residue);\r\nspin_unlock_irqrestore(&chan->vchan.lock, flags);\r\nreturn status;\r\n}\r\nstatic int stm32_dma_alloc_chan_resources(struct dma_chan *c)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nint ret;\r\nchan->config_init = false;\r\nret = clk_prepare_enable(dmadev->clk);\r\nif (ret < 0) {\r\ndev_err(chan2dev(chan), "clk_prepare_enable failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nret = stm32_dma_disable_chan(chan);\r\nif (ret < 0)\r\nclk_disable_unprepare(dmadev->clk);\r\nreturn ret;\r\n}\r\nstatic void stm32_dma_free_chan_resources(struct dma_chan *c)\r\n{\r\nstruct stm32_dma_chan *chan = to_stm32_dma_chan(c);\r\nstruct stm32_dma_device *dmadev = stm32_dma_get_dev(chan);\r\nunsigned long flags;\r\ndev_dbg(chan2dev(chan), "Freeing channel %d\n", chan->id);\r\nif (chan->busy) {\r\nspin_lock_irqsave(&chan->vchan.lock, flags);\r\nstm32_dma_stop(chan);\r\nchan->desc = NULL;\r\nspin_unlock_irqrestore(&chan->vchan.lock, flags);\r\n}\r\nclk_disable_unprepare(dmadev->clk);\r\nvchan_free_chan_resources(to_virt_chan(c));\r\n}\r\nstatic void stm32_dma_desc_free(struct virt_dma_desc *vdesc)\r\n{\r\nkfree(container_of(vdesc, struct stm32_dma_desc, vdesc));\r\n}\r\nstatic void stm32_dma_set_config(struct stm32_dma_chan *chan,\r\nstruct stm32_dma_cfg *cfg)\r\n{\r\nstm32_dma_clear_reg(&chan->chan_reg);\r\nchan->chan_reg.dma_scr = cfg->stream_config & STM32_DMA_SCR_CFG_MASK;\r\nchan->chan_reg.dma_scr |= STM32_DMA_SCR_REQ(cfg->request_line);\r\nchan->chan_reg.dma_scr |= STM32_DMA_SCR_TEIE | STM32_DMA_SCR_TCIE;\r\nchan->chan_reg.dma_sfcr = cfg->threshold & STM32_DMA_SFCR_FTH_MASK;\r\n}\r\nstatic struct dma_chan *stm32_dma_of_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct stm32_dma_device *dmadev = ofdma->of_dma_data;\r\nstruct device *dev = dmadev->ddev.dev;\r\nstruct stm32_dma_cfg cfg;\r\nstruct stm32_dma_chan *chan;\r\nstruct dma_chan *c;\r\nif (dma_spec->args_count < 4) {\r\ndev_err(dev, "Bad number of cells\n");\r\nreturn NULL;\r\n}\r\ncfg.channel_id = dma_spec->args[0];\r\ncfg.request_line = dma_spec->args[1];\r\ncfg.stream_config = dma_spec->args[2];\r\ncfg.threshold = dma_spec->args[3];\r\nif ((cfg.channel_id >= STM32_DMA_MAX_CHANNELS) ||\r\n(cfg.request_line >= STM32_DMA_MAX_REQUEST_ID)) {\r\ndev_err(dev, "Bad channel and/or request id\n");\r\nreturn NULL;\r\n}\r\nchan = &dmadev->chan[cfg.channel_id];\r\nc = dma_get_slave_channel(&chan->vchan.chan);\r\nif (!c) {\r\ndev_err(dev, "No more channels available\n");\r\nreturn NULL;\r\n}\r\nstm32_dma_set_config(chan, &cfg);\r\nreturn c;\r\n}\r\nstatic int stm32_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct stm32_dma_chan *chan;\r\nstruct stm32_dma_device *dmadev;\r\nstruct dma_device *dd;\r\nconst struct of_device_id *match;\r\nstruct resource *res;\r\nint i, ret;\r\nmatch = of_match_device(stm32_dma_of_match, &pdev->dev);\r\nif (!match) {\r\ndev_err(&pdev->dev, "Error: No device match found\n");\r\nreturn -ENODEV;\r\n}\r\ndmadev = devm_kzalloc(&pdev->dev, sizeof(*dmadev), GFP_KERNEL);\r\nif (!dmadev)\r\nreturn -ENOMEM;\r\ndd = &dmadev->ddev;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ndmadev->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(dmadev->base))\r\nreturn PTR_ERR(dmadev->base);\r\ndmadev->clk = devm_clk_get(&pdev->dev, NULL);\r\nif (IS_ERR(dmadev->clk)) {\r\ndev_err(&pdev->dev, "Error: Missing controller clock\n");\r\nreturn PTR_ERR(dmadev->clk);\r\n}\r\ndmadev->mem2mem = of_property_read_bool(pdev->dev.of_node,\r\n"st,mem2mem");\r\ndmadev->rst = devm_reset_control_get(&pdev->dev, NULL);\r\nif (!IS_ERR(dmadev->rst)) {\r\nreset_control_assert(dmadev->rst);\r\nudelay(2);\r\nreset_control_deassert(dmadev->rst);\r\n}\r\ndma_cap_set(DMA_SLAVE, dd->cap_mask);\r\ndma_cap_set(DMA_PRIVATE, dd->cap_mask);\r\ndma_cap_set(DMA_CYCLIC, dd->cap_mask);\r\ndd->device_alloc_chan_resources = stm32_dma_alloc_chan_resources;\r\ndd->device_free_chan_resources = stm32_dma_free_chan_resources;\r\ndd->device_tx_status = stm32_dma_tx_status;\r\ndd->device_issue_pending = stm32_dma_issue_pending;\r\ndd->device_prep_slave_sg = stm32_dma_prep_slave_sg;\r\ndd->device_prep_dma_cyclic = stm32_dma_prep_dma_cyclic;\r\ndd->device_config = stm32_dma_slave_config;\r\ndd->device_terminate_all = stm32_dma_terminate_all;\r\ndd->device_synchronize = stm32_dma_synchronize;\r\ndd->src_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) |\r\nBIT(DMA_SLAVE_BUSWIDTH_2_BYTES) |\r\nBIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\r\ndd->dst_addr_widths = BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) |\r\nBIT(DMA_SLAVE_BUSWIDTH_2_BYTES) |\r\nBIT(DMA_SLAVE_BUSWIDTH_4_BYTES);\r\ndd->directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\ndd->residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\ndd->max_burst = STM32_DMA_MAX_BURST;\r\ndd->dev = &pdev->dev;\r\nINIT_LIST_HEAD(&dd->channels);\r\nif (dmadev->mem2mem) {\r\ndma_cap_set(DMA_MEMCPY, dd->cap_mask);\r\ndd->device_prep_dma_memcpy = stm32_dma_prep_dma_memcpy;\r\ndd->directions |= BIT(DMA_MEM_TO_MEM);\r\n}\r\nfor (i = 0; i < STM32_DMA_MAX_CHANNELS; i++) {\r\nchan = &dmadev->chan[i];\r\nchan->id = i;\r\nchan->vchan.desc_free = stm32_dma_desc_free;\r\nvchan_init(&chan->vchan, dd);\r\n}\r\nret = dma_async_device_register(dd);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < STM32_DMA_MAX_CHANNELS; i++) {\r\nchan = &dmadev->chan[i];\r\nres = platform_get_resource(pdev, IORESOURCE_IRQ, i);\r\nif (!res) {\r\nret = -EINVAL;\r\ndev_err(&pdev->dev, "No irq resource for chan %d\n", i);\r\ngoto err_unregister;\r\n}\r\nchan->irq = res->start;\r\nret = devm_request_irq(&pdev->dev, chan->irq,\r\nstm32_dma_chan_irq, 0,\r\ndev_name(chan2dev(chan)), chan);\r\nif (ret) {\r\ndev_err(&pdev->dev,\r\n"request_irq failed with err %d channel %d\n",\r\nret, i);\r\ngoto err_unregister;\r\n}\r\n}\r\nret = of_dma_controller_register(pdev->dev.of_node,\r\nstm32_dma_of_xlate, dmadev);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev,\r\n"STM32 DMA DMA OF registration failed %d\n", ret);\r\ngoto err_unregister;\r\n}\r\nplatform_set_drvdata(pdev, dmadev);\r\ndev_info(&pdev->dev, "STM32 DMA driver registered\n");\r\nreturn 0;\r\nerr_unregister:\r\ndma_async_device_unregister(dd);\r\nreturn ret;\r\n}\r\nstatic int __init stm32_dma_init(void)\r\n{\r\nreturn platform_driver_probe(&stm32_dma_driver, stm32_dma_probe);\r\n}
