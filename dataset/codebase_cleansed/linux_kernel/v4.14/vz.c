static inline long kvm_vz_read_gc0_ebase(void)\r\n{\r\nif (sizeof(long) == 8 && cpu_has_ebase_wg)\r\nreturn read_gc0_ebase_64();\r\nelse\r\nreturn read_gc0_ebase();\r\n}\r\nstatic inline void kvm_vz_write_gc0_ebase(long v)\r\n{\r\nif (sizeof(long) == 8 &&\r\n(cpu_has_mips64r6 || cpu_has_ebase_wg)) {\r\nwrite_gc0_ebase_64(v | MIPS_EBASE_WG);\r\nwrite_gc0_ebase_64(v);\r\n} else {\r\nwrite_gc0_ebase(v | MIPS_EBASE_WG);\r\nwrite_gc0_ebase(v);\r\n}\r\n}\r\nstatic inline unsigned int kvm_vz_config_guest_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nreturn CONF_CM_CMASK;\r\n}\r\nstatic inline unsigned int kvm_vz_config1_guest_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline unsigned int kvm_vz_config2_guest_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline unsigned int kvm_vz_config3_guest_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nreturn MIPS_CONF3_ISA_OE;\r\n}\r\nstatic inline unsigned int kvm_vz_config4_guest_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nreturn MIPS_CONF4_VFTLBPAGESIZE;\r\n}\r\nstatic inline unsigned int kvm_vz_config5_guest_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned int mask = MIPS_CONF5_K | MIPS_CONF5_CV | MIPS_CONF5_SBRI;\r\nif (kvm_mips_guest_has_msa(&vcpu->arch))\r\nmask |= MIPS_CONF5_MSAEN;\r\nif (kvm_mips_guest_has_fpu(&vcpu->arch)) {\r\nif (cpu_has_ufr)\r\nmask |= MIPS_CONF5_UFR;\r\nif (cpu_has_fre)\r\nmask |= MIPS_CONF5_FRE | MIPS_CONF5_UFE;\r\n}\r\nreturn mask;\r\n}\r\nstatic inline unsigned int kvm_vz_config_user_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvm_vz_config_guest_wrmask(vcpu) | MIPS_CONF_M;\r\n}\r\nstatic inline unsigned int kvm_vz_config1_user_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned int mask = kvm_vz_config1_guest_wrmask(vcpu) | MIPS_CONF_M;\r\nif (kvm_mips_guest_can_have_fpu(&vcpu->arch))\r\nmask |= MIPS_CONF1_FP;\r\nreturn mask;\r\n}\r\nstatic inline unsigned int kvm_vz_config2_user_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvm_vz_config2_guest_wrmask(vcpu) | MIPS_CONF_M;\r\n}\r\nstatic inline unsigned int kvm_vz_config3_user_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned int mask = kvm_vz_config3_guest_wrmask(vcpu) | MIPS_CONF_M |\r\nMIPS_CONF3_ULRI | MIPS_CONF3_CTXTC;\r\nif (kvm_mips_guest_can_have_msa(&vcpu->arch))\r\nmask |= MIPS_CONF3_MSA;\r\nreturn mask;\r\n}\r\nstatic inline unsigned int kvm_vz_config4_user_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvm_vz_config4_guest_wrmask(vcpu) | MIPS_CONF_M;\r\n}\r\nstatic inline unsigned int kvm_vz_config5_user_wrmask(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvm_vz_config5_guest_wrmask(vcpu) | MIPS_CONF5_MRP;\r\n}\r\nstatic gpa_t kvm_vz_gva_to_gpa_cb(gva_t gva)\r\n{\r\nreturn gva;\r\n}\r\nstatic void kvm_vz_queue_irq(struct kvm_vcpu *vcpu, unsigned int priority)\r\n{\r\nset_bit(priority, &vcpu->arch.pending_exceptions);\r\nclear_bit(priority, &vcpu->arch.pending_exceptions_clr);\r\n}\r\nstatic void kvm_vz_dequeue_irq(struct kvm_vcpu *vcpu, unsigned int priority)\r\n{\r\nclear_bit(priority, &vcpu->arch.pending_exceptions);\r\nset_bit(priority, &vcpu->arch.pending_exceptions_clr);\r\n}\r\nstatic void kvm_vz_queue_timer_int_cb(struct kvm_vcpu *vcpu)\r\n{\r\nkvm_vz_queue_irq(vcpu, MIPS_EXC_INT_TIMER);\r\n}\r\nstatic void kvm_vz_dequeue_timer_int_cb(struct kvm_vcpu *vcpu)\r\n{\r\nkvm_vz_dequeue_irq(vcpu, MIPS_EXC_INT_TIMER);\r\n}\r\nstatic void kvm_vz_queue_io_int_cb(struct kvm_vcpu *vcpu,\r\nstruct kvm_mips_interrupt *irq)\r\n{\r\nint intr = (int)irq->irq;\r\nswitch (intr) {\r\ncase 2:\r\nkvm_vz_queue_irq(vcpu, MIPS_EXC_INT_IO);\r\nbreak;\r\ncase 3:\r\nkvm_vz_queue_irq(vcpu, MIPS_EXC_INT_IPI_1);\r\nbreak;\r\ncase 4:\r\nkvm_vz_queue_irq(vcpu, MIPS_EXC_INT_IPI_2);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void kvm_vz_dequeue_io_int_cb(struct kvm_vcpu *vcpu,\r\nstruct kvm_mips_interrupt *irq)\r\n{\r\nint intr = (int)irq->irq;\r\nswitch (intr) {\r\ncase -2:\r\nkvm_vz_dequeue_irq(vcpu, MIPS_EXC_INT_IO);\r\nbreak;\r\ncase -3:\r\nkvm_vz_dequeue_irq(vcpu, MIPS_EXC_INT_IPI_1);\r\nbreak;\r\ncase -4:\r\nkvm_vz_dequeue_irq(vcpu, MIPS_EXC_INT_IPI_2);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic int kvm_vz_irq_deliver_cb(struct kvm_vcpu *vcpu, unsigned int priority,\r\nu32 cause)\r\n{\r\nu32 irq = (priority < MIPS_EXC_MAX) ?\r\nkvm_vz_priority_to_irq[priority] : 0;\r\nswitch (priority) {\r\ncase MIPS_EXC_INT_TIMER:\r\nset_gc0_cause(C_TI);\r\nbreak;\r\ncase MIPS_EXC_INT_IO:\r\ncase MIPS_EXC_INT_IPI_1:\r\ncase MIPS_EXC_INT_IPI_2:\r\nif (cpu_has_guestctl2)\r\nset_c0_guestctl2(irq);\r\nelse\r\nset_gc0_cause(irq);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nclear_bit(priority, &vcpu->arch.pending_exceptions);\r\nreturn 1;\r\n}\r\nstatic int kvm_vz_irq_clear_cb(struct kvm_vcpu *vcpu, unsigned int priority,\r\nu32 cause)\r\n{\r\nu32 irq = (priority < MIPS_EXC_MAX) ?\r\nkvm_vz_priority_to_irq[priority] : 0;\r\nswitch (priority) {\r\ncase MIPS_EXC_INT_TIMER:\r\nif (cpu_has_guestctl2) {\r\nif (!(read_c0_guestctl2() & (irq << 14)))\r\nclear_c0_guestctl2(irq);\r\n} else {\r\nclear_gc0_cause(irq);\r\n}\r\nbreak;\r\ncase MIPS_EXC_INT_IO:\r\ncase MIPS_EXC_INT_IPI_1:\r\ncase MIPS_EXC_INT_IPI_2:\r\nif (cpu_has_guestctl2) {\r\nif (!(read_c0_guestctl2() & (irq << 14)))\r\nclear_c0_guestctl2(irq);\r\n} else {\r\nclear_gc0_cause(irq);\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nclear_bit(priority, &vcpu->arch.pending_exceptions_clr);\r\nreturn 1;\r\n}\r\nstatic bool kvm_vz_should_use_htimer(struct kvm_vcpu *vcpu)\r\n{\r\nif (kvm_mips_count_disabled(vcpu))\r\nreturn false;\r\nif (mips_hpt_frequency != vcpu->arch.count_hz)\r\nreturn false;\r\nif (current_cpu_data.gtoffset_mask != 0xffffffff)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic void _kvm_vz_restore_stimer(struct kvm_vcpu *vcpu, u32 compare,\r\nu32 cause)\r\n{\r\nwrite_c0_gtoffset(compare - read_c0_count());\r\nback_to_back_c0_hazard();\r\nwrite_gc0_cause(cause);\r\n}\r\nstatic void _kvm_vz_restore_htimer(struct kvm_vcpu *vcpu,\r\nu32 compare, u32 cause)\r\n{\r\nu32 start_count, after_count;\r\nktime_t freeze_time;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nfreeze_time = kvm_mips_freeze_hrtimer(vcpu, &start_count);\r\nwrite_c0_gtoffset(start_count - read_c0_count());\r\nlocal_irq_restore(flags);\r\nback_to_back_c0_hazard();\r\nwrite_gc0_cause(cause);\r\nback_to_back_c0_hazard();\r\nafter_count = read_gc0_count();\r\nif (after_count - start_count > compare - start_count - 1)\r\nkvm_vz_queue_irq(vcpu, MIPS_EXC_INT_TIMER);\r\n}\r\nstatic void kvm_vz_restore_timer(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nu32 cause, compare;\r\ncompare = kvm_read_sw_gc0_compare(cop0);\r\ncause = kvm_read_sw_gc0_cause(cop0);\r\nwrite_gc0_compare(compare);\r\n_kvm_vz_restore_stimer(vcpu, compare, cause);\r\n}\r\nvoid kvm_vz_acquire_htimer(struct kvm_vcpu *vcpu)\r\n{\r\nu32 gctl0;\r\ngctl0 = read_c0_guestctl0();\r\nif (!(gctl0 & MIPS_GCTL0_GT) && kvm_vz_should_use_htimer(vcpu)) {\r\nwrite_c0_guestctl0(gctl0 | MIPS_GCTL0_GT);\r\n_kvm_vz_restore_htimer(vcpu, read_gc0_compare(),\r\nread_gc0_cause());\r\n}\r\n}\r\nstatic void _kvm_vz_save_htimer(struct kvm_vcpu *vcpu,\r\nu32 *out_compare, u32 *out_cause)\r\n{\r\nu32 cause, compare, before_count, end_count;\r\nktime_t before_time;\r\ncompare = read_gc0_compare();\r\n*out_compare = compare;\r\nbefore_time = ktime_get();\r\nbefore_count = read_gc0_count();\r\nback_to_back_c0_hazard();\r\ncause = read_gc0_cause();\r\n*out_cause = cause;\r\nback_to_back_c0_hazard();\r\nend_count = read_gc0_count();\r\nif (end_count - before_count > compare - before_count - 1)\r\nkvm_vz_queue_irq(vcpu, MIPS_EXC_INT_TIMER);\r\nkvm_mips_restore_hrtimer(vcpu, before_time, end_count, -0x10000);\r\n}\r\nstatic void kvm_vz_save_timer(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nu32 gctl0, compare, cause;\r\ngctl0 = read_c0_guestctl0();\r\nif (gctl0 & MIPS_GCTL0_GT) {\r\nwrite_c0_guestctl0(gctl0 & ~MIPS_GCTL0_GT);\r\n_kvm_vz_save_htimer(vcpu, &compare, &cause);\r\n} else {\r\ncompare = read_gc0_compare();\r\ncause = read_gc0_cause();\r\n}\r\nkvm_write_sw_gc0_cause(cop0, cause);\r\nkvm_write_sw_gc0_compare(cop0, compare);\r\n}\r\nvoid kvm_vz_lose_htimer(struct kvm_vcpu *vcpu)\r\n{\r\nu32 gctl0, compare, cause;\r\npreempt_disable();\r\ngctl0 = read_c0_guestctl0();\r\nif (gctl0 & MIPS_GCTL0_GT) {\r\nwrite_c0_guestctl0(gctl0 & ~MIPS_GCTL0_GT);\r\n_kvm_vz_save_htimer(vcpu, &compare, &cause);\r\n_kvm_vz_restore_stimer(vcpu, compare, cause);\r\n}\r\npreempt_enable();\r\n}\r\nstatic bool is_eva_access(union mips_instruction inst)\r\n{\r\nif (inst.spec3_format.opcode != spec3_op)\r\nreturn false;\r\nswitch (inst.spec3_format.func) {\r\ncase lwle_op:\r\ncase lwre_op:\r\ncase cachee_op:\r\ncase sbe_op:\r\ncase she_op:\r\ncase sce_op:\r\ncase swe_op:\r\ncase swle_op:\r\ncase swre_op:\r\ncase prefe_op:\r\ncase lbue_op:\r\ncase lhue_op:\r\ncase lbe_op:\r\ncase lhe_op:\r\ncase lle_op:\r\ncase lwe_op:\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic bool is_eva_am_mapped(struct kvm_vcpu *vcpu, unsigned int am, bool eu)\r\n{\r\nu32 am_lookup;\r\nint err;\r\nam_lookup = 0x70080000 << am;\r\nif ((s32)am_lookup < 0) {\r\nif (!eu || !(read_gc0_status() & ST0_ERL))\r\nreturn true;\r\n} else {\r\nam_lookup <<= 8;\r\nif ((s32)am_lookup < 0) {\r\nunion mips_instruction inst;\r\nunsigned int status;\r\nu32 *opc;\r\nstatus = read_gc0_status();\r\nif (!(status & (ST0_EXL | ST0_ERL)) &&\r\n(status & ST0_KSU))\r\nreturn true;\r\nopc = (u32 *)vcpu->arch.pc;\r\nif (vcpu->arch.host_cp0_cause & CAUSEF_BD)\r\nopc += 1;\r\nerr = kvm_get_badinstr(opc, vcpu, &inst.word);\r\nif (!err && is_eva_access(inst))\r\nreturn true;\r\n}\r\n}\r\nreturn false;\r\n}\r\nstatic int kvm_vz_gva_to_gpa(struct kvm_vcpu *vcpu, unsigned long gva,\r\nunsigned long *gpa)\r\n{\r\nu32 gva32 = gva;\r\nunsigned long segctl;\r\nif ((long)gva == (s32)gva32) {\r\nif (cpu_guest_has_segments) {\r\nunsigned long mask, pa;\r\nswitch (gva32 >> 29) {\r\ncase 0:\r\ncase 1:\r\nsegctl = read_gc0_segctl2() >> 16;\r\nmask = (unsigned long)0xfc0000000ull;\r\nbreak;\r\ncase 2:\r\ncase 3:\r\nsegctl = read_gc0_segctl2();\r\nmask = (unsigned long)0xfc0000000ull;\r\nbreak;\r\ncase 4:\r\nsegctl = read_gc0_segctl1() >> 16;\r\nmask = (unsigned long)0xfe0000000ull;\r\nbreak;\r\ncase 5:\r\nsegctl = read_gc0_segctl1();\r\nmask = (unsigned long)0xfe0000000ull;\r\nbreak;\r\ncase 6:\r\nsegctl = read_gc0_segctl0() >> 16;\r\nmask = (unsigned long)0xfe0000000ull;\r\nbreak;\r\ncase 7:\r\nsegctl = read_gc0_segctl0();\r\nmask = (unsigned long)0xfe0000000ull;\r\nbreak;\r\ndefault:\r\nunreachable();\r\n}\r\nif (is_eva_am_mapped(vcpu, (segctl >> 4) & 0x7,\r\nsegctl & 0x0008))\r\ngoto tlb_mapped;\r\npa = (segctl << 20) & mask;\r\npa |= gva32 & ~mask;\r\n*gpa = pa;\r\nreturn 0;\r\n} else if ((s32)gva32 < (s32)0xc0000000) {\r\n*gpa = gva32 & 0x1fffffff;\r\nreturn 0;\r\n}\r\n#ifdef CONFIG_64BIT\r\n} else if ((gva & 0xc000000000000000) == 0x8000000000000000) {\r\nif (cpu_guest_has_segments) {\r\nsegctl = read_gc0_segctl2();\r\nif (segctl & (1ull << (56 + ((gva >> 59) & 0x7)))) {\r\nsegctl = read_gc0_segctl1();\r\nif (is_eva_am_mapped(vcpu, (segctl >> 59) & 0x7,\r\n0))\r\ngoto tlb_mapped;\r\n}\r\n}\r\n*gpa = gva & 0x07ffffffffffffff;\r\nreturn 0;\r\n#endif\r\n}\r\ntlb_mapped:\r\nreturn kvm_vz_guest_tlb_lookup(vcpu, gva, gpa);\r\n}\r\nstatic int kvm_vz_badvaddr_to_gpa(struct kvm_vcpu *vcpu, unsigned long badvaddr,\r\nunsigned long *gpa)\r\n{\r\nunsigned int gexccode = (vcpu->arch.host_cp0_guestctl0 &\r\nMIPS_GCTL0_GEXC) >> MIPS_GCTL0_GEXC_SHIFT;\r\nif (likely(gexccode == MIPS_GCTL0_GEXC_GPA)) {\r\n*gpa = badvaddr;\r\nreturn 0;\r\n}\r\nif (WARN(gexccode != MIPS_GCTL0_GEXC_GVA,\r\n"Unexpected gexccode %#x\n", gexccode))\r\nreturn -EINVAL;\r\nreturn kvm_vz_gva_to_gpa(vcpu, badvaddr, gpa);\r\n}\r\nstatic int kvm_trap_vz_no_handler(struct kvm_vcpu *vcpu)\r\n{\r\nu32 *opc = (u32 *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nu32 exccode = (cause & CAUSEF_EXCCODE) >> CAUSEB_EXCCODE;\r\nunsigned long badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nu32 inst = 0;\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nkvm_get_badinstr(opc, vcpu, &inst);\r\nkvm_err("Exception Code: %d not handled @ PC: %p, inst: 0x%08x BadVaddr: %#lx Status: %#x\n",\r\nexccode, opc, inst, badvaddr,\r\nread_gc0_status());\r\nkvm_arch_vcpu_dump_regs(vcpu);\r\nvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\nstatic unsigned long mips_process_maar(unsigned int op, unsigned long val)\r\n{\r\nunsigned long mask = 0xfffff000 | MIPS_MAAR_S | MIPS_MAAR_VL;\r\nif (read_gc0_pagegrain() & PG_ELPA)\r\nmask |= 0x00ffffff00000000ull;\r\nif (cpu_guest_has_mvh)\r\nmask |= MIPS_MAAR_VH;\r\nif (op == mtc_op) {\r\nval &= ~MIPS_MAAR_VH;\r\n} else if (op == dmtc_op) {\r\nval &= ~MIPS_MAAR_VH;\r\nif (val & MIPS_MAAR_VL)\r\nval |= MIPS_MAAR_VH;\r\n}\r\nreturn val & mask;\r\n}\r\nstatic void kvm_write_maari(struct kvm_vcpu *vcpu, unsigned long val)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nval &= MIPS_MAARI_INDEX;\r\nif (val == MIPS_MAARI_INDEX)\r\nkvm_write_sw_gc0_maari(cop0, ARRAY_SIZE(vcpu->arch.maar) - 1);\r\nelse if (val < ARRAY_SIZE(vcpu->arch.maar))\r\nkvm_write_sw_gc0_maari(cop0, val);\r\n}\r\nstatic enum emulation_result kvm_vz_gpsi_cop0(union mips_instruction inst,\r\nu32 *opc, u32 cause,\r\nstruct kvm_run *run,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nenum emulation_result er = EMULATE_DONE;\r\nu32 rt, rd, sel;\r\nunsigned long curr_pc;\r\nunsigned long val;\r\ncurr_pc = vcpu->arch.pc;\r\ner = update_pc(vcpu, cause);\r\nif (er == EMULATE_FAIL)\r\nreturn er;\r\nif (inst.co_format.co) {\r\nswitch (inst.co_format.func) {\r\ncase wait_op:\r\ner = kvm_mips_emul_wait(vcpu);\r\nbreak;\r\ndefault:\r\ner = EMULATE_FAIL;\r\n}\r\n} else {\r\nrt = inst.c0r_format.rt;\r\nrd = inst.c0r_format.rd;\r\nsel = inst.c0r_format.sel;\r\nswitch (inst.c0r_format.rs) {\r\ncase dmfc_op:\r\ncase mfc_op:\r\n#ifdef CONFIG_KVM_MIPS_DEBUG_COP0_COUNTERS\r\ncop0->stat[rd][sel]++;\r\n#endif\r\nif (rd == MIPS_CP0_COUNT &&\r\nsel == 0) {\r\nval = kvm_mips_read_count(vcpu);\r\n} else if (rd == MIPS_CP0_COMPARE &&\r\nsel == 0) {\r\nval = read_gc0_compare();\r\n} else if (rd == MIPS_CP0_LLADDR &&\r\nsel == 0) {\r\nif (cpu_guest_has_rw_llb)\r\nval = read_gc0_lladdr() &\r\nMIPS_LLADDR_LLB;\r\nelse\r\nval = 0;\r\n} else if (rd == MIPS_CP0_LLADDR &&\r\nsel == 1 &&\r\ncpu_guest_has_maar &&\r\n!cpu_guest_has_dyn_maar) {\r\nBUG_ON(kvm_read_sw_gc0_maari(cop0) >=\r\nARRAY_SIZE(vcpu->arch.maar));\r\nval = vcpu->arch.maar[\r\nkvm_read_sw_gc0_maari(cop0)];\r\n} else if ((rd == MIPS_CP0_PRID &&\r\n(sel == 0 ||\r\nsel == 2 ||\r\nsel == 3)) ||\r\n(rd == MIPS_CP0_STATUS &&\r\n(sel == 2 ||\r\nsel == 3)) ||\r\n(rd == MIPS_CP0_CONFIG &&\r\n(sel == 7)) ||\r\n(rd == MIPS_CP0_LLADDR &&\r\n(sel == 2) &&\r\ncpu_guest_has_maar &&\r\n!cpu_guest_has_dyn_maar) ||\r\n(rd == MIPS_CP0_ERRCTL &&\r\n(sel == 0))) {\r\nval = cop0->reg[rd][sel];\r\n} else {\r\nval = 0;\r\ner = EMULATE_FAIL;\r\n}\r\nif (er != EMULATE_FAIL) {\r\nif (inst.c0r_format.rs == mfc_op)\r\nval = (int)val;\r\nvcpu->arch.gprs[rt] = val;\r\n}\r\ntrace_kvm_hwr(vcpu, (inst.c0r_format.rs == mfc_op) ?\r\nKVM_TRACE_MFC0 : KVM_TRACE_DMFC0,\r\nKVM_TRACE_COP0(rd, sel), val);\r\nbreak;\r\ncase dmtc_op:\r\ncase mtc_op:\r\n#ifdef CONFIG_KVM_MIPS_DEBUG_COP0_COUNTERS\r\ncop0->stat[rd][sel]++;\r\n#endif\r\nval = vcpu->arch.gprs[rt];\r\ntrace_kvm_hwr(vcpu, (inst.c0r_format.rs == mtc_op) ?\r\nKVM_TRACE_MTC0 : KVM_TRACE_DMTC0,\r\nKVM_TRACE_COP0(rd, sel), val);\r\nif (rd == MIPS_CP0_COUNT &&\r\nsel == 0) {\r\nkvm_vz_lose_htimer(vcpu);\r\nkvm_mips_write_count(vcpu, vcpu->arch.gprs[rt]);\r\n} else if (rd == MIPS_CP0_COMPARE &&\r\nsel == 0) {\r\nkvm_mips_write_compare(vcpu,\r\nvcpu->arch.gprs[rt],\r\ntrue);\r\n} else if (rd == MIPS_CP0_LLADDR &&\r\nsel == 0) {\r\nif (cpu_guest_has_rw_llb &&\r\n!(val & MIPS_LLADDR_LLB))\r\nwrite_gc0_lladdr(0);\r\n} else if (rd == MIPS_CP0_LLADDR &&\r\nsel == 1 &&\r\ncpu_guest_has_maar &&\r\n!cpu_guest_has_dyn_maar) {\r\nval = mips_process_maar(inst.c0r_format.rs,\r\nval);\r\nBUG_ON(kvm_read_sw_gc0_maari(cop0) >=\r\nARRAY_SIZE(vcpu->arch.maar));\r\nvcpu->arch.maar[kvm_read_sw_gc0_maari(cop0)] =\r\nval;\r\n} else if (rd == MIPS_CP0_LLADDR &&\r\n(sel == 2) &&\r\ncpu_guest_has_maar &&\r\n!cpu_guest_has_dyn_maar) {\r\nkvm_write_maari(vcpu, val);\r\n} else if (rd == MIPS_CP0_ERRCTL &&\r\n(sel == 0)) {\r\n} else {\r\ner = EMULATE_FAIL;\r\n}\r\nbreak;\r\ndefault:\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\n}\r\nif (er == EMULATE_FAIL) {\r\nkvm_err("[%#lx]%s: unsupported cop0 instruction 0x%08x\n",\r\ncurr_pc, __func__, inst.word);\r\nvcpu->arch.pc = curr_pc;\r\n}\r\nreturn er;\r\n}\r\nstatic enum emulation_result kvm_vz_gpsi_cache(union mips_instruction inst,\r\nu32 *opc, u32 cause,\r\nstruct kvm_run *run,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DONE;\r\nu32 cache, op_inst, op, base;\r\ns16 offset;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nunsigned long va, curr_pc;\r\ncurr_pc = vcpu->arch.pc;\r\ner = update_pc(vcpu, cause);\r\nif (er == EMULATE_FAIL)\r\nreturn er;\r\nbase = inst.i_format.rs;\r\nop_inst = inst.i_format.rt;\r\nif (cpu_has_mips_r6)\r\noffset = inst.spec3_format.simmediate;\r\nelse\r\noffset = inst.i_format.simmediate;\r\ncache = op_inst & CacheOp_Cache;\r\nop = op_inst & CacheOp_Op;\r\nva = arch->gprs[base] + offset;\r\nkvm_debug("CACHE (cache: %#x, op: %#x, base[%d]: %#lx, offset: %#x\n",\r\ncache, op, base, arch->gprs[base], offset);\r\nif (cache != Cache_I && cache != Cache_D)\r\nreturn EMULATE_DONE;\r\nswitch (op_inst) {\r\ncase Index_Invalidate_I:\r\nflush_icache_line_indexed(va);\r\nreturn EMULATE_DONE;\r\ncase Index_Writeback_Inv_D:\r\nflush_dcache_line_indexed(va);\r\nreturn EMULATE_DONE;\r\ncase Hit_Invalidate_I:\r\ncase Hit_Invalidate_D:\r\ncase Hit_Writeback_Inv_D:\r\nif (boot_cpu_type() == CPU_CAVIUM_OCTEON3) {\r\nlocal_flush_icache_range(0, 0);\r\nreturn EMULATE_DONE;\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n};\r\nkvm_err("@ %#lx/%#lx CACHE (cache: %#x, op: %#x, base[%d]: %#lx, offset: %#x\n",\r\ncurr_pc, vcpu->arch.gprs[31], cache, op, base, arch->gprs[base],\r\noffset);\r\nvcpu->arch.pc = curr_pc;\r\nreturn EMULATE_FAIL;\r\n}\r\nstatic enum emulation_result kvm_trap_vz_handle_gpsi(u32 cause, u32 *opc,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DONE;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nstruct kvm_run *run = vcpu->run;\r\nunion mips_instruction inst;\r\nint rd, rt, sel;\r\nint err;\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nerr = kvm_get_badinstr(opc, vcpu, &inst.word);\r\nif (err)\r\nreturn EMULATE_FAIL;\r\nswitch (inst.r_format.opcode) {\r\ncase cop0_op:\r\ner = kvm_vz_gpsi_cop0(inst, opc, cause, run, vcpu);\r\nbreak;\r\n#ifndef CONFIG_CPU_MIPSR6\r\ncase cache_op:\r\ntrace_kvm_exit(vcpu, KVM_TRACE_EXIT_CACHE);\r\ner = kvm_vz_gpsi_cache(inst, opc, cause, run, vcpu);\r\nbreak;\r\n#endif\r\ncase spec3_op:\r\nswitch (inst.spec3_format.func) {\r\n#ifdef CONFIG_CPU_MIPSR6\r\ncase cache6_op:\r\ntrace_kvm_exit(vcpu, KVM_TRACE_EXIT_CACHE);\r\ner = kvm_vz_gpsi_cache(inst, opc, cause, run, vcpu);\r\nbreak;\r\n#endif\r\ncase rdhwr_op:\r\nif (inst.r_format.rs || (inst.r_format.re >> 3))\r\ngoto unknown;\r\nrd = inst.r_format.rd;\r\nrt = inst.r_format.rt;\r\nsel = inst.r_format.re & 0x7;\r\nswitch (rd) {\r\ncase MIPS_HWR_CC:\r\narch->gprs[rt] =\r\n(long)(int)kvm_mips_read_count(vcpu);\r\nbreak;\r\ndefault:\r\ntrace_kvm_hwr(vcpu, KVM_TRACE_RDHWR,\r\nKVM_TRACE_HWR(rd, sel), 0);\r\ngoto unknown;\r\n};\r\ntrace_kvm_hwr(vcpu, KVM_TRACE_RDHWR,\r\nKVM_TRACE_HWR(rd, sel), arch->gprs[rt]);\r\ner = update_pc(vcpu, cause);\r\nbreak;\r\ndefault:\r\ngoto unknown;\r\n};\r\nbreak;\r\nunknown:\r\ndefault:\r\nkvm_err("GPSI exception not supported (%p/%#x)\n",\r\nopc, inst.word);\r\nkvm_arch_vcpu_dump_regs(vcpu);\r\ner = EMULATE_FAIL;\r\nbreak;\r\n}\r\nreturn er;\r\n}\r\nstatic enum emulation_result kvm_trap_vz_handle_gsfc(u32 cause, u32 *opc,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er = EMULATE_DONE;\r\nstruct kvm_vcpu_arch *arch = &vcpu->arch;\r\nunion mips_instruction inst;\r\nint err;\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nerr = kvm_get_badinstr(opc, vcpu, &inst.word);\r\nif (err)\r\nreturn EMULATE_FAIL;\r\nif (inst.c0r_format.opcode == cop0_op &&\r\ninst.c0r_format.rs == mtc_op &&\r\ninst.c0r_format.z == 0) {\r\nint rt = inst.c0r_format.rt;\r\nint rd = inst.c0r_format.rd;\r\nint sel = inst.c0r_format.sel;\r\nunsigned int val = arch->gprs[rt];\r\nunsigned int old_val, change;\r\ntrace_kvm_hwr(vcpu, KVM_TRACE_MTC0, KVM_TRACE_COP0(rd, sel),\r\nval);\r\nif ((rd == MIPS_CP0_STATUS) && (sel == 0)) {\r\nif (!kvm_mips_guest_has_fpu(&vcpu->arch))\r\nval &= ~(ST0_CU1 | ST0_FR);\r\nif (!(boot_cpu_data.fpu_id & MIPS_FPIR_F64))\r\nval &= ~ST0_FR;\r\nold_val = read_gc0_status();\r\nchange = val ^ old_val;\r\nif (change & ST0_FR) {\r\nkvm_drop_fpu(vcpu);\r\n}\r\nif (change & ST0_CU1 && !(val & ST0_FR) &&\r\nvcpu->arch.aux_inuse & KVM_MIPS_AUX_MSA)\r\nkvm_lose_fpu(vcpu);\r\nwrite_gc0_status(val);\r\n} else if ((rd == MIPS_CP0_CAUSE) && (sel == 0)) {\r\nu32 old_cause = read_gc0_cause();\r\nu32 change = old_cause ^ val;\r\nif (change & CAUSEF_DC) {\r\nif (val & CAUSEF_DC) {\r\nkvm_vz_lose_htimer(vcpu);\r\nkvm_mips_count_disable_cause(vcpu);\r\n} else {\r\nkvm_mips_count_enable_cause(vcpu);\r\n}\r\n}\r\nchange &= (CAUSEF_DC | CAUSEF_IV | CAUSEF_WP |\r\nCAUSEF_IP0 | CAUSEF_IP1);\r\nchange &= ~CAUSEF_WP | old_cause;\r\nwrite_gc0_cause(old_cause ^ change);\r\n} else if ((rd == MIPS_CP0_STATUS) && (sel == 1)) {\r\nwrite_gc0_intctl(val);\r\n} else if ((rd == MIPS_CP0_CONFIG) && (sel == 5)) {\r\nold_val = read_gc0_config5();\r\nchange = val ^ old_val;\r\npreempt_disable();\r\nif (change & MIPS_CONF5_FRE &&\r\nvcpu->arch.aux_inuse & KVM_MIPS_AUX_FPU)\r\nchange_c0_config5(MIPS_CONF5_FRE, val);\r\npreempt_enable();\r\nval = old_val ^\r\n(change & kvm_vz_config5_guest_wrmask(vcpu));\r\nwrite_gc0_config5(val);\r\n} else {\r\nkvm_err("Handle GSFC, unsupported field change @ %p: %#x\n",\r\nopc, inst.word);\r\ner = EMULATE_FAIL;\r\n}\r\nif (er != EMULATE_FAIL)\r\ner = update_pc(vcpu, cause);\r\n} else {\r\nkvm_err("Handle GSFC, unrecognized instruction @ %p: %#x\n",\r\nopc, inst.word);\r\ner = EMULATE_FAIL;\r\n}\r\nreturn er;\r\n}\r\nstatic enum emulation_result kvm_trap_vz_handle_ghfc(u32 cause, u32 *opc,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\ntrace_kvm_guest_mode_change(vcpu);\r\nreturn EMULATE_DONE;\r\n}\r\nstatic enum emulation_result kvm_trap_vz_handle_hc(u32 cause, u32 *opc,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er;\r\nunion mips_instruction inst;\r\nunsigned long curr_pc;\r\nint err;\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nerr = kvm_get_badinstr(opc, vcpu, &inst.word);\r\nif (err)\r\nreturn EMULATE_FAIL;\r\ncurr_pc = vcpu->arch.pc;\r\ner = update_pc(vcpu, cause);\r\nif (er == EMULATE_FAIL)\r\nreturn er;\r\ner = kvm_mips_emul_hypcall(vcpu, inst);\r\nif (er == EMULATE_FAIL)\r\nvcpu->arch.pc = curr_pc;\r\nreturn er;\r\n}\r\nstatic enum emulation_result kvm_trap_vz_no_handler_guest_exit(u32 gexccode,\r\nu32 cause,\r\nu32 *opc,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nu32 inst;\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nkvm_get_badinstr(opc, vcpu, &inst);\r\nkvm_err("Guest Exception Code: %d not yet handled @ PC: %p, inst: 0x%08x Status: %#x\n",\r\ngexccode, opc, inst, read_gc0_status());\r\nreturn EMULATE_FAIL;\r\n}\r\nstatic int kvm_trap_vz_handle_guest_exit(struct kvm_vcpu *vcpu)\r\n{\r\nu32 *opc = (u32 *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nu32 gexccode = (vcpu->arch.host_cp0_guestctl0 &\r\nMIPS_GCTL0_GEXC) >> MIPS_GCTL0_GEXC_SHIFT;\r\nint ret = RESUME_GUEST;\r\ntrace_kvm_exit(vcpu, KVM_TRACE_EXIT_GEXCCODE_BASE + gexccode);\r\nswitch (gexccode) {\r\ncase MIPS_GCTL0_GEXC_GPSI:\r\n++vcpu->stat.vz_gpsi_exits;\r\ner = kvm_trap_vz_handle_gpsi(cause, opc, vcpu);\r\nbreak;\r\ncase MIPS_GCTL0_GEXC_GSFC:\r\n++vcpu->stat.vz_gsfc_exits;\r\ner = kvm_trap_vz_handle_gsfc(cause, opc, vcpu);\r\nbreak;\r\ncase MIPS_GCTL0_GEXC_HC:\r\n++vcpu->stat.vz_hc_exits;\r\ner = kvm_trap_vz_handle_hc(cause, opc, vcpu);\r\nbreak;\r\ncase MIPS_GCTL0_GEXC_GRR:\r\n++vcpu->stat.vz_grr_exits;\r\ner = kvm_trap_vz_no_handler_guest_exit(gexccode, cause, opc,\r\nvcpu);\r\nbreak;\r\ncase MIPS_GCTL0_GEXC_GVA:\r\n++vcpu->stat.vz_gva_exits;\r\ner = kvm_trap_vz_no_handler_guest_exit(gexccode, cause, opc,\r\nvcpu);\r\nbreak;\r\ncase MIPS_GCTL0_GEXC_GHFC:\r\n++vcpu->stat.vz_ghfc_exits;\r\ner = kvm_trap_vz_handle_ghfc(cause, opc, vcpu);\r\nbreak;\r\ncase MIPS_GCTL0_GEXC_GPA:\r\n++vcpu->stat.vz_gpa_exits;\r\ner = kvm_trap_vz_no_handler_guest_exit(gexccode, cause, opc,\r\nvcpu);\r\nbreak;\r\ndefault:\r\n++vcpu->stat.vz_resvd_exits;\r\ner = kvm_trap_vz_no_handler_guest_exit(gexccode, cause, opc,\r\nvcpu);\r\nbreak;\r\n}\r\nif (er == EMULATE_DONE) {\r\nret = RESUME_GUEST;\r\n} else if (er == EMULATE_HYPERCALL) {\r\nret = kvm_mips_handle_hypcall(vcpu);\r\n} else {\r\nvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_vz_handle_cop_unusable(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_FAIL;\r\nint ret = RESUME_GUEST;\r\nif (((cause & CAUSEF_CE) >> CAUSEB_CE) == 1) {\r\nif (WARN_ON(!kvm_mips_guest_has_fpu(&vcpu->arch) ||\r\nvcpu->arch.aux_inuse & KVM_MIPS_AUX_FPU)) {\r\npreempt_enable();\r\nreturn EMULATE_FAIL;\r\n}\r\nkvm_own_fpu(vcpu);\r\ner = EMULATE_DONE;\r\n}\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nret = RESUME_GUEST;\r\nbreak;\r\ncase EMULATE_FAIL:\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_vz_handle_msa_disabled(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nif (!kvm_mips_guest_has_msa(&vcpu->arch) ||\r\n(read_gc0_status() & (ST0_CU1 | ST0_FR)) == ST0_CU1 ||\r\n!(read_gc0_config5() & MIPS_CONF5_MSAEN) ||\r\nvcpu->arch.aux_inuse & KVM_MIPS_AUX_MSA) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\nkvm_own_msa(vcpu);\r\nreturn RESUME_GUEST;\r\n}\r\nstatic int kvm_trap_vz_handle_tlb_ld_miss(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 *opc = (u32 *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nulong badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nunion mips_instruction inst;\r\nenum emulation_result er = EMULATE_DONE;\r\nint err, ret = RESUME_GUEST;\r\nif (kvm_mips_handle_vz_root_tlb_fault(badvaddr, vcpu, false)) {\r\nif (kvm_is_ifetch_fault(&vcpu->arch)) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nerr = kvm_get_badinstr(opc, vcpu, &inst.word);\r\nif (err) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\ner = kvm_mips_emulate_load(inst, cause, run, vcpu);\r\nif (er == EMULATE_FAIL) {\r\nkvm_err("Guest Emulate Load from MMIO space failed: PC: %p, BadVaddr: %#lx\n",\r\nopc, badvaddr);\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\n}\r\n}\r\nif (er == EMULATE_DONE) {\r\nret = RESUME_GUEST;\r\n} else if (er == EMULATE_DO_MMIO) {\r\nrun->exit_reason = KVM_EXIT_MMIO;\r\nret = RESUME_HOST;\r\n} else {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_vz_handle_tlb_st_miss(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 *opc = (u32 *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nulong badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nunion mips_instruction inst;\r\nenum emulation_result er = EMULATE_DONE;\r\nint err;\r\nint ret = RESUME_GUEST;\r\nif (kvm_vz_badvaddr_to_gpa(vcpu, badvaddr, &badvaddr))\r\nreturn RESUME_GUEST;\r\nvcpu->arch.host_cp0_badvaddr = badvaddr;\r\nif (kvm_mips_handle_vz_root_tlb_fault(badvaddr, vcpu, true)) {\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nerr = kvm_get_badinstr(opc, vcpu, &inst.word);\r\nif (err) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\ner = kvm_mips_emulate_store(inst, cause, run, vcpu);\r\nif (er == EMULATE_FAIL) {\r\nkvm_err("Guest Emulate Store to MMIO space failed: PC: %p, BadVaddr: %#lx\n",\r\nopc, badvaddr);\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\n}\r\n}\r\nif (er == EMULATE_DONE) {\r\nret = RESUME_GUEST;\r\n} else if (er == EMULATE_DO_MMIO) {\r\nrun->exit_reason = KVM_EXIT_MMIO;\r\nret = RESUME_HOST;\r\n} else {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic unsigned long kvm_vz_num_regs(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long ret;\r\nret = ARRAY_SIZE(kvm_vz_get_one_regs);\r\nif (cpu_guest_has_userlocal)\r\n++ret;\r\nif (cpu_guest_has_badinstr)\r\n++ret;\r\nif (cpu_guest_has_badinstrp)\r\n++ret;\r\nif (cpu_guest_has_contextconfig)\r\nret += ARRAY_SIZE(kvm_vz_get_one_regs_contextconfig);\r\nif (cpu_guest_has_segments)\r\nret += ARRAY_SIZE(kvm_vz_get_one_regs_segments);\r\nif (cpu_guest_has_htw)\r\nret += ARRAY_SIZE(kvm_vz_get_one_regs_htw);\r\nif (cpu_guest_has_maar && !cpu_guest_has_dyn_maar)\r\nret += 1 + ARRAY_SIZE(vcpu->arch.maar);\r\nret += __arch_hweight8(cpu_data[0].guest.kscratch_mask);\r\nreturn ret;\r\n}\r\nstatic int kvm_vz_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *indices)\r\n{\r\nu64 index;\r\nunsigned int i;\r\nif (copy_to_user(indices, kvm_vz_get_one_regs,\r\nsizeof(kvm_vz_get_one_regs)))\r\nreturn -EFAULT;\r\nindices += ARRAY_SIZE(kvm_vz_get_one_regs);\r\nif (cpu_guest_has_userlocal) {\r\nindex = KVM_REG_MIPS_CP0_USERLOCAL;\r\nif (copy_to_user(indices, &index, sizeof(index)))\r\nreturn -EFAULT;\r\n++indices;\r\n}\r\nif (cpu_guest_has_badinstr) {\r\nindex = KVM_REG_MIPS_CP0_BADINSTR;\r\nif (copy_to_user(indices, &index, sizeof(index)))\r\nreturn -EFAULT;\r\n++indices;\r\n}\r\nif (cpu_guest_has_badinstrp) {\r\nindex = KVM_REG_MIPS_CP0_BADINSTRP;\r\nif (copy_to_user(indices, &index, sizeof(index)))\r\nreturn -EFAULT;\r\n++indices;\r\n}\r\nif (cpu_guest_has_contextconfig) {\r\nif (copy_to_user(indices, kvm_vz_get_one_regs_contextconfig,\r\nsizeof(kvm_vz_get_one_regs_contextconfig)))\r\nreturn -EFAULT;\r\nindices += ARRAY_SIZE(kvm_vz_get_one_regs_contextconfig);\r\n}\r\nif (cpu_guest_has_segments) {\r\nif (copy_to_user(indices, kvm_vz_get_one_regs_segments,\r\nsizeof(kvm_vz_get_one_regs_segments)))\r\nreturn -EFAULT;\r\nindices += ARRAY_SIZE(kvm_vz_get_one_regs_segments);\r\n}\r\nif (cpu_guest_has_htw) {\r\nif (copy_to_user(indices, kvm_vz_get_one_regs_htw,\r\nsizeof(kvm_vz_get_one_regs_htw)))\r\nreturn -EFAULT;\r\nindices += ARRAY_SIZE(kvm_vz_get_one_regs_htw);\r\n}\r\nif (cpu_guest_has_maar && !cpu_guest_has_dyn_maar) {\r\nfor (i = 0; i < ARRAY_SIZE(vcpu->arch.maar); ++i) {\r\nindex = KVM_REG_MIPS_CP0_MAAR(i);\r\nif (copy_to_user(indices, &index, sizeof(index)))\r\nreturn -EFAULT;\r\n++indices;\r\n}\r\nindex = KVM_REG_MIPS_CP0_MAARI;\r\nif (copy_to_user(indices, &index, sizeof(index)))\r\nreturn -EFAULT;\r\n++indices;\r\n}\r\nfor (i = 0; i < 6; ++i) {\r\nif (!cpu_guest_has_kscr(i + 2))\r\ncontinue;\r\nif (copy_to_user(indices, &kvm_vz_get_one_regs_kscratch[i],\r\nsizeof(kvm_vz_get_one_regs_kscratch[i])))\r\nreturn -EFAULT;\r\n++indices;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline s64 entrylo_kvm_to_user(unsigned long v)\r\n{\r\ns64 mask, ret = v;\r\nif (BITS_PER_LONG == 32) {\r\nmask = MIPS_ENTRYLO_RI | MIPS_ENTRYLO_XI;\r\nret &= ~mask;\r\nret |= ((s64)v & mask) << 32;\r\n}\r\nreturn ret;\r\n}\r\nstatic inline unsigned long entrylo_user_to_kvm(s64 v)\r\n{\r\nunsigned long mask, ret = v;\r\nif (BITS_PER_LONG == 32) {\r\nmask = MIPS_ENTRYLO_RI | MIPS_ENTRYLO_XI;\r\nret &= ~mask;\r\nret |= (v >> 32) & mask;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_vz_get_one_reg(struct kvm_vcpu *vcpu,\r\nconst struct kvm_one_reg *reg,\r\ns64 *v)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nunsigned int idx;\r\nswitch (reg->id) {\r\ncase KVM_REG_MIPS_CP0_INDEX:\r\n*v = (long)read_gc0_index();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYLO0:\r\n*v = entrylo_kvm_to_user(read_gc0_entrylo0());\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYLO1:\r\n*v = entrylo_kvm_to_user(read_gc0_entrylo1());\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONTEXT:\r\n*v = (long)read_gc0_context();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONTEXTCONFIG:\r\nif (!cpu_guest_has_contextconfig)\r\nreturn -EINVAL;\r\n*v = read_gc0_contextconfig();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_USERLOCAL:\r\nif (!cpu_guest_has_userlocal)\r\nreturn -EINVAL;\r\n*v = read_gc0_userlocal();\r\nbreak;\r\n#ifdef CONFIG_64BIT\r\ncase KVM_REG_MIPS_CP0_XCONTEXTCONFIG:\r\nif (!cpu_guest_has_contextconfig)\r\nreturn -EINVAL;\r\n*v = read_gc0_xcontextconfig();\r\nbreak;\r\n#endif\r\ncase KVM_REG_MIPS_CP0_PAGEMASK:\r\n*v = (long)read_gc0_pagemask();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PAGEGRAIN:\r\n*v = (long)read_gc0_pagegrain();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_SEGCTL0:\r\nif (!cpu_guest_has_segments)\r\nreturn -EINVAL;\r\n*v = read_gc0_segctl0();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_SEGCTL1:\r\nif (!cpu_guest_has_segments)\r\nreturn -EINVAL;\r\n*v = read_gc0_segctl1();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_SEGCTL2:\r\nif (!cpu_guest_has_segments)\r\nreturn -EINVAL;\r\n*v = read_gc0_segctl2();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PWBASE:\r\nif (!cpu_guest_has_htw)\r\nreturn -EINVAL;\r\n*v = read_gc0_pwbase();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PWFIELD:\r\nif (!cpu_guest_has_htw)\r\nreturn -EINVAL;\r\n*v = read_gc0_pwfield();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PWSIZE:\r\nif (!cpu_guest_has_htw)\r\nreturn -EINVAL;\r\n*v = read_gc0_pwsize();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_WIRED:\r\n*v = (long)read_gc0_wired();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PWCTL:\r\nif (!cpu_guest_has_htw)\r\nreturn -EINVAL;\r\n*v = read_gc0_pwctl();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_HWRENA:\r\n*v = (long)read_gc0_hwrena();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADVADDR:\r\n*v = (long)read_gc0_badvaddr();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADINSTR:\r\nif (!cpu_guest_has_badinstr)\r\nreturn -EINVAL;\r\n*v = read_gc0_badinstr();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADINSTRP:\r\nif (!cpu_guest_has_badinstrp)\r\nreturn -EINVAL;\r\n*v = read_gc0_badinstrp();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_COUNT:\r\n*v = kvm_mips_read_count(vcpu);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYHI:\r\n*v = (long)read_gc0_entryhi();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_COMPARE:\r\n*v = (long)read_gc0_compare();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_STATUS:\r\n*v = (long)read_gc0_status();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_INTCTL:\r\n*v = read_gc0_intctl();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CAUSE:\r\n*v = (long)read_gc0_cause();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_EPC:\r\n*v = (long)read_gc0_epc();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PRID:\r\nswitch (boot_cpu_type()) {\r\ncase CPU_CAVIUM_OCTEON3:\r\n*v = read_gc0_prid();\r\nbreak;\r\ndefault:\r\n*v = (long)kvm_read_c0_guest_prid(cop0);\r\nbreak;\r\n};\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_EBASE:\r\n*v = kvm_vz_read_gc0_ebase();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG:\r\n*v = read_gc0_config();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG1:\r\nif (!cpu_guest_has_conf1)\r\nreturn -EINVAL;\r\n*v = read_gc0_config1();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG2:\r\nif (!cpu_guest_has_conf2)\r\nreturn -EINVAL;\r\n*v = read_gc0_config2();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG3:\r\nif (!cpu_guest_has_conf3)\r\nreturn -EINVAL;\r\n*v = read_gc0_config3();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG4:\r\nif (!cpu_guest_has_conf4)\r\nreturn -EINVAL;\r\n*v = read_gc0_config4();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG5:\r\nif (!cpu_guest_has_conf5)\r\nreturn -EINVAL;\r\n*v = read_gc0_config5();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_MAAR(0) ... KVM_REG_MIPS_CP0_MAAR(0x3f):\r\nif (!cpu_guest_has_maar || cpu_guest_has_dyn_maar)\r\nreturn -EINVAL;\r\nidx = reg->id - KVM_REG_MIPS_CP0_MAAR(0);\r\nif (idx >= ARRAY_SIZE(vcpu->arch.maar))\r\nreturn -EINVAL;\r\n*v = vcpu->arch.maar[idx];\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_MAARI:\r\nif (!cpu_guest_has_maar || cpu_guest_has_dyn_maar)\r\nreturn -EINVAL;\r\n*v = kvm_read_sw_gc0_maari(vcpu->arch.cop0);\r\nbreak;\r\n#ifdef CONFIG_64BIT\r\ncase KVM_REG_MIPS_CP0_XCONTEXT:\r\n*v = read_gc0_xcontext();\r\nbreak;\r\n#endif\r\ncase KVM_REG_MIPS_CP0_ERROREPC:\r\n*v = (long)read_gc0_errorepc();\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH1 ... KVM_REG_MIPS_CP0_KSCRATCH6:\r\nidx = reg->id - KVM_REG_MIPS_CP0_KSCRATCH1 + 2;\r\nif (!cpu_guest_has_kscr(idx))\r\nreturn -EINVAL;\r\nswitch (idx) {\r\ncase 2:\r\n*v = (long)read_gc0_kscratch1();\r\nbreak;\r\ncase 3:\r\n*v = (long)read_gc0_kscratch2();\r\nbreak;\r\ncase 4:\r\n*v = (long)read_gc0_kscratch3();\r\nbreak;\r\ncase 5:\r\n*v = (long)read_gc0_kscratch4();\r\nbreak;\r\ncase 6:\r\n*v = (long)read_gc0_kscratch5();\r\nbreak;\r\ncase 7:\r\n*v = (long)read_gc0_kscratch6();\r\nbreak;\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_CTL:\r\n*v = vcpu->arch.count_ctl;\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_RESUME:\r\n*v = ktime_to_ns(vcpu->arch.count_resume);\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_HZ:\r\n*v = vcpu->arch.count_hz;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int kvm_vz_set_one_reg(struct kvm_vcpu *vcpu,\r\nconst struct kvm_one_reg *reg,\r\ns64 v)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nunsigned int idx;\r\nint ret = 0;\r\nunsigned int cur, change;\r\nswitch (reg->id) {\r\ncase KVM_REG_MIPS_CP0_INDEX:\r\nwrite_gc0_index(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYLO0:\r\nwrite_gc0_entrylo0(entrylo_user_to_kvm(v));\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYLO1:\r\nwrite_gc0_entrylo1(entrylo_user_to_kvm(v));\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONTEXT:\r\nwrite_gc0_context(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONTEXTCONFIG:\r\nif (!cpu_guest_has_contextconfig)\r\nreturn -EINVAL;\r\nwrite_gc0_contextconfig(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_USERLOCAL:\r\nif (!cpu_guest_has_userlocal)\r\nreturn -EINVAL;\r\nwrite_gc0_userlocal(v);\r\nbreak;\r\n#ifdef CONFIG_64BIT\r\ncase KVM_REG_MIPS_CP0_XCONTEXTCONFIG:\r\nif (!cpu_guest_has_contextconfig)\r\nreturn -EINVAL;\r\nwrite_gc0_xcontextconfig(v);\r\nbreak;\r\n#endif\r\ncase KVM_REG_MIPS_CP0_PAGEMASK:\r\nwrite_gc0_pagemask(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PAGEGRAIN:\r\nwrite_gc0_pagegrain(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_SEGCTL0:\r\nif (!cpu_guest_has_segments)\r\nreturn -EINVAL;\r\nwrite_gc0_segctl0(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_SEGCTL1:\r\nif (!cpu_guest_has_segments)\r\nreturn -EINVAL;\r\nwrite_gc0_segctl1(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_SEGCTL2:\r\nif (!cpu_guest_has_segments)\r\nreturn -EINVAL;\r\nwrite_gc0_segctl2(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PWBASE:\r\nif (!cpu_guest_has_htw)\r\nreturn -EINVAL;\r\nwrite_gc0_pwbase(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PWFIELD:\r\nif (!cpu_guest_has_htw)\r\nreturn -EINVAL;\r\nwrite_gc0_pwfield(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PWSIZE:\r\nif (!cpu_guest_has_htw)\r\nreturn -EINVAL;\r\nwrite_gc0_pwsize(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_WIRED:\r\nchange_gc0_wired(MIPSR6_WIRED_WIRED, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PWCTL:\r\nif (!cpu_guest_has_htw)\r\nreturn -EINVAL;\r\nwrite_gc0_pwctl(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_HWRENA:\r\nwrite_gc0_hwrena(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADVADDR:\r\nwrite_gc0_badvaddr(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADINSTR:\r\nif (!cpu_guest_has_badinstr)\r\nreturn -EINVAL;\r\nwrite_gc0_badinstr(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADINSTRP:\r\nif (!cpu_guest_has_badinstrp)\r\nreturn -EINVAL;\r\nwrite_gc0_badinstrp(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_COUNT:\r\nkvm_mips_write_count(vcpu, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYHI:\r\nwrite_gc0_entryhi(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_COMPARE:\r\nkvm_mips_write_compare(vcpu, v, false);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_STATUS:\r\nwrite_gc0_status(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_INTCTL:\r\nwrite_gc0_intctl(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CAUSE:\r\nif ((read_gc0_cause() ^ v) & CAUSEF_DC) {\r\nif (v & CAUSEF_DC) {\r\nkvm_mips_count_disable_cause(vcpu);\r\nchange_gc0_cause((u32)~CAUSEF_DC, v);\r\n} else {\r\nchange_gc0_cause((u32)~CAUSEF_DC, v);\r\nkvm_mips_count_enable_cause(vcpu);\r\n}\r\n} else {\r\nwrite_gc0_cause(v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_EPC:\r\nwrite_gc0_epc(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PRID:\r\nswitch (boot_cpu_type()) {\r\ncase CPU_CAVIUM_OCTEON3:\r\nbreak;\r\ndefault:\r\nkvm_write_c0_guest_prid(cop0, v);\r\nbreak;\r\n};\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_EBASE:\r\nkvm_vz_write_gc0_ebase(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG:\r\ncur = read_gc0_config();\r\nchange = (cur ^ v) & kvm_vz_config_user_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nwrite_gc0_config(v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG1:\r\nif (!cpu_guest_has_conf1)\r\nbreak;\r\ncur = read_gc0_config1();\r\nchange = (cur ^ v) & kvm_vz_config1_user_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nwrite_gc0_config1(v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG2:\r\nif (!cpu_guest_has_conf2)\r\nbreak;\r\ncur = read_gc0_config2();\r\nchange = (cur ^ v) & kvm_vz_config2_user_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nwrite_gc0_config2(v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG3:\r\nif (!cpu_guest_has_conf3)\r\nbreak;\r\ncur = read_gc0_config3();\r\nchange = (cur ^ v) & kvm_vz_config3_user_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nwrite_gc0_config3(v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG4:\r\nif (!cpu_guest_has_conf4)\r\nbreak;\r\ncur = read_gc0_config4();\r\nchange = (cur ^ v) & kvm_vz_config4_user_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nwrite_gc0_config4(v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG5:\r\nif (!cpu_guest_has_conf5)\r\nbreak;\r\ncur = read_gc0_config5();\r\nchange = (cur ^ v) & kvm_vz_config5_user_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nwrite_gc0_config5(v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_MAAR(0) ... KVM_REG_MIPS_CP0_MAAR(0x3f):\r\nif (!cpu_guest_has_maar || cpu_guest_has_dyn_maar)\r\nreturn -EINVAL;\r\nidx = reg->id - KVM_REG_MIPS_CP0_MAAR(0);\r\nif (idx >= ARRAY_SIZE(vcpu->arch.maar))\r\nreturn -EINVAL;\r\nvcpu->arch.maar[idx] = mips_process_maar(dmtc_op, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_MAARI:\r\nif (!cpu_guest_has_maar || cpu_guest_has_dyn_maar)\r\nreturn -EINVAL;\r\nkvm_write_maari(vcpu, v);\r\nbreak;\r\n#ifdef CONFIG_64BIT\r\ncase KVM_REG_MIPS_CP0_XCONTEXT:\r\nwrite_gc0_xcontext(v);\r\nbreak;\r\n#endif\r\ncase KVM_REG_MIPS_CP0_ERROREPC:\r\nwrite_gc0_errorepc(v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH1 ... KVM_REG_MIPS_CP0_KSCRATCH6:\r\nidx = reg->id - KVM_REG_MIPS_CP0_KSCRATCH1 + 2;\r\nif (!cpu_guest_has_kscr(idx))\r\nreturn -EINVAL;\r\nswitch (idx) {\r\ncase 2:\r\nwrite_gc0_kscratch1(v);\r\nbreak;\r\ncase 3:\r\nwrite_gc0_kscratch2(v);\r\nbreak;\r\ncase 4:\r\nwrite_gc0_kscratch3(v);\r\nbreak;\r\ncase 5:\r\nwrite_gc0_kscratch4(v);\r\nbreak;\r\ncase 6:\r\nwrite_gc0_kscratch5(v);\r\nbreak;\r\ncase 7:\r\nwrite_gc0_kscratch6(v);\r\nbreak;\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_CTL:\r\nret = kvm_mips_set_count_ctl(vcpu, v);\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_RESUME:\r\nret = kvm_mips_set_count_resume(vcpu, v);\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_HZ:\r\nret = kvm_mips_set_count_hz(vcpu, v);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn ret;\r\n}\r\nstatic void kvm_vz_get_new_guestid(unsigned long cpu, struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long guestid = guestid_cache(cpu);\r\nif (!(++guestid & GUESTID_MASK)) {\r\nif (cpu_has_vtag_icache)\r\nflush_icache_all();\r\nif (!guestid)\r\nguestid = GUESTID_FIRST_VERSION;\r\n++guestid;\r\nkvm_vz_local_flush_roottlb_all_guests();\r\nkvm_vz_local_flush_guesttlb_all();\r\n}\r\nguestid_cache(cpu) = guestid;\r\n}\r\nstatic int kvm_vz_check_requests(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nint ret = 0;\r\nint i;\r\nif (!kvm_request_pending(vcpu))\r\nreturn 0;\r\nif (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu)) {\r\nif (cpu_has_guestid) {\r\nfor_each_possible_cpu(i)\r\nvcpu->arch.vzguestid[i] = 0;\r\nret = 1;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void kvm_vz_vcpu_save_wired(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned int wired = read_gc0_wired();\r\nstruct kvm_mips_tlb *tlbs;\r\nint i;\r\nwired &= MIPSR6_WIRED_WIRED;\r\nif (wired > vcpu->arch.wired_tlb_limit) {\r\ntlbs = krealloc(vcpu->arch.wired_tlb, wired *\r\nsizeof(*vcpu->arch.wired_tlb), GFP_ATOMIC);\r\nif (WARN_ON(!tlbs)) {\r\nwired = vcpu->arch.wired_tlb_limit;\r\n} else {\r\nvcpu->arch.wired_tlb = tlbs;\r\nvcpu->arch.wired_tlb_limit = wired;\r\n}\r\n}\r\nif (wired)\r\nkvm_vz_save_guesttlb(vcpu->arch.wired_tlb, 0, wired);\r\nfor (i = wired; i < vcpu->arch.wired_tlb_used; ++i) {\r\nvcpu->arch.wired_tlb[i].tlb_hi = UNIQUE_GUEST_ENTRYHI(i);\r\nvcpu->arch.wired_tlb[i].tlb_lo[0] = 0;\r\nvcpu->arch.wired_tlb[i].tlb_lo[1] = 0;\r\nvcpu->arch.wired_tlb[i].tlb_mask = 0;\r\n}\r\nvcpu->arch.wired_tlb_used = wired;\r\n}\r\nstatic void kvm_vz_vcpu_load_wired(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->arch.wired_tlb)\r\nkvm_vz_load_guesttlb(vcpu->arch.wired_tlb, 0,\r\nvcpu->arch.wired_tlb_used);\r\n}\r\nstatic void kvm_vz_vcpu_load_tlb(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct mm_struct *gpa_mm = &kvm->arch.gpa_mm;\r\nbool migrated;\r\nmigrated = (vcpu->arch.last_exec_cpu != cpu);\r\nvcpu->arch.last_exec_cpu = cpu;\r\nif (cpu_has_guestid) {\r\nif (migrated ||\r\n(vcpu->arch.vzguestid[cpu] ^ guestid_cache(cpu)) &\r\nGUESTID_VERSION_MASK) {\r\nkvm_vz_get_new_guestid(cpu, vcpu);\r\nvcpu->arch.vzguestid[cpu] = guestid_cache(cpu);\r\ntrace_kvm_guestid_change(vcpu,\r\nvcpu->arch.vzguestid[cpu]);\r\n}\r\nchange_c0_guestctl1(GUESTID_MASK, vcpu->arch.vzguestid[cpu]);\r\n} else {\r\nif (migrated || last_exec_vcpu[cpu] != vcpu)\r\nkvm_vz_local_flush_guesttlb_all();\r\nlast_exec_vcpu[cpu] = vcpu;\r\nif (cpumask_test_and_clear_cpu(cpu, &kvm->arch.asid_flush_mask)\r\n|| (cpu_context(cpu, gpa_mm) ^ asid_cache(cpu)) &\r\nasid_version_mask(cpu))\r\nget_new_mmu_context(gpa_mm, cpu);\r\n}\r\n}\r\nstatic int kvm_vz_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nbool migrated, all;\r\nmigrated = (vcpu->arch.last_sched_cpu != cpu);\r\nall = migrated || (last_vcpu[cpu] != vcpu);\r\nlast_vcpu[cpu] = vcpu;\r\nkvm_restore_gc0_wired(cop0);\r\nif (current->flags & PF_VCPU) {\r\ntlbw_use_hazard();\r\nkvm_vz_vcpu_load_tlb(vcpu, cpu);\r\nkvm_vz_vcpu_load_wired(vcpu);\r\n}\r\nkvm_vz_restore_timer(vcpu);\r\nif (kvm_trace_guest_mode_change)\r\nset_c0_guestctl0(MIPS_GCTL0_MC);\r\nelse\r\nclear_c0_guestctl0(MIPS_GCTL0_MC);\r\nif (!all)\r\nreturn 0;\r\nkvm_restore_gc0_config(cop0);\r\nif (cpu_guest_has_conf1)\r\nkvm_restore_gc0_config1(cop0);\r\nif (cpu_guest_has_conf2)\r\nkvm_restore_gc0_config2(cop0);\r\nif (cpu_guest_has_conf3)\r\nkvm_restore_gc0_config3(cop0);\r\nif (cpu_guest_has_conf4)\r\nkvm_restore_gc0_config4(cop0);\r\nif (cpu_guest_has_conf5)\r\nkvm_restore_gc0_config5(cop0);\r\nif (cpu_guest_has_conf6)\r\nkvm_restore_gc0_config6(cop0);\r\nif (cpu_guest_has_conf7)\r\nkvm_restore_gc0_config7(cop0);\r\nkvm_restore_gc0_index(cop0);\r\nkvm_restore_gc0_entrylo0(cop0);\r\nkvm_restore_gc0_entrylo1(cop0);\r\nkvm_restore_gc0_context(cop0);\r\nif (cpu_guest_has_contextconfig)\r\nkvm_restore_gc0_contextconfig(cop0);\r\n#ifdef CONFIG_64BIT\r\nkvm_restore_gc0_xcontext(cop0);\r\nif (cpu_guest_has_contextconfig)\r\nkvm_restore_gc0_xcontextconfig(cop0);\r\n#endif\r\nkvm_restore_gc0_pagemask(cop0);\r\nkvm_restore_gc0_pagegrain(cop0);\r\nkvm_restore_gc0_hwrena(cop0);\r\nkvm_restore_gc0_badvaddr(cop0);\r\nkvm_restore_gc0_entryhi(cop0);\r\nkvm_restore_gc0_status(cop0);\r\nkvm_restore_gc0_intctl(cop0);\r\nkvm_restore_gc0_epc(cop0);\r\nkvm_vz_write_gc0_ebase(kvm_read_sw_gc0_ebase(cop0));\r\nif (cpu_guest_has_userlocal)\r\nkvm_restore_gc0_userlocal(cop0);\r\nkvm_restore_gc0_errorepc(cop0);\r\nif (cpu_guest_has_conf4) {\r\nif (cpu_guest_has_kscr(2))\r\nkvm_restore_gc0_kscratch1(cop0);\r\nif (cpu_guest_has_kscr(3))\r\nkvm_restore_gc0_kscratch2(cop0);\r\nif (cpu_guest_has_kscr(4))\r\nkvm_restore_gc0_kscratch3(cop0);\r\nif (cpu_guest_has_kscr(5))\r\nkvm_restore_gc0_kscratch4(cop0);\r\nif (cpu_guest_has_kscr(6))\r\nkvm_restore_gc0_kscratch5(cop0);\r\nif (cpu_guest_has_kscr(7))\r\nkvm_restore_gc0_kscratch6(cop0);\r\n}\r\nif (cpu_guest_has_badinstr)\r\nkvm_restore_gc0_badinstr(cop0);\r\nif (cpu_guest_has_badinstrp)\r\nkvm_restore_gc0_badinstrp(cop0);\r\nif (cpu_guest_has_segments) {\r\nkvm_restore_gc0_segctl0(cop0);\r\nkvm_restore_gc0_segctl1(cop0);\r\nkvm_restore_gc0_segctl2(cop0);\r\n}\r\nif (cpu_guest_has_htw) {\r\nkvm_restore_gc0_pwbase(cop0);\r\nkvm_restore_gc0_pwfield(cop0);\r\nkvm_restore_gc0_pwsize(cop0);\r\nkvm_restore_gc0_pwctl(cop0);\r\n}\r\nif (cpu_has_guestctl2)\r\nwrite_c0_guestctl2(\r\ncop0->reg[MIPS_CP0_GUESTCTL2][MIPS_CP0_GUESTCTL2_SEL]);\r\nif (cpu_guest_has_rw_llb)\r\nwrite_gc0_lladdr(0);\r\nreturn 0;\r\n}\r\nstatic int kvm_vz_vcpu_put(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nif (current->flags & PF_VCPU)\r\nkvm_vz_vcpu_save_wired(vcpu);\r\nkvm_lose_fpu(vcpu);\r\nkvm_save_gc0_index(cop0);\r\nkvm_save_gc0_entrylo0(cop0);\r\nkvm_save_gc0_entrylo1(cop0);\r\nkvm_save_gc0_context(cop0);\r\nif (cpu_guest_has_contextconfig)\r\nkvm_save_gc0_contextconfig(cop0);\r\n#ifdef CONFIG_64BIT\r\nkvm_save_gc0_xcontext(cop0);\r\nif (cpu_guest_has_contextconfig)\r\nkvm_save_gc0_xcontextconfig(cop0);\r\n#endif\r\nkvm_save_gc0_pagemask(cop0);\r\nkvm_save_gc0_pagegrain(cop0);\r\nkvm_save_gc0_wired(cop0);\r\nclear_gc0_wired(MIPSR6_WIRED_WIRED);\r\nkvm_save_gc0_hwrena(cop0);\r\nkvm_save_gc0_badvaddr(cop0);\r\nkvm_save_gc0_entryhi(cop0);\r\nkvm_save_gc0_status(cop0);\r\nkvm_save_gc0_intctl(cop0);\r\nkvm_save_gc0_epc(cop0);\r\nkvm_write_sw_gc0_ebase(cop0, kvm_vz_read_gc0_ebase());\r\nif (cpu_guest_has_userlocal)\r\nkvm_save_gc0_userlocal(cop0);\r\nkvm_save_gc0_config(cop0);\r\nif (cpu_guest_has_conf1)\r\nkvm_save_gc0_config1(cop0);\r\nif (cpu_guest_has_conf2)\r\nkvm_save_gc0_config2(cop0);\r\nif (cpu_guest_has_conf3)\r\nkvm_save_gc0_config3(cop0);\r\nif (cpu_guest_has_conf4)\r\nkvm_save_gc0_config4(cop0);\r\nif (cpu_guest_has_conf5)\r\nkvm_save_gc0_config5(cop0);\r\nif (cpu_guest_has_conf6)\r\nkvm_save_gc0_config6(cop0);\r\nif (cpu_guest_has_conf7)\r\nkvm_save_gc0_config7(cop0);\r\nkvm_save_gc0_errorepc(cop0);\r\nif (cpu_guest_has_conf4) {\r\nif (cpu_guest_has_kscr(2))\r\nkvm_save_gc0_kscratch1(cop0);\r\nif (cpu_guest_has_kscr(3))\r\nkvm_save_gc0_kscratch2(cop0);\r\nif (cpu_guest_has_kscr(4))\r\nkvm_save_gc0_kscratch3(cop0);\r\nif (cpu_guest_has_kscr(5))\r\nkvm_save_gc0_kscratch4(cop0);\r\nif (cpu_guest_has_kscr(6))\r\nkvm_save_gc0_kscratch5(cop0);\r\nif (cpu_guest_has_kscr(7))\r\nkvm_save_gc0_kscratch6(cop0);\r\n}\r\nif (cpu_guest_has_badinstr)\r\nkvm_save_gc0_badinstr(cop0);\r\nif (cpu_guest_has_badinstrp)\r\nkvm_save_gc0_badinstrp(cop0);\r\nif (cpu_guest_has_segments) {\r\nkvm_save_gc0_segctl0(cop0);\r\nkvm_save_gc0_segctl1(cop0);\r\nkvm_save_gc0_segctl2(cop0);\r\n}\r\nif (cpu_guest_has_htw &&\r\nkvm_read_sw_gc0_config3(cop0) & MIPS_CONF3_PW) {\r\nkvm_save_gc0_pwbase(cop0);\r\nkvm_save_gc0_pwfield(cop0);\r\nkvm_save_gc0_pwsize(cop0);\r\nkvm_save_gc0_pwctl(cop0);\r\n}\r\nkvm_vz_save_timer(vcpu);\r\nif (cpu_has_guestctl2)\r\ncop0->reg[MIPS_CP0_GUESTCTL2][MIPS_CP0_GUESTCTL2_SEL] =\r\nread_c0_guestctl2();\r\nreturn 0;\r\n}\r\nstatic unsigned int kvm_vz_resize_guest_vtlb(unsigned int size)\r\n{\r\nunsigned int config4 = 0, ret = 0, limit;\r\nif (cpu_guest_has_conf1)\r\nchange_gc0_config1(MIPS_CONF1_TLBS,\r\n(size - 1) << MIPS_CONF1_TLBS_SHIFT);\r\nif (cpu_guest_has_conf4) {\r\nconfig4 = read_gc0_config4();\r\nif (cpu_has_mips_r6 || (config4 & MIPS_CONF4_MMUEXTDEF) ==\r\nMIPS_CONF4_MMUEXTDEF_VTLBSIZEEXT) {\r\nconfig4 &= ~MIPS_CONF4_VTLBSIZEEXT;\r\nconfig4 |= ((size - 1) >> MIPS_CONF1_TLBS_SIZE) <<\r\nMIPS_CONF4_VTLBSIZEEXT_SHIFT;\r\n} else if ((config4 & MIPS_CONF4_MMUEXTDEF) ==\r\nMIPS_CONF4_MMUEXTDEF_MMUSIZEEXT) {\r\nconfig4 &= ~MIPS_CONF4_MMUSIZEEXT;\r\nconfig4 |= ((size - 1) >> MIPS_CONF1_TLBS_SIZE) <<\r\nMIPS_CONF4_MMUSIZEEXT_SHIFT;\r\n}\r\nwrite_gc0_config4(config4);\r\n}\r\nif (cpu_has_mips_r6) {\r\nlimit = (read_c0_wired() & MIPSR6_WIRED_LIMIT) >>\r\nMIPSR6_WIRED_LIMIT_SHIFT;\r\nif (size - 1 <= limit)\r\nlimit = 0;\r\nwrite_gc0_wired(limit << MIPSR6_WIRED_LIMIT_SHIFT);\r\n}\r\nback_to_back_c0_hazard();\r\nif (cpu_guest_has_conf1)\r\nret = (read_gc0_config1() & MIPS_CONF1_TLBS) >>\r\nMIPS_CONF1_TLBS_SHIFT;\r\nif (config4) {\r\nif (cpu_has_mips_r6 || (config4 & MIPS_CONF4_MMUEXTDEF) ==\r\nMIPS_CONF4_MMUEXTDEF_VTLBSIZEEXT)\r\nret |= ((config4 & MIPS_CONF4_VTLBSIZEEXT) >>\r\nMIPS_CONF4_VTLBSIZEEXT_SHIFT) <<\r\nMIPS_CONF1_TLBS_SIZE;\r\nelse if ((config4 & MIPS_CONF4_MMUEXTDEF) ==\r\nMIPS_CONF4_MMUEXTDEF_MMUSIZEEXT)\r\nret |= ((config4 & MIPS_CONF4_MMUSIZEEXT) >>\r\nMIPS_CONF4_MMUSIZEEXT_SHIFT) <<\r\nMIPS_CONF1_TLBS_SIZE;\r\n}\r\nreturn ret + 1;\r\n}\r\nstatic int kvm_vz_hardware_enable(void)\r\n{\r\nunsigned int mmu_size, guest_mmu_size, ftlb_size;\r\nu64 guest_cvmctl, cvmvmconfig;\r\nswitch (current_cpu_type()) {\r\ncase CPU_CAVIUM_OCTEON3:\r\nguest_cvmctl = read_gc0_cvmctl();\r\nguest_cvmctl &= ~CVMCTL_IPTI;\r\nguest_cvmctl |= 7ull << CVMCTL_IPTI_SHIFT;\r\nguest_cvmctl &= ~CVMCTL_IPPCI;\r\nguest_cvmctl |= 6ull << CVMCTL_IPPCI_SHIFT;\r\nwrite_gc0_cvmctl(guest_cvmctl);\r\ncvmvmconfig = read_c0_cvmvmconfig();\r\ncvmvmconfig |= CVMVMCONF_DGHT;\r\nmmu_size = ((cvmvmconfig & CVMVMCONF_MMUSIZEM1)\r\n>> CVMVMCONF_MMUSIZEM1_S) + 1;\r\nguest_mmu_size = mmu_size / 2;\r\nmmu_size -= guest_mmu_size;\r\ncvmvmconfig &= ~CVMVMCONF_RMMUSIZEM1;\r\ncvmvmconfig |= mmu_size - 1;\r\nwrite_c0_cvmvmconfig(cvmvmconfig);\r\ncurrent_cpu_data.tlbsize = mmu_size;\r\ncurrent_cpu_data.tlbsizevtlb = mmu_size;\r\ncurrent_cpu_data.guest.tlbsize = guest_mmu_size;\r\nkvm_vz_local_flush_guesttlb_all();\r\nbreak;\r\ndefault:\r\nmmu_size = current_cpu_data.tlbsizevtlb;\r\nftlb_size = current_cpu_data.tlbsize - mmu_size;\r\nguest_mmu_size = kvm_vz_resize_guest_vtlb(mmu_size);\r\ncurrent_cpu_data.guest.tlbsize = guest_mmu_size + ftlb_size;\r\nkvm_vz_local_flush_guesttlb_all();\r\nguest_mmu_size = mmu_size - num_wired_entries() - 2;\r\nguest_mmu_size = kvm_vz_resize_guest_vtlb(guest_mmu_size);\r\ncurrent_cpu_data.guest.tlbsize = guest_mmu_size + ftlb_size;\r\nif (cmpxchg(&kvm_vz_guest_vtlb_size, 0, guest_mmu_size) &&\r\nWARN(guest_mmu_size != kvm_vz_guest_vtlb_size,\r\n"Available guest VTLB size mismatch"))\r\nreturn -EINVAL;\r\nbreak;\r\n}\r\nwrite_c0_guestctl0(MIPS_GCTL0_CP0 |\r\n(MIPS_GCTL0_AT_GUEST << MIPS_GCTL0_AT_SHIFT) |\r\nMIPS_GCTL0_CG | MIPS_GCTL0_CF);\r\nif (cpu_has_guestctl0ext)\r\nset_c0_guestctl0ext(MIPS_GCTL0EXT_CGI);\r\nif (cpu_has_guestid) {\r\nwrite_c0_guestctl1(0);\r\nkvm_vz_local_flush_roottlb_all_guests();\r\nGUESTID_MASK = current_cpu_data.guestid_mask;\r\nGUESTID_FIRST_VERSION = GUESTID_MASK + 1;\r\nGUESTID_VERSION_MASK = ~GUESTID_MASK;\r\ncurrent_cpu_data.guestid_cache = GUESTID_FIRST_VERSION;\r\n}\r\nif (cpu_has_guestctl2)\r\nclear_c0_guestctl2(0x3f << 10);\r\nreturn 0;\r\n}\r\nstatic void kvm_vz_hardware_disable(void)\r\n{\r\nu64 cvmvmconfig;\r\nunsigned int mmu_size;\r\nkvm_vz_local_flush_guesttlb_all();\r\nswitch (current_cpu_type()) {\r\ncase CPU_CAVIUM_OCTEON3:\r\ncvmvmconfig = read_c0_cvmvmconfig();\r\nmmu_size = ((cvmvmconfig & CVMVMCONF_MMUSIZEM1)\r\n>> CVMVMCONF_MMUSIZEM1_S) + 1;\r\ncvmvmconfig &= ~CVMVMCONF_RMMUSIZEM1;\r\ncvmvmconfig |= mmu_size - 1;\r\nwrite_c0_cvmvmconfig(cvmvmconfig);\r\ncurrent_cpu_data.tlbsize = mmu_size;\r\ncurrent_cpu_data.tlbsizevtlb = mmu_size;\r\ncurrent_cpu_data.guest.tlbsize = 0;\r\nlocal_flush_tlb_all();\r\nbreak;\r\n}\r\nif (cpu_has_guestid) {\r\nwrite_c0_guestctl1(0);\r\nkvm_vz_local_flush_roottlb_all_guests();\r\n}\r\n}\r\nstatic int kvm_vz_check_extension(struct kvm *kvm, long ext)\r\n{\r\nint r;\r\nswitch (ext) {\r\ncase KVM_CAP_MIPS_VZ:\r\nr = 1;\r\nbreak;\r\n#ifdef CONFIG_64BIT\r\ncase KVM_CAP_MIPS_64BIT:\r\nr = 2;\r\nbreak;\r\n#endif\r\ndefault:\r\nr = 0;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstatic int kvm_vz_vcpu_init(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nfor_each_possible_cpu(i)\r\nvcpu->arch.vzguestid[i] = 0;\r\nreturn 0;\r\n}\r\nstatic void kvm_vz_vcpu_uninit(struct kvm_vcpu *vcpu)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\nif (last_vcpu[cpu] == vcpu)\r\nlast_vcpu[cpu] = NULL;\r\nif (last_exec_vcpu[cpu] == vcpu)\r\nlast_exec_vcpu[cpu] = NULL;\r\n}\r\n}\r\nstatic int kvm_vz_vcpu_setup(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nunsigned long count_hz = 100*1000*1000;\r\nif (mips_hpt_frequency && mips_hpt_frequency <= NSEC_PER_SEC)\r\ncount_hz = mips_hpt_frequency;\r\nkvm_mips_init_count(vcpu, count_hz);\r\nif (cpu_has_mips_r6)\r\nkvm_write_sw_gc0_pagegrain(cop0, PG_RIE | PG_XIE | PG_IEC);\r\nif (cpu_has_mips_r6)\r\nkvm_write_sw_gc0_wired(cop0,\r\nread_gc0_wired() & MIPSR6_WIRED_LIMIT);\r\nkvm_write_sw_gc0_status(cop0, ST0_BEV | ST0_ERL);\r\nif (cpu_has_mips_r6)\r\nkvm_change_sw_gc0_status(cop0, ST0_FR, read_gc0_status());\r\nkvm_write_sw_gc0_intctl(cop0, read_gc0_intctl() &\r\n(INTCTLF_IPFDC | INTCTLF_IPPCI | INTCTLF_IPTI));\r\nkvm_write_sw_gc0_prid(cop0, boot_cpu_data.processor_id);\r\nkvm_write_sw_gc0_ebase(cop0, (s32)0x80000000 | vcpu->vcpu_id);\r\nkvm_save_gc0_config(cop0);\r\nkvm_change_sw_gc0_config(cop0, CONF_CM_CMASK,\r\n_page_cachable_default >> _CACHE_SHIFT);\r\nkvm_change_sw_gc0_config(cop0, MIPS_CONF_MT, read_c0_config());\r\nif (cpu_guest_has_conf1) {\r\nkvm_set_sw_gc0_config(cop0, MIPS_CONF_M);\r\nkvm_save_gc0_config1(cop0);\r\nkvm_clear_sw_gc0_config1(cop0, MIPS_CONF1_C2 |\r\nMIPS_CONF1_MD |\r\nMIPS_CONF1_PC |\r\nMIPS_CONF1_WR |\r\nMIPS_CONF1_CA |\r\nMIPS_CONF1_FP);\r\n}\r\nif (cpu_guest_has_conf2) {\r\nkvm_set_sw_gc0_config1(cop0, MIPS_CONF_M);\r\nkvm_save_gc0_config2(cop0);\r\n}\r\nif (cpu_guest_has_conf3) {\r\nkvm_set_sw_gc0_config2(cop0, MIPS_CONF_M);\r\nkvm_save_gc0_config3(cop0);\r\nkvm_clear_sw_gc0_config3(cop0, MIPS_CONF3_ISA_OE);\r\nkvm_clear_sw_gc0_config3(cop0, MIPS_CONF3_MSA |\r\nMIPS_CONF3_BPG |\r\nMIPS_CONF3_ULRI |\r\nMIPS_CONF3_DSP |\r\nMIPS_CONF3_CTXTC |\r\nMIPS_CONF3_ITL |\r\nMIPS_CONF3_LPA |\r\nMIPS_CONF3_VEIC |\r\nMIPS_CONF3_VINT |\r\nMIPS_CONF3_SP |\r\nMIPS_CONF3_CDMM |\r\nMIPS_CONF3_MT |\r\nMIPS_CONF3_SM |\r\nMIPS_CONF3_TL);\r\n}\r\nif (cpu_guest_has_conf4) {\r\nkvm_set_sw_gc0_config3(cop0, MIPS_CONF_M);\r\nkvm_save_gc0_config4(cop0);\r\n}\r\nif (cpu_guest_has_conf5) {\r\nkvm_set_sw_gc0_config4(cop0, MIPS_CONF_M);\r\nkvm_save_gc0_config5(cop0);\r\nkvm_clear_sw_gc0_config5(cop0, MIPS_CONF5_K |\r\nMIPS_CONF5_CV |\r\nMIPS_CONF5_MSAEN |\r\nMIPS_CONF5_UFE |\r\nMIPS_CONF5_FRE |\r\nMIPS_CONF5_SBRI |\r\nMIPS_CONF5_UFR);\r\nkvm_clear_sw_gc0_config5(cop0, MIPS_CONF5_MRP);\r\n}\r\nif (cpu_guest_has_contextconfig) {\r\nkvm_write_sw_gc0_contextconfig(cop0, 0x007ffff0);\r\n#ifdef CONFIG_64BIT\r\nkvm_write_sw_gc0_xcontextconfig(cop0,\r\n((1ull << (cpu_vmbits - 13)) - 1) << 4);\r\n#endif\r\n}\r\nif (cpu_guest_has_segments) {\r\nkvm_write_sw_gc0_segctl0(cop0, 0x00200010);\r\nkvm_write_sw_gc0_segctl1(cop0, 0x00000002 |\r\n(_page_cachable_default >> _CACHE_SHIFT) <<\r\n(16 + MIPS_SEGCFG_C_SHIFT));\r\nkvm_write_sw_gc0_segctl2(cop0, 0x00380438);\r\n}\r\nif (cpu_guest_has_htw && cpu_has_mips_r6) {\r\nkvm_write_sw_gc0_pwfield(cop0, 0x0c30c302);\r\nkvm_write_sw_gc0_pwsize(cop0, 1 << MIPS_PWSIZE_PTW_SHIFT);\r\n}\r\nif (cpu_has_guestctl2)\r\ncop0->reg[MIPS_CP0_GUESTCTL2][MIPS_CP0_GUESTCTL2_SEL] = 0;\r\nvcpu->arch.pc = CKSEG1ADDR(0x1fc00000);\r\nreturn 0;\r\n}\r\nstatic void kvm_vz_flush_shadow_all(struct kvm *kvm)\r\n{\r\nif (cpu_has_guestid) {\r\nkvm_flush_remote_tlbs(kvm);\r\n} else {\r\ncpumask_setall(&kvm->arch.asid_flush_mask);\r\nkvm_flush_remote_tlbs(kvm);\r\n}\r\n}\r\nstatic void kvm_vz_flush_shadow_memslot(struct kvm *kvm,\r\nconst struct kvm_memory_slot *slot)\r\n{\r\nkvm_vz_flush_shadow_all(kvm);\r\n}\r\nstatic void kvm_vz_vcpu_reenter(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nint cpu = smp_processor_id();\r\nint preserve_guest_tlb;\r\npreserve_guest_tlb = kvm_vz_check_requests(vcpu, cpu);\r\nif (preserve_guest_tlb)\r\nkvm_vz_vcpu_save_wired(vcpu);\r\nkvm_vz_vcpu_load_tlb(vcpu, cpu);\r\nif (preserve_guest_tlb)\r\nkvm_vz_vcpu_load_wired(vcpu);\r\n}\r\nstatic int kvm_vz_vcpu_run(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nint cpu = smp_processor_id();\r\nint r;\r\nkvm_vz_acquire_htimer(vcpu);\r\nkvm_mips_deliver_interrupts(vcpu, read_gc0_cause());\r\nkvm_vz_check_requests(vcpu, cpu);\r\nkvm_vz_vcpu_load_tlb(vcpu, cpu);\r\nkvm_vz_vcpu_load_wired(vcpu);\r\nr = vcpu->arch.vcpu_run(run, vcpu);\r\nkvm_vz_vcpu_save_wired(vcpu);\r\nreturn r;\r\n}\r\nint kvm_mips_emulation_init(struct kvm_mips_callbacks **install_callbacks)\r\n{\r\nif (!cpu_has_vz)\r\nreturn -ENODEV;\r\nif (WARN(pgd_reg == -1,\r\n"pgd_reg not allocated even though cpu_has_vz\n"))\r\nreturn -ENODEV;\r\npr_info("Starting KVM with MIPS VZ extensions\n");\r\n*install_callbacks = &kvm_vz_callbacks;\r\nreturn 0;\r\n}
