static int mlx5_cmd_create_lag(struct mlx5_core_dev *dev, u8 remap_port1,\r\nu8 remap_port2)\r\n{\r\nu32 in[MLX5_ST_SZ_DW(create_lag_in)] = {0};\r\nu32 out[MLX5_ST_SZ_DW(create_lag_out)] = {0};\r\nvoid *lag_ctx = MLX5_ADDR_OF(create_lag_in, in, ctx);\r\nMLX5_SET(create_lag_in, in, opcode, MLX5_CMD_OP_CREATE_LAG);\r\nMLX5_SET(lagc, lag_ctx, tx_remap_affinity_1, remap_port1);\r\nMLX5_SET(lagc, lag_ctx, tx_remap_affinity_2, remap_port2);\r\nreturn mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));\r\n}\r\nstatic int mlx5_cmd_modify_lag(struct mlx5_core_dev *dev, u8 remap_port1,\r\nu8 remap_port2)\r\n{\r\nu32 in[MLX5_ST_SZ_DW(modify_lag_in)] = {0};\r\nu32 out[MLX5_ST_SZ_DW(modify_lag_out)] = {0};\r\nvoid *lag_ctx = MLX5_ADDR_OF(modify_lag_in, in, ctx);\r\nMLX5_SET(modify_lag_in, in, opcode, MLX5_CMD_OP_MODIFY_LAG);\r\nMLX5_SET(modify_lag_in, in, field_select, 0x1);\r\nMLX5_SET(lagc, lag_ctx, tx_remap_affinity_1, remap_port1);\r\nMLX5_SET(lagc, lag_ctx, tx_remap_affinity_2, remap_port2);\r\nreturn mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));\r\n}\r\nstatic int mlx5_cmd_destroy_lag(struct mlx5_core_dev *dev)\r\n{\r\nu32 in[MLX5_ST_SZ_DW(destroy_lag_in)] = {0};\r\nu32 out[MLX5_ST_SZ_DW(destroy_lag_out)] = {0};\r\nMLX5_SET(destroy_lag_in, in, opcode, MLX5_CMD_OP_DESTROY_LAG);\r\nreturn mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));\r\n}\r\nint mlx5_cmd_create_vport_lag(struct mlx5_core_dev *dev)\r\n{\r\nu32 in[MLX5_ST_SZ_DW(create_vport_lag_in)] = {0};\r\nu32 out[MLX5_ST_SZ_DW(create_vport_lag_out)] = {0};\r\nMLX5_SET(create_vport_lag_in, in, opcode, MLX5_CMD_OP_CREATE_VPORT_LAG);\r\nreturn mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));\r\n}\r\nint mlx5_cmd_destroy_vport_lag(struct mlx5_core_dev *dev)\r\n{\r\nu32 in[MLX5_ST_SZ_DW(destroy_vport_lag_in)] = {0};\r\nu32 out[MLX5_ST_SZ_DW(destroy_vport_lag_out)] = {0};\r\nMLX5_SET(destroy_vport_lag_in, in, opcode, MLX5_CMD_OP_DESTROY_VPORT_LAG);\r\nreturn mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));\r\n}\r\nstatic struct mlx5_lag *mlx5_lag_dev_get(struct mlx5_core_dev *dev)\r\n{\r\nreturn dev->priv.lag;\r\n}\r\nstatic int mlx5_lag_dev_get_netdev_idx(struct mlx5_lag *ldev,\r\nstruct net_device *ndev)\r\n{\r\nint i;\r\nfor (i = 0; i < MLX5_MAX_PORTS; i++)\r\nif (ldev->pf[i].netdev == ndev)\r\nreturn i;\r\nreturn -1;\r\n}\r\nstatic bool mlx5_lag_is_bonded(struct mlx5_lag *ldev)\r\n{\r\nreturn !!(ldev->flags & MLX5_LAG_FLAG_BONDED);\r\n}\r\nstatic void mlx5_infer_tx_affinity_mapping(struct lag_tracker *tracker,\r\nu8 *port1, u8 *port2)\r\n{\r\n*port1 = 1;\r\n*port2 = 2;\r\nif (!tracker->netdev_state[0].tx_enabled ||\r\n!tracker->netdev_state[0].link_up) {\r\n*port1 = 2;\r\nreturn;\r\n}\r\nif (!tracker->netdev_state[1].tx_enabled ||\r\n!tracker->netdev_state[1].link_up)\r\n*port2 = 1;\r\n}\r\nstatic void mlx5_activate_lag(struct mlx5_lag *ldev,\r\nstruct lag_tracker *tracker)\r\n{\r\nstruct mlx5_core_dev *dev0 = ldev->pf[0].dev;\r\nint err;\r\nldev->flags |= MLX5_LAG_FLAG_BONDED;\r\nmlx5_infer_tx_affinity_mapping(tracker, &ldev->v2p_map[0],\r\n&ldev->v2p_map[1]);\r\nerr = mlx5_cmd_create_lag(dev0, ldev->v2p_map[0], ldev->v2p_map[1]);\r\nif (err)\r\nmlx5_core_err(dev0,\r\n"Failed to create LAG (%d)\n",\r\nerr);\r\n}\r\nstatic void mlx5_deactivate_lag(struct mlx5_lag *ldev)\r\n{\r\nstruct mlx5_core_dev *dev0 = ldev->pf[0].dev;\r\nint err;\r\nldev->flags &= ~MLX5_LAG_FLAG_BONDED;\r\nerr = mlx5_cmd_destroy_lag(dev0);\r\nif (err)\r\nmlx5_core_err(dev0,\r\n"Failed to destroy LAG (%d)\n",\r\nerr);\r\n}\r\nstatic void mlx5_do_bond(struct mlx5_lag *ldev)\r\n{\r\nstruct mlx5_core_dev *dev0 = ldev->pf[0].dev;\r\nstruct mlx5_core_dev *dev1 = ldev->pf[1].dev;\r\nstruct lag_tracker tracker;\r\nu8 v2p_port1, v2p_port2;\r\nint i, err;\r\nbool do_bond;\r\nif (!dev0 || !dev1)\r\nreturn;\r\nmutex_lock(&lag_mutex);\r\ntracker = ldev->tracker;\r\nmutex_unlock(&lag_mutex);\r\ndo_bond = tracker.is_bonded && ldev->allowed;\r\nif (do_bond && !mlx5_lag_is_bonded(ldev)) {\r\nfor (i = 0; i < MLX5_MAX_PORTS; i++)\r\nmlx5_remove_dev_by_protocol(ldev->pf[i].dev,\r\nMLX5_INTERFACE_PROTOCOL_IB);\r\nmlx5_activate_lag(ldev, &tracker);\r\nmlx5_add_dev_by_protocol(dev0, MLX5_INTERFACE_PROTOCOL_IB);\r\nmlx5_nic_vport_enable_roce(dev1);\r\n} else if (do_bond && mlx5_lag_is_bonded(ldev)) {\r\nmlx5_infer_tx_affinity_mapping(&tracker, &v2p_port1,\r\n&v2p_port2);\r\nif ((v2p_port1 != ldev->v2p_map[0]) ||\r\n(v2p_port2 != ldev->v2p_map[1])) {\r\nldev->v2p_map[0] = v2p_port1;\r\nldev->v2p_map[1] = v2p_port2;\r\nerr = mlx5_cmd_modify_lag(dev0, v2p_port1, v2p_port2);\r\nif (err)\r\nmlx5_core_err(dev0,\r\n"Failed to modify LAG (%d)\n",\r\nerr);\r\n}\r\n} else if (!do_bond && mlx5_lag_is_bonded(ldev)) {\r\nmlx5_remove_dev_by_protocol(dev0, MLX5_INTERFACE_PROTOCOL_IB);\r\nmlx5_nic_vport_disable_roce(dev1);\r\nmlx5_deactivate_lag(ldev);\r\nfor (i = 0; i < MLX5_MAX_PORTS; i++)\r\nif (ldev->pf[i].dev)\r\nmlx5_add_dev_by_protocol(ldev->pf[i].dev,\r\nMLX5_INTERFACE_PROTOCOL_IB);\r\n}\r\n}\r\nstatic void mlx5_queue_bond_work(struct mlx5_lag *ldev, unsigned long delay)\r\n{\r\nschedule_delayed_work(&ldev->bond_work, delay);\r\n}\r\nstatic void mlx5_do_bond_work(struct work_struct *work)\r\n{\r\nstruct delayed_work *delayed_work = to_delayed_work(work);\r\nstruct mlx5_lag *ldev = container_of(delayed_work, struct mlx5_lag,\r\nbond_work);\r\nint status;\r\nstatus = mlx5_dev_list_trylock();\r\nif (!status) {\r\nmlx5_queue_bond_work(ldev, HZ);\r\nreturn;\r\n}\r\nmlx5_do_bond(ldev);\r\nmlx5_dev_list_unlock();\r\n}\r\nstatic int mlx5_handle_changeupper_event(struct mlx5_lag *ldev,\r\nstruct lag_tracker *tracker,\r\nstruct net_device *ndev,\r\nstruct netdev_notifier_changeupper_info *info)\r\n{\r\nstruct net_device *upper = info->upper_dev, *ndev_tmp;\r\nstruct netdev_lag_upper_info *lag_upper_info = NULL;\r\nbool is_bonded;\r\nint bond_status = 0;\r\nint num_slaves = 0;\r\nint idx;\r\nif (!netif_is_lag_master(upper))\r\nreturn 0;\r\nif (info->linking)\r\nlag_upper_info = info->upper_info;\r\nrcu_read_lock();\r\nfor_each_netdev_in_bond_rcu(upper, ndev_tmp) {\r\nidx = mlx5_lag_dev_get_netdev_idx(ldev, ndev_tmp);\r\nif (idx > -1)\r\nbond_status |= (1 << idx);\r\nnum_slaves++;\r\n}\r\nrcu_read_unlock();\r\nif (!(bond_status & 0x3))\r\nreturn 0;\r\nif (lag_upper_info)\r\ntracker->tx_type = lag_upper_info->tx_type;\r\nis_bonded = (num_slaves == MLX5_MAX_PORTS) &&\r\n(bond_status == 0x3) &&\r\n((tracker->tx_type == NETDEV_LAG_TX_TYPE_ACTIVEBACKUP) ||\r\n(tracker->tx_type == NETDEV_LAG_TX_TYPE_HASH));\r\nif (tracker->is_bonded != is_bonded) {\r\ntracker->is_bonded = is_bonded;\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mlx5_handle_changelowerstate_event(struct mlx5_lag *ldev,\r\nstruct lag_tracker *tracker,\r\nstruct net_device *ndev,\r\nstruct netdev_notifier_changelowerstate_info *info)\r\n{\r\nstruct netdev_lag_lower_state_info *lag_lower_info;\r\nint idx;\r\nif (!netif_is_lag_port(ndev))\r\nreturn 0;\r\nidx = mlx5_lag_dev_get_netdev_idx(ldev, ndev);\r\nif (idx == -1)\r\nreturn 0;\r\nlag_lower_info = info->lower_state_info;\r\nif (!lag_lower_info)\r\nreturn 0;\r\ntracker->netdev_state[idx] = *lag_lower_info;\r\nreturn 1;\r\n}\r\nstatic int mlx5_lag_netdev_event(struct notifier_block *this,\r\nunsigned long event, void *ptr)\r\n{\r\nstruct net_device *ndev = netdev_notifier_info_to_dev(ptr);\r\nstruct lag_tracker tracker;\r\nstruct mlx5_lag *ldev;\r\nint changed = 0;\r\nif (!net_eq(dev_net(ndev), &init_net))\r\nreturn NOTIFY_DONE;\r\nif ((event != NETDEV_CHANGEUPPER) && (event != NETDEV_CHANGELOWERSTATE))\r\nreturn NOTIFY_DONE;\r\nldev = container_of(this, struct mlx5_lag, nb);\r\ntracker = ldev->tracker;\r\nswitch (event) {\r\ncase NETDEV_CHANGEUPPER:\r\nchanged = mlx5_handle_changeupper_event(ldev, &tracker, ndev,\r\nptr);\r\nbreak;\r\ncase NETDEV_CHANGELOWERSTATE:\r\nchanged = mlx5_handle_changelowerstate_event(ldev, &tracker,\r\nndev, ptr);\r\nbreak;\r\n}\r\nmutex_lock(&lag_mutex);\r\nldev->tracker = tracker;\r\nmutex_unlock(&lag_mutex);\r\nif (changed)\r\nmlx5_queue_bond_work(ldev, 0);\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic bool mlx5_lag_check_prereq(struct mlx5_lag *ldev)\r\n{\r\nif ((ldev->pf[0].dev && mlx5_sriov_is_enabled(ldev->pf[0].dev)) ||\r\n(ldev->pf[1].dev && mlx5_sriov_is_enabled(ldev->pf[1].dev)))\r\nreturn false;\r\nelse\r\nreturn true;\r\n}\r\nstatic struct mlx5_lag *mlx5_lag_dev_alloc(void)\r\n{\r\nstruct mlx5_lag *ldev;\r\nldev = kzalloc(sizeof(*ldev), GFP_KERNEL);\r\nif (!ldev)\r\nreturn NULL;\r\nINIT_DELAYED_WORK(&ldev->bond_work, mlx5_do_bond_work);\r\nldev->allowed = mlx5_lag_check_prereq(ldev);\r\nreturn ldev;\r\n}\r\nstatic void mlx5_lag_dev_free(struct mlx5_lag *ldev)\r\n{\r\nkfree(ldev);\r\n}\r\nstatic void mlx5_lag_dev_add_pf(struct mlx5_lag *ldev,\r\nstruct mlx5_core_dev *dev,\r\nstruct net_device *netdev)\r\n{\r\nunsigned int fn = PCI_FUNC(dev->pdev->devfn);\r\nif (fn >= MLX5_MAX_PORTS)\r\nreturn;\r\nmutex_lock(&lag_mutex);\r\nldev->pf[fn].dev = dev;\r\nldev->pf[fn].netdev = netdev;\r\nldev->tracker.netdev_state[fn].link_up = 0;\r\nldev->tracker.netdev_state[fn].tx_enabled = 0;\r\nldev->allowed = mlx5_lag_check_prereq(ldev);\r\ndev->priv.lag = ldev;\r\nmutex_unlock(&lag_mutex);\r\n}\r\nstatic void mlx5_lag_dev_remove_pf(struct mlx5_lag *ldev,\r\nstruct mlx5_core_dev *dev)\r\n{\r\nint i;\r\nfor (i = 0; i < MLX5_MAX_PORTS; i++)\r\nif (ldev->pf[i].dev == dev)\r\nbreak;\r\nif (i == MLX5_MAX_PORTS)\r\nreturn;\r\nmutex_lock(&lag_mutex);\r\nmemset(&ldev->pf[i], 0, sizeof(*ldev->pf));\r\ndev->priv.lag = NULL;\r\nldev->allowed = mlx5_lag_check_prereq(ldev);\r\nmutex_unlock(&lag_mutex);\r\n}\r\nvoid mlx5_lag_add(struct mlx5_core_dev *dev, struct net_device *netdev)\r\n{\r\nstruct mlx5_lag *ldev = NULL;\r\nstruct mlx5_core_dev *tmp_dev;\r\nif (!MLX5_CAP_GEN(dev, vport_group_manager) ||\r\n!MLX5_CAP_GEN(dev, lag_master) ||\r\n(MLX5_CAP_GEN(dev, num_lag_ports) != MLX5_MAX_PORTS))\r\nreturn;\r\ntmp_dev = mlx5_get_next_phys_dev(dev);\r\nif (tmp_dev)\r\nldev = tmp_dev->priv.lag;\r\nif (!ldev) {\r\nldev = mlx5_lag_dev_alloc();\r\nif (!ldev) {\r\nmlx5_core_err(dev, "Failed to alloc lag dev\n");\r\nreturn;\r\n}\r\n}\r\nmlx5_lag_dev_add_pf(ldev, dev, netdev);\r\nif (!ldev->nb.notifier_call) {\r\nldev->nb.notifier_call = mlx5_lag_netdev_event;\r\nif (register_netdevice_notifier(&ldev->nb)) {\r\nldev->nb.notifier_call = NULL;\r\nmlx5_core_err(dev, "Failed to register LAG netdev notifier\n");\r\n}\r\n}\r\n}\r\nvoid mlx5_lag_remove(struct mlx5_core_dev *dev)\r\n{\r\nstruct mlx5_lag *ldev;\r\nint i;\r\nldev = mlx5_lag_dev_get(dev);\r\nif (!ldev)\r\nreturn;\r\nif (mlx5_lag_is_bonded(ldev))\r\nmlx5_deactivate_lag(ldev);\r\nmlx5_lag_dev_remove_pf(ldev, dev);\r\nfor (i = 0; i < MLX5_MAX_PORTS; i++)\r\nif (ldev->pf[i].dev)\r\nbreak;\r\nif (i == MLX5_MAX_PORTS) {\r\nif (ldev->nb.notifier_call)\r\nunregister_netdevice_notifier(&ldev->nb);\r\ncancel_delayed_work_sync(&ldev->bond_work);\r\nmlx5_lag_dev_free(ldev);\r\n}\r\n}\r\nbool mlx5_lag_is_active(struct mlx5_core_dev *dev)\r\n{\r\nstruct mlx5_lag *ldev;\r\nbool res;\r\nmutex_lock(&lag_mutex);\r\nldev = mlx5_lag_dev_get(dev);\r\nres = ldev && mlx5_lag_is_bonded(ldev);\r\nmutex_unlock(&lag_mutex);\r\nreturn res;\r\n}\r\nstatic int mlx5_lag_set_state(struct mlx5_core_dev *dev, bool allow)\r\n{\r\nstruct mlx5_lag *ldev;\r\nint ret = 0;\r\nbool lag_active;\r\nmlx5_dev_list_lock();\r\nldev = mlx5_lag_dev_get(dev);\r\nif (!ldev) {\r\nret = -ENODEV;\r\ngoto unlock;\r\n}\r\nlag_active = mlx5_lag_is_bonded(ldev);\r\nif (!mlx5_lag_check_prereq(ldev) && allow) {\r\nret = -EINVAL;\r\ngoto unlock;\r\n}\r\nif (ldev->allowed == allow)\r\ngoto unlock;\r\nldev->allowed = allow;\r\nif ((lag_active && !allow) || allow)\r\nmlx5_do_bond(ldev);\r\nunlock:\r\nmlx5_dev_list_unlock();\r\nreturn ret;\r\n}\r\nint mlx5_lag_forbid(struct mlx5_core_dev *dev)\r\n{\r\nreturn mlx5_lag_set_state(dev, false);\r\n}\r\nint mlx5_lag_allow(struct mlx5_core_dev *dev)\r\n{\r\nreturn mlx5_lag_set_state(dev, true);\r\n}\r\nstruct net_device *mlx5_lag_get_roce_netdev(struct mlx5_core_dev *dev)\r\n{\r\nstruct net_device *ndev = NULL;\r\nstruct mlx5_lag *ldev;\r\nmutex_lock(&lag_mutex);\r\nldev = mlx5_lag_dev_get(dev);\r\nif (!(ldev && mlx5_lag_is_bonded(ldev)))\r\ngoto unlock;\r\nif (ldev->tracker.tx_type == NETDEV_LAG_TX_TYPE_ACTIVEBACKUP) {\r\nndev = ldev->tracker.netdev_state[0].tx_enabled ?\r\nldev->pf[0].netdev : ldev->pf[1].netdev;\r\n} else {\r\nndev = ldev->pf[0].netdev;\r\n}\r\nif (ndev)\r\ndev_hold(ndev);\r\nunlock:\r\nmutex_unlock(&lag_mutex);\r\nreturn ndev;\r\n}\r\nbool mlx5_lag_intf_add(struct mlx5_interface *intf, struct mlx5_priv *priv)\r\n{\r\nstruct mlx5_core_dev *dev = container_of(priv, struct mlx5_core_dev,\r\npriv);\r\nstruct mlx5_lag *ldev;\r\nif (intf->protocol != MLX5_INTERFACE_PROTOCOL_IB)\r\nreturn true;\r\nldev = mlx5_lag_dev_get(dev);\r\nif (!ldev || !mlx5_lag_is_bonded(ldev) || ldev->pf[0].dev == dev)\r\nreturn true;\r\nreturn false;\r\n}
