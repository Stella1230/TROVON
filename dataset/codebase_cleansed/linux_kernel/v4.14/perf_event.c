const char *perf_pmu_name(void)\r\n{\r\nif (!metag_pmu)\r\nreturn NULL;\r\nreturn metag_pmu->name;\r\n}\r\nint perf_num_counters(void)\r\n{\r\nif (metag_pmu)\r\nreturn metag_pmu->max_events;\r\nreturn 0;\r\n}\r\nstatic inline int metag_pmu_initialised(void)\r\n{\r\nreturn !!metag_pmu;\r\n}\r\nstatic void release_pmu_hardware(void)\r\n{\r\nint irq;\r\nunsigned int version = (metag_pmu->version &\r\n(METAC_ID_MINOR_BITS | METAC_ID_REV_BITS)) >>\r\nMETAC_ID_REV_S;\r\nif (version < 0x0104)\r\nreturn;\r\nirq = internal_irq_map(17);\r\nif (irq >= 0)\r\nfree_irq(irq, (void *)1);\r\nirq = internal_irq_map(16);\r\nif (irq >= 0)\r\nfree_irq(irq, (void *)0);\r\n}\r\nstatic int reserve_pmu_hardware(void)\r\n{\r\nint err = 0, irq[2];\r\nunsigned int version = (metag_pmu->version &\r\n(METAC_ID_MINOR_BITS | METAC_ID_REV_BITS)) >>\r\nMETAC_ID_REV_S;\r\nif (version < 0x0104)\r\ngoto out;\r\nirq[0] = internal_irq_map(16);\r\nif (irq[0] < 0) {\r\npr_err("unable to map internal IRQ %d\n", 16);\r\ngoto out;\r\n}\r\nerr = request_irq(irq[0], metag_pmu->handle_irq, IRQF_NOBALANCING,\r\n"metagpmu0", (void *)0);\r\nif (err) {\r\npr_err("unable to request IRQ%d for metag PMU counters\n",\r\nirq[0]);\r\ngoto out;\r\n}\r\nirq[1] = internal_irq_map(17);\r\nif (irq[1] < 0) {\r\npr_err("unable to map internal IRQ %d\n", 17);\r\ngoto out_irq1;\r\n}\r\nerr = request_irq(irq[1], metag_pmu->handle_irq, IRQF_NOBALANCING,\r\n"metagpmu1", (void *)1);\r\nif (err) {\r\npr_err("unable to request IRQ%d for metag PMU counters\n",\r\nirq[1]);\r\ngoto out_irq1;\r\n}\r\nreturn 0;\r\nout_irq1:\r\nfree_irq(irq[0], (void *)0);\r\nout:\r\nreturn err;\r\n}\r\nstatic void metag_pmu_enable(struct pmu *pmu)\r\n{\r\n}\r\nstatic void metag_pmu_disable(struct pmu *pmu)\r\n{\r\n}\r\nstatic int metag_pmu_event_init(struct perf_event *event)\r\n{\r\nint err = 0;\r\natomic_t *active_events = &metag_pmu->active_events;\r\nif (!metag_pmu_initialised()) {\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\nif (has_branch_stack(event))\r\nreturn -EOPNOTSUPP;\r\nevent->destroy = _hw_perf_event_destroy;\r\nif (!atomic_inc_not_zero(active_events)) {\r\nmutex_lock(&metag_pmu->reserve_mutex);\r\nif (atomic_read(active_events) == 0)\r\nerr = reserve_pmu_hardware();\r\nif (!err)\r\natomic_inc(active_events);\r\nmutex_unlock(&metag_pmu->reserve_mutex);\r\n}\r\nswitch (event->attr.type) {\r\ncase PERF_TYPE_HARDWARE:\r\ncase PERF_TYPE_HW_CACHE:\r\ncase PERF_TYPE_RAW:\r\nerr = _hw_perf_event_init(event);\r\nbreak;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\nif (err)\r\nevent->destroy(event);\r\nout:\r\nreturn err;\r\n}\r\nvoid metag_pmu_event_update(struct perf_event *event,\r\nstruct hw_perf_event *hwc, int idx)\r\n{\r\nu64 prev_raw_count, new_raw_count;\r\ns64 delta;\r\nagain:\r\nprev_raw_count = local64_read(&hwc->prev_count);\r\nnew_raw_count = metag_pmu->read(idx);\r\nif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\r\nnew_raw_count) != prev_raw_count)\r\ngoto again;\r\ndelta = (new_raw_count - prev_raw_count) & MAX_PERIOD;\r\nlocal64_add(delta, &event->count);\r\nlocal64_sub(delta, &hwc->period_left);\r\n}\r\nint metag_pmu_event_set_period(struct perf_event *event,\r\nstruct hw_perf_event *hwc, int idx)\r\n{\r\ns64 left = local64_read(&hwc->period_left);\r\ns64 period = hwc->sample_period;\r\nint ret = 0;\r\nif (unlikely(period != hwc->last_period))\r\nleft += period - hwc->last_period;\r\nif (unlikely(left <= -period)) {\r\nleft = period;\r\nlocal64_set(&hwc->period_left, left);\r\nhwc->last_period = period;\r\nret = 1;\r\n}\r\nif (unlikely(left <= 0)) {\r\nleft += period;\r\nlocal64_set(&hwc->period_left, left);\r\nhwc->last_period = period;\r\nret = 1;\r\n}\r\nif (left > (s64)metag_pmu->max_period)\r\nleft = metag_pmu->max_period;\r\nif (metag_pmu->write) {\r\nlocal64_set(&hwc->prev_count, -(s32)left);\r\nmetag_pmu->write(idx, -left & MAX_PERIOD);\r\n}\r\nperf_event_update_userpage(event);\r\nreturn ret;\r\n}\r\nstatic void metag_pmu_start(struct perf_event *event, int flags)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nif (WARN_ON_ONCE(idx == -1))\r\nreturn;\r\nif (flags & PERF_EF_RELOAD)\r\nWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\r\nhwc->state = 0;\r\nif (metag_pmu->max_period)\r\nmetag_pmu_event_set_period(event, hwc, hwc->idx);\r\ncpuc->events[idx] = event;\r\nmetag_pmu->enable(hwc, idx);\r\n}\r\nstatic void metag_pmu_stop(struct perf_event *event, int flags)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (!(hwc->state & PERF_HES_STOPPED)) {\r\nmetag_pmu_event_update(event, hwc, hwc->idx);\r\nmetag_pmu->disable(hwc, hwc->idx);\r\nhwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\n}\r\n}\r\nstatic int metag_pmu_add(struct perf_event *event, int flags)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = 0, ret = 0;\r\nperf_pmu_disable(event->pmu);\r\nif (hwc->config == 0x100) {\r\nif (__test_and_set_bit(METAG_INST_COUNTER,\r\ncpuc->used_mask)) {\r\nret = -EAGAIN;\r\ngoto out;\r\n}\r\nidx = METAG_INST_COUNTER;\r\n} else {\r\nidx = find_first_zero_bit(cpuc->used_mask,\r\natomic_read(&metag_pmu->active_events));\r\nif (idx >= METAG_INST_COUNTER) {\r\nret = -EAGAIN;\r\ngoto out;\r\n}\r\n__set_bit(idx, cpuc->used_mask);\r\n}\r\nhwc->idx = idx;\r\nmetag_pmu->disable(hwc, idx);\r\nhwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\nif (flags & PERF_EF_START)\r\nmetag_pmu_start(event, PERF_EF_RELOAD);\r\nperf_event_update_userpage(event);\r\nout:\r\nperf_pmu_enable(event->pmu);\r\nreturn ret;\r\n}\r\nstatic void metag_pmu_del(struct perf_event *event, int flags)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nWARN_ON(idx < 0);\r\nmetag_pmu_stop(event, PERF_EF_UPDATE);\r\ncpuc->events[idx] = NULL;\r\n__clear_bit(idx, cpuc->used_mask);\r\nperf_event_update_userpage(event);\r\n}\r\nstatic void metag_pmu_read(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (hwc->idx < 0)\r\nreturn;\r\nmetag_pmu_event_update(event, hwc, hwc->idx);\r\n}\r\nstatic void _hw_perf_event_destroy(struct perf_event *event)\r\n{\r\natomic_t *active_events = &metag_pmu->active_events;\r\nstruct mutex *pmu_mutex = &metag_pmu->reserve_mutex;\r\nif (atomic_dec_and_mutex_lock(active_events, pmu_mutex)) {\r\nrelease_pmu_hardware();\r\nmutex_unlock(pmu_mutex);\r\n}\r\n}\r\nstatic int _hw_perf_cache_event(int config, int *evp)\r\n{\r\nunsigned long type, op, result;\r\nint ev;\r\nif (!metag_pmu->cache_events)\r\nreturn -EINVAL;\r\ntype = config & 0xff;\r\nop = (config >> 8) & 0xff;\r\nresult = (config >> 16) & 0xff;\r\nif (type >= PERF_COUNT_HW_CACHE_MAX ||\r\nop >= PERF_COUNT_HW_CACHE_OP_MAX ||\r\nresult >= PERF_COUNT_HW_CACHE_RESULT_MAX)\r\nreturn -EINVAL;\r\nev = (*metag_pmu->cache_events)[type][op][result];\r\nif (ev == 0)\r\nreturn -EOPNOTSUPP;\r\nif (ev == -1)\r\nreturn -EINVAL;\r\n*evp = ev;\r\nreturn 0;\r\n}\r\nstatic int _hw_perf_event_init(struct perf_event *event)\r\n{\r\nstruct perf_event_attr *attr = &event->attr;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint mapping = 0, err;\r\nswitch (attr->type) {\r\ncase PERF_TYPE_HARDWARE:\r\nif (attr->config >= PERF_COUNT_HW_MAX)\r\nreturn -EINVAL;\r\nmapping = metag_pmu->event_map(attr->config);\r\nbreak;\r\ncase PERF_TYPE_HW_CACHE:\r\nerr = _hw_perf_cache_event(attr->config, &mapping);\r\nif (err)\r\nreturn err;\r\nbreak;\r\ncase PERF_TYPE_RAW:\r\nmapping = attr->config;\r\nbreak;\r\n}\r\nif (mapping == -1)\r\nreturn -EINVAL;\r\nhwc->idx = -1;\r\nhwc->config |= (unsigned long)mapping;\r\nif (metag_pmu->max_period) {\r\nif (!hwc->sample_period) {\r\nhwc->sample_period = metag_pmu->max_period >> 1;\r\nhwc->last_period = hwc->sample_period;\r\nlocal64_set(&hwc->period_left, hwc->sample_period);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void metag_pmu_enable_counter(struct hw_perf_event *event, int idx)\r\n{\r\nstruct cpu_hw_events *events = this_cpu_ptr(&cpu_hw_events);\r\nunsigned int config = event->config;\r\nunsigned int tmp = config & 0xf0;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nif (METAG_INST_COUNTER == idx) {\r\nWARN_ONCE((config != 0x100),\r\n"invalid configuration (%d) for counter (%d)\n",\r\nconfig, idx);\r\nlocal64_set(&event->prev_count, __core_reg_get(TXTACTCYC));\r\ngoto unlock;\r\n}\r\nif (tmp) {\r\n#ifdef METAC_2_1\r\nvoid *perf_addr;\r\nswitch (tmp) {\r\ncase 0xd0:\r\nperf_addr = (void *)PERF_ICORE(idx);\r\nbreak;\r\ncase 0xf0:\r\nperf_addr = (void *)PERF_CHAN(idx);\r\nbreak;\r\ndefault:\r\nperf_addr = NULL;\r\nbreak;\r\n}\r\nif (perf_addr)\r\nmetag_out32((config & 0x0f), perf_addr);\r\n#endif\r\nconfig = tmp >> 4;\r\n}\r\ntmp = ((config & 0xf) << 28) |\r\n((1 << 24) << hard_processor_id());\r\nif (metag_pmu->max_period)\r\ntmp |= metag_in32(PERF_COUNT(idx)) & 0x00ffffff;\r\nelse\r\nlocal64_set(&event->prev_count, 0);\r\nmetag_out32(tmp, PERF_COUNT(idx));\r\nunlock:\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void metag_pmu_disable_counter(struct hw_perf_event *event, int idx)\r\n{\r\nstruct cpu_hw_events *events = this_cpu_ptr(&cpu_hw_events);\r\nunsigned int tmp = 0;\r\nunsigned long flags;\r\nif (METAG_INST_COUNTER == idx)\r\nreturn;\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\ntmp = metag_in32(PERF_COUNT(idx));\r\ntmp &= 0x00ffffff;\r\nmetag_out32(tmp, PERF_COUNT(idx));\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic u64 metag_pmu_read_counter(int idx)\r\n{\r\nu32 tmp = 0;\r\nif (METAG_INST_COUNTER == idx) {\r\ntmp = __core_reg_get(TXTACTCYC);\r\ngoto out;\r\n}\r\ntmp = metag_in32(PERF_COUNT(idx)) & 0x00ffffff;\r\nout:\r\nreturn tmp;\r\n}\r\nstatic void metag_pmu_write_counter(int idx, u32 val)\r\n{\r\nstruct cpu_hw_events *events = this_cpu_ptr(&cpu_hw_events);\r\nu32 tmp = 0;\r\nunsigned long flags;\r\nif (METAG_INST_COUNTER == idx)\r\nreturn;\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval &= 0x00ffffff;\r\ntmp = metag_in32(PERF_COUNT(idx)) & 0xff000000;\r\nval |= tmp;\r\nmetag_out32(val, PERF_COUNT(idx));\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic int metag_pmu_event_map(int idx)\r\n{\r\nreturn metag_general_events[idx];\r\n}\r\nstatic irqreturn_t metag_pmu_counter_overflow(int irq, void *dev)\r\n{\r\nint idx = (int)dev;\r\nstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\r\nstruct perf_event *event = cpuhw->events[idx];\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct pt_regs *regs = get_irq_regs();\r\nstruct perf_sample_data sampledata;\r\nunsigned long flags;\r\nu32 counter = 0;\r\n__global_lock2(flags);\r\ncounter = metag_in32(PERF_COUNT(idx));\r\nmetag_out32((counter & 0x00ffffff), PERF_COUNT(idx));\r\n__global_unlock2(flags);\r\nmetag_pmu_event_update(event, hwc, idx);\r\nperf_sample_data_init(&sampledata, 0, hwc->last_period);\r\nmetag_pmu_event_set_period(event, hwc, idx);\r\nif (!perf_event_overflow(event, &sampledata, regs)) {\r\n__global_lock2(flags);\r\ncounter = (counter & 0xff000000) |\r\n(metag_in32(PERF_COUNT(idx)) & 0x00ffffff);\r\nmetag_out32(counter, PERF_COUNT(idx));\r\n__global_unlock2(flags);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int metag_pmu_starting_cpu(unsigned int cpu)\r\n{\r\nstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\r\nmemset(cpuc, 0, sizeof(struct cpu_hw_events));\r\nraw_spin_lock_init(&cpuc->pmu_lock);\r\nreturn 0;\r\n}\r\nstatic int __init init_hw_perf_events(void)\r\n{\r\nint ret = 0, cpu;\r\nu32 version = *(u32 *)METAC_ID;\r\nint major = (version & METAC_ID_MAJOR_BITS) >> METAC_ID_MAJOR_S;\r\nint min_rev = (version & (METAC_ID_MINOR_BITS | METAC_ID_REV_BITS))\r\n>> METAC_ID_REV_S;\r\nif (0x02 > major) {\r\npr_info("no hardware counter support available\n");\r\ngoto out;\r\n} else if (0x02 == major) {\r\nmetag_pmu = &_metag_pmu;\r\nif (min_rev < 0x0104) {\r\nmetag_pmu->handle_irq = NULL;\r\nmetag_pmu->write = NULL;\r\nmetag_pmu->max_period = 0;\r\n}\r\nmetag_pmu->name = "meta2";\r\nmetag_pmu->version = version;\r\nmetag_pmu->pmu = pmu;\r\n}\r\npr_info("enabled with %s PMU driver, %d counters available\n",\r\nmetag_pmu->name, metag_pmu->max_events);\r\nif (metag_pmu->max_period == 0) {\r\nmetag_pmu->pmu.capabilities |= PERF_PMU_CAP_NO_INTERRUPT;\r\n}\r\natomic_set(&metag_pmu->active_events, 0);\r\nmutex_init(&metag_pmu->reserve_mutex);\r\nmetag_out32(0, PERF_COUNT(0));\r\nmetag_out32(0, PERF_COUNT(1));\r\ncpuhp_setup_state(CPUHP_AP_PERF_METAG_STARTING,\r\n"perf/metag:starting", metag_pmu_starting_cpu,\r\nNULL);\r\nret = perf_pmu_register(&pmu, metag_pmu->name, PERF_TYPE_RAW);\r\nif (ret)\r\ncpuhp_remove_state_nocalls(CPUHP_AP_PERF_METAG_STARTING);\r\nreturn ret;\r\n}
