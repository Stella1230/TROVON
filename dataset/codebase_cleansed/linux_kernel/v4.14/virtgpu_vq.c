void virtio_gpu_resource_id_get(struct virtio_gpu_device *vgdev,\r\nuint32_t *resid)\r\n{\r\nint handle;\r\nidr_preload(GFP_KERNEL);\r\nspin_lock(&vgdev->resource_idr_lock);\r\nhandle = idr_alloc(&vgdev->resource_idr, NULL, 1, 0, GFP_NOWAIT);\r\nspin_unlock(&vgdev->resource_idr_lock);\r\nidr_preload_end();\r\n*resid = handle;\r\n}\r\nvoid virtio_gpu_resource_id_put(struct virtio_gpu_device *vgdev, uint32_t id)\r\n{\r\nspin_lock(&vgdev->resource_idr_lock);\r\nidr_remove(&vgdev->resource_idr, id);\r\nspin_unlock(&vgdev->resource_idr_lock);\r\n}\r\nvoid virtio_gpu_ctrl_ack(struct virtqueue *vq)\r\n{\r\nstruct drm_device *dev = vq->vdev->priv;\r\nstruct virtio_gpu_device *vgdev = dev->dev_private;\r\nschedule_work(&vgdev->ctrlq.dequeue_work);\r\n}\r\nvoid virtio_gpu_cursor_ack(struct virtqueue *vq)\r\n{\r\nstruct drm_device *dev = vq->vdev->priv;\r\nstruct virtio_gpu_device *vgdev = dev->dev_private;\r\nschedule_work(&vgdev->cursorq.dequeue_work);\r\n}\r\nint virtio_gpu_alloc_vbufs(struct virtio_gpu_device *vgdev)\r\n{\r\nvgdev->vbufs = kmem_cache_create("virtio-gpu-vbufs",\r\nVBUFFER_SIZE,\r\n__alignof__(struct virtio_gpu_vbuffer),\r\n0, NULL);\r\nif (!vgdev->vbufs)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid virtio_gpu_free_vbufs(struct virtio_gpu_device *vgdev)\r\n{\r\nkmem_cache_destroy(vgdev->vbufs);\r\nvgdev->vbufs = NULL;\r\n}\r\nstatic struct virtio_gpu_vbuffer*\r\nvirtio_gpu_get_vbuf(struct virtio_gpu_device *vgdev,\r\nint size, int resp_size, void *resp_buf,\r\nvirtio_gpu_resp_cb resp_cb)\r\n{\r\nstruct virtio_gpu_vbuffer *vbuf;\r\nvbuf = kmem_cache_alloc(vgdev->vbufs, GFP_KERNEL);\r\nif (!vbuf)\r\nreturn ERR_PTR(-ENOMEM);\r\nmemset(vbuf, 0, VBUFFER_SIZE);\r\nBUG_ON(size > MAX_INLINE_CMD_SIZE);\r\nvbuf->buf = (void *)vbuf + sizeof(*vbuf);\r\nvbuf->size = size;\r\nvbuf->resp_cb = resp_cb;\r\nvbuf->resp_size = resp_size;\r\nif (resp_size <= MAX_INLINE_RESP_SIZE)\r\nvbuf->resp_buf = (void *)vbuf->buf + size;\r\nelse\r\nvbuf->resp_buf = resp_buf;\r\nBUG_ON(!vbuf->resp_buf);\r\nreturn vbuf;\r\n}\r\nstatic void *virtio_gpu_alloc_cmd(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer **vbuffer_p,\r\nint size)\r\n{\r\nstruct virtio_gpu_vbuffer *vbuf;\r\nvbuf = virtio_gpu_get_vbuf(vgdev, size,\r\nsizeof(struct virtio_gpu_ctrl_hdr),\r\nNULL, NULL);\r\nif (IS_ERR(vbuf)) {\r\n*vbuffer_p = NULL;\r\nreturn ERR_CAST(vbuf);\r\n}\r\n*vbuffer_p = vbuf;\r\nreturn vbuf->buf;\r\n}\r\nstatic struct virtio_gpu_update_cursor*\r\nvirtio_gpu_alloc_cursor(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer **vbuffer_p)\r\n{\r\nstruct virtio_gpu_vbuffer *vbuf;\r\nvbuf = virtio_gpu_get_vbuf\r\n(vgdev, sizeof(struct virtio_gpu_update_cursor),\r\n0, NULL, NULL);\r\nif (IS_ERR(vbuf)) {\r\n*vbuffer_p = NULL;\r\nreturn ERR_CAST(vbuf);\r\n}\r\n*vbuffer_p = vbuf;\r\nreturn (struct virtio_gpu_update_cursor *)vbuf->buf;\r\n}\r\nstatic void *virtio_gpu_alloc_cmd_resp(struct virtio_gpu_device *vgdev,\r\nvirtio_gpu_resp_cb cb,\r\nstruct virtio_gpu_vbuffer **vbuffer_p,\r\nint cmd_size, int resp_size,\r\nvoid *resp_buf)\r\n{\r\nstruct virtio_gpu_vbuffer *vbuf;\r\nvbuf = virtio_gpu_get_vbuf(vgdev, cmd_size,\r\nresp_size, resp_buf, cb);\r\nif (IS_ERR(vbuf)) {\r\n*vbuffer_p = NULL;\r\nreturn ERR_CAST(vbuf);\r\n}\r\n*vbuffer_p = vbuf;\r\nreturn (struct virtio_gpu_command *)vbuf->buf;\r\n}\r\nstatic void free_vbuf(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer *vbuf)\r\n{\r\nif (vbuf->resp_size > MAX_INLINE_RESP_SIZE)\r\nkfree(vbuf->resp_buf);\r\nkfree(vbuf->data_buf);\r\nkmem_cache_free(vgdev->vbufs, vbuf);\r\n}\r\nstatic void reclaim_vbufs(struct virtqueue *vq, struct list_head *reclaim_list)\r\n{\r\nstruct virtio_gpu_vbuffer *vbuf;\r\nunsigned int len;\r\nint freed = 0;\r\nwhile ((vbuf = virtqueue_get_buf(vq, &len))) {\r\nlist_add_tail(&vbuf->list, reclaim_list);\r\nfreed++;\r\n}\r\nif (freed == 0)\r\nDRM_DEBUG("Huh? zero vbufs reclaimed");\r\n}\r\nvoid virtio_gpu_dequeue_ctrl_func(struct work_struct *work)\r\n{\r\nstruct virtio_gpu_device *vgdev =\r\ncontainer_of(work, struct virtio_gpu_device,\r\nctrlq.dequeue_work);\r\nstruct list_head reclaim_list;\r\nstruct virtio_gpu_vbuffer *entry, *tmp;\r\nstruct virtio_gpu_ctrl_hdr *resp;\r\nu64 fence_id = 0;\r\nINIT_LIST_HEAD(&reclaim_list);\r\nspin_lock(&vgdev->ctrlq.qlock);\r\ndo {\r\nvirtqueue_disable_cb(vgdev->ctrlq.vq);\r\nreclaim_vbufs(vgdev->ctrlq.vq, &reclaim_list);\r\n} while (!virtqueue_enable_cb(vgdev->ctrlq.vq));\r\nspin_unlock(&vgdev->ctrlq.qlock);\r\nlist_for_each_entry_safe(entry, tmp, &reclaim_list, list) {\r\nresp = (struct virtio_gpu_ctrl_hdr *)entry->resp_buf;\r\nif (resp->type != cpu_to_le32(VIRTIO_GPU_RESP_OK_NODATA))\r\nDRM_DEBUG("response 0x%x\n", le32_to_cpu(resp->type));\r\nif (resp->flags & cpu_to_le32(VIRTIO_GPU_FLAG_FENCE)) {\r\nu64 f = le64_to_cpu(resp->fence_id);\r\nif (fence_id > f) {\r\nDRM_ERROR("%s: Oops: fence %llx -> %llx\n",\r\n__func__, fence_id, f);\r\n} else {\r\nfence_id = f;\r\n}\r\n}\r\nif (entry->resp_cb)\r\nentry->resp_cb(vgdev, entry);\r\nlist_del(&entry->list);\r\nfree_vbuf(vgdev, entry);\r\n}\r\nwake_up(&vgdev->ctrlq.ack_queue);\r\nif (fence_id)\r\nvirtio_gpu_fence_event_process(vgdev, fence_id);\r\n}\r\nvoid virtio_gpu_dequeue_cursor_func(struct work_struct *work)\r\n{\r\nstruct virtio_gpu_device *vgdev =\r\ncontainer_of(work, struct virtio_gpu_device,\r\ncursorq.dequeue_work);\r\nstruct list_head reclaim_list;\r\nstruct virtio_gpu_vbuffer *entry, *tmp;\r\nINIT_LIST_HEAD(&reclaim_list);\r\nspin_lock(&vgdev->cursorq.qlock);\r\ndo {\r\nvirtqueue_disable_cb(vgdev->cursorq.vq);\r\nreclaim_vbufs(vgdev->cursorq.vq, &reclaim_list);\r\n} while (!virtqueue_enable_cb(vgdev->cursorq.vq));\r\nspin_unlock(&vgdev->cursorq.qlock);\r\nlist_for_each_entry_safe(entry, tmp, &reclaim_list, list) {\r\nlist_del(&entry->list);\r\nfree_vbuf(vgdev, entry);\r\n}\r\nwake_up(&vgdev->cursorq.ack_queue);\r\n}\r\nstatic int virtio_gpu_queue_ctrl_buffer_locked(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer *vbuf)\r\n__releases(&vgdev->ctrlq.qlock\r\nstatic int virtio_gpu_queue_ctrl_buffer(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer *vbuf)\r\n{\r\nint rc;\r\nspin_lock(&vgdev->ctrlq.qlock);\r\nrc = virtio_gpu_queue_ctrl_buffer_locked(vgdev, vbuf);\r\nspin_unlock(&vgdev->ctrlq.qlock);\r\nreturn rc;\r\n}\r\nstatic int virtio_gpu_queue_fenced_ctrl_buffer(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer *vbuf,\r\nstruct virtio_gpu_ctrl_hdr *hdr,\r\nstruct virtio_gpu_fence **fence)\r\n{\r\nstruct virtqueue *vq = vgdev->ctrlq.vq;\r\nint rc;\r\nagain:\r\nspin_lock(&vgdev->ctrlq.qlock);\r\nif (vq->num_free < 3) {\r\nspin_unlock(&vgdev->ctrlq.qlock);\r\nwait_event(vgdev->ctrlq.ack_queue, vq->num_free >= 3);\r\ngoto again;\r\n}\r\nif (fence)\r\nvirtio_gpu_fence_emit(vgdev, hdr, fence);\r\nrc = virtio_gpu_queue_ctrl_buffer_locked(vgdev, vbuf);\r\nspin_unlock(&vgdev->ctrlq.qlock);\r\nreturn rc;\r\n}\r\nstatic int virtio_gpu_queue_cursor(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer *vbuf)\r\n{\r\nstruct virtqueue *vq = vgdev->cursorq.vq;\r\nstruct scatterlist *sgs[1], ccmd;\r\nint ret;\r\nint outcnt;\r\nif (!vgdev->vqs_ready)\r\nreturn -ENODEV;\r\nsg_init_one(&ccmd, vbuf->buf, vbuf->size);\r\nsgs[0] = &ccmd;\r\noutcnt = 1;\r\nspin_lock(&vgdev->cursorq.qlock);\r\nretry:\r\nret = virtqueue_add_sgs(vq, sgs, outcnt, 0, vbuf, GFP_ATOMIC);\r\nif (ret == -ENOSPC) {\r\nspin_unlock(&vgdev->cursorq.qlock);\r\nwait_event(vgdev->cursorq.ack_queue, vq->num_free);\r\nspin_lock(&vgdev->cursorq.qlock);\r\ngoto retry;\r\n} else {\r\nvirtqueue_kick(vq);\r\n}\r\nspin_unlock(&vgdev->cursorq.qlock);\r\nif (!ret)\r\nret = vq->num_free;\r\nreturn ret;\r\n}\r\nvoid virtio_gpu_cmd_create_resource(struct virtio_gpu_device *vgdev,\r\nuint32_t resource_id,\r\nuint32_t format,\r\nuint32_t width,\r\nuint32_t height)\r\n{\r\nstruct virtio_gpu_resource_create_2d *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_CREATE_2D);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\ncmd_p->format = cpu_to_le32(format);\r\ncmd_p->width = cpu_to_le32(width);\r\ncmd_p->height = cpu_to_le32(height);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\n}\r\nvoid virtio_gpu_cmd_unref_resource(struct virtio_gpu_device *vgdev,\r\nuint32_t resource_id)\r\n{\r\nstruct virtio_gpu_resource_unref *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_UNREF);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\n}\r\nvoid virtio_gpu_cmd_resource_inval_backing(struct virtio_gpu_device *vgdev,\r\nuint32_t resource_id)\r\n{\r\nstruct virtio_gpu_resource_detach_backing *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_DETACH_BACKING);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\n}\r\nvoid virtio_gpu_cmd_set_scanout(struct virtio_gpu_device *vgdev,\r\nuint32_t scanout_id, uint32_t resource_id,\r\nuint32_t width, uint32_t height,\r\nuint32_t x, uint32_t y)\r\n{\r\nstruct virtio_gpu_set_scanout *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_SET_SCANOUT);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\ncmd_p->scanout_id = cpu_to_le32(scanout_id);\r\ncmd_p->r.width = cpu_to_le32(width);\r\ncmd_p->r.height = cpu_to_le32(height);\r\ncmd_p->r.x = cpu_to_le32(x);\r\ncmd_p->r.y = cpu_to_le32(y);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\n}\r\nvoid virtio_gpu_cmd_resource_flush(struct virtio_gpu_device *vgdev,\r\nuint32_t resource_id,\r\nuint32_t x, uint32_t y,\r\nuint32_t width, uint32_t height)\r\n{\r\nstruct virtio_gpu_resource_flush *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_FLUSH);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\ncmd_p->r.width = cpu_to_le32(width);\r\ncmd_p->r.height = cpu_to_le32(height);\r\ncmd_p->r.x = cpu_to_le32(x);\r\ncmd_p->r.y = cpu_to_le32(y);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\n}\r\nvoid virtio_gpu_cmd_transfer_to_host_2d(struct virtio_gpu_device *vgdev,\r\nuint32_t resource_id, uint64_t offset,\r\n__le32 width, __le32 height,\r\n__le32 x, __le32 y,\r\nstruct virtio_gpu_fence **fence)\r\n{\r\nstruct virtio_gpu_transfer_to_host_2d *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_TRANSFER_TO_HOST_2D);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\ncmd_p->offset = cpu_to_le64(offset);\r\ncmd_p->r.width = width;\r\ncmd_p->r.height = height;\r\ncmd_p->r.x = x;\r\ncmd_p->r.y = y;\r\nvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, &cmd_p->hdr, fence);\r\n}\r\nstatic void\r\nvirtio_gpu_cmd_resource_attach_backing(struct virtio_gpu_device *vgdev,\r\nuint32_t resource_id,\r\nstruct virtio_gpu_mem_entry *ents,\r\nuint32_t nents,\r\nstruct virtio_gpu_fence **fence)\r\n{\r\nstruct virtio_gpu_resource_attach_backing *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_ATTACH_BACKING);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\ncmd_p->nr_entries = cpu_to_le32(nents);\r\nvbuf->data_buf = ents;\r\nvbuf->data_size = sizeof(*ents) * nents;\r\nvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, &cmd_p->hdr, fence);\r\n}\r\nstatic void virtio_gpu_cmd_get_display_info_cb(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer *vbuf)\r\n{\r\nstruct virtio_gpu_resp_display_info *resp =\r\n(struct virtio_gpu_resp_display_info *)vbuf->resp_buf;\r\nint i;\r\nspin_lock(&vgdev->display_info_lock);\r\nfor (i = 0; i < vgdev->num_scanouts; i++) {\r\nvgdev->outputs[i].info = resp->pmodes[i];\r\nif (resp->pmodes[i].enabled) {\r\nDRM_DEBUG("output %d: %dx%d+%d+%d", i,\r\nle32_to_cpu(resp->pmodes[i].r.width),\r\nle32_to_cpu(resp->pmodes[i].r.height),\r\nle32_to_cpu(resp->pmodes[i].r.x),\r\nle32_to_cpu(resp->pmodes[i].r.y));\r\n} else {\r\nDRM_DEBUG("output %d: disabled", i);\r\n}\r\n}\r\nvgdev->display_info_pending = false;\r\nspin_unlock(&vgdev->display_info_lock);\r\nwake_up(&vgdev->resp_wq);\r\nif (!drm_helper_hpd_irq_event(vgdev->ddev))\r\ndrm_kms_helper_hotplug_event(vgdev->ddev);\r\n}\r\nstatic void virtio_gpu_cmd_get_capset_info_cb(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer *vbuf)\r\n{\r\nstruct virtio_gpu_get_capset_info *cmd =\r\n(struct virtio_gpu_get_capset_info *)vbuf->buf;\r\nstruct virtio_gpu_resp_capset_info *resp =\r\n(struct virtio_gpu_resp_capset_info *)vbuf->resp_buf;\r\nint i = le32_to_cpu(cmd->capset_index);\r\nspin_lock(&vgdev->display_info_lock);\r\nvgdev->capsets[i].id = le32_to_cpu(resp->capset_id);\r\nvgdev->capsets[i].max_version = le32_to_cpu(resp->capset_max_version);\r\nvgdev->capsets[i].max_size = le32_to_cpu(resp->capset_max_size);\r\nspin_unlock(&vgdev->display_info_lock);\r\nwake_up(&vgdev->resp_wq);\r\n}\r\nstatic void virtio_gpu_cmd_capset_cb(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_vbuffer *vbuf)\r\n{\r\nstruct virtio_gpu_get_capset *cmd =\r\n(struct virtio_gpu_get_capset *)vbuf->buf;\r\nstruct virtio_gpu_resp_capset *resp =\r\n(struct virtio_gpu_resp_capset *)vbuf->resp_buf;\r\nstruct virtio_gpu_drv_cap_cache *cache_ent;\r\nspin_lock(&vgdev->display_info_lock);\r\nlist_for_each_entry(cache_ent, &vgdev->cap_cache, head) {\r\nif (cache_ent->version == le32_to_cpu(cmd->capset_version) &&\r\ncache_ent->id == le32_to_cpu(cmd->capset_id)) {\r\nmemcpy(cache_ent->caps_cache, resp->capset_data,\r\ncache_ent->size);\r\natomic_set(&cache_ent->is_valid, 1);\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&vgdev->display_info_lock);\r\nwake_up(&vgdev->resp_wq);\r\n}\r\nint virtio_gpu_cmd_get_display_info(struct virtio_gpu_device *vgdev)\r\n{\r\nstruct virtio_gpu_ctrl_hdr *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\nvoid *resp_buf;\r\nresp_buf = kzalloc(sizeof(struct virtio_gpu_resp_display_info),\r\nGFP_KERNEL);\r\nif (!resp_buf)\r\nreturn -ENOMEM;\r\ncmd_p = virtio_gpu_alloc_cmd_resp\r\n(vgdev, &virtio_gpu_cmd_get_display_info_cb, &vbuf,\r\nsizeof(*cmd_p), sizeof(struct virtio_gpu_resp_display_info),\r\nresp_buf);\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\nvgdev->display_info_pending = true;\r\ncmd_p->type = cpu_to_le32(VIRTIO_GPU_CMD_GET_DISPLAY_INFO);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\nreturn 0;\r\n}\r\nint virtio_gpu_cmd_get_capset_info(struct virtio_gpu_device *vgdev, int idx)\r\n{\r\nstruct virtio_gpu_get_capset_info *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\nvoid *resp_buf;\r\nresp_buf = kzalloc(sizeof(struct virtio_gpu_resp_capset_info),\r\nGFP_KERNEL);\r\nif (!resp_buf)\r\nreturn -ENOMEM;\r\ncmd_p = virtio_gpu_alloc_cmd_resp\r\n(vgdev, &virtio_gpu_cmd_get_capset_info_cb, &vbuf,\r\nsizeof(*cmd_p), sizeof(struct virtio_gpu_resp_capset_info),\r\nresp_buf);\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_GET_CAPSET_INFO);\r\ncmd_p->capset_index = cpu_to_le32(idx);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\nreturn 0;\r\n}\r\nint virtio_gpu_cmd_get_capset(struct virtio_gpu_device *vgdev,\r\nint idx, int version,\r\nstruct virtio_gpu_drv_cap_cache **cache_p)\r\n{\r\nstruct virtio_gpu_get_capset *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\nint max_size = vgdev->capsets[idx].max_size;\r\nstruct virtio_gpu_drv_cap_cache *cache_ent;\r\nvoid *resp_buf;\r\nif (idx > vgdev->num_capsets)\r\nreturn -EINVAL;\r\nif (version > vgdev->capsets[idx].max_version)\r\nreturn -EINVAL;\r\ncache_ent = kzalloc(sizeof(*cache_ent), GFP_KERNEL);\r\nif (!cache_ent)\r\nreturn -ENOMEM;\r\ncache_ent->caps_cache = kmalloc(max_size, GFP_KERNEL);\r\nif (!cache_ent->caps_cache) {\r\nkfree(cache_ent);\r\nreturn -ENOMEM;\r\n}\r\nresp_buf = kzalloc(sizeof(struct virtio_gpu_resp_capset) + max_size,\r\nGFP_KERNEL);\r\nif (!resp_buf) {\r\nkfree(cache_ent->caps_cache);\r\nkfree(cache_ent);\r\nreturn -ENOMEM;\r\n}\r\ncache_ent->version = version;\r\ncache_ent->id = vgdev->capsets[idx].id;\r\natomic_set(&cache_ent->is_valid, 0);\r\ncache_ent->size = max_size;\r\nspin_lock(&vgdev->display_info_lock);\r\nlist_add_tail(&cache_ent->head, &vgdev->cap_cache);\r\nspin_unlock(&vgdev->display_info_lock);\r\ncmd_p = virtio_gpu_alloc_cmd_resp\r\n(vgdev, &virtio_gpu_cmd_capset_cb, &vbuf, sizeof(*cmd_p),\r\nsizeof(struct virtio_gpu_resp_capset) + max_size,\r\nresp_buf);\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_GET_CAPSET);\r\ncmd_p->capset_id = cpu_to_le32(vgdev->capsets[idx].id);\r\ncmd_p->capset_version = cpu_to_le32(version);\r\n*cache_p = cache_ent;\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\nreturn 0;\r\n}\r\nvoid virtio_gpu_cmd_context_create(struct virtio_gpu_device *vgdev, uint32_t id,\r\nuint32_t nlen, const char *name)\r\n{\r\nstruct virtio_gpu_ctx_create *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_CTX_CREATE);\r\ncmd_p->hdr.ctx_id = cpu_to_le32(id);\r\ncmd_p->nlen = cpu_to_le32(nlen);\r\nstrncpy(cmd_p->debug_name, name, sizeof(cmd_p->debug_name)-1);\r\ncmd_p->debug_name[sizeof(cmd_p->debug_name)-1] = 0;\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\n}\r\nvoid virtio_gpu_cmd_context_destroy(struct virtio_gpu_device *vgdev,\r\nuint32_t id)\r\n{\r\nstruct virtio_gpu_ctx_destroy *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_CTX_DESTROY);\r\ncmd_p->hdr.ctx_id = cpu_to_le32(id);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\n}\r\nvoid virtio_gpu_cmd_context_attach_resource(struct virtio_gpu_device *vgdev,\r\nuint32_t ctx_id,\r\nuint32_t resource_id)\r\n{\r\nstruct virtio_gpu_ctx_resource *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_CTX_ATTACH_RESOURCE);\r\ncmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\n}\r\nvoid virtio_gpu_cmd_context_detach_resource(struct virtio_gpu_device *vgdev,\r\nuint32_t ctx_id,\r\nuint32_t resource_id)\r\n{\r\nstruct virtio_gpu_ctx_resource *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_CTX_DETACH_RESOURCE);\r\ncmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\nvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\r\n}\r\nvoid\r\nvirtio_gpu_cmd_resource_create_3d(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_resource_create_3d *rc_3d,\r\nstruct virtio_gpu_fence **fence)\r\n{\r\nstruct virtio_gpu_resource_create_3d *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\n*cmd_p = *rc_3d;\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_CREATE_3D);\r\ncmd_p->hdr.flags = 0;\r\nvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, &cmd_p->hdr, fence);\r\n}\r\nvoid virtio_gpu_cmd_transfer_to_host_3d(struct virtio_gpu_device *vgdev,\r\nuint32_t resource_id, uint32_t ctx_id,\r\nuint64_t offset, uint32_t level,\r\nstruct virtio_gpu_box *box,\r\nstruct virtio_gpu_fence **fence)\r\n{\r\nstruct virtio_gpu_transfer_host_3d *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_TRANSFER_TO_HOST_3D);\r\ncmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\ncmd_p->box = *box;\r\ncmd_p->offset = cpu_to_le64(offset);\r\ncmd_p->level = cpu_to_le32(level);\r\nvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, &cmd_p->hdr, fence);\r\n}\r\nvoid virtio_gpu_cmd_transfer_from_host_3d(struct virtio_gpu_device *vgdev,\r\nuint32_t resource_id, uint32_t ctx_id,\r\nuint64_t offset, uint32_t level,\r\nstruct virtio_gpu_box *box,\r\nstruct virtio_gpu_fence **fence)\r\n{\r\nstruct virtio_gpu_transfer_host_3d *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_TRANSFER_FROM_HOST_3D);\r\ncmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\r\ncmd_p->resource_id = cpu_to_le32(resource_id);\r\ncmd_p->box = *box;\r\ncmd_p->offset = cpu_to_le64(offset);\r\ncmd_p->level = cpu_to_le32(level);\r\nvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, &cmd_p->hdr, fence);\r\n}\r\nvoid virtio_gpu_cmd_submit(struct virtio_gpu_device *vgdev,\r\nvoid *data, uint32_t data_size,\r\nuint32_t ctx_id, struct virtio_gpu_fence **fence)\r\n{\r\nstruct virtio_gpu_cmd_submit *cmd_p;\r\nstruct virtio_gpu_vbuffer *vbuf;\r\ncmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\r\nmemset(cmd_p, 0, sizeof(*cmd_p));\r\nvbuf->data_buf = data;\r\nvbuf->data_size = data_size;\r\ncmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_SUBMIT_3D);\r\ncmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\r\ncmd_p->size = cpu_to_le32(data_size);\r\nvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, &cmd_p->hdr, fence);\r\n}\r\nint virtio_gpu_object_attach(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_object *obj,\r\nuint32_t resource_id,\r\nstruct virtio_gpu_fence **fence)\r\n{\r\nstruct virtio_gpu_mem_entry *ents;\r\nstruct scatterlist *sg;\r\nint si;\r\nif (!obj->pages) {\r\nint ret;\r\nret = virtio_gpu_object_get_sg_table(vgdev, obj);\r\nif (ret)\r\nreturn ret;\r\n}\r\nents = kmalloc_array(obj->pages->nents,\r\nsizeof(struct virtio_gpu_mem_entry),\r\nGFP_KERNEL);\r\nif (!ents) {\r\nDRM_ERROR("failed to allocate ent list\n");\r\nreturn -ENOMEM;\r\n}\r\nfor_each_sg(obj->pages->sgl, sg, obj->pages->nents, si) {\r\nents[si].addr = cpu_to_le64(sg_phys(sg));\r\nents[si].length = cpu_to_le32(sg->length);\r\nents[si].padding = 0;\r\n}\r\nvirtio_gpu_cmd_resource_attach_backing(vgdev, resource_id,\r\nents, obj->pages->nents,\r\nfence);\r\nobj->hw_res_handle = resource_id;\r\nreturn 0;\r\n}\r\nvoid virtio_gpu_cursor_ping(struct virtio_gpu_device *vgdev,\r\nstruct virtio_gpu_output *output)\r\n{\r\nstruct virtio_gpu_vbuffer *vbuf;\r\nstruct virtio_gpu_update_cursor *cur_p;\r\noutput->cursor.pos.scanout_id = cpu_to_le32(output->index);\r\ncur_p = virtio_gpu_alloc_cursor(vgdev, &vbuf);\r\nmemcpy(cur_p, &output->cursor, sizeof(output->cursor));\r\nvirtio_gpu_queue_cursor(vgdev, vbuf);\r\n}
