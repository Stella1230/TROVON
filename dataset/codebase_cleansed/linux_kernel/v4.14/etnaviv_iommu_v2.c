static struct etnaviv_iommuv2_domain *to_etnaviv_domain(struct iommu_domain *domain)\r\n{\r\nreturn container_of(domain, struct etnaviv_iommuv2_domain, domain);\r\n}\r\nstatic int etnaviv_iommuv2_map(struct iommu_domain *domain, unsigned long iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nstruct etnaviv_iommuv2_domain *etnaviv_domain =\r\nto_etnaviv_domain(domain);\r\nint mtlb_entry, stlb_entry;\r\nu32 entry = (u32)paddr | MMUv2_PTE_PRESENT;\r\nif (size != SZ_4K)\r\nreturn -EINVAL;\r\nif (prot & IOMMU_WRITE)\r\nentry |= MMUv2_PTE_WRITEABLE;\r\nmtlb_entry = (iova & MMUv2_MTLB_MASK) >> MMUv2_MTLB_SHIFT;\r\nstlb_entry = (iova & MMUv2_STLB_MASK) >> MMUv2_STLB_SHIFT;\r\netnaviv_domain->stlb_cpu[mtlb_entry][stlb_entry] = entry;\r\nreturn 0;\r\n}\r\nstatic size_t etnaviv_iommuv2_unmap(struct iommu_domain *domain,\r\nunsigned long iova, size_t size)\r\n{\r\nstruct etnaviv_iommuv2_domain *etnaviv_domain =\r\nto_etnaviv_domain(domain);\r\nint mtlb_entry, stlb_entry;\r\nif (size != SZ_4K)\r\nreturn -EINVAL;\r\nmtlb_entry = (iova & MMUv2_MTLB_MASK) >> MMUv2_MTLB_SHIFT;\r\nstlb_entry = (iova & MMUv2_STLB_MASK) >> MMUv2_STLB_SHIFT;\r\netnaviv_domain->stlb_cpu[mtlb_entry][stlb_entry] = MMUv2_PTE_EXCEPTION;\r\nreturn SZ_4K;\r\n}\r\nstatic phys_addr_t etnaviv_iommuv2_iova_to_phys(struct iommu_domain *domain,\r\ndma_addr_t iova)\r\n{\r\nstruct etnaviv_iommuv2_domain *etnaviv_domain =\r\nto_etnaviv_domain(domain);\r\nint mtlb_entry, stlb_entry;\r\nmtlb_entry = (iova & MMUv2_MTLB_MASK) >> MMUv2_MTLB_SHIFT;\r\nstlb_entry = (iova & MMUv2_STLB_MASK) >> MMUv2_STLB_SHIFT;\r\nreturn etnaviv_domain->stlb_cpu[mtlb_entry][stlb_entry] & ~(SZ_4K - 1);\r\n}\r\nstatic int etnaviv_iommuv2_init(struct etnaviv_iommuv2_domain *etnaviv_domain)\r\n{\r\nu32 *p;\r\nint ret, i, j;\r\netnaviv_domain->bad_page_cpu = dma_alloc_coherent(etnaviv_domain->dev,\r\nSZ_4K,\r\n&etnaviv_domain->bad_page_dma,\r\nGFP_KERNEL);\r\nif (!etnaviv_domain->bad_page_cpu) {\r\nret = -ENOMEM;\r\ngoto fail_mem;\r\n}\r\np = etnaviv_domain->bad_page_cpu;\r\nfor (i = 0; i < SZ_4K / 4; i++)\r\n*p++ = 0xdead55aa;\r\netnaviv_domain->mtlb_cpu = dma_alloc_coherent(etnaviv_domain->dev,\r\nSZ_4K,\r\n&etnaviv_domain->mtlb_dma,\r\nGFP_KERNEL);\r\nif (!etnaviv_domain->mtlb_cpu) {\r\nret = -ENOMEM;\r\ngoto fail_mem;\r\n}\r\nfor (i = 0; i < MMUv2_MAX_STLB_ENTRIES; i++) {\r\netnaviv_domain->stlb_cpu[i] =\r\ndma_alloc_coherent(etnaviv_domain->dev,\r\nSZ_4K,\r\n&etnaviv_domain->stlb_dma[i],\r\nGFP_KERNEL);\r\nif (!etnaviv_domain->stlb_cpu[i]) {\r\nret = -ENOMEM;\r\ngoto fail_mem;\r\n}\r\np = etnaviv_domain->stlb_cpu[i];\r\nfor (j = 0; j < SZ_4K / 4; j++)\r\n*p++ = MMUv2_PTE_EXCEPTION;\r\netnaviv_domain->mtlb_cpu[i] = etnaviv_domain->stlb_dma[i] |\r\nMMUv2_PTE_PRESENT;\r\n}\r\nreturn 0;\r\nfail_mem:\r\nif (etnaviv_domain->bad_page_cpu)\r\ndma_free_coherent(etnaviv_domain->dev, SZ_4K,\r\netnaviv_domain->bad_page_cpu,\r\netnaviv_domain->bad_page_dma);\r\nif (etnaviv_domain->mtlb_cpu)\r\ndma_free_coherent(etnaviv_domain->dev, SZ_4K,\r\netnaviv_domain->mtlb_cpu,\r\netnaviv_domain->mtlb_dma);\r\nfor (i = 0; i < MMUv2_MAX_STLB_ENTRIES; i++) {\r\nif (etnaviv_domain->stlb_cpu[i])\r\ndma_free_coherent(etnaviv_domain->dev, SZ_4K,\r\netnaviv_domain->stlb_cpu[i],\r\netnaviv_domain->stlb_dma[i]);\r\n}\r\nreturn ret;\r\n}\r\nstatic void etnaviv_iommuv2_domain_free(struct iommu_domain *domain)\r\n{\r\nstruct etnaviv_iommuv2_domain *etnaviv_domain =\r\nto_etnaviv_domain(domain);\r\nint i;\r\ndma_free_coherent(etnaviv_domain->dev, SZ_4K,\r\netnaviv_domain->bad_page_cpu,\r\netnaviv_domain->bad_page_dma);\r\ndma_free_coherent(etnaviv_domain->dev, SZ_4K,\r\netnaviv_domain->mtlb_cpu,\r\netnaviv_domain->mtlb_dma);\r\nfor (i = 0; i < MMUv2_MAX_STLB_ENTRIES; i++) {\r\nif (etnaviv_domain->stlb_cpu[i])\r\ndma_free_coherent(etnaviv_domain->dev, SZ_4K,\r\netnaviv_domain->stlb_cpu[i],\r\netnaviv_domain->stlb_dma[i]);\r\n}\r\nvfree(etnaviv_domain);\r\n}\r\nstatic size_t etnaviv_iommuv2_dump_size(struct iommu_domain *domain)\r\n{\r\nstruct etnaviv_iommuv2_domain *etnaviv_domain =\r\nto_etnaviv_domain(domain);\r\nsize_t dump_size = SZ_4K;\r\nint i;\r\nfor (i = 0; i < MMUv2_MAX_STLB_ENTRIES; i++)\r\nif (etnaviv_domain->mtlb_cpu[i] & MMUv2_PTE_PRESENT)\r\ndump_size += SZ_4K;\r\nreturn dump_size;\r\n}\r\nstatic void etnaviv_iommuv2_dump(struct iommu_domain *domain, void *buf)\r\n{\r\nstruct etnaviv_iommuv2_domain *etnaviv_domain =\r\nto_etnaviv_domain(domain);\r\nint i;\r\nmemcpy(buf, etnaviv_domain->mtlb_cpu, SZ_4K);\r\nbuf += SZ_4K;\r\nfor (i = 0; i < MMUv2_MAX_STLB_ENTRIES; i++, buf += SZ_4K)\r\nif (etnaviv_domain->mtlb_cpu[i] & MMUv2_PTE_PRESENT)\r\nmemcpy(buf, etnaviv_domain->stlb_cpu[i], SZ_4K);\r\n}\r\nvoid etnaviv_iommuv2_restore(struct etnaviv_gpu *gpu)\r\n{\r\nstruct etnaviv_iommuv2_domain *etnaviv_domain =\r\nto_etnaviv_domain(gpu->mmu->domain);\r\nu16 prefetch;\r\nif (gpu_read(gpu, VIVS_MMUv2_CONTROL) & VIVS_MMUv2_CONTROL_ENABLE)\r\nreturn;\r\nprefetch = etnaviv_buffer_config_mmuv2(gpu,\r\n(u32)etnaviv_domain->mtlb_dma,\r\n(u32)etnaviv_domain->bad_page_dma);\r\netnaviv_gpu_start_fe(gpu, (u32)etnaviv_cmdbuf_get_pa(gpu->buffer),\r\nprefetch);\r\netnaviv_gpu_wait_idle(gpu, 100);\r\ngpu_write(gpu, VIVS_MMUv2_CONTROL, VIVS_MMUv2_CONTROL_ENABLE);\r\n}\r\nstruct iommu_domain *etnaviv_iommuv2_domain_alloc(struct etnaviv_gpu *gpu)\r\n{\r\nstruct etnaviv_iommuv2_domain *etnaviv_domain;\r\nint ret;\r\netnaviv_domain = vzalloc(sizeof(*etnaviv_domain));\r\nif (!etnaviv_domain)\r\nreturn NULL;\r\netnaviv_domain->dev = gpu->dev;\r\netnaviv_domain->domain.type = __IOMMU_DOMAIN_PAGING;\r\netnaviv_domain->domain.ops = &etnaviv_iommu_ops.ops;\r\netnaviv_domain->domain.pgsize_bitmap = SZ_4K;\r\netnaviv_domain->domain.geometry.aperture_start = 0;\r\netnaviv_domain->domain.geometry.aperture_end = ~0UL & ~(SZ_4K - 1);\r\nret = etnaviv_iommuv2_init(etnaviv_domain);\r\nif (ret)\r\ngoto out_free;\r\nreturn &etnaviv_domain->domain;\r\nout_free:\r\nvfree(etnaviv_domain);\r\nreturn NULL;\r\n}
