static inline u32 get_event_type(struct perf_event *event)\r\n{\r\nreturn (event->attr.config) & L3_EVTYPE_MASK;\r\n}\r\nstatic inline bool event_uses_long_counter(struct perf_event *event)\r\n{\r\nreturn !!(event->attr.config & BIT_ULL(L3_EVENT_LC_BIT));\r\n}\r\nstatic inline int event_num_counters(struct perf_event *event)\r\n{\r\nreturn event_uses_long_counter(event) ? 2 : 1;\r\n}\r\nstatic void qcom_l3_cache__64bit_counter_start(struct perf_event *event)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\r\nint idx = event->hw.idx;\r\nu32 evsel = get_event_type(event);\r\nu32 gang;\r\ngang = readl_relaxed(l3pmu->regs + L3_M_BC_GANG);\r\ngang |= GANG_EN(idx + 1);\r\nwritel_relaxed(gang, l3pmu->regs + L3_M_BC_GANG);\r\nlocal64_set(&event->hw.prev_count, 0);\r\nwritel_relaxed(0, l3pmu->regs + L3_HML3_PM_EVCNTR(idx + 1));\r\nwritel_relaxed(0, l3pmu->regs + L3_HML3_PM_EVCNTR(idx));\r\nwritel_relaxed(EVSEL(0), l3pmu->regs + L3_HML3_PM_EVTYPE(idx + 1));\r\nwritel_relaxed(EVSEL(evsel), l3pmu->regs + L3_HML3_PM_EVTYPE(idx));\r\nwritel_relaxed(PMCNT_RESET, l3pmu->regs + L3_HML3_PM_CNTCTL(idx + 1));\r\nwritel_relaxed(PMCNTENSET(idx + 1), l3pmu->regs + L3_M_BC_CNTENSET);\r\nwritel_relaxed(PMCNT_RESET, l3pmu->regs + L3_HML3_PM_CNTCTL(idx));\r\nwritel_relaxed(PMCNTENSET(idx), l3pmu->regs + L3_M_BC_CNTENSET);\r\n}\r\nstatic void qcom_l3_cache__64bit_counter_stop(struct perf_event *event,\r\nint flags)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\r\nint idx = event->hw.idx;\r\nu32 gang = readl_relaxed(l3pmu->regs + L3_M_BC_GANG);\r\nwritel_relaxed(PMCNTENCLR(idx), l3pmu->regs + L3_M_BC_CNTENCLR);\r\nwritel_relaxed(PMCNTENCLR(idx + 1), l3pmu->regs + L3_M_BC_CNTENCLR);\r\nwritel_relaxed(gang & ~GANG_EN(idx + 1), l3pmu->regs + L3_M_BC_GANG);\r\n}\r\nstatic void qcom_l3_cache__64bit_counter_update(struct perf_event *event)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\r\nint idx = event->hw.idx;\r\nu32 hi, lo;\r\nu64 prev, new;\r\ndo {\r\nprev = local64_read(&event->hw.prev_count);\r\ndo {\r\nhi = readl_relaxed(l3pmu->regs + L3_HML3_PM_EVCNTR(idx + 1));\r\nlo = readl_relaxed(l3pmu->regs + L3_HML3_PM_EVCNTR(idx));\r\n} while (hi != readl_relaxed(l3pmu->regs + L3_HML3_PM_EVCNTR(idx + 1)));\r\nnew = ((u64)hi << 32) | lo;\r\n} while (local64_cmpxchg(&event->hw.prev_count, prev, new) != prev);\r\nlocal64_add(new - prev, &event->count);\r\n}\r\nstatic void qcom_l3_cache__32bit_counter_start(struct perf_event *event)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\r\nint idx = event->hw.idx;\r\nu32 evsel = get_event_type(event);\r\nu32 irqctl = readl_relaxed(l3pmu->regs + L3_M_BC_IRQCTL);\r\nwritel_relaxed(irqctl | PMIRQONMSBEN(idx), l3pmu->regs + L3_M_BC_IRQCTL);\r\nlocal64_set(&event->hw.prev_count, 0);\r\nwritel_relaxed(0, l3pmu->regs + L3_HML3_PM_EVCNTR(idx));\r\nwritel_relaxed(EVSEL(evsel), l3pmu->regs + L3_HML3_PM_EVTYPE(idx));\r\nwritel_relaxed(PMINTENSET(idx), l3pmu->regs + L3_M_BC_INTENSET);\r\nwritel_relaxed(PMCNT_RESET, l3pmu->regs + L3_HML3_PM_CNTCTL(idx));\r\nwritel_relaxed(PMCNTENSET(idx), l3pmu->regs + L3_M_BC_CNTENSET);\r\n}\r\nstatic void qcom_l3_cache__32bit_counter_stop(struct perf_event *event,\r\nint flags)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\r\nint idx = event->hw.idx;\r\nu32 irqctl = readl_relaxed(l3pmu->regs + L3_M_BC_IRQCTL);\r\nwritel_relaxed(PMCNTENCLR(idx), l3pmu->regs + L3_M_BC_CNTENCLR);\r\nwritel_relaxed(PMINTENCLR(idx), l3pmu->regs + L3_M_BC_INTENCLR);\r\nwritel_relaxed(irqctl & ~PMIRQONMSBEN(idx), l3pmu->regs + L3_M_BC_IRQCTL);\r\n}\r\nstatic void qcom_l3_cache__32bit_counter_update(struct perf_event *event)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\r\nint idx = event->hw.idx;\r\nu32 prev, new;\r\ndo {\r\nprev = local64_read(&event->hw.prev_count);\r\nnew = readl_relaxed(l3pmu->regs + L3_HML3_PM_EVCNTR(idx));\r\n} while (local64_cmpxchg(&event->hw.prev_count, prev, new) != prev);\r\nlocal64_add(new - prev, &event->count);\r\n}\r\nstatic\r\nconst struct l3cache_event_ops *l3cache_event_get_ops(struct perf_event *event)\r\n{\r\nif (event_uses_long_counter(event))\r\nreturn &event_ops_long;\r\nelse\r\nreturn &event_ops_std;\r\n}\r\nstatic inline void qcom_l3_cache__init(struct l3cache_pmu *l3pmu)\r\n{\r\nint i;\r\nwritel_relaxed(BC_RESET, l3pmu->regs + L3_M_BC_CR);\r\nwritel(BC_SATROLL_CR_RESET, l3pmu->regs + L3_M_BC_SATROLL_CR);\r\nwritel_relaxed(BC_CNTENCLR_RESET, l3pmu->regs + L3_M_BC_CNTENCLR);\r\nwritel_relaxed(BC_INTENCLR_RESET, l3pmu->regs + L3_M_BC_INTENCLR);\r\nwritel_relaxed(PMOVSRCLR_RESET, l3pmu->regs + L3_M_BC_OVSR);\r\nwritel_relaxed(BC_GANG_RESET, l3pmu->regs + L3_M_BC_GANG);\r\nwritel_relaxed(BC_IRQCTL_RESET, l3pmu->regs + L3_M_BC_IRQCTL);\r\nwritel_relaxed(PM_CR_RESET, l3pmu->regs + L3_HML3_PM_CR);\r\nfor (i = 0; i < L3_NUM_COUNTERS; ++i) {\r\nwritel_relaxed(PMCNT_RESET, l3pmu->regs + L3_HML3_PM_CNTCTL(i));\r\nwritel_relaxed(EVSEL(0), l3pmu->regs + L3_HML3_PM_EVTYPE(i));\r\n}\r\nwritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRA);\r\nwritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRAM);\r\nwritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRB);\r\nwritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRBM);\r\nwritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRC);\r\nwritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRCM);\r\nwritel(BC_ENABLE, l3pmu->regs + L3_M_BC_CR);\r\n}\r\nstatic irqreturn_t qcom_l3_cache__handle_irq(int irq_num, void *data)\r\n{\r\nstruct l3cache_pmu *l3pmu = data;\r\nlong status = readl_relaxed(l3pmu->regs + L3_M_BC_OVSR);\r\nint idx;\r\nif (status == 0)\r\nreturn IRQ_NONE;\r\nwritel_relaxed(status, l3pmu->regs + L3_M_BC_OVSR);\r\nfor_each_set_bit(idx, &status, L3_NUM_COUNTERS) {\r\nstruct perf_event *event;\r\nconst struct l3cache_event_ops *ops;\r\nevent = l3pmu->events[idx];\r\nif (!event)\r\ncontinue;\r\nops = l3cache_event_get_ops(event);\r\nops->update(event);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void qcom_l3_cache__pmu_enable(struct pmu *pmu)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(pmu);\r\nwmb();\r\nwritel_relaxed(BC_ENABLE, l3pmu->regs + L3_M_BC_CR);\r\n}\r\nstatic void qcom_l3_cache__pmu_disable(struct pmu *pmu)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(pmu);\r\nwritel_relaxed(0, l3pmu->regs + L3_M_BC_CR);\r\nwmb();\r\n}\r\nstatic bool qcom_l3_cache__validate_event_group(struct perf_event *event)\r\n{\r\nstruct perf_event *leader = event->group_leader;\r\nstruct perf_event *sibling;\r\nint counters = 0;\r\nif (leader->pmu != event->pmu && !is_software_event(leader))\r\nreturn false;\r\ncounters = event_num_counters(event);\r\ncounters += event_num_counters(leader);\r\nlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\r\nif (is_software_event(sibling))\r\ncontinue;\r\nif (sibling->pmu != event->pmu)\r\nreturn false;\r\ncounters += event_num_counters(sibling);\r\n}\r\nreturn counters <= L3_NUM_COUNTERS;\r\n}\r\nstatic int qcom_l3_cache__event_init(struct perf_event *event)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (event->attr.type != event->pmu->type)\r\nreturn -ENOENT;\r\nif (event->attr.exclude_user || event->attr.exclude_kernel ||\r\nevent->attr.exclude_hv || event->attr.exclude_idle)\r\nreturn -EINVAL;\r\nif (hwc->sample_period)\r\nreturn -EINVAL;\r\nif (event->cpu < 0)\r\nreturn -EINVAL;\r\nif (!qcom_l3_cache__validate_event_group(event))\r\nreturn -EINVAL;\r\nhwc->idx = -1;\r\nevent->cpu = cpumask_first(&l3pmu->cpumask);\r\nreturn 0;\r\n}\r\nstatic void qcom_l3_cache__event_start(struct perf_event *event, int flags)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nconst struct l3cache_event_ops *ops = l3cache_event_get_ops(event);\r\nhwc->state = 0;\r\nops->start(event);\r\n}\r\nstatic void qcom_l3_cache__event_stop(struct perf_event *event, int flags)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nconst struct l3cache_event_ops *ops = l3cache_event_get_ops(event);\r\nif (hwc->state & PERF_HES_STOPPED)\r\nreturn;\r\nops->stop(event, flags);\r\nif (flags & PERF_EF_UPDATE)\r\nops->update(event);\r\nhwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\n}\r\nstatic int qcom_l3_cache__event_add(struct perf_event *event, int flags)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint order = event_uses_long_counter(event) ? 1 : 0;\r\nint idx;\r\nidx = bitmap_find_free_region(l3pmu->used_mask, L3_NUM_COUNTERS, order);\r\nif (idx < 0)\r\nreturn -EAGAIN;\r\nhwc->idx = idx;\r\nhwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\nl3pmu->events[idx] = event;\r\nif (flags & PERF_EF_START)\r\nqcom_l3_cache__event_start(event, 0);\r\nperf_event_update_userpage(event);\r\nreturn 0;\r\n}\r\nstatic void qcom_l3_cache__event_del(struct perf_event *event, int flags)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint order = event_uses_long_counter(event) ? 1 : 0;\r\nqcom_l3_cache__event_stop(event, flags | PERF_EF_UPDATE);\r\nl3pmu->events[hwc->idx] = NULL;\r\nbitmap_release_region(l3pmu->used_mask, hwc->idx, order);\r\nperf_event_update_userpage(event);\r\n}\r\nstatic void qcom_l3_cache__event_read(struct perf_event *event)\r\n{\r\nconst struct l3cache_event_ops *ops = l3cache_event_get_ops(event);\r\nops->update(event);\r\n}\r\nstatic ssize_t l3cache_pmu_format_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct dev_ext_attribute *eattr;\r\neattr = container_of(attr, struct dev_ext_attribute, attr);\r\nreturn sprintf(buf, "%s\n", (char *) eattr->var);\r\n}\r\nstatic ssize_t l3cache_pmu_event_show(struct device *dev,\r\nstruct device_attribute *attr, char *page)\r\n{\r\nstruct perf_pmu_events_attr *pmu_attr;\r\npmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);\r\nreturn sprintf(page, "event=0x%02llx\n", pmu_attr->id);\r\n}\r\nstatic ssize_t qcom_l3_cache_pmu_cpumask_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct l3cache_pmu *l3pmu = to_l3cache_pmu(dev_get_drvdata(dev));\r\nreturn cpumap_print_to_pagebuf(true, buf, &l3pmu->cpumask);\r\n}\r\nstatic int qcom_l3_cache_pmu_online_cpu(unsigned int cpu, struct hlist_node *node)\r\n{\r\nstruct l3cache_pmu *l3pmu = hlist_entry_safe(node, struct l3cache_pmu, node);\r\nif (cpumask_empty(&l3pmu->cpumask))\r\ncpumask_set_cpu(cpu, &l3pmu->cpumask);\r\nreturn 0;\r\n}\r\nstatic int qcom_l3_cache_pmu_offline_cpu(unsigned int cpu, struct hlist_node *node)\r\n{\r\nstruct l3cache_pmu *l3pmu = hlist_entry_safe(node, struct l3cache_pmu, node);\r\nunsigned int target;\r\nif (!cpumask_test_and_clear_cpu(cpu, &l3pmu->cpumask))\r\nreturn 0;\r\ntarget = cpumask_any_but(cpu_online_mask, cpu);\r\nif (target >= nr_cpu_ids)\r\nreturn 0;\r\nperf_pmu_migrate_context(&l3pmu->pmu, cpu, target);\r\ncpumask_set_cpu(target, &l3pmu->cpumask);\r\nreturn 0;\r\n}\r\nstatic int qcom_l3_cache_pmu_probe(struct platform_device *pdev)\r\n{\r\nstruct l3cache_pmu *l3pmu;\r\nstruct acpi_device *acpi_dev;\r\nstruct resource *memrc;\r\nint ret;\r\nchar *name;\r\nacpi_dev = ACPI_COMPANION(&pdev->dev);\r\nif (!acpi_dev)\r\nreturn -ENODEV;\r\nl3pmu = devm_kzalloc(&pdev->dev, sizeof(*l3pmu), GFP_KERNEL);\r\nname = devm_kasprintf(&pdev->dev, GFP_KERNEL, "l3cache_%s_%s",\r\nacpi_dev->parent->pnp.unique_id, acpi_dev->pnp.unique_id);\r\nif (!l3pmu || !name)\r\nreturn -ENOMEM;\r\nl3pmu->pmu = (struct pmu) {\r\n.task_ctx_nr = perf_invalid_context,\r\n.pmu_enable = qcom_l3_cache__pmu_enable,\r\n.pmu_disable = qcom_l3_cache__pmu_disable,\r\n.event_init = qcom_l3_cache__event_init,\r\n.add = qcom_l3_cache__event_add,\r\n.del = qcom_l3_cache__event_del,\r\n.start = qcom_l3_cache__event_start,\r\n.stop = qcom_l3_cache__event_stop,\r\n.read = qcom_l3_cache__event_read,\r\n.attr_groups = qcom_l3_cache_pmu_attr_grps,\r\n};\r\nmemrc = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nl3pmu->regs = devm_ioremap_resource(&pdev->dev, memrc);\r\nif (IS_ERR(l3pmu->regs)) {\r\ndev_err(&pdev->dev, "Can't map PMU @%pa\n", &memrc->start);\r\nreturn PTR_ERR(l3pmu->regs);\r\n}\r\nqcom_l3_cache__init(l3pmu);\r\nret = platform_get_irq(pdev, 0);\r\nif (ret <= 0)\r\nreturn ret;\r\nret = devm_request_irq(&pdev->dev, ret, qcom_l3_cache__handle_irq, 0,\r\nname, l3pmu);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Request for IRQ failed for slice @%pa\n",\r\n&memrc->start);\r\nreturn ret;\r\n}\r\nret = cpuhp_state_add_instance(CPUHP_AP_PERF_ARM_QCOM_L3_ONLINE, &l3pmu->node);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Error %d registering hotplug", ret);\r\nreturn ret;\r\n}\r\nret = perf_pmu_register(&l3pmu->pmu, name, -1);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev, "Failed to register L3 cache PMU (%d)\n", ret);\r\nreturn ret;\r\n}\r\ndev_info(&pdev->dev, "Registered %s, type: %d\n", name, l3pmu->pmu.type);\r\nreturn 0;\r\n}\r\nstatic int __init register_qcom_l3_cache_pmu_driver(void)\r\n{\r\nint ret;\r\nret = cpuhp_setup_state_multi(CPUHP_AP_PERF_ARM_QCOM_L3_ONLINE,\r\n"perf/qcom/l3cache:online",\r\nqcom_l3_cache_pmu_online_cpu,\r\nqcom_l3_cache_pmu_offline_cpu);\r\nif (ret)\r\nreturn ret;\r\nreturn platform_driver_register(&qcom_l3_cache_pmu_driver);\r\n}
