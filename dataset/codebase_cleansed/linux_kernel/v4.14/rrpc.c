static void rrpc_page_invalidate(struct rrpc *rrpc, struct rrpc_addr *a)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_block *rblk = a->rblk;\r\nunsigned int pg_offset;\r\nlockdep_assert_held(&rrpc->rev_lock);\r\nif (a->addr == ADDR_EMPTY || !rblk)\r\nreturn;\r\nspin_lock(&rblk->lock);\r\ndiv_u64_rem(a->addr, dev->geo.sec_per_blk, &pg_offset);\r\nWARN_ON(test_and_set_bit(pg_offset, rblk->invalid_pages));\r\nrblk->nr_invalid_pages++;\r\nspin_unlock(&rblk->lock);\r\nrrpc->rev_trans_map[a->addr].addr = ADDR_EMPTY;\r\n}\r\nstatic void rrpc_invalidate_range(struct rrpc *rrpc, sector_t slba,\r\nunsigned int len)\r\n{\r\nsector_t i;\r\nspin_lock(&rrpc->rev_lock);\r\nfor (i = slba; i < slba + len; i++) {\r\nstruct rrpc_addr *gp = &rrpc->trans_map[i];\r\nrrpc_page_invalidate(rrpc, gp);\r\ngp->rblk = NULL;\r\n}\r\nspin_unlock(&rrpc->rev_lock);\r\n}\r\nstatic struct nvm_rq *rrpc_inflight_laddr_acquire(struct rrpc *rrpc,\r\nsector_t laddr, unsigned int pages)\r\n{\r\nstruct nvm_rq *rqd;\r\nstruct rrpc_inflight_rq *inf;\r\nrqd = mempool_alloc(rrpc->rq_pool, GFP_ATOMIC);\r\nif (!rqd)\r\nreturn ERR_PTR(-ENOMEM);\r\ninf = rrpc_get_inflight_rq(rqd);\r\nif (rrpc_lock_laddr(rrpc, laddr, pages, inf)) {\r\nmempool_free(rqd, rrpc->rq_pool);\r\nreturn NULL;\r\n}\r\nreturn rqd;\r\n}\r\nstatic void rrpc_inflight_laddr_release(struct rrpc *rrpc, struct nvm_rq *rqd)\r\n{\r\nstruct rrpc_inflight_rq *inf = rrpc_get_inflight_rq(rqd);\r\nrrpc_unlock_laddr(rrpc, inf);\r\nmempool_free(rqd, rrpc->rq_pool);\r\n}\r\nstatic void rrpc_discard(struct rrpc *rrpc, struct bio *bio)\r\n{\r\nsector_t slba = bio->bi_iter.bi_sector / NR_PHY_IN_LOG;\r\nsector_t len = bio->bi_iter.bi_size / RRPC_EXPOSED_PAGE_SIZE;\r\nstruct nvm_rq *rqd;\r\nwhile (1) {\r\nrqd = rrpc_inflight_laddr_acquire(rrpc, slba, len);\r\nif (rqd)\r\nbreak;\r\nschedule();\r\n}\r\nif (IS_ERR(rqd)) {\r\npr_err("rrpc: unable to acquire inflight IO\n");\r\nbio_io_error(bio);\r\nreturn;\r\n}\r\nrrpc_invalidate_range(rrpc, slba, len);\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\n}\r\nstatic int block_is_full(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nreturn (rblk->next_page == dev->geo.sec_per_blk);\r\n}\r\nstatic u64 block_to_rel_addr(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_lun *rlun = rblk->rlun;\r\nreturn rlun->id * dev->geo.sec_per_blk;\r\n}\r\nstatic struct ppa_addr rrpc_ppa_to_gaddr(struct nvm_tgt_dev *dev,\r\nstruct rrpc_addr *gp)\r\n{\r\nstruct rrpc_block *rblk = gp->rblk;\r\nstruct rrpc_lun *rlun = rblk->rlun;\r\nu64 addr = gp->addr;\r\nstruct ppa_addr paddr;\r\npaddr.ppa = addr;\r\npaddr = rrpc_linear_to_generic_addr(&dev->geo, paddr);\r\npaddr.g.ch = rlun->bppa.g.ch;\r\npaddr.g.lun = rlun->bppa.g.lun;\r\npaddr.g.blk = rblk->id;\r\nreturn paddr;\r\n}\r\nstatic void rrpc_set_lun_cur(struct rrpc_lun *rlun, struct rrpc_block *new_rblk,\r\nstruct rrpc_block **cur_rblk)\r\n{\r\nstruct rrpc *rrpc = rlun->rrpc;\r\nif (*cur_rblk) {\r\nspin_lock(&(*cur_rblk)->lock);\r\nWARN_ON(!block_is_full(rrpc, *cur_rblk));\r\nspin_unlock(&(*cur_rblk)->lock);\r\n}\r\n*cur_rblk = new_rblk;\r\n}\r\nstatic struct rrpc_block *__rrpc_get_blk(struct rrpc *rrpc,\r\nstruct rrpc_lun *rlun)\r\n{\r\nstruct rrpc_block *rblk = NULL;\r\nif (list_empty(&rlun->free_list))\r\ngoto out;\r\nrblk = list_first_entry(&rlun->free_list, struct rrpc_block, list);\r\nlist_move_tail(&rblk->list, &rlun->used_list);\r\nrblk->state = NVM_BLK_ST_TGT;\r\nrlun->nr_free_blocks--;\r\nout:\r\nreturn rblk;\r\n}\r\nstatic struct rrpc_block *rrpc_get_blk(struct rrpc *rrpc, struct rrpc_lun *rlun,\r\nunsigned long flags)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_block *rblk;\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nspin_lock(&rlun->lock);\r\nif (!is_gc && rlun->nr_free_blocks < rlun->reserved_blocks) {\r\npr_err("nvm: rrpc: cannot give block to non GC request\n");\r\nspin_unlock(&rlun->lock);\r\nreturn NULL;\r\n}\r\nrblk = __rrpc_get_blk(rrpc, rlun);\r\nif (!rblk) {\r\npr_err("nvm: rrpc: cannot get new block\n");\r\nspin_unlock(&rlun->lock);\r\nreturn NULL;\r\n}\r\nspin_unlock(&rlun->lock);\r\nbitmap_zero(rblk->invalid_pages, dev->geo.sec_per_blk);\r\nrblk->next_page = 0;\r\nrblk->nr_invalid_pages = 0;\r\natomic_set(&rblk->data_cmnt_size, 0);\r\nreturn rblk;\r\n}\r\nstatic void rrpc_put_blk(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct rrpc_lun *rlun = rblk->rlun;\r\nspin_lock(&rlun->lock);\r\nif (rblk->state & NVM_BLK_ST_TGT) {\r\nlist_move_tail(&rblk->list, &rlun->free_list);\r\nrlun->nr_free_blocks++;\r\nrblk->state = NVM_BLK_ST_FREE;\r\n} else if (rblk->state & NVM_BLK_ST_BAD) {\r\nlist_move_tail(&rblk->list, &rlun->bb_list);\r\nrblk->state = NVM_BLK_ST_BAD;\r\n} else {\r\nWARN_ON_ONCE(1);\r\npr_err("rrpc: erroneous type (ch:%d,lun:%d,blk%d-> %u)\n",\r\nrlun->bppa.g.ch, rlun->bppa.g.lun,\r\nrblk->id, rblk->state);\r\nlist_move_tail(&rblk->list, &rlun->bb_list);\r\n}\r\nspin_unlock(&rlun->lock);\r\n}\r\nstatic void rrpc_put_blks(struct rrpc *rrpc)\r\n{\r\nstruct rrpc_lun *rlun;\r\nint i;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nrlun = &rrpc->luns[i];\r\nif (rlun->cur)\r\nrrpc_put_blk(rrpc, rlun->cur);\r\nif (rlun->gc_cur)\r\nrrpc_put_blk(rrpc, rlun->gc_cur);\r\n}\r\n}\r\nstatic struct rrpc_lun *get_next_lun(struct rrpc *rrpc)\r\n{\r\nint next = atomic_inc_return(&rrpc->next_lun);\r\nreturn &rrpc->luns[next % rrpc->nr_luns];\r\n}\r\nstatic void rrpc_gc_kick(struct rrpc *rrpc)\r\n{\r\nstruct rrpc_lun *rlun;\r\nunsigned int i;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nrlun = &rrpc->luns[i];\r\nqueue_work(rrpc->krqd_wq, &rlun->ws_gc);\r\n}\r\n}\r\nstatic void rrpc_gc_timer(unsigned long data)\r\n{\r\nstruct rrpc *rrpc = (struct rrpc *)data;\r\nrrpc_gc_kick(rrpc);\r\nmod_timer(&rrpc->gc_timer, jiffies + msecs_to_jiffies(10));\r\n}\r\nstatic void rrpc_end_sync_bio(struct bio *bio)\r\n{\r\nstruct completion *waiting = bio->bi_private;\r\nif (bio->bi_status)\r\npr_err("nvm: gc request failed (%u).\n", bio->bi_status);\r\ncomplete(waiting);\r\n}\r\nstatic int rrpc_move_valid_pages(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct request_queue *q = dev->q;\r\nstruct rrpc_rev_addr *rev;\r\nstruct nvm_rq *rqd;\r\nstruct bio *bio;\r\nstruct page *page;\r\nint slot;\r\nint nr_sec_per_blk = dev->geo.sec_per_blk;\r\nu64 phys_addr;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nif (bitmap_full(rblk->invalid_pages, nr_sec_per_blk))\r\nreturn 0;\r\nbio = bio_alloc(GFP_NOIO, 1);\r\nif (!bio) {\r\npr_err("nvm: could not alloc bio to gc\n");\r\nreturn -ENOMEM;\r\n}\r\npage = mempool_alloc(rrpc->page_pool, GFP_NOIO);\r\nwhile ((slot = find_first_zero_bit(rblk->invalid_pages,\r\nnr_sec_per_blk)) < nr_sec_per_blk) {\r\nphys_addr = rrpc_blk_to_ppa(rrpc, rblk) + slot;\r\ntry:\r\nspin_lock(&rrpc->rev_lock);\r\nrev = &rrpc->rev_trans_map[phys_addr];\r\nif (rev->addr == ADDR_EMPTY) {\r\nspin_unlock(&rrpc->rev_lock);\r\ncontinue;\r\n}\r\nrqd = rrpc_inflight_laddr_acquire(rrpc, rev->addr, 1);\r\nif (IS_ERR_OR_NULL(rqd)) {\r\nspin_unlock(&rrpc->rev_lock);\r\nschedule();\r\ngoto try;\r\n}\r\nspin_unlock(&rrpc->rev_lock);\r\nbio->bi_iter.bi_sector = rrpc_get_sector(rev->addr);\r\nbio_set_op_attrs(bio, REQ_OP_READ, 0);\r\nbio->bi_private = &wait;\r\nbio->bi_end_io = rrpc_end_sync_bio;\r\nbio_add_pc_page(q, bio, page, RRPC_EXPOSED_PAGE_SIZE, 0);\r\nif (rrpc_submit_io(rrpc, bio, rqd, NVM_IOTYPE_GC)) {\r\npr_err("rrpc: gc read failed.\n");\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\ngoto finished;\r\n}\r\nwait_for_completion_io(&wait);\r\nif (bio->bi_status) {\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\ngoto finished;\r\n}\r\nbio_reset(bio);\r\nreinit_completion(&wait);\r\nbio->bi_iter.bi_sector = rrpc_get_sector(rev->addr);\r\nbio_set_op_attrs(bio, REQ_OP_WRITE, 0);\r\nbio->bi_private = &wait;\r\nbio->bi_end_io = rrpc_end_sync_bio;\r\nbio_add_pc_page(q, bio, page, RRPC_EXPOSED_PAGE_SIZE, 0);\r\nif (rrpc_submit_io(rrpc, bio, rqd, NVM_IOTYPE_GC)) {\r\npr_err("rrpc: gc write failed.\n");\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\ngoto finished;\r\n}\r\nwait_for_completion_io(&wait);\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\nif (bio->bi_status)\r\ngoto finished;\r\nbio_reset(bio);\r\n}\r\nfinished:\r\nmempool_free(page, rrpc->page_pool);\r\nbio_put(bio);\r\nif (!bitmap_full(rblk->invalid_pages, nr_sec_per_blk)) {\r\npr_err("nvm: failed to garbage collect block\n");\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic void rrpc_block_gc(struct work_struct *work)\r\n{\r\nstruct rrpc_block_gc *gcb = container_of(work, struct rrpc_block_gc,\r\nws_gc);\r\nstruct rrpc *rrpc = gcb->rrpc;\r\nstruct rrpc_block *rblk = gcb->rblk;\r\nstruct rrpc_lun *rlun = rblk->rlun;\r\nstruct ppa_addr ppa;\r\nmempool_free(gcb, rrpc->gcb_pool);\r\npr_debug("nvm: block 'ch:%d,lun:%d,blk:%d' being reclaimed\n",\r\nrlun->bppa.g.ch, rlun->bppa.g.lun,\r\nrblk->id);\r\nif (rrpc_move_valid_pages(rrpc, rblk))\r\ngoto put_back;\r\nppa.ppa = 0;\r\nppa.g.ch = rlun->bppa.g.ch;\r\nppa.g.lun = rlun->bppa.g.lun;\r\nppa.g.blk = rblk->id;\r\nif (nvm_erase_sync(rrpc->dev, &ppa, 1))\r\ngoto put_back;\r\nrrpc_put_blk(rrpc, rblk);\r\nreturn;\r\nput_back:\r\nspin_lock(&rlun->lock);\r\nlist_add_tail(&rblk->prio, &rlun->prio_list);\r\nspin_unlock(&rlun->lock);\r\n}\r\nstatic struct rrpc_block *rblk_max_invalid(struct rrpc_block *ra,\r\nstruct rrpc_block *rb)\r\n{\r\nif (ra->nr_invalid_pages == rb->nr_invalid_pages)\r\nreturn ra;\r\nreturn (ra->nr_invalid_pages < rb->nr_invalid_pages) ? rb : ra;\r\n}\r\nstatic struct rrpc_block *block_prio_find_max(struct rrpc_lun *rlun)\r\n{\r\nstruct list_head *prio_list = &rlun->prio_list;\r\nstruct rrpc_block *rblk, *max;\r\nBUG_ON(list_empty(prio_list));\r\nmax = list_first_entry(prio_list, struct rrpc_block, prio);\r\nlist_for_each_entry(rblk, prio_list, prio)\r\nmax = rblk_max_invalid(max, rblk);\r\nreturn max;\r\n}\r\nstatic void rrpc_lun_gc(struct work_struct *work)\r\n{\r\nstruct rrpc_lun *rlun = container_of(work, struct rrpc_lun, ws_gc);\r\nstruct rrpc *rrpc = rlun->rrpc;\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_block_gc *gcb;\r\nunsigned int nr_blocks_need;\r\nnr_blocks_need = dev->geo.blks_per_lun / GC_LIMIT_INVERSE;\r\nif (nr_blocks_need < rrpc->nr_luns)\r\nnr_blocks_need = rrpc->nr_luns;\r\nspin_lock(&rlun->lock);\r\nwhile (nr_blocks_need > rlun->nr_free_blocks &&\r\n!list_empty(&rlun->prio_list)) {\r\nstruct rrpc_block *rblk = block_prio_find_max(rlun);\r\nif (!rblk->nr_invalid_pages)\r\nbreak;\r\ngcb = mempool_alloc(rrpc->gcb_pool, GFP_ATOMIC);\r\nif (!gcb)\r\nbreak;\r\nlist_del_init(&rblk->prio);\r\nWARN_ON(!block_is_full(rrpc, rblk));\r\npr_debug("rrpc: selected block 'ch:%d,lun:%d,blk:%d' for GC\n",\r\nrlun->bppa.g.ch, rlun->bppa.g.lun,\r\nrblk->id);\r\ngcb->rrpc = rrpc;\r\ngcb->rblk = rblk;\r\nINIT_WORK(&gcb->ws_gc, rrpc_block_gc);\r\nqueue_work(rrpc->kgc_wq, &gcb->ws_gc);\r\nnr_blocks_need--;\r\n}\r\nspin_unlock(&rlun->lock);\r\n}\r\nstatic void rrpc_gc_queue(struct work_struct *work)\r\n{\r\nstruct rrpc_block_gc *gcb = container_of(work, struct rrpc_block_gc,\r\nws_gc);\r\nstruct rrpc *rrpc = gcb->rrpc;\r\nstruct rrpc_block *rblk = gcb->rblk;\r\nstruct rrpc_lun *rlun = rblk->rlun;\r\nspin_lock(&rlun->lock);\r\nlist_add_tail(&rblk->prio, &rlun->prio_list);\r\nspin_unlock(&rlun->lock);\r\nmempool_free(gcb, rrpc->gcb_pool);\r\npr_debug("nvm: block 'ch:%d,lun:%d,blk:%d' full, allow GC (sched)\n",\r\nrlun->bppa.g.ch, rlun->bppa.g.lun,\r\nrblk->id);\r\n}\r\nstatic struct rrpc_lun *rrpc_get_lun_rr(struct rrpc *rrpc, int is_gc)\r\n{\r\nunsigned int i;\r\nstruct rrpc_lun *rlun, *max_free;\r\nif (!is_gc)\r\nreturn get_next_lun(rrpc);\r\nmax_free = &rrpc->luns[0];\r\nrrpc_for_each_lun(rrpc, rlun, i) {\r\nif (rlun->nr_free_blocks > max_free->nr_free_blocks)\r\nmax_free = rlun;\r\n}\r\nreturn max_free;\r\n}\r\nstatic struct rrpc_addr *rrpc_update_map(struct rrpc *rrpc, sector_t laddr,\r\nstruct rrpc_block *rblk, u64 paddr)\r\n{\r\nstruct rrpc_addr *gp;\r\nstruct rrpc_rev_addr *rev;\r\nBUG_ON(laddr >= rrpc->nr_sects);\r\ngp = &rrpc->trans_map[laddr];\r\nspin_lock(&rrpc->rev_lock);\r\nif (gp->rblk)\r\nrrpc_page_invalidate(rrpc, gp);\r\ngp->addr = paddr;\r\ngp->rblk = rblk;\r\nrev = &rrpc->rev_trans_map[gp->addr];\r\nrev->addr = laddr;\r\nspin_unlock(&rrpc->rev_lock);\r\nreturn gp;\r\n}\r\nstatic u64 rrpc_alloc_addr(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nu64 addr = ADDR_EMPTY;\r\nspin_lock(&rblk->lock);\r\nif (block_is_full(rrpc, rblk))\r\ngoto out;\r\naddr = rblk->next_page;\r\nrblk->next_page++;\r\nout:\r\nspin_unlock(&rblk->lock);\r\nreturn addr;\r\n}\r\nstatic struct ppa_addr rrpc_map_page(struct rrpc *rrpc, sector_t laddr,\r\nint is_gc)\r\n{\r\nstruct nvm_tgt_dev *tgt_dev = rrpc->dev;\r\nstruct rrpc_lun *rlun;\r\nstruct rrpc_block *rblk, **cur_rblk;\r\nstruct rrpc_addr *p;\r\nstruct ppa_addr ppa;\r\nu64 paddr;\r\nint gc_force = 0;\r\nppa.ppa = ADDR_EMPTY;\r\nrlun = rrpc_get_lun_rr(rrpc, is_gc);\r\nif (!is_gc && rlun->nr_free_blocks < rrpc->nr_luns * 4)\r\nreturn ppa;\r\nspin_lock(&rlun->lock);\r\ncur_rblk = &rlun->cur;\r\nrblk = rlun->cur;\r\nretry:\r\npaddr = rrpc_alloc_addr(rrpc, rblk);\r\nif (paddr != ADDR_EMPTY)\r\ngoto done;\r\nif (!list_empty(&rlun->wblk_list)) {\r\nnew_blk:\r\nrblk = list_first_entry(&rlun->wblk_list, struct rrpc_block,\r\nprio);\r\nrrpc_set_lun_cur(rlun, rblk, cur_rblk);\r\nlist_del(&rblk->prio);\r\ngoto retry;\r\n}\r\nspin_unlock(&rlun->lock);\r\nrblk = rrpc_get_blk(rrpc, rlun, gc_force);\r\nif (rblk) {\r\nspin_lock(&rlun->lock);\r\nlist_add_tail(&rblk->prio, &rlun->wblk_list);\r\ngoto new_blk;\r\n}\r\nif (unlikely(is_gc) && !gc_force) {\r\ncur_rblk = &rlun->gc_cur;\r\nrblk = rlun->gc_cur;\r\ngc_force = 1;\r\nspin_lock(&rlun->lock);\r\ngoto retry;\r\n}\r\npr_err("rrpc: failed to allocate new block\n");\r\nreturn ppa;\r\ndone:\r\nspin_unlock(&rlun->lock);\r\np = rrpc_update_map(rrpc, laddr, rblk, paddr);\r\nif (!p)\r\nreturn ppa;\r\nreturn rrpc_ppa_to_gaddr(tgt_dev, p);\r\n}\r\nstatic void rrpc_run_gc(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct rrpc_block_gc *gcb;\r\ngcb = mempool_alloc(rrpc->gcb_pool, GFP_ATOMIC);\r\nif (!gcb) {\r\npr_err("rrpc: unable to queue block for gc.");\r\nreturn;\r\n}\r\ngcb->rrpc = rrpc;\r\ngcb->rblk = rblk;\r\nINIT_WORK(&gcb->ws_gc, rrpc_gc_queue);\r\nqueue_work(rrpc->kgc_wq, &gcb->ws_gc);\r\n}\r\nstatic struct rrpc_lun *rrpc_ppa_to_lun(struct rrpc *rrpc, struct ppa_addr p)\r\n{\r\nstruct rrpc_lun *rlun = NULL;\r\nint i;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nif (rrpc->luns[i].bppa.g.ch == p.g.ch &&\r\nrrpc->luns[i].bppa.g.lun == p.g.lun) {\r\nrlun = &rrpc->luns[i];\r\nbreak;\r\n}\r\n}\r\nreturn rlun;\r\n}\r\nstatic void __rrpc_mark_bad_block(struct rrpc *rrpc, struct ppa_addr ppa)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_lun *rlun;\r\nstruct rrpc_block *rblk;\r\nrlun = rrpc_ppa_to_lun(rrpc, ppa);\r\nrblk = &rlun->blocks[ppa.g.blk];\r\nrblk->state = NVM_BLK_ST_BAD;\r\nnvm_set_tgt_bb_tbl(dev, &ppa, 1, NVM_BLK_T_GRWN_BAD);\r\n}\r\nstatic void rrpc_mark_bad_block(struct rrpc *rrpc, struct nvm_rq *rqd)\r\n{\r\nvoid *comp_bits = &rqd->ppa_status;\r\nstruct ppa_addr ppa, prev_ppa;\r\nint nr_ppas = rqd->nr_ppas;\r\nint bit;\r\nif (rqd->nr_ppas == 1)\r\n__rrpc_mark_bad_block(rrpc, rqd->ppa_addr);\r\nppa_set_empty(&prev_ppa);\r\nbit = -1;\r\nwhile ((bit = find_next_bit(comp_bits, nr_ppas, bit + 1)) < nr_ppas) {\r\nppa = rqd->ppa_list[bit];\r\nif (ppa_cmp_blk(ppa, prev_ppa))\r\ncontinue;\r\n__rrpc_mark_bad_block(rrpc, ppa);\r\n}\r\n}\r\nstatic void rrpc_end_io_write(struct rrpc *rrpc, struct rrpc_rq *rrqd,\r\nsector_t laddr, uint8_t npages)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_addr *p;\r\nstruct rrpc_block *rblk;\r\nint cmnt_size, i;\r\nfor (i = 0; i < npages; i++) {\r\np = &rrpc->trans_map[laddr + i];\r\nrblk = p->rblk;\r\ncmnt_size = atomic_inc_return(&rblk->data_cmnt_size);\r\nif (unlikely(cmnt_size == dev->geo.sec_per_blk))\r\nrrpc_run_gc(rrpc, rblk);\r\n}\r\n}\r\nstatic void rrpc_end_io(struct nvm_rq *rqd)\r\n{\r\nstruct rrpc *rrpc = rqd->private;\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_rq *rrqd = nvm_rq_to_pdu(rqd);\r\nuint8_t npages = rqd->nr_ppas;\r\nsector_t laddr = rrpc_get_laddr(rqd->bio) - npages;\r\nif (bio_data_dir(rqd->bio) == WRITE) {\r\nif (rqd->error == NVM_RSP_ERR_FAILWRITE)\r\nrrpc_mark_bad_block(rrpc, rqd);\r\nrrpc_end_io_write(rrpc, rrqd, laddr, npages);\r\n}\r\nbio_put(rqd->bio);\r\nif (rrqd->flags & NVM_IOTYPE_GC)\r\nreturn;\r\nrrpc_unlock_rq(rrpc, rqd);\r\nif (npages > 1)\r\nnvm_dev_dma_free(dev->parent, rqd->ppa_list, rqd->dma_ppa_list);\r\nmempool_free(rqd, rrpc->rq_pool);\r\n}\r\nstatic int rrpc_read_ppalist_rq(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags, int npages)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_inflight_rq *r = rrpc_get_inflight_rq(rqd);\r\nstruct rrpc_addr *gp;\r\nsector_t laddr = rrpc_get_laddr(bio);\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nint i;\r\nif (!is_gc && rrpc_lock_rq(rrpc, bio, rqd)) {\r\nnvm_dev_dma_free(dev->parent, rqd->ppa_list, rqd->dma_ppa_list);\r\nreturn NVM_IO_REQUEUE;\r\n}\r\nfor (i = 0; i < npages; i++) {\r\nBUG_ON(!(laddr + i < rrpc->nr_sects));\r\ngp = &rrpc->trans_map[laddr + i];\r\nif (gp->rblk) {\r\nrqd->ppa_list[i] = rrpc_ppa_to_gaddr(dev, gp);\r\n} else {\r\nBUG_ON(is_gc);\r\nrrpc_unlock_laddr(rrpc, r);\r\nnvm_dev_dma_free(dev->parent, rqd->ppa_list,\r\nrqd->dma_ppa_list);\r\nreturn NVM_IO_DONE;\r\n}\r\n}\r\nrqd->opcode = NVM_OP_HBREAD;\r\nreturn NVM_IO_OK;\r\n}\r\nstatic int rrpc_read_rq(struct rrpc *rrpc, struct bio *bio, struct nvm_rq *rqd,\r\nunsigned long flags)\r\n{\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nsector_t laddr = rrpc_get_laddr(bio);\r\nstruct rrpc_addr *gp;\r\nif (!is_gc && rrpc_lock_rq(rrpc, bio, rqd))\r\nreturn NVM_IO_REQUEUE;\r\nBUG_ON(!(laddr < rrpc->nr_sects));\r\ngp = &rrpc->trans_map[laddr];\r\nif (gp->rblk) {\r\nrqd->ppa_addr = rrpc_ppa_to_gaddr(rrpc->dev, gp);\r\n} else {\r\nBUG_ON(is_gc);\r\nrrpc_unlock_rq(rrpc, rqd);\r\nreturn NVM_IO_DONE;\r\n}\r\nrqd->opcode = NVM_OP_HBREAD;\r\nreturn NVM_IO_OK;\r\n}\r\nstatic int rrpc_write_ppalist_rq(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags, int npages)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_inflight_rq *r = rrpc_get_inflight_rq(rqd);\r\nstruct ppa_addr p;\r\nsector_t laddr = rrpc_get_laddr(bio);\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nint i;\r\nif (!is_gc && rrpc_lock_rq(rrpc, bio, rqd)) {\r\nnvm_dev_dma_free(dev->parent, rqd->ppa_list, rqd->dma_ppa_list);\r\nreturn NVM_IO_REQUEUE;\r\n}\r\nfor (i = 0; i < npages; i++) {\r\np = rrpc_map_page(rrpc, laddr + i, is_gc);\r\nif (p.ppa == ADDR_EMPTY) {\r\nBUG_ON(is_gc);\r\nrrpc_unlock_laddr(rrpc, r);\r\nnvm_dev_dma_free(dev->parent, rqd->ppa_list,\r\nrqd->dma_ppa_list);\r\nrrpc_gc_kick(rrpc);\r\nreturn NVM_IO_REQUEUE;\r\n}\r\nrqd->ppa_list[i] = p;\r\n}\r\nrqd->opcode = NVM_OP_HBWRITE;\r\nreturn NVM_IO_OK;\r\n}\r\nstatic int rrpc_write_rq(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags)\r\n{\r\nstruct ppa_addr p;\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nsector_t laddr = rrpc_get_laddr(bio);\r\nif (!is_gc && rrpc_lock_rq(rrpc, bio, rqd))\r\nreturn NVM_IO_REQUEUE;\r\np = rrpc_map_page(rrpc, laddr, is_gc);\r\nif (p.ppa == ADDR_EMPTY) {\r\nBUG_ON(is_gc);\r\nrrpc_unlock_rq(rrpc, rqd);\r\nrrpc_gc_kick(rrpc);\r\nreturn NVM_IO_REQUEUE;\r\n}\r\nrqd->ppa_addr = p;\r\nrqd->opcode = NVM_OP_HBWRITE;\r\nreturn NVM_IO_OK;\r\n}\r\nstatic int rrpc_setup_rq(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags, uint8_t npages)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nif (npages > 1) {\r\nrqd->ppa_list = nvm_dev_dma_alloc(dev->parent, GFP_KERNEL,\r\n&rqd->dma_ppa_list);\r\nif (!rqd->ppa_list) {\r\npr_err("rrpc: not able to allocate ppa list\n");\r\nreturn NVM_IO_ERR;\r\n}\r\nif (bio_op(bio) == REQ_OP_WRITE)\r\nreturn rrpc_write_ppalist_rq(rrpc, bio, rqd, flags,\r\nnpages);\r\nreturn rrpc_read_ppalist_rq(rrpc, bio, rqd, flags, npages);\r\n}\r\nif (bio_op(bio) == REQ_OP_WRITE)\r\nreturn rrpc_write_rq(rrpc, bio, rqd, flags);\r\nreturn rrpc_read_rq(rrpc, bio, rqd, flags);\r\n}\r\nstatic int rrpc_submit_io(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_rq *rrq = nvm_rq_to_pdu(rqd);\r\nuint8_t nr_pages = rrpc_get_pages(bio);\r\nint bio_size = bio_sectors(bio) << 9;\r\nint err;\r\nif (bio_size < dev->geo.sec_size)\r\nreturn NVM_IO_ERR;\r\nelse if (bio_size > dev->geo.max_rq_size)\r\nreturn NVM_IO_ERR;\r\nerr = rrpc_setup_rq(rrpc, bio, rqd, flags, nr_pages);\r\nif (err)\r\nreturn err;\r\nbio_get(bio);\r\nrqd->bio = bio;\r\nrqd->private = rrpc;\r\nrqd->nr_ppas = nr_pages;\r\nrqd->end_io = rrpc_end_io;\r\nrrq->flags = flags;\r\nerr = nvm_submit_io(dev, rqd);\r\nif (err) {\r\npr_err("rrpc: I/O submission failed: %d\n", err);\r\nbio_put(bio);\r\nif (!(flags & NVM_IOTYPE_GC)) {\r\nrrpc_unlock_rq(rrpc, rqd);\r\nif (rqd->nr_ppas > 1)\r\nnvm_dev_dma_free(dev->parent, rqd->ppa_list,\r\nrqd->dma_ppa_list);\r\n}\r\nreturn NVM_IO_ERR;\r\n}\r\nreturn NVM_IO_OK;\r\n}\r\nstatic blk_qc_t rrpc_make_rq(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct rrpc *rrpc = q->queuedata;\r\nstruct nvm_rq *rqd;\r\nint err;\r\nblk_queue_split(q, &bio);\r\nif (bio_op(bio) == REQ_OP_DISCARD) {\r\nrrpc_discard(rrpc, bio);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nrqd = mempool_alloc(rrpc->rq_pool, GFP_KERNEL);\r\nmemset(rqd, 0, sizeof(struct nvm_rq));\r\nerr = rrpc_submit_io(rrpc, bio, rqd, NVM_IOTYPE_NONE);\r\nswitch (err) {\r\ncase NVM_IO_OK:\r\nreturn BLK_QC_T_NONE;\r\ncase NVM_IO_ERR:\r\nbio_io_error(bio);\r\nbreak;\r\ncase NVM_IO_DONE:\r\nbio_endio(bio);\r\nbreak;\r\ncase NVM_IO_REQUEUE:\r\nspin_lock(&rrpc->bio_lock);\r\nbio_list_add(&rrpc->requeue_bios, bio);\r\nspin_unlock(&rrpc->bio_lock);\r\nqueue_work(rrpc->kgc_wq, &rrpc->ws_requeue);\r\nbreak;\r\n}\r\nmempool_free(rqd, rrpc->rq_pool);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nstatic void rrpc_requeue(struct work_struct *work)\r\n{\r\nstruct rrpc *rrpc = container_of(work, struct rrpc, ws_requeue);\r\nstruct bio_list bios;\r\nstruct bio *bio;\r\nbio_list_init(&bios);\r\nspin_lock(&rrpc->bio_lock);\r\nbio_list_merge(&bios, &rrpc->requeue_bios);\r\nbio_list_init(&rrpc->requeue_bios);\r\nspin_unlock(&rrpc->bio_lock);\r\nwhile ((bio = bio_list_pop(&bios)))\r\nrrpc_make_rq(rrpc->disk->queue, bio);\r\n}\r\nstatic void rrpc_gc_free(struct rrpc *rrpc)\r\n{\r\nif (rrpc->krqd_wq)\r\ndestroy_workqueue(rrpc->krqd_wq);\r\nif (rrpc->kgc_wq)\r\ndestroy_workqueue(rrpc->kgc_wq);\r\n}\r\nstatic int rrpc_gc_init(struct rrpc *rrpc)\r\n{\r\nrrpc->krqd_wq = alloc_workqueue("rrpc-lun", WQ_MEM_RECLAIM|WQ_UNBOUND,\r\nrrpc->nr_luns);\r\nif (!rrpc->krqd_wq)\r\nreturn -ENOMEM;\r\nrrpc->kgc_wq = alloc_workqueue("rrpc-bg", WQ_MEM_RECLAIM, 1);\r\nif (!rrpc->kgc_wq)\r\nreturn -ENOMEM;\r\nsetup_timer(&rrpc->gc_timer, rrpc_gc_timer, (unsigned long)rrpc);\r\nreturn 0;\r\n}\r\nstatic void rrpc_map_free(struct rrpc *rrpc)\r\n{\r\nvfree(rrpc->rev_trans_map);\r\nvfree(rrpc->trans_map);\r\n}\r\nstatic int rrpc_l2p_update(u64 slba, u32 nlb, __le64 *entries, void *private)\r\n{\r\nstruct rrpc *rrpc = (struct rrpc *)private;\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_addr *addr = rrpc->trans_map + slba;\r\nstruct rrpc_rev_addr *raddr = rrpc->rev_trans_map;\r\nstruct rrpc_lun *rlun;\r\nstruct rrpc_block *rblk;\r\nu64 i;\r\nfor (i = 0; i < nlb; i++) {\r\nstruct ppa_addr gaddr;\r\nu64 pba = le64_to_cpu(entries[i]);\r\nunsigned int mod;\r\nif (unlikely(pba >= dev->total_secs && pba != U64_MAX)) {\r\npr_err("nvm: L2P data entry is out of bounds!\n");\r\npr_err("nvm: Maybe loaded an old target L2P\n");\r\nreturn -EINVAL;\r\n}\r\nif (!pba)\r\ncontinue;\r\ndiv_u64_rem(pba, rrpc->nr_sects, &mod);\r\ngaddr = rrpc_recov_addr(dev, pba);\r\nrlun = rrpc_ppa_to_lun(rrpc, gaddr);\r\nif (!rlun) {\r\npr_err("rrpc: l2p corruption on lba %llu\n",\r\nslba + i);\r\nreturn -EINVAL;\r\n}\r\nrblk = &rlun->blocks[gaddr.g.blk];\r\nif (!rblk->state) {\r\nlist_move_tail(&rblk->list, &rlun->used_list);\r\nrblk->state = NVM_BLK_ST_TGT;\r\nrlun->nr_free_blocks--;\r\n}\r\naddr[i].addr = pba;\r\naddr[i].rblk = rblk;\r\nraddr[mod].addr = slba + i;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rrpc_map_init(struct rrpc *rrpc)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nsector_t i;\r\nint ret;\r\nrrpc->trans_map = vzalloc(sizeof(struct rrpc_addr) * rrpc->nr_sects);\r\nif (!rrpc->trans_map)\r\nreturn -ENOMEM;\r\nrrpc->rev_trans_map = vmalloc(sizeof(struct rrpc_rev_addr)\r\n* rrpc->nr_sects);\r\nif (!rrpc->rev_trans_map)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < rrpc->nr_sects; i++) {\r\nstruct rrpc_addr *p = &rrpc->trans_map[i];\r\nstruct rrpc_rev_addr *r = &rrpc->rev_trans_map[i];\r\np->addr = ADDR_EMPTY;\r\nr->addr = ADDR_EMPTY;\r\n}\r\nret = nvm_get_l2p_tbl(dev, rrpc->soffset, rrpc->nr_sects,\r\nrrpc_l2p_update, rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not read L2P table.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rrpc_core_init(struct rrpc *rrpc)\r\n{\r\ndown_write(&rrpc_lock);\r\nif (!rrpc_gcb_cache) {\r\nrrpc_gcb_cache = kmem_cache_create("rrpc_gcb",\r\nsizeof(struct rrpc_block_gc), 0, 0, NULL);\r\nif (!rrpc_gcb_cache) {\r\nup_write(&rrpc_lock);\r\nreturn -ENOMEM;\r\n}\r\nrrpc_rq_cache = kmem_cache_create("rrpc_rq",\r\nsizeof(struct nvm_rq) + sizeof(struct rrpc_rq),\r\n0, 0, NULL);\r\nif (!rrpc_rq_cache) {\r\nkmem_cache_destroy(rrpc_gcb_cache);\r\nup_write(&rrpc_lock);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nup_write(&rrpc_lock);\r\nrrpc->page_pool = mempool_create_page_pool(PAGE_POOL_SIZE, 0);\r\nif (!rrpc->page_pool)\r\nreturn -ENOMEM;\r\nrrpc->gcb_pool = mempool_create_slab_pool(rrpc->dev->geo.nr_luns,\r\nrrpc_gcb_cache);\r\nif (!rrpc->gcb_pool)\r\nreturn -ENOMEM;\r\nrrpc->rq_pool = mempool_create_slab_pool(64, rrpc_rq_cache);\r\nif (!rrpc->rq_pool)\r\nreturn -ENOMEM;\r\nspin_lock_init(&rrpc->inflights.lock);\r\nINIT_LIST_HEAD(&rrpc->inflights.reqs);\r\nreturn 0;\r\n}\r\nstatic void rrpc_core_free(struct rrpc *rrpc)\r\n{\r\nmempool_destroy(rrpc->page_pool);\r\nmempool_destroy(rrpc->gcb_pool);\r\nmempool_destroy(rrpc->rq_pool);\r\n}\r\nstatic void rrpc_luns_free(struct rrpc *rrpc)\r\n{\r\nstruct rrpc_lun *rlun;\r\nint i;\r\nif (!rrpc->luns)\r\nreturn;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nrlun = &rrpc->luns[i];\r\nvfree(rlun->blocks);\r\n}\r\nkfree(rrpc->luns);\r\n}\r\nstatic int rrpc_bb_discovery(struct nvm_tgt_dev *dev, struct rrpc_lun *rlun)\r\n{\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct rrpc_block *rblk;\r\nstruct ppa_addr ppa;\r\nu8 *blks;\r\nint nr_blks;\r\nint i;\r\nint ret;\r\nif (!dev->parent->ops->get_bb_tbl)\r\nreturn 0;\r\nnr_blks = geo->blks_per_lun * geo->plane_mode;\r\nblks = kmalloc(nr_blks, GFP_KERNEL);\r\nif (!blks)\r\nreturn -ENOMEM;\r\nppa.ppa = 0;\r\nppa.g.ch = rlun->bppa.g.ch;\r\nppa.g.lun = rlun->bppa.g.lun;\r\nret = nvm_get_tgt_bb_tbl(dev, ppa, blks);\r\nif (ret) {\r\npr_err("rrpc: could not get BB table\n");\r\ngoto out;\r\n}\r\nnr_blks = nvm_bb_tbl_fold(dev->parent, blks, nr_blks);\r\nif (nr_blks < 0) {\r\nret = nr_blks;\r\ngoto out;\r\n}\r\nfor (i = 0; i < nr_blks; i++) {\r\nif (blks[i] == NVM_BLK_T_FREE)\r\ncontinue;\r\nrblk = &rlun->blocks[i];\r\nlist_move_tail(&rblk->list, &rlun->bb_list);\r\nrblk->state = NVM_BLK_ST_BAD;\r\nrlun->nr_free_blocks--;\r\n}\r\nout:\r\nkfree(blks);\r\nreturn ret;\r\n}\r\nstatic void rrpc_set_lun_ppa(struct rrpc_lun *rlun, struct ppa_addr ppa)\r\n{\r\nrlun->bppa.ppa = 0;\r\nrlun->bppa.g.ch = ppa.g.ch;\r\nrlun->bppa.g.lun = ppa.g.lun;\r\n}\r\nstatic int rrpc_luns_init(struct rrpc *rrpc, struct ppa_addr *luns)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct rrpc_lun *rlun;\r\nint i, j, ret = -EINVAL;\r\nif (geo->sec_per_blk > MAX_INVALID_PAGES_STORAGE * BITS_PER_LONG) {\r\npr_err("rrpc: number of pages per block too high.");\r\nreturn -EINVAL;\r\n}\r\nspin_lock_init(&rrpc->rev_lock);\r\nrrpc->luns = kcalloc(rrpc->nr_luns, sizeof(struct rrpc_lun),\r\nGFP_KERNEL);\r\nif (!rrpc->luns)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nrlun = &rrpc->luns[i];\r\nrlun->id = i;\r\nrrpc_set_lun_ppa(rlun, luns[i]);\r\nrlun->blocks = vzalloc(sizeof(struct rrpc_block) *\r\ngeo->blks_per_lun);\r\nif (!rlun->blocks) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nINIT_LIST_HEAD(&rlun->free_list);\r\nINIT_LIST_HEAD(&rlun->used_list);\r\nINIT_LIST_HEAD(&rlun->bb_list);\r\nfor (j = 0; j < geo->blks_per_lun; j++) {\r\nstruct rrpc_block *rblk = &rlun->blocks[j];\r\nrblk->id = j;\r\nrblk->rlun = rlun;\r\nrblk->state = NVM_BLK_T_FREE;\r\nINIT_LIST_HEAD(&rblk->prio);\r\nINIT_LIST_HEAD(&rblk->list);\r\nspin_lock_init(&rblk->lock);\r\nlist_add_tail(&rblk->list, &rlun->free_list);\r\n}\r\nrlun->rrpc = rrpc;\r\nrlun->nr_free_blocks = geo->blks_per_lun;\r\nrlun->reserved_blocks = 2;\r\nINIT_LIST_HEAD(&rlun->prio_list);\r\nINIT_LIST_HEAD(&rlun->wblk_list);\r\nINIT_WORK(&rlun->ws_gc, rrpc_lun_gc);\r\nspin_lock_init(&rlun->lock);\r\nif (rrpc_bb_discovery(dev, rlun))\r\ngoto err;\r\n}\r\nreturn 0;\r\nerr:\r\nreturn ret;\r\n}\r\nstatic int rrpc_area_init(struct rrpc *rrpc, sector_t *begin)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nsector_t size = rrpc->nr_sects * dev->geo.sec_size;\r\nint ret;\r\nsize >>= 9;\r\nret = nvm_get_area(dev, begin, size);\r\nif (!ret)\r\n*begin >>= (ilog2(dev->geo.sec_size) - 9);\r\nreturn ret;\r\n}\r\nstatic void rrpc_area_free(struct rrpc *rrpc)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nsector_t begin = rrpc->soffset << (ilog2(dev->geo.sec_size) - 9);\r\nnvm_put_area(dev, begin);\r\n}\r\nstatic void rrpc_free(struct rrpc *rrpc)\r\n{\r\nrrpc_gc_free(rrpc);\r\nrrpc_map_free(rrpc);\r\nrrpc_core_free(rrpc);\r\nrrpc_luns_free(rrpc);\r\nrrpc_area_free(rrpc);\r\nkfree(rrpc);\r\n}\r\nstatic void rrpc_exit(void *private)\r\n{\r\nstruct rrpc *rrpc = private;\r\ndel_timer(&rrpc->gc_timer);\r\nflush_workqueue(rrpc->krqd_wq);\r\nflush_workqueue(rrpc->kgc_wq);\r\nrrpc_free(rrpc);\r\n}\r\nstatic sector_t rrpc_capacity(void *private)\r\n{\r\nstruct rrpc *rrpc = private;\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nsector_t reserved, provisioned;\r\nreserved = rrpc->nr_luns * dev->geo.sec_per_blk * 4;\r\nprovisioned = rrpc->nr_sects - reserved;\r\nif (reserved > rrpc->nr_sects) {\r\npr_err("rrpc: not enough space available to expose storage.\n");\r\nreturn 0;\r\n}\r\nsector_div(provisioned, 10);\r\nreturn provisioned * 9 * NR_PHY_IN_LOG;\r\n}\r\nstatic void rrpc_block_map_update(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nint offset;\r\nstruct rrpc_addr *laddr;\r\nu64 bpaddr, paddr, pladdr;\r\nbpaddr = block_to_rel_addr(rrpc, rblk);\r\nfor (offset = 0; offset < dev->geo.sec_per_blk; offset++) {\r\npaddr = bpaddr + offset;\r\npladdr = rrpc->rev_trans_map[paddr].addr;\r\nif (pladdr == ADDR_EMPTY)\r\ncontinue;\r\nladdr = &rrpc->trans_map[pladdr];\r\nif (paddr == laddr->addr) {\r\nladdr->rblk = rblk;\r\n} else {\r\nset_bit(offset, rblk->invalid_pages);\r\nrblk->nr_invalid_pages++;\r\n}\r\n}\r\n}\r\nstatic int rrpc_blocks_init(struct rrpc *rrpc)\r\n{\r\nstruct nvm_tgt_dev *dev = rrpc->dev;\r\nstruct rrpc_lun *rlun;\r\nstruct rrpc_block *rblk;\r\nint lun_iter, blk_iter;\r\nfor (lun_iter = 0; lun_iter < rrpc->nr_luns; lun_iter++) {\r\nrlun = &rrpc->luns[lun_iter];\r\nfor (blk_iter = 0; blk_iter < dev->geo.blks_per_lun;\r\nblk_iter++) {\r\nrblk = &rlun->blocks[blk_iter];\r\nrrpc_block_map_update(rrpc, rblk);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int rrpc_luns_configure(struct rrpc *rrpc)\r\n{\r\nstruct rrpc_lun *rlun;\r\nstruct rrpc_block *rblk;\r\nint i;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nrlun = &rrpc->luns[i];\r\nrblk = rrpc_get_blk(rrpc, rlun, 0);\r\nif (!rblk)\r\ngoto err;\r\nrrpc_set_lun_cur(rlun, rblk, &rlun->cur);\r\nrblk = rrpc_get_blk(rrpc, rlun, 1);\r\nif (!rblk)\r\ngoto err;\r\nrrpc_set_lun_cur(rlun, rblk, &rlun->gc_cur);\r\n}\r\nreturn 0;\r\nerr:\r\nrrpc_put_blks(rrpc);\r\nreturn -EINVAL;\r\n}\r\nstatic void *rrpc_init(struct nvm_tgt_dev *dev, struct gendisk *tdisk,\r\nint flags)\r\n{\r\nstruct request_queue *bqueue = dev->q;\r\nstruct request_queue *tqueue = tdisk->queue;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct rrpc *rrpc;\r\nsector_t soffset;\r\nint ret;\r\nif (!(dev->identity.dom & NVM_RSP_L2P)) {\r\npr_err("nvm: rrpc: device does not support l2p (%x)\n",\r\ndev->identity.dom);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nrrpc = kzalloc(sizeof(struct rrpc), GFP_KERNEL);\r\nif (!rrpc)\r\nreturn ERR_PTR(-ENOMEM);\r\nrrpc->dev = dev;\r\nrrpc->disk = tdisk;\r\nbio_list_init(&rrpc->requeue_bios);\r\nspin_lock_init(&rrpc->bio_lock);\r\nINIT_WORK(&rrpc->ws_requeue, rrpc_requeue);\r\nrrpc->nr_luns = geo->nr_luns;\r\nrrpc->nr_sects = (unsigned long long)geo->sec_per_lun * rrpc->nr_luns;\r\natomic_set(&rrpc->next_lun, -1);\r\nret = rrpc_area_init(rrpc, &soffset);\r\nif (ret < 0) {\r\npr_err("nvm: rrpc: could not initialize area\n");\r\nreturn ERR_PTR(ret);\r\n}\r\nrrpc->soffset = soffset;\r\nret = rrpc_luns_init(rrpc, dev->luns);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize luns\n");\r\ngoto err;\r\n}\r\nret = rrpc_core_init(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize core\n");\r\ngoto err;\r\n}\r\nret = rrpc_map_init(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize maps\n");\r\ngoto err;\r\n}\r\nret = rrpc_blocks_init(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize state for blocks\n");\r\ngoto err;\r\n}\r\nret = rrpc_luns_configure(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: not enough blocks available in LUNs.\n");\r\ngoto err;\r\n}\r\nret = rrpc_gc_init(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize gc\n");\r\ngoto err;\r\n}\r\nblk_queue_logical_block_size(tqueue, queue_physical_block_size(bqueue));\r\nblk_queue_max_hw_sectors(tqueue, queue_max_hw_sectors(bqueue));\r\npr_info("nvm: rrpc initialized with %u luns and %llu pages.\n",\r\nrrpc->nr_luns, (unsigned long long)rrpc->nr_sects);\r\nmod_timer(&rrpc->gc_timer, jiffies + msecs_to_jiffies(10));\r\nreturn rrpc;\r\nerr:\r\nrrpc_free(rrpc);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic int __init rrpc_module_init(void)\r\n{\r\nreturn nvm_register_tgt_type(&tt_rrpc);\r\n}\r\nstatic void rrpc_module_exit(void)\r\n{\r\nnvm_unregister_tgt_type(&tt_rrpc);\r\n}
