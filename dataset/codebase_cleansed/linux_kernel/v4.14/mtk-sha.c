static inline u32 mtk_sha_read(struct mtk_cryp *cryp, u32 offset)\r\n{\r\nreturn readl_relaxed(cryp->base + offset);\r\n}\r\nstatic inline void mtk_sha_write(struct mtk_cryp *cryp,\r\nu32 offset, u32 value)\r\n{\r\nwritel_relaxed(value, cryp->base + offset);\r\n}\r\nstatic inline void mtk_sha_ring_shift(struct mtk_ring *ring,\r\nstruct mtk_desc **cmd_curr,\r\nstruct mtk_desc **res_curr,\r\nint *count)\r\n{\r\n*cmd_curr = ring->cmd_next++;\r\n*res_curr = ring->res_next++;\r\n(*count)++;\r\nif (ring->cmd_next == ring->cmd_base + MTK_DESC_NUM) {\r\nring->cmd_next = ring->cmd_base;\r\nring->res_next = ring->res_base;\r\n}\r\n}\r\nstatic struct mtk_cryp *mtk_sha_find_dev(struct mtk_sha_ctx *tctx)\r\n{\r\nstruct mtk_cryp *cryp = NULL;\r\nstruct mtk_cryp *tmp;\r\nspin_lock_bh(&mtk_sha.lock);\r\nif (!tctx->cryp) {\r\nlist_for_each_entry(tmp, &mtk_sha.dev_list, sha_list) {\r\ncryp = tmp;\r\nbreak;\r\n}\r\ntctx->cryp = cryp;\r\n} else {\r\ncryp = tctx->cryp;\r\n}\r\ntctx->id = cryp->rec;\r\ncryp->rec = !cryp->rec;\r\nspin_unlock_bh(&mtk_sha.lock);\r\nreturn cryp;\r\n}\r\nstatic int mtk_sha_append_sg(struct mtk_sha_reqctx *ctx)\r\n{\r\nsize_t count;\r\nwhile ((ctx->bufcnt < SHA_BUF_SIZE) && ctx->total) {\r\ncount = min(ctx->sg->length - ctx->offset, ctx->total);\r\ncount = min(count, SHA_BUF_SIZE - ctx->bufcnt);\r\nif (count <= 0) {\r\nif ((ctx->sg->length == 0) && !sg_is_last(ctx->sg)) {\r\nctx->sg = sg_next(ctx->sg);\r\ncontinue;\r\n} else {\r\nbreak;\r\n}\r\n}\r\nscatterwalk_map_and_copy(ctx->buffer + ctx->bufcnt, ctx->sg,\r\nctx->offset, count, 0);\r\nctx->bufcnt += count;\r\nctx->offset += count;\r\nctx->total -= count;\r\nif (ctx->offset == ctx->sg->length) {\r\nctx->sg = sg_next(ctx->sg);\r\nif (ctx->sg)\r\nctx->offset = 0;\r\nelse\r\nctx->total = 0;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void mtk_sha_fill_padding(struct mtk_sha_reqctx *ctx, u32 len)\r\n{\r\nu32 index, padlen;\r\nu64 bits[2];\r\nu64 size = ctx->digcnt;\r\nsize += ctx->bufcnt;\r\nsize += len;\r\nbits[1] = cpu_to_be64(size << 3);\r\nbits[0] = cpu_to_be64(size >> 61);\r\nswitch (ctx->flags & SHA_FLAGS_ALGO_MSK) {\r\ncase SHA_FLAGS_SHA384:\r\ncase SHA_FLAGS_SHA512:\r\nindex = ctx->bufcnt & 0x7f;\r\npadlen = (index < 112) ? (112 - index) : ((128 + 112) - index);\r\n*(ctx->buffer + ctx->bufcnt) = 0x80;\r\nmemset(ctx->buffer + ctx->bufcnt + 1, 0, padlen - 1);\r\nmemcpy(ctx->buffer + ctx->bufcnt + padlen, bits, 16);\r\nctx->bufcnt += padlen + 16;\r\nctx->flags |= SHA_FLAGS_PAD;\r\nbreak;\r\ndefault:\r\nindex = ctx->bufcnt & 0x3f;\r\npadlen = (index < 56) ? (56 - index) : ((64 + 56) - index);\r\n*(ctx->buffer + ctx->bufcnt) = 0x80;\r\nmemset(ctx->buffer + ctx->bufcnt + 1, 0, padlen - 1);\r\nmemcpy(ctx->buffer + ctx->bufcnt + padlen, &bits[1], 8);\r\nctx->bufcnt += padlen + 8;\r\nctx->flags |= SHA_FLAGS_PAD;\r\nbreak;\r\n}\r\n}\r\nstatic void mtk_sha_info_init(struct mtk_sha_reqctx *ctx)\r\n{\r\nstruct mtk_sha_info *info = &ctx->info;\r\nctx->ct_hdr = SHA_CT_CTRL_HDR;\r\nctx->ct_size = SHA_CT_SIZE;\r\ninfo->tfm[0] = SHA_TFM_HASH | SHA_TFM_SIZE(SIZE_IN_WORDS(ctx->ds));\r\nswitch (ctx->flags & SHA_FLAGS_ALGO_MSK) {\r\ncase SHA_FLAGS_SHA1:\r\ninfo->tfm[0] |= SHA_TFM_SHA1;\r\nbreak;\r\ncase SHA_FLAGS_SHA224:\r\ninfo->tfm[0] |= SHA_TFM_SHA224;\r\nbreak;\r\ncase SHA_FLAGS_SHA256:\r\ninfo->tfm[0] |= SHA_TFM_SHA256;\r\nbreak;\r\ncase SHA_FLAGS_SHA384:\r\ninfo->tfm[0] |= SHA_TFM_SHA384;\r\nbreak;\r\ncase SHA_FLAGS_SHA512:\r\ninfo->tfm[0] |= SHA_TFM_SHA512;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\ninfo->tfm[1] = SHA_TFM_HASH_STORE;\r\ninfo->ctrl[0] = info->tfm[0] | SHA_TFM_CONTINUE | SHA_TFM_START;\r\ninfo->ctrl[1] = info->tfm[1];\r\ninfo->cmd[0] = SHA_CMD0;\r\ninfo->cmd[1] = SHA_CMD1;\r\ninfo->cmd[2] = SHA_CMD2 | SHA_TFM_DIGEST(SIZE_IN_WORDS(ctx->ds));\r\n}\r\nstatic int mtk_sha_info_update(struct mtk_cryp *cryp,\r\nstruct mtk_sha_rec *sha,\r\nsize_t len1, size_t len2)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(sha->req);\r\nstruct mtk_sha_info *info = &ctx->info;\r\nctx->ct_hdr &= ~SHA_DATA_LEN_MSK;\r\nctx->ct_hdr |= cpu_to_le32(len1 + len2);\r\ninfo->cmd[0] &= ~SHA_DATA_LEN_MSK;\r\ninfo->cmd[0] |= cpu_to_le32(len1 + len2);\r\nif (ctx->digcnt)\r\ninfo->ctrl[0] &= ~SHA_TFM_START;\r\nctx->digcnt += len1;\r\nctx->ct_dma = dma_map_single(cryp->dev, info, sizeof(*info),\r\nDMA_BIDIRECTIONAL);\r\nif (unlikely(dma_mapping_error(cryp->dev, ctx->ct_dma))) {\r\ndev_err(cryp->dev, "dma %zu bytes error\n", sizeof(*info));\r\nreturn -EINVAL;\r\n}\r\nctx->tfm_dma = ctx->ct_dma + sizeof(info->ctrl) + sizeof(info->cmd);\r\nreturn 0;\r\n}\r\nstatic int mtk_sha_finish_hmac(struct ahash_request *req)\r\n{\r\nstruct mtk_sha_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct mtk_sha_hmac_ctx *bctx = tctx->base;\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(req);\r\nSHASH_DESC_ON_STACK(shash, bctx->shash);\r\nshash->tfm = bctx->shash;\r\nshash->flags = 0;\r\nreturn crypto_shash_init(shash) ?:\r\ncrypto_shash_update(shash, bctx->opad, ctx->bs) ?:\r\ncrypto_shash_finup(shash, req->result, ctx->ds, req->result);\r\n}\r\nstatic int mtk_sha_init(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct mtk_sha_ctx *tctx = crypto_ahash_ctx(tfm);\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(req);\r\nctx->flags = 0;\r\nctx->ds = crypto_ahash_digestsize(tfm);\r\nswitch (ctx->ds) {\r\ncase SHA1_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA1;\r\nctx->bs = SHA1_BLOCK_SIZE;\r\nbreak;\r\ncase SHA224_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA224;\r\nctx->bs = SHA224_BLOCK_SIZE;\r\nbreak;\r\ncase SHA256_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA256;\r\nctx->bs = SHA256_BLOCK_SIZE;\r\nbreak;\r\ncase SHA384_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA384;\r\nctx->bs = SHA384_BLOCK_SIZE;\r\nbreak;\r\ncase SHA512_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA512;\r\nctx->bs = SHA512_BLOCK_SIZE;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nctx->bufcnt = 0;\r\nctx->digcnt = 0;\r\nctx->buffer = tctx->buf;\r\nif (tctx->flags & SHA_FLAGS_HMAC) {\r\nstruct mtk_sha_hmac_ctx *bctx = tctx->base;\r\nmemcpy(ctx->buffer, bctx->ipad, ctx->bs);\r\nctx->bufcnt = ctx->bs;\r\nctx->flags |= SHA_FLAGS_HMAC;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtk_sha_xmit(struct mtk_cryp *cryp, struct mtk_sha_rec *sha,\r\ndma_addr_t addr1, size_t len1,\r\ndma_addr_t addr2, size_t len2)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(sha->req);\r\nstruct mtk_ring *ring = cryp->ring[sha->id];\r\nstruct mtk_desc *cmd, *res;\r\nint err, count = 0;\r\nerr = mtk_sha_info_update(cryp, sha, len1, len2);\r\nif (err)\r\nreturn err;\r\nmtk_sha_ring_shift(ring, &cmd, &res, &count);\r\nres->hdr = MTK_DESC_FIRST | MTK_DESC_BUF_LEN(len1);\r\ncmd->hdr = MTK_DESC_FIRST | MTK_DESC_BUF_LEN(len1) |\r\nMTK_DESC_CT_LEN(ctx->ct_size);\r\ncmd->buf = cpu_to_le32(addr1);\r\ncmd->ct = cpu_to_le32(ctx->ct_dma);\r\ncmd->ct_hdr = ctx->ct_hdr;\r\ncmd->tfm = cpu_to_le32(ctx->tfm_dma);\r\nif (len2) {\r\nmtk_sha_ring_shift(ring, &cmd, &res, &count);\r\nres->hdr = MTK_DESC_BUF_LEN(len2);\r\ncmd->hdr = MTK_DESC_BUF_LEN(len2);\r\ncmd->buf = cpu_to_le32(addr2);\r\n}\r\ncmd->hdr |= MTK_DESC_LAST;\r\nres->hdr |= MTK_DESC_LAST;\r\nwmb();\r\nmtk_sha_write(cryp, RDR_PREP_COUNT(sha->id), MTK_DESC_CNT(count));\r\nmtk_sha_write(cryp, CDR_PREP_COUNT(sha->id), MTK_DESC_CNT(count));\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int mtk_sha_dma_map(struct mtk_cryp *cryp,\r\nstruct mtk_sha_rec *sha,\r\nstruct mtk_sha_reqctx *ctx,\r\nsize_t count)\r\n{\r\nctx->dma_addr = dma_map_single(cryp->dev, ctx->buffer,\r\nSHA_BUF_SIZE, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(cryp->dev, ctx->dma_addr))) {\r\ndev_err(cryp->dev, "dma map error\n");\r\nreturn -EINVAL;\r\n}\r\nctx->flags &= ~SHA_FLAGS_SG;\r\nreturn mtk_sha_xmit(cryp, sha, ctx->dma_addr, count, 0, 0);\r\n}\r\nstatic int mtk_sha_update_slow(struct mtk_cryp *cryp,\r\nstruct mtk_sha_rec *sha)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(sha->req);\r\nsize_t count;\r\nu32 final;\r\nmtk_sha_append_sg(ctx);\r\nfinal = (ctx->flags & SHA_FLAGS_FINUP) && !ctx->total;\r\ndev_dbg(cryp->dev, "slow: bufcnt: %zu\n", ctx->bufcnt);\r\nif (final) {\r\nsha->flags |= SHA_FLAGS_FINAL;\r\nmtk_sha_fill_padding(ctx, 0);\r\n}\r\nif (final || (ctx->bufcnt == SHA_BUF_SIZE && ctx->total)) {\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn mtk_sha_dma_map(cryp, sha, ctx, count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtk_sha_update_start(struct mtk_cryp *cryp,\r\nstruct mtk_sha_rec *sha)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(sha->req);\r\nu32 len, final, tail;\r\nstruct scatterlist *sg;\r\nif (!ctx->total)\r\nreturn 0;\r\nif (ctx->bufcnt || ctx->offset)\r\nreturn mtk_sha_update_slow(cryp, sha);\r\nsg = ctx->sg;\r\nif (!IS_ALIGNED(sg->offset, sizeof(u32)))\r\nreturn mtk_sha_update_slow(cryp, sha);\r\nif (!sg_is_last(sg) && !IS_ALIGNED(sg->length, ctx->bs))\r\nreturn mtk_sha_update_slow(cryp, sha);\r\nlen = min(ctx->total, sg->length);\r\nif (sg_is_last(sg)) {\r\nif (!(ctx->flags & SHA_FLAGS_FINUP)) {\r\ntail = len & (ctx->bs - 1);\r\nlen -= tail;\r\n}\r\n}\r\nctx->total -= len;\r\nctx->offset = len;\r\nfinal = (ctx->flags & SHA_FLAGS_FINUP) && !ctx->total;\r\nif (final) {\r\nsize_t count;\r\ntail = len & (ctx->bs - 1);\r\nlen -= tail;\r\nctx->total += tail;\r\nctx->offset = len;\r\nsg = ctx->sg;\r\nmtk_sha_append_sg(ctx);\r\nmtk_sha_fill_padding(ctx, len);\r\nctx->dma_addr = dma_map_single(cryp->dev, ctx->buffer,\r\nSHA_BUF_SIZE, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(cryp->dev, ctx->dma_addr))) {\r\ndev_err(cryp->dev, "dma map bytes error\n");\r\nreturn -EINVAL;\r\n}\r\nsha->flags |= SHA_FLAGS_FINAL;\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nif (len == 0) {\r\nctx->flags &= ~SHA_FLAGS_SG;\r\nreturn mtk_sha_xmit(cryp, sha, ctx->dma_addr,\r\ncount, 0, 0);\r\n} else {\r\nctx->sg = sg;\r\nif (!dma_map_sg(cryp->dev, ctx->sg, 1, DMA_TO_DEVICE)) {\r\ndev_err(cryp->dev, "dma_map_sg error\n");\r\nreturn -EINVAL;\r\n}\r\nctx->flags |= SHA_FLAGS_SG;\r\nreturn mtk_sha_xmit(cryp, sha, sg_dma_address(ctx->sg),\r\nlen, ctx->dma_addr, count);\r\n}\r\n}\r\nif (!dma_map_sg(cryp->dev, ctx->sg, 1, DMA_TO_DEVICE)) {\r\ndev_err(cryp->dev, "dma_map_sg error\n");\r\nreturn -EINVAL;\r\n}\r\nctx->flags |= SHA_FLAGS_SG;\r\nreturn mtk_sha_xmit(cryp, sha, sg_dma_address(ctx->sg),\r\nlen, 0, 0);\r\n}\r\nstatic int mtk_sha_final_req(struct mtk_cryp *cryp,\r\nstruct mtk_sha_rec *sha)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(sha->req);\r\nsize_t count;\r\nmtk_sha_fill_padding(ctx, 0);\r\nsha->flags |= SHA_FLAGS_FINAL;\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn mtk_sha_dma_map(cryp, sha, ctx, count);\r\n}\r\nstatic int mtk_sha_finish(struct ahash_request *req)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(req);\r\n__le32 *digest = ctx->info.digest;\r\nu32 *result = (u32 *)req->result;\r\nint i;\r\nfor (i = 0; i < SIZE_IN_WORDS(ctx->ds); i++)\r\nresult[i] = le32_to_cpu(digest[i]);\r\nif (ctx->flags & SHA_FLAGS_HMAC)\r\nreturn mtk_sha_finish_hmac(req);\r\nreturn 0;\r\n}\r\nstatic void mtk_sha_finish_req(struct mtk_cryp *cryp,\r\nstruct mtk_sha_rec *sha,\r\nint err)\r\n{\r\nif (likely(!err && (SHA_FLAGS_FINAL & sha->flags)))\r\nerr = mtk_sha_finish(sha->req);\r\nsha->flags &= ~(SHA_FLAGS_BUSY | SHA_FLAGS_FINAL);\r\nsha->req->base.complete(&sha->req->base, err);\r\ntasklet_schedule(&sha->queue_task);\r\n}\r\nstatic int mtk_sha_handle_queue(struct mtk_cryp *cryp, u8 id,\r\nstruct ahash_request *req)\r\n{\r\nstruct mtk_sha_rec *sha = cryp->sha[id];\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct mtk_sha_reqctx *ctx;\r\nunsigned long flags;\r\nint err = 0, ret = 0;\r\nspin_lock_irqsave(&sha->lock, flags);\r\nif (req)\r\nret = ahash_enqueue_request(&sha->queue, req);\r\nif (SHA_FLAGS_BUSY & sha->flags) {\r\nspin_unlock_irqrestore(&sha->lock, flags);\r\nreturn ret;\r\n}\r\nbacklog = crypto_get_backlog(&sha->queue);\r\nasync_req = crypto_dequeue_request(&sha->queue);\r\nif (async_req)\r\nsha->flags |= SHA_FLAGS_BUSY;\r\nspin_unlock_irqrestore(&sha->lock, flags);\r\nif (!async_req)\r\nreturn ret;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ahash_request_cast(async_req);\r\nctx = ahash_request_ctx(req);\r\nsha->req = req;\r\nmtk_sha_info_init(ctx);\r\nif (ctx->op == SHA_OP_UPDATE) {\r\nerr = mtk_sha_update_start(cryp, sha);\r\nif (err != -EINPROGRESS && (ctx->flags & SHA_FLAGS_FINUP))\r\nerr = mtk_sha_final_req(cryp, sha);\r\n} else if (ctx->op == SHA_OP_FINAL) {\r\nerr = mtk_sha_final_req(cryp, sha);\r\n}\r\nif (unlikely(err != -EINPROGRESS))\r\nmtk_sha_finish_req(cryp, sha, err);\r\nreturn ret;\r\n}\r\nstatic int mtk_sha_enqueue(struct ahash_request *req, u32 op)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct mtk_sha_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\r\nctx->op = op;\r\nreturn mtk_sha_handle_queue(tctx->cryp, tctx->id, req);\r\n}\r\nstatic void mtk_sha_unmap(struct mtk_cryp *cryp, struct mtk_sha_rec *sha)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(sha->req);\r\ndma_unmap_single(cryp->dev, ctx->ct_dma, sizeof(ctx->info),\r\nDMA_BIDIRECTIONAL);\r\nif (ctx->flags & SHA_FLAGS_SG) {\r\ndma_unmap_sg(cryp->dev, ctx->sg, 1, DMA_TO_DEVICE);\r\nif (ctx->sg->length == ctx->offset) {\r\nctx->sg = sg_next(ctx->sg);\r\nif (ctx->sg)\r\nctx->offset = 0;\r\n}\r\nif (ctx->flags & SHA_FLAGS_PAD) {\r\ndma_unmap_single(cryp->dev, ctx->dma_addr,\r\nSHA_BUF_SIZE, DMA_TO_DEVICE);\r\n}\r\n} else\r\ndma_unmap_single(cryp->dev, ctx->dma_addr,\r\nSHA_BUF_SIZE, DMA_TO_DEVICE);\r\n}\r\nstatic void mtk_sha_complete(struct mtk_cryp *cryp,\r\nstruct mtk_sha_rec *sha)\r\n{\r\nint err = 0;\r\nerr = mtk_sha_update_start(cryp, sha);\r\nif (err != -EINPROGRESS)\r\nmtk_sha_finish_req(cryp, sha, err);\r\n}\r\nstatic int mtk_sha_update(struct ahash_request *req)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(req);\r\nctx->total = req->nbytes;\r\nctx->sg = req->src;\r\nctx->offset = 0;\r\nif ((ctx->bufcnt + ctx->total < SHA_BUF_SIZE) &&\r\n!(ctx->flags & SHA_FLAGS_FINUP))\r\nreturn mtk_sha_append_sg(ctx);\r\nreturn mtk_sha_enqueue(req, SHA_OP_UPDATE);\r\n}\r\nstatic int mtk_sha_final(struct ahash_request *req)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(req);\r\nctx->flags |= SHA_FLAGS_FINUP;\r\nif (ctx->flags & SHA_FLAGS_PAD)\r\nreturn mtk_sha_finish(req);\r\nreturn mtk_sha_enqueue(req, SHA_OP_FINAL);\r\n}\r\nstatic int mtk_sha_finup(struct ahash_request *req)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(req);\r\nint err1, err2;\r\nctx->flags |= SHA_FLAGS_FINUP;\r\nerr1 = mtk_sha_update(req);\r\nif (err1 == -EINPROGRESS || err1 == -EBUSY)\r\nreturn err1;\r\nerr2 = mtk_sha_final(req);\r\nreturn err1 ?: err2;\r\n}\r\nstatic int mtk_sha_digest(struct ahash_request *req)\r\n{\r\nreturn mtk_sha_init(req) ?: mtk_sha_finup(req);\r\n}\r\nstatic int mtk_sha_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nu32 keylen)\r\n{\r\nstruct mtk_sha_ctx *tctx = crypto_ahash_ctx(tfm);\r\nstruct mtk_sha_hmac_ctx *bctx = tctx->base;\r\nsize_t bs = crypto_shash_blocksize(bctx->shash);\r\nsize_t ds = crypto_shash_digestsize(bctx->shash);\r\nint err, i;\r\nSHASH_DESC_ON_STACK(shash, bctx->shash);\r\nshash->tfm = bctx->shash;\r\nshash->flags = crypto_shash_get_flags(bctx->shash) &\r\nCRYPTO_TFM_REQ_MAY_SLEEP;\r\nif (keylen > bs) {\r\nerr = crypto_shash_digest(shash, key, keylen, bctx->ipad);\r\nif (err)\r\nreturn err;\r\nkeylen = ds;\r\n} else {\r\nmemcpy(bctx->ipad, key, keylen);\r\n}\r\nmemset(bctx->ipad + keylen, 0, bs - keylen);\r\nmemcpy(bctx->opad, bctx->ipad, bs);\r\nfor (i = 0; i < bs; i++) {\r\nbctx->ipad[i] ^= HMAC_IPAD_VALUE;\r\nbctx->opad[i] ^= HMAC_OPAD_VALUE;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtk_sha_export(struct ahash_request *req, void *out)\r\n{\r\nconst struct mtk_sha_reqctx *ctx = ahash_request_ctx(req);\r\nmemcpy(out, ctx, sizeof(*ctx));\r\nreturn 0;\r\n}\r\nstatic int mtk_sha_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct mtk_sha_reqctx *ctx = ahash_request_ctx(req);\r\nmemcpy(ctx, in, sizeof(*ctx));\r\nreturn 0;\r\n}\r\nstatic int mtk_sha_cra_init_alg(struct crypto_tfm *tfm,\r\nconst char *alg_base)\r\n{\r\nstruct mtk_sha_ctx *tctx = crypto_tfm_ctx(tfm);\r\nstruct mtk_cryp *cryp = NULL;\r\ncryp = mtk_sha_find_dev(tctx);\r\nif (!cryp)\r\nreturn -ENODEV;\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct mtk_sha_reqctx));\r\nif (alg_base) {\r\nstruct mtk_sha_hmac_ctx *bctx = tctx->base;\r\ntctx->flags |= SHA_FLAGS_HMAC;\r\nbctx->shash = crypto_alloc_shash(alg_base, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(bctx->shash)) {\r\npr_err("base driver %s could not be loaded.\n",\r\nalg_base);\r\nreturn PTR_ERR(bctx->shash);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int mtk_sha_cra_init(struct crypto_tfm *tfm)\r\n{\r\nreturn mtk_sha_cra_init_alg(tfm, NULL);\r\n}\r\nstatic int mtk_sha_cra_sha1_init(struct crypto_tfm *tfm)\r\n{\r\nreturn mtk_sha_cra_init_alg(tfm, "sha1");\r\n}\r\nstatic int mtk_sha_cra_sha224_init(struct crypto_tfm *tfm)\r\n{\r\nreturn mtk_sha_cra_init_alg(tfm, "sha224");\r\n}\r\nstatic int mtk_sha_cra_sha256_init(struct crypto_tfm *tfm)\r\n{\r\nreturn mtk_sha_cra_init_alg(tfm, "sha256");\r\n}\r\nstatic int mtk_sha_cra_sha384_init(struct crypto_tfm *tfm)\r\n{\r\nreturn mtk_sha_cra_init_alg(tfm, "sha384");\r\n}\r\nstatic int mtk_sha_cra_sha512_init(struct crypto_tfm *tfm)\r\n{\r\nreturn mtk_sha_cra_init_alg(tfm, "sha512");\r\n}\r\nstatic void mtk_sha_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct mtk_sha_ctx *tctx = crypto_tfm_ctx(tfm);\r\nif (tctx->flags & SHA_FLAGS_HMAC) {\r\nstruct mtk_sha_hmac_ctx *bctx = tctx->base;\r\ncrypto_free_shash(bctx->shash);\r\n}\r\n}\r\nstatic void mtk_sha_queue_task(unsigned long data)\r\n{\r\nstruct mtk_sha_rec *sha = (struct mtk_sha_rec *)data;\r\nmtk_sha_handle_queue(sha->cryp, sha->id - MTK_RING2, NULL);\r\n}\r\nstatic void mtk_sha_done_task(unsigned long data)\r\n{\r\nstruct mtk_sha_rec *sha = (struct mtk_sha_rec *)data;\r\nstruct mtk_cryp *cryp = sha->cryp;\r\nmtk_sha_unmap(cryp, sha);\r\nmtk_sha_complete(cryp, sha);\r\n}\r\nstatic irqreturn_t mtk_sha_irq(int irq, void *dev_id)\r\n{\r\nstruct mtk_sha_rec *sha = (struct mtk_sha_rec *)dev_id;\r\nstruct mtk_cryp *cryp = sha->cryp;\r\nu32 val = mtk_sha_read(cryp, RDR_STAT(sha->id));\r\nmtk_sha_write(cryp, RDR_STAT(sha->id), val);\r\nif (likely((SHA_FLAGS_BUSY & sha->flags))) {\r\nmtk_sha_write(cryp, RDR_PROC_COUNT(sha->id), MTK_CNT_RST);\r\nmtk_sha_write(cryp, RDR_THRESH(sha->id),\r\nMTK_RDR_PROC_THRESH | MTK_RDR_PROC_MODE);\r\ntasklet_schedule(&sha->done_task);\r\n} else {\r\ndev_warn(cryp->dev, "SHA interrupt when no active requests.\n");\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int mtk_sha_record_init(struct mtk_cryp *cryp)\r\n{\r\nstruct mtk_sha_rec **sha = cryp->sha;\r\nint i, err = -ENOMEM;\r\nfor (i = 0; i < MTK_REC_NUM; i++) {\r\nsha[i] = kzalloc(sizeof(**sha), GFP_KERNEL);\r\nif (!sha[i])\r\ngoto err_cleanup;\r\nsha[i]->cryp = cryp;\r\nspin_lock_init(&sha[i]->lock);\r\ncrypto_init_queue(&sha[i]->queue, SHA_QUEUE_SIZE);\r\ntasklet_init(&sha[i]->queue_task, mtk_sha_queue_task,\r\n(unsigned long)sha[i]);\r\ntasklet_init(&sha[i]->done_task, mtk_sha_done_task,\r\n(unsigned long)sha[i]);\r\n}\r\nsha[0]->id = MTK_RING2;\r\nsha[1]->id = MTK_RING3;\r\ncryp->rec = 1;\r\nreturn 0;\r\nerr_cleanup:\r\nfor (; i--; )\r\nkfree(sha[i]);\r\nreturn err;\r\n}\r\nstatic void mtk_sha_record_free(struct mtk_cryp *cryp)\r\n{\r\nint i;\r\nfor (i = 0; i < MTK_REC_NUM; i++) {\r\ntasklet_kill(&cryp->sha[i]->done_task);\r\ntasklet_kill(&cryp->sha[i]->queue_task);\r\nkfree(cryp->sha[i]);\r\n}\r\n}\r\nstatic void mtk_sha_unregister_algs(void)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(algs_sha1_sha224_sha256); i++)\r\ncrypto_unregister_ahash(&algs_sha1_sha224_sha256[i]);\r\nfor (i = 0; i < ARRAY_SIZE(algs_sha384_sha512); i++)\r\ncrypto_unregister_ahash(&algs_sha384_sha512[i]);\r\n}\r\nstatic int mtk_sha_register_algs(void)\r\n{\r\nint err, i;\r\nfor (i = 0; i < ARRAY_SIZE(algs_sha1_sha224_sha256); i++) {\r\nerr = crypto_register_ahash(&algs_sha1_sha224_sha256[i]);\r\nif (err)\r\ngoto err_sha_224_256_algs;\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(algs_sha384_sha512); i++) {\r\nerr = crypto_register_ahash(&algs_sha384_sha512[i]);\r\nif (err)\r\ngoto err_sha_384_512_algs;\r\n}\r\nreturn 0;\r\nerr_sha_384_512_algs:\r\nfor (; i--; )\r\ncrypto_unregister_ahash(&algs_sha384_sha512[i]);\r\ni = ARRAY_SIZE(algs_sha1_sha224_sha256);\r\nerr_sha_224_256_algs:\r\nfor (; i--; )\r\ncrypto_unregister_ahash(&algs_sha1_sha224_sha256[i]);\r\nreturn err;\r\n}\r\nint mtk_hash_alg_register(struct mtk_cryp *cryp)\r\n{\r\nint err;\r\nINIT_LIST_HEAD(&cryp->sha_list);\r\nerr = mtk_sha_record_init(cryp);\r\nif (err)\r\ngoto err_record;\r\nerr = devm_request_irq(cryp->dev, cryp->irq[MTK_RING2], mtk_sha_irq,\r\n0, "mtk-sha", cryp->sha[0]);\r\nif (err) {\r\ndev_err(cryp->dev, "unable to request sha irq0.\n");\r\ngoto err_res;\r\n}\r\nerr = devm_request_irq(cryp->dev, cryp->irq[MTK_RING3], mtk_sha_irq,\r\n0, "mtk-sha", cryp->sha[1]);\r\nif (err) {\r\ndev_err(cryp->dev, "unable to request sha irq1.\n");\r\ngoto err_res;\r\n}\r\nmtk_sha_write(cryp, AIC_ENABLE_SET(MTK_RING2), MTK_IRQ_RDR2);\r\nmtk_sha_write(cryp, AIC_ENABLE_SET(MTK_RING3), MTK_IRQ_RDR3);\r\nspin_lock(&mtk_sha.lock);\r\nlist_add_tail(&cryp->sha_list, &mtk_sha.dev_list);\r\nspin_unlock(&mtk_sha.lock);\r\nerr = mtk_sha_register_algs();\r\nif (err)\r\ngoto err_algs;\r\nreturn 0;\r\nerr_algs:\r\nspin_lock(&mtk_sha.lock);\r\nlist_del(&cryp->sha_list);\r\nspin_unlock(&mtk_sha.lock);\r\nerr_res:\r\nmtk_sha_record_free(cryp);\r\nerr_record:\r\ndev_err(cryp->dev, "mtk-sha initialization failed.\n");\r\nreturn err;\r\n}\r\nvoid mtk_hash_alg_release(struct mtk_cryp *cryp)\r\n{\r\nspin_lock(&mtk_sha.lock);\r\nlist_del(&cryp->sha_list);\r\nspin_unlock(&mtk_sha.lock);\r\nmtk_sha_unregister_algs();\r\nmtk_sha_record_free(cryp);\r\n}
