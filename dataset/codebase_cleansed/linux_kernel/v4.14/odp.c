static int check_parent(struct ib_umem_odp *odp,\r\nstruct mlx5_ib_mr *parent)\r\n{\r\nstruct mlx5_ib_mr *mr = odp->private;\r\nreturn mr && mr->parent == parent && !odp->dying;\r\n}\r\nstatic struct ib_umem_odp *odp_next(struct ib_umem_odp *odp)\r\n{\r\nstruct mlx5_ib_mr *mr = odp->private, *parent = mr->parent;\r\nstruct ib_ucontext *ctx = odp->umem->context;\r\nstruct rb_node *rb;\r\ndown_read(&ctx->umem_rwsem);\r\nwhile (1) {\r\nrb = rb_next(&odp->interval_tree.rb);\r\nif (!rb)\r\ngoto not_found;\r\nodp = rb_entry(rb, struct ib_umem_odp, interval_tree.rb);\r\nif (check_parent(odp, parent))\r\ngoto end;\r\n}\r\nnot_found:\r\nodp = NULL;\r\nend:\r\nup_read(&ctx->umem_rwsem);\r\nreturn odp;\r\n}\r\nstatic struct ib_umem_odp *odp_lookup(struct ib_ucontext *ctx,\r\nu64 start, u64 length,\r\nstruct mlx5_ib_mr *parent)\r\n{\r\nstruct ib_umem_odp *odp;\r\nstruct rb_node *rb;\r\ndown_read(&ctx->umem_rwsem);\r\nodp = rbt_ib_umem_lookup(&ctx->umem_tree, start, length);\r\nif (!odp)\r\ngoto end;\r\nwhile (1) {\r\nif (check_parent(odp, parent))\r\ngoto end;\r\nrb = rb_next(&odp->interval_tree.rb);\r\nif (!rb)\r\ngoto not_found;\r\nodp = rb_entry(rb, struct ib_umem_odp, interval_tree.rb);\r\nif (ib_umem_start(odp->umem) > start + length)\r\ngoto not_found;\r\n}\r\nnot_found:\r\nodp = NULL;\r\nend:\r\nup_read(&ctx->umem_rwsem);\r\nreturn odp;\r\n}\r\nvoid mlx5_odp_populate_klm(struct mlx5_klm *pklm, size_t offset,\r\nsize_t nentries, struct mlx5_ib_mr *mr, int flags)\r\n{\r\nstruct ib_pd *pd = mr->ibmr.pd;\r\nstruct ib_ucontext *ctx = pd->uobject->context;\r\nstruct mlx5_ib_dev *dev = to_mdev(pd->device);\r\nstruct ib_umem_odp *odp;\r\nunsigned long va;\r\nint i;\r\nif (flags & MLX5_IB_UPD_XLT_ZAP) {\r\nfor (i = 0; i < nentries; i++, pklm++) {\r\npklm->bcount = cpu_to_be32(MLX5_IMR_MTT_SIZE);\r\npklm->key = cpu_to_be32(dev->null_mkey);\r\npklm->va = 0;\r\n}\r\nreturn;\r\n}\r\nodp = odp_lookup(ctx, offset * MLX5_IMR_MTT_SIZE,\r\nnentries * MLX5_IMR_MTT_SIZE, mr);\r\nfor (i = 0; i < nentries; i++, pklm++) {\r\npklm->bcount = cpu_to_be32(MLX5_IMR_MTT_SIZE);\r\nva = (offset + i) * MLX5_IMR_MTT_SIZE;\r\nif (odp && odp->umem->address == va) {\r\nstruct mlx5_ib_mr *mtt = odp->private;\r\npklm->key = cpu_to_be32(mtt->ibmr.lkey);\r\nodp = odp_next(odp);\r\n} else {\r\npklm->key = cpu_to_be32(dev->null_mkey);\r\n}\r\nmlx5_ib_dbg(dev, "[%d] va %lx key %x\n",\r\ni, va, be32_to_cpu(pklm->key));\r\n}\r\n}\r\nstatic void mr_leaf_free_action(struct work_struct *work)\r\n{\r\nstruct ib_umem_odp *odp = container_of(work, struct ib_umem_odp, work);\r\nint idx = ib_umem_start(odp->umem) >> MLX5_IMR_MTT_SHIFT;\r\nstruct mlx5_ib_mr *mr = odp->private, *imr = mr->parent;\r\nmr->parent = NULL;\r\nsynchronize_srcu(&mr->dev->mr_srcu);\r\nib_umem_release(odp->umem);\r\nif (imr->live)\r\nmlx5_ib_update_xlt(imr, idx, 1, 0,\r\nMLX5_IB_UPD_XLT_INDIRECT |\r\nMLX5_IB_UPD_XLT_ATOMIC);\r\nmlx5_mr_cache_free(mr->dev, mr);\r\nif (atomic_dec_and_test(&imr->num_leaf_free))\r\nwake_up(&imr->q_leaf_free);\r\n}\r\nvoid mlx5_ib_invalidate_range(struct ib_umem *umem, unsigned long start,\r\nunsigned long end)\r\n{\r\nstruct mlx5_ib_mr *mr;\r\nconst u64 umr_block_mask = (MLX5_UMR_MTT_ALIGNMENT /\r\nsizeof(struct mlx5_mtt)) - 1;\r\nu64 idx = 0, blk_start_idx = 0;\r\nint in_block = 0;\r\nu64 addr;\r\nif (!umem || !umem->odp_data) {\r\npr_err("invalidation called on NULL umem or non-ODP umem\n");\r\nreturn;\r\n}\r\nmr = umem->odp_data->private;\r\nif (!mr || !mr->ibmr.pd)\r\nreturn;\r\nstart = max_t(u64, ib_umem_start(umem), start);\r\nend = min_t(u64, ib_umem_end(umem), end);\r\nfor (addr = start; addr < end; addr += BIT(umem->page_shift)) {\r\nidx = (addr - ib_umem_start(umem)) >> umem->page_shift;\r\nif (umem->odp_data->dma_list[idx] &\r\n(ODP_READ_ALLOWED_BIT | ODP_WRITE_ALLOWED_BIT)) {\r\nif (!in_block) {\r\nblk_start_idx = idx;\r\nin_block = 1;\r\n}\r\n} else {\r\nu64 umr_offset = idx & umr_block_mask;\r\nif (in_block && umr_offset == 0) {\r\nmlx5_ib_update_xlt(mr, blk_start_idx,\r\nidx - blk_start_idx, 0,\r\nMLX5_IB_UPD_XLT_ZAP |\r\nMLX5_IB_UPD_XLT_ATOMIC);\r\nin_block = 0;\r\n}\r\n}\r\n}\r\nif (in_block)\r\nmlx5_ib_update_xlt(mr, blk_start_idx,\r\nidx - blk_start_idx + 1, 0,\r\nMLX5_IB_UPD_XLT_ZAP |\r\nMLX5_IB_UPD_XLT_ATOMIC);\r\nib_umem_odp_unmap_dma_pages(umem, start, end);\r\nif (unlikely(!umem->npages && mr->parent &&\r\n!umem->odp_data->dying)) {\r\nWRITE_ONCE(umem->odp_data->dying, 1);\r\natomic_inc(&mr->parent->num_leaf_free);\r\nschedule_work(&umem->odp_data->work);\r\n}\r\n}\r\nvoid mlx5_ib_internal_fill_odp_caps(struct mlx5_ib_dev *dev)\r\n{\r\nstruct ib_odp_caps *caps = &dev->odp_caps;\r\nmemset(caps, 0, sizeof(*caps));\r\nif (!MLX5_CAP_GEN(dev->mdev, pg))\r\nreturn;\r\ncaps->general_caps = IB_ODP_SUPPORT;\r\nif (MLX5_CAP_GEN(dev->mdev, umr_extended_translation_offset))\r\ndev->odp_max_size = U64_MAX;\r\nelse\r\ndev->odp_max_size = BIT_ULL(MLX5_MAX_UMR_SHIFT + PAGE_SHIFT);\r\nif (MLX5_CAP_ODP(dev->mdev, ud_odp_caps.send))\r\ncaps->per_transport_caps.ud_odp_caps |= IB_ODP_SUPPORT_SEND;\r\nif (MLX5_CAP_ODP(dev->mdev, rc_odp_caps.send))\r\ncaps->per_transport_caps.rc_odp_caps |= IB_ODP_SUPPORT_SEND;\r\nif (MLX5_CAP_ODP(dev->mdev, rc_odp_caps.receive))\r\ncaps->per_transport_caps.rc_odp_caps |= IB_ODP_SUPPORT_RECV;\r\nif (MLX5_CAP_ODP(dev->mdev, rc_odp_caps.write))\r\ncaps->per_transport_caps.rc_odp_caps |= IB_ODP_SUPPORT_WRITE;\r\nif (MLX5_CAP_ODP(dev->mdev, rc_odp_caps.read))\r\ncaps->per_transport_caps.rc_odp_caps |= IB_ODP_SUPPORT_READ;\r\nif (MLX5_CAP_ODP(dev->mdev, rc_odp_caps.atomic))\r\ncaps->per_transport_caps.rc_odp_caps |= IB_ODP_SUPPORT_ATOMIC;\r\nif (MLX5_CAP_GEN(dev->mdev, fixed_buffer_size) &&\r\nMLX5_CAP_GEN(dev->mdev, null_mkey) &&\r\nMLX5_CAP_GEN(dev->mdev, umr_extended_translation_offset))\r\ncaps->general_caps |= IB_ODP_SUPPORT_IMPLICIT;\r\nreturn;\r\n}\r\nstatic void mlx5_ib_page_fault_resume(struct mlx5_ib_dev *dev,\r\nstruct mlx5_pagefault *pfault,\r\nint error)\r\n{\r\nint wq_num = pfault->event_subtype == MLX5_PFAULT_SUBTYPE_WQE ?\r\npfault->wqe.wq_num : pfault->token;\r\nint ret = mlx5_core_page_fault_resume(dev->mdev,\r\npfault->token,\r\nwq_num,\r\npfault->type,\r\nerror);\r\nif (ret)\r\nmlx5_ib_err(dev, "Failed to resolve the page fault on WQ 0x%x\n",\r\nwq_num);\r\n}\r\nstatic struct mlx5_ib_mr *implicit_mr_alloc(struct ib_pd *pd,\r\nstruct ib_umem *umem,\r\nbool ksm, int access_flags)\r\n{\r\nstruct mlx5_ib_dev *dev = to_mdev(pd->device);\r\nstruct mlx5_ib_mr *mr;\r\nint err;\r\nmr = mlx5_mr_cache_alloc(dev, ksm ? MLX5_IMR_KSM_CACHE_ENTRY :\r\nMLX5_IMR_MTT_CACHE_ENTRY);\r\nif (IS_ERR(mr))\r\nreturn mr;\r\nmr->ibmr.pd = pd;\r\nmr->dev = dev;\r\nmr->access_flags = access_flags;\r\nmr->mmkey.iova = 0;\r\nmr->umem = umem;\r\nif (ksm) {\r\nerr = mlx5_ib_update_xlt(mr, 0,\r\nmlx5_imr_ksm_entries,\r\nMLX5_KSM_PAGE_SHIFT,\r\nMLX5_IB_UPD_XLT_INDIRECT |\r\nMLX5_IB_UPD_XLT_ZAP |\r\nMLX5_IB_UPD_XLT_ENABLE);\r\n} else {\r\nerr = mlx5_ib_update_xlt(mr, 0,\r\nMLX5_IMR_MTT_ENTRIES,\r\nPAGE_SHIFT,\r\nMLX5_IB_UPD_XLT_ZAP |\r\nMLX5_IB_UPD_XLT_ENABLE |\r\nMLX5_IB_UPD_XLT_ATOMIC);\r\n}\r\nif (err)\r\ngoto fail;\r\nmr->ibmr.lkey = mr->mmkey.key;\r\nmr->ibmr.rkey = mr->mmkey.key;\r\nmr->live = 1;\r\nmlx5_ib_dbg(dev, "key %x dev %p mr %p\n",\r\nmr->mmkey.key, dev->mdev, mr);\r\nreturn mr;\r\nfail:\r\nmlx5_ib_err(dev, "Failed to register MKEY %d\n", err);\r\nmlx5_mr_cache_free(dev, mr);\r\nreturn ERR_PTR(err);\r\n}\r\nstatic struct ib_umem_odp *implicit_mr_get_data(struct mlx5_ib_mr *mr,\r\nu64 io_virt, size_t bcnt)\r\n{\r\nstruct ib_ucontext *ctx = mr->ibmr.pd->uobject->context;\r\nstruct mlx5_ib_dev *dev = to_mdev(mr->ibmr.pd->device);\r\nstruct ib_umem_odp *odp, *result = NULL;\r\nu64 addr = io_virt & MLX5_IMR_MTT_MASK;\r\nint nentries = 0, start_idx = 0, ret;\r\nstruct mlx5_ib_mr *mtt;\r\nstruct ib_umem *umem;\r\nmutex_lock(&mr->umem->odp_data->umem_mutex);\r\nodp = odp_lookup(ctx, addr, 1, mr);\r\nmlx5_ib_dbg(dev, "io_virt:%llx bcnt:%zx addr:%llx odp:%p\n",\r\nio_virt, bcnt, addr, odp);\r\nnext_mr:\r\nif (likely(odp)) {\r\nif (nentries)\r\nnentries++;\r\n} else {\r\numem = ib_alloc_odp_umem(ctx, addr, MLX5_IMR_MTT_SIZE);\r\nif (IS_ERR(umem)) {\r\nmutex_unlock(&mr->umem->odp_data->umem_mutex);\r\nreturn ERR_CAST(umem);\r\n}\r\nmtt = implicit_mr_alloc(mr->ibmr.pd, umem, 0, mr->access_flags);\r\nif (IS_ERR(mtt)) {\r\nmutex_unlock(&mr->umem->odp_data->umem_mutex);\r\nib_umem_release(umem);\r\nreturn ERR_CAST(mtt);\r\n}\r\nodp = umem->odp_data;\r\nodp->private = mtt;\r\nmtt->umem = umem;\r\nmtt->mmkey.iova = addr;\r\nmtt->parent = mr;\r\nINIT_WORK(&odp->work, mr_leaf_free_action);\r\nif (!nentries)\r\nstart_idx = addr >> MLX5_IMR_MTT_SHIFT;\r\nnentries++;\r\n}\r\nif (likely(!result))\r\nresult = odp;\r\naddr += MLX5_IMR_MTT_SIZE;\r\nif (unlikely(addr < io_virt + bcnt)) {\r\nodp = odp_next(odp);\r\nif (odp && odp->umem->address != addr)\r\nodp = NULL;\r\ngoto next_mr;\r\n}\r\nif (unlikely(nentries)) {\r\nret = mlx5_ib_update_xlt(mr, start_idx, nentries, 0,\r\nMLX5_IB_UPD_XLT_INDIRECT |\r\nMLX5_IB_UPD_XLT_ATOMIC);\r\nif (ret) {\r\nmlx5_ib_err(dev, "Failed to update PAS\n");\r\nresult = ERR_PTR(ret);\r\n}\r\n}\r\nmutex_unlock(&mr->umem->odp_data->umem_mutex);\r\nreturn result;\r\n}\r\nstruct mlx5_ib_mr *mlx5_ib_alloc_implicit_mr(struct mlx5_ib_pd *pd,\r\nint access_flags)\r\n{\r\nstruct ib_ucontext *ctx = pd->ibpd.uobject->context;\r\nstruct mlx5_ib_mr *imr;\r\nstruct ib_umem *umem;\r\numem = ib_umem_get(ctx, 0, 0, IB_ACCESS_ON_DEMAND, 0);\r\nif (IS_ERR(umem))\r\nreturn ERR_CAST(umem);\r\nimr = implicit_mr_alloc(&pd->ibpd, umem, 1, access_flags);\r\nif (IS_ERR(imr)) {\r\nib_umem_release(umem);\r\nreturn ERR_CAST(imr);\r\n}\r\nimr->umem = umem;\r\ninit_waitqueue_head(&imr->q_leaf_free);\r\natomic_set(&imr->num_leaf_free, 0);\r\nreturn imr;\r\n}\r\nstatic int mr_leaf_free(struct ib_umem *umem, u64 start,\r\nu64 end, void *cookie)\r\n{\r\nstruct mlx5_ib_mr *mr = umem->odp_data->private, *imr = cookie;\r\nif (mr->parent != imr)\r\nreturn 0;\r\nib_umem_odp_unmap_dma_pages(umem,\r\nib_umem_start(umem),\r\nib_umem_end(umem));\r\nif (umem->odp_data->dying)\r\nreturn 0;\r\nWRITE_ONCE(umem->odp_data->dying, 1);\r\natomic_inc(&imr->num_leaf_free);\r\nschedule_work(&umem->odp_data->work);\r\nreturn 0;\r\n}\r\nvoid mlx5_ib_free_implicit_mr(struct mlx5_ib_mr *imr)\r\n{\r\nstruct ib_ucontext *ctx = imr->ibmr.pd->uobject->context;\r\ndown_read(&ctx->umem_rwsem);\r\nrbt_ib_umem_for_each_in_range(&ctx->umem_tree, 0, ULLONG_MAX,\r\nmr_leaf_free, imr);\r\nup_read(&ctx->umem_rwsem);\r\nwait_event(imr->q_leaf_free, !atomic_read(&imr->num_leaf_free));\r\n}\r\nstatic int pagefault_mr(struct mlx5_ib_dev *dev, struct mlx5_ib_mr *mr,\r\nu64 io_virt, size_t bcnt, u32 *bytes_mapped)\r\n{\r\nu64 access_mask = ODP_READ_ALLOWED_BIT;\r\nint npages = 0, page_shift, np;\r\nu64 start_idx, page_mask;\r\nstruct ib_umem_odp *odp;\r\nint current_seq;\r\nsize_t size;\r\nint ret;\r\nif (!mr->umem->odp_data->page_list) {\r\nodp = implicit_mr_get_data(mr, io_virt, bcnt);\r\nif (IS_ERR(odp))\r\nreturn PTR_ERR(odp);\r\nmr = odp->private;\r\n} else {\r\nodp = mr->umem->odp_data;\r\n}\r\nnext_mr:\r\nsize = min_t(size_t, bcnt, ib_umem_end(odp->umem) - io_virt);\r\npage_shift = mr->umem->page_shift;\r\npage_mask = ~(BIT(page_shift) - 1);\r\nstart_idx = (io_virt - (mr->mmkey.iova & page_mask)) >> page_shift;\r\nif (mr->umem->writable)\r\naccess_mask |= ODP_WRITE_ALLOWED_BIT;\r\ncurrent_seq = READ_ONCE(odp->notifiers_seq);\r\nsmp_rmb();\r\nret = ib_umem_odp_map_dma_pages(mr->umem, io_virt, size,\r\naccess_mask, current_seq);\r\nif (ret < 0)\r\ngoto out;\r\nnp = ret;\r\nmutex_lock(&odp->umem_mutex);\r\nif (!ib_umem_mmu_notifier_retry(mr->umem, current_seq)) {\r\nret = mlx5_ib_update_xlt(mr, start_idx, np,\r\npage_shift, MLX5_IB_UPD_XLT_ATOMIC);\r\n} else {\r\nret = -EAGAIN;\r\n}\r\nmutex_unlock(&odp->umem_mutex);\r\nif (ret < 0) {\r\nif (ret != -EAGAIN)\r\nmlx5_ib_err(dev, "Failed to update mkey page tables\n");\r\ngoto out;\r\n}\r\nif (bytes_mapped) {\r\nu32 new_mappings = (np << page_shift) -\r\n(io_virt - round_down(io_virt, 1 << page_shift));\r\n*bytes_mapped += min_t(u32, new_mappings, size);\r\n}\r\nnpages += np << (page_shift - PAGE_SHIFT);\r\nbcnt -= size;\r\nif (unlikely(bcnt)) {\r\nstruct ib_umem_odp *next;\r\nio_virt += size;\r\nnext = odp_next(odp);\r\nif (unlikely(!next || next->umem->address != io_virt)) {\r\nmlx5_ib_dbg(dev, "next implicit leaf removed at 0x%llx. got %p\n",\r\nio_virt, next);\r\nreturn -EAGAIN;\r\n}\r\nodp = next;\r\nmr = odp->private;\r\ngoto next_mr;\r\n}\r\nreturn npages;\r\nout:\r\nif (ret == -EAGAIN) {\r\nif (mr->parent || !odp->dying) {\r\nunsigned long timeout =\r\nmsecs_to_jiffies(MMU_NOTIFIER_TIMEOUT);\r\nif (!wait_for_completion_timeout(\r\n&odp->notifier_completion,\r\ntimeout)) {\r\nmlx5_ib_warn(dev, "timeout waiting for mmu notifier. seq %d against %d\n",\r\ncurrent_seq, odp->notifiers_seq);\r\n}\r\n} else {\r\nret = -EFAULT;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int pagefault_single_data_segment(struct mlx5_ib_dev *dev,\r\nu32 key, u64 io_virt, size_t bcnt,\r\nu32 *bytes_committed,\r\nu32 *bytes_mapped)\r\n{\r\nint npages = 0, srcu_key, ret, i, outlen, cur_outlen = 0, depth = 0;\r\nstruct pf_frame *head = NULL, *frame;\r\nstruct mlx5_core_mkey *mmkey;\r\nstruct mlx5_ib_mw *mw;\r\nstruct mlx5_ib_mr *mr;\r\nstruct mlx5_klm *pklm;\r\nu32 *out = NULL;\r\nsize_t offset;\r\nsrcu_key = srcu_read_lock(&dev->mr_srcu);\r\nio_virt += *bytes_committed;\r\nbcnt -= *bytes_committed;\r\nnext_mr:\r\nmmkey = __mlx5_mr_lookup(dev->mdev, mlx5_base_mkey(key));\r\nif (!mmkey || mmkey->key != key) {\r\nmlx5_ib_dbg(dev, "failed to find mkey %x\n", key);\r\nret = -EFAULT;\r\ngoto srcu_unlock;\r\n}\r\nswitch (mmkey->type) {\r\ncase MLX5_MKEY_MR:\r\nmr = container_of(mmkey, struct mlx5_ib_mr, mmkey);\r\nif (!mr->live || !mr->ibmr.pd) {\r\nmlx5_ib_dbg(dev, "got dead MR\n");\r\nret = -EFAULT;\r\ngoto srcu_unlock;\r\n}\r\nret = pagefault_mr(dev, mr, io_virt, bcnt, bytes_mapped);\r\nif (ret < 0)\r\ngoto srcu_unlock;\r\nnpages += ret;\r\nret = 0;\r\nbreak;\r\ncase MLX5_MKEY_MW:\r\nmw = container_of(mmkey, struct mlx5_ib_mw, mmkey);\r\nif (depth >= MLX5_CAP_GEN(dev->mdev, max_indirection)) {\r\nmlx5_ib_dbg(dev, "indirection level exceeded\n");\r\nret = -EFAULT;\r\ngoto srcu_unlock;\r\n}\r\noutlen = MLX5_ST_SZ_BYTES(query_mkey_out) +\r\nsizeof(*pklm) * (mw->ndescs - 2);\r\nif (outlen > cur_outlen) {\r\nkfree(out);\r\nout = kzalloc(outlen, GFP_KERNEL);\r\nif (!out) {\r\nret = -ENOMEM;\r\ngoto srcu_unlock;\r\n}\r\ncur_outlen = outlen;\r\n}\r\npklm = (struct mlx5_klm *)MLX5_ADDR_OF(query_mkey_out, out,\r\nbsf0_klm0_pas_mtt0_1);\r\nret = mlx5_core_query_mkey(dev->mdev, &mw->mmkey, out, outlen);\r\nif (ret)\r\ngoto srcu_unlock;\r\noffset = io_virt - MLX5_GET64(query_mkey_out, out,\r\nmemory_key_mkey_entry.start_addr);\r\nfor (i = 0; bcnt && i < mw->ndescs; i++, pklm++) {\r\nif (offset >= be32_to_cpu(pklm->bcount)) {\r\noffset -= be32_to_cpu(pklm->bcount);\r\ncontinue;\r\n}\r\nframe = kzalloc(sizeof(*frame), GFP_KERNEL);\r\nif (!frame) {\r\nret = -ENOMEM;\r\ngoto srcu_unlock;\r\n}\r\nframe->key = be32_to_cpu(pklm->key);\r\nframe->io_virt = be64_to_cpu(pklm->va) + offset;\r\nframe->bcnt = min_t(size_t, bcnt,\r\nbe32_to_cpu(pklm->bcount) - offset);\r\nframe->depth = depth + 1;\r\nframe->next = head;\r\nhead = frame;\r\nbcnt -= frame->bcnt;\r\n}\r\nbreak;\r\ndefault:\r\nmlx5_ib_dbg(dev, "wrong mkey type %d\n", mmkey->type);\r\nret = -EFAULT;\r\ngoto srcu_unlock;\r\n}\r\nif (head) {\r\nframe = head;\r\nhead = frame->next;\r\nkey = frame->key;\r\nio_virt = frame->io_virt;\r\nbcnt = frame->bcnt;\r\ndepth = frame->depth;\r\nkfree(frame);\r\ngoto next_mr;\r\n}\r\nsrcu_unlock:\r\nwhile (head) {\r\nframe = head;\r\nhead = frame->next;\r\nkfree(frame);\r\n}\r\nkfree(out);\r\nsrcu_read_unlock(&dev->mr_srcu, srcu_key);\r\n*bytes_committed = 0;\r\nreturn ret ? ret : npages;\r\n}\r\nstatic int pagefault_data_segments(struct mlx5_ib_dev *dev,\r\nstruct mlx5_pagefault *pfault,\r\nstruct mlx5_ib_qp *qp, void *wqe,\r\nvoid *wqe_end, u32 *bytes_mapped,\r\nu32 *total_wqe_bytes, int receive_queue)\r\n{\r\nint ret = 0, npages = 0;\r\nu64 io_virt;\r\nu32 key;\r\nu32 byte_count;\r\nsize_t bcnt;\r\nint inline_segment;\r\nif (receive_queue && qp->ibqp.srq)\r\nwqe += sizeof(struct mlx5_wqe_srq_next_seg);\r\nif (bytes_mapped)\r\n*bytes_mapped = 0;\r\nif (total_wqe_bytes)\r\n*total_wqe_bytes = 0;\r\nwhile (wqe < wqe_end) {\r\nstruct mlx5_wqe_data_seg *dseg = wqe;\r\nio_virt = be64_to_cpu(dseg->addr);\r\nkey = be32_to_cpu(dseg->lkey);\r\nbyte_count = be32_to_cpu(dseg->byte_count);\r\ninline_segment = !!(byte_count & MLX5_INLINE_SEG);\r\nbcnt = byte_count & ~MLX5_INLINE_SEG;\r\nif (inline_segment) {\r\nbcnt = bcnt & MLX5_WQE_INLINE_SEG_BYTE_COUNT_MASK;\r\nwqe += ALIGN(sizeof(struct mlx5_wqe_inline_seg) + bcnt,\r\n16);\r\n} else {\r\nwqe += sizeof(*dseg);\r\n}\r\nif (receive_queue && bcnt == 0 && key == MLX5_INVALID_LKEY &&\r\nio_virt == 0)\r\nbreak;\r\nif (!inline_segment && total_wqe_bytes) {\r\n*total_wqe_bytes += bcnt - min_t(size_t, bcnt,\r\npfault->bytes_committed);\r\n}\r\nif (bcnt == 0)\r\nbcnt = 1U << 31;\r\nif (inline_segment || bcnt <= pfault->bytes_committed) {\r\npfault->bytes_committed -=\r\nmin_t(size_t, bcnt,\r\npfault->bytes_committed);\r\ncontinue;\r\n}\r\nret = pagefault_single_data_segment(dev, key, io_virt, bcnt,\r\n&pfault->bytes_committed,\r\nbytes_mapped);\r\nif (ret < 0)\r\nbreak;\r\nnpages += ret;\r\n}\r\nreturn ret < 0 ? ret : npages;\r\n}\r\nstatic int mlx5_ib_mr_initiator_pfault_handler(\r\nstruct mlx5_ib_dev *dev, struct mlx5_pagefault *pfault,\r\nstruct mlx5_ib_qp *qp, void **wqe, void **wqe_end, int wqe_length)\r\n{\r\nstruct mlx5_wqe_ctrl_seg *ctrl = *wqe;\r\nu16 wqe_index = pfault->wqe.wqe_index;\r\nu32 transport_caps;\r\nstruct mlx5_base_av *av;\r\nunsigned ds, opcode;\r\n#if defined(DEBUG)\r\nu32 ctrl_wqe_index, ctrl_qpn;\r\n#endif\r\nu32 qpn = qp->trans_qp.base.mqp.qpn;\r\nds = be32_to_cpu(ctrl->qpn_ds) & MLX5_WQE_CTRL_DS_MASK;\r\nif (ds * MLX5_WQE_DS_UNITS > wqe_length) {\r\nmlx5_ib_err(dev, "Unable to read the complete WQE. ds = 0x%x, ret = 0x%x\n",\r\nds, wqe_length);\r\nreturn -EFAULT;\r\n}\r\nif (ds == 0) {\r\nmlx5_ib_err(dev, "Got WQE with zero DS. wqe_index=%x, qpn=%x\n",\r\nwqe_index, qpn);\r\nreturn -EFAULT;\r\n}\r\n#if defined(DEBUG)\r\nctrl_wqe_index = (be32_to_cpu(ctrl->opmod_idx_opcode) &\r\nMLX5_WQE_CTRL_WQE_INDEX_MASK) >>\r\nMLX5_WQE_CTRL_WQE_INDEX_SHIFT;\r\nif (wqe_index != ctrl_wqe_index) {\r\nmlx5_ib_err(dev, "Got WQE with invalid wqe_index. wqe_index=0x%x, qpn=0x%x ctrl->wqe_index=0x%x\n",\r\nwqe_index, qpn,\r\nctrl_wqe_index);\r\nreturn -EFAULT;\r\n}\r\nctrl_qpn = (be32_to_cpu(ctrl->qpn_ds) & MLX5_WQE_CTRL_QPN_MASK) >>\r\nMLX5_WQE_CTRL_QPN_SHIFT;\r\nif (qpn != ctrl_qpn) {\r\nmlx5_ib_err(dev, "Got WQE with incorrect QP number. wqe_index=0x%x, qpn=0x%x ctrl->qpn=0x%x\n",\r\nwqe_index, qpn,\r\nctrl_qpn);\r\nreturn -EFAULT;\r\n}\r\n#endif\r\n*wqe_end = *wqe + ds * MLX5_WQE_DS_UNITS;\r\n*wqe += sizeof(*ctrl);\r\nopcode = be32_to_cpu(ctrl->opmod_idx_opcode) &\r\nMLX5_WQE_CTRL_OPCODE_MASK;\r\nswitch (qp->ibqp.qp_type) {\r\ncase IB_QPT_RC:\r\ntransport_caps = dev->odp_caps.per_transport_caps.rc_odp_caps;\r\nbreak;\r\ncase IB_QPT_UD:\r\ntransport_caps = dev->odp_caps.per_transport_caps.ud_odp_caps;\r\nbreak;\r\ndefault:\r\nmlx5_ib_err(dev, "ODP fault on QP of an unsupported transport 0x%x\n",\r\nqp->ibqp.qp_type);\r\nreturn -EFAULT;\r\n}\r\nif (unlikely(opcode >= sizeof(mlx5_ib_odp_opcode_cap) /\r\nsizeof(mlx5_ib_odp_opcode_cap[0]) ||\r\n!(transport_caps & mlx5_ib_odp_opcode_cap[opcode]))) {\r\nmlx5_ib_err(dev, "ODP fault on QP of an unsupported opcode 0x%x\n",\r\nopcode);\r\nreturn -EFAULT;\r\n}\r\nif (qp->ibqp.qp_type != IB_QPT_RC) {\r\nav = *wqe;\r\nif (av->dqp_dct & cpu_to_be32(MLX5_EXTENDED_UD_AV))\r\n*wqe += sizeof(struct mlx5_av);\r\nelse\r\n*wqe += sizeof(struct mlx5_base_av);\r\n}\r\nswitch (opcode) {\r\ncase MLX5_OPCODE_RDMA_WRITE:\r\ncase MLX5_OPCODE_RDMA_WRITE_IMM:\r\ncase MLX5_OPCODE_RDMA_READ:\r\n*wqe += sizeof(struct mlx5_wqe_raddr_seg);\r\nbreak;\r\ncase MLX5_OPCODE_ATOMIC_CS:\r\ncase MLX5_OPCODE_ATOMIC_FA:\r\n*wqe += sizeof(struct mlx5_wqe_raddr_seg);\r\n*wqe += sizeof(struct mlx5_wqe_atomic_seg);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mlx5_ib_mr_responder_pfault_handler(\r\nstruct mlx5_ib_dev *dev, struct mlx5_pagefault *pfault,\r\nstruct mlx5_ib_qp *qp, void **wqe, void **wqe_end, int wqe_length)\r\n{\r\nstruct mlx5_ib_wq *wq = &qp->rq;\r\nint wqe_size = 1 << wq->wqe_shift;\r\nif (qp->ibqp.srq) {\r\nmlx5_ib_err(dev, "ODP fault on SRQ is not supported\n");\r\nreturn -EFAULT;\r\n}\r\nif (qp->wq_sig) {\r\nmlx5_ib_err(dev, "ODP fault with WQE signatures is not supported\n");\r\nreturn -EFAULT;\r\n}\r\nif (wqe_size > wqe_length) {\r\nmlx5_ib_err(dev, "Couldn't read all of the receive WQE's content\n");\r\nreturn -EFAULT;\r\n}\r\nswitch (qp->ibqp.qp_type) {\r\ncase IB_QPT_RC:\r\nif (!(dev->odp_caps.per_transport_caps.rc_odp_caps &\r\nIB_ODP_SUPPORT_RECV))\r\ngoto invalid_transport_or_opcode;\r\nbreak;\r\ndefault:\r\ninvalid_transport_or_opcode:\r\nmlx5_ib_err(dev, "ODP fault on QP of an unsupported transport. transport: 0x%x\n",\r\nqp->ibqp.qp_type);\r\nreturn -EFAULT;\r\n}\r\n*wqe_end = *wqe + wqe_size;\r\nreturn 0;\r\n}\r\nstatic struct mlx5_ib_qp *mlx5_ib_odp_find_qp(struct mlx5_ib_dev *dev,\r\nu32 wq_num)\r\n{\r\nstruct mlx5_core_qp *mqp = __mlx5_qp_lookup(dev->mdev, wq_num);\r\nif (!mqp) {\r\nmlx5_ib_err(dev, "QPN 0x%6x not found\n", wq_num);\r\nreturn NULL;\r\n}\r\nreturn to_mibqp(mqp);\r\n}\r\nstatic void mlx5_ib_mr_wqe_pfault_handler(struct mlx5_ib_dev *dev,\r\nstruct mlx5_pagefault *pfault)\r\n{\r\nint ret;\r\nvoid *wqe, *wqe_end;\r\nu32 bytes_mapped, total_wqe_bytes;\r\nchar *buffer = NULL;\r\nint resume_with_error = 1;\r\nu16 wqe_index = pfault->wqe.wqe_index;\r\nint requestor = pfault->type & MLX5_PFAULT_REQUESTOR;\r\nstruct mlx5_ib_qp *qp;\r\nbuffer = (char *)__get_free_page(GFP_KERNEL);\r\nif (!buffer) {\r\nmlx5_ib_err(dev, "Error allocating memory for IO page fault handling.\n");\r\ngoto resolve_page_fault;\r\n}\r\nqp = mlx5_ib_odp_find_qp(dev, pfault->wqe.wq_num);\r\nif (!qp)\r\ngoto resolve_page_fault;\r\nret = mlx5_ib_read_user_wqe(qp, requestor, wqe_index, buffer,\r\nPAGE_SIZE, &qp->trans_qp.base);\r\nif (ret < 0) {\r\nmlx5_ib_err(dev, "Failed reading a WQE following page fault, error=%d, wqe_index=%x, qpn=%x\n",\r\nret, wqe_index, pfault->token);\r\ngoto resolve_page_fault;\r\n}\r\nwqe = buffer;\r\nif (requestor)\r\nret = mlx5_ib_mr_initiator_pfault_handler(dev, pfault, qp, &wqe,\r\n&wqe_end, ret);\r\nelse\r\nret = mlx5_ib_mr_responder_pfault_handler(dev, pfault, qp, &wqe,\r\n&wqe_end, ret);\r\nif (ret < 0)\r\ngoto resolve_page_fault;\r\nif (wqe >= wqe_end) {\r\nmlx5_ib_err(dev, "ODP fault on invalid WQE.\n");\r\ngoto resolve_page_fault;\r\n}\r\nret = pagefault_data_segments(dev, pfault, qp, wqe, wqe_end,\r\n&bytes_mapped, &total_wqe_bytes,\r\n!requestor);\r\nif (ret == -EAGAIN) {\r\nresume_with_error = 0;\r\ngoto resolve_page_fault;\r\n} else if (ret < 0 || total_wqe_bytes > bytes_mapped) {\r\ngoto resolve_page_fault;\r\n}\r\nresume_with_error = 0;\r\nresolve_page_fault:\r\nmlx5_ib_page_fault_resume(dev, pfault, resume_with_error);\r\nmlx5_ib_dbg(dev, "PAGE FAULT completed. QP 0x%x resume_with_error=%d, type: 0x%x\n",\r\npfault->wqe.wq_num, resume_with_error,\r\npfault->type);\r\nfree_page((unsigned long)buffer);\r\n}\r\nstatic int pages_in_range(u64 address, u32 length)\r\n{\r\nreturn (ALIGN(address + length, PAGE_SIZE) -\r\n(address & PAGE_MASK)) >> PAGE_SHIFT;\r\n}\r\nstatic void mlx5_ib_mr_rdma_pfault_handler(struct mlx5_ib_dev *dev,\r\nstruct mlx5_pagefault *pfault)\r\n{\r\nu64 address;\r\nu32 length;\r\nu32 prefetch_len = pfault->bytes_committed;\r\nint prefetch_activated = 0;\r\nu32 rkey = pfault->rdma.r_key;\r\nint ret;\r\npfault->rdma.rdma_va += pfault->bytes_committed;\r\npfault->rdma.rdma_op_len -= min(pfault->bytes_committed,\r\npfault->rdma.rdma_op_len);\r\npfault->bytes_committed = 0;\r\naddress = pfault->rdma.rdma_va;\r\nlength = pfault->rdma.rdma_op_len;\r\nif (length == 0) {\r\nprefetch_activated = 1;\r\nlength = pfault->rdma.packet_size;\r\nprefetch_len = min(MAX_PREFETCH_LEN, prefetch_len);\r\n}\r\nret = pagefault_single_data_segment(dev, rkey, address, length,\r\n&pfault->bytes_committed, NULL);\r\nif (ret == -EAGAIN) {\r\nprefetch_activated = 0;\r\n} else if (ret < 0 || pages_in_range(address, length) > ret) {\r\nmlx5_ib_page_fault_resume(dev, pfault, 1);\r\nif (ret != -ENOENT)\r\nmlx5_ib_dbg(dev, "PAGE FAULT error %d. QP 0x%x, type: 0x%x\n",\r\nret, pfault->token, pfault->type);\r\nreturn;\r\n}\r\nmlx5_ib_page_fault_resume(dev, pfault, 0);\r\nmlx5_ib_dbg(dev, "PAGE FAULT completed. QP 0x%x, type: 0x%x, prefetch_activated: %d\n",\r\npfault->token, pfault->type,\r\nprefetch_activated);\r\nif (prefetch_activated) {\r\nu32 bytes_committed = 0;\r\nret = pagefault_single_data_segment(dev, rkey, address,\r\nprefetch_len,\r\n&bytes_committed, NULL);\r\nif (ret < 0 && ret != -EAGAIN) {\r\nmlx5_ib_dbg(dev, "Prefetch failed. ret: %d, QP 0x%x, address: 0x%.16llx, length = 0x%.16x\n",\r\nret, pfault->token, address, prefetch_len);\r\n}\r\n}\r\n}\r\nvoid mlx5_ib_pfault(struct mlx5_core_dev *mdev, void *context,\r\nstruct mlx5_pagefault *pfault)\r\n{\r\nstruct mlx5_ib_dev *dev = context;\r\nu8 event_subtype = pfault->event_subtype;\r\nswitch (event_subtype) {\r\ncase MLX5_PFAULT_SUBTYPE_WQE:\r\nmlx5_ib_mr_wqe_pfault_handler(dev, pfault);\r\nbreak;\r\ncase MLX5_PFAULT_SUBTYPE_RDMA:\r\nmlx5_ib_mr_rdma_pfault_handler(dev, pfault);\r\nbreak;\r\ndefault:\r\nmlx5_ib_err(dev, "Invalid page fault event subtype: 0x%x\n",\r\nevent_subtype);\r\nmlx5_ib_page_fault_resume(dev, pfault, 1);\r\n}\r\n}\r\nvoid mlx5_odp_init_mr_cache_entry(struct mlx5_cache_ent *ent)\r\n{\r\nif (!(ent->dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT))\r\nreturn;\r\nswitch (ent->order - 2) {\r\ncase MLX5_IMR_MTT_CACHE_ENTRY:\r\nent->page = PAGE_SHIFT;\r\nent->xlt = MLX5_IMR_MTT_ENTRIES *\r\nsizeof(struct mlx5_mtt) /\r\nMLX5_IB_UMR_OCTOWORD;\r\nent->access_mode = MLX5_MKC_ACCESS_MODE_MTT;\r\nent->limit = 0;\r\nbreak;\r\ncase MLX5_IMR_KSM_CACHE_ENTRY:\r\nent->page = MLX5_KSM_PAGE_SHIFT;\r\nent->xlt = mlx5_imr_ksm_entries *\r\nsizeof(struct mlx5_klm) /\r\nMLX5_IB_UMR_OCTOWORD;\r\nent->access_mode = MLX5_MKC_ACCESS_MODE_KSM;\r\nent->limit = 0;\r\nbreak;\r\n}\r\n}\r\nint mlx5_ib_odp_init_one(struct mlx5_ib_dev *dev)\r\n{\r\nint ret;\r\nret = init_srcu_struct(&dev->mr_srcu);\r\nif (ret)\r\nreturn ret;\r\nif (dev->odp_caps.general_caps & IB_ODP_SUPPORT_IMPLICIT) {\r\nret = mlx5_cmd_null_mkey(dev->mdev, &dev->null_mkey);\r\nif (ret) {\r\nmlx5_ib_err(dev, "Error getting null_mkey %d\n", ret);\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid mlx5_ib_odp_remove_one(struct mlx5_ib_dev *dev)\r\n{\r\ncleanup_srcu_struct(&dev->mr_srcu);\r\n}\r\nint mlx5_ib_odp_init(void)\r\n{\r\nmlx5_imr_ksm_entries = BIT_ULL(get_order(TASK_SIZE) -\r\nMLX5_IMR_MTT_BITS);\r\nreturn 0;\r\n}
