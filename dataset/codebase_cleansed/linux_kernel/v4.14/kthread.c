static inline void set_kthread_struct(void *kthread)\r\n{\r\ncurrent->set_child_tid = (__force void __user *)kthread;\r\n}\r\nstatic inline struct kthread *to_kthread(struct task_struct *k)\r\n{\r\nWARN_ON(!(k->flags & PF_KTHREAD));\r\nreturn (__force void *)k->set_child_tid;\r\n}\r\nvoid free_kthread_struct(struct task_struct *k)\r\n{\r\nkfree(to_kthread(k));\r\n}\r\nbool kthread_should_stop(void)\r\n{\r\nreturn test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags);\r\n}\r\nbool kthread_should_park(void)\r\n{\r\nreturn test_bit(KTHREAD_SHOULD_PARK, &to_kthread(current)->flags);\r\n}\r\nbool kthread_freezable_should_stop(bool *was_frozen)\r\n{\r\nbool frozen = false;\r\nmight_sleep();\r\nif (unlikely(freezing(current)))\r\nfrozen = __refrigerator(true);\r\nif (was_frozen)\r\n*was_frozen = frozen;\r\nreturn kthread_should_stop();\r\n}\r\nvoid *kthread_data(struct task_struct *task)\r\n{\r\nreturn to_kthread(task)->data;\r\n}\r\nvoid *kthread_probe_data(struct task_struct *task)\r\n{\r\nstruct kthread *kthread = to_kthread(task);\r\nvoid *data = NULL;\r\nprobe_kernel_read(&data, &kthread->data, sizeof(data));\r\nreturn data;\r\n}\r\nstatic void __kthread_parkme(struct kthread *self)\r\n{\r\n__set_current_state(TASK_PARKED);\r\nwhile (test_bit(KTHREAD_SHOULD_PARK, &self->flags)) {\r\nif (!test_and_set_bit(KTHREAD_IS_PARKED, &self->flags))\r\ncomplete(&self->parked);\r\nschedule();\r\n__set_current_state(TASK_PARKED);\r\n}\r\nclear_bit(KTHREAD_IS_PARKED, &self->flags);\r\n__set_current_state(TASK_RUNNING);\r\n}\r\nvoid kthread_parkme(void)\r\n{\r\n__kthread_parkme(to_kthread(current));\r\n}\r\nstatic int kthread(void *_create)\r\n{\r\nstruct kthread_create_info *create = _create;\r\nint (*threadfn)(void *data) = create->threadfn;\r\nvoid *data = create->data;\r\nstruct completion *done;\r\nstruct kthread *self;\r\nint ret;\r\nself = kmalloc(sizeof(*self), GFP_KERNEL);\r\nset_kthread_struct(self);\r\ndone = xchg(&create->done, NULL);\r\nif (!done) {\r\nkfree(create);\r\ndo_exit(-EINTR);\r\n}\r\nif (!self) {\r\ncreate->result = ERR_PTR(-ENOMEM);\r\ncomplete(done);\r\ndo_exit(-ENOMEM);\r\n}\r\nself->flags = 0;\r\nself->data = data;\r\ninit_completion(&self->exited);\r\ninit_completion(&self->parked);\r\ncurrent->vfork_done = &self->exited;\r\n__set_current_state(TASK_UNINTERRUPTIBLE);\r\ncreate->result = current;\r\ncomplete(done);\r\nschedule();\r\nret = -EINTR;\r\nif (!test_bit(KTHREAD_SHOULD_STOP, &self->flags)) {\r\ncgroup_kthread_ready();\r\n__kthread_parkme(self);\r\nret = threadfn(data);\r\n}\r\ndo_exit(ret);\r\n}\r\nint tsk_fork_get_node(struct task_struct *tsk)\r\n{\r\n#ifdef CONFIG_NUMA\r\nif (tsk == kthreadd_task)\r\nreturn tsk->pref_node_fork;\r\n#endif\r\nreturn NUMA_NO_NODE;\r\n}\r\nstatic void create_kthread(struct kthread_create_info *create)\r\n{\r\nint pid;\r\n#ifdef CONFIG_NUMA\r\ncurrent->pref_node_fork = create->node;\r\n#endif\r\npid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);\r\nif (pid < 0) {\r\nstruct completion *done = xchg(&create->done, NULL);\r\nif (!done) {\r\nkfree(create);\r\nreturn;\r\n}\r\ncreate->result = ERR_PTR(pid);\r\ncomplete(done);\r\n}\r\n}\r\ntask_struct *kthread_create_on_node(int (*threadfn)(void *data),\r\nvoid *data, int node,\r\nconst char namefmt[],\r\n...)\r\n{\r\nstruct task_struct *task;\r\nva_list args;\r\nva_start(args, namefmt);\r\ntask = __kthread_create_on_node(threadfn, data, node, namefmt, args);\r\nva_end(args);\r\nreturn task;\r\n}\r\nstatic void __kthread_bind_mask(struct task_struct *p, const struct cpumask *mask, long state)\r\n{\r\nunsigned long flags;\r\nif (!wait_task_inactive(p, state)) {\r\nWARN_ON(1);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&p->pi_lock, flags);\r\ndo_set_cpus_allowed(p, mask);\r\np->flags |= PF_NO_SETAFFINITY;\r\nraw_spin_unlock_irqrestore(&p->pi_lock, flags);\r\n}\r\nstatic void __kthread_bind(struct task_struct *p, unsigned int cpu, long state)\r\n{\r\n__kthread_bind_mask(p, cpumask_of(cpu), state);\r\n}\r\nvoid kthread_bind_mask(struct task_struct *p, const struct cpumask *mask)\r\n{\r\n__kthread_bind_mask(p, mask, TASK_UNINTERRUPTIBLE);\r\n}\r\nvoid kthread_bind(struct task_struct *p, unsigned int cpu)\r\n{\r\n__kthread_bind(p, cpu, TASK_UNINTERRUPTIBLE);\r\n}\r\nstruct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),\r\nvoid *data, unsigned int cpu,\r\nconst char *namefmt)\r\n{\r\nstruct task_struct *p;\r\np = kthread_create_on_node(threadfn, data, cpu_to_node(cpu), namefmt,\r\ncpu);\r\nif (IS_ERR(p))\r\nreturn p;\r\nkthread_bind(p, cpu);\r\nset_bit(KTHREAD_IS_PER_CPU, &to_kthread(p)->flags);\r\nto_kthread(p)->cpu = cpu;\r\nreturn p;\r\n}\r\nvoid kthread_unpark(struct task_struct *k)\r\n{\r\nstruct kthread *kthread = to_kthread(k);\r\nclear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);\r\nif (test_and_clear_bit(KTHREAD_IS_PARKED, &kthread->flags)) {\r\nif (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))\r\n__kthread_bind(k, kthread->cpu, TASK_PARKED);\r\nwake_up_state(k, TASK_PARKED);\r\n}\r\n}\r\nint kthread_park(struct task_struct *k)\r\n{\r\nstruct kthread *kthread = to_kthread(k);\r\nif (WARN_ON(k->flags & PF_EXITING))\r\nreturn -ENOSYS;\r\nif (!test_bit(KTHREAD_IS_PARKED, &kthread->flags)) {\r\nset_bit(KTHREAD_SHOULD_PARK, &kthread->flags);\r\nif (k != current) {\r\nwake_up_process(k);\r\nwait_for_completion(&kthread->parked);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint kthread_stop(struct task_struct *k)\r\n{\r\nstruct kthread *kthread;\r\nint ret;\r\ntrace_sched_kthread_stop(k);\r\nget_task_struct(k);\r\nkthread = to_kthread(k);\r\nset_bit(KTHREAD_SHOULD_STOP, &kthread->flags);\r\nkthread_unpark(k);\r\nwake_up_process(k);\r\nwait_for_completion(&kthread->exited);\r\nret = k->exit_code;\r\nput_task_struct(k);\r\ntrace_sched_kthread_stop_ret(ret);\r\nreturn ret;\r\n}\r\nint kthreadd(void *unused)\r\n{\r\nstruct task_struct *tsk = current;\r\nset_task_comm(tsk, "kthreadd");\r\nignore_signals(tsk);\r\nset_cpus_allowed_ptr(tsk, cpu_all_mask);\r\nset_mems_allowed(node_states[N_MEMORY]);\r\ncurrent->flags |= PF_NOFREEZE;\r\ncgroup_init_kthreadd();\r\nfor (;;) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (list_empty(&kthread_create_list))\r\nschedule();\r\n__set_current_state(TASK_RUNNING);\r\nspin_lock(&kthread_create_lock);\r\nwhile (!list_empty(&kthread_create_list)) {\r\nstruct kthread_create_info *create;\r\ncreate = list_entry(kthread_create_list.next,\r\nstruct kthread_create_info, list);\r\nlist_del_init(&create->list);\r\nspin_unlock(&kthread_create_lock);\r\ncreate_kthread(create);\r\nspin_lock(&kthread_create_lock);\r\n}\r\nspin_unlock(&kthread_create_lock);\r\n}\r\nreturn 0;\r\n}\r\nvoid __kthread_init_worker(struct kthread_worker *worker,\r\nconst char *name,\r\nstruct lock_class_key *key)\r\n{\r\nmemset(worker, 0, sizeof(struct kthread_worker));\r\nspin_lock_init(&worker->lock);\r\nlockdep_set_class_and_name(&worker->lock, key, name);\r\nINIT_LIST_HEAD(&worker->work_list);\r\nINIT_LIST_HEAD(&worker->delayed_work_list);\r\n}\r\nint kthread_worker_fn(void *worker_ptr)\r\n{\r\nstruct kthread_worker *worker = worker_ptr;\r\nstruct kthread_work *work;\r\nWARN_ON(worker->task && worker->task != current);\r\nworker->task = current;\r\nif (worker->flags & KTW_FREEZABLE)\r\nset_freezable();\r\nrepeat:\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (kthread_should_stop()) {\r\n__set_current_state(TASK_RUNNING);\r\nspin_lock_irq(&worker->lock);\r\nworker->task = NULL;\r\nspin_unlock_irq(&worker->lock);\r\nreturn 0;\r\n}\r\nwork = NULL;\r\nspin_lock_irq(&worker->lock);\r\nif (!list_empty(&worker->work_list)) {\r\nwork = list_first_entry(&worker->work_list,\r\nstruct kthread_work, node);\r\nlist_del_init(&work->node);\r\n}\r\nworker->current_work = work;\r\nspin_unlock_irq(&worker->lock);\r\nif (work) {\r\n__set_current_state(TASK_RUNNING);\r\nwork->func(work);\r\n} else if (!freezing(current))\r\nschedule();\r\ntry_to_freeze();\r\ncond_resched();\r\ngoto repeat;\r\n}\r\nkthread_worker *\r\nkthread_create_worker(unsigned int flags, const char namefmt[], ...)\r\n{\r\nstruct kthread_worker *worker;\r\nva_list args;\r\nva_start(args, namefmt);\r\nworker = __kthread_create_worker(-1, flags, namefmt, args);\r\nva_end(args);\r\nreturn worker;\r\n}\r\nstruct kthread_worker *\r\nkthread_create_worker_on_cpu(int cpu, unsigned int flags,\r\nconst char namefmt[], ...)\r\n{\r\nstruct kthread_worker *worker;\r\nva_list args;\r\nva_start(args, namefmt);\r\nworker = __kthread_create_worker(cpu, flags, namefmt, args);\r\nva_end(args);\r\nreturn worker;\r\n}\r\nstatic inline bool queuing_blocked(struct kthread_worker *worker,\r\nstruct kthread_work *work)\r\n{\r\nlockdep_assert_held(&worker->lock);\r\nreturn !list_empty(&work->node) || work->canceling;\r\n}\r\nstatic void kthread_insert_work_sanity_check(struct kthread_worker *worker,\r\nstruct kthread_work *work)\r\n{\r\nlockdep_assert_held(&worker->lock);\r\nWARN_ON_ONCE(!list_empty(&work->node));\r\nWARN_ON_ONCE(work->worker && work->worker != worker);\r\n}\r\nstatic void kthread_insert_work(struct kthread_worker *worker,\r\nstruct kthread_work *work,\r\nstruct list_head *pos)\r\n{\r\nkthread_insert_work_sanity_check(worker, work);\r\nlist_add_tail(&work->node, pos);\r\nwork->worker = worker;\r\nif (!worker->current_work && likely(worker->task))\r\nwake_up_process(worker->task);\r\n}\r\nbool kthread_queue_work(struct kthread_worker *worker,\r\nstruct kthread_work *work)\r\n{\r\nbool ret = false;\r\nunsigned long flags;\r\nspin_lock_irqsave(&worker->lock, flags);\r\nif (!queuing_blocked(worker, work)) {\r\nkthread_insert_work(worker, work, &worker->work_list);\r\nret = true;\r\n}\r\nspin_unlock_irqrestore(&worker->lock, flags);\r\nreturn ret;\r\n}\r\nvoid kthread_delayed_work_timer_fn(unsigned long __data)\r\n{\r\nstruct kthread_delayed_work *dwork =\r\n(struct kthread_delayed_work *)__data;\r\nstruct kthread_work *work = &dwork->work;\r\nstruct kthread_worker *worker = work->worker;\r\nif (WARN_ON_ONCE(!worker))\r\nreturn;\r\nspin_lock(&worker->lock);\r\nWARN_ON_ONCE(work->worker != worker);\r\nWARN_ON_ONCE(list_empty(&work->node));\r\nlist_del_init(&work->node);\r\nkthread_insert_work(worker, work, &worker->work_list);\r\nspin_unlock(&worker->lock);\r\n}\r\nvoid __kthread_queue_delayed_work(struct kthread_worker *worker,\r\nstruct kthread_delayed_work *dwork,\r\nunsigned long delay)\r\n{\r\nstruct timer_list *timer = &dwork->timer;\r\nstruct kthread_work *work = &dwork->work;\r\nWARN_ON_ONCE(timer->function != kthread_delayed_work_timer_fn ||\r\ntimer->data != (unsigned long)dwork);\r\nif (!delay) {\r\nkthread_insert_work(worker, work, &worker->work_list);\r\nreturn;\r\n}\r\nkthread_insert_work_sanity_check(worker, work);\r\nlist_add(&work->node, &worker->delayed_work_list);\r\nwork->worker = worker;\r\ntimer->expires = jiffies + delay;\r\nadd_timer(timer);\r\n}\r\nbool kthread_queue_delayed_work(struct kthread_worker *worker,\r\nstruct kthread_delayed_work *dwork,\r\nunsigned long delay)\r\n{\r\nstruct kthread_work *work = &dwork->work;\r\nunsigned long flags;\r\nbool ret = false;\r\nspin_lock_irqsave(&worker->lock, flags);\r\nif (!queuing_blocked(worker, work)) {\r\n__kthread_queue_delayed_work(worker, dwork, delay);\r\nret = true;\r\n}\r\nspin_unlock_irqrestore(&worker->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void kthread_flush_work_fn(struct kthread_work *work)\r\n{\r\nstruct kthread_flush_work *fwork =\r\ncontainer_of(work, struct kthread_flush_work, work);\r\ncomplete(&fwork->done);\r\n}\r\nvoid kthread_flush_work(struct kthread_work *work)\r\n{\r\nstruct kthread_flush_work fwork = {\r\nKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),\r\nCOMPLETION_INITIALIZER_ONSTACK(fwork.done),\r\n};\r\nstruct kthread_worker *worker;\r\nbool noop = false;\r\nworker = work->worker;\r\nif (!worker)\r\nreturn;\r\nspin_lock_irq(&worker->lock);\r\nWARN_ON_ONCE(work->worker != worker);\r\nif (!list_empty(&work->node))\r\nkthread_insert_work(worker, &fwork.work, work->node.next);\r\nelse if (worker->current_work == work)\r\nkthread_insert_work(worker, &fwork.work,\r\nworker->work_list.next);\r\nelse\r\nnoop = true;\r\nspin_unlock_irq(&worker->lock);\r\nif (!noop)\r\nwait_for_completion(&fwork.done);\r\n}\r\nstatic bool __kthread_cancel_work(struct kthread_work *work, bool is_dwork,\r\nunsigned long *flags)\r\n{\r\nif (is_dwork) {\r\nstruct kthread_delayed_work *dwork =\r\ncontainer_of(work, struct kthread_delayed_work, work);\r\nstruct kthread_worker *worker = work->worker;\r\nwork->canceling++;\r\nspin_unlock_irqrestore(&worker->lock, *flags);\r\ndel_timer_sync(&dwork->timer);\r\nspin_lock_irqsave(&worker->lock, *flags);\r\nwork->canceling--;\r\n}\r\nif (!list_empty(&work->node)) {\r\nlist_del_init(&work->node);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nbool kthread_mod_delayed_work(struct kthread_worker *worker,\r\nstruct kthread_delayed_work *dwork,\r\nunsigned long delay)\r\n{\r\nstruct kthread_work *work = &dwork->work;\r\nunsigned long flags;\r\nint ret = false;\r\nspin_lock_irqsave(&worker->lock, flags);\r\nif (!work->worker)\r\ngoto fast_queue;\r\nWARN_ON_ONCE(work->worker != worker);\r\nif (work->canceling)\r\ngoto out;\r\nret = __kthread_cancel_work(work, true, &flags);\r\nfast_queue:\r\n__kthread_queue_delayed_work(worker, dwork, delay);\r\nout:\r\nspin_unlock_irqrestore(&worker->lock, flags);\r\nreturn ret;\r\n}\r\nstatic bool __kthread_cancel_work_sync(struct kthread_work *work, bool is_dwork)\r\n{\r\nstruct kthread_worker *worker = work->worker;\r\nunsigned long flags;\r\nint ret = false;\r\nif (!worker)\r\ngoto out;\r\nspin_lock_irqsave(&worker->lock, flags);\r\nWARN_ON_ONCE(work->worker != worker);\r\nret = __kthread_cancel_work(work, is_dwork, &flags);\r\nif (worker->current_work != work)\r\ngoto out_fast;\r\nwork->canceling++;\r\nspin_unlock_irqrestore(&worker->lock, flags);\r\nkthread_flush_work(work);\r\nspin_lock_irqsave(&worker->lock, flags);\r\nwork->canceling--;\r\nout_fast:\r\nspin_unlock_irqrestore(&worker->lock, flags);\r\nout:\r\nreturn ret;\r\n}\r\nbool kthread_cancel_work_sync(struct kthread_work *work)\r\n{\r\nreturn __kthread_cancel_work_sync(work, false);\r\n}\r\nbool kthread_cancel_delayed_work_sync(struct kthread_delayed_work *dwork)\r\n{\r\nreturn __kthread_cancel_work_sync(&dwork->work, true);\r\n}\r\nvoid kthread_flush_worker(struct kthread_worker *worker)\r\n{\r\nstruct kthread_flush_work fwork = {\r\nKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),\r\nCOMPLETION_INITIALIZER_ONSTACK(fwork.done),\r\n};\r\nkthread_queue_work(worker, &fwork.work);\r\nwait_for_completion(&fwork.done);\r\n}\r\nvoid kthread_destroy_worker(struct kthread_worker *worker)\r\n{\r\nstruct task_struct *task;\r\ntask = worker->task;\r\nif (WARN_ON(!task))\r\nreturn;\r\nkthread_flush_worker(worker);\r\nkthread_stop(task);\r\nWARN_ON(!list_empty(&worker->work_list));\r\nkfree(worker);\r\n}
