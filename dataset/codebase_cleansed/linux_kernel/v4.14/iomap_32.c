static int is_io_mapping_possible(resource_size_t base, unsigned long size)\r\n{\r\n#if !defined(CONFIG_X86_PAE) && defined(CONFIG_PHYS_ADDR_T_64BIT)\r\nif (base + size > 0x100000000ULL)\r\nreturn 0;\r\n#endif\r\nreturn 1;\r\n}\r\nint iomap_create_wc(resource_size_t base, unsigned long size, pgprot_t *prot)\r\n{\r\nenum page_cache_mode pcm = _PAGE_CACHE_MODE_WC;\r\nint ret;\r\nif (!is_io_mapping_possible(base, size))\r\nreturn -EINVAL;\r\nret = io_reserve_memtype(base, base + size, &pcm);\r\nif (ret)\r\nreturn ret;\r\n*prot = __pgprot(__PAGE_KERNEL | cachemode2protval(pcm));\r\nreturn 0;\r\n}\r\nvoid iomap_free(resource_size_t base, unsigned long size)\r\n{\r\nio_free_memtype(base, base + size);\r\n}\r\nvoid *kmap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)\r\n{\r\nunsigned long vaddr;\r\nint idx, type;\r\npreempt_disable();\r\npagefault_disable();\r\ntype = kmap_atomic_idx_push();\r\nidx = type + KM_TYPE_NR * smp_processor_id();\r\nvaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);\r\nset_pte(kmap_pte - idx, pfn_pte(pfn, prot));\r\narch_flush_lazy_mmu_mode();\r\nreturn (void *)vaddr;\r\n}\r\nvoid __iomem *\r\niomap_atomic_prot_pfn(unsigned long pfn, pgprot_t prot)\r\n{\r\nif (!pat_enabled() && pgprot2cachemode(prot) != _PAGE_CACHE_MODE_WB)\r\nprot = __pgprot(__PAGE_KERNEL |\r\ncachemode2protval(_PAGE_CACHE_MODE_UC_MINUS));\r\nreturn (void __force __iomem *) kmap_atomic_prot_pfn(pfn, prot);\r\n}\r\nvoid\r\niounmap_atomic(void __iomem *kvaddr)\r\n{\r\nunsigned long vaddr = (unsigned long) kvaddr & PAGE_MASK;\r\nif (vaddr >= __fix_to_virt(FIX_KMAP_END) &&\r\nvaddr <= __fix_to_virt(FIX_KMAP_BEGIN)) {\r\nint idx, type;\r\ntype = kmap_atomic_idx();\r\nidx = type + KM_TYPE_NR * smp_processor_id();\r\n#ifdef CONFIG_DEBUG_HIGHMEM\r\nWARN_ON_ONCE(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));\r\n#endif\r\nkpte_clear_flush(kmap_pte-idx, vaddr);\r\nkmap_atomic_idx_pop();\r\n}\r\npagefault_enable();\r\npreempt_enable();\r\n}
