static int __init early_parse_kvm_cma_resv(char *p)\r\n{\r\npr_debug("%s(%s)\n", __func__, p);\r\nif (!p)\r\nreturn -EINVAL;\r\nreturn kstrtoul(p, 0, &kvm_cma_resv_ratio);\r\n}\r\nstruct page *kvm_alloc_hpt_cma(unsigned long nr_pages)\r\n{\r\nVM_BUG_ON(order_base_2(nr_pages) < KVM_CMA_CHUNK_ORDER - PAGE_SHIFT);\r\nreturn cma_alloc(kvm_cma, nr_pages, order_base_2(HPT_ALIGN_PAGES),\r\nGFP_KERNEL);\r\n}\r\nvoid kvm_free_hpt_cma(struct page *page, unsigned long nr_pages)\r\n{\r\ncma_release(kvm_cma, page, nr_pages);\r\n}\r\nvoid __init kvm_cma_reserve(void)\r\n{\r\nunsigned long align_size;\r\nstruct memblock_region *reg;\r\nphys_addr_t selected_size = 0;\r\nif (!cpu_has_feature(CPU_FTR_HVMODE))\r\nreturn;\r\nfor_each_memblock(memory, reg)\r\nselected_size += memblock_region_memory_end_pfn(reg) -\r\nmemblock_region_memory_base_pfn(reg);\r\nselected_size = (selected_size * kvm_cma_resv_ratio / 100) << PAGE_SHIFT;\r\nif (selected_size) {\r\npr_debug("%s: reserving %ld MiB for global area\n", __func__,\r\n(unsigned long)selected_size / SZ_1M);\r\nalign_size = HPT_ALIGN_PAGES << PAGE_SHIFT;\r\ncma_declare_contiguous(0, selected_size, 0, align_size,\r\nKVM_CMA_CHUNK_ORDER - PAGE_SHIFT, false, "kvm_cma",\r\n&kvm_cma);\r\n}\r\n}\r\nlong int kvmppc_rm_h_confer(struct kvm_vcpu *vcpu, int target,\r\nunsigned int yield_count)\r\n{\r\nstruct kvmppc_vcore *vc = local_paca->kvm_hstate.kvm_vcore;\r\nint ptid = local_paca->kvm_hstate.ptid;\r\nint threads_running;\r\nint threads_ceded;\r\nint threads_conferring;\r\nu64 stop = get_tb() + 10 * tb_ticks_per_usec;\r\nint rv = H_SUCCESS;\r\nset_bit(ptid, &vc->conferring_threads);\r\nwhile ((get_tb() < stop) && !VCORE_IS_EXITING(vc)) {\r\nthreads_running = VCORE_ENTRY_MAP(vc);\r\nthreads_ceded = vc->napping_threads;\r\nthreads_conferring = vc->conferring_threads;\r\nif ((threads_ceded | threads_conferring) == threads_running) {\r\nrv = H_TOO_HARD;\r\nbreak;\r\n}\r\n}\r\nclear_bit(ptid, &vc->conferring_threads);\r\nreturn rv;\r\n}\r\nvoid kvm_hv_vm_activated(void)\r\n{\r\nget_online_cpus();\r\natomic_inc(&hv_vm_count);\r\nput_online_cpus();\r\n}\r\nvoid kvm_hv_vm_deactivated(void)\r\n{\r\nget_online_cpus();\r\natomic_dec(&hv_vm_count);\r\nput_online_cpus();\r\n}\r\nbool kvm_hv_mode_active(void)\r\n{\r\nreturn atomic_read(&hv_vm_count) != 0;\r\n}\r\nint kvmppc_hcall_impl_hv_realmode(unsigned long cmd)\r\n{\r\ncmd /= 4;\r\nif (cmd < hcall_real_table_end - hcall_real_table &&\r\nhcall_real_table[cmd])\r\nreturn 1;\r\nreturn 0;\r\n}\r\nint kvmppc_hwrng_present(void)\r\n{\r\nreturn powernv_hwrng_present();\r\n}\r\nlong kvmppc_h_random(struct kvm_vcpu *vcpu)\r\n{\r\nint r;\r\nif (kvm_is_radix(vcpu->kvm) && (mfmsr() & MSR_IR))\r\nr = powernv_get_random_long(&vcpu->arch.gpr[4]);\r\nelse\r\nr = powernv_get_random_real_mode(&vcpu->arch.gpr[4]);\r\nif (r)\r\nreturn H_SUCCESS;\r\nreturn H_HARDWARE;\r\n}\r\nvoid kvmhv_rm_send_ipi(int cpu)\r\n{\r\nvoid __iomem *xics_phys;\r\nunsigned long msg = PPC_DBELL_TYPE(PPC_DBELL_SERVER);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nmsg |= get_hard_smp_processor_id(cpu);\r\n__asm__ __volatile__ (PPC_MSGSND(%0) : : "r" (msg));\r\nreturn;\r\n}\r\nif (cpu_has_feature(CPU_FTR_ARCH_207S) &&\r\ncpu_first_thread_sibling(cpu) ==\r\ncpu_first_thread_sibling(raw_smp_processor_id())) {\r\nmsg |= cpu_thread_in_core(cpu);\r\n__asm__ __volatile__ (PPC_MSGSND(%0) : : "r" (msg));\r\nreturn;\r\n}\r\nif (WARN_ON_ONCE(xive_enabled()))\r\nreturn;\r\nxics_phys = paca[cpu].kvm_hstate.xics_phys;\r\nif (xics_phys)\r\n__raw_rm_writeb(IPI_PRIORITY, xics_phys + XICS_MFRR);\r\nelse\r\nopal_int_set_mfrr(get_hard_smp_processor_id(cpu), IPI_PRIORITY);\r\n}\r\nstatic void kvmhv_interrupt_vcore(struct kvmppc_vcore *vc, int active)\r\n{\r\nint cpu = vc->pcpu;\r\nsmp_mb();\r\nfor (; active; active >>= 1, ++cpu)\r\nif (active & 1)\r\nkvmhv_rm_send_ipi(cpu);\r\n}\r\nvoid kvmhv_commence_exit(int trap)\r\n{\r\nstruct kvmppc_vcore *vc = local_paca->kvm_hstate.kvm_vcore;\r\nint ptid = local_paca->kvm_hstate.ptid;\r\nstruct kvm_split_mode *sip = local_paca->kvm_hstate.kvm_split_mode;\r\nint me, ee, i;\r\nme = 0x100 << ptid;\r\ndo {\r\nee = vc->entry_exit_map;\r\n} while (cmpxchg(&vc->entry_exit_map, ee, ee | me) != ee);\r\nif ((ee >> 8) != 0)\r\nreturn;\r\nif (trap != BOOK3S_INTERRUPT_HV_DECREMENTER)\r\nkvmhv_interrupt_vcore(vc, ee & ~(1 << ptid));\r\nif (!sip)\r\nreturn;\r\nfor (i = 0; i < MAX_SUBCORES; ++i) {\r\nvc = sip->vc[i];\r\nif (!vc)\r\nbreak;\r\ndo {\r\nee = vc->entry_exit_map;\r\nif ((ee >> 8) != 0)\r\nbreak;\r\n} while (cmpxchg(&vc->entry_exit_map, ee,\r\nee | VCORE_EXIT_REQ) != ee);\r\nif ((ee >> 8) == 0)\r\nkvmhv_interrupt_vcore(vc, ee);\r\n}\r\n}\r\nstatic struct kvmppc_irq_map *get_irqmap(struct kvmppc_passthru_irqmap *pimap,\r\nu32 xisr)\r\n{\r\nint i;\r\nfor (i = 0; i < pimap->n_mapped; i++) {\r\nif (xisr == pimap->mapped[i].r_hwirq) {\r\nsmp_rmb();\r\nreturn &pimap->mapped[i];\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic int kvmppc_check_passthru(u32 xisr, __be32 xirr, bool *again)\r\n{\r\nstruct kvmppc_passthru_irqmap *pimap;\r\nstruct kvmppc_irq_map *irq_map;\r\nstruct kvm_vcpu *vcpu;\r\nvcpu = local_paca->kvm_hstate.kvm_vcpu;\r\nif (!vcpu)\r\nreturn 1;\r\npimap = kvmppc_get_passthru_irqmap(vcpu->kvm);\r\nif (!pimap)\r\nreturn 1;\r\nirq_map = get_irqmap(pimap, xisr);\r\nif (!irq_map)\r\nreturn 1;\r\nlocal_paca->kvm_hstate.saved_xirr = 0;\r\nreturn kvmppc_deliver_irq_passthru(vcpu, xirr, irq_map, pimap, again);\r\n}\r\nstatic inline int kvmppc_check_passthru(u32 xisr, __be32 xirr, bool *again)\r\n{\r\nreturn 1;\r\n}\r\nlong kvmppc_read_intr(void)\r\n{\r\nlong ret = 0;\r\nlong rc;\r\nbool again;\r\nif (xive_enabled())\r\nreturn 1;\r\ndo {\r\nagain = false;\r\nrc = kvmppc_read_one_intr(&again);\r\nif (rc && (ret == 0 || rc > ret))\r\nret = rc;\r\n} while (again);\r\nreturn ret;\r\n}\r\nstatic long kvmppc_read_one_intr(bool *again)\r\n{\r\nvoid __iomem *xics_phys;\r\nu32 h_xirr;\r\n__be32 xirr;\r\nu32 xisr;\r\nu8 host_ipi;\r\nint64_t rc;\r\nif (xive_enabled())\r\nreturn 1;\r\nhost_ipi = local_paca->kvm_hstate.host_ipi;\r\nif (host_ipi)\r\nreturn 1;\r\nxics_phys = local_paca->kvm_hstate.xics_phys;\r\nrc = 0;\r\nif (!xics_phys)\r\nrc = opal_int_get_xirr(&xirr, false);\r\nelse\r\nxirr = __raw_rm_readl(xics_phys + XICS_XIRR);\r\nif (rc < 0)\r\nreturn 1;\r\nh_xirr = be32_to_cpu(xirr);\r\nlocal_paca->kvm_hstate.saved_xirr = h_xirr;\r\nxisr = h_xirr & 0xffffff;\r\nsmp_mb();\r\nif (!xisr)\r\nreturn 0;\r\nif (xisr == XICS_IPI) {\r\nrc = 0;\r\nif (xics_phys) {\r\n__raw_rm_writeb(0xff, xics_phys + XICS_MFRR);\r\n__raw_rm_writel(xirr, xics_phys + XICS_XIRR);\r\n} else {\r\nopal_int_set_mfrr(hard_smp_processor_id(), 0xff);\r\nrc = opal_int_eoi(h_xirr);\r\n}\r\n*again = rc > 0;\r\nsmp_mb();\r\nhost_ipi = local_paca->kvm_hstate.host_ipi;\r\nif (unlikely(host_ipi != 0)) {\r\nif (xics_phys)\r\n__raw_rm_writeb(IPI_PRIORITY,\r\nxics_phys + XICS_MFRR);\r\nelse\r\nopal_int_set_mfrr(hard_smp_processor_id(),\r\nIPI_PRIORITY);\r\nsmp_mb();\r\nreturn 1;\r\n}\r\nlocal_paca->kvm_hstate.saved_xirr = 0;\r\nreturn -1;\r\n}\r\nreturn kvmppc_check_passthru(xisr, xirr, again);\r\n}\r\nstatic inline bool is_rm(void)\r\n{\r\nreturn !(mfmsr() & MSR_DR);\r\n}\r\nunsigned long kvmppc_rm_h_xirr(struct kvm_vcpu *vcpu)\r\n{\r\nif (xive_enabled()) {\r\nif (is_rm())\r\nreturn xive_rm_h_xirr(vcpu);\r\nif (unlikely(!__xive_vm_h_xirr))\r\nreturn H_NOT_AVAILABLE;\r\nreturn __xive_vm_h_xirr(vcpu);\r\n} else\r\nreturn xics_rm_h_xirr(vcpu);\r\n}\r\nunsigned long kvmppc_rm_h_xirr_x(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->arch.gpr[5] = get_tb();\r\nif (xive_enabled()) {\r\nif (is_rm())\r\nreturn xive_rm_h_xirr(vcpu);\r\nif (unlikely(!__xive_vm_h_xirr))\r\nreturn H_NOT_AVAILABLE;\r\nreturn __xive_vm_h_xirr(vcpu);\r\n} else\r\nreturn xics_rm_h_xirr(vcpu);\r\n}\r\nunsigned long kvmppc_rm_h_ipoll(struct kvm_vcpu *vcpu, unsigned long server)\r\n{\r\nif (xive_enabled()) {\r\nif (is_rm())\r\nreturn xive_rm_h_ipoll(vcpu, server);\r\nif (unlikely(!__xive_vm_h_ipoll))\r\nreturn H_NOT_AVAILABLE;\r\nreturn __xive_vm_h_ipoll(vcpu, server);\r\n} else\r\nreturn H_TOO_HARD;\r\n}\r\nint kvmppc_rm_h_ipi(struct kvm_vcpu *vcpu, unsigned long server,\r\nunsigned long mfrr)\r\n{\r\nif (xive_enabled()) {\r\nif (is_rm())\r\nreturn xive_rm_h_ipi(vcpu, server, mfrr);\r\nif (unlikely(!__xive_vm_h_ipi))\r\nreturn H_NOT_AVAILABLE;\r\nreturn __xive_vm_h_ipi(vcpu, server, mfrr);\r\n} else\r\nreturn xics_rm_h_ipi(vcpu, server, mfrr);\r\n}\r\nint kvmppc_rm_h_cppr(struct kvm_vcpu *vcpu, unsigned long cppr)\r\n{\r\nif (xive_enabled()) {\r\nif (is_rm())\r\nreturn xive_rm_h_cppr(vcpu, cppr);\r\nif (unlikely(!__xive_vm_h_cppr))\r\nreturn H_NOT_AVAILABLE;\r\nreturn __xive_vm_h_cppr(vcpu, cppr);\r\n} else\r\nreturn xics_rm_h_cppr(vcpu, cppr);\r\n}\r\nint kvmppc_rm_h_eoi(struct kvm_vcpu *vcpu, unsigned long xirr)\r\n{\r\nif (xive_enabled()) {\r\nif (is_rm())\r\nreturn xive_rm_h_eoi(vcpu, xirr);\r\nif (unlikely(!__xive_vm_h_eoi))\r\nreturn H_NOT_AVAILABLE;\r\nreturn __xive_vm_h_eoi(vcpu, xirr);\r\n} else\r\nreturn xics_rm_h_eoi(vcpu, xirr);\r\n}
