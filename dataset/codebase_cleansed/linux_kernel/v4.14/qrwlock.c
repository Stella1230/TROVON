static __always_inline void\r\nrspin_until_writer_unlock(struct qrwlock *lock, u32 cnts)\r\n{\r\nwhile ((cnts & _QW_WMASK) == _QW_LOCKED) {\r\ncpu_relax();\r\ncnts = atomic_read_acquire(&lock->cnts);\r\n}\r\n}\r\nvoid queued_read_lock_slowpath(struct qrwlock *lock, u32 cnts)\r\n{\r\nif (unlikely(in_interrupt())) {\r\nrspin_until_writer_unlock(lock, cnts);\r\nreturn;\r\n}\r\natomic_sub(_QR_BIAS, &lock->cnts);\r\narch_spin_lock(&lock->wait_lock);\r\ncnts = atomic_fetch_add_acquire(_QR_BIAS, &lock->cnts);\r\nrspin_until_writer_unlock(lock, cnts);\r\narch_spin_unlock(&lock->wait_lock);\r\n}\r\nvoid queued_write_lock_slowpath(struct qrwlock *lock)\r\n{\r\nu32 cnts;\r\narch_spin_lock(&lock->wait_lock);\r\nif (!atomic_read(&lock->cnts) &&\r\n(atomic_cmpxchg_acquire(&lock->cnts, 0, _QW_LOCKED) == 0))\r\ngoto unlock;\r\nfor (;;) {\r\nstruct __qrwlock *l = (struct __qrwlock *)lock;\r\nif (!READ_ONCE(l->wmode) &&\r\n(cmpxchg_relaxed(&l->wmode, 0, _QW_WAITING) == 0))\r\nbreak;\r\ncpu_relax();\r\n}\r\nfor (;;) {\r\ncnts = atomic_read(&lock->cnts);\r\nif ((cnts == _QW_WAITING) &&\r\n(atomic_cmpxchg_acquire(&lock->cnts, _QW_WAITING,\r\n_QW_LOCKED) == _QW_WAITING))\r\nbreak;\r\ncpu_relax();\r\n}\r\nunlock:\r\narch_spin_unlock(&lock->wait_lock);\r\n}
