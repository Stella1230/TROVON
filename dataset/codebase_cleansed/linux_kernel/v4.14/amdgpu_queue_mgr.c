static int amdgpu_queue_mapper_init(struct amdgpu_queue_mapper *mapper,\r\nint hw_ip)\r\n{\r\nif (!mapper)\r\nreturn -EINVAL;\r\nif (hw_ip > AMDGPU_MAX_IP_NUM)\r\nreturn -EINVAL;\r\nmapper->hw_ip = hw_ip;\r\nmutex_init(&mapper->lock);\r\nmemset(mapper->queue_map, 0, sizeof(mapper->queue_map));\r\nreturn 0;\r\n}\r\nstatic struct amdgpu_ring *amdgpu_get_cached_map(struct amdgpu_queue_mapper *mapper,\r\nint ring)\r\n{\r\nreturn mapper->queue_map[ring];\r\n}\r\nstatic int amdgpu_update_cached_map(struct amdgpu_queue_mapper *mapper,\r\nint ring, struct amdgpu_ring *pring)\r\n{\r\nif (WARN_ON(mapper->queue_map[ring])) {\r\nDRM_ERROR("Un-expected ring re-map\n");\r\nreturn -EINVAL;\r\n}\r\nmapper->queue_map[ring] = pring;\r\nreturn 0;\r\n}\r\nstatic int amdgpu_identity_map(struct amdgpu_device *adev,\r\nstruct amdgpu_queue_mapper *mapper,\r\nint ring,\r\nstruct amdgpu_ring **out_ring)\r\n{\r\nswitch (mapper->hw_ip) {\r\ncase AMDGPU_HW_IP_GFX:\r\n*out_ring = &adev->gfx.gfx_ring[ring];\r\nbreak;\r\ncase AMDGPU_HW_IP_COMPUTE:\r\n*out_ring = &adev->gfx.compute_ring[ring];\r\nbreak;\r\ncase AMDGPU_HW_IP_DMA:\r\n*out_ring = &adev->sdma.instance[ring].ring;\r\nbreak;\r\ncase AMDGPU_HW_IP_UVD:\r\n*out_ring = &adev->uvd.ring;\r\nbreak;\r\ncase AMDGPU_HW_IP_VCE:\r\n*out_ring = &adev->vce.ring[ring];\r\nbreak;\r\ncase AMDGPU_HW_IP_UVD_ENC:\r\n*out_ring = &adev->uvd.ring_enc[ring];\r\nbreak;\r\ncase AMDGPU_HW_IP_VCN_DEC:\r\n*out_ring = &adev->vcn.ring_dec;\r\nbreak;\r\ncase AMDGPU_HW_IP_VCN_ENC:\r\n*out_ring = &adev->vcn.ring_enc[ring];\r\nbreak;\r\ndefault:\r\n*out_ring = NULL;\r\nDRM_ERROR("unknown HW IP type: %d\n", mapper->hw_ip);\r\nreturn -EINVAL;\r\n}\r\nreturn amdgpu_update_cached_map(mapper, ring, *out_ring);\r\n}\r\nstatic enum amdgpu_ring_type amdgpu_hw_ip_to_ring_type(int hw_ip)\r\n{\r\nswitch (hw_ip) {\r\ncase AMDGPU_HW_IP_GFX:\r\nreturn AMDGPU_RING_TYPE_GFX;\r\ncase AMDGPU_HW_IP_COMPUTE:\r\nreturn AMDGPU_RING_TYPE_COMPUTE;\r\ncase AMDGPU_HW_IP_DMA:\r\nreturn AMDGPU_RING_TYPE_SDMA;\r\ncase AMDGPU_HW_IP_UVD:\r\nreturn AMDGPU_RING_TYPE_UVD;\r\ncase AMDGPU_HW_IP_VCE:\r\nreturn AMDGPU_RING_TYPE_VCE;\r\ndefault:\r\nDRM_ERROR("Invalid HW IP specified %d\n", hw_ip);\r\nreturn -1;\r\n}\r\n}\r\nstatic int amdgpu_lru_map(struct amdgpu_device *adev,\r\nstruct amdgpu_queue_mapper *mapper,\r\nint user_ring,\r\nstruct amdgpu_ring **out_ring)\r\n{\r\nint r, i, j;\r\nint ring_type = amdgpu_hw_ip_to_ring_type(mapper->hw_ip);\r\nint ring_blacklist[AMDGPU_MAX_RINGS];\r\nstruct amdgpu_ring *ring;\r\nmemset(ring_blacklist, 0xff, sizeof(ring_blacklist));\r\nfor (i = 0, j = 0; i < AMDGPU_MAX_RINGS; i++) {\r\nring = mapper->queue_map[i];\r\nif (ring)\r\nring_blacklist[j++] = ring->idx;\r\n}\r\nr = amdgpu_ring_lru_get(adev, ring_type, ring_blacklist,\r\nj, out_ring);\r\nif (r)\r\nreturn r;\r\nreturn amdgpu_update_cached_map(mapper, user_ring, *out_ring);\r\n}\r\nint amdgpu_queue_mgr_init(struct amdgpu_device *adev,\r\nstruct amdgpu_queue_mgr *mgr)\r\n{\r\nint i, r;\r\nif (!adev || !mgr)\r\nreturn -EINVAL;\r\nmemset(mgr, 0, sizeof(*mgr));\r\nfor (i = 0; i < AMDGPU_MAX_IP_NUM; ++i) {\r\nr = amdgpu_queue_mapper_init(&mgr->mapper[i], i);\r\nif (r)\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nint amdgpu_queue_mgr_fini(struct amdgpu_device *adev,\r\nstruct amdgpu_queue_mgr *mgr)\r\n{\r\nreturn 0;\r\n}\r\nint amdgpu_queue_mgr_map(struct amdgpu_device *adev,\r\nstruct amdgpu_queue_mgr *mgr,\r\nint hw_ip, int instance, int ring,\r\nstruct amdgpu_ring **out_ring)\r\n{\r\nint r, ip_num_rings;\r\nstruct amdgpu_queue_mapper *mapper = &mgr->mapper[hw_ip];\r\nif (!adev || !mgr || !out_ring)\r\nreturn -EINVAL;\r\nif (hw_ip >= AMDGPU_MAX_IP_NUM)\r\nreturn -EINVAL;\r\nif (ring >= AMDGPU_MAX_RINGS)\r\nreturn -EINVAL;\r\nif (instance != 0) {\r\nDRM_ERROR("invalid ip instance: %d\n", instance);\r\nreturn -EINVAL;\r\n}\r\nswitch (hw_ip) {\r\ncase AMDGPU_HW_IP_GFX:\r\nip_num_rings = adev->gfx.num_gfx_rings;\r\nbreak;\r\ncase AMDGPU_HW_IP_COMPUTE:\r\nip_num_rings = adev->gfx.num_compute_rings;\r\nbreak;\r\ncase AMDGPU_HW_IP_DMA:\r\nip_num_rings = adev->sdma.num_instances;\r\nbreak;\r\ncase AMDGPU_HW_IP_UVD:\r\nip_num_rings = 1;\r\nbreak;\r\ncase AMDGPU_HW_IP_VCE:\r\nip_num_rings = adev->vce.num_rings;\r\nbreak;\r\ncase AMDGPU_HW_IP_UVD_ENC:\r\nip_num_rings = adev->uvd.num_enc_rings;\r\nbreak;\r\ncase AMDGPU_HW_IP_VCN_DEC:\r\nip_num_rings = 1;\r\nbreak;\r\ncase AMDGPU_HW_IP_VCN_ENC:\r\nip_num_rings = adev->vcn.num_enc_rings;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("unknown ip type: %d\n", hw_ip);\r\nreturn -EINVAL;\r\n}\r\nif (ring >= ip_num_rings) {\r\nDRM_ERROR("Ring index:%d exceeds maximum:%d for ip:%d\n",\r\nring, ip_num_rings, hw_ip);\r\nreturn -EINVAL;\r\n}\r\nmutex_lock(&mapper->lock);\r\n*out_ring = amdgpu_get_cached_map(mapper, ring);\r\nif (*out_ring) {\r\nr = 0;\r\ngoto out_unlock;\r\n}\r\nswitch (mapper->hw_ip) {\r\ncase AMDGPU_HW_IP_GFX:\r\ncase AMDGPU_HW_IP_UVD:\r\ncase AMDGPU_HW_IP_VCE:\r\ncase AMDGPU_HW_IP_UVD_ENC:\r\ncase AMDGPU_HW_IP_VCN_DEC:\r\ncase AMDGPU_HW_IP_VCN_ENC:\r\nr = amdgpu_identity_map(adev, mapper, ring, out_ring);\r\nbreak;\r\ncase AMDGPU_HW_IP_DMA:\r\ncase AMDGPU_HW_IP_COMPUTE:\r\nr = amdgpu_lru_map(adev, mapper, ring, out_ring);\r\nbreak;\r\ndefault:\r\n*out_ring = NULL;\r\nr = -EINVAL;\r\nDRM_ERROR("unknown HW IP type: %d\n", mapper->hw_ip);\r\n}\r\nout_unlock:\r\nmutex_unlock(&mapper->lock);\r\nreturn r;\r\n}
