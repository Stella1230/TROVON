static unsigned int atomisp_get_pte(phys_addr_t pt, unsigned int idx)\r\n{\r\nunsigned int *pt_virt = phys_to_virt(pt);\r\nreturn *(pt_virt + idx);\r\n}\r\nstatic void atomisp_set_pte(phys_addr_t pt,\r\nunsigned int idx, unsigned int pte)\r\n{\r\nunsigned int *pt_virt = phys_to_virt(pt);\r\n*(pt_virt + idx) = pte;\r\n}\r\nstatic void *isp_pt_phys_to_virt(phys_addr_t phys)\r\n{\r\nreturn phys_to_virt(phys);\r\n}\r\nstatic phys_addr_t isp_pte_to_pgaddr(struct isp_mmu *mmu,\r\nunsigned int pte)\r\n{\r\nreturn mmu->driver->pte_to_phys(mmu, pte);\r\n}\r\nstatic unsigned int isp_pgaddr_to_pte_valid(struct isp_mmu *mmu,\r\nphys_addr_t phys)\r\n{\r\nunsigned int pte = mmu->driver->phys_to_pte(mmu, phys);\r\nreturn (unsigned int) (pte | ISP_PTE_VALID_MASK(mmu));\r\n}\r\nstatic phys_addr_t alloc_page_table(struct isp_mmu *mmu)\r\n{\r\nint i;\r\nphys_addr_t page;\r\nvoid *virt;\r\nif (totalram_pages > (unsigned long)NR_PAGES_2GB)\r\nvirt = (void *)__get_free_page(GFP_KERNEL | GFP_DMA32);\r\nelse\r\nvirt = kmem_cache_zalloc(mmu->tbl_cache, GFP_KERNEL);\r\nif (!virt)\r\nreturn (phys_addr_t)NULL_PAGE;\r\n#ifdef CONFIG_X86\r\nset_memory_uc((unsigned long)virt, 1);\r\n#endif\r\npage = virt_to_phys(virt);\r\nfor (i = 0; i < 1024; i++) {\r\natomisp_set_pte(page, i, mmu->driver->null_pte);\r\n}\r\nreturn page;\r\n}\r\nstatic void free_page_table(struct isp_mmu *mmu, phys_addr_t page)\r\n{\r\nvoid *virt;\r\npage &= ISP_PAGE_MASK;\r\nvirt = phys_to_virt(page);\r\n#ifdef CONFIG_X86\r\nset_memory_wb((unsigned long)virt, 1);\r\n#endif\r\nkmem_cache_free(mmu->tbl_cache, virt);\r\n}\r\nstatic void mmu_remap_error(struct isp_mmu *mmu,\r\nphys_addr_t l1_pt, unsigned int l1_idx,\r\nphys_addr_t l2_pt, unsigned int l2_idx,\r\nunsigned int isp_virt, phys_addr_t old_phys,\r\nphys_addr_t new_phys)\r\n{\r\ndev_err(atomisp_dev, "address remap:\n\n"\r\n"\tL1 PT: virt = %p, phys = 0x%llx, "\r\n"idx = %d\n"\r\n"\tL2 PT: virt = %p, phys = 0x%llx, "\r\n"idx = %d\n"\r\n"\told: isp_virt = 0x%x, phys = 0x%llx\n"\r\n"\tnew: isp_virt = 0x%x, phys = 0x%llx\n",\r\nisp_pt_phys_to_virt(l1_pt),\r\n(u64)l1_pt, l1_idx,\r\nisp_pt_phys_to_virt(l2_pt),\r\n(u64)l2_pt, l2_idx, isp_virt,\r\n(u64)old_phys, isp_virt,\r\n(u64)new_phys);\r\n}\r\nstatic void mmu_unmap_l2_pte_error(struct isp_mmu *mmu,\r\nphys_addr_t l1_pt, unsigned int l1_idx,\r\nphys_addr_t l2_pt, unsigned int l2_idx,\r\nunsigned int isp_virt, unsigned int pte)\r\n{\r\ndev_err(atomisp_dev, "unmap unvalid L2 pte:\n\n"\r\n"\tL1 PT: virt = %p, phys = 0x%llx, "\r\n"idx = %d\n"\r\n"\tL2 PT: virt = %p, phys = 0x%llx, "\r\n"idx = %d\n"\r\n"\tisp_virt = 0x%x, pte(page phys) = 0x%x\n",\r\nisp_pt_phys_to_virt(l1_pt),\r\n(u64)l1_pt, l1_idx,\r\nisp_pt_phys_to_virt(l2_pt),\r\n(u64)l2_pt, l2_idx, isp_virt,\r\npte);\r\n}\r\nstatic void mmu_unmap_l1_pte_error(struct isp_mmu *mmu,\r\nphys_addr_t l1_pt, unsigned int l1_idx,\r\nunsigned int isp_virt, unsigned int pte)\r\n{\r\ndev_err(atomisp_dev, "unmap unvalid L1 pte (L2 PT):\n\n"\r\n"\tL1 PT: virt = %p, phys = 0x%llx, "\r\n"idx = %d\n"\r\n"\tisp_virt = 0x%x, l1_pte(L2 PT) = 0x%x\n",\r\nisp_pt_phys_to_virt(l1_pt),\r\n(u64)l1_pt, l1_idx, (unsigned int)isp_virt,\r\npte);\r\n}\r\nstatic void mmu_unmap_l1_pt_error(struct isp_mmu *mmu, unsigned int pte)\r\n{\r\ndev_err(atomisp_dev, "unmap unvalid L1PT:\n\n"\r\n"L1PT = 0x%x\n", (unsigned int)pte);\r\n}\r\nstatic int mmu_l2_map(struct isp_mmu *mmu, phys_addr_t l1_pt,\r\nunsigned int l1_idx, phys_addr_t l2_pt,\r\nunsigned int start, unsigned int end, phys_addr_t phys)\r\n{\r\nunsigned int ptr;\r\nunsigned int idx;\r\nunsigned int pte;\r\nl2_pt &= ISP_PAGE_MASK;\r\nstart = start & ISP_PAGE_MASK;\r\nend = ISP_PAGE_ALIGN(end);\r\nphys &= ISP_PAGE_MASK;\r\nptr = start;\r\ndo {\r\nidx = ISP_PTR_TO_L2_IDX(ptr);\r\npte = atomisp_get_pte(l2_pt, idx);\r\nif (ISP_PTE_VALID(mmu, pte)) {\r\nmmu_remap_error(mmu, l1_pt, l1_idx,\r\nl2_pt, idx, ptr, pte, phys);\r\nfree_mmu_map(mmu, start, ptr);\r\nreturn -EINVAL;\r\n}\r\npte = isp_pgaddr_to_pte_valid(mmu, phys);\r\natomisp_set_pte(l2_pt, idx, pte);\r\nmmu->l2_pgt_refcount[l1_idx]++;\r\nptr += (1U << ISP_L2PT_OFFSET);\r\nphys += (1U << ISP_L2PT_OFFSET);\r\n} while (ptr < end && idx < ISP_L2PT_PTES - 1);\r\nreturn 0;\r\n}\r\nstatic int mmu_l1_map(struct isp_mmu *mmu, phys_addr_t l1_pt,\r\nunsigned int start, unsigned int end,\r\nphys_addr_t phys)\r\n{\r\nphys_addr_t l2_pt;\r\nunsigned int ptr, l1_aligned;\r\nunsigned int idx;\r\nunsigned int l2_pte;\r\nint ret;\r\nl1_pt &= ISP_PAGE_MASK;\r\nstart = start & ISP_PAGE_MASK;\r\nend = ISP_PAGE_ALIGN(end);\r\nphys &= ISP_PAGE_MASK;\r\nptr = start;\r\ndo {\r\nidx = ISP_PTR_TO_L1_IDX(ptr);\r\nl2_pte = atomisp_get_pte(l1_pt, idx);\r\nif (!ISP_PTE_VALID(mmu, l2_pte)) {\r\nl2_pt = alloc_page_table(mmu);\r\nif (l2_pt == NULL_PAGE) {\r\ndev_err(atomisp_dev,\r\n"alloc page table fail.\n");\r\nfree_mmu_map(mmu, start, ptr);\r\nreturn -ENOMEM;\r\n}\r\nl2_pte = isp_pgaddr_to_pte_valid(mmu, l2_pt);\r\natomisp_set_pte(l1_pt, idx, l2_pte);\r\nmmu->l2_pgt_refcount[idx] = 0;\r\n}\r\nl2_pt = isp_pte_to_pgaddr(mmu, l2_pte);\r\nl1_aligned = (ptr & ISP_PAGE_MASK) + (1U << ISP_L1PT_OFFSET);\r\nif (l1_aligned < end) {\r\nret = mmu_l2_map(mmu, l1_pt, idx,\r\nl2_pt, ptr, l1_aligned, phys);\r\nphys += (l1_aligned - ptr);\r\nptr = l1_aligned;\r\n} else {\r\nret = mmu_l2_map(mmu, l1_pt, idx,\r\nl2_pt, ptr, end, phys);\r\nphys += (end - ptr);\r\nptr = end;\r\n}\r\nif (ret) {\r\ndev_err(atomisp_dev, "setup mapping in L2PT fail.\n");\r\nfree_mmu_map(mmu, start, ptr);\r\nreturn -EINVAL;\r\n}\r\n} while (ptr < end && idx < ISP_L1PT_PTES);\r\nreturn 0;\r\n}\r\nstatic int mmu_map(struct isp_mmu *mmu, unsigned int isp_virt,\r\nphys_addr_t phys, unsigned int pgnr)\r\n{\r\nunsigned int start, end;\r\nphys_addr_t l1_pt;\r\nint ret;\r\nmutex_lock(&mmu->pt_mutex);\r\nif (!ISP_PTE_VALID(mmu, mmu->l1_pte)) {\r\nl1_pt = alloc_page_table(mmu);\r\nif (l1_pt == NULL_PAGE) {\r\ndev_err(atomisp_dev, "alloc page table fail.\n");\r\nmutex_unlock(&mmu->pt_mutex);\r\nreturn -ENOMEM;\r\n}\r\nret = mmu->driver->set_pd_base(mmu, l1_pt);\r\nif (ret) {\r\ndev_err(atomisp_dev,\r\n"set page directory base address fail.\n");\r\nmutex_unlock(&mmu->pt_mutex);\r\nreturn ret;\r\n}\r\nmmu->base_address = l1_pt;\r\nmmu->l1_pte = isp_pgaddr_to_pte_valid(mmu, l1_pt);\r\nmemset(mmu->l2_pgt_refcount, 0, sizeof(int) * ISP_L1PT_PTES);\r\n}\r\nl1_pt = isp_pte_to_pgaddr(mmu, mmu->l1_pte);\r\nstart = (isp_virt) & ISP_PAGE_MASK;\r\nend = start + (pgnr << ISP_PAGE_OFFSET);\r\nphys &= ISP_PAGE_MASK;\r\nret = mmu_l1_map(mmu, l1_pt, start, end, phys);\r\nif (ret)\r\ndev_err(atomisp_dev, "setup mapping in L1PT fail.\n");\r\nmutex_unlock(&mmu->pt_mutex);\r\nreturn ret;\r\n}\r\nstatic void mmu_l2_unmap(struct isp_mmu *mmu, phys_addr_t l1_pt,\r\nunsigned int l1_idx, phys_addr_t l2_pt,\r\nunsigned int start, unsigned int end)\r\n{\r\nunsigned int ptr;\r\nunsigned int idx;\r\nunsigned int pte;\r\nl2_pt &= ISP_PAGE_MASK;\r\nstart = start & ISP_PAGE_MASK;\r\nend = ISP_PAGE_ALIGN(end);\r\nptr = start;\r\ndo {\r\nidx = ISP_PTR_TO_L2_IDX(ptr);\r\npte = atomisp_get_pte(l2_pt, idx);\r\nif (!ISP_PTE_VALID(mmu, pte))\r\nmmu_unmap_l2_pte_error(mmu, l1_pt, l1_idx,\r\nl2_pt, idx, ptr, pte);\r\natomisp_set_pte(l2_pt, idx, mmu->driver->null_pte);\r\nmmu->l2_pgt_refcount[l1_idx]--;\r\nptr += (1U << ISP_L2PT_OFFSET);\r\n} while (ptr < end && idx < ISP_L2PT_PTES - 1);\r\nif (mmu->l2_pgt_refcount[l1_idx] == 0) {\r\nfree_page_table(mmu, l2_pt);\r\natomisp_set_pte(l1_pt, l1_idx, mmu->driver->null_pte);\r\n}\r\n}\r\nstatic void mmu_l1_unmap(struct isp_mmu *mmu, phys_addr_t l1_pt,\r\nunsigned int start, unsigned int end)\r\n{\r\nphys_addr_t l2_pt;\r\nunsigned int ptr, l1_aligned;\r\nunsigned int idx;\r\nunsigned int l2_pte;\r\nl1_pt &= ISP_PAGE_MASK;\r\nstart = start & ISP_PAGE_MASK;\r\nend = ISP_PAGE_ALIGN(end);\r\nptr = start;\r\ndo {\r\nidx = ISP_PTR_TO_L1_IDX(ptr);\r\nl2_pte = atomisp_get_pte(l1_pt, idx);\r\nif (!ISP_PTE_VALID(mmu, l2_pte)) {\r\nmmu_unmap_l1_pte_error(mmu, l1_pt, idx, ptr, l2_pte);\r\ncontinue;\r\n}\r\nl2_pt = isp_pte_to_pgaddr(mmu, l2_pte);\r\nl1_aligned = (ptr & ISP_PAGE_MASK) + (1U << ISP_L1PT_OFFSET);\r\nif (l1_aligned < end) {\r\nmmu_l2_unmap(mmu, l1_pt, idx, l2_pt, ptr, l1_aligned);\r\nptr = l1_aligned;\r\n} else {\r\nmmu_l2_unmap(mmu, l1_pt, idx, l2_pt, ptr, end);\r\nptr = end;\r\n}\r\n} while (ptr < end && idx < ISP_L1PT_PTES);\r\n}\r\nstatic void mmu_unmap(struct isp_mmu *mmu, unsigned int isp_virt,\r\nunsigned int pgnr)\r\n{\r\nunsigned int start, end;\r\nphys_addr_t l1_pt;\r\nmutex_lock(&mmu->pt_mutex);\r\nif (!ISP_PTE_VALID(mmu, mmu->l1_pte)) {\r\nmmu_unmap_l1_pt_error(mmu, mmu->l1_pte);\r\nmutex_unlock(&mmu->pt_mutex);\r\nreturn;\r\n}\r\nl1_pt = isp_pte_to_pgaddr(mmu, mmu->l1_pte);\r\nstart = (isp_virt) & ISP_PAGE_MASK;\r\nend = start + (pgnr << ISP_PAGE_OFFSET);\r\nmmu_l1_unmap(mmu, l1_pt, start, end);\r\nmutex_unlock(&mmu->pt_mutex);\r\n}\r\nstatic void free_mmu_map(struct isp_mmu *mmu, unsigned int start_isp_virt,\r\nunsigned int end_isp_virt)\r\n{\r\nunsigned int pgnr;\r\nunsigned int start, end;\r\nstart = (start_isp_virt) & ISP_PAGE_MASK;\r\nend = (end_isp_virt) & ISP_PAGE_MASK;\r\npgnr = (end - start) >> ISP_PAGE_OFFSET;\r\nmmu_unmap(mmu, start, pgnr);\r\n}\r\nint isp_mmu_map(struct isp_mmu *mmu, unsigned int isp_virt,\r\nphys_addr_t phys, unsigned int pgnr)\r\n{\r\nreturn mmu_map(mmu, isp_virt, phys, pgnr);\r\n}\r\nvoid isp_mmu_unmap(struct isp_mmu *mmu, unsigned int isp_virt,\r\nunsigned int pgnr)\r\n{\r\nmmu_unmap(mmu, isp_virt, pgnr);\r\n}\r\nstatic void isp_mmu_flush_tlb_range_default(struct isp_mmu *mmu,\r\nunsigned int start,\r\nunsigned int size)\r\n{\r\nisp_mmu_flush_tlb(mmu);\r\n}\r\nint isp_mmu_init(struct isp_mmu *mmu, struct isp_mmu_client *driver)\r\n{\r\nif (!mmu)\r\nreturn -EINVAL;\r\nif (!driver)\r\nreturn -EINVAL;\r\nif (!driver->name)\r\ndev_warn(atomisp_dev, "NULL name for MMU driver...\n");\r\nmmu->driver = driver;\r\nif (!driver->set_pd_base || !driver->tlb_flush_all) {\r\ndev_err(atomisp_dev,\r\n"set_pd_base or tlb_flush_all operation "\r\n"not provided.\n");\r\nreturn -EINVAL;\r\n}\r\nif (!driver->tlb_flush_range)\r\ndriver->tlb_flush_range = isp_mmu_flush_tlb_range_default;\r\nif (!driver->pte_valid_mask) {\r\ndev_err(atomisp_dev, "PTE_MASK is missing from mmu driver\n");\r\nreturn -EINVAL;\r\n}\r\nmmu->l1_pte = driver->null_pte;\r\nmutex_init(&mmu->pt_mutex);\r\nmmu->tbl_cache = kmem_cache_create("iopte_cache", ISP_PAGE_SIZE,\r\nISP_PAGE_SIZE, SLAB_HWCACHE_ALIGN,\r\nNULL);\r\nif (!mmu->tbl_cache)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid isp_mmu_exit(struct isp_mmu *mmu)\r\n{\r\nunsigned int idx;\r\nunsigned int pte;\r\nphys_addr_t l1_pt, l2_pt;\r\nif (!mmu)\r\nreturn;\r\nif (!ISP_PTE_VALID(mmu, mmu->l1_pte)) {\r\ndev_warn(atomisp_dev, "invalid L1PT: pte = 0x%x\n",\r\n(unsigned int)mmu->l1_pte);\r\nreturn;\r\n}\r\nl1_pt = isp_pte_to_pgaddr(mmu, mmu->l1_pte);\r\nfor (idx = 0; idx < ISP_L1PT_PTES; idx++) {\r\npte = atomisp_get_pte(l1_pt, idx);\r\nif (ISP_PTE_VALID(mmu, pte)) {\r\nl2_pt = isp_pte_to_pgaddr(mmu, pte);\r\nfree_page_table(mmu, l2_pt);\r\n}\r\n}\r\nfree_page_table(mmu, l1_pt);\r\nkmem_cache_destroy(mmu->tbl_cache);\r\n}
