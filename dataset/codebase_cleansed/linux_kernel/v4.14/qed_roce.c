static int\r\nqed_roce_async_event(struct qed_hwfn *p_hwfn,\r\nu8 fw_event_code,\r\nu16 echo, union event_ring_data *data, u8 fw_return_code)\r\n{\r\nif (fw_event_code == ROCE_ASYNC_EVENT_DESTROY_QP_DONE) {\r\nu16 icid =\r\n(u16)le32_to_cpu(data->rdma_data.rdma_destroy_qp_data.cid);\r\nqed_roce_free_real_icid(p_hwfn, icid);\r\n} else {\r\nstruct qed_rdma_events *events = &p_hwfn->p_rdma_info->events;\r\nevents->affiliated_event(p_hwfn->p_rdma_info->events.context,\r\nfw_event_code,\r\n(void *)&data->rdma_data.async_handle);\r\n}\r\nreturn 0;\r\n}\r\nvoid qed_roce_stop(struct qed_hwfn *p_hwfn)\r\n{\r\nstruct qed_bmap *rcid_map = &p_hwfn->p_rdma_info->real_cid_map;\r\nint wait_count = 0;\r\nwhile (bitmap_weight(rcid_map->bitmap, rcid_map->max_count)) {\r\nmsleep(100);\r\nif (wait_count++ > 20) {\r\nDP_NOTICE(p_hwfn, "cid bitmap wait timed out\n");\r\nbreak;\r\n}\r\n}\r\nqed_spq_unregister_async_cb(p_hwfn, PROTOCOLID_ROCE);\r\n}\r\nstatic void qed_rdma_copy_gids(struct qed_rdma_qp *qp, __le32 *src_gid,\r\n__le32 *dst_gid)\r\n{\r\nu32 i;\r\nif (qp->roce_mode == ROCE_V2_IPV4) {\r\nmemset(src_gid, 0, sizeof(union qed_gid));\r\nmemset(dst_gid, 0, sizeof(union qed_gid));\r\nsrc_gid[3] = cpu_to_le32(qp->sgid.ipv4_addr);\r\ndst_gid[3] = cpu_to_le32(qp->dgid.ipv4_addr);\r\n} else {\r\nfor (i = 0; i < ARRAY_SIZE(qp->sgid.dwords); i++) {\r\nsrc_gid[i] = cpu_to_le32(qp->sgid.dwords[i]);\r\ndst_gid[i] = cpu_to_le32(qp->dgid.dwords[i]);\r\n}\r\n}\r\n}\r\nstatic enum roce_flavor qed_roce_mode_to_flavor(enum roce_mode roce_mode)\r\n{\r\nenum roce_flavor flavor;\r\nswitch (roce_mode) {\r\ncase ROCE_V1:\r\nflavor = PLAIN_ROCE;\r\nbreak;\r\ncase ROCE_V2_IPV4:\r\nflavor = RROCE_IPV4;\r\nbreak;\r\ncase ROCE_V2_IPV6:\r\nflavor = ROCE_V2_IPV6;\r\nbreak;\r\ndefault:\r\nflavor = MAX_ROCE_MODE;\r\nbreak;\r\n}\r\nreturn flavor;\r\n}\r\nvoid qed_roce_free_cid_pair(struct qed_hwfn *p_hwfn, u16 cid)\r\n{\r\nspin_lock_bh(&p_hwfn->p_rdma_info->lock);\r\nqed_bmap_release_id(p_hwfn, &p_hwfn->p_rdma_info->cid_map, cid);\r\nqed_bmap_release_id(p_hwfn, &p_hwfn->p_rdma_info->cid_map, cid + 1);\r\nspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\r\n}\r\nint qed_roce_alloc_cid(struct qed_hwfn *p_hwfn, u16 *cid)\r\n{\r\nstruct qed_rdma_info *p_rdma_info = p_hwfn->p_rdma_info;\r\nu32 responder_icid;\r\nu32 requester_icid;\r\nint rc;\r\nspin_lock_bh(&p_hwfn->p_rdma_info->lock);\r\nrc = qed_rdma_bmap_alloc_id(p_hwfn, &p_rdma_info->cid_map,\r\n&responder_icid);\r\nif (rc) {\r\nspin_unlock_bh(&p_rdma_info->lock);\r\nreturn rc;\r\n}\r\nrc = qed_rdma_bmap_alloc_id(p_hwfn, &p_rdma_info->cid_map,\r\n&requester_icid);\r\nspin_unlock_bh(&p_rdma_info->lock);\r\nif (rc)\r\ngoto err;\r\nif ((requester_icid - responder_icid) != 1) {\r\nDP_NOTICE(p_hwfn, "Failed to allocate two adjacent qp's'\n");\r\nrc = -EINVAL;\r\ngoto err;\r\n}\r\nresponder_icid += qed_cxt_get_proto_cid_start(p_hwfn,\r\np_rdma_info->proto);\r\nrequester_icid += qed_cxt_get_proto_cid_start(p_hwfn,\r\np_rdma_info->proto);\r\nrc = qed_cxt_dynamic_ilt_alloc(p_hwfn, QED_ELEM_CXT, responder_icid);\r\nif (rc)\r\ngoto err;\r\nrc = qed_cxt_dynamic_ilt_alloc(p_hwfn, QED_ELEM_CXT, requester_icid);\r\nif (rc)\r\ngoto err;\r\n*cid = (u16)responder_icid;\r\nreturn rc;\r\nerr:\r\nspin_lock_bh(&p_rdma_info->lock);\r\nqed_bmap_release_id(p_hwfn, &p_rdma_info->cid_map, responder_icid);\r\nqed_bmap_release_id(p_hwfn, &p_rdma_info->cid_map, requester_icid);\r\nspin_unlock_bh(&p_rdma_info->lock);\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\r\n"Allocate CID - failed, rc = %d\n", rc);\r\nreturn rc;\r\n}\r\nstatic void qed_roce_set_real_cid(struct qed_hwfn *p_hwfn, u32 cid)\r\n{\r\nspin_lock_bh(&p_hwfn->p_rdma_info->lock);\r\nqed_bmap_set_id(p_hwfn, &p_hwfn->p_rdma_info->real_cid_map, cid);\r\nspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\r\n}\r\nstatic int qed_roce_sp_create_responder(struct qed_hwfn *p_hwfn,\r\nstruct qed_rdma_qp *qp)\r\n{\r\nstruct roce_create_qp_resp_ramrod_data *p_ramrod;\r\nstruct qed_sp_init_data init_data;\r\nenum roce_flavor roce_flavor;\r\nstruct qed_spq_entry *p_ent;\r\nu16 regular_latency_queue;\r\nenum protocol_type proto;\r\nint rc;\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "icid = %08x\n", qp->icid);\r\nqp->irq_num_pages = 1;\r\nqp->irq = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\r\nRDMA_RING_PAGE_SIZE,\r\n&qp->irq_phys_addr, GFP_KERNEL);\r\nif (!qp->irq) {\r\nrc = -ENOMEM;\r\nDP_NOTICE(p_hwfn,\r\n"qed create responder failed: cannot allocate memory (irq). rc = %d\n",\r\nrc);\r\nreturn rc;\r\n}\r\nmemset(&init_data, 0, sizeof(init_data));\r\ninit_data.cid = qp->icid;\r\ninit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\r\ninit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\r\nrc = qed_sp_init_request(p_hwfn, &p_ent, ROCE_RAMROD_CREATE_QP,\r\nPROTOCOLID_ROCE, &init_data);\r\nif (rc)\r\ngoto err;\r\np_ramrod = &p_ent->ramrod.roce_create_qp_resp;\r\np_ramrod->flags = 0;\r\nroce_flavor = qed_roce_mode_to_flavor(qp->roce_mode);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_RESP_RAMROD_DATA_ROCE_FLAVOR, roce_flavor);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_RESP_RAMROD_DATA_RDMA_RD_EN,\r\nqp->incoming_rdma_read_en);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_RESP_RAMROD_DATA_RDMA_WR_EN,\r\nqp->incoming_rdma_write_en);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_RESP_RAMROD_DATA_ATOMIC_EN,\r\nqp->incoming_atomic_en);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_RESP_RAMROD_DATA_E2E_FLOW_CONTROL_EN,\r\nqp->e2e_flow_control_en);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_RESP_RAMROD_DATA_SRQ_FLG, qp->use_srq);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_RESP_RAMROD_DATA_RESERVED_KEY_EN,\r\nqp->fmr_and_reserved_lkey);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_RESP_RAMROD_DATA_MIN_RNR_NAK_TIMER,\r\nqp->min_rnr_nak_timer);\r\np_ramrod->max_ird = qp->max_rd_atomic_resp;\r\np_ramrod->traffic_class = qp->traffic_class_tos;\r\np_ramrod->hop_limit = qp->hop_limit_ttl;\r\np_ramrod->irq_num_pages = qp->irq_num_pages;\r\np_ramrod->p_key = cpu_to_le16(qp->pkey);\r\np_ramrod->flow_label = cpu_to_le32(qp->flow_label);\r\np_ramrod->dst_qp_id = cpu_to_le32(qp->dest_qp);\r\np_ramrod->mtu = cpu_to_le16(qp->mtu);\r\np_ramrod->initial_psn = cpu_to_le32(qp->rq_psn);\r\np_ramrod->pd = cpu_to_le16(qp->pd);\r\np_ramrod->rq_num_pages = cpu_to_le16(qp->rq_num_pages);\r\nDMA_REGPAIR_LE(p_ramrod->rq_pbl_addr, qp->rq_pbl_ptr);\r\nDMA_REGPAIR_LE(p_ramrod->irq_pbl_addr, qp->irq_phys_addr);\r\nqed_rdma_copy_gids(qp, p_ramrod->src_gid, p_ramrod->dst_gid);\r\np_ramrod->qp_handle_for_async.hi = cpu_to_le32(qp->qp_handle_async.hi);\r\np_ramrod->qp_handle_for_async.lo = cpu_to_le32(qp->qp_handle_async.lo);\r\np_ramrod->qp_handle_for_cqe.hi = cpu_to_le32(qp->qp_handle.hi);\r\np_ramrod->qp_handle_for_cqe.lo = cpu_to_le32(qp->qp_handle.lo);\r\np_ramrod->cq_cid = cpu_to_le32((p_hwfn->hw_info.opaque_fid << 16) |\r\nqp->rq_cq_id);\r\nregular_latency_queue = qed_get_cm_pq_idx(p_hwfn, PQ_FLAGS_OFLD);\r\np_ramrod->regular_latency_phy_queue =\r\ncpu_to_le16(regular_latency_queue);\r\np_ramrod->low_latency_phy_queue =\r\ncpu_to_le16(regular_latency_queue);\r\np_ramrod->dpi = cpu_to_le16(qp->dpi);\r\nqed_rdma_set_fw_mac(p_ramrod->remote_mac_addr, qp->remote_mac_addr);\r\nqed_rdma_set_fw_mac(p_ramrod->local_mac_addr, qp->local_mac_addr);\r\np_ramrod->udp_src_port = qp->udp_src_port;\r\np_ramrod->vlan_id = cpu_to_le16(qp->vlan_id);\r\np_ramrod->srq_id.srq_idx = cpu_to_le16(qp->srq_id);\r\np_ramrod->srq_id.opaque_fid = cpu_to_le16(p_hwfn->hw_info.opaque_fid);\r\np_ramrod->stats_counter_id = RESC_START(p_hwfn, QED_RDMA_STATS_QUEUE) +\r\nqp->stats_queue;\r\nrc = qed_spq_post(p_hwfn, p_ent, NULL);\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA,\r\n"rc = %d regular physical queue = 0x%x\n", rc,\r\nregular_latency_queue);\r\nif (rc)\r\ngoto err;\r\nqp->resp_offloaded = true;\r\nqp->cq_prod = 0;\r\nproto = p_hwfn->p_rdma_info->proto;\r\nqed_roce_set_real_cid(p_hwfn, qp->icid -\r\nqed_cxt_get_proto_cid_start(p_hwfn, proto));\r\nreturn rc;\r\nerr:\r\nDP_NOTICE(p_hwfn, "create responder - failed, rc = %d\n", rc);\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev,\r\nqp->irq_num_pages * RDMA_RING_PAGE_SIZE,\r\nqp->irq, qp->irq_phys_addr);\r\nreturn rc;\r\n}\r\nstatic int qed_roce_sp_create_requester(struct qed_hwfn *p_hwfn,\r\nstruct qed_rdma_qp *qp)\r\n{\r\nstruct roce_create_qp_req_ramrod_data *p_ramrod;\r\nstruct qed_sp_init_data init_data;\r\nenum roce_flavor roce_flavor;\r\nstruct qed_spq_entry *p_ent;\r\nu16 regular_latency_queue;\r\nenum protocol_type proto;\r\nint rc;\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "icid = %08x\n", qp->icid);\r\nqp->orq_num_pages = 1;\r\nqp->orq = dma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\r\nRDMA_RING_PAGE_SIZE,\r\n&qp->orq_phys_addr, GFP_KERNEL);\r\nif (!qp->orq) {\r\nrc = -ENOMEM;\r\nDP_NOTICE(p_hwfn,\r\n"qed create requester failed: cannot allocate memory (orq). rc = %d\n",\r\nrc);\r\nreturn rc;\r\n}\r\nmemset(&init_data, 0, sizeof(init_data));\r\ninit_data.cid = qp->icid + 1;\r\ninit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\r\ninit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\r\nrc = qed_sp_init_request(p_hwfn, &p_ent,\r\nROCE_RAMROD_CREATE_QP,\r\nPROTOCOLID_ROCE, &init_data);\r\nif (rc)\r\ngoto err;\r\np_ramrod = &p_ent->ramrod.roce_create_qp_req;\r\np_ramrod->flags = 0;\r\nroce_flavor = qed_roce_mode_to_flavor(qp->roce_mode);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_REQ_RAMROD_DATA_ROCE_FLAVOR, roce_flavor);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_REQ_RAMROD_DATA_FMR_AND_RESERVED_EN,\r\nqp->fmr_and_reserved_lkey);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_REQ_RAMROD_DATA_SIGNALED_COMP, qp->signal_all);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_REQ_RAMROD_DATA_ERR_RETRY_CNT, qp->retry_cnt);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_REQ_RAMROD_DATA_RNR_NAK_CNT,\r\nqp->rnr_retry_cnt);\r\np_ramrod->max_ord = qp->max_rd_atomic_req;\r\np_ramrod->traffic_class = qp->traffic_class_tos;\r\np_ramrod->hop_limit = qp->hop_limit_ttl;\r\np_ramrod->orq_num_pages = qp->orq_num_pages;\r\np_ramrod->p_key = cpu_to_le16(qp->pkey);\r\np_ramrod->flow_label = cpu_to_le32(qp->flow_label);\r\np_ramrod->dst_qp_id = cpu_to_le32(qp->dest_qp);\r\np_ramrod->ack_timeout_val = cpu_to_le32(qp->ack_timeout);\r\np_ramrod->mtu = cpu_to_le16(qp->mtu);\r\np_ramrod->initial_psn = cpu_to_le32(qp->sq_psn);\r\np_ramrod->pd = cpu_to_le16(qp->pd);\r\np_ramrod->sq_num_pages = cpu_to_le16(qp->sq_num_pages);\r\nDMA_REGPAIR_LE(p_ramrod->sq_pbl_addr, qp->sq_pbl_ptr);\r\nDMA_REGPAIR_LE(p_ramrod->orq_pbl_addr, qp->orq_phys_addr);\r\nqed_rdma_copy_gids(qp, p_ramrod->src_gid, p_ramrod->dst_gid);\r\np_ramrod->qp_handle_for_async.hi = cpu_to_le32(qp->qp_handle_async.hi);\r\np_ramrod->qp_handle_for_async.lo = cpu_to_le32(qp->qp_handle_async.lo);\r\np_ramrod->qp_handle_for_cqe.hi = cpu_to_le32(qp->qp_handle.hi);\r\np_ramrod->qp_handle_for_cqe.lo = cpu_to_le32(qp->qp_handle.lo);\r\np_ramrod->cq_cid =\r\ncpu_to_le32((p_hwfn->hw_info.opaque_fid << 16) | qp->sq_cq_id);\r\nregular_latency_queue = qed_get_cm_pq_idx(p_hwfn, PQ_FLAGS_OFLD);\r\np_ramrod->regular_latency_phy_queue =\r\ncpu_to_le16(regular_latency_queue);\r\np_ramrod->low_latency_phy_queue =\r\ncpu_to_le16(regular_latency_queue);\r\np_ramrod->dpi = cpu_to_le16(qp->dpi);\r\nqed_rdma_set_fw_mac(p_ramrod->remote_mac_addr, qp->remote_mac_addr);\r\nqed_rdma_set_fw_mac(p_ramrod->local_mac_addr, qp->local_mac_addr);\r\np_ramrod->udp_src_port = qp->udp_src_port;\r\np_ramrod->vlan_id = cpu_to_le16(qp->vlan_id);\r\np_ramrod->stats_counter_id = RESC_START(p_hwfn, QED_RDMA_STATS_QUEUE) +\r\nqp->stats_queue;\r\nrc = qed_spq_post(p_hwfn, p_ent, NULL);\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "rc = %d\n", rc);\r\nif (rc)\r\ngoto err;\r\nqp->req_offloaded = true;\r\nproto = p_hwfn->p_rdma_info->proto;\r\nqed_roce_set_real_cid(p_hwfn,\r\nqp->icid + 1 -\r\nqed_cxt_get_proto_cid_start(p_hwfn, proto));\r\nreturn rc;\r\nerr:\r\nDP_NOTICE(p_hwfn, "Create requested - failed, rc = %d\n", rc);\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev,\r\nqp->orq_num_pages * RDMA_RING_PAGE_SIZE,\r\nqp->orq, qp->orq_phys_addr);\r\nreturn rc;\r\n}\r\nstatic int qed_roce_sp_modify_responder(struct qed_hwfn *p_hwfn,\r\nstruct qed_rdma_qp *qp,\r\nbool move_to_err, u32 modify_flags)\r\n{\r\nstruct roce_modify_qp_resp_ramrod_data *p_ramrod;\r\nstruct qed_sp_init_data init_data;\r\nstruct qed_spq_entry *p_ent;\r\nint rc;\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "icid = %08x\n", qp->icid);\r\nif (move_to_err && !qp->resp_offloaded)\r\nreturn 0;\r\nmemset(&init_data, 0, sizeof(init_data));\r\ninit_data.cid = qp->icid;\r\ninit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\r\ninit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\r\nrc = qed_sp_init_request(p_hwfn, &p_ent,\r\nROCE_EVENT_MODIFY_QP,\r\nPROTOCOLID_ROCE, &init_data);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn, "rc = %d\n", rc);\r\nreturn rc;\r\n}\r\np_ramrod = &p_ent->ramrod.roce_modify_qp_resp;\r\np_ramrod->flags = 0;\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_MOVE_TO_ERR_FLG, move_to_err);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_RDMA_RD_EN,\r\nqp->incoming_rdma_read_en);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_RDMA_WR_EN,\r\nqp->incoming_rdma_write_en);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_ATOMIC_EN,\r\nqp->incoming_atomic_en);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_CREATE_QP_RESP_RAMROD_DATA_E2E_FLOW_CONTROL_EN,\r\nqp->e2e_flow_control_en);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_RDMA_OPS_EN_FLG,\r\nGET_FIELD(modify_flags,\r\nQED_RDMA_MODIFY_QP_VALID_RDMA_OPS_EN));\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_P_KEY_FLG,\r\nGET_FIELD(modify_flags, QED_ROCE_MODIFY_QP_VALID_PKEY));\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_ADDRESS_VECTOR_FLG,\r\nGET_FIELD(modify_flags,\r\nQED_ROCE_MODIFY_QP_VALID_ADDRESS_VECTOR));\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_MAX_IRD_FLG,\r\nGET_FIELD(modify_flags,\r\nQED_RDMA_MODIFY_QP_VALID_MAX_RD_ATOMIC_RESP));\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_MIN_RNR_NAK_TIMER_FLG,\r\nGET_FIELD(modify_flags,\r\nQED_ROCE_MODIFY_QP_VALID_MIN_RNR_NAK_TIMER));\r\np_ramrod->fields = 0;\r\nSET_FIELD(p_ramrod->fields,\r\nROCE_MODIFY_QP_RESP_RAMROD_DATA_MIN_RNR_NAK_TIMER,\r\nqp->min_rnr_nak_timer);\r\np_ramrod->max_ird = qp->max_rd_atomic_resp;\r\np_ramrod->traffic_class = qp->traffic_class_tos;\r\np_ramrod->hop_limit = qp->hop_limit_ttl;\r\np_ramrod->p_key = cpu_to_le16(qp->pkey);\r\np_ramrod->flow_label = cpu_to_le32(qp->flow_label);\r\np_ramrod->mtu = cpu_to_le16(qp->mtu);\r\nqed_rdma_copy_gids(qp, p_ramrod->src_gid, p_ramrod->dst_gid);\r\nrc = qed_spq_post(p_hwfn, p_ent, NULL);\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "Modify responder, rc = %d\n", rc);\r\nreturn rc;\r\n}\r\nstatic int qed_roce_sp_modify_requester(struct qed_hwfn *p_hwfn,\r\nstruct qed_rdma_qp *qp,\r\nbool move_to_sqd,\r\nbool move_to_err, u32 modify_flags)\r\n{\r\nstruct roce_modify_qp_req_ramrod_data *p_ramrod;\r\nstruct qed_sp_init_data init_data;\r\nstruct qed_spq_entry *p_ent;\r\nint rc;\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "icid = %08x\n", qp->icid);\r\nif (move_to_err && !(qp->req_offloaded))\r\nreturn 0;\r\nmemset(&init_data, 0, sizeof(init_data));\r\ninit_data.cid = qp->icid + 1;\r\ninit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\r\ninit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\r\nrc = qed_sp_init_request(p_hwfn, &p_ent,\r\nROCE_EVENT_MODIFY_QP,\r\nPROTOCOLID_ROCE, &init_data);\r\nif (rc) {\r\nDP_NOTICE(p_hwfn, "rc = %d\n", rc);\r\nreturn rc;\r\n}\r\np_ramrod = &p_ent->ramrod.roce_modify_qp_req;\r\np_ramrod->flags = 0;\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_MOVE_TO_ERR_FLG, move_to_err);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_MOVE_TO_SQD_FLG, move_to_sqd);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_EN_SQD_ASYNC_NOTIFY,\r\nqp->sqd_async);\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_P_KEY_FLG,\r\nGET_FIELD(modify_flags, QED_ROCE_MODIFY_QP_VALID_PKEY));\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_ADDRESS_VECTOR_FLG,\r\nGET_FIELD(modify_flags,\r\nQED_ROCE_MODIFY_QP_VALID_ADDRESS_VECTOR));\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_MAX_ORD_FLG,\r\nGET_FIELD(modify_flags,\r\nQED_RDMA_MODIFY_QP_VALID_MAX_RD_ATOMIC_REQ));\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_RNR_NAK_CNT_FLG,\r\nGET_FIELD(modify_flags,\r\nQED_ROCE_MODIFY_QP_VALID_RNR_RETRY_CNT));\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_ERR_RETRY_CNT_FLG,\r\nGET_FIELD(modify_flags, QED_ROCE_MODIFY_QP_VALID_RETRY_CNT));\r\nSET_FIELD(p_ramrod->flags,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_ACK_TIMEOUT_FLG,\r\nGET_FIELD(modify_flags,\r\nQED_ROCE_MODIFY_QP_VALID_ACK_TIMEOUT));\r\np_ramrod->fields = 0;\r\nSET_FIELD(p_ramrod->fields,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_ERR_RETRY_CNT, qp->retry_cnt);\r\nSET_FIELD(p_ramrod->fields,\r\nROCE_MODIFY_QP_REQ_RAMROD_DATA_RNR_NAK_CNT,\r\nqp->rnr_retry_cnt);\r\np_ramrod->max_ord = qp->max_rd_atomic_req;\r\np_ramrod->traffic_class = qp->traffic_class_tos;\r\np_ramrod->hop_limit = qp->hop_limit_ttl;\r\np_ramrod->p_key = cpu_to_le16(qp->pkey);\r\np_ramrod->flow_label = cpu_to_le32(qp->flow_label);\r\np_ramrod->ack_timeout_val = cpu_to_le32(qp->ack_timeout);\r\np_ramrod->mtu = cpu_to_le16(qp->mtu);\r\nqed_rdma_copy_gids(qp, p_ramrod->src_gid, p_ramrod->dst_gid);\r\nrc = qed_spq_post(p_hwfn, p_ent, NULL);\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "Modify requester, rc = %d\n", rc);\r\nreturn rc;\r\n}\r\nstatic int qed_roce_sp_destroy_qp_responder(struct qed_hwfn *p_hwfn,\r\nstruct qed_rdma_qp *qp,\r\nu32 *num_invalidated_mw,\r\nu32 *cq_prod)\r\n{\r\nstruct roce_destroy_qp_resp_output_params *p_ramrod_res;\r\nstruct roce_destroy_qp_resp_ramrod_data *p_ramrod;\r\nstruct qed_sp_init_data init_data;\r\nstruct qed_spq_entry *p_ent;\r\ndma_addr_t ramrod_res_phys;\r\nint rc;\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "icid = %08x\n", qp->icid);\r\n*num_invalidated_mw = 0;\r\n*cq_prod = qp->cq_prod;\r\nif (!qp->resp_offloaded) {\r\nu32 cid;\r\ncid = qp->icid -\r\nqed_cxt_get_proto_cid_start(p_hwfn,\r\np_hwfn->p_rdma_info->proto);\r\nqed_roce_free_cid_pair(p_hwfn, (u16)cid);\r\nreturn 0;\r\n}\r\nmemset(&init_data, 0, sizeof(init_data));\r\ninit_data.cid = qp->icid;\r\ninit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\r\ninit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\r\nrc = qed_sp_init_request(p_hwfn, &p_ent,\r\nROCE_RAMROD_DESTROY_QP,\r\nPROTOCOLID_ROCE, &init_data);\r\nif (rc)\r\nreturn rc;\r\np_ramrod = &p_ent->ramrod.roce_destroy_qp_resp;\r\np_ramrod_res = (struct roce_destroy_qp_resp_output_params *)\r\ndma_alloc_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_ramrod_res),\r\n&ramrod_res_phys, GFP_KERNEL);\r\nif (!p_ramrod_res) {\r\nrc = -ENOMEM;\r\nDP_NOTICE(p_hwfn,\r\n"qed destroy responder failed: cannot allocate memory (ramrod). rc = %d\n",\r\nrc);\r\nreturn rc;\r\n}\r\nDMA_REGPAIR_LE(p_ramrod->output_params_addr, ramrod_res_phys);\r\nrc = qed_spq_post(p_hwfn, p_ent, NULL);\r\nif (rc)\r\ngoto err;\r\n*num_invalidated_mw = le32_to_cpu(p_ramrod_res->num_invalidated_mw);\r\n*cq_prod = le32_to_cpu(p_ramrod_res->cq_prod);\r\nqp->cq_prod = *cq_prod;\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev,\r\nqp->irq_num_pages * RDMA_RING_PAGE_SIZE,\r\nqp->irq, qp->irq_phys_addr);\r\nqp->resp_offloaded = false;\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "Destroy responder, rc = %d\n", rc);\r\nerr:\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev,\r\nsizeof(struct roce_destroy_qp_resp_output_params),\r\np_ramrod_res, ramrod_res_phys);\r\nreturn rc;\r\n}\r\nstatic int qed_roce_sp_destroy_qp_requester(struct qed_hwfn *p_hwfn,\r\nstruct qed_rdma_qp *qp,\r\nu32 *num_bound_mw)\r\n{\r\nstruct roce_destroy_qp_req_output_params *p_ramrod_res;\r\nstruct roce_destroy_qp_req_ramrod_data *p_ramrod;\r\nstruct qed_sp_init_data init_data;\r\nstruct qed_spq_entry *p_ent;\r\ndma_addr_t ramrod_res_phys;\r\nint rc = -ENOMEM;\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "icid = %08x\n", qp->icid);\r\nif (!qp->req_offloaded)\r\nreturn 0;\r\np_ramrod_res = (struct roce_destroy_qp_req_output_params *)\r\ndma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\r\nsizeof(*p_ramrod_res),\r\n&ramrod_res_phys, GFP_KERNEL);\r\nif (!p_ramrod_res) {\r\nDP_NOTICE(p_hwfn,\r\n"qed destroy requester failed: cannot allocate memory (ramrod)\n");\r\nreturn rc;\r\n}\r\nmemset(&init_data, 0, sizeof(init_data));\r\ninit_data.cid = qp->icid + 1;\r\ninit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\r\ninit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\r\nrc = qed_sp_init_request(p_hwfn, &p_ent, ROCE_RAMROD_DESTROY_QP,\r\nPROTOCOLID_ROCE, &init_data);\r\nif (rc)\r\ngoto err;\r\np_ramrod = &p_ent->ramrod.roce_destroy_qp_req;\r\nDMA_REGPAIR_LE(p_ramrod->output_params_addr, ramrod_res_phys);\r\nrc = qed_spq_post(p_hwfn, p_ent, NULL);\r\nif (rc)\r\ngoto err;\r\n*num_bound_mw = le32_to_cpu(p_ramrod_res->num_bound_mw);\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev,\r\nqp->orq_num_pages * RDMA_RING_PAGE_SIZE,\r\nqp->orq, qp->orq_phys_addr);\r\nqp->req_offloaded = false;\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "Destroy requester, rc = %d\n", rc);\r\nerr:\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_ramrod_res),\r\np_ramrod_res, ramrod_res_phys);\r\nreturn rc;\r\n}\r\nint qed_roce_query_qp(struct qed_hwfn *p_hwfn,\r\nstruct qed_rdma_qp *qp,\r\nstruct qed_rdma_query_qp_out_params *out_params)\r\n{\r\nstruct roce_query_qp_resp_output_params *p_resp_ramrod_res;\r\nstruct roce_query_qp_req_output_params *p_req_ramrod_res;\r\nstruct roce_query_qp_resp_ramrod_data *p_resp_ramrod;\r\nstruct roce_query_qp_req_ramrod_data *p_req_ramrod;\r\nstruct qed_sp_init_data init_data;\r\ndma_addr_t resp_ramrod_res_phys;\r\ndma_addr_t req_ramrod_res_phys;\r\nstruct qed_spq_entry *p_ent;\r\nbool rq_err_state;\r\nbool sq_err_state;\r\nbool sq_draining;\r\nint rc = -ENOMEM;\r\nif ((!(qp->resp_offloaded)) && (!(qp->req_offloaded))) {\r\nout_params->draining = false;\r\nout_params->rq_psn = qp->rq_psn;\r\nout_params->sq_psn = qp->sq_psn;\r\nout_params->state = qp->cur_state;\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "No QPs as no offload\n");\r\nreturn 0;\r\n}\r\nif (!(qp->resp_offloaded)) {\r\nDP_NOTICE(p_hwfn,\r\n"The responder's qp should be offloded before requester's\n");\r\nreturn -EINVAL;\r\n}\r\np_resp_ramrod_res = (struct roce_query_qp_resp_output_params *)\r\ndma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\r\nsizeof(*p_resp_ramrod_res),\r\n&resp_ramrod_res_phys, GFP_KERNEL);\r\nif (!p_resp_ramrod_res) {\r\nDP_NOTICE(p_hwfn,\r\n"qed query qp failed: cannot allocate memory (ramrod)\n");\r\nreturn rc;\r\n}\r\nmemset(&init_data, 0, sizeof(init_data));\r\ninit_data.cid = qp->icid;\r\ninit_data.opaque_fid = p_hwfn->hw_info.opaque_fid;\r\ninit_data.comp_mode = QED_SPQ_MODE_EBLOCK;\r\nrc = qed_sp_init_request(p_hwfn, &p_ent, ROCE_RAMROD_QUERY_QP,\r\nPROTOCOLID_ROCE, &init_data);\r\nif (rc)\r\ngoto err_resp;\r\np_resp_ramrod = &p_ent->ramrod.roce_query_qp_resp;\r\nDMA_REGPAIR_LE(p_resp_ramrod->output_params_addr, resp_ramrod_res_phys);\r\nrc = qed_spq_post(p_hwfn, p_ent, NULL);\r\nif (rc)\r\ngoto err_resp;\r\nout_params->rq_psn = le32_to_cpu(p_resp_ramrod_res->psn);\r\nrq_err_state = GET_FIELD(le32_to_cpu(p_resp_ramrod_res->err_flag),\r\nROCE_QUERY_QP_RESP_OUTPUT_PARAMS_ERROR_FLG);\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_resp_ramrod_res),\r\np_resp_ramrod_res, resp_ramrod_res_phys);\r\nif (!(qp->req_offloaded)) {\r\nout_params->sq_psn = qp->sq_psn;\r\nout_params->draining = false;\r\nif (rq_err_state)\r\nqp->cur_state = QED_ROCE_QP_STATE_ERR;\r\nout_params->state = qp->cur_state;\r\nreturn 0;\r\n}\r\np_req_ramrod_res = (struct roce_query_qp_req_output_params *)\r\ndma_alloc_coherent(&p_hwfn->cdev->pdev->dev,\r\nsizeof(*p_req_ramrod_res),\r\n&req_ramrod_res_phys,\r\nGFP_KERNEL);\r\nif (!p_req_ramrod_res) {\r\nrc = -ENOMEM;\r\nDP_NOTICE(p_hwfn,\r\n"qed query qp failed: cannot allocate memory (ramrod)\n");\r\nreturn rc;\r\n}\r\ninit_data.cid = qp->icid + 1;\r\nrc = qed_sp_init_request(p_hwfn, &p_ent, ROCE_RAMROD_QUERY_QP,\r\nPROTOCOLID_ROCE, &init_data);\r\nif (rc)\r\ngoto err_req;\r\np_req_ramrod = &p_ent->ramrod.roce_query_qp_req;\r\nDMA_REGPAIR_LE(p_req_ramrod->output_params_addr, req_ramrod_res_phys);\r\nrc = qed_spq_post(p_hwfn, p_ent, NULL);\r\nif (rc)\r\ngoto err_req;\r\nout_params->sq_psn = le32_to_cpu(p_req_ramrod_res->psn);\r\nsq_err_state = GET_FIELD(le32_to_cpu(p_req_ramrod_res->flags),\r\nROCE_QUERY_QP_REQ_OUTPUT_PARAMS_ERR_FLG);\r\nsq_draining =\r\nGET_FIELD(le32_to_cpu(p_req_ramrod_res->flags),\r\nROCE_QUERY_QP_REQ_OUTPUT_PARAMS_SQ_DRAINING_FLG);\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_req_ramrod_res),\r\np_req_ramrod_res, req_ramrod_res_phys);\r\nout_params->draining = false;\r\nif (rq_err_state || sq_err_state)\r\nqp->cur_state = QED_ROCE_QP_STATE_ERR;\r\nelse if (sq_draining)\r\nout_params->draining = true;\r\nout_params->state = qp->cur_state;\r\nreturn 0;\r\nerr_req:\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_req_ramrod_res),\r\np_req_ramrod_res, req_ramrod_res_phys);\r\nreturn rc;\r\nerr_resp:\r\ndma_free_coherent(&p_hwfn->cdev->pdev->dev, sizeof(*p_resp_ramrod_res),\r\np_resp_ramrod_res, resp_ramrod_res_phys);\r\nreturn rc;\r\n}\r\nint qed_roce_destroy_qp(struct qed_hwfn *p_hwfn, struct qed_rdma_qp *qp)\r\n{\r\nu32 num_invalidated_mw = 0;\r\nu32 num_bound_mw = 0;\r\nu32 cq_prod;\r\nint rc;\r\nif ((qp->cur_state != QED_ROCE_QP_STATE_RESET) &&\r\n(qp->cur_state != QED_ROCE_QP_STATE_ERR) &&\r\n(qp->cur_state != QED_ROCE_QP_STATE_INIT)) {\r\nDP_NOTICE(p_hwfn,\r\n"QP must be in error, reset or init state before destroying it\n");\r\nreturn -EINVAL;\r\n}\r\nif (qp->cur_state != QED_ROCE_QP_STATE_RESET) {\r\nrc = qed_roce_sp_destroy_qp_responder(p_hwfn, qp,\r\n&num_invalidated_mw,\r\n&cq_prod);\r\nif (rc)\r\nreturn rc;\r\nrc = qed_roce_sp_destroy_qp_requester(p_hwfn, qp,\r\n&num_bound_mw);\r\nif (rc)\r\nreturn rc;\r\nif (num_invalidated_mw != num_bound_mw) {\r\nDP_NOTICE(p_hwfn,\r\n"number of invalidate memory windows is different from bounded ones\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint qed_roce_modify_qp(struct qed_hwfn *p_hwfn,\r\nstruct qed_rdma_qp *qp,\r\nenum qed_roce_qp_state prev_state,\r\nstruct qed_rdma_modify_qp_in_params *params)\r\n{\r\nu32 num_invalidated_mw = 0, num_bound_mw = 0;\r\nint rc = 0;\r\nif (((prev_state == QED_ROCE_QP_STATE_INIT) ||\r\n(prev_state == QED_ROCE_QP_STATE_RESET)) &&\r\n(qp->cur_state == QED_ROCE_QP_STATE_RTR)) {\r\nrc = qed_roce_sp_create_responder(p_hwfn, qp);\r\nreturn rc;\r\n} else if ((prev_state == QED_ROCE_QP_STATE_RTR) &&\r\n(qp->cur_state == QED_ROCE_QP_STATE_RTS)) {\r\nrc = qed_roce_sp_create_requester(p_hwfn, qp);\r\nif (rc)\r\nreturn rc;\r\nrc = qed_roce_sp_modify_responder(p_hwfn, qp, false,\r\nparams->modify_flags);\r\nreturn rc;\r\n} else if ((prev_state == QED_ROCE_QP_STATE_RTS) &&\r\n(qp->cur_state == QED_ROCE_QP_STATE_RTS)) {\r\nrc = qed_roce_sp_modify_responder(p_hwfn, qp, false,\r\nparams->modify_flags);\r\nif (rc)\r\nreturn rc;\r\nrc = qed_roce_sp_modify_requester(p_hwfn, qp, false, false,\r\nparams->modify_flags);\r\nreturn rc;\r\n} else if ((prev_state == QED_ROCE_QP_STATE_RTS) &&\r\n(qp->cur_state == QED_ROCE_QP_STATE_SQD)) {\r\nrc = qed_roce_sp_modify_requester(p_hwfn, qp, true, false,\r\nparams->modify_flags);\r\nreturn rc;\r\n} else if ((prev_state == QED_ROCE_QP_STATE_SQD) &&\r\n(qp->cur_state == QED_ROCE_QP_STATE_SQD)) {\r\nrc = qed_roce_sp_modify_responder(p_hwfn, qp, false,\r\nparams->modify_flags);\r\nif (rc)\r\nreturn rc;\r\nrc = qed_roce_sp_modify_requester(p_hwfn, qp, false, false,\r\nparams->modify_flags);\r\nreturn rc;\r\n} else if ((prev_state == QED_ROCE_QP_STATE_SQD) &&\r\n(qp->cur_state == QED_ROCE_QP_STATE_RTS)) {\r\nrc = qed_roce_sp_modify_responder(p_hwfn, qp, false,\r\nparams->modify_flags);\r\nif (rc)\r\nreturn rc;\r\nrc = qed_roce_sp_modify_requester(p_hwfn, qp, false, false,\r\nparams->modify_flags);\r\nreturn rc;\r\n} else if (qp->cur_state == QED_ROCE_QP_STATE_ERR) {\r\nrc = qed_roce_sp_modify_responder(p_hwfn, qp, true,\r\nparams->modify_flags);\r\nif (rc)\r\nreturn rc;\r\nrc = qed_roce_sp_modify_requester(p_hwfn, qp, false, true,\r\nparams->modify_flags);\r\nreturn rc;\r\n} else if (qp->cur_state == QED_ROCE_QP_STATE_RESET) {\r\nu32 cq_prod;\r\nrc = qed_roce_sp_destroy_qp_responder(p_hwfn,\r\nqp,\r\n&num_invalidated_mw,\r\n&cq_prod);\r\nif (rc)\r\nreturn rc;\r\nqp->cq_prod = cq_prod;\r\nrc = qed_roce_sp_destroy_qp_requester(p_hwfn, qp,\r\n&num_bound_mw);\r\nif (num_invalidated_mw != num_bound_mw) {\r\nDP_NOTICE(p_hwfn,\r\n"number of invalidate memory windows is different from bounded ones\n");\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "0\n");\r\n}\r\nreturn rc;\r\n}\r\nstatic void qed_roce_free_real_icid(struct qed_hwfn *p_hwfn, u16 icid)\r\n{\r\nstruct qed_rdma_info *p_rdma_info = p_hwfn->p_rdma_info;\r\nu32 start_cid, cid, xcid;\r\nstart_cid = qed_cxt_get_proto_cid_start(p_hwfn, p_rdma_info->proto);\r\ncid = icid - start_cid;\r\nxcid = cid ^ 1;\r\nspin_lock_bh(&p_rdma_info->lock);\r\nqed_bmap_release_id(p_hwfn, &p_rdma_info->real_cid_map, cid);\r\nif (qed_bmap_test_id(p_hwfn, &p_rdma_info->real_cid_map, xcid) == 0) {\r\nqed_bmap_release_id(p_hwfn, &p_rdma_info->cid_map, cid);\r\nqed_bmap_release_id(p_hwfn, &p_rdma_info->cid_map, xcid);\r\n}\r\nspin_unlock_bh(&p_hwfn->p_rdma_info->lock);\r\n}\r\nvoid qed_roce_dpm_dcbx(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)\r\n{\r\nu8 val;\r\nval = qed_rdma_allocated_qps(p_hwfn) ? true : false;\r\np_hwfn->dcbx_no_edpm = (u8)val;\r\nqed_rdma_dpm_conf(p_hwfn, p_ptt);\r\n}\r\nint qed_roce_setup(struct qed_hwfn *p_hwfn)\r\n{\r\nreturn qed_spq_register_async_cb(p_hwfn, PROTOCOLID_ROCE,\r\nqed_roce_async_event);\r\n}\r\nint qed_roce_init_hw(struct qed_hwfn *p_hwfn, struct qed_ptt *p_ptt)\r\n{\r\nu32 ll2_ethertype_en;\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_ROCE_DEST_QP_MAX_PF, 0);\r\np_hwfn->rdma_prs_search_reg = PRS_REG_SEARCH_ROCE;\r\nll2_ethertype_en = qed_rd(p_hwfn, p_ptt, PRS_REG_LIGHT_L2_ETHERTYPE_EN);\r\nqed_wr(p_hwfn, p_ptt, PRS_REG_LIGHT_L2_ETHERTYPE_EN,\r\n(ll2_ethertype_en | 0x01));\r\nif (qed_cxt_get_proto_cid_start(p_hwfn, PROTOCOLID_ROCE) % 2) {\r\nDP_NOTICE(p_hwfn, "The first RoCE's cid should be even\n");\r\nreturn -EINVAL;\r\n}\r\nDP_VERBOSE(p_hwfn, QED_MSG_RDMA, "Initializing HW - Done\n");\r\nreturn 0;\r\n}
