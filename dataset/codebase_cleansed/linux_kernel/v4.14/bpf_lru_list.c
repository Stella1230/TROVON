static int get_next_cpu(int cpu)\r\n{\r\ncpu = cpumask_next(cpu, cpu_possible_mask);\r\nif (cpu >= nr_cpu_ids)\r\ncpu = cpumask_first(cpu_possible_mask);\r\nreturn cpu;\r\n}\r\nstatic struct list_head *local_free_list(struct bpf_lru_locallist *loc_l)\r\n{\r\nreturn &loc_l->lists[LOCAL_FREE_LIST_IDX];\r\n}\r\nstatic struct list_head *local_pending_list(struct bpf_lru_locallist *loc_l)\r\n{\r\nreturn &loc_l->lists[LOCAL_PENDING_LIST_IDX];\r\n}\r\nstatic bool bpf_lru_node_is_ref(const struct bpf_lru_node *node)\r\n{\r\nreturn node->ref;\r\n}\r\nstatic void bpf_lru_list_count_inc(struct bpf_lru_list *l,\r\nenum bpf_lru_list_type type)\r\n{\r\nif (type < NR_BPF_LRU_LIST_COUNT)\r\nl->counts[type]++;\r\n}\r\nstatic void bpf_lru_list_count_dec(struct bpf_lru_list *l,\r\nenum bpf_lru_list_type type)\r\n{\r\nif (type < NR_BPF_LRU_LIST_COUNT)\r\nl->counts[type]--;\r\n}\r\nstatic void __bpf_lru_node_move_to_free(struct bpf_lru_list *l,\r\nstruct bpf_lru_node *node,\r\nstruct list_head *free_list,\r\nenum bpf_lru_list_type tgt_free_type)\r\n{\r\nif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))\r\nreturn;\r\nif (&node->list == l->next_inactive_rotation)\r\nl->next_inactive_rotation = l->next_inactive_rotation->prev;\r\nbpf_lru_list_count_dec(l, node->type);\r\nnode->type = tgt_free_type;\r\nlist_move(&node->list, free_list);\r\n}\r\nstatic void __bpf_lru_node_move_in(struct bpf_lru_list *l,\r\nstruct bpf_lru_node *node,\r\nenum bpf_lru_list_type tgt_type)\r\n{\r\nif (WARN_ON_ONCE(!IS_LOCAL_LIST_TYPE(node->type)) ||\r\nWARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))\r\nreturn;\r\nbpf_lru_list_count_inc(l, tgt_type);\r\nnode->type = tgt_type;\r\nnode->ref = 0;\r\nlist_move(&node->list, &l->lists[tgt_type]);\r\n}\r\nstatic void __bpf_lru_node_move(struct bpf_lru_list *l,\r\nstruct bpf_lru_node *node,\r\nenum bpf_lru_list_type tgt_type)\r\n{\r\nif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)) ||\r\nWARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))\r\nreturn;\r\nif (node->type != tgt_type) {\r\nbpf_lru_list_count_dec(l, node->type);\r\nbpf_lru_list_count_inc(l, tgt_type);\r\nnode->type = tgt_type;\r\n}\r\nnode->ref = 0;\r\nif (&node->list == l->next_inactive_rotation)\r\nl->next_inactive_rotation = l->next_inactive_rotation->prev;\r\nlist_move(&node->list, &l->lists[tgt_type]);\r\n}\r\nstatic bool bpf_lru_list_inactive_low(const struct bpf_lru_list *l)\r\n{\r\nreturn l->counts[BPF_LRU_LIST_T_INACTIVE] <\r\nl->counts[BPF_LRU_LIST_T_ACTIVE];\r\n}\r\nstatic void __bpf_lru_list_rotate_active(struct bpf_lru *lru,\r\nstruct bpf_lru_list *l)\r\n{\r\nstruct list_head *active = &l->lists[BPF_LRU_LIST_T_ACTIVE];\r\nstruct bpf_lru_node *node, *tmp_node, *first_node;\r\nunsigned int i = 0;\r\nfirst_node = list_first_entry(active, struct bpf_lru_node, list);\r\nlist_for_each_entry_safe_reverse(node, tmp_node, active, list) {\r\nif (bpf_lru_node_is_ref(node))\r\n__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);\r\nelse\r\n__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_INACTIVE);\r\nif (++i == lru->nr_scans || node == first_node)\r\nbreak;\r\n}\r\n}\r\nstatic void __bpf_lru_list_rotate_inactive(struct bpf_lru *lru,\r\nstruct bpf_lru_list *l)\r\n{\r\nstruct list_head *inactive = &l->lists[BPF_LRU_LIST_T_INACTIVE];\r\nstruct list_head *cur, *last, *next = inactive;\r\nstruct bpf_lru_node *node;\r\nunsigned int i = 0;\r\nif (list_empty(inactive))\r\nreturn;\r\nlast = l->next_inactive_rotation->next;\r\nif (last == inactive)\r\nlast = last->next;\r\ncur = l->next_inactive_rotation;\r\nwhile (i < lru->nr_scans) {\r\nif (cur == inactive) {\r\ncur = cur->prev;\r\ncontinue;\r\n}\r\nnode = list_entry(cur, struct bpf_lru_node, list);\r\nnext = cur->prev;\r\nif (bpf_lru_node_is_ref(node))\r\n__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);\r\nif (cur == last)\r\nbreak;\r\ncur = next;\r\ni++;\r\n}\r\nl->next_inactive_rotation = next;\r\n}\r\nstatic unsigned int\r\n__bpf_lru_list_shrink_inactive(struct bpf_lru *lru,\r\nstruct bpf_lru_list *l,\r\nunsigned int tgt_nshrink,\r\nstruct list_head *free_list,\r\nenum bpf_lru_list_type tgt_free_type)\r\n{\r\nstruct list_head *inactive = &l->lists[BPF_LRU_LIST_T_INACTIVE];\r\nstruct bpf_lru_node *node, *tmp_node;\r\nunsigned int nshrinked = 0;\r\nunsigned int i = 0;\r\nlist_for_each_entry_safe_reverse(node, tmp_node, inactive, list) {\r\nif (bpf_lru_node_is_ref(node)) {\r\n__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);\r\n} else if (lru->del_from_htab(lru->del_arg, node)) {\r\n__bpf_lru_node_move_to_free(l, node, free_list,\r\ntgt_free_type);\r\nif (++nshrinked == tgt_nshrink)\r\nbreak;\r\n}\r\nif (++i == lru->nr_scans)\r\nbreak;\r\n}\r\nreturn nshrinked;\r\n}\r\nstatic void __bpf_lru_list_rotate(struct bpf_lru *lru, struct bpf_lru_list *l)\r\n{\r\nif (bpf_lru_list_inactive_low(l))\r\n__bpf_lru_list_rotate_active(lru, l);\r\n__bpf_lru_list_rotate_inactive(lru, l);\r\n}\r\nstatic unsigned int __bpf_lru_list_shrink(struct bpf_lru *lru,\r\nstruct bpf_lru_list *l,\r\nunsigned int tgt_nshrink,\r\nstruct list_head *free_list,\r\nenum bpf_lru_list_type tgt_free_type)\r\n{\r\nstruct bpf_lru_node *node, *tmp_node;\r\nstruct list_head *force_shrink_list;\r\nunsigned int nshrinked;\r\nnshrinked = __bpf_lru_list_shrink_inactive(lru, l, tgt_nshrink,\r\nfree_list, tgt_free_type);\r\nif (nshrinked)\r\nreturn nshrinked;\r\nif (!list_empty(&l->lists[BPF_LRU_LIST_T_INACTIVE]))\r\nforce_shrink_list = &l->lists[BPF_LRU_LIST_T_INACTIVE];\r\nelse\r\nforce_shrink_list = &l->lists[BPF_LRU_LIST_T_ACTIVE];\r\nlist_for_each_entry_safe_reverse(node, tmp_node, force_shrink_list,\r\nlist) {\r\nif (lru->del_from_htab(lru->del_arg, node)) {\r\n__bpf_lru_node_move_to_free(l, node, free_list,\r\ntgt_free_type);\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void __local_list_flush(struct bpf_lru_list *l,\r\nstruct bpf_lru_locallist *loc_l)\r\n{\r\nstruct bpf_lru_node *node, *tmp_node;\r\nlist_for_each_entry_safe_reverse(node, tmp_node,\r\nlocal_pending_list(loc_l), list) {\r\nif (bpf_lru_node_is_ref(node))\r\n__bpf_lru_node_move_in(l, node, BPF_LRU_LIST_T_ACTIVE);\r\nelse\r\n__bpf_lru_node_move_in(l, node,\r\nBPF_LRU_LIST_T_INACTIVE);\r\n}\r\n}\r\nstatic void bpf_lru_list_push_free(struct bpf_lru_list *l,\r\nstruct bpf_lru_node *node)\r\n{\r\nunsigned long flags;\r\nif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))\r\nreturn;\r\nraw_spin_lock_irqsave(&l->lock, flags);\r\n__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);\r\nraw_spin_unlock_irqrestore(&l->lock, flags);\r\n}\r\nstatic void bpf_lru_list_pop_free_to_local(struct bpf_lru *lru,\r\nstruct bpf_lru_locallist *loc_l)\r\n{\r\nstruct bpf_lru_list *l = &lru->common_lru.lru_list;\r\nstruct bpf_lru_node *node, *tmp_node;\r\nunsigned int nfree = 0;\r\nraw_spin_lock(&l->lock);\r\n__local_list_flush(l, loc_l);\r\n__bpf_lru_list_rotate(lru, l);\r\nlist_for_each_entry_safe(node, tmp_node, &l->lists[BPF_LRU_LIST_T_FREE],\r\nlist) {\r\n__bpf_lru_node_move_to_free(l, node, local_free_list(loc_l),\r\nBPF_LRU_LOCAL_LIST_T_FREE);\r\nif (++nfree == LOCAL_FREE_TARGET)\r\nbreak;\r\n}\r\nif (nfree < LOCAL_FREE_TARGET)\r\n__bpf_lru_list_shrink(lru, l, LOCAL_FREE_TARGET - nfree,\r\nlocal_free_list(loc_l),\r\nBPF_LRU_LOCAL_LIST_T_FREE);\r\nraw_spin_unlock(&l->lock);\r\n}\r\nstatic void __local_list_add_pending(struct bpf_lru *lru,\r\nstruct bpf_lru_locallist *loc_l,\r\nint cpu,\r\nstruct bpf_lru_node *node,\r\nu32 hash)\r\n{\r\n*(u32 *)((void *)node + lru->hash_offset) = hash;\r\nnode->cpu = cpu;\r\nnode->type = BPF_LRU_LOCAL_LIST_T_PENDING;\r\nnode->ref = 0;\r\nlist_add(&node->list, local_pending_list(loc_l));\r\n}\r\nstatic struct bpf_lru_node *\r\n__local_list_pop_free(struct bpf_lru_locallist *loc_l)\r\n{\r\nstruct bpf_lru_node *node;\r\nnode = list_first_entry_or_null(local_free_list(loc_l),\r\nstruct bpf_lru_node,\r\nlist);\r\nif (node)\r\nlist_del(&node->list);\r\nreturn node;\r\n}\r\nstatic struct bpf_lru_node *\r\n__local_list_pop_pending(struct bpf_lru *lru, struct bpf_lru_locallist *loc_l)\r\n{\r\nstruct bpf_lru_node *node;\r\nbool force = false;\r\nignore_ref:\r\nlist_for_each_entry_reverse(node, local_pending_list(loc_l),\r\nlist) {\r\nif ((!bpf_lru_node_is_ref(node) || force) &&\r\nlru->del_from_htab(lru->del_arg, node)) {\r\nlist_del(&node->list);\r\nreturn node;\r\n}\r\n}\r\nif (!force) {\r\nforce = true;\r\ngoto ignore_ref;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct bpf_lru_node *bpf_percpu_lru_pop_free(struct bpf_lru *lru,\r\nu32 hash)\r\n{\r\nstruct list_head *free_list;\r\nstruct bpf_lru_node *node = NULL;\r\nstruct bpf_lru_list *l;\r\nunsigned long flags;\r\nint cpu = raw_smp_processor_id();\r\nl = per_cpu_ptr(lru->percpu_lru, cpu);\r\nraw_spin_lock_irqsave(&l->lock, flags);\r\n__bpf_lru_list_rotate(lru, l);\r\nfree_list = &l->lists[BPF_LRU_LIST_T_FREE];\r\nif (list_empty(free_list))\r\n__bpf_lru_list_shrink(lru, l, PERCPU_FREE_TARGET, free_list,\r\nBPF_LRU_LIST_T_FREE);\r\nif (!list_empty(free_list)) {\r\nnode = list_first_entry(free_list, struct bpf_lru_node, list);\r\n*(u32 *)((void *)node + lru->hash_offset) = hash;\r\nnode->ref = 0;\r\n__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_INACTIVE);\r\n}\r\nraw_spin_unlock_irqrestore(&l->lock, flags);\r\nreturn node;\r\n}\r\nstatic struct bpf_lru_node *bpf_common_lru_pop_free(struct bpf_lru *lru,\r\nu32 hash)\r\n{\r\nstruct bpf_lru_locallist *loc_l, *steal_loc_l;\r\nstruct bpf_common_lru *clru = &lru->common_lru;\r\nstruct bpf_lru_node *node;\r\nint steal, first_steal;\r\nunsigned long flags;\r\nint cpu = raw_smp_processor_id();\r\nloc_l = per_cpu_ptr(clru->local_list, cpu);\r\nraw_spin_lock_irqsave(&loc_l->lock, flags);\r\nnode = __local_list_pop_free(loc_l);\r\nif (!node) {\r\nbpf_lru_list_pop_free_to_local(lru, loc_l);\r\nnode = __local_list_pop_free(loc_l);\r\n}\r\nif (node)\r\n__local_list_add_pending(lru, loc_l, cpu, node, hash);\r\nraw_spin_unlock_irqrestore(&loc_l->lock, flags);\r\nif (node)\r\nreturn node;\r\nfirst_steal = loc_l->next_steal;\r\nsteal = first_steal;\r\ndo {\r\nsteal_loc_l = per_cpu_ptr(clru->local_list, steal);\r\nraw_spin_lock_irqsave(&steal_loc_l->lock, flags);\r\nnode = __local_list_pop_free(steal_loc_l);\r\nif (!node)\r\nnode = __local_list_pop_pending(lru, steal_loc_l);\r\nraw_spin_unlock_irqrestore(&steal_loc_l->lock, flags);\r\nsteal = get_next_cpu(steal);\r\n} while (!node && steal != first_steal);\r\nloc_l->next_steal = steal;\r\nif (node) {\r\nraw_spin_lock_irqsave(&loc_l->lock, flags);\r\n__local_list_add_pending(lru, loc_l, cpu, node, hash);\r\nraw_spin_unlock_irqrestore(&loc_l->lock, flags);\r\n}\r\nreturn node;\r\n}\r\nstruct bpf_lru_node *bpf_lru_pop_free(struct bpf_lru *lru, u32 hash)\r\n{\r\nif (lru->percpu)\r\nreturn bpf_percpu_lru_pop_free(lru, hash);\r\nelse\r\nreturn bpf_common_lru_pop_free(lru, hash);\r\n}\r\nstatic void bpf_common_lru_push_free(struct bpf_lru *lru,\r\nstruct bpf_lru_node *node)\r\n{\r\nunsigned long flags;\r\nif (WARN_ON_ONCE(node->type == BPF_LRU_LIST_T_FREE) ||\r\nWARN_ON_ONCE(node->type == BPF_LRU_LOCAL_LIST_T_FREE))\r\nreturn;\r\nif (node->type == BPF_LRU_LOCAL_LIST_T_PENDING) {\r\nstruct bpf_lru_locallist *loc_l;\r\nloc_l = per_cpu_ptr(lru->common_lru.local_list, node->cpu);\r\nraw_spin_lock_irqsave(&loc_l->lock, flags);\r\nif (unlikely(node->type != BPF_LRU_LOCAL_LIST_T_PENDING)) {\r\nraw_spin_unlock_irqrestore(&loc_l->lock, flags);\r\ngoto check_lru_list;\r\n}\r\nnode->type = BPF_LRU_LOCAL_LIST_T_FREE;\r\nnode->ref = 0;\r\nlist_move(&node->list, local_free_list(loc_l));\r\nraw_spin_unlock_irqrestore(&loc_l->lock, flags);\r\nreturn;\r\n}\r\ncheck_lru_list:\r\nbpf_lru_list_push_free(&lru->common_lru.lru_list, node);\r\n}\r\nstatic void bpf_percpu_lru_push_free(struct bpf_lru *lru,\r\nstruct bpf_lru_node *node)\r\n{\r\nstruct bpf_lru_list *l;\r\nunsigned long flags;\r\nl = per_cpu_ptr(lru->percpu_lru, node->cpu);\r\nraw_spin_lock_irqsave(&l->lock, flags);\r\n__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);\r\nraw_spin_unlock_irqrestore(&l->lock, flags);\r\n}\r\nvoid bpf_lru_push_free(struct bpf_lru *lru, struct bpf_lru_node *node)\r\n{\r\nif (lru->percpu)\r\nbpf_percpu_lru_push_free(lru, node);\r\nelse\r\nbpf_common_lru_push_free(lru, node);\r\n}\r\nstatic void bpf_common_lru_populate(struct bpf_lru *lru, void *buf,\r\nu32 node_offset, u32 elem_size,\r\nu32 nr_elems)\r\n{\r\nstruct bpf_lru_list *l = &lru->common_lru.lru_list;\r\nu32 i;\r\nfor (i = 0; i < nr_elems; i++) {\r\nstruct bpf_lru_node *node;\r\nnode = (struct bpf_lru_node *)(buf + node_offset);\r\nnode->type = BPF_LRU_LIST_T_FREE;\r\nnode->ref = 0;\r\nlist_add(&node->list, &l->lists[BPF_LRU_LIST_T_FREE]);\r\nbuf += elem_size;\r\n}\r\n}\r\nstatic void bpf_percpu_lru_populate(struct bpf_lru *lru, void *buf,\r\nu32 node_offset, u32 elem_size,\r\nu32 nr_elems)\r\n{\r\nu32 i, pcpu_entries;\r\nint cpu;\r\nstruct bpf_lru_list *l;\r\npcpu_entries = nr_elems / num_possible_cpus();\r\ni = 0;\r\nfor_each_possible_cpu(cpu) {\r\nstruct bpf_lru_node *node;\r\nl = per_cpu_ptr(lru->percpu_lru, cpu);\r\nagain:\r\nnode = (struct bpf_lru_node *)(buf + node_offset);\r\nnode->cpu = cpu;\r\nnode->type = BPF_LRU_LIST_T_FREE;\r\nnode->ref = 0;\r\nlist_add(&node->list, &l->lists[BPF_LRU_LIST_T_FREE]);\r\ni++;\r\nbuf += elem_size;\r\nif (i == nr_elems)\r\nbreak;\r\nif (i % pcpu_entries)\r\ngoto again;\r\n}\r\n}\r\nvoid bpf_lru_populate(struct bpf_lru *lru, void *buf, u32 node_offset,\r\nu32 elem_size, u32 nr_elems)\r\n{\r\nif (lru->percpu)\r\nbpf_percpu_lru_populate(lru, buf, node_offset, elem_size,\r\nnr_elems);\r\nelse\r\nbpf_common_lru_populate(lru, buf, node_offset, elem_size,\r\nnr_elems);\r\n}\r\nstatic void bpf_lru_locallist_init(struct bpf_lru_locallist *loc_l, int cpu)\r\n{\r\nint i;\r\nfor (i = 0; i < NR_BPF_LRU_LOCAL_LIST_T; i++)\r\nINIT_LIST_HEAD(&loc_l->lists[i]);\r\nloc_l->next_steal = cpu;\r\nraw_spin_lock_init(&loc_l->lock);\r\n}\r\nstatic void bpf_lru_list_init(struct bpf_lru_list *l)\r\n{\r\nint i;\r\nfor (i = 0; i < NR_BPF_LRU_LIST_T; i++)\r\nINIT_LIST_HEAD(&l->lists[i]);\r\nfor (i = 0; i < NR_BPF_LRU_LIST_COUNT; i++)\r\nl->counts[i] = 0;\r\nl->next_inactive_rotation = &l->lists[BPF_LRU_LIST_T_INACTIVE];\r\nraw_spin_lock_init(&l->lock);\r\n}\r\nint bpf_lru_init(struct bpf_lru *lru, bool percpu, u32 hash_offset,\r\ndel_from_htab_func del_from_htab, void *del_arg)\r\n{\r\nint cpu;\r\nif (percpu) {\r\nlru->percpu_lru = alloc_percpu(struct bpf_lru_list);\r\nif (!lru->percpu_lru)\r\nreturn -ENOMEM;\r\nfor_each_possible_cpu(cpu) {\r\nstruct bpf_lru_list *l;\r\nl = per_cpu_ptr(lru->percpu_lru, cpu);\r\nbpf_lru_list_init(l);\r\n}\r\nlru->nr_scans = PERCPU_NR_SCANS;\r\n} else {\r\nstruct bpf_common_lru *clru = &lru->common_lru;\r\nclru->local_list = alloc_percpu(struct bpf_lru_locallist);\r\nif (!clru->local_list)\r\nreturn -ENOMEM;\r\nfor_each_possible_cpu(cpu) {\r\nstruct bpf_lru_locallist *loc_l;\r\nloc_l = per_cpu_ptr(clru->local_list, cpu);\r\nbpf_lru_locallist_init(loc_l, cpu);\r\n}\r\nbpf_lru_list_init(&clru->lru_list);\r\nlru->nr_scans = LOCAL_NR_SCANS;\r\n}\r\nlru->percpu = percpu;\r\nlru->del_from_htab = del_from_htab;\r\nlru->del_arg = del_arg;\r\nlru->hash_offset = hash_offset;\r\nreturn 0;\r\n}\r\nvoid bpf_lru_destroy(struct bpf_lru *lru)\r\n{\r\nif (lru->percpu)\r\nfree_percpu(lru->percpu_lru);\r\nelse\r\nfree_percpu(lru->common_lru.local_list);\r\n}
