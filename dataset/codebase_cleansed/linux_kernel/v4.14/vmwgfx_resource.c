static inline struct vmw_dma_buffer *\r\nvmw_dma_buffer(struct ttm_buffer_object *bo)\r\n{\r\nreturn container_of(bo, struct vmw_dma_buffer, base);\r\n}\r\nstatic inline struct vmw_user_dma_buffer *\r\nvmw_user_dma_buffer(struct ttm_buffer_object *bo)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo = vmw_dma_buffer(bo);\r\nreturn container_of(vmw_bo, struct vmw_user_dma_buffer, dma);\r\n}\r\nstruct vmw_resource *vmw_resource_reference(struct vmw_resource *res)\r\n{\r\nkref_get(&res->kref);\r\nreturn res;\r\n}\r\nstruct vmw_resource *\r\nvmw_resource_reference_unless_doomed(struct vmw_resource *res)\r\n{\r\nreturn kref_get_unless_zero(&res->kref) ? res : NULL;\r\n}\r\nvoid vmw_resource_release_id(struct vmw_resource *res)\r\n{\r\nstruct vmw_private *dev_priv = res->dev_priv;\r\nstruct idr *idr = &dev_priv->res_idr[res->func->res_type];\r\nwrite_lock(&dev_priv->resource_lock);\r\nif (res->id != -1)\r\nidr_remove(idr, res->id);\r\nres->id = -1;\r\nwrite_unlock(&dev_priv->resource_lock);\r\n}\r\nstatic void vmw_resource_release(struct kref *kref)\r\n{\r\nstruct vmw_resource *res =\r\ncontainer_of(kref, struct vmw_resource, kref);\r\nstruct vmw_private *dev_priv = res->dev_priv;\r\nint id;\r\nstruct idr *idr = &dev_priv->res_idr[res->func->res_type];\r\nwrite_lock(&dev_priv->resource_lock);\r\nres->avail = false;\r\nlist_del_init(&res->lru_head);\r\nwrite_unlock(&dev_priv->resource_lock);\r\nif (res->backup) {\r\nstruct ttm_buffer_object *bo = &res->backup->base;\r\nttm_bo_reserve(bo, false, false, NULL);\r\nif (!list_empty(&res->mob_head) &&\r\nres->func->unbind != NULL) {\r\nstruct ttm_validate_buffer val_buf;\r\nval_buf.bo = bo;\r\nval_buf.shared = false;\r\nres->func->unbind(res, false, &val_buf);\r\n}\r\nres->backup_dirty = false;\r\nlist_del_init(&res->mob_head);\r\nttm_bo_unreserve(bo);\r\nvmw_dmabuf_unreference(&res->backup);\r\n}\r\nif (likely(res->hw_destroy != NULL)) {\r\nmutex_lock(&dev_priv->binding_mutex);\r\nvmw_binding_res_list_kill(&res->binding_head);\r\nmutex_unlock(&dev_priv->binding_mutex);\r\nres->hw_destroy(res);\r\n}\r\nid = res->id;\r\nif (res->res_free != NULL)\r\nres->res_free(res);\r\nelse\r\nkfree(res);\r\nwrite_lock(&dev_priv->resource_lock);\r\nif (id != -1)\r\nidr_remove(idr, id);\r\nwrite_unlock(&dev_priv->resource_lock);\r\n}\r\nvoid vmw_resource_unreference(struct vmw_resource **p_res)\r\n{\r\nstruct vmw_resource *res = *p_res;\r\n*p_res = NULL;\r\nkref_put(&res->kref, vmw_resource_release);\r\n}\r\nint vmw_resource_alloc_id(struct vmw_resource *res)\r\n{\r\nstruct vmw_private *dev_priv = res->dev_priv;\r\nint ret;\r\nstruct idr *idr = &dev_priv->res_idr[res->func->res_type];\r\nBUG_ON(res->id != -1);\r\nidr_preload(GFP_KERNEL);\r\nwrite_lock(&dev_priv->resource_lock);\r\nret = idr_alloc(idr, res, 1, 0, GFP_NOWAIT);\r\nif (ret >= 0)\r\nres->id = ret;\r\nwrite_unlock(&dev_priv->resource_lock);\r\nidr_preload_end();\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nint vmw_resource_init(struct vmw_private *dev_priv, struct vmw_resource *res,\r\nbool delay_id,\r\nvoid (*res_free) (struct vmw_resource *res),\r\nconst struct vmw_res_func *func)\r\n{\r\nkref_init(&res->kref);\r\nres->hw_destroy = NULL;\r\nres->res_free = res_free;\r\nres->avail = false;\r\nres->dev_priv = dev_priv;\r\nres->func = func;\r\nINIT_LIST_HEAD(&res->lru_head);\r\nINIT_LIST_HEAD(&res->mob_head);\r\nINIT_LIST_HEAD(&res->binding_head);\r\nres->id = -1;\r\nres->backup = NULL;\r\nres->backup_offset = 0;\r\nres->backup_dirty = false;\r\nres->res_dirty = false;\r\nif (delay_id)\r\nreturn 0;\r\nelse\r\nreturn vmw_resource_alloc_id(res);\r\n}\r\nvoid vmw_resource_activate(struct vmw_resource *res,\r\nvoid (*hw_destroy) (struct vmw_resource *))\r\n{\r\nstruct vmw_private *dev_priv = res->dev_priv;\r\nwrite_lock(&dev_priv->resource_lock);\r\nres->avail = true;\r\nres->hw_destroy = hw_destroy;\r\nwrite_unlock(&dev_priv->resource_lock);\r\n}\r\nint vmw_user_resource_lookup_handle(struct vmw_private *dev_priv,\r\nstruct ttm_object_file *tfile,\r\nuint32_t handle,\r\nconst struct vmw_user_resource_conv\r\n*converter,\r\nstruct vmw_resource **p_res)\r\n{\r\nstruct ttm_base_object *base;\r\nstruct vmw_resource *res;\r\nint ret = -EINVAL;\r\nbase = ttm_base_object_lookup(tfile, handle);\r\nif (unlikely(base == NULL))\r\nreturn -EINVAL;\r\nif (unlikely(ttm_base_object_type(base) != converter->object_type))\r\ngoto out_bad_resource;\r\nres = converter->base_obj_to_res(base);\r\nread_lock(&dev_priv->resource_lock);\r\nif (!res->avail || res->res_free != converter->res_free) {\r\nread_unlock(&dev_priv->resource_lock);\r\ngoto out_bad_resource;\r\n}\r\nkref_get(&res->kref);\r\nread_unlock(&dev_priv->resource_lock);\r\n*p_res = res;\r\nret = 0;\r\nout_bad_resource:\r\nttm_base_object_unref(&base);\r\nreturn ret;\r\n}\r\nint vmw_user_lookup_handle(struct vmw_private *dev_priv,\r\nstruct ttm_object_file *tfile,\r\nuint32_t handle,\r\nstruct vmw_surface **out_surf,\r\nstruct vmw_dma_buffer **out_buf)\r\n{\r\nstruct vmw_resource *res;\r\nint ret;\r\nBUG_ON(*out_surf || *out_buf);\r\nret = vmw_user_resource_lookup_handle(dev_priv, tfile, handle,\r\nuser_surface_converter,\r\n&res);\r\nif (!ret) {\r\n*out_surf = vmw_res_to_srf(res);\r\nreturn 0;\r\n}\r\n*out_surf = NULL;\r\nret = vmw_user_dmabuf_lookup(tfile, handle, out_buf, NULL);\r\nreturn ret;\r\n}\r\nstatic size_t vmw_dmabuf_acc_size(struct vmw_private *dev_priv, size_t size,\r\nbool user)\r\n{\r\nstatic size_t struct_size, user_struct_size;\r\nsize_t num_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;\r\nsize_t page_array_size = ttm_round_pot(num_pages * sizeof(void *));\r\nif (unlikely(struct_size == 0)) {\r\nsize_t backend_size = ttm_round_pot(vmw_tt_size);\r\nstruct_size = backend_size +\r\nttm_round_pot(sizeof(struct vmw_dma_buffer));\r\nuser_struct_size = backend_size +\r\nttm_round_pot(sizeof(struct vmw_user_dma_buffer));\r\n}\r\nif (dev_priv->map_mode == vmw_dma_alloc_coherent)\r\npage_array_size +=\r\nttm_round_pot(num_pages * sizeof(dma_addr_t));\r\nreturn ((user) ? user_struct_size : struct_size) +\r\npage_array_size;\r\n}\r\nvoid vmw_dmabuf_bo_free(struct ttm_buffer_object *bo)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo = vmw_dma_buffer(bo);\r\nkfree(vmw_bo);\r\n}\r\nstatic void vmw_user_dmabuf_destroy(struct ttm_buffer_object *bo)\r\n{\r\nstruct vmw_user_dma_buffer *vmw_user_bo = vmw_user_dma_buffer(bo);\r\nttm_prime_object_kfree(vmw_user_bo, prime);\r\n}\r\nint vmw_dmabuf_init(struct vmw_private *dev_priv,\r\nstruct vmw_dma_buffer *vmw_bo,\r\nsize_t size, struct ttm_placement *placement,\r\nbool interruptible,\r\nvoid (*bo_free) (struct ttm_buffer_object *bo))\r\n{\r\nstruct ttm_bo_device *bdev = &dev_priv->bdev;\r\nsize_t acc_size;\r\nint ret;\r\nbool user = (bo_free == &vmw_user_dmabuf_destroy);\r\nBUG_ON(!bo_free && (!user && (bo_free != vmw_dmabuf_bo_free)));\r\nacc_size = vmw_dmabuf_acc_size(dev_priv, size, user);\r\nmemset(vmw_bo, 0, sizeof(*vmw_bo));\r\nINIT_LIST_HEAD(&vmw_bo->res_list);\r\nret = ttm_bo_init(bdev, &vmw_bo->base, size,\r\nttm_bo_type_device, placement,\r\n0, interruptible,\r\nNULL, acc_size, NULL, NULL, bo_free);\r\nreturn ret;\r\n}\r\nstatic void vmw_user_dmabuf_release(struct ttm_base_object **p_base)\r\n{\r\nstruct vmw_user_dma_buffer *vmw_user_bo;\r\nstruct ttm_base_object *base = *p_base;\r\nstruct ttm_buffer_object *bo;\r\n*p_base = NULL;\r\nif (unlikely(base == NULL))\r\nreturn;\r\nvmw_user_bo = container_of(base, struct vmw_user_dma_buffer,\r\nprime.base);\r\nbo = &vmw_user_bo->dma.base;\r\nttm_bo_unref(&bo);\r\n}\r\nstatic void vmw_user_dmabuf_ref_obj_release(struct ttm_base_object *base,\r\nenum ttm_ref_type ref_type)\r\n{\r\nstruct vmw_user_dma_buffer *user_bo;\r\nuser_bo = container_of(base, struct vmw_user_dma_buffer, prime.base);\r\nswitch (ref_type) {\r\ncase TTM_REF_SYNCCPU_WRITE:\r\nttm_bo_synccpu_write_release(&user_bo->dma.base);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nint vmw_user_dmabuf_alloc(struct vmw_private *dev_priv,\r\nstruct ttm_object_file *tfile,\r\nuint32_t size,\r\nbool shareable,\r\nuint32_t *handle,\r\nstruct vmw_dma_buffer **p_dma_buf,\r\nstruct ttm_base_object **p_base)\r\n{\r\nstruct vmw_user_dma_buffer *user_bo;\r\nstruct ttm_buffer_object *tmp;\r\nint ret;\r\nuser_bo = kzalloc(sizeof(*user_bo), GFP_KERNEL);\r\nif (unlikely(!user_bo)) {\r\nDRM_ERROR("Failed to allocate a buffer.\n");\r\nreturn -ENOMEM;\r\n}\r\nret = vmw_dmabuf_init(dev_priv, &user_bo->dma, size,\r\n(dev_priv->has_mob) ?\r\n&vmw_sys_placement :\r\n&vmw_vram_sys_placement, true,\r\n&vmw_user_dmabuf_destroy);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\ntmp = ttm_bo_reference(&user_bo->dma.base);\r\nret = ttm_prime_object_init(tfile,\r\nsize,\r\n&user_bo->prime,\r\nshareable,\r\nttm_buffer_type,\r\n&vmw_user_dmabuf_release,\r\n&vmw_user_dmabuf_ref_obj_release);\r\nif (unlikely(ret != 0)) {\r\nttm_bo_unref(&tmp);\r\ngoto out_no_base_object;\r\n}\r\n*p_dma_buf = &user_bo->dma;\r\nif (p_base) {\r\n*p_base = &user_bo->prime.base;\r\nkref_get(&(*p_base)->refcount);\r\n}\r\n*handle = user_bo->prime.base.hash.key;\r\nout_no_base_object:\r\nreturn ret;\r\n}\r\nint vmw_user_dmabuf_verify_access(struct ttm_buffer_object *bo,\r\nstruct ttm_object_file *tfile)\r\n{\r\nstruct vmw_user_dma_buffer *vmw_user_bo;\r\nif (unlikely(bo->destroy != vmw_user_dmabuf_destroy))\r\nreturn -EPERM;\r\nvmw_user_bo = vmw_user_dma_buffer(bo);\r\nif (likely(ttm_ref_object_exists(tfile, &vmw_user_bo->prime.base)))\r\nreturn 0;\r\nDRM_ERROR("Could not grant buffer access.\n");\r\nreturn -EPERM;\r\n}\r\nstatic int vmw_user_dmabuf_synccpu_grab(struct vmw_user_dma_buffer *user_bo,\r\nstruct ttm_object_file *tfile,\r\nuint32_t flags)\r\n{\r\nstruct ttm_buffer_object *bo = &user_bo->dma.base;\r\nbool existed;\r\nint ret;\r\nif (flags & drm_vmw_synccpu_allow_cs) {\r\nbool nonblock = !!(flags & drm_vmw_synccpu_dontblock);\r\nlong lret;\r\nlret = reservation_object_wait_timeout_rcu(bo->resv, true, true,\r\nnonblock ? 0 : MAX_SCHEDULE_TIMEOUT);\r\nif (!lret)\r\nreturn -EBUSY;\r\nelse if (lret < 0)\r\nreturn lret;\r\nreturn 0;\r\n}\r\nret = ttm_bo_synccpu_write_grab\r\n(bo, !!(flags & drm_vmw_synccpu_dontblock));\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = ttm_ref_object_add(tfile, &user_bo->prime.base,\r\nTTM_REF_SYNCCPU_WRITE, &existed, false);\r\nif (ret != 0 || existed)\r\nttm_bo_synccpu_write_release(&user_bo->dma.base);\r\nreturn ret;\r\n}\r\nstatic int vmw_user_dmabuf_synccpu_release(uint32_t handle,\r\nstruct ttm_object_file *tfile,\r\nuint32_t flags)\r\n{\r\nif (!(flags & drm_vmw_synccpu_allow_cs))\r\nreturn ttm_ref_object_base_unref(tfile, handle,\r\nTTM_REF_SYNCCPU_WRITE);\r\nreturn 0;\r\n}\r\nint vmw_user_dmabuf_synccpu_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_vmw_synccpu_arg *arg =\r\n(struct drm_vmw_synccpu_arg *) data;\r\nstruct vmw_dma_buffer *dma_buf;\r\nstruct vmw_user_dma_buffer *user_bo;\r\nstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\r\nstruct ttm_base_object *buffer_base;\r\nint ret;\r\nif ((arg->flags & (drm_vmw_synccpu_read | drm_vmw_synccpu_write)) == 0\r\n|| (arg->flags & ~(drm_vmw_synccpu_read | drm_vmw_synccpu_write |\r\ndrm_vmw_synccpu_dontblock |\r\ndrm_vmw_synccpu_allow_cs)) != 0) {\r\nDRM_ERROR("Illegal synccpu flags.\n");\r\nreturn -EINVAL;\r\n}\r\nswitch (arg->op) {\r\ncase drm_vmw_synccpu_grab:\r\nret = vmw_user_dmabuf_lookup(tfile, arg->handle, &dma_buf,\r\n&buffer_base);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nuser_bo = container_of(dma_buf, struct vmw_user_dma_buffer,\r\ndma);\r\nret = vmw_user_dmabuf_synccpu_grab(user_bo, tfile, arg->flags);\r\nvmw_dmabuf_unreference(&dma_buf);\r\nttm_base_object_unref(&buffer_base);\r\nif (unlikely(ret != 0 && ret != -ERESTARTSYS &&\r\nret != -EBUSY)) {\r\nDRM_ERROR("Failed synccpu grab on handle 0x%08x.\n",\r\n(unsigned int) arg->handle);\r\nreturn ret;\r\n}\r\nbreak;\r\ncase drm_vmw_synccpu_release:\r\nret = vmw_user_dmabuf_synccpu_release(arg->handle, tfile,\r\narg->flags);\r\nif (unlikely(ret != 0)) {\r\nDRM_ERROR("Failed synccpu release on handle 0x%08x.\n",\r\n(unsigned int) arg->handle);\r\nreturn ret;\r\n}\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Invalid synccpu operation.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint vmw_dmabuf_alloc_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct vmw_private *dev_priv = vmw_priv(dev);\r\nunion drm_vmw_alloc_dmabuf_arg *arg =\r\n(union drm_vmw_alloc_dmabuf_arg *)data;\r\nstruct drm_vmw_alloc_dmabuf_req *req = &arg->req;\r\nstruct drm_vmw_dmabuf_rep *rep = &arg->rep;\r\nstruct vmw_dma_buffer *dma_buf;\r\nuint32_t handle;\r\nint ret;\r\nret = ttm_read_lock(&dev_priv->reservation_sem, true);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_user_dmabuf_alloc(dev_priv, vmw_fpriv(file_priv)->tfile,\r\nreq->size, false, &handle, &dma_buf,\r\nNULL);\r\nif (unlikely(ret != 0))\r\ngoto out_no_dmabuf;\r\nrep->handle = handle;\r\nrep->map_handle = drm_vma_node_offset_addr(&dma_buf->base.vma_node);\r\nrep->cur_gmr_id = handle;\r\nrep->cur_gmr_offset = 0;\r\nvmw_dmabuf_unreference(&dma_buf);\r\nout_no_dmabuf:\r\nttm_read_unlock(&dev_priv->reservation_sem);\r\nreturn ret;\r\n}\r\nint vmw_dmabuf_unref_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_vmw_unref_dmabuf_arg *arg =\r\n(struct drm_vmw_unref_dmabuf_arg *)data;\r\nreturn ttm_ref_object_base_unref(vmw_fpriv(file_priv)->tfile,\r\narg->handle,\r\nTTM_REF_USAGE);\r\n}\r\nint vmw_user_dmabuf_lookup(struct ttm_object_file *tfile,\r\nuint32_t handle, struct vmw_dma_buffer **out,\r\nstruct ttm_base_object **p_base)\r\n{\r\nstruct vmw_user_dma_buffer *vmw_user_bo;\r\nstruct ttm_base_object *base;\r\nbase = ttm_base_object_lookup(tfile, handle);\r\nif (unlikely(base == NULL)) {\r\npr_err("Invalid buffer object handle 0x%08lx\n",\r\n(unsigned long)handle);\r\nreturn -ESRCH;\r\n}\r\nif (unlikely(ttm_base_object_type(base) != ttm_buffer_type)) {\r\nttm_base_object_unref(&base);\r\npr_err("Invalid buffer object handle 0x%08lx\n",\r\n(unsigned long)handle);\r\nreturn -EINVAL;\r\n}\r\nvmw_user_bo = container_of(base, struct vmw_user_dma_buffer,\r\nprime.base);\r\n(void)ttm_bo_reference(&vmw_user_bo->dma.base);\r\nif (p_base)\r\n*p_base = base;\r\nelse\r\nttm_base_object_unref(&base);\r\n*out = &vmw_user_bo->dma;\r\nreturn 0;\r\n}\r\nint vmw_user_dmabuf_reference(struct ttm_object_file *tfile,\r\nstruct vmw_dma_buffer *dma_buf,\r\nuint32_t *handle)\r\n{\r\nstruct vmw_user_dma_buffer *user_bo;\r\nif (dma_buf->base.destroy != vmw_user_dmabuf_destroy)\r\nreturn -EINVAL;\r\nuser_bo = container_of(dma_buf, struct vmw_user_dma_buffer, dma);\r\n*handle = user_bo->prime.base.hash.key;\r\nreturn ttm_ref_object_add(tfile, &user_bo->prime.base,\r\nTTM_REF_USAGE, NULL, false);\r\n}\r\nint vmw_dumb_create(struct drm_file *file_priv,\r\nstruct drm_device *dev,\r\nstruct drm_mode_create_dumb *args)\r\n{\r\nstruct vmw_private *dev_priv = vmw_priv(dev);\r\nstruct vmw_dma_buffer *dma_buf;\r\nint ret;\r\nargs->pitch = args->width * ((args->bpp + 7) / 8);\r\nargs->size = args->pitch * args->height;\r\nret = ttm_read_lock(&dev_priv->reservation_sem, true);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_user_dmabuf_alloc(dev_priv, vmw_fpriv(file_priv)->tfile,\r\nargs->size, false, &args->handle,\r\n&dma_buf, NULL);\r\nif (unlikely(ret != 0))\r\ngoto out_no_dmabuf;\r\nvmw_dmabuf_unreference(&dma_buf);\r\nout_no_dmabuf:\r\nttm_read_unlock(&dev_priv->reservation_sem);\r\nreturn ret;\r\n}\r\nint vmw_dumb_map_offset(struct drm_file *file_priv,\r\nstruct drm_device *dev, uint32_t handle,\r\nuint64_t *offset)\r\n{\r\nstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\r\nstruct vmw_dma_buffer *out_buf;\r\nint ret;\r\nret = vmw_user_dmabuf_lookup(tfile, handle, &out_buf, NULL);\r\nif (ret != 0)\r\nreturn -EINVAL;\r\n*offset = drm_vma_node_offset_addr(&out_buf->base.vma_node);\r\nvmw_dmabuf_unreference(&out_buf);\r\nreturn 0;\r\n}\r\nint vmw_dumb_destroy(struct drm_file *file_priv,\r\nstruct drm_device *dev,\r\nuint32_t handle)\r\n{\r\nreturn ttm_ref_object_base_unref(vmw_fpriv(file_priv)->tfile,\r\nhandle, TTM_REF_USAGE);\r\n}\r\nstatic int vmw_resource_buf_alloc(struct vmw_resource *res,\r\nbool interruptible)\r\n{\r\nunsigned long size =\r\n(res->backup_size + PAGE_SIZE - 1) & PAGE_MASK;\r\nstruct vmw_dma_buffer *backup;\r\nint ret;\r\nif (likely(res->backup)) {\r\nBUG_ON(res->backup->base.num_pages * PAGE_SIZE < size);\r\nreturn 0;\r\n}\r\nbackup = kzalloc(sizeof(*backup), GFP_KERNEL);\r\nif (unlikely(!backup))\r\nreturn -ENOMEM;\r\nret = vmw_dmabuf_init(res->dev_priv, backup, res->backup_size,\r\nres->func->backup_placement,\r\ninterruptible,\r\n&vmw_dmabuf_bo_free);\r\nif (unlikely(ret != 0))\r\ngoto out_no_dmabuf;\r\nres->backup = backup;\r\nout_no_dmabuf:\r\nreturn ret;\r\n}\r\nstatic int vmw_resource_do_validate(struct vmw_resource *res,\r\nstruct ttm_validate_buffer *val_buf)\r\n{\r\nint ret = 0;\r\nconst struct vmw_res_func *func = res->func;\r\nif (unlikely(res->id == -1)) {\r\nret = func->create(res);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\nif (func->bind &&\r\n((func->needs_backup && list_empty(&res->mob_head) &&\r\nval_buf->bo != NULL) ||\r\n(!func->needs_backup && val_buf->bo != NULL))) {\r\nret = func->bind(res, val_buf);\r\nif (unlikely(ret != 0))\r\ngoto out_bind_failed;\r\nif (func->needs_backup)\r\nlist_add_tail(&res->mob_head, &res->backup->res_list);\r\n}\r\nres->res_dirty = true;\r\nreturn 0;\r\nout_bind_failed:\r\nfunc->destroy(res);\r\nreturn ret;\r\n}\r\nvoid vmw_resource_unreserve(struct vmw_resource *res,\r\nbool switch_backup,\r\nstruct vmw_dma_buffer *new_backup,\r\nunsigned long new_backup_offset)\r\n{\r\nstruct vmw_private *dev_priv = res->dev_priv;\r\nif (!list_empty(&res->lru_head))\r\nreturn;\r\nif (switch_backup && new_backup != res->backup) {\r\nif (res->backup) {\r\nlockdep_assert_held(&res->backup->base.resv->lock.base);\r\nlist_del_init(&res->mob_head);\r\nvmw_dmabuf_unreference(&res->backup);\r\n}\r\nif (new_backup) {\r\nres->backup = vmw_dmabuf_reference(new_backup);\r\nlockdep_assert_held(&new_backup->base.resv->lock.base);\r\nlist_add_tail(&res->mob_head, &new_backup->res_list);\r\n} else {\r\nres->backup = NULL;\r\n}\r\n}\r\nif (switch_backup)\r\nres->backup_offset = new_backup_offset;\r\nif (!res->func->may_evict || res->id == -1 || res->pin_count)\r\nreturn;\r\nwrite_lock(&dev_priv->resource_lock);\r\nlist_add_tail(&res->lru_head,\r\n&res->dev_priv->res_lru[res->func->res_type]);\r\nwrite_unlock(&dev_priv->resource_lock);\r\n}\r\nstatic int\r\nvmw_resource_check_buffer(struct vmw_resource *res,\r\nbool interruptible,\r\nstruct ttm_validate_buffer *val_buf)\r\n{\r\nstruct list_head val_list;\r\nbool backup_dirty = false;\r\nint ret;\r\nif (unlikely(res->backup == NULL)) {\r\nret = vmw_resource_buf_alloc(res, interruptible);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\nINIT_LIST_HEAD(&val_list);\r\nval_buf->bo = ttm_bo_reference(&res->backup->base);\r\nval_buf->shared = false;\r\nlist_add_tail(&val_buf->head, &val_list);\r\nret = ttm_eu_reserve_buffers(NULL, &val_list, interruptible, NULL);\r\nif (unlikely(ret != 0))\r\ngoto out_no_reserve;\r\nif (res->func->needs_backup && list_empty(&res->mob_head))\r\nreturn 0;\r\nbackup_dirty = res->backup_dirty;\r\nret = ttm_bo_validate(&res->backup->base,\r\nres->func->backup_placement,\r\ntrue, false);\r\nif (unlikely(ret != 0))\r\ngoto out_no_validate;\r\nreturn 0;\r\nout_no_validate:\r\nttm_eu_backoff_reservation(NULL, &val_list);\r\nout_no_reserve:\r\nttm_bo_unref(&val_buf->bo);\r\nif (backup_dirty)\r\nvmw_dmabuf_unreference(&res->backup);\r\nreturn ret;\r\n}\r\nint vmw_resource_reserve(struct vmw_resource *res, bool interruptible,\r\nbool no_backup)\r\n{\r\nstruct vmw_private *dev_priv = res->dev_priv;\r\nint ret;\r\nwrite_lock(&dev_priv->resource_lock);\r\nlist_del_init(&res->lru_head);\r\nwrite_unlock(&dev_priv->resource_lock);\r\nif (res->func->needs_backup && res->backup == NULL &&\r\n!no_backup) {\r\nret = vmw_resource_buf_alloc(res, interruptible);\r\nif (unlikely(ret != 0)) {\r\nDRM_ERROR("Failed to allocate a backup buffer "\r\n"of size %lu. bytes\n",\r\n(unsigned long) res->backup_size);\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nvmw_resource_backoff_reservation(struct ttm_validate_buffer *val_buf)\r\n{\r\nstruct list_head val_list;\r\nif (likely(val_buf->bo == NULL))\r\nreturn;\r\nINIT_LIST_HEAD(&val_list);\r\nlist_add_tail(&val_buf->head, &val_list);\r\nttm_eu_backoff_reservation(NULL, &val_list);\r\nttm_bo_unref(&val_buf->bo);\r\n}\r\nstatic int vmw_resource_do_evict(struct vmw_resource *res, bool interruptible)\r\n{\r\nstruct ttm_validate_buffer val_buf;\r\nconst struct vmw_res_func *func = res->func;\r\nint ret;\r\nBUG_ON(!func->may_evict);\r\nval_buf.bo = NULL;\r\nval_buf.shared = false;\r\nret = vmw_resource_check_buffer(res, interruptible, &val_buf);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (unlikely(func->unbind != NULL &&\r\n(!func->needs_backup || !list_empty(&res->mob_head)))) {\r\nret = func->unbind(res, res->res_dirty, &val_buf);\r\nif (unlikely(ret != 0))\r\ngoto out_no_unbind;\r\nlist_del_init(&res->mob_head);\r\n}\r\nret = func->destroy(res);\r\nres->backup_dirty = true;\r\nres->res_dirty = false;\r\nout_no_unbind:\r\nvmw_resource_backoff_reservation(&val_buf);\r\nreturn ret;\r\n}\r\nint vmw_resource_validate(struct vmw_resource *res)\r\n{\r\nint ret;\r\nstruct vmw_resource *evict_res;\r\nstruct vmw_private *dev_priv = res->dev_priv;\r\nstruct list_head *lru_list = &dev_priv->res_lru[res->func->res_type];\r\nstruct ttm_validate_buffer val_buf;\r\nunsigned err_count = 0;\r\nif (!res->func->create)\r\nreturn 0;\r\nval_buf.bo = NULL;\r\nval_buf.shared = false;\r\nif (res->backup)\r\nval_buf.bo = &res->backup->base;\r\ndo {\r\nret = vmw_resource_do_validate(res, &val_buf);\r\nif (likely(ret != -EBUSY))\r\nbreak;\r\nwrite_lock(&dev_priv->resource_lock);\r\nif (list_empty(lru_list) || !res->func->may_evict) {\r\nDRM_ERROR("Out of device device resources "\r\n"for %s.\n", res->func->type_name);\r\nret = -EBUSY;\r\nwrite_unlock(&dev_priv->resource_lock);\r\nbreak;\r\n}\r\nevict_res = vmw_resource_reference\r\n(list_first_entry(lru_list, struct vmw_resource,\r\nlru_head));\r\nlist_del_init(&evict_res->lru_head);\r\nwrite_unlock(&dev_priv->resource_lock);\r\nret = vmw_resource_do_evict(evict_res, true);\r\nif (unlikely(ret != 0)) {\r\nwrite_lock(&dev_priv->resource_lock);\r\nlist_add_tail(&evict_res->lru_head, lru_list);\r\nwrite_unlock(&dev_priv->resource_lock);\r\nif (ret == -ERESTARTSYS ||\r\n++err_count > VMW_RES_EVICT_ERR_COUNT) {\r\nvmw_resource_unreference(&evict_res);\r\ngoto out_no_validate;\r\n}\r\n}\r\nvmw_resource_unreference(&evict_res);\r\n} while (1);\r\nif (unlikely(ret != 0))\r\ngoto out_no_validate;\r\nelse if (!res->func->needs_backup && res->backup) {\r\nlist_del_init(&res->mob_head);\r\nvmw_dmabuf_unreference(&res->backup);\r\n}\r\nreturn 0;\r\nout_no_validate:\r\nreturn ret;\r\n}\r\nvoid vmw_fence_single_bo(struct ttm_buffer_object *bo,\r\nstruct vmw_fence_obj *fence)\r\n{\r\nstruct ttm_bo_device *bdev = bo->bdev;\r\nstruct vmw_private *dev_priv =\r\ncontainer_of(bdev, struct vmw_private, bdev);\r\nif (fence == NULL) {\r\nvmw_execbuf_fence_commands(NULL, dev_priv, &fence, NULL);\r\nreservation_object_add_excl_fence(bo->resv, &fence->base);\r\ndma_fence_put(&fence->base);\r\n} else\r\nreservation_object_add_excl_fence(bo->resv, &fence->base);\r\n}\r\nvoid vmw_resource_move_notify(struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nstruct vmw_dma_buffer *dma_buf;\r\nif (mem == NULL)\r\nreturn;\r\nif (bo->destroy != vmw_dmabuf_bo_free &&\r\nbo->destroy != vmw_user_dmabuf_destroy)\r\nreturn;\r\ndma_buf = container_of(bo, struct vmw_dma_buffer, base);\r\nif (mem->mem_type != VMW_PL_MOB) {\r\nstruct vmw_resource *res, *n;\r\nstruct ttm_validate_buffer val_buf;\r\nval_buf.bo = bo;\r\nval_buf.shared = false;\r\nlist_for_each_entry_safe(res, n, &dma_buf->res_list, mob_head) {\r\nif (unlikely(res->func->unbind == NULL))\r\ncontinue;\r\n(void) res->func->unbind(res, true, &val_buf);\r\nres->backup_dirty = true;\r\nres->res_dirty = false;\r\nlist_del_init(&res->mob_head);\r\n}\r\n(void) ttm_bo_wait(bo, false, false);\r\n}\r\n}\r\nint vmw_query_readback_all(struct vmw_dma_buffer *dx_query_mob)\r\n{\r\nstruct vmw_resource *dx_query_ctx;\r\nstruct vmw_private *dev_priv;\r\nstruct {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdDXReadbackAllQuery body;\r\n} *cmd;\r\nif (!dx_query_mob || !dx_query_mob->dx_query_ctx)\r\nreturn 0;\r\ndx_query_ctx = dx_query_mob->dx_query_ctx;\r\ndev_priv = dx_query_ctx->dev_priv;\r\ncmd = vmw_fifo_reserve_dx(dev_priv, sizeof(*cmd), dx_query_ctx->id);\r\nif (unlikely(cmd == NULL)) {\r\nDRM_ERROR("Failed reserving FIFO space for "\r\n"query MOB read back.\n");\r\nreturn -ENOMEM;\r\n}\r\ncmd->header.id = SVGA_3D_CMD_DX_READBACK_ALL_QUERY;\r\ncmd->header.size = sizeof(cmd->body);\r\ncmd->body.cid = dx_query_ctx->id;\r\nvmw_fifo_commit(dev_priv, sizeof(*cmd));\r\ndx_query_mob->dx_query_ctx = NULL;\r\nreturn 0;\r\n}\r\nvoid vmw_query_move_notify(struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nstruct vmw_dma_buffer *dx_query_mob;\r\nstruct ttm_bo_device *bdev = bo->bdev;\r\nstruct vmw_private *dev_priv;\r\ndev_priv = container_of(bdev, struct vmw_private, bdev);\r\nmutex_lock(&dev_priv->binding_mutex);\r\ndx_query_mob = container_of(bo, struct vmw_dma_buffer, base);\r\nif (mem == NULL || !dx_query_mob || !dx_query_mob->dx_query_ctx) {\r\nmutex_unlock(&dev_priv->binding_mutex);\r\nreturn;\r\n}\r\nif (mem->mem_type == TTM_PL_SYSTEM && bo->mem.mem_type == VMW_PL_MOB) {\r\nstruct vmw_fence_obj *fence;\r\n(void) vmw_query_readback_all(dx_query_mob);\r\nmutex_unlock(&dev_priv->binding_mutex);\r\n(void) vmw_execbuf_fence_commands(NULL, dev_priv, &fence, NULL);\r\nvmw_fence_single_bo(bo, fence);\r\nif (fence != NULL)\r\nvmw_fence_obj_unreference(&fence);\r\n(void) ttm_bo_wait(bo, false, false);\r\n} else\r\nmutex_unlock(&dev_priv->binding_mutex);\r\n}\r\nbool vmw_resource_needs_backup(const struct vmw_resource *res)\r\n{\r\nreturn res->func->needs_backup;\r\n}\r\nstatic void vmw_resource_evict_type(struct vmw_private *dev_priv,\r\nenum vmw_res_type type)\r\n{\r\nstruct list_head *lru_list = &dev_priv->res_lru[type];\r\nstruct vmw_resource *evict_res;\r\nunsigned err_count = 0;\r\nint ret;\r\ndo {\r\nwrite_lock(&dev_priv->resource_lock);\r\nif (list_empty(lru_list))\r\ngoto out_unlock;\r\nevict_res = vmw_resource_reference(\r\nlist_first_entry(lru_list, struct vmw_resource,\r\nlru_head));\r\nlist_del_init(&evict_res->lru_head);\r\nwrite_unlock(&dev_priv->resource_lock);\r\nret = vmw_resource_do_evict(evict_res, false);\r\nif (unlikely(ret != 0)) {\r\nwrite_lock(&dev_priv->resource_lock);\r\nlist_add_tail(&evict_res->lru_head, lru_list);\r\nwrite_unlock(&dev_priv->resource_lock);\r\nif (++err_count > VMW_RES_EVICT_ERR_COUNT) {\r\nvmw_resource_unreference(&evict_res);\r\nreturn;\r\n}\r\n}\r\nvmw_resource_unreference(&evict_res);\r\n} while (1);\r\nout_unlock:\r\nwrite_unlock(&dev_priv->resource_lock);\r\n}\r\nvoid vmw_resource_evict_all(struct vmw_private *dev_priv)\r\n{\r\nenum vmw_res_type type;\r\nmutex_lock(&dev_priv->cmdbuf_mutex);\r\nfor (type = 0; type < vmw_res_max; ++type)\r\nvmw_resource_evict_type(dev_priv, type);\r\nmutex_unlock(&dev_priv->cmdbuf_mutex);\r\n}\r\nint vmw_resource_pin(struct vmw_resource *res, bool interruptible)\r\n{\r\nstruct vmw_private *dev_priv = res->dev_priv;\r\nint ret;\r\nttm_write_lock(&dev_priv->reservation_sem, interruptible);\r\nmutex_lock(&dev_priv->cmdbuf_mutex);\r\nret = vmw_resource_reserve(res, interruptible, false);\r\nif (ret)\r\ngoto out_no_reserve;\r\nif (res->pin_count == 0) {\r\nstruct vmw_dma_buffer *vbo = NULL;\r\nif (res->backup) {\r\nvbo = res->backup;\r\nttm_bo_reserve(&vbo->base, interruptible, false, NULL);\r\nif (!vbo->pin_count) {\r\nret = ttm_bo_validate\r\n(&vbo->base,\r\nres->func->backup_placement,\r\ninterruptible, false);\r\nif (ret) {\r\nttm_bo_unreserve(&vbo->base);\r\ngoto out_no_validate;\r\n}\r\n}\r\nvmw_bo_pin_reserved(vbo, true);\r\n}\r\nret = vmw_resource_validate(res);\r\nif (vbo)\r\nttm_bo_unreserve(&vbo->base);\r\nif (ret)\r\ngoto out_no_validate;\r\n}\r\nres->pin_count++;\r\nout_no_validate:\r\nvmw_resource_unreserve(res, false, NULL, 0UL);\r\nout_no_reserve:\r\nmutex_unlock(&dev_priv->cmdbuf_mutex);\r\nttm_write_unlock(&dev_priv->reservation_sem);\r\nreturn ret;\r\n}\r\nvoid vmw_resource_unpin(struct vmw_resource *res)\r\n{\r\nstruct vmw_private *dev_priv = res->dev_priv;\r\nint ret;\r\n(void) ttm_read_lock(&dev_priv->reservation_sem, false);\r\nmutex_lock(&dev_priv->cmdbuf_mutex);\r\nret = vmw_resource_reserve(res, false, true);\r\nWARN_ON(ret);\r\nWARN_ON(res->pin_count == 0);\r\nif (--res->pin_count == 0 && res->backup) {\r\nstruct vmw_dma_buffer *vbo = res->backup;\r\n(void) ttm_bo_reserve(&vbo->base, false, false, NULL);\r\nvmw_bo_pin_reserved(vbo, false);\r\nttm_bo_unreserve(&vbo->base);\r\n}\r\nvmw_resource_unreserve(res, false, NULL, 0UL);\r\nmutex_unlock(&dev_priv->cmdbuf_mutex);\r\nttm_read_unlock(&dev_priv->reservation_sem);\r\n}\r\nenum vmw_res_type vmw_res_type(const struct vmw_resource *res)\r\n{\r\nreturn res->func->res_type;\r\n}
