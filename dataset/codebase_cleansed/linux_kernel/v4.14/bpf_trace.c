unsigned int trace_call_bpf(struct bpf_prog *prog, void *ctx)\r\n{\r\nunsigned int ret;\r\nif (in_nmi())\r\nreturn 1;\r\npreempt_disable();\r\nif (unlikely(__this_cpu_inc_return(bpf_prog_active) != 1)) {\r\nret = 0;\r\ngoto out;\r\n}\r\nrcu_read_lock();\r\nret = BPF_PROG_RUN(prog, ctx);\r\nrcu_read_unlock();\r\nout:\r\n__this_cpu_dec(bpf_prog_active);\r\npreempt_enable();\r\nreturn ret;\r\n}\r\nstatic const struct bpf_func_proto *bpf_get_probe_write_proto(void)\r\n{\r\npr_warn_ratelimited("%s[%d] is installing a program with bpf_probe_write_user helper that may corrupt user memory!",\r\ncurrent->comm, task_pid_nr(current));\r\nreturn &bpf_probe_write_user_proto;\r\n}\r\nconst struct bpf_func_proto *bpf_get_trace_printk_proto(void)\r\n{\r\ntrace_printk_init_buffers();\r\nreturn &bpf_trace_printk_proto;\r\n}\r\nstatic __always_inline u64\r\n__bpf_perf_event_output(struct pt_regs *regs, struct bpf_map *map,\r\nu64 flags, struct perf_raw_record *raw)\r\n{\r\nstruct bpf_array *array = container_of(map, struct bpf_array, map);\r\nstruct perf_sample_data *sd = this_cpu_ptr(&bpf_sd);\r\nunsigned int cpu = smp_processor_id();\r\nu64 index = flags & BPF_F_INDEX_MASK;\r\nstruct bpf_event_entry *ee;\r\nstruct perf_event *event;\r\nif (index == BPF_F_CURRENT_CPU)\r\nindex = cpu;\r\nif (unlikely(index >= array->map.max_entries))\r\nreturn -E2BIG;\r\nee = READ_ONCE(array->ptrs[index]);\r\nif (!ee)\r\nreturn -ENOENT;\r\nevent = ee->event;\r\nif (unlikely(event->attr.type != PERF_TYPE_SOFTWARE ||\r\nevent->attr.config != PERF_COUNT_SW_BPF_OUTPUT))\r\nreturn -EINVAL;\r\nif (unlikely(event->oncpu != cpu))\r\nreturn -EOPNOTSUPP;\r\nperf_sample_data_init(sd, 0, 0);\r\nsd->raw = raw;\r\nperf_event_output(event, sd, regs);\r\nreturn 0;\r\n}\r\nu64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,\r\nvoid *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy)\r\n{\r\nstruct pt_regs *regs = this_cpu_ptr(&bpf_pt_regs);\r\nstruct perf_raw_frag frag = {\r\n.copy = ctx_copy,\r\n.size = ctx_size,\r\n.data = ctx,\r\n};\r\nstruct perf_raw_record raw = {\r\n.frag = {\r\n{\r\n.next = ctx_size ? &frag : NULL,\r\n},\r\n.size = meta_size,\r\n.data = meta,\r\n},\r\n};\r\nperf_fetch_caller_regs(regs);\r\nreturn __bpf_perf_event_output(regs, map, flags, &raw);\r\n}\r\nstatic const struct bpf_func_proto *tracing_func_proto(enum bpf_func_id func_id)\r\n{\r\nswitch (func_id) {\r\ncase BPF_FUNC_map_lookup_elem:\r\nreturn &bpf_map_lookup_elem_proto;\r\ncase BPF_FUNC_map_update_elem:\r\nreturn &bpf_map_update_elem_proto;\r\ncase BPF_FUNC_map_delete_elem:\r\nreturn &bpf_map_delete_elem_proto;\r\ncase BPF_FUNC_probe_read:\r\nreturn &bpf_probe_read_proto;\r\ncase BPF_FUNC_ktime_get_ns:\r\nreturn &bpf_ktime_get_ns_proto;\r\ncase BPF_FUNC_tail_call:\r\nreturn &bpf_tail_call_proto;\r\ncase BPF_FUNC_get_current_pid_tgid:\r\nreturn &bpf_get_current_pid_tgid_proto;\r\ncase BPF_FUNC_get_current_task:\r\nreturn &bpf_get_current_task_proto;\r\ncase BPF_FUNC_get_current_uid_gid:\r\nreturn &bpf_get_current_uid_gid_proto;\r\ncase BPF_FUNC_get_current_comm:\r\nreturn &bpf_get_current_comm_proto;\r\ncase BPF_FUNC_trace_printk:\r\nreturn bpf_get_trace_printk_proto();\r\ncase BPF_FUNC_get_smp_processor_id:\r\nreturn &bpf_get_smp_processor_id_proto;\r\ncase BPF_FUNC_get_numa_node_id:\r\nreturn &bpf_get_numa_node_id_proto;\r\ncase BPF_FUNC_perf_event_read:\r\nreturn &bpf_perf_event_read_proto;\r\ncase BPF_FUNC_probe_write_user:\r\nreturn bpf_get_probe_write_proto();\r\ncase BPF_FUNC_current_task_under_cgroup:\r\nreturn &bpf_current_task_under_cgroup_proto;\r\ncase BPF_FUNC_get_prandom_u32:\r\nreturn &bpf_get_prandom_u32_proto;\r\ncase BPF_FUNC_probe_read_str:\r\nreturn &bpf_probe_read_str_proto;\r\ndefault:\r\nreturn NULL;\r\n}\r\n}\r\nstatic const struct bpf_func_proto *kprobe_prog_func_proto(enum bpf_func_id func_id)\r\n{\r\nswitch (func_id) {\r\ncase BPF_FUNC_perf_event_output:\r\nreturn &bpf_perf_event_output_proto;\r\ncase BPF_FUNC_get_stackid:\r\nreturn &bpf_get_stackid_proto;\r\ndefault:\r\nreturn tracing_func_proto(func_id);\r\n}\r\n}\r\nstatic bool kprobe_prog_is_valid_access(int off, int size, enum bpf_access_type type,\r\nstruct bpf_insn_access_aux *info)\r\n{\r\nif (off < 0 || off >= sizeof(struct pt_regs))\r\nreturn false;\r\nif (type != BPF_READ)\r\nreturn false;\r\nif (off % size != 0)\r\nreturn false;\r\nif (off + size > sizeof(struct pt_regs))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic const struct bpf_func_proto *tp_prog_func_proto(enum bpf_func_id func_id)\r\n{\r\nswitch (func_id) {\r\ncase BPF_FUNC_perf_event_output:\r\nreturn &bpf_perf_event_output_proto_tp;\r\ncase BPF_FUNC_get_stackid:\r\nreturn &bpf_get_stackid_proto_tp;\r\ndefault:\r\nreturn tracing_func_proto(func_id);\r\n}\r\n}\r\nstatic bool tp_prog_is_valid_access(int off, int size, enum bpf_access_type type,\r\nstruct bpf_insn_access_aux *info)\r\n{\r\nif (off < sizeof(void *) || off >= PERF_MAX_TRACE_SIZE)\r\nreturn false;\r\nif (type != BPF_READ)\r\nreturn false;\r\nif (off % size != 0)\r\nreturn false;\r\nBUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(__u64));\r\nreturn true;\r\n}\r\nstatic bool pe_prog_is_valid_access(int off, int size, enum bpf_access_type type,\r\nstruct bpf_insn_access_aux *info)\r\n{\r\nconst int size_sp = FIELD_SIZEOF(struct bpf_perf_event_data,\r\nsample_period);\r\nif (off < 0 || off >= sizeof(struct bpf_perf_event_data))\r\nreturn false;\r\nif (type != BPF_READ)\r\nreturn false;\r\nif (off % size != 0)\r\nreturn false;\r\nswitch (off) {\r\ncase bpf_ctx_range(struct bpf_perf_event_data, sample_period):\r\nbpf_ctx_record_field_size(info, size_sp);\r\nif (!bpf_ctx_narrow_access_ok(off, size, size_sp))\r\nreturn false;\r\nbreak;\r\ndefault:\r\nif (size != sizeof(long))\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic u32 pe_prog_convert_ctx_access(enum bpf_access_type type,\r\nconst struct bpf_insn *si,\r\nstruct bpf_insn *insn_buf,\r\nstruct bpf_prog *prog, u32 *target_size)\r\n{\r\nstruct bpf_insn *insn = insn_buf;\r\nswitch (si->off) {\r\ncase offsetof(struct bpf_perf_event_data, sample_period):\r\n*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_perf_event_data_kern,\r\ndata), si->dst_reg, si->src_reg,\r\noffsetof(struct bpf_perf_event_data_kern, data));\r\n*insn++ = BPF_LDX_MEM(BPF_DW, si->dst_reg, si->dst_reg,\r\nbpf_target_off(struct perf_sample_data, period, 8,\r\ntarget_size));\r\nbreak;\r\ndefault:\r\n*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_perf_event_data_kern,\r\nregs), si->dst_reg, si->src_reg,\r\noffsetof(struct bpf_perf_event_data_kern, regs));\r\n*insn++ = BPF_LDX_MEM(BPF_SIZEOF(long), si->dst_reg, si->dst_reg,\r\nsi->off);\r\nbreak;\r\n}\r\nreturn insn - insn_buf;\r\n}
