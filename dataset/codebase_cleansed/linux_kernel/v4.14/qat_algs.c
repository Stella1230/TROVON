static int qat_get_inter_state_size(enum icp_qat_hw_auth_algo qat_hash_alg)\r\n{\r\nswitch (qat_hash_alg) {\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA1:\r\nreturn ICP_QAT_HW_SHA1_STATE1_SZ;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA256:\r\nreturn ICP_QAT_HW_SHA256_STATE1_SZ;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA512:\r\nreturn ICP_QAT_HW_SHA512_STATE1_SZ;\r\ndefault:\r\nreturn -EFAULT;\r\n};\r\nreturn -EFAULT;\r\n}\r\nstatic int qat_alg_do_precomputes(struct icp_qat_hw_auth_algo_blk *hash,\r\nstruct qat_alg_aead_ctx *ctx,\r\nconst uint8_t *auth_key,\r\nunsigned int auth_keylen)\r\n{\r\nSHASH_DESC_ON_STACK(shash, ctx->hash_tfm);\r\nstruct sha1_state sha1;\r\nstruct sha256_state sha256;\r\nstruct sha512_state sha512;\r\nint block_size = crypto_shash_blocksize(ctx->hash_tfm);\r\nint digest_size = crypto_shash_digestsize(ctx->hash_tfm);\r\nchar ipad[block_size];\r\nchar opad[block_size];\r\n__be32 *hash_state_out;\r\n__be64 *hash512_state_out;\r\nint i, offset;\r\nmemset(ipad, 0, block_size);\r\nmemset(opad, 0, block_size);\r\nshash->tfm = ctx->hash_tfm;\r\nshash->flags = 0x0;\r\nif (auth_keylen > block_size) {\r\nint ret = crypto_shash_digest(shash, auth_key,\r\nauth_keylen, ipad);\r\nif (ret)\r\nreturn ret;\r\nmemcpy(opad, ipad, digest_size);\r\n} else {\r\nmemcpy(ipad, auth_key, auth_keylen);\r\nmemcpy(opad, auth_key, auth_keylen);\r\n}\r\nfor (i = 0; i < block_size; i++) {\r\nchar *ipad_ptr = ipad + i;\r\nchar *opad_ptr = opad + i;\r\n*ipad_ptr ^= HMAC_IPAD_VALUE;\r\n*opad_ptr ^= HMAC_OPAD_VALUE;\r\n}\r\nif (crypto_shash_init(shash))\r\nreturn -EFAULT;\r\nif (crypto_shash_update(shash, ipad, block_size))\r\nreturn -EFAULT;\r\nhash_state_out = (__be32 *)hash->sha.state1;\r\nhash512_state_out = (__be64 *)hash_state_out;\r\nswitch (ctx->qat_hash_alg) {\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA1:\r\nif (crypto_shash_export(shash, &sha1))\r\nreturn -EFAULT;\r\nfor (i = 0; i < digest_size >> 2; i++, hash_state_out++)\r\n*hash_state_out = cpu_to_be32(*(sha1.state + i));\r\nbreak;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA256:\r\nif (crypto_shash_export(shash, &sha256))\r\nreturn -EFAULT;\r\nfor (i = 0; i < digest_size >> 2; i++, hash_state_out++)\r\n*hash_state_out = cpu_to_be32(*(sha256.state + i));\r\nbreak;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA512:\r\nif (crypto_shash_export(shash, &sha512))\r\nreturn -EFAULT;\r\nfor (i = 0; i < digest_size >> 3; i++, hash512_state_out++)\r\n*hash512_state_out = cpu_to_be64(*(sha512.state + i));\r\nbreak;\r\ndefault:\r\nreturn -EFAULT;\r\n}\r\nif (crypto_shash_init(shash))\r\nreturn -EFAULT;\r\nif (crypto_shash_update(shash, opad, block_size))\r\nreturn -EFAULT;\r\noffset = round_up(qat_get_inter_state_size(ctx->qat_hash_alg), 8);\r\nhash_state_out = (__be32 *)(hash->sha.state1 + offset);\r\nhash512_state_out = (__be64 *)hash_state_out;\r\nswitch (ctx->qat_hash_alg) {\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA1:\r\nif (crypto_shash_export(shash, &sha1))\r\nreturn -EFAULT;\r\nfor (i = 0; i < digest_size >> 2; i++, hash_state_out++)\r\n*hash_state_out = cpu_to_be32(*(sha1.state + i));\r\nbreak;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA256:\r\nif (crypto_shash_export(shash, &sha256))\r\nreturn -EFAULT;\r\nfor (i = 0; i < digest_size >> 2; i++, hash_state_out++)\r\n*hash_state_out = cpu_to_be32(*(sha256.state + i));\r\nbreak;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA512:\r\nif (crypto_shash_export(shash, &sha512))\r\nreturn -EFAULT;\r\nfor (i = 0; i < digest_size >> 3; i++, hash512_state_out++)\r\n*hash512_state_out = cpu_to_be64(*(sha512.state + i));\r\nbreak;\r\ndefault:\r\nreturn -EFAULT;\r\n}\r\nmemzero_explicit(ipad, block_size);\r\nmemzero_explicit(opad, block_size);\r\nreturn 0;\r\n}\r\nstatic void qat_alg_init_common_hdr(struct icp_qat_fw_comn_req_hdr *header)\r\n{\r\nheader->hdr_flags =\r\nICP_QAT_FW_COMN_HDR_FLAGS_BUILD(ICP_QAT_FW_COMN_REQ_FLAG_SET);\r\nheader->service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_LA;\r\nheader->comn_req_flags =\r\nICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_CD_FLD_TYPE_64BIT_ADR,\r\nQAT_COMN_PTR_TYPE_SGL);\r\nICP_QAT_FW_LA_PARTIAL_SET(header->serv_specif_flags,\r\nICP_QAT_FW_LA_PARTIAL_NONE);\r\nICP_QAT_FW_LA_CIPH_IV_FLD_FLAG_SET(header->serv_specif_flags,\r\nICP_QAT_FW_CIPH_IV_16BYTE_DATA);\r\nICP_QAT_FW_LA_PROTO_SET(header->serv_specif_flags,\r\nICP_QAT_FW_LA_NO_PROTO);\r\nICP_QAT_FW_LA_UPDATE_STATE_SET(header->serv_specif_flags,\r\nICP_QAT_FW_LA_NO_UPDATE_STATE);\r\n}\r\nstatic int qat_alg_aead_init_enc_session(struct crypto_aead *aead_tfm,\r\nint alg,\r\nstruct crypto_authenc_keys *keys,\r\nint mode)\r\n{\r\nstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(aead_tfm);\r\nunsigned int digestsize = crypto_aead_authsize(aead_tfm);\r\nstruct qat_enc *enc_ctx = &ctx->enc_cd->qat_enc_cd;\r\nstruct icp_qat_hw_cipher_algo_blk *cipher = &enc_ctx->cipher;\r\nstruct icp_qat_hw_auth_algo_blk *hash =\r\n(struct icp_qat_hw_auth_algo_blk *)((char *)enc_ctx +\r\nsizeof(struct icp_qat_hw_auth_setup) + keys->enckeylen);\r\nstruct icp_qat_fw_la_bulk_req *req_tmpl = &ctx->enc_fw_req;\r\nstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req_tmpl->cd_pars;\r\nstruct icp_qat_fw_comn_req_hdr *header = &req_tmpl->comn_hdr;\r\nvoid *ptr = &req_tmpl->cd_ctrl;\r\nstruct icp_qat_fw_cipher_cd_ctrl_hdr *cipher_cd_ctrl = ptr;\r\nstruct icp_qat_fw_auth_cd_ctrl_hdr *hash_cd_ctrl = ptr;\r\ncipher->aes.cipher_config.val = QAT_AES_HW_CONFIG_ENC(alg, mode);\r\nmemcpy(cipher->aes.key, keys->enckey, keys->enckeylen);\r\nhash->sha.inner_setup.auth_config.config =\r\nICP_QAT_HW_AUTH_CONFIG_BUILD(ICP_QAT_HW_AUTH_MODE1,\r\nctx->qat_hash_alg, digestsize);\r\nhash->sha.inner_setup.auth_counter.counter =\r\ncpu_to_be32(crypto_shash_blocksize(ctx->hash_tfm));\r\nif (qat_alg_do_precomputes(hash, ctx, keys->authkey, keys->authkeylen))\r\nreturn -EFAULT;\r\nqat_alg_init_common_hdr(header);\r\nheader->service_cmd_id = ICP_QAT_FW_LA_CMD_CIPHER_HASH;\r\nICP_QAT_FW_LA_DIGEST_IN_BUFFER_SET(header->serv_specif_flags,\r\nICP_QAT_FW_LA_DIGEST_IN_BUFFER);\r\nICP_QAT_FW_LA_RET_AUTH_SET(header->serv_specif_flags,\r\nICP_QAT_FW_LA_RET_AUTH_RES);\r\nICP_QAT_FW_LA_CMP_AUTH_SET(header->serv_specif_flags,\r\nICP_QAT_FW_LA_NO_CMP_AUTH_RES);\r\ncd_pars->u.s.content_desc_addr = ctx->enc_cd_paddr;\r\ncd_pars->u.s.content_desc_params_sz = sizeof(struct qat_alg_cd) >> 3;\r\ncipher_cd_ctrl->cipher_key_sz = keys->enckeylen >> 3;\r\ncipher_cd_ctrl->cipher_state_sz = AES_BLOCK_SIZE >> 3;\r\ncipher_cd_ctrl->cipher_cfg_offset = 0;\r\nICP_QAT_FW_COMN_CURR_ID_SET(cipher_cd_ctrl, ICP_QAT_FW_SLICE_CIPHER);\r\nICP_QAT_FW_COMN_NEXT_ID_SET(cipher_cd_ctrl, ICP_QAT_FW_SLICE_AUTH);\r\nhash_cd_ctrl->hash_cfg_offset = ((char *)hash - (char *)cipher) >> 3;\r\nhash_cd_ctrl->hash_flags = ICP_QAT_FW_AUTH_HDR_FLAG_NO_NESTED;\r\nhash_cd_ctrl->inner_res_sz = digestsize;\r\nhash_cd_ctrl->final_sz = digestsize;\r\nswitch (ctx->qat_hash_alg) {\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA1:\r\nhash_cd_ctrl->inner_state1_sz =\r\nround_up(ICP_QAT_HW_SHA1_STATE1_SZ, 8);\r\nhash_cd_ctrl->inner_state2_sz =\r\nround_up(ICP_QAT_HW_SHA1_STATE2_SZ, 8);\r\nbreak;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA256:\r\nhash_cd_ctrl->inner_state1_sz = ICP_QAT_HW_SHA256_STATE1_SZ;\r\nhash_cd_ctrl->inner_state2_sz = ICP_QAT_HW_SHA256_STATE2_SZ;\r\nbreak;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA512:\r\nhash_cd_ctrl->inner_state1_sz = ICP_QAT_HW_SHA512_STATE1_SZ;\r\nhash_cd_ctrl->inner_state2_sz = ICP_QAT_HW_SHA512_STATE2_SZ;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nhash_cd_ctrl->inner_state2_offset = hash_cd_ctrl->hash_cfg_offset +\r\n((sizeof(struct icp_qat_hw_auth_setup) +\r\nround_up(hash_cd_ctrl->inner_state1_sz, 8)) >> 3);\r\nICP_QAT_FW_COMN_CURR_ID_SET(hash_cd_ctrl, ICP_QAT_FW_SLICE_AUTH);\r\nICP_QAT_FW_COMN_NEXT_ID_SET(hash_cd_ctrl, ICP_QAT_FW_SLICE_DRAM_WR);\r\nreturn 0;\r\n}\r\nstatic int qat_alg_aead_init_dec_session(struct crypto_aead *aead_tfm,\r\nint alg,\r\nstruct crypto_authenc_keys *keys,\r\nint mode)\r\n{\r\nstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(aead_tfm);\r\nunsigned int digestsize = crypto_aead_authsize(aead_tfm);\r\nstruct qat_dec *dec_ctx = &ctx->dec_cd->qat_dec_cd;\r\nstruct icp_qat_hw_auth_algo_blk *hash = &dec_ctx->hash;\r\nstruct icp_qat_hw_cipher_algo_blk *cipher =\r\n(struct icp_qat_hw_cipher_algo_blk *)((char *)dec_ctx +\r\nsizeof(struct icp_qat_hw_auth_setup) +\r\nroundup(crypto_shash_digestsize(ctx->hash_tfm), 8) * 2);\r\nstruct icp_qat_fw_la_bulk_req *req_tmpl = &ctx->dec_fw_req;\r\nstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req_tmpl->cd_pars;\r\nstruct icp_qat_fw_comn_req_hdr *header = &req_tmpl->comn_hdr;\r\nvoid *ptr = &req_tmpl->cd_ctrl;\r\nstruct icp_qat_fw_cipher_cd_ctrl_hdr *cipher_cd_ctrl = ptr;\r\nstruct icp_qat_fw_auth_cd_ctrl_hdr *hash_cd_ctrl = ptr;\r\nstruct icp_qat_fw_la_auth_req_params *auth_param =\r\n(struct icp_qat_fw_la_auth_req_params *)\r\n((char *)&req_tmpl->serv_specif_rqpars +\r\nsizeof(struct icp_qat_fw_la_cipher_req_params));\r\ncipher->aes.cipher_config.val = QAT_AES_HW_CONFIG_DEC(alg, mode);\r\nmemcpy(cipher->aes.key, keys->enckey, keys->enckeylen);\r\nhash->sha.inner_setup.auth_config.config =\r\nICP_QAT_HW_AUTH_CONFIG_BUILD(ICP_QAT_HW_AUTH_MODE1,\r\nctx->qat_hash_alg,\r\ndigestsize);\r\nhash->sha.inner_setup.auth_counter.counter =\r\ncpu_to_be32(crypto_shash_blocksize(ctx->hash_tfm));\r\nif (qat_alg_do_precomputes(hash, ctx, keys->authkey, keys->authkeylen))\r\nreturn -EFAULT;\r\nqat_alg_init_common_hdr(header);\r\nheader->service_cmd_id = ICP_QAT_FW_LA_CMD_HASH_CIPHER;\r\nICP_QAT_FW_LA_DIGEST_IN_BUFFER_SET(header->serv_specif_flags,\r\nICP_QAT_FW_LA_DIGEST_IN_BUFFER);\r\nICP_QAT_FW_LA_RET_AUTH_SET(header->serv_specif_flags,\r\nICP_QAT_FW_LA_NO_RET_AUTH_RES);\r\nICP_QAT_FW_LA_CMP_AUTH_SET(header->serv_specif_flags,\r\nICP_QAT_FW_LA_CMP_AUTH_RES);\r\ncd_pars->u.s.content_desc_addr = ctx->dec_cd_paddr;\r\ncd_pars->u.s.content_desc_params_sz = sizeof(struct qat_alg_cd) >> 3;\r\ncipher_cd_ctrl->cipher_key_sz = keys->enckeylen >> 3;\r\ncipher_cd_ctrl->cipher_state_sz = AES_BLOCK_SIZE >> 3;\r\ncipher_cd_ctrl->cipher_cfg_offset =\r\n(sizeof(struct icp_qat_hw_auth_setup) +\r\nroundup(crypto_shash_digestsize(ctx->hash_tfm), 8) * 2) >> 3;\r\nICP_QAT_FW_COMN_CURR_ID_SET(cipher_cd_ctrl, ICP_QAT_FW_SLICE_CIPHER);\r\nICP_QAT_FW_COMN_NEXT_ID_SET(cipher_cd_ctrl, ICP_QAT_FW_SLICE_DRAM_WR);\r\nhash_cd_ctrl->hash_cfg_offset = 0;\r\nhash_cd_ctrl->hash_flags = ICP_QAT_FW_AUTH_HDR_FLAG_NO_NESTED;\r\nhash_cd_ctrl->inner_res_sz = digestsize;\r\nhash_cd_ctrl->final_sz = digestsize;\r\nswitch (ctx->qat_hash_alg) {\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA1:\r\nhash_cd_ctrl->inner_state1_sz =\r\nround_up(ICP_QAT_HW_SHA1_STATE1_SZ, 8);\r\nhash_cd_ctrl->inner_state2_sz =\r\nround_up(ICP_QAT_HW_SHA1_STATE2_SZ, 8);\r\nbreak;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA256:\r\nhash_cd_ctrl->inner_state1_sz = ICP_QAT_HW_SHA256_STATE1_SZ;\r\nhash_cd_ctrl->inner_state2_sz = ICP_QAT_HW_SHA256_STATE2_SZ;\r\nbreak;\r\ncase ICP_QAT_HW_AUTH_ALGO_SHA512:\r\nhash_cd_ctrl->inner_state1_sz = ICP_QAT_HW_SHA512_STATE1_SZ;\r\nhash_cd_ctrl->inner_state2_sz = ICP_QAT_HW_SHA512_STATE2_SZ;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nhash_cd_ctrl->inner_state2_offset = hash_cd_ctrl->hash_cfg_offset +\r\n((sizeof(struct icp_qat_hw_auth_setup) +\r\nround_up(hash_cd_ctrl->inner_state1_sz, 8)) >> 3);\r\nauth_param->auth_res_sz = digestsize;\r\nICP_QAT_FW_COMN_CURR_ID_SET(hash_cd_ctrl, ICP_QAT_FW_SLICE_AUTH);\r\nICP_QAT_FW_COMN_NEXT_ID_SET(hash_cd_ctrl, ICP_QAT_FW_SLICE_CIPHER);\r\nreturn 0;\r\n}\r\nstatic void qat_alg_ablkcipher_init_com(struct qat_alg_ablkcipher_ctx *ctx,\r\nstruct icp_qat_fw_la_bulk_req *req,\r\nstruct icp_qat_hw_cipher_algo_blk *cd,\r\nconst uint8_t *key, unsigned int keylen)\r\n{\r\nstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req->cd_pars;\r\nstruct icp_qat_fw_comn_req_hdr *header = &req->comn_hdr;\r\nstruct icp_qat_fw_cipher_cd_ctrl_hdr *cd_ctrl = (void *)&req->cd_ctrl;\r\nmemcpy(cd->aes.key, key, keylen);\r\nqat_alg_init_common_hdr(header);\r\nheader->service_cmd_id = ICP_QAT_FW_LA_CMD_CIPHER;\r\ncd_pars->u.s.content_desc_params_sz =\r\nsizeof(struct icp_qat_hw_cipher_algo_blk) >> 3;\r\ncd_ctrl->cipher_key_sz = keylen >> 3;\r\ncd_ctrl->cipher_state_sz = AES_BLOCK_SIZE >> 3;\r\ncd_ctrl->cipher_cfg_offset = 0;\r\nICP_QAT_FW_COMN_CURR_ID_SET(cd_ctrl, ICP_QAT_FW_SLICE_CIPHER);\r\nICP_QAT_FW_COMN_NEXT_ID_SET(cd_ctrl, ICP_QAT_FW_SLICE_DRAM_WR);\r\n}\r\nstatic void qat_alg_ablkcipher_init_enc(struct qat_alg_ablkcipher_ctx *ctx,\r\nint alg, const uint8_t *key,\r\nunsigned int keylen, int mode)\r\n{\r\nstruct icp_qat_hw_cipher_algo_blk *enc_cd = ctx->enc_cd;\r\nstruct icp_qat_fw_la_bulk_req *req = &ctx->enc_fw_req;\r\nstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req->cd_pars;\r\nqat_alg_ablkcipher_init_com(ctx, req, enc_cd, key, keylen);\r\ncd_pars->u.s.content_desc_addr = ctx->enc_cd_paddr;\r\nenc_cd->aes.cipher_config.val = QAT_AES_HW_CONFIG_ENC(alg, mode);\r\n}\r\nstatic void qat_alg_ablkcipher_init_dec(struct qat_alg_ablkcipher_ctx *ctx,\r\nint alg, const uint8_t *key,\r\nunsigned int keylen, int mode)\r\n{\r\nstruct icp_qat_hw_cipher_algo_blk *dec_cd = ctx->dec_cd;\r\nstruct icp_qat_fw_la_bulk_req *req = &ctx->dec_fw_req;\r\nstruct icp_qat_fw_comn_req_hdr_cd_pars *cd_pars = &req->cd_pars;\r\nqat_alg_ablkcipher_init_com(ctx, req, dec_cd, key, keylen);\r\ncd_pars->u.s.content_desc_addr = ctx->dec_cd_paddr;\r\nif (mode != ICP_QAT_HW_CIPHER_CTR_MODE)\r\ndec_cd->aes.cipher_config.val =\r\nQAT_AES_HW_CONFIG_DEC(alg, mode);\r\nelse\r\ndec_cd->aes.cipher_config.val =\r\nQAT_AES_HW_CONFIG_ENC(alg, mode);\r\n}\r\nstatic int qat_alg_validate_key(int key_len, int *alg, int mode)\r\n{\r\nif (mode != ICP_QAT_HW_CIPHER_XTS_MODE) {\r\nswitch (key_len) {\r\ncase AES_KEYSIZE_128:\r\n*alg = ICP_QAT_HW_CIPHER_ALGO_AES128;\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\n*alg = ICP_QAT_HW_CIPHER_ALGO_AES192;\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\n*alg = ICP_QAT_HW_CIPHER_ALGO_AES256;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nswitch (key_len) {\r\ncase AES_KEYSIZE_128 << 1:\r\n*alg = ICP_QAT_HW_CIPHER_ALGO_AES128;\r\nbreak;\r\ncase AES_KEYSIZE_256 << 1:\r\n*alg = ICP_QAT_HW_CIPHER_ALGO_AES256;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int qat_alg_aead_init_sessions(struct crypto_aead *tfm, const u8 *key,\r\nunsigned int keylen, int mode)\r\n{\r\nstruct crypto_authenc_keys keys;\r\nint alg;\r\nif (crypto_authenc_extractkeys(&keys, key, keylen))\r\ngoto bad_key;\r\nif (qat_alg_validate_key(keys.enckeylen, &alg, mode))\r\ngoto bad_key;\r\nif (qat_alg_aead_init_enc_session(tfm, alg, &keys, mode))\r\ngoto error;\r\nif (qat_alg_aead_init_dec_session(tfm, alg, &keys, mode))\r\ngoto error;\r\nreturn 0;\r\nbad_key:\r\ncrypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\nerror:\r\nreturn -EFAULT;\r\n}\r\nstatic int qat_alg_ablkcipher_init_sessions(struct qat_alg_ablkcipher_ctx *ctx,\r\nconst uint8_t *key,\r\nunsigned int keylen,\r\nint mode)\r\n{\r\nint alg;\r\nif (qat_alg_validate_key(keylen, &alg, mode))\r\ngoto bad_key;\r\nqat_alg_ablkcipher_init_enc(ctx, alg, key, keylen, mode);\r\nqat_alg_ablkcipher_init_dec(ctx, alg, key, keylen, mode);\r\nreturn 0;\r\nbad_key:\r\ncrypto_tfm_set_flags(ctx->tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nstatic int qat_alg_aead_setkey(struct crypto_aead *tfm, const uint8_t *key,\r\nunsigned int keylen)\r\n{\r\nstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(tfm);\r\nstruct device *dev;\r\nif (ctx->enc_cd) {\r\ndev = &GET_DEV(ctx->inst->accel_dev);\r\nmemset(ctx->enc_cd, 0, sizeof(*ctx->enc_cd));\r\nmemset(ctx->dec_cd, 0, sizeof(*ctx->dec_cd));\r\nmemset(&ctx->enc_fw_req, 0, sizeof(ctx->enc_fw_req));\r\nmemset(&ctx->dec_fw_req, 0, sizeof(ctx->dec_fw_req));\r\n} else {\r\nint node = get_current_node();\r\nstruct qat_crypto_instance *inst =\r\nqat_crypto_get_instance_node(node);\r\nif (!inst) {\r\nreturn -EINVAL;\r\n}\r\ndev = &GET_DEV(inst->accel_dev);\r\nctx->inst = inst;\r\nctx->enc_cd = dma_zalloc_coherent(dev, sizeof(*ctx->enc_cd),\r\n&ctx->enc_cd_paddr,\r\nGFP_ATOMIC);\r\nif (!ctx->enc_cd) {\r\nreturn -ENOMEM;\r\n}\r\nctx->dec_cd = dma_zalloc_coherent(dev, sizeof(*ctx->dec_cd),\r\n&ctx->dec_cd_paddr,\r\nGFP_ATOMIC);\r\nif (!ctx->dec_cd) {\r\ngoto out_free_enc;\r\n}\r\n}\r\nif (qat_alg_aead_init_sessions(tfm, key, keylen,\r\nICP_QAT_HW_CIPHER_CBC_MODE))\r\ngoto out_free_all;\r\nreturn 0;\r\nout_free_all:\r\nmemset(ctx->dec_cd, 0, sizeof(struct qat_alg_cd));\r\ndma_free_coherent(dev, sizeof(struct qat_alg_cd),\r\nctx->dec_cd, ctx->dec_cd_paddr);\r\nctx->dec_cd = NULL;\r\nout_free_enc:\r\nmemset(ctx->enc_cd, 0, sizeof(struct qat_alg_cd));\r\ndma_free_coherent(dev, sizeof(struct qat_alg_cd),\r\nctx->enc_cd, ctx->enc_cd_paddr);\r\nctx->enc_cd = NULL;\r\nreturn -ENOMEM;\r\n}\r\nstatic void qat_alg_free_bufl(struct qat_crypto_instance *inst,\r\nstruct qat_crypto_request *qat_req)\r\n{\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nstruct qat_alg_buf_list *bl = qat_req->buf.bl;\r\nstruct qat_alg_buf_list *blout = qat_req->buf.blout;\r\ndma_addr_t blp = qat_req->buf.blp;\r\ndma_addr_t blpout = qat_req->buf.bloutp;\r\nsize_t sz = qat_req->buf.sz;\r\nsize_t sz_out = qat_req->buf.sz_out;\r\nint i;\r\nfor (i = 0; i < bl->num_bufs; i++)\r\ndma_unmap_single(dev, bl->bufers[i].addr,\r\nbl->bufers[i].len, DMA_BIDIRECTIONAL);\r\ndma_unmap_single(dev, blp, sz, DMA_TO_DEVICE);\r\nkfree(bl);\r\nif (blp != blpout) {\r\nint bufless = blout->num_bufs - blout->num_mapped_bufs;\r\nfor (i = bufless; i < blout->num_bufs; i++) {\r\ndma_unmap_single(dev, blout->bufers[i].addr,\r\nblout->bufers[i].len,\r\nDMA_BIDIRECTIONAL);\r\n}\r\ndma_unmap_single(dev, blpout, sz_out, DMA_TO_DEVICE);\r\nkfree(blout);\r\n}\r\n}\r\nstatic int qat_alg_sgl_to_bufl(struct qat_crypto_instance *inst,\r\nstruct scatterlist *sgl,\r\nstruct scatterlist *sglout,\r\nstruct qat_crypto_request *qat_req)\r\n{\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nint i, sg_nctr = 0;\r\nint n = sg_nents(sgl);\r\nstruct qat_alg_buf_list *bufl;\r\nstruct qat_alg_buf_list *buflout = NULL;\r\ndma_addr_t blp;\r\ndma_addr_t bloutp = 0;\r\nstruct scatterlist *sg;\r\nsize_t sz_out, sz = sizeof(struct qat_alg_buf_list) +\r\n((1 + n) * sizeof(struct qat_alg_buf));\r\nif (unlikely(!n))\r\nreturn -EINVAL;\r\nbufl = kzalloc_node(sz, GFP_ATOMIC,\r\ndev_to_node(&GET_DEV(inst->accel_dev)));\r\nif (unlikely(!bufl))\r\nreturn -ENOMEM;\r\nblp = dma_map_single(dev, bufl, sz, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, blp)))\r\ngoto err_in;\r\nfor_each_sg(sgl, sg, n, i) {\r\nint y = sg_nctr;\r\nif (!sg->length)\r\ncontinue;\r\nbufl->bufers[y].addr = dma_map_single(dev, sg_virt(sg),\r\nsg->length,\r\nDMA_BIDIRECTIONAL);\r\nbufl->bufers[y].len = sg->length;\r\nif (unlikely(dma_mapping_error(dev, bufl->bufers[y].addr)))\r\ngoto err_in;\r\nsg_nctr++;\r\n}\r\nbufl->num_bufs = sg_nctr;\r\nqat_req->buf.bl = bufl;\r\nqat_req->buf.blp = blp;\r\nqat_req->buf.sz = sz;\r\nif (sgl != sglout) {\r\nstruct qat_alg_buf *bufers;\r\nn = sg_nents(sglout);\r\nsz_out = sizeof(struct qat_alg_buf_list) +\r\n((1 + n) * sizeof(struct qat_alg_buf));\r\nsg_nctr = 0;\r\nbuflout = kzalloc_node(sz_out, GFP_ATOMIC,\r\ndev_to_node(&GET_DEV(inst->accel_dev)));\r\nif (unlikely(!buflout))\r\ngoto err_in;\r\nbloutp = dma_map_single(dev, buflout, sz_out, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, bloutp)))\r\ngoto err_out;\r\nbufers = buflout->bufers;\r\nfor_each_sg(sglout, sg, n, i) {\r\nint y = sg_nctr;\r\nif (!sg->length)\r\ncontinue;\r\nbufers[y].addr = dma_map_single(dev, sg_virt(sg),\r\nsg->length,\r\nDMA_BIDIRECTIONAL);\r\nif (unlikely(dma_mapping_error(dev, bufers[y].addr)))\r\ngoto err_out;\r\nbufers[y].len = sg->length;\r\nsg_nctr++;\r\n}\r\nbuflout->num_bufs = sg_nctr;\r\nbuflout->num_mapped_bufs = sg_nctr;\r\nqat_req->buf.blout = buflout;\r\nqat_req->buf.bloutp = bloutp;\r\nqat_req->buf.sz_out = sz_out;\r\n} else {\r\nqat_req->buf.bloutp = qat_req->buf.blp;\r\nqat_req->buf.sz_out = 0;\r\n}\r\nreturn 0;\r\nerr_out:\r\nn = sg_nents(sglout);\r\nfor (i = 0; i < n; i++)\r\nif (!dma_mapping_error(dev, buflout->bufers[i].addr))\r\ndma_unmap_single(dev, buflout->bufers[i].addr,\r\nbuflout->bufers[i].len,\r\nDMA_BIDIRECTIONAL);\r\nif (!dma_mapping_error(dev, bloutp))\r\ndma_unmap_single(dev, bloutp, sz_out, DMA_TO_DEVICE);\r\nkfree(buflout);\r\nerr_in:\r\nn = sg_nents(sgl);\r\nfor (i = 0; i < n; i++)\r\nif (!dma_mapping_error(dev, bufl->bufers[i].addr))\r\ndma_unmap_single(dev, bufl->bufers[i].addr,\r\nbufl->bufers[i].len,\r\nDMA_BIDIRECTIONAL);\r\nif (!dma_mapping_error(dev, blp))\r\ndma_unmap_single(dev, blp, sz, DMA_TO_DEVICE);\r\nkfree(bufl);\r\ndev_err(dev, "Failed to map buf for dma\n");\r\nreturn -ENOMEM;\r\n}\r\nstatic void qat_aead_alg_callback(struct icp_qat_fw_la_resp *qat_resp,\r\nstruct qat_crypto_request *qat_req)\r\n{\r\nstruct qat_alg_aead_ctx *ctx = qat_req->aead_ctx;\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct aead_request *areq = qat_req->aead_req;\r\nuint8_t stat_filed = qat_resp->comn_resp.comn_status;\r\nint res = 0, qat_res = ICP_QAT_FW_COMN_RESP_CRYPTO_STAT_GET(stat_filed);\r\nqat_alg_free_bufl(inst, qat_req);\r\nif (unlikely(qat_res != ICP_QAT_FW_COMN_STATUS_FLAG_OK))\r\nres = -EBADMSG;\r\nareq->base.complete(&areq->base, res);\r\n}\r\nstatic void qat_ablkcipher_alg_callback(struct icp_qat_fw_la_resp *qat_resp,\r\nstruct qat_crypto_request *qat_req)\r\n{\r\nstruct qat_alg_ablkcipher_ctx *ctx = qat_req->ablkcipher_ctx;\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct ablkcipher_request *areq = qat_req->ablkcipher_req;\r\nuint8_t stat_filed = qat_resp->comn_resp.comn_status;\r\nint res = 0, qat_res = ICP_QAT_FW_COMN_RESP_CRYPTO_STAT_GET(stat_filed);\r\nqat_alg_free_bufl(inst, qat_req);\r\nif (unlikely(qat_res != ICP_QAT_FW_COMN_STATUS_FLAG_OK))\r\nres = -EINVAL;\r\nareq->base.complete(&areq->base, res);\r\n}\r\nvoid qat_alg_callback(void *resp)\r\n{\r\nstruct icp_qat_fw_la_resp *qat_resp = resp;\r\nstruct qat_crypto_request *qat_req =\r\n(void *)(__force long)qat_resp->opaque_data;\r\nqat_req->cb(qat_resp, qat_req);\r\n}\r\nstatic int qat_alg_aead_dec(struct aead_request *areq)\r\n{\r\nstruct crypto_aead *aead_tfm = crypto_aead_reqtfm(areq);\r\nstruct crypto_tfm *tfm = crypto_aead_tfm(aead_tfm);\r\nstruct qat_alg_aead_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct qat_crypto_request *qat_req = aead_request_ctx(areq);\r\nstruct icp_qat_fw_la_cipher_req_params *cipher_param;\r\nstruct icp_qat_fw_la_auth_req_params *auth_param;\r\nstruct icp_qat_fw_la_bulk_req *msg;\r\nint digst_size = crypto_aead_authsize(aead_tfm);\r\nint ret, ctr = 0;\r\nret = qat_alg_sgl_to_bufl(ctx->inst, areq->src, areq->dst, qat_req);\r\nif (unlikely(ret))\r\nreturn ret;\r\nmsg = &qat_req->req;\r\n*msg = ctx->dec_fw_req;\r\nqat_req->aead_ctx = ctx;\r\nqat_req->aead_req = areq;\r\nqat_req->cb = qat_aead_alg_callback;\r\nqat_req->req.comn_mid.opaque_data = (uint64_t)(__force long)qat_req;\r\nqat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;\r\nqat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;\r\ncipher_param = (void *)&qat_req->req.serv_specif_rqpars;\r\ncipher_param->cipher_length = areq->cryptlen - digst_size;\r\ncipher_param->cipher_offset = areq->assoclen;\r\nmemcpy(cipher_param->u.cipher_IV_array, areq->iv, AES_BLOCK_SIZE);\r\nauth_param = (void *)((uint8_t *)cipher_param + sizeof(*cipher_param));\r\nauth_param->auth_off = 0;\r\nauth_param->auth_len = areq->assoclen + cipher_param->cipher_length;\r\ndo {\r\nret = adf_send_message(ctx->inst->sym_tx, (uint32_t *)msg);\r\n} while (ret == -EAGAIN && ctr++ < 10);\r\nif (ret == -EAGAIN) {\r\nqat_alg_free_bufl(ctx->inst, qat_req);\r\nreturn -EBUSY;\r\n}\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int qat_alg_aead_enc(struct aead_request *areq)\r\n{\r\nstruct crypto_aead *aead_tfm = crypto_aead_reqtfm(areq);\r\nstruct crypto_tfm *tfm = crypto_aead_tfm(aead_tfm);\r\nstruct qat_alg_aead_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct qat_crypto_request *qat_req = aead_request_ctx(areq);\r\nstruct icp_qat_fw_la_cipher_req_params *cipher_param;\r\nstruct icp_qat_fw_la_auth_req_params *auth_param;\r\nstruct icp_qat_fw_la_bulk_req *msg;\r\nuint8_t *iv = areq->iv;\r\nint ret, ctr = 0;\r\nret = qat_alg_sgl_to_bufl(ctx->inst, areq->src, areq->dst, qat_req);\r\nif (unlikely(ret))\r\nreturn ret;\r\nmsg = &qat_req->req;\r\n*msg = ctx->enc_fw_req;\r\nqat_req->aead_ctx = ctx;\r\nqat_req->aead_req = areq;\r\nqat_req->cb = qat_aead_alg_callback;\r\nqat_req->req.comn_mid.opaque_data = (uint64_t)(__force long)qat_req;\r\nqat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;\r\nqat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;\r\ncipher_param = (void *)&qat_req->req.serv_specif_rqpars;\r\nauth_param = (void *)((uint8_t *)cipher_param + sizeof(*cipher_param));\r\nmemcpy(cipher_param->u.cipher_IV_array, iv, AES_BLOCK_SIZE);\r\ncipher_param->cipher_length = areq->cryptlen;\r\ncipher_param->cipher_offset = areq->assoclen;\r\nauth_param->auth_off = 0;\r\nauth_param->auth_len = areq->assoclen + areq->cryptlen;\r\ndo {\r\nret = adf_send_message(ctx->inst->sym_tx, (uint32_t *)msg);\r\n} while (ret == -EAGAIN && ctr++ < 10);\r\nif (ret == -EAGAIN) {\r\nqat_alg_free_bufl(ctx->inst, qat_req);\r\nreturn -EBUSY;\r\n}\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int qat_alg_ablkcipher_setkey(struct crypto_ablkcipher *tfm,\r\nconst u8 *key, unsigned int keylen,\r\nint mode)\r\n{\r\nstruct qat_alg_ablkcipher_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct device *dev;\r\nspin_lock(&ctx->lock);\r\nif (ctx->enc_cd) {\r\ndev = &GET_DEV(ctx->inst->accel_dev);\r\nmemset(ctx->enc_cd, 0, sizeof(*ctx->enc_cd));\r\nmemset(ctx->dec_cd, 0, sizeof(*ctx->dec_cd));\r\nmemset(&ctx->enc_fw_req, 0, sizeof(ctx->enc_fw_req));\r\nmemset(&ctx->dec_fw_req, 0, sizeof(ctx->dec_fw_req));\r\n} else {\r\nint node = get_current_node();\r\nstruct qat_crypto_instance *inst =\r\nqat_crypto_get_instance_node(node);\r\nif (!inst) {\r\nspin_unlock(&ctx->lock);\r\nreturn -EINVAL;\r\n}\r\ndev = &GET_DEV(inst->accel_dev);\r\nctx->inst = inst;\r\nctx->enc_cd = dma_zalloc_coherent(dev, sizeof(*ctx->enc_cd),\r\n&ctx->enc_cd_paddr,\r\nGFP_ATOMIC);\r\nif (!ctx->enc_cd) {\r\nspin_unlock(&ctx->lock);\r\nreturn -ENOMEM;\r\n}\r\nctx->dec_cd = dma_zalloc_coherent(dev, sizeof(*ctx->dec_cd),\r\n&ctx->dec_cd_paddr,\r\nGFP_ATOMIC);\r\nif (!ctx->dec_cd) {\r\nspin_unlock(&ctx->lock);\r\ngoto out_free_enc;\r\n}\r\n}\r\nspin_unlock(&ctx->lock);\r\nif (qat_alg_ablkcipher_init_sessions(ctx, key, keylen, mode))\r\ngoto out_free_all;\r\nreturn 0;\r\nout_free_all:\r\nmemset(ctx->dec_cd, 0, sizeof(*ctx->dec_cd));\r\ndma_free_coherent(dev, sizeof(*ctx->dec_cd),\r\nctx->dec_cd, ctx->dec_cd_paddr);\r\nctx->dec_cd = NULL;\r\nout_free_enc:\r\nmemset(ctx->enc_cd, 0, sizeof(*ctx->enc_cd));\r\ndma_free_coherent(dev, sizeof(*ctx->enc_cd),\r\nctx->enc_cd, ctx->enc_cd_paddr);\r\nctx->enc_cd = NULL;\r\nreturn -ENOMEM;\r\n}\r\nstatic int qat_alg_ablkcipher_cbc_setkey(struct crypto_ablkcipher *tfm,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nreturn qat_alg_ablkcipher_setkey(tfm, key, keylen,\r\nICP_QAT_HW_CIPHER_CBC_MODE);\r\n}\r\nstatic int qat_alg_ablkcipher_ctr_setkey(struct crypto_ablkcipher *tfm,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nreturn qat_alg_ablkcipher_setkey(tfm, key, keylen,\r\nICP_QAT_HW_CIPHER_CTR_MODE);\r\n}\r\nstatic int qat_alg_ablkcipher_xts_setkey(struct crypto_ablkcipher *tfm,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nreturn qat_alg_ablkcipher_setkey(tfm, key, keylen,\r\nICP_QAT_HW_CIPHER_XTS_MODE);\r\n}\r\nstatic int qat_alg_ablkcipher_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_ablkcipher *atfm = crypto_ablkcipher_reqtfm(req);\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(atfm);\r\nstruct qat_alg_ablkcipher_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct qat_crypto_request *qat_req = ablkcipher_request_ctx(req);\r\nstruct icp_qat_fw_la_cipher_req_params *cipher_param;\r\nstruct icp_qat_fw_la_bulk_req *msg;\r\nint ret, ctr = 0;\r\nret = qat_alg_sgl_to_bufl(ctx->inst, req->src, req->dst, qat_req);\r\nif (unlikely(ret))\r\nreturn ret;\r\nmsg = &qat_req->req;\r\n*msg = ctx->enc_fw_req;\r\nqat_req->ablkcipher_ctx = ctx;\r\nqat_req->ablkcipher_req = req;\r\nqat_req->cb = qat_ablkcipher_alg_callback;\r\nqat_req->req.comn_mid.opaque_data = (uint64_t)(__force long)qat_req;\r\nqat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;\r\nqat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;\r\ncipher_param = (void *)&qat_req->req.serv_specif_rqpars;\r\ncipher_param->cipher_length = req->nbytes;\r\ncipher_param->cipher_offset = 0;\r\nmemcpy(cipher_param->u.cipher_IV_array, req->info, AES_BLOCK_SIZE);\r\ndo {\r\nret = adf_send_message(ctx->inst->sym_tx, (uint32_t *)msg);\r\n} while (ret == -EAGAIN && ctr++ < 10);\r\nif (ret == -EAGAIN) {\r\nqat_alg_free_bufl(ctx->inst, qat_req);\r\nreturn -EBUSY;\r\n}\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int qat_alg_ablkcipher_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_ablkcipher *atfm = crypto_ablkcipher_reqtfm(req);\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(atfm);\r\nstruct qat_alg_ablkcipher_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct qat_crypto_request *qat_req = ablkcipher_request_ctx(req);\r\nstruct icp_qat_fw_la_cipher_req_params *cipher_param;\r\nstruct icp_qat_fw_la_bulk_req *msg;\r\nint ret, ctr = 0;\r\nret = qat_alg_sgl_to_bufl(ctx->inst, req->src, req->dst, qat_req);\r\nif (unlikely(ret))\r\nreturn ret;\r\nmsg = &qat_req->req;\r\n*msg = ctx->dec_fw_req;\r\nqat_req->ablkcipher_ctx = ctx;\r\nqat_req->ablkcipher_req = req;\r\nqat_req->cb = qat_ablkcipher_alg_callback;\r\nqat_req->req.comn_mid.opaque_data = (uint64_t)(__force long)qat_req;\r\nqat_req->req.comn_mid.src_data_addr = qat_req->buf.blp;\r\nqat_req->req.comn_mid.dest_data_addr = qat_req->buf.bloutp;\r\ncipher_param = (void *)&qat_req->req.serv_specif_rqpars;\r\ncipher_param->cipher_length = req->nbytes;\r\ncipher_param->cipher_offset = 0;\r\nmemcpy(cipher_param->u.cipher_IV_array, req->info, AES_BLOCK_SIZE);\r\ndo {\r\nret = adf_send_message(ctx->inst->sym_tx, (uint32_t *)msg);\r\n} while (ret == -EAGAIN && ctr++ < 10);\r\nif (ret == -EAGAIN) {\r\nqat_alg_free_bufl(ctx->inst, qat_req);\r\nreturn -EBUSY;\r\n}\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int qat_alg_aead_init(struct crypto_aead *tfm,\r\nenum icp_qat_hw_auth_algo hash,\r\nconst char *hash_name)\r\n{\r\nstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(tfm);\r\nctx->hash_tfm = crypto_alloc_shash(hash_name, 0, 0);\r\nif (IS_ERR(ctx->hash_tfm))\r\nreturn PTR_ERR(ctx->hash_tfm);\r\nctx->qat_hash_alg = hash;\r\ncrypto_aead_set_reqsize(tfm, sizeof(struct qat_crypto_request));\r\nreturn 0;\r\n}\r\nstatic int qat_alg_aead_sha1_init(struct crypto_aead *tfm)\r\n{\r\nreturn qat_alg_aead_init(tfm, ICP_QAT_HW_AUTH_ALGO_SHA1, "sha1");\r\n}\r\nstatic int qat_alg_aead_sha256_init(struct crypto_aead *tfm)\r\n{\r\nreturn qat_alg_aead_init(tfm, ICP_QAT_HW_AUTH_ALGO_SHA256, "sha256");\r\n}\r\nstatic int qat_alg_aead_sha512_init(struct crypto_aead *tfm)\r\n{\r\nreturn qat_alg_aead_init(tfm, ICP_QAT_HW_AUTH_ALGO_SHA512, "sha512");\r\n}\r\nstatic void qat_alg_aead_exit(struct crypto_aead *tfm)\r\n{\r\nstruct qat_alg_aead_ctx *ctx = crypto_aead_ctx(tfm);\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev;\r\ncrypto_free_shash(ctx->hash_tfm);\r\nif (!inst)\r\nreturn;\r\ndev = &GET_DEV(inst->accel_dev);\r\nif (ctx->enc_cd) {\r\nmemset(ctx->enc_cd, 0, sizeof(struct qat_alg_cd));\r\ndma_free_coherent(dev, sizeof(struct qat_alg_cd),\r\nctx->enc_cd, ctx->enc_cd_paddr);\r\n}\r\nif (ctx->dec_cd) {\r\nmemset(ctx->dec_cd, 0, sizeof(struct qat_alg_cd));\r\ndma_free_coherent(dev, sizeof(struct qat_alg_cd),\r\nctx->dec_cd, ctx->dec_cd_paddr);\r\n}\r\nqat_crypto_put_instance(inst);\r\n}\r\nstatic int qat_alg_ablkcipher_init(struct crypto_tfm *tfm)\r\n{\r\nstruct qat_alg_ablkcipher_ctx *ctx = crypto_tfm_ctx(tfm);\r\nspin_lock_init(&ctx->lock);\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct qat_crypto_request);\r\nctx->tfm = tfm;\r\nreturn 0;\r\n}\r\nstatic void qat_alg_ablkcipher_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct qat_alg_ablkcipher_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev;\r\nif (!inst)\r\nreturn;\r\ndev = &GET_DEV(inst->accel_dev);\r\nif (ctx->enc_cd) {\r\nmemset(ctx->enc_cd, 0,\r\nsizeof(struct icp_qat_hw_cipher_algo_blk));\r\ndma_free_coherent(dev,\r\nsizeof(struct icp_qat_hw_cipher_algo_blk),\r\nctx->enc_cd, ctx->enc_cd_paddr);\r\n}\r\nif (ctx->dec_cd) {\r\nmemset(ctx->dec_cd, 0,\r\nsizeof(struct icp_qat_hw_cipher_algo_blk));\r\ndma_free_coherent(dev,\r\nsizeof(struct icp_qat_hw_cipher_algo_blk),\r\nctx->dec_cd, ctx->dec_cd_paddr);\r\n}\r\nqat_crypto_put_instance(inst);\r\n}\r\nint qat_algs_register(void)\r\n{\r\nint ret = 0, i;\r\nmutex_lock(&algs_lock);\r\nif (++active_devs != 1)\r\ngoto unlock;\r\nfor (i = 0; i < ARRAY_SIZE(qat_algs); i++)\r\nqat_algs[i].cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC;\r\nret = crypto_register_algs(qat_algs, ARRAY_SIZE(qat_algs));\r\nif (ret)\r\ngoto unlock;\r\nfor (i = 0; i < ARRAY_SIZE(qat_aeads); i++)\r\nqat_aeads[i].base.cra_flags = CRYPTO_ALG_ASYNC;\r\nret = crypto_register_aeads(qat_aeads, ARRAY_SIZE(qat_aeads));\r\nif (ret)\r\ngoto unreg_algs;\r\nunlock:\r\nmutex_unlock(&algs_lock);\r\nreturn ret;\r\nunreg_algs:\r\ncrypto_unregister_algs(qat_algs, ARRAY_SIZE(qat_algs));\r\ngoto unlock;\r\n}\r\nvoid qat_algs_unregister(void)\r\n{\r\nmutex_lock(&algs_lock);\r\nif (--active_devs != 0)\r\ngoto unlock;\r\ncrypto_unregister_aeads(qat_aeads, ARRAY_SIZE(qat_aeads));\r\ncrypto_unregister_algs(qat_algs, ARRAY_SIZE(qat_algs));\r\nunlock:\r\nmutex_unlock(&algs_lock);\r\n}
