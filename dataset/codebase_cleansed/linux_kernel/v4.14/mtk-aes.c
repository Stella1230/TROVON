static inline u32 mtk_aes_read(struct mtk_cryp *cryp, u32 offset)\r\n{\r\nreturn readl_relaxed(cryp->base + offset);\r\n}\r\nstatic inline void mtk_aes_write(struct mtk_cryp *cryp,\r\nu32 offset, u32 value)\r\n{\r\nwritel_relaxed(value, cryp->base + offset);\r\n}\r\nstatic struct mtk_cryp *mtk_aes_find_dev(struct mtk_aes_base_ctx *ctx)\r\n{\r\nstruct mtk_cryp *cryp = NULL;\r\nstruct mtk_cryp *tmp;\r\nspin_lock_bh(&mtk_aes.lock);\r\nif (!ctx->cryp) {\r\nlist_for_each_entry(tmp, &mtk_aes.dev_list, aes_list) {\r\ncryp = tmp;\r\nbreak;\r\n}\r\nctx->cryp = cryp;\r\n} else {\r\ncryp = ctx->cryp;\r\n}\r\nspin_unlock_bh(&mtk_aes.lock);\r\nreturn cryp;\r\n}\r\nstatic inline size_t mtk_aes_padlen(size_t len)\r\n{\r\nlen &= AES_BLOCK_SIZE - 1;\r\nreturn len ? AES_BLOCK_SIZE - len : 0;\r\n}\r\nstatic bool mtk_aes_check_aligned(struct scatterlist *sg, size_t len,\r\nstruct mtk_aes_dma *dma)\r\n{\r\nint nents;\r\nif (!IS_ALIGNED(len, AES_BLOCK_SIZE))\r\nreturn false;\r\nfor (nents = 0; sg; sg = sg_next(sg), ++nents) {\r\nif (!IS_ALIGNED(sg->offset, sizeof(u32)))\r\nreturn false;\r\nif (len <= sg->length) {\r\nif (!IS_ALIGNED(len, AES_BLOCK_SIZE))\r\nreturn false;\r\ndma->nents = nents + 1;\r\ndma->remainder = sg->length - len;\r\nsg->length = len;\r\nreturn true;\r\n}\r\nif (!IS_ALIGNED(sg->length, AES_BLOCK_SIZE))\r\nreturn false;\r\nlen -= sg->length;\r\n}\r\nreturn false;\r\n}\r\nstatic inline void mtk_aes_set_mode(struct mtk_aes_rec *aes,\r\nconst struct mtk_aes_reqctx *rctx)\r\n{\r\naes->flags = (aes->flags & AES_FLAGS_BUSY) | rctx->mode;\r\n}\r\nstatic inline void mtk_aes_restore_sg(const struct mtk_aes_dma *dma)\r\n{\r\nstruct scatterlist *sg = dma->sg;\r\nint nents = dma->nents;\r\nif (!dma->remainder)\r\nreturn;\r\nwhile (--nents > 0 && sg)\r\nsg = sg_next(sg);\r\nif (!sg)\r\nreturn;\r\nsg->length += dma->remainder;\r\n}\r\nstatic inline void mtk_aes_write_state_le(__le32 *dst, const u32 *src, u32 size)\r\n{\r\nint i;\r\nfor (i = 0; i < SIZE_IN_WORDS(size); i++)\r\ndst[i] = cpu_to_le32(src[i]);\r\n}\r\nstatic inline void mtk_aes_write_state_be(__be32 *dst, const u32 *src, u32 size)\r\n{\r\nint i;\r\nfor (i = 0; i < SIZE_IN_WORDS(size); i++)\r\ndst[i] = cpu_to_be32(src[i]);\r\n}\r\nstatic inline int mtk_aes_complete(struct mtk_cryp *cryp,\r\nstruct mtk_aes_rec *aes,\r\nint err)\r\n{\r\naes->flags &= ~AES_FLAGS_BUSY;\r\naes->areq->complete(aes->areq, err);\r\ntasklet_schedule(&aes->queue_task);\r\nreturn err;\r\n}\r\nstatic int mtk_aes_xmit(struct mtk_cryp *cryp, struct mtk_aes_rec *aes)\r\n{\r\nstruct mtk_ring *ring = cryp->ring[aes->id];\r\nstruct mtk_desc *cmd = NULL, *res = NULL;\r\nstruct scatterlist *ssg = aes->src.sg, *dsg = aes->dst.sg;\r\nu32 slen = aes->src.sg_len, dlen = aes->dst.sg_len;\r\nint nents;\r\nfor (nents = 0; nents < slen; ++nents, ssg = sg_next(ssg)) {\r\ncmd = ring->cmd_next;\r\ncmd->hdr = MTK_DESC_BUF_LEN(ssg->length);\r\ncmd->buf = cpu_to_le32(sg_dma_address(ssg));\r\nif (nents == 0) {\r\ncmd->hdr |= MTK_DESC_FIRST |\r\nMTK_DESC_CT_LEN(aes->ctx->ct_size);\r\ncmd->ct = cpu_to_le32(aes->ctx->ct_dma);\r\ncmd->ct_hdr = aes->ctx->ct_hdr;\r\ncmd->tfm = cpu_to_le32(aes->ctx->tfm_dma);\r\n}\r\nif (++ring->cmd_next == ring->cmd_base + MTK_DESC_NUM)\r\nring->cmd_next = ring->cmd_base;\r\n}\r\ncmd->hdr |= MTK_DESC_LAST;\r\nfor (nents = 0; nents < dlen; ++nents, dsg = sg_next(dsg)) {\r\nres = ring->res_next;\r\nres->hdr = MTK_DESC_BUF_LEN(dsg->length);\r\nres->buf = cpu_to_le32(sg_dma_address(dsg));\r\nif (nents == 0)\r\nres->hdr |= MTK_DESC_FIRST;\r\nif (++ring->res_next == ring->res_base + MTK_DESC_NUM)\r\nring->res_next = ring->res_base;\r\n}\r\nres->hdr |= MTK_DESC_LAST;\r\nring->res_prev = res;\r\nif (aes->flags & AES_FLAGS_GCM)\r\nres->hdr += AES_BLOCK_SIZE;\r\nwmb();\r\nmtk_aes_write(cryp, RDR_PREP_COUNT(aes->id), MTK_DESC_CNT(dlen));\r\nmtk_aes_write(cryp, CDR_PREP_COUNT(aes->id), MTK_DESC_CNT(slen));\r\nreturn -EINPROGRESS;\r\n}\r\nstatic void mtk_aes_unmap(struct mtk_cryp *cryp, struct mtk_aes_rec *aes)\r\n{\r\nstruct mtk_aes_base_ctx *ctx = aes->ctx;\r\ndma_unmap_single(cryp->dev, ctx->ct_dma, sizeof(ctx->info),\r\nDMA_TO_DEVICE);\r\nif (aes->src.sg == aes->dst.sg) {\r\ndma_unmap_sg(cryp->dev, aes->src.sg, aes->src.nents,\r\nDMA_BIDIRECTIONAL);\r\nif (aes->src.sg != &aes->aligned_sg)\r\nmtk_aes_restore_sg(&aes->src);\r\n} else {\r\ndma_unmap_sg(cryp->dev, aes->dst.sg, aes->dst.nents,\r\nDMA_FROM_DEVICE);\r\nif (aes->dst.sg != &aes->aligned_sg)\r\nmtk_aes_restore_sg(&aes->dst);\r\ndma_unmap_sg(cryp->dev, aes->src.sg, aes->src.nents,\r\nDMA_TO_DEVICE);\r\nif (aes->src.sg != &aes->aligned_sg)\r\nmtk_aes_restore_sg(&aes->src);\r\n}\r\nif (aes->dst.sg == &aes->aligned_sg)\r\nsg_copy_from_buffer(aes->real_dst, sg_nents(aes->real_dst),\r\naes->buf, aes->total);\r\n}\r\nstatic int mtk_aes_map(struct mtk_cryp *cryp, struct mtk_aes_rec *aes)\r\n{\r\nstruct mtk_aes_base_ctx *ctx = aes->ctx;\r\nstruct mtk_aes_info *info = &ctx->info;\r\nctx->ct_dma = dma_map_single(cryp->dev, info, sizeof(*info),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(cryp->dev, ctx->ct_dma)))\r\ngoto exit;\r\nctx->tfm_dma = ctx->ct_dma + sizeof(info->cmd);\r\nif (aes->src.sg == aes->dst.sg) {\r\naes->src.sg_len = dma_map_sg(cryp->dev, aes->src.sg,\r\naes->src.nents,\r\nDMA_BIDIRECTIONAL);\r\naes->dst.sg_len = aes->src.sg_len;\r\nif (unlikely(!aes->src.sg_len))\r\ngoto sg_map_err;\r\n} else {\r\naes->src.sg_len = dma_map_sg(cryp->dev, aes->src.sg,\r\naes->src.nents, DMA_TO_DEVICE);\r\nif (unlikely(!aes->src.sg_len))\r\ngoto sg_map_err;\r\naes->dst.sg_len = dma_map_sg(cryp->dev, aes->dst.sg,\r\naes->dst.nents, DMA_FROM_DEVICE);\r\nif (unlikely(!aes->dst.sg_len)) {\r\ndma_unmap_sg(cryp->dev, aes->src.sg, aes->src.nents,\r\nDMA_TO_DEVICE);\r\ngoto sg_map_err;\r\n}\r\n}\r\nreturn mtk_aes_xmit(cryp, aes);\r\nsg_map_err:\r\ndma_unmap_single(cryp->dev, ctx->ct_dma, sizeof(*info), DMA_TO_DEVICE);\r\nexit:\r\nreturn mtk_aes_complete(cryp, aes, -EINVAL);\r\n}\r\nstatic void mtk_aes_info_init(struct mtk_cryp *cryp, struct mtk_aes_rec *aes,\r\nsize_t len)\r\n{\r\nstruct ablkcipher_request *req = ablkcipher_request_cast(aes->areq);\r\nstruct mtk_aes_base_ctx *ctx = aes->ctx;\r\nstruct mtk_aes_info *info = &ctx->info;\r\nu32 cnt = 0;\r\nctx->ct_hdr = AES_CT_CTRL_HDR | cpu_to_le32(len);\r\ninfo->cmd[cnt++] = AES_CMD0 | cpu_to_le32(len);\r\ninfo->cmd[cnt++] = AES_CMD1;\r\ninfo->tfm[0] = AES_TFM_SIZE(ctx->keylen) | ctx->keymode;\r\nif (aes->flags & AES_FLAGS_ENCRYPT)\r\ninfo->tfm[0] |= AES_TFM_BASIC_OUT;\r\nelse\r\ninfo->tfm[0] |= AES_TFM_BASIC_IN;\r\nswitch (aes->flags & AES_FLAGS_CIPHER_MSK) {\r\ncase AES_FLAGS_CBC:\r\ninfo->tfm[1] = AES_TFM_CBC;\r\nbreak;\r\ncase AES_FLAGS_ECB:\r\ninfo->tfm[1] = AES_TFM_ECB;\r\ngoto ecb;\r\ncase AES_FLAGS_CTR:\r\ninfo->tfm[1] = AES_TFM_CTR_LOAD;\r\ngoto ctr;\r\ndefault:\r\nreturn;\r\n}\r\nmtk_aes_write_state_le(info->state + ctx->keylen, req->info,\r\nAES_BLOCK_SIZE);\r\nctr:\r\ninfo->tfm[0] += AES_TFM_SIZE(SIZE_IN_WORDS(AES_BLOCK_SIZE));\r\ninfo->tfm[1] |= AES_TFM_FULL_IV;\r\ninfo->cmd[cnt++] = AES_CMD2;\r\necb:\r\nctx->ct_size = cnt;\r\n}\r\nstatic int mtk_aes_dma(struct mtk_cryp *cryp, struct mtk_aes_rec *aes,\r\nstruct scatterlist *src, struct scatterlist *dst,\r\nsize_t len)\r\n{\r\nsize_t padlen = 0;\r\nbool src_aligned, dst_aligned;\r\naes->total = len;\r\naes->src.sg = src;\r\naes->dst.sg = dst;\r\naes->real_dst = dst;\r\nsrc_aligned = mtk_aes_check_aligned(src, len, &aes->src);\r\nif (src == dst)\r\ndst_aligned = src_aligned;\r\nelse\r\ndst_aligned = mtk_aes_check_aligned(dst, len, &aes->dst);\r\nif (!src_aligned || !dst_aligned) {\r\npadlen = mtk_aes_padlen(len);\r\nif (len + padlen > AES_BUF_SIZE)\r\nreturn mtk_aes_complete(cryp, aes, -ENOMEM);\r\nif (!src_aligned) {\r\nsg_copy_to_buffer(src, sg_nents(src), aes->buf, len);\r\naes->src.sg = &aes->aligned_sg;\r\naes->src.nents = 1;\r\naes->src.remainder = 0;\r\n}\r\nif (!dst_aligned) {\r\naes->dst.sg = &aes->aligned_sg;\r\naes->dst.nents = 1;\r\naes->dst.remainder = 0;\r\n}\r\nsg_init_table(&aes->aligned_sg, 1);\r\nsg_set_buf(&aes->aligned_sg, aes->buf, len + padlen);\r\n}\r\nmtk_aes_info_init(cryp, aes, len + padlen);\r\nreturn mtk_aes_map(cryp, aes);\r\n}\r\nstatic int mtk_aes_handle_queue(struct mtk_cryp *cryp, u8 id,\r\nstruct crypto_async_request *new_areq)\r\n{\r\nstruct mtk_aes_rec *aes = cryp->aes[id];\r\nstruct crypto_async_request *areq, *backlog;\r\nstruct mtk_aes_base_ctx *ctx;\r\nunsigned long flags;\r\nint ret = 0;\r\nspin_lock_irqsave(&aes->lock, flags);\r\nif (new_areq)\r\nret = crypto_enqueue_request(&aes->queue, new_areq);\r\nif (aes->flags & AES_FLAGS_BUSY) {\r\nspin_unlock_irqrestore(&aes->lock, flags);\r\nreturn ret;\r\n}\r\nbacklog = crypto_get_backlog(&aes->queue);\r\nareq = crypto_dequeue_request(&aes->queue);\r\nif (areq)\r\naes->flags |= AES_FLAGS_BUSY;\r\nspin_unlock_irqrestore(&aes->lock, flags);\r\nif (!areq)\r\nreturn ret;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nctx = crypto_tfm_ctx(areq->tfm);\r\naes->areq = areq;\r\naes->ctx = ctx;\r\nreturn ctx->start(cryp, aes);\r\n}\r\nstatic int mtk_aes_transfer_complete(struct mtk_cryp *cryp,\r\nstruct mtk_aes_rec *aes)\r\n{\r\nreturn mtk_aes_complete(cryp, aes, 0);\r\n}\r\nstatic int mtk_aes_start(struct mtk_cryp *cryp, struct mtk_aes_rec *aes)\r\n{\r\nstruct ablkcipher_request *req = ablkcipher_request_cast(aes->areq);\r\nstruct mtk_aes_reqctx *rctx = ablkcipher_request_ctx(req);\r\nmtk_aes_set_mode(aes, rctx);\r\naes->resume = mtk_aes_transfer_complete;\r\nreturn mtk_aes_dma(cryp, aes, req->src, req->dst, req->nbytes);\r\n}\r\nstatic inline struct mtk_aes_ctr_ctx *\r\nmtk_aes_ctr_ctx_cast(struct mtk_aes_base_ctx *ctx)\r\n{\r\nreturn container_of(ctx, struct mtk_aes_ctr_ctx, base);\r\n}\r\nstatic int mtk_aes_ctr_transfer(struct mtk_cryp *cryp, struct mtk_aes_rec *aes)\r\n{\r\nstruct mtk_aes_base_ctx *ctx = aes->ctx;\r\nstruct mtk_aes_ctr_ctx *cctx = mtk_aes_ctr_ctx_cast(ctx);\r\nstruct ablkcipher_request *req = ablkcipher_request_cast(aes->areq);\r\nstruct scatterlist *src, *dst;\r\nu32 start, end, ctr, blocks;\r\nsize_t datalen;\r\nbool fragmented = false;\r\ncctx->offset += aes->total;\r\nif (cctx->offset >= req->nbytes)\r\nreturn mtk_aes_transfer_complete(cryp, aes);\r\ndatalen = req->nbytes - cctx->offset;\r\nblocks = DIV_ROUND_UP(datalen, AES_BLOCK_SIZE);\r\nctr = be32_to_cpu(cctx->iv[3]);\r\nstart = ctr;\r\nend = start + blocks - 1;\r\nif (end < start) {\r\nctr |= 0xffffffff;\r\ndatalen = AES_BLOCK_SIZE * -start;\r\nfragmented = true;\r\n}\r\nsrc = scatterwalk_ffwd(cctx->src, req->src, cctx->offset);\r\ndst = ((req->src == req->dst) ? src :\r\nscatterwalk_ffwd(cctx->dst, req->dst, cctx->offset));\r\nmtk_aes_write_state_le(ctx->info.state + ctx->keylen, cctx->iv,\r\nAES_BLOCK_SIZE);\r\nif (unlikely(fragmented)) {\r\ncctx->iv[3] = cpu_to_be32(ctr);\r\ncrypto_inc((u8 *)cctx->iv, AES_BLOCK_SIZE);\r\n}\r\nreturn mtk_aes_dma(cryp, aes, src, dst, datalen);\r\n}\r\nstatic int mtk_aes_ctr_start(struct mtk_cryp *cryp, struct mtk_aes_rec *aes)\r\n{\r\nstruct mtk_aes_ctr_ctx *cctx = mtk_aes_ctr_ctx_cast(aes->ctx);\r\nstruct ablkcipher_request *req = ablkcipher_request_cast(aes->areq);\r\nstruct mtk_aes_reqctx *rctx = ablkcipher_request_ctx(req);\r\nmtk_aes_set_mode(aes, rctx);\r\nmemcpy(cctx->iv, req->info, AES_BLOCK_SIZE);\r\ncctx->offset = 0;\r\naes->total = 0;\r\naes->resume = mtk_aes_ctr_transfer;\r\nreturn mtk_aes_ctr_transfer(cryp, aes);\r\n}\r\nstatic int mtk_aes_setkey(struct crypto_ablkcipher *tfm,\r\nconst u8 *key, u32 keylen)\r\n{\r\nstruct mtk_aes_base_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nswitch (keylen) {\r\ncase AES_KEYSIZE_128:\r\nctx->keymode = AES_TFM_128BITS;\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nctx->keymode = AES_TFM_192BITS;\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nctx->keymode = AES_TFM_256BITS;\r\nbreak;\r\ndefault:\r\ncrypto_ablkcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nctx->keylen = SIZE_IN_WORDS(keylen);\r\nmtk_aes_write_state_le(ctx->info.state, (const u32 *)key, keylen);\r\nreturn 0;\r\n}\r\nstatic int mtk_aes_crypt(struct ablkcipher_request *req, u64 mode)\r\n{\r\nstruct mtk_aes_base_ctx *ctx;\r\nstruct mtk_aes_reqctx *rctx;\r\nctx = crypto_ablkcipher_ctx(crypto_ablkcipher_reqtfm(req));\r\nrctx = ablkcipher_request_ctx(req);\r\nrctx->mode = mode;\r\nreturn mtk_aes_handle_queue(ctx->cryp, !(mode & AES_FLAGS_ENCRYPT),\r\n&req->base);\r\n}\r\nstatic int mtk_aes_ecb_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn mtk_aes_crypt(req, AES_FLAGS_ENCRYPT | AES_FLAGS_ECB);\r\n}\r\nstatic int mtk_aes_ecb_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn mtk_aes_crypt(req, AES_FLAGS_ECB);\r\n}\r\nstatic int mtk_aes_cbc_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn mtk_aes_crypt(req, AES_FLAGS_ENCRYPT | AES_FLAGS_CBC);\r\n}\r\nstatic int mtk_aes_cbc_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn mtk_aes_crypt(req, AES_FLAGS_CBC);\r\n}\r\nstatic int mtk_aes_ctr_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn mtk_aes_crypt(req, AES_FLAGS_ENCRYPT | AES_FLAGS_CTR);\r\n}\r\nstatic int mtk_aes_ctr_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn mtk_aes_crypt(req, AES_FLAGS_CTR);\r\n}\r\nstatic int mtk_aes_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct mtk_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct mtk_cryp *cryp = NULL;\r\ncryp = mtk_aes_find_dev(&ctx->base);\r\nif (!cryp) {\r\npr_err("can't find crypto device\n");\r\nreturn -ENODEV;\r\n}\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct mtk_aes_reqctx);\r\nctx->base.start = mtk_aes_start;\r\nreturn 0;\r\n}\r\nstatic int mtk_aes_ctr_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct mtk_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct mtk_cryp *cryp = NULL;\r\ncryp = mtk_aes_find_dev(&ctx->base);\r\nif (!cryp) {\r\npr_err("can't find crypto device\n");\r\nreturn -ENODEV;\r\n}\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct mtk_aes_reqctx);\r\nctx->base.start = mtk_aes_ctr_start;\r\nreturn 0;\r\n}\r\nstatic inline struct mtk_aes_gcm_ctx *\r\nmtk_aes_gcm_ctx_cast(struct mtk_aes_base_ctx *ctx)\r\n{\r\nreturn container_of(ctx, struct mtk_aes_gcm_ctx, base);\r\n}\r\nstatic int mtk_aes_gcm_tag_verify(struct mtk_cryp *cryp,\r\nstruct mtk_aes_rec *aes)\r\n{\r\nu32 status = cryp->ring[aes->id]->res_prev->ct;\r\nreturn mtk_aes_complete(cryp, aes, (status & AES_AUTH_TAG_ERR) ?\r\n-EBADMSG : 0);\r\n}\r\nstatic void mtk_aes_gcm_info_init(struct mtk_cryp *cryp,\r\nstruct mtk_aes_rec *aes,\r\nsize_t len)\r\n{\r\nstruct aead_request *req = aead_request_cast(aes->areq);\r\nstruct mtk_aes_base_ctx *ctx = aes->ctx;\r\nstruct mtk_aes_gcm_ctx *gctx = mtk_aes_gcm_ctx_cast(ctx);\r\nstruct mtk_aes_info *info = &ctx->info;\r\nu32 ivsize = crypto_aead_ivsize(crypto_aead_reqtfm(req));\r\nu32 cnt = 0;\r\nctx->ct_hdr = AES_CT_CTRL_HDR | len;\r\ninfo->cmd[cnt++] = AES_GCM_CMD0 | cpu_to_le32(req->assoclen);\r\ninfo->cmd[cnt++] = AES_GCM_CMD1 | cpu_to_le32(req->assoclen);\r\ninfo->cmd[cnt++] = AES_GCM_CMD2;\r\ninfo->cmd[cnt++] = AES_GCM_CMD3 | cpu_to_le32(gctx->textlen);\r\nif (aes->flags & AES_FLAGS_ENCRYPT) {\r\ninfo->cmd[cnt++] = AES_GCM_CMD4 | cpu_to_le32(gctx->authsize);\r\ninfo->tfm[0] = AES_TFM_GCM_OUT;\r\n} else {\r\ninfo->cmd[cnt++] = AES_GCM_CMD5 | cpu_to_le32(gctx->authsize);\r\ninfo->cmd[cnt++] = AES_GCM_CMD6 | cpu_to_le32(gctx->authsize);\r\ninfo->tfm[0] = AES_TFM_GCM_IN;\r\n}\r\nctx->ct_size = cnt;\r\ninfo->tfm[0] |= AES_TFM_GHASH_DIGEST | AES_TFM_GHASH | AES_TFM_SIZE(\r\nctx->keylen + SIZE_IN_WORDS(AES_BLOCK_SIZE + ivsize)) |\r\nctx->keymode;\r\ninfo->tfm[1] = AES_TFM_CTR_INIT | AES_TFM_IV_CTR_MODE | AES_TFM_3IV |\r\nAES_TFM_ENC_HASH;\r\nmtk_aes_write_state_le(info->state + ctx->keylen + SIZE_IN_WORDS(\r\nAES_BLOCK_SIZE), (const u32 *)req->iv, ivsize);\r\n}\r\nstatic int mtk_aes_gcm_dma(struct mtk_cryp *cryp, struct mtk_aes_rec *aes,\r\nstruct scatterlist *src, struct scatterlist *dst,\r\nsize_t len)\r\n{\r\nbool src_aligned, dst_aligned;\r\naes->src.sg = src;\r\naes->dst.sg = dst;\r\naes->real_dst = dst;\r\nsrc_aligned = mtk_aes_check_aligned(src, len, &aes->src);\r\nif (src == dst)\r\ndst_aligned = src_aligned;\r\nelse\r\ndst_aligned = mtk_aes_check_aligned(dst, len, &aes->dst);\r\nif (!src_aligned || !dst_aligned) {\r\nif (aes->total > AES_BUF_SIZE)\r\nreturn mtk_aes_complete(cryp, aes, -ENOMEM);\r\nif (!src_aligned) {\r\nsg_copy_to_buffer(src, sg_nents(src), aes->buf, len);\r\naes->src.sg = &aes->aligned_sg;\r\naes->src.nents = 1;\r\naes->src.remainder = 0;\r\n}\r\nif (!dst_aligned) {\r\naes->dst.sg = &aes->aligned_sg;\r\naes->dst.nents = 1;\r\naes->dst.remainder = 0;\r\n}\r\nsg_init_table(&aes->aligned_sg, 1);\r\nsg_set_buf(&aes->aligned_sg, aes->buf, aes->total);\r\n}\r\nmtk_aes_gcm_info_init(cryp, aes, len);\r\nreturn mtk_aes_map(cryp, aes);\r\n}\r\nstatic int mtk_aes_gcm_start(struct mtk_cryp *cryp, struct mtk_aes_rec *aes)\r\n{\r\nstruct mtk_aes_gcm_ctx *gctx = mtk_aes_gcm_ctx_cast(aes->ctx);\r\nstruct aead_request *req = aead_request_cast(aes->areq);\r\nstruct mtk_aes_reqctx *rctx = aead_request_ctx(req);\r\nu32 len = req->assoclen + req->cryptlen;\r\nmtk_aes_set_mode(aes, rctx);\r\nif (aes->flags & AES_FLAGS_ENCRYPT) {\r\nu32 tag[4];\r\naes->resume = mtk_aes_transfer_complete;\r\naes->total = len + gctx->authsize;\r\ngctx->textlen = req->cryptlen;\r\nscatterwalk_map_and_copy(tag, req->dst, len, gctx->authsize, 1);\r\n} else {\r\naes->resume = mtk_aes_gcm_tag_verify;\r\naes->total = len;\r\ngctx->textlen = req->cryptlen - gctx->authsize;\r\n}\r\nreturn mtk_aes_gcm_dma(cryp, aes, req->src, req->dst, len);\r\n}\r\nstatic int mtk_aes_gcm_crypt(struct aead_request *req, u64 mode)\r\n{\r\nstruct mtk_aes_base_ctx *ctx = crypto_aead_ctx(crypto_aead_reqtfm(req));\r\nstruct mtk_aes_reqctx *rctx = aead_request_ctx(req);\r\nrctx->mode = AES_FLAGS_GCM | mode;\r\nreturn mtk_aes_handle_queue(ctx->cryp, !!(mode & AES_FLAGS_ENCRYPT),\r\n&req->base);\r\n}\r\nstatic void mtk_gcm_setkey_done(struct crypto_async_request *req, int err)\r\n{\r\nstruct mtk_aes_gcm_setkey_result *result = req->data;\r\nif (err == -EINPROGRESS)\r\nreturn;\r\nresult->err = err;\r\ncomplete(&result->completion);\r\n}\r\nstatic int mtk_aes_gcm_setkey(struct crypto_aead *aead, const u8 *key,\r\nu32 keylen)\r\n{\r\nstruct mtk_aes_base_ctx *ctx = crypto_aead_ctx(aead);\r\nstruct mtk_aes_gcm_ctx *gctx = mtk_aes_gcm_ctx_cast(ctx);\r\nstruct crypto_skcipher *ctr = gctx->ctr;\r\nstruct {\r\nu32 hash[4];\r\nu8 iv[8];\r\nstruct mtk_aes_gcm_setkey_result result;\r\nstruct scatterlist sg[1];\r\nstruct skcipher_request req;\r\n} *data;\r\nint err;\r\nswitch (keylen) {\r\ncase AES_KEYSIZE_128:\r\nctx->keymode = AES_TFM_128BITS;\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nctx->keymode = AES_TFM_192BITS;\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nctx->keymode = AES_TFM_256BITS;\r\nbreak;\r\ndefault:\r\ncrypto_aead_set_flags(aead, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nctx->keylen = SIZE_IN_WORDS(keylen);\r\ncrypto_skcipher_clear_flags(ctr, CRYPTO_TFM_REQ_MASK);\r\ncrypto_skcipher_set_flags(ctr, crypto_aead_get_flags(aead) &\r\nCRYPTO_TFM_REQ_MASK);\r\nerr = crypto_skcipher_setkey(ctr, key, keylen);\r\ncrypto_aead_set_flags(aead, crypto_skcipher_get_flags(ctr) &\r\nCRYPTO_TFM_RES_MASK);\r\nif (err)\r\nreturn err;\r\ndata = kzalloc(sizeof(*data) + crypto_skcipher_reqsize(ctr),\r\nGFP_KERNEL);\r\nif (!data)\r\nreturn -ENOMEM;\r\ninit_completion(&data->result.completion);\r\nsg_init_one(data->sg, &data->hash, AES_BLOCK_SIZE);\r\nskcipher_request_set_tfm(&data->req, ctr);\r\nskcipher_request_set_callback(&data->req, CRYPTO_TFM_REQ_MAY_SLEEP |\r\nCRYPTO_TFM_REQ_MAY_BACKLOG,\r\nmtk_gcm_setkey_done, &data->result);\r\nskcipher_request_set_crypt(&data->req, data->sg, data->sg,\r\nAES_BLOCK_SIZE, data->iv);\r\nerr = crypto_skcipher_encrypt(&data->req);\r\nif (err == -EINPROGRESS || err == -EBUSY) {\r\nerr = wait_for_completion_interruptible(\r\n&data->result.completion);\r\nif (!err)\r\nerr = data->result.err;\r\n}\r\nif (err)\r\ngoto out;\r\nmtk_aes_write_state_le(ctx->info.state, (const u32 *)key, keylen);\r\nmtk_aes_write_state_be(ctx->info.state + ctx->keylen, data->hash,\r\nAES_BLOCK_SIZE);\r\nout:\r\nkzfree(data);\r\nreturn err;\r\n}\r\nstatic int mtk_aes_gcm_setauthsize(struct crypto_aead *aead,\r\nu32 authsize)\r\n{\r\nstruct mtk_aes_base_ctx *ctx = crypto_aead_ctx(aead);\r\nstruct mtk_aes_gcm_ctx *gctx = mtk_aes_gcm_ctx_cast(ctx);\r\nswitch (authsize) {\r\ncase 8:\r\ncase 12:\r\ncase 16:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ngctx->authsize = authsize;\r\nreturn 0;\r\n}\r\nstatic int mtk_aes_gcm_encrypt(struct aead_request *req)\r\n{\r\nreturn mtk_aes_gcm_crypt(req, AES_FLAGS_ENCRYPT);\r\n}\r\nstatic int mtk_aes_gcm_decrypt(struct aead_request *req)\r\n{\r\nreturn mtk_aes_gcm_crypt(req, 0);\r\n}\r\nstatic int mtk_aes_gcm_init(struct crypto_aead *aead)\r\n{\r\nstruct mtk_aes_gcm_ctx *ctx = crypto_aead_ctx(aead);\r\nstruct mtk_cryp *cryp = NULL;\r\ncryp = mtk_aes_find_dev(&ctx->base);\r\nif (!cryp) {\r\npr_err("can't find crypto device\n");\r\nreturn -ENODEV;\r\n}\r\nctx->ctr = crypto_alloc_skcipher("ctr(aes)", 0,\r\nCRYPTO_ALG_ASYNC);\r\nif (IS_ERR(ctx->ctr)) {\r\npr_err("Error allocating ctr(aes)\n");\r\nreturn PTR_ERR(ctx->ctr);\r\n}\r\ncrypto_aead_set_reqsize(aead, sizeof(struct mtk_aes_reqctx));\r\nctx->base.start = mtk_aes_gcm_start;\r\nreturn 0;\r\n}\r\nstatic void mtk_aes_gcm_exit(struct crypto_aead *aead)\r\n{\r\nstruct mtk_aes_gcm_ctx *ctx = crypto_aead_ctx(aead);\r\ncrypto_free_skcipher(ctx->ctr);\r\n}\r\nstatic void mtk_aes_queue_task(unsigned long data)\r\n{\r\nstruct mtk_aes_rec *aes = (struct mtk_aes_rec *)data;\r\nmtk_aes_handle_queue(aes->cryp, aes->id, NULL);\r\n}\r\nstatic void mtk_aes_done_task(unsigned long data)\r\n{\r\nstruct mtk_aes_rec *aes = (struct mtk_aes_rec *)data;\r\nstruct mtk_cryp *cryp = aes->cryp;\r\nmtk_aes_unmap(cryp, aes);\r\naes->resume(cryp, aes);\r\n}\r\nstatic irqreturn_t mtk_aes_irq(int irq, void *dev_id)\r\n{\r\nstruct mtk_aes_rec *aes = (struct mtk_aes_rec *)dev_id;\r\nstruct mtk_cryp *cryp = aes->cryp;\r\nu32 val = mtk_aes_read(cryp, RDR_STAT(aes->id));\r\nmtk_aes_write(cryp, RDR_STAT(aes->id), val);\r\nif (likely(AES_FLAGS_BUSY & aes->flags)) {\r\nmtk_aes_write(cryp, RDR_PROC_COUNT(aes->id), MTK_CNT_RST);\r\nmtk_aes_write(cryp, RDR_THRESH(aes->id),\r\nMTK_RDR_PROC_THRESH | MTK_RDR_PROC_MODE);\r\ntasklet_schedule(&aes->done_task);\r\n} else {\r\ndev_warn(cryp->dev, "AES interrupt when no active requests.\n");\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int mtk_aes_record_init(struct mtk_cryp *cryp)\r\n{\r\nstruct mtk_aes_rec **aes = cryp->aes;\r\nint i, err = -ENOMEM;\r\nfor (i = 0; i < MTK_REC_NUM; i++) {\r\naes[i] = kzalloc(sizeof(**aes), GFP_KERNEL);\r\nif (!aes[i])\r\ngoto err_cleanup;\r\naes[i]->buf = (void *)__get_free_pages(GFP_KERNEL,\r\nAES_BUF_ORDER);\r\nif (!aes[i]->buf)\r\ngoto err_cleanup;\r\naes[i]->cryp = cryp;\r\nspin_lock_init(&aes[i]->lock);\r\ncrypto_init_queue(&aes[i]->queue, AES_QUEUE_SIZE);\r\ntasklet_init(&aes[i]->queue_task, mtk_aes_queue_task,\r\n(unsigned long)aes[i]);\r\ntasklet_init(&aes[i]->done_task, mtk_aes_done_task,\r\n(unsigned long)aes[i]);\r\n}\r\naes[0]->id = MTK_RING0;\r\naes[1]->id = MTK_RING1;\r\nreturn 0;\r\nerr_cleanup:\r\nfor (; i--; ) {\r\nfree_page((unsigned long)aes[i]->buf);\r\nkfree(aes[i]);\r\n}\r\nreturn err;\r\n}\r\nstatic void mtk_aes_record_free(struct mtk_cryp *cryp)\r\n{\r\nint i;\r\nfor (i = 0; i < MTK_REC_NUM; i++) {\r\ntasklet_kill(&cryp->aes[i]->done_task);\r\ntasklet_kill(&cryp->aes[i]->queue_task);\r\nfree_page((unsigned long)cryp->aes[i]->buf);\r\nkfree(cryp->aes[i]);\r\n}\r\n}\r\nstatic void mtk_aes_unregister_algs(void)\r\n{\r\nint i;\r\ncrypto_unregister_aead(&aes_gcm_alg);\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++)\r\ncrypto_unregister_alg(&aes_algs[i]);\r\n}\r\nstatic int mtk_aes_register_algs(void)\r\n{\r\nint err, i;\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++) {\r\nerr = crypto_register_alg(&aes_algs[i]);\r\nif (err)\r\ngoto err_aes_algs;\r\n}\r\nerr = crypto_register_aead(&aes_gcm_alg);\r\nif (err)\r\ngoto err_aes_algs;\r\nreturn 0;\r\nerr_aes_algs:\r\nfor (; i--; )\r\ncrypto_unregister_alg(&aes_algs[i]);\r\nreturn err;\r\n}\r\nint mtk_cipher_alg_register(struct mtk_cryp *cryp)\r\n{\r\nint ret;\r\nINIT_LIST_HEAD(&cryp->aes_list);\r\nret = mtk_aes_record_init(cryp);\r\nif (ret)\r\ngoto err_record;\r\nret = devm_request_irq(cryp->dev, cryp->irq[MTK_RING0], mtk_aes_irq,\r\n0, "mtk-aes", cryp->aes[0]);\r\nif (ret) {\r\ndev_err(cryp->dev, "unable to request AES irq.\n");\r\ngoto err_res;\r\n}\r\nret = devm_request_irq(cryp->dev, cryp->irq[MTK_RING1], mtk_aes_irq,\r\n0, "mtk-aes", cryp->aes[1]);\r\nif (ret) {\r\ndev_err(cryp->dev, "unable to request AES irq.\n");\r\ngoto err_res;\r\n}\r\nmtk_aes_write(cryp, AIC_ENABLE_SET(MTK_RING0), MTK_IRQ_RDR0);\r\nmtk_aes_write(cryp, AIC_ENABLE_SET(MTK_RING1), MTK_IRQ_RDR1);\r\nspin_lock(&mtk_aes.lock);\r\nlist_add_tail(&cryp->aes_list, &mtk_aes.dev_list);\r\nspin_unlock(&mtk_aes.lock);\r\nret = mtk_aes_register_algs();\r\nif (ret)\r\ngoto err_algs;\r\nreturn 0;\r\nerr_algs:\r\nspin_lock(&mtk_aes.lock);\r\nlist_del(&cryp->aes_list);\r\nspin_unlock(&mtk_aes.lock);\r\nerr_res:\r\nmtk_aes_record_free(cryp);\r\nerr_record:\r\ndev_err(cryp->dev, "mtk-aes initialization failed.\n");\r\nreturn ret;\r\n}\r\nvoid mtk_cipher_alg_release(struct mtk_cryp *cryp)\r\n{\r\nspin_lock(&mtk_aes.lock);\r\nlist_del(&cryp->aes_list);\r\nspin_unlock(&mtk_aes.lock);\r\nmtk_aes_unregister_algs();\r\nmtk_aes_record_free(cryp);\r\n}
