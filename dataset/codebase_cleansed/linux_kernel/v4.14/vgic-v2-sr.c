static void __hyp_text save_elrsr(struct kvm_vcpu *vcpu, void __iomem *base)\r\n{\r\nstruct vgic_v2_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v2;\r\nint nr_lr = (kern_hyp_va(&kvm_vgic_global_state))->nr_lr;\r\nu32 elrsr0, elrsr1;\r\nelrsr0 = readl_relaxed(base + GICH_ELRSR0);\r\nif (unlikely(nr_lr > 32))\r\nelrsr1 = readl_relaxed(base + GICH_ELRSR1);\r\nelse\r\nelrsr1 = 0;\r\n#ifdef CONFIG_CPU_BIG_ENDIAN\r\ncpu_if->vgic_elrsr = ((u64)elrsr0 << 32) | elrsr1;\r\n#else\r\ncpu_if->vgic_elrsr = ((u64)elrsr1 << 32) | elrsr0;\r\n#endif\r\n}\r\nstatic void __hyp_text save_lrs(struct kvm_vcpu *vcpu, void __iomem *base)\r\n{\r\nstruct vgic_v2_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v2;\r\nint i;\r\nu64 used_lrs = vcpu->arch.vgic_cpu.used_lrs;\r\nfor (i = 0; i < used_lrs; i++) {\r\nif (cpu_if->vgic_elrsr & (1UL << i))\r\ncpu_if->vgic_lr[i] &= ~GICH_LR_STATE;\r\nelse\r\ncpu_if->vgic_lr[i] = readl_relaxed(base + GICH_LR0 + (i * 4));\r\nwritel_relaxed(0, base + GICH_LR0 + (i * 4));\r\n}\r\n}\r\nvoid __hyp_text __vgic_v2_save_state(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm *kvm = kern_hyp_va(vcpu->kvm);\r\nstruct vgic_v2_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v2;\r\nstruct vgic_dist *vgic = &kvm->arch.vgic;\r\nvoid __iomem *base = kern_hyp_va(vgic->vctrl_base);\r\nu64 used_lrs = vcpu->arch.vgic_cpu.used_lrs;\r\nif (!base)\r\nreturn;\r\nif (used_lrs) {\r\ncpu_if->vgic_apr = readl_relaxed(base + GICH_APR);\r\nsave_elrsr(vcpu, base);\r\nsave_lrs(vcpu, base);\r\nwritel_relaxed(0, base + GICH_HCR);\r\n} else {\r\ncpu_if->vgic_elrsr = ~0UL;\r\ncpu_if->vgic_apr = 0;\r\n}\r\n}\r\nvoid __hyp_text __vgic_v2_restore_state(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm *kvm = kern_hyp_va(vcpu->kvm);\r\nstruct vgic_v2_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v2;\r\nstruct vgic_dist *vgic = &kvm->arch.vgic;\r\nvoid __iomem *base = kern_hyp_va(vgic->vctrl_base);\r\nint i;\r\nu64 used_lrs = vcpu->arch.vgic_cpu.used_lrs;\r\nif (!base)\r\nreturn;\r\nif (used_lrs) {\r\nwritel_relaxed(cpu_if->vgic_hcr, base + GICH_HCR);\r\nwritel_relaxed(cpu_if->vgic_apr, base + GICH_APR);\r\nfor (i = 0; i < used_lrs; i++) {\r\nwritel_relaxed(cpu_if->vgic_lr[i],\r\nbase + GICH_LR0 + (i * 4));\r\n}\r\n}\r\n}\r\nint __hyp_text __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm *kvm = kern_hyp_va(vcpu->kvm);\r\nstruct vgic_dist *vgic = &kvm->arch.vgic;\r\nphys_addr_t fault_ipa;\r\nvoid __iomem *addr;\r\nint rd;\r\nfault_ipa = kvm_vcpu_get_fault_ipa(vcpu);\r\nfault_ipa |= kvm_vcpu_get_hfar(vcpu) & GENMASK(11, 0);\r\nif (fault_ipa < vgic->vgic_cpu_base ||\r\nfault_ipa >= (vgic->vgic_cpu_base + KVM_VGIC_V2_CPU_SIZE))\r\nreturn 0;\r\nif (kvm_vcpu_dabt_get_as(vcpu) != sizeof(u32))\r\nreturn -1;\r\nif (fault_ipa & 3)\r\nreturn -1;\r\nrd = kvm_vcpu_dabt_get_rd(vcpu);\r\naddr = kern_hyp_va((kern_hyp_va(&kvm_vgic_global_state))->vcpu_base_va);\r\naddr += fault_ipa - vgic->vgic_cpu_base;\r\nif (kvm_vcpu_dabt_iswrite(vcpu)) {\r\nu32 data = vcpu_data_guest_to_host(vcpu,\r\nvcpu_get_reg(vcpu, rd),\r\nsizeof(u32));\r\nwritel_relaxed(data, addr);\r\n} else {\r\nu32 data = readl_relaxed(addr);\r\nvcpu_set_reg(vcpu, rd, vcpu_data_host_to_guest(vcpu, data,\r\nsizeof(u32)));\r\n}\r\nreturn 1;\r\n}
