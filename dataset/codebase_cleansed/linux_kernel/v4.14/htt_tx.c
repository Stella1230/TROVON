static u8 ath10k_htt_tx_txq_calc_size(size_t count)\r\n{\r\nint exp;\r\nint factor;\r\nexp = 0;\r\nfactor = count >> 7;\r\nwhile (factor >= 64 && exp < 4) {\r\nfactor >>= 3;\r\nexp++;\r\n}\r\nif (exp == 4)\r\nreturn 0xff;\r\nif (count > 0)\r\nfactor = max(1, factor);\r\nreturn SM(exp, HTT_TX_Q_STATE_ENTRY_EXP) |\r\nSM(factor, HTT_TX_Q_STATE_ENTRY_FACTOR);\r\n}\r\nstatic void __ath10k_htt_tx_txq_recalc(struct ieee80211_hw *hw,\r\nstruct ieee80211_txq *txq)\r\n{\r\nstruct ath10k *ar = hw->priv;\r\nstruct ath10k_sta *arsta;\r\nstruct ath10k_vif *arvif = (void *)txq->vif->drv_priv;\r\nunsigned long frame_cnt;\r\nunsigned long byte_cnt;\r\nint idx;\r\nu32 bit;\r\nu16 peer_id;\r\nu8 tid;\r\nu8 count;\r\nlockdep_assert_held(&ar->htt.tx_lock);\r\nif (!ar->htt.tx_q_state.enabled)\r\nreturn;\r\nif (ar->htt.tx_q_state.mode != HTT_TX_MODE_SWITCH_PUSH_PULL)\r\nreturn;\r\nif (txq->sta) {\r\narsta = (void *)txq->sta->drv_priv;\r\npeer_id = arsta->peer_id;\r\n} else {\r\npeer_id = arvif->peer_id;\r\n}\r\ntid = txq->tid;\r\nbit = BIT(peer_id % 32);\r\nidx = peer_id / 32;\r\nieee80211_txq_get_depth(txq, &frame_cnt, &byte_cnt);\r\ncount = ath10k_htt_tx_txq_calc_size(byte_cnt);\r\nif (unlikely(peer_id >= ar->htt.tx_q_state.num_peers) ||\r\nunlikely(tid >= ar->htt.tx_q_state.num_tids)) {\r\nath10k_warn(ar, "refusing to update txq for peer_id %hu tid %hhu due to out of bounds\n",\r\npeer_id, tid);\r\nreturn;\r\n}\r\nar->htt.tx_q_state.vaddr->count[tid][peer_id] = count;\r\nar->htt.tx_q_state.vaddr->map[tid][idx] &= ~bit;\r\nar->htt.tx_q_state.vaddr->map[tid][idx] |= count ? bit : 0;\r\nath10k_dbg(ar, ATH10K_DBG_HTT, "htt tx txq state update peer_id %hu tid %hhu count %hhu\n",\r\npeer_id, tid, count);\r\n}\r\nstatic void __ath10k_htt_tx_txq_sync(struct ath10k *ar)\r\n{\r\nu32 seq;\r\nsize_t size;\r\nlockdep_assert_held(&ar->htt.tx_lock);\r\nif (!ar->htt.tx_q_state.enabled)\r\nreturn;\r\nif (ar->htt.tx_q_state.mode != HTT_TX_MODE_SWITCH_PUSH_PULL)\r\nreturn;\r\nseq = le32_to_cpu(ar->htt.tx_q_state.vaddr->seq);\r\nseq++;\r\nar->htt.tx_q_state.vaddr->seq = cpu_to_le32(seq);\r\nath10k_dbg(ar, ATH10K_DBG_HTT, "htt tx txq state update commit seq %u\n",\r\nseq);\r\nsize = sizeof(*ar->htt.tx_q_state.vaddr);\r\ndma_sync_single_for_device(ar->dev,\r\nar->htt.tx_q_state.paddr,\r\nsize,\r\nDMA_TO_DEVICE);\r\n}\r\nvoid ath10k_htt_tx_txq_recalc(struct ieee80211_hw *hw,\r\nstruct ieee80211_txq *txq)\r\n{\r\nstruct ath10k *ar = hw->priv;\r\nspin_lock_bh(&ar->htt.tx_lock);\r\n__ath10k_htt_tx_txq_recalc(hw, txq);\r\nspin_unlock_bh(&ar->htt.tx_lock);\r\n}\r\nvoid ath10k_htt_tx_txq_sync(struct ath10k *ar)\r\n{\r\nspin_lock_bh(&ar->htt.tx_lock);\r\n__ath10k_htt_tx_txq_sync(ar);\r\nspin_unlock_bh(&ar->htt.tx_lock);\r\n}\r\nvoid ath10k_htt_tx_txq_update(struct ieee80211_hw *hw,\r\nstruct ieee80211_txq *txq)\r\n{\r\nstruct ath10k *ar = hw->priv;\r\nspin_lock_bh(&ar->htt.tx_lock);\r\n__ath10k_htt_tx_txq_recalc(hw, txq);\r\n__ath10k_htt_tx_txq_sync(ar);\r\nspin_unlock_bh(&ar->htt.tx_lock);\r\n}\r\nvoid ath10k_htt_tx_dec_pending(struct ath10k_htt *htt)\r\n{\r\nlockdep_assert_held(&htt->tx_lock);\r\nhtt->num_pending_tx--;\r\nif (htt->num_pending_tx == htt->max_num_pending_tx - 1)\r\nath10k_mac_tx_unlock(htt->ar, ATH10K_TX_PAUSE_Q_FULL);\r\n}\r\nint ath10k_htt_tx_inc_pending(struct ath10k_htt *htt)\r\n{\r\nlockdep_assert_held(&htt->tx_lock);\r\nif (htt->num_pending_tx >= htt->max_num_pending_tx)\r\nreturn -EBUSY;\r\nhtt->num_pending_tx++;\r\nif (htt->num_pending_tx == htt->max_num_pending_tx)\r\nath10k_mac_tx_lock(htt->ar, ATH10K_TX_PAUSE_Q_FULL);\r\nreturn 0;\r\n}\r\nint ath10k_htt_tx_mgmt_inc_pending(struct ath10k_htt *htt, bool is_mgmt,\r\nbool is_presp)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nlockdep_assert_held(&htt->tx_lock);\r\nif (!is_mgmt || !ar->hw_params.max_probe_resp_desc_thres)\r\nreturn 0;\r\nif (is_presp &&\r\nar->hw_params.max_probe_resp_desc_thres < htt->num_pending_mgmt_tx)\r\nreturn -EBUSY;\r\nhtt->num_pending_mgmt_tx++;\r\nreturn 0;\r\n}\r\nvoid ath10k_htt_tx_mgmt_dec_pending(struct ath10k_htt *htt)\r\n{\r\nlockdep_assert_held(&htt->tx_lock);\r\nif (!htt->ar->hw_params.max_probe_resp_desc_thres)\r\nreturn;\r\nhtt->num_pending_mgmt_tx--;\r\n}\r\nint ath10k_htt_tx_alloc_msdu_id(struct ath10k_htt *htt, struct sk_buff *skb)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nint ret;\r\nlockdep_assert_held(&htt->tx_lock);\r\nret = idr_alloc(&htt->pending_tx, skb, 0,\r\nhtt->max_num_pending_tx, GFP_ATOMIC);\r\nath10k_dbg(ar, ATH10K_DBG_HTT, "htt tx alloc msdu_id %d\n", ret);\r\nreturn ret;\r\n}\r\nvoid ath10k_htt_tx_free_msdu_id(struct ath10k_htt *htt, u16 msdu_id)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nlockdep_assert_held(&htt->tx_lock);\r\nath10k_dbg(ar, ATH10K_DBG_HTT, "htt tx free msdu_id %hu\n", msdu_id);\r\nidr_remove(&htt->pending_tx, msdu_id);\r\n}\r\nstatic void ath10k_htt_tx_free_cont_txbuf(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nsize_t size;\r\nif (!htt->txbuf.vaddr)\r\nreturn;\r\nsize = htt->max_num_pending_tx * sizeof(struct ath10k_htt_txbuf);\r\ndma_free_coherent(ar->dev, size, htt->txbuf.vaddr, htt->txbuf.paddr);\r\nhtt->txbuf.vaddr = NULL;\r\n}\r\nstatic int ath10k_htt_tx_alloc_cont_txbuf(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nsize_t size;\r\nsize = htt->max_num_pending_tx * sizeof(struct ath10k_htt_txbuf);\r\nhtt->txbuf.vaddr = dma_alloc_coherent(ar->dev, size, &htt->txbuf.paddr,\r\nGFP_KERNEL);\r\nif (!htt->txbuf.vaddr)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void ath10k_htt_tx_free_cont_frag_desc(struct ath10k_htt *htt)\r\n{\r\nsize_t size;\r\nif (!htt->frag_desc.vaddr)\r\nreturn;\r\nsize = htt->max_num_pending_tx * sizeof(struct htt_msdu_ext_desc);\r\ndma_free_coherent(htt->ar->dev,\r\nsize,\r\nhtt->frag_desc.vaddr,\r\nhtt->frag_desc.paddr);\r\nhtt->frag_desc.vaddr = NULL;\r\n}\r\nstatic int ath10k_htt_tx_alloc_cont_frag_desc(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nsize_t size;\r\nif (!ar->hw_params.continuous_frag_desc)\r\nreturn 0;\r\nsize = htt->max_num_pending_tx * sizeof(struct htt_msdu_ext_desc);\r\nhtt->frag_desc.vaddr = dma_alloc_coherent(ar->dev, size,\r\n&htt->frag_desc.paddr,\r\nGFP_KERNEL);\r\nif (!htt->frag_desc.vaddr)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void ath10k_htt_tx_free_txq(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nsize_t size;\r\nif (!test_bit(ATH10K_FW_FEATURE_PEER_FLOW_CONTROL,\r\nar->running_fw->fw_file.fw_features))\r\nreturn;\r\nsize = sizeof(*htt->tx_q_state.vaddr);\r\ndma_unmap_single(ar->dev, htt->tx_q_state.paddr, size, DMA_TO_DEVICE);\r\nkfree(htt->tx_q_state.vaddr);\r\n}\r\nstatic int ath10k_htt_tx_alloc_txq(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nsize_t size;\r\nint ret;\r\nif (!test_bit(ATH10K_FW_FEATURE_PEER_FLOW_CONTROL,\r\nar->running_fw->fw_file.fw_features))\r\nreturn 0;\r\nhtt->tx_q_state.num_peers = HTT_TX_Q_STATE_NUM_PEERS;\r\nhtt->tx_q_state.num_tids = HTT_TX_Q_STATE_NUM_TIDS;\r\nhtt->tx_q_state.type = HTT_Q_DEPTH_TYPE_BYTES;\r\nsize = sizeof(*htt->tx_q_state.vaddr);\r\nhtt->tx_q_state.vaddr = kzalloc(size, GFP_KERNEL);\r\nif (!htt->tx_q_state.vaddr)\r\nreturn -ENOMEM;\r\nhtt->tx_q_state.paddr = dma_map_single(ar->dev, htt->tx_q_state.vaddr,\r\nsize, DMA_TO_DEVICE);\r\nret = dma_mapping_error(ar->dev, htt->tx_q_state.paddr);\r\nif (ret) {\r\nath10k_warn(ar, "failed to dma map tx_q_state: %d\n", ret);\r\nkfree(htt->tx_q_state.vaddr);\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ath10k_htt_tx_free_txdone_fifo(struct ath10k_htt *htt)\r\n{\r\nWARN_ON(!kfifo_is_empty(&htt->txdone_fifo));\r\nkfifo_free(&htt->txdone_fifo);\r\n}\r\nstatic int ath10k_htt_tx_alloc_txdone_fifo(struct ath10k_htt *htt)\r\n{\r\nint ret;\r\nsize_t size;\r\nsize = roundup_pow_of_two(htt->max_num_pending_tx);\r\nret = kfifo_alloc(&htt->txdone_fifo, size, GFP_KERNEL);\r\nreturn ret;\r\n}\r\nstatic int ath10k_htt_tx_alloc_buf(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nint ret;\r\nret = ath10k_htt_tx_alloc_cont_txbuf(htt);\r\nif (ret) {\r\nath10k_err(ar, "failed to alloc cont tx buffer: %d\n", ret);\r\nreturn ret;\r\n}\r\nret = ath10k_htt_tx_alloc_cont_frag_desc(htt);\r\nif (ret) {\r\nath10k_err(ar, "failed to alloc cont frag desc: %d\n", ret);\r\ngoto free_txbuf;\r\n}\r\nret = ath10k_htt_tx_alloc_txq(htt);\r\nif (ret) {\r\nath10k_err(ar, "failed to alloc txq: %d\n", ret);\r\ngoto free_frag_desc;\r\n}\r\nret = ath10k_htt_tx_alloc_txdone_fifo(htt);\r\nif (ret) {\r\nath10k_err(ar, "failed to alloc txdone fifo: %d\n", ret);\r\ngoto free_txq;\r\n}\r\nreturn 0;\r\nfree_txq:\r\nath10k_htt_tx_free_txq(htt);\r\nfree_frag_desc:\r\nath10k_htt_tx_free_cont_frag_desc(htt);\r\nfree_txbuf:\r\nath10k_htt_tx_free_cont_txbuf(htt);\r\nreturn ret;\r\n}\r\nint ath10k_htt_tx_start(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nint ret;\r\nath10k_dbg(ar, ATH10K_DBG_BOOT, "htt tx max num pending tx %d\n",\r\nhtt->max_num_pending_tx);\r\nspin_lock_init(&htt->tx_lock);\r\nidr_init(&htt->pending_tx);\r\nif (htt->tx_mem_allocated)\r\nreturn 0;\r\nret = ath10k_htt_tx_alloc_buf(htt);\r\nif (ret)\r\ngoto free_idr_pending_tx;\r\nhtt->tx_mem_allocated = true;\r\nreturn 0;\r\nfree_idr_pending_tx:\r\nidr_destroy(&htt->pending_tx);\r\nreturn ret;\r\n}\r\nstatic int ath10k_htt_tx_clean_up_pending(int msdu_id, void *skb, void *ctx)\r\n{\r\nstruct ath10k *ar = ctx;\r\nstruct ath10k_htt *htt = &ar->htt;\r\nstruct htt_tx_done tx_done = {0};\r\nath10k_dbg(ar, ATH10K_DBG_HTT, "force cleanup msdu_id %hu\n", msdu_id);\r\ntx_done.msdu_id = msdu_id;\r\ntx_done.status = HTT_TX_COMPL_STATE_DISCARD;\r\nath10k_txrx_tx_unref(htt, &tx_done);\r\nreturn 0;\r\n}\r\nvoid ath10k_htt_tx_destroy(struct ath10k_htt *htt)\r\n{\r\nif (!htt->tx_mem_allocated)\r\nreturn;\r\nath10k_htt_tx_free_cont_txbuf(htt);\r\nath10k_htt_tx_free_txq(htt);\r\nath10k_htt_tx_free_cont_frag_desc(htt);\r\nath10k_htt_tx_free_txdone_fifo(htt);\r\nhtt->tx_mem_allocated = false;\r\n}\r\nvoid ath10k_htt_tx_stop(struct ath10k_htt *htt)\r\n{\r\nidr_for_each(&htt->pending_tx, ath10k_htt_tx_clean_up_pending, htt->ar);\r\nidr_destroy(&htt->pending_tx);\r\n}\r\nvoid ath10k_htt_tx_free(struct ath10k_htt *htt)\r\n{\r\nath10k_htt_tx_stop(htt);\r\nath10k_htt_tx_destroy(htt);\r\n}\r\nvoid ath10k_htt_htc_tx_complete(struct ath10k *ar, struct sk_buff *skb)\r\n{\r\ndev_kfree_skb_any(skb);\r\n}\r\nvoid ath10k_htt_hif_tx_complete(struct ath10k *ar, struct sk_buff *skb)\r\n{\r\ndev_kfree_skb_any(skb);\r\n}\r\nint ath10k_htt_h2t_ver_req_msg(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nstruct sk_buff *skb;\r\nstruct htt_cmd *cmd;\r\nint len = 0;\r\nint ret;\r\nlen += sizeof(cmd->hdr);\r\nlen += sizeof(cmd->ver_req);\r\nskb = ath10k_htc_alloc_skb(ar, len);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nskb_put(skb, len);\r\ncmd = (struct htt_cmd *)skb->data;\r\ncmd->hdr.msg_type = HTT_H2T_MSG_TYPE_VERSION_REQ;\r\nret = ath10k_htc_send(&htt->ar->htc, htt->eid, skb);\r\nif (ret) {\r\ndev_kfree_skb_any(skb);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint ath10k_htt_h2t_stats_req(struct ath10k_htt *htt, u8 mask, u64 cookie)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nstruct htt_stats_req *req;\r\nstruct sk_buff *skb;\r\nstruct htt_cmd *cmd;\r\nint len = 0, ret;\r\nlen += sizeof(cmd->hdr);\r\nlen += sizeof(cmd->stats_req);\r\nskb = ath10k_htc_alloc_skb(ar, len);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nskb_put(skb, len);\r\ncmd = (struct htt_cmd *)skb->data;\r\ncmd->hdr.msg_type = HTT_H2T_MSG_TYPE_STATS_REQ;\r\nreq = &cmd->stats_req;\r\nmemset(req, 0, sizeof(*req));\r\nreq->upload_types[0] = mask;\r\nreq->reset_types[0] = mask;\r\nreq->stat_type = HTT_STATS_REQ_CFG_STAT_TYPE_INVALID;\r\nreq->cookie_lsb = cpu_to_le32(cookie & 0xffffffff);\r\nreq->cookie_msb = cpu_to_le32((cookie & 0xffffffff00000000ULL) >> 32);\r\nret = ath10k_htc_send(&htt->ar->htc, htt->eid, skb);\r\nif (ret) {\r\nath10k_warn(ar, "failed to send htt type stats request: %d",\r\nret);\r\ndev_kfree_skb_any(skb);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint ath10k_htt_send_frag_desc_bank_cfg(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nstruct sk_buff *skb;\r\nstruct htt_cmd *cmd;\r\nstruct htt_frag_desc_bank_cfg *cfg;\r\nint ret, size;\r\nu8 info;\r\nif (!ar->hw_params.continuous_frag_desc)\r\nreturn 0;\r\nif (!htt->frag_desc.paddr) {\r\nath10k_warn(ar, "invalid frag desc memory\n");\r\nreturn -EINVAL;\r\n}\r\nsize = sizeof(cmd->hdr) + sizeof(cmd->frag_desc_bank_cfg);\r\nskb = ath10k_htc_alloc_skb(ar, size);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nskb_put(skb, size);\r\ncmd = (struct htt_cmd *)skb->data;\r\ncmd->hdr.msg_type = HTT_H2T_MSG_TYPE_FRAG_DESC_BANK_CFG;\r\ninfo = 0;\r\ninfo |= SM(htt->tx_q_state.type,\r\nHTT_FRAG_DESC_BANK_CFG_INFO_Q_STATE_DEPTH_TYPE);\r\nif (test_bit(ATH10K_FW_FEATURE_PEER_FLOW_CONTROL,\r\nar->running_fw->fw_file.fw_features))\r\ninfo |= HTT_FRAG_DESC_BANK_CFG_INFO_Q_STATE_VALID;\r\ncfg = &cmd->frag_desc_bank_cfg;\r\ncfg->info = info;\r\ncfg->num_banks = 1;\r\ncfg->desc_size = sizeof(struct htt_msdu_ext_desc);\r\ncfg->bank_base_addrs[0] = __cpu_to_le32(htt->frag_desc.paddr);\r\ncfg->bank_id[0].bank_min_id = 0;\r\ncfg->bank_id[0].bank_max_id = __cpu_to_le16(htt->max_num_pending_tx -\r\n1);\r\ncfg->q_state.paddr = cpu_to_le32(htt->tx_q_state.paddr);\r\ncfg->q_state.num_peers = cpu_to_le16(htt->tx_q_state.num_peers);\r\ncfg->q_state.num_tids = cpu_to_le16(htt->tx_q_state.num_tids);\r\ncfg->q_state.record_size = HTT_TX_Q_STATE_ENTRY_SIZE;\r\ncfg->q_state.record_multiplier = HTT_TX_Q_STATE_ENTRY_MULTIPLIER;\r\nath10k_dbg(ar, ATH10K_DBG_HTT, "htt frag desc bank cmd\n");\r\nret = ath10k_htc_send(&htt->ar->htc, htt->eid, skb);\r\nif (ret) {\r\nath10k_warn(ar, "failed to send frag desc bank cfg request: %d\n",\r\nret);\r\ndev_kfree_skb_any(skb);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint ath10k_htt_send_rx_ring_cfg_ll(struct ath10k_htt *htt)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nstruct sk_buff *skb;\r\nstruct htt_cmd *cmd;\r\nstruct htt_rx_ring_setup_ring *ring;\r\nconst int num_rx_ring = 1;\r\nu16 flags;\r\nu32 fw_idx;\r\nint len;\r\nint ret;\r\nBUILD_BUG_ON(!IS_ALIGNED(HTT_RX_BUF_SIZE, 4));\r\nBUILD_BUG_ON((HTT_RX_BUF_SIZE & HTT_MAX_CACHE_LINE_SIZE_MASK) != 0);\r\nlen = sizeof(cmd->hdr) + sizeof(cmd->rx_setup.hdr)\r\n+ (sizeof(*ring) * num_rx_ring);\r\nskb = ath10k_htc_alloc_skb(ar, len);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nskb_put(skb, len);\r\ncmd = (struct htt_cmd *)skb->data;\r\nring = &cmd->rx_setup.rings[0];\r\ncmd->hdr.msg_type = HTT_H2T_MSG_TYPE_RX_RING_CFG;\r\ncmd->rx_setup.hdr.num_rings = 1;\r\nflags = 0;\r\nflags |= HTT_RX_RING_FLAGS_MAC80211_HDR;\r\nflags |= HTT_RX_RING_FLAGS_MSDU_PAYLOAD;\r\nflags |= HTT_RX_RING_FLAGS_PPDU_START;\r\nflags |= HTT_RX_RING_FLAGS_PPDU_END;\r\nflags |= HTT_RX_RING_FLAGS_MPDU_START;\r\nflags |= HTT_RX_RING_FLAGS_MPDU_END;\r\nflags |= HTT_RX_RING_FLAGS_MSDU_START;\r\nflags |= HTT_RX_RING_FLAGS_MSDU_END;\r\nflags |= HTT_RX_RING_FLAGS_RX_ATTENTION;\r\nflags |= HTT_RX_RING_FLAGS_FRAG_INFO;\r\nflags |= HTT_RX_RING_FLAGS_UNICAST_RX;\r\nflags |= HTT_RX_RING_FLAGS_MULTICAST_RX;\r\nflags |= HTT_RX_RING_FLAGS_CTRL_RX;\r\nflags |= HTT_RX_RING_FLAGS_MGMT_RX;\r\nflags |= HTT_RX_RING_FLAGS_NULL_RX;\r\nflags |= HTT_RX_RING_FLAGS_PHY_DATA_RX;\r\nfw_idx = __le32_to_cpu(*htt->rx_ring.alloc_idx.vaddr);\r\nring->fw_idx_shadow_reg_paddr =\r\n__cpu_to_le32(htt->rx_ring.alloc_idx.paddr);\r\nring->rx_ring_base_paddr = __cpu_to_le32(htt->rx_ring.base_paddr);\r\nring->rx_ring_len = __cpu_to_le16(htt->rx_ring.size);\r\nring->rx_ring_bufsize = __cpu_to_le16(HTT_RX_BUF_SIZE);\r\nring->flags = __cpu_to_le16(flags);\r\nring->fw_idx_init_val = __cpu_to_le16(fw_idx);\r\n#define desc_offset(x) (offsetof(struct htt_rx_desc, x) / 4)\r\nring->mac80211_hdr_offset = __cpu_to_le16(desc_offset(rx_hdr_status));\r\nring->msdu_payload_offset = __cpu_to_le16(desc_offset(msdu_payload));\r\nring->ppdu_start_offset = __cpu_to_le16(desc_offset(ppdu_start));\r\nring->ppdu_end_offset = __cpu_to_le16(desc_offset(ppdu_end));\r\nring->mpdu_start_offset = __cpu_to_le16(desc_offset(mpdu_start));\r\nring->mpdu_end_offset = __cpu_to_le16(desc_offset(mpdu_end));\r\nring->msdu_start_offset = __cpu_to_le16(desc_offset(msdu_start));\r\nring->msdu_end_offset = __cpu_to_le16(desc_offset(msdu_end));\r\nring->rx_attention_offset = __cpu_to_le16(desc_offset(attention));\r\nring->frag_info_offset = __cpu_to_le16(desc_offset(frag_info));\r\n#undef desc_offset\r\nret = ath10k_htc_send(&htt->ar->htc, htt->eid, skb);\r\nif (ret) {\r\ndev_kfree_skb_any(skb);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint ath10k_htt_h2t_aggr_cfg_msg(struct ath10k_htt *htt,\r\nu8 max_subfrms_ampdu,\r\nu8 max_subfrms_amsdu)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nstruct htt_aggr_conf *aggr_conf;\r\nstruct sk_buff *skb;\r\nstruct htt_cmd *cmd;\r\nint len;\r\nint ret;\r\nif (max_subfrms_ampdu == 0 || max_subfrms_ampdu > 64)\r\nreturn -EINVAL;\r\nif (max_subfrms_amsdu == 0 || max_subfrms_amsdu > 31)\r\nreturn -EINVAL;\r\nlen = sizeof(cmd->hdr);\r\nlen += sizeof(cmd->aggr_conf);\r\nskb = ath10k_htc_alloc_skb(ar, len);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nskb_put(skb, len);\r\ncmd = (struct htt_cmd *)skb->data;\r\ncmd->hdr.msg_type = HTT_H2T_MSG_TYPE_AGGR_CFG;\r\naggr_conf = &cmd->aggr_conf;\r\naggr_conf->max_num_ampdu_subframes = max_subfrms_ampdu;\r\naggr_conf->max_num_amsdu_subframes = max_subfrms_amsdu;\r\nath10k_dbg(ar, ATH10K_DBG_HTT, "htt h2t aggr cfg msg amsdu %d ampdu %d",\r\naggr_conf->max_num_amsdu_subframes,\r\naggr_conf->max_num_ampdu_subframes);\r\nret = ath10k_htc_send(&htt->ar->htc, htt->eid, skb);\r\nif (ret) {\r\ndev_kfree_skb_any(skb);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint ath10k_htt_tx_fetch_resp(struct ath10k *ar,\r\n__le32 token,\r\n__le16 fetch_seq_num,\r\nstruct htt_tx_fetch_record *records,\r\nsize_t num_records)\r\n{\r\nstruct sk_buff *skb;\r\nstruct htt_cmd *cmd;\r\nconst u16 resp_id = 0;\r\nint len = 0;\r\nint ret;\r\nlen += sizeof(cmd->hdr);\r\nlen += sizeof(cmd->tx_fetch_resp);\r\nlen += sizeof(cmd->tx_fetch_resp.records[0]) * num_records;\r\nskb = ath10k_htc_alloc_skb(ar, len);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nskb_put(skb, len);\r\ncmd = (struct htt_cmd *)skb->data;\r\ncmd->hdr.msg_type = HTT_H2T_MSG_TYPE_TX_FETCH_RESP;\r\ncmd->tx_fetch_resp.resp_id = cpu_to_le16(resp_id);\r\ncmd->tx_fetch_resp.fetch_seq_num = fetch_seq_num;\r\ncmd->tx_fetch_resp.num_records = cpu_to_le16(num_records);\r\ncmd->tx_fetch_resp.token = token;\r\nmemcpy(cmd->tx_fetch_resp.records, records,\r\nsizeof(records[0]) * num_records);\r\nret = ath10k_htc_send(&ar->htc, ar->htt.eid, skb);\r\nif (ret) {\r\nath10k_warn(ar, "failed to submit htc command: %d\n", ret);\r\ngoto err_free_skb;\r\n}\r\nreturn 0;\r\nerr_free_skb:\r\ndev_kfree_skb_any(skb);\r\nreturn ret;\r\n}\r\nstatic u8 ath10k_htt_tx_get_vdev_id(struct ath10k *ar, struct sk_buff *skb)\r\n{\r\nstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\r\nstruct ath10k_skb_cb *cb = ATH10K_SKB_CB(skb);\r\nstruct ath10k_vif *arvif;\r\nif (info->flags & IEEE80211_TX_CTL_TX_OFFCHAN) {\r\nreturn ar->scan.vdev_id;\r\n} else if (cb->vif) {\r\narvif = (void *)cb->vif->drv_priv;\r\nreturn arvif->vdev_id;\r\n} else if (ar->monitor_started) {\r\nreturn ar->monitor_vdev_id;\r\n} else {\r\nreturn 0;\r\n}\r\n}\r\nstatic u8 ath10k_htt_tx_get_tid(struct sk_buff *skb, bool is_eth)\r\n{\r\nstruct ieee80211_hdr *hdr = (void *)skb->data;\r\nstruct ath10k_skb_cb *cb = ATH10K_SKB_CB(skb);\r\nif (!is_eth && ieee80211_is_mgmt(hdr->frame_control))\r\nreturn HTT_DATA_TX_EXT_TID_MGMT;\r\nelse if (cb->flags & ATH10K_SKB_F_QOS)\r\nreturn skb->priority % IEEE80211_QOS_CTL_TID_MASK;\r\nelse\r\nreturn HTT_DATA_TX_EXT_TID_NON_QOS_MCAST_BCAST;\r\n}\r\nint ath10k_htt_mgmt_tx(struct ath10k_htt *htt, struct sk_buff *msdu)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nstruct device *dev = ar->dev;\r\nstruct sk_buff *txdesc = NULL;\r\nstruct htt_cmd *cmd;\r\nstruct ath10k_skb_cb *skb_cb = ATH10K_SKB_CB(msdu);\r\nu8 vdev_id = ath10k_htt_tx_get_vdev_id(ar, msdu);\r\nint len = 0;\r\nint msdu_id = -1;\r\nint res;\r\nstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)msdu->data;\r\nlen += sizeof(cmd->hdr);\r\nlen += sizeof(cmd->mgmt_tx);\r\nspin_lock_bh(&htt->tx_lock);\r\nres = ath10k_htt_tx_alloc_msdu_id(htt, msdu);\r\nspin_unlock_bh(&htt->tx_lock);\r\nif (res < 0)\r\ngoto err;\r\nmsdu_id = res;\r\nif ((ieee80211_is_action(hdr->frame_control) ||\r\nieee80211_is_deauth(hdr->frame_control) ||\r\nieee80211_is_disassoc(hdr->frame_control)) &&\r\nieee80211_has_protected(hdr->frame_control)) {\r\nskb_put(msdu, IEEE80211_CCMP_MIC_LEN);\r\n}\r\ntxdesc = ath10k_htc_alloc_skb(ar, len);\r\nif (!txdesc) {\r\nres = -ENOMEM;\r\ngoto err_free_msdu_id;\r\n}\r\nskb_cb->paddr = dma_map_single(dev, msdu->data, msdu->len,\r\nDMA_TO_DEVICE);\r\nres = dma_mapping_error(dev, skb_cb->paddr);\r\nif (res) {\r\nres = -EIO;\r\ngoto err_free_txdesc;\r\n}\r\nskb_put(txdesc, len);\r\ncmd = (struct htt_cmd *)txdesc->data;\r\nmemset(cmd, 0, len);\r\ncmd->hdr.msg_type = HTT_H2T_MSG_TYPE_MGMT_TX;\r\ncmd->mgmt_tx.msdu_paddr = __cpu_to_le32(ATH10K_SKB_CB(msdu)->paddr);\r\ncmd->mgmt_tx.len = __cpu_to_le32(msdu->len);\r\ncmd->mgmt_tx.desc_id = __cpu_to_le32(msdu_id);\r\ncmd->mgmt_tx.vdev_id = __cpu_to_le32(vdev_id);\r\nmemcpy(cmd->mgmt_tx.hdr, msdu->data,\r\nmin_t(int, msdu->len, HTT_MGMT_FRM_HDR_DOWNLOAD_LEN));\r\nres = ath10k_htc_send(&htt->ar->htc, htt->eid, txdesc);\r\nif (res)\r\ngoto err_unmap_msdu;\r\nreturn 0;\r\nerr_unmap_msdu:\r\ndma_unmap_single(dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);\r\nerr_free_txdesc:\r\ndev_kfree_skb_any(txdesc);\r\nerr_free_msdu_id:\r\nspin_lock_bh(&htt->tx_lock);\r\nath10k_htt_tx_free_msdu_id(htt, msdu_id);\r\nspin_unlock_bh(&htt->tx_lock);\r\nerr:\r\nreturn res;\r\n}\r\nint ath10k_htt_tx(struct ath10k_htt *htt, enum ath10k_hw_txrx_mode txmode,\r\nstruct sk_buff *msdu)\r\n{\r\nstruct ath10k *ar = htt->ar;\r\nstruct device *dev = ar->dev;\r\nstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)msdu->data;\r\nstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(msdu);\r\nstruct ath10k_skb_cb *skb_cb = ATH10K_SKB_CB(msdu);\r\nstruct ath10k_hif_sg_item sg_items[2];\r\nstruct ath10k_htt_txbuf *txbuf;\r\nstruct htt_data_tx_desc_frag *frags;\r\nbool is_eth = (txmode == ATH10K_HW_TXRX_ETHERNET);\r\nu8 vdev_id = ath10k_htt_tx_get_vdev_id(ar, msdu);\r\nu8 tid = ath10k_htt_tx_get_tid(msdu, is_eth);\r\nint prefetch_len;\r\nint res;\r\nu8 flags0 = 0;\r\nu16 msdu_id, flags1 = 0;\r\nu16 freq = 0;\r\nu32 frags_paddr = 0;\r\nu32 txbuf_paddr;\r\nstruct htt_msdu_ext_desc *ext_desc = NULL;\r\nspin_lock_bh(&htt->tx_lock);\r\nres = ath10k_htt_tx_alloc_msdu_id(htt, msdu);\r\nspin_unlock_bh(&htt->tx_lock);\r\nif (res < 0)\r\ngoto err;\r\nmsdu_id = res;\r\nprefetch_len = min(htt->prefetch_len, msdu->len);\r\nprefetch_len = roundup(prefetch_len, 4);\r\ntxbuf = &htt->txbuf.vaddr[msdu_id];\r\ntxbuf_paddr = htt->txbuf.paddr +\r\n(sizeof(struct ath10k_htt_txbuf) * msdu_id);\r\nif ((ieee80211_is_action(hdr->frame_control) ||\r\nieee80211_is_deauth(hdr->frame_control) ||\r\nieee80211_is_disassoc(hdr->frame_control)) &&\r\nieee80211_has_protected(hdr->frame_control)) {\r\nskb_put(msdu, IEEE80211_CCMP_MIC_LEN);\r\n} else if (!(skb_cb->flags & ATH10K_SKB_F_NO_HWCRYPT) &&\r\ntxmode == ATH10K_HW_TXRX_RAW &&\r\nieee80211_has_protected(hdr->frame_control)) {\r\nskb_put(msdu, IEEE80211_CCMP_MIC_LEN);\r\n}\r\nskb_cb->paddr = dma_map_single(dev, msdu->data, msdu->len,\r\nDMA_TO_DEVICE);\r\nres = dma_mapping_error(dev, skb_cb->paddr);\r\nif (res) {\r\nres = -EIO;\r\ngoto err_free_msdu_id;\r\n}\r\nif (unlikely(info->flags & IEEE80211_TX_CTL_TX_OFFCHAN))\r\nfreq = ar->scan.roc_freq;\r\nswitch (txmode) {\r\ncase ATH10K_HW_TXRX_RAW:\r\ncase ATH10K_HW_TXRX_NATIVE_WIFI:\r\nflags0 |= HTT_DATA_TX_DESC_FLAGS0_MAC_HDR_PRESENT;\r\ncase ATH10K_HW_TXRX_ETHERNET:\r\nif (ar->hw_params.continuous_frag_desc) {\r\nmemset(&htt->frag_desc.vaddr[msdu_id], 0,\r\nsizeof(struct htt_msdu_ext_desc));\r\nfrags = (struct htt_data_tx_desc_frag *)\r\n&htt->frag_desc.vaddr[msdu_id].frags;\r\next_desc = &htt->frag_desc.vaddr[msdu_id];\r\nfrags[0].tword_addr.paddr_lo =\r\n__cpu_to_le32(skb_cb->paddr);\r\nfrags[0].tword_addr.paddr_hi = 0;\r\nfrags[0].tword_addr.len_16 = __cpu_to_le16(msdu->len);\r\nfrags_paddr = htt->frag_desc.paddr +\r\n(sizeof(struct htt_msdu_ext_desc) * msdu_id);\r\n} else {\r\nfrags = txbuf->frags;\r\nfrags[0].dword_addr.paddr =\r\n__cpu_to_le32(skb_cb->paddr);\r\nfrags[0].dword_addr.len = __cpu_to_le32(msdu->len);\r\nfrags[1].dword_addr.paddr = 0;\r\nfrags[1].dword_addr.len = 0;\r\nfrags_paddr = txbuf_paddr;\r\n}\r\nflags0 |= SM(txmode, HTT_DATA_TX_DESC_FLAGS0_PKT_TYPE);\r\nbreak;\r\ncase ATH10K_HW_TXRX_MGMT:\r\nflags0 |= SM(ATH10K_HW_TXRX_MGMT,\r\nHTT_DATA_TX_DESC_FLAGS0_PKT_TYPE);\r\nflags0 |= HTT_DATA_TX_DESC_FLAGS0_MAC_HDR_PRESENT;\r\nfrags_paddr = skb_cb->paddr;\r\nbreak;\r\n}\r\ntxbuf->htc_hdr.eid = htt->eid;\r\ntxbuf->htc_hdr.len = __cpu_to_le16(sizeof(txbuf->cmd_hdr) +\r\nsizeof(txbuf->cmd_tx) +\r\nprefetch_len);\r\ntxbuf->htc_hdr.flags = 0;\r\nif (skb_cb->flags & ATH10K_SKB_F_NO_HWCRYPT)\r\nflags0 |= HTT_DATA_TX_DESC_FLAGS0_NO_ENCRYPT;\r\nflags1 |= SM((u16)vdev_id, HTT_DATA_TX_DESC_FLAGS1_VDEV_ID);\r\nflags1 |= SM((u16)tid, HTT_DATA_TX_DESC_FLAGS1_EXT_TID);\r\nif (msdu->ip_summed == CHECKSUM_PARTIAL &&\r\n!test_bit(ATH10K_FLAG_RAW_MODE, &ar->dev_flags)) {\r\nflags1 |= HTT_DATA_TX_DESC_FLAGS1_CKSUM_L3_OFFLOAD;\r\nflags1 |= HTT_DATA_TX_DESC_FLAGS1_CKSUM_L4_OFFLOAD;\r\nif (ar->hw_params.continuous_frag_desc)\r\next_desc->flags |= HTT_MSDU_CHECKSUM_ENABLE;\r\n}\r\nflags1 |= HTT_DATA_TX_DESC_FLAGS1_POSTPONED;\r\ntxbuf->cmd_hdr.msg_type = HTT_H2T_MSG_TYPE_TX_FRM;\r\ntxbuf->cmd_tx.flags0 = flags0;\r\ntxbuf->cmd_tx.flags1 = __cpu_to_le16(flags1);\r\ntxbuf->cmd_tx.len = __cpu_to_le16(msdu->len);\r\ntxbuf->cmd_tx.id = __cpu_to_le16(msdu_id);\r\ntxbuf->cmd_tx.frags_paddr = __cpu_to_le32(frags_paddr);\r\nif (ath10k_mac_tx_frm_has_freq(ar)) {\r\ntxbuf->cmd_tx.offchan_tx.peerid =\r\n__cpu_to_le16(HTT_INVALID_PEERID);\r\ntxbuf->cmd_tx.offchan_tx.freq =\r\n__cpu_to_le16(freq);\r\n} else {\r\ntxbuf->cmd_tx.peerid =\r\n__cpu_to_le32(HTT_INVALID_PEERID);\r\n}\r\ntrace_ath10k_htt_tx(ar, msdu_id, msdu->len, vdev_id, tid);\r\nath10k_dbg(ar, ATH10K_DBG_HTT,\r\n"htt tx flags0 %hhu flags1 %hu len %d id %hu frags_paddr %08x, msdu_paddr %08x vdev %hhu tid %hhu freq %hu\n",\r\nflags0, flags1, msdu->len, msdu_id, frags_paddr,\r\n(u32)skb_cb->paddr, vdev_id, tid, freq);\r\nath10k_dbg_dump(ar, ATH10K_DBG_HTT_DUMP, NULL, "htt tx msdu: ",\r\nmsdu->data, msdu->len);\r\ntrace_ath10k_tx_hdr(ar, msdu->data, msdu->len);\r\ntrace_ath10k_tx_payload(ar, msdu->data, msdu->len);\r\nsg_items[0].transfer_id = 0;\r\nsg_items[0].transfer_context = NULL;\r\nsg_items[0].vaddr = &txbuf->htc_hdr;\r\nsg_items[0].paddr = txbuf_paddr +\r\nsizeof(txbuf->frags);\r\nsg_items[0].len = sizeof(txbuf->htc_hdr) +\r\nsizeof(txbuf->cmd_hdr) +\r\nsizeof(txbuf->cmd_tx);\r\nsg_items[1].transfer_id = 0;\r\nsg_items[1].transfer_context = NULL;\r\nsg_items[1].vaddr = msdu->data;\r\nsg_items[1].paddr = skb_cb->paddr;\r\nsg_items[1].len = prefetch_len;\r\nres = ath10k_hif_tx_sg(htt->ar,\r\nhtt->ar->htc.endpoint[htt->eid].ul_pipe_id,\r\nsg_items, ARRAY_SIZE(sg_items));\r\nif (res)\r\ngoto err_unmap_msdu;\r\nreturn 0;\r\nerr_unmap_msdu:\r\ndma_unmap_single(dev, skb_cb->paddr, msdu->len, DMA_TO_DEVICE);\r\nerr_free_msdu_id:\r\nath10k_htt_tx_free_msdu_id(htt, msdu_id);\r\nerr:\r\nreturn res;\r\n}
