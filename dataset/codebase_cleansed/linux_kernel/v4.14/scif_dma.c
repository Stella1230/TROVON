int scif_reserve_dma_chan(struct scif_endpt *ep)\r\n{\r\nint err = 0;\r\nstruct scif_dev *scifdev;\r\nstruct scif_hw_dev *sdev;\r\nstruct dma_chan *chan;\r\nif (!scif_info.nodeid && scifdev_self(ep->remote_dev))\r\nreturn 0;\r\nif (scif_info.nodeid)\r\nscifdev = &scif_dev[0];\r\nelse\r\nscifdev = ep->remote_dev;\r\nsdev = scifdev->sdev;\r\nif (!sdev->num_dma_ch)\r\nreturn -ENODEV;\r\nchan = sdev->dma_ch[scifdev->dma_ch_idx];\r\nscifdev->dma_ch_idx = (scifdev->dma_ch_idx + 1) % sdev->num_dma_ch;\r\nmutex_lock(&ep->rma_info.rma_lock);\r\nep->rma_info.dma_chan = chan;\r\nmutex_unlock(&ep->rma_info.rma_lock);\r\nreturn err;\r\n}\r\nstatic\r\nvoid __scif_rma_destroy_tcw(struct scif_mmu_notif *mmn,\r\nu64 start, u64 len)\r\n{\r\nstruct list_head *item, *tmp;\r\nstruct scif_window *window;\r\nu64 start_va, end_va;\r\nu64 end = start + len;\r\nif (end <= start)\r\nreturn;\r\nlist_for_each_safe(item, tmp, &mmn->tc_reg_list) {\r\nwindow = list_entry(item, struct scif_window, list);\r\nif (!len)\r\nbreak;\r\nstart_va = window->va_for_temp;\r\nend_va = start_va + (window->nr_pages << PAGE_SHIFT);\r\nif (start < start_va && end <= start_va)\r\nbreak;\r\nif (start >= end_va)\r\ncontinue;\r\n__scif_rma_destroy_tcw_helper(window);\r\n}\r\n}\r\nstatic void scif_rma_destroy_tcw(struct scif_mmu_notif *mmn, u64 start, u64 len)\r\n{\r\nstruct scif_endpt *ep = mmn->ep;\r\nspin_lock(&ep->rma_info.tc_lock);\r\n__scif_rma_destroy_tcw(mmn, start, len);\r\nspin_unlock(&ep->rma_info.tc_lock);\r\n}\r\nstatic void scif_rma_destroy_tcw_ep(struct scif_endpt *ep)\r\n{\r\nstruct list_head *item, *tmp;\r\nstruct scif_mmu_notif *mmn;\r\nlist_for_each_safe(item, tmp, &ep->rma_info.mmn_list) {\r\nmmn = list_entry(item, struct scif_mmu_notif, list);\r\nscif_rma_destroy_tcw(mmn, 0, ULONG_MAX);\r\n}\r\n}\r\nstatic void __scif_rma_destroy_tcw_ep(struct scif_endpt *ep)\r\n{\r\nstruct list_head *item, *tmp;\r\nstruct scif_mmu_notif *mmn;\r\nspin_lock(&ep->rma_info.tc_lock);\r\nlist_for_each_safe(item, tmp, &ep->rma_info.mmn_list) {\r\nmmn = list_entry(item, struct scif_mmu_notif, list);\r\n__scif_rma_destroy_tcw(mmn, 0, ULONG_MAX);\r\n}\r\nspin_unlock(&ep->rma_info.tc_lock);\r\n}\r\nstatic bool scif_rma_tc_can_cache(struct scif_endpt *ep, size_t cur_bytes)\r\n{\r\nif ((cur_bytes >> PAGE_SHIFT) > scif_info.rma_tc_limit)\r\nreturn false;\r\nif ((atomic_read(&ep->rma_info.tcw_total_pages)\r\n+ (cur_bytes >> PAGE_SHIFT)) >\r\nscif_info.rma_tc_limit) {\r\ndev_info(scif_info.mdev.this_device,\r\n"%s %d total=%d, current=%zu reached max\n",\r\n__func__, __LINE__,\r\natomic_read(&ep->rma_info.tcw_total_pages),\r\n(1 + (cur_bytes >> PAGE_SHIFT)));\r\nscif_rma_destroy_tcw_invalid();\r\n__scif_rma_destroy_tcw_ep(ep);\r\n}\r\nreturn true;\r\n}\r\nstatic void scif_mmu_notifier_release(struct mmu_notifier *mn,\r\nstruct mm_struct *mm)\r\n{\r\nstruct scif_mmu_notif *mmn;\r\nmmn = container_of(mn, struct scif_mmu_notif, ep_mmu_notifier);\r\nscif_rma_destroy_tcw(mmn, 0, ULONG_MAX);\r\nschedule_work(&scif_info.misc_work);\r\n}\r\nstatic void scif_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start,\r\nunsigned long end)\r\n{\r\nstruct scif_mmu_notif *mmn;\r\nmmn = container_of(mn, struct scif_mmu_notif, ep_mmu_notifier);\r\nscif_rma_destroy_tcw(mmn, start, end - start);\r\n}\r\nstatic void scif_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start,\r\nunsigned long end)\r\n{\r\n}\r\nstatic void scif_ep_unregister_mmu_notifier(struct scif_endpt *ep)\r\n{\r\nstruct scif_endpt_rma_info *rma = &ep->rma_info;\r\nstruct scif_mmu_notif *mmn = NULL;\r\nstruct list_head *item, *tmp;\r\nmutex_lock(&ep->rma_info.mmn_lock);\r\nlist_for_each_safe(item, tmp, &rma->mmn_list) {\r\nmmn = list_entry(item, struct scif_mmu_notif, list);\r\nmmu_notifier_unregister(&mmn->ep_mmu_notifier, mmn->mm);\r\nlist_del(item);\r\nkfree(mmn);\r\n}\r\nmutex_unlock(&ep->rma_info.mmn_lock);\r\n}\r\nstatic void scif_init_mmu_notifier(struct scif_mmu_notif *mmn,\r\nstruct mm_struct *mm, struct scif_endpt *ep)\r\n{\r\nmmn->ep = ep;\r\nmmn->mm = mm;\r\nmmn->ep_mmu_notifier.ops = &scif_mmu_notifier_ops;\r\nINIT_LIST_HEAD(&mmn->list);\r\nINIT_LIST_HEAD(&mmn->tc_reg_list);\r\n}\r\nstatic struct scif_mmu_notif *\r\nscif_find_mmu_notifier(struct mm_struct *mm, struct scif_endpt_rma_info *rma)\r\n{\r\nstruct scif_mmu_notif *mmn;\r\nlist_for_each_entry(mmn, &rma->mmn_list, list)\r\nif (mmn->mm == mm)\r\nreturn mmn;\r\nreturn NULL;\r\n}\r\nstatic struct scif_mmu_notif *\r\nscif_add_mmu_notifier(struct mm_struct *mm, struct scif_endpt *ep)\r\n{\r\nstruct scif_mmu_notif *mmn\r\n= kzalloc(sizeof(*mmn), GFP_KERNEL);\r\nif (!mmn)\r\nreturn ERR_PTR(-ENOMEM);\r\nscif_init_mmu_notifier(mmn, current->mm, ep);\r\nif (mmu_notifier_register(&mmn->ep_mmu_notifier, current->mm)) {\r\nkfree(mmn);\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\nlist_add(&mmn->list, &ep->rma_info.mmn_list);\r\nreturn mmn;\r\n}\r\nvoid scif_mmu_notif_handler(struct work_struct *work)\r\n{\r\nstruct list_head *pos, *tmpq;\r\nstruct scif_endpt *ep;\r\nrestart:\r\nscif_rma_destroy_tcw_invalid();\r\nspin_lock(&scif_info.rmalock);\r\nlist_for_each_safe(pos, tmpq, &scif_info.mmu_notif_cleanup) {\r\nep = list_entry(pos, struct scif_endpt, mmu_list);\r\nlist_del(&ep->mmu_list);\r\nspin_unlock(&scif_info.rmalock);\r\nscif_rma_destroy_tcw_ep(ep);\r\nscif_ep_unregister_mmu_notifier(ep);\r\ngoto restart;\r\n}\r\nspin_unlock(&scif_info.rmalock);\r\n}\r\nstatic bool scif_is_set_reg_cache(int flags)\r\n{\r\nreturn !!(flags & SCIF_RMA_USECACHE);\r\n}\r\nstatic struct scif_mmu_notif *\r\nscif_find_mmu_notifier(struct mm_struct *mm,\r\nstruct scif_endpt_rma_info *rma)\r\n{\r\nreturn NULL;\r\n}\r\nstatic struct scif_mmu_notif *\r\nscif_add_mmu_notifier(struct mm_struct *mm, struct scif_endpt *ep)\r\n{\r\nreturn NULL;\r\n}\r\nvoid scif_mmu_notif_handler(struct work_struct *work)\r\n{\r\n}\r\nstatic bool scif_is_set_reg_cache(int flags)\r\n{\r\nreturn false;\r\n}\r\nstatic bool scif_rma_tc_can_cache(struct scif_endpt *ep, size_t cur_bytes)\r\n{\r\nreturn false;\r\n}\r\nstatic int\r\nscif_register_temp(scif_epd_t epd, unsigned long addr, size_t len, int prot,\r\noff_t *out_offset, struct scif_window **out_window)\r\n{\r\nstruct scif_endpt *ep = (struct scif_endpt *)epd;\r\nint err;\r\nscif_pinned_pages_t pinned_pages;\r\nsize_t aligned_len;\r\naligned_len = ALIGN(len, PAGE_SIZE);\r\nerr = __scif_pin_pages((void *)(addr & PAGE_MASK),\r\naligned_len, &prot, 0, &pinned_pages);\r\nif (err)\r\nreturn err;\r\npinned_pages->prot = prot;\r\nerr = scif_get_window_offset(ep, 0, 0,\r\naligned_len >> PAGE_SHIFT,\r\n(s64 *)out_offset);\r\nif (err)\r\ngoto error_unpin;\r\n*out_window = scif_create_window(ep, aligned_len >> PAGE_SHIFT,\r\n*out_offset, true);\r\nif (!*out_window) {\r\nscif_free_window_offset(ep, NULL, *out_offset);\r\nerr = -ENOMEM;\r\ngoto error_unpin;\r\n}\r\n(*out_window)->pinned_pages = pinned_pages;\r\n(*out_window)->nr_pages = pinned_pages->nr_pages;\r\n(*out_window)->prot = pinned_pages->prot;\r\n(*out_window)->va_for_temp = addr & PAGE_MASK;\r\nerr = scif_map_window(ep->remote_dev, *out_window);\r\nif (err) {\r\nscif_destroy_window(ep, *out_window);\r\n*out_window = NULL;\r\n} else {\r\n*out_offset |= (addr - (*out_window)->va_for_temp);\r\n}\r\nreturn err;\r\nerror_unpin:\r\nif (err)\r\ndev_err(&ep->remote_dev->sdev->dev,\r\n"%s %d err %d\n", __func__, __LINE__, err);\r\nscif_unpin_pages(pinned_pages);\r\nreturn err;\r\n}\r\nstatic int scif_sync_dma(struct scif_hw_dev *sdev, struct dma_chan *chan,\r\nbool sync_wait)\r\n{\r\nint err = 0;\r\nstruct dma_async_tx_descriptor *tx = NULL;\r\nenum dma_ctrl_flags flags = DMA_PREP_FENCE;\r\ndma_cookie_t cookie;\r\nstruct dma_device *ddev;\r\nif (!chan) {\r\nerr = -EIO;\r\ndev_err(&sdev->dev, "%s %d err %d\n",\r\n__func__, __LINE__, err);\r\nreturn err;\r\n}\r\nddev = chan->device;\r\ntx = ddev->device_prep_dma_memcpy(chan, 0, 0, 0, flags);\r\nif (!tx) {\r\nerr = -ENOMEM;\r\ndev_err(&sdev->dev, "%s %d err %d\n",\r\n__func__, __LINE__, err);\r\ngoto release;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nerr = -ENOMEM;\r\ndev_err(&sdev->dev, "%s %d err %d\n",\r\n__func__, __LINE__, err);\r\ngoto release;\r\n}\r\nif (!sync_wait) {\r\ndma_async_issue_pending(chan);\r\n} else {\r\nif (dma_sync_wait(chan, cookie) == DMA_COMPLETE) {\r\nerr = 0;\r\n} else {\r\nerr = -EIO;\r\ndev_err(&sdev->dev, "%s %d err %d\n",\r\n__func__, __LINE__, err);\r\n}\r\n}\r\nrelease:\r\nreturn err;\r\n}\r\nstatic void scif_dma_callback(void *arg)\r\n{\r\nstruct completion *done = (struct completion *)arg;\r\ncomplete(done);\r\n}\r\nstatic int scif_async_dma(struct scif_hw_dev *sdev, struct dma_chan *chan)\r\n{\r\nint err = 0;\r\nstruct dma_device *ddev;\r\nstruct dma_async_tx_descriptor *tx = NULL;\r\nenum dma_ctrl_flags flags = DMA_PREP_INTERRUPT | DMA_PREP_FENCE;\r\nDECLARE_COMPLETION_ONSTACK(done_wait);\r\ndma_cookie_t cookie;\r\nenum dma_status status;\r\nif (!chan) {\r\nerr = -EIO;\r\ndev_err(&sdev->dev, "%s %d err %d\n",\r\n__func__, __LINE__, err);\r\nreturn err;\r\n}\r\nddev = chan->device;\r\ntx = ddev->device_prep_dma_memcpy(chan, 0, 0, 0, flags);\r\nif (!tx) {\r\nerr = -ENOMEM;\r\ndev_err(&sdev->dev, "%s %d err %d\n",\r\n__func__, __LINE__, err);\r\ngoto release;\r\n}\r\nreinit_completion(&done_wait);\r\ntx->callback = scif_dma_callback;\r\ntx->callback_param = &done_wait;\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nerr = -ENOMEM;\r\ndev_err(&sdev->dev, "%s %d err %d\n",\r\n__func__, __LINE__, err);\r\ngoto release;\r\n}\r\ndma_async_issue_pending(chan);\r\nerr = wait_for_completion_timeout(&done_wait, SCIF_DMA_TO);\r\nif (!err) {\r\nerr = -EIO;\r\ndev_err(&sdev->dev, "%s %d err %d\n",\r\n__func__, __LINE__, err);\r\ngoto release;\r\n}\r\nerr = 0;\r\nstatus = dma_async_is_tx_complete(chan, cookie, NULL, NULL);\r\nif (status != DMA_COMPLETE) {\r\nerr = -EIO;\r\ndev_err(&sdev->dev, "%s %d err %d\n",\r\n__func__, __LINE__, err);\r\ngoto release;\r\n}\r\nrelease:\r\nreturn err;\r\n}\r\nstatic int scif_drain_dma_poll(struct scif_hw_dev *sdev, struct dma_chan *chan)\r\n{\r\nif (!chan)\r\nreturn -EINVAL;\r\nreturn scif_sync_dma(sdev, chan, SCIF_DMA_SYNC_WAIT);\r\n}\r\nint scif_drain_dma_intr(struct scif_hw_dev *sdev, struct dma_chan *chan)\r\n{\r\nif (!chan)\r\nreturn -EINVAL;\r\nreturn scif_async_dma(sdev, chan);\r\n}\r\nvoid scif_rma_destroy_windows(void)\r\n{\r\nstruct list_head *item, *tmp;\r\nstruct scif_window *window;\r\nstruct scif_endpt *ep;\r\nstruct dma_chan *chan;\r\nmight_sleep();\r\nrestart:\r\nspin_lock(&scif_info.rmalock);\r\nlist_for_each_safe(item, tmp, &scif_info.rma) {\r\nwindow = list_entry(item, struct scif_window,\r\nlist);\r\nep = (struct scif_endpt *)window->ep;\r\nchan = ep->rma_info.dma_chan;\r\nlist_del_init(&window->list);\r\nspin_unlock(&scif_info.rmalock);\r\nif (!chan || !scifdev_alive(ep) ||\r\n!scif_drain_dma_intr(ep->remote_dev->sdev,\r\nep->rma_info.dma_chan))\r\nwindow->unreg_state = OP_COMPLETED;\r\nelse\r\ndev_warn(&ep->remote_dev->sdev->dev,\r\n"DMA engine hung?\n");\r\nif (window->unreg_state == OP_COMPLETED) {\r\nif (window->type == SCIF_WINDOW_SELF)\r\nscif_destroy_window(ep, window);\r\nelse\r\nscif_destroy_remote_window(window);\r\natomic_dec(&ep->rma_info.tw_refcount);\r\n}\r\ngoto restart;\r\n}\r\nspin_unlock(&scif_info.rmalock);\r\n}\r\nvoid scif_rma_destroy_tcw_invalid(void)\r\n{\r\nstruct list_head *item, *tmp;\r\nstruct scif_window *window;\r\nstruct scif_endpt *ep;\r\nstruct dma_chan *chan;\r\nmight_sleep();\r\nrestart:\r\nspin_lock(&scif_info.rmalock);\r\nlist_for_each_safe(item, tmp, &scif_info.rma_tc) {\r\nwindow = list_entry(item, struct scif_window, list);\r\nep = (struct scif_endpt *)window->ep;\r\nchan = ep->rma_info.dma_chan;\r\nlist_del_init(&window->list);\r\nspin_unlock(&scif_info.rmalock);\r\nmutex_lock(&ep->rma_info.rma_lock);\r\nif (!chan || !scifdev_alive(ep) ||\r\n!scif_drain_dma_intr(ep->remote_dev->sdev,\r\nep->rma_info.dma_chan)) {\r\natomic_sub(window->nr_pages,\r\n&ep->rma_info.tcw_total_pages);\r\nscif_destroy_window(ep, window);\r\natomic_dec(&ep->rma_info.tcw_refcount);\r\n} else {\r\ndev_warn(&ep->remote_dev->sdev->dev,\r\n"DMA engine hung?\n");\r\n}\r\nmutex_unlock(&ep->rma_info.rma_lock);\r\ngoto restart;\r\n}\r\nspin_unlock(&scif_info.rmalock);\r\n}\r\nstatic inline\r\nvoid *_get_local_va(off_t off, struct scif_window *window, size_t len)\r\n{\r\nint page_nr = (off - window->offset) >> PAGE_SHIFT;\r\noff_t page_off = off & ~PAGE_MASK;\r\nvoid *va = NULL;\r\nif (window->type == SCIF_WINDOW_SELF) {\r\nstruct page **pages = window->pinned_pages->pages;\r\nva = page_address(pages[page_nr]) + page_off;\r\n}\r\nreturn va;\r\n}\r\nstatic inline\r\nvoid *ioremap_remote(off_t off, struct scif_window *window,\r\nsize_t len, struct scif_dev *dev,\r\nstruct scif_window_iter *iter)\r\n{\r\ndma_addr_t phys = scif_off_to_dma_addr(window, off, NULL, iter);\r\nif (!scifdev_self(dev) && window->type == SCIF_WINDOW_PEER &&\r\ndev->sdev->aper && !dev->sdev->card_rel_da)\r\nphys = phys - dev->sdev->aper->pa;\r\nreturn scif_ioremap(phys, len, dev);\r\n}\r\nstatic inline void\r\niounmap_remote(void *virt, size_t size, struct scif_copy_work *work)\r\n{\r\nscif_iounmap(virt, size, work->remote_dev);\r\n}\r\nstatic inline void\r\nscif_ordered_memcpy_toio(char *dst, const char *src, size_t count)\r\n{\r\nif (!count)\r\nreturn;\r\nmemcpy_toio((void __iomem __force *)dst, src, --count);\r\nwmb();\r\n*(dst + count) = *(src + count);\r\n}\r\nstatic inline void scif_unaligned_cpy_toio(char *dst, const char *src,\r\nsize_t count, bool ordered)\r\n{\r\nif (ordered)\r\nscif_ordered_memcpy_toio(dst, src, count);\r\nelse\r\nmemcpy_toio((void __iomem __force *)dst, src, count);\r\n}\r\nstatic inline\r\nvoid scif_ordered_memcpy_fromio(char *dst, const char *src, size_t count)\r\n{\r\nif (!count)\r\nreturn;\r\nmemcpy_fromio(dst, (void __iomem __force *)src, --count);\r\nrmb();\r\n*(dst + count) = *(src + count);\r\n}\r\nstatic inline void scif_unaligned_cpy_fromio(char *dst, const char *src,\r\nsize_t count, bool ordered)\r\n{\r\nif (ordered)\r\nscif_ordered_memcpy_fromio(dst, src, count);\r\nelse\r\nmemcpy_fromio(dst, (void __iomem __force *)src, count);\r\n}\r\ndma_addr_t scif_off_to_dma_addr(struct scif_window *window, s64 off,\r\nsize_t *nr_bytes, struct scif_window_iter *iter)\r\n{\r\nint i, page_nr;\r\ns64 start, end;\r\noff_t page_off;\r\nif (window->nr_pages == window->nr_contig_chunks) {\r\npage_nr = (off - window->offset) >> PAGE_SHIFT;\r\npage_off = off & ~PAGE_MASK;\r\nif (nr_bytes)\r\n*nr_bytes = PAGE_SIZE - page_off;\r\nreturn window->dma_addr[page_nr] | page_off;\r\n}\r\nif (iter) {\r\ni = iter->index;\r\nstart = iter->offset;\r\n} else {\r\ni = 0;\r\nstart = window->offset;\r\n}\r\nfor (; i < window->nr_contig_chunks; i++) {\r\nend = start + (window->num_pages[i] << PAGE_SHIFT);\r\nif (off >= start && off < end) {\r\nif (iter) {\r\niter->index = i;\r\niter->offset = start;\r\n}\r\nif (nr_bytes)\r\n*nr_bytes = end - off;\r\nreturn (window->dma_addr[i] + (off - start));\r\n}\r\nstart += (window->num_pages[i] << PAGE_SHIFT);\r\n}\r\ndev_err(scif_info.mdev.this_device,\r\n"%s %d BUG. Addr not found? window %p off 0x%llx\n",\r\n__func__, __LINE__, window, off);\r\nreturn SCIF_RMA_ERROR_CODE;\r\n}\r\nstatic void scif_rma_local_cpu_copy(s64 offset, struct scif_window *window,\r\nu8 *temp, size_t rem_len, bool to_temp)\r\n{\r\nvoid *window_virt;\r\nsize_t loop_len;\r\nint offset_in_page;\r\ns64 end_offset;\r\noffset_in_page = offset & ~PAGE_MASK;\r\nloop_len = PAGE_SIZE - offset_in_page;\r\nif (rem_len < loop_len)\r\nloop_len = rem_len;\r\nwindow_virt = _get_local_va(offset, window, loop_len);\r\nif (!window_virt)\r\nreturn;\r\nif (to_temp)\r\nmemcpy(temp, window_virt, loop_len);\r\nelse\r\nmemcpy(window_virt, temp, loop_len);\r\noffset += loop_len;\r\ntemp += loop_len;\r\nrem_len -= loop_len;\r\nend_offset = window->offset +\r\n(window->nr_pages << PAGE_SHIFT);\r\nwhile (rem_len) {\r\nif (offset == end_offset) {\r\nwindow = list_next_entry(window, list);\r\nend_offset = window->offset +\r\n(window->nr_pages << PAGE_SHIFT);\r\n}\r\nloop_len = min(PAGE_SIZE, rem_len);\r\nwindow_virt = _get_local_va(offset, window, loop_len);\r\nif (!window_virt)\r\nreturn;\r\nif (to_temp)\r\nmemcpy(temp, window_virt, loop_len);\r\nelse\r\nmemcpy(window_virt, temp, loop_len);\r\noffset += loop_len;\r\ntemp += loop_len;\r\nrem_len -= loop_len;\r\n}\r\n}\r\nstatic void scif_rma_completion_cb(void *data)\r\n{\r\nstruct scif_dma_comp_cb *comp_cb = data;\r\nif (comp_cb->dst_window)\r\nscif_rma_local_cpu_copy(comp_cb->dst_offset,\r\ncomp_cb->dst_window,\r\ncomp_cb->temp_buf +\r\ncomp_cb->header_padding,\r\ncomp_cb->len, false);\r\nscif_unmap_single(comp_cb->temp_phys, comp_cb->sdev,\r\nSCIF_KMEM_UNALIGNED_BUF_SIZE);\r\nif (comp_cb->is_cache)\r\nkmem_cache_free(unaligned_cache,\r\ncomp_cb->temp_buf_to_free);\r\nelse\r\nkfree(comp_cb->temp_buf_to_free);\r\n}\r\nstatic int\r\nscif_rma_list_dma_copy_unaligned(struct scif_copy_work *work,\r\nu8 *temp, struct dma_chan *chan,\r\nbool src_local)\r\n{\r\nstruct scif_dma_comp_cb *comp_cb = work->comp_cb;\r\ndma_addr_t window_dma_addr, temp_dma_addr;\r\ndma_addr_t temp_phys = comp_cb->temp_phys;\r\nsize_t loop_len, nr_contig_bytes = 0, remaining_len = work->len;\r\nint offset_in_ca, ret = 0;\r\ns64 end_offset, offset;\r\nstruct scif_window *window;\r\nvoid *window_virt_addr;\r\nsize_t tail_len;\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct dma_device *dev = chan->device;\r\ndma_cookie_t cookie;\r\nif (src_local) {\r\noffset = work->dst_offset;\r\nwindow = work->dst_window;\r\n} else {\r\noffset = work->src_offset;\r\nwindow = work->src_window;\r\n}\r\noffset_in_ca = offset & (L1_CACHE_BYTES - 1);\r\nif (offset_in_ca) {\r\nloop_len = L1_CACHE_BYTES - offset_in_ca;\r\nloop_len = min(loop_len, remaining_len);\r\nwindow_virt_addr = ioremap_remote(offset, window,\r\nloop_len,\r\nwork->remote_dev,\r\nNULL);\r\nif (!window_virt_addr)\r\nreturn -ENOMEM;\r\nif (src_local)\r\nscif_unaligned_cpy_toio(window_virt_addr, temp,\r\nloop_len,\r\nwork->ordered &&\r\n!(remaining_len - loop_len));\r\nelse\r\nscif_unaligned_cpy_fromio(temp, window_virt_addr,\r\nloop_len, work->ordered &&\r\n!(remaining_len - loop_len));\r\niounmap_remote(window_virt_addr, loop_len, work);\r\noffset += loop_len;\r\ntemp += loop_len;\r\ntemp_phys += loop_len;\r\nremaining_len -= loop_len;\r\n}\r\noffset_in_ca = offset & ~PAGE_MASK;\r\nend_offset = window->offset +\r\n(window->nr_pages << PAGE_SHIFT);\r\ntail_len = remaining_len & (L1_CACHE_BYTES - 1);\r\nremaining_len -= tail_len;\r\nwhile (remaining_len) {\r\nif (offset == end_offset) {\r\nwindow = list_next_entry(window, list);\r\nend_offset = window->offset +\r\n(window->nr_pages << PAGE_SHIFT);\r\n}\r\nif (scif_is_mgmt_node())\r\ntemp_dma_addr = temp_phys;\r\nelse\r\ntemp_dma_addr = (dma_addr_t)virt_to_phys(temp);\r\nwindow_dma_addr = scif_off_to_dma_addr(window, offset,\r\n&nr_contig_bytes,\r\nNULL);\r\nloop_len = min(nr_contig_bytes, remaining_len);\r\nif (src_local) {\r\nif (work->ordered && !tail_len &&\r\n!(remaining_len - loop_len) &&\r\nloop_len != L1_CACHE_BYTES) {\r\ntx =\r\ndev->device_prep_dma_memcpy(chan,\r\nwindow_dma_addr,\r\ntemp_dma_addr,\r\nloop_len -\r\nL1_CACHE_BYTES,\r\nDMA_PREP_FENCE);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndma_async_issue_pending(chan);\r\noffset += (loop_len - L1_CACHE_BYTES);\r\ntemp_dma_addr += (loop_len - L1_CACHE_BYTES);\r\nwindow_dma_addr += (loop_len - L1_CACHE_BYTES);\r\nremaining_len -= (loop_len - L1_CACHE_BYTES);\r\nloop_len = remaining_len;\r\ntx =\r\ndev->device_prep_dma_memcpy(chan,\r\nwindow_dma_addr,\r\ntemp_dma_addr,\r\nloop_len, 0);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndma_async_issue_pending(chan);\r\n} else {\r\ntx =\r\ndev->device_prep_dma_memcpy(chan,\r\nwindow_dma_addr,\r\ntemp_dma_addr,\r\nloop_len, 0);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndma_async_issue_pending(chan);\r\n}\r\n} else {\r\ntx = dev->device_prep_dma_memcpy(chan, temp_dma_addr,\r\nwindow_dma_addr, loop_len, 0);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndma_async_issue_pending(chan);\r\n}\r\nif (ret < 0)\r\ngoto err;\r\noffset += loop_len;\r\ntemp += loop_len;\r\ntemp_phys += loop_len;\r\nremaining_len -= loop_len;\r\noffset_in_ca = 0;\r\n}\r\nif (tail_len) {\r\nif (offset == end_offset) {\r\nwindow = list_next_entry(window, list);\r\nend_offset = window->offset +\r\n(window->nr_pages << PAGE_SHIFT);\r\n}\r\nwindow_virt_addr = ioremap_remote(offset, window, tail_len,\r\nwork->remote_dev,\r\nNULL);\r\nif (!window_virt_addr)\r\nreturn -ENOMEM;\r\nif (work->ordered) {\r\nstruct scif_dev *rdev = work->remote_dev;\r\nret = scif_drain_dma_intr(rdev->sdev, chan);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (src_local)\r\nscif_unaligned_cpy_toio(window_virt_addr, temp,\r\ntail_len, work->ordered);\r\nelse\r\nscif_unaligned_cpy_fromio(temp, window_virt_addr,\r\ntail_len, work->ordered);\r\niounmap_remote(window_virt_addr, tail_len, work);\r\n}\r\ntx = dev->device_prep_dma_memcpy(chan, 0, 0, 0, DMA_PREP_INTERRUPT);\r\nif (!tx) {\r\nret = -ENOMEM;\r\nreturn ret;\r\n}\r\ntx->callback = &scif_rma_completion_cb;\r\ntx->callback_param = comp_cb;\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\nreturn ret;\r\n}\r\ndma_async_issue_pending(chan);\r\nreturn 0;\r\nerr:\r\ndev_err(scif_info.mdev.this_device,\r\n"%s %d Desc Prog Failed ret %d\n",\r\n__func__, __LINE__, ret);\r\nreturn ret;\r\n}\r\nstatic int _scif_rma_list_dma_copy_aligned(struct scif_copy_work *work,\r\nstruct dma_chan *chan)\r\n{\r\ndma_addr_t src_dma_addr, dst_dma_addr;\r\nsize_t loop_len, remaining_len, src_contig_bytes = 0;\r\nsize_t dst_contig_bytes = 0;\r\nstruct scif_window_iter src_win_iter;\r\nstruct scif_window_iter dst_win_iter;\r\ns64 end_src_offset, end_dst_offset;\r\nstruct scif_window *src_window = work->src_window;\r\nstruct scif_window *dst_window = work->dst_window;\r\ns64 src_offset = work->src_offset, dst_offset = work->dst_offset;\r\nint ret = 0;\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct dma_device *dev = chan->device;\r\ndma_cookie_t cookie;\r\nremaining_len = work->len;\r\nscif_init_window_iter(src_window, &src_win_iter);\r\nscif_init_window_iter(dst_window, &dst_win_iter);\r\nend_src_offset = src_window->offset +\r\n(src_window->nr_pages << PAGE_SHIFT);\r\nend_dst_offset = dst_window->offset +\r\n(dst_window->nr_pages << PAGE_SHIFT);\r\nwhile (remaining_len) {\r\nif (src_offset == end_src_offset) {\r\nsrc_window = list_next_entry(src_window, list);\r\nend_src_offset = src_window->offset +\r\n(src_window->nr_pages << PAGE_SHIFT);\r\nscif_init_window_iter(src_window, &src_win_iter);\r\n}\r\nif (dst_offset == end_dst_offset) {\r\ndst_window = list_next_entry(dst_window, list);\r\nend_dst_offset = dst_window->offset +\r\n(dst_window->nr_pages << PAGE_SHIFT);\r\nscif_init_window_iter(dst_window, &dst_win_iter);\r\n}\r\nsrc_dma_addr = scif_off_to_dma_addr(src_window, src_offset,\r\n&src_contig_bytes,\r\n&src_win_iter);\r\ndst_dma_addr = scif_off_to_dma_addr(dst_window, dst_offset,\r\n&dst_contig_bytes,\r\n&dst_win_iter);\r\nloop_len = min(src_contig_bytes, dst_contig_bytes);\r\nloop_len = min(loop_len, remaining_len);\r\nif (work->ordered && !(remaining_len - loop_len)) {\r\ntx = dev->device_prep_dma_memcpy(chan, dst_dma_addr,\r\nsrc_dma_addr,\r\nloop_len - 1,\r\nDMA_PREP_FENCE);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nsrc_offset += (loop_len - 1);\r\ndst_offset += (loop_len - 1);\r\nsrc_dma_addr += (loop_len - 1);\r\ndst_dma_addr += (loop_len - 1);\r\nremaining_len -= (loop_len - 1);\r\nloop_len = remaining_len;\r\ntx = dev->device_prep_dma_memcpy(chan, dst_dma_addr,\r\nsrc_dma_addr, loop_len, 0);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndma_async_issue_pending(chan);\r\n} else {\r\ntx = dev->device_prep_dma_memcpy(chan, dst_dma_addr,\r\nsrc_dma_addr, loop_len, 0);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\n}\r\nsrc_offset += loop_len;\r\ndst_offset += loop_len;\r\nremaining_len -= loop_len;\r\n}\r\nreturn ret;\r\nerr:\r\ndev_err(scif_info.mdev.this_device,\r\n"%s %d Desc Prog Failed ret %d\n",\r\n__func__, __LINE__, ret);\r\nreturn ret;\r\n}\r\nstatic int scif_rma_list_dma_copy_aligned(struct scif_copy_work *work,\r\nstruct dma_chan *chan)\r\n{\r\ndma_addr_t src_dma_addr, dst_dma_addr;\r\nsize_t loop_len, remaining_len, tail_len, src_contig_bytes = 0;\r\nsize_t dst_contig_bytes = 0;\r\nint src_cache_off;\r\ns64 end_src_offset, end_dst_offset;\r\nstruct scif_window_iter src_win_iter;\r\nstruct scif_window_iter dst_win_iter;\r\nvoid *src_virt, *dst_virt;\r\nstruct scif_window *src_window = work->src_window;\r\nstruct scif_window *dst_window = work->dst_window;\r\ns64 src_offset = work->src_offset, dst_offset = work->dst_offset;\r\nint ret = 0;\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct dma_device *dev = chan->device;\r\ndma_cookie_t cookie;\r\nremaining_len = work->len;\r\nscif_init_window_iter(src_window, &src_win_iter);\r\nscif_init_window_iter(dst_window, &dst_win_iter);\r\nsrc_cache_off = src_offset & (L1_CACHE_BYTES - 1);\r\nif (src_cache_off != 0) {\r\nloop_len = L1_CACHE_BYTES - src_cache_off;\r\nloop_len = min(loop_len, remaining_len);\r\nsrc_dma_addr = __scif_off_to_dma_addr(src_window, src_offset);\r\ndst_dma_addr = __scif_off_to_dma_addr(dst_window, dst_offset);\r\nif (src_window->type == SCIF_WINDOW_SELF)\r\nsrc_virt = _get_local_va(src_offset, src_window,\r\nloop_len);\r\nelse\r\nsrc_virt = ioremap_remote(src_offset, src_window,\r\nloop_len,\r\nwork->remote_dev, NULL);\r\nif (!src_virt)\r\nreturn -ENOMEM;\r\nif (dst_window->type == SCIF_WINDOW_SELF)\r\ndst_virt = _get_local_va(dst_offset, dst_window,\r\nloop_len);\r\nelse\r\ndst_virt = ioremap_remote(dst_offset, dst_window,\r\nloop_len,\r\nwork->remote_dev, NULL);\r\nif (!dst_virt) {\r\nif (src_window->type != SCIF_WINDOW_SELF)\r\niounmap_remote(src_virt, loop_len, work);\r\nreturn -ENOMEM;\r\n}\r\nif (src_window->type == SCIF_WINDOW_SELF)\r\nscif_unaligned_cpy_toio(dst_virt, src_virt, loop_len,\r\nremaining_len == loop_len ?\r\nwork->ordered : false);\r\nelse\r\nscif_unaligned_cpy_fromio(dst_virt, src_virt, loop_len,\r\nremaining_len == loop_len ?\r\nwork->ordered : false);\r\nif (src_window->type != SCIF_WINDOW_SELF)\r\niounmap_remote(src_virt, loop_len, work);\r\nif (dst_window->type != SCIF_WINDOW_SELF)\r\niounmap_remote(dst_virt, loop_len, work);\r\nsrc_offset += loop_len;\r\ndst_offset += loop_len;\r\nremaining_len -= loop_len;\r\n}\r\nend_src_offset = src_window->offset +\r\n(src_window->nr_pages << PAGE_SHIFT);\r\nend_dst_offset = dst_window->offset +\r\n(dst_window->nr_pages << PAGE_SHIFT);\r\ntail_len = remaining_len & (L1_CACHE_BYTES - 1);\r\nremaining_len -= tail_len;\r\nwhile (remaining_len) {\r\nif (src_offset == end_src_offset) {\r\nsrc_window = list_next_entry(src_window, list);\r\nend_src_offset = src_window->offset +\r\n(src_window->nr_pages << PAGE_SHIFT);\r\nscif_init_window_iter(src_window, &src_win_iter);\r\n}\r\nif (dst_offset == end_dst_offset) {\r\ndst_window = list_next_entry(dst_window, list);\r\nend_dst_offset = dst_window->offset +\r\n(dst_window->nr_pages << PAGE_SHIFT);\r\nscif_init_window_iter(dst_window, &dst_win_iter);\r\n}\r\nsrc_dma_addr = scif_off_to_dma_addr(src_window, src_offset,\r\n&src_contig_bytes,\r\n&src_win_iter);\r\ndst_dma_addr = scif_off_to_dma_addr(dst_window, dst_offset,\r\n&dst_contig_bytes,\r\n&dst_win_iter);\r\nloop_len = min(src_contig_bytes, dst_contig_bytes);\r\nloop_len = min(loop_len, remaining_len);\r\nif (work->ordered && !tail_len &&\r\n!(remaining_len - loop_len)) {\r\ntx = dev->device_prep_dma_memcpy(chan, dst_dma_addr,\r\nsrc_dma_addr,\r\nloop_len -\r\nL1_CACHE_BYTES,\r\nDMA_PREP_FENCE);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndma_async_issue_pending(chan);\r\nsrc_offset += (loop_len - L1_CACHE_BYTES);\r\ndst_offset += (loop_len - L1_CACHE_BYTES);\r\nsrc_dma_addr += (loop_len - L1_CACHE_BYTES);\r\ndst_dma_addr += (loop_len - L1_CACHE_BYTES);\r\nremaining_len -= (loop_len - L1_CACHE_BYTES);\r\nloop_len = remaining_len;\r\ntx = dev->device_prep_dma_memcpy(chan, dst_dma_addr,\r\nsrc_dma_addr,\r\nloop_len, 0);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndma_async_issue_pending(chan);\r\n} else {\r\ntx = dev->device_prep_dma_memcpy(chan, dst_dma_addr,\r\nsrc_dma_addr,\r\nloop_len, 0);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ncookie = tx->tx_submit(tx);\r\nif (dma_submit_error(cookie)) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndma_async_issue_pending(chan);\r\n}\r\nsrc_offset += loop_len;\r\ndst_offset += loop_len;\r\nremaining_len -= loop_len;\r\n}\r\nremaining_len = tail_len;\r\nif (remaining_len) {\r\nloop_len = remaining_len;\r\nif (src_offset == end_src_offset)\r\nsrc_window = list_next_entry(src_window, list);\r\nif (dst_offset == end_dst_offset)\r\ndst_window = list_next_entry(dst_window, list);\r\nsrc_dma_addr = __scif_off_to_dma_addr(src_window, src_offset);\r\ndst_dma_addr = __scif_off_to_dma_addr(dst_window, dst_offset);\r\nif (work->ordered) {\r\nstruct scif_dev *rdev = work->remote_dev;\r\nret = scif_drain_dma_poll(rdev->sdev, chan);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (src_window->type == SCIF_WINDOW_SELF)\r\nsrc_virt = _get_local_va(src_offset, src_window,\r\nloop_len);\r\nelse\r\nsrc_virt = ioremap_remote(src_offset, src_window,\r\nloop_len,\r\nwork->remote_dev, NULL);\r\nif (!src_virt)\r\nreturn -ENOMEM;\r\nif (dst_window->type == SCIF_WINDOW_SELF)\r\ndst_virt = _get_local_va(dst_offset, dst_window,\r\nloop_len);\r\nelse\r\ndst_virt = ioremap_remote(dst_offset, dst_window,\r\nloop_len,\r\nwork->remote_dev, NULL);\r\nif (!dst_virt) {\r\nif (src_window->type != SCIF_WINDOW_SELF)\r\niounmap_remote(src_virt, loop_len, work);\r\nreturn -ENOMEM;\r\n}\r\nif (src_window->type == SCIF_WINDOW_SELF)\r\nscif_unaligned_cpy_toio(dst_virt, src_virt, loop_len,\r\nwork->ordered);\r\nelse\r\nscif_unaligned_cpy_fromio(dst_virt, src_virt,\r\nloop_len, work->ordered);\r\nif (src_window->type != SCIF_WINDOW_SELF)\r\niounmap_remote(src_virt, loop_len, work);\r\nif (dst_window->type != SCIF_WINDOW_SELF)\r\niounmap_remote(dst_virt, loop_len, work);\r\nremaining_len -= loop_len;\r\n}\r\nreturn ret;\r\nerr:\r\ndev_err(scif_info.mdev.this_device,\r\n"%s %d Desc Prog Failed ret %d\n",\r\n__func__, __LINE__, ret);\r\nreturn ret;\r\n}\r\nstatic int scif_rma_list_cpu_copy(struct scif_copy_work *work)\r\n{\r\nvoid *src_virt, *dst_virt;\r\nsize_t loop_len, remaining_len;\r\nint src_page_off, dst_page_off;\r\ns64 src_offset = work->src_offset, dst_offset = work->dst_offset;\r\nstruct scif_window *src_window = work->src_window;\r\nstruct scif_window *dst_window = work->dst_window;\r\ns64 end_src_offset, end_dst_offset;\r\nint ret = 0;\r\nstruct scif_window_iter src_win_iter;\r\nstruct scif_window_iter dst_win_iter;\r\nremaining_len = work->len;\r\nscif_init_window_iter(src_window, &src_win_iter);\r\nscif_init_window_iter(dst_window, &dst_win_iter);\r\nwhile (remaining_len) {\r\nsrc_page_off = src_offset & ~PAGE_MASK;\r\ndst_page_off = dst_offset & ~PAGE_MASK;\r\nloop_len = min(PAGE_SIZE -\r\nmax(src_page_off, dst_page_off),\r\nremaining_len);\r\nif (src_window->type == SCIF_WINDOW_SELF)\r\nsrc_virt = _get_local_va(src_offset, src_window,\r\nloop_len);\r\nelse\r\nsrc_virt = ioremap_remote(src_offset, src_window,\r\nloop_len,\r\nwork->remote_dev,\r\n&src_win_iter);\r\nif (!src_virt) {\r\nret = -ENOMEM;\r\ngoto error;\r\n}\r\nif (dst_window->type == SCIF_WINDOW_SELF)\r\ndst_virt = _get_local_va(dst_offset, dst_window,\r\nloop_len);\r\nelse\r\ndst_virt = ioremap_remote(dst_offset, dst_window,\r\nloop_len,\r\nwork->remote_dev,\r\n&dst_win_iter);\r\nif (!dst_virt) {\r\nif (src_window->type == SCIF_WINDOW_PEER)\r\niounmap_remote(src_virt, loop_len, work);\r\nret = -ENOMEM;\r\ngoto error;\r\n}\r\nif (work->loopback) {\r\nmemcpy(dst_virt, src_virt, loop_len);\r\n} else {\r\nif (src_window->type == SCIF_WINDOW_SELF)\r\nmemcpy_toio((void __iomem __force *)dst_virt,\r\nsrc_virt, loop_len);\r\nelse\r\nmemcpy_fromio(dst_virt,\r\n(void __iomem __force *)src_virt,\r\nloop_len);\r\n}\r\nif (src_window->type == SCIF_WINDOW_PEER)\r\niounmap_remote(src_virt, loop_len, work);\r\nif (dst_window->type == SCIF_WINDOW_PEER)\r\niounmap_remote(dst_virt, loop_len, work);\r\nsrc_offset += loop_len;\r\ndst_offset += loop_len;\r\nremaining_len -= loop_len;\r\nif (remaining_len) {\r\nend_src_offset = src_window->offset +\r\n(src_window->nr_pages << PAGE_SHIFT);\r\nend_dst_offset = dst_window->offset +\r\n(dst_window->nr_pages << PAGE_SHIFT);\r\nif (src_offset == end_src_offset) {\r\nsrc_window = list_next_entry(src_window, list);\r\nscif_init_window_iter(src_window,\r\n&src_win_iter);\r\n}\r\nif (dst_offset == end_dst_offset) {\r\ndst_window = list_next_entry(dst_window, list);\r\nscif_init_window_iter(dst_window,\r\n&dst_win_iter);\r\n}\r\n}\r\n}\r\nerror:\r\nreturn ret;\r\n}\r\nstatic int scif_rma_list_dma_copy_wrapper(struct scif_endpt *epd,\r\nstruct scif_copy_work *work,\r\nstruct dma_chan *chan, off_t loffset)\r\n{\r\nint src_cache_off, dst_cache_off;\r\ns64 src_offset = work->src_offset, dst_offset = work->dst_offset;\r\nu8 *temp = NULL;\r\nbool src_local = true, dst_local = false;\r\nstruct scif_dma_comp_cb *comp_cb;\r\ndma_addr_t src_dma_addr, dst_dma_addr;\r\nint err;\r\nif (is_dma_copy_aligned(chan->device, 1, 1, 1))\r\nreturn _scif_rma_list_dma_copy_aligned(work, chan);\r\nsrc_cache_off = src_offset & (L1_CACHE_BYTES - 1);\r\ndst_cache_off = dst_offset & (L1_CACHE_BYTES - 1);\r\nif (dst_cache_off == src_cache_off)\r\nreturn scif_rma_list_dma_copy_aligned(work, chan);\r\nif (work->loopback)\r\nreturn scif_rma_list_cpu_copy(work);\r\nsrc_dma_addr = __scif_off_to_dma_addr(work->src_window, src_offset);\r\ndst_dma_addr = __scif_off_to_dma_addr(work->dst_window, dst_offset);\r\nsrc_local = work->src_window->type == SCIF_WINDOW_SELF;\r\ndst_local = work->dst_window->type == SCIF_WINDOW_SELF;\r\ndst_local = dst_local;\r\ncomp_cb = kzalloc(sizeof(*comp_cb), GFP_KERNEL);\r\nif (!comp_cb)\r\ngoto error;\r\nwork->comp_cb = comp_cb;\r\ncomp_cb->cb_cookie = comp_cb;\r\ncomp_cb->dma_completion_func = &scif_rma_completion_cb;\r\nif (work->len + (L1_CACHE_BYTES << 1) < SCIF_KMEM_UNALIGNED_BUF_SIZE) {\r\ncomp_cb->is_cache = false;\r\ntemp = kmalloc(work->len + (L1_CACHE_BYTES << 1),\r\nGFP_KERNEL);\r\nif (!temp)\r\ngoto free_comp_cb;\r\ncomp_cb->temp_buf_to_free = temp;\r\nif (!IS_ALIGNED((u64)temp, L1_CACHE_BYTES))\r\ntemp = PTR_ALIGN(temp, L1_CACHE_BYTES);\r\n} else {\r\ncomp_cb->is_cache = true;\r\ntemp = kmem_cache_alloc(unaligned_cache, GFP_KERNEL);\r\nif (!temp)\r\ngoto free_comp_cb;\r\ncomp_cb->temp_buf_to_free = temp;\r\n}\r\nif (src_local) {\r\ntemp += dst_cache_off;\r\nscif_rma_local_cpu_copy(work->src_offset, work->src_window,\r\ntemp, work->len, true);\r\n} else {\r\ncomp_cb->dst_window = work->dst_window;\r\ncomp_cb->dst_offset = work->dst_offset;\r\nwork->src_offset = work->src_offset - src_cache_off;\r\ncomp_cb->len = work->len;\r\nwork->len = ALIGN(work->len + src_cache_off, L1_CACHE_BYTES);\r\ncomp_cb->header_padding = src_cache_off;\r\n}\r\ncomp_cb->temp_buf = temp;\r\nerr = scif_map_single(&comp_cb->temp_phys, temp,\r\nwork->remote_dev, SCIF_KMEM_UNALIGNED_BUF_SIZE);\r\nif (err)\r\ngoto free_temp_buf;\r\ncomp_cb->sdev = work->remote_dev;\r\nif (scif_rma_list_dma_copy_unaligned(work, temp, chan, src_local) < 0)\r\ngoto free_temp_buf;\r\nif (!src_local)\r\nwork->fence_type = SCIF_DMA_INTR;\r\nreturn 0;\r\nfree_temp_buf:\r\nif (comp_cb->is_cache)\r\nkmem_cache_free(unaligned_cache, comp_cb->temp_buf_to_free);\r\nelse\r\nkfree(comp_cb->temp_buf_to_free);\r\nfree_comp_cb:\r\nkfree(comp_cb);\r\nerror:\r\nreturn -ENOMEM;\r\n}\r\nstatic int scif_rma_copy(scif_epd_t epd, off_t loffset, unsigned long addr,\r\nsize_t len, off_t roffset, int flags,\r\nenum scif_rma_dir dir, bool last_chunk)\r\n{\r\nstruct scif_endpt *ep = (struct scif_endpt *)epd;\r\nstruct scif_rma_req remote_req;\r\nstruct scif_rma_req req;\r\nstruct scif_window *local_window = NULL;\r\nstruct scif_window *remote_window = NULL;\r\nstruct scif_copy_work copy_work;\r\nbool loopback;\r\nint err = 0;\r\nstruct dma_chan *chan;\r\nstruct scif_mmu_notif *mmn = NULL;\r\nbool cache = false;\r\nstruct device *spdev;\r\nerr = scif_verify_epd(ep);\r\nif (err)\r\nreturn err;\r\nif (flags && !(flags & (SCIF_RMA_USECPU | SCIF_RMA_USECACHE |\r\nSCIF_RMA_SYNC | SCIF_RMA_ORDERED)))\r\nreturn -EINVAL;\r\nloopback = scifdev_self(ep->remote_dev) ? true : false;\r\ncopy_work.fence_type = ((flags & SCIF_RMA_SYNC) && last_chunk) ?\r\nSCIF_DMA_POLL : 0;\r\ncopy_work.ordered = !!((flags & SCIF_RMA_ORDERED) && last_chunk);\r\nif (loopback && scif_is_mgmt_node()) {\r\nflags |= SCIF_RMA_USECPU;\r\ncopy_work.fence_type = 0x0;\r\n}\r\ncache = scif_is_set_reg_cache(flags);\r\nremote_req.out_window = &remote_window;\r\nremote_req.offset = roffset;\r\nremote_req.nr_bytes = len;\r\nremote_req.prot = dir == SCIF_LOCAL_TO_REMOTE ? VM_WRITE : VM_READ;\r\nremote_req.type = SCIF_WINDOW_PARTIAL;\r\nremote_req.head = &ep->rma_info.remote_reg_list;\r\nspdev = scif_get_peer_dev(ep->remote_dev);\r\nif (IS_ERR(spdev)) {\r\nerr = PTR_ERR(spdev);\r\nreturn err;\r\n}\r\nif (addr && cache) {\r\nmutex_lock(&ep->rma_info.mmn_lock);\r\nmmn = scif_find_mmu_notifier(current->mm, &ep->rma_info);\r\nif (!mmn)\r\nmmn = scif_add_mmu_notifier(current->mm, ep);\r\nmutex_unlock(&ep->rma_info.mmn_lock);\r\nif (IS_ERR(mmn)) {\r\nscif_put_peer_dev(spdev);\r\nreturn PTR_ERR(mmn);\r\n}\r\ncache = cache && !scif_rma_tc_can_cache(ep, len);\r\n}\r\nmutex_lock(&ep->rma_info.rma_lock);\r\nif (addr) {\r\nreq.out_window = &local_window;\r\nreq.nr_bytes = ALIGN(len + (addr & ~PAGE_MASK),\r\nPAGE_SIZE);\r\nreq.va_for_temp = addr & PAGE_MASK;\r\nreq.prot = (dir == SCIF_LOCAL_TO_REMOTE ?\r\nVM_READ : VM_WRITE | VM_READ);\r\nif (mmn) {\r\nspin_lock(&ep->rma_info.tc_lock);\r\nreq.head = &mmn->tc_reg_list;\r\nerr = scif_query_tcw(ep, &req);\r\nspin_unlock(&ep->rma_info.tc_lock);\r\n}\r\nif (!mmn || err) {\r\nerr = scif_register_temp(epd, req.va_for_temp,\r\nreq.nr_bytes, req.prot,\r\n&loffset, &local_window);\r\nif (err) {\r\nmutex_unlock(&ep->rma_info.rma_lock);\r\ngoto error;\r\n}\r\nif (!cache)\r\ngoto skip_cache;\r\natomic_inc(&ep->rma_info.tcw_refcount);\r\natomic_add_return(local_window->nr_pages,\r\n&ep->rma_info.tcw_total_pages);\r\nif (mmn) {\r\nspin_lock(&ep->rma_info.tc_lock);\r\nscif_insert_tcw(local_window,\r\n&mmn->tc_reg_list);\r\nspin_unlock(&ep->rma_info.tc_lock);\r\n}\r\n}\r\nskip_cache:\r\nloffset = local_window->offset +\r\n(addr - local_window->va_for_temp);\r\n} else {\r\nreq.out_window = &local_window;\r\nreq.offset = loffset;\r\nreq.prot = dir == SCIF_LOCAL_TO_REMOTE ? VM_READ : VM_WRITE;\r\nreq.nr_bytes = len;\r\nreq.type = SCIF_WINDOW_PARTIAL;\r\nreq.head = &ep->rma_info.reg_list;\r\nerr = scif_query_window(&req);\r\nif (err) {\r\nmutex_unlock(&ep->rma_info.rma_lock);\r\ngoto error;\r\n}\r\n}\r\nerr = scif_query_window(&remote_req);\r\nif (err) {\r\nmutex_unlock(&ep->rma_info.rma_lock);\r\ngoto error;\r\n}\r\ncopy_work.len = len;\r\ncopy_work.loopback = loopback;\r\ncopy_work.remote_dev = ep->remote_dev;\r\nif (dir == SCIF_LOCAL_TO_REMOTE) {\r\ncopy_work.src_offset = loffset;\r\ncopy_work.src_window = local_window;\r\ncopy_work.dst_offset = roffset;\r\ncopy_work.dst_window = remote_window;\r\n} else {\r\ncopy_work.src_offset = roffset;\r\ncopy_work.src_window = remote_window;\r\ncopy_work.dst_offset = loffset;\r\ncopy_work.dst_window = local_window;\r\n}\r\nif (flags & SCIF_RMA_USECPU) {\r\nscif_rma_list_cpu_copy(&copy_work);\r\n} else {\r\nchan = ep->rma_info.dma_chan;\r\nerr = scif_rma_list_dma_copy_wrapper(epd, &copy_work,\r\nchan, loffset);\r\n}\r\nif (addr && !cache)\r\natomic_inc(&ep->rma_info.tw_refcount);\r\nmutex_unlock(&ep->rma_info.rma_lock);\r\nif (last_chunk) {\r\nstruct scif_dev *rdev = ep->remote_dev;\r\nif (copy_work.fence_type == SCIF_DMA_POLL)\r\nerr = scif_drain_dma_poll(rdev->sdev,\r\nep->rma_info.dma_chan);\r\nelse if (copy_work.fence_type == SCIF_DMA_INTR)\r\nerr = scif_drain_dma_intr(rdev->sdev,\r\nep->rma_info.dma_chan);\r\n}\r\nif (addr && !cache)\r\nscif_queue_for_cleanup(local_window, &scif_info.rma);\r\nscif_put_peer_dev(spdev);\r\nreturn err;\r\nerror:\r\nif (err) {\r\nif (addr && local_window && !cache)\r\nscif_destroy_window(ep, local_window);\r\ndev_err(scif_info.mdev.this_device,\r\n"%s %d err %d len 0x%lx\n",\r\n__func__, __LINE__, err, len);\r\n}\r\nscif_put_peer_dev(spdev);\r\nreturn err;\r\n}\r\nint scif_readfrom(scif_epd_t epd, off_t loffset, size_t len,\r\noff_t roffset, int flags)\r\n{\r\nint err;\r\ndev_dbg(scif_info.mdev.this_device,\r\n"SCIFAPI readfrom: ep %p loffset 0x%lx len 0x%lx offset 0x%lx flags 0x%x\n",\r\nepd, loffset, len, roffset, flags);\r\nif (scif_unaligned(loffset, roffset)) {\r\nwhile (len > SCIF_MAX_UNALIGNED_BUF_SIZE) {\r\nerr = scif_rma_copy(epd, loffset, 0x0,\r\nSCIF_MAX_UNALIGNED_BUF_SIZE,\r\nroffset, flags,\r\nSCIF_REMOTE_TO_LOCAL, false);\r\nif (err)\r\ngoto readfrom_err;\r\nloffset += SCIF_MAX_UNALIGNED_BUF_SIZE;\r\nroffset += SCIF_MAX_UNALIGNED_BUF_SIZE;\r\nlen -= SCIF_MAX_UNALIGNED_BUF_SIZE;\r\n}\r\n}\r\nerr = scif_rma_copy(epd, loffset, 0x0, len,\r\nroffset, flags, SCIF_REMOTE_TO_LOCAL, true);\r\nreadfrom_err:\r\nreturn err;\r\n}\r\nint scif_writeto(scif_epd_t epd, off_t loffset, size_t len,\r\noff_t roffset, int flags)\r\n{\r\nint err;\r\ndev_dbg(scif_info.mdev.this_device,\r\n"SCIFAPI writeto: ep %p loffset 0x%lx len 0x%lx roffset 0x%lx flags 0x%x\n",\r\nepd, loffset, len, roffset, flags);\r\nif (scif_unaligned(loffset, roffset)) {\r\nwhile (len > SCIF_MAX_UNALIGNED_BUF_SIZE) {\r\nerr = scif_rma_copy(epd, loffset, 0x0,\r\nSCIF_MAX_UNALIGNED_BUF_SIZE,\r\nroffset, flags,\r\nSCIF_LOCAL_TO_REMOTE, false);\r\nif (err)\r\ngoto writeto_err;\r\nloffset += SCIF_MAX_UNALIGNED_BUF_SIZE;\r\nroffset += SCIF_MAX_UNALIGNED_BUF_SIZE;\r\nlen -= SCIF_MAX_UNALIGNED_BUF_SIZE;\r\n}\r\n}\r\nerr = scif_rma_copy(epd, loffset, 0x0, len,\r\nroffset, flags, SCIF_LOCAL_TO_REMOTE, true);\r\nwriteto_err:\r\nreturn err;\r\n}\r\nint scif_vreadfrom(scif_epd_t epd, void *addr, size_t len,\r\noff_t roffset, int flags)\r\n{\r\nint err;\r\ndev_dbg(scif_info.mdev.this_device,\r\n"SCIFAPI vreadfrom: ep %p addr %p len 0x%lx roffset 0x%lx flags 0x%x\n",\r\nepd, addr, len, roffset, flags);\r\nif (scif_unaligned((off_t __force)addr, roffset)) {\r\nif (len > SCIF_MAX_UNALIGNED_BUF_SIZE)\r\nflags &= ~SCIF_RMA_USECACHE;\r\nwhile (len > SCIF_MAX_UNALIGNED_BUF_SIZE) {\r\nerr = scif_rma_copy(epd, 0, (u64)addr,\r\nSCIF_MAX_UNALIGNED_BUF_SIZE,\r\nroffset, flags,\r\nSCIF_REMOTE_TO_LOCAL, false);\r\nif (err)\r\ngoto vreadfrom_err;\r\naddr += SCIF_MAX_UNALIGNED_BUF_SIZE;\r\nroffset += SCIF_MAX_UNALIGNED_BUF_SIZE;\r\nlen -= SCIF_MAX_UNALIGNED_BUF_SIZE;\r\n}\r\n}\r\nerr = scif_rma_copy(epd, 0, (u64)addr, len,\r\nroffset, flags, SCIF_REMOTE_TO_LOCAL, true);\r\nvreadfrom_err:\r\nreturn err;\r\n}\r\nint scif_vwriteto(scif_epd_t epd, void *addr, size_t len,\r\noff_t roffset, int flags)\r\n{\r\nint err;\r\ndev_dbg(scif_info.mdev.this_device,\r\n"SCIFAPI vwriteto: ep %p addr %p len 0x%lx roffset 0x%lx flags 0x%x\n",\r\nepd, addr, len, roffset, flags);\r\nif (scif_unaligned((off_t __force)addr, roffset)) {\r\nif (len > SCIF_MAX_UNALIGNED_BUF_SIZE)\r\nflags &= ~SCIF_RMA_USECACHE;\r\nwhile (len > SCIF_MAX_UNALIGNED_BUF_SIZE) {\r\nerr = scif_rma_copy(epd, 0, (u64)addr,\r\nSCIF_MAX_UNALIGNED_BUF_SIZE,\r\nroffset, flags,\r\nSCIF_LOCAL_TO_REMOTE, false);\r\nif (err)\r\ngoto vwriteto_err;\r\naddr += SCIF_MAX_UNALIGNED_BUF_SIZE;\r\nroffset += SCIF_MAX_UNALIGNED_BUF_SIZE;\r\nlen -= SCIF_MAX_UNALIGNED_BUF_SIZE;\r\n}\r\n}\r\nerr = scif_rma_copy(epd, 0, (u64)addr, len,\r\nroffset, flags, SCIF_LOCAL_TO_REMOTE, true);\r\nvwriteto_err:\r\nreturn err;\r\n}
