static long mm_iommu_adjust_locked_vm(struct mm_struct *mm,\r\nunsigned long npages, bool incr)\r\n{\r\nlong ret = 0, locked, lock_limit;\r\nif (!npages)\r\nreturn 0;\r\ndown_write(&mm->mmap_sem);\r\nif (incr) {\r\nlocked = mm->locked_vm + npages;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif (locked > lock_limit && !capable(CAP_IPC_LOCK))\r\nret = -ENOMEM;\r\nelse\r\nmm->locked_vm += npages;\r\n} else {\r\nif (WARN_ON_ONCE(npages > mm->locked_vm))\r\nnpages = mm->locked_vm;\r\nmm->locked_vm -= npages;\r\n}\r\npr_debug("[%d] RLIMIT_MEMLOCK HASH64 %c%ld %ld/%ld\n",\r\ncurrent ? current->pid : 0,\r\nincr ? '+' : '-',\r\nnpages << PAGE_SHIFT,\r\nmm->locked_vm << PAGE_SHIFT,\r\nrlimit(RLIMIT_MEMLOCK));\r\nup_write(&mm->mmap_sem);\r\nreturn ret;\r\n}\r\nbool mm_iommu_preregistered(struct mm_struct *mm)\r\n{\r\nreturn !list_empty(&mm->context.iommu_group_mem_list);\r\n}\r\nstruct page *new_iommu_non_cma_page(struct page *page, unsigned long private,\r\nint **resultp)\r\n{\r\ngfp_t gfp_mask = GFP_USER;\r\nstruct page *new_page;\r\nif (PageCompound(page))\r\nreturn NULL;\r\nif (PageHighMem(page))\r\ngfp_mask |= __GFP_HIGHMEM;\r\nnew_page = alloc_page(gfp_mask | __GFP_NORETRY | __GFP_NOWARN);\r\nreturn new_page;\r\n}\r\nstatic int mm_iommu_move_page_from_cma(struct page *page)\r\n{\r\nint ret = 0;\r\nLIST_HEAD(cma_migrate_pages);\r\nif (PageCompound(page))\r\nreturn -EBUSY;\r\nlru_add_drain();\r\nret = isolate_lru_page(page);\r\nif (ret)\r\nreturn ret;\r\nlist_add(&page->lru, &cma_migrate_pages);\r\nput_page(page);\r\nret = migrate_pages(&cma_migrate_pages, new_iommu_non_cma_page,\r\nNULL, 0, MIGRATE_SYNC, MR_CMA);\r\nif (ret) {\r\nif (!list_empty(&cma_migrate_pages))\r\nputback_movable_pages(&cma_migrate_pages);\r\n}\r\nreturn 0;\r\n}\r\nlong mm_iommu_get(struct mm_struct *mm, unsigned long ua, unsigned long entries,\r\nstruct mm_iommu_table_group_mem_t **pmem)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem;\r\nlong i, j, ret = 0, locked_entries = 0;\r\nstruct page *page = NULL;\r\nmutex_lock(&mem_list_mutex);\r\nlist_for_each_entry_rcu(mem, &mm->context.iommu_group_mem_list,\r\nnext) {\r\nif ((mem->ua == ua) && (mem->entries == entries)) {\r\n++mem->used;\r\n*pmem = mem;\r\ngoto unlock_exit;\r\n}\r\nif ((mem->ua < (ua + (entries << PAGE_SHIFT))) &&\r\n(ua < (mem->ua +\r\n(mem->entries << PAGE_SHIFT)))) {\r\nret = -EINVAL;\r\ngoto unlock_exit;\r\n}\r\n}\r\nret = mm_iommu_adjust_locked_vm(mm, entries, true);\r\nif (ret)\r\ngoto unlock_exit;\r\nlocked_entries = entries;\r\nmem = kzalloc(sizeof(*mem), GFP_KERNEL);\r\nif (!mem) {\r\nret = -ENOMEM;\r\ngoto unlock_exit;\r\n}\r\nmem->hpas = vzalloc(entries * sizeof(mem->hpas[0]));\r\nif (!mem->hpas) {\r\nkfree(mem);\r\nret = -ENOMEM;\r\ngoto unlock_exit;\r\n}\r\nfor (i = 0; i < entries; ++i) {\r\nif (1 != get_user_pages_fast(ua + (i << PAGE_SHIFT),\r\n1, 1, &page)) {\r\nret = -EFAULT;\r\nfor (j = 0; j < i; ++j)\r\nput_page(pfn_to_page(mem->hpas[j] >>\r\nPAGE_SHIFT));\r\nvfree(mem->hpas);\r\nkfree(mem);\r\ngoto unlock_exit;\r\n}\r\nif (is_migrate_cma_page(page)) {\r\nif (mm_iommu_move_page_from_cma(page))\r\ngoto populate;\r\nif (1 != get_user_pages_fast(ua + (i << PAGE_SHIFT),\r\n1, 1,\r\n&page)) {\r\nret = -EFAULT;\r\nfor (j = 0; j < i; ++j)\r\nput_page(pfn_to_page(mem->hpas[j] >>\r\nPAGE_SHIFT));\r\nvfree(mem->hpas);\r\nkfree(mem);\r\ngoto unlock_exit;\r\n}\r\n}\r\npopulate:\r\nmem->hpas[i] = page_to_pfn(page) << PAGE_SHIFT;\r\n}\r\natomic64_set(&mem->mapped, 1);\r\nmem->used = 1;\r\nmem->ua = ua;\r\nmem->entries = entries;\r\n*pmem = mem;\r\nlist_add_rcu(&mem->next, &mm->context.iommu_group_mem_list);\r\nunlock_exit:\r\nif (locked_entries && ret)\r\nmm_iommu_adjust_locked_vm(mm, locked_entries, false);\r\nmutex_unlock(&mem_list_mutex);\r\nreturn ret;\r\n}\r\nstatic void mm_iommu_unpin(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nlong i;\r\nstruct page *page = NULL;\r\nfor (i = 0; i < mem->entries; ++i) {\r\nif (!mem->hpas[i])\r\ncontinue;\r\npage = pfn_to_page(mem->hpas[i] >> PAGE_SHIFT);\r\nif (!page)\r\ncontinue;\r\nput_page(page);\r\nmem->hpas[i] = 0;\r\n}\r\n}\r\nstatic void mm_iommu_do_free(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nmm_iommu_unpin(mem);\r\nvfree(mem->hpas);\r\nkfree(mem);\r\n}\r\nstatic void mm_iommu_free(struct rcu_head *head)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem = container_of(head,\r\nstruct mm_iommu_table_group_mem_t, rcu);\r\nmm_iommu_do_free(mem);\r\n}\r\nstatic void mm_iommu_release(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nlist_del_rcu(&mem->next);\r\ncall_rcu(&mem->rcu, mm_iommu_free);\r\n}\r\nlong mm_iommu_put(struct mm_struct *mm, struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nlong ret = 0;\r\nmutex_lock(&mem_list_mutex);\r\nif (mem->used == 0) {\r\nret = -ENOENT;\r\ngoto unlock_exit;\r\n}\r\n--mem->used;\r\nif (mem->used)\r\ngoto unlock_exit;\r\nif (atomic_cmpxchg(&mem->mapped, 1, 0) != 1) {\r\n++mem->used;\r\nret = -EBUSY;\r\ngoto unlock_exit;\r\n}\r\nmm_iommu_release(mem);\r\nmm_iommu_adjust_locked_vm(mm, mem->entries, false);\r\nunlock_exit:\r\nmutex_unlock(&mem_list_mutex);\r\nreturn ret;\r\n}\r\nstruct mm_iommu_table_group_mem_t *mm_iommu_lookup(struct mm_struct *mm,\r\nunsigned long ua, unsigned long size)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem, *ret = NULL;\r\nlist_for_each_entry_rcu(mem, &mm->context.iommu_group_mem_list, next) {\r\nif ((mem->ua <= ua) &&\r\n(ua + size <= mem->ua +\r\n(mem->entries << PAGE_SHIFT))) {\r\nret = mem;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstruct mm_iommu_table_group_mem_t *mm_iommu_lookup_rm(struct mm_struct *mm,\r\nunsigned long ua, unsigned long size)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem, *ret = NULL;\r\nlist_for_each_entry_lockless(mem, &mm->context.iommu_group_mem_list,\r\nnext) {\r\nif ((mem->ua <= ua) &&\r\n(ua + size <= mem->ua +\r\n(mem->entries << PAGE_SHIFT))) {\r\nret = mem;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstruct mm_iommu_table_group_mem_t *mm_iommu_find(struct mm_struct *mm,\r\nunsigned long ua, unsigned long entries)\r\n{\r\nstruct mm_iommu_table_group_mem_t *mem, *ret = NULL;\r\nlist_for_each_entry_rcu(mem, &mm->context.iommu_group_mem_list, next) {\r\nif ((mem->ua == ua) && (mem->entries == entries)) {\r\nret = mem;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nlong mm_iommu_ua_to_hpa(struct mm_iommu_table_group_mem_t *mem,\r\nunsigned long ua, unsigned long *hpa)\r\n{\r\nconst long entry = (ua - mem->ua) >> PAGE_SHIFT;\r\nu64 *va = &mem->hpas[entry];\r\nif (entry >= mem->entries)\r\nreturn -EFAULT;\r\n*hpa = *va | (ua & ~PAGE_MASK);\r\nreturn 0;\r\n}\r\nlong mm_iommu_ua_to_hpa_rm(struct mm_iommu_table_group_mem_t *mem,\r\nunsigned long ua, unsigned long *hpa)\r\n{\r\nconst long entry = (ua - mem->ua) >> PAGE_SHIFT;\r\nvoid *va = &mem->hpas[entry];\r\nunsigned long *pa;\r\nif (entry >= mem->entries)\r\nreturn -EFAULT;\r\npa = (void *) vmalloc_to_phys(va);\r\nif (!pa)\r\nreturn -EFAULT;\r\n*hpa = *pa | (ua & ~PAGE_MASK);\r\nreturn 0;\r\n}\r\nlong mm_iommu_mapped_inc(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\nif (atomic64_inc_not_zero(&mem->mapped))\r\nreturn 0;\r\nreturn -ENXIO;\r\n}\r\nvoid mm_iommu_mapped_dec(struct mm_iommu_table_group_mem_t *mem)\r\n{\r\natomic64_add_unless(&mem->mapped, -1, 1);\r\n}\r\nvoid mm_iommu_init(struct mm_struct *mm)\r\n{\r\nINIT_LIST_HEAD_RCU(&mm->context.iommu_group_mem_list);\r\n}
