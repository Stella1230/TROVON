static void qce_ablkcipher_done(void *data)\r\n{\r\nstruct crypto_async_request *async_req = data;\r\nstruct ablkcipher_request *req = ablkcipher_request_cast(async_req);\r\nstruct qce_cipher_reqctx *rctx = ablkcipher_request_ctx(req);\r\nstruct qce_alg_template *tmpl = to_cipher_tmpl(async_req->tfm);\r\nstruct qce_device *qce = tmpl->qce;\r\nenum dma_data_direction dir_src, dir_dst;\r\nu32 status;\r\nint error;\r\nbool diff_dst;\r\ndiff_dst = (req->src != req->dst) ? true : false;\r\ndir_src = diff_dst ? DMA_TO_DEVICE : DMA_BIDIRECTIONAL;\r\ndir_dst = diff_dst ? DMA_FROM_DEVICE : DMA_BIDIRECTIONAL;\r\nerror = qce_dma_terminate_all(&qce->dma);\r\nif (error)\r\ndev_dbg(qce->dev, "ablkcipher dma termination error (%d)\n",\r\nerror);\r\nif (diff_dst)\r\ndma_unmap_sg(qce->dev, rctx->src_sg, rctx->src_nents, dir_src);\r\ndma_unmap_sg(qce->dev, rctx->dst_sg, rctx->dst_nents, dir_dst);\r\nsg_free_table(&rctx->dst_tbl);\r\nerror = qce_check_status(qce, &status);\r\nif (error < 0)\r\ndev_dbg(qce->dev, "ablkcipher operation error (%x)\n", status);\r\nqce->async_req_done(tmpl->qce, error);\r\n}\r\nstatic int\r\nqce_ablkcipher_async_req_handle(struct crypto_async_request *async_req)\r\n{\r\nstruct ablkcipher_request *req = ablkcipher_request_cast(async_req);\r\nstruct qce_cipher_reqctx *rctx = ablkcipher_request_ctx(req);\r\nstruct crypto_ablkcipher *ablkcipher = crypto_ablkcipher_reqtfm(req);\r\nstruct qce_alg_template *tmpl = to_cipher_tmpl(async_req->tfm);\r\nstruct qce_device *qce = tmpl->qce;\r\nenum dma_data_direction dir_src, dir_dst;\r\nstruct scatterlist *sg;\r\nbool diff_dst;\r\ngfp_t gfp;\r\nint ret;\r\nrctx->iv = req->info;\r\nrctx->ivsize = crypto_ablkcipher_ivsize(ablkcipher);\r\nrctx->cryptlen = req->nbytes;\r\ndiff_dst = (req->src != req->dst) ? true : false;\r\ndir_src = diff_dst ? DMA_TO_DEVICE : DMA_BIDIRECTIONAL;\r\ndir_dst = diff_dst ? DMA_FROM_DEVICE : DMA_BIDIRECTIONAL;\r\nrctx->src_nents = sg_nents_for_len(req->src, req->nbytes);\r\nif (diff_dst)\r\nrctx->dst_nents = sg_nents_for_len(req->dst, req->nbytes);\r\nelse\r\nrctx->dst_nents = rctx->src_nents;\r\nif (rctx->src_nents < 0) {\r\ndev_err(qce->dev, "Invalid numbers of src SG.\n");\r\nreturn rctx->src_nents;\r\n}\r\nif (rctx->dst_nents < 0) {\r\ndev_err(qce->dev, "Invalid numbers of dst SG.\n");\r\nreturn -rctx->dst_nents;\r\n}\r\nrctx->dst_nents += 1;\r\ngfp = (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?\r\nGFP_KERNEL : GFP_ATOMIC;\r\nret = sg_alloc_table(&rctx->dst_tbl, rctx->dst_nents, gfp);\r\nif (ret)\r\nreturn ret;\r\nsg_init_one(&rctx->result_sg, qce->dma.result_buf, QCE_RESULT_BUF_SZ);\r\nsg = qce_sgtable_add(&rctx->dst_tbl, req->dst);\r\nif (IS_ERR(sg)) {\r\nret = PTR_ERR(sg);\r\ngoto error_free;\r\n}\r\nsg = qce_sgtable_add(&rctx->dst_tbl, &rctx->result_sg);\r\nif (IS_ERR(sg)) {\r\nret = PTR_ERR(sg);\r\ngoto error_free;\r\n}\r\nsg_mark_end(sg);\r\nrctx->dst_sg = rctx->dst_tbl.sgl;\r\nret = dma_map_sg(qce->dev, rctx->dst_sg, rctx->dst_nents, dir_dst);\r\nif (ret < 0)\r\ngoto error_free;\r\nif (diff_dst) {\r\nret = dma_map_sg(qce->dev, req->src, rctx->src_nents, dir_src);\r\nif (ret < 0)\r\ngoto error_unmap_dst;\r\nrctx->src_sg = req->src;\r\n} else {\r\nrctx->src_sg = rctx->dst_sg;\r\n}\r\nret = qce_dma_prep_sgs(&qce->dma, rctx->src_sg, rctx->src_nents,\r\nrctx->dst_sg, rctx->dst_nents,\r\nqce_ablkcipher_done, async_req);\r\nif (ret)\r\ngoto error_unmap_src;\r\nqce_dma_issue_pending(&qce->dma);\r\nret = qce_start(async_req, tmpl->crypto_alg_type, req->nbytes, 0);\r\nif (ret)\r\ngoto error_terminate;\r\nreturn 0;\r\nerror_terminate:\r\nqce_dma_terminate_all(&qce->dma);\r\nerror_unmap_src:\r\nif (diff_dst)\r\ndma_unmap_sg(qce->dev, req->src, rctx->src_nents, dir_src);\r\nerror_unmap_dst:\r\ndma_unmap_sg(qce->dev, rctx->dst_sg, rctx->dst_nents, dir_dst);\r\nerror_free:\r\nsg_free_table(&rctx->dst_tbl);\r\nreturn ret;\r\n}\r\nstatic int qce_ablkcipher_setkey(struct crypto_ablkcipher *ablk, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(ablk);\r\nstruct qce_cipher_ctx *ctx = crypto_tfm_ctx(tfm);\r\nunsigned long flags = to_cipher_tmpl(tfm)->alg_flags;\r\nint ret;\r\nif (!key || !keylen)\r\nreturn -EINVAL;\r\nif (IS_AES(flags)) {\r\nswitch (keylen) {\r\ncase AES_KEYSIZE_128:\r\ncase AES_KEYSIZE_256:\r\nbreak;\r\ndefault:\r\ngoto fallback;\r\n}\r\n} else if (IS_DES(flags)) {\r\nu32 tmp[DES_EXPKEY_WORDS];\r\nret = des_ekey(tmp, key);\r\nif (!ret && crypto_ablkcipher_get_flags(ablk) &\r\nCRYPTO_TFM_REQ_WEAK_KEY)\r\ngoto weakkey;\r\n}\r\nctx->enc_keylen = keylen;\r\nmemcpy(ctx->enc_key, key, keylen);\r\nreturn 0;\r\nfallback:\r\nret = crypto_skcipher_setkey(ctx->fallback, key, keylen);\r\nif (!ret)\r\nctx->enc_keylen = keylen;\r\nreturn ret;\r\nweakkey:\r\ncrypto_ablkcipher_set_flags(ablk, CRYPTO_TFM_RES_WEAK_KEY);\r\nreturn -EINVAL;\r\n}\r\nstatic int qce_ablkcipher_crypt(struct ablkcipher_request *req, int encrypt)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct qce_cipher_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct qce_cipher_reqctx *rctx = ablkcipher_request_ctx(req);\r\nstruct qce_alg_template *tmpl = to_cipher_tmpl(tfm);\r\nint ret;\r\nrctx->flags = tmpl->alg_flags;\r\nrctx->flags |= encrypt ? QCE_ENCRYPT : QCE_DECRYPT;\r\nif (IS_AES(rctx->flags) && ctx->enc_keylen != AES_KEYSIZE_128 &&\r\nctx->enc_keylen != AES_KEYSIZE_256) {\r\nSKCIPHER_REQUEST_ON_STACK(subreq, ctx->fallback);\r\nskcipher_request_set_tfm(subreq, ctx->fallback);\r\nskcipher_request_set_callback(subreq, req->base.flags,\r\nNULL, NULL);\r\nskcipher_request_set_crypt(subreq, req->src, req->dst,\r\nreq->nbytes, req->info);\r\nret = encrypt ? crypto_skcipher_encrypt(subreq) :\r\ncrypto_skcipher_decrypt(subreq);\r\nskcipher_request_zero(subreq);\r\nreturn ret;\r\n}\r\nreturn tmpl->qce->async_req_enqueue(tmpl->qce, &req->base);\r\n}\r\nstatic int qce_ablkcipher_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn qce_ablkcipher_crypt(req, 1);\r\n}\r\nstatic int qce_ablkcipher_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn qce_ablkcipher_crypt(req, 0);\r\n}\r\nstatic int qce_ablkcipher_init(struct crypto_tfm *tfm)\r\n{\r\nstruct qce_cipher_ctx *ctx = crypto_tfm_ctx(tfm);\r\nmemset(ctx, 0, sizeof(*ctx));\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct qce_cipher_reqctx);\r\nctx->fallback = crypto_alloc_skcipher(crypto_tfm_alg_name(tfm), 0,\r\nCRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(ctx->fallback))\r\nreturn PTR_ERR(ctx->fallback);\r\nreturn 0;\r\n}\r\nstatic void qce_ablkcipher_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct qce_cipher_ctx *ctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_skcipher(ctx->fallback);\r\n}\r\nstatic int qce_ablkcipher_register_one(const struct qce_ablkcipher_def *def,\r\nstruct qce_device *qce)\r\n{\r\nstruct qce_alg_template *tmpl;\r\nstruct crypto_alg *alg;\r\nint ret;\r\ntmpl = kzalloc(sizeof(*tmpl), GFP_KERNEL);\r\nif (!tmpl)\r\nreturn -ENOMEM;\r\nalg = &tmpl->alg.crypto;\r\nsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s", def->name);\r\nsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ndef->drv_name);\r\nalg->cra_blocksize = def->blocksize;\r\nalg->cra_ablkcipher.ivsize = def->ivsize;\r\nalg->cra_ablkcipher.min_keysize = def->min_keysize;\r\nalg->cra_ablkcipher.max_keysize = def->max_keysize;\r\nalg->cra_ablkcipher.setkey = qce_ablkcipher_setkey;\r\nalg->cra_ablkcipher.encrypt = qce_ablkcipher_encrypt;\r\nalg->cra_ablkcipher.decrypt = qce_ablkcipher_decrypt;\r\nalg->cra_priority = 300;\r\nalg->cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_NEED_FALLBACK;\r\nalg->cra_ctxsize = sizeof(struct qce_cipher_ctx);\r\nalg->cra_alignmask = 0;\r\nalg->cra_type = &crypto_ablkcipher_type;\r\nalg->cra_module = THIS_MODULE;\r\nalg->cra_init = qce_ablkcipher_init;\r\nalg->cra_exit = qce_ablkcipher_exit;\r\nINIT_LIST_HEAD(&alg->cra_list);\r\nINIT_LIST_HEAD(&tmpl->entry);\r\ntmpl->crypto_alg_type = CRYPTO_ALG_TYPE_ABLKCIPHER;\r\ntmpl->alg_flags = def->flags;\r\ntmpl->qce = qce;\r\nret = crypto_register_alg(alg);\r\nif (ret) {\r\nkfree(tmpl);\r\ndev_err(qce->dev, "%s registration failed\n", alg->cra_name);\r\nreturn ret;\r\n}\r\nlist_add_tail(&tmpl->entry, &ablkcipher_algs);\r\ndev_dbg(qce->dev, "%s is registered\n", alg->cra_name);\r\nreturn 0;\r\n}\r\nstatic void qce_ablkcipher_unregister(struct qce_device *qce)\r\n{\r\nstruct qce_alg_template *tmpl, *n;\r\nlist_for_each_entry_safe(tmpl, n, &ablkcipher_algs, entry) {\r\ncrypto_unregister_alg(&tmpl->alg.crypto);\r\nlist_del(&tmpl->entry);\r\nkfree(tmpl);\r\n}\r\n}\r\nstatic int qce_ablkcipher_register(struct qce_device *qce)\r\n{\r\nint ret, i;\r\nfor (i = 0; i < ARRAY_SIZE(ablkcipher_def); i++) {\r\nret = qce_ablkcipher_register_one(&ablkcipher_def[i], qce);\r\nif (ret)\r\ngoto err;\r\n}\r\nreturn 0;\r\nerr:\r\nqce_ablkcipher_unregister(qce);\r\nreturn ret;\r\n}
