static void xgbe_free_ring(struct xgbe_prv_data *pdata,\r\nstruct xgbe_ring *ring)\r\n{\r\nstruct xgbe_ring_data *rdata;\r\nunsigned int i;\r\nif (!ring)\r\nreturn;\r\nif (ring->rdata) {\r\nfor (i = 0; i < ring->rdesc_count; i++) {\r\nrdata = XGBE_GET_DESC_DATA(ring, i);\r\nxgbe_unmap_rdata(pdata, rdata);\r\n}\r\nkfree(ring->rdata);\r\nring->rdata = NULL;\r\n}\r\nif (ring->rx_hdr_pa.pages) {\r\ndma_unmap_page(pdata->dev, ring->rx_hdr_pa.pages_dma,\r\nring->rx_hdr_pa.pages_len, DMA_FROM_DEVICE);\r\nput_page(ring->rx_hdr_pa.pages);\r\nring->rx_hdr_pa.pages = NULL;\r\nring->rx_hdr_pa.pages_len = 0;\r\nring->rx_hdr_pa.pages_offset = 0;\r\nring->rx_hdr_pa.pages_dma = 0;\r\n}\r\nif (ring->rx_buf_pa.pages) {\r\ndma_unmap_page(pdata->dev, ring->rx_buf_pa.pages_dma,\r\nring->rx_buf_pa.pages_len, DMA_FROM_DEVICE);\r\nput_page(ring->rx_buf_pa.pages);\r\nring->rx_buf_pa.pages = NULL;\r\nring->rx_buf_pa.pages_len = 0;\r\nring->rx_buf_pa.pages_offset = 0;\r\nring->rx_buf_pa.pages_dma = 0;\r\n}\r\nif (ring->rdesc) {\r\ndma_free_coherent(pdata->dev,\r\n(sizeof(struct xgbe_ring_desc) *\r\nring->rdesc_count),\r\nring->rdesc, ring->rdesc_dma);\r\nring->rdesc = NULL;\r\n}\r\n}\r\nstatic void xgbe_free_ring_resources(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\nDBGPR("-->xgbe_free_ring_resources\n");\r\nfor (i = 0; i < pdata->channel_count; i++) {\r\nchannel = pdata->channel[i];\r\nxgbe_free_ring(pdata, channel->tx_ring);\r\nxgbe_free_ring(pdata, channel->rx_ring);\r\n}\r\nDBGPR("<--xgbe_free_ring_resources\n");\r\n}\r\nstatic void *xgbe_alloc_node(size_t size, int node)\r\n{\r\nvoid *mem;\r\nmem = kzalloc_node(size, GFP_KERNEL, node);\r\nif (!mem)\r\nmem = kzalloc(size, GFP_KERNEL);\r\nreturn mem;\r\n}\r\nstatic void *xgbe_dma_alloc_node(struct device *dev, size_t size,\r\ndma_addr_t *dma, int node)\r\n{\r\nvoid *mem;\r\nint cur_node = dev_to_node(dev);\r\nset_dev_node(dev, node);\r\nmem = dma_alloc_coherent(dev, size, dma, GFP_KERNEL);\r\nset_dev_node(dev, cur_node);\r\nif (!mem)\r\nmem = dma_alloc_coherent(dev, size, dma, GFP_KERNEL);\r\nreturn mem;\r\n}\r\nstatic int xgbe_init_ring(struct xgbe_prv_data *pdata,\r\nstruct xgbe_ring *ring, unsigned int rdesc_count)\r\n{\r\nsize_t size;\r\nif (!ring)\r\nreturn 0;\r\nsize = rdesc_count * sizeof(struct xgbe_ring_desc);\r\nring->rdesc_count = rdesc_count;\r\nring->rdesc = xgbe_dma_alloc_node(pdata->dev, size, &ring->rdesc_dma,\r\nring->node);\r\nif (!ring->rdesc)\r\nreturn -ENOMEM;\r\nsize = rdesc_count * sizeof(struct xgbe_ring_data);\r\nring->rdata = xgbe_alloc_node(size, ring->node);\r\nif (!ring->rdata)\r\nreturn -ENOMEM;\r\nnetif_dbg(pdata, drv, pdata->netdev,\r\n"rdesc=%p, rdesc_dma=%pad, rdata=%p, node=%d\n",\r\nring->rdesc, &ring->rdesc_dma, ring->rdata, ring->node);\r\nreturn 0;\r\n}\r\nstatic int xgbe_alloc_ring_resources(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\nint ret;\r\nfor (i = 0; i < pdata->channel_count; i++) {\r\nchannel = pdata->channel[i];\r\nnetif_dbg(pdata, drv, pdata->netdev, "%s - Tx ring:\n",\r\nchannel->name);\r\nret = xgbe_init_ring(pdata, channel->tx_ring,\r\npdata->tx_desc_count);\r\nif (ret) {\r\nnetdev_alert(pdata->netdev,\r\n"error initializing Tx ring\n");\r\ngoto err_ring;\r\n}\r\nnetif_dbg(pdata, drv, pdata->netdev, "%s - Rx ring:\n",\r\nchannel->name);\r\nret = xgbe_init_ring(pdata, channel->rx_ring,\r\npdata->rx_desc_count);\r\nif (ret) {\r\nnetdev_alert(pdata->netdev,\r\n"error initializing Rx ring\n");\r\ngoto err_ring;\r\n}\r\n}\r\nreturn 0;\r\nerr_ring:\r\nxgbe_free_ring_resources(pdata);\r\nreturn ret;\r\n}\r\nstatic int xgbe_alloc_pages(struct xgbe_prv_data *pdata,\r\nstruct xgbe_page_alloc *pa, int alloc_order,\r\nint node)\r\n{\r\nstruct page *pages = NULL;\r\ndma_addr_t pages_dma;\r\ngfp_t gfp;\r\nint order, ret;\r\nagain:\r\norder = alloc_order;\r\ngfp = GFP_ATOMIC | __GFP_COLD | __GFP_COMP | __GFP_NOWARN;\r\nwhile (order >= 0) {\r\npages = alloc_pages_node(node, gfp, order);\r\nif (pages)\r\nbreak;\r\norder--;\r\n}\r\nif (!pages && (node != NUMA_NO_NODE)) {\r\nnode = NUMA_NO_NODE;\r\ngoto again;\r\n}\r\nif (!pages)\r\nreturn -ENOMEM;\r\npages_dma = dma_map_page(pdata->dev, pages, 0,\r\nPAGE_SIZE << order, DMA_FROM_DEVICE);\r\nret = dma_mapping_error(pdata->dev, pages_dma);\r\nif (ret) {\r\nput_page(pages);\r\nreturn ret;\r\n}\r\npa->pages = pages;\r\npa->pages_len = PAGE_SIZE << order;\r\npa->pages_offset = 0;\r\npa->pages_dma = pages_dma;\r\nreturn 0;\r\n}\r\nstatic void xgbe_set_buffer_data(struct xgbe_buffer_data *bd,\r\nstruct xgbe_page_alloc *pa,\r\nunsigned int len)\r\n{\r\nget_page(pa->pages);\r\nbd->pa = *pa;\r\nbd->dma_base = pa->pages_dma;\r\nbd->dma_off = pa->pages_offset;\r\nbd->dma_len = len;\r\npa->pages_offset += len;\r\nif ((pa->pages_offset + len) > pa->pages_len) {\r\nbd->pa_unmap = *pa;\r\npa->pages = NULL;\r\npa->pages_len = 0;\r\npa->pages_offset = 0;\r\npa->pages_dma = 0;\r\n}\r\n}\r\nstatic int xgbe_map_rx_buffer(struct xgbe_prv_data *pdata,\r\nstruct xgbe_ring *ring,\r\nstruct xgbe_ring_data *rdata)\r\n{\r\nint ret;\r\nif (!ring->rx_hdr_pa.pages) {\r\nret = xgbe_alloc_pages(pdata, &ring->rx_hdr_pa, 0, ring->node);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (!ring->rx_buf_pa.pages) {\r\nret = xgbe_alloc_pages(pdata, &ring->rx_buf_pa,\r\nPAGE_ALLOC_COSTLY_ORDER, ring->node);\r\nif (ret)\r\nreturn ret;\r\n}\r\nxgbe_set_buffer_data(&rdata->rx.hdr, &ring->rx_hdr_pa,\r\nXGBE_SKB_ALLOC_SIZE);\r\nxgbe_set_buffer_data(&rdata->rx.buf, &ring->rx_buf_pa,\r\npdata->rx_buf_size);\r\nreturn 0;\r\n}\r\nstatic void xgbe_wrapper_tx_descriptor_init(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_channel *channel;\r\nstruct xgbe_ring *ring;\r\nstruct xgbe_ring_data *rdata;\r\nstruct xgbe_ring_desc *rdesc;\r\ndma_addr_t rdesc_dma;\r\nunsigned int i, j;\r\nDBGPR("-->xgbe_wrapper_tx_descriptor_init\n");\r\nfor (i = 0; i < pdata->channel_count; i++) {\r\nchannel = pdata->channel[i];\r\nring = channel->tx_ring;\r\nif (!ring)\r\nbreak;\r\nrdesc = ring->rdesc;\r\nrdesc_dma = ring->rdesc_dma;\r\nfor (j = 0; j < ring->rdesc_count; j++) {\r\nrdata = XGBE_GET_DESC_DATA(ring, j);\r\nrdata->rdesc = rdesc;\r\nrdata->rdesc_dma = rdesc_dma;\r\nrdesc++;\r\nrdesc_dma += sizeof(struct xgbe_ring_desc);\r\n}\r\nring->cur = 0;\r\nring->dirty = 0;\r\nmemset(&ring->tx, 0, sizeof(ring->tx));\r\nhw_if->tx_desc_init(channel);\r\n}\r\nDBGPR("<--xgbe_wrapper_tx_descriptor_init\n");\r\n}\r\nstatic void xgbe_wrapper_rx_descriptor_init(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_channel *channel;\r\nstruct xgbe_ring *ring;\r\nstruct xgbe_ring_desc *rdesc;\r\nstruct xgbe_ring_data *rdata;\r\ndma_addr_t rdesc_dma;\r\nunsigned int i, j;\r\nDBGPR("-->xgbe_wrapper_rx_descriptor_init\n");\r\nfor (i = 0; i < pdata->channel_count; i++) {\r\nchannel = pdata->channel[i];\r\nring = channel->rx_ring;\r\nif (!ring)\r\nbreak;\r\nrdesc = ring->rdesc;\r\nrdesc_dma = ring->rdesc_dma;\r\nfor (j = 0; j < ring->rdesc_count; j++) {\r\nrdata = XGBE_GET_DESC_DATA(ring, j);\r\nrdata->rdesc = rdesc;\r\nrdata->rdesc_dma = rdesc_dma;\r\nif (xgbe_map_rx_buffer(pdata, ring, rdata))\r\nbreak;\r\nrdesc++;\r\nrdesc_dma += sizeof(struct xgbe_ring_desc);\r\n}\r\nring->cur = 0;\r\nring->dirty = 0;\r\nhw_if->rx_desc_init(channel);\r\n}\r\nDBGPR("<--xgbe_wrapper_rx_descriptor_init\n");\r\n}\r\nstatic void xgbe_unmap_rdata(struct xgbe_prv_data *pdata,\r\nstruct xgbe_ring_data *rdata)\r\n{\r\nif (rdata->skb_dma) {\r\nif (rdata->mapped_as_page) {\r\ndma_unmap_page(pdata->dev, rdata->skb_dma,\r\nrdata->skb_dma_len, DMA_TO_DEVICE);\r\n} else {\r\ndma_unmap_single(pdata->dev, rdata->skb_dma,\r\nrdata->skb_dma_len, DMA_TO_DEVICE);\r\n}\r\nrdata->skb_dma = 0;\r\nrdata->skb_dma_len = 0;\r\n}\r\nif (rdata->skb) {\r\ndev_kfree_skb_any(rdata->skb);\r\nrdata->skb = NULL;\r\n}\r\nif (rdata->rx.hdr.pa.pages)\r\nput_page(rdata->rx.hdr.pa.pages);\r\nif (rdata->rx.hdr.pa_unmap.pages) {\r\ndma_unmap_page(pdata->dev, rdata->rx.hdr.pa_unmap.pages_dma,\r\nrdata->rx.hdr.pa_unmap.pages_len,\r\nDMA_FROM_DEVICE);\r\nput_page(rdata->rx.hdr.pa_unmap.pages);\r\n}\r\nif (rdata->rx.buf.pa.pages)\r\nput_page(rdata->rx.buf.pa.pages);\r\nif (rdata->rx.buf.pa_unmap.pages) {\r\ndma_unmap_page(pdata->dev, rdata->rx.buf.pa_unmap.pages_dma,\r\nrdata->rx.buf.pa_unmap.pages_len,\r\nDMA_FROM_DEVICE);\r\nput_page(rdata->rx.buf.pa_unmap.pages);\r\n}\r\nmemset(&rdata->tx, 0, sizeof(rdata->tx));\r\nmemset(&rdata->rx, 0, sizeof(rdata->rx));\r\nrdata->mapped_as_page = 0;\r\nif (rdata->state_saved) {\r\nrdata->state_saved = 0;\r\nrdata->state.skb = NULL;\r\nrdata->state.len = 0;\r\nrdata->state.error = 0;\r\n}\r\n}\r\nstatic int xgbe_map_tx_skb(struct xgbe_channel *channel, struct sk_buff *skb)\r\n{\r\nstruct xgbe_prv_data *pdata = channel->pdata;\r\nstruct xgbe_ring *ring = channel->tx_ring;\r\nstruct xgbe_ring_data *rdata;\r\nstruct xgbe_packet_data *packet;\r\nstruct skb_frag_struct *frag;\r\ndma_addr_t skb_dma;\r\nunsigned int start_index, cur_index;\r\nunsigned int offset, tso, vlan, datalen, len;\r\nunsigned int i;\r\nDBGPR("-->xgbe_map_tx_skb: cur = %d\n", ring->cur);\r\noffset = 0;\r\nstart_index = ring->cur;\r\ncur_index = ring->cur;\r\npacket = &ring->packet_data;\r\npacket->rdesc_count = 0;\r\npacket->length = 0;\r\ntso = XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nTSO_ENABLE);\r\nvlan = XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nVLAN_CTAG);\r\nif ((tso && (packet->mss != ring->tx.cur_mss)) ||\r\n(vlan && (packet->vlan_ctag != ring->tx.cur_vlan_ctag)))\r\ncur_index++;\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index);\r\nif (tso) {\r\nskb_dma = dma_map_single(pdata->dev, skb->data,\r\npacket->header_len, DMA_TO_DEVICE);\r\nif (dma_mapping_error(pdata->dev, skb_dma)) {\r\nnetdev_alert(pdata->netdev, "dma_map_single failed\n");\r\ngoto err_out;\r\n}\r\nrdata->skb_dma = skb_dma;\r\nrdata->skb_dma_len = packet->header_len;\r\nnetif_dbg(pdata, tx_queued, pdata->netdev,\r\n"skb header: index=%u, dma=%pad, len=%u\n",\r\ncur_index, &skb_dma, packet->header_len);\r\noffset = packet->header_len;\r\npacket->length += packet->header_len;\r\ncur_index++;\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index);\r\n}\r\nfor (datalen = skb_headlen(skb) - offset; datalen; ) {\r\nlen = min_t(unsigned int, datalen, XGBE_TX_MAX_BUF_SIZE);\r\nskb_dma = dma_map_single(pdata->dev, skb->data + offset, len,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(pdata->dev, skb_dma)) {\r\nnetdev_alert(pdata->netdev, "dma_map_single failed\n");\r\ngoto err_out;\r\n}\r\nrdata->skb_dma = skb_dma;\r\nrdata->skb_dma_len = len;\r\nnetif_dbg(pdata, tx_queued, pdata->netdev,\r\n"skb data: index=%u, dma=%pad, len=%u\n",\r\ncur_index, &skb_dma, len);\r\ndatalen -= len;\r\noffset += len;\r\npacket->length += len;\r\ncur_index++;\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index);\r\n}\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nnetif_dbg(pdata, tx_queued, pdata->netdev,\r\n"mapping frag %u\n", i);\r\nfrag = &skb_shinfo(skb)->frags[i];\r\noffset = 0;\r\nfor (datalen = skb_frag_size(frag); datalen; ) {\r\nlen = min_t(unsigned int, datalen,\r\nXGBE_TX_MAX_BUF_SIZE);\r\nskb_dma = skb_frag_dma_map(pdata->dev, frag, offset,\r\nlen, DMA_TO_DEVICE);\r\nif (dma_mapping_error(pdata->dev, skb_dma)) {\r\nnetdev_alert(pdata->netdev,\r\n"skb_frag_dma_map failed\n");\r\ngoto err_out;\r\n}\r\nrdata->skb_dma = skb_dma;\r\nrdata->skb_dma_len = len;\r\nrdata->mapped_as_page = 1;\r\nnetif_dbg(pdata, tx_queued, pdata->netdev,\r\n"skb frag: index=%u, dma=%pad, len=%u\n",\r\ncur_index, &skb_dma, len);\r\ndatalen -= len;\r\noffset += len;\r\npacket->length += len;\r\ncur_index++;\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index);\r\n}\r\n}\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index - 1);\r\nrdata->skb = skb;\r\npacket->rdesc_count = cur_index - start_index;\r\nDBGPR("<--xgbe_map_tx_skb: count=%u\n", packet->rdesc_count);\r\nreturn packet->rdesc_count;\r\nerr_out:\r\nwhile (start_index < cur_index) {\r\nrdata = XGBE_GET_DESC_DATA(ring, start_index++);\r\nxgbe_unmap_rdata(pdata, rdata);\r\n}\r\nDBGPR("<--xgbe_map_tx_skb: count=0\n");\r\nreturn 0;\r\n}\r\nvoid xgbe_init_function_ptrs_desc(struct xgbe_desc_if *desc_if)\r\n{\r\nDBGPR("-->xgbe_init_function_ptrs_desc\n");\r\ndesc_if->alloc_ring_resources = xgbe_alloc_ring_resources;\r\ndesc_if->free_ring_resources = xgbe_free_ring_resources;\r\ndesc_if->map_tx_skb = xgbe_map_tx_skb;\r\ndesc_if->map_rx_buffer = xgbe_map_rx_buffer;\r\ndesc_if->unmap_rdata = xgbe_unmap_rdata;\r\ndesc_if->wrapper_tx_desc_init = xgbe_wrapper_tx_descriptor_init;\r\ndesc_if->wrapper_rx_desc_init = xgbe_wrapper_rx_descriptor_init;\r\nDBGPR("<--xgbe_init_function_ptrs_desc\n");\r\n}
