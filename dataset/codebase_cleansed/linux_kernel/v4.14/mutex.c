void\r\n__mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)\r\n{\r\natomic_long_set(&lock->owner, 0);\r\nspin_lock_init(&lock->wait_lock);\r\nINIT_LIST_HEAD(&lock->wait_list);\r\n#ifdef CONFIG_MUTEX_SPIN_ON_OWNER\r\nosq_lock_init(&lock->osq);\r\n#endif\r\ndebug_mutex_init(lock, name, key);\r\n}\r\nstatic inline struct task_struct *__owner_task(unsigned long owner)\r\n{\r\nreturn (struct task_struct *)(owner & ~MUTEX_FLAGS);\r\n}\r\nstatic inline unsigned long __owner_flags(unsigned long owner)\r\n{\r\nreturn owner & MUTEX_FLAGS;\r\n}\r\nstatic inline struct task_struct *__mutex_trylock_or_owner(struct mutex *lock)\r\n{\r\nunsigned long owner, curr = (unsigned long)current;\r\nowner = atomic_long_read(&lock->owner);\r\nfor (;;) {\r\nunsigned long old, flags = __owner_flags(owner);\r\nunsigned long task = owner & ~MUTEX_FLAGS;\r\nif (task) {\r\nif (likely(task != curr))\r\nbreak;\r\nif (likely(!(flags & MUTEX_FLAG_PICKUP)))\r\nbreak;\r\nflags &= ~MUTEX_FLAG_PICKUP;\r\n} else {\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nDEBUG_LOCKS_WARN_ON(flags & MUTEX_FLAG_PICKUP);\r\n#endif\r\n}\r\nflags &= ~MUTEX_FLAG_HANDOFF;\r\nold = atomic_long_cmpxchg_acquire(&lock->owner, owner, curr | flags);\r\nif (old == owner)\r\nreturn NULL;\r\nowner = old;\r\n}\r\nreturn __owner_task(owner);\r\n}\r\nstatic inline bool __mutex_trylock(struct mutex *lock)\r\n{\r\nreturn !__mutex_trylock_or_owner(lock);\r\n}\r\nstatic __always_inline bool __mutex_trylock_fast(struct mutex *lock)\r\n{\r\nunsigned long curr = (unsigned long)current;\r\nif (!atomic_long_cmpxchg_acquire(&lock->owner, 0UL, curr))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic __always_inline bool __mutex_unlock_fast(struct mutex *lock)\r\n{\r\nunsigned long curr = (unsigned long)current;\r\nif (atomic_long_cmpxchg_release(&lock->owner, curr, 0UL) == curr)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline void __mutex_set_flag(struct mutex *lock, unsigned long flag)\r\n{\r\natomic_long_or(flag, &lock->owner);\r\n}\r\nstatic inline void __mutex_clear_flag(struct mutex *lock, unsigned long flag)\r\n{\r\natomic_long_andnot(flag, &lock->owner);\r\n}\r\nstatic inline bool __mutex_waiter_is_first(struct mutex *lock, struct mutex_waiter *waiter)\r\n{\r\nreturn list_first_entry(&lock->wait_list, struct mutex_waiter, list) == waiter;\r\n}\r\nstatic void __mutex_handoff(struct mutex *lock, struct task_struct *task)\r\n{\r\nunsigned long owner = atomic_long_read(&lock->owner);\r\nfor (;;) {\r\nunsigned long old, new;\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nDEBUG_LOCKS_WARN_ON(__owner_task(owner) != current);\r\nDEBUG_LOCKS_WARN_ON(owner & MUTEX_FLAG_PICKUP);\r\n#endif\r\nnew = (owner & MUTEX_FLAG_WAITERS);\r\nnew |= (unsigned long)task;\r\nif (task)\r\nnew |= MUTEX_FLAG_PICKUP;\r\nold = atomic_long_cmpxchg_release(&lock->owner, owner, new);\r\nif (old == owner)\r\nbreak;\r\nowner = old;\r\n}\r\n}\r\nvoid __sched mutex_lock(struct mutex *lock)\r\n{\r\nmight_sleep();\r\nif (!__mutex_trylock_fast(lock))\r\n__mutex_lock_slowpath(lock);\r\n}\r\nstatic __always_inline void\r\nww_mutex_lock_acquired(struct ww_mutex *ww, struct ww_acquire_ctx *ww_ctx)\r\n{\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nDEBUG_LOCKS_WARN_ON(ww->ctx);\r\nDEBUG_LOCKS_WARN_ON(ww_ctx->done_acquire);\r\nif (ww_ctx->contending_lock) {\r\nDEBUG_LOCKS_WARN_ON(ww_ctx->contending_lock != ww);\r\nDEBUG_LOCKS_WARN_ON(ww_ctx->acquired > 0);\r\nww_ctx->contending_lock = NULL;\r\n}\r\nDEBUG_LOCKS_WARN_ON(ww_ctx->ww_class != ww->ww_class);\r\n#endif\r\nww_ctx->acquired++;\r\n}\r\nstatic inline bool __sched\r\n__ww_ctx_stamp_after(struct ww_acquire_ctx *a, struct ww_acquire_ctx *b)\r\n{\r\nreturn a->stamp - b->stamp <= LONG_MAX &&\r\n(a->stamp != b->stamp || a > b);\r\n}\r\nstatic void __sched\r\n__ww_mutex_wakeup_for_backoff(struct mutex *lock, struct ww_acquire_ctx *ww_ctx)\r\n{\r\nstruct mutex_waiter *cur;\r\nlockdep_assert_held(&lock->wait_lock);\r\nlist_for_each_entry(cur, &lock->wait_list, list) {\r\nif (!cur->ww_ctx)\r\ncontinue;\r\nif (cur->ww_ctx->acquired > 0 &&\r\n__ww_ctx_stamp_after(cur->ww_ctx, ww_ctx)) {\r\ndebug_mutex_wake_waiter(lock, cur);\r\nwake_up_process(cur->task);\r\n}\r\nbreak;\r\n}\r\n}\r\nstatic __always_inline void\r\nww_mutex_set_context_fastpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nww_mutex_lock_acquired(lock, ctx);\r\nlock->ctx = ctx;\r\nsmp_mb();\r\nif (likely(!(atomic_long_read(&lock->base.owner) & MUTEX_FLAG_WAITERS)))\r\nreturn;\r\nspin_lock(&lock->base.wait_lock);\r\n__ww_mutex_wakeup_for_backoff(&lock->base, ctx);\r\nspin_unlock(&lock->base.wait_lock);\r\n}\r\nstatic __always_inline void\r\nww_mutex_set_context_slowpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nww_mutex_lock_acquired(lock, ctx);\r\nlock->ctx = ctx;\r\n}\r\nstatic inline\r\nbool ww_mutex_spin_on_owner(struct mutex *lock, struct ww_acquire_ctx *ww_ctx,\r\nstruct mutex_waiter *waiter)\r\n{\r\nstruct ww_mutex *ww;\r\nww = container_of(lock, struct ww_mutex, base);\r\nif (ww_ctx->acquired > 0 && READ_ONCE(ww->ctx))\r\nreturn false;\r\nif (!waiter && (atomic_long_read(&lock->owner) & MUTEX_FLAG_WAITERS))\r\nreturn false;\r\nif (waiter && !__mutex_waiter_is_first(lock, waiter))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic noinline\r\nbool mutex_spin_on_owner(struct mutex *lock, struct task_struct *owner,\r\nstruct ww_acquire_ctx *ww_ctx, struct mutex_waiter *waiter)\r\n{\r\nbool ret = true;\r\nrcu_read_lock();\r\nwhile (__mutex_owner(lock) == owner) {\r\nbarrier();\r\nif (!owner->on_cpu || need_resched() ||\r\nvcpu_is_preempted(task_cpu(owner))) {\r\nret = false;\r\nbreak;\r\n}\r\nif (ww_ctx && !ww_mutex_spin_on_owner(lock, ww_ctx, waiter)) {\r\nret = false;\r\nbreak;\r\n}\r\ncpu_relax();\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic inline int mutex_can_spin_on_owner(struct mutex *lock)\r\n{\r\nstruct task_struct *owner;\r\nint retval = 1;\r\nif (need_resched())\r\nreturn 0;\r\nrcu_read_lock();\r\nowner = __mutex_owner(lock);\r\nif (owner)\r\nretval = owner->on_cpu && !vcpu_is_preempted(task_cpu(owner));\r\nrcu_read_unlock();\r\nreturn retval;\r\n}\r\nstatic __always_inline bool\r\nmutex_optimistic_spin(struct mutex *lock, struct ww_acquire_ctx *ww_ctx,\r\nconst bool use_ww_ctx, struct mutex_waiter *waiter)\r\n{\r\nif (!waiter) {\r\nif (!mutex_can_spin_on_owner(lock))\r\ngoto fail;\r\nif (!osq_lock(&lock->osq))\r\ngoto fail;\r\n}\r\nfor (;;) {\r\nstruct task_struct *owner;\r\nowner = __mutex_trylock_or_owner(lock);\r\nif (!owner)\r\nbreak;\r\nif (!mutex_spin_on_owner(lock, owner, ww_ctx, waiter))\r\ngoto fail_unlock;\r\ncpu_relax();\r\n}\r\nif (!waiter)\r\nosq_unlock(&lock->osq);\r\nreturn true;\r\nfail_unlock:\r\nif (!waiter)\r\nosq_unlock(&lock->osq);\r\nfail:\r\nif (need_resched()) {\r\n__set_current_state(TASK_RUNNING);\r\nschedule_preempt_disabled();\r\n}\r\nreturn false;\r\n}\r\nstatic __always_inline bool\r\nmutex_optimistic_spin(struct mutex *lock, struct ww_acquire_ctx *ww_ctx,\r\nconst bool use_ww_ctx, struct mutex_waiter *waiter)\r\n{\r\nreturn false;\r\n}\r\nvoid __sched mutex_unlock(struct mutex *lock)\r\n{\r\n#ifndef CONFIG_DEBUG_LOCK_ALLOC\r\nif (__mutex_unlock_fast(lock))\r\nreturn;\r\n#endif\r\n__mutex_unlock_slowpath(lock, _RET_IP_);\r\n}\r\nvoid __sched ww_mutex_unlock(struct ww_mutex *lock)\r\n{\r\nif (lock->ctx) {\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nDEBUG_LOCKS_WARN_ON(!lock->ctx->acquired);\r\n#endif\r\nif (lock->ctx->acquired > 0)\r\nlock->ctx->acquired--;\r\nlock->ctx = NULL;\r\n}\r\nmutex_unlock(&lock->base);\r\n}\r\nstatic inline int __sched\r\n__ww_mutex_lock_check_stamp(struct mutex *lock, struct mutex_waiter *waiter,\r\nstruct ww_acquire_ctx *ctx)\r\n{\r\nstruct ww_mutex *ww = container_of(lock, struct ww_mutex, base);\r\nstruct ww_acquire_ctx *hold_ctx = READ_ONCE(ww->ctx);\r\nstruct mutex_waiter *cur;\r\nif (hold_ctx && __ww_ctx_stamp_after(ctx, hold_ctx))\r\ngoto deadlock;\r\ncur = waiter;\r\nlist_for_each_entry_continue_reverse(cur, &lock->wait_list, list) {\r\nif (cur->ww_ctx)\r\ngoto deadlock;\r\n}\r\nreturn 0;\r\ndeadlock:\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nDEBUG_LOCKS_WARN_ON(ctx->contending_lock);\r\nctx->contending_lock = ww;\r\n#endif\r\nreturn -EDEADLK;\r\n}\r\nstatic inline int __sched\r\n__ww_mutex_add_waiter(struct mutex_waiter *waiter,\r\nstruct mutex *lock,\r\nstruct ww_acquire_ctx *ww_ctx)\r\n{\r\nstruct mutex_waiter *cur;\r\nstruct list_head *pos;\r\nif (!ww_ctx) {\r\nlist_add_tail(&waiter->list, &lock->wait_list);\r\nreturn 0;\r\n}\r\npos = &lock->wait_list;\r\nlist_for_each_entry_reverse(cur, &lock->wait_list, list) {\r\nif (!cur->ww_ctx)\r\ncontinue;\r\nif (__ww_ctx_stamp_after(ww_ctx, cur->ww_ctx)) {\r\nif (ww_ctx->acquired > 0) {\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nstruct ww_mutex *ww;\r\nww = container_of(lock, struct ww_mutex, base);\r\nDEBUG_LOCKS_WARN_ON(ww_ctx->contending_lock);\r\nww_ctx->contending_lock = ww;\r\n#endif\r\nreturn -EDEADLK;\r\n}\r\nbreak;\r\n}\r\npos = &cur->list;\r\nif (cur->ww_ctx->acquired > 0) {\r\ndebug_mutex_wake_waiter(lock, cur);\r\nwake_up_process(cur->task);\r\n}\r\n}\r\nlist_add_tail(&waiter->list, pos);\r\nreturn 0;\r\n}\r\nstatic __always_inline int __sched\r\n__mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,\r\nstruct lockdep_map *nest_lock, unsigned long ip,\r\nstruct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)\r\n{\r\nstruct mutex_waiter waiter;\r\nbool first = false;\r\nstruct ww_mutex *ww;\r\nint ret;\r\nmight_sleep();\r\nww = container_of(lock, struct ww_mutex, base);\r\nif (use_ww_ctx && ww_ctx) {\r\nif (unlikely(ww_ctx == READ_ONCE(ww->ctx)))\r\nreturn -EALREADY;\r\n}\r\npreempt_disable();\r\nmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\r\nif (__mutex_trylock(lock) ||\r\nmutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, NULL)) {\r\nlock_acquired(&lock->dep_map, ip);\r\nif (use_ww_ctx && ww_ctx)\r\nww_mutex_set_context_fastpath(ww, ww_ctx);\r\npreempt_enable();\r\nreturn 0;\r\n}\r\nspin_lock(&lock->wait_lock);\r\nif (__mutex_trylock(lock)) {\r\nif (use_ww_ctx && ww_ctx)\r\n__ww_mutex_wakeup_for_backoff(lock, ww_ctx);\r\ngoto skip_wait;\r\n}\r\ndebug_mutex_lock_common(lock, &waiter);\r\ndebug_mutex_add_waiter(lock, &waiter, current);\r\nlock_contended(&lock->dep_map, ip);\r\nif (!use_ww_ctx) {\r\nlist_add_tail(&waiter.list, &lock->wait_list);\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nwaiter.ww_ctx = MUTEX_POISON_WW_CTX;\r\n#endif\r\n} else {\r\nret = __ww_mutex_add_waiter(&waiter, lock, ww_ctx);\r\nif (ret)\r\ngoto err_early_backoff;\r\nwaiter.ww_ctx = ww_ctx;\r\n}\r\nwaiter.task = current;\r\nif (__mutex_waiter_is_first(lock, &waiter))\r\n__mutex_set_flag(lock, MUTEX_FLAG_WAITERS);\r\nset_current_state(state);\r\nfor (;;) {\r\nif (__mutex_trylock(lock))\r\ngoto acquired;\r\nif (unlikely(signal_pending_state(state, current))) {\r\nret = -EINTR;\r\ngoto err;\r\n}\r\nif (use_ww_ctx && ww_ctx && ww_ctx->acquired > 0) {\r\nret = __ww_mutex_lock_check_stamp(lock, &waiter, ww_ctx);\r\nif (ret)\r\ngoto err;\r\n}\r\nspin_unlock(&lock->wait_lock);\r\nschedule_preempt_disabled();\r\nif ((use_ww_ctx && ww_ctx) || !first) {\r\nfirst = __mutex_waiter_is_first(lock, &waiter);\r\nif (first)\r\n__mutex_set_flag(lock, MUTEX_FLAG_HANDOFF);\r\n}\r\nset_current_state(state);\r\nif (__mutex_trylock(lock) ||\r\n(first && mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, &waiter)))\r\nbreak;\r\nspin_lock(&lock->wait_lock);\r\n}\r\nspin_lock(&lock->wait_lock);\r\nacquired:\r\n__set_current_state(TASK_RUNNING);\r\nmutex_remove_waiter(lock, &waiter, current);\r\nif (likely(list_empty(&lock->wait_list)))\r\n__mutex_clear_flag(lock, MUTEX_FLAGS);\r\ndebug_mutex_free_waiter(&waiter);\r\nskip_wait:\r\nlock_acquired(&lock->dep_map, ip);\r\nif (use_ww_ctx && ww_ctx)\r\nww_mutex_set_context_slowpath(ww, ww_ctx);\r\nspin_unlock(&lock->wait_lock);\r\npreempt_enable();\r\nreturn 0;\r\nerr:\r\n__set_current_state(TASK_RUNNING);\r\nmutex_remove_waiter(lock, &waiter, current);\r\nerr_early_backoff:\r\nspin_unlock(&lock->wait_lock);\r\ndebug_mutex_free_waiter(&waiter);\r\nmutex_release(&lock->dep_map, 1, ip);\r\npreempt_enable();\r\nreturn ret;\r\n}\r\nstatic int __sched\r\n__mutex_lock(struct mutex *lock, long state, unsigned int subclass,\r\nstruct lockdep_map *nest_lock, unsigned long ip)\r\n{\r\nreturn __mutex_lock_common(lock, state, subclass, nest_lock, ip, NULL, false);\r\n}\r\nstatic int __sched\r\n__ww_mutex_lock(struct mutex *lock, long state, unsigned int subclass,\r\nstruct lockdep_map *nest_lock, unsigned long ip,\r\nstruct ww_acquire_ctx *ww_ctx)\r\n{\r\nreturn __mutex_lock_common(lock, state, subclass, nest_lock, ip, ww_ctx, true);\r\n}\r\nvoid __sched\r\nmutex_lock_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\n__mutex_lock(lock, TASK_UNINTERRUPTIBLE, subclass, NULL, _RET_IP_);\r\n}\r\nvoid __sched\r\n_mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest)\r\n{\r\n__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, nest, _RET_IP_);\r\n}\r\nint __sched\r\nmutex_lock_killable_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\nreturn __mutex_lock(lock, TASK_KILLABLE, subclass, NULL, _RET_IP_);\r\n}\r\nint __sched\r\nmutex_lock_interruptible_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\nreturn __mutex_lock(lock, TASK_INTERRUPTIBLE, subclass, NULL, _RET_IP_);\r\n}\r\nvoid __sched\r\nmutex_lock_io_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\nint token;\r\nmight_sleep();\r\ntoken = io_schedule_prepare();\r\n__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE,\r\nsubclass, NULL, _RET_IP_, NULL, 0);\r\nio_schedule_finish(token);\r\n}\r\nstatic inline int\r\nww_mutex_deadlock_injection(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\n#ifdef CONFIG_DEBUG_WW_MUTEX_SLOWPATH\r\nunsigned tmp;\r\nif (ctx->deadlock_inject_countdown-- == 0) {\r\ntmp = ctx->deadlock_inject_interval;\r\nif (tmp > UINT_MAX/4)\r\ntmp = UINT_MAX;\r\nelse\r\ntmp = tmp*2 + tmp + tmp/2;\r\nctx->deadlock_inject_interval = tmp;\r\nctx->deadlock_inject_countdown = tmp;\r\nctx->contending_lock = lock;\r\nww_mutex_unlock(lock);\r\nreturn -EDEADLK;\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nint __sched\r\nww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __ww_mutex_lock(&lock->base, TASK_UNINTERRUPTIBLE,\r\n0, ctx ? &ctx->dep_map : NULL, _RET_IP_,\r\nctx);\r\nif (!ret && ctx && ctx->acquired > 1)\r\nreturn ww_mutex_deadlock_injection(lock, ctx);\r\nreturn ret;\r\n}\r\nint __sched\r\nww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __ww_mutex_lock(&lock->base, TASK_INTERRUPTIBLE,\r\n0, ctx ? &ctx->dep_map : NULL, _RET_IP_,\r\nctx);\r\nif (!ret && ctx && ctx->acquired > 1)\r\nreturn ww_mutex_deadlock_injection(lock, ctx);\r\nreturn ret;\r\n}\r\nstatic noinline void __sched __mutex_unlock_slowpath(struct mutex *lock, unsigned long ip)\r\n{\r\nstruct task_struct *next = NULL;\r\nDEFINE_WAKE_Q(wake_q);\r\nunsigned long owner;\r\nmutex_release(&lock->dep_map, 1, ip);\r\nowner = atomic_long_read(&lock->owner);\r\nfor (;;) {\r\nunsigned long old;\r\n#ifdef CONFIG_DEBUG_MUTEXES\r\nDEBUG_LOCKS_WARN_ON(__owner_task(owner) != current);\r\nDEBUG_LOCKS_WARN_ON(owner & MUTEX_FLAG_PICKUP);\r\n#endif\r\nif (owner & MUTEX_FLAG_HANDOFF)\r\nbreak;\r\nold = atomic_long_cmpxchg_release(&lock->owner, owner,\r\n__owner_flags(owner));\r\nif (old == owner) {\r\nif (owner & MUTEX_FLAG_WAITERS)\r\nbreak;\r\nreturn;\r\n}\r\nowner = old;\r\n}\r\nspin_lock(&lock->wait_lock);\r\ndebug_mutex_unlock(lock);\r\nif (!list_empty(&lock->wait_list)) {\r\nstruct mutex_waiter *waiter =\r\nlist_first_entry(&lock->wait_list,\r\nstruct mutex_waiter, list);\r\nnext = waiter->task;\r\ndebug_mutex_wake_waiter(lock, waiter);\r\nwake_q_add(&wake_q, next);\r\n}\r\nif (owner & MUTEX_FLAG_HANDOFF)\r\n__mutex_handoff(lock, next);\r\nspin_unlock(&lock->wait_lock);\r\nwake_up_q(&wake_q);\r\n}\r\nint __sched mutex_lock_interruptible(struct mutex *lock)\r\n{\r\nmight_sleep();\r\nif (__mutex_trylock_fast(lock))\r\nreturn 0;\r\nreturn __mutex_lock_interruptible_slowpath(lock);\r\n}\r\nint __sched mutex_lock_killable(struct mutex *lock)\r\n{\r\nmight_sleep();\r\nif (__mutex_trylock_fast(lock))\r\nreturn 0;\r\nreturn __mutex_lock_killable_slowpath(lock);\r\n}\r\nvoid __sched mutex_lock_io(struct mutex *lock)\r\n{\r\nint token;\r\ntoken = io_schedule_prepare();\r\nmutex_lock(lock);\r\nio_schedule_finish(token);\r\n}\r\nstatic noinline void __sched\r\n__mutex_lock_slowpath(struct mutex *lock)\r\n{\r\n__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\r\n}\r\nstatic noinline int __sched\r\n__mutex_lock_killable_slowpath(struct mutex *lock)\r\n{\r\nreturn __mutex_lock(lock, TASK_KILLABLE, 0, NULL, _RET_IP_);\r\n}\r\nstatic noinline int __sched\r\n__mutex_lock_interruptible_slowpath(struct mutex *lock)\r\n{\r\nreturn __mutex_lock(lock, TASK_INTERRUPTIBLE, 0, NULL, _RET_IP_);\r\n}\r\nstatic noinline int __sched\r\n__ww_mutex_lock_slowpath(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nreturn __ww_mutex_lock(&lock->base, TASK_UNINTERRUPTIBLE, 0, NULL,\r\n_RET_IP_, ctx);\r\n}\r\nstatic noinline int __sched\r\n__ww_mutex_lock_interruptible_slowpath(struct ww_mutex *lock,\r\nstruct ww_acquire_ctx *ctx)\r\n{\r\nreturn __ww_mutex_lock(&lock->base, TASK_INTERRUPTIBLE, 0, NULL,\r\n_RET_IP_, ctx);\r\n}\r\nint __sched mutex_trylock(struct mutex *lock)\r\n{\r\nbool locked = __mutex_trylock(lock);\r\nif (locked)\r\nmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\r\nreturn locked;\r\n}\r\nint __sched\r\nww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nmight_sleep();\r\nif (__mutex_trylock_fast(&lock->base)) {\r\nif (ctx)\r\nww_mutex_set_context_fastpath(lock, ctx);\r\nreturn 0;\r\n}\r\nreturn __ww_mutex_lock_slowpath(lock, ctx);\r\n}\r\nint __sched\r\nww_mutex_lock_interruptible(struct ww_mutex *lock, struct ww_acquire_ctx *ctx)\r\n{\r\nmight_sleep();\r\nif (__mutex_trylock_fast(&lock->base)) {\r\nif (ctx)\r\nww_mutex_set_context_fastpath(lock, ctx);\r\nreturn 0;\r\n}\r\nreturn __ww_mutex_lock_interruptible_slowpath(lock, ctx);\r\n}\r\nint atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)\r\n{\r\nif (atomic_add_unless(cnt, -1, 1))\r\nreturn 0;\r\nmutex_lock(lock);\r\nif (!atomic_dec_and_test(cnt)) {\r\nmutex_unlock(lock);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}
