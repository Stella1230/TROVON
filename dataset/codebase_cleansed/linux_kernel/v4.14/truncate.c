static void clear_shadow_entry(struct address_space *mapping, pgoff_t index,\r\nvoid *entry)\r\n{\r\nstruct radix_tree_node *node;\r\nvoid **slot;\r\nspin_lock_irq(&mapping->tree_lock);\r\nif (!__radix_tree_lookup(&mapping->page_tree, index, &node, &slot))\r\ngoto unlock;\r\nif (*slot != entry)\r\ngoto unlock;\r\n__radix_tree_replace(&mapping->page_tree, node, slot, NULL,\r\nworkingset_update_node, mapping);\r\nmapping->nrexceptional--;\r\nunlock:\r\nspin_unlock_irq(&mapping->tree_lock);\r\n}\r\nstatic void truncate_exceptional_entry(struct address_space *mapping,\r\npgoff_t index, void *entry)\r\n{\r\nif (shmem_mapping(mapping))\r\nreturn;\r\nif (dax_mapping(mapping)) {\r\ndax_delete_mapping_entry(mapping, index);\r\nreturn;\r\n}\r\nclear_shadow_entry(mapping, index, entry);\r\n}\r\nstatic int invalidate_exceptional_entry(struct address_space *mapping,\r\npgoff_t index, void *entry)\r\n{\r\nif (shmem_mapping(mapping) || dax_mapping(mapping))\r\nreturn 1;\r\nclear_shadow_entry(mapping, index, entry);\r\nreturn 1;\r\n}\r\nstatic int invalidate_exceptional_entry2(struct address_space *mapping,\r\npgoff_t index, void *entry)\r\n{\r\nif (shmem_mapping(mapping))\r\nreturn 1;\r\nif (dax_mapping(mapping))\r\nreturn dax_invalidate_mapping_entry_sync(mapping, index);\r\nclear_shadow_entry(mapping, index, entry);\r\nreturn 1;\r\n}\r\nvoid do_invalidatepage(struct page *page, unsigned int offset,\r\nunsigned int length)\r\n{\r\nvoid (*invalidatepage)(struct page *, unsigned int, unsigned int);\r\ninvalidatepage = page->mapping->a_ops->invalidatepage;\r\n#ifdef CONFIG_BLOCK\r\nif (!invalidatepage)\r\ninvalidatepage = block_invalidatepage;\r\n#endif\r\nif (invalidatepage)\r\n(*invalidatepage)(page, offset, length);\r\n}\r\nstatic int\r\ntruncate_complete_page(struct address_space *mapping, struct page *page)\r\n{\r\nif (page->mapping != mapping)\r\nreturn -EIO;\r\nif (page_has_private(page))\r\ndo_invalidatepage(page, 0, PAGE_SIZE);\r\ncancel_dirty_page(page);\r\nClearPageMappedToDisk(page);\r\ndelete_from_page_cache(page);\r\nreturn 0;\r\n}\r\nstatic int\r\ninvalidate_complete_page(struct address_space *mapping, struct page *page)\r\n{\r\nint ret;\r\nif (page->mapping != mapping)\r\nreturn 0;\r\nif (page_has_private(page) && !try_to_release_page(page, 0))\r\nreturn 0;\r\nret = remove_mapping(mapping, page);\r\nreturn ret;\r\n}\r\nint truncate_inode_page(struct address_space *mapping, struct page *page)\r\n{\r\nloff_t holelen;\r\nVM_BUG_ON_PAGE(PageTail(page), page);\r\nholelen = PageTransHuge(page) ? HPAGE_PMD_SIZE : PAGE_SIZE;\r\nif (page_mapped(page)) {\r\nunmap_mapping_range(mapping,\r\n(loff_t)page->index << PAGE_SHIFT,\r\nholelen, 0);\r\n}\r\nreturn truncate_complete_page(mapping, page);\r\n}\r\nint generic_error_remove_page(struct address_space *mapping, struct page *page)\r\n{\r\nif (!mapping)\r\nreturn -EINVAL;\r\nif (!S_ISREG(mapping->host->i_mode))\r\nreturn -EIO;\r\nreturn truncate_inode_page(mapping, page);\r\n}\r\nint invalidate_inode_page(struct page *page)\r\n{\r\nstruct address_space *mapping = page_mapping(page);\r\nif (!mapping)\r\nreturn 0;\r\nif (PageDirty(page) || PageWriteback(page))\r\nreturn 0;\r\nif (page_mapped(page))\r\nreturn 0;\r\nreturn invalidate_complete_page(mapping, page);\r\n}\r\nvoid truncate_inode_pages_range(struct address_space *mapping,\r\nloff_t lstart, loff_t lend)\r\n{\r\npgoff_t start;\r\npgoff_t end;\r\nunsigned int partial_start;\r\nunsigned int partial_end;\r\nstruct pagevec pvec;\r\npgoff_t indices[PAGEVEC_SIZE];\r\npgoff_t index;\r\nint i;\r\nif (mapping->nrpages == 0 && mapping->nrexceptional == 0)\r\ngoto out;\r\npartial_start = lstart & (PAGE_SIZE - 1);\r\npartial_end = (lend + 1) & (PAGE_SIZE - 1);\r\nstart = (lstart + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nif (lend == -1)\r\nend = -1;\r\nelse\r\nend = (lend + 1) >> PAGE_SHIFT;\r\npagevec_init(&pvec, 0);\r\nindex = start;\r\nwhile (index < end && pagevec_lookup_entries(&pvec, mapping, index,\r\nmin(end - index, (pgoff_t)PAGEVEC_SIZE),\r\nindices)) {\r\nfor (i = 0; i < pagevec_count(&pvec); i++) {\r\nstruct page *page = pvec.pages[i];\r\nindex = indices[i];\r\nif (index >= end)\r\nbreak;\r\nif (radix_tree_exceptional_entry(page)) {\r\ntruncate_exceptional_entry(mapping, index,\r\npage);\r\ncontinue;\r\n}\r\nif (!trylock_page(page))\r\ncontinue;\r\nWARN_ON(page_to_index(page) != index);\r\nif (PageWriteback(page)) {\r\nunlock_page(page);\r\ncontinue;\r\n}\r\ntruncate_inode_page(mapping, page);\r\nunlock_page(page);\r\n}\r\npagevec_remove_exceptionals(&pvec);\r\npagevec_release(&pvec);\r\ncond_resched();\r\nindex++;\r\n}\r\nif (partial_start) {\r\nstruct page *page = find_lock_page(mapping, start - 1);\r\nif (page) {\r\nunsigned int top = PAGE_SIZE;\r\nif (start > end) {\r\ntop = partial_end;\r\npartial_end = 0;\r\n}\r\nwait_on_page_writeback(page);\r\nzero_user_segment(page, partial_start, top);\r\ncleancache_invalidate_page(mapping, page);\r\nif (page_has_private(page))\r\ndo_invalidatepage(page, partial_start,\r\ntop - partial_start);\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\n}\r\nif (partial_end) {\r\nstruct page *page = find_lock_page(mapping, end);\r\nif (page) {\r\nwait_on_page_writeback(page);\r\nzero_user_segment(page, 0, partial_end);\r\ncleancache_invalidate_page(mapping, page);\r\nif (page_has_private(page))\r\ndo_invalidatepage(page, 0,\r\npartial_end);\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\n}\r\nif (start >= end)\r\ngoto out;\r\nindex = start;\r\nfor ( ; ; ) {\r\ncond_resched();\r\nif (!pagevec_lookup_entries(&pvec, mapping, index,\r\nmin(end - index, (pgoff_t)PAGEVEC_SIZE), indices)) {\r\nif (index == start)\r\nbreak;\r\nindex = start;\r\ncontinue;\r\n}\r\nif (index == start && indices[0] >= end) {\r\npagevec_remove_exceptionals(&pvec);\r\npagevec_release(&pvec);\r\nbreak;\r\n}\r\nfor (i = 0; i < pagevec_count(&pvec); i++) {\r\nstruct page *page = pvec.pages[i];\r\nindex = indices[i];\r\nif (index >= end) {\r\nindex = start - 1;\r\nbreak;\r\n}\r\nif (radix_tree_exceptional_entry(page)) {\r\ntruncate_exceptional_entry(mapping, index,\r\npage);\r\ncontinue;\r\n}\r\nlock_page(page);\r\nWARN_ON(page_to_index(page) != index);\r\nwait_on_page_writeback(page);\r\ntruncate_inode_page(mapping, page);\r\nunlock_page(page);\r\n}\r\npagevec_remove_exceptionals(&pvec);\r\npagevec_release(&pvec);\r\nindex++;\r\n}\r\nout:\r\ncleancache_invalidate_inode(mapping);\r\n}\r\nvoid truncate_inode_pages(struct address_space *mapping, loff_t lstart)\r\n{\r\ntruncate_inode_pages_range(mapping, lstart, (loff_t)-1);\r\n}\r\nvoid truncate_inode_pages_final(struct address_space *mapping)\r\n{\r\nunsigned long nrexceptional;\r\nunsigned long nrpages;\r\nmapping_set_exiting(mapping);\r\nnrpages = mapping->nrpages;\r\nsmp_rmb();\r\nnrexceptional = mapping->nrexceptional;\r\nif (nrpages || nrexceptional) {\r\nspin_lock_irq(&mapping->tree_lock);\r\nspin_unlock_irq(&mapping->tree_lock);\r\ntruncate_inode_pages(mapping, 0);\r\n}\r\n}\r\nunsigned long invalidate_mapping_pages(struct address_space *mapping,\r\npgoff_t start, pgoff_t end)\r\n{\r\npgoff_t indices[PAGEVEC_SIZE];\r\nstruct pagevec pvec;\r\npgoff_t index = start;\r\nunsigned long ret;\r\nunsigned long count = 0;\r\nint i;\r\npagevec_init(&pvec, 0);\r\nwhile (index <= end && pagevec_lookup_entries(&pvec, mapping, index,\r\nmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1,\r\nindices)) {\r\nfor (i = 0; i < pagevec_count(&pvec); i++) {\r\nstruct page *page = pvec.pages[i];\r\nindex = indices[i];\r\nif (index > end)\r\nbreak;\r\nif (radix_tree_exceptional_entry(page)) {\r\ninvalidate_exceptional_entry(mapping, index,\r\npage);\r\ncontinue;\r\n}\r\nif (!trylock_page(page))\r\ncontinue;\r\nWARN_ON(page_to_index(page) != index);\r\nif (PageTransTail(page)) {\r\nunlock_page(page);\r\ncontinue;\r\n} else if (PageTransHuge(page)) {\r\nindex += HPAGE_PMD_NR - 1;\r\ni += HPAGE_PMD_NR - 1;\r\nif (index > end) {\r\nunlock_page(page);\r\ncontinue;\r\n}\r\n}\r\nret = invalidate_inode_page(page);\r\nunlock_page(page);\r\nif (!ret)\r\ndeactivate_file_page(page);\r\ncount += ret;\r\n}\r\npagevec_remove_exceptionals(&pvec);\r\npagevec_release(&pvec);\r\ncond_resched();\r\nindex++;\r\n}\r\nreturn count;\r\n}\r\nstatic int\r\ninvalidate_complete_page2(struct address_space *mapping, struct page *page)\r\n{\r\nunsigned long flags;\r\nif (page->mapping != mapping)\r\nreturn 0;\r\nif (page_has_private(page) && !try_to_release_page(page, GFP_KERNEL))\r\nreturn 0;\r\nspin_lock_irqsave(&mapping->tree_lock, flags);\r\nif (PageDirty(page))\r\ngoto failed;\r\nBUG_ON(page_has_private(page));\r\n__delete_from_page_cache(page, NULL);\r\nspin_unlock_irqrestore(&mapping->tree_lock, flags);\r\nif (mapping->a_ops->freepage)\r\nmapping->a_ops->freepage(page);\r\nput_page(page);\r\nreturn 1;\r\nfailed:\r\nspin_unlock_irqrestore(&mapping->tree_lock, flags);\r\nreturn 0;\r\n}\r\nstatic int do_launder_page(struct address_space *mapping, struct page *page)\r\n{\r\nif (!PageDirty(page))\r\nreturn 0;\r\nif (page->mapping != mapping || mapping->a_ops->launder_page == NULL)\r\nreturn 0;\r\nreturn mapping->a_ops->launder_page(page);\r\n}\r\nint invalidate_inode_pages2_range(struct address_space *mapping,\r\npgoff_t start, pgoff_t end)\r\n{\r\npgoff_t indices[PAGEVEC_SIZE];\r\nstruct pagevec pvec;\r\npgoff_t index;\r\nint i;\r\nint ret = 0;\r\nint ret2 = 0;\r\nint did_range_unmap = 0;\r\nif (mapping->nrpages == 0 && mapping->nrexceptional == 0)\r\ngoto out;\r\npagevec_init(&pvec, 0);\r\nindex = start;\r\nwhile (index <= end && pagevec_lookup_entries(&pvec, mapping, index,\r\nmin(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1,\r\nindices)) {\r\nfor (i = 0; i < pagevec_count(&pvec); i++) {\r\nstruct page *page = pvec.pages[i];\r\nindex = indices[i];\r\nif (index > end)\r\nbreak;\r\nif (radix_tree_exceptional_entry(page)) {\r\nif (!invalidate_exceptional_entry2(mapping,\r\nindex, page))\r\nret = -EBUSY;\r\ncontinue;\r\n}\r\nlock_page(page);\r\nWARN_ON(page_to_index(page) != index);\r\nif (page->mapping != mapping) {\r\nunlock_page(page);\r\ncontinue;\r\n}\r\nwait_on_page_writeback(page);\r\nif (page_mapped(page)) {\r\nif (!did_range_unmap) {\r\nunmap_mapping_range(mapping,\r\n(loff_t)index << PAGE_SHIFT,\r\n(loff_t)(1 + end - index)\r\n<< PAGE_SHIFT,\r\n0);\r\ndid_range_unmap = 1;\r\n} else {\r\nunmap_mapping_range(mapping,\r\n(loff_t)index << PAGE_SHIFT,\r\nPAGE_SIZE, 0);\r\n}\r\n}\r\nBUG_ON(page_mapped(page));\r\nret2 = do_launder_page(mapping, page);\r\nif (ret2 == 0) {\r\nif (!invalidate_complete_page2(mapping, page))\r\nret2 = -EBUSY;\r\n}\r\nif (ret2 < 0)\r\nret = ret2;\r\nunlock_page(page);\r\n}\r\npagevec_remove_exceptionals(&pvec);\r\npagevec_release(&pvec);\r\ncond_resched();\r\nindex++;\r\n}\r\nif (dax_mapping(mapping)) {\r\nunmap_mapping_range(mapping, (loff_t)start << PAGE_SHIFT,\r\n(loff_t)(end - start + 1) << PAGE_SHIFT, 0);\r\n}\r\nout:\r\ncleancache_invalidate_inode(mapping);\r\nreturn ret;\r\n}\r\nint invalidate_inode_pages2(struct address_space *mapping)\r\n{\r\nreturn invalidate_inode_pages2_range(mapping, 0, -1);\r\n}\r\nvoid truncate_pagecache(struct inode *inode, loff_t newsize)\r\n{\r\nstruct address_space *mapping = inode->i_mapping;\r\nloff_t holebegin = round_up(newsize, PAGE_SIZE);\r\nunmap_mapping_range(mapping, holebegin, 0, 1);\r\ntruncate_inode_pages(mapping, newsize);\r\nunmap_mapping_range(mapping, holebegin, 0, 1);\r\n}\r\nvoid truncate_setsize(struct inode *inode, loff_t newsize)\r\n{\r\nloff_t oldsize = inode->i_size;\r\ni_size_write(inode, newsize);\r\nif (newsize > oldsize)\r\npagecache_isize_extended(inode, oldsize, newsize);\r\ntruncate_pagecache(inode, newsize);\r\n}\r\nvoid pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to)\r\n{\r\nint bsize = i_blocksize(inode);\r\nloff_t rounded_from;\r\nstruct page *page;\r\npgoff_t index;\r\nWARN_ON(to > inode->i_size);\r\nif (from >= to || bsize == PAGE_SIZE)\r\nreturn;\r\nrounded_from = round_up(from, bsize);\r\nif (to <= rounded_from || !(rounded_from & (PAGE_SIZE - 1)))\r\nreturn;\r\nindex = from >> PAGE_SHIFT;\r\npage = find_lock_page(inode->i_mapping, index);\r\nif (!page)\r\nreturn;\r\nif (page_mkclean(page))\r\nset_page_dirty(page);\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\nvoid truncate_pagecache_range(struct inode *inode, loff_t lstart, loff_t lend)\r\n{\r\nstruct address_space *mapping = inode->i_mapping;\r\nloff_t unmap_start = round_up(lstart, PAGE_SIZE);\r\nloff_t unmap_end = round_down(1 + lend, PAGE_SIZE) - 1;\r\nif ((u64)unmap_end > (u64)unmap_start)\r\nunmap_mapping_range(mapping, unmap_start,\r\n1 + unmap_end - unmap_start, 0);\r\ntruncate_inode_pages_range(mapping, lstart, lend);\r\n}
