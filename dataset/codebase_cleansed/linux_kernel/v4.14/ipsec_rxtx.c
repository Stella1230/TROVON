static inline __be16 mlx5e_ipsec_mss_inv(struct sk_buff *skb)\r\n{\r\nreturn mlx5e_ipsec_inverse_table[skb_shinfo(skb)->gso_size];\r\n}\r\nstatic struct mlx5e_ipsec_metadata *mlx5e_ipsec_add_metadata(struct sk_buff *skb)\r\n{\r\nstruct mlx5e_ipsec_metadata *mdata;\r\nstruct ethhdr *eth;\r\nif (unlikely(skb_cow_head(skb, sizeof(*mdata))))\r\nreturn ERR_PTR(-ENOMEM);\r\neth = (struct ethhdr *)skb_push(skb, sizeof(*mdata));\r\nskb->mac_header -= sizeof(*mdata);\r\nmdata = (struct mlx5e_ipsec_metadata *)(eth + 1);\r\nmemmove(skb->data, skb->data + sizeof(*mdata),\r\n2 * ETH_ALEN);\r\neth->h_proto = cpu_to_be16(MLX5E_METADATA_ETHER_TYPE);\r\nmemset(mdata->content.raw, 0, sizeof(mdata->content.raw));\r\nreturn mdata;\r\n}\r\nstatic int mlx5e_ipsec_remove_trailer(struct sk_buff *skb, struct xfrm_state *x)\r\n{\r\nunsigned int alen = crypto_aead_authsize(x->data);\r\nstruct ipv6hdr *ipv6hdr = ipv6_hdr(skb);\r\nstruct iphdr *ipv4hdr = ip_hdr(skb);\r\nunsigned int trailer_len;\r\nu8 plen;\r\nint ret;\r\nret = skb_copy_bits(skb, skb->len - alen - 2, &plen, 1);\r\nif (unlikely(ret))\r\nreturn ret;\r\ntrailer_len = alen + plen + 2;\r\npskb_trim(skb, skb->len - trailer_len);\r\nif (skb->protocol == htons(ETH_P_IP)) {\r\nipv4hdr->tot_len = htons(ntohs(ipv4hdr->tot_len) - trailer_len);\r\nip_send_check(ipv4hdr);\r\n} else {\r\nipv6hdr->payload_len = htons(ntohs(ipv6hdr->payload_len) -\r\ntrailer_len);\r\n}\r\nreturn 0;\r\n}\r\nstatic void mlx5e_ipsec_set_swp(struct sk_buff *skb,\r\nstruct mlx5_wqe_eth_seg *eseg, u8 mode,\r\nstruct xfrm_offload *xo)\r\n{\r\nu8 proto;\r\neseg->swp_outer_l3_offset = skb_network_offset(skb) / 2;\r\nif (skb->protocol == htons(ETH_P_IPV6))\r\neseg->swp_flags |= MLX5_ETH_WQE_SWP_OUTER_L3_IPV6;\r\nif (mode == XFRM_MODE_TUNNEL) {\r\neseg->swp_inner_l3_offset = skb_inner_network_offset(skb) / 2;\r\nif (xo->proto == IPPROTO_IPV6) {\r\neseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;\r\nproto = inner_ipv6_hdr(skb)->nexthdr;\r\n} else {\r\nproto = inner_ip_hdr(skb)->protocol;\r\n}\r\n} else {\r\neseg->swp_inner_l3_offset = skb_network_offset(skb) / 2;\r\nif (skb->protocol == htons(ETH_P_IPV6))\r\neseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;\r\nproto = xo->proto;\r\n}\r\nswitch (proto) {\r\ncase IPPROTO_UDP:\r\neseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L4_UDP;\r\ncase IPPROTO_TCP:\r\neseg->swp_inner_l4_offset = skb_inner_transport_offset(skb) / 2;\r\nbreak;\r\n}\r\n}\r\nstatic void mlx5e_ipsec_set_iv(struct sk_buff *skb, struct xfrm_offload *xo)\r\n{\r\nint iv_offset;\r\n__be64 seqno;\r\nseqno = cpu_to_be64(xo->seq.low + ((u64)xo->seq.hi << 32));\r\niv_offset = skb_transport_offset(skb) + sizeof(struct ip_esp_hdr);\r\nskb_store_bits(skb, iv_offset, &seqno, 8);\r\n}\r\nstatic void mlx5e_ipsec_set_metadata(struct sk_buff *skb,\r\nstruct mlx5e_ipsec_metadata *mdata,\r\nstruct xfrm_offload *xo)\r\n{\r\nstruct ip_esp_hdr *esph;\r\nstruct tcphdr *tcph;\r\nif (skb_is_gso(skb)) {\r\nesph = ip_esp_hdr(skb);\r\ntcph = inner_tcp_hdr(skb);\r\nnetdev_dbg(skb->dev, " Offloading GSO packet outer L3 %u; L4 %u; Inner L3 %u; L4 %u\n",\r\nskb->network_header,\r\nskb->transport_header,\r\nskb->inner_network_header,\r\nskb->inner_transport_header);\r\nnetdev_dbg(skb->dev, " Offloading GSO packet of len %u; mss %u; TCP sp %u dp %u seq 0x%x ESP seq 0x%x\n",\r\nskb->len, skb_shinfo(skb)->gso_size,\r\nntohs(tcph->source), ntohs(tcph->dest),\r\nntohl(tcph->seq), ntohl(esph->seq_no));\r\nmdata->syndrome = MLX5E_IPSEC_TX_SYNDROME_OFFLOAD_WITH_LSO_TCP;\r\nmdata->content.tx.mss_inv = mlx5e_ipsec_mss_inv(skb);\r\nmdata->content.tx.seq = htons(ntohl(tcph->seq) & 0xFFFF);\r\n} else {\r\nmdata->syndrome = MLX5E_IPSEC_TX_SYNDROME_OFFLOAD;\r\n}\r\nmdata->content.tx.esp_next_proto = xo->proto;\r\nnetdev_dbg(skb->dev, " TX metadata syndrome %u proto %u mss_inv %04x seq %04x\n",\r\nmdata->syndrome, mdata->content.tx.esp_next_proto,\r\nntohs(mdata->content.tx.mss_inv),\r\nntohs(mdata->content.tx.seq));\r\n}\r\nstruct sk_buff *mlx5e_ipsec_handle_tx_skb(struct net_device *netdev,\r\nstruct mlx5e_tx_wqe *wqe,\r\nstruct sk_buff *skb)\r\n{\r\nstruct mlx5e_priv *priv = netdev_priv(netdev);\r\nstruct xfrm_offload *xo = xfrm_offload(skb);\r\nstruct mlx5e_ipsec_metadata *mdata;\r\nstruct xfrm_state *x;\r\nif (!xo)\r\nreturn skb;\r\nif (unlikely(skb->sp->len != 1)) {\r\natomic64_inc(&priv->ipsec->sw_stats.ipsec_tx_drop_bundle);\r\ngoto drop;\r\n}\r\nx = xfrm_input_state(skb);\r\nif (unlikely(!x)) {\r\natomic64_inc(&priv->ipsec->sw_stats.ipsec_tx_drop_no_state);\r\ngoto drop;\r\n}\r\nif (unlikely(!x->xso.offload_handle ||\r\n(skb->protocol != htons(ETH_P_IP) &&\r\nskb->protocol != htons(ETH_P_IPV6)))) {\r\natomic64_inc(&priv->ipsec->sw_stats.ipsec_tx_drop_not_ip);\r\ngoto drop;\r\n}\r\nif (!skb_is_gso(skb))\r\nif (unlikely(mlx5e_ipsec_remove_trailer(skb, x))) {\r\natomic64_inc(&priv->ipsec->sw_stats.ipsec_tx_drop_trailer);\r\ngoto drop;\r\n}\r\nmdata = mlx5e_ipsec_add_metadata(skb);\r\nif (unlikely(IS_ERR(mdata))) {\r\natomic64_inc(&priv->ipsec->sw_stats.ipsec_tx_drop_metadata);\r\ngoto drop;\r\n}\r\nmlx5e_ipsec_set_swp(skb, &wqe->eth, x->props.mode, xo);\r\nmlx5e_ipsec_set_iv(skb, xo);\r\nmlx5e_ipsec_set_metadata(skb, mdata, xo);\r\nreturn skb;\r\ndrop:\r\nkfree_skb(skb);\r\nreturn NULL;\r\n}\r\nstatic inline struct xfrm_state *\r\nmlx5e_ipsec_build_sp(struct net_device *netdev, struct sk_buff *skb,\r\nstruct mlx5e_ipsec_metadata *mdata)\r\n{\r\nstruct mlx5e_priv *priv = netdev_priv(netdev);\r\nstruct xfrm_offload *xo;\r\nstruct xfrm_state *xs;\r\nu32 sa_handle;\r\nskb->sp = secpath_dup(skb->sp);\r\nif (unlikely(!skb->sp)) {\r\natomic64_inc(&priv->ipsec->sw_stats.ipsec_rx_drop_sp_alloc);\r\nreturn NULL;\r\n}\r\nsa_handle = be32_to_cpu(mdata->content.rx.sa_handle);\r\nxs = mlx5e_ipsec_sadb_rx_lookup(priv->ipsec, sa_handle);\r\nif (unlikely(!xs)) {\r\natomic64_inc(&priv->ipsec->sw_stats.ipsec_rx_drop_sadb_miss);\r\nreturn NULL;\r\n}\r\nskb->sp->xvec[skb->sp->len++] = xs;\r\nskb->sp->olen++;\r\nxo = xfrm_offload(skb);\r\nxo->flags = CRYPTO_DONE;\r\nswitch (mdata->syndrome) {\r\ncase MLX5E_IPSEC_RX_SYNDROME_DECRYPTED:\r\nxo->status = CRYPTO_SUCCESS;\r\nbreak;\r\ncase MLX5E_IPSEC_RX_SYNDROME_AUTH_FAILED:\r\nxo->status = CRYPTO_TUNNEL_ESP_AUTH_FAILED;\r\nbreak;\r\ndefault:\r\natomic64_inc(&priv->ipsec->sw_stats.ipsec_rx_drop_syndrome);\r\nreturn NULL;\r\n}\r\nreturn xs;\r\n}\r\nstruct sk_buff *mlx5e_ipsec_handle_rx_skb(struct net_device *netdev,\r\nstruct sk_buff *skb)\r\n{\r\nstruct mlx5e_ipsec_metadata *mdata;\r\nstruct ethhdr *old_eth;\r\nstruct ethhdr *new_eth;\r\nstruct xfrm_state *xs;\r\n__be16 *ethtype;\r\nif (skb->len < ETH_HLEN + MLX5E_METADATA_ETHER_LEN)\r\nreturn skb;\r\nethtype = (__be16 *)(skb->data + ETH_ALEN * 2);\r\nif (*ethtype != cpu_to_be16(MLX5E_METADATA_ETHER_TYPE))\r\nreturn skb;\r\nmdata = (struct mlx5e_ipsec_metadata *)(skb->data + ETH_HLEN);\r\nxs = mlx5e_ipsec_build_sp(netdev, skb, mdata);\r\nif (unlikely(!xs)) {\r\nkfree_skb(skb);\r\nreturn NULL;\r\n}\r\nold_eth = (struct ethhdr *)skb->data;\r\nnew_eth = (struct ethhdr *)(skb->data + MLX5E_METADATA_ETHER_LEN);\r\nmemmove(new_eth, old_eth, 2 * ETH_ALEN);\r\nskb_pull_inline(skb, MLX5E_METADATA_ETHER_LEN);\r\nreturn skb;\r\n}\r\nbool mlx5e_ipsec_feature_check(struct sk_buff *skb, struct net_device *netdev,\r\nnetdev_features_t features)\r\n{\r\nstruct xfrm_state *x;\r\nif (skb->sp && skb->sp->len) {\r\nx = skb->sp->xvec[0];\r\nif (x && x->xso.offload_handle)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nvoid mlx5e_ipsec_build_inverse_table(void)\r\n{\r\nu16 mss_inv;\r\nu32 mss;\r\nmlx5e_ipsec_inverse_table[1] = htons(0xFFFF);\r\nfor (mss = 2; mss < MAX_LSO_MSS; mss++) {\r\nmss_inv = div_u64(1ULL << 32, mss) >> 16;\r\nmlx5e_ipsec_inverse_table[mss] = htons(mss_inv);\r\n}\r\n}
