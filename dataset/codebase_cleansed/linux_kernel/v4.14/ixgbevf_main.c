static void ixgbevf_service_event_schedule(struct ixgbevf_adapter *adapter)\r\n{\r\nif (!test_bit(__IXGBEVF_DOWN, &adapter->state) &&\r\n!test_bit(__IXGBEVF_REMOVING, &adapter->state) &&\r\n!test_and_set_bit(__IXGBEVF_SERVICE_SCHED, &adapter->state))\r\nqueue_work(ixgbevf_wq, &adapter->service_task);\r\n}\r\nstatic void ixgbevf_service_event_complete(struct ixgbevf_adapter *adapter)\r\n{\r\nBUG_ON(!test_bit(__IXGBEVF_SERVICE_SCHED, &adapter->state));\r\nsmp_mb__before_atomic();\r\nclear_bit(__IXGBEVF_SERVICE_SCHED, &adapter->state);\r\n}\r\nstatic void ixgbevf_remove_adapter(struct ixgbe_hw *hw)\r\n{\r\nstruct ixgbevf_adapter *adapter = hw->back;\r\nif (!hw->hw_addr)\r\nreturn;\r\nhw->hw_addr = NULL;\r\ndev_err(&adapter->pdev->dev, "Adapter removed\n");\r\nif (test_bit(__IXGBEVF_SERVICE_INITED, &adapter->state))\r\nixgbevf_service_event_schedule(adapter);\r\n}\r\nstatic void ixgbevf_check_remove(struct ixgbe_hw *hw, u32 reg)\r\n{\r\nu32 value;\r\nif (reg == IXGBE_VFSTATUS) {\r\nixgbevf_remove_adapter(hw);\r\nreturn;\r\n}\r\nvalue = ixgbevf_read_reg(hw, IXGBE_VFSTATUS);\r\nif (value == IXGBE_FAILED_READ_REG)\r\nixgbevf_remove_adapter(hw);\r\n}\r\nu32 ixgbevf_read_reg(struct ixgbe_hw *hw, u32 reg)\r\n{\r\nu8 __iomem *reg_addr = ACCESS_ONCE(hw->hw_addr);\r\nu32 value;\r\nif (IXGBE_REMOVED(reg_addr))\r\nreturn IXGBE_FAILED_READ_REG;\r\nvalue = readl(reg_addr + reg);\r\nif (unlikely(value == IXGBE_FAILED_READ_REG))\r\nixgbevf_check_remove(hw, reg);\r\nreturn value;\r\n}\r\nstatic void ixgbevf_set_ivar(struct ixgbevf_adapter *adapter, s8 direction,\r\nu8 queue, u8 msix_vector)\r\n{\r\nu32 ivar, index;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nif (direction == -1) {\r\nmsix_vector |= IXGBE_IVAR_ALLOC_VAL;\r\nivar = IXGBE_READ_REG(hw, IXGBE_VTIVAR_MISC);\r\nivar &= ~0xFF;\r\nivar |= msix_vector;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTIVAR_MISC, ivar);\r\n} else {\r\nmsix_vector |= IXGBE_IVAR_ALLOC_VAL;\r\nindex = ((16 * (queue & 1)) + (8 * direction));\r\nivar = IXGBE_READ_REG(hw, IXGBE_VTIVAR(queue >> 1));\r\nivar &= ~(0xFF << index);\r\nivar |= (msix_vector << index);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTIVAR(queue >> 1), ivar);\r\n}\r\n}\r\nstatic void ixgbevf_unmap_and_free_tx_resource(struct ixgbevf_ring *tx_ring,\r\nstruct ixgbevf_tx_buffer *tx_buffer)\r\n{\r\nif (tx_buffer->skb) {\r\ndev_kfree_skb_any(tx_buffer->skb);\r\nif (dma_unmap_len(tx_buffer, len))\r\ndma_unmap_single(tx_ring->dev,\r\ndma_unmap_addr(tx_buffer, dma),\r\ndma_unmap_len(tx_buffer, len),\r\nDMA_TO_DEVICE);\r\n} else if (dma_unmap_len(tx_buffer, len)) {\r\ndma_unmap_page(tx_ring->dev,\r\ndma_unmap_addr(tx_buffer, dma),\r\ndma_unmap_len(tx_buffer, len),\r\nDMA_TO_DEVICE);\r\n}\r\ntx_buffer->next_to_watch = NULL;\r\ntx_buffer->skb = NULL;\r\ndma_unmap_len_set(tx_buffer, len, 0);\r\n}\r\nstatic u64 ixgbevf_get_tx_completed(struct ixgbevf_ring *ring)\r\n{\r\nreturn ring->stats.packets;\r\n}\r\nstatic u32 ixgbevf_get_tx_pending(struct ixgbevf_ring *ring)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(ring->netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 head = IXGBE_READ_REG(hw, IXGBE_VFTDH(ring->reg_idx));\r\nu32 tail = IXGBE_READ_REG(hw, IXGBE_VFTDT(ring->reg_idx));\r\nif (head != tail)\r\nreturn (head < tail) ?\r\ntail - head : (tail + ring->count - head);\r\nreturn 0;\r\n}\r\nstatic inline bool ixgbevf_check_tx_hang(struct ixgbevf_ring *tx_ring)\r\n{\r\nu32 tx_done = ixgbevf_get_tx_completed(tx_ring);\r\nu32 tx_done_old = tx_ring->tx_stats.tx_done_old;\r\nu32 tx_pending = ixgbevf_get_tx_pending(tx_ring);\r\nclear_check_for_tx_hang(tx_ring);\r\nif ((tx_done_old == tx_done) && tx_pending) {\r\nreturn test_and_set_bit(__IXGBEVF_HANG_CHECK_ARMED,\r\n&tx_ring->state);\r\n}\r\nclear_bit(__IXGBEVF_HANG_CHECK_ARMED, &tx_ring->state);\r\ntx_ring->tx_stats.tx_done_old = tx_done;\r\nreturn false;\r\n}\r\nstatic void ixgbevf_tx_timeout_reset(struct ixgbevf_adapter *adapter)\r\n{\r\nif (!test_bit(__IXGBEVF_DOWN, &adapter->state)) {\r\nset_bit(__IXGBEVF_RESET_REQUESTED, &adapter->state);\r\nixgbevf_service_event_schedule(adapter);\r\n}\r\n}\r\nstatic void ixgbevf_tx_timeout(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nixgbevf_tx_timeout_reset(adapter);\r\n}\r\nstatic bool ixgbevf_clean_tx_irq(struct ixgbevf_q_vector *q_vector,\r\nstruct ixgbevf_ring *tx_ring, int napi_budget)\r\n{\r\nstruct ixgbevf_adapter *adapter = q_vector->adapter;\r\nstruct ixgbevf_tx_buffer *tx_buffer;\r\nunion ixgbe_adv_tx_desc *tx_desc;\r\nunsigned int total_bytes = 0, total_packets = 0;\r\nunsigned int budget = tx_ring->count / 2;\r\nunsigned int i = tx_ring->next_to_clean;\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state))\r\nreturn true;\r\ntx_buffer = &tx_ring->tx_buffer_info[i];\r\ntx_desc = IXGBEVF_TX_DESC(tx_ring, i);\r\ni -= tx_ring->count;\r\ndo {\r\nunion ixgbe_adv_tx_desc *eop_desc = tx_buffer->next_to_watch;\r\nif (!eop_desc)\r\nbreak;\r\nread_barrier_depends();\r\nif (!(eop_desc->wb.status & cpu_to_le32(IXGBE_TXD_STAT_DD)))\r\nbreak;\r\ntx_buffer->next_to_watch = NULL;\r\ntotal_bytes += tx_buffer->bytecount;\r\ntotal_packets += tx_buffer->gso_segs;\r\nnapi_consume_skb(tx_buffer->skb, napi_budget);\r\ndma_unmap_single(tx_ring->dev,\r\ndma_unmap_addr(tx_buffer, dma),\r\ndma_unmap_len(tx_buffer, len),\r\nDMA_TO_DEVICE);\r\ntx_buffer->skb = NULL;\r\ndma_unmap_len_set(tx_buffer, len, 0);\r\nwhile (tx_desc != eop_desc) {\r\ntx_buffer++;\r\ntx_desc++;\r\ni++;\r\nif (unlikely(!i)) {\r\ni -= tx_ring->count;\r\ntx_buffer = tx_ring->tx_buffer_info;\r\ntx_desc = IXGBEVF_TX_DESC(tx_ring, 0);\r\n}\r\nif (dma_unmap_len(tx_buffer, len)) {\r\ndma_unmap_page(tx_ring->dev,\r\ndma_unmap_addr(tx_buffer, dma),\r\ndma_unmap_len(tx_buffer, len),\r\nDMA_TO_DEVICE);\r\ndma_unmap_len_set(tx_buffer, len, 0);\r\n}\r\n}\r\ntx_buffer++;\r\ntx_desc++;\r\ni++;\r\nif (unlikely(!i)) {\r\ni -= tx_ring->count;\r\ntx_buffer = tx_ring->tx_buffer_info;\r\ntx_desc = IXGBEVF_TX_DESC(tx_ring, 0);\r\n}\r\nprefetch(tx_desc);\r\nbudget--;\r\n} while (likely(budget));\r\ni += tx_ring->count;\r\ntx_ring->next_to_clean = i;\r\nu64_stats_update_begin(&tx_ring->syncp);\r\ntx_ring->stats.bytes += total_bytes;\r\ntx_ring->stats.packets += total_packets;\r\nu64_stats_update_end(&tx_ring->syncp);\r\nq_vector->tx.total_bytes += total_bytes;\r\nq_vector->tx.total_packets += total_packets;\r\nif (check_for_tx_hang(tx_ring) && ixgbevf_check_tx_hang(tx_ring)) {\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nunion ixgbe_adv_tx_desc *eop_desc;\r\neop_desc = tx_ring->tx_buffer_info[i].next_to_watch;\r\npr_err("Detected Tx Unit Hang\n"\r\n" Tx Queue <%d>\n"\r\n" TDH, TDT <%x>, <%x>\n"\r\n" next_to_use <%x>\n"\r\n" next_to_clean <%x>\n"\r\n"tx_buffer_info[next_to_clean]\n"\r\n" next_to_watch <%p>\n"\r\n" eop_desc->wb.status <%x>\n"\r\n" time_stamp <%lx>\n"\r\n" jiffies <%lx>\n",\r\ntx_ring->queue_index,\r\nIXGBE_READ_REG(hw, IXGBE_VFTDH(tx_ring->reg_idx)),\r\nIXGBE_READ_REG(hw, IXGBE_VFTDT(tx_ring->reg_idx)),\r\ntx_ring->next_to_use, i,\r\neop_desc, (eop_desc ? eop_desc->wb.status : 0),\r\ntx_ring->tx_buffer_info[i].time_stamp, jiffies);\r\nnetif_stop_subqueue(tx_ring->netdev, tx_ring->queue_index);\r\nixgbevf_tx_timeout_reset(adapter);\r\nreturn true;\r\n}\r\n#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)\r\nif (unlikely(total_packets && netif_carrier_ok(tx_ring->netdev) &&\r\n(ixgbevf_desc_unused(tx_ring) >= TX_WAKE_THRESHOLD))) {\r\nsmp_mb();\r\nif (__netif_subqueue_stopped(tx_ring->netdev,\r\ntx_ring->queue_index) &&\r\n!test_bit(__IXGBEVF_DOWN, &adapter->state)) {\r\nnetif_wake_subqueue(tx_ring->netdev,\r\ntx_ring->queue_index);\r\n++tx_ring->tx_stats.restart_queue;\r\n}\r\n}\r\nreturn !!budget;\r\n}\r\nstatic void ixgbevf_rx_skb(struct ixgbevf_q_vector *q_vector,\r\nstruct sk_buff *skb)\r\n{\r\nnapi_gro_receive(&q_vector->napi, skb);\r\n}\r\nstatic inline void ixgbevf_rx_hash(struct ixgbevf_ring *ring,\r\nunion ixgbe_adv_rx_desc *rx_desc,\r\nstruct sk_buff *skb)\r\n{\r\nu16 rss_type;\r\nif (!(ring->netdev->features & NETIF_F_RXHASH))\r\nreturn;\r\nrss_type = le16_to_cpu(rx_desc->wb.lower.lo_dword.hs_rss.pkt_info) &\r\nIXGBE_RXDADV_RSSTYPE_MASK;\r\nif (!rss_type)\r\nreturn;\r\nskb_set_hash(skb, le32_to_cpu(rx_desc->wb.lower.hi_dword.rss),\r\n(IXGBE_RSS_L4_TYPES_MASK & (1ul << rss_type)) ?\r\nPKT_HASH_TYPE_L4 : PKT_HASH_TYPE_L3);\r\n}\r\nstatic inline void ixgbevf_rx_checksum(struct ixgbevf_ring *ring,\r\nunion ixgbe_adv_rx_desc *rx_desc,\r\nstruct sk_buff *skb)\r\n{\r\nskb_checksum_none_assert(skb);\r\nif (!(ring->netdev->features & NETIF_F_RXCSUM))\r\nreturn;\r\nif (ixgbevf_test_staterr(rx_desc, IXGBE_RXD_STAT_IPCS) &&\r\nixgbevf_test_staterr(rx_desc, IXGBE_RXDADV_ERR_IPE)) {\r\nring->rx_stats.csum_err++;\r\nreturn;\r\n}\r\nif (!ixgbevf_test_staterr(rx_desc, IXGBE_RXD_STAT_L4CS))\r\nreturn;\r\nif (ixgbevf_test_staterr(rx_desc, IXGBE_RXDADV_ERR_TCPE)) {\r\nring->rx_stats.csum_err++;\r\nreturn;\r\n}\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\n}\r\nstatic void ixgbevf_process_skb_fields(struct ixgbevf_ring *rx_ring,\r\nunion ixgbe_adv_rx_desc *rx_desc,\r\nstruct sk_buff *skb)\r\n{\r\nixgbevf_rx_hash(rx_ring, rx_desc, skb);\r\nixgbevf_rx_checksum(rx_ring, rx_desc, skb);\r\nif (ixgbevf_test_staterr(rx_desc, IXGBE_RXD_STAT_VP)) {\r\nu16 vid = le16_to_cpu(rx_desc->wb.upper.vlan);\r\nunsigned long *active_vlans = netdev_priv(rx_ring->netdev);\r\nif (test_bit(vid & VLAN_VID_MASK, active_vlans))\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vid);\r\n}\r\nskb->protocol = eth_type_trans(skb, rx_ring->netdev);\r\n}\r\nstatic bool ixgbevf_is_non_eop(struct ixgbevf_ring *rx_ring,\r\nunion ixgbe_adv_rx_desc *rx_desc)\r\n{\r\nu32 ntc = rx_ring->next_to_clean + 1;\r\nntc = (ntc < rx_ring->count) ? ntc : 0;\r\nrx_ring->next_to_clean = ntc;\r\nprefetch(IXGBEVF_RX_DESC(rx_ring, ntc));\r\nif (likely(ixgbevf_test_staterr(rx_desc, IXGBE_RXD_STAT_EOP)))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic bool ixgbevf_alloc_mapped_page(struct ixgbevf_ring *rx_ring,\r\nstruct ixgbevf_rx_buffer *bi)\r\n{\r\nstruct page *page = bi->page;\r\ndma_addr_t dma = bi->dma;\r\nif (likely(page))\r\nreturn true;\r\npage = dev_alloc_page();\r\nif (unlikely(!page)) {\r\nrx_ring->rx_stats.alloc_rx_page_failed++;\r\nreturn false;\r\n}\r\ndma = dma_map_page(rx_ring->dev, page, 0,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(rx_ring->dev, dma)) {\r\n__free_page(page);\r\nrx_ring->rx_stats.alloc_rx_buff_failed++;\r\nreturn false;\r\n}\r\nbi->dma = dma;\r\nbi->page = page;\r\nbi->page_offset = 0;\r\nreturn true;\r\n}\r\nstatic void ixgbevf_alloc_rx_buffers(struct ixgbevf_ring *rx_ring,\r\nu16 cleaned_count)\r\n{\r\nunion ixgbe_adv_rx_desc *rx_desc;\r\nstruct ixgbevf_rx_buffer *bi;\r\nunsigned int i = rx_ring->next_to_use;\r\nif (!cleaned_count || !rx_ring->netdev)\r\nreturn;\r\nrx_desc = IXGBEVF_RX_DESC(rx_ring, i);\r\nbi = &rx_ring->rx_buffer_info[i];\r\ni -= rx_ring->count;\r\ndo {\r\nif (!ixgbevf_alloc_mapped_page(rx_ring, bi))\r\nbreak;\r\nrx_desc->read.pkt_addr = cpu_to_le64(bi->dma + bi->page_offset);\r\nrx_desc++;\r\nbi++;\r\ni++;\r\nif (unlikely(!i)) {\r\nrx_desc = IXGBEVF_RX_DESC(rx_ring, 0);\r\nbi = rx_ring->rx_buffer_info;\r\ni -= rx_ring->count;\r\n}\r\nrx_desc->read.hdr_addr = 0;\r\ncleaned_count--;\r\n} while (cleaned_count);\r\ni += rx_ring->count;\r\nif (rx_ring->next_to_use != i) {\r\nrx_ring->next_to_use = i;\r\nrx_ring->next_to_alloc = i;\r\nwmb();\r\nixgbevf_write_tail(rx_ring, i);\r\n}\r\n}\r\nstatic bool ixgbevf_cleanup_headers(struct ixgbevf_ring *rx_ring,\r\nunion ixgbe_adv_rx_desc *rx_desc,\r\nstruct sk_buff *skb)\r\n{\r\nif (unlikely(ixgbevf_test_staterr(rx_desc,\r\nIXGBE_RXDADV_ERR_FRAME_ERR_MASK))) {\r\nstruct net_device *netdev = rx_ring->netdev;\r\nif (!(netdev->features & NETIF_F_RXALL)) {\r\ndev_kfree_skb_any(skb);\r\nreturn true;\r\n}\r\n}\r\nif (eth_skb_pad(skb))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void ixgbevf_reuse_rx_page(struct ixgbevf_ring *rx_ring,\r\nstruct ixgbevf_rx_buffer *old_buff)\r\n{\r\nstruct ixgbevf_rx_buffer *new_buff;\r\nu16 nta = rx_ring->next_to_alloc;\r\nnew_buff = &rx_ring->rx_buffer_info[nta];\r\nnta++;\r\nrx_ring->next_to_alloc = (nta < rx_ring->count) ? nta : 0;\r\nnew_buff->page = old_buff->page;\r\nnew_buff->dma = old_buff->dma;\r\nnew_buff->page_offset = old_buff->page_offset;\r\ndma_sync_single_range_for_device(rx_ring->dev, new_buff->dma,\r\nnew_buff->page_offset,\r\nIXGBEVF_RX_BUFSZ,\r\nDMA_FROM_DEVICE);\r\n}\r\nstatic inline bool ixgbevf_page_is_reserved(struct page *page)\r\n{\r\nreturn (page_to_nid(page) != numa_mem_id()) || page_is_pfmemalloc(page);\r\n}\r\nstatic bool ixgbevf_add_rx_frag(struct ixgbevf_ring *rx_ring,\r\nstruct ixgbevf_rx_buffer *rx_buffer,\r\nunion ixgbe_adv_rx_desc *rx_desc,\r\nstruct sk_buff *skb)\r\n{\r\nstruct page *page = rx_buffer->page;\r\nunsigned char *va = page_address(page) + rx_buffer->page_offset;\r\nunsigned int size = le16_to_cpu(rx_desc->wb.upper.length);\r\n#if (PAGE_SIZE < 8192)\r\nunsigned int truesize = IXGBEVF_RX_BUFSZ;\r\n#else\r\nunsigned int truesize = ALIGN(size, L1_CACHE_BYTES);\r\n#endif\r\nunsigned int pull_len;\r\nif (unlikely(skb_is_nonlinear(skb)))\r\ngoto add_tail_frag;\r\nif (likely(size <= IXGBEVF_RX_HDR_SIZE)) {\r\nmemcpy(__skb_put(skb, size), va, ALIGN(size, sizeof(long)));\r\nif (likely(!ixgbevf_page_is_reserved(page)))\r\nreturn true;\r\nput_page(page);\r\nreturn false;\r\n}\r\npull_len = eth_get_headlen(va, IXGBEVF_RX_HDR_SIZE);\r\nmemcpy(__skb_put(skb, pull_len), va, ALIGN(pull_len, sizeof(long)));\r\nva += pull_len;\r\nsize -= pull_len;\r\nadd_tail_frag:\r\nskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,\r\n(unsigned long)va & ~PAGE_MASK, size, truesize);\r\nif (unlikely(ixgbevf_page_is_reserved(page)))\r\nreturn false;\r\n#if (PAGE_SIZE < 8192)\r\nif (unlikely(page_count(page) != 1))\r\nreturn false;\r\nrx_buffer->page_offset ^= IXGBEVF_RX_BUFSZ;\r\n#else\r\nrx_buffer->page_offset += truesize;\r\nif (rx_buffer->page_offset > (PAGE_SIZE - IXGBEVF_RX_BUFSZ))\r\nreturn false;\r\n#endif\r\npage_ref_inc(page);\r\nreturn true;\r\n}\r\nstatic struct sk_buff *ixgbevf_fetch_rx_buffer(struct ixgbevf_ring *rx_ring,\r\nunion ixgbe_adv_rx_desc *rx_desc,\r\nstruct sk_buff *skb)\r\n{\r\nstruct ixgbevf_rx_buffer *rx_buffer;\r\nstruct page *page;\r\nrx_buffer = &rx_ring->rx_buffer_info[rx_ring->next_to_clean];\r\npage = rx_buffer->page;\r\nprefetchw(page);\r\nif (likely(!skb)) {\r\nvoid *page_addr = page_address(page) +\r\nrx_buffer->page_offset;\r\nprefetch(page_addr);\r\n#if L1_CACHE_BYTES < 128\r\nprefetch(page_addr + L1_CACHE_BYTES);\r\n#endif\r\nskb = netdev_alloc_skb_ip_align(rx_ring->netdev,\r\nIXGBEVF_RX_HDR_SIZE);\r\nif (unlikely(!skb)) {\r\nrx_ring->rx_stats.alloc_rx_buff_failed++;\r\nreturn NULL;\r\n}\r\nprefetchw(skb->data);\r\n}\r\ndma_sync_single_range_for_cpu(rx_ring->dev,\r\nrx_buffer->dma,\r\nrx_buffer->page_offset,\r\nIXGBEVF_RX_BUFSZ,\r\nDMA_FROM_DEVICE);\r\nif (ixgbevf_add_rx_frag(rx_ring, rx_buffer, rx_desc, skb)) {\r\nixgbevf_reuse_rx_page(rx_ring, rx_buffer);\r\n} else {\r\ndma_unmap_page(rx_ring->dev, rx_buffer->dma,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\n}\r\nrx_buffer->dma = 0;\r\nrx_buffer->page = NULL;\r\nreturn skb;\r\n}\r\nstatic inline void ixgbevf_irq_enable_queues(struct ixgbevf_adapter *adapter,\r\nu32 qmask)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIMS, qmask);\r\n}\r\nstatic int ixgbevf_clean_rx_irq(struct ixgbevf_q_vector *q_vector,\r\nstruct ixgbevf_ring *rx_ring,\r\nint budget)\r\n{\r\nunsigned int total_rx_bytes = 0, total_rx_packets = 0;\r\nu16 cleaned_count = ixgbevf_desc_unused(rx_ring);\r\nstruct sk_buff *skb = rx_ring->skb;\r\nwhile (likely(total_rx_packets < budget)) {\r\nunion ixgbe_adv_rx_desc *rx_desc;\r\nif (cleaned_count >= IXGBEVF_RX_BUFFER_WRITE) {\r\nixgbevf_alloc_rx_buffers(rx_ring, cleaned_count);\r\ncleaned_count = 0;\r\n}\r\nrx_desc = IXGBEVF_RX_DESC(rx_ring, rx_ring->next_to_clean);\r\nif (!ixgbevf_test_staterr(rx_desc, IXGBE_RXD_STAT_DD))\r\nbreak;\r\nrmb();\r\nskb = ixgbevf_fetch_rx_buffer(rx_ring, rx_desc, skb);\r\nif (!skb)\r\nbreak;\r\ncleaned_count++;\r\nif (ixgbevf_is_non_eop(rx_ring, rx_desc))\r\ncontinue;\r\nif (ixgbevf_cleanup_headers(rx_ring, rx_desc, skb)) {\r\nskb = NULL;\r\ncontinue;\r\n}\r\ntotal_rx_bytes += skb->len;\r\nif ((skb->pkt_type == PACKET_BROADCAST ||\r\nskb->pkt_type == PACKET_MULTICAST) &&\r\nether_addr_equal(rx_ring->netdev->dev_addr,\r\neth_hdr(skb)->h_source)) {\r\ndev_kfree_skb_irq(skb);\r\ncontinue;\r\n}\r\nixgbevf_process_skb_fields(rx_ring, rx_desc, skb);\r\nixgbevf_rx_skb(q_vector, skb);\r\nskb = NULL;\r\ntotal_rx_packets++;\r\n}\r\nrx_ring->skb = skb;\r\nu64_stats_update_begin(&rx_ring->syncp);\r\nrx_ring->stats.packets += total_rx_packets;\r\nrx_ring->stats.bytes += total_rx_bytes;\r\nu64_stats_update_end(&rx_ring->syncp);\r\nq_vector->rx.total_packets += total_rx_packets;\r\nq_vector->rx.total_bytes += total_rx_bytes;\r\nreturn total_rx_packets;\r\n}\r\nstatic int ixgbevf_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct ixgbevf_q_vector *q_vector =\r\ncontainer_of(napi, struct ixgbevf_q_vector, napi);\r\nstruct ixgbevf_adapter *adapter = q_vector->adapter;\r\nstruct ixgbevf_ring *ring;\r\nint per_ring_budget, work_done = 0;\r\nbool clean_complete = true;\r\nixgbevf_for_each_ring(ring, q_vector->tx) {\r\nif (!ixgbevf_clean_tx_irq(q_vector, ring, budget))\r\nclean_complete = false;\r\n}\r\nif (budget <= 0)\r\nreturn budget;\r\nif (q_vector->rx.count > 1)\r\nper_ring_budget = max(budget/q_vector->rx.count, 1);\r\nelse\r\nper_ring_budget = budget;\r\nixgbevf_for_each_ring(ring, q_vector->rx) {\r\nint cleaned = ixgbevf_clean_rx_irq(q_vector, ring,\r\nper_ring_budget);\r\nwork_done += cleaned;\r\nif (cleaned >= per_ring_budget)\r\nclean_complete = false;\r\n}\r\nif (!clean_complete)\r\nreturn budget;\r\nnapi_complete_done(napi, work_done);\r\nif (adapter->rx_itr_setting == 1)\r\nixgbevf_set_itr(q_vector);\r\nif (!test_bit(__IXGBEVF_DOWN, &adapter->state) &&\r\n!test_bit(__IXGBEVF_REMOVING, &adapter->state))\r\nixgbevf_irq_enable_queues(adapter,\r\nBIT(q_vector->v_idx));\r\nreturn 0;\r\n}\r\nvoid ixgbevf_write_eitr(struct ixgbevf_q_vector *q_vector)\r\n{\r\nstruct ixgbevf_adapter *adapter = q_vector->adapter;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint v_idx = q_vector->v_idx;\r\nu32 itr_reg = q_vector->itr & IXGBE_MAX_EITR;\r\nitr_reg |= IXGBE_EITR_CNT_WDIS;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEITR(v_idx), itr_reg);\r\n}\r\nstatic void ixgbevf_configure_msix(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbevf_q_vector *q_vector;\r\nint q_vectors, v_idx;\r\nq_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nadapter->eims_enable_mask = 0;\r\nfor (v_idx = 0; v_idx < q_vectors; v_idx++) {\r\nstruct ixgbevf_ring *ring;\r\nq_vector = adapter->q_vector[v_idx];\r\nixgbevf_for_each_ring(ring, q_vector->rx)\r\nixgbevf_set_ivar(adapter, 0, ring->reg_idx, v_idx);\r\nixgbevf_for_each_ring(ring, q_vector->tx)\r\nixgbevf_set_ivar(adapter, 1, ring->reg_idx, v_idx);\r\nif (q_vector->tx.ring && !q_vector->rx.ring) {\r\nif (adapter->tx_itr_setting == 1)\r\nq_vector->itr = IXGBE_12K_ITR;\r\nelse\r\nq_vector->itr = adapter->tx_itr_setting;\r\n} else {\r\nif (adapter->rx_itr_setting == 1)\r\nq_vector->itr = IXGBE_20K_ITR;\r\nelse\r\nq_vector->itr = adapter->rx_itr_setting;\r\n}\r\nadapter->eims_enable_mask |= BIT(v_idx);\r\nixgbevf_write_eitr(q_vector);\r\n}\r\nixgbevf_set_ivar(adapter, -1, 1, v_idx);\r\nadapter->eims_other = BIT(v_idx);\r\nadapter->eims_enable_mask |= adapter->eims_other;\r\n}\r\nstatic void ixgbevf_update_itr(struct ixgbevf_q_vector *q_vector,\r\nstruct ixgbevf_ring_container *ring_container)\r\n{\r\nint bytes = ring_container->total_bytes;\r\nint packets = ring_container->total_packets;\r\nu32 timepassed_us;\r\nu64 bytes_perint;\r\nu8 itr_setting = ring_container->itr;\r\nif (packets == 0)\r\nreturn;\r\ntimepassed_us = q_vector->itr >> 2;\r\nbytes_perint = bytes / timepassed_us;\r\nswitch (itr_setting) {\r\ncase lowest_latency:\r\nif (bytes_perint > 10)\r\nitr_setting = low_latency;\r\nbreak;\r\ncase low_latency:\r\nif (bytes_perint > 20)\r\nitr_setting = bulk_latency;\r\nelse if (bytes_perint <= 10)\r\nitr_setting = lowest_latency;\r\nbreak;\r\ncase bulk_latency:\r\nif (bytes_perint <= 20)\r\nitr_setting = low_latency;\r\nbreak;\r\n}\r\nring_container->total_bytes = 0;\r\nring_container->total_packets = 0;\r\nring_container->itr = itr_setting;\r\n}\r\nstatic void ixgbevf_set_itr(struct ixgbevf_q_vector *q_vector)\r\n{\r\nu32 new_itr = q_vector->itr;\r\nu8 current_itr;\r\nixgbevf_update_itr(q_vector, &q_vector->tx);\r\nixgbevf_update_itr(q_vector, &q_vector->rx);\r\ncurrent_itr = max(q_vector->rx.itr, q_vector->tx.itr);\r\nswitch (current_itr) {\r\ncase lowest_latency:\r\nnew_itr = IXGBE_100K_ITR;\r\nbreak;\r\ncase low_latency:\r\nnew_itr = IXGBE_20K_ITR;\r\nbreak;\r\ncase bulk_latency:\r\nnew_itr = IXGBE_12K_ITR;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (new_itr != q_vector->itr) {\r\nnew_itr = (10 * new_itr * q_vector->itr) /\r\n((9 * new_itr) + q_vector->itr);\r\nq_vector->itr = new_itr;\r\nixgbevf_write_eitr(q_vector);\r\n}\r\n}\r\nstatic irqreturn_t ixgbevf_msix_other(int irq, void *data)\r\n{\r\nstruct ixgbevf_adapter *adapter = data;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nhw->mac.get_link_status = 1;\r\nixgbevf_service_event_schedule(adapter);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIMS, adapter->eims_other);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t ixgbevf_msix_clean_rings(int irq, void *data)\r\n{\r\nstruct ixgbevf_q_vector *q_vector = data;\r\nif (q_vector->rx.ring || q_vector->tx.ring)\r\nnapi_schedule_irqoff(&q_vector->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic inline void map_vector_to_rxq(struct ixgbevf_adapter *a, int v_idx,\r\nint r_idx)\r\n{\r\nstruct ixgbevf_q_vector *q_vector = a->q_vector[v_idx];\r\na->rx_ring[r_idx]->next = q_vector->rx.ring;\r\nq_vector->rx.ring = a->rx_ring[r_idx];\r\nq_vector->rx.count++;\r\n}\r\nstatic inline void map_vector_to_txq(struct ixgbevf_adapter *a, int v_idx,\r\nint t_idx)\r\n{\r\nstruct ixgbevf_q_vector *q_vector = a->q_vector[v_idx];\r\na->tx_ring[t_idx]->next = q_vector->tx.ring;\r\nq_vector->tx.ring = a->tx_ring[t_idx];\r\nq_vector->tx.count++;\r\n}\r\nstatic int ixgbevf_map_rings_to_vectors(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_vectors;\r\nint v_start = 0;\r\nint rxr_idx = 0, txr_idx = 0;\r\nint rxr_remaining = adapter->num_rx_queues;\r\nint txr_remaining = adapter->num_tx_queues;\r\nint i, j;\r\nint rqpv, tqpv;\r\nq_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nif (q_vectors == adapter->num_rx_queues + adapter->num_tx_queues) {\r\nfor (; rxr_idx < rxr_remaining; v_start++, rxr_idx++)\r\nmap_vector_to_rxq(adapter, v_start, rxr_idx);\r\nfor (; txr_idx < txr_remaining; v_start++, txr_idx++)\r\nmap_vector_to_txq(adapter, v_start, txr_idx);\r\nreturn 0;\r\n}\r\nfor (i = v_start; i < q_vectors; i++) {\r\nrqpv = DIV_ROUND_UP(rxr_remaining, q_vectors - i);\r\nfor (j = 0; j < rqpv; j++) {\r\nmap_vector_to_rxq(adapter, i, rxr_idx);\r\nrxr_idx++;\r\nrxr_remaining--;\r\n}\r\n}\r\nfor (i = v_start; i < q_vectors; i++) {\r\ntqpv = DIV_ROUND_UP(txr_remaining, q_vectors - i);\r\nfor (j = 0; j < tqpv; j++) {\r\nmap_vector_to_txq(adapter, i, txr_idx);\r\ntxr_idx++;\r\ntxr_remaining--;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int ixgbevf_request_msix_irqs(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nunsigned int ri = 0, ti = 0;\r\nint vector, err;\r\nfor (vector = 0; vector < q_vectors; vector++) {\r\nstruct ixgbevf_q_vector *q_vector = adapter->q_vector[vector];\r\nstruct msix_entry *entry = &adapter->msix_entries[vector];\r\nif (q_vector->tx.ring && q_vector->rx.ring) {\r\nsnprintf(q_vector->name, sizeof(q_vector->name),\r\n"%s-TxRx-%u", netdev->name, ri++);\r\nti++;\r\n} else if (q_vector->rx.ring) {\r\nsnprintf(q_vector->name, sizeof(q_vector->name),\r\n"%s-rx-%u", netdev->name, ri++);\r\n} else if (q_vector->tx.ring) {\r\nsnprintf(q_vector->name, sizeof(q_vector->name),\r\n"%s-tx-%u", netdev->name, ti++);\r\n} else {\r\ncontinue;\r\n}\r\nerr = request_irq(entry->vector, &ixgbevf_msix_clean_rings, 0,\r\nq_vector->name, q_vector);\r\nif (err) {\r\nhw_dbg(&adapter->hw,\r\n"request_irq failed for MSIX interrupt Error: %d\n",\r\nerr);\r\ngoto free_queue_irqs;\r\n}\r\n}\r\nerr = request_irq(adapter->msix_entries[vector].vector,\r\n&ixgbevf_msix_other, 0, netdev->name, adapter);\r\nif (err) {\r\nhw_dbg(&adapter->hw, "request_irq for msix_other failed: %d\n",\r\nerr);\r\ngoto free_queue_irqs;\r\n}\r\nreturn 0;\r\nfree_queue_irqs:\r\nwhile (vector) {\r\nvector--;\r\nfree_irq(adapter->msix_entries[vector].vector,\r\nadapter->q_vector[vector]);\r\n}\r\nadapter->num_msix_vectors = 0;\r\nreturn err;\r\n}\r\nstatic inline void ixgbevf_reset_q_vectors(struct ixgbevf_adapter *adapter)\r\n{\r\nint i, q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (i = 0; i < q_vectors; i++) {\r\nstruct ixgbevf_q_vector *q_vector = adapter->q_vector[i];\r\nq_vector->rx.ring = NULL;\r\nq_vector->tx.ring = NULL;\r\nq_vector->rx.count = 0;\r\nq_vector->tx.count = 0;\r\n}\r\n}\r\nstatic int ixgbevf_request_irq(struct ixgbevf_adapter *adapter)\r\n{\r\nint err = ixgbevf_request_msix_irqs(adapter);\r\nif (err)\r\nhw_dbg(&adapter->hw, "request_irq failed, Error %d\n", err);\r\nreturn err;\r\n}\r\nstatic void ixgbevf_free_irq(struct ixgbevf_adapter *adapter)\r\n{\r\nint i, q_vectors;\r\nif (!adapter->msix_entries)\r\nreturn;\r\nq_vectors = adapter->num_msix_vectors;\r\ni = q_vectors - 1;\r\nfree_irq(adapter->msix_entries[i].vector, adapter);\r\ni--;\r\nfor (; i >= 0; i--) {\r\nif (!adapter->q_vector[i]->rx.ring &&\r\n!adapter->q_vector[i]->tx.ring)\r\ncontinue;\r\nfree_irq(adapter->msix_entries[i].vector,\r\nadapter->q_vector[i]);\r\n}\r\nixgbevf_reset_q_vectors(adapter);\r\n}\r\nstatic inline void ixgbevf_irq_disable(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint i;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIAM, 0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIMC, ~0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIAC, 0);\r\nIXGBE_WRITE_FLUSH(hw);\r\nfor (i = 0; i < adapter->num_msix_vectors; i++)\r\nsynchronize_irq(adapter->msix_entries[i].vector);\r\n}\r\nstatic inline void ixgbevf_irq_enable(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIAM, adapter->eims_enable_mask);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIAC, adapter->eims_enable_mask);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIMS, adapter->eims_enable_mask);\r\n}\r\nstatic void ixgbevf_configure_tx_ring(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *ring)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu64 tdba = ring->dma;\r\nint wait_loop = 10;\r\nu32 txdctl = IXGBE_TXDCTL_ENABLE;\r\nu8 reg_idx = ring->reg_idx;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(reg_idx), IXGBE_TXDCTL_SWFLSH);\r\nIXGBE_WRITE_FLUSH(hw);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDBAL(reg_idx), tdba & DMA_BIT_MASK(32));\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDBAH(reg_idx), tdba >> 32);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDLEN(reg_idx),\r\nring->count * sizeof(union ixgbe_adv_tx_desc));\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDWBAH(reg_idx), 0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDWBAL(reg_idx), 0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFDCA_TXCTRL(reg_idx),\r\n(IXGBE_DCA_TXCTRL_DESC_RRO_EN |\r\nIXGBE_DCA_TXCTRL_DATA_RRO_EN));\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDH(reg_idx), 0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDT(reg_idx), 0);\r\nring->tail = adapter->io_addr + IXGBE_VFTDT(reg_idx);\r\nring->next_to_clean = 0;\r\nring->next_to_use = 0;\r\ntxdctl |= (8 << 16);\r\ntxdctl |= (1u << 8) |\r\n32;\r\nclear_bit(__IXGBEVF_HANG_CHECK_ARMED, &ring->state);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(reg_idx), txdctl);\r\ndo {\r\nusleep_range(1000, 2000);\r\ntxdctl = IXGBE_READ_REG(hw, IXGBE_VFTXDCTL(reg_idx));\r\n} while (--wait_loop && !(txdctl & IXGBE_TXDCTL_ENABLE));\r\nif (!wait_loop)\r\nhw_dbg(hw, "Could not enable Tx Queue %d\n", reg_idx);\r\n}\r\nstatic void ixgbevf_configure_tx(struct ixgbevf_adapter *adapter)\r\n{\r\nu32 i;\r\nfor (i = 0; i < adapter->num_tx_queues; i++)\r\nixgbevf_configure_tx_ring(adapter, adapter->tx_ring[i]);\r\n}\r\nstatic void ixgbevf_configure_srrctl(struct ixgbevf_adapter *adapter, int index)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 srrctl;\r\nsrrctl = IXGBE_SRRCTL_DROP_EN;\r\nsrrctl |= IXGBEVF_RX_HDR_SIZE << IXGBE_SRRCTL_BSIZEHDRSIZE_SHIFT;\r\nsrrctl |= IXGBEVF_RX_BUFSZ >> IXGBE_SRRCTL_BSIZEPKT_SHIFT;\r\nsrrctl |= IXGBE_SRRCTL_DESCTYPE_ADV_ONEBUF;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFSRRCTL(index), srrctl);\r\n}\r\nstatic void ixgbevf_setup_psrtype(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 psrtype = IXGBE_PSRTYPE_TCPHDR | IXGBE_PSRTYPE_UDPHDR |\r\nIXGBE_PSRTYPE_IPV4HDR | IXGBE_PSRTYPE_IPV6HDR |\r\nIXGBE_PSRTYPE_L2HDR;\r\nif (adapter->num_rx_queues > 1)\r\npsrtype |= BIT(29);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFPSRTYPE, psrtype);\r\n}\r\nstatic void ixgbevf_disable_rx_queue(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *ring)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint wait_loop = IXGBEVF_MAX_RX_DESC_POLL;\r\nu32 rxdctl;\r\nu8 reg_idx = ring->reg_idx;\r\nif (IXGBE_REMOVED(hw->hw_addr))\r\nreturn;\r\nrxdctl = IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(reg_idx));\r\nrxdctl &= ~IXGBE_RXDCTL_ENABLE;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRXDCTL(reg_idx), rxdctl);\r\ndo {\r\nudelay(10);\r\nrxdctl = IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(reg_idx));\r\n} while (--wait_loop && (rxdctl & IXGBE_RXDCTL_ENABLE));\r\nif (!wait_loop)\r\npr_err("RXDCTL.ENABLE queue %d not cleared while polling\n",\r\nreg_idx);\r\n}\r\nstatic void ixgbevf_rx_desc_queue_enable(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *ring)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint wait_loop = IXGBEVF_MAX_RX_DESC_POLL;\r\nu32 rxdctl;\r\nu8 reg_idx = ring->reg_idx;\r\nif (IXGBE_REMOVED(hw->hw_addr))\r\nreturn;\r\ndo {\r\nusleep_range(1000, 2000);\r\nrxdctl = IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(reg_idx));\r\n} while (--wait_loop && !(rxdctl & IXGBE_RXDCTL_ENABLE));\r\nif (!wait_loop)\r\npr_err("RXDCTL.ENABLE queue %d not set while polling\n",\r\nreg_idx);\r\n}\r\nstatic inline int ixgbevf_init_rss_key(struct ixgbevf_adapter *adapter)\r\n{\r\nu32 *rss_key;\r\nif (!adapter->rss_key) {\r\nrss_key = kzalloc(IXGBEVF_RSS_HASH_KEY_SIZE, GFP_KERNEL);\r\nif (unlikely(!rss_key))\r\nreturn -ENOMEM;\r\nnetdev_rss_key_fill(rss_key, IXGBEVF_RSS_HASH_KEY_SIZE);\r\nadapter->rss_key = rss_key;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ixgbevf_setup_vfmrqc(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 vfmrqc = 0, vfreta = 0;\r\nu16 rss_i = adapter->num_rx_queues;\r\nu8 i, j;\r\nfor (i = 0; i < IXGBEVF_VFRSSRK_REGS; i++)\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRSSRK(i), *(adapter->rss_key + i));\r\nfor (i = 0, j = 0; i < IXGBEVF_X550_VFRETA_SIZE; i++, j++) {\r\nif (j == rss_i)\r\nj = 0;\r\nadapter->rss_indir_tbl[i] = j;\r\nvfreta |= j << (i & 0x3) * 8;\r\nif ((i & 3) == 3) {\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRETA(i >> 2), vfreta);\r\nvfreta = 0;\r\n}\r\n}\r\nvfmrqc |= IXGBE_VFMRQC_RSS_FIELD_IPV4 |\r\nIXGBE_VFMRQC_RSS_FIELD_IPV4_TCP |\r\nIXGBE_VFMRQC_RSS_FIELD_IPV6 |\r\nIXGBE_VFMRQC_RSS_FIELD_IPV6_TCP;\r\nvfmrqc |= IXGBE_VFMRQC_RSSEN;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFMRQC, vfmrqc);\r\n}\r\nstatic void ixgbevf_configure_rx_ring(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *ring)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu64 rdba = ring->dma;\r\nu32 rxdctl;\r\nu8 reg_idx = ring->reg_idx;\r\nrxdctl = IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(reg_idx));\r\nixgbevf_disable_rx_queue(adapter, ring);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDBAL(reg_idx), rdba & DMA_BIT_MASK(32));\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDBAH(reg_idx), rdba >> 32);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDLEN(reg_idx),\r\nring->count * sizeof(union ixgbe_adv_rx_desc));\r\n#ifndef CONFIG_SPARC\r\nIXGBE_WRITE_REG(hw, IXGBE_VFDCA_RXCTRL(reg_idx),\r\nIXGBE_DCA_RXCTRL_DESC_RRO_EN);\r\n#else\r\nIXGBE_WRITE_REG(hw, IXGBE_VFDCA_RXCTRL(reg_idx),\r\nIXGBE_DCA_RXCTRL_DESC_RRO_EN |\r\nIXGBE_DCA_RXCTRL_DATA_WRO_EN);\r\n#endif\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDH(reg_idx), 0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDT(reg_idx), 0);\r\nring->tail = adapter->io_addr + IXGBE_VFRDT(reg_idx);\r\nring->next_to_clean = 0;\r\nring->next_to_use = 0;\r\nring->next_to_alloc = 0;\r\nixgbevf_configure_srrctl(adapter, reg_idx);\r\nrxdctl &= ~IXGBE_RXDCTL_RLPML_EN;\r\nrxdctl |= IXGBE_RXDCTL_ENABLE | IXGBE_RXDCTL_VME;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRXDCTL(reg_idx), rxdctl);\r\nixgbevf_rx_desc_queue_enable(adapter, ring);\r\nixgbevf_alloc_rx_buffers(ring, ixgbevf_desc_unused(ring));\r\n}\r\nstatic void ixgbevf_configure_rx(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct net_device *netdev = adapter->netdev;\r\nint i, ret;\r\nixgbevf_setup_psrtype(adapter);\r\nif (hw->mac.type >= ixgbe_mac_X550_vf)\r\nixgbevf_setup_vfmrqc(adapter);\r\nspin_lock_bh(&adapter->mbx_lock);\r\nret = hw->mac.ops.set_rlpml(hw, netdev->mtu + ETH_HLEN + ETH_FCS_LEN);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (ret)\r\ndev_err(&adapter->pdev->dev,\r\n"Failed to set MTU at %d\n", netdev->mtu);\r\nfor (i = 0; i < adapter->num_rx_queues; i++)\r\nixgbevf_configure_rx_ring(adapter, adapter->rx_ring[i]);\r\n}\r\nstatic int ixgbevf_vlan_rx_add_vid(struct net_device *netdev,\r\n__be16 proto, u16 vid)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint err;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = hw->mac.ops.set_vfta(hw, vid, 0, true);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (err == IXGBE_ERR_MBX)\r\nreturn -EIO;\r\nif (err == IXGBE_ERR_INVALID_ARGUMENT)\r\nreturn -EACCES;\r\nset_bit(vid, adapter->active_vlans);\r\nreturn err;\r\n}\r\nstatic int ixgbevf_vlan_rx_kill_vid(struct net_device *netdev,\r\n__be16 proto, u16 vid)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint err;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = hw->mac.ops.set_vfta(hw, vid, 0, false);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nclear_bit(vid, adapter->active_vlans);\r\nreturn err;\r\n}\r\nstatic void ixgbevf_restore_vlan(struct ixgbevf_adapter *adapter)\r\n{\r\nu16 vid;\r\nfor_each_set_bit(vid, adapter->active_vlans, VLAN_N_VID)\r\nixgbevf_vlan_rx_add_vid(adapter->netdev,\r\nhtons(ETH_P_8021Q), vid);\r\n}\r\nstatic int ixgbevf_write_uc_addr_list(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint count = 0;\r\nif ((netdev_uc_count(netdev)) > 10) {\r\npr_err("Too many unicast filters - No Space\n");\r\nreturn -ENOSPC;\r\n}\r\nif (!netdev_uc_empty(netdev)) {\r\nstruct netdev_hw_addr *ha;\r\nnetdev_for_each_uc_addr(ha, netdev) {\r\nhw->mac.ops.set_uc_addr(hw, ++count, ha->addr);\r\nudelay(200);\r\n}\r\n} else {\r\nhw->mac.ops.set_uc_addr(hw, 0, NULL);\r\n}\r\nreturn count;\r\n}\r\nstatic void ixgbevf_set_rx_mode(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nunsigned int flags = netdev->flags;\r\nint xcast_mode;\r\nxcast_mode = (flags & IFF_ALLMULTI) ? IXGBEVF_XCAST_MODE_ALLMULTI :\r\n(flags & (IFF_BROADCAST | IFF_MULTICAST)) ?\r\nIXGBEVF_XCAST_MODE_MULTI : IXGBEVF_XCAST_MODE_NONE;\r\nif (flags & IFF_PROMISC)\r\nxcast_mode = IXGBEVF_XCAST_MODE_PROMISC;\r\nelse if (flags & IFF_ALLMULTI)\r\nxcast_mode = IXGBEVF_XCAST_MODE_ALLMULTI;\r\nelse if (flags & (IFF_BROADCAST | IFF_MULTICAST))\r\nxcast_mode = IXGBEVF_XCAST_MODE_MULTI;\r\nelse\r\nxcast_mode = IXGBEVF_XCAST_MODE_NONE;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nhw->mac.ops.update_xcast_mode(hw, xcast_mode);\r\nhw->mac.ops.update_mc_addr_list(hw, netdev);\r\nixgbevf_write_uc_addr_list(netdev);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\n}\r\nstatic void ixgbevf_napi_enable_all(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_idx;\r\nstruct ixgbevf_q_vector *q_vector;\r\nint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (q_idx = 0; q_idx < q_vectors; q_idx++) {\r\nq_vector = adapter->q_vector[q_idx];\r\nnapi_enable(&q_vector->napi);\r\n}\r\n}\r\nstatic void ixgbevf_napi_disable_all(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_idx;\r\nstruct ixgbevf_q_vector *q_vector;\r\nint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (q_idx = 0; q_idx < q_vectors; q_idx++) {\r\nq_vector = adapter->q_vector[q_idx];\r\nnapi_disable(&q_vector->napi);\r\n}\r\n}\r\nstatic int ixgbevf_configure_dcb(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nunsigned int def_q = 0;\r\nunsigned int num_tcs = 0;\r\nunsigned int num_rx_queues = adapter->num_rx_queues;\r\nunsigned int num_tx_queues = adapter->num_tx_queues;\r\nint err;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = ixgbevf_get_queues(hw, &num_tcs, &def_q);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (err)\r\nreturn err;\r\nif (num_tcs > 1) {\r\nnum_tx_queues = 1;\r\nadapter->tx_ring[0]->reg_idx = def_q;\r\nnum_rx_queues = num_tcs;\r\n}\r\nif ((adapter->num_rx_queues != num_rx_queues) ||\r\n(adapter->num_tx_queues != num_tx_queues)) {\r\nhw->mbx.timeout = 0;\r\nset_bit(__IXGBEVF_QUEUE_RESET_REQUESTED, &adapter->state);\r\n}\r\nreturn 0;\r\n}\r\nstatic void ixgbevf_configure(struct ixgbevf_adapter *adapter)\r\n{\r\nixgbevf_configure_dcb(adapter);\r\nixgbevf_set_rx_mode(adapter->netdev);\r\nixgbevf_restore_vlan(adapter);\r\nixgbevf_configure_tx(adapter);\r\nixgbevf_configure_rx(adapter);\r\n}\r\nstatic void ixgbevf_save_reset_stats(struct ixgbevf_adapter *adapter)\r\n{\r\nif (adapter->stats.vfgprc || adapter->stats.vfgptc) {\r\nadapter->stats.saved_reset_vfgprc += adapter->stats.vfgprc -\r\nadapter->stats.base_vfgprc;\r\nadapter->stats.saved_reset_vfgptc += adapter->stats.vfgptc -\r\nadapter->stats.base_vfgptc;\r\nadapter->stats.saved_reset_vfgorc += adapter->stats.vfgorc -\r\nadapter->stats.base_vfgorc;\r\nadapter->stats.saved_reset_vfgotc += adapter->stats.vfgotc -\r\nadapter->stats.base_vfgotc;\r\nadapter->stats.saved_reset_vfmprc += adapter->stats.vfmprc -\r\nadapter->stats.base_vfmprc;\r\n}\r\n}\r\nstatic void ixgbevf_init_last_counter_stats(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nadapter->stats.last_vfgprc = IXGBE_READ_REG(hw, IXGBE_VFGPRC);\r\nadapter->stats.last_vfgorc = IXGBE_READ_REG(hw, IXGBE_VFGORC_LSB);\r\nadapter->stats.last_vfgorc |=\r\n(((u64)(IXGBE_READ_REG(hw, IXGBE_VFGORC_MSB))) << 32);\r\nadapter->stats.last_vfgptc = IXGBE_READ_REG(hw, IXGBE_VFGPTC);\r\nadapter->stats.last_vfgotc = IXGBE_READ_REG(hw, IXGBE_VFGOTC_LSB);\r\nadapter->stats.last_vfgotc |=\r\n(((u64)(IXGBE_READ_REG(hw, IXGBE_VFGOTC_MSB))) << 32);\r\nadapter->stats.last_vfmprc = IXGBE_READ_REG(hw, IXGBE_VFMPRC);\r\nadapter->stats.base_vfgprc = adapter->stats.last_vfgprc;\r\nadapter->stats.base_vfgorc = adapter->stats.last_vfgorc;\r\nadapter->stats.base_vfgptc = adapter->stats.last_vfgptc;\r\nadapter->stats.base_vfgotc = adapter->stats.last_vfgotc;\r\nadapter->stats.base_vfmprc = adapter->stats.last_vfmprc;\r\n}\r\nstatic void ixgbevf_negotiate_api(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint api[] = { ixgbe_mbox_api_13,\r\nixgbe_mbox_api_12,\r\nixgbe_mbox_api_11,\r\nixgbe_mbox_api_10,\r\nixgbe_mbox_api_unknown };\r\nint err, idx = 0;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nwhile (api[idx] != ixgbe_mbox_api_unknown) {\r\nerr = hw->mac.ops.negotiate_api_version(hw, api[idx]);\r\nif (!err)\r\nbreak;\r\nidx++;\r\n}\r\nspin_unlock_bh(&adapter->mbx_lock);\r\n}\r\nstatic void ixgbevf_up_complete(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nixgbevf_configure_msix(adapter);\r\nspin_lock_bh(&adapter->mbx_lock);\r\nif (is_valid_ether_addr(hw->mac.addr))\r\nhw->mac.ops.set_rar(hw, 0, hw->mac.addr, 0);\r\nelse\r\nhw->mac.ops.set_rar(hw, 0, hw->mac.perm_addr, 0);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nsmp_mb__before_atomic();\r\nclear_bit(__IXGBEVF_DOWN, &adapter->state);\r\nixgbevf_napi_enable_all(adapter);\r\nIXGBE_READ_REG(hw, IXGBE_VTEICR);\r\nixgbevf_irq_enable(adapter);\r\nnetif_tx_start_all_queues(netdev);\r\nixgbevf_save_reset_stats(adapter);\r\nixgbevf_init_last_counter_stats(adapter);\r\nhw->mac.get_link_status = 1;\r\nmod_timer(&adapter->service_timer, jiffies);\r\n}\r\nvoid ixgbevf_up(struct ixgbevf_adapter *adapter)\r\n{\r\nixgbevf_configure(adapter);\r\nixgbevf_up_complete(adapter);\r\n}\r\nstatic void ixgbevf_clean_rx_ring(struct ixgbevf_ring *rx_ring)\r\n{\r\nstruct device *dev = rx_ring->dev;\r\nunsigned long size;\r\nunsigned int i;\r\nif (rx_ring->skb) {\r\ndev_kfree_skb(rx_ring->skb);\r\nrx_ring->skb = NULL;\r\n}\r\nif (!rx_ring->rx_buffer_info)\r\nreturn;\r\nfor (i = 0; i < rx_ring->count; i++) {\r\nstruct ixgbevf_rx_buffer *rx_buffer;\r\nrx_buffer = &rx_ring->rx_buffer_info[i];\r\nif (rx_buffer->dma)\r\ndma_unmap_page(dev, rx_buffer->dma,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\nrx_buffer->dma = 0;\r\nif (rx_buffer->page)\r\n__free_page(rx_buffer->page);\r\nrx_buffer->page = NULL;\r\n}\r\nsize = sizeof(struct ixgbevf_rx_buffer) * rx_ring->count;\r\nmemset(rx_ring->rx_buffer_info, 0, size);\r\nmemset(rx_ring->desc, 0, rx_ring->size);\r\n}\r\nstatic void ixgbevf_clean_tx_ring(struct ixgbevf_ring *tx_ring)\r\n{\r\nstruct ixgbevf_tx_buffer *tx_buffer_info;\r\nunsigned long size;\r\nunsigned int i;\r\nif (!tx_ring->tx_buffer_info)\r\nreturn;\r\nfor (i = 0; i < tx_ring->count; i++) {\r\ntx_buffer_info = &tx_ring->tx_buffer_info[i];\r\nixgbevf_unmap_and_free_tx_resource(tx_ring, tx_buffer_info);\r\n}\r\nsize = sizeof(struct ixgbevf_tx_buffer) * tx_ring->count;\r\nmemset(tx_ring->tx_buffer_info, 0, size);\r\nmemset(tx_ring->desc, 0, tx_ring->size);\r\n}\r\nstatic void ixgbevf_clean_all_rx_rings(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_rx_queues; i++)\r\nixgbevf_clean_rx_ring(adapter->rx_ring[i]);\r\n}\r\nstatic void ixgbevf_clean_all_tx_rings(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_tx_queues; i++)\r\nixgbevf_clean_tx_ring(adapter->tx_ring[i]);\r\n}\r\nvoid ixgbevf_down(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint i;\r\nif (test_and_set_bit(__IXGBEVF_DOWN, &adapter->state))\r\nreturn;\r\nfor (i = 0; i < adapter->num_rx_queues; i++)\r\nixgbevf_disable_rx_queue(adapter, adapter->rx_ring[i]);\r\nusleep_range(10000, 20000);\r\nnetif_tx_stop_all_queues(netdev);\r\nnetif_carrier_off(netdev);\r\nnetif_tx_disable(netdev);\r\nixgbevf_irq_disable(adapter);\r\nixgbevf_napi_disable_all(adapter);\r\ndel_timer_sync(&adapter->service_timer);\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nu8 reg_idx = adapter->tx_ring[i]->reg_idx;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(reg_idx),\r\nIXGBE_TXDCTL_SWFLSH);\r\n}\r\nif (!pci_channel_offline(adapter->pdev))\r\nixgbevf_reset(adapter);\r\nixgbevf_clean_all_tx_rings(adapter);\r\nixgbevf_clean_all_rx_rings(adapter);\r\n}\r\nvoid ixgbevf_reinit_locked(struct ixgbevf_adapter *adapter)\r\n{\r\nWARN_ON(in_interrupt());\r\nwhile (test_and_set_bit(__IXGBEVF_RESETTING, &adapter->state))\r\nmsleep(1);\r\nixgbevf_down(adapter);\r\nixgbevf_up(adapter);\r\nclear_bit(__IXGBEVF_RESETTING, &adapter->state);\r\n}\r\nvoid ixgbevf_reset(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct net_device *netdev = adapter->netdev;\r\nif (hw->mac.ops.reset_hw(hw)) {\r\nhw_dbg(hw, "PF still resetting\n");\r\n} else {\r\nhw->mac.ops.init_hw(hw);\r\nixgbevf_negotiate_api(adapter);\r\n}\r\nif (is_valid_ether_addr(adapter->hw.mac.addr)) {\r\nether_addr_copy(netdev->dev_addr, adapter->hw.mac.addr);\r\nether_addr_copy(netdev->perm_addr, adapter->hw.mac.addr);\r\n}\r\nadapter->last_reset = jiffies;\r\n}\r\nstatic int ixgbevf_acquire_msix_vectors(struct ixgbevf_adapter *adapter,\r\nint vectors)\r\n{\r\nint vector_threshold;\r\nvector_threshold = MIN_MSIX_COUNT;\r\nvectors = pci_enable_msix_range(adapter->pdev, adapter->msix_entries,\r\nvector_threshold, vectors);\r\nif (vectors < 0) {\r\ndev_err(&adapter->pdev->dev,\r\n"Unable to allocate MSI-X interrupts\n");\r\nkfree(adapter->msix_entries);\r\nadapter->msix_entries = NULL;\r\nreturn vectors;\r\n}\r\nadapter->num_msix_vectors = vectors;\r\nreturn 0;\r\n}\r\nstatic void ixgbevf_set_num_queues(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nunsigned int def_q = 0;\r\nunsigned int num_tcs = 0;\r\nint err;\r\nadapter->num_rx_queues = 1;\r\nadapter->num_tx_queues = 1;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = ixgbevf_get_queues(hw, &num_tcs, &def_q);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (err)\r\nreturn;\r\nif (num_tcs > 1) {\r\nadapter->num_rx_queues = num_tcs;\r\n} else {\r\nu16 rss = min_t(u16, num_online_cpus(), IXGBEVF_MAX_RSS_QUEUES);\r\nswitch (hw->api_version) {\r\ncase ixgbe_mbox_api_11:\r\ncase ixgbe_mbox_api_12:\r\ncase ixgbe_mbox_api_13:\r\nadapter->num_rx_queues = rss;\r\nadapter->num_tx_queues = rss;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic int ixgbevf_alloc_queues(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbevf_ring *ring;\r\nint rx = 0, tx = 0;\r\nfor (; tx < adapter->num_tx_queues; tx++) {\r\nring = kzalloc(sizeof(*ring), GFP_KERNEL);\r\nif (!ring)\r\ngoto err_allocation;\r\nring->dev = &adapter->pdev->dev;\r\nring->netdev = adapter->netdev;\r\nring->count = adapter->tx_ring_count;\r\nring->queue_index = tx;\r\nring->reg_idx = tx;\r\nadapter->tx_ring[tx] = ring;\r\n}\r\nfor (; rx < adapter->num_rx_queues; rx++) {\r\nring = kzalloc(sizeof(*ring), GFP_KERNEL);\r\nif (!ring)\r\ngoto err_allocation;\r\nring->dev = &adapter->pdev->dev;\r\nring->netdev = adapter->netdev;\r\nring->count = adapter->rx_ring_count;\r\nring->queue_index = rx;\r\nring->reg_idx = rx;\r\nadapter->rx_ring[rx] = ring;\r\n}\r\nreturn 0;\r\nerr_allocation:\r\nwhile (tx) {\r\nkfree(adapter->tx_ring[--tx]);\r\nadapter->tx_ring[tx] = NULL;\r\n}\r\nwhile (rx) {\r\nkfree(adapter->rx_ring[--rx]);\r\nadapter->rx_ring[rx] = NULL;\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic int ixgbevf_set_interrupt_capability(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nint err;\r\nint vector, v_budget;\r\nv_budget = max(adapter->num_rx_queues, adapter->num_tx_queues);\r\nv_budget = min_t(int, v_budget, num_online_cpus());\r\nv_budget += NON_Q_VECTORS;\r\nadapter->msix_entries = kcalloc(v_budget,\r\nsizeof(struct msix_entry), GFP_KERNEL);\r\nif (!adapter->msix_entries)\r\nreturn -ENOMEM;\r\nfor (vector = 0; vector < v_budget; vector++)\r\nadapter->msix_entries[vector].entry = vector;\r\nerr = ixgbevf_acquire_msix_vectors(adapter, v_budget);\r\nif (err)\r\nreturn err;\r\nerr = netif_set_real_num_tx_queues(netdev, adapter->num_tx_queues);\r\nif (err)\r\nreturn err;\r\nreturn netif_set_real_num_rx_queues(netdev, adapter->num_rx_queues);\r\n}\r\nstatic int ixgbevf_alloc_q_vectors(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_idx, num_q_vectors;\r\nstruct ixgbevf_q_vector *q_vector;\r\nnum_q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (q_idx = 0; q_idx < num_q_vectors; q_idx++) {\r\nq_vector = kzalloc(sizeof(struct ixgbevf_q_vector), GFP_KERNEL);\r\nif (!q_vector)\r\ngoto err_out;\r\nq_vector->adapter = adapter;\r\nq_vector->v_idx = q_idx;\r\nnetif_napi_add(adapter->netdev, &q_vector->napi,\r\nixgbevf_poll, 64);\r\nadapter->q_vector[q_idx] = q_vector;\r\n}\r\nreturn 0;\r\nerr_out:\r\nwhile (q_idx) {\r\nq_idx--;\r\nq_vector = adapter->q_vector[q_idx];\r\n#ifdef CONFIG_NET_RX_BUSY_POLL\r\nnapi_hash_del(&q_vector->napi);\r\n#endif\r\nnetif_napi_del(&q_vector->napi);\r\nkfree(q_vector);\r\nadapter->q_vector[q_idx] = NULL;\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void ixgbevf_free_q_vectors(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_idx, num_q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (q_idx = 0; q_idx < num_q_vectors; q_idx++) {\r\nstruct ixgbevf_q_vector *q_vector = adapter->q_vector[q_idx];\r\nadapter->q_vector[q_idx] = NULL;\r\n#ifdef CONFIG_NET_RX_BUSY_POLL\r\nnapi_hash_del(&q_vector->napi);\r\n#endif\r\nnetif_napi_del(&q_vector->napi);\r\nkfree(q_vector);\r\n}\r\n}\r\nstatic void ixgbevf_reset_interrupt_capability(struct ixgbevf_adapter *adapter)\r\n{\r\nif (!adapter->msix_entries)\r\nreturn;\r\npci_disable_msix(adapter->pdev);\r\nkfree(adapter->msix_entries);\r\nadapter->msix_entries = NULL;\r\n}\r\nstatic int ixgbevf_init_interrupt_scheme(struct ixgbevf_adapter *adapter)\r\n{\r\nint err;\r\nixgbevf_set_num_queues(adapter);\r\nerr = ixgbevf_set_interrupt_capability(adapter);\r\nif (err) {\r\nhw_dbg(&adapter->hw,\r\n"Unable to setup interrupt capabilities\n");\r\ngoto err_set_interrupt;\r\n}\r\nerr = ixgbevf_alloc_q_vectors(adapter);\r\nif (err) {\r\nhw_dbg(&adapter->hw, "Unable to allocate memory for queue vectors\n");\r\ngoto err_alloc_q_vectors;\r\n}\r\nerr = ixgbevf_alloc_queues(adapter);\r\nif (err) {\r\npr_err("Unable to allocate memory for queues\n");\r\ngoto err_alloc_queues;\r\n}\r\nhw_dbg(&adapter->hw, "Multiqueue %s: Rx Queue count = %u, Tx Queue count = %u\n",\r\n(adapter->num_rx_queues > 1) ? "Enabled" :\r\n"Disabled", adapter->num_rx_queues, adapter->num_tx_queues);\r\nset_bit(__IXGBEVF_DOWN, &adapter->state);\r\nreturn 0;\r\nerr_alloc_queues:\r\nixgbevf_free_q_vectors(adapter);\r\nerr_alloc_q_vectors:\r\nixgbevf_reset_interrupt_capability(adapter);\r\nerr_set_interrupt:\r\nreturn err;\r\n}\r\nstatic void ixgbevf_clear_interrupt_scheme(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nkfree(adapter->tx_ring[i]);\r\nadapter->tx_ring[i] = NULL;\r\n}\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nkfree(adapter->rx_ring[i]);\r\nadapter->rx_ring[i] = NULL;\r\n}\r\nadapter->num_tx_queues = 0;\r\nadapter->num_rx_queues = 0;\r\nixgbevf_free_q_vectors(adapter);\r\nixgbevf_reset_interrupt_capability(adapter);\r\n}\r\nstatic int ixgbevf_sw_init(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct pci_dev *pdev = adapter->pdev;\r\nstruct net_device *netdev = adapter->netdev;\r\nint err;\r\nhw->vendor_id = pdev->vendor;\r\nhw->device_id = pdev->device;\r\nhw->revision_id = pdev->revision;\r\nhw->subsystem_vendor_id = pdev->subsystem_vendor;\r\nhw->subsystem_device_id = pdev->subsystem_device;\r\nhw->mbx.ops.init_params(hw);\r\nif (hw->mac.type >= ixgbe_mac_X550_vf) {\r\nerr = ixgbevf_init_rss_key(adapter);\r\nif (err)\r\ngoto out;\r\n}\r\nhw->mac.max_tx_queues = 2;\r\nhw->mac.max_rx_queues = 2;\r\nspin_lock_init(&adapter->mbx_lock);\r\nerr = hw->mac.ops.reset_hw(hw);\r\nif (err) {\r\ndev_info(&pdev->dev,\r\n"PF still in reset state. Is the PF interface up?\n");\r\n} else {\r\nerr = hw->mac.ops.init_hw(hw);\r\nif (err) {\r\npr_err("init_shared_code failed: %d\n", err);\r\ngoto out;\r\n}\r\nixgbevf_negotiate_api(adapter);\r\nerr = hw->mac.ops.get_mac_addr(hw, hw->mac.addr);\r\nif (err)\r\ndev_info(&pdev->dev, "Error reading MAC address\n");\r\nelse if (is_zero_ether_addr(adapter->hw.mac.addr))\r\ndev_info(&pdev->dev,\r\n"MAC address not assigned by administrator.\n");\r\nether_addr_copy(netdev->dev_addr, hw->mac.addr);\r\n}\r\nif (!is_valid_ether_addr(netdev->dev_addr)) {\r\ndev_info(&pdev->dev, "Assigning random MAC address\n");\r\neth_hw_addr_random(netdev);\r\nether_addr_copy(hw->mac.addr, netdev->dev_addr);\r\nether_addr_copy(hw->mac.perm_addr, netdev->dev_addr);\r\n}\r\nadapter->rx_itr_setting = 1;\r\nadapter->tx_itr_setting = 1;\r\nadapter->tx_ring_count = IXGBEVF_DEFAULT_TXD;\r\nadapter->rx_ring_count = IXGBEVF_DEFAULT_RXD;\r\nset_bit(__IXGBEVF_DOWN, &adapter->state);\r\nreturn 0;\r\nout:\r\nreturn err;\r\n}\r\nvoid ixgbevf_update_stats(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint i;\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\r\ntest_bit(__IXGBEVF_RESETTING, &adapter->state))\r\nreturn;\r\nUPDATE_VF_COUNTER_32bit(IXGBE_VFGPRC, adapter->stats.last_vfgprc,\r\nadapter->stats.vfgprc);\r\nUPDATE_VF_COUNTER_32bit(IXGBE_VFGPTC, adapter->stats.last_vfgptc,\r\nadapter->stats.vfgptc);\r\nUPDATE_VF_COUNTER_36bit(IXGBE_VFGORC_LSB, IXGBE_VFGORC_MSB,\r\nadapter->stats.last_vfgorc,\r\nadapter->stats.vfgorc);\r\nUPDATE_VF_COUNTER_36bit(IXGBE_VFGOTC_LSB, IXGBE_VFGOTC_MSB,\r\nadapter->stats.last_vfgotc,\r\nadapter->stats.vfgotc);\r\nUPDATE_VF_COUNTER_32bit(IXGBE_VFMPRC, adapter->stats.last_vfmprc,\r\nadapter->stats.vfmprc);\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nadapter->hw_csum_rx_error +=\r\nadapter->rx_ring[i]->hw_csum_rx_error;\r\nadapter->rx_ring[i]->hw_csum_rx_error = 0;\r\n}\r\n}\r\nstatic void ixgbevf_service_timer(unsigned long data)\r\n{\r\nstruct ixgbevf_adapter *adapter = (struct ixgbevf_adapter *)data;\r\nmod_timer(&adapter->service_timer, (HZ * 2) + jiffies);\r\nixgbevf_service_event_schedule(adapter);\r\n}\r\nstatic void ixgbevf_reset_subtask(struct ixgbevf_adapter *adapter)\r\n{\r\nif (!test_and_clear_bit(__IXGBEVF_RESET_REQUESTED, &adapter->state))\r\nreturn;\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\r\ntest_bit(__IXGBEVF_REMOVING, &adapter->state) ||\r\ntest_bit(__IXGBEVF_RESETTING, &adapter->state))\r\nreturn;\r\nadapter->tx_timeout_count++;\r\nrtnl_lock();\r\nixgbevf_reinit_locked(adapter);\r\nrtnl_unlock();\r\n}\r\nstatic void ixgbevf_check_hang_subtask(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 eics = 0;\r\nint i;\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\r\ntest_bit(__IXGBEVF_RESETTING, &adapter->state))\r\nreturn;\r\nif (netif_carrier_ok(adapter->netdev)) {\r\nfor (i = 0; i < adapter->num_tx_queues; i++)\r\nset_check_for_tx_hang(adapter->tx_ring[i]);\r\n}\r\nfor (i = 0; i < adapter->num_msix_vectors - NON_Q_VECTORS; i++) {\r\nstruct ixgbevf_q_vector *qv = adapter->q_vector[i];\r\nif (qv->rx.ring || qv->tx.ring)\r\neics |= BIT(i);\r\n}\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEICS, eics);\r\n}\r\nstatic void ixgbevf_watchdog_update_link(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 link_speed = adapter->link_speed;\r\nbool link_up = adapter->link_up;\r\ns32 err;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = hw->mac.ops.check_link(hw, &link_speed, &link_up, false);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (err && time_after(jiffies, adapter->last_reset + (10 * HZ))) {\r\nset_bit(__IXGBEVF_RESET_REQUESTED, &adapter->state);\r\nlink_up = false;\r\n}\r\nadapter->link_up = link_up;\r\nadapter->link_speed = link_speed;\r\n}\r\nstatic void ixgbevf_watchdog_link_is_up(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nif (netif_carrier_ok(netdev))\r\nreturn;\r\ndev_info(&adapter->pdev->dev, "NIC Link is Up %s\n",\r\n(adapter->link_speed == IXGBE_LINK_SPEED_10GB_FULL) ?\r\n"10 Gbps" :\r\n(adapter->link_speed == IXGBE_LINK_SPEED_1GB_FULL) ?\r\n"1 Gbps" :\r\n(adapter->link_speed == IXGBE_LINK_SPEED_100_FULL) ?\r\n"100 Mbps" :\r\n"unknown speed");\r\nnetif_carrier_on(netdev);\r\n}\r\nstatic void ixgbevf_watchdog_link_is_down(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nadapter->link_speed = 0;\r\nif (!netif_carrier_ok(netdev))\r\nreturn;\r\ndev_info(&adapter->pdev->dev, "NIC Link is Down\n");\r\nnetif_carrier_off(netdev);\r\n}\r\nstatic void ixgbevf_watchdog_subtask(struct ixgbevf_adapter *adapter)\r\n{\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\r\ntest_bit(__IXGBEVF_RESETTING, &adapter->state))\r\nreturn;\r\nixgbevf_watchdog_update_link(adapter);\r\nif (adapter->link_up)\r\nixgbevf_watchdog_link_is_up(adapter);\r\nelse\r\nixgbevf_watchdog_link_is_down(adapter);\r\nixgbevf_update_stats(adapter);\r\n}\r\nstatic void ixgbevf_service_task(struct work_struct *work)\r\n{\r\nstruct ixgbevf_adapter *adapter = container_of(work,\r\nstruct ixgbevf_adapter,\r\nservice_task);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nif (IXGBE_REMOVED(hw->hw_addr)) {\r\nif (!test_bit(__IXGBEVF_DOWN, &adapter->state)) {\r\nrtnl_lock();\r\nixgbevf_down(adapter);\r\nrtnl_unlock();\r\n}\r\nreturn;\r\n}\r\nixgbevf_queue_reset_subtask(adapter);\r\nixgbevf_reset_subtask(adapter);\r\nixgbevf_watchdog_subtask(adapter);\r\nixgbevf_check_hang_subtask(adapter);\r\nixgbevf_service_event_complete(adapter);\r\n}\r\nvoid ixgbevf_free_tx_resources(struct ixgbevf_ring *tx_ring)\r\n{\r\nixgbevf_clean_tx_ring(tx_ring);\r\nvfree(tx_ring->tx_buffer_info);\r\ntx_ring->tx_buffer_info = NULL;\r\nif (!tx_ring->desc)\r\nreturn;\r\ndma_free_coherent(tx_ring->dev, tx_ring->size, tx_ring->desc,\r\ntx_ring->dma);\r\ntx_ring->desc = NULL;\r\n}\r\nstatic void ixgbevf_free_all_tx_resources(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_tx_queues; i++)\r\nif (adapter->tx_ring[i]->desc)\r\nixgbevf_free_tx_resources(adapter->tx_ring[i]);\r\n}\r\nint ixgbevf_setup_tx_resources(struct ixgbevf_ring *tx_ring)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(tx_ring->netdev);\r\nint size;\r\nsize = sizeof(struct ixgbevf_tx_buffer) * tx_ring->count;\r\ntx_ring->tx_buffer_info = vzalloc(size);\r\nif (!tx_ring->tx_buffer_info)\r\ngoto err;\r\nu64_stats_init(&tx_ring->syncp);\r\ntx_ring->size = tx_ring->count * sizeof(union ixgbe_adv_tx_desc);\r\ntx_ring->size = ALIGN(tx_ring->size, 4096);\r\ntx_ring->desc = dma_alloc_coherent(tx_ring->dev, tx_ring->size,\r\n&tx_ring->dma, GFP_KERNEL);\r\nif (!tx_ring->desc)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\nvfree(tx_ring->tx_buffer_info);\r\ntx_ring->tx_buffer_info = NULL;\r\nhw_dbg(&adapter->hw, "Unable to allocate memory for the transmit descriptor ring\n");\r\nreturn -ENOMEM;\r\n}\r\nstatic int ixgbevf_setup_all_tx_resources(struct ixgbevf_adapter *adapter)\r\n{\r\nint i, err = 0;\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nerr = ixgbevf_setup_tx_resources(adapter->tx_ring[i]);\r\nif (!err)\r\ncontinue;\r\nhw_dbg(&adapter->hw, "Allocation for Tx Queue %u failed\n", i);\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nint ixgbevf_setup_rx_resources(struct ixgbevf_ring *rx_ring)\r\n{\r\nint size;\r\nsize = sizeof(struct ixgbevf_rx_buffer) * rx_ring->count;\r\nrx_ring->rx_buffer_info = vzalloc(size);\r\nif (!rx_ring->rx_buffer_info)\r\ngoto err;\r\nu64_stats_init(&rx_ring->syncp);\r\nrx_ring->size = rx_ring->count * sizeof(union ixgbe_adv_rx_desc);\r\nrx_ring->size = ALIGN(rx_ring->size, 4096);\r\nrx_ring->desc = dma_alloc_coherent(rx_ring->dev, rx_ring->size,\r\n&rx_ring->dma, GFP_KERNEL);\r\nif (!rx_ring->desc)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\nvfree(rx_ring->rx_buffer_info);\r\nrx_ring->rx_buffer_info = NULL;\r\ndev_err(rx_ring->dev, "Unable to allocate memory for the Rx descriptor ring\n");\r\nreturn -ENOMEM;\r\n}\r\nstatic int ixgbevf_setup_all_rx_resources(struct ixgbevf_adapter *adapter)\r\n{\r\nint i, err = 0;\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nerr = ixgbevf_setup_rx_resources(adapter->rx_ring[i]);\r\nif (!err)\r\ncontinue;\r\nhw_dbg(&adapter->hw, "Allocation for Rx Queue %u failed\n", i);\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nvoid ixgbevf_free_rx_resources(struct ixgbevf_ring *rx_ring)\r\n{\r\nixgbevf_clean_rx_ring(rx_ring);\r\nvfree(rx_ring->rx_buffer_info);\r\nrx_ring->rx_buffer_info = NULL;\r\ndma_free_coherent(rx_ring->dev, rx_ring->size, rx_ring->desc,\r\nrx_ring->dma);\r\nrx_ring->desc = NULL;\r\n}\r\nstatic void ixgbevf_free_all_rx_resources(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_rx_queues; i++)\r\nif (adapter->rx_ring[i]->desc)\r\nixgbevf_free_rx_resources(adapter->rx_ring[i]);\r\n}\r\nint ixgbevf_open(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint err;\r\nif (!adapter->num_msix_vectors)\r\nreturn -ENOMEM;\r\nif (hw->adapter_stopped) {\r\nixgbevf_reset(adapter);\r\nif (hw->adapter_stopped) {\r\nerr = IXGBE_ERR_MBX;\r\npr_err("Unable to start - perhaps the PF Driver isn't up yet\n");\r\ngoto err_setup_reset;\r\n}\r\n}\r\nif (test_bit(__IXGBEVF_TESTING, &adapter->state))\r\nreturn -EBUSY;\r\nnetif_carrier_off(netdev);\r\nerr = ixgbevf_setup_all_tx_resources(adapter);\r\nif (err)\r\ngoto err_setup_tx;\r\nerr = ixgbevf_setup_all_rx_resources(adapter);\r\nif (err)\r\ngoto err_setup_rx;\r\nixgbevf_configure(adapter);\r\nixgbevf_map_rings_to_vectors(adapter);\r\nerr = ixgbevf_request_irq(adapter);\r\nif (err)\r\ngoto err_req_irq;\r\nixgbevf_up_complete(adapter);\r\nreturn 0;\r\nerr_req_irq:\r\nixgbevf_down(adapter);\r\nerr_setup_rx:\r\nixgbevf_free_all_rx_resources(adapter);\r\nerr_setup_tx:\r\nixgbevf_free_all_tx_resources(adapter);\r\nixgbevf_reset(adapter);\r\nerr_setup_reset:\r\nreturn err;\r\n}\r\nstatic void ixgbevf_close_suspend(struct ixgbevf_adapter *adapter)\r\n{\r\nixgbevf_down(adapter);\r\nixgbevf_free_irq(adapter);\r\nixgbevf_free_all_tx_resources(adapter);\r\nixgbevf_free_all_rx_resources(adapter);\r\n}\r\nint ixgbevf_close(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nif (netif_device_present(netdev))\r\nixgbevf_close_suspend(adapter);\r\nreturn 0;\r\n}\r\nstatic void ixgbevf_queue_reset_subtask(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *dev = adapter->netdev;\r\nif (!test_and_clear_bit(__IXGBEVF_QUEUE_RESET_REQUESTED,\r\n&adapter->state))\r\nreturn;\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\r\ntest_bit(__IXGBEVF_RESETTING, &adapter->state))\r\nreturn;\r\nrtnl_lock();\r\nif (netif_running(dev))\r\nixgbevf_close(dev);\r\nixgbevf_clear_interrupt_scheme(adapter);\r\nixgbevf_init_interrupt_scheme(adapter);\r\nif (netif_running(dev))\r\nixgbevf_open(dev);\r\nrtnl_unlock();\r\n}\r\nstatic void ixgbevf_tx_ctxtdesc(struct ixgbevf_ring *tx_ring,\r\nu32 vlan_macip_lens, u32 type_tucmd,\r\nu32 mss_l4len_idx)\r\n{\r\nstruct ixgbe_adv_tx_context_desc *context_desc;\r\nu16 i = tx_ring->next_to_use;\r\ncontext_desc = IXGBEVF_TX_CTXTDESC(tx_ring, i);\r\ni++;\r\ntx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\r\ntype_tucmd |= IXGBE_TXD_CMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;\r\ncontext_desc->vlan_macip_lens = cpu_to_le32(vlan_macip_lens);\r\ncontext_desc->seqnum_seed = 0;\r\ncontext_desc->type_tucmd_mlhl = cpu_to_le32(type_tucmd);\r\ncontext_desc->mss_l4len_idx = cpu_to_le32(mss_l4len_idx);\r\n}\r\nstatic int ixgbevf_tso(struct ixgbevf_ring *tx_ring,\r\nstruct ixgbevf_tx_buffer *first,\r\nu8 *hdr_len)\r\n{\r\nu32 vlan_macip_lens, type_tucmd, mss_l4len_idx;\r\nstruct sk_buff *skb = first->skb;\r\nunion {\r\nstruct iphdr *v4;\r\nstruct ipv6hdr *v6;\r\nunsigned char *hdr;\r\n} ip;\r\nunion {\r\nstruct tcphdr *tcp;\r\nunsigned char *hdr;\r\n} l4;\r\nu32 paylen, l4_offset;\r\nint err;\r\nif (skb->ip_summed != CHECKSUM_PARTIAL)\r\nreturn 0;\r\nif (!skb_is_gso(skb))\r\nreturn 0;\r\nerr = skb_cow_head(skb, 0);\r\nif (err < 0)\r\nreturn err;\r\nif (eth_p_mpls(first->protocol))\r\nip.hdr = skb_inner_network_header(skb);\r\nelse\r\nip.hdr = skb_network_header(skb);\r\nl4.hdr = skb_checksum_start(skb);\r\ntype_tucmd = IXGBE_ADVTXD_TUCMD_L4T_TCP;\r\nif (ip.v4->version == 4) {\r\nunsigned char *csum_start = skb_checksum_start(skb);\r\nunsigned char *trans_start = ip.hdr + (ip.v4->ihl * 4);\r\nip.v4->check = csum_fold(csum_partial(trans_start,\r\ncsum_start - trans_start,\r\n0));\r\ntype_tucmd |= IXGBE_ADVTXD_TUCMD_IPV4;\r\nip.v4->tot_len = 0;\r\nfirst->tx_flags |= IXGBE_TX_FLAGS_TSO |\r\nIXGBE_TX_FLAGS_CSUM |\r\nIXGBE_TX_FLAGS_IPV4;\r\n} else {\r\nip.v6->payload_len = 0;\r\nfirst->tx_flags |= IXGBE_TX_FLAGS_TSO |\r\nIXGBE_TX_FLAGS_CSUM;\r\n}\r\nl4_offset = l4.hdr - skb->data;\r\n*hdr_len = (l4.tcp->doff * 4) + l4_offset;\r\npaylen = skb->len - l4_offset;\r\ncsum_replace_by_diff(&l4.tcp->check, htonl(paylen));\r\nfirst->gso_segs = skb_shinfo(skb)->gso_segs;\r\nfirst->bytecount += (first->gso_segs - 1) * *hdr_len;\r\nmss_l4len_idx = (*hdr_len - l4_offset) << IXGBE_ADVTXD_L4LEN_SHIFT;\r\nmss_l4len_idx |= skb_shinfo(skb)->gso_size << IXGBE_ADVTXD_MSS_SHIFT;\r\nmss_l4len_idx |= (1u << IXGBE_ADVTXD_IDX_SHIFT);\r\nvlan_macip_lens = l4.hdr - ip.hdr;\r\nvlan_macip_lens |= (ip.hdr - skb->data) << IXGBE_ADVTXD_MACLEN_SHIFT;\r\nvlan_macip_lens |= first->tx_flags & IXGBE_TX_FLAGS_VLAN_MASK;\r\nixgbevf_tx_ctxtdesc(tx_ring, vlan_macip_lens,\r\ntype_tucmd, mss_l4len_idx);\r\nreturn 1;\r\n}\r\nstatic inline bool ixgbevf_ipv6_csum_is_sctp(struct sk_buff *skb)\r\n{\r\nunsigned int offset = 0;\r\nipv6_find_hdr(skb, &offset, IPPROTO_SCTP, NULL, NULL);\r\nreturn offset == skb_checksum_start_offset(skb);\r\n}\r\nstatic void ixgbevf_tx_csum(struct ixgbevf_ring *tx_ring,\r\nstruct ixgbevf_tx_buffer *first)\r\n{\r\nstruct sk_buff *skb = first->skb;\r\nu32 vlan_macip_lens = 0;\r\nu32 type_tucmd = 0;\r\nif (skb->ip_summed != CHECKSUM_PARTIAL)\r\ngoto no_csum;\r\nswitch (skb->csum_offset) {\r\ncase offsetof(struct tcphdr, check):\r\ntype_tucmd = IXGBE_ADVTXD_TUCMD_L4T_TCP;\r\ncase offsetof(struct udphdr, check):\r\nbreak;\r\ncase offsetof(struct sctphdr, checksum):\r\nif (((first->protocol == htons(ETH_P_IP)) &&\r\n(ip_hdr(skb)->protocol == IPPROTO_SCTP)) ||\r\n((first->protocol == htons(ETH_P_IPV6)) &&\r\nixgbevf_ipv6_csum_is_sctp(skb))) {\r\ntype_tucmd = IXGBE_ADVTXD_TUCMD_L4T_SCTP;\r\nbreak;\r\n}\r\ndefault:\r\nskb_checksum_help(skb);\r\ngoto no_csum;\r\n}\r\nfirst->tx_flags |= IXGBE_TX_FLAGS_CSUM;\r\nvlan_macip_lens = skb_checksum_start_offset(skb) -\r\nskb_network_offset(skb);\r\nno_csum:\r\nvlan_macip_lens |= skb_network_offset(skb) << IXGBE_ADVTXD_MACLEN_SHIFT;\r\nvlan_macip_lens |= first->tx_flags & IXGBE_TX_FLAGS_VLAN_MASK;\r\nixgbevf_tx_ctxtdesc(tx_ring, vlan_macip_lens, type_tucmd, 0);\r\n}\r\nstatic __le32 ixgbevf_tx_cmd_type(u32 tx_flags)\r\n{\r\n__le32 cmd_type = cpu_to_le32(IXGBE_ADVTXD_DTYP_DATA |\r\nIXGBE_ADVTXD_DCMD_IFCS |\r\nIXGBE_ADVTXD_DCMD_DEXT);\r\nif (tx_flags & IXGBE_TX_FLAGS_VLAN)\r\ncmd_type |= cpu_to_le32(IXGBE_ADVTXD_DCMD_VLE);\r\nif (tx_flags & IXGBE_TX_FLAGS_TSO)\r\ncmd_type |= cpu_to_le32(IXGBE_ADVTXD_DCMD_TSE);\r\nreturn cmd_type;\r\n}\r\nstatic void ixgbevf_tx_olinfo_status(union ixgbe_adv_tx_desc *tx_desc,\r\nu32 tx_flags, unsigned int paylen)\r\n{\r\n__le32 olinfo_status = cpu_to_le32(paylen << IXGBE_ADVTXD_PAYLEN_SHIFT);\r\nif (tx_flags & IXGBE_TX_FLAGS_CSUM)\r\nolinfo_status |= cpu_to_le32(IXGBE_ADVTXD_POPTS_TXSM);\r\nif (tx_flags & IXGBE_TX_FLAGS_IPV4)\r\nolinfo_status |= cpu_to_le32(IXGBE_ADVTXD_POPTS_IXSM);\r\nif (tx_flags & IXGBE_TX_FLAGS_TSO)\r\nolinfo_status |= cpu_to_le32(1u << IXGBE_ADVTXD_IDX_SHIFT);\r\nolinfo_status |= cpu_to_le32(IXGBE_ADVTXD_CC);\r\ntx_desc->read.olinfo_status = olinfo_status;\r\n}\r\nstatic void ixgbevf_tx_map(struct ixgbevf_ring *tx_ring,\r\nstruct ixgbevf_tx_buffer *first,\r\nconst u8 hdr_len)\r\n{\r\ndma_addr_t dma;\r\nstruct sk_buff *skb = first->skb;\r\nstruct ixgbevf_tx_buffer *tx_buffer;\r\nunion ixgbe_adv_tx_desc *tx_desc;\r\nstruct skb_frag_struct *frag = &skb_shinfo(skb)->frags[0];\r\nunsigned int data_len = skb->data_len;\r\nunsigned int size = skb_headlen(skb);\r\nunsigned int paylen = skb->len - hdr_len;\r\nu32 tx_flags = first->tx_flags;\r\n__le32 cmd_type;\r\nu16 i = tx_ring->next_to_use;\r\ntx_desc = IXGBEVF_TX_DESC(tx_ring, i);\r\nixgbevf_tx_olinfo_status(tx_desc, tx_flags, paylen);\r\ncmd_type = ixgbevf_tx_cmd_type(tx_flags);\r\ndma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);\r\nif (dma_mapping_error(tx_ring->dev, dma))\r\ngoto dma_error;\r\ndma_unmap_len_set(first, len, size);\r\ndma_unmap_addr_set(first, dma, dma);\r\ntx_desc->read.buffer_addr = cpu_to_le64(dma);\r\nfor (;;) {\r\nwhile (unlikely(size > IXGBE_MAX_DATA_PER_TXD)) {\r\ntx_desc->read.cmd_type_len =\r\ncmd_type | cpu_to_le32(IXGBE_MAX_DATA_PER_TXD);\r\ni++;\r\ntx_desc++;\r\nif (i == tx_ring->count) {\r\ntx_desc = IXGBEVF_TX_DESC(tx_ring, 0);\r\ni = 0;\r\n}\r\ndma += IXGBE_MAX_DATA_PER_TXD;\r\nsize -= IXGBE_MAX_DATA_PER_TXD;\r\ntx_desc->read.buffer_addr = cpu_to_le64(dma);\r\ntx_desc->read.olinfo_status = 0;\r\n}\r\nif (likely(!data_len))\r\nbreak;\r\ntx_desc->read.cmd_type_len = cmd_type | cpu_to_le32(size);\r\ni++;\r\ntx_desc++;\r\nif (i == tx_ring->count) {\r\ntx_desc = IXGBEVF_TX_DESC(tx_ring, 0);\r\ni = 0;\r\n}\r\nsize = skb_frag_size(frag);\r\ndata_len -= size;\r\ndma = skb_frag_dma_map(tx_ring->dev, frag, 0, size,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(tx_ring->dev, dma))\r\ngoto dma_error;\r\ntx_buffer = &tx_ring->tx_buffer_info[i];\r\ndma_unmap_len_set(tx_buffer, len, size);\r\ndma_unmap_addr_set(tx_buffer, dma, dma);\r\ntx_desc->read.buffer_addr = cpu_to_le64(dma);\r\ntx_desc->read.olinfo_status = 0;\r\nfrag++;\r\n}\r\ncmd_type |= cpu_to_le32(size) | cpu_to_le32(IXGBE_TXD_CMD);\r\ntx_desc->read.cmd_type_len = cmd_type;\r\nfirst->time_stamp = jiffies;\r\nwmb();\r\nfirst->next_to_watch = tx_desc;\r\ni++;\r\nif (i == tx_ring->count)\r\ni = 0;\r\ntx_ring->next_to_use = i;\r\nixgbevf_write_tail(tx_ring, i);\r\nreturn;\r\ndma_error:\r\ndev_err(tx_ring->dev, "TX DMA map failed\n");\r\nfor (;;) {\r\ntx_buffer = &tx_ring->tx_buffer_info[i];\r\nixgbevf_unmap_and_free_tx_resource(tx_ring, tx_buffer);\r\nif (tx_buffer == first)\r\nbreak;\r\nif (i == 0)\r\ni = tx_ring->count;\r\ni--;\r\n}\r\ntx_ring->next_to_use = i;\r\n}\r\nstatic int __ixgbevf_maybe_stop_tx(struct ixgbevf_ring *tx_ring, int size)\r\n{\r\nnetif_stop_subqueue(tx_ring->netdev, tx_ring->queue_index);\r\nsmp_mb();\r\nif (likely(ixgbevf_desc_unused(tx_ring) < size))\r\nreturn -EBUSY;\r\nnetif_start_subqueue(tx_ring->netdev, tx_ring->queue_index);\r\n++tx_ring->tx_stats.restart_queue;\r\nreturn 0;\r\n}\r\nstatic int ixgbevf_maybe_stop_tx(struct ixgbevf_ring *tx_ring, int size)\r\n{\r\nif (likely(ixgbevf_desc_unused(tx_ring) >= size))\r\nreturn 0;\r\nreturn __ixgbevf_maybe_stop_tx(tx_ring, size);\r\n}\r\nstatic int ixgbevf_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbevf_tx_buffer *first;\r\nstruct ixgbevf_ring *tx_ring;\r\nint tso;\r\nu32 tx_flags = 0;\r\nu16 count = TXD_USE_COUNT(skb_headlen(skb));\r\n#if PAGE_SIZE > IXGBE_MAX_DATA_PER_TXD\r\nunsigned short f;\r\n#endif\r\nu8 hdr_len = 0;\r\nu8 *dst_mac = skb_header_pointer(skb, 0, 0, NULL);\r\nif (!dst_mac || is_link_local_ether_addr(dst_mac)) {\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\ntx_ring = adapter->tx_ring[skb->queue_mapping];\r\n#if PAGE_SIZE > IXGBE_MAX_DATA_PER_TXD\r\nfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++)\r\ncount += TXD_USE_COUNT(skb_shinfo(skb)->frags[f].size);\r\n#else\r\ncount += skb_shinfo(skb)->nr_frags;\r\n#endif\r\nif (ixgbevf_maybe_stop_tx(tx_ring, count + 3)) {\r\ntx_ring->tx_stats.tx_busy++;\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nfirst = &tx_ring->tx_buffer_info[tx_ring->next_to_use];\r\nfirst->skb = skb;\r\nfirst->bytecount = skb->len;\r\nfirst->gso_segs = 1;\r\nif (skb_vlan_tag_present(skb)) {\r\ntx_flags |= skb_vlan_tag_get(skb);\r\ntx_flags <<= IXGBE_TX_FLAGS_VLAN_SHIFT;\r\ntx_flags |= IXGBE_TX_FLAGS_VLAN;\r\n}\r\nfirst->tx_flags = tx_flags;\r\nfirst->protocol = vlan_get_protocol(skb);\r\ntso = ixgbevf_tso(tx_ring, first, &hdr_len);\r\nif (tso < 0)\r\ngoto out_drop;\r\nelse if (!tso)\r\nixgbevf_tx_csum(tx_ring, first);\r\nixgbevf_tx_map(tx_ring, first, hdr_len);\r\nixgbevf_maybe_stop_tx(tx_ring, DESC_NEEDED);\r\nreturn NETDEV_TX_OK;\r\nout_drop:\r\ndev_kfree_skb_any(first->skb);\r\nfirst->skb = NULL;\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int ixgbevf_set_mac(struct net_device *netdev, void *p)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct sockaddr *addr = p;\r\nint err;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = hw->mac.ops.set_rar(hw, 0, addr->sa_data, 0);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (err)\r\nreturn -EPERM;\r\nether_addr_copy(hw->mac.addr, addr->sa_data);\r\nether_addr_copy(netdev->dev_addr, addr->sa_data);\r\nreturn 0;\r\n}\r\nstatic int ixgbevf_change_mtu(struct net_device *netdev, int new_mtu)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN;\r\nint ret;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nret = hw->mac.ops.set_rlpml(hw, max_frame);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (ret)\r\nreturn -EINVAL;\r\nhw_dbg(hw, "changing MTU from %d to %d\n",\r\nnetdev->mtu, new_mtu);\r\nnetdev->mtu = new_mtu;\r\nreturn 0;\r\n}\r\nstatic void ixgbevf_netpoll(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nint i;\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state))\r\nreturn;\r\nfor (i = 0; i < adapter->num_rx_queues; i++)\r\nixgbevf_msix_clean_rings(0, adapter->q_vector[i]);\r\n}\r\nstatic int ixgbevf_suspend(struct pci_dev *pdev, pm_message_t state)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\n#ifdef CONFIG_PM\r\nint retval = 0;\r\n#endif\r\nrtnl_lock();\r\nnetif_device_detach(netdev);\r\nif (netif_running(netdev))\r\nixgbevf_close_suspend(adapter);\r\nixgbevf_clear_interrupt_scheme(adapter);\r\nrtnl_unlock();\r\n#ifdef CONFIG_PM\r\nretval = pci_save_state(pdev);\r\nif (retval)\r\nreturn retval;\r\n#endif\r\nif (!test_and_set_bit(__IXGBEVF_DISABLED, &adapter->state))\r\npci_disable_device(pdev);\r\nreturn 0;\r\n}\r\nstatic int ixgbevf_resume(struct pci_dev *pdev)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nu32 err;\r\npci_restore_state(pdev);\r\npci_save_state(pdev);\r\nerr = pci_enable_device_mem(pdev);\r\nif (err) {\r\ndev_err(&pdev->dev, "Cannot enable PCI device from suspend\n");\r\nreturn err;\r\n}\r\nadapter->hw.hw_addr = adapter->io_addr;\r\nsmp_mb__before_atomic();\r\nclear_bit(__IXGBEVF_DISABLED, &adapter->state);\r\npci_set_master(pdev);\r\nixgbevf_reset(adapter);\r\nrtnl_lock();\r\nerr = ixgbevf_init_interrupt_scheme(adapter);\r\nrtnl_unlock();\r\nif (err) {\r\ndev_err(&pdev->dev, "Cannot initialize interrupts\n");\r\nreturn err;\r\n}\r\nif (netif_running(netdev)) {\r\nerr = ixgbevf_open(netdev);\r\nif (err)\r\nreturn err;\r\n}\r\nnetif_device_attach(netdev);\r\nreturn err;\r\n}\r\nstatic void ixgbevf_shutdown(struct pci_dev *pdev)\r\n{\r\nixgbevf_suspend(pdev, PMSG_SUSPEND);\r\n}\r\nstatic void ixgbevf_get_stats(struct net_device *netdev,\r\nstruct rtnl_link_stats64 *stats)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nunsigned int start;\r\nu64 bytes, packets;\r\nconst struct ixgbevf_ring *ring;\r\nint i;\r\nixgbevf_update_stats(adapter);\r\nstats->multicast = adapter->stats.vfmprc - adapter->stats.base_vfmprc;\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nring = adapter->rx_ring[i];\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&ring->syncp);\r\nbytes = ring->stats.bytes;\r\npackets = ring->stats.packets;\r\n} while (u64_stats_fetch_retry_irq(&ring->syncp, start));\r\nstats->rx_bytes += bytes;\r\nstats->rx_packets += packets;\r\n}\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nring = adapter->tx_ring[i];\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&ring->syncp);\r\nbytes = ring->stats.bytes;\r\npackets = ring->stats.packets;\r\n} while (u64_stats_fetch_retry_irq(&ring->syncp, start));\r\nstats->tx_bytes += bytes;\r\nstats->tx_packets += packets;\r\n}\r\n}\r\nstatic netdev_features_t\r\nixgbevf_features_check(struct sk_buff *skb, struct net_device *dev,\r\nnetdev_features_t features)\r\n{\r\nunsigned int network_hdr_len, mac_hdr_len;\r\nmac_hdr_len = skb_network_header(skb) - skb->data;\r\nif (unlikely(mac_hdr_len > IXGBEVF_MAX_MAC_HDR_LEN))\r\nreturn features & ~(NETIF_F_HW_CSUM |\r\nNETIF_F_SCTP_CRC |\r\nNETIF_F_HW_VLAN_CTAG_TX |\r\nNETIF_F_TSO |\r\nNETIF_F_TSO6);\r\nnetwork_hdr_len = skb_checksum_start(skb) - skb_network_header(skb);\r\nif (unlikely(network_hdr_len > IXGBEVF_MAX_NETWORK_HDR_LEN))\r\nreturn features & ~(NETIF_F_HW_CSUM |\r\nNETIF_F_SCTP_CRC |\r\nNETIF_F_TSO |\r\nNETIF_F_TSO6);\r\nif (skb->encapsulation && !(features & NETIF_F_TSO_MANGLEID))\r\nfeatures &= ~NETIF_F_TSO;\r\nreturn features;\r\n}\r\nstatic void ixgbevf_assign_netdev_ops(struct net_device *dev)\r\n{\r\ndev->netdev_ops = &ixgbevf_netdev_ops;\r\nixgbevf_set_ethtool_ops(dev);\r\ndev->watchdog_timeo = 5 * HZ;\r\n}\r\nstatic int ixgbevf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nstruct net_device *netdev;\r\nstruct ixgbevf_adapter *adapter = NULL;\r\nstruct ixgbe_hw *hw = NULL;\r\nconst struct ixgbevf_info *ii = ixgbevf_info_tbl[ent->driver_data];\r\nint err, pci_using_dac;\r\nbool disable_dev = false;\r\nerr = pci_enable_device(pdev);\r\nif (err)\r\nreturn err;\r\nif (!dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64))) {\r\npci_using_dac = 1;\r\n} else {\r\nerr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\r\nif (err) {\r\ndev_err(&pdev->dev, "No usable DMA configuration, aborting\n");\r\ngoto err_dma;\r\n}\r\npci_using_dac = 0;\r\n}\r\nerr = pci_request_regions(pdev, ixgbevf_driver_name);\r\nif (err) {\r\ndev_err(&pdev->dev, "pci_request_regions failed 0x%x\n", err);\r\ngoto err_pci_reg;\r\n}\r\npci_set_master(pdev);\r\nnetdev = alloc_etherdev_mq(sizeof(struct ixgbevf_adapter),\r\nMAX_TX_QUEUES);\r\nif (!netdev) {\r\nerr = -ENOMEM;\r\ngoto err_alloc_etherdev;\r\n}\r\nSET_NETDEV_DEV(netdev, &pdev->dev);\r\nadapter = netdev_priv(netdev);\r\nadapter->netdev = netdev;\r\nadapter->pdev = pdev;\r\nhw = &adapter->hw;\r\nhw->back = adapter;\r\nadapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\r\npci_save_state(pdev);\r\nhw->hw_addr = ioremap(pci_resource_start(pdev, 0),\r\npci_resource_len(pdev, 0));\r\nadapter->io_addr = hw->hw_addr;\r\nif (!hw->hw_addr) {\r\nerr = -EIO;\r\ngoto err_ioremap;\r\n}\r\nixgbevf_assign_netdev_ops(netdev);\r\nmemcpy(&hw->mac.ops, ii->mac_ops, sizeof(hw->mac.ops));\r\nhw->mac.type = ii->mac;\r\nmemcpy(&hw->mbx.ops, &ixgbevf_mbx_ops,\r\nsizeof(struct ixgbe_mbx_operations));\r\nerr = ixgbevf_sw_init(adapter);\r\nif (err)\r\ngoto err_sw_init;\r\nif (!is_valid_ether_addr(netdev->dev_addr)) {\r\npr_err("invalid MAC address\n");\r\nerr = -EIO;\r\ngoto err_sw_init;\r\n}\r\nnetdev->hw_features = NETIF_F_SG |\r\nNETIF_F_TSO |\r\nNETIF_F_TSO6 |\r\nNETIF_F_RXCSUM |\r\nNETIF_F_HW_CSUM |\r\nNETIF_F_SCTP_CRC;\r\n#define IXGBEVF_GSO_PARTIAL_FEATURES (NETIF_F_GSO_GRE | \\r\nNETIF_F_GSO_GRE_CSUM | \\r\nNETIF_F_GSO_IPXIP4 | \\r\nNETIF_F_GSO_IPXIP6 | \\r\nNETIF_F_GSO_UDP_TUNNEL | \\r\nNETIF_F_GSO_UDP_TUNNEL_CSUM)\r\nnetdev->gso_partial_features = IXGBEVF_GSO_PARTIAL_FEATURES;\r\nnetdev->hw_features |= NETIF_F_GSO_PARTIAL |\r\nIXGBEVF_GSO_PARTIAL_FEATURES;\r\nnetdev->features = netdev->hw_features;\r\nif (pci_using_dac)\r\nnetdev->features |= NETIF_F_HIGHDMA;\r\nnetdev->vlan_features |= netdev->features | NETIF_F_TSO_MANGLEID;\r\nnetdev->mpls_features |= NETIF_F_SG |\r\nNETIF_F_TSO |\r\nNETIF_F_TSO6 |\r\nNETIF_F_HW_CSUM;\r\nnetdev->mpls_features |= IXGBEVF_GSO_PARTIAL_FEATURES;\r\nnetdev->hw_enc_features |= netdev->vlan_features;\r\nnetdev->features |= NETIF_F_HW_VLAN_CTAG_FILTER |\r\nNETIF_F_HW_VLAN_CTAG_RX |\r\nNETIF_F_HW_VLAN_CTAG_TX;\r\nnetdev->priv_flags |= IFF_UNICAST_FLT;\r\nnetdev->min_mtu = ETH_MIN_MTU;\r\nswitch (adapter->hw.api_version) {\r\ncase ixgbe_mbox_api_11:\r\ncase ixgbe_mbox_api_12:\r\ncase ixgbe_mbox_api_13:\r\nnetdev->max_mtu = IXGBE_MAX_JUMBO_FRAME_SIZE -\r\n(ETH_HLEN + ETH_FCS_LEN);\r\nbreak;\r\ndefault:\r\nif (adapter->hw.mac.type != ixgbe_mac_82599_vf)\r\nnetdev->max_mtu = IXGBE_MAX_JUMBO_FRAME_SIZE -\r\n(ETH_HLEN + ETH_FCS_LEN);\r\nelse\r\nnetdev->max_mtu = ETH_DATA_LEN + ETH_FCS_LEN;\r\nbreak;\r\n}\r\nif (IXGBE_REMOVED(hw->hw_addr)) {\r\nerr = -EIO;\r\ngoto err_sw_init;\r\n}\r\nsetup_timer(&adapter->service_timer, &ixgbevf_service_timer,\r\n(unsigned long)adapter);\r\nINIT_WORK(&adapter->service_task, ixgbevf_service_task);\r\nset_bit(__IXGBEVF_SERVICE_INITED, &adapter->state);\r\nclear_bit(__IXGBEVF_SERVICE_SCHED, &adapter->state);\r\nerr = ixgbevf_init_interrupt_scheme(adapter);\r\nif (err)\r\ngoto err_sw_init;\r\nstrcpy(netdev->name, "eth%d");\r\nerr = register_netdev(netdev);\r\nif (err)\r\ngoto err_register;\r\npci_set_drvdata(pdev, netdev);\r\nnetif_carrier_off(netdev);\r\nixgbevf_init_last_counter_stats(adapter);\r\ndev_info(&pdev->dev, "%pM\n", netdev->dev_addr);\r\ndev_info(&pdev->dev, "MAC: %d\n", hw->mac.type);\r\nswitch (hw->mac.type) {\r\ncase ixgbe_mac_X550_vf:\r\ndev_info(&pdev->dev, "Intel(R) X550 Virtual Function\n");\r\nbreak;\r\ncase ixgbe_mac_X540_vf:\r\ndev_info(&pdev->dev, "Intel(R) X540 Virtual Function\n");\r\nbreak;\r\ncase ixgbe_mac_82599_vf:\r\ndefault:\r\ndev_info(&pdev->dev, "Intel(R) 82599 Virtual Function\n");\r\nbreak;\r\n}\r\nreturn 0;\r\nerr_register:\r\nixgbevf_clear_interrupt_scheme(adapter);\r\nerr_sw_init:\r\nixgbevf_reset_interrupt_capability(adapter);\r\niounmap(adapter->io_addr);\r\nkfree(adapter->rss_key);\r\nerr_ioremap:\r\ndisable_dev = !test_and_set_bit(__IXGBEVF_DISABLED, &adapter->state);\r\nfree_netdev(netdev);\r\nerr_alloc_etherdev:\r\npci_release_regions(pdev);\r\nerr_pci_reg:\r\nerr_dma:\r\nif (!adapter || disable_dev)\r\npci_disable_device(pdev);\r\nreturn err;\r\n}\r\nstatic void ixgbevf_remove(struct pci_dev *pdev)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter;\r\nbool disable_dev;\r\nif (!netdev)\r\nreturn;\r\nadapter = netdev_priv(netdev);\r\nset_bit(__IXGBEVF_REMOVING, &adapter->state);\r\ncancel_work_sync(&adapter->service_task);\r\nif (netdev->reg_state == NETREG_REGISTERED)\r\nunregister_netdev(netdev);\r\nixgbevf_clear_interrupt_scheme(adapter);\r\nixgbevf_reset_interrupt_capability(adapter);\r\niounmap(adapter->io_addr);\r\npci_release_regions(pdev);\r\nhw_dbg(&adapter->hw, "Remove complete\n");\r\nkfree(adapter->rss_key);\r\ndisable_dev = !test_and_set_bit(__IXGBEVF_DISABLED, &adapter->state);\r\nfree_netdev(netdev);\r\nif (disable_dev)\r\npci_disable_device(pdev);\r\n}\r\nstatic pci_ers_result_t ixgbevf_io_error_detected(struct pci_dev *pdev,\r\npci_channel_state_t state)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nif (!test_bit(__IXGBEVF_SERVICE_INITED, &adapter->state))\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\nrtnl_lock();\r\nnetif_device_detach(netdev);\r\nif (state == pci_channel_io_perm_failure) {\r\nrtnl_unlock();\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\nif (netif_running(netdev))\r\nixgbevf_close_suspend(adapter);\r\nif (!test_and_set_bit(__IXGBEVF_DISABLED, &adapter->state))\r\npci_disable_device(pdev);\r\nrtnl_unlock();\r\nreturn PCI_ERS_RESULT_NEED_RESET;\r\n}\r\nstatic pci_ers_result_t ixgbevf_io_slot_reset(struct pci_dev *pdev)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nif (pci_enable_device_mem(pdev)) {\r\ndev_err(&pdev->dev,\r\n"Cannot re-enable PCI device after reset.\n");\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\nadapter->hw.hw_addr = adapter->io_addr;\r\nsmp_mb__before_atomic();\r\nclear_bit(__IXGBEVF_DISABLED, &adapter->state);\r\npci_set_master(pdev);\r\nixgbevf_reset(adapter);\r\nreturn PCI_ERS_RESULT_RECOVERED;\r\n}\r\nstatic void ixgbevf_io_resume(struct pci_dev *pdev)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nrtnl_lock();\r\nif (netif_running(netdev))\r\nixgbevf_open(netdev);\r\nnetif_device_attach(netdev);\r\nrtnl_unlock();\r\n}\r\nstatic int __init ixgbevf_init_module(void)\r\n{\r\npr_info("%s - version %s\n", ixgbevf_driver_string,\r\nixgbevf_driver_version);\r\npr_info("%s\n", ixgbevf_copyright);\r\nixgbevf_wq = create_singlethread_workqueue(ixgbevf_driver_name);\r\nif (!ixgbevf_wq) {\r\npr_err("%s: Failed to create workqueue\n", ixgbevf_driver_name);\r\nreturn -ENOMEM;\r\n}\r\nreturn pci_register_driver(&ixgbevf_driver);\r\n}\r\nstatic void __exit ixgbevf_exit_module(void)\r\n{\r\npci_unregister_driver(&ixgbevf_driver);\r\nif (ixgbevf_wq) {\r\ndestroy_workqueue(ixgbevf_wq);\r\nixgbevf_wq = NULL;\r\n}\r\n}\r\nchar *ixgbevf_get_hw_dev_name(struct ixgbe_hw *hw)\r\n{\r\nstruct ixgbevf_adapter *adapter = hw->back;\r\nreturn adapter->netdev->name;\r\n}
