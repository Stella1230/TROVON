static void sdma_v2_4_init_golden_registers(struct amdgpu_device *adev)\r\n{\r\nswitch (adev->asic_type) {\r\ncase CHIP_TOPAZ:\r\namdgpu_program_register_sequence(adev,\r\niceland_mgcg_cgcg_init,\r\n(const u32)ARRAY_SIZE(iceland_mgcg_cgcg_init));\r\namdgpu_program_register_sequence(adev,\r\ngolden_settings_iceland_a11,\r\n(const u32)ARRAY_SIZE(golden_settings_iceland_a11));\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void sdma_v2_4_free_microcode(struct amdgpu_device *adev)\r\n{\r\nint i;\r\nfor (i = 0; i < adev->sdma.num_instances; i++) {\r\nrelease_firmware(adev->sdma.instance[i].fw);\r\nadev->sdma.instance[i].fw = NULL;\r\n}\r\n}\r\nstatic int sdma_v2_4_init_microcode(struct amdgpu_device *adev)\r\n{\r\nconst char *chip_name;\r\nchar fw_name[30];\r\nint err = 0, i;\r\nstruct amdgpu_firmware_info *info = NULL;\r\nconst struct common_firmware_header *header = NULL;\r\nconst struct sdma_firmware_header_v1_0 *hdr;\r\nDRM_DEBUG("\n");\r\nswitch (adev->asic_type) {\r\ncase CHIP_TOPAZ:\r\nchip_name = "topaz";\r\nbreak;\r\ndefault: BUG();\r\n}\r\nfor (i = 0; i < adev->sdma.num_instances; i++) {\r\nif (i == 0)\r\nsnprintf(fw_name, sizeof(fw_name), "amdgpu/%s_sdma.bin", chip_name);\r\nelse\r\nsnprintf(fw_name, sizeof(fw_name), "amdgpu/%s_sdma1.bin", chip_name);\r\nerr = request_firmware(&adev->sdma.instance[i].fw, fw_name, adev->dev);\r\nif (err)\r\ngoto out;\r\nerr = amdgpu_ucode_validate(adev->sdma.instance[i].fw);\r\nif (err)\r\ngoto out;\r\nhdr = (const struct sdma_firmware_header_v1_0 *)adev->sdma.instance[i].fw->data;\r\nadev->sdma.instance[i].fw_version = le32_to_cpu(hdr->header.ucode_version);\r\nadev->sdma.instance[i].feature_version = le32_to_cpu(hdr->ucode_feature_version);\r\nif (adev->sdma.instance[i].feature_version >= 20)\r\nadev->sdma.instance[i].burst_nop = true;\r\nif (adev->firmware.load_type == AMDGPU_FW_LOAD_SMU) {\r\ninfo = &adev->firmware.ucode[AMDGPU_UCODE_ID_SDMA0 + i];\r\ninfo->ucode_id = AMDGPU_UCODE_ID_SDMA0 + i;\r\ninfo->fw = adev->sdma.instance[i].fw;\r\nheader = (const struct common_firmware_header *)info->fw->data;\r\nadev->firmware.fw_size +=\r\nALIGN(le32_to_cpu(header->ucode_size_bytes), PAGE_SIZE);\r\n}\r\n}\r\nout:\r\nif (err) {\r\npr_err("sdma_v2_4: Failed to load firmware \"%s\"\n", fw_name);\r\nfor (i = 0; i < adev->sdma.num_instances; i++) {\r\nrelease_firmware(adev->sdma.instance[i].fw);\r\nadev->sdma.instance[i].fw = NULL;\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic uint64_t sdma_v2_4_ring_get_rptr(struct amdgpu_ring *ring)\r\n{\r\nreturn ring->adev->wb.wb[ring->rptr_offs] >> 2;\r\n}\r\nstatic uint64_t sdma_v2_4_ring_get_wptr(struct amdgpu_ring *ring)\r\n{\r\nstruct amdgpu_device *adev = ring->adev;\r\nint me = (ring == &ring->adev->sdma.instance[0].ring) ? 0 : 1;\r\nu32 wptr = RREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[me]) >> 2;\r\nreturn wptr;\r\n}\r\nstatic void sdma_v2_4_ring_set_wptr(struct amdgpu_ring *ring)\r\n{\r\nstruct amdgpu_device *adev = ring->adev;\r\nint me = (ring == &ring->adev->sdma.instance[0].ring) ? 0 : 1;\r\nWREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[me], lower_32_bits(ring->wptr) << 2);\r\n}\r\nstatic void sdma_v2_4_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)\r\n{\r\nstruct amdgpu_sdma_instance *sdma = amdgpu_get_sdma_instance(ring);\r\nint i;\r\nfor (i = 0; i < count; i++)\r\nif (sdma && sdma->burst_nop && (i == 0))\r\namdgpu_ring_write(ring, ring->funcs->nop |\r\nSDMA_PKT_NOP_HEADER_COUNT(count - 1));\r\nelse\r\namdgpu_ring_write(ring, ring->funcs->nop);\r\n}\r\nstatic void sdma_v2_4_ring_emit_ib(struct amdgpu_ring *ring,\r\nstruct amdgpu_ib *ib,\r\nunsigned vm_id, bool ctx_switch)\r\n{\r\nu32 vmid = vm_id & 0xf;\r\nsdma_v2_4_ring_insert_nop(ring, (10 - (lower_32_bits(ring->wptr) & 7)) % 8);\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_INDIRECT) |\r\nSDMA_PKT_INDIRECT_HEADER_VMID(vmid));\r\namdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr) & 0xffffffe0);\r\namdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\r\namdgpu_ring_write(ring, ib->length_dw);\r\namdgpu_ring_write(ring, 0);\r\namdgpu_ring_write(ring, 0);\r\n}\r\nstatic void sdma_v2_4_ring_emit_hdp_flush(struct amdgpu_ring *ring)\r\n{\r\nu32 ref_and_mask = 0;\r\nif (ring == &ring->adev->sdma.instance[0].ring)\r\nref_and_mask = REG_SET_FIELD(ref_and_mask, GPU_HDP_FLUSH_DONE, SDMA0, 1);\r\nelse\r\nref_and_mask = REG_SET_FIELD(ref_and_mask, GPU_HDP_FLUSH_DONE, SDMA1, 1);\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_POLL_REGMEM) |\r\nSDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(1) |\r\nSDMA_PKT_POLL_REGMEM_HEADER_FUNC(3));\r\namdgpu_ring_write(ring, mmGPU_HDP_FLUSH_DONE << 2);\r\namdgpu_ring_write(ring, mmGPU_HDP_FLUSH_REQ << 2);\r\namdgpu_ring_write(ring, ref_and_mask);\r\namdgpu_ring_write(ring, ref_and_mask);\r\namdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\r\nSDMA_PKT_POLL_REGMEM_DW5_INTERVAL(10));\r\n}\r\nstatic void sdma_v2_4_ring_emit_hdp_invalidate(struct amdgpu_ring *ring)\r\n{\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_SRBM_WRITE) |\r\nSDMA_PKT_SRBM_WRITE_HEADER_BYTE_EN(0xf));\r\namdgpu_ring_write(ring, mmHDP_DEBUG0);\r\namdgpu_ring_write(ring, 1);\r\n}\r\nstatic void sdma_v2_4_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\r\nunsigned flags)\r\n{\r\nbool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_FENCE));\r\namdgpu_ring_write(ring, lower_32_bits(addr));\r\namdgpu_ring_write(ring, upper_32_bits(addr));\r\namdgpu_ring_write(ring, lower_32_bits(seq));\r\nif (write64bit) {\r\naddr += 4;\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_FENCE));\r\namdgpu_ring_write(ring, lower_32_bits(addr));\r\namdgpu_ring_write(ring, upper_32_bits(addr));\r\namdgpu_ring_write(ring, upper_32_bits(seq));\r\n}\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_TRAP));\r\namdgpu_ring_write(ring, SDMA_PKT_TRAP_INT_CONTEXT_INT_CONTEXT(0));\r\n}\r\nstatic void sdma_v2_4_gfx_stop(struct amdgpu_device *adev)\r\n{\r\nstruct amdgpu_ring *sdma0 = &adev->sdma.instance[0].ring;\r\nstruct amdgpu_ring *sdma1 = &adev->sdma.instance[1].ring;\r\nu32 rb_cntl, ib_cntl;\r\nint i;\r\nif ((adev->mman.buffer_funcs_ring == sdma0) ||\r\n(adev->mman.buffer_funcs_ring == sdma1))\r\namdgpu_ttm_set_active_vram_size(adev, adev->mc.visible_vram_size);\r\nfor (i = 0; i < adev->sdma.num_instances; i++) {\r\nrb_cntl = RREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i]);\r\nrb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_ENABLE, 0);\r\nWREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i], rb_cntl);\r\nib_cntl = RREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i]);\r\nib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_GFX_IB_CNTL, IB_ENABLE, 0);\r\nWREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i], ib_cntl);\r\n}\r\nsdma0->ready = false;\r\nsdma1->ready = false;\r\n}\r\nstatic void sdma_v2_4_rlc_stop(struct amdgpu_device *adev)\r\n{\r\n}\r\nstatic void sdma_v2_4_enable(struct amdgpu_device *adev, bool enable)\r\n{\r\nu32 f32_cntl;\r\nint i;\r\nif (!enable) {\r\nsdma_v2_4_gfx_stop(adev);\r\nsdma_v2_4_rlc_stop(adev);\r\n}\r\nfor (i = 0; i < adev->sdma.num_instances; i++) {\r\nf32_cntl = RREG32(mmSDMA0_F32_CNTL + sdma_offsets[i]);\r\nif (enable)\r\nf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_F32_CNTL, HALT, 0);\r\nelse\r\nf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_F32_CNTL, HALT, 1);\r\nWREG32(mmSDMA0_F32_CNTL + sdma_offsets[i], f32_cntl);\r\n}\r\n}\r\nstatic int sdma_v2_4_gfx_resume(struct amdgpu_device *adev)\r\n{\r\nstruct amdgpu_ring *ring;\r\nu32 rb_cntl, ib_cntl;\r\nu32 rb_bufsz;\r\nu32 wb_offset;\r\nint i, j, r;\r\nfor (i = 0; i < adev->sdma.num_instances; i++) {\r\nring = &adev->sdma.instance[i].ring;\r\nwb_offset = (ring->rptr_offs * 4);\r\nmutex_lock(&adev->srbm_mutex);\r\nfor (j = 0; j < 16; j++) {\r\nvi_srbm_select(adev, 0, 0, 0, j);\r\nWREG32(mmSDMA0_GFX_VIRTUAL_ADDR + sdma_offsets[i], 0);\r\nWREG32(mmSDMA0_GFX_APE1_CNTL + sdma_offsets[i], 0);\r\n}\r\nvi_srbm_select(adev, 0, 0, 0, 0);\r\nmutex_unlock(&adev->srbm_mutex);\r\nWREG32(mmSDMA0_TILING_CONFIG + sdma_offsets[i],\r\nadev->gfx.config.gb_addr_config & 0x70);\r\nWREG32(mmSDMA0_SEM_WAIT_FAIL_TIMER_CNTL + sdma_offsets[i], 0);\r\nrb_bufsz = order_base_2(ring->ring_size / 4);\r\nrb_cntl = RREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i]);\r\nrb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_SIZE, rb_bufsz);\r\n#ifdef __BIG_ENDIAN\r\nrb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_SWAP_ENABLE, 1);\r\nrb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL,\r\nRPTR_WRITEBACK_SWAP_ENABLE, 1);\r\n#endif\r\nWREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i], rb_cntl);\r\nWREG32(mmSDMA0_GFX_RB_RPTR + sdma_offsets[i], 0);\r\nWREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[i], 0);\r\nWREG32(mmSDMA0_GFX_IB_RPTR + sdma_offsets[i], 0);\r\nWREG32(mmSDMA0_GFX_IB_OFFSET + sdma_offsets[i], 0);\r\nWREG32(mmSDMA0_GFX_RB_RPTR_ADDR_HI + sdma_offsets[i],\r\nupper_32_bits(adev->wb.gpu_addr + wb_offset) & 0xFFFFFFFF);\r\nWREG32(mmSDMA0_GFX_RB_RPTR_ADDR_LO + sdma_offsets[i],\r\nlower_32_bits(adev->wb.gpu_addr + wb_offset) & 0xFFFFFFFC);\r\nrb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RPTR_WRITEBACK_ENABLE, 1);\r\nWREG32(mmSDMA0_GFX_RB_BASE + sdma_offsets[i], ring->gpu_addr >> 8);\r\nWREG32(mmSDMA0_GFX_RB_BASE_HI + sdma_offsets[i], ring->gpu_addr >> 40);\r\nring->wptr = 0;\r\nWREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[i], lower_32_bits(ring->wptr) << 2);\r\nrb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_ENABLE, 1);\r\nWREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i], rb_cntl);\r\nib_cntl = RREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i]);\r\nib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_GFX_IB_CNTL, IB_ENABLE, 1);\r\n#ifdef __BIG_ENDIAN\r\nib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_GFX_IB_CNTL, IB_SWAP_ENABLE, 1);\r\n#endif\r\nWREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i], ib_cntl);\r\nring->ready = true;\r\n}\r\nsdma_v2_4_enable(adev, true);\r\nfor (i = 0; i < adev->sdma.num_instances; i++) {\r\nring = &adev->sdma.instance[i].ring;\r\nr = amdgpu_ring_test_ring(ring);\r\nif (r) {\r\nring->ready = false;\r\nreturn r;\r\n}\r\nif (adev->mman.buffer_funcs_ring == ring)\r\namdgpu_ttm_set_active_vram_size(adev, adev->mc.real_vram_size);\r\n}\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_rlc_resume(struct amdgpu_device *adev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_load_microcode(struct amdgpu_device *adev)\r\n{\r\nconst struct sdma_firmware_header_v1_0 *hdr;\r\nconst __le32 *fw_data;\r\nu32 fw_size;\r\nint i, j;\r\nsdma_v2_4_enable(adev, false);\r\nfor (i = 0; i < adev->sdma.num_instances; i++) {\r\nif (!adev->sdma.instance[i].fw)\r\nreturn -EINVAL;\r\nhdr = (const struct sdma_firmware_header_v1_0 *)adev->sdma.instance[i].fw->data;\r\namdgpu_ucode_print_sdma_hdr(&hdr->header);\r\nfw_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\r\nfw_data = (const __le32 *)\r\n(adev->sdma.instance[i].fw->data +\r\nle32_to_cpu(hdr->header.ucode_array_offset_bytes));\r\nWREG32(mmSDMA0_UCODE_ADDR + sdma_offsets[i], 0);\r\nfor (j = 0; j < fw_size; j++)\r\nWREG32(mmSDMA0_UCODE_DATA + sdma_offsets[i], le32_to_cpup(fw_data++));\r\nWREG32(mmSDMA0_UCODE_ADDR + sdma_offsets[i], adev->sdma.instance[i].fw_version);\r\n}\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_start(struct amdgpu_device *adev)\r\n{\r\nint r;\r\nif (!adev->pp_enabled) {\r\nif (adev->firmware.load_type != AMDGPU_FW_LOAD_SMU) {\r\nr = sdma_v2_4_load_microcode(adev);\r\nif (r)\r\nreturn r;\r\n} else {\r\nr = adev->smu.smumgr_funcs->check_fw_load_finish(adev,\r\nAMDGPU_UCODE_ID_SDMA0);\r\nif (r)\r\nreturn -EINVAL;\r\nr = adev->smu.smumgr_funcs->check_fw_load_finish(adev,\r\nAMDGPU_UCODE_ID_SDMA1);\r\nif (r)\r\nreturn -EINVAL;\r\n}\r\n}\r\nsdma_v2_4_enable(adev, false);\r\nr = sdma_v2_4_gfx_resume(adev);\r\nif (r)\r\nreturn r;\r\nr = sdma_v2_4_rlc_resume(adev);\r\nif (r)\r\nreturn r;\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_ring_test_ring(struct amdgpu_ring *ring)\r\n{\r\nstruct amdgpu_device *adev = ring->adev;\r\nunsigned i;\r\nunsigned index;\r\nint r;\r\nu32 tmp;\r\nu64 gpu_addr;\r\nr = amdgpu_wb_get(adev, &index);\r\nif (r) {\r\ndev_err(adev->dev, "(%d) failed to allocate wb slot\n", r);\r\nreturn r;\r\n}\r\ngpu_addr = adev->wb.gpu_addr + (index * 4);\r\ntmp = 0xCAFEDEAD;\r\nadev->wb.wb[index] = cpu_to_le32(tmp);\r\nr = amdgpu_ring_alloc(ring, 5);\r\nif (r) {\r\nDRM_ERROR("amdgpu: dma failed to lock ring %d (%d).\n", ring->idx, r);\r\namdgpu_wb_free(adev, index);\r\nreturn r;\r\n}\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\r\nSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR));\r\namdgpu_ring_write(ring, lower_32_bits(gpu_addr));\r\namdgpu_ring_write(ring, upper_32_bits(gpu_addr));\r\namdgpu_ring_write(ring, SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(1));\r\namdgpu_ring_write(ring, 0xDEADBEEF);\r\namdgpu_ring_commit(ring);\r\nfor (i = 0; i < adev->usec_timeout; i++) {\r\ntmp = le32_to_cpu(adev->wb.wb[index]);\r\nif (tmp == 0xDEADBEEF)\r\nbreak;\r\nDRM_UDELAY(1);\r\n}\r\nif (i < adev->usec_timeout) {\r\nDRM_INFO("ring test on %d succeeded in %d usecs\n", ring->idx, i);\r\n} else {\r\nDRM_ERROR("amdgpu: ring %d test failed (0x%08X)\n",\r\nring->idx, tmp);\r\nr = -EINVAL;\r\n}\r\namdgpu_wb_free(adev, index);\r\nreturn r;\r\n}\r\nstatic int sdma_v2_4_ring_test_ib(struct amdgpu_ring *ring, long timeout)\r\n{\r\nstruct amdgpu_device *adev = ring->adev;\r\nstruct amdgpu_ib ib;\r\nstruct dma_fence *f = NULL;\r\nunsigned index;\r\nu32 tmp = 0;\r\nu64 gpu_addr;\r\nlong r;\r\nr = amdgpu_wb_get(adev, &index);\r\nif (r) {\r\ndev_err(adev->dev, "(%ld) failed to allocate wb slot\n", r);\r\nreturn r;\r\n}\r\ngpu_addr = adev->wb.gpu_addr + (index * 4);\r\ntmp = 0xCAFEDEAD;\r\nadev->wb.wb[index] = cpu_to_le32(tmp);\r\nmemset(&ib, 0, sizeof(ib));\r\nr = amdgpu_ib_get(adev, NULL, 256, &ib);\r\nif (r) {\r\nDRM_ERROR("amdgpu: failed to get ib (%ld).\n", r);\r\ngoto err0;\r\n}\r\nib.ptr[0] = SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\r\nSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\r\nib.ptr[1] = lower_32_bits(gpu_addr);\r\nib.ptr[2] = upper_32_bits(gpu_addr);\r\nib.ptr[3] = SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(1);\r\nib.ptr[4] = 0xDEADBEEF;\r\nib.ptr[5] = SDMA_PKT_HEADER_OP(SDMA_OP_NOP);\r\nib.ptr[6] = SDMA_PKT_HEADER_OP(SDMA_OP_NOP);\r\nib.ptr[7] = SDMA_PKT_HEADER_OP(SDMA_OP_NOP);\r\nib.length_dw = 8;\r\nr = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);\r\nif (r)\r\ngoto err1;\r\nr = dma_fence_wait_timeout(f, false, timeout);\r\nif (r == 0) {\r\nDRM_ERROR("amdgpu: IB test timed out\n");\r\nr = -ETIMEDOUT;\r\ngoto err1;\r\n} else if (r < 0) {\r\nDRM_ERROR("amdgpu: fence wait failed (%ld).\n", r);\r\ngoto err1;\r\n}\r\ntmp = le32_to_cpu(adev->wb.wb[index]);\r\nif (tmp == 0xDEADBEEF) {\r\nDRM_INFO("ib test on ring %d succeeded\n", ring->idx);\r\nr = 0;\r\n} else {\r\nDRM_ERROR("amdgpu: ib test failed (0x%08X)\n", tmp);\r\nr = -EINVAL;\r\n}\r\nerr1:\r\namdgpu_ib_free(adev, &ib, NULL);\r\ndma_fence_put(f);\r\nerr0:\r\namdgpu_wb_free(adev, index);\r\nreturn r;\r\n}\r\nstatic void sdma_v2_4_vm_copy_pte(struct amdgpu_ib *ib,\r\nuint64_t pe, uint64_t src,\r\nunsigned count)\r\n{\r\nunsigned bytes = count * 8;\r\nib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_COPY) |\r\nSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR);\r\nib->ptr[ib->length_dw++] = bytes;\r\nib->ptr[ib->length_dw++] = 0;\r\nib->ptr[ib->length_dw++] = lower_32_bits(src);\r\nib->ptr[ib->length_dw++] = upper_32_bits(src);\r\nib->ptr[ib->length_dw++] = lower_32_bits(pe);\r\nib->ptr[ib->length_dw++] = upper_32_bits(pe);\r\n}\r\nstatic void sdma_v2_4_vm_write_pte(struct amdgpu_ib *ib, uint64_t pe,\r\nuint64_t value, unsigned count,\r\nuint32_t incr)\r\n{\r\nunsigned ndw = count * 2;\r\nib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\r\nSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\r\nib->ptr[ib->length_dw++] = pe;\r\nib->ptr[ib->length_dw++] = upper_32_bits(pe);\r\nib->ptr[ib->length_dw++] = ndw;\r\nfor (; ndw > 0; ndw -= 2) {\r\nib->ptr[ib->length_dw++] = lower_32_bits(value);\r\nib->ptr[ib->length_dw++] = upper_32_bits(value);\r\nvalue += incr;\r\n}\r\n}\r\nstatic void sdma_v2_4_vm_set_pte_pde(struct amdgpu_ib *ib, uint64_t pe,\r\nuint64_t addr, unsigned count,\r\nuint32_t incr, uint64_t flags)\r\n{\r\nib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_GEN_PTEPDE);\r\nib->ptr[ib->length_dw++] = lower_32_bits(pe);\r\nib->ptr[ib->length_dw++] = upper_32_bits(pe);\r\nib->ptr[ib->length_dw++] = lower_32_bits(flags);\r\nib->ptr[ib->length_dw++] = upper_32_bits(flags);\r\nib->ptr[ib->length_dw++] = lower_32_bits(addr);\r\nib->ptr[ib->length_dw++] = upper_32_bits(addr);\r\nib->ptr[ib->length_dw++] = incr;\r\nib->ptr[ib->length_dw++] = 0;\r\nib->ptr[ib->length_dw++] = count;\r\n}\r\nstatic void sdma_v2_4_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)\r\n{\r\nstruct amdgpu_sdma_instance *sdma = amdgpu_get_sdma_instance(ring);\r\nu32 pad_count;\r\nint i;\r\npad_count = (8 - (ib->length_dw & 0x7)) % 8;\r\nfor (i = 0; i < pad_count; i++)\r\nif (sdma && sdma->burst_nop && (i == 0))\r\nib->ptr[ib->length_dw++] =\r\nSDMA_PKT_HEADER_OP(SDMA_OP_NOP) |\r\nSDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);\r\nelse\r\nib->ptr[ib->length_dw++] =\r\nSDMA_PKT_HEADER_OP(SDMA_OP_NOP);\r\n}\r\nstatic void sdma_v2_4_ring_emit_pipeline_sync(struct amdgpu_ring *ring)\r\n{\r\nuint32_t seq = ring->fence_drv.sync_seq;\r\nuint64_t addr = ring->fence_drv.gpu_addr;\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_POLL_REGMEM) |\r\nSDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(0) |\r\nSDMA_PKT_POLL_REGMEM_HEADER_FUNC(3) |\r\nSDMA_PKT_POLL_REGMEM_HEADER_MEM_POLL(1));\r\namdgpu_ring_write(ring, addr & 0xfffffffc);\r\namdgpu_ring_write(ring, upper_32_bits(addr) & 0xffffffff);\r\namdgpu_ring_write(ring, seq);\r\namdgpu_ring_write(ring, 0xfffffff);\r\namdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\r\nSDMA_PKT_POLL_REGMEM_DW5_INTERVAL(4));\r\n}\r\nstatic void sdma_v2_4_ring_emit_vm_flush(struct amdgpu_ring *ring,\r\nunsigned vm_id, uint64_t pd_addr)\r\n{\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_SRBM_WRITE) |\r\nSDMA_PKT_SRBM_WRITE_HEADER_BYTE_EN(0xf));\r\nif (vm_id < 8) {\r\namdgpu_ring_write(ring, (mmVM_CONTEXT0_PAGE_TABLE_BASE_ADDR + vm_id));\r\n} else {\r\namdgpu_ring_write(ring, (mmVM_CONTEXT8_PAGE_TABLE_BASE_ADDR + vm_id - 8));\r\n}\r\namdgpu_ring_write(ring, pd_addr >> 12);\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_SRBM_WRITE) |\r\nSDMA_PKT_SRBM_WRITE_HEADER_BYTE_EN(0xf));\r\namdgpu_ring_write(ring, mmVM_INVALIDATE_REQUEST);\r\namdgpu_ring_write(ring, 1 << vm_id);\r\namdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_POLL_REGMEM) |\r\nSDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(0) |\r\nSDMA_PKT_POLL_REGMEM_HEADER_FUNC(0));\r\namdgpu_ring_write(ring, mmVM_INVALIDATE_REQUEST << 2);\r\namdgpu_ring_write(ring, 0);\r\namdgpu_ring_write(ring, 0);\r\namdgpu_ring_write(ring, 0);\r\namdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\r\nSDMA_PKT_POLL_REGMEM_DW5_INTERVAL(10));\r\n}\r\nstatic int sdma_v2_4_early_init(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nadev->sdma.num_instances = SDMA_MAX_INSTANCE;\r\nsdma_v2_4_set_ring_funcs(adev);\r\nsdma_v2_4_set_buffer_funcs(adev);\r\nsdma_v2_4_set_vm_pte_funcs(adev);\r\nsdma_v2_4_set_irq_funcs(adev);\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_sw_init(void *handle)\r\n{\r\nstruct amdgpu_ring *ring;\r\nint r, i;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nr = amdgpu_irq_add_id(adev, AMDGPU_IH_CLIENTID_LEGACY, 224,\r\n&adev->sdma.trap_irq);\r\nif (r)\r\nreturn r;\r\nr = amdgpu_irq_add_id(adev, AMDGPU_IH_CLIENTID_LEGACY, 241,\r\n&adev->sdma.illegal_inst_irq);\r\nif (r)\r\nreturn r;\r\nr = amdgpu_irq_add_id(adev, AMDGPU_IH_CLIENTID_LEGACY, 247,\r\n&adev->sdma.illegal_inst_irq);\r\nif (r)\r\nreturn r;\r\nr = sdma_v2_4_init_microcode(adev);\r\nif (r) {\r\nDRM_ERROR("Failed to load sdma firmware!\n");\r\nreturn r;\r\n}\r\nfor (i = 0; i < adev->sdma.num_instances; i++) {\r\nring = &adev->sdma.instance[i].ring;\r\nring->ring_obj = NULL;\r\nring->use_doorbell = false;\r\nsprintf(ring->name, "sdma%d", i);\r\nr = amdgpu_ring_init(adev, ring, 1024,\r\n&adev->sdma.trap_irq,\r\n(i == 0) ?\r\nAMDGPU_SDMA_IRQ_TRAP0 :\r\nAMDGPU_SDMA_IRQ_TRAP1);\r\nif (r)\r\nreturn r;\r\n}\r\nreturn r;\r\n}\r\nstatic int sdma_v2_4_sw_fini(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nint i;\r\nfor (i = 0; i < adev->sdma.num_instances; i++)\r\namdgpu_ring_fini(&adev->sdma.instance[i].ring);\r\nsdma_v2_4_free_microcode(adev);\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_hw_init(void *handle)\r\n{\r\nint r;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nsdma_v2_4_init_golden_registers(adev);\r\nr = sdma_v2_4_start(adev);\r\nif (r)\r\nreturn r;\r\nreturn r;\r\n}\r\nstatic int sdma_v2_4_hw_fini(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nsdma_v2_4_enable(adev, false);\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_suspend(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nreturn sdma_v2_4_hw_fini(adev);\r\n}\r\nstatic int sdma_v2_4_resume(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nreturn sdma_v2_4_hw_init(adev);\r\n}\r\nstatic bool sdma_v2_4_is_idle(void *handle)\r\n{\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nu32 tmp = RREG32(mmSRBM_STATUS2);\r\nif (tmp & (SRBM_STATUS2__SDMA_BUSY_MASK |\r\nSRBM_STATUS2__SDMA1_BUSY_MASK))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic int sdma_v2_4_wait_for_idle(void *handle)\r\n{\r\nunsigned i;\r\nu32 tmp;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nfor (i = 0; i < adev->usec_timeout; i++) {\r\ntmp = RREG32(mmSRBM_STATUS2) & (SRBM_STATUS2__SDMA_BUSY_MASK |\r\nSRBM_STATUS2__SDMA1_BUSY_MASK);\r\nif (!tmp)\r\nreturn 0;\r\nudelay(1);\r\n}\r\nreturn -ETIMEDOUT;\r\n}\r\nstatic int sdma_v2_4_soft_reset(void *handle)\r\n{\r\nu32 srbm_soft_reset = 0;\r\nstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\r\nu32 tmp = RREG32(mmSRBM_STATUS2);\r\nif (tmp & SRBM_STATUS2__SDMA_BUSY_MASK) {\r\ntmp = RREG32(mmSDMA0_F32_CNTL + SDMA0_REGISTER_OFFSET);\r\ntmp = REG_SET_FIELD(tmp, SDMA0_F32_CNTL, HALT, 0);\r\nWREG32(mmSDMA0_F32_CNTL + SDMA0_REGISTER_OFFSET, tmp);\r\nsrbm_soft_reset |= SRBM_SOFT_RESET__SOFT_RESET_SDMA_MASK;\r\n}\r\nif (tmp & SRBM_STATUS2__SDMA1_BUSY_MASK) {\r\ntmp = RREG32(mmSDMA0_F32_CNTL + SDMA1_REGISTER_OFFSET);\r\ntmp = REG_SET_FIELD(tmp, SDMA0_F32_CNTL, HALT, 0);\r\nWREG32(mmSDMA0_F32_CNTL + SDMA1_REGISTER_OFFSET, tmp);\r\nsrbm_soft_reset |= SRBM_SOFT_RESET__SOFT_RESET_SDMA1_MASK;\r\n}\r\nif (srbm_soft_reset) {\r\ntmp = RREG32(mmSRBM_SOFT_RESET);\r\ntmp |= srbm_soft_reset;\r\ndev_info(adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);\r\nWREG32(mmSRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(mmSRBM_SOFT_RESET);\r\nudelay(50);\r\ntmp &= ~srbm_soft_reset;\r\nWREG32(mmSRBM_SOFT_RESET, tmp);\r\ntmp = RREG32(mmSRBM_SOFT_RESET);\r\nudelay(50);\r\n}\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_set_trap_irq_state(struct amdgpu_device *adev,\r\nstruct amdgpu_irq_src *src,\r\nunsigned type,\r\nenum amdgpu_interrupt_state state)\r\n{\r\nu32 sdma_cntl;\r\nswitch (type) {\r\ncase AMDGPU_SDMA_IRQ_TRAP0:\r\nswitch (state) {\r\ncase AMDGPU_IRQ_STATE_DISABLE:\r\nsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET);\r\nsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE, 0);\r\nWREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET, sdma_cntl);\r\nbreak;\r\ncase AMDGPU_IRQ_STATE_ENABLE:\r\nsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET);\r\nsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE, 1);\r\nWREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET, sdma_cntl);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ncase AMDGPU_SDMA_IRQ_TRAP1:\r\nswitch (state) {\r\ncase AMDGPU_IRQ_STATE_DISABLE:\r\nsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET);\r\nsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE, 0);\r\nWREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET, sdma_cntl);\r\nbreak;\r\ncase AMDGPU_IRQ_STATE_ENABLE:\r\nsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET);\r\nsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE, 1);\r\nWREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET, sdma_cntl);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_process_trap_irq(struct amdgpu_device *adev,\r\nstruct amdgpu_irq_src *source,\r\nstruct amdgpu_iv_entry *entry)\r\n{\r\nu8 instance_id, queue_id;\r\ninstance_id = (entry->ring_id & 0x3) >> 0;\r\nqueue_id = (entry->ring_id & 0xc) >> 2;\r\nDRM_DEBUG("IH: SDMA trap\n");\r\nswitch (instance_id) {\r\ncase 0:\r\nswitch (queue_id) {\r\ncase 0:\r\namdgpu_fence_process(&adev->sdma.instance[0].ring);\r\nbreak;\r\ncase 1:\r\nbreak;\r\ncase 2:\r\nbreak;\r\n}\r\nbreak;\r\ncase 1:\r\nswitch (queue_id) {\r\ncase 0:\r\namdgpu_fence_process(&adev->sdma.instance[1].ring);\r\nbreak;\r\ncase 1:\r\nbreak;\r\ncase 2:\r\nbreak;\r\n}\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_process_illegal_inst_irq(struct amdgpu_device *adev,\r\nstruct amdgpu_irq_src *source,\r\nstruct amdgpu_iv_entry *entry)\r\n{\r\nDRM_ERROR("Illegal instruction in SDMA command stream\n");\r\nschedule_work(&adev->reset_work);\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_set_clockgating_state(void *handle,\r\nenum amd_clockgating_state state)\r\n{\r\nreturn 0;\r\n}\r\nstatic int sdma_v2_4_set_powergating_state(void *handle,\r\nenum amd_powergating_state state)\r\n{\r\nreturn 0;\r\n}\r\nstatic void sdma_v2_4_set_ring_funcs(struct amdgpu_device *adev)\r\n{\r\nint i;\r\nfor (i = 0; i < adev->sdma.num_instances; i++)\r\nadev->sdma.instance[i].ring.funcs = &sdma_v2_4_ring_funcs;\r\n}\r\nstatic void sdma_v2_4_set_irq_funcs(struct amdgpu_device *adev)\r\n{\r\nadev->sdma.trap_irq.num_types = AMDGPU_SDMA_IRQ_LAST;\r\nadev->sdma.trap_irq.funcs = &sdma_v2_4_trap_irq_funcs;\r\nadev->sdma.illegal_inst_irq.funcs = &sdma_v2_4_illegal_inst_irq_funcs;\r\n}\r\nstatic void sdma_v2_4_emit_copy_buffer(struct amdgpu_ib *ib,\r\nuint64_t src_offset,\r\nuint64_t dst_offset,\r\nuint32_t byte_count)\r\n{\r\nib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_COPY) |\r\nSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR);\r\nib->ptr[ib->length_dw++] = byte_count;\r\nib->ptr[ib->length_dw++] = 0;\r\nib->ptr[ib->length_dw++] = lower_32_bits(src_offset);\r\nib->ptr[ib->length_dw++] = upper_32_bits(src_offset);\r\nib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\r\nib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\r\n}\r\nstatic void sdma_v2_4_emit_fill_buffer(struct amdgpu_ib *ib,\r\nuint32_t src_data,\r\nuint64_t dst_offset,\r\nuint32_t byte_count)\r\n{\r\nib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_CONST_FILL);\r\nib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\r\nib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\r\nib->ptr[ib->length_dw++] = src_data;\r\nib->ptr[ib->length_dw++] = byte_count;\r\n}\r\nstatic void sdma_v2_4_set_buffer_funcs(struct amdgpu_device *adev)\r\n{\r\nif (adev->mman.buffer_funcs == NULL) {\r\nadev->mman.buffer_funcs = &sdma_v2_4_buffer_funcs;\r\nadev->mman.buffer_funcs_ring = &adev->sdma.instance[0].ring;\r\n}\r\n}\r\nstatic void sdma_v2_4_set_vm_pte_funcs(struct amdgpu_device *adev)\r\n{\r\nunsigned i;\r\nif (adev->vm_manager.vm_pte_funcs == NULL) {\r\nadev->vm_manager.vm_pte_funcs = &sdma_v2_4_vm_pte_funcs;\r\nfor (i = 0; i < adev->sdma.num_instances; i++)\r\nadev->vm_manager.vm_pte_rings[i] =\r\n&adev->sdma.instance[i].ring;\r\nadev->vm_manager.vm_pte_num_rings = adev->sdma.num_instances;\r\n}\r\n}
