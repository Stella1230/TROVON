static struct scatterlist *sg_get(struct scatterlist *sg_list, unsigned int nents,\r\nunsigned int index)\r\n{\r\nstruct scatterlist *sg = NULL;\r\nint i;\r\nfor_each_sg(sg_list, sg, nents, i)\r\nif (i == index)\r\nbreak;\r\nreturn sg;\r\n}\r\nstatic int bfin_crypto_crc_init_hw(struct bfin_crypto_crc *crc, u32 key)\r\n{\r\nwritel(0, &crc->regs->datacntrld);\r\nwritel(MODE_CALC_CRC << OPMODE_OFFSET, &crc->regs->control);\r\nwritel(key, &crc->regs->curresult);\r\nwritel(CMPERRI | DCNTEXPI, &crc->regs->status);\r\nwritel(CMPERRI | DCNTEXPI, &crc->regs->intrenset);\r\nreturn 0;\r\n}\r\nstatic int bfin_crypto_crc_init(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct bfin_crypto_crc_ctx *crc_ctx = crypto_ahash_ctx(tfm);\r\nstruct bfin_crypto_crc_reqctx *ctx = ahash_request_ctx(req);\r\nstruct bfin_crypto_crc *crc;\r\ndev_dbg(ctx->crc->dev, "crc_init\n");\r\nspin_lock_bh(&crc_list.lock);\r\nlist_for_each_entry(crc, &crc_list.dev_list, list) {\r\ncrc_ctx->crc = crc;\r\nbreak;\r\n}\r\nspin_unlock_bh(&crc_list.lock);\r\nif (sg_nents(req->src) > CRC_MAX_DMA_DESC) {\r\ndev_dbg(ctx->crc->dev, "init: requested sg list is too big > %d\n",\r\nCRC_MAX_DMA_DESC);\r\nreturn -EINVAL;\r\n}\r\nctx->crc = crc;\r\nctx->bufnext_len = 0;\r\nctx->buflast_len = 0;\r\nctx->sg_buflen = 0;\r\nctx->total = 0;\r\nctx->flag = 0;\r\nput_unaligned_le32(crc_ctx->key, req->result);\r\ndev_dbg(ctx->crc->dev, "init: digest size: %d\n",\r\ncrypto_ahash_digestsize(tfm));\r\nreturn bfin_crypto_crc_init_hw(crc, crc_ctx->key);\r\n}\r\nstatic void bfin_crypto_crc_config_dma(struct bfin_crypto_crc *crc)\r\n{\r\nstruct scatterlist *sg;\r\nstruct bfin_crypto_crc_reqctx *ctx = ahash_request_ctx(crc->req);\r\nint i = 0, j = 0;\r\nunsigned long dma_config;\r\nunsigned int dma_count;\r\nunsigned int dma_addr;\r\nunsigned int mid_dma_count = 0;\r\nint dma_mod;\r\ndma_map_sg(crc->dev, ctx->sg, ctx->sg_nents, DMA_TO_DEVICE);\r\nfor_each_sg(ctx->sg, sg, ctx->sg_nents, j) {\r\ndma_addr = sg_dma_address(sg);\r\nif (sg_is_last(sg))\r\ndma_count = sg_dma_len(sg) - ctx->bufnext_len;\r\nelse\r\ndma_count = sg_dma_len(sg);\r\nif (mid_dma_count) {\r\nmemcpy(crc->sg_mid_buf +(i << 2) + mid_dma_count,\r\nsg_virt(sg),\r\nCHKSUM_DIGEST_SIZE - mid_dma_count);\r\ndma_addr += CHKSUM_DIGEST_SIZE - mid_dma_count;\r\ndma_count -= CHKSUM_DIGEST_SIZE - mid_dma_count;\r\ndma_config = DMAFLOW_ARRAY | RESTART | NDSIZE_3 |\r\nDMAEN | PSIZE_32 | WDSIZE_32;\r\ncrc->sg_cpu[i].start_addr = crc->sg_mid_dma + (i << 2);\r\ncrc->sg_cpu[i].cfg = dma_config;\r\ncrc->sg_cpu[i].x_count = 1;\r\ncrc->sg_cpu[i].x_modify = CHKSUM_DIGEST_SIZE;\r\ndev_dbg(crc->dev, "%d: crc_dma: start_addr:0x%lx, "\r\n"cfg:0x%x, x_count:0x%x, x_modify:0x%x\n",\r\ni, crc->sg_cpu[i].start_addr,\r\ncrc->sg_cpu[i].cfg, crc->sg_cpu[i].x_count,\r\ncrc->sg_cpu[i].x_modify);\r\ni++;\r\n}\r\ndma_config = DMAFLOW_ARRAY | RESTART | NDSIZE_3 | DMAEN | PSIZE_32;\r\nmid_dma_count = dma_count % 4;\r\ndma_count &= ~0x3;\r\nif (dma_addr % 4 == 0) {\r\ndma_config |= WDSIZE_32;\r\ndma_count >>= 2;\r\ndma_mod = 4;\r\n} else if (dma_addr % 2 == 0) {\r\ndma_config |= WDSIZE_16;\r\ndma_count >>= 1;\r\ndma_mod = 2;\r\n} else {\r\ndma_config |= WDSIZE_8;\r\ndma_mod = 1;\r\n}\r\ncrc->sg_cpu[i].start_addr = dma_addr;\r\ncrc->sg_cpu[i].cfg = dma_config;\r\ncrc->sg_cpu[i].x_count = dma_count;\r\ncrc->sg_cpu[i].x_modify = dma_mod;\r\ndev_dbg(crc->dev, "%d: crc_dma: start_addr:0x%lx, "\r\n"cfg:0x%x, x_count:0x%x, x_modify:0x%x\n",\r\ni, crc->sg_cpu[i].start_addr,\r\ncrc->sg_cpu[i].cfg, crc->sg_cpu[i].x_count,\r\ncrc->sg_cpu[i].x_modify);\r\ni++;\r\nif (mid_dma_count) {\r\nmemcpy(crc->sg_mid_buf + (i << 2),\r\n(u8*)sg_virt(sg) + (dma_count << 2),\r\nmid_dma_count);\r\n}\r\n}\r\ndma_config = DMAFLOW_ARRAY | RESTART | NDSIZE_3 | DMAEN | PSIZE_32 | WDSIZE_32;\r\nif (ctx->bufnext_len && (ctx->flag == CRC_CRYPTO_STATE_FINALUPDATE ||\r\nctx->flag == CRC_CRYPTO_STATE_FINISH)) {\r\ncrc->sg_cpu[i].start_addr = dma_map_single(crc->dev, ctx->bufnext,\r\nCHKSUM_DIGEST_SIZE, DMA_TO_DEVICE);\r\ncrc->sg_cpu[i].cfg = dma_config;\r\ncrc->sg_cpu[i].x_count = 1;\r\ncrc->sg_cpu[i].x_modify = CHKSUM_DIGEST_SIZE;\r\ndev_dbg(crc->dev, "%d: crc_dma: start_addr:0x%lx, "\r\n"cfg:0x%x, x_count:0x%x, x_modify:0x%x\n",\r\ni, crc->sg_cpu[i].start_addr,\r\ncrc->sg_cpu[i].cfg, crc->sg_cpu[i].x_count,\r\ncrc->sg_cpu[i].x_modify);\r\ni++;\r\n}\r\nif (i == 0)\r\nreturn;\r\ncrc->sg_cpu[i - 1].cfg &= ~(DMAFLOW | NDSIZE);\r\ncrc->sg_cpu[i - 1].cfg |= DI_EN;\r\nset_dma_curr_desc_addr(crc->dma_ch, (unsigned long *)crc->sg_dma);\r\nset_dma_x_count(crc->dma_ch, 0);\r\nset_dma_x_modify(crc->dma_ch, 0);\r\nset_dma_config(crc->dma_ch, dma_config);\r\n}\r\nstatic int bfin_crypto_crc_handle_queue(struct bfin_crypto_crc *crc,\r\nstruct ahash_request *req)\r\n{\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct bfin_crypto_crc_reqctx *ctx;\r\nstruct scatterlist *sg;\r\nint ret = 0;\r\nint nsg, i, j;\r\nunsigned int nextlen;\r\nunsigned long flags;\r\nu32 reg;\r\nspin_lock_irqsave(&crc->lock, flags);\r\nif (req)\r\nret = ahash_enqueue_request(&crc->queue, req);\r\nif (crc->busy) {\r\nspin_unlock_irqrestore(&crc->lock, flags);\r\nreturn ret;\r\n}\r\nbacklog = crypto_get_backlog(&crc->queue);\r\nasync_req = crypto_dequeue_request(&crc->queue);\r\nif (async_req)\r\ncrc->busy = 1;\r\nspin_unlock_irqrestore(&crc->lock, flags);\r\nif (!async_req)\r\nreturn ret;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ahash_request_cast(async_req);\r\ncrc->req = req;\r\nctx = ahash_request_ctx(req);\r\nctx->sg = NULL;\r\nctx->sg_buflen = 0;\r\nctx->sg_nents = 0;\r\ndev_dbg(crc->dev, "handling new req, flag=%u, nbytes: %d\n",\r\nctx->flag, req->nbytes);\r\nif (ctx->flag == CRC_CRYPTO_STATE_FINISH) {\r\nif (ctx->bufnext_len == 0) {\r\ncrc->busy = 0;\r\nreturn 0;\r\n}\r\nmemset(ctx->bufnext + ctx->bufnext_len, 0,\r\nCHKSUM_DIGEST_SIZE - ctx->bufnext_len);\r\n} else {\r\nif (ctx->bufnext_len + req->nbytes < CHKSUM_DIGEST_SIZE) {\r\nmemcpy(ctx->bufnext + ctx->bufnext_len,\r\nsg_virt(req->src), req->nbytes);\r\nctx->bufnext_len += req->nbytes;\r\nif (ctx->flag == CRC_CRYPTO_STATE_FINALUPDATE &&\r\nctx->bufnext_len) {\r\ngoto finish_update;\r\n} else {\r\ncrc->busy = 0;\r\nreturn 0;\r\n}\r\n}\r\nif (ctx->bufnext_len) {\r\nctx->buflast_len = ctx->bufnext_len;\r\nmemcpy(ctx->buflast, ctx->bufnext, ctx->buflast_len);\r\nnsg = ctx->sg_buflen ? 2 : 1;\r\nsg_init_table(ctx->bufsl, nsg);\r\nsg_set_buf(ctx->bufsl, ctx->buflast, ctx->buflast_len);\r\nif (nsg > 1)\r\nsg_chain(ctx->bufsl, nsg, req->src);\r\nctx->sg = ctx->bufsl;\r\n} else\r\nctx->sg = req->src;\r\nnsg = sg_nents(ctx->sg);\r\nctx->sg_nents = nsg;\r\nctx->sg_buflen = ctx->buflast_len + req->nbytes;\r\nctx->bufnext_len = ctx->sg_buflen % 4;\r\nctx->sg_buflen &= ~0x3;\r\nif (ctx->bufnext_len) {\r\nmemset(ctx->bufnext, 0, CHKSUM_DIGEST_SIZE);\r\nnextlen = ctx->bufnext_len;\r\nfor (i = nsg - 1; i >= 0; i--) {\r\nsg = sg_get(ctx->sg, nsg, i);\r\nj = min(nextlen, sg_dma_len(sg));\r\nmemcpy(ctx->bufnext + nextlen - j,\r\nsg_virt(sg) + sg_dma_len(sg) - j, j);\r\nif (j == sg_dma_len(sg))\r\nctx->sg_nents--;\r\nnextlen -= j;\r\nif (nextlen == 0)\r\nbreak;\r\n}\r\n}\r\n}\r\nfinish_update:\r\nif (ctx->bufnext_len && (ctx->flag == CRC_CRYPTO_STATE_FINALUPDATE ||\r\nctx->flag == CRC_CRYPTO_STATE_FINISH))\r\nctx->sg_buflen += CHKSUM_DIGEST_SIZE;\r\nwritel(ctx->sg_buflen >> 2, &crc->regs->datacnt);\r\nbfin_crypto_crc_config_dma(crc);\r\nreg = readl(&crc->regs->control);\r\nwritel(reg | BLKEN, &crc->regs->control);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int bfin_crypto_crc_update(struct ahash_request *req)\r\n{\r\nstruct bfin_crypto_crc_reqctx *ctx = ahash_request_ctx(req);\r\nif (!req->nbytes)\r\nreturn 0;\r\ndev_dbg(ctx->crc->dev, "crc_update\n");\r\nctx->total += req->nbytes;\r\nctx->flag = CRC_CRYPTO_STATE_UPDATE;\r\nreturn bfin_crypto_crc_handle_queue(ctx->crc, req);\r\n}\r\nstatic int bfin_crypto_crc_final(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct bfin_crypto_crc_ctx *crc_ctx = crypto_ahash_ctx(tfm);\r\nstruct bfin_crypto_crc_reqctx *ctx = ahash_request_ctx(req);\r\ndev_dbg(ctx->crc->dev, "crc_final\n");\r\nctx->flag = CRC_CRYPTO_STATE_FINISH;\r\ncrc_ctx->key = 0;\r\nreturn bfin_crypto_crc_handle_queue(ctx->crc, req);\r\n}\r\nstatic int bfin_crypto_crc_finup(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct bfin_crypto_crc_ctx *crc_ctx = crypto_ahash_ctx(tfm);\r\nstruct bfin_crypto_crc_reqctx *ctx = ahash_request_ctx(req);\r\ndev_dbg(ctx->crc->dev, "crc_finishupdate\n");\r\nctx->total += req->nbytes;\r\nctx->flag = CRC_CRYPTO_STATE_FINALUPDATE;\r\ncrc_ctx->key = 0;\r\nreturn bfin_crypto_crc_handle_queue(ctx->crc, req);\r\n}\r\nstatic int bfin_crypto_crc_digest(struct ahash_request *req)\r\n{\r\nint ret;\r\nret = bfin_crypto_crc_init(req);\r\nif (ret)\r\nreturn ret;\r\nreturn bfin_crypto_crc_finup(req);\r\n}\r\nstatic int bfin_crypto_crc_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct bfin_crypto_crc_ctx *crc_ctx = crypto_ahash_ctx(tfm);\r\ndev_dbg(crc_ctx->crc->dev, "crc_setkey\n");\r\nif (keylen != CHKSUM_DIGEST_SIZE) {\r\ncrypto_ahash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\ncrc_ctx->key = get_unaligned_le32(key);\r\nreturn 0;\r\n}\r\nstatic int bfin_crypto_crc_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct bfin_crypto_crc_ctx *crc_ctx = crypto_tfm_ctx(tfm);\r\ncrc_ctx->key = 0;\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct bfin_crypto_crc_reqctx));\r\nreturn 0;\r\n}\r\nstatic void bfin_crypto_crc_cra_exit(struct crypto_tfm *tfm)\r\n{\r\n}\r\nstatic void bfin_crypto_crc_done_task(unsigned long data)\r\n{\r\nstruct bfin_crypto_crc *crc = (struct bfin_crypto_crc *)data;\r\nbfin_crypto_crc_handle_queue(crc, NULL);\r\n}\r\nstatic irqreturn_t bfin_crypto_crc_handler(int irq, void *dev_id)\r\n{\r\nstruct bfin_crypto_crc *crc = dev_id;\r\nu32 reg;\r\nif (readl(&crc->regs->status) & DCNTEXP) {\r\nwritel(DCNTEXP, &crc->regs->status);\r\nput_unaligned_le32(readl(&crc->regs->result),\r\ncrc->req->result);\r\nreg = readl(&crc->regs->control);\r\nwritel(reg & ~BLKEN, &crc->regs->control);\r\ncrc->busy = 0;\r\nif (crc->req->base.complete)\r\ncrc->req->base.complete(&crc->req->base, 0);\r\ntasklet_schedule(&crc->done_task);\r\nreturn IRQ_HANDLED;\r\n} else\r\nreturn IRQ_NONE;\r\n}\r\nstatic int bfin_crypto_crc_suspend(struct platform_device *pdev, pm_message_t state)\r\n{\r\nstruct bfin_crypto_crc *crc = platform_get_drvdata(pdev);\r\nint i = 100000;\r\nwhile ((readl(&crc->regs->control) & BLKEN) && --i)\r\ncpu_relax();\r\nif (i == 0)\r\nreturn -EBUSY;\r\nreturn 0;\r\n}\r\nstatic int bfin_crypto_crc_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct resource *res;\r\nstruct bfin_crypto_crc *crc;\r\nunsigned int timeout = 100000;\r\nint ret;\r\ncrc = devm_kzalloc(dev, sizeof(*crc), GFP_KERNEL);\r\nif (!crc) {\r\ndev_err(&pdev->dev, "fail to malloc bfin_crypto_crc\n");\r\nreturn -ENOMEM;\r\n}\r\ncrc->dev = dev;\r\nINIT_LIST_HEAD(&crc->list);\r\nspin_lock_init(&crc->lock);\r\ntasklet_init(&crc->done_task, bfin_crypto_crc_done_task, (unsigned long)crc);\r\ncrypto_init_queue(&crc->queue, CRC_CCRYPTO_QUEUE_LENGTH);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ncrc->regs = devm_ioremap_resource(dev, res);\r\nif (IS_ERR((void *)crc->regs)) {\r\ndev_err(&pdev->dev, "Cannot map CRC IO\n");\r\nreturn PTR_ERR((void *)crc->regs);\r\n}\r\ncrc->irq = platform_get_irq(pdev, 0);\r\nif (crc->irq < 0) {\r\ndev_err(&pdev->dev, "No CRC DCNTEXP IRQ specified\n");\r\nreturn -ENOENT;\r\n}\r\nret = devm_request_irq(dev, crc->irq, bfin_crypto_crc_handler,\r\nIRQF_SHARED, dev_name(dev), crc);\r\nif (ret) {\r\ndev_err(&pdev->dev, "Unable to request blackfin crc irq\n");\r\nreturn ret;\r\n}\r\nres = platform_get_resource(pdev, IORESOURCE_DMA, 0);\r\nif (res == NULL) {\r\ndev_err(&pdev->dev, "No CRC DMA channel specified\n");\r\nreturn -ENOENT;\r\n}\r\ncrc->dma_ch = res->start;\r\nret = request_dma(crc->dma_ch, dev_name(dev));\r\nif (ret) {\r\ndev_err(&pdev->dev, "Unable to attach Blackfin CRC DMA channel\n");\r\nreturn ret;\r\n}\r\ncrc->sg_cpu = dma_alloc_coherent(&pdev->dev, PAGE_SIZE, &crc->sg_dma, GFP_KERNEL);\r\nif (crc->sg_cpu == NULL) {\r\nret = -ENOMEM;\r\ngoto out_error_dma;\r\n}\r\ncrc->sg_mid_buf = (u8 *)(crc->sg_cpu + ((CRC_MAX_DMA_DESC + 1) << 1));\r\ncrc->sg_mid_dma = crc->sg_dma + sizeof(struct dma_desc_array)\r\n* ((CRC_MAX_DMA_DESC + 1) << 1);\r\nwritel(0, &crc->regs->control);\r\ncrc->poly = (u32)pdev->dev.platform_data;\r\nwritel(crc->poly, &crc->regs->poly);\r\nwhile (!(readl(&crc->regs->status) & LUTDONE) && (--timeout) > 0)\r\ncpu_relax();\r\nif (timeout == 0)\r\ndev_info(&pdev->dev, "init crc poly timeout\n");\r\nplatform_set_drvdata(pdev, crc);\r\nspin_lock(&crc_list.lock);\r\nlist_add(&crc->list, &crc_list.dev_list);\r\nspin_unlock(&crc_list.lock);\r\nif (list_is_singular(&crc_list.dev_list)) {\r\nret = crypto_register_ahash(&algs);\r\nif (ret) {\r\ndev_err(&pdev->dev,\r\n"Can't register crypto ahash device\n");\r\ngoto out_error_dma;\r\n}\r\n}\r\ndev_info(&pdev->dev, "initialized\n");\r\nreturn 0;\r\nout_error_dma:\r\nif (crc->sg_cpu)\r\ndma_free_coherent(&pdev->dev, PAGE_SIZE, crc->sg_cpu, crc->sg_dma);\r\nfree_dma(crc->dma_ch);\r\nreturn ret;\r\n}\r\nstatic int bfin_crypto_crc_remove(struct platform_device *pdev)\r\n{\r\nstruct bfin_crypto_crc *crc = platform_get_drvdata(pdev);\r\nif (!crc)\r\nreturn -ENODEV;\r\nspin_lock(&crc_list.lock);\r\nlist_del(&crc->list);\r\nspin_unlock(&crc_list.lock);\r\ncrypto_unregister_ahash(&algs);\r\ntasklet_kill(&crc->done_task);\r\nfree_dma(crc->dma_ch);\r\nreturn 0;\r\n}\r\nstatic int __init bfin_crypto_crc_mod_init(void)\r\n{\r\nint ret;\r\npr_info("Blackfin hardware CRC crypto driver\n");\r\nINIT_LIST_HEAD(&crc_list.dev_list);\r\nspin_lock_init(&crc_list.lock);\r\nret = platform_driver_register(&bfin_crypto_crc_driver);\r\nif (ret) {\r\npr_err("unable to register driver\n");\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __exit bfin_crypto_crc_mod_exit(void)\r\n{\r\nplatform_driver_unregister(&bfin_crypto_crc_driver);\r\n}
