static void __iomem *__ioremap(phys_addr_t addr, unsigned long size,\r\nunsigned long flags)\r\n{\r\nunsigned long v, i;\r\nphys_addr_t p;\r\nint err;\r\np = addr & PAGE_MASK;\r\nsize = PAGE_ALIGN(addr + size) - p;\r\nif (mem_init_done &&\r\np >= memory_start && p < virt_to_phys(high_memory) &&\r\n!(p >= __virt_to_phys((phys_addr_t)__bss_stop) &&\r\np < __virt_to_phys((phys_addr_t)__bss_stop))) {\r\npr_warn("__ioremap(): phys addr "PTE_FMT" is RAM lr %pf\n",\r\n(unsigned long)p, __builtin_return_address(0));\r\nreturn NULL;\r\n}\r\nif (size == 0)\r\nreturn NULL;\r\nif (mem_init_done) {\r\nstruct vm_struct *area;\r\narea = get_vm_area(size, VM_IOREMAP);\r\nif (area == NULL)\r\nreturn NULL;\r\nv = (unsigned long) area->addr;\r\n} else {\r\nv = (ioremap_bot -= size);\r\n}\r\nif ((flags & _PAGE_PRESENT) == 0)\r\nflags |= _PAGE_KERNEL;\r\nif (flags & _PAGE_NO_CACHE)\r\nflags |= _PAGE_GUARDED;\r\nerr = 0;\r\nfor (i = 0; i < size && err == 0; i += PAGE_SIZE)\r\nerr = map_page(v + i, p + i, flags);\r\nif (err) {\r\nif (mem_init_done)\r\nvfree((void *)v);\r\nreturn NULL;\r\n}\r\nreturn (void __iomem *) (v + ((unsigned long)addr & ~PAGE_MASK));\r\n}\r\nvoid __iomem *ioremap(phys_addr_t addr, unsigned long size)\r\n{\r\nreturn __ioremap(addr, size, _PAGE_NO_CACHE);\r\n}\r\nvoid iounmap(void __iomem *addr)\r\n{\r\nif ((__force void *)addr > high_memory &&\r\n(unsigned long) addr < ioremap_bot)\r\nvfree((void *) (PAGE_MASK & (unsigned long) addr));\r\n}\r\nint map_page(unsigned long va, phys_addr_t pa, int flags)\r\n{\r\npmd_t *pd;\r\npte_t *pg;\r\nint err = -ENOMEM;\r\npd = pmd_offset(pgd_offset_k(va), va);\r\npg = pte_alloc_kernel(pd, va);\r\nif (pg != NULL) {\r\nerr = 0;\r\nset_pte_at(&init_mm, va, pg, pfn_pte(pa >> PAGE_SHIFT,\r\n__pgprot(flags)));\r\nif (unlikely(mem_init_done))\r\n_tlbie(va);\r\n}\r\nreturn err;\r\n}\r\nvoid __init mapin_ram(void)\r\n{\r\nunsigned long v, p, s, f;\r\nv = CONFIG_KERNEL_START;\r\np = memory_start;\r\nfor (s = 0; s < lowmem_size; s += PAGE_SIZE) {\r\nf = _PAGE_PRESENT | _PAGE_ACCESSED |\r\n_PAGE_SHARED | _PAGE_HWEXEC;\r\nif ((char *) v < _stext || (char *) v >= _etext)\r\nf |= _PAGE_WRENABLE;\r\nelse\r\nf |= _PAGE_USER;\r\nmap_page(v, p, f);\r\nv += PAGE_SIZE;\r\np += PAGE_SIZE;\r\n}\r\n}\r\nstatic int get_pteptr(struct mm_struct *mm, unsigned long addr, pte_t **ptep)\r\n{\r\npgd_t *pgd;\r\npmd_t *pmd;\r\npte_t *pte;\r\nint retval = 0;\r\npgd = pgd_offset(mm, addr & PAGE_MASK);\r\nif (pgd) {\r\npmd = pmd_offset(pgd, addr & PAGE_MASK);\r\nif (pmd_present(*pmd)) {\r\npte = pte_offset_kernel(pmd, addr & PAGE_MASK);\r\nif (pte) {\r\nretval = 1;\r\n*ptep = pte;\r\n}\r\n}\r\n}\r\nreturn retval;\r\n}\r\nunsigned long iopa(unsigned long addr)\r\n{\r\nunsigned long pa;\r\npte_t *pte;\r\nstruct mm_struct *mm;\r\nif (addr < TASK_SIZE)\r\nmm = current->mm;\r\nelse\r\nmm = &init_mm;\r\npa = 0;\r\nif (get_pteptr(mm, addr, &pte))\r\npa = (pte_val(*pte) & PAGE_MASK) | (addr & ~PAGE_MASK);\r\nreturn pa;\r\n}\r\n__ref pte_t *pte_alloc_one_kernel(struct mm_struct *mm,\r\nunsigned long address)\r\n{\r\npte_t *pte;\r\nif (mem_init_done) {\r\npte = (pte_t *)__get_free_page(GFP_KERNEL | __GFP_ZERO);\r\n} else {\r\npte = (pte_t *)early_get_page();\r\nif (pte)\r\nclear_page(pte);\r\n}\r\nreturn pte;\r\n}\r\nvoid __set_fixmap(enum fixed_addresses idx, phys_addr_t phys, pgprot_t flags)\r\n{\r\nunsigned long address = __fix_to_virt(idx);\r\nif (idx >= __end_of_fixed_addresses)\r\nBUG();\r\nmap_page(address, phys, pgprot_val(flags));\r\n}
