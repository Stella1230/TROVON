static unsigned int order_to_nr(unsigned int order)\r\n{\r\nreturn 1U << order;\r\n}\r\nstatic unsigned int nr_to_order_bottom(unsigned int nr)\r\n{\r\nreturn fls(nr) - 1;\r\n}\r\nstruct hmm_buffer_object *__bo_alloc(struct kmem_cache *bo_cache)\r\n{\r\nstruct hmm_buffer_object *bo;\r\nbo = kmem_cache_alloc(bo_cache, GFP_KERNEL);\r\nif (!bo)\r\ndev_err(atomisp_dev, "%s: failed!\n", __func__);\r\nreturn bo;\r\n}\r\nstatic int __bo_init(struct hmm_bo_device *bdev, struct hmm_buffer_object *bo,\r\nunsigned int pgnr)\r\n{\r\ncheck_bodev_null_return(bdev, -EINVAL);\r\nvar_equal_return(hmm_bo_device_inited(bdev), 0, -EINVAL,\r\n"hmm_bo_device not inited yet.\n");\r\nif (pgnr == 0) {\r\ndev_err(atomisp_dev, "0 size buffer is not allowed.\n");\r\nreturn -EINVAL;\r\n}\r\nmemset(bo, 0, sizeof(*bo));\r\nmutex_init(&bo->mutex);\r\nINIT_LIST_HEAD(&bo->list);\r\nbo->bdev = bdev;\r\nbo->vmap_addr = NULL;\r\nbo->status = HMM_BO_FREE;\r\nbo->start = bdev->start;\r\nbo->pgnr = pgnr;\r\nbo->end = bo->start + pgnr_to_size(pgnr);\r\nbo->prev = NULL;\r\nbo->next = NULL;\r\nreturn 0;\r\n}\r\nstruct hmm_buffer_object *__bo_search_and_remove_from_free_rbtree(\r\nstruct rb_node *node, unsigned int pgnr)\r\n{\r\nstruct hmm_buffer_object *this, *ret_bo, *temp_bo;\r\nthis = rb_entry(node, struct hmm_buffer_object, node);\r\nif (this->pgnr == pgnr ||\r\n(this->pgnr > pgnr && this->node.rb_left == NULL)) {\r\ngoto remove_bo_and_return;\r\n} else {\r\nif (this->pgnr < pgnr) {\r\nif (!this->node.rb_right)\r\nreturn NULL;\r\nret_bo = __bo_search_and_remove_from_free_rbtree(\r\nthis->node.rb_right, pgnr);\r\n} else {\r\nret_bo = __bo_search_and_remove_from_free_rbtree(\r\nthis->node.rb_left, pgnr);\r\n}\r\nif (!ret_bo) {\r\nif (this->pgnr > pgnr)\r\ngoto remove_bo_and_return;\r\nelse\r\nreturn NULL;\r\n}\r\nreturn ret_bo;\r\n}\r\nremove_bo_and_return:\r\nif (this->next == NULL) {\r\nrb_erase(&this->node, &this->bdev->free_rbtree);\r\nreturn this;\r\n}\r\ntemp_bo = this->next;\r\nthis->next = temp_bo->next;\r\nif (temp_bo->next)\r\ntemp_bo->next->prev = this;\r\ntemp_bo->next = NULL;\r\ntemp_bo->prev = NULL;\r\nreturn temp_bo;\r\n}\r\nstruct hmm_buffer_object *__bo_search_by_addr(struct rb_root *root,\r\nia_css_ptr start)\r\n{\r\nstruct rb_node *n = root->rb_node;\r\nstruct hmm_buffer_object *bo;\r\ndo {\r\nbo = rb_entry(n, struct hmm_buffer_object, node);\r\nif (bo->start > start) {\r\nif (n->rb_left == NULL)\r\nreturn NULL;\r\nn = n->rb_left;\r\n} else if (bo->start < start) {\r\nif (n->rb_right == NULL)\r\nreturn NULL;\r\nn = n->rb_right;\r\n} else {\r\nreturn bo;\r\n}\r\n} while (n);\r\nreturn NULL;\r\n}\r\nstruct hmm_buffer_object *__bo_search_by_addr_in_range(struct rb_root *root,\r\nunsigned int start)\r\n{\r\nstruct rb_node *n = root->rb_node;\r\nstruct hmm_buffer_object *bo;\r\ndo {\r\nbo = rb_entry(n, struct hmm_buffer_object, node);\r\nif (bo->start > start) {\r\nif (n->rb_left == NULL)\r\nreturn NULL;\r\nn = n->rb_left;\r\n} else {\r\nif (bo->end > start)\r\nreturn bo;\r\nif (n->rb_right == NULL)\r\nreturn NULL;\r\nn = n->rb_right;\r\n}\r\n} while (n);\r\nreturn NULL;\r\n}\r\nstatic void __bo_insert_to_free_rbtree(struct rb_root *root,\r\nstruct hmm_buffer_object *bo)\r\n{\r\nstruct rb_node **new = &(root->rb_node);\r\nstruct rb_node *parent = NULL;\r\nstruct hmm_buffer_object *this;\r\nunsigned int pgnr = bo->pgnr;\r\nwhile (*new) {\r\nparent = *new;\r\nthis = container_of(*new, struct hmm_buffer_object, node);\r\nif (pgnr < this->pgnr) {\r\nnew = &((*new)->rb_left);\r\n} else if (pgnr > this->pgnr) {\r\nnew = &((*new)->rb_right);\r\n} else {\r\nbo->prev = this;\r\nbo->next = this->next;\r\nif (this->next)\r\nthis->next->prev = bo;\r\nthis->next = bo;\r\nbo->status = (bo->status & ~HMM_BO_MASK) | HMM_BO_FREE;\r\nreturn;\r\n}\r\n}\r\nbo->status = (bo->status & ~HMM_BO_MASK) | HMM_BO_FREE;\r\nrb_link_node(&bo->node, parent, new);\r\nrb_insert_color(&bo->node, root);\r\n}\r\nstatic void __bo_insert_to_alloc_rbtree(struct rb_root *root,\r\nstruct hmm_buffer_object *bo)\r\n{\r\nstruct rb_node **new = &(root->rb_node);\r\nstruct rb_node *parent = NULL;\r\nstruct hmm_buffer_object *this;\r\nunsigned int start = bo->start;\r\nwhile (*new) {\r\nparent = *new;\r\nthis = container_of(*new, struct hmm_buffer_object, node);\r\nif (start < this->start)\r\nnew = &((*new)->rb_left);\r\nelse\r\nnew = &((*new)->rb_right);\r\n}\r\nkref_init(&bo->kref);\r\nbo->status = (bo->status & ~HMM_BO_MASK) | HMM_BO_ALLOCED;\r\nrb_link_node(&bo->node, parent, new);\r\nrb_insert_color(&bo->node, root);\r\n}\r\nstruct hmm_buffer_object *__bo_break_up(struct hmm_bo_device *bdev,\r\nstruct hmm_buffer_object *bo,\r\nunsigned int pgnr)\r\n{\r\nstruct hmm_buffer_object *new_bo;\r\nunsigned long flags;\r\nint ret;\r\nnew_bo = __bo_alloc(bdev->bo_cache);\r\nif (!new_bo) {\r\ndev_err(atomisp_dev, "%s: __bo_alloc failed!\n", __func__);\r\nreturn NULL;\r\n}\r\nret = __bo_init(bdev, new_bo, pgnr);\r\nif (ret) {\r\ndev_err(atomisp_dev, "%s: __bo_init failed!\n", __func__);\r\nkmem_cache_free(bdev->bo_cache, new_bo);\r\nreturn NULL;\r\n}\r\nnew_bo->start = bo->start;\r\nnew_bo->end = new_bo->start + pgnr_to_size(pgnr);\r\nbo->start = new_bo->end;\r\nbo->pgnr = bo->pgnr - pgnr;\r\nspin_lock_irqsave(&bdev->list_lock, flags);\r\nlist_add_tail(&new_bo->list, &bo->list);\r\nspin_unlock_irqrestore(&bdev->list_lock, flags);\r\nreturn new_bo;\r\n}\r\nstatic void __bo_take_off_handling(struct hmm_buffer_object *bo)\r\n{\r\nstruct hmm_bo_device *bdev = bo->bdev;\r\nif (bo->prev == NULL && bo->next == NULL) {\r\nrb_erase(&bo->node, &bdev->free_rbtree);\r\n} else if (bo->prev == NULL && bo->next != NULL) {\r\nbo->next->prev = NULL;\r\nrb_erase(&bo->node, &bdev->free_rbtree);\r\n__bo_insert_to_free_rbtree(&bdev->free_rbtree, bo->next);\r\nbo->next = NULL;\r\n} else if (bo->prev != NULL && bo->next == NULL) {\r\nbo->prev->next = NULL;\r\nbo->prev = NULL;\r\n} else {\r\nbo->next->prev = bo->prev;\r\nbo->prev->next = bo->next;\r\nbo->next = NULL;\r\nbo->prev = NULL;\r\n}\r\n}\r\nstruct hmm_buffer_object *__bo_merge(struct hmm_buffer_object *bo,\r\nstruct hmm_buffer_object *next_bo)\r\n{\r\nstruct hmm_bo_device *bdev;\r\nunsigned long flags;\r\nbdev = bo->bdev;\r\nnext_bo->start = bo->start;\r\nnext_bo->pgnr = next_bo->pgnr + bo->pgnr;\r\nspin_lock_irqsave(&bdev->list_lock, flags);\r\nlist_del(&bo->list);\r\nspin_unlock_irqrestore(&bdev->list_lock, flags);\r\nkmem_cache_free(bo->bdev->bo_cache, bo);\r\nreturn next_bo;\r\n}\r\nint hmm_bo_device_init(struct hmm_bo_device *bdev,\r\nstruct isp_mmu_client *mmu_driver,\r\nunsigned int vaddr_start,\r\nunsigned int size)\r\n{\r\nstruct hmm_buffer_object *bo;\r\nunsigned long flags;\r\nint ret;\r\ncheck_bodev_null_return(bdev, -EINVAL);\r\nret = isp_mmu_init(&bdev->mmu, mmu_driver);\r\nif (ret) {\r\ndev_err(atomisp_dev, "isp_mmu_init failed.\n");\r\nreturn ret;\r\n}\r\nbdev->start = vaddr_start;\r\nbdev->pgnr = size_to_pgnr_ceil(size);\r\nbdev->size = pgnr_to_size(bdev->pgnr);\r\nspin_lock_init(&bdev->list_lock);\r\nmutex_init(&bdev->rbtree_mutex);\r\nbdev->flag = HMM_BO_DEVICE_INITED;\r\nINIT_LIST_HEAD(&bdev->entire_bo_list);\r\nbdev->allocated_rbtree = RB_ROOT;\r\nbdev->free_rbtree = RB_ROOT;\r\nbdev->bo_cache = kmem_cache_create("bo_cache",\r\nsizeof(struct hmm_buffer_object), 0, 0, NULL);\r\nif (!bdev->bo_cache) {\r\ndev_err(atomisp_dev, "%s: create cache failed!\n", __func__);\r\nisp_mmu_exit(&bdev->mmu);\r\nreturn -ENOMEM;\r\n}\r\nbo = __bo_alloc(bdev->bo_cache);\r\nif (!bo) {\r\ndev_err(atomisp_dev, "%s: __bo_alloc failed!\n", __func__);\r\nisp_mmu_exit(&bdev->mmu);\r\nreturn -ENOMEM;\r\n}\r\nret = __bo_init(bdev, bo, bdev->pgnr);\r\nif (ret) {\r\ndev_err(atomisp_dev, "%s: __bo_init failed!\n", __func__);\r\nkmem_cache_free(bdev->bo_cache, bo);\r\nisp_mmu_exit(&bdev->mmu);\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&bdev->list_lock, flags);\r\nlist_add_tail(&bo->list, &bdev->entire_bo_list);\r\nspin_unlock_irqrestore(&bdev->list_lock, flags);\r\n__bo_insert_to_free_rbtree(&bdev->free_rbtree, bo);\r\nreturn 0;\r\n}\r\nstruct hmm_buffer_object *hmm_bo_alloc(struct hmm_bo_device *bdev,\r\nunsigned int pgnr)\r\n{\r\nstruct hmm_buffer_object *bo, *new_bo;\r\nstruct rb_root *root = &bdev->free_rbtree;\r\ncheck_bodev_null_return(bdev, NULL);\r\nvar_equal_return(hmm_bo_device_inited(bdev), 0, NULL,\r\n"hmm_bo_device not inited yet.\n");\r\nif (pgnr == 0) {\r\ndev_err(atomisp_dev, "0 size buffer is not allowed.\n");\r\nreturn NULL;\r\n}\r\nmutex_lock(&bdev->rbtree_mutex);\r\nbo = __bo_search_and_remove_from_free_rbtree(root->rb_node, pgnr);\r\nif (!bo) {\r\nmutex_unlock(&bdev->rbtree_mutex);\r\ndev_err(atomisp_dev, "%s: Out of Memory! hmm_bo_alloc failed",\r\n__func__);\r\nreturn NULL;\r\n}\r\nif (bo->pgnr > pgnr) {\r\nnew_bo = __bo_break_up(bdev, bo, pgnr);\r\nif (!new_bo) {\r\nmutex_unlock(&bdev->rbtree_mutex);\r\ndev_err(atomisp_dev, "%s: __bo_break_up failed!\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\n__bo_insert_to_alloc_rbtree(&bdev->allocated_rbtree, new_bo);\r\n__bo_insert_to_free_rbtree(&bdev->free_rbtree, bo);\r\nmutex_unlock(&bdev->rbtree_mutex);\r\nreturn new_bo;\r\n}\r\n__bo_insert_to_alloc_rbtree(&bdev->allocated_rbtree, bo);\r\nmutex_unlock(&bdev->rbtree_mutex);\r\nreturn bo;\r\n}\r\nvoid hmm_bo_release(struct hmm_buffer_object *bo)\r\n{\r\nstruct hmm_bo_device *bdev = bo->bdev;\r\nstruct hmm_buffer_object *next_bo, *prev_bo;\r\nmutex_lock(&bdev->rbtree_mutex);\r\nif (bo->status & HMM_BO_MMAPED) {\r\nmutex_unlock(&bdev->rbtree_mutex);\r\ndev_dbg(atomisp_dev, "destroy bo which is MMAPED, do nothing\n");\r\nreturn;\r\n}\r\nif (bo->status & HMM_BO_BINDED) {\r\ndev_warn(atomisp_dev, "the bo is still binded, unbind it first...\n");\r\nhmm_bo_unbind(bo);\r\n}\r\nif (bo->status & HMM_BO_PAGE_ALLOCED) {\r\ndev_warn(atomisp_dev, "the pages is not freed, free pages first\n");\r\nhmm_bo_free_pages(bo);\r\n}\r\nif (bo->status & HMM_BO_VMAPED || bo->status & HMM_BO_VMAPED_CACHED) {\r\ndev_warn(atomisp_dev, "the vunmap is not done, do it...\n");\r\nhmm_bo_vunmap(bo);\r\n}\r\nrb_erase(&bo->node, &bdev->allocated_rbtree);\r\nprev_bo = list_entry(bo->list.prev, struct hmm_buffer_object, list);\r\nnext_bo = list_entry(bo->list.next, struct hmm_buffer_object, list);\r\nif (bo->list.prev != &bdev->entire_bo_list &&\r\nprev_bo->end == bo->start &&\r\n(prev_bo->status & HMM_BO_MASK) == HMM_BO_FREE) {\r\n__bo_take_off_handling(prev_bo);\r\nbo = __bo_merge(prev_bo, bo);\r\n}\r\nif (bo->list.next != &bdev->entire_bo_list &&\r\nnext_bo->start == bo->end &&\r\n(next_bo->status & HMM_BO_MASK) == HMM_BO_FREE) {\r\n__bo_take_off_handling(next_bo);\r\nbo = __bo_merge(bo, next_bo);\r\n}\r\n__bo_insert_to_free_rbtree(&bdev->free_rbtree, bo);\r\nmutex_unlock(&bdev->rbtree_mutex);\r\nreturn;\r\n}\r\nvoid hmm_bo_device_exit(struct hmm_bo_device *bdev)\r\n{\r\nstruct hmm_buffer_object *bo;\r\nunsigned long flags;\r\ndev_dbg(atomisp_dev, "%s: entering!\n", __func__);\r\ncheck_bodev_null_return_void(bdev);\r\nwhile (!RB_EMPTY_ROOT(&bdev->allocated_rbtree))\r\nhmm_bo_release(\r\nrbtree_node_to_hmm_bo(bdev->allocated_rbtree.rb_node));\r\ndev_dbg(atomisp_dev, "%s: finished releasing all allocated bos!\n",\r\n__func__);\r\nwhile (!list_empty(&bdev->entire_bo_list)) {\r\nbo = list_to_hmm_bo(bdev->entire_bo_list.next);\r\nspin_lock_irqsave(&bdev->list_lock, flags);\r\nlist_del(&bo->list);\r\nspin_unlock_irqrestore(&bdev->list_lock, flags);\r\nkmem_cache_free(bdev->bo_cache, bo);\r\n}\r\ndev_dbg(atomisp_dev, "%s: finished to free all bos!\n", __func__);\r\nkmem_cache_destroy(bdev->bo_cache);\r\nisp_mmu_exit(&bdev->mmu);\r\n}\r\nint hmm_bo_device_inited(struct hmm_bo_device *bdev)\r\n{\r\ncheck_bodev_null_return(bdev, -EINVAL);\r\nreturn bdev->flag == HMM_BO_DEVICE_INITED;\r\n}\r\nint hmm_bo_allocated(struct hmm_buffer_object *bo)\r\n{\r\ncheck_bo_null_return(bo, 0);\r\nreturn bo->status & HMM_BO_ALLOCED;\r\n}\r\nstruct hmm_buffer_object *hmm_bo_device_search_start(\r\nstruct hmm_bo_device *bdev, ia_css_ptr vaddr)\r\n{\r\nstruct hmm_buffer_object *bo;\r\ncheck_bodev_null_return(bdev, NULL);\r\nmutex_lock(&bdev->rbtree_mutex);\r\nbo = __bo_search_by_addr(&bdev->allocated_rbtree, vaddr);\r\nif (!bo) {\r\nmutex_unlock(&bdev->rbtree_mutex);\r\ndev_err(atomisp_dev, "%s can not find bo with addr: 0x%x\n",\r\n__func__, vaddr);\r\nreturn NULL;\r\n}\r\nmutex_unlock(&bdev->rbtree_mutex);\r\nreturn bo;\r\n}\r\nstruct hmm_buffer_object *hmm_bo_device_search_in_range(\r\nstruct hmm_bo_device *bdev, unsigned int vaddr)\r\n{\r\nstruct hmm_buffer_object *bo;\r\ncheck_bodev_null_return(bdev, NULL);\r\nmutex_lock(&bdev->rbtree_mutex);\r\nbo = __bo_search_by_addr_in_range(&bdev->allocated_rbtree, vaddr);\r\nif (!bo) {\r\nmutex_unlock(&bdev->rbtree_mutex);\r\ndev_err(atomisp_dev, "%s can not find bo contain addr: 0x%x\n",\r\n__func__, vaddr);\r\nreturn NULL;\r\n}\r\nmutex_unlock(&bdev->rbtree_mutex);\r\nreturn bo;\r\n}\r\nstruct hmm_buffer_object *hmm_bo_device_search_vmap_start(\r\nstruct hmm_bo_device *bdev, const void *vaddr)\r\n{\r\nstruct list_head *pos;\r\nstruct hmm_buffer_object *bo;\r\nunsigned long flags;\r\ncheck_bodev_null_return(bdev, NULL);\r\nspin_lock_irqsave(&bdev->list_lock, flags);\r\nlist_for_each(pos, &bdev->entire_bo_list) {\r\nbo = list_to_hmm_bo(pos);\r\nif ((bo->status & HMM_BO_MASK) == HMM_BO_FREE)\r\ncontinue;\r\nif (bo->vmap_addr == vaddr)\r\ngoto found;\r\n}\r\nspin_unlock_irqrestore(&bdev->list_lock, flags);\r\nreturn NULL;\r\nfound:\r\nspin_unlock_irqrestore(&bdev->list_lock, flags);\r\nreturn bo;\r\n}\r\nstatic void free_private_bo_pages(struct hmm_buffer_object *bo,\r\nstruct hmm_pool *dypool,\r\nstruct hmm_pool *repool,\r\nint free_pgnr)\r\n{\r\nint i, ret;\r\nfor (i = 0; i < free_pgnr; i++) {\r\nswitch (bo->page_obj[i].type) {\r\ncase HMM_PAGE_TYPE_RESERVED:\r\nif (repool->pops\r\n&& repool->pops->pool_free_pages) {\r\nrepool->pops->pool_free_pages(repool->pool_info,\r\n&bo->page_obj[i]);\r\nhmm_mem_stat.res_cnt--;\r\n}\r\nbreak;\r\ncase HMM_PAGE_TYPE_DYNAMIC:\r\ncase HMM_PAGE_TYPE_GENERAL:\r\nif (dypool->pops\r\n&& dypool->pops->pool_inited\r\n&& dypool->pops->pool_inited(dypool->pool_info)) {\r\nif (dypool->pops->pool_free_pages)\r\ndypool->pops->pool_free_pages(\r\ndypool->pool_info,\r\n&bo->page_obj[i]);\r\nbreak;\r\n}\r\ndefault:\r\nret = set_pages_wb(bo->page_obj[i].page, 1);\r\nif (ret)\r\ndev_err(atomisp_dev,\r\n"set page to WB err ...ret = %d\n",\r\nret);\r\nif (!ret) {\r\n__free_pages(bo->page_obj[i].page, 0);\r\nhmm_mem_stat.sys_size--;\r\n}\r\nbreak;\r\n}\r\n}\r\nreturn;\r\n}\r\nstatic int alloc_private_pages(struct hmm_buffer_object *bo,\r\nint from_highmem,\r\nbool cached,\r\nstruct hmm_pool *dypool,\r\nstruct hmm_pool *repool)\r\n{\r\nint ret;\r\nunsigned int pgnr, order, blk_pgnr, alloc_pgnr;\r\nstruct page *pages;\r\ngfp_t gfp = GFP_NOWAIT | __GFP_NOWARN;\r\nint i, j;\r\nint failure_number = 0;\r\nbool reduce_order = false;\r\nbool lack_mem = true;\r\nif (from_highmem)\r\ngfp |= __GFP_HIGHMEM;\r\npgnr = bo->pgnr;\r\nbo->page_obj = kmalloc(sizeof(struct hmm_page_object) * pgnr,\r\nGFP_KERNEL);\r\nif (unlikely(!bo->page_obj)) {\r\ndev_err(atomisp_dev, "out of memory for bo->page_obj\n");\r\nreturn -ENOMEM;\r\n}\r\ni = 0;\r\nalloc_pgnr = 0;\r\nif (dypool->pops && dypool->pops->pool_alloc_pages) {\r\nalloc_pgnr = dypool->pops->pool_alloc_pages(dypool->pool_info,\r\nbo->page_obj, pgnr,\r\ncached);\r\nhmm_mem_stat.dyc_size -= alloc_pgnr;\r\nif (alloc_pgnr == pgnr)\r\nreturn 0;\r\n}\r\npgnr -= alloc_pgnr;\r\ni += alloc_pgnr;\r\nif (repool->pops && repool->pops->pool_alloc_pages) {\r\nalloc_pgnr = repool->pops->pool_alloc_pages(repool->pool_info,\r\n&bo->page_obj[i], pgnr,\r\ncached);\r\nhmm_mem_stat.res_cnt += alloc_pgnr;\r\nif (alloc_pgnr == pgnr)\r\nreturn 0;\r\n}\r\npgnr -= alloc_pgnr;\r\ni += alloc_pgnr;\r\nwhile (pgnr) {\r\norder = nr_to_order_bottom(pgnr);\r\nif (lack_mem)\r\norder = HMM_MIN_ORDER;\r\nelse if (order > HMM_MAX_ORDER)\r\norder = HMM_MAX_ORDER;\r\nretry:\r\nif (order == HMM_MIN_ORDER) {\r\ngfp &= ~GFP_NOWAIT;\r\ngfp |= __GFP_RECLAIM | __GFP_FS;\r\n}\r\npages = alloc_pages(gfp, order);\r\nif (unlikely(!pages)) {\r\nif (order == HMM_MIN_ORDER) {\r\ndev_err(atomisp_dev,\r\n"%s: cannot allocate pages\n",\r\n__func__);\r\ngoto cleanup;\r\n}\r\norder = HMM_MIN_ORDER;\r\nfailure_number++;\r\nreduce_order = true;\r\nif (failure_number == 2) {\r\nlack_mem = true;\r\nfailure_number = 0;\r\n}\r\ngoto retry;\r\n} else {\r\nblk_pgnr = order_to_nr(order);\r\nif (!cached) {\r\nret = set_pages_uc(pages, blk_pgnr);\r\nif (ret) {\r\ndev_err(atomisp_dev,\r\n"set page uncacheable"\r\n"failed.\n");\r\n__free_pages(pages, order);\r\ngoto cleanup;\r\n}\r\n}\r\nfor (j = 0; j < blk_pgnr; j++) {\r\nbo->page_obj[i].page = pages + j;\r\nbo->page_obj[i++].type = HMM_PAGE_TYPE_GENERAL;\r\n}\r\npgnr -= blk_pgnr;\r\nhmm_mem_stat.sys_size += blk_pgnr;\r\nif (reduce_order)\r\nreduce_order = false;\r\nelse\r\nfailure_number = 0;\r\n}\r\n}\r\nreturn 0;\r\ncleanup:\r\nalloc_pgnr = i;\r\nfree_private_bo_pages(bo, dypool, repool, alloc_pgnr);\r\nkfree(bo->page_obj);\r\nreturn -ENOMEM;\r\n}\r\nstatic void free_private_pages(struct hmm_buffer_object *bo,\r\nstruct hmm_pool *dypool,\r\nstruct hmm_pool *repool)\r\n{\r\nfree_private_bo_pages(bo, dypool, repool, bo->pgnr);\r\nkfree(bo->page_obj);\r\n}\r\nstatic int __get_pfnmap_pages(struct task_struct *tsk, struct mm_struct *mm,\r\nunsigned long start, int nr_pages,\r\nunsigned int gup_flags, struct page **pages,\r\nstruct vm_area_struct **vmas)\r\n{\r\nint i, ret;\r\nunsigned long vm_flags;\r\nif (nr_pages <= 0)\r\nreturn 0;\r\nVM_BUG_ON(!!pages != !!(gup_flags & FOLL_GET));\r\nvm_flags = (gup_flags & FOLL_WRITE) ?\r\n(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);\r\nvm_flags &= (gup_flags & FOLL_FORCE) ?\r\n(VM_MAYREAD | VM_MAYWRITE) : (VM_READ | VM_WRITE);\r\ni = 0;\r\ndo {\r\nstruct vm_area_struct *vma;\r\nvma = find_vma(mm, start);\r\nif (!vma) {\r\ndev_err(atomisp_dev, "find_vma failed\n");\r\nreturn i ? : -EFAULT;\r\n}\r\nif (is_vm_hugetlb_page(vma)) {\r\ncontinue;\r\n}\r\ndo {\r\nstruct page *page;\r\nunsigned long pfn;\r\nif (unlikely(fatal_signal_pending(current))) {\r\ndev_err(atomisp_dev,\r\n"fatal_signal_pending in %s\n",\r\n__func__);\r\nreturn i ? i : -ERESTARTSYS;\r\n}\r\nret = follow_pfn(vma, start, &pfn);\r\nif (ret) {\r\ndev_err(atomisp_dev, "follow_pfn() failed\n");\r\nreturn i ? : -EFAULT;\r\n}\r\npage = pfn_to_page(pfn);\r\nif (IS_ERR(page))\r\nreturn i ? i : PTR_ERR(page);\r\nif (pages) {\r\npages[i] = page;\r\nget_page(page);\r\nflush_anon_page(vma, page, start);\r\nflush_dcache_page(page);\r\n}\r\nif (vmas)\r\nvmas[i] = vma;\r\ni++;\r\nstart += PAGE_SIZE;\r\nnr_pages--;\r\n} while (nr_pages && start < vma->vm_end);\r\n} while (nr_pages);\r\nreturn i;\r\n}\r\nstatic int get_pfnmap_pages(struct task_struct *tsk, struct mm_struct *mm,\r\nunsigned long start, int nr_pages, int write, int force,\r\nstruct page **pages, struct vm_area_struct **vmas)\r\n{\r\nint flags = FOLL_TOUCH;\r\nif (pages)\r\nflags |= FOLL_GET;\r\nif (write)\r\nflags |= FOLL_WRITE;\r\nif (force)\r\nflags |= FOLL_FORCE;\r\nreturn __get_pfnmap_pages(tsk, mm, start, nr_pages, flags, pages, vmas);\r\n}\r\nstatic int alloc_user_pages(struct hmm_buffer_object *bo,\r\nvoid *userptr, bool cached)\r\n{\r\nint page_nr;\r\nint i;\r\nstruct vm_area_struct *vma;\r\nstruct page **pages;\r\npages = kmalloc(sizeof(struct page *) * bo->pgnr, GFP_KERNEL);\r\nif (unlikely(!pages)) {\r\ndev_err(atomisp_dev, "out of memory for pages...\n");\r\nreturn -ENOMEM;\r\n}\r\nbo->page_obj = kmalloc(sizeof(struct hmm_page_object) * bo->pgnr,\r\nGFP_KERNEL);\r\nif (unlikely(!bo->page_obj)) {\r\ndev_err(atomisp_dev, "out of memory for bo->page_obj...\n");\r\nkfree(pages);\r\nreturn -ENOMEM;\r\n}\r\nmutex_unlock(&bo->mutex);\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma(current->mm, (unsigned long)userptr);\r\nup_read(&current->mm->mmap_sem);\r\nif (vma == NULL) {\r\ndev_err(atomisp_dev, "find_vma failed\n");\r\nkfree(bo->page_obj);\r\nkfree(pages);\r\nmutex_lock(&bo->mutex);\r\nreturn -EFAULT;\r\n}\r\nmutex_lock(&bo->mutex);\r\nif (vma->vm_flags & (VM_IO | VM_PFNMAP)) {\r\npage_nr = get_pfnmap_pages(current, current->mm,\r\n(unsigned long)userptr,\r\n(int)(bo->pgnr), 1, 0,\r\npages, NULL);\r\nbo->mem_type = HMM_BO_MEM_TYPE_PFN;\r\n} else {\r\nmutex_unlock(&bo->mutex);\r\ndown_read(&current->mm->mmap_sem);\r\npage_nr = get_user_pages((unsigned long)userptr,\r\n(int)(bo->pgnr), 1, pages, NULL);\r\nup_read(&current->mm->mmap_sem);\r\nmutex_lock(&bo->mutex);\r\nbo->mem_type = HMM_BO_MEM_TYPE_USER;\r\n}\r\nif (page_nr != bo->pgnr) {\r\ndev_err(atomisp_dev,\r\n"get_user_pages err: bo->pgnr = %d, "\r\n"pgnr actually pinned = %d.\n",\r\nbo->pgnr, page_nr);\r\ngoto out_of_mem;\r\n}\r\nfor (i = 0; i < bo->pgnr; i++) {\r\nbo->page_obj[i].page = pages[i];\r\nbo->page_obj[i].type = HMM_PAGE_TYPE_GENERAL;\r\n}\r\nhmm_mem_stat.usr_size += bo->pgnr;\r\nkfree(pages);\r\nreturn 0;\r\nout_of_mem:\r\nfor (i = 0; i < page_nr; i++)\r\nput_page(pages[i]);\r\nkfree(pages);\r\nkfree(bo->page_obj);\r\nreturn -ENOMEM;\r\n}\r\nstatic void free_user_pages(struct hmm_buffer_object *bo)\r\n{\r\nint i;\r\nfor (i = 0; i < bo->pgnr; i++)\r\nput_page(bo->page_obj[i].page);\r\nhmm_mem_stat.usr_size -= bo->pgnr;\r\nkfree(bo->page_obj);\r\n}\r\nint hmm_bo_alloc_pages(struct hmm_buffer_object *bo,\r\nenum hmm_bo_type type, int from_highmem,\r\nvoid *userptr, bool cached)\r\n{\r\nint ret = -EINVAL;\r\ncheck_bo_null_return(bo, -EINVAL);\r\nmutex_lock(&bo->mutex);\r\ncheck_bo_status_no_goto(bo, HMM_BO_PAGE_ALLOCED, status_err);\r\nif (type == HMM_BO_PRIVATE) {\r\nret = alloc_private_pages(bo, from_highmem,\r\ncached, &dynamic_pool, &reserved_pool);\r\n} else if (type == HMM_BO_USER) {\r\nret = alloc_user_pages(bo, userptr, cached);\r\n} else {\r\ndev_err(atomisp_dev, "invalid buffer type.\n");\r\nret = -EINVAL;\r\n}\r\nif (ret)\r\ngoto alloc_err;\r\nbo->type = type;\r\nbo->status |= HMM_BO_PAGE_ALLOCED;\r\nmutex_unlock(&bo->mutex);\r\nreturn 0;\r\nalloc_err:\r\nmutex_unlock(&bo->mutex);\r\ndev_err(atomisp_dev, "alloc pages err...\n");\r\nreturn ret;\r\nstatus_err:\r\nmutex_unlock(&bo->mutex);\r\ndev_err(atomisp_dev,\r\n"buffer object has already page allocated.\n");\r\nreturn -EINVAL;\r\n}\r\nvoid hmm_bo_free_pages(struct hmm_buffer_object *bo)\r\n{\r\ncheck_bo_null_return_void(bo);\r\nmutex_lock(&bo->mutex);\r\ncheck_bo_status_yes_goto(bo, HMM_BO_PAGE_ALLOCED, status_err2);\r\nbo->status &= (~HMM_BO_PAGE_ALLOCED);\r\nif (bo->type == HMM_BO_PRIVATE)\r\nfree_private_pages(bo, &dynamic_pool, &reserved_pool);\r\nelse if (bo->type == HMM_BO_USER)\r\nfree_user_pages(bo);\r\nelse\r\ndev_err(atomisp_dev, "invalid buffer type.\n");\r\nmutex_unlock(&bo->mutex);\r\nreturn;\r\nstatus_err2:\r\nmutex_unlock(&bo->mutex);\r\ndev_err(atomisp_dev,\r\n"buffer object not page allocated yet.\n");\r\n}\r\nint hmm_bo_page_allocated(struct hmm_buffer_object *bo)\r\n{\r\nint ret;\r\ncheck_bo_null_return(bo, 0);\r\nret = bo->status & HMM_BO_PAGE_ALLOCED;\r\nreturn ret;\r\n}\r\nint hmm_bo_get_page_info(struct hmm_buffer_object *bo,\r\nstruct hmm_page_object **page_obj, int *pgnr)\r\n{\r\ncheck_bo_null_return(bo, -EINVAL);\r\nmutex_lock(&bo->mutex);\r\ncheck_bo_status_yes_goto(bo, HMM_BO_PAGE_ALLOCED, status_err);\r\n*page_obj = bo->page_obj;\r\n*pgnr = bo->pgnr;\r\nmutex_unlock(&bo->mutex);\r\nreturn 0;\r\nstatus_err:\r\ndev_err(atomisp_dev,\r\n"buffer object not page allocated yet.\n");\r\nmutex_unlock(&bo->mutex);\r\nreturn -EINVAL;\r\n}\r\nint hmm_bo_bind(struct hmm_buffer_object *bo)\r\n{\r\nint ret;\r\nunsigned int virt;\r\nstruct hmm_bo_device *bdev;\r\nunsigned int i;\r\ncheck_bo_null_return(bo, -EINVAL);\r\nmutex_lock(&bo->mutex);\r\ncheck_bo_status_yes_goto(bo,\r\nHMM_BO_PAGE_ALLOCED | HMM_BO_ALLOCED,\r\nstatus_err1);\r\ncheck_bo_status_no_goto(bo, HMM_BO_BINDED, status_err2);\r\nbdev = bo->bdev;\r\nvirt = bo->start;\r\nfor (i = 0; i < bo->pgnr; i++) {\r\nret =\r\nisp_mmu_map(&bdev->mmu, virt,\r\npage_to_phys(bo->page_obj[i].page), 1);\r\nif (ret)\r\ngoto map_err;\r\nvirt += (1 << PAGE_SHIFT);\r\n}\r\nif (bo->start != 0x0)\r\nisp_mmu_flush_tlb_range(&bdev->mmu, bo->start,\r\n(bo->pgnr << PAGE_SHIFT));\r\nbo->status |= HMM_BO_BINDED;\r\nmutex_unlock(&bo->mutex);\r\nreturn 0;\r\nmap_err:\r\nvirt = bo->start;\r\nfor ( ; i > 0; i--) {\r\nisp_mmu_unmap(&bdev->mmu, virt, 1);\r\nvirt += pgnr_to_size(1);\r\n}\r\nmutex_unlock(&bo->mutex);\r\ndev_err(atomisp_dev,\r\n"setup MMU address mapping failed.\n");\r\nreturn ret;\r\nstatus_err2:\r\nmutex_unlock(&bo->mutex);\r\ndev_err(atomisp_dev, "buffer object already binded.\n");\r\nreturn -EINVAL;\r\nstatus_err1:\r\nmutex_unlock(&bo->mutex);\r\ndev_err(atomisp_dev,\r\n"buffer object vm_node or page not allocated.\n");\r\nreturn -EINVAL;\r\n}\r\nvoid hmm_bo_unbind(struct hmm_buffer_object *bo)\r\n{\r\nunsigned int virt;\r\nstruct hmm_bo_device *bdev;\r\nunsigned int i;\r\ncheck_bo_null_return_void(bo);\r\nmutex_lock(&bo->mutex);\r\ncheck_bo_status_yes_goto(bo,\r\nHMM_BO_PAGE_ALLOCED |\r\nHMM_BO_ALLOCED |\r\nHMM_BO_BINDED, status_err);\r\nbdev = bo->bdev;\r\nvirt = bo->start;\r\nfor (i = 0; i < bo->pgnr; i++) {\r\nisp_mmu_unmap(&bdev->mmu, virt, 1);\r\nvirt += pgnr_to_size(1);\r\n}\r\nisp_mmu_flush_tlb_range(&bdev->mmu, bo->start,\r\n(bo->pgnr << PAGE_SHIFT));\r\nbo->status &= (~HMM_BO_BINDED);\r\nmutex_unlock(&bo->mutex);\r\nreturn;\r\nstatus_err:\r\nmutex_unlock(&bo->mutex);\r\ndev_err(atomisp_dev,\r\n"buffer vm or page not allocated or not binded yet.\n");\r\n}\r\nint hmm_bo_binded(struct hmm_buffer_object *bo)\r\n{\r\nint ret;\r\ncheck_bo_null_return(bo, 0);\r\nmutex_lock(&bo->mutex);\r\nret = bo->status & HMM_BO_BINDED;\r\nmutex_unlock(&bo->mutex);\r\nreturn ret;\r\n}\r\nvoid *hmm_bo_vmap(struct hmm_buffer_object *bo, bool cached)\r\n{\r\nstruct page **pages;\r\nint i;\r\ncheck_bo_null_return(bo, NULL);\r\nmutex_lock(&bo->mutex);\r\nif (((bo->status & HMM_BO_VMAPED) && !cached) ||\r\n((bo->status & HMM_BO_VMAPED_CACHED) && cached)) {\r\nmutex_unlock(&bo->mutex);\r\nreturn bo->vmap_addr;\r\n}\r\nif (bo->status & HMM_BO_VMAPED || bo->status & HMM_BO_VMAPED_CACHED) {\r\nvunmap(bo->vmap_addr);\r\nbo->vmap_addr = NULL;\r\nbo->status &= ~(HMM_BO_VMAPED | HMM_BO_VMAPED_CACHED);\r\n}\r\npages = kmalloc(sizeof(*pages) * bo->pgnr, GFP_KERNEL);\r\nif (unlikely(!pages)) {\r\nmutex_unlock(&bo->mutex);\r\ndev_err(atomisp_dev, "out of memory for pages...\n");\r\nreturn NULL;\r\n}\r\nfor (i = 0; i < bo->pgnr; i++)\r\npages[i] = bo->page_obj[i].page;\r\nbo->vmap_addr = vmap(pages, bo->pgnr, VM_MAP,\r\ncached ? PAGE_KERNEL : PAGE_KERNEL_NOCACHE);\r\nif (unlikely(!bo->vmap_addr)) {\r\nkfree(pages);\r\nmutex_unlock(&bo->mutex);\r\ndev_err(atomisp_dev, "vmap failed...\n");\r\nreturn NULL;\r\n}\r\nbo->status |= (cached ? HMM_BO_VMAPED_CACHED : HMM_BO_VMAPED);\r\nkfree(pages);\r\nmutex_unlock(&bo->mutex);\r\nreturn bo->vmap_addr;\r\n}\r\nvoid hmm_bo_flush_vmap(struct hmm_buffer_object *bo)\r\n{\r\ncheck_bo_null_return_void(bo);\r\nmutex_lock(&bo->mutex);\r\nif (!(bo->status & HMM_BO_VMAPED_CACHED) || !bo->vmap_addr) {\r\nmutex_unlock(&bo->mutex);\r\nreturn;\r\n}\r\nclflush_cache_range(bo->vmap_addr, bo->pgnr * PAGE_SIZE);\r\nmutex_unlock(&bo->mutex);\r\n}\r\nvoid hmm_bo_vunmap(struct hmm_buffer_object *bo)\r\n{\r\ncheck_bo_null_return_void(bo);\r\nmutex_lock(&bo->mutex);\r\nif (bo->status & HMM_BO_VMAPED || bo->status & HMM_BO_VMAPED_CACHED) {\r\nvunmap(bo->vmap_addr);\r\nbo->vmap_addr = NULL;\r\nbo->status &= ~(HMM_BO_VMAPED | HMM_BO_VMAPED_CACHED);\r\n}\r\nmutex_unlock(&bo->mutex);\r\nreturn;\r\n}\r\nvoid hmm_bo_ref(struct hmm_buffer_object *bo)\r\n{\r\ncheck_bo_null_return_void(bo);\r\nkref_get(&bo->kref);\r\n}\r\nstatic void kref_hmm_bo_release(struct kref *kref)\r\n{\r\nif (!kref)\r\nreturn;\r\nhmm_bo_release(kref_to_hmm_bo(kref));\r\n}\r\nvoid hmm_bo_unref(struct hmm_buffer_object *bo)\r\n{\r\ncheck_bo_null_return_void(bo);\r\nkref_put(&bo->kref, kref_hmm_bo_release);\r\n}\r\nstatic void hmm_bo_vm_open(struct vm_area_struct *vma)\r\n{\r\nstruct hmm_buffer_object *bo =\r\n(struct hmm_buffer_object *)vma->vm_private_data;\r\ncheck_bo_null_return_void(bo);\r\nhmm_bo_ref(bo);\r\nmutex_lock(&bo->mutex);\r\nbo->status |= HMM_BO_MMAPED;\r\nbo->mmap_count++;\r\nmutex_unlock(&bo->mutex);\r\n}\r\nstatic void hmm_bo_vm_close(struct vm_area_struct *vma)\r\n{\r\nstruct hmm_buffer_object *bo =\r\n(struct hmm_buffer_object *)vma->vm_private_data;\r\ncheck_bo_null_return_void(bo);\r\nhmm_bo_unref(bo);\r\nmutex_lock(&bo->mutex);\r\nbo->mmap_count--;\r\nif (!bo->mmap_count) {\r\nbo->status &= (~HMM_BO_MMAPED);\r\nvma->vm_private_data = NULL;\r\n}\r\nmutex_unlock(&bo->mutex);\r\n}\r\nint hmm_bo_mmap(struct vm_area_struct *vma, struct hmm_buffer_object *bo)\r\n{\r\nunsigned int start, end;\r\nunsigned int virt;\r\nunsigned int pgnr, i;\r\nunsigned int pfn;\r\ncheck_bo_null_return(bo, -EINVAL);\r\ncheck_bo_status_yes_goto(bo, HMM_BO_PAGE_ALLOCED, status_err);\r\npgnr = bo->pgnr;\r\nstart = vma->vm_start;\r\nend = vma->vm_end;\r\nif ((start + pgnr_to_size(pgnr)) != end) {\r\ndev_warn(atomisp_dev,\r\n"vma's address space size not equal"\r\n" to buffer object's size");\r\nreturn -EINVAL;\r\n}\r\nvirt = vma->vm_start;\r\nfor (i = 0; i < pgnr; i++) {\r\npfn = page_to_pfn(bo->page_obj[i].page);\r\nif (remap_pfn_range(vma, virt, pfn, PAGE_SIZE, PAGE_SHARED)) {\r\ndev_warn(atomisp_dev,\r\n"remap_pfn_range failed:"\r\n" virt = 0x%x, pfn = 0x%x,"\r\n" mapped_pgnr = %d\n", virt, pfn, 1);\r\nreturn -EINVAL;\r\n}\r\nvirt += PAGE_SIZE;\r\n}\r\nvma->vm_private_data = bo;\r\nvma->vm_ops = &hmm_bo_vm_ops;\r\nvma->vm_flags |= VM_IO|VM_DONTEXPAND|VM_DONTDUMP;\r\nhmm_bo_vm_open(vma);\r\nreturn 0;\r\nstatus_err:\r\ndev_err(atomisp_dev, "buffer page not allocated yet.\n");\r\nreturn -EINVAL;\r\n}
