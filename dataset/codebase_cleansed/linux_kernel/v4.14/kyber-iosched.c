static int rq_sched_domain(const struct request *rq)\r\n{\r\nunsigned int op = rq->cmd_flags;\r\nif ((op & REQ_OP_MASK) == REQ_OP_READ)\r\nreturn KYBER_READ;\r\nelse if ((op & REQ_OP_MASK) == REQ_OP_WRITE && op_is_sync(op))\r\nreturn KYBER_SYNC_WRITE;\r\nelse\r\nreturn KYBER_OTHER;\r\n}\r\nstatic int kyber_lat_status(struct blk_stat_callback *cb,\r\nunsigned int sched_domain, u64 target)\r\n{\r\nu64 latency;\r\nif (!cb->stat[sched_domain].nr_samples)\r\nreturn NONE;\r\nlatency = cb->stat[sched_domain].mean;\r\nif (latency >= 2 * target)\r\nreturn AWFUL;\r\nelse if (latency > target)\r\nreturn BAD;\r\nelse if (latency <= target / 2)\r\nreturn GREAT;\r\nelse\r\nreturn GOOD;\r\n}\r\nstatic void kyber_adjust_rw_depth(struct kyber_queue_data *kqd,\r\nunsigned int sched_domain, int this_status,\r\nint other_status)\r\n{\r\nunsigned int orig_depth, depth;\r\nif (this_status == NONE ||\r\n(IS_GOOD(this_status) && IS_GOOD(other_status)) ||\r\n(IS_BAD(this_status) && IS_BAD(other_status)))\r\nreturn;\r\norig_depth = depth = kqd->domain_tokens[sched_domain].sb.depth;\r\nif (other_status == NONE) {\r\ndepth++;\r\n} else {\r\nswitch (this_status) {\r\ncase GOOD:\r\nif (other_status == AWFUL)\r\ndepth -= max(depth / 4, 1U);\r\nelse\r\ndepth -= max(depth / 8, 1U);\r\nbreak;\r\ncase GREAT:\r\nif (other_status == AWFUL)\r\ndepth /= 2;\r\nelse\r\ndepth -= max(depth / 4, 1U);\r\nbreak;\r\ncase BAD:\r\ndepth++;\r\nbreak;\r\ncase AWFUL:\r\nif (other_status == GREAT)\r\ndepth += 2;\r\nelse\r\ndepth++;\r\nbreak;\r\n}\r\n}\r\ndepth = clamp(depth, 1U, kyber_depth[sched_domain]);\r\nif (depth != orig_depth)\r\nsbitmap_queue_resize(&kqd->domain_tokens[sched_domain], depth);\r\n}\r\nstatic void kyber_adjust_other_depth(struct kyber_queue_data *kqd,\r\nint read_status, int write_status,\r\nbool have_samples)\r\n{\r\nunsigned int orig_depth, depth;\r\nint status;\r\norig_depth = depth = kqd->domain_tokens[KYBER_OTHER].sb.depth;\r\nif (read_status == NONE && write_status == NONE) {\r\ndepth += 2;\r\n} else if (have_samples) {\r\nif (read_status == NONE)\r\nstatus = write_status;\r\nelse if (write_status == NONE)\r\nstatus = read_status;\r\nelse\r\nstatus = max(read_status, write_status);\r\nswitch (status) {\r\ncase GREAT:\r\ndepth += 2;\r\nbreak;\r\ncase GOOD:\r\ndepth++;\r\nbreak;\r\ncase BAD:\r\ndepth -= max(depth / 4, 1U);\r\nbreak;\r\ncase AWFUL:\r\ndepth /= 2;\r\nbreak;\r\n}\r\n}\r\ndepth = clamp(depth, 1U, kyber_depth[KYBER_OTHER]);\r\nif (depth != orig_depth)\r\nsbitmap_queue_resize(&kqd->domain_tokens[KYBER_OTHER], depth);\r\n}\r\nstatic void kyber_stat_timer_fn(struct blk_stat_callback *cb)\r\n{\r\nstruct kyber_queue_data *kqd = cb->data;\r\nint read_status, write_status;\r\nread_status = kyber_lat_status(cb, KYBER_READ, kqd->read_lat_nsec);\r\nwrite_status = kyber_lat_status(cb, KYBER_SYNC_WRITE, kqd->write_lat_nsec);\r\nkyber_adjust_rw_depth(kqd, KYBER_READ, read_status, write_status);\r\nkyber_adjust_rw_depth(kqd, KYBER_SYNC_WRITE, write_status, read_status);\r\nkyber_adjust_other_depth(kqd, read_status, write_status,\r\ncb->stat[KYBER_OTHER].nr_samples != 0);\r\nif (!blk_stat_is_active(kqd->cb) &&\r\n((IS_BAD(read_status) || IS_BAD(write_status) ||\r\nkqd->domain_tokens[KYBER_OTHER].sb.depth < kyber_depth[KYBER_OTHER])))\r\nblk_stat_activate_msecs(kqd->cb, 100);\r\n}\r\nstatic unsigned int kyber_sched_tags_shift(struct kyber_queue_data *kqd)\r\n{\r\nreturn kqd->q->queue_hw_ctx[0]->sched_tags->bitmap_tags.sb.shift;\r\n}\r\nstatic struct kyber_queue_data *kyber_queue_data_alloc(struct request_queue *q)\r\n{\r\nstruct kyber_queue_data *kqd;\r\nunsigned int max_tokens;\r\nunsigned int shift;\r\nint ret = -ENOMEM;\r\nint i;\r\nkqd = kmalloc_node(sizeof(*kqd), GFP_KERNEL, q->node);\r\nif (!kqd)\r\ngoto err;\r\nkqd->q = q;\r\nkqd->cb = blk_stat_alloc_callback(kyber_stat_timer_fn, rq_sched_domain,\r\nKYBER_NUM_DOMAINS, kqd);\r\nif (!kqd->cb)\r\ngoto err_kqd;\r\nmax_tokens = max_t(unsigned int, q->tag_set->queue_depth,\r\nKYBER_MIN_DEPTH);\r\nfor (i = 0; i < KYBER_NUM_DOMAINS; i++) {\r\nWARN_ON(!kyber_depth[i]);\r\nWARN_ON(!kyber_batch_size[i]);\r\nret = sbitmap_queue_init_node(&kqd->domain_tokens[i],\r\nmax_tokens, -1, false, GFP_KERNEL,\r\nq->node);\r\nif (ret) {\r\nwhile (--i >= 0)\r\nsbitmap_queue_free(&kqd->domain_tokens[i]);\r\ngoto err_cb;\r\n}\r\nsbitmap_queue_resize(&kqd->domain_tokens[i], kyber_depth[i]);\r\n}\r\nshift = kyber_sched_tags_shift(kqd);\r\nkqd->async_depth = (1U << shift) * KYBER_ASYNC_PERCENT / 100U;\r\nkqd->read_lat_nsec = 2000000ULL;\r\nkqd->write_lat_nsec = 10000000ULL;\r\nreturn kqd;\r\nerr_cb:\r\nblk_stat_free_callback(kqd->cb);\r\nerr_kqd:\r\nkfree(kqd);\r\nerr:\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic int kyber_init_sched(struct request_queue *q, struct elevator_type *e)\r\n{\r\nstruct kyber_queue_data *kqd;\r\nstruct elevator_queue *eq;\r\neq = elevator_alloc(q, e);\r\nif (!eq)\r\nreturn -ENOMEM;\r\nkqd = kyber_queue_data_alloc(q);\r\nif (IS_ERR(kqd)) {\r\nkobject_put(&eq->kobj);\r\nreturn PTR_ERR(kqd);\r\n}\r\neq->elevator_data = kqd;\r\nq->elevator = eq;\r\nblk_stat_add_callback(q, kqd->cb);\r\nreturn 0;\r\n}\r\nstatic void kyber_exit_sched(struct elevator_queue *e)\r\n{\r\nstruct kyber_queue_data *kqd = e->elevator_data;\r\nstruct request_queue *q = kqd->q;\r\nint i;\r\nblk_stat_remove_callback(q, kqd->cb);\r\nfor (i = 0; i < KYBER_NUM_DOMAINS; i++)\r\nsbitmap_queue_free(&kqd->domain_tokens[i]);\r\nblk_stat_free_callback(kqd->cb);\r\nkfree(kqd);\r\n}\r\nstatic int kyber_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)\r\n{\r\nstruct kyber_hctx_data *khd;\r\nint i;\r\nkhd = kmalloc_node(sizeof(*khd), GFP_KERNEL, hctx->numa_node);\r\nif (!khd)\r\nreturn -ENOMEM;\r\nspin_lock_init(&khd->lock);\r\nfor (i = 0; i < KYBER_NUM_DOMAINS; i++) {\r\nINIT_LIST_HEAD(&khd->rqs[i]);\r\nINIT_LIST_HEAD(&khd->domain_wait[i].entry);\r\natomic_set(&khd->wait_index[i], 0);\r\n}\r\nkhd->cur_domain = 0;\r\nkhd->batching = 0;\r\nhctx->sched_data = khd;\r\nreturn 0;\r\n}\r\nstatic void kyber_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)\r\n{\r\nkfree(hctx->sched_data);\r\n}\r\nstatic int rq_get_domain_token(struct request *rq)\r\n{\r\nreturn (long)rq->elv.priv[0];\r\n}\r\nstatic void rq_set_domain_token(struct request *rq, int token)\r\n{\r\nrq->elv.priv[0] = (void *)(long)token;\r\n}\r\nstatic void rq_clear_domain_token(struct kyber_queue_data *kqd,\r\nstruct request *rq)\r\n{\r\nunsigned int sched_domain;\r\nint nr;\r\nnr = rq_get_domain_token(rq);\r\nif (nr != -1) {\r\nsched_domain = rq_sched_domain(rq);\r\nsbitmap_queue_clear(&kqd->domain_tokens[sched_domain], nr,\r\nrq->mq_ctx->cpu);\r\n}\r\n}\r\nstatic void kyber_limit_depth(unsigned int op, struct blk_mq_alloc_data *data)\r\n{\r\nif (!op_is_sync(op)) {\r\nstruct kyber_queue_data *kqd = data->q->elevator->elevator_data;\r\ndata->shallow_depth = kqd->async_depth;\r\n}\r\n}\r\nstatic void kyber_prepare_request(struct request *rq, struct bio *bio)\r\n{\r\nrq_set_domain_token(rq, -1);\r\n}\r\nstatic void kyber_finish_request(struct request *rq)\r\n{\r\nstruct kyber_queue_data *kqd = rq->q->elevator->elevator_data;\r\nrq_clear_domain_token(kqd, rq);\r\n}\r\nstatic void kyber_completed_request(struct request *rq)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct kyber_queue_data *kqd = q->elevator->elevator_data;\r\nunsigned int sched_domain;\r\nu64 now, latency, target;\r\nsched_domain = rq_sched_domain(rq);\r\nswitch (sched_domain) {\r\ncase KYBER_READ:\r\ntarget = kqd->read_lat_nsec;\r\nbreak;\r\ncase KYBER_SYNC_WRITE:\r\ntarget = kqd->write_lat_nsec;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\nif (blk_stat_is_active(kqd->cb))\r\nreturn;\r\nnow = __blk_stat_time(ktime_to_ns(ktime_get()));\r\nif (now < blk_stat_time(&rq->issue_stat))\r\nreturn;\r\nlatency = now - blk_stat_time(&rq->issue_stat);\r\nif (latency > target)\r\nblk_stat_activate_msecs(kqd->cb, 10);\r\n}\r\nstatic void kyber_flush_busy_ctxs(struct kyber_hctx_data *khd,\r\nstruct blk_mq_hw_ctx *hctx)\r\n{\r\nLIST_HEAD(rq_list);\r\nstruct request *rq, *next;\r\nblk_mq_flush_busy_ctxs(hctx, &rq_list);\r\nlist_for_each_entry_safe(rq, next, &rq_list, queuelist) {\r\nunsigned int sched_domain;\r\nsched_domain = rq_sched_domain(rq);\r\nlist_move_tail(&rq->queuelist, &khd->rqs[sched_domain]);\r\n}\r\n}\r\nstatic int kyber_domain_wake(wait_queue_entry_t *wait, unsigned mode, int flags,\r\nvoid *key)\r\n{\r\nstruct blk_mq_hw_ctx *hctx = READ_ONCE(wait->private);\r\nlist_del_init(&wait->entry);\r\nblk_mq_run_hw_queue(hctx, true);\r\nreturn 1;\r\n}\r\nstatic int kyber_get_domain_token(struct kyber_queue_data *kqd,\r\nstruct kyber_hctx_data *khd,\r\nstruct blk_mq_hw_ctx *hctx)\r\n{\r\nunsigned int sched_domain = khd->cur_domain;\r\nstruct sbitmap_queue *domain_tokens = &kqd->domain_tokens[sched_domain];\r\nwait_queue_entry_t *wait = &khd->domain_wait[sched_domain];\r\nstruct sbq_wait_state *ws;\r\nint nr;\r\nnr = __sbitmap_queue_get(domain_tokens);\r\nif (nr >= 0)\r\nreturn nr;\r\nif (list_empty_careful(&wait->entry)) {\r\ninit_waitqueue_func_entry(wait, kyber_domain_wake);\r\nwait->private = hctx;\r\nws = sbq_wait_ptr(domain_tokens,\r\n&khd->wait_index[sched_domain]);\r\nadd_wait_queue(&ws->wait, wait);\r\nnr = __sbitmap_queue_get(domain_tokens);\r\n}\r\nreturn nr;\r\n}\r\nstatic struct request *\r\nkyber_dispatch_cur_domain(struct kyber_queue_data *kqd,\r\nstruct kyber_hctx_data *khd,\r\nstruct blk_mq_hw_ctx *hctx,\r\nbool *flushed)\r\n{\r\nstruct list_head *rqs;\r\nstruct request *rq;\r\nint nr;\r\nrqs = &khd->rqs[khd->cur_domain];\r\nrq = list_first_entry_or_null(rqs, struct request, queuelist);\r\nif (!rq && !*flushed) {\r\nkyber_flush_busy_ctxs(khd, hctx);\r\n*flushed = true;\r\nrq = list_first_entry_or_null(rqs, struct request, queuelist);\r\n}\r\nif (rq) {\r\nnr = kyber_get_domain_token(kqd, khd, hctx);\r\nif (nr >= 0) {\r\nkhd->batching++;\r\nrq_set_domain_token(rq, nr);\r\nlist_del_init(&rq->queuelist);\r\nreturn rq;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct request *kyber_dispatch_request(struct blk_mq_hw_ctx *hctx)\r\n{\r\nstruct kyber_queue_data *kqd = hctx->queue->elevator->elevator_data;\r\nstruct kyber_hctx_data *khd = hctx->sched_data;\r\nbool flushed = false;\r\nstruct request *rq;\r\nint i;\r\nspin_lock(&khd->lock);\r\nif (khd->batching < kyber_batch_size[khd->cur_domain]) {\r\nrq = kyber_dispatch_cur_domain(kqd, khd, hctx, &flushed);\r\nif (rq)\r\ngoto out;\r\n}\r\nkhd->batching = 0;\r\nfor (i = 0; i < KYBER_NUM_DOMAINS; i++) {\r\nif (khd->cur_domain == KYBER_NUM_DOMAINS - 1)\r\nkhd->cur_domain = 0;\r\nelse\r\nkhd->cur_domain++;\r\nrq = kyber_dispatch_cur_domain(kqd, khd, hctx, &flushed);\r\nif (rq)\r\ngoto out;\r\n}\r\nrq = NULL;\r\nout:\r\nspin_unlock(&khd->lock);\r\nreturn rq;\r\n}\r\nstatic bool kyber_has_work(struct blk_mq_hw_ctx *hctx)\r\n{\r\nstruct kyber_hctx_data *khd = hctx->sched_data;\r\nint i;\r\nfor (i = 0; i < KYBER_NUM_DOMAINS; i++) {\r\nif (!list_empty_careful(&khd->rqs[i]))\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int kyber_async_depth_show(void *data, struct seq_file *m)\r\n{\r\nstruct request_queue *q = data;\r\nstruct kyber_queue_data *kqd = q->elevator->elevator_data;\r\nseq_printf(m, "%u\n", kqd->async_depth);\r\nreturn 0;\r\n}\r\nstatic int kyber_cur_domain_show(void *data, struct seq_file *m)\r\n{\r\nstruct blk_mq_hw_ctx *hctx = data;\r\nstruct kyber_hctx_data *khd = hctx->sched_data;\r\nswitch (khd->cur_domain) {\r\ncase KYBER_READ:\r\nseq_puts(m, "READ\n");\r\nbreak;\r\ncase KYBER_SYNC_WRITE:\r\nseq_puts(m, "SYNC_WRITE\n");\r\nbreak;\r\ncase KYBER_OTHER:\r\nseq_puts(m, "OTHER\n");\r\nbreak;\r\ndefault:\r\nseq_printf(m, "%u\n", khd->cur_domain);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int kyber_batching_show(void *data, struct seq_file *m)\r\n{\r\nstruct blk_mq_hw_ctx *hctx = data;\r\nstruct kyber_hctx_data *khd = hctx->sched_data;\r\nseq_printf(m, "%u\n", khd->batching);\r\nreturn 0;\r\n}\r\nstatic int __init kyber_init(void)\r\n{\r\nreturn elv_register(&kyber_sched);\r\n}\r\nstatic void __exit kyber_exit(void)\r\n{\r\nelv_unregister(&kyber_sched);\r\n}
