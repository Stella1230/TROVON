static bool kvmppc_is_split_real(struct kvm_vcpu *vcpu)\r\n{\r\nulong msr = kvmppc_get_msr(vcpu);\r\nreturn (msr & (MSR_IR|MSR_DR)) == MSR_DR;\r\n}\r\nstatic void kvmppc_fixup_split_real(struct kvm_vcpu *vcpu)\r\n{\r\nulong msr = kvmppc_get_msr(vcpu);\r\nulong pc = kvmppc_get_pc(vcpu);\r\nif ((msr & (MSR_IR|MSR_DR)) != MSR_DR)\r\nreturn;\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_SPLIT_HACK)\r\nreturn;\r\nif (pc & SPLIT_HACK_MASK)\r\nreturn;\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_SPLIT_HACK;\r\nkvmppc_set_pc(vcpu, pc | SPLIT_HACK_OFFS);\r\n}\r\nstatic void kvmppc_core_vcpu_load_pr(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nmemcpy(svcpu->slb, to_book3s(vcpu)->slb_shadow, sizeof(svcpu->slb));\r\nsvcpu->slb_max = to_book3s(vcpu)->slb_shadow_max;\r\nsvcpu->in_use = 0;\r\nsvcpu_put(svcpu);\r\n#endif\r\nif (cpu_has_feature(CPU_FTR_HVMODE) &&\r\ncpu_has_feature(CPU_FTR_ARCH_207S))\r\nmtspr(SPRN_LPCR, mfspr(SPRN_LPCR) & ~LPCR_AIL);\r\nvcpu->cpu = smp_processor_id();\r\n#ifdef CONFIG_PPC_BOOK3S_32\r\ncurrent->thread.kvm_shadow_vcpu = vcpu->arch.shadow_vcpu;\r\n#endif\r\nif (kvmppc_is_split_real(vcpu))\r\nkvmppc_fixup_split_real(vcpu);\r\n}\r\nstatic void kvmppc_core_vcpu_put_pr(struct kvm_vcpu *vcpu)\r\n{\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nif (svcpu->in_use) {\r\nkvmppc_copy_from_svcpu(vcpu, svcpu);\r\n}\r\nmemcpy(to_book3s(vcpu)->slb_shadow, svcpu->slb, sizeof(svcpu->slb));\r\nto_book3s(vcpu)->slb_shadow_max = svcpu->slb_max;\r\nsvcpu_put(svcpu);\r\n#endif\r\nif (kvmppc_is_split_real(vcpu))\r\nkvmppc_unfixup_split_real(vcpu);\r\nkvmppc_giveup_ext(vcpu, MSR_FP | MSR_VEC | MSR_VSX);\r\nkvmppc_giveup_fac(vcpu, FSCR_TAR_LG);\r\nif (cpu_has_feature(CPU_FTR_HVMODE) &&\r\ncpu_has_feature(CPU_FTR_ARCH_207S))\r\nmtspr(SPRN_LPCR, mfspr(SPRN_LPCR) | LPCR_AIL_3);\r\nvcpu->cpu = -1;\r\n}\r\nvoid kvmppc_copy_to_svcpu(struct kvmppc_book3s_shadow_vcpu *svcpu,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nsvcpu->gpr[0] = vcpu->arch.gpr[0];\r\nsvcpu->gpr[1] = vcpu->arch.gpr[1];\r\nsvcpu->gpr[2] = vcpu->arch.gpr[2];\r\nsvcpu->gpr[3] = vcpu->arch.gpr[3];\r\nsvcpu->gpr[4] = vcpu->arch.gpr[4];\r\nsvcpu->gpr[5] = vcpu->arch.gpr[5];\r\nsvcpu->gpr[6] = vcpu->arch.gpr[6];\r\nsvcpu->gpr[7] = vcpu->arch.gpr[7];\r\nsvcpu->gpr[8] = vcpu->arch.gpr[8];\r\nsvcpu->gpr[9] = vcpu->arch.gpr[9];\r\nsvcpu->gpr[10] = vcpu->arch.gpr[10];\r\nsvcpu->gpr[11] = vcpu->arch.gpr[11];\r\nsvcpu->gpr[12] = vcpu->arch.gpr[12];\r\nsvcpu->gpr[13] = vcpu->arch.gpr[13];\r\nsvcpu->cr = vcpu->arch.cr;\r\nsvcpu->xer = vcpu->arch.xer;\r\nsvcpu->ctr = vcpu->arch.ctr;\r\nsvcpu->lr = vcpu->arch.lr;\r\nsvcpu->pc = vcpu->arch.pc;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nsvcpu->shadow_fscr = vcpu->arch.shadow_fscr;\r\n#endif\r\nvcpu->arch.entry_tb = get_tb();\r\nvcpu->arch.entry_vtb = get_vtb();\r\nif (cpu_has_feature(CPU_FTR_ARCH_207S))\r\nvcpu->arch.entry_ic = mfspr(SPRN_IC);\r\nsvcpu->in_use = true;\r\n}\r\nvoid kvmppc_copy_from_svcpu(struct kvm_vcpu *vcpu,\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu)\r\n{\r\npreempt_disable();\r\nif (!svcpu->in_use)\r\ngoto out;\r\nvcpu->arch.gpr[0] = svcpu->gpr[0];\r\nvcpu->arch.gpr[1] = svcpu->gpr[1];\r\nvcpu->arch.gpr[2] = svcpu->gpr[2];\r\nvcpu->arch.gpr[3] = svcpu->gpr[3];\r\nvcpu->arch.gpr[4] = svcpu->gpr[4];\r\nvcpu->arch.gpr[5] = svcpu->gpr[5];\r\nvcpu->arch.gpr[6] = svcpu->gpr[6];\r\nvcpu->arch.gpr[7] = svcpu->gpr[7];\r\nvcpu->arch.gpr[8] = svcpu->gpr[8];\r\nvcpu->arch.gpr[9] = svcpu->gpr[9];\r\nvcpu->arch.gpr[10] = svcpu->gpr[10];\r\nvcpu->arch.gpr[11] = svcpu->gpr[11];\r\nvcpu->arch.gpr[12] = svcpu->gpr[12];\r\nvcpu->arch.gpr[13] = svcpu->gpr[13];\r\nvcpu->arch.cr = svcpu->cr;\r\nvcpu->arch.xer = svcpu->xer;\r\nvcpu->arch.ctr = svcpu->ctr;\r\nvcpu->arch.lr = svcpu->lr;\r\nvcpu->arch.pc = svcpu->pc;\r\nvcpu->arch.shadow_srr1 = svcpu->shadow_srr1;\r\nvcpu->arch.fault_dar = svcpu->fault_dar;\r\nvcpu->arch.fault_dsisr = svcpu->fault_dsisr;\r\nvcpu->arch.last_inst = svcpu->last_inst;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nvcpu->arch.shadow_fscr = svcpu->shadow_fscr;\r\n#endif\r\nvcpu->arch.purr += get_tb() - vcpu->arch.entry_tb;\r\nvcpu->arch.spurr += get_tb() - vcpu->arch.entry_tb;\r\nto_book3s(vcpu)->vtb += get_vtb() - vcpu->arch.entry_vtb;\r\nif (cpu_has_feature(CPU_FTR_ARCH_207S))\r\nvcpu->arch.ic += mfspr(SPRN_IC) - vcpu->arch.entry_ic;\r\nsvcpu->in_use = false;\r\nout:\r\npreempt_enable();\r\n}\r\nstatic int kvmppc_core_check_requests_pr(struct kvm_vcpu *vcpu)\r\n{\r\nint r = 1;\r\nif (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu))\r\nkvmppc_mmu_pte_flush(vcpu, 0, 0);\r\nreturn r;\r\n}\r\nstatic void do_kvm_unmap_hva(struct kvm *kvm, unsigned long start,\r\nunsigned long end)\r\n{\r\nlong i;\r\nstruct kvm_vcpu *vcpu;\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nslots = kvm_memslots(kvm);\r\nkvm_for_each_memslot(memslot, slots) {\r\nunsigned long hva_start, hva_end;\r\ngfn_t gfn, gfn_end;\r\nhva_start = max(start, memslot->userspace_addr);\r\nhva_end = min(end, memslot->userspace_addr +\r\n(memslot->npages << PAGE_SHIFT));\r\nif (hva_start >= hva_end)\r\ncontinue;\r\ngfn = hva_to_gfn_memslot(hva_start, memslot);\r\ngfn_end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, memslot);\r\nkvm_for_each_vcpu(i, vcpu, kvm)\r\nkvmppc_mmu_pte_pflush(vcpu, gfn << PAGE_SHIFT,\r\ngfn_end << PAGE_SHIFT);\r\n}\r\n}\r\nstatic int kvm_unmap_hva_pr(struct kvm *kvm, unsigned long hva)\r\n{\r\ntrace_kvm_unmap_hva(hva);\r\ndo_kvm_unmap_hva(kvm, hva, hva + PAGE_SIZE);\r\nreturn 0;\r\n}\r\nstatic int kvm_unmap_hva_range_pr(struct kvm *kvm, unsigned long start,\r\nunsigned long end)\r\n{\r\ndo_kvm_unmap_hva(kvm, start, end);\r\nreturn 0;\r\n}\r\nstatic int kvm_age_hva_pr(struct kvm *kvm, unsigned long start,\r\nunsigned long end)\r\n{\r\nreturn 0;\r\n}\r\nstatic int kvm_test_age_hva_pr(struct kvm *kvm, unsigned long hva)\r\n{\r\nreturn 0;\r\n}\r\nstatic void kvm_set_spte_hva_pr(struct kvm *kvm, unsigned long hva, pte_t pte)\r\n{\r\ndo_kvm_unmap_hva(kvm, hva, hva + PAGE_SIZE);\r\n}\r\nstatic void kvmppc_recalc_shadow_msr(struct kvm_vcpu *vcpu)\r\n{\r\nulong guest_msr = kvmppc_get_msr(vcpu);\r\nulong smsr = guest_msr;\r\nsmsr &= MSR_FE0 | MSR_FE1 | MSR_SF | MSR_SE | MSR_BE | MSR_LE;\r\nsmsr |= MSR_ME | MSR_RI | MSR_IR | MSR_DR | MSR_PR | MSR_EE;\r\nsmsr |= (guest_msr & vcpu->arch.guest_owned_ext);\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nsmsr |= MSR_ISF | MSR_HV;\r\n#endif\r\nvcpu->arch.shadow_msr = smsr;\r\n}\r\nstatic void kvmppc_set_msr_pr(struct kvm_vcpu *vcpu, u64 msr)\r\n{\r\nulong old_msr = kvmppc_get_msr(vcpu);\r\n#ifdef EXIT_DEBUG\r\nprintk(KERN_INFO "KVM: Set MSR to 0x%llx\n", msr);\r\n#endif\r\nmsr &= to_book3s(vcpu)->msr_mask;\r\nkvmppc_set_msr_fast(vcpu, msr);\r\nkvmppc_recalc_shadow_msr(vcpu);\r\nif (msr & MSR_POW) {\r\nif (!vcpu->arch.pending_exceptions) {\r\nkvm_vcpu_block(vcpu);\r\nkvm_clear_request(KVM_REQ_UNHALT, vcpu);\r\nvcpu->stat.halt_wakeup++;\r\nmsr &= ~MSR_POW;\r\nkvmppc_set_msr_fast(vcpu, msr);\r\n}\r\n}\r\nif (kvmppc_is_split_real(vcpu))\r\nkvmppc_fixup_split_real(vcpu);\r\nelse\r\nkvmppc_unfixup_split_real(vcpu);\r\nif ((kvmppc_get_msr(vcpu) & (MSR_PR|MSR_IR|MSR_DR)) !=\r\n(old_msr & (MSR_PR|MSR_IR|MSR_DR))) {\r\nkvmppc_mmu_flush_segments(vcpu);\r\nkvmppc_mmu_map_segment(vcpu, kvmppc_get_pc(vcpu));\r\nif (!(msr & MSR_PR) && vcpu->arch.magic_page_pa) {\r\nstruct kvm_vcpu_arch *a = &vcpu->arch;\r\nif (msr & MSR_DR)\r\nkvmppc_mmu_map_segment(vcpu, a->magic_page_ea);\r\nelse\r\nkvmppc_mmu_map_segment(vcpu, a->magic_page_pa);\r\n}\r\n}\r\nif (vcpu->arch.magic_page_pa &&\r\n!(old_msr & MSR_PR) && !(old_msr & MSR_SF) && (msr & MSR_SF)) {\r\nkvmppc_mmu_pte_flush(vcpu, (uint32_t)vcpu->arch.magic_page_pa,\r\n~0xFFFUL);\r\n}\r\nif (kvmppc_get_msr(vcpu) & MSR_FP)\r\nkvmppc_handle_ext(vcpu, BOOK3S_INTERRUPT_FP_UNAVAIL, MSR_FP);\r\n}\r\nvoid kvmppc_set_pvr_pr(struct kvm_vcpu *vcpu, u32 pvr)\r\n{\r\nu32 host_pvr;\r\nvcpu->arch.hflags &= ~BOOK3S_HFLAG_SLB;\r\nvcpu->arch.pvr = pvr;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nif ((pvr >= 0x330000) && (pvr < 0x70330000)) {\r\nkvmppc_mmu_book3s_64_init(vcpu);\r\nif (!to_book3s(vcpu)->hior_explicit)\r\nto_book3s(vcpu)->hior = 0xfff00000;\r\nto_book3s(vcpu)->msr_mask = 0xffffffffffffffffULL;\r\nvcpu->arch.cpu_type = KVM_CPU_3S_64;\r\n} else\r\n#endif\r\n{\r\nkvmppc_mmu_book3s_32_init(vcpu);\r\nif (!to_book3s(vcpu)->hior_explicit)\r\nto_book3s(vcpu)->hior = 0;\r\nto_book3s(vcpu)->msr_mask = 0xffffffffULL;\r\nvcpu->arch.cpu_type = KVM_CPU_3S_32;\r\n}\r\nkvmppc_sanity_check(vcpu);\r\nvcpu->arch.hflags &= ~BOOK3S_HFLAG_DCBZ32;\r\nif (vcpu->arch.mmu.is_dcbz32(vcpu) && (mfmsr() & MSR_HV) &&\r\n!strcmp(cur_cpu_spec->platform, "ppc970"))\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_DCBZ32;\r\nif (!strcmp(cur_cpu_spec->platform, "ppc-cell-be"))\r\nto_book3s(vcpu)->msr_mask &= ~(MSR_FE0 | MSR_FE1);\r\nswitch (PVR_VER(pvr)) {\r\ncase PVR_POWER6:\r\ncase PVR_POWER7:\r\ncase PVR_POWER7p:\r\ncase PVR_POWER8:\r\ncase PVR_POWER8E:\r\ncase PVR_POWER8NVL:\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_MULTI_PGSIZE |\r\nBOOK3S_HFLAG_NEW_TLBIE;\r\nbreak;\r\n}\r\n#ifdef CONFIG_PPC_BOOK3S_32\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_DCBZ32;\r\n#endif\r\nasm ( "mfpvr %0" : "=r"(host_pvr));\r\nswitch (host_pvr) {\r\ncase 0x00080200:\r\ncase 0x00088202:\r\ncase 0x70000100:\r\ncase 0x00080100:\r\ncase 0x00083203:\r\ncase 0x00083213:\r\ncase 0x00083204:\r\ncase 0x00083214:\r\ncase 0x00087200:\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_NATIVE_PS;\r\nmtspr(SPRN_HID2_GEKKO, mfspr(SPRN_HID2_GEKKO) | (1 << 29));\r\n}\r\n}\r\nstatic void kvmppc_patch_dcbz(struct kvm_vcpu *vcpu, struct kvmppc_pte *pte)\r\n{\r\nstruct page *hpage;\r\nu64 hpage_offset;\r\nu32 *page;\r\nint i;\r\nhpage = gfn_to_page(vcpu->kvm, pte->raddr >> PAGE_SHIFT);\r\nif (is_error_page(hpage))\r\nreturn;\r\nhpage_offset = pte->raddr & ~PAGE_MASK;\r\nhpage_offset &= ~0xFFFULL;\r\nhpage_offset /= 4;\r\nget_page(hpage);\r\npage = kmap_atomic(hpage);\r\nfor (i=hpage_offset; i < hpage_offset + (HW_PAGE_SIZE / 4); i++)\r\nif ((be32_to_cpu(page[i]) & 0xff0007ff) == INS_DCBZ)\r\npage[i] &= cpu_to_be32(0xfffffff7);\r\nkunmap_atomic(page);\r\nput_page(hpage);\r\n}\r\nstatic bool kvmppc_visible_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)\r\n{\r\nulong mp_pa = vcpu->arch.magic_page_pa;\r\nif (!(kvmppc_get_msr(vcpu) & MSR_SF))\r\nmp_pa = (uint32_t)mp_pa;\r\ngpa &= ~0xFFFULL;\r\nif (unlikely(mp_pa) && unlikely((mp_pa & KVM_PAM) == (gpa & KVM_PAM))) {\r\nreturn true;\r\n}\r\nreturn kvm_is_visible_gfn(vcpu->kvm, gpa >> PAGE_SHIFT);\r\n}\r\nint kvmppc_handle_pagefault(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nulong eaddr, int vec)\r\n{\r\nbool data = (vec == BOOK3S_INTERRUPT_DATA_STORAGE);\r\nbool iswrite = false;\r\nint r = RESUME_GUEST;\r\nint relocated;\r\nint page_found = 0;\r\nstruct kvmppc_pte pte = { 0 };\r\nbool dr = (kvmppc_get_msr(vcpu) & MSR_DR) ? true : false;\r\nbool ir = (kvmppc_get_msr(vcpu) & MSR_IR) ? true : false;\r\nu64 vsid;\r\nrelocated = data ? dr : ir;\r\nif (data && (vcpu->arch.fault_dsisr & DSISR_ISSTORE))\r\niswrite = true;\r\nif (relocated) {\r\npage_found = vcpu->arch.mmu.xlate(vcpu, eaddr, &pte, data, iswrite);\r\n} else {\r\npte.may_execute = true;\r\npte.may_read = true;\r\npte.may_write = true;\r\npte.raddr = eaddr & KVM_PAM;\r\npte.eaddr = eaddr;\r\npte.vpage = eaddr >> 12;\r\npte.page_size = MMU_PAGE_64K;\r\n}\r\nswitch (kvmppc_get_msr(vcpu) & (MSR_DR|MSR_IR)) {\r\ncase 0:\r\npte.vpage |= ((u64)VSID_REAL << (SID_SHIFT - 12));\r\nbreak;\r\ncase MSR_DR:\r\nif (!data &&\r\n(vcpu->arch.hflags & BOOK3S_HFLAG_SPLIT_HACK) &&\r\n((pte.raddr & SPLIT_HACK_MASK) == SPLIT_HACK_OFFS))\r\npte.raddr &= ~SPLIT_HACK_MASK;\r\ncase MSR_IR:\r\nvcpu->arch.mmu.esid_to_vsid(vcpu, eaddr >> SID_SHIFT, &vsid);\r\nif ((kvmppc_get_msr(vcpu) & (MSR_DR|MSR_IR)) == MSR_DR)\r\npte.vpage |= ((u64)VSID_REAL_DR << (SID_SHIFT - 12));\r\nelse\r\npte.vpage |= ((u64)VSID_REAL_IR << (SID_SHIFT - 12));\r\npte.vpage |= vsid;\r\nif (vsid == -1)\r\npage_found = -EINVAL;\r\nbreak;\r\n}\r\nif (vcpu->arch.mmu.is_dcbz32(vcpu) &&\r\n(!(vcpu->arch.hflags & BOOK3S_HFLAG_DCBZ32))) {\r\npte.may_execute = !data;\r\n}\r\nif (page_found == -ENOENT) {\r\nu64 ssrr1 = vcpu->arch.shadow_srr1;\r\nu64 msr = kvmppc_get_msr(vcpu);\r\nkvmppc_set_dar(vcpu, kvmppc_get_fault_dar(vcpu));\r\nkvmppc_set_dsisr(vcpu, vcpu->arch.fault_dsisr);\r\nkvmppc_set_msr_fast(vcpu, msr | (ssrr1 & 0xf8000000ULL));\r\nkvmppc_book3s_queue_irqprio(vcpu, vec);\r\n} else if (page_found == -EPERM) {\r\nu32 dsisr = vcpu->arch.fault_dsisr;\r\nu64 ssrr1 = vcpu->arch.shadow_srr1;\r\nu64 msr = kvmppc_get_msr(vcpu);\r\nkvmppc_set_dar(vcpu, kvmppc_get_fault_dar(vcpu));\r\ndsisr = (dsisr & ~DSISR_NOHPTE) | DSISR_PROTFAULT;\r\nkvmppc_set_dsisr(vcpu, dsisr);\r\nkvmppc_set_msr_fast(vcpu, msr | (ssrr1 & 0xf8000000ULL));\r\nkvmppc_book3s_queue_irqprio(vcpu, vec);\r\n} else if (page_found == -EINVAL) {\r\nkvmppc_set_dar(vcpu, kvmppc_get_fault_dar(vcpu));\r\nkvmppc_book3s_queue_irqprio(vcpu, vec + 0x80);\r\n} else if (kvmppc_visible_gpa(vcpu, pte.raddr)) {\r\nif (data && !(vcpu->arch.fault_dsisr & DSISR_NOHPTE)) {\r\nkvmppc_mmu_unmap_page(vcpu, &pte);\r\n}\r\nif (kvmppc_mmu_map_page(vcpu, &pte, iswrite) == -EIO) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\nif (data)\r\nvcpu->stat.sp_storage++;\r\nelse if (vcpu->arch.mmu.is_dcbz32(vcpu) &&\r\n(!(vcpu->arch.hflags & BOOK3S_HFLAG_DCBZ32)))\r\nkvmppc_patch_dcbz(vcpu, &pte);\r\n} else {\r\nvcpu->stat.mmio_exits++;\r\nvcpu->arch.paddr_accessed = pte.raddr;\r\nvcpu->arch.vaddr_accessed = pte.eaddr;\r\nr = kvmppc_emulate_mmio(run, vcpu);\r\nif ( r == RESUME_HOST_NV )\r\nr = RESUME_HOST;\r\n}\r\nreturn r;\r\n}\r\nvoid kvmppc_giveup_ext(struct kvm_vcpu *vcpu, ulong msr)\r\n{\r\nstruct thread_struct *t = &current->thread;\r\nif (msr & MSR_VSX)\r\nmsr |= MSR_FP | MSR_VEC;\r\nmsr &= vcpu->arch.guest_owned_ext;\r\nif (!msr)\r\nreturn;\r\n#ifdef DEBUG_EXT\r\nprintk(KERN_INFO "Giving up ext 0x%lx\n", msr);\r\n#endif\r\nif (msr & MSR_FP) {\r\nif (t->regs->msr & MSR_FP)\r\ngiveup_fpu(current);\r\nt->fp_save_area = NULL;\r\n}\r\n#ifdef CONFIG_ALTIVEC\r\nif (msr & MSR_VEC) {\r\nif (current->thread.regs->msr & MSR_VEC)\r\ngiveup_altivec(current);\r\nt->vr_save_area = NULL;\r\n}\r\n#endif\r\nvcpu->arch.guest_owned_ext &= ~(msr | MSR_VSX);\r\nkvmppc_recalc_shadow_msr(vcpu);\r\n}\r\nstatic void kvmppc_giveup_fac(struct kvm_vcpu *vcpu, ulong fac)\r\n{\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nif (!(vcpu->arch.shadow_fscr & (1ULL << fac))) {\r\nreturn;\r\n}\r\nswitch (fac) {\r\ncase FSCR_TAR_LG:\r\nvcpu->arch.tar = mfspr(SPRN_TAR);\r\nmtspr(SPRN_TAR, current->thread.tar);\r\nvcpu->arch.shadow_fscr &= ~FSCR_TAR;\r\nbreak;\r\n}\r\n#endif\r\n}\r\nstatic int kvmppc_handle_ext(struct kvm_vcpu *vcpu, unsigned int exit_nr,\r\nulong msr)\r\n{\r\nstruct thread_struct *t = &current->thread;\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_PAIRED_SINGLE)\r\nreturn RESUME_GUEST;\r\nif (!(kvmppc_get_msr(vcpu) & msr)) {\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nreturn RESUME_GUEST;\r\n}\r\nif (msr == MSR_VSX) {\r\n#ifdef CONFIG_VSX\r\nif (!cpu_has_feature(CPU_FTR_VSX))\r\n#endif\r\n{\r\nkvmppc_core_queue_program(vcpu, SRR1_PROGILL);\r\nreturn RESUME_GUEST;\r\n}\r\nmsr = MSR_FP | MSR_VEC | MSR_VSX;\r\n}\r\nmsr &= ~vcpu->arch.guest_owned_ext;\r\nif (!msr)\r\nreturn RESUME_GUEST;\r\n#ifdef DEBUG_EXT\r\nprintk(KERN_INFO "Loading up ext 0x%lx\n", msr);\r\n#endif\r\nif (msr & MSR_FP) {\r\npreempt_disable();\r\nenable_kernel_fp();\r\nload_fp_state(&vcpu->arch.fp);\r\ndisable_kernel_fp();\r\nt->fp_save_area = &vcpu->arch.fp;\r\npreempt_enable();\r\n}\r\nif (msr & MSR_VEC) {\r\n#ifdef CONFIG_ALTIVEC\r\npreempt_disable();\r\nenable_kernel_altivec();\r\nload_vr_state(&vcpu->arch.vr);\r\ndisable_kernel_altivec();\r\nt->vr_save_area = &vcpu->arch.vr;\r\npreempt_enable();\r\n#endif\r\n}\r\nt->regs->msr |= msr;\r\nvcpu->arch.guest_owned_ext |= msr;\r\nkvmppc_recalc_shadow_msr(vcpu);\r\nreturn RESUME_GUEST;\r\n}\r\nstatic void kvmppc_handle_lost_ext(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long lost_ext;\r\nlost_ext = vcpu->arch.guest_owned_ext & ~current->thread.regs->msr;\r\nif (!lost_ext)\r\nreturn;\r\nif (lost_ext & MSR_FP) {\r\npreempt_disable();\r\nenable_kernel_fp();\r\nload_fp_state(&vcpu->arch.fp);\r\ndisable_kernel_fp();\r\npreempt_enable();\r\n}\r\n#ifdef CONFIG_ALTIVEC\r\nif (lost_ext & MSR_VEC) {\r\npreempt_disable();\r\nenable_kernel_altivec();\r\nload_vr_state(&vcpu->arch.vr);\r\ndisable_kernel_altivec();\r\npreempt_enable();\r\n}\r\n#endif\r\ncurrent->thread.regs->msr |= lost_ext;\r\n}\r\nstatic void kvmppc_trigger_fac_interrupt(struct kvm_vcpu *vcpu, ulong fac)\r\n{\r\nvcpu->arch.fscr &= ~(0xffULL << 56);\r\nvcpu->arch.fscr |= (fac << 56);\r\nkvmppc_book3s_queue_irqprio(vcpu, BOOK3S_INTERRUPT_FAC_UNAVAIL);\r\n}\r\nstatic void kvmppc_emulate_fac(struct kvm_vcpu *vcpu, ulong fac)\r\n{\r\nenum emulation_result er = EMULATE_FAIL;\r\nif (!(kvmppc_get_msr(vcpu) & MSR_PR))\r\ner = kvmppc_emulate_instruction(vcpu->run, vcpu);\r\nif ((er != EMULATE_DONE) && (er != EMULATE_AGAIN)) {\r\nkvmppc_trigger_fac_interrupt(vcpu, fac);\r\n}\r\n}\r\nstatic int kvmppc_handle_fac(struct kvm_vcpu *vcpu, ulong fac)\r\n{\r\nbool guest_fac_enabled;\r\nBUG_ON(!cpu_has_feature(CPU_FTR_ARCH_207S));\r\nswitch (fac) {\r\ncase FSCR_TAR_LG:\r\ncase FSCR_EBB_LG:\r\nguest_fac_enabled = (vcpu->arch.fscr & (1ULL << fac));\r\nbreak;\r\ncase FSCR_TM_LG:\r\nguest_fac_enabled = kvmppc_get_msr(vcpu) & MSR_TM;\r\nbreak;\r\ndefault:\r\nguest_fac_enabled = false;\r\nbreak;\r\n}\r\nif (!guest_fac_enabled) {\r\nkvmppc_trigger_fac_interrupt(vcpu, fac);\r\nreturn RESUME_GUEST;\r\n}\r\nswitch (fac) {\r\ncase FSCR_TAR_LG:\r\ncurrent->thread.tar = mfspr(SPRN_TAR);\r\nmtspr(SPRN_TAR, vcpu->arch.tar);\r\nvcpu->arch.shadow_fscr |= FSCR_TAR;\r\nbreak;\r\ndefault:\r\nkvmppc_emulate_fac(vcpu, fac);\r\nbreak;\r\n}\r\nreturn RESUME_GUEST;\r\n}\r\nvoid kvmppc_set_fscr(struct kvm_vcpu *vcpu, u64 fscr)\r\n{\r\nif ((vcpu->arch.fscr & FSCR_TAR) && !(fscr & FSCR_TAR)) {\r\nkvmppc_giveup_fac(vcpu, FSCR_TAR_LG);\r\n}\r\nvcpu->arch.fscr = fscr;\r\n}\r\nstatic void kvmppc_setup_debug(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP) {\r\nu64 msr = kvmppc_get_msr(vcpu);\r\nkvmppc_set_msr(vcpu, msr | MSR_SE);\r\n}\r\n}\r\nstatic void kvmppc_clear_debug(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP) {\r\nu64 msr = kvmppc_get_msr(vcpu);\r\nkvmppc_set_msr(vcpu, msr & ~MSR_SE);\r\n}\r\n}\r\nstatic int kvmppc_exit_pr_progint(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int exit_nr)\r\n{\r\nenum emulation_result er;\r\nulong flags;\r\nu32 last_inst;\r\nint emul, r;\r\nif (exit_nr == BOOK3S_INTERRUPT_PROGRAM)\r\nflags = vcpu->arch.shadow_srr1 & 0x1f0000ull;\r\nelse\r\nflags = SRR1_PROGILL;\r\nemul = kvmppc_get_last_inst(vcpu, INST_GENERIC, &last_inst);\r\nif (emul != EMULATE_DONE)\r\nreturn RESUME_GUEST;\r\nif (kvmppc_get_msr(vcpu) & MSR_PR) {\r\n#ifdef EXIT_DEBUG\r\npr_info("Userspace triggered 0x700 exception at\n 0x%lx (0x%x)\n",\r\nkvmppc_get_pc(vcpu), last_inst);\r\n#endif\r\nif ((last_inst & 0xff0007ff) != (INS_DCBZ & 0xfffffff7)) {\r\nkvmppc_core_queue_program(vcpu, flags);\r\nreturn RESUME_GUEST;\r\n}\r\n}\r\nvcpu->stat.emulated_inst_exits++;\r\ner = kvmppc_emulate_instruction(run, vcpu);\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nr = RESUME_GUEST_NV;\r\nbreak;\r\ncase EMULATE_AGAIN:\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase EMULATE_FAIL:\r\npr_crit("%s: emulation at %lx failed (%08x)\n",\r\n__func__, kvmppc_get_pc(vcpu), last_inst);\r\nkvmppc_core_queue_program(vcpu, flags);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase EMULATE_DO_MMIO:\r\nrun->exit_reason = KVM_EXIT_MMIO;\r\nr = RESUME_HOST_NV;\r\nbreak;\r\ncase EMULATE_EXIT_USER:\r\nr = RESUME_HOST_NV;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn r;\r\n}\r\nint kvmppc_handle_exit_pr(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int exit_nr)\r\n{\r\nint r = RESUME_HOST;\r\nint s;\r\nvcpu->stat.sum_exits++;\r\nrun->exit_reason = KVM_EXIT_UNKNOWN;\r\nrun->ready_for_interrupt_injection = 1;\r\ntrace_kvm_exit(exit_nr, vcpu);\r\nguest_exit();\r\nswitch (exit_nr) {\r\ncase BOOK3S_INTERRUPT_INST_STORAGE:\r\n{\r\nulong shadow_srr1 = vcpu->arch.shadow_srr1;\r\nvcpu->stat.pf_instruc++;\r\nif (kvmppc_is_split_real(vcpu))\r\nkvmppc_fixup_split_real(vcpu);\r\n#ifdef CONFIG_PPC_BOOK3S_32\r\n{\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu;\r\nu32 sr;\r\nsvcpu = svcpu_get(vcpu);\r\nsr = svcpu->sr[kvmppc_get_pc(vcpu) >> SID_SHIFT];\r\nsvcpu_put(svcpu);\r\nif (sr == SR_INVALID) {\r\nkvmppc_mmu_map_segment(vcpu, kvmppc_get_pc(vcpu));\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\n}\r\n#endif\r\nif (shadow_srr1 & 0x40000000) {\r\nint idx = srcu_read_lock(&vcpu->kvm->srcu);\r\nr = kvmppc_handle_pagefault(run, vcpu, kvmppc_get_pc(vcpu), exit_nr);\r\nsrcu_read_unlock(&vcpu->kvm->srcu, idx);\r\nvcpu->stat.sp_instruc++;\r\n} else if (vcpu->arch.mmu.is_dcbz32(vcpu) &&\r\n(!(vcpu->arch.hflags & BOOK3S_HFLAG_DCBZ32))) {\r\nkvmppc_mmu_pte_flush(vcpu, kvmppc_get_pc(vcpu), ~0xFFFUL);\r\nr = RESUME_GUEST;\r\n} else {\r\nu64 msr = kvmppc_get_msr(vcpu);\r\nmsr |= shadow_srr1 & 0x58000000;\r\nkvmppc_set_msr_fast(vcpu, msr);\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nr = RESUME_GUEST;\r\n}\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_DATA_STORAGE:\r\n{\r\nulong dar = kvmppc_get_fault_dar(vcpu);\r\nu32 fault_dsisr = vcpu->arch.fault_dsisr;\r\nvcpu->stat.pf_storage++;\r\n#ifdef CONFIG_PPC_BOOK3S_32\r\n{\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu;\r\nu32 sr;\r\nsvcpu = svcpu_get(vcpu);\r\nsr = svcpu->sr[dar >> SID_SHIFT];\r\nsvcpu_put(svcpu);\r\nif (sr == SR_INVALID) {\r\nkvmppc_mmu_map_segment(vcpu, dar);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\n}\r\n#endif\r\nif (fault_dsisr & (DSISR_NOHPTE | DSISR_PROTFAULT)) {\r\nint idx = srcu_read_lock(&vcpu->kvm->srcu);\r\nr = kvmppc_handle_pagefault(run, vcpu, dar, exit_nr);\r\nsrcu_read_unlock(&vcpu->kvm->srcu, idx);\r\n} else {\r\nkvmppc_set_dar(vcpu, dar);\r\nkvmppc_set_dsisr(vcpu, fault_dsisr);\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nr = RESUME_GUEST;\r\n}\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_DATA_SEGMENT:\r\nif (kvmppc_mmu_map_segment(vcpu, kvmppc_get_fault_dar(vcpu)) < 0) {\r\nkvmppc_set_dar(vcpu, kvmppc_get_fault_dar(vcpu));\r\nkvmppc_book3s_queue_irqprio(vcpu,\r\nBOOK3S_INTERRUPT_DATA_SEGMENT);\r\n}\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_INST_SEGMENT:\r\nif (kvmppc_mmu_map_segment(vcpu, kvmppc_get_pc(vcpu)) < 0) {\r\nkvmppc_book3s_queue_irqprio(vcpu,\r\nBOOK3S_INTERRUPT_INST_SEGMENT);\r\n}\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_DECREMENTER:\r\ncase BOOK3S_INTERRUPT_HV_DECREMENTER:\r\ncase BOOK3S_INTERRUPT_DOORBELL:\r\ncase BOOK3S_INTERRUPT_H_DOORBELL:\r\nvcpu->stat.dec_exits++;\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_EXTERNAL:\r\ncase BOOK3S_INTERRUPT_EXTERNAL_LEVEL:\r\ncase BOOK3S_INTERRUPT_EXTERNAL_HV:\r\nvcpu->stat.ext_intr_exits++;\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_PERFMON:\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_PROGRAM:\r\ncase BOOK3S_INTERRUPT_H_EMUL_ASSIST:\r\nr = kvmppc_exit_pr_progint(run, vcpu, exit_nr);\r\nbreak;\r\ncase BOOK3S_INTERRUPT_SYSCALL:\r\n{\r\nu32 last_sc;\r\nint emul;\r\nif (vcpu->arch.papr_enabled) {\r\nemul = kvmppc_get_last_inst(vcpu, INST_SC, &last_sc);\r\nif (emul != EMULATE_DONE) {\r\nkvmppc_set_pc(vcpu, kvmppc_get_pc(vcpu) - 4);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\n}\r\nif (vcpu->arch.papr_enabled &&\r\n(last_sc == 0x44000022) &&\r\n!(kvmppc_get_msr(vcpu) & MSR_PR)) {\r\nulong cmd = kvmppc_get_gpr(vcpu, 3);\r\nint i;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nif (kvmppc_h_pr(vcpu, cmd) == EMULATE_DONE) {\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\n#endif\r\nrun->papr_hcall.nr = cmd;\r\nfor (i = 0; i < 9; ++i) {\r\nulong gpr = kvmppc_get_gpr(vcpu, 4 + i);\r\nrun->papr_hcall.args[i] = gpr;\r\n}\r\nrun->exit_reason = KVM_EXIT_PAPR_HCALL;\r\nvcpu->arch.hcall_needed = 1;\r\nr = RESUME_HOST;\r\n} else if (vcpu->arch.osi_enabled &&\r\n(((u32)kvmppc_get_gpr(vcpu, 3)) == OSI_SC_MAGIC_R3) &&\r\n(((u32)kvmppc_get_gpr(vcpu, 4)) == OSI_SC_MAGIC_R4)) {\r\nu64 *gprs = run->osi.gprs;\r\nint i;\r\nrun->exit_reason = KVM_EXIT_OSI;\r\nfor (i = 0; i < 32; i++)\r\ngprs[i] = kvmppc_get_gpr(vcpu, i);\r\nvcpu->arch.osi_needed = 1;\r\nr = RESUME_HOST_NV;\r\n} else if (!(kvmppc_get_msr(vcpu) & MSR_PR) &&\r\n(((u32)kvmppc_get_gpr(vcpu, 0)) == KVM_SC_MAGIC_R0)) {\r\nkvmppc_set_gpr(vcpu, 3, kvmppc_kvm_pv(vcpu));\r\nr = RESUME_GUEST;\r\n} else {\r\nvcpu->stat.syscall_exits++;\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nr = RESUME_GUEST;\r\n}\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_FP_UNAVAIL:\r\ncase BOOK3S_INTERRUPT_ALTIVEC:\r\ncase BOOK3S_INTERRUPT_VSX:\r\n{\r\nint ext_msr = 0;\r\nint emul;\r\nu32 last_inst;\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_PAIRED_SINGLE) {\r\nemul = kvmppc_get_last_inst(vcpu, INST_GENERIC,\r\n&last_inst);\r\nif (emul == EMULATE_DONE)\r\nr = kvmppc_exit_pr_progint(run, vcpu, exit_nr);\r\nelse\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\nswitch (exit_nr) {\r\ncase BOOK3S_INTERRUPT_FP_UNAVAIL:\r\next_msr = MSR_FP;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_ALTIVEC:\r\next_msr = MSR_VEC;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_VSX:\r\next_msr = MSR_VSX;\r\nbreak;\r\n}\r\nr = kvmppc_handle_ext(vcpu, exit_nr, ext_msr);\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_ALIGNMENT:\r\n{\r\nu32 last_inst;\r\nint emul = kvmppc_get_last_inst(vcpu, INST_GENERIC, &last_inst);\r\nif (emul == EMULATE_DONE) {\r\nu32 dsisr;\r\nu64 dar;\r\ndsisr = kvmppc_alignment_dsisr(vcpu, last_inst);\r\ndar = kvmppc_alignment_dar(vcpu, last_inst);\r\nkvmppc_set_dsisr(vcpu, dsisr);\r\nkvmppc_set_dar(vcpu, dar);\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\n}\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\ncase BOOK3S_INTERRUPT_FAC_UNAVAIL:\r\nkvmppc_handle_fac(vcpu, vcpu->arch.shadow_fscr >> 56);\r\nr = RESUME_GUEST;\r\nbreak;\r\n#endif\r\ncase BOOK3S_INTERRUPT_MACHINE_CHECK:\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_TRACE:\r\nif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP) {\r\nrun->exit_reason = KVM_EXIT_DEBUG;\r\nr = RESUME_HOST;\r\n} else {\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nr = RESUME_GUEST;\r\n}\r\nbreak;\r\ndefault:\r\n{\r\nulong shadow_srr1 = vcpu->arch.shadow_srr1;\r\nprintk(KERN_EMERG "exit_nr=0x%x | pc=0x%lx | msr=0x%lx\n",\r\nexit_nr, kvmppc_get_pc(vcpu), shadow_srr1);\r\nr = RESUME_HOST;\r\nBUG();\r\nbreak;\r\n}\r\n}\r\nif (!(r & RESUME_HOST)) {\r\ns = kvmppc_prepare_to_enter(vcpu);\r\nif (s <= 0)\r\nr = s;\r\nelse {\r\nkvmppc_fix_ee_before_entry();\r\n}\r\nkvmppc_handle_lost_ext(vcpu);\r\n}\r\ntrace_kvm_book3s_reenter(r, vcpu);\r\nreturn r;\r\n}\r\nstatic int kvm_arch_vcpu_ioctl_get_sregs_pr(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nint i;\r\nsregs->pvr = vcpu->arch.pvr;\r\nsregs->u.s.sdr1 = to_book3s(vcpu)->sdr1;\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_SLB) {\r\nfor (i = 0; i < 64; i++) {\r\nsregs->u.s.ppc64.slb[i].slbe = vcpu->arch.slb[i].orige | i;\r\nsregs->u.s.ppc64.slb[i].slbv = vcpu->arch.slb[i].origv;\r\n}\r\n} else {\r\nfor (i = 0; i < 16; i++)\r\nsregs->u.s.ppc32.sr[i] = kvmppc_get_sr(vcpu, i);\r\nfor (i = 0; i < 8; i++) {\r\nsregs->u.s.ppc32.ibat[i] = vcpu3s->ibat[i].raw;\r\nsregs->u.s.ppc32.dbat[i] = vcpu3s->dbat[i].raw;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int kvm_arch_vcpu_ioctl_set_sregs_pr(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nint i;\r\nkvmppc_set_pvr_pr(vcpu, sregs->pvr);\r\nvcpu3s->sdr1 = sregs->u.s.sdr1;\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_SLB) {\r\nfor (i = 0; i < 64; i++) {\r\nvcpu->arch.mmu.slbmte(vcpu, sregs->u.s.ppc64.slb[i].slbv,\r\nsregs->u.s.ppc64.slb[i].slbe);\r\n}\r\n} else {\r\nfor (i = 0; i < 16; i++) {\r\nvcpu->arch.mmu.mtsrin(vcpu, i, sregs->u.s.ppc32.sr[i]);\r\n}\r\nfor (i = 0; i < 8; i++) {\r\nkvmppc_set_bat(vcpu, &(vcpu3s->ibat[i]), false,\r\n(u32)sregs->u.s.ppc32.ibat[i]);\r\nkvmppc_set_bat(vcpu, &(vcpu3s->ibat[i]), true,\r\n(u32)(sregs->u.s.ppc32.ibat[i] >> 32));\r\nkvmppc_set_bat(vcpu, &(vcpu3s->dbat[i]), false,\r\n(u32)sregs->u.s.ppc32.dbat[i]);\r\nkvmppc_set_bat(vcpu, &(vcpu3s->dbat[i]), true,\r\n(u32)(sregs->u.s.ppc32.dbat[i] >> 32));\r\n}\r\n}\r\nkvmppc_mmu_pte_flush(vcpu, 0, 0);\r\nreturn 0;\r\n}\r\nstatic int kvmppc_get_one_reg_pr(struct kvm_vcpu *vcpu, u64 id,\r\nunion kvmppc_one_reg *val)\r\n{\r\nint r = 0;\r\nswitch (id) {\r\ncase KVM_REG_PPC_DEBUG_INST:\r\n*val = get_reg_val(id, KVMPPC_INST_SW_BREAKPOINT);\r\nbreak;\r\ncase KVM_REG_PPC_HIOR:\r\n*val = get_reg_val(id, to_book3s(vcpu)->hior);\r\nbreak;\r\ncase KVM_REG_PPC_VTB:\r\n*val = get_reg_val(id, to_book3s(vcpu)->vtb);\r\nbreak;\r\ncase KVM_REG_PPC_LPCR:\r\ncase KVM_REG_PPC_LPCR_64:\r\nif (vcpu->arch.intr_msr & MSR_LE)\r\n*val = get_reg_val(id, LPCR_ILE);\r\nelse\r\n*val = get_reg_val(id, 0);\r\nbreak;\r\ndefault:\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstatic void kvmppc_set_lpcr_pr(struct kvm_vcpu *vcpu, u64 new_lpcr)\r\n{\r\nif (new_lpcr & LPCR_ILE)\r\nvcpu->arch.intr_msr |= MSR_LE;\r\nelse\r\nvcpu->arch.intr_msr &= ~MSR_LE;\r\n}\r\nstatic int kvmppc_set_one_reg_pr(struct kvm_vcpu *vcpu, u64 id,\r\nunion kvmppc_one_reg *val)\r\n{\r\nint r = 0;\r\nswitch (id) {\r\ncase KVM_REG_PPC_HIOR:\r\nto_book3s(vcpu)->hior = set_reg_val(id, *val);\r\nto_book3s(vcpu)->hior_explicit = true;\r\nbreak;\r\ncase KVM_REG_PPC_VTB:\r\nto_book3s(vcpu)->vtb = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_LPCR:\r\ncase KVM_REG_PPC_LPCR_64:\r\nkvmppc_set_lpcr_pr(vcpu, set_reg_val(id, *val));\r\nbreak;\r\ndefault:\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstatic struct kvm_vcpu *kvmppc_core_vcpu_create_pr(struct kvm *kvm,\r\nunsigned int id)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu_book3s;\r\nstruct kvm_vcpu *vcpu;\r\nint err = -ENOMEM;\r\nunsigned long p;\r\nvcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);\r\nif (!vcpu)\r\ngoto out;\r\nvcpu_book3s = vzalloc(sizeof(struct kvmppc_vcpu_book3s));\r\nif (!vcpu_book3s)\r\ngoto free_vcpu;\r\nvcpu->arch.book3s = vcpu_book3s;\r\n#ifdef CONFIG_KVM_BOOK3S_32_HANDLER\r\nvcpu->arch.shadow_vcpu =\r\nkzalloc(sizeof(*vcpu->arch.shadow_vcpu), GFP_KERNEL);\r\nif (!vcpu->arch.shadow_vcpu)\r\ngoto free_vcpu3s;\r\n#endif\r\nerr = kvm_vcpu_init(vcpu, kvm, id);\r\nif (err)\r\ngoto free_shadow_vcpu;\r\nerr = -ENOMEM;\r\np = __get_free_page(GFP_KERNEL|__GFP_ZERO);\r\nif (!p)\r\ngoto uninit_vcpu;\r\nvcpu->arch.shared = (void *)p;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\n#ifdef __BIG_ENDIAN__\r\nvcpu->arch.shared_big_endian = true;\r\n#else\r\nvcpu->arch.shared_big_endian = false;\r\n#endif\r\nvcpu->arch.pvr = 0x3C0301;\r\nif (mmu_has_feature(MMU_FTR_1T_SEGMENT))\r\nvcpu->arch.pvr = mfspr(SPRN_PVR);\r\nvcpu->arch.intr_msr = MSR_SF;\r\n#else\r\nvcpu->arch.pvr = 0x84202;\r\n#endif\r\nkvmppc_set_pvr_pr(vcpu, vcpu->arch.pvr);\r\nvcpu->arch.slb_nr = 64;\r\nvcpu->arch.shadow_msr = MSR_USER64 & ~MSR_LE;\r\nerr = kvmppc_mmu_init(vcpu);\r\nif (err < 0)\r\ngoto uninit_vcpu;\r\nreturn vcpu;\r\nuninit_vcpu:\r\nkvm_vcpu_uninit(vcpu);\r\nfree_shadow_vcpu:\r\n#ifdef CONFIG_KVM_BOOK3S_32_HANDLER\r\nkfree(vcpu->arch.shadow_vcpu);\r\nfree_vcpu3s:\r\n#endif\r\nvfree(vcpu_book3s);\r\nfree_vcpu:\r\nkmem_cache_free(kvm_vcpu_cache, vcpu);\r\nout:\r\nreturn ERR_PTR(err);\r\n}\r\nstatic void kvmppc_core_vcpu_free_pr(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu_book3s = to_book3s(vcpu);\r\nfree_page((unsigned long)vcpu->arch.shared & PAGE_MASK);\r\nkvm_vcpu_uninit(vcpu);\r\n#ifdef CONFIG_KVM_BOOK3S_32_HANDLER\r\nkfree(vcpu->arch.shadow_vcpu);\r\n#endif\r\nvfree(vcpu_book3s);\r\nkmem_cache_free(kvm_vcpu_cache, vcpu);\r\n}\r\nstatic int kvmppc_vcpu_run_pr(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)\r\n{\r\nint ret;\r\n#ifdef CONFIG_ALTIVEC\r\nunsigned long uninitialized_var(vrsave);\r\n#endif\r\nif (!vcpu->arch.sane) {\r\nkvm_run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nkvmppc_setup_debug(vcpu);\r\nret = kvmppc_prepare_to_enter(vcpu);\r\nif (ret <= 0)\r\ngoto out;\r\ngiveup_all(current);\r\nif (kvmppc_get_msr(vcpu) & MSR_FP)\r\nkvmppc_handle_ext(vcpu, BOOK3S_INTERRUPT_FP_UNAVAIL, MSR_FP);\r\nkvmppc_fix_ee_before_entry();\r\nret = __kvmppc_vcpu_run(kvm_run, vcpu);\r\nkvmppc_clear_debug(vcpu);\r\nkvmppc_giveup_ext(vcpu, MSR_FP | MSR_VEC | MSR_VSX);\r\nkvmppc_giveup_fac(vcpu, FSCR_TAR_LG);\r\nout:\r\nvcpu->mode = OUTSIDE_GUEST_MODE;\r\nreturn ret;\r\n}\r\nstatic int kvm_vm_ioctl_get_dirty_log_pr(struct kvm *kvm,\r\nstruct kvm_dirty_log *log)\r\n{\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nstruct kvm_vcpu *vcpu;\r\nulong ga, ga_end;\r\nint is_dirty = 0;\r\nint r;\r\nunsigned long n;\r\nmutex_lock(&kvm->slots_lock);\r\nr = kvm_get_dirty_log(kvm, log, &is_dirty);\r\nif (r)\r\ngoto out;\r\nif (is_dirty) {\r\nslots = kvm_memslots(kvm);\r\nmemslot = id_to_memslot(slots, log->slot);\r\nga = memslot->base_gfn << PAGE_SHIFT;\r\nga_end = ga + (memslot->npages << PAGE_SHIFT);\r\nkvm_for_each_vcpu(n, vcpu, kvm)\r\nkvmppc_mmu_pte_pflush(vcpu, ga, ga_end);\r\nn = kvm_dirty_bitmap_bytes(memslot);\r\nmemset(memslot->dirty_bitmap, 0, n);\r\n}\r\nr = 0;\r\nout:\r\nmutex_unlock(&kvm->slots_lock);\r\nreturn r;\r\n}\r\nstatic void kvmppc_core_flush_memslot_pr(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot)\r\n{\r\nreturn;\r\n}\r\nstatic int kvmppc_core_prepare_memory_region_pr(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot,\r\nconst struct kvm_userspace_memory_region *mem)\r\n{\r\nreturn 0;\r\n}\r\nstatic void kvmppc_core_commit_memory_region_pr(struct kvm *kvm,\r\nconst struct kvm_userspace_memory_region *mem,\r\nconst struct kvm_memory_slot *old,\r\nconst struct kvm_memory_slot *new)\r\n{\r\nreturn;\r\n}\r\nstatic void kvmppc_core_free_memslot_pr(struct kvm_memory_slot *free,\r\nstruct kvm_memory_slot *dont)\r\n{\r\nreturn;\r\n}\r\nstatic int kvmppc_core_create_memslot_pr(struct kvm_memory_slot *slot,\r\nunsigned long npages)\r\n{\r\nreturn 0;\r\n}\r\nstatic int kvm_vm_ioctl_get_smmu_info_pr(struct kvm *kvm,\r\nstruct kvm_ppc_smmu_info *info)\r\n{\r\nlong int i;\r\nstruct kvm_vcpu *vcpu;\r\ninfo->flags = 0;\r\ninfo->slb_size = 64;\r\ninfo->sps[0].page_shift = 12;\r\ninfo->sps[0].slb_enc = 0;\r\ninfo->sps[0].enc[0].page_shift = 12;\r\ninfo->sps[0].enc[0].pte_enc = 0;\r\ni = 1;\r\nvcpu = kvm_get_vcpu(kvm, 0);\r\nif (vcpu && (vcpu->arch.hflags & BOOK3S_HFLAG_MULTI_PGSIZE)) {\r\ninfo->flags = KVM_PPC_1T_SEGMENTS;\r\ninfo->sps[i].page_shift = 16;\r\ninfo->sps[i].slb_enc = SLB_VSID_L | SLB_VSID_LP_01;\r\ninfo->sps[i].enc[0].page_shift = 16;\r\ninfo->sps[i].enc[0].pte_enc = 1;\r\n++i;\r\n}\r\ninfo->sps[i].page_shift = 24;\r\ninfo->sps[i].slb_enc = SLB_VSID_L;\r\ninfo->sps[i].enc[0].page_shift = 24;\r\ninfo->sps[i].enc[0].pte_enc = 0;\r\nreturn 0;\r\n}\r\nstatic int kvm_vm_ioctl_get_smmu_info_pr(struct kvm *kvm,\r\nstruct kvm_ppc_smmu_info *info)\r\n{\r\nBUG();\r\n}\r\nstatic int kvmppc_core_init_vm_pr(struct kvm *kvm)\r\n{\r\nmutex_init(&kvm->arch.hpt_mutex);\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nkvmppc_pr_init_default_hcalls(kvm);\r\n#endif\r\nif (firmware_has_feature(FW_FEATURE_SET_MODE)) {\r\nspin_lock(&kvm_global_user_count_lock);\r\nif (++kvm_global_user_count == 1)\r\npseries_disable_reloc_on_exc();\r\nspin_unlock(&kvm_global_user_count_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic void kvmppc_core_destroy_vm_pr(struct kvm *kvm)\r\n{\r\n#ifdef CONFIG_PPC64\r\nWARN_ON(!list_empty(&kvm->arch.spapr_tce_tables));\r\n#endif\r\nif (firmware_has_feature(FW_FEATURE_SET_MODE)) {\r\nspin_lock(&kvm_global_user_count_lock);\r\nBUG_ON(kvm_global_user_count == 0);\r\nif (--kvm_global_user_count == 0)\r\npseries_enable_reloc_on_exc();\r\nspin_unlock(&kvm_global_user_count_lock);\r\n}\r\n}\r\nstatic int kvmppc_core_check_processor_compat_pr(void)\r\n{\r\nif (cpu_has_feature(CPU_FTR_ARCH_300))\r\nreturn -EIO;\r\nreturn 0;\r\n}\r\nstatic long kvm_arch_vm_ioctl_pr(struct file *filp,\r\nunsigned int ioctl, unsigned long arg)\r\n{\r\nreturn -ENOTTY;\r\n}\r\nint kvmppc_book3s_init_pr(void)\r\n{\r\nint r;\r\nr = kvmppc_core_check_processor_compat_pr();\r\nif (r < 0)\r\nreturn r;\r\nkvm_ops_pr.owner = THIS_MODULE;\r\nkvmppc_pr_ops = &kvm_ops_pr;\r\nr = kvmppc_mmu_hpte_sysinit();\r\nreturn r;\r\n}\r\nvoid kvmppc_book3s_exit_pr(void)\r\n{\r\nkvmppc_pr_ops = NULL;\r\nkvmppc_mmu_hpte_sysexit();\r\n}
