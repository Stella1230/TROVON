static inline struct chcr_aead_ctx *AEAD_CTX(struct chcr_context *ctx)\r\n{\r\nreturn ctx->crypto_ctx->aeadctx;\r\n}\r\nstatic inline struct ablk_ctx *ABLK_CTX(struct chcr_context *ctx)\r\n{\r\nreturn ctx->crypto_ctx->ablkctx;\r\n}\r\nstatic inline struct hmac_ctx *HMAC_CTX(struct chcr_context *ctx)\r\n{\r\nreturn ctx->crypto_ctx->hmacctx;\r\n}\r\nstatic inline struct chcr_gcm_ctx *GCM_CTX(struct chcr_aead_ctx *gctx)\r\n{\r\nreturn gctx->ctx->gcm;\r\n}\r\nstatic inline struct chcr_authenc_ctx *AUTHENC_CTX(struct chcr_aead_ctx *gctx)\r\n{\r\nreturn gctx->ctx->authenc;\r\n}\r\nstatic inline struct uld_ctx *ULD_CTX(struct chcr_context *ctx)\r\n{\r\nreturn ctx->dev->u_ctx;\r\n}\r\nstatic inline int is_ofld_imm(const struct sk_buff *skb)\r\n{\r\nreturn (skb->len <= CRYPTO_MAX_IMM_TX_PKT_LEN);\r\n}\r\nstatic inline unsigned int sgl_len(unsigned int n)\r\n{\r\nn--;\r\nreturn (3 * n) / 2 + (n & 1) + 2;\r\n}\r\nstatic void chcr_verify_tag(struct aead_request *req, u8 *input, int *err)\r\n{\r\nu8 temp[SHA512_DIGEST_SIZE];\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nint authsize = crypto_aead_authsize(tfm);\r\nstruct cpl_fw6_pld *fw6_pld;\r\nint cmp = 0;\r\nfw6_pld = (struct cpl_fw6_pld *)input;\r\nif ((get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106) ||\r\n(get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_GCM)) {\r\ncmp = crypto_memneq(&fw6_pld->data[2], (fw6_pld + 1), authsize);\r\n} else {\r\nsg_pcopy_to_buffer(req->src, sg_nents(req->src), temp,\r\nauthsize, req->assoclen +\r\nreq->cryptlen - authsize);\r\ncmp = crypto_memneq(temp, (fw6_pld + 1), authsize);\r\n}\r\nif (cmp)\r\n*err = -EBADMSG;\r\nelse\r\n*err = 0;\r\n}\r\nint chcr_handle_resp(struct crypto_async_request *req, unsigned char *input,\r\nint err)\r\n{\r\nstruct crypto_tfm *tfm = req->tfm;\r\nstruct chcr_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct uld_ctx *u_ctx = ULD_CTX(ctx);\r\nstruct chcr_req_ctx ctx_req;\r\nunsigned int digestsize, updated_digestsize;\r\nstruct adapter *adap = padap(ctx->dev);\r\nswitch (tfm->__crt_alg->cra_flags & CRYPTO_ALG_TYPE_MASK) {\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\nctx_req.req.aead_req = aead_request_cast(req);\r\nctx_req.ctx.reqctx = aead_request_ctx(ctx_req.req.aead_req);\r\ndma_unmap_sg(&u_ctx->lldi.pdev->dev, ctx_req.ctx.reqctx->dst,\r\nctx_req.ctx.reqctx->dst_nents, DMA_FROM_DEVICE);\r\nif (ctx_req.ctx.reqctx->skb) {\r\nkfree_skb(ctx_req.ctx.reqctx->skb);\r\nctx_req.ctx.reqctx->skb = NULL;\r\n}\r\nfree_new_sg(ctx_req.ctx.reqctx->newdstsg);\r\nctx_req.ctx.reqctx->newdstsg = NULL;\r\nif (ctx_req.ctx.reqctx->verify == VERIFY_SW) {\r\nchcr_verify_tag(ctx_req.req.aead_req, input,\r\n&err);\r\nctx_req.ctx.reqctx->verify = VERIFY_HW;\r\n}\r\nctx_req.req.aead_req->base.complete(req, err);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\nerr = chcr_handle_cipher_resp(ablkcipher_request_cast(req),\r\ninput, err);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\nctx_req.req.ahash_req = ahash_request_cast(req);\r\nctx_req.ctx.ahash_ctx =\r\nahash_request_ctx(ctx_req.req.ahash_req);\r\ndigestsize =\r\ncrypto_ahash_digestsize(crypto_ahash_reqtfm(\r\nctx_req.req.ahash_req));\r\nupdated_digestsize = digestsize;\r\nif (digestsize == SHA224_DIGEST_SIZE)\r\nupdated_digestsize = SHA256_DIGEST_SIZE;\r\nelse if (digestsize == SHA384_DIGEST_SIZE)\r\nupdated_digestsize = SHA512_DIGEST_SIZE;\r\nif (ctx_req.ctx.ahash_ctx->skb) {\r\nkfree_skb(ctx_req.ctx.ahash_ctx->skb);\r\nctx_req.ctx.ahash_ctx->skb = NULL;\r\n}\r\nif (ctx_req.ctx.ahash_ctx->result == 1) {\r\nctx_req.ctx.ahash_ctx->result = 0;\r\nmemcpy(ctx_req.req.ahash_req->result, input +\r\nsizeof(struct cpl_fw6_pld),\r\ndigestsize);\r\n} else {\r\nmemcpy(ctx_req.ctx.ahash_ctx->partial_hash, input +\r\nsizeof(struct cpl_fw6_pld),\r\nupdated_digestsize);\r\n}\r\nctx_req.req.ahash_req->base.complete(req, err);\r\nbreak;\r\n}\r\natomic_inc(&adap->chcr_stats.complete);\r\nreturn err;\r\n}\r\nstatic inline unsigned int calc_tx_flits_ofld(const struct sk_buff *skb)\r\n{\r\nunsigned int flits, cnt;\r\nif (is_ofld_imm(skb))\r\nreturn DIV_ROUND_UP(skb->len, 8);\r\nflits = skb_transport_offset(skb) / 8;\r\ncnt = skb_shinfo(skb)->nr_frags;\r\nif (skb_tail_pointer(skb) != skb_transport_header(skb))\r\ncnt++;\r\nreturn flits + sgl_len(cnt);\r\n}\r\nstatic inline void get_aes_decrypt_key(unsigned char *dec_key,\r\nconst unsigned char *key,\r\nunsigned int keylength)\r\n{\r\nu32 temp;\r\nu32 w_ring[MAX_NK];\r\nint i, j, k;\r\nu8 nr, nk;\r\nswitch (keylength) {\r\ncase AES_KEYLENGTH_128BIT:\r\nnk = KEYLENGTH_4BYTES;\r\nnr = NUMBER_OF_ROUNDS_10;\r\nbreak;\r\ncase AES_KEYLENGTH_192BIT:\r\nnk = KEYLENGTH_6BYTES;\r\nnr = NUMBER_OF_ROUNDS_12;\r\nbreak;\r\ncase AES_KEYLENGTH_256BIT:\r\nnk = KEYLENGTH_8BYTES;\r\nnr = NUMBER_OF_ROUNDS_14;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\nfor (i = 0; i < nk; i++)\r\nw_ring[i] = be32_to_cpu(*(u32 *)&key[4 * i]);\r\ni = 0;\r\ntemp = w_ring[nk - 1];\r\nwhile (i + nk < (nr + 1) * 4) {\r\nif (!(i % nk)) {\r\ntemp = (temp << 8) | (temp >> 24);\r\ntemp = aes_ks_subword(temp);\r\ntemp ^= round_constant[i / nk];\r\n} else if (nk == 8 && (i % 4 == 0)) {\r\ntemp = aes_ks_subword(temp);\r\n}\r\nw_ring[i % nk] ^= temp;\r\ntemp = w_ring[i % nk];\r\ni++;\r\n}\r\ni--;\r\nfor (k = 0, j = i % nk; k < nk; k++) {\r\n*((u32 *)dec_key + k) = htonl(w_ring[j]);\r\nj--;\r\nif (j < 0)\r\nj += nk;\r\n}\r\n}\r\nstatic struct crypto_shash *chcr_alloc_shash(unsigned int ds)\r\n{\r\nstruct crypto_shash *base_hash = ERR_PTR(-EINVAL);\r\nswitch (ds) {\r\ncase SHA1_DIGEST_SIZE:\r\nbase_hash = crypto_alloc_shash("sha1", 0, 0);\r\nbreak;\r\ncase SHA224_DIGEST_SIZE:\r\nbase_hash = crypto_alloc_shash("sha224", 0, 0);\r\nbreak;\r\ncase SHA256_DIGEST_SIZE:\r\nbase_hash = crypto_alloc_shash("sha256", 0, 0);\r\nbreak;\r\ncase SHA384_DIGEST_SIZE:\r\nbase_hash = crypto_alloc_shash("sha384", 0, 0);\r\nbreak;\r\ncase SHA512_DIGEST_SIZE:\r\nbase_hash = crypto_alloc_shash("sha512", 0, 0);\r\nbreak;\r\n}\r\nreturn base_hash;\r\n}\r\nstatic int chcr_compute_partial_hash(struct shash_desc *desc,\r\nchar *iopad, char *result_hash,\r\nint digest_size)\r\n{\r\nstruct sha1_state sha1_st;\r\nstruct sha256_state sha256_st;\r\nstruct sha512_state sha512_st;\r\nint error;\r\nif (digest_size == SHA1_DIGEST_SIZE) {\r\nerror = crypto_shash_init(desc) ?:\r\ncrypto_shash_update(desc, iopad, SHA1_BLOCK_SIZE) ?:\r\ncrypto_shash_export(desc, (void *)&sha1_st);\r\nmemcpy(result_hash, sha1_st.state, SHA1_DIGEST_SIZE);\r\n} else if (digest_size == SHA224_DIGEST_SIZE) {\r\nerror = crypto_shash_init(desc) ?:\r\ncrypto_shash_update(desc, iopad, SHA256_BLOCK_SIZE) ?:\r\ncrypto_shash_export(desc, (void *)&sha256_st);\r\nmemcpy(result_hash, sha256_st.state, SHA256_DIGEST_SIZE);\r\n} else if (digest_size == SHA256_DIGEST_SIZE) {\r\nerror = crypto_shash_init(desc) ?:\r\ncrypto_shash_update(desc, iopad, SHA256_BLOCK_SIZE) ?:\r\ncrypto_shash_export(desc, (void *)&sha256_st);\r\nmemcpy(result_hash, sha256_st.state, SHA256_DIGEST_SIZE);\r\n} else if (digest_size == SHA384_DIGEST_SIZE) {\r\nerror = crypto_shash_init(desc) ?:\r\ncrypto_shash_update(desc, iopad, SHA512_BLOCK_SIZE) ?:\r\ncrypto_shash_export(desc, (void *)&sha512_st);\r\nmemcpy(result_hash, sha512_st.state, SHA512_DIGEST_SIZE);\r\n} else if (digest_size == SHA512_DIGEST_SIZE) {\r\nerror = crypto_shash_init(desc) ?:\r\ncrypto_shash_update(desc, iopad, SHA512_BLOCK_SIZE) ?:\r\ncrypto_shash_export(desc, (void *)&sha512_st);\r\nmemcpy(result_hash, sha512_st.state, SHA512_DIGEST_SIZE);\r\n} else {\r\nerror = -EINVAL;\r\npr_err("Unknown digest size %d\n", digest_size);\r\n}\r\nreturn error;\r\n}\r\nstatic void chcr_change_order(char *buf, int ds)\r\n{\r\nint i;\r\nif (ds == SHA512_DIGEST_SIZE) {\r\nfor (i = 0; i < (ds / sizeof(u64)); i++)\r\n*((__be64 *)buf + i) =\r\ncpu_to_be64(*((u64 *)buf + i));\r\n} else {\r\nfor (i = 0; i < (ds / sizeof(u32)); i++)\r\n*((__be32 *)buf + i) =\r\ncpu_to_be32(*((u32 *)buf + i));\r\n}\r\n}\r\nstatic inline int is_hmac(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct chcr_alg_template *chcr_crypto_alg =\r\ncontainer_of(__crypto_ahash_alg(alg), struct chcr_alg_template,\r\nalg.hash);\r\nif (chcr_crypto_alg->type == CRYPTO_ALG_TYPE_HMAC)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void write_phys_cpl(struct cpl_rx_phys_dsgl *phys_cpl,\r\nstruct scatterlist *sg,\r\nstruct phys_sge_parm *sg_param)\r\n{\r\nstruct phys_sge_pairs *to;\r\nunsigned int len = 0, left_size = sg_param->obsize;\r\nunsigned int nents = sg_param->nents, i, j = 0;\r\nphys_cpl->op_to_tid = htonl(CPL_RX_PHYS_DSGL_OPCODE_V(CPL_RX_PHYS_DSGL)\r\n| CPL_RX_PHYS_DSGL_ISRDMA_V(0));\r\nphys_cpl->pcirlxorder_to_noofsgentr =\r\nhtonl(CPL_RX_PHYS_DSGL_PCIRLXORDER_V(0) |\r\nCPL_RX_PHYS_DSGL_PCINOSNOOP_V(0) |\r\nCPL_RX_PHYS_DSGL_PCITPHNTENB_V(0) |\r\nCPL_RX_PHYS_DSGL_PCITPHNT_V(0) |\r\nCPL_RX_PHYS_DSGL_DCAID_V(0) |\r\nCPL_RX_PHYS_DSGL_NOOFSGENTR_V(nents));\r\nphys_cpl->rss_hdr_int.opcode = CPL_RX_PHYS_ADDR;\r\nphys_cpl->rss_hdr_int.qid = htons(sg_param->qid);\r\nphys_cpl->rss_hdr_int.hash_val = 0;\r\nto = (struct phys_sge_pairs *)((unsigned char *)phys_cpl +\r\nsizeof(struct cpl_rx_phys_dsgl));\r\nfor (i = 0; nents && left_size; to++) {\r\nfor (j = 0; j < 8 && nents && left_size; j++, nents--) {\r\nlen = min(left_size, sg_dma_len(sg));\r\nto->len[j] = htons(len);\r\nto->addr[j] = cpu_to_be64(sg_dma_address(sg));\r\nleft_size -= len;\r\nsg = sg_next(sg);\r\n}\r\n}\r\n}\r\nstatic inline int map_writesg_phys_cpl(struct device *dev,\r\nstruct cpl_rx_phys_dsgl *phys_cpl,\r\nstruct scatterlist *sg,\r\nstruct phys_sge_parm *sg_param)\r\n{\r\nif (!sg || !sg_param->nents)\r\nreturn -EINVAL;\r\nsg_param->nents = dma_map_sg(dev, sg, sg_param->nents, DMA_FROM_DEVICE);\r\nif (sg_param->nents == 0) {\r\npr_err("CHCR : DMA mapping failed\n");\r\nreturn -EINVAL;\r\n}\r\nwrite_phys_cpl(phys_cpl, sg, sg_param);\r\nreturn 0;\r\n}\r\nstatic inline int get_aead_subtype(struct crypto_aead *aead)\r\n{\r\nstruct aead_alg *alg = crypto_aead_alg(aead);\r\nstruct chcr_alg_template *chcr_crypto_alg =\r\ncontainer_of(alg, struct chcr_alg_template, alg.aead);\r\nreturn chcr_crypto_alg->type & CRYPTO_ALG_SUB_TYPE_MASK;\r\n}\r\nstatic inline int get_cryptoalg_subtype(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct chcr_alg_template *chcr_crypto_alg =\r\ncontainer_of(alg, struct chcr_alg_template, alg.crypto);\r\nreturn chcr_crypto_alg->type & CRYPTO_ALG_SUB_TYPE_MASK;\r\n}\r\nstatic inline void write_buffer_to_skb(struct sk_buff *skb,\r\nunsigned int *frags,\r\nchar *bfr,\r\nu8 bfr_len)\r\n{\r\nskb->len += bfr_len;\r\nskb->data_len += bfr_len;\r\nskb->truesize += bfr_len;\r\nget_page(virt_to_page(bfr));\r\nskb_fill_page_desc(skb, *frags, virt_to_page(bfr),\r\noffset_in_page(bfr), bfr_len);\r\n(*frags)++;\r\n}\r\nstatic inline void\r\nwrite_sg_to_skb(struct sk_buff *skb, unsigned int *frags,\r\nstruct scatterlist *sg, unsigned int count)\r\n{\r\nstruct page *spage;\r\nunsigned int page_len;\r\nskb->len += count;\r\nskb->data_len += count;\r\nskb->truesize += count;\r\nwhile (count > 0) {\r\nif (!sg || (!(sg->length)))\r\nbreak;\r\nspage = sg_page(sg);\r\nget_page(spage);\r\npage_len = min(sg->length, count);\r\nskb_fill_page_desc(skb, *frags, spage, sg->offset, page_len);\r\n(*frags)++;\r\ncount -= page_len;\r\nsg = sg_next(sg);\r\n}\r\n}\r\nstatic int cxgb4_is_crypto_q_full(struct net_device *dev, unsigned int idx)\r\n{\r\nstruct adapter *adap = netdev2adap(dev);\r\nstruct sge_uld_txq_info *txq_info =\r\nadap->sge.uld_txq_info[CXGB4_TX_CRYPTO];\r\nstruct sge_uld_txq *txq;\r\nint ret = 0;\r\nlocal_bh_disable();\r\ntxq = &txq_info->uldtxq[idx];\r\nspin_lock(&txq->sendq.lock);\r\nif (txq->full)\r\nret = -1;\r\nspin_unlock(&txq->sendq.lock);\r\nlocal_bh_enable();\r\nreturn ret;\r\n}\r\nstatic int generate_copy_rrkey(struct ablk_ctx *ablkctx,\r\nstruct _key_ctx *key_ctx)\r\n{\r\nif (ablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CBC) {\r\nmemcpy(key_ctx->key, ablkctx->rrkey, ablkctx->enckey_len);\r\n} else {\r\nmemcpy(key_ctx->key,\r\nablkctx->key + (ablkctx->enckey_len >> 1),\r\nablkctx->enckey_len >> 1);\r\nmemcpy(key_ctx->key + (ablkctx->enckey_len >> 1),\r\nablkctx->rrkey, ablkctx->enckey_len >> 1);\r\n}\r\nreturn 0;\r\n}\r\nstatic int chcr_sg_ent_in_wr(struct scatterlist *src,\r\nstruct scatterlist *dst,\r\nunsigned int minsg,\r\nunsigned int space,\r\nshort int *sent,\r\nshort int *dent)\r\n{\r\nint srclen = 0, dstlen = 0;\r\nint srcsg = minsg, dstsg = 0;\r\n*sent = 0;\r\n*dent = 0;\r\nwhile (src && dst && ((srcsg + 1) <= MAX_SKB_FRAGS) &&\r\nspace > (sgl_ent_len[srcsg + 1] + dsgl_ent_len[dstsg])) {\r\nsrclen += src->length;\r\nsrcsg++;\r\nwhile (dst && ((dstsg + 1) <= MAX_DSGL_ENT) &&\r\nspace > (sgl_ent_len[srcsg] + dsgl_ent_len[dstsg + 1])) {\r\nif (srclen <= dstlen)\r\nbreak;\r\ndstlen += dst->length;\r\ndst = sg_next(dst);\r\ndstsg++;\r\n}\r\nsrc = sg_next(src);\r\n}\r\n*sent = srcsg - minsg;\r\n*dent = dstsg;\r\nreturn min(srclen, dstlen);\r\n}\r\nstatic int chcr_cipher_fallback(struct crypto_skcipher *cipher,\r\nu32 flags,\r\nstruct scatterlist *src,\r\nstruct scatterlist *dst,\r\nunsigned int nbytes,\r\nu8 *iv,\r\nunsigned short op_type)\r\n{\r\nint err;\r\nSKCIPHER_REQUEST_ON_STACK(subreq, cipher);\r\nskcipher_request_set_tfm(subreq, cipher);\r\nskcipher_request_set_callback(subreq, flags, NULL, NULL);\r\nskcipher_request_set_crypt(subreq, src, dst,\r\nnbytes, iv);\r\nerr = op_type ? crypto_skcipher_decrypt(subreq) :\r\ncrypto_skcipher_encrypt(subreq);\r\nskcipher_request_zero(subreq);\r\nreturn err;\r\n}\r\nstatic inline void create_wreq(struct chcr_context *ctx,\r\nstruct chcr_wr *chcr_req,\r\nvoid *req, struct sk_buff *skb,\r\nint kctx_len, int hash_sz,\r\nint is_iv,\r\nunsigned int sc_len,\r\nunsigned int lcb)\r\n{\r\nstruct uld_ctx *u_ctx = ULD_CTX(ctx);\r\nint iv_loc = IV_DSGL;\r\nint qid = u_ctx->lldi.rxq_ids[ctx->rx_qidx];\r\nunsigned int immdatalen = 0, nr_frags = 0;\r\nif (is_ofld_imm(skb)) {\r\nimmdatalen = skb->data_len;\r\niv_loc = IV_IMMEDIATE;\r\n} else {\r\nnr_frags = skb_shinfo(skb)->nr_frags;\r\n}\r\nchcr_req->wreq.op_to_cctx_size = FILL_WR_OP_CCTX_SIZE(immdatalen,\r\n((sizeof(chcr_req->key_ctx) + kctx_len) >> 4));\r\nchcr_req->wreq.pld_size_hash_size =\r\nhtonl(FW_CRYPTO_LOOKASIDE_WR_PLD_SIZE_V(sgl_lengths[nr_frags]) |\r\nFW_CRYPTO_LOOKASIDE_WR_HASH_SIZE_V(hash_sz));\r\nchcr_req->wreq.len16_pkd =\r\nhtonl(FW_CRYPTO_LOOKASIDE_WR_LEN16_V(DIV_ROUND_UP(\r\n(calc_tx_flits_ofld(skb) * 8), 16)));\r\nchcr_req->wreq.cookie = cpu_to_be64((uintptr_t)req);\r\nchcr_req->wreq.rx_chid_to_rx_q_id =\r\nFILL_WR_RX_Q_ID(ctx->dev->rx_channel_id, qid,\r\nis_iv ? iv_loc : IV_NOP, !!lcb,\r\nctx->tx_qidx);\r\nchcr_req->ulptx.cmd_dest = FILL_ULPTX_CMD_DEST(ctx->dev->tx_channel_id,\r\nqid);\r\nchcr_req->ulptx.len = htonl((DIV_ROUND_UP((calc_tx_flits_ofld(skb) * 8),\r\n16) - ((sizeof(chcr_req->wreq)) >> 4)));\r\nchcr_req->sc_imm.cmd_more = FILL_CMD_MORE(immdatalen);\r\nchcr_req->sc_imm.len = cpu_to_be32(sizeof(struct cpl_tx_sec_pdu) +\r\nsizeof(chcr_req->key_ctx) +\r\nkctx_len + sc_len + immdatalen);\r\n}\r\nstatic struct sk_buff *create_cipher_wr(struct cipher_wr_param *wrparam)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(wrparam->req);\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct uld_ctx *u_ctx = ULD_CTX(ctx);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nstruct sk_buff *skb = NULL;\r\nstruct chcr_wr *chcr_req;\r\nstruct cpl_rx_phys_dsgl *phys_cpl;\r\nstruct chcr_blkcipher_req_ctx *reqctx =\r\nablkcipher_request_ctx(wrparam->req);\r\nstruct phys_sge_parm sg_param;\r\nunsigned int frags = 0, transhdr_len, phys_dsgl;\r\nint error;\r\nunsigned int ivsize = AES_BLOCK_SIZE, kctx_len;\r\ngfp_t flags = wrparam->req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ?\r\nGFP_KERNEL : GFP_ATOMIC;\r\nstruct adapter *adap = padap(ctx->dev);\r\nphys_dsgl = get_space_for_phys_dsgl(reqctx->dst_nents);\r\nkctx_len = (DIV_ROUND_UP(ablkctx->enckey_len, 16) * 16);\r\ntranshdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, phys_dsgl);\r\nskb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);\r\nif (!skb) {\r\nerror = -ENOMEM;\r\ngoto err;\r\n}\r\nskb_reserve(skb, sizeof(struct sge_opaque_hdr));\r\nchcr_req = __skb_put_zero(skb, transhdr_len);\r\nchcr_req->sec_cpl.op_ivinsrtofst =\r\nFILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2, 1);\r\nchcr_req->sec_cpl.pldlen = htonl(ivsize + wrparam->bytes);\r\nchcr_req->sec_cpl.aadstart_cipherstop_hi =\r\nFILL_SEC_CPL_CIPHERSTOP_HI(0, 0, ivsize + 1, 0);\r\nchcr_req->sec_cpl.cipherstop_lo_authinsert =\r\nFILL_SEC_CPL_AUTHINSERT(0, 0, 0, 0);\r\nchcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(reqctx->op, 0,\r\nablkctx->ciph_mode,\r\n0, 0, ivsize >> 1);\r\nchcr_req->sec_cpl.ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 0,\r\n0, 1, phys_dsgl);\r\nchcr_req->key_ctx.ctx_hdr = ablkctx->key_ctx_hdr;\r\nif ((reqctx->op == CHCR_DECRYPT_OP) &&\r\n(!(get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==\r\nCRYPTO_ALG_SUB_TYPE_CTR)) &&\r\n(!(get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==\r\nCRYPTO_ALG_SUB_TYPE_CTR_RFC3686))) {\r\ngenerate_copy_rrkey(ablkctx, &chcr_req->key_ctx);\r\n} else {\r\nif ((ablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CBC) ||\r\n(ablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CTR)) {\r\nmemcpy(chcr_req->key_ctx.key, ablkctx->key,\r\nablkctx->enckey_len);\r\n} else {\r\nmemcpy(chcr_req->key_ctx.key, ablkctx->key +\r\n(ablkctx->enckey_len >> 1),\r\nablkctx->enckey_len >> 1);\r\nmemcpy(chcr_req->key_ctx.key +\r\n(ablkctx->enckey_len >> 1),\r\nablkctx->key,\r\nablkctx->enckey_len >> 1);\r\n}\r\n}\r\nphys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);\r\nsg_param.nents = reqctx->dst_nents;\r\nsg_param.obsize = wrparam->bytes;\r\nsg_param.qid = wrparam->qid;\r\nerror = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,\r\nreqctx->dst, &sg_param);\r\nif (error)\r\ngoto map_fail1;\r\nskb_set_transport_header(skb, transhdr_len);\r\nwrite_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);\r\nwrite_sg_to_skb(skb, &frags, wrparam->srcsg, wrparam->bytes);\r\natomic_inc(&adap->chcr_stats.cipher_rqst);\r\ncreate_wreq(ctx, chcr_req, &(wrparam->req->base), skb, kctx_len, 0, 1,\r\nsizeof(struct cpl_rx_phys_dsgl) + phys_dsgl,\r\nablkctx->ciph_mode == CHCR_SCMD_CIPHER_MODE_AES_CBC);\r\nreqctx->skb = skb;\r\nskb_get(skb);\r\nreturn skb;\r\nmap_fail1:\r\nkfree_skb(skb);\r\nerr:\r\nreturn ERR_PTR(error);\r\n}\r\nstatic inline int chcr_keyctx_ck_size(unsigned int keylen)\r\n{\r\nint ck_size = 0;\r\nif (keylen == AES_KEYSIZE_128)\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\r\nelse if (keylen == AES_KEYSIZE_192)\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\r\nelse if (keylen == AES_KEYSIZE_256)\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\r\nelse\r\nck_size = 0;\r\nreturn ck_size;\r\n}\r\nstatic int chcr_cipher_fallback_setkey(struct crypto_ablkcipher *cipher,\r\nconst u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(cipher);\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nint err = 0;\r\ncrypto_skcipher_clear_flags(ablkctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\r\ncrypto_skcipher_set_flags(ablkctx->sw_cipher, cipher->base.crt_flags &\r\nCRYPTO_TFM_REQ_MASK);\r\nerr = crypto_skcipher_setkey(ablkctx->sw_cipher, key, keylen);\r\ntfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm->crt_flags |=\r\ncrypto_skcipher_get_flags(ablkctx->sw_cipher) &\r\nCRYPTO_TFM_RES_MASK;\r\nreturn err;\r\n}\r\nstatic int chcr_aes_cbc_setkey(struct crypto_ablkcipher *cipher,\r\nconst u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nunsigned int ck_size, context_size;\r\nu16 alignment = 0;\r\nint err;\r\nerr = chcr_cipher_fallback_setkey(cipher, key, keylen);\r\nif (err)\r\ngoto badkey_err;\r\nck_size = chcr_keyctx_ck_size(keylen);\r\nalignment = ck_size == CHCR_KEYCTX_CIPHER_KEY_SIZE_192 ? 8 : 0;\r\nmemcpy(ablkctx->key, key, keylen);\r\nablkctx->enckey_len = keylen;\r\nget_aes_decrypt_key(ablkctx->rrkey, ablkctx->key, keylen << 3);\r\ncontext_size = (KEY_CONTEXT_HDR_SALT_AND_PAD +\r\nkeylen + alignment) >> 4;\r\nablkctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY,\r\n0, 0, context_size);\r\nablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CBC;\r\nreturn 0;\r\nbadkey_err:\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nablkctx->enckey_len = 0;\r\nreturn err;\r\n}\r\nstatic int chcr_aes_ctr_setkey(struct crypto_ablkcipher *cipher,\r\nconst u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nunsigned int ck_size, context_size;\r\nu16 alignment = 0;\r\nint err;\r\nerr = chcr_cipher_fallback_setkey(cipher, key, keylen);\r\nif (err)\r\ngoto badkey_err;\r\nck_size = chcr_keyctx_ck_size(keylen);\r\nalignment = (ck_size == CHCR_KEYCTX_CIPHER_KEY_SIZE_192) ? 8 : 0;\r\nmemcpy(ablkctx->key, key, keylen);\r\nablkctx->enckey_len = keylen;\r\ncontext_size = (KEY_CONTEXT_HDR_SALT_AND_PAD +\r\nkeylen + alignment) >> 4;\r\nablkctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY,\r\n0, 0, context_size);\r\nablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CTR;\r\nreturn 0;\r\nbadkey_err:\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nablkctx->enckey_len = 0;\r\nreturn err;\r\n}\r\nstatic int chcr_aes_rfc3686_setkey(struct crypto_ablkcipher *cipher,\r\nconst u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nunsigned int ck_size, context_size;\r\nu16 alignment = 0;\r\nint err;\r\nif (keylen < CTR_RFC3686_NONCE_SIZE)\r\nreturn -EINVAL;\r\nmemcpy(ablkctx->nonce, key + (keylen - CTR_RFC3686_NONCE_SIZE),\r\nCTR_RFC3686_NONCE_SIZE);\r\nkeylen -= CTR_RFC3686_NONCE_SIZE;\r\nerr = chcr_cipher_fallback_setkey(cipher, key, keylen);\r\nif (err)\r\ngoto badkey_err;\r\nck_size = chcr_keyctx_ck_size(keylen);\r\nalignment = (ck_size == CHCR_KEYCTX_CIPHER_KEY_SIZE_192) ? 8 : 0;\r\nmemcpy(ablkctx->key, key, keylen);\r\nablkctx->enckey_len = keylen;\r\ncontext_size = (KEY_CONTEXT_HDR_SALT_AND_PAD +\r\nkeylen + alignment) >> 4;\r\nablkctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY,\r\n0, 0, context_size);\r\nablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_CTR;\r\nreturn 0;\r\nbadkey_err:\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nablkctx->enckey_len = 0;\r\nreturn err;\r\n}\r\nstatic void ctr_add_iv(u8 *dstiv, u8 *srciv, u32 add)\r\n{\r\nunsigned int size = AES_BLOCK_SIZE;\r\n__be32 *b = (__be32 *)(dstiv + size);\r\nu32 c, prev;\r\nmemcpy(dstiv, srciv, AES_BLOCK_SIZE);\r\nfor (; size >= 4; size -= 4) {\r\nprev = be32_to_cpu(*--b);\r\nc = prev + add;\r\n*b = cpu_to_be32(c);\r\nif (prev < c)\r\nbreak;\r\nadd = 1;\r\n}\r\n}\r\nstatic unsigned int adjust_ctr_overflow(u8 *iv, u32 bytes)\r\n{\r\n__be32 *b = (__be32 *)(iv + AES_BLOCK_SIZE);\r\nu64 c;\r\nu32 temp = be32_to_cpu(*--b);\r\ntemp = ~temp;\r\nc = (u64)temp + 1;\r\nif ((bytes / AES_BLOCK_SIZE) > c)\r\nbytes = c * AES_BLOCK_SIZE;\r\nreturn bytes;\r\n}\r\nstatic int chcr_update_tweak(struct ablkcipher_request *req, u8 *iv)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nstruct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);\r\nstruct crypto_cipher *cipher;\r\nint ret, i;\r\nu8 *key;\r\nunsigned int keylen;\r\ncipher = ablkctx->aes_generic;\r\nmemcpy(iv, req->info, AES_BLOCK_SIZE);\r\nkeylen = ablkctx->enckey_len / 2;\r\nkey = ablkctx->key + keylen;\r\nret = crypto_cipher_setkey(cipher, key, keylen);\r\nif (ret)\r\ngoto out;\r\ncrypto_cipher_encrypt_one(cipher, iv, iv);\r\nfor (i = 0; i < (reqctx->processed / AES_BLOCK_SIZE); i++)\r\ngf128mul_x_ble((le128 *)iv, (le128 *)iv);\r\ncrypto_cipher_decrypt_one(cipher, iv, iv);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int chcr_update_cipher_iv(struct ablkcipher_request *req,\r\nstruct cpl_fw6_pld *fw6_pld, u8 *iv)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nstruct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);\r\nint subtype = get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm));\r\nint ret = 0;\r\nif (subtype == CRYPTO_ALG_SUB_TYPE_CTR)\r\nctr_add_iv(iv, req->info, (reqctx->processed /\r\nAES_BLOCK_SIZE));\r\nelse if (subtype == CRYPTO_ALG_SUB_TYPE_CTR_RFC3686)\r\n*(__be32 *)(reqctx->iv + CTR_RFC3686_NONCE_SIZE +\r\nCTR_RFC3686_IV_SIZE) = cpu_to_be32((reqctx->processed /\r\nAES_BLOCK_SIZE) + 1);\r\nelse if (subtype == CRYPTO_ALG_SUB_TYPE_XTS)\r\nret = chcr_update_tweak(req, iv);\r\nelse if (subtype == CRYPTO_ALG_SUB_TYPE_CBC) {\r\nif (reqctx->op)\r\nsg_pcopy_to_buffer(req->src, sg_nents(req->src), iv,\r\n16,\r\nreqctx->processed - AES_BLOCK_SIZE);\r\nelse\r\nmemcpy(iv, &fw6_pld->data[2], AES_BLOCK_SIZE);\r\n}\r\nreturn ret;\r\n}\r\nstatic int chcr_final_cipher_iv(struct ablkcipher_request *req,\r\nstruct cpl_fw6_pld *fw6_pld, u8 *iv)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nstruct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);\r\nint subtype = get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm));\r\nint ret = 0;\r\nif (subtype == CRYPTO_ALG_SUB_TYPE_CTR)\r\nctr_add_iv(iv, req->info, (reqctx->processed /\r\nAES_BLOCK_SIZE));\r\nelse if (subtype == CRYPTO_ALG_SUB_TYPE_XTS)\r\nret = chcr_update_tweak(req, iv);\r\nelse if (subtype == CRYPTO_ALG_SUB_TYPE_CBC) {\r\nif (reqctx->op)\r\nsg_pcopy_to_buffer(req->src, sg_nents(req->src), iv,\r\n16,\r\nreqctx->processed - AES_BLOCK_SIZE);\r\nelse\r\nmemcpy(iv, &fw6_pld->data[2], AES_BLOCK_SIZE);\r\n}\r\nreturn ret;\r\n}\r\nstatic int chcr_handle_cipher_resp(struct ablkcipher_request *req,\r\nunsigned char *input, int err)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct uld_ctx *u_ctx = ULD_CTX(ctx);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nstruct sk_buff *skb;\r\nstruct cpl_fw6_pld *fw6_pld = (struct cpl_fw6_pld *)input;\r\nstruct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);\r\nstruct cipher_wr_param wrparam;\r\nint bytes;\r\ndma_unmap_sg(&u_ctx->lldi.pdev->dev, reqctx->dst, reqctx->dst_nents,\r\nDMA_FROM_DEVICE);\r\nif (reqctx->skb) {\r\nkfree_skb(reqctx->skb);\r\nreqctx->skb = NULL;\r\n}\r\nif (err)\r\ngoto complete;\r\nif (req->nbytes == reqctx->processed) {\r\nerr = chcr_final_cipher_iv(req, fw6_pld, req->info);\r\ngoto complete;\r\n}\r\nif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\r\nctx->tx_qidx))) {\r\nif (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG)) {\r\nerr = -EBUSY;\r\ngoto complete;\r\n}\r\n}\r\nwrparam.srcsg = scatterwalk_ffwd(reqctx->srcffwd, req->src,\r\nreqctx->processed);\r\nreqctx->dst = scatterwalk_ffwd(reqctx->dstffwd, reqctx->dstsg,\r\nreqctx->processed);\r\nif (!wrparam.srcsg || !reqctx->dst) {\r\npr_err("Input sg list length less that nbytes\n");\r\nerr = -EINVAL;\r\ngoto complete;\r\n}\r\nbytes = chcr_sg_ent_in_wr(wrparam.srcsg, reqctx->dst, 1,\r\nSPACE_LEFT(ablkctx->enckey_len),\r\n&wrparam.snent, &reqctx->dst_nents);\r\nif ((bytes + reqctx->processed) >= req->nbytes)\r\nbytes = req->nbytes - reqctx->processed;\r\nelse\r\nbytes = ROUND_16(bytes);\r\nerr = chcr_update_cipher_iv(req, fw6_pld, reqctx->iv);\r\nif (err)\r\ngoto complete;\r\nif (unlikely(bytes == 0)) {\r\nerr = chcr_cipher_fallback(ablkctx->sw_cipher,\r\nreq->base.flags,\r\nwrparam.srcsg,\r\nreqctx->dst,\r\nreq->nbytes - reqctx->processed,\r\nreqctx->iv,\r\nreqctx->op);\r\ngoto complete;\r\n}\r\nif (get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==\r\nCRYPTO_ALG_SUB_TYPE_CTR)\r\nbytes = adjust_ctr_overflow(reqctx->iv, bytes);\r\nreqctx->processed += bytes;\r\nwrparam.qid = u_ctx->lldi.rxq_ids[ctx->rx_qidx];\r\nwrparam.req = req;\r\nwrparam.bytes = bytes;\r\nskb = create_cipher_wr(&wrparam);\r\nif (IS_ERR(skb)) {\r\npr_err("chcr : %s : Failed to form WR. No memory\n", __func__);\r\nerr = PTR_ERR(skb);\r\ngoto complete;\r\n}\r\nskb->dev = u_ctx->lldi.ports[0];\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);\r\nchcr_send_wr(skb);\r\nreturn 0;\r\ncomplete:\r\nfree_new_sg(reqctx->newdstsg);\r\nreqctx->newdstsg = NULL;\r\nreq->base.complete(&req->base, err);\r\nreturn err;\r\n}\r\nstatic int process_cipher(struct ablkcipher_request *req,\r\nunsigned short qid,\r\nstruct sk_buff **skb,\r\nunsigned short op_type)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nunsigned int ivsize = crypto_ablkcipher_ivsize(tfm);\r\nstruct chcr_blkcipher_req_ctx *reqctx = ablkcipher_request_ctx(req);\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nstruct cipher_wr_param wrparam;\r\nint bytes, nents, err = -EINVAL;\r\nreqctx->newdstsg = NULL;\r\nreqctx->processed = 0;\r\nif (!req->info)\r\ngoto error;\r\nif ((ablkctx->enckey_len == 0) || (ivsize > AES_BLOCK_SIZE) ||\r\n(req->nbytes == 0) ||\r\n(req->nbytes % crypto_ablkcipher_blocksize(tfm))) {\r\npr_err("AES: Invalid value of Key Len %d nbytes %d IV Len %d\n",\r\nablkctx->enckey_len, req->nbytes, ivsize);\r\ngoto error;\r\n}\r\nwrparam.srcsg = req->src;\r\nif (is_newsg(req->dst, &nents)) {\r\nreqctx->newdstsg = alloc_new_sg(req->dst, nents);\r\nif (IS_ERR(reqctx->newdstsg))\r\nreturn PTR_ERR(reqctx->newdstsg);\r\nreqctx->dstsg = reqctx->newdstsg;\r\n} else {\r\nreqctx->dstsg = req->dst;\r\n}\r\nbytes = chcr_sg_ent_in_wr(wrparam.srcsg, reqctx->dstsg, MIN_CIPHER_SG,\r\nSPACE_LEFT(ablkctx->enckey_len),\r\n&wrparam.snent,\r\n&reqctx->dst_nents);\r\nif ((bytes + reqctx->processed) >= req->nbytes)\r\nbytes = req->nbytes - reqctx->processed;\r\nelse\r\nbytes = ROUND_16(bytes);\r\nif (unlikely(bytes > req->nbytes))\r\nbytes = req->nbytes;\r\nif (get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==\r\nCRYPTO_ALG_SUB_TYPE_CTR) {\r\nbytes = adjust_ctr_overflow(req->info, bytes);\r\n}\r\nif (get_cryptoalg_subtype(crypto_ablkcipher_tfm(tfm)) ==\r\nCRYPTO_ALG_SUB_TYPE_CTR_RFC3686) {\r\nmemcpy(reqctx->iv, ablkctx->nonce, CTR_RFC3686_NONCE_SIZE);\r\nmemcpy(reqctx->iv + CTR_RFC3686_NONCE_SIZE, req->info,\r\nCTR_RFC3686_IV_SIZE);\r\n*(__be32 *)(reqctx->iv + CTR_RFC3686_NONCE_SIZE +\r\nCTR_RFC3686_IV_SIZE) = cpu_to_be32(1);\r\n} else {\r\nmemcpy(reqctx->iv, req->info, ivsize);\r\n}\r\nif (unlikely(bytes == 0)) {\r\nerr = chcr_cipher_fallback(ablkctx->sw_cipher,\r\nreq->base.flags,\r\nreq->src,\r\nreq->dst,\r\nreq->nbytes,\r\nreq->info,\r\nop_type);\r\ngoto error;\r\n}\r\nreqctx->processed = bytes;\r\nreqctx->dst = reqctx->dstsg;\r\nreqctx->op = op_type;\r\nwrparam.qid = qid;\r\nwrparam.req = req;\r\nwrparam.bytes = bytes;\r\n*skb = create_cipher_wr(&wrparam);\r\nif (IS_ERR(*skb)) {\r\nerr = PTR_ERR(*skb);\r\ngoto error;\r\n}\r\nreturn 0;\r\nerror:\r\nfree_new_sg(reqctx->newdstsg);\r\nreqctx->newdstsg = NULL;\r\nreturn err;\r\n}\r\nstatic int chcr_aes_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct sk_buff *skb = NULL;\r\nint err;\r\nstruct uld_ctx *u_ctx = ULD_CTX(ctx);\r\nif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\r\nctx->tx_qidx))) {\r\nif (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\r\nreturn -EBUSY;\r\n}\r\nerr = process_cipher(req, u_ctx->lldi.rxq_ids[ctx->rx_qidx], &skb,\r\nCHCR_ENCRYPT_OP);\r\nif (err || !skb)\r\nreturn err;\r\nskb->dev = u_ctx->lldi.ports[0];\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);\r\nchcr_send_wr(skb);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int chcr_aes_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct uld_ctx *u_ctx = ULD_CTX(ctx);\r\nstruct sk_buff *skb = NULL;\r\nint err;\r\nif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\r\nctx->tx_qidx))) {\r\nif (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\r\nreturn -EBUSY;\r\n}\r\nerr = process_cipher(req, u_ctx->lldi.rxq_ids[ctx->rx_qidx], &skb,\r\nCHCR_DECRYPT_OP);\r\nif (err || !skb)\r\nreturn err;\r\nskb->dev = u_ctx->lldi.ports[0];\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);\r\nchcr_send_wr(skb);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int chcr_device_init(struct chcr_context *ctx)\r\n{\r\nstruct uld_ctx *u_ctx = NULL;\r\nstruct adapter *adap;\r\nunsigned int id;\r\nint txq_perchan, txq_idx, ntxq;\r\nint err = 0, rxq_perchan, rxq_idx;\r\nid = smp_processor_id();\r\nif (!ctx->dev) {\r\nu_ctx = assign_chcr_device();\r\nif (!u_ctx) {\r\npr_err("chcr device assignment fails\n");\r\ngoto out;\r\n}\r\nctx->dev = u_ctx->dev;\r\nadap = padap(ctx->dev);\r\nntxq = min_not_zero((unsigned int)u_ctx->lldi.nrxq,\r\nadap->vres.ncrypto_fc);\r\nrxq_perchan = u_ctx->lldi.nrxq / u_ctx->lldi.nchan;\r\ntxq_perchan = ntxq / u_ctx->lldi.nchan;\r\nrxq_idx = ctx->dev->tx_channel_id * rxq_perchan;\r\nrxq_idx += id % rxq_perchan;\r\ntxq_idx = ctx->dev->tx_channel_id * txq_perchan;\r\ntxq_idx += id % txq_perchan;\r\nspin_lock(&ctx->dev->lock_chcr_dev);\r\nctx->rx_qidx = rxq_idx;\r\nctx->tx_qidx = txq_idx;\r\nctx->dev->tx_channel_id = !ctx->dev->tx_channel_id;\r\nctx->dev->rx_channel_id = 0;\r\nspin_unlock(&ctx->dev->lock_chcr_dev);\r\n}\r\nout:\r\nreturn err;\r\n}\r\nstatic int chcr_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct chcr_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nablkctx->sw_cipher = crypto_alloc_skcipher(alg->cra_name, 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(ablkctx->sw_cipher)) {\r\npr_err("failed to allocate fallback for %s\n", alg->cra_name);\r\nreturn PTR_ERR(ablkctx->sw_cipher);\r\n}\r\nif (get_cryptoalg_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_XTS) {\r\nablkctx->aes_generic = crypto_alloc_cipher("aes-generic", 0, 0);\r\nif (IS_ERR(ablkctx->aes_generic)) {\r\npr_err("failed to allocate aes cipher for tweak\n");\r\nreturn PTR_ERR(ablkctx->aes_generic);\r\n}\r\n} else\r\nablkctx->aes_generic = NULL;\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct chcr_blkcipher_req_ctx);\r\nreturn chcr_device_init(crypto_tfm_ctx(tfm));\r\n}\r\nstatic int chcr_rfc3686_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct chcr_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nablkctx->sw_cipher = crypto_alloc_skcipher("ctr(aes)", 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(ablkctx->sw_cipher)) {\r\npr_err("failed to allocate fallback for %s\n", alg->cra_name);\r\nreturn PTR_ERR(ablkctx->sw_cipher);\r\n}\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct chcr_blkcipher_req_ctx);\r\nreturn chcr_device_init(crypto_tfm_ctx(tfm));\r\n}\r\nstatic void chcr_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct chcr_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\ncrypto_free_skcipher(ablkctx->sw_cipher);\r\nif (ablkctx->aes_generic)\r\ncrypto_free_cipher(ablkctx->aes_generic);\r\n}\r\nstatic int get_alg_config(struct algo_param *params,\r\nunsigned int auth_size)\r\n{\r\nswitch (auth_size) {\r\ncase SHA1_DIGEST_SIZE:\r\nparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_160;\r\nparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA1;\r\nparams->result_size = SHA1_DIGEST_SIZE;\r\nbreak;\r\ncase SHA224_DIGEST_SIZE:\r\nparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;\r\nparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA224;\r\nparams->result_size = SHA256_DIGEST_SIZE;\r\nbreak;\r\ncase SHA256_DIGEST_SIZE:\r\nparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;\r\nparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA256;\r\nparams->result_size = SHA256_DIGEST_SIZE;\r\nbreak;\r\ncase SHA384_DIGEST_SIZE:\r\nparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_512;\r\nparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA512_384;\r\nparams->result_size = SHA512_DIGEST_SIZE;\r\nbreak;\r\ncase SHA512_DIGEST_SIZE:\r\nparams->mk_size = CHCR_KEYCTX_MAC_KEY_SIZE_512;\r\nparams->auth_mode = CHCR_SCMD_AUTH_MODE_SHA512_512;\r\nparams->result_size = SHA512_DIGEST_SIZE;\r\nbreak;\r\ndefault:\r\npr_err("chcr : ERROR, unsupported digest size\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void chcr_free_shash(struct crypto_shash *base_hash)\r\n{\r\ncrypto_free_shash(base_hash);\r\n}\r\nstatic struct sk_buff *create_hash_wr(struct ahash_request *req,\r\nstruct hash_wr_param *param)\r\n{\r\nstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));\r\nstruct hmac_ctx *hmacctx = HMAC_CTX(ctx);\r\nstruct sk_buff *skb = NULL;\r\nstruct chcr_wr *chcr_req;\r\nunsigned int frags = 0, transhdr_len, iopad_alignment = 0;\r\nunsigned int digestsize = crypto_ahash_digestsize(tfm);\r\nunsigned int kctx_len = 0;\r\nu8 hash_size_in_response = 0;\r\ngfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\r\nGFP_ATOMIC;\r\nstruct adapter *adap = padap(ctx->dev);\r\niopad_alignment = KEYCTX_ALIGN_PAD(digestsize);\r\nkctx_len = param->alg_prm.result_size + iopad_alignment;\r\nif (param->opad_needed)\r\nkctx_len += param->alg_prm.result_size + iopad_alignment;\r\nif (req_ctx->result)\r\nhash_size_in_response = digestsize;\r\nelse\r\nhash_size_in_response = param->alg_prm.result_size;\r\ntranshdr_len = HASH_TRANSHDR_SIZE(kctx_len);\r\nskb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);\r\nif (!skb)\r\nreturn skb;\r\nskb_reserve(skb, sizeof(struct sge_opaque_hdr));\r\nchcr_req = __skb_put_zero(skb, transhdr_len);\r\nchcr_req->sec_cpl.op_ivinsrtofst =\r\nFILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2, 0);\r\nchcr_req->sec_cpl.pldlen = htonl(param->bfr_len + param->sg_len);\r\nchcr_req->sec_cpl.aadstart_cipherstop_hi =\r\nFILL_SEC_CPL_CIPHERSTOP_HI(0, 0, 0, 0);\r\nchcr_req->sec_cpl.cipherstop_lo_authinsert =\r\nFILL_SEC_CPL_AUTHINSERT(0, 1, 0, 0);\r\nchcr_req->sec_cpl.seqno_numivs =\r\nFILL_SEC_CPL_SCMD0_SEQNO(0, 0, 0, param->alg_prm.auth_mode,\r\nparam->opad_needed, 0);\r\nchcr_req->sec_cpl.ivgen_hdrlen =\r\nFILL_SEC_CPL_IVGEN_HDRLEN(param->last, param->more, 0, 1, 0, 0);\r\nmemcpy(chcr_req->key_ctx.key, req_ctx->partial_hash,\r\nparam->alg_prm.result_size);\r\nif (param->opad_needed)\r\nmemcpy(chcr_req->key_ctx.key +\r\n((param->alg_prm.result_size <= 32) ? 32 :\r\nCHCR_HASH_MAX_DIGEST_SIZE),\r\nhmacctx->opad, param->alg_prm.result_size);\r\nchcr_req->key_ctx.ctx_hdr = FILL_KEY_CTX_HDR(CHCR_KEYCTX_NO_KEY,\r\nparam->alg_prm.mk_size, 0,\r\nparam->opad_needed,\r\n((kctx_len +\r\nsizeof(chcr_req->key_ctx)) >> 4));\r\nchcr_req->sec_cpl.scmd1 = cpu_to_be64((u64)param->scmd1);\r\nskb_set_transport_header(skb, transhdr_len);\r\nif (param->bfr_len != 0)\r\nwrite_buffer_to_skb(skb, &frags, req_ctx->reqbfr,\r\nparam->bfr_len);\r\nif (param->sg_len != 0)\r\nwrite_sg_to_skb(skb, &frags, req->src, param->sg_len);\r\natomic_inc(&adap->chcr_stats.digest_rqst);\r\ncreate_wreq(ctx, chcr_req, &req->base, skb, kctx_len,\r\nhash_size_in_response, 0, DUMMY_BYTES, 0);\r\nreq_ctx->skb = skb;\r\nskb_get(skb);\r\nreturn skb;\r\n}\r\nstatic int chcr_ahash_update(struct ahash_request *req)\r\n{\r\nstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_tfm_ctx(crypto_ahash_tfm(rtfm));\r\nstruct uld_ctx *u_ctx = NULL;\r\nstruct sk_buff *skb;\r\nu8 remainder = 0, bs;\r\nunsigned int nbytes = req->nbytes;\r\nstruct hash_wr_param params;\r\nbs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\r\nu_ctx = ULD_CTX(ctx);\r\nif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\r\nctx->tx_qidx))) {\r\nif (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\r\nreturn -EBUSY;\r\n}\r\nif (nbytes + req_ctx->reqlen >= bs) {\r\nremainder = (nbytes + req_ctx->reqlen) % bs;\r\nnbytes = nbytes + req_ctx->reqlen - remainder;\r\n} else {\r\nsg_pcopy_to_buffer(req->src, sg_nents(req->src), req_ctx->reqbfr\r\n+ req_ctx->reqlen, nbytes, 0);\r\nreq_ctx->reqlen += nbytes;\r\nreturn 0;\r\n}\r\nparams.opad_needed = 0;\r\nparams.more = 1;\r\nparams.last = 0;\r\nparams.sg_len = nbytes - req_ctx->reqlen;\r\nparams.bfr_len = req_ctx->reqlen;\r\nparams.scmd1 = 0;\r\nget_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));\r\nreq_ctx->result = 0;\r\nreq_ctx->data_len += params.sg_len + params.bfr_len;\r\nskb = create_hash_wr(req, &params);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nif (remainder) {\r\nu8 *temp;\r\ntemp = req_ctx->reqbfr;\r\nreq_ctx->reqbfr = req_ctx->skbfr;\r\nreq_ctx->skbfr = temp;\r\nsg_pcopy_to_buffer(req->src, sg_nents(req->src),\r\nreq_ctx->reqbfr, remainder, req->nbytes -\r\nremainder);\r\n}\r\nreq_ctx->reqlen = remainder;\r\nskb->dev = u_ctx->lldi.ports[0];\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);\r\nchcr_send_wr(skb);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic void create_last_hash_block(char *bfr_ptr, unsigned int bs, u64 scmd1)\r\n{\r\nmemset(bfr_ptr, 0, bs);\r\n*bfr_ptr = 0x80;\r\nif (bs == 64)\r\n*(__be64 *)(bfr_ptr + 56) = cpu_to_be64(scmd1 << 3);\r\nelse\r\n*(__be64 *)(bfr_ptr + 120) = cpu_to_be64(scmd1 << 3);\r\n}\r\nstatic int chcr_ahash_final(struct ahash_request *req)\r\n{\r\nstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_tfm_ctx(crypto_ahash_tfm(rtfm));\r\nstruct hash_wr_param params;\r\nstruct sk_buff *skb;\r\nstruct uld_ctx *u_ctx = NULL;\r\nu8 bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\r\nu_ctx = ULD_CTX(ctx);\r\nif (is_hmac(crypto_ahash_tfm(rtfm)))\r\nparams.opad_needed = 1;\r\nelse\r\nparams.opad_needed = 0;\r\nparams.sg_len = 0;\r\nget_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));\r\nreq_ctx->result = 1;\r\nparams.bfr_len = req_ctx->reqlen;\r\nreq_ctx->data_len += params.bfr_len + params.sg_len;\r\nif (req_ctx->reqlen == 0) {\r\ncreate_last_hash_block(req_ctx->reqbfr, bs, req_ctx->data_len);\r\nparams.last = 0;\r\nparams.more = 1;\r\nparams.scmd1 = 0;\r\nparams.bfr_len = bs;\r\n} else {\r\nparams.scmd1 = req_ctx->data_len;\r\nparams.last = 1;\r\nparams.more = 0;\r\n}\r\nskb = create_hash_wr(req, &params);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nskb->dev = u_ctx->lldi.ports[0];\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);\r\nchcr_send_wr(skb);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int chcr_ahash_finup(struct ahash_request *req)\r\n{\r\nstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_tfm_ctx(crypto_ahash_tfm(rtfm));\r\nstruct uld_ctx *u_ctx = NULL;\r\nstruct sk_buff *skb;\r\nstruct hash_wr_param params;\r\nu8 bs;\r\nbs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\r\nu_ctx = ULD_CTX(ctx);\r\nif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\r\nctx->tx_qidx))) {\r\nif (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\r\nreturn -EBUSY;\r\n}\r\nif (is_hmac(crypto_ahash_tfm(rtfm)))\r\nparams.opad_needed = 1;\r\nelse\r\nparams.opad_needed = 0;\r\nparams.sg_len = req->nbytes;\r\nparams.bfr_len = req_ctx->reqlen;\r\nget_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));\r\nreq_ctx->data_len += params.bfr_len + params.sg_len;\r\nreq_ctx->result = 1;\r\nif ((req_ctx->reqlen + req->nbytes) == 0) {\r\ncreate_last_hash_block(req_ctx->reqbfr, bs, req_ctx->data_len);\r\nparams.last = 0;\r\nparams.more = 1;\r\nparams.scmd1 = 0;\r\nparams.bfr_len = bs;\r\n} else {\r\nparams.scmd1 = req_ctx->data_len;\r\nparams.last = 1;\r\nparams.more = 0;\r\n}\r\nskb = create_hash_wr(req, &params);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nskb->dev = u_ctx->lldi.ports[0];\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);\r\nchcr_send_wr(skb);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int chcr_ahash_digest(struct ahash_request *req)\r\n{\r\nstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_tfm_ctx(crypto_ahash_tfm(rtfm));\r\nstruct uld_ctx *u_ctx = NULL;\r\nstruct sk_buff *skb;\r\nstruct hash_wr_param params;\r\nu8 bs;\r\nrtfm->init(req);\r\nbs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\r\nu_ctx = ULD_CTX(ctx);\r\nif (unlikely(cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\r\nctx->tx_qidx))) {\r\nif (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\r\nreturn -EBUSY;\r\n}\r\nif (is_hmac(crypto_ahash_tfm(rtfm)))\r\nparams.opad_needed = 1;\r\nelse\r\nparams.opad_needed = 0;\r\nparams.last = 0;\r\nparams.more = 0;\r\nparams.sg_len = req->nbytes;\r\nparams.bfr_len = 0;\r\nparams.scmd1 = 0;\r\nget_alg_config(&params.alg_prm, crypto_ahash_digestsize(rtfm));\r\nreq_ctx->result = 1;\r\nreq_ctx->data_len += params.bfr_len + params.sg_len;\r\nif (req->nbytes == 0) {\r\ncreate_last_hash_block(req_ctx->reqbfr, bs, 0);\r\nparams.more = 1;\r\nparams.bfr_len = bs;\r\n}\r\nskb = create_hash_wr(req, &params);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nskb->dev = u_ctx->lldi.ports[0];\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);\r\nchcr_send_wr(skb);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int chcr_ahash_export(struct ahash_request *areq, void *out)\r\n{\r\nstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nstruct chcr_ahash_req_ctx *state = out;\r\nstate->reqlen = req_ctx->reqlen;\r\nstate->data_len = req_ctx->data_len;\r\nmemcpy(state->bfr1, req_ctx->reqbfr, req_ctx->reqlen);\r\nmemcpy(state->partial_hash, req_ctx->partial_hash,\r\nCHCR_HASH_MAX_DIGEST_SIZE);\r\nreturn 0;\r\n}\r\nstatic int chcr_ahash_import(struct ahash_request *areq, const void *in)\r\n{\r\nstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nstruct chcr_ahash_req_ctx *state = (struct chcr_ahash_req_ctx *)in;\r\nreq_ctx->reqlen = state->reqlen;\r\nreq_ctx->data_len = state->data_len;\r\nreq_ctx->reqbfr = req_ctx->bfr1;\r\nreq_ctx->skbfr = req_ctx->bfr2;\r\nmemcpy(req_ctx->bfr1, state->bfr1, CHCR_HASH_MAX_BLOCK_SIZE_128);\r\nmemcpy(req_ctx->partial_hash, state->partial_hash,\r\nCHCR_HASH_MAX_DIGEST_SIZE);\r\nreturn 0;\r\n}\r\nstatic int chcr_ahash_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));\r\nstruct hmac_ctx *hmacctx = HMAC_CTX(ctx);\r\nunsigned int digestsize = crypto_ahash_digestsize(tfm);\r\nunsigned int bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nunsigned int i, err = 0, updated_digestsize;\r\nSHASH_DESC_ON_STACK(shash, hmacctx->base_hash);\r\nshash->tfm = hmacctx->base_hash;\r\nshash->flags = crypto_shash_get_flags(hmacctx->base_hash);\r\nif (keylen > bs) {\r\nerr = crypto_shash_digest(shash, key, keylen,\r\nhmacctx->ipad);\r\nif (err)\r\ngoto out;\r\nkeylen = digestsize;\r\n} else {\r\nmemcpy(hmacctx->ipad, key, keylen);\r\n}\r\nmemset(hmacctx->ipad + keylen, 0, bs - keylen);\r\nmemcpy(hmacctx->opad, hmacctx->ipad, bs);\r\nfor (i = 0; i < bs / sizeof(int); i++) {\r\n*((unsigned int *)(&hmacctx->ipad) + i) ^= IPAD_DATA;\r\n*((unsigned int *)(&hmacctx->opad) + i) ^= OPAD_DATA;\r\n}\r\nupdated_digestsize = digestsize;\r\nif (digestsize == SHA224_DIGEST_SIZE)\r\nupdated_digestsize = SHA256_DIGEST_SIZE;\r\nelse if (digestsize == SHA384_DIGEST_SIZE)\r\nupdated_digestsize = SHA512_DIGEST_SIZE;\r\nerr = chcr_compute_partial_hash(shash, hmacctx->ipad,\r\nhmacctx->ipad, digestsize);\r\nif (err)\r\ngoto out;\r\nchcr_change_order(hmacctx->ipad, updated_digestsize);\r\nerr = chcr_compute_partial_hash(shash, hmacctx->opad,\r\nhmacctx->opad, digestsize);\r\nif (err)\r\ngoto out;\r\nchcr_change_order(hmacctx->opad, updated_digestsize);\r\nout:\r\nreturn err;\r\n}\r\nstatic int chcr_aes_xts_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int key_len)\r\n{\r\nstruct chcr_context *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct ablk_ctx *ablkctx = ABLK_CTX(ctx);\r\nunsigned short context_size = 0;\r\nint err;\r\nerr = chcr_cipher_fallback_setkey(cipher, key, key_len);\r\nif (err)\r\ngoto badkey_err;\r\nmemcpy(ablkctx->key, key, key_len);\r\nablkctx->enckey_len = key_len;\r\nget_aes_decrypt_key(ablkctx->rrkey, ablkctx->key, key_len << 2);\r\ncontext_size = (KEY_CONTEXT_HDR_SALT_AND_PAD + key_len) >> 4;\r\nablkctx->key_ctx_hdr =\r\nFILL_KEY_CTX_HDR((key_len == AES_KEYSIZE_256) ?\r\nCHCR_KEYCTX_CIPHER_KEY_SIZE_128 :\r\nCHCR_KEYCTX_CIPHER_KEY_SIZE_256,\r\nCHCR_KEYCTX_NO_KEY, 1,\r\n0, context_size);\r\nablkctx->ciph_mode = CHCR_SCMD_CIPHER_MODE_AES_XTS;\r\nreturn 0;\r\nbadkey_err:\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nablkctx->enckey_len = 0;\r\nreturn err;\r\n}\r\nstatic int chcr_sha_init(struct ahash_request *areq)\r\n{\r\nstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\r\nint digestsize = crypto_ahash_digestsize(tfm);\r\nreq_ctx->data_len = 0;\r\nreq_ctx->reqlen = 0;\r\nreq_ctx->reqbfr = req_ctx->bfr1;\r\nreq_ctx->skbfr = req_ctx->bfr2;\r\nreq_ctx->skb = NULL;\r\nreq_ctx->result = 0;\r\ncopy_hash_init_values(req_ctx->partial_hash, digestsize);\r\nreturn 0;\r\n}\r\nstatic int chcr_sha_cra_init(struct crypto_tfm *tfm)\r\n{\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct chcr_ahash_req_ctx));\r\nreturn chcr_device_init(crypto_tfm_ctx(tfm));\r\n}\r\nstatic int chcr_hmac_init(struct ahash_request *areq)\r\n{\r\nstruct chcr_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nstruct crypto_ahash *rtfm = crypto_ahash_reqtfm(areq);\r\nstruct chcr_context *ctx = crypto_tfm_ctx(crypto_ahash_tfm(rtfm));\r\nstruct hmac_ctx *hmacctx = HMAC_CTX(ctx);\r\nunsigned int digestsize = crypto_ahash_digestsize(rtfm);\r\nunsigned int bs = crypto_tfm_alg_blocksize(crypto_ahash_tfm(rtfm));\r\nchcr_sha_init(areq);\r\nreq_ctx->data_len = bs;\r\nif (is_hmac(crypto_ahash_tfm(rtfm))) {\r\nif (digestsize == SHA224_DIGEST_SIZE)\r\nmemcpy(req_ctx->partial_hash, hmacctx->ipad,\r\nSHA256_DIGEST_SIZE);\r\nelse if (digestsize == SHA384_DIGEST_SIZE)\r\nmemcpy(req_ctx->partial_hash, hmacctx->ipad,\r\nSHA512_DIGEST_SIZE);\r\nelse\r\nmemcpy(req_ctx->partial_hash, hmacctx->ipad,\r\ndigestsize);\r\n}\r\nreturn 0;\r\n}\r\nstatic int chcr_hmac_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct chcr_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct hmac_ctx *hmacctx = HMAC_CTX(ctx);\r\nunsigned int digestsize =\r\ncrypto_ahash_digestsize(__crypto_ahash_cast(tfm));\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct chcr_ahash_req_ctx));\r\nhmacctx->base_hash = chcr_alloc_shash(digestsize);\r\nif (IS_ERR(hmacctx->base_hash))\r\nreturn PTR_ERR(hmacctx->base_hash);\r\nreturn chcr_device_init(crypto_tfm_ctx(tfm));\r\n}\r\nstatic void chcr_hmac_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct chcr_context *ctx = crypto_tfm_ctx(tfm);\r\nstruct hmac_ctx *hmacctx = HMAC_CTX(ctx);\r\nif (hmacctx->base_hash) {\r\nchcr_free_shash(hmacctx->base_hash);\r\nhmacctx->base_hash = NULL;\r\n}\r\n}\r\nstatic int is_newsg(struct scatterlist *sgl, unsigned int *newents)\r\n{\r\nint nents = 0;\r\nint ret = 0;\r\nwhile (sgl) {\r\nif (sgl->length > CHCR_SG_SIZE)\r\nret = 1;\r\nnents += DIV_ROUND_UP(sgl->length, CHCR_SG_SIZE);\r\nsgl = sg_next(sgl);\r\n}\r\n*newents = nents;\r\nreturn ret;\r\n}\r\nstatic inline void free_new_sg(struct scatterlist *sgl)\r\n{\r\nkfree(sgl);\r\n}\r\nstatic struct scatterlist *alloc_new_sg(struct scatterlist *sgl,\r\nunsigned int nents)\r\n{\r\nstruct scatterlist *newsg, *sg;\r\nint i, len, processed = 0;\r\nstruct page *spage;\r\nint offset;\r\nnewsg = kmalloc_array(nents, sizeof(struct scatterlist), GFP_KERNEL);\r\nif (!newsg)\r\nreturn ERR_PTR(-ENOMEM);\r\nsg = newsg;\r\nsg_init_table(sg, nents);\r\noffset = sgl->offset;\r\nspage = sg_page(sgl);\r\nfor (i = 0; i < nents; i++) {\r\nlen = min_t(u32, sgl->length - processed, CHCR_SG_SIZE);\r\nsg_set_page(sg, spage, len, offset);\r\nprocessed += len;\r\noffset += len;\r\nif (offset >= PAGE_SIZE) {\r\noffset = offset % PAGE_SIZE;\r\nspage++;\r\n}\r\nif (processed == sgl->length) {\r\nprocessed = 0;\r\nsgl = sg_next(sgl);\r\nif (!sgl)\r\nbreak;\r\nspage = sg_page(sgl);\r\noffset = sgl->offset;\r\n}\r\nsg = sg_next(sg);\r\n}\r\nreturn newsg;\r\n}\r\nstatic int chcr_copy_assoc(struct aead_request *req,\r\nstruct chcr_aead_ctx *ctx)\r\n{\r\nSKCIPHER_REQUEST_ON_STACK(skreq, ctx->null);\r\nskcipher_request_set_tfm(skreq, ctx->null);\r\nskcipher_request_set_callback(skreq, aead_request_flags(req),\r\nNULL, NULL);\r\nskcipher_request_set_crypt(skreq, req->src, req->dst, req->assoclen,\r\nNULL);\r\nreturn crypto_skcipher_encrypt(skreq);\r\n}\r\nstatic int chcr_aead_need_fallback(struct aead_request *req, int src_nent,\r\nint aadmax, int wrlen,\r\nunsigned short op_type)\r\n{\r\nunsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));\r\nif (((req->cryptlen - (op_type ? authsize : 0)) == 0) ||\r\n(req->assoclen > aadmax) ||\r\n(src_nent > MAX_SKB_FRAGS) ||\r\n(wrlen > MAX_WR_SIZE))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int chcr_aead_fallback(struct aead_request *req, unsigned short op_type)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_aead_ctx(tfm);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nstruct aead_request *subreq = aead_request_ctx(req);\r\naead_request_set_tfm(subreq, aeadctx->sw_cipher);\r\naead_request_set_callback(subreq, req->base.flags,\r\nreq->base.complete, req->base.data);\r\naead_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,\r\nreq->iv);\r\naead_request_set_ad(subreq, req->assoclen);\r\nreturn op_type ? crypto_aead_decrypt(subreq) :\r\ncrypto_aead_encrypt(subreq);\r\n}\r\nstatic struct sk_buff *create_authenc_wr(struct aead_request *req,\r\nunsigned short qid,\r\nint size,\r\nunsigned short op_type)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_aead_ctx(tfm);\r\nstruct uld_ctx *u_ctx = ULD_CTX(ctx);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nstruct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);\r\nstruct chcr_aead_reqctx *reqctx = aead_request_ctx(req);\r\nstruct sk_buff *skb = NULL;\r\nstruct chcr_wr *chcr_req;\r\nstruct cpl_rx_phys_dsgl *phys_cpl;\r\nstruct phys_sge_parm sg_param;\r\nstruct scatterlist *src;\r\nunsigned int frags = 0, transhdr_len;\r\nunsigned int ivsize = crypto_aead_ivsize(tfm), dst_size = 0;\r\nunsigned int kctx_len = 0, nents;\r\nunsigned short stop_offset = 0;\r\nunsigned int assoclen = req->assoclen;\r\nunsigned int authsize = crypto_aead_authsize(tfm);\r\nint error = -EINVAL, src_nent;\r\nint null = 0;\r\ngfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\r\nGFP_ATOMIC;\r\nstruct adapter *adap = padap(ctx->dev);\r\nreqctx->newdstsg = NULL;\r\ndst_size = req->assoclen + req->cryptlen + (op_type ? -authsize :\r\nauthsize);\r\nif (aeadctx->enckey_len == 0 || (req->cryptlen <= 0))\r\ngoto err;\r\nif (op_type && req->cryptlen < crypto_aead_authsize(tfm))\r\ngoto err;\r\nsrc_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);\r\nif (src_nent < 0)\r\ngoto err;\r\nsrc = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);\r\nif (req->src != req->dst) {\r\nerror = chcr_copy_assoc(req, aeadctx);\r\nif (error)\r\nreturn ERR_PTR(error);\r\n}\r\nif (dst_size && is_newsg(req->dst, &nents)) {\r\nreqctx->newdstsg = alloc_new_sg(req->dst, nents);\r\nif (IS_ERR(reqctx->newdstsg))\r\nreturn ERR_CAST(reqctx->newdstsg);\r\nreqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,\r\nreqctx->newdstsg, req->assoclen);\r\n} else {\r\nif (req->src == req->dst)\r\nreqctx->dst = src;\r\nelse\r\nreqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,\r\nreq->dst, req->assoclen);\r\n}\r\nif (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_NULL) {\r\nnull = 1;\r\nassoclen = 0;\r\n}\r\nreqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +\r\n(op_type ? -authsize : authsize));\r\nif (reqctx->dst_nents < 0) {\r\npr_err("AUTHENC:Invalid Destination sg entries\n");\r\nerror = -EINVAL;\r\ngoto err;\r\n}\r\ndst_size = get_space_for_phys_dsgl(reqctx->dst_nents);\r\nkctx_len = (ntohl(KEY_CONTEXT_CTX_LEN_V(aeadctx->key_ctx_hdr)) << 4)\r\n- sizeof(chcr_req->key_ctx);\r\ntranshdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);\r\nif (chcr_aead_need_fallback(req, src_nent + MIN_AUTH_SG,\r\nT6_MAX_AAD_SIZE,\r\ntranshdr_len + (sgl_len(src_nent + MIN_AUTH_SG) * 8),\r\nop_type)) {\r\natomic_inc(&adap->chcr_stats.fallback);\r\nfree_new_sg(reqctx->newdstsg);\r\nreqctx->newdstsg = NULL;\r\nreturn ERR_PTR(chcr_aead_fallback(req, op_type));\r\n}\r\nskb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);\r\nif (!skb) {\r\nerror = -ENOMEM;\r\ngoto err;\r\n}\r\nskb_reserve(skb, sizeof(struct sge_opaque_hdr));\r\nchcr_req = __skb_put_zero(skb, transhdr_len);\r\nstop_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;\r\nchcr_req->sec_cpl.op_ivinsrtofst =\r\nFILL_SEC_CPL_OP_IVINSR(ctx->dev->rx_channel_id, 2,\r\n(ivsize ? (assoclen + 1) : 0));\r\nchcr_req->sec_cpl.pldlen = htonl(assoclen + ivsize + req->cryptlen);\r\nchcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(\r\nassoclen ? 1 : 0, assoclen,\r\nassoclen + ivsize + 1,\r\n(stop_offset & 0x1F0) >> 4);\r\nchcr_req->sec_cpl.cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(\r\nstop_offset & 0xF,\r\nnull ? 0 : assoclen + ivsize + 1,\r\nstop_offset, stop_offset);\r\nchcr_req->sec_cpl.seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(op_type,\r\n(op_type == CHCR_ENCRYPT_OP) ? 1 : 0,\r\nCHCR_SCMD_CIPHER_MODE_AES_CBC,\r\nactx->auth_mode, aeadctx->hmac_ctrl,\r\nivsize >> 1);\r\nchcr_req->sec_cpl.ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,\r\n0, 1, dst_size);\r\nchcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;\r\nif (op_type == CHCR_ENCRYPT_OP)\r\nmemcpy(chcr_req->key_ctx.key, aeadctx->key,\r\naeadctx->enckey_len);\r\nelse\r\nmemcpy(chcr_req->key_ctx.key, actx->dec_rrkey,\r\naeadctx->enckey_len);\r\nmemcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) <<\r\n4), actx->h_iopad, kctx_len -\r\n(DIV_ROUND_UP(aeadctx->enckey_len, 16) << 4));\r\nphys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);\r\nsg_param.nents = reqctx->dst_nents;\r\nsg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);\r\nsg_param.qid = qid;\r\nerror = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,\r\nreqctx->dst, &sg_param);\r\nif (error)\r\ngoto dstmap_fail;\r\nskb_set_transport_header(skb, transhdr_len);\r\nif (assoclen) {\r\nwrite_sg_to_skb(skb, &frags, req->src, assoclen);\r\n}\r\nwrite_buffer_to_skb(skb, &frags, req->iv, ivsize);\r\nwrite_sg_to_skb(skb, &frags, src, req->cryptlen);\r\natomic_inc(&adap->chcr_stats.cipher_rqst);\r\ncreate_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,\r\nsizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);\r\nreqctx->skb = skb;\r\nskb_get(skb);\r\nreturn skb;\r\ndstmap_fail:\r\nkfree_skb(skb);\r\nerr:\r\nfree_new_sg(reqctx->newdstsg);\r\nreqctx->newdstsg = NULL;\r\nreturn ERR_PTR(error);\r\n}\r\nstatic int set_msg_len(u8 *block, unsigned int msglen, int csize)\r\n{\r\n__be32 data;\r\nmemset(block, 0, csize);\r\nblock += csize;\r\nif (csize >= 4)\r\ncsize = 4;\r\nelse if (msglen > (unsigned int)(1 << (8 * csize)))\r\nreturn -EOVERFLOW;\r\ndata = cpu_to_be32(msglen);\r\nmemcpy(block - csize, (u8 *)&data + 4 - csize, csize);\r\nreturn 0;\r\n}\r\nstatic void generate_b0(struct aead_request *req,\r\nstruct chcr_aead_ctx *aeadctx,\r\nunsigned short op_type)\r\n{\r\nunsigned int l, lp, m;\r\nint rc;\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(req);\r\nstruct chcr_aead_reqctx *reqctx = aead_request_ctx(req);\r\nu8 *b0 = reqctx->scratch_pad;\r\nm = crypto_aead_authsize(aead);\r\nmemcpy(b0, reqctx->iv, 16);\r\nlp = b0[0];\r\nl = lp + 1;\r\n*b0 |= (8 * ((m - 2) / 2));\r\nif (req->assoclen)\r\n*b0 |= 64;\r\nrc = set_msg_len(b0 + 16 - l,\r\n(op_type == CHCR_DECRYPT_OP) ?\r\nreq->cryptlen - m : req->cryptlen, l);\r\n}\r\nstatic inline int crypto_ccm_check_iv(const u8 *iv)\r\n{\r\nif (iv[0] < 1 || iv[0] > 7)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int ccm_format_packet(struct aead_request *req,\r\nstruct chcr_aead_ctx *aeadctx,\r\nunsigned int sub_type,\r\nunsigned short op_type)\r\n{\r\nstruct chcr_aead_reqctx *reqctx = aead_request_ctx(req);\r\nint rc = 0;\r\nif (sub_type == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {\r\nreqctx->iv[0] = 3;\r\nmemcpy(reqctx->iv + 1, &aeadctx->salt[0], 3);\r\nmemcpy(reqctx->iv + 4, req->iv, 8);\r\nmemset(reqctx->iv + 12, 0, 4);\r\n*((unsigned short *)(reqctx->scratch_pad + 16)) =\r\nhtons(req->assoclen - 8);\r\n} else {\r\nmemcpy(reqctx->iv, req->iv, 16);\r\n*((unsigned short *)(reqctx->scratch_pad + 16)) =\r\nhtons(req->assoclen);\r\n}\r\ngenerate_b0(req, aeadctx, op_type);\r\nmemset(reqctx->iv + 15 - reqctx->iv[0], 0, reqctx->iv[0] + 1);\r\nreturn rc;\r\n}\r\nstatic void fill_sec_cpl_for_aead(struct cpl_tx_sec_pdu *sec_cpl,\r\nunsigned int dst_size,\r\nstruct aead_request *req,\r\nunsigned short op_type,\r\nstruct chcr_context *chcrctx)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));\r\nunsigned int ivsize = AES_BLOCK_SIZE;\r\nunsigned int cipher_mode = CHCR_SCMD_CIPHER_MODE_AES_CCM;\r\nunsigned int mac_mode = CHCR_SCMD_AUTH_MODE_CBCMAC;\r\nunsigned int c_id = chcrctx->dev->rx_channel_id;\r\nunsigned int ccm_xtra;\r\nunsigned char tag_offset = 0, auth_offset = 0;\r\nunsigned int assoclen;\r\nif (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)\r\nassoclen = req->assoclen - 8;\r\nelse\r\nassoclen = req->assoclen;\r\nccm_xtra = CCM_B0_SIZE +\r\n((assoclen) ? CCM_AAD_FIELD_SIZE : 0);\r\nauth_offset = req->cryptlen ?\r\n(assoclen + ivsize + 1 + ccm_xtra) : 0;\r\nif (op_type == CHCR_DECRYPT_OP) {\r\nif (crypto_aead_authsize(tfm) != req->cryptlen)\r\ntag_offset = crypto_aead_authsize(tfm);\r\nelse\r\nauth_offset = 0;\r\n}\r\nsec_cpl->op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(c_id,\r\n2, (ivsize ? (assoclen + 1) : 0) +\r\nccm_xtra);\r\nsec_cpl->pldlen =\r\nhtonl(assoclen + ivsize + req->cryptlen + ccm_xtra);\r\nsec_cpl->aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(\r\n1, assoclen + ccm_xtra, assoclen\r\n+ ivsize + 1 + ccm_xtra, 0);\r\nsec_cpl->cipherstop_lo_authinsert = FILL_SEC_CPL_AUTHINSERT(0,\r\nauth_offset, tag_offset,\r\n(op_type == CHCR_ENCRYPT_OP) ? 0 :\r\ncrypto_aead_authsize(tfm));\r\nsec_cpl->seqno_numivs = FILL_SEC_CPL_SCMD0_SEQNO(op_type,\r\n(op_type == CHCR_ENCRYPT_OP) ? 0 : 1,\r\ncipher_mode, mac_mode,\r\naeadctx->hmac_ctrl, ivsize >> 1);\r\nsec_cpl->ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1, 0,\r\n1, dst_size);\r\n}\r\nint aead_ccm_validate_input(unsigned short op_type,\r\nstruct aead_request *req,\r\nstruct chcr_aead_ctx *aeadctx,\r\nunsigned int sub_type)\r\n{\r\nif (sub_type != CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309) {\r\nif (crypto_ccm_check_iv(req->iv)) {\r\npr_err("CCM: IV check fails\n");\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nif (req->assoclen != 16 && req->assoclen != 20) {\r\npr_err("RFC4309: Invalid AAD length %d\n",\r\nreq->assoclen);\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (aeadctx->enckey_len == 0) {\r\npr_err("CCM: Encryption key not set\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nunsigned int fill_aead_req_fields(struct sk_buff *skb,\r\nstruct aead_request *req,\r\nstruct scatterlist *src,\r\nunsigned int ivsize,\r\nstruct chcr_aead_ctx *aeadctx)\r\n{\r\nunsigned int frags = 0;\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct chcr_aead_reqctx *reqctx = aead_request_ctx(req);\r\nwrite_buffer_to_skb(skb, &frags, reqctx->scratch_pad, CCM_B0_SIZE +\r\n(req->assoclen ? CCM_AAD_FIELD_SIZE : 0));\r\nif (req->assoclen) {\r\nif (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309)\r\nwrite_sg_to_skb(skb, &frags, req->src,\r\nreq->assoclen - 8);\r\nelse\r\nwrite_sg_to_skb(skb, &frags, req->src, req->assoclen);\r\n}\r\nwrite_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);\r\nif (req->cryptlen)\r\nwrite_sg_to_skb(skb, &frags, src, req->cryptlen);\r\nreturn frags;\r\n}\r\nstatic struct sk_buff *create_aead_ccm_wr(struct aead_request *req,\r\nunsigned short qid,\r\nint size,\r\nunsigned short op_type)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_aead_ctx(tfm);\r\nstruct uld_ctx *u_ctx = ULD_CTX(ctx);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nstruct chcr_aead_reqctx *reqctx = aead_request_ctx(req);\r\nstruct sk_buff *skb = NULL;\r\nstruct chcr_wr *chcr_req;\r\nstruct cpl_rx_phys_dsgl *phys_cpl;\r\nstruct phys_sge_parm sg_param;\r\nstruct scatterlist *src;\r\nunsigned int frags = 0, transhdr_len, ivsize = AES_BLOCK_SIZE;\r\nunsigned int dst_size = 0, kctx_len, nents;\r\nunsigned int sub_type;\r\nunsigned int authsize = crypto_aead_authsize(tfm);\r\nint error = -EINVAL, src_nent;\r\ngfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\r\nGFP_ATOMIC;\r\nstruct adapter *adap = padap(ctx->dev);\r\ndst_size = req->assoclen + req->cryptlen + (op_type ? -authsize :\r\nauthsize);\r\nreqctx->newdstsg = NULL;\r\nif (op_type && req->cryptlen < crypto_aead_authsize(tfm))\r\ngoto err;\r\nsrc_nent = sg_nents_for_len(req->src, req->assoclen + req->cryptlen);\r\nif (src_nent < 0)\r\ngoto err;\r\nsub_type = get_aead_subtype(tfm);\r\nsrc = scatterwalk_ffwd(reqctx->srcffwd, req->src, req->assoclen);\r\nif (req->src != req->dst) {\r\nerror = chcr_copy_assoc(req, aeadctx);\r\nif (error) {\r\npr_err("AAD copy to destination buffer fails\n");\r\nreturn ERR_PTR(error);\r\n}\r\n}\r\nif (dst_size && is_newsg(req->dst, &nents)) {\r\nreqctx->newdstsg = alloc_new_sg(req->dst, nents);\r\nif (IS_ERR(reqctx->newdstsg))\r\nreturn ERR_CAST(reqctx->newdstsg);\r\nreqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,\r\nreqctx->newdstsg, req->assoclen);\r\n} else {\r\nif (req->src == req->dst)\r\nreqctx->dst = src;\r\nelse\r\nreqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,\r\nreq->dst, req->assoclen);\r\n}\r\nreqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +\r\n(op_type ? -authsize : authsize));\r\nif (reqctx->dst_nents < 0) {\r\npr_err("CCM:Invalid Destination sg entries\n");\r\nerror = -EINVAL;\r\ngoto err;\r\n}\r\nerror = aead_ccm_validate_input(op_type, req, aeadctx, sub_type);\r\nif (error)\r\ngoto err;\r\ndst_size = get_space_for_phys_dsgl(reqctx->dst_nents);\r\nkctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) * 2;\r\ntranshdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);\r\nif (chcr_aead_need_fallback(req, src_nent + MIN_CCM_SG,\r\nT6_MAX_AAD_SIZE - 18,\r\ntranshdr_len + (sgl_len(src_nent + MIN_CCM_SG) * 8),\r\nop_type)) {\r\natomic_inc(&adap->chcr_stats.fallback);\r\nfree_new_sg(reqctx->newdstsg);\r\nreqctx->newdstsg = NULL;\r\nreturn ERR_PTR(chcr_aead_fallback(req, op_type));\r\n}\r\nskb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);\r\nif (!skb) {\r\nerror = -ENOMEM;\r\ngoto err;\r\n}\r\nskb_reserve(skb, sizeof(struct sge_opaque_hdr));\r\nchcr_req = __skb_put_zero(skb, transhdr_len);\r\nfill_sec_cpl_for_aead(&chcr_req->sec_cpl, dst_size, req, op_type, ctx);\r\nchcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;\r\nmemcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);\r\nmemcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *\r\n16), aeadctx->key, aeadctx->enckey_len);\r\nphys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);\r\nerror = ccm_format_packet(req, aeadctx, sub_type, op_type);\r\nif (error)\r\ngoto dstmap_fail;\r\nsg_param.nents = reqctx->dst_nents;\r\nsg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);\r\nsg_param.qid = qid;\r\nerror = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,\r\nreqctx->dst, &sg_param);\r\nif (error)\r\ngoto dstmap_fail;\r\nskb_set_transport_header(skb, transhdr_len);\r\nfrags = fill_aead_req_fields(skb, req, src, ivsize, aeadctx);\r\natomic_inc(&adap->chcr_stats.aead_rqst);\r\ncreate_wreq(ctx, chcr_req, &req->base, skb, kctx_len, 0, 1,\r\nsizeof(struct cpl_rx_phys_dsgl) + dst_size, 0);\r\nreqctx->skb = skb;\r\nskb_get(skb);\r\nreturn skb;\r\ndstmap_fail:\r\nkfree_skb(skb);\r\nerr:\r\nfree_new_sg(reqctx->newdstsg);\r\nreqctx->newdstsg = NULL;\r\nreturn ERR_PTR(error);\r\n}\r\nstatic struct sk_buff *create_gcm_wr(struct aead_request *req,\r\nunsigned short qid,\r\nint size,\r\nunsigned short op_type)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_aead_ctx(tfm);\r\nstruct uld_ctx *u_ctx = ULD_CTX(ctx);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nstruct chcr_aead_reqctx *reqctx = aead_request_ctx(req);\r\nstruct sk_buff *skb = NULL;\r\nstruct chcr_wr *chcr_req;\r\nstruct cpl_rx_phys_dsgl *phys_cpl;\r\nstruct phys_sge_parm sg_param;\r\nstruct scatterlist *src;\r\nunsigned int frags = 0, transhdr_len;\r\nunsigned int ivsize = AES_BLOCK_SIZE;\r\nunsigned int dst_size = 0, kctx_len, nents, assoclen = req->assoclen;\r\nunsigned char tag_offset = 0;\r\nunsigned int authsize = crypto_aead_authsize(tfm);\r\nint error = -EINVAL, src_nent;\r\ngfp_t flags = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\r\nGFP_ATOMIC;\r\nstruct adapter *adap = padap(ctx->dev);\r\nreqctx->newdstsg = NULL;\r\ndst_size = assoclen + req->cryptlen + (op_type ? -authsize :\r\nauthsize);\r\nif (aeadctx->enckey_len == 0)\r\ngoto err;\r\nif (op_type && req->cryptlen < crypto_aead_authsize(tfm))\r\ngoto err;\r\nsrc_nent = sg_nents_for_len(req->src, assoclen + req->cryptlen);\r\nif (src_nent < 0)\r\ngoto err;\r\nsrc = scatterwalk_ffwd(reqctx->srcffwd, req->src, assoclen);\r\nif (req->src != req->dst) {\r\nerror = chcr_copy_assoc(req, aeadctx);\r\nif (error)\r\nreturn ERR_PTR(error);\r\n}\r\nif (dst_size && is_newsg(req->dst, &nents)) {\r\nreqctx->newdstsg = alloc_new_sg(req->dst, nents);\r\nif (IS_ERR(reqctx->newdstsg))\r\nreturn ERR_CAST(reqctx->newdstsg);\r\nreqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,\r\nreqctx->newdstsg, assoclen);\r\n} else {\r\nif (req->src == req->dst)\r\nreqctx->dst = src;\r\nelse\r\nreqctx->dst = scatterwalk_ffwd(reqctx->dstffwd,\r\nreq->dst, assoclen);\r\n}\r\nreqctx->dst_nents = sg_nents_for_len(reqctx->dst, req->cryptlen +\r\n(op_type ? -authsize : authsize));\r\nif (reqctx->dst_nents < 0) {\r\npr_err("GCM:Invalid Destination sg entries\n");\r\nerror = -EINVAL;\r\ngoto err;\r\n}\r\ndst_size = get_space_for_phys_dsgl(reqctx->dst_nents);\r\nkctx_len = ((DIV_ROUND_UP(aeadctx->enckey_len, 16)) << 4) +\r\nAEAD_H_SIZE;\r\ntranshdr_len = CIPHER_TRANSHDR_SIZE(kctx_len, dst_size);\r\nif (chcr_aead_need_fallback(req, src_nent + MIN_GCM_SG,\r\nT6_MAX_AAD_SIZE,\r\ntranshdr_len + (sgl_len(src_nent + MIN_GCM_SG) * 8),\r\nop_type)) {\r\natomic_inc(&adap->chcr_stats.fallback);\r\nfree_new_sg(reqctx->newdstsg);\r\nreqctx->newdstsg = NULL;\r\nreturn ERR_PTR(chcr_aead_fallback(req, op_type));\r\n}\r\nskb = alloc_skb((transhdr_len + sizeof(struct sge_opaque_hdr)), flags);\r\nif (!skb) {\r\nerror = -ENOMEM;\r\ngoto err;\r\n}\r\nskb_reserve(skb, sizeof(struct sge_opaque_hdr));\r\nchcr_req = __skb_put_zero(skb, transhdr_len);\r\nif (get_aead_subtype(tfm) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106)\r\nassoclen = req->assoclen - 8;\r\ntag_offset = (op_type == CHCR_ENCRYPT_OP) ? 0 : authsize;\r\nchcr_req->sec_cpl.op_ivinsrtofst = FILL_SEC_CPL_OP_IVINSR(\r\nctx->dev->rx_channel_id, 2, (ivsize ?\r\n(assoclen + 1) : 0));\r\nchcr_req->sec_cpl.pldlen =\r\nhtonl(assoclen + ivsize + req->cryptlen);\r\nchcr_req->sec_cpl.aadstart_cipherstop_hi = FILL_SEC_CPL_CIPHERSTOP_HI(\r\nassoclen ? 1 : 0, assoclen,\r\nassoclen + ivsize + 1, 0);\r\nchcr_req->sec_cpl.cipherstop_lo_authinsert =\r\nFILL_SEC_CPL_AUTHINSERT(0, assoclen + ivsize + 1,\r\ntag_offset, tag_offset);\r\nchcr_req->sec_cpl.seqno_numivs =\r\nFILL_SEC_CPL_SCMD0_SEQNO(op_type, (op_type ==\r\nCHCR_ENCRYPT_OP) ? 1 : 0,\r\nCHCR_SCMD_CIPHER_MODE_AES_GCM,\r\nCHCR_SCMD_AUTH_MODE_GHASH,\r\naeadctx->hmac_ctrl, ivsize >> 1);\r\nchcr_req->sec_cpl.ivgen_hdrlen = FILL_SEC_CPL_IVGEN_HDRLEN(0, 0, 1,\r\n0, 1, dst_size);\r\nchcr_req->key_ctx.ctx_hdr = aeadctx->key_ctx_hdr;\r\nmemcpy(chcr_req->key_ctx.key, aeadctx->key, aeadctx->enckey_len);\r\nmemcpy(chcr_req->key_ctx.key + (DIV_ROUND_UP(aeadctx->enckey_len, 16) *\r\n16), GCM_CTX(aeadctx)->ghash_h, AEAD_H_SIZE);\r\nif (get_aead_subtype(tfm) ==\r\nCRYPTO_ALG_SUB_TYPE_AEAD_RFC4106) {\r\nmemcpy(reqctx->iv, aeadctx->salt, 4);\r\nmemcpy(reqctx->iv + 4, req->iv, 8);\r\n} else {\r\nmemcpy(reqctx->iv, req->iv, 12);\r\n}\r\n*((unsigned int *)(reqctx->iv + 12)) = htonl(0x01);\r\nphys_cpl = (struct cpl_rx_phys_dsgl *)((u8 *)(chcr_req + 1) + kctx_len);\r\nsg_param.nents = reqctx->dst_nents;\r\nsg_param.obsize = req->cryptlen + (op_type ? -authsize : authsize);\r\nsg_param.qid = qid;\r\nerror = map_writesg_phys_cpl(&u_ctx->lldi.pdev->dev, phys_cpl,\r\nreqctx->dst, &sg_param);\r\nif (error)\r\ngoto dstmap_fail;\r\nskb_set_transport_header(skb, transhdr_len);\r\nwrite_sg_to_skb(skb, &frags, req->src, assoclen);\r\nwrite_buffer_to_skb(skb, &frags, reqctx->iv, ivsize);\r\nwrite_sg_to_skb(skb, &frags, src, req->cryptlen);\r\natomic_inc(&adap->chcr_stats.aead_rqst);\r\ncreate_wreq(ctx, chcr_req, &req->base, skb, kctx_len, size, 1,\r\nsizeof(struct cpl_rx_phys_dsgl) + dst_size,\r\nreqctx->verify);\r\nreqctx->skb = skb;\r\nskb_get(skb);\r\nreturn skb;\r\ndstmap_fail:\r\nkfree_skb(skb);\r\nerr:\r\nfree_new_sg(reqctx->newdstsg);\r\nreqctx->newdstsg = NULL;\r\nreturn ERR_PTR(error);\r\n}\r\nstatic int chcr_aead_cra_init(struct crypto_aead *tfm)\r\n{\r\nstruct chcr_context *ctx = crypto_aead_ctx(tfm);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nstruct aead_alg *alg = crypto_aead_alg(tfm);\r\naeadctx->sw_cipher = crypto_alloc_aead(alg->base.cra_name, 0,\r\nCRYPTO_ALG_NEED_FALLBACK |\r\nCRYPTO_ALG_ASYNC);\r\nif (IS_ERR(aeadctx->sw_cipher))\r\nreturn PTR_ERR(aeadctx->sw_cipher);\r\ncrypto_aead_set_reqsize(tfm, max(sizeof(struct chcr_aead_reqctx),\r\nsizeof(struct aead_request) +\r\ncrypto_aead_reqsize(aeadctx->sw_cipher)));\r\naeadctx->null = crypto_get_default_null_skcipher();\r\nif (IS_ERR(aeadctx->null))\r\nreturn PTR_ERR(aeadctx->null);\r\nreturn chcr_device_init(ctx);\r\n}\r\nstatic void chcr_aead_cra_exit(struct crypto_aead *tfm)\r\n{\r\nstruct chcr_context *ctx = crypto_aead_ctx(tfm);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\ncrypto_put_default_null_skcipher();\r\ncrypto_free_aead(aeadctx->sw_cipher);\r\n}\r\nstatic int chcr_authenc_null_setauthsize(struct crypto_aead *tfm,\r\nunsigned int authsize)\r\n{\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NOP;\r\naeadctx->mayverify = VERIFY_HW;\r\nreturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\r\n}\r\nstatic int chcr_authenc_setauthsize(struct crypto_aead *tfm,\r\nunsigned int authsize)\r\n{\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));\r\nu32 maxauth = crypto_aead_maxauthsize(tfm);\r\nif (authsize == ICV_4) {\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;\r\naeadctx->mayverify = VERIFY_HW;\r\n} else if (authsize == ICV_6) {\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;\r\naeadctx->mayverify = VERIFY_HW;\r\n} else if (authsize == ICV_10) {\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;\r\naeadctx->mayverify = VERIFY_HW;\r\n} else if (authsize == ICV_12) {\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;\r\naeadctx->mayverify = VERIFY_HW;\r\n} else if (authsize == ICV_14) {\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;\r\naeadctx->mayverify = VERIFY_HW;\r\n} else if (authsize == (maxauth >> 1)) {\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;\r\naeadctx->mayverify = VERIFY_HW;\r\n} else if (authsize == maxauth) {\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\r\naeadctx->mayverify = VERIFY_HW;\r\n} else {\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\r\naeadctx->mayverify = VERIFY_SW;\r\n}\r\nreturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\r\n}\r\nstatic int chcr_gcm_setauthsize(struct crypto_aead *tfm, unsigned int authsize)\r\n{\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));\r\nswitch (authsize) {\r\ncase ICV_4:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_8:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_12:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_14:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_16:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_13:\r\ncase ICV_15:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\r\naeadctx->mayverify = VERIFY_SW;\r\nbreak;\r\ndefault:\r\ncrypto_tfm_set_flags((struct crypto_tfm *) tfm,\r\nCRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nreturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\r\n}\r\nstatic int chcr_4106_4309_setauthsize(struct crypto_aead *tfm,\r\nunsigned int authsize)\r\n{\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));\r\nswitch (authsize) {\r\ncase ICV_8:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_12:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_16:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ndefault:\r\ncrypto_tfm_set_flags((struct crypto_tfm *)tfm,\r\nCRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nreturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\r\n}\r\nstatic int chcr_ccm_setauthsize(struct crypto_aead *tfm,\r\nunsigned int authsize)\r\n{\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));\r\nswitch (authsize) {\r\ncase ICV_4:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL1;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_6:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL2;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_8:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_DIV2;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_10:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_TRUNC_RFC4366;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_12:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_IPSEC_96BIT;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_14:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_PL3;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ncase ICV_16:\r\naeadctx->hmac_ctrl = CHCR_SCMD_HMAC_CTRL_NO_TRUNC;\r\naeadctx->mayverify = VERIFY_HW;\r\nbreak;\r\ndefault:\r\ncrypto_tfm_set_flags((struct crypto_tfm *)tfm,\r\nCRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nreturn crypto_aead_setauthsize(aeadctx->sw_cipher, authsize);\r\n}\r\nstatic int chcr_ccm_common_setkey(struct crypto_aead *aead,\r\nconst u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_aead_ctx(aead);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nunsigned char ck_size, mk_size;\r\nint key_ctx_size = 0;\r\nkey_ctx_size = sizeof(struct _key_ctx) +\r\n((DIV_ROUND_UP(keylen, 16)) << 4) * 2;\r\nif (keylen == AES_KEYSIZE_128) {\r\nmk_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\r\n} else if (keylen == AES_KEYSIZE_192) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\r\nmk_size = CHCR_KEYCTX_MAC_KEY_SIZE_192;\r\n} else if (keylen == AES_KEYSIZE_256) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\r\nmk_size = CHCR_KEYCTX_MAC_KEY_SIZE_256;\r\n} else {\r\ncrypto_tfm_set_flags((struct crypto_tfm *)aead,\r\nCRYPTO_TFM_RES_BAD_KEY_LEN);\r\naeadctx->enckey_len = 0;\r\nreturn -EINVAL;\r\n}\r\naeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, mk_size, 0, 0,\r\nkey_ctx_size >> 4);\r\nmemcpy(aeadctx->key, key, keylen);\r\naeadctx->enckey_len = keylen;\r\nreturn 0;\r\n}\r\nstatic int chcr_aead_ccm_setkey(struct crypto_aead *aead,\r\nconst u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_aead_ctx(aead);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nint error;\r\ncrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\r\ncrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &\r\nCRYPTO_TFM_REQ_MASK);\r\nerror = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\r\ncrypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);\r\ncrypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &\r\nCRYPTO_TFM_RES_MASK);\r\nif (error)\r\nreturn error;\r\nreturn chcr_ccm_common_setkey(aead, key, keylen);\r\n}\r\nstatic int chcr_aead_rfc4309_setkey(struct crypto_aead *aead, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_aead_ctx(aead);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nint error;\r\nif (keylen < 3) {\r\ncrypto_tfm_set_flags((struct crypto_tfm *)aead,\r\nCRYPTO_TFM_RES_BAD_KEY_LEN);\r\naeadctx->enckey_len = 0;\r\nreturn -EINVAL;\r\n}\r\ncrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\r\ncrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead) &\r\nCRYPTO_TFM_REQ_MASK);\r\nerror = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\r\ncrypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);\r\ncrypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &\r\nCRYPTO_TFM_RES_MASK);\r\nif (error)\r\nreturn error;\r\nkeylen -= 3;\r\nmemcpy(aeadctx->salt, key + keylen, 3);\r\nreturn chcr_ccm_common_setkey(aead, key, keylen);\r\n}\r\nstatic int chcr_gcm_setkey(struct crypto_aead *aead, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_aead_ctx(aead);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nstruct chcr_gcm_ctx *gctx = GCM_CTX(aeadctx);\r\nstruct crypto_cipher *cipher;\r\nunsigned int ck_size;\r\nint ret = 0, key_ctx_size = 0;\r\naeadctx->enckey_len = 0;\r\ncrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\r\ncrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(aead)\r\n& CRYPTO_TFM_REQ_MASK);\r\nret = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\r\ncrypto_aead_clear_flags(aead, CRYPTO_TFM_RES_MASK);\r\ncrypto_aead_set_flags(aead, crypto_aead_get_flags(aeadctx->sw_cipher) &\r\nCRYPTO_TFM_RES_MASK);\r\nif (ret)\r\ngoto out;\r\nif (get_aead_subtype(aead) == CRYPTO_ALG_SUB_TYPE_AEAD_RFC4106 &&\r\nkeylen > 3) {\r\nkeylen -= 4;\r\nmemcpy(aeadctx->salt, key + keylen, 4);\r\n}\r\nif (keylen == AES_KEYSIZE_128) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\r\n} else if (keylen == AES_KEYSIZE_192) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\r\n} else if (keylen == AES_KEYSIZE_256) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\r\n} else {\r\ncrypto_tfm_set_flags((struct crypto_tfm *)aead,\r\nCRYPTO_TFM_RES_BAD_KEY_LEN);\r\npr_err("GCM: Invalid key length %d\n", keylen);\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nmemcpy(aeadctx->key, key, keylen);\r\naeadctx->enckey_len = keylen;\r\nkey_ctx_size = sizeof(struct _key_ctx) +\r\n((DIV_ROUND_UP(keylen, 16)) << 4) +\r\nAEAD_H_SIZE;\r\naeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size,\r\nCHCR_KEYCTX_MAC_KEY_SIZE_128,\r\n0, 0,\r\nkey_ctx_size >> 4);\r\ncipher = crypto_alloc_cipher("aes-generic", 0, 0);\r\nif (IS_ERR(cipher)) {\r\naeadctx->enckey_len = 0;\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = crypto_cipher_setkey(cipher, key, keylen);\r\nif (ret) {\r\naeadctx->enckey_len = 0;\r\ngoto out1;\r\n}\r\nmemset(gctx->ghash_h, 0, AEAD_H_SIZE);\r\ncrypto_cipher_encrypt_one(cipher, gctx->ghash_h, gctx->ghash_h);\r\nout1:\r\ncrypto_free_cipher(cipher);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int chcr_authenc_setkey(struct crypto_aead *authenc, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_aead_ctx(authenc);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nstruct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);\r\nstruct crypto_authenc_keys keys;\r\nunsigned int bs;\r\nunsigned int max_authsize = crypto_aead_alg(authenc)->maxauthsize;\r\nint err = 0, i, key_ctx_len = 0;\r\nunsigned char ck_size = 0;\r\nunsigned char pad[CHCR_HASH_MAX_BLOCK_SIZE_128] = { 0 };\r\nstruct crypto_shash *base_hash = ERR_PTR(-EINVAL);\r\nstruct algo_param param;\r\nint align;\r\nu8 *o_ptr = NULL;\r\ncrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\r\ncrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)\r\n& CRYPTO_TFM_REQ_MASK);\r\nerr = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\r\ncrypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);\r\ncrypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)\r\n& CRYPTO_TFM_RES_MASK);\r\nif (err)\r\ngoto out;\r\nif (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {\r\ncrypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\ngoto out;\r\n}\r\nif (get_alg_config(&param, max_authsize)) {\r\npr_err("chcr : Unsupported digest size\n");\r\ngoto out;\r\n}\r\nif (keys.enckeylen == AES_KEYSIZE_128) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\r\n} else if (keys.enckeylen == AES_KEYSIZE_192) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\r\n} else if (keys.enckeylen == AES_KEYSIZE_256) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\r\n} else {\r\npr_err("chcr : Unsupported cipher key\n");\r\ngoto out;\r\n}\r\nmemcpy(aeadctx->key, keys.enckey, keys.enckeylen);\r\naeadctx->enckey_len = keys.enckeylen;\r\nget_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,\r\naeadctx->enckey_len << 3);\r\nbase_hash = chcr_alloc_shash(max_authsize);\r\nif (IS_ERR(base_hash)) {\r\npr_err("chcr : Base driver cannot be loaded\n");\r\naeadctx->enckey_len = 0;\r\nreturn -EINVAL;\r\n}\r\n{\r\nSHASH_DESC_ON_STACK(shash, base_hash);\r\nshash->tfm = base_hash;\r\nshash->flags = crypto_shash_get_flags(base_hash);\r\nbs = crypto_shash_blocksize(base_hash);\r\nalign = KEYCTX_ALIGN_PAD(max_authsize);\r\no_ptr = actx->h_iopad + param.result_size + align;\r\nif (keys.authkeylen > bs) {\r\nerr = crypto_shash_digest(shash, keys.authkey,\r\nkeys.authkeylen,\r\no_ptr);\r\nif (err) {\r\npr_err("chcr : Base driver cannot be loaded\n");\r\ngoto out;\r\n}\r\nkeys.authkeylen = max_authsize;\r\n} else\r\nmemcpy(o_ptr, keys.authkey, keys.authkeylen);\r\nmemset(pad + keys.authkeylen, 0, bs - keys.authkeylen);\r\nmemcpy(pad, o_ptr, keys.authkeylen);\r\nfor (i = 0; i < bs >> 2; i++)\r\n*((unsigned int *)pad + i) ^= IPAD_DATA;\r\nif (chcr_compute_partial_hash(shash, pad, actx->h_iopad,\r\nmax_authsize))\r\ngoto out;\r\nmemset(pad + keys.authkeylen, 0, bs - keys.authkeylen);\r\nmemcpy(pad, o_ptr, keys.authkeylen);\r\nfor (i = 0; i < bs >> 2; i++)\r\n*((unsigned int *)pad + i) ^= OPAD_DATA;\r\nif (chcr_compute_partial_hash(shash, pad, o_ptr, max_authsize))\r\ngoto out;\r\nchcr_change_order(actx->h_iopad, param.result_size);\r\nchcr_change_order(o_ptr, param.result_size);\r\nkey_ctx_len = sizeof(struct _key_ctx) +\r\n((DIV_ROUND_UP(keys.enckeylen, 16)) << 4) +\r\n(param.result_size + align) * 2;\r\naeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, param.mk_size,\r\n0, 1, key_ctx_len >> 4);\r\nactx->auth_mode = param.auth_mode;\r\nchcr_free_shash(base_hash);\r\nreturn 0;\r\n}\r\nout:\r\naeadctx->enckey_len = 0;\r\nif (!IS_ERR(base_hash))\r\nchcr_free_shash(base_hash);\r\nreturn -EINVAL;\r\n}\r\nstatic int chcr_aead_digest_null_setkey(struct crypto_aead *authenc,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct chcr_context *ctx = crypto_aead_ctx(authenc);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(ctx);\r\nstruct chcr_authenc_ctx *actx = AUTHENC_CTX(aeadctx);\r\nstruct crypto_authenc_keys keys;\r\nint err;\r\nint key_ctx_len = 0;\r\nunsigned char ck_size = 0;\r\ncrypto_aead_clear_flags(aeadctx->sw_cipher, CRYPTO_TFM_REQ_MASK);\r\ncrypto_aead_set_flags(aeadctx->sw_cipher, crypto_aead_get_flags(authenc)\r\n& CRYPTO_TFM_REQ_MASK);\r\nerr = crypto_aead_setkey(aeadctx->sw_cipher, key, keylen);\r\ncrypto_aead_clear_flags(authenc, CRYPTO_TFM_RES_MASK);\r\ncrypto_aead_set_flags(authenc, crypto_aead_get_flags(aeadctx->sw_cipher)\r\n& CRYPTO_TFM_RES_MASK);\r\nif (err)\r\ngoto out;\r\nif (crypto_authenc_extractkeys(&keys, key, keylen) != 0) {\r\ncrypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\ngoto out;\r\n}\r\nif (keys.enckeylen == AES_KEYSIZE_128) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_128;\r\n} else if (keys.enckeylen == AES_KEYSIZE_192) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_192;\r\n} else if (keys.enckeylen == AES_KEYSIZE_256) {\r\nck_size = CHCR_KEYCTX_CIPHER_KEY_SIZE_256;\r\n} else {\r\npr_err("chcr : Unsupported cipher key\n");\r\ngoto out;\r\n}\r\nmemcpy(aeadctx->key, keys.enckey, keys.enckeylen);\r\naeadctx->enckey_len = keys.enckeylen;\r\nget_aes_decrypt_key(actx->dec_rrkey, aeadctx->key,\r\naeadctx->enckey_len << 3);\r\nkey_ctx_len = sizeof(struct _key_ctx)\r\n+ ((DIV_ROUND_UP(keys.enckeylen, 16)) << 4);\r\naeadctx->key_ctx_hdr = FILL_KEY_CTX_HDR(ck_size, CHCR_KEYCTX_NO_KEY, 0,\r\n0, key_ctx_len >> 4);\r\nactx->auth_mode = CHCR_SCMD_AUTH_MODE_NOP;\r\nreturn 0;\r\nout:\r\naeadctx->enckey_len = 0;\r\nreturn -EINVAL;\r\n}\r\nstatic int chcr_aead_encrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct chcr_aead_reqctx *reqctx = aead_request_ctx(req);\r\nreqctx->verify = VERIFY_HW;\r\nswitch (get_aead_subtype(tfm)) {\r\ncase CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:\r\ncase CRYPTO_ALG_SUB_TYPE_AEAD_NULL:\r\nreturn chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,\r\ncreate_authenc_wr);\r\ncase CRYPTO_ALG_SUB_TYPE_AEAD_CCM:\r\ncase CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:\r\nreturn chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,\r\ncreate_aead_ccm_wr);\r\ndefault:\r\nreturn chcr_aead_op(req, CHCR_ENCRYPT_OP, 0,\r\ncreate_gcm_wr);\r\n}\r\n}\r\nstatic int chcr_aead_decrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct chcr_aead_ctx *aeadctx = AEAD_CTX(crypto_aead_ctx(tfm));\r\nstruct chcr_aead_reqctx *reqctx = aead_request_ctx(req);\r\nint size;\r\nif (aeadctx->mayverify == VERIFY_SW) {\r\nsize = crypto_aead_maxauthsize(tfm);\r\nreqctx->verify = VERIFY_SW;\r\n} else {\r\nsize = 0;\r\nreqctx->verify = VERIFY_HW;\r\n}\r\nswitch (get_aead_subtype(tfm)) {\r\ncase CRYPTO_ALG_SUB_TYPE_AEAD_AUTHENC:\r\ncase CRYPTO_ALG_SUB_TYPE_AEAD_NULL:\r\nreturn chcr_aead_op(req, CHCR_DECRYPT_OP, size,\r\ncreate_authenc_wr);\r\ncase CRYPTO_ALG_SUB_TYPE_AEAD_CCM:\r\ncase CRYPTO_ALG_SUB_TYPE_AEAD_RFC4309:\r\nreturn chcr_aead_op(req, CHCR_DECRYPT_OP, size,\r\ncreate_aead_ccm_wr);\r\ndefault:\r\nreturn chcr_aead_op(req, CHCR_DECRYPT_OP, size,\r\ncreate_gcm_wr);\r\n}\r\n}\r\nstatic int chcr_aead_op(struct aead_request *req,\r\nunsigned short op_type,\r\nint size,\r\ncreate_wr_t create_wr_fn)\r\n{\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct chcr_context *ctx = crypto_aead_ctx(tfm);\r\nstruct uld_ctx *u_ctx;\r\nstruct sk_buff *skb;\r\nif (!ctx->dev) {\r\npr_err("chcr : %s : No crypto device.\n", __func__);\r\nreturn -ENXIO;\r\n}\r\nu_ctx = ULD_CTX(ctx);\r\nif (cxgb4_is_crypto_q_full(u_ctx->lldi.ports[0],\r\nctx->tx_qidx)) {\r\nif (!(req->base.flags & CRYPTO_TFM_REQ_MAY_BACKLOG))\r\nreturn -EBUSY;\r\n}\r\nskb = create_wr_fn(req, u_ctx->lldi.rxq_ids[ctx->rx_qidx], size,\r\nop_type);\r\nif (IS_ERR(skb) || !skb)\r\nreturn PTR_ERR(skb);\r\nskb->dev = u_ctx->lldi.ports[0];\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, ctx->tx_qidx);\r\nchcr_send_wr(skb);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int chcr_unregister_alg(void)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(driver_algs); i++) {\r\nswitch (driver_algs[i].type & CRYPTO_ALG_TYPE_MASK) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\nif (driver_algs[i].is_registered)\r\ncrypto_unregister_alg(\r\n&driver_algs[i].alg.crypto);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\nif (driver_algs[i].is_registered)\r\ncrypto_unregister_aead(\r\n&driver_algs[i].alg.aead);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\nif (driver_algs[i].is_registered)\r\ncrypto_unregister_ahash(\r\n&driver_algs[i].alg.hash);\r\nbreak;\r\n}\r\ndriver_algs[i].is_registered = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic int chcr_register_alg(void)\r\n{\r\nstruct crypto_alg ai;\r\nstruct ahash_alg *a_hash;\r\nint err = 0, i;\r\nchar *name = NULL;\r\nfor (i = 0; i < ARRAY_SIZE(driver_algs); i++) {\r\nif (driver_algs[i].is_registered)\r\ncontinue;\r\nswitch (driver_algs[i].type & CRYPTO_ALG_TYPE_MASK) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\ndriver_algs[i].alg.crypto.cra_priority =\r\nCHCR_CRA_PRIORITY;\r\ndriver_algs[i].alg.crypto.cra_module = THIS_MODULE;\r\ndriver_algs[i].alg.crypto.cra_flags =\r\nCRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_NEED_FALLBACK;\r\ndriver_algs[i].alg.crypto.cra_ctxsize =\r\nsizeof(struct chcr_context) +\r\nsizeof(struct ablk_ctx);\r\ndriver_algs[i].alg.crypto.cra_alignmask = 0;\r\ndriver_algs[i].alg.crypto.cra_type =\r\n&crypto_ablkcipher_type;\r\nerr = crypto_register_alg(&driver_algs[i].alg.crypto);\r\nname = driver_algs[i].alg.crypto.cra_driver_name;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\ndriver_algs[i].alg.aead.base.cra_flags =\r\nCRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_NEED_FALLBACK;\r\ndriver_algs[i].alg.aead.encrypt = chcr_aead_encrypt;\r\ndriver_algs[i].alg.aead.decrypt = chcr_aead_decrypt;\r\ndriver_algs[i].alg.aead.init = chcr_aead_cra_init;\r\ndriver_algs[i].alg.aead.exit = chcr_aead_cra_exit;\r\ndriver_algs[i].alg.aead.base.cra_module = THIS_MODULE;\r\nerr = crypto_register_aead(&driver_algs[i].alg.aead);\r\nname = driver_algs[i].alg.aead.base.cra_driver_name;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\na_hash = &driver_algs[i].alg.hash;\r\na_hash->update = chcr_ahash_update;\r\na_hash->final = chcr_ahash_final;\r\na_hash->finup = chcr_ahash_finup;\r\na_hash->digest = chcr_ahash_digest;\r\na_hash->export = chcr_ahash_export;\r\na_hash->import = chcr_ahash_import;\r\na_hash->halg.statesize = SZ_AHASH_REQ_CTX;\r\na_hash->halg.base.cra_priority = CHCR_CRA_PRIORITY;\r\na_hash->halg.base.cra_module = THIS_MODULE;\r\na_hash->halg.base.cra_flags = AHASH_CRA_FLAGS;\r\na_hash->halg.base.cra_alignmask = 0;\r\na_hash->halg.base.cra_exit = NULL;\r\na_hash->halg.base.cra_type = &crypto_ahash_type;\r\nif (driver_algs[i].type == CRYPTO_ALG_TYPE_HMAC) {\r\na_hash->halg.base.cra_init = chcr_hmac_cra_init;\r\na_hash->halg.base.cra_exit = chcr_hmac_cra_exit;\r\na_hash->init = chcr_hmac_init;\r\na_hash->setkey = chcr_ahash_setkey;\r\na_hash->halg.base.cra_ctxsize = SZ_AHASH_H_CTX;\r\n} else {\r\na_hash->init = chcr_sha_init;\r\na_hash->halg.base.cra_ctxsize = SZ_AHASH_CTX;\r\na_hash->halg.base.cra_init = chcr_sha_cra_init;\r\n}\r\nerr = crypto_register_ahash(&driver_algs[i].alg.hash);\r\nai = driver_algs[i].alg.hash.halg.base;\r\nname = ai.cra_driver_name;\r\nbreak;\r\n}\r\nif (err) {\r\npr_err("chcr : %s : Algorithm registration failed\n",\r\nname);\r\ngoto register_err;\r\n} else {\r\ndriver_algs[i].is_registered = 1;\r\n}\r\n}\r\nreturn 0;\r\nregister_err:\r\nchcr_unregister_alg();\r\nreturn err;\r\n}\r\nint start_crypto(void)\r\n{\r\nreturn chcr_register_alg();\r\n}\r\nint stop_crypto(void)\r\n{\r\nchcr_unregister_alg();\r\nreturn 0;\r\n}
