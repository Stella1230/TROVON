static struct cppi41_channel *to_cpp41_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct cppi41_channel, chan);\r\n}\r\nstatic struct cppi41_channel *desc_to_chan(struct cppi41_dd *cdd, u32 desc)\r\n{\r\nstruct cppi41_channel *c;\r\nu32 descs_size;\r\nu32 desc_num;\r\ndescs_size = sizeof(struct cppi41_desc) * ALLOC_DECS_NUM;\r\nif (!((desc >= cdd->descs_phys) &&\r\n(desc < (cdd->descs_phys + descs_size)))) {\r\nreturn NULL;\r\n}\r\ndesc_num = (desc - cdd->descs_phys) / sizeof(struct cppi41_desc);\r\nBUG_ON(desc_num >= ALLOC_DECS_NUM);\r\nc = cdd->chan_busy[desc_num];\r\ncdd->chan_busy[desc_num] = NULL;\r\npm_runtime_put(cdd->ddev.dev);\r\nreturn c;\r\n}\r\nstatic void cppi_writel(u32 val, void *__iomem *mem)\r\n{\r\n__raw_writel(val, mem);\r\n}\r\nstatic u32 cppi_readl(void *__iomem *mem)\r\n{\r\nreturn __raw_readl(mem);\r\n}\r\nstatic u32 pd_trans_len(u32 val)\r\n{\r\nreturn val & ((1 << (DESC_LENGTH_BITS_NUM + 1)) - 1);\r\n}\r\nstatic u32 cppi41_pop_desc(struct cppi41_dd *cdd, unsigned queue_num)\r\n{\r\nu32 desc;\r\ndesc = cppi_readl(cdd->qmgr_mem + QMGR_QUEUE_D(queue_num));\r\ndesc &= ~0x1f;\r\nreturn desc;\r\n}\r\nstatic irqreturn_t cppi41_irq(int irq, void *data)\r\n{\r\nstruct cppi41_dd *cdd = data;\r\nu16 first_completion_queue = cdd->first_completion_queue;\r\nu16 qmgr_num_pend = cdd->qmgr_num_pend;\r\nstruct cppi41_channel *c;\r\nint i;\r\nfor (i = QMGR_PENDING_SLOT_Q(first_completion_queue); i < qmgr_num_pend;\r\ni++) {\r\nu32 val;\r\nu32 q_num;\r\nval = cppi_readl(cdd->qmgr_mem + QMGR_PEND(i));\r\nif (i == QMGR_PENDING_SLOT_Q(first_completion_queue) && val) {\r\nu32 mask;\r\nmask = 1 << QMGR_PENDING_BIT_Q(first_completion_queue);\r\nmask--;\r\nval &= ~mask;\r\n}\r\nif (val)\r\n__iormb();\r\nwhile (val) {\r\nu32 desc, len;\r\nWARN_ON(cdd->is_suspended);\r\nq_num = __fls(val);\r\nval &= ~(1 << q_num);\r\nq_num += 32 * i;\r\ndesc = cppi41_pop_desc(cdd, q_num);\r\nc = desc_to_chan(cdd, desc);\r\nif (WARN_ON(!c)) {\r\npr_err("%s() q %d desc %08x\n", __func__,\r\nq_num, desc);\r\ncontinue;\r\n}\r\nif (c->desc->pd2 & PD2_ZERO_LENGTH)\r\nlen = 0;\r\nelse\r\nlen = pd_trans_len(c->desc->pd0);\r\nc->residue = pd_trans_len(c->desc->pd6) - len;\r\ndma_cookie_complete(&c->txd);\r\ndmaengine_desc_get_callback_invoke(&c->txd, NULL);\r\n}\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic dma_cookie_t cppi41_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\ndma_cookie_t cookie;\r\ncookie = dma_cookie_assign(tx);\r\nreturn cookie;\r\n}\r\nstatic int cppi41_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nstruct cppi41_dd *cdd = c->cdd;\r\nint error;\r\nerror = pm_runtime_get_sync(cdd->ddev.dev);\r\nif (error < 0) {\r\ndev_err(cdd->ddev.dev, "%s pm runtime get: %i\n",\r\n__func__, error);\r\npm_runtime_put_noidle(cdd->ddev.dev);\r\nreturn error;\r\n}\r\ndma_cookie_init(chan);\r\ndma_async_tx_descriptor_init(&c->txd, chan);\r\nc->txd.tx_submit = cppi41_tx_submit;\r\nif (!c->is_tx)\r\ncppi_writel(c->q_num, c->gcr_reg + RXHPCRA0);\r\npm_runtime_mark_last_busy(cdd->ddev.dev);\r\npm_runtime_put_autosuspend(cdd->ddev.dev);\r\nreturn 0;\r\n}\r\nstatic void cppi41_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nstruct cppi41_dd *cdd = c->cdd;\r\nint error;\r\nerror = pm_runtime_get_sync(cdd->ddev.dev);\r\nif (error < 0) {\r\npm_runtime_put_noidle(cdd->ddev.dev);\r\nreturn;\r\n}\r\nWARN_ON(!list_empty(&cdd->pending));\r\npm_runtime_mark_last_busy(cdd->ddev.dev);\r\npm_runtime_put_autosuspend(cdd->ddev.dev);\r\n}\r\nstatic enum dma_status cppi41_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nenum dma_status ret;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\ndma_set_residue(txstate, c->residue);\r\nreturn ret;\r\n}\r\nstatic void push_desc_queue(struct cppi41_channel *c)\r\n{\r\nstruct cppi41_dd *cdd = c->cdd;\r\nu32 desc_num;\r\nu32 desc_phys;\r\nu32 reg;\r\nc->residue = 0;\r\nreg = GCR_CHAN_ENABLE;\r\nif (!c->is_tx) {\r\nreg |= GCR_STARV_RETRY;\r\nreg |= GCR_DESC_TYPE_HOST;\r\nreg |= c->q_comp_num;\r\n}\r\ncppi_writel(reg, c->gcr_reg);\r\n__iowmb();\r\npm_runtime_get(cdd->ddev.dev);\r\ndesc_phys = lower_32_bits(c->desc_phys);\r\ndesc_num = (desc_phys - cdd->descs_phys) / sizeof(struct cppi41_desc);\r\nWARN_ON(cdd->chan_busy[desc_num]);\r\ncdd->chan_busy[desc_num] = c;\r\nreg = (sizeof(struct cppi41_desc) - 24) / 4;\r\nreg |= desc_phys;\r\ncppi_writel(reg, cdd->qmgr_mem + QMGR_QUEUE_D(c->q_num));\r\n}\r\nstatic void cppi41_run_queue(struct cppi41_dd *cdd)\r\n{\r\nstruct cppi41_channel *c, *_c;\r\nlist_for_each_entry_safe(c, _c, &cdd->pending, node) {\r\npush_desc_queue(c);\r\nlist_del(&c->node);\r\n}\r\n}\r\nstatic void cppi41_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nstruct cppi41_dd *cdd = c->cdd;\r\nunsigned long flags;\r\nint error;\r\nerror = pm_runtime_get(cdd->ddev.dev);\r\nif ((error != -EINPROGRESS) && error < 0) {\r\npm_runtime_put_noidle(cdd->ddev.dev);\r\ndev_err(cdd->ddev.dev, "Failed to pm_runtime_get: %i\n",\r\nerror);\r\nreturn;\r\n}\r\nspin_lock_irqsave(&cdd->lock, flags);\r\nlist_add_tail(&c->node, &cdd->pending);\r\nif (!cdd->is_suspended)\r\ncppi41_run_queue(cdd);\r\nspin_unlock_irqrestore(&cdd->lock, flags);\r\npm_runtime_mark_last_busy(cdd->ddev.dev);\r\npm_runtime_put_autosuspend(cdd->ddev.dev);\r\n}\r\nstatic u32 get_host_pd0(u32 length)\r\n{\r\nu32 reg;\r\nreg = DESC_TYPE_HOST << DESC_TYPE;\r\nreg |= length;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd1(struct cppi41_channel *c)\r\n{\r\nu32 reg;\r\nreg = 0;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd2(struct cppi41_channel *c)\r\n{\r\nu32 reg;\r\nreg = DESC_TYPE_USB;\r\nreg |= c->q_comp_num;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd3(u32 length)\r\n{\r\nu32 reg;\r\nreg = length;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd6(u32 length)\r\n{\r\nu32 reg;\r\nreg = DESC_PD_COMPLETE;\r\nreg |= length;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd4_or_7(u32 addr)\r\n{\r\nu32 reg;\r\nreg = addr;\r\nreturn reg;\r\n}\r\nstatic u32 get_host_pd5(void)\r\n{\r\nu32 reg;\r\nreg = 0;\r\nreturn reg;\r\n}\r\nstatic struct dma_async_tx_descriptor *cppi41_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl, unsigned sg_len,\r\nenum dma_transfer_direction dir, unsigned long tx_flags, void *context)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nstruct cppi41_desc *d;\r\nstruct scatterlist *sg;\r\nunsigned int i;\r\nd = c->desc;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nu32 addr;\r\nu32 len;\r\naddr = lower_32_bits(sg_dma_address(sg));\r\nlen = sg_dma_len(sg);\r\nd->pd0 = get_host_pd0(len);\r\nd->pd1 = get_host_pd1(c);\r\nd->pd2 = get_host_pd2(c);\r\nd->pd3 = get_host_pd3(len);\r\nd->pd4 = get_host_pd4_or_7(addr);\r\nd->pd5 = get_host_pd5();\r\nd->pd6 = get_host_pd6(len);\r\nd->pd7 = get_host_pd4_or_7(addr);\r\nd++;\r\n}\r\nreturn &c->txd;\r\n}\r\nstatic void cppi41_compute_td_desc(struct cppi41_desc *d)\r\n{\r\nd->pd0 = DESC_TYPE_TEARD << DESC_TYPE;\r\n}\r\nstatic int cppi41_tear_down_chan(struct cppi41_channel *c)\r\n{\r\nstruct dmaengine_result abort_result;\r\nstruct cppi41_dd *cdd = c->cdd;\r\nstruct cppi41_desc *td;\r\nu32 reg;\r\nu32 desc_phys;\r\nu32 td_desc_phys;\r\ntd = cdd->cd;\r\ntd += cdd->first_td_desc;\r\ntd_desc_phys = cdd->descs_phys;\r\ntd_desc_phys += cdd->first_td_desc * sizeof(struct cppi41_desc);\r\nif (!c->td_queued) {\r\ncppi41_compute_td_desc(td);\r\n__iowmb();\r\nreg = (sizeof(struct cppi41_desc) - 24) / 4;\r\nreg |= td_desc_phys;\r\ncppi_writel(reg, cdd->qmgr_mem +\r\nQMGR_QUEUE_D(cdd->td_queue.submit));\r\nreg = GCR_CHAN_ENABLE;\r\nif (!c->is_tx) {\r\nreg |= GCR_STARV_RETRY;\r\nreg |= GCR_DESC_TYPE_HOST;\r\nreg |= cdd->td_queue.complete;\r\n}\r\nreg |= GCR_TEARDOWN;\r\ncppi_writel(reg, c->gcr_reg);\r\nc->td_queued = 1;\r\nc->td_retry = 500;\r\n}\r\nif (!c->td_seen || !c->td_desc_seen) {\r\ndesc_phys = cppi41_pop_desc(cdd, cdd->td_queue.complete);\r\nif (!desc_phys && c->is_tx)\r\ndesc_phys = cppi41_pop_desc(cdd, c->q_comp_num);\r\nif (desc_phys == c->desc_phys) {\r\nc->td_desc_seen = 1;\r\n} else if (desc_phys == td_desc_phys) {\r\nu32 pd0;\r\n__iormb();\r\npd0 = td->pd0;\r\nWARN_ON((pd0 >> DESC_TYPE) != DESC_TYPE_TEARD);\r\nWARN_ON(!c->is_tx && !(pd0 & TD_DESC_IS_RX));\r\nWARN_ON((pd0 & 0x1f) != c->port_num);\r\nc->td_seen = 1;\r\n} else if (desc_phys) {\r\nWARN_ON_ONCE(1);\r\n}\r\n}\r\nc->td_retry--;\r\nif (!c->td_seen && c->td_retry) {\r\nudelay(1);\r\nreturn -EAGAIN;\r\n}\r\nWARN_ON(!c->td_retry);\r\nif (!c->td_desc_seen) {\r\ndesc_phys = cppi41_pop_desc(cdd, c->q_num);\r\nif (!desc_phys)\r\ndesc_phys = cppi41_pop_desc(cdd, c->q_comp_num);\r\nWARN_ON(!desc_phys);\r\n}\r\nc->td_queued = 0;\r\nc->td_seen = 0;\r\nc->td_desc_seen = 0;\r\ncppi_writel(0, c->gcr_reg);\r\nabort_result.result = DMA_TRANS_ABORTED;\r\ndma_cookie_complete(&c->txd);\r\ndmaengine_desc_get_callback_invoke(&c->txd, &abort_result);\r\nreturn 0;\r\n}\r\nstatic int cppi41_stop_chan(struct dma_chan *chan)\r\n{\r\nstruct cppi41_channel *c = to_cpp41_chan(chan);\r\nstruct cppi41_dd *cdd = c->cdd;\r\nu32 desc_num;\r\nu32 desc_phys;\r\nint ret;\r\ndesc_phys = lower_32_bits(c->desc_phys);\r\ndesc_num = (desc_phys - cdd->descs_phys) / sizeof(struct cppi41_desc);\r\nif (!cdd->chan_busy[desc_num])\r\nreturn 0;\r\nret = cppi41_tear_down_chan(c);\r\nif (ret)\r\nreturn ret;\r\nWARN_ON(!cdd->chan_busy[desc_num]);\r\ncdd->chan_busy[desc_num] = NULL;\r\npm_runtime_put(cdd->ddev.dev);\r\nreturn 0;\r\n}\r\nstatic int cppi41_add_chans(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\nstruct cppi41_channel *cchan, *chans;\r\nint i;\r\nu32 n_chans = cdd->n_chans;\r\nn_chans *= 2;\r\nchans = devm_kcalloc(dev, n_chans, sizeof(*chans), GFP_KERNEL);\r\nif (!chans)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < n_chans; i++) {\r\ncchan = &chans[i];\r\ncchan->cdd = cdd;\r\nif (i & 1) {\r\ncchan->gcr_reg = cdd->ctrl_mem + DMA_TXGCR(i >> 1);\r\ncchan->is_tx = 1;\r\n} else {\r\ncchan->gcr_reg = cdd->ctrl_mem + DMA_RXGCR(i >> 1);\r\ncchan->is_tx = 0;\r\n}\r\ncchan->port_num = i >> 1;\r\ncchan->desc = &cdd->cd[i];\r\ncchan->desc_phys = cdd->descs_phys;\r\ncchan->desc_phys += i * sizeof(struct cppi41_desc);\r\ncchan->chan.device = &cdd->ddev;\r\nlist_add_tail(&cchan->chan.device_node, &cdd->ddev.channels);\r\n}\r\ncdd->first_td_desc = n_chans;\r\nreturn 0;\r\n}\r\nstatic void purge_descs(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\nunsigned int mem_decs;\r\nint i;\r\nmem_decs = ALLOC_DECS_NUM * sizeof(struct cppi41_desc);\r\nfor (i = 0; i < DESCS_AREAS; i++) {\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_MEMBASE(i));\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_MEMCTRL(i));\r\ndma_free_coherent(dev, mem_decs, cdd->cd,\r\ncdd->descs_phys);\r\n}\r\n}\r\nstatic void disable_sched(struct cppi41_dd *cdd)\r\n{\r\ncppi_writel(0, cdd->sched_mem + DMA_SCHED_CTRL);\r\n}\r\nstatic void deinit_cppi41(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\ndisable_sched(cdd);\r\npurge_descs(dev, cdd);\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_LRAM0_BASE);\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_LRAM0_BASE);\r\ndma_free_coherent(dev, QMGR_SCRATCH_SIZE, cdd->qmgr_scratch,\r\ncdd->scratch_phys);\r\n}\r\nstatic int init_descs(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\nunsigned int desc_size;\r\nunsigned int mem_decs;\r\nint i;\r\nu32 reg;\r\nu32 idx;\r\nBUILD_BUG_ON(sizeof(struct cppi41_desc) &\r\n(sizeof(struct cppi41_desc) - 1));\r\nBUILD_BUG_ON(sizeof(struct cppi41_desc) < 32);\r\nBUILD_BUG_ON(ALLOC_DECS_NUM < 32);\r\ndesc_size = sizeof(struct cppi41_desc);\r\nmem_decs = ALLOC_DECS_NUM * desc_size;\r\nidx = 0;\r\nfor (i = 0; i < DESCS_AREAS; i++) {\r\nreg = idx << QMGR_MEMCTRL_IDX_SH;\r\nreg |= (ilog2(desc_size) - 5) << QMGR_MEMCTRL_DESC_SH;\r\nreg |= ilog2(ALLOC_DECS_NUM) - 5;\r\nBUILD_BUG_ON(DESCS_AREAS != 1);\r\ncdd->cd = dma_alloc_coherent(dev, mem_decs,\r\n&cdd->descs_phys, GFP_KERNEL);\r\nif (!cdd->cd)\r\nreturn -ENOMEM;\r\ncppi_writel(cdd->descs_phys, cdd->qmgr_mem + QMGR_MEMBASE(i));\r\ncppi_writel(reg, cdd->qmgr_mem + QMGR_MEMCTRL(i));\r\nidx += ALLOC_DECS_NUM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void init_sched(struct cppi41_dd *cdd)\r\n{\r\nunsigned ch;\r\nunsigned word;\r\nu32 reg;\r\nword = 0;\r\ncppi_writel(0, cdd->sched_mem + DMA_SCHED_CTRL);\r\nfor (ch = 0; ch < cdd->n_chans; ch += 2) {\r\nreg = SCHED_ENTRY0_CHAN(ch);\r\nreg |= SCHED_ENTRY1_CHAN(ch) | SCHED_ENTRY1_IS_RX;\r\nreg |= SCHED_ENTRY2_CHAN(ch + 1);\r\nreg |= SCHED_ENTRY3_CHAN(ch + 1) | SCHED_ENTRY3_IS_RX;\r\ncppi_writel(reg, cdd->sched_mem + DMA_SCHED_WORD(word));\r\nword++;\r\n}\r\nreg = cdd->n_chans * 2 - 1;\r\nreg |= DMA_SCHED_CTRL_EN;\r\ncppi_writel(reg, cdd->sched_mem + DMA_SCHED_CTRL);\r\n}\r\nstatic int init_cppi41(struct device *dev, struct cppi41_dd *cdd)\r\n{\r\nint ret;\r\nBUILD_BUG_ON(QMGR_SCRATCH_SIZE > ((1 << 14) - 1));\r\ncdd->qmgr_scratch = dma_alloc_coherent(dev, QMGR_SCRATCH_SIZE,\r\n&cdd->scratch_phys, GFP_KERNEL);\r\nif (!cdd->qmgr_scratch)\r\nreturn -ENOMEM;\r\ncppi_writel(cdd->scratch_phys, cdd->qmgr_mem + QMGR_LRAM0_BASE);\r\ncppi_writel(TOTAL_DESCS_NUM, cdd->qmgr_mem + QMGR_LRAM_SIZE);\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_LRAM1_BASE);\r\nret = init_descs(dev, cdd);\r\nif (ret)\r\ngoto err_td;\r\ncppi_writel(cdd->td_queue.submit, cdd->ctrl_mem + DMA_TDFDQ);\r\ninit_sched(cdd);\r\nreturn 0;\r\nerr_td:\r\ndeinit_cppi41(dev, cdd);\r\nreturn ret;\r\n}\r\nstatic bool cpp41_dma_filter_fn(struct dma_chan *chan, void *param)\r\n{\r\nstruct cppi41_channel *cchan;\r\nstruct cppi41_dd *cdd;\r\nconst struct chan_queues *queues;\r\nu32 *num = param;\r\nif (chan->device->dev->driver != &cpp41_dma_driver.driver)\r\nreturn false;\r\ncchan = to_cpp41_chan(chan);\r\nif (cchan->port_num != num[INFO_PORT])\r\nreturn false;\r\nif (cchan->is_tx && !num[INFO_IS_TX])\r\nreturn false;\r\ncdd = cchan->cdd;\r\nif (cchan->is_tx)\r\nqueues = cdd->queues_tx;\r\nelse\r\nqueues = cdd->queues_rx;\r\nBUILD_BUG_ON(ARRAY_SIZE(am335x_usb_queues_rx) !=\r\nARRAY_SIZE(am335x_usb_queues_tx));\r\nif (WARN_ON(cchan->port_num > ARRAY_SIZE(am335x_usb_queues_rx)))\r\nreturn false;\r\ncchan->q_num = queues[cchan->port_num].submit;\r\ncchan->q_comp_num = queues[cchan->port_num].complete;\r\nreturn true;\r\n}\r\nstatic struct dma_chan *cppi41_dma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nint count = dma_spec->args_count;\r\nstruct of_dma_filter_info *info = ofdma->of_dma_data;\r\nif (!info || !info->filter_fn)\r\nreturn NULL;\r\nif (count != 2)\r\nreturn NULL;\r\nreturn dma_request_channel(info->dma_cap, info->filter_fn,\r\n&dma_spec->args[0]);\r\n}\r\nstatic const struct cppi_glue_infos *get_glue_info(struct device *dev)\r\n{\r\nconst struct of_device_id *of_id;\r\nof_id = of_match_node(cppi41_dma_ids, dev->of_node);\r\nif (!of_id)\r\nreturn NULL;\r\nreturn of_id->data;\r\n}\r\nstatic int cppi41_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct cppi41_dd *cdd;\r\nstruct device *dev = &pdev->dev;\r\nconst struct cppi_glue_infos *glue_info;\r\nstruct resource *mem;\r\nint index;\r\nint irq;\r\nint ret;\r\nglue_info = get_glue_info(dev);\r\nif (!glue_info)\r\nreturn -EINVAL;\r\ncdd = devm_kzalloc(&pdev->dev, sizeof(*cdd), GFP_KERNEL);\r\nif (!cdd)\r\nreturn -ENOMEM;\r\ndma_cap_set(DMA_SLAVE, cdd->ddev.cap_mask);\r\ncdd->ddev.device_alloc_chan_resources = cppi41_dma_alloc_chan_resources;\r\ncdd->ddev.device_free_chan_resources = cppi41_dma_free_chan_resources;\r\ncdd->ddev.device_tx_status = cppi41_dma_tx_status;\r\ncdd->ddev.device_issue_pending = cppi41_dma_issue_pending;\r\ncdd->ddev.device_prep_slave_sg = cppi41_dma_prep_slave_sg;\r\ncdd->ddev.device_terminate_all = cppi41_stop_chan;\r\ncdd->ddev.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\ncdd->ddev.src_addr_widths = CPPI41_DMA_BUSWIDTHS;\r\ncdd->ddev.dst_addr_widths = CPPI41_DMA_BUSWIDTHS;\r\ncdd->ddev.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\ncdd->ddev.dev = dev;\r\nINIT_LIST_HEAD(&cdd->ddev.channels);\r\ncpp41_dma_info.dma_cap = cdd->ddev.cap_mask;\r\nindex = of_property_match_string(dev->of_node,\r\n"reg-names", "controller");\r\nif (index < 0)\r\nreturn index;\r\nmem = platform_get_resource(pdev, IORESOURCE_MEM, index);\r\ncdd->ctrl_mem = devm_ioremap_resource(dev, mem);\r\nif (IS_ERR(cdd->ctrl_mem))\r\nreturn PTR_ERR(cdd->ctrl_mem);\r\nmem = platform_get_resource(pdev, IORESOURCE_MEM, index + 1);\r\ncdd->sched_mem = devm_ioremap_resource(dev, mem);\r\nif (IS_ERR(cdd->sched_mem))\r\nreturn PTR_ERR(cdd->sched_mem);\r\nmem = platform_get_resource(pdev, IORESOURCE_MEM, index + 2);\r\ncdd->qmgr_mem = devm_ioremap_resource(dev, mem);\r\nif (IS_ERR(cdd->qmgr_mem))\r\nreturn PTR_ERR(cdd->qmgr_mem);\r\nspin_lock_init(&cdd->lock);\r\nINIT_LIST_HEAD(&cdd->pending);\r\nplatform_set_drvdata(pdev, cdd);\r\npm_runtime_enable(dev);\r\npm_runtime_set_autosuspend_delay(dev, 100);\r\npm_runtime_use_autosuspend(dev);\r\nret = pm_runtime_get_sync(dev);\r\nif (ret < 0)\r\ngoto err_get_sync;\r\ncdd->queues_rx = glue_info->queues_rx;\r\ncdd->queues_tx = glue_info->queues_tx;\r\ncdd->td_queue = glue_info->td_queue;\r\ncdd->qmgr_num_pend = glue_info->qmgr_num_pend;\r\ncdd->first_completion_queue = glue_info->first_completion_queue;\r\nret = of_property_read_u32(dev->of_node,\r\n"#dma-channels", &cdd->n_chans);\r\nif (ret)\r\ngoto err_get_n_chans;\r\nret = init_cppi41(dev, cdd);\r\nif (ret)\r\ngoto err_init_cppi;\r\nret = cppi41_add_chans(dev, cdd);\r\nif (ret)\r\ngoto err_chans;\r\nirq = irq_of_parse_and_map(dev->of_node, 0);\r\nif (!irq) {\r\nret = -EINVAL;\r\ngoto err_chans;\r\n}\r\nret = devm_request_irq(&pdev->dev, irq, cppi41_irq, IRQF_SHARED,\r\ndev_name(dev), cdd);\r\nif (ret)\r\ngoto err_chans;\r\ncdd->irq = irq;\r\nret = dma_async_device_register(&cdd->ddev);\r\nif (ret)\r\ngoto err_chans;\r\nret = of_dma_controller_register(dev->of_node,\r\ncppi41_dma_xlate, &cpp41_dma_info);\r\nif (ret)\r\ngoto err_of;\r\npm_runtime_mark_last_busy(dev);\r\npm_runtime_put_autosuspend(dev);\r\nreturn 0;\r\nerr_of:\r\ndma_async_device_unregister(&cdd->ddev);\r\nerr_chans:\r\ndeinit_cppi41(dev, cdd);\r\nerr_init_cppi:\r\npm_runtime_dont_use_autosuspend(dev);\r\nerr_get_n_chans:\r\nerr_get_sync:\r\npm_runtime_put_sync(dev);\r\npm_runtime_disable(dev);\r\nreturn ret;\r\n}\r\nstatic int cppi41_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct cppi41_dd *cdd = platform_get_drvdata(pdev);\r\nint error;\r\nerror = pm_runtime_get_sync(&pdev->dev);\r\nif (error < 0)\r\ndev_err(&pdev->dev, "%s could not pm_runtime_get: %i\n",\r\n__func__, error);\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&cdd->ddev);\r\ndevm_free_irq(&pdev->dev, cdd->irq, cdd);\r\ndeinit_cppi41(&pdev->dev, cdd);\r\npm_runtime_dont_use_autosuspend(&pdev->dev);\r\npm_runtime_put_sync(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused cppi41_suspend(struct device *dev)\r\n{\r\nstruct cppi41_dd *cdd = dev_get_drvdata(dev);\r\ncdd->dma_tdfdq = cppi_readl(cdd->ctrl_mem + DMA_TDFDQ);\r\ndisable_sched(cdd);\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused cppi41_resume(struct device *dev)\r\n{\r\nstruct cppi41_dd *cdd = dev_get_drvdata(dev);\r\nstruct cppi41_channel *c;\r\nint i;\r\nfor (i = 0; i < DESCS_AREAS; i++)\r\ncppi_writel(cdd->descs_phys, cdd->qmgr_mem + QMGR_MEMBASE(i));\r\nlist_for_each_entry(c, &cdd->ddev.channels, chan.device_node)\r\nif (!c->is_tx)\r\ncppi_writel(c->q_num, c->gcr_reg + RXHPCRA0);\r\ninit_sched(cdd);\r\ncppi_writel(cdd->dma_tdfdq, cdd->ctrl_mem + DMA_TDFDQ);\r\ncppi_writel(cdd->scratch_phys, cdd->qmgr_mem + QMGR_LRAM0_BASE);\r\ncppi_writel(QMGR_SCRATCH_SIZE, cdd->qmgr_mem + QMGR_LRAM_SIZE);\r\ncppi_writel(0, cdd->qmgr_mem + QMGR_LRAM1_BASE);\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused cppi41_runtime_suspend(struct device *dev)\r\n{\r\nstruct cppi41_dd *cdd = dev_get_drvdata(dev);\r\nunsigned long flags;\r\nspin_lock_irqsave(&cdd->lock, flags);\r\ncdd->is_suspended = true;\r\nWARN_ON(!list_empty(&cdd->pending));\r\nspin_unlock_irqrestore(&cdd->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int __maybe_unused cppi41_runtime_resume(struct device *dev)\r\n{\r\nstruct cppi41_dd *cdd = dev_get_drvdata(dev);\r\nunsigned long flags;\r\nspin_lock_irqsave(&cdd->lock, flags);\r\ncdd->is_suspended = false;\r\ncppi41_run_queue(cdd);\r\nspin_unlock_irqrestore(&cdd->lock, flags);\r\nreturn 0;\r\n}
