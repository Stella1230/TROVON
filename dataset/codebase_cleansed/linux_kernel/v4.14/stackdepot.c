static bool init_stack_slab(void **prealloc)\r\n{\r\nif (!*prealloc)\r\nreturn false;\r\nif (smp_load_acquire(&next_slab_inited))\r\nreturn true;\r\nif (stack_slabs[depot_index] == NULL) {\r\nstack_slabs[depot_index] = *prealloc;\r\n} else {\r\nstack_slabs[depot_index + 1] = *prealloc;\r\nsmp_store_release(&next_slab_inited, 1);\r\n}\r\n*prealloc = NULL;\r\nreturn true;\r\n}\r\nstatic struct stack_record *depot_alloc_stack(unsigned long *entries, int size,\r\nu32 hash, void **prealloc, gfp_t alloc_flags)\r\n{\r\nint required_size = offsetof(struct stack_record, entries) +\r\nsizeof(unsigned long) * size;\r\nstruct stack_record *stack;\r\nrequired_size = ALIGN(required_size, 1 << STACK_ALLOC_ALIGN);\r\nif (unlikely(depot_offset + required_size > STACK_ALLOC_SIZE)) {\r\nif (unlikely(depot_index + 1 >= STACK_ALLOC_MAX_SLABS)) {\r\nWARN_ONCE(1, "Stack depot reached limit capacity");\r\nreturn NULL;\r\n}\r\ndepot_index++;\r\ndepot_offset = 0;\r\nif (depot_index + 1 < STACK_ALLOC_MAX_SLABS)\r\nsmp_store_release(&next_slab_inited, 0);\r\n}\r\ninit_stack_slab(prealloc);\r\nif (stack_slabs[depot_index] == NULL)\r\nreturn NULL;\r\nstack = stack_slabs[depot_index] + depot_offset;\r\nstack->hash = hash;\r\nstack->size = size;\r\nstack->handle.slabindex = depot_index;\r\nstack->handle.offset = depot_offset >> STACK_ALLOC_ALIGN;\r\nstack->handle.valid = 1;\r\nmemcpy(stack->entries, entries, size * sizeof(unsigned long));\r\ndepot_offset += required_size;\r\nreturn stack;\r\n}\r\nstatic inline u32 hash_stack(unsigned long *entries, unsigned int size)\r\n{\r\nreturn jhash2((u32 *)entries,\r\nsize * sizeof(unsigned long) / sizeof(u32),\r\nSTACK_HASH_SEED);\r\n}\r\nstatic inline struct stack_record *find_stack(struct stack_record *bucket,\r\nunsigned long *entries, int size,\r\nu32 hash)\r\n{\r\nstruct stack_record *found;\r\nfor (found = bucket; found; found = found->next) {\r\nif (found->hash == hash &&\r\nfound->size == size &&\r\n!memcmp(entries, found->entries,\r\nsize * sizeof(unsigned long))) {\r\nreturn found;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nvoid depot_fetch_stack(depot_stack_handle_t handle, struct stack_trace *trace)\r\n{\r\nunion handle_parts parts = { .handle = handle };\r\nvoid *slab = stack_slabs[parts.slabindex];\r\nsize_t offset = parts.offset << STACK_ALLOC_ALIGN;\r\nstruct stack_record *stack = slab + offset;\r\ntrace->nr_entries = trace->max_entries = stack->size;\r\ntrace->entries = stack->entries;\r\ntrace->skip = 0;\r\n}\r\ndepot_stack_handle_t depot_save_stack(struct stack_trace *trace,\r\ngfp_t alloc_flags)\r\n{\r\nu32 hash;\r\ndepot_stack_handle_t retval = 0;\r\nstruct stack_record *found = NULL, **bucket;\r\nunsigned long flags;\r\nstruct page *page = NULL;\r\nvoid *prealloc = NULL;\r\nif (unlikely(trace->nr_entries == 0))\r\ngoto fast_exit;\r\nhash = hash_stack(trace->entries, trace->nr_entries);\r\nbucket = &stack_table[hash & STACK_HASH_MASK];\r\nfound = find_stack(smp_load_acquire(bucket), trace->entries,\r\ntrace->nr_entries, hash);\r\nif (found)\r\ngoto exit;\r\nif (unlikely(!smp_load_acquire(&next_slab_inited))) {\r\nalloc_flags &= ~GFP_ZONEMASK;\r\nalloc_flags &= (GFP_ATOMIC | GFP_KERNEL);\r\nalloc_flags |= __GFP_NOWARN;\r\npage = alloc_pages(alloc_flags, STACK_ALLOC_ORDER);\r\nif (page)\r\nprealloc = page_address(page);\r\n}\r\nspin_lock_irqsave(&depot_lock, flags);\r\nfound = find_stack(*bucket, trace->entries, trace->nr_entries, hash);\r\nif (!found) {\r\nstruct stack_record *new =\r\ndepot_alloc_stack(trace->entries, trace->nr_entries,\r\nhash, &prealloc, alloc_flags);\r\nif (new) {\r\nnew->next = *bucket;\r\nsmp_store_release(bucket, new);\r\nfound = new;\r\n}\r\n} else if (prealloc) {\r\nWARN_ON(!init_stack_slab(&prealloc));\r\n}\r\nspin_unlock_irqrestore(&depot_lock, flags);\r\nexit:\r\nif (prealloc) {\r\nfree_pages((unsigned long)prealloc, STACK_ALLOC_ORDER);\r\n}\r\nif (found)\r\nretval = found->handle.handle;\r\nfast_exit:\r\nreturn retval;\r\n}
