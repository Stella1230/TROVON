static int __init setup_unaligned_printk(char *str)\r\n{\r\nlong val;\r\nif (kstrtol(str, 0, &val) != 0)\r\nreturn 0;\r\nunaligned_printk = val;\r\npr_info("Printk for each unaligned data accesses is %s\n",\r\nunaligned_printk ? "enabled" : "disabled");\r\nreturn 1;\r\n}\r\nstatic bool is_bundle_x0_nop(tilegx_bundle_bits bundle)\r\n{\r\nreturn (((get_UnaryOpcodeExtension_X0(bundle) ==\r\nNOP_UNARY_OPCODE_X0) &&\r\n(get_RRROpcodeExtension_X0(bundle) ==\r\nUNARY_RRR_0_OPCODE_X0) &&\r\n(get_Opcode_X0(bundle) ==\r\nRRR_0_OPCODE_X0)) ||\r\n((get_UnaryOpcodeExtension_X0(bundle) ==\r\nFNOP_UNARY_OPCODE_X0) &&\r\n(get_RRROpcodeExtension_X0(bundle) ==\r\nUNARY_RRR_0_OPCODE_X0) &&\r\n(get_Opcode_X0(bundle) ==\r\nRRR_0_OPCODE_X0)));\r\n}\r\nstatic bool is_bundle_x1_nop(tilegx_bundle_bits bundle)\r\n{\r\nreturn (((get_UnaryOpcodeExtension_X1(bundle) ==\r\nNOP_UNARY_OPCODE_X1) &&\r\n(get_RRROpcodeExtension_X1(bundle) ==\r\nUNARY_RRR_0_OPCODE_X1) &&\r\n(get_Opcode_X1(bundle) ==\r\nRRR_0_OPCODE_X1)) ||\r\n((get_UnaryOpcodeExtension_X1(bundle) ==\r\nFNOP_UNARY_OPCODE_X1) &&\r\n(get_RRROpcodeExtension_X1(bundle) ==\r\nUNARY_RRR_0_OPCODE_X1) &&\r\n(get_Opcode_X1(bundle) ==\r\nRRR_0_OPCODE_X1)));\r\n}\r\nstatic bool is_bundle_y0_nop(tilegx_bundle_bits bundle)\r\n{\r\nreturn (((get_UnaryOpcodeExtension_Y0(bundle) ==\r\nNOP_UNARY_OPCODE_Y0) &&\r\n(get_RRROpcodeExtension_Y0(bundle) ==\r\nUNARY_RRR_1_OPCODE_Y0) &&\r\n(get_Opcode_Y0(bundle) ==\r\nRRR_1_OPCODE_Y0)) ||\r\n((get_UnaryOpcodeExtension_Y0(bundle) ==\r\nFNOP_UNARY_OPCODE_Y0) &&\r\n(get_RRROpcodeExtension_Y0(bundle) ==\r\nUNARY_RRR_1_OPCODE_Y0) &&\r\n(get_Opcode_Y0(bundle) ==\r\nRRR_1_OPCODE_Y0)));\r\n}\r\nstatic bool is_bundle_y1_nop(tilegx_bundle_bits bundle)\r\n{\r\nreturn (((get_UnaryOpcodeExtension_Y1(bundle) ==\r\nNOP_UNARY_OPCODE_Y1) &&\r\n(get_RRROpcodeExtension_Y1(bundle) ==\r\nUNARY_RRR_1_OPCODE_Y1) &&\r\n(get_Opcode_Y1(bundle) ==\r\nRRR_1_OPCODE_Y1)) ||\r\n((get_UnaryOpcodeExtension_Y1(bundle) ==\r\nFNOP_UNARY_OPCODE_Y1) &&\r\n(get_RRROpcodeExtension_Y1(bundle) ==\r\nUNARY_RRR_1_OPCODE_Y1) &&\r\n(get_Opcode_Y1(bundle) ==\r\nRRR_1_OPCODE_Y1)));\r\n}\r\nstatic bool is_y0_y1_nop(tilegx_bundle_bits bundle)\r\n{\r\nreturn is_bundle_y0_nop(bundle) && is_bundle_y1_nop(bundle);\r\n}\r\nstatic bool is_x0_x1_nop(tilegx_bundle_bits bundle)\r\n{\r\nreturn is_bundle_x0_nop(bundle) && is_bundle_x1_nop(bundle);\r\n}\r\nstatic void find_regs(tilegx_bundle_bits bundle, uint64_t *rd, uint64_t *ra,\r\nuint64_t *rb, uint64_t *clob1, uint64_t *clob2,\r\nuint64_t *clob3, bool *r_alias)\r\n{\r\nint i;\r\nuint64_t reg;\r\nuint64_t reg_map = 0, alias_reg_map = 0, map;\r\nbool alias = false;\r\nif (bundle & TILEGX_BUNDLE_MODE_MASK) {\r\nreg = get_SrcA_Y2(bundle);\r\nreg_map |= 1ULL << reg;\r\n*ra = reg;\r\nreg = get_SrcBDest_Y2(bundle);\r\nreg_map |= 1ULL << reg;\r\nif (rd) {\r\n*rd = reg;\r\nalias_reg_map = (1ULL << *rd) | (1ULL << *ra);\r\n} else {\r\n*rb = reg;\r\nalias_reg_map = (1ULL << *ra) | (1ULL << *rb);\r\n}\r\nif (!is_bundle_y1_nop(bundle)) {\r\nreg = get_SrcA_Y1(bundle);\r\nreg_map |= (1ULL << reg);\r\nmap = (1ULL << reg);\r\nreg = get_SrcB_Y1(bundle);\r\nreg_map |= (1ULL << reg);\r\nmap |= (1ULL << reg);\r\nreg = get_Dest_Y1(bundle);\r\nreg_map |= (1ULL << reg);\r\nmap |= (1ULL << reg);\r\nif (map & alias_reg_map)\r\nalias = true;\r\n}\r\nif (!is_bundle_y0_nop(bundle)) {\r\nreg = get_SrcA_Y0(bundle);\r\nreg_map |= (1ULL << reg);\r\nmap = (1ULL << reg);\r\nreg = get_SrcB_Y0(bundle);\r\nreg_map |= (1ULL << reg);\r\nmap |= (1ULL << reg);\r\nreg = get_Dest_Y0(bundle);\r\nreg_map |= (1ULL << reg);\r\nmap |= (1ULL << reg);\r\nif (map & alias_reg_map)\r\nalias = true;\r\n}\r\n} else {\r\nreg = get_SrcA_X1(bundle);\r\nreg_map |= (1ULL << reg);\r\n*ra = reg;\r\nif (rd) {\r\nreg = get_Dest_X1(bundle);\r\nreg_map |= (1ULL << reg);\r\n*rd = reg;\r\nalias_reg_map = (1ULL << *rd) | (1ULL << *ra);\r\n} else {\r\nreg = get_SrcB_X1(bundle);\r\nreg_map |= (1ULL << reg);\r\n*rb = reg;\r\nalias_reg_map = (1ULL << *ra) | (1ULL << *rb);\r\n}\r\nif (!is_bundle_x0_nop(bundle)) {\r\nreg = get_SrcA_X0(bundle);\r\nreg_map |= (1ULL << reg);\r\nmap = (1ULL << reg);\r\nreg = get_SrcB_X0(bundle);\r\nreg_map |= (1ULL << reg);\r\nmap |= (1ULL << reg);\r\nreg = get_Dest_X0(bundle);\r\nreg_map |= (1ULL << reg);\r\nmap |= (1ULL << reg);\r\nif (map & alias_reg_map)\r\nalias = true;\r\n}\r\n}\r\n*r_alias = alias;\r\nreg_map ^= -1ULL;\r\nfor (i = 0; i < TREG_SP; i++) {\r\nif (reg_map & (0x1ULL << i)) {\r\nif (*clob1 == -1) {\r\n*clob1 = i;\r\n} else if (*clob2 == -1) {\r\n*clob2 = i;\r\n} else if (*clob3 == -1) {\r\n*clob3 = i;\r\nreturn;\r\n}\r\n}\r\n}\r\n}\r\nstatic bool check_regs(uint64_t rd, uint64_t ra, uint64_t rb,\r\nuint64_t clob1, uint64_t clob2, uint64_t clob3)\r\n{\r\nbool unexpected = false;\r\nif ((ra >= 56) && (ra != TREG_ZERO))\r\nunexpected = true;\r\nif ((clob1 >= 56) || (clob2 >= 56) || (clob3 >= 56))\r\nunexpected = true;\r\nif (rd != -1) {\r\nif ((rd >= 56) && (rd != TREG_ZERO))\r\nunexpected = true;\r\n} else {\r\nif ((rb >= 56) && (rb != TREG_ZERO))\r\nunexpected = true;\r\n}\r\nreturn unexpected;\r\n}\r\nstatic tilegx_bundle_bits jit_x1_mtspr(int spr, int reg)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_mtspr;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_mtspr) & GX_INSN_X1_MASK) |\r\ncreate_MT_Imm14_X1(spr) | create_SrcA_X1(reg);\r\n}\r\nstatic tilegx_bundle_bits jit_x1_mfspr(int reg, int spr)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_mfspr;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_mfspr) & GX_INSN_X1_MASK) |\r\ncreate_MF_Imm14_X1(spr) | create_Dest_X1(reg);\r\n}\r\nstatic tilegx_bundle_bits jit_x0_addi(int rd, int ra, int imm8)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x0_addi;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x0_addi) & GX_INSN_X0_MASK) |\r\ncreate_Dest_X0(rd) | create_SrcA_X0(ra) |\r\ncreate_Imm8_X0(imm8);\r\n}\r\nstatic tilegx_bundle_bits jit_x1_ldna(int rd, int ra)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_ldna;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_ldna) & GX_INSN_X1_MASK) |\r\ncreate_Dest_X1(rd) | create_SrcA_X1(ra);\r\n}\r\nstatic tilegx_bundle_bits jit_x0_dblalign(int rd, int ra, int rb)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x0_dblalign;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x0_dblalign) & GX_INSN_X0_MASK) |\r\ncreate_Dest_X0(rd) | create_SrcA_X0(ra) |\r\ncreate_SrcB_X0(rb);\r\n}\r\nstatic tilegx_bundle_bits jit_x1_iret(void)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_iret;\r\nreturn GX_INSN_BSWAP(__unalign_jit_x1_iret) & GX_INSN_X1_MASK;\r\n}\r\nstatic tilegx_bundle_bits jit_x0_fnop(void)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x01_fnop;\r\nreturn GX_INSN_BSWAP(__unalign_jit_x01_fnop) & GX_INSN_X0_MASK;\r\n}\r\nstatic tilegx_bundle_bits jit_x1_fnop(void)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x01_fnop;\r\nreturn GX_INSN_BSWAP(__unalign_jit_x01_fnop) & GX_INSN_X1_MASK;\r\n}\r\nstatic tilegx_bundle_bits jit_y2_dummy(void)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_y2_dummy;\r\nreturn GX_INSN_BSWAP(__unalign_jit_y2_dummy) & GX_INSN_Y2_MASK;\r\n}\r\nstatic tilegx_bundle_bits jit_y1_fnop(void)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_y2_dummy;\r\nreturn GX_INSN_BSWAP(__unalign_jit_y2_dummy) & GX_INSN_Y1_MASK;\r\n}\r\nstatic tilegx_bundle_bits jit_x1_st1_add(int ra, int rb, int imm8)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_st1_add;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_st1_add) &\r\n(~create_SrcA_X1(-1)) &\r\nGX_INSN_X1_MASK) | create_SrcA_X1(ra) |\r\ncreate_SrcB_X1(rb) | create_Dest_Imm8_X1(imm8);\r\n}\r\nstatic tilegx_bundle_bits jit_x1_st(int ra, int rb)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_st;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_st) & GX_INSN_X1_MASK) |\r\ncreate_SrcA_X1(ra) | create_SrcB_X1(rb);\r\n}\r\nstatic tilegx_bundle_bits jit_x1_st_add(int ra, int rb, int imm8)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_st_add;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_st_add) &\r\n(~create_SrcA_X1(-1)) &\r\nGX_INSN_X1_MASK) | create_SrcA_X1(ra) |\r\ncreate_SrcB_X1(rb) | create_Dest_Imm8_X1(imm8);\r\n}\r\nstatic tilegx_bundle_bits jit_x1_ld(int rd, int ra)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_ld;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_ld) & GX_INSN_X1_MASK) |\r\ncreate_Dest_X1(rd) | create_SrcA_X1(ra);\r\n}\r\nstatic tilegx_bundle_bits jit_x1_ld_add(int rd, int ra, int imm8)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_ld_add;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_ld_add) &\r\n(~create_Dest_X1(-1)) &\r\nGX_INSN_X1_MASK) | create_Dest_X1(rd) |\r\ncreate_SrcA_X1(ra) | create_Imm8_X1(imm8);\r\n}\r\nstatic tilegx_bundle_bits jit_x0_bfexts(int rd, int ra, int bfs, int bfe)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x0_bfexts;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x0_bfexts) &\r\nGX_INSN_X0_MASK) |\r\ncreate_Dest_X0(rd) | create_SrcA_X0(ra) |\r\ncreate_BFStart_X0(bfs) | create_BFEnd_X0(bfe);\r\n}\r\nstatic tilegx_bundle_bits jit_x0_bfextu(int rd, int ra, int bfs, int bfe)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x0_bfextu;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x0_bfextu) &\r\nGX_INSN_X0_MASK) |\r\ncreate_Dest_X0(rd) | create_SrcA_X0(ra) |\r\ncreate_BFStart_X0(bfs) | create_BFEnd_X0(bfe);\r\n}\r\nstatic tilegx_bundle_bits jit_x1_addi(int rd, int ra, int imm8)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_addi;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_addi) & GX_INSN_X1_MASK) |\r\ncreate_Dest_X1(rd) | create_SrcA_X1(ra) |\r\ncreate_Imm8_X1(imm8);\r\n}\r\nstatic tilegx_bundle_bits jit_x0_shrui(int rd, int ra, int imm6)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x0_shrui;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x0_shrui) &\r\nGX_INSN_X0_MASK) |\r\ncreate_Dest_X0(rd) | create_SrcA_X0(ra) |\r\ncreate_ShAmt_X0(imm6);\r\n}\r\nstatic tilegx_bundle_bits jit_x0_rotli(int rd, int ra, int imm6)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x0_rotli;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x0_rotli) &\r\nGX_INSN_X0_MASK) |\r\ncreate_Dest_X0(rd) | create_SrcA_X0(ra) |\r\ncreate_ShAmt_X0(imm6);\r\n}\r\nstatic tilegx_bundle_bits jit_x1_bnezt(int ra, int broff)\r\n{\r\nextern tilegx_bundle_bits __unalign_jit_x1_bnezt;\r\nreturn (GX_INSN_BSWAP(__unalign_jit_x1_bnezt) &\r\nGX_INSN_X1_MASK) |\r\ncreate_SrcA_X1(ra) | create_BrOff_X1(broff);\r\n}\r\nstatic\r\nvoid jit_bundle_gen(struct pt_regs *regs, tilegx_bundle_bits bundle,\r\nint align_ctl)\r\n{\r\nstruct thread_info *info = current_thread_info();\r\nstruct unaligned_jit_fragment frag;\r\nstruct unaligned_jit_fragment *jit_code_area;\r\ntilegx_bundle_bits bundle_2 = 0;\r\nbool bundle_2_enable = true;\r\nuint64_t ra = -1, rb = -1, rd = -1, clob1 = -1, clob2 = -1, clob3 = -1;\r\nbool alias = false;\r\nbool load_n_store = true;\r\nbool load_store_signed = false;\r\nunsigned int load_store_size = 8;\r\nbool y1_br = false;\r\nint y1_br_reg = 0;\r\nbool y1_lr = false;\r\nint y1_lr_reg = 0;\r\nbool x1_add = false;\r\nint x1_add_imm8 = 0;\r\nbool unexpected = false;\r\nint n = 0, k;\r\njit_code_area =\r\n(struct unaligned_jit_fragment *)(info->unalign_jit_base);\r\nmemset((void *)&frag, 0, sizeof(frag));\r\nif (bundle & TILEGX_BUNDLE_MODE_MASK) {\r\nunsigned int mod, opcode;\r\nif (get_Opcode_Y1(bundle) == RRR_1_OPCODE_Y1 &&\r\nget_RRROpcodeExtension_Y1(bundle) ==\r\nUNARY_RRR_1_OPCODE_Y1) {\r\nopcode = get_UnaryOpcodeExtension_Y1(bundle);\r\nswitch (opcode) {\r\ncase JALR_UNARY_OPCODE_Y1:\r\ncase JALRP_UNARY_OPCODE_Y1:\r\ny1_lr = true;\r\ny1_lr_reg = 55;\r\ncase JR_UNARY_OPCODE_Y1:\r\ncase JRP_UNARY_OPCODE_Y1:\r\ny1_br = true;\r\ny1_br_reg = get_SrcA_Y1(bundle);\r\nbreak;\r\ncase LNK_UNARY_OPCODE_Y1:\r\ny1_lr = true;\r\ny1_lr_reg = get_Dest_Y1(bundle);\r\nbreak;\r\n}\r\n}\r\nopcode = get_Opcode_Y2(bundle);\r\nmod = get_Mode(bundle);\r\nbundle_2 = (bundle & (~GX_INSN_Y2_MASK)) | jit_y2_dummy();\r\nif (y1_br || y1_lr) {\r\nbundle_2 &= ~(GX_INSN_Y1_MASK);\r\nbundle_2 |= jit_y1_fnop();\r\n}\r\nif (is_y0_y1_nop(bundle_2))\r\nbundle_2_enable = false;\r\nif (mod == MODE_OPCODE_YC2) {\r\nload_n_store = false;\r\nload_store_size = 1 << opcode;\r\nload_store_signed = false;\r\nfind_regs(bundle, 0, &ra, &rb, &clob1, &clob2,\r\n&clob3, &alias);\r\nif (load_store_size > 8)\r\nunexpected = true;\r\n} else {\r\nload_n_store = true;\r\nif (mod == MODE_OPCODE_YB2) {\r\nswitch (opcode) {\r\ncase LD_OPCODE_Y2:\r\nload_store_signed = false;\r\nload_store_size = 8;\r\nbreak;\r\ncase LD4S_OPCODE_Y2:\r\nload_store_signed = true;\r\nload_store_size = 4;\r\nbreak;\r\ncase LD4U_OPCODE_Y2:\r\nload_store_signed = false;\r\nload_store_size = 4;\r\nbreak;\r\ndefault:\r\nunexpected = true;\r\n}\r\n} else if (mod == MODE_OPCODE_YA2) {\r\nif (opcode == LD2S_OPCODE_Y2) {\r\nload_store_signed = true;\r\nload_store_size = 2;\r\n} else if (opcode == LD2U_OPCODE_Y2) {\r\nload_store_signed = false;\r\nload_store_size = 2;\r\n} else\r\nunexpected = true;\r\n} else\r\nunexpected = true;\r\nfind_regs(bundle, &rd, &ra, &rb, &clob1, &clob2,\r\n&clob3, &alias);\r\n}\r\n} else {\r\nunsigned int opcode;\r\nbundle_2 = (bundle & (~GX_INSN_X1_MASK)) | jit_x1_fnop();\r\nif (is_x0_x1_nop(bundle_2))\r\nbundle_2_enable = false;\r\nif (get_Opcode_X1(bundle) == RRR_0_OPCODE_X1) {\r\nopcode = get_UnaryOpcodeExtension_X1(bundle);\r\nif (get_RRROpcodeExtension_X1(bundle) ==\r\nUNARY_RRR_0_OPCODE_X1) {\r\nload_n_store = true;\r\nfind_regs(bundle, &rd, &ra, &rb, &clob1,\r\n&clob2, &clob3, &alias);\r\nswitch (opcode) {\r\ncase LD_UNARY_OPCODE_X1:\r\nload_store_signed = false;\r\nload_store_size = 8;\r\nbreak;\r\ncase LD4S_UNARY_OPCODE_X1:\r\nload_store_signed = true;\r\ncase LD4U_UNARY_OPCODE_X1:\r\nload_store_size = 4;\r\nbreak;\r\ncase LD2S_UNARY_OPCODE_X1:\r\nload_store_signed = true;\r\ncase LD2U_UNARY_OPCODE_X1:\r\nload_store_size = 2;\r\nbreak;\r\ndefault:\r\nunexpected = true;\r\n}\r\n} else {\r\nload_n_store = false;\r\nload_store_signed = false;\r\nfind_regs(bundle, 0, &ra, &rb,\r\n&clob1, &clob2, &clob3,\r\n&alias);\r\nopcode = get_RRROpcodeExtension_X1(bundle);\r\nswitch (opcode) {\r\ncase ST_RRR_0_OPCODE_X1:\r\nload_store_size = 8;\r\nbreak;\r\ncase ST4_RRR_0_OPCODE_X1:\r\nload_store_size = 4;\r\nbreak;\r\ncase ST2_RRR_0_OPCODE_X1:\r\nload_store_size = 2;\r\nbreak;\r\ndefault:\r\nunexpected = true;\r\n}\r\n}\r\n} else if (get_Opcode_X1(bundle) == IMM8_OPCODE_X1) {\r\nload_n_store = true;\r\nopcode = get_Imm8OpcodeExtension_X1(bundle);\r\nswitch (opcode) {\r\ncase LD_ADD_IMM8_OPCODE_X1:\r\nload_store_size = 8;\r\nbreak;\r\ncase LD4S_ADD_IMM8_OPCODE_X1:\r\nload_store_signed = true;\r\ncase LD4U_ADD_IMM8_OPCODE_X1:\r\nload_store_size = 4;\r\nbreak;\r\ncase LD2S_ADD_IMM8_OPCODE_X1:\r\nload_store_signed = true;\r\ncase LD2U_ADD_IMM8_OPCODE_X1:\r\nload_store_size = 2;\r\nbreak;\r\ncase ST_ADD_IMM8_OPCODE_X1:\r\nload_n_store = false;\r\nload_store_size = 8;\r\nbreak;\r\ncase ST4_ADD_IMM8_OPCODE_X1:\r\nload_n_store = false;\r\nload_store_size = 4;\r\nbreak;\r\ncase ST2_ADD_IMM8_OPCODE_X1:\r\nload_n_store = false;\r\nload_store_size = 2;\r\nbreak;\r\ndefault:\r\nunexpected = true;\r\n}\r\nif (!unexpected) {\r\nx1_add = true;\r\nif (load_n_store)\r\nx1_add_imm8 = get_Imm8_X1(bundle);\r\nelse\r\nx1_add_imm8 = get_Dest_Imm8_X1(bundle);\r\n}\r\nfind_regs(bundle, load_n_store ? (&rd) : NULL,\r\n&ra, &rb, &clob1, &clob2, &clob3, &alias);\r\n} else\r\nunexpected = true;\r\n}\r\nif (check_regs(rd, ra, rb, clob1, clob2, clob3) == true)\r\nunexpected = true;\r\nif (!unexpected)\r\nWARN_ON(!((load_store_size - 1) & (regs->regs[ra])));\r\nif (EX1_PL(regs->ex1) != USER_PL) {\r\nunsigned long rx = 0;\r\nunsigned long x = 0, ret = 0;\r\nif (y1_br || y1_lr || x1_add ||\r\n(load_store_signed !=\r\n(load_n_store && load_store_size == 4))) {\r\nunexpected = true;\r\n} else if (!unexpected) {\r\nif (bundle & TILEGX_BUNDLE_MODE_MASK) {\r\nif ((get_Opcode_Y1(bundle) == ADDI_OPCODE_Y1)\r\n&& (get_SrcA_Y1(bundle) == TREG_ZERO) &&\r\n(get_Imm8_Y1(bundle) == 0) &&\r\nis_bundle_y0_nop(bundle)) {\r\nrx = get_Dest_Y1(bundle);\r\n} else if ((get_Opcode_Y0(bundle) ==\r\nADDI_OPCODE_Y0) &&\r\n(get_SrcA_Y0(bundle) == TREG_ZERO) &&\r\n(get_Imm8_Y0(bundle) == 0) &&\r\nis_bundle_y1_nop(bundle)) {\r\nrx = get_Dest_Y0(bundle);\r\n} else {\r\nunexpected = true;\r\n}\r\n} else {\r\nif ((get_Opcode_X0(bundle) == IMM8_OPCODE_X0)\r\n&& (get_Imm8OpcodeExtension_X0(bundle) ==\r\nADDI_IMM8_OPCODE_X0) &&\r\n(get_SrcA_X0(bundle) == TREG_ZERO) &&\r\n(get_Imm8_X0(bundle) == 0)) {\r\nrx = get_Dest_X0(bundle);\r\n} else {\r\nunexpected = true;\r\n}\r\n}\r\nif (!unexpected && (rx >= 56))\r\nunexpected = true;\r\n}\r\nif (!search_exception_tables(regs->pc)) {\r\nunexpected = true;\r\n}\r\nif (unexpected) {\r\nstruct task_struct *tsk = validate_current();\r\nbust_spinlocks(1);\r\nshow_regs(regs);\r\nif (unlikely(tsk->pid < 2)) {\r\npanic("Kernel unalign fault running %s!",\r\ntsk->pid ? "init" : "the idle task");\r\n}\r\n#ifdef SUPPORT_DIE\r\ndie("Oops", regs);\r\n#endif\r\nbust_spinlocks(1);\r\ndo_group_exit(SIGKILL);\r\n} else {\r\nunsigned long i, b = 0;\r\nunsigned char *ptr =\r\n(unsigned char *)regs->regs[ra];\r\nif (load_n_store) {\r\nfor (i = 0; i < load_store_size; i++) {\r\nret = get_user(b, ptr++);\r\nif (!ret) {\r\n#ifdef __LITTLE_ENDIAN\r\nx |= (b << (8 * i));\r\n#else\r\nx <<= 8;\r\nx |= b;\r\n#endif\r\n} else {\r\nx = 0;\r\nbreak;\r\n}\r\n}\r\nif (load_store_size == 4)\r\nx = (long)(int)x;\r\nregs->regs[rd] = x;\r\nregs->regs[rx] = ret;\r\nregs->pc += 8;\r\n} else {\r\nx = regs->regs[rb];\r\n#ifdef __LITTLE_ENDIAN\r\nb = x;\r\n#else\r\nswitch (load_store_size) {\r\ncase 8:\r\nb = swab64(x);\r\nbreak;\r\ncase 4:\r\nb = swab32(x);\r\nbreak;\r\ncase 2:\r\nb = swab16(x);\r\nbreak;\r\n}\r\n#endif\r\nfor (i = 0; i < load_store_size; i++) {\r\nret = put_user(b, ptr++);\r\nif (ret)\r\nbreak;\r\nb >>= 8;\r\n}\r\nregs->regs[rx] = ret;\r\nregs->pc += 8;\r\n}\r\n}\r\nunaligned_fixup_count++;\r\nif (unaligned_printk) {\r\npr_info("%s/%d - Unalign fixup for kernel access to userspace %lx\n",\r\ncurrent->comm, current->pid, regs->regs[ra]);\r\n}\r\nreturn;\r\n}\r\nif ((align_ctl == 0) || unexpected) {\r\nsiginfo_t info = {\r\n.si_signo = SIGBUS,\r\n.si_code = BUS_ADRALN,\r\n.si_addr = (unsigned char __user *)0\r\n};\r\nif (unaligned_printk)\r\npr_info("Unalign bundle: unexp @%llx, %llx\n",\r\n(unsigned long long)regs->pc,\r\n(unsigned long long)bundle);\r\nif (ra < 56) {\r\nunsigned long uaa = (unsigned long)regs->regs[ra];\r\ninfo.si_addr = (unsigned char __user *)uaa;\r\n}\r\nunaligned_fixup_count++;\r\ntrace_unhandled_signal("unaligned fixup trap", regs,\r\n(unsigned long)info.si_addr, SIGBUS);\r\nforce_sig_info(info.si_signo, &info, current);\r\nreturn;\r\n}\r\n#ifdef __LITTLE_ENDIAN\r\n#define UA_FIXUP_ADDR_DELTA 1\r\n#define UA_FIXUP_BFEXT_START(_B_) 0\r\n#define UA_FIXUP_BFEXT_END(_B_) (8 * (_B_) - 1)\r\n#else\r\n#define UA_FIXUP_ADDR_DELTA -1\r\n#define UA_FIXUP_BFEXT_START(_B_) (64 - 8 * (_B_))\r\n#define UA_FIXUP_BFEXT_END(_B_) 63\r\n#endif\r\nif ((ra != rb) && (rd != TREG_SP) && !alias &&\r\n!y1_br && !y1_lr && !x1_add) {\r\nif (!load_n_store) {\r\n#ifdef __BIG_ENDIAN\r\nfrag.insn[n++] =\r\njit_x0_addi(ra, ra, load_store_size - 1) |\r\njit_x1_fnop();\r\n#endif\r\nfor (k = 0; k < load_store_size; k++) {\r\nfrag.insn[n++] =\r\njit_x0_rotli(rb, rb, 56) |\r\njit_x1_st1_add(ra, rb,\r\nUA_FIXUP_ADDR_DELTA);\r\n}\r\n#ifdef __BIG_ENDIAN\r\nfrag.insn[n] = jit_x1_addi(ra, ra, 1);\r\n#else\r\nfrag.insn[n] = jit_x1_addi(ra, ra,\r\n-1 * load_store_size);\r\n#endif\r\nif (load_store_size == 8) {\r\nfrag.insn[n] |= jit_x0_fnop();\r\n} else if (load_store_size == 4) {\r\nfrag.insn[n] |= jit_x0_rotli(rb, rb, 32);\r\n} else {\r\nfrag.insn[n] |= jit_x0_rotli(rb, rb, 16);\r\n}\r\nn++;\r\nif (bundle_2_enable)\r\nfrag.insn[n++] = bundle_2;\r\nfrag.insn[n++] = jit_x0_fnop() | jit_x1_iret();\r\n} else {\r\nif (rd == ra) {\r\nfrag.insn[n++] =\r\njit_x0_addi(TREG_SP, TREG_SP, -16) |\r\njit_x1_fnop();\r\nfrag.insn[n++] =\r\njit_x0_addi(clob1, ra, 7) |\r\njit_x1_st_add(TREG_SP, clob1, -8);\r\nfrag.insn[n++] =\r\njit_x0_addi(clob2, ra, 0) |\r\njit_x1_st(TREG_SP, clob2);\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ldna(rd, ra);\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ldna(clob1, clob1);\r\nfrag.insn[n++] =\r\njit_x0_dblalign(rd, clob1, clob2) |\r\njit_x1_ld_add(clob2, TREG_SP, 8);\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ld_add(clob1, TREG_SP, 16);\r\n} else {\r\nfrag.insn[n++] =\r\njit_x0_addi(TREG_SP, TREG_SP, -16) |\r\njit_x1_fnop();\r\nfrag.insn[n++] =\r\njit_x0_addi(clob1, ra, 7) |\r\njit_x1_st(TREG_SP, clob1);\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ldna(rd, ra);\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ldna(clob1, clob1);\r\nfrag.insn[n++] =\r\njit_x0_dblalign(rd, clob1, ra) |\r\njit_x1_ld_add(clob1, TREG_SP, 16);\r\n}\r\nif (bundle_2_enable)\r\nfrag.insn[n++] = bundle_2;\r\nif (load_store_size == 4) {\r\nif (load_store_signed)\r\nfrag.insn[n++] =\r\njit_x0_bfexts(\r\nrd, rd,\r\nUA_FIXUP_BFEXT_START(4),\r\nUA_FIXUP_BFEXT_END(4)) |\r\njit_x1_fnop();\r\nelse\r\nfrag.insn[n++] =\r\njit_x0_bfextu(\r\nrd, rd,\r\nUA_FIXUP_BFEXT_START(4),\r\nUA_FIXUP_BFEXT_END(4)) |\r\njit_x1_fnop();\r\n} else if (load_store_size == 2) {\r\nif (load_store_signed)\r\nfrag.insn[n++] =\r\njit_x0_bfexts(\r\nrd, rd,\r\nUA_FIXUP_BFEXT_START(2),\r\nUA_FIXUP_BFEXT_END(2)) |\r\njit_x1_fnop();\r\nelse\r\nfrag.insn[n++] =\r\njit_x0_bfextu(\r\nrd, rd,\r\nUA_FIXUP_BFEXT_START(2),\r\nUA_FIXUP_BFEXT_END(2)) |\r\njit_x1_fnop();\r\n}\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_iret();\r\n}\r\n} else if (!load_n_store) {\r\nfrag.insn[n++] =\r\njit_x0_addi(TREG_SP, TREG_SP, -32) |\r\njit_x1_fnop();\r\nfrag.insn[n++] =\r\njit_x0_addi(clob3, TREG_SP, 16) |\r\njit_x1_st_add(TREG_SP, clob3, 8);\r\n#ifdef __LITTLE_ENDIAN\r\nfrag.insn[n++] =\r\njit_x0_addi(clob1, ra, 0) |\r\njit_x1_st_add(TREG_SP, clob1, 8);\r\n#else\r\nfrag.insn[n++] =\r\njit_x0_addi(clob1, ra, load_store_size - 1) |\r\njit_x1_st_add(TREG_SP, clob1, 8);\r\n#endif\r\nif (load_store_size == 8) {\r\nfrag.insn[n++] =\r\njit_x0_addi(clob2, TREG_ZERO, 7) |\r\njit_x1_st_add(TREG_SP, clob2, 16);\r\nfrag.insn[n++] =\r\njit_x0_rotli(rb, rb, 56) |\r\njit_x1_st1_add(clob1, rb, UA_FIXUP_ADDR_DELTA);\r\nfrag.insn[n++] =\r\njit_x0_addi(clob2, clob2, -1) |\r\njit_x1_bnezt(clob2, -1);\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_addi(clob2, y1_br_reg, 0);\r\n} else if (load_store_size == 4) {\r\nfrag.insn[n++] =\r\njit_x0_addi(clob2, TREG_ZERO, 3) |\r\njit_x1_st_add(TREG_SP, clob2, 16);\r\nfrag.insn[n++] =\r\njit_x0_rotli(rb, rb, 56) |\r\njit_x1_st1_add(clob1, rb, UA_FIXUP_ADDR_DELTA);\r\nfrag.insn[n++] =\r\njit_x0_addi(clob2, clob2, -1) |\r\njit_x1_bnezt(clob2, -1);\r\nfrag.insn[n++] = jit_x0_rotli(rb, rb, 32) |\r\njit_x1_addi(clob2, y1_br_reg, 0);\r\n} else {\r\nfrag.insn[n++] =\r\njit_x0_addi(clob2, rb, 0) |\r\njit_x1_st_add(TREG_SP, clob2, 16);\r\nfor (k = 0; k < 2; k++) {\r\nfrag.insn[n++] =\r\njit_x0_shrui(rb, rb, 8) |\r\njit_x1_st1_add(clob1, rb,\r\nUA_FIXUP_ADDR_DELTA);\r\n}\r\nfrag.insn[n++] =\r\njit_x0_addi(rb, clob2, 0) |\r\njit_x1_addi(clob2, y1_br_reg, 0);\r\n}\r\nif (bundle_2_enable)\r\nfrag.insn[n++] = bundle_2;\r\nif (y1_lr) {\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_mfspr(y1_lr_reg,\r\nSPR_EX_CONTEXT_0_0);\r\n}\r\nif (y1_br) {\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_mtspr(SPR_EX_CONTEXT_0_0,\r\nclob2);\r\n}\r\nif (x1_add) {\r\nfrag.insn[n++] =\r\njit_x0_addi(ra, ra, x1_add_imm8) |\r\njit_x1_ld_add(clob2, clob3, -8);\r\n} else {\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ld_add(clob2, clob3, -8);\r\n}\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ld_add(clob1, clob3, -8);\r\nfrag.insn[n++] = jit_x0_fnop() | jit_x1_ld(clob3, clob3);\r\nfrag.insn[n++] = jit_x0_fnop() | jit_x1_iret();\r\n} else {\r\nfrag.insn[n++] =\r\njit_x0_addi(TREG_SP, TREG_SP, -32) |\r\njit_x1_fnop();\r\nfrag.insn[n++] =\r\njit_x0_addi(clob3, TREG_SP, 16) |\r\njit_x1_st_add(TREG_SP, clob3, 8);\r\nfrag.insn[n++] =\r\njit_x0_addi(clob2, ra, 0) |\r\njit_x1_st_add(TREG_SP, clob2, 8);\r\nif (y1_br) {\r\nfrag.insn[n++] =\r\njit_x0_addi(clob1, y1_br_reg, 0) |\r\njit_x1_st_add(TREG_SP, clob1, 16);\r\n} else {\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_st_add(TREG_SP, clob1, 16);\r\n}\r\nif (bundle_2_enable)\r\nfrag.insn[n++] = bundle_2;\r\nif (y1_lr) {\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_mfspr(y1_lr_reg,\r\nSPR_EX_CONTEXT_0_0);\r\n}\r\nif (y1_br) {\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_mtspr(SPR_EX_CONTEXT_0_0,\r\nclob1);\r\n}\r\nfrag.insn[n++] =\r\njit_x0_addi(clob1, clob2, 7) |\r\njit_x1_ldna(rd, clob2);\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ldna(clob1, clob1);\r\nfrag.insn[n++] =\r\njit_x0_dblalign(rd, clob1, clob2) |\r\njit_x1_ld_add(clob1, clob3, -8);\r\nif (x1_add) {\r\nfrag.insn[n++] =\r\njit_x0_addi(ra, ra, x1_add_imm8) |\r\njit_x1_ld_add(clob2, clob3, -8);\r\n} else {\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ld_add(clob2, clob3, -8);\r\n}\r\nfrag.insn[n++] =\r\njit_x0_fnop() |\r\njit_x1_ld(clob3, clob3);\r\nif (load_store_size == 4) {\r\nif (load_store_signed)\r\nfrag.insn[n++] =\r\njit_x0_bfexts(\r\nrd, rd,\r\nUA_FIXUP_BFEXT_START(4),\r\nUA_FIXUP_BFEXT_END(4)) |\r\njit_x1_fnop();\r\nelse\r\nfrag.insn[n++] =\r\njit_x0_bfextu(\r\nrd, rd,\r\nUA_FIXUP_BFEXT_START(4),\r\nUA_FIXUP_BFEXT_END(4)) |\r\njit_x1_fnop();\r\n} else if (load_store_size == 2) {\r\nif (load_store_signed)\r\nfrag.insn[n++] =\r\njit_x0_bfexts(\r\nrd, rd,\r\nUA_FIXUP_BFEXT_START(2),\r\nUA_FIXUP_BFEXT_END(2)) |\r\njit_x1_fnop();\r\nelse\r\nfrag.insn[n++] =\r\njit_x0_bfextu(\r\nrd, rd,\r\nUA_FIXUP_BFEXT_START(2),\r\nUA_FIXUP_BFEXT_END(2)) |\r\njit_x1_fnop();\r\n}\r\nfrag.insn[n++] = jit_x0_fnop() | jit_x1_iret();\r\n}\r\nWARN_ON(n > 14);\r\nif (!unexpected) {\r\nint status = 0;\r\nint idx = (regs->pc >> 3) &\r\n((1ULL << (PAGE_SHIFT - UNALIGN_JIT_SHIFT)) - 1);\r\nfrag.pc = regs->pc;\r\nfrag.bundle = bundle;\r\nif (unaligned_printk) {\r\npr_info("%s/%d, Unalign fixup: pc=%lx bundle=%lx %d %d %d %d %d %d %d %d\n",\r\ncurrent->comm, current->pid,\r\n(unsigned long)frag.pc,\r\n(unsigned long)frag.bundle,\r\n(int)alias, (int)rd, (int)ra,\r\n(int)rb, (int)bundle_2_enable,\r\n(int)y1_lr, (int)y1_br, (int)x1_add);\r\nfor (k = 0; k < n; k += 2)\r\npr_info("[%d] %016llx %016llx\n",\r\nk, (unsigned long long)frag.insn[k],\r\n(unsigned long long)frag.insn[k+1]);\r\n}\r\n#ifdef __BIG_ENDIAN\r\nfrag.bundle = GX_INSN_BSWAP(frag.bundle);\r\nfor (k = 0; k < n; k++)\r\nfrag.insn[k] = GX_INSN_BSWAP(frag.insn[k]);\r\n#endif\r\nstatus = copy_to_user((void __user *)&jit_code_area[idx],\r\n&frag, sizeof(frag));\r\nif (status) {\r\nsiginfo_t info = {\r\n.si_signo = SIGSEGV,\r\n.si_code = SEGV_MAPERR,\r\n.si_addr = (void __user *)&jit_code_area[idx]\r\n};\r\npr_warn("Unalign fixup: pid=%d %s jit_code_area=%llx\n",\r\ncurrent->pid, current->comm,\r\n(unsigned long long)&jit_code_area[idx]);\r\ntrace_unhandled_signal("segfault in unalign fixup",\r\nregs,\r\n(unsigned long)info.si_addr,\r\nSIGSEGV);\r\nforce_sig_info(info.si_signo, &info, current);\r\nreturn;\r\n}\r\nunaligned_fixup_count++;\r\n__flush_icache_range((unsigned long)&jit_code_area[idx],\r\n(unsigned long)&jit_code_area[idx] +\r\nsizeof(frag));\r\n__insn_mtspr(SPR_EX_CONTEXT_0_0, regs->pc + 8);\r\n__insn_mtspr(SPR_EX_CONTEXT_0_1, PL_ICS_EX1(USER_PL, 0));\r\nregs->pc = (unsigned long)&jit_code_area[idx].insn[0];\r\nregs->ex1 = PL_ICS_EX1(USER_PL, 1);\r\n}\r\n}\r\nvoid do_unaligned(struct pt_regs *regs, int vecnum)\r\n{\r\ntilegx_bundle_bits __user *pc;\r\ntilegx_bundle_bits bundle;\r\nstruct thread_info *info = current_thread_info();\r\nint align_ctl;\r\nalign_ctl = unaligned_fixup;\r\nswitch (task_thread_info(current)->align_ctl) {\r\ncase PR_UNALIGN_NOPRINT:\r\nalign_ctl = 1;\r\nbreak;\r\ncase PR_UNALIGN_SIGBUS:\r\nalign_ctl = 0;\r\nbreak;\r\n}\r\nlocal_irq_enable();\r\nif (EX1_PL(regs->ex1) != USER_PL) {\r\nif (align_ctl < 1) {\r\nunaligned_fixup_count++;\r\nif (fixup_exception(regs)) {\r\nif (unaligned_printk)\r\npr_info("Unalign fixup: %d %llx @%llx\n",\r\n(int)unaligned_fixup,\r\n(unsigned long long)regs->ex1,\r\n(unsigned long long)regs->pc);\r\n} else {\r\npanic("Unalign exception in Kernel. pc=%lx",\r\nregs->pc);\r\n}\r\n} else {\r\nbundle = GX_INSN_BSWAP(\r\n*((tilegx_bundle_bits *)(regs->pc)));\r\njit_bundle_gen(regs, bundle, align_ctl);\r\n}\r\nreturn;\r\n}\r\nif ((regs->sp & 0x7) || (regs->ex1) || (align_ctl < 0)) {\r\nsiginfo_t info = {\r\n.si_signo = SIGBUS,\r\n.si_code = BUS_ADRALN,\r\n.si_addr = (unsigned char __user *)0\r\n};\r\nif (unaligned_printk)\r\npr_info("Unalign fixup: %d %llx @%llx\n",\r\n(int)unaligned_fixup,\r\n(unsigned long long)regs->ex1,\r\n(unsigned long long)regs->pc);\r\nunaligned_fixup_count++;\r\ntrace_unhandled_signal("unaligned fixup trap", regs, 0, SIGBUS);\r\nforce_sig_info(info.si_signo, &info, current);\r\nreturn;\r\n}\r\npc = (tilegx_bundle_bits __user *)(regs->pc);\r\nif (get_user(bundle, pc) != 0) {\r\nsiginfo_t info = {\r\n.si_signo = SIGSEGV,\r\n.si_code = SEGV_MAPERR,\r\n.si_addr = (void __user *)pc\r\n};\r\npr_err("Couldn't read instruction at %p trying to step\n", pc);\r\ntrace_unhandled_signal("segfault in unalign fixup", regs,\r\n(unsigned long)info.si_addr, SIGSEGV);\r\nforce_sig_info(info.si_signo, &info, current);\r\nreturn;\r\n}\r\nif (!info->unalign_jit_base) {\r\nvoid __user *user_page;\r\nif (is_compat_task())\r\nuser_page = NULL;\r\nelse\r\nuser_page = (void __user *)(TASK_SIZE - (1UL << 36)) +\r\n(current->pid << PAGE_SHIFT);\r\nuser_page = (void __user *) vm_mmap(NULL,\r\n(unsigned long)user_page,\r\nPAGE_SIZE,\r\nPROT_EXEC | PROT_READ |\r\nPROT_WRITE,\r\n#ifdef CONFIG_HOMECACHE\r\nMAP_CACHE_HOME_TASK |\r\n#endif\r\nMAP_PRIVATE |\r\nMAP_ANONYMOUS,\r\n0);\r\nif (IS_ERR((void __force *)user_page)) {\r\npr_err("Out of kernel pages trying do_mmap\n");\r\nreturn;\r\n}\r\ninfo->unalign_jit_base = user_page;\r\nif (unaligned_printk)\r\npr_info("Unalign bundle: %d:%d, allocate page @%llx\n",\r\nraw_smp_processor_id(), current->pid,\r\n(unsigned long long)user_page);\r\n}\r\njit_bundle_gen(regs, GX_INSN_BSWAP(bundle), align_ctl);\r\n}
