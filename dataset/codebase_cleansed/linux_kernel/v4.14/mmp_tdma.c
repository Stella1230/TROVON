static void mmp_tdma_chan_set_desc(struct mmp_tdma_chan *tdmac, dma_addr_t phys)\r\n{\r\nwritel(phys, tdmac->reg_base + TDNDPR);\r\nwritel(readl(tdmac->reg_base + TDCR) | TDCR_FETCHND,\r\ntdmac->reg_base + TDCR);\r\n}\r\nstatic void mmp_tdma_enable_irq(struct mmp_tdma_chan *tdmac, bool enable)\r\n{\r\nif (enable)\r\nwritel(TDIMR_COMP, tdmac->reg_base + TDIMR);\r\nelse\r\nwritel(0, tdmac->reg_base + TDIMR);\r\n}\r\nstatic void mmp_tdma_enable_chan(struct mmp_tdma_chan *tdmac)\r\n{\r\nwritel(readl(tdmac->reg_base + TDCR) | TDCR_CHANEN,\r\ntdmac->reg_base + TDCR);\r\ntdmac->status = DMA_IN_PROGRESS;\r\n}\r\nstatic int mmp_tdma_disable_chan(struct dma_chan *chan)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nu32 tdcr;\r\ntdcr = readl(tdmac->reg_base + TDCR);\r\ntdcr |= TDCR_ABR;\r\ntdcr &= ~TDCR_CHANEN;\r\nwritel(tdcr, tdmac->reg_base + TDCR);\r\ntdmac->status = DMA_COMPLETE;\r\nreturn 0;\r\n}\r\nstatic int mmp_tdma_resume_chan(struct dma_chan *chan)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nwritel(readl(tdmac->reg_base + TDCR) | TDCR_CHANEN,\r\ntdmac->reg_base + TDCR);\r\ntdmac->status = DMA_IN_PROGRESS;\r\nreturn 0;\r\n}\r\nstatic int mmp_tdma_pause_chan(struct dma_chan *chan)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nwritel(readl(tdmac->reg_base + TDCR) & ~TDCR_CHANEN,\r\ntdmac->reg_base + TDCR);\r\ntdmac->status = DMA_PAUSED;\r\nreturn 0;\r\n}\r\nstatic int mmp_tdma_config_chan(struct dma_chan *chan)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nunsigned int tdcr = 0;\r\nmmp_tdma_disable_chan(chan);\r\nif (tdmac->dir == DMA_MEM_TO_DEV)\r\ntdcr = TDCR_DSTDIR_ADDR_HOLD | TDCR_SRCDIR_ADDR_INC;\r\nelse if (tdmac->dir == DMA_DEV_TO_MEM)\r\ntdcr = TDCR_SRCDIR_ADDR_HOLD | TDCR_DSTDIR_ADDR_INC;\r\nif (tdmac->type == MMP_AUD_TDMA) {\r\ntdcr |= TDCR_PACKMOD;\r\nswitch (tdmac->burst_sz) {\r\ncase 4:\r\ntdcr |= TDCR_BURSTSZ_4B;\r\nbreak;\r\ncase 8:\r\ntdcr |= TDCR_BURSTSZ_8B;\r\nbreak;\r\ncase 16:\r\ntdcr |= TDCR_BURSTSZ_16B;\r\nbreak;\r\ncase 32:\r\ntdcr |= TDCR_BURSTSZ_32B;\r\nbreak;\r\ncase 64:\r\ntdcr |= TDCR_BURSTSZ_64B;\r\nbreak;\r\ncase 128:\r\ntdcr |= TDCR_BURSTSZ_128B;\r\nbreak;\r\ndefault:\r\ndev_err(tdmac->dev, "mmp_tdma: unknown burst size.\n");\r\nreturn -EINVAL;\r\n}\r\nswitch (tdmac->buswidth) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\ntdcr |= TDCR_SSZ_8_BITS;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\ntdcr |= TDCR_SSZ_16_BITS;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\ntdcr |= TDCR_SSZ_32_BITS;\r\nbreak;\r\ndefault:\r\ndev_err(tdmac->dev, "mmp_tdma: unknown bus size.\n");\r\nreturn -EINVAL;\r\n}\r\n} else if (tdmac->type == PXA910_SQU) {\r\ntdcr |= TDCR_SSPMOD;\r\nswitch (tdmac->burst_sz) {\r\ncase 1:\r\ntdcr |= TDCR_BURSTSZ_SQU_1B;\r\nbreak;\r\ncase 2:\r\ntdcr |= TDCR_BURSTSZ_SQU_2B;\r\nbreak;\r\ncase 4:\r\ntdcr |= TDCR_BURSTSZ_SQU_4B;\r\nbreak;\r\ncase 8:\r\ntdcr |= TDCR_BURSTSZ_SQU_8B;\r\nbreak;\r\ncase 16:\r\ntdcr |= TDCR_BURSTSZ_SQU_16B;\r\nbreak;\r\ncase 32:\r\ntdcr |= TDCR_BURSTSZ_SQU_32B;\r\nbreak;\r\ndefault:\r\ndev_err(tdmac->dev, "mmp_tdma: unknown burst size.\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nwritel(tdcr, tdmac->reg_base + TDCR);\r\nreturn 0;\r\n}\r\nstatic int mmp_tdma_clear_chan_irq(struct mmp_tdma_chan *tdmac)\r\n{\r\nu32 reg = readl(tdmac->reg_base + TDISR);\r\nif (reg & TDISR_COMP) {\r\nreg &= ~TDISR_COMP;\r\nwritel(reg, tdmac->reg_base + TDISR);\r\nreturn 0;\r\n}\r\nreturn -EAGAIN;\r\n}\r\nstatic size_t mmp_tdma_get_pos(struct mmp_tdma_chan *tdmac)\r\n{\r\nsize_t reg;\r\nif (tdmac->idx == 0) {\r\nreg = __raw_readl(tdmac->reg_base + TDSAR);\r\nreg -= tdmac->desc_arr[0].src_addr;\r\n} else if (tdmac->idx == 1) {\r\nreg = __raw_readl(tdmac->reg_base + TDDAR);\r\nreg -= tdmac->desc_arr[0].dst_addr;\r\n} else\r\nreturn -EINVAL;\r\nreturn reg;\r\n}\r\nstatic irqreturn_t mmp_tdma_chan_handler(int irq, void *dev_id)\r\n{\r\nstruct mmp_tdma_chan *tdmac = dev_id;\r\nif (mmp_tdma_clear_chan_irq(tdmac) == 0) {\r\ntasklet_schedule(&tdmac->tasklet);\r\nreturn IRQ_HANDLED;\r\n} else\r\nreturn IRQ_NONE;\r\n}\r\nstatic irqreturn_t mmp_tdma_int_handler(int irq, void *dev_id)\r\n{\r\nstruct mmp_tdma_device *tdev = dev_id;\r\nint i, ret;\r\nint irq_num = 0;\r\nfor (i = 0; i < TDMA_CHANNEL_NUM; i++) {\r\nstruct mmp_tdma_chan *tdmac = tdev->tdmac[i];\r\nret = mmp_tdma_chan_handler(irq, tdmac);\r\nif (ret == IRQ_HANDLED)\r\nirq_num++;\r\n}\r\nif (irq_num)\r\nreturn IRQ_HANDLED;\r\nelse\r\nreturn IRQ_NONE;\r\n}\r\nstatic void dma_do_tasklet(unsigned long data)\r\n{\r\nstruct mmp_tdma_chan *tdmac = (struct mmp_tdma_chan *)data;\r\ndmaengine_desc_get_callback_invoke(&tdmac->desc, NULL);\r\n}\r\nstatic void mmp_tdma_free_descriptor(struct mmp_tdma_chan *tdmac)\r\n{\r\nstruct gen_pool *gpool;\r\nint size = tdmac->desc_num * sizeof(struct mmp_tdma_desc);\r\ngpool = tdmac->pool;\r\nif (gpool && tdmac->desc_arr)\r\ngen_pool_free(gpool, (unsigned long)tdmac->desc_arr,\r\nsize);\r\ntdmac->desc_arr = NULL;\r\nreturn;\r\n}\r\nstatic dma_cookie_t mmp_tdma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(tx->chan);\r\nmmp_tdma_chan_set_desc(tdmac, tdmac->desc_arr_phys);\r\nreturn 0;\r\n}\r\nstatic int mmp_tdma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nint ret;\r\ndma_async_tx_descriptor_init(&tdmac->desc, chan);\r\ntdmac->desc.tx_submit = mmp_tdma_tx_submit;\r\nif (tdmac->irq) {\r\nret = devm_request_irq(tdmac->dev, tdmac->irq,\r\nmmp_tdma_chan_handler, 0, "tdma", tdmac);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 1;\r\n}\r\nstatic void mmp_tdma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nif (tdmac->irq)\r\ndevm_free_irq(tdmac->dev, tdmac->irq, tdmac);\r\nmmp_tdma_free_descriptor(tdmac);\r\nreturn;\r\n}\r\nstatic struct mmp_tdma_desc *mmp_tdma_alloc_descriptor(struct mmp_tdma_chan *tdmac)\r\n{\r\nstruct gen_pool *gpool;\r\nint size = tdmac->desc_num * sizeof(struct mmp_tdma_desc);\r\ngpool = tdmac->pool;\r\nif (!gpool)\r\nreturn NULL;\r\ntdmac->desc_arr = gen_pool_dma_alloc(gpool, size, &tdmac->desc_arr_phys);\r\nreturn tdmac->desc_arr;\r\n}\r\nstatic struct dma_async_tx_descriptor *mmp_tdma_prep_dma_cyclic(\r\nstruct dma_chan *chan, dma_addr_t dma_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nstruct mmp_tdma_desc *desc;\r\nint num_periods = buf_len / period_len;\r\nint i = 0, buf = 0;\r\nif (tdmac->status != DMA_COMPLETE)\r\nreturn NULL;\r\nif (period_len > TDMA_MAX_XFER_BYTES) {\r\ndev_err(tdmac->dev,\r\n"maximum period size exceeded: %zu > %d\n",\r\nperiod_len, TDMA_MAX_XFER_BYTES);\r\ngoto err_out;\r\n}\r\ntdmac->status = DMA_IN_PROGRESS;\r\ntdmac->desc_num = num_periods;\r\ndesc = mmp_tdma_alloc_descriptor(tdmac);\r\nif (!desc)\r\ngoto err_out;\r\nwhile (buf < buf_len) {\r\ndesc = &tdmac->desc_arr[i];\r\nif (i + 1 == num_periods)\r\ndesc->nxt_desc = tdmac->desc_arr_phys;\r\nelse\r\ndesc->nxt_desc = tdmac->desc_arr_phys +\r\nsizeof(*desc) * (i + 1);\r\nif (direction == DMA_MEM_TO_DEV) {\r\ndesc->src_addr = dma_addr;\r\ndesc->dst_addr = tdmac->dev_addr;\r\n} else {\r\ndesc->src_addr = tdmac->dev_addr;\r\ndesc->dst_addr = dma_addr;\r\n}\r\ndesc->byte_cnt = period_len;\r\ndma_addr += period_len;\r\nbuf += period_len;\r\ni++;\r\n}\r\nif (flags & DMA_PREP_INTERRUPT)\r\nmmp_tdma_enable_irq(tdmac, true);\r\ntdmac->buf_len = buf_len;\r\ntdmac->period_len = period_len;\r\ntdmac->pos = 0;\r\nreturn &tdmac->desc;\r\nerr_out:\r\ntdmac->status = DMA_ERROR;\r\nreturn NULL;\r\n}\r\nstatic int mmp_tdma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nmmp_tdma_disable_chan(chan);\r\nmmp_tdma_enable_irq(tdmac, false);\r\nreturn 0;\r\n}\r\nstatic int mmp_tdma_config(struct dma_chan *chan,\r\nstruct dma_slave_config *dmaengine_cfg)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nif (dmaengine_cfg->direction == DMA_DEV_TO_MEM) {\r\ntdmac->dev_addr = dmaengine_cfg->src_addr;\r\ntdmac->burst_sz = dmaengine_cfg->src_maxburst;\r\ntdmac->buswidth = dmaengine_cfg->src_addr_width;\r\n} else {\r\ntdmac->dev_addr = dmaengine_cfg->dst_addr;\r\ntdmac->burst_sz = dmaengine_cfg->dst_maxburst;\r\ntdmac->buswidth = dmaengine_cfg->dst_addr_width;\r\n}\r\ntdmac->dir = dmaengine_cfg->direction;\r\nreturn mmp_tdma_config_chan(chan);\r\n}\r\nstatic enum dma_status mmp_tdma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\ntdmac->pos = mmp_tdma_get_pos(tdmac);\r\ndma_set_tx_state(txstate, chan->completed_cookie, chan->cookie,\r\ntdmac->buf_len - tdmac->pos);\r\nreturn tdmac->status;\r\n}\r\nstatic void mmp_tdma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nmmp_tdma_enable_chan(tdmac);\r\n}\r\nstatic int mmp_tdma_remove(struct platform_device *pdev)\r\n{\r\nstruct mmp_tdma_device *tdev = platform_get_drvdata(pdev);\r\ndma_async_device_unregister(&tdev->device);\r\nreturn 0;\r\n}\r\nstatic int mmp_tdma_chan_init(struct mmp_tdma_device *tdev,\r\nint idx, int irq,\r\nint type, struct gen_pool *pool)\r\n{\r\nstruct mmp_tdma_chan *tdmac;\r\nif (idx >= TDMA_CHANNEL_NUM) {\r\ndev_err(tdev->dev, "too many channels for device!\n");\r\nreturn -EINVAL;\r\n}\r\ntdmac = devm_kzalloc(tdev->dev, sizeof(*tdmac), GFP_KERNEL);\r\nif (!tdmac)\r\nreturn -ENOMEM;\r\nif (irq)\r\ntdmac->irq = irq;\r\ntdmac->dev = tdev->dev;\r\ntdmac->chan.device = &tdev->device;\r\ntdmac->idx = idx;\r\ntdmac->type = type;\r\ntdmac->reg_base = tdev->base + idx * 4;\r\ntdmac->pool = pool;\r\ntdmac->status = DMA_COMPLETE;\r\ntdev->tdmac[tdmac->idx] = tdmac;\r\ntasklet_init(&tdmac->tasklet, dma_do_tasklet, (unsigned long)tdmac);\r\nlist_add_tail(&tdmac->chan.device_node,\r\n&tdev->device.channels);\r\nreturn 0;\r\n}\r\nstatic bool mmp_tdma_filter_fn(struct dma_chan *chan, void *fn_param)\r\n{\r\nstruct mmp_tdma_filter_param *param = fn_param;\r\nstruct mmp_tdma_chan *tdmac = to_mmp_tdma_chan(chan);\r\nstruct dma_device *pdma_device = tdmac->chan.device;\r\nif (pdma_device->dev->of_node != param->of_node)\r\nreturn false;\r\nif (chan->chan_id != param->chan_id)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic struct dma_chan *mmp_tdma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct mmp_tdma_device *tdev = ofdma->of_dma_data;\r\ndma_cap_mask_t mask = tdev->device.cap_mask;\r\nstruct mmp_tdma_filter_param param;\r\nif (dma_spec->args_count != 1)\r\nreturn NULL;\r\nparam.of_node = ofdma->of_node;\r\nparam.chan_id = dma_spec->args[0];\r\nif (param.chan_id >= TDMA_CHANNEL_NUM)\r\nreturn NULL;\r\nreturn dma_request_channel(mask, mmp_tdma_filter_fn, &param);\r\n}\r\nstatic int mmp_tdma_probe(struct platform_device *pdev)\r\n{\r\nenum mmp_tdma_type type;\r\nconst struct of_device_id *of_id;\r\nstruct mmp_tdma_device *tdev;\r\nstruct resource *iores;\r\nint i, ret;\r\nint irq = 0, irq_num = 0;\r\nint chan_num = TDMA_CHANNEL_NUM;\r\nstruct gen_pool *pool = NULL;\r\nof_id = of_match_device(mmp_tdma_dt_ids, &pdev->dev);\r\nif (of_id)\r\ntype = (enum mmp_tdma_type) of_id->data;\r\nelse\r\ntype = platform_get_device_id(pdev)->driver_data;\r\ntdev = devm_kzalloc(&pdev->dev, sizeof(*tdev), GFP_KERNEL);\r\nif (!tdev)\r\nreturn -ENOMEM;\r\ntdev->dev = &pdev->dev;\r\nfor (i = 0; i < chan_num; i++) {\r\nif (platform_get_irq(pdev, i) > 0)\r\nirq_num++;\r\n}\r\niores = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ntdev->base = devm_ioremap_resource(&pdev->dev, iores);\r\nif (IS_ERR(tdev->base))\r\nreturn PTR_ERR(tdev->base);\r\nINIT_LIST_HEAD(&tdev->device.channels);\r\nif (pdev->dev.of_node)\r\npool = of_gen_pool_get(pdev->dev.of_node, "asram", 0);\r\nelse\r\npool = sram_get_gpool("asram");\r\nif (!pool) {\r\ndev_err(&pdev->dev, "asram pool not available\n");\r\nreturn -ENOMEM;\r\n}\r\nif (irq_num != chan_num) {\r\nirq = platform_get_irq(pdev, 0);\r\nret = devm_request_irq(&pdev->dev, irq,\r\nmmp_tdma_int_handler, 0, "tdma", tdev);\r\nif (ret)\r\nreturn ret;\r\n}\r\nfor (i = 0; i < chan_num; i++) {\r\nirq = (irq_num != chan_num) ? 0 : platform_get_irq(pdev, i);\r\nret = mmp_tdma_chan_init(tdev, i, irq, type, pool);\r\nif (ret)\r\nreturn ret;\r\n}\r\ndma_cap_set(DMA_SLAVE, tdev->device.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, tdev->device.cap_mask);\r\ntdev->device.dev = &pdev->dev;\r\ntdev->device.device_alloc_chan_resources =\r\nmmp_tdma_alloc_chan_resources;\r\ntdev->device.device_free_chan_resources =\r\nmmp_tdma_free_chan_resources;\r\ntdev->device.device_prep_dma_cyclic = mmp_tdma_prep_dma_cyclic;\r\ntdev->device.device_tx_status = mmp_tdma_tx_status;\r\ntdev->device.device_issue_pending = mmp_tdma_issue_pending;\r\ntdev->device.device_config = mmp_tdma_config;\r\ntdev->device.device_pause = mmp_tdma_pause_chan;\r\ntdev->device.device_resume = mmp_tdma_resume_chan;\r\ntdev->device.device_terminate_all = mmp_tdma_terminate_all;\r\ntdev->device.copy_align = DMAENGINE_ALIGN_8_BYTES;\r\ndma_set_mask(&pdev->dev, DMA_BIT_MASK(64));\r\nplatform_set_drvdata(pdev, tdev);\r\nret = dma_async_device_register(&tdev->device);\r\nif (ret) {\r\ndev_err(tdev->device.dev, "unable to register\n");\r\nreturn ret;\r\n}\r\nif (pdev->dev.of_node) {\r\nret = of_dma_controller_register(pdev->dev.of_node,\r\nmmp_tdma_xlate, tdev);\r\nif (ret) {\r\ndev_err(tdev->device.dev,\r\n"failed to register controller\n");\r\ndma_async_device_unregister(&tdev->device);\r\n}\r\n}\r\ndev_info(tdev->device.dev, "initialized\n");\r\nreturn 0;\r\n}
