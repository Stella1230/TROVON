static void nicvf_get_page(struct nicvf *nic)\r\n{\r\nif (!nic->rb_pageref || !nic->rb_page)\r\nreturn;\r\npage_ref_add(nic->rb_page, nic->rb_pageref);\r\nnic->rb_pageref = 0;\r\n}\r\nstatic int nicvf_poll_reg(struct nicvf *nic, int qidx,\r\nu64 reg, int bit_pos, int bits, int val)\r\n{\r\nu64 bit_mask;\r\nu64 reg_val;\r\nint timeout = 10;\r\nbit_mask = (1ULL << bits) - 1;\r\nbit_mask = (bit_mask << bit_pos);\r\nwhile (timeout) {\r\nreg_val = nicvf_queue_reg_read(nic, reg, qidx);\r\nif (((reg_val & bit_mask) >> bit_pos) == val)\r\nreturn 0;\r\nusleep_range(1000, 2000);\r\ntimeout--;\r\n}\r\nnetdev_err(nic->netdev, "Poll on reg 0x%llx failed\n", reg);\r\nreturn 1;\r\n}\r\nstatic int nicvf_alloc_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem,\r\nint q_len, int desc_size, int align_bytes)\r\n{\r\ndmem->q_len = q_len;\r\ndmem->size = (desc_size * q_len) + align_bytes;\r\ndmem->unalign_base = dma_zalloc_coherent(&nic->pdev->dev, dmem->size,\r\n&dmem->dma, GFP_KERNEL);\r\nif (!dmem->unalign_base)\r\nreturn -ENOMEM;\r\ndmem->phys_base = NICVF_ALIGNED_ADDR((u64)dmem->dma, align_bytes);\r\ndmem->base = dmem->unalign_base + (dmem->phys_base - dmem->dma);\r\nreturn 0;\r\n}\r\nstatic void nicvf_free_q_desc_mem(struct nicvf *nic, struct q_desc_mem *dmem)\r\n{\r\nif (!dmem)\r\nreturn;\r\ndma_free_coherent(&nic->pdev->dev, dmem->size,\r\ndmem->unalign_base, dmem->dma);\r\ndmem->unalign_base = NULL;\r\ndmem->base = NULL;\r\n}\r\nstatic inline struct pgcache *nicvf_alloc_page(struct nicvf *nic,\r\nstruct rbdr *rbdr, gfp_t gfp)\r\n{\r\nint ref_count;\r\nstruct page *page = NULL;\r\nstruct pgcache *pgcache, *next;\r\npgcache = &rbdr->pgcache[rbdr->pgidx];\r\npage = pgcache->page;\r\nif (page) {\r\nref_count = page_ref_count(page);\r\nif (rbdr->is_xdp && (ref_count == pgcache->ref_count))\r\npgcache->ref_count--;\r\nelse\r\npage = NULL;\r\nif (!rbdr->is_xdp && (ref_count != 1))\r\npage = NULL;\r\n}\r\nif (!page) {\r\npage = alloc_pages(gfp | __GFP_COMP | __GFP_NOWARN, 0);\r\nif (!page)\r\nreturn NULL;\r\nthis_cpu_inc(nic->pnicvf->drv_stats->page_alloc);\r\nif (rbdr->pgalloc >= rbdr->pgcnt) {\r\nnic->rb_page = page;\r\nreturn NULL;\r\n}\r\npgcache->page = page;\r\npgcache->dma_addr = 0;\r\npgcache->ref_count = 0;\r\nrbdr->pgalloc++;\r\n}\r\nif (rbdr->is_xdp) {\r\nif (!pgcache->ref_count) {\r\npgcache->ref_count = XDP_PAGE_REFCNT_REFILL;\r\npage_ref_add(page, XDP_PAGE_REFCNT_REFILL);\r\n}\r\n} else {\r\npage_ref_add(page, 1);\r\n}\r\nrbdr->pgidx++;\r\nrbdr->pgidx &= (rbdr->pgcnt - 1);\r\nnext = &rbdr->pgcache[rbdr->pgidx];\r\npage = next->page;\r\nif (page)\r\nprefetch(&page->_refcount);\r\nreturn pgcache;\r\n}\r\nstatic inline int nicvf_alloc_rcv_buffer(struct nicvf *nic, struct rbdr *rbdr,\r\ngfp_t gfp, u32 buf_len, u64 *rbuf)\r\n{\r\nstruct pgcache *pgcache = NULL;\r\nif (!rbdr->is_xdp && nic->rb_page &&\r\n((nic->rb_page_offset + buf_len) <= PAGE_SIZE)) {\r\nnic->rb_pageref++;\r\ngoto ret;\r\n}\r\nnicvf_get_page(nic);\r\nnic->rb_page = NULL;\r\npgcache = nicvf_alloc_page(nic, rbdr, gfp);\r\nif (!pgcache && !nic->rb_page) {\r\nthis_cpu_inc(nic->pnicvf->drv_stats->rcv_buffer_alloc_failures);\r\nreturn -ENOMEM;\r\n}\r\nnic->rb_page_offset = 0;\r\nif (rbdr->is_xdp)\r\nbuf_len += XDP_PACKET_HEADROOM;\r\nif (pgcache)\r\nnic->rb_page = pgcache->page;\r\nret:\r\nif (rbdr->is_xdp && pgcache && pgcache->dma_addr) {\r\n*rbuf = pgcache->dma_addr;\r\n} else {\r\n*rbuf = (u64)dma_map_page_attrs(&nic->pdev->dev, nic->rb_page,\r\nnic->rb_page_offset, buf_len,\r\nDMA_FROM_DEVICE,\r\nDMA_ATTR_SKIP_CPU_SYNC);\r\nif (dma_mapping_error(&nic->pdev->dev, (dma_addr_t)*rbuf)) {\r\nif (!nic->rb_page_offset)\r\n__free_pages(nic->rb_page, 0);\r\nnic->rb_page = NULL;\r\nreturn -ENOMEM;\r\n}\r\nif (pgcache)\r\npgcache->dma_addr = *rbuf + XDP_PACKET_HEADROOM;\r\nnic->rb_page_offset += buf_len;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct sk_buff *nicvf_rb_ptr_to_skb(struct nicvf *nic,\r\nu64 rb_ptr, int len)\r\n{\r\nvoid *data;\r\nstruct sk_buff *skb;\r\ndata = phys_to_virt(rb_ptr);\r\nskb = build_skb(data, RCV_FRAG_LEN);\r\nif (!skb) {\r\nput_page(virt_to_page(data));\r\nreturn NULL;\r\n}\r\nprefetch(skb->data);\r\nreturn skb;\r\n}\r\nstatic int nicvf_init_rbdr(struct nicvf *nic, struct rbdr *rbdr,\r\nint ring_len, int buf_size)\r\n{\r\nint idx;\r\nu64 rbuf;\r\nstruct rbdr_entry_t *desc;\r\nint err;\r\nerr = nicvf_alloc_q_desc_mem(nic, &rbdr->dmem, ring_len,\r\nsizeof(struct rbdr_entry_t),\r\nNICVF_RCV_BUF_ALIGN_BYTES);\r\nif (err)\r\nreturn err;\r\nrbdr->desc = rbdr->dmem.base;\r\nrbdr->dma_size = buf_size;\r\nrbdr->enable = true;\r\nrbdr->thresh = RBDR_THRESH;\r\nrbdr->head = 0;\r\nrbdr->tail = 0;\r\nif (!nic->pnicvf->xdp_prog) {\r\nrbdr->pgcnt = ring_len / (PAGE_SIZE / buf_size);\r\nrbdr->is_xdp = false;\r\n} else {\r\nrbdr->pgcnt = ring_len;\r\nrbdr->is_xdp = true;\r\n}\r\nrbdr->pgcnt = roundup_pow_of_two(rbdr->pgcnt);\r\nrbdr->pgcache = kzalloc(sizeof(*rbdr->pgcache) *\r\nrbdr->pgcnt, GFP_KERNEL);\r\nif (!rbdr->pgcache)\r\nreturn -ENOMEM;\r\nrbdr->pgidx = 0;\r\nrbdr->pgalloc = 0;\r\nnic->rb_page = NULL;\r\nfor (idx = 0; idx < ring_len; idx++) {\r\nerr = nicvf_alloc_rcv_buffer(nic, rbdr, GFP_KERNEL,\r\nRCV_FRAG_LEN, &rbuf);\r\nif (err) {\r\nrbdr->tail = idx - 1;\r\nreturn err;\r\n}\r\ndesc = GET_RBDR_DESC(rbdr, idx);\r\ndesc->buf_addr = rbuf & ~(NICVF_RCV_BUF_ALIGN_BYTES - 1);\r\n}\r\nnicvf_get_page(nic);\r\nreturn 0;\r\n}\r\nstatic void nicvf_free_rbdr(struct nicvf *nic, struct rbdr *rbdr)\r\n{\r\nint head, tail;\r\nu64 buf_addr, phys_addr;\r\nstruct pgcache *pgcache;\r\nstruct rbdr_entry_t *desc;\r\nif (!rbdr)\r\nreturn;\r\nrbdr->enable = false;\r\nif (!rbdr->dmem.base)\r\nreturn;\r\nhead = rbdr->head;\r\ntail = rbdr->tail;\r\nwhile (head != tail) {\r\ndesc = GET_RBDR_DESC(rbdr, head);\r\nbuf_addr = desc->buf_addr;\r\nphys_addr = nicvf_iova_to_phys(nic, buf_addr);\r\ndma_unmap_page_attrs(&nic->pdev->dev, buf_addr, RCV_FRAG_LEN,\r\nDMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\r\nif (phys_addr)\r\nput_page(virt_to_page(phys_to_virt(phys_addr)));\r\nhead++;\r\nhead &= (rbdr->dmem.q_len - 1);\r\n}\r\ndesc = GET_RBDR_DESC(rbdr, tail);\r\nbuf_addr = desc->buf_addr;\r\nphys_addr = nicvf_iova_to_phys(nic, buf_addr);\r\ndma_unmap_page_attrs(&nic->pdev->dev, buf_addr, RCV_FRAG_LEN,\r\nDMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\r\nif (phys_addr)\r\nput_page(virt_to_page(phys_to_virt(phys_addr)));\r\nsmp_rmb();\r\nhead = 0;\r\nwhile (head < rbdr->pgcnt) {\r\npgcache = &rbdr->pgcache[head];\r\nif (pgcache->page && page_ref_count(pgcache->page) != 0) {\r\nif (!rbdr->is_xdp) {\r\nput_page(pgcache->page);\r\ncontinue;\r\n}\r\npage_ref_sub(pgcache->page, pgcache->ref_count - 1);\r\nput_page(pgcache->page);\r\n}\r\nhead++;\r\n}\r\nnicvf_free_q_desc_mem(nic, &rbdr->dmem);\r\n}\r\nstatic void nicvf_refill_rbdr(struct nicvf *nic, gfp_t gfp)\r\n{\r\nstruct queue_set *qs = nic->qs;\r\nint rbdr_idx = qs->rbdr_cnt;\r\nint tail, qcount;\r\nint refill_rb_cnt;\r\nstruct rbdr *rbdr;\r\nstruct rbdr_entry_t *desc;\r\nu64 rbuf;\r\nint new_rb = 0;\r\nrefill:\r\nif (!rbdr_idx)\r\nreturn;\r\nrbdr_idx--;\r\nrbdr = &qs->rbdr[rbdr_idx];\r\nif (!rbdr->enable)\r\ngoto next_rbdr;\r\nqcount = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0, rbdr_idx);\r\nqcount &= 0x7FFFF;\r\nif (qcount >= (qs->rbdr_len - 1))\r\ngoto next_rbdr;\r\nelse\r\nrefill_rb_cnt = qs->rbdr_len - qcount - 1;\r\nsmp_rmb();\r\ntail = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_TAIL, rbdr_idx) >> 3;\r\nwhile (refill_rb_cnt) {\r\ntail++;\r\ntail &= (rbdr->dmem.q_len - 1);\r\nif (nicvf_alloc_rcv_buffer(nic, rbdr, gfp, RCV_FRAG_LEN, &rbuf))\r\nbreak;\r\ndesc = GET_RBDR_DESC(rbdr, tail);\r\ndesc->buf_addr = rbuf & ~(NICVF_RCV_BUF_ALIGN_BYTES - 1);\r\nrefill_rb_cnt--;\r\nnew_rb++;\r\n}\r\nnicvf_get_page(nic);\r\nsmp_wmb();\r\nif (refill_rb_cnt)\r\nnic->rb_alloc_fail = true;\r\nelse\r\nnic->rb_alloc_fail = false;\r\nnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,\r\nrbdr_idx, new_rb);\r\nnext_rbdr:\r\nif (!nic->rb_alloc_fail && rbdr->enable &&\r\nnetif_running(nic->pnicvf->netdev))\r\nnicvf_enable_intr(nic, NICVF_INTR_RBDR, rbdr_idx);\r\nif (rbdr_idx)\r\ngoto refill;\r\n}\r\nvoid nicvf_rbdr_work(struct work_struct *work)\r\n{\r\nstruct nicvf *nic = container_of(work, struct nicvf, rbdr_work.work);\r\nnicvf_refill_rbdr(nic, GFP_KERNEL);\r\nif (nic->rb_alloc_fail)\r\nschedule_delayed_work(&nic->rbdr_work, msecs_to_jiffies(10));\r\nelse\r\nnic->rb_work_scheduled = false;\r\n}\r\nvoid nicvf_rbdr_task(unsigned long data)\r\n{\r\nstruct nicvf *nic = (struct nicvf *)data;\r\nnicvf_refill_rbdr(nic, GFP_ATOMIC);\r\nif (nic->rb_alloc_fail) {\r\nnic->rb_work_scheduled = true;\r\nschedule_delayed_work(&nic->rbdr_work, msecs_to_jiffies(10));\r\n}\r\n}\r\nstatic int nicvf_init_cmp_queue(struct nicvf *nic,\r\nstruct cmp_queue *cq, int q_len)\r\n{\r\nint err;\r\nerr = nicvf_alloc_q_desc_mem(nic, &cq->dmem, q_len, CMP_QUEUE_DESC_SIZE,\r\nNICVF_CQ_BASE_ALIGN_BYTES);\r\nif (err)\r\nreturn err;\r\ncq->desc = cq->dmem.base;\r\ncq->thresh = pass1_silicon(nic->pdev) ? 0 : CMP_QUEUE_CQE_THRESH;\r\nnic->cq_coalesce_usecs = (CMP_QUEUE_TIMER_THRESH * 0.05) - 1;\r\nreturn 0;\r\n}\r\nstatic void nicvf_free_cmp_queue(struct nicvf *nic, struct cmp_queue *cq)\r\n{\r\nif (!cq)\r\nreturn;\r\nif (!cq->dmem.base)\r\nreturn;\r\nnicvf_free_q_desc_mem(nic, &cq->dmem);\r\n}\r\nstatic int nicvf_init_snd_queue(struct nicvf *nic,\r\nstruct snd_queue *sq, int q_len, int qidx)\r\n{\r\nint err;\r\nerr = nicvf_alloc_q_desc_mem(nic, &sq->dmem, q_len, SND_QUEUE_DESC_SIZE,\r\nNICVF_SQ_BASE_ALIGN_BYTES);\r\nif (err)\r\nreturn err;\r\nsq->desc = sq->dmem.base;\r\nsq->skbuff = kcalloc(q_len, sizeof(u64), GFP_KERNEL);\r\nif (!sq->skbuff)\r\nreturn -ENOMEM;\r\nsq->head = 0;\r\nsq->tail = 0;\r\nsq->thresh = SND_QUEUE_THRESH;\r\nif (nic->sqs_mode)\r\nqidx += ((nic->sqs_id + 1) * MAX_SND_QUEUES_PER_QS);\r\nif (qidx < nic->pnicvf->xdp_tx_queues) {\r\nsq->xdp_page = kcalloc(q_len, sizeof(u64), GFP_KERNEL);\r\nif (!sq->xdp_page)\r\nreturn -ENOMEM;\r\nsq->xdp_desc_cnt = 0;\r\nsq->xdp_free_cnt = q_len - 1;\r\nsq->is_xdp = true;\r\n} else {\r\nsq->xdp_page = NULL;\r\nsq->xdp_desc_cnt = 0;\r\nsq->xdp_free_cnt = 0;\r\nsq->is_xdp = false;\r\natomic_set(&sq->free_cnt, q_len - 1);\r\nsq->tso_hdrs = dma_alloc_coherent(&nic->pdev->dev,\r\nq_len * TSO_HEADER_SIZE,\r\n&sq->tso_hdrs_phys,\r\nGFP_KERNEL);\r\nif (!sq->tso_hdrs)\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nvoid nicvf_unmap_sndq_buffers(struct nicvf *nic, struct snd_queue *sq,\r\nint hdr_sqe, u8 subdesc_cnt)\r\n{\r\nu8 idx;\r\nstruct sq_gather_subdesc *gather;\r\nfor (idx = 0; idx < subdesc_cnt; idx++) {\r\nhdr_sqe++;\r\nhdr_sqe &= (sq->dmem.q_len - 1);\r\ngather = (struct sq_gather_subdesc *)GET_SQ_DESC(sq, hdr_sqe);\r\ndma_unmap_page_attrs(&nic->pdev->dev, gather->addr,\r\ngather->size, DMA_TO_DEVICE,\r\nDMA_ATTR_SKIP_CPU_SYNC);\r\n}\r\n}\r\nstatic void nicvf_free_snd_queue(struct nicvf *nic, struct snd_queue *sq)\r\n{\r\nstruct sk_buff *skb;\r\nstruct page *page;\r\nstruct sq_hdr_subdesc *hdr;\r\nstruct sq_hdr_subdesc *tso_sqe;\r\nif (!sq)\r\nreturn;\r\nif (!sq->dmem.base)\r\nreturn;\r\nif (sq->tso_hdrs)\r\ndma_free_coherent(&nic->pdev->dev,\r\nsq->dmem.q_len * TSO_HEADER_SIZE,\r\nsq->tso_hdrs, sq->tso_hdrs_phys);\r\nsmp_rmb();\r\nwhile (sq->head != sq->tail) {\r\nskb = (struct sk_buff *)sq->skbuff[sq->head];\r\nif (!skb || !sq->xdp_page)\r\ngoto next;\r\npage = (struct page *)sq->xdp_page[sq->head];\r\nif (!page)\r\ngoto next;\r\nelse\r\nput_page(page);\r\nhdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, sq->head);\r\nif (hdr->dont_send) {\r\ntso_sqe =\r\n(struct sq_hdr_subdesc *)GET_SQ_DESC(sq, hdr->rsvd2);\r\nnicvf_unmap_sndq_buffers(nic, sq, hdr->rsvd2,\r\ntso_sqe->subdesc_cnt);\r\n} else {\r\nnicvf_unmap_sndq_buffers(nic, sq, sq->head,\r\nhdr->subdesc_cnt);\r\n}\r\nif (skb)\r\ndev_kfree_skb_any(skb);\r\nnext:\r\nsq->head++;\r\nsq->head &= (sq->dmem.q_len - 1);\r\n}\r\nkfree(sq->skbuff);\r\nkfree(sq->xdp_page);\r\nnicvf_free_q_desc_mem(nic, &sq->dmem);\r\n}\r\nstatic void nicvf_reclaim_snd_queue(struct nicvf *nic,\r\nstruct queue_set *qs, int qidx)\r\n{\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, 0);\r\nif (nicvf_poll_reg(nic, qidx, NIC_QSET_SQ_0_7_STATUS, 21, 1, 0x01))\r\nreturn;\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, NICVF_SQ_RESET);\r\n}\r\nstatic void nicvf_reclaim_rcv_queue(struct nicvf *nic,\r\nstruct queue_set *qs, int qidx)\r\n{\r\nunion nic_mbx mbx = {};\r\nmbx.msg.msg = NIC_MBOX_MSG_RQ_SW_SYNC;\r\nnicvf_send_msg_to_pf(nic, &mbx);\r\n}\r\nstatic void nicvf_reclaim_cmp_queue(struct nicvf *nic,\r\nstruct queue_set *qs, int qidx)\r\n{\r\nnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2, qidx, 0);\r\nnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, 0);\r\nnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, NICVF_CQ_RESET);\r\n}\r\nstatic void nicvf_reclaim_rbdr(struct nicvf *nic,\r\nstruct rbdr *rbdr, int qidx)\r\n{\r\nu64 tmp, fifo_state;\r\nint timeout = 10;\r\nrbdr->head = nicvf_queue_reg_read(nic,\r\nNIC_QSET_RBDR_0_1_HEAD,\r\nqidx) >> 3;\r\nrbdr->tail = nicvf_queue_reg_read(nic,\r\nNIC_QSET_RBDR_0_1_TAIL,\r\nqidx) >> 3;\r\nfifo_state = nicvf_queue_reg_read(nic, NIC_QSET_RBDR_0_1_STATUS0, qidx);\r\nif (((fifo_state >> 62) & 0x03) == 0x3)\r\nnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,\r\nqidx, NICVF_RBDR_RESET);\r\nnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0);\r\nif (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x00))\r\nreturn;\r\nwhile (1) {\r\ntmp = nicvf_queue_reg_read(nic,\r\nNIC_QSET_RBDR_0_1_PREFETCH_STATUS,\r\nqidx);\r\nif ((tmp & 0xFFFFFFFF) == ((tmp >> 32) & 0xFFFFFFFF))\r\nbreak;\r\nusleep_range(1000, 2000);\r\ntimeout--;\r\nif (!timeout) {\r\nnetdev_err(nic->netdev,\r\n"Failed polling on prefetch status\n");\r\nreturn;\r\n}\r\n}\r\nnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,\r\nqidx, NICVF_RBDR_RESET);\r\nif (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x02))\r\nreturn;\r\nnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG, qidx, 0x00);\r\nif (nicvf_poll_reg(nic, qidx, NIC_QSET_RBDR_0_1_STATUS0, 62, 2, 0x00))\r\nreturn;\r\n}\r\nvoid nicvf_config_vlan_stripping(struct nicvf *nic, netdev_features_t features)\r\n{\r\nu64 rq_cfg;\r\nint sqs;\r\nrq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_RQ_GEN_CFG, 0);\r\nif (features & NETIF_F_HW_VLAN_CTAG_RX)\r\nrq_cfg |= (1ULL << 25);\r\nelse\r\nrq_cfg &= ~(1ULL << 25);\r\nnicvf_queue_reg_write(nic, NIC_QSET_RQ_GEN_CFG, 0, rq_cfg);\r\nfor (sqs = 0; sqs < nic->sqs_count; sqs++)\r\nif (nic->snicvf[sqs])\r\nnicvf_queue_reg_write(nic->snicvf[sqs],\r\nNIC_QSET_RQ_GEN_CFG, 0, rq_cfg);\r\n}\r\nstatic void nicvf_reset_rcv_queue_stats(struct nicvf *nic)\r\n{\r\nunion nic_mbx mbx = {};\r\nmbx.reset_stat.msg = NIC_MBOX_MSG_RESET_STAT_COUNTER;\r\nmbx.reset_stat.rx_stat_mask = 0x3FFF;\r\nmbx.reset_stat.tx_stat_mask = 0x1F;\r\nmbx.reset_stat.rq_stat_mask = 0xFFFF;\r\nmbx.reset_stat.sq_stat_mask = 0xFFFF;\r\nnicvf_send_msg_to_pf(nic, &mbx);\r\n}\r\nstatic void nicvf_rcv_queue_config(struct nicvf *nic, struct queue_set *qs,\r\nint qidx, bool enable)\r\n{\r\nunion nic_mbx mbx = {};\r\nstruct rcv_queue *rq;\r\nstruct rq_cfg rq_cfg;\r\nrq = &qs->rq[qidx];\r\nrq->enable = enable;\r\nnicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, 0);\r\nif (!rq->enable) {\r\nnicvf_reclaim_rcv_queue(nic, qs, qidx);\r\nreturn;\r\n}\r\nrq->cq_qs = qs->vnic_id;\r\nrq->cq_idx = qidx;\r\nrq->start_rbdr_qs = qs->vnic_id;\r\nrq->start_qs_rbdr_idx = qs->rbdr_cnt - 1;\r\nrq->cont_rbdr_qs = qs->vnic_id;\r\nrq->cont_qs_rbdr_idx = qs->rbdr_cnt - 1;\r\nrq->caching = 1;\r\nmbx.rq.msg = NIC_MBOX_MSG_RQ_CFG;\r\nmbx.rq.qs_num = qs->vnic_id;\r\nmbx.rq.rq_num = qidx;\r\nmbx.rq.cfg = (rq->caching << 26) | (rq->cq_qs << 19) |\r\n(rq->cq_idx << 16) | (rq->cont_rbdr_qs << 9) |\r\n(rq->cont_qs_rbdr_idx << 8) |\r\n(rq->start_rbdr_qs << 1) | (rq->start_qs_rbdr_idx);\r\nnicvf_send_msg_to_pf(nic, &mbx);\r\nmbx.rq.msg = NIC_MBOX_MSG_RQ_BP_CFG;\r\nmbx.rq.cfg = BIT_ULL(63) | BIT_ULL(62) |\r\n(RQ_PASS_RBDR_LVL << 16) | (RQ_PASS_CQ_LVL << 8) |\r\n(qs->vnic_id << 0);\r\nnicvf_send_msg_to_pf(nic, &mbx);\r\nmbx.rq.msg = NIC_MBOX_MSG_RQ_DROP_CFG;\r\nmbx.rq.cfg = BIT_ULL(63) | BIT_ULL(62) |\r\n(RQ_PASS_RBDR_LVL << 40) | (RQ_DROP_RBDR_LVL << 32) |\r\n(RQ_PASS_CQ_LVL << 16) | (RQ_DROP_CQ_LVL << 8);\r\nnicvf_send_msg_to_pf(nic, &mbx);\r\nif (!nic->sqs_mode && (qidx == 0)) {\r\nnicvf_queue_reg_write(nic, NIC_QSET_RQ_GEN_CFG, 0,\r\n(BIT(24) | BIT(23) | BIT(21) | BIT(20)));\r\nnicvf_config_vlan_stripping(nic, nic->netdev->features);\r\n}\r\nmemset(&rq_cfg, 0, sizeof(struct rq_cfg));\r\nrq_cfg.ena = 1;\r\nrq_cfg.tcp_ena = 0;\r\nnicvf_queue_reg_write(nic, NIC_QSET_RQ_0_7_CFG, qidx, *(u64 *)&rq_cfg);\r\n}\r\nvoid nicvf_cmp_queue_config(struct nicvf *nic, struct queue_set *qs,\r\nint qidx, bool enable)\r\n{\r\nstruct cmp_queue *cq;\r\nstruct cq_cfg cq_cfg;\r\ncq = &qs->cq[qidx];\r\ncq->enable = enable;\r\nif (!cq->enable) {\r\nnicvf_reclaim_cmp_queue(nic, qs, qidx);\r\nreturn;\r\n}\r\nnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, NICVF_CQ_RESET);\r\nif (!cq->enable)\r\nreturn;\r\nspin_lock_init(&cq->lock);\r\nnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_BASE,\r\nqidx, (u64)(cq->dmem.phys_base));\r\nmemset(&cq_cfg, 0, sizeof(struct cq_cfg));\r\ncq_cfg.ena = 1;\r\ncq_cfg.reset = 0;\r\ncq_cfg.caching = 0;\r\ncq_cfg.qsize = ilog2(qs->cq_len >> 10);\r\ncq_cfg.avg_con = 0;\r\nnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG, qidx, *(u64 *)&cq_cfg);\r\nnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_THRESH, qidx, cq->thresh);\r\nnicvf_queue_reg_write(nic, NIC_QSET_CQ_0_7_CFG2,\r\nqidx, CMP_QUEUE_TIMER_THRESH);\r\n}\r\nstatic void nicvf_snd_queue_config(struct nicvf *nic, struct queue_set *qs,\r\nint qidx, bool enable)\r\n{\r\nunion nic_mbx mbx = {};\r\nstruct snd_queue *sq;\r\nstruct sq_cfg sq_cfg;\r\nsq = &qs->sq[qidx];\r\nsq->enable = enable;\r\nif (!sq->enable) {\r\nnicvf_reclaim_snd_queue(nic, qs, qidx);\r\nreturn;\r\n}\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, NICVF_SQ_RESET);\r\nsq->cq_qs = qs->vnic_id;\r\nsq->cq_idx = qidx;\r\nmbx.sq.msg = NIC_MBOX_MSG_SQ_CFG;\r\nmbx.sq.qs_num = qs->vnic_id;\r\nmbx.sq.sq_num = qidx;\r\nmbx.sq.sqs_mode = nic->sqs_mode;\r\nmbx.sq.cfg = (sq->cq_qs << 3) | sq->cq_idx;\r\nnicvf_send_msg_to_pf(nic, &mbx);\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_BASE,\r\nqidx, (u64)(sq->dmem.phys_base));\r\nmemset(&sq_cfg, 0, sizeof(struct sq_cfg));\r\nsq_cfg.ena = 1;\r\nsq_cfg.reset = 0;\r\nsq_cfg.ldwb = 0;\r\nsq_cfg.qsize = ilog2(qs->sq_len >> 10);\r\nsq_cfg.tstmp_bgx_intf = 0;\r\nsq_cfg.cq_limit = (CMP_QUEUE_PIPELINE_RSVD * 256) / qs->cq_len;\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, *(u64 *)&sq_cfg);\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_THRESH, qidx, sq->thresh);\r\nif (cpu_online(qidx)) {\r\ncpumask_set_cpu(qidx, &sq->affinity_mask);\r\nnetif_set_xps_queue(nic->netdev,\r\n&sq->affinity_mask, qidx);\r\n}\r\n}\r\nstatic void nicvf_rbdr_config(struct nicvf *nic, struct queue_set *qs,\r\nint qidx, bool enable)\r\n{\r\nstruct rbdr *rbdr;\r\nstruct rbdr_cfg rbdr_cfg;\r\nrbdr = &qs->rbdr[qidx];\r\nnicvf_reclaim_rbdr(nic, rbdr, qidx);\r\nif (!enable)\r\nreturn;\r\nnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_BASE,\r\nqidx, (u64)(rbdr->dmem.phys_base));\r\nmemset(&rbdr_cfg, 0, sizeof(struct rbdr_cfg));\r\nrbdr_cfg.ena = 1;\r\nrbdr_cfg.reset = 0;\r\nrbdr_cfg.ldwb = 0;\r\nrbdr_cfg.qsize = RBDR_SIZE;\r\nrbdr_cfg.avg_con = 0;\r\nrbdr_cfg.lines = rbdr->dma_size / 128;\r\nnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_CFG,\r\nqidx, *(u64 *)&rbdr_cfg);\r\nnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_DOOR,\r\nqidx, qs->rbdr_len - 1);\r\nnicvf_queue_reg_write(nic, NIC_QSET_RBDR_0_1_THRESH,\r\nqidx, rbdr->thresh - 1);\r\n}\r\nvoid nicvf_qset_config(struct nicvf *nic, bool enable)\r\n{\r\nunion nic_mbx mbx = {};\r\nstruct queue_set *qs = nic->qs;\r\nstruct qs_cfg *qs_cfg;\r\nif (!qs) {\r\nnetdev_warn(nic->netdev,\r\n"Qset is still not allocated, don't init queues\n");\r\nreturn;\r\n}\r\nqs->enable = enable;\r\nqs->vnic_id = nic->vf_id;\r\nmbx.qs.msg = NIC_MBOX_MSG_QS_CFG;\r\nmbx.qs.num = qs->vnic_id;\r\nmbx.qs.sqs_count = nic->sqs_count;\r\nmbx.qs.cfg = 0;\r\nqs_cfg = (struct qs_cfg *)&mbx.qs.cfg;\r\nif (qs->enable) {\r\nqs_cfg->ena = 1;\r\n#ifdef __BIG_ENDIAN\r\nqs_cfg->be = 1;\r\n#endif\r\nqs_cfg->vnic = qs->vnic_id;\r\n}\r\nnicvf_send_msg_to_pf(nic, &mbx);\r\n}\r\nstatic void nicvf_free_resources(struct nicvf *nic)\r\n{\r\nint qidx;\r\nstruct queue_set *qs = nic->qs;\r\nfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++)\r\nnicvf_free_rbdr(nic, &qs->rbdr[qidx]);\r\nfor (qidx = 0; qidx < qs->cq_cnt; qidx++)\r\nnicvf_free_cmp_queue(nic, &qs->cq[qidx]);\r\nfor (qidx = 0; qidx < qs->sq_cnt; qidx++)\r\nnicvf_free_snd_queue(nic, &qs->sq[qidx]);\r\n}\r\nstatic int nicvf_alloc_resources(struct nicvf *nic)\r\n{\r\nint qidx;\r\nstruct queue_set *qs = nic->qs;\r\nfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++) {\r\nif (nicvf_init_rbdr(nic, &qs->rbdr[qidx], qs->rbdr_len,\r\nDMA_BUFFER_LEN))\r\ngoto alloc_fail;\r\n}\r\nfor (qidx = 0; qidx < qs->sq_cnt; qidx++) {\r\nif (nicvf_init_snd_queue(nic, &qs->sq[qidx], qs->sq_len, qidx))\r\ngoto alloc_fail;\r\n}\r\nfor (qidx = 0; qidx < qs->cq_cnt; qidx++) {\r\nif (nicvf_init_cmp_queue(nic, &qs->cq[qidx], qs->cq_len))\r\ngoto alloc_fail;\r\n}\r\nreturn 0;\r\nalloc_fail:\r\nnicvf_free_resources(nic);\r\nreturn -ENOMEM;\r\n}\r\nint nicvf_set_qset_resources(struct nicvf *nic)\r\n{\r\nstruct queue_set *qs;\r\nqs = devm_kzalloc(&nic->pdev->dev, sizeof(*qs), GFP_KERNEL);\r\nif (!qs)\r\nreturn -ENOMEM;\r\nnic->qs = qs;\r\nqs->rbdr_cnt = DEFAULT_RBDR_CNT;\r\nqs->rq_cnt = min_t(u8, MAX_RCV_QUEUES_PER_QS, num_online_cpus());\r\nqs->sq_cnt = min_t(u8, MAX_SND_QUEUES_PER_QS, num_online_cpus());\r\nqs->cq_cnt = max_t(u8, qs->rq_cnt, qs->sq_cnt);\r\nqs->rbdr_len = RCV_BUF_COUNT;\r\nqs->sq_len = SND_QUEUE_LEN;\r\nqs->cq_len = CMP_QUEUE_LEN;\r\nnic->rx_queues = qs->rq_cnt;\r\nnic->tx_queues = qs->sq_cnt;\r\nnic->xdp_tx_queues = 0;\r\nreturn 0;\r\n}\r\nint nicvf_config_data_transfer(struct nicvf *nic, bool enable)\r\n{\r\nbool disable = false;\r\nstruct queue_set *qs = nic->qs;\r\nstruct queue_set *pqs = nic->pnicvf->qs;\r\nint qidx;\r\nif (!qs)\r\nreturn 0;\r\nif (nic->sqs_mode && pqs) {\r\nqs->cq_len = pqs->cq_len;\r\nqs->sq_len = pqs->sq_len;\r\n}\r\nif (enable) {\r\nif (nicvf_alloc_resources(nic))\r\nreturn -ENOMEM;\r\nfor (qidx = 0; qidx < qs->sq_cnt; qidx++)\r\nnicvf_snd_queue_config(nic, qs, qidx, enable);\r\nfor (qidx = 0; qidx < qs->cq_cnt; qidx++)\r\nnicvf_cmp_queue_config(nic, qs, qidx, enable);\r\nfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++)\r\nnicvf_rbdr_config(nic, qs, qidx, enable);\r\nfor (qidx = 0; qidx < qs->rq_cnt; qidx++)\r\nnicvf_rcv_queue_config(nic, qs, qidx, enable);\r\n} else {\r\nfor (qidx = 0; qidx < qs->rq_cnt; qidx++)\r\nnicvf_rcv_queue_config(nic, qs, qidx, disable);\r\nfor (qidx = 0; qidx < qs->rbdr_cnt; qidx++)\r\nnicvf_rbdr_config(nic, qs, qidx, disable);\r\nfor (qidx = 0; qidx < qs->sq_cnt; qidx++)\r\nnicvf_snd_queue_config(nic, qs, qidx, disable);\r\nfor (qidx = 0; qidx < qs->cq_cnt; qidx++)\r\nnicvf_cmp_queue_config(nic, qs, qidx, disable);\r\nnicvf_free_resources(nic);\r\n}\r\nnicvf_reset_rcv_queue_stats(nic);\r\nreturn 0;\r\n}\r\nstatic inline int nicvf_get_sq_desc(struct snd_queue *sq, int desc_cnt)\r\n{\r\nint qentry;\r\nqentry = sq->tail;\r\nif (!sq->is_xdp)\r\natomic_sub(desc_cnt, &sq->free_cnt);\r\nelse\r\nsq->xdp_free_cnt -= desc_cnt;\r\nsq->tail += desc_cnt;\r\nsq->tail &= (sq->dmem.q_len - 1);\r\nreturn qentry;\r\n}\r\nstatic inline void nicvf_rollback_sq_desc(struct snd_queue *sq,\r\nint qentry, int desc_cnt)\r\n{\r\nsq->tail = qentry;\r\natomic_add(desc_cnt, &sq->free_cnt);\r\n}\r\nvoid nicvf_put_sq_desc(struct snd_queue *sq, int desc_cnt)\r\n{\r\nif (!sq->is_xdp)\r\natomic_add(desc_cnt, &sq->free_cnt);\r\nelse\r\nsq->xdp_free_cnt += desc_cnt;\r\nsq->head += desc_cnt;\r\nsq->head &= (sq->dmem.q_len - 1);\r\n}\r\nstatic inline int nicvf_get_nxt_sqentry(struct snd_queue *sq, int qentry)\r\n{\r\nqentry++;\r\nqentry &= (sq->dmem.q_len - 1);\r\nreturn qentry;\r\n}\r\nvoid nicvf_sq_enable(struct nicvf *nic, struct snd_queue *sq, int qidx)\r\n{\r\nu64 sq_cfg;\r\nsq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);\r\nsq_cfg |= NICVF_SQ_EN;\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg);\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR, qidx, 0);\r\n}\r\nvoid nicvf_sq_disable(struct nicvf *nic, int qidx)\r\n{\r\nu64 sq_cfg;\r\nsq_cfg = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_CFG, qidx);\r\nsq_cfg &= ~NICVF_SQ_EN;\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_CFG, qidx, sq_cfg);\r\n}\r\nvoid nicvf_sq_free_used_descs(struct net_device *netdev, struct snd_queue *sq,\r\nint qidx)\r\n{\r\nu64 head, tail;\r\nstruct sk_buff *skb;\r\nstruct nicvf *nic = netdev_priv(netdev);\r\nstruct sq_hdr_subdesc *hdr;\r\nhead = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_HEAD, qidx) >> 4;\r\ntail = nicvf_queue_reg_read(nic, NIC_QSET_SQ_0_7_TAIL, qidx) >> 4;\r\nwhile (sq->head != head) {\r\nhdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, sq->head);\r\nif (hdr->subdesc_type != SQ_DESC_TYPE_HEADER) {\r\nnicvf_put_sq_desc(sq, 1);\r\ncontinue;\r\n}\r\nskb = (struct sk_buff *)sq->skbuff[sq->head];\r\nif (skb)\r\ndev_kfree_skb_any(skb);\r\natomic64_add(1, (atomic64_t *)&netdev->stats.tx_packets);\r\natomic64_add(hdr->tot_len,\r\n(atomic64_t *)&netdev->stats.tx_bytes);\r\nnicvf_put_sq_desc(sq, hdr->subdesc_cnt + 1);\r\n}\r\n}\r\nvoid nicvf_xdp_sq_doorbell(struct nicvf *nic,\r\nstruct snd_queue *sq, int sq_num)\r\n{\r\nif (!sq->xdp_desc_cnt)\r\nreturn;\r\nwmb();\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR,\r\nsq_num, sq->xdp_desc_cnt);\r\nsq->xdp_desc_cnt = 0;\r\n}\r\nstatic inline void\r\nnicvf_xdp_sq_add_hdr_subdesc(struct snd_queue *sq, int qentry,\r\nint subdesc_cnt, u64 data, int len)\r\n{\r\nstruct sq_hdr_subdesc *hdr;\r\nhdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, qentry);\r\nmemset(hdr, 0, SND_QUEUE_DESC_SIZE);\r\nhdr->subdesc_type = SQ_DESC_TYPE_HEADER;\r\nhdr->subdesc_cnt = subdesc_cnt;\r\nhdr->tot_len = len;\r\nhdr->post_cqe = 1;\r\nsq->xdp_page[qentry] = (u64)virt_to_page((void *)data);\r\n}\r\nint nicvf_xdp_sq_append_pkt(struct nicvf *nic, struct snd_queue *sq,\r\nu64 bufaddr, u64 dma_addr, u16 len)\r\n{\r\nint subdesc_cnt = MIN_SQ_DESC_PER_PKT_XMIT;\r\nint qentry;\r\nif (subdesc_cnt > sq->xdp_free_cnt)\r\nreturn 0;\r\nqentry = nicvf_get_sq_desc(sq, subdesc_cnt);\r\nnicvf_xdp_sq_add_hdr_subdesc(sq, qentry, subdesc_cnt - 1, bufaddr, len);\r\nqentry = nicvf_get_nxt_sqentry(sq, qentry);\r\nnicvf_sq_add_gather_subdesc(sq, qentry, len, dma_addr);\r\nsq->xdp_desc_cnt += subdesc_cnt;\r\nreturn 1;\r\n}\r\nstatic int nicvf_tso_count_subdescs(struct sk_buff *skb)\r\n{\r\nstruct skb_shared_info *sh = skb_shinfo(skb);\r\nunsigned int sh_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nunsigned int data_len = skb->len - sh_len;\r\nunsigned int p_len = sh->gso_size;\r\nlong f_id = -1;\r\nlong f_size = skb_headlen(skb) - sh_len;\r\nlong f_used = 0;\r\nlong n;\r\nint num_edescs = 0;\r\nint segment;\r\nfor (segment = 0; segment < sh->gso_segs; segment++) {\r\nunsigned int p_used = 0;\r\nfor (num_edescs++; p_used < p_len; num_edescs++) {\r\nwhile (f_used >= f_size) {\r\nf_id++;\r\nf_size = skb_frag_size(&sh->frags[f_id]);\r\nf_used = 0;\r\n}\r\nn = p_len - p_used;\r\nif (n > f_size - f_used)\r\nn = f_size - f_used;\r\nf_used += n;\r\np_used += n;\r\n}\r\ndata_len -= p_len;\r\nif (data_len < p_len)\r\np_len = data_len;\r\n}\r\nreturn num_edescs + sh->gso_segs;\r\n}\r\nstatic int nicvf_sq_subdesc_required(struct nicvf *nic, struct sk_buff *skb)\r\n{\r\nint subdesc_cnt = MIN_SQ_DESC_PER_PKT_XMIT;\r\nif (skb_shinfo(skb)->gso_size && !nic->hw_tso) {\r\nsubdesc_cnt = nicvf_tso_count_subdescs(skb);\r\nreturn subdesc_cnt;\r\n}\r\nif (nic->t88 && nic->hw_tso && skb_shinfo(skb)->gso_size)\r\nsubdesc_cnt += POST_CQE_DESC_COUNT;\r\nif (skb_shinfo(skb)->nr_frags)\r\nsubdesc_cnt += skb_shinfo(skb)->nr_frags;\r\nreturn subdesc_cnt;\r\n}\r\nstatic inline void\r\nnicvf_sq_add_hdr_subdesc(struct nicvf *nic, struct snd_queue *sq, int qentry,\r\nint subdesc_cnt, struct sk_buff *skb, int len)\r\n{\r\nint proto;\r\nstruct sq_hdr_subdesc *hdr;\r\nunion {\r\nstruct iphdr *v4;\r\nstruct ipv6hdr *v6;\r\nunsigned char *hdr;\r\n} ip;\r\nip.hdr = skb_network_header(skb);\r\nhdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, qentry);\r\nmemset(hdr, 0, SND_QUEUE_DESC_SIZE);\r\nhdr->subdesc_type = SQ_DESC_TYPE_HEADER;\r\nif (nic->t88 && nic->hw_tso && skb_shinfo(skb)->gso_size) {\r\nhdr->subdesc_cnt = subdesc_cnt - POST_CQE_DESC_COUNT;\r\n} else {\r\nsq->skbuff[qentry] = (u64)skb;\r\nhdr->post_cqe = 1;\r\nhdr->subdesc_cnt = subdesc_cnt;\r\n}\r\nhdr->tot_len = len;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nhdr->csum_l3 = 1;\r\nhdr->l3_offset = skb_network_offset(skb);\r\nhdr->l4_offset = skb_transport_offset(skb);\r\nproto = (ip.v4->version == 4) ? ip.v4->protocol :\r\nip.v6->nexthdr;\r\nswitch (proto) {\r\ncase IPPROTO_TCP:\r\nhdr->csum_l4 = SEND_L4_CSUM_TCP;\r\nbreak;\r\ncase IPPROTO_UDP:\r\nhdr->csum_l4 = SEND_L4_CSUM_UDP;\r\nbreak;\r\ncase IPPROTO_SCTP:\r\nhdr->csum_l4 = SEND_L4_CSUM_SCTP;\r\nbreak;\r\n}\r\n}\r\nif (nic->hw_tso && skb_shinfo(skb)->gso_size) {\r\nhdr->tso = 1;\r\nhdr->tso_start = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nhdr->tso_max_paysize = skb_shinfo(skb)->gso_size;\r\nhdr->inner_l3_offset = skb_network_offset(skb) - 2;\r\nthis_cpu_inc(nic->pnicvf->drv_stats->tx_tso);\r\n}\r\n}\r\nstatic inline void nicvf_sq_add_gather_subdesc(struct snd_queue *sq, int qentry,\r\nint size, u64 data)\r\n{\r\nstruct sq_gather_subdesc *gather;\r\nqentry &= (sq->dmem.q_len - 1);\r\ngather = (struct sq_gather_subdesc *)GET_SQ_DESC(sq, qentry);\r\nmemset(gather, 0, SND_QUEUE_DESC_SIZE);\r\ngather->subdesc_type = SQ_DESC_TYPE_GATHER;\r\ngather->ld_type = NIC_SEND_LD_TYPE_E_LDD;\r\ngather->size = size;\r\ngather->addr = data;\r\n}\r\nstatic inline void nicvf_sq_add_cqe_subdesc(struct snd_queue *sq, int qentry,\r\nint tso_sqe, struct sk_buff *skb)\r\n{\r\nstruct sq_imm_subdesc *imm;\r\nstruct sq_hdr_subdesc *hdr;\r\nsq->skbuff[qentry] = (u64)skb;\r\nhdr = (struct sq_hdr_subdesc *)GET_SQ_DESC(sq, qentry);\r\nmemset(hdr, 0, SND_QUEUE_DESC_SIZE);\r\nhdr->subdesc_type = SQ_DESC_TYPE_HEADER;\r\nhdr->post_cqe = 1;\r\nhdr->dont_send = 1;\r\nhdr->subdesc_cnt = POST_CQE_DESC_COUNT - 1;\r\nhdr->tot_len = 1;\r\nhdr->rsvd2 = tso_sqe;\r\nqentry = nicvf_get_nxt_sqentry(sq, qentry);\r\nimm = (struct sq_imm_subdesc *)GET_SQ_DESC(sq, qentry);\r\nmemset(imm, 0, SND_QUEUE_DESC_SIZE);\r\nimm->subdesc_type = SQ_DESC_TYPE_IMMEDIATE;\r\nimm->len = 1;\r\n}\r\nstatic inline void nicvf_sq_doorbell(struct nicvf *nic, struct sk_buff *skb,\r\nint sq_num, int desc_cnt)\r\n{\r\nstruct netdev_queue *txq;\r\ntxq = netdev_get_tx_queue(nic->pnicvf->netdev,\r\nskb_get_queue_mapping(skb));\r\nnetdev_tx_sent_queue(txq, skb->len);\r\nsmp_wmb();\r\nnicvf_queue_reg_write(nic, NIC_QSET_SQ_0_7_DOOR,\r\nsq_num, desc_cnt);\r\n}\r\nstatic int nicvf_sq_append_tso(struct nicvf *nic, struct snd_queue *sq,\r\nint sq_num, int qentry, struct sk_buff *skb)\r\n{\r\nstruct tso_t tso;\r\nint seg_subdescs = 0, desc_cnt = 0;\r\nint seg_len, total_len, data_left;\r\nint hdr_qentry = qentry;\r\nint hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\ntso_start(skb, &tso);\r\ntotal_len = skb->len - hdr_len;\r\nwhile (total_len > 0) {\r\nchar *hdr;\r\nhdr_qentry = qentry;\r\ndata_left = min_t(int, skb_shinfo(skb)->gso_size, total_len);\r\ntotal_len -= data_left;\r\nqentry = nicvf_get_nxt_sqentry(sq, qentry);\r\nhdr = sq->tso_hdrs + qentry * TSO_HEADER_SIZE;\r\ntso_build_hdr(skb, hdr, &tso, data_left, total_len == 0);\r\nnicvf_sq_add_gather_subdesc(sq, qentry, hdr_len,\r\nsq->tso_hdrs_phys +\r\nqentry * TSO_HEADER_SIZE);\r\nseg_subdescs = 2;\r\nseg_len = hdr_len;\r\nwhile (data_left > 0) {\r\nint size;\r\nsize = min_t(int, tso.size, data_left);\r\nqentry = nicvf_get_nxt_sqentry(sq, qentry);\r\nnicvf_sq_add_gather_subdesc(sq, qentry, size,\r\nvirt_to_phys(tso.data));\r\nseg_subdescs++;\r\nseg_len += size;\r\ndata_left -= size;\r\ntso_build_data(skb, &tso, size);\r\n}\r\nnicvf_sq_add_hdr_subdesc(nic, sq, hdr_qentry,\r\nseg_subdescs - 1, skb, seg_len);\r\nsq->skbuff[hdr_qentry] = (u64)NULL;\r\nqentry = nicvf_get_nxt_sqentry(sq, qentry);\r\ndesc_cnt += seg_subdescs;\r\n}\r\nsq->skbuff[hdr_qentry] = (u64)skb;\r\nnicvf_sq_doorbell(nic, skb, sq_num, desc_cnt);\r\nthis_cpu_inc(nic->pnicvf->drv_stats->tx_tso);\r\nreturn 1;\r\n}\r\nint nicvf_sq_append_skb(struct nicvf *nic, struct snd_queue *sq,\r\nstruct sk_buff *skb, u8 sq_num)\r\n{\r\nint i, size;\r\nint subdesc_cnt, hdr_sqe = 0;\r\nint qentry;\r\nu64 dma_addr;\r\nsubdesc_cnt = nicvf_sq_subdesc_required(nic, skb);\r\nif (subdesc_cnt > atomic_read(&sq->free_cnt))\r\ngoto append_fail;\r\nqentry = nicvf_get_sq_desc(sq, subdesc_cnt);\r\nif (skb_shinfo(skb)->gso_size && !nic->hw_tso)\r\nreturn nicvf_sq_append_tso(nic, sq, sq_num, qentry, skb);\r\nnicvf_sq_add_hdr_subdesc(nic, sq, qentry, subdesc_cnt - 1,\r\nskb, skb->len);\r\nhdr_sqe = qentry;\r\nqentry = nicvf_get_nxt_sqentry(sq, qentry);\r\nsize = skb_is_nonlinear(skb) ? skb_headlen(skb) : skb->len;\r\ndma_addr = dma_map_page_attrs(&nic->pdev->dev, virt_to_page(skb->data),\r\noffset_in_page(skb->data), size,\r\nDMA_TO_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\r\nif (dma_mapping_error(&nic->pdev->dev, dma_addr)) {\r\nnicvf_rollback_sq_desc(sq, qentry, subdesc_cnt);\r\nreturn 0;\r\n}\r\nnicvf_sq_add_gather_subdesc(sq, qentry, size, dma_addr);\r\nif (!skb_is_nonlinear(skb))\r\ngoto doorbell;\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nconst struct skb_frag_struct *frag;\r\nfrag = &skb_shinfo(skb)->frags[i];\r\nqentry = nicvf_get_nxt_sqentry(sq, qentry);\r\nsize = skb_frag_size(frag);\r\ndma_addr = dma_map_page_attrs(&nic->pdev->dev,\r\nskb_frag_page(frag),\r\nfrag->page_offset, size,\r\nDMA_TO_DEVICE,\r\nDMA_ATTR_SKIP_CPU_SYNC);\r\nif (dma_mapping_error(&nic->pdev->dev, dma_addr)) {\r\nnicvf_unmap_sndq_buffers(nic, sq, hdr_sqe, i);\r\nnicvf_rollback_sq_desc(sq, qentry, subdesc_cnt);\r\nreturn 0;\r\n}\r\nnicvf_sq_add_gather_subdesc(sq, qentry, size, dma_addr);\r\n}\r\ndoorbell:\r\nif (nic->t88 && skb_shinfo(skb)->gso_size) {\r\nqentry = nicvf_get_nxt_sqentry(sq, qentry);\r\nnicvf_sq_add_cqe_subdesc(sq, qentry, hdr_sqe, skb);\r\n}\r\nnicvf_sq_doorbell(nic, skb, sq_num, subdesc_cnt);\r\nreturn 1;\r\nappend_fail:\r\nnic = nic->pnicvf;\r\nnetdev_dbg(nic->netdev, "Not enough SQ descriptors to xmit pkt\n");\r\nreturn 0;\r\n}\r\nstatic inline unsigned frag_num(unsigned i)\r\n{\r\n#ifdef __BIG_ENDIAN\r\nreturn (i & ~3) + 3 - (i & 3);\r\n#else\r\nreturn i;\r\n#endif\r\n}\r\nstatic void nicvf_unmap_rcv_buffer(struct nicvf *nic, u64 dma_addr,\r\nu64 buf_addr, bool xdp)\r\n{\r\nstruct page *page = NULL;\r\nint len = RCV_FRAG_LEN;\r\nif (xdp) {\r\npage = virt_to_page(phys_to_virt(buf_addr));\r\nif (page_ref_count(page) != 1)\r\nreturn;\r\nlen += XDP_PACKET_HEADROOM;\r\ndma_addr &= PAGE_MASK;\r\n}\r\ndma_unmap_page_attrs(&nic->pdev->dev, dma_addr, len,\r\nDMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\r\n}\r\nstruct sk_buff *nicvf_get_rcv_skb(struct nicvf *nic,\r\nstruct cqe_rx_t *cqe_rx, bool xdp)\r\n{\r\nint frag;\r\nint payload_len = 0;\r\nstruct sk_buff *skb = NULL;\r\nstruct page *page;\r\nint offset;\r\nu16 *rb_lens = NULL;\r\nu64 *rb_ptrs = NULL;\r\nu64 phys_addr;\r\nrb_lens = (void *)cqe_rx + (3 * sizeof(u64));\r\nif (!nic->hw_tso)\r\nrb_ptrs = (void *)cqe_rx + (6 * sizeof(u64));\r\nelse\r\nrb_ptrs = (void *)cqe_rx + (7 * sizeof(u64));\r\nfor (frag = 0; frag < cqe_rx->rb_cnt; frag++) {\r\npayload_len = rb_lens[frag_num(frag)];\r\nphys_addr = nicvf_iova_to_phys(nic, *rb_ptrs);\r\nif (!phys_addr) {\r\nif (skb)\r\ndev_kfree_skb_any(skb);\r\nreturn NULL;\r\n}\r\nif (!frag) {\r\nnicvf_unmap_rcv_buffer(nic,\r\n*rb_ptrs - cqe_rx->align_pad,\r\nphys_addr, xdp);\r\nskb = nicvf_rb_ptr_to_skb(nic,\r\nphys_addr - cqe_rx->align_pad,\r\npayload_len);\r\nif (!skb)\r\nreturn NULL;\r\nskb_reserve(skb, cqe_rx->align_pad);\r\nskb_put(skb, payload_len);\r\n} else {\r\nnicvf_unmap_rcv_buffer(nic, *rb_ptrs, phys_addr, xdp);\r\npage = virt_to_page(phys_to_virt(phys_addr));\r\noffset = phys_to_virt(phys_addr) - page_address(page);\r\nskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,\r\noffset, payload_len, RCV_FRAG_LEN);\r\n}\r\nrb_ptrs++;\r\n}\r\nreturn skb;\r\n}\r\nstatic u64 nicvf_int_type_to_mask(int int_type, int q_idx)\r\n{\r\nu64 reg_val;\r\nswitch (int_type) {\r\ncase NICVF_INTR_CQ:\r\nreg_val = ((1ULL << q_idx) << NICVF_INTR_CQ_SHIFT);\r\nbreak;\r\ncase NICVF_INTR_SQ:\r\nreg_val = ((1ULL << q_idx) << NICVF_INTR_SQ_SHIFT);\r\nbreak;\r\ncase NICVF_INTR_RBDR:\r\nreg_val = ((1ULL << q_idx) << NICVF_INTR_RBDR_SHIFT);\r\nbreak;\r\ncase NICVF_INTR_PKT_DROP:\r\nreg_val = (1ULL << NICVF_INTR_PKT_DROP_SHIFT);\r\nbreak;\r\ncase NICVF_INTR_TCP_TIMER:\r\nreg_val = (1ULL << NICVF_INTR_TCP_TIMER_SHIFT);\r\nbreak;\r\ncase NICVF_INTR_MBOX:\r\nreg_val = (1ULL << NICVF_INTR_MBOX_SHIFT);\r\nbreak;\r\ncase NICVF_INTR_QS_ERR:\r\nreg_val = (1ULL << NICVF_INTR_QS_ERR_SHIFT);\r\nbreak;\r\ndefault:\r\nreg_val = 0;\r\n}\r\nreturn reg_val;\r\n}\r\nvoid nicvf_enable_intr(struct nicvf *nic, int int_type, int q_idx)\r\n{\r\nu64 mask = nicvf_int_type_to_mask(int_type, q_idx);\r\nif (!mask) {\r\nnetdev_dbg(nic->netdev,\r\n"Failed to enable interrupt: unknown type\n");\r\nreturn;\r\n}\r\nnicvf_reg_write(nic, NIC_VF_ENA_W1S,\r\nnicvf_reg_read(nic, NIC_VF_ENA_W1S) | mask);\r\n}\r\nvoid nicvf_disable_intr(struct nicvf *nic, int int_type, int q_idx)\r\n{\r\nu64 mask = nicvf_int_type_to_mask(int_type, q_idx);\r\nif (!mask) {\r\nnetdev_dbg(nic->netdev,\r\n"Failed to disable interrupt: unknown type\n");\r\nreturn;\r\n}\r\nnicvf_reg_write(nic, NIC_VF_ENA_W1C, mask);\r\n}\r\nvoid nicvf_clear_intr(struct nicvf *nic, int int_type, int q_idx)\r\n{\r\nu64 mask = nicvf_int_type_to_mask(int_type, q_idx);\r\nif (!mask) {\r\nnetdev_dbg(nic->netdev,\r\n"Failed to clear interrupt: unknown type\n");\r\nreturn;\r\n}\r\nnicvf_reg_write(nic, NIC_VF_INT, mask);\r\n}\r\nint nicvf_is_intr_enabled(struct nicvf *nic, int int_type, int q_idx)\r\n{\r\nu64 mask = nicvf_int_type_to_mask(int_type, q_idx);\r\nif (!mask) {\r\nnetdev_dbg(nic->netdev,\r\n"Failed to check interrupt enable: unknown type\n");\r\nreturn 0;\r\n}\r\nreturn mask & nicvf_reg_read(nic, NIC_VF_ENA_W1S);\r\n}\r\nvoid nicvf_update_rq_stats(struct nicvf *nic, int rq_idx)\r\n{\r\nstruct rcv_queue *rq;\r\n#define GET_RQ_STATS(reg) \\r\nnicvf_reg_read(nic, NIC_QSET_RQ_0_7_STAT_0_1 |\\r\n(rq_idx << NIC_Q_NUM_SHIFT) | (reg << 3))\r\nrq = &nic->qs->rq[rq_idx];\r\nrq->stats.bytes = GET_RQ_STATS(RQ_SQ_STATS_OCTS);\r\nrq->stats.pkts = GET_RQ_STATS(RQ_SQ_STATS_PKTS);\r\n}\r\nvoid nicvf_update_sq_stats(struct nicvf *nic, int sq_idx)\r\n{\r\nstruct snd_queue *sq;\r\n#define GET_SQ_STATS(reg) \\r\nnicvf_reg_read(nic, NIC_QSET_SQ_0_7_STAT_0_1 |\\r\n(sq_idx << NIC_Q_NUM_SHIFT) | (reg << 3))\r\nsq = &nic->qs->sq[sq_idx];\r\nsq->stats.bytes = GET_SQ_STATS(RQ_SQ_STATS_OCTS);\r\nsq->stats.pkts = GET_SQ_STATS(RQ_SQ_STATS_PKTS);\r\n}\r\nint nicvf_check_cqe_rx_errs(struct nicvf *nic, struct cqe_rx_t *cqe_rx)\r\n{\r\nnetif_err(nic, rx_err, nic->netdev,\r\n"RX error CQE err_level 0x%x err_opcode 0x%x\n",\r\ncqe_rx->err_level, cqe_rx->err_opcode);\r\nswitch (cqe_rx->err_opcode) {\r\ncase CQ_RX_ERROP_RE_PARTIAL:\r\nthis_cpu_inc(nic->drv_stats->rx_bgx_truncated_pkts);\r\nbreak;\r\ncase CQ_RX_ERROP_RE_JABBER:\r\nthis_cpu_inc(nic->drv_stats->rx_jabber_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_RE_FCS:\r\nthis_cpu_inc(nic->drv_stats->rx_fcs_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_RE_RX_CTL:\r\nthis_cpu_inc(nic->drv_stats->rx_bgx_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_PREL2_ERR:\r\nthis_cpu_inc(nic->drv_stats->rx_prel2_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_L2_MAL:\r\nthis_cpu_inc(nic->drv_stats->rx_l2_hdr_malformed);\r\nbreak;\r\ncase CQ_RX_ERROP_L2_OVERSIZE:\r\nthis_cpu_inc(nic->drv_stats->rx_oversize);\r\nbreak;\r\ncase CQ_RX_ERROP_L2_UNDERSIZE:\r\nthis_cpu_inc(nic->drv_stats->rx_undersize);\r\nbreak;\r\ncase CQ_RX_ERROP_L2_LENMISM:\r\nthis_cpu_inc(nic->drv_stats->rx_l2_len_mismatch);\r\nbreak;\r\ncase CQ_RX_ERROP_L2_PCLP:\r\nthis_cpu_inc(nic->drv_stats->rx_l2_pclp);\r\nbreak;\r\ncase CQ_RX_ERROP_IP_NOT:\r\nthis_cpu_inc(nic->drv_stats->rx_ip_ver_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_IP_CSUM_ERR:\r\nthis_cpu_inc(nic->drv_stats->rx_ip_csum_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_IP_MAL:\r\nthis_cpu_inc(nic->drv_stats->rx_ip_hdr_malformed);\r\nbreak;\r\ncase CQ_RX_ERROP_IP_MALD:\r\nthis_cpu_inc(nic->drv_stats->rx_ip_payload_malformed);\r\nbreak;\r\ncase CQ_RX_ERROP_IP_HOP:\r\nthis_cpu_inc(nic->drv_stats->rx_ip_ttl_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_L3_PCLP:\r\nthis_cpu_inc(nic->drv_stats->rx_l3_pclp);\r\nbreak;\r\ncase CQ_RX_ERROP_L4_MAL:\r\nthis_cpu_inc(nic->drv_stats->rx_l4_malformed);\r\nbreak;\r\ncase CQ_RX_ERROP_L4_CHK:\r\nthis_cpu_inc(nic->drv_stats->rx_l4_csum_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_UDP_LEN:\r\nthis_cpu_inc(nic->drv_stats->rx_udp_len_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_L4_PORT:\r\nthis_cpu_inc(nic->drv_stats->rx_l4_port_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_TCP_FLAG:\r\nthis_cpu_inc(nic->drv_stats->rx_tcp_flag_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_TCP_OFFSET:\r\nthis_cpu_inc(nic->drv_stats->rx_tcp_offset_errs);\r\nbreak;\r\ncase CQ_RX_ERROP_L4_PCLP:\r\nthis_cpu_inc(nic->drv_stats->rx_l4_pclp);\r\nbreak;\r\ncase CQ_RX_ERROP_RBDR_TRUNC:\r\nthis_cpu_inc(nic->drv_stats->rx_truncated_pkts);\r\nbreak;\r\n}\r\nreturn 1;\r\n}\r\nint nicvf_check_cqe_tx_errs(struct nicvf *nic, struct cqe_send_t *cqe_tx)\r\n{\r\nswitch (cqe_tx->send_status) {\r\ncase CQ_TX_ERROP_DESC_FAULT:\r\nthis_cpu_inc(nic->drv_stats->tx_desc_fault);\r\nbreak;\r\ncase CQ_TX_ERROP_HDR_CONS_ERR:\r\nthis_cpu_inc(nic->drv_stats->tx_hdr_cons_err);\r\nbreak;\r\ncase CQ_TX_ERROP_SUBDC_ERR:\r\nthis_cpu_inc(nic->drv_stats->tx_subdesc_err);\r\nbreak;\r\ncase CQ_TX_ERROP_MAX_SIZE_VIOL:\r\nthis_cpu_inc(nic->drv_stats->tx_max_size_exceeded);\r\nbreak;\r\ncase CQ_TX_ERROP_IMM_SIZE_OFLOW:\r\nthis_cpu_inc(nic->drv_stats->tx_imm_size_oflow);\r\nbreak;\r\ncase CQ_TX_ERROP_DATA_SEQUENCE_ERR:\r\nthis_cpu_inc(nic->drv_stats->tx_data_seq_err);\r\nbreak;\r\ncase CQ_TX_ERROP_MEM_SEQUENCE_ERR:\r\nthis_cpu_inc(nic->drv_stats->tx_mem_seq_err);\r\nbreak;\r\ncase CQ_TX_ERROP_LOCK_VIOL:\r\nthis_cpu_inc(nic->drv_stats->tx_lock_viol);\r\nbreak;\r\ncase CQ_TX_ERROP_DATA_FAULT:\r\nthis_cpu_inc(nic->drv_stats->tx_data_fault);\r\nbreak;\r\ncase CQ_TX_ERROP_TSTMP_CONFLICT:\r\nthis_cpu_inc(nic->drv_stats->tx_tstmp_conflict);\r\nbreak;\r\ncase CQ_TX_ERROP_TSTMP_TIMEOUT:\r\nthis_cpu_inc(nic->drv_stats->tx_tstmp_timeout);\r\nbreak;\r\ncase CQ_TX_ERROP_MEM_FAULT:\r\nthis_cpu_inc(nic->drv_stats->tx_mem_fault);\r\nbreak;\r\ncase CQ_TX_ERROP_CK_OVERLAP:\r\nthis_cpu_inc(nic->drv_stats->tx_csum_overlap);\r\nbreak;\r\ncase CQ_TX_ERROP_CK_OFLOW:\r\nthis_cpu_inc(nic->drv_stats->tx_csum_overflow);\r\nbreak;\r\n}\r\nreturn 1;\r\n}
