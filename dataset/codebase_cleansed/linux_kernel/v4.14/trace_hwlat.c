static void trace_hwlat_sample(struct hwlat_sample *sample)\r\n{\r\nstruct trace_array *tr = hwlat_trace;\r\nstruct trace_event_call *call = &event_hwlat;\r\nstruct ring_buffer *buffer = tr->trace_buffer.buffer;\r\nstruct ring_buffer_event *event;\r\nstruct hwlat_entry *entry;\r\nunsigned long flags;\r\nint pc;\r\npc = preempt_count();\r\nlocal_save_flags(flags);\r\nevent = trace_buffer_lock_reserve(buffer, TRACE_HWLAT, sizeof(*entry),\r\nflags, pc);\r\nif (!event)\r\nreturn;\r\nentry = ring_buffer_event_data(event);\r\nentry->seqnum = sample->seqnum;\r\nentry->duration = sample->duration;\r\nentry->outer_duration = sample->outer_duration;\r\nentry->timestamp = sample->timestamp;\r\nentry->nmi_total_ts = sample->nmi_total_ts;\r\nentry->nmi_count = sample->nmi_count;\r\nif (!call_filter_check_discard(call, entry, buffer, event))\r\ntrace_buffer_unlock_commit_nostack(buffer, event);\r\n}\r\nvoid trace_hwlat_callback(bool enter)\r\n{\r\nif (smp_processor_id() != nmi_cpu)\r\nreturn;\r\nif (!IS_ENABLED(CONFIG_GENERIC_SCHED_CLOCK)) {\r\nif (enter)\r\nnmi_ts_start = time_get();\r\nelse\r\nnmi_total_ts = time_get() - nmi_ts_start;\r\n}\r\nif (enter)\r\nnmi_count++;\r\n}\r\nstatic int get_sample(void)\r\n{\r\nstruct trace_array *tr = hwlat_trace;\r\ntime_type start, t1, t2, last_t2;\r\ns64 diff, total, last_total = 0;\r\nu64 sample = 0;\r\nu64 thresh = tracing_thresh;\r\nu64 outer_sample = 0;\r\nint ret = -1;\r\ndo_div(thresh, NSEC_PER_USEC);\r\nnmi_cpu = smp_processor_id();\r\nnmi_total_ts = 0;\r\nnmi_count = 0;\r\nbarrier();\r\ntrace_hwlat_callback_enabled = true;\r\ninit_time(last_t2, 0);\r\nstart = time_get();\r\ndo {\r\nt1 = time_get();\r\nt2 = time_get();\r\nif (time_u64(last_t2)) {\r\ndiff = time_to_us(time_sub(t1, last_t2));\r\nif (diff < 0) {\r\npr_err(BANNER "time running backwards\n");\r\ngoto out;\r\n}\r\nif (diff > outer_sample)\r\nouter_sample = diff;\r\n}\r\nlast_t2 = t2;\r\ntotal = time_to_us(time_sub(t2, start));\r\nif (total < last_total) {\r\npr_err("Time total overflowed\n");\r\nbreak;\r\n}\r\nlast_total = total;\r\ndiff = time_to_us(time_sub(t2, t1));\r\nif (diff < 0) {\r\npr_err(BANNER "time running backwards\n");\r\ngoto out;\r\n}\r\nif (diff > sample)\r\nsample = diff;\r\n} while (total <= hwlat_data.sample_width);\r\nbarrier();\r\ntrace_hwlat_callback_enabled = false;\r\nbarrier();\r\nret = 0;\r\nif (sample > thresh || outer_sample > thresh) {\r\nstruct hwlat_sample s;\r\nret = 1;\r\nif (nmi_total_ts)\r\ndo_div(nmi_total_ts, NSEC_PER_USEC);\r\nhwlat_data.count++;\r\ns.seqnum = hwlat_data.count;\r\ns.duration = sample;\r\ns.outer_duration = outer_sample;\r\nktime_get_real_ts64(&s.timestamp);\r\ns.nmi_total_ts = nmi_total_ts;\r\ns.nmi_count = nmi_count;\r\ntrace_hwlat_sample(&s);\r\nif (sample > tr->max_latency)\r\ntr->max_latency = sample;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic void move_to_next_cpu(void)\r\n{\r\nstruct cpumask *current_mask = &save_cpumask;\r\nint next_cpu;\r\nif (disable_migrate)\r\nreturn;\r\nif (!cpumask_equal(current_mask, &current->cpus_allowed))\r\ngoto disable;\r\nget_online_cpus();\r\ncpumask_and(current_mask, cpu_online_mask, tracing_buffer_mask);\r\nnext_cpu = cpumask_next(smp_processor_id(), current_mask);\r\nput_online_cpus();\r\nif (next_cpu >= nr_cpu_ids)\r\nnext_cpu = cpumask_first(current_mask);\r\nif (next_cpu >= nr_cpu_ids)\r\ngoto disable;\r\ncpumask_clear(current_mask);\r\ncpumask_set_cpu(next_cpu, current_mask);\r\nsched_setaffinity(0, current_mask);\r\nreturn;\r\ndisable:\r\ndisable_migrate = true;\r\n}\r\nstatic int kthread_fn(void *data)\r\n{\r\nu64 interval;\r\nwhile (!kthread_should_stop()) {\r\nmove_to_next_cpu();\r\nlocal_irq_disable();\r\nget_sample();\r\nlocal_irq_enable();\r\nmutex_lock(&hwlat_data.lock);\r\ninterval = hwlat_data.sample_window - hwlat_data.sample_width;\r\nmutex_unlock(&hwlat_data.lock);\r\ndo_div(interval, USEC_PER_MSEC);\r\nif (interval < 1)\r\ninterval = 1;\r\nif (msleep_interruptible(interval))\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int start_kthread(struct trace_array *tr)\r\n{\r\nstruct cpumask *current_mask = &save_cpumask;\r\nstruct task_struct *kthread;\r\nint next_cpu;\r\ncurrent_mask = &save_cpumask;\r\nget_online_cpus();\r\ncpumask_and(current_mask, cpu_online_mask, tracing_buffer_mask);\r\nput_online_cpus();\r\nnext_cpu = cpumask_first(current_mask);\r\nkthread = kthread_create(kthread_fn, NULL, "hwlatd");\r\nif (IS_ERR(kthread)) {\r\npr_err(BANNER "could not start sampling thread\n");\r\nreturn -ENOMEM;\r\n}\r\ncpumask_clear(current_mask);\r\ncpumask_set_cpu(next_cpu, current_mask);\r\nsched_setaffinity(kthread->pid, current_mask);\r\nhwlat_kthread = kthread;\r\nwake_up_process(kthread);\r\nreturn 0;\r\n}\r\nstatic void stop_kthread(void)\r\n{\r\nif (!hwlat_kthread)\r\nreturn;\r\nkthread_stop(hwlat_kthread);\r\nhwlat_kthread = NULL;\r\n}\r\nstatic ssize_t hwlat_read(struct file *filp, char __user *ubuf,\r\nsize_t cnt, loff_t *ppos)\r\n{\r\nchar buf[U64STR_SIZE];\r\nu64 *entry = filp->private_data;\r\nu64 val;\r\nint len;\r\nif (!entry)\r\nreturn -EFAULT;\r\nif (cnt > sizeof(buf))\r\ncnt = sizeof(buf);\r\nval = *entry;\r\nlen = snprintf(buf, sizeof(buf), "%llu\n", val);\r\nreturn simple_read_from_buffer(ubuf, cnt, ppos, buf, len);\r\n}\r\nstatic ssize_t\r\nhwlat_width_write(struct file *filp, const char __user *ubuf,\r\nsize_t cnt, loff_t *ppos)\r\n{\r\nu64 val;\r\nint err;\r\nerr = kstrtoull_from_user(ubuf, cnt, 10, &val);\r\nif (err)\r\nreturn err;\r\nmutex_lock(&hwlat_data.lock);\r\nif (val < hwlat_data.sample_window)\r\nhwlat_data.sample_width = val;\r\nelse\r\nerr = -EINVAL;\r\nmutex_unlock(&hwlat_data.lock);\r\nif (err)\r\nreturn err;\r\nreturn cnt;\r\n}\r\nstatic ssize_t\r\nhwlat_window_write(struct file *filp, const char __user *ubuf,\r\nsize_t cnt, loff_t *ppos)\r\n{\r\nu64 val;\r\nint err;\r\nerr = kstrtoull_from_user(ubuf, cnt, 10, &val);\r\nif (err)\r\nreturn err;\r\nmutex_lock(&hwlat_data.lock);\r\nif (hwlat_data.sample_width < val)\r\nhwlat_data.sample_window = val;\r\nelse\r\nerr = -EINVAL;\r\nmutex_unlock(&hwlat_data.lock);\r\nif (err)\r\nreturn err;\r\nreturn cnt;\r\n}\r\nstatic int init_tracefs(void)\r\n{\r\nstruct dentry *d_tracer;\r\nstruct dentry *top_dir;\r\nd_tracer = tracing_init_dentry();\r\nif (IS_ERR(d_tracer))\r\nreturn -ENOMEM;\r\ntop_dir = tracefs_create_dir("hwlat_detector", d_tracer);\r\nif (!top_dir)\r\nreturn -ENOMEM;\r\nhwlat_sample_window = tracefs_create_file("window", 0640,\r\ntop_dir,\r\n&hwlat_data.sample_window,\r\n&window_fops);\r\nif (!hwlat_sample_window)\r\ngoto err;\r\nhwlat_sample_width = tracefs_create_file("width", 0644,\r\ntop_dir,\r\n&hwlat_data.sample_width,\r\n&width_fops);\r\nif (!hwlat_sample_width)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\ntracefs_remove_recursive(top_dir);\r\nreturn -ENOMEM;\r\n}\r\nstatic void hwlat_tracer_start(struct trace_array *tr)\r\n{\r\nint err;\r\nerr = start_kthread(tr);\r\nif (err)\r\npr_err(BANNER "Cannot start hwlat kthread\n");\r\n}\r\nstatic void hwlat_tracer_stop(struct trace_array *tr)\r\n{\r\nstop_kthread();\r\n}\r\nstatic int hwlat_tracer_init(struct trace_array *tr)\r\n{\r\nif (hwlat_busy)\r\nreturn -EBUSY;\r\nhwlat_trace = tr;\r\ndisable_migrate = false;\r\nhwlat_data.count = 0;\r\ntr->max_latency = 0;\r\nsave_tracing_thresh = tracing_thresh;\r\nif (!tracing_thresh)\r\ntracing_thresh = last_tracing_thresh;\r\nif (tracer_tracing_is_on(tr))\r\nhwlat_tracer_start(tr);\r\nhwlat_busy = true;\r\nreturn 0;\r\n}\r\nstatic void hwlat_tracer_reset(struct trace_array *tr)\r\n{\r\nstop_kthread();\r\nlast_tracing_thresh = tracing_thresh;\r\ntracing_thresh = save_tracing_thresh;\r\nhwlat_busy = false;\r\n}\r\n__init static int init_hwlat_tracer(void)\r\n{\r\nint ret;\r\nmutex_init(&hwlat_data.lock);\r\nret = register_tracer(&hwlat_tracer);\r\nif (ret)\r\nreturn ret;\r\ninit_tracefs();\r\nreturn 0;\r\n}
