void pblk_rb_data_free(struct pblk_rb *rb)\r\n{\r\nstruct pblk_rb_pages *p, *t;\r\ndown_write(&pblk_rb_lock);\r\nlist_for_each_entry_safe(p, t, &rb->pages, list) {\r\nfree_pages((unsigned long)page_address(p->pages), p->order);\r\nlist_del(&p->list);\r\nkfree(p);\r\n}\r\nup_write(&pblk_rb_lock);\r\n}\r\nint pblk_rb_init(struct pblk_rb *rb, struct pblk_rb_entry *rb_entry_base,\r\nunsigned int power_size, unsigned int power_seg_sz)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nunsigned int init_entry = 0;\r\nunsigned int alloc_order = power_size;\r\nunsigned int max_order = MAX_ORDER - 1;\r\nunsigned int order, iter;\r\ndown_write(&pblk_rb_lock);\r\nrb->entries = rb_entry_base;\r\nrb->seg_size = (1 << power_seg_sz);\r\nrb->nr_entries = (1 << power_size);\r\nrb->mem = rb->subm = rb->sync = rb->l2p_update = 0;\r\nrb->sync_point = EMPTY_ENTRY;\r\nspin_lock_init(&rb->w_lock);\r\nspin_lock_init(&rb->s_lock);\r\nINIT_LIST_HEAD(&rb->pages);\r\nif (alloc_order >= max_order) {\r\norder = max_order;\r\niter = (1 << (alloc_order - max_order));\r\n} else {\r\norder = alloc_order;\r\niter = 1;\r\n}\r\ndo {\r\nstruct pblk_rb_entry *entry;\r\nstruct pblk_rb_pages *page_set;\r\nvoid *kaddr;\r\nunsigned long set_size;\r\nint i;\r\npage_set = kmalloc(sizeof(struct pblk_rb_pages), GFP_KERNEL);\r\nif (!page_set) {\r\nup_write(&pblk_rb_lock);\r\nreturn -ENOMEM;\r\n}\r\npage_set->order = order;\r\npage_set->pages = alloc_pages(GFP_KERNEL, order);\r\nif (!page_set->pages) {\r\nkfree(page_set);\r\npblk_rb_data_free(rb);\r\nup_write(&pblk_rb_lock);\r\nreturn -ENOMEM;\r\n}\r\nkaddr = page_address(page_set->pages);\r\nentry = &rb->entries[init_entry];\r\nentry->data = kaddr;\r\nentry->cacheline = pblk_cacheline_to_addr(init_entry++);\r\nentry->w_ctx.flags = PBLK_WRITABLE_ENTRY;\r\nset_size = (1 << order);\r\nfor (i = 1; i < set_size; i++) {\r\nentry = &rb->entries[init_entry];\r\nentry->cacheline = pblk_cacheline_to_addr(init_entry++);\r\nentry->data = kaddr + (i * rb->seg_size);\r\nentry->w_ctx.flags = PBLK_WRITABLE_ENTRY;\r\nbio_list_init(&entry->w_ctx.bios);\r\n}\r\nlist_add_tail(&page_set->list, &rb->pages);\r\niter--;\r\n} while (iter > 0);\r\nup_write(&pblk_rb_lock);\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_set(&rb->inflight_sync_point, 0);\r\n#endif\r\npblk_rl_init(&pblk->rl, rb->nr_entries);\r\nreturn 0;\r\n}\r\nunsigned int pblk_rb_calculate_size(unsigned int nr_entries)\r\n{\r\nreturn (1 << max(get_count_order(nr_entries), 7));\r\n}\r\nvoid *pblk_rb_entries_ref(struct pblk_rb *rb)\r\n{\r\nreturn rb->entries;\r\n}\r\nstatic void clean_wctx(struct pblk_w_ctx *w_ctx)\r\n{\r\nint flags;\r\ntry:\r\nflags = READ_ONCE(w_ctx->flags);\r\nif (!(flags & PBLK_SUBMITTED_ENTRY))\r\ngoto try;\r\nsmp_store_release(&w_ctx->flags, PBLK_WRITABLE_ENTRY);\r\npblk_ppa_set_empty(&w_ctx->ppa);\r\nw_ctx->lba = ADDR_EMPTY;\r\n}\r\nstatic unsigned int pblk_rb_space(struct pblk_rb *rb)\r\n{\r\nunsigned int mem = READ_ONCE(rb->mem);\r\nunsigned int sync = READ_ONCE(rb->sync);\r\nreturn pblk_rb_ring_space(rb, mem, sync, rb->nr_entries);\r\n}\r\nunsigned int pblk_rb_read_count(struct pblk_rb *rb)\r\n{\r\nunsigned int mem = READ_ONCE(rb->mem);\r\nunsigned int subm = READ_ONCE(rb->subm);\r\nreturn pblk_rb_ring_count(mem, subm, rb->nr_entries);\r\n}\r\nunsigned int pblk_rb_sync_count(struct pblk_rb *rb)\r\n{\r\nunsigned int mem = READ_ONCE(rb->mem);\r\nunsigned int sync = READ_ONCE(rb->sync);\r\nreturn pblk_rb_ring_count(mem, sync, rb->nr_entries);\r\n}\r\nunsigned int pblk_rb_read_commit(struct pblk_rb *rb, unsigned int nr_entries)\r\n{\r\nunsigned int subm;\r\nsubm = READ_ONCE(rb->subm);\r\nsmp_store_release(&rb->subm,\r\n(subm + nr_entries) & (rb->nr_entries - 1));\r\nreturn subm;\r\n}\r\nstatic int __pblk_rb_update_l2p(struct pblk_rb *rb, unsigned int *l2p_upd,\r\nunsigned int to_update)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nstruct pblk_line *line;\r\nstruct pblk_rb_entry *entry;\r\nstruct pblk_w_ctx *w_ctx;\r\nunsigned int user_io = 0, gc_io = 0;\r\nunsigned int i;\r\nint flags;\r\nfor (i = 0; i < to_update; i++) {\r\nentry = &rb->entries[*l2p_upd];\r\nw_ctx = &entry->w_ctx;\r\nflags = READ_ONCE(entry->w_ctx.flags);\r\nif (flags & PBLK_IOTYPE_USER)\r\nuser_io++;\r\nelse if (flags & PBLK_IOTYPE_GC)\r\ngc_io++;\r\nelse\r\nWARN(1, "pblk: unknown IO type\n");\r\npblk_update_map_dev(pblk, w_ctx->lba, w_ctx->ppa,\r\nentry->cacheline);\r\nline = &pblk->lines[pblk_tgt_ppa_to_line(w_ctx->ppa)];\r\nkref_put(&line->ref, pblk_line_put);\r\nclean_wctx(w_ctx);\r\n*l2p_upd = (*l2p_upd + 1) & (rb->nr_entries - 1);\r\n}\r\npblk_rl_out(&pblk->rl, user_io, gc_io);\r\nreturn 0;\r\n}\r\nstatic int pblk_rb_update_l2p(struct pblk_rb *rb, unsigned int nr_entries,\r\nunsigned int mem, unsigned int sync)\r\n{\r\nunsigned int space, count;\r\nint ret = 0;\r\nlockdep_assert_held(&rb->w_lock);\r\nspace = pblk_rb_ring_space(rb, mem, rb->l2p_update, rb->nr_entries);\r\nif (space > nr_entries)\r\ngoto out;\r\ncount = nr_entries - space;\r\nret = __pblk_rb_update_l2p(rb, &rb->l2p_update, count);\r\nout:\r\nreturn ret;\r\n}\r\nvoid pblk_rb_sync_l2p(struct pblk_rb *rb)\r\n{\r\nunsigned int sync;\r\nunsigned int to_update;\r\nspin_lock(&rb->w_lock);\r\nsync = smp_load_acquire(&rb->sync);\r\nto_update = pblk_rb_ring_count(sync, rb->l2p_update, rb->nr_entries);\r\n__pblk_rb_update_l2p(rb, &rb->l2p_update, to_update);\r\nspin_unlock(&rb->w_lock);\r\n}\r\nstatic void __pblk_rb_write_entry(struct pblk_rb *rb, void *data,\r\nstruct pblk_w_ctx w_ctx,\r\nstruct pblk_rb_entry *entry)\r\n{\r\nmemcpy(entry->data, data, rb->seg_size);\r\nentry->w_ctx.lba = w_ctx.lba;\r\nentry->w_ctx.ppa = w_ctx.ppa;\r\n}\r\nvoid pblk_rb_write_entry_user(struct pblk_rb *rb, void *data,\r\nstruct pblk_w_ctx w_ctx, unsigned int ring_pos)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nstruct pblk_rb_entry *entry;\r\nint flags;\r\nentry = &rb->entries[ring_pos];\r\nflags = READ_ONCE(entry->w_ctx.flags);\r\n#ifdef CONFIG_NVM_DEBUG\r\nBUG_ON(!(flags & PBLK_WRITABLE_ENTRY));\r\n#endif\r\n__pblk_rb_write_entry(rb, data, w_ctx, entry);\r\npblk_update_map_cache(pblk, w_ctx.lba, entry->cacheline);\r\nflags = w_ctx.flags | PBLK_WRITTEN_DATA;\r\nsmp_store_release(&entry->w_ctx.flags, flags);\r\n}\r\nvoid pblk_rb_write_entry_gc(struct pblk_rb *rb, void *data,\r\nstruct pblk_w_ctx w_ctx, struct pblk_line *gc_line,\r\nunsigned int ring_pos)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nstruct pblk_rb_entry *entry;\r\nint flags;\r\nentry = &rb->entries[ring_pos];\r\nflags = READ_ONCE(entry->w_ctx.flags);\r\n#ifdef CONFIG_NVM_DEBUG\r\nBUG_ON(!(flags & PBLK_WRITABLE_ENTRY));\r\n#endif\r\n__pblk_rb_write_entry(rb, data, w_ctx, entry);\r\nif (!pblk_update_map_gc(pblk, w_ctx.lba, entry->cacheline, gc_line))\r\nentry->w_ctx.lba = ADDR_EMPTY;\r\nflags = w_ctx.flags | PBLK_WRITTEN_DATA;\r\nsmp_store_release(&entry->w_ctx.flags, flags);\r\n}\r\nstatic int pblk_rb_sync_point_set(struct pblk_rb *rb, struct bio *bio,\r\nunsigned int pos)\r\n{\r\nstruct pblk_rb_entry *entry;\r\nunsigned int subm, sync_point;\r\nint flags;\r\nsubm = READ_ONCE(rb->subm);\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_inc(&rb->inflight_sync_point);\r\n#endif\r\nif (pos == subm)\r\nreturn 0;\r\nsync_point = (pos == 0) ? (rb->nr_entries - 1) : (pos - 1);\r\nentry = &rb->entries[sync_point];\r\nflags = READ_ONCE(entry->w_ctx.flags);\r\nflags |= PBLK_FLUSH_ENTRY;\r\nsmp_store_release(&entry->w_ctx.flags, flags);\r\nsmp_store_release(&rb->sync_point, sync_point);\r\nif (!bio)\r\nreturn 0;\r\nspin_lock_irq(&rb->s_lock);\r\nbio_list_add(&entry->w_ctx.bios, bio);\r\nspin_unlock_irq(&rb->s_lock);\r\nreturn 1;\r\n}\r\nstatic int __pblk_rb_may_write(struct pblk_rb *rb, unsigned int nr_entries,\r\nunsigned int *pos)\r\n{\r\nunsigned int mem;\r\nunsigned int sync;\r\nsync = READ_ONCE(rb->sync);\r\nmem = READ_ONCE(rb->mem);\r\nif (pblk_rb_ring_space(rb, mem, sync, rb->nr_entries) < nr_entries)\r\nreturn 0;\r\nif (pblk_rb_update_l2p(rb, nr_entries, mem, sync))\r\nreturn 0;\r\n*pos = mem;\r\nreturn 1;\r\n}\r\nstatic int pblk_rb_may_write(struct pblk_rb *rb, unsigned int nr_entries,\r\nunsigned int *pos)\r\n{\r\nif (!__pblk_rb_may_write(rb, nr_entries, pos))\r\nreturn 0;\r\nsmp_store_release(&rb->mem, (*pos + nr_entries) & (rb->nr_entries - 1));\r\nreturn 1;\r\n}\r\nvoid pblk_rb_flush(struct pblk_rb *rb)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nunsigned int mem = READ_ONCE(rb->mem);\r\nif (pblk_rb_sync_point_set(rb, NULL, mem))\r\nreturn;\r\npblk_write_should_kick(pblk);\r\n}\r\nstatic int pblk_rb_may_write_flush(struct pblk_rb *rb, unsigned int nr_entries,\r\nunsigned int *pos, struct bio *bio,\r\nint *io_ret)\r\n{\r\nunsigned int mem;\r\nif (!__pblk_rb_may_write(rb, nr_entries, pos))\r\nreturn 0;\r\nmem = (*pos + nr_entries) & (rb->nr_entries - 1);\r\n*io_ret = NVM_IO_DONE;\r\nif (bio->bi_opf & REQ_PREFLUSH) {\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_inc(&pblk->nr_flush);\r\n#endif\r\nif (pblk_rb_sync_point_set(&pblk->rwb, bio, mem))\r\n*io_ret = NVM_IO_OK;\r\n}\r\nsmp_store_release(&rb->mem, mem);\r\nreturn 1;\r\n}\r\nint pblk_rb_may_write_user(struct pblk_rb *rb, struct bio *bio,\r\nunsigned int nr_entries, unsigned int *pos)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nint io_ret;\r\nspin_lock(&rb->w_lock);\r\nio_ret = pblk_rl_user_may_insert(&pblk->rl, nr_entries);\r\nif (io_ret) {\r\nspin_unlock(&rb->w_lock);\r\nreturn io_ret;\r\n}\r\nif (!pblk_rb_may_write_flush(rb, nr_entries, pos, bio, &io_ret)) {\r\nspin_unlock(&rb->w_lock);\r\nreturn NVM_IO_REQUEUE;\r\n}\r\npblk_rl_user_in(&pblk->rl, nr_entries);\r\nspin_unlock(&rb->w_lock);\r\nreturn io_ret;\r\n}\r\nint pblk_rb_may_write_gc(struct pblk_rb *rb, unsigned int nr_entries,\r\nunsigned int *pos)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nspin_lock(&rb->w_lock);\r\nif (!pblk_rl_gc_may_insert(&pblk->rl, nr_entries)) {\r\nspin_unlock(&rb->w_lock);\r\nreturn 0;\r\n}\r\nif (!pblk_rb_may_write(rb, nr_entries, pos)) {\r\nspin_unlock(&rb->w_lock);\r\nreturn 0;\r\n}\r\npblk_rl_gc_in(&pblk->rl, nr_entries);\r\nspin_unlock(&rb->w_lock);\r\nreturn 1;\r\n}\r\nunsigned int pblk_rb_read_to_bio_list(struct pblk_rb *rb, struct bio *bio,\r\nstruct list_head *list,\r\nunsigned int max)\r\n{\r\nstruct pblk_rb_entry *entry, *tentry;\r\nstruct page *page;\r\nunsigned int read = 0;\r\nint ret;\r\nlist_for_each_entry_safe(entry, tentry, list, index) {\r\nif (read > max) {\r\npr_err("pblk: too many entries on list\n");\r\ngoto out;\r\n}\r\npage = virt_to_page(entry->data);\r\nif (!page) {\r\npr_err("pblk: could not allocate write bio page\n");\r\ngoto out;\r\n}\r\nret = bio_add_page(bio, page, rb->seg_size, 0);\r\nif (ret != rb->seg_size) {\r\npr_err("pblk: could not add page to write bio\n");\r\ngoto out;\r\n}\r\nlist_del(&entry->index);\r\nread++;\r\n}\r\nout:\r\nreturn read;\r\n}\r\nunsigned int pblk_rb_read_to_bio(struct pblk_rb *rb, struct nvm_rq *rqd,\r\nstruct bio *bio, unsigned int pos,\r\nunsigned int nr_entries, unsigned int count)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nstruct request_queue *q = pblk->dev->q;\r\nstruct pblk_c_ctx *c_ctx = nvm_rq_to_pdu(rqd);\r\nstruct pblk_rb_entry *entry;\r\nstruct page *page;\r\nunsigned int pad = 0, to_read = nr_entries;\r\nunsigned int i;\r\nint flags;\r\nif (count < nr_entries) {\r\npad = nr_entries - count;\r\nto_read = count;\r\n}\r\nc_ctx->sentry = pos;\r\nc_ctx->nr_valid = to_read;\r\nc_ctx->nr_padded = pad;\r\nfor (i = 0; i < to_read; i++) {\r\nentry = &rb->entries[pos];\r\ntry:\r\nflags = READ_ONCE(entry->w_ctx.flags);\r\nif (!(flags & PBLK_WRITTEN_DATA)) {\r\nio_schedule();\r\ngoto try;\r\n}\r\npage = virt_to_page(entry->data);\r\nif (!page) {\r\npr_err("pblk: could not allocate write bio page\n");\r\nflags &= ~PBLK_WRITTEN_DATA;\r\nflags |= PBLK_SUBMITTED_ENTRY;\r\nsmp_store_release(&entry->w_ctx.flags, flags);\r\nreturn NVM_IO_ERR;\r\n}\r\nif (bio_add_pc_page(q, bio, page, rb->seg_size, 0) !=\r\nrb->seg_size) {\r\npr_err("pblk: could not add page to write bio\n");\r\nflags &= ~PBLK_WRITTEN_DATA;\r\nflags |= PBLK_SUBMITTED_ENTRY;\r\nsmp_store_release(&entry->w_ctx.flags, flags);\r\nreturn NVM_IO_ERR;\r\n}\r\nif (flags & PBLK_FLUSH_ENTRY) {\r\nunsigned int sync_point;\r\nsync_point = READ_ONCE(rb->sync_point);\r\nif (sync_point == pos) {\r\nsmp_store_release(&rb->sync_point, EMPTY_ENTRY);\r\n}\r\nflags &= ~PBLK_FLUSH_ENTRY;\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_dec(&rb->inflight_sync_point);\r\n#endif\r\n}\r\nflags &= ~PBLK_WRITTEN_DATA;\r\nflags |= PBLK_SUBMITTED_ENTRY;\r\nsmp_store_release(&entry->w_ctx.flags, flags);\r\npos = (pos + 1) & (rb->nr_entries - 1);\r\n}\r\nif (pad) {\r\nif (pblk_bio_add_pages(pblk, bio, GFP_KERNEL, pad)) {\r\npr_err("pblk: could not pad page in write bio\n");\r\nreturn NVM_IO_ERR;\r\n}\r\n}\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_add(pad, &((struct pblk *)\r\n(container_of(rb, struct pblk, rwb)))->padded_writes);\r\n#endif\r\nreturn NVM_IO_OK;\r\n}\r\nint pblk_rb_copy_to_bio(struct pblk_rb *rb, struct bio *bio, sector_t lba,\r\nstruct ppa_addr ppa, int bio_iter, bool advanced_bio)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nstruct pblk_rb_entry *entry;\r\nstruct pblk_w_ctx *w_ctx;\r\nstruct ppa_addr l2p_ppa;\r\nu64 pos = pblk_addr_to_cacheline(ppa);\r\nvoid *data;\r\nint flags;\r\nint ret = 1;\r\n#ifdef CONFIG_NVM_DEBUG\r\nBUG_ON(pos >= rb->nr_entries);\r\n#endif\r\nentry = &rb->entries[pos];\r\nw_ctx = &entry->w_ctx;\r\nflags = READ_ONCE(w_ctx->flags);\r\nspin_lock(&rb->w_lock);\r\nspin_lock(&pblk->trans_lock);\r\nl2p_ppa = pblk_trans_map_get(pblk, lba);\r\nspin_unlock(&pblk->trans_lock);\r\nif (!pblk_ppa_comp(l2p_ppa, ppa) || w_ctx->lba != lba ||\r\nflags & PBLK_WRITABLE_ENTRY) {\r\nret = 0;\r\ngoto out;\r\n}\r\nif (unlikely(!advanced_bio))\r\nbio_advance(bio, bio_iter * PBLK_EXPOSED_PAGE_SIZE);\r\ndata = bio_data(bio);\r\nmemcpy(data, entry->data, rb->seg_size);\r\nout:\r\nspin_unlock(&rb->w_lock);\r\nreturn ret;\r\n}\r\nstruct pblk_w_ctx *pblk_rb_w_ctx(struct pblk_rb *rb, unsigned int pos)\r\n{\r\nunsigned int entry = pos & (rb->nr_entries - 1);\r\nreturn &rb->entries[entry].w_ctx;\r\n}\r\nunsigned int pblk_rb_sync_init(struct pblk_rb *rb, unsigned long *flags)\r\n__acquires(&rb->s_lock\r\nvoid pblk_rb_sync_end(struct pblk_rb *rb, unsigned long *flags)\r\n__releases(&rb->s_lock\r\nunsigned int pblk_rb_sync_advance(struct pblk_rb *rb, unsigned int nr_entries)\r\n{\r\nunsigned int sync;\r\nunsigned int i;\r\nlockdep_assert_held(&rb->s_lock);\r\nsync = READ_ONCE(rb->sync);\r\nfor (i = 0; i < nr_entries; i++)\r\nsync = (sync + 1) & (rb->nr_entries - 1);\r\nsmp_store_release(&rb->sync, sync);\r\nreturn sync;\r\n}\r\nunsigned int pblk_rb_sync_point_count(struct pblk_rb *rb)\r\n{\r\nunsigned int subm, sync_point;\r\nunsigned int count;\r\nsync_point = smp_load_acquire(&rb->sync_point);\r\nif (sync_point == EMPTY_ENTRY)\r\nreturn 0;\r\nsubm = READ_ONCE(rb->subm);\r\ncount = pblk_rb_ring_count(sync_point, subm, rb->nr_entries) + 1;\r\nreturn count;\r\n}\r\nstruct pblk_rb_entry *pblk_rb_sync_scan_entry(struct pblk_rb *rb,\r\nstruct ppa_addr *ppa)\r\n{\r\nunsigned int sync, subm, count;\r\nunsigned int i;\r\nsync = READ_ONCE(rb->sync);\r\nsubm = READ_ONCE(rb->subm);\r\ncount = pblk_rb_ring_count(subm, sync, rb->nr_entries);\r\nfor (i = 0; i < count; i++)\r\nsync = (sync + 1) & (rb->nr_entries - 1);\r\nreturn NULL;\r\n}\r\nint pblk_rb_tear_down_check(struct pblk_rb *rb)\r\n{\r\nstruct pblk_rb_entry *entry;\r\nint i;\r\nint ret = 0;\r\nspin_lock(&rb->w_lock);\r\nspin_lock_irq(&rb->s_lock);\r\nif ((rb->mem == rb->subm) && (rb->subm == rb->sync) &&\r\n(rb->sync == rb->l2p_update) &&\r\n(rb->sync_point == EMPTY_ENTRY)) {\r\ngoto out;\r\n}\r\nif (!rb->entries) {\r\nret = 1;\r\ngoto out;\r\n}\r\nfor (i = 0; i < rb->nr_entries; i++) {\r\nentry = &rb->entries[i];\r\nif (!entry->data) {\r\nret = 1;\r\ngoto out;\r\n}\r\n}\r\nout:\r\nspin_unlock(&rb->w_lock);\r\nspin_unlock_irq(&rb->s_lock);\r\nreturn ret;\r\n}\r\nunsigned int pblk_rb_wrap_pos(struct pblk_rb *rb, unsigned int pos)\r\n{\r\nreturn (pos & (rb->nr_entries - 1));\r\n}\r\nint pblk_rb_pos_oob(struct pblk_rb *rb, u64 pos)\r\n{\r\nreturn (pos >= rb->nr_entries);\r\n}\r\nssize_t pblk_rb_sysfs(struct pblk_rb *rb, char *buf)\r\n{\r\nstruct pblk *pblk = container_of(rb, struct pblk, rwb);\r\nstruct pblk_c_ctx *c;\r\nssize_t offset;\r\nint queued_entries = 0;\r\nspin_lock_irq(&rb->s_lock);\r\nlist_for_each_entry(c, &pblk->compl_list, list)\r\nqueued_entries++;\r\nspin_unlock_irq(&rb->s_lock);\r\nif (rb->sync_point != EMPTY_ENTRY)\r\noffset = scnprintf(buf, PAGE_SIZE,\r\n"%u\t%u\t%u\t%u\t%u\t%u\t%u - %u/%u/%u - %d\n",\r\nrb->nr_entries,\r\nrb->mem,\r\nrb->subm,\r\nrb->sync,\r\nrb->l2p_update,\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_read(&rb->inflight_sync_point),\r\n#else\r\n0,\r\n#endif\r\nrb->sync_point,\r\npblk_rb_read_count(rb),\r\npblk_rb_space(rb),\r\npblk_rb_sync_point_count(rb),\r\nqueued_entries);\r\nelse\r\noffset = scnprintf(buf, PAGE_SIZE,\r\n"%u\t%u\t%u\t%u\t%u\t%u\tNULL - %u/%u/%u - %d\n",\r\nrb->nr_entries,\r\nrb->mem,\r\nrb->subm,\r\nrb->sync,\r\nrb->l2p_update,\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_read(&rb->inflight_sync_point),\r\n#else\r\n0,\r\n#endif\r\npblk_rb_read_count(rb),\r\npblk_rb_space(rb),\r\npblk_rb_sync_point_count(rb),\r\nqueued_entries);\r\nreturn offset;\r\n}
