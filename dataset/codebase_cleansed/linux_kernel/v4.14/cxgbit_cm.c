static void cxgbit_init_wr_wait(struct cxgbit_wr_wait *wr_waitp)\r\n{\r\nwr_waitp->ret = 0;\r\nreinit_completion(&wr_waitp->completion);\r\n}\r\nstatic void\r\ncxgbit_wake_up(struct cxgbit_wr_wait *wr_waitp, const char *func, u8 ret)\r\n{\r\nif (ret == CPL_ERR_NONE)\r\nwr_waitp->ret = 0;\r\nelse\r\nwr_waitp->ret = -EIO;\r\nif (wr_waitp->ret)\r\npr_err("%s: err:%u", func, ret);\r\ncomplete(&wr_waitp->completion);\r\n}\r\nstatic int\r\ncxgbit_wait_for_reply(struct cxgbit_device *cdev,\r\nstruct cxgbit_wr_wait *wr_waitp, u32 tid, u32 timeout,\r\nconst char *func)\r\n{\r\nint ret;\r\nif (!test_bit(CDEV_STATE_UP, &cdev->flags)) {\r\nwr_waitp->ret = -EIO;\r\ngoto out;\r\n}\r\nret = wait_for_completion_timeout(&wr_waitp->completion, timeout * HZ);\r\nif (!ret) {\r\npr_info("%s - Device %s not responding tid %u\n",\r\nfunc, pci_name(cdev->lldi.pdev), tid);\r\nwr_waitp->ret = -ETIMEDOUT;\r\n}\r\nout:\r\nif (wr_waitp->ret)\r\npr_info("%s: FW reply %d tid %u\n",\r\npci_name(cdev->lldi.pdev), wr_waitp->ret, tid);\r\nreturn wr_waitp->ret;\r\n}\r\nstatic int cxgbit_np_hashfn(const struct cxgbit_np *cnp)\r\n{\r\nreturn ((unsigned long)cnp >> 10) & (NP_INFO_HASH_SIZE - 1);\r\n}\r\nstatic struct np_info *\r\ncxgbit_np_hash_add(struct cxgbit_device *cdev, struct cxgbit_np *cnp,\r\nunsigned int stid)\r\n{\r\nstruct np_info *p = kzalloc(sizeof(*p), GFP_KERNEL);\r\nif (p) {\r\nint bucket = cxgbit_np_hashfn(cnp);\r\np->cnp = cnp;\r\np->stid = stid;\r\nspin_lock(&cdev->np_lock);\r\np->next = cdev->np_hash_tab[bucket];\r\ncdev->np_hash_tab[bucket] = p;\r\nspin_unlock(&cdev->np_lock);\r\n}\r\nreturn p;\r\n}\r\nstatic int\r\ncxgbit_np_hash_find(struct cxgbit_device *cdev, struct cxgbit_np *cnp)\r\n{\r\nint stid = -1, bucket = cxgbit_np_hashfn(cnp);\r\nstruct np_info *p;\r\nspin_lock(&cdev->np_lock);\r\nfor (p = cdev->np_hash_tab[bucket]; p; p = p->next) {\r\nif (p->cnp == cnp) {\r\nstid = p->stid;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&cdev->np_lock);\r\nreturn stid;\r\n}\r\nstatic int cxgbit_np_hash_del(struct cxgbit_device *cdev, struct cxgbit_np *cnp)\r\n{\r\nint stid = -1, bucket = cxgbit_np_hashfn(cnp);\r\nstruct np_info *p, **prev = &cdev->np_hash_tab[bucket];\r\nspin_lock(&cdev->np_lock);\r\nfor (p = *prev; p; prev = &p->next, p = p->next) {\r\nif (p->cnp == cnp) {\r\nstid = p->stid;\r\n*prev = p->next;\r\nkfree(p);\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&cdev->np_lock);\r\nreturn stid;\r\n}\r\nvoid _cxgbit_free_cnp(struct kref *kref)\r\n{\r\nstruct cxgbit_np *cnp;\r\ncnp = container_of(kref, struct cxgbit_np, kref);\r\nkfree(cnp);\r\n}\r\nstatic int\r\ncxgbit_create_server6(struct cxgbit_device *cdev, unsigned int stid,\r\nstruct cxgbit_np *cnp)\r\n{\r\nstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)\r\n&cnp->com.local_addr;\r\nint addr_type;\r\nint ret;\r\npr_debug("%s: dev = %s; stid = %u; sin6_port = %u\n",\r\n__func__, cdev->lldi.ports[0]->name, stid, sin6->sin6_port);\r\naddr_type = ipv6_addr_type((const struct in6_addr *)\r\n&sin6->sin6_addr);\r\nif (addr_type != IPV6_ADDR_ANY) {\r\nret = cxgb4_clip_get(cdev->lldi.ports[0],\r\n(const u32 *)&sin6->sin6_addr.s6_addr, 1);\r\nif (ret) {\r\npr_err("Unable to find clip table entry. laddr %pI6. Error:%d.\n",\r\nsin6->sin6_addr.s6_addr, ret);\r\nreturn -ENOMEM;\r\n}\r\n}\r\ncxgbit_get_cnp(cnp);\r\ncxgbit_init_wr_wait(&cnp->com.wr_wait);\r\nret = cxgb4_create_server6(cdev->lldi.ports[0],\r\nstid, &sin6->sin6_addr,\r\nsin6->sin6_port,\r\ncdev->lldi.rxq_ids[0]);\r\nif (!ret)\r\nret = cxgbit_wait_for_reply(cdev, &cnp->com.wr_wait,\r\n0, 10, __func__);\r\nelse if (ret > 0)\r\nret = net_xmit_errno(ret);\r\nelse\r\ncxgbit_put_cnp(cnp);\r\nif (ret) {\r\nif (ret != -ETIMEDOUT)\r\ncxgb4_clip_release(cdev->lldi.ports[0],\r\n(const u32 *)&sin6->sin6_addr.s6_addr, 1);\r\npr_err("create server6 err %d stid %d laddr %pI6 lport %d\n",\r\nret, stid, sin6->sin6_addr.s6_addr,\r\nntohs(sin6->sin6_port));\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\ncxgbit_create_server4(struct cxgbit_device *cdev, unsigned int stid,\r\nstruct cxgbit_np *cnp)\r\n{\r\nstruct sockaddr_in *sin = (struct sockaddr_in *)\r\n&cnp->com.local_addr;\r\nint ret;\r\npr_debug("%s: dev = %s; stid = %u; sin_port = %u\n",\r\n__func__, cdev->lldi.ports[0]->name, stid, sin->sin_port);\r\ncxgbit_get_cnp(cnp);\r\ncxgbit_init_wr_wait(&cnp->com.wr_wait);\r\nret = cxgb4_create_server(cdev->lldi.ports[0],\r\nstid, sin->sin_addr.s_addr,\r\nsin->sin_port, 0,\r\ncdev->lldi.rxq_ids[0]);\r\nif (!ret)\r\nret = cxgbit_wait_for_reply(cdev,\r\n&cnp->com.wr_wait,\r\n0, 10, __func__);\r\nelse if (ret > 0)\r\nret = net_xmit_errno(ret);\r\nelse\r\ncxgbit_put_cnp(cnp);\r\nif (ret)\r\npr_err("create server failed err %d stid %d laddr %pI4 lport %d\n",\r\nret, stid, &sin->sin_addr, ntohs(sin->sin_port));\r\nreturn ret;\r\n}\r\nstruct cxgbit_device *cxgbit_find_device(struct net_device *ndev, u8 *port_id)\r\n{\r\nstruct cxgbit_device *cdev;\r\nu8 i;\r\nlist_for_each_entry(cdev, &cdev_list_head, list) {\r\nstruct cxgb4_lld_info *lldi = &cdev->lldi;\r\nfor (i = 0; i < lldi->nports; i++) {\r\nif (lldi->ports[i] == ndev) {\r\nif (port_id)\r\n*port_id = i;\r\nreturn cdev;\r\n}\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct net_device *cxgbit_get_real_dev(struct net_device *ndev)\r\n{\r\nif (ndev->priv_flags & IFF_BONDING) {\r\npr_err("Bond devices are not supported. Interface:%s\n",\r\nndev->name);\r\nreturn NULL;\r\n}\r\nif (is_vlan_dev(ndev))\r\nreturn vlan_dev_real_dev(ndev);\r\nreturn ndev;\r\n}\r\nstatic struct net_device *cxgbit_ipv4_netdev(__be32 saddr)\r\n{\r\nstruct net_device *ndev;\r\nndev = __ip_dev_find(&init_net, saddr, false);\r\nif (!ndev)\r\nreturn NULL;\r\nreturn cxgbit_get_real_dev(ndev);\r\n}\r\nstatic struct net_device *cxgbit_ipv6_netdev(struct in6_addr *addr6)\r\n{\r\nstruct net_device *ndev = NULL;\r\nbool found = false;\r\nif (IS_ENABLED(CONFIG_IPV6)) {\r\nfor_each_netdev_rcu(&init_net, ndev)\r\nif (ipv6_chk_addr(&init_net, addr6, ndev, 1)) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found)\r\nreturn NULL;\r\nreturn cxgbit_get_real_dev(ndev);\r\n}\r\nstatic struct cxgbit_device *cxgbit_find_np_cdev(struct cxgbit_np *cnp)\r\n{\r\nstruct sockaddr_storage *sockaddr = &cnp->com.local_addr;\r\nint ss_family = sockaddr->ss_family;\r\nstruct net_device *ndev = NULL;\r\nstruct cxgbit_device *cdev = NULL;\r\nrcu_read_lock();\r\nif (ss_family == AF_INET) {\r\nstruct sockaddr_in *sin;\r\nsin = (struct sockaddr_in *)sockaddr;\r\nndev = cxgbit_ipv4_netdev(sin->sin_addr.s_addr);\r\n} else if (ss_family == AF_INET6) {\r\nstruct sockaddr_in6 *sin6;\r\nsin6 = (struct sockaddr_in6 *)sockaddr;\r\nndev = cxgbit_ipv6_netdev(&sin6->sin6_addr);\r\n}\r\nif (!ndev)\r\ngoto out;\r\ncdev = cxgbit_find_device(ndev, NULL);\r\nout:\r\nrcu_read_unlock();\r\nreturn cdev;\r\n}\r\nstatic bool cxgbit_inaddr_any(struct cxgbit_np *cnp)\r\n{\r\nstruct sockaddr_storage *sockaddr = &cnp->com.local_addr;\r\nint ss_family = sockaddr->ss_family;\r\nint addr_type;\r\nif (ss_family == AF_INET) {\r\nstruct sockaddr_in *sin;\r\nsin = (struct sockaddr_in *)sockaddr;\r\nif (sin->sin_addr.s_addr == htonl(INADDR_ANY))\r\nreturn true;\r\n} else if (ss_family == AF_INET6) {\r\nstruct sockaddr_in6 *sin6;\r\nsin6 = (struct sockaddr_in6 *)sockaddr;\r\naddr_type = ipv6_addr_type((const struct in6_addr *)\r\n&sin6->sin6_addr);\r\nif (addr_type == IPV6_ADDR_ANY)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int\r\n__cxgbit_setup_cdev_np(struct cxgbit_device *cdev, struct cxgbit_np *cnp)\r\n{\r\nint stid, ret;\r\nint ss_family = cnp->com.local_addr.ss_family;\r\nif (!test_bit(CDEV_STATE_UP, &cdev->flags))\r\nreturn -EINVAL;\r\nstid = cxgb4_alloc_stid(cdev->lldi.tids, ss_family, cnp);\r\nif (stid < 0)\r\nreturn -EINVAL;\r\nif (!cxgbit_np_hash_add(cdev, cnp, stid)) {\r\ncxgb4_free_stid(cdev->lldi.tids, stid, ss_family);\r\nreturn -EINVAL;\r\n}\r\nif (ss_family == AF_INET)\r\nret = cxgbit_create_server4(cdev, stid, cnp);\r\nelse\r\nret = cxgbit_create_server6(cdev, stid, cnp);\r\nif (ret) {\r\nif (ret != -ETIMEDOUT)\r\ncxgb4_free_stid(cdev->lldi.tids, stid,\r\nss_family);\r\ncxgbit_np_hash_del(cdev, cnp);\r\nreturn ret;\r\n}\r\nreturn ret;\r\n}\r\nstatic int cxgbit_setup_cdev_np(struct cxgbit_np *cnp)\r\n{\r\nstruct cxgbit_device *cdev;\r\nint ret = -1;\r\nmutex_lock(&cdev_list_lock);\r\ncdev = cxgbit_find_np_cdev(cnp);\r\nif (!cdev)\r\ngoto out;\r\nif (cxgbit_np_hash_find(cdev, cnp) >= 0)\r\ngoto out;\r\nif (__cxgbit_setup_cdev_np(cdev, cnp))\r\ngoto out;\r\ncnp->com.cdev = cdev;\r\nret = 0;\r\nout:\r\nmutex_unlock(&cdev_list_lock);\r\nreturn ret;\r\n}\r\nstatic int cxgbit_setup_all_np(struct cxgbit_np *cnp)\r\n{\r\nstruct cxgbit_device *cdev;\r\nint ret;\r\nu32 count = 0;\r\nmutex_lock(&cdev_list_lock);\r\nlist_for_each_entry(cdev, &cdev_list_head, list) {\r\nif (cxgbit_np_hash_find(cdev, cnp) >= 0) {\r\nmutex_unlock(&cdev_list_lock);\r\nreturn -1;\r\n}\r\n}\r\nlist_for_each_entry(cdev, &cdev_list_head, list) {\r\nret = __cxgbit_setup_cdev_np(cdev, cnp);\r\nif (ret == -ETIMEDOUT)\r\nbreak;\r\nif (ret != 0)\r\ncontinue;\r\ncount++;\r\n}\r\nmutex_unlock(&cdev_list_lock);\r\nreturn count ? 0 : -1;\r\n}\r\nint cxgbit_setup_np(struct iscsi_np *np, struct sockaddr_storage *ksockaddr)\r\n{\r\nstruct cxgbit_np *cnp;\r\nint ret;\r\nif ((ksockaddr->ss_family != AF_INET) &&\r\n(ksockaddr->ss_family != AF_INET6))\r\nreturn -EINVAL;\r\ncnp = kzalloc(sizeof(*cnp), GFP_KERNEL);\r\nif (!cnp)\r\nreturn -ENOMEM;\r\ninit_waitqueue_head(&cnp->accept_wait);\r\ninit_completion(&cnp->com.wr_wait.completion);\r\ninit_completion(&cnp->accept_comp);\r\nINIT_LIST_HEAD(&cnp->np_accept_list);\r\nspin_lock_init(&cnp->np_accept_lock);\r\nkref_init(&cnp->kref);\r\nmemcpy(&np->np_sockaddr, ksockaddr,\r\nsizeof(struct sockaddr_storage));\r\nmemcpy(&cnp->com.local_addr, &np->np_sockaddr,\r\nsizeof(cnp->com.local_addr));\r\ncnp->np = np;\r\ncnp->com.cdev = NULL;\r\nif (cxgbit_inaddr_any(cnp))\r\nret = cxgbit_setup_all_np(cnp);\r\nelse\r\nret = cxgbit_setup_cdev_np(cnp);\r\nif (ret) {\r\ncxgbit_put_cnp(cnp);\r\nreturn -EINVAL;\r\n}\r\nnp->np_context = cnp;\r\ncnp->com.state = CSK_STATE_LISTEN;\r\nreturn 0;\r\n}\r\nstatic void\r\ncxgbit_set_conn_info(struct iscsi_np *np, struct iscsi_conn *conn,\r\nstruct cxgbit_sock *csk)\r\n{\r\nconn->login_family = np->np_sockaddr.ss_family;\r\nconn->login_sockaddr = csk->com.remote_addr;\r\nconn->local_sockaddr = csk->com.local_addr;\r\n}\r\nint cxgbit_accept_np(struct iscsi_np *np, struct iscsi_conn *conn)\r\n{\r\nstruct cxgbit_np *cnp = np->np_context;\r\nstruct cxgbit_sock *csk;\r\nint ret = 0;\r\naccept_wait:\r\nret = wait_for_completion_interruptible(&cnp->accept_comp);\r\nif (ret)\r\nreturn -ENODEV;\r\nspin_lock_bh(&np->np_thread_lock);\r\nif (np->np_thread_state >= ISCSI_NP_THREAD_RESET) {\r\nspin_unlock_bh(&np->np_thread_lock);\r\nreturn -ENODEV;\r\n}\r\nspin_unlock_bh(&np->np_thread_lock);\r\nspin_lock_bh(&cnp->np_accept_lock);\r\nif (list_empty(&cnp->np_accept_list)) {\r\nspin_unlock_bh(&cnp->np_accept_lock);\r\ngoto accept_wait;\r\n}\r\ncsk = list_first_entry(&cnp->np_accept_list,\r\nstruct cxgbit_sock,\r\naccept_node);\r\nlist_del_init(&csk->accept_node);\r\nspin_unlock_bh(&cnp->np_accept_lock);\r\nconn->context = csk;\r\ncsk->conn = conn;\r\ncxgbit_set_conn_info(np, conn, csk);\r\nreturn 0;\r\n}\r\nstatic int\r\n__cxgbit_free_cdev_np(struct cxgbit_device *cdev, struct cxgbit_np *cnp)\r\n{\r\nint stid, ret;\r\nbool ipv6 = false;\r\nstid = cxgbit_np_hash_del(cdev, cnp);\r\nif (stid < 0)\r\nreturn -EINVAL;\r\nif (!test_bit(CDEV_STATE_UP, &cdev->flags))\r\nreturn -EINVAL;\r\nif (cnp->np->np_sockaddr.ss_family == AF_INET6)\r\nipv6 = true;\r\ncxgbit_get_cnp(cnp);\r\ncxgbit_init_wr_wait(&cnp->com.wr_wait);\r\nret = cxgb4_remove_server(cdev->lldi.ports[0], stid,\r\ncdev->lldi.rxq_ids[0], ipv6);\r\nif (ret > 0)\r\nret = net_xmit_errno(ret);\r\nif (ret) {\r\ncxgbit_put_cnp(cnp);\r\nreturn ret;\r\n}\r\nret = cxgbit_wait_for_reply(cdev, &cnp->com.wr_wait,\r\n0, 10, __func__);\r\nif (ret == -ETIMEDOUT)\r\nreturn ret;\r\nif (ipv6 && cnp->com.cdev) {\r\nstruct sockaddr_in6 *sin6;\r\nsin6 = (struct sockaddr_in6 *)&cnp->com.local_addr;\r\ncxgb4_clip_release(cdev->lldi.ports[0],\r\n(const u32 *)&sin6->sin6_addr.s6_addr,\r\n1);\r\n}\r\ncxgb4_free_stid(cdev->lldi.tids, stid,\r\ncnp->com.local_addr.ss_family);\r\nreturn 0;\r\n}\r\nstatic void cxgbit_free_all_np(struct cxgbit_np *cnp)\r\n{\r\nstruct cxgbit_device *cdev;\r\nint ret;\r\nmutex_lock(&cdev_list_lock);\r\nlist_for_each_entry(cdev, &cdev_list_head, list) {\r\nret = __cxgbit_free_cdev_np(cdev, cnp);\r\nif (ret == -ETIMEDOUT)\r\nbreak;\r\n}\r\nmutex_unlock(&cdev_list_lock);\r\n}\r\nstatic void cxgbit_free_cdev_np(struct cxgbit_np *cnp)\r\n{\r\nstruct cxgbit_device *cdev;\r\nbool found = false;\r\nmutex_lock(&cdev_list_lock);\r\nlist_for_each_entry(cdev, &cdev_list_head, list) {\r\nif (cdev == cnp->com.cdev) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found)\r\ngoto out;\r\n__cxgbit_free_cdev_np(cdev, cnp);\r\nout:\r\nmutex_unlock(&cdev_list_lock);\r\n}\r\nvoid cxgbit_free_np(struct iscsi_np *np)\r\n{\r\nstruct cxgbit_np *cnp = np->np_context;\r\ncnp->com.state = CSK_STATE_DEAD;\r\nif (cnp->com.cdev)\r\ncxgbit_free_cdev_np(cnp);\r\nelse\r\ncxgbit_free_all_np(cnp);\r\nnp->np_context = NULL;\r\ncxgbit_put_cnp(cnp);\r\n}\r\nstatic void cxgbit_send_halfclose(struct cxgbit_sock *csk)\r\n{\r\nstruct sk_buff *skb;\r\nu32 len = roundup(sizeof(struct cpl_close_con_req), 16);\r\nskb = alloc_skb(len, GFP_ATOMIC);\r\nif (!skb)\r\nreturn;\r\ncxgb_mk_close_con_req(skb, len, csk->tid, csk->txq_idx,\r\nNULL, NULL);\r\ncxgbit_skcb_flags(skb) |= SKCBF_TX_FLAG_COMPL;\r\n__skb_queue_tail(&csk->txq, skb);\r\ncxgbit_push_tx_frames(csk);\r\n}\r\nstatic void cxgbit_arp_failure_discard(void *handle, struct sk_buff *skb)\r\n{\r\npr_debug("%s cxgbit_device %p\n", __func__, handle);\r\nkfree_skb(skb);\r\n}\r\nstatic void cxgbit_abort_arp_failure(void *handle, struct sk_buff *skb)\r\n{\r\nstruct cxgbit_device *cdev = handle;\r\nstruct cpl_abort_req *req = cplhdr(skb);\r\npr_debug("%s cdev %p\n", __func__, cdev);\r\nreq->cmd = CPL_ABORT_NO_RST;\r\ncxgbit_ofld_send(cdev, skb);\r\n}\r\nstatic int cxgbit_send_abort_req(struct cxgbit_sock *csk)\r\n{\r\nstruct sk_buff *skb;\r\nu32 len = roundup(sizeof(struct cpl_abort_req), 16);\r\npr_debug("%s: csk %p tid %u; state %d\n",\r\n__func__, csk, csk->tid, csk->com.state);\r\n__skb_queue_purge(&csk->txq);\r\nif (!test_and_set_bit(CSK_TX_DATA_SENT, &csk->com.flags))\r\ncxgbit_send_tx_flowc_wr(csk);\r\nskb = __skb_dequeue(&csk->skbq);\r\ncxgb_mk_abort_req(skb, len, csk->tid, csk->txq_idx,\r\ncsk->com.cdev, cxgbit_abort_arp_failure);\r\nreturn cxgbit_l2t_send(csk->com.cdev, skb, csk->l2t);\r\n}\r\nvoid cxgbit_free_conn(struct iscsi_conn *conn)\r\n{\r\nstruct cxgbit_sock *csk = conn->context;\r\nbool release = false;\r\npr_debug("%s: state %d\n",\r\n__func__, csk->com.state);\r\nspin_lock_bh(&csk->lock);\r\nswitch (csk->com.state) {\r\ncase CSK_STATE_ESTABLISHED:\r\nif (conn->conn_state == TARG_CONN_STATE_IN_LOGOUT) {\r\ncsk->com.state = CSK_STATE_CLOSING;\r\ncxgbit_send_halfclose(csk);\r\n} else {\r\ncsk->com.state = CSK_STATE_ABORTING;\r\ncxgbit_send_abort_req(csk);\r\n}\r\nbreak;\r\ncase CSK_STATE_CLOSING:\r\ncsk->com.state = CSK_STATE_MORIBUND;\r\ncxgbit_send_halfclose(csk);\r\nbreak;\r\ncase CSK_STATE_DEAD:\r\nrelease = true;\r\nbreak;\r\ndefault:\r\npr_err("%s: csk %p; state %d\n",\r\n__func__, csk, csk->com.state);\r\n}\r\nspin_unlock_bh(&csk->lock);\r\nif (release)\r\ncxgbit_put_csk(csk);\r\n}\r\nstatic void cxgbit_set_emss(struct cxgbit_sock *csk, u16 opt)\r\n{\r\ncsk->emss = csk->com.cdev->lldi.mtus[TCPOPT_MSS_G(opt)] -\r\n((csk->com.remote_addr.ss_family == AF_INET) ?\r\nsizeof(struct iphdr) : sizeof(struct ipv6hdr)) -\r\nsizeof(struct tcphdr);\r\ncsk->mss = csk->emss;\r\nif (TCPOPT_TSTAMP_G(opt))\r\ncsk->emss -= round_up(TCPOLEN_TIMESTAMP, 4);\r\nif (csk->emss < 128)\r\ncsk->emss = 128;\r\nif (csk->emss & 7)\r\npr_info("Warning: misaligned mtu idx %u mss %u emss=%u\n",\r\nTCPOPT_MSS_G(opt), csk->mss, csk->emss);\r\npr_debug("%s mss_idx %u mss %u emss=%u\n", __func__, TCPOPT_MSS_G(opt),\r\ncsk->mss, csk->emss);\r\n}\r\nstatic void cxgbit_free_skb(struct cxgbit_sock *csk)\r\n{\r\nstruct sk_buff *skb;\r\n__skb_queue_purge(&csk->txq);\r\n__skb_queue_purge(&csk->rxq);\r\n__skb_queue_purge(&csk->backlogq);\r\n__skb_queue_purge(&csk->ppodq);\r\n__skb_queue_purge(&csk->skbq);\r\nwhile ((skb = cxgbit_sock_dequeue_wr(csk)))\r\nkfree_skb(skb);\r\n__kfree_skb(csk->lro_hskb);\r\n}\r\nvoid _cxgbit_free_csk(struct kref *kref)\r\n{\r\nstruct cxgbit_sock *csk;\r\nstruct cxgbit_device *cdev;\r\ncsk = container_of(kref, struct cxgbit_sock, kref);\r\npr_debug("%s csk %p state %d\n", __func__, csk, csk->com.state);\r\nif (csk->com.local_addr.ss_family == AF_INET6) {\r\nstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)\r\n&csk->com.local_addr;\r\ncxgb4_clip_release(csk->com.cdev->lldi.ports[0],\r\n(const u32 *)\r\n&sin6->sin6_addr.s6_addr, 1);\r\n}\r\ncxgb4_remove_tid(csk->com.cdev->lldi.tids, 0, csk->tid,\r\ncsk->com.local_addr.ss_family);\r\ndst_release(csk->dst);\r\ncxgb4_l2t_release(csk->l2t);\r\ncdev = csk->com.cdev;\r\nspin_lock_bh(&cdev->cskq.lock);\r\nlist_del(&csk->list);\r\nspin_unlock_bh(&cdev->cskq.lock);\r\ncxgbit_free_skb(csk);\r\ncxgbit_put_cdev(cdev);\r\nkfree(csk);\r\n}\r\nstatic void cxgbit_set_tcp_window(struct cxgbit_sock *csk, struct port_info *pi)\r\n{\r\nunsigned int linkspeed;\r\nu8 scale;\r\nlinkspeed = pi->link_cfg.speed;\r\nscale = linkspeed / SPEED_10000;\r\n#define CXGBIT_10G_RCV_WIN (256 * 1024)\r\ncsk->rcv_win = CXGBIT_10G_RCV_WIN;\r\nif (scale)\r\ncsk->rcv_win *= scale;\r\n#define CXGBIT_10G_SND_WIN (256 * 1024)\r\ncsk->snd_win = CXGBIT_10G_SND_WIN;\r\nif (scale)\r\ncsk->snd_win *= scale;\r\npr_debug("%s snd_win %d rcv_win %d\n",\r\n__func__, csk->snd_win, csk->rcv_win);\r\n}\r\nstatic u8 cxgbit_get_iscsi_dcb_state(struct net_device *ndev)\r\n{\r\nreturn ndev->dcbnl_ops->getstate(ndev);\r\n}\r\nstatic int cxgbit_select_priority(int pri_mask)\r\n{\r\nif (!pri_mask)\r\nreturn 0;\r\nreturn (ffs(pri_mask) - 1);\r\n}\r\nstatic u8 cxgbit_get_iscsi_dcb_priority(struct net_device *ndev, u16 local_port)\r\n{\r\nint ret;\r\nu8 caps;\r\nstruct dcb_app iscsi_dcb_app = {\r\n.protocol = local_port\r\n};\r\nret = (int)ndev->dcbnl_ops->getcap(ndev, DCB_CAP_ATTR_DCBX, &caps);\r\nif (ret)\r\nreturn 0;\r\nif (caps & DCB_CAP_DCBX_VER_IEEE) {\r\niscsi_dcb_app.selector = IEEE_8021QAZ_APP_SEL_ANY;\r\nret = dcb_ieee_getapp_mask(ndev, &iscsi_dcb_app);\r\n} else if (caps & DCB_CAP_DCBX_VER_CEE) {\r\niscsi_dcb_app.selector = DCB_APP_IDTYPE_PORTNUM;\r\nret = dcb_getapp(ndev, &iscsi_dcb_app);\r\n}\r\npr_info("iSCSI priority is set to %u\n", cxgbit_select_priority(ret));\r\nreturn cxgbit_select_priority(ret);\r\n}\r\nstatic int\r\ncxgbit_offload_init(struct cxgbit_sock *csk, int iptype, __u8 *peer_ip,\r\nu16 local_port, struct dst_entry *dst,\r\nstruct cxgbit_device *cdev)\r\n{\r\nstruct neighbour *n;\r\nint ret, step;\r\nstruct net_device *ndev;\r\nu16 rxq_idx, port_id;\r\n#ifdef CONFIG_CHELSIO_T4_DCB\r\nu8 priority = 0;\r\n#endif\r\nn = dst_neigh_lookup(dst, peer_ip);\r\nif (!n)\r\nreturn -ENODEV;\r\nrcu_read_lock();\r\nret = -ENOMEM;\r\nif (n->dev->flags & IFF_LOOPBACK) {\r\nif (iptype == 4)\r\nndev = cxgbit_ipv4_netdev(*(__be32 *)peer_ip);\r\nelse if (IS_ENABLED(CONFIG_IPV6))\r\nndev = cxgbit_ipv6_netdev((struct in6_addr *)peer_ip);\r\nelse\r\nndev = NULL;\r\nif (!ndev) {\r\nret = -ENODEV;\r\ngoto out;\r\n}\r\ncsk->l2t = cxgb4_l2t_get(cdev->lldi.l2t,\r\nn, ndev, 0);\r\nif (!csk->l2t)\r\ngoto out;\r\ncsk->mtu = ndev->mtu;\r\ncsk->tx_chan = cxgb4_port_chan(ndev);\r\ncsk->smac_idx = cxgb4_tp_smt_idx(cdev->lldi.adapter_type,\r\ncxgb4_port_viid(ndev));\r\nstep = cdev->lldi.ntxq /\r\ncdev->lldi.nchan;\r\ncsk->txq_idx = cxgb4_port_idx(ndev) * step;\r\nstep = cdev->lldi.nrxq /\r\ncdev->lldi.nchan;\r\ncsk->ctrlq_idx = cxgb4_port_idx(ndev);\r\ncsk->rss_qid = cdev->lldi.rxq_ids[\r\ncxgb4_port_idx(ndev) * step];\r\ncsk->port_id = cxgb4_port_idx(ndev);\r\ncxgbit_set_tcp_window(csk,\r\n(struct port_info *)netdev_priv(ndev));\r\n} else {\r\nndev = cxgbit_get_real_dev(n->dev);\r\nif (!ndev) {\r\nret = -ENODEV;\r\ngoto out;\r\n}\r\n#ifdef CONFIG_CHELSIO_T4_DCB\r\nif (cxgbit_get_iscsi_dcb_state(ndev))\r\npriority = cxgbit_get_iscsi_dcb_priority(ndev,\r\nlocal_port);\r\ncsk->dcb_priority = priority;\r\ncsk->l2t = cxgb4_l2t_get(cdev->lldi.l2t, n, ndev, priority);\r\n#else\r\ncsk->l2t = cxgb4_l2t_get(cdev->lldi.l2t, n, ndev, 0);\r\n#endif\r\nif (!csk->l2t)\r\ngoto out;\r\nport_id = cxgb4_port_idx(ndev);\r\ncsk->mtu = dst_mtu(dst);\r\ncsk->tx_chan = cxgb4_port_chan(ndev);\r\ncsk->smac_idx = cxgb4_tp_smt_idx(cdev->lldi.adapter_type,\r\ncxgb4_port_viid(ndev));\r\nstep = cdev->lldi.ntxq /\r\ncdev->lldi.nports;\r\ncsk->txq_idx = (port_id * step) +\r\n(cdev->selectq[port_id][0]++ % step);\r\ncsk->ctrlq_idx = cxgb4_port_idx(ndev);\r\nstep = cdev->lldi.nrxq /\r\ncdev->lldi.nports;\r\nrxq_idx = (port_id * step) +\r\n(cdev->selectq[port_id][1]++ % step);\r\ncsk->rss_qid = cdev->lldi.rxq_ids[rxq_idx];\r\ncsk->port_id = port_id;\r\ncxgbit_set_tcp_window(csk,\r\n(struct port_info *)netdev_priv(ndev));\r\n}\r\nret = 0;\r\nout:\r\nrcu_read_unlock();\r\nneigh_release(n);\r\nreturn ret;\r\n}\r\nint cxgbit_ofld_send(struct cxgbit_device *cdev, struct sk_buff *skb)\r\n{\r\nint ret = 0;\r\nif (!test_bit(CDEV_STATE_UP, &cdev->flags)) {\r\nkfree_skb(skb);\r\npr_err("%s - device not up - dropping\n", __func__);\r\nreturn -EIO;\r\n}\r\nret = cxgb4_ofld_send(cdev->lldi.ports[0], skb);\r\nif (ret < 0)\r\nkfree_skb(skb);\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic void cxgbit_release_tid(struct cxgbit_device *cdev, u32 tid)\r\n{\r\nu32 len = roundup(sizeof(struct cpl_tid_release), 16);\r\nstruct sk_buff *skb;\r\nskb = alloc_skb(len, GFP_ATOMIC);\r\nif (!skb)\r\nreturn;\r\ncxgb_mk_tid_release(skb, len, tid, 0);\r\ncxgbit_ofld_send(cdev, skb);\r\n}\r\nint\r\ncxgbit_l2t_send(struct cxgbit_device *cdev, struct sk_buff *skb,\r\nstruct l2t_entry *l2e)\r\n{\r\nint ret = 0;\r\nif (!test_bit(CDEV_STATE_UP, &cdev->flags)) {\r\nkfree_skb(skb);\r\npr_err("%s - device not up - dropping\n", __func__);\r\nreturn -EIO;\r\n}\r\nret = cxgb4_l2t_send(cdev->lldi.ports[0], skb, l2e);\r\nif (ret < 0)\r\nkfree_skb(skb);\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic void cxgbit_send_rx_credits(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\nif (csk->com.state != CSK_STATE_ESTABLISHED) {\r\n__kfree_skb(skb);\r\nreturn;\r\n}\r\ncxgbit_ofld_send(csk->com.cdev, skb);\r\n}\r\nint cxgbit_rx_data_ack(struct cxgbit_sock *csk)\r\n{\r\nstruct sk_buff *skb;\r\nu32 len = roundup(sizeof(struct cpl_rx_data_ack), 16);\r\nu32 credit_dack;\r\nskb = alloc_skb(len, GFP_KERNEL);\r\nif (!skb)\r\nreturn -1;\r\ncredit_dack = RX_DACK_CHANGE_F | RX_DACK_MODE_V(1) |\r\nRX_CREDITS_V(csk->rx_credits);\r\ncxgb_mk_rx_data_ack(skb, len, csk->tid, csk->ctrlq_idx,\r\ncredit_dack);\r\ncsk->rx_credits = 0;\r\nspin_lock_bh(&csk->lock);\r\nif (csk->lock_owner) {\r\ncxgbit_skcb_rx_backlog_fn(skb) = cxgbit_send_rx_credits;\r\n__skb_queue_tail(&csk->backlogq, skb);\r\nspin_unlock_bh(&csk->lock);\r\nreturn 0;\r\n}\r\ncxgbit_send_rx_credits(csk, skb);\r\nspin_unlock_bh(&csk->lock);\r\nreturn 0;\r\n}\r\nstatic int cxgbit_alloc_csk_skb(struct cxgbit_sock *csk)\r\n{\r\nstruct sk_buff *skb;\r\nu32 len, flowclen;\r\nu8 i;\r\nflowclen = offsetof(struct fw_flowc_wr,\r\nmnemval[FLOWC_WR_NPARAMS_MAX]);\r\nlen = max_t(u32, sizeof(struct cpl_abort_req),\r\nsizeof(struct cpl_abort_rpl));\r\nlen = max(len, flowclen);\r\nlen = roundup(len, 16);\r\nfor (i = 0; i < 3; i++) {\r\nskb = alloc_skb(len, GFP_ATOMIC);\r\nif (!skb)\r\ngoto out;\r\n__skb_queue_tail(&csk->skbq, skb);\r\n}\r\nskb = alloc_skb(LRO_SKB_MIN_HEADROOM, GFP_ATOMIC);\r\nif (!skb)\r\ngoto out;\r\nmemset(skb->data, 0, LRO_SKB_MIN_HEADROOM);\r\ncsk->lro_hskb = skb;\r\nreturn 0;\r\nout:\r\n__skb_queue_purge(&csk->skbq);\r\nreturn -ENOMEM;\r\n}\r\nstatic void\r\ncxgbit_pass_accept_rpl(struct cxgbit_sock *csk, struct cpl_pass_accept_req *req)\r\n{\r\nstruct sk_buff *skb;\r\nconst struct tcphdr *tcph;\r\nstruct cpl_t5_pass_accept_rpl *rpl5;\r\nstruct cxgb4_lld_info *lldi = &csk->com.cdev->lldi;\r\nunsigned int len = roundup(sizeof(*rpl5), 16);\r\nunsigned int mtu_idx;\r\nu64 opt0;\r\nu32 opt2, hlen;\r\nu32 wscale;\r\nu32 win;\r\npr_debug("%s csk %p tid %u\n", __func__, csk, csk->tid);\r\nskb = alloc_skb(len, GFP_ATOMIC);\r\nif (!skb) {\r\ncxgbit_put_csk(csk);\r\nreturn;\r\n}\r\nrpl5 = __skb_put_zero(skb, len);\r\nINIT_TP_WR(rpl5, csk->tid);\r\nOPCODE_TID(rpl5) = cpu_to_be32(MK_OPCODE_TID(CPL_PASS_ACCEPT_RPL,\r\ncsk->tid));\r\ncxgb_best_mtu(csk->com.cdev->lldi.mtus, csk->mtu, &mtu_idx,\r\nreq->tcpopt.tstamp,\r\n(csk->com.remote_addr.ss_family == AF_INET) ? 0 : 1);\r\nwscale = cxgb_compute_wscale(csk->rcv_win);\r\nwin = csk->rcv_win >> 10;\r\nif (win > RCV_BUFSIZ_M)\r\nwin = RCV_BUFSIZ_M;\r\nopt0 = TCAM_BYPASS_F |\r\nWND_SCALE_V(wscale) |\r\nMSS_IDX_V(mtu_idx) |\r\nL2T_IDX_V(csk->l2t->idx) |\r\nTX_CHAN_V(csk->tx_chan) |\r\nSMAC_SEL_V(csk->smac_idx) |\r\nDSCP_V(csk->tos >> 2) |\r\nULP_MODE_V(ULP_MODE_ISCSI) |\r\nRCV_BUFSIZ_V(win);\r\nopt2 = RX_CHANNEL_V(0) |\r\nRSS_QUEUE_VALID_F | RSS_QUEUE_V(csk->rss_qid);\r\nif (!is_t5(lldi->adapter_type))\r\nopt2 |= RX_FC_DISABLE_F;\r\nif (req->tcpopt.tstamp)\r\nopt2 |= TSTAMPS_EN_F;\r\nif (req->tcpopt.sack)\r\nopt2 |= SACK_EN_F;\r\nif (wscale)\r\nopt2 |= WND_SCALE_EN_F;\r\nhlen = ntohl(req->hdr_len);\r\nif (is_t5(lldi->adapter_type))\r\ntcph = (struct tcphdr *)((u8 *)(req + 1) +\r\nETH_HDR_LEN_G(hlen) + IP_HDR_LEN_G(hlen));\r\nelse\r\ntcph = (struct tcphdr *)((u8 *)(req + 1) +\r\nT6_ETH_HDR_LEN_G(hlen) + T6_IP_HDR_LEN_G(hlen));\r\nif (tcph->ece && tcph->cwr)\r\nopt2 |= CCTRL_ECN_V(1);\r\nopt2 |= RX_COALESCE_V(3);\r\nopt2 |= CONG_CNTRL_V(CONG_ALG_NEWRENO);\r\nopt2 |= T5_ISS_F;\r\nrpl5->iss = cpu_to_be32((prandom_u32() & ~7UL) - 1);\r\nopt2 |= T5_OPT_2_VALID_F;\r\nrpl5->opt0 = cpu_to_be64(opt0);\r\nrpl5->opt2 = cpu_to_be32(opt2);\r\nset_wr_txq(skb, CPL_PRIORITY_SETUP, csk->ctrlq_idx);\r\nt4_set_arp_err_handler(skb, NULL, cxgbit_arp_failure_discard);\r\ncxgbit_l2t_send(csk->com.cdev, skb, csk->l2t);\r\n}\r\nstatic void\r\ncxgbit_pass_accept_req(struct cxgbit_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbit_sock *csk = NULL;\r\nstruct cxgbit_np *cnp;\r\nstruct cpl_pass_accept_req *req = cplhdr(skb);\r\nunsigned int stid = PASS_OPEN_TID_G(ntohl(req->tos_stid));\r\nstruct tid_info *t = cdev->lldi.tids;\r\nunsigned int tid = GET_TID(req);\r\nu16 peer_mss = ntohs(req->tcpopt.mss);\r\nunsigned short hdrs;\r\nstruct dst_entry *dst;\r\n__u8 local_ip[16], peer_ip[16];\r\n__be16 local_port, peer_port;\r\nint ret;\r\nint iptype;\r\npr_debug("%s: cdev = %p; stid = %u; tid = %u\n",\r\n__func__, cdev, stid, tid);\r\ncnp = lookup_stid(t, stid);\r\nif (!cnp) {\r\npr_err("%s connect request on invalid stid %d\n",\r\n__func__, stid);\r\ngoto rel_skb;\r\n}\r\nif (cnp->com.state != CSK_STATE_LISTEN) {\r\npr_err("%s - listening parent not in CSK_STATE_LISTEN\n",\r\n__func__);\r\ngoto reject;\r\n}\r\ncsk = lookup_tid(t, tid);\r\nif (csk) {\r\npr_err("%s csk not null tid %u\n",\r\n__func__, tid);\r\ngoto rel_skb;\r\n}\r\ncxgb_get_4tuple(req, cdev->lldi.adapter_type, &iptype, local_ip,\r\npeer_ip, &local_port, &peer_port);\r\nif (iptype == 4) {\r\npr_debug("%s parent sock %p tid %u laddr %pI4 raddr %pI4 "\r\n"lport %d rport %d peer_mss %d\n"\r\n, __func__, cnp, tid,\r\nlocal_ip, peer_ip, ntohs(local_port),\r\nntohs(peer_port), peer_mss);\r\ndst = cxgb_find_route(&cdev->lldi, cxgbit_get_real_dev,\r\n*(__be32 *)local_ip,\r\n*(__be32 *)peer_ip,\r\nlocal_port, peer_port,\r\nPASS_OPEN_TOS_G(ntohl(req->tos_stid)));\r\n} else {\r\npr_debug("%s parent sock %p tid %u laddr %pI6 raddr %pI6 "\r\n"lport %d rport %d peer_mss %d\n"\r\n, __func__, cnp, tid,\r\nlocal_ip, peer_ip, ntohs(local_port),\r\nntohs(peer_port), peer_mss);\r\ndst = cxgb_find_route6(&cdev->lldi, cxgbit_get_real_dev,\r\nlocal_ip, peer_ip,\r\nlocal_port, peer_port,\r\nPASS_OPEN_TOS_G(ntohl(req->tos_stid)),\r\n((struct sockaddr_in6 *)\r\n&cnp->com.local_addr)->sin6_scope_id);\r\n}\r\nif (!dst) {\r\npr_err("%s - failed to find dst entry!\n",\r\n__func__);\r\ngoto reject;\r\n}\r\ncsk = kzalloc(sizeof(*csk), GFP_ATOMIC);\r\nif (!csk) {\r\ndst_release(dst);\r\ngoto rel_skb;\r\n}\r\nret = cxgbit_offload_init(csk, iptype, peer_ip, ntohs(local_port),\r\ndst, cdev);\r\nif (ret) {\r\npr_err("%s - failed to allocate l2t entry!\n",\r\n__func__);\r\ndst_release(dst);\r\nkfree(csk);\r\ngoto reject;\r\n}\r\nkref_init(&csk->kref);\r\ninit_completion(&csk->com.wr_wait.completion);\r\nINIT_LIST_HEAD(&csk->accept_node);\r\nhdrs = (iptype == 4 ? sizeof(struct iphdr) : sizeof(struct ipv6hdr)) +\r\nsizeof(struct tcphdr) + (req->tcpopt.tstamp ? 12 : 0);\r\nif (peer_mss && csk->mtu > (peer_mss + hdrs))\r\ncsk->mtu = peer_mss + hdrs;\r\ncsk->com.state = CSK_STATE_CONNECTING;\r\ncsk->com.cdev = cdev;\r\ncsk->cnp = cnp;\r\ncsk->tos = PASS_OPEN_TOS_G(ntohl(req->tos_stid));\r\ncsk->dst = dst;\r\ncsk->tid = tid;\r\ncsk->wr_cred = cdev->lldi.wr_cred -\r\nDIV_ROUND_UP(sizeof(struct cpl_abort_req), 16);\r\ncsk->wr_max_cred = csk->wr_cred;\r\ncsk->wr_una_cred = 0;\r\nif (iptype == 4) {\r\nstruct sockaddr_in *sin = (struct sockaddr_in *)\r\n&csk->com.local_addr;\r\nsin->sin_family = AF_INET;\r\nsin->sin_port = local_port;\r\nsin->sin_addr.s_addr = *(__be32 *)local_ip;\r\nsin = (struct sockaddr_in *)&csk->com.remote_addr;\r\nsin->sin_family = AF_INET;\r\nsin->sin_port = peer_port;\r\nsin->sin_addr.s_addr = *(__be32 *)peer_ip;\r\n} else {\r\nstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)\r\n&csk->com.local_addr;\r\nsin6->sin6_family = PF_INET6;\r\nsin6->sin6_port = local_port;\r\nmemcpy(sin6->sin6_addr.s6_addr, local_ip, 16);\r\ncxgb4_clip_get(cdev->lldi.ports[0],\r\n(const u32 *)&sin6->sin6_addr.s6_addr,\r\n1);\r\nsin6 = (struct sockaddr_in6 *)&csk->com.remote_addr;\r\nsin6->sin6_family = PF_INET6;\r\nsin6->sin6_port = peer_port;\r\nmemcpy(sin6->sin6_addr.s6_addr, peer_ip, 16);\r\n}\r\nskb_queue_head_init(&csk->rxq);\r\nskb_queue_head_init(&csk->txq);\r\nskb_queue_head_init(&csk->ppodq);\r\nskb_queue_head_init(&csk->backlogq);\r\nskb_queue_head_init(&csk->skbq);\r\ncxgbit_sock_reset_wr_list(csk);\r\nspin_lock_init(&csk->lock);\r\ninit_waitqueue_head(&csk->waitq);\r\ninit_waitqueue_head(&csk->ack_waitq);\r\ncsk->lock_owner = false;\r\nif (cxgbit_alloc_csk_skb(csk)) {\r\ndst_release(dst);\r\nkfree(csk);\r\ngoto rel_skb;\r\n}\r\ncxgbit_get_cdev(cdev);\r\nspin_lock(&cdev->cskq.lock);\r\nlist_add_tail(&csk->list, &cdev->cskq.list);\r\nspin_unlock(&cdev->cskq.lock);\r\ncxgb4_insert_tid(t, csk, tid, csk->com.local_addr.ss_family);\r\ncxgbit_pass_accept_rpl(csk, req);\r\ngoto rel_skb;\r\nreject:\r\ncxgbit_release_tid(cdev, tid);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic u32\r\ncxgbit_tx_flowc_wr_credits(struct cxgbit_sock *csk, u32 *nparamsp,\r\nu32 *flowclenp)\r\n{\r\nu32 nparams, flowclen16, flowclen;\r\nnparams = FLOWC_WR_NPARAMS_MIN;\r\nif (csk->snd_wscale)\r\nnparams++;\r\n#ifdef CONFIG_CHELSIO_T4_DCB\r\nnparams++;\r\n#endif\r\nflowclen = offsetof(struct fw_flowc_wr, mnemval[nparams]);\r\nflowclen16 = DIV_ROUND_UP(flowclen, 16);\r\nflowclen = flowclen16 * 16;\r\nif (nparamsp)\r\n*nparamsp = nparams;\r\nif (flowclenp)\r\n*flowclenp = flowclen;\r\nreturn flowclen16;\r\n}\r\nu32 cxgbit_send_tx_flowc_wr(struct cxgbit_sock *csk)\r\n{\r\nstruct cxgbit_device *cdev = csk->com.cdev;\r\nstruct fw_flowc_wr *flowc;\r\nu32 nparams, flowclen16, flowclen;\r\nstruct sk_buff *skb;\r\nu8 index;\r\n#ifdef CONFIG_CHELSIO_T4_DCB\r\nu16 vlan = ((struct l2t_entry *)csk->l2t)->vlan;\r\n#endif\r\nflowclen16 = cxgbit_tx_flowc_wr_credits(csk, &nparams, &flowclen);\r\nskb = __skb_dequeue(&csk->skbq);\r\nflowc = __skb_put_zero(skb, flowclen);\r\nflowc->op_to_nparams = cpu_to_be32(FW_WR_OP_V(FW_FLOWC_WR) |\r\nFW_FLOWC_WR_NPARAMS_V(nparams));\r\nflowc->flowid_len16 = cpu_to_be32(FW_WR_LEN16_V(flowclen16) |\r\nFW_WR_FLOWID_V(csk->tid));\r\nflowc->mnemval[0].mnemonic = FW_FLOWC_MNEM_PFNVFN;\r\nflowc->mnemval[0].val = cpu_to_be32(FW_PFVF_CMD_PFN_V\r\n(csk->com.cdev->lldi.pf));\r\nflowc->mnemval[1].mnemonic = FW_FLOWC_MNEM_CH;\r\nflowc->mnemval[1].val = cpu_to_be32(csk->tx_chan);\r\nflowc->mnemval[2].mnemonic = FW_FLOWC_MNEM_PORT;\r\nflowc->mnemval[2].val = cpu_to_be32(csk->tx_chan);\r\nflowc->mnemval[3].mnemonic = FW_FLOWC_MNEM_IQID;\r\nflowc->mnemval[3].val = cpu_to_be32(csk->rss_qid);\r\nflowc->mnemval[4].mnemonic = FW_FLOWC_MNEM_SNDNXT;\r\nflowc->mnemval[4].val = cpu_to_be32(csk->snd_nxt);\r\nflowc->mnemval[5].mnemonic = FW_FLOWC_MNEM_RCVNXT;\r\nflowc->mnemval[5].val = cpu_to_be32(csk->rcv_nxt);\r\nflowc->mnemval[6].mnemonic = FW_FLOWC_MNEM_SNDBUF;\r\nflowc->mnemval[6].val = cpu_to_be32(csk->snd_win);\r\nflowc->mnemval[7].mnemonic = FW_FLOWC_MNEM_MSS;\r\nflowc->mnemval[7].val = cpu_to_be32(csk->emss);\r\nflowc->mnemval[8].mnemonic = FW_FLOWC_MNEM_TXDATAPLEN_MAX;\r\nif (test_bit(CDEV_ISO_ENABLE, &cdev->flags))\r\nflowc->mnemval[8].val = cpu_to_be32(CXGBIT_MAX_ISO_PAYLOAD);\r\nelse\r\nflowc->mnemval[8].val = cpu_to_be32(16384);\r\nindex = 9;\r\nif (csk->snd_wscale) {\r\nflowc->mnemval[index].mnemonic = FW_FLOWC_MNEM_RCV_SCALE;\r\nflowc->mnemval[index].val = cpu_to_be32(csk->snd_wscale);\r\nindex++;\r\n}\r\n#ifdef CONFIG_CHELSIO_T4_DCB\r\nflowc->mnemval[index].mnemonic = FW_FLOWC_MNEM_DCBPRIO;\r\nif (vlan == VLAN_NONE) {\r\npr_warn("csk %u without VLAN Tag on DCB Link\n", csk->tid);\r\nflowc->mnemval[index].val = cpu_to_be32(0);\r\n} else\r\nflowc->mnemval[index].val = cpu_to_be32(\r\n(vlan & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT);\r\n#endif\r\npr_debug("%s: csk %p; tx_chan = %u; rss_qid = %u; snd_seq = %u;"\r\n" rcv_seq = %u; snd_win = %u; emss = %u\n",\r\n__func__, csk, csk->tx_chan, csk->rss_qid, csk->snd_nxt,\r\ncsk->rcv_nxt, csk->snd_win, csk->emss);\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, csk->txq_idx);\r\ncxgbit_ofld_send(csk->com.cdev, skb);\r\nreturn flowclen16;\r\n}\r\nint cxgbit_setup_conn_digest(struct cxgbit_sock *csk)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cpl_set_tcb_field *req;\r\nu8 hcrc = csk->submode & CXGBIT_SUBMODE_HCRC;\r\nu8 dcrc = csk->submode & CXGBIT_SUBMODE_DCRC;\r\nunsigned int len = roundup(sizeof(*req), 16);\r\nint ret;\r\nskb = alloc_skb(len, GFP_KERNEL);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nreq = __skb_put_zero(skb, len);\r\nINIT_TP_WR(req, csk->tid);\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, csk->tid));\r\nreq->reply_ctrl = htons(NO_REPLY_V(0) | QUEUENO_V(csk->rss_qid));\r\nreq->word_cookie = htons(0);\r\nreq->mask = cpu_to_be64(0x3 << 4);\r\nreq->val = cpu_to_be64(((hcrc ? ULP_CRC_HEADER : 0) |\r\n(dcrc ? ULP_CRC_DATA : 0)) << 4);\r\nset_wr_txq(skb, CPL_PRIORITY_CONTROL, csk->ctrlq_idx);\r\ncxgbit_get_csk(csk);\r\ncxgbit_init_wr_wait(&csk->com.wr_wait);\r\ncxgbit_ofld_send(csk->com.cdev, skb);\r\nret = cxgbit_wait_for_reply(csk->com.cdev,\r\n&csk->com.wr_wait,\r\ncsk->tid, 5, __func__);\r\nif (ret)\r\nreturn -1;\r\nreturn 0;\r\n}\r\nint cxgbit_setup_conn_pgidx(struct cxgbit_sock *csk, u32 pg_idx)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cpl_set_tcb_field *req;\r\nunsigned int len = roundup(sizeof(*req), 16);\r\nint ret;\r\nskb = alloc_skb(len, GFP_KERNEL);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nreq = __skb_put_zero(skb, len);\r\nINIT_TP_WR(req, csk->tid);\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, csk->tid));\r\nreq->reply_ctrl = htons(NO_REPLY_V(0) | QUEUENO_V(csk->rss_qid));\r\nreq->word_cookie = htons(0);\r\nreq->mask = cpu_to_be64(0x3 << 8);\r\nreq->val = cpu_to_be64(pg_idx << 8);\r\nset_wr_txq(skb, CPL_PRIORITY_CONTROL, csk->ctrlq_idx);\r\ncxgbit_get_csk(csk);\r\ncxgbit_init_wr_wait(&csk->com.wr_wait);\r\ncxgbit_ofld_send(csk->com.cdev, skb);\r\nret = cxgbit_wait_for_reply(csk->com.cdev,\r\n&csk->com.wr_wait,\r\ncsk->tid, 5, __func__);\r\nif (ret)\r\nreturn -1;\r\nreturn 0;\r\n}\r\nstatic void\r\ncxgbit_pass_open_rpl(struct cxgbit_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cpl_pass_open_rpl *rpl = cplhdr(skb);\r\nstruct tid_info *t = cdev->lldi.tids;\r\nunsigned int stid = GET_TID(rpl);\r\nstruct cxgbit_np *cnp = lookup_stid(t, stid);\r\npr_debug("%s: cnp = %p; stid = %u; status = %d\n",\r\n__func__, cnp, stid, rpl->status);\r\nif (!cnp) {\r\npr_info("%s stid %d lookup failure\n", __func__, stid);\r\ngoto rel_skb;\r\n}\r\ncxgbit_wake_up(&cnp->com.wr_wait, __func__, rpl->status);\r\ncxgbit_put_cnp(cnp);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void\r\ncxgbit_close_listsrv_rpl(struct cxgbit_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cpl_close_listsvr_rpl *rpl = cplhdr(skb);\r\nstruct tid_info *t = cdev->lldi.tids;\r\nunsigned int stid = GET_TID(rpl);\r\nstruct cxgbit_np *cnp = lookup_stid(t, stid);\r\npr_debug("%s: cnp = %p; stid = %u; status = %d\n",\r\n__func__, cnp, stid, rpl->status);\r\nif (!cnp) {\r\npr_info("%s stid %d lookup failure\n", __func__, stid);\r\ngoto rel_skb;\r\n}\r\ncxgbit_wake_up(&cnp->com.wr_wait, __func__, rpl->status);\r\ncxgbit_put_cnp(cnp);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void\r\ncxgbit_pass_establish(struct cxgbit_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cpl_pass_establish *req = cplhdr(skb);\r\nstruct tid_info *t = cdev->lldi.tids;\r\nunsigned int tid = GET_TID(req);\r\nstruct cxgbit_sock *csk;\r\nstruct cxgbit_np *cnp;\r\nu16 tcp_opt = be16_to_cpu(req->tcp_opt);\r\nu32 snd_isn = be32_to_cpu(req->snd_isn);\r\nu32 rcv_isn = be32_to_cpu(req->rcv_isn);\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk)) {\r\npr_err("can't find connection for tid %u.\n", tid);\r\ngoto rel_skb;\r\n}\r\ncnp = csk->cnp;\r\npr_debug("%s: csk %p; tid %u; cnp %p\n",\r\n__func__, csk, tid, cnp);\r\ncsk->write_seq = snd_isn;\r\ncsk->snd_una = snd_isn;\r\ncsk->snd_nxt = snd_isn;\r\ncsk->rcv_nxt = rcv_isn;\r\nif (csk->rcv_win > (RCV_BUFSIZ_M << 10))\r\ncsk->rx_credits = (csk->rcv_win - (RCV_BUFSIZ_M << 10));\r\ncsk->snd_wscale = TCPOPT_SND_WSCALE_G(tcp_opt);\r\ncxgbit_set_emss(csk, tcp_opt);\r\ndst_confirm(csk->dst);\r\ncsk->com.state = CSK_STATE_ESTABLISHED;\r\nspin_lock_bh(&cnp->np_accept_lock);\r\nlist_add_tail(&csk->accept_node, &cnp->np_accept_list);\r\nspin_unlock_bh(&cnp->np_accept_lock);\r\ncomplete(&cnp->accept_comp);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void cxgbit_queue_rx_skb(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\ncxgbit_skcb_flags(skb) = 0;\r\nspin_lock_bh(&csk->rxq.lock);\r\n__skb_queue_tail(&csk->rxq, skb);\r\nspin_unlock_bh(&csk->rxq.lock);\r\nwake_up(&csk->waitq);\r\n}\r\nstatic void cxgbit_peer_close(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\npr_debug("%s: csk %p; tid %u; state %d\n",\r\n__func__, csk, csk->tid, csk->com.state);\r\nswitch (csk->com.state) {\r\ncase CSK_STATE_ESTABLISHED:\r\ncsk->com.state = CSK_STATE_CLOSING;\r\ncxgbit_queue_rx_skb(csk, skb);\r\nreturn;\r\ncase CSK_STATE_CLOSING:\r\ncsk->com.state = CSK_STATE_MORIBUND;\r\nbreak;\r\ncase CSK_STATE_MORIBUND:\r\ncsk->com.state = CSK_STATE_DEAD;\r\ncxgbit_put_csk(csk);\r\nbreak;\r\ncase CSK_STATE_ABORTING:\r\nbreak;\r\ndefault:\r\npr_info("%s: cpl_peer_close in bad state %d\n",\r\n__func__, csk->com.state);\r\n}\r\n__kfree_skb(skb);\r\n}\r\nstatic void cxgbit_close_con_rpl(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\npr_debug("%s: csk %p; tid %u; state %d\n",\r\n__func__, csk, csk->tid, csk->com.state);\r\nswitch (csk->com.state) {\r\ncase CSK_STATE_CLOSING:\r\ncsk->com.state = CSK_STATE_MORIBUND;\r\nbreak;\r\ncase CSK_STATE_MORIBUND:\r\ncsk->com.state = CSK_STATE_DEAD;\r\ncxgbit_put_csk(csk);\r\nbreak;\r\ncase CSK_STATE_ABORTING:\r\ncase CSK_STATE_DEAD:\r\nbreak;\r\ndefault:\r\npr_info("%s: cpl_close_con_rpl in bad state %d\n",\r\n__func__, csk->com.state);\r\n}\r\n__kfree_skb(skb);\r\n}\r\nstatic void cxgbit_abort_req_rss(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\nstruct cpl_abort_req_rss *hdr = cplhdr(skb);\r\nunsigned int tid = GET_TID(hdr);\r\nstruct sk_buff *rpl_skb;\r\nbool release = false;\r\nbool wakeup_thread = false;\r\nu32 len = roundup(sizeof(struct cpl_abort_rpl), 16);\r\npr_debug("%s: csk %p; tid %u; state %d\n",\r\n__func__, csk, tid, csk->com.state);\r\nif (cxgb_is_neg_adv(hdr->status)) {\r\npr_err("%s: got neg advise %d on tid %u\n",\r\n__func__, hdr->status, tid);\r\ngoto rel_skb;\r\n}\r\nswitch (csk->com.state) {\r\ncase CSK_STATE_CONNECTING:\r\ncase CSK_STATE_MORIBUND:\r\ncsk->com.state = CSK_STATE_DEAD;\r\nrelease = true;\r\nbreak;\r\ncase CSK_STATE_ESTABLISHED:\r\ncsk->com.state = CSK_STATE_DEAD;\r\nwakeup_thread = true;\r\nbreak;\r\ncase CSK_STATE_CLOSING:\r\ncsk->com.state = CSK_STATE_DEAD;\r\nif (!csk->conn)\r\nrelease = true;\r\nbreak;\r\ncase CSK_STATE_ABORTING:\r\nbreak;\r\ndefault:\r\npr_info("%s: cpl_abort_req_rss in bad state %d\n",\r\n__func__, csk->com.state);\r\ncsk->com.state = CSK_STATE_DEAD;\r\n}\r\n__skb_queue_purge(&csk->txq);\r\nif (!test_and_set_bit(CSK_TX_DATA_SENT, &csk->com.flags))\r\ncxgbit_send_tx_flowc_wr(csk);\r\nrpl_skb = __skb_dequeue(&csk->skbq);\r\ncxgb_mk_abort_rpl(rpl_skb, len, csk->tid, csk->txq_idx);\r\ncxgbit_ofld_send(csk->com.cdev, rpl_skb);\r\nif (wakeup_thread) {\r\ncxgbit_queue_rx_skb(csk, skb);\r\nreturn;\r\n}\r\nif (release)\r\ncxgbit_put_csk(csk);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void cxgbit_abort_rpl_rss(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\npr_debug("%s: csk %p; tid %u; state %d\n",\r\n__func__, csk, csk->tid, csk->com.state);\r\nswitch (csk->com.state) {\r\ncase CSK_STATE_ABORTING:\r\ncsk->com.state = CSK_STATE_DEAD;\r\ncxgbit_put_csk(csk);\r\nbreak;\r\ndefault:\r\npr_info("%s: cpl_abort_rpl_rss in state %d\n",\r\n__func__, csk->com.state);\r\n}\r\n__kfree_skb(skb);\r\n}\r\nstatic bool cxgbit_credit_err(const struct cxgbit_sock *csk)\r\n{\r\nconst struct sk_buff *skb = csk->wr_pending_head;\r\nu32 credit = 0;\r\nif (unlikely(csk->wr_cred > csk->wr_max_cred)) {\r\npr_err("csk 0x%p, tid %u, credit %u > %u\n",\r\ncsk, csk->tid, csk->wr_cred, csk->wr_max_cred);\r\nreturn true;\r\n}\r\nwhile (skb) {\r\ncredit += (__force u32)skb->csum;\r\nskb = cxgbit_skcb_tx_wr_next(skb);\r\n}\r\nif (unlikely((csk->wr_cred + credit) != csk->wr_max_cred)) {\r\npr_err("csk 0x%p, tid %u, credit %u + %u != %u.\n",\r\ncsk, csk->tid, csk->wr_cred,\r\ncredit, csk->wr_max_cred);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void cxgbit_fw4_ack(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\nstruct cpl_fw4_ack *rpl = (struct cpl_fw4_ack *)cplhdr(skb);\r\nu32 credits = rpl->credits;\r\nu32 snd_una = ntohl(rpl->snd_una);\r\ncsk->wr_cred += credits;\r\nif (csk->wr_una_cred > (csk->wr_max_cred - csk->wr_cred))\r\ncsk->wr_una_cred = csk->wr_max_cred - csk->wr_cred;\r\nwhile (credits) {\r\nstruct sk_buff *p = cxgbit_sock_peek_wr(csk);\r\nconst u32 csum = (__force u32)p->csum;\r\nif (unlikely(!p)) {\r\npr_err("csk 0x%p,%u, cr %u,%u+%u, empty.\n",\r\ncsk, csk->tid, credits,\r\ncsk->wr_cred, csk->wr_una_cred);\r\nbreak;\r\n}\r\nif (unlikely(credits < csum)) {\r\npr_warn("csk 0x%p,%u, cr %u,%u+%u, < %u.\n",\r\ncsk, csk->tid,\r\ncredits, csk->wr_cred, csk->wr_una_cred,\r\ncsum);\r\np->csum = (__force __wsum)(csum - credits);\r\nbreak;\r\n}\r\ncxgbit_sock_dequeue_wr(csk);\r\ncredits -= csum;\r\nkfree_skb(p);\r\n}\r\nif (unlikely(cxgbit_credit_err(csk))) {\r\ncxgbit_queue_rx_skb(csk, skb);\r\nreturn;\r\n}\r\nif (rpl->seq_vld & CPL_FW4_ACK_FLAGS_SEQVAL) {\r\nif (unlikely(before(snd_una, csk->snd_una))) {\r\npr_warn("csk 0x%p,%u, snd_una %u/%u.",\r\ncsk, csk->tid, snd_una,\r\ncsk->snd_una);\r\ngoto rel_skb;\r\n}\r\nif (csk->snd_una != snd_una) {\r\ncsk->snd_una = snd_una;\r\ndst_confirm(csk->dst);\r\nwake_up(&csk->ack_waitq);\r\n}\r\n}\r\nif (skb_queue_len(&csk->txq))\r\ncxgbit_push_tx_frames(csk);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void cxgbit_set_tcb_rpl(struct cxgbit_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbit_sock *csk;\r\nstruct cpl_set_tcb_rpl *rpl = (struct cpl_set_tcb_rpl *)skb->data;\r\nunsigned int tid = GET_TID(rpl);\r\nstruct cxgb4_lld_info *lldi = &cdev->lldi;\r\nstruct tid_info *t = lldi->tids;\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk)) {\r\npr_err("can't find connection for tid %u.\n", tid);\r\ngoto rel_skb;\r\n} else {\r\ncxgbit_wake_up(&csk->com.wr_wait, __func__, rpl->status);\r\n}\r\ncxgbit_put_csk(csk);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void cxgbit_rx_data(struct cxgbit_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbit_sock *csk;\r\nstruct cpl_rx_data *cpl = cplhdr(skb);\r\nunsigned int tid = GET_TID(cpl);\r\nstruct cxgb4_lld_info *lldi = &cdev->lldi;\r\nstruct tid_info *t = lldi->tids;\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk)) {\r\npr_err("can't find conn. for tid %u.\n", tid);\r\ngoto rel_skb;\r\n}\r\ncxgbit_queue_rx_skb(csk, skb);\r\nreturn;\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void\r\n__cxgbit_process_rx_cpl(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\nspin_lock(&csk->lock);\r\nif (csk->lock_owner) {\r\n__skb_queue_tail(&csk->backlogq, skb);\r\nspin_unlock(&csk->lock);\r\nreturn;\r\n}\r\ncxgbit_skcb_rx_backlog_fn(skb)(csk, skb);\r\nspin_unlock(&csk->lock);\r\n}\r\nstatic void cxgbit_process_rx_cpl(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\ncxgbit_get_csk(csk);\r\n__cxgbit_process_rx_cpl(csk, skb);\r\ncxgbit_put_csk(csk);\r\n}\r\nstatic void cxgbit_rx_cpl(struct cxgbit_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbit_sock *csk;\r\nstruct cpl_tx_data *cpl = cplhdr(skb);\r\nstruct cxgb4_lld_info *lldi = &cdev->lldi;\r\nstruct tid_info *t = lldi->tids;\r\nunsigned int tid = GET_TID(cpl);\r\nu8 opcode = cxgbit_skcb_rx_opcode(skb);\r\nbool ref = true;\r\nswitch (opcode) {\r\ncase CPL_FW4_ACK:\r\ncxgbit_skcb_rx_backlog_fn(skb) = cxgbit_fw4_ack;\r\nref = false;\r\nbreak;\r\ncase CPL_PEER_CLOSE:\r\ncxgbit_skcb_rx_backlog_fn(skb) = cxgbit_peer_close;\r\nbreak;\r\ncase CPL_CLOSE_CON_RPL:\r\ncxgbit_skcb_rx_backlog_fn(skb) = cxgbit_close_con_rpl;\r\nbreak;\r\ncase CPL_ABORT_REQ_RSS:\r\ncxgbit_skcb_rx_backlog_fn(skb) = cxgbit_abort_req_rss;\r\nbreak;\r\ncase CPL_ABORT_RPL_RSS:\r\ncxgbit_skcb_rx_backlog_fn(skb) = cxgbit_abort_rpl_rss;\r\nbreak;\r\ndefault:\r\ngoto rel_skb;\r\n}\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk)) {\r\npr_err("can't find conn. for tid %u.\n", tid);\r\ngoto rel_skb;\r\n}\r\nif (ref)\r\ncxgbit_process_rx_cpl(csk, skb);\r\nelse\r\n__cxgbit_process_rx_cpl(csk, skb);\r\nreturn;\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}
