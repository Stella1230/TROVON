static void klp_transition_work_fn(struct work_struct *work)\r\n{\r\nmutex_lock(&klp_mutex);\r\nif (klp_transition_patch)\r\nklp_try_complete_transition();\r\nmutex_unlock(&klp_mutex);\r\n}\r\nstatic void klp_sync(struct work_struct *work)\r\n{\r\n}\r\nstatic void klp_synchronize_transition(void)\r\n{\r\nschedule_on_each_cpu(klp_sync);\r\n}\r\nstatic void klp_complete_transition(void)\r\n{\r\nstruct klp_object *obj;\r\nstruct klp_func *func;\r\nstruct task_struct *g, *task;\r\nunsigned int cpu;\r\nbool immediate_func = false;\r\nif (klp_target_state == KLP_UNPATCHED) {\r\nklp_unpatch_objects(klp_transition_patch);\r\nklp_synchronize_transition();\r\n}\r\nif (klp_transition_patch->immediate)\r\ngoto done;\r\nklp_for_each_object(klp_transition_patch, obj) {\r\nklp_for_each_func(obj, func) {\r\nfunc->transition = false;\r\nif (func->immediate)\r\nimmediate_func = true;\r\n}\r\n}\r\nif (klp_target_state == KLP_UNPATCHED && !immediate_func)\r\nmodule_put(klp_transition_patch->mod);\r\nif (klp_target_state == KLP_PATCHED)\r\nklp_synchronize_transition();\r\nread_lock(&tasklist_lock);\r\nfor_each_process_thread(g, task) {\r\nWARN_ON_ONCE(test_tsk_thread_flag(task, TIF_PATCH_PENDING));\r\ntask->patch_state = KLP_UNDEFINED;\r\n}\r\nread_unlock(&tasklist_lock);\r\nfor_each_possible_cpu(cpu) {\r\ntask = idle_task(cpu);\r\nWARN_ON_ONCE(test_tsk_thread_flag(task, TIF_PATCH_PENDING));\r\ntask->patch_state = KLP_UNDEFINED;\r\n}\r\ndone:\r\nklp_target_state = KLP_UNDEFINED;\r\nklp_transition_patch = NULL;\r\n}\r\nvoid klp_cancel_transition(void)\r\n{\r\nif (WARN_ON_ONCE(klp_target_state != KLP_PATCHED))\r\nreturn;\r\nklp_target_state = KLP_UNPATCHED;\r\nklp_complete_transition();\r\n}\r\nvoid klp_update_patch_state(struct task_struct *task)\r\n{\r\npreempt_disable_notrace();\r\nif (test_and_clear_tsk_thread_flag(task, TIF_PATCH_PENDING))\r\ntask->patch_state = READ_ONCE(klp_target_state);\r\npreempt_enable_notrace();\r\n}\r\nstatic int klp_check_stack_func(struct klp_func *func,\r\nstruct stack_trace *trace)\r\n{\r\nunsigned long func_addr, func_size, address;\r\nstruct klp_ops *ops;\r\nint i;\r\nif (func->immediate)\r\nreturn 0;\r\nfor (i = 0; i < trace->nr_entries; i++) {\r\naddress = trace->entries[i];\r\nif (klp_target_state == KLP_UNPATCHED) {\r\nfunc_addr = (unsigned long)func->new_func;\r\nfunc_size = func->new_size;\r\n} else {\r\nops = klp_find_ops(func->old_addr);\r\nif (list_is_singular(&ops->func_stack)) {\r\nfunc_addr = func->old_addr;\r\nfunc_size = func->old_size;\r\n} else {\r\nstruct klp_func *prev;\r\nprev = list_next_entry(func, stack_node);\r\nfunc_addr = (unsigned long)prev->new_func;\r\nfunc_size = prev->new_size;\r\n}\r\n}\r\nif (address >= func_addr && address < func_addr + func_size)\r\nreturn -EAGAIN;\r\n}\r\nreturn 0;\r\n}\r\nstatic int klp_check_stack(struct task_struct *task, char *err_buf)\r\n{\r\nstatic unsigned long entries[MAX_STACK_ENTRIES];\r\nstruct stack_trace trace;\r\nstruct klp_object *obj;\r\nstruct klp_func *func;\r\nint ret;\r\ntrace.skip = 0;\r\ntrace.nr_entries = 0;\r\ntrace.max_entries = MAX_STACK_ENTRIES;\r\ntrace.entries = entries;\r\nret = save_stack_trace_tsk_reliable(task, &trace);\r\nWARN_ON_ONCE(ret == -ENOSYS);\r\nif (ret) {\r\nsnprintf(err_buf, STACK_ERR_BUF_SIZE,\r\n"%s: %s:%d has an unreliable stack\n",\r\n__func__, task->comm, task->pid);\r\nreturn ret;\r\n}\r\nklp_for_each_object(klp_transition_patch, obj) {\r\nif (!obj->patched)\r\ncontinue;\r\nklp_for_each_func(obj, func) {\r\nret = klp_check_stack_func(func, &trace);\r\nif (ret) {\r\nsnprintf(err_buf, STACK_ERR_BUF_SIZE,\r\n"%s: %s:%d is sleeping on function %s\n",\r\n__func__, task->comm, task->pid,\r\nfunc->old_name);\r\nreturn ret;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic bool klp_try_switch_task(struct task_struct *task)\r\n{\r\nstruct rq *rq;\r\nstruct rq_flags flags;\r\nint ret;\r\nbool success = false;\r\nchar err_buf[STACK_ERR_BUF_SIZE];\r\nerr_buf[0] = '\0';\r\nif (task->patch_state == klp_target_state)\r\nreturn true;\r\nif (!klp_have_reliable_stack())\r\nreturn false;\r\nrq = task_rq_lock(task, &flags);\r\nif (task_running(rq, task) && task != current) {\r\nsnprintf(err_buf, STACK_ERR_BUF_SIZE,\r\n"%s: %s:%d is running\n", __func__, task->comm,\r\ntask->pid);\r\ngoto done;\r\n}\r\nret = klp_check_stack(task, err_buf);\r\nif (ret)\r\ngoto done;\r\nsuccess = true;\r\nclear_tsk_thread_flag(task, TIF_PATCH_PENDING);\r\ntask->patch_state = klp_target_state;\r\ndone:\r\ntask_rq_unlock(rq, task, &flags);\r\nif (err_buf[0] != '\0')\r\npr_debug("%s", err_buf);\r\nreturn success;\r\n}\r\nvoid klp_try_complete_transition(void)\r\n{\r\nunsigned int cpu;\r\nstruct task_struct *g, *task;\r\nbool complete = true;\r\nWARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);\r\nif (klp_transition_patch->immediate)\r\ngoto success;\r\nread_lock(&tasklist_lock);\r\nfor_each_process_thread(g, task)\r\nif (!klp_try_switch_task(task))\r\ncomplete = false;\r\nread_unlock(&tasklist_lock);\r\nget_online_cpus();\r\nfor_each_possible_cpu(cpu) {\r\ntask = idle_task(cpu);\r\nif (cpu_online(cpu)) {\r\nif (!klp_try_switch_task(task))\r\ncomplete = false;\r\n} else if (task->patch_state != klp_target_state) {\r\nclear_tsk_thread_flag(task, TIF_PATCH_PENDING);\r\ntask->patch_state = klp_target_state;\r\n}\r\n}\r\nput_online_cpus();\r\nif (!complete) {\r\nschedule_delayed_work(&klp_transition_work,\r\nround_jiffies_relative(HZ));\r\nreturn;\r\n}\r\nsuccess:\r\npr_notice("'%s': %s complete\n", klp_transition_patch->mod->name,\r\nklp_target_state == KLP_PATCHED ? "patching" : "unpatching");\r\nklp_complete_transition();\r\n}\r\nvoid klp_start_transition(void)\r\n{\r\nstruct task_struct *g, *task;\r\nunsigned int cpu;\r\nWARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);\r\npr_notice("'%s': %s...\n", klp_transition_patch->mod->name,\r\nklp_target_state == KLP_PATCHED ? "patching" : "unpatching");\r\nif (klp_transition_patch->immediate)\r\nreturn;\r\nread_lock(&tasklist_lock);\r\nfor_each_process_thread(g, task)\r\nif (task->patch_state != klp_target_state)\r\nset_tsk_thread_flag(task, TIF_PATCH_PENDING);\r\nread_unlock(&tasklist_lock);\r\nfor_each_possible_cpu(cpu) {\r\ntask = idle_task(cpu);\r\nif (task->patch_state != klp_target_state)\r\nset_tsk_thread_flag(task, TIF_PATCH_PENDING);\r\n}\r\n}\r\nvoid klp_init_transition(struct klp_patch *patch, int state)\r\n{\r\nstruct task_struct *g, *task;\r\nunsigned int cpu;\r\nstruct klp_object *obj;\r\nstruct klp_func *func;\r\nint initial_state = !state;\r\nWARN_ON_ONCE(klp_target_state != KLP_UNDEFINED);\r\nklp_transition_patch = patch;\r\nklp_target_state = state;\r\nif (patch->immediate)\r\nreturn;\r\nread_lock(&tasklist_lock);\r\nfor_each_process_thread(g, task) {\r\nWARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);\r\ntask->patch_state = initial_state;\r\n}\r\nread_unlock(&tasklist_lock);\r\nfor_each_possible_cpu(cpu) {\r\ntask = idle_task(cpu);\r\nWARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);\r\ntask->patch_state = initial_state;\r\n}\r\nsmp_wmb();\r\nklp_for_each_object(patch, obj)\r\nklp_for_each_func(obj, func)\r\nfunc->transition = true;\r\n}\r\nvoid klp_reverse_transition(void)\r\n{\r\nunsigned int cpu;\r\nstruct task_struct *g, *task;\r\nklp_transition_patch->enabled = !klp_transition_patch->enabled;\r\nklp_target_state = !klp_target_state;\r\nread_lock(&tasklist_lock);\r\nfor_each_process_thread(g, task)\r\nclear_tsk_thread_flag(task, TIF_PATCH_PENDING);\r\nread_unlock(&tasklist_lock);\r\nfor_each_possible_cpu(cpu)\r\nclear_tsk_thread_flag(idle_task(cpu), TIF_PATCH_PENDING);\r\nklp_synchronize_transition();\r\nklp_start_transition();\r\n}\r\nvoid klp_copy_process(struct task_struct *child)\r\n{\r\nchild->patch_state = current->patch_state;\r\n}
