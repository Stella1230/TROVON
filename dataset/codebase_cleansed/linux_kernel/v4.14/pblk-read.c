static int pblk_read_from_cache(struct pblk *pblk, struct bio *bio,\r\nsector_t lba, struct ppa_addr ppa,\r\nint bio_iter, bool advanced_bio)\r\n{\r\n#ifdef CONFIG_NVM_DEBUG\r\nBUG_ON(pblk_ppa_empty(ppa));\r\nBUG_ON(!pblk_addr_in_cache(ppa));\r\n#endif\r\nreturn pblk_rb_copy_to_bio(&pblk->rwb, bio, lba, ppa,\r\nbio_iter, advanced_bio);\r\n}\r\nstatic void pblk_read_ppalist_rq(struct pblk *pblk, struct nvm_rq *rqd,\r\nunsigned long *read_bitmap)\r\n{\r\nstruct bio *bio = rqd->bio;\r\nstruct ppa_addr ppas[PBLK_MAX_REQ_ADDRS];\r\nsector_t blba = pblk_get_lba(bio);\r\nint nr_secs = rqd->nr_ppas;\r\nbool advanced_bio = false;\r\nint i, j = 0;\r\nif (blba + nr_secs >= pblk->rl.nr_secs) {\r\nWARN(1, "pblk: read lbas out of bounds\n");\r\nreturn;\r\n}\r\npblk_lookup_l2p_seq(pblk, ppas, blba, nr_secs);\r\nfor (i = 0; i < nr_secs; i++) {\r\nstruct ppa_addr p = ppas[i];\r\nsector_t lba = blba + i;\r\nretry:\r\nif (pblk_ppa_empty(p)) {\r\nWARN_ON(test_and_set_bit(i, read_bitmap));\r\nif (unlikely(!advanced_bio)) {\r\nbio_advance(bio, (i) * PBLK_EXPOSED_PAGE_SIZE);\r\nadvanced_bio = true;\r\n}\r\ngoto next;\r\n}\r\nif (pblk_addr_in_cache(p)) {\r\nif (!pblk_read_from_cache(pblk, bio, lba, p, i,\r\nadvanced_bio)) {\r\npblk_lookup_l2p_seq(pblk, &p, lba, 1);\r\ngoto retry;\r\n}\r\nWARN_ON(test_and_set_bit(i, read_bitmap));\r\nadvanced_bio = true;\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_inc(&pblk->cache_reads);\r\n#endif\r\n} else {\r\nrqd->ppa_list[j++] = p;\r\n}\r\nnext:\r\nif (advanced_bio)\r\nbio_advance(bio, PBLK_EXPOSED_PAGE_SIZE);\r\n}\r\nif (pblk_io_aligned(pblk, nr_secs))\r\nrqd->flags = pblk_set_read_mode(pblk, PBLK_READ_SEQUENTIAL);\r\nelse\r\nrqd->flags = pblk_set_read_mode(pblk, PBLK_READ_RANDOM);\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_add(nr_secs, &pblk->inflight_reads);\r\n#endif\r\n}\r\nstatic int pblk_submit_read_io(struct pblk *pblk, struct nvm_rq *rqd)\r\n{\r\nint err;\r\nerr = pblk_submit_io(pblk, rqd);\r\nif (err)\r\nreturn NVM_IO_ERR;\r\nreturn NVM_IO_OK;\r\n}\r\nstatic void pblk_end_io_read(struct nvm_rq *rqd)\r\n{\r\nstruct pblk *pblk = rqd->private;\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct pblk_g_ctx *r_ctx = nvm_rq_to_pdu(rqd);\r\nstruct bio *bio = rqd->bio;\r\nif (rqd->error)\r\npblk_log_read_err(pblk, rqd);\r\n#ifdef CONFIG_NVM_DEBUG\r\nelse\r\nWARN_ONCE(bio->bi_status, "pblk: corrupted read error\n");\r\n#endif\r\nnvm_dev_dma_free(dev->parent, rqd->meta_list, rqd->dma_meta_list);\r\nbio_put(bio);\r\nif (r_ctx->private) {\r\nstruct bio *orig_bio = r_ctx->private;\r\n#ifdef CONFIG_NVM_DEBUG\r\nWARN_ONCE(orig_bio->bi_status, "pblk: corrupted read bio\n");\r\n#endif\r\nbio_endio(orig_bio);\r\nbio_put(orig_bio);\r\n}\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_add(rqd->nr_ppas, &pblk->sync_reads);\r\natomic_long_sub(rqd->nr_ppas, &pblk->inflight_reads);\r\n#endif\r\npblk_free_rqd(pblk, rqd, READ);\r\natomic_dec(&pblk->inflight_io);\r\n}\r\nstatic int pblk_fill_partial_read_bio(struct pblk *pblk, struct nvm_rq *rqd,\r\nunsigned int bio_init_idx,\r\nunsigned long *read_bitmap)\r\n{\r\nstruct bio *new_bio, *bio = rqd->bio;\r\nstruct bio_vec src_bv, dst_bv;\r\nvoid *ppa_ptr = NULL;\r\nvoid *src_p, *dst_p;\r\ndma_addr_t dma_ppa_list = 0;\r\nint nr_secs = rqd->nr_ppas;\r\nint nr_holes = nr_secs - bitmap_weight(read_bitmap, nr_secs);\r\nint i, ret, hole;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nnew_bio = bio_alloc(GFP_KERNEL, nr_holes);\r\nif (!new_bio) {\r\npr_err("pblk: could not alloc read bio\n");\r\nreturn NVM_IO_ERR;\r\n}\r\nif (pblk_bio_add_pages(pblk, new_bio, GFP_KERNEL, nr_holes))\r\ngoto err;\r\nif (nr_holes != new_bio->bi_vcnt) {\r\npr_err("pblk: malformed bio\n");\r\ngoto err;\r\n}\r\nnew_bio->bi_iter.bi_sector = 0;\r\nbio_set_op_attrs(new_bio, REQ_OP_READ, 0);\r\nnew_bio->bi_private = &wait;\r\nnew_bio->bi_end_io = pblk_end_bio_sync;\r\nrqd->bio = new_bio;\r\nrqd->nr_ppas = nr_holes;\r\nrqd->flags = pblk_set_read_mode(pblk, PBLK_READ_RANDOM);\r\nrqd->end_io = NULL;\r\nif (unlikely(nr_secs > 1 && nr_holes == 1)) {\r\nppa_ptr = rqd->ppa_list;\r\ndma_ppa_list = rqd->dma_ppa_list;\r\nrqd->ppa_addr = rqd->ppa_list[0];\r\n}\r\nret = pblk_submit_read_io(pblk, rqd);\r\nif (ret) {\r\nbio_put(rqd->bio);\r\npr_err("pblk: read IO submission failed\n");\r\ngoto err;\r\n}\r\nif (!wait_for_completion_io_timeout(&wait,\r\nmsecs_to_jiffies(PBLK_COMMAND_TIMEOUT_MS))) {\r\npr_err("pblk: partial read I/O timed out\n");\r\n}\r\nif (rqd->error) {\r\natomic_long_inc(&pblk->read_failed);\r\n#ifdef CONFIG_NVM_DEBUG\r\npblk_print_failed_rqd(pblk, rqd, rqd->error);\r\n#endif\r\n}\r\nif (unlikely(nr_secs > 1 && nr_holes == 1)) {\r\nrqd->ppa_list = ppa_ptr;\r\nrqd->dma_ppa_list = dma_ppa_list;\r\n}\r\ni = 0;\r\nhole = find_first_zero_bit(read_bitmap, nr_secs);\r\ndo {\r\nsrc_bv = new_bio->bi_io_vec[i++];\r\ndst_bv = bio->bi_io_vec[bio_init_idx + hole];\r\nsrc_p = kmap_atomic(src_bv.bv_page);\r\ndst_p = kmap_atomic(dst_bv.bv_page);\r\nmemcpy(dst_p + dst_bv.bv_offset,\r\nsrc_p + src_bv.bv_offset,\r\nPBLK_EXPOSED_PAGE_SIZE);\r\nkunmap_atomic(src_p);\r\nkunmap_atomic(dst_p);\r\nmempool_free(src_bv.bv_page, pblk->page_pool);\r\nhole = find_next_zero_bit(read_bitmap, nr_secs, hole + 1);\r\n} while (hole < nr_secs);\r\nbio_put(new_bio);\r\nrqd->bio = bio;\r\nrqd->nr_ppas = nr_secs;\r\nrqd->private = pblk;\r\nbio_endio(bio);\r\npblk_end_io_read(rqd);\r\nreturn NVM_IO_OK;\r\nerr:\r\npblk_bio_free_pages(pblk, bio, 0, new_bio->bi_vcnt);\r\nrqd->private = pblk;\r\npblk_end_io_read(rqd);\r\nreturn NVM_IO_ERR;\r\n}\r\nstatic void pblk_read_rq(struct pblk *pblk, struct nvm_rq *rqd,\r\nunsigned long *read_bitmap)\r\n{\r\nstruct bio *bio = rqd->bio;\r\nstruct ppa_addr ppa;\r\nsector_t lba = pblk_get_lba(bio);\r\nif (lba >= pblk->rl.nr_secs) {\r\nWARN(1, "pblk: read lba out of bounds\n");\r\nreturn;\r\n}\r\npblk_lookup_l2p_seq(pblk, &ppa, lba, 1);\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_inc(&pblk->inflight_reads);\r\n#endif\r\nretry:\r\nif (pblk_ppa_empty(ppa)) {\r\nWARN_ON(test_and_set_bit(0, read_bitmap));\r\nreturn;\r\n}\r\nif (pblk_addr_in_cache(ppa)) {\r\nif (!pblk_read_from_cache(pblk, bio, lba, ppa, 0, 1)) {\r\npblk_lookup_l2p_seq(pblk, &ppa, lba, 1);\r\ngoto retry;\r\n}\r\nWARN_ON(test_and_set_bit(0, read_bitmap));\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_inc(&pblk->cache_reads);\r\n#endif\r\n} else {\r\nrqd->ppa_addr = ppa;\r\n}\r\nrqd->flags = pblk_set_read_mode(pblk, PBLK_READ_RANDOM);\r\n}\r\nint pblk_submit_read(struct pblk *pblk, struct bio *bio)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nunsigned int nr_secs = pblk_get_secs(bio);\r\nstruct nvm_rq *rqd;\r\nunsigned long read_bitmap;\r\nunsigned int bio_init_idx;\r\nint ret = NVM_IO_ERR;\r\nif (nr_secs > PBLK_MAX_REQ_ADDRS)\r\nreturn NVM_IO_ERR;\r\nbitmap_zero(&read_bitmap, nr_secs);\r\nrqd = pblk_alloc_rqd(pblk, READ);\r\nif (IS_ERR(rqd)) {\r\npr_err_ratelimited("pblk: not able to alloc rqd");\r\nreturn NVM_IO_ERR;\r\n}\r\nrqd->opcode = NVM_OP_PREAD;\r\nrqd->bio = bio;\r\nrqd->nr_ppas = nr_secs;\r\nrqd->private = pblk;\r\nrqd->end_io = pblk_end_io_read;\r\nbio_init_idx = pblk_get_bi_idx(bio);\r\nrqd->meta_list = nvm_dev_dma_alloc(dev->parent, GFP_KERNEL,\r\n&rqd->dma_meta_list);\r\nif (!rqd->meta_list) {\r\npr_err("pblk: not able to allocate ppa list\n");\r\ngoto fail_rqd_free;\r\n}\r\nif (nr_secs > 1) {\r\nrqd->ppa_list = rqd->meta_list + pblk_dma_meta_size;\r\nrqd->dma_ppa_list = rqd->dma_meta_list + pblk_dma_meta_size;\r\npblk_read_ppalist_rq(pblk, rqd, &read_bitmap);\r\n} else {\r\npblk_read_rq(pblk, rqd, &read_bitmap);\r\n}\r\nbio_get(bio);\r\nif (bitmap_full(&read_bitmap, nr_secs)) {\r\nbio_endio(bio);\r\natomic_inc(&pblk->inflight_io);\r\npblk_end_io_read(rqd);\r\nreturn NVM_IO_OK;\r\n}\r\nif (bitmap_empty(&read_bitmap, rqd->nr_ppas)) {\r\nstruct bio *int_bio = NULL;\r\nstruct pblk_g_ctx *r_ctx = nvm_rq_to_pdu(rqd);\r\nint_bio = bio_clone_fast(bio, GFP_KERNEL, pblk_bio_set);\r\nif (!int_bio) {\r\npr_err("pblk: could not clone read bio\n");\r\nreturn NVM_IO_ERR;\r\n}\r\nrqd->bio = int_bio;\r\nr_ctx->private = bio;\r\nret = pblk_submit_read_io(pblk, rqd);\r\nif (ret) {\r\npr_err("pblk: read IO submission failed\n");\r\nif (int_bio)\r\nbio_put(int_bio);\r\nreturn ret;\r\n}\r\nreturn NVM_IO_OK;\r\n}\r\nret = pblk_fill_partial_read_bio(pblk, rqd, bio_init_idx, &read_bitmap);\r\nif (ret) {\r\npr_err("pblk: failed to perform partial read\n");\r\nreturn ret;\r\n}\r\nreturn NVM_IO_OK;\r\nfail_rqd_free:\r\npblk_free_rqd(pblk, rqd, READ);\r\nreturn ret;\r\n}\r\nstatic int read_ppalist_rq_gc(struct pblk *pblk, struct nvm_rq *rqd,\r\nstruct pblk_line *line, u64 *lba_list,\r\nunsigned int nr_secs)\r\n{\r\nstruct ppa_addr ppas[PBLK_MAX_REQ_ADDRS];\r\nint valid_secs = 0;\r\nint i;\r\npblk_lookup_l2p_rand(pblk, ppas, lba_list, nr_secs);\r\nfor (i = 0; i < nr_secs; i++) {\r\nif (pblk_addr_in_cache(ppas[i]) || ppas[i].g.blk != line->id ||\r\npblk_ppa_empty(ppas[i])) {\r\nlba_list[i] = ADDR_EMPTY;\r\ncontinue;\r\n}\r\nrqd->ppa_list[valid_secs++] = ppas[i];\r\n}\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_add(valid_secs, &pblk->inflight_reads);\r\n#endif\r\nreturn valid_secs;\r\n}\r\nstatic int read_rq_gc(struct pblk *pblk, struct nvm_rq *rqd,\r\nstruct pblk_line *line, sector_t lba)\r\n{\r\nstruct ppa_addr ppa;\r\nint valid_secs = 0;\r\nif (lba == ADDR_EMPTY)\r\ngoto out;\r\nif (lba >= pblk->rl.nr_secs) {\r\nWARN(1, "pblk: read lba out of bounds\n");\r\ngoto out;\r\n}\r\nspin_lock(&pblk->trans_lock);\r\nppa = pblk_trans_map_get(pblk, lba);\r\nspin_unlock(&pblk->trans_lock);\r\nif (pblk_addr_in_cache(ppa) || ppa.g.blk != line->id ||\r\npblk_ppa_empty(ppa))\r\ngoto out;\r\nrqd->ppa_addr = ppa;\r\nvalid_secs = 1;\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_inc(&pblk->inflight_reads);\r\n#endif\r\nout:\r\nreturn valid_secs;\r\n}\r\nint pblk_submit_read_gc(struct pblk *pblk, u64 *lba_list, void *data,\r\nunsigned int nr_secs, unsigned int *secs_to_gc,\r\nstruct pblk_line *line)\r\n{\r\nstruct nvm_tgt_dev *dev = pblk->dev;\r\nstruct nvm_geo *geo = &dev->geo;\r\nstruct bio *bio;\r\nstruct nvm_rq rqd;\r\nint ret, data_len;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nmemset(&rqd, 0, sizeof(struct nvm_rq));\r\nrqd.meta_list = nvm_dev_dma_alloc(dev->parent, GFP_KERNEL,\r\n&rqd.dma_meta_list);\r\nif (!rqd.meta_list)\r\nreturn NVM_IO_ERR;\r\nif (nr_secs > 1) {\r\nrqd.ppa_list = rqd.meta_list + pblk_dma_meta_size;\r\nrqd.dma_ppa_list = rqd.dma_meta_list + pblk_dma_meta_size;\r\n*secs_to_gc = read_ppalist_rq_gc(pblk, &rqd, line, lba_list,\r\nnr_secs);\r\nif (*secs_to_gc == 1)\r\nrqd.ppa_addr = rqd.ppa_list[0];\r\n} else {\r\n*secs_to_gc = read_rq_gc(pblk, &rqd, line, lba_list[0]);\r\n}\r\nif (!(*secs_to_gc))\r\ngoto out;\r\ndata_len = (*secs_to_gc) * geo->sec_size;\r\nbio = pblk_bio_map_addr(pblk, data, *secs_to_gc, data_len,\r\nPBLK_KMALLOC_META, GFP_KERNEL);\r\nif (IS_ERR(bio)) {\r\npr_err("pblk: could not allocate GC bio (%lu)\n", PTR_ERR(bio));\r\ngoto err_free_dma;\r\n}\r\nbio->bi_iter.bi_sector = 0;\r\nbio_set_op_attrs(bio, REQ_OP_READ, 0);\r\nrqd.opcode = NVM_OP_PREAD;\r\nrqd.end_io = pblk_end_io_sync;\r\nrqd.private = &wait;\r\nrqd.nr_ppas = *secs_to_gc;\r\nrqd.flags = pblk_set_read_mode(pblk, PBLK_READ_RANDOM);\r\nrqd.bio = bio;\r\nret = pblk_submit_read_io(pblk, &rqd);\r\nif (ret) {\r\nbio_endio(bio);\r\npr_err("pblk: GC read request failed\n");\r\ngoto err_free_dma;\r\n}\r\nif (!wait_for_completion_io_timeout(&wait,\r\nmsecs_to_jiffies(PBLK_COMMAND_TIMEOUT_MS))) {\r\npr_err("pblk: GC read I/O timed out\n");\r\n}\r\natomic_dec(&pblk->inflight_io);\r\nif (rqd.error) {\r\natomic_long_inc(&pblk->read_failed_gc);\r\n#ifdef CONFIG_NVM_DEBUG\r\npblk_print_failed_rqd(pblk, &rqd, rqd.error);\r\n#endif\r\n}\r\n#ifdef CONFIG_NVM_DEBUG\r\natomic_long_add(*secs_to_gc, &pblk->sync_reads);\r\natomic_long_add(*secs_to_gc, &pblk->recov_gc_reads);\r\natomic_long_sub(*secs_to_gc, &pblk->inflight_reads);\r\n#endif\r\nout:\r\nnvm_dev_dma_free(dev->parent, rqd.meta_list, rqd.dma_meta_list);\r\nreturn NVM_IO_OK;\r\nerr_free_dma:\r\nnvm_dev_dma_free(dev->parent, rqd.meta_list, rqd.dma_meta_list);\r\nreturn NVM_IO_ERR;\r\n}
