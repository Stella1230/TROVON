bool ion_buffer_cached(struct ion_buffer *buffer)\r\n{\r\nreturn !!(buffer->flags & ION_FLAG_CACHED);\r\n}\r\nstatic void ion_buffer_add(struct ion_device *dev,\r\nstruct ion_buffer *buffer)\r\n{\r\nstruct rb_node **p = &dev->buffers.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct ion_buffer *entry;\r\nwhile (*p) {\r\nparent = *p;\r\nentry = rb_entry(parent, struct ion_buffer, node);\r\nif (buffer < entry) {\r\np = &(*p)->rb_left;\r\n} else if (buffer > entry) {\r\np = &(*p)->rb_right;\r\n} else {\r\npr_err("%s: buffer already found.", __func__);\r\nBUG();\r\n}\r\n}\r\nrb_link_node(&buffer->node, parent, p);\r\nrb_insert_color(&buffer->node, &dev->buffers);\r\n}\r\nstatic struct ion_buffer *ion_buffer_create(struct ion_heap *heap,\r\nstruct ion_device *dev,\r\nunsigned long len,\r\nunsigned long flags)\r\n{\r\nstruct ion_buffer *buffer;\r\nstruct sg_table *table;\r\nint ret;\r\nbuffer = kzalloc(sizeof(*buffer), GFP_KERNEL);\r\nif (!buffer)\r\nreturn ERR_PTR(-ENOMEM);\r\nbuffer->heap = heap;\r\nbuffer->flags = flags;\r\nret = heap->ops->allocate(heap, buffer, len, flags);\r\nif (ret) {\r\nif (!(heap->flags & ION_HEAP_FLAG_DEFER_FREE))\r\ngoto err2;\r\nion_heap_freelist_drain(heap, 0);\r\nret = heap->ops->allocate(heap, buffer, len, flags);\r\nif (ret)\r\ngoto err2;\r\n}\r\nif (!buffer->sg_table) {\r\nWARN_ONCE(1, "This heap needs to set the sgtable");\r\nret = -EINVAL;\r\ngoto err1;\r\n}\r\ntable = buffer->sg_table;\r\nbuffer->dev = dev;\r\nbuffer->size = len;\r\nbuffer->dev = dev;\r\nbuffer->size = len;\r\nINIT_LIST_HEAD(&buffer->attachments);\r\nmutex_init(&buffer->lock);\r\nmutex_lock(&dev->buffer_lock);\r\nion_buffer_add(dev, buffer);\r\nmutex_unlock(&dev->buffer_lock);\r\nreturn buffer;\r\nerr1:\r\nheap->ops->free(buffer);\r\nerr2:\r\nkfree(buffer);\r\nreturn ERR_PTR(ret);\r\n}\r\nvoid ion_buffer_destroy(struct ion_buffer *buffer)\r\n{\r\nif (WARN_ON(buffer->kmap_cnt > 0))\r\nbuffer->heap->ops->unmap_kernel(buffer->heap, buffer);\r\nbuffer->heap->ops->free(buffer);\r\nkfree(buffer);\r\n}\r\nstatic void _ion_buffer_destroy(struct ion_buffer *buffer)\r\n{\r\nstruct ion_heap *heap = buffer->heap;\r\nstruct ion_device *dev = buffer->dev;\r\nmutex_lock(&dev->buffer_lock);\r\nrb_erase(&buffer->node, &dev->buffers);\r\nmutex_unlock(&dev->buffer_lock);\r\nif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\r\nion_heap_freelist_add(heap, buffer);\r\nelse\r\nion_buffer_destroy(buffer);\r\n}\r\nstatic void *ion_buffer_kmap_get(struct ion_buffer *buffer)\r\n{\r\nvoid *vaddr;\r\nif (buffer->kmap_cnt) {\r\nbuffer->kmap_cnt++;\r\nreturn buffer->vaddr;\r\n}\r\nvaddr = buffer->heap->ops->map_kernel(buffer->heap, buffer);\r\nif (WARN_ONCE(!vaddr,\r\n"heap->ops->map_kernel should return ERR_PTR on error"))\r\nreturn ERR_PTR(-EINVAL);\r\nif (IS_ERR(vaddr))\r\nreturn vaddr;\r\nbuffer->vaddr = vaddr;\r\nbuffer->kmap_cnt++;\r\nreturn vaddr;\r\n}\r\nstatic void ion_buffer_kmap_put(struct ion_buffer *buffer)\r\n{\r\nbuffer->kmap_cnt--;\r\nif (!buffer->kmap_cnt) {\r\nbuffer->heap->ops->unmap_kernel(buffer->heap, buffer);\r\nbuffer->vaddr = NULL;\r\n}\r\n}\r\nstatic struct sg_table *dup_sg_table(struct sg_table *table)\r\n{\r\nstruct sg_table *new_table;\r\nint ret, i;\r\nstruct scatterlist *sg, *new_sg;\r\nnew_table = kzalloc(sizeof(*new_table), GFP_KERNEL);\r\nif (!new_table)\r\nreturn ERR_PTR(-ENOMEM);\r\nret = sg_alloc_table(new_table, table->nents, GFP_KERNEL);\r\nif (ret) {\r\nkfree(new_table);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nnew_sg = new_table->sgl;\r\nfor_each_sg(table->sgl, sg, table->nents, i) {\r\nmemcpy(new_sg, sg, sizeof(*sg));\r\nsg->dma_address = 0;\r\nnew_sg = sg_next(new_sg);\r\n}\r\nreturn new_table;\r\n}\r\nstatic void free_duped_table(struct sg_table *table)\r\n{\r\nsg_free_table(table);\r\nkfree(table);\r\n}\r\nstatic int ion_dma_buf_attach(struct dma_buf *dmabuf, struct device *dev,\r\nstruct dma_buf_attachment *attachment)\r\n{\r\nstruct ion_dma_buf_attachment *a;\r\nstruct sg_table *table;\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\na = kzalloc(sizeof(*a), GFP_KERNEL);\r\nif (!a)\r\nreturn -ENOMEM;\r\ntable = dup_sg_table(buffer->sg_table);\r\nif (IS_ERR(table)) {\r\nkfree(a);\r\nreturn -ENOMEM;\r\n}\r\na->table = table;\r\na->dev = dev;\r\nINIT_LIST_HEAD(&a->list);\r\nattachment->priv = a;\r\nmutex_lock(&buffer->lock);\r\nlist_add(&a->list, &buffer->attachments);\r\nmutex_unlock(&buffer->lock);\r\nreturn 0;\r\n}\r\nstatic void ion_dma_buf_detatch(struct dma_buf *dmabuf,\r\nstruct dma_buf_attachment *attachment)\r\n{\r\nstruct ion_dma_buf_attachment *a = attachment->priv;\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nfree_duped_table(a->table);\r\nmutex_lock(&buffer->lock);\r\nlist_del(&a->list);\r\nmutex_unlock(&buffer->lock);\r\nkfree(a);\r\n}\r\nstatic struct sg_table *ion_map_dma_buf(struct dma_buf_attachment *attachment,\r\nenum dma_data_direction direction)\r\n{\r\nstruct ion_dma_buf_attachment *a = attachment->priv;\r\nstruct sg_table *table;\r\ntable = a->table;\r\nif (!dma_map_sg(attachment->dev, table->sgl, table->nents,\r\ndirection))\r\nreturn ERR_PTR(-ENOMEM);\r\nreturn table;\r\n}\r\nstatic void ion_unmap_dma_buf(struct dma_buf_attachment *attachment,\r\nstruct sg_table *table,\r\nenum dma_data_direction direction)\r\n{\r\ndma_unmap_sg(attachment->dev, table->sgl, table->nents, direction);\r\n}\r\nstatic int ion_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nint ret = 0;\r\nif (!buffer->heap->ops->map_user) {\r\npr_err("%s: this heap does not define a method for mapping to userspace\n",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nif (!(buffer->flags & ION_FLAG_CACHED))\r\nvma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);\r\nmutex_lock(&buffer->lock);\r\nret = buffer->heap->ops->map_user(buffer->heap, buffer, vma);\r\nmutex_unlock(&buffer->lock);\r\nif (ret)\r\npr_err("%s: failure mapping buffer to userspace\n",\r\n__func__);\r\nreturn ret;\r\n}\r\nstatic void ion_dma_buf_release(struct dma_buf *dmabuf)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\n_ion_buffer_destroy(buffer);\r\n}\r\nstatic void *ion_dma_buf_kmap(struct dma_buf *dmabuf, unsigned long offset)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nreturn buffer->vaddr + offset * PAGE_SIZE;\r\n}\r\nstatic void ion_dma_buf_kunmap(struct dma_buf *dmabuf, unsigned long offset,\r\nvoid *ptr)\r\n{\r\n}\r\nstatic int ion_dma_buf_begin_cpu_access(struct dma_buf *dmabuf,\r\nenum dma_data_direction direction)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nvoid *vaddr;\r\nstruct ion_dma_buf_attachment *a;\r\nif (buffer->heap->ops->map_kernel) {\r\nmutex_lock(&buffer->lock);\r\nvaddr = ion_buffer_kmap_get(buffer);\r\nmutex_unlock(&buffer->lock);\r\n}\r\nmutex_lock(&buffer->lock);\r\nlist_for_each_entry(a, &buffer->attachments, list) {\r\ndma_sync_sg_for_cpu(a->dev, a->table->sgl, a->table->nents,\r\nDMA_BIDIRECTIONAL);\r\n}\r\nmutex_unlock(&buffer->lock);\r\nreturn 0;\r\n}\r\nstatic int ion_dma_buf_end_cpu_access(struct dma_buf *dmabuf,\r\nenum dma_data_direction direction)\r\n{\r\nstruct ion_buffer *buffer = dmabuf->priv;\r\nstruct ion_dma_buf_attachment *a;\r\nif (buffer->heap->ops->map_kernel) {\r\nmutex_lock(&buffer->lock);\r\nion_buffer_kmap_put(buffer);\r\nmutex_unlock(&buffer->lock);\r\n}\r\nmutex_lock(&buffer->lock);\r\nlist_for_each_entry(a, &buffer->attachments, list) {\r\ndma_sync_sg_for_device(a->dev, a->table->sgl, a->table->nents,\r\nDMA_BIDIRECTIONAL);\r\n}\r\nmutex_unlock(&buffer->lock);\r\nreturn 0;\r\n}\r\nint ion_alloc(size_t len, unsigned int heap_id_mask, unsigned int flags)\r\n{\r\nstruct ion_device *dev = internal_dev;\r\nstruct ion_buffer *buffer = NULL;\r\nstruct ion_heap *heap;\r\nDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\r\nint fd;\r\nstruct dma_buf *dmabuf;\r\npr_debug("%s: len %zu heap_id_mask %u flags %x\n", __func__,\r\nlen, heap_id_mask, flags);\r\nlen = PAGE_ALIGN(len);\r\nif (!len)\r\nreturn -EINVAL;\r\ndown_read(&dev->lock);\r\nplist_for_each_entry(heap, &dev->heaps, node) {\r\nif (!((1 << heap->id) & heap_id_mask))\r\ncontinue;\r\nbuffer = ion_buffer_create(heap, dev, len, flags);\r\nif (!IS_ERR(buffer))\r\nbreak;\r\n}\r\nup_read(&dev->lock);\r\nif (!buffer)\r\nreturn -ENODEV;\r\nif (IS_ERR(buffer))\r\nreturn PTR_ERR(buffer);\r\nexp_info.ops = &dma_buf_ops;\r\nexp_info.size = buffer->size;\r\nexp_info.flags = O_RDWR;\r\nexp_info.priv = buffer;\r\ndmabuf = dma_buf_export(&exp_info);\r\nif (IS_ERR(dmabuf)) {\r\n_ion_buffer_destroy(buffer);\r\nreturn PTR_ERR(dmabuf);\r\n}\r\nfd = dma_buf_fd(dmabuf, O_CLOEXEC);\r\nif (fd < 0)\r\ndma_buf_put(dmabuf);\r\nreturn fd;\r\n}\r\nint ion_query_heaps(struct ion_heap_query *query)\r\n{\r\nstruct ion_device *dev = internal_dev;\r\nstruct ion_heap_data __user *buffer = u64_to_user_ptr(query->heaps);\r\nint ret = -EINVAL, cnt = 0, max_cnt;\r\nstruct ion_heap *heap;\r\nstruct ion_heap_data hdata;\r\nmemset(&hdata, 0, sizeof(hdata));\r\ndown_read(&dev->lock);\r\nif (!buffer) {\r\nquery->cnt = dev->heap_cnt;\r\nret = 0;\r\ngoto out;\r\n}\r\nif (query->cnt <= 0)\r\ngoto out;\r\nmax_cnt = query->cnt;\r\nplist_for_each_entry(heap, &dev->heaps, node) {\r\nstrncpy(hdata.name, heap->name, MAX_HEAP_NAME);\r\nhdata.name[sizeof(hdata.name) - 1] = '\0';\r\nhdata.type = heap->type;\r\nhdata.heap_id = heap->id;\r\nif (copy_to_user(&buffer[cnt], &hdata, sizeof(hdata))) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\ncnt++;\r\nif (cnt >= max_cnt)\r\nbreak;\r\n}\r\nquery->cnt = cnt;\r\nret = 0;\r\nout:\r\nup_read(&dev->lock);\r\nreturn ret;\r\n}\r\nstatic int debug_shrink_set(void *data, u64 val)\r\n{\r\nstruct ion_heap *heap = data;\r\nstruct shrink_control sc;\r\nint objs;\r\nsc.gfp_mask = GFP_HIGHUSER;\r\nsc.nr_to_scan = val;\r\nif (!val) {\r\nobjs = heap->shrinker.count_objects(&heap->shrinker, &sc);\r\nsc.nr_to_scan = objs;\r\n}\r\nheap->shrinker.scan_objects(&heap->shrinker, &sc);\r\nreturn 0;\r\n}\r\nstatic int debug_shrink_get(void *data, u64 *val)\r\n{\r\nstruct ion_heap *heap = data;\r\nstruct shrink_control sc;\r\nint objs;\r\nsc.gfp_mask = GFP_HIGHUSER;\r\nsc.nr_to_scan = 0;\r\nobjs = heap->shrinker.count_objects(&heap->shrinker, &sc);\r\n*val = objs;\r\nreturn 0;\r\n}\r\nvoid ion_device_add_heap(struct ion_heap *heap)\r\n{\r\nstruct dentry *debug_file;\r\nstruct ion_device *dev = internal_dev;\r\nif (!heap->ops->allocate || !heap->ops->free)\r\npr_err("%s: can not add heap with invalid ops struct.\n",\r\n__func__);\r\nspin_lock_init(&heap->free_lock);\r\nheap->free_list_size = 0;\r\nif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\r\nion_heap_init_deferred_free(heap);\r\nif ((heap->flags & ION_HEAP_FLAG_DEFER_FREE) || heap->ops->shrink)\r\nion_heap_init_shrinker(heap);\r\nheap->dev = dev;\r\ndown_write(&dev->lock);\r\nheap->id = heap_id++;\r\nplist_node_init(&heap->node, -heap->id);\r\nplist_add(&heap->node, &dev->heaps);\r\nif (heap->shrinker.count_objects && heap->shrinker.scan_objects) {\r\nchar debug_name[64];\r\nsnprintf(debug_name, 64, "%s_shrink", heap->name);\r\ndebug_file = debugfs_create_file(\r\ndebug_name, 0644, dev->debug_root, heap,\r\n&debug_shrink_fops);\r\nif (!debug_file) {\r\nchar buf[256], *path;\r\npath = dentry_path(dev->debug_root, buf, 256);\r\npr_err("Failed to create heap shrinker debugfs at %s/%s\n",\r\npath, debug_name);\r\n}\r\n}\r\ndev->heap_cnt++;\r\nup_write(&dev->lock);\r\n}\r\nstatic int ion_device_create(void)\r\n{\r\nstruct ion_device *idev;\r\nint ret;\r\nidev = kzalloc(sizeof(*idev), GFP_KERNEL);\r\nif (!idev)\r\nreturn -ENOMEM;\r\nidev->dev.minor = MISC_DYNAMIC_MINOR;\r\nidev->dev.name = "ion";\r\nidev->dev.fops = &ion_fops;\r\nidev->dev.parent = NULL;\r\nret = misc_register(&idev->dev);\r\nif (ret) {\r\npr_err("ion: failed to register misc device.\n");\r\nkfree(idev);\r\nreturn ret;\r\n}\r\nidev->debug_root = debugfs_create_dir("ion", NULL);\r\nif (!idev->debug_root) {\r\npr_err("ion: failed to create debugfs root directory.\n");\r\ngoto debugfs_done;\r\n}\r\ndebugfs_done:\r\nidev->buffers = RB_ROOT;\r\nmutex_init(&idev->buffer_lock);\r\ninit_rwsem(&idev->lock);\r\nplist_head_init(&idev->heaps);\r\ninternal_dev = idev;\r\nreturn 0;\r\n}
