static int p8_aes_cbc_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *alg = crypto_tfm_alg_name(tfm);\r\nstruct crypto_skcipher *fallback;\r\nstruct p8_aes_cbc_ctx *ctx = crypto_tfm_ctx(tfm);\r\nfallback = crypto_alloc_skcipher(alg, 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(fallback)) {\r\nprintk(KERN_ERR\r\n"Failed to allocate transformation for '%s': %ld\n",\r\nalg, PTR_ERR(fallback));\r\nreturn PTR_ERR(fallback);\r\n}\r\nprintk(KERN_INFO "Using '%s' as fallback implementation.\n",\r\ncrypto_skcipher_driver_name(fallback));\r\ncrypto_skcipher_set_flags(\r\nfallback,\r\ncrypto_skcipher_get_flags((struct crypto_skcipher *)tfm));\r\nctx->fallback = fallback;\r\nreturn 0;\r\n}\r\nstatic void p8_aes_cbc_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct p8_aes_cbc_ctx *ctx = crypto_tfm_ctx(tfm);\r\nif (ctx->fallback) {\r\ncrypto_free_skcipher(ctx->fallback);\r\nctx->fallback = NULL;\r\n}\r\n}\r\nstatic int p8_aes_cbc_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nint ret;\r\nstruct p8_aes_cbc_ctx *ctx = crypto_tfm_ctx(tfm);\r\npreempt_disable();\r\npagefault_disable();\r\nenable_kernel_vsx();\r\nret = aes_p8_set_encrypt_key(key, keylen * 8, &ctx->enc_key);\r\nret += aes_p8_set_decrypt_key(key, keylen * 8, &ctx->dec_key);\r\ndisable_kernel_vsx();\r\npagefault_enable();\r\npreempt_enable();\r\nret += crypto_skcipher_setkey(ctx->fallback, key, keylen);\r\nreturn ret;\r\n}\r\nstatic int p8_aes_cbc_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nint ret;\r\nstruct blkcipher_walk walk;\r\nstruct p8_aes_cbc_ctx *ctx =\r\ncrypto_tfm_ctx(crypto_blkcipher_tfm(desc->tfm));\r\nif (in_interrupt()) {\r\nSKCIPHER_REQUEST_ON_STACK(req, ctx->fallback);\r\nskcipher_request_set_tfm(req, ctx->fallback);\r\nskcipher_request_set_callback(req, desc->flags, NULL, NULL);\r\nskcipher_request_set_crypt(req, src, dst, nbytes, desc->info);\r\nret = crypto_skcipher_encrypt(req);\r\nskcipher_request_zero(req);\r\n} else {\r\npreempt_disable();\r\npagefault_disable();\r\nenable_kernel_vsx();\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nret = blkcipher_walk_virt(desc, &walk);\r\nwhile ((nbytes = walk.nbytes)) {\r\naes_p8_cbc_encrypt(walk.src.virt.addr,\r\nwalk.dst.virt.addr,\r\nnbytes & AES_BLOCK_MASK,\r\n&ctx->enc_key, walk.iv, 1);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nret = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\ndisable_kernel_vsx();\r\npagefault_enable();\r\npreempt_enable();\r\n}\r\nreturn ret;\r\n}\r\nstatic int p8_aes_cbc_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nint ret;\r\nstruct blkcipher_walk walk;\r\nstruct p8_aes_cbc_ctx *ctx =\r\ncrypto_tfm_ctx(crypto_blkcipher_tfm(desc->tfm));\r\nif (in_interrupt()) {\r\nSKCIPHER_REQUEST_ON_STACK(req, ctx->fallback);\r\nskcipher_request_set_tfm(req, ctx->fallback);\r\nskcipher_request_set_callback(req, desc->flags, NULL, NULL);\r\nskcipher_request_set_crypt(req, src, dst, nbytes, desc->info);\r\nret = crypto_skcipher_decrypt(req);\r\nskcipher_request_zero(req);\r\n} else {\r\npreempt_disable();\r\npagefault_disable();\r\nenable_kernel_vsx();\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nret = blkcipher_walk_virt(desc, &walk);\r\nwhile ((nbytes = walk.nbytes)) {\r\naes_p8_cbc_encrypt(walk.src.virt.addr,\r\nwalk.dst.virt.addr,\r\nnbytes & AES_BLOCK_MASK,\r\n&ctx->dec_key, walk.iv, 0);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nret = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\ndisable_kernel_vsx();\r\npagefault_enable();\r\npreempt_enable();\r\n}\r\nreturn ret;\r\n}
