static bool read_from_write_only(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *params,\r\nconst struct sys_reg_desc *r)\r\n{\r\nWARN_ONCE(1, "Unexpected sys_reg read to write-only register\n");\r\nprint_sys_reg_instr(params);\r\nkvm_inject_undefined(vcpu);\r\nreturn false;\r\n}\r\nstatic bool write_to_read_only(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *params,\r\nconst struct sys_reg_desc *r)\r\n{\r\nWARN_ONCE(1, "Unexpected sys_reg write to read-only register\n");\r\nprint_sys_reg_instr(params);\r\nkvm_inject_undefined(vcpu);\r\nreturn false;\r\n}\r\nstatic u32 get_ccsidr(u32 csselr)\r\n{\r\nu32 ccsidr;\r\nlocal_irq_disable();\r\nwrite_sysreg(csselr, csselr_el1);\r\nisb();\r\nccsidr = read_sysreg(ccsidr_el1);\r\nlocal_irq_enable();\r\nreturn ccsidr;\r\n}\r\nstatic bool access_dcsw(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (!p->is_write)\r\nreturn read_from_write_only(vcpu, p, r);\r\nkvm_set_way_flush(vcpu);\r\nreturn true;\r\n}\r\nstatic bool access_vm_reg(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nbool was_enabled = vcpu_has_cache_enabled(vcpu);\r\nBUG_ON(!p->is_write);\r\nif (!p->is_aarch32) {\r\nvcpu_sys_reg(vcpu, r->reg) = p->regval;\r\n} else {\r\nif (!p->is_32bit)\r\nvcpu_cp15_64_high(vcpu, r->reg) = upper_32_bits(p->regval);\r\nvcpu_cp15_64_low(vcpu, r->reg) = lower_32_bits(p->regval);\r\n}\r\nkvm_toggle_cache(vcpu, was_enabled);\r\nreturn true;\r\n}\r\nstatic bool access_gic_sgi(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (!p->is_write)\r\nreturn read_from_write_only(vcpu, p, r);\r\nvgic_v3_dispatch_sgi(vcpu, p->regval);\r\nreturn true;\r\n}\r\nstatic bool access_gic_sre(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (p->is_write)\r\nreturn ignore_write(vcpu, p);\r\np->regval = vcpu->arch.vgic_cpu.vgic_v3.vgic_sre;\r\nreturn true;\r\n}\r\nstatic bool trap_raz_wi(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (p->is_write)\r\nreturn ignore_write(vcpu, p);\r\nelse\r\nreturn read_zero(vcpu, p);\r\n}\r\nstatic bool trap_oslsr_el1(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (p->is_write) {\r\nreturn ignore_write(vcpu, p);\r\n} else {\r\np->regval = (1 << 3);\r\nreturn true;\r\n}\r\n}\r\nstatic bool trap_dbgauthstatus_el1(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (p->is_write) {\r\nreturn ignore_write(vcpu, p);\r\n} else {\r\np->regval = read_sysreg(dbgauthstatus_el1);\r\nreturn true;\r\n}\r\n}\r\nstatic bool trap_debug_regs(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (p->is_write) {\r\nvcpu_sys_reg(vcpu, r->reg) = p->regval;\r\nvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\r\n} else {\r\np->regval = vcpu_sys_reg(vcpu, r->reg);\r\n}\r\ntrace_trap_reg(__func__, r->reg, p->is_write, p->regval);\r\nreturn true;\r\n}\r\nstatic void reg_to_dbg(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nu64 *dbg_reg)\r\n{\r\nu64 val = p->regval;\r\nif (p->is_32bit) {\r\nval &= 0xffffffffUL;\r\nval |= ((*dbg_reg >> 32) << 32);\r\n}\r\n*dbg_reg = val;\r\nvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\r\n}\r\nstatic void dbg_to_reg(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nu64 *dbg_reg)\r\n{\r\np->regval = *dbg_reg;\r\nif (p->is_32bit)\r\np->regval &= 0xffffffffUL;\r\n}\r\nstatic bool trap_bvr(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *rd)\r\n{\r\nu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\r\nif (p->is_write)\r\nreg_to_dbg(vcpu, p, dbg_reg);\r\nelse\r\ndbg_to_reg(vcpu, p, dbg_reg);\r\ntrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\r\nreturn true;\r\n}\r\nstatic int set_bvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\r\nconst struct kvm_one_reg *reg, void __user *uaddr)\r\n{\r\n__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\r\nif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int get_bvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\r\nconst struct kvm_one_reg *reg, void __user *uaddr)\r\n{\r\n__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\r\nif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic void reset_bvr(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_desc *rd)\r\n{\r\nvcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg] = rd->val;\r\n}\r\nstatic bool trap_bcr(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *rd)\r\n{\r\nu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];\r\nif (p->is_write)\r\nreg_to_dbg(vcpu, p, dbg_reg);\r\nelse\r\ndbg_to_reg(vcpu, p, dbg_reg);\r\ntrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\r\nreturn true;\r\n}\r\nstatic int set_bcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\r\nconst struct kvm_one_reg *reg, void __user *uaddr)\r\n{\r\n__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];\r\nif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int get_bcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\r\nconst struct kvm_one_reg *reg, void __user *uaddr)\r\n{\r\n__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];\r\nif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic void reset_bcr(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_desc *rd)\r\n{\r\nvcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg] = rd->val;\r\n}\r\nstatic bool trap_wvr(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *rd)\r\n{\r\nu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];\r\nif (p->is_write)\r\nreg_to_dbg(vcpu, p, dbg_reg);\r\nelse\r\ndbg_to_reg(vcpu, p, dbg_reg);\r\ntrace_trap_reg(__func__, rd->reg, p->is_write,\r\nvcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg]);\r\nreturn true;\r\n}\r\nstatic int set_wvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\r\nconst struct kvm_one_reg *reg, void __user *uaddr)\r\n{\r\n__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];\r\nif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int get_wvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\r\nconst struct kvm_one_reg *reg, void __user *uaddr)\r\n{\r\n__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];\r\nif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic void reset_wvr(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_desc *rd)\r\n{\r\nvcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg] = rd->val;\r\n}\r\nstatic bool trap_wcr(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *rd)\r\n{\r\nu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];\r\nif (p->is_write)\r\nreg_to_dbg(vcpu, p, dbg_reg);\r\nelse\r\ndbg_to_reg(vcpu, p, dbg_reg);\r\ntrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\r\nreturn true;\r\n}\r\nstatic int set_wcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\r\nconst struct kvm_one_reg *reg, void __user *uaddr)\r\n{\r\n__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];\r\nif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int get_wcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\r\nconst struct kvm_one_reg *reg, void __user *uaddr)\r\n{\r\n__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];\r\nif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic void reset_wcr(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_desc *rd)\r\n{\r\nvcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg] = rd->val;\r\n}\r\nstatic void reset_amair_el1(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\r\n{\r\nvcpu_sys_reg(vcpu, AMAIR_EL1) = read_sysreg(amair_el1);\r\n}\r\nstatic void reset_mpidr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\r\n{\r\nu64 mpidr;\r\nmpidr = (vcpu->vcpu_id & 0x0f) << MPIDR_LEVEL_SHIFT(0);\r\nmpidr |= ((vcpu->vcpu_id >> 4) & 0xff) << MPIDR_LEVEL_SHIFT(1);\r\nmpidr |= ((vcpu->vcpu_id >> 12) & 0xff) << MPIDR_LEVEL_SHIFT(2);\r\nvcpu_sys_reg(vcpu, MPIDR_EL1) = (1ULL << 31) | mpidr;\r\n}\r\nstatic void reset_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\r\n{\r\nu64 pmcr, val;\r\npmcr = read_sysreg(pmcr_el0);\r\nval = ((pmcr & ~ARMV8_PMU_PMCR_MASK)\r\n| (ARMV8_PMU_PMCR_MASK & 0xdecafbad)) & (~ARMV8_PMU_PMCR_E);\r\nvcpu_sys_reg(vcpu, PMCR_EL0) = val;\r\n}\r\nstatic bool check_pmu_access_disabled(struct kvm_vcpu *vcpu, u64 flags)\r\n{\r\nu64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);\r\nbool enabled = (reg & flags) || vcpu_mode_priv(vcpu);\r\nif (!enabled)\r\nkvm_inject_undefined(vcpu);\r\nreturn !enabled;\r\n}\r\nstatic bool pmu_access_el0_disabled(struct kvm_vcpu *vcpu)\r\n{\r\nreturn check_pmu_access_disabled(vcpu, ARMV8_PMU_USERENR_EN);\r\n}\r\nstatic bool pmu_write_swinc_el0_disabled(struct kvm_vcpu *vcpu)\r\n{\r\nreturn check_pmu_access_disabled(vcpu, ARMV8_PMU_USERENR_SW | ARMV8_PMU_USERENR_EN);\r\n}\r\nstatic bool pmu_access_cycle_counter_el0_disabled(struct kvm_vcpu *vcpu)\r\n{\r\nreturn check_pmu_access_disabled(vcpu, ARMV8_PMU_USERENR_CR | ARMV8_PMU_USERENR_EN);\r\n}\r\nstatic bool pmu_access_event_counter_el0_disabled(struct kvm_vcpu *vcpu)\r\n{\r\nreturn check_pmu_access_disabled(vcpu, ARMV8_PMU_USERENR_ER | ARMV8_PMU_USERENR_EN);\r\n}\r\nstatic bool access_pmcr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nu64 val;\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nif (pmu_access_el0_disabled(vcpu))\r\nreturn false;\r\nif (p->is_write) {\r\nval = vcpu_sys_reg(vcpu, PMCR_EL0);\r\nval &= ~ARMV8_PMU_PMCR_MASK;\r\nval |= p->regval & ARMV8_PMU_PMCR_MASK;\r\nvcpu_sys_reg(vcpu, PMCR_EL0) = val;\r\nkvm_pmu_handle_pmcr(vcpu, val);\r\n} else {\r\nval = vcpu_sys_reg(vcpu, PMCR_EL0)\r\n& ~(ARMV8_PMU_PMCR_P | ARMV8_PMU_PMCR_C);\r\np->regval = val;\r\n}\r\nreturn true;\r\n}\r\nstatic bool access_pmselr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nif (pmu_access_event_counter_el0_disabled(vcpu))\r\nreturn false;\r\nif (p->is_write)\r\nvcpu_sys_reg(vcpu, PMSELR_EL0) = p->regval;\r\nelse\r\np->regval = vcpu_sys_reg(vcpu, PMSELR_EL0)\r\n& ARMV8_PMU_COUNTER_MASK;\r\nreturn true;\r\n}\r\nstatic bool access_pmceid(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nu64 pmceid;\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nBUG_ON(p->is_write);\r\nif (pmu_access_el0_disabled(vcpu))\r\nreturn false;\r\nif (!(p->Op2 & 1))\r\npmceid = read_sysreg(pmceid0_el0);\r\nelse\r\npmceid = read_sysreg(pmceid1_el0);\r\np->regval = pmceid;\r\nreturn true;\r\n}\r\nstatic bool pmu_counter_idx_valid(struct kvm_vcpu *vcpu, u64 idx)\r\n{\r\nu64 pmcr, val;\r\npmcr = vcpu_sys_reg(vcpu, PMCR_EL0);\r\nval = (pmcr >> ARMV8_PMU_PMCR_N_SHIFT) & ARMV8_PMU_PMCR_N_MASK;\r\nif (idx >= val && idx != ARMV8_PMU_CYCLE_IDX) {\r\nkvm_inject_undefined(vcpu);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic bool access_pmu_evcntr(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nu64 idx;\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nif (r->CRn == 9 && r->CRm == 13) {\r\nif (r->Op2 == 2) {\r\nif (pmu_access_event_counter_el0_disabled(vcpu))\r\nreturn false;\r\nidx = vcpu_sys_reg(vcpu, PMSELR_EL0)\r\n& ARMV8_PMU_COUNTER_MASK;\r\n} else if (r->Op2 == 0) {\r\nif (pmu_access_cycle_counter_el0_disabled(vcpu))\r\nreturn false;\r\nidx = ARMV8_PMU_CYCLE_IDX;\r\n} else {\r\nreturn false;\r\n}\r\n} else if (r->CRn == 0 && r->CRm == 9) {\r\nif (pmu_access_event_counter_el0_disabled(vcpu))\r\nreturn false;\r\nidx = ARMV8_PMU_CYCLE_IDX;\r\n} else if (r->CRn == 14 && (r->CRm & 12) == 8) {\r\nif (pmu_access_event_counter_el0_disabled(vcpu))\r\nreturn false;\r\nidx = ((r->CRm & 3) << 3) | (r->Op2 & 7);\r\n} else {\r\nreturn false;\r\n}\r\nif (!pmu_counter_idx_valid(vcpu, idx))\r\nreturn false;\r\nif (p->is_write) {\r\nif (pmu_access_el0_disabled(vcpu))\r\nreturn false;\r\nkvm_pmu_set_counter_value(vcpu, idx, p->regval);\r\n} else {\r\np->regval = kvm_pmu_get_counter_value(vcpu, idx);\r\n}\r\nreturn true;\r\n}\r\nstatic bool access_pmu_evtyper(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nu64 idx, reg;\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nif (pmu_access_el0_disabled(vcpu))\r\nreturn false;\r\nif (r->CRn == 9 && r->CRm == 13 && r->Op2 == 1) {\r\nidx = vcpu_sys_reg(vcpu, PMSELR_EL0) & ARMV8_PMU_COUNTER_MASK;\r\nreg = PMEVTYPER0_EL0 + idx;\r\n} else if (r->CRn == 14 && (r->CRm & 12) == 12) {\r\nidx = ((r->CRm & 3) << 3) | (r->Op2 & 7);\r\nif (idx == ARMV8_PMU_CYCLE_IDX)\r\nreg = PMCCFILTR_EL0;\r\nelse\r\nreg = PMEVTYPER0_EL0 + idx;\r\n} else {\r\nBUG();\r\n}\r\nif (!pmu_counter_idx_valid(vcpu, idx))\r\nreturn false;\r\nif (p->is_write) {\r\nkvm_pmu_set_counter_event_type(vcpu, p->regval, idx);\r\nvcpu_sys_reg(vcpu, reg) = p->regval & ARMV8_PMU_EVTYPE_MASK;\r\n} else {\r\np->regval = vcpu_sys_reg(vcpu, reg) & ARMV8_PMU_EVTYPE_MASK;\r\n}\r\nreturn true;\r\n}\r\nstatic bool access_pmcnten(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nu64 val, mask;\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nif (pmu_access_el0_disabled(vcpu))\r\nreturn false;\r\nmask = kvm_pmu_valid_counter_mask(vcpu);\r\nif (p->is_write) {\r\nval = p->regval & mask;\r\nif (r->Op2 & 0x1) {\r\nvcpu_sys_reg(vcpu, PMCNTENSET_EL0) |= val;\r\nkvm_pmu_enable_counter(vcpu, val);\r\n} else {\r\nvcpu_sys_reg(vcpu, PMCNTENSET_EL0) &= ~val;\r\nkvm_pmu_disable_counter(vcpu, val);\r\n}\r\n} else {\r\np->regval = vcpu_sys_reg(vcpu, PMCNTENSET_EL0) & mask;\r\n}\r\nreturn true;\r\n}\r\nstatic bool access_pminten(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nu64 mask = kvm_pmu_valid_counter_mask(vcpu);\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nif (!vcpu_mode_priv(vcpu)) {\r\nkvm_inject_undefined(vcpu);\r\nreturn false;\r\n}\r\nif (p->is_write) {\r\nu64 val = p->regval & mask;\r\nif (r->Op2 & 0x1)\r\nvcpu_sys_reg(vcpu, PMINTENSET_EL1) |= val;\r\nelse\r\nvcpu_sys_reg(vcpu, PMINTENSET_EL1) &= ~val;\r\n} else {\r\np->regval = vcpu_sys_reg(vcpu, PMINTENSET_EL1) & mask;\r\n}\r\nreturn true;\r\n}\r\nstatic bool access_pmovs(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nu64 mask = kvm_pmu_valid_counter_mask(vcpu);\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nif (pmu_access_el0_disabled(vcpu))\r\nreturn false;\r\nif (p->is_write) {\r\nif (r->CRm & 0x2)\r\nvcpu_sys_reg(vcpu, PMOVSSET_EL0) |= (p->regval & mask);\r\nelse\r\nvcpu_sys_reg(vcpu, PMOVSSET_EL0) &= ~(p->regval & mask);\r\n} else {\r\np->regval = vcpu_sys_reg(vcpu, PMOVSSET_EL0) & mask;\r\n}\r\nreturn true;\r\n}\r\nstatic bool access_pmswinc(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nu64 mask;\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nif (!p->is_write)\r\nreturn read_from_write_only(vcpu, p, r);\r\nif (pmu_write_swinc_el0_disabled(vcpu))\r\nreturn false;\r\nmask = kvm_pmu_valid_counter_mask(vcpu);\r\nkvm_pmu_software_increment(vcpu, p->regval & mask);\r\nreturn true;\r\n}\r\nstatic bool access_pmuserenr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (!kvm_arm_pmu_v3_ready(vcpu))\r\nreturn trap_raz_wi(vcpu, p, r);\r\nif (p->is_write) {\r\nif (!vcpu_mode_priv(vcpu)) {\r\nkvm_inject_undefined(vcpu);\r\nreturn false;\r\n}\r\nvcpu_sys_reg(vcpu, PMUSERENR_EL0) = p->regval\r\n& ARMV8_PMU_USERENR_MASK;\r\n} else {\r\np->regval = vcpu_sys_reg(vcpu, PMUSERENR_EL0)\r\n& ARMV8_PMU_USERENR_MASK;\r\n}\r\nreturn true;\r\n}\r\nstatic bool access_cntp_tval(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nstruct arch_timer_context *ptimer = vcpu_ptimer(vcpu);\r\nu64 now = kvm_phys_timer_read();\r\nif (p->is_write)\r\nptimer->cnt_cval = p->regval + now;\r\nelse\r\np->regval = ptimer->cnt_cval - now;\r\nreturn true;\r\n}\r\nstatic bool access_cntp_ctl(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nstruct arch_timer_context *ptimer = vcpu_ptimer(vcpu);\r\nif (p->is_write) {\r\nptimer->cnt_ctl = p->regval & ~ARCH_TIMER_CTRL_IT_STAT;\r\n} else {\r\nu64 now = kvm_phys_timer_read();\r\np->regval = ptimer->cnt_ctl;\r\nif (ptimer->cnt_cval <= now)\r\np->regval |= ARCH_TIMER_CTRL_IT_STAT;\r\n}\r\nreturn true;\r\n}\r\nstatic bool access_cntp_cval(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nstruct arch_timer_context *ptimer = vcpu_ptimer(vcpu);\r\nif (p->is_write)\r\nptimer->cnt_cval = p->regval;\r\nelse\r\np->regval = ptimer->cnt_cval;\r\nreturn true;\r\n}\r\nstatic bool trap_dbgidr(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (p->is_write) {\r\nreturn ignore_write(vcpu, p);\r\n} else {\r\nu64 dfr = read_sanitised_ftr_reg(SYS_ID_AA64DFR0_EL1);\r\nu64 pfr = read_sanitised_ftr_reg(SYS_ID_AA64PFR0_EL1);\r\nu32 el3 = !!cpuid_feature_extract_unsigned_field(pfr, ID_AA64PFR0_EL3_SHIFT);\r\np->regval = ((((dfr >> ID_AA64DFR0_WRPS_SHIFT) & 0xf) << 28) |\r\n(((dfr >> ID_AA64DFR0_BRPS_SHIFT) & 0xf) << 24) |\r\n(((dfr >> ID_AA64DFR0_CTX_CMPS_SHIFT) & 0xf) << 20)\r\n| (6 << 16) | (el3 << 14) | (el3 << 12));\r\nreturn true;\r\n}\r\n}\r\nstatic bool trap_debug32(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *r)\r\n{\r\nif (p->is_write) {\r\nvcpu_cp14(vcpu, r->reg) = p->regval;\r\nvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\r\n} else {\r\np->regval = vcpu_cp14(vcpu, r->reg);\r\n}\r\nreturn true;\r\n}\r\nstatic bool trap_xvr(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *p,\r\nconst struct sys_reg_desc *rd)\r\n{\r\nu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\r\nif (p->is_write) {\r\nu64 val = *dbg_reg;\r\nval &= 0xffffffffUL;\r\nval |= p->regval << 32;\r\n*dbg_reg = val;\r\nvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\r\n} else {\r\np->regval = *dbg_reg >> 32;\r\n}\r\ntrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\r\nreturn true;\r\n}\r\nvoid kvm_register_target_sys_reg_table(unsigned int target,\r\nstruct kvm_sys_reg_target_table *table)\r\n{\r\ntarget_tables[target] = table;\r\n}\r\nstatic const struct sys_reg_desc *get_target_table(unsigned target,\r\nbool mode_is_64,\r\nsize_t *num)\r\n{\r\nstruct kvm_sys_reg_target_table *table;\r\ntable = target_tables[target];\r\nif (mode_is_64) {\r\n*num = table->table64.num;\r\nreturn table->table64.table;\r\n} else {\r\n*num = table->table32.num;\r\nreturn table->table32.table;\r\n}\r\n}\r\nstatic int match_sys_reg(const void *key, const void *elt)\r\n{\r\nconst unsigned long pval = (unsigned long)key;\r\nconst struct sys_reg_desc *r = elt;\r\nreturn pval - reg_to_match_value(r);\r\n}\r\nstatic const struct sys_reg_desc *find_reg(const struct sys_reg_params *params,\r\nconst struct sys_reg_desc table[],\r\nunsigned int num)\r\n{\r\nunsigned long pval = reg_to_match_value(params);\r\nreturn bsearch((void *)pval, table, num, sizeof(table[0]), match_sys_reg);\r\n}\r\nint kvm_handle_cp14_load_store(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nkvm_inject_undefined(vcpu);\r\nreturn 1;\r\n}\r\nstatic void perform_access(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *params,\r\nconst struct sys_reg_desc *r)\r\n{\r\nBUG_ON(!r->access);\r\nif (likely(r->access(vcpu, params, r)))\r\nkvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));\r\n}\r\nstatic int emulate_cp(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *params,\r\nconst struct sys_reg_desc *table,\r\nsize_t num)\r\n{\r\nconst struct sys_reg_desc *r;\r\nif (!table)\r\nreturn -1;\r\nr = find_reg(params, table, num);\r\nif (r) {\r\nperform_access(vcpu, params, r);\r\nreturn 0;\r\n}\r\nreturn -1;\r\n}\r\nstatic void unhandled_cp_access(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *params)\r\n{\r\nu8 hsr_ec = kvm_vcpu_trap_get_class(vcpu);\r\nint cp = -1;\r\nswitch(hsr_ec) {\r\ncase ESR_ELx_EC_CP15_32:\r\ncase ESR_ELx_EC_CP15_64:\r\ncp = 15;\r\nbreak;\r\ncase ESR_ELx_EC_CP14_MR:\r\ncase ESR_ELx_EC_CP14_64:\r\ncp = 14;\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\n}\r\nkvm_err("Unsupported guest CP%d access at: %08lx\n",\r\ncp, *vcpu_pc(vcpu));\r\nprint_sys_reg_instr(params);\r\nkvm_inject_undefined(vcpu);\r\n}\r\nstatic int kvm_handle_cp_64(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_desc *global,\r\nsize_t nr_global,\r\nconst struct sys_reg_desc *target_specific,\r\nsize_t nr_specific)\r\n{\r\nstruct sys_reg_params params;\r\nu32 hsr = kvm_vcpu_get_hsr(vcpu);\r\nint Rt = kvm_vcpu_sys_get_rt(vcpu);\r\nint Rt2 = (hsr >> 10) & 0x1f;\r\nparams.is_aarch32 = true;\r\nparams.is_32bit = false;\r\nparams.CRm = (hsr >> 1) & 0xf;\r\nparams.is_write = ((hsr & 1) == 0);\r\nparams.Op0 = 0;\r\nparams.Op1 = (hsr >> 16) & 0xf;\r\nparams.Op2 = 0;\r\nparams.CRn = 0;\r\nif (params.is_write) {\r\nparams.regval = vcpu_get_reg(vcpu, Rt) & 0xffffffff;\r\nparams.regval |= vcpu_get_reg(vcpu, Rt2) << 32;\r\n}\r\nif (!emulate_cp(vcpu, &params, target_specific, nr_specific) ||\r\n!emulate_cp(vcpu, &params, global, nr_global)) {\r\nif (!params.is_write) {\r\nvcpu_set_reg(vcpu, Rt, lower_32_bits(params.regval));\r\nvcpu_set_reg(vcpu, Rt2, upper_32_bits(params.regval));\r\n}\r\nreturn 1;\r\n}\r\nunhandled_cp_access(vcpu, &params);\r\nreturn 1;\r\n}\r\nstatic int kvm_handle_cp_32(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_desc *global,\r\nsize_t nr_global,\r\nconst struct sys_reg_desc *target_specific,\r\nsize_t nr_specific)\r\n{\r\nstruct sys_reg_params params;\r\nu32 hsr = kvm_vcpu_get_hsr(vcpu);\r\nint Rt = kvm_vcpu_sys_get_rt(vcpu);\r\nparams.is_aarch32 = true;\r\nparams.is_32bit = true;\r\nparams.CRm = (hsr >> 1) & 0xf;\r\nparams.regval = vcpu_get_reg(vcpu, Rt);\r\nparams.is_write = ((hsr & 1) == 0);\r\nparams.CRn = (hsr >> 10) & 0xf;\r\nparams.Op0 = 0;\r\nparams.Op1 = (hsr >> 14) & 0x7;\r\nparams.Op2 = (hsr >> 17) & 0x7;\r\nif (!emulate_cp(vcpu, &params, target_specific, nr_specific) ||\r\n!emulate_cp(vcpu, &params, global, nr_global)) {\r\nif (!params.is_write)\r\nvcpu_set_reg(vcpu, Rt, params.regval);\r\nreturn 1;\r\n}\r\nunhandled_cp_access(vcpu, &params);\r\nreturn 1;\r\n}\r\nint kvm_handle_cp15_64(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nconst struct sys_reg_desc *target_specific;\r\nsize_t num;\r\ntarget_specific = get_target_table(vcpu->arch.target, false, &num);\r\nreturn kvm_handle_cp_64(vcpu,\r\ncp15_64_regs, ARRAY_SIZE(cp15_64_regs),\r\ntarget_specific, num);\r\n}\r\nint kvm_handle_cp15_32(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nconst struct sys_reg_desc *target_specific;\r\nsize_t num;\r\ntarget_specific = get_target_table(vcpu->arch.target, false, &num);\r\nreturn kvm_handle_cp_32(vcpu,\r\ncp15_regs, ARRAY_SIZE(cp15_regs),\r\ntarget_specific, num);\r\n}\r\nint kvm_handle_cp14_64(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nreturn kvm_handle_cp_64(vcpu,\r\ncp14_64_regs, ARRAY_SIZE(cp14_64_regs),\r\nNULL, 0);\r\n}\r\nint kvm_handle_cp14_32(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nreturn kvm_handle_cp_32(vcpu,\r\ncp14_regs, ARRAY_SIZE(cp14_regs),\r\nNULL, 0);\r\n}\r\nstatic int emulate_sys_reg(struct kvm_vcpu *vcpu,\r\nstruct sys_reg_params *params)\r\n{\r\nsize_t num;\r\nconst struct sys_reg_desc *table, *r;\r\ntable = get_target_table(vcpu->arch.target, true, &num);\r\nr = find_reg(params, table, num);\r\nif (!r)\r\nr = find_reg(params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\r\nif (likely(r)) {\r\nperform_access(vcpu, params, r);\r\n} else {\r\nkvm_err("Unsupported guest sys_reg access at: %lx\n",\r\n*vcpu_pc(vcpu));\r\nprint_sys_reg_instr(params);\r\nkvm_inject_undefined(vcpu);\r\n}\r\nreturn 1;\r\n}\r\nstatic void reset_sys_reg_descs(struct kvm_vcpu *vcpu,\r\nconst struct sys_reg_desc *table, size_t num)\r\n{\r\nunsigned long i;\r\nfor (i = 0; i < num; i++)\r\nif (table[i].reset)\r\ntable[i].reset(vcpu, &table[i]);\r\n}\r\nint kvm_handle_sys_reg(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nstruct sys_reg_params params;\r\nunsigned long esr = kvm_vcpu_get_hsr(vcpu);\r\nint Rt = kvm_vcpu_sys_get_rt(vcpu);\r\nint ret;\r\ntrace_kvm_handle_sys_reg(esr);\r\nparams.is_aarch32 = false;\r\nparams.is_32bit = false;\r\nparams.Op0 = (esr >> 20) & 3;\r\nparams.Op1 = (esr >> 14) & 0x7;\r\nparams.CRn = (esr >> 10) & 0xf;\r\nparams.CRm = (esr >> 1) & 0xf;\r\nparams.Op2 = (esr >> 17) & 0x7;\r\nparams.regval = vcpu_get_reg(vcpu, Rt);\r\nparams.is_write = !(esr & 1);\r\nret = emulate_sys_reg(vcpu, &params);\r\nif (!params.is_write)\r\nvcpu_set_reg(vcpu, Rt, params.regval);\r\nreturn ret;\r\n}\r\nstatic bool index_to_params(u64 id, struct sys_reg_params *params)\r\n{\r\nswitch (id & KVM_REG_SIZE_MASK) {\r\ncase KVM_REG_SIZE_U64:\r\nif (id & ~(KVM_REG_ARCH_MASK | KVM_REG_SIZE_MASK\r\n| KVM_REG_ARM_COPROC_MASK\r\n| KVM_REG_ARM64_SYSREG_OP0_MASK\r\n| KVM_REG_ARM64_SYSREG_OP1_MASK\r\n| KVM_REG_ARM64_SYSREG_CRN_MASK\r\n| KVM_REG_ARM64_SYSREG_CRM_MASK\r\n| KVM_REG_ARM64_SYSREG_OP2_MASK))\r\nreturn false;\r\nparams->Op0 = ((id & KVM_REG_ARM64_SYSREG_OP0_MASK)\r\n>> KVM_REG_ARM64_SYSREG_OP0_SHIFT);\r\nparams->Op1 = ((id & KVM_REG_ARM64_SYSREG_OP1_MASK)\r\n>> KVM_REG_ARM64_SYSREG_OP1_SHIFT);\r\nparams->CRn = ((id & KVM_REG_ARM64_SYSREG_CRN_MASK)\r\n>> KVM_REG_ARM64_SYSREG_CRN_SHIFT);\r\nparams->CRm = ((id & KVM_REG_ARM64_SYSREG_CRM_MASK)\r\n>> KVM_REG_ARM64_SYSREG_CRM_SHIFT);\r\nparams->Op2 = ((id & KVM_REG_ARM64_SYSREG_OP2_MASK)\r\n>> KVM_REG_ARM64_SYSREG_OP2_SHIFT);\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nconst struct sys_reg_desc *find_reg_by_id(u64 id,\r\nstruct sys_reg_params *params,\r\nconst struct sys_reg_desc table[],\r\nunsigned int num)\r\n{\r\nif (!index_to_params(id, params))\r\nreturn NULL;\r\nreturn find_reg(params, table, num);\r\n}\r\nstatic const struct sys_reg_desc *index_to_sys_reg_desc(struct kvm_vcpu *vcpu,\r\nu64 id)\r\n{\r\nsize_t num;\r\nconst struct sys_reg_desc *table, *r;\r\nstruct sys_reg_params params;\r\nif ((id & KVM_REG_ARM_COPROC_MASK) != KVM_REG_ARM64_SYSREG)\r\nreturn NULL;\r\ntable = get_target_table(vcpu->arch.target, true, &num);\r\nr = find_reg_by_id(id, &params, table, num);\r\nif (!r)\r\nr = find_reg(&params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\r\nif (r && !r->reg)\r\nr = NULL;\r\nreturn r;\r\n}\r\nstatic int reg_from_user(u64 *val, const void __user *uaddr, u64 id)\r\n{\r\nif (copy_from_user(val, uaddr, KVM_REG_SIZE(id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int reg_to_user(void __user *uaddr, const u64 *val, u64 id)\r\n{\r\nif (copy_to_user(uaddr, val, KVM_REG_SIZE(id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int get_invariant_sys_reg(u64 id, void __user *uaddr)\r\n{\r\nstruct sys_reg_params params;\r\nconst struct sys_reg_desc *r;\r\nr = find_reg_by_id(id, &params, invariant_sys_regs,\r\nARRAY_SIZE(invariant_sys_regs));\r\nif (!r)\r\nreturn -ENOENT;\r\nreturn reg_to_user(uaddr, &r->val, id);\r\n}\r\nstatic int set_invariant_sys_reg(u64 id, void __user *uaddr)\r\n{\r\nstruct sys_reg_params params;\r\nconst struct sys_reg_desc *r;\r\nint err;\r\nu64 val = 0;\r\nr = find_reg_by_id(id, &params, invariant_sys_regs,\r\nARRAY_SIZE(invariant_sys_regs));\r\nif (!r)\r\nreturn -ENOENT;\r\nerr = reg_from_user(&val, uaddr, id);\r\nif (err)\r\nreturn err;\r\nif (r->val != val)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic bool is_valid_cache(u32 val)\r\n{\r\nu32 level, ctype;\r\nif (val >= CSSELR_MAX)\r\nreturn false;\r\nlevel = (val >> 1);\r\nctype = (cache_levels >> (level * 3)) & 7;\r\nswitch (ctype) {\r\ncase 0:\r\nreturn false;\r\ncase 1:\r\nreturn (val & 1);\r\ncase 2:\r\ncase 4:\r\nreturn !(val & 1);\r\ncase 3:\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic int demux_c15_get(u64 id, void __user *uaddr)\r\n{\r\nu32 val;\r\nu32 __user *uval = uaddr;\r\nif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\r\n| ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\r\nreturn -ENOENT;\r\nswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\r\ncase KVM_REG_ARM_DEMUX_ID_CCSIDR:\r\nif (KVM_REG_SIZE(id) != 4)\r\nreturn -ENOENT;\r\nval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\r\n>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\r\nif (!is_valid_cache(val))\r\nreturn -ENOENT;\r\nreturn put_user(get_ccsidr(val), uval);\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\n}\r\nstatic int demux_c15_set(u64 id, void __user *uaddr)\r\n{\r\nu32 val, newval;\r\nu32 __user *uval = uaddr;\r\nif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\r\n| ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\r\nreturn -ENOENT;\r\nswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\r\ncase KVM_REG_ARM_DEMUX_ID_CCSIDR:\r\nif (KVM_REG_SIZE(id) != 4)\r\nreturn -ENOENT;\r\nval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\r\n>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\r\nif (!is_valid_cache(val))\r\nreturn -ENOENT;\r\nif (get_user(newval, uval))\r\nreturn -EFAULT;\r\nif (newval != get_ccsidr(val))\r\nreturn -EINVAL;\r\nreturn 0;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\n}\r\nint kvm_arm_sys_reg_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\r\n{\r\nconst struct sys_reg_desc *r;\r\nvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\r\nif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\r\nreturn demux_c15_get(reg->id, uaddr);\r\nif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\r\nreturn -ENOENT;\r\nr = index_to_sys_reg_desc(vcpu, reg->id);\r\nif (!r)\r\nreturn get_invariant_sys_reg(reg->id, uaddr);\r\nif (r->get_user)\r\nreturn (r->get_user)(vcpu, r, reg, uaddr);\r\nreturn reg_to_user(uaddr, &vcpu_sys_reg(vcpu, r->reg), reg->id);\r\n}\r\nint kvm_arm_sys_reg_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\r\n{\r\nconst struct sys_reg_desc *r;\r\nvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\r\nif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\r\nreturn demux_c15_set(reg->id, uaddr);\r\nif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\r\nreturn -ENOENT;\r\nr = index_to_sys_reg_desc(vcpu, reg->id);\r\nif (!r)\r\nreturn set_invariant_sys_reg(reg->id, uaddr);\r\nif (r->set_user)\r\nreturn (r->set_user)(vcpu, r, reg, uaddr);\r\nreturn reg_from_user(&vcpu_sys_reg(vcpu, r->reg), uaddr, reg->id);\r\n}\r\nstatic unsigned int num_demux_regs(void)\r\n{\r\nunsigned int i, count = 0;\r\nfor (i = 0; i < CSSELR_MAX; i++)\r\nif (is_valid_cache(i))\r\ncount++;\r\nreturn count;\r\n}\r\nstatic int write_demux_regids(u64 __user *uindices)\r\n{\r\nu64 val = KVM_REG_ARM64 | KVM_REG_SIZE_U32 | KVM_REG_ARM_DEMUX;\r\nunsigned int i;\r\nval |= KVM_REG_ARM_DEMUX_ID_CCSIDR;\r\nfor (i = 0; i < CSSELR_MAX; i++) {\r\nif (!is_valid_cache(i))\r\ncontinue;\r\nif (put_user(val | i, uindices))\r\nreturn -EFAULT;\r\nuindices++;\r\n}\r\nreturn 0;\r\n}\r\nstatic u64 sys_reg_to_index(const struct sys_reg_desc *reg)\r\n{\r\nreturn (KVM_REG_ARM64 | KVM_REG_SIZE_U64 |\r\nKVM_REG_ARM64_SYSREG |\r\n(reg->Op0 << KVM_REG_ARM64_SYSREG_OP0_SHIFT) |\r\n(reg->Op1 << KVM_REG_ARM64_SYSREG_OP1_SHIFT) |\r\n(reg->CRn << KVM_REG_ARM64_SYSREG_CRN_SHIFT) |\r\n(reg->CRm << KVM_REG_ARM64_SYSREG_CRM_SHIFT) |\r\n(reg->Op2 << KVM_REG_ARM64_SYSREG_OP2_SHIFT));\r\n}\r\nstatic bool copy_reg_to_user(const struct sys_reg_desc *reg, u64 __user **uind)\r\n{\r\nif (!*uind)\r\nreturn true;\r\nif (put_user(sys_reg_to_index(reg), *uind))\r\nreturn false;\r\n(*uind)++;\r\nreturn true;\r\n}\r\nstatic int walk_sys_regs(struct kvm_vcpu *vcpu, u64 __user *uind)\r\n{\r\nconst struct sys_reg_desc *i1, *i2, *end1, *end2;\r\nunsigned int total = 0;\r\nsize_t num;\r\ni1 = get_target_table(vcpu->arch.target, true, &num);\r\nend1 = i1 + num;\r\ni2 = sys_reg_descs;\r\nend2 = sys_reg_descs + ARRAY_SIZE(sys_reg_descs);\r\nBUG_ON(i1 == end1 || i2 == end2);\r\nwhile (i1 || i2) {\r\nint cmp = cmp_sys_reg(i1, i2);\r\nif (cmp <= 0) {\r\nif (i1->reg) {\r\nif (!copy_reg_to_user(i1, &uind))\r\nreturn -EFAULT;\r\ntotal++;\r\n}\r\n} else {\r\nif (i2->reg) {\r\nif (!copy_reg_to_user(i2, &uind))\r\nreturn -EFAULT;\r\ntotal++;\r\n}\r\n}\r\nif (cmp <= 0 && ++i1 == end1)\r\ni1 = NULL;\r\nif (cmp >= 0 && ++i2 == end2)\r\ni2 = NULL;\r\n}\r\nreturn total;\r\n}\r\nunsigned long kvm_arm_num_sys_reg_descs(struct kvm_vcpu *vcpu)\r\n{\r\nreturn ARRAY_SIZE(invariant_sys_regs)\r\n+ num_demux_regs()\r\n+ walk_sys_regs(vcpu, (u64 __user *)NULL);\r\n}\r\nint kvm_arm_copy_sys_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)\r\n{\r\nunsigned int i;\r\nint err;\r\nfor (i = 0; i < ARRAY_SIZE(invariant_sys_regs); i++) {\r\nif (put_user(sys_reg_to_index(&invariant_sys_regs[i]), uindices))\r\nreturn -EFAULT;\r\nuindices++;\r\n}\r\nerr = walk_sys_regs(vcpu, uindices);\r\nif (err < 0)\r\nreturn err;\r\nuindices += err;\r\nreturn write_demux_regids(uindices);\r\n}\r\nstatic int check_sysreg_table(const struct sys_reg_desc *table, unsigned int n)\r\n{\r\nunsigned int i;\r\nfor (i = 1; i < n; i++) {\r\nif (cmp_sys_reg(&table[i-1], &table[i]) >= 0) {\r\nkvm_err("sys_reg table %p out of order (%d)\n", table, i - 1);\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid kvm_sys_reg_table_init(void)\r\n{\r\nunsigned int i;\r\nstruct sys_reg_desc clidr;\r\nBUG_ON(check_sysreg_table(sys_reg_descs, ARRAY_SIZE(sys_reg_descs)));\r\nBUG_ON(check_sysreg_table(cp14_regs, ARRAY_SIZE(cp14_regs)));\r\nBUG_ON(check_sysreg_table(cp14_64_regs, ARRAY_SIZE(cp14_64_regs)));\r\nBUG_ON(check_sysreg_table(cp15_regs, ARRAY_SIZE(cp15_regs)));\r\nBUG_ON(check_sysreg_table(cp15_64_regs, ARRAY_SIZE(cp15_64_regs)));\r\nBUG_ON(check_sysreg_table(invariant_sys_regs, ARRAY_SIZE(invariant_sys_regs)));\r\nfor (i = 0; i < ARRAY_SIZE(invariant_sys_regs); i++)\r\ninvariant_sys_regs[i].reset(NULL, &invariant_sys_regs[i]);\r\nget_clidr_el1(NULL, &clidr);\r\ncache_levels = clidr.val;\r\nfor (i = 0; i < 7; i++)\r\nif (((cache_levels >> (i*3)) & 7) == 0)\r\nbreak;\r\ncache_levels &= (1 << (i*3))-1;\r\n}\r\nvoid kvm_reset_sys_regs(struct kvm_vcpu *vcpu)\r\n{\r\nsize_t num;\r\nconst struct sys_reg_desc *table;\r\nmemset(&vcpu->arch.ctxt.sys_regs, 0x42, sizeof(vcpu->arch.ctxt.sys_regs));\r\nreset_sys_reg_descs(vcpu, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\r\ntable = get_target_table(vcpu->arch.target, true, &num);\r\nreset_sys_reg_descs(vcpu, table, num);\r\nfor (num = 1; num < NR_SYS_REGS; num++)\r\nif (vcpu_sys_reg(vcpu, num) == 0x4242424242424242)\r\npanic("Didn't reset vcpu_sys_reg(%zi)", num);\r\n}
