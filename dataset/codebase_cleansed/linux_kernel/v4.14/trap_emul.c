static gpa_t kvm_trap_emul_gva_to_gpa_cb(gva_t gva)\r\n{\r\ngpa_t gpa;\r\ngva_t kseg = KSEGX(gva);\r\ngva_t gkseg = KVM_GUEST_KSEGX(gva);\r\nif ((kseg == CKSEG0) || (kseg == CKSEG1))\r\ngpa = CPHYSADDR(gva);\r\nelse if (gkseg == KVM_GUEST_KSEG0)\r\ngpa = KVM_GUEST_CPHYSADDR(gva);\r\nelse {\r\nkvm_err("%s: cannot find GPA for GVA: %#lx\n", __func__, gva);\r\nkvm_mips_dump_host_tlbs();\r\ngpa = KVM_INVALID_ADDR;\r\n}\r\nkvm_debug("%s: gva %#lx, gpa: %#llx\n", __func__, gva, gpa);\r\nreturn gpa;\r\n}\r\nstatic int kvm_trap_emul_no_handler(struct kvm_vcpu *vcpu)\r\n{\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nu32 exccode = (cause & CAUSEF_EXCCODE) >> CAUSEB_EXCCODE;\r\nunsigned long badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nu32 inst = 0;\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nkvm_get_badinstr(opc, vcpu, &inst);\r\nkvm_err("Exception Code: %d not handled @ PC: %p, inst: 0x%08x BadVaddr: %#lx Status: %#x\n",\r\nexccode, opc, inst, badvaddr,\r\nkvm_read_c0_guest_status(vcpu->arch.cop0));\r\nkvm_arch_vcpu_dump_regs(vcpu);\r\nvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\nstatic int kvm_trap_emul_handle_cop_unusable(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\nif (((cause & CAUSEF_CE) >> CAUSEB_CE) == 1) {\r\nif (!kvm_mips_guest_has_fpu(&vcpu->arch) ||\r\n(kvm_read_c0_guest_status(cop0) & ST0_CU1) == 0) {\r\ner = kvm_mips_emulate_fpu_exc(cause, opc, run, vcpu);\r\n} else {\r\nkvm_own_fpu(vcpu);\r\ner = EMULATE_DONE;\r\n}\r\n} else {\r\ner = kvm_mips_emulate_inst(cause, opc, run, vcpu);\r\n}\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nret = RESUME_GUEST;\r\nbreak;\r\ncase EMULATE_FAIL:\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\nbreak;\r\ncase EMULATE_WAIT:\r\nrun->exit_reason = KVM_EXIT_INTR;\r\nret = RESUME_HOST;\r\nbreak;\r\ncase EMULATE_HYPERCALL:\r\nret = kvm_mips_handle_hypcall(vcpu);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_mips_bad_load(u32 cause, u32 *opc, struct kvm_run *run,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er;\r\nunion mips_instruction inst;\r\nint err;\r\nif (kvm_is_ifetch_fault(&vcpu->arch)) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nerr = kvm_get_badinstr(opc, vcpu, &inst.word);\r\nif (err) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\ner = kvm_mips_emulate_load(inst, cause, run, vcpu);\r\nif (er == EMULATE_FAIL) {\r\nkvm_err("Emulate load from MMIO space failed\n");\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\n} else {\r\nrun->exit_reason = KVM_EXIT_MMIO;\r\n}\r\nreturn RESUME_HOST;\r\n}\r\nstatic int kvm_mips_bad_store(u32 cause, u32 *opc, struct kvm_run *run,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er;\r\nunion mips_instruction inst;\r\nint err;\r\nif (cause & CAUSEF_BD)\r\nopc += 1;\r\nerr = kvm_get_badinstr(opc, vcpu, &inst.word);\r\nif (err) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\ner = kvm_mips_emulate_store(inst, cause, run, vcpu);\r\nif (er == EMULATE_FAIL) {\r\nkvm_err("Emulate store to MMIO space failed\n");\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\n} else {\r\nrun->exit_reason = KVM_EXIT_MMIO;\r\n}\r\nreturn RESUME_HOST;\r\n}\r\nstatic int kvm_mips_bad_access(u32 cause, u32 *opc, struct kvm_run *run,\r\nstruct kvm_vcpu *vcpu, bool store)\r\n{\r\nif (store)\r\nreturn kvm_mips_bad_store(cause, opc, run, vcpu);\r\nelse\r\nreturn kvm_mips_bad_load(cause, opc, run, vcpu);\r\n}\r\nstatic int kvm_trap_emul_handle_tlb_mod(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nunsigned long badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nstruct kvm_mips_tlb *tlb;\r\nunsigned long entryhi;\r\nint index;\r\nif (KVM_GUEST_KSEGX(badvaddr) < KVM_GUEST_KSEG0\r\n|| KVM_GUEST_KSEGX(badvaddr) == KVM_GUEST_KSEG23) {\r\nentryhi = (badvaddr & VPN2_MASK) |\r\n(kvm_read_c0_guest_entryhi(cop0) & KVM_ENTRYHI_ASID);\r\nindex = kvm_mips_guest_tlb_lookup(vcpu, entryhi);\r\nif (unlikely(index < 0)) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\ntlb = vcpu->arch.guest_tlb + index;\r\nif (unlikely(!TLB_IS_VALID(*tlb, badvaddr))) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn RESUME_HOST;\r\n}\r\nif (!TLB_IS_DIRTY(*tlb, badvaddr)) {\r\nkvm_mips_emulate_tlbmod(cause, opc, run, vcpu);\r\nreturn RESUME_GUEST;\r\n}\r\nif (kvm_mips_handle_mapped_seg_tlb_fault(vcpu, tlb, badvaddr,\r\ntrue))\r\nreturn kvm_mips_bad_store(cause, opc, run, vcpu);\r\nreturn RESUME_GUEST;\r\n} else if (KVM_GUEST_KSEGX(badvaddr) == KVM_GUEST_KSEG0) {\r\nif (kvm_mips_handle_kseg0_tlb_fault(badvaddr, vcpu, true) < 0)\r\nreturn kvm_mips_bad_store(cause, opc, run, vcpu);\r\nreturn RESUME_GUEST;\r\n} else {\r\nreturn kvm_mips_bad_store(cause, opc, run, vcpu);\r\n}\r\n}\r\nstatic int kvm_trap_emul_handle_tlb_miss(struct kvm_vcpu *vcpu, bool store)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nunsigned long badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\nif (((badvaddr & PAGE_MASK) == KVM_GUEST_COMMPAGE_ADDR)\r\n&& KVM_GUEST_KERNEL_MODE(vcpu)) {\r\nif (kvm_mips_handle_commpage_tlb_fault(badvaddr, vcpu) < 0) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\n} else if (KVM_GUEST_KSEGX(badvaddr) < KVM_GUEST_KSEG0\r\n|| KVM_GUEST_KSEGX(badvaddr) == KVM_GUEST_KSEG23) {\r\nkvm_debug("USER ADDR TLB %s fault: cause %#x, PC: %p, BadVaddr: %#lx\n",\r\nstore ? "ST" : "LD", cause, opc, badvaddr);\r\ner = kvm_mips_handle_tlbmiss(cause, opc, run, vcpu, store);\r\nif (er == EMULATE_DONE)\r\nret = RESUME_GUEST;\r\nelse {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\n} else if (KVM_GUEST_KSEGX(badvaddr) == KVM_GUEST_KSEG0) {\r\nif (kvm_mips_handle_kseg0_tlb_fault(badvaddr, vcpu, store) < 0)\r\nret = kvm_mips_bad_access(cause, opc, run, vcpu, store);\r\n} else if (KVM_GUEST_KERNEL_MODE(vcpu)\r\n&& (KSEGX(badvaddr) == CKSEG0 || KSEGX(badvaddr) == CKSEG1)) {\r\nret = kvm_mips_bad_access(cause, opc, run, vcpu, store);\r\n} else {\r\nkvm_err("Illegal TLB %s fault address , cause %#x, PC: %p, BadVaddr: %#lx\n",\r\nstore ? "ST" : "LD", cause, opc, badvaddr);\r\nkvm_mips_dump_host_tlbs();\r\nkvm_arch_vcpu_dump_regs(vcpu);\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_handle_tlb_st_miss(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvm_trap_emul_handle_tlb_miss(vcpu, true);\r\n}\r\nstatic int kvm_trap_emul_handle_tlb_ld_miss(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvm_trap_emul_handle_tlb_miss(vcpu, false);\r\n}\r\nstatic int kvm_trap_emul_handle_addr_err_st(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nunsigned long badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nint ret = RESUME_GUEST;\r\nif (KVM_GUEST_KERNEL_MODE(vcpu)\r\n&& (KSEGX(badvaddr) == CKSEG0 || KSEGX(badvaddr) == CKSEG1)) {\r\nret = kvm_mips_bad_store(cause, opc, run, vcpu);\r\n} else {\r\nkvm_err("Address Error (STORE): cause %#x, PC: %p, BadVaddr: %#lx\n",\r\ncause, opc, badvaddr);\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_handle_addr_err_ld(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nunsigned long badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nint ret = RESUME_GUEST;\r\nif (KSEGX(badvaddr) == CKSEG0 || KSEGX(badvaddr) == CKSEG1) {\r\nret = kvm_mips_bad_load(cause, opc, run, vcpu);\r\n} else {\r\nkvm_err("Address Error (LOAD): cause %#x, PC: %p, BadVaddr: %#lx\n",\r\ncause, opc, badvaddr);\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_handle_syscall(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\ner = kvm_mips_emulate_syscall(cause, opc, run, vcpu);\r\nif (er == EMULATE_DONE)\r\nret = RESUME_GUEST;\r\nelse {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_handle_res_inst(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\ner = kvm_mips_handle_ri(cause, opc, run, vcpu);\r\nif (er == EMULATE_DONE)\r\nret = RESUME_GUEST;\r\nelse {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_handle_break(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\ner = kvm_mips_emulate_bp_exc(cause, opc, run, vcpu);\r\nif (er == EMULATE_DONE)\r\nret = RESUME_GUEST;\r\nelse {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_handle_trap(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *)vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\ner = kvm_mips_emulate_trap_exc(cause, opc, run, vcpu);\r\nif (er == EMULATE_DONE) {\r\nret = RESUME_GUEST;\r\n} else {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_handle_msa_fpe(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *)vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\ner = kvm_mips_emulate_msafpe_exc(cause, opc, run, vcpu);\r\nif (er == EMULATE_DONE) {\r\nret = RESUME_GUEST;\r\n} else {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_handle_fpe(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *)vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\ner = kvm_mips_emulate_fpe_exc(cause, opc, run, vcpu);\r\nif (er == EMULATE_DONE) {\r\nret = RESUME_GUEST;\r\n} else {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_handle_msa_disabled(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_run *run = vcpu->run;\r\nu32 __user *opc = (u32 __user *) vcpu->arch.pc;\r\nu32 cause = vcpu->arch.host_cp0_cause;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\nif (!kvm_mips_guest_has_msa(&vcpu->arch) ||\r\n(kvm_read_c0_guest_status(cop0) & (ST0_CU1 | ST0_FR)) == ST0_CU1) {\r\ner = kvm_mips_emulate_ri_exc(cause, opc, run, vcpu);\r\n} else if (!(kvm_read_c0_guest_config5(cop0) & MIPS_CONF5_MSAEN)) {\r\ner = kvm_mips_emulate_msadis_exc(cause, opc, run, vcpu);\r\n} else {\r\nkvm_own_msa(vcpu);\r\ner = EMULATE_DONE;\r\n}\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nret = RESUME_GUEST;\r\nbreak;\r\ncase EMULATE_FAIL:\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_hardware_enable(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic void kvm_trap_emul_hardware_disable(void)\r\n{\r\n}\r\nstatic int kvm_trap_emul_check_extension(struct kvm *kvm, long ext)\r\n{\r\nint r;\r\nswitch (ext) {\r\ncase KVM_CAP_MIPS_TE:\r\nr = 1;\r\nbreak;\r\ndefault:\r\nr = 0;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstatic int kvm_trap_emul_vcpu_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mm_struct *kern_mm = &vcpu->arch.guest_kernel_mm;\r\nstruct mm_struct *user_mm = &vcpu->arch.guest_user_mm;\r\nkern_mm->pgd = pgd_alloc(kern_mm);\r\nif (!kern_mm->pgd)\r\nreturn -ENOMEM;\r\nuser_mm->pgd = pgd_alloc(user_mm);\r\nif (!user_mm->pgd) {\r\npgd_free(kern_mm, kern_mm->pgd);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void kvm_mips_emul_free_gva_pt(pgd_t *pgd)\r\n{\r\nconst unsigned long end = 0x80000000;\r\nunsigned long pgd_va, pud_va, pmd_va;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nint i, j, k;\r\nfor (i = 0; i < USER_PTRS_PER_PGD; i++) {\r\nif (pgd_none(pgd[i]))\r\ncontinue;\r\npgd_va = (unsigned long)i << PGDIR_SHIFT;\r\nif (pgd_va >= end)\r\nbreak;\r\npud = pud_offset(pgd + i, 0);\r\nfor (j = 0; j < PTRS_PER_PUD; j++) {\r\nif (pud_none(pud[j]))\r\ncontinue;\r\npud_va = pgd_va | ((unsigned long)j << PUD_SHIFT);\r\nif (pud_va >= end)\r\nbreak;\r\npmd = pmd_offset(pud + j, 0);\r\nfor (k = 0; k < PTRS_PER_PMD; k++) {\r\nif (pmd_none(pmd[k]))\r\ncontinue;\r\npmd_va = pud_va | (k << PMD_SHIFT);\r\nif (pmd_va >= end)\r\nbreak;\r\npte = pte_offset(pmd + k, 0);\r\npte_free_kernel(NULL, pte);\r\n}\r\npmd_free(NULL, pmd);\r\n}\r\npud_free(NULL, pud);\r\n}\r\npgd_free(NULL, pgd);\r\n}\r\nstatic void kvm_trap_emul_vcpu_uninit(struct kvm_vcpu *vcpu)\r\n{\r\nkvm_mips_emul_free_gva_pt(vcpu->arch.guest_kernel_mm.pgd);\r\nkvm_mips_emul_free_gva_pt(vcpu->arch.guest_user_mm.pgd);\r\n}\r\nstatic int kvm_trap_emul_vcpu_setup(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nu32 config, config1;\r\nint vcpu_id = vcpu->vcpu_id;\r\nkvm_mips_init_count(vcpu, 100*1000*1000);\r\n#ifndef CONFIG_CPU_MIPSR6\r\nkvm_write_c0_guest_prid(cop0, 0x00019300);\r\n#else\r\nkvm_write_c0_guest_prid(cop0, 0x00010000);\r\n#endif\r\nconfig = read_c0_config() & MIPS_CONF_AR;\r\nconfig |= MIPS_CONF_M | CONF_CM_CACHABLE_NONCOHERENT | MIPS_CONF_MT_TLB;\r\n#ifdef CONFIG_CPU_BIG_ENDIAN\r\nconfig |= CONF_BE;\r\n#endif\r\nif (cpu_has_vtag_icache)\r\nconfig |= MIPS_CONF_VI;\r\nkvm_write_c0_guest_config(cop0, config);\r\nconfig1 = (read_c0_config1() & ~0x7f);\r\nif (cpu_dcache_line_size()) {\r\nconfig1 &= ~MIPS_CONF1_DL;\r\nconfig1 |= ((ilog2(cpu_dcache_line_size()) - 1) <<\r\nMIPS_CONF1_DL_SHF) & MIPS_CONF1_DL;\r\n}\r\nconfig1 &= ~(0x3f << 25);\r\nconfig1 |= ((KVM_MIPS_GUEST_TLB_SIZE - 1) << 25);\r\nconfig1 &= ~(MIPS_CONF1_C2 | MIPS_CONF1_MD | MIPS_CONF1_PC |\r\nMIPS_CONF1_WR | MIPS_CONF1_CA);\r\nkvm_write_c0_guest_config1(cop0, config1);\r\nkvm_write_c0_guest_config2(cop0, MIPS_CONF_M);\r\nkvm_write_c0_guest_config3(cop0, MIPS_CONF_M | MIPS_CONF3_ULRI);\r\nkvm_write_c0_guest_config4(cop0, MIPS_CONF_M);\r\nkvm_write_c0_guest_config5(cop0, 0);\r\nkvm_write_c0_guest_config7(cop0, (MIPS_CONF7_WII) | (1 << 10));\r\nkvm_write_c0_guest_status(cop0, ST0_BEV | ST0_ERL);\r\nkvm_write_c0_guest_intctl(cop0, 0xFC000000);\r\nkvm_write_c0_guest_ebase(cop0, KVM_GUEST_KSEG0 |\r\n(vcpu_id & MIPS_EBASE_CPUNUM));\r\nvcpu->arch.pc = KVM_GUEST_CKSEG1ADDR(0x1fc00000);\r\nreturn 0;\r\n}\r\nstatic void kvm_trap_emul_flush_shadow_all(struct kvm *kvm)\r\n{\r\nkvm_flush_remote_tlbs(kvm);\r\n}\r\nstatic void kvm_trap_emul_flush_shadow_memslot(struct kvm *kvm,\r\nconst struct kvm_memory_slot *slot)\r\n{\r\nkvm_trap_emul_flush_shadow_all(kvm);\r\n}\r\nstatic unsigned long kvm_trap_emul_num_regs(struct kvm_vcpu *vcpu)\r\n{\r\nreturn ARRAY_SIZE(kvm_trap_emul_get_one_regs);\r\n}\r\nstatic int kvm_trap_emul_copy_reg_indices(struct kvm_vcpu *vcpu,\r\nu64 __user *indices)\r\n{\r\nif (copy_to_user(indices, kvm_trap_emul_get_one_regs,\r\nsizeof(kvm_trap_emul_get_one_regs)))\r\nreturn -EFAULT;\r\nindices += ARRAY_SIZE(kvm_trap_emul_get_one_regs);\r\nreturn 0;\r\n}\r\nstatic int kvm_trap_emul_get_one_reg(struct kvm_vcpu *vcpu,\r\nconst struct kvm_one_reg *reg,\r\ns64 *v)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nswitch (reg->id) {\r\ncase KVM_REG_MIPS_CP0_INDEX:\r\n*v = (long)kvm_read_c0_guest_index(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYLO0:\r\n*v = kvm_read_c0_guest_entrylo0(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYLO1:\r\n*v = kvm_read_c0_guest_entrylo1(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONTEXT:\r\n*v = (long)kvm_read_c0_guest_context(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_USERLOCAL:\r\n*v = (long)kvm_read_c0_guest_userlocal(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PAGEMASK:\r\n*v = (long)kvm_read_c0_guest_pagemask(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_WIRED:\r\n*v = (long)kvm_read_c0_guest_wired(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_HWRENA:\r\n*v = (long)kvm_read_c0_guest_hwrena(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADVADDR:\r\n*v = (long)kvm_read_c0_guest_badvaddr(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYHI:\r\n*v = (long)kvm_read_c0_guest_entryhi(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_COMPARE:\r\n*v = (long)kvm_read_c0_guest_compare(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_STATUS:\r\n*v = (long)kvm_read_c0_guest_status(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_INTCTL:\r\n*v = (long)kvm_read_c0_guest_intctl(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CAUSE:\r\n*v = (long)kvm_read_c0_guest_cause(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_EPC:\r\n*v = (long)kvm_read_c0_guest_epc(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PRID:\r\n*v = (long)kvm_read_c0_guest_prid(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_EBASE:\r\n*v = (long)kvm_read_c0_guest_ebase(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG:\r\n*v = (long)kvm_read_c0_guest_config(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG1:\r\n*v = (long)kvm_read_c0_guest_config1(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG2:\r\n*v = (long)kvm_read_c0_guest_config2(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG3:\r\n*v = (long)kvm_read_c0_guest_config3(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG4:\r\n*v = (long)kvm_read_c0_guest_config4(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG5:\r\n*v = (long)kvm_read_c0_guest_config5(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG7:\r\n*v = (long)kvm_read_c0_guest_config7(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_COUNT:\r\n*v = kvm_mips_read_count(vcpu);\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_CTL:\r\n*v = vcpu->arch.count_ctl;\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_RESUME:\r\n*v = ktime_to_ns(vcpu->arch.count_resume);\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_HZ:\r\n*v = vcpu->arch.count_hz;\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ERROREPC:\r\n*v = (long)kvm_read_c0_guest_errorepc(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH1:\r\n*v = (long)kvm_read_c0_guest_kscratch1(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH2:\r\n*v = (long)kvm_read_c0_guest_kscratch2(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH3:\r\n*v = (long)kvm_read_c0_guest_kscratch3(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH4:\r\n*v = (long)kvm_read_c0_guest_kscratch4(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH5:\r\n*v = (long)kvm_read_c0_guest_kscratch5(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH6:\r\n*v = (long)kvm_read_c0_guest_kscratch6(cop0);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int kvm_trap_emul_set_one_reg(struct kvm_vcpu *vcpu,\r\nconst struct kvm_one_reg *reg,\r\ns64 v)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nint ret = 0;\r\nunsigned int cur, change;\r\nswitch (reg->id) {\r\ncase KVM_REG_MIPS_CP0_INDEX:\r\nkvm_write_c0_guest_index(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYLO0:\r\nkvm_write_c0_guest_entrylo0(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYLO1:\r\nkvm_write_c0_guest_entrylo1(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONTEXT:\r\nkvm_write_c0_guest_context(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_USERLOCAL:\r\nkvm_write_c0_guest_userlocal(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PAGEMASK:\r\nkvm_write_c0_guest_pagemask(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_WIRED:\r\nkvm_write_c0_guest_wired(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_HWRENA:\r\nkvm_write_c0_guest_hwrena(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADVADDR:\r\nkvm_write_c0_guest_badvaddr(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYHI:\r\nkvm_write_c0_guest_entryhi(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_STATUS:\r\nkvm_write_c0_guest_status(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_INTCTL:\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_EPC:\r\nkvm_write_c0_guest_epc(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PRID:\r\nkvm_write_c0_guest_prid(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_EBASE:\r\nkvm_change_c0_guest_ebase(cop0, 0x1ffff000 | MIPS_EBASE_CPUNUM,\r\nv);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_COUNT:\r\nkvm_mips_write_count(vcpu, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_COMPARE:\r\nkvm_mips_write_compare(vcpu, v, false);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CAUSE:\r\nif ((kvm_read_c0_guest_cause(cop0) ^ v) & CAUSEF_DC) {\r\nif (v & CAUSEF_DC) {\r\nkvm_mips_count_disable_cause(vcpu);\r\nkvm_change_c0_guest_cause(cop0, (u32)~CAUSEF_DC,\r\nv);\r\n} else {\r\nkvm_change_c0_guest_cause(cop0, (u32)~CAUSEF_DC,\r\nv);\r\nkvm_mips_count_enable_cause(vcpu);\r\n}\r\n} else {\r\nkvm_write_c0_guest_cause(cop0, v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG:\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG1:\r\ncur = kvm_read_c0_guest_config1(cop0);\r\nchange = (cur ^ v) & kvm_mips_config1_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nkvm_write_c0_guest_config1(cop0, v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG2:\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG3:\r\ncur = kvm_read_c0_guest_config3(cop0);\r\nchange = (cur ^ v) & kvm_mips_config3_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nkvm_write_c0_guest_config3(cop0, v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG4:\r\ncur = kvm_read_c0_guest_config4(cop0);\r\nchange = (cur ^ v) & kvm_mips_config4_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nkvm_write_c0_guest_config4(cop0, v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG5:\r\ncur = kvm_read_c0_guest_config5(cop0);\r\nchange = (cur ^ v) & kvm_mips_config5_wrmask(vcpu);\r\nif (change) {\r\nv = cur ^ change;\r\nkvm_write_c0_guest_config5(cop0, v);\r\n}\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG7:\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_CTL:\r\nret = kvm_mips_set_count_ctl(vcpu, v);\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_RESUME:\r\nret = kvm_mips_set_count_resume(vcpu, v);\r\nbreak;\r\ncase KVM_REG_MIPS_COUNT_HZ:\r\nret = kvm_mips_set_count_hz(vcpu, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ERROREPC:\r\nkvm_write_c0_guest_errorepc(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH1:\r\nkvm_write_c0_guest_kscratch1(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH2:\r\nkvm_write_c0_guest_kscratch2(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH3:\r\nkvm_write_c0_guest_kscratch3(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH4:\r\nkvm_write_c0_guest_kscratch4(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH5:\r\nkvm_write_c0_guest_kscratch5(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_KSCRATCH6:\r\nkvm_write_c0_guest_kscratch6(cop0, v);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn ret;\r\n}\r\nstatic int kvm_trap_emul_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nstruct mm_struct *kern_mm = &vcpu->arch.guest_kernel_mm;\r\nstruct mm_struct *user_mm = &vcpu->arch.guest_user_mm;\r\nstruct mm_struct *mm;\r\nif (current->flags & PF_VCPU) {\r\nmm = KVM_GUEST_KERNEL_MODE(vcpu) ? kern_mm : user_mm;\r\nif ((cpu_context(cpu, mm) ^ asid_cache(cpu)) &\r\nasid_version_mask(cpu))\r\nget_new_mmu_context(mm, cpu);\r\nwrite_c0_entryhi(cpu_asid(cpu, mm));\r\nTLBMISS_HANDLER_SETUP_PGD(mm->pgd);\r\nkvm_mips_suspend_mm(cpu);\r\nehb();\r\n}\r\nreturn 0;\r\n}\r\nstatic int kvm_trap_emul_vcpu_put(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nkvm_lose_fpu(vcpu);\r\nif (current->flags & PF_VCPU) {\r\nif (((cpu_context(cpu, current->mm) ^ asid_cache(cpu)) &\r\nasid_version_mask(cpu)))\r\nget_new_mmu_context(current->mm, cpu);\r\nwrite_c0_entryhi(cpu_asid(cpu, current->mm));\r\nTLBMISS_HANDLER_SETUP_PGD(current->mm->pgd);\r\nkvm_mips_resume_mm(cpu);\r\nehb();\r\n}\r\nreturn 0;\r\n}\r\nstatic void kvm_trap_emul_check_requests(struct kvm_vcpu *vcpu, int cpu,\r\nbool reload_asid)\r\n{\r\nstruct mm_struct *kern_mm = &vcpu->arch.guest_kernel_mm;\r\nstruct mm_struct *user_mm = &vcpu->arch.guest_user_mm;\r\nstruct mm_struct *mm;\r\nint i;\r\nif (likely(!kvm_request_pending(vcpu)))\r\nreturn;\r\nif (kvm_check_request(KVM_REQ_TLB_FLUSH, vcpu)) {\r\nkvm_mips_flush_gva_pt(kern_mm->pgd, KMF_GPA | KMF_KERN);\r\nkvm_mips_flush_gva_pt(user_mm->pgd, KMF_GPA | KMF_USER);\r\nfor_each_possible_cpu(i) {\r\ncpu_context(i, kern_mm) = 0;\r\ncpu_context(i, user_mm) = 0;\r\n}\r\nif (reload_asid) {\r\nmm = KVM_GUEST_KERNEL_MODE(vcpu) ? kern_mm : user_mm;\r\nget_new_mmu_context(mm, cpu);\r\nhtw_stop();\r\nwrite_c0_entryhi(cpu_asid(cpu, mm));\r\nTLBMISS_HANDLER_SETUP_PGD(mm->pgd);\r\nhtw_start();\r\n}\r\n}\r\n}\r\nvoid kvm_trap_emul_gva_lockless_begin(struct kvm_vcpu *vcpu)\r\n{\r\nWARN_ON_ONCE(irqs_disabled());\r\nlocal_irq_disable();\r\nsmp_store_mb(vcpu->mode, READING_SHADOW_PAGE_TABLES);\r\nkvm_trap_emul_check_requests(vcpu, smp_processor_id(), true);\r\n}\r\nvoid kvm_trap_emul_gva_lockless_end(struct kvm_vcpu *vcpu)\r\n{\r\nsmp_store_release(&vcpu->mode, OUTSIDE_GUEST_MODE);\r\nlocal_irq_enable();\r\n}\r\nstatic void kvm_trap_emul_vcpu_reenter(struct kvm_run *run,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nstruct mm_struct *kern_mm = &vcpu->arch.guest_kernel_mm;\r\nstruct mm_struct *user_mm = &vcpu->arch.guest_user_mm;\r\nstruct mm_struct *mm;\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nint i, cpu = smp_processor_id();\r\nunsigned int gasid;\r\nkvm_trap_emul_check_requests(vcpu, cpu, false);\r\nif (KVM_GUEST_KERNEL_MODE(vcpu)) {\r\nmm = kern_mm;\r\n} else {\r\nmm = user_mm;\r\ngasid = kvm_read_c0_guest_entryhi(cop0) & KVM_ENTRYHI_ASID;\r\nif (gasid != vcpu->arch.last_user_gasid) {\r\nkvm_mips_flush_gva_pt(user_mm->pgd, KMF_USER);\r\nfor_each_possible_cpu(i)\r\ncpu_context(i, user_mm) = 0;\r\nvcpu->arch.last_user_gasid = gasid;\r\n}\r\n}\r\nif ((cpu_context(cpu, mm) ^ asid_cache(cpu)) &\r\nasid_version_mask(cpu))\r\nget_new_mmu_context(mm, cpu);\r\n}\r\nstatic int kvm_trap_emul_vcpu_run(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nint cpu = smp_processor_id();\r\nint r;\r\nkvm_mips_deliver_interrupts(vcpu,\r\nkvm_read_c0_guest_cause(vcpu->arch.cop0));\r\nkvm_trap_emul_vcpu_reenter(run, vcpu);\r\npagefault_disable();\r\nhtw_stop();\r\nkvm_mips_suspend_mm(cpu);\r\nr = vcpu->arch.vcpu_run(run, vcpu);\r\ncpu = smp_processor_id();\r\nif (((cpu_context(cpu, current->mm) ^ asid_cache(cpu)) &\r\nasid_version_mask(cpu)))\r\nget_new_mmu_context(current->mm, cpu);\r\nwrite_c0_entryhi(cpu_asid(cpu, current->mm));\r\nTLBMISS_HANDLER_SETUP_PGD(current->mm->pgd);\r\nkvm_mips_resume_mm(cpu);\r\nhtw_start();\r\npagefault_enable();\r\nreturn r;\r\n}\r\nint kvm_mips_emulation_init(struct kvm_mips_callbacks **install_callbacks)\r\n{\r\n*install_callbacks = &kvm_trap_emul_callbacks;\r\nreturn 0;\r\n}
