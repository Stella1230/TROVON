static unsigned sev_pos(const struct v4l2_subscribed_event *sev, unsigned idx)\r\n{\r\nidx += sev->first;\r\nreturn idx >= sev->elems ? idx - sev->elems : idx;\r\n}\r\nstatic int __v4l2_event_dequeue(struct v4l2_fh *fh, struct v4l2_event *event)\r\n{\r\nstruct v4l2_kevent *kev;\r\nunsigned long flags;\r\nspin_lock_irqsave(&fh->vdev->fh_lock, flags);\r\nif (list_empty(&fh->available)) {\r\nspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\r\nreturn -ENOENT;\r\n}\r\nWARN_ON(fh->navailable == 0);\r\nkev = list_first_entry(&fh->available, struct v4l2_kevent, list);\r\nlist_del(&kev->list);\r\nfh->navailable--;\r\nkev->event.pending = fh->navailable;\r\n*event = kev->event;\r\nkev->sev->first = sev_pos(kev->sev, 1);\r\nkev->sev->in_use--;\r\nspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\r\nreturn 0;\r\n}\r\nint v4l2_event_dequeue(struct v4l2_fh *fh, struct v4l2_event *event,\r\nint nonblocking)\r\n{\r\nint ret;\r\nif (nonblocking)\r\nreturn __v4l2_event_dequeue(fh, event);\r\nif (fh->vdev->lock)\r\nmutex_unlock(fh->vdev->lock);\r\ndo {\r\nret = wait_event_interruptible(fh->wait,\r\nfh->navailable != 0);\r\nif (ret < 0)\r\nbreak;\r\nret = __v4l2_event_dequeue(fh, event);\r\n} while (ret == -ENOENT);\r\nif (fh->vdev->lock)\r\nmutex_lock(fh->vdev->lock);\r\nreturn ret;\r\n}\r\nstatic struct v4l2_subscribed_event *v4l2_event_subscribed(\r\nstruct v4l2_fh *fh, u32 type, u32 id)\r\n{\r\nstruct v4l2_subscribed_event *sev;\r\nassert_spin_locked(&fh->vdev->fh_lock);\r\nlist_for_each_entry(sev, &fh->subscribed, list)\r\nif (sev->type == type && sev->id == id)\r\nreturn sev;\r\nreturn NULL;\r\n}\r\nstatic void __v4l2_event_queue_fh(struct v4l2_fh *fh, const struct v4l2_event *ev,\r\nconst struct timespec *ts)\r\n{\r\nstruct v4l2_subscribed_event *sev;\r\nstruct v4l2_kevent *kev;\r\nbool copy_payload = true;\r\nsev = v4l2_event_subscribed(fh, ev->type, ev->id);\r\nif (sev == NULL)\r\nreturn;\r\nif (!sev->elems)\r\nreturn;\r\nfh->sequence++;\r\nif (sev->in_use == sev->elems) {\r\nkev = sev->events + sev_pos(sev, 0);\r\nlist_del(&kev->list);\r\nsev->in_use--;\r\nsev->first = sev_pos(sev, 1);\r\nfh->navailable--;\r\nif (sev->elems == 1) {\r\nif (sev->ops && sev->ops->replace) {\r\nsev->ops->replace(&kev->event, ev);\r\ncopy_payload = false;\r\n}\r\n} else if (sev->ops && sev->ops->merge) {\r\nstruct v4l2_kevent *second_oldest =\r\nsev->events + sev_pos(sev, 0);\r\nsev->ops->merge(&kev->event, &second_oldest->event);\r\n}\r\n}\r\nkev = sev->events + sev_pos(sev, sev->in_use);\r\nkev->event.type = ev->type;\r\nif (copy_payload)\r\nkev->event.u = ev->u;\r\nkev->event.id = ev->id;\r\nkev->event.timestamp = *ts;\r\nkev->event.sequence = fh->sequence;\r\nsev->in_use++;\r\nlist_add_tail(&kev->list, &fh->available);\r\nfh->navailable++;\r\nwake_up_all(&fh->wait);\r\n}\r\nvoid v4l2_event_queue(struct video_device *vdev, const struct v4l2_event *ev)\r\n{\r\nstruct v4l2_fh *fh;\r\nunsigned long flags;\r\nstruct timespec timestamp;\r\nktime_get_ts(&timestamp);\r\nspin_lock_irqsave(&vdev->fh_lock, flags);\r\nlist_for_each_entry(fh, &vdev->fh_list, list)\r\n__v4l2_event_queue_fh(fh, ev, &timestamp);\r\nspin_unlock_irqrestore(&vdev->fh_lock, flags);\r\n}\r\nvoid v4l2_event_queue_fh(struct v4l2_fh *fh, const struct v4l2_event *ev)\r\n{\r\nunsigned long flags;\r\nstruct timespec timestamp;\r\nktime_get_ts(&timestamp);\r\nspin_lock_irqsave(&fh->vdev->fh_lock, flags);\r\n__v4l2_event_queue_fh(fh, ev, &timestamp);\r\nspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\r\n}\r\nint v4l2_event_pending(struct v4l2_fh *fh)\r\n{\r\nreturn fh->navailable;\r\n}\r\nint v4l2_event_subscribe(struct v4l2_fh *fh,\r\nstruct v4l2_event_subscription *sub, unsigned elems,\r\nconst struct v4l2_subscribed_event_ops *ops)\r\n{\r\nstruct v4l2_subscribed_event *sev, *found_ev;\r\nunsigned long flags;\r\nunsigned i;\r\nif (sub->type == V4L2_EVENT_ALL)\r\nreturn -EINVAL;\r\nif (elems < 1)\r\nelems = 1;\r\nsev = kzalloc(sizeof(*sev) + sizeof(struct v4l2_kevent) * elems, GFP_KERNEL);\r\nif (!sev)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < elems; i++)\r\nsev->events[i].sev = sev;\r\nsev->type = sub->type;\r\nsev->id = sub->id;\r\nsev->flags = sub->flags;\r\nsev->fh = fh;\r\nsev->ops = ops;\r\nspin_lock_irqsave(&fh->vdev->fh_lock, flags);\r\nfound_ev = v4l2_event_subscribed(fh, sub->type, sub->id);\r\nif (!found_ev)\r\nlist_add(&sev->list, &fh->subscribed);\r\nspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\r\nif (found_ev) {\r\nkfree(sev);\r\nreturn 0;\r\n}\r\nif (sev->ops && sev->ops->add) {\r\nint ret = sev->ops->add(sev, elems);\r\nif (ret) {\r\nsev->ops = NULL;\r\nv4l2_event_unsubscribe(fh, sub);\r\nreturn ret;\r\n}\r\n}\r\nsev->elems = elems;\r\nreturn 0;\r\n}\r\nvoid v4l2_event_unsubscribe_all(struct v4l2_fh *fh)\r\n{\r\nstruct v4l2_event_subscription sub;\r\nstruct v4l2_subscribed_event *sev;\r\nunsigned long flags;\r\ndo {\r\nsev = NULL;\r\nspin_lock_irqsave(&fh->vdev->fh_lock, flags);\r\nif (!list_empty(&fh->subscribed)) {\r\nsev = list_first_entry(&fh->subscribed,\r\nstruct v4l2_subscribed_event, list);\r\nsub.type = sev->type;\r\nsub.id = sev->id;\r\n}\r\nspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\r\nif (sev)\r\nv4l2_event_unsubscribe(fh, &sub);\r\n} while (sev);\r\n}\r\nint v4l2_event_unsubscribe(struct v4l2_fh *fh,\r\nstruct v4l2_event_subscription *sub)\r\n{\r\nstruct v4l2_subscribed_event *sev;\r\nunsigned long flags;\r\nint i;\r\nif (sub->type == V4L2_EVENT_ALL) {\r\nv4l2_event_unsubscribe_all(fh);\r\nreturn 0;\r\n}\r\nspin_lock_irqsave(&fh->vdev->fh_lock, flags);\r\nsev = v4l2_event_subscribed(fh, sub->type, sub->id);\r\nif (sev != NULL) {\r\nfor (i = 0; i < sev->in_use; i++) {\r\nlist_del(&sev->events[sev_pos(sev, i)].list);\r\nfh->navailable--;\r\n}\r\nlist_del(&sev->list);\r\n}\r\nspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\r\nif (sev && sev->ops && sev->ops->del)\r\nsev->ops->del(sev);\r\nkfree(sev);\r\nreturn 0;\r\n}
