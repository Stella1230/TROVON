void __init hibernate_reserved_size_init(void)\r\n{\r\nreserved_size = SPARE_PAGES * PAGE_SIZE;\r\n}\r\nvoid __init hibernate_image_size_init(void)\r\n{\r\nimage_size = ((totalram_pages * 2) / 5) * PAGE_SIZE;\r\n}\r\nstatic void *get_image_page(gfp_t gfp_mask, int safe_needed)\r\n{\r\nvoid *res;\r\nres = (void *)get_zeroed_page(gfp_mask);\r\nif (safe_needed)\r\nwhile (res && swsusp_page_is_free(virt_to_page(res))) {\r\nswsusp_set_page_forbidden(virt_to_page(res));\r\nallocated_unsafe_pages++;\r\nres = (void *)get_zeroed_page(gfp_mask);\r\n}\r\nif (res) {\r\nswsusp_set_page_forbidden(virt_to_page(res));\r\nswsusp_set_page_free(virt_to_page(res));\r\n}\r\nreturn res;\r\n}\r\nunsigned long get_safe_page(gfp_t gfp_mask)\r\n{\r\nreturn (unsigned long)get_image_page(gfp_mask, PG_SAFE);\r\n}\r\nstatic struct page *alloc_image_page(gfp_t gfp_mask)\r\n{\r\nstruct page *page;\r\npage = alloc_page(gfp_mask);\r\nif (page) {\r\nswsusp_set_page_forbidden(page);\r\nswsusp_set_page_free(page);\r\n}\r\nreturn page;\r\n}\r\nstatic inline void free_image_page(void *addr, int clear_nosave_free)\r\n{\r\nstruct page *page;\r\nBUG_ON(!virt_addr_valid(addr));\r\npage = virt_to_page(addr);\r\nswsusp_unset_page_forbidden(page);\r\nif (clear_nosave_free)\r\nswsusp_unset_page_free(page);\r\n__free_page(page);\r\n}\r\nstatic inline void\r\nfree_list_of_pages(struct linked_page *list, int clear_page_nosave)\r\n{\r\nwhile (list) {\r\nstruct linked_page *lp = list->next;\r\nfree_image_page(list, clear_page_nosave);\r\nlist = lp;\r\n}\r\n}\r\nstatic void\r\nchain_init(struct chain_allocator *ca, gfp_t gfp_mask, int safe_needed)\r\n{\r\nca->chain = NULL;\r\nca->used_space = LINKED_PAGE_DATA_SIZE;\r\nca->gfp_mask = gfp_mask;\r\nca->safe_needed = safe_needed;\r\n}\r\nstatic void *chain_alloc(struct chain_allocator *ca, unsigned int size)\r\n{\r\nvoid *ret;\r\nif (LINKED_PAGE_DATA_SIZE - ca->used_space < size) {\r\nstruct linked_page *lp;\r\nlp = get_image_page(ca->gfp_mask, ca->safe_needed);\r\nif (!lp)\r\nreturn NULL;\r\nlp->next = ca->chain;\r\nca->chain = lp;\r\nca->used_space = 0;\r\n}\r\nret = ca->chain->data + ca->used_space;\r\nca->used_space += size;\r\nreturn ret;\r\n}\r\nstatic inline unsigned long bm_block_bits(struct bm_block *bb)\r\n{\r\nreturn bb->end_pfn - bb->start_pfn;\r\n}\r\nstatic void memory_bm_position_reset(struct memory_bitmap *bm)\r\n{\r\nbm->cur.block = list_entry(bm->blocks.next, struct bm_block, hook);\r\nbm->cur.bit = 0;\r\n}\r\nstatic int create_bm_block_list(unsigned long pages,\r\nstruct list_head *list,\r\nstruct chain_allocator *ca)\r\n{\r\nunsigned int nr_blocks = DIV_ROUND_UP(pages, BM_BITS_PER_BLOCK);\r\nwhile (nr_blocks-- > 0) {\r\nstruct bm_block *bb;\r\nbb = chain_alloc(ca, sizeof(struct bm_block));\r\nif (!bb)\r\nreturn -ENOMEM;\r\nlist_add(&bb->hook, list);\r\n}\r\nreturn 0;\r\n}\r\nstatic void free_mem_extents(struct list_head *list)\r\n{\r\nstruct mem_extent *ext, *aux;\r\nlist_for_each_entry_safe(ext, aux, list, hook) {\r\nlist_del(&ext->hook);\r\nkfree(ext);\r\n}\r\n}\r\nstatic int create_mem_extents(struct list_head *list, gfp_t gfp_mask)\r\n{\r\nstruct zone *zone;\r\nINIT_LIST_HEAD(list);\r\nfor_each_populated_zone(zone) {\r\nunsigned long zone_start, zone_end;\r\nstruct mem_extent *ext, *cur, *aux;\r\nzone_start = zone->zone_start_pfn;\r\nzone_end = zone->zone_start_pfn + zone->spanned_pages;\r\nlist_for_each_entry(ext, list, hook)\r\nif (zone_start <= ext->end)\r\nbreak;\r\nif (&ext->hook == list || zone_end < ext->start) {\r\nstruct mem_extent *new_ext;\r\nnew_ext = kzalloc(sizeof(struct mem_extent), gfp_mask);\r\nif (!new_ext) {\r\nfree_mem_extents(list);\r\nreturn -ENOMEM;\r\n}\r\nnew_ext->start = zone_start;\r\nnew_ext->end = zone_end;\r\nlist_add_tail(&new_ext->hook, &ext->hook);\r\ncontinue;\r\n}\r\nif (zone_start < ext->start)\r\next->start = zone_start;\r\nif (zone_end > ext->end)\r\next->end = zone_end;\r\ncur = ext;\r\nlist_for_each_entry_safe_continue(cur, aux, list, hook) {\r\nif (zone_end < cur->start)\r\nbreak;\r\nif (zone_end < cur->end)\r\next->end = cur->end;\r\nlist_del(&cur->hook);\r\nkfree(cur);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nmemory_bm_create(struct memory_bitmap *bm, gfp_t gfp_mask, int safe_needed)\r\n{\r\nstruct chain_allocator ca;\r\nstruct list_head mem_extents;\r\nstruct mem_extent *ext;\r\nint error;\r\nchain_init(&ca, gfp_mask, safe_needed);\r\nINIT_LIST_HEAD(&bm->blocks);\r\nerror = create_mem_extents(&mem_extents, gfp_mask);\r\nif (error)\r\nreturn error;\r\nlist_for_each_entry(ext, &mem_extents, hook) {\r\nstruct bm_block *bb;\r\nunsigned long pfn = ext->start;\r\nunsigned long pages = ext->end - ext->start;\r\nbb = list_entry(bm->blocks.prev, struct bm_block, hook);\r\nerror = create_bm_block_list(pages, bm->blocks.prev, &ca);\r\nif (error)\r\ngoto Error;\r\nlist_for_each_entry_continue(bb, &bm->blocks, hook) {\r\nbb->data = get_image_page(gfp_mask, safe_needed);\r\nif (!bb->data) {\r\nerror = -ENOMEM;\r\ngoto Error;\r\n}\r\nbb->start_pfn = pfn;\r\nif (pages >= BM_BITS_PER_BLOCK) {\r\npfn += BM_BITS_PER_BLOCK;\r\npages -= BM_BITS_PER_BLOCK;\r\n} else {\r\npfn += pages;\r\n}\r\nbb->end_pfn = pfn;\r\n}\r\n}\r\nbm->p_list = ca.chain;\r\nmemory_bm_position_reset(bm);\r\nExit:\r\nfree_mem_extents(&mem_extents);\r\nreturn error;\r\nError:\r\nbm->p_list = ca.chain;\r\nmemory_bm_free(bm, PG_UNSAFE_CLEAR);\r\ngoto Exit;\r\n}\r\nstatic void memory_bm_free(struct memory_bitmap *bm, int clear_nosave_free)\r\n{\r\nstruct bm_block *bb;\r\nlist_for_each_entry(bb, &bm->blocks, hook)\r\nif (bb->data)\r\nfree_image_page(bb->data, clear_nosave_free);\r\nfree_list_of_pages(bm->p_list, clear_nosave_free);\r\nINIT_LIST_HEAD(&bm->blocks);\r\n}\r\nstatic int memory_bm_find_bit(struct memory_bitmap *bm, unsigned long pfn,\r\nvoid **addr, unsigned int *bit_nr)\r\n{\r\nstruct bm_block *bb;\r\nbb = bm->cur.block;\r\nif (pfn < bb->start_pfn)\r\nlist_for_each_entry_continue_reverse(bb, &bm->blocks, hook)\r\nif (pfn >= bb->start_pfn)\r\nbreak;\r\nif (pfn >= bb->end_pfn)\r\nlist_for_each_entry_continue(bb, &bm->blocks, hook)\r\nif (pfn >= bb->start_pfn && pfn < bb->end_pfn)\r\nbreak;\r\nif (&bb->hook == &bm->blocks)\r\nreturn -EFAULT;\r\nbm->cur.block = bb;\r\npfn -= bb->start_pfn;\r\nbm->cur.bit = pfn + 1;\r\n*bit_nr = pfn;\r\n*addr = bb->data;\r\nreturn 0;\r\n}\r\nstatic void memory_bm_set_bit(struct memory_bitmap *bm, unsigned long pfn)\r\n{\r\nvoid *addr;\r\nunsigned int bit;\r\nint error;\r\nerror = memory_bm_find_bit(bm, pfn, &addr, &bit);\r\nBUG_ON(error);\r\nset_bit(bit, addr);\r\n}\r\nstatic int mem_bm_set_bit_check(struct memory_bitmap *bm, unsigned long pfn)\r\n{\r\nvoid *addr;\r\nunsigned int bit;\r\nint error;\r\nerror = memory_bm_find_bit(bm, pfn, &addr, &bit);\r\nif (!error)\r\nset_bit(bit, addr);\r\nreturn error;\r\n}\r\nstatic void memory_bm_clear_bit(struct memory_bitmap *bm, unsigned long pfn)\r\n{\r\nvoid *addr;\r\nunsigned int bit;\r\nint error;\r\nerror = memory_bm_find_bit(bm, pfn, &addr, &bit);\r\nBUG_ON(error);\r\nclear_bit(bit, addr);\r\n}\r\nstatic int memory_bm_test_bit(struct memory_bitmap *bm, unsigned long pfn)\r\n{\r\nvoid *addr;\r\nunsigned int bit;\r\nint error;\r\nerror = memory_bm_find_bit(bm, pfn, &addr, &bit);\r\nBUG_ON(error);\r\nreturn test_bit(bit, addr);\r\n}\r\nstatic bool memory_bm_pfn_present(struct memory_bitmap *bm, unsigned long pfn)\r\n{\r\nvoid *addr;\r\nunsigned int bit;\r\nreturn !memory_bm_find_bit(bm, pfn, &addr, &bit);\r\n}\r\nstatic unsigned long memory_bm_next_pfn(struct memory_bitmap *bm)\r\n{\r\nstruct bm_block *bb;\r\nint bit;\r\nbb = bm->cur.block;\r\ndo {\r\nbit = bm->cur.bit;\r\nbit = find_next_bit(bb->data, bm_block_bits(bb), bit);\r\nif (bit < bm_block_bits(bb))\r\ngoto Return_pfn;\r\nbb = list_entry(bb->hook.next, struct bm_block, hook);\r\nbm->cur.block = bb;\r\nbm->cur.bit = 0;\r\n} while (&bb->hook != &bm->blocks);\r\nmemory_bm_position_reset(bm);\r\nreturn BM_END_OF_MAP;\r\nReturn_pfn:\r\nbm->cur.bit = bit + 1;\r\nreturn bb->start_pfn + bit;\r\n}\r\nvoid __init\r\n__register_nosave_region(unsigned long start_pfn, unsigned long end_pfn,\r\nint use_kmalloc)\r\n{\r\nstruct nosave_region *region;\r\nif (start_pfn >= end_pfn)\r\nreturn;\r\nif (!list_empty(&nosave_regions)) {\r\nregion = list_entry(nosave_regions.prev,\r\nstruct nosave_region, list);\r\nif (region->end_pfn == start_pfn) {\r\nregion->end_pfn = end_pfn;\r\ngoto Report;\r\n}\r\n}\r\nif (use_kmalloc) {\r\nregion = kmalloc(sizeof(struct nosave_region), GFP_KERNEL);\r\nBUG_ON(!region);\r\n} else\r\nregion = alloc_bootmem(sizeof(struct nosave_region));\r\nregion->start_pfn = start_pfn;\r\nregion->end_pfn = end_pfn;\r\nlist_add_tail(&region->list, &nosave_regions);\r\nReport:\r\nprintk(KERN_INFO "PM: Registered nosave memory: %016lx - %016lx\n",\r\nstart_pfn << PAGE_SHIFT, end_pfn << PAGE_SHIFT);\r\n}\r\nvoid swsusp_set_page_free(struct page *page)\r\n{\r\nif (free_pages_map)\r\nmemory_bm_set_bit(free_pages_map, page_to_pfn(page));\r\n}\r\nstatic int swsusp_page_is_free(struct page *page)\r\n{\r\nreturn free_pages_map ?\r\nmemory_bm_test_bit(free_pages_map, page_to_pfn(page)) : 0;\r\n}\r\nvoid swsusp_unset_page_free(struct page *page)\r\n{\r\nif (free_pages_map)\r\nmemory_bm_clear_bit(free_pages_map, page_to_pfn(page));\r\n}\r\nstatic void swsusp_set_page_forbidden(struct page *page)\r\n{\r\nif (forbidden_pages_map)\r\nmemory_bm_set_bit(forbidden_pages_map, page_to_pfn(page));\r\n}\r\nint swsusp_page_is_forbidden(struct page *page)\r\n{\r\nreturn forbidden_pages_map ?\r\nmemory_bm_test_bit(forbidden_pages_map, page_to_pfn(page)) : 0;\r\n}\r\nstatic void swsusp_unset_page_forbidden(struct page *page)\r\n{\r\nif (forbidden_pages_map)\r\nmemory_bm_clear_bit(forbidden_pages_map, page_to_pfn(page));\r\n}\r\nstatic void mark_nosave_pages(struct memory_bitmap *bm)\r\n{\r\nstruct nosave_region *region;\r\nif (list_empty(&nosave_regions))\r\nreturn;\r\nlist_for_each_entry(region, &nosave_regions, list) {\r\nunsigned long pfn;\r\npr_debug("PM: Marking nosave pages: [mem %#010llx-%#010llx]\n",\r\n(unsigned long long) region->start_pfn << PAGE_SHIFT,\r\n((unsigned long long) region->end_pfn << PAGE_SHIFT)\r\n- 1);\r\nfor (pfn = region->start_pfn; pfn < region->end_pfn; pfn++)\r\nif (pfn_valid(pfn)) {\r\nmem_bm_set_bit_check(bm, pfn);\r\n}\r\n}\r\n}\r\nint create_basic_memory_bitmaps(void)\r\n{\r\nstruct memory_bitmap *bm1, *bm2;\r\nint error = 0;\r\nBUG_ON(forbidden_pages_map || free_pages_map);\r\nbm1 = kzalloc(sizeof(struct memory_bitmap), GFP_KERNEL);\r\nif (!bm1)\r\nreturn -ENOMEM;\r\nerror = memory_bm_create(bm1, GFP_KERNEL, PG_ANY);\r\nif (error)\r\ngoto Free_first_object;\r\nbm2 = kzalloc(sizeof(struct memory_bitmap), GFP_KERNEL);\r\nif (!bm2)\r\ngoto Free_first_bitmap;\r\nerror = memory_bm_create(bm2, GFP_KERNEL, PG_ANY);\r\nif (error)\r\ngoto Free_second_object;\r\nforbidden_pages_map = bm1;\r\nfree_pages_map = bm2;\r\nmark_nosave_pages(forbidden_pages_map);\r\npr_debug("PM: Basic memory bitmaps created\n");\r\nreturn 0;\r\nFree_second_object:\r\nkfree(bm2);\r\nFree_first_bitmap:\r\nmemory_bm_free(bm1, PG_UNSAFE_CLEAR);\r\nFree_first_object:\r\nkfree(bm1);\r\nreturn -ENOMEM;\r\n}\r\nvoid free_basic_memory_bitmaps(void)\r\n{\r\nstruct memory_bitmap *bm1, *bm2;\r\nBUG_ON(!(forbidden_pages_map && free_pages_map));\r\nbm1 = forbidden_pages_map;\r\nbm2 = free_pages_map;\r\nforbidden_pages_map = NULL;\r\nfree_pages_map = NULL;\r\nmemory_bm_free(bm1, PG_UNSAFE_CLEAR);\r\nkfree(bm1);\r\nmemory_bm_free(bm2, PG_UNSAFE_CLEAR);\r\nkfree(bm2);\r\npr_debug("PM: Basic memory bitmaps freed\n");\r\n}\r\nunsigned int snapshot_additional_pages(struct zone *zone)\r\n{\r\nunsigned int res;\r\nres = DIV_ROUND_UP(zone->spanned_pages, BM_BITS_PER_BLOCK);\r\nres += DIV_ROUND_UP(res * sizeof(struct bm_block),\r\nLINKED_PAGE_DATA_SIZE);\r\nreturn 2 * res;\r\n}\r\nstatic unsigned int count_free_highmem_pages(void)\r\n{\r\nstruct zone *zone;\r\nunsigned int cnt = 0;\r\nfor_each_populated_zone(zone)\r\nif (is_highmem(zone))\r\ncnt += zone_page_state(zone, NR_FREE_PAGES);\r\nreturn cnt;\r\n}\r\nstatic struct page *saveable_highmem_page(struct zone *zone, unsigned long pfn)\r\n{\r\nstruct page *page;\r\nif (!pfn_valid(pfn))\r\nreturn NULL;\r\npage = pfn_to_page(pfn);\r\nif (page_zone(page) != zone)\r\nreturn NULL;\r\nBUG_ON(!PageHighMem(page));\r\nif (swsusp_page_is_forbidden(page) || swsusp_page_is_free(page) ||\r\nPageReserved(page))\r\nreturn NULL;\r\nif (page_is_guard(page))\r\nreturn NULL;\r\nreturn page;\r\n}\r\nstatic unsigned int count_highmem_pages(void)\r\n{\r\nstruct zone *zone;\r\nunsigned int n = 0;\r\nfor_each_populated_zone(zone) {\r\nunsigned long pfn, max_zone_pfn;\r\nif (!is_highmem(zone))\r\ncontinue;\r\nmark_free_pages(zone);\r\nmax_zone_pfn = zone->zone_start_pfn + zone->spanned_pages;\r\nfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\r\nif (saveable_highmem_page(zone, pfn))\r\nn++;\r\n}\r\nreturn n;\r\n}\r\nstatic inline void *saveable_highmem_page(struct zone *z, unsigned long p)\r\n{\r\nreturn NULL;\r\n}\r\nstatic struct page *saveable_page(struct zone *zone, unsigned long pfn)\r\n{\r\nstruct page *page;\r\nif (!pfn_valid(pfn))\r\nreturn NULL;\r\npage = pfn_to_page(pfn);\r\nif (page_zone(page) != zone)\r\nreturn NULL;\r\nBUG_ON(PageHighMem(page));\r\nif (swsusp_page_is_forbidden(page) || swsusp_page_is_free(page))\r\nreturn NULL;\r\nif (PageReserved(page)\r\n&& (!kernel_page_present(page) || pfn_is_nosave(pfn)))\r\nreturn NULL;\r\nif (page_is_guard(page))\r\nreturn NULL;\r\nreturn page;\r\n}\r\nstatic unsigned int count_data_pages(void)\r\n{\r\nstruct zone *zone;\r\nunsigned long pfn, max_zone_pfn;\r\nunsigned int n = 0;\r\nfor_each_populated_zone(zone) {\r\nif (is_highmem(zone))\r\ncontinue;\r\nmark_free_pages(zone);\r\nmax_zone_pfn = zone->zone_start_pfn + zone->spanned_pages;\r\nfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\r\nif (saveable_page(zone, pfn))\r\nn++;\r\n}\r\nreturn n;\r\n}\r\nstatic inline void do_copy_page(long *dst, long *src)\r\n{\r\nint n;\r\nfor (n = PAGE_SIZE / sizeof(long); n; n--)\r\n*dst++ = *src++;\r\n}\r\nstatic void safe_copy_page(void *dst, struct page *s_page)\r\n{\r\nif (kernel_page_present(s_page)) {\r\ndo_copy_page(dst, page_address(s_page));\r\n} else {\r\nkernel_map_pages(s_page, 1, 1);\r\ndo_copy_page(dst, page_address(s_page));\r\nkernel_map_pages(s_page, 1, 0);\r\n}\r\n}\r\nstatic inline struct page *\r\npage_is_saveable(struct zone *zone, unsigned long pfn)\r\n{\r\nreturn is_highmem(zone) ?\r\nsaveable_highmem_page(zone, pfn) : saveable_page(zone, pfn);\r\n}\r\nstatic void copy_data_page(unsigned long dst_pfn, unsigned long src_pfn)\r\n{\r\nstruct page *s_page, *d_page;\r\nvoid *src, *dst;\r\ns_page = pfn_to_page(src_pfn);\r\nd_page = pfn_to_page(dst_pfn);\r\nif (PageHighMem(s_page)) {\r\nsrc = kmap_atomic(s_page);\r\ndst = kmap_atomic(d_page);\r\ndo_copy_page(dst, src);\r\nkunmap_atomic(dst);\r\nkunmap_atomic(src);\r\n} else {\r\nif (PageHighMem(d_page)) {\r\nsafe_copy_page(buffer, s_page);\r\ndst = kmap_atomic(d_page);\r\ncopy_page(dst, buffer);\r\nkunmap_atomic(dst);\r\n} else {\r\nsafe_copy_page(page_address(d_page), s_page);\r\n}\r\n}\r\n}\r\nstatic inline void copy_data_page(unsigned long dst_pfn, unsigned long src_pfn)\r\n{\r\nsafe_copy_page(page_address(pfn_to_page(dst_pfn)),\r\npfn_to_page(src_pfn));\r\n}\r\nstatic void\r\ncopy_data_pages(struct memory_bitmap *copy_bm, struct memory_bitmap *orig_bm)\r\n{\r\nstruct zone *zone;\r\nunsigned long pfn;\r\nfor_each_populated_zone(zone) {\r\nunsigned long max_zone_pfn;\r\nmark_free_pages(zone);\r\nmax_zone_pfn = zone->zone_start_pfn + zone->spanned_pages;\r\nfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\r\nif (page_is_saveable(zone, pfn))\r\nmemory_bm_set_bit(orig_bm, pfn);\r\n}\r\nmemory_bm_position_reset(orig_bm);\r\nmemory_bm_position_reset(copy_bm);\r\nfor(;;) {\r\npfn = memory_bm_next_pfn(orig_bm);\r\nif (unlikely(pfn == BM_END_OF_MAP))\r\nbreak;\r\ncopy_data_page(memory_bm_next_pfn(copy_bm), pfn);\r\n}\r\n}\r\nvoid swsusp_free(void)\r\n{\r\nstruct zone *zone;\r\nunsigned long pfn, max_zone_pfn;\r\nfor_each_populated_zone(zone) {\r\nmax_zone_pfn = zone->zone_start_pfn + zone->spanned_pages;\r\nfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\r\nif (pfn_valid(pfn)) {\r\nstruct page *page = pfn_to_page(pfn);\r\nif (swsusp_page_is_forbidden(page) &&\r\nswsusp_page_is_free(page)) {\r\nswsusp_unset_page_forbidden(page);\r\nswsusp_unset_page_free(page);\r\n__free_page(page);\r\n}\r\n}\r\n}\r\nnr_copy_pages = 0;\r\nnr_meta_pages = 0;\r\nrestore_pblist = NULL;\r\nbuffer = NULL;\r\nalloc_normal = 0;\r\nalloc_highmem = 0;\r\n}\r\nstatic unsigned long preallocate_image_pages(unsigned long nr_pages, gfp_t mask)\r\n{\r\nunsigned long nr_alloc = 0;\r\nwhile (nr_pages > 0) {\r\nstruct page *page;\r\npage = alloc_image_page(mask);\r\nif (!page)\r\nbreak;\r\nmemory_bm_set_bit(&copy_bm, page_to_pfn(page));\r\nif (PageHighMem(page))\r\nalloc_highmem++;\r\nelse\r\nalloc_normal++;\r\nnr_pages--;\r\nnr_alloc++;\r\n}\r\nreturn nr_alloc;\r\n}\r\nstatic unsigned long preallocate_image_memory(unsigned long nr_pages,\r\nunsigned long avail_normal)\r\n{\r\nunsigned long alloc;\r\nif (avail_normal <= alloc_normal)\r\nreturn 0;\r\nalloc = avail_normal - alloc_normal;\r\nif (nr_pages < alloc)\r\nalloc = nr_pages;\r\nreturn preallocate_image_pages(alloc, GFP_IMAGE);\r\n}\r\nstatic unsigned long preallocate_image_highmem(unsigned long nr_pages)\r\n{\r\nreturn preallocate_image_pages(nr_pages, GFP_IMAGE | __GFP_HIGHMEM);\r\n}\r\nstatic unsigned long __fraction(u64 x, u64 multiplier, u64 base)\r\n{\r\nx *= multiplier;\r\ndo_div(x, base);\r\nreturn (unsigned long)x;\r\n}\r\nstatic unsigned long preallocate_highmem_fraction(unsigned long nr_pages,\r\nunsigned long highmem,\r\nunsigned long total)\r\n{\r\nunsigned long alloc = __fraction(nr_pages, highmem, total);\r\nreturn preallocate_image_pages(alloc, GFP_IMAGE | __GFP_HIGHMEM);\r\n}\r\nstatic inline unsigned long preallocate_image_highmem(unsigned long nr_pages)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline unsigned long preallocate_highmem_fraction(unsigned long nr_pages,\r\nunsigned long highmem,\r\nunsigned long total)\r\n{\r\nreturn 0;\r\n}\r\nstatic void free_unnecessary_pages(void)\r\n{\r\nunsigned long save, to_free_normal, to_free_highmem;\r\nsave = count_data_pages();\r\nif (alloc_normal >= save) {\r\nto_free_normal = alloc_normal - save;\r\nsave = 0;\r\n} else {\r\nto_free_normal = 0;\r\nsave -= alloc_normal;\r\n}\r\nsave += count_highmem_pages();\r\nif (alloc_highmem >= save) {\r\nto_free_highmem = alloc_highmem - save;\r\n} else {\r\nto_free_highmem = 0;\r\nsave -= alloc_highmem;\r\nif (to_free_normal > save)\r\nto_free_normal -= save;\r\nelse\r\nto_free_normal = 0;\r\n}\r\nmemory_bm_position_reset(&copy_bm);\r\nwhile (to_free_normal > 0 || to_free_highmem > 0) {\r\nunsigned long pfn = memory_bm_next_pfn(&copy_bm);\r\nstruct page *page = pfn_to_page(pfn);\r\nif (PageHighMem(page)) {\r\nif (!to_free_highmem)\r\ncontinue;\r\nto_free_highmem--;\r\nalloc_highmem--;\r\n} else {\r\nif (!to_free_normal)\r\ncontinue;\r\nto_free_normal--;\r\nalloc_normal--;\r\n}\r\nmemory_bm_clear_bit(&copy_bm, pfn);\r\nswsusp_unset_page_forbidden(page);\r\nswsusp_unset_page_free(page);\r\n__free_page(page);\r\n}\r\n}\r\nstatic unsigned long minimum_image_size(unsigned long saveable)\r\n{\r\nunsigned long size;\r\nsize = global_page_state(NR_SLAB_RECLAIMABLE)\r\n+ global_page_state(NR_ACTIVE_ANON)\r\n+ global_page_state(NR_INACTIVE_ANON)\r\n+ global_page_state(NR_ACTIVE_FILE)\r\n+ global_page_state(NR_INACTIVE_FILE)\r\n- global_page_state(NR_FILE_MAPPED);\r\nreturn saveable <= size ? 0 : saveable - size;\r\n}\r\nint hibernate_preallocate_memory(void)\r\n{\r\nstruct zone *zone;\r\nunsigned long saveable, size, max_size, count, highmem, pages = 0;\r\nunsigned long alloc, save_highmem, pages_highmem, avail_normal;\r\nstruct timeval start, stop;\r\nint error;\r\nprintk(KERN_INFO "PM: Preallocating image memory... ");\r\ndo_gettimeofday(&start);\r\nerror = memory_bm_create(&orig_bm, GFP_IMAGE, PG_ANY);\r\nif (error)\r\ngoto err_out;\r\nerror = memory_bm_create(&copy_bm, GFP_IMAGE, PG_ANY);\r\nif (error)\r\ngoto err_out;\r\nalloc_normal = 0;\r\nalloc_highmem = 0;\r\nsave_highmem = count_highmem_pages();\r\nsaveable = count_data_pages();\r\ncount = saveable;\r\nsaveable += save_highmem;\r\nhighmem = save_highmem;\r\nsize = 0;\r\nfor_each_populated_zone(zone) {\r\nsize += snapshot_additional_pages(zone);\r\nif (is_highmem(zone))\r\nhighmem += zone_page_state(zone, NR_FREE_PAGES);\r\nelse\r\ncount += zone_page_state(zone, NR_FREE_PAGES);\r\n}\r\navail_normal = count;\r\ncount += highmem;\r\ncount -= totalreserve_pages;\r\nsize += page_key_additional_pages(saveable);\r\nmax_size = (count - (size + PAGES_FOR_IO)) / 2\r\n- 2 * DIV_ROUND_UP(reserved_size, PAGE_SIZE);\r\nsize = DIV_ROUND_UP(image_size, PAGE_SIZE);\r\nif (size > max_size)\r\nsize = max_size;\r\nif (size >= saveable) {\r\npages = preallocate_image_highmem(save_highmem);\r\npages += preallocate_image_memory(saveable - pages, avail_normal);\r\ngoto out;\r\n}\r\npages = minimum_image_size(saveable);\r\nif (avail_normal > pages)\r\navail_normal -= pages;\r\nelse\r\navail_normal = 0;\r\nif (size < pages)\r\nsize = min_t(unsigned long, pages, max_size);\r\nshrink_all_memory(saveable - size);\r\npages_highmem = preallocate_image_highmem(highmem / 2);\r\nalloc = (count - max_size) - pages_highmem;\r\npages = preallocate_image_memory(alloc, avail_normal);\r\nif (pages < alloc) {\r\nalloc -= pages;\r\npages += pages_highmem;\r\npages_highmem = preallocate_image_highmem(alloc);\r\nif (pages_highmem < alloc)\r\ngoto err_out;\r\npages += pages_highmem;\r\nalloc = (count - pages) - size;\r\npages += preallocate_image_highmem(alloc);\r\n} else {\r\nalloc = max_size - size;\r\nsize = preallocate_highmem_fraction(alloc, highmem, count);\r\npages_highmem += size;\r\nalloc -= size;\r\nsize = preallocate_image_memory(alloc, avail_normal);\r\npages_highmem += preallocate_image_highmem(alloc - size);\r\npages += pages_highmem + size;\r\n}\r\nfree_unnecessary_pages();\r\nout:\r\ndo_gettimeofday(&stop);\r\nprintk(KERN_CONT "done (allocated %lu pages)\n", pages);\r\nswsusp_show_speed(&start, &stop, pages, "Allocated");\r\nreturn 0;\r\nerr_out:\r\nprintk(KERN_CONT "\n");\r\nswsusp_free();\r\nreturn -ENOMEM;\r\n}\r\nstatic unsigned int count_pages_for_highmem(unsigned int nr_highmem)\r\n{\r\nunsigned int free_highmem = count_free_highmem_pages() + alloc_highmem;\r\nif (free_highmem >= nr_highmem)\r\nnr_highmem = 0;\r\nelse\r\nnr_highmem -= free_highmem;\r\nreturn nr_highmem;\r\n}\r\nstatic unsigned int\r\ncount_pages_for_highmem(unsigned int nr_highmem) { return 0; }\r\nstatic int enough_free_mem(unsigned int nr_pages, unsigned int nr_highmem)\r\n{\r\nstruct zone *zone;\r\nunsigned int free = alloc_normal;\r\nfor_each_populated_zone(zone)\r\nif (!is_highmem(zone))\r\nfree += zone_page_state(zone, NR_FREE_PAGES);\r\nnr_pages += count_pages_for_highmem(nr_highmem);\r\npr_debug("PM: Normal pages needed: %u + %u, available pages: %u\n",\r\nnr_pages, PAGES_FOR_IO, free);\r\nreturn free > nr_pages + PAGES_FOR_IO;\r\n}\r\nstatic inline int get_highmem_buffer(int safe_needed)\r\n{\r\nbuffer = get_image_page(GFP_ATOMIC | __GFP_COLD, safe_needed);\r\nreturn buffer ? 0 : -ENOMEM;\r\n}\r\nstatic inline unsigned int\r\nalloc_highmem_pages(struct memory_bitmap *bm, unsigned int nr_highmem)\r\n{\r\nunsigned int to_alloc = count_free_highmem_pages();\r\nif (to_alloc > nr_highmem)\r\nto_alloc = nr_highmem;\r\nnr_highmem -= to_alloc;\r\nwhile (to_alloc-- > 0) {\r\nstruct page *page;\r\npage = alloc_image_page(__GFP_HIGHMEM);\r\nmemory_bm_set_bit(bm, page_to_pfn(page));\r\n}\r\nreturn nr_highmem;\r\n}\r\nstatic inline int get_highmem_buffer(int safe_needed) { return 0; }\r\nstatic inline unsigned int\r\nalloc_highmem_pages(struct memory_bitmap *bm, unsigned int n) { return 0; }\r\nstatic int\r\nswsusp_alloc(struct memory_bitmap *orig_bm, struct memory_bitmap *copy_bm,\r\nunsigned int nr_pages, unsigned int nr_highmem)\r\n{\r\nif (nr_highmem > 0) {\r\nif (get_highmem_buffer(PG_ANY))\r\ngoto err_out;\r\nif (nr_highmem > alloc_highmem) {\r\nnr_highmem -= alloc_highmem;\r\nnr_pages += alloc_highmem_pages(copy_bm, nr_highmem);\r\n}\r\n}\r\nif (nr_pages > alloc_normal) {\r\nnr_pages -= alloc_normal;\r\nwhile (nr_pages-- > 0) {\r\nstruct page *page;\r\npage = alloc_image_page(GFP_ATOMIC | __GFP_COLD);\r\nif (!page)\r\ngoto err_out;\r\nmemory_bm_set_bit(copy_bm, page_to_pfn(page));\r\n}\r\n}\r\nreturn 0;\r\nerr_out:\r\nswsusp_free();\r\nreturn -ENOMEM;\r\n}\r\nasmlinkage int swsusp_save(void)\r\n{\r\nunsigned int nr_pages, nr_highmem;\r\nprintk(KERN_INFO "PM: Creating hibernation image:\n");\r\ndrain_local_pages(NULL);\r\nnr_pages = count_data_pages();\r\nnr_highmem = count_highmem_pages();\r\nprintk(KERN_INFO "PM: Need to copy %u pages\n", nr_pages + nr_highmem);\r\nif (!enough_free_mem(nr_pages, nr_highmem)) {\r\nprintk(KERN_ERR "PM: Not enough free memory\n");\r\nreturn -ENOMEM;\r\n}\r\nif (swsusp_alloc(&orig_bm, &copy_bm, nr_pages, nr_highmem)) {\r\nprintk(KERN_ERR "PM: Memory allocation failed\n");\r\nreturn -ENOMEM;\r\n}\r\ndrain_local_pages(NULL);\r\ncopy_data_pages(&copy_bm, &orig_bm);\r\nnr_pages += nr_highmem;\r\nnr_copy_pages = nr_pages;\r\nnr_meta_pages = DIV_ROUND_UP(nr_pages * sizeof(long), PAGE_SIZE);\r\nprintk(KERN_INFO "PM: Hibernation image created (%d pages copied)\n",\r\nnr_pages);\r\nreturn 0;\r\n}\r\nstatic int init_header_complete(struct swsusp_info *info)\r\n{\r\nmemcpy(&info->uts, init_utsname(), sizeof(struct new_utsname));\r\ninfo->version_code = LINUX_VERSION_CODE;\r\nreturn 0;\r\n}\r\nstatic char *check_image_kernel(struct swsusp_info *info)\r\n{\r\nif (info->version_code != LINUX_VERSION_CODE)\r\nreturn "kernel version";\r\nif (strcmp(info->uts.sysname,init_utsname()->sysname))\r\nreturn "system type";\r\nif (strcmp(info->uts.release,init_utsname()->release))\r\nreturn "kernel release";\r\nif (strcmp(info->uts.version,init_utsname()->version))\r\nreturn "version";\r\nif (strcmp(info->uts.machine,init_utsname()->machine))\r\nreturn "machine";\r\nreturn NULL;\r\n}\r\nunsigned long snapshot_get_image_size(void)\r\n{\r\nreturn nr_copy_pages + nr_meta_pages + 1;\r\n}\r\nstatic int init_header(struct swsusp_info *info)\r\n{\r\nmemset(info, 0, sizeof(struct swsusp_info));\r\ninfo->num_physpages = num_physpages;\r\ninfo->image_pages = nr_copy_pages;\r\ninfo->pages = snapshot_get_image_size();\r\ninfo->size = info->pages;\r\ninfo->size <<= PAGE_SHIFT;\r\nreturn init_header_complete(info);\r\n}\r\nstatic inline void\r\npack_pfns(unsigned long *buf, struct memory_bitmap *bm)\r\n{\r\nint j;\r\nfor (j = 0; j < PAGE_SIZE / sizeof(long); j++) {\r\nbuf[j] = memory_bm_next_pfn(bm);\r\nif (unlikely(buf[j] == BM_END_OF_MAP))\r\nbreak;\r\npage_key_read(buf + j);\r\n}\r\n}\r\nint snapshot_read_next(struct snapshot_handle *handle)\r\n{\r\nif (handle->cur > nr_meta_pages + nr_copy_pages)\r\nreturn 0;\r\nif (!buffer) {\r\nbuffer = get_image_page(GFP_ATOMIC, PG_ANY);\r\nif (!buffer)\r\nreturn -ENOMEM;\r\n}\r\nif (!handle->cur) {\r\nint error;\r\nerror = init_header((struct swsusp_info *)buffer);\r\nif (error)\r\nreturn error;\r\nhandle->buffer = buffer;\r\nmemory_bm_position_reset(&orig_bm);\r\nmemory_bm_position_reset(&copy_bm);\r\n} else if (handle->cur <= nr_meta_pages) {\r\nclear_page(buffer);\r\npack_pfns(buffer, &orig_bm);\r\n} else {\r\nstruct page *page;\r\npage = pfn_to_page(memory_bm_next_pfn(&copy_bm));\r\nif (PageHighMem(page)) {\r\nvoid *kaddr;\r\nkaddr = kmap_atomic(page);\r\ncopy_page(buffer, kaddr);\r\nkunmap_atomic(kaddr);\r\nhandle->buffer = buffer;\r\n} else {\r\nhandle->buffer = page_address(page);\r\n}\r\n}\r\nhandle->cur++;\r\nreturn PAGE_SIZE;\r\n}\r\nstatic int mark_unsafe_pages(struct memory_bitmap *bm)\r\n{\r\nstruct zone *zone;\r\nunsigned long pfn, max_zone_pfn;\r\nfor_each_populated_zone(zone) {\r\nmax_zone_pfn = zone->zone_start_pfn + zone->spanned_pages;\r\nfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\r\nif (pfn_valid(pfn))\r\nswsusp_unset_page_free(pfn_to_page(pfn));\r\n}\r\nmemory_bm_position_reset(bm);\r\ndo {\r\npfn = memory_bm_next_pfn(bm);\r\nif (likely(pfn != BM_END_OF_MAP)) {\r\nif (likely(pfn_valid(pfn)))\r\nswsusp_set_page_free(pfn_to_page(pfn));\r\nelse\r\nreturn -EFAULT;\r\n}\r\n} while (pfn != BM_END_OF_MAP);\r\nallocated_unsafe_pages = 0;\r\nreturn 0;\r\n}\r\nstatic void\r\nduplicate_memory_bitmap(struct memory_bitmap *dst, struct memory_bitmap *src)\r\n{\r\nunsigned long pfn;\r\nmemory_bm_position_reset(src);\r\npfn = memory_bm_next_pfn(src);\r\nwhile (pfn != BM_END_OF_MAP) {\r\nmemory_bm_set_bit(dst, pfn);\r\npfn = memory_bm_next_pfn(src);\r\n}\r\n}\r\nstatic int check_header(struct swsusp_info *info)\r\n{\r\nchar *reason;\r\nreason = check_image_kernel(info);\r\nif (!reason && info->num_physpages != num_physpages)\r\nreason = "memory size";\r\nif (reason) {\r\nprintk(KERN_ERR "PM: Image mismatch: %s\n", reason);\r\nreturn -EPERM;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nload_header(struct swsusp_info *info)\r\n{\r\nint error;\r\nrestore_pblist = NULL;\r\nerror = check_header(info);\r\nif (!error) {\r\nnr_copy_pages = info->image_pages;\r\nnr_meta_pages = info->pages - info->image_pages - 1;\r\n}\r\nreturn error;\r\n}\r\nstatic int unpack_orig_pfns(unsigned long *buf, struct memory_bitmap *bm)\r\n{\r\nint j;\r\nfor (j = 0; j < PAGE_SIZE / sizeof(long); j++) {\r\nif (unlikely(buf[j] == BM_END_OF_MAP))\r\nbreak;\r\npage_key_memorize(buf + j);\r\nif (memory_bm_pfn_present(bm, buf[j]))\r\nmemory_bm_set_bit(bm, buf[j]);\r\nelse\r\nreturn -EFAULT;\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned int count_highmem_image_pages(struct memory_bitmap *bm)\r\n{\r\nunsigned long pfn;\r\nunsigned int cnt = 0;\r\nmemory_bm_position_reset(bm);\r\npfn = memory_bm_next_pfn(bm);\r\nwhile (pfn != BM_END_OF_MAP) {\r\nif (PageHighMem(pfn_to_page(pfn)))\r\ncnt++;\r\npfn = memory_bm_next_pfn(bm);\r\n}\r\nreturn cnt;\r\n}\r\nstatic int\r\nprepare_highmem_image(struct memory_bitmap *bm, unsigned int *nr_highmem_p)\r\n{\r\nunsigned int to_alloc;\r\nif (memory_bm_create(bm, GFP_ATOMIC, PG_SAFE))\r\nreturn -ENOMEM;\r\nif (get_highmem_buffer(PG_SAFE))\r\nreturn -ENOMEM;\r\nto_alloc = count_free_highmem_pages();\r\nif (to_alloc > *nr_highmem_p)\r\nto_alloc = *nr_highmem_p;\r\nelse\r\n*nr_highmem_p = to_alloc;\r\nsafe_highmem_pages = 0;\r\nwhile (to_alloc-- > 0) {\r\nstruct page *page;\r\npage = alloc_page(__GFP_HIGHMEM);\r\nif (!swsusp_page_is_free(page)) {\r\nmemory_bm_set_bit(bm, page_to_pfn(page));\r\nsafe_highmem_pages++;\r\n}\r\nswsusp_set_page_forbidden(page);\r\nswsusp_set_page_free(page);\r\n}\r\nmemory_bm_position_reset(bm);\r\nsafe_highmem_bm = bm;\r\nreturn 0;\r\n}\r\nstatic void *\r\nget_highmem_page_buffer(struct page *page, struct chain_allocator *ca)\r\n{\r\nstruct highmem_pbe *pbe;\r\nvoid *kaddr;\r\nif (swsusp_page_is_forbidden(page) && swsusp_page_is_free(page)) {\r\nlast_highmem_page = page;\r\nreturn buffer;\r\n}\r\npbe = chain_alloc(ca, sizeof(struct highmem_pbe));\r\nif (!pbe) {\r\nswsusp_free();\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\npbe->orig_page = page;\r\nif (safe_highmem_pages > 0) {\r\nstruct page *tmp;\r\nkaddr = buffer;\r\ntmp = pfn_to_page(memory_bm_next_pfn(safe_highmem_bm));\r\nsafe_highmem_pages--;\r\nlast_highmem_page = tmp;\r\npbe->copy_page = tmp;\r\n} else {\r\nkaddr = safe_pages_list;\r\nsafe_pages_list = safe_pages_list->next;\r\npbe->copy_page = virt_to_page(kaddr);\r\n}\r\npbe->next = highmem_pblist;\r\nhighmem_pblist = pbe;\r\nreturn kaddr;\r\n}\r\nstatic void copy_last_highmem_page(void)\r\n{\r\nif (last_highmem_page) {\r\nvoid *dst;\r\ndst = kmap_atomic(last_highmem_page);\r\ncopy_page(dst, buffer);\r\nkunmap_atomic(dst);\r\nlast_highmem_page = NULL;\r\n}\r\n}\r\nstatic inline int last_highmem_page_copied(void)\r\n{\r\nreturn !last_highmem_page;\r\n}\r\nstatic inline void free_highmem_data(void)\r\n{\r\nif (safe_highmem_bm)\r\nmemory_bm_free(safe_highmem_bm, PG_UNSAFE_CLEAR);\r\nif (buffer)\r\nfree_image_page(buffer, PG_UNSAFE_CLEAR);\r\n}\r\nstatic inline int get_safe_write_buffer(void) { return 0; }\r\nstatic unsigned int\r\ncount_highmem_image_pages(struct memory_bitmap *bm) { return 0; }\r\nstatic inline int\r\nprepare_highmem_image(struct memory_bitmap *bm, unsigned int *nr_highmem_p)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void *\r\nget_highmem_page_buffer(struct page *page, struct chain_allocator *ca)\r\n{\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nstatic inline void copy_last_highmem_page(void) {}\r\nstatic inline int last_highmem_page_copied(void) { return 1; }\r\nstatic inline void free_highmem_data(void) {}\r\nstatic int\r\nprepare_image(struct memory_bitmap *new_bm, struct memory_bitmap *bm)\r\n{\r\nunsigned int nr_pages, nr_highmem;\r\nstruct linked_page *sp_list, *lp;\r\nint error;\r\nfree_image_page(buffer, PG_UNSAFE_CLEAR);\r\nbuffer = NULL;\r\nnr_highmem = count_highmem_image_pages(bm);\r\nerror = mark_unsafe_pages(bm);\r\nif (error)\r\ngoto Free;\r\nerror = memory_bm_create(new_bm, GFP_ATOMIC, PG_SAFE);\r\nif (error)\r\ngoto Free;\r\nduplicate_memory_bitmap(new_bm, bm);\r\nmemory_bm_free(bm, PG_UNSAFE_KEEP);\r\nif (nr_highmem > 0) {\r\nerror = prepare_highmem_image(bm, &nr_highmem);\r\nif (error)\r\ngoto Free;\r\n}\r\nsp_list = NULL;\r\nnr_pages = nr_copy_pages - nr_highmem - allocated_unsafe_pages;\r\nnr_pages = DIV_ROUND_UP(nr_pages, PBES_PER_LINKED_PAGE);\r\nwhile (nr_pages > 0) {\r\nlp = get_image_page(GFP_ATOMIC, PG_SAFE);\r\nif (!lp) {\r\nerror = -ENOMEM;\r\ngoto Free;\r\n}\r\nlp->next = sp_list;\r\nsp_list = lp;\r\nnr_pages--;\r\n}\r\nsafe_pages_list = NULL;\r\nnr_pages = nr_copy_pages - nr_highmem - allocated_unsafe_pages;\r\nwhile (nr_pages > 0) {\r\nlp = (struct linked_page *)get_zeroed_page(GFP_ATOMIC);\r\nif (!lp) {\r\nerror = -ENOMEM;\r\ngoto Free;\r\n}\r\nif (!swsusp_page_is_free(virt_to_page(lp))) {\r\nlp->next = safe_pages_list;\r\nsafe_pages_list = lp;\r\n}\r\nswsusp_set_page_forbidden(virt_to_page(lp));\r\nswsusp_set_page_free(virt_to_page(lp));\r\nnr_pages--;\r\n}\r\nwhile (sp_list) {\r\nlp = sp_list->next;\r\nfree_image_page(sp_list, PG_UNSAFE_CLEAR);\r\nsp_list = lp;\r\n}\r\nreturn 0;\r\nFree:\r\nswsusp_free();\r\nreturn error;\r\n}\r\nstatic void *get_buffer(struct memory_bitmap *bm, struct chain_allocator *ca)\r\n{\r\nstruct pbe *pbe;\r\nstruct page *page;\r\nunsigned long pfn = memory_bm_next_pfn(bm);\r\nif (pfn == BM_END_OF_MAP)\r\nreturn ERR_PTR(-EFAULT);\r\npage = pfn_to_page(pfn);\r\nif (PageHighMem(page))\r\nreturn get_highmem_page_buffer(page, ca);\r\nif (swsusp_page_is_forbidden(page) && swsusp_page_is_free(page))\r\nreturn page_address(page);\r\npbe = chain_alloc(ca, sizeof(struct pbe));\r\nif (!pbe) {\r\nswsusp_free();\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\npbe->orig_address = page_address(page);\r\npbe->address = safe_pages_list;\r\nsafe_pages_list = safe_pages_list->next;\r\npbe->next = restore_pblist;\r\nrestore_pblist = pbe;\r\nreturn pbe->address;\r\n}\r\nint snapshot_write_next(struct snapshot_handle *handle)\r\n{\r\nstatic struct chain_allocator ca;\r\nint error = 0;\r\nif (handle->cur > 1 && handle->cur > nr_meta_pages + nr_copy_pages)\r\nreturn 0;\r\nhandle->sync_read = 1;\r\nif (!handle->cur) {\r\nif (!buffer)\r\nbuffer = get_image_page(GFP_ATOMIC, PG_ANY);\r\nif (!buffer)\r\nreturn -ENOMEM;\r\nhandle->buffer = buffer;\r\n} else if (handle->cur == 1) {\r\nerror = load_header(buffer);\r\nif (error)\r\nreturn error;\r\nerror = memory_bm_create(&copy_bm, GFP_ATOMIC, PG_ANY);\r\nif (error)\r\nreturn error;\r\nerror = page_key_alloc(nr_copy_pages);\r\nif (error)\r\nreturn error;\r\n} else if (handle->cur <= nr_meta_pages + 1) {\r\nerror = unpack_orig_pfns(buffer, &copy_bm);\r\nif (error)\r\nreturn error;\r\nif (handle->cur == nr_meta_pages + 1) {\r\nerror = prepare_image(&orig_bm, &copy_bm);\r\nif (error)\r\nreturn error;\r\nchain_init(&ca, GFP_ATOMIC, PG_SAFE);\r\nmemory_bm_position_reset(&orig_bm);\r\nrestore_pblist = NULL;\r\nhandle->buffer = get_buffer(&orig_bm, &ca);\r\nhandle->sync_read = 0;\r\nif (IS_ERR(handle->buffer))\r\nreturn PTR_ERR(handle->buffer);\r\n}\r\n} else {\r\ncopy_last_highmem_page();\r\npage_key_write(handle->buffer);\r\nhandle->buffer = get_buffer(&orig_bm, &ca);\r\nif (IS_ERR(handle->buffer))\r\nreturn PTR_ERR(handle->buffer);\r\nif (handle->buffer != buffer)\r\nhandle->sync_read = 0;\r\n}\r\nhandle->cur++;\r\nreturn PAGE_SIZE;\r\n}\r\nvoid snapshot_write_finalize(struct snapshot_handle *handle)\r\n{\r\ncopy_last_highmem_page();\r\npage_key_write(handle->buffer);\r\npage_key_free();\r\nif (handle->cur > 1 && handle->cur > nr_meta_pages + nr_copy_pages) {\r\nmemory_bm_free(&orig_bm, PG_UNSAFE_CLEAR);\r\nfree_highmem_data();\r\n}\r\n}\r\nint snapshot_image_loaded(struct snapshot_handle *handle)\r\n{\r\nreturn !(!nr_copy_pages || !last_highmem_page_copied() ||\r\nhandle->cur <= nr_meta_pages + nr_copy_pages);\r\n}\r\nstatic inline void\r\nswap_two_pages_data(struct page *p1, struct page *p2, void *buf)\r\n{\r\nvoid *kaddr1, *kaddr2;\r\nkaddr1 = kmap_atomic(p1);\r\nkaddr2 = kmap_atomic(p2);\r\ncopy_page(buf, kaddr1);\r\ncopy_page(kaddr1, kaddr2);\r\ncopy_page(kaddr2, buf);\r\nkunmap_atomic(kaddr2);\r\nkunmap_atomic(kaddr1);\r\n}\r\nint restore_highmem(void)\r\n{\r\nstruct highmem_pbe *pbe = highmem_pblist;\r\nvoid *buf;\r\nif (!pbe)\r\nreturn 0;\r\nbuf = get_image_page(GFP_ATOMIC, PG_SAFE);\r\nif (!buf)\r\nreturn -ENOMEM;\r\nwhile (pbe) {\r\nswap_two_pages_data(pbe->copy_page, pbe->orig_page, buf);\r\npbe = pbe->next;\r\n}\r\nfree_image_page(buf, PG_UNSAFE_CLEAR);\r\nreturn 0;\r\n}
