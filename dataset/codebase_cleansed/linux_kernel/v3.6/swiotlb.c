static int __init\r\nsetup_io_tlb_npages(char *str)\r\n{\r\nif (isdigit(*str)) {\r\nio_tlb_nslabs = simple_strtoul(str, &str, 0);\r\nio_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);\r\n}\r\nif (*str == ',')\r\n++str;\r\nif (!strcmp(str, "force"))\r\nswiotlb_force = 1;\r\nreturn 1;\r\n}\r\nunsigned long swiotlb_nr_tbl(void)\r\n{\r\nreturn io_tlb_nslabs;\r\n}\r\nstatic dma_addr_t swiotlb_virt_to_bus(struct device *hwdev,\r\nvolatile void *address)\r\n{\r\nreturn phys_to_dma(hwdev, virt_to_phys(address));\r\n}\r\nvoid swiotlb_print_info(void)\r\n{\r\nunsigned long bytes = io_tlb_nslabs << IO_TLB_SHIFT;\r\nphys_addr_t pstart, pend;\r\npstart = virt_to_phys(io_tlb_start);\r\npend = virt_to_phys(io_tlb_end);\r\nprintk(KERN_INFO "software IO TLB [mem %#010llx-%#010llx] (%luMB) mapped at [%p-%p]\n",\r\n(unsigned long long)pstart, (unsigned long long)pend - 1,\r\nbytes >> 20, io_tlb_start, io_tlb_end - 1);\r\n}\r\nvoid __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)\r\n{\r\nunsigned long i, bytes;\r\nbytes = nslabs << IO_TLB_SHIFT;\r\nio_tlb_nslabs = nslabs;\r\nio_tlb_start = tlb;\r\nio_tlb_end = io_tlb_start + bytes;\r\nio_tlb_list = alloc_bootmem_pages(PAGE_ALIGN(io_tlb_nslabs * sizeof(int)));\r\nfor (i = 0; i < io_tlb_nslabs; i++)\r\nio_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);\r\nio_tlb_index = 0;\r\nio_tlb_orig_addr = alloc_bootmem_pages(PAGE_ALIGN(io_tlb_nslabs * sizeof(phys_addr_t)));\r\nio_tlb_overflow_buffer = alloc_bootmem_low_pages(PAGE_ALIGN(io_tlb_overflow));\r\nif (!io_tlb_overflow_buffer)\r\npanic("Cannot allocate SWIOTLB overflow buffer!\n");\r\nif (verbose)\r\nswiotlb_print_info();\r\n}\r\nvoid __init\r\nswiotlb_init_with_default_size(size_t default_size, int verbose)\r\n{\r\nunsigned long bytes;\r\nif (!io_tlb_nslabs) {\r\nio_tlb_nslabs = (default_size >> IO_TLB_SHIFT);\r\nio_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);\r\n}\r\nbytes = io_tlb_nslabs << IO_TLB_SHIFT;\r\nio_tlb_start = alloc_bootmem_low_pages(PAGE_ALIGN(bytes));\r\nif (!io_tlb_start)\r\npanic("Cannot allocate SWIOTLB buffer");\r\nswiotlb_init_with_tbl(io_tlb_start, io_tlb_nslabs, verbose);\r\n}\r\nvoid __init\r\nswiotlb_init(int verbose)\r\n{\r\nswiotlb_init_with_default_size(64 * (1<<20), verbose);\r\n}\r\nint\r\nswiotlb_late_init_with_default_size(size_t default_size)\r\n{\r\nunsigned long i, bytes, req_nslabs = io_tlb_nslabs;\r\nunsigned int order;\r\nif (!io_tlb_nslabs) {\r\nio_tlb_nslabs = (default_size >> IO_TLB_SHIFT);\r\nio_tlb_nslabs = ALIGN(io_tlb_nslabs, IO_TLB_SEGSIZE);\r\n}\r\norder = get_order(io_tlb_nslabs << IO_TLB_SHIFT);\r\nio_tlb_nslabs = SLABS_PER_PAGE << order;\r\nbytes = io_tlb_nslabs << IO_TLB_SHIFT;\r\nwhile ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {\r\nio_tlb_start = (void *)__get_free_pages(GFP_DMA | __GFP_NOWARN,\r\norder);\r\nif (io_tlb_start)\r\nbreak;\r\norder--;\r\n}\r\nif (!io_tlb_start)\r\ngoto cleanup1;\r\nif (order != get_order(bytes)) {\r\nprintk(KERN_WARNING "Warning: only able to allocate %ld MB "\r\n"for software IO TLB\n", (PAGE_SIZE << order) >> 20);\r\nio_tlb_nslabs = SLABS_PER_PAGE << order;\r\nbytes = io_tlb_nslabs << IO_TLB_SHIFT;\r\n}\r\nio_tlb_end = io_tlb_start + bytes;\r\nmemset(io_tlb_start, 0, bytes);\r\nio_tlb_list = (unsigned int *)__get_free_pages(GFP_KERNEL,\r\nget_order(io_tlb_nslabs * sizeof(int)));\r\nif (!io_tlb_list)\r\ngoto cleanup2;\r\nfor (i = 0; i < io_tlb_nslabs; i++)\r\nio_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);\r\nio_tlb_index = 0;\r\nio_tlb_orig_addr = (phys_addr_t *)\r\n__get_free_pages(GFP_KERNEL,\r\nget_order(io_tlb_nslabs *\r\nsizeof(phys_addr_t)));\r\nif (!io_tlb_orig_addr)\r\ngoto cleanup3;\r\nmemset(io_tlb_orig_addr, 0, io_tlb_nslabs * sizeof(phys_addr_t));\r\nio_tlb_overflow_buffer = (void *)__get_free_pages(GFP_DMA,\r\nget_order(io_tlb_overflow));\r\nif (!io_tlb_overflow_buffer)\r\ngoto cleanup4;\r\nswiotlb_print_info();\r\nlate_alloc = 1;\r\nreturn 0;\r\ncleanup4:\r\nfree_pages((unsigned long)io_tlb_orig_addr,\r\nget_order(io_tlb_nslabs * sizeof(phys_addr_t)));\r\nio_tlb_orig_addr = NULL;\r\ncleanup3:\r\nfree_pages((unsigned long)io_tlb_list, get_order(io_tlb_nslabs *\r\nsizeof(int)));\r\nio_tlb_list = NULL;\r\ncleanup2:\r\nio_tlb_end = NULL;\r\nfree_pages((unsigned long)io_tlb_start, order);\r\nio_tlb_start = NULL;\r\ncleanup1:\r\nio_tlb_nslabs = req_nslabs;\r\nreturn -ENOMEM;\r\n}\r\nvoid __init swiotlb_free(void)\r\n{\r\nif (!io_tlb_overflow_buffer)\r\nreturn;\r\nif (late_alloc) {\r\nfree_pages((unsigned long)io_tlb_overflow_buffer,\r\nget_order(io_tlb_overflow));\r\nfree_pages((unsigned long)io_tlb_orig_addr,\r\nget_order(io_tlb_nslabs * sizeof(phys_addr_t)));\r\nfree_pages((unsigned long)io_tlb_list, get_order(io_tlb_nslabs *\r\nsizeof(int)));\r\nfree_pages((unsigned long)io_tlb_start,\r\nget_order(io_tlb_nslabs << IO_TLB_SHIFT));\r\n} else {\r\nfree_bootmem_late(__pa(io_tlb_overflow_buffer),\r\nPAGE_ALIGN(io_tlb_overflow));\r\nfree_bootmem_late(__pa(io_tlb_orig_addr),\r\nPAGE_ALIGN(io_tlb_nslabs * sizeof(phys_addr_t)));\r\nfree_bootmem_late(__pa(io_tlb_list),\r\nPAGE_ALIGN(io_tlb_nslabs * sizeof(int)));\r\nfree_bootmem_late(__pa(io_tlb_start),\r\nPAGE_ALIGN(io_tlb_nslabs << IO_TLB_SHIFT));\r\n}\r\nio_tlb_nslabs = 0;\r\n}\r\nstatic int is_swiotlb_buffer(phys_addr_t paddr)\r\n{\r\nreturn paddr >= virt_to_phys(io_tlb_start) &&\r\npaddr < virt_to_phys(io_tlb_end);\r\n}\r\nvoid swiotlb_bounce(phys_addr_t phys, char *dma_addr, size_t size,\r\nenum dma_data_direction dir)\r\n{\r\nunsigned long pfn = PFN_DOWN(phys);\r\nif (PageHighMem(pfn_to_page(pfn))) {\r\nunsigned int offset = phys & ~PAGE_MASK;\r\nchar *buffer;\r\nunsigned int sz = 0;\r\nunsigned long flags;\r\nwhile (size) {\r\nsz = min_t(size_t, PAGE_SIZE - offset, size);\r\nlocal_irq_save(flags);\r\nbuffer = kmap_atomic(pfn_to_page(pfn));\r\nif (dir == DMA_TO_DEVICE)\r\nmemcpy(dma_addr, buffer + offset, sz);\r\nelse\r\nmemcpy(buffer + offset, dma_addr, sz);\r\nkunmap_atomic(buffer);\r\nlocal_irq_restore(flags);\r\nsize -= sz;\r\npfn++;\r\ndma_addr += sz;\r\noffset = 0;\r\n}\r\n} else {\r\nif (dir == DMA_TO_DEVICE)\r\nmemcpy(dma_addr, phys_to_virt(phys), size);\r\nelse\r\nmemcpy(phys_to_virt(phys), dma_addr, size);\r\n}\r\n}\r\nvoid *swiotlb_tbl_map_single(struct device *hwdev, dma_addr_t tbl_dma_addr,\r\nphys_addr_t phys, size_t size,\r\nenum dma_data_direction dir)\r\n{\r\nunsigned long flags;\r\nchar *dma_addr;\r\nunsigned int nslots, stride, index, wrap;\r\nint i;\r\nunsigned long mask;\r\nunsigned long offset_slots;\r\nunsigned long max_slots;\r\nmask = dma_get_seg_boundary(hwdev);\r\ntbl_dma_addr &= mask;\r\noffset_slots = ALIGN(tbl_dma_addr, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;\r\nmax_slots = mask + 1\r\n? ALIGN(mask + 1, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT\r\n: 1UL << (BITS_PER_LONG - IO_TLB_SHIFT);\r\nnslots = ALIGN(size, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;\r\nif (size > PAGE_SIZE)\r\nstride = (1 << (PAGE_SHIFT - IO_TLB_SHIFT));\r\nelse\r\nstride = 1;\r\nBUG_ON(!nslots);\r\nspin_lock_irqsave(&io_tlb_lock, flags);\r\nindex = ALIGN(io_tlb_index, stride);\r\nif (index >= io_tlb_nslabs)\r\nindex = 0;\r\nwrap = index;\r\ndo {\r\nwhile (iommu_is_span_boundary(index, nslots, offset_slots,\r\nmax_slots)) {\r\nindex += stride;\r\nif (index >= io_tlb_nslabs)\r\nindex = 0;\r\nif (index == wrap)\r\ngoto not_found;\r\n}\r\nif (io_tlb_list[index] >= nslots) {\r\nint count = 0;\r\nfor (i = index; i < (int) (index + nslots); i++)\r\nio_tlb_list[i] = 0;\r\nfor (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE - 1) && io_tlb_list[i]; i--)\r\nio_tlb_list[i] = ++count;\r\ndma_addr = io_tlb_start + (index << IO_TLB_SHIFT);\r\nio_tlb_index = ((index + nslots) < io_tlb_nslabs\r\n? (index + nslots) : 0);\r\ngoto found;\r\n}\r\nindex += stride;\r\nif (index >= io_tlb_nslabs)\r\nindex = 0;\r\n} while (index != wrap);\r\nnot_found:\r\nspin_unlock_irqrestore(&io_tlb_lock, flags);\r\nreturn NULL;\r\nfound:\r\nspin_unlock_irqrestore(&io_tlb_lock, flags);\r\nfor (i = 0; i < nslots; i++)\r\nio_tlb_orig_addr[index+i] = phys + (i << IO_TLB_SHIFT);\r\nif (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)\r\nswiotlb_bounce(phys, dma_addr, size, DMA_TO_DEVICE);\r\nreturn dma_addr;\r\n}\r\nstatic void *\r\nmap_single(struct device *hwdev, phys_addr_t phys, size_t size,\r\nenum dma_data_direction dir)\r\n{\r\ndma_addr_t start_dma_addr = swiotlb_virt_to_bus(hwdev, io_tlb_start);\r\nreturn swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size, dir);\r\n}\r\nvoid\r\nswiotlb_tbl_unmap_single(struct device *hwdev, char *dma_addr, size_t size,\r\nenum dma_data_direction dir)\r\n{\r\nunsigned long flags;\r\nint i, count, nslots = ALIGN(size, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;\r\nint index = (dma_addr - io_tlb_start) >> IO_TLB_SHIFT;\r\nphys_addr_t phys = io_tlb_orig_addr[index];\r\nif (phys && ((dir == DMA_FROM_DEVICE) || (dir == DMA_BIDIRECTIONAL)))\r\nswiotlb_bounce(phys, dma_addr, size, DMA_FROM_DEVICE);\r\nspin_lock_irqsave(&io_tlb_lock, flags);\r\n{\r\ncount = ((index + nslots) < ALIGN(index + 1, IO_TLB_SEGSIZE) ?\r\nio_tlb_list[index + nslots] : 0);\r\nfor (i = index + nslots - 1; i >= index; i--)\r\nio_tlb_list[i] = ++count;\r\nfor (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE -1) && io_tlb_list[i]; i--)\r\nio_tlb_list[i] = ++count;\r\n}\r\nspin_unlock_irqrestore(&io_tlb_lock, flags);\r\n}\r\nvoid\r\nswiotlb_tbl_sync_single(struct device *hwdev, char *dma_addr, size_t size,\r\nenum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nint index = (dma_addr - io_tlb_start) >> IO_TLB_SHIFT;\r\nphys_addr_t phys = io_tlb_orig_addr[index];\r\nphys += ((unsigned long)dma_addr & ((1 << IO_TLB_SHIFT) - 1));\r\nswitch (target) {\r\ncase SYNC_FOR_CPU:\r\nif (likely(dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))\r\nswiotlb_bounce(phys, dma_addr, size, DMA_FROM_DEVICE);\r\nelse\r\nBUG_ON(dir != DMA_TO_DEVICE);\r\nbreak;\r\ncase SYNC_FOR_DEVICE:\r\nif (likely(dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL))\r\nswiotlb_bounce(phys, dma_addr, size, DMA_TO_DEVICE);\r\nelse\r\nBUG_ON(dir != DMA_FROM_DEVICE);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nvoid *\r\nswiotlb_alloc_coherent(struct device *hwdev, size_t size,\r\ndma_addr_t *dma_handle, gfp_t flags)\r\n{\r\ndma_addr_t dev_addr;\r\nvoid *ret;\r\nint order = get_order(size);\r\nu64 dma_mask = DMA_BIT_MASK(32);\r\nif (hwdev && hwdev->coherent_dma_mask)\r\ndma_mask = hwdev->coherent_dma_mask;\r\nret = (void *)__get_free_pages(flags, order);\r\nif (ret && swiotlb_virt_to_bus(hwdev, ret) + size - 1 > dma_mask) {\r\nfree_pages((unsigned long) ret, order);\r\nret = NULL;\r\n}\r\nif (!ret) {\r\nret = map_single(hwdev, 0, size, DMA_FROM_DEVICE);\r\nif (!ret)\r\nreturn NULL;\r\n}\r\nmemset(ret, 0, size);\r\ndev_addr = swiotlb_virt_to_bus(hwdev, ret);\r\nif (dev_addr + size - 1 > dma_mask) {\r\nprintk("hwdev DMA mask = 0x%016Lx, dev_addr = 0x%016Lx\n",\r\n(unsigned long long)dma_mask,\r\n(unsigned long long)dev_addr);\r\nswiotlb_tbl_unmap_single(hwdev, ret, size, DMA_TO_DEVICE);\r\nreturn NULL;\r\n}\r\n*dma_handle = dev_addr;\r\nreturn ret;\r\n}\r\nvoid\r\nswiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,\r\ndma_addr_t dev_addr)\r\n{\r\nphys_addr_t paddr = dma_to_phys(hwdev, dev_addr);\r\nWARN_ON(irqs_disabled());\r\nif (!is_swiotlb_buffer(paddr))\r\nfree_pages((unsigned long)vaddr, get_order(size));\r\nelse\r\nswiotlb_tbl_unmap_single(hwdev, vaddr, size, DMA_TO_DEVICE);\r\n}\r\nstatic void\r\nswiotlb_full(struct device *dev, size_t size, enum dma_data_direction dir,\r\nint do_panic)\r\n{\r\nprintk(KERN_ERR "DMA: Out of SW-IOMMU space for %zu bytes at "\r\n"device %s\n", size, dev ? dev_name(dev) : "?");\r\nif (size <= io_tlb_overflow || !do_panic)\r\nreturn;\r\nif (dir == DMA_BIDIRECTIONAL)\r\npanic("DMA: Random memory could be DMA accessed\n");\r\nif (dir == DMA_FROM_DEVICE)\r\npanic("DMA: Random memory could be DMA written\n");\r\nif (dir == DMA_TO_DEVICE)\r\npanic("DMA: Random memory could be DMA read\n");\r\n}\r\ndma_addr_t swiotlb_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nphys_addr_t phys = page_to_phys(page) + offset;\r\ndma_addr_t dev_addr = phys_to_dma(dev, phys);\r\nvoid *map;\r\nBUG_ON(dir == DMA_NONE);\r\nif (dma_capable(dev, dev_addr, size) && !swiotlb_force)\r\nreturn dev_addr;\r\nmap = map_single(dev, phys, size, dir);\r\nif (!map) {\r\nswiotlb_full(dev, size, dir, 1);\r\nmap = io_tlb_overflow_buffer;\r\n}\r\ndev_addr = swiotlb_virt_to_bus(dev, map);\r\nif (!dma_capable(dev, dev_addr, size)) {\r\nswiotlb_tbl_unmap_single(dev, map, size, dir);\r\ndev_addr = swiotlb_virt_to_bus(dev, io_tlb_overflow_buffer);\r\n}\r\nreturn dev_addr;\r\n}\r\nstatic void unmap_single(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nphys_addr_t paddr = dma_to_phys(hwdev, dev_addr);\r\nBUG_ON(dir == DMA_NONE);\r\nif (is_swiotlb_buffer(paddr)) {\r\nswiotlb_tbl_unmap_single(hwdev, phys_to_virt(paddr), size, dir);\r\nreturn;\r\n}\r\nif (dir != DMA_FROM_DEVICE)\r\nreturn;\r\ndma_mark_clean(phys_to_virt(paddr), size);\r\n}\r\nvoid swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nunmap_single(hwdev, dev_addr, size, dir);\r\n}\r\nstatic void\r\nswiotlb_sync_single(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nphys_addr_t paddr = dma_to_phys(hwdev, dev_addr);\r\nBUG_ON(dir == DMA_NONE);\r\nif (is_swiotlb_buffer(paddr)) {\r\nswiotlb_tbl_sync_single(hwdev, phys_to_virt(paddr), size, dir,\r\ntarget);\r\nreturn;\r\n}\r\nif (dir != DMA_FROM_DEVICE)\r\nreturn;\r\ndma_mark_clean(phys_to_virt(paddr), size);\r\n}\r\nvoid\r\nswiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nswiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_CPU);\r\n}\r\nvoid\r\nswiotlb_sync_single_for_device(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nswiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_DEVICE);\r\n}\r\nint\r\nswiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,\r\nenum dma_data_direction dir, struct dma_attrs *attrs)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nBUG_ON(dir == DMA_NONE);\r\nfor_each_sg(sgl, sg, nelems, i) {\r\nphys_addr_t paddr = sg_phys(sg);\r\ndma_addr_t dev_addr = phys_to_dma(hwdev, paddr);\r\nif (swiotlb_force ||\r\n!dma_capable(hwdev, dev_addr, sg->length)) {\r\nvoid *map = map_single(hwdev, sg_phys(sg),\r\nsg->length, dir);\r\nif (!map) {\r\nswiotlb_full(hwdev, sg->length, dir, 0);\r\nswiotlb_unmap_sg_attrs(hwdev, sgl, i, dir,\r\nattrs);\r\nsgl[0].dma_length = 0;\r\nreturn 0;\r\n}\r\nsg->dma_address = swiotlb_virt_to_bus(hwdev, map);\r\n} else\r\nsg->dma_address = dev_addr;\r\nsg->dma_length = sg->length;\r\n}\r\nreturn nelems;\r\n}\r\nint\r\nswiotlb_map_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,\r\nenum dma_data_direction dir)\r\n{\r\nreturn swiotlb_map_sg_attrs(hwdev, sgl, nelems, dir, NULL);\r\n}\r\nvoid\r\nswiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir, struct dma_attrs *attrs)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nBUG_ON(dir == DMA_NONE);\r\nfor_each_sg(sgl, sg, nelems, i)\r\nunmap_single(hwdev, sg->dma_address, sg->dma_length, dir);\r\n}\r\nvoid\r\nswiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,\r\nenum dma_data_direction dir)\r\n{\r\nreturn swiotlb_unmap_sg_attrs(hwdev, sgl, nelems, dir, NULL);\r\n}\r\nstatic void\r\nswiotlb_sync_sg(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(sgl, sg, nelems, i)\r\nswiotlb_sync_single(hwdev, sg->dma_address,\r\nsg->dma_length, dir, target);\r\n}\r\nvoid\r\nswiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,\r\nint nelems, enum dma_data_direction dir)\r\n{\r\nswiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_CPU);\r\n}\r\nvoid\r\nswiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,\r\nint nelems, enum dma_data_direction dir)\r\n{\r\nswiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_DEVICE);\r\n}\r\nint\r\nswiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)\r\n{\r\nreturn (dma_addr == swiotlb_virt_to_bus(hwdev, io_tlb_overflow_buffer));\r\n}\r\nint\r\nswiotlb_dma_supported(struct device *hwdev, u64 mask)\r\n{\r\nreturn swiotlb_virt_to_bus(hwdev, io_tlb_end - 1) <= mask;\r\n}
