static int __init hardlockup_panic_setup(char *str)\r\n{\r\nif (!strncmp(str, "panic", 5))\r\nhardlockup_panic = 1;\r\nelse if (!strncmp(str, "nopanic", 7))\r\nhardlockup_panic = 0;\r\nelse if (!strncmp(str, "0", 1))\r\nwatchdog_enabled = 0;\r\nreturn 1;\r\n}\r\nstatic int __init softlockup_panic_setup(char *str)\r\n{\r\nsoftlockup_panic = simple_strtoul(str, NULL, 0);\r\nreturn 1;\r\n}\r\nstatic int __init nowatchdog_setup(char *str)\r\n{\r\nwatchdog_enabled = 0;\r\nreturn 1;\r\n}\r\nstatic int __init nosoftlockup_setup(char *str)\r\n{\r\nwatchdog_enabled = 0;\r\nreturn 1;\r\n}\r\nstatic int get_softlockup_thresh(void)\r\n{\r\nreturn watchdog_thresh * 2;\r\n}\r\nstatic unsigned long get_timestamp(int this_cpu)\r\n{\r\nreturn cpu_clock(this_cpu) >> 30LL;\r\n}\r\nstatic unsigned long get_sample_period(void)\r\n{\r\nreturn get_softlockup_thresh() * (NSEC_PER_SEC / 5);\r\n}\r\nstatic void __touch_watchdog(void)\r\n{\r\nint this_cpu = smp_processor_id();\r\n__this_cpu_write(watchdog_touch_ts, get_timestamp(this_cpu));\r\n}\r\nvoid touch_softlockup_watchdog(void)\r\n{\r\n__this_cpu_write(watchdog_touch_ts, 0);\r\n}\r\nvoid touch_all_softlockup_watchdogs(void)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nper_cpu(watchdog_touch_ts, cpu) = 0;\r\n}\r\nvoid touch_nmi_watchdog(void)\r\n{\r\nif (watchdog_enabled) {\r\nunsigned cpu;\r\nfor_each_present_cpu(cpu) {\r\nif (per_cpu(watchdog_nmi_touch, cpu) != true)\r\nper_cpu(watchdog_nmi_touch, cpu) = true;\r\n}\r\n}\r\ntouch_softlockup_watchdog();\r\n}\r\nvoid touch_softlockup_watchdog_sync(void)\r\n{\r\n__raw_get_cpu_var(softlockup_touch_sync) = true;\r\n__raw_get_cpu_var(watchdog_touch_ts) = 0;\r\n}\r\nstatic int is_hardlockup(void)\r\n{\r\nunsigned long hrint = __this_cpu_read(hrtimer_interrupts);\r\nif (__this_cpu_read(hrtimer_interrupts_saved) == hrint)\r\nreturn 1;\r\n__this_cpu_write(hrtimer_interrupts_saved, hrint);\r\nreturn 0;\r\n}\r\nstatic int is_softlockup(unsigned long touch_ts)\r\n{\r\nunsigned long now = get_timestamp(smp_processor_id());\r\nif (time_after(now, touch_ts + get_softlockup_thresh()))\r\nreturn now - touch_ts;\r\nreturn 0;\r\n}\r\nstatic void watchdog_overflow_callback(struct perf_event *event,\r\nstruct perf_sample_data *data,\r\nstruct pt_regs *regs)\r\n{\r\nevent->hw.interrupts = 0;\r\nif (__this_cpu_read(watchdog_nmi_touch) == true) {\r\n__this_cpu_write(watchdog_nmi_touch, false);\r\nreturn;\r\n}\r\nif (is_hardlockup()) {\r\nint this_cpu = smp_processor_id();\r\nif (__this_cpu_read(hard_watchdog_warn) == true)\r\nreturn;\r\nif (hardlockup_panic)\r\npanic("Watchdog detected hard LOCKUP on cpu %d", this_cpu);\r\nelse\r\nWARN(1, "Watchdog detected hard LOCKUP on cpu %d", this_cpu);\r\n__this_cpu_write(hard_watchdog_warn, true);\r\nreturn;\r\n}\r\n__this_cpu_write(hard_watchdog_warn, false);\r\nreturn;\r\n}\r\nstatic void watchdog_interrupt_count(void)\r\n{\r\n__this_cpu_inc(hrtimer_interrupts);\r\n}\r\nstatic inline void watchdog_interrupt_count(void) { return; }\r\nstatic enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)\r\n{\r\nunsigned long touch_ts = __this_cpu_read(watchdog_touch_ts);\r\nstruct pt_regs *regs = get_irq_regs();\r\nint duration;\r\nwatchdog_interrupt_count();\r\nwake_up_process(__this_cpu_read(softlockup_watchdog));\r\nhrtimer_forward_now(hrtimer, ns_to_ktime(get_sample_period()));\r\nif (touch_ts == 0) {\r\nif (unlikely(__this_cpu_read(softlockup_touch_sync))) {\r\n__this_cpu_write(softlockup_touch_sync, false);\r\nsched_clock_tick();\r\n}\r\nkvm_check_and_clear_guest_paused();\r\n__touch_watchdog();\r\nreturn HRTIMER_RESTART;\r\n}\r\nduration = is_softlockup(touch_ts);\r\nif (unlikely(duration)) {\r\nif (kvm_check_and_clear_guest_paused())\r\nreturn HRTIMER_RESTART;\r\nif (__this_cpu_read(soft_watchdog_warn) == true)\r\nreturn HRTIMER_RESTART;\r\nprintk(KERN_EMERG "BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\n",\r\nsmp_processor_id(), duration,\r\ncurrent->comm, task_pid_nr(current));\r\nprint_modules();\r\nprint_irqtrace_events(current);\r\nif (regs)\r\nshow_regs(regs);\r\nelse\r\ndump_stack();\r\nif (softlockup_panic)\r\npanic("softlockup: hung tasks");\r\n__this_cpu_write(soft_watchdog_warn, true);\r\n} else\r\n__this_cpu_write(soft_watchdog_warn, false);\r\nreturn HRTIMER_RESTART;\r\n}\r\nstatic int watchdog(void *unused)\r\n{\r\nstruct sched_param param = { .sched_priority = 0 };\r\nstruct hrtimer *hrtimer = &__raw_get_cpu_var(watchdog_hrtimer);\r\n__touch_watchdog();\r\nhrtimer_start(hrtimer, ns_to_ktime(get_sample_period()),\r\nHRTIMER_MODE_REL_PINNED);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nwhile (!kthread_should_stop()) {\r\n__touch_watchdog();\r\nschedule();\r\nif (kthread_should_stop())\r\nbreak;\r\nset_current_state(TASK_INTERRUPTIBLE);\r\n}\r\n__set_current_state(TASK_RUNNING);\r\nsched_setscheduler(current, SCHED_NORMAL, &param);\r\nreturn 0;\r\n}\r\nstatic int watchdog_nmi_enable(int cpu)\r\n{\r\nstruct perf_event_attr *wd_attr;\r\nstruct perf_event *event = per_cpu(watchdog_ev, cpu);\r\nif (event && event->state > PERF_EVENT_STATE_OFF)\r\ngoto out;\r\nif (event != NULL)\r\ngoto out_enable;\r\nwd_attr = &wd_hw_attr;\r\nwd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);\r\nevent = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);\r\nif (cpu == 0 && IS_ERR(event))\r\ncpu0_err = PTR_ERR(event);\r\nif (!IS_ERR(event)) {\r\nif (cpu == 0 || cpu0_err)\r\npr_info("enabled on all CPUs, permanently consumes one hw-PMU counter.\n");\r\ngoto out_save;\r\n}\r\nif (cpu > 0 && (PTR_ERR(event) == cpu0_err))\r\nreturn PTR_ERR(event);\r\nif (PTR_ERR(event) == -EOPNOTSUPP)\r\npr_info("disabled (cpu%i): not supported (no LAPIC?)\n", cpu);\r\nelse if (PTR_ERR(event) == -ENOENT)\r\npr_warning("disabled (cpu%i): hardware events not enabled\n",\r\ncpu);\r\nelse\r\npr_err("disabled (cpu%i): unable to create perf event: %ld\n",\r\ncpu, PTR_ERR(event));\r\nreturn PTR_ERR(event);\r\nout_save:\r\nper_cpu(watchdog_ev, cpu) = event;\r\nout_enable:\r\nperf_event_enable(per_cpu(watchdog_ev, cpu));\r\nout:\r\nreturn 0;\r\n}\r\nstatic void watchdog_nmi_disable(int cpu)\r\n{\r\nstruct perf_event *event = per_cpu(watchdog_ev, cpu);\r\nif (event) {\r\nperf_event_disable(event);\r\nper_cpu(watchdog_ev, cpu) = NULL;\r\nperf_event_release_kernel(event);\r\n}\r\nreturn;\r\n}\r\nstatic int watchdog_nmi_enable(int cpu) { return 0; }\r\nstatic void watchdog_nmi_disable(int cpu) { return; }\r\nstatic void watchdog_prepare_cpu(int cpu)\r\n{\r\nstruct hrtimer *hrtimer = &per_cpu(watchdog_hrtimer, cpu);\r\nWARN_ON(per_cpu(softlockup_watchdog, cpu));\r\nhrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nhrtimer->function = watchdog_timer_fn;\r\n}\r\nstatic int watchdog_enable(int cpu)\r\n{\r\nstruct task_struct *p = per_cpu(softlockup_watchdog, cpu);\r\nint err = 0;\r\nerr = watchdog_nmi_enable(cpu);\r\nif (!p) {\r\nstruct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };\r\np = kthread_create_on_node(watchdog, NULL, cpu_to_node(cpu), "watchdog/%d", cpu);\r\nif (IS_ERR(p)) {\r\npr_err("softlockup watchdog for %i failed\n", cpu);\r\nif (!err) {\r\nerr = PTR_ERR(p);\r\nwatchdog_nmi_disable(cpu);\r\n}\r\ngoto out;\r\n}\r\nsched_setscheduler(p, SCHED_FIFO, &param);\r\nkthread_bind(p, cpu);\r\nper_cpu(watchdog_touch_ts, cpu) = 0;\r\nper_cpu(softlockup_watchdog, cpu) = p;\r\nwake_up_process(p);\r\n}\r\nout:\r\nreturn err;\r\n}\r\nstatic void watchdog_disable(int cpu)\r\n{\r\nstruct task_struct *p = per_cpu(softlockup_watchdog, cpu);\r\nstruct hrtimer *hrtimer = &per_cpu(watchdog_hrtimer, cpu);\r\nhrtimer_cancel(hrtimer);\r\nwatchdog_nmi_disable(cpu);\r\nif (p) {\r\nper_cpu(softlockup_watchdog, cpu) = NULL;\r\nkthread_stop(p);\r\n}\r\n}\r\nstatic void watchdog_enable_all_cpus(void)\r\n{\r\nint cpu;\r\nwatchdog_enabled = 0;\r\nfor_each_online_cpu(cpu)\r\nif (!watchdog_enable(cpu))\r\nwatchdog_enabled = 1;\r\nif (!watchdog_enabled)\r\npr_err("failed to be enabled on some cpus\n");\r\n}\r\nstatic void watchdog_disable_all_cpus(void)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nwatchdog_disable(cpu);\r\nwatchdog_enabled = 0;\r\n}\r\nint proc_dowatchdog(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp, loff_t *ppos)\r\n{\r\nint ret;\r\nret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\r\nif (ret || !write)\r\ngoto out;\r\nif (watchdog_enabled && watchdog_thresh)\r\nwatchdog_enable_all_cpus();\r\nelse\r\nwatchdog_disable_all_cpus();\r\nout:\r\nreturn ret;\r\n}\r\nstatic int __cpuinit\r\ncpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)\r\n{\r\nint hotcpu = (unsigned long)hcpu;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_UP_PREPARE_FROZEN:\r\nwatchdog_prepare_cpu(hotcpu);\r\nbreak;\r\ncase CPU_ONLINE:\r\ncase CPU_ONLINE_FROZEN:\r\nif (watchdog_enabled)\r\nwatchdog_enable(hotcpu);\r\nbreak;\r\n#ifdef CONFIG_HOTPLUG_CPU\r\ncase CPU_UP_CANCELED:\r\ncase CPU_UP_CANCELED_FROZEN:\r\nwatchdog_disable(hotcpu);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\nwatchdog_disable(hotcpu);\r\nbreak;\r\n#endif\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nvoid __init lockup_detector_init(void)\r\n{\r\nvoid *cpu = (void *)(long)smp_processor_id();\r\nint err;\r\nerr = cpu_callback(&cpu_nfb, CPU_UP_PREPARE, cpu);\r\nWARN_ON(notifier_to_errno(err));\r\ncpu_callback(&cpu_nfb, CPU_ONLINE, cpu);\r\nregister_cpu_notifier(&cpu_nfb);\r\nreturn;\r\n}
