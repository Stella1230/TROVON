static inline uint16_t get_client_id_from_client(struct zcache_client *cli)\r\n{\r\nBUG_ON(cli == NULL);\r\nif (cli == &zcache_host)\r\nreturn LOCAL_CLIENT;\r\nreturn cli - &zcache_clients[0];\r\n}\r\nstatic inline bool is_local_client(struct zcache_client *cli)\r\n{\r\nreturn cli == &zcache_host;\r\n}\r\nstatic inline unsigned zbud_max_buddy_size(void)\r\n{\r\nreturn MAX_CHUNK << CHUNK_SHIFT;\r\n}\r\nstatic inline unsigned zbud_size_to_chunks(unsigned size)\r\n{\r\nBUG_ON(size == 0 || size > zbud_max_buddy_size());\r\nreturn (size + CHUNK_SIZE - 1) >> CHUNK_SHIFT;\r\n}\r\nstatic inline int zbud_budnum(struct zbud_hdr *zh)\r\n{\r\nunsigned offset = (unsigned long)zh & (PAGE_SIZE - 1);\r\nstruct zbud_page *zbpg = NULL;\r\nunsigned budnum = -1U;\r\nint i;\r\nfor (i = 0; i < ZBUD_MAX_BUDS; i++)\r\nif (offset == offsetof(typeof(*zbpg), buddy[i])) {\r\nbudnum = i;\r\nbreak;\r\n}\r\nBUG_ON(budnum == -1U);\r\nreturn budnum;\r\n}\r\nstatic char *zbud_data(struct zbud_hdr *zh, unsigned size)\r\n{\r\nstruct zbud_page *zbpg;\r\nchar *p;\r\nunsigned budnum;\r\nASSERT_SENTINEL(zh, ZBH);\r\nbudnum = zbud_budnum(zh);\r\nBUG_ON(size == 0 || size > zbud_max_buddy_size());\r\nzbpg = container_of(zh, struct zbud_page, buddy[budnum]);\r\nASSERT_SPINLOCK(&zbpg->lock);\r\np = (char *)zbpg;\r\nif (budnum == 0)\r\np += ((sizeof(struct zbud_page) + CHUNK_SIZE - 1) &\r\nCHUNK_MASK);\r\nelse if (budnum == 1)\r\np += PAGE_SIZE - ((size + CHUNK_SIZE - 1) & CHUNK_MASK);\r\nreturn p;\r\n}\r\nstatic void zbud_copy_from_pampd(char *data, size_t *size, struct zbud_hdr *zh)\r\n{\r\nstruct zbud_page *zbpg;\r\nchar *p;\r\nunsigned budnum;\r\nASSERT_SENTINEL(zh, ZBH);\r\nbudnum = zbud_budnum(zh);\r\nzbpg = container_of(zh, struct zbud_page, buddy[budnum]);\r\nspin_lock(&zbpg->lock);\r\nBUG_ON(zh->size > *size);\r\np = (char *)zbpg;\r\nif (budnum == 0)\r\np += ((sizeof(struct zbud_page) + CHUNK_SIZE - 1) &\r\nCHUNK_MASK);\r\nelse if (budnum == 1)\r\np += PAGE_SIZE - ((zh->size + CHUNK_SIZE - 1) & CHUNK_MASK);\r\nmemcpy(data, p, zh->size);\r\n*size = zh->size;\r\nspin_unlock(&zbpg->lock);\r\n}\r\nstatic struct zbud_page *zbud_alloc_raw_page(void)\r\n{\r\nstruct zbud_page *zbpg = NULL;\r\nstruct zbud_hdr *zh0, *zh1;\r\nzbpg = zcache_get_free_page();\r\nif (likely(zbpg != NULL)) {\r\nINIT_LIST_HEAD(&zbpg->bud_list);\r\nzh0 = &zbpg->buddy[0]; zh1 = &zbpg->buddy[1];\r\nspin_lock_init(&zbpg->lock);\r\natomic_inc(&zcache_zbud_curr_raw_pages);\r\nINIT_LIST_HEAD(&zbpg->bud_list);\r\nSET_SENTINEL(zbpg, ZBPG);\r\nzh0->size = 0; zh1->size = 0;\r\ntmem_oid_set_invalid(&zh0->oid);\r\ntmem_oid_set_invalid(&zh1->oid);\r\n}\r\nreturn zbpg;\r\n}\r\nstatic void zbud_free_raw_page(struct zbud_page *zbpg)\r\n{\r\nstruct zbud_hdr *zh0 = &zbpg->buddy[0], *zh1 = &zbpg->buddy[1];\r\nASSERT_SENTINEL(zbpg, ZBPG);\r\nBUG_ON(!list_empty(&zbpg->bud_list));\r\nASSERT_SPINLOCK(&zbpg->lock);\r\nBUG_ON(zh0->size != 0 || tmem_oid_valid(&zh0->oid));\r\nBUG_ON(zh1->size != 0 || tmem_oid_valid(&zh1->oid));\r\nINVERT_SENTINEL(zbpg, ZBPG);\r\nspin_unlock(&zbpg->lock);\r\natomic_dec(&zcache_zbud_curr_raw_pages);\r\nzcache_free_page(zbpg);\r\n}\r\nstatic unsigned zbud_free(struct zbud_hdr *zh)\r\n{\r\nunsigned size;\r\nASSERT_SENTINEL(zh, ZBH);\r\nBUG_ON(!tmem_oid_valid(&zh->oid));\r\nsize = zh->size;\r\nBUG_ON(zh->size == 0 || zh->size > zbud_max_buddy_size());\r\nzh->size = 0;\r\ntmem_oid_set_invalid(&zh->oid);\r\nINVERT_SENTINEL(zh, ZBH);\r\nzcache_zbud_curr_zbytes -= size;\r\natomic_dec(&zcache_zbud_curr_zpages);\r\nreturn size;\r\n}\r\nstatic void zbud_free_and_delist(struct zbud_hdr *zh)\r\n{\r\nunsigned chunks;\r\nstruct zbud_hdr *zh_other;\r\nunsigned budnum = zbud_budnum(zh), size;\r\nstruct zbud_page *zbpg =\r\ncontainer_of(zh, struct zbud_page, buddy[budnum]);\r\nWARN_ON(!irqs_disabled());\r\nspin_lock(&zbpg->lock);\r\nif (list_empty(&zbpg->bud_list)) {\r\nspin_unlock(&zbpg->lock);\r\nreturn;\r\n}\r\nsize = zbud_free(zh);\r\nASSERT_SPINLOCK(&zbpg->lock);\r\nzh_other = &zbpg->buddy[(budnum == 0) ? 1 : 0];\r\nif (zh_other->size == 0) {\r\nchunks = zbud_size_to_chunks(size) ;\r\nspin_lock(&zbud_budlists_spinlock);\r\nBUG_ON(list_empty(&zbud_unbuddied[chunks].list));\r\nlist_del_init(&zbpg->bud_list);\r\nzbud_unbuddied[chunks].count--;\r\nspin_unlock(&zbud_budlists_spinlock);\r\nzbud_free_raw_page(zbpg);\r\n} else {\r\nchunks = zbud_size_to_chunks(zh_other->size) ;\r\nspin_lock(&zbud_budlists_spinlock);\r\nlist_del_init(&zbpg->bud_list);\r\nzcache_zbud_buddied_count--;\r\nlist_add_tail(&zbpg->bud_list, &zbud_unbuddied[chunks].list);\r\nzbud_unbuddied[chunks].count++;\r\nspin_unlock(&zbud_budlists_spinlock);\r\nspin_unlock(&zbpg->lock);\r\n}\r\n}\r\nstatic struct zbud_hdr *zbud_create(uint16_t client_id, uint16_t pool_id,\r\nstruct tmem_oid *oid,\r\nuint32_t index, struct page *page,\r\nvoid *cdata, unsigned size)\r\n{\r\nstruct zbud_hdr *zh0, *zh1, *zh = NULL;\r\nstruct zbud_page *zbpg = NULL, *ztmp;\r\nunsigned nchunks;\r\nchar *to;\r\nint i, found_good_buddy = 0;\r\nnchunks = zbud_size_to_chunks(size) ;\r\nfor (i = MAX_CHUNK - nchunks + 1; i > 0; i--) {\r\nspin_lock(&zbud_budlists_spinlock);\r\nif (!list_empty(&zbud_unbuddied[i].list)) {\r\nlist_for_each_entry_safe(zbpg, ztmp,\r\n&zbud_unbuddied[i].list, bud_list) {\r\nif (spin_trylock(&zbpg->lock)) {\r\nfound_good_buddy = i;\r\ngoto found_unbuddied;\r\n}\r\n}\r\n}\r\nspin_unlock(&zbud_budlists_spinlock);\r\n}\r\nzbpg = zbud_alloc_raw_page();\r\nif (unlikely(zbpg == NULL))\r\ngoto out;\r\nspin_lock(&zbud_budlists_spinlock);\r\nspin_lock(&zbpg->lock);\r\nlist_add_tail(&zbpg->bud_list, &zbud_unbuddied[nchunks].list);\r\nzbud_unbuddied[nchunks].count++;\r\nzh = &zbpg->buddy[0];\r\ngoto init_zh;\r\nfound_unbuddied:\r\nASSERT_SPINLOCK(&zbpg->lock);\r\nzh0 = &zbpg->buddy[0]; zh1 = &zbpg->buddy[1];\r\nBUG_ON(!((zh0->size == 0) ^ (zh1->size == 0)));\r\nif (zh0->size != 0) {\r\nASSERT_SENTINEL(zh0, ZBH);\r\nzh = zh1;\r\n} else if (zh1->size != 0) {\r\nASSERT_SENTINEL(zh1, ZBH);\r\nzh = zh0;\r\n} else\r\nBUG();\r\nlist_del_init(&zbpg->bud_list);\r\nzbud_unbuddied[found_good_buddy].count--;\r\nlist_add_tail(&zbpg->bud_list, &zbud_buddied_list);\r\nzcache_zbud_buddied_count++;\r\ninit_zh:\r\nSET_SENTINEL(zh, ZBH);\r\nzh->size = size;\r\nzh->index = index;\r\nzh->oid = *oid;\r\nzh->pool_id = pool_id;\r\nzh->client_id = client_id;\r\nto = zbud_data(zh, size);\r\nmemcpy(to, cdata, size);\r\nspin_unlock(&zbpg->lock);\r\nspin_unlock(&zbud_budlists_spinlock);\r\nzbud_cumul_chunk_counts[nchunks]++;\r\natomic_inc(&zcache_zbud_curr_zpages);\r\nzcache_zbud_cumul_zpages++;\r\nzcache_zbud_curr_zbytes += size;\r\nzcache_zbud_cumul_zbytes += size;\r\nout:\r\nreturn zh;\r\n}\r\nstatic int zbud_decompress(struct page *page, struct zbud_hdr *zh)\r\n{\r\nstruct zbud_page *zbpg;\r\nunsigned budnum = zbud_budnum(zh);\r\nsize_t out_len = PAGE_SIZE;\r\nchar *to_va, *from_va;\r\nunsigned size;\r\nint ret = 0;\r\nzbpg = container_of(zh, struct zbud_page, buddy[budnum]);\r\nspin_lock(&zbpg->lock);\r\nif (list_empty(&zbpg->bud_list)) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nASSERT_SENTINEL(zh, ZBH);\r\nBUG_ON(zh->size == 0 || zh->size > zbud_max_buddy_size());\r\nto_va = kmap_atomic(page);\r\nsize = zh->size;\r\nfrom_va = zbud_data(zh, size);\r\nret = lzo1x_decompress_safe(from_va, size, to_va, &out_len);\r\nBUG_ON(ret != LZO_E_OK);\r\nBUG_ON(out_len != PAGE_SIZE);\r\nkunmap_atomic(to_va);\r\nout:\r\nspin_unlock(&zbpg->lock);\r\nreturn ret;\r\n}\r\nstatic void zbud_evict_zbpg(struct zbud_page *zbpg)\r\n{\r\nstruct zbud_hdr *zh;\r\nint i, j;\r\nuint32_t pool_id[ZBUD_MAX_BUDS], client_id[ZBUD_MAX_BUDS];\r\nuint32_t index[ZBUD_MAX_BUDS];\r\nstruct tmem_oid oid[ZBUD_MAX_BUDS];\r\nstruct tmem_pool *pool;\r\nunsigned long flags;\r\nASSERT_SPINLOCK(&zbpg->lock);\r\nfor (i = 0, j = 0; i < ZBUD_MAX_BUDS; i++) {\r\nzh = &zbpg->buddy[i];\r\nif (zh->size) {\r\nclient_id[j] = zh->client_id;\r\npool_id[j] = zh->pool_id;\r\noid[j] = zh->oid;\r\nindex[j] = zh->index;\r\nj++;\r\n}\r\n}\r\nspin_unlock(&zbpg->lock);\r\nfor (i = 0; i < j; i++) {\r\npool = zcache_get_pool_by_id(client_id[i], pool_id[i]);\r\nBUG_ON(pool == NULL);\r\nlocal_irq_save(flags);\r\ntmem_flush_page(pool, &oid[i], index[i]);\r\nlocal_irq_restore(flags);\r\nzcache_put_pool(pool);\r\n}\r\n}\r\nstatic void zbud_evict_pages(int nr)\r\n{\r\nstruct zbud_page *zbpg;\r\nint i, newly_unused_pages = 0;\r\nfor (i = 0; i < MAX_CHUNK; i++) {\r\nretry_unbud_list_i:\r\nspin_lock_bh(&zbud_budlists_spinlock);\r\nif (list_empty(&zbud_unbuddied[i].list)) {\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\ncontinue;\r\n}\r\nlist_for_each_entry(zbpg, &zbud_unbuddied[i].list, bud_list) {\r\nif (unlikely(!spin_trylock(&zbpg->lock)))\r\ncontinue;\r\nzbud_unbuddied[i].count--;\r\nspin_unlock(&zbud_budlists_spinlock);\r\nzcache_evicted_unbuddied_pages++;\r\nzbud_evict_zbpg(zbpg);\r\nnewly_unused_pages++;\r\nlocal_bh_enable();\r\nif (--nr <= 0)\r\ngoto evict_unused;\r\ngoto retry_unbud_list_i;\r\n}\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\n}\r\nretry_bud_list:\r\nspin_lock_bh(&zbud_budlists_spinlock);\r\nif (list_empty(&zbud_buddied_list)) {\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\ngoto evict_unused;\r\n}\r\nlist_for_each_entry(zbpg, &zbud_buddied_list, bud_list) {\r\nif (unlikely(!spin_trylock(&zbpg->lock)))\r\ncontinue;\r\nzcache_zbud_buddied_count--;\r\nspin_unlock(&zbud_budlists_spinlock);\r\nzcache_evicted_buddied_pages++;\r\nzbud_evict_zbpg(zbpg);\r\nnewly_unused_pages++;\r\nlocal_bh_enable();\r\nif (--nr <= 0)\r\ngoto evict_unused;\r\ngoto retry_bud_list;\r\n}\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\nevict_unused:\r\nreturn;\r\n}\r\nstatic int zbud_remotify_zbud(struct tmem_xhandle *xh, char *data,\r\nsize_t size)\r\n{\r\nstruct tmem_pool *pool;\r\nint i, remotenode, ret = -1;\r\nunsigned char cksum, *p;\r\nunsigned long flags;\r\nfor (p = data, cksum = 0, i = 0; i < size; i++)\r\ncksum += *p;\r\nret = ramster_remote_put(xh, data, size, true, &remotenode);\r\nif (ret == 0) {\r\npool = zcache_get_pool_by_id(LOCAL_CLIENT, xh->pool_id);\r\nBUG_ON(pool == NULL);\r\nlocal_irq_save(flags);\r\n(void)tmem_replace(pool, &xh->oid, xh->index,\r\npampd_make_remote(remotenode, size, cksum));\r\nlocal_irq_restore(flags);\r\nzcache_put_pool(pool);\r\nramster_eph_pages_remoted++;\r\nret = 0;\r\n} else\r\nramster_eph_pages_remote_failed++;\r\nreturn ret;\r\n}\r\nstatic int zbud_remotify_zbpg(struct zbud_page *zbpg)\r\n{\r\nstruct zbud_hdr *zh1, *zh2 = NULL;\r\nstruct tmem_xhandle xh1, xh2 = { 0 };\r\nchar *data1 = NULL, *data2 = NULL;\r\nsize_t size1 = 0, size2 = 0;\r\nint ret = 0;\r\nunsigned char *tmpmem = __get_cpu_var(zcache_remoteputmem);\r\nASSERT_SPINLOCK(&zbpg->lock);\r\nif (zbpg->buddy[0].size == 0)\r\nzh1 = &zbpg->buddy[1];\r\nelse if (zbpg->buddy[1].size == 0)\r\nzh1 = &zbpg->buddy[0];\r\nelse {\r\nzh1 = &zbpg->buddy[0];\r\nzh2 = &zbpg->buddy[1];\r\n}\r\nif (zh1->client_id != LOCAL_CLIENT)\r\nzh1 = NULL;\r\nif ((zh2 != NULL) && (zh2->client_id != LOCAL_CLIENT))\r\nzh2 = NULL;\r\nif (zh1 != NULL) {\r\nxh1.client_id = zh1->client_id;\r\nxh1.pool_id = zh1->pool_id;\r\nxh1.oid = zh1->oid;\r\nxh1.index = zh1->index;\r\nsize1 = zh1->size;\r\ndata1 = zbud_data(zh1, size1);\r\nmemcpy(tmpmem, zbud_data(zh1, size1), size1);\r\ndata1 = tmpmem;\r\ntmpmem += size1;\r\n}\r\nif (zh2 != NULL) {\r\nxh2.client_id = zh2->client_id;\r\nxh2.pool_id = zh2->pool_id;\r\nxh2.oid = zh2->oid;\r\nxh2.index = zh2->index;\r\nsize2 = zh2->size;\r\nmemcpy(tmpmem, zbud_data(zh2, size2), size2);\r\ndata2 = tmpmem;\r\n}\r\nspin_unlock(&zbpg->lock);\r\npreempt_enable();\r\nif (zh1 != NULL)\r\nret = zbud_remotify_zbud(&xh1, data1, size1);\r\nif (zh2 != NULL)\r\nret |= zbud_remotify_zbud(&xh2, data2, size2);\r\nreturn ret;\r\n}\r\nvoid zbud_remotify_pages(int nr)\r\n{\r\nstruct zbud_page *zbpg;\r\nint i, ret;\r\nfor (i = 0; i < MAX_CHUNK; i++) {\r\nretry_unbud_list_i:\r\npreempt_disable();\r\nspin_lock_bh(&zbud_budlists_spinlock);\r\nif (list_empty(&zbud_unbuddied[i].list)) {\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\npreempt_enable();\r\ncontinue;\r\n}\r\nlist_for_each_entry(zbpg, &zbud_unbuddied[i].list, bud_list) {\r\nif (unlikely(!spin_trylock(&zbpg->lock)))\r\ncontinue;\r\nzbud_unbuddied[i].count--;\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\nret = zbud_remotify_zbpg(zbpg);\r\nif (ret == 0) {\r\nif (--nr <= 0)\r\ngoto out;\r\ngoto retry_unbud_list_i;\r\n}\r\npr_err("TESTING zbud_remotify_pages failed on page,"\r\n" trying to re-add\n");\r\nspin_lock_bh(&zbud_budlists_spinlock);\r\nspin_lock(&zbpg->lock);\r\nlist_add_tail(&zbpg->bud_list, &zbud_unbuddied[i].list);\r\nzbud_unbuddied[i].count++;\r\nspin_unlock(&zbpg->lock);\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\npr_err("TESTING zbud_remotify_pages failed on page,"\r\n" finished re-add\n");\r\ngoto out;\r\n}\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\npreempt_enable();\r\n}\r\nnext_buddied_zbpg:\r\npreempt_disable();\r\nspin_lock_bh(&zbud_budlists_spinlock);\r\nif (list_empty(&zbud_buddied_list))\r\ngoto unlock_out;\r\nlist_for_each_entry(zbpg, &zbud_buddied_list, bud_list) {\r\nif (unlikely(!spin_trylock(&zbpg->lock)))\r\ncontinue;\r\nzcache_zbud_buddied_count--;\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\nret = zbud_remotify_zbpg(zbpg);\r\nif (ret == 0) {\r\nif (--nr <= 0)\r\ngoto out;\r\ngoto next_buddied_zbpg;\r\n}\r\npr_err("TESTING zbud_remotify_pages failed on BUDDIED page,"\r\n" trying to re-add\n");\r\nspin_lock_bh(&zbud_budlists_spinlock);\r\nspin_lock(&zbpg->lock);\r\nlist_add_tail(&zbpg->bud_list, &zbud_buddied_list);\r\nzcache_zbud_buddied_count++;\r\nspin_unlock(&zbpg->lock);\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\npr_err("TESTING zbud_remotify_pages failed on BUDDIED page,"\r\n" finished re-add\n");\r\ngoto out;\r\n}\r\nunlock_out:\r\nspin_unlock_bh(&zbud_budlists_spinlock);\r\npreempt_enable();\r\nout:\r\nreturn;\r\n}\r\nstatic void zcache_remote_flush_page(struct flushlist_node *flnode)\r\n{\r\nstruct tmem_xhandle *xh;\r\nint remotenode, ret;\r\npreempt_disable();\r\nxh = &flnode->xh;\r\nremotenode = flnode->xh.client_id;\r\nret = ramster_remote_flush(xh, remotenode);\r\nif (ret >= 0)\r\nramster_remote_pages_flushed++;\r\nelse\r\nramster_remote_page_flushes_failed++;\r\npreempt_enable_no_resched();\r\nramster_flnode_free(flnode, NULL);\r\n}\r\nstatic void zcache_remote_flush_object(struct flushlist_node *flnode)\r\n{\r\nstruct tmem_xhandle *xh;\r\nint remotenode, ret;\r\npreempt_disable();\r\nxh = &flnode->xh;\r\nremotenode = flnode->xh.client_id;\r\nret = ramster_remote_flush_object(xh, remotenode);\r\nif (ret >= 0)\r\nramster_remote_objects_flushed++;\r\nelse\r\nramster_remote_object_flushes_failed++;\r\npreempt_enable_no_resched();\r\nramster_flnode_free(flnode, NULL);\r\n}\r\nstatic void zcache_remote_eph_put(struct zbud_hdr *zbud)\r\n{\r\n}\r\nstatic void zcache_remote_pers_put(struct zv_hdr *zv)\r\n{\r\nstruct tmem_xhandle xh;\r\nuint16_t size;\r\nbool ephemeral;\r\nint remotenode, ret = -1;\r\nchar *data;\r\nstruct tmem_pool *pool;\r\nunsigned long flags;\r\nunsigned char cksum;\r\nchar *p;\r\nint i;\r\nunsigned char *tmpmem = __get_cpu_var(zcache_remoteputmem);\r\nASSERT_SENTINEL(zv, ZVH);\r\nBUG_ON(zv->client_id != LOCAL_CLIENT);\r\nlocal_bh_disable();\r\nxh.client_id = zv->client_id;\r\nxh.pool_id = zv->pool_id;\r\nxh.oid = zv->oid;\r\nxh.index = zv->index;\r\nsize = xv_get_object_size(zv) - sizeof(*zv);\r\nBUG_ON(size == 0 || size > zv_max_page_size);\r\ndata = (char *)zv + sizeof(*zv);\r\nfor (p = data, cksum = 0, i = 0; i < size; i++)\r\ncksum += *p;\r\nmemcpy(tmpmem, data, size);\r\ndata = tmpmem;\r\npool = zcache_get_pool_by_id(zv->client_id, zv->pool_id);\r\nephemeral = is_ephemeral(pool);\r\nzcache_put_pool(pool);\r\nspin_unlock(&zcache_rem_op_list_lock);\r\nlocal_bh_enable();\r\npreempt_disable();\r\nret = ramster_remote_put(&xh, data, size, ephemeral, &remotenode);\r\npreempt_enable_no_resched();\r\nif (ret != 0) {\r\nramster_pers_pages_remote_failed++;\r\ngoto out;\r\n} else\r\natomic_inc(&ramster_remote_pers_pages);\r\nramster_pers_pages_remoted++;\r\nlocal_bh_disable();\r\npool = zcache_get_pool_by_id(LOCAL_CLIENT, xh.pool_id);\r\nlocal_irq_save(flags);\r\n(void)tmem_replace(pool, &xh.oid, xh.index,\r\npampd_make_remote(remotenode, size, cksum));\r\nlocal_irq_restore(flags);\r\nzcache_put_pool(pool);\r\nlocal_bh_enable();\r\nout:\r\nreturn;\r\n}\r\nstatic void zcache_do_remotify_ops(int nr)\r\n{\r\nstruct ramster_remotify_hdr *rem_op;\r\nunion remotify_list_node *u;\r\nwhile (1) {\r\nif (!nr)\r\ngoto out;\r\nspin_lock(&zcache_rem_op_list_lock);\r\nif (list_empty(&zcache_rem_op_list)) {\r\nspin_unlock(&zcache_rem_op_list_lock);\r\ngoto out;\r\n}\r\nrem_op = list_first_entry(&zcache_rem_op_list,\r\nstruct ramster_remotify_hdr, list);\r\nlist_del_init(&rem_op->list);\r\nif (rem_op->op != RAMSTER_REMOTIFY_PERS_PUT)\r\nspin_unlock(&zcache_rem_op_list_lock);\r\nu = (union remotify_list_node *)rem_op;\r\nswitch (rem_op->op) {\r\ncase RAMSTER_REMOTIFY_EPH_PUT:\r\nBUG();\r\nzcache_remote_eph_put((struct zbud_hdr *)rem_op);\r\nbreak;\r\ncase RAMSTER_REMOTIFY_PERS_PUT:\r\nzcache_remote_pers_put((struct zv_hdr *)rem_op);\r\nbreak;\r\ncase RAMSTER_REMOTIFY_FLUSH_PAGE:\r\nzcache_remote_flush_page((struct flushlist_node *)u);\r\nbreak;\r\ncase RAMSTER_REMOTIFY_FLUSH_OBJ:\r\nzcache_remote_flush_object((struct flushlist_node *)u);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nout:\r\nreturn;\r\n}\r\nstatic void ramster_remotify_queue_delayed_work(unsigned long delay)\r\n{\r\nif (!queue_delayed_work(ramster_remotify_workqueue,\r\n&ramster_remotify_worker, delay))\r\npr_err("ramster_remotify: bad workqueue\n");\r\n}\r\nstatic void ramster_remotify_process(struct work_struct *work)\r\n{\r\nstatic bool remotify_in_progress;\r\nBUG_ON(irqs_disabled());\r\nif (remotify_in_progress)\r\nramster_remotify_queue_delayed_work(HZ);\r\nelse if (ramster_remote_target_nodenum != -1) {\r\nremotify_in_progress = true;\r\n#ifdef CONFIG_CLEANCACHE\r\nif (use_cleancache && ramster_eph_remotify_enable)\r\nzbud_remotify_pages(5000);\r\n#endif\r\n#ifdef CONFIG_FRONTSWAP\r\nif (use_frontswap && ramster_pers_remotify_enable)\r\nzcache_do_remotify_ops(500);\r\n#endif\r\nremotify_in_progress = false;\r\nramster_remotify_queue_delayed_work(HZ);\r\n}\r\n}\r\nstatic void ramster_remotify_init(void)\r\n{\r\nunsigned long n = 60UL;\r\nramster_remotify_workqueue =\r\ncreate_singlethread_workqueue("ramster_remotify");\r\nramster_remotify_queue_delayed_work(n * HZ);\r\n}\r\nstatic void zbud_init(void)\r\n{\r\nint i;\r\nINIT_LIST_HEAD(&zbud_buddied_list);\r\nzcache_zbud_buddied_count = 0;\r\nfor (i = 0; i < NCHUNKS; i++) {\r\nINIT_LIST_HEAD(&zbud_unbuddied[i].list);\r\nzbud_unbuddied[i].count = 0;\r\n}\r\n}\r\nstatic int zbud_show_unbuddied_list_counts(char *buf)\r\n{\r\nint i;\r\nchar *p = buf;\r\nfor (i = 0; i < NCHUNKS; i++)\r\np += sprintf(p, "%u ", zbud_unbuddied[i].count);\r\nreturn p - buf;\r\n}\r\nstatic int zbud_show_cumul_chunk_counts(char *buf)\r\n{\r\nunsigned long i, chunks = 0, total_chunks = 0, sum_total_chunks = 0;\r\nunsigned long total_chunks_lte_21 = 0, total_chunks_lte_32 = 0;\r\nunsigned long total_chunks_lte_42 = 0;\r\nchar *p = buf;\r\nfor (i = 0; i < NCHUNKS; i++) {\r\np += sprintf(p, "%lu ", zbud_cumul_chunk_counts[i]);\r\nchunks += zbud_cumul_chunk_counts[i];\r\ntotal_chunks += zbud_cumul_chunk_counts[i];\r\nsum_total_chunks += i * zbud_cumul_chunk_counts[i];\r\nif (i == 21)\r\ntotal_chunks_lte_21 = total_chunks;\r\nif (i == 32)\r\ntotal_chunks_lte_32 = total_chunks;\r\nif (i == 42)\r\ntotal_chunks_lte_42 = total_chunks;\r\n}\r\np += sprintf(p, "<=21:%lu <=32:%lu <=42:%lu, mean:%lu\n",\r\ntotal_chunks_lte_21, total_chunks_lte_32, total_chunks_lte_42,\r\nchunks == 0 ? 0 : sum_total_chunks / chunks);\r\nreturn p - buf;\r\n}\r\nstatic struct zv_hdr *zv_create(struct zcache_client *cli, uint32_t pool_id,\r\nstruct tmem_oid *oid, uint32_t index,\r\nvoid *cdata, unsigned clen)\r\n{\r\nstruct page *page;\r\nstruct zv_hdr *zv = NULL;\r\nuint32_t offset;\r\nint alloc_size = clen + sizeof(struct zv_hdr);\r\nint chunks = (alloc_size + (CHUNK_SIZE - 1)) >> CHUNK_SHIFT;\r\nint ret;\r\nBUG_ON(!irqs_disabled());\r\nBUG_ON(chunks >= NCHUNKS);\r\nret = xv_malloc(cli->xvpool, clen + sizeof(struct zv_hdr),\r\n&page, &offset, ZCACHE_GFP_MASK);\r\nif (unlikely(ret))\r\ngoto out;\r\natomic_inc(&zv_curr_dist_counts[chunks]);\r\natomic_inc(&zv_cumul_dist_counts[chunks]);\r\nzv = kmap_atomic(page) + offset;\r\nzv->index = index;\r\nzv->oid = *oid;\r\nzv->pool_id = pool_id;\r\nSET_SENTINEL(zv, ZVH);\r\nINIT_LIST_HEAD(&zv->rem_op.list);\r\nzv->client_id = get_client_id_from_client(cli);\r\nzv->rem_op.op = RAMSTER_REMOTIFY_PERS_PUT;\r\nif (zv->client_id == LOCAL_CLIENT) {\r\nspin_lock(&zcache_rem_op_list_lock);\r\nlist_add_tail(&zv->rem_op.list, &zcache_rem_op_list);\r\nspin_unlock(&zcache_rem_op_list_lock);\r\n}\r\nmemcpy((char *)zv + sizeof(struct zv_hdr), cdata, clen);\r\nkunmap_atomic(zv);\r\nout:\r\nreturn zv;\r\n}\r\nstatic struct zv_hdr *zv_alloc(struct tmem_pool *pool,\r\nstruct tmem_oid *oid, uint32_t index,\r\nunsigned clen)\r\n{\r\nstruct zcache_client *cli = pool->client;\r\nstruct page *page;\r\nstruct zv_hdr *zv = NULL;\r\nuint32_t offset;\r\nint ret;\r\nBUG_ON(!irqs_disabled());\r\nBUG_ON(!is_local_client(pool->client));\r\nret = xv_malloc(cli->xvpool, clen + sizeof(struct zv_hdr),\r\n&page, &offset, ZCACHE_GFP_MASK);\r\nif (unlikely(ret))\r\ngoto out;\r\nzv = kmap_atomic(page) + offset;\r\nSET_SENTINEL(zv, ZVH);\r\nINIT_LIST_HEAD(&zv->rem_op.list);\r\nzv->client_id = LOCAL_CLIENT;\r\nzv->rem_op.op = RAMSTER_INTRANSIT_PERS;\r\nzv->index = index;\r\nzv->oid = *oid;\r\nzv->pool_id = pool->pool_id;\r\nkunmap_atomic(zv);\r\nout:\r\nreturn zv;\r\n}\r\nstatic void zv_free(struct xv_pool *xvpool, struct zv_hdr *zv)\r\n{\r\nunsigned long flags;\r\nstruct page *page;\r\nuint32_t offset;\r\nuint16_t size = xv_get_object_size(zv);\r\nint chunks = (size + (CHUNK_SIZE - 1)) >> CHUNK_SHIFT;\r\nASSERT_SENTINEL(zv, ZVH);\r\nBUG_ON(chunks >= NCHUNKS);\r\natomic_dec(&zv_curr_dist_counts[chunks]);\r\nsize -= sizeof(*zv);\r\nspin_lock(&zcache_rem_op_list_lock);\r\nsize = xv_get_object_size(zv) - sizeof(*zv);\r\nBUG_ON(size == 0);\r\nINVERT_SENTINEL(zv, ZVH);\r\nif (!list_empty(&zv->rem_op.list))\r\nlist_del_init(&zv->rem_op.list);\r\nspin_unlock(&zcache_rem_op_list_lock);\r\npage = virt_to_page(zv);\r\noffset = (unsigned long)zv & ~PAGE_MASK;\r\nlocal_irq_save(flags);\r\nxv_free(xvpool, page, offset);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void zv_decompress(struct page *page, struct zv_hdr *zv)\r\n{\r\nsize_t clen = PAGE_SIZE;\r\nchar *to_va;\r\nunsigned size;\r\nint ret;\r\nASSERT_SENTINEL(zv, ZVH);\r\nsize = xv_get_object_size(zv) - sizeof(*zv);\r\nBUG_ON(size == 0);\r\nto_va = kmap_atomic(page);\r\nret = lzo1x_decompress_safe((char *)zv + sizeof(*zv),\r\nsize, to_va, &clen);\r\nkunmap_atomic(to_va);\r\nBUG_ON(ret != LZO_E_OK);\r\nBUG_ON(clen != PAGE_SIZE);\r\n}\r\nstatic void zv_copy_from_pampd(char *data, size_t *bufsize, struct zv_hdr *zv)\r\n{\r\nunsigned size;\r\nASSERT_SENTINEL(zv, ZVH);\r\nsize = xv_get_object_size(zv) - sizeof(*zv);\r\nBUG_ON(size == 0 || size > zv_max_page_size);\r\nBUG_ON(size > *bufsize);\r\nmemcpy(data, (char *)zv + sizeof(*zv), size);\r\n*bufsize = size;\r\n}\r\nstatic void zv_copy_to_pampd(struct zv_hdr *zv, char *data, size_t size)\r\n{\r\nunsigned zv_size;\r\nASSERT_SENTINEL(zv, ZVH);\r\nzv_size = xv_get_object_size(zv) - sizeof(*zv);\r\nBUG_ON(zv_size != size);\r\nBUG_ON(zv_size == 0 || zv_size > zv_max_page_size);\r\nmemcpy((char *)zv + sizeof(*zv), data, size);\r\n}\r\nstatic int zv_curr_dist_counts_show(char *buf)\r\n{\r\nunsigned long i, n, chunks = 0, sum_total_chunks = 0;\r\nchar *p = buf;\r\nfor (i = 0; i < NCHUNKS; i++) {\r\nn = atomic_read(&zv_curr_dist_counts[i]);\r\np += sprintf(p, "%lu ", n);\r\nchunks += n;\r\nsum_total_chunks += i * n;\r\n}\r\np += sprintf(p, "mean:%lu\n",\r\nchunks == 0 ? 0 : sum_total_chunks / chunks);\r\nreturn p - buf;\r\n}\r\nstatic int zv_cumul_dist_counts_show(char *buf)\r\n{\r\nunsigned long i, n, chunks = 0, sum_total_chunks = 0;\r\nchar *p = buf;\r\nfor (i = 0; i < NCHUNKS; i++) {\r\nn = atomic_read(&zv_cumul_dist_counts[i]);\r\np += sprintf(p, "%lu ", n);\r\nchunks += n;\r\nsum_total_chunks += i * n;\r\n}\r\np += sprintf(p, "mean:%lu\n",\r\nchunks == 0 ? 0 : sum_total_chunks / chunks);\r\nreturn p - buf;\r\n}\r\nstatic ssize_t zv_max_zsize_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", zv_max_zsize);\r\n}\r\nstatic ssize_t zv_max_zsize_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned long val;\r\nint err;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nerr = kstrtoul(buf, 10, &val);\r\nif (err || (val == 0) || (val > (PAGE_SIZE / 8) * 7))\r\nreturn -EINVAL;\r\nzv_max_zsize = val;\r\nreturn count;\r\n}\r\nstatic ssize_t zv_max_mean_zsize_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", zv_max_mean_zsize);\r\n}\r\nstatic ssize_t zv_max_mean_zsize_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned long val;\r\nint err;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nerr = kstrtoul(buf, 10, &val);\r\nif (err || (val == 0) || (val > (PAGE_SIZE / 8) * 7))\r\nreturn -EINVAL;\r\nzv_max_mean_zsize = val;\r\nreturn count;\r\n}\r\nstatic ssize_t zv_page_count_policy_percent_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", zv_page_count_policy_percent);\r\n}\r\nstatic ssize_t zv_page_count_policy_percent_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned long val;\r\nint err;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nerr = kstrtoul(buf, 10, &val);\r\nif (err || (val == 0) || (val > 150))\r\nreturn -EINVAL;\r\nzv_page_count_policy_percent = val;\r\nreturn count;\r\n}\r\nstatic struct tmem_pool *zcache_get_pool_by_id(uint16_t cli_id, uint16_t poolid)\r\n{\r\nstruct tmem_pool *pool = NULL;\r\nstruct zcache_client *cli = NULL;\r\nif (cli_id == LOCAL_CLIENT)\r\ncli = &zcache_host;\r\nelse {\r\nif (cli_id >= MAX_CLIENTS)\r\ngoto out;\r\ncli = &zcache_clients[cli_id];\r\nif (cli == NULL)\r\ngoto out;\r\natomic_inc(&cli->refcount);\r\n}\r\nif (poolid < MAX_POOLS_PER_CLIENT) {\r\npool = cli->tmem_pools[poolid];\r\nif (pool != NULL)\r\natomic_inc(&pool->refcount);\r\n}\r\nout:\r\nreturn pool;\r\n}\r\nstatic void zcache_put_pool(struct tmem_pool *pool)\r\n{\r\nstruct zcache_client *cli = NULL;\r\nif (pool == NULL)\r\nBUG();\r\ncli = pool->client;\r\natomic_dec(&pool->refcount);\r\natomic_dec(&cli->refcount);\r\n}\r\nint zcache_new_client(uint16_t cli_id)\r\n{\r\nstruct zcache_client *cli = NULL;\r\nint ret = -1;\r\nif (cli_id == LOCAL_CLIENT)\r\ncli = &zcache_host;\r\nelse if ((unsigned int)cli_id < MAX_CLIENTS)\r\ncli = &zcache_clients[cli_id];\r\nif (cli == NULL)\r\ngoto out;\r\nif (cli->allocated)\r\ngoto out;\r\ncli->allocated = 1;\r\n#ifdef CONFIG_FRONTSWAP\r\ncli->xvpool = xv_create_pool();\r\nif (cli->xvpool == NULL)\r\ngoto out;\r\n#endif\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic int zcache_do_preload(struct tmem_pool *pool)\r\n{\r\nstruct zcache_preload *kp;\r\nstruct tmem_objnode *objnode;\r\nstruct tmem_obj *obj;\r\nstruct flushlist_node *flnode;\r\nvoid *page;\r\nint ret = -ENOMEM;\r\nif (unlikely(zcache_objnode_cache == NULL))\r\ngoto out;\r\nif (unlikely(zcache_obj_cache == NULL))\r\ngoto out;\r\npreempt_disable();\r\nkp = &__get_cpu_var(zcache_preloads);\r\nwhile (kp->nr < ARRAY_SIZE(kp->objnodes)) {\r\npreempt_enable_no_resched();\r\nobjnode = kmem_cache_alloc(zcache_objnode_cache,\r\nZCACHE_GFP_MASK);\r\nif (unlikely(objnode == NULL)) {\r\nzcache_failed_alloc++;\r\ngoto out;\r\n}\r\npreempt_disable();\r\nkp = &__get_cpu_var(zcache_preloads);\r\nif (kp->nr < ARRAY_SIZE(kp->objnodes))\r\nkp->objnodes[kp->nr++] = objnode;\r\nelse\r\nkmem_cache_free(zcache_objnode_cache, objnode);\r\n}\r\npreempt_enable_no_resched();\r\nobj = kmem_cache_alloc(zcache_obj_cache, ZCACHE_GFP_MASK);\r\nif (unlikely(obj == NULL)) {\r\nzcache_failed_alloc++;\r\ngoto out;\r\n}\r\nflnode = kmem_cache_alloc(ramster_flnode_cache, ZCACHE_GFP_MASK);\r\nif (unlikely(flnode == NULL)) {\r\nzcache_failed_alloc++;\r\ngoto out;\r\n}\r\nif (is_ephemeral(pool)) {\r\npage = (void *)__get_free_page(ZCACHE_GFP_MASK);\r\nif (unlikely(page == NULL)) {\r\nzcache_failed_get_free_pages++;\r\nkmem_cache_free(zcache_obj_cache, obj);\r\nkmem_cache_free(ramster_flnode_cache, flnode);\r\ngoto out;\r\n}\r\n}\r\npreempt_disable();\r\nkp = &__get_cpu_var(zcache_preloads);\r\nif (kp->obj == NULL)\r\nkp->obj = obj;\r\nelse\r\nkmem_cache_free(zcache_obj_cache, obj);\r\nif (kp->flnode == NULL)\r\nkp->flnode = flnode;\r\nelse\r\nkmem_cache_free(ramster_flnode_cache, flnode);\r\nif (is_ephemeral(pool)) {\r\nif (kp->page == NULL)\r\nkp->page = page;\r\nelse\r\nfree_page((unsigned long)page);\r\n}\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic int ramster_do_preload_flnode_only(struct tmem_pool *pool)\r\n{\r\nstruct zcache_preload *kp;\r\nstruct flushlist_node *flnode;\r\nint ret = -ENOMEM;\r\nBUG_ON(!irqs_disabled());\r\nif (unlikely(ramster_flnode_cache == NULL))\r\nBUG();\r\nkp = &__get_cpu_var(zcache_preloads);\r\nflnode = kmem_cache_alloc(ramster_flnode_cache, GFP_ATOMIC);\r\nif (unlikely(flnode == NULL) && kp->flnode == NULL)\r\nBUG();\r\nelse if (kp->flnode == NULL)\r\nkp->flnode = flnode;\r\nelse\r\nkmem_cache_free(ramster_flnode_cache, flnode);\r\nreturn ret;\r\n}\r\nstatic void *zcache_get_free_page(void)\r\n{\r\nstruct zcache_preload *kp;\r\nvoid *page;\r\nkp = &__get_cpu_var(zcache_preloads);\r\npage = kp->page;\r\nBUG_ON(page == NULL);\r\nkp->page = NULL;\r\nreturn page;\r\n}\r\nstatic void zcache_free_page(void *p)\r\n{\r\nfree_page((unsigned long)p);\r\n}\r\nstatic struct tmem_objnode *zcache_objnode_alloc(struct tmem_pool *pool)\r\n{\r\nstruct tmem_objnode *objnode = NULL;\r\nunsigned long count;\r\nstruct zcache_preload *kp;\r\nkp = &__get_cpu_var(zcache_preloads);\r\nif (kp->nr <= 0)\r\ngoto out;\r\nobjnode = kp->objnodes[kp->nr - 1];\r\nBUG_ON(objnode == NULL);\r\nkp->objnodes[kp->nr - 1] = NULL;\r\nkp->nr--;\r\ncount = atomic_inc_return(&zcache_curr_objnode_count);\r\nif (count > zcache_curr_objnode_count_max)\r\nzcache_curr_objnode_count_max = count;\r\nout:\r\nreturn objnode;\r\n}\r\nstatic void zcache_objnode_free(struct tmem_objnode *objnode,\r\nstruct tmem_pool *pool)\r\n{\r\natomic_dec(&zcache_curr_objnode_count);\r\nBUG_ON(atomic_read(&zcache_curr_objnode_count) < 0);\r\nkmem_cache_free(zcache_objnode_cache, objnode);\r\n}\r\nstatic struct tmem_obj *zcache_obj_alloc(struct tmem_pool *pool)\r\n{\r\nstruct tmem_obj *obj = NULL;\r\nunsigned long count;\r\nstruct zcache_preload *kp;\r\nkp = &__get_cpu_var(zcache_preloads);\r\nobj = kp->obj;\r\nBUG_ON(obj == NULL);\r\nkp->obj = NULL;\r\ncount = atomic_inc_return(&zcache_curr_obj_count);\r\nif (count > zcache_curr_obj_count_max)\r\nzcache_curr_obj_count_max = count;\r\nreturn obj;\r\n}\r\nstatic void zcache_obj_free(struct tmem_obj *obj, struct tmem_pool *pool)\r\n{\r\natomic_dec(&zcache_curr_obj_count);\r\nBUG_ON(atomic_read(&zcache_curr_obj_count) < 0);\r\nkmem_cache_free(zcache_obj_cache, obj);\r\n}\r\nstatic struct flushlist_node *ramster_flnode_alloc(struct tmem_pool *pool)\r\n{\r\nstruct flushlist_node *flnode = NULL;\r\nstruct zcache_preload *kp;\r\nint count;\r\nkp = &__get_cpu_var(zcache_preloads);\r\nflnode = kp->flnode;\r\nBUG_ON(flnode == NULL);\r\nkp->flnode = NULL;\r\ncount = atomic_inc_return(&ramster_curr_flnode_count);\r\nif (count > ramster_curr_flnode_count_max)\r\nramster_curr_flnode_count_max = count;\r\nreturn flnode;\r\n}\r\nstatic void ramster_flnode_free(struct flushlist_node *flnode,\r\nstruct tmem_pool *pool)\r\n{\r\natomic_dec(&ramster_curr_flnode_count);\r\nBUG_ON(atomic_read(&ramster_curr_flnode_count) < 0);\r\nkmem_cache_free(ramster_flnode_cache, flnode);\r\n}\r\nstatic inline void dec_and_check(atomic_t *pvar)\r\n{\r\natomic_dec(pvar);\r\nWARN_ON_ONCE(atomic_read(pvar) < 0);\r\n}\r\nstatic int zcache_pampd_eph_create(char *data, size_t size, bool raw,\r\nstruct tmem_pool *pool, struct tmem_oid *oid,\r\nuint32_t index, void **pampd)\r\n{\r\nint ret = -1;\r\nvoid *cdata = data;\r\nsize_t clen = size;\r\nstruct zcache_client *cli = pool->client;\r\nuint16_t client_id = get_client_id_from_client(cli);\r\nstruct page *page = NULL;\r\nunsigned long count;\r\nif (!raw) {\r\npage = virt_to_page(data);\r\nret = zcache_compress(page, &cdata, &clen);\r\nif (ret == 0)\r\ngoto out;\r\nif (clen == 0 || clen > zbud_max_buddy_size()) {\r\nzcache_compress_poor++;\r\ngoto out;\r\n}\r\n}\r\n*pampd = (void *)zbud_create(client_id, pool->pool_id, oid,\r\nindex, page, cdata, clen);\r\nif (*pampd == NULL) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = 0;\r\ncount = atomic_inc_return(&zcache_curr_eph_pampd_count);\r\nif (count > zcache_curr_eph_pampd_count_max)\r\nzcache_curr_eph_pampd_count_max = count;\r\nif (client_id != LOCAL_CLIENT) {\r\ncount = atomic_inc_return(&ramster_foreign_eph_pampd_count);\r\nif (count > ramster_foreign_eph_pampd_count_max)\r\nramster_foreign_eph_pampd_count_max = count;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int zcache_pampd_pers_create(char *data, size_t size, bool raw,\r\nstruct tmem_pool *pool, struct tmem_oid *oid,\r\nuint32_t index, void **pampd)\r\n{\r\nint ret = -1;\r\nvoid *cdata = data;\r\nsize_t clen = size;\r\nstruct zcache_client *cli = pool->client;\r\nstruct page *page;\r\nunsigned long count;\r\nunsigned long zv_mean_zsize;\r\nstruct zv_hdr *zv;\r\nlong curr_pers_pampd_count;\r\nu64 total_zsize;\r\n#ifdef RAMSTER_TESTING\r\nstatic bool pampd_neg_warned;\r\n#endif\r\ncurr_pers_pampd_count = atomic_read(&zcache_curr_pers_pampd_count) -\r\natomic_read(&ramster_remote_pers_pages);\r\n#ifdef RAMSTER_TESTING\r\nif (!pampd_neg_warned) {\r\npr_warn("ramster: bad accounting for curr_pers_pampd_count\n");\r\npampd_neg_warned = true;\r\n}\r\n#endif\r\nif (curr_pers_pampd_count >\r\n(zv_page_count_policy_percent * totalram_pages) / 100) {\r\nzcache_policy_percent_exceeded++;\r\ngoto out;\r\n}\r\nif (raw)\r\ngoto ok_to_create;\r\npage = virt_to_page(data);\r\nif (zcache_compress(page, &cdata, &clen) == 0)\r\ngoto out;\r\nif (clen > zv_max_zsize) {\r\nzcache_compress_poor++;\r\ngoto out;\r\n}\r\nif ((clen > zv_max_mean_zsize) && (curr_pers_pampd_count > 0)) {\r\ntotal_zsize = xv_get_total_size_bytes(cli->xvpool);\r\nzv_mean_zsize = div_u64(total_zsize, curr_pers_pampd_count);\r\nif (zv_mean_zsize > zv_max_mean_zsize) {\r\nzcache_mean_compress_poor++;\r\ngoto out;\r\n}\r\n}\r\nok_to_create:\r\n*pampd = (void *)zv_create(cli, pool->pool_id, oid, index, cdata, clen);\r\nif (*pampd == NULL) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = 0;\r\ncount = atomic_inc_return(&zcache_curr_pers_pampd_count);\r\nif (count > zcache_curr_pers_pampd_count_max)\r\nzcache_curr_pers_pampd_count_max = count;\r\nif (is_local_client(cli))\r\ngoto out;\r\nzv = *(struct zv_hdr **)pampd;\r\ncount = atomic_inc_return(&ramster_foreign_pers_pampd_count);\r\nif (count > ramster_foreign_pers_pampd_count_max)\r\nramster_foreign_pers_pampd_count_max = count;\r\nout:\r\nreturn ret;\r\n}\r\nstatic void *zcache_pampd_create(char *data, size_t size, bool raw, int eph,\r\nstruct tmem_pool *pool, struct tmem_oid *oid,\r\nuint32_t index)\r\n{\r\nvoid *pampd = NULL;\r\nint ret;\r\nbool ephemeral;\r\nBUG_ON(preemptible());\r\nephemeral = (eph == 1) || ((eph == 0) && is_ephemeral(pool));\r\nif (ephemeral)\r\nret = zcache_pampd_eph_create(data, size, raw, pool,\r\noid, index, &pampd);\r\nelse\r\nret = zcache_pampd_pers_create(data, size, raw, pool,\r\noid, index, &pampd);\r\nreturn pampd;\r\n}\r\nstatic int zcache_pampd_get_data(char *data, size_t *bufsize, bool raw,\r\nvoid *pampd, struct tmem_pool *pool,\r\nstruct tmem_oid *oid, uint32_t index)\r\n{\r\nint ret = 0;\r\nBUG_ON(preemptible());\r\nBUG_ON(is_ephemeral(pool));\r\nBUG_ON(pampd_is_remote(pampd));\r\nif (raw)\r\nzv_copy_from_pampd(data, bufsize, pampd);\r\nelse\r\nzv_decompress(virt_to_page(data), pampd);\r\nreturn ret;\r\n}\r\nstatic int zcache_pampd_get_data_and_free(char *data, size_t *bufsize, bool raw,\r\nvoid *pampd, struct tmem_pool *pool,\r\nstruct tmem_oid *oid, uint32_t index)\r\n{\r\nint ret = 0;\r\nunsigned long flags;\r\nstruct zcache_client *cli = pool->client;\r\nBUG_ON(preemptible());\r\nBUG_ON(pampd_is_remote(pampd));\r\nif (is_ephemeral(pool)) {\r\nlocal_irq_save(flags);\r\nif (raw)\r\nzbud_copy_from_pampd(data, bufsize, pampd);\r\nelse\r\nret = zbud_decompress(virt_to_page(data), pampd);\r\nzbud_free_and_delist((struct zbud_hdr *)pampd);\r\nlocal_irq_restore(flags);\r\nif (!is_local_client(cli))\r\ndec_and_check(&ramster_foreign_eph_pampd_count);\r\ndec_and_check(&zcache_curr_eph_pampd_count);\r\n} else {\r\nif (is_local_client(cli))\r\nBUG();\r\nif (raw)\r\nzv_copy_from_pampd(data, bufsize, pampd);\r\nelse\r\nzv_decompress(virt_to_page(data), pampd);\r\nzv_free(cli->xvpool, pampd);\r\nif (!is_local_client(cli))\r\ndec_and_check(&ramster_foreign_pers_pampd_count);\r\ndec_and_check(&zcache_curr_pers_pampd_count);\r\nret = 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic bool zcache_pampd_is_remote(void *pampd)\r\n{\r\nreturn pampd_is_remote(pampd);\r\n}\r\nstatic void zcache_pampd_free(void *pampd, struct tmem_pool *pool,\r\nstruct tmem_oid *oid, uint32_t index, bool acct)\r\n{\r\nstruct zcache_client *cli = pool->client;\r\nbool eph = is_ephemeral(pool);\r\nstruct zv_hdr *zv;\r\nBUG_ON(preemptible());\r\nif (pampd_is_remote(pampd)) {\r\nWARN_ON(acct == false);\r\nif (oid == NULL) {\r\n} else if (eph) {\r\ndec_and_check(&zcache_curr_eph_pampd_count);\r\n} else if (pampd_is_intransit(pampd)) {\r\npampd = pampd_mask_intransit_and_remote(pampd);\r\ngoto local_pers;\r\n} else {\r\nstruct flushlist_node *flnode =\r\nramster_flnode_alloc(pool);\r\nflnode->xh.client_id = pampd_remote_node(pampd);\r\nflnode->xh.pool_id = pool->pool_id;\r\nflnode->xh.oid = *oid;\r\nflnode->xh.index = index;\r\nflnode->rem_op.op = RAMSTER_REMOTIFY_FLUSH_PAGE;\r\nspin_lock(&zcache_rem_op_list_lock);\r\nlist_add(&flnode->rem_op.list, &zcache_rem_op_list);\r\nspin_unlock(&zcache_rem_op_list_lock);\r\ndec_and_check(&zcache_curr_pers_pampd_count);\r\ndec_and_check(&ramster_remote_pers_pages);\r\n}\r\n} else if (eph) {\r\nzbud_free_and_delist((struct zbud_hdr *)pampd);\r\nif (!is_local_client(pool->client))\r\ndec_and_check(&ramster_foreign_eph_pampd_count);\r\nif (acct)\r\ndec_and_check(&zcache_curr_eph_pampd_count);\r\n} else {\r\nlocal_pers:\r\nzv = (struct zv_hdr *)pampd;\r\nif (!is_local_client(pool->client))\r\ndec_and_check(&ramster_foreign_pers_pampd_count);\r\nzv_free(cli->xvpool, zv);\r\nif (acct)\r\ndec_and_check(&zcache_curr_pers_pampd_count);\r\n}\r\n}\r\nstatic void zcache_pampd_free_obj(struct tmem_pool *pool,\r\nstruct tmem_obj *obj)\r\n{\r\nstruct flushlist_node *flnode;\r\nBUG_ON(preemptible());\r\nif (obj->extra == NULL)\r\nreturn;\r\nBUG_ON(!pampd_is_remote(obj->extra));\r\nflnode = ramster_flnode_alloc(pool);\r\nflnode->xh.client_id = pampd_remote_node(obj->extra);\r\nflnode->xh.pool_id = pool->pool_id;\r\nflnode->xh.oid = obj->oid;\r\nflnode->xh.index = FLUSH_ENTIRE_OBJECT;\r\nflnode->rem_op.op = RAMSTER_REMOTIFY_FLUSH_OBJ;\r\nspin_lock(&zcache_rem_op_list_lock);\r\nlist_add(&flnode->rem_op.list, &zcache_rem_op_list);\r\nspin_unlock(&zcache_rem_op_list_lock);\r\n}\r\nvoid zcache_pampd_new_obj(struct tmem_obj *obj)\r\n{\r\nobj->extra = NULL;\r\n}\r\nint zcache_pampd_replace_in_obj(void *new_pampd, struct tmem_obj *obj)\r\n{\r\nint ret = -1;\r\nif (new_pampd != NULL) {\r\nif (obj->extra == NULL)\r\nobj->extra = new_pampd;\r\nelse if (pampd_remote_node(new_pampd) !=\r\npampd_remote_node((void *)(obj->extra)))\r\nBUG();\r\nret = 0;\r\n}\r\nreturn ret;\r\n}\r\nint zcache_localify(int pool_id, struct tmem_oid *oidp,\r\nuint32_t index, char *data, size_t size,\r\nvoid *extra)\r\n{\r\nint ret = -ENOENT;\r\nunsigned long flags;\r\nstruct tmem_pool *pool;\r\nbool ephemeral, delete = false;\r\nsize_t clen = PAGE_SIZE;\r\nvoid *pampd, *saved_hb;\r\nstruct tmem_obj *obj;\r\npool = zcache_get_pool_by_id(LOCAL_CLIENT, pool_id);\r\nif (unlikely(pool == NULL))\r\ngoto out;\r\nephemeral = is_ephemeral(pool);\r\nlocal_irq_save(flags);\r\npampd = tmem_localify_get_pampd(pool, oidp, index, &obj, &saved_hb);\r\nif (pampd == NULL) {\r\n#ifdef RAMSTER_TESTING\r\npr_err("UNTESTED pampd==NULL in zcache_localify\n");\r\n#endif\r\nif (ephemeral)\r\nramster_remote_eph_pages_unsucc_get++;\r\nelse\r\nramster_remote_pers_pages_unsucc_get++;\r\nobj = NULL;\r\ngoto finish;\r\n} else if (unlikely(!pampd_is_remote(pampd))) {\r\n#ifdef RAMSTER_TESTING\r\npr_err("UNTESTED dup while waiting in zcache_localify\n");\r\n#endif\r\nif (ephemeral)\r\nramster_remote_eph_pages_unsucc_get++;\r\nelse\r\nramster_remote_pers_pages_unsucc_get++;\r\nobj = NULL;\r\npampd = NULL;\r\nret = -EEXIST;\r\ngoto finish;\r\n} else if (size == 0) {\r\npampd = NULL;\r\nif (ephemeral)\r\nramster_remote_eph_pages_unsucc_get++;\r\nelse\r\nBUG();\r\ndelete = true;\r\ngoto finish;\r\n}\r\nif (!ephemeral && pampd_is_intransit(pampd)) {\r\npampd = pampd_mask_intransit_and_remote(pampd);\r\nzv_copy_to_pampd(pampd, data, size);\r\n} else {\r\npampd = NULL;\r\nobj = NULL;\r\n}\r\nif (extra != NULL) {\r\nret = lzo1x_decompress_safe((char *)data, size,\r\n(char *)extra, &clen);\r\nBUG_ON(ret != LZO_E_OK);\r\nBUG_ON(clen != PAGE_SIZE);\r\n}\r\nif (ephemeral)\r\nramster_remote_eph_pages_succ_get++;\r\nelse\r\nramster_remote_pers_pages_succ_get++;\r\nret = 0;\r\nfinish:\r\ntmem_localify_finish(obj, index, pampd, saved_hb, delete);\r\nzcache_put_pool(pool);\r\nlocal_irq_restore(flags);\r\nout:\r\nreturn ret;\r\n}\r\nstatic void *zcache_pampd_repatriate_preload(void *pampd,\r\nstruct tmem_pool *pool,\r\nstruct tmem_oid *oid,\r\nuint32_t index,\r\nbool *intransit)\r\n{\r\nint clen = pampd_remote_size(pampd);\r\nvoid *ret_pampd = NULL;\r\nunsigned long flags;\r\nif (!pampd_is_remote(pampd))\r\nBUG();\r\nif (is_ephemeral(pool))\r\nBUG();\r\nif (pampd_is_intransit(pampd)) {\r\n*intransit = true;\r\ngoto out;\r\n}\r\n*intransit = false;\r\nlocal_irq_save(flags);\r\nret_pampd = (void *)zv_alloc(pool, oid, index, clen);\r\nif (ret_pampd != NULL) {\r\nret_pampd = pampd_mark_intransit(ret_pampd);\r\ndec_and_check(&ramster_remote_pers_pages);\r\n} else\r\nramster_pers_pages_remote_nomem++;\r\nlocal_irq_restore(flags);\r\nout:\r\nreturn ret_pampd;\r\n}\r\nstatic int zcache_pampd_repatriate(void *fake_pampd, void *real_pampd,\r\nstruct tmem_pool *pool,\r\nstruct tmem_oid *oid, uint32_t index,\r\nbool free, void *extra)\r\n{\r\nstruct tmem_xhandle xh;\r\nint ret;\r\nif (pampd_is_intransit(real_pampd))\r\nfree = true;\r\nxh = tmem_xhandle_fill(LOCAL_CLIENT, pool, oid, index);\r\nret = ramster_remote_async_get(&xh, free,\r\npampd_remote_node(fake_pampd),\r\npampd_remote_size(fake_pampd),\r\npampd_remote_cksum(fake_pampd),\r\nextra);\r\n#ifdef RAMSTER_TESTING\r\nif (ret != 0 && ret != -ENOENT)\r\npr_err("TESTING zcache_pampd_repatriate returns, ret=%d\n",\r\nret);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int zcache_compress(struct page *from, void **out_va, size_t *out_len)\r\n{\r\nint ret = 0;\r\nunsigned char *dmem = __get_cpu_var(zcache_dstmem);\r\nunsigned char *wmem = __get_cpu_var(zcache_workmem);\r\nchar *from_va;\r\nBUG_ON(!irqs_disabled());\r\nif (unlikely(dmem == NULL || wmem == NULL))\r\ngoto out;\r\nfrom_va = kmap_atomic(from);\r\nmb();\r\nret = lzo1x_1_compress(from_va, PAGE_SIZE, dmem, out_len, wmem);\r\nBUG_ON(ret != LZO_E_OK);\r\n*out_va = dmem;\r\nkunmap_atomic(from_va);\r\nret = 1;\r\nout:\r\nreturn ret;\r\n}\r\nstatic int zcache_cpu_notifier(struct notifier_block *nb,\r\nunsigned long action, void *pcpu)\r\n{\r\nint cpu = (long)pcpu;\r\nstruct zcache_preload *kp;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\nper_cpu(zcache_dstmem, cpu) = (void *)__get_free_pages(\r\nGFP_KERNEL | __GFP_REPEAT,\r\nLZO_DSTMEM_PAGE_ORDER),\r\nper_cpu(zcache_workmem, cpu) =\r\nkzalloc(LZO1X_MEM_COMPRESS,\r\nGFP_KERNEL | __GFP_REPEAT);\r\nper_cpu(zcache_remoteputmem, cpu) =\r\nkzalloc(PAGE_SIZE, GFP_KERNEL | __GFP_REPEAT);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_UP_CANCELED:\r\nkfree(per_cpu(zcache_remoteputmem, cpu));\r\nper_cpu(zcache_remoteputmem, cpu) = NULL;\r\nfree_pages((unsigned long)per_cpu(zcache_dstmem, cpu),\r\nLZO_DSTMEM_PAGE_ORDER);\r\nper_cpu(zcache_dstmem, cpu) = NULL;\r\nkfree(per_cpu(zcache_workmem, cpu));\r\nper_cpu(zcache_workmem, cpu) = NULL;\r\nkp = &per_cpu(zcache_preloads, cpu);\r\nwhile (kp->nr) {\r\nkmem_cache_free(zcache_objnode_cache,\r\nkp->objnodes[kp->nr - 1]);\r\nkp->objnodes[kp->nr - 1] = NULL;\r\nkp->nr--;\r\n}\r\nif (kp->obj) {\r\nkmem_cache_free(zcache_obj_cache, kp->obj);\r\nkp->obj = NULL;\r\n}\r\nif (kp->flnode) {\r\nkmem_cache_free(ramster_flnode_cache, kp->flnode);\r\nkp->flnode = NULL;\r\n}\r\nif (kp->page) {\r\nfree_page((unsigned long)kp->page);\r\nkp->page = NULL;\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic ssize_t ramster_manual_node_up_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nint i;\r\nchar *p = buf;\r\nfor (i = 0; i < MANUAL_NODES; i++)\r\nif (ramster_nodes_manual_up[i])\r\np += sprintf(p, "%d ", i);\r\np += sprintf(p, "\n");\r\nreturn p - buf;\r\n}\r\nstatic ssize_t ramster_manual_node_up_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr, const char *buf, size_t count)\r\n{\r\nint err;\r\nunsigned long node_num;\r\nerr = kstrtoul(buf, 10, &node_num);\r\nif (err) {\r\npr_err("ramster: bad strtoul?\n");\r\nreturn -EINVAL;\r\n}\r\nif (node_num >= MANUAL_NODES) {\r\npr_err("ramster: bad node_num=%lu?\n", node_num);\r\nreturn -EINVAL;\r\n}\r\nif (ramster_nodes_manual_up[node_num]) {\r\npr_err("ramster: node %d already up, ignoring\n",\r\n(int)node_num);\r\n} else {\r\nramster_nodes_manual_up[node_num] = true;\r\nr2net_hb_node_up_manual((int)node_num);\r\n}\r\nreturn count;\r\n}\r\nstatic ssize_t ramster_remote_target_nodenum_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nif (ramster_remote_target_nodenum == -1UL)\r\nreturn sprintf(buf, "unset\n");\r\nelse\r\nreturn sprintf(buf, "%d\n", ramster_remote_target_nodenum);\r\n}\r\nstatic ssize_t ramster_remote_target_nodenum_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr, const char *buf, size_t count)\r\n{\r\nint err;\r\nunsigned long node_num;\r\nerr = kstrtoul(buf, 10, &node_num);\r\nif (err) {\r\npr_err("ramster: bad strtoul?\n");\r\nreturn -EINVAL;\r\n} else if (node_num == -1UL) {\r\npr_err("ramster: disabling all remotification, "\r\n"data may still reside on remote nodes however\n");\r\nreturn -EINVAL;\r\n} else if (node_num >= MANUAL_NODES) {\r\npr_err("ramster: bad node_num=%lu?\n", node_num);\r\nreturn -EINVAL;\r\n} else if (!ramster_nodes_manual_up[node_num]) {\r\npr_err("ramster: node %d not up, ignoring setting "\r\n"of remotification target\n", (int)node_num);\r\n} else if (r2net_remote_target_node_set((int)node_num) >= 0) {\r\npr_info("ramster: node %d set as remotification target\n",\r\n(int)node_num);\r\nramster_remote_target_nodenum = (int)node_num;\r\n} else {\r\npr_err("ramster: bad num to node node_num=%d?\n",\r\n(int)node_num);\r\nreturn -EINVAL;\r\n}\r\nreturn count;\r\n}\r\nstatic int shrink_zcache_memory(struct shrinker *shrink,\r\nstruct shrink_control *sc)\r\n{\r\nint ret = -1;\r\nint nr = sc->nr_to_scan;\r\ngfp_t gfp_mask = sc->gfp_mask;\r\nif (nr >= 0) {\r\nif (!(gfp_mask & __GFP_FS))\r\ngoto out;\r\nzbud_evict_pages(nr);\r\n}\r\nret = (int)atomic_read(&zcache_zbud_curr_raw_pages);\r\nout:\r\nreturn ret;\r\n}\r\nint zcache_put(int cli_id, int pool_id, struct tmem_oid *oidp,\r\nuint32_t index, char *data, size_t size,\r\nbool raw, int ephemeral)\r\n{\r\nstruct tmem_pool *pool;\r\nint ret = -1;\r\nBUG_ON(!irqs_disabled());\r\npool = zcache_get_pool_by_id(cli_id, pool_id);\r\nif (unlikely(pool == NULL))\r\ngoto out;\r\nif (!zcache_freeze && zcache_do_preload(pool) == 0) {\r\nret = tmem_put(pool, oidp, index, data, size, raw, ephemeral);\r\nif (ret < 0) {\r\nif (is_ephemeral(pool))\r\nzcache_failed_eph_puts++;\r\nelse\r\nzcache_failed_pers_puts++;\r\n}\r\nzcache_put_pool(pool);\r\npreempt_enable_no_resched();\r\n} else {\r\nzcache_put_to_flush++;\r\nif (atomic_read(&pool->obj_count) > 0)\r\n(void)tmem_flush_page(pool, oidp, index);\r\nzcache_put_pool(pool);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nint zcache_get(int cli_id, int pool_id, struct tmem_oid *oidp,\r\nuint32_t index, char *data, size_t *sizep,\r\nbool raw, int get_and_free)\r\n{\r\nstruct tmem_pool *pool;\r\nint ret = -1;\r\nbool eph;\r\nif (!raw) {\r\nBUG_ON(irqs_disabled());\r\nBUG_ON(in_softirq());\r\n}\r\npool = zcache_get_pool_by_id(cli_id, pool_id);\r\neph = is_ephemeral(pool);\r\nif (likely(pool != NULL)) {\r\nif (atomic_read(&pool->obj_count) > 0)\r\nret = tmem_get(pool, oidp, index, data, sizep,\r\nraw, get_and_free);\r\nzcache_put_pool(pool);\r\n}\r\nWARN_ONCE((!eph && (ret != 0)), "zcache_get fails on persistent pool, "\r\n"bad things are very likely to happen soon\n");\r\n#ifdef RAMSTER_TESTING\r\nif (ret != 0 && ret != -1 && !(ret == -EINVAL && is_ephemeral(pool)))\r\npr_err("TESTING zcache_get tmem_get returns ret=%d\n", ret);\r\n#endif\r\nif (ret == -EAGAIN)\r\nBUG();\r\nreturn ret;\r\n}\r\nint zcache_flush(int cli_id, int pool_id,\r\nstruct tmem_oid *oidp, uint32_t index)\r\n{\r\nstruct tmem_pool *pool;\r\nint ret = -1;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nzcache_flush_total++;\r\npool = zcache_get_pool_by_id(cli_id, pool_id);\r\nramster_do_preload_flnode_only(pool);\r\nif (likely(pool != NULL)) {\r\nif (atomic_read(&pool->obj_count) > 0)\r\nret = tmem_flush_page(pool, oidp, index);\r\nzcache_put_pool(pool);\r\n}\r\nif (ret >= 0)\r\nzcache_flush_found++;\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nint zcache_flush_object(int cli_id, int pool_id, struct tmem_oid *oidp)\r\n{\r\nstruct tmem_pool *pool;\r\nint ret = -1;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nzcache_flobj_total++;\r\npool = zcache_get_pool_by_id(cli_id, pool_id);\r\nramster_do_preload_flnode_only(pool);\r\nif (likely(pool != NULL)) {\r\nif (atomic_read(&pool->obj_count) > 0)\r\nret = tmem_flush_object(pool, oidp);\r\nzcache_put_pool(pool);\r\n}\r\nif (ret >= 0)\r\nzcache_flobj_found++;\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nint zcache_client_destroy_pool(int cli_id, int pool_id)\r\n{\r\nstruct tmem_pool *pool = NULL;\r\nstruct zcache_client *cli = NULL;\r\nint ret = -1;\r\nif (pool_id < 0)\r\ngoto out;\r\nif (cli_id == LOCAL_CLIENT)\r\ncli = &zcache_host;\r\nelse if ((unsigned int)cli_id < MAX_CLIENTS)\r\ncli = &zcache_clients[cli_id];\r\nif (cli == NULL)\r\ngoto out;\r\natomic_inc(&cli->refcount);\r\npool = cli->tmem_pools[pool_id];\r\nif (pool == NULL)\r\ngoto out;\r\ncli->tmem_pools[pool_id] = NULL;\r\nwhile (atomic_read(&pool->refcount) != 0)\r\n;\r\natomic_dec(&cli->refcount);\r\nlocal_bh_disable();\r\nret = tmem_destroy_pool(pool);\r\nlocal_bh_enable();\r\nkfree(pool);\r\npr_info("ramster: destroyed pool id=%d cli_id=%d\n", pool_id, cli_id);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int zcache_destroy_pool(int pool_id)\r\n{\r\nreturn zcache_client_destroy_pool(LOCAL_CLIENT, pool_id);\r\n}\r\nint zcache_new_pool(uint16_t cli_id, uint32_t flags)\r\n{\r\nint poolid = -1;\r\nstruct tmem_pool *pool;\r\nstruct zcache_client *cli = NULL;\r\nif (cli_id == LOCAL_CLIENT)\r\ncli = &zcache_host;\r\nelse if ((unsigned int)cli_id < MAX_CLIENTS)\r\ncli = &zcache_clients[cli_id];\r\nif (cli == NULL)\r\ngoto out;\r\natomic_inc(&cli->refcount);\r\npool = kmalloc(sizeof(struct tmem_pool), GFP_ATOMIC);\r\nif (pool == NULL) {\r\npr_info("ramster: pool creation failed: out of memory\n");\r\ngoto out;\r\n}\r\nfor (poolid = 0; poolid < MAX_POOLS_PER_CLIENT; poolid++)\r\nif (cli->tmem_pools[poolid] == NULL)\r\nbreak;\r\nif (poolid >= MAX_POOLS_PER_CLIENT) {\r\npr_info("ramster: pool creation failed: max exceeded\n");\r\nkfree(pool);\r\npoolid = -1;\r\ngoto out;\r\n}\r\natomic_set(&pool->refcount, 0);\r\npool->client = cli;\r\npool->pool_id = poolid;\r\ntmem_new_pool(pool, flags);\r\ncli->tmem_pools[poolid] = pool;\r\nif (cli_id == LOCAL_CLIENT)\r\npr_info("ramster: created %s tmem pool, id=%d, local client\n",\r\nflags & TMEM_POOL_PERSIST ? "persistent" : "ephemeral",\r\npoolid);\r\nelse\r\npr_info("ramster: created %s tmem pool, id=%d, client=%d\n",\r\nflags & TMEM_POOL_PERSIST ? "persistent" : "ephemeral",\r\npoolid, cli_id);\r\nout:\r\nif (cli != NULL)\r\natomic_dec(&cli->refcount);\r\nreturn poolid;\r\n}\r\nstatic int zcache_local_new_pool(uint32_t flags)\r\n{\r\nreturn zcache_new_pool(LOCAL_CLIENT, flags);\r\n}\r\nint zcache_autocreate_pool(int cli_id, int pool_id, bool ephemeral)\r\n{\r\nstruct tmem_pool *pool;\r\nstruct zcache_client *cli = NULL;\r\nuint32_t flags = ephemeral ? 0 : TMEM_POOL_PERSIST;\r\nint ret = -1;\r\nif (cli_id == LOCAL_CLIENT)\r\ngoto out;\r\nif (pool_id >= MAX_POOLS_PER_CLIENT)\r\ngoto out;\r\nelse if ((unsigned int)cli_id < MAX_CLIENTS)\r\ncli = &zcache_clients[cli_id];\r\nif ((ephemeral && !use_cleancache) || (!ephemeral && !use_frontswap))\r\nBUG();\r\nif (!cli->allocated) {\r\nif (zcache_new_client(cli_id))\r\nBUG();\r\ncli = &zcache_clients[cli_id];\r\n}\r\natomic_inc(&cli->refcount);\r\npool = cli->tmem_pools[pool_id];\r\nif (pool != NULL) {\r\nif (pool->persistent && ephemeral) {\r\npr_err("zcache_autocreate_pool: type mismatch\n");\r\ngoto out;\r\n}\r\nret = 0;\r\ngoto out;\r\n}\r\npool = kmalloc(sizeof(struct tmem_pool), GFP_KERNEL);\r\nif (pool == NULL) {\r\npr_info("ramster: pool creation failed: out of memory\n");\r\ngoto out;\r\n}\r\natomic_set(&pool->refcount, 0);\r\npool->client = cli;\r\npool->pool_id = pool_id;\r\ntmem_new_pool(pool, flags);\r\ncli->tmem_pools[pool_id] = pool;\r\npr_info("ramster: AUTOcreated %s tmem poolid=%d, for remote client=%d\n",\r\nflags & TMEM_POOL_PERSIST ? "persistent" : "ephemeral",\r\npool_id, cli_id);\r\nret = 0;\r\nout:\r\nif (cli == NULL)\r\nBUG();\r\nif (cli != NULL)\r\natomic_dec(&cli->refcount);\r\nreturn ret;\r\n}\r\nstatic void zcache_cleancache_put_page(int pool_id,\r\nstruct cleancache_filekey key,\r\npgoff_t index, struct page *page)\r\n{\r\nu32 ind = (u32) index;\r\nstruct tmem_oid oid = *(struct tmem_oid *)&key;\r\n#ifdef __PG_WAS_ACTIVE\r\nif (!PageWasActive(page)) {\r\nzcache_nonactive_puts++;\r\nreturn;\r\n}\r\n#endif\r\nif (likely(ind == index)) {\r\nchar *kva = page_address(page);\r\n(void)zcache_put(LOCAL_CLIENT, pool_id, &oid, index,\r\nkva, PAGE_SIZE, 0, 1);\r\n}\r\n}\r\nstatic int zcache_cleancache_get_page(int pool_id,\r\nstruct cleancache_filekey key,\r\npgoff_t index, struct page *page)\r\n{\r\nu32 ind = (u32) index;\r\nstruct tmem_oid oid = *(struct tmem_oid *)&key;\r\nint ret = -1;\r\npreempt_disable();\r\nif (likely(ind == index)) {\r\nchar *kva = page_address(page);\r\nsize_t size = PAGE_SIZE;\r\nret = zcache_get(LOCAL_CLIENT, pool_id, &oid, index,\r\nkva, &size, 0, 0);\r\n#ifdef __PG_WAS_ACTIVE\r\nif (ret == 0)\r\nSetPageWasActive(page);\r\n#endif\r\n}\r\npreempt_enable();\r\nreturn ret;\r\n}\r\nstatic void zcache_cleancache_flush_page(int pool_id,\r\nstruct cleancache_filekey key,\r\npgoff_t index)\r\n{\r\nu32 ind = (u32) index;\r\nstruct tmem_oid oid = *(struct tmem_oid *)&key;\r\nif (likely(ind == index))\r\n(void)zcache_flush(LOCAL_CLIENT, pool_id, &oid, ind);\r\n}\r\nstatic void zcache_cleancache_flush_inode(int pool_id,\r\nstruct cleancache_filekey key)\r\n{\r\nstruct tmem_oid oid = *(struct tmem_oid *)&key;\r\n(void)zcache_flush_object(LOCAL_CLIENT, pool_id, &oid);\r\n}\r\nstatic void zcache_cleancache_flush_fs(int pool_id)\r\n{\r\nif (pool_id >= 0)\r\n(void)zcache_destroy_pool(pool_id);\r\n}\r\nstatic int zcache_cleancache_init_fs(size_t pagesize)\r\n{\r\nBUG_ON(sizeof(struct cleancache_filekey) !=\r\nsizeof(struct tmem_oid));\r\nBUG_ON(pagesize != PAGE_SIZE);\r\nreturn zcache_local_new_pool(0);\r\n}\r\nstatic int zcache_cleancache_init_shared_fs(char *uuid, size_t pagesize)\r\n{\r\nBUG_ON(sizeof(struct cleancache_filekey) !=\r\nsizeof(struct tmem_oid));\r\nBUG_ON(pagesize != PAGE_SIZE);\r\nreturn zcache_local_new_pool(0);\r\n}\r\nstruct cleancache_ops zcache_cleancache_register_ops(void)\r\n{\r\nstruct cleancache_ops old_ops =\r\ncleancache_register_ops(&zcache_cleancache_ops);\r\nreturn old_ops;\r\n}\r\nstatic inline struct tmem_oid oswiz(unsigned type, u32 ind)\r\n{\r\nstruct tmem_oid oid = { .oid = { 0 } };\r\noid.oid[0] = _oswiz(type, ind);\r\nreturn oid;\r\n}\r\nstatic int zcache_frontswap_store(unsigned type, pgoff_t offset,\r\nstruct page *page)\r\n{\r\nu64 ind64 = (u64)offset;\r\nu32 ind = (u32)offset;\r\nstruct tmem_oid oid = oswiz(type, ind);\r\nint ret = -1;\r\nunsigned long flags;\r\nchar *kva;\r\nBUG_ON(!PageLocked(page));\r\nif (likely(ind64 == ind)) {\r\nlocal_irq_save(flags);\r\nkva = page_address(page);\r\nret = zcache_put(LOCAL_CLIENT, zcache_frontswap_poolid,\r\n&oid, iswiz(ind), kva, PAGE_SIZE, 0, 0);\r\nlocal_irq_restore(flags);\r\n}\r\nreturn ret;\r\n}\r\nstatic int zcache_frontswap_load(unsigned type, pgoff_t offset,\r\nstruct page *page)\r\n{\r\nu64 ind64 = (u64)offset;\r\nu32 ind = (u32)offset;\r\nstruct tmem_oid oid = oswiz(type, ind);\r\nint ret = -1;\r\npreempt_disable();\r\nBUG_ON(!PageLocked(page));\r\nif (likely(ind64 == ind)) {\r\nchar *kva = page_address(page);\r\nsize_t size = PAGE_SIZE;\r\nret = zcache_get(LOCAL_CLIENT, zcache_frontswap_poolid,\r\n&oid, iswiz(ind), kva, &size, 0, -1);\r\n}\r\npreempt_enable();\r\nreturn ret;\r\n}\r\nstatic void zcache_frontswap_flush_page(unsigned type, pgoff_t offset)\r\n{\r\nu64 ind64 = (u64)offset;\r\nu32 ind = (u32)offset;\r\nstruct tmem_oid oid = oswiz(type, ind);\r\nif (likely(ind64 == ind))\r\n(void)zcache_flush(LOCAL_CLIENT, zcache_frontswap_poolid,\r\n&oid, iswiz(ind));\r\n}\r\nstatic void zcache_frontswap_flush_area(unsigned type)\r\n{\r\nstruct tmem_oid oid;\r\nint ind;\r\nfor (ind = SWIZ_MASK; ind >= 0; ind--) {\r\noid = oswiz(type, ind);\r\n(void)zcache_flush_object(LOCAL_CLIENT,\r\nzcache_frontswap_poolid, &oid);\r\n}\r\n}\r\nstatic void zcache_frontswap_init(unsigned ignored)\r\n{\r\nif (zcache_frontswap_poolid < 0)\r\nzcache_frontswap_poolid =\r\nzcache_local_new_pool(TMEM_POOL_PERSIST);\r\n}\r\nstruct frontswap_ops zcache_frontswap_register_ops(void)\r\n{\r\nstruct frontswap_ops old_ops =\r\nfrontswap_register_ops(&zcache_frontswap_ops);\r\nreturn old_ops;\r\n}\r\nstatic void frontswap_selfshrink(void)\r\n{\r\nstatic unsigned long cur_frontswap_pages;\r\nstatic unsigned long last_frontswap_pages;\r\nstatic unsigned long tgt_frontswap_pages;\r\nlast_frontswap_pages = cur_frontswap_pages;\r\ncur_frontswap_pages = frontswap_curr_pages();\r\nif (!cur_frontswap_pages ||\r\n(cur_frontswap_pages > last_frontswap_pages)) {\r\nfrontswap_inertia_counter = frontswap_inertia;\r\nreturn;\r\n}\r\nif (frontswap_inertia_counter && --frontswap_inertia_counter)\r\nreturn;\r\nif (cur_frontswap_pages <= frontswap_hysteresis)\r\ntgt_frontswap_pages = 0;\r\nelse\r\ntgt_frontswap_pages = cur_frontswap_pages -\r\n(cur_frontswap_pages / frontswap_hysteresis);\r\nfrontswap_shrink(tgt_frontswap_pages);\r\n}\r\nstatic int __init ramster_nofrontswap_selfshrink_setup(char *s)\r\n{\r\nuse_frontswap_selfshrink = false;\r\nreturn 1;\r\n}\r\nstatic void selfshrink_process(struct work_struct *work)\r\n{\r\nif (frontswap_selfshrinking && frontswap_enabled) {\r\nfrontswap_selfshrink();\r\nschedule_delayed_work(&selfshrink_worker,\r\nselfshrink_interval * HZ);\r\n}\r\n}\r\nstatic int __init ramster_selfshrink_init(void)\r\n{\r\nfrontswap_selfshrinking = ramster_enabled && use_frontswap_selfshrink;\r\nif (frontswap_selfshrinking)\r\npr_info("ramster: Initializing frontswap "\r\n"selfshrinking driver.\n");\r\nelse\r\nreturn -ENODEV;\r\nschedule_delayed_work(&selfshrink_worker, selfshrink_interval * HZ);\r\nreturn 0;\r\n}\r\nstatic int __init enable_ramster(char *s)\r\n{\r\nramster_enabled = 1;\r\nreturn 1;\r\n}\r\nstatic int __init no_cleancache(char *s)\r\n{\r\npr_info("INIT no_cleancache called\n");\r\nuse_cleancache = 0;\r\nreturn 1;\r\n}\r\nstatic int __init no_frontswap(char *s)\r\n{\r\npr_info("INIT no_frontswap called\n");\r\nuse_frontswap = 0;\r\nreturn 1;\r\n}\r\nstatic int __init zcache_init(void)\r\n{\r\nint ret = 0;\r\n#ifdef CONFIG_SYSFS\r\nret = sysfs_create_group(mm_kobj, &zcache_attr_group);\r\nret = sysfs_create_group(mm_kobj, &ramster_attr_group);\r\nif (ret) {\r\npr_err("ramster: can't create sysfs\n");\r\ngoto out;\r\n}\r\n#endif\r\n#if defined(CONFIG_CLEANCACHE) || defined(CONFIG_FRONTSWAP)\r\nif (ramster_enabled) {\r\nunsigned int cpu;\r\n(void)r2net_register_handlers();\r\ntmem_register_hostops(&zcache_hostops);\r\ntmem_register_pamops(&zcache_pamops);\r\nret = register_cpu_notifier(&zcache_cpu_notifier_block);\r\nif (ret) {\r\npr_err("ramster: can't register cpu notifier\n");\r\ngoto out;\r\n}\r\nfor_each_online_cpu(cpu) {\r\nvoid *pcpu = (void *)(long)cpu;\r\nzcache_cpu_notifier(&zcache_cpu_notifier_block,\r\nCPU_UP_PREPARE, pcpu);\r\n}\r\n}\r\nzcache_objnode_cache = kmem_cache_create("zcache_objnode",\r\nsizeof(struct tmem_objnode), 0, 0, NULL);\r\nzcache_obj_cache = kmem_cache_create("zcache_obj",\r\nsizeof(struct tmem_obj), 0, 0, NULL);\r\nramster_flnode_cache = kmem_cache_create("ramster_flnode",\r\nsizeof(struct flushlist_node), 0, 0, NULL);\r\n#endif\r\n#ifdef CONFIG_CLEANCACHE\r\npr_info("INIT ramster_enabled=%d use_cleancache=%d\n",\r\nramster_enabled, use_cleancache);\r\nif (ramster_enabled && use_cleancache) {\r\nstruct cleancache_ops old_ops;\r\nzbud_init();\r\nregister_shrinker(&zcache_shrinker);\r\nold_ops = zcache_cleancache_register_ops();\r\npr_info("ramster: cleancache enabled using kernel "\r\n"transcendent memory and compression buddies\n");\r\nif (old_ops.init_fs != NULL)\r\npr_warning("ramster: cleancache_ops overridden");\r\n}\r\n#endif\r\n#ifdef CONFIG_FRONTSWAP\r\npr_info("INIT ramster_enabled=%d use_frontswap=%d\n",\r\nramster_enabled, use_frontswap);\r\nif (ramster_enabled && use_frontswap) {\r\nstruct frontswap_ops old_ops;\r\nzcache_new_client(LOCAL_CLIENT);\r\nold_ops = zcache_frontswap_register_ops();\r\npr_info("ramster: frontswap enabled using kernel "\r\n"transcendent memory and xvmalloc\n");\r\nif (old_ops.init != NULL)\r\npr_warning("ramster: frontswap_ops overridden");\r\n}\r\nif (ramster_enabled && (use_frontswap || use_cleancache))\r\nramster_remotify_init();\r\n#endif\r\nout:\r\nreturn ret;\r\n}
