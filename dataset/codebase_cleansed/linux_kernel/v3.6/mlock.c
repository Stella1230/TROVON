int can_do_mlock(void)\r\n{\r\nif (capable(CAP_IPC_LOCK))\r\nreturn 1;\r\nif (rlimit(RLIMIT_MEMLOCK) != 0)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nvoid __clear_page_mlock(struct page *page)\r\n{\r\nVM_BUG_ON(!PageLocked(page));\r\nif (!page->mapping) {\r\nreturn;\r\n}\r\ndec_zone_page_state(page, NR_MLOCK);\r\ncount_vm_event(UNEVICTABLE_PGCLEARED);\r\nif (!isolate_lru_page(page)) {\r\nputback_lru_page(page);\r\n} else {\r\nif (PageUnevictable(page))\r\ncount_vm_event(UNEVICTABLE_PGSTRANDED);\r\n}\r\n}\r\nvoid mlock_vma_page(struct page *page)\r\n{\r\nBUG_ON(!PageLocked(page));\r\nif (!TestSetPageMlocked(page)) {\r\ninc_zone_page_state(page, NR_MLOCK);\r\ncount_vm_event(UNEVICTABLE_PGMLOCKED);\r\nif (!isolate_lru_page(page))\r\nputback_lru_page(page);\r\n}\r\n}\r\nvoid munlock_vma_page(struct page *page)\r\n{\r\nBUG_ON(!PageLocked(page));\r\nif (TestClearPageMlocked(page)) {\r\ndec_zone_page_state(page, NR_MLOCK);\r\nif (!isolate_lru_page(page)) {\r\nint ret = SWAP_AGAIN;\r\nif (page_mapcount(page) > 1)\r\nret = try_to_munlock(page);\r\nif (ret != SWAP_MLOCK)\r\ncount_vm_event(UNEVICTABLE_PGMUNLOCKED);\r\nputback_lru_page(page);\r\n} else {\r\nif (PageUnevictable(page))\r\ncount_vm_event(UNEVICTABLE_PGSTRANDED);\r\nelse\r\ncount_vm_event(UNEVICTABLE_PGMUNLOCKED);\r\n}\r\n}\r\n}\r\nstatic long __mlock_vma_pages_range(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end,\r\nint *nonblocking)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nunsigned long addr = start;\r\nint nr_pages = (end - start) / PAGE_SIZE;\r\nint gup_flags;\r\nVM_BUG_ON(start & ~PAGE_MASK);\r\nVM_BUG_ON(end & ~PAGE_MASK);\r\nVM_BUG_ON(start < vma->vm_start);\r\nVM_BUG_ON(end > vma->vm_end);\r\nVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\r\ngup_flags = FOLL_TOUCH | FOLL_MLOCK;\r\nif ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)\r\ngup_flags |= FOLL_WRITE;\r\nif (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC))\r\ngup_flags |= FOLL_FORCE;\r\nreturn __get_user_pages(current, mm, addr, nr_pages, gup_flags,\r\nNULL, NULL, nonblocking);\r\n}\r\nstatic int __mlock_posix_error_return(long retval)\r\n{\r\nif (retval == -EFAULT)\r\nretval = -ENOMEM;\r\nelse if (retval == -ENOMEM)\r\nretval = -EAGAIN;\r\nreturn retval;\r\n}\r\nlong mlock_vma_pages_range(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end)\r\n{\r\nint nr_pages = (end - start) / PAGE_SIZE;\r\nBUG_ON(!(vma->vm_flags & VM_LOCKED));\r\nif (vma->vm_flags & (VM_IO | VM_PFNMAP))\r\ngoto no_mlock;\r\nif (!((vma->vm_flags & (VM_DONTEXPAND | VM_RESERVED)) ||\r\nis_vm_hugetlb_page(vma) ||\r\nvma == get_gate_vma(current->mm))) {\r\n__mlock_vma_pages_range(vma, start, end, NULL);\r\nreturn 0;\r\n}\r\nmake_pages_present(start, end);\r\nno_mlock:\r\nvma->vm_flags &= ~VM_LOCKED;\r\nreturn nr_pages;\r\n}\r\nvoid munlock_vma_pages_range(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end)\r\n{\r\nunsigned long addr;\r\nlru_add_drain();\r\nvma->vm_flags &= ~VM_LOCKED;\r\nfor (addr = start; addr < end; addr += PAGE_SIZE) {\r\nstruct page *page;\r\npage = follow_page(vma, addr, FOLL_GET | FOLL_DUMP);\r\nif (page && !IS_ERR(page)) {\r\nlock_page(page);\r\nif (page->mapping)\r\nmunlock_vma_page(page);\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\ncond_resched();\r\n}\r\n}\r\nstatic int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,\r\nunsigned long start, unsigned long end, vm_flags_t newflags)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npgoff_t pgoff;\r\nint nr_pages;\r\nint ret = 0;\r\nint lock = !!(newflags & VM_LOCKED);\r\nif (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||\r\nis_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm))\r\ngoto out;\r\npgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\r\n*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,\r\nvma->vm_file, pgoff, vma_policy(vma));\r\nif (*prev) {\r\nvma = *prev;\r\ngoto success;\r\n}\r\nif (start != vma->vm_start) {\r\nret = split_vma(mm, vma, start, 1);\r\nif (ret)\r\ngoto out;\r\n}\r\nif (end != vma->vm_end) {\r\nret = split_vma(mm, vma, end, 0);\r\nif (ret)\r\ngoto out;\r\n}\r\nsuccess:\r\nnr_pages = (end - start) >> PAGE_SHIFT;\r\nif (!lock)\r\nnr_pages = -nr_pages;\r\nmm->locked_vm += nr_pages;\r\nif (lock)\r\nvma->vm_flags = newflags;\r\nelse\r\nmunlock_vma_pages_range(vma, start, end);\r\nout:\r\n*prev = vma;\r\nreturn ret;\r\n}\r\nstatic int do_mlock(unsigned long start, size_t len, int on)\r\n{\r\nunsigned long nstart, end, tmp;\r\nstruct vm_area_struct * vma, * prev;\r\nint error;\r\nVM_BUG_ON(start & ~PAGE_MASK);\r\nVM_BUG_ON(len != PAGE_ALIGN(len));\r\nend = start + len;\r\nif (end < start)\r\nreturn -EINVAL;\r\nif (end == start)\r\nreturn 0;\r\nvma = find_vma(current->mm, start);\r\nif (!vma || vma->vm_start > start)\r\nreturn -ENOMEM;\r\nprev = vma->vm_prev;\r\nif (start > vma->vm_start)\r\nprev = vma;\r\nfor (nstart = start ; ; ) {\r\nvm_flags_t newflags;\r\nnewflags = vma->vm_flags | VM_LOCKED;\r\nif (!on)\r\nnewflags &= ~VM_LOCKED;\r\ntmp = vma->vm_end;\r\nif (tmp > end)\r\ntmp = end;\r\nerror = mlock_fixup(vma, &prev, nstart, tmp, newflags);\r\nif (error)\r\nbreak;\r\nnstart = tmp;\r\nif (nstart < prev->vm_end)\r\nnstart = prev->vm_end;\r\nif (nstart >= end)\r\nbreak;\r\nvma = prev->vm_next;\r\nif (!vma || vma->vm_start != nstart) {\r\nerror = -ENOMEM;\r\nbreak;\r\n}\r\n}\r\nreturn error;\r\n}\r\nstatic int do_mlock_pages(unsigned long start, size_t len, int ignore_errors)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long end, nstart, nend;\r\nstruct vm_area_struct *vma = NULL;\r\nint locked = 0;\r\nint ret = 0;\r\nVM_BUG_ON(start & ~PAGE_MASK);\r\nVM_BUG_ON(len != PAGE_ALIGN(len));\r\nend = start + len;\r\nfor (nstart = start; nstart < end; nstart = nend) {\r\nif (!locked) {\r\nlocked = 1;\r\ndown_read(&mm->mmap_sem);\r\nvma = find_vma(mm, nstart);\r\n} else if (nstart >= vma->vm_end)\r\nvma = vma->vm_next;\r\nif (!vma || vma->vm_start >= end)\r\nbreak;\r\nnend = min(end, vma->vm_end);\r\nif (vma->vm_flags & (VM_IO | VM_PFNMAP))\r\ncontinue;\r\nif (nstart < vma->vm_start)\r\nnstart = vma->vm_start;\r\nret = __mlock_vma_pages_range(vma, nstart, nend, &locked);\r\nif (ret < 0) {\r\nif (ignore_errors) {\r\nret = 0;\r\ncontinue;\r\n}\r\nret = __mlock_posix_error_return(ret);\r\nbreak;\r\n}\r\nnend = nstart + ret * PAGE_SIZE;\r\nret = 0;\r\n}\r\nif (locked)\r\nup_read(&mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic int do_mlockall(int flags)\r\n{\r\nstruct vm_area_struct * vma, * prev = NULL;\r\nunsigned int def_flags = 0;\r\nif (flags & MCL_FUTURE)\r\ndef_flags = VM_LOCKED;\r\ncurrent->mm->def_flags = def_flags;\r\nif (flags == MCL_FUTURE)\r\ngoto out;\r\nfor (vma = current->mm->mmap; vma ; vma = prev->vm_next) {\r\nvm_flags_t newflags;\r\nnewflags = vma->vm_flags | VM_LOCKED;\r\nif (!(flags & MCL_CURRENT))\r\nnewflags &= ~VM_LOCKED;\r\nmlock_fixup(vma, &prev, vma->vm_start, vma->vm_end, newflags);\r\n}\r\nout:\r\nreturn 0;\r\n}\r\nint user_shm_lock(size_t size, struct user_struct *user)\r\n{\r\nunsigned long lock_limit, locked;\r\nint allowed = 0;\r\nlocked = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK);\r\nif (lock_limit == RLIM_INFINITY)\r\nallowed = 1;\r\nlock_limit >>= PAGE_SHIFT;\r\nspin_lock(&shmlock_user_lock);\r\nif (!allowed &&\r\nlocked + user->locked_shm > lock_limit && !capable(CAP_IPC_LOCK))\r\ngoto out;\r\nget_uid(user);\r\nuser->locked_shm += locked;\r\nallowed = 1;\r\nout:\r\nspin_unlock(&shmlock_user_lock);\r\nreturn allowed;\r\n}\r\nvoid user_shm_unlock(size_t size, struct user_struct *user)\r\n{\r\nspin_lock(&shmlock_user_lock);\r\nuser->locked_shm -= (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nspin_unlock(&shmlock_user_lock);\r\nfree_uid(user);\r\n}
