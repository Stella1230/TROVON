static void gfs2_ail_error(struct gfs2_glock *gl, const struct buffer_head *bh)\r\n{\r\nfs_err(gl->gl_sbd, "AIL buffer %p: blocknr %llu state 0x%08lx mapping %p page state 0x%lx\n",\r\nbh, (unsigned long long)bh->b_blocknr, bh->b_state,\r\nbh->b_page->mapping, bh->b_page->flags);\r\nfs_err(gl->gl_sbd, "AIL glock %u:%llu mapping %p\n",\r\ngl->gl_name.ln_type, gl->gl_name.ln_number,\r\ngfs2_glock2aspace(gl));\r\ngfs2_lm_withdraw(gl->gl_sbd, "AIL error\n");\r\n}\r\nstatic void __gfs2_ail_flush(struct gfs2_glock *gl, bool fsync)\r\n{\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nstruct list_head *head = &gl->gl_ail_list;\r\nstruct gfs2_bufdata *bd, *tmp;\r\nstruct buffer_head *bh;\r\nconst unsigned long b_state = (1UL << BH_Dirty)|(1UL << BH_Pinned)|(1UL << BH_Lock);\r\nsector_t blocknr;\r\ngfs2_log_lock(sdp);\r\nspin_lock(&sdp->sd_ail_lock);\r\nlist_for_each_entry_safe(bd, tmp, head, bd_ail_gl_list) {\r\nbh = bd->bd_bh;\r\nif (bh->b_state & b_state) {\r\nif (fsync)\r\ncontinue;\r\ngfs2_ail_error(gl, bh);\r\n}\r\nblocknr = bh->b_blocknr;\r\nbh->b_private = NULL;\r\ngfs2_remove_from_ail(bd);\r\nbd->bd_bh = NULL;\r\nbd->bd_blkno = blocknr;\r\ngfs2_trans_add_revoke(sdp, bd);\r\n}\r\nBUG_ON(!fsync && atomic_read(&gl->gl_ail_count));\r\nspin_unlock(&sdp->sd_ail_lock);\r\ngfs2_log_unlock(sdp);\r\n}\r\nstatic void gfs2_ail_empty_gl(struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nstruct gfs2_trans tr;\r\nmemset(&tr, 0, sizeof(tr));\r\ntr.tr_revokes = atomic_read(&gl->gl_ail_count);\r\nif (!tr.tr_revokes)\r\nreturn;\r\ntr.tr_reserved = 1 + gfs2_struct2blk(sdp, tr.tr_revokes, sizeof(u64));\r\ntr.tr_ip = (unsigned long)__builtin_return_address(0);\r\ngfs2_log_reserve(sdp, tr.tr_reserved);\r\nBUG_ON(current->journal_info);\r\ncurrent->journal_info = &tr;\r\n__gfs2_ail_flush(gl, 0);\r\ngfs2_trans_end(sdp);\r\ngfs2_log_flush(sdp, NULL);\r\n}\r\nvoid gfs2_ail_flush(struct gfs2_glock *gl, bool fsync)\r\n{\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nunsigned int revokes = atomic_read(&gl->gl_ail_count);\r\nint ret;\r\nif (!revokes)\r\nreturn;\r\nret = gfs2_trans_begin(sdp, 0, revokes);\r\nif (ret)\r\nreturn;\r\n__gfs2_ail_flush(gl, fsync);\r\ngfs2_trans_end(sdp);\r\ngfs2_log_flush(sdp, NULL);\r\n}\r\nstatic void rgrp_go_sync(struct gfs2_glock *gl)\r\n{\r\nstruct address_space *metamapping = gfs2_glock2aspace(gl);\r\nstruct gfs2_rgrpd *rgd;\r\nint error;\r\nif (!test_and_clear_bit(GLF_DIRTY, &gl->gl_flags))\r\nreturn;\r\nBUG_ON(gl->gl_state != LM_ST_EXCLUSIVE);\r\ngfs2_log_flush(gl->gl_sbd, gl);\r\nfilemap_fdatawrite(metamapping);\r\nerror = filemap_fdatawait(metamapping);\r\nmapping_set_error(metamapping, error);\r\ngfs2_ail_empty_gl(gl);\r\nspin_lock(&gl->gl_spin);\r\nrgd = gl->gl_object;\r\nif (rgd)\r\ngfs2_free_clones(rgd);\r\nspin_unlock(&gl->gl_spin);\r\n}\r\nstatic void rgrp_go_inval(struct gfs2_glock *gl, int flags)\r\n{\r\nstruct address_space *mapping = gfs2_glock2aspace(gl);\r\nBUG_ON(!(flags & DIO_METADATA));\r\ngfs2_assert_withdraw(gl->gl_sbd, !atomic_read(&gl->gl_ail_count));\r\ntruncate_inode_pages(mapping, 0);\r\nif (gl->gl_object) {\r\nstruct gfs2_rgrpd *rgd = (struct gfs2_rgrpd *)gl->gl_object;\r\nrgd->rd_flags &= ~GFS2_RDF_UPTODATE;\r\n}\r\n}\r\nstatic void inode_go_sync(struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_inode *ip = gl->gl_object;\r\nstruct address_space *metamapping = gfs2_glock2aspace(gl);\r\nint error;\r\nif (ip && !S_ISREG(ip->i_inode.i_mode))\r\nip = NULL;\r\nif (ip && test_and_clear_bit(GIF_SW_PAGED, &ip->i_flags))\r\nunmap_shared_mapping_range(ip->i_inode.i_mapping, 0, 0);\r\nif (!test_and_clear_bit(GLF_DIRTY, &gl->gl_flags))\r\nreturn;\r\nBUG_ON(gl->gl_state != LM_ST_EXCLUSIVE);\r\ngfs2_log_flush(gl->gl_sbd, gl);\r\nfilemap_fdatawrite(metamapping);\r\nif (ip) {\r\nstruct address_space *mapping = ip->i_inode.i_mapping;\r\nfilemap_fdatawrite(mapping);\r\nerror = filemap_fdatawait(mapping);\r\nmapping_set_error(mapping, error);\r\n}\r\nerror = filemap_fdatawait(metamapping);\r\nmapping_set_error(metamapping, error);\r\ngfs2_ail_empty_gl(gl);\r\nsmp_mb__before_clear_bit();\r\nclear_bit(GLF_DIRTY, &gl->gl_flags);\r\n}\r\nstatic void inode_go_inval(struct gfs2_glock *gl, int flags)\r\n{\r\nstruct gfs2_inode *ip = gl->gl_object;\r\ngfs2_assert_withdraw(gl->gl_sbd, !atomic_read(&gl->gl_ail_count));\r\nif (flags & DIO_METADATA) {\r\nstruct address_space *mapping = gfs2_glock2aspace(gl);\r\ntruncate_inode_pages(mapping, 0);\r\nif (ip) {\r\nset_bit(GIF_INVALID, &ip->i_flags);\r\nforget_all_cached_acls(&ip->i_inode);\r\ngfs2_dir_hash_inval(ip);\r\n}\r\n}\r\nif (ip == GFS2_I(gl->gl_sbd->sd_rindex)) {\r\ngfs2_log_flush(gl->gl_sbd, NULL);\r\ngl->gl_sbd->sd_rindex_uptodate = 0;\r\n}\r\nif (ip && S_ISREG(ip->i_inode.i_mode))\r\ntruncate_inode_pages(ip->i_inode.i_mapping, 0);\r\n}\r\nstatic int inode_go_demote_ok(const struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nstruct gfs2_holder *gh;\r\nif (sdp->sd_jindex == gl->gl_object || sdp->sd_rindex == gl->gl_object)\r\nreturn 0;\r\nif (!list_empty(&gl->gl_holders)) {\r\ngh = list_entry(gl->gl_holders.next, struct gfs2_holder, gh_list);\r\nif (gh->gh_list.next != &gl->gl_holders)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void gfs2_set_nlink(struct inode *inode, u32 nlink)\r\n{\r\nif ((inode->i_nlink != nlink) && (inode->i_nlink != 0)) {\r\nif (nlink == 0)\r\nclear_nlink(inode);\r\nelse\r\nset_nlink(inode, nlink);\r\n}\r\n}\r\nstatic int gfs2_dinode_in(struct gfs2_inode *ip, const void *buf)\r\n{\r\nconst struct gfs2_dinode *str = buf;\r\nstruct timespec atime;\r\nu16 height, depth;\r\nif (unlikely(ip->i_no_addr != be64_to_cpu(str->di_num.no_addr)))\r\ngoto corrupt;\r\nip->i_no_formal_ino = be64_to_cpu(str->di_num.no_formal_ino);\r\nip->i_inode.i_mode = be32_to_cpu(str->di_mode);\r\nip->i_inode.i_rdev = 0;\r\nswitch (ip->i_inode.i_mode & S_IFMT) {\r\ncase S_IFBLK:\r\ncase S_IFCHR:\r\nip->i_inode.i_rdev = MKDEV(be32_to_cpu(str->di_major),\r\nbe32_to_cpu(str->di_minor));\r\nbreak;\r\n};\r\nip->i_inode.i_uid = be32_to_cpu(str->di_uid);\r\nip->i_inode.i_gid = be32_to_cpu(str->di_gid);\r\ngfs2_set_nlink(&ip->i_inode, be32_to_cpu(str->di_nlink));\r\ni_size_write(&ip->i_inode, be64_to_cpu(str->di_size));\r\ngfs2_set_inode_blocks(&ip->i_inode, be64_to_cpu(str->di_blocks));\r\natime.tv_sec = be64_to_cpu(str->di_atime);\r\natime.tv_nsec = be32_to_cpu(str->di_atime_nsec);\r\nif (timespec_compare(&ip->i_inode.i_atime, &atime) < 0)\r\nip->i_inode.i_atime = atime;\r\nip->i_inode.i_mtime.tv_sec = be64_to_cpu(str->di_mtime);\r\nip->i_inode.i_mtime.tv_nsec = be32_to_cpu(str->di_mtime_nsec);\r\nip->i_inode.i_ctime.tv_sec = be64_to_cpu(str->di_ctime);\r\nip->i_inode.i_ctime.tv_nsec = be32_to_cpu(str->di_ctime_nsec);\r\nip->i_goal = be64_to_cpu(str->di_goal_meta);\r\nip->i_generation = be64_to_cpu(str->di_generation);\r\nip->i_diskflags = be32_to_cpu(str->di_flags);\r\nip->i_eattr = be64_to_cpu(str->di_eattr);\r\ngfs2_set_inode_flags(&ip->i_inode);\r\nheight = be16_to_cpu(str->di_height);\r\nif (unlikely(height > GFS2_MAX_META_HEIGHT))\r\ngoto corrupt;\r\nip->i_height = (u8)height;\r\ndepth = be16_to_cpu(str->di_depth);\r\nif (unlikely(depth > GFS2_DIR_MAX_DEPTH))\r\ngoto corrupt;\r\nip->i_depth = (u8)depth;\r\nip->i_entries = be32_to_cpu(str->di_entries);\r\nif (S_ISREG(ip->i_inode.i_mode))\r\ngfs2_set_aops(&ip->i_inode);\r\nreturn 0;\r\ncorrupt:\r\ngfs2_consist_inode(ip);\r\nreturn -EIO;\r\n}\r\nint gfs2_inode_refresh(struct gfs2_inode *ip)\r\n{\r\nstruct buffer_head *dibh;\r\nint error;\r\nerror = gfs2_meta_inode_buffer(ip, &dibh);\r\nif (error)\r\nreturn error;\r\nerror = gfs2_dinode_in(ip, dibh->b_data);\r\nbrelse(dibh);\r\nclear_bit(GIF_INVALID, &ip->i_flags);\r\nreturn error;\r\n}\r\nstatic int inode_go_lock(struct gfs2_holder *gh)\r\n{\r\nstruct gfs2_glock *gl = gh->gh_gl;\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nstruct gfs2_inode *ip = gl->gl_object;\r\nint error = 0;\r\nif (!ip || (gh->gh_flags & GL_SKIP))\r\nreturn 0;\r\nif (test_bit(GIF_INVALID, &ip->i_flags)) {\r\nerror = gfs2_inode_refresh(ip);\r\nif (error)\r\nreturn error;\r\n}\r\nif ((ip->i_diskflags & GFS2_DIF_TRUNC_IN_PROG) &&\r\n(gl->gl_state == LM_ST_EXCLUSIVE) &&\r\n(gh->gh_state == LM_ST_EXCLUSIVE)) {\r\nspin_lock(&sdp->sd_trunc_lock);\r\nif (list_empty(&ip->i_trunc_list))\r\nlist_add(&sdp->sd_trunc_list, &ip->i_trunc_list);\r\nspin_unlock(&sdp->sd_trunc_lock);\r\nwake_up(&sdp->sd_quota_wait);\r\nreturn 1;\r\n}\r\nreturn error;\r\n}\r\nstatic int inode_go_dump(struct seq_file *seq, const struct gfs2_glock *gl)\r\n{\r\nconst struct gfs2_inode *ip = gl->gl_object;\r\nif (ip == NULL)\r\nreturn 0;\r\ngfs2_print_dbg(seq, " I: n:%llu/%llu t:%u f:0x%02lx d:0x%08x s:%llu\n",\r\n(unsigned long long)ip->i_no_formal_ino,\r\n(unsigned long long)ip->i_no_addr,\r\nIF2DT(ip->i_inode.i_mode), ip->i_flags,\r\n(unsigned int)ip->i_diskflags,\r\n(unsigned long long)i_size_read(&ip->i_inode));\r\nreturn 0;\r\n}\r\nstatic void trans_go_sync(struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nif (gl->gl_state != LM_ST_UNLOCKED &&\r\ntest_bit(SDF_JOURNAL_LIVE, &sdp->sd_flags)) {\r\ngfs2_meta_syncfs(sdp);\r\ngfs2_log_shutdown(sdp);\r\n}\r\n}\r\nstatic int trans_go_xmote_bh(struct gfs2_glock *gl, struct gfs2_holder *gh)\r\n{\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nstruct gfs2_inode *ip = GFS2_I(sdp->sd_jdesc->jd_inode);\r\nstruct gfs2_glock *j_gl = ip->i_gl;\r\nstruct gfs2_log_header_host head;\r\nint error;\r\nif (test_bit(SDF_JOURNAL_LIVE, &sdp->sd_flags)) {\r\nj_gl->gl_ops->go_inval(j_gl, DIO_METADATA);\r\nerror = gfs2_find_jhead(sdp->sd_jdesc, &head);\r\nif (error)\r\ngfs2_consist(sdp);\r\nif (!(head.lh_flags & GFS2_LOG_HEAD_UNMOUNT))\r\ngfs2_consist(sdp);\r\nif (!test_bit(SDF_SHUTDOWN, &sdp->sd_flags)) {\r\nsdp->sd_log_sequence = head.lh_sequence + 1;\r\ngfs2_log_pointers_init(sdp, head.lh_blkno);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int trans_go_demote_ok(const struct gfs2_glock *gl)\r\n{\r\nreturn 0;\r\n}\r\nstatic void iopen_go_callback(struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_inode *ip = (struct gfs2_inode *)gl->gl_object;\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nif (sdp->sd_vfs->s_flags & MS_RDONLY)\r\nreturn;\r\nif (gl->gl_demote_state == LM_ST_UNLOCKED &&\r\ngl->gl_state == LM_ST_SHARED && ip) {\r\ngfs2_glock_hold(gl);\r\nif (queue_work(gfs2_delete_workqueue, &gl->gl_delete) == 0)\r\ngfs2_glock_put_nolock(gl);\r\n}\r\n}
