static inline struct anon_vma *anon_vma_alloc(void)\r\n{\r\nstruct anon_vma *anon_vma;\r\nanon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);\r\nif (anon_vma) {\r\natomic_set(&anon_vma->refcount, 1);\r\nanon_vma->root = anon_vma;\r\n}\r\nreturn anon_vma;\r\n}\r\nstatic inline void anon_vma_free(struct anon_vma *anon_vma)\r\n{\r\nVM_BUG_ON(atomic_read(&anon_vma->refcount));\r\nif (mutex_is_locked(&anon_vma->root->mutex)) {\r\nanon_vma_lock(anon_vma);\r\nanon_vma_unlock(anon_vma);\r\n}\r\nkmem_cache_free(anon_vma_cachep, anon_vma);\r\n}\r\nstatic inline struct anon_vma_chain *anon_vma_chain_alloc(gfp_t gfp)\r\n{\r\nreturn kmem_cache_alloc(anon_vma_chain_cachep, gfp);\r\n}\r\nstatic void anon_vma_chain_free(struct anon_vma_chain *anon_vma_chain)\r\n{\r\nkmem_cache_free(anon_vma_chain_cachep, anon_vma_chain);\r\n}\r\nstatic void anon_vma_chain_link(struct vm_area_struct *vma,\r\nstruct anon_vma_chain *avc,\r\nstruct anon_vma *anon_vma)\r\n{\r\navc->vma = vma;\r\navc->anon_vma = anon_vma;\r\nlist_add(&avc->same_vma, &vma->anon_vma_chain);\r\nlist_add_tail(&avc->same_anon_vma, &anon_vma->head);\r\n}\r\nint anon_vma_prepare(struct vm_area_struct *vma)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\nstruct anon_vma_chain *avc;\r\nmight_sleep();\r\nif (unlikely(!anon_vma)) {\r\nstruct mm_struct *mm = vma->vm_mm;\r\nstruct anon_vma *allocated;\r\navc = anon_vma_chain_alloc(GFP_KERNEL);\r\nif (!avc)\r\ngoto out_enomem;\r\nanon_vma = find_mergeable_anon_vma(vma);\r\nallocated = NULL;\r\nif (!anon_vma) {\r\nanon_vma = anon_vma_alloc();\r\nif (unlikely(!anon_vma))\r\ngoto out_enomem_free_avc;\r\nallocated = anon_vma;\r\n}\r\nanon_vma_lock(anon_vma);\r\nspin_lock(&mm->page_table_lock);\r\nif (likely(!vma->anon_vma)) {\r\nvma->anon_vma = anon_vma;\r\nanon_vma_chain_link(vma, avc, anon_vma);\r\nallocated = NULL;\r\navc = NULL;\r\n}\r\nspin_unlock(&mm->page_table_lock);\r\nanon_vma_unlock(anon_vma);\r\nif (unlikely(allocated))\r\nput_anon_vma(allocated);\r\nif (unlikely(avc))\r\nanon_vma_chain_free(avc);\r\n}\r\nreturn 0;\r\nout_enomem_free_avc:\r\nanon_vma_chain_free(avc);\r\nout_enomem:\r\nreturn -ENOMEM;\r\n}\r\nstatic inline struct anon_vma *lock_anon_vma_root(struct anon_vma *root, struct anon_vma *anon_vma)\r\n{\r\nstruct anon_vma *new_root = anon_vma->root;\r\nif (new_root != root) {\r\nif (WARN_ON_ONCE(root))\r\nmutex_unlock(&root->mutex);\r\nroot = new_root;\r\nmutex_lock(&root->mutex);\r\n}\r\nreturn root;\r\n}\r\nstatic inline void unlock_anon_vma_root(struct anon_vma *root)\r\n{\r\nif (root)\r\nmutex_unlock(&root->mutex);\r\n}\r\nint anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)\r\n{\r\nstruct anon_vma_chain *avc, *pavc;\r\nstruct anon_vma *root = NULL;\r\nlist_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {\r\nstruct anon_vma *anon_vma;\r\navc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);\r\nif (unlikely(!avc)) {\r\nunlock_anon_vma_root(root);\r\nroot = NULL;\r\navc = anon_vma_chain_alloc(GFP_KERNEL);\r\nif (!avc)\r\ngoto enomem_failure;\r\n}\r\nanon_vma = pavc->anon_vma;\r\nroot = lock_anon_vma_root(root, anon_vma);\r\nanon_vma_chain_link(dst, avc, anon_vma);\r\n}\r\nunlock_anon_vma_root(root);\r\nreturn 0;\r\nenomem_failure:\r\nunlink_anon_vmas(dst);\r\nreturn -ENOMEM;\r\n}\r\nvoid anon_vma_moveto_tail(struct vm_area_struct *dst)\r\n{\r\nstruct anon_vma_chain *pavc;\r\nstruct anon_vma *root = NULL;\r\nlist_for_each_entry_reverse(pavc, &dst->anon_vma_chain, same_vma) {\r\nstruct anon_vma *anon_vma = pavc->anon_vma;\r\nVM_BUG_ON(pavc->vma != dst);\r\nroot = lock_anon_vma_root(root, anon_vma);\r\nlist_del(&pavc->same_anon_vma);\r\nlist_add_tail(&pavc->same_anon_vma, &anon_vma->head);\r\n}\r\nunlock_anon_vma_root(root);\r\n}\r\nint anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\r\n{\r\nstruct anon_vma_chain *avc;\r\nstruct anon_vma *anon_vma;\r\nif (!pvma->anon_vma)\r\nreturn 0;\r\nif (anon_vma_clone(vma, pvma))\r\nreturn -ENOMEM;\r\nanon_vma = anon_vma_alloc();\r\nif (!anon_vma)\r\ngoto out_error;\r\navc = anon_vma_chain_alloc(GFP_KERNEL);\r\nif (!avc)\r\ngoto out_error_free_anon_vma;\r\nanon_vma->root = pvma->anon_vma->root;\r\nget_anon_vma(anon_vma->root);\r\nvma->anon_vma = anon_vma;\r\nanon_vma_lock(anon_vma);\r\nanon_vma_chain_link(vma, avc, anon_vma);\r\nanon_vma_unlock(anon_vma);\r\nreturn 0;\r\nout_error_free_anon_vma:\r\nput_anon_vma(anon_vma);\r\nout_error:\r\nunlink_anon_vmas(vma);\r\nreturn -ENOMEM;\r\n}\r\nvoid unlink_anon_vmas(struct vm_area_struct *vma)\r\n{\r\nstruct anon_vma_chain *avc, *next;\r\nstruct anon_vma *root = NULL;\r\nlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\r\nstruct anon_vma *anon_vma = avc->anon_vma;\r\nroot = lock_anon_vma_root(root, anon_vma);\r\nlist_del(&avc->same_anon_vma);\r\nif (list_empty(&anon_vma->head))\r\ncontinue;\r\nlist_del(&avc->same_vma);\r\nanon_vma_chain_free(avc);\r\n}\r\nunlock_anon_vma_root(root);\r\nlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\r\nstruct anon_vma *anon_vma = avc->anon_vma;\r\nput_anon_vma(anon_vma);\r\nlist_del(&avc->same_vma);\r\nanon_vma_chain_free(avc);\r\n}\r\n}\r\nstatic void anon_vma_ctor(void *data)\r\n{\r\nstruct anon_vma *anon_vma = data;\r\nmutex_init(&anon_vma->mutex);\r\natomic_set(&anon_vma->refcount, 0);\r\nINIT_LIST_HEAD(&anon_vma->head);\r\n}\r\nvoid __init anon_vma_init(void)\r\n{\r\nanon_vma_cachep = kmem_cache_create("anon_vma", sizeof(struct anon_vma),\r\n0, SLAB_DESTROY_BY_RCU|SLAB_PANIC, anon_vma_ctor);\r\nanon_vma_chain_cachep = KMEM_CACHE(anon_vma_chain, SLAB_PANIC);\r\n}\r\nstruct anon_vma *page_get_anon_vma(struct page *page)\r\n{\r\nstruct anon_vma *anon_vma = NULL;\r\nunsigned long anon_mapping;\r\nrcu_read_lock();\r\nanon_mapping = (unsigned long) ACCESS_ONCE(page->mapping);\r\nif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\r\ngoto out;\r\nif (!page_mapped(page))\r\ngoto out;\r\nanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\r\nif (!atomic_inc_not_zero(&anon_vma->refcount)) {\r\nanon_vma = NULL;\r\ngoto out;\r\n}\r\nif (!page_mapped(page)) {\r\nput_anon_vma(anon_vma);\r\nanon_vma = NULL;\r\n}\r\nout:\r\nrcu_read_unlock();\r\nreturn anon_vma;\r\n}\r\nstruct anon_vma *page_lock_anon_vma(struct page *page)\r\n{\r\nstruct anon_vma *anon_vma = NULL;\r\nstruct anon_vma *root_anon_vma;\r\nunsigned long anon_mapping;\r\nrcu_read_lock();\r\nanon_mapping = (unsigned long) ACCESS_ONCE(page->mapping);\r\nif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\r\ngoto out;\r\nif (!page_mapped(page))\r\ngoto out;\r\nanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\r\nroot_anon_vma = ACCESS_ONCE(anon_vma->root);\r\nif (mutex_trylock(&root_anon_vma->mutex)) {\r\nif (!page_mapped(page)) {\r\nmutex_unlock(&root_anon_vma->mutex);\r\nanon_vma = NULL;\r\n}\r\ngoto out;\r\n}\r\nif (!atomic_inc_not_zero(&anon_vma->refcount)) {\r\nanon_vma = NULL;\r\ngoto out;\r\n}\r\nif (!page_mapped(page)) {\r\nput_anon_vma(anon_vma);\r\nanon_vma = NULL;\r\ngoto out;\r\n}\r\nrcu_read_unlock();\r\nanon_vma_lock(anon_vma);\r\nif (atomic_dec_and_test(&anon_vma->refcount)) {\r\nanon_vma_unlock(anon_vma);\r\n__put_anon_vma(anon_vma);\r\nanon_vma = NULL;\r\n}\r\nreturn anon_vma;\r\nout:\r\nrcu_read_unlock();\r\nreturn anon_vma;\r\n}\r\nvoid page_unlock_anon_vma(struct anon_vma *anon_vma)\r\n{\r\nanon_vma_unlock(anon_vma);\r\n}\r\ninline unsigned long\r\nvma_address(struct page *page, struct vm_area_struct *vma)\r\n{\r\npgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);\r\nunsigned long address;\r\nif (unlikely(is_vm_hugetlb_page(vma)))\r\npgoff = page->index << huge_page_order(page_hstate(page));\r\naddress = vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);\r\nif (unlikely(address < vma->vm_start || address >= vma->vm_end)) {\r\nreturn -EFAULT;\r\n}\r\nreturn address;\r\n}\r\nunsigned long page_address_in_vma(struct page *page, struct vm_area_struct *vma)\r\n{\r\nif (PageAnon(page)) {\r\nstruct anon_vma *page__anon_vma = page_anon_vma(page);\r\nif (!vma->anon_vma || !page__anon_vma ||\r\nvma->anon_vma->root != page__anon_vma->root)\r\nreturn -EFAULT;\r\n} else if (page->mapping && !(vma->vm_flags & VM_NONLINEAR)) {\r\nif (!vma->vm_file ||\r\nvma->vm_file->f_mapping != page->mapping)\r\nreturn -EFAULT;\r\n} else\r\nreturn -EFAULT;\r\nreturn vma_address(page, vma);\r\n}\r\npte_t *__page_check_address(struct page *page, struct mm_struct *mm,\r\nunsigned long address, spinlock_t **ptlp, int sync)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\nif (unlikely(PageHuge(page))) {\r\npte = huge_pte_offset(mm, address);\r\nptl = &mm->page_table_lock;\r\ngoto check;\r\n}\r\npgd = pgd_offset(mm, address);\r\nif (!pgd_present(*pgd))\r\nreturn NULL;\r\npud = pud_offset(pgd, address);\r\nif (!pud_present(*pud))\r\nreturn NULL;\r\npmd = pmd_offset(pud, address);\r\nif (!pmd_present(*pmd))\r\nreturn NULL;\r\nif (pmd_trans_huge(*pmd))\r\nreturn NULL;\r\npte = pte_offset_map(pmd, address);\r\nif (!sync && !pte_present(*pte)) {\r\npte_unmap(pte);\r\nreturn NULL;\r\n}\r\nptl = pte_lockptr(mm, pmd);\r\ncheck:\r\nspin_lock(ptl);\r\nif (pte_present(*pte) && page_to_pfn(page) == pte_pfn(*pte)) {\r\n*ptlp = ptl;\r\nreturn pte;\r\n}\r\npte_unmap_unlock(pte, ptl);\r\nreturn NULL;\r\n}\r\nint page_mapped_in_vma(struct page *page, struct vm_area_struct *vma)\r\n{\r\nunsigned long address;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\naddress = vma_address(page, vma);\r\nif (address == -EFAULT)\r\nreturn 0;\r\npte = page_check_address(page, vma->vm_mm, address, &ptl, 1);\r\nif (!pte)\r\nreturn 0;\r\npte_unmap_unlock(pte, ptl);\r\nreturn 1;\r\n}\r\nint page_referenced_one(struct page *page, struct vm_area_struct *vma,\r\nunsigned long address, unsigned int *mapcount,\r\nunsigned long *vm_flags)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nint referenced = 0;\r\nif (unlikely(PageTransHuge(page))) {\r\npmd_t *pmd;\r\nspin_lock(&mm->page_table_lock);\r\npmd = page_check_address_pmd(page, mm, address,\r\nPAGE_CHECK_ADDRESS_PMD_FLAG);\r\nif (!pmd) {\r\nspin_unlock(&mm->page_table_lock);\r\ngoto out;\r\n}\r\nif (vma->vm_flags & VM_LOCKED) {\r\nspin_unlock(&mm->page_table_lock);\r\n*mapcount = 0;\r\n*vm_flags |= VM_LOCKED;\r\ngoto out;\r\n}\r\nif (pmdp_clear_flush_young_notify(vma, address, pmd))\r\nreferenced++;\r\nspin_unlock(&mm->page_table_lock);\r\n} else {\r\npte_t *pte;\r\nspinlock_t *ptl;\r\npte = page_check_address(page, mm, address, &ptl, 0);\r\nif (!pte)\r\ngoto out;\r\nif (vma->vm_flags & VM_LOCKED) {\r\npte_unmap_unlock(pte, ptl);\r\n*mapcount = 0;\r\n*vm_flags |= VM_LOCKED;\r\ngoto out;\r\n}\r\nif (ptep_clear_flush_young_notify(vma, address, pte)) {\r\nif (likely(!VM_SequentialReadHint(vma)))\r\nreferenced++;\r\n}\r\npte_unmap_unlock(pte, ptl);\r\n}\r\n(*mapcount)--;\r\nif (referenced)\r\n*vm_flags |= vma->vm_flags;\r\nout:\r\nreturn referenced;\r\n}\r\nstatic int page_referenced_anon(struct page *page,\r\nstruct mem_cgroup *memcg,\r\nunsigned long *vm_flags)\r\n{\r\nunsigned int mapcount;\r\nstruct anon_vma *anon_vma;\r\nstruct anon_vma_chain *avc;\r\nint referenced = 0;\r\nanon_vma = page_lock_anon_vma(page);\r\nif (!anon_vma)\r\nreturn referenced;\r\nmapcount = page_mapcount(page);\r\nlist_for_each_entry(avc, &anon_vma->head, same_anon_vma) {\r\nstruct vm_area_struct *vma = avc->vma;\r\nunsigned long address = vma_address(page, vma);\r\nif (address == -EFAULT)\r\ncontinue;\r\nif (memcg && !mm_match_cgroup(vma->vm_mm, memcg))\r\ncontinue;\r\nreferenced += page_referenced_one(page, vma, address,\r\n&mapcount, vm_flags);\r\nif (!mapcount)\r\nbreak;\r\n}\r\npage_unlock_anon_vma(anon_vma);\r\nreturn referenced;\r\n}\r\nstatic int page_referenced_file(struct page *page,\r\nstruct mem_cgroup *memcg,\r\nunsigned long *vm_flags)\r\n{\r\nunsigned int mapcount;\r\nstruct address_space *mapping = page->mapping;\r\npgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);\r\nstruct vm_area_struct *vma;\r\nstruct prio_tree_iter iter;\r\nint referenced = 0;\r\nBUG_ON(PageAnon(page));\r\nBUG_ON(!PageLocked(page));\r\nmutex_lock(&mapping->i_mmap_mutex);\r\nmapcount = page_mapcount(page);\r\nvma_prio_tree_foreach(vma, &iter, &mapping->i_mmap, pgoff, pgoff) {\r\nunsigned long address = vma_address(page, vma);\r\nif (address == -EFAULT)\r\ncontinue;\r\nif (memcg && !mm_match_cgroup(vma->vm_mm, memcg))\r\ncontinue;\r\nreferenced += page_referenced_one(page, vma, address,\r\n&mapcount, vm_flags);\r\nif (!mapcount)\r\nbreak;\r\n}\r\nmutex_unlock(&mapping->i_mmap_mutex);\r\nreturn referenced;\r\n}\r\nint page_referenced(struct page *page,\r\nint is_locked,\r\nstruct mem_cgroup *memcg,\r\nunsigned long *vm_flags)\r\n{\r\nint referenced = 0;\r\nint we_locked = 0;\r\n*vm_flags = 0;\r\nif (page_mapped(page) && page_rmapping(page)) {\r\nif (!is_locked && (!PageAnon(page) || PageKsm(page))) {\r\nwe_locked = trylock_page(page);\r\nif (!we_locked) {\r\nreferenced++;\r\ngoto out;\r\n}\r\n}\r\nif (unlikely(PageKsm(page)))\r\nreferenced += page_referenced_ksm(page, memcg,\r\nvm_flags);\r\nelse if (PageAnon(page))\r\nreferenced += page_referenced_anon(page, memcg,\r\nvm_flags);\r\nelse if (page->mapping)\r\nreferenced += page_referenced_file(page, memcg,\r\nvm_flags);\r\nif (we_locked)\r\nunlock_page(page);\r\nif (page_test_and_clear_young(page_to_pfn(page)))\r\nreferenced++;\r\n}\r\nout:\r\nreturn referenced;\r\n}\r\nstatic int page_mkclean_one(struct page *page, struct vm_area_struct *vma,\r\nunsigned long address)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\nint ret = 0;\r\npte = page_check_address(page, mm, address, &ptl, 1);\r\nif (!pte)\r\ngoto out;\r\nif (pte_dirty(*pte) || pte_write(*pte)) {\r\npte_t entry;\r\nflush_cache_page(vma, address, pte_pfn(*pte));\r\nentry = ptep_clear_flush_notify(vma, address, pte);\r\nentry = pte_wrprotect(entry);\r\nentry = pte_mkclean(entry);\r\nset_pte_at(mm, address, pte, entry);\r\nret = 1;\r\n}\r\npte_unmap_unlock(pte, ptl);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int page_mkclean_file(struct address_space *mapping, struct page *page)\r\n{\r\npgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);\r\nstruct vm_area_struct *vma;\r\nstruct prio_tree_iter iter;\r\nint ret = 0;\r\nBUG_ON(PageAnon(page));\r\nmutex_lock(&mapping->i_mmap_mutex);\r\nvma_prio_tree_foreach(vma, &iter, &mapping->i_mmap, pgoff, pgoff) {\r\nif (vma->vm_flags & VM_SHARED) {\r\nunsigned long address = vma_address(page, vma);\r\nif (address == -EFAULT)\r\ncontinue;\r\nret += page_mkclean_one(page, vma, address);\r\n}\r\n}\r\nmutex_unlock(&mapping->i_mmap_mutex);\r\nreturn ret;\r\n}\r\nint page_mkclean(struct page *page)\r\n{\r\nint ret = 0;\r\nBUG_ON(!PageLocked(page));\r\nif (page_mapped(page)) {\r\nstruct address_space *mapping = page_mapping(page);\r\nif (mapping) {\r\nret = page_mkclean_file(mapping, page);\r\nif (page_test_and_clear_dirty(page_to_pfn(page), 1))\r\nret = 1;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nvoid page_move_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\nVM_BUG_ON(!PageLocked(page));\r\nVM_BUG_ON(!anon_vma);\r\nVM_BUG_ON(page->index != linear_page_index(vma, address));\r\nanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\r\npage->mapping = (struct address_space *) anon_vma;\r\n}\r\nstatic void __page_set_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address, int exclusive)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\nBUG_ON(!anon_vma);\r\nif (PageAnon(page))\r\nreturn;\r\nif (!exclusive)\r\nanon_vma = anon_vma->root;\r\nanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\r\npage->mapping = (struct address_space *) anon_vma;\r\npage->index = linear_page_index(vma, address);\r\n}\r\nstatic void __page_check_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\n#ifdef CONFIG_DEBUG_VM\r\nBUG_ON(page_anon_vma(page)->root != vma->anon_vma->root);\r\nBUG_ON(page->index != linear_page_index(vma, address));\r\n#endif\r\n}\r\nvoid page_add_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\ndo_page_add_anon_rmap(page, vma, address, 0);\r\n}\r\nvoid do_page_add_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address, int exclusive)\r\n{\r\nint first = atomic_inc_and_test(&page->_mapcount);\r\nif (first) {\r\nif (!PageTransHuge(page))\r\n__inc_zone_page_state(page, NR_ANON_PAGES);\r\nelse\r\n__inc_zone_page_state(page,\r\nNR_ANON_TRANSPARENT_HUGEPAGES);\r\n}\r\nif (unlikely(PageKsm(page)))\r\nreturn;\r\nVM_BUG_ON(!PageLocked(page));\r\nif (first)\r\n__page_set_anon_rmap(page, vma, address, exclusive);\r\nelse\r\n__page_check_anon_rmap(page, vma, address);\r\n}\r\nvoid page_add_new_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\nVM_BUG_ON(address < vma->vm_start || address >= vma->vm_end);\r\nSetPageSwapBacked(page);\r\natomic_set(&page->_mapcount, 0);\r\nif (!PageTransHuge(page))\r\n__inc_zone_page_state(page, NR_ANON_PAGES);\r\nelse\r\n__inc_zone_page_state(page, NR_ANON_TRANSPARENT_HUGEPAGES);\r\n__page_set_anon_rmap(page, vma, address, 1);\r\nif (page_evictable(page, vma))\r\nlru_cache_add_lru(page, LRU_ACTIVE_ANON);\r\nelse\r\nadd_page_to_unevictable_list(page);\r\n}\r\nvoid page_add_file_rmap(struct page *page)\r\n{\r\nbool locked;\r\nunsigned long flags;\r\nmem_cgroup_begin_update_page_stat(page, &locked, &flags);\r\nif (atomic_inc_and_test(&page->_mapcount)) {\r\n__inc_zone_page_state(page, NR_FILE_MAPPED);\r\nmem_cgroup_inc_page_stat(page, MEMCG_NR_FILE_MAPPED);\r\n}\r\nmem_cgroup_end_update_page_stat(page, &locked, &flags);\r\n}\r\nvoid page_remove_rmap(struct page *page)\r\n{\r\nbool anon = PageAnon(page);\r\nbool locked;\r\nunsigned long flags;\r\nif (!anon)\r\nmem_cgroup_begin_update_page_stat(page, &locked, &flags);\r\nif (!atomic_add_negative(-1, &page->_mapcount))\r\ngoto out;\r\nif ((!anon || PageSwapCache(page)) &&\r\npage_test_and_clear_dirty(page_to_pfn(page), 1))\r\nset_page_dirty(page);\r\nif (unlikely(PageHuge(page)))\r\ngoto out;\r\nif (anon) {\r\nmem_cgroup_uncharge_page(page);\r\nif (!PageTransHuge(page))\r\n__dec_zone_page_state(page, NR_ANON_PAGES);\r\nelse\r\n__dec_zone_page_state(page,\r\nNR_ANON_TRANSPARENT_HUGEPAGES);\r\n} else {\r\n__dec_zone_page_state(page, NR_FILE_MAPPED);\r\nmem_cgroup_dec_page_stat(page, MEMCG_NR_FILE_MAPPED);\r\n}\r\nout:\r\nif (!anon)\r\nmem_cgroup_end_update_page_stat(page, &locked, &flags);\r\n}\r\nint try_to_unmap_one(struct page *page, struct vm_area_struct *vma,\r\nunsigned long address, enum ttu_flags flags)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npte_t *pte;\r\npte_t pteval;\r\nspinlock_t *ptl;\r\nint ret = SWAP_AGAIN;\r\npte = page_check_address(page, mm, address, &ptl, 0);\r\nif (!pte)\r\ngoto out;\r\nif (!(flags & TTU_IGNORE_MLOCK)) {\r\nif (vma->vm_flags & VM_LOCKED)\r\ngoto out_mlock;\r\nif (TTU_ACTION(flags) == TTU_MUNLOCK)\r\ngoto out_unmap;\r\n}\r\nif (!(flags & TTU_IGNORE_ACCESS)) {\r\nif (ptep_clear_flush_young_notify(vma, address, pte)) {\r\nret = SWAP_FAIL;\r\ngoto out_unmap;\r\n}\r\n}\r\nflush_cache_page(vma, address, page_to_pfn(page));\r\npteval = ptep_clear_flush_notify(vma, address, pte);\r\nif (pte_dirty(pteval))\r\nset_page_dirty(page);\r\nupdate_hiwater_rss(mm);\r\nif (PageHWPoison(page) && !(flags & TTU_IGNORE_HWPOISON)) {\r\nif (PageAnon(page))\r\ndec_mm_counter(mm, MM_ANONPAGES);\r\nelse\r\ndec_mm_counter(mm, MM_FILEPAGES);\r\nset_pte_at(mm, address, pte,\r\nswp_entry_to_pte(make_hwpoison_entry(page)));\r\n} else if (PageAnon(page)) {\r\nswp_entry_t entry = { .val = page_private(page) };\r\nif (PageSwapCache(page)) {\r\nif (swap_duplicate(entry) < 0) {\r\nset_pte_at(mm, address, pte, pteval);\r\nret = SWAP_FAIL;\r\ngoto out_unmap;\r\n}\r\nif (list_empty(&mm->mmlist)) {\r\nspin_lock(&mmlist_lock);\r\nif (list_empty(&mm->mmlist))\r\nlist_add(&mm->mmlist, &init_mm.mmlist);\r\nspin_unlock(&mmlist_lock);\r\n}\r\ndec_mm_counter(mm, MM_ANONPAGES);\r\ninc_mm_counter(mm, MM_SWAPENTS);\r\n} else if (IS_ENABLED(CONFIG_MIGRATION)) {\r\nBUG_ON(TTU_ACTION(flags) != TTU_MIGRATION);\r\nentry = make_migration_entry(page, pte_write(pteval));\r\n}\r\nset_pte_at(mm, address, pte, swp_entry_to_pte(entry));\r\nBUG_ON(pte_file(*pte));\r\n} else if (IS_ENABLED(CONFIG_MIGRATION) &&\r\n(TTU_ACTION(flags) == TTU_MIGRATION)) {\r\nswp_entry_t entry;\r\nentry = make_migration_entry(page, pte_write(pteval));\r\nset_pte_at(mm, address, pte, swp_entry_to_pte(entry));\r\n} else\r\ndec_mm_counter(mm, MM_FILEPAGES);\r\npage_remove_rmap(page);\r\npage_cache_release(page);\r\nout_unmap:\r\npte_unmap_unlock(pte, ptl);\r\nout:\r\nreturn ret;\r\nout_mlock:\r\npte_unmap_unlock(pte, ptl);\r\nif (down_read_trylock(&vma->vm_mm->mmap_sem)) {\r\nif (vma->vm_flags & VM_LOCKED) {\r\nmlock_vma_page(page);\r\nret = SWAP_MLOCK;\r\n}\r\nup_read(&vma->vm_mm->mmap_sem);\r\n}\r\nreturn ret;\r\n}\r\nstatic int try_to_unmap_cluster(unsigned long cursor, unsigned int *mapcount,\r\nstruct vm_area_struct *vma, struct page *check_page)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\npte_t pteval;\r\nspinlock_t *ptl;\r\nstruct page *page;\r\nunsigned long address;\r\nunsigned long end;\r\nint ret = SWAP_AGAIN;\r\nint locked_vma = 0;\r\naddress = (vma->vm_start + cursor) & CLUSTER_MASK;\r\nend = address + CLUSTER_SIZE;\r\nif (address < vma->vm_start)\r\naddress = vma->vm_start;\r\nif (end > vma->vm_end)\r\nend = vma->vm_end;\r\npgd = pgd_offset(mm, address);\r\nif (!pgd_present(*pgd))\r\nreturn ret;\r\npud = pud_offset(pgd, address);\r\nif (!pud_present(*pud))\r\nreturn ret;\r\npmd = pmd_offset(pud, address);\r\nif (!pmd_present(*pmd))\r\nreturn ret;\r\nif (down_read_trylock(&vma->vm_mm->mmap_sem)) {\r\nlocked_vma = (vma->vm_flags & VM_LOCKED);\r\nif (!locked_vma)\r\nup_read(&vma->vm_mm->mmap_sem);\r\n}\r\npte = pte_offset_map_lock(mm, pmd, address, &ptl);\r\nupdate_hiwater_rss(mm);\r\nfor (; address < end; pte++, address += PAGE_SIZE) {\r\nif (!pte_present(*pte))\r\ncontinue;\r\npage = vm_normal_page(vma, address, *pte);\r\nBUG_ON(!page || PageAnon(page));\r\nif (locked_vma) {\r\nmlock_vma_page(page);\r\nif (page == check_page)\r\nret = SWAP_MLOCK;\r\ncontinue;\r\n}\r\nif (ptep_clear_flush_young_notify(vma, address, pte))\r\ncontinue;\r\nflush_cache_page(vma, address, pte_pfn(*pte));\r\npteval = ptep_clear_flush_notify(vma, address, pte);\r\nif (page->index != linear_page_index(vma, address))\r\nset_pte_at(mm, address, pte, pgoff_to_pte(page->index));\r\nif (pte_dirty(pteval))\r\nset_page_dirty(page);\r\npage_remove_rmap(page);\r\npage_cache_release(page);\r\ndec_mm_counter(mm, MM_FILEPAGES);\r\n(*mapcount)--;\r\n}\r\npte_unmap_unlock(pte - 1, ptl);\r\nif (locked_vma)\r\nup_read(&vma->vm_mm->mmap_sem);\r\nreturn ret;\r\n}\r\nbool is_vma_temporary_stack(struct vm_area_struct *vma)\r\n{\r\nint maybe_stack = vma->vm_flags & (VM_GROWSDOWN | VM_GROWSUP);\r\nif (!maybe_stack)\r\nreturn false;\r\nif ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==\r\nVM_STACK_INCOMPLETE_SETUP)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic int try_to_unmap_anon(struct page *page, enum ttu_flags flags)\r\n{\r\nstruct anon_vma *anon_vma;\r\nstruct anon_vma_chain *avc;\r\nint ret = SWAP_AGAIN;\r\nanon_vma = page_lock_anon_vma(page);\r\nif (!anon_vma)\r\nreturn ret;\r\nlist_for_each_entry(avc, &anon_vma->head, same_anon_vma) {\r\nstruct vm_area_struct *vma = avc->vma;\r\nunsigned long address;\r\nif (IS_ENABLED(CONFIG_MIGRATION) && (flags & TTU_MIGRATION) &&\r\nis_vma_temporary_stack(vma))\r\ncontinue;\r\naddress = vma_address(page, vma);\r\nif (address == -EFAULT)\r\ncontinue;\r\nret = try_to_unmap_one(page, vma, address, flags);\r\nif (ret != SWAP_AGAIN || !page_mapped(page))\r\nbreak;\r\n}\r\npage_unlock_anon_vma(anon_vma);\r\nreturn ret;\r\n}\r\nstatic int try_to_unmap_file(struct page *page, enum ttu_flags flags)\r\n{\r\nstruct address_space *mapping = page->mapping;\r\npgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);\r\nstruct vm_area_struct *vma;\r\nstruct prio_tree_iter iter;\r\nint ret = SWAP_AGAIN;\r\nunsigned long cursor;\r\nunsigned long max_nl_cursor = 0;\r\nunsigned long max_nl_size = 0;\r\nunsigned int mapcount;\r\nmutex_lock(&mapping->i_mmap_mutex);\r\nvma_prio_tree_foreach(vma, &iter, &mapping->i_mmap, pgoff, pgoff) {\r\nunsigned long address = vma_address(page, vma);\r\nif (address == -EFAULT)\r\ncontinue;\r\nret = try_to_unmap_one(page, vma, address, flags);\r\nif (ret != SWAP_AGAIN || !page_mapped(page))\r\ngoto out;\r\n}\r\nif (list_empty(&mapping->i_mmap_nonlinear))\r\ngoto out;\r\nif (TTU_ACTION(flags) == TTU_MUNLOCK)\r\ngoto out;\r\nlist_for_each_entry(vma, &mapping->i_mmap_nonlinear,\r\nshared.vm_set.list) {\r\ncursor = (unsigned long) vma->vm_private_data;\r\nif (cursor > max_nl_cursor)\r\nmax_nl_cursor = cursor;\r\ncursor = vma->vm_end - vma->vm_start;\r\nif (cursor > max_nl_size)\r\nmax_nl_size = cursor;\r\n}\r\nif (max_nl_size == 0) {\r\nret = SWAP_FAIL;\r\ngoto out;\r\n}\r\nmapcount = page_mapcount(page);\r\nif (!mapcount)\r\ngoto out;\r\ncond_resched();\r\nmax_nl_size = (max_nl_size + CLUSTER_SIZE - 1) & CLUSTER_MASK;\r\nif (max_nl_cursor == 0)\r\nmax_nl_cursor = CLUSTER_SIZE;\r\ndo {\r\nlist_for_each_entry(vma, &mapping->i_mmap_nonlinear,\r\nshared.vm_set.list) {\r\ncursor = (unsigned long) vma->vm_private_data;\r\nwhile ( cursor < max_nl_cursor &&\r\ncursor < vma->vm_end - vma->vm_start) {\r\nif (try_to_unmap_cluster(cursor, &mapcount,\r\nvma, page) == SWAP_MLOCK)\r\nret = SWAP_MLOCK;\r\ncursor += CLUSTER_SIZE;\r\nvma->vm_private_data = (void *) cursor;\r\nif ((int)mapcount <= 0)\r\ngoto out;\r\n}\r\nvma->vm_private_data = (void *) max_nl_cursor;\r\n}\r\ncond_resched();\r\nmax_nl_cursor += CLUSTER_SIZE;\r\n} while (max_nl_cursor <= max_nl_size);\r\nlist_for_each_entry(vma, &mapping->i_mmap_nonlinear, shared.vm_set.list)\r\nvma->vm_private_data = NULL;\r\nout:\r\nmutex_unlock(&mapping->i_mmap_mutex);\r\nreturn ret;\r\n}\r\nint try_to_unmap(struct page *page, enum ttu_flags flags)\r\n{\r\nint ret;\r\nBUG_ON(!PageLocked(page));\r\nVM_BUG_ON(!PageHuge(page) && PageTransHuge(page));\r\nif (unlikely(PageKsm(page)))\r\nret = try_to_unmap_ksm(page, flags);\r\nelse if (PageAnon(page))\r\nret = try_to_unmap_anon(page, flags);\r\nelse\r\nret = try_to_unmap_file(page, flags);\r\nif (ret != SWAP_MLOCK && !page_mapped(page))\r\nret = SWAP_SUCCESS;\r\nreturn ret;\r\n}\r\nint try_to_munlock(struct page *page)\r\n{\r\nVM_BUG_ON(!PageLocked(page) || PageLRU(page));\r\nif (unlikely(PageKsm(page)))\r\nreturn try_to_unmap_ksm(page, TTU_MUNLOCK);\r\nelse if (PageAnon(page))\r\nreturn try_to_unmap_anon(page, TTU_MUNLOCK);\r\nelse\r\nreturn try_to_unmap_file(page, TTU_MUNLOCK);\r\n}\r\nvoid __put_anon_vma(struct anon_vma *anon_vma)\r\n{\r\nstruct anon_vma *root = anon_vma->root;\r\nif (root != anon_vma && atomic_dec_and_test(&root->refcount))\r\nanon_vma_free(root);\r\nanon_vma_free(anon_vma);\r\n}\r\nstatic int rmap_walk_anon(struct page *page, int (*rmap_one)(struct page *,\r\nstruct vm_area_struct *, unsigned long, void *), void *arg)\r\n{\r\nstruct anon_vma *anon_vma;\r\nstruct anon_vma_chain *avc;\r\nint ret = SWAP_AGAIN;\r\nanon_vma = page_anon_vma(page);\r\nif (!anon_vma)\r\nreturn ret;\r\nanon_vma_lock(anon_vma);\r\nlist_for_each_entry(avc, &anon_vma->head, same_anon_vma) {\r\nstruct vm_area_struct *vma = avc->vma;\r\nunsigned long address = vma_address(page, vma);\r\nif (address == -EFAULT)\r\ncontinue;\r\nret = rmap_one(page, vma, address, arg);\r\nif (ret != SWAP_AGAIN)\r\nbreak;\r\n}\r\nanon_vma_unlock(anon_vma);\r\nreturn ret;\r\n}\r\nstatic int rmap_walk_file(struct page *page, int (*rmap_one)(struct page *,\r\nstruct vm_area_struct *, unsigned long, void *), void *arg)\r\n{\r\nstruct address_space *mapping = page->mapping;\r\npgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);\r\nstruct vm_area_struct *vma;\r\nstruct prio_tree_iter iter;\r\nint ret = SWAP_AGAIN;\r\nif (!mapping)\r\nreturn ret;\r\nmutex_lock(&mapping->i_mmap_mutex);\r\nvma_prio_tree_foreach(vma, &iter, &mapping->i_mmap, pgoff, pgoff) {\r\nunsigned long address = vma_address(page, vma);\r\nif (address == -EFAULT)\r\ncontinue;\r\nret = rmap_one(page, vma, address, arg);\r\nif (ret != SWAP_AGAIN)\r\nbreak;\r\n}\r\nmutex_unlock(&mapping->i_mmap_mutex);\r\nreturn ret;\r\n}\r\nint rmap_walk(struct page *page, int (*rmap_one)(struct page *,\r\nstruct vm_area_struct *, unsigned long, void *), void *arg)\r\n{\r\nVM_BUG_ON(!PageLocked(page));\r\nif (unlikely(PageKsm(page)))\r\nreturn rmap_walk_ksm(page, rmap_one, arg);\r\nelse if (PageAnon(page))\r\nreturn rmap_walk_anon(page, rmap_one, arg);\r\nelse\r\nreturn rmap_walk_file(page, rmap_one, arg);\r\n}\r\nstatic void __hugepage_set_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address, int exclusive)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\nBUG_ON(!anon_vma);\r\nif (PageAnon(page))\r\nreturn;\r\nif (!exclusive)\r\nanon_vma = anon_vma->root;\r\nanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\r\npage->mapping = (struct address_space *) anon_vma;\r\npage->index = linear_page_index(vma, address);\r\n}\r\nvoid hugepage_add_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\nint first;\r\nBUG_ON(!PageLocked(page));\r\nBUG_ON(!anon_vma);\r\nfirst = atomic_inc_and_test(&page->_mapcount);\r\nif (first)\r\n__hugepage_set_anon_rmap(page, vma, address, 0);\r\n}\r\nvoid hugepage_add_new_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\nBUG_ON(address < vma->vm_start || address >= vma->vm_end);\r\natomic_set(&page->_mapcount, 0);\r\n__hugepage_set_anon_rmap(page, vma, address, 1);\r\n}
