static void wl_tree_add(struct ubi_wl_entry *e, struct rb_root *root)\r\n{\r\nstruct rb_node **p, *parent = NULL;\r\np = &root->rb_node;\r\nwhile (*p) {\r\nstruct ubi_wl_entry *e1;\r\nparent = *p;\r\ne1 = rb_entry(parent, struct ubi_wl_entry, u.rb);\r\nif (e->ec < e1->ec)\r\np = &(*p)->rb_left;\r\nelse if (e->ec > e1->ec)\r\np = &(*p)->rb_right;\r\nelse {\r\nubi_assert(e->pnum != e1->pnum);\r\nif (e->pnum < e1->pnum)\r\np = &(*p)->rb_left;\r\nelse\r\np = &(*p)->rb_right;\r\n}\r\n}\r\nrb_link_node(&e->u.rb, parent, p);\r\nrb_insert_color(&e->u.rb, root);\r\n}\r\nstatic int do_work(struct ubi_device *ubi)\r\n{\r\nint err;\r\nstruct ubi_work *wrk;\r\ncond_resched();\r\ndown_read(&ubi->work_sem);\r\nspin_lock(&ubi->wl_lock);\r\nif (list_empty(&ubi->works)) {\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->work_sem);\r\nreturn 0;\r\n}\r\nwrk = list_entry(ubi->works.next, struct ubi_work, list);\r\nlist_del(&wrk->list);\r\nubi->works_count -= 1;\r\nubi_assert(ubi->works_count >= 0);\r\nspin_unlock(&ubi->wl_lock);\r\nerr = wrk->func(ubi, wrk, 0);\r\nif (err)\r\nubi_err("work failed with error code %d", err);\r\nup_read(&ubi->work_sem);\r\nreturn err;\r\n}\r\nstatic int produce_free_peb(struct ubi_device *ubi)\r\n{\r\nint err;\r\nspin_lock(&ubi->wl_lock);\r\nwhile (!ubi->free.rb_node) {\r\nspin_unlock(&ubi->wl_lock);\r\ndbg_wl("do one work synchronously");\r\nerr = do_work(ubi);\r\nif (err)\r\nreturn err;\r\nspin_lock(&ubi->wl_lock);\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nreturn 0;\r\n}\r\nstatic int in_wl_tree(struct ubi_wl_entry *e, struct rb_root *root)\r\n{\r\nstruct rb_node *p;\r\np = root->rb_node;\r\nwhile (p) {\r\nstruct ubi_wl_entry *e1;\r\ne1 = rb_entry(p, struct ubi_wl_entry, u.rb);\r\nif (e->pnum == e1->pnum) {\r\nubi_assert(e == e1);\r\nreturn 1;\r\n}\r\nif (e->ec < e1->ec)\r\np = p->rb_left;\r\nelse if (e->ec > e1->ec)\r\np = p->rb_right;\r\nelse {\r\nubi_assert(e->pnum != e1->pnum);\r\nif (e->pnum < e1->pnum)\r\np = p->rb_left;\r\nelse\r\np = p->rb_right;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void prot_queue_add(struct ubi_device *ubi, struct ubi_wl_entry *e)\r\n{\r\nint pq_tail = ubi->pq_head - 1;\r\nif (pq_tail < 0)\r\npq_tail = UBI_PROT_QUEUE_LEN - 1;\r\nubi_assert(pq_tail >= 0 && pq_tail < UBI_PROT_QUEUE_LEN);\r\nlist_add_tail(&e->u.list, &ubi->pq[pq_tail]);\r\ndbg_wl("added PEB %d EC %d to the protection queue", e->pnum, e->ec);\r\n}\r\nstatic struct ubi_wl_entry *find_wl_entry(struct rb_root *root, int diff)\r\n{\r\nstruct rb_node *p;\r\nstruct ubi_wl_entry *e;\r\nint max;\r\ne = rb_entry(rb_first(root), struct ubi_wl_entry, u.rb);\r\nmax = e->ec + diff;\r\np = root->rb_node;\r\nwhile (p) {\r\nstruct ubi_wl_entry *e1;\r\ne1 = rb_entry(p, struct ubi_wl_entry, u.rb);\r\nif (e1->ec >= max)\r\np = p->rb_left;\r\nelse {\r\np = p->rb_right;\r\ne = e1;\r\n}\r\n}\r\nreturn e;\r\n}\r\nint ubi_wl_get_peb(struct ubi_device *ubi)\r\n{\r\nint err;\r\nstruct ubi_wl_entry *e, *first, *last;\r\nretry:\r\nspin_lock(&ubi->wl_lock);\r\nif (!ubi->free.rb_node) {\r\nif (ubi->works_count == 0) {\r\nubi_assert(list_empty(&ubi->works));\r\nubi_err("no free eraseblocks");\r\nspin_unlock(&ubi->wl_lock);\r\nreturn -ENOSPC;\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nerr = produce_free_peb(ubi);\r\nif (err < 0)\r\nreturn err;\r\ngoto retry;\r\n}\r\nfirst = rb_entry(rb_first(&ubi->free), struct ubi_wl_entry, u.rb);\r\nlast = rb_entry(rb_last(&ubi->free), struct ubi_wl_entry, u.rb);\r\nif (last->ec - first->ec < WL_FREE_MAX_DIFF)\r\ne = rb_entry(ubi->free.rb_node, struct ubi_wl_entry, u.rb);\r\nelse\r\ne = find_wl_entry(&ubi->free, WL_FREE_MAX_DIFF/2);\r\nself_check_in_wl_tree(ubi, e, &ubi->free);\r\nrb_erase(&e->u.rb, &ubi->free);\r\ndbg_wl("PEB %d EC %d", e->pnum, e->ec);\r\nprot_queue_add(ubi, e);\r\nspin_unlock(&ubi->wl_lock);\r\nerr = ubi_self_check_all_ff(ubi, e->pnum, ubi->vid_hdr_aloffset,\r\nubi->peb_size - ubi->vid_hdr_aloffset);\r\nif (err) {\r\nubi_err("new PEB %d does not contain all 0xFF bytes", e->pnum);\r\nreturn err;\r\n}\r\nreturn e->pnum;\r\n}\r\nstatic int prot_queue_del(struct ubi_device *ubi, int pnum)\r\n{\r\nstruct ubi_wl_entry *e;\r\ne = ubi->lookuptbl[pnum];\r\nif (!e)\r\nreturn -ENODEV;\r\nif (self_check_in_pq(ubi, e))\r\nreturn -ENODEV;\r\nlist_del(&e->u.list);\r\ndbg_wl("deleted PEB %d from the protection queue", e->pnum);\r\nreturn 0;\r\n}\r\nstatic int sync_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\r\nint torture)\r\n{\r\nint err;\r\nstruct ubi_ec_hdr *ec_hdr;\r\nunsigned long long ec = e->ec;\r\ndbg_wl("erase PEB %d, old EC %llu", e->pnum, ec);\r\nerr = self_check_ec(ubi, e->pnum, e->ec);\r\nif (err)\r\nreturn -EINVAL;\r\nec_hdr = kzalloc(ubi->ec_hdr_alsize, GFP_NOFS);\r\nif (!ec_hdr)\r\nreturn -ENOMEM;\r\nerr = ubi_io_sync_erase(ubi, e->pnum, torture);\r\nif (err < 0)\r\ngoto out_free;\r\nec += err;\r\nif (ec > UBI_MAX_ERASECOUNTER) {\r\nubi_err("erase counter overflow at PEB %d, EC %llu",\r\ne->pnum, ec);\r\nerr = -EINVAL;\r\ngoto out_free;\r\n}\r\ndbg_wl("erased PEB %d, new EC %llu", e->pnum, ec);\r\nec_hdr->ec = cpu_to_be64(ec);\r\nerr = ubi_io_write_ec_hdr(ubi, e->pnum, ec_hdr);\r\nif (err)\r\ngoto out_free;\r\ne->ec = ec;\r\nspin_lock(&ubi->wl_lock);\r\nif (e->ec > ubi->max_ec)\r\nubi->max_ec = e->ec;\r\nspin_unlock(&ubi->wl_lock);\r\nout_free:\r\nkfree(ec_hdr);\r\nreturn err;\r\n}\r\nstatic void serve_prot_queue(struct ubi_device *ubi)\r\n{\r\nstruct ubi_wl_entry *e, *tmp;\r\nint count;\r\nrepeat:\r\ncount = 0;\r\nspin_lock(&ubi->wl_lock);\r\nlist_for_each_entry_safe(e, tmp, &ubi->pq[ubi->pq_head], u.list) {\r\ndbg_wl("PEB %d EC %d protection over, move to used tree",\r\ne->pnum, e->ec);\r\nlist_del(&e->u.list);\r\nwl_tree_add(e, &ubi->used);\r\nif (count++ > 32) {\r\nspin_unlock(&ubi->wl_lock);\r\ncond_resched();\r\ngoto repeat;\r\n}\r\n}\r\nubi->pq_head += 1;\r\nif (ubi->pq_head == UBI_PROT_QUEUE_LEN)\r\nubi->pq_head = 0;\r\nubi_assert(ubi->pq_head >= 0 && ubi->pq_head < UBI_PROT_QUEUE_LEN);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nstatic void schedule_ubi_work(struct ubi_device *ubi, struct ubi_work *wrk)\r\n{\r\nspin_lock(&ubi->wl_lock);\r\nlist_add_tail(&wrk->list, &ubi->works);\r\nubi_assert(ubi->works_count >= 0);\r\nubi->works_count += 1;\r\nif (ubi->thread_enabled && !ubi_dbg_is_bgt_disabled(ubi))\r\nwake_up_process(ubi->bgt_thread);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nstatic int schedule_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\r\nint vol_id, int lnum, int torture)\r\n{\r\nstruct ubi_work *wl_wrk;\r\ndbg_wl("schedule erasure of PEB %d, EC %d, torture %d",\r\ne->pnum, e->ec, torture);\r\nwl_wrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\r\nif (!wl_wrk)\r\nreturn -ENOMEM;\r\nwl_wrk->func = &erase_worker;\r\nwl_wrk->e = e;\r\nwl_wrk->vol_id = vol_id;\r\nwl_wrk->lnum = lnum;\r\nwl_wrk->torture = torture;\r\nschedule_ubi_work(ubi, wl_wrk);\r\nreturn 0;\r\n}\r\nstatic int wear_leveling_worker(struct ubi_device *ubi, struct ubi_work *wrk,\r\nint cancel)\r\n{\r\nint err, scrubbing = 0, torture = 0, protect = 0, erroneous = 0;\r\nint vol_id = -1, uninitialized_var(lnum);\r\nstruct ubi_wl_entry *e1, *e2;\r\nstruct ubi_vid_hdr *vid_hdr;\r\nkfree(wrk);\r\nif (cancel)\r\nreturn 0;\r\nvid_hdr = ubi_zalloc_vid_hdr(ubi, GFP_NOFS);\r\nif (!vid_hdr)\r\nreturn -ENOMEM;\r\nmutex_lock(&ubi->move_mutex);\r\nspin_lock(&ubi->wl_lock);\r\nubi_assert(!ubi->move_from && !ubi->move_to);\r\nubi_assert(!ubi->move_to_put);\r\nif (!ubi->free.rb_node ||\r\n(!ubi->used.rb_node && !ubi->scrub.rb_node)) {\r\ndbg_wl("cancel WL, a list is empty: free %d, used %d",\r\n!ubi->free.rb_node, !ubi->used.rb_node);\r\ngoto out_cancel;\r\n}\r\nif (!ubi->scrub.rb_node) {\r\ne1 = rb_entry(rb_first(&ubi->used), struct ubi_wl_entry, u.rb);\r\ne2 = find_wl_entry(&ubi->free, WL_FREE_MAX_DIFF);\r\nif (!(e2->ec - e1->ec >= UBI_WL_THRESHOLD)) {\r\ndbg_wl("no WL needed: min used EC %d, max free EC %d",\r\ne1->ec, e2->ec);\r\ngoto out_cancel;\r\n}\r\nself_check_in_wl_tree(ubi, e1, &ubi->used);\r\nrb_erase(&e1->u.rb, &ubi->used);\r\ndbg_wl("move PEB %d EC %d to PEB %d EC %d",\r\ne1->pnum, e1->ec, e2->pnum, e2->ec);\r\n} else {\r\nscrubbing = 1;\r\ne1 = rb_entry(rb_first(&ubi->scrub), struct ubi_wl_entry, u.rb);\r\ne2 = find_wl_entry(&ubi->free, WL_FREE_MAX_DIFF);\r\nself_check_in_wl_tree(ubi, e1, &ubi->scrub);\r\nrb_erase(&e1->u.rb, &ubi->scrub);\r\ndbg_wl("scrub PEB %d to PEB %d", e1->pnum, e2->pnum);\r\n}\r\nself_check_in_wl_tree(ubi, e2, &ubi->free);\r\nrb_erase(&e2->u.rb, &ubi->free);\r\nubi->move_from = e1;\r\nubi->move_to = e2;\r\nspin_unlock(&ubi->wl_lock);\r\nerr = ubi_io_read_vid_hdr(ubi, e1->pnum, vid_hdr, 0);\r\nif (err && err != UBI_IO_BITFLIPS) {\r\nif (err == UBI_IO_FF) {\r\ndbg_wl("PEB %d has no VID header", e1->pnum);\r\nprotect = 1;\r\ngoto out_not_moved;\r\n} else if (err == UBI_IO_FF_BITFLIPS) {\r\ndbg_wl("PEB %d has no VID header but has bit-flips",\r\ne1->pnum);\r\nscrubbing = 1;\r\ngoto out_not_moved;\r\n}\r\nubi_err("error %d while reading VID header from PEB %d",\r\nerr, e1->pnum);\r\ngoto out_error;\r\n}\r\nvol_id = be32_to_cpu(vid_hdr->vol_id);\r\nlnum = be32_to_cpu(vid_hdr->lnum);\r\nerr = ubi_eba_copy_leb(ubi, e1->pnum, e2->pnum, vid_hdr);\r\nif (err) {\r\nif (err == MOVE_CANCEL_RACE) {\r\nprotect = 1;\r\ngoto out_not_moved;\r\n}\r\nif (err == MOVE_RETRY) {\r\nscrubbing = 1;\r\ngoto out_not_moved;\r\n}\r\nif (err == MOVE_TARGET_BITFLIPS || err == MOVE_TARGET_WR_ERR ||\r\nerr == MOVE_TARGET_RD_ERR) {\r\ntorture = 1;\r\ngoto out_not_moved;\r\n}\r\nif (err == MOVE_SOURCE_RD_ERR) {\r\nif (ubi->erroneous_peb_count > ubi->max_erroneous) {\r\nubi_err("too many erroneous eraseblocks (%d)",\r\nubi->erroneous_peb_count);\r\ngoto out_error;\r\n}\r\nerroneous = 1;\r\ngoto out_not_moved;\r\n}\r\nif (err < 0)\r\ngoto out_error;\r\nubi_assert(0);\r\n}\r\nif (scrubbing)\r\nubi_msg("scrubbed PEB %d (LEB %d:%d), data moved to PEB %d",\r\ne1->pnum, vol_id, lnum, e2->pnum);\r\nubi_free_vid_hdr(ubi, vid_hdr);\r\nspin_lock(&ubi->wl_lock);\r\nif (!ubi->move_to_put) {\r\nwl_tree_add(e2, &ubi->used);\r\ne2 = NULL;\r\n}\r\nubi->move_from = ubi->move_to = NULL;\r\nubi->move_to_put = ubi->wl_scheduled = 0;\r\nspin_unlock(&ubi->wl_lock);\r\nerr = schedule_erase(ubi, e1, vol_id, lnum, 0);\r\nif (err) {\r\nkmem_cache_free(ubi_wl_entry_slab, e1);\r\nif (e2)\r\nkmem_cache_free(ubi_wl_entry_slab, e2);\r\ngoto out_ro;\r\n}\r\nif (e2) {\r\ndbg_wl("PEB %d (LEB %d:%d) was put meanwhile, erase",\r\ne2->pnum, vol_id, lnum);\r\nerr = schedule_erase(ubi, e2, vol_id, lnum, 0);\r\nif (err) {\r\nkmem_cache_free(ubi_wl_entry_slab, e2);\r\ngoto out_ro;\r\n}\r\n}\r\ndbg_wl("done");\r\nmutex_unlock(&ubi->move_mutex);\r\nreturn 0;\r\nout_not_moved:\r\nif (vol_id != -1)\r\ndbg_wl("cancel moving PEB %d (LEB %d:%d) to PEB %d (%d)",\r\ne1->pnum, vol_id, lnum, e2->pnum, err);\r\nelse\r\ndbg_wl("cancel moving PEB %d to PEB %d (%d)",\r\ne1->pnum, e2->pnum, err);\r\nspin_lock(&ubi->wl_lock);\r\nif (protect)\r\nprot_queue_add(ubi, e1);\r\nelse if (erroneous) {\r\nwl_tree_add(e1, &ubi->erroneous);\r\nubi->erroneous_peb_count += 1;\r\n} else if (scrubbing)\r\nwl_tree_add(e1, &ubi->scrub);\r\nelse\r\nwl_tree_add(e1, &ubi->used);\r\nubi_assert(!ubi->move_to_put);\r\nubi->move_from = ubi->move_to = NULL;\r\nubi->wl_scheduled = 0;\r\nspin_unlock(&ubi->wl_lock);\r\nubi_free_vid_hdr(ubi, vid_hdr);\r\nerr = schedule_erase(ubi, e2, vol_id, lnum, torture);\r\nif (err) {\r\nkmem_cache_free(ubi_wl_entry_slab, e2);\r\ngoto out_ro;\r\n}\r\nmutex_unlock(&ubi->move_mutex);\r\nreturn 0;\r\nout_error:\r\nif (vol_id != -1)\r\nubi_err("error %d while moving PEB %d to PEB %d",\r\nerr, e1->pnum, e2->pnum);\r\nelse\r\nubi_err("error %d while moving PEB %d (LEB %d:%d) to PEB %d",\r\nerr, e1->pnum, vol_id, lnum, e2->pnum);\r\nspin_lock(&ubi->wl_lock);\r\nubi->move_from = ubi->move_to = NULL;\r\nubi->move_to_put = ubi->wl_scheduled = 0;\r\nspin_unlock(&ubi->wl_lock);\r\nubi_free_vid_hdr(ubi, vid_hdr);\r\nkmem_cache_free(ubi_wl_entry_slab, e1);\r\nkmem_cache_free(ubi_wl_entry_slab, e2);\r\nout_ro:\r\nubi_ro_mode(ubi);\r\nmutex_unlock(&ubi->move_mutex);\r\nubi_assert(err != 0);\r\nreturn err < 0 ? err : -EIO;\r\nout_cancel:\r\nubi->wl_scheduled = 0;\r\nspin_unlock(&ubi->wl_lock);\r\nmutex_unlock(&ubi->move_mutex);\r\nubi_free_vid_hdr(ubi, vid_hdr);\r\nreturn 0;\r\n}\r\nstatic int ensure_wear_leveling(struct ubi_device *ubi)\r\n{\r\nint err = 0;\r\nstruct ubi_wl_entry *e1;\r\nstruct ubi_wl_entry *e2;\r\nstruct ubi_work *wrk;\r\nspin_lock(&ubi->wl_lock);\r\nif (ubi->wl_scheduled)\r\ngoto out_unlock;\r\nif (!ubi->scrub.rb_node) {\r\nif (!ubi->used.rb_node || !ubi->free.rb_node)\r\ngoto out_unlock;\r\ne1 = rb_entry(rb_first(&ubi->used), struct ubi_wl_entry, u.rb);\r\ne2 = find_wl_entry(&ubi->free, WL_FREE_MAX_DIFF);\r\nif (!(e2->ec - e1->ec >= UBI_WL_THRESHOLD))\r\ngoto out_unlock;\r\ndbg_wl("schedule wear-leveling");\r\n} else\r\ndbg_wl("schedule scrubbing");\r\nubi->wl_scheduled = 1;\r\nspin_unlock(&ubi->wl_lock);\r\nwrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\r\nif (!wrk) {\r\nerr = -ENOMEM;\r\ngoto out_cancel;\r\n}\r\nwrk->func = &wear_leveling_worker;\r\nschedule_ubi_work(ubi, wrk);\r\nreturn err;\r\nout_cancel:\r\nspin_lock(&ubi->wl_lock);\r\nubi->wl_scheduled = 0;\r\nout_unlock:\r\nspin_unlock(&ubi->wl_lock);\r\nreturn err;\r\n}\r\nstatic int erase_worker(struct ubi_device *ubi, struct ubi_work *wl_wrk,\r\nint cancel)\r\n{\r\nstruct ubi_wl_entry *e = wl_wrk->e;\r\nint pnum = e->pnum, err, need;\r\nint vol_id = wl_wrk->vol_id;\r\nint lnum = wl_wrk->lnum;\r\nif (cancel) {\r\ndbg_wl("cancel erasure of PEB %d EC %d", pnum, e->ec);\r\nkfree(wl_wrk);\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\nreturn 0;\r\n}\r\ndbg_wl("erase PEB %d EC %d LEB %d:%d",\r\npnum, e->ec, wl_wrk->vol_id, wl_wrk->lnum);\r\nerr = sync_erase(ubi, e, wl_wrk->torture);\r\nif (!err) {\r\nkfree(wl_wrk);\r\nspin_lock(&ubi->wl_lock);\r\nwl_tree_add(e, &ubi->free);\r\nspin_unlock(&ubi->wl_lock);\r\nserve_prot_queue(ubi);\r\nerr = ensure_wear_leveling(ubi);\r\nreturn err;\r\n}\r\nubi_err("failed to erase PEB %d, error %d", pnum, err);\r\nkfree(wl_wrk);\r\nif (err == -EINTR || err == -ENOMEM || err == -EAGAIN ||\r\nerr == -EBUSY) {\r\nint err1;\r\nerr1 = schedule_erase(ubi, e, vol_id, lnum, 0);\r\nif (err1) {\r\nerr = err1;\r\ngoto out_ro;\r\n}\r\nreturn err;\r\n}\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\nif (err != -EIO)\r\ngoto out_ro;\r\nif (!ubi->bad_allowed) {\r\nubi_err("bad physical eraseblock %d detected", pnum);\r\ngoto out_ro;\r\n}\r\nspin_lock(&ubi->volumes_lock);\r\nneed = ubi->beb_rsvd_level - ubi->beb_rsvd_pebs + 1;\r\nif (need > 0) {\r\nneed = ubi->avail_pebs >= need ? need : ubi->avail_pebs;\r\nubi->avail_pebs -= need;\r\nubi->rsvd_pebs += need;\r\nubi->beb_rsvd_pebs += need;\r\nif (need > 0)\r\nubi_msg("reserve more %d PEBs", need);\r\n}\r\nif (ubi->beb_rsvd_pebs == 0) {\r\nspin_unlock(&ubi->volumes_lock);\r\nubi_err("no reserved physical eraseblocks");\r\ngoto out_ro;\r\n}\r\nspin_unlock(&ubi->volumes_lock);\r\nubi_msg("mark PEB %d as bad", pnum);\r\nerr = ubi_io_mark_bad(ubi, pnum);\r\nif (err)\r\ngoto out_ro;\r\nspin_lock(&ubi->volumes_lock);\r\nubi->beb_rsvd_pebs -= 1;\r\nubi->bad_peb_count += 1;\r\nubi->good_peb_count -= 1;\r\nubi_calculate_reserved(ubi);\r\nif (ubi->beb_rsvd_pebs)\r\nubi_msg("%d PEBs left in the reserve", ubi->beb_rsvd_pebs);\r\nelse\r\nubi_warn("last PEB from the reserved pool was used");\r\nspin_unlock(&ubi->volumes_lock);\r\nreturn err;\r\nout_ro:\r\nubi_ro_mode(ubi);\r\nreturn err;\r\n}\r\nint ubi_wl_put_peb(struct ubi_device *ubi, int vol_id, int lnum,\r\nint pnum, int torture)\r\n{\r\nint err;\r\nstruct ubi_wl_entry *e;\r\ndbg_wl("PEB %d", pnum);\r\nubi_assert(pnum >= 0);\r\nubi_assert(pnum < ubi->peb_count);\r\nretry:\r\nspin_lock(&ubi->wl_lock);\r\ne = ubi->lookuptbl[pnum];\r\nif (e == ubi->move_from) {\r\ndbg_wl("PEB %d is being moved, wait", pnum);\r\nspin_unlock(&ubi->wl_lock);\r\nmutex_lock(&ubi->move_mutex);\r\nmutex_unlock(&ubi->move_mutex);\r\ngoto retry;\r\n} else if (e == ubi->move_to) {\r\ndbg_wl("PEB %d is the target of data moving", pnum);\r\nubi_assert(!ubi->move_to_put);\r\nubi->move_to_put = 1;\r\nspin_unlock(&ubi->wl_lock);\r\nreturn 0;\r\n} else {\r\nif (in_wl_tree(e, &ubi->used)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->used);\r\nrb_erase(&e->u.rb, &ubi->used);\r\n} else if (in_wl_tree(e, &ubi->scrub)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->scrub);\r\nrb_erase(&e->u.rb, &ubi->scrub);\r\n} else if (in_wl_tree(e, &ubi->erroneous)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->erroneous);\r\nrb_erase(&e->u.rb, &ubi->erroneous);\r\nubi->erroneous_peb_count -= 1;\r\nubi_assert(ubi->erroneous_peb_count >= 0);\r\ntorture = 1;\r\n} else {\r\nerr = prot_queue_del(ubi, e->pnum);\r\nif (err) {\r\nubi_err("PEB %d not found", pnum);\r\nubi_ro_mode(ubi);\r\nspin_unlock(&ubi->wl_lock);\r\nreturn err;\r\n}\r\n}\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nerr = schedule_erase(ubi, e, vol_id, lnum, torture);\r\nif (err) {\r\nspin_lock(&ubi->wl_lock);\r\nwl_tree_add(e, &ubi->used);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nreturn err;\r\n}\r\nint ubi_wl_scrub_peb(struct ubi_device *ubi, int pnum)\r\n{\r\nstruct ubi_wl_entry *e;\r\ndbg_msg("schedule PEB %d for scrubbing", pnum);\r\nretry:\r\nspin_lock(&ubi->wl_lock);\r\ne = ubi->lookuptbl[pnum];\r\nif (e == ubi->move_from || in_wl_tree(e, &ubi->scrub) ||\r\nin_wl_tree(e, &ubi->erroneous)) {\r\nspin_unlock(&ubi->wl_lock);\r\nreturn 0;\r\n}\r\nif (e == ubi->move_to) {\r\nspin_unlock(&ubi->wl_lock);\r\ndbg_wl("the PEB %d is not in proper tree, retry", pnum);\r\nyield();\r\ngoto retry;\r\n}\r\nif (in_wl_tree(e, &ubi->used)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->used);\r\nrb_erase(&e->u.rb, &ubi->used);\r\n} else {\r\nint err;\r\nerr = prot_queue_del(ubi, e->pnum);\r\nif (err) {\r\nubi_err("PEB %d not found", pnum);\r\nubi_ro_mode(ubi);\r\nspin_unlock(&ubi->wl_lock);\r\nreturn err;\r\n}\r\n}\r\nwl_tree_add(e, &ubi->scrub);\r\nspin_unlock(&ubi->wl_lock);\r\nreturn ensure_wear_leveling(ubi);\r\n}\r\nint ubi_wl_flush(struct ubi_device *ubi, int vol_id, int lnum)\r\n{\r\nint err = 0;\r\nint found = 1;\r\ndbg_wl("flush pending work for LEB %d:%d (%d pending works)",\r\nvol_id, lnum, ubi->works_count);\r\nwhile (found) {\r\nstruct ubi_work *wrk;\r\nfound = 0;\r\ndown_read(&ubi->work_sem);\r\nspin_lock(&ubi->wl_lock);\r\nlist_for_each_entry(wrk, &ubi->works, list) {\r\nif ((vol_id == UBI_ALL || wrk->vol_id == vol_id) &&\r\n(lnum == UBI_ALL || wrk->lnum == lnum)) {\r\nlist_del(&wrk->list);\r\nubi->works_count -= 1;\r\nubi_assert(ubi->works_count >= 0);\r\nspin_unlock(&ubi->wl_lock);\r\nerr = wrk->func(ubi, wrk, 0);\r\nif (err) {\r\nup_read(&ubi->work_sem);\r\nreturn err;\r\n}\r\nspin_lock(&ubi->wl_lock);\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->work_sem);\r\n}\r\ndown_write(&ubi->work_sem);\r\nup_write(&ubi->work_sem);\r\nreturn err;\r\n}\r\nstatic void tree_destroy(struct rb_root *root)\r\n{\r\nstruct rb_node *rb;\r\nstruct ubi_wl_entry *e;\r\nrb = root->rb_node;\r\nwhile (rb) {\r\nif (rb->rb_left)\r\nrb = rb->rb_left;\r\nelse if (rb->rb_right)\r\nrb = rb->rb_right;\r\nelse {\r\ne = rb_entry(rb, struct ubi_wl_entry, u.rb);\r\nrb = rb_parent(rb);\r\nif (rb) {\r\nif (rb->rb_left == &e->u.rb)\r\nrb->rb_left = NULL;\r\nelse\r\nrb->rb_right = NULL;\r\n}\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\n}\r\n}\r\n}\r\nint ubi_thread(void *u)\r\n{\r\nint failures = 0;\r\nstruct ubi_device *ubi = u;\r\nubi_msg("background thread \"%s\" started, PID %d",\r\nubi->bgt_name, task_pid_nr(current));\r\nset_freezable();\r\nfor (;;) {\r\nint err;\r\nif (kthread_should_stop())\r\nbreak;\r\nif (try_to_freeze())\r\ncontinue;\r\nspin_lock(&ubi->wl_lock);\r\nif (list_empty(&ubi->works) || ubi->ro_mode ||\r\n!ubi->thread_enabled || ubi_dbg_is_bgt_disabled(ubi)) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nspin_unlock(&ubi->wl_lock);\r\nschedule();\r\ncontinue;\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nerr = do_work(ubi);\r\nif (err) {\r\nubi_err("%s: work failed with error code %d",\r\nubi->bgt_name, err);\r\nif (failures++ > WL_MAX_FAILURES) {\r\nubi_msg("%s: %d consecutive failures",\r\nubi->bgt_name, WL_MAX_FAILURES);\r\nubi_ro_mode(ubi);\r\nubi->thread_enabled = 0;\r\ncontinue;\r\n}\r\n} else\r\nfailures = 0;\r\ncond_resched();\r\n}\r\ndbg_wl("background thread \"%s\" is killed", ubi->bgt_name);\r\nreturn 0;\r\n}\r\nstatic void cancel_pending(struct ubi_device *ubi)\r\n{\r\nwhile (!list_empty(&ubi->works)) {\r\nstruct ubi_work *wrk;\r\nwrk = list_entry(ubi->works.next, struct ubi_work, list);\r\nlist_del(&wrk->list);\r\nwrk->func(ubi, wrk, 1);\r\nubi->works_count -= 1;\r\nubi_assert(ubi->works_count >= 0);\r\n}\r\n}\r\nint ubi_wl_init(struct ubi_device *ubi, struct ubi_attach_info *ai)\r\n{\r\nint err, i;\r\nstruct rb_node *rb1, *rb2;\r\nstruct ubi_ainf_volume *av;\r\nstruct ubi_ainf_peb *aeb, *tmp;\r\nstruct ubi_wl_entry *e;\r\nubi->used = ubi->erroneous = ubi->free = ubi->scrub = RB_ROOT;\r\nspin_lock_init(&ubi->wl_lock);\r\nmutex_init(&ubi->move_mutex);\r\ninit_rwsem(&ubi->work_sem);\r\nubi->max_ec = ai->max_ec;\r\nINIT_LIST_HEAD(&ubi->works);\r\nsprintf(ubi->bgt_name, UBI_BGT_NAME_PATTERN, ubi->ubi_num);\r\nerr = -ENOMEM;\r\nubi->lookuptbl = kzalloc(ubi->peb_count * sizeof(void *), GFP_KERNEL);\r\nif (!ubi->lookuptbl)\r\nreturn err;\r\nfor (i = 0; i < UBI_PROT_QUEUE_LEN; i++)\r\nINIT_LIST_HEAD(&ubi->pq[i]);\r\nubi->pq_head = 0;\r\nlist_for_each_entry_safe(aeb, tmp, &ai->erase, u.list) {\r\ncond_resched();\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi->lookuptbl[e->pnum] = e;\r\nif (schedule_erase(ubi, e, aeb->vol_id, aeb->lnum, 0)) {\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\ngoto out_free;\r\n}\r\n}\r\nlist_for_each_entry(aeb, &ai->free, u.list) {\r\ncond_resched();\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi_assert(e->ec >= 0);\r\nwl_tree_add(e, &ubi->free);\r\nubi->lookuptbl[e->pnum] = e;\r\n}\r\nubi_rb_for_each_entry(rb1, av, &ai->volumes, rb) {\r\nubi_rb_for_each_entry(rb2, aeb, &av->root, u.rb) {\r\ncond_resched();\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi->lookuptbl[e->pnum] = e;\r\nif (!aeb->scrub) {\r\ndbg_wl("add PEB %d EC %d to the used tree",\r\ne->pnum, e->ec);\r\nwl_tree_add(e, &ubi->used);\r\n} else {\r\ndbg_wl("add PEB %d EC %d to the scrub tree",\r\ne->pnum, e->ec);\r\nwl_tree_add(e, &ubi->scrub);\r\n}\r\n}\r\n}\r\nif (ubi->avail_pebs < WL_RESERVED_PEBS) {\r\nubi_err("no enough physical eraseblocks (%d, need %d)",\r\nubi->avail_pebs, WL_RESERVED_PEBS);\r\nif (ubi->corr_peb_count)\r\nubi_err("%d PEBs are corrupted and not used",\r\nubi->corr_peb_count);\r\ngoto out_free;\r\n}\r\nubi->avail_pebs -= WL_RESERVED_PEBS;\r\nubi->rsvd_pebs += WL_RESERVED_PEBS;\r\nerr = ensure_wear_leveling(ubi);\r\nif (err)\r\ngoto out_free;\r\nreturn 0;\r\nout_free:\r\ncancel_pending(ubi);\r\ntree_destroy(&ubi->used);\r\ntree_destroy(&ubi->free);\r\ntree_destroy(&ubi->scrub);\r\nkfree(ubi->lookuptbl);\r\nreturn err;\r\n}\r\nstatic void protection_queue_destroy(struct ubi_device *ubi)\r\n{\r\nint i;\r\nstruct ubi_wl_entry *e, *tmp;\r\nfor (i = 0; i < UBI_PROT_QUEUE_LEN; ++i) {\r\nlist_for_each_entry_safe(e, tmp, &ubi->pq[i], u.list) {\r\nlist_del(&e->u.list);\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\n}\r\n}\r\n}\r\nvoid ubi_wl_close(struct ubi_device *ubi)\r\n{\r\ndbg_wl("close the WL sub-system");\r\ncancel_pending(ubi);\r\nprotection_queue_destroy(ubi);\r\ntree_destroy(&ubi->used);\r\ntree_destroy(&ubi->erroneous);\r\ntree_destroy(&ubi->free);\r\ntree_destroy(&ubi->scrub);\r\nkfree(ubi->lookuptbl);\r\n}\r\nstatic int self_check_ec(struct ubi_device *ubi, int pnum, int ec)\r\n{\r\nint err;\r\nlong long read_ec;\r\nstruct ubi_ec_hdr *ec_hdr;\r\nif (!ubi->dbg->chk_gen)\r\nreturn 0;\r\nec_hdr = kzalloc(ubi->ec_hdr_alsize, GFP_NOFS);\r\nif (!ec_hdr)\r\nreturn -ENOMEM;\r\nerr = ubi_io_read_ec_hdr(ubi, pnum, ec_hdr, 0);\r\nif (err && err != UBI_IO_BITFLIPS) {\r\nerr = 0;\r\ngoto out_free;\r\n}\r\nread_ec = be64_to_cpu(ec_hdr->ec);\r\nif (ec != read_ec) {\r\nubi_err("self-check failed for PEB %d", pnum);\r\nubi_err("read EC is %lld, should be %d", read_ec, ec);\r\ndump_stack();\r\nerr = 1;\r\n} else\r\nerr = 0;\r\nout_free:\r\nkfree(ec_hdr);\r\nreturn err;\r\n}\r\nstatic int self_check_in_wl_tree(const struct ubi_device *ubi,\r\nstruct ubi_wl_entry *e, struct rb_root *root)\r\n{\r\nif (!ubi->dbg->chk_gen)\r\nreturn 0;\r\nif (in_wl_tree(e, root))\r\nreturn 0;\r\nubi_err("self-check failed for PEB %d, EC %d, RB-tree %p ",\r\ne->pnum, e->ec, root);\r\ndump_stack();\r\nreturn -EINVAL;\r\n}\r\nstatic int self_check_in_pq(const struct ubi_device *ubi,\r\nstruct ubi_wl_entry *e)\r\n{\r\nstruct ubi_wl_entry *p;\r\nint i;\r\nif (!ubi->dbg->chk_gen)\r\nreturn 0;\r\nfor (i = 0; i < UBI_PROT_QUEUE_LEN; ++i)\r\nlist_for_each_entry(p, &ubi->pq[i], u.list)\r\nif (p == e)\r\nreturn 0;\r\nubi_err("self-check failed for PEB %d, EC %d, Protect queue",\r\ne->pnum, e->ec);\r\ndump_stack();\r\nreturn -EINVAL;\r\n}
