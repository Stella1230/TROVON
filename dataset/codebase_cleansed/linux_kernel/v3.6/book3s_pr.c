void kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nmemcpy(svcpu->slb, to_book3s(vcpu)->slb_shadow, sizeof(svcpu->slb));\r\nmemcpy(&get_paca()->shadow_vcpu, to_book3s(vcpu)->shadow_vcpu,\r\nsizeof(get_paca()->shadow_vcpu));\r\nsvcpu->slb_max = to_book3s(vcpu)->slb_shadow_max;\r\nsvcpu_put(svcpu);\r\n#endif\r\n#ifdef CONFIG_PPC_BOOK3S_32\r\ncurrent->thread.kvm_shadow_vcpu = to_book3s(vcpu)->shadow_vcpu;\r\n#endif\r\n}\r\nvoid kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nmemcpy(to_book3s(vcpu)->slb_shadow, svcpu->slb, sizeof(svcpu->slb));\r\nmemcpy(to_book3s(vcpu)->shadow_vcpu, &get_paca()->shadow_vcpu,\r\nsizeof(get_paca()->shadow_vcpu));\r\nto_book3s(vcpu)->slb_shadow_max = svcpu->slb_max;\r\nsvcpu_put(svcpu);\r\n#endif\r\nkvmppc_giveup_ext(vcpu, MSR_FP);\r\nkvmppc_giveup_ext(vcpu, MSR_VEC);\r\nkvmppc_giveup_ext(vcpu, MSR_VSX);\r\n}\r\nstatic void kvmppc_recalc_shadow_msr(struct kvm_vcpu *vcpu)\r\n{\r\nulong smsr = vcpu->arch.shared->msr;\r\nsmsr &= MSR_FE0 | MSR_FE1 | MSR_SF | MSR_SE | MSR_BE | MSR_DE;\r\nsmsr |= MSR_ME | MSR_RI | MSR_IR | MSR_DR | MSR_PR | MSR_EE;\r\nsmsr |= (vcpu->arch.shared->msr & vcpu->arch.guest_owned_ext);\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nsmsr |= MSR_ISF | MSR_HV;\r\n#endif\r\nvcpu->arch.shadow_msr = smsr;\r\n}\r\nvoid kvmppc_set_msr(struct kvm_vcpu *vcpu, u64 msr)\r\n{\r\nulong old_msr = vcpu->arch.shared->msr;\r\n#ifdef EXIT_DEBUG\r\nprintk(KERN_INFO "KVM: Set MSR to 0x%llx\n", msr);\r\n#endif\r\nmsr &= to_book3s(vcpu)->msr_mask;\r\nvcpu->arch.shared->msr = msr;\r\nkvmppc_recalc_shadow_msr(vcpu);\r\nif (msr & MSR_POW) {\r\nif (!vcpu->arch.pending_exceptions) {\r\nkvm_vcpu_block(vcpu);\r\nclear_bit(KVM_REQ_UNHALT, &vcpu->requests);\r\nvcpu->stat.halt_wakeup++;\r\nmsr &= ~MSR_POW;\r\nvcpu->arch.shared->msr = msr;\r\n}\r\n}\r\nif ((vcpu->arch.shared->msr & (MSR_PR|MSR_IR|MSR_DR)) !=\r\n(old_msr & (MSR_PR|MSR_IR|MSR_DR))) {\r\nkvmppc_mmu_flush_segments(vcpu);\r\nkvmppc_mmu_map_segment(vcpu, kvmppc_get_pc(vcpu));\r\nif (!(msr & MSR_PR) && vcpu->arch.magic_page_pa) {\r\nstruct kvm_vcpu_arch *a = &vcpu->arch;\r\nif (msr & MSR_DR)\r\nkvmppc_mmu_map_segment(vcpu, a->magic_page_ea);\r\nelse\r\nkvmppc_mmu_map_segment(vcpu, a->magic_page_pa);\r\n}\r\n}\r\nif (vcpu->arch.magic_page_pa &&\r\n!(old_msr & MSR_PR) && !(old_msr & MSR_SF) && (msr & MSR_SF)) {\r\nkvmppc_mmu_pte_flush(vcpu, (uint32_t)vcpu->arch.magic_page_pa,\r\n~0xFFFUL);\r\n}\r\nif (vcpu->arch.shared->msr & MSR_FP)\r\nkvmppc_handle_ext(vcpu, BOOK3S_INTERRUPT_FP_UNAVAIL, MSR_FP);\r\n}\r\nvoid kvmppc_set_pvr(struct kvm_vcpu *vcpu, u32 pvr)\r\n{\r\nu32 host_pvr;\r\nvcpu->arch.hflags &= ~BOOK3S_HFLAG_SLB;\r\nvcpu->arch.pvr = pvr;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nif ((pvr >= 0x330000) && (pvr < 0x70330000)) {\r\nkvmppc_mmu_book3s_64_init(vcpu);\r\nif (!to_book3s(vcpu)->hior_explicit)\r\nto_book3s(vcpu)->hior = 0xfff00000;\r\nto_book3s(vcpu)->msr_mask = 0xffffffffffffffffULL;\r\nvcpu->arch.cpu_type = KVM_CPU_3S_64;\r\n} else\r\n#endif\r\n{\r\nkvmppc_mmu_book3s_32_init(vcpu);\r\nif (!to_book3s(vcpu)->hior_explicit)\r\nto_book3s(vcpu)->hior = 0;\r\nto_book3s(vcpu)->msr_mask = 0xffffffffULL;\r\nvcpu->arch.cpu_type = KVM_CPU_3S_32;\r\n}\r\nkvmppc_sanity_check(vcpu);\r\nvcpu->arch.hflags &= ~BOOK3S_HFLAG_DCBZ32;\r\nif (vcpu->arch.mmu.is_dcbz32(vcpu) && (mfmsr() & MSR_HV) &&\r\n!strcmp(cur_cpu_spec->platform, "ppc970"))\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_DCBZ32;\r\nif (!strcmp(cur_cpu_spec->platform, "ppc-cell-be"))\r\nto_book3s(vcpu)->msr_mask &= ~(MSR_FE0 | MSR_FE1);\r\n#ifdef CONFIG_PPC_BOOK3S_32\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_DCBZ32;\r\n#endif\r\nasm ( "mfpvr %0" : "=r"(host_pvr));\r\nswitch (host_pvr) {\r\ncase 0x00080200:\r\ncase 0x00088202:\r\ncase 0x70000100:\r\ncase 0x00080100:\r\ncase 0x00083203:\r\ncase 0x00083213:\r\ncase 0x00083204:\r\ncase 0x00083214:\r\ncase 0x00087200:\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_NATIVE_PS;\r\nmtspr(SPRN_HID2_GEKKO, mfspr(SPRN_HID2_GEKKO) | (1 << 29));\r\n}\r\n}\r\nstatic void kvmppc_patch_dcbz(struct kvm_vcpu *vcpu, struct kvmppc_pte *pte)\r\n{\r\nstruct page *hpage;\r\nu64 hpage_offset;\r\nu32 *page;\r\nint i;\r\nhpage = gfn_to_page(vcpu->kvm, pte->raddr >> PAGE_SHIFT);\r\nif (is_error_page(hpage)) {\r\nkvm_release_page_clean(hpage);\r\nreturn;\r\n}\r\nhpage_offset = pte->raddr & ~PAGE_MASK;\r\nhpage_offset &= ~0xFFFULL;\r\nhpage_offset /= 4;\r\nget_page(hpage);\r\npage = kmap_atomic(hpage);\r\nfor (i=hpage_offset; i < hpage_offset + (HW_PAGE_SIZE / 4); i++)\r\nif ((page[i] & 0xff0007ff) == INS_DCBZ)\r\npage[i] &= 0xfffffff7;\r\nkunmap_atomic(page);\r\nput_page(hpage);\r\n}\r\nstatic int kvmppc_visible_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)\r\n{\r\nulong mp_pa = vcpu->arch.magic_page_pa;\r\nif (!(vcpu->arch.shared->msr & MSR_SF))\r\nmp_pa = (uint32_t)mp_pa;\r\nif (unlikely(mp_pa) &&\r\nunlikely((mp_pa & KVM_PAM) >> PAGE_SHIFT == gfn)) {\r\nreturn 1;\r\n}\r\nreturn kvm_is_visible_gfn(vcpu->kvm, gfn);\r\n}\r\nint kvmppc_handle_pagefault(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nulong eaddr, int vec)\r\n{\r\nbool data = (vec == BOOK3S_INTERRUPT_DATA_STORAGE);\r\nint r = RESUME_GUEST;\r\nint relocated;\r\nint page_found = 0;\r\nstruct kvmppc_pte pte;\r\nbool is_mmio = false;\r\nbool dr = (vcpu->arch.shared->msr & MSR_DR) ? true : false;\r\nbool ir = (vcpu->arch.shared->msr & MSR_IR) ? true : false;\r\nu64 vsid;\r\nrelocated = data ? dr : ir;\r\nif (relocated) {\r\npage_found = vcpu->arch.mmu.xlate(vcpu, eaddr, &pte, data);\r\n} else {\r\npte.may_execute = true;\r\npte.may_read = true;\r\npte.may_write = true;\r\npte.raddr = eaddr & KVM_PAM;\r\npte.eaddr = eaddr;\r\npte.vpage = eaddr >> 12;\r\n}\r\nswitch (vcpu->arch.shared->msr & (MSR_DR|MSR_IR)) {\r\ncase 0:\r\npte.vpage |= ((u64)VSID_REAL << (SID_SHIFT - 12));\r\nbreak;\r\ncase MSR_DR:\r\ncase MSR_IR:\r\nvcpu->arch.mmu.esid_to_vsid(vcpu, eaddr >> SID_SHIFT, &vsid);\r\nif ((vcpu->arch.shared->msr & (MSR_DR|MSR_IR)) == MSR_DR)\r\npte.vpage |= ((u64)VSID_REAL_DR << (SID_SHIFT - 12));\r\nelse\r\npte.vpage |= ((u64)VSID_REAL_IR << (SID_SHIFT - 12));\r\npte.vpage |= vsid;\r\nif (vsid == -1)\r\npage_found = -EINVAL;\r\nbreak;\r\n}\r\nif (vcpu->arch.mmu.is_dcbz32(vcpu) &&\r\n(!(vcpu->arch.hflags & BOOK3S_HFLAG_DCBZ32))) {\r\npte.may_execute = !data;\r\n}\r\nif (page_found == -ENOENT) {\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nvcpu->arch.shared->dar = kvmppc_get_fault_dar(vcpu);\r\nvcpu->arch.shared->dsisr = svcpu->fault_dsisr;\r\nvcpu->arch.shared->msr |=\r\n(svcpu->shadow_srr1 & 0x00000000f8000000ULL);\r\nsvcpu_put(svcpu);\r\nkvmppc_book3s_queue_irqprio(vcpu, vec);\r\n} else if (page_found == -EPERM) {\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nvcpu->arch.shared->dar = kvmppc_get_fault_dar(vcpu);\r\nvcpu->arch.shared->dsisr = svcpu->fault_dsisr & ~DSISR_NOHPTE;\r\nvcpu->arch.shared->dsisr |= DSISR_PROTFAULT;\r\nvcpu->arch.shared->msr |=\r\nsvcpu->shadow_srr1 & 0x00000000f8000000ULL;\r\nsvcpu_put(svcpu);\r\nkvmppc_book3s_queue_irqprio(vcpu, vec);\r\n} else if (page_found == -EINVAL) {\r\nvcpu->arch.shared->dar = kvmppc_get_fault_dar(vcpu);\r\nkvmppc_book3s_queue_irqprio(vcpu, vec + 0x80);\r\n} else if (!is_mmio &&\r\nkvmppc_visible_gfn(vcpu, pte.raddr >> PAGE_SHIFT)) {\r\nkvmppc_mmu_map_page(vcpu, &pte);\r\nif (data)\r\nvcpu->stat.sp_storage++;\r\nelse if (vcpu->arch.mmu.is_dcbz32(vcpu) &&\r\n(!(vcpu->arch.hflags & BOOK3S_HFLAG_DCBZ32)))\r\nkvmppc_patch_dcbz(vcpu, &pte);\r\n} else {\r\nvcpu->stat.mmio_exits++;\r\nvcpu->arch.paddr_accessed = pte.raddr;\r\nvcpu->arch.vaddr_accessed = pte.eaddr;\r\nr = kvmppc_emulate_mmio(run, vcpu);\r\nif ( r == RESUME_HOST_NV )\r\nr = RESUME_HOST;\r\n}\r\nreturn r;\r\n}\r\nstatic inline int get_fpr_index(int i)\r\n{\r\n#ifdef CONFIG_VSX\r\ni *= 2;\r\n#endif\r\nreturn i;\r\n}\r\nvoid kvmppc_giveup_ext(struct kvm_vcpu *vcpu, ulong msr)\r\n{\r\nstruct thread_struct *t = &current->thread;\r\nu64 *vcpu_fpr = vcpu->arch.fpr;\r\n#ifdef CONFIG_VSX\r\nu64 *vcpu_vsx = vcpu->arch.vsr;\r\n#endif\r\nu64 *thread_fpr = (u64*)t->fpr;\r\nint i;\r\nif (!(vcpu->arch.guest_owned_ext & msr))\r\nreturn;\r\n#ifdef DEBUG_EXT\r\nprintk(KERN_INFO "Giving up ext 0x%lx\n", msr);\r\n#endif\r\nswitch (msr) {\r\ncase MSR_FP:\r\ngiveup_fpu(current);\r\nfor (i = 0; i < ARRAY_SIZE(vcpu->arch.fpr); i++)\r\nvcpu_fpr[i] = thread_fpr[get_fpr_index(i)];\r\nvcpu->arch.fpscr = t->fpscr.val;\r\nbreak;\r\ncase MSR_VEC:\r\n#ifdef CONFIG_ALTIVEC\r\ngiveup_altivec(current);\r\nmemcpy(vcpu->arch.vr, t->vr, sizeof(vcpu->arch.vr));\r\nvcpu->arch.vscr = t->vscr;\r\n#endif\r\nbreak;\r\ncase MSR_VSX:\r\n#ifdef CONFIG_VSX\r\n__giveup_vsx(current);\r\nfor (i = 0; i < ARRAY_SIZE(vcpu->arch.vsr); i++)\r\nvcpu_vsx[i] = thread_fpr[get_fpr_index(i) + 1];\r\n#endif\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nvcpu->arch.guest_owned_ext &= ~msr;\r\ncurrent->thread.regs->msr &= ~msr;\r\nkvmppc_recalc_shadow_msr(vcpu);\r\n}\r\nstatic int kvmppc_read_inst(struct kvm_vcpu *vcpu)\r\n{\r\nulong srr0 = kvmppc_get_pc(vcpu);\r\nu32 last_inst = kvmppc_get_last_inst(vcpu);\r\nint ret;\r\nret = kvmppc_ld(vcpu, &srr0, sizeof(u32), &last_inst, false);\r\nif (ret == -ENOENT) {\r\nulong msr = vcpu->arch.shared->msr;\r\nmsr = kvmppc_set_field(msr, 33, 33, 1);\r\nmsr = kvmppc_set_field(msr, 34, 36, 0);\r\nvcpu->arch.shared->msr = kvmppc_set_field(msr, 42, 47, 0);\r\nkvmppc_book3s_queue_irqprio(vcpu, BOOK3S_INTERRUPT_INST_STORAGE);\r\nreturn EMULATE_AGAIN;\r\n}\r\nreturn EMULATE_DONE;\r\n}\r\nstatic int kvmppc_check_ext(struct kvm_vcpu *vcpu, unsigned int exit_nr)\r\n{\r\nif (!(vcpu->arch.hflags & BOOK3S_HFLAG_PAIRED_SINGLE))\r\nreturn EMULATE_DONE;\r\nif (kvmppc_read_inst(vcpu) == EMULATE_DONE)\r\nreturn EMULATE_FAIL;\r\nreturn EMULATE_AGAIN;\r\n}\r\nstatic int kvmppc_handle_ext(struct kvm_vcpu *vcpu, unsigned int exit_nr,\r\nulong msr)\r\n{\r\nstruct thread_struct *t = &current->thread;\r\nu64 *vcpu_fpr = vcpu->arch.fpr;\r\n#ifdef CONFIG_VSX\r\nu64 *vcpu_vsx = vcpu->arch.vsr;\r\n#endif\r\nu64 *thread_fpr = (u64*)t->fpr;\r\nint i;\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_PAIRED_SINGLE)\r\nreturn RESUME_GUEST;\r\nif (!(vcpu->arch.shared->msr & msr)) {\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nreturn RESUME_GUEST;\r\n}\r\nif (vcpu->arch.guest_owned_ext & msr) {\r\nreturn RESUME_GUEST;\r\n}\r\n#ifdef DEBUG_EXT\r\nprintk(KERN_INFO "Loading up ext 0x%lx\n", msr);\r\n#endif\r\ncurrent->thread.regs->msr |= msr;\r\nswitch (msr) {\r\ncase MSR_FP:\r\nfor (i = 0; i < ARRAY_SIZE(vcpu->arch.fpr); i++)\r\nthread_fpr[get_fpr_index(i)] = vcpu_fpr[i];\r\nt->fpscr.val = vcpu->arch.fpscr;\r\nt->fpexc_mode = 0;\r\nkvmppc_load_up_fpu();\r\nbreak;\r\ncase MSR_VEC:\r\n#ifdef CONFIG_ALTIVEC\r\nmemcpy(t->vr, vcpu->arch.vr, sizeof(vcpu->arch.vr));\r\nt->vscr = vcpu->arch.vscr;\r\nt->vrsave = -1;\r\nkvmppc_load_up_altivec();\r\n#endif\r\nbreak;\r\ncase MSR_VSX:\r\n#ifdef CONFIG_VSX\r\nfor (i = 0; i < ARRAY_SIZE(vcpu->arch.vsr); i++)\r\nthread_fpr[get_fpr_index(i) + 1] = vcpu_vsx[i];\r\nkvmppc_load_up_vsx();\r\n#endif\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nvcpu->arch.guest_owned_ext |= msr;\r\nkvmppc_recalc_shadow_msr(vcpu);\r\nreturn RESUME_GUEST;\r\n}\r\nint kvmppc_handle_exit(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int exit_nr)\r\n{\r\nint r = RESUME_HOST;\r\nvcpu->stat.sum_exits++;\r\nrun->exit_reason = KVM_EXIT_UNKNOWN;\r\nrun->ready_for_interrupt_injection = 1;\r\n__hard_irq_enable();\r\ntrace_kvm_book3s_exit(exit_nr, vcpu);\r\npreempt_enable();\r\nkvm_resched(vcpu);\r\nswitch (exit_nr) {\r\ncase BOOK3S_INTERRUPT_INST_STORAGE:\r\n{\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nulong shadow_srr1 = svcpu->shadow_srr1;\r\nvcpu->stat.pf_instruc++;\r\n#ifdef CONFIG_PPC_BOOK3S_32\r\nif (svcpu->sr[kvmppc_get_pc(vcpu) >> SID_SHIFT] == SR_INVALID) {\r\nkvmppc_mmu_map_segment(vcpu, kvmppc_get_pc(vcpu));\r\nr = RESUME_GUEST;\r\nsvcpu_put(svcpu);\r\nbreak;\r\n}\r\n#endif\r\nsvcpu_put(svcpu);\r\nif (shadow_srr1 & 0x40000000) {\r\nr = kvmppc_handle_pagefault(run, vcpu, kvmppc_get_pc(vcpu), exit_nr);\r\nvcpu->stat.sp_instruc++;\r\n} else if (vcpu->arch.mmu.is_dcbz32(vcpu) &&\r\n(!(vcpu->arch.hflags & BOOK3S_HFLAG_DCBZ32))) {\r\nkvmppc_mmu_pte_flush(vcpu, kvmppc_get_pc(vcpu), ~0xFFFUL);\r\nr = RESUME_GUEST;\r\n} else {\r\nvcpu->arch.shared->msr |= shadow_srr1 & 0x58000000;\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nr = RESUME_GUEST;\r\n}\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_DATA_STORAGE:\r\n{\r\nulong dar = kvmppc_get_fault_dar(vcpu);\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nu32 fault_dsisr = svcpu->fault_dsisr;\r\nvcpu->stat.pf_storage++;\r\n#ifdef CONFIG_PPC_BOOK3S_32\r\nif ((svcpu->sr[dar >> SID_SHIFT]) == SR_INVALID) {\r\nkvmppc_mmu_map_segment(vcpu, dar);\r\nr = RESUME_GUEST;\r\nsvcpu_put(svcpu);\r\nbreak;\r\n}\r\n#endif\r\nsvcpu_put(svcpu);\r\nif (fault_dsisr & DSISR_NOHPTE) {\r\nr = kvmppc_handle_pagefault(run, vcpu, dar, exit_nr);\r\n} else {\r\nvcpu->arch.shared->dar = dar;\r\nvcpu->arch.shared->dsisr = fault_dsisr;\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nr = RESUME_GUEST;\r\n}\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_DATA_SEGMENT:\r\nif (kvmppc_mmu_map_segment(vcpu, kvmppc_get_fault_dar(vcpu)) < 0) {\r\nvcpu->arch.shared->dar = kvmppc_get_fault_dar(vcpu);\r\nkvmppc_book3s_queue_irqprio(vcpu,\r\nBOOK3S_INTERRUPT_DATA_SEGMENT);\r\n}\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_INST_SEGMENT:\r\nif (kvmppc_mmu_map_segment(vcpu, kvmppc_get_pc(vcpu)) < 0) {\r\nkvmppc_book3s_queue_irqprio(vcpu,\r\nBOOK3S_INTERRUPT_INST_SEGMENT);\r\n}\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_DECREMENTER:\r\ncase BOOK3S_INTERRUPT_HV_DECREMENTER:\r\nvcpu->stat.dec_exits++;\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_EXTERNAL:\r\ncase BOOK3S_INTERRUPT_EXTERNAL_LEVEL:\r\ncase BOOK3S_INTERRUPT_EXTERNAL_HV:\r\nvcpu->stat.ext_intr_exits++;\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_PERFMON:\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_PROGRAM:\r\ncase BOOK3S_INTERRUPT_H_EMUL_ASSIST:\r\n{\r\nenum emulation_result er;\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu;\r\nulong flags;\r\nprogram_interrupt:\r\nsvcpu = svcpu_get(vcpu);\r\nflags = svcpu->shadow_srr1 & 0x1f0000ull;\r\nsvcpu_put(svcpu);\r\nif (vcpu->arch.shared->msr & MSR_PR) {\r\n#ifdef EXIT_DEBUG\r\nprintk(KERN_INFO "Userspace triggered 0x700 exception at 0x%lx (0x%x)\n", kvmppc_get_pc(vcpu), kvmppc_get_last_inst(vcpu));\r\n#endif\r\nif ((kvmppc_get_last_inst(vcpu) & 0xff0007ff) !=\r\n(INS_DCBZ & 0xfffffff7)) {\r\nkvmppc_core_queue_program(vcpu, flags);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\n}\r\nvcpu->stat.emulated_inst_exits++;\r\ner = kvmppc_emulate_instruction(run, vcpu);\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nr = RESUME_GUEST_NV;\r\nbreak;\r\ncase EMULATE_AGAIN:\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase EMULATE_FAIL:\r\nprintk(KERN_CRIT "%s: emulation at %lx failed (%08x)\n",\r\n__func__, kvmppc_get_pc(vcpu), kvmppc_get_last_inst(vcpu));\r\nkvmppc_core_queue_program(vcpu, flags);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase EMULATE_DO_MMIO:\r\nrun->exit_reason = KVM_EXIT_MMIO;\r\nr = RESUME_HOST_NV;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_SYSCALL:\r\nif (vcpu->arch.papr_enabled &&\r\n(kvmppc_get_last_inst(vcpu) == 0x44000022) &&\r\n!(vcpu->arch.shared->msr & MSR_PR)) {\r\nulong cmd = kvmppc_get_gpr(vcpu, 3);\r\nint i;\r\n#ifdef CONFIG_KVM_BOOK3S_64_PR\r\nif (kvmppc_h_pr(vcpu, cmd) == EMULATE_DONE) {\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\n#endif\r\nrun->papr_hcall.nr = cmd;\r\nfor (i = 0; i < 9; ++i) {\r\nulong gpr = kvmppc_get_gpr(vcpu, 4 + i);\r\nrun->papr_hcall.args[i] = gpr;\r\n}\r\nrun->exit_reason = KVM_EXIT_PAPR_HCALL;\r\nvcpu->arch.hcall_needed = 1;\r\nr = RESUME_HOST;\r\n} else if (vcpu->arch.osi_enabled &&\r\n(((u32)kvmppc_get_gpr(vcpu, 3)) == OSI_SC_MAGIC_R3) &&\r\n(((u32)kvmppc_get_gpr(vcpu, 4)) == OSI_SC_MAGIC_R4)) {\r\nu64 *gprs = run->osi.gprs;\r\nint i;\r\nrun->exit_reason = KVM_EXIT_OSI;\r\nfor (i = 0; i < 32; i++)\r\ngprs[i] = kvmppc_get_gpr(vcpu, i);\r\nvcpu->arch.osi_needed = 1;\r\nr = RESUME_HOST_NV;\r\n} else if (!(vcpu->arch.shared->msr & MSR_PR) &&\r\n(((u32)kvmppc_get_gpr(vcpu, 0)) == KVM_SC_MAGIC_R0)) {\r\nkvmppc_set_gpr(vcpu, 3, kvmppc_kvm_pv(vcpu));\r\nr = RESUME_GUEST;\r\n} else {\r\nvcpu->stat.syscall_exits++;\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nr = RESUME_GUEST;\r\n}\r\nbreak;\r\ncase BOOK3S_INTERRUPT_FP_UNAVAIL:\r\ncase BOOK3S_INTERRUPT_ALTIVEC:\r\ncase BOOK3S_INTERRUPT_VSX:\r\n{\r\nint ext_msr = 0;\r\nswitch (exit_nr) {\r\ncase BOOK3S_INTERRUPT_FP_UNAVAIL: ext_msr = MSR_FP; break;\r\ncase BOOK3S_INTERRUPT_ALTIVEC: ext_msr = MSR_VEC; break;\r\ncase BOOK3S_INTERRUPT_VSX: ext_msr = MSR_VSX; break;\r\n}\r\nswitch (kvmppc_check_ext(vcpu, exit_nr)) {\r\ncase EMULATE_DONE:\r\nr = kvmppc_handle_ext(vcpu, exit_nr, ext_msr);\r\nbreak;\r\ncase EMULATE_FAIL:\r\ngoto program_interrupt;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_ALIGNMENT:\r\nif (kvmppc_read_inst(vcpu) == EMULATE_DONE) {\r\nvcpu->arch.shared->dsisr = kvmppc_alignment_dsisr(vcpu,\r\nkvmppc_get_last_inst(vcpu));\r\nvcpu->arch.shared->dar = kvmppc_alignment_dar(vcpu,\r\nkvmppc_get_last_inst(vcpu));\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\n}\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_MACHINE_CHECK:\r\ncase BOOK3S_INTERRUPT_TRACE:\r\nkvmppc_book3s_queue_irqprio(vcpu, exit_nr);\r\nr = RESUME_GUEST;\r\nbreak;\r\ndefault:\r\n{\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nulong shadow_srr1 = svcpu->shadow_srr1;\r\nsvcpu_put(svcpu);\r\nprintk(KERN_EMERG "exit_nr=0x%x | pc=0x%lx | msr=0x%lx\n",\r\nexit_nr, kvmppc_get_pc(vcpu), shadow_srr1);\r\nr = RESUME_HOST;\r\nBUG();\r\nbreak;\r\n}\r\n}\r\npreempt_disable();\r\nif (!(r & RESUME_HOST)) {\r\n__hard_irq_disable();\r\nif (signal_pending(current)) {\r\n__hard_irq_enable();\r\n#ifdef EXIT_DEBUG\r\nprintk(KERN_EMERG "KVM: Going back to host\n");\r\n#endif\r\nvcpu->stat.signal_exits++;\r\nrun->exit_reason = KVM_EXIT_INTR;\r\nr = -EINTR;\r\n} else {\r\nkvmppc_core_prepare_to_enter(vcpu);\r\n}\r\n}\r\ntrace_kvm_book3s_reenter(r, vcpu);\r\nreturn r;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nint i;\r\nsregs->pvr = vcpu->arch.pvr;\r\nsregs->u.s.sdr1 = to_book3s(vcpu)->sdr1;\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_SLB) {\r\nfor (i = 0; i < 64; i++) {\r\nsregs->u.s.ppc64.slb[i].slbe = vcpu->arch.slb[i].orige | i;\r\nsregs->u.s.ppc64.slb[i].slbv = vcpu->arch.slb[i].origv;\r\n}\r\n} else {\r\nfor (i = 0; i < 16; i++)\r\nsregs->u.s.ppc32.sr[i] = vcpu->arch.shared->sr[i];\r\nfor (i = 0; i < 8; i++) {\r\nsregs->u.s.ppc32.ibat[i] = vcpu3s->ibat[i].raw;\r\nsregs->u.s.ppc32.dbat[i] = vcpu3s->dbat[i].raw;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nint i;\r\nkvmppc_set_pvr(vcpu, sregs->pvr);\r\nvcpu3s->sdr1 = sregs->u.s.sdr1;\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_SLB) {\r\nfor (i = 0; i < 64; i++) {\r\nvcpu->arch.mmu.slbmte(vcpu, sregs->u.s.ppc64.slb[i].slbv,\r\nsregs->u.s.ppc64.slb[i].slbe);\r\n}\r\n} else {\r\nfor (i = 0; i < 16; i++) {\r\nvcpu->arch.mmu.mtsrin(vcpu, i, sregs->u.s.ppc32.sr[i]);\r\n}\r\nfor (i = 0; i < 8; i++) {\r\nkvmppc_set_bat(vcpu, &(vcpu3s->ibat[i]), false,\r\n(u32)sregs->u.s.ppc32.ibat[i]);\r\nkvmppc_set_bat(vcpu, &(vcpu3s->ibat[i]), true,\r\n(u32)(sregs->u.s.ppc32.ibat[i] >> 32));\r\nkvmppc_set_bat(vcpu, &(vcpu3s->dbat[i]), false,\r\n(u32)sregs->u.s.ppc32.dbat[i]);\r\nkvmppc_set_bat(vcpu, &(vcpu3s->dbat[i]), true,\r\n(u32)(sregs->u.s.ppc32.dbat[i] >> 32));\r\n}\r\n}\r\nkvmppc_mmu_pte_flush(vcpu, 0, 0);\r\nreturn 0;\r\n}\r\nint kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)\r\n{\r\nint r = -EINVAL;\r\nswitch (reg->id) {\r\ncase KVM_REG_PPC_HIOR:\r\nr = copy_to_user((u64 __user *)(long)reg->addr,\r\n&to_book3s(vcpu)->hior, sizeof(u64));\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nint kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)\r\n{\r\nint r = -EINVAL;\r\nswitch (reg->id) {\r\ncase KVM_REG_PPC_HIOR:\r\nr = copy_from_user(&to_book3s(vcpu)->hior,\r\n(u64 __user *)(long)reg->addr, sizeof(u64));\r\nif (!r)\r\nto_book3s(vcpu)->hior_explicit = true;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nint kvmppc_core_check_processor_compat(void)\r\n{\r\nreturn 0;\r\n}\r\nstruct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm, unsigned int id)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu_book3s;\r\nstruct kvm_vcpu *vcpu;\r\nint err = -ENOMEM;\r\nunsigned long p;\r\nvcpu_book3s = vzalloc(sizeof(struct kvmppc_vcpu_book3s));\r\nif (!vcpu_book3s)\r\ngoto out;\r\nvcpu_book3s->shadow_vcpu = (struct kvmppc_book3s_shadow_vcpu *)\r\nkzalloc(sizeof(*vcpu_book3s->shadow_vcpu), GFP_KERNEL);\r\nif (!vcpu_book3s->shadow_vcpu)\r\ngoto free_vcpu;\r\nvcpu = &vcpu_book3s->vcpu;\r\nerr = kvm_vcpu_init(vcpu, kvm, id);\r\nif (err)\r\ngoto free_shadow_vcpu;\r\np = __get_free_page(GFP_KERNEL|__GFP_ZERO);\r\nvcpu->arch.shared = (void*)(p + PAGE_SIZE - 4096);\r\nif (!p)\r\ngoto uninit_vcpu;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nvcpu->arch.pvr = 0x3C0301;\r\n#else\r\nvcpu->arch.pvr = 0x84202;\r\n#endif\r\nkvmppc_set_pvr(vcpu, vcpu->arch.pvr);\r\nvcpu->arch.slb_nr = 64;\r\nvcpu->arch.shadow_msr = MSR_USER64;\r\nerr = kvmppc_mmu_init(vcpu);\r\nif (err < 0)\r\ngoto uninit_vcpu;\r\nreturn vcpu;\r\nuninit_vcpu:\r\nkvm_vcpu_uninit(vcpu);\r\nfree_shadow_vcpu:\r\nkfree(vcpu_book3s->shadow_vcpu);\r\nfree_vcpu:\r\nvfree(vcpu_book3s);\r\nout:\r\nreturn ERR_PTR(err);\r\n}\r\nvoid kvmppc_core_vcpu_free(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu_book3s = to_book3s(vcpu);\r\nfree_page((unsigned long)vcpu->arch.shared & PAGE_MASK);\r\nkvm_vcpu_uninit(vcpu);\r\nkfree(vcpu_book3s->shadow_vcpu);\r\nvfree(vcpu_book3s);\r\n}\r\nint kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)\r\n{\r\nint ret;\r\ndouble fpr[32][TS_FPRWIDTH];\r\nunsigned int fpscr;\r\nint fpexc_mode;\r\n#ifdef CONFIG_ALTIVEC\r\nvector128 vr[32];\r\nvector128 vscr;\r\nunsigned long uninitialized_var(vrsave);\r\nint used_vr;\r\n#endif\r\n#ifdef CONFIG_VSX\r\nint used_vsr;\r\n#endif\r\nulong ext_msr;\r\npreempt_disable();\r\nif (!vcpu->arch.sane) {\r\nkvm_run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nkvmppc_core_prepare_to_enter(vcpu);\r\n__hard_irq_disable();\r\nif (signal_pending(current)) {\r\n__hard_irq_enable();\r\nkvm_run->exit_reason = KVM_EXIT_INTR;\r\nret = -EINTR;\r\ngoto out;\r\n}\r\nif (current->thread.regs->msr & MSR_FP)\r\ngiveup_fpu(current);\r\nmemcpy(fpr, current->thread.fpr, sizeof(current->thread.fpr));\r\nfpscr = current->thread.fpscr.val;\r\nfpexc_mode = current->thread.fpexc_mode;\r\n#ifdef CONFIG_ALTIVEC\r\nused_vr = current->thread.used_vr;\r\nif (used_vr) {\r\nif (current->thread.regs->msr & MSR_VEC)\r\ngiveup_altivec(current);\r\nmemcpy(vr, current->thread.vr, sizeof(current->thread.vr));\r\nvscr = current->thread.vscr;\r\nvrsave = current->thread.vrsave;\r\n}\r\n#endif\r\n#ifdef CONFIG_VSX\r\nused_vsr = current->thread.used_vsr;\r\nif (used_vsr && (current->thread.regs->msr & MSR_VSX))\r\n__giveup_vsx(current);\r\n#endif\r\next_msr = current->thread.regs->msr;\r\nif (vcpu->arch.shared->msr & MSR_FP)\r\nkvmppc_handle_ext(vcpu, BOOK3S_INTERRUPT_FP_UNAVAIL, MSR_FP);\r\nkvm_guest_enter();\r\nret = __kvmppc_vcpu_run(kvm_run, vcpu);\r\nkvm_guest_exit();\r\ncurrent->thread.regs->msr = ext_msr;\r\nkvmppc_giveup_ext(vcpu, MSR_FP);\r\nkvmppc_giveup_ext(vcpu, MSR_VEC);\r\nkvmppc_giveup_ext(vcpu, MSR_VSX);\r\nmemcpy(current->thread.fpr, fpr, sizeof(current->thread.fpr));\r\ncurrent->thread.fpscr.val = fpscr;\r\ncurrent->thread.fpexc_mode = fpexc_mode;\r\n#ifdef CONFIG_ALTIVEC\r\nif (used_vr && current->thread.used_vr) {\r\nmemcpy(current->thread.vr, vr, sizeof(current->thread.vr));\r\ncurrent->thread.vscr = vscr;\r\ncurrent->thread.vrsave = vrsave;\r\n}\r\ncurrent->thread.used_vr = used_vr;\r\n#endif\r\n#ifdef CONFIG_VSX\r\ncurrent->thread.used_vsr = used_vsr;\r\n#endif\r\nout:\r\npreempt_enable();\r\nreturn ret;\r\n}\r\nint kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,\r\nstruct kvm_dirty_log *log)\r\n{\r\nstruct kvm_memory_slot *memslot;\r\nstruct kvm_vcpu *vcpu;\r\nulong ga, ga_end;\r\nint is_dirty = 0;\r\nint r;\r\nunsigned long n;\r\nmutex_lock(&kvm->slots_lock);\r\nr = kvm_get_dirty_log(kvm, log, &is_dirty);\r\nif (r)\r\ngoto out;\r\nif (is_dirty) {\r\nmemslot = id_to_memslot(kvm->memslots, log->slot);\r\nga = memslot->base_gfn << PAGE_SHIFT;\r\nga_end = ga + (memslot->npages << PAGE_SHIFT);\r\nkvm_for_each_vcpu(n, vcpu, kvm)\r\nkvmppc_mmu_pte_pflush(vcpu, ga, ga_end);\r\nn = kvm_dirty_bitmap_bytes(memslot);\r\nmemset(memslot->dirty_bitmap, 0, n);\r\n}\r\nr = 0;\r\nout:\r\nmutex_unlock(&kvm->slots_lock);\r\nreturn r;\r\n}\r\nint kvm_vm_ioctl_get_smmu_info(struct kvm *kvm, struct kvm_ppc_smmu_info *info)\r\n{\r\ninfo->flags = 0;\r\ninfo->slb_size = 64;\r\ninfo->sps[0].page_shift = 12;\r\ninfo->sps[0].slb_enc = 0;\r\ninfo->sps[0].enc[0].page_shift = 12;\r\ninfo->sps[0].enc[0].pte_enc = 0;\r\ninfo->sps[1].page_shift = 24;\r\ninfo->sps[1].slb_enc = SLB_VSID_L;\r\ninfo->sps[1].enc[0].page_shift = 24;\r\ninfo->sps[1].enc[0].pte_enc = 0;\r\nreturn 0;\r\n}\r\nint kvmppc_core_prepare_memory_region(struct kvm *kvm,\r\nstruct kvm_userspace_memory_region *mem)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvmppc_core_commit_memory_region(struct kvm *kvm,\r\nstruct kvm_userspace_memory_region *mem)\r\n{\r\n}\r\nint kvmppc_core_init_vm(struct kvm *kvm)\r\n{\r\n#ifdef CONFIG_PPC64\r\nINIT_LIST_HEAD(&kvm->arch.spapr_tce_tables);\r\n#endif\r\nreturn 0;\r\n}\r\nvoid kvmppc_core_destroy_vm(struct kvm *kvm)\r\n{\r\n#ifdef CONFIG_PPC64\r\nWARN_ON(!list_empty(&kvm->arch.spapr_tce_tables));\r\n#endif\r\n}\r\nstatic int kvmppc_book3s_init(void)\r\n{\r\nint r;\r\nr = kvm_init(NULL, sizeof(struct kvmppc_vcpu_book3s), 0,\r\nTHIS_MODULE);\r\nif (r)\r\nreturn r;\r\nr = kvmppc_mmu_hpte_sysinit();\r\nreturn r;\r\n}\r\nstatic void kvmppc_book3s_exit(void)\r\n{\r\nkvmppc_mmu_hpte_sysexit();\r\nkvm_exit();\r\n}
