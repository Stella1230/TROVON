static void\r\nnouveau_bo_del_ttm(struct ttm_buffer_object *bo)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bo->bdev);\r\nstruct drm_device *dev = dev_priv->dev;\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nif (unlikely(nvbo->gem))\r\nDRM_ERROR("bo %p still attached to GEM object\n", bo);\r\nnv10_mem_put_tile_region(dev, nvbo->tile, NULL);\r\nkfree(nvbo);\r\n}\r\nstatic void\r\nnouveau_bo_fixup_align(struct nouveau_bo *nvbo, u32 flags,\r\nint *align, int *size)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(nvbo->bo.bdev);\r\nif (dev_priv->card_type < NV_50) {\r\nif (nvbo->tile_mode) {\r\nif (dev_priv->chipset >= 0x40) {\r\n*align = 65536;\r\n*size = roundup(*size, 64 * nvbo->tile_mode);\r\n} else if (dev_priv->chipset >= 0x30) {\r\n*align = 32768;\r\n*size = roundup(*size, 64 * nvbo->tile_mode);\r\n} else if (dev_priv->chipset >= 0x20) {\r\n*align = 16384;\r\n*size = roundup(*size, 64 * nvbo->tile_mode);\r\n} else if (dev_priv->chipset >= 0x10) {\r\n*align = 16384;\r\n*size = roundup(*size, 32 * nvbo->tile_mode);\r\n}\r\n}\r\n} else {\r\n*size = roundup(*size, (1 << nvbo->page_shift));\r\n*align = max((1 << nvbo->page_shift), *align);\r\n}\r\n*size = roundup(*size, PAGE_SIZE);\r\n}\r\nint\r\nnouveau_bo_new(struct drm_device *dev, int size, int align,\r\nuint32_t flags, uint32_t tile_mode, uint32_t tile_flags,\r\nstruct sg_table *sg,\r\nstruct nouveau_bo **pnvbo)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_bo *nvbo;\r\nsize_t acc_size;\r\nint ret;\r\nint type = ttm_bo_type_device;\r\nif (sg)\r\ntype = ttm_bo_type_sg;\r\nnvbo = kzalloc(sizeof(struct nouveau_bo), GFP_KERNEL);\r\nif (!nvbo)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&nvbo->head);\r\nINIT_LIST_HEAD(&nvbo->entry);\r\nINIT_LIST_HEAD(&nvbo->vma_list);\r\nnvbo->tile_mode = tile_mode;\r\nnvbo->tile_flags = tile_flags;\r\nnvbo->bo.bdev = &dev_priv->ttm.bdev;\r\nnvbo->page_shift = 12;\r\nif (dev_priv->bar1_vm) {\r\nif (!(flags & TTM_PL_FLAG_TT) && size > 256 * 1024)\r\nnvbo->page_shift = dev_priv->bar1_vm->lpg_shift;\r\n}\r\nnouveau_bo_fixup_align(nvbo, flags, &align, &size);\r\nnvbo->bo.mem.num_pages = size >> PAGE_SHIFT;\r\nnouveau_bo_placement_set(nvbo, flags, 0);\r\nacc_size = ttm_bo_dma_acc_size(&dev_priv->ttm.bdev, size,\r\nsizeof(struct nouveau_bo));\r\nret = ttm_bo_init(&dev_priv->ttm.bdev, &nvbo->bo, size,\r\ntype, &nvbo->placement,\r\nalign >> PAGE_SHIFT, 0, false, NULL, acc_size, sg,\r\nnouveau_bo_del_ttm);\r\nif (ret) {\r\nreturn ret;\r\n}\r\n*pnvbo = nvbo;\r\nreturn 0;\r\n}\r\nstatic void\r\nset_placement_list(uint32_t *pl, unsigned *n, uint32_t type, uint32_t flags)\r\n{\r\n*n = 0;\r\nif (type & TTM_PL_FLAG_VRAM)\r\npl[(*n)++] = TTM_PL_FLAG_VRAM | flags;\r\nif (type & TTM_PL_FLAG_TT)\r\npl[(*n)++] = TTM_PL_FLAG_TT | flags;\r\nif (type & TTM_PL_FLAG_SYSTEM)\r\npl[(*n)++] = TTM_PL_FLAG_SYSTEM | flags;\r\n}\r\nstatic void\r\nset_placement_range(struct nouveau_bo *nvbo, uint32_t type)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(nvbo->bo.bdev);\r\nint vram_pages = dev_priv->vram_size >> PAGE_SHIFT;\r\nif (dev_priv->card_type == NV_10 &&\r\nnvbo->tile_mode && (type & TTM_PL_FLAG_VRAM) &&\r\nnvbo->bo.mem.num_pages < vram_pages / 4) {\r\nif (nvbo->tile_flags & NOUVEAU_GEM_TILE_ZETA) {\r\nnvbo->placement.fpfn = vram_pages / 2;\r\nnvbo->placement.lpfn = ~0;\r\n} else {\r\nnvbo->placement.fpfn = 0;\r\nnvbo->placement.lpfn = vram_pages / 2;\r\n}\r\n}\r\n}\r\nvoid\r\nnouveau_bo_placement_set(struct nouveau_bo *nvbo, uint32_t type, uint32_t busy)\r\n{\r\nstruct ttm_placement *pl = &nvbo->placement;\r\nuint32_t flags = TTM_PL_MASK_CACHING |\r\n(nvbo->pin_refcnt ? TTM_PL_FLAG_NO_EVICT : 0);\r\npl->placement = nvbo->placements;\r\nset_placement_list(nvbo->placements, &pl->num_placement,\r\ntype, flags);\r\npl->busy_placement = nvbo->busy_placements;\r\nset_placement_list(nvbo->busy_placements, &pl->num_busy_placement,\r\ntype | busy, flags);\r\nset_placement_range(nvbo, type);\r\n}\r\nint\r\nnouveau_bo_pin(struct nouveau_bo *nvbo, uint32_t memtype)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(nvbo->bo.bdev);\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nint ret;\r\nif (nvbo->pin_refcnt && !(memtype & (1 << bo->mem.mem_type))) {\r\nNV_ERROR(nouveau_bdev(bo->bdev)->dev,\r\n"bo %p pinned elsewhere: 0x%08x vs 0x%08x\n", bo,\r\n1 << bo->mem.mem_type, memtype);\r\nreturn -EINVAL;\r\n}\r\nif (nvbo->pin_refcnt++)\r\nreturn 0;\r\nret = ttm_bo_reserve(bo, false, false, false, 0);\r\nif (ret)\r\ngoto out;\r\nnouveau_bo_placement_set(nvbo, memtype, 0);\r\nret = nouveau_bo_validate(nvbo, false, false, false);\r\nif (ret == 0) {\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\ndev_priv->fb_aper_free -= bo->mem.size;\r\nbreak;\r\ncase TTM_PL_TT:\r\ndev_priv->gart_info.aper_free -= bo->mem.size;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nttm_bo_unreserve(bo);\r\nout:\r\nif (unlikely(ret))\r\nnvbo->pin_refcnt--;\r\nreturn ret;\r\n}\r\nint\r\nnouveau_bo_unpin(struct nouveau_bo *nvbo)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(nvbo->bo.bdev);\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nint ret;\r\nif (--nvbo->pin_refcnt)\r\nreturn 0;\r\nret = ttm_bo_reserve(bo, false, false, false, 0);\r\nif (ret)\r\nreturn ret;\r\nnouveau_bo_placement_set(nvbo, bo->mem.placement, 0);\r\nret = nouveau_bo_validate(nvbo, false, false, false);\r\nif (ret == 0) {\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\ndev_priv->fb_aper_free += bo->mem.size;\r\nbreak;\r\ncase TTM_PL_TT:\r\ndev_priv->gart_info.aper_free += bo->mem.size;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nttm_bo_unreserve(bo);\r\nreturn ret;\r\n}\r\nint\r\nnouveau_bo_map(struct nouveau_bo *nvbo)\r\n{\r\nint ret;\r\nret = ttm_bo_reserve(&nvbo->bo, false, false, false, 0);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_bo_kmap(&nvbo->bo, 0, nvbo->bo.mem.num_pages, &nvbo->kmap);\r\nttm_bo_unreserve(&nvbo->bo);\r\nreturn ret;\r\n}\r\nvoid\r\nnouveau_bo_unmap(struct nouveau_bo *nvbo)\r\n{\r\nif (nvbo)\r\nttm_bo_kunmap(&nvbo->kmap);\r\n}\r\nint\r\nnouveau_bo_validate(struct nouveau_bo *nvbo, bool interruptible,\r\nbool no_wait_reserve, bool no_wait_gpu)\r\n{\r\nint ret;\r\nret = ttm_bo_validate(&nvbo->bo, &nvbo->placement, interruptible,\r\nno_wait_reserve, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nu16\r\nnouveau_bo_rd16(struct nouveau_bo *nvbo, unsigned index)\r\n{\r\nbool is_iomem;\r\nu16 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem = &mem[index];\r\nif (is_iomem)\r\nreturn ioread16_native((void __force __iomem *)mem);\r\nelse\r\nreturn *mem;\r\n}\r\nvoid\r\nnouveau_bo_wr16(struct nouveau_bo *nvbo, unsigned index, u16 val)\r\n{\r\nbool is_iomem;\r\nu16 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem = &mem[index];\r\nif (is_iomem)\r\niowrite16_native(val, (void __force __iomem *)mem);\r\nelse\r\n*mem = val;\r\n}\r\nu32\r\nnouveau_bo_rd32(struct nouveau_bo *nvbo, unsigned index)\r\n{\r\nbool is_iomem;\r\nu32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem = &mem[index];\r\nif (is_iomem)\r\nreturn ioread32_native((void __force __iomem *)mem);\r\nelse\r\nreturn *mem;\r\n}\r\nvoid\r\nnouveau_bo_wr32(struct nouveau_bo *nvbo, unsigned index, u32 val)\r\n{\r\nbool is_iomem;\r\nu32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem = &mem[index];\r\nif (is_iomem)\r\niowrite32_native(val, (void __force __iomem *)mem);\r\nelse\r\n*mem = val;\r\n}\r\nstatic struct ttm_tt *\r\nnouveau_ttm_tt_create(struct ttm_bo_device *bdev,\r\nunsigned long size, uint32_t page_flags,\r\nstruct page *dummy_read_page)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bdev);\r\nstruct drm_device *dev = dev_priv->dev;\r\nswitch (dev_priv->gart_info.type) {\r\n#if __OS_HAS_AGP\r\ncase NOUVEAU_GART_AGP:\r\nreturn ttm_agp_tt_create(bdev, dev->agp->bridge,\r\nsize, page_flags, dummy_read_page);\r\n#endif\r\ncase NOUVEAU_GART_PDMA:\r\ncase NOUVEAU_GART_HW:\r\nreturn nouveau_sgdma_create_ttm(bdev, size, page_flags,\r\ndummy_read_page);\r\ndefault:\r\nNV_ERROR(dev, "Unknown GART type %d\n",\r\ndev_priv->gart_info.type);\r\nbreak;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int\r\nnouveau_bo_invalidate_caches(struct ttm_bo_device *bdev, uint32_t flags)\r\n{\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_bo_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,\r\nstruct ttm_mem_type_manager *man)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bdev);\r\nstruct drm_device *dev = dev_priv->dev;\r\nswitch (type) {\r\ncase TTM_PL_SYSTEM:\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_MASK_CACHING;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ncase TTM_PL_VRAM:\r\nif (dev_priv->card_type >= NV_50) {\r\nman->func = &nouveau_vram_manager;\r\nman->io_reserve_fastpath = false;\r\nman->use_io_reserve_lru = true;\r\n} else {\r\nman->func = &ttm_bo_manager_func;\r\n}\r\nman->flags = TTM_MEMTYPE_FLAG_FIXED |\r\nTTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_WC;\r\nman->default_caching = TTM_PL_FLAG_WC;\r\nbreak;\r\ncase TTM_PL_TT:\r\nif (dev_priv->card_type >= NV_50)\r\nman->func = &nouveau_gart_manager;\r\nelse\r\nman->func = &ttm_bo_manager_func;\r\nswitch (dev_priv->gart_info.type) {\r\ncase NOUVEAU_GART_AGP:\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_WC;\r\nman->default_caching = TTM_PL_FLAG_WC;\r\nbreak;\r\ncase NOUVEAU_GART_PDMA:\r\ncase NOUVEAU_GART_HW:\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE |\r\nTTM_MEMTYPE_FLAG_CMA;\r\nman->available_caching = TTM_PL_MASK_CACHING;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ndefault:\r\nNV_ERROR(dev, "Unknown GART type: %d\n",\r\ndev_priv->gart_info.type);\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ndefault:\r\nNV_ERROR(dev, "Unsupported memory type %u\n", (unsigned)type);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_bo_evict_flags(struct ttm_buffer_object *bo, struct ttm_placement *pl)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_TT,\r\nTTM_PL_FLAG_SYSTEM);\r\nbreak;\r\ndefault:\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_SYSTEM, 0);\r\nbreak;\r\n}\r\n*pl = nvbo->placement;\r\n}\r\nstatic int\r\nnouveau_bo_move_accel_cleanup(struct nouveau_channel *chan,\r\nstruct nouveau_bo *nvbo, bool evict,\r\nbool no_wait_reserve, bool no_wait_gpu,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_fence *fence = NULL;\r\nint ret;\r\nret = nouveau_fence_new(chan, &fence);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_bo_move_accel_cleanup(&nvbo->bo, fence, NULL, evict,\r\nno_wait_reserve, no_wait_gpu, new_mem);\r\nnouveau_fence_unref(&fence);\r\nreturn ret;\r\n}\r\nstatic int\r\nnve0_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nint ret = RING_SPACE(chan, 10);\r\nif (ret == 0) {\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0400, 8);\r\nOUT_RING (chan, upper_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, upper_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, new_mem->num_pages);\r\nBEGIN_IMC0(chan, NvSubCopy, 0x0300, 0x0386);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnvc0_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = RING_SPACE(chan, 2);\r\nif (ret == 0) {\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnvc0_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nu64 src_offset = node->vma[0].offset;\r\nu64 dst_offset = node->vma[1].offset;\r\nu32 page_count = new_mem->num_pages;\r\nint ret;\r\npage_count = new_mem->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 8191) ? 8191 : page_count;\r\nret = RING_SPACE(chan, 11);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NVC0(chan, NvSubCopy, 0x030c, 8);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0300, 1);\r\nOUT_RING (chan, 0x00000110);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnvc0_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nu64 src_offset = node->vma[0].offset;\r\nu64 dst_offset = node->vma[1].offset;\r\nu32 page_count = new_mem->num_pages;\r\nint ret;\r\npage_count = new_mem->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 2047) ? 2047 : page_count;\r\nret = RING_SPACE(chan, 12);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0238, 2);\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nBEGIN_NVC0(chan, NvSubCopy, 0x030c, 6);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0300, 1);\r\nOUT_RING (chan, 0x00100110);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnva3_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nu64 src_offset = node->vma[0].offset;\r\nu64 dst_offset = node->vma[1].offset;\r\nu32 page_count = new_mem->num_pages;\r\nint ret;\r\npage_count = new_mem->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 8191) ? 8191 : page_count;\r\nret = RING_SPACE(chan, 11);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, 0x030c, 8);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nBEGIN_NV04(chan, NvSubCopy, 0x0300, 1);\r\nOUT_RING (chan, 0x00000110);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnv98_bo_move_exec(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nint ret = RING_SPACE(chan, 7);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0320, 6);\r\nOUT_RING (chan, upper_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, upper_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, 0x00000000 );\r\nOUT_RING (chan, new_mem->num_pages << PAGE_SHIFT);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnv84_bo_move_exec(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nint ret = RING_SPACE(chan, 7);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0304, 6);\r\nOUT_RING (chan, new_mem->num_pages << PAGE_SHIFT);\r\nOUT_RING (chan, upper_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, upper_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, 0x00000000 );\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnv50_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = nouveau_notifier_alloc(chan, NvNotify0, 32, 0xfe0, 0x1000,\r\n&chan->m2mf_ntfy);\r\nif (ret == 0) {\r\nret = RING_SPACE(chan, 6);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle);\r\nBEGIN_NV04(chan, NvSubCopy, 0x0180, 3);\r\nOUT_RING (chan, NvNotify0);\r\nOUT_RING (chan, NvDmaFB);\r\nOUT_RING (chan, NvDmaFB);\r\n} else {\r\nnouveau_ramht_remove(chan, NvNotify0);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnv50_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nu64 length = (new_mem->num_pages << PAGE_SHIFT);\r\nu64 src_offset = node->vma[0].offset;\r\nu64 dst_offset = node->vma[1].offset;\r\nint ret;\r\nwhile (length) {\r\nu32 amount, stride, height;\r\namount = min(length, (u64)(4 * 1024 * 1024));\r\nstride = 16 * 4;\r\nheight = amount / stride;\r\nif (new_mem->mem_type == TTM_PL_VRAM &&\r\nnouveau_bo_tile_layout(nvbo)) {\r\nret = RING_SPACE(chan, 8);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, 0x0200, 7);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, height);\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\n} else {\r\nret = RING_SPACE(chan, 2);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, 0x0200, 1);\r\nOUT_RING (chan, 1);\r\n}\r\nif (old_mem->mem_type == TTM_PL_VRAM &&\r\nnouveau_bo_tile_layout(nvbo)) {\r\nret = RING_SPACE(chan, 8);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, 0x021c, 7);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, height);\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\n} else {\r\nret = RING_SPACE(chan, 2);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, 0x021c, 1);\r\nOUT_RING (chan, 1);\r\n}\r\nret = RING_SPACE(chan, 14);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, 0x0238, 2);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nBEGIN_NV04(chan, NvSubCopy, 0x030c, 8);\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, height);\r\nOUT_RING (chan, 0x00000101);\r\nOUT_RING (chan, 0x00000000);\r\nBEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_NOP, 1);\r\nOUT_RING (chan, 0);\r\nlength -= amount;\r\nsrc_offset += amount;\r\ndst_offset += amount;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnv04_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = nouveau_notifier_alloc(chan, NvNotify0, 32, 0xfe0, 0x1000,\r\n&chan->m2mf_ntfy);\r\nif (ret == 0) {\r\nret = RING_SPACE(chan, 4);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle);\r\nBEGIN_NV04(chan, NvSubCopy, 0x0180, 1);\r\nOUT_RING (chan, NvNotify0);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic inline uint32_t\r\nnouveau_bo_mem_ctxdma(struct ttm_buffer_object *bo,\r\nstruct nouveau_channel *chan, struct ttm_mem_reg *mem)\r\n{\r\nif (mem->mem_type == TTM_PL_TT)\r\nreturn chan->gart_handle;\r\nreturn chan->vram_handle;\r\n}\r\nstatic int\r\nnv04_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nu32 src_offset = old_mem->start << PAGE_SHIFT;\r\nu32 dst_offset = new_mem->start << PAGE_SHIFT;\r\nu32 page_count = new_mem->num_pages;\r\nint ret;\r\nret = RING_SPACE(chan, 3);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_DMA_SOURCE, 2);\r\nOUT_RING (chan, nouveau_bo_mem_ctxdma(bo, chan, old_mem));\r\nOUT_RING (chan, nouveau_bo_mem_ctxdma(bo, chan, new_mem));\r\npage_count = new_mem->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 2047) ? 2047 : page_count;\r\nret = RING_SPACE(chan, 11);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy,\r\nNV_MEMORY_TO_MEMORY_FORMAT_OFFSET_IN, 8);\r\nOUT_RING (chan, src_offset);\r\nOUT_RING (chan, dst_offset);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nOUT_RING (chan, 0x00000101);\r\nOUT_RING (chan, 0x00000000);\r\nBEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_NOP, 1);\r\nOUT_RING (chan, 0);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_vma_getmap(struct nouveau_channel *chan, struct nouveau_bo *nvbo,\r\nstruct ttm_mem_reg *mem, struct nouveau_vma *vma)\r\n{\r\nstruct nouveau_mem *node = mem->mm_node;\r\nint ret;\r\nret = nouveau_vm_get(chan->vm, mem->num_pages << PAGE_SHIFT,\r\nnode->page_shift, NV_MEM_ACCESS_RO, vma);\r\nif (ret)\r\nreturn ret;\r\nif (mem->mem_type == TTM_PL_VRAM)\r\nnouveau_vm_map(vma, node);\r\nelse\r\nnouveau_vm_map_sg(vma, 0, mem->num_pages << PAGE_SHIFT, node);\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_bo_move_m2mf(struct ttm_buffer_object *bo, int evict, bool intr,\r\nbool no_wait_reserve, bool no_wait_gpu,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bo->bdev);\r\nstruct nouveau_channel *chan = chan = dev_priv->channel;\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nstruct ttm_mem_reg *old_mem = &bo->mem;\r\nint ret;\r\nmutex_lock_nested(&chan->mutex, NOUVEAU_KCHANNEL_MUTEX);\r\nif (dev_priv->card_type >= NV_50) {\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nret = nouveau_vma_getmap(chan, nvbo, old_mem, &node->vma[0]);\r\nif (ret)\r\ngoto out;\r\nret = nouveau_vma_getmap(chan, nvbo, new_mem, &node->vma[1]);\r\nif (ret)\r\ngoto out;\r\n}\r\nret = dev_priv->ttm.move(chan, bo, &bo->mem, new_mem);\r\nif (ret == 0) {\r\nret = nouveau_bo_move_accel_cleanup(chan, nvbo, evict,\r\nno_wait_reserve,\r\nno_wait_gpu, new_mem);\r\n}\r\nout:\r\nmutex_unlock(&chan->mutex);\r\nreturn ret;\r\n}\r\nvoid\r\nnouveau_bo_move_init(struct nouveau_channel *chan)\r\n{\r\nstruct drm_nouveau_private *dev_priv = chan->dev->dev_private;\r\nstatic const struct {\r\nconst char *name;\r\nint engine;\r\nu32 oclass;\r\nint (*exec)(struct nouveau_channel *,\r\nstruct ttm_buffer_object *,\r\nstruct ttm_mem_reg *, struct ttm_mem_reg *);\r\nint (*init)(struct nouveau_channel *, u32 handle);\r\n} _methods[] = {\r\n{ "COPY", 0, 0xa0b5, nve0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY1", 5, 0x90b8, nvc0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY0", 4, 0x90b5, nvc0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY", 0, 0x85b5, nva3_bo_move_copy, nv50_bo_move_init },\r\n{ "CRYPT", 0, 0x74c1, nv84_bo_move_exec, nv50_bo_move_init },\r\n{ "M2MF", 0, 0x9039, nvc0_bo_move_m2mf, nvc0_bo_move_init },\r\n{ "M2MF", 0, 0x5039, nv50_bo_move_m2mf, nv50_bo_move_init },\r\n{ "M2MF", 0, 0x0039, nv04_bo_move_m2mf, nv04_bo_move_init },\r\n{},\r\n{ "CRYPT", 0, 0x88b4, nv98_bo_move_exec, nv50_bo_move_init },\r\n}, *mthd = _methods;\r\nconst char *name = "CPU";\r\nint ret;\r\ndo {\r\nu32 handle = (mthd->engine << 16) | mthd->oclass;\r\nret = nouveau_gpuobj_gr_new(chan, handle, mthd->oclass);\r\nif (ret == 0) {\r\nret = mthd->init(chan, handle);\r\nif (ret == 0) {\r\ndev_priv->ttm.move = mthd->exec;\r\nname = mthd->name;\r\nbreak;\r\n}\r\n}\r\n} while ((++mthd)->exec);\r\nNV_INFO(chan->dev, "MM: using %s for buffer copies\n", name);\r\n}\r\nstatic int\r\nnouveau_bo_move_flipd(struct ttm_buffer_object *bo, bool evict, bool intr,\r\nbool no_wait_reserve, bool no_wait_gpu,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nu32 placement_memtype = TTM_PL_FLAG_TT | TTM_PL_MASK_CACHING;\r\nstruct ttm_placement placement;\r\nstruct ttm_mem_reg tmp_mem;\r\nint ret;\r\nplacement.fpfn = placement.lpfn = 0;\r\nplacement.num_placement = placement.num_busy_placement = 1;\r\nplacement.placement = placement.busy_placement = &placement_memtype;\r\ntmp_mem = *new_mem;\r\ntmp_mem.mm_node = NULL;\r\nret = ttm_bo_mem_space(bo, &placement, &tmp_mem, intr, no_wait_reserve, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_tt_bind(bo->ttm, &tmp_mem);\r\nif (ret)\r\ngoto out;\r\nret = nouveau_bo_move_m2mf(bo, true, intr, no_wait_reserve, no_wait_gpu, &tmp_mem);\r\nif (ret)\r\ngoto out;\r\nret = ttm_bo_move_ttm(bo, true, no_wait_reserve, no_wait_gpu, new_mem);\r\nout:\r\nttm_bo_mem_put(bo, &tmp_mem);\r\nreturn ret;\r\n}\r\nstatic int\r\nnouveau_bo_move_flips(struct ttm_buffer_object *bo, bool evict, bool intr,\r\nbool no_wait_reserve, bool no_wait_gpu,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nu32 placement_memtype = TTM_PL_FLAG_TT | TTM_PL_MASK_CACHING;\r\nstruct ttm_placement placement;\r\nstruct ttm_mem_reg tmp_mem;\r\nint ret;\r\nplacement.fpfn = placement.lpfn = 0;\r\nplacement.num_placement = placement.num_busy_placement = 1;\r\nplacement.placement = placement.busy_placement = &placement_memtype;\r\ntmp_mem = *new_mem;\r\ntmp_mem.mm_node = NULL;\r\nret = ttm_bo_mem_space(bo, &placement, &tmp_mem, intr, no_wait_reserve, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_bo_move_ttm(bo, true, no_wait_reserve, no_wait_gpu, &tmp_mem);\r\nif (ret)\r\ngoto out;\r\nret = nouveau_bo_move_m2mf(bo, true, intr, no_wait_reserve, no_wait_gpu, new_mem);\r\nif (ret)\r\ngoto out;\r\nout:\r\nttm_bo_mem_put(bo, &tmp_mem);\r\nreturn ret;\r\n}\r\nstatic void\r\nnouveau_bo_move_ntfy(struct ttm_buffer_object *bo, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nstruct nouveau_vma *vma;\r\nif (bo->destroy != nouveau_bo_del_ttm)\r\nreturn;\r\nlist_for_each_entry(vma, &nvbo->vma_list, head) {\r\nif (new_mem && new_mem->mem_type == TTM_PL_VRAM) {\r\nnouveau_vm_map(vma, new_mem->mm_node);\r\n} else\r\nif (new_mem && new_mem->mem_type == TTM_PL_TT &&\r\nnvbo->page_shift == vma->vm->spg_shift) {\r\nif (((struct nouveau_mem *)new_mem->mm_node)->sg)\r\nnouveau_vm_map_sg_table(vma, 0, new_mem->\r\nnum_pages << PAGE_SHIFT,\r\nnew_mem->mm_node);\r\nelse\r\nnouveau_vm_map_sg(vma, 0, new_mem->\r\nnum_pages << PAGE_SHIFT,\r\nnew_mem->mm_node);\r\n} else {\r\nnouveau_vm_unmap(vma);\r\n}\r\n}\r\n}\r\nstatic int\r\nnouveau_bo_vm_bind(struct ttm_buffer_object *bo, struct ttm_mem_reg *new_mem,\r\nstruct nouveau_tile_reg **new_tile)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bo->bdev);\r\nstruct drm_device *dev = dev_priv->dev;\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nu64 offset = new_mem->start << PAGE_SHIFT;\r\n*new_tile = NULL;\r\nif (new_mem->mem_type != TTM_PL_VRAM)\r\nreturn 0;\r\nif (dev_priv->card_type >= NV_10) {\r\n*new_tile = nv10_mem_set_tiling(dev, offset, new_mem->size,\r\nnvbo->tile_mode,\r\nnvbo->tile_flags);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_bo_vm_cleanup(struct ttm_buffer_object *bo,\r\nstruct nouveau_tile_reg *new_tile,\r\nstruct nouveau_tile_reg **old_tile)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bo->bdev);\r\nstruct drm_device *dev = dev_priv->dev;\r\nnv10_mem_put_tile_region(dev, *old_tile, bo->sync_obj);\r\n*old_tile = new_tile;\r\n}\r\nstatic int\r\nnouveau_bo_move(struct ttm_buffer_object *bo, bool evict, bool intr,\r\nbool no_wait_reserve, bool no_wait_gpu,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bo->bdev);\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nstruct ttm_mem_reg *old_mem = &bo->mem;\r\nstruct nouveau_tile_reg *new_tile = NULL;\r\nint ret = 0;\r\nif (dev_priv->card_type < NV_50) {\r\nret = nouveau_bo_vm_bind(bo, new_mem, &new_tile);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (old_mem->mem_type == TTM_PL_SYSTEM && !bo->ttm) {\r\nBUG_ON(bo->mem.mm_node != NULL);\r\nbo->mem = *new_mem;\r\nnew_mem->mm_node = NULL;\r\ngoto out;\r\n}\r\nif (!dev_priv->ttm.move) {\r\nret = ttm_bo_move_memcpy(bo, evict, no_wait_reserve, no_wait_gpu, new_mem);\r\ngoto out;\r\n}\r\nif (new_mem->mem_type == TTM_PL_SYSTEM)\r\nret = nouveau_bo_move_flipd(bo, evict, intr, no_wait_reserve, no_wait_gpu, new_mem);\r\nelse if (old_mem->mem_type == TTM_PL_SYSTEM)\r\nret = nouveau_bo_move_flips(bo, evict, intr, no_wait_reserve, no_wait_gpu, new_mem);\r\nelse\r\nret = nouveau_bo_move_m2mf(bo, evict, intr, no_wait_reserve, no_wait_gpu, new_mem);\r\nif (!ret)\r\ngoto out;\r\nret = ttm_bo_move_memcpy(bo, evict, no_wait_reserve, no_wait_gpu, new_mem);\r\nout:\r\nif (dev_priv->card_type < NV_50) {\r\nif (ret)\r\nnouveau_bo_vm_cleanup(bo, NULL, &new_tile);\r\nelse\r\nnouveau_bo_vm_cleanup(bo, new_tile, &nvbo->tile);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnouveau_bo_verify_access(struct ttm_buffer_object *bo, struct file *filp)\r\n{\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_ttm_io_mem_reserve(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)\r\n{\r\nstruct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bdev);\r\nstruct drm_device *dev = dev_priv->dev;\r\nint ret;\r\nmem->bus.addr = NULL;\r\nmem->bus.offset = 0;\r\nmem->bus.size = mem->num_pages << PAGE_SHIFT;\r\nmem->bus.base = 0;\r\nmem->bus.is_iomem = false;\r\nif (!(man->flags & TTM_MEMTYPE_FLAG_MAPPABLE))\r\nreturn -EINVAL;\r\nswitch (mem->mem_type) {\r\ncase TTM_PL_SYSTEM:\r\nreturn 0;\r\ncase TTM_PL_TT:\r\n#if __OS_HAS_AGP\r\nif (dev_priv->gart_info.type == NOUVEAU_GART_AGP) {\r\nmem->bus.offset = mem->start << PAGE_SHIFT;\r\nmem->bus.base = dev_priv->gart_info.aper_base;\r\nmem->bus.is_iomem = true;\r\n}\r\n#endif\r\nbreak;\r\ncase TTM_PL_VRAM:\r\n{\r\nstruct nouveau_mem *node = mem->mm_node;\r\nu8 page_shift;\r\nif (!dev_priv->bar1_vm) {\r\nmem->bus.offset = mem->start << PAGE_SHIFT;\r\nmem->bus.base = pci_resource_start(dev->pdev, 1);\r\nmem->bus.is_iomem = true;\r\nbreak;\r\n}\r\nif (dev_priv->card_type >= NV_C0)\r\npage_shift = node->page_shift;\r\nelse\r\npage_shift = 12;\r\nret = nouveau_vm_get(dev_priv->bar1_vm, mem->bus.size,\r\npage_shift, NV_MEM_ACCESS_RW,\r\n&node->bar_vma);\r\nif (ret)\r\nreturn ret;\r\nnouveau_vm_map(&node->bar_vma, node);\r\nif (ret) {\r\nnouveau_vm_put(&node->bar_vma);\r\nreturn ret;\r\n}\r\nmem->bus.offset = node->bar_vma.offset;\r\nif (dev_priv->card_type == NV_50)\r\nmem->bus.offset -= 0x0020000000ULL;\r\nmem->bus.base = pci_resource_start(dev->pdev, 1);\r\nmem->bus.is_iomem = true;\r\n}\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_ttm_io_mem_free(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bdev);\r\nstruct nouveau_mem *node = mem->mm_node;\r\nif (!dev_priv->bar1_vm || mem->mem_type != TTM_PL_VRAM)\r\nreturn;\r\nif (!node->bar_vma.node)\r\nreturn;\r\nnouveau_vm_unmap(&node->bar_vma);\r\nnouveau_vm_put(&node->bar_vma);\r\n}\r\nstatic int\r\nnouveau_ttm_fault_reserve_notify(struct ttm_buffer_object *bo)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bo->bdev);\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nif (bo->mem.mem_type != TTM_PL_VRAM) {\r\nif (dev_priv->card_type < NV_50 ||\r\n!nouveau_bo_tile_layout(nvbo))\r\nreturn 0;\r\n}\r\nif (bo->mem.start + bo->mem.num_pages < dev_priv->fb_mappable_pages)\r\nreturn 0;\r\nnvbo->placement.fpfn = 0;\r\nnvbo->placement.lpfn = dev_priv->fb_mappable_pages;\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_VRAM, 0);\r\nreturn nouveau_bo_validate(nvbo, false, true, false);\r\n}\r\nstatic int\r\nnouveau_ttm_tt_populate(struct ttm_tt *ttm)\r\n{\r\nstruct ttm_dma_tt *ttm_dma = (void *)ttm;\r\nstruct drm_nouveau_private *dev_priv;\r\nstruct drm_device *dev;\r\nunsigned i;\r\nint r;\r\nbool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);\r\nif (ttm->state != tt_unpopulated)\r\nreturn 0;\r\nif (slave && ttm->sg) {\r\ndrm_prime_sg_to_page_addr_arrays(ttm->sg, ttm->pages,\r\nttm_dma->dma_address, ttm->num_pages);\r\nttm->state = tt_unbound;\r\nreturn 0;\r\n}\r\ndev_priv = nouveau_bdev(ttm->bdev);\r\ndev = dev_priv->dev;\r\n#if __OS_HAS_AGP\r\nif (dev_priv->gart_info.type == NOUVEAU_GART_AGP) {\r\nreturn ttm_agp_tt_populate(ttm);\r\n}\r\n#endif\r\n#ifdef CONFIG_SWIOTLB\r\nif (swiotlb_nr_tbl()) {\r\nreturn ttm_dma_populate((void *)ttm, dev->dev);\r\n}\r\n#endif\r\nr = ttm_pool_populate(ttm);\r\nif (r) {\r\nreturn r;\r\n}\r\nfor (i = 0; i < ttm->num_pages; i++) {\r\nttm_dma->dma_address[i] = pci_map_page(dev->pdev, ttm->pages[i],\r\n0, PAGE_SIZE,\r\nPCI_DMA_BIDIRECTIONAL);\r\nif (pci_dma_mapping_error(dev->pdev, ttm_dma->dma_address[i])) {\r\nwhile (--i) {\r\npci_unmap_page(dev->pdev, ttm_dma->dma_address[i],\r\nPAGE_SIZE, PCI_DMA_BIDIRECTIONAL);\r\nttm_dma->dma_address[i] = 0;\r\n}\r\nttm_pool_unpopulate(ttm);\r\nreturn -EFAULT;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_ttm_tt_unpopulate(struct ttm_tt *ttm)\r\n{\r\nstruct ttm_dma_tt *ttm_dma = (void *)ttm;\r\nstruct drm_nouveau_private *dev_priv;\r\nstruct drm_device *dev;\r\nunsigned i;\r\nbool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);\r\nif (slave)\r\nreturn;\r\ndev_priv = nouveau_bdev(ttm->bdev);\r\ndev = dev_priv->dev;\r\n#if __OS_HAS_AGP\r\nif (dev_priv->gart_info.type == NOUVEAU_GART_AGP) {\r\nttm_agp_tt_unpopulate(ttm);\r\nreturn;\r\n}\r\n#endif\r\n#ifdef CONFIG_SWIOTLB\r\nif (swiotlb_nr_tbl()) {\r\nttm_dma_unpopulate((void *)ttm, dev->dev);\r\nreturn;\r\n}\r\n#endif\r\nfor (i = 0; i < ttm->num_pages; i++) {\r\nif (ttm_dma->dma_address[i]) {\r\npci_unmap_page(dev->pdev, ttm_dma->dma_address[i],\r\nPAGE_SIZE, PCI_DMA_BIDIRECTIONAL);\r\n}\r\n}\r\nttm_pool_unpopulate(ttm);\r\n}\r\nvoid\r\nnouveau_bo_fence(struct nouveau_bo *nvbo, struct nouveau_fence *fence)\r\n{\r\nstruct nouveau_fence *old_fence = NULL;\r\nif (likely(fence))\r\nnouveau_fence_ref(fence);\r\nspin_lock(&nvbo->bo.bdev->fence_lock);\r\nold_fence = nvbo->bo.sync_obj;\r\nnvbo->bo.sync_obj = fence;\r\nspin_unlock(&nvbo->bo.bdev->fence_lock);\r\nnouveau_fence_unref(&old_fence);\r\n}\r\nstatic void\r\nnouveau_bo_fence_unref(void **sync_obj)\r\n{\r\nnouveau_fence_unref((struct nouveau_fence **)sync_obj);\r\n}\r\nstatic void *\r\nnouveau_bo_fence_ref(void *sync_obj)\r\n{\r\nreturn nouveau_fence_ref(sync_obj);\r\n}\r\nstatic bool\r\nnouveau_bo_fence_signalled(void *sync_obj, void *sync_arg)\r\n{\r\nreturn nouveau_fence_done(sync_obj);\r\n}\r\nstatic int\r\nnouveau_bo_fence_wait(void *sync_obj, void *sync_arg, bool lazy, bool intr)\r\n{\r\nreturn nouveau_fence_wait(sync_obj, lazy, intr);\r\n}\r\nstatic int\r\nnouveau_bo_fence_flush(void *sync_obj, void *sync_arg)\r\n{\r\nreturn 0;\r\n}\r\nstruct nouveau_vma *\r\nnouveau_bo_vma_find(struct nouveau_bo *nvbo, struct nouveau_vm *vm)\r\n{\r\nstruct nouveau_vma *vma;\r\nlist_for_each_entry(vma, &nvbo->vma_list, head) {\r\nif (vma->vm == vm)\r\nreturn vma;\r\n}\r\nreturn NULL;\r\n}\r\nint\r\nnouveau_bo_vma_add(struct nouveau_bo *nvbo, struct nouveau_vm *vm,\r\nstruct nouveau_vma *vma)\r\n{\r\nconst u32 size = nvbo->bo.mem.num_pages << PAGE_SHIFT;\r\nstruct nouveau_mem *node = nvbo->bo.mem.mm_node;\r\nint ret;\r\nret = nouveau_vm_get(vm, size, nvbo->page_shift,\r\nNV_MEM_ACCESS_RW, vma);\r\nif (ret)\r\nreturn ret;\r\nif (nvbo->bo.mem.mem_type == TTM_PL_VRAM)\r\nnouveau_vm_map(vma, nvbo->bo.mem.mm_node);\r\nelse if (nvbo->bo.mem.mem_type == TTM_PL_TT) {\r\nif (node->sg)\r\nnouveau_vm_map_sg_table(vma, 0, size, node);\r\nelse\r\nnouveau_vm_map_sg(vma, 0, size, node);\r\n}\r\nlist_add_tail(&vma->head, &nvbo->vma_list);\r\nvma->refcount = 1;\r\nreturn 0;\r\n}\r\nvoid\r\nnouveau_bo_vma_del(struct nouveau_bo *nvbo, struct nouveau_vma *vma)\r\n{\r\nif (vma->node) {\r\nif (nvbo->bo.mem.mem_type != TTM_PL_SYSTEM) {\r\nspin_lock(&nvbo->bo.bdev->fence_lock);\r\nttm_bo_wait(&nvbo->bo, false, false, false);\r\nspin_unlock(&nvbo->bo.bdev->fence_lock);\r\nnouveau_vm_unmap(vma);\r\n}\r\nnouveau_vm_put(vma);\r\nlist_del(&vma->head);\r\n}\r\n}
