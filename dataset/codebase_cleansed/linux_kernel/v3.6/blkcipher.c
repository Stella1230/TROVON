static inline void blkcipher_map_src(struct blkcipher_walk *walk)\r\n{\r\nwalk->src.virt.addr = scatterwalk_map(&walk->in);\r\n}\r\nstatic inline void blkcipher_map_dst(struct blkcipher_walk *walk)\r\n{\r\nwalk->dst.virt.addr = scatterwalk_map(&walk->out);\r\n}\r\nstatic inline void blkcipher_unmap_src(struct blkcipher_walk *walk)\r\n{\r\nscatterwalk_unmap(walk->src.virt.addr);\r\n}\r\nstatic inline void blkcipher_unmap_dst(struct blkcipher_walk *walk)\r\n{\r\nscatterwalk_unmap(walk->dst.virt.addr);\r\n}\r\nstatic inline u8 *blkcipher_get_spot(u8 *start, unsigned int len)\r\n{\r\nu8 *end_page = (u8 *)(((unsigned long)(start + len - 1)) & PAGE_MASK);\r\nreturn max(start, end_page);\r\n}\r\nstatic inline unsigned int blkcipher_done_slow(struct crypto_blkcipher *tfm,\r\nstruct blkcipher_walk *walk,\r\nunsigned int bsize)\r\n{\r\nu8 *addr;\r\nunsigned int alignmask = crypto_blkcipher_alignmask(tfm);\r\naddr = (u8 *)ALIGN((unsigned long)walk->buffer, alignmask + 1);\r\naddr = blkcipher_get_spot(addr, bsize);\r\nscatterwalk_copychunks(addr, &walk->out, bsize, 1);\r\nreturn bsize;\r\n}\r\nstatic inline unsigned int blkcipher_done_fast(struct blkcipher_walk *walk,\r\nunsigned int n)\r\n{\r\nif (walk->flags & BLKCIPHER_WALK_COPY) {\r\nblkcipher_map_dst(walk);\r\nmemcpy(walk->dst.virt.addr, walk->page, n);\r\nblkcipher_unmap_dst(walk);\r\n} else if (!(walk->flags & BLKCIPHER_WALK_PHYS)) {\r\nif (walk->flags & BLKCIPHER_WALK_DIFF)\r\nblkcipher_unmap_dst(walk);\r\nblkcipher_unmap_src(walk);\r\n}\r\nscatterwalk_advance(&walk->in, n);\r\nscatterwalk_advance(&walk->out, n);\r\nreturn n;\r\n}\r\nint blkcipher_walk_done(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk, int err)\r\n{\r\nstruct crypto_blkcipher *tfm = desc->tfm;\r\nunsigned int nbytes = 0;\r\nif (likely(err >= 0)) {\r\nunsigned int n = walk->nbytes - err;\r\nif (likely(!(walk->flags & BLKCIPHER_WALK_SLOW)))\r\nn = blkcipher_done_fast(walk, n);\r\nelse if (WARN_ON(err)) {\r\nerr = -EINVAL;\r\ngoto err;\r\n} else\r\nn = blkcipher_done_slow(tfm, walk, n);\r\nnbytes = walk->total - n;\r\nerr = 0;\r\n}\r\nscatterwalk_done(&walk->in, 0, nbytes);\r\nscatterwalk_done(&walk->out, 1, nbytes);\r\nerr:\r\nwalk->total = nbytes;\r\nwalk->nbytes = nbytes;\r\nif (nbytes) {\r\ncrypto_yield(desc->flags);\r\nreturn blkcipher_walk_next(desc, walk);\r\n}\r\nif (walk->iv != desc->info)\r\nmemcpy(desc->info, walk->iv, crypto_blkcipher_ivsize(tfm));\r\nif (walk->buffer != walk->page)\r\nkfree(walk->buffer);\r\nif (walk->page)\r\nfree_page((unsigned long)walk->page);\r\nreturn err;\r\n}\r\nstatic inline int blkcipher_next_slow(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk,\r\nunsigned int bsize,\r\nunsigned int alignmask)\r\n{\r\nunsigned int n;\r\nunsigned aligned_bsize = ALIGN(bsize, alignmask + 1);\r\nif (walk->buffer)\r\ngoto ok;\r\nwalk->buffer = walk->page;\r\nif (walk->buffer)\r\ngoto ok;\r\nn = aligned_bsize * 3 - (alignmask + 1) +\r\n(alignmask & ~(crypto_tfm_ctx_alignment() - 1));\r\nwalk->buffer = kmalloc(n, GFP_ATOMIC);\r\nif (!walk->buffer)\r\nreturn blkcipher_walk_done(desc, walk, -ENOMEM);\r\nok:\r\nwalk->dst.virt.addr = (u8 *)ALIGN((unsigned long)walk->buffer,\r\nalignmask + 1);\r\nwalk->dst.virt.addr = blkcipher_get_spot(walk->dst.virt.addr, bsize);\r\nwalk->src.virt.addr = blkcipher_get_spot(walk->dst.virt.addr +\r\naligned_bsize, bsize);\r\nscatterwalk_copychunks(walk->src.virt.addr, &walk->in, bsize, 0);\r\nwalk->nbytes = bsize;\r\nwalk->flags |= BLKCIPHER_WALK_SLOW;\r\nreturn 0;\r\n}\r\nstatic inline int blkcipher_next_copy(struct blkcipher_walk *walk)\r\n{\r\nu8 *tmp = walk->page;\r\nblkcipher_map_src(walk);\r\nmemcpy(tmp, walk->src.virt.addr, walk->nbytes);\r\nblkcipher_unmap_src(walk);\r\nwalk->src.virt.addr = tmp;\r\nwalk->dst.virt.addr = tmp;\r\nreturn 0;\r\n}\r\nstatic inline int blkcipher_next_fast(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nunsigned long diff;\r\nwalk->src.phys.page = scatterwalk_page(&walk->in);\r\nwalk->src.phys.offset = offset_in_page(walk->in.offset);\r\nwalk->dst.phys.page = scatterwalk_page(&walk->out);\r\nwalk->dst.phys.offset = offset_in_page(walk->out.offset);\r\nif (walk->flags & BLKCIPHER_WALK_PHYS)\r\nreturn 0;\r\ndiff = walk->src.phys.offset - walk->dst.phys.offset;\r\ndiff |= walk->src.virt.page - walk->dst.virt.page;\r\nblkcipher_map_src(walk);\r\nwalk->dst.virt.addr = walk->src.virt.addr;\r\nif (diff) {\r\nwalk->flags |= BLKCIPHER_WALK_DIFF;\r\nblkcipher_map_dst(walk);\r\n}\r\nreturn 0;\r\n}\r\nstatic int blkcipher_walk_next(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct crypto_blkcipher *tfm = desc->tfm;\r\nunsigned int alignmask = crypto_blkcipher_alignmask(tfm);\r\nunsigned int bsize;\r\nunsigned int n;\r\nint err;\r\nn = walk->total;\r\nif (unlikely(n < crypto_blkcipher_blocksize(tfm))) {\r\ndesc->flags |= CRYPTO_TFM_RES_BAD_BLOCK_LEN;\r\nreturn blkcipher_walk_done(desc, walk, -EINVAL);\r\n}\r\nwalk->flags &= ~(BLKCIPHER_WALK_SLOW | BLKCIPHER_WALK_COPY |\r\nBLKCIPHER_WALK_DIFF);\r\nif (!scatterwalk_aligned(&walk->in, alignmask) ||\r\n!scatterwalk_aligned(&walk->out, alignmask)) {\r\nwalk->flags |= BLKCIPHER_WALK_COPY;\r\nif (!walk->page) {\r\nwalk->page = (void *)__get_free_page(GFP_ATOMIC);\r\nif (!walk->page)\r\nn = 0;\r\n}\r\n}\r\nbsize = min(walk->blocksize, n);\r\nn = scatterwalk_clamp(&walk->in, n);\r\nn = scatterwalk_clamp(&walk->out, n);\r\nif (unlikely(n < bsize)) {\r\nerr = blkcipher_next_slow(desc, walk, bsize, alignmask);\r\ngoto set_phys_lowmem;\r\n}\r\nwalk->nbytes = n;\r\nif (walk->flags & BLKCIPHER_WALK_COPY) {\r\nerr = blkcipher_next_copy(walk);\r\ngoto set_phys_lowmem;\r\n}\r\nreturn blkcipher_next_fast(desc, walk);\r\nset_phys_lowmem:\r\nif (walk->flags & BLKCIPHER_WALK_PHYS) {\r\nwalk->src.phys.page = virt_to_page(walk->src.virt.addr);\r\nwalk->dst.phys.page = virt_to_page(walk->dst.virt.addr);\r\nwalk->src.phys.offset &= PAGE_SIZE - 1;\r\nwalk->dst.phys.offset &= PAGE_SIZE - 1;\r\n}\r\nreturn err;\r\n}\r\nstatic inline int blkcipher_copy_iv(struct blkcipher_walk *walk,\r\nstruct crypto_blkcipher *tfm,\r\nunsigned int alignmask)\r\n{\r\nunsigned bs = walk->blocksize;\r\nunsigned int ivsize = crypto_blkcipher_ivsize(tfm);\r\nunsigned aligned_bs = ALIGN(bs, alignmask + 1);\r\nunsigned int size = aligned_bs * 2 + ivsize + max(aligned_bs, ivsize) -\r\n(alignmask + 1);\r\nu8 *iv;\r\nsize += alignmask & ~(crypto_tfm_ctx_alignment() - 1);\r\nwalk->buffer = kmalloc(size, GFP_ATOMIC);\r\nif (!walk->buffer)\r\nreturn -ENOMEM;\r\niv = (u8 *)ALIGN((unsigned long)walk->buffer, alignmask + 1);\r\niv = blkcipher_get_spot(iv, bs) + aligned_bs;\r\niv = blkcipher_get_spot(iv, bs) + aligned_bs;\r\niv = blkcipher_get_spot(iv, ivsize);\r\nwalk->iv = memcpy(iv, walk->iv, ivsize);\r\nreturn 0;\r\n}\r\nint blkcipher_walk_virt(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nwalk->flags &= ~BLKCIPHER_WALK_PHYS;\r\nwalk->blocksize = crypto_blkcipher_blocksize(desc->tfm);\r\nreturn blkcipher_walk_first(desc, walk);\r\n}\r\nint blkcipher_walk_phys(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nwalk->flags |= BLKCIPHER_WALK_PHYS;\r\nwalk->blocksize = crypto_blkcipher_blocksize(desc->tfm);\r\nreturn blkcipher_walk_first(desc, walk);\r\n}\r\nstatic int blkcipher_walk_first(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct crypto_blkcipher *tfm = desc->tfm;\r\nunsigned int alignmask = crypto_blkcipher_alignmask(tfm);\r\nif (WARN_ON_ONCE(in_irq()))\r\nreturn -EDEADLK;\r\nwalk->nbytes = walk->total;\r\nif (unlikely(!walk->total))\r\nreturn 0;\r\nwalk->buffer = NULL;\r\nwalk->iv = desc->info;\r\nif (unlikely(((unsigned long)walk->iv & alignmask))) {\r\nint err = blkcipher_copy_iv(walk, tfm, alignmask);\r\nif (err)\r\nreturn err;\r\n}\r\nscatterwalk_start(&walk->in, walk->in.sg);\r\nscatterwalk_start(&walk->out, walk->out.sg);\r\nwalk->page = NULL;\r\nreturn blkcipher_walk_next(desc, walk);\r\n}\r\nint blkcipher_walk_virt_block(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk,\r\nunsigned int blocksize)\r\n{\r\nwalk->flags &= ~BLKCIPHER_WALK_PHYS;\r\nwalk->blocksize = blocksize;\r\nreturn blkcipher_walk_first(desc, walk);\r\n}\r\nstatic int setkey_unaligned(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct blkcipher_alg *cipher = &tfm->__crt_alg->cra_blkcipher;\r\nunsigned long alignmask = crypto_tfm_alg_alignmask(tfm);\r\nint ret;\r\nu8 *buffer, *alignbuffer;\r\nunsigned long absize;\r\nabsize = keylen + alignmask;\r\nbuffer = kmalloc(absize, GFP_ATOMIC);\r\nif (!buffer)\r\nreturn -ENOMEM;\r\nalignbuffer = (u8 *)ALIGN((unsigned long)buffer, alignmask + 1);\r\nmemcpy(alignbuffer, key, keylen);\r\nret = cipher->setkey(tfm, alignbuffer, keylen);\r\nmemset(alignbuffer, 0, keylen);\r\nkfree(buffer);\r\nreturn ret;\r\n}\r\nstatic int setkey(struct crypto_tfm *tfm, const u8 *key, unsigned int keylen)\r\n{\r\nstruct blkcipher_alg *cipher = &tfm->__crt_alg->cra_blkcipher;\r\nunsigned long alignmask = crypto_tfm_alg_alignmask(tfm);\r\nif (keylen < cipher->min_keysize || keylen > cipher->max_keysize) {\r\ntfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nif ((unsigned long)key & alignmask)\r\nreturn setkey_unaligned(tfm, key, keylen);\r\nreturn cipher->setkey(tfm, key, keylen);\r\n}\r\nstatic int async_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nreturn setkey(crypto_ablkcipher_tfm(tfm), key, keylen);\r\n}\r\nstatic int async_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm = req->base.tfm;\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\nstruct blkcipher_desc desc = {\r\n.tfm = __crypto_blkcipher_cast(tfm),\r\n.info = req->info,\r\n.flags = req->base.flags,\r\n};\r\nreturn alg->encrypt(&desc, req->dst, req->src, req->nbytes);\r\n}\r\nstatic int async_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm = req->base.tfm;\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\nstruct blkcipher_desc desc = {\r\n.tfm = __crypto_blkcipher_cast(tfm),\r\n.info = req->info,\r\n.flags = req->base.flags,\r\n};\r\nreturn alg->decrypt(&desc, req->dst, req->src, req->nbytes);\r\n}\r\nstatic unsigned int crypto_blkcipher_ctxsize(struct crypto_alg *alg, u32 type,\r\nu32 mask)\r\n{\r\nstruct blkcipher_alg *cipher = &alg->cra_blkcipher;\r\nunsigned int len = alg->cra_ctxsize;\r\nif ((mask & CRYPTO_ALG_TYPE_MASK) == CRYPTO_ALG_TYPE_MASK &&\r\ncipher->ivsize) {\r\nlen = ALIGN(len, (unsigned long)alg->cra_alignmask + 1);\r\nlen += cipher->ivsize;\r\n}\r\nreturn len;\r\n}\r\nstatic int crypto_init_blkcipher_ops_async(struct crypto_tfm *tfm)\r\n{\r\nstruct ablkcipher_tfm *crt = &tfm->crt_ablkcipher;\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\ncrt->setkey = async_setkey;\r\ncrt->encrypt = async_encrypt;\r\ncrt->decrypt = async_decrypt;\r\nif (!alg->ivsize) {\r\ncrt->givencrypt = skcipher_null_givencrypt;\r\ncrt->givdecrypt = skcipher_null_givdecrypt;\r\n}\r\ncrt->base = __crypto_ablkcipher_cast(tfm);\r\ncrt->ivsize = alg->ivsize;\r\nreturn 0;\r\n}\r\nstatic int crypto_init_blkcipher_ops_sync(struct crypto_tfm *tfm)\r\n{\r\nstruct blkcipher_tfm *crt = &tfm->crt_blkcipher;\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\nunsigned long align = crypto_tfm_alg_alignmask(tfm) + 1;\r\nunsigned long addr;\r\ncrt->setkey = setkey;\r\ncrt->encrypt = alg->encrypt;\r\ncrt->decrypt = alg->decrypt;\r\naddr = (unsigned long)crypto_tfm_ctx(tfm);\r\naddr = ALIGN(addr, align);\r\naddr += ALIGN(tfm->__crt_alg->cra_ctxsize, align);\r\ncrt->iv = (void *)addr;\r\nreturn 0;\r\n}\r\nstatic int crypto_init_blkcipher_ops(struct crypto_tfm *tfm, u32 type, u32 mask)\r\n{\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\nif (alg->ivsize > PAGE_SIZE / 8)\r\nreturn -EINVAL;\r\nif ((mask & CRYPTO_ALG_TYPE_MASK) == CRYPTO_ALG_TYPE_MASK)\r\nreturn crypto_init_blkcipher_ops_sync(tfm);\r\nelse\r\nreturn crypto_init_blkcipher_ops_async(tfm);\r\n}\r\nstatic int crypto_blkcipher_report(struct sk_buff *skb, struct crypto_alg *alg)\r\n{\r\nstruct crypto_report_blkcipher rblkcipher;\r\nsnprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, "%s", "blkcipher");\r\nsnprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, "%s",\r\nalg->cra_blkcipher.geniv ?: "<default>");\r\nrblkcipher.blocksize = alg->cra_blocksize;\r\nrblkcipher.min_keysize = alg->cra_blkcipher.min_keysize;\r\nrblkcipher.max_keysize = alg->cra_blkcipher.max_keysize;\r\nrblkcipher.ivsize = alg->cra_blkcipher.ivsize;\r\nif (nla_put(skb, CRYPTOCFGA_REPORT_BLKCIPHER,\r\nsizeof(struct crypto_report_blkcipher), &rblkcipher))\r\ngoto nla_put_failure;\r\nreturn 0;\r\nnla_put_failure:\r\nreturn -EMSGSIZE;\r\n}\r\nstatic int crypto_blkcipher_report(struct sk_buff *skb, struct crypto_alg *alg)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic void crypto_blkcipher_show(struct seq_file *m, struct crypto_alg *alg)\r\n{\r\nseq_printf(m, "type : blkcipher\n");\r\nseq_printf(m, "blocksize : %u\n", alg->cra_blocksize);\r\nseq_printf(m, "min keysize : %u\n", alg->cra_blkcipher.min_keysize);\r\nseq_printf(m, "max keysize : %u\n", alg->cra_blkcipher.max_keysize);\r\nseq_printf(m, "ivsize : %u\n", alg->cra_blkcipher.ivsize);\r\nseq_printf(m, "geniv : %s\n", alg->cra_blkcipher.geniv ?:\r\n"<default>");\r\n}\r\nstatic int crypto_grab_nivcipher(struct crypto_skcipher_spawn *spawn,\r\nconst char *name, u32 type, u32 mask)\r\n{\r\nstruct crypto_alg *alg;\r\nint err;\r\ntype = crypto_skcipher_type(type);\r\nmask = crypto_skcipher_mask(mask)| CRYPTO_ALG_GENIV;\r\nalg = crypto_alg_mod_lookup(name, type, mask);\r\nif (IS_ERR(alg))\r\nreturn PTR_ERR(alg);\r\nerr = crypto_init_spawn(&spawn->base, alg, spawn->base.inst, mask);\r\ncrypto_mod_put(alg);\r\nreturn err;\r\n}\r\nstruct crypto_instance *skcipher_geniv_alloc(struct crypto_template *tmpl,\r\nstruct rtattr **tb, u32 type,\r\nu32 mask)\r\n{\r\nstruct {\r\nint (*setkey)(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int keylen);\r\nint (*encrypt)(struct ablkcipher_request *req);\r\nint (*decrypt)(struct ablkcipher_request *req);\r\nunsigned int min_keysize;\r\nunsigned int max_keysize;\r\nunsigned int ivsize;\r\nconst char *geniv;\r\n} balg;\r\nconst char *name;\r\nstruct crypto_skcipher_spawn *spawn;\r\nstruct crypto_attr_type *algt;\r\nstruct crypto_instance *inst;\r\nstruct crypto_alg *alg;\r\nint err;\r\nalgt = crypto_get_attr_type(tb);\r\nerr = PTR_ERR(algt);\r\nif (IS_ERR(algt))\r\nreturn ERR_PTR(err);\r\nif ((algt->type ^ (CRYPTO_ALG_TYPE_GIVCIPHER | CRYPTO_ALG_GENIV)) &\r\nalgt->mask)\r\nreturn ERR_PTR(-EINVAL);\r\nname = crypto_attr_alg_name(tb[1]);\r\nerr = PTR_ERR(name);\r\nif (IS_ERR(name))\r\nreturn ERR_PTR(err);\r\ninst = kzalloc(sizeof(*inst) + sizeof(*spawn), GFP_KERNEL);\r\nif (!inst)\r\nreturn ERR_PTR(-ENOMEM);\r\nspawn = crypto_instance_ctx(inst);\r\nmask |= crypto_requires_sync(algt->type, algt->mask);\r\ncrypto_set_skcipher_spawn(spawn, inst);\r\nerr = crypto_grab_nivcipher(spawn, name, type, mask);\r\nif (err)\r\ngoto err_free_inst;\r\nalg = crypto_skcipher_spawn_alg(spawn);\r\nif ((alg->cra_flags & CRYPTO_ALG_TYPE_MASK) ==\r\nCRYPTO_ALG_TYPE_BLKCIPHER) {\r\nbalg.ivsize = alg->cra_blkcipher.ivsize;\r\nbalg.min_keysize = alg->cra_blkcipher.min_keysize;\r\nbalg.max_keysize = alg->cra_blkcipher.max_keysize;\r\nbalg.setkey = async_setkey;\r\nbalg.encrypt = async_encrypt;\r\nbalg.decrypt = async_decrypt;\r\nbalg.geniv = alg->cra_blkcipher.geniv;\r\n} else {\r\nbalg.ivsize = alg->cra_ablkcipher.ivsize;\r\nbalg.min_keysize = alg->cra_ablkcipher.min_keysize;\r\nbalg.max_keysize = alg->cra_ablkcipher.max_keysize;\r\nbalg.setkey = alg->cra_ablkcipher.setkey;\r\nbalg.encrypt = alg->cra_ablkcipher.encrypt;\r\nbalg.decrypt = alg->cra_ablkcipher.decrypt;\r\nbalg.geniv = alg->cra_ablkcipher.geniv;\r\n}\r\nerr = -EINVAL;\r\nif (!balg.ivsize)\r\ngoto err_drop_alg;\r\nif (algt->mask & CRYPTO_ALG_GENIV) {\r\nif (!balg.geniv)\r\nbalg.geniv = crypto_default_geniv(alg);\r\nerr = -EAGAIN;\r\nif (strcmp(tmpl->name, balg.geniv))\r\ngoto err_drop_alg;\r\nmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\r\nmemcpy(inst->alg.cra_driver_name, alg->cra_driver_name,\r\nCRYPTO_MAX_ALG_NAME);\r\n} else {\r\nerr = -ENAMETOOLONG;\r\nif (snprintf(inst->alg.cra_name, CRYPTO_MAX_ALG_NAME,\r\n"%s(%s)", tmpl->name, alg->cra_name) >=\r\nCRYPTO_MAX_ALG_NAME)\r\ngoto err_drop_alg;\r\nif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\r\n"%s(%s)", tmpl->name, alg->cra_driver_name) >=\r\nCRYPTO_MAX_ALG_NAME)\r\ngoto err_drop_alg;\r\n}\r\ninst->alg.cra_flags = CRYPTO_ALG_TYPE_GIVCIPHER | CRYPTO_ALG_GENIV;\r\ninst->alg.cra_flags |= alg->cra_flags & CRYPTO_ALG_ASYNC;\r\ninst->alg.cra_priority = alg->cra_priority;\r\ninst->alg.cra_blocksize = alg->cra_blocksize;\r\ninst->alg.cra_alignmask = alg->cra_alignmask;\r\ninst->alg.cra_type = &crypto_givcipher_type;\r\ninst->alg.cra_ablkcipher.ivsize = balg.ivsize;\r\ninst->alg.cra_ablkcipher.min_keysize = balg.min_keysize;\r\ninst->alg.cra_ablkcipher.max_keysize = balg.max_keysize;\r\ninst->alg.cra_ablkcipher.geniv = balg.geniv;\r\ninst->alg.cra_ablkcipher.setkey = balg.setkey;\r\ninst->alg.cra_ablkcipher.encrypt = balg.encrypt;\r\ninst->alg.cra_ablkcipher.decrypt = balg.decrypt;\r\nout:\r\nreturn inst;\r\nerr_drop_alg:\r\ncrypto_drop_skcipher(spawn);\r\nerr_free_inst:\r\nkfree(inst);\r\ninst = ERR_PTR(err);\r\ngoto out;\r\n}\r\nvoid skcipher_geniv_free(struct crypto_instance *inst)\r\n{\r\ncrypto_drop_skcipher(crypto_instance_ctx(inst));\r\nkfree(inst);\r\n}\r\nint skcipher_geniv_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_instance *inst = (void *)tfm->__crt_alg;\r\nstruct crypto_ablkcipher *cipher;\r\ncipher = crypto_spawn_skcipher(crypto_instance_ctx(inst));\r\nif (IS_ERR(cipher))\r\nreturn PTR_ERR(cipher);\r\ntfm->crt_ablkcipher.base = cipher;\r\ntfm->crt_ablkcipher.reqsize += crypto_ablkcipher_reqsize(cipher);\r\nreturn 0;\r\n}\r\nvoid skcipher_geniv_exit(struct crypto_tfm *tfm)\r\n{\r\ncrypto_free_ablkcipher(tfm->crt_ablkcipher.base);\r\n}
