static void mv_desc_init(struct mv_xor_desc_slot *desc, unsigned long flags)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nhw_desc->status = (1 << 31);\r\nhw_desc->phy_next_desc = 0;\r\nhw_desc->desc_command = (1 << 31);\r\n}\r\nstatic u32 mv_desc_get_dest_addr(struct mv_xor_desc_slot *desc)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nreturn hw_desc->phy_dest_addr;\r\n}\r\nstatic u32 mv_desc_get_src_addr(struct mv_xor_desc_slot *desc,\r\nint src_idx)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nreturn hw_desc->phy_src_addr[src_idx];\r\n}\r\nstatic void mv_desc_set_byte_count(struct mv_xor_desc_slot *desc,\r\nu32 byte_count)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nhw_desc->byte_count = byte_count;\r\n}\r\nstatic void mv_desc_set_next_desc(struct mv_xor_desc_slot *desc,\r\nu32 next_desc_addr)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nBUG_ON(hw_desc->phy_next_desc);\r\nhw_desc->phy_next_desc = next_desc_addr;\r\n}\r\nstatic void mv_desc_clear_next_desc(struct mv_xor_desc_slot *desc)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nhw_desc->phy_next_desc = 0;\r\n}\r\nstatic void mv_desc_set_block_fill_val(struct mv_xor_desc_slot *desc, u32 val)\r\n{\r\ndesc->value = val;\r\n}\r\nstatic void mv_desc_set_dest_addr(struct mv_xor_desc_slot *desc,\r\ndma_addr_t addr)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nhw_desc->phy_dest_addr = addr;\r\n}\r\nstatic int mv_chan_memset_slot_count(size_t len)\r\n{\r\nreturn 1;\r\n}\r\nstatic void mv_desc_set_src_addr(struct mv_xor_desc_slot *desc,\r\nint index, dma_addr_t addr)\r\n{\r\nstruct mv_xor_desc *hw_desc = desc->hw_desc;\r\nhw_desc->phy_src_addr[index] = addr;\r\nif (desc->type == DMA_XOR)\r\nhw_desc->desc_command |= (1 << index);\r\n}\r\nstatic u32 mv_chan_get_current_desc(struct mv_xor_chan *chan)\r\n{\r\nreturn __raw_readl(XOR_CURR_DESC(chan));\r\n}\r\nstatic void mv_chan_set_next_descriptor(struct mv_xor_chan *chan,\r\nu32 next_desc_addr)\r\n{\r\n__raw_writel(next_desc_addr, XOR_NEXT_DESC(chan));\r\n}\r\nstatic void mv_chan_set_dest_pointer(struct mv_xor_chan *chan, u32 desc_addr)\r\n{\r\n__raw_writel(desc_addr, XOR_DEST_POINTER(chan));\r\n}\r\nstatic void mv_chan_set_block_size(struct mv_xor_chan *chan, u32 block_size)\r\n{\r\n__raw_writel(block_size, XOR_BLOCK_SIZE(chan));\r\n}\r\nstatic void mv_chan_set_value(struct mv_xor_chan *chan, u32 value)\r\n{\r\n__raw_writel(value, XOR_INIT_VALUE_LOW(chan));\r\n__raw_writel(value, XOR_INIT_VALUE_HIGH(chan));\r\n}\r\nstatic void mv_chan_unmask_interrupts(struct mv_xor_chan *chan)\r\n{\r\nu32 val = __raw_readl(XOR_INTR_MASK(chan));\r\nval |= XOR_INTR_MASK_VALUE << (chan->idx * 16);\r\n__raw_writel(val, XOR_INTR_MASK(chan));\r\n}\r\nstatic u32 mv_chan_get_intr_cause(struct mv_xor_chan *chan)\r\n{\r\nu32 intr_cause = __raw_readl(XOR_INTR_CAUSE(chan));\r\nintr_cause = (intr_cause >> (chan->idx * 16)) & 0xFFFF;\r\nreturn intr_cause;\r\n}\r\nstatic int mv_is_err_intr(u32 intr_cause)\r\n{\r\nif (intr_cause & ((1<<4)|(1<<5)|(1<<6)|(1<<7)|(1<<8)|(1<<9)))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void mv_xor_device_clear_eoc_cause(struct mv_xor_chan *chan)\r\n{\r\nu32 val = ~(1 << (chan->idx * 16));\r\ndev_dbg(chan->device->common.dev, "%s, val 0x%08x\n", __func__, val);\r\n__raw_writel(val, XOR_INTR_CAUSE(chan));\r\n}\r\nstatic void mv_xor_device_clear_err_status(struct mv_xor_chan *chan)\r\n{\r\nu32 val = 0xFFFF0000 >> (chan->idx * 16);\r\n__raw_writel(val, XOR_INTR_CAUSE(chan));\r\n}\r\nstatic int mv_can_chain(struct mv_xor_desc_slot *desc)\r\n{\r\nstruct mv_xor_desc_slot *chain_old_tail = list_entry(\r\ndesc->chain_node.prev, struct mv_xor_desc_slot, chain_node);\r\nif (chain_old_tail->type != desc->type)\r\nreturn 0;\r\nif (desc->type == DMA_MEMSET)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void mv_set_mode(struct mv_xor_chan *chan,\r\nenum dma_transaction_type type)\r\n{\r\nu32 op_mode;\r\nu32 config = __raw_readl(XOR_CONFIG(chan));\r\nswitch (type) {\r\ncase DMA_XOR:\r\nop_mode = XOR_OPERATION_MODE_XOR;\r\nbreak;\r\ncase DMA_MEMCPY:\r\nop_mode = XOR_OPERATION_MODE_MEMCPY;\r\nbreak;\r\ncase DMA_MEMSET:\r\nop_mode = XOR_OPERATION_MODE_MEMSET;\r\nbreak;\r\ndefault:\r\ndev_printk(KERN_ERR, chan->device->common.dev,\r\n"error: unsupported operation %d.\n",\r\ntype);\r\nBUG();\r\nreturn;\r\n}\r\nconfig &= ~0x7;\r\nconfig |= op_mode;\r\n__raw_writel(config, XOR_CONFIG(chan));\r\nchan->current_type = type;\r\n}\r\nstatic void mv_chan_activate(struct mv_xor_chan *chan)\r\n{\r\nu32 activation;\r\ndev_dbg(chan->device->common.dev, " activate chan.\n");\r\nactivation = __raw_readl(XOR_ACTIVATION(chan));\r\nactivation |= 0x1;\r\n__raw_writel(activation, XOR_ACTIVATION(chan));\r\n}\r\nstatic char mv_chan_is_busy(struct mv_xor_chan *chan)\r\n{\r\nu32 state = __raw_readl(XOR_ACTIVATION(chan));\r\nstate = (state >> 4) & 0x3;\r\nreturn (state == 1) ? 1 : 0;\r\n}\r\nstatic int mv_chan_xor_slot_count(size_t len, int src_cnt)\r\n{\r\nreturn 1;\r\n}\r\nstatic void mv_xor_free_slots(struct mv_xor_chan *mv_chan,\r\nstruct mv_xor_desc_slot *slot)\r\n{\r\ndev_dbg(mv_chan->device->common.dev, "%s %d slot %p\n",\r\n__func__, __LINE__, slot);\r\nslot->slots_per_op = 0;\r\n}\r\nstatic void mv_xor_start_new_chain(struct mv_xor_chan *mv_chan,\r\nstruct mv_xor_desc_slot *sw_desc)\r\n{\r\ndev_dbg(mv_chan->device->common.dev, "%s %d: sw_desc %p\n",\r\n__func__, __LINE__, sw_desc);\r\nif (sw_desc->type != mv_chan->current_type)\r\nmv_set_mode(mv_chan, sw_desc->type);\r\nif (sw_desc->type == DMA_MEMSET) {\r\nstruct mv_xor_desc *hw_desc = sw_desc->hw_desc;\r\nmv_chan_set_dest_pointer(mv_chan, hw_desc->phy_dest_addr);\r\nmv_chan_set_block_size(mv_chan, sw_desc->unmap_len);\r\nmv_chan_set_value(mv_chan, sw_desc->value);\r\n} else {\r\nmv_chan_set_next_descriptor(mv_chan, sw_desc->async_tx.phys);\r\n}\r\nmv_chan->pending += sw_desc->slot_cnt;\r\nmv_xor_issue_pending(&mv_chan->common);\r\n}\r\nstatic dma_cookie_t\r\nmv_xor_run_tx_complete_actions(struct mv_xor_desc_slot *desc,\r\nstruct mv_xor_chan *mv_chan, dma_cookie_t cookie)\r\n{\r\nBUG_ON(desc->async_tx.cookie < 0);\r\nif (desc->async_tx.cookie > 0) {\r\ncookie = desc->async_tx.cookie;\r\nif (desc->async_tx.callback)\r\ndesc->async_tx.callback(\r\ndesc->async_tx.callback_param);\r\nif (desc->group_head && desc->unmap_len) {\r\nstruct mv_xor_desc_slot *unmap = desc->group_head;\r\nstruct device *dev =\r\n&mv_chan->device->pdev->dev;\r\nu32 len = unmap->unmap_len;\r\nenum dma_ctrl_flags flags = desc->async_tx.flags;\r\nu32 src_cnt;\r\ndma_addr_t addr;\r\ndma_addr_t dest;\r\nsrc_cnt = unmap->unmap_src_cnt;\r\ndest = mv_desc_get_dest_addr(unmap);\r\nif (!(flags & DMA_COMPL_SKIP_DEST_UNMAP)) {\r\nenum dma_data_direction dir;\r\nif (src_cnt > 1)\r\ndir = DMA_BIDIRECTIONAL;\r\nelse\r\ndir = DMA_FROM_DEVICE;\r\ndma_unmap_page(dev, dest, len, dir);\r\n}\r\nif (!(flags & DMA_COMPL_SKIP_SRC_UNMAP)) {\r\nwhile (src_cnt--) {\r\naddr = mv_desc_get_src_addr(unmap,\r\nsrc_cnt);\r\nif (addr == dest)\r\ncontinue;\r\ndma_unmap_page(dev, addr, len,\r\nDMA_TO_DEVICE);\r\n}\r\n}\r\ndesc->group_head = NULL;\r\n}\r\n}\r\ndma_run_dependencies(&desc->async_tx);\r\nreturn cookie;\r\n}\r\nstatic int\r\nmv_xor_clean_completed_slots(struct mv_xor_chan *mv_chan)\r\n{\r\nstruct mv_xor_desc_slot *iter, *_iter;\r\ndev_dbg(mv_chan->device->common.dev, "%s %d\n", __func__, __LINE__);\r\nlist_for_each_entry_safe(iter, _iter, &mv_chan->completed_slots,\r\ncompleted_node) {\r\nif (async_tx_test_ack(&iter->async_tx)) {\r\nlist_del(&iter->completed_node);\r\nmv_xor_free_slots(mv_chan, iter);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nmv_xor_clean_slot(struct mv_xor_desc_slot *desc,\r\nstruct mv_xor_chan *mv_chan)\r\n{\r\ndev_dbg(mv_chan->device->common.dev, "%s %d: desc %p flags %d\n",\r\n__func__, __LINE__, desc, desc->async_tx.flags);\r\nlist_del(&desc->chain_node);\r\nif (!async_tx_test_ack(&desc->async_tx)) {\r\nlist_add_tail(&desc->completed_node, &mv_chan->completed_slots);\r\nreturn 0;\r\n}\r\nmv_xor_free_slots(mv_chan, desc);\r\nreturn 0;\r\n}\r\nstatic void __mv_xor_slot_cleanup(struct mv_xor_chan *mv_chan)\r\n{\r\nstruct mv_xor_desc_slot *iter, *_iter;\r\ndma_cookie_t cookie = 0;\r\nint busy = mv_chan_is_busy(mv_chan);\r\nu32 current_desc = mv_chan_get_current_desc(mv_chan);\r\nint seen_current = 0;\r\ndev_dbg(mv_chan->device->common.dev, "%s %d\n", __func__, __LINE__);\r\ndev_dbg(mv_chan->device->common.dev, "current_desc %x\n", current_desc);\r\nmv_xor_clean_completed_slots(mv_chan);\r\nlist_for_each_entry_safe(iter, _iter, &mv_chan->chain,\r\nchain_node) {\r\nprefetch(_iter);\r\nprefetch(&_iter->async_tx);\r\nif (seen_current)\r\nbreak;\r\nif (iter->async_tx.phys == current_desc) {\r\nseen_current = 1;\r\nif (busy)\r\nbreak;\r\n}\r\ncookie = mv_xor_run_tx_complete_actions(iter, mv_chan, cookie);\r\nif (mv_xor_clean_slot(iter, mv_chan))\r\nbreak;\r\n}\r\nif ((busy == 0) && !list_empty(&mv_chan->chain)) {\r\nstruct mv_xor_desc_slot *chain_head;\r\nchain_head = list_entry(mv_chan->chain.next,\r\nstruct mv_xor_desc_slot,\r\nchain_node);\r\nmv_xor_start_new_chain(mv_chan, chain_head);\r\n}\r\nif (cookie > 0)\r\nmv_chan->common.completed_cookie = cookie;\r\n}\r\nstatic void\r\nmv_xor_slot_cleanup(struct mv_xor_chan *mv_chan)\r\n{\r\nspin_lock_bh(&mv_chan->lock);\r\n__mv_xor_slot_cleanup(mv_chan);\r\nspin_unlock_bh(&mv_chan->lock);\r\n}\r\nstatic void mv_xor_tasklet(unsigned long data)\r\n{\r\nstruct mv_xor_chan *chan = (struct mv_xor_chan *) data;\r\nmv_xor_slot_cleanup(chan);\r\n}\r\nstatic struct mv_xor_desc_slot *\r\nmv_xor_alloc_slots(struct mv_xor_chan *mv_chan, int num_slots,\r\nint slots_per_op)\r\n{\r\nstruct mv_xor_desc_slot *iter, *_iter, *alloc_start = NULL;\r\nLIST_HEAD(chain);\r\nint slots_found, retry = 0;\r\nretry:\r\nslots_found = 0;\r\nif (retry == 0)\r\niter = mv_chan->last_used;\r\nelse\r\niter = list_entry(&mv_chan->all_slots,\r\nstruct mv_xor_desc_slot,\r\nslot_node);\r\nlist_for_each_entry_safe_continue(\r\niter, _iter, &mv_chan->all_slots, slot_node) {\r\nprefetch(_iter);\r\nprefetch(&_iter->async_tx);\r\nif (iter->slots_per_op) {\r\nif (retry)\r\nbreak;\r\nslots_found = 0;\r\ncontinue;\r\n}\r\nif (!slots_found++)\r\nalloc_start = iter;\r\nif (slots_found == num_slots) {\r\nstruct mv_xor_desc_slot *alloc_tail = NULL;\r\nstruct mv_xor_desc_slot *last_used = NULL;\r\niter = alloc_start;\r\nwhile (num_slots) {\r\nint i;\r\nasync_tx_ack(&iter->async_tx);\r\nlist_add_tail(&iter->chain_node, &chain);\r\nalloc_tail = iter;\r\niter->async_tx.cookie = 0;\r\niter->slot_cnt = num_slots;\r\niter->xor_check_result = NULL;\r\nfor (i = 0; i < slots_per_op; i++) {\r\niter->slots_per_op = slots_per_op - i;\r\nlast_used = iter;\r\niter = list_entry(iter->slot_node.next,\r\nstruct mv_xor_desc_slot,\r\nslot_node);\r\n}\r\nnum_slots -= slots_per_op;\r\n}\r\nalloc_tail->group_head = alloc_start;\r\nalloc_tail->async_tx.cookie = -EBUSY;\r\nlist_splice(&chain, &alloc_tail->tx_list);\r\nmv_chan->last_used = last_used;\r\nmv_desc_clear_next_desc(alloc_start);\r\nmv_desc_clear_next_desc(alloc_tail);\r\nreturn alloc_tail;\r\n}\r\n}\r\nif (!retry++)\r\ngoto retry;\r\ntasklet_schedule(&mv_chan->irq_tasklet);\r\nreturn NULL;\r\n}\r\nstatic dma_cookie_t\r\nmv_xor_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct mv_xor_desc_slot *sw_desc = to_mv_xor_slot(tx);\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(tx->chan);\r\nstruct mv_xor_desc_slot *grp_start, *old_chain_tail;\r\ndma_cookie_t cookie;\r\nint new_hw_chain = 1;\r\ndev_dbg(mv_chan->device->common.dev,\r\n"%s sw_desc %p: async_tx %p\n",\r\n__func__, sw_desc, &sw_desc->async_tx);\r\ngrp_start = sw_desc->group_head;\r\nspin_lock_bh(&mv_chan->lock);\r\ncookie = dma_cookie_assign(tx);\r\nif (list_empty(&mv_chan->chain))\r\nlist_splice_init(&sw_desc->tx_list, &mv_chan->chain);\r\nelse {\r\nnew_hw_chain = 0;\r\nold_chain_tail = list_entry(mv_chan->chain.prev,\r\nstruct mv_xor_desc_slot,\r\nchain_node);\r\nlist_splice_init(&grp_start->tx_list,\r\n&old_chain_tail->chain_node);\r\nif (!mv_can_chain(grp_start))\r\ngoto submit_done;\r\ndev_dbg(mv_chan->device->common.dev, "Append to last desc %x\n",\r\nold_chain_tail->async_tx.phys);\r\nmv_desc_set_next_desc(old_chain_tail, grp_start->async_tx.phys);\r\nif (!mv_chan_is_busy(mv_chan)) {\r\nu32 current_desc = mv_chan_get_current_desc(mv_chan);\r\nif (current_desc == old_chain_tail->async_tx.phys)\r\nnew_hw_chain = 1;\r\n}\r\n}\r\nif (new_hw_chain)\r\nmv_xor_start_new_chain(mv_chan, grp_start);\r\nsubmit_done:\r\nspin_unlock_bh(&mv_chan->lock);\r\nreturn cookie;\r\n}\r\nstatic int mv_xor_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nchar *hw_desc;\r\nint idx;\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nstruct mv_xor_desc_slot *slot = NULL;\r\nstruct mv_xor_platform_data *plat_data =\r\nmv_chan->device->pdev->dev.platform_data;\r\nint num_descs_in_pool = plat_data->pool_size/MV_XOR_SLOT_SIZE;\r\nidx = mv_chan->slots_allocated;\r\nwhile (idx < num_descs_in_pool) {\r\nslot = kzalloc(sizeof(*slot), GFP_KERNEL);\r\nif (!slot) {\r\nprintk(KERN_INFO "MV XOR Channel only initialized"\r\n" %d descriptor slots", idx);\r\nbreak;\r\n}\r\nhw_desc = (char *) mv_chan->device->dma_desc_pool_virt;\r\nslot->hw_desc = (void *) &hw_desc[idx * MV_XOR_SLOT_SIZE];\r\ndma_async_tx_descriptor_init(&slot->async_tx, chan);\r\nslot->async_tx.tx_submit = mv_xor_tx_submit;\r\nINIT_LIST_HEAD(&slot->chain_node);\r\nINIT_LIST_HEAD(&slot->slot_node);\r\nINIT_LIST_HEAD(&slot->tx_list);\r\nhw_desc = (char *) mv_chan->device->dma_desc_pool;\r\nslot->async_tx.phys =\r\n(dma_addr_t) &hw_desc[idx * MV_XOR_SLOT_SIZE];\r\nslot->idx = idx++;\r\nspin_lock_bh(&mv_chan->lock);\r\nmv_chan->slots_allocated = idx;\r\nlist_add_tail(&slot->slot_node, &mv_chan->all_slots);\r\nspin_unlock_bh(&mv_chan->lock);\r\n}\r\nif (mv_chan->slots_allocated && !mv_chan->last_used)\r\nmv_chan->last_used = list_entry(mv_chan->all_slots.next,\r\nstruct mv_xor_desc_slot,\r\nslot_node);\r\ndev_dbg(mv_chan->device->common.dev,\r\n"allocated %d descriptor slots last_used: %p\n",\r\nmv_chan->slots_allocated, mv_chan->last_used);\r\nreturn mv_chan->slots_allocated ? : -ENOMEM;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmv_xor_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nstruct mv_xor_desc_slot *sw_desc, *grp_start;\r\nint slot_cnt;\r\ndev_dbg(mv_chan->device->common.dev,\r\n"%s dest: %x src %x len: %u flags: %ld\n",\r\n__func__, dest, src, len, flags);\r\nif (unlikely(len < MV_XOR_MIN_BYTE_COUNT))\r\nreturn NULL;\r\nBUG_ON(len > MV_XOR_MAX_BYTE_COUNT);\r\nspin_lock_bh(&mv_chan->lock);\r\nslot_cnt = mv_chan_memcpy_slot_count(len);\r\nsw_desc = mv_xor_alloc_slots(mv_chan, slot_cnt, 1);\r\nif (sw_desc) {\r\nsw_desc->type = DMA_MEMCPY;\r\nsw_desc->async_tx.flags = flags;\r\ngrp_start = sw_desc->group_head;\r\nmv_desc_init(grp_start, flags);\r\nmv_desc_set_byte_count(grp_start, len);\r\nmv_desc_set_dest_addr(sw_desc->group_head, dest);\r\nmv_desc_set_src_addr(grp_start, 0, src);\r\nsw_desc->unmap_src_cnt = 1;\r\nsw_desc->unmap_len = len;\r\n}\r\nspin_unlock_bh(&mv_chan->lock);\r\ndev_dbg(mv_chan->device->common.dev,\r\n"%s sw_desc %p async_tx %p\n",\r\n__func__, sw_desc, sw_desc ? &sw_desc->async_tx : 0);\r\nreturn sw_desc ? &sw_desc->async_tx : NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmv_xor_prep_dma_memset(struct dma_chan *chan, dma_addr_t dest, int value,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nstruct mv_xor_desc_slot *sw_desc, *grp_start;\r\nint slot_cnt;\r\ndev_dbg(mv_chan->device->common.dev,\r\n"%s dest: %x len: %u flags: %ld\n",\r\n__func__, dest, len, flags);\r\nif (unlikely(len < MV_XOR_MIN_BYTE_COUNT))\r\nreturn NULL;\r\nBUG_ON(len > MV_XOR_MAX_BYTE_COUNT);\r\nspin_lock_bh(&mv_chan->lock);\r\nslot_cnt = mv_chan_memset_slot_count(len);\r\nsw_desc = mv_xor_alloc_slots(mv_chan, slot_cnt, 1);\r\nif (sw_desc) {\r\nsw_desc->type = DMA_MEMSET;\r\nsw_desc->async_tx.flags = flags;\r\ngrp_start = sw_desc->group_head;\r\nmv_desc_init(grp_start, flags);\r\nmv_desc_set_byte_count(grp_start, len);\r\nmv_desc_set_dest_addr(sw_desc->group_head, dest);\r\nmv_desc_set_block_fill_val(grp_start, value);\r\nsw_desc->unmap_src_cnt = 1;\r\nsw_desc->unmap_len = len;\r\n}\r\nspin_unlock_bh(&mv_chan->lock);\r\ndev_dbg(mv_chan->device->common.dev,\r\n"%s sw_desc %p async_tx %p \n",\r\n__func__, sw_desc, &sw_desc->async_tx);\r\nreturn sw_desc ? &sw_desc->async_tx : NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmv_xor_prep_dma_xor(struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,\r\nunsigned int src_cnt, size_t len, unsigned long flags)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nstruct mv_xor_desc_slot *sw_desc, *grp_start;\r\nint slot_cnt;\r\nif (unlikely(len < MV_XOR_MIN_BYTE_COUNT))\r\nreturn NULL;\r\nBUG_ON(len > MV_XOR_MAX_BYTE_COUNT);\r\ndev_dbg(mv_chan->device->common.dev,\r\n"%s src_cnt: %d len: dest %x %u flags: %ld\n",\r\n__func__, src_cnt, len, dest, flags);\r\nspin_lock_bh(&mv_chan->lock);\r\nslot_cnt = mv_chan_xor_slot_count(len, src_cnt);\r\nsw_desc = mv_xor_alloc_slots(mv_chan, slot_cnt, 1);\r\nif (sw_desc) {\r\nsw_desc->type = DMA_XOR;\r\nsw_desc->async_tx.flags = flags;\r\ngrp_start = sw_desc->group_head;\r\nmv_desc_init(grp_start, flags);\r\nmv_desc_set_byte_count(grp_start, len);\r\nmv_desc_set_dest_addr(sw_desc->group_head, dest);\r\nsw_desc->unmap_src_cnt = src_cnt;\r\nsw_desc->unmap_len = len;\r\nwhile (src_cnt--)\r\nmv_desc_set_src_addr(grp_start, src_cnt, src[src_cnt]);\r\n}\r\nspin_unlock_bh(&mv_chan->lock);\r\ndev_dbg(mv_chan->device->common.dev,\r\n"%s sw_desc %p async_tx %p \n",\r\n__func__, sw_desc, &sw_desc->async_tx);\r\nreturn sw_desc ? &sw_desc->async_tx : NULL;\r\n}\r\nstatic void mv_xor_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nstruct mv_xor_desc_slot *iter, *_iter;\r\nint in_use_descs = 0;\r\nmv_xor_slot_cleanup(mv_chan);\r\nspin_lock_bh(&mv_chan->lock);\r\nlist_for_each_entry_safe(iter, _iter, &mv_chan->chain,\r\nchain_node) {\r\nin_use_descs++;\r\nlist_del(&iter->chain_node);\r\n}\r\nlist_for_each_entry_safe(iter, _iter, &mv_chan->completed_slots,\r\ncompleted_node) {\r\nin_use_descs++;\r\nlist_del(&iter->completed_node);\r\n}\r\nlist_for_each_entry_safe_reverse(\r\niter, _iter, &mv_chan->all_slots, slot_node) {\r\nlist_del(&iter->slot_node);\r\nkfree(iter);\r\nmv_chan->slots_allocated--;\r\n}\r\nmv_chan->last_used = NULL;\r\ndev_dbg(mv_chan->device->common.dev, "%s slots_allocated %d\n",\r\n__func__, mv_chan->slots_allocated);\r\nspin_unlock_bh(&mv_chan->lock);\r\nif (in_use_descs)\r\ndev_err(mv_chan->device->common.dev,\r\n"freeing %d in use descriptors!\n", in_use_descs);\r\n}\r\nstatic enum dma_status mv_xor_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nenum dma_status ret;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_SUCCESS) {\r\nmv_xor_clean_completed_slots(mv_chan);\r\nreturn ret;\r\n}\r\nmv_xor_slot_cleanup(mv_chan);\r\nreturn dma_cookie_status(chan, cookie, txstate);\r\n}\r\nstatic void mv_dump_xor_regs(struct mv_xor_chan *chan)\r\n{\r\nu32 val;\r\nval = __raw_readl(XOR_CONFIG(chan));\r\ndev_printk(KERN_ERR, chan->device->common.dev,\r\n"config 0x%08x.\n", val);\r\nval = __raw_readl(XOR_ACTIVATION(chan));\r\ndev_printk(KERN_ERR, chan->device->common.dev,\r\n"activation 0x%08x.\n", val);\r\nval = __raw_readl(XOR_INTR_CAUSE(chan));\r\ndev_printk(KERN_ERR, chan->device->common.dev,\r\n"intr cause 0x%08x.\n", val);\r\nval = __raw_readl(XOR_INTR_MASK(chan));\r\ndev_printk(KERN_ERR, chan->device->common.dev,\r\n"intr mask 0x%08x.\n", val);\r\nval = __raw_readl(XOR_ERROR_CAUSE(chan));\r\ndev_printk(KERN_ERR, chan->device->common.dev,\r\n"error cause 0x%08x.\n", val);\r\nval = __raw_readl(XOR_ERROR_ADDR(chan));\r\ndev_printk(KERN_ERR, chan->device->common.dev,\r\n"error addr 0x%08x.\n", val);\r\n}\r\nstatic void mv_xor_err_interrupt_handler(struct mv_xor_chan *chan,\r\nu32 intr_cause)\r\n{\r\nif (intr_cause & (1 << 4)) {\r\ndev_dbg(chan->device->common.dev,\r\n"ignore this error\n");\r\nreturn;\r\n}\r\ndev_printk(KERN_ERR, chan->device->common.dev,\r\n"error on chan %d. intr cause 0x%08x.\n",\r\nchan->idx, intr_cause);\r\nmv_dump_xor_regs(chan);\r\nBUG();\r\n}\r\nstatic irqreturn_t mv_xor_interrupt_handler(int irq, void *data)\r\n{\r\nstruct mv_xor_chan *chan = data;\r\nu32 intr_cause = mv_chan_get_intr_cause(chan);\r\ndev_dbg(chan->device->common.dev, "intr cause %x\n", intr_cause);\r\nif (mv_is_err_intr(intr_cause))\r\nmv_xor_err_interrupt_handler(chan, intr_cause);\r\ntasklet_schedule(&chan->irq_tasklet);\r\nmv_xor_device_clear_eoc_cause(chan);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void mv_xor_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct mv_xor_chan *mv_chan = to_mv_xor_chan(chan);\r\nif (mv_chan->pending >= MV_XOR_THRESHOLD) {\r\nmv_chan->pending = 0;\r\nmv_chan_activate(mv_chan);\r\n}\r\n}\r\nstatic int __devinit mv_xor_memcpy_self_test(struct mv_xor_device *device)\r\n{\r\nint i;\r\nvoid *src, *dest;\r\ndma_addr_t src_dma, dest_dma;\r\nstruct dma_chan *dma_chan;\r\ndma_cookie_t cookie;\r\nstruct dma_async_tx_descriptor *tx;\r\nint err = 0;\r\nstruct mv_xor_chan *mv_chan;\r\nsrc = kmalloc(sizeof(u8) * MV_XOR_TEST_SIZE, GFP_KERNEL);\r\nif (!src)\r\nreturn -ENOMEM;\r\ndest = kzalloc(sizeof(u8) * MV_XOR_TEST_SIZE, GFP_KERNEL);\r\nif (!dest) {\r\nkfree(src);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < MV_XOR_TEST_SIZE; i++)\r\n((u8 *) src)[i] = (u8)i;\r\ndma_chan = container_of(device->common.channels.next,\r\nstruct dma_chan,\r\ndevice_node);\r\nif (mv_xor_alloc_chan_resources(dma_chan) < 1) {\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\ndest_dma = dma_map_single(dma_chan->device->dev, dest,\r\nMV_XOR_TEST_SIZE, DMA_FROM_DEVICE);\r\nsrc_dma = dma_map_single(dma_chan->device->dev, src,\r\nMV_XOR_TEST_SIZE, DMA_TO_DEVICE);\r\ntx = mv_xor_prep_dma_memcpy(dma_chan, dest_dma, src_dma,\r\nMV_XOR_TEST_SIZE, 0);\r\ncookie = mv_xor_tx_submit(tx);\r\nmv_xor_issue_pending(dma_chan);\r\nasync_tx_ack(tx);\r\nmsleep(1);\r\nif (mv_xor_status(dma_chan, cookie, NULL) !=\r\nDMA_SUCCESS) {\r\ndev_printk(KERN_ERR, dma_chan->device->dev,\r\n"Self-test copy timed out, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nmv_chan = to_mv_xor_chan(dma_chan);\r\ndma_sync_single_for_cpu(&mv_chan->device->pdev->dev, dest_dma,\r\nMV_XOR_TEST_SIZE, DMA_FROM_DEVICE);\r\nif (memcmp(src, dest, MV_XOR_TEST_SIZE)) {\r\ndev_printk(KERN_ERR, dma_chan->device->dev,\r\n"Self-test copy failed compare, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nfree_resources:\r\nmv_xor_free_chan_resources(dma_chan);\r\nout:\r\nkfree(src);\r\nkfree(dest);\r\nreturn err;\r\n}\r\nstatic int __devinit\r\nmv_xor_xor_self_test(struct mv_xor_device *device)\r\n{\r\nint i, src_idx;\r\nstruct page *dest;\r\nstruct page *xor_srcs[MV_XOR_NUM_SRC_TEST];\r\ndma_addr_t dma_srcs[MV_XOR_NUM_SRC_TEST];\r\ndma_addr_t dest_dma;\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct dma_chan *dma_chan;\r\ndma_cookie_t cookie;\r\nu8 cmp_byte = 0;\r\nu32 cmp_word;\r\nint err = 0;\r\nstruct mv_xor_chan *mv_chan;\r\nfor (src_idx = 0; src_idx < MV_XOR_NUM_SRC_TEST; src_idx++) {\r\nxor_srcs[src_idx] = alloc_page(GFP_KERNEL);\r\nif (!xor_srcs[src_idx]) {\r\nwhile (src_idx--)\r\n__free_page(xor_srcs[src_idx]);\r\nreturn -ENOMEM;\r\n}\r\n}\r\ndest = alloc_page(GFP_KERNEL);\r\nif (!dest) {\r\nwhile (src_idx--)\r\n__free_page(xor_srcs[src_idx]);\r\nreturn -ENOMEM;\r\n}\r\nfor (src_idx = 0; src_idx < MV_XOR_NUM_SRC_TEST; src_idx++) {\r\nu8 *ptr = page_address(xor_srcs[src_idx]);\r\nfor (i = 0; i < PAGE_SIZE; i++)\r\nptr[i] = (1 << src_idx);\r\n}\r\nfor (src_idx = 0; src_idx < MV_XOR_NUM_SRC_TEST; src_idx++)\r\ncmp_byte ^= (u8) (1 << src_idx);\r\ncmp_word = (cmp_byte << 24) | (cmp_byte << 16) |\r\n(cmp_byte << 8) | cmp_byte;\r\nmemset(page_address(dest), 0, PAGE_SIZE);\r\ndma_chan = container_of(device->common.channels.next,\r\nstruct dma_chan,\r\ndevice_node);\r\nif (mv_xor_alloc_chan_resources(dma_chan) < 1) {\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\ndest_dma = dma_map_page(dma_chan->device->dev, dest, 0, PAGE_SIZE,\r\nDMA_FROM_DEVICE);\r\nfor (i = 0; i < MV_XOR_NUM_SRC_TEST; i++)\r\ndma_srcs[i] = dma_map_page(dma_chan->device->dev, xor_srcs[i],\r\n0, PAGE_SIZE, DMA_TO_DEVICE);\r\ntx = mv_xor_prep_dma_xor(dma_chan, dest_dma, dma_srcs,\r\nMV_XOR_NUM_SRC_TEST, PAGE_SIZE, 0);\r\ncookie = mv_xor_tx_submit(tx);\r\nmv_xor_issue_pending(dma_chan);\r\nasync_tx_ack(tx);\r\nmsleep(8);\r\nif (mv_xor_status(dma_chan, cookie, NULL) !=\r\nDMA_SUCCESS) {\r\ndev_printk(KERN_ERR, dma_chan->device->dev,\r\n"Self-test xor timed out, disabling\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nmv_chan = to_mv_xor_chan(dma_chan);\r\ndma_sync_single_for_cpu(&mv_chan->device->pdev->dev, dest_dma,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\nfor (i = 0; i < (PAGE_SIZE / sizeof(u32)); i++) {\r\nu32 *ptr = page_address(dest);\r\nif (ptr[i] != cmp_word) {\r\ndev_printk(KERN_ERR, dma_chan->device->dev,\r\n"Self-test xor failed compare, disabling."\r\n" index %d, data %x, expected %x\n", i,\r\nptr[i], cmp_word);\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\n}\r\nfree_resources:\r\nmv_xor_free_chan_resources(dma_chan);\r\nout:\r\nsrc_idx = MV_XOR_NUM_SRC_TEST;\r\nwhile (src_idx--)\r\n__free_page(xor_srcs[src_idx]);\r\n__free_page(dest);\r\nreturn err;\r\n}\r\nstatic int __devexit mv_xor_remove(struct platform_device *dev)\r\n{\r\nstruct mv_xor_device *device = platform_get_drvdata(dev);\r\nstruct dma_chan *chan, *_chan;\r\nstruct mv_xor_chan *mv_chan;\r\nstruct mv_xor_platform_data *plat_data = dev->dev.platform_data;\r\ndma_async_device_unregister(&device->common);\r\ndma_free_coherent(&dev->dev, plat_data->pool_size,\r\ndevice->dma_desc_pool_virt, device->dma_desc_pool);\r\nlist_for_each_entry_safe(chan, _chan, &device->common.channels,\r\ndevice_node) {\r\nmv_chan = to_mv_xor_chan(chan);\r\nlist_del(&chan->device_node);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __devinit mv_xor_probe(struct platform_device *pdev)\r\n{\r\nint ret = 0;\r\nint irq;\r\nstruct mv_xor_device *adev;\r\nstruct mv_xor_chan *mv_chan;\r\nstruct dma_device *dma_dev;\r\nstruct mv_xor_platform_data *plat_data = pdev->dev.platform_data;\r\nadev = devm_kzalloc(&pdev->dev, sizeof(*adev), GFP_KERNEL);\r\nif (!adev)\r\nreturn -ENOMEM;\r\ndma_dev = &adev->common;\r\nadev->dma_desc_pool_virt = dma_alloc_writecombine(&pdev->dev,\r\nplat_data->pool_size,\r\n&adev->dma_desc_pool,\r\nGFP_KERNEL);\r\nif (!adev->dma_desc_pool_virt)\r\nreturn -ENOMEM;\r\nadev->id = plat_data->hw_id;\r\ndma_dev->cap_mask = plat_data->cap_mask;\r\nadev->pdev = pdev;\r\nplatform_set_drvdata(pdev, adev);\r\nadev->shared = platform_get_drvdata(plat_data->shared);\r\nINIT_LIST_HEAD(&dma_dev->channels);\r\ndma_dev->device_alloc_chan_resources = mv_xor_alloc_chan_resources;\r\ndma_dev->device_free_chan_resources = mv_xor_free_chan_resources;\r\ndma_dev->device_tx_status = mv_xor_status;\r\ndma_dev->device_issue_pending = mv_xor_issue_pending;\r\ndma_dev->dev = &pdev->dev;\r\nif (dma_has_cap(DMA_MEMCPY, dma_dev->cap_mask))\r\ndma_dev->device_prep_dma_memcpy = mv_xor_prep_dma_memcpy;\r\nif (dma_has_cap(DMA_MEMSET, dma_dev->cap_mask))\r\ndma_dev->device_prep_dma_memset = mv_xor_prep_dma_memset;\r\nif (dma_has_cap(DMA_XOR, dma_dev->cap_mask)) {\r\ndma_dev->max_xor = 8;\r\ndma_dev->device_prep_dma_xor = mv_xor_prep_dma_xor;\r\n}\r\nmv_chan = devm_kzalloc(&pdev->dev, sizeof(*mv_chan), GFP_KERNEL);\r\nif (!mv_chan) {\r\nret = -ENOMEM;\r\ngoto err_free_dma;\r\n}\r\nmv_chan->device = adev;\r\nmv_chan->idx = plat_data->hw_id;\r\nmv_chan->mmr_base = adev->shared->xor_base;\r\nif (!mv_chan->mmr_base) {\r\nret = -ENOMEM;\r\ngoto err_free_dma;\r\n}\r\ntasklet_init(&mv_chan->irq_tasklet, mv_xor_tasklet, (unsigned long)\r\nmv_chan);\r\nmv_xor_device_clear_err_status(mv_chan);\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0) {\r\nret = irq;\r\ngoto err_free_dma;\r\n}\r\nret = devm_request_irq(&pdev->dev, irq,\r\nmv_xor_interrupt_handler,\r\n0, dev_name(&pdev->dev), mv_chan);\r\nif (ret)\r\ngoto err_free_dma;\r\nmv_chan_unmask_interrupts(mv_chan);\r\nmv_set_mode(mv_chan, DMA_MEMCPY);\r\nspin_lock_init(&mv_chan->lock);\r\nINIT_LIST_HEAD(&mv_chan->chain);\r\nINIT_LIST_HEAD(&mv_chan->completed_slots);\r\nINIT_LIST_HEAD(&mv_chan->all_slots);\r\nmv_chan->common.device = dma_dev;\r\ndma_cookie_init(&mv_chan->common);\r\nlist_add_tail(&mv_chan->common.device_node, &dma_dev->channels);\r\nif (dma_has_cap(DMA_MEMCPY, dma_dev->cap_mask)) {\r\nret = mv_xor_memcpy_self_test(adev);\r\ndev_dbg(&pdev->dev, "memcpy self test returned %d\n", ret);\r\nif (ret)\r\ngoto err_free_dma;\r\n}\r\nif (dma_has_cap(DMA_XOR, dma_dev->cap_mask)) {\r\nret = mv_xor_xor_self_test(adev);\r\ndev_dbg(&pdev->dev, "xor self test returned %d\n", ret);\r\nif (ret)\r\ngoto err_free_dma;\r\n}\r\ndev_printk(KERN_INFO, &pdev->dev, "Marvell XOR: "\r\n"( %s%s%s%s)\n",\r\ndma_has_cap(DMA_XOR, dma_dev->cap_mask) ? "xor " : "",\r\ndma_has_cap(DMA_MEMSET, dma_dev->cap_mask) ? "fill " : "",\r\ndma_has_cap(DMA_MEMCPY, dma_dev->cap_mask) ? "cpy " : "",\r\ndma_has_cap(DMA_INTERRUPT, dma_dev->cap_mask) ? "intr " : "");\r\ndma_async_device_register(dma_dev);\r\ngoto out;\r\nerr_free_dma:\r\ndma_free_coherent(&adev->pdev->dev, plat_data->pool_size,\r\nadev->dma_desc_pool_virt, adev->dma_desc_pool);\r\nout:\r\nreturn ret;\r\n}\r\nstatic void\r\nmv_xor_conf_mbus_windows(struct mv_xor_shared_private *msp,\r\nconst struct mbus_dram_target_info *dram)\r\n{\r\nvoid __iomem *base = msp->xor_base;\r\nu32 win_enable = 0;\r\nint i;\r\nfor (i = 0; i < 8; i++) {\r\nwritel(0, base + WINDOW_BASE(i));\r\nwritel(0, base + WINDOW_SIZE(i));\r\nif (i < 4)\r\nwritel(0, base + WINDOW_REMAP_HIGH(i));\r\n}\r\nfor (i = 0; i < dram->num_cs; i++) {\r\nconst struct mbus_dram_window *cs = dram->cs + i;\r\nwritel((cs->base & 0xffff0000) |\r\n(cs->mbus_attr << 8) |\r\ndram->mbus_dram_target_id, base + WINDOW_BASE(i));\r\nwritel((cs->size - 1) & 0xffff0000, base + WINDOW_SIZE(i));\r\nwin_enable |= (1 << i);\r\nwin_enable |= 3 << (16 + (2 * i));\r\n}\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE(0));\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE(1));\r\n}\r\nstatic int mv_xor_shared_probe(struct platform_device *pdev)\r\n{\r\nconst struct mbus_dram_target_info *dram;\r\nstruct mv_xor_shared_private *msp;\r\nstruct resource *res;\r\ndev_printk(KERN_NOTICE, &pdev->dev, "Marvell shared XOR driver\n");\r\nmsp = devm_kzalloc(&pdev->dev, sizeof(*msp), GFP_KERNEL);\r\nif (!msp)\r\nreturn -ENOMEM;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res)\r\nreturn -ENODEV;\r\nmsp->xor_base = devm_ioremap(&pdev->dev, res->start,\r\nresource_size(res));\r\nif (!msp->xor_base)\r\nreturn -EBUSY;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nif (!res)\r\nreturn -ENODEV;\r\nmsp->xor_high_base = devm_ioremap(&pdev->dev, res->start,\r\nresource_size(res));\r\nif (!msp->xor_high_base)\r\nreturn -EBUSY;\r\nplatform_set_drvdata(pdev, msp);\r\ndram = mv_mbus_dram_info();\r\nif (dram)\r\nmv_xor_conf_mbus_windows(msp, dram);\r\nmsp->clk = clk_get(&pdev->dev, NULL);\r\nif (!IS_ERR(msp->clk))\r\nclk_prepare_enable(msp->clk);\r\nreturn 0;\r\n}\r\nstatic int mv_xor_shared_remove(struct platform_device *pdev)\r\n{\r\nstruct mv_xor_shared_private *msp = platform_get_drvdata(pdev);\r\nif (!IS_ERR(msp->clk)) {\r\nclk_disable_unprepare(msp->clk);\r\nclk_put(msp->clk);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init mv_xor_init(void)\r\n{\r\nint rc;\r\nrc = platform_driver_register(&mv_xor_shared_driver);\r\nif (!rc) {\r\nrc = platform_driver_register(&mv_xor_driver);\r\nif (rc)\r\nplatform_driver_unregister(&mv_xor_shared_driver);\r\n}\r\nreturn rc;\r\n}
