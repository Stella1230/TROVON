static inline u32 omap_aes_read(struct omap_aes_dev *dd, u32 offset)\r\n{\r\nreturn __raw_readl(dd->io_base + offset);\r\n}\r\nstatic inline void omap_aes_write(struct omap_aes_dev *dd, u32 offset,\r\nu32 value)\r\n{\r\n__raw_writel(value, dd->io_base + offset);\r\n}\r\nstatic inline void omap_aes_write_mask(struct omap_aes_dev *dd, u32 offset,\r\nu32 value, u32 mask)\r\n{\r\nu32 val;\r\nval = omap_aes_read(dd, offset);\r\nval &= ~mask;\r\nval |= value;\r\nomap_aes_write(dd, offset, val);\r\n}\r\nstatic void omap_aes_write_n(struct omap_aes_dev *dd, u32 offset,\r\nu32 *value, int count)\r\n{\r\nfor (; count--; value++, offset += 4)\r\nomap_aes_write(dd, offset, *value);\r\n}\r\nstatic int omap_aes_wait(struct omap_aes_dev *dd, u32 offset, u32 bit)\r\n{\r\nunsigned long timeout = jiffies + DEFAULT_TIMEOUT;\r\nwhile (!(omap_aes_read(dd, offset) & bit)) {\r\nif (time_is_before_jiffies(timeout)) {\r\ndev_err(dd->dev, "omap-aes timeout\n");\r\nreturn -ETIMEDOUT;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_aes_hw_init(struct omap_aes_dev *dd)\r\n{\r\nclk_enable(dd->iclk);\r\nif (!(dd->flags & FLAGS_INIT)) {\r\nomap_aes_write_mask(dd, AES_REG_MASK, AES_REG_MASK_SOFTRESET,\r\nAES_REG_MASK_SOFTRESET);\r\n__asm__ __volatile__("nop");\r\n__asm__ __volatile__("nop");\r\nif (omap_aes_wait(dd, AES_REG_SYSSTATUS,\r\nAES_REG_SYSSTATUS_RESETDONE))\r\nreturn -ETIMEDOUT;\r\ndd->flags |= FLAGS_INIT;\r\ndd->err = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_aes_write_ctrl(struct omap_aes_dev *dd)\r\n{\r\nunsigned int key32;\r\nint i, err;\r\nu32 val, mask;\r\nerr = omap_aes_hw_init(dd);\r\nif (err)\r\nreturn err;\r\nval = 0;\r\nif (dd->dma_lch_out >= 0)\r\nval |= AES_REG_MASK_DMA_OUT_EN;\r\nif (dd->dma_lch_in >= 0)\r\nval |= AES_REG_MASK_DMA_IN_EN;\r\nmask = AES_REG_MASK_DMA_IN_EN | AES_REG_MASK_DMA_OUT_EN;\r\nomap_aes_write_mask(dd, AES_REG_MASK, val, mask);\r\nkey32 = dd->ctx->keylen / sizeof(u32);\r\nfor (i = 0; i < key32; i++) {\r\nomap_aes_write(dd, AES_REG_KEY(i),\r\n__le32_to_cpu(dd->ctx->key[i]));\r\n}\r\nif ((dd->flags & FLAGS_CBC) && dd->req->info)\r\nomap_aes_write_n(dd, AES_REG_IV(0), dd->req->info, 4);\r\nval = FLD_VAL(((dd->ctx->keylen >> 3) - 1), 4, 3);\r\nif (dd->flags & FLAGS_CBC)\r\nval |= AES_REG_CTRL_CBC;\r\nif (dd->flags & FLAGS_ENCRYPT)\r\nval |= AES_REG_CTRL_DIRECTION;\r\nmask = AES_REG_CTRL_CBC | AES_REG_CTRL_DIRECTION |\r\nAES_REG_CTRL_KEY_SIZE;\r\nomap_aes_write_mask(dd, AES_REG_CTRL, val, mask);\r\nomap_set_dma_dest_params(dd->dma_lch_in, 0, OMAP_DMA_AMODE_CONSTANT,\r\ndd->phys_base + AES_REG_DATA, 0, 4);\r\nomap_set_dma_dest_burst_mode(dd->dma_lch_in, OMAP_DMA_DATA_BURST_4);\r\nomap_set_dma_src_burst_mode(dd->dma_lch_in, OMAP_DMA_DATA_BURST_4);\r\nomap_set_dma_src_params(dd->dma_lch_out, 0, OMAP_DMA_AMODE_CONSTANT,\r\ndd->phys_base + AES_REG_DATA, 0, 4);\r\nomap_set_dma_src_burst_mode(dd->dma_lch_out, OMAP_DMA_DATA_BURST_4);\r\nomap_set_dma_dest_burst_mode(dd->dma_lch_out, OMAP_DMA_DATA_BURST_4);\r\nreturn 0;\r\n}\r\nstatic struct omap_aes_dev *omap_aes_find_dev(struct omap_aes_ctx *ctx)\r\n{\r\nstruct omap_aes_dev *dd = NULL, *tmp;\r\nspin_lock_bh(&list_lock);\r\nif (!ctx->dd) {\r\nlist_for_each_entry(tmp, &dev_list, list) {\r\ndd = tmp;\r\nbreak;\r\n}\r\nctx->dd = dd;\r\n} else {\r\ndd = ctx->dd;\r\n}\r\nspin_unlock_bh(&list_lock);\r\nreturn dd;\r\n}\r\nstatic void omap_aes_dma_callback(int lch, u16 ch_status, void *data)\r\n{\r\nstruct omap_aes_dev *dd = data;\r\nif (ch_status != OMAP_DMA_BLOCK_IRQ) {\r\npr_err("omap-aes DMA error status: 0x%hx\n", ch_status);\r\ndd->err = -EIO;\r\ndd->flags &= ~FLAGS_INIT;\r\n} else if (lch == dd->dma_lch_in) {\r\nreturn;\r\n}\r\ntasklet_schedule(&dd->done_task);\r\n}\r\nstatic int omap_aes_dma_init(struct omap_aes_dev *dd)\r\n{\r\nint err = -ENOMEM;\r\ndd->dma_lch_out = -1;\r\ndd->dma_lch_in = -1;\r\ndd->buf_in = (void *)__get_free_pages(GFP_KERNEL, OMAP_AES_CACHE_SIZE);\r\ndd->buf_out = (void *)__get_free_pages(GFP_KERNEL, OMAP_AES_CACHE_SIZE);\r\ndd->buflen = PAGE_SIZE << OMAP_AES_CACHE_SIZE;\r\ndd->buflen &= ~(AES_BLOCK_SIZE - 1);\r\nif (!dd->buf_in || !dd->buf_out) {\r\ndev_err(dd->dev, "unable to alloc pages.\n");\r\ngoto err_alloc;\r\n}\r\ndd->dma_addr_in = dma_map_single(dd->dev, dd->buf_in, dd->buflen,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(dd->dev, dd->dma_addr_in)) {\r\ndev_err(dd->dev, "dma %d bytes error\n", dd->buflen);\r\nerr = -EINVAL;\r\ngoto err_map_in;\r\n}\r\ndd->dma_addr_out = dma_map_single(dd->dev, dd->buf_out, dd->buflen,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(dd->dev, dd->dma_addr_out)) {\r\ndev_err(dd->dev, "dma %d bytes error\n", dd->buflen);\r\nerr = -EINVAL;\r\ngoto err_map_out;\r\n}\r\nerr = omap_request_dma(dd->dma_in, "omap-aes-rx",\r\nomap_aes_dma_callback, dd, &dd->dma_lch_in);\r\nif (err) {\r\ndev_err(dd->dev, "Unable to request DMA channel\n");\r\ngoto err_dma_in;\r\n}\r\nerr = omap_request_dma(dd->dma_out, "omap-aes-tx",\r\nomap_aes_dma_callback, dd, &dd->dma_lch_out);\r\nif (err) {\r\ndev_err(dd->dev, "Unable to request DMA channel\n");\r\ngoto err_dma_out;\r\n}\r\nreturn 0;\r\nerr_dma_out:\r\nomap_free_dma(dd->dma_lch_in);\r\nerr_dma_in:\r\ndma_unmap_single(dd->dev, dd->dma_addr_out, dd->buflen,\r\nDMA_FROM_DEVICE);\r\nerr_map_out:\r\ndma_unmap_single(dd->dev, dd->dma_addr_in, dd->buflen, DMA_TO_DEVICE);\r\nerr_map_in:\r\nfree_pages((unsigned long)dd->buf_out, OMAP_AES_CACHE_SIZE);\r\nfree_pages((unsigned long)dd->buf_in, OMAP_AES_CACHE_SIZE);\r\nerr_alloc:\r\nif (err)\r\npr_err("error: %d\n", err);\r\nreturn err;\r\n}\r\nstatic void omap_aes_dma_cleanup(struct omap_aes_dev *dd)\r\n{\r\nomap_free_dma(dd->dma_lch_out);\r\nomap_free_dma(dd->dma_lch_in);\r\ndma_unmap_single(dd->dev, dd->dma_addr_out, dd->buflen,\r\nDMA_FROM_DEVICE);\r\ndma_unmap_single(dd->dev, dd->dma_addr_in, dd->buflen, DMA_TO_DEVICE);\r\nfree_pages((unsigned long)dd->buf_out, OMAP_AES_CACHE_SIZE);\r\nfree_pages((unsigned long)dd->buf_in, OMAP_AES_CACHE_SIZE);\r\n}\r\nstatic void sg_copy_buf(void *buf, struct scatterlist *sg,\r\nunsigned int start, unsigned int nbytes, int out)\r\n{\r\nstruct scatter_walk walk;\r\nif (!nbytes)\r\nreturn;\r\nscatterwalk_start(&walk, sg);\r\nscatterwalk_advance(&walk, start);\r\nscatterwalk_copychunks(buf, &walk, nbytes, out);\r\nscatterwalk_done(&walk, out, 0);\r\n}\r\nstatic int sg_copy(struct scatterlist **sg, size_t *offset, void *buf,\r\nsize_t buflen, size_t total, int out)\r\n{\r\nunsigned int count, off = 0;\r\nwhile (buflen && total) {\r\ncount = min((*sg)->length - *offset, total);\r\ncount = min(count, buflen);\r\nif (!count)\r\nreturn off;\r\nsg_copy_buf(buf + off, *sg, *offset, count, out);\r\noff += count;\r\nbuflen -= count;\r\n*offset += count;\r\ntotal -= count;\r\nif (*offset == (*sg)->length) {\r\n*sg = sg_next(*sg);\r\nif (*sg)\r\n*offset = 0;\r\nelse\r\ntotal = 0;\r\n}\r\n}\r\nreturn off;\r\n}\r\nstatic int omap_aes_crypt_dma(struct crypto_tfm *tfm, dma_addr_t dma_addr_in,\r\ndma_addr_t dma_addr_out, int length)\r\n{\r\nstruct omap_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct omap_aes_dev *dd = ctx->dd;\r\nint len32;\r\npr_debug("len: %d\n", length);\r\ndd->dma_size = length;\r\nif (!(dd->flags & FLAGS_FAST))\r\ndma_sync_single_for_device(dd->dev, dma_addr_in, length,\r\nDMA_TO_DEVICE);\r\nlen32 = DIV_ROUND_UP(length, sizeof(u32));\r\nomap_set_dma_transfer_params(dd->dma_lch_in, OMAP_DMA_DATA_TYPE_S32,\r\nlen32, 1, OMAP_DMA_SYNC_PACKET, dd->dma_in,\r\nOMAP_DMA_DST_SYNC);\r\nomap_set_dma_src_params(dd->dma_lch_in, 0, OMAP_DMA_AMODE_POST_INC,\r\ndma_addr_in, 0, 0);\r\nomap_set_dma_transfer_params(dd->dma_lch_out, OMAP_DMA_DATA_TYPE_S32,\r\nlen32, 1, OMAP_DMA_SYNC_PACKET,\r\ndd->dma_out, OMAP_DMA_SRC_SYNC);\r\nomap_set_dma_dest_params(dd->dma_lch_out, 0, OMAP_DMA_AMODE_POST_INC,\r\ndma_addr_out, 0, 0);\r\nomap_start_dma(dd->dma_lch_in);\r\nomap_start_dma(dd->dma_lch_out);\r\nomap_aes_write_mask(dd, AES_REG_MASK, AES_REG_MASK_START,\r\nAES_REG_MASK_START);\r\nreturn 0;\r\n}\r\nstatic int omap_aes_crypt_dma_start(struct omap_aes_dev *dd)\r\n{\r\nstruct crypto_tfm *tfm = crypto_ablkcipher_tfm(\r\ncrypto_ablkcipher_reqtfm(dd->req));\r\nint err, fast = 0, in, out;\r\nsize_t count;\r\ndma_addr_t addr_in, addr_out;\r\npr_debug("total: %d\n", dd->total);\r\nif (sg_is_last(dd->in_sg) && sg_is_last(dd->out_sg)) {\r\nin = IS_ALIGNED((u32)dd->in_sg->offset, sizeof(u32));\r\nout = IS_ALIGNED((u32)dd->out_sg->offset, sizeof(u32));\r\nfast = in && out;\r\n}\r\nif (fast) {\r\ncount = min(dd->total, sg_dma_len(dd->in_sg));\r\ncount = min(count, sg_dma_len(dd->out_sg));\r\nif (count != dd->total) {\r\npr_err("request length != buffer length\n");\r\nreturn -EINVAL;\r\n}\r\npr_debug("fast\n");\r\nerr = dma_map_sg(dd->dev, dd->in_sg, 1, DMA_TO_DEVICE);\r\nif (!err) {\r\ndev_err(dd->dev, "dma_map_sg() error\n");\r\nreturn -EINVAL;\r\n}\r\nerr = dma_map_sg(dd->dev, dd->out_sg, 1, DMA_FROM_DEVICE);\r\nif (!err) {\r\ndev_err(dd->dev, "dma_map_sg() error\n");\r\ndma_unmap_sg(dd->dev, dd->in_sg, 1, DMA_TO_DEVICE);\r\nreturn -EINVAL;\r\n}\r\naddr_in = sg_dma_address(dd->in_sg);\r\naddr_out = sg_dma_address(dd->out_sg);\r\ndd->flags |= FLAGS_FAST;\r\n} else {\r\ncount = sg_copy(&dd->in_sg, &dd->in_offset, dd->buf_in,\r\ndd->buflen, dd->total, 0);\r\naddr_in = dd->dma_addr_in;\r\naddr_out = dd->dma_addr_out;\r\ndd->flags &= ~FLAGS_FAST;\r\n}\r\ndd->total -= count;\r\nerr = omap_aes_crypt_dma(tfm, addr_in, addr_out, count);\r\nif (err) {\r\ndma_unmap_sg(dd->dev, dd->in_sg, 1, DMA_TO_DEVICE);\r\ndma_unmap_sg(dd->dev, dd->out_sg, 1, DMA_TO_DEVICE);\r\n}\r\nreturn err;\r\n}\r\nstatic void omap_aes_finish_req(struct omap_aes_dev *dd, int err)\r\n{\r\nstruct ablkcipher_request *req = dd->req;\r\npr_debug("err: %d\n", err);\r\nclk_disable(dd->iclk);\r\ndd->flags &= ~FLAGS_BUSY;\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic int omap_aes_crypt_dma_stop(struct omap_aes_dev *dd)\r\n{\r\nint err = 0;\r\nsize_t count;\r\npr_debug("total: %d\n", dd->total);\r\nomap_aes_write_mask(dd, AES_REG_MASK, 0, AES_REG_MASK_START);\r\nomap_stop_dma(dd->dma_lch_in);\r\nomap_stop_dma(dd->dma_lch_out);\r\nif (dd->flags & FLAGS_FAST) {\r\ndma_unmap_sg(dd->dev, dd->out_sg, 1, DMA_FROM_DEVICE);\r\ndma_unmap_sg(dd->dev, dd->in_sg, 1, DMA_TO_DEVICE);\r\n} else {\r\ndma_sync_single_for_device(dd->dev, dd->dma_addr_out,\r\ndd->dma_size, DMA_FROM_DEVICE);\r\ncount = sg_copy(&dd->out_sg, &dd->out_offset, dd->buf_out,\r\ndd->buflen, dd->dma_size, 1);\r\nif (count != dd->dma_size) {\r\nerr = -EINVAL;\r\npr_err("not all data converted: %u\n", count);\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic int omap_aes_handle_queue(struct omap_aes_dev *dd,\r\nstruct ablkcipher_request *req)\r\n{\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct omap_aes_ctx *ctx;\r\nstruct omap_aes_reqctx *rctx;\r\nunsigned long flags;\r\nint err, ret = 0;\r\nspin_lock_irqsave(&dd->lock, flags);\r\nif (req)\r\nret = ablkcipher_enqueue_request(&dd->queue, req);\r\nif (dd->flags & FLAGS_BUSY) {\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nreturn ret;\r\n}\r\nbacklog = crypto_get_backlog(&dd->queue);\r\nasync_req = crypto_dequeue_request(&dd->queue);\r\nif (async_req)\r\ndd->flags |= FLAGS_BUSY;\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nif (!async_req)\r\nreturn ret;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ablkcipher_request_cast(async_req);\r\ndd->req = req;\r\ndd->total = req->nbytes;\r\ndd->in_offset = 0;\r\ndd->in_sg = req->src;\r\ndd->out_offset = 0;\r\ndd->out_sg = req->dst;\r\nrctx = ablkcipher_request_ctx(req);\r\nctx = crypto_ablkcipher_ctx(crypto_ablkcipher_reqtfm(req));\r\nrctx->mode &= FLAGS_MODE_MASK;\r\ndd->flags = (dd->flags & ~FLAGS_MODE_MASK) | rctx->mode;\r\ndd->ctx = ctx;\r\nctx->dd = dd;\r\nerr = omap_aes_write_ctrl(dd);\r\nif (!err)\r\nerr = omap_aes_crypt_dma_start(dd);\r\nif (err) {\r\nomap_aes_finish_req(dd, err);\r\ntasklet_schedule(&dd->queue_task);\r\n}\r\nreturn ret;\r\n}\r\nstatic void omap_aes_done_task(unsigned long data)\r\n{\r\nstruct omap_aes_dev *dd = (struct omap_aes_dev *)data;\r\nint err;\r\npr_debug("enter\n");\r\nerr = omap_aes_crypt_dma_stop(dd);\r\nerr = dd->err ? : err;\r\nif (dd->total && !err) {\r\nerr = omap_aes_crypt_dma_start(dd);\r\nif (!err)\r\nreturn;\r\n}\r\nomap_aes_finish_req(dd, err);\r\nomap_aes_handle_queue(dd, NULL);\r\npr_debug("exit\n");\r\n}\r\nstatic void omap_aes_queue_task(unsigned long data)\r\n{\r\nstruct omap_aes_dev *dd = (struct omap_aes_dev *)data;\r\nomap_aes_handle_queue(dd, NULL);\r\n}\r\nstatic int omap_aes_crypt(struct ablkcipher_request *req, unsigned long mode)\r\n{\r\nstruct omap_aes_ctx *ctx = crypto_ablkcipher_ctx(\r\ncrypto_ablkcipher_reqtfm(req));\r\nstruct omap_aes_reqctx *rctx = ablkcipher_request_ctx(req);\r\nstruct omap_aes_dev *dd;\r\npr_debug("nbytes: %d, enc: %d, cbc: %d\n", req->nbytes,\r\n!!(mode & FLAGS_ENCRYPT),\r\n!!(mode & FLAGS_CBC));\r\nif (!IS_ALIGNED(req->nbytes, AES_BLOCK_SIZE)) {\r\npr_err("request size is not exact amount of AES blocks\n");\r\nreturn -EINVAL;\r\n}\r\ndd = omap_aes_find_dev(ctx);\r\nif (!dd)\r\nreturn -ENODEV;\r\nrctx->mode = mode;\r\nreturn omap_aes_handle_queue(dd, req);\r\n}\r\nstatic int omap_aes_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct omap_aes_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nif (keylen != AES_KEYSIZE_128 && keylen != AES_KEYSIZE_192 &&\r\nkeylen != AES_KEYSIZE_256)\r\nreturn -EINVAL;\r\npr_debug("enter, keylen: %d\n", keylen);\r\nmemcpy(ctx->key, key, keylen);\r\nctx->keylen = keylen;\r\nreturn 0;\r\n}\r\nstatic int omap_aes_ecb_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, FLAGS_ENCRYPT);\r\n}\r\nstatic int omap_aes_ecb_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, 0);\r\n}\r\nstatic int omap_aes_cbc_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, FLAGS_ENCRYPT | FLAGS_CBC);\r\n}\r\nstatic int omap_aes_cbc_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn omap_aes_crypt(req, FLAGS_CBC);\r\n}\r\nstatic int omap_aes_cra_init(struct crypto_tfm *tfm)\r\n{\r\npr_debug("enter\n");\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct omap_aes_reqctx);\r\nreturn 0;\r\n}\r\nstatic void omap_aes_cra_exit(struct crypto_tfm *tfm)\r\n{\r\npr_debug("enter\n");\r\n}\r\nstatic int omap_aes_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct omap_aes_dev *dd;\r\nstruct resource *res;\r\nint err = -ENOMEM, i, j;\r\nu32 reg;\r\ndd = kzalloc(sizeof(struct omap_aes_dev), GFP_KERNEL);\r\nif (dd == NULL) {\r\ndev_err(dev, "unable to alloc data struct.\n");\r\ngoto err_data;\r\n}\r\ndd->dev = dev;\r\nplatform_set_drvdata(pdev, dd);\r\nspin_lock_init(&dd->lock);\r\ncrypto_init_queue(&dd->queue, OMAP_AES_QUEUE_LENGTH);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res) {\r\ndev_err(dev, "invalid resource type\n");\r\nerr = -ENODEV;\r\ngoto err_res;\r\n}\r\ndd->phys_base = res->start;\r\nres = platform_get_resource(pdev, IORESOURCE_DMA, 0);\r\nif (!res)\r\ndev_info(dev, "no DMA info\n");\r\nelse\r\ndd->dma_out = res->start;\r\nres = platform_get_resource(pdev, IORESOURCE_DMA, 1);\r\nif (!res)\r\ndev_info(dev, "no DMA info\n");\r\nelse\r\ndd->dma_in = res->start;\r\ndd->iclk = clk_get(dev, "ick");\r\nif (IS_ERR(dd->iclk)) {\r\ndev_err(dev, "clock intialization failed.\n");\r\nerr = PTR_ERR(dd->iclk);\r\ngoto err_res;\r\n}\r\ndd->io_base = ioremap(dd->phys_base, SZ_4K);\r\nif (!dd->io_base) {\r\ndev_err(dev, "can't ioremap\n");\r\nerr = -ENOMEM;\r\ngoto err_io;\r\n}\r\nclk_enable(dd->iclk);\r\nreg = omap_aes_read(dd, AES_REG_REV);\r\ndev_info(dev, "OMAP AES hw accel rev: %u.%u\n",\r\n(reg & AES_REG_REV_MAJOR) >> 4, reg & AES_REG_REV_MINOR);\r\nclk_disable(dd->iclk);\r\ntasklet_init(&dd->done_task, omap_aes_done_task, (unsigned long)dd);\r\ntasklet_init(&dd->queue_task, omap_aes_queue_task, (unsigned long)dd);\r\nerr = omap_aes_dma_init(dd);\r\nif (err)\r\ngoto err_dma;\r\nINIT_LIST_HEAD(&dd->list);\r\nspin_lock(&list_lock);\r\nlist_add_tail(&dd->list, &dev_list);\r\nspin_unlock(&list_lock);\r\nfor (i = 0; i < ARRAY_SIZE(algs); i++) {\r\npr_debug("i: %d\n", i);\r\nINIT_LIST_HEAD(&algs[i].cra_list);\r\nerr = crypto_register_alg(&algs[i]);\r\nif (err)\r\ngoto err_algs;\r\n}\r\npr_info("probe() done\n");\r\nreturn 0;\r\nerr_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_alg(&algs[j]);\r\nomap_aes_dma_cleanup(dd);\r\nerr_dma:\r\ntasklet_kill(&dd->done_task);\r\ntasklet_kill(&dd->queue_task);\r\niounmap(dd->io_base);\r\nerr_io:\r\nclk_put(dd->iclk);\r\nerr_res:\r\nkfree(dd);\r\ndd = NULL;\r\nerr_data:\r\ndev_err(dev, "initialization failed.\n");\r\nreturn err;\r\n}\r\nstatic int omap_aes_remove(struct platform_device *pdev)\r\n{\r\nstruct omap_aes_dev *dd = platform_get_drvdata(pdev);\r\nint i;\r\nif (!dd)\r\nreturn -ENODEV;\r\nspin_lock(&list_lock);\r\nlist_del(&dd->list);\r\nspin_unlock(&list_lock);\r\nfor (i = 0; i < ARRAY_SIZE(algs); i++)\r\ncrypto_unregister_alg(&algs[i]);\r\ntasklet_kill(&dd->done_task);\r\ntasklet_kill(&dd->queue_task);\r\nomap_aes_dma_cleanup(dd);\r\niounmap(dd->io_base);\r\nclk_put(dd->iclk);\r\nkfree(dd);\r\ndd = NULL;\r\nreturn 0;\r\n}\r\nstatic int __init omap_aes_mod_init(void)\r\n{\r\npr_info("loading %s driver\n", "omap-aes");\r\nif (!cpu_class_is_omap2() || omap_type() != OMAP2_DEVICE_TYPE_SEC) {\r\npr_err("Unsupported cpu\n");\r\nreturn -ENODEV;\r\n}\r\nreturn platform_driver_register(&omap_aes_driver);\r\n}\r\nstatic void __exit omap_aes_mod_exit(void)\r\n{\r\nplatform_driver_unregister(&omap_aes_driver);\r\n}
