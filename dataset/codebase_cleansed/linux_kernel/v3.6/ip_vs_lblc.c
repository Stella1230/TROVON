static inline void ip_vs_lblc_free(struct ip_vs_lblc_entry *en)\r\n{\r\nlist_del(&en->list);\r\natomic_dec(&en->dest->refcnt);\r\nkfree(en);\r\n}\r\nstatic inline unsigned int\r\nip_vs_lblc_hashkey(int af, const union nf_inet_addr *addr)\r\n{\r\n__be32 addr_fold = addr->ip;\r\n#ifdef CONFIG_IP_VS_IPV6\r\nif (af == AF_INET6)\r\naddr_fold = addr->ip6[0]^addr->ip6[1]^\r\naddr->ip6[2]^addr->ip6[3];\r\n#endif\r\nreturn (ntohl(addr_fold)*2654435761UL) & IP_VS_LBLC_TAB_MASK;\r\n}\r\nstatic void\r\nip_vs_lblc_hash(struct ip_vs_lblc_table *tbl, struct ip_vs_lblc_entry *en)\r\n{\r\nunsigned int hash = ip_vs_lblc_hashkey(en->af, &en->addr);\r\nlist_add(&en->list, &tbl->bucket[hash]);\r\natomic_inc(&tbl->entries);\r\n}\r\nstatic inline struct ip_vs_lblc_entry *\r\nip_vs_lblc_get(int af, struct ip_vs_lblc_table *tbl,\r\nconst union nf_inet_addr *addr)\r\n{\r\nunsigned int hash = ip_vs_lblc_hashkey(af, addr);\r\nstruct ip_vs_lblc_entry *en;\r\nlist_for_each_entry(en, &tbl->bucket[hash], list)\r\nif (ip_vs_addr_equal(af, &en->addr, addr))\r\nreturn en;\r\nreturn NULL;\r\n}\r\nstatic inline struct ip_vs_lblc_entry *\r\nip_vs_lblc_new(struct ip_vs_lblc_table *tbl, const union nf_inet_addr *daddr,\r\nstruct ip_vs_dest *dest)\r\n{\r\nstruct ip_vs_lblc_entry *en;\r\nen = ip_vs_lblc_get(dest->af, tbl, daddr);\r\nif (!en) {\r\nen = kmalloc(sizeof(*en), GFP_ATOMIC);\r\nif (!en)\r\nreturn NULL;\r\nen->af = dest->af;\r\nip_vs_addr_copy(dest->af, &en->addr, daddr);\r\nen->lastuse = jiffies;\r\natomic_inc(&dest->refcnt);\r\nen->dest = dest;\r\nip_vs_lblc_hash(tbl, en);\r\n} else if (en->dest != dest) {\r\natomic_dec(&en->dest->refcnt);\r\natomic_inc(&dest->refcnt);\r\nen->dest = dest;\r\n}\r\nreturn en;\r\n}\r\nstatic void ip_vs_lblc_flush(struct ip_vs_lblc_table *tbl)\r\n{\r\nstruct ip_vs_lblc_entry *en, *nxt;\r\nint i;\r\nfor (i=0; i<IP_VS_LBLC_TAB_SIZE; i++) {\r\nlist_for_each_entry_safe(en, nxt, &tbl->bucket[i], list) {\r\nip_vs_lblc_free(en);\r\natomic_dec(&tbl->entries);\r\n}\r\n}\r\n}\r\nstatic int sysctl_lblc_expiration(struct ip_vs_service *svc)\r\n{\r\n#ifdef CONFIG_SYSCTL\r\nstruct netns_ipvs *ipvs = net_ipvs(svc->net);\r\nreturn ipvs->sysctl_lblc_expiration;\r\n#else\r\nreturn DEFAULT_EXPIRATION;\r\n#endif\r\n}\r\nstatic inline void ip_vs_lblc_full_check(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblc_table *tbl = svc->sched_data;\r\nstruct ip_vs_lblc_entry *en, *nxt;\r\nunsigned long now = jiffies;\r\nint i, j;\r\nfor (i=0, j=tbl->rover; i<IP_VS_LBLC_TAB_SIZE; i++) {\r\nj = (j + 1) & IP_VS_LBLC_TAB_MASK;\r\nwrite_lock(&svc->sched_lock);\r\nlist_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {\r\nif (time_before(now,\r\nen->lastuse +\r\nsysctl_lblc_expiration(svc)))\r\ncontinue;\r\nip_vs_lblc_free(en);\r\natomic_dec(&tbl->entries);\r\n}\r\nwrite_unlock(&svc->sched_lock);\r\n}\r\ntbl->rover = j;\r\n}\r\nstatic void ip_vs_lblc_check_expire(unsigned long data)\r\n{\r\nstruct ip_vs_service *svc = (struct ip_vs_service *) data;\r\nstruct ip_vs_lblc_table *tbl = svc->sched_data;\r\nunsigned long now = jiffies;\r\nint goal;\r\nint i, j;\r\nstruct ip_vs_lblc_entry *en, *nxt;\r\nif ((tbl->counter % COUNT_FOR_FULL_EXPIRATION) == 0) {\r\nip_vs_lblc_full_check(svc);\r\ntbl->counter = 1;\r\ngoto out;\r\n}\r\nif (atomic_read(&tbl->entries) <= tbl->max_size) {\r\ntbl->counter++;\r\ngoto out;\r\n}\r\ngoal = (atomic_read(&tbl->entries) - tbl->max_size)*4/3;\r\nif (goal > tbl->max_size/2)\r\ngoal = tbl->max_size/2;\r\nfor (i=0, j=tbl->rover; i<IP_VS_LBLC_TAB_SIZE; i++) {\r\nj = (j + 1) & IP_VS_LBLC_TAB_MASK;\r\nwrite_lock(&svc->sched_lock);\r\nlist_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {\r\nif (time_before(now, en->lastuse + ENTRY_TIMEOUT))\r\ncontinue;\r\nip_vs_lblc_free(en);\r\natomic_dec(&tbl->entries);\r\ngoal--;\r\n}\r\nwrite_unlock(&svc->sched_lock);\r\nif (goal <= 0)\r\nbreak;\r\n}\r\ntbl->rover = j;\r\nout:\r\nmod_timer(&tbl->periodic_timer, jiffies+CHECK_EXPIRE_INTERVAL);\r\n}\r\nstatic int ip_vs_lblc_init_svc(struct ip_vs_service *svc)\r\n{\r\nint i;\r\nstruct ip_vs_lblc_table *tbl;\r\ntbl = kmalloc(sizeof(*tbl), GFP_KERNEL);\r\nif (tbl == NULL)\r\nreturn -ENOMEM;\r\nsvc->sched_data = tbl;\r\nIP_VS_DBG(6, "LBLC hash table (memory=%Zdbytes) allocated for "\r\n"current service\n", sizeof(*tbl));\r\nfor (i=0; i<IP_VS_LBLC_TAB_SIZE; i++) {\r\nINIT_LIST_HEAD(&tbl->bucket[i]);\r\n}\r\ntbl->max_size = IP_VS_LBLC_TAB_SIZE*16;\r\ntbl->rover = 0;\r\ntbl->counter = 1;\r\nsetup_timer(&tbl->periodic_timer, ip_vs_lblc_check_expire,\r\n(unsigned long)svc);\r\nmod_timer(&tbl->periodic_timer, jiffies + CHECK_EXPIRE_INTERVAL);\r\nreturn 0;\r\n}\r\nstatic int ip_vs_lblc_done_svc(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblc_table *tbl = svc->sched_data;\r\ndel_timer_sync(&tbl->periodic_timer);\r\nip_vs_lblc_flush(tbl);\r\nkfree(tbl);\r\nIP_VS_DBG(6, "LBLC hash table (memory=%Zdbytes) released\n",\r\nsizeof(*tbl));\r\nreturn 0;\r\n}\r\nstatic inline struct ip_vs_dest *\r\n__ip_vs_lblc_schedule(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_dest *dest, *least;\r\nint loh, doh;\r\nlist_for_each_entry(dest, &svc->destinations, n_list) {\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\nif (atomic_read(&dest->weight) > 0) {\r\nleast = dest;\r\nloh = ip_vs_dest_conn_overhead(least);\r\ngoto nextstage;\r\n}\r\n}\r\nreturn NULL;\r\nnextstage:\r\nlist_for_each_entry_continue(dest, &svc->destinations, n_list) {\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\ndoh = ip_vs_dest_conn_overhead(dest);\r\nif (loh * atomic_read(&dest->weight) >\r\ndoh * atomic_read(&least->weight)) {\r\nleast = dest;\r\nloh = doh;\r\n}\r\n}\r\nIP_VS_DBG_BUF(6, "LBLC: server %s:%d "\r\n"activeconns %d refcnt %d weight %d overhead %d\n",\r\nIP_VS_DBG_ADDR(least->af, &least->addr),\r\nntohs(least->port),\r\natomic_read(&least->activeconns),\r\natomic_read(&least->refcnt),\r\natomic_read(&least->weight), loh);\r\nreturn least;\r\n}\r\nstatic inline int\r\nis_overloaded(struct ip_vs_dest *dest, struct ip_vs_service *svc)\r\n{\r\nif (atomic_read(&dest->activeconns) > atomic_read(&dest->weight)) {\r\nstruct ip_vs_dest *d;\r\nlist_for_each_entry(d, &svc->destinations, n_list) {\r\nif (atomic_read(&d->activeconns)*2\r\n< atomic_read(&d->weight)) {\r\nreturn 1;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic struct ip_vs_dest *\r\nip_vs_lblc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)\r\n{\r\nstruct ip_vs_lblc_table *tbl = svc->sched_data;\r\nstruct ip_vs_iphdr iph;\r\nstruct ip_vs_dest *dest = NULL;\r\nstruct ip_vs_lblc_entry *en;\r\nip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);\r\nIP_VS_DBG(6, "%s(): Scheduling...\n", __func__);\r\nread_lock(&svc->sched_lock);\r\nen = ip_vs_lblc_get(svc->af, tbl, &iph.daddr);\r\nif (en) {\r\nen->lastuse = jiffies;\r\nif (en->dest->flags & IP_VS_DEST_F_AVAILABLE)\r\ndest = en->dest;\r\n}\r\nread_unlock(&svc->sched_lock);\r\nif (dest && atomic_read(&dest->weight) > 0 && !is_overloaded(dest, svc))\r\ngoto out;\r\ndest = __ip_vs_lblc_schedule(svc);\r\nif (!dest) {\r\nip_vs_scheduler_err(svc, "no destination available");\r\nreturn NULL;\r\n}\r\nwrite_lock(&svc->sched_lock);\r\nip_vs_lblc_new(tbl, &iph.daddr, dest);\r\nwrite_unlock(&svc->sched_lock);\r\nout:\r\nIP_VS_DBG_BUF(6, "LBLC: destination IP address %s --> server %s:%d\n",\r\nIP_VS_DBG_ADDR(svc->af, &iph.daddr),\r\nIP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port));\r\nreturn dest;\r\n}\r\nstatic int __net_init __ip_vs_lblc_init(struct net *net)\r\n{\r\nstruct netns_ipvs *ipvs = net_ipvs(net);\r\nif (!ipvs)\r\nreturn -ENOENT;\r\nif (!net_eq(net, &init_net)) {\r\nipvs->lblc_ctl_table = kmemdup(vs_vars_table,\r\nsizeof(vs_vars_table),\r\nGFP_KERNEL);\r\nif (ipvs->lblc_ctl_table == NULL)\r\nreturn -ENOMEM;\r\n} else\r\nipvs->lblc_ctl_table = vs_vars_table;\r\nipvs->sysctl_lblc_expiration = DEFAULT_EXPIRATION;\r\nipvs->lblc_ctl_table[0].data = &ipvs->sysctl_lblc_expiration;\r\nipvs->lblc_ctl_header =\r\nregister_net_sysctl(net, "net/ipv4/vs", ipvs->lblc_ctl_table);\r\nif (!ipvs->lblc_ctl_header) {\r\nif (!net_eq(net, &init_net))\r\nkfree(ipvs->lblc_ctl_table);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __net_exit __ip_vs_lblc_exit(struct net *net)\r\n{\r\nstruct netns_ipvs *ipvs = net_ipvs(net);\r\nunregister_net_sysctl_table(ipvs->lblc_ctl_header);\r\nif (!net_eq(net, &init_net))\r\nkfree(ipvs->lblc_ctl_table);\r\n}\r\nstatic int __net_init __ip_vs_lblc_init(struct net *net) { return 0; }\r\nstatic void __net_exit __ip_vs_lblc_exit(struct net *net) { }\r\nstatic int __init ip_vs_lblc_init(void)\r\n{\r\nint ret;\r\nret = register_pernet_subsys(&ip_vs_lblc_ops);\r\nif (ret)\r\nreturn ret;\r\nret = register_ip_vs_scheduler(&ip_vs_lblc_scheduler);\r\nif (ret)\r\nunregister_pernet_subsys(&ip_vs_lblc_ops);\r\nreturn ret;\r\n}\r\nstatic void __exit ip_vs_lblc_cleanup(void)\r\n{\r\nunregister_ip_vs_scheduler(&ip_vs_lblc_scheduler);\r\nunregister_pernet_subsys(&ip_vs_lblc_ops);\r\n}
