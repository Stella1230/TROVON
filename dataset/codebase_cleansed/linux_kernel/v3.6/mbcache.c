static inline int\r\n__mb_cache_entry_is_hashed(struct mb_cache_entry *ce)\r\n{\r\nreturn !list_empty(&ce->e_block_list);\r\n}\r\nstatic void\r\n__mb_cache_entry_unhash(struct mb_cache_entry *ce)\r\n{\r\nif (__mb_cache_entry_is_hashed(ce)) {\r\nlist_del_init(&ce->e_block_list);\r\nlist_del(&ce->e_index.o_list);\r\n}\r\n}\r\nstatic void\r\n__mb_cache_entry_forget(struct mb_cache_entry *ce, gfp_t gfp_mask)\r\n{\r\nstruct mb_cache *cache = ce->e_cache;\r\nmb_assert(!(ce->e_used || ce->e_queued));\r\nkmem_cache_free(cache->c_entry_cache, ce);\r\natomic_dec(&cache->c_entry_count);\r\n}\r\nstatic void\r\n__mb_cache_entry_release_unlock(struct mb_cache_entry *ce)\r\n__releases(mb_cache_spinlock)\r\n{\r\nif (ce->e_queued)\r\nwake_up_all(&mb_cache_queue);\r\nif (ce->e_used >= MB_CACHE_WRITER)\r\nce->e_used -= MB_CACHE_WRITER;\r\nce->e_used--;\r\nif (!(ce->e_used || ce->e_queued)) {\r\nif (!__mb_cache_entry_is_hashed(ce))\r\ngoto forget;\r\nmb_assert(list_empty(&ce->e_lru_list));\r\nlist_add_tail(&ce->e_lru_list, &mb_cache_lru_list);\r\n}\r\nspin_unlock(&mb_cache_spinlock);\r\nreturn;\r\nforget:\r\nspin_unlock(&mb_cache_spinlock);\r\n__mb_cache_entry_forget(ce, GFP_KERNEL);\r\n}\r\nstatic int\r\nmb_cache_shrink_fn(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nLIST_HEAD(free_list);\r\nstruct mb_cache *cache;\r\nstruct mb_cache_entry *entry, *tmp;\r\nint count = 0;\r\nint nr_to_scan = sc->nr_to_scan;\r\ngfp_t gfp_mask = sc->gfp_mask;\r\nmb_debug("trying to free %d entries", nr_to_scan);\r\nspin_lock(&mb_cache_spinlock);\r\nwhile (nr_to_scan-- && !list_empty(&mb_cache_lru_list)) {\r\nstruct mb_cache_entry *ce =\r\nlist_entry(mb_cache_lru_list.next,\r\nstruct mb_cache_entry, e_lru_list);\r\nlist_move_tail(&ce->e_lru_list, &free_list);\r\n__mb_cache_entry_unhash(ce);\r\n}\r\nlist_for_each_entry(cache, &mb_cache_list, c_cache_list) {\r\nmb_debug("cache %s (%d)", cache->c_name,\r\natomic_read(&cache->c_entry_count));\r\ncount += atomic_read(&cache->c_entry_count);\r\n}\r\nspin_unlock(&mb_cache_spinlock);\r\nlist_for_each_entry_safe(entry, tmp, &free_list, e_lru_list) {\r\n__mb_cache_entry_forget(entry, gfp_mask);\r\n}\r\nreturn (count / 100) * sysctl_vfs_cache_pressure;\r\n}\r\nstruct mb_cache *\r\nmb_cache_create(const char *name, int bucket_bits)\r\n{\r\nint n, bucket_count = 1 << bucket_bits;\r\nstruct mb_cache *cache = NULL;\r\ncache = kmalloc(sizeof(struct mb_cache), GFP_KERNEL);\r\nif (!cache)\r\nreturn NULL;\r\ncache->c_name = name;\r\natomic_set(&cache->c_entry_count, 0);\r\ncache->c_bucket_bits = bucket_bits;\r\ncache->c_block_hash = kmalloc(bucket_count * sizeof(struct list_head),\r\nGFP_KERNEL);\r\nif (!cache->c_block_hash)\r\ngoto fail;\r\nfor (n=0; n<bucket_count; n++)\r\nINIT_LIST_HEAD(&cache->c_block_hash[n]);\r\ncache->c_index_hash = kmalloc(bucket_count * sizeof(struct list_head),\r\nGFP_KERNEL);\r\nif (!cache->c_index_hash)\r\ngoto fail;\r\nfor (n=0; n<bucket_count; n++)\r\nINIT_LIST_HEAD(&cache->c_index_hash[n]);\r\ncache->c_entry_cache = kmem_cache_create(name,\r\nsizeof(struct mb_cache_entry), 0,\r\nSLAB_RECLAIM_ACCOUNT|SLAB_MEM_SPREAD, NULL);\r\nif (!cache->c_entry_cache)\r\ngoto fail2;\r\ncache->c_max_entries = bucket_count << 4;\r\nspin_lock(&mb_cache_spinlock);\r\nlist_add(&cache->c_cache_list, &mb_cache_list);\r\nspin_unlock(&mb_cache_spinlock);\r\nreturn cache;\r\nfail2:\r\nkfree(cache->c_index_hash);\r\nfail:\r\nkfree(cache->c_block_hash);\r\nkfree(cache);\r\nreturn NULL;\r\n}\r\nvoid\r\nmb_cache_shrink(struct block_device *bdev)\r\n{\r\nLIST_HEAD(free_list);\r\nstruct list_head *l, *ltmp;\r\nspin_lock(&mb_cache_spinlock);\r\nlist_for_each_safe(l, ltmp, &mb_cache_lru_list) {\r\nstruct mb_cache_entry *ce =\r\nlist_entry(l, struct mb_cache_entry, e_lru_list);\r\nif (ce->e_bdev == bdev) {\r\nlist_move_tail(&ce->e_lru_list, &free_list);\r\n__mb_cache_entry_unhash(ce);\r\n}\r\n}\r\nspin_unlock(&mb_cache_spinlock);\r\nlist_for_each_safe(l, ltmp, &free_list) {\r\n__mb_cache_entry_forget(list_entry(l, struct mb_cache_entry,\r\ne_lru_list), GFP_KERNEL);\r\n}\r\n}\r\nvoid\r\nmb_cache_destroy(struct mb_cache *cache)\r\n{\r\nLIST_HEAD(free_list);\r\nstruct list_head *l, *ltmp;\r\nspin_lock(&mb_cache_spinlock);\r\nlist_for_each_safe(l, ltmp, &mb_cache_lru_list) {\r\nstruct mb_cache_entry *ce =\r\nlist_entry(l, struct mb_cache_entry, e_lru_list);\r\nif (ce->e_cache == cache) {\r\nlist_move_tail(&ce->e_lru_list, &free_list);\r\n__mb_cache_entry_unhash(ce);\r\n}\r\n}\r\nlist_del(&cache->c_cache_list);\r\nspin_unlock(&mb_cache_spinlock);\r\nlist_for_each_safe(l, ltmp, &free_list) {\r\n__mb_cache_entry_forget(list_entry(l, struct mb_cache_entry,\r\ne_lru_list), GFP_KERNEL);\r\n}\r\nif (atomic_read(&cache->c_entry_count) > 0) {\r\nmb_error("cache %s: %d orphaned entries",\r\ncache->c_name,\r\natomic_read(&cache->c_entry_count));\r\n}\r\nkmem_cache_destroy(cache->c_entry_cache);\r\nkfree(cache->c_index_hash);\r\nkfree(cache->c_block_hash);\r\nkfree(cache);\r\n}\r\nstruct mb_cache_entry *\r\nmb_cache_entry_alloc(struct mb_cache *cache, gfp_t gfp_flags)\r\n{\r\nstruct mb_cache_entry *ce = NULL;\r\nif (atomic_read(&cache->c_entry_count) >= cache->c_max_entries) {\r\nspin_lock(&mb_cache_spinlock);\r\nif (!list_empty(&mb_cache_lru_list)) {\r\nce = list_entry(mb_cache_lru_list.next,\r\nstruct mb_cache_entry, e_lru_list);\r\nlist_del_init(&ce->e_lru_list);\r\n__mb_cache_entry_unhash(ce);\r\n}\r\nspin_unlock(&mb_cache_spinlock);\r\n}\r\nif (!ce) {\r\nce = kmem_cache_alloc(cache->c_entry_cache, gfp_flags);\r\nif (!ce)\r\nreturn NULL;\r\natomic_inc(&cache->c_entry_count);\r\nINIT_LIST_HEAD(&ce->e_lru_list);\r\nINIT_LIST_HEAD(&ce->e_block_list);\r\nce->e_cache = cache;\r\nce->e_queued = 0;\r\n}\r\nce->e_used = 1 + MB_CACHE_WRITER;\r\nreturn ce;\r\n}\r\nint\r\nmb_cache_entry_insert(struct mb_cache_entry *ce, struct block_device *bdev,\r\nsector_t block, unsigned int key)\r\n{\r\nstruct mb_cache *cache = ce->e_cache;\r\nunsigned int bucket;\r\nstruct list_head *l;\r\nint error = -EBUSY;\r\nbucket = hash_long((unsigned long)bdev + (block & 0xffffffff),\r\ncache->c_bucket_bits);\r\nspin_lock(&mb_cache_spinlock);\r\nlist_for_each_prev(l, &cache->c_block_hash[bucket]) {\r\nstruct mb_cache_entry *ce =\r\nlist_entry(l, struct mb_cache_entry, e_block_list);\r\nif (ce->e_bdev == bdev && ce->e_block == block)\r\ngoto out;\r\n}\r\n__mb_cache_entry_unhash(ce);\r\nce->e_bdev = bdev;\r\nce->e_block = block;\r\nlist_add(&ce->e_block_list, &cache->c_block_hash[bucket]);\r\nce->e_index.o_key = key;\r\nbucket = hash_long(key, cache->c_bucket_bits);\r\nlist_add(&ce->e_index.o_list, &cache->c_index_hash[bucket]);\r\nerror = 0;\r\nout:\r\nspin_unlock(&mb_cache_spinlock);\r\nreturn error;\r\n}\r\nvoid\r\nmb_cache_entry_release(struct mb_cache_entry *ce)\r\n{\r\nspin_lock(&mb_cache_spinlock);\r\n__mb_cache_entry_release_unlock(ce);\r\n}\r\nvoid\r\nmb_cache_entry_free(struct mb_cache_entry *ce)\r\n{\r\nspin_lock(&mb_cache_spinlock);\r\nmb_assert(list_empty(&ce->e_lru_list));\r\n__mb_cache_entry_unhash(ce);\r\n__mb_cache_entry_release_unlock(ce);\r\n}\r\nstruct mb_cache_entry *\r\nmb_cache_entry_get(struct mb_cache *cache, struct block_device *bdev,\r\nsector_t block)\r\n{\r\nunsigned int bucket;\r\nstruct list_head *l;\r\nstruct mb_cache_entry *ce;\r\nbucket = hash_long((unsigned long)bdev + (block & 0xffffffff),\r\ncache->c_bucket_bits);\r\nspin_lock(&mb_cache_spinlock);\r\nlist_for_each(l, &cache->c_block_hash[bucket]) {\r\nce = list_entry(l, struct mb_cache_entry, e_block_list);\r\nif (ce->e_bdev == bdev && ce->e_block == block) {\r\nDEFINE_WAIT(wait);\r\nif (!list_empty(&ce->e_lru_list))\r\nlist_del_init(&ce->e_lru_list);\r\nwhile (ce->e_used > 0) {\r\nce->e_queued++;\r\nprepare_to_wait(&mb_cache_queue, &wait,\r\nTASK_UNINTERRUPTIBLE);\r\nspin_unlock(&mb_cache_spinlock);\r\nschedule();\r\nspin_lock(&mb_cache_spinlock);\r\nce->e_queued--;\r\n}\r\nfinish_wait(&mb_cache_queue, &wait);\r\nce->e_used += 1 + MB_CACHE_WRITER;\r\nif (!__mb_cache_entry_is_hashed(ce)) {\r\n__mb_cache_entry_release_unlock(ce);\r\nreturn NULL;\r\n}\r\ngoto cleanup;\r\n}\r\n}\r\nce = NULL;\r\ncleanup:\r\nspin_unlock(&mb_cache_spinlock);\r\nreturn ce;\r\n}\r\nstatic struct mb_cache_entry *\r\n__mb_cache_entry_find(struct list_head *l, struct list_head *head,\r\nstruct block_device *bdev, unsigned int key)\r\n{\r\nwhile (l != head) {\r\nstruct mb_cache_entry *ce =\r\nlist_entry(l, struct mb_cache_entry, e_index.o_list);\r\nif (ce->e_bdev == bdev && ce->e_index.o_key == key) {\r\nDEFINE_WAIT(wait);\r\nif (!list_empty(&ce->e_lru_list))\r\nlist_del_init(&ce->e_lru_list);\r\nce->e_used++;\r\nwhile (ce->e_used >= MB_CACHE_WRITER) {\r\nce->e_queued++;\r\nprepare_to_wait(&mb_cache_queue, &wait,\r\nTASK_UNINTERRUPTIBLE);\r\nspin_unlock(&mb_cache_spinlock);\r\nschedule();\r\nspin_lock(&mb_cache_spinlock);\r\nce->e_queued--;\r\n}\r\nfinish_wait(&mb_cache_queue, &wait);\r\nif (!__mb_cache_entry_is_hashed(ce)) {\r\n__mb_cache_entry_release_unlock(ce);\r\nspin_lock(&mb_cache_spinlock);\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nreturn ce;\r\n}\r\nl = l->next;\r\n}\r\nreturn NULL;\r\n}\r\nstruct mb_cache_entry *\r\nmb_cache_entry_find_first(struct mb_cache *cache, struct block_device *bdev,\r\nunsigned int key)\r\n{\r\nunsigned int bucket = hash_long(key, cache->c_bucket_bits);\r\nstruct list_head *l;\r\nstruct mb_cache_entry *ce;\r\nspin_lock(&mb_cache_spinlock);\r\nl = cache->c_index_hash[bucket].next;\r\nce = __mb_cache_entry_find(l, &cache->c_index_hash[bucket], bdev, key);\r\nspin_unlock(&mb_cache_spinlock);\r\nreturn ce;\r\n}\r\nstruct mb_cache_entry *\r\nmb_cache_entry_find_next(struct mb_cache_entry *prev,\r\nstruct block_device *bdev, unsigned int key)\r\n{\r\nstruct mb_cache *cache = prev->e_cache;\r\nunsigned int bucket = hash_long(key, cache->c_bucket_bits);\r\nstruct list_head *l;\r\nstruct mb_cache_entry *ce;\r\nspin_lock(&mb_cache_spinlock);\r\nl = prev->e_index.o_list.next;\r\nce = __mb_cache_entry_find(l, &cache->c_index_hash[bucket], bdev, key);\r\n__mb_cache_entry_release_unlock(prev);\r\nreturn ce;\r\n}\r\nstatic int __init init_mbcache(void)\r\n{\r\nregister_shrinker(&mb_cache_shrinker);\r\nreturn 0;\r\n}\r\nstatic void __exit exit_mbcache(void)\r\n{\r\nunregister_shrinker(&mb_cache_shrinker);\r\n}
