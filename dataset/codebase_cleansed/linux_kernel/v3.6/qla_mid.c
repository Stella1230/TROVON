void\r\nqla2x00_vp_stop_timer(scsi_qla_host_t *vha)\r\n{\r\nif (vha->vp_idx && vha->timer_active) {\r\ndel_timer_sync(&vha->timer);\r\nvha->timer_active = 0;\r\n}\r\n}\r\nstatic uint32_t\r\nqla24xx_allocate_vp_id(scsi_qla_host_t *vha)\r\n{\r\nuint32_t vp_id;\r\nstruct qla_hw_data *ha = vha->hw;\r\nunsigned long flags;\r\nmutex_lock(&ha->vport_lock);\r\nvp_id = find_first_zero_bit(ha->vp_idx_map, ha->max_npiv_vports + 1);\r\nif (vp_id > ha->max_npiv_vports) {\r\nql_dbg(ql_dbg_vport, vha, 0xa000,\r\n"vp_id %d is bigger than max-supported %d.\n",\r\nvp_id, ha->max_npiv_vports);\r\nmutex_unlock(&ha->vport_lock);\r\nreturn vp_id;\r\n}\r\nset_bit(vp_id, ha->vp_idx_map);\r\nha->num_vhosts++;\r\nvha->vp_idx = vp_id;\r\nspin_lock_irqsave(&ha->vport_slock, flags);\r\nlist_add_tail(&vha->list, &ha->vp_list);\r\nqlt_update_vp_map(vha, SET_VP_IDX);\r\nspin_unlock_irqrestore(&ha->vport_slock, flags);\r\nmutex_unlock(&ha->vport_lock);\r\nreturn vp_id;\r\n}\r\nvoid\r\nqla24xx_deallocate_vp_id(scsi_qla_host_t *vha)\r\n{\r\nuint16_t vp_id;\r\nstruct qla_hw_data *ha = vha->hw;\r\nunsigned long flags = 0;\r\nmutex_lock(&ha->vport_lock);\r\nspin_lock_irqsave(&ha->vport_slock, flags);\r\nwhile (atomic_read(&vha->vref_count)) {\r\nspin_unlock_irqrestore(&ha->vport_slock, flags);\r\nmsleep(500);\r\nspin_lock_irqsave(&ha->vport_slock, flags);\r\n}\r\nlist_del(&vha->list);\r\nqlt_update_vp_map(vha, RESET_VP_IDX);\r\nspin_unlock_irqrestore(&ha->vport_slock, flags);\r\nvp_id = vha->vp_idx;\r\nha->num_vhosts--;\r\nclear_bit(vp_id, ha->vp_idx_map);\r\nmutex_unlock(&ha->vport_lock);\r\n}\r\nstatic scsi_qla_host_t *\r\nqla24xx_find_vhost_by_name(struct qla_hw_data *ha, uint8_t *port_name)\r\n{\r\nscsi_qla_host_t *vha;\r\nstruct scsi_qla_host *tvha;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ha->vport_slock, flags);\r\nlist_for_each_entry_safe(vha, tvha, &ha->vp_list, list) {\r\nif (!memcmp(port_name, vha->port_name, WWN_SIZE)) {\r\nspin_unlock_irqrestore(&ha->vport_slock, flags);\r\nreturn vha;\r\n}\r\n}\r\nspin_unlock_irqrestore(&ha->vport_slock, flags);\r\nreturn NULL;\r\n}\r\nstatic void\r\nqla2x00_mark_vp_devices_dead(scsi_qla_host_t *vha)\r\n{\r\nfc_port_t *fcport;\r\nlist_for_each_entry(fcport, &vha->vp_fcports, list) {\r\nql_dbg(ql_dbg_vport, vha, 0xa001,\r\n"Marking port dead, loop_id=0x%04x : %x.\n",\r\nfcport->loop_id, fcport->vha->vp_idx);\r\nqla2x00_mark_device_lost(vha, fcport, 0, 0);\r\nqla2x00_set_fcport_state(fcport, FCS_UNCONFIGURED);\r\n}\r\n}\r\nint\r\nqla24xx_disable_vp(scsi_qla_host_t *vha)\r\n{\r\nint ret;\r\nret = qla24xx_control_vp(vha, VCE_COMMAND_DISABLE_VPS_LOGO_ALL);\r\natomic_set(&vha->loop_state, LOOP_DOWN);\r\natomic_set(&vha->loop_down_timer, LOOP_DOWN_TIME);\r\nqlt_update_vp_map(vha, RESET_AL_PA);\r\nqla2x00_mark_vp_devices_dead(vha);\r\natomic_set(&vha->vp_state, VP_FAILED);\r\nvha->flags.management_server_logged_in = 0;\r\nif (ret == QLA_SUCCESS) {\r\nfc_vport_set_state(vha->fc_vport, FC_VPORT_DISABLED);\r\n} else {\r\nfc_vport_set_state(vha->fc_vport, FC_VPORT_FAILED);\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nint\r\nqla24xx_enable_vp(scsi_qla_host_t *vha)\r\n{\r\nint ret;\r\nstruct qla_hw_data *ha = vha->hw;\r\nscsi_qla_host_t *base_vha = pci_get_drvdata(ha->pdev);\r\nif (atomic_read(&base_vha->loop_state) == LOOP_DOWN ||\r\natomic_read(&base_vha->loop_state) == LOOP_DEAD ||\r\n!(ha->current_topology & ISP_CFG_F)) {\r\nvha->vp_err_state = VP_ERR_PORTDWN;\r\nfc_vport_set_state(vha->fc_vport, FC_VPORT_LINKDOWN);\r\ngoto enable_failed;\r\n}\r\nmutex_lock(&ha->vport_lock);\r\nret = qla24xx_modify_vp_config(vha);\r\nmutex_unlock(&ha->vport_lock);\r\nif (ret != QLA_SUCCESS) {\r\nfc_vport_set_state(vha->fc_vport, FC_VPORT_FAILED);\r\ngoto enable_failed;\r\n}\r\nql_dbg(ql_dbg_taskm, vha, 0x801a,\r\n"Virtual port with id: %d - Enabled.\n", vha->vp_idx);\r\nreturn 0;\r\nenable_failed:\r\nql_dbg(ql_dbg_taskm, vha, 0x801b,\r\n"Virtual port with id: %d - Disabled.\n", vha->vp_idx);\r\nreturn 1;\r\n}\r\nstatic void\r\nqla24xx_configure_vp(scsi_qla_host_t *vha)\r\n{\r\nstruct fc_vport *fc_vport;\r\nint ret;\r\nfc_vport = vha->fc_vport;\r\nql_dbg(ql_dbg_vport, vha, 0xa002,\r\n"%s: change request #3.\n", __func__);\r\nret = qla2x00_send_change_request(vha, 0x3, vha->vp_idx);\r\nif (ret != QLA_SUCCESS) {\r\nql_dbg(ql_dbg_vport, vha, 0xa003, "Failed to enable "\r\n"receiving of RSCN requests: 0x%x.\n", ret);\r\nreturn;\r\n} else {\r\nclear_bit(VP_SCR_NEEDED, &vha->vp_flags);\r\n}\r\nvha->flags.online = 1;\r\nif (qla24xx_configure_vhba(vha))\r\nreturn;\r\natomic_set(&vha->vp_state, VP_ACTIVE);\r\nfc_vport_set_state(fc_vport, FC_VPORT_ACTIVE);\r\n}\r\nvoid\r\nqla2x00_alert_all_vps(struct rsp_que *rsp, uint16_t *mb)\r\n{\r\nscsi_qla_host_t *vha;\r\nstruct qla_hw_data *ha = rsp->hw;\r\nint i = 0;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ha->vport_slock, flags);\r\nlist_for_each_entry(vha, &ha->vp_list, list) {\r\nif (vha->vp_idx) {\r\natomic_inc(&vha->vref_count);\r\nspin_unlock_irqrestore(&ha->vport_slock, flags);\r\nswitch (mb[0]) {\r\ncase MBA_LIP_OCCURRED:\r\ncase MBA_LOOP_UP:\r\ncase MBA_LOOP_DOWN:\r\ncase MBA_LIP_RESET:\r\ncase MBA_POINT_TO_POINT:\r\ncase MBA_CHG_IN_CONNECTION:\r\ncase MBA_PORT_UPDATE:\r\ncase MBA_RSCN_UPDATE:\r\nql_dbg(ql_dbg_async, vha, 0x5024,\r\n"Async_event for VP[%d], mb=0x%x vha=%p.\n",\r\ni, *mb, vha);\r\nqla2x00_async_event(vha, rsp, mb);\r\nbreak;\r\n}\r\nspin_lock_irqsave(&ha->vport_slock, flags);\r\natomic_dec(&vha->vref_count);\r\n}\r\ni++;\r\n}\r\nspin_unlock_irqrestore(&ha->vport_slock, flags);\r\n}\r\nint\r\nqla2x00_vp_abort_isp(scsi_qla_host_t *vha)\r\n{\r\nif (atomic_read(&vha->loop_state) != LOOP_DOWN) {\r\natomic_set(&vha->loop_state, LOOP_DOWN);\r\nqla2x00_mark_all_devices_lost(vha, 0);\r\n} else {\r\nif (!atomic_read(&vha->loop_down_timer))\r\natomic_set(&vha->loop_down_timer, LOOP_DOWN_TIME);\r\n}\r\nif (!test_bit(ABORT_ISP_ACTIVE, &vha->dpc_flags))\r\nqla24xx_control_vp(vha, VCE_COMMAND_DISABLE_VPS_LOGO_ALL);\r\nql_dbg(ql_dbg_taskm, vha, 0x801d,\r\n"Scheduling enable of Vport %d.\n", vha->vp_idx);\r\nreturn qla24xx_enable_vp(vha);\r\n}\r\nstatic int\r\nqla2x00_do_dpc_vp(scsi_qla_host_t *vha)\r\n{\r\nql_dbg(ql_dbg_dpc + ql_dbg_verbose, vha, 0x4012,\r\n"Entering %s vp_flags: 0x%lx.\n", __func__, vha->vp_flags);\r\nqla2x00_do_work(vha);\r\nif (test_and_clear_bit(VP_IDX_ACQUIRED, &vha->vp_flags)) {\r\nql_dbg(ql_dbg_dpc, vha, 0x4014,\r\n"Configure VP scheduled.\n");\r\nqla24xx_configure_vp(vha);\r\nql_dbg(ql_dbg_dpc, vha, 0x4015,\r\n"Configure VP end.\n");\r\nreturn 0;\r\n}\r\nif (test_bit(FCPORT_UPDATE_NEEDED, &vha->dpc_flags)) {\r\nql_dbg(ql_dbg_dpc, vha, 0x4016,\r\n"FCPort update scheduled.\n");\r\nqla2x00_update_fcports(vha);\r\nclear_bit(FCPORT_UPDATE_NEEDED, &vha->dpc_flags);\r\nql_dbg(ql_dbg_dpc, vha, 0x4017,\r\n"FCPort update end.\n");\r\n}\r\nif ((test_and_clear_bit(RELOGIN_NEEDED, &vha->dpc_flags)) &&\r\n!test_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags) &&\r\natomic_read(&vha->loop_state) != LOOP_DOWN) {\r\nql_dbg(ql_dbg_dpc, vha, 0x4018,\r\n"Relogin needed scheduled.\n");\r\nqla2x00_relogin(vha);\r\nql_dbg(ql_dbg_dpc, vha, 0x4019,\r\n"Relogin needed end.\n");\r\n}\r\nif (test_and_clear_bit(RESET_MARKER_NEEDED, &vha->dpc_flags) &&\r\n(!(test_and_set_bit(RESET_ACTIVE, &vha->dpc_flags)))) {\r\nclear_bit(RESET_ACTIVE, &vha->dpc_flags);\r\n}\r\nif (test_and_clear_bit(LOOP_RESYNC_NEEDED, &vha->dpc_flags)) {\r\nif (!(test_and_set_bit(LOOP_RESYNC_ACTIVE, &vha->dpc_flags))) {\r\nql_dbg(ql_dbg_dpc, vha, 0x401a,\r\n"Loop resync scheduled.\n");\r\nqla2x00_loop_resync(vha);\r\nclear_bit(LOOP_RESYNC_ACTIVE, &vha->dpc_flags);\r\nql_dbg(ql_dbg_dpc, vha, 0x401b,\r\n"Loop resync end.\n");\r\n}\r\n}\r\nql_dbg(ql_dbg_dpc + ql_dbg_verbose, vha, 0x401c,\r\n"Exiting %s.\n", __func__);\r\nreturn 0;\r\n}\r\nvoid\r\nqla2x00_do_dpc_all_vps(scsi_qla_host_t *vha)\r\n{\r\nint ret;\r\nstruct qla_hw_data *ha = vha->hw;\r\nscsi_qla_host_t *vp;\r\nunsigned long flags = 0;\r\nif (vha->vp_idx)\r\nreturn;\r\nif (list_empty(&ha->vp_list))\r\nreturn;\r\nclear_bit(VP_DPC_NEEDED, &vha->dpc_flags);\r\nif (!(ha->current_topology & ISP_CFG_F))\r\nreturn;\r\nspin_lock_irqsave(&ha->vport_slock, flags);\r\nlist_for_each_entry(vp, &ha->vp_list, list) {\r\nif (vp->vp_idx) {\r\natomic_inc(&vp->vref_count);\r\nspin_unlock_irqrestore(&ha->vport_slock, flags);\r\nret = qla2x00_do_dpc_vp(vp);\r\nspin_lock_irqsave(&ha->vport_slock, flags);\r\natomic_dec(&vp->vref_count);\r\n}\r\n}\r\nspin_unlock_irqrestore(&ha->vport_slock, flags);\r\n}\r\nint\r\nqla24xx_vport_create_req_sanity_check(struct fc_vport *fc_vport)\r\n{\r\nscsi_qla_host_t *base_vha = shost_priv(fc_vport->shost);\r\nstruct qla_hw_data *ha = base_vha->hw;\r\nscsi_qla_host_t *vha;\r\nuint8_t port_name[WWN_SIZE];\r\nif (fc_vport->roles != FC_PORT_ROLE_FCP_INITIATOR)\r\nreturn VPCERR_UNSUPPORTED;\r\nif (!ha->flags.npiv_supported)\r\nreturn VPCERR_UNSUPPORTED;\r\nif (!(ha->switch_cap & FLOGI_MID_SUPPORT))\r\nreturn VPCERR_NO_FABRIC_SUPP;\r\nu64_to_wwn(fc_vport->port_name, port_name);\r\nif (!memcmp(port_name, base_vha->port_name, WWN_SIZE))\r\nreturn VPCERR_BAD_WWN;\r\nvha = qla24xx_find_vhost_by_name(ha, port_name);\r\nif (vha)\r\nreturn VPCERR_BAD_WWN;\r\nif (ha->num_vhosts > ha->max_npiv_vports) {\r\nql_dbg(ql_dbg_vport, vha, 0xa004,\r\n"num_vhosts %ud is bigger "\r\n"than max_npiv_vports %ud.\n",\r\nha->num_vhosts, ha->max_npiv_vports);\r\nreturn VPCERR_UNSUPPORTED;\r\n}\r\nreturn 0;\r\n}\r\nscsi_qla_host_t *\r\nqla24xx_create_vhost(struct fc_vport *fc_vport)\r\n{\r\nscsi_qla_host_t *base_vha = shost_priv(fc_vport->shost);\r\nstruct qla_hw_data *ha = base_vha->hw;\r\nscsi_qla_host_t *vha;\r\nstruct scsi_host_template *sht = &qla2xxx_driver_template;\r\nstruct Scsi_Host *host;\r\nvha = qla2x00_create_host(sht, ha);\r\nif (!vha) {\r\nql_log(ql_log_warn, vha, 0xa005,\r\n"scsi_host_alloc() failed for vport.\n");\r\nreturn(NULL);\r\n}\r\nhost = vha->host;\r\nfc_vport->dd_data = vha;\r\nu64_to_wwn(fc_vport->node_name, vha->node_name);\r\nu64_to_wwn(fc_vport->port_name, vha->port_name);\r\nvha->fc_vport = fc_vport;\r\nvha->device_flags = 0;\r\nvha->vp_idx = qla24xx_allocate_vp_id(vha);\r\nif (vha->vp_idx > ha->max_npiv_vports) {\r\nql_dbg(ql_dbg_vport, vha, 0xa006,\r\n"Couldn't allocate vp_id.\n");\r\ngoto create_vhost_failed;\r\n}\r\nvha->mgmt_svr_loop_id = 10 + vha->vp_idx;\r\nvha->dpc_flags = 0L;\r\nset_bit(VP_SCR_NEEDED, &vha->vp_flags);\r\natomic_set(&vha->loop_state, LOOP_DOWN);\r\natomic_set(&vha->loop_down_timer, LOOP_DOWN_TIME);\r\nqla2x00_start_timer(vha, qla2x00_timer, WATCH_INTERVAL);\r\nvha->req = base_vha->req;\r\nhost->can_queue = base_vha->req->length + 128;\r\nhost->this_id = 255;\r\nhost->cmd_per_lun = 3;\r\nif (IS_T10_PI_CAPABLE(ha) && ql2xenabledif)\r\nhost->max_cmd_len = 32;\r\nelse\r\nhost->max_cmd_len = MAX_CMDSZ;\r\nhost->max_channel = MAX_BUSES - 1;\r\nhost->max_lun = ql2xmaxlun;\r\nhost->unique_id = host->host_no;\r\nhost->max_id = ha->max_fibre_devices;\r\nhost->transportt = qla2xxx_transport_vport_template;\r\nql_dbg(ql_dbg_vport, vha, 0xa007,\r\n"Detect vport hba %ld at address = %p.\n",\r\nvha->host_no, vha);\r\nvha->flags.init_done = 1;\r\nmutex_lock(&ha->vport_lock);\r\nset_bit(vha->vp_idx, ha->vp_idx_map);\r\nha->cur_vport_count++;\r\nmutex_unlock(&ha->vport_lock);\r\nreturn vha;\r\ncreate_vhost_failed:\r\nreturn NULL;\r\n}\r\nstatic void\r\nqla25xx_free_req_que(struct scsi_qla_host *vha, struct req_que *req)\r\n{\r\nstruct qla_hw_data *ha = vha->hw;\r\nuint16_t que_id = req->id;\r\ndma_free_coherent(&ha->pdev->dev, (req->length + 1) *\r\nsizeof(request_t), req->ring, req->dma);\r\nreq->ring = NULL;\r\nreq->dma = 0;\r\nif (que_id) {\r\nha->req_q_map[que_id] = NULL;\r\nmutex_lock(&ha->vport_lock);\r\nclear_bit(que_id, ha->req_qid_map);\r\nmutex_unlock(&ha->vport_lock);\r\n}\r\nkfree(req);\r\nreq = NULL;\r\n}\r\nstatic void\r\nqla25xx_free_rsp_que(struct scsi_qla_host *vha, struct rsp_que *rsp)\r\n{\r\nstruct qla_hw_data *ha = vha->hw;\r\nuint16_t que_id = rsp->id;\r\nif (rsp->msix && rsp->msix->have_irq) {\r\nfree_irq(rsp->msix->vector, rsp);\r\nrsp->msix->have_irq = 0;\r\nrsp->msix->rsp = NULL;\r\n}\r\ndma_free_coherent(&ha->pdev->dev, (rsp->length + 1) *\r\nsizeof(response_t), rsp->ring, rsp->dma);\r\nrsp->ring = NULL;\r\nrsp->dma = 0;\r\nif (que_id) {\r\nha->rsp_q_map[que_id] = NULL;\r\nmutex_lock(&ha->vport_lock);\r\nclear_bit(que_id, ha->rsp_qid_map);\r\nmutex_unlock(&ha->vport_lock);\r\n}\r\nkfree(rsp);\r\nrsp = NULL;\r\n}\r\nint\r\nqla25xx_delete_req_que(struct scsi_qla_host *vha, struct req_que *req)\r\n{\r\nint ret = -1;\r\nif (req) {\r\nreq->options |= BIT_0;\r\nret = qla25xx_init_req_que(vha, req);\r\n}\r\nif (ret == QLA_SUCCESS)\r\nqla25xx_free_req_que(vha, req);\r\nreturn ret;\r\n}\r\nstatic int\r\nqla25xx_delete_rsp_que(struct scsi_qla_host *vha, struct rsp_que *rsp)\r\n{\r\nint ret = -1;\r\nif (rsp) {\r\nrsp->options |= BIT_0;\r\nret = qla25xx_init_rsp_que(vha, rsp);\r\n}\r\nif (ret == QLA_SUCCESS)\r\nqla25xx_free_rsp_que(vha, rsp);\r\nreturn ret;\r\n}\r\nint\r\nqla25xx_delete_queues(struct scsi_qla_host *vha)\r\n{\r\nint cnt, ret = 0;\r\nstruct req_que *req = NULL;\r\nstruct rsp_que *rsp = NULL;\r\nstruct qla_hw_data *ha = vha->hw;\r\nfor (cnt = 1; cnt < ha->max_req_queues; cnt++) {\r\nreq = ha->req_q_map[cnt];\r\nif (req) {\r\nret = qla25xx_delete_req_que(vha, req);\r\nif (ret != QLA_SUCCESS) {\r\nql_log(ql_log_warn, vha, 0x00ea,\r\n"Couldn't delete req que %d.\n",\r\nreq->id);\r\nreturn ret;\r\n}\r\n}\r\n}\r\nfor (cnt = 1; cnt < ha->max_rsp_queues; cnt++) {\r\nrsp = ha->rsp_q_map[cnt];\r\nif (rsp) {\r\nret = qla25xx_delete_rsp_que(vha, rsp);\r\nif (ret != QLA_SUCCESS) {\r\nql_log(ql_log_warn, vha, 0x00eb,\r\n"Couldn't delete rsp que %d.\n",\r\nrsp->id);\r\nreturn ret;\r\n}\r\n}\r\n}\r\nreturn ret;\r\n}\r\nint\r\nqla25xx_create_req_que(struct qla_hw_data *ha, uint16_t options,\r\nuint8_t vp_idx, uint16_t rid, int rsp_que, uint8_t qos)\r\n{\r\nint ret = 0;\r\nstruct req_que *req = NULL;\r\nstruct scsi_qla_host *base_vha = pci_get_drvdata(ha->pdev);\r\nuint16_t que_id = 0;\r\ndevice_reg_t __iomem *reg;\r\nuint32_t cnt;\r\nreq = kzalloc(sizeof(struct req_que), GFP_KERNEL);\r\nif (req == NULL) {\r\nql_log(ql_log_fatal, base_vha, 0x00d9,\r\n"Failed to allocate memory for request queue.\n");\r\ngoto failed;\r\n}\r\nreq->length = REQUEST_ENTRY_CNT_24XX;\r\nreq->ring = dma_alloc_coherent(&ha->pdev->dev,\r\n(req->length + 1) * sizeof(request_t),\r\n&req->dma, GFP_KERNEL);\r\nif (req->ring == NULL) {\r\nql_log(ql_log_fatal, base_vha, 0x00da,\r\n"Failed to allocte memory for request_ring.\n");\r\ngoto que_failed;\r\n}\r\nmutex_lock(&ha->vport_lock);\r\nque_id = find_first_zero_bit(ha->req_qid_map, ha->max_req_queues);\r\nif (que_id >= ha->max_req_queues) {\r\nmutex_unlock(&ha->vport_lock);\r\nql_log(ql_log_warn, base_vha, 0x00db,\r\n"No resources to create additional request queue.\n");\r\ngoto que_failed;\r\n}\r\nset_bit(que_id, ha->req_qid_map);\r\nha->req_q_map[que_id] = req;\r\nreq->rid = rid;\r\nreq->vp_idx = vp_idx;\r\nreq->qos = qos;\r\nql_dbg(ql_dbg_multiq, base_vha, 0xc002,\r\n"queue_id=%d rid=%d vp_idx=%d qos=%d.\n",\r\nque_id, req->rid, req->vp_idx, req->qos);\r\nql_dbg(ql_dbg_init, base_vha, 0x00dc,\r\n"queue_id=%d rid=%d vp_idx=%d qos=%d.\n",\r\nque_id, req->rid, req->vp_idx, req->qos);\r\nif (rsp_que < 0)\r\nreq->rsp = NULL;\r\nelse\r\nreq->rsp = ha->rsp_q_map[rsp_que];\r\nif (MSB(req->rid))\r\noptions |= BIT_4;\r\nif (LSB(req->rid))\r\noptions |= BIT_5;\r\nreq->options = options;\r\nql_dbg(ql_dbg_multiq, base_vha, 0xc003,\r\n"options=0x%x.\n", req->options);\r\nql_dbg(ql_dbg_init, base_vha, 0x00dd,\r\n"options=0x%x.\n", req->options);\r\nfor (cnt = 1; cnt < MAX_OUTSTANDING_COMMANDS; cnt++)\r\nreq->outstanding_cmds[cnt] = NULL;\r\nreq->current_outstanding_cmd = 1;\r\nreq->ring_ptr = req->ring;\r\nreq->ring_index = 0;\r\nreq->cnt = req->length;\r\nreq->id = que_id;\r\nreg = ISP_QUE_REG(ha, que_id);\r\nreq->max_q_depth = ha->req_q_map[0]->max_q_depth;\r\nmutex_unlock(&ha->vport_lock);\r\nql_dbg(ql_dbg_multiq, base_vha, 0xc004,\r\n"ring_ptr=%p ring_index=%d, "\r\n"cnt=%d id=%d max_q_depth=%d.\n",\r\nreq->ring_ptr, req->ring_index,\r\nreq->cnt, req->id, req->max_q_depth);\r\nql_dbg(ql_dbg_init, base_vha, 0x00de,\r\n"ring_ptr=%p ring_index=%d, "\r\n"cnt=%d id=%d max_q_depth=%d.\n",\r\nreq->ring_ptr, req->ring_index, req->cnt,\r\nreq->id, req->max_q_depth);\r\nret = qla25xx_init_req_que(base_vha, req);\r\nif (ret != QLA_SUCCESS) {\r\nql_log(ql_log_fatal, base_vha, 0x00df,\r\n"%s failed.\n", __func__);\r\nmutex_lock(&ha->vport_lock);\r\nclear_bit(que_id, ha->req_qid_map);\r\nmutex_unlock(&ha->vport_lock);\r\ngoto que_failed;\r\n}\r\nreturn req->id;\r\nque_failed:\r\nqla25xx_free_req_que(base_vha, req);\r\nfailed:\r\nreturn 0;\r\n}\r\nstatic void qla_do_work(struct work_struct *work)\r\n{\r\nunsigned long flags;\r\nstruct rsp_que *rsp = container_of(work, struct rsp_que, q_work);\r\nstruct scsi_qla_host *vha;\r\nstruct qla_hw_data *ha = rsp->hw;\r\nspin_lock_irqsave(&rsp->hw->hardware_lock, flags);\r\nvha = pci_get_drvdata(ha->pdev);\r\nqla24xx_process_response_queue(vha, rsp);\r\nspin_unlock_irqrestore(&rsp->hw->hardware_lock, flags);\r\n}\r\nint\r\nqla25xx_create_rsp_que(struct qla_hw_data *ha, uint16_t options,\r\nuint8_t vp_idx, uint16_t rid, int req)\r\n{\r\nint ret = 0;\r\nstruct rsp_que *rsp = NULL;\r\nstruct scsi_qla_host *base_vha = pci_get_drvdata(ha->pdev);\r\nuint16_t que_id = 0;\r\ndevice_reg_t __iomem *reg;\r\nrsp = kzalloc(sizeof(struct rsp_que), GFP_KERNEL);\r\nif (rsp == NULL) {\r\nql_log(ql_log_warn, base_vha, 0x0066,\r\n"Failed to allocate memory for response queue.\n");\r\ngoto failed;\r\n}\r\nrsp->length = RESPONSE_ENTRY_CNT_MQ;\r\nrsp->ring = dma_alloc_coherent(&ha->pdev->dev,\r\n(rsp->length + 1) * sizeof(response_t),\r\n&rsp->dma, GFP_KERNEL);\r\nif (rsp->ring == NULL) {\r\nql_log(ql_log_warn, base_vha, 0x00e1,\r\n"Failed to allocate memory for response ring.\n");\r\ngoto que_failed;\r\n}\r\nmutex_lock(&ha->vport_lock);\r\nque_id = find_first_zero_bit(ha->rsp_qid_map, ha->max_rsp_queues);\r\nif (que_id >= ha->max_rsp_queues) {\r\nmutex_unlock(&ha->vport_lock);\r\nql_log(ql_log_warn, base_vha, 0x00e2,\r\n"No resources to create additional request queue.\n");\r\ngoto que_failed;\r\n}\r\nset_bit(que_id, ha->rsp_qid_map);\r\nif (ha->flags.msix_enabled)\r\nrsp->msix = &ha->msix_entries[que_id + 1];\r\nelse\r\nql_log(ql_log_warn, base_vha, 0x00e3,\r\n"MSIX not enalbled.\n");\r\nha->rsp_q_map[que_id] = rsp;\r\nrsp->rid = rid;\r\nrsp->vp_idx = vp_idx;\r\nrsp->hw = ha;\r\nql_dbg(ql_dbg_init, base_vha, 0x00e4,\r\n"queue_id=%d rid=%d vp_idx=%d hw=%p.\n",\r\nque_id, rsp->rid, rsp->vp_idx, rsp->hw);\r\nif (MSB(rsp->rid))\r\noptions |= BIT_4;\r\nif (LSB(rsp->rid))\r\noptions |= BIT_5;\r\nif (!IS_MSIX_NACK_CAPABLE(ha))\r\noptions |= BIT_6;\r\nrsp->options = options;\r\nrsp->id = que_id;\r\nreg = ISP_QUE_REG(ha, que_id);\r\nrsp->rsp_q_in = &reg->isp25mq.rsp_q_in;\r\nrsp->rsp_q_out = &reg->isp25mq.rsp_q_out;\r\nmutex_unlock(&ha->vport_lock);\r\nql_dbg(ql_dbg_multiq, base_vha, 0xc00b,\r\n"options=%x id=%d rsp_q_in=%p rsp_q_out=%p",\r\nrsp->options, rsp->id, rsp->rsp_q_in,\r\nrsp->rsp_q_out);\r\nql_dbg(ql_dbg_init, base_vha, 0x00e5,\r\n"options=%x id=%d rsp_q_in=%p rsp_q_out=%p",\r\nrsp->options, rsp->id, rsp->rsp_q_in,\r\nrsp->rsp_q_out);\r\nret = qla25xx_request_irq(rsp);\r\nif (ret)\r\ngoto que_failed;\r\nret = qla25xx_init_rsp_que(base_vha, rsp);\r\nif (ret != QLA_SUCCESS) {\r\nql_log(ql_log_fatal, base_vha, 0x00e7,\r\n"%s failed.\n", __func__);\r\nmutex_lock(&ha->vport_lock);\r\nclear_bit(que_id, ha->rsp_qid_map);\r\nmutex_unlock(&ha->vport_lock);\r\ngoto que_failed;\r\n}\r\nif (req >= 0)\r\nrsp->req = ha->req_q_map[req];\r\nelse\r\nrsp->req = NULL;\r\nqla2x00_init_response_q_entries(rsp);\r\nif (rsp->hw->wq)\r\nINIT_WORK(&rsp->q_work, qla_do_work);\r\nreturn rsp->id;\r\nque_failed:\r\nqla25xx_free_rsp_que(base_vha, rsp);\r\nfailed:\r\nreturn 0;\r\n}
