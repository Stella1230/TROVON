static bool is_enabled(void)\r\n{\r\nreturn atomic_read(&mmiotrace_enabled);\r\n}\r\nstatic void print_pte(unsigned long address)\r\n{\r\nunsigned int level;\r\npte_t *pte = lookup_address(address, &level);\r\nif (!pte) {\r\npr_err("Error in %s: no pte for page 0x%08lx\n",\r\n__func__, address);\r\nreturn;\r\n}\r\nif (level == PG_LEVEL_2M) {\r\npr_emerg("4MB pages are not currently supported: 0x%08lx\n",\r\naddress);\r\nBUG();\r\n}\r\npr_info("pte for 0x%lx: 0x%llx 0x%llx\n",\r\naddress,\r\n(unsigned long long)pte_val(*pte),\r\n(unsigned long long)pte_val(*pte) & _PAGE_PRESENT);\r\n}\r\nstatic void die_kmmio_nesting_error(struct pt_regs *regs, unsigned long addr)\r\n{\r\nconst struct trap_reason *my_reason = &get_cpu_var(pf_reason);\r\npr_emerg("unexpected fault for address: 0x%08lx, last fault for address: 0x%08lx\n",\r\naddr, my_reason->addr);\r\nprint_pte(addr);\r\nprint_symbol(KERN_EMERG "faulting IP is at %s\n", regs->ip);\r\nprint_symbol(KERN_EMERG "last faulting IP was at %s\n", my_reason->ip);\r\n#ifdef __i386__\r\npr_emerg("eax: %08lx ebx: %08lx ecx: %08lx edx: %08lx\n",\r\nregs->ax, regs->bx, regs->cx, regs->dx);\r\npr_emerg("esi: %08lx edi: %08lx ebp: %08lx esp: %08lx\n",\r\nregs->si, regs->di, regs->bp, regs->sp);\r\n#else\r\npr_emerg("rax: %016lx rcx: %016lx rdx: %016lx\n",\r\nregs->ax, regs->cx, regs->dx);\r\npr_emerg("rsi: %016lx rdi: %016lx rbp: %016lx rsp: %016lx\n",\r\nregs->si, regs->di, regs->bp, regs->sp);\r\n#endif\r\nput_cpu_var(pf_reason);\r\nBUG();\r\n}\r\nstatic void pre(struct kmmio_probe *p, struct pt_regs *regs,\r\nunsigned long addr)\r\n{\r\nstruct trap_reason *my_reason = &get_cpu_var(pf_reason);\r\nstruct mmiotrace_rw *my_trace = &get_cpu_var(cpu_trace);\r\nconst unsigned long instptr = instruction_pointer(regs);\r\nconst enum reason_type type = get_ins_type(instptr);\r\nstruct remap_trace *trace = p->private;\r\nif (my_reason->active_traces)\r\ndie_kmmio_nesting_error(regs, addr);\r\nelse\r\nmy_reason->active_traces++;\r\nmy_reason->type = type;\r\nmy_reason->addr = addr;\r\nmy_reason->ip = instptr;\r\nmy_trace->phys = addr - trace->probe.addr + trace->phys;\r\nmy_trace->map_id = trace->id;\r\nif (trace_pc)\r\nmy_trace->pc = instptr;\r\nelse\r\nmy_trace->pc = 0;\r\nswitch (type) {\r\ncase REG_READ:\r\nmy_trace->opcode = MMIO_READ;\r\nmy_trace->width = get_ins_mem_width(instptr);\r\nbreak;\r\ncase REG_WRITE:\r\nmy_trace->opcode = MMIO_WRITE;\r\nmy_trace->width = get_ins_mem_width(instptr);\r\nmy_trace->value = get_ins_reg_val(instptr, regs);\r\nbreak;\r\ncase IMM_WRITE:\r\nmy_trace->opcode = MMIO_WRITE;\r\nmy_trace->width = get_ins_mem_width(instptr);\r\nmy_trace->value = get_ins_imm_val(instptr);\r\nbreak;\r\ndefault:\r\n{\r\nunsigned char *ip = (unsigned char *)instptr;\r\nmy_trace->opcode = MMIO_UNKNOWN_OP;\r\nmy_trace->width = 0;\r\nmy_trace->value = (*ip) << 16 | *(ip + 1) << 8 |\r\n*(ip + 2);\r\n}\r\n}\r\nput_cpu_var(cpu_trace);\r\nput_cpu_var(pf_reason);\r\n}\r\nstatic void post(struct kmmio_probe *p, unsigned long condition,\r\nstruct pt_regs *regs)\r\n{\r\nstruct trap_reason *my_reason = &get_cpu_var(pf_reason);\r\nstruct mmiotrace_rw *my_trace = &get_cpu_var(cpu_trace);\r\nmy_reason->active_traces--;\r\nif (my_reason->active_traces) {\r\npr_emerg("unexpected post handler");\r\nBUG();\r\n}\r\nswitch (my_reason->type) {\r\ncase REG_READ:\r\nmy_trace->value = get_ins_reg_val(my_reason->ip, regs);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nmmio_trace_rw(my_trace);\r\nput_cpu_var(cpu_trace);\r\nput_cpu_var(pf_reason);\r\n}\r\nstatic void ioremap_trace_core(resource_size_t offset, unsigned long size,\r\nvoid __iomem *addr)\r\n{\r\nstatic atomic_t next_id;\r\nstruct remap_trace *trace = kmalloc(sizeof(*trace), GFP_KERNEL);\r\nstruct mmiotrace_map map = {\r\n.phys = offset,\r\n.virt = (unsigned long)addr,\r\n.len = size,\r\n.opcode = MMIO_PROBE\r\n};\r\nif (!trace) {\r\npr_err("kmalloc failed in ioremap\n");\r\nreturn;\r\n}\r\n*trace = (struct remap_trace) {\r\n.probe = {\r\n.addr = (unsigned long)addr,\r\n.len = size,\r\n.pre_handler = pre,\r\n.post_handler = post,\r\n.private = trace\r\n},\r\n.phys = offset,\r\n.id = atomic_inc_return(&next_id)\r\n};\r\nmap.map_id = trace->id;\r\nspin_lock_irq(&trace_lock);\r\nif (!is_enabled()) {\r\nkfree(trace);\r\ngoto not_enabled;\r\n}\r\nmmio_trace_mapping(&map);\r\nlist_add_tail(&trace->list, &trace_list);\r\nif (!nommiotrace)\r\nregister_kmmio_probe(&trace->probe);\r\nnot_enabled:\r\nspin_unlock_irq(&trace_lock);\r\n}\r\nvoid mmiotrace_ioremap(resource_size_t offset, unsigned long size,\r\nvoid __iomem *addr)\r\n{\r\nif (!is_enabled())\r\nreturn;\r\npr_debug("ioremap_*(0x%llx, 0x%lx) = %p\n",\r\n(unsigned long long)offset, size, addr);\r\nif ((filter_offset) && (offset != filter_offset))\r\nreturn;\r\nioremap_trace_core(offset, size, addr);\r\n}\r\nstatic void iounmap_trace_core(volatile void __iomem *addr)\r\n{\r\nstruct mmiotrace_map map = {\r\n.phys = 0,\r\n.virt = (unsigned long)addr,\r\n.len = 0,\r\n.opcode = MMIO_UNPROBE\r\n};\r\nstruct remap_trace *trace;\r\nstruct remap_trace *tmp;\r\nstruct remap_trace *found_trace = NULL;\r\npr_debug("Unmapping %p.\n", addr);\r\nspin_lock_irq(&trace_lock);\r\nif (!is_enabled())\r\ngoto not_enabled;\r\nlist_for_each_entry_safe(trace, tmp, &trace_list, list) {\r\nif ((unsigned long)addr == trace->probe.addr) {\r\nif (!nommiotrace)\r\nunregister_kmmio_probe(&trace->probe);\r\nlist_del(&trace->list);\r\nfound_trace = trace;\r\nbreak;\r\n}\r\n}\r\nmap.map_id = (found_trace) ? found_trace->id : -1;\r\nmmio_trace_mapping(&map);\r\nnot_enabled:\r\nspin_unlock_irq(&trace_lock);\r\nif (found_trace) {\r\nsynchronize_rcu();\r\nkfree(found_trace);\r\n}\r\n}\r\nvoid mmiotrace_iounmap(volatile void __iomem *addr)\r\n{\r\nmight_sleep();\r\nif (is_enabled())\r\niounmap_trace_core(addr);\r\n}\r\nint mmiotrace_printk(const char *fmt, ...)\r\n{\r\nint ret = 0;\r\nva_list args;\r\nunsigned long flags;\r\nva_start(args, fmt);\r\nspin_lock_irqsave(&trace_lock, flags);\r\nif (is_enabled())\r\nret = mmio_trace_printk(fmt, args);\r\nspin_unlock_irqrestore(&trace_lock, flags);\r\nva_end(args);\r\nreturn ret;\r\n}\r\nstatic void clear_trace_list(void)\r\n{\r\nstruct remap_trace *trace;\r\nstruct remap_trace *tmp;\r\nlist_for_each_entry(trace, &trace_list, list) {\r\npr_notice("purging non-iounmapped trace @0x%08lx, size 0x%lx.\n",\r\ntrace->probe.addr, trace->probe.len);\r\nif (!nommiotrace)\r\nunregister_kmmio_probe(&trace->probe);\r\n}\r\nsynchronize_rcu();\r\nlist_for_each_entry_safe(trace, tmp, &trace_list, list) {\r\nlist_del(&trace->list);\r\nkfree(trace);\r\n}\r\n}\r\nstatic void enter_uniprocessor(void)\r\n{\r\nint cpu;\r\nint err;\r\nif (downed_cpus == NULL &&\r\n!alloc_cpumask_var(&downed_cpus, GFP_KERNEL)) {\r\npr_notice("Failed to allocate mask\n");\r\ngoto out;\r\n}\r\nget_online_cpus();\r\ncpumask_copy(downed_cpus, cpu_online_mask);\r\ncpumask_clear_cpu(cpumask_first(cpu_online_mask), downed_cpus);\r\nif (num_online_cpus() > 1)\r\npr_notice("Disabling non-boot CPUs...\n");\r\nput_online_cpus();\r\nfor_each_cpu(cpu, downed_cpus) {\r\nerr = cpu_down(cpu);\r\nif (!err)\r\npr_info("CPU%d is down.\n", cpu);\r\nelse\r\npr_err("Error taking CPU%d down: %d\n", cpu, err);\r\n}\r\nout:\r\nif (num_online_cpus() > 1)\r\npr_warning("multiple CPUs still online, may miss events.\n");\r\n}\r\nstatic void __ref leave_uniprocessor(void)\r\n{\r\nint cpu;\r\nint err;\r\nif (downed_cpus == NULL || cpumask_weight(downed_cpus) == 0)\r\nreturn;\r\npr_notice("Re-enabling CPUs...\n");\r\nfor_each_cpu(cpu, downed_cpus) {\r\nerr = cpu_up(cpu);\r\nif (!err)\r\npr_info("enabled CPU%d.\n", cpu);\r\nelse\r\npr_err("cannot re-enable CPU%d: %d\n", cpu, err);\r\n}\r\n}\r\nstatic void enter_uniprocessor(void)\r\n{\r\nif (num_online_cpus() > 1)\r\npr_warning("multiple CPUs are online, may miss events. "\r\n"Suggest booting with maxcpus=1 kernel argument.\n");\r\n}\r\nstatic void leave_uniprocessor(void)\r\n{\r\n}\r\nvoid enable_mmiotrace(void)\r\n{\r\nmutex_lock(&mmiotrace_mutex);\r\nif (is_enabled())\r\ngoto out;\r\nif (nommiotrace)\r\npr_info("MMIO tracing disabled.\n");\r\nkmmio_init();\r\nenter_uniprocessor();\r\nspin_lock_irq(&trace_lock);\r\natomic_inc(&mmiotrace_enabled);\r\nspin_unlock_irq(&trace_lock);\r\npr_info("enabled.\n");\r\nout:\r\nmutex_unlock(&mmiotrace_mutex);\r\n}\r\nvoid disable_mmiotrace(void)\r\n{\r\nmutex_lock(&mmiotrace_mutex);\r\nif (!is_enabled())\r\ngoto out;\r\nspin_lock_irq(&trace_lock);\r\natomic_dec(&mmiotrace_enabled);\r\nBUG_ON(is_enabled());\r\nspin_unlock_irq(&trace_lock);\r\nclear_trace_list();\r\nleave_uniprocessor();\r\nkmmio_cleanup();\r\npr_info("disabled.\n");\r\nout:\r\nmutex_unlock(&mmiotrace_mutex);\r\n}
