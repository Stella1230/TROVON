static inline unsigned long\r\nmk_iommu_pte(unsigned long paddr)\r\n{\r\nreturn (paddr >> (PAGE_SHIFT-1)) | 1;\r\n}\r\nunsigned long\r\nsize_for_memory(unsigned long max)\r\n{\r\nunsigned long mem = max_low_pfn << PAGE_SHIFT;\r\nif (mem < max)\r\nmax = roundup_pow_of_two(mem);\r\nreturn max;\r\n}\r\nstruct pci_iommu_arena * __init\r\niommu_arena_new_node(int nid, struct pci_controller *hose, dma_addr_t base,\r\nunsigned long window_size, unsigned long align)\r\n{\r\nunsigned long mem_size;\r\nstruct pci_iommu_arena *arena;\r\nmem_size = window_size / (PAGE_SIZE / sizeof(unsigned long));\r\nif (align < mem_size)\r\nalign = mem_size;\r\n#ifdef CONFIG_DISCONTIGMEM\r\narena = alloc_bootmem_node(NODE_DATA(nid), sizeof(*arena));\r\nif (!NODE_DATA(nid) || !arena) {\r\nprintk("%s: couldn't allocate arena from node %d\n"\r\n" falling back to system-wide allocation\n",\r\n__func__, nid);\r\narena = alloc_bootmem(sizeof(*arena));\r\n}\r\narena->ptes = __alloc_bootmem_node(NODE_DATA(nid), mem_size, align, 0);\r\nif (!NODE_DATA(nid) || !arena->ptes) {\r\nprintk("%s: couldn't allocate arena ptes from node %d\n"\r\n" falling back to system-wide allocation\n",\r\n__func__, nid);\r\narena->ptes = __alloc_bootmem(mem_size, align, 0);\r\n}\r\n#else\r\narena = alloc_bootmem(sizeof(*arena));\r\narena->ptes = __alloc_bootmem(mem_size, align, 0);\r\n#endif\r\nspin_lock_init(&arena->lock);\r\narena->hose = hose;\r\narena->dma_base = base;\r\narena->size = window_size;\r\narena->next_entry = 0;\r\narena->align_entry = 1;\r\nreturn arena;\r\n}\r\nstruct pci_iommu_arena * __init\r\niommu_arena_new(struct pci_controller *hose, dma_addr_t base,\r\nunsigned long window_size, unsigned long align)\r\n{\r\nreturn iommu_arena_new_node(0, hose, base, window_size, align);\r\n}\r\nstatic long\r\niommu_arena_find_pages(struct device *dev, struct pci_iommu_arena *arena,\r\nlong n, long mask)\r\n{\r\nunsigned long *ptes;\r\nlong i, p, nent;\r\nint pass = 0;\r\nunsigned long base;\r\nunsigned long boundary_size;\r\nbase = arena->dma_base >> PAGE_SHIFT;\r\nif (dev) {\r\nboundary_size = dma_get_seg_boundary(dev) + 1;\r\nboundary_size >>= PAGE_SHIFT;\r\n} else {\r\nboundary_size = 1UL << (32 - PAGE_SHIFT);\r\n}\r\nptes = arena->ptes;\r\nnent = arena->size >> PAGE_SHIFT;\r\np = ALIGN(arena->next_entry, mask + 1);\r\ni = 0;\r\nagain:\r\nwhile (i < n && p+i < nent) {\r\nif (!i && iommu_is_span_boundary(p, n, base, boundary_size)) {\r\np = ALIGN(p + 1, mask + 1);\r\ngoto again;\r\n}\r\nif (ptes[p+i])\r\np = ALIGN(p + i + 1, mask + 1), i = 0;\r\nelse\r\ni = i + 1;\r\n}\r\nif (i < n) {\r\nif (pass < 1) {\r\nalpha_mv.mv_pci_tbi(arena->hose, 0, -1);\r\npass++;\r\np = 0;\r\ni = 0;\r\ngoto again;\r\n} else\r\nreturn -1;\r\n}\r\nreturn p;\r\n}\r\nstatic long\r\niommu_arena_alloc(struct device *dev, struct pci_iommu_arena *arena, long n,\r\nunsigned int align)\r\n{\r\nunsigned long flags;\r\nunsigned long *ptes;\r\nlong i, p, mask;\r\nspin_lock_irqsave(&arena->lock, flags);\r\nptes = arena->ptes;\r\nmask = max(align, arena->align_entry) - 1;\r\np = iommu_arena_find_pages(dev, arena, n, mask);\r\nif (p < 0) {\r\nspin_unlock_irqrestore(&arena->lock, flags);\r\nreturn -1;\r\n}\r\nfor (i = 0; i < n; ++i)\r\nptes[p+i] = IOMMU_INVALID_PTE;\r\narena->next_entry = p + n;\r\nspin_unlock_irqrestore(&arena->lock, flags);\r\nreturn p;\r\n}\r\nstatic void\r\niommu_arena_free(struct pci_iommu_arena *arena, long ofs, long n)\r\n{\r\nunsigned long *p;\r\nlong i;\r\np = arena->ptes + ofs;\r\nfor (i = 0; i < n; ++i)\r\np[i] = 0;\r\n}\r\nstatic int pci_dac_dma_supported(struct pci_dev *dev, u64 mask)\r\n{\r\ndma_addr_t dac_offset = alpha_mv.pci_dac_offset;\r\nint ok = 1;\r\nif (dac_offset == 0)\r\nok = 0;\r\nif ((dac_offset & dev->dma_mask) != dac_offset)\r\nok = 0;\r\nDBGA("pci_dac_dma_supported %s from %pf\n",\r\nok ? "yes" : "no", __builtin_return_address(0));\r\nreturn ok;\r\n}\r\nstatic dma_addr_t\r\npci_map_single_1(struct pci_dev *pdev, void *cpu_addr, size_t size,\r\nint dac_allowed)\r\n{\r\nstruct pci_controller *hose = pdev ? pdev->sysdata : pci_isa_hose;\r\ndma_addr_t max_dma = pdev ? pdev->dma_mask : ISA_DMA_MASK;\r\nstruct pci_iommu_arena *arena;\r\nlong npages, dma_ofs, i;\r\nunsigned long paddr;\r\ndma_addr_t ret;\r\nunsigned int align = 0;\r\nstruct device *dev = pdev ? &pdev->dev : NULL;\r\npaddr = __pa(cpu_addr);\r\n#if !DEBUG_NODIRECT\r\nif (paddr + size + __direct_map_base - 1 <= max_dma\r\n&& paddr + size <= __direct_map_size) {\r\nret = paddr + __direct_map_base;\r\nDBGA2("pci_map_single: [%p,%zx] -> direct %llx from %pf\n",\r\ncpu_addr, size, ret, __builtin_return_address(0));\r\nreturn ret;\r\n}\r\n#endif\r\nif (dac_allowed) {\r\nret = paddr + alpha_mv.pci_dac_offset;\r\nDBGA2("pci_map_single: [%p,%zx] -> DAC %llx from %pf\n",\r\ncpu_addr, size, ret, __builtin_return_address(0));\r\nreturn ret;\r\n}\r\nif (! alpha_mv.mv_pci_tbi) {\r\nprintk_once(KERN_WARNING "pci_map_single: no HW sg\n");\r\nreturn 0;\r\n}\r\narena = hose->sg_pci;\r\nif (!arena || arena->dma_base + arena->size - 1 > max_dma)\r\narena = hose->sg_isa;\r\nnpages = iommu_num_pages(paddr, size, PAGE_SIZE);\r\nif (pdev && pdev == isa_bridge)\r\nalign = 8;\r\ndma_ofs = iommu_arena_alloc(dev, arena, npages, align);\r\nif (dma_ofs < 0) {\r\nprintk(KERN_WARNING "pci_map_single failed: "\r\n"could not allocate dma page tables\n");\r\nreturn 0;\r\n}\r\npaddr &= PAGE_MASK;\r\nfor (i = 0; i < npages; ++i, paddr += PAGE_SIZE)\r\narena->ptes[i + dma_ofs] = mk_iommu_pte(paddr);\r\nret = arena->dma_base + dma_ofs * PAGE_SIZE;\r\nret += (unsigned long)cpu_addr & ~PAGE_MASK;\r\nDBGA2("pci_map_single: [%p,%zx] np %ld -> sg %llx from %pf\n",\r\ncpu_addr, size, npages, ret, __builtin_return_address(0));\r\nreturn ret;\r\n}\r\nstatic struct pci_dev *alpha_gendev_to_pci(struct device *dev)\r\n{\r\nif (dev && dev->bus == &pci_bus_type)\r\nreturn to_pci_dev(dev);\r\nBUG_ON(!isa_bridge);\r\nif (!dev || !dev->dma_mask || !*dev->dma_mask)\r\nreturn isa_bridge;\r\nif (*dev->dma_mask >= isa_bridge->dma_mask)\r\nreturn isa_bridge;\r\nreturn NULL;\r\n}\r\nstatic dma_addr_t alpha_pci_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct pci_dev *pdev = alpha_gendev_to_pci(dev);\r\nint dac_allowed;\r\nif (dir == PCI_DMA_NONE)\r\nBUG();\r\ndac_allowed = pdev ? pci_dac_dma_supported(pdev, pdev->dma_mask) : 0;\r\nreturn pci_map_single_1(pdev, (char *)page_address(page) + offset,\r\nsize, dac_allowed);\r\n}\r\nstatic void alpha_pci_unmap_page(struct device *dev, dma_addr_t dma_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nunsigned long flags;\r\nstruct pci_dev *pdev = alpha_gendev_to_pci(dev);\r\nstruct pci_controller *hose = pdev ? pdev->sysdata : pci_isa_hose;\r\nstruct pci_iommu_arena *arena;\r\nlong dma_ofs, npages;\r\nif (dir == PCI_DMA_NONE)\r\nBUG();\r\nif (dma_addr >= __direct_map_base\r\n&& dma_addr < __direct_map_base + __direct_map_size) {\r\nDBGA2("pci_unmap_single: direct [%llx,%zx] from %pf\n",\r\ndma_addr, size, __builtin_return_address(0));\r\nreturn;\r\n}\r\nif (dma_addr > 0xffffffff) {\r\nDBGA2("pci64_unmap_single: DAC [%llx,%zx] from %pf\n",\r\ndma_addr, size, __builtin_return_address(0));\r\nreturn;\r\n}\r\narena = hose->sg_pci;\r\nif (!arena || dma_addr < arena->dma_base)\r\narena = hose->sg_isa;\r\ndma_ofs = (dma_addr - arena->dma_base) >> PAGE_SHIFT;\r\nif (dma_ofs * PAGE_SIZE >= arena->size) {\r\nprintk(KERN_ERR "Bogus pci_unmap_single: dma_addr %llx "\r\n" base %llx size %x\n",\r\ndma_addr, arena->dma_base, arena->size);\r\nreturn;\r\nBUG();\r\n}\r\nnpages = iommu_num_pages(dma_addr, size, PAGE_SIZE);\r\nspin_lock_irqsave(&arena->lock, flags);\r\niommu_arena_free(arena, dma_ofs, npages);\r\nif (dma_ofs >= arena->next_entry)\r\nalpha_mv.mv_pci_tbi(hose, dma_addr, dma_addr + size - 1);\r\nspin_unlock_irqrestore(&arena->lock, flags);\r\nDBGA2("pci_unmap_single: sg [%llx,%zx] np %ld from %pf\n",\r\ndma_addr, size, npages, __builtin_return_address(0));\r\n}\r\nstatic void *alpha_pci_alloc_coherent(struct device *dev, size_t size,\r\ndma_addr_t *dma_addrp, gfp_t gfp,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct pci_dev *pdev = alpha_gendev_to_pci(dev);\r\nvoid *cpu_addr;\r\nlong order = get_order(size);\r\ngfp &= ~GFP_DMA;\r\ntry_again:\r\ncpu_addr = (void *)__get_free_pages(gfp, order);\r\nif (! cpu_addr) {\r\nprintk(KERN_INFO "pci_alloc_consistent: "\r\n"get_free_pages failed from %pf\n",\r\n__builtin_return_address(0));\r\nreturn NULL;\r\n}\r\nmemset(cpu_addr, 0, size);\r\n*dma_addrp = pci_map_single_1(pdev, cpu_addr, size, 0);\r\nif (*dma_addrp == 0) {\r\nfree_pages((unsigned long)cpu_addr, order);\r\nif (alpha_mv.mv_pci_tbi || (gfp & GFP_DMA))\r\nreturn NULL;\r\ngfp |= GFP_DMA;\r\ngoto try_again;\r\n}\r\nDBGA2("pci_alloc_consistent: %zx -> [%p,%llx] from %pf\n",\r\nsize, cpu_addr, *dma_addrp, __builtin_return_address(0));\r\nreturn cpu_addr;\r\n}\r\nstatic void alpha_pci_free_coherent(struct device *dev, size_t size,\r\nvoid *cpu_addr, dma_addr_t dma_addr,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct pci_dev *pdev = alpha_gendev_to_pci(dev);\r\npci_unmap_single(pdev, dma_addr, size, PCI_DMA_BIDIRECTIONAL);\r\nfree_pages((unsigned long)cpu_addr, get_order(size));\r\nDBGA2("pci_free_consistent: [%llx,%zx] from %pf\n",\r\ndma_addr, size, __builtin_return_address(0));\r\n}\r\nstatic void\r\nsg_classify(struct device *dev, struct scatterlist *sg, struct scatterlist *end,\r\nint virt_ok)\r\n{\r\nunsigned long next_paddr;\r\nstruct scatterlist *leader;\r\nlong leader_flag, leader_length;\r\nunsigned int max_seg_size;\r\nleader = sg;\r\nleader_flag = 0;\r\nleader_length = leader->length;\r\nnext_paddr = SG_ENT_PHYS_ADDRESS(leader) + leader_length;\r\nmax_seg_size = dev ? dma_get_max_seg_size(dev) : 0;\r\nfor (++sg; sg < end; ++sg) {\r\nunsigned long addr, len;\r\naddr = SG_ENT_PHYS_ADDRESS(sg);\r\nlen = sg->length;\r\nif (leader_length + len > max_seg_size)\r\ngoto new_segment;\r\nif (next_paddr == addr) {\r\nsg->dma_address = -1;\r\nleader_length += len;\r\n} else if (((next_paddr | addr) & ~PAGE_MASK) == 0 && virt_ok) {\r\nsg->dma_address = -2;\r\nleader_flag = 1;\r\nleader_length += len;\r\n} else {\r\nnew_segment:\r\nleader->dma_address = leader_flag;\r\nleader->dma_length = leader_length;\r\nleader = sg;\r\nleader_flag = 0;\r\nleader_length = len;\r\n}\r\nnext_paddr = addr + len;\r\n}\r\nleader->dma_address = leader_flag;\r\nleader->dma_length = leader_length;\r\n}\r\nstatic int\r\nsg_fill(struct device *dev, struct scatterlist *leader, struct scatterlist *end,\r\nstruct scatterlist *out, struct pci_iommu_arena *arena,\r\ndma_addr_t max_dma, int dac_allowed)\r\n{\r\nunsigned long paddr = SG_ENT_PHYS_ADDRESS(leader);\r\nlong size = leader->dma_length;\r\nstruct scatterlist *sg;\r\nunsigned long *ptes;\r\nlong npages, dma_ofs, i;\r\n#if !DEBUG_NODIRECT\r\nif (leader->dma_address == 0\r\n&& paddr + size + __direct_map_base - 1 <= max_dma\r\n&& paddr + size <= __direct_map_size) {\r\nout->dma_address = paddr + __direct_map_base;\r\nout->dma_length = size;\r\nDBGA(" sg_fill: [%p,%lx] -> direct %llx\n",\r\n__va(paddr), size, out->dma_address);\r\nreturn 0;\r\n}\r\n#endif\r\nif (leader->dma_address == 0 && dac_allowed) {\r\nout->dma_address = paddr + alpha_mv.pci_dac_offset;\r\nout->dma_length = size;\r\nDBGA(" sg_fill: [%p,%lx] -> DAC %llx\n",\r\n__va(paddr), size, out->dma_address);\r\nreturn 0;\r\n}\r\npaddr &= ~PAGE_MASK;\r\nnpages = iommu_num_pages(paddr, size, PAGE_SIZE);\r\ndma_ofs = iommu_arena_alloc(dev, arena, npages, 0);\r\nif (dma_ofs < 0) {\r\nif (leader->dma_address == 0)\r\nreturn -1;\r\nsg_classify(dev, leader, end, 0);\r\nreturn sg_fill(dev, leader, end, out, arena, max_dma, dac_allowed);\r\n}\r\nout->dma_address = arena->dma_base + dma_ofs*PAGE_SIZE + paddr;\r\nout->dma_length = size;\r\nDBGA(" sg_fill: [%p,%lx] -> sg %llx np %ld\n",\r\n__va(paddr), size, out->dma_address, npages);\r\nptes = &arena->ptes[dma_ofs];\r\nsg = leader;\r\ndo {\r\n#if DEBUG_ALLOC > 0\r\nstruct scatterlist *last_sg = sg;\r\n#endif\r\nsize = sg->length;\r\npaddr = SG_ENT_PHYS_ADDRESS(sg);\r\nwhile (sg+1 < end && (int) sg[1].dma_address == -1) {\r\nsize += sg[1].length;\r\nsg++;\r\n}\r\nnpages = iommu_num_pages(paddr, size, PAGE_SIZE);\r\npaddr &= PAGE_MASK;\r\nfor (i = 0; i < npages; ++i, paddr += PAGE_SIZE)\r\n*ptes++ = mk_iommu_pte(paddr);\r\n#if DEBUG_ALLOC > 0\r\nDBGA(" (%ld) [%p,%x] np %ld\n",\r\nlast_sg - leader, SG_ENT_VIRT_ADDRESS(last_sg),\r\nlast_sg->length, npages);\r\nwhile (++last_sg <= sg) {\r\nDBGA(" (%ld) [%p,%x] cont\n",\r\nlast_sg - leader, SG_ENT_VIRT_ADDRESS(last_sg),\r\nlast_sg->length);\r\n}\r\n#endif\r\n} while (++sg < end && (int) sg->dma_address < 0);\r\nreturn 1;\r\n}\r\nstatic int alpha_pci_map_sg(struct device *dev, struct scatterlist *sg,\r\nint nents, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct pci_dev *pdev = alpha_gendev_to_pci(dev);\r\nstruct scatterlist *start, *end, *out;\r\nstruct pci_controller *hose;\r\nstruct pci_iommu_arena *arena;\r\ndma_addr_t max_dma;\r\nint dac_allowed;\r\nif (dir == PCI_DMA_NONE)\r\nBUG();\r\ndac_allowed = dev ? pci_dac_dma_supported(pdev, pdev->dma_mask) : 0;\r\nif (nents == 1) {\r\nsg->dma_length = sg->length;\r\nsg->dma_address\r\n= pci_map_single_1(pdev, SG_ENT_VIRT_ADDRESS(sg),\r\nsg->length, dac_allowed);\r\nreturn sg->dma_address != 0;\r\n}\r\nstart = sg;\r\nend = sg + nents;\r\nsg_classify(dev, sg, end, alpha_mv.mv_pci_tbi != 0);\r\nif (alpha_mv.mv_pci_tbi) {\r\nhose = pdev ? pdev->sysdata : pci_isa_hose;\r\nmax_dma = pdev ? pdev->dma_mask : ISA_DMA_MASK;\r\narena = hose->sg_pci;\r\nif (!arena || arena->dma_base + arena->size - 1 > max_dma)\r\narena = hose->sg_isa;\r\n} else {\r\nmax_dma = -1;\r\narena = NULL;\r\nhose = NULL;\r\n}\r\nfor (out = sg; sg < end; ++sg) {\r\nif ((int) sg->dma_address < 0)\r\ncontinue;\r\nif (sg_fill(dev, sg, end, out, arena, max_dma, dac_allowed) < 0)\r\ngoto error;\r\nout++;\r\n}\r\nif (out < end)\r\nout->dma_length = 0;\r\nif (out - start == 0)\r\nprintk(KERN_WARNING "pci_map_sg failed: no entries?\n");\r\nDBGA("pci_map_sg: %ld entries\n", out - start);\r\nreturn out - start;\r\nerror:\r\nprintk(KERN_WARNING "pci_map_sg failed: "\r\n"could not allocate dma page tables\n");\r\nif (out > start)\r\npci_unmap_sg(pdev, start, out - start, dir);\r\nreturn 0;\r\n}\r\nstatic void alpha_pci_unmap_sg(struct device *dev, struct scatterlist *sg,\r\nint nents, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct pci_dev *pdev = alpha_gendev_to_pci(dev);\r\nunsigned long flags;\r\nstruct pci_controller *hose;\r\nstruct pci_iommu_arena *arena;\r\nstruct scatterlist *end;\r\ndma_addr_t max_dma;\r\ndma_addr_t fbeg, fend;\r\nif (dir == PCI_DMA_NONE)\r\nBUG();\r\nif (! alpha_mv.mv_pci_tbi)\r\nreturn;\r\nhose = pdev ? pdev->sysdata : pci_isa_hose;\r\nmax_dma = pdev ? pdev->dma_mask : ISA_DMA_MASK;\r\narena = hose->sg_pci;\r\nif (!arena || arena->dma_base + arena->size - 1 > max_dma)\r\narena = hose->sg_isa;\r\nfbeg = -1, fend = 0;\r\nspin_lock_irqsave(&arena->lock, flags);\r\nfor (end = sg + nents; sg < end; ++sg) {\r\ndma_addr_t addr;\r\nsize_t size;\r\nlong npages, ofs;\r\ndma_addr_t tend;\r\naddr = sg->dma_address;\r\nsize = sg->dma_length;\r\nif (!size)\r\nbreak;\r\nif (addr > 0xffffffff) {\r\nDBGA(" (%ld) DAC [%llx,%zx]\n",\r\nsg - end + nents, addr, size);\r\ncontinue;\r\n}\r\nif (addr >= __direct_map_base\r\n&& addr < __direct_map_base + __direct_map_size) {\r\nDBGA(" (%ld) direct [%llx,%zx]\n",\r\nsg - end + nents, addr, size);\r\ncontinue;\r\n}\r\nDBGA(" (%ld) sg [%llx,%zx]\n",\r\nsg - end + nents, addr, size);\r\nnpages = iommu_num_pages(addr, size, PAGE_SIZE);\r\nofs = (addr - arena->dma_base) >> PAGE_SHIFT;\r\niommu_arena_free(arena, ofs, npages);\r\ntend = addr + size - 1;\r\nif (fbeg > addr) fbeg = addr;\r\nif (fend < tend) fend = tend;\r\n}\r\nif ((fend - arena->dma_base) >> PAGE_SHIFT >= arena->next_entry)\r\nalpha_mv.mv_pci_tbi(hose, fbeg, fend);\r\nspin_unlock_irqrestore(&arena->lock, flags);\r\nDBGA("pci_unmap_sg: %ld entries\n", nents - (end - sg));\r\n}\r\nstatic int alpha_pci_supported(struct device *dev, u64 mask)\r\n{\r\nstruct pci_dev *pdev = alpha_gendev_to_pci(dev);\r\nstruct pci_controller *hose;\r\nstruct pci_iommu_arena *arena;\r\nif (__direct_map_size != 0\r\n&& (__direct_map_base + __direct_map_size - 1 <= mask ||\r\n__direct_map_base + (max_low_pfn << PAGE_SHIFT) - 1 <= mask))\r\nreturn 1;\r\nhose = pdev ? pdev->sysdata : pci_isa_hose;\r\narena = hose->sg_isa;\r\nif (arena && arena->dma_base + arena->size - 1 <= mask)\r\nreturn 1;\r\narena = hose->sg_pci;\r\nif (arena && arena->dma_base + arena->size - 1 <= mask)\r\nreturn 1;\r\nif (!__direct_map_base && MAX_DMA_ADDRESS - IDENT_ADDR - 1 <= mask)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nint\r\niommu_reserve(struct pci_iommu_arena *arena, long pg_count, long align_mask)\r\n{\r\nunsigned long flags;\r\nunsigned long *ptes;\r\nlong i, p;\r\nif (!arena) return -EINVAL;\r\nspin_lock_irqsave(&arena->lock, flags);\r\nptes = arena->ptes;\r\np = iommu_arena_find_pages(NULL, arena, pg_count, align_mask);\r\nif (p < 0) {\r\nspin_unlock_irqrestore(&arena->lock, flags);\r\nreturn -1;\r\n}\r\nfor (i = 0; i < pg_count; ++i)\r\nptes[p+i] = IOMMU_RESERVED_PTE;\r\narena->next_entry = p + pg_count;\r\nspin_unlock_irqrestore(&arena->lock, flags);\r\nreturn p;\r\n}\r\nint\r\niommu_release(struct pci_iommu_arena *arena, long pg_start, long pg_count)\r\n{\r\nunsigned long *ptes;\r\nlong i;\r\nif (!arena) return -EINVAL;\r\nptes = arena->ptes;\r\nfor(i = pg_start; i < pg_start + pg_count; i++)\r\nif (ptes[i] != IOMMU_RESERVED_PTE)\r\nreturn -EBUSY;\r\niommu_arena_free(arena, pg_start, pg_count);\r\nreturn 0;\r\n}\r\nint\r\niommu_bind(struct pci_iommu_arena *arena, long pg_start, long pg_count,\r\nstruct page **pages)\r\n{\r\nunsigned long flags;\r\nunsigned long *ptes;\r\nlong i, j;\r\nif (!arena) return -EINVAL;\r\nspin_lock_irqsave(&arena->lock, flags);\r\nptes = arena->ptes;\r\nfor(j = pg_start; j < pg_start + pg_count; j++) {\r\nif (ptes[j] != IOMMU_RESERVED_PTE) {\r\nspin_unlock_irqrestore(&arena->lock, flags);\r\nreturn -EBUSY;\r\n}\r\n}\r\nfor(i = 0, j = pg_start; i < pg_count; i++, j++)\r\nptes[j] = mk_iommu_pte(page_to_phys(pages[i]));\r\nspin_unlock_irqrestore(&arena->lock, flags);\r\nreturn 0;\r\n}\r\nint\r\niommu_unbind(struct pci_iommu_arena *arena, long pg_start, long pg_count)\r\n{\r\nunsigned long *p;\r\nlong i;\r\nif (!arena) return -EINVAL;\r\np = arena->ptes + pg_start;\r\nfor(i = 0; i < pg_count; i++)\r\np[i] = IOMMU_RESERVED_PTE;\r\nreturn 0;\r\n}\r\nstatic int alpha_pci_mapping_error(struct device *dev, dma_addr_t dma_addr)\r\n{\r\nreturn dma_addr == 0;\r\n}\r\nstatic int alpha_pci_set_mask(struct device *dev, u64 mask)\r\n{\r\nif (!dev->dma_mask ||\r\n!pci_dma_supported(alpha_gendev_to_pci(dev), mask))\r\nreturn -EIO;\r\n*dev->dma_mask = mask;\r\nreturn 0;\r\n}
