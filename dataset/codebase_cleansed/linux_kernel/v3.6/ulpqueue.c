struct sctp_ulpq *sctp_ulpq_init(struct sctp_ulpq *ulpq,\r\nstruct sctp_association *asoc)\r\n{\r\nmemset(ulpq, 0, sizeof(struct sctp_ulpq));\r\nulpq->asoc = asoc;\r\nskb_queue_head_init(&ulpq->reasm);\r\nskb_queue_head_init(&ulpq->lobby);\r\nulpq->pd_mode = 0;\r\nulpq->malloced = 0;\r\nreturn ulpq;\r\n}\r\nvoid sctp_ulpq_flush(struct sctp_ulpq *ulpq)\r\n{\r\nstruct sk_buff *skb;\r\nstruct sctp_ulpevent *event;\r\nwhile ((skb = __skb_dequeue(&ulpq->lobby)) != NULL) {\r\nevent = sctp_skb2event(skb);\r\nsctp_ulpevent_free(event);\r\n}\r\nwhile ((skb = __skb_dequeue(&ulpq->reasm)) != NULL) {\r\nevent = sctp_skb2event(skb);\r\nsctp_ulpevent_free(event);\r\n}\r\n}\r\nvoid sctp_ulpq_free(struct sctp_ulpq *ulpq)\r\n{\r\nsctp_ulpq_flush(ulpq);\r\nif (ulpq->malloced)\r\nkfree(ulpq);\r\n}\r\nint sctp_ulpq_tail_data(struct sctp_ulpq *ulpq, struct sctp_chunk *chunk,\r\ngfp_t gfp)\r\n{\r\nstruct sk_buff_head temp;\r\nstruct sctp_ulpevent *event;\r\nevent = sctp_ulpevent_make_rcvmsg(chunk->asoc, chunk, gfp);\r\nif (!event)\r\nreturn -ENOMEM;\r\nevent = sctp_ulpq_reasm(ulpq, event);\r\nif ((event) && (event->msg_flags & MSG_EOR)){\r\nskb_queue_head_init(&temp);\r\n__skb_queue_tail(&temp, sctp_event2skb(event));\r\nevent = sctp_ulpq_order(ulpq, event);\r\n}\r\nif (event)\r\nsctp_ulpq_tail_event(ulpq, event);\r\nreturn 0;\r\n}\r\nint sctp_clear_pd(struct sock *sk, struct sctp_association *asoc)\r\n{\r\nstruct sctp_sock *sp = sctp_sk(sk);\r\nif (atomic_dec_and_test(&sp->pd_mode)) {\r\nif (!skb_queue_empty(&sp->pd_lobby)) {\r\nstruct list_head *list;\r\nsctp_skb_list_tail(&sp->pd_lobby, &sk->sk_receive_queue);\r\nlist = (struct list_head *)&sctp_sk(sk)->pd_lobby;\r\nINIT_LIST_HEAD(list);\r\nreturn 1;\r\n}\r\n} else {\r\nif (!skb_queue_empty(&sp->pd_lobby) && asoc) {\r\nstruct sk_buff *skb, *tmp;\r\nstruct sctp_ulpevent *event;\r\nsctp_skb_for_each(skb, &sp->pd_lobby, tmp) {\r\nevent = sctp_skb2event(skb);\r\nif (event->asoc == asoc) {\r\n__skb_unlink(skb, &sp->pd_lobby);\r\n__skb_queue_tail(&sk->sk_receive_queue,\r\nskb);\r\n}\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void sctp_ulpq_set_pd(struct sctp_ulpq *ulpq)\r\n{\r\nstruct sctp_sock *sp = sctp_sk(ulpq->asoc->base.sk);\r\natomic_inc(&sp->pd_mode);\r\nulpq->pd_mode = 1;\r\n}\r\nstatic int sctp_ulpq_clear_pd(struct sctp_ulpq *ulpq)\r\n{\r\nulpq->pd_mode = 0;\r\nsctp_ulpq_reasm_drain(ulpq);\r\nreturn sctp_clear_pd(ulpq->asoc->base.sk, ulpq->asoc);\r\n}\r\nint sctp_ulpq_tail_event(struct sctp_ulpq *ulpq, struct sctp_ulpevent *event)\r\n{\r\nstruct sock *sk = ulpq->asoc->base.sk;\r\nstruct sk_buff_head *queue, *skb_list;\r\nstruct sk_buff *skb = sctp_event2skb(event);\r\nint clear_pd = 0;\r\nskb_list = (struct sk_buff_head *) skb->prev;\r\nif (sock_flag(sk, SOCK_DEAD) || (sk->sk_shutdown & RCV_SHUTDOWN))\r\ngoto out_free;\r\nif (!sctp_ulpevent_is_enabled(event, &sctp_sk(sk)->subscribe))\r\ngoto out_free;\r\nif (atomic_read(&sctp_sk(sk)->pd_mode) == 0) {\r\nqueue = &sk->sk_receive_queue;\r\n} else {\r\nif (ulpq->pd_mode) {\r\nif ((event->msg_flags & MSG_NOTIFICATION) ||\r\n(SCTP_DATA_NOT_FRAG ==\r\n(event->msg_flags & SCTP_DATA_FRAG_MASK)))\r\nqueue = &sctp_sk(sk)->pd_lobby;\r\nelse {\r\nclear_pd = event->msg_flags & MSG_EOR;\r\nqueue = &sk->sk_receive_queue;\r\n}\r\n} else {\r\nif (sctp_sk(sk)->frag_interleave)\r\nqueue = &sk->sk_receive_queue;\r\nelse\r\nqueue = &sctp_sk(sk)->pd_lobby;\r\n}\r\n}\r\nif (skb_list)\r\nsctp_skb_list_tail(skb_list, queue);\r\nelse\r\n__skb_queue_tail(queue, skb);\r\nif (clear_pd)\r\nsctp_ulpq_clear_pd(ulpq);\r\nif (queue == &sk->sk_receive_queue)\r\nsk->sk_data_ready(sk, 0);\r\nreturn 1;\r\nout_free:\r\nif (skb_list)\r\nsctp_queue_purge_ulpevents(skb_list);\r\nelse\r\nsctp_ulpevent_free(event);\r\nreturn 0;\r\n}\r\nstatic void sctp_ulpq_store_reasm(struct sctp_ulpq *ulpq,\r\nstruct sctp_ulpevent *event)\r\n{\r\nstruct sk_buff *pos;\r\nstruct sctp_ulpevent *cevent;\r\n__u32 tsn, ctsn;\r\ntsn = event->tsn;\r\npos = skb_peek_tail(&ulpq->reasm);\r\nif (!pos) {\r\n__skb_queue_tail(&ulpq->reasm, sctp_event2skb(event));\r\nreturn;\r\n}\r\ncevent = sctp_skb2event(pos);\r\nctsn = cevent->tsn;\r\nif (TSN_lt(ctsn, tsn)) {\r\n__skb_queue_tail(&ulpq->reasm, sctp_event2skb(event));\r\nreturn;\r\n}\r\nskb_queue_walk(&ulpq->reasm, pos) {\r\ncevent = sctp_skb2event(pos);\r\nctsn = cevent->tsn;\r\nif (TSN_lt(tsn, ctsn))\r\nbreak;\r\n}\r\n__skb_queue_before(&ulpq->reasm, pos, sctp_event2skb(event));\r\n}\r\nstatic struct sctp_ulpevent *sctp_make_reassembled_event(struct sk_buff_head *queue, struct sk_buff *f_frag, struct sk_buff *l_frag)\r\n{\r\nstruct sk_buff *pos;\r\nstruct sk_buff *new = NULL;\r\nstruct sctp_ulpevent *event;\r\nstruct sk_buff *pnext, *last;\r\nstruct sk_buff *list = skb_shinfo(f_frag)->frag_list;\r\nif (f_frag == l_frag)\r\npos = NULL;\r\nelse\r\npos = f_frag->next;\r\nfor (last = list; list; last = list, list = list->next);\r\nif (last)\r\nlast->next = pos;\r\nelse {\r\nif (skb_cloned(f_frag)) {\r\nnew = skb_copy(f_frag, GFP_ATOMIC);\r\nif (!new)\r\nreturn NULL;\r\nsctp_skb_set_owner_r(new, f_frag->sk);\r\nskb_shinfo(new)->frag_list = pos;\r\n} else\r\nskb_shinfo(f_frag)->frag_list = pos;\r\n}\r\n__skb_unlink(f_frag, queue);\r\nif (new) {\r\nkfree_skb(f_frag);\r\nf_frag = new;\r\n}\r\nwhile (pos) {\r\npnext = pos->next;\r\nf_frag->len += pos->len;\r\nf_frag->data_len += pos->len;\r\n__skb_unlink(pos, queue);\r\nif (pos == l_frag)\r\nbreak;\r\npos->next = pnext;\r\npos = pnext;\r\n}\r\nevent = sctp_skb2event(f_frag);\r\nSCTP_INC_STATS(SCTP_MIB_REASMUSRMSGS);\r\nreturn event;\r\n}\r\nstatic struct sctp_ulpevent *sctp_ulpq_retrieve_reassembled(struct sctp_ulpq *ulpq)\r\n{\r\nstruct sk_buff *pos;\r\nstruct sctp_ulpevent *cevent;\r\nstruct sk_buff *first_frag = NULL;\r\n__u32 ctsn, next_tsn;\r\nstruct sctp_ulpevent *retval = NULL;\r\nstruct sk_buff *pd_first = NULL;\r\nstruct sk_buff *pd_last = NULL;\r\nsize_t pd_len = 0;\r\nstruct sctp_association *asoc;\r\nu32 pd_point;\r\nnext_tsn = 0;\r\nskb_queue_walk(&ulpq->reasm, pos) {\r\ncevent = sctp_skb2event(pos);\r\nctsn = cevent->tsn;\r\nswitch (cevent->msg_flags & SCTP_DATA_FRAG_MASK) {\r\ncase SCTP_DATA_FIRST_FRAG:\r\nif (pos == ulpq->reasm.next) {\r\npd_first = pos;\r\npd_last = pos;\r\npd_len = pos->len;\r\n} else {\r\npd_first = NULL;\r\npd_last = NULL;\r\npd_len = 0;\r\n}\r\nfirst_frag = pos;\r\nnext_tsn = ctsn + 1;\r\nbreak;\r\ncase SCTP_DATA_MIDDLE_FRAG:\r\nif ((first_frag) && (ctsn == next_tsn)) {\r\nnext_tsn++;\r\nif (pd_first) {\r\npd_last = pos;\r\npd_len += pos->len;\r\n}\r\n} else\r\nfirst_frag = NULL;\r\nbreak;\r\ncase SCTP_DATA_LAST_FRAG:\r\nif (first_frag && (ctsn == next_tsn))\r\ngoto found;\r\nelse\r\nfirst_frag = NULL;\r\nbreak;\r\n}\r\n}\r\nasoc = ulpq->asoc;\r\nif (pd_first) {\r\nif (!sctp_sk(asoc->base.sk)->frag_interleave &&\r\natomic_read(&sctp_sk(asoc->base.sk)->pd_mode))\r\ngoto done;\r\ncevent = sctp_skb2event(pd_first);\r\npd_point = sctp_sk(asoc->base.sk)->pd_point;\r\nif (pd_point && pd_point <= pd_len) {\r\nretval = sctp_make_reassembled_event(&ulpq->reasm,\r\npd_first,\r\npd_last);\r\nif (retval)\r\nsctp_ulpq_set_pd(ulpq);\r\n}\r\n}\r\ndone:\r\nreturn retval;\r\nfound:\r\nretval = sctp_make_reassembled_event(&ulpq->reasm, first_frag, pos);\r\nif (retval)\r\nretval->msg_flags |= MSG_EOR;\r\ngoto done;\r\n}\r\nstatic struct sctp_ulpevent *sctp_ulpq_retrieve_partial(struct sctp_ulpq *ulpq)\r\n{\r\nstruct sk_buff *pos, *last_frag, *first_frag;\r\nstruct sctp_ulpevent *cevent;\r\n__u32 ctsn, next_tsn;\r\nint is_last;\r\nstruct sctp_ulpevent *retval;\r\nif (skb_queue_empty(&ulpq->reasm))\r\nreturn NULL;\r\nlast_frag = first_frag = NULL;\r\nretval = NULL;\r\nnext_tsn = 0;\r\nis_last = 0;\r\nskb_queue_walk(&ulpq->reasm, pos) {\r\ncevent = sctp_skb2event(pos);\r\nctsn = cevent->tsn;\r\nswitch (cevent->msg_flags & SCTP_DATA_FRAG_MASK) {\r\ncase SCTP_DATA_MIDDLE_FRAG:\r\nif (!first_frag) {\r\nfirst_frag = pos;\r\nnext_tsn = ctsn + 1;\r\nlast_frag = pos;\r\n} else if (next_tsn == ctsn)\r\nnext_tsn++;\r\nelse\r\ngoto done;\r\nbreak;\r\ncase SCTP_DATA_LAST_FRAG:\r\nif (!first_frag)\r\nfirst_frag = pos;\r\nelse if (ctsn != next_tsn)\r\ngoto done;\r\nlast_frag = pos;\r\nis_last = 1;\r\ngoto done;\r\ndefault:\r\nreturn NULL;\r\n}\r\n}\r\ndone:\r\nretval = sctp_make_reassembled_event(&ulpq->reasm, first_frag, last_frag);\r\nif (retval && is_last)\r\nretval->msg_flags |= MSG_EOR;\r\nreturn retval;\r\n}\r\nstatic struct sctp_ulpevent *sctp_ulpq_reasm(struct sctp_ulpq *ulpq,\r\nstruct sctp_ulpevent *event)\r\n{\r\nstruct sctp_ulpevent *retval = NULL;\r\nif (SCTP_DATA_NOT_FRAG == (event->msg_flags & SCTP_DATA_FRAG_MASK)) {\r\nevent->msg_flags |= MSG_EOR;\r\nreturn event;\r\n}\r\nsctp_ulpq_store_reasm(ulpq, event);\r\nif (!ulpq->pd_mode)\r\nretval = sctp_ulpq_retrieve_reassembled(ulpq);\r\nelse {\r\n__u32 ctsn, ctsnap;\r\nctsn = event->tsn;\r\nctsnap = sctp_tsnmap_get_ctsn(&ulpq->asoc->peer.tsn_map);\r\nif (TSN_lte(ctsn, ctsnap))\r\nretval = sctp_ulpq_retrieve_partial(ulpq);\r\n}\r\nreturn retval;\r\n}\r\nstatic struct sctp_ulpevent *sctp_ulpq_retrieve_first(struct sctp_ulpq *ulpq)\r\n{\r\nstruct sk_buff *pos, *last_frag, *first_frag;\r\nstruct sctp_ulpevent *cevent;\r\n__u32 ctsn, next_tsn;\r\nstruct sctp_ulpevent *retval;\r\nif (skb_queue_empty(&ulpq->reasm))\r\nreturn NULL;\r\nlast_frag = first_frag = NULL;\r\nretval = NULL;\r\nnext_tsn = 0;\r\nskb_queue_walk(&ulpq->reasm, pos) {\r\ncevent = sctp_skb2event(pos);\r\nctsn = cevent->tsn;\r\nswitch (cevent->msg_flags & SCTP_DATA_FRAG_MASK) {\r\ncase SCTP_DATA_FIRST_FRAG:\r\nif (!first_frag) {\r\nfirst_frag = pos;\r\nnext_tsn = ctsn + 1;\r\nlast_frag = pos;\r\n} else\r\ngoto done;\r\nbreak;\r\ncase SCTP_DATA_MIDDLE_FRAG:\r\nif (!first_frag)\r\nreturn NULL;\r\nif (ctsn == next_tsn) {\r\nnext_tsn++;\r\nlast_frag = pos;\r\n} else\r\ngoto done;\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\n}\r\ndone:\r\nretval = sctp_make_reassembled_event(&ulpq->reasm, first_frag, last_frag);\r\nreturn retval;\r\n}\r\nvoid sctp_ulpq_reasm_flushtsn(struct sctp_ulpq *ulpq, __u32 fwd_tsn)\r\n{\r\nstruct sk_buff *pos, *tmp;\r\nstruct sctp_ulpevent *event;\r\n__u32 tsn;\r\nif (skb_queue_empty(&ulpq->reasm))\r\nreturn;\r\nskb_queue_walk_safe(&ulpq->reasm, pos, tmp) {\r\nevent = sctp_skb2event(pos);\r\ntsn = event->tsn;\r\nif (TSN_lte(tsn, fwd_tsn)) {\r\n__skb_unlink(pos, &ulpq->reasm);\r\nsctp_ulpevent_free(event);\r\n} else\r\nbreak;\r\n}\r\n}\r\nstatic void sctp_ulpq_reasm_drain(struct sctp_ulpq *ulpq)\r\n{\r\nstruct sctp_ulpevent *event = NULL;\r\nstruct sk_buff_head temp;\r\nif (skb_queue_empty(&ulpq->reasm))\r\nreturn;\r\nwhile ((event = sctp_ulpq_retrieve_reassembled(ulpq)) != NULL) {\r\nif ((event) && (event->msg_flags & MSG_EOR)){\r\nskb_queue_head_init(&temp);\r\n__skb_queue_tail(&temp, sctp_event2skb(event));\r\nevent = sctp_ulpq_order(ulpq, event);\r\n}\r\nif (event)\r\nsctp_ulpq_tail_event(ulpq, event);\r\n}\r\n}\r\nstatic void sctp_ulpq_retrieve_ordered(struct sctp_ulpq *ulpq,\r\nstruct sctp_ulpevent *event)\r\n{\r\nstruct sk_buff_head *event_list;\r\nstruct sk_buff *pos, *tmp;\r\nstruct sctp_ulpevent *cevent;\r\nstruct sctp_stream *in;\r\n__u16 sid, csid, cssn;\r\nsid = event->stream;\r\nin = &ulpq->asoc->ssnmap->in;\r\nevent_list = (struct sk_buff_head *) sctp_event2skb(event)->prev;\r\nsctp_skb_for_each(pos, &ulpq->lobby, tmp) {\r\ncevent = (struct sctp_ulpevent *) pos->cb;\r\ncsid = cevent->stream;\r\ncssn = cevent->ssn;\r\nif (csid > sid)\r\nbreak;\r\nif (csid < sid)\r\ncontinue;\r\nif (cssn != sctp_ssn_peek(in, sid))\r\nbreak;\r\nsctp_ssn_next(in, sid);\r\n__skb_unlink(pos, &ulpq->lobby);\r\n__skb_queue_tail(event_list, pos);\r\n}\r\n}\r\nstatic void sctp_ulpq_store_ordered(struct sctp_ulpq *ulpq,\r\nstruct sctp_ulpevent *event)\r\n{\r\nstruct sk_buff *pos;\r\nstruct sctp_ulpevent *cevent;\r\n__u16 sid, csid;\r\n__u16 ssn, cssn;\r\npos = skb_peek_tail(&ulpq->lobby);\r\nif (!pos) {\r\n__skb_queue_tail(&ulpq->lobby, sctp_event2skb(event));\r\nreturn;\r\n}\r\nsid = event->stream;\r\nssn = event->ssn;\r\ncevent = (struct sctp_ulpevent *) pos->cb;\r\ncsid = cevent->stream;\r\ncssn = cevent->ssn;\r\nif (sid > csid) {\r\n__skb_queue_tail(&ulpq->lobby, sctp_event2skb(event));\r\nreturn;\r\n}\r\nif ((sid == csid) && SSN_lt(cssn, ssn)) {\r\n__skb_queue_tail(&ulpq->lobby, sctp_event2skb(event));\r\nreturn;\r\n}\r\nskb_queue_walk(&ulpq->lobby, pos) {\r\ncevent = (struct sctp_ulpevent *) pos->cb;\r\ncsid = cevent->stream;\r\ncssn = cevent->ssn;\r\nif (csid > sid)\r\nbreak;\r\nif (csid == sid && SSN_lt(ssn, cssn))\r\nbreak;\r\n}\r\n__skb_queue_before(&ulpq->lobby, pos, sctp_event2skb(event));\r\n}\r\nstatic struct sctp_ulpevent *sctp_ulpq_order(struct sctp_ulpq *ulpq,\r\nstruct sctp_ulpevent *event)\r\n{\r\n__u16 sid, ssn;\r\nstruct sctp_stream *in;\r\nif (SCTP_DATA_UNORDERED & event->msg_flags)\r\nreturn event;\r\nsid = event->stream;\r\nssn = event->ssn;\r\nin = &ulpq->asoc->ssnmap->in;\r\nif (ssn != sctp_ssn_peek(in, sid)) {\r\nsctp_ulpq_store_ordered(ulpq, event);\r\nreturn NULL;\r\n}\r\nsctp_ssn_next(in, sid);\r\nsctp_ulpq_retrieve_ordered(ulpq, event);\r\nreturn event;\r\n}\r\nstatic void sctp_ulpq_reap_ordered(struct sctp_ulpq *ulpq, __u16 sid)\r\n{\r\nstruct sk_buff *pos, *tmp;\r\nstruct sctp_ulpevent *cevent;\r\nstruct sctp_ulpevent *event;\r\nstruct sctp_stream *in;\r\nstruct sk_buff_head temp;\r\nstruct sk_buff_head *lobby = &ulpq->lobby;\r\n__u16 csid, cssn;\r\nin = &ulpq->asoc->ssnmap->in;\r\nskb_queue_head_init(&temp);\r\nevent = NULL;\r\nsctp_skb_for_each(pos, lobby, tmp) {\r\ncevent = (struct sctp_ulpevent *) pos->cb;\r\ncsid = cevent->stream;\r\ncssn = cevent->ssn;\r\nif (csid > sid)\r\nbreak;\r\nif (csid < sid)\r\ncontinue;\r\nif (!SSN_lt(cssn, sctp_ssn_peek(in, csid)))\r\nbreak;\r\n__skb_unlink(pos, lobby);\r\nif (!event)\r\nevent = sctp_skb2event(pos);\r\n__skb_queue_tail(&temp, pos);\r\n}\r\nif (event == NULL && pos != (struct sk_buff *)lobby) {\r\ncevent = (struct sctp_ulpevent *) pos->cb;\r\ncsid = cevent->stream;\r\ncssn = cevent->ssn;\r\nif (csid == sid && cssn == sctp_ssn_peek(in, csid)) {\r\nsctp_ssn_next(in, csid);\r\n__skb_unlink(pos, lobby);\r\n__skb_queue_tail(&temp, pos);\r\nevent = sctp_skb2event(pos);\r\n}\r\n}\r\nif (event) {\r\nsctp_ulpq_retrieve_ordered(ulpq, event);\r\nsctp_ulpq_tail_event(ulpq, event);\r\n}\r\n}\r\nvoid sctp_ulpq_skip(struct sctp_ulpq *ulpq, __u16 sid, __u16 ssn)\r\n{\r\nstruct sctp_stream *in;\r\nin = &ulpq->asoc->ssnmap->in;\r\nif (SSN_lt(ssn, sctp_ssn_peek(in, sid)))\r\nreturn;\r\nsctp_ssn_skip(in, sid, ssn);\r\nsctp_ulpq_reap_ordered(ulpq, sid);\r\n}\r\nstatic __u16 sctp_ulpq_renege_list(struct sctp_ulpq *ulpq,\r\nstruct sk_buff_head *list, __u16 needed)\r\n{\r\n__u16 freed = 0;\r\n__u32 tsn;\r\nstruct sk_buff *skb;\r\nstruct sctp_ulpevent *event;\r\nstruct sctp_tsnmap *tsnmap;\r\ntsnmap = &ulpq->asoc->peer.tsn_map;\r\nwhile ((skb = __skb_dequeue_tail(list)) != NULL) {\r\nfreed += skb_headlen(skb);\r\nevent = sctp_skb2event(skb);\r\ntsn = event->tsn;\r\nsctp_ulpevent_free(event);\r\nsctp_tsnmap_renege(tsnmap, tsn);\r\nif (freed >= needed)\r\nreturn freed;\r\n}\r\nreturn freed;\r\n}\r\nstatic __u16 sctp_ulpq_renege_order(struct sctp_ulpq *ulpq, __u16 needed)\r\n{\r\nreturn sctp_ulpq_renege_list(ulpq, &ulpq->lobby, needed);\r\n}\r\nstatic __u16 sctp_ulpq_renege_frags(struct sctp_ulpq *ulpq, __u16 needed)\r\n{\r\nreturn sctp_ulpq_renege_list(ulpq, &ulpq->reasm, needed);\r\n}\r\nvoid sctp_ulpq_partial_delivery(struct sctp_ulpq *ulpq,\r\nstruct sctp_chunk *chunk,\r\ngfp_t gfp)\r\n{\r\nstruct sctp_ulpevent *event;\r\nstruct sctp_association *asoc;\r\nstruct sctp_sock *sp;\r\nasoc = ulpq->asoc;\r\nsp = sctp_sk(asoc->base.sk);\r\nif (ulpq->pd_mode)\r\nreturn;\r\nif (sp->frag_interleave || atomic_read(&sp->pd_mode) == 0) {\r\nevent = sctp_ulpq_retrieve_first(ulpq);\r\nif (event) {\r\nsctp_ulpq_tail_event(ulpq, event);\r\nsctp_ulpq_set_pd(ulpq);\r\nreturn;\r\n}\r\n}\r\n}\r\nvoid sctp_ulpq_renege(struct sctp_ulpq *ulpq, struct sctp_chunk *chunk,\r\ngfp_t gfp)\r\n{\r\nstruct sctp_association *asoc;\r\n__u16 needed, freed;\r\nasoc = ulpq->asoc;\r\nif (chunk) {\r\nneeded = ntohs(chunk->chunk_hdr->length);\r\nneeded -= sizeof(sctp_data_chunk_t);\r\n} else\r\nneeded = SCTP_DEFAULT_MAXWINDOW;\r\nfreed = 0;\r\nif (skb_queue_empty(&asoc->base.sk->sk_receive_queue)) {\r\nfreed = sctp_ulpq_renege_order(ulpq, needed);\r\nif (freed < needed) {\r\nfreed += sctp_ulpq_renege_frags(ulpq, needed - freed);\r\n}\r\n}\r\nif (chunk && (freed >= needed)) {\r\n__u32 tsn;\r\ntsn = ntohl(chunk->subh.data_hdr->tsn);\r\nsctp_tsnmap_mark(&asoc->peer.tsn_map, tsn, chunk->transport);\r\nsctp_ulpq_tail_data(ulpq, chunk, gfp);\r\nsctp_ulpq_partial_delivery(ulpq, chunk, gfp);\r\n}\r\nsk_mem_reclaim(asoc->base.sk);\r\n}\r\nvoid sctp_ulpq_abort_pd(struct sctp_ulpq *ulpq, gfp_t gfp)\r\n{\r\nstruct sctp_ulpevent *ev = NULL;\r\nstruct sock *sk;\r\nif (!ulpq->pd_mode)\r\nreturn;\r\nsk = ulpq->asoc->base.sk;\r\nif (sctp_ulpevent_type_enabled(SCTP_PARTIAL_DELIVERY_EVENT,\r\n&sctp_sk(sk)->subscribe))\r\nev = sctp_ulpevent_make_pdapi(ulpq->asoc,\r\nSCTP_PARTIAL_DELIVERY_ABORTED,\r\ngfp);\r\nif (ev)\r\n__skb_queue_tail(&sk->sk_receive_queue, sctp_event2skb(ev));\r\nif (sctp_ulpq_clear_pd(ulpq) || ev)\r\nsk->sk_data_ready(sk, 0);\r\n}
