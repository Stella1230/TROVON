struct list_head *\r\ntrace_get_fields(struct ftrace_event_call *event_call)\r\n{\r\nif (!event_call->class->get_fields)\r\nreturn &event_call->class->fields;\r\nreturn event_call->class->get_fields(event_call);\r\n}\r\nstatic int __trace_define_field(struct list_head *head, const char *type,\r\nconst char *name, int offset, int size,\r\nint is_signed, int filter_type)\r\n{\r\nstruct ftrace_event_field *field;\r\nfield = kzalloc(sizeof(*field), GFP_KERNEL);\r\nif (!field)\r\ngoto err;\r\nfield->name = kstrdup(name, GFP_KERNEL);\r\nif (!field->name)\r\ngoto err;\r\nfield->type = kstrdup(type, GFP_KERNEL);\r\nif (!field->type)\r\ngoto err;\r\nif (filter_type == FILTER_OTHER)\r\nfield->filter_type = filter_assign_type(type);\r\nelse\r\nfield->filter_type = filter_type;\r\nfield->offset = offset;\r\nfield->size = size;\r\nfield->is_signed = is_signed;\r\nlist_add(&field->link, head);\r\nreturn 0;\r\nerr:\r\nif (field)\r\nkfree(field->name);\r\nkfree(field);\r\nreturn -ENOMEM;\r\n}\r\nint trace_define_field(struct ftrace_event_call *call, const char *type,\r\nconst char *name, int offset, int size, int is_signed,\r\nint filter_type)\r\n{\r\nstruct list_head *head;\r\nif (WARN_ON(!call->class))\r\nreturn 0;\r\nhead = trace_get_fields(call);\r\nreturn __trace_define_field(head, type, name, offset, size,\r\nis_signed, filter_type);\r\n}\r\nstatic int trace_define_common_fields(void)\r\n{\r\nint ret;\r\nstruct trace_entry ent;\r\n__common_field(unsigned short, type);\r\n__common_field(unsigned char, flags);\r\n__common_field(unsigned char, preempt_count);\r\n__common_field(int, pid);\r\n__common_field(int, padding);\r\nreturn ret;\r\n}\r\nvoid trace_destroy_fields(struct ftrace_event_call *call)\r\n{\r\nstruct ftrace_event_field *field, *next;\r\nstruct list_head *head;\r\nhead = trace_get_fields(call);\r\nlist_for_each_entry_safe(field, next, head, link) {\r\nlist_del(&field->link);\r\nkfree(field->type);\r\nkfree(field->name);\r\nkfree(field);\r\n}\r\n}\r\nint trace_event_raw_init(struct ftrace_event_call *call)\r\n{\r\nint id;\r\nid = register_ftrace_event(&call->event);\r\nif (!id)\r\nreturn -ENODEV;\r\nreturn 0;\r\n}\r\nint ftrace_event_reg(struct ftrace_event_call *call,\r\nenum trace_reg type, void *data)\r\n{\r\nswitch (type) {\r\ncase TRACE_REG_REGISTER:\r\nreturn tracepoint_probe_register(call->name,\r\ncall->class->probe,\r\ncall);\r\ncase TRACE_REG_UNREGISTER:\r\ntracepoint_probe_unregister(call->name,\r\ncall->class->probe,\r\ncall);\r\nreturn 0;\r\n#ifdef CONFIG_PERF_EVENTS\r\ncase TRACE_REG_PERF_REGISTER:\r\nreturn tracepoint_probe_register(call->name,\r\ncall->class->perf_probe,\r\ncall);\r\ncase TRACE_REG_PERF_UNREGISTER:\r\ntracepoint_probe_unregister(call->name,\r\ncall->class->perf_probe,\r\ncall);\r\nreturn 0;\r\ncase TRACE_REG_PERF_OPEN:\r\ncase TRACE_REG_PERF_CLOSE:\r\ncase TRACE_REG_PERF_ADD:\r\ncase TRACE_REG_PERF_DEL:\r\nreturn 0;\r\n#endif\r\n}\r\nreturn 0;\r\n}\r\nvoid trace_event_enable_cmd_record(bool enable)\r\n{\r\nstruct ftrace_event_call *call;\r\nmutex_lock(&event_mutex);\r\nlist_for_each_entry(call, &ftrace_events, list) {\r\nif (!(call->flags & TRACE_EVENT_FL_ENABLED))\r\ncontinue;\r\nif (enable) {\r\ntracing_start_cmdline_record();\r\ncall->flags |= TRACE_EVENT_FL_RECORDED_CMD;\r\n} else {\r\ntracing_stop_cmdline_record();\r\ncall->flags &= ~TRACE_EVENT_FL_RECORDED_CMD;\r\n}\r\n}\r\nmutex_unlock(&event_mutex);\r\n}\r\nstatic int ftrace_event_enable_disable(struct ftrace_event_call *call,\r\nint enable)\r\n{\r\nint ret = 0;\r\nswitch (enable) {\r\ncase 0:\r\nif (call->flags & TRACE_EVENT_FL_ENABLED) {\r\ncall->flags &= ~TRACE_EVENT_FL_ENABLED;\r\nif (call->flags & TRACE_EVENT_FL_RECORDED_CMD) {\r\ntracing_stop_cmdline_record();\r\ncall->flags &= ~TRACE_EVENT_FL_RECORDED_CMD;\r\n}\r\ncall->class->reg(call, TRACE_REG_UNREGISTER, NULL);\r\n}\r\nbreak;\r\ncase 1:\r\nif (!(call->flags & TRACE_EVENT_FL_ENABLED)) {\r\nif (trace_flags & TRACE_ITER_RECORD_CMD) {\r\ntracing_start_cmdline_record();\r\ncall->flags |= TRACE_EVENT_FL_RECORDED_CMD;\r\n}\r\nret = call->class->reg(call, TRACE_REG_REGISTER, NULL);\r\nif (ret) {\r\ntracing_stop_cmdline_record();\r\npr_info("event trace: Could not enable event "\r\n"%s\n", call->name);\r\nbreak;\r\n}\r\ncall->flags |= TRACE_EVENT_FL_ENABLED;\r\n}\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic void ftrace_clear_events(void)\r\n{\r\nstruct ftrace_event_call *call;\r\nmutex_lock(&event_mutex);\r\nlist_for_each_entry(call, &ftrace_events, list) {\r\nftrace_event_enable_disable(call, 0);\r\n}\r\nmutex_unlock(&event_mutex);\r\n}\r\nstatic void __put_system(struct event_subsystem *system)\r\n{\r\nstruct event_filter *filter = system->filter;\r\nWARN_ON_ONCE(system->ref_count == 0);\r\nif (--system->ref_count)\r\nreturn;\r\nif (filter) {\r\nkfree(filter->filter_string);\r\nkfree(filter);\r\n}\r\nkfree(system->name);\r\nkfree(system);\r\n}\r\nstatic void __get_system(struct event_subsystem *system)\r\n{\r\nWARN_ON_ONCE(system->ref_count == 0);\r\nsystem->ref_count++;\r\n}\r\nstatic void put_system(struct event_subsystem *system)\r\n{\r\nmutex_lock(&event_mutex);\r\n__put_system(system);\r\nmutex_unlock(&event_mutex);\r\n}\r\nstatic int __ftrace_set_clr_event(const char *match, const char *sub,\r\nconst char *event, int set)\r\n{\r\nstruct ftrace_event_call *call;\r\nint ret = -EINVAL;\r\nmutex_lock(&event_mutex);\r\nlist_for_each_entry(call, &ftrace_events, list) {\r\nif (!call->name || !call->class || !call->class->reg)\r\ncontinue;\r\nif (call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)\r\ncontinue;\r\nif (match &&\r\nstrcmp(match, call->name) != 0 &&\r\nstrcmp(match, call->class->system) != 0)\r\ncontinue;\r\nif (sub && strcmp(sub, call->class->system) != 0)\r\ncontinue;\r\nif (event && strcmp(event, call->name) != 0)\r\ncontinue;\r\nftrace_event_enable_disable(call, set);\r\nret = 0;\r\n}\r\nmutex_unlock(&event_mutex);\r\nreturn ret;\r\n}\r\nstatic int ftrace_set_clr_event(char *buf, int set)\r\n{\r\nchar *event = NULL, *sub = NULL, *match;\r\nmatch = strsep(&buf, ":");\r\nif (buf) {\r\nsub = match;\r\nevent = buf;\r\nmatch = NULL;\r\nif (!strlen(sub) || strcmp(sub, "*") == 0)\r\nsub = NULL;\r\nif (!strlen(event) || strcmp(event, "*") == 0)\r\nevent = NULL;\r\n}\r\nreturn __ftrace_set_clr_event(match, sub, event, set);\r\n}\r\nint trace_set_clr_event(const char *system, const char *event, int set)\r\n{\r\nreturn __ftrace_set_clr_event(NULL, system, event, set);\r\n}\r\nstatic ssize_t\r\nftrace_event_write(struct file *file, const char __user *ubuf,\r\nsize_t cnt, loff_t *ppos)\r\n{\r\nstruct trace_parser parser;\r\nssize_t read, ret;\r\nif (!cnt)\r\nreturn 0;\r\nret = tracing_update_buffers();\r\nif (ret < 0)\r\nreturn ret;\r\nif (trace_parser_get_init(&parser, EVENT_BUF_SIZE + 1))\r\nreturn -ENOMEM;\r\nread = trace_get_user(&parser, ubuf, cnt, ppos);\r\nif (read >= 0 && trace_parser_loaded((&parser))) {\r\nint set = 1;\r\nif (*parser.buffer == '!')\r\nset = 0;\r\nparser.buffer[parser.idx] = 0;\r\nret = ftrace_set_clr_event(parser.buffer + !set, set);\r\nif (ret)\r\ngoto out_put;\r\n}\r\nret = read;\r\nout_put:\r\ntrace_parser_put(&parser);\r\nreturn ret;\r\n}\r\nstatic void *\r\nt_next(struct seq_file *m, void *v, loff_t *pos)\r\n{\r\nstruct ftrace_event_call *call = v;\r\n(*pos)++;\r\nlist_for_each_entry_continue(call, &ftrace_events, list) {\r\nif (call->class && call->class->reg)\r\nreturn call;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void *t_start(struct seq_file *m, loff_t *pos)\r\n{\r\nstruct ftrace_event_call *call;\r\nloff_t l;\r\nmutex_lock(&event_mutex);\r\ncall = list_entry(&ftrace_events, struct ftrace_event_call, list);\r\nfor (l = 0; l <= *pos; ) {\r\ncall = t_next(m, call, &l);\r\nif (!call)\r\nbreak;\r\n}\r\nreturn call;\r\n}\r\nstatic void *\r\ns_next(struct seq_file *m, void *v, loff_t *pos)\r\n{\r\nstruct ftrace_event_call *call = v;\r\n(*pos)++;\r\nlist_for_each_entry_continue(call, &ftrace_events, list) {\r\nif (call->flags & TRACE_EVENT_FL_ENABLED)\r\nreturn call;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void *s_start(struct seq_file *m, loff_t *pos)\r\n{\r\nstruct ftrace_event_call *call;\r\nloff_t l;\r\nmutex_lock(&event_mutex);\r\ncall = list_entry(&ftrace_events, struct ftrace_event_call, list);\r\nfor (l = 0; l <= *pos; ) {\r\ncall = s_next(m, call, &l);\r\nif (!call)\r\nbreak;\r\n}\r\nreturn call;\r\n}\r\nstatic int t_show(struct seq_file *m, void *v)\r\n{\r\nstruct ftrace_event_call *call = v;\r\nif (strcmp(call->class->system, TRACE_SYSTEM) != 0)\r\nseq_printf(m, "%s:", call->class->system);\r\nseq_printf(m, "%s\n", call->name);\r\nreturn 0;\r\n}\r\nstatic void t_stop(struct seq_file *m, void *p)\r\n{\r\nmutex_unlock(&event_mutex);\r\n}\r\nstatic int\r\nftrace_event_seq_open(struct inode *inode, struct file *file)\r\n{\r\nconst struct seq_operations *seq_ops;\r\nif ((file->f_mode & FMODE_WRITE) &&\r\n(file->f_flags & O_TRUNC))\r\nftrace_clear_events();\r\nseq_ops = inode->i_private;\r\nreturn seq_open(file, seq_ops);\r\n}\r\nstatic ssize_t\r\nevent_enable_read(struct file *filp, char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nstruct ftrace_event_call *call = filp->private_data;\r\nchar *buf;\r\nif (call->flags & TRACE_EVENT_FL_ENABLED)\r\nbuf = "1\n";\r\nelse\r\nbuf = "0\n";\r\nreturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\r\n}\r\nstatic ssize_t\r\nevent_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nstruct ftrace_event_call *call = filp->private_data;\r\nunsigned long val;\r\nint ret;\r\nret = kstrtoul_from_user(ubuf, cnt, 10, &val);\r\nif (ret)\r\nreturn ret;\r\nret = tracing_update_buffers();\r\nif (ret < 0)\r\nreturn ret;\r\nswitch (val) {\r\ncase 0:\r\ncase 1:\r\nmutex_lock(&event_mutex);\r\nret = ftrace_event_enable_disable(call, val);\r\nmutex_unlock(&event_mutex);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n*ppos += cnt;\r\nreturn ret ? ret : cnt;\r\n}\r\nstatic ssize_t\r\nsystem_enable_read(struct file *filp, char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nconst char set_to_char[4] = { '?', '0', '1', 'X' };\r\nstruct event_subsystem *system = filp->private_data;\r\nstruct ftrace_event_call *call;\r\nchar buf[2];\r\nint set = 0;\r\nint ret;\r\nmutex_lock(&event_mutex);\r\nlist_for_each_entry(call, &ftrace_events, list) {\r\nif (!call->name || !call->class || !call->class->reg)\r\ncontinue;\r\nif (system && strcmp(call->class->system, system->name) != 0)\r\ncontinue;\r\nset |= (1 << !!(call->flags & TRACE_EVENT_FL_ENABLED));\r\nif (set == 3)\r\nbreak;\r\n}\r\nmutex_unlock(&event_mutex);\r\nbuf[0] = set_to_char[set];\r\nbuf[1] = '\n';\r\nret = simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\r\nreturn ret;\r\n}\r\nstatic ssize_t\r\nsystem_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nstruct event_subsystem *system = filp->private_data;\r\nconst char *name = NULL;\r\nunsigned long val;\r\nssize_t ret;\r\nret = kstrtoul_from_user(ubuf, cnt, 10, &val);\r\nif (ret)\r\nreturn ret;\r\nret = tracing_update_buffers();\r\nif (ret < 0)\r\nreturn ret;\r\nif (val != 0 && val != 1)\r\nreturn -EINVAL;\r\nif (system)\r\nname = system->name;\r\nret = __ftrace_set_clr_event(NULL, name, NULL, val);\r\nif (ret)\r\ngoto out;\r\nret = cnt;\r\nout:\r\n*ppos += cnt;\r\nreturn ret;\r\n}\r\nstatic void *f_next(struct seq_file *m, void *v, loff_t *pos)\r\n{\r\nstruct ftrace_event_call *call = m->private;\r\nstruct ftrace_event_field *field;\r\nstruct list_head *common_head = &ftrace_common_fields;\r\nstruct list_head *head = trace_get_fields(call);\r\n(*pos)++;\r\nswitch ((unsigned long)v) {\r\ncase FORMAT_HEADER:\r\nif (unlikely(list_empty(common_head)))\r\nreturn NULL;\r\nfield = list_entry(common_head->prev,\r\nstruct ftrace_event_field, link);\r\nreturn field;\r\ncase FORMAT_FIELD_SEPERATOR:\r\nif (unlikely(list_empty(head)))\r\nreturn NULL;\r\nfield = list_entry(head->prev, struct ftrace_event_field, link);\r\nreturn field;\r\ncase FORMAT_PRINTFMT:\r\nreturn NULL;\r\n}\r\nfield = v;\r\nif (field->link.prev == common_head)\r\nreturn (void *)FORMAT_FIELD_SEPERATOR;\r\nelse if (field->link.prev == head)\r\nreturn (void *)FORMAT_PRINTFMT;\r\nfield = list_entry(field->link.prev, struct ftrace_event_field, link);\r\nreturn field;\r\n}\r\nstatic void *f_start(struct seq_file *m, loff_t *pos)\r\n{\r\nloff_t l = 0;\r\nvoid *p;\r\nif (!*pos)\r\nreturn (void *)FORMAT_HEADER;\r\np = (void *)FORMAT_HEADER;\r\ndo {\r\np = f_next(m, p, &l);\r\n} while (p && l < *pos);\r\nreturn p;\r\n}\r\nstatic int f_show(struct seq_file *m, void *v)\r\n{\r\nstruct ftrace_event_call *call = m->private;\r\nstruct ftrace_event_field *field;\r\nconst char *array_descriptor;\r\nswitch ((unsigned long)v) {\r\ncase FORMAT_HEADER:\r\nseq_printf(m, "name: %s\n", call->name);\r\nseq_printf(m, "ID: %d\n", call->event.type);\r\nseq_printf(m, "format:\n");\r\nreturn 0;\r\ncase FORMAT_FIELD_SEPERATOR:\r\nseq_putc(m, '\n');\r\nreturn 0;\r\ncase FORMAT_PRINTFMT:\r\nseq_printf(m, "\nprint fmt: %s\n",\r\ncall->print_fmt);\r\nreturn 0;\r\n}\r\nfield = v;\r\narray_descriptor = strchr(field->type, '[');\r\nif (!strncmp(field->type, "__data_loc", 10))\r\narray_descriptor = NULL;\r\nif (!array_descriptor)\r\nseq_printf(m, "\tfield:%s %s;\toffset:%u;\tsize:%u;\tsigned:%d;\n",\r\nfield->type, field->name, field->offset,\r\nfield->size, !!field->is_signed);\r\nelse\r\nseq_printf(m, "\tfield:%.*s %s%s;\toffset:%u;\tsize:%u;\tsigned:%d;\n",\r\n(int)(array_descriptor - field->type),\r\nfield->type, field->name,\r\narray_descriptor, field->offset,\r\nfield->size, !!field->is_signed);\r\nreturn 0;\r\n}\r\nstatic void f_stop(struct seq_file *m, void *p)\r\n{\r\n}\r\nstatic int trace_format_open(struct inode *inode, struct file *file)\r\n{\r\nstruct ftrace_event_call *call = inode->i_private;\r\nstruct seq_file *m;\r\nint ret;\r\nret = seq_open(file, &trace_format_seq_ops);\r\nif (ret < 0)\r\nreturn ret;\r\nm = file->private_data;\r\nm->private = call;\r\nreturn 0;\r\n}\r\nstatic ssize_t\r\nevent_id_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)\r\n{\r\nstruct ftrace_event_call *call = filp->private_data;\r\nstruct trace_seq *s;\r\nint r;\r\nif (*ppos)\r\nreturn 0;\r\ns = kmalloc(sizeof(*s), GFP_KERNEL);\r\nif (!s)\r\nreturn -ENOMEM;\r\ntrace_seq_init(s);\r\ntrace_seq_printf(s, "%d\n", call->event.type);\r\nr = simple_read_from_buffer(ubuf, cnt, ppos,\r\ns->buffer, s->len);\r\nkfree(s);\r\nreturn r;\r\n}\r\nstatic ssize_t\r\nevent_filter_read(struct file *filp, char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nstruct ftrace_event_call *call = filp->private_data;\r\nstruct trace_seq *s;\r\nint r;\r\nif (*ppos)\r\nreturn 0;\r\ns = kmalloc(sizeof(*s), GFP_KERNEL);\r\nif (!s)\r\nreturn -ENOMEM;\r\ntrace_seq_init(s);\r\nprint_event_filter(call, s);\r\nr = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);\r\nkfree(s);\r\nreturn r;\r\n}\r\nstatic ssize_t\r\nevent_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nstruct ftrace_event_call *call = filp->private_data;\r\nchar *buf;\r\nint err;\r\nif (cnt >= PAGE_SIZE)\r\nreturn -EINVAL;\r\nbuf = (char *)__get_free_page(GFP_TEMPORARY);\r\nif (!buf)\r\nreturn -ENOMEM;\r\nif (copy_from_user(buf, ubuf, cnt)) {\r\nfree_page((unsigned long) buf);\r\nreturn -EFAULT;\r\n}\r\nbuf[cnt] = '\0';\r\nerr = apply_event_filter(call, buf);\r\nfree_page((unsigned long) buf);\r\nif (err < 0)\r\nreturn err;\r\n*ppos += cnt;\r\nreturn cnt;\r\n}\r\nstatic int subsystem_open(struct inode *inode, struct file *filp)\r\n{\r\nstruct event_subsystem *system = NULL;\r\nint ret;\r\nif (!inode->i_private)\r\ngoto skip_search;\r\nmutex_lock(&event_mutex);\r\nlist_for_each_entry(system, &event_subsystems, list) {\r\nif (system == inode->i_private) {\r\nif (!system->nr_events) {\r\nsystem = NULL;\r\nbreak;\r\n}\r\n__get_system(system);\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&event_mutex);\r\nif (system != inode->i_private)\r\nreturn -ENODEV;\r\nskip_search:\r\nret = tracing_open_generic(inode, filp);\r\nif (ret < 0 && system)\r\nput_system(system);\r\nreturn ret;\r\n}\r\nstatic int subsystem_release(struct inode *inode, struct file *file)\r\n{\r\nstruct event_subsystem *system = inode->i_private;\r\nif (system)\r\nput_system(system);\r\nreturn 0;\r\n}\r\nstatic ssize_t\r\nsubsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nstruct event_subsystem *system = filp->private_data;\r\nstruct trace_seq *s;\r\nint r;\r\nif (*ppos)\r\nreturn 0;\r\ns = kmalloc(sizeof(*s), GFP_KERNEL);\r\nif (!s)\r\nreturn -ENOMEM;\r\ntrace_seq_init(s);\r\nprint_subsystem_event_filter(system, s);\r\nr = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);\r\nkfree(s);\r\nreturn r;\r\n}\r\nstatic ssize_t\r\nsubsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,\r\nloff_t *ppos)\r\n{\r\nstruct event_subsystem *system = filp->private_data;\r\nchar *buf;\r\nint err;\r\nif (cnt >= PAGE_SIZE)\r\nreturn -EINVAL;\r\nbuf = (char *)__get_free_page(GFP_TEMPORARY);\r\nif (!buf)\r\nreturn -ENOMEM;\r\nif (copy_from_user(buf, ubuf, cnt)) {\r\nfree_page((unsigned long) buf);\r\nreturn -EFAULT;\r\n}\r\nbuf[cnt] = '\0';\r\nerr = apply_subsystem_event_filter(system, buf);\r\nfree_page((unsigned long) buf);\r\nif (err < 0)\r\nreturn err;\r\n*ppos += cnt;\r\nreturn cnt;\r\n}\r\nstatic ssize_t\r\nshow_header(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)\r\n{\r\nint (*func)(struct trace_seq *s) = filp->private_data;\r\nstruct trace_seq *s;\r\nint r;\r\nif (*ppos)\r\nreturn 0;\r\ns = kmalloc(sizeof(*s), GFP_KERNEL);\r\nif (!s)\r\nreturn -ENOMEM;\r\ntrace_seq_init(s);\r\nfunc(s);\r\nr = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);\r\nkfree(s);\r\nreturn r;\r\n}\r\nstatic struct dentry *event_trace_events_dir(void)\r\n{\r\nstatic struct dentry *d_tracer;\r\nstatic struct dentry *d_events;\r\nif (d_events)\r\nreturn d_events;\r\nd_tracer = tracing_init_dentry();\r\nif (!d_tracer)\r\nreturn NULL;\r\nd_events = debugfs_create_dir("events", d_tracer);\r\nif (!d_events)\r\npr_warning("Could not create debugfs "\r\n"'events' directory\n");\r\nreturn d_events;\r\n}\r\nstatic struct dentry *\r\nevent_subsystem_dir(const char *name, struct dentry *d_events)\r\n{\r\nstruct event_subsystem *system;\r\nstruct dentry *entry;\r\nlist_for_each_entry(system, &event_subsystems, list) {\r\nif (strcmp(system->name, name) == 0) {\r\nsystem->nr_events++;\r\nreturn system->entry;\r\n}\r\n}\r\nsystem = kmalloc(sizeof(*system), GFP_KERNEL);\r\nif (!system) {\r\npr_warning("No memory to create event subsystem %s\n",\r\nname);\r\nreturn d_events;\r\n}\r\nsystem->entry = debugfs_create_dir(name, d_events);\r\nif (!system->entry) {\r\npr_warning("Could not create event subsystem %s\n",\r\nname);\r\nkfree(system);\r\nreturn d_events;\r\n}\r\nsystem->nr_events = 1;\r\nsystem->ref_count = 1;\r\nsystem->name = kstrdup(name, GFP_KERNEL);\r\nif (!system->name) {\r\ndebugfs_remove(system->entry);\r\nkfree(system);\r\nreturn d_events;\r\n}\r\nlist_add(&system->list, &event_subsystems);\r\nsystem->filter = NULL;\r\nsystem->filter = kzalloc(sizeof(struct event_filter), GFP_KERNEL);\r\nif (!system->filter) {\r\npr_warning("Could not allocate filter for subsystem "\r\n"'%s'\n", name);\r\nreturn system->entry;\r\n}\r\nentry = debugfs_create_file("filter", 0644, system->entry, system,\r\n&ftrace_subsystem_filter_fops);\r\nif (!entry) {\r\nkfree(system->filter);\r\nsystem->filter = NULL;\r\npr_warning("Could not create debugfs "\r\n"'%s/filter' entry\n", name);\r\n}\r\ntrace_create_file("enable", 0644, system->entry, system,\r\n&ftrace_system_enable_fops);\r\nreturn system->entry;\r\n}\r\nstatic int\r\nevent_create_dir(struct ftrace_event_call *call, struct dentry *d_events,\r\nconst struct file_operations *id,\r\nconst struct file_operations *enable,\r\nconst struct file_operations *filter,\r\nconst struct file_operations *format)\r\n{\r\nstruct list_head *head;\r\nint ret;\r\nif (strcmp(call->class->system, TRACE_SYSTEM) != 0)\r\nd_events = event_subsystem_dir(call->class->system, d_events);\r\ncall->dir = debugfs_create_dir(call->name, d_events);\r\nif (!call->dir) {\r\npr_warning("Could not create debugfs "\r\n"'%s' directory\n", call->name);\r\nreturn -1;\r\n}\r\nif (call->class->reg && !(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE))\r\ntrace_create_file("enable", 0644, call->dir, call,\r\nenable);\r\n#ifdef CONFIG_PERF_EVENTS\r\nif (call->event.type && call->class->reg)\r\ntrace_create_file("id", 0444, call->dir, call,\r\nid);\r\n#endif\r\nhead = trace_get_fields(call);\r\nif (list_empty(head)) {\r\nret = call->class->define_fields(call);\r\nif (ret < 0) {\r\npr_warning("Could not initialize trace point"\r\n" events/%s\n", call->name);\r\nreturn ret;\r\n}\r\n}\r\ntrace_create_file("filter", 0644, call->dir, call,\r\nfilter);\r\ntrace_create_file("format", 0444, call->dir, call,\r\nformat);\r\nreturn 0;\r\n}\r\nstatic int\r\n__trace_add_event_call(struct ftrace_event_call *call, struct module *mod,\r\nconst struct file_operations *id,\r\nconst struct file_operations *enable,\r\nconst struct file_operations *filter,\r\nconst struct file_operations *format)\r\n{\r\nstruct dentry *d_events;\r\nint ret;\r\nif (!call->name)\r\nreturn -EINVAL;\r\nif (call->class->raw_init) {\r\nret = call->class->raw_init(call);\r\nif (ret < 0) {\r\nif (ret != -ENOSYS)\r\npr_warning("Could not initialize trace events/%s\n",\r\ncall->name);\r\nreturn ret;\r\n}\r\n}\r\nd_events = event_trace_events_dir();\r\nif (!d_events)\r\nreturn -ENOENT;\r\nret = event_create_dir(call, d_events, id, enable, filter, format);\r\nif (!ret)\r\nlist_add(&call->list, &ftrace_events);\r\ncall->mod = mod;\r\nreturn ret;\r\n}\r\nint trace_add_event_call(struct ftrace_event_call *call)\r\n{\r\nint ret;\r\nmutex_lock(&event_mutex);\r\nret = __trace_add_event_call(call, NULL, &ftrace_event_id_fops,\r\n&ftrace_enable_fops,\r\n&ftrace_event_filter_fops,\r\n&ftrace_event_format_fops);\r\nmutex_unlock(&event_mutex);\r\nreturn ret;\r\n}\r\nstatic void remove_subsystem_dir(const char *name)\r\n{\r\nstruct event_subsystem *system;\r\nif (strcmp(name, TRACE_SYSTEM) == 0)\r\nreturn;\r\nlist_for_each_entry(system, &event_subsystems, list) {\r\nif (strcmp(system->name, name) == 0) {\r\nif (!--system->nr_events) {\r\ndebugfs_remove_recursive(system->entry);\r\nlist_del(&system->list);\r\n__put_system(system);\r\n}\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void __trace_remove_event_call(struct ftrace_event_call *call)\r\n{\r\nftrace_event_enable_disable(call, 0);\r\nif (call->event.funcs)\r\n__unregister_ftrace_event(&call->event);\r\ndebugfs_remove_recursive(call->dir);\r\nlist_del(&call->list);\r\ntrace_destroy_fields(call);\r\ndestroy_preds(call);\r\nremove_subsystem_dir(call->class->system);\r\n}\r\nvoid trace_remove_event_call(struct ftrace_event_call *call)\r\n{\r\nmutex_lock(&event_mutex);\r\ndown_write(&trace_event_mutex);\r\n__trace_remove_event_call(call);\r\nup_write(&trace_event_mutex);\r\nmutex_unlock(&event_mutex);\r\n}\r\nstatic struct ftrace_module_file_ops *\r\ntrace_create_file_ops(struct module *mod)\r\n{\r\nstruct ftrace_module_file_ops *file_ops;\r\nfile_ops = kmalloc(sizeof(*file_ops), GFP_KERNEL);\r\nif (!file_ops)\r\nreturn NULL;\r\nfile_ops->mod = mod;\r\nfile_ops->id = ftrace_event_id_fops;\r\nfile_ops->id.owner = mod;\r\nfile_ops->enable = ftrace_enable_fops;\r\nfile_ops->enable.owner = mod;\r\nfile_ops->filter = ftrace_event_filter_fops;\r\nfile_ops->filter.owner = mod;\r\nfile_ops->format = ftrace_event_format_fops;\r\nfile_ops->format.owner = mod;\r\nlist_add(&file_ops->list, &ftrace_module_file_list);\r\nreturn file_ops;\r\n}\r\nstatic void trace_module_add_events(struct module *mod)\r\n{\r\nstruct ftrace_module_file_ops *file_ops = NULL;\r\nstruct ftrace_event_call **call, **start, **end;\r\nstart = mod->trace_events;\r\nend = mod->trace_events + mod->num_trace_events;\r\nif (start == end)\r\nreturn;\r\nfile_ops = trace_create_file_ops(mod);\r\nif (!file_ops)\r\nreturn;\r\nfor_each_event(call, start, end) {\r\n__trace_add_event_call(*call, mod,\r\n&file_ops->id, &file_ops->enable,\r\n&file_ops->filter, &file_ops->format);\r\n}\r\n}\r\nstatic void trace_module_remove_events(struct module *mod)\r\n{\r\nstruct ftrace_module_file_ops *file_ops;\r\nstruct ftrace_event_call *call, *p;\r\nbool found = false;\r\ndown_write(&trace_event_mutex);\r\nlist_for_each_entry_safe(call, p, &ftrace_events, list) {\r\nif (call->mod == mod) {\r\nfound = true;\r\n__trace_remove_event_call(call);\r\n}\r\n}\r\nlist_for_each_entry(file_ops, &ftrace_module_file_list, list) {\r\nif (file_ops->mod == mod)\r\nbreak;\r\n}\r\nif (&file_ops->list != &ftrace_module_file_list) {\r\nlist_del(&file_ops->list);\r\nkfree(file_ops);\r\n}\r\nif (found)\r\ntracing_reset_current_online_cpus();\r\nup_write(&trace_event_mutex);\r\n}\r\nstatic int trace_module_notify(struct notifier_block *self,\r\nunsigned long val, void *data)\r\n{\r\nstruct module *mod = data;\r\nmutex_lock(&event_mutex);\r\nswitch (val) {\r\ncase MODULE_STATE_COMING:\r\ntrace_module_add_events(mod);\r\nbreak;\r\ncase MODULE_STATE_GOING:\r\ntrace_module_remove_events(mod);\r\nbreak;\r\n}\r\nmutex_unlock(&event_mutex);\r\nreturn 0;\r\n}\r\nstatic int trace_module_notify(struct notifier_block *self,\r\nunsigned long val, void *data)\r\n{\r\nreturn 0;\r\n}\r\nstatic __init int setup_trace_event(char *str)\r\n{\r\nstrlcpy(bootup_event_buf, str, COMMAND_LINE_SIZE);\r\nring_buffer_expanded = 1;\r\ntracing_selftest_disabled = 1;\r\nreturn 1;\r\n}\r\nstatic __init int event_trace_init(void)\r\n{\r\nstruct ftrace_event_call **call;\r\nstruct dentry *d_tracer;\r\nstruct dentry *entry;\r\nstruct dentry *d_events;\r\nint ret;\r\nchar *buf = bootup_event_buf;\r\nchar *token;\r\nd_tracer = tracing_init_dentry();\r\nif (!d_tracer)\r\nreturn 0;\r\nentry = debugfs_create_file("available_events", 0444, d_tracer,\r\n(void *)&show_event_seq_ops,\r\n&ftrace_avail_fops);\r\nif (!entry)\r\npr_warning("Could not create debugfs "\r\n"'available_events' entry\n");\r\nentry = debugfs_create_file("set_event", 0644, d_tracer,\r\n(void *)&show_set_event_seq_ops,\r\n&ftrace_set_event_fops);\r\nif (!entry)\r\npr_warning("Could not create debugfs "\r\n"'set_event' entry\n");\r\nd_events = event_trace_events_dir();\r\nif (!d_events)\r\nreturn 0;\r\ntrace_create_file("header_page", 0444, d_events,\r\nring_buffer_print_page_header,\r\n&ftrace_show_header_fops);\r\ntrace_create_file("header_event", 0444, d_events,\r\nring_buffer_print_entry_header,\r\n&ftrace_show_header_fops);\r\ntrace_create_file("enable", 0644, d_events,\r\nNULL, &ftrace_system_enable_fops);\r\nif (trace_define_common_fields())\r\npr_warning("tracing: Failed to allocate common fields");\r\nfor_each_event(call, __start_ftrace_events, __stop_ftrace_events) {\r\n__trace_add_event_call(*call, NULL, &ftrace_event_id_fops,\r\n&ftrace_enable_fops,\r\n&ftrace_event_filter_fops,\r\n&ftrace_event_format_fops);\r\n}\r\nwhile (true) {\r\ntoken = strsep(&buf, ",");\r\nif (!token)\r\nbreak;\r\nif (!*token)\r\ncontinue;\r\nret = ftrace_set_clr_event(token, 1);\r\nif (ret)\r\npr_warning("Failed to enable trace event: %s\n", token);\r\n}\r\nret = register_module_notifier(&trace_module_nb);\r\nif (ret)\r\npr_warning("Failed to register trace events module notifier\n");\r\nreturn 0;\r\n}\r\nstatic __init void test_work(struct work_struct *dummy)\r\n{\r\nspin_lock(&test_spinlock);\r\nspin_lock_irq(&test_spinlock_irq);\r\nudelay(1);\r\nspin_unlock_irq(&test_spinlock_irq);\r\nspin_unlock(&test_spinlock);\r\nmutex_lock(&test_mutex);\r\nmsleep(1);\r\nmutex_unlock(&test_mutex);\r\n}\r\nstatic __init int event_test_thread(void *unused)\r\n{\r\nvoid *test_malloc;\r\ntest_malloc = kmalloc(1234, GFP_KERNEL);\r\nif (!test_malloc)\r\npr_info("failed to kmalloc\n");\r\nschedule_on_each_cpu(test_work);\r\nkfree(test_malloc);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nwhile (!kthread_should_stop())\r\nschedule();\r\nreturn 0;\r\n}\r\nstatic __init void event_test_stuff(void)\r\n{\r\nstruct task_struct *test_thread;\r\ntest_thread = kthread_run(event_test_thread, NULL, "test-events");\r\nmsleep(1);\r\nkthread_stop(test_thread);\r\n}\r\nstatic __init void event_trace_self_tests(void)\r\n{\r\nstruct ftrace_event_call *call;\r\nstruct event_subsystem *system;\r\nint ret;\r\npr_info("Running tests on trace events:\n");\r\nlist_for_each_entry(call, &ftrace_events, list) {\r\nif (!call->class || !call->class->probe)\r\ncontinue;\r\n#ifndef CONFIG_EVENT_TRACE_TEST_SYSCALLS\r\nif (call->class->system &&\r\nstrcmp(call->class->system, "syscalls") == 0)\r\ncontinue;\r\n#endif\r\npr_info("Testing event %s: ", call->name);\r\nif (call->flags & TRACE_EVENT_FL_ENABLED) {\r\npr_warning("Enabled event during self test!\n");\r\nWARN_ON_ONCE(1);\r\ncontinue;\r\n}\r\nftrace_event_enable_disable(call, 1);\r\nevent_test_stuff();\r\nftrace_event_enable_disable(call, 0);\r\npr_cont("OK\n");\r\n}\r\npr_info("Running tests on trace event systems:\n");\r\nlist_for_each_entry(system, &event_subsystems, list) {\r\nif (strcmp(system->name, "ftrace") == 0)\r\ncontinue;\r\npr_info("Testing event system %s: ", system->name);\r\nret = __ftrace_set_clr_event(NULL, system->name, NULL, 1);\r\nif (WARN_ON_ONCE(ret)) {\r\npr_warning("error enabling system %s\n",\r\nsystem->name);\r\ncontinue;\r\n}\r\nevent_test_stuff();\r\nret = __ftrace_set_clr_event(NULL, system->name, NULL, 0);\r\nif (WARN_ON_ONCE(ret))\r\npr_warning("error disabling system %s\n",\r\nsystem->name);\r\npr_cont("OK\n");\r\n}\r\npr_info("Running tests on all trace events:\n");\r\npr_info("Testing all events: ");\r\nret = __ftrace_set_clr_event(NULL, NULL, NULL, 1);\r\nif (WARN_ON_ONCE(ret)) {\r\npr_warning("error enabling all events\n");\r\nreturn;\r\n}\r\nevent_test_stuff();\r\nret = __ftrace_set_clr_event(NULL, NULL, NULL, 0);\r\nif (WARN_ON_ONCE(ret)) {\r\npr_warning("error disabling all events\n");\r\nreturn;\r\n}\r\npr_cont("OK\n");\r\n}\r\nstatic void\r\nfunction_test_events_call(unsigned long ip, unsigned long parent_ip)\r\n{\r\nstruct ring_buffer_event *event;\r\nstruct ring_buffer *buffer;\r\nstruct ftrace_entry *entry;\r\nunsigned long flags;\r\nlong disabled;\r\nint cpu;\r\nint pc;\r\npc = preempt_count();\r\npreempt_disable_notrace();\r\ncpu = raw_smp_processor_id();\r\ndisabled = atomic_inc_return(&per_cpu(ftrace_test_event_disable, cpu));\r\nif (disabled != 1)\r\ngoto out;\r\nlocal_save_flags(flags);\r\nevent = trace_current_buffer_lock_reserve(&buffer,\r\nTRACE_FN, sizeof(*entry),\r\nflags, pc);\r\nif (!event)\r\ngoto out;\r\nentry = ring_buffer_event_data(event);\r\nentry->ip = ip;\r\nentry->parent_ip = parent_ip;\r\ntrace_nowake_buffer_unlock_commit(buffer, event, flags, pc);\r\nout:\r\natomic_dec(&per_cpu(ftrace_test_event_disable, cpu));\r\npreempt_enable_notrace();\r\n}\r\nstatic __init void event_trace_self_test_with_function(void)\r\n{\r\nint ret;\r\nret = register_ftrace_function(&trace_ops);\r\nif (WARN_ON(ret < 0)) {\r\npr_info("Failed to enable function tracer for event tests\n");\r\nreturn;\r\n}\r\npr_info("Running tests again, along with the function tracer\n");\r\nevent_trace_self_tests();\r\nunregister_ftrace_function(&trace_ops);\r\n}\r\nstatic __init void event_trace_self_test_with_function(void)\r\n{\r\n}\r\nstatic __init int event_trace_self_tests_init(void)\r\n{\r\nif (!tracing_selftest_disabled) {\r\nevent_trace_self_tests();\r\nevent_trace_self_test_with_function();\r\n}\r\nreturn 0;\r\n}
