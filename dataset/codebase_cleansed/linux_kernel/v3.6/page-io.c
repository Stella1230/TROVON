int __init ext4_init_pageio(void)\r\n{\r\nio_page_cachep = KMEM_CACHE(ext4_io_page, SLAB_RECLAIM_ACCOUNT);\r\nif (io_page_cachep == NULL)\r\nreturn -ENOMEM;\r\nio_end_cachep = KMEM_CACHE(ext4_io_end, SLAB_RECLAIM_ACCOUNT);\r\nif (io_end_cachep == NULL) {\r\nkmem_cache_destroy(io_page_cachep);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nvoid ext4_exit_pageio(void)\r\n{\r\nkmem_cache_destroy(io_end_cachep);\r\nkmem_cache_destroy(io_page_cachep);\r\n}\r\nvoid ext4_ioend_wait(struct inode *inode)\r\n{\r\nwait_queue_head_t *wq = ext4_ioend_wq(inode);\r\nwait_event(*wq, (atomic_read(&EXT4_I(inode)->i_ioend_count) == 0));\r\n}\r\nstatic void put_io_page(struct ext4_io_page *io_page)\r\n{\r\nif (atomic_dec_and_test(&io_page->p_count)) {\r\nend_page_writeback(io_page->p_page);\r\nput_page(io_page->p_page);\r\nkmem_cache_free(io_page_cachep, io_page);\r\n}\r\n}\r\nvoid ext4_free_io_end(ext4_io_end_t *io)\r\n{\r\nint i;\r\nBUG_ON(!io);\r\nif (io->page)\r\nput_page(io->page);\r\nfor (i = 0; i < io->num_io_pages; i++)\r\nput_io_page(io->pages[i]);\r\nio->num_io_pages = 0;\r\nif (atomic_dec_and_test(&EXT4_I(io->inode)->i_ioend_count))\r\nwake_up_all(ext4_ioend_wq(io->inode));\r\nkmem_cache_free(io_end_cachep, io);\r\n}\r\nint ext4_end_io_nolock(ext4_io_end_t *io)\r\n{\r\nstruct inode *inode = io->inode;\r\nloff_t offset = io->offset;\r\nssize_t size = io->size;\r\nint ret = 0;\r\next4_debug("ext4_end_io_nolock: io 0x%p from inode %lu,list->next 0x%p,"\r\n"list->prev 0x%p\n",\r\nio, inode->i_ino, io->list.next, io->list.prev);\r\nret = ext4_convert_unwritten_extents(inode, offset, size);\r\nif (ret < 0) {\r\next4_msg(inode->i_sb, KERN_EMERG,\r\n"failed to convert unwritten extents to written "\r\n"extents -- potential data loss! "\r\n"(inode %lu, offset %llu, size %zd, error %d)",\r\ninode->i_ino, offset, size, ret);\r\n}\r\nif (io->iocb)\r\naio_complete(io->iocb, io->result, 0);\r\nif (io->flag & EXT4_IO_END_DIRECT)\r\ninode_dio_done(inode);\r\nif (atomic_dec_and_test(&EXT4_I(inode)->i_aiodio_unwritten))\r\nwake_up_all(ext4_ioend_wq(io->inode));\r\nreturn ret;\r\n}\r\nstatic void ext4_end_io_work(struct work_struct *work)\r\n{\r\next4_io_end_t *io = container_of(work, ext4_io_end_t, work);\r\nstruct inode *inode = io->inode;\r\nstruct ext4_inode_info *ei = EXT4_I(inode);\r\nunsigned long flags;\r\nspin_lock_irqsave(&ei->i_completed_io_lock, flags);\r\nif (io->flag & EXT4_IO_END_IN_FSYNC)\r\ngoto requeue;\r\nif (list_empty(&io->list)) {\r\nspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\r\ngoto free;\r\n}\r\nif (!mutex_trylock(&inode->i_mutex)) {\r\nbool was_queued;\r\nrequeue:\r\nwas_queued = !!(io->flag & EXT4_IO_END_QUEUED);\r\nio->flag |= EXT4_IO_END_QUEUED;\r\nspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\r\nqueue_work(EXT4_SB(inode->i_sb)->dio_unwritten_wq, &io->work);\r\nif (was_queued)\r\nyield();\r\nreturn;\r\n}\r\nlist_del_init(&io->list);\r\nspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\r\n(void) ext4_end_io_nolock(io);\r\nmutex_unlock(&inode->i_mutex);\r\nfree:\r\next4_free_io_end(io);\r\n}\r\next4_io_end_t *ext4_init_io_end(struct inode *inode, gfp_t flags)\r\n{\r\next4_io_end_t *io = kmem_cache_zalloc(io_end_cachep, flags);\r\nif (io) {\r\natomic_inc(&EXT4_I(inode)->i_ioend_count);\r\nio->inode = inode;\r\nINIT_WORK(&io->work, ext4_end_io_work);\r\nINIT_LIST_HEAD(&io->list);\r\n}\r\nreturn io;\r\n}\r\nstatic void buffer_io_error(struct buffer_head *bh)\r\n{\r\nchar b[BDEVNAME_SIZE];\r\nprintk(KERN_ERR "Buffer I/O error on device %s, logical block %llu\n",\r\nbdevname(bh->b_bdev, b),\r\n(unsigned long long)bh->b_blocknr);\r\n}\r\nstatic void ext4_end_bio(struct bio *bio, int error)\r\n{\r\next4_io_end_t *io_end = bio->bi_private;\r\nstruct workqueue_struct *wq;\r\nstruct inode *inode;\r\nunsigned long flags;\r\nint i;\r\nsector_t bi_sector = bio->bi_sector;\r\nBUG_ON(!io_end);\r\nbio->bi_private = NULL;\r\nbio->bi_end_io = NULL;\r\nif (test_bit(BIO_UPTODATE, &bio->bi_flags))\r\nerror = 0;\r\nbio_put(bio);\r\nfor (i = 0; i < io_end->num_io_pages; i++) {\r\nstruct page *page = io_end->pages[i]->p_page;\r\nstruct buffer_head *bh, *head;\r\nloff_t offset;\r\nloff_t io_end_offset;\r\nif (error) {\r\nSetPageError(page);\r\nset_bit(AS_EIO, &page->mapping->flags);\r\nhead = page_buffers(page);\r\nBUG_ON(!head);\r\nio_end_offset = io_end->offset + io_end->size;\r\noffset = (sector_t) page->index << PAGE_CACHE_SHIFT;\r\nbh = head;\r\ndo {\r\nif ((offset >= io_end->offset) &&\r\n(offset+bh->b_size <= io_end_offset))\r\nbuffer_io_error(bh);\r\noffset += bh->b_size;\r\nbh = bh->b_this_page;\r\n} while (bh != head);\r\n}\r\nput_io_page(io_end->pages[i]);\r\n}\r\nio_end->num_io_pages = 0;\r\ninode = io_end->inode;\r\nif (error) {\r\nio_end->flag |= EXT4_IO_END_ERROR;\r\next4_warning(inode->i_sb, "I/O error writing to inode %lu "\r\n"(offset %llu size %ld starting block %llu)",\r\ninode->i_ino,\r\n(unsigned long long) io_end->offset,\r\n(long) io_end->size,\r\n(unsigned long long)\r\nbi_sector >> (inode->i_blkbits - 9));\r\n}\r\nif (!(io_end->flag & EXT4_IO_END_UNWRITTEN)) {\r\next4_free_io_end(io_end);\r\nreturn;\r\n}\r\nspin_lock_irqsave(&EXT4_I(inode)->i_completed_io_lock, flags);\r\nlist_add_tail(&io_end->list, &EXT4_I(inode)->i_completed_io_list);\r\nspin_unlock_irqrestore(&EXT4_I(inode)->i_completed_io_lock, flags);\r\nwq = EXT4_SB(inode->i_sb)->dio_unwritten_wq;\r\nqueue_work(wq, &io_end->work);\r\n}\r\nvoid ext4_io_submit(struct ext4_io_submit *io)\r\n{\r\nstruct bio *bio = io->io_bio;\r\nif (bio) {\r\nbio_get(io->io_bio);\r\nsubmit_bio(io->io_op, io->io_bio);\r\nBUG_ON(bio_flagged(io->io_bio, BIO_EOPNOTSUPP));\r\nbio_put(io->io_bio);\r\n}\r\nio->io_bio = NULL;\r\nio->io_op = 0;\r\nio->io_end = NULL;\r\n}\r\nstatic int io_submit_init(struct ext4_io_submit *io,\r\nstruct inode *inode,\r\nstruct writeback_control *wbc,\r\nstruct buffer_head *bh)\r\n{\r\next4_io_end_t *io_end;\r\nstruct page *page = bh->b_page;\r\nint nvecs = bio_get_nr_vecs(bh->b_bdev);\r\nstruct bio *bio;\r\nio_end = ext4_init_io_end(inode, GFP_NOFS);\r\nif (!io_end)\r\nreturn -ENOMEM;\r\nbio = bio_alloc(GFP_NOIO, min(nvecs, BIO_MAX_PAGES));\r\nbio->bi_sector = bh->b_blocknr * (bh->b_size >> 9);\r\nbio->bi_bdev = bh->b_bdev;\r\nbio->bi_private = io->io_end = io_end;\r\nbio->bi_end_io = ext4_end_bio;\r\nio_end->offset = (page->index << PAGE_CACHE_SHIFT) + bh_offset(bh);\r\nio->io_bio = bio;\r\nio->io_op = (wbc->sync_mode == WB_SYNC_ALL ? WRITE_SYNC : WRITE);\r\nio->io_next_block = bh->b_blocknr;\r\nreturn 0;\r\n}\r\nstatic int io_submit_add_bh(struct ext4_io_submit *io,\r\nstruct ext4_io_page *io_page,\r\nstruct inode *inode,\r\nstruct writeback_control *wbc,\r\nstruct buffer_head *bh)\r\n{\r\next4_io_end_t *io_end;\r\nint ret;\r\nif (buffer_new(bh)) {\r\nclear_buffer_new(bh);\r\nunmap_underlying_metadata(bh->b_bdev, bh->b_blocknr);\r\n}\r\nif (!buffer_mapped(bh) || buffer_delay(bh)) {\r\nif (!buffer_mapped(bh))\r\nclear_buffer_dirty(bh);\r\nif (io->io_bio)\r\next4_io_submit(io);\r\nreturn 0;\r\n}\r\nif (io->io_bio && bh->b_blocknr != io->io_next_block) {\r\nsubmit_and_retry:\r\next4_io_submit(io);\r\n}\r\nif (io->io_bio == NULL) {\r\nret = io_submit_init(io, inode, wbc, bh);\r\nif (ret)\r\nreturn ret;\r\n}\r\nio_end = io->io_end;\r\nif ((io_end->num_io_pages >= MAX_IO_PAGES) &&\r\n(io_end->pages[io_end->num_io_pages-1] != io_page))\r\ngoto submit_and_retry;\r\nif (buffer_uninit(bh))\r\next4_set_io_unwritten_flag(inode, io_end);\r\nio->io_end->size += bh->b_size;\r\nio->io_next_block++;\r\nret = bio_add_page(io->io_bio, bh->b_page, bh->b_size, bh_offset(bh));\r\nif (ret != bh->b_size)\r\ngoto submit_and_retry;\r\nif ((io_end->num_io_pages == 0) ||\r\n(io_end->pages[io_end->num_io_pages-1] != io_page)) {\r\nio_end->pages[io_end->num_io_pages++] = io_page;\r\natomic_inc(&io_page->p_count);\r\n}\r\nreturn 0;\r\n}\r\nint ext4_bio_write_page(struct ext4_io_submit *io,\r\nstruct page *page,\r\nint len,\r\nstruct writeback_control *wbc)\r\n{\r\nstruct inode *inode = page->mapping->host;\r\nunsigned block_start, block_end, blocksize;\r\nstruct ext4_io_page *io_page;\r\nstruct buffer_head *bh, *head;\r\nint ret = 0;\r\nblocksize = 1 << inode->i_blkbits;\r\nBUG_ON(!PageLocked(page));\r\nBUG_ON(PageWriteback(page));\r\nio_page = kmem_cache_alloc(io_page_cachep, GFP_NOFS);\r\nif (!io_page) {\r\nset_page_dirty(page);\r\nunlock_page(page);\r\nreturn -ENOMEM;\r\n}\r\nio_page->p_page = page;\r\natomic_set(&io_page->p_count, 1);\r\nget_page(page);\r\nset_page_writeback(page);\r\nClearPageError(page);\r\nfor (bh = head = page_buffers(page), block_start = 0;\r\nbh != head || !block_start;\r\nblock_start = block_end, bh = bh->b_this_page) {\r\nblock_end = block_start + blocksize;\r\nif (block_start >= len) {\r\nzero_user_segment(page, block_start, block_end);\r\nclear_buffer_dirty(bh);\r\nset_buffer_uptodate(bh);\r\ncontinue;\r\n}\r\nclear_buffer_dirty(bh);\r\nret = io_submit_add_bh(io, io_page, inode, wbc, bh);\r\nif (ret) {\r\nset_page_dirty(page);\r\nbreak;\r\n}\r\n}\r\nunlock_page(page);\r\nput_io_page(io_page);\r\nreturn ret;\r\n}
