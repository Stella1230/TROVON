static int get_ch_index(int *status, unsigned int base)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_CHAN; i++) {\r\nif (*status & (1 << (i + base))) {\r\n*status = *status & ~(1 << (i + base));\r\npr_debug("MDMA: index %d New status %x\n", i, *status);\r\nreturn i;\r\n}\r\n}\r\nreturn -1;\r\n}\r\nstatic int get_block_ts(int len, int tx_width, int block_size)\r\n{\r\nint byte_width = 0, block_ts = 0;\r\nswitch (tx_width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nbyte_width = 1;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nbyte_width = 2;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\ndefault:\r\nbyte_width = 4;\r\nbreak;\r\n}\r\nblock_ts = len/byte_width;\r\nif (block_ts > block_size)\r\nblock_ts = 0xFFFF;\r\nreturn block_ts;\r\n}\r\nstatic void dmac1_mask_periphral_intr(struct middma_device *mid)\r\n{\r\nu32 pimr;\r\nif (mid->pimr_mask) {\r\npimr = readl(mid->mask_reg + LNW_PERIPHRAL_MASK);\r\npimr |= mid->pimr_mask;\r\nwritel(pimr, mid->mask_reg + LNW_PERIPHRAL_MASK);\r\n}\r\nreturn;\r\n}\r\nstatic void dmac1_unmask_periphral_intr(struct intel_mid_dma_chan *midc)\r\n{\r\nu32 pimr;\r\nstruct middma_device *mid = to_middma_device(midc->chan.device);\r\nif (mid->pimr_mask) {\r\npimr = readl(mid->mask_reg + LNW_PERIPHRAL_MASK);\r\npimr &= ~mid->pimr_mask;\r\nwritel(pimr, mid->mask_reg + LNW_PERIPHRAL_MASK);\r\n}\r\nreturn;\r\n}\r\nstatic void enable_dma_interrupt(struct intel_mid_dma_chan *midc)\r\n{\r\ndmac1_unmask_periphral_intr(midc);\r\niowrite32(UNMASK_INTR_REG(midc->ch_id), midc->dma_base + MASK_TFR);\r\niowrite32(UNMASK_INTR_REG(midc->ch_id), midc->dma_base + MASK_ERR);\r\nreturn;\r\n}\r\nstatic void disable_dma_interrupt(struct intel_mid_dma_chan *midc)\r\n{\r\niowrite32(MASK_INTR_REG(midc->ch_id), midc->dma_base + MASK_BLOCK);\r\niowrite32(MASK_INTR_REG(midc->ch_id), midc->dma_base + MASK_TFR);\r\niowrite32(MASK_INTR_REG(midc->ch_id), midc->dma_base + MASK_ERR);\r\nreturn;\r\n}\r\nstatic struct intel_mid_dma_desc *midc_desc_get(struct intel_mid_dma_chan *midc)\r\n{\r\nstruct intel_mid_dma_desc *desc, *_desc;\r\nstruct intel_mid_dma_desc *ret = NULL;\r\nspin_lock_bh(&midc->lock);\r\nlist_for_each_entry_safe(desc, _desc, &midc->free_list, desc_node) {\r\nif (async_tx_test_ack(&desc->txd)) {\r\nlist_del(&desc->desc_node);\r\nret = desc;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_bh(&midc->lock);\r\nreturn ret;\r\n}\r\nstatic void midc_desc_put(struct intel_mid_dma_chan *midc,\r\nstruct intel_mid_dma_desc *desc)\r\n{\r\nif (desc) {\r\nspin_lock_bh(&midc->lock);\r\nlist_add_tail(&desc->desc_node, &midc->free_list);\r\nspin_unlock_bh(&midc->lock);\r\n}\r\n}\r\nstatic void midc_dostart(struct intel_mid_dma_chan *midc,\r\nstruct intel_mid_dma_desc *first)\r\n{\r\nstruct middma_device *mid = to_middma_device(midc->chan.device);\r\nif (midc->busy && test_ch_en(midc->dma_base, midc->ch_id)) {\r\npr_err("ERR_MDMA: channel is busy in start\n");\r\nreturn;\r\n}\r\nmidc->busy = true;\r\niowrite32(first->sar, midc->ch_regs + SAR);\r\niowrite32(first->dar, midc->ch_regs + DAR);\r\niowrite32(first->lli_phys, midc->ch_regs + LLP);\r\niowrite32(first->cfg_hi, midc->ch_regs + CFG_HIGH);\r\niowrite32(first->cfg_lo, midc->ch_regs + CFG_LOW);\r\niowrite32(first->ctl_lo, midc->ch_regs + CTL_LOW);\r\niowrite32(first->ctl_hi, midc->ch_regs + CTL_HIGH);\r\npr_debug("MDMA:TX SAR %x,DAR %x,CFGL %x,CFGH %x,CTLH %x, CTLL %x\n",\r\n(int)first->sar, (int)first->dar, first->cfg_hi,\r\nfirst->cfg_lo, first->ctl_hi, first->ctl_lo);\r\nfirst->status = DMA_IN_PROGRESS;\r\niowrite32(ENABLE_CHANNEL(midc->ch_id), mid->dma_base + DMA_CHAN_EN);\r\n}\r\nstatic void midc_descriptor_complete(struct intel_mid_dma_chan *midc,\r\nstruct intel_mid_dma_desc *desc)\r\n__releases(&midc->lock\r\nstatic void midc_scan_descriptors(struct middma_device *mid,\r\nstruct intel_mid_dma_chan *midc)\r\n{\r\nstruct intel_mid_dma_desc *desc = NULL, *_desc = NULL;\r\nlist_for_each_entry_safe(desc, _desc, &midc->active_list, desc_node) {\r\nif (desc->status == DMA_IN_PROGRESS)\r\nmidc_descriptor_complete(midc, desc);\r\n}\r\nreturn;\r\n}\r\nstatic int midc_lli_fill_sg(struct intel_mid_dma_chan *midc,\r\nstruct intel_mid_dma_desc *desc,\r\nstruct scatterlist *sglist,\r\nunsigned int sglen,\r\nunsigned int flags)\r\n{\r\nstruct intel_mid_dma_slave *mids;\r\nstruct scatterlist *sg;\r\ndma_addr_t lli_next, sg_phy_addr;\r\nstruct intel_mid_dma_lli *lli_bloc_desc;\r\nunion intel_mid_dma_ctl_lo ctl_lo;\r\nunion intel_mid_dma_ctl_hi ctl_hi;\r\nint i;\r\npr_debug("MDMA: Entered midc_lli_fill_sg\n");\r\nmids = midc->mid_slave;\r\nlli_bloc_desc = desc->lli;\r\nlli_next = desc->lli_phys;\r\nctl_lo.ctl_lo = desc->ctl_lo;\r\nctl_hi.ctl_hi = desc->ctl_hi;\r\nfor_each_sg(sglist, sg, sglen, i) {\r\nif (i != sglen - 1) {\r\nlli_next = lli_next +\r\nsizeof(struct intel_mid_dma_lli);\r\n} else {\r\nif (flags & DMA_PREP_CIRCULAR_LIST) {\r\npr_debug("MDMA: LLI is configured in circular mode\n");\r\nlli_next = desc->lli_phys;\r\n} else {\r\nlli_next = 0;\r\nctl_lo.ctlx.llp_dst_en = 0;\r\nctl_lo.ctlx.llp_src_en = 0;\r\n}\r\n}\r\nctl_hi.ctlx.block_ts = get_block_ts(sg_dma_len(sg),\r\ndesc->width,\r\nmidc->dma->block_size);\r\nsg_phy_addr = sg_dma_address(sg);\r\nif (desc->dirn == DMA_MEM_TO_DEV) {\r\nlli_bloc_desc->sar = sg_phy_addr;\r\nlli_bloc_desc->dar = mids->dma_slave.dst_addr;\r\n} else if (desc->dirn == DMA_DEV_TO_MEM) {\r\nlli_bloc_desc->sar = mids->dma_slave.src_addr;\r\nlli_bloc_desc->dar = sg_phy_addr;\r\n}\r\nlli_bloc_desc->llp = lli_next;\r\nlli_bloc_desc->ctl_lo = ctl_lo.ctl_lo;\r\nlli_bloc_desc->ctl_hi = ctl_hi.ctl_hi;\r\nlli_bloc_desc++;\r\n}\r\ndesc->ctl_lo = desc->lli->ctl_lo;\r\ndesc->ctl_hi = desc->lli->ctl_hi;\r\ndesc->sar = desc->lli->sar;\r\ndesc->dar = desc->lli->dar;\r\nreturn 0;\r\n}\r\nstatic dma_cookie_t intel_mid_dma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct intel_mid_dma_desc *desc = to_intel_mid_dma_desc(tx);\r\nstruct intel_mid_dma_chan *midc = to_intel_mid_dma_chan(tx->chan);\r\ndma_cookie_t cookie;\r\nspin_lock_bh(&midc->lock);\r\ncookie = dma_cookie_assign(tx);\r\nif (list_empty(&midc->active_list))\r\nlist_add_tail(&desc->desc_node, &midc->active_list);\r\nelse\r\nlist_add_tail(&desc->desc_node, &midc->queue);\r\nmidc_dostart(midc, desc);\r\nspin_unlock_bh(&midc->lock);\r\nreturn cookie;\r\n}\r\nstatic void intel_mid_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct intel_mid_dma_chan *midc = to_intel_mid_dma_chan(chan);\r\nspin_lock_bh(&midc->lock);\r\nif (!list_empty(&midc->queue))\r\nmidc_scan_descriptors(to_middma_device(chan->device), midc);\r\nspin_unlock_bh(&midc->lock);\r\n}\r\nstatic enum dma_status intel_mid_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct intel_mid_dma_chan *midc = to_intel_mid_dma_chan(chan);\r\nenum dma_status ret;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret != DMA_SUCCESS) {\r\nspin_lock_bh(&midc->lock);\r\nmidc_scan_descriptors(to_middma_device(chan->device), midc);\r\nspin_unlock_bh(&midc->lock);\r\nret = dma_cookie_status(chan, cookie, txstate);\r\n}\r\nreturn ret;\r\n}\r\nstatic int dma_slave_control(struct dma_chan *chan, unsigned long arg)\r\n{\r\nstruct intel_mid_dma_chan *midc = to_intel_mid_dma_chan(chan);\r\nstruct dma_slave_config *slave = (struct dma_slave_config *)arg;\r\nstruct intel_mid_dma_slave *mid_slave;\r\nBUG_ON(!midc);\r\nBUG_ON(!slave);\r\npr_debug("MDMA: slave control called\n");\r\nmid_slave = to_intel_mid_dma_slave(slave);\r\nBUG_ON(!mid_slave);\r\nmidc->mid_slave = mid_slave;\r\nreturn 0;\r\n}\r\nstatic int intel_mid_dma_device_control(struct dma_chan *chan,\r\nenum dma_ctrl_cmd cmd, unsigned long arg)\r\n{\r\nstruct intel_mid_dma_chan *midc = to_intel_mid_dma_chan(chan);\r\nstruct middma_device *mid = to_middma_device(chan->device);\r\nstruct intel_mid_dma_desc *desc, *_desc;\r\nunion intel_mid_dma_cfg_lo cfg_lo;\r\nif (cmd == DMA_SLAVE_CONFIG)\r\nreturn dma_slave_control(chan, arg);\r\nif (cmd != DMA_TERMINATE_ALL)\r\nreturn -ENXIO;\r\nspin_lock_bh(&midc->lock);\r\nif (midc->busy == false) {\r\nspin_unlock_bh(&midc->lock);\r\nreturn 0;\r\n}\r\ncfg_lo.cfg_lo = ioread32(midc->ch_regs + CFG_LOW);\r\ncfg_lo.cfgx.ch_susp = 1;\r\niowrite32(cfg_lo.cfg_lo, midc->ch_regs + CFG_LOW);\r\niowrite32(DISABLE_CHANNEL(midc->ch_id), mid->dma_base + DMA_CHAN_EN);\r\nmidc->busy = false;\r\ndisable_dma_interrupt(midc);\r\nmidc->descs_allocated = 0;\r\nspin_unlock_bh(&midc->lock);\r\nlist_for_each_entry_safe(desc, _desc, &midc->active_list, desc_node) {\r\nif (desc->lli != NULL) {\r\npci_pool_free(desc->lli_pool, desc->lli,\r\ndesc->lli_phys);\r\npci_pool_destroy(desc->lli_pool);\r\ndesc->lli = NULL;\r\n}\r\nlist_move(&desc->desc_node, &midc->free_list);\r\n}\r\nreturn 0;\r\n}\r\nstatic struct dma_async_tx_descriptor *intel_mid_dma_prep_memcpy(\r\nstruct dma_chan *chan, dma_addr_t dest,\r\ndma_addr_t src, size_t len, unsigned long flags)\r\n{\r\nstruct intel_mid_dma_chan *midc;\r\nstruct intel_mid_dma_desc *desc = NULL;\r\nstruct intel_mid_dma_slave *mids;\r\nunion intel_mid_dma_ctl_lo ctl_lo;\r\nunion intel_mid_dma_ctl_hi ctl_hi;\r\nunion intel_mid_dma_cfg_lo cfg_lo;\r\nunion intel_mid_dma_cfg_hi cfg_hi;\r\nenum dma_slave_buswidth width;\r\npr_debug("MDMA: Prep for memcpy\n");\r\nBUG_ON(!chan);\r\nif (!len)\r\nreturn NULL;\r\nmidc = to_intel_mid_dma_chan(chan);\r\nBUG_ON(!midc);\r\nmids = midc->mid_slave;\r\nBUG_ON(!mids);\r\npr_debug("MDMA:called for DMA %x CH %d Length %zu\n",\r\nmidc->dma->pci_id, midc->ch_id, len);\r\npr_debug("MDMA:Cfg passed Mode %x, Dirn %x, HS %x, Width %x\n",\r\nmids->cfg_mode, mids->dma_slave.direction,\r\nmids->hs_mode, mids->dma_slave.src_addr_width);\r\nif (mids->hs_mode == LNW_DMA_SW_HS) {\r\ncfg_lo.cfg_lo = 0;\r\ncfg_lo.cfgx.hs_sel_dst = 1;\r\ncfg_lo.cfgx.hs_sel_src = 1;\r\n} else if (mids->hs_mode == LNW_DMA_HW_HS)\r\ncfg_lo.cfg_lo = 0x00000;\r\nif (mids->cfg_mode == LNW_DMA_MEM_TO_MEM) {\r\ncfg_hi.cfg_hi = 0;\r\n} else {\r\ncfg_hi.cfg_hi = 0;\r\nif (midc->dma->pimr_mask) {\r\ncfg_hi.cfgx.protctl = 0x0;\r\ncfg_hi.cfgx.fifo_mode = 1;\r\nif (mids->dma_slave.direction == DMA_MEM_TO_DEV) {\r\ncfg_hi.cfgx.src_per = 0;\r\nif (mids->device_instance == 0)\r\ncfg_hi.cfgx.dst_per = 3;\r\nif (mids->device_instance == 1)\r\ncfg_hi.cfgx.dst_per = 1;\r\n} else if (mids->dma_slave.direction == DMA_DEV_TO_MEM) {\r\nif (mids->device_instance == 0)\r\ncfg_hi.cfgx.src_per = 2;\r\nif (mids->device_instance == 1)\r\ncfg_hi.cfgx.src_per = 0;\r\ncfg_hi.cfgx.dst_per = 0;\r\n}\r\n} else {\r\ncfg_hi.cfgx.protctl = 0x1;\r\ncfg_hi.cfgx.src_per = cfg_hi.cfgx.dst_per =\r\nmidc->ch_id - midc->dma->chan_base;\r\n}\r\n}\r\nctl_hi.ctlx.reser = 0;\r\nctl_hi.ctlx.done = 0;\r\nwidth = mids->dma_slave.src_addr_width;\r\nctl_hi.ctlx.block_ts = get_block_ts(len, width, midc->dma->block_size);\r\npr_debug("MDMA:calc len %d for block size %d\n",\r\nctl_hi.ctlx.block_ts, midc->dma->block_size);\r\nctl_lo.ctl_lo = 0;\r\nctl_lo.ctlx.int_en = 1;\r\nctl_lo.ctlx.dst_msize = mids->dma_slave.src_maxburst;\r\nctl_lo.ctlx.src_msize = mids->dma_slave.dst_maxburst;\r\nctl_lo.ctlx.dst_tr_width = mids->dma_slave.dst_addr_width / 2;\r\nctl_lo.ctlx.src_tr_width = mids->dma_slave.src_addr_width / 2;\r\nif (mids->cfg_mode == LNW_DMA_MEM_TO_MEM) {\r\nctl_lo.ctlx.tt_fc = 0;\r\nctl_lo.ctlx.sinc = 0;\r\nctl_lo.ctlx.dinc = 0;\r\n} else {\r\nif (mids->dma_slave.direction == DMA_MEM_TO_DEV) {\r\nctl_lo.ctlx.sinc = 0;\r\nctl_lo.ctlx.dinc = 2;\r\nctl_lo.ctlx.tt_fc = 1;\r\n} else if (mids->dma_slave.direction == DMA_DEV_TO_MEM) {\r\nctl_lo.ctlx.sinc = 2;\r\nctl_lo.ctlx.dinc = 0;\r\nctl_lo.ctlx.tt_fc = 2;\r\n}\r\n}\r\npr_debug("MDMA:Calc CTL LO %x, CTL HI %x, CFG LO %x, CFG HI %x\n",\r\nctl_lo.ctl_lo, ctl_hi.ctl_hi, cfg_lo.cfg_lo, cfg_hi.cfg_hi);\r\nenable_dma_interrupt(midc);\r\ndesc = midc_desc_get(midc);\r\nif (desc == NULL)\r\ngoto err_desc_get;\r\ndesc->sar = src;\r\ndesc->dar = dest ;\r\ndesc->len = len;\r\ndesc->cfg_hi = cfg_hi.cfg_hi;\r\ndesc->cfg_lo = cfg_lo.cfg_lo;\r\ndesc->ctl_lo = ctl_lo.ctl_lo;\r\ndesc->ctl_hi = ctl_hi.ctl_hi;\r\ndesc->width = width;\r\ndesc->dirn = mids->dma_slave.direction;\r\ndesc->lli_phys = 0;\r\ndesc->lli = NULL;\r\ndesc->lli_pool = NULL;\r\nreturn &desc->txd;\r\nerr_desc_get:\r\npr_err("ERR_MDMA: Failed to get desc\n");\r\nmidc_desc_put(midc, desc);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *intel_mid_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct intel_mid_dma_chan *midc = NULL;\r\nstruct intel_mid_dma_slave *mids = NULL;\r\nstruct intel_mid_dma_desc *desc = NULL;\r\nstruct dma_async_tx_descriptor *txd = NULL;\r\nunion intel_mid_dma_ctl_lo ctl_lo;\r\npr_debug("MDMA: Prep for slave SG\n");\r\nif (!sg_len) {\r\npr_err("MDMA: Invalid SG length\n");\r\nreturn NULL;\r\n}\r\nmidc = to_intel_mid_dma_chan(chan);\r\nBUG_ON(!midc);\r\nmids = midc->mid_slave;\r\nBUG_ON(!mids);\r\nif (!midc->dma->pimr_mask) {\r\nif (sg_len == 1) {\r\ntxd = intel_mid_dma_prep_memcpy(chan,\r\nmids->dma_slave.dst_addr,\r\nmids->dma_slave.src_addr,\r\nsg_dma_len(sgl),\r\nflags);\r\nreturn txd;\r\n} else {\r\npr_warn("MDMA: SG list is not supported by this controller\n");\r\nreturn NULL;\r\n}\r\n}\r\npr_debug("MDMA: SG Length = %d, direction = %d, Flags = %#lx\n",\r\nsg_len, direction, flags);\r\ntxd = intel_mid_dma_prep_memcpy(chan, 0, 0, sg_dma_len(sgl), flags);\r\nif (NULL == txd) {\r\npr_err("MDMA: Prep memcpy failed\n");\r\nreturn NULL;\r\n}\r\ndesc = to_intel_mid_dma_desc(txd);\r\ndesc->dirn = direction;\r\nctl_lo.ctl_lo = desc->ctl_lo;\r\nctl_lo.ctlx.llp_dst_en = 1;\r\nctl_lo.ctlx.llp_src_en = 1;\r\ndesc->ctl_lo = ctl_lo.ctl_lo;\r\ndesc->lli_length = sg_len;\r\ndesc->current_lli = 0;\r\ndesc->lli_pool = pci_pool_create("intel_mid_dma_lli_pool",\r\nmidc->dma->pdev,\r\n(sizeof(struct intel_mid_dma_lli)*sg_len),\r\n32, 0);\r\nif (NULL == desc->lli_pool) {\r\npr_err("MID_DMA:LLI pool create failed\n");\r\nreturn NULL;\r\n}\r\ndesc->lli = pci_pool_alloc(desc->lli_pool, GFP_KERNEL, &desc->lli_phys);\r\nif (!desc->lli) {\r\npr_err("MID_DMA: LLI alloc failed\n");\r\npci_pool_destroy(desc->lli_pool);\r\nreturn NULL;\r\n}\r\nmidc_lli_fill_sg(midc, desc, sgl, sg_len, flags);\r\nif (flags & DMA_PREP_INTERRUPT) {\r\niowrite32(UNMASK_INTR_REG(midc->ch_id),\r\nmidc->dma_base + MASK_BLOCK);\r\npr_debug("MDMA:Enabled Block interrupt\n");\r\n}\r\nreturn &desc->txd;\r\n}\r\nstatic void intel_mid_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct intel_mid_dma_chan *midc = to_intel_mid_dma_chan(chan);\r\nstruct middma_device *mid = to_middma_device(chan->device);\r\nstruct intel_mid_dma_desc *desc, *_desc;\r\nif (true == midc->busy) {\r\npr_err("ERR_MDMA: trying to free ch in use\n");\r\n}\r\nspin_lock_bh(&midc->lock);\r\nmidc->descs_allocated = 0;\r\nlist_for_each_entry_safe(desc, _desc, &midc->active_list, desc_node) {\r\nlist_del(&desc->desc_node);\r\npci_pool_free(mid->dma_pool, desc, desc->txd.phys);\r\n}\r\nlist_for_each_entry_safe(desc, _desc, &midc->free_list, desc_node) {\r\nlist_del(&desc->desc_node);\r\npci_pool_free(mid->dma_pool, desc, desc->txd.phys);\r\n}\r\nlist_for_each_entry_safe(desc, _desc, &midc->queue, desc_node) {\r\nlist_del(&desc->desc_node);\r\npci_pool_free(mid->dma_pool, desc, desc->txd.phys);\r\n}\r\nspin_unlock_bh(&midc->lock);\r\nmidc->in_use = false;\r\nmidc->busy = false;\r\niowrite32(MASK_INTR_REG(midc->ch_id), mid->dma_base + MASK_BLOCK);\r\niowrite32(MASK_INTR_REG(midc->ch_id), mid->dma_base + MASK_ERR);\r\npm_runtime_put(&mid->pdev->dev);\r\n}\r\nstatic int intel_mid_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct intel_mid_dma_chan *midc = to_intel_mid_dma_chan(chan);\r\nstruct middma_device *mid = to_middma_device(chan->device);\r\nstruct intel_mid_dma_desc *desc;\r\ndma_addr_t phys;\r\nint i = 0;\r\npm_runtime_get_sync(&mid->pdev->dev);\r\nif (mid->state == SUSPENDED) {\r\nif (dma_resume(&mid->pdev->dev)) {\r\npr_err("ERR_MDMA: resume failed");\r\nreturn -EFAULT;\r\n}\r\n}\r\nif (test_ch_en(mid->dma_base, midc->ch_id)) {\r\npr_err("ERR_MDMA: ch not idle\n");\r\npm_runtime_put(&mid->pdev->dev);\r\nreturn -EIO;\r\n}\r\ndma_cookie_init(chan);\r\nspin_lock_bh(&midc->lock);\r\nwhile (midc->descs_allocated < DESCS_PER_CHANNEL) {\r\nspin_unlock_bh(&midc->lock);\r\ndesc = pci_pool_alloc(mid->dma_pool, GFP_KERNEL, &phys);\r\nif (!desc) {\r\npr_err("ERR_MDMA: desc failed\n");\r\npm_runtime_put(&mid->pdev->dev);\r\nreturn -ENOMEM;\r\n}\r\ndma_async_tx_descriptor_init(&desc->txd, chan);\r\ndesc->txd.tx_submit = intel_mid_dma_tx_submit;\r\ndesc->txd.flags = DMA_CTRL_ACK;\r\ndesc->txd.phys = phys;\r\nspin_lock_bh(&midc->lock);\r\ni = ++midc->descs_allocated;\r\nlist_add_tail(&desc->desc_node, &midc->free_list);\r\n}\r\nspin_unlock_bh(&midc->lock);\r\nmidc->in_use = true;\r\nmidc->busy = false;\r\npr_debug("MID_DMA: Desc alloc done ret: %d desc\n", i);\r\nreturn i;\r\n}\r\nstatic void midc_handle_error(struct middma_device *mid,\r\nstruct intel_mid_dma_chan *midc)\r\n{\r\nmidc_scan_descriptors(mid, midc);\r\n}\r\nstatic void dma_tasklet(unsigned long data)\r\n{\r\nstruct middma_device *mid = NULL;\r\nstruct intel_mid_dma_chan *midc = NULL;\r\nu32 status, raw_tfr, raw_block;\r\nint i;\r\nmid = (struct middma_device *)data;\r\nif (mid == NULL) {\r\npr_err("ERR_MDMA: tasklet Null param\n");\r\nreturn;\r\n}\r\npr_debug("MDMA: in tasklet for device %x\n", mid->pci_id);\r\nraw_tfr = ioread32(mid->dma_base + RAW_TFR);\r\nraw_block = ioread32(mid->dma_base + RAW_BLOCK);\r\nstatus = raw_tfr | raw_block;\r\nstatus &= mid->intr_mask;\r\nwhile (status) {\r\ni = get_ch_index(&status, mid->chan_base);\r\nif (i < 0) {\r\npr_err("ERR_MDMA:Invalid ch index %x\n", i);\r\nreturn;\r\n}\r\nmidc = &mid->ch[i];\r\nif (midc == NULL) {\r\npr_err("ERR_MDMA:Null param midc\n");\r\nreturn;\r\n}\r\npr_debug("MDMA:Tx complete interrupt %x, Ch No %d Index %d\n",\r\nstatus, midc->ch_id, i);\r\nmidc->raw_tfr = raw_tfr;\r\nmidc->raw_block = raw_block;\r\nspin_lock_bh(&midc->lock);\r\niowrite32((1 << midc->ch_id), mid->dma_base + CLEAR_TFR);\r\nif (raw_block) {\r\niowrite32((1 << midc->ch_id),\r\nmid->dma_base + CLEAR_BLOCK);\r\n}\r\nmidc_scan_descriptors(mid, midc);\r\npr_debug("MDMA:Scan of desc... complete, unmasking\n");\r\niowrite32(UNMASK_INTR_REG(midc->ch_id),\r\nmid->dma_base + MASK_TFR);\r\nif (raw_block) {\r\niowrite32(UNMASK_INTR_REG(midc->ch_id),\r\nmid->dma_base + MASK_BLOCK);\r\n}\r\nspin_unlock_bh(&midc->lock);\r\n}\r\nstatus = ioread32(mid->dma_base + RAW_ERR);\r\nstatus &= mid->intr_mask;\r\nwhile (status) {\r\ni = get_ch_index(&status, mid->chan_base);\r\nif (i < 0) {\r\npr_err("ERR_MDMA:Invalid ch index %x\n", i);\r\nreturn;\r\n}\r\nmidc = &mid->ch[i];\r\nif (midc == NULL) {\r\npr_err("ERR_MDMA:Null param midc\n");\r\nreturn;\r\n}\r\npr_debug("MDMA:Tx complete interrupt %x, Ch No %d Index %d\n",\r\nstatus, midc->ch_id, i);\r\niowrite32((1 << midc->ch_id), mid->dma_base + CLEAR_ERR);\r\nspin_lock_bh(&midc->lock);\r\nmidc_handle_error(mid, midc);\r\niowrite32(UNMASK_INTR_REG(midc->ch_id),\r\nmid->dma_base + MASK_ERR);\r\nspin_unlock_bh(&midc->lock);\r\n}\r\npr_debug("MDMA:Exiting takslet...\n");\r\nreturn;\r\n}\r\nstatic void dma_tasklet1(unsigned long data)\r\n{\r\npr_debug("MDMA:in takslet1...\n");\r\nreturn dma_tasklet(data);\r\n}\r\nstatic void dma_tasklet2(unsigned long data)\r\n{\r\npr_debug("MDMA:in takslet2...\n");\r\nreturn dma_tasklet(data);\r\n}\r\nstatic irqreturn_t intel_mid_dma_interrupt(int irq, void *data)\r\n{\r\nstruct middma_device *mid = data;\r\nu32 tfr_status, err_status;\r\nint call_tasklet = 0;\r\ntfr_status = ioread32(mid->dma_base + RAW_TFR);\r\nerr_status = ioread32(mid->dma_base + RAW_ERR);\r\nif (!tfr_status && !err_status)\r\nreturn IRQ_NONE;\r\npr_debug("MDMA:Got an interrupt on irq %d\n", irq);\r\npr_debug("MDMA: Status %x, Mask %x\n", tfr_status, mid->intr_mask);\r\ntfr_status &= mid->intr_mask;\r\nif (tfr_status) {\r\niowrite32((tfr_status << INT_MASK_WE), mid->dma_base + MASK_TFR);\r\niowrite32((tfr_status << INT_MASK_WE), mid->dma_base + MASK_BLOCK);\r\npr_debug("MDMA: Calling tasklet %x\n", tfr_status);\r\ncall_tasklet = 1;\r\n}\r\nerr_status &= mid->intr_mask;\r\nif (err_status) {\r\niowrite32((err_status << INT_MASK_WE),\r\nmid->dma_base + MASK_ERR);\r\ncall_tasklet = 1;\r\n}\r\nif (call_tasklet)\r\ntasklet_schedule(&mid->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t intel_mid_dma_interrupt1(int irq, void *data)\r\n{\r\nreturn intel_mid_dma_interrupt(irq, data);\r\n}\r\nstatic irqreturn_t intel_mid_dma_interrupt2(int irq, void *data)\r\n{\r\nreturn intel_mid_dma_interrupt(irq, data);\r\n}\r\nstatic int mid_setup_dma(struct pci_dev *pdev)\r\n{\r\nstruct middma_device *dma = pci_get_drvdata(pdev);\r\nint err, i;\r\ndma->dma_pool = pci_pool_create("intel_mid_dma_desc_pool", pdev,\r\nsizeof(struct intel_mid_dma_desc),\r\n32, 0);\r\nif (NULL == dma->dma_pool) {\r\npr_err("ERR_MDMA:pci_pool_create failed\n");\r\nerr = -ENOMEM;\r\ngoto err_dma_pool;\r\n}\r\nINIT_LIST_HEAD(&dma->common.channels);\r\ndma->pci_id = pdev->device;\r\nif (dma->pimr_mask) {\r\ndma->mask_reg = ioremap(LNW_PERIPHRAL_MASK_BASE,\r\nLNW_PERIPHRAL_MASK_SIZE);\r\nif (dma->mask_reg == NULL) {\r\npr_err("ERR_MDMA:Can't map periphral intr space !!\n");\r\nerr = -ENOMEM;\r\ngoto err_ioremap;\r\n}\r\n} else\r\ndma->mask_reg = NULL;\r\npr_debug("MDMA:Adding %d channel for this controller\n", dma->max_chan);\r\ndma->intr_mask = 0;\r\ndma->state = RUNNING;\r\nfor (i = 0; i < dma->max_chan; i++) {\r\nstruct intel_mid_dma_chan *midch = &dma->ch[i];\r\nmidch->chan.device = &dma->common;\r\ndma_cookie_init(&midch->chan);\r\nmidch->ch_id = dma->chan_base + i;\r\npr_debug("MDMA:Init CH %d, ID %d\n", i, midch->ch_id);\r\nmidch->dma_base = dma->dma_base;\r\nmidch->ch_regs = dma->dma_base + DMA_CH_SIZE * midch->ch_id;\r\nmidch->dma = dma;\r\ndma->intr_mask |= 1 << (dma->chan_base + i);\r\nspin_lock_init(&midch->lock);\r\nINIT_LIST_HEAD(&midch->active_list);\r\nINIT_LIST_HEAD(&midch->queue);\r\nINIT_LIST_HEAD(&midch->free_list);\r\niowrite32(MASK_INTR_REG(midch->ch_id),\r\ndma->dma_base + MASK_BLOCK);\r\niowrite32(MASK_INTR_REG(midch->ch_id),\r\ndma->dma_base + MASK_SRC_TRAN);\r\niowrite32(MASK_INTR_REG(midch->ch_id),\r\ndma->dma_base + MASK_DST_TRAN);\r\niowrite32(MASK_INTR_REG(midch->ch_id),\r\ndma->dma_base + MASK_ERR);\r\niowrite32(MASK_INTR_REG(midch->ch_id),\r\ndma->dma_base + MASK_TFR);\r\ndisable_dma_interrupt(midch);\r\nlist_add_tail(&midch->chan.device_node, &dma->common.channels);\r\n}\r\npr_debug("MDMA: Calc Mask as %x for this controller\n", dma->intr_mask);\r\ndma_cap_zero(dma->common.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, dma->common.cap_mask);\r\ndma_cap_set(DMA_SLAVE, dma->common.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, dma->common.cap_mask);\r\ndma->common.dev = &pdev->dev;\r\ndma->common.device_alloc_chan_resources =\r\nintel_mid_dma_alloc_chan_resources;\r\ndma->common.device_free_chan_resources =\r\nintel_mid_dma_free_chan_resources;\r\ndma->common.device_tx_status = intel_mid_dma_tx_status;\r\ndma->common.device_prep_dma_memcpy = intel_mid_dma_prep_memcpy;\r\ndma->common.device_issue_pending = intel_mid_dma_issue_pending;\r\ndma->common.device_prep_slave_sg = intel_mid_dma_prep_slave_sg;\r\ndma->common.device_control = intel_mid_dma_device_control;\r\niowrite32(REG_BIT0, dma->dma_base + DMA_CFG);\r\nif (dma->pimr_mask) {\r\npr_debug("MDMA:Requesting irq shared for DMAC1\n");\r\nerr = request_irq(pdev->irq, intel_mid_dma_interrupt1,\r\nIRQF_SHARED, "INTEL_MID_DMAC1", dma);\r\nif (0 != err)\r\ngoto err_irq;\r\n} else {\r\ndma->intr_mask = 0x03;\r\npr_debug("MDMA:Requesting irq for DMAC2\n");\r\nerr = request_irq(pdev->irq, intel_mid_dma_interrupt2,\r\nIRQF_SHARED, "INTEL_MID_DMAC2", dma);\r\nif (0 != err)\r\ngoto err_irq;\r\n}\r\nerr = dma_async_device_register(&dma->common);\r\nif (0 != err) {\r\npr_err("ERR_MDMA:device_register failed: %d\n", err);\r\ngoto err_engine;\r\n}\r\nif (dma->pimr_mask) {\r\npr_debug("setting up tasklet1 for DMAC1\n");\r\ntasklet_init(&dma->tasklet, dma_tasklet1, (unsigned long)dma);\r\n} else {\r\npr_debug("setting up tasklet2 for DMAC2\n");\r\ntasklet_init(&dma->tasklet, dma_tasklet2, (unsigned long)dma);\r\n}\r\nreturn 0;\r\nerr_engine:\r\nfree_irq(pdev->irq, dma);\r\nerr_irq:\r\nif (dma->mask_reg)\r\niounmap(dma->mask_reg);\r\nerr_ioremap:\r\npci_pool_destroy(dma->dma_pool);\r\nerr_dma_pool:\r\npr_err("ERR_MDMA:setup_dma failed: %d\n", err);\r\nreturn err;\r\n}\r\nstatic void middma_shutdown(struct pci_dev *pdev)\r\n{\r\nstruct middma_device *device = pci_get_drvdata(pdev);\r\ndma_async_device_unregister(&device->common);\r\npci_pool_destroy(device->dma_pool);\r\nif (device->mask_reg)\r\niounmap(device->mask_reg);\r\nif (device->dma_base)\r\niounmap(device->dma_base);\r\nfree_irq(pdev->irq, device);\r\nreturn;\r\n}\r\nstatic int __devinit intel_mid_dma_probe(struct pci_dev *pdev,\r\nconst struct pci_device_id *id)\r\n{\r\nstruct middma_device *device;\r\nu32 base_addr, bar_size;\r\nstruct intel_mid_dma_probe_info *info;\r\nint err;\r\npr_debug("MDMA: probe for %x\n", pdev->device);\r\ninfo = (void *)id->driver_data;\r\npr_debug("MDMA: CH %d, base %d, block len %d, Periphral mask %x\n",\r\ninfo->max_chan, info->ch_base,\r\ninfo->block_size, info->pimr_mask);\r\nerr = pci_enable_device(pdev);\r\nif (err)\r\ngoto err_enable_device;\r\nerr = pci_request_regions(pdev, "intel_mid_dmac");\r\nif (err)\r\ngoto err_request_regions;\r\nerr = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (err)\r\ngoto err_set_dma_mask;\r\nerr = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (err)\r\ngoto err_set_dma_mask;\r\ndevice = kzalloc(sizeof(*device), GFP_KERNEL);\r\nif (!device) {\r\npr_err("ERR_MDMA:kzalloc failed probe\n");\r\nerr = -ENOMEM;\r\ngoto err_kzalloc;\r\n}\r\ndevice->pdev = pci_dev_get(pdev);\r\nbase_addr = pci_resource_start(pdev, 0);\r\nbar_size = pci_resource_len(pdev, 0);\r\ndevice->dma_base = ioremap_nocache(base_addr, DMA_REG_SIZE);\r\nif (!device->dma_base) {\r\npr_err("ERR_MDMA:ioremap failed\n");\r\nerr = -ENOMEM;\r\ngoto err_ioremap;\r\n}\r\npci_set_drvdata(pdev, device);\r\npci_set_master(pdev);\r\ndevice->max_chan = info->max_chan;\r\ndevice->chan_base = info->ch_base;\r\ndevice->block_size = info->block_size;\r\ndevice->pimr_mask = info->pimr_mask;\r\nerr = mid_setup_dma(pdev);\r\nif (err)\r\ngoto err_dma;\r\npm_runtime_put_noidle(&pdev->dev);\r\npm_runtime_allow(&pdev->dev);\r\nreturn 0;\r\nerr_dma:\r\niounmap(device->dma_base);\r\nerr_ioremap:\r\npci_dev_put(pdev);\r\nkfree(device);\r\nerr_kzalloc:\r\nerr_set_dma_mask:\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\nerr_request_regions:\r\nerr_enable_device:\r\npr_err("ERR_MDMA:Probe failed %d\n", err);\r\nreturn err;\r\n}\r\nstatic void __devexit intel_mid_dma_remove(struct pci_dev *pdev)\r\n{\r\nstruct middma_device *device = pci_get_drvdata(pdev);\r\npm_runtime_get_noresume(&pdev->dev);\r\npm_runtime_forbid(&pdev->dev);\r\nmiddma_shutdown(pdev);\r\npci_dev_put(pdev);\r\nkfree(device);\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\n}\r\nstatic int dma_suspend(struct device *dev)\r\n{\r\nstruct pci_dev *pci = to_pci_dev(dev);\r\nint i;\r\nstruct middma_device *device = pci_get_drvdata(pci);\r\npr_debug("MDMA: dma_suspend called\n");\r\nfor (i = 0; i < device->max_chan; i++) {\r\nif (device->ch[i].in_use)\r\nreturn -EAGAIN;\r\n}\r\ndmac1_mask_periphral_intr(device);\r\ndevice->state = SUSPENDED;\r\npci_save_state(pci);\r\npci_disable_device(pci);\r\npci_set_power_state(pci, PCI_D3hot);\r\nreturn 0;\r\n}\r\nint dma_resume(struct device *dev)\r\n{\r\nstruct pci_dev *pci = to_pci_dev(dev);\r\nint ret;\r\nstruct middma_device *device = pci_get_drvdata(pci);\r\npr_debug("MDMA: dma_resume called\n");\r\npci_set_power_state(pci, PCI_D0);\r\npci_restore_state(pci);\r\nret = pci_enable_device(pci);\r\nif (ret) {\r\npr_err("MDMA: device can't be enabled for %x\n", pci->device);\r\nreturn ret;\r\n}\r\ndevice->state = RUNNING;\r\niowrite32(REG_BIT0, device->dma_base + DMA_CFG);\r\nreturn 0;\r\n}\r\nstatic int dma_runtime_suspend(struct device *dev)\r\n{\r\nstruct pci_dev *pci_dev = to_pci_dev(dev);\r\nstruct middma_device *device = pci_get_drvdata(pci_dev);\r\ndevice->state = SUSPENDED;\r\nreturn 0;\r\n}\r\nstatic int dma_runtime_resume(struct device *dev)\r\n{\r\nstruct pci_dev *pci_dev = to_pci_dev(dev);\r\nstruct middma_device *device = pci_get_drvdata(pci_dev);\r\ndevice->state = RUNNING;\r\niowrite32(REG_BIT0, device->dma_base + DMA_CFG);\r\nreturn 0;\r\n}\r\nstatic int dma_runtime_idle(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nstruct middma_device *device = pci_get_drvdata(pdev);\r\nint i;\r\nfor (i = 0; i < device->max_chan; i++) {\r\nif (device->ch[i].in_use)\r\nreturn -EAGAIN;\r\n}\r\nreturn pm_schedule_suspend(dev, 0);\r\n}\r\nstatic int __init intel_mid_dma_init(void)\r\n{\r\npr_debug("INFO_MDMA: LNW DMA Driver Version %s\n",\r\nINTEL_MID_DMA_DRIVER_VERSION);\r\nreturn pci_register_driver(&intel_mid_dma_pci_driver);\r\n}\r\nstatic void __exit intel_mid_dma_exit(void)\r\n{\r\npci_unregister_driver(&intel_mid_dma_pci_driver);\r\n}
