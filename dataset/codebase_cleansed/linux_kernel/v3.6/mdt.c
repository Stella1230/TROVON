static int\r\nnilfs_mdt_insert_new_block(struct inode *inode, unsigned long block,\r\nstruct buffer_head *bh,\r\nvoid (*init_block)(struct inode *,\r\nstruct buffer_head *, void *))\r\n{\r\nstruct nilfs_inode_info *ii = NILFS_I(inode);\r\nvoid *kaddr;\r\nint ret;\r\nbh->b_blocknr = 0;\r\nret = nilfs_bmap_insert(ii->i_bmap, block, (unsigned long)bh);\r\nif (unlikely(ret))\r\nreturn ret;\r\nset_buffer_mapped(bh);\r\nkaddr = kmap_atomic(bh->b_page);\r\nmemset(kaddr + bh_offset(bh), 0, 1 << inode->i_blkbits);\r\nif (init_block)\r\ninit_block(inode, bh, kaddr);\r\nflush_dcache_page(bh->b_page);\r\nkunmap_atomic(kaddr);\r\nset_buffer_uptodate(bh);\r\nmark_buffer_dirty(bh);\r\nnilfs_mdt_mark_dirty(inode);\r\nreturn 0;\r\n}\r\nstatic int nilfs_mdt_create_block(struct inode *inode, unsigned long block,\r\nstruct buffer_head **out_bh,\r\nvoid (*init_block)(struct inode *,\r\nstruct buffer_head *,\r\nvoid *))\r\n{\r\nstruct super_block *sb = inode->i_sb;\r\nstruct nilfs_transaction_info ti;\r\nstruct buffer_head *bh;\r\nint err;\r\nnilfs_transaction_begin(sb, &ti, 0);\r\nerr = -ENOMEM;\r\nbh = nilfs_grab_buffer(inode, inode->i_mapping, block, 0);\r\nif (unlikely(!bh))\r\ngoto failed_unlock;\r\nerr = -EEXIST;\r\nif (buffer_uptodate(bh))\r\ngoto failed_bh;\r\nwait_on_buffer(bh);\r\nif (buffer_uptodate(bh))\r\ngoto failed_bh;\r\nbh->b_bdev = sb->s_bdev;\r\nerr = nilfs_mdt_insert_new_block(inode, block, bh, init_block);\r\nif (likely(!err)) {\r\nget_bh(bh);\r\n*out_bh = bh;\r\n}\r\nfailed_bh:\r\nunlock_page(bh->b_page);\r\npage_cache_release(bh->b_page);\r\nbrelse(bh);\r\nfailed_unlock:\r\nif (likely(!err))\r\nerr = nilfs_transaction_commit(sb);\r\nelse\r\nnilfs_transaction_abort(sb);\r\nreturn err;\r\n}\r\nstatic int\r\nnilfs_mdt_submit_block(struct inode *inode, unsigned long blkoff,\r\nint mode, struct buffer_head **out_bh)\r\n{\r\nstruct buffer_head *bh;\r\n__u64 blknum = 0;\r\nint ret = -ENOMEM;\r\nbh = nilfs_grab_buffer(inode, inode->i_mapping, blkoff, 0);\r\nif (unlikely(!bh))\r\ngoto failed;\r\nret = -EEXIST;\r\nif (buffer_uptodate(bh))\r\ngoto out;\r\nif (mode == READA) {\r\nif (!trylock_buffer(bh)) {\r\nret = -EBUSY;\r\ngoto failed_bh;\r\n}\r\n} else\r\nlock_buffer(bh);\r\nif (buffer_uptodate(bh)) {\r\nunlock_buffer(bh);\r\ngoto out;\r\n}\r\nret = nilfs_bmap_lookup(NILFS_I(inode)->i_bmap, blkoff, &blknum);\r\nif (unlikely(ret)) {\r\nunlock_buffer(bh);\r\ngoto failed_bh;\r\n}\r\nmap_bh(bh, inode->i_sb, (sector_t)blknum);\r\nbh->b_end_io = end_buffer_read_sync;\r\nget_bh(bh);\r\nsubmit_bh(mode, bh);\r\nret = 0;\r\nout:\r\nget_bh(bh);\r\n*out_bh = bh;\r\nfailed_bh:\r\nunlock_page(bh->b_page);\r\npage_cache_release(bh->b_page);\r\nbrelse(bh);\r\nfailed:\r\nreturn ret;\r\n}\r\nstatic int nilfs_mdt_read_block(struct inode *inode, unsigned long block,\r\nint readahead, struct buffer_head **out_bh)\r\n{\r\nstruct buffer_head *first_bh, *bh;\r\nunsigned long blkoff;\r\nint i, nr_ra_blocks = NILFS_MDT_MAX_RA_BLOCKS;\r\nint err;\r\nerr = nilfs_mdt_submit_block(inode, block, READ, &first_bh);\r\nif (err == -EEXIST)\r\ngoto out;\r\nif (unlikely(err))\r\ngoto failed;\r\nif (readahead) {\r\nblkoff = block + 1;\r\nfor (i = 0; i < nr_ra_blocks; i++, blkoff++) {\r\nerr = nilfs_mdt_submit_block(inode, blkoff, READA, &bh);\r\nif (likely(!err || err == -EEXIST))\r\nbrelse(bh);\r\nelse if (err != -EBUSY)\r\nbreak;\r\nif (!buffer_locked(first_bh))\r\ngoto out_no_wait;\r\n}\r\n}\r\nwait_on_buffer(first_bh);\r\nout_no_wait:\r\nerr = -EIO;\r\nif (!buffer_uptodate(first_bh))\r\ngoto failed_bh;\r\nout:\r\n*out_bh = first_bh;\r\nreturn 0;\r\nfailed_bh:\r\nbrelse(first_bh);\r\nfailed:\r\nreturn err;\r\n}\r\nint nilfs_mdt_get_block(struct inode *inode, unsigned long blkoff, int create,\r\nvoid (*init_block)(struct inode *,\r\nstruct buffer_head *, void *),\r\nstruct buffer_head **out_bh)\r\n{\r\nint ret;\r\nretry:\r\nret = nilfs_mdt_read_block(inode, blkoff, !create, out_bh);\r\nif (!create || ret != -ENOENT)\r\nreturn ret;\r\nret = nilfs_mdt_create_block(inode, blkoff, out_bh, init_block);\r\nif (unlikely(ret == -EEXIST)) {\r\ngoto retry;\r\n}\r\nreturn ret;\r\n}\r\nint nilfs_mdt_delete_block(struct inode *inode, unsigned long block)\r\n{\r\nstruct nilfs_inode_info *ii = NILFS_I(inode);\r\nint err;\r\nerr = nilfs_bmap_delete(ii->i_bmap, block);\r\nif (!err || err == -ENOENT) {\r\nnilfs_mdt_mark_dirty(inode);\r\nnilfs_mdt_forget_block(inode, block);\r\n}\r\nreturn err;\r\n}\r\nint nilfs_mdt_forget_block(struct inode *inode, unsigned long block)\r\n{\r\npgoff_t index = (pgoff_t)block >>\r\n(PAGE_CACHE_SHIFT - inode->i_blkbits);\r\nstruct page *page;\r\nunsigned long first_block;\r\nint ret = 0;\r\nint still_dirty;\r\npage = find_lock_page(inode->i_mapping, index);\r\nif (!page)\r\nreturn -ENOENT;\r\nwait_on_page_writeback(page);\r\nfirst_block = (unsigned long)index <<\r\n(PAGE_CACHE_SHIFT - inode->i_blkbits);\r\nif (page_has_buffers(page)) {\r\nstruct buffer_head *bh;\r\nbh = nilfs_page_get_nth_block(page, block - first_block);\r\nnilfs_forget_buffer(bh);\r\n}\r\nstill_dirty = PageDirty(page);\r\nunlock_page(page);\r\npage_cache_release(page);\r\nif (still_dirty ||\r\ninvalidate_inode_pages2_range(inode->i_mapping, index, index) != 0)\r\nret = -EBUSY;\r\nreturn ret;\r\n}\r\nint nilfs_mdt_mark_block_dirty(struct inode *inode, unsigned long block)\r\n{\r\nstruct buffer_head *bh;\r\nint err;\r\nerr = nilfs_mdt_read_block(inode, block, 0, &bh);\r\nif (unlikely(err))\r\nreturn err;\r\nmark_buffer_dirty(bh);\r\nnilfs_mdt_mark_dirty(inode);\r\nbrelse(bh);\r\nreturn 0;\r\n}\r\nint nilfs_mdt_fetch_dirty(struct inode *inode)\r\n{\r\nstruct nilfs_inode_info *ii = NILFS_I(inode);\r\nif (nilfs_bmap_test_and_clear_dirty(ii->i_bmap)) {\r\nset_bit(NILFS_I_DIRTY, &ii->i_state);\r\nreturn 1;\r\n}\r\nreturn test_bit(NILFS_I_DIRTY, &ii->i_state);\r\n}\r\nstatic int\r\nnilfs_mdt_write_page(struct page *page, struct writeback_control *wbc)\r\n{\r\nstruct inode *inode;\r\nstruct super_block *sb;\r\nint err = 0;\r\nredirty_page_for_writepage(wbc, page);\r\nunlock_page(page);\r\ninode = page->mapping->host;\r\nif (!inode)\r\nreturn 0;\r\nsb = inode->i_sb;\r\nif (wbc->sync_mode == WB_SYNC_ALL)\r\nerr = nilfs_construct_segment(sb);\r\nelse if (wbc->for_reclaim)\r\nnilfs_flush_segment(sb, inode->i_ino);\r\nreturn err;\r\n}\r\nint nilfs_mdt_init(struct inode *inode, gfp_t gfp_mask, size_t objsz)\r\n{\r\nstruct nilfs_mdt_info *mi;\r\nmi = kzalloc(max(sizeof(*mi), objsz), GFP_NOFS);\r\nif (!mi)\r\nreturn -ENOMEM;\r\ninit_rwsem(&mi->mi_sem);\r\ninode->i_private = mi;\r\ninode->i_mode = S_IFREG;\r\nmapping_set_gfp_mask(inode->i_mapping, gfp_mask);\r\ninode->i_mapping->backing_dev_info = inode->i_sb->s_bdi;\r\ninode->i_op = &def_mdt_iops;\r\ninode->i_fop = &def_mdt_fops;\r\ninode->i_mapping->a_ops = &def_mdt_aops;\r\nreturn 0;\r\n}\r\nvoid nilfs_mdt_set_entry_size(struct inode *inode, unsigned entry_size,\r\nunsigned header_size)\r\n{\r\nstruct nilfs_mdt_info *mi = NILFS_MDT(inode);\r\nmi->mi_entry_size = entry_size;\r\nmi->mi_entries_per_block = (1 << inode->i_blkbits) / entry_size;\r\nmi->mi_first_entry_offset = DIV_ROUND_UP(header_size, entry_size);\r\n}\r\nint nilfs_mdt_setup_shadow_map(struct inode *inode,\r\nstruct nilfs_shadow_map *shadow)\r\n{\r\nstruct nilfs_mdt_info *mi = NILFS_MDT(inode);\r\nstruct backing_dev_info *bdi = inode->i_sb->s_bdi;\r\nINIT_LIST_HEAD(&shadow->frozen_buffers);\r\naddress_space_init_once(&shadow->frozen_data);\r\nnilfs_mapping_init(&shadow->frozen_data, inode, bdi);\r\naddress_space_init_once(&shadow->frozen_btnodes);\r\nnilfs_mapping_init(&shadow->frozen_btnodes, inode, bdi);\r\nmi->mi_shadow = shadow;\r\nreturn 0;\r\n}\r\nint nilfs_mdt_save_to_shadow_map(struct inode *inode)\r\n{\r\nstruct nilfs_mdt_info *mi = NILFS_MDT(inode);\r\nstruct nilfs_inode_info *ii = NILFS_I(inode);\r\nstruct nilfs_shadow_map *shadow = mi->mi_shadow;\r\nint ret;\r\nret = nilfs_copy_dirty_pages(&shadow->frozen_data, inode->i_mapping);\r\nif (ret)\r\ngoto out;\r\nret = nilfs_copy_dirty_pages(&shadow->frozen_btnodes,\r\n&ii->i_btnode_cache);\r\nif (ret)\r\ngoto out;\r\nnilfs_bmap_save(ii->i_bmap, &shadow->bmap_store);\r\nout:\r\nreturn ret;\r\n}\r\nint nilfs_mdt_freeze_buffer(struct inode *inode, struct buffer_head *bh)\r\n{\r\nstruct nilfs_shadow_map *shadow = NILFS_MDT(inode)->mi_shadow;\r\nstruct buffer_head *bh_frozen;\r\nstruct page *page;\r\nint blkbits = inode->i_blkbits;\r\npage = grab_cache_page(&shadow->frozen_data, bh->b_page->index);\r\nif (!page)\r\nreturn -ENOMEM;\r\nif (!page_has_buffers(page))\r\ncreate_empty_buffers(page, 1 << blkbits, 0);\r\nbh_frozen = nilfs_page_get_nth_block(page, bh_offset(bh) >> blkbits);\r\nif (!buffer_uptodate(bh_frozen))\r\nnilfs_copy_buffer(bh_frozen, bh);\r\nif (list_empty(&bh_frozen->b_assoc_buffers)) {\r\nlist_add_tail(&bh_frozen->b_assoc_buffers,\r\n&shadow->frozen_buffers);\r\nset_buffer_nilfs_redirected(bh);\r\n} else {\r\nbrelse(bh_frozen);\r\n}\r\nunlock_page(page);\r\npage_cache_release(page);\r\nreturn 0;\r\n}\r\nstruct buffer_head *\r\nnilfs_mdt_get_frozen_buffer(struct inode *inode, struct buffer_head *bh)\r\n{\r\nstruct nilfs_shadow_map *shadow = NILFS_MDT(inode)->mi_shadow;\r\nstruct buffer_head *bh_frozen = NULL;\r\nstruct page *page;\r\nint n;\r\npage = find_lock_page(&shadow->frozen_data, bh->b_page->index);\r\nif (page) {\r\nif (page_has_buffers(page)) {\r\nn = bh_offset(bh) >> inode->i_blkbits;\r\nbh_frozen = nilfs_page_get_nth_block(page, n);\r\n}\r\nunlock_page(page);\r\npage_cache_release(page);\r\n}\r\nreturn bh_frozen;\r\n}\r\nstatic void nilfs_release_frozen_buffers(struct nilfs_shadow_map *shadow)\r\n{\r\nstruct list_head *head = &shadow->frozen_buffers;\r\nstruct buffer_head *bh;\r\nwhile (!list_empty(head)) {\r\nbh = list_first_entry(head, struct buffer_head,\r\nb_assoc_buffers);\r\nlist_del_init(&bh->b_assoc_buffers);\r\nbrelse(bh);\r\n}\r\n}\r\nvoid nilfs_mdt_restore_from_shadow_map(struct inode *inode)\r\n{\r\nstruct nilfs_mdt_info *mi = NILFS_MDT(inode);\r\nstruct nilfs_inode_info *ii = NILFS_I(inode);\r\nstruct nilfs_shadow_map *shadow = mi->mi_shadow;\r\ndown_write(&mi->mi_sem);\r\nif (mi->mi_palloc_cache)\r\nnilfs_palloc_clear_cache(inode);\r\nnilfs_clear_dirty_pages(inode->i_mapping);\r\nnilfs_copy_back_pages(inode->i_mapping, &shadow->frozen_data);\r\nnilfs_clear_dirty_pages(&ii->i_btnode_cache);\r\nnilfs_copy_back_pages(&ii->i_btnode_cache, &shadow->frozen_btnodes);\r\nnilfs_bmap_restore(ii->i_bmap, &shadow->bmap_store);\r\nup_write(&mi->mi_sem);\r\n}\r\nvoid nilfs_mdt_clear_shadow_map(struct inode *inode)\r\n{\r\nstruct nilfs_mdt_info *mi = NILFS_MDT(inode);\r\nstruct nilfs_shadow_map *shadow = mi->mi_shadow;\r\ndown_write(&mi->mi_sem);\r\nnilfs_release_frozen_buffers(shadow);\r\ntruncate_inode_pages(&shadow->frozen_data, 0);\r\ntruncate_inode_pages(&shadow->frozen_btnodes, 0);\r\nup_write(&mi->mi_sem);\r\n}
