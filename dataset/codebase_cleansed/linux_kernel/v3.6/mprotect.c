static inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)\r\n{\r\nreturn newprot;\r\n}\r\nstatic void change_pte_range(struct mm_struct *mm, pmd_t *pmd,\r\nunsigned long addr, unsigned long end, pgprot_t newprot,\r\nint dirty_accountable)\r\n{\r\npte_t *pte, oldpte;\r\nspinlock_t *ptl;\r\npte = pte_offset_map_lock(mm, pmd, addr, &ptl);\r\narch_enter_lazy_mmu_mode();\r\ndo {\r\noldpte = *pte;\r\nif (pte_present(oldpte)) {\r\npte_t ptent;\r\nptent = ptep_modify_prot_start(mm, addr, pte);\r\nptent = pte_modify(ptent, newprot);\r\nif (dirty_accountable && pte_dirty(ptent))\r\nptent = pte_mkwrite(ptent);\r\nptep_modify_prot_commit(mm, addr, pte, ptent);\r\n} else if (IS_ENABLED(CONFIG_MIGRATION) && !pte_file(oldpte)) {\r\nswp_entry_t entry = pte_to_swp_entry(oldpte);\r\nif (is_write_migration_entry(entry)) {\r\nmake_migration_entry_read(&entry);\r\nset_pte_at(mm, addr, pte,\r\nswp_entry_to_pte(entry));\r\n}\r\n}\r\n} while (pte++, addr += PAGE_SIZE, addr != end);\r\narch_leave_lazy_mmu_mode();\r\npte_unmap_unlock(pte - 1, ptl);\r\n}\r\nstatic inline void change_pmd_range(struct vm_area_struct *vma, pud_t *pud,\r\nunsigned long addr, unsigned long end, pgprot_t newprot,\r\nint dirty_accountable)\r\n{\r\npmd_t *pmd;\r\nunsigned long next;\r\npmd = pmd_offset(pud, addr);\r\ndo {\r\nnext = pmd_addr_end(addr, end);\r\nif (pmd_trans_huge(*pmd)) {\r\nif (next - addr != HPAGE_PMD_SIZE)\r\nsplit_huge_page_pmd(vma->vm_mm, pmd);\r\nelse if (change_huge_pmd(vma, pmd, addr, newprot))\r\ncontinue;\r\n}\r\nif (pmd_none_or_clear_bad(pmd))\r\ncontinue;\r\nchange_pte_range(vma->vm_mm, pmd, addr, next, newprot,\r\ndirty_accountable);\r\n} while (pmd++, addr = next, addr != end);\r\n}\r\nstatic inline void change_pud_range(struct vm_area_struct *vma, pgd_t *pgd,\r\nunsigned long addr, unsigned long end, pgprot_t newprot,\r\nint dirty_accountable)\r\n{\r\npud_t *pud;\r\nunsigned long next;\r\npud = pud_offset(pgd, addr);\r\ndo {\r\nnext = pud_addr_end(addr, end);\r\nif (pud_none_or_clear_bad(pud))\r\ncontinue;\r\nchange_pmd_range(vma, pud, addr, next, newprot,\r\ndirty_accountable);\r\n} while (pud++, addr = next, addr != end);\r\n}\r\nstatic void change_protection(struct vm_area_struct *vma,\r\nunsigned long addr, unsigned long end, pgprot_t newprot,\r\nint dirty_accountable)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npgd_t *pgd;\r\nunsigned long next;\r\nunsigned long start = addr;\r\nBUG_ON(addr >= end);\r\npgd = pgd_offset(mm, addr);\r\nflush_cache_range(vma, addr, end);\r\ndo {\r\nnext = pgd_addr_end(addr, end);\r\nif (pgd_none_or_clear_bad(pgd))\r\ncontinue;\r\nchange_pud_range(vma, pgd, addr, next, newprot,\r\ndirty_accountable);\r\n} while (pgd++, addr = next, addr != end);\r\nflush_tlb_range(vma, start, end);\r\n}\r\nint\r\nmprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,\r\nunsigned long start, unsigned long end, unsigned long newflags)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nunsigned long oldflags = vma->vm_flags;\r\nlong nrpages = (end - start) >> PAGE_SHIFT;\r\nunsigned long charged = 0;\r\npgoff_t pgoff;\r\nint error;\r\nint dirty_accountable = 0;\r\nif (newflags == oldflags) {\r\n*pprev = vma;\r\nreturn 0;\r\n}\r\nif (newflags & VM_WRITE) {\r\nif (!(oldflags & (VM_ACCOUNT|VM_WRITE|VM_HUGETLB|\r\nVM_SHARED|VM_NORESERVE))) {\r\ncharged = nrpages;\r\nif (security_vm_enough_memory_mm(mm, charged))\r\nreturn -ENOMEM;\r\nnewflags |= VM_ACCOUNT;\r\n}\r\n}\r\npgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\r\n*pprev = vma_merge(mm, *pprev, start, end, newflags,\r\nvma->anon_vma, vma->vm_file, pgoff, vma_policy(vma));\r\nif (*pprev) {\r\nvma = *pprev;\r\ngoto success;\r\n}\r\n*pprev = vma;\r\nif (start != vma->vm_start) {\r\nerror = split_vma(mm, vma, start, 1);\r\nif (error)\r\ngoto fail;\r\n}\r\nif (end != vma->vm_end) {\r\nerror = split_vma(mm, vma, end, 0);\r\nif (error)\r\ngoto fail;\r\n}\r\nsuccess:\r\nvma->vm_flags = newflags;\r\nvma->vm_page_prot = pgprot_modify(vma->vm_page_prot,\r\nvm_get_page_prot(newflags));\r\nif (vma_wants_writenotify(vma)) {\r\nvma->vm_page_prot = vm_get_page_prot(newflags & ~VM_SHARED);\r\ndirty_accountable = 1;\r\n}\r\nmmu_notifier_invalidate_range_start(mm, start, end);\r\nif (is_vm_hugetlb_page(vma))\r\nhugetlb_change_protection(vma, start, end, vma->vm_page_prot);\r\nelse\r\nchange_protection(vma, start, end, vma->vm_page_prot, dirty_accountable);\r\nmmu_notifier_invalidate_range_end(mm, start, end);\r\nvm_stat_account(mm, oldflags, vma->vm_file, -nrpages);\r\nvm_stat_account(mm, newflags, vma->vm_file, nrpages);\r\nperf_event_mmap(vma);\r\nreturn 0;\r\nfail:\r\nvm_unacct_memory(charged);\r\nreturn error;\r\n}
