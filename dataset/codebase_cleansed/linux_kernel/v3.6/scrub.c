static void scrub_free_csums(struct scrub_dev *sdev)\r\n{\r\nwhile (!list_empty(&sdev->csum_list)) {\r\nstruct btrfs_ordered_sum *sum;\r\nsum = list_first_entry(&sdev->csum_list,\r\nstruct btrfs_ordered_sum, list);\r\nlist_del(&sum->list);\r\nkfree(sum);\r\n}\r\n}\r\nstatic noinline_for_stack void scrub_free_dev(struct scrub_dev *sdev)\r\n{\r\nint i;\r\nif (!sdev)\r\nreturn;\r\nif (sdev->curr != -1) {\r\nstruct scrub_bio *sbio = sdev->bios[sdev->curr];\r\nfor (i = 0; i < sbio->page_count; i++) {\r\nBUG_ON(!sbio->pagev[i]);\r\nBUG_ON(!sbio->pagev[i]->page);\r\nscrub_block_put(sbio->pagev[i]->sblock);\r\n}\r\nbio_put(sbio->bio);\r\n}\r\nfor (i = 0; i < SCRUB_BIOS_PER_DEV; ++i) {\r\nstruct scrub_bio *sbio = sdev->bios[i];\r\nif (!sbio)\r\nbreak;\r\nkfree(sbio);\r\n}\r\nscrub_free_csums(sdev);\r\nkfree(sdev);\r\n}\r\nint scrub_print_warning_inode(u64 inum, u64 offset, u64 root, void *ctx)\r\n{\r\nu64 isize;\r\nu32 nlink;\r\nint ret;\r\nint i;\r\nstruct extent_buffer *eb;\r\nstruct btrfs_inode_item *inode_item;\r\nstruct scrub_warning *swarn = ctx;\r\nstruct btrfs_fs_info *fs_info = swarn->dev->dev_root->fs_info;\r\nstruct inode_fs_paths *ipath = NULL;\r\nstruct btrfs_root *local_root;\r\nstruct btrfs_key root_key;\r\nroot_key.objectid = root;\r\nroot_key.type = BTRFS_ROOT_ITEM_KEY;\r\nroot_key.offset = (u64)-1;\r\nlocal_root = btrfs_read_fs_root_no_name(fs_info, &root_key);\r\nif (IS_ERR(local_root)) {\r\nret = PTR_ERR(local_root);\r\ngoto err;\r\n}\r\nret = inode_item_info(inum, 0, local_root, swarn->path);\r\nif (ret) {\r\nbtrfs_release_path(swarn->path);\r\ngoto err;\r\n}\r\neb = swarn->path->nodes[0];\r\ninode_item = btrfs_item_ptr(eb, swarn->path->slots[0],\r\nstruct btrfs_inode_item);\r\nisize = btrfs_inode_size(eb, inode_item);\r\nnlink = btrfs_inode_nlink(eb, inode_item);\r\nbtrfs_release_path(swarn->path);\r\nipath = init_ipath(4096, local_root, swarn->path);\r\nif (IS_ERR(ipath)) {\r\nret = PTR_ERR(ipath);\r\nipath = NULL;\r\ngoto err;\r\n}\r\nret = paths_from_inode(inum, ipath);\r\nif (ret < 0)\r\ngoto err;\r\nfor (i = 0; i < ipath->fspath->elem_cnt; ++i)\r\nprintk_in_rcu(KERN_WARNING "btrfs: %s at logical %llu on dev "\r\n"%s, sector %llu, root %llu, inode %llu, offset %llu, "\r\n"length %llu, links %u (path: %s)\n", swarn->errstr,\r\nswarn->logical, rcu_str_deref(swarn->dev->name),\r\n(unsigned long long)swarn->sector, root, inum, offset,\r\nmin(isize - offset, (u64)PAGE_SIZE), nlink,\r\n(char *)(unsigned long)ipath->fspath->val[i]);\r\nfree_ipath(ipath);\r\nreturn 0;\r\nerr:\r\nprintk_in_rcu(KERN_WARNING "btrfs: %s at logical %llu on dev "\r\n"%s, sector %llu, root %llu, inode %llu, offset %llu: path "\r\n"resolving failed with ret=%d\n", swarn->errstr,\r\nswarn->logical, rcu_str_deref(swarn->dev->name),\r\n(unsigned long long)swarn->sector, root, inum, offset, ret);\r\nfree_ipath(ipath);\r\nreturn 0;\r\n}\r\nstatic void scrub_print_warning(const char *errstr, struct scrub_block *sblock)\r\n{\r\nstruct btrfs_device *dev = sblock->sdev->dev;\r\nstruct btrfs_fs_info *fs_info = dev->dev_root->fs_info;\r\nstruct btrfs_path *path;\r\nstruct btrfs_key found_key;\r\nstruct extent_buffer *eb;\r\nstruct btrfs_extent_item *ei;\r\nstruct scrub_warning swarn;\r\nu32 item_size;\r\nint ret;\r\nu64 ref_root;\r\nu8 ref_level;\r\nunsigned long ptr = 0;\r\nconst int bufsize = 4096;\r\nu64 extent_item_pos;\r\npath = btrfs_alloc_path();\r\nswarn.scratch_buf = kmalloc(bufsize, GFP_NOFS);\r\nswarn.msg_buf = kmalloc(bufsize, GFP_NOFS);\r\nBUG_ON(sblock->page_count < 1);\r\nswarn.sector = (sblock->pagev[0].physical) >> 9;\r\nswarn.logical = sblock->pagev[0].logical;\r\nswarn.errstr = errstr;\r\nswarn.dev = dev;\r\nswarn.msg_bufsize = bufsize;\r\nswarn.scratch_bufsize = bufsize;\r\nif (!path || !swarn.scratch_buf || !swarn.msg_buf)\r\ngoto out;\r\nret = extent_from_logical(fs_info, swarn.logical, path, &found_key);\r\nif (ret < 0)\r\ngoto out;\r\nextent_item_pos = swarn.logical - found_key.objectid;\r\nswarn.extent_item_size = found_key.offset;\r\neb = path->nodes[0];\r\nei = btrfs_item_ptr(eb, path->slots[0], struct btrfs_extent_item);\r\nitem_size = btrfs_item_size_nr(eb, path->slots[0]);\r\nbtrfs_release_path(path);\r\nif (ret & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\r\ndo {\r\nret = tree_backref_for_extent(&ptr, eb, ei, item_size,\r\n&ref_root, &ref_level);\r\nprintk_in_rcu(KERN_WARNING\r\n"btrfs: %s at logical %llu on dev %s, "\r\n"sector %llu: metadata %s (level %d) in tree "\r\n"%llu\n", errstr, swarn.logical,\r\nrcu_str_deref(dev->name),\r\n(unsigned long long)swarn.sector,\r\nref_level ? "node" : "leaf",\r\nret < 0 ? -1 : ref_level,\r\nret < 0 ? -1 : ref_root);\r\n} while (ret != 1);\r\n} else {\r\nswarn.path = path;\r\niterate_extent_inodes(fs_info, found_key.objectid,\r\nextent_item_pos, 1,\r\nscrub_print_warning_inode, &swarn);\r\n}\r\nout:\r\nbtrfs_free_path(path);\r\nkfree(swarn.scratch_buf);\r\nkfree(swarn.msg_buf);\r\n}\r\nstatic int scrub_fixup_readpage(u64 inum, u64 offset, u64 root, void *ctx)\r\n{\r\nstruct page *page = NULL;\r\nunsigned long index;\r\nstruct scrub_fixup_nodatasum *fixup = ctx;\r\nint ret;\r\nint corrected = 0;\r\nstruct btrfs_key key;\r\nstruct inode *inode = NULL;\r\nu64 end = offset + PAGE_SIZE - 1;\r\nstruct btrfs_root *local_root;\r\nkey.objectid = root;\r\nkey.type = BTRFS_ROOT_ITEM_KEY;\r\nkey.offset = (u64)-1;\r\nlocal_root = btrfs_read_fs_root_no_name(fixup->root->fs_info, &key);\r\nif (IS_ERR(local_root))\r\nreturn PTR_ERR(local_root);\r\nkey.type = BTRFS_INODE_ITEM_KEY;\r\nkey.objectid = inum;\r\nkey.offset = 0;\r\ninode = btrfs_iget(fixup->root->fs_info->sb, &key, local_root, NULL);\r\nif (IS_ERR(inode))\r\nreturn PTR_ERR(inode);\r\nindex = offset >> PAGE_CACHE_SHIFT;\r\npage = find_or_create_page(inode->i_mapping, index, GFP_NOFS);\r\nif (!page) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (PageUptodate(page)) {\r\nstruct btrfs_mapping_tree *map_tree;\r\nif (PageDirty(page)) {\r\nret = -EIO;\r\ngoto out;\r\n}\r\nmap_tree = &BTRFS_I(inode)->root->fs_info->mapping_tree;\r\nret = repair_io_failure(map_tree, offset, PAGE_SIZE,\r\nfixup->logical, page,\r\nfixup->mirror_num);\r\nunlock_page(page);\r\ncorrected = !ret;\r\n} else {\r\nret = set_extent_bits(&BTRFS_I(inode)->io_tree, offset, end,\r\nEXTENT_DAMAGED, GFP_NOFS);\r\nif (ret) {\r\nWARN_ON(ret > 0);\r\nif (ret > 0)\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\nret = extent_read_full_page(&BTRFS_I(inode)->io_tree, page,\r\nbtrfs_get_extent,\r\nfixup->mirror_num);\r\nwait_on_page_locked(page);\r\ncorrected = !test_range_bit(&BTRFS_I(inode)->io_tree, offset,\r\nend, EXTENT_DAMAGED, 0, NULL);\r\nif (!corrected)\r\nclear_extent_bits(&BTRFS_I(inode)->io_tree, offset, end,\r\nEXTENT_DAMAGED, GFP_NOFS);\r\n}\r\nout:\r\nif (page)\r\nput_page(page);\r\nif (inode)\r\niput(inode);\r\nif (ret < 0)\r\nreturn ret;\r\nif (ret == 0 && corrected) {\r\nreturn 1;\r\n}\r\nreturn -EIO;\r\n}\r\nstatic void scrub_fixup_nodatasum(struct btrfs_work *work)\r\n{\r\nint ret;\r\nstruct scrub_fixup_nodatasum *fixup;\r\nstruct scrub_dev *sdev;\r\nstruct btrfs_trans_handle *trans = NULL;\r\nstruct btrfs_fs_info *fs_info;\r\nstruct btrfs_path *path;\r\nint uncorrectable = 0;\r\nfixup = container_of(work, struct scrub_fixup_nodatasum, work);\r\nsdev = fixup->sdev;\r\nfs_info = fixup->root->fs_info;\r\npath = btrfs_alloc_path();\r\nif (!path) {\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.malloc_errors;\r\nspin_unlock(&sdev->stat_lock);\r\nuncorrectable = 1;\r\ngoto out;\r\n}\r\ntrans = btrfs_join_transaction(fixup->root);\r\nif (IS_ERR(trans)) {\r\nuncorrectable = 1;\r\ngoto out;\r\n}\r\nret = iterate_inodes_from_logical(fixup->logical, fixup->root->fs_info,\r\npath, scrub_fixup_readpage,\r\nfixup);\r\nif (ret < 0) {\r\nuncorrectable = 1;\r\ngoto out;\r\n}\r\nWARN_ON(ret != 1);\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.corrected_errors;\r\nspin_unlock(&sdev->stat_lock);\r\nout:\r\nif (trans && !IS_ERR(trans))\r\nbtrfs_end_transaction(trans, fixup->root);\r\nif (uncorrectable) {\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.uncorrectable_errors;\r\nspin_unlock(&sdev->stat_lock);\r\nprintk_ratelimited_in_rcu(KERN_ERR\r\n"btrfs: unable to fixup (nodatasum) error at logical %llu on dev %s\n",\r\n(unsigned long long)fixup->logical,\r\nrcu_str_deref(sdev->dev->name));\r\n}\r\nbtrfs_free_path(path);\r\nkfree(fixup);\r\nmutex_lock(&fs_info->scrub_lock);\r\natomic_dec(&fs_info->scrubs_running);\r\natomic_dec(&fs_info->scrubs_paused);\r\nmutex_unlock(&fs_info->scrub_lock);\r\natomic_dec(&sdev->fixup_cnt);\r\nwake_up(&fs_info->scrub_pause_wait);\r\nwake_up(&sdev->list_wait);\r\n}\r\nstatic int scrub_handle_errored_block(struct scrub_block *sblock_to_check)\r\n{\r\nstruct scrub_dev *sdev = sblock_to_check->sdev;\r\nstruct btrfs_fs_info *fs_info;\r\nu64 length;\r\nu64 logical;\r\nu64 generation;\r\nunsigned int failed_mirror_index;\r\nunsigned int is_metadata;\r\nunsigned int have_csum;\r\nu8 *csum;\r\nstruct scrub_block *sblocks_for_recheck;\r\nstruct scrub_block *sblock_bad;\r\nint ret;\r\nint mirror_index;\r\nint page_num;\r\nint success;\r\nstatic DEFINE_RATELIMIT_STATE(_rs, DEFAULT_RATELIMIT_INTERVAL,\r\nDEFAULT_RATELIMIT_BURST);\r\nBUG_ON(sblock_to_check->page_count < 1);\r\nfs_info = sdev->dev->dev_root->fs_info;\r\nlength = sblock_to_check->page_count * PAGE_SIZE;\r\nlogical = sblock_to_check->pagev[0].logical;\r\ngeneration = sblock_to_check->pagev[0].generation;\r\nBUG_ON(sblock_to_check->pagev[0].mirror_num < 1);\r\nfailed_mirror_index = sblock_to_check->pagev[0].mirror_num - 1;\r\nis_metadata = !(sblock_to_check->pagev[0].flags &\r\nBTRFS_EXTENT_FLAG_DATA);\r\nhave_csum = sblock_to_check->pagev[0].have_csum;\r\ncsum = sblock_to_check->pagev[0].csum;\r\nsblocks_for_recheck = kzalloc(BTRFS_MAX_MIRRORS *\r\nsizeof(*sblocks_for_recheck),\r\nGFP_NOFS);\r\nif (!sblocks_for_recheck) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.malloc_errors++;\r\nsdev->stat.read_errors++;\r\nsdev->stat.uncorrectable_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nbtrfs_dev_stat_inc_and_print(sdev->dev,\r\nBTRFS_DEV_STAT_READ_ERRS);\r\ngoto out;\r\n}\r\nret = scrub_setup_recheck_block(sdev, &fs_info->mapping_tree, length,\r\nlogical, sblocks_for_recheck);\r\nif (ret) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.read_errors++;\r\nsdev->stat.uncorrectable_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nbtrfs_dev_stat_inc_and_print(sdev->dev,\r\nBTRFS_DEV_STAT_READ_ERRS);\r\ngoto out;\r\n}\r\nBUG_ON(failed_mirror_index >= BTRFS_MAX_MIRRORS);\r\nsblock_bad = sblocks_for_recheck + failed_mirror_index;\r\nret = scrub_recheck_block(fs_info, sblock_bad, is_metadata, have_csum,\r\ncsum, generation, sdev->csum_size);\r\nif (ret) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.read_errors++;\r\nsdev->stat.uncorrectable_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nbtrfs_dev_stat_inc_and_print(sdev->dev,\r\nBTRFS_DEV_STAT_READ_ERRS);\r\ngoto out;\r\n}\r\nif (!sblock_bad->header_error && !sblock_bad->checksum_error &&\r\nsblock_bad->no_io_error_seen) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.unverified_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\ngoto out;\r\n}\r\nif (!sblock_bad->no_io_error_seen) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.read_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nif (__ratelimit(&_rs))\r\nscrub_print_warning("i/o error", sblock_to_check);\r\nbtrfs_dev_stat_inc_and_print(sdev->dev,\r\nBTRFS_DEV_STAT_READ_ERRS);\r\n} else if (sblock_bad->checksum_error) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.csum_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nif (__ratelimit(&_rs))\r\nscrub_print_warning("checksum error", sblock_to_check);\r\nbtrfs_dev_stat_inc_and_print(sdev->dev,\r\nBTRFS_DEV_STAT_CORRUPTION_ERRS);\r\n} else if (sblock_bad->header_error) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.verify_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nif (__ratelimit(&_rs))\r\nscrub_print_warning("checksum/header error",\r\nsblock_to_check);\r\nif (sblock_bad->generation_error)\r\nbtrfs_dev_stat_inc_and_print(sdev->dev,\r\nBTRFS_DEV_STAT_GENERATION_ERRS);\r\nelse\r\nbtrfs_dev_stat_inc_and_print(sdev->dev,\r\nBTRFS_DEV_STAT_CORRUPTION_ERRS);\r\n}\r\nif (sdev->readonly)\r\ngoto did_not_correct_error;\r\nif (!is_metadata && !have_csum) {\r\nstruct scrub_fixup_nodatasum *fixup_nodatasum;\r\nfixup_nodatasum = kzalloc(sizeof(*fixup_nodatasum), GFP_NOFS);\r\nif (!fixup_nodatasum)\r\ngoto did_not_correct_error;\r\nfixup_nodatasum->sdev = sdev;\r\nfixup_nodatasum->logical = logical;\r\nfixup_nodatasum->root = fs_info->extent_root;\r\nfixup_nodatasum->mirror_num = failed_mirror_index + 1;\r\nmutex_lock(&fs_info->scrub_lock);\r\natomic_inc(&fs_info->scrubs_running);\r\natomic_inc(&fs_info->scrubs_paused);\r\nmutex_unlock(&fs_info->scrub_lock);\r\natomic_inc(&sdev->fixup_cnt);\r\nfixup_nodatasum->work.func = scrub_fixup_nodatasum;\r\nbtrfs_queue_worker(&fs_info->scrub_workers,\r\n&fixup_nodatasum->work);\r\ngoto out;\r\n}\r\nfor (mirror_index = 0;\r\nmirror_index < BTRFS_MAX_MIRRORS &&\r\nsblocks_for_recheck[mirror_index].page_count > 0;\r\nmirror_index++) {\r\nif (mirror_index == failed_mirror_index)\r\ncontinue;\r\nret = scrub_recheck_block(fs_info,\r\nsblocks_for_recheck + mirror_index,\r\nis_metadata, have_csum, csum,\r\ngeneration, sdev->csum_size);\r\nif (ret)\r\ngoto did_not_correct_error;\r\n}\r\nfor (mirror_index = 0;\r\nmirror_index < BTRFS_MAX_MIRRORS &&\r\nsblocks_for_recheck[mirror_index].page_count > 0;\r\nmirror_index++) {\r\nstruct scrub_block *sblock_other = sblocks_for_recheck +\r\nmirror_index;\r\nif (!sblock_other->header_error &&\r\n!sblock_other->checksum_error &&\r\nsblock_other->no_io_error_seen) {\r\nint force_write = is_metadata || have_csum;\r\nret = scrub_repair_block_from_good_copy(sblock_bad,\r\nsblock_other,\r\nforce_write);\r\nif (0 == ret)\r\ngoto corrected_error;\r\n}\r\n}\r\nif (sblock_bad->no_io_error_seen)\r\ngoto did_not_correct_error;\r\nsuccess = 1;\r\nfor (page_num = 0; page_num < sblock_bad->page_count; page_num++) {\r\nstruct scrub_page *page_bad = sblock_bad->pagev + page_num;\r\nif (!page_bad->io_error)\r\ncontinue;\r\nfor (mirror_index = 0;\r\nmirror_index < BTRFS_MAX_MIRRORS &&\r\nsblocks_for_recheck[mirror_index].page_count > 0;\r\nmirror_index++) {\r\nstruct scrub_block *sblock_other = sblocks_for_recheck +\r\nmirror_index;\r\nstruct scrub_page *page_other = sblock_other->pagev +\r\npage_num;\r\nif (!page_other->io_error) {\r\nret = scrub_repair_page_from_good_copy(\r\nsblock_bad, sblock_other, page_num, 0);\r\nif (0 == ret) {\r\npage_bad->io_error = 0;\r\nbreak;\r\n}\r\n}\r\n}\r\nif (page_bad->io_error) {\r\nsuccess = 0;\r\n}\r\n}\r\nif (success) {\r\nif (is_metadata || have_csum) {\r\nret = scrub_recheck_block(fs_info, sblock_bad,\r\nis_metadata, have_csum, csum,\r\ngeneration, sdev->csum_size);\r\nif (!ret && !sblock_bad->header_error &&\r\n!sblock_bad->checksum_error &&\r\nsblock_bad->no_io_error_seen)\r\ngoto corrected_error;\r\nelse\r\ngoto did_not_correct_error;\r\n} else {\r\ncorrected_error:\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.corrected_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nprintk_ratelimited_in_rcu(KERN_ERR\r\n"btrfs: fixed up error at logical %llu on dev %s\n",\r\n(unsigned long long)logical,\r\nrcu_str_deref(sdev->dev->name));\r\n}\r\n} else {\r\ndid_not_correct_error:\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.uncorrectable_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nprintk_ratelimited_in_rcu(KERN_ERR\r\n"btrfs: unable to fixup (regular) error at logical %llu on dev %s\n",\r\n(unsigned long long)logical,\r\nrcu_str_deref(sdev->dev->name));\r\n}\r\nout:\r\nif (sblocks_for_recheck) {\r\nfor (mirror_index = 0; mirror_index < BTRFS_MAX_MIRRORS;\r\nmirror_index++) {\r\nstruct scrub_block *sblock = sblocks_for_recheck +\r\nmirror_index;\r\nint page_index;\r\nfor (page_index = 0; page_index < SCRUB_PAGES_PER_BIO;\r\npage_index++)\r\nif (sblock->pagev[page_index].page)\r\n__free_page(\r\nsblock->pagev[page_index].page);\r\n}\r\nkfree(sblocks_for_recheck);\r\n}\r\nreturn 0;\r\n}\r\nstatic int scrub_setup_recheck_block(struct scrub_dev *sdev,\r\nstruct btrfs_mapping_tree *map_tree,\r\nu64 length, u64 logical,\r\nstruct scrub_block *sblocks_for_recheck)\r\n{\r\nint page_index;\r\nint mirror_index;\r\nint ret;\r\npage_index = 0;\r\nwhile (length > 0) {\r\nu64 sublen = min_t(u64, length, PAGE_SIZE);\r\nu64 mapped_length = sublen;\r\nstruct btrfs_bio *bbio = NULL;\r\nret = btrfs_map_block(map_tree, WRITE, logical, &mapped_length,\r\n&bbio, 0);\r\nif (ret || !bbio || mapped_length < sublen) {\r\nkfree(bbio);\r\nreturn -EIO;\r\n}\r\nBUG_ON(page_index >= SCRUB_PAGES_PER_BIO);\r\nfor (mirror_index = 0; mirror_index < (int)bbio->num_stripes;\r\nmirror_index++) {\r\nstruct scrub_block *sblock;\r\nstruct scrub_page *page;\r\nif (mirror_index >= BTRFS_MAX_MIRRORS)\r\ncontinue;\r\nsblock = sblocks_for_recheck + mirror_index;\r\npage = sblock->pagev + page_index;\r\npage->logical = logical;\r\npage->physical = bbio->stripes[mirror_index].physical;\r\npage->dev = bbio->stripes[mirror_index].dev;\r\npage->mirror_num = mirror_index + 1;\r\npage->page = alloc_page(GFP_NOFS);\r\nif (!page->page) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.malloc_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nreturn -ENOMEM;\r\n}\r\nsblock->page_count++;\r\n}\r\nkfree(bbio);\r\nlength -= sublen;\r\nlogical += sublen;\r\npage_index++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int scrub_recheck_block(struct btrfs_fs_info *fs_info,\r\nstruct scrub_block *sblock, int is_metadata,\r\nint have_csum, u8 *csum, u64 generation,\r\nu16 csum_size)\r\n{\r\nint page_num;\r\nsblock->no_io_error_seen = 1;\r\nsblock->header_error = 0;\r\nsblock->checksum_error = 0;\r\nfor (page_num = 0; page_num < sblock->page_count; page_num++) {\r\nstruct bio *bio;\r\nint ret;\r\nstruct scrub_page *page = sblock->pagev + page_num;\r\nDECLARE_COMPLETION_ONSTACK(complete);\r\nif (page->dev->bdev == NULL) {\r\npage->io_error = 1;\r\nsblock->no_io_error_seen = 0;\r\ncontinue;\r\n}\r\nBUG_ON(!page->page);\r\nbio = bio_alloc(GFP_NOFS, 1);\r\nif (!bio)\r\nreturn -EIO;\r\nbio->bi_bdev = page->dev->bdev;\r\nbio->bi_sector = page->physical >> 9;\r\nbio->bi_end_io = scrub_complete_bio_end_io;\r\nbio->bi_private = &complete;\r\nret = bio_add_page(bio, page->page, PAGE_SIZE, 0);\r\nif (PAGE_SIZE != ret) {\r\nbio_put(bio);\r\nreturn -EIO;\r\n}\r\nbtrfsic_submit_bio(READ, bio);\r\nwait_for_completion(&complete);\r\npage->io_error = !test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nif (!test_bit(BIO_UPTODATE, &bio->bi_flags))\r\nsblock->no_io_error_seen = 0;\r\nbio_put(bio);\r\n}\r\nif (sblock->no_io_error_seen)\r\nscrub_recheck_block_checksum(fs_info, sblock, is_metadata,\r\nhave_csum, csum, generation,\r\ncsum_size);\r\nreturn 0;\r\n}\r\nstatic void scrub_recheck_block_checksum(struct btrfs_fs_info *fs_info,\r\nstruct scrub_block *sblock,\r\nint is_metadata, int have_csum,\r\nconst u8 *csum, u64 generation,\r\nu16 csum_size)\r\n{\r\nint page_num;\r\nu8 calculated_csum[BTRFS_CSUM_SIZE];\r\nu32 crc = ~(u32)0;\r\nstruct btrfs_root *root = fs_info->extent_root;\r\nvoid *mapped_buffer;\r\nBUG_ON(!sblock->pagev[0].page);\r\nif (is_metadata) {\r\nstruct btrfs_header *h;\r\nmapped_buffer = kmap_atomic(sblock->pagev[0].page);\r\nh = (struct btrfs_header *)mapped_buffer;\r\nif (sblock->pagev[0].logical != le64_to_cpu(h->bytenr) ||\r\nmemcmp(h->fsid, fs_info->fsid, BTRFS_UUID_SIZE) ||\r\nmemcmp(h->chunk_tree_uuid, fs_info->chunk_tree_uuid,\r\nBTRFS_UUID_SIZE)) {\r\nsblock->header_error = 1;\r\n} else if (generation != le64_to_cpu(h->generation)) {\r\nsblock->header_error = 1;\r\nsblock->generation_error = 1;\r\n}\r\ncsum = h->csum;\r\n} else {\r\nif (!have_csum)\r\nreturn;\r\nmapped_buffer = kmap_atomic(sblock->pagev[0].page);\r\n}\r\nfor (page_num = 0;;) {\r\nif (page_num == 0 && is_metadata)\r\ncrc = btrfs_csum_data(root,\r\n((u8 *)mapped_buffer) + BTRFS_CSUM_SIZE,\r\ncrc, PAGE_SIZE - BTRFS_CSUM_SIZE);\r\nelse\r\ncrc = btrfs_csum_data(root, mapped_buffer, crc,\r\nPAGE_SIZE);\r\nkunmap_atomic(mapped_buffer);\r\npage_num++;\r\nif (page_num >= sblock->page_count)\r\nbreak;\r\nBUG_ON(!sblock->pagev[page_num].page);\r\nmapped_buffer = kmap_atomic(sblock->pagev[page_num].page);\r\n}\r\nbtrfs_csum_final(crc, calculated_csum);\r\nif (memcmp(calculated_csum, csum, csum_size))\r\nsblock->checksum_error = 1;\r\n}\r\nstatic void scrub_complete_bio_end_io(struct bio *bio, int err)\r\n{\r\ncomplete((struct completion *)bio->bi_private);\r\n}\r\nstatic int scrub_repair_block_from_good_copy(struct scrub_block *sblock_bad,\r\nstruct scrub_block *sblock_good,\r\nint force_write)\r\n{\r\nint page_num;\r\nint ret = 0;\r\nfor (page_num = 0; page_num < sblock_bad->page_count; page_num++) {\r\nint ret_sub;\r\nret_sub = scrub_repair_page_from_good_copy(sblock_bad,\r\nsblock_good,\r\npage_num,\r\nforce_write);\r\nif (ret_sub)\r\nret = ret_sub;\r\n}\r\nreturn ret;\r\n}\r\nstatic int scrub_repair_page_from_good_copy(struct scrub_block *sblock_bad,\r\nstruct scrub_block *sblock_good,\r\nint page_num, int force_write)\r\n{\r\nstruct scrub_page *page_bad = sblock_bad->pagev + page_num;\r\nstruct scrub_page *page_good = sblock_good->pagev + page_num;\r\nBUG_ON(sblock_bad->pagev[page_num].page == NULL);\r\nBUG_ON(sblock_good->pagev[page_num].page == NULL);\r\nif (force_write || sblock_bad->header_error ||\r\nsblock_bad->checksum_error || page_bad->io_error) {\r\nstruct bio *bio;\r\nint ret;\r\nDECLARE_COMPLETION_ONSTACK(complete);\r\nbio = bio_alloc(GFP_NOFS, 1);\r\nif (!bio)\r\nreturn -EIO;\r\nbio->bi_bdev = page_bad->dev->bdev;\r\nbio->bi_sector = page_bad->physical >> 9;\r\nbio->bi_end_io = scrub_complete_bio_end_io;\r\nbio->bi_private = &complete;\r\nret = bio_add_page(bio, page_good->page, PAGE_SIZE, 0);\r\nif (PAGE_SIZE != ret) {\r\nbio_put(bio);\r\nreturn -EIO;\r\n}\r\nbtrfsic_submit_bio(WRITE, bio);\r\nwait_for_completion(&complete);\r\nif (!bio_flagged(bio, BIO_UPTODATE)) {\r\nbtrfs_dev_stat_inc_and_print(page_bad->dev,\r\nBTRFS_DEV_STAT_WRITE_ERRS);\r\nbio_put(bio);\r\nreturn -EIO;\r\n}\r\nbio_put(bio);\r\n}\r\nreturn 0;\r\n}\r\nstatic void scrub_checksum(struct scrub_block *sblock)\r\n{\r\nu64 flags;\r\nint ret;\r\nBUG_ON(sblock->page_count < 1);\r\nflags = sblock->pagev[0].flags;\r\nret = 0;\r\nif (flags & BTRFS_EXTENT_FLAG_DATA)\r\nret = scrub_checksum_data(sblock);\r\nelse if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK)\r\nret = scrub_checksum_tree_block(sblock);\r\nelse if (flags & BTRFS_EXTENT_FLAG_SUPER)\r\n(void)scrub_checksum_super(sblock);\r\nelse\r\nWARN_ON(1);\r\nif (ret)\r\nscrub_handle_errored_block(sblock);\r\n}\r\nstatic int scrub_checksum_data(struct scrub_block *sblock)\r\n{\r\nstruct scrub_dev *sdev = sblock->sdev;\r\nu8 csum[BTRFS_CSUM_SIZE];\r\nu8 *on_disk_csum;\r\nstruct page *page;\r\nvoid *buffer;\r\nu32 crc = ~(u32)0;\r\nint fail = 0;\r\nstruct btrfs_root *root = sdev->dev->dev_root;\r\nu64 len;\r\nint index;\r\nBUG_ON(sblock->page_count < 1);\r\nif (!sblock->pagev[0].have_csum)\r\nreturn 0;\r\non_disk_csum = sblock->pagev[0].csum;\r\npage = sblock->pagev[0].page;\r\nbuffer = kmap_atomic(page);\r\nlen = sdev->sectorsize;\r\nindex = 0;\r\nfor (;;) {\r\nu64 l = min_t(u64, len, PAGE_SIZE);\r\ncrc = btrfs_csum_data(root, buffer, crc, l);\r\nkunmap_atomic(buffer);\r\nlen -= l;\r\nif (len == 0)\r\nbreak;\r\nindex++;\r\nBUG_ON(index >= sblock->page_count);\r\nBUG_ON(!sblock->pagev[index].page);\r\npage = sblock->pagev[index].page;\r\nbuffer = kmap_atomic(page);\r\n}\r\nbtrfs_csum_final(crc, csum);\r\nif (memcmp(csum, on_disk_csum, sdev->csum_size))\r\nfail = 1;\r\nreturn fail;\r\n}\r\nstatic int scrub_checksum_tree_block(struct scrub_block *sblock)\r\n{\r\nstruct scrub_dev *sdev = sblock->sdev;\r\nstruct btrfs_header *h;\r\nstruct btrfs_root *root = sdev->dev->dev_root;\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nu8 calculated_csum[BTRFS_CSUM_SIZE];\r\nu8 on_disk_csum[BTRFS_CSUM_SIZE];\r\nstruct page *page;\r\nvoid *mapped_buffer;\r\nu64 mapped_size;\r\nvoid *p;\r\nu32 crc = ~(u32)0;\r\nint fail = 0;\r\nint crc_fail = 0;\r\nu64 len;\r\nint index;\r\nBUG_ON(sblock->page_count < 1);\r\npage = sblock->pagev[0].page;\r\nmapped_buffer = kmap_atomic(page);\r\nh = (struct btrfs_header *)mapped_buffer;\r\nmemcpy(on_disk_csum, h->csum, sdev->csum_size);\r\nif (sblock->pagev[0].logical != le64_to_cpu(h->bytenr))\r\n++fail;\r\nif (sblock->pagev[0].generation != le64_to_cpu(h->generation))\r\n++fail;\r\nif (memcmp(h->fsid, fs_info->fsid, BTRFS_UUID_SIZE))\r\n++fail;\r\nif (memcmp(h->chunk_tree_uuid, fs_info->chunk_tree_uuid,\r\nBTRFS_UUID_SIZE))\r\n++fail;\r\nBUG_ON(sdev->nodesize != sdev->leafsize);\r\nlen = sdev->nodesize - BTRFS_CSUM_SIZE;\r\nmapped_size = PAGE_SIZE - BTRFS_CSUM_SIZE;\r\np = ((u8 *)mapped_buffer) + BTRFS_CSUM_SIZE;\r\nindex = 0;\r\nfor (;;) {\r\nu64 l = min_t(u64, len, mapped_size);\r\ncrc = btrfs_csum_data(root, p, crc, l);\r\nkunmap_atomic(mapped_buffer);\r\nlen -= l;\r\nif (len == 0)\r\nbreak;\r\nindex++;\r\nBUG_ON(index >= sblock->page_count);\r\nBUG_ON(!sblock->pagev[index].page);\r\npage = sblock->pagev[index].page;\r\nmapped_buffer = kmap_atomic(page);\r\nmapped_size = PAGE_SIZE;\r\np = mapped_buffer;\r\n}\r\nbtrfs_csum_final(crc, calculated_csum);\r\nif (memcmp(calculated_csum, on_disk_csum, sdev->csum_size))\r\n++crc_fail;\r\nreturn fail || crc_fail;\r\n}\r\nstatic int scrub_checksum_super(struct scrub_block *sblock)\r\n{\r\nstruct btrfs_super_block *s;\r\nstruct scrub_dev *sdev = sblock->sdev;\r\nstruct btrfs_root *root = sdev->dev->dev_root;\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nu8 calculated_csum[BTRFS_CSUM_SIZE];\r\nu8 on_disk_csum[BTRFS_CSUM_SIZE];\r\nstruct page *page;\r\nvoid *mapped_buffer;\r\nu64 mapped_size;\r\nvoid *p;\r\nu32 crc = ~(u32)0;\r\nint fail_gen = 0;\r\nint fail_cor = 0;\r\nu64 len;\r\nint index;\r\nBUG_ON(sblock->page_count < 1);\r\npage = sblock->pagev[0].page;\r\nmapped_buffer = kmap_atomic(page);\r\ns = (struct btrfs_super_block *)mapped_buffer;\r\nmemcpy(on_disk_csum, s->csum, sdev->csum_size);\r\nif (sblock->pagev[0].logical != le64_to_cpu(s->bytenr))\r\n++fail_cor;\r\nif (sblock->pagev[0].generation != le64_to_cpu(s->generation))\r\n++fail_gen;\r\nif (memcmp(s->fsid, fs_info->fsid, BTRFS_UUID_SIZE))\r\n++fail_cor;\r\nlen = BTRFS_SUPER_INFO_SIZE - BTRFS_CSUM_SIZE;\r\nmapped_size = PAGE_SIZE - BTRFS_CSUM_SIZE;\r\np = ((u8 *)mapped_buffer) + BTRFS_CSUM_SIZE;\r\nindex = 0;\r\nfor (;;) {\r\nu64 l = min_t(u64, len, mapped_size);\r\ncrc = btrfs_csum_data(root, p, crc, l);\r\nkunmap_atomic(mapped_buffer);\r\nlen -= l;\r\nif (len == 0)\r\nbreak;\r\nindex++;\r\nBUG_ON(index >= sblock->page_count);\r\nBUG_ON(!sblock->pagev[index].page);\r\npage = sblock->pagev[index].page;\r\nmapped_buffer = kmap_atomic(page);\r\nmapped_size = PAGE_SIZE;\r\np = mapped_buffer;\r\n}\r\nbtrfs_csum_final(crc, calculated_csum);\r\nif (memcmp(calculated_csum, on_disk_csum, sdev->csum_size))\r\n++fail_cor;\r\nif (fail_cor + fail_gen) {\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.super_errors;\r\nspin_unlock(&sdev->stat_lock);\r\nif (fail_cor)\r\nbtrfs_dev_stat_inc_and_print(sdev->dev,\r\nBTRFS_DEV_STAT_CORRUPTION_ERRS);\r\nelse\r\nbtrfs_dev_stat_inc_and_print(sdev->dev,\r\nBTRFS_DEV_STAT_GENERATION_ERRS);\r\n}\r\nreturn fail_cor + fail_gen;\r\n}\r\nstatic void scrub_block_get(struct scrub_block *sblock)\r\n{\r\natomic_inc(&sblock->ref_count);\r\n}\r\nstatic void scrub_block_put(struct scrub_block *sblock)\r\n{\r\nif (atomic_dec_and_test(&sblock->ref_count)) {\r\nint i;\r\nfor (i = 0; i < sblock->page_count; i++)\r\nif (sblock->pagev[i].page)\r\n__free_page(sblock->pagev[i].page);\r\nkfree(sblock);\r\n}\r\n}\r\nstatic void scrub_submit(struct scrub_dev *sdev)\r\n{\r\nstruct scrub_bio *sbio;\r\nif (sdev->curr == -1)\r\nreturn;\r\nsbio = sdev->bios[sdev->curr];\r\nsdev->curr = -1;\r\natomic_inc(&sdev->in_flight);\r\nbtrfsic_submit_bio(READ, sbio->bio);\r\n}\r\nstatic int scrub_add_page_to_bio(struct scrub_dev *sdev,\r\nstruct scrub_page *spage)\r\n{\r\nstruct scrub_block *sblock = spage->sblock;\r\nstruct scrub_bio *sbio;\r\nint ret;\r\nagain:\r\nwhile (sdev->curr == -1) {\r\nspin_lock(&sdev->list_lock);\r\nsdev->curr = sdev->first_free;\r\nif (sdev->curr != -1) {\r\nsdev->first_free = sdev->bios[sdev->curr]->next_free;\r\nsdev->bios[sdev->curr]->next_free = -1;\r\nsdev->bios[sdev->curr]->page_count = 0;\r\nspin_unlock(&sdev->list_lock);\r\n} else {\r\nspin_unlock(&sdev->list_lock);\r\nwait_event(sdev->list_wait, sdev->first_free != -1);\r\n}\r\n}\r\nsbio = sdev->bios[sdev->curr];\r\nif (sbio->page_count == 0) {\r\nstruct bio *bio;\r\nsbio->physical = spage->physical;\r\nsbio->logical = spage->logical;\r\nbio = sbio->bio;\r\nif (!bio) {\r\nbio = bio_alloc(GFP_NOFS, sdev->pages_per_bio);\r\nif (!bio)\r\nreturn -ENOMEM;\r\nsbio->bio = bio;\r\n}\r\nbio->bi_private = sbio;\r\nbio->bi_end_io = scrub_bio_end_io;\r\nbio->bi_bdev = sdev->dev->bdev;\r\nbio->bi_sector = spage->physical >> 9;\r\nsbio->err = 0;\r\n} else if (sbio->physical + sbio->page_count * PAGE_SIZE !=\r\nspage->physical ||\r\nsbio->logical + sbio->page_count * PAGE_SIZE !=\r\nspage->logical) {\r\nscrub_submit(sdev);\r\ngoto again;\r\n}\r\nsbio->pagev[sbio->page_count] = spage;\r\nret = bio_add_page(sbio->bio, spage->page, PAGE_SIZE, 0);\r\nif (ret != PAGE_SIZE) {\r\nif (sbio->page_count < 1) {\r\nbio_put(sbio->bio);\r\nsbio->bio = NULL;\r\nreturn -EIO;\r\n}\r\nscrub_submit(sdev);\r\ngoto again;\r\n}\r\nscrub_block_get(sblock);\r\natomic_inc(&sblock->outstanding_pages);\r\nsbio->page_count++;\r\nif (sbio->page_count == sdev->pages_per_bio)\r\nscrub_submit(sdev);\r\nreturn 0;\r\n}\r\nstatic int scrub_pages(struct scrub_dev *sdev, u64 logical, u64 len,\r\nu64 physical, u64 flags, u64 gen, int mirror_num,\r\nu8 *csum, int force)\r\n{\r\nstruct scrub_block *sblock;\r\nint index;\r\nsblock = kzalloc(sizeof(*sblock), GFP_NOFS);\r\nif (!sblock) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.malloc_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nreturn -ENOMEM;\r\n}\r\natomic_set(&sblock->ref_count, 1);\r\nsblock->sdev = sdev;\r\nsblock->no_io_error_seen = 1;\r\nfor (index = 0; len > 0; index++) {\r\nstruct scrub_page *spage = sblock->pagev + index;\r\nu64 l = min_t(u64, len, PAGE_SIZE);\r\nBUG_ON(index >= SCRUB_MAX_PAGES_PER_BLOCK);\r\nspage->page = alloc_page(GFP_NOFS);\r\nif (!spage->page) {\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.malloc_errors++;\r\nspin_unlock(&sdev->stat_lock);\r\nwhile (index > 0) {\r\nindex--;\r\n__free_page(sblock->pagev[index].page);\r\n}\r\nkfree(sblock);\r\nreturn -ENOMEM;\r\n}\r\nspage->sblock = sblock;\r\nspage->dev = sdev->dev;\r\nspage->flags = flags;\r\nspage->generation = gen;\r\nspage->logical = logical;\r\nspage->physical = physical;\r\nspage->mirror_num = mirror_num;\r\nif (csum) {\r\nspage->have_csum = 1;\r\nmemcpy(spage->csum, csum, sdev->csum_size);\r\n} else {\r\nspage->have_csum = 0;\r\n}\r\nsblock->page_count++;\r\nlen -= l;\r\nlogical += l;\r\nphysical += l;\r\n}\r\nBUG_ON(sblock->page_count == 0);\r\nfor (index = 0; index < sblock->page_count; index++) {\r\nstruct scrub_page *spage = sblock->pagev + index;\r\nint ret;\r\nret = scrub_add_page_to_bio(sdev, spage);\r\nif (ret) {\r\nscrub_block_put(sblock);\r\nreturn ret;\r\n}\r\n}\r\nif (force)\r\nscrub_submit(sdev);\r\nscrub_block_put(sblock);\r\nreturn 0;\r\n}\r\nstatic void scrub_bio_end_io(struct bio *bio, int err)\r\n{\r\nstruct scrub_bio *sbio = bio->bi_private;\r\nstruct scrub_dev *sdev = sbio->sdev;\r\nstruct btrfs_fs_info *fs_info = sdev->dev->dev_root->fs_info;\r\nsbio->err = err;\r\nsbio->bio = bio;\r\nbtrfs_queue_worker(&fs_info->scrub_workers, &sbio->work);\r\n}\r\nstatic void scrub_bio_end_io_worker(struct btrfs_work *work)\r\n{\r\nstruct scrub_bio *sbio = container_of(work, struct scrub_bio, work);\r\nstruct scrub_dev *sdev = sbio->sdev;\r\nint i;\r\nBUG_ON(sbio->page_count > SCRUB_PAGES_PER_BIO);\r\nif (sbio->err) {\r\nfor (i = 0; i < sbio->page_count; i++) {\r\nstruct scrub_page *spage = sbio->pagev[i];\r\nspage->io_error = 1;\r\nspage->sblock->no_io_error_seen = 0;\r\n}\r\n}\r\nfor (i = 0; i < sbio->page_count; i++) {\r\nstruct scrub_page *spage = sbio->pagev[i];\r\nstruct scrub_block *sblock = spage->sblock;\r\nif (atomic_dec_and_test(&sblock->outstanding_pages))\r\nscrub_block_complete(sblock);\r\nscrub_block_put(sblock);\r\n}\r\nif (sbio->err) {\r\nsbio->bio->bi_flags &= ~(BIO_POOL_MASK - 1);\r\nsbio->bio->bi_flags |= 1 << BIO_UPTODATE;\r\nsbio->bio->bi_phys_segments = 0;\r\nsbio->bio->bi_idx = 0;\r\nfor (i = 0; i < sbio->page_count; i++) {\r\nstruct bio_vec *bi;\r\nbi = &sbio->bio->bi_io_vec[i];\r\nbi->bv_offset = 0;\r\nbi->bv_len = PAGE_SIZE;\r\n}\r\n}\r\nbio_put(sbio->bio);\r\nsbio->bio = NULL;\r\nspin_lock(&sdev->list_lock);\r\nsbio->next_free = sdev->first_free;\r\nsdev->first_free = sbio->index;\r\nspin_unlock(&sdev->list_lock);\r\natomic_dec(&sdev->in_flight);\r\nwake_up(&sdev->list_wait);\r\n}\r\nstatic void scrub_block_complete(struct scrub_block *sblock)\r\n{\r\nif (!sblock->no_io_error_seen)\r\nscrub_handle_errored_block(sblock);\r\nelse\r\nscrub_checksum(sblock);\r\n}\r\nstatic int scrub_find_csum(struct scrub_dev *sdev, u64 logical, u64 len,\r\nu8 *csum)\r\n{\r\nstruct btrfs_ordered_sum *sum = NULL;\r\nint ret = 0;\r\nunsigned long i;\r\nunsigned long num_sectors;\r\nwhile (!list_empty(&sdev->csum_list)) {\r\nsum = list_first_entry(&sdev->csum_list,\r\nstruct btrfs_ordered_sum, list);\r\nif (sum->bytenr > logical)\r\nreturn 0;\r\nif (sum->bytenr + sum->len > logical)\r\nbreak;\r\n++sdev->stat.csum_discards;\r\nlist_del(&sum->list);\r\nkfree(sum);\r\nsum = NULL;\r\n}\r\nif (!sum)\r\nreturn 0;\r\nnum_sectors = sum->len / sdev->sectorsize;\r\nfor (i = 0; i < num_sectors; ++i) {\r\nif (sum->sums[i].bytenr == logical) {\r\nmemcpy(csum, &sum->sums[i].sum, sdev->csum_size);\r\nret = 1;\r\nbreak;\r\n}\r\n}\r\nif (ret && i == num_sectors - 1) {\r\nlist_del(&sum->list);\r\nkfree(sum);\r\n}\r\nreturn ret;\r\n}\r\nstatic int scrub_extent(struct scrub_dev *sdev, u64 logical, u64 len,\r\nu64 physical, u64 flags, u64 gen, int mirror_num)\r\n{\r\nint ret;\r\nu8 csum[BTRFS_CSUM_SIZE];\r\nu32 blocksize;\r\nif (flags & BTRFS_EXTENT_FLAG_DATA) {\r\nblocksize = sdev->sectorsize;\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.data_extents_scrubbed++;\r\nsdev->stat.data_bytes_scrubbed += len;\r\nspin_unlock(&sdev->stat_lock);\r\n} else if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\r\nBUG_ON(sdev->nodesize != sdev->leafsize);\r\nblocksize = sdev->nodesize;\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.tree_extents_scrubbed++;\r\nsdev->stat.tree_bytes_scrubbed += len;\r\nspin_unlock(&sdev->stat_lock);\r\n} else {\r\nblocksize = sdev->sectorsize;\r\nBUG_ON(1);\r\n}\r\nwhile (len) {\r\nu64 l = min_t(u64, len, blocksize);\r\nint have_csum = 0;\r\nif (flags & BTRFS_EXTENT_FLAG_DATA) {\r\nhave_csum = scrub_find_csum(sdev, logical, l, csum);\r\nif (have_csum == 0)\r\n++sdev->stat.no_csum;\r\n}\r\nret = scrub_pages(sdev, logical, l, physical, flags, gen,\r\nmirror_num, have_csum ? csum : NULL, 0);\r\nif (ret)\r\nreturn ret;\r\nlen -= l;\r\nlogical += l;\r\nphysical += l;\r\n}\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack int scrub_stripe(struct scrub_dev *sdev,\r\nstruct map_lookup *map, int num, u64 base, u64 length)\r\n{\r\nstruct btrfs_path *path;\r\nstruct btrfs_fs_info *fs_info = sdev->dev->dev_root->fs_info;\r\nstruct btrfs_root *root = fs_info->extent_root;\r\nstruct btrfs_root *csum_root = fs_info->csum_root;\r\nstruct btrfs_extent_item *extent;\r\nstruct blk_plug plug;\r\nu64 flags;\r\nint ret;\r\nint slot;\r\nint i;\r\nu64 nstripes;\r\nstruct extent_buffer *l;\r\nstruct btrfs_key key;\r\nu64 physical;\r\nu64 logical;\r\nu64 generation;\r\nint mirror_num;\r\nstruct reada_control *reada1;\r\nstruct reada_control *reada2;\r\nstruct btrfs_key key_start;\r\nstruct btrfs_key key_end;\r\nu64 increment = map->stripe_len;\r\nu64 offset;\r\nnstripes = length;\r\noffset = 0;\r\ndo_div(nstripes, map->stripe_len);\r\nif (map->type & BTRFS_BLOCK_GROUP_RAID0) {\r\noffset = map->stripe_len * num;\r\nincrement = map->stripe_len * map->num_stripes;\r\nmirror_num = 1;\r\n} else if (map->type & BTRFS_BLOCK_GROUP_RAID10) {\r\nint factor = map->num_stripes / map->sub_stripes;\r\noffset = map->stripe_len * (num / map->sub_stripes);\r\nincrement = map->stripe_len * factor;\r\nmirror_num = num % map->sub_stripes + 1;\r\n} else if (map->type & BTRFS_BLOCK_GROUP_RAID1) {\r\nincrement = map->stripe_len;\r\nmirror_num = num % map->num_stripes + 1;\r\n} else if (map->type & BTRFS_BLOCK_GROUP_DUP) {\r\nincrement = map->stripe_len;\r\nmirror_num = num % map->num_stripes + 1;\r\n} else {\r\nincrement = map->stripe_len;\r\nmirror_num = 1;\r\n}\r\npath = btrfs_alloc_path();\r\nif (!path)\r\nreturn -ENOMEM;\r\npath->search_commit_root = 1;\r\npath->skip_locking = 1;\r\nlogical = base + offset;\r\nwait_event(sdev->list_wait,\r\natomic_read(&sdev->in_flight) == 0);\r\natomic_inc(&fs_info->scrubs_paused);\r\nwake_up(&fs_info->scrub_pause_wait);\r\nkey_start.objectid = logical;\r\nkey_start.type = BTRFS_EXTENT_ITEM_KEY;\r\nkey_start.offset = (u64)0;\r\nkey_end.objectid = base + offset + nstripes * increment;\r\nkey_end.type = BTRFS_EXTENT_ITEM_KEY;\r\nkey_end.offset = (u64)0;\r\nreada1 = btrfs_reada_add(root, &key_start, &key_end);\r\nkey_start.objectid = BTRFS_EXTENT_CSUM_OBJECTID;\r\nkey_start.type = BTRFS_EXTENT_CSUM_KEY;\r\nkey_start.offset = logical;\r\nkey_end.objectid = BTRFS_EXTENT_CSUM_OBJECTID;\r\nkey_end.type = BTRFS_EXTENT_CSUM_KEY;\r\nkey_end.offset = base + offset + nstripes * increment;\r\nreada2 = btrfs_reada_add(csum_root, &key_start, &key_end);\r\nif (!IS_ERR(reada1))\r\nbtrfs_reada_wait(reada1);\r\nif (!IS_ERR(reada2))\r\nbtrfs_reada_wait(reada2);\r\nmutex_lock(&fs_info->scrub_lock);\r\nwhile (atomic_read(&fs_info->scrub_pause_req)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwait_event(fs_info->scrub_pause_wait,\r\natomic_read(&fs_info->scrub_pause_req) == 0);\r\nmutex_lock(&fs_info->scrub_lock);\r\n}\r\natomic_dec(&fs_info->scrubs_paused);\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwake_up(&fs_info->scrub_pause_wait);\r\nblk_start_plug(&plug);\r\nlogical = base + offset;\r\nphysical = map->stripes[num].physical;\r\nret = 0;\r\nfor (i = 0; i < nstripes; ++i) {\r\nif (atomic_read(&fs_info->scrub_cancel_req) ||\r\natomic_read(&sdev->cancel_req)) {\r\nret = -ECANCELED;\r\ngoto out;\r\n}\r\nif (atomic_read(&fs_info->scrub_pause_req)) {\r\nscrub_submit(sdev);\r\nwait_event(sdev->list_wait,\r\natomic_read(&sdev->in_flight) == 0);\r\natomic_inc(&fs_info->scrubs_paused);\r\nwake_up(&fs_info->scrub_pause_wait);\r\nmutex_lock(&fs_info->scrub_lock);\r\nwhile (atomic_read(&fs_info->scrub_pause_req)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwait_event(fs_info->scrub_pause_wait,\r\natomic_read(&fs_info->scrub_pause_req) == 0);\r\nmutex_lock(&fs_info->scrub_lock);\r\n}\r\natomic_dec(&fs_info->scrubs_paused);\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwake_up(&fs_info->scrub_pause_wait);\r\n}\r\nret = btrfs_lookup_csums_range(csum_root, logical,\r\nlogical + map->stripe_len - 1,\r\n&sdev->csum_list, 1);\r\nif (ret)\r\ngoto out;\r\nkey.objectid = logical;\r\nkey.type = BTRFS_EXTENT_ITEM_KEY;\r\nkey.offset = (u64)0;\r\nret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\r\nif (ret < 0)\r\ngoto out;\r\nif (ret > 0) {\r\nret = btrfs_previous_item(root, path, 0,\r\nBTRFS_EXTENT_ITEM_KEY);\r\nif (ret < 0)\r\ngoto out;\r\nif (ret > 0) {\r\nbtrfs_release_path(path);\r\nret = btrfs_search_slot(NULL, root, &key,\r\npath, 0, 0);\r\nif (ret < 0)\r\ngoto out;\r\n}\r\n}\r\nwhile (1) {\r\nl = path->nodes[0];\r\nslot = path->slots[0];\r\nif (slot >= btrfs_header_nritems(l)) {\r\nret = btrfs_next_leaf(root, path);\r\nif (ret == 0)\r\ncontinue;\r\nif (ret < 0)\r\ngoto out;\r\nbreak;\r\n}\r\nbtrfs_item_key_to_cpu(l, &key, slot);\r\nif (key.objectid + key.offset <= logical)\r\ngoto next;\r\nif (key.objectid >= logical + map->stripe_len)\r\nbreak;\r\nif (btrfs_key_type(&key) != BTRFS_EXTENT_ITEM_KEY)\r\ngoto next;\r\nextent = btrfs_item_ptr(l, slot,\r\nstruct btrfs_extent_item);\r\nflags = btrfs_extent_flags(l, extent);\r\ngeneration = btrfs_extent_generation(l, extent);\r\nif (key.objectid < logical &&\r\n(flags & BTRFS_EXTENT_FLAG_TREE_BLOCK)) {\r\nprintk(KERN_ERR\r\n"btrfs scrub: tree block %llu spanning "\r\n"stripes, ignored. logical=%llu\n",\r\n(unsigned long long)key.objectid,\r\n(unsigned long long)logical);\r\ngoto next;\r\n}\r\nif (key.objectid < logical) {\r\nkey.offset -= logical - key.objectid;\r\nkey.objectid = logical;\r\n}\r\nif (key.objectid + key.offset >\r\nlogical + map->stripe_len) {\r\nkey.offset = logical + map->stripe_len -\r\nkey.objectid;\r\n}\r\nret = scrub_extent(sdev, key.objectid, key.offset,\r\nkey.objectid - logical + physical,\r\nflags, generation, mirror_num);\r\nif (ret)\r\ngoto out;\r\nnext:\r\npath->slots[0]++;\r\n}\r\nbtrfs_release_path(path);\r\nlogical += increment;\r\nphysical += map->stripe_len;\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.last_physical = physical;\r\nspin_unlock(&sdev->stat_lock);\r\n}\r\nscrub_submit(sdev);\r\nout:\r\nblk_finish_plug(&plug);\r\nbtrfs_free_path(path);\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic noinline_for_stack int scrub_chunk(struct scrub_dev *sdev,\r\nu64 chunk_tree, u64 chunk_objectid, u64 chunk_offset, u64 length,\r\nu64 dev_offset)\r\n{\r\nstruct btrfs_mapping_tree *map_tree =\r\n&sdev->dev->dev_root->fs_info->mapping_tree;\r\nstruct map_lookup *map;\r\nstruct extent_map *em;\r\nint i;\r\nint ret = -EINVAL;\r\nread_lock(&map_tree->map_tree.lock);\r\nem = lookup_extent_mapping(&map_tree->map_tree, chunk_offset, 1);\r\nread_unlock(&map_tree->map_tree.lock);\r\nif (!em)\r\nreturn -EINVAL;\r\nmap = (struct map_lookup *)em->bdev;\r\nif (em->start != chunk_offset)\r\ngoto out;\r\nif (em->len < length)\r\ngoto out;\r\nfor (i = 0; i < map->num_stripes; ++i) {\r\nif (map->stripes[i].dev == sdev->dev &&\r\nmap->stripes[i].physical == dev_offset) {\r\nret = scrub_stripe(sdev, map, i, chunk_offset, length);\r\nif (ret)\r\ngoto out;\r\n}\r\n}\r\nout:\r\nfree_extent_map(em);\r\nreturn ret;\r\n}\r\nstatic noinline_for_stack\r\nint scrub_enumerate_chunks(struct scrub_dev *sdev, u64 start, u64 end)\r\n{\r\nstruct btrfs_dev_extent *dev_extent = NULL;\r\nstruct btrfs_path *path;\r\nstruct btrfs_root *root = sdev->dev->dev_root;\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nu64 length;\r\nu64 chunk_tree;\r\nu64 chunk_objectid;\r\nu64 chunk_offset;\r\nint ret;\r\nint slot;\r\nstruct extent_buffer *l;\r\nstruct btrfs_key key;\r\nstruct btrfs_key found_key;\r\nstruct btrfs_block_group_cache *cache;\r\npath = btrfs_alloc_path();\r\nif (!path)\r\nreturn -ENOMEM;\r\npath->reada = 2;\r\npath->search_commit_root = 1;\r\npath->skip_locking = 1;\r\nkey.objectid = sdev->dev->devid;\r\nkey.offset = 0ull;\r\nkey.type = BTRFS_DEV_EXTENT_KEY;\r\nwhile (1) {\r\nret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\r\nif (ret < 0)\r\nbreak;\r\nif (ret > 0) {\r\nif (path->slots[0] >=\r\nbtrfs_header_nritems(path->nodes[0])) {\r\nret = btrfs_next_leaf(root, path);\r\nif (ret)\r\nbreak;\r\n}\r\n}\r\nl = path->nodes[0];\r\nslot = path->slots[0];\r\nbtrfs_item_key_to_cpu(l, &found_key, slot);\r\nif (found_key.objectid != sdev->dev->devid)\r\nbreak;\r\nif (btrfs_key_type(&found_key) != BTRFS_DEV_EXTENT_KEY)\r\nbreak;\r\nif (found_key.offset >= end)\r\nbreak;\r\nif (found_key.offset < key.offset)\r\nbreak;\r\ndev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\r\nlength = btrfs_dev_extent_length(l, dev_extent);\r\nif (found_key.offset + length <= start) {\r\nkey.offset = found_key.offset + length;\r\nbtrfs_release_path(path);\r\ncontinue;\r\n}\r\nchunk_tree = btrfs_dev_extent_chunk_tree(l, dev_extent);\r\nchunk_objectid = btrfs_dev_extent_chunk_objectid(l, dev_extent);\r\nchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\r\ncache = btrfs_lookup_block_group(fs_info, chunk_offset);\r\nif (!cache) {\r\nret = -ENOENT;\r\nbreak;\r\n}\r\nret = scrub_chunk(sdev, chunk_tree, chunk_objectid,\r\nchunk_offset, length, found_key.offset);\r\nbtrfs_put_block_group(cache);\r\nif (ret)\r\nbreak;\r\nkey.offset = found_key.offset + length;\r\nbtrfs_release_path(path);\r\n}\r\nbtrfs_free_path(path);\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic noinline_for_stack int scrub_supers(struct scrub_dev *sdev)\r\n{\r\nint i;\r\nu64 bytenr;\r\nu64 gen;\r\nint ret;\r\nstruct btrfs_device *device = sdev->dev;\r\nstruct btrfs_root *root = device->dev_root;\r\nif (root->fs_info->fs_state & BTRFS_SUPER_FLAG_ERROR)\r\nreturn -EIO;\r\ngen = root->fs_info->last_trans_committed;\r\nfor (i = 0; i < BTRFS_SUPER_MIRROR_MAX; i++) {\r\nbytenr = btrfs_sb_offset(i);\r\nif (bytenr + BTRFS_SUPER_INFO_SIZE > device->total_bytes)\r\nbreak;\r\nret = scrub_pages(sdev, bytenr, BTRFS_SUPER_INFO_SIZE, bytenr,\r\nBTRFS_EXTENT_FLAG_SUPER, gen, i, NULL, 1);\r\nif (ret)\r\nreturn ret;\r\n}\r\nwait_event(sdev->list_wait, atomic_read(&sdev->in_flight) == 0);\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack int scrub_workers_get(struct btrfs_root *root)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nint ret = 0;\r\nmutex_lock(&fs_info->scrub_lock);\r\nif (fs_info->scrub_workers_refcnt == 0) {\r\nbtrfs_init_workers(&fs_info->scrub_workers, "scrub",\r\nfs_info->thread_pool_size, &fs_info->generic_worker);\r\nfs_info->scrub_workers.idle_thresh = 4;\r\nret = btrfs_start_workers(&fs_info->scrub_workers);\r\nif (ret)\r\ngoto out;\r\n}\r\n++fs_info->scrub_workers_refcnt;\r\nout:\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn ret;\r\n}\r\nstatic noinline_for_stack void scrub_workers_put(struct btrfs_root *root)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nmutex_lock(&fs_info->scrub_lock);\r\nif (--fs_info->scrub_workers_refcnt == 0)\r\nbtrfs_stop_workers(&fs_info->scrub_workers);\r\nWARN_ON(fs_info->scrub_workers_refcnt < 0);\r\nmutex_unlock(&fs_info->scrub_lock);\r\n}\r\nint btrfs_scrub_dev(struct btrfs_root *root, u64 devid, u64 start, u64 end,\r\nstruct btrfs_scrub_progress *progress, int readonly)\r\n{\r\nstruct scrub_dev *sdev;\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nint ret;\r\nstruct btrfs_device *dev;\r\nif (btrfs_fs_closing(root->fs_info))\r\nreturn -EINVAL;\r\nif (root->nodesize != root->leafsize) {\r\nprintk(KERN_ERR\r\n"btrfs_scrub: size assumption nodesize == leafsize (%d == %d) fails\n",\r\nroot->nodesize, root->leafsize);\r\nreturn -EINVAL;\r\n}\r\nif (root->nodesize > BTRFS_STRIPE_LEN) {\r\nprintk(KERN_ERR\r\n"btrfs_scrub: size assumption nodesize <= BTRFS_STRIPE_LEN (%d <= %d) fails\n",\r\nroot->nodesize, BTRFS_STRIPE_LEN);\r\nreturn -EINVAL;\r\n}\r\nif (root->sectorsize != PAGE_SIZE) {\r\nprintk(KERN_ERR\r\n"btrfs_scrub: size assumption sectorsize != PAGE_SIZE (%d != %lld) fails\n",\r\nroot->sectorsize, (unsigned long long)PAGE_SIZE);\r\nreturn -EINVAL;\r\n}\r\nret = scrub_workers_get(root);\r\nif (ret)\r\nreturn ret;\r\nmutex_lock(&root->fs_info->fs_devices->device_list_mutex);\r\ndev = btrfs_find_device(root, devid, NULL, NULL);\r\nif (!dev || dev->missing) {\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nscrub_workers_put(root);\r\nreturn -ENODEV;\r\n}\r\nmutex_lock(&fs_info->scrub_lock);\r\nif (!dev->in_fs_metadata) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nscrub_workers_put(root);\r\nreturn -ENODEV;\r\n}\r\nif (dev->scrub_device) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nscrub_workers_put(root);\r\nreturn -EINPROGRESS;\r\n}\r\nsdev = scrub_setup_dev(dev);\r\nif (IS_ERR(sdev)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nscrub_workers_put(root);\r\nreturn PTR_ERR(sdev);\r\n}\r\nsdev->readonly = readonly;\r\ndev->scrub_device = sdev;\r\natomic_inc(&fs_info->scrubs_running);\r\nmutex_unlock(&fs_info->scrub_lock);\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\ndown_read(&fs_info->scrub_super_lock);\r\nret = scrub_supers(sdev);\r\nup_read(&fs_info->scrub_super_lock);\r\nif (!ret)\r\nret = scrub_enumerate_chunks(sdev, start, end);\r\nwait_event(sdev->list_wait, atomic_read(&sdev->in_flight) == 0);\r\natomic_dec(&fs_info->scrubs_running);\r\nwake_up(&fs_info->scrub_pause_wait);\r\nwait_event(sdev->list_wait, atomic_read(&sdev->fixup_cnt) == 0);\r\nif (progress)\r\nmemcpy(progress, &sdev->stat, sizeof(*progress));\r\nmutex_lock(&fs_info->scrub_lock);\r\ndev->scrub_device = NULL;\r\nmutex_unlock(&fs_info->scrub_lock);\r\nscrub_free_dev(sdev);\r\nscrub_workers_put(root);\r\nreturn ret;\r\n}\r\nvoid btrfs_scrub_pause(struct btrfs_root *root)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nmutex_lock(&fs_info->scrub_lock);\r\natomic_inc(&fs_info->scrub_pause_req);\r\nwhile (atomic_read(&fs_info->scrubs_paused) !=\r\natomic_read(&fs_info->scrubs_running)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwait_event(fs_info->scrub_pause_wait,\r\natomic_read(&fs_info->scrubs_paused) ==\r\natomic_read(&fs_info->scrubs_running));\r\nmutex_lock(&fs_info->scrub_lock);\r\n}\r\nmutex_unlock(&fs_info->scrub_lock);\r\n}\r\nvoid btrfs_scrub_continue(struct btrfs_root *root)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\natomic_dec(&fs_info->scrub_pause_req);\r\nwake_up(&fs_info->scrub_pause_wait);\r\n}\r\nvoid btrfs_scrub_pause_super(struct btrfs_root *root)\r\n{\r\ndown_write(&root->fs_info->scrub_super_lock);\r\n}\r\nvoid btrfs_scrub_continue_super(struct btrfs_root *root)\r\n{\r\nup_write(&root->fs_info->scrub_super_lock);\r\n}\r\nint __btrfs_scrub_cancel(struct btrfs_fs_info *fs_info)\r\n{\r\nmutex_lock(&fs_info->scrub_lock);\r\nif (!atomic_read(&fs_info->scrubs_running)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn -ENOTCONN;\r\n}\r\natomic_inc(&fs_info->scrub_cancel_req);\r\nwhile (atomic_read(&fs_info->scrubs_running)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwait_event(fs_info->scrub_pause_wait,\r\natomic_read(&fs_info->scrubs_running) == 0);\r\nmutex_lock(&fs_info->scrub_lock);\r\n}\r\natomic_dec(&fs_info->scrub_cancel_req);\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn 0;\r\n}\r\nint btrfs_scrub_cancel(struct btrfs_root *root)\r\n{\r\nreturn __btrfs_scrub_cancel(root->fs_info);\r\n}\r\nint btrfs_scrub_cancel_dev(struct btrfs_root *root, struct btrfs_device *dev)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nstruct scrub_dev *sdev;\r\nmutex_lock(&fs_info->scrub_lock);\r\nsdev = dev->scrub_device;\r\nif (!sdev) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn -ENOTCONN;\r\n}\r\natomic_inc(&sdev->cancel_req);\r\nwhile (dev->scrub_device) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwait_event(fs_info->scrub_pause_wait,\r\ndev->scrub_device == NULL);\r\nmutex_lock(&fs_info->scrub_lock);\r\n}\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn 0;\r\n}\r\nint btrfs_scrub_cancel_devid(struct btrfs_root *root, u64 devid)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nstruct btrfs_device *dev;\r\nint ret;\r\nmutex_lock(&fs_info->fs_devices->device_list_mutex);\r\ndev = btrfs_find_device(root, devid, NULL, NULL);\r\nif (!dev) {\r\nmutex_unlock(&fs_info->fs_devices->device_list_mutex);\r\nreturn -ENODEV;\r\n}\r\nret = btrfs_scrub_cancel_dev(root, dev);\r\nmutex_unlock(&fs_info->fs_devices->device_list_mutex);\r\nreturn ret;\r\n}\r\nint btrfs_scrub_progress(struct btrfs_root *root, u64 devid,\r\nstruct btrfs_scrub_progress *progress)\r\n{\r\nstruct btrfs_device *dev;\r\nstruct scrub_dev *sdev = NULL;\r\nmutex_lock(&root->fs_info->fs_devices->device_list_mutex);\r\ndev = btrfs_find_device(root, devid, NULL, NULL);\r\nif (dev)\r\nsdev = dev->scrub_device;\r\nif (sdev)\r\nmemcpy(progress, &sdev->stat, sizeof(*progress));\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nreturn dev ? (sdev ? 0 : -ENOTCONN) : -ENODEV;\r\n}
