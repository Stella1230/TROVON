static int __init parse_direct_gbpages_off(char *arg)\r\n{\r\ndirect_gbpages = 0;\r\nreturn 0;\r\n}\r\nstatic int __init parse_direct_gbpages_on(char *arg)\r\n{\r\ndirect_gbpages = 1;\r\nreturn 0;\r\n}\r\nstatic int __init nonx32_setup(char *str)\r\n{\r\nif (!strcmp(str, "on"))\r\nforce_personality32 &= ~READ_IMPLIES_EXEC;\r\nelse if (!strcmp(str, "off"))\r\nforce_personality32 |= READ_IMPLIES_EXEC;\r\nreturn 1;\r\n}\r\nvoid sync_global_pgds(unsigned long start, unsigned long end)\r\n{\r\nunsigned long address;\r\nfor (address = start; address <= end; address += PGDIR_SIZE) {\r\nconst pgd_t *pgd_ref = pgd_offset_k(address);\r\nstruct page *page;\r\nif (pgd_none(*pgd_ref))\r\ncontinue;\r\nspin_lock(&pgd_lock);\r\nlist_for_each_entry(page, &pgd_list, lru) {\r\npgd_t *pgd;\r\nspinlock_t *pgt_lock;\r\npgd = (pgd_t *)page_address(page) + pgd_index(address);\r\npgt_lock = &pgd_page_get_mm(page)->page_table_lock;\r\nspin_lock(pgt_lock);\r\nif (pgd_none(*pgd))\r\nset_pgd(pgd, *pgd_ref);\r\nelse\r\nBUG_ON(pgd_page_vaddr(*pgd)\r\n!= pgd_page_vaddr(*pgd_ref));\r\nspin_unlock(pgt_lock);\r\n}\r\nspin_unlock(&pgd_lock);\r\n}\r\n}\r\nstatic __ref void *spp_getpage(void)\r\n{\r\nvoid *ptr;\r\nif (after_bootmem)\r\nptr = (void *) get_zeroed_page(GFP_ATOMIC | __GFP_NOTRACK);\r\nelse\r\nptr = alloc_bootmem_pages(PAGE_SIZE);\r\nif (!ptr || ((unsigned long)ptr & ~PAGE_MASK)) {\r\npanic("set_pte_phys: cannot allocate page data %s\n",\r\nafter_bootmem ? "after bootmem" : "");\r\n}\r\npr_debug("spp_getpage %p\n", ptr);\r\nreturn ptr;\r\n}\r\nstatic pud_t *fill_pud(pgd_t *pgd, unsigned long vaddr)\r\n{\r\nif (pgd_none(*pgd)) {\r\npud_t *pud = (pud_t *)spp_getpage();\r\npgd_populate(&init_mm, pgd, pud);\r\nif (pud != pud_offset(pgd, 0))\r\nprintk(KERN_ERR "PAGETABLE BUG #00! %p <-> %p\n",\r\npud, pud_offset(pgd, 0));\r\n}\r\nreturn pud_offset(pgd, vaddr);\r\n}\r\nstatic pmd_t *fill_pmd(pud_t *pud, unsigned long vaddr)\r\n{\r\nif (pud_none(*pud)) {\r\npmd_t *pmd = (pmd_t *) spp_getpage();\r\npud_populate(&init_mm, pud, pmd);\r\nif (pmd != pmd_offset(pud, 0))\r\nprintk(KERN_ERR "PAGETABLE BUG #01! %p <-> %p\n",\r\npmd, pmd_offset(pud, 0));\r\n}\r\nreturn pmd_offset(pud, vaddr);\r\n}\r\nstatic pte_t *fill_pte(pmd_t *pmd, unsigned long vaddr)\r\n{\r\nif (pmd_none(*pmd)) {\r\npte_t *pte = (pte_t *) spp_getpage();\r\npmd_populate_kernel(&init_mm, pmd, pte);\r\nif (pte != pte_offset_kernel(pmd, 0))\r\nprintk(KERN_ERR "PAGETABLE BUG #02!\n");\r\n}\r\nreturn pte_offset_kernel(pmd, vaddr);\r\n}\r\nvoid set_pte_vaddr_pud(pud_t *pud_page, unsigned long vaddr, pte_t new_pte)\r\n{\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\npud = pud_page + pud_index(vaddr);\r\npmd = fill_pmd(pud, vaddr);\r\npte = fill_pte(pmd, vaddr);\r\nset_pte(pte, new_pte);\r\n__flush_tlb_one(vaddr);\r\n}\r\nvoid set_pte_vaddr(unsigned long vaddr, pte_t pteval)\r\n{\r\npgd_t *pgd;\r\npud_t *pud_page;\r\npr_debug("set_pte_vaddr %lx to %lx\n", vaddr, native_pte_val(pteval));\r\npgd = pgd_offset_k(vaddr);\r\nif (pgd_none(*pgd)) {\r\nprintk(KERN_ERR\r\n"PGD FIXMAP MISSING, it should be setup in head.S!\n");\r\nreturn;\r\n}\r\npud_page = (pud_t*)pgd_page_vaddr(*pgd);\r\nset_pte_vaddr_pud(pud_page, vaddr, pteval);\r\n}\r\npmd_t * __init populate_extra_pmd(unsigned long vaddr)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npgd = pgd_offset_k(vaddr);\r\npud = fill_pud(pgd, vaddr);\r\nreturn fill_pmd(pud, vaddr);\r\n}\r\npte_t * __init populate_extra_pte(unsigned long vaddr)\r\n{\r\npmd_t *pmd;\r\npmd = populate_extra_pmd(vaddr);\r\nreturn fill_pte(pmd, vaddr);\r\n}\r\nstatic void __init __init_extra_mapping(unsigned long phys, unsigned long size,\r\npgprot_t prot)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\nBUG_ON((phys & ~PMD_MASK) || (size & ~PMD_MASK));\r\nfor (; size; phys += PMD_SIZE, size -= PMD_SIZE) {\r\npgd = pgd_offset_k((unsigned long)__va(phys));\r\nif (pgd_none(*pgd)) {\r\npud = (pud_t *) spp_getpage();\r\nset_pgd(pgd, __pgd(__pa(pud) | _KERNPG_TABLE |\r\n_PAGE_USER));\r\n}\r\npud = pud_offset(pgd, (unsigned long)__va(phys));\r\nif (pud_none(*pud)) {\r\npmd = (pmd_t *) spp_getpage();\r\nset_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE |\r\n_PAGE_USER));\r\n}\r\npmd = pmd_offset(pud, phys);\r\nBUG_ON(!pmd_none(*pmd));\r\nset_pmd(pmd, __pmd(phys | pgprot_val(prot)));\r\n}\r\n}\r\nvoid __init init_extra_mapping_wb(unsigned long phys, unsigned long size)\r\n{\r\n__init_extra_mapping(phys, size, PAGE_KERNEL_LARGE);\r\n}\r\nvoid __init init_extra_mapping_uc(unsigned long phys, unsigned long size)\r\n{\r\n__init_extra_mapping(phys, size, PAGE_KERNEL_LARGE_NOCACHE);\r\n}\r\nvoid __init cleanup_highmap(void)\r\n{\r\nunsigned long vaddr = __START_KERNEL_map;\r\nunsigned long vaddr_end = __START_KERNEL_map + (max_pfn_mapped << PAGE_SHIFT);\r\nunsigned long end = roundup((unsigned long)_brk_end, PMD_SIZE) - 1;\r\npmd_t *pmd = level2_kernel_pgt;\r\nfor (; vaddr + PMD_SIZE - 1 < vaddr_end; pmd++, vaddr += PMD_SIZE) {\r\nif (pmd_none(*pmd))\r\ncontinue;\r\nif (vaddr < (unsigned long) _text || vaddr > end)\r\nset_pmd(pmd, __pmd(0));\r\n}\r\n}\r\nstatic __ref void *alloc_low_page(unsigned long *phys)\r\n{\r\nunsigned long pfn = pgt_buf_end++;\r\nvoid *adr;\r\nif (after_bootmem) {\r\nadr = (void *)get_zeroed_page(GFP_ATOMIC | __GFP_NOTRACK);\r\n*phys = __pa(adr);\r\nreturn adr;\r\n}\r\nif (pfn >= pgt_buf_top)\r\npanic("alloc_low_page: ran out of memory");\r\nadr = early_memremap(pfn * PAGE_SIZE, PAGE_SIZE);\r\nclear_page(adr);\r\n*phys = pfn * PAGE_SIZE;\r\nreturn adr;\r\n}\r\nstatic __ref void *map_low_page(void *virt)\r\n{\r\nvoid *adr;\r\nunsigned long phys, left;\r\nif (after_bootmem)\r\nreturn virt;\r\nphys = __pa(virt);\r\nleft = phys & (PAGE_SIZE - 1);\r\nadr = early_memremap(phys & PAGE_MASK, PAGE_SIZE);\r\nadr = (void *)(((unsigned long)adr) | left);\r\nreturn adr;\r\n}\r\nstatic __ref void unmap_low_page(void *adr)\r\n{\r\nif (after_bootmem)\r\nreturn;\r\nearly_iounmap((void *)((unsigned long)adr & PAGE_MASK), PAGE_SIZE);\r\n}\r\nstatic unsigned long __meminit\r\nphys_pte_init(pte_t *pte_page, unsigned long addr, unsigned long end,\r\npgprot_t prot)\r\n{\r\nunsigned pages = 0;\r\nunsigned long last_map_addr = end;\r\nint i;\r\npte_t *pte = pte_page + pte_index(addr);\r\nfor(i = pte_index(addr); i < PTRS_PER_PTE; i++, addr += PAGE_SIZE, pte++) {\r\nif (addr >= end) {\r\nif (!after_bootmem) {\r\nfor(; i < PTRS_PER_PTE; i++, pte++)\r\nset_pte(pte, __pte(0));\r\n}\r\nbreak;\r\n}\r\nif (pte_val(*pte)) {\r\npages++;\r\ncontinue;\r\n}\r\nif (0)\r\nprintk(" pte=%p addr=%lx pte=%016lx\n",\r\npte, addr, pfn_pte(addr >> PAGE_SHIFT, PAGE_KERNEL).pte);\r\npages++;\r\nset_pte(pte, pfn_pte(addr >> PAGE_SHIFT, prot));\r\nlast_map_addr = (addr & PAGE_MASK) + PAGE_SIZE;\r\n}\r\nupdate_page_count(PG_LEVEL_4K, pages);\r\nreturn last_map_addr;\r\n}\r\nstatic unsigned long __meminit\r\nphys_pmd_init(pmd_t *pmd_page, unsigned long address, unsigned long end,\r\nunsigned long page_size_mask, pgprot_t prot)\r\n{\r\nunsigned long pages = 0, next;\r\nunsigned long last_map_addr = end;\r\nint i = pmd_index(address);\r\nfor (; i < PTRS_PER_PMD; i++, address = next) {\r\nunsigned long pte_phys;\r\npmd_t *pmd = pmd_page + pmd_index(address);\r\npte_t *pte;\r\npgprot_t new_prot = prot;\r\nif (address >= end) {\r\nif (!after_bootmem) {\r\nfor (; i < PTRS_PER_PMD; i++, pmd++)\r\nset_pmd(pmd, __pmd(0));\r\n}\r\nbreak;\r\n}\r\nnext = (address & PMD_MASK) + PMD_SIZE;\r\nif (pmd_val(*pmd)) {\r\nif (!pmd_large(*pmd)) {\r\nspin_lock(&init_mm.page_table_lock);\r\npte = map_low_page((pte_t *)pmd_page_vaddr(*pmd));\r\nlast_map_addr = phys_pte_init(pte, address,\r\nend, prot);\r\nunmap_low_page(pte);\r\nspin_unlock(&init_mm.page_table_lock);\r\ncontinue;\r\n}\r\nif (page_size_mask & (1 << PG_LEVEL_2M)) {\r\nlast_map_addr = next;\r\ncontinue;\r\n}\r\nnew_prot = pte_pgprot(pte_clrhuge(*(pte_t *)pmd));\r\n}\r\nif (page_size_mask & (1<<PG_LEVEL_2M)) {\r\npages++;\r\nspin_lock(&init_mm.page_table_lock);\r\nset_pte((pte_t *)pmd,\r\npfn_pte(address >> PAGE_SHIFT,\r\n__pgprot(pgprot_val(prot) | _PAGE_PSE)));\r\nspin_unlock(&init_mm.page_table_lock);\r\nlast_map_addr = next;\r\ncontinue;\r\n}\r\npte = alloc_low_page(&pte_phys);\r\nlast_map_addr = phys_pte_init(pte, address, end, new_prot);\r\nunmap_low_page(pte);\r\nspin_lock(&init_mm.page_table_lock);\r\npmd_populate_kernel(&init_mm, pmd, __va(pte_phys));\r\nspin_unlock(&init_mm.page_table_lock);\r\n}\r\nupdate_page_count(PG_LEVEL_2M, pages);\r\nreturn last_map_addr;\r\n}\r\nstatic unsigned long __meminit\r\nphys_pud_init(pud_t *pud_page, unsigned long addr, unsigned long end,\r\nunsigned long page_size_mask)\r\n{\r\nunsigned long pages = 0, next;\r\nunsigned long last_map_addr = end;\r\nint i = pud_index(addr);\r\nfor (; i < PTRS_PER_PUD; i++, addr = next) {\r\nunsigned long pmd_phys;\r\npud_t *pud = pud_page + pud_index(addr);\r\npmd_t *pmd;\r\npgprot_t prot = PAGE_KERNEL;\r\nif (addr >= end)\r\nbreak;\r\nnext = (addr & PUD_MASK) + PUD_SIZE;\r\nif (!after_bootmem && !e820_any_mapped(addr, next, 0)) {\r\nset_pud(pud, __pud(0));\r\ncontinue;\r\n}\r\nif (pud_val(*pud)) {\r\nif (!pud_large(*pud)) {\r\npmd = map_low_page(pmd_offset(pud, 0));\r\nlast_map_addr = phys_pmd_init(pmd, addr, end,\r\npage_size_mask, prot);\r\nunmap_low_page(pmd);\r\n__flush_tlb_all();\r\ncontinue;\r\n}\r\nif (page_size_mask & (1 << PG_LEVEL_1G)) {\r\nlast_map_addr = next;\r\ncontinue;\r\n}\r\nprot = pte_pgprot(pte_clrhuge(*(pte_t *)pud));\r\n}\r\nif (page_size_mask & (1<<PG_LEVEL_1G)) {\r\npages++;\r\nspin_lock(&init_mm.page_table_lock);\r\nset_pte((pte_t *)pud,\r\npfn_pte(addr >> PAGE_SHIFT, PAGE_KERNEL_LARGE));\r\nspin_unlock(&init_mm.page_table_lock);\r\nlast_map_addr = next;\r\ncontinue;\r\n}\r\npmd = alloc_low_page(&pmd_phys);\r\nlast_map_addr = phys_pmd_init(pmd, addr, end, page_size_mask,\r\nprot);\r\nunmap_low_page(pmd);\r\nspin_lock(&init_mm.page_table_lock);\r\npud_populate(&init_mm, pud, __va(pmd_phys));\r\nspin_unlock(&init_mm.page_table_lock);\r\n}\r\n__flush_tlb_all();\r\nupdate_page_count(PG_LEVEL_1G, pages);\r\nreturn last_map_addr;\r\n}\r\nunsigned long __meminit\r\nkernel_physical_mapping_init(unsigned long start,\r\nunsigned long end,\r\nunsigned long page_size_mask)\r\n{\r\nbool pgd_changed = false;\r\nunsigned long next, last_map_addr = end;\r\nunsigned long addr;\r\nstart = (unsigned long)__va(start);\r\nend = (unsigned long)__va(end);\r\naddr = start;\r\nfor (; start < end; start = next) {\r\npgd_t *pgd = pgd_offset_k(start);\r\nunsigned long pud_phys;\r\npud_t *pud;\r\nnext = (start + PGDIR_SIZE) & PGDIR_MASK;\r\nif (next > end)\r\nnext = end;\r\nif (pgd_val(*pgd)) {\r\npud = map_low_page((pud_t *)pgd_page_vaddr(*pgd));\r\nlast_map_addr = phys_pud_init(pud, __pa(start),\r\n__pa(end), page_size_mask);\r\nunmap_low_page(pud);\r\ncontinue;\r\n}\r\npud = alloc_low_page(&pud_phys);\r\nlast_map_addr = phys_pud_init(pud, __pa(start), __pa(next),\r\npage_size_mask);\r\nunmap_low_page(pud);\r\nspin_lock(&init_mm.page_table_lock);\r\npgd_populate(&init_mm, pgd, __va(pud_phys));\r\nspin_unlock(&init_mm.page_table_lock);\r\npgd_changed = true;\r\n}\r\nif (pgd_changed)\r\nsync_global_pgds(addr, end);\r\n__flush_tlb_all();\r\nreturn last_map_addr;\r\n}\r\nvoid __init initmem_init(void)\r\n{\r\nmemblock_set_node(0, (phys_addr_t)ULLONG_MAX, 0);\r\n}\r\nvoid __init paging_init(void)\r\n{\r\nsparse_memory_present_with_active_regions(MAX_NUMNODES);\r\nsparse_init();\r\nnode_clear_state(0, N_NORMAL_MEMORY);\r\nzone_sizes_init();\r\n}\r\nstatic void update_end_of_memory_vars(u64 start, u64 size)\r\n{\r\nunsigned long end_pfn = PFN_UP(start + size);\r\nif (end_pfn > max_pfn) {\r\nmax_pfn = end_pfn;\r\nmax_low_pfn = end_pfn;\r\nhigh_memory = (void *)__va(max_pfn * PAGE_SIZE - 1) + 1;\r\n}\r\n}\r\nint arch_add_memory(int nid, u64 start, u64 size)\r\n{\r\nstruct pglist_data *pgdat = NODE_DATA(nid);\r\nstruct zone *zone = pgdat->node_zones + ZONE_NORMAL;\r\nunsigned long last_mapped_pfn, start_pfn = start >> PAGE_SHIFT;\r\nunsigned long nr_pages = size >> PAGE_SHIFT;\r\nint ret;\r\nlast_mapped_pfn = init_memory_mapping(start, start + size);\r\nif (last_mapped_pfn > max_pfn_mapped)\r\nmax_pfn_mapped = last_mapped_pfn;\r\nret = __add_pages(nid, zone, start_pfn, nr_pages);\r\nWARN_ON_ONCE(ret);\r\nupdate_end_of_memory_vars(start, size);\r\nreturn ret;\r\n}\r\nvoid __init mem_init(void)\r\n{\r\nlong codesize, reservedpages, datasize, initsize;\r\nunsigned long absent_pages;\r\npci_iommu_alloc();\r\nreservedpages = 0;\r\n#ifdef CONFIG_NUMA\r\ntotalram_pages = numa_free_all_bootmem();\r\n#else\r\ntotalram_pages = free_all_bootmem();\r\n#endif\r\nabsent_pages = absent_pages_in_range(0, max_pfn);\r\nreservedpages = max_pfn - totalram_pages - absent_pages;\r\nafter_bootmem = 1;\r\ncodesize = (unsigned long) &_etext - (unsigned long) &_text;\r\ndatasize = (unsigned long) &_edata - (unsigned long) &_etext;\r\ninitsize = (unsigned long) &__init_end - (unsigned long) &__init_begin;\r\nkclist_add(&kcore_vsyscall, (void *)VSYSCALL_START,\r\nVSYSCALL_END - VSYSCALL_START, KCORE_OTHER);\r\nprintk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "\r\n"%ldk absent, %ldk reserved, %ldk data, %ldk init)\n",\r\nnr_free_pages() << (PAGE_SHIFT-10),\r\nmax_pfn << (PAGE_SHIFT-10),\r\ncodesize >> 10,\r\nabsent_pages << (PAGE_SHIFT-10),\r\nreservedpages << (PAGE_SHIFT-10),\r\ndatasize >> 10,\r\ninitsize >> 10);\r\n}\r\nvoid set_kernel_text_rw(void)\r\n{\r\nunsigned long start = PFN_ALIGN(_text);\r\nunsigned long end = PFN_ALIGN(__stop___ex_table);\r\nif (!kernel_set_to_readonly)\r\nreturn;\r\npr_debug("Set kernel text: %lx - %lx for read write\n",\r\nstart, end);\r\nset_memory_rw(start, (end - start) >> PAGE_SHIFT);\r\n}\r\nvoid set_kernel_text_ro(void)\r\n{\r\nunsigned long start = PFN_ALIGN(_text);\r\nunsigned long end = PFN_ALIGN(__stop___ex_table);\r\nif (!kernel_set_to_readonly)\r\nreturn;\r\npr_debug("Set kernel text: %lx - %lx for read only\n",\r\nstart, end);\r\nset_memory_ro(start, (end - start) >> PAGE_SHIFT);\r\n}\r\nvoid mark_rodata_ro(void)\r\n{\r\nunsigned long start = PFN_ALIGN(_text);\r\nunsigned long rodata_start =\r\n((unsigned long)__start_rodata + PAGE_SIZE - 1) & PAGE_MASK;\r\nunsigned long end = (unsigned long) &__end_rodata_hpage_align;\r\nunsigned long text_end = PAGE_ALIGN((unsigned long) &__stop___ex_table);\r\nunsigned long rodata_end = PAGE_ALIGN((unsigned long) &__end_rodata);\r\nunsigned long data_start = (unsigned long) &_sdata;\r\nprintk(KERN_INFO "Write protecting the kernel read-only data: %luk\n",\r\n(end - start) >> 10);\r\nset_memory_ro(start, (end - start) >> PAGE_SHIFT);\r\nkernel_set_to_readonly = 1;\r\nset_memory_nx(rodata_start, (end - rodata_start) >> PAGE_SHIFT);\r\nrodata_test();\r\n#ifdef CONFIG_CPA_DEBUG\r\nprintk(KERN_INFO "Testing CPA: undo %lx-%lx\n", start, end);\r\nset_memory_rw(start, (end-start) >> PAGE_SHIFT);\r\nprintk(KERN_INFO "Testing CPA: again\n");\r\nset_memory_ro(start, (end-start) >> PAGE_SHIFT);\r\n#endif\r\nfree_init_pages("unused kernel memory",\r\n(unsigned long) page_address(virt_to_page(text_end)),\r\n(unsigned long)\r\npage_address(virt_to_page(rodata_start)));\r\nfree_init_pages("unused kernel memory",\r\n(unsigned long) page_address(virt_to_page(rodata_end)),\r\n(unsigned long) page_address(virt_to_page(data_start)));\r\n}\r\nint kern_addr_valid(unsigned long addr)\r\n{\r\nunsigned long above = ((long)addr) >> __VIRTUAL_MASK_SHIFT;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nif (above != 0 && above != -1UL)\r\nreturn 0;\r\npgd = pgd_offset_k(addr);\r\nif (pgd_none(*pgd))\r\nreturn 0;\r\npud = pud_offset(pgd, addr);\r\nif (pud_none(*pud))\r\nreturn 0;\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd))\r\nreturn 0;\r\nif (pmd_large(*pmd))\r\nreturn pfn_valid(pmd_pfn(*pmd));\r\npte = pte_offset_kernel(pmd, addr);\r\nif (pte_none(*pte))\r\nreturn 0;\r\nreturn pfn_valid(pte_pfn(*pte));\r\n}\r\nstruct vm_area_struct *get_gate_vma(struct mm_struct *mm)\r\n{\r\n#ifdef CONFIG_IA32_EMULATION\r\nif (!mm || mm->context.ia32_compat)\r\nreturn NULL;\r\n#endif\r\nreturn &gate_vma;\r\n}\r\nint in_gate_area(struct mm_struct *mm, unsigned long addr)\r\n{\r\nstruct vm_area_struct *vma = get_gate_vma(mm);\r\nif (!vma)\r\nreturn 0;\r\nreturn (addr >= vma->vm_start) && (addr < vma->vm_end);\r\n}\r\nint in_gate_area_no_mm(unsigned long addr)\r\n{\r\nreturn (addr >= VSYSCALL_START) && (addr < VSYSCALL_END);\r\n}\r\nconst char *arch_vma_name(struct vm_area_struct *vma)\r\n{\r\nif (vma->vm_mm && vma->vm_start == (long)vma->vm_mm->context.vdso)\r\nreturn "[vdso]";\r\nif (vma == &gate_vma)\r\nreturn "[vsyscall]";\r\nreturn NULL;\r\n}\r\nunsigned long memory_block_size_bytes(void)\r\n{\r\nif (is_uv_system()) {\r\nprintk(KERN_INFO "UV: memory block size 2GB\n");\r\nreturn 2UL * 1024 * 1024 * 1024;\r\n}\r\nreturn MIN_MEMORY_BLOCK_SIZE;\r\n}\r\nint __meminit\r\nvmemmap_populate(struct page *start_page, unsigned long size, int node)\r\n{\r\nunsigned long addr = (unsigned long)start_page;\r\nunsigned long end = (unsigned long)(start_page + size);\r\nunsigned long next;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\nfor (; addr < end; addr = next) {\r\nvoid *p = NULL;\r\npgd = vmemmap_pgd_populate(addr, node);\r\nif (!pgd)\r\nreturn -ENOMEM;\r\npud = vmemmap_pud_populate(pgd, addr, node);\r\nif (!pud)\r\nreturn -ENOMEM;\r\nif (!cpu_has_pse) {\r\nnext = (addr + PAGE_SIZE) & PAGE_MASK;\r\npmd = vmemmap_pmd_populate(pud, addr, node);\r\nif (!pmd)\r\nreturn -ENOMEM;\r\np = vmemmap_pte_populate(pmd, addr, node);\r\nif (!p)\r\nreturn -ENOMEM;\r\naddr_end = addr + PAGE_SIZE;\r\np_end = p + PAGE_SIZE;\r\n} else {\r\nnext = pmd_addr_end(addr, end);\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd)) {\r\npte_t entry;\r\np = vmemmap_alloc_block_buf(PMD_SIZE, node);\r\nif (!p)\r\nreturn -ENOMEM;\r\nentry = pfn_pte(__pa(p) >> PAGE_SHIFT,\r\nPAGE_KERNEL_LARGE);\r\nset_pmd(pmd, __pmd(pte_val(entry)));\r\nif (p_end != p || node_start != node) {\r\nif (p_start)\r\nprintk(KERN_DEBUG " [%lx-%lx] PMD -> [%p-%p] on node %d\n",\r\naddr_start, addr_end-1, p_start, p_end-1, node_start);\r\naddr_start = addr;\r\nnode_start = node;\r\np_start = p;\r\n}\r\naddr_end = addr + PMD_SIZE;\r\np_end = p + PMD_SIZE;\r\n} else\r\nvmemmap_verify((pte_t *)pmd, node, addr, next);\r\n}\r\n}\r\nsync_global_pgds((unsigned long)start_page, end);\r\nreturn 0;\r\n}\r\nvoid __meminit vmemmap_populate_print_last(void)\r\n{\r\nif (p_start) {\r\nprintk(KERN_DEBUG " [%lx-%lx] PMD -> [%p-%p] on node %d\n",\r\naddr_start, addr_end-1, p_start, p_end-1, node_start);\r\np_start = NULL;\r\np_end = NULL;\r\nnode_start = 0;\r\n}\r\n}
