static inline void dvma_unmap_iommu(unsigned long a, int b)\r\n{\r\n}\r\nstatic void print_use(void)\r\n{\r\nint i;\r\nint j = 0;\r\nprintk("dvma entry usage:\n");\r\nfor(i = 0; i < IOMMU_TOTAL_ENTRIES; i++) {\r\nif(!iommu_use[i])\r\ncontinue;\r\nj++;\r\nprintk("dvma entry: %08lx len %08lx\n",\r\n( i << DVMA_PAGE_SHIFT) + DVMA_START,\r\niommu_use[i]);\r\n}\r\nprintk("%d entries in use total\n", j);\r\nprintk("allocation/free calls: %lu/%lu\n", dvma_allocs, dvma_frees);\r\nprintk("allocation/free bytes: %Lx/%Lx\n", dvma_alloc_bytes,\r\ndvma_free_bytes);\r\n}\r\nstatic void print_holes(struct list_head *holes)\r\n{\r\nstruct list_head *cur;\r\nstruct hole *hole;\r\nprintk("listing dvma holes\n");\r\nlist_for_each(cur, holes) {\r\nhole = list_entry(cur, struct hole, list);\r\nif((hole->start == 0) && (hole->end == 0) && (hole->size == 0))\r\ncontinue;\r\nprintk("hole: start %08lx end %08lx size %08lx\n", hole->start, hole->end, hole->size);\r\n}\r\nprintk("end of hole listing...\n");\r\n}\r\nstatic inline int refill(void)\r\n{\r\nstruct hole *hole;\r\nstruct hole *prev = NULL;\r\nstruct list_head *cur;\r\nint ret = 0;\r\nlist_for_each(cur, &hole_list) {\r\nhole = list_entry(cur, struct hole, list);\r\nif(!prev) {\r\nprev = hole;\r\ncontinue;\r\n}\r\nif(hole->end == prev->start) {\r\nhole->size += prev->size;\r\nhole->end = prev->end;\r\nlist_move(&(prev->list), &hole_cache);\r\nret++;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic inline struct hole *rmcache(void)\r\n{\r\nstruct hole *ret;\r\nif(list_empty(&hole_cache)) {\r\nif(!refill()) {\r\nprintk("out of dvma hole cache!\n");\r\nBUG();\r\n}\r\n}\r\nret = list_entry(hole_cache.next, struct hole, list);\r\nlist_del(&(ret->list));\r\nreturn ret;\r\n}\r\nstatic inline unsigned long get_baddr(int len, unsigned long align)\r\n{\r\nstruct list_head *cur;\r\nstruct hole *hole;\r\nif(list_empty(&hole_list)) {\r\n#ifdef DVMA_DEBUG\r\nprintk("out of dvma holes! (printing hole cache)\n");\r\nprint_holes(&hole_cache);\r\nprint_use();\r\n#endif\r\nBUG();\r\n}\r\nlist_for_each(cur, &hole_list) {\r\nunsigned long newlen;\r\nhole = list_entry(cur, struct hole, list);\r\nif(align > DVMA_PAGE_SIZE)\r\nnewlen = len + ((hole->end - len) & (align-1));\r\nelse\r\nnewlen = len;\r\nif(hole->size > newlen) {\r\nhole->end -= newlen;\r\nhole->size -= newlen;\r\ndvma_entry_use(hole->end) = newlen;\r\n#ifdef DVMA_DEBUG\r\ndvma_allocs++;\r\ndvma_alloc_bytes += newlen;\r\n#endif\r\nreturn hole->end;\r\n} else if(hole->size == newlen) {\r\nlist_move(&(hole->list), &hole_cache);\r\ndvma_entry_use(hole->start) = newlen;\r\n#ifdef DVMA_DEBUG\r\ndvma_allocs++;\r\ndvma_alloc_bytes += newlen;\r\n#endif\r\nreturn hole->start;\r\n}\r\n}\r\nprintk("unable to find dvma hole!\n");\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic inline int free_baddr(unsigned long baddr)\r\n{\r\nunsigned long len;\r\nstruct hole *hole;\r\nstruct list_head *cur;\r\nunsigned long orig_baddr;\r\norig_baddr = baddr;\r\nlen = dvma_entry_use(baddr);\r\ndvma_entry_use(baddr) = 0;\r\nbaddr &= DVMA_PAGE_MASK;\r\ndvma_unmap_iommu(baddr, len);\r\n#ifdef DVMA_DEBUG\r\ndvma_frees++;\r\ndvma_free_bytes += len;\r\n#endif\r\nlist_for_each(cur, &hole_list) {\r\nhole = list_entry(cur, struct hole, list);\r\nif(hole->end == baddr) {\r\nhole->end += len;\r\nhole->size += len;\r\nreturn 0;\r\n} else if(hole->start == (baddr + len)) {\r\nhole->start = baddr;\r\nhole->size += len;\r\nreturn 0;\r\n}\r\n}\r\nhole = rmcache();\r\nhole->start = baddr;\r\nhole->end = baddr + len;\r\nhole->size = len;\r\nlist_add(&(hole->list), cur);\r\nreturn 0;\r\n}\r\nvoid dvma_init(void)\r\n{\r\nstruct hole *hole;\r\nint i;\r\nINIT_LIST_HEAD(&hole_list);\r\nINIT_LIST_HEAD(&hole_cache);\r\nfor(i = 0; i < 64; i++)\r\nlist_add(&(initholes[i].list), &hole_cache);\r\nhole = rmcache();\r\nhole->start = DVMA_START;\r\nhole->end = DVMA_END;\r\nhole->size = DVMA_SIZE;\r\nlist_add(&(hole->list), &hole_list);\r\nmemset(iommu_use, 0, sizeof(iommu_use));\r\ndvma_unmap_iommu(DVMA_START, DVMA_SIZE);\r\n#ifdef CONFIG_SUN3\r\nsun3_dvma_init();\r\n#endif\r\n}\r\ninline unsigned long dvma_map_align(unsigned long kaddr, int len, int align)\r\n{\r\nunsigned long baddr;\r\nunsigned long off;\r\nif(!len)\r\nlen = 0x800;\r\nif(!kaddr || !len) {\r\nreturn 0;\r\n}\r\n#ifdef DEBUG\r\nprintk("dvma_map request %08lx bytes from %08lx\n",\r\nlen, kaddr);\r\n#endif\r\noff = kaddr & ~DVMA_PAGE_MASK;\r\nkaddr &= PAGE_MASK;\r\nlen += off;\r\nlen = ((len + (DVMA_PAGE_SIZE-1)) & DVMA_PAGE_MASK);\r\nif(align == 0)\r\nalign = DVMA_PAGE_SIZE;\r\nelse\r\nalign = ((align + (DVMA_PAGE_SIZE-1)) & DVMA_PAGE_MASK);\r\nbaddr = get_baddr(len, align);\r\nif(!dvma_map_iommu(kaddr, baddr, len))\r\nreturn (baddr + off);\r\nprintk("dvma_map failed kaddr %lx baddr %lx len %x\n", kaddr, baddr, len);\r\nBUG();\r\nreturn 0;\r\n}\r\nvoid dvma_unmap(void *baddr)\r\n{\r\nunsigned long addr;\r\naddr = (unsigned long)baddr;\r\nif(!(addr & 0x00f00000))\r\naddr |= 0xf00000;\r\nfree_baddr(addr);\r\nreturn;\r\n}\r\nvoid *dvma_malloc_align(unsigned long len, unsigned long align)\r\n{\r\nunsigned long kaddr;\r\nunsigned long baddr;\r\nunsigned long vaddr;\r\nif(!len)\r\nreturn NULL;\r\n#ifdef DEBUG\r\nprintk("dvma_malloc request %lx bytes\n", len);\r\n#endif\r\nlen = ((len + (DVMA_PAGE_SIZE-1)) & DVMA_PAGE_MASK);\r\nif((kaddr = __get_free_pages(GFP_ATOMIC, get_order(len))) == 0)\r\nreturn NULL;\r\nif((baddr = (unsigned long)dvma_map_align(kaddr, len, align)) == 0) {\r\nfree_pages(kaddr, get_order(len));\r\nreturn NULL;\r\n}\r\nvaddr = dvma_btov(baddr);\r\nif(dvma_map_cpu(kaddr, vaddr, len) < 0) {\r\ndvma_unmap((void *)baddr);\r\nfree_pages(kaddr, get_order(len));\r\nreturn NULL;\r\n}\r\n#ifdef DEBUG\r\nprintk("mapped %08lx bytes %08lx kern -> %08lx bus\n",\r\nlen, kaddr, baddr);\r\n#endif\r\nreturn (void *)vaddr;\r\n}\r\nvoid dvma_free(void *vaddr)\r\n{\r\nreturn;\r\n}
