const char *xenbus_strstate(enum xenbus_state state)\r\n{\r\nstatic const char *const name[] = {\r\n[ XenbusStateUnknown ] = "Unknown",\r\n[ XenbusStateInitialising ] = "Initialising",\r\n[ XenbusStateInitWait ] = "InitWait",\r\n[ XenbusStateInitialised ] = "Initialised",\r\n[ XenbusStateConnected ] = "Connected",\r\n[ XenbusStateClosing ] = "Closing",\r\n[ XenbusStateClosed ] = "Closed",\r\n[XenbusStateReconfiguring] = "Reconfiguring",\r\n[XenbusStateReconfigured] = "Reconfigured",\r\n};\r\nreturn (state < ARRAY_SIZE(name)) ? name[state] : "INVALID";\r\n}\r\nint xenbus_watch_path(struct xenbus_device *dev, const char *path,\r\nstruct xenbus_watch *watch,\r\nvoid (*callback)(struct xenbus_watch *,\r\nconst char **, unsigned int))\r\n{\r\nint err;\r\nwatch->node = path;\r\nwatch->callback = callback;\r\nerr = register_xenbus_watch(watch);\r\nif (err) {\r\nwatch->node = NULL;\r\nwatch->callback = NULL;\r\nxenbus_dev_fatal(dev, err, "adding watch on %s", path);\r\n}\r\nreturn err;\r\n}\r\nint xenbus_watch_pathfmt(struct xenbus_device *dev,\r\nstruct xenbus_watch *watch,\r\nvoid (*callback)(struct xenbus_watch *,\r\nconst char **, unsigned int),\r\nconst char *pathfmt, ...)\r\n{\r\nint err;\r\nva_list ap;\r\nchar *path;\r\nva_start(ap, pathfmt);\r\npath = kvasprintf(GFP_NOIO | __GFP_HIGH, pathfmt, ap);\r\nva_end(ap);\r\nif (!path) {\r\nxenbus_dev_fatal(dev, -ENOMEM, "allocating path for watch");\r\nreturn -ENOMEM;\r\n}\r\nerr = xenbus_watch_path(dev, path, watch, callback);\r\nif (err)\r\nkfree(path);\r\nreturn err;\r\n}\r\nstatic int\r\n__xenbus_switch_state(struct xenbus_device *dev,\r\nenum xenbus_state state, int depth)\r\n{\r\nstruct xenbus_transaction xbt;\r\nint current_state;\r\nint err, abort;\r\nif (state == dev->state)\r\nreturn 0;\r\nagain:\r\nabort = 1;\r\nerr = xenbus_transaction_start(&xbt);\r\nif (err) {\r\nxenbus_switch_fatal(dev, depth, err, "starting transaction");\r\nreturn 0;\r\n}\r\nerr = xenbus_scanf(xbt, dev->nodename, "state", "%d", &current_state);\r\nif (err != 1)\r\ngoto abort;\r\nerr = xenbus_printf(xbt, dev->nodename, "state", "%d", state);\r\nif (err) {\r\nxenbus_switch_fatal(dev, depth, err, "writing new state");\r\ngoto abort;\r\n}\r\nabort = 0;\r\nabort:\r\nerr = xenbus_transaction_end(xbt, abort);\r\nif (err) {\r\nif (err == -EAGAIN && !abort)\r\ngoto again;\r\nxenbus_switch_fatal(dev, depth, err, "ending transaction");\r\n} else\r\ndev->state = state;\r\nreturn 0;\r\n}\r\nint xenbus_switch_state(struct xenbus_device *dev, enum xenbus_state state)\r\n{\r\nreturn __xenbus_switch_state(dev, state, 0);\r\n}\r\nint xenbus_frontend_closed(struct xenbus_device *dev)\r\n{\r\nxenbus_switch_state(dev, XenbusStateClosed);\r\ncomplete(&dev->down);\r\nreturn 0;\r\n}\r\nstatic char *error_path(struct xenbus_device *dev)\r\n{\r\nreturn kasprintf(GFP_KERNEL, "error/%s", dev->nodename);\r\n}\r\nstatic void xenbus_va_dev_error(struct xenbus_device *dev, int err,\r\nconst char *fmt, va_list ap)\r\n{\r\nint ret;\r\nunsigned int len;\r\nchar *printf_buffer = NULL;\r\nchar *path_buffer = NULL;\r\n#define PRINTF_BUFFER_SIZE 4096\r\nprintf_buffer = kmalloc(PRINTF_BUFFER_SIZE, GFP_KERNEL);\r\nif (printf_buffer == NULL)\r\ngoto fail;\r\nlen = sprintf(printf_buffer, "%i ", -err);\r\nret = vsnprintf(printf_buffer+len, PRINTF_BUFFER_SIZE-len, fmt, ap);\r\nBUG_ON(len + ret > PRINTF_BUFFER_SIZE-1);\r\ndev_err(&dev->dev, "%s\n", printf_buffer);\r\npath_buffer = error_path(dev);\r\nif (path_buffer == NULL) {\r\ndev_err(&dev->dev, "failed to write error node for %s (%s)\n",\r\ndev->nodename, printf_buffer);\r\ngoto fail;\r\n}\r\nif (xenbus_write(XBT_NIL, path_buffer, "error", printf_buffer) != 0) {\r\ndev_err(&dev->dev, "failed to write error node for %s (%s)\n",\r\ndev->nodename, printf_buffer);\r\ngoto fail;\r\n}\r\nfail:\r\nkfree(printf_buffer);\r\nkfree(path_buffer);\r\n}\r\nvoid xenbus_dev_error(struct xenbus_device *dev, int err, const char *fmt, ...)\r\n{\r\nva_list ap;\r\nva_start(ap, fmt);\r\nxenbus_va_dev_error(dev, err, fmt, ap);\r\nva_end(ap);\r\n}\r\nvoid xenbus_dev_fatal(struct xenbus_device *dev, int err, const char *fmt, ...)\r\n{\r\nva_list ap;\r\nva_start(ap, fmt);\r\nxenbus_va_dev_error(dev, err, fmt, ap);\r\nva_end(ap);\r\nxenbus_switch_state(dev, XenbusStateClosing);\r\n}\r\nstatic void xenbus_switch_fatal(struct xenbus_device *dev, int depth, int err,\r\nconst char *fmt, ...)\r\n{\r\nva_list ap;\r\nva_start(ap, fmt);\r\nxenbus_va_dev_error(dev, err, fmt, ap);\r\nva_end(ap);\r\nif (!depth)\r\n__xenbus_switch_state(dev, XenbusStateClosing, 1);\r\n}\r\nint xenbus_grant_ring(struct xenbus_device *dev, unsigned long ring_mfn)\r\n{\r\nint err = gnttab_grant_foreign_access(dev->otherend_id, ring_mfn, 0);\r\nif (err < 0)\r\nxenbus_dev_fatal(dev, err, "granting access to ring page");\r\nreturn err;\r\n}\r\nint xenbus_alloc_evtchn(struct xenbus_device *dev, int *port)\r\n{\r\nstruct evtchn_alloc_unbound alloc_unbound;\r\nint err;\r\nalloc_unbound.dom = DOMID_SELF;\r\nalloc_unbound.remote_dom = dev->otherend_id;\r\nerr = HYPERVISOR_event_channel_op(EVTCHNOP_alloc_unbound,\r\n&alloc_unbound);\r\nif (err)\r\nxenbus_dev_fatal(dev, err, "allocating event channel");\r\nelse\r\n*port = alloc_unbound.port;\r\nreturn err;\r\n}\r\nint xenbus_bind_evtchn(struct xenbus_device *dev, int remote_port, int *port)\r\n{\r\nstruct evtchn_bind_interdomain bind_interdomain;\r\nint err;\r\nbind_interdomain.remote_dom = dev->otherend_id;\r\nbind_interdomain.remote_port = remote_port;\r\nerr = HYPERVISOR_event_channel_op(EVTCHNOP_bind_interdomain,\r\n&bind_interdomain);\r\nif (err)\r\nxenbus_dev_fatal(dev, err,\r\n"binding to event channel %d from domain %d",\r\nremote_port, dev->otherend_id);\r\nelse\r\n*port = bind_interdomain.local_port;\r\nreturn err;\r\n}\r\nint xenbus_free_evtchn(struct xenbus_device *dev, int port)\r\n{\r\nstruct evtchn_close close;\r\nint err;\r\nclose.port = port;\r\nerr = HYPERVISOR_event_channel_op(EVTCHNOP_close, &close);\r\nif (err)\r\nxenbus_dev_error(dev, err, "freeing event channel %d", port);\r\nreturn err;\r\n}\r\nint xenbus_map_ring_valloc(struct xenbus_device *dev, int gnt_ref, void **vaddr)\r\n{\r\nreturn ring_ops->map(dev, gnt_ref, vaddr);\r\n}\r\nstatic int xenbus_map_ring_valloc_pv(struct xenbus_device *dev,\r\nint gnt_ref, void **vaddr)\r\n{\r\nstruct gnttab_map_grant_ref op = {\r\n.flags = GNTMAP_host_map | GNTMAP_contains_pte,\r\n.ref = gnt_ref,\r\n.dom = dev->otherend_id,\r\n};\r\nstruct xenbus_map_node *node;\r\nstruct vm_struct *area;\r\npte_t *pte;\r\n*vaddr = NULL;\r\nnode = kzalloc(sizeof(*node), GFP_KERNEL);\r\nif (!node)\r\nreturn -ENOMEM;\r\narea = alloc_vm_area(PAGE_SIZE, &pte);\r\nif (!area) {\r\nkfree(node);\r\nreturn -ENOMEM;\r\n}\r\nop.host_addr = arbitrary_virt_to_machine(pte).maddr;\r\nif (HYPERVISOR_grant_table_op(GNTTABOP_map_grant_ref, &op, 1))\r\nBUG();\r\nif (op.status != GNTST_okay) {\r\nfree_vm_area(area);\r\nkfree(node);\r\nxenbus_dev_fatal(dev, op.status,\r\n"mapping in shared page %d from domain %d",\r\ngnt_ref, dev->otherend_id);\r\nreturn op.status;\r\n}\r\nnode->handle = op.handle;\r\nnode->area = area;\r\nspin_lock(&xenbus_valloc_lock);\r\nlist_add(&node->next, &xenbus_valloc_pages);\r\nspin_unlock(&xenbus_valloc_lock);\r\n*vaddr = area->addr;\r\nreturn 0;\r\n}\r\nstatic int xenbus_map_ring_valloc_hvm(struct xenbus_device *dev,\r\nint gnt_ref, void **vaddr)\r\n{\r\nstruct xenbus_map_node *node;\r\nint err;\r\nvoid *addr;\r\n*vaddr = NULL;\r\nnode = kzalloc(sizeof(*node), GFP_KERNEL);\r\nif (!node)\r\nreturn -ENOMEM;\r\nerr = alloc_xenballooned_pages(1, &node->page, false );\r\nif (err)\r\ngoto out_err;\r\naddr = pfn_to_kaddr(page_to_pfn(node->page));\r\nerr = xenbus_map_ring(dev, gnt_ref, &node->handle, addr);\r\nif (err)\r\ngoto out_err;\r\nspin_lock(&xenbus_valloc_lock);\r\nlist_add(&node->next, &xenbus_valloc_pages);\r\nspin_unlock(&xenbus_valloc_lock);\r\n*vaddr = addr;\r\nreturn 0;\r\nout_err:\r\nfree_xenballooned_pages(1, &node->page);\r\nkfree(node);\r\nreturn err;\r\n}\r\nint xenbus_map_ring(struct xenbus_device *dev, int gnt_ref,\r\ngrant_handle_t *handle, void *vaddr)\r\n{\r\nstruct gnttab_map_grant_ref op;\r\ngnttab_set_map_op(&op, (unsigned long)vaddr, GNTMAP_host_map, gnt_ref,\r\ndev->otherend_id);\r\nif (HYPERVISOR_grant_table_op(GNTTABOP_map_grant_ref, &op, 1))\r\nBUG();\r\nif (op.status != GNTST_okay) {\r\nxenbus_dev_fatal(dev, op.status,\r\n"mapping in shared page %d from domain %d",\r\ngnt_ref, dev->otherend_id);\r\n} else\r\n*handle = op.handle;\r\nreturn op.status;\r\n}\r\nint xenbus_unmap_ring_vfree(struct xenbus_device *dev, void *vaddr)\r\n{\r\nreturn ring_ops->unmap(dev, vaddr);\r\n}\r\nstatic int xenbus_unmap_ring_vfree_pv(struct xenbus_device *dev, void *vaddr)\r\n{\r\nstruct xenbus_map_node *node;\r\nstruct gnttab_unmap_grant_ref op = {\r\n.host_addr = (unsigned long)vaddr,\r\n};\r\nunsigned int level;\r\nspin_lock(&xenbus_valloc_lock);\r\nlist_for_each_entry(node, &xenbus_valloc_pages, next) {\r\nif (node->area->addr == vaddr) {\r\nlist_del(&node->next);\r\ngoto found;\r\n}\r\n}\r\nnode = NULL;\r\nfound:\r\nspin_unlock(&xenbus_valloc_lock);\r\nif (!node) {\r\nxenbus_dev_error(dev, -ENOENT,\r\n"can't find mapped virtual address %p", vaddr);\r\nreturn GNTST_bad_virt_addr;\r\n}\r\nop.handle = node->handle;\r\nop.host_addr = arbitrary_virt_to_machine(\r\nlookup_address((unsigned long)vaddr, &level)).maddr;\r\nif (HYPERVISOR_grant_table_op(GNTTABOP_unmap_grant_ref, &op, 1))\r\nBUG();\r\nif (op.status == GNTST_okay)\r\nfree_vm_area(node->area);\r\nelse\r\nxenbus_dev_error(dev, op.status,\r\n"unmapping page at handle %d error %d",\r\nnode->handle, op.status);\r\nkfree(node);\r\nreturn op.status;\r\n}\r\nstatic int xenbus_unmap_ring_vfree_hvm(struct xenbus_device *dev, void *vaddr)\r\n{\r\nint rv;\r\nstruct xenbus_map_node *node;\r\nvoid *addr;\r\nspin_lock(&xenbus_valloc_lock);\r\nlist_for_each_entry(node, &xenbus_valloc_pages, next) {\r\naddr = pfn_to_kaddr(page_to_pfn(node->page));\r\nif (addr == vaddr) {\r\nlist_del(&node->next);\r\ngoto found;\r\n}\r\n}\r\nnode = addr = NULL;\r\nfound:\r\nspin_unlock(&xenbus_valloc_lock);\r\nif (!node) {\r\nxenbus_dev_error(dev, -ENOENT,\r\n"can't find mapped virtual address %p", vaddr);\r\nreturn GNTST_bad_virt_addr;\r\n}\r\nrv = xenbus_unmap_ring(dev, node->handle, addr);\r\nif (!rv)\r\nfree_xenballooned_pages(1, &node->page);\r\nelse\r\nWARN(1, "Leaking %p\n", vaddr);\r\nkfree(node);\r\nreturn rv;\r\n}\r\nint xenbus_unmap_ring(struct xenbus_device *dev,\r\ngrant_handle_t handle, void *vaddr)\r\n{\r\nstruct gnttab_unmap_grant_ref op;\r\ngnttab_set_unmap_op(&op, (unsigned long)vaddr, GNTMAP_host_map, handle);\r\nif (HYPERVISOR_grant_table_op(GNTTABOP_unmap_grant_ref, &op, 1))\r\nBUG();\r\nif (op.status != GNTST_okay)\r\nxenbus_dev_error(dev, op.status,\r\n"unmapping page at handle %d error %d",\r\nhandle, op.status);\r\nreturn op.status;\r\n}\r\nenum xenbus_state xenbus_read_driver_state(const char *path)\r\n{\r\nenum xenbus_state result;\r\nint err = xenbus_gather(XBT_NIL, path, "state", "%d", &result, NULL);\r\nif (err)\r\nresult = XenbusStateUnknown;\r\nreturn result;\r\n}\r\nvoid __init xenbus_ring_ops_init(void)\r\n{\r\nif (xen_pv_domain())\r\nring_ops = &ring_ops_pv;\r\nelse\r\nring_ops = &ring_ops_hvm;\r\n}
