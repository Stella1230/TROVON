static unsigned abs_index(struct efx_vf *vf, unsigned index)\r\n{\r\nreturn EFX_VI_BASE + vf->index * efx_vf_size(vf->efx) + index;\r\n}\r\nstatic int efx_sriov_cmd(struct efx_nic *efx, bool enable,\r\nunsigned *vi_scale_out, unsigned *vf_total_out)\r\n{\r\nu8 inbuf[MC_CMD_SRIOV_IN_LEN];\r\nu8 outbuf[MC_CMD_SRIOV_OUT_LEN];\r\nunsigned vi_scale, vf_total;\r\nsize_t outlen;\r\nint rc;\r\nMCDI_SET_DWORD(inbuf, SRIOV_IN_ENABLE, enable ? 1 : 0);\r\nMCDI_SET_DWORD(inbuf, SRIOV_IN_VI_BASE, EFX_VI_BASE);\r\nMCDI_SET_DWORD(inbuf, SRIOV_IN_VF_COUNT, efx->vf_count);\r\nrc = efx_mcdi_rpc(efx, MC_CMD_SRIOV, inbuf, MC_CMD_SRIOV_IN_LEN,\r\noutbuf, MC_CMD_SRIOV_OUT_LEN, &outlen);\r\nif (rc)\r\nreturn rc;\r\nif (outlen < MC_CMD_SRIOV_OUT_LEN)\r\nreturn -EIO;\r\nvf_total = MCDI_DWORD(outbuf, SRIOV_OUT_VF_TOTAL);\r\nvi_scale = MCDI_DWORD(outbuf, SRIOV_OUT_VI_SCALE);\r\nif (vi_scale > EFX_VI_SCALE_MAX)\r\nreturn -EOPNOTSUPP;\r\nif (vi_scale_out)\r\n*vi_scale_out = vi_scale;\r\nif (vf_total_out)\r\n*vf_total_out = vf_total;\r\nreturn 0;\r\n}\r\nstatic void efx_sriov_usrev(struct efx_nic *efx, bool enabled)\r\n{\r\nefx_oword_t reg;\r\nEFX_POPULATE_OWORD_2(reg,\r\nFRF_CZ_USREV_DIS, enabled ? 0 : 1,\r\nFRF_CZ_DFLT_EVQ, efx->vfdi_channel->channel);\r\nefx_writeo(efx, &reg, FR_CZ_USR_EV_CFG);\r\n}\r\nstatic int efx_sriov_memcpy(struct efx_nic *efx, struct efx_memcpy_req *req,\r\nunsigned int count)\r\n{\r\nu8 *inbuf, *record;\r\nunsigned int used;\r\nu32 from_rid, from_hi, from_lo;\r\nint rc;\r\nmb();\r\nused = MC_CMD_MEMCPY_IN_LEN(count);\r\nif (WARN_ON(used > MCDI_CTL_SDU_LEN_MAX))\r\nreturn -ENOBUFS;\r\ninbuf = kzalloc(MCDI_CTL_SDU_LEN_MAX, GFP_KERNEL);\r\nif (inbuf == NULL)\r\nreturn -ENOMEM;\r\nrecord = inbuf;\r\nMCDI_SET_DWORD(record, MEMCPY_IN_RECORD, count);\r\nwhile (count-- > 0) {\r\nMCDI_SET_DWORD(record, MEMCPY_RECORD_TYPEDEF_TO_RID,\r\nreq->to_rid);\r\nMCDI_SET_DWORD(record, MEMCPY_RECORD_TYPEDEF_TO_ADDR_LO,\r\n(u32)req->to_addr);\r\nMCDI_SET_DWORD(record, MEMCPY_RECORD_TYPEDEF_TO_ADDR_HI,\r\n(u32)(req->to_addr >> 32));\r\nif (req->from_buf == NULL) {\r\nfrom_rid = req->from_rid;\r\nfrom_lo = (u32)req->from_addr;\r\nfrom_hi = (u32)(req->from_addr >> 32);\r\n} else {\r\nif (WARN_ON(used + req->length > MCDI_CTL_SDU_LEN_MAX)) {\r\nrc = -ENOBUFS;\r\ngoto out;\r\n}\r\nfrom_rid = MC_CMD_MEMCPY_RECORD_TYPEDEF_RID_INLINE;\r\nfrom_lo = used;\r\nfrom_hi = 0;\r\nmemcpy(inbuf + used, req->from_buf, req->length);\r\nused += req->length;\r\n}\r\nMCDI_SET_DWORD(record, MEMCPY_RECORD_TYPEDEF_FROM_RID, from_rid);\r\nMCDI_SET_DWORD(record, MEMCPY_RECORD_TYPEDEF_FROM_ADDR_LO,\r\nfrom_lo);\r\nMCDI_SET_DWORD(record, MEMCPY_RECORD_TYPEDEF_FROM_ADDR_HI,\r\nfrom_hi);\r\nMCDI_SET_DWORD(record, MEMCPY_RECORD_TYPEDEF_LENGTH,\r\nreq->length);\r\n++req;\r\nrecord += MC_CMD_MEMCPY_IN_RECORD_LEN;\r\n}\r\nrc = efx_mcdi_rpc(efx, MC_CMD_MEMCPY, inbuf, used, NULL, 0, NULL);\r\nout:\r\nkfree(inbuf);\r\nmb();\r\nreturn rc;\r\n}\r\nstatic void efx_sriov_reset_tx_filter(struct efx_vf *vf)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nstruct efx_filter_spec filter;\r\nu16 vlan;\r\nint rc;\r\nif (vf->tx_filter_id != -1) {\r\nefx_filter_remove_id_safe(efx, EFX_FILTER_PRI_REQUIRED,\r\nvf->tx_filter_id);\r\nnetif_dbg(efx, hw, efx->net_dev, "Removed vf %s tx filter %d\n",\r\nvf->pci_name, vf->tx_filter_id);\r\nvf->tx_filter_id = -1;\r\n}\r\nif (is_zero_ether_addr(vf->addr.mac_addr))\r\nreturn;\r\nif (vf->tx_filter_mode == VF_TX_FILTER_AUTO && vf_max_tx_channels <= 2)\r\nvf->tx_filter_mode = VF_TX_FILTER_ON;\r\nvlan = ntohs(vf->addr.tci) & VLAN_VID_MASK;\r\nefx_filter_init_tx(&filter, abs_index(vf, 0));\r\nrc = efx_filter_set_eth_local(&filter,\r\nvlan ? vlan : EFX_FILTER_VID_UNSPEC,\r\nvf->addr.mac_addr);\r\nBUG_ON(rc);\r\nrc = efx_filter_insert_filter(efx, &filter, true);\r\nif (rc < 0) {\r\nnetif_warn(efx, hw, efx->net_dev,\r\n"Unable to migrate tx filter for vf %s\n",\r\nvf->pci_name);\r\n} else {\r\nnetif_dbg(efx, hw, efx->net_dev, "Inserted vf %s tx filter %d\n",\r\nvf->pci_name, rc);\r\nvf->tx_filter_id = rc;\r\n}\r\n}\r\nstatic void efx_sriov_reset_rx_filter(struct efx_vf *vf)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nstruct efx_filter_spec filter;\r\nu16 vlan;\r\nint rc;\r\nif (vf->rx_filter_id != -1) {\r\nefx_filter_remove_id_safe(efx, EFX_FILTER_PRI_REQUIRED,\r\nvf->rx_filter_id);\r\nnetif_dbg(efx, hw, efx->net_dev, "Removed vf %s rx filter %d\n",\r\nvf->pci_name, vf->rx_filter_id);\r\nvf->rx_filter_id = -1;\r\n}\r\nif (!vf->rx_filtering || is_zero_ether_addr(vf->addr.mac_addr))\r\nreturn;\r\nvlan = ntohs(vf->addr.tci) & VLAN_VID_MASK;\r\nefx_filter_init_rx(&filter, EFX_FILTER_PRI_REQUIRED,\r\nvf->rx_filter_flags,\r\nabs_index(vf, vf->rx_filter_qid));\r\nrc = efx_filter_set_eth_local(&filter,\r\nvlan ? vlan : EFX_FILTER_VID_UNSPEC,\r\nvf->addr.mac_addr);\r\nBUG_ON(rc);\r\nrc = efx_filter_insert_filter(efx, &filter, true);\r\nif (rc < 0) {\r\nnetif_warn(efx, hw, efx->net_dev,\r\n"Unable to insert rx filter for vf %s\n",\r\nvf->pci_name);\r\n} else {\r\nnetif_dbg(efx, hw, efx->net_dev, "Inserted vf %s rx filter %d\n",\r\nvf->pci_name, rc);\r\nvf->rx_filter_id = rc;\r\n}\r\n}\r\nstatic void __efx_sriov_update_vf_addr(struct efx_vf *vf)\r\n{\r\nefx_sriov_reset_tx_filter(vf);\r\nefx_sriov_reset_rx_filter(vf);\r\nqueue_work(vfdi_workqueue, &vf->efx->peer_work);\r\n}\r\nstatic void __efx_sriov_push_vf_status(struct efx_vf *vf)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nstruct vfdi_status *status = efx->vfdi_status.addr;\r\nstruct efx_memcpy_req copy[4];\r\nstruct efx_endpoint_page *epp;\r\nunsigned int pos, count;\r\nunsigned data_offset;\r\nefx_qword_t event;\r\nWARN_ON(!mutex_is_locked(&vf->status_lock));\r\nWARN_ON(!vf->status_addr);\r\nstatus->local = vf->addr;\r\nstatus->generation_end = ++status->generation_start;\r\nmemset(copy, '\0', sizeof(copy));\r\ncopy[0].from_buf = &status->generation_start;\r\ncopy[0].to_rid = vf->pci_rid;\r\ncopy[0].to_addr = vf->status_addr + offsetof(struct vfdi_status,\r\ngeneration_start);\r\ncopy[0].length = sizeof(status->generation_start);\r\ndata_offset = offsetof(struct vfdi_status, version);\r\ncopy[1].from_rid = efx->pci_dev->devfn;\r\ncopy[1].from_addr = efx->vfdi_status.dma_addr + data_offset;\r\ncopy[1].to_rid = vf->pci_rid;\r\ncopy[1].to_addr = vf->status_addr + data_offset;\r\ncopy[1].length = status->length - data_offset;\r\npos = 2;\r\ncount = 0;\r\nlist_for_each_entry(epp, &efx->local_page_list, link) {\r\nif (count == vf->peer_page_count) {\r\nbreak;\r\n}\r\ncopy[pos].from_buf = NULL;\r\ncopy[pos].from_rid = efx->pci_dev->devfn;\r\ncopy[pos].from_addr = epp->addr;\r\ncopy[pos].to_rid = vf->pci_rid;\r\ncopy[pos].to_addr = vf->peer_page_addrs[count];\r\ncopy[pos].length = EFX_PAGE_SIZE;\r\nif (++pos == ARRAY_SIZE(copy)) {\r\nefx_sriov_memcpy(efx, copy, ARRAY_SIZE(copy));\r\npos = 0;\r\n}\r\n++count;\r\n}\r\ncopy[pos].from_buf = &status->generation_end;\r\ncopy[pos].to_rid = vf->pci_rid;\r\ncopy[pos].to_addr = vf->status_addr + offsetof(struct vfdi_status,\r\ngeneration_end);\r\ncopy[pos].length = sizeof(status->generation_end);\r\nefx_sriov_memcpy(efx, copy, pos + 1);\r\nEFX_POPULATE_QWORD_3(event,\r\nFSF_AZ_EV_CODE, FSE_CZ_EV_CODE_USER_EV,\r\nVFDI_EV_SEQ, (vf->msg_seqno & 0xff),\r\nVFDI_EV_TYPE, VFDI_EV_TYPE_STATUS);\r\n++vf->msg_seqno;\r\nefx_generate_event(efx, EFX_VI_BASE + vf->index * efx_vf_size(efx),\r\n&event);\r\n}\r\nstatic void efx_sriov_bufs(struct efx_nic *efx, unsigned offset,\r\nu64 *addr, unsigned count)\r\n{\r\nefx_qword_t buf;\r\nunsigned pos;\r\nfor (pos = 0; pos < count; ++pos) {\r\nEFX_POPULATE_QWORD_3(buf,\r\nFRF_AZ_BUF_ADR_REGION, 0,\r\nFRF_AZ_BUF_ADR_FBUF,\r\naddr ? addr[pos] >> 12 : 0,\r\nFRF_AZ_BUF_OWNER_ID_FBUF, 0);\r\nefx_sram_writeq(efx, efx->membase + FR_BZ_BUF_FULL_TBL,\r\n&buf, offset + pos);\r\n}\r\n}\r\nstatic bool bad_vf_index(struct efx_nic *efx, unsigned index)\r\n{\r\nreturn index >= efx_vf_size(efx);\r\n}\r\nstatic bool bad_buf_count(unsigned buf_count, unsigned max_entry_count)\r\n{\r\nunsigned max_buf_count = max_entry_count *\r\nsizeof(efx_qword_t) / EFX_BUF_SIZE;\r\nreturn ((buf_count & (buf_count - 1)) || buf_count > max_buf_count);\r\n}\r\nstatic bool map_vi_index(struct efx_nic *efx, unsigned abs_index,\r\nstruct efx_vf **vf_out, unsigned *rel_index_out)\r\n{\r\nunsigned vf_i;\r\nif (abs_index < EFX_VI_BASE)\r\nreturn true;\r\nvf_i = (abs_index - EFX_VI_BASE) / efx_vf_size(efx);\r\nif (vf_i >= efx->vf_init_count)\r\nreturn true;\r\nif (vf_out)\r\n*vf_out = efx->vf + vf_i;\r\nif (rel_index_out)\r\n*rel_index_out = abs_index % efx_vf_size(efx);\r\nreturn false;\r\n}\r\nstatic int efx_vfdi_init_evq(struct efx_vf *vf)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nstruct vfdi_req *req = vf->buf.addr;\r\nunsigned vf_evq = req->u.init_evq.index;\r\nunsigned buf_count = req->u.init_evq.buf_count;\r\nunsigned abs_evq = abs_index(vf, vf_evq);\r\nunsigned buftbl = EFX_BUFTBL_EVQ_BASE(vf, vf_evq);\r\nefx_oword_t reg;\r\nif (bad_vf_index(efx, vf_evq) ||\r\nbad_buf_count(buf_count, EFX_MAX_VF_EVQ_SIZE)) {\r\nif (net_ratelimit())\r\nnetif_err(efx, hw, efx->net_dev,\r\n"ERROR: Invalid INIT_EVQ from %s: evq %d bufs %d\n",\r\nvf->pci_name, vf_evq, buf_count);\r\nreturn VFDI_RC_EINVAL;\r\n}\r\nefx_sriov_bufs(efx, buftbl, req->u.init_evq.addr, buf_count);\r\nEFX_POPULATE_OWORD_3(reg,\r\nFRF_CZ_TIMER_Q_EN, 1,\r\nFRF_CZ_HOST_NOTIFY_MODE, 0,\r\nFRF_CZ_TIMER_MODE, FFE_CZ_TIMER_MODE_DIS);\r\nefx_writeo_table(efx, &reg, FR_BZ_TIMER_TBL, abs_evq);\r\nEFX_POPULATE_OWORD_3(reg,\r\nFRF_AZ_EVQ_EN, 1,\r\nFRF_AZ_EVQ_SIZE, __ffs(buf_count),\r\nFRF_AZ_EVQ_BUF_BASE_ID, buftbl);\r\nefx_writeo_table(efx, &reg, FR_BZ_EVQ_PTR_TBL, abs_evq);\r\nif (vf_evq == 0) {\r\nmemcpy(vf->evq0_addrs, req->u.init_evq.addr,\r\nbuf_count * sizeof(u64));\r\nvf->evq0_count = buf_count;\r\n}\r\nreturn VFDI_RC_SUCCESS;\r\n}\r\nstatic int efx_vfdi_init_rxq(struct efx_vf *vf)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nstruct vfdi_req *req = vf->buf.addr;\r\nunsigned vf_rxq = req->u.init_rxq.index;\r\nunsigned vf_evq = req->u.init_rxq.evq;\r\nunsigned buf_count = req->u.init_rxq.buf_count;\r\nunsigned buftbl = EFX_BUFTBL_RXQ_BASE(vf, vf_rxq);\r\nunsigned label;\r\nefx_oword_t reg;\r\nif (bad_vf_index(efx, vf_evq) || bad_vf_index(efx, vf_rxq) ||\r\nbad_buf_count(buf_count, EFX_MAX_DMAQ_SIZE)) {\r\nif (net_ratelimit())\r\nnetif_err(efx, hw, efx->net_dev,\r\n"ERROR: Invalid INIT_RXQ from %s: rxq %d evq %d "\r\n"buf_count %d\n", vf->pci_name, vf_rxq,\r\nvf_evq, buf_count);\r\nreturn VFDI_RC_EINVAL;\r\n}\r\nif (__test_and_set_bit(req->u.init_rxq.index, vf->rxq_mask))\r\n++vf->rxq_count;\r\nefx_sriov_bufs(efx, buftbl, req->u.init_rxq.addr, buf_count);\r\nlabel = req->u.init_rxq.label & EFX_FIELD_MASK(FRF_AZ_RX_DESCQ_LABEL);\r\nEFX_POPULATE_OWORD_6(reg,\r\nFRF_AZ_RX_DESCQ_BUF_BASE_ID, buftbl,\r\nFRF_AZ_RX_DESCQ_EVQ_ID, abs_index(vf, vf_evq),\r\nFRF_AZ_RX_DESCQ_LABEL, label,\r\nFRF_AZ_RX_DESCQ_SIZE, __ffs(buf_count),\r\nFRF_AZ_RX_DESCQ_JUMBO,\r\n!!(req->u.init_rxq.flags &\r\nVFDI_RXQ_FLAG_SCATTER_EN),\r\nFRF_AZ_RX_DESCQ_EN, 1);\r\nefx_writeo_table(efx, &reg, FR_BZ_RX_DESC_PTR_TBL,\r\nabs_index(vf, vf_rxq));\r\nreturn VFDI_RC_SUCCESS;\r\n}\r\nstatic int efx_vfdi_init_txq(struct efx_vf *vf)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nstruct vfdi_req *req = vf->buf.addr;\r\nunsigned vf_txq = req->u.init_txq.index;\r\nunsigned vf_evq = req->u.init_txq.evq;\r\nunsigned buf_count = req->u.init_txq.buf_count;\r\nunsigned buftbl = EFX_BUFTBL_TXQ_BASE(vf, vf_txq);\r\nunsigned label, eth_filt_en;\r\nefx_oword_t reg;\r\nif (bad_vf_index(efx, vf_evq) || bad_vf_index(efx, vf_txq) ||\r\nvf_txq >= vf_max_tx_channels ||\r\nbad_buf_count(buf_count, EFX_MAX_DMAQ_SIZE)) {\r\nif (net_ratelimit())\r\nnetif_err(efx, hw, efx->net_dev,\r\n"ERROR: Invalid INIT_TXQ from %s: txq %d evq %d "\r\n"buf_count %d\n", vf->pci_name, vf_txq,\r\nvf_evq, buf_count);\r\nreturn VFDI_RC_EINVAL;\r\n}\r\nmutex_lock(&vf->txq_lock);\r\nif (__test_and_set_bit(req->u.init_txq.index, vf->txq_mask))\r\n++vf->txq_count;\r\nmutex_unlock(&vf->txq_lock);\r\nefx_sriov_bufs(efx, buftbl, req->u.init_txq.addr, buf_count);\r\neth_filt_en = vf->tx_filter_mode == VF_TX_FILTER_ON;\r\nlabel = req->u.init_txq.label & EFX_FIELD_MASK(FRF_AZ_TX_DESCQ_LABEL);\r\nEFX_POPULATE_OWORD_8(reg,\r\nFRF_CZ_TX_DPT_Q_MASK_WIDTH, min(efx->vi_scale, 1U),\r\nFRF_CZ_TX_DPT_ETH_FILT_EN, eth_filt_en,\r\nFRF_AZ_TX_DESCQ_EN, 1,\r\nFRF_AZ_TX_DESCQ_BUF_BASE_ID, buftbl,\r\nFRF_AZ_TX_DESCQ_EVQ_ID, abs_index(vf, vf_evq),\r\nFRF_AZ_TX_DESCQ_LABEL, label,\r\nFRF_AZ_TX_DESCQ_SIZE, __ffs(buf_count),\r\nFRF_BZ_TX_NON_IP_DROP_DIS, 1);\r\nefx_writeo_table(efx, &reg, FR_BZ_TX_DESC_PTR_TBL,\r\nabs_index(vf, vf_txq));\r\nreturn VFDI_RC_SUCCESS;\r\n}\r\nstatic bool efx_vfdi_flush_wake(struct efx_vf *vf)\r\n{\r\nsmp_mb();\r\nreturn (!vf->txq_count && !vf->rxq_count) ||\r\natomic_read(&vf->rxq_retry_count);\r\n}\r\nstatic void efx_vfdi_flush_clear(struct efx_vf *vf)\r\n{\r\nmemset(vf->txq_mask, 0, sizeof(vf->txq_mask));\r\nvf->txq_count = 0;\r\nmemset(vf->rxq_mask, 0, sizeof(vf->rxq_mask));\r\nvf->rxq_count = 0;\r\nmemset(vf->rxq_retry_mask, 0, sizeof(vf->rxq_retry_mask));\r\natomic_set(&vf->rxq_retry_count, 0);\r\n}\r\nstatic int efx_vfdi_fini_all_queues(struct efx_vf *vf)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nefx_oword_t reg;\r\nunsigned count = efx_vf_size(efx);\r\nunsigned vf_offset = EFX_VI_BASE + vf->index * efx_vf_size(efx);\r\nunsigned timeout = HZ;\r\nunsigned index, rxqs_count;\r\n__le32 *rxqs;\r\nint rc;\r\nrxqs = kmalloc(count * sizeof(*rxqs), GFP_KERNEL);\r\nif (rxqs == NULL)\r\nreturn VFDI_RC_ENOMEM;\r\nrtnl_lock();\r\nif (efx->fc_disable++ == 0)\r\nefx_mcdi_set_mac(efx);\r\nrtnl_unlock();\r\nrxqs_count = 0;\r\nfor (index = 0; index < count; ++index) {\r\nif (test_bit(index, vf->txq_mask)) {\r\nEFX_POPULATE_OWORD_2(reg,\r\nFRF_AZ_TX_FLUSH_DESCQ_CMD, 1,\r\nFRF_AZ_TX_FLUSH_DESCQ,\r\nvf_offset + index);\r\nefx_writeo(efx, &reg, FR_AZ_TX_FLUSH_DESCQ);\r\n}\r\nif (test_bit(index, vf->rxq_mask))\r\nrxqs[rxqs_count++] = cpu_to_le32(vf_offset + index);\r\n}\r\natomic_set(&vf->rxq_retry_count, 0);\r\nwhile (timeout && (vf->rxq_count || vf->txq_count)) {\r\nrc = efx_mcdi_rpc(efx, MC_CMD_FLUSH_RX_QUEUES, (u8 *)rxqs,\r\nrxqs_count * sizeof(*rxqs), NULL, 0, NULL);\r\nWARN_ON(rc < 0);\r\ntimeout = wait_event_timeout(vf->flush_waitq,\r\nefx_vfdi_flush_wake(vf),\r\ntimeout);\r\nrxqs_count = 0;\r\nfor (index = 0; index < count; ++index) {\r\nif (test_and_clear_bit(index, vf->rxq_retry_mask)) {\r\natomic_dec(&vf->rxq_retry_count);\r\nrxqs[rxqs_count++] =\r\ncpu_to_le32(vf_offset + index);\r\n}\r\n}\r\n}\r\nrtnl_lock();\r\nif (--efx->fc_disable == 0)\r\nefx_mcdi_set_mac(efx);\r\nrtnl_unlock();\r\nEFX_ZERO_OWORD(reg);\r\nfor (index = 0; index < count; ++index) {\r\nefx_writeo_table(efx, &reg, FR_BZ_RX_DESC_PTR_TBL,\r\nvf_offset + index);\r\nefx_writeo_table(efx, &reg, FR_BZ_TX_DESC_PTR_TBL,\r\nvf_offset + index);\r\nefx_writeo_table(efx, &reg, FR_BZ_EVQ_PTR_TBL,\r\nvf_offset + index);\r\nefx_writeo_table(efx, &reg, FR_BZ_TIMER_TBL,\r\nvf_offset + index);\r\n}\r\nefx_sriov_bufs(efx, vf->buftbl_base, NULL,\r\nEFX_VF_BUFTBL_PER_VI * efx_vf_size(efx));\r\nkfree(rxqs);\r\nefx_vfdi_flush_clear(vf);\r\nvf->evq0_count = 0;\r\nreturn timeout ? 0 : VFDI_RC_ETIMEDOUT;\r\n}\r\nstatic int efx_vfdi_insert_filter(struct efx_vf *vf)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nstruct vfdi_req *req = vf->buf.addr;\r\nunsigned vf_rxq = req->u.mac_filter.rxq;\r\nunsigned flags;\r\nif (bad_vf_index(efx, vf_rxq) || vf->rx_filtering) {\r\nif (net_ratelimit())\r\nnetif_err(efx, hw, efx->net_dev,\r\n"ERROR: Invalid INSERT_FILTER from %s: rxq %d "\r\n"flags 0x%x\n", vf->pci_name, vf_rxq,\r\nreq->u.mac_filter.flags);\r\nreturn VFDI_RC_EINVAL;\r\n}\r\nflags = 0;\r\nif (req->u.mac_filter.flags & VFDI_MAC_FILTER_FLAG_RSS)\r\nflags |= EFX_FILTER_FLAG_RX_RSS;\r\nif (req->u.mac_filter.flags & VFDI_MAC_FILTER_FLAG_SCATTER)\r\nflags |= EFX_FILTER_FLAG_RX_SCATTER;\r\nvf->rx_filter_flags = flags;\r\nvf->rx_filter_qid = vf_rxq;\r\nvf->rx_filtering = true;\r\nefx_sriov_reset_rx_filter(vf);\r\nqueue_work(vfdi_workqueue, &efx->peer_work);\r\nreturn VFDI_RC_SUCCESS;\r\n}\r\nstatic int efx_vfdi_remove_all_filters(struct efx_vf *vf)\r\n{\r\nvf->rx_filtering = false;\r\nefx_sriov_reset_rx_filter(vf);\r\nqueue_work(vfdi_workqueue, &vf->efx->peer_work);\r\nreturn VFDI_RC_SUCCESS;\r\n}\r\nstatic int efx_vfdi_set_status_page(struct efx_vf *vf)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nstruct vfdi_req *req = vf->buf.addr;\r\nu64 page_count = req->u.set_status_page.peer_page_count;\r\nu64 max_page_count =\r\n(EFX_PAGE_SIZE -\r\noffsetof(struct vfdi_req, u.set_status_page.peer_page_addr[0]))\r\n/ sizeof(req->u.set_status_page.peer_page_addr[0]);\r\nif (!req->u.set_status_page.dma_addr || page_count > max_page_count) {\r\nif (net_ratelimit())\r\nnetif_err(efx, hw, efx->net_dev,\r\n"ERROR: Invalid SET_STATUS_PAGE from %s\n",\r\nvf->pci_name);\r\nreturn VFDI_RC_EINVAL;\r\n}\r\nmutex_lock(&efx->local_lock);\r\nmutex_lock(&vf->status_lock);\r\nvf->status_addr = req->u.set_status_page.dma_addr;\r\nkfree(vf->peer_page_addrs);\r\nvf->peer_page_addrs = NULL;\r\nvf->peer_page_count = 0;\r\nif (page_count) {\r\nvf->peer_page_addrs = kcalloc(page_count, sizeof(u64),\r\nGFP_KERNEL);\r\nif (vf->peer_page_addrs) {\r\nmemcpy(vf->peer_page_addrs,\r\nreq->u.set_status_page.peer_page_addr,\r\npage_count * sizeof(u64));\r\nvf->peer_page_count = page_count;\r\n}\r\n}\r\n__efx_sriov_push_vf_status(vf);\r\nmutex_unlock(&vf->status_lock);\r\nmutex_unlock(&efx->local_lock);\r\nreturn VFDI_RC_SUCCESS;\r\n}\r\nstatic int efx_vfdi_clear_status_page(struct efx_vf *vf)\r\n{\r\nmutex_lock(&vf->status_lock);\r\nvf->status_addr = 0;\r\nmutex_unlock(&vf->status_lock);\r\nreturn VFDI_RC_SUCCESS;\r\n}\r\nstatic void efx_sriov_vfdi(struct work_struct *work)\r\n{\r\nstruct efx_vf *vf = container_of(work, struct efx_vf, req);\r\nstruct efx_nic *efx = vf->efx;\r\nstruct vfdi_req *req = vf->buf.addr;\r\nstruct efx_memcpy_req copy[2];\r\nint rc;\r\nmemset(copy, '\0', sizeof(copy));\r\ncopy[0].from_rid = vf->pci_rid;\r\ncopy[0].from_addr = vf->req_addr;\r\ncopy[0].to_rid = efx->pci_dev->devfn;\r\ncopy[0].to_addr = vf->buf.dma_addr;\r\ncopy[0].length = EFX_PAGE_SIZE;\r\nrc = efx_sriov_memcpy(efx, copy, 1);\r\nif (rc) {\r\nif (net_ratelimit())\r\nnetif_err(efx, hw, efx->net_dev,\r\n"ERROR: Unable to fetch VFDI request from %s rc %d\n",\r\nvf->pci_name, -rc);\r\nvf->busy = false;\r\nreturn;\r\n}\r\nif (req->op < VFDI_OP_LIMIT && vfdi_ops[req->op] != NULL) {\r\nrc = vfdi_ops[req->op](vf);\r\nif (rc == 0) {\r\nnetif_dbg(efx, hw, efx->net_dev,\r\n"vfdi request %d from %s ok\n",\r\nreq->op, vf->pci_name);\r\n}\r\n} else {\r\nnetif_dbg(efx, hw, efx->net_dev,\r\n"ERROR: Unrecognised request %d from VF %s addr "\r\n"%llx\n", req->op, vf->pci_name,\r\n(unsigned long long)vf->req_addr);\r\nrc = VFDI_RC_EOPNOTSUPP;\r\n}\r\nvf->busy = false;\r\nsmp_wmb();\r\nreq->rc = rc;\r\nreq->op = VFDI_OP_RESPONSE;\r\nmemset(copy, '\0', sizeof(copy));\r\ncopy[0].from_buf = &req->rc;\r\ncopy[0].to_rid = vf->pci_rid;\r\ncopy[0].to_addr = vf->req_addr + offsetof(struct vfdi_req, rc);\r\ncopy[0].length = sizeof(req->rc);\r\ncopy[1].from_buf = &req->op;\r\ncopy[1].to_rid = vf->pci_rid;\r\ncopy[1].to_addr = vf->req_addr + offsetof(struct vfdi_req, op);\r\ncopy[1].length = sizeof(req->op);\r\n(void) efx_sriov_memcpy(efx, copy, ARRAY_SIZE(copy));\r\n}\r\nstatic void efx_sriov_reset_vf(struct efx_vf *vf, struct efx_buffer *buffer)\r\n{\r\nstruct efx_nic *efx = vf->efx;\r\nstruct efx_memcpy_req copy_req[4];\r\nefx_qword_t event;\r\nunsigned int pos, count, k, buftbl, abs_evq;\r\nefx_oword_t reg;\r\nefx_dword_t ptr;\r\nint rc;\r\nBUG_ON(buffer->len != EFX_PAGE_SIZE);\r\nif (!vf->evq0_count)\r\nreturn;\r\nBUG_ON(vf->evq0_count & (vf->evq0_count - 1));\r\nmutex_lock(&vf->status_lock);\r\nEFX_POPULATE_QWORD_3(event,\r\nFSF_AZ_EV_CODE, FSE_CZ_EV_CODE_USER_EV,\r\nVFDI_EV_SEQ, vf->msg_seqno,\r\nVFDI_EV_TYPE, VFDI_EV_TYPE_RESET);\r\nvf->msg_seqno++;\r\nfor (pos = 0; pos < EFX_PAGE_SIZE; pos += sizeof(event))\r\nmemcpy(buffer->addr + pos, &event, sizeof(event));\r\nfor (pos = 0; pos < vf->evq0_count; pos += count) {\r\ncount = min_t(unsigned, vf->evq0_count - pos,\r\nARRAY_SIZE(copy_req));\r\nfor (k = 0; k < count; k++) {\r\ncopy_req[k].from_buf = NULL;\r\ncopy_req[k].from_rid = efx->pci_dev->devfn;\r\ncopy_req[k].from_addr = buffer->dma_addr;\r\ncopy_req[k].to_rid = vf->pci_rid;\r\ncopy_req[k].to_addr = vf->evq0_addrs[pos + k];\r\ncopy_req[k].length = EFX_PAGE_SIZE;\r\n}\r\nrc = efx_sriov_memcpy(efx, copy_req, count);\r\nif (rc) {\r\nif (net_ratelimit())\r\nnetif_err(efx, hw, efx->net_dev,\r\n"ERROR: Unable to notify %s of reset"\r\n": %d\n", vf->pci_name, -rc);\r\nbreak;\r\n}\r\n}\r\nabs_evq = abs_index(vf, 0);\r\nbuftbl = EFX_BUFTBL_EVQ_BASE(vf, 0);\r\nefx_sriov_bufs(efx, buftbl, vf->evq0_addrs, vf->evq0_count);\r\nEFX_POPULATE_OWORD_3(reg,\r\nFRF_CZ_TIMER_Q_EN, 1,\r\nFRF_CZ_HOST_NOTIFY_MODE, 0,\r\nFRF_CZ_TIMER_MODE, FFE_CZ_TIMER_MODE_DIS);\r\nefx_writeo_table(efx, &reg, FR_BZ_TIMER_TBL, abs_evq);\r\nEFX_POPULATE_OWORD_3(reg,\r\nFRF_AZ_EVQ_EN, 1,\r\nFRF_AZ_EVQ_SIZE, __ffs(vf->evq0_count),\r\nFRF_AZ_EVQ_BUF_BASE_ID, buftbl);\r\nefx_writeo_table(efx, &reg, FR_BZ_EVQ_PTR_TBL, abs_evq);\r\nEFX_POPULATE_DWORD_1(ptr, FRF_AZ_EVQ_RPTR, 0);\r\nefx_writed_table(efx, &ptr, FR_BZ_EVQ_RPTR, abs_evq);\r\nmutex_unlock(&vf->status_lock);\r\n}\r\nstatic void efx_sriov_reset_vf_work(struct work_struct *work)\r\n{\r\nstruct efx_vf *vf = container_of(work, struct efx_vf, req);\r\nstruct efx_nic *efx = vf->efx;\r\nstruct efx_buffer buf;\r\nif (!efx_nic_alloc_buffer(efx, &buf, EFX_PAGE_SIZE)) {\r\nefx_sriov_reset_vf(vf, &buf);\r\nefx_nic_free_buffer(efx, &buf);\r\n}\r\n}\r\nstatic void efx_sriov_handle_no_channel(struct efx_nic *efx)\r\n{\r\nnetif_err(efx, drv, efx->net_dev,\r\n"ERROR: IOV requires MSI-X and 1 additional interrupt"\r\n"vector. IOV disabled\n");\r\nefx->vf_count = 0;\r\n}\r\nstatic int efx_sriov_probe_channel(struct efx_channel *channel)\r\n{\r\nchannel->efx->vfdi_channel = channel;\r\nreturn 0;\r\n}\r\nstatic void\r\nefx_sriov_get_channel_name(struct efx_channel *channel, char *buf, size_t len)\r\n{\r\nsnprintf(buf, len, "%s-iov", channel->efx->name);\r\n}\r\nvoid efx_sriov_probe(struct efx_nic *efx)\r\n{\r\nunsigned count;\r\nif (!max_vfs)\r\nreturn;\r\nif (efx_sriov_cmd(efx, false, &efx->vi_scale, &count))\r\nreturn;\r\nif (count > 0 && count > max_vfs)\r\ncount = max_vfs;\r\nefx->vf_count = count;\r\nefx->extra_channel_type[EFX_EXTRA_CHANNEL_IOV] = &efx_sriov_channel_type;\r\n}\r\nstatic void efx_sriov_peer_work(struct work_struct *data)\r\n{\r\nstruct efx_nic *efx = container_of(data, struct efx_nic, peer_work);\r\nstruct vfdi_status *vfdi_status = efx->vfdi_status.addr;\r\nstruct efx_vf *vf;\r\nstruct efx_local_addr *local_addr;\r\nstruct vfdi_endpoint *peer;\r\nstruct efx_endpoint_page *epp;\r\nstruct list_head pages;\r\nunsigned int peer_space;\r\nunsigned int peer_count;\r\nunsigned int pos;\r\nmutex_lock(&efx->local_lock);\r\nINIT_LIST_HEAD(&pages);\r\nlist_splice_tail_init(&efx->local_page_list, &pages);\r\npeer = vfdi_status->peers + 1;\r\npeer_space = ARRAY_SIZE(vfdi_status->peers) - 1;\r\npeer_count = 1;\r\nfor (pos = 0; pos < efx->vf_count; ++pos) {\r\nvf = efx->vf + pos;\r\nmutex_lock(&vf->status_lock);\r\nif (vf->rx_filtering && !is_zero_ether_addr(vf->addr.mac_addr)) {\r\n*peer++ = vf->addr;\r\n++peer_count;\r\n--peer_space;\r\nBUG_ON(peer_space == 0);\r\n}\r\nmutex_unlock(&vf->status_lock);\r\n}\r\nlist_for_each_entry(local_addr, &efx->local_addr_list, link) {\r\nmemcpy(peer->mac_addr, local_addr->addr, ETH_ALEN);\r\npeer->tci = 0;\r\n++peer;\r\n++peer_count;\r\nif (--peer_space == 0) {\r\nif (list_empty(&pages)) {\r\nepp = kmalloc(sizeof(*epp), GFP_KERNEL);\r\nif (!epp)\r\nbreak;\r\nepp->ptr = dma_alloc_coherent(\r\n&efx->pci_dev->dev, EFX_PAGE_SIZE,\r\n&epp->addr, GFP_KERNEL);\r\nif (!epp->ptr) {\r\nkfree(epp);\r\nbreak;\r\n}\r\n} else {\r\nepp = list_first_entry(\r\n&pages, struct efx_endpoint_page, link);\r\nlist_del(&epp->link);\r\n}\r\nlist_add_tail(&epp->link, &efx->local_page_list);\r\npeer = (struct vfdi_endpoint *)epp->ptr;\r\npeer_space = EFX_PAGE_SIZE / sizeof(struct vfdi_endpoint);\r\n}\r\n}\r\nvfdi_status->peer_count = peer_count;\r\nmutex_unlock(&efx->local_lock);\r\nwhile (!list_empty(&pages)) {\r\nepp = list_first_entry(\r\n&pages, struct efx_endpoint_page, link);\r\nlist_del(&epp->link);\r\ndma_free_coherent(&efx->pci_dev->dev, EFX_PAGE_SIZE,\r\nepp->ptr, epp->addr);\r\nkfree(epp);\r\n}\r\nfor (pos = 0; pos < efx->vf_count; ++pos) {\r\nvf = efx->vf + pos;\r\nmutex_lock(&vf->status_lock);\r\nif (vf->status_addr)\r\n__efx_sriov_push_vf_status(vf);\r\nmutex_unlock(&vf->status_lock);\r\n}\r\n}\r\nstatic void efx_sriov_free_local(struct efx_nic *efx)\r\n{\r\nstruct efx_local_addr *local_addr;\r\nstruct efx_endpoint_page *epp;\r\nwhile (!list_empty(&efx->local_addr_list)) {\r\nlocal_addr = list_first_entry(&efx->local_addr_list,\r\nstruct efx_local_addr, link);\r\nlist_del(&local_addr->link);\r\nkfree(local_addr);\r\n}\r\nwhile (!list_empty(&efx->local_page_list)) {\r\nepp = list_first_entry(&efx->local_page_list,\r\nstruct efx_endpoint_page, link);\r\nlist_del(&epp->link);\r\ndma_free_coherent(&efx->pci_dev->dev, EFX_PAGE_SIZE,\r\nepp->ptr, epp->addr);\r\nkfree(epp);\r\n}\r\n}\r\nstatic int efx_sriov_vf_alloc(struct efx_nic *efx)\r\n{\r\nunsigned index;\r\nstruct efx_vf *vf;\r\nefx->vf = kzalloc(sizeof(struct efx_vf) * efx->vf_count, GFP_KERNEL);\r\nif (!efx->vf)\r\nreturn -ENOMEM;\r\nfor (index = 0; index < efx->vf_count; ++index) {\r\nvf = efx->vf + index;\r\nvf->efx = efx;\r\nvf->index = index;\r\nvf->rx_filter_id = -1;\r\nvf->tx_filter_mode = VF_TX_FILTER_AUTO;\r\nvf->tx_filter_id = -1;\r\nINIT_WORK(&vf->req, efx_sriov_vfdi);\r\nINIT_WORK(&vf->reset_work, efx_sriov_reset_vf_work);\r\ninit_waitqueue_head(&vf->flush_waitq);\r\nmutex_init(&vf->status_lock);\r\nmutex_init(&vf->txq_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic void efx_sriov_vfs_fini(struct efx_nic *efx)\r\n{\r\nstruct efx_vf *vf;\r\nunsigned int pos;\r\nfor (pos = 0; pos < efx->vf_count; ++pos) {\r\nvf = efx->vf + pos;\r\nefx_nic_free_buffer(efx, &vf->buf);\r\nkfree(vf->peer_page_addrs);\r\nvf->peer_page_addrs = NULL;\r\nvf->peer_page_count = 0;\r\nvf->evq0_count = 0;\r\n}\r\n}\r\nstatic int efx_sriov_vfs_init(struct efx_nic *efx)\r\n{\r\nstruct pci_dev *pci_dev = efx->pci_dev;\r\nunsigned index, devfn, sriov, buftbl_base;\r\nu16 offset, stride;\r\nstruct efx_vf *vf;\r\nint rc;\r\nsriov = pci_find_ext_capability(pci_dev, PCI_EXT_CAP_ID_SRIOV);\r\nif (!sriov)\r\nreturn -ENOENT;\r\npci_read_config_word(pci_dev, sriov + PCI_SRIOV_VF_OFFSET, &offset);\r\npci_read_config_word(pci_dev, sriov + PCI_SRIOV_VF_STRIDE, &stride);\r\nbuftbl_base = efx->vf_buftbl_base;\r\ndevfn = pci_dev->devfn + offset;\r\nfor (index = 0; index < efx->vf_count; ++index) {\r\nvf = efx->vf + index;\r\nvf->buftbl_base = buftbl_base;\r\nbuftbl_base += EFX_VF_BUFTBL_PER_VI * efx_vf_size(efx);\r\nvf->pci_rid = devfn;\r\nsnprintf(vf->pci_name, sizeof(vf->pci_name),\r\n"%04x:%02x:%02x.%d",\r\npci_domain_nr(pci_dev->bus), pci_dev->bus->number,\r\nPCI_SLOT(devfn), PCI_FUNC(devfn));\r\nrc = efx_nic_alloc_buffer(efx, &vf->buf, EFX_PAGE_SIZE);\r\nif (rc)\r\ngoto fail;\r\ndevfn += stride;\r\n}\r\nreturn 0;\r\nfail:\r\nefx_sriov_vfs_fini(efx);\r\nreturn rc;\r\n}\r\nint efx_sriov_init(struct efx_nic *efx)\r\n{\r\nstruct net_device *net_dev = efx->net_dev;\r\nstruct vfdi_status *vfdi_status;\r\nint rc;\r\nBUILD_BUG_ON(EFX_MAX_CHANNELS + 1 >= EFX_VI_BASE);\r\nBUILD_BUG_ON(EFX_VI_BASE & ((1 << EFX_VI_SCALE_MAX) - 1));\r\nif (efx->vf_count == 0)\r\nreturn 0;\r\nrc = efx_sriov_cmd(efx, true, NULL, NULL);\r\nif (rc)\r\ngoto fail_cmd;\r\nrc = efx_nic_alloc_buffer(efx, &efx->vfdi_status, sizeof(*vfdi_status));\r\nif (rc)\r\ngoto fail_status;\r\nvfdi_status = efx->vfdi_status.addr;\r\nmemset(vfdi_status, 0, sizeof(*vfdi_status));\r\nvfdi_status->version = 1;\r\nvfdi_status->length = sizeof(*vfdi_status);\r\nvfdi_status->max_tx_channels = vf_max_tx_channels;\r\nvfdi_status->vi_scale = efx->vi_scale;\r\nvfdi_status->rss_rxq_count = efx->rss_spread;\r\nvfdi_status->peer_count = 1 + efx->vf_count;\r\nvfdi_status->timer_quantum_ns = efx->timer_quantum_ns;\r\nrc = efx_sriov_vf_alloc(efx);\r\nif (rc)\r\ngoto fail_alloc;\r\nmutex_init(&efx->local_lock);\r\nINIT_WORK(&efx->peer_work, efx_sriov_peer_work);\r\nINIT_LIST_HEAD(&efx->local_addr_list);\r\nINIT_LIST_HEAD(&efx->local_page_list);\r\nrc = efx_sriov_vfs_init(efx);\r\nif (rc)\r\ngoto fail_vfs;\r\nrtnl_lock();\r\nmemcpy(vfdi_status->peers[0].mac_addr,\r\nnet_dev->dev_addr, ETH_ALEN);\r\nefx->vf_init_count = efx->vf_count;\r\nrtnl_unlock();\r\nefx_sriov_usrev(efx, true);\r\nrc = pci_enable_sriov(efx->pci_dev, efx->vf_count);\r\nif (rc)\r\ngoto fail_pci;\r\nnetif_info(efx, probe, net_dev,\r\n"enabled SR-IOV for %d VFs, %d VI per VF\n",\r\nefx->vf_count, efx_vf_size(efx));\r\nreturn 0;\r\nfail_pci:\r\nefx_sriov_usrev(efx, false);\r\nrtnl_lock();\r\nefx->vf_init_count = 0;\r\nrtnl_unlock();\r\nefx_sriov_vfs_fini(efx);\r\nfail_vfs:\r\ncancel_work_sync(&efx->peer_work);\r\nefx_sriov_free_local(efx);\r\nkfree(efx->vf);\r\nfail_alloc:\r\nefx_nic_free_buffer(efx, &efx->vfdi_status);\r\nfail_status:\r\nefx_sriov_cmd(efx, false, NULL, NULL);\r\nfail_cmd:\r\nreturn rc;\r\n}\r\nvoid efx_sriov_fini(struct efx_nic *efx)\r\n{\r\nstruct efx_vf *vf;\r\nunsigned int pos;\r\nif (efx->vf_init_count == 0)\r\nreturn;\r\nBUG_ON(efx->vfdi_channel->enabled);\r\nefx_sriov_usrev(efx, false);\r\nrtnl_lock();\r\nefx->vf_init_count = 0;\r\nrtnl_unlock();\r\nfor (pos = 0; pos < efx->vf_count; ++pos) {\r\nvf = efx->vf + pos;\r\ncancel_work_sync(&vf->req);\r\ncancel_work_sync(&vf->reset_work);\r\n}\r\ncancel_work_sync(&efx->peer_work);\r\npci_disable_sriov(efx->pci_dev);\r\nefx_sriov_vfs_fini(efx);\r\nefx_sriov_free_local(efx);\r\nkfree(efx->vf);\r\nefx_nic_free_buffer(efx, &efx->vfdi_status);\r\nefx_sriov_cmd(efx, false, NULL, NULL);\r\n}\r\nvoid efx_sriov_event(struct efx_channel *channel, efx_qword_t *event)\r\n{\r\nstruct efx_nic *efx = channel->efx;\r\nstruct efx_vf *vf;\r\nunsigned qid, seq, type, data;\r\nqid = EFX_QWORD_FIELD(*event, FSF_CZ_USER_QID);\r\nBUILD_BUG_ON(FSF_CZ_USER_EV_REG_VALUE_LBN != 0);\r\nseq = EFX_QWORD_FIELD(*event, VFDI_EV_SEQ);\r\ntype = EFX_QWORD_FIELD(*event, VFDI_EV_TYPE);\r\ndata = EFX_QWORD_FIELD(*event, VFDI_EV_DATA);\r\nnetif_vdbg(efx, hw, efx->net_dev,\r\n"USR_EV event from qid %d seq 0x%x type %d data 0x%x\n",\r\nqid, seq, type, data);\r\nif (map_vi_index(efx, qid, &vf, NULL))\r\nreturn;\r\nif (vf->busy)\r\ngoto error;\r\nif (type == VFDI_EV_TYPE_REQ_WORD0) {\r\nvf->req_type = VFDI_EV_TYPE_REQ_WORD0;\r\nvf->req_seqno = seq + 1;\r\nvf->req_addr = 0;\r\n} else if (seq != (vf->req_seqno++ & 0xff) || type != vf->req_type)\r\ngoto error;\r\nswitch (vf->req_type) {\r\ncase VFDI_EV_TYPE_REQ_WORD0:\r\ncase VFDI_EV_TYPE_REQ_WORD1:\r\ncase VFDI_EV_TYPE_REQ_WORD2:\r\nvf->req_addr |= (u64)data << (vf->req_type << 4);\r\n++vf->req_type;\r\nreturn;\r\ncase VFDI_EV_TYPE_REQ_WORD3:\r\nvf->req_addr |= (u64)data << 48;\r\nvf->req_type = VFDI_EV_TYPE_REQ_WORD0;\r\nvf->busy = true;\r\nqueue_work(vfdi_workqueue, &vf->req);\r\nreturn;\r\n}\r\nerror:\r\nif (net_ratelimit())\r\nnetif_err(efx, hw, efx->net_dev,\r\n"ERROR: Screaming VFDI request from %s\n",\r\nvf->pci_name);\r\nvf->req_type = VFDI_EV_TYPE_REQ_WORD0;\r\nvf->req_seqno = seq + 1;\r\n}\r\nvoid efx_sriov_flr(struct efx_nic *efx, unsigned vf_i)\r\n{\r\nstruct efx_vf *vf;\r\nif (vf_i > efx->vf_init_count)\r\nreturn;\r\nvf = efx->vf + vf_i;\r\nnetif_info(efx, hw, efx->net_dev,\r\n"FLR on VF %s\n", vf->pci_name);\r\nvf->status_addr = 0;\r\nefx_vfdi_remove_all_filters(vf);\r\nefx_vfdi_flush_clear(vf);\r\nvf->evq0_count = 0;\r\n}\r\nvoid efx_sriov_mac_address_changed(struct efx_nic *efx)\r\n{\r\nstruct vfdi_status *vfdi_status = efx->vfdi_status.addr;\r\nif (!efx->vf_init_count)\r\nreturn;\r\nmemcpy(vfdi_status->peers[0].mac_addr,\r\nefx->net_dev->dev_addr, ETH_ALEN);\r\nqueue_work(vfdi_workqueue, &efx->peer_work);\r\n}\r\nvoid efx_sriov_tx_flush_done(struct efx_nic *efx, efx_qword_t *event)\r\n{\r\nstruct efx_vf *vf;\r\nunsigned queue, qid;\r\nqueue = EFX_QWORD_FIELD(*event, FSF_AZ_DRIVER_EV_SUBDATA);\r\nif (map_vi_index(efx, queue, &vf, &qid))\r\nreturn;\r\nif (!test_bit(qid, vf->txq_mask))\r\nreturn;\r\n__clear_bit(qid, vf->txq_mask);\r\n--vf->txq_count;\r\nif (efx_vfdi_flush_wake(vf))\r\nwake_up(&vf->flush_waitq);\r\n}\r\nvoid efx_sriov_rx_flush_done(struct efx_nic *efx, efx_qword_t *event)\r\n{\r\nstruct efx_vf *vf;\r\nunsigned ev_failed, queue, qid;\r\nqueue = EFX_QWORD_FIELD(*event, FSF_AZ_DRIVER_EV_RX_DESCQ_ID);\r\nev_failed = EFX_QWORD_FIELD(*event,\r\nFSF_AZ_DRIVER_EV_RX_FLUSH_FAIL);\r\nif (map_vi_index(efx, queue, &vf, &qid))\r\nreturn;\r\nif (!test_bit(qid, vf->rxq_mask))\r\nreturn;\r\nif (ev_failed) {\r\nset_bit(qid, vf->rxq_retry_mask);\r\natomic_inc(&vf->rxq_retry_count);\r\n} else {\r\n__clear_bit(qid, vf->rxq_mask);\r\n--vf->rxq_count;\r\n}\r\nif (efx_vfdi_flush_wake(vf))\r\nwake_up(&vf->flush_waitq);\r\n}\r\nvoid efx_sriov_desc_fetch_err(struct efx_nic *efx, unsigned dmaq)\r\n{\r\nstruct efx_vf *vf;\r\nunsigned int rel;\r\nif (map_vi_index(efx, dmaq, &vf, &rel))\r\nreturn;\r\nif (net_ratelimit())\r\nnetif_err(efx, hw, efx->net_dev,\r\n"VF %d DMA Q %d reports descriptor fetch error.\n",\r\nvf->index, rel);\r\nqueue_work(vfdi_workqueue, &vf->reset_work);\r\n}\r\nvoid efx_sriov_reset(struct efx_nic *efx)\r\n{\r\nunsigned int vf_i;\r\nstruct efx_buffer buf;\r\nstruct efx_vf *vf;\r\nASSERT_RTNL();\r\nif (efx->vf_init_count == 0)\r\nreturn;\r\nefx_sriov_usrev(efx, true);\r\n(void)efx_sriov_cmd(efx, true, NULL, NULL);\r\nif (efx_nic_alloc_buffer(efx, &buf, EFX_PAGE_SIZE))\r\nreturn;\r\nfor (vf_i = 0; vf_i < efx->vf_init_count; ++vf_i) {\r\nvf = efx->vf + vf_i;\r\nefx_sriov_reset_vf(vf, &buf);\r\n}\r\nefx_nic_free_buffer(efx, &buf);\r\n}\r\nint efx_init_sriov(void)\r\n{\r\nvfdi_workqueue = create_singlethread_workqueue("sfc_vfdi");\r\nif (!vfdi_workqueue)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid efx_fini_sriov(void)\r\n{\r\ndestroy_workqueue(vfdi_workqueue);\r\n}\r\nint efx_sriov_set_vf_mac(struct net_device *net_dev, int vf_i, u8 *mac)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nstruct efx_vf *vf;\r\nif (vf_i >= efx->vf_init_count)\r\nreturn -EINVAL;\r\nvf = efx->vf + vf_i;\r\nmutex_lock(&vf->status_lock);\r\nmemcpy(vf->addr.mac_addr, mac, ETH_ALEN);\r\n__efx_sriov_update_vf_addr(vf);\r\nmutex_unlock(&vf->status_lock);\r\nreturn 0;\r\n}\r\nint efx_sriov_set_vf_vlan(struct net_device *net_dev, int vf_i,\r\nu16 vlan, u8 qos)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nstruct efx_vf *vf;\r\nu16 tci;\r\nif (vf_i >= efx->vf_init_count)\r\nreturn -EINVAL;\r\nvf = efx->vf + vf_i;\r\nmutex_lock(&vf->status_lock);\r\ntci = (vlan & VLAN_VID_MASK) | ((qos & 0x7) << VLAN_PRIO_SHIFT);\r\nvf->addr.tci = htons(tci);\r\n__efx_sriov_update_vf_addr(vf);\r\nmutex_unlock(&vf->status_lock);\r\nreturn 0;\r\n}\r\nint efx_sriov_set_vf_spoofchk(struct net_device *net_dev, int vf_i,\r\nbool spoofchk)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nstruct efx_vf *vf;\r\nint rc;\r\nif (vf_i >= efx->vf_init_count)\r\nreturn -EINVAL;\r\nvf = efx->vf + vf_i;\r\nmutex_lock(&vf->txq_lock);\r\nif (vf->txq_count == 0) {\r\nvf->tx_filter_mode =\r\nspoofchk ? VF_TX_FILTER_ON : VF_TX_FILTER_OFF;\r\nrc = 0;\r\n} else {\r\nrc = -EBUSY;\r\n}\r\nmutex_unlock(&vf->txq_lock);\r\nreturn rc;\r\n}\r\nint efx_sriov_get_vf_config(struct net_device *net_dev, int vf_i,\r\nstruct ifla_vf_info *ivi)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nstruct efx_vf *vf;\r\nu16 tci;\r\nif (vf_i >= efx->vf_init_count)\r\nreturn -EINVAL;\r\nvf = efx->vf + vf_i;\r\nivi->vf = vf_i;\r\nmemcpy(ivi->mac, vf->addr.mac_addr, ETH_ALEN);\r\nivi->tx_rate = 0;\r\ntci = ntohs(vf->addr.tci);\r\nivi->vlan = tci & VLAN_VID_MASK;\r\nivi->qos = (tci >> VLAN_PRIO_SHIFT) & 0x7;\r\nivi->spoofchk = vf->tx_filter_mode == VF_TX_FILTER_ON;\r\nreturn 0;\r\n}
