static void cvm_oct_enable_napi(void *_)\r\n{\r\nint cpu = smp_processor_id();\r\nnapi_schedule(&cvm_oct_napi[cpu].napi);\r\n}\r\nstatic void cvm_oct_enable_one_cpu(void)\r\n{\r\nint v;\r\nint cpu;\r\nv = atomic_sub_if_positive(1, &core_state.available_cores);\r\nif (v < 0)\r\nreturn;\r\nfor_each_online_cpu(cpu) {\r\nif (!cpu_test_and_set(cpu, core_state.cpu_state)) {\r\nv = smp_call_function_single(cpu, cvm_oct_enable_napi,\r\nNULL, 0);\r\nif (v)\r\npanic("Can't enable NAPI.");\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void cvm_oct_no_more_work(void)\r\n{\r\nint cpu = smp_processor_id();\r\nif (cpu == 0) {\r\nenable_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group);\r\nreturn;\r\n}\r\ncpu_clear(cpu, core_state.cpu_state);\r\natomic_add(1, &core_state.available_cores);\r\n}\r\nstatic irqreturn_t cvm_oct_do_interrupt(int cpl, void *dev_id)\r\n{\r\ndisable_irq_nosync(OCTEON_IRQ_WORKQ0 + pow_receive_group);\r\ncvm_oct_enable_napi(NULL);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic inline int cvm_oct_check_rcv_error(cvmx_wqe_t *work)\r\n{\r\nif ((work->word2.snoip.err_code == 10) && (work->len <= 64)) {\r\n} else\r\nif (USE_10MBPS_PREAMBLE_WORKAROUND\r\n&& ((work->word2.snoip.err_code == 5)\r\n|| (work->word2.snoip.err_code == 7))) {\r\nint interface = cvmx_helper_get_interface_num(work->ipprt);\r\nint index = cvmx_helper_get_interface_index_num(work->ipprt);\r\nunion cvmx_gmxx_rxx_frm_ctl gmxx_rxx_frm_ctl;\r\ngmxx_rxx_frm_ctl.u64 =\r\ncvmx_read_csr(CVMX_GMXX_RXX_FRM_CTL(index, interface));\r\nif (gmxx_rxx_frm_ctl.s.pre_chk == 0) {\r\nuint8_t *ptr =\r\ncvmx_phys_to_ptr(work->packet_ptr.s.addr);\r\nint i = 0;\r\nwhile (i < work->len - 1) {\r\nif (*ptr != 0x55)\r\nbreak;\r\nptr++;\r\ni++;\r\n}\r\nif (*ptr == 0xd5) {\r\nwork->packet_ptr.s.addr += i + 1;\r\nwork->len -= i + 5;\r\n} else if ((*ptr & 0xf) == 0xd) {\r\nwork->packet_ptr.s.addr += i;\r\nwork->len -= i + 4;\r\nfor (i = 0; i < work->len; i++) {\r\n*ptr =\r\n((*ptr & 0xf0) >> 4) |\r\n((*(ptr + 1) & 0xf) << 4);\r\nptr++;\r\n}\r\n} else {\r\nprintk_ratelimited("Port %d unknown preamble, packet "\r\n"dropped\n",\r\nwork->ipprt);\r\ncvm_oct_free_work(work);\r\nreturn 1;\r\n}\r\n}\r\n} else {\r\nprintk_ratelimited("Port %d receive error code %d, packet dropped\n",\r\nwork->ipprt, work->word2.snoip.err_code);\r\ncvm_oct_free_work(work);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int cvm_oct_napi_poll(struct napi_struct *napi, int budget)\r\n{\r\nconst int coreid = cvmx_get_core_num();\r\nuint64_t old_group_mask;\r\nuint64_t old_scratch;\r\nint rx_count = 0;\r\nint did_work_request = 0;\r\nint packet_not_copied;\r\nprefetch(cvm_oct_device);\r\nif (USE_ASYNC_IOBDMA) {\r\nCVMX_SYNCIOBDMA;\r\nold_scratch = cvmx_scratch_read64(CVMX_SCR_SCRATCH);\r\n}\r\nold_group_mask = cvmx_read_csr(CVMX_POW_PP_GRP_MSKX(coreid));\r\ncvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid),\r\n(old_group_mask & ~0xFFFFull) | 1 << pow_receive_group);\r\nif (USE_ASYNC_IOBDMA) {\r\ncvmx_pow_work_request_async(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);\r\ndid_work_request = 1;\r\n}\r\nwhile (rx_count < budget) {\r\nstruct sk_buff *skb = NULL;\r\nstruct sk_buff **pskb = NULL;\r\nint skb_in_hw;\r\ncvmx_wqe_t *work;\r\nif (USE_ASYNC_IOBDMA && did_work_request)\r\nwork = cvmx_pow_work_response_async(CVMX_SCR_SCRATCH);\r\nelse\r\nwork = cvmx_pow_work_request_sync(CVMX_POW_NO_WAIT);\r\nprefetch(work);\r\ndid_work_request = 0;\r\nif (work == NULL) {\r\nunion cvmx_pow_wq_int wq_int;\r\nwq_int.u64 = 0;\r\nwq_int.s.iq_dis = 1 << pow_receive_group;\r\nwq_int.s.wq_int = 1 << pow_receive_group;\r\ncvmx_write_csr(CVMX_POW_WQ_INT, wq_int.u64);\r\nbreak;\r\n}\r\npskb = (struct sk_buff **)(cvm_oct_get_buffer_ptr(work->packet_ptr) - sizeof(void *));\r\nprefetch(pskb);\r\nif (USE_ASYNC_IOBDMA && rx_count < (budget - 1)) {\r\ncvmx_pow_work_request_async_nocheck(CVMX_SCR_SCRATCH, CVMX_POW_NO_WAIT);\r\ndid_work_request = 1;\r\n}\r\nif (rx_count == 0) {\r\nunion cvmx_pow_wq_int_cntx counts;\r\nint backlog;\r\nint cores_in_use = core_state.baseline_cores - atomic_read(&core_state.available_cores);\r\ncounts.u64 = cvmx_read_csr(CVMX_POW_WQ_INT_CNTX(pow_receive_group));\r\nbacklog = counts.s.iq_cnt + counts.s.ds_cnt;\r\nif (backlog > budget * cores_in_use && napi != NULL)\r\ncvm_oct_enable_one_cpu();\r\n}\r\nskb_in_hw = USE_SKBUFFS_IN_HW && work->word2.s.bufs == 1;\r\nif (likely(skb_in_hw)) {\r\nskb = *pskb;\r\nprefetch(&skb->head);\r\nprefetch(&skb->len);\r\n}\r\nprefetch(cvm_oct_device[work->ipprt]);\r\nif (unlikely(work->word2.snoip.rcv_error)) {\r\nif (cvm_oct_check_rcv_error(work))\r\ncontinue;\r\n}\r\nif (likely(skb_in_hw)) {\r\nskb->data = skb->head + work->packet_ptr.s.addr - cvmx_ptr_to_phys(skb->head);\r\nprefetch(skb->data);\r\nskb->len = work->len;\r\nskb_set_tail_pointer(skb, skb->len);\r\npacket_not_copied = 1;\r\n} else {\r\nskb = dev_alloc_skb(work->len);\r\nif (!skb) {\r\nprintk_ratelimited("Port %d failed to allocate "\r\n"skbuff, packet dropped\n",\r\nwork->ipprt);\r\ncvm_oct_free_work(work);\r\ncontinue;\r\n}\r\nif (unlikely(work->word2.s.bufs == 0)) {\r\nuint8_t *ptr = work->packet_data;\r\nif (likely(!work->word2.s.not_IP)) {\r\nif (work->word2.s.is_v6)\r\nptr += 2;\r\nelse\r\nptr += 6;\r\n}\r\nmemcpy(skb_put(skb, work->len), ptr, work->len);\r\n} else {\r\nint segments = work->word2.s.bufs;\r\nunion cvmx_buf_ptr segment_ptr = work->packet_ptr;\r\nint len = work->len;\r\nwhile (segments--) {\r\nunion cvmx_buf_ptr next_ptr =\r\n*(union cvmx_buf_ptr *)cvmx_phys_to_ptr(segment_ptr.s.addr - 8);\r\nint segment_size = CVMX_FPA_PACKET_POOL_SIZE -\r\n(segment_ptr.s.addr - (((segment_ptr.s.addr >> 7) - segment_ptr.s.back) << 7));\r\nif (segment_size > len)\r\nsegment_size = len;\r\nmemcpy(skb_put(skb, segment_size),\r\ncvmx_phys_to_ptr(segment_ptr.s.addr),\r\nsegment_size);\r\nlen -= segment_size;\r\nsegment_ptr = next_ptr;\r\n}\r\n}\r\npacket_not_copied = 0;\r\n}\r\nif (likely((work->ipprt < TOTAL_NUMBER_OF_PORTS) &&\r\ncvm_oct_device[work->ipprt])) {\r\nstruct net_device *dev = cvm_oct_device[work->ipprt];\r\nstruct octeon_ethernet *priv = netdev_priv(dev);\r\nif (likely(dev->flags & IFF_UP)) {\r\nskb->protocol = eth_type_trans(skb, dev);\r\nskb->dev = dev;\r\nif (unlikely(work->word2.s.not_IP || work->word2.s.IP_exc ||\r\nwork->word2.s.L4_error || !work->word2.s.tcp_or_udp))\r\nskb->ip_summed = CHECKSUM_NONE;\r\nelse\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nif (work->ipprt >= CVMX_PIP_NUM_INPUT_PORTS) {\r\n#ifdef CONFIG_64BIT\r\natomic64_add(1, (atomic64_t *)&priv->stats.rx_packets);\r\natomic64_add(skb->len, (atomic64_t *)&priv->stats.rx_bytes);\r\n#else\r\natomic_add(1, (atomic_t *)&priv->stats.rx_packets);\r\natomic_add(skb->len, (atomic_t *)&priv->stats.rx_bytes);\r\n#endif\r\n}\r\nnetif_receive_skb(skb);\r\nrx_count++;\r\n} else {\r\n#ifdef CONFIG_64BIT\r\natomic64_add(1, (atomic64_t *)&priv->stats.rx_dropped);\r\n#else\r\natomic_add(1, (atomic_t *)&priv->stats.rx_dropped);\r\n#endif\r\ndev_kfree_skb_irq(skb);\r\n}\r\n} else {\r\nprintk_ratelimited("Port %d not controlled by Linux, packet dropped\n",\r\nwork->ipprt);\r\ndev_kfree_skb_irq(skb);\r\n}\r\nif (USE_SKBUFFS_IN_HW && likely(packet_not_copied)) {\r\ncvmx_fau_atomic_add32(FAU_NUM_PACKET_BUFFERS_TO_FREE,\r\n1);\r\ncvmx_fpa_free(work, CVMX_FPA_WQE_POOL,\r\nDONT_WRITEBACK(1));\r\n} else {\r\ncvm_oct_free_work(work);\r\n}\r\n}\r\ncvmx_write_csr(CVMX_POW_PP_GRP_MSKX(coreid), old_group_mask);\r\nif (USE_ASYNC_IOBDMA) {\r\ncvmx_scratch_write64(CVMX_SCR_SCRATCH, old_scratch);\r\n}\r\ncvm_oct_rx_refill_pool(0);\r\nif (rx_count < budget && napi != NULL) {\r\nnapi_complete(napi);\r\ncvm_oct_no_more_work();\r\n}\r\nreturn rx_count;\r\n}\r\nvoid cvm_oct_poll_controller(struct net_device *dev)\r\n{\r\ncvm_oct_napi_poll(NULL, 16);\r\n}\r\nvoid cvm_oct_rx_initialize(void)\r\n{\r\nint i;\r\nstruct net_device *dev_for_napi = NULL;\r\nunion cvmx_pow_wq_int_thrx int_thr;\r\nunion cvmx_pow_wq_int_pc int_pc;\r\nfor (i = 0; i < TOTAL_NUMBER_OF_PORTS; i++) {\r\nif (cvm_oct_device[i]) {\r\ndev_for_napi = cvm_oct_device[i];\r\nbreak;\r\n}\r\n}\r\nif (NULL == dev_for_napi)\r\npanic("No net_devices were allocated.");\r\nif (max_rx_cpus > 1 && max_rx_cpus < num_online_cpus())\r\natomic_set(&core_state.available_cores, max_rx_cpus);\r\nelse\r\natomic_set(&core_state.available_cores, num_online_cpus());\r\ncore_state.baseline_cores = atomic_read(&core_state.available_cores);\r\ncore_state.cpu_state = CPU_MASK_NONE;\r\nfor_each_possible_cpu(i) {\r\nnetif_napi_add(dev_for_napi, &cvm_oct_napi[i].napi,\r\ncvm_oct_napi_poll, rx_napi_weight);\r\nnapi_enable(&cvm_oct_napi[i].napi);\r\n}\r\ni = request_irq(OCTEON_IRQ_WORKQ0 + pow_receive_group,\r\ncvm_oct_do_interrupt, 0, "Ethernet", cvm_oct_device);\r\nif (i)\r\npanic("Could not acquire Ethernet IRQ %d\n",\r\nOCTEON_IRQ_WORKQ0 + pow_receive_group);\r\ndisable_irq_nosync(OCTEON_IRQ_WORKQ0 + pow_receive_group);\r\nint_thr.u64 = 0;\r\nint_thr.s.tc_en = 1;\r\nint_thr.s.tc_thr = 1;\r\ncvmx_write_csr(CVMX_POW_WQ_INT_THRX(pow_receive_group), int_thr.u64);\r\nint_pc.u64 = 0;\r\nint_pc.s.pc_thr = 5;\r\ncvmx_write_csr(CVMX_POW_WQ_INT_PC, int_pc.u64);\r\ncvm_oct_enable_one_cpu();\r\n}\r\nvoid cvm_oct_rx_shutdown(void)\r\n{\r\nint i;\r\nfor_each_possible_cpu(i)\r\nnetif_napi_del(&cvm_oct_napi[i].napi);\r\n}
