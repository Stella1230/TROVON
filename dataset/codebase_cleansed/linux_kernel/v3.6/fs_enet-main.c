static void fs_set_multicast_list(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\n(*fep->ops->set_multicast_list)(dev);\r\n}\r\nstatic void skb_align(struct sk_buff *skb, int align)\r\n{\r\nint off = ((unsigned long)skb->data) & (align - 1);\r\nif (off)\r\nskb_reserve(skb, align - off);\r\n}\r\nstatic int fs_enet_rx_napi(struct napi_struct *napi, int budget)\r\n{\r\nstruct fs_enet_private *fep = container_of(napi, struct fs_enet_private, napi);\r\nstruct net_device *dev = fep->ndev;\r\nconst struct fs_platform_info *fpi = fep->fpi;\r\ncbd_t __iomem *bdp;\r\nstruct sk_buff *skb, *skbn, *skbt;\r\nint received = 0;\r\nu16 pkt_len, sc;\r\nint curidx;\r\nbdp = fep->cur_rx;\r\n(*fep->ops->napi_clear_rx_event)(dev);\r\nwhile (((sc = CBDR_SC(bdp)) & BD_ENET_RX_EMPTY) == 0) {\r\ncuridx = bdp - fep->rx_bd_base;\r\nif ((sc & BD_ENET_RX_LAST) == 0)\r\ndev_warn(fep->dev, "rcv is not +last\n");\r\nif (sc & (BD_ENET_RX_LG | BD_ENET_RX_SH | BD_ENET_RX_CL |\r\nBD_ENET_RX_NO | BD_ENET_RX_CR | BD_ENET_RX_OV)) {\r\nfep->stats.rx_errors++;\r\nif (sc & (BD_ENET_RX_LG | BD_ENET_RX_SH))\r\nfep->stats.rx_length_errors++;\r\nif (sc & (BD_ENET_RX_NO | BD_ENET_RX_CL))\r\nfep->stats.rx_frame_errors++;\r\nif (sc & BD_ENET_RX_CR)\r\nfep->stats.rx_crc_errors++;\r\nif (sc & BD_ENET_RX_OV)\r\nfep->stats.rx_crc_errors++;\r\nskb = fep->rx_skbuff[curidx];\r\ndma_unmap_single(fep->dev, CBDR_BUFADDR(bdp),\r\nL1_CACHE_ALIGN(PKT_MAXBUF_SIZE),\r\nDMA_FROM_DEVICE);\r\nskbn = skb;\r\n} else {\r\nskb = fep->rx_skbuff[curidx];\r\ndma_unmap_single(fep->dev, CBDR_BUFADDR(bdp),\r\nL1_CACHE_ALIGN(PKT_MAXBUF_SIZE),\r\nDMA_FROM_DEVICE);\r\nfep->stats.rx_packets++;\r\npkt_len = CBDR_DATLEN(bdp) - 4;\r\nfep->stats.rx_bytes += pkt_len + 4;\r\nif (pkt_len <= fpi->rx_copybreak) {\r\nskbn = netdev_alloc_skb(dev, pkt_len + 2);\r\nif (skbn != NULL) {\r\nskb_reserve(skbn, 2);\r\nskb_copy_from_linear_data(skb,\r\nskbn->data, pkt_len);\r\nskbt = skb;\r\nskb = skbn;\r\nskbn = skbt;\r\n}\r\n} else {\r\nskbn = netdev_alloc_skb(dev, ENET_RX_FRSIZE);\r\nif (skbn)\r\nskb_align(skbn, ENET_RX_ALIGN);\r\n}\r\nif (skbn != NULL) {\r\nskb_put(skb, pkt_len);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nreceived++;\r\nnetif_receive_skb(skb);\r\n} else {\r\ndev_warn(fep->dev,\r\n"Memory squeeze, dropping packet.\n");\r\nfep->stats.rx_dropped++;\r\nskbn = skb;\r\n}\r\n}\r\nfep->rx_skbuff[curidx] = skbn;\r\nCBDW_BUFADDR(bdp, dma_map_single(fep->dev, skbn->data,\r\nL1_CACHE_ALIGN(PKT_MAXBUF_SIZE),\r\nDMA_FROM_DEVICE));\r\nCBDW_DATLEN(bdp, 0);\r\nCBDW_SC(bdp, (sc & ~BD_ENET_RX_STATS) | BD_ENET_RX_EMPTY);\r\nif ((sc & BD_ENET_RX_WRAP) == 0)\r\nbdp++;\r\nelse\r\nbdp = fep->rx_bd_base;\r\n(*fep->ops->rx_bd_done)(dev);\r\nif (received >= budget)\r\nbreak;\r\n}\r\nfep->cur_rx = bdp;\r\nif (received < budget) {\r\nnapi_complete(napi);\r\n(*fep->ops->napi_enable_rx)(dev);\r\n}\r\nreturn received;\r\n}\r\nstatic int fs_enet_rx_non_napi(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nconst struct fs_platform_info *fpi = fep->fpi;\r\ncbd_t __iomem *bdp;\r\nstruct sk_buff *skb, *skbn, *skbt;\r\nint received = 0;\r\nu16 pkt_len, sc;\r\nint curidx;\r\nbdp = fep->cur_rx;\r\nwhile (((sc = CBDR_SC(bdp)) & BD_ENET_RX_EMPTY) == 0) {\r\ncuridx = bdp - fep->rx_bd_base;\r\nif ((sc & BD_ENET_RX_LAST) == 0)\r\ndev_warn(fep->dev, "rcv is not +last\n");\r\nif (sc & (BD_ENET_RX_LG | BD_ENET_RX_SH | BD_ENET_RX_CL |\r\nBD_ENET_RX_NO | BD_ENET_RX_CR | BD_ENET_RX_OV)) {\r\nfep->stats.rx_errors++;\r\nif (sc & (BD_ENET_RX_LG | BD_ENET_RX_SH))\r\nfep->stats.rx_length_errors++;\r\nif (sc & (BD_ENET_RX_NO | BD_ENET_RX_CL))\r\nfep->stats.rx_frame_errors++;\r\nif (sc & BD_ENET_RX_CR)\r\nfep->stats.rx_crc_errors++;\r\nif (sc & BD_ENET_RX_OV)\r\nfep->stats.rx_crc_errors++;\r\nskb = fep->rx_skbuff[curidx];\r\ndma_unmap_single(fep->dev, CBDR_BUFADDR(bdp),\r\nL1_CACHE_ALIGN(PKT_MAXBUF_SIZE),\r\nDMA_FROM_DEVICE);\r\nskbn = skb;\r\n} else {\r\nskb = fep->rx_skbuff[curidx];\r\ndma_unmap_single(fep->dev, CBDR_BUFADDR(bdp),\r\nL1_CACHE_ALIGN(PKT_MAXBUF_SIZE),\r\nDMA_FROM_DEVICE);\r\nfep->stats.rx_packets++;\r\npkt_len = CBDR_DATLEN(bdp) - 4;\r\nfep->stats.rx_bytes += pkt_len + 4;\r\nif (pkt_len <= fpi->rx_copybreak) {\r\nskbn = netdev_alloc_skb(dev, pkt_len + 2);\r\nif (skbn != NULL) {\r\nskb_reserve(skbn, 2);\r\nskb_copy_from_linear_data(skb,\r\nskbn->data, pkt_len);\r\nskbt = skb;\r\nskb = skbn;\r\nskbn = skbt;\r\n}\r\n} else {\r\nskbn = netdev_alloc_skb(dev, ENET_RX_FRSIZE);\r\nif (skbn)\r\nskb_align(skbn, ENET_RX_ALIGN);\r\n}\r\nif (skbn != NULL) {\r\nskb_put(skb, pkt_len);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nreceived++;\r\nnetif_rx(skb);\r\n} else {\r\ndev_warn(fep->dev,\r\n"Memory squeeze, dropping packet.\n");\r\nfep->stats.rx_dropped++;\r\nskbn = skb;\r\n}\r\n}\r\nfep->rx_skbuff[curidx] = skbn;\r\nCBDW_BUFADDR(bdp, dma_map_single(fep->dev, skbn->data,\r\nL1_CACHE_ALIGN(PKT_MAXBUF_SIZE),\r\nDMA_FROM_DEVICE));\r\nCBDW_DATLEN(bdp, 0);\r\nCBDW_SC(bdp, (sc & ~BD_ENET_RX_STATS) | BD_ENET_RX_EMPTY);\r\nif ((sc & BD_ENET_RX_WRAP) == 0)\r\nbdp++;\r\nelse\r\nbdp = fep->rx_bd_base;\r\n(*fep->ops->rx_bd_done)(dev);\r\n}\r\nfep->cur_rx = bdp;\r\nreturn 0;\r\n}\r\nstatic void fs_enet_tx(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\ncbd_t __iomem *bdp;\r\nstruct sk_buff *skb;\r\nint dirtyidx, do_wake, do_restart;\r\nu16 sc;\r\nspin_lock(&fep->tx_lock);\r\nbdp = fep->dirty_tx;\r\ndo_wake = do_restart = 0;\r\nwhile (((sc = CBDR_SC(bdp)) & BD_ENET_TX_READY) == 0) {\r\ndirtyidx = bdp - fep->tx_bd_base;\r\nif (fep->tx_free == fep->tx_ring)\r\nbreak;\r\nskb = fep->tx_skbuff[dirtyidx];\r\nif (sc & (BD_ENET_TX_HB | BD_ENET_TX_LC |\r\nBD_ENET_TX_RL | BD_ENET_TX_UN | BD_ENET_TX_CSL)) {\r\nif (sc & BD_ENET_TX_HB)\r\nfep->stats.tx_heartbeat_errors++;\r\nif (sc & BD_ENET_TX_LC)\r\nfep->stats.tx_window_errors++;\r\nif (sc & BD_ENET_TX_RL)\r\nfep->stats.tx_aborted_errors++;\r\nif (sc & BD_ENET_TX_UN)\r\nfep->stats.tx_fifo_errors++;\r\nif (sc & BD_ENET_TX_CSL)\r\nfep->stats.tx_carrier_errors++;\r\nif (sc & (BD_ENET_TX_LC | BD_ENET_TX_RL | BD_ENET_TX_UN)) {\r\nfep->stats.tx_errors++;\r\ndo_restart = 1;\r\n}\r\n} else\r\nfep->stats.tx_packets++;\r\nif (sc & BD_ENET_TX_READY) {\r\ndev_warn(fep->dev,\r\n"HEY! Enet xmit interrupt and TX_READY.\n");\r\n}\r\nif (sc & BD_ENET_TX_DEF)\r\nfep->stats.collisions++;\r\ndma_unmap_single(fep->dev, CBDR_BUFADDR(bdp),\r\nskb->len, DMA_TO_DEVICE);\r\ndev_kfree_skb_irq(skb);\r\nfep->tx_skbuff[dirtyidx] = NULL;\r\nif ((sc & BD_ENET_TX_WRAP) == 0)\r\nbdp++;\r\nelse\r\nbdp = fep->tx_bd_base;\r\nif (!fep->tx_free++)\r\ndo_wake = 1;\r\n}\r\nfep->dirty_tx = bdp;\r\nif (do_restart)\r\n(*fep->ops->tx_restart)(dev);\r\nspin_unlock(&fep->tx_lock);\r\nif (do_wake)\r\nnetif_wake_queue(dev);\r\n}\r\nstatic irqreturn_t\r\nfs_enet_interrupt(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev = dev_id;\r\nstruct fs_enet_private *fep;\r\nconst struct fs_platform_info *fpi;\r\nu32 int_events;\r\nu32 int_clr_events;\r\nint nr, napi_ok;\r\nint handled;\r\nfep = netdev_priv(dev);\r\nfpi = fep->fpi;\r\nnr = 0;\r\nwhile ((int_events = (*fep->ops->get_int_events)(dev)) != 0) {\r\nnr++;\r\nint_clr_events = int_events;\r\nif (fpi->use_napi)\r\nint_clr_events &= ~fep->ev_napi_rx;\r\n(*fep->ops->clear_int_events)(dev, int_clr_events);\r\nif (int_events & fep->ev_err)\r\n(*fep->ops->ev_error)(dev, int_events);\r\nif (int_events & fep->ev_rx) {\r\nif (!fpi->use_napi)\r\nfs_enet_rx_non_napi(dev);\r\nelse {\r\nnapi_ok = napi_schedule_prep(&fep->napi);\r\n(*fep->ops->napi_disable_rx)(dev);\r\n(*fep->ops->clear_int_events)(dev, fep->ev_napi_rx);\r\nif (napi_ok)\r\n__napi_schedule(&fep->napi);\r\n}\r\n}\r\nif (int_events & fep->ev_tx)\r\nfs_enet_tx(dev);\r\n}\r\nhandled = nr > 0;\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nvoid fs_init_bds(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\ncbd_t __iomem *bdp;\r\nstruct sk_buff *skb;\r\nint i;\r\nfs_cleanup_bds(dev);\r\nfep->dirty_tx = fep->cur_tx = fep->tx_bd_base;\r\nfep->tx_free = fep->tx_ring;\r\nfep->cur_rx = fep->rx_bd_base;\r\nfor (i = 0, bdp = fep->rx_bd_base; i < fep->rx_ring; i++, bdp++) {\r\nskb = netdev_alloc_skb(dev, ENET_RX_FRSIZE);\r\nif (skb == NULL) {\r\ndev_warn(fep->dev,\r\n"Memory squeeze, unable to allocate skb\n");\r\nbreak;\r\n}\r\nskb_align(skb, ENET_RX_ALIGN);\r\nfep->rx_skbuff[i] = skb;\r\nCBDW_BUFADDR(bdp,\r\ndma_map_single(fep->dev, skb->data,\r\nL1_CACHE_ALIGN(PKT_MAXBUF_SIZE),\r\nDMA_FROM_DEVICE));\r\nCBDW_DATLEN(bdp, 0);\r\nCBDW_SC(bdp, BD_ENET_RX_EMPTY |\r\n((i < fep->rx_ring - 1) ? 0 : BD_SC_WRAP));\r\n}\r\nfor (; i < fep->rx_ring; i++, bdp++) {\r\nfep->rx_skbuff[i] = NULL;\r\nCBDW_SC(bdp, (i < fep->rx_ring - 1) ? 0 : BD_SC_WRAP);\r\n}\r\nfor (i = 0, bdp = fep->tx_bd_base; i < fep->tx_ring; i++, bdp++) {\r\nfep->tx_skbuff[i] = NULL;\r\nCBDW_BUFADDR(bdp, 0);\r\nCBDW_DATLEN(bdp, 0);\r\nCBDW_SC(bdp, (i < fep->tx_ring - 1) ? 0 : BD_SC_WRAP);\r\n}\r\n}\r\nvoid fs_cleanup_bds(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nstruct sk_buff *skb;\r\ncbd_t __iomem *bdp;\r\nint i;\r\nfor (i = 0, bdp = fep->tx_bd_base; i < fep->tx_ring; i++, bdp++) {\r\nif ((skb = fep->tx_skbuff[i]) == NULL)\r\ncontinue;\r\ndma_unmap_single(fep->dev, CBDR_BUFADDR(bdp),\r\nskb->len, DMA_TO_DEVICE);\r\nfep->tx_skbuff[i] = NULL;\r\ndev_kfree_skb(skb);\r\n}\r\nfor (i = 0, bdp = fep->rx_bd_base; i < fep->rx_ring; i++, bdp++) {\r\nif ((skb = fep->rx_skbuff[i]) == NULL)\r\ncontinue;\r\ndma_unmap_single(fep->dev, CBDR_BUFADDR(bdp),\r\nL1_CACHE_ALIGN(PKT_MAXBUF_SIZE),\r\nDMA_FROM_DEVICE);\r\nfep->rx_skbuff[i] = NULL;\r\ndev_kfree_skb(skb);\r\n}\r\n}\r\nstatic struct sk_buff *tx_skb_align_workaround(struct net_device *dev,\r\nstruct sk_buff *skb)\r\n{\r\nstruct sk_buff *new_skb;\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nnew_skb = netdev_alloc_skb(dev, skb->len + 4);\r\nif (!new_skb) {\r\nif (net_ratelimit()) {\r\ndev_warn(fep->dev,\r\n"Memory squeeze, dropping tx packet.\n");\r\n}\r\nreturn NULL;\r\n}\r\nskb_align(new_skb, 4);\r\nskb_copy_from_linear_data(skb, new_skb->data, skb->len);\r\nskb_put(new_skb, skb->len);\r\ndev_kfree_skb_any(skb);\r\nreturn new_skb;\r\n}\r\nstatic int fs_enet_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\ncbd_t __iomem *bdp;\r\nint curidx;\r\nu16 sc;\r\nunsigned long flags;\r\n#ifdef CONFIG_FS_ENET_MPC5121_FEC\r\nif (((unsigned long)skb->data) & 0x3) {\r\nskb = tx_skb_align_workaround(dev, skb);\r\nif (!skb) {\r\nreturn NETDEV_TX_BUSY;\r\n}\r\n}\r\n#endif\r\nspin_lock_irqsave(&fep->tx_lock, flags);\r\nbdp = fep->cur_tx;\r\nif (!fep->tx_free || (CBDR_SC(bdp) & BD_ENET_TX_READY)) {\r\nnetif_stop_queue(dev);\r\nspin_unlock_irqrestore(&fep->tx_lock, flags);\r\ndev_warn(fep->dev, "tx queue full!.\n");\r\nreturn NETDEV_TX_BUSY;\r\n}\r\ncuridx = bdp - fep->tx_bd_base;\r\nCBDC_SC(bdp, BD_ENET_TX_STATS);\r\nfep->tx_skbuff[curidx] = skb;\r\nfep->stats.tx_bytes += skb->len;\r\nCBDW_BUFADDR(bdp, dma_map_single(fep->dev,\r\nskb->data, skb->len, DMA_TO_DEVICE));\r\nCBDW_DATLEN(bdp, skb->len);\r\nif ((CBDR_SC(bdp) & BD_ENET_TX_WRAP) == 0)\r\nfep->cur_tx++;\r\nelse\r\nfep->cur_tx = fep->tx_bd_base;\r\nif (!--fep->tx_free)\r\nnetif_stop_queue(dev);\r\nsc = BD_ENET_TX_READY | BD_ENET_TX_INTR |\r\nBD_ENET_TX_LAST | BD_ENET_TX_TC;\r\nif (skb->len <= 60)\r\nsc |= BD_ENET_TX_PAD;\r\nCBDS_SC(bdp, sc);\r\nskb_tx_timestamp(skb);\r\n(*fep->ops->tx_kickstart)(dev);\r\nspin_unlock_irqrestore(&fep->tx_lock, flags);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void fs_timeout(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nunsigned long flags;\r\nint wake = 0;\r\nfep->stats.tx_errors++;\r\nspin_lock_irqsave(&fep->lock, flags);\r\nif (dev->flags & IFF_UP) {\r\nphy_stop(fep->phydev);\r\n(*fep->ops->stop)(dev);\r\n(*fep->ops->restart)(dev);\r\nphy_start(fep->phydev);\r\n}\r\nphy_start(fep->phydev);\r\nwake = fep->tx_free && !(CBDR_SC(fep->cur_tx) & BD_ENET_TX_READY);\r\nspin_unlock_irqrestore(&fep->lock, flags);\r\nif (wake)\r\nnetif_wake_queue(dev);\r\n}\r\nstatic void generic_adjust_link(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nstruct phy_device *phydev = fep->phydev;\r\nint new_state = 0;\r\nif (phydev->link) {\r\nif (phydev->duplex != fep->oldduplex) {\r\nnew_state = 1;\r\nfep->oldduplex = phydev->duplex;\r\n}\r\nif (phydev->speed != fep->oldspeed) {\r\nnew_state = 1;\r\nfep->oldspeed = phydev->speed;\r\n}\r\nif (!fep->oldlink) {\r\nnew_state = 1;\r\nfep->oldlink = 1;\r\n}\r\nif (new_state)\r\nfep->ops->restart(dev);\r\n} else if (fep->oldlink) {\r\nnew_state = 1;\r\nfep->oldlink = 0;\r\nfep->oldspeed = 0;\r\nfep->oldduplex = -1;\r\n}\r\nif (new_state && netif_msg_link(fep))\r\nphy_print_status(phydev);\r\n}\r\nstatic void fs_adjust_link(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nunsigned long flags;\r\nspin_lock_irqsave(&fep->lock, flags);\r\nif(fep->ops->adjust_link)\r\nfep->ops->adjust_link(dev);\r\nelse\r\ngeneric_adjust_link(dev);\r\nspin_unlock_irqrestore(&fep->lock, flags);\r\n}\r\nstatic int fs_init_phy(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nstruct phy_device *phydev;\r\nphy_interface_t iface;\r\nfep->oldlink = 0;\r\nfep->oldspeed = 0;\r\nfep->oldduplex = -1;\r\niface = fep->fpi->use_rmii ?\r\nPHY_INTERFACE_MODE_RMII : PHY_INTERFACE_MODE_MII;\r\nphydev = of_phy_connect(dev, fep->fpi->phy_node, &fs_adjust_link, 0,\r\niface);\r\nif (!phydev) {\r\nphydev = of_phy_connect_fixed_link(dev, &fs_adjust_link,\r\niface);\r\n}\r\nif (!phydev) {\r\ndev_err(&dev->dev, "Could not attach to PHY\n");\r\nreturn -ENODEV;\r\n}\r\nfep->phydev = phydev;\r\nreturn 0;\r\n}\r\nstatic int fs_enet_open(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nint r;\r\nint err;\r\nfs_init_bds(fep->ndev);\r\nif (fep->fpi->use_napi)\r\nnapi_enable(&fep->napi);\r\nr = request_irq(fep->interrupt, fs_enet_interrupt, IRQF_SHARED,\r\n"fs_enet-mac", dev);\r\nif (r != 0) {\r\ndev_err(fep->dev, "Could not allocate FS_ENET IRQ!");\r\nif (fep->fpi->use_napi)\r\nnapi_disable(&fep->napi);\r\nreturn -EINVAL;\r\n}\r\nerr = fs_init_phy(dev);\r\nif (err) {\r\nfree_irq(fep->interrupt, dev);\r\nif (fep->fpi->use_napi)\r\nnapi_disable(&fep->napi);\r\nreturn err;\r\n}\r\nphy_start(fep->phydev);\r\nnetif_start_queue(dev);\r\nreturn 0;\r\n}\r\nstatic int fs_enet_close(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nunsigned long flags;\r\nnetif_stop_queue(dev);\r\nnetif_carrier_off(dev);\r\nif (fep->fpi->use_napi)\r\nnapi_disable(&fep->napi);\r\nphy_stop(fep->phydev);\r\nspin_lock_irqsave(&fep->lock, flags);\r\nspin_lock(&fep->tx_lock);\r\n(*fep->ops->stop)(dev);\r\nspin_unlock(&fep->tx_lock);\r\nspin_unlock_irqrestore(&fep->lock, flags);\r\nphy_disconnect(fep->phydev);\r\nfep->phydev = NULL;\r\nfree_irq(fep->interrupt, dev);\r\nreturn 0;\r\n}\r\nstatic struct net_device_stats *fs_enet_get_stats(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nreturn &fep->stats;\r\n}\r\nstatic void fs_get_drvinfo(struct net_device *dev,\r\nstruct ethtool_drvinfo *info)\r\n{\r\nstrcpy(info->driver, DRV_MODULE_NAME);\r\nstrcpy(info->version, DRV_MODULE_VERSION);\r\n}\r\nstatic int fs_get_regs_len(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nreturn (*fep->ops->get_regs_len)(dev);\r\n}\r\nstatic void fs_get_regs(struct net_device *dev, struct ethtool_regs *regs,\r\nvoid *p)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nunsigned long flags;\r\nint r, len;\r\nlen = regs->len;\r\nspin_lock_irqsave(&fep->lock, flags);\r\nr = (*fep->ops->get_regs)(dev, p, &len);\r\nspin_unlock_irqrestore(&fep->lock, flags);\r\nif (r == 0)\r\nregs->version = 0;\r\n}\r\nstatic int fs_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nif (!fep->phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_gset(fep->phydev, cmd);\r\n}\r\nstatic int fs_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nif (!fep->phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_sset(fep->phydev, cmd);\r\n}\r\nstatic int fs_nway_reset(struct net_device *dev)\r\n{\r\nreturn 0;\r\n}\r\nstatic u32 fs_get_msglevel(struct net_device *dev)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nreturn fep->msg_enable;\r\n}\r\nstatic void fs_set_msglevel(struct net_device *dev, u32 value)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nfep->msg_enable = value;\r\n}\r\nstatic int fs_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct fs_enet_private *fep = netdev_priv(dev);\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nreturn phy_mii_ioctl(fep->phydev, rq, cmd);\r\n}\r\nstatic int __devinit fs_enet_probe(struct platform_device *ofdev)\r\n{\r\nconst struct of_device_id *match;\r\nstruct net_device *ndev;\r\nstruct fs_enet_private *fep;\r\nstruct fs_platform_info *fpi;\r\nconst u32 *data;\r\nconst u8 *mac_addr;\r\nconst char *phy_connection_type;\r\nint privsize, len, ret = -ENODEV;\r\nmatch = of_match_device(fs_enet_match, &ofdev->dev);\r\nif (!match)\r\nreturn -EINVAL;\r\nfpi = kzalloc(sizeof(*fpi), GFP_KERNEL);\r\nif (!fpi)\r\nreturn -ENOMEM;\r\nif (!IS_FEC(match)) {\r\ndata = of_get_property(ofdev->dev.of_node, "fsl,cpm-command", &len);\r\nif (!data || len != 4)\r\ngoto out_free_fpi;\r\nfpi->cp_command = *data;\r\n}\r\nfpi->rx_ring = 32;\r\nfpi->tx_ring = 32;\r\nfpi->rx_copybreak = 240;\r\nfpi->use_napi = 1;\r\nfpi->napi_weight = 17;\r\nfpi->phy_node = of_parse_phandle(ofdev->dev.of_node, "phy-handle", 0);\r\nif ((!fpi->phy_node) && (!of_get_property(ofdev->dev.of_node, "fixed-link",\r\nNULL)))\r\ngoto out_free_fpi;\r\nif (of_device_is_compatible(ofdev->dev.of_node, "fsl,mpc5125-fec")) {\r\nphy_connection_type = of_get_property(ofdev->dev.of_node,\r\n"phy-connection-type", NULL);\r\nif (phy_connection_type && !strcmp("rmii", phy_connection_type))\r\nfpi->use_rmii = 1;\r\n}\r\nprivsize = sizeof(*fep) +\r\nsizeof(struct sk_buff **) *\r\n(fpi->rx_ring + fpi->tx_ring);\r\nndev = alloc_etherdev(privsize);\r\nif (!ndev) {\r\nret = -ENOMEM;\r\ngoto out_put;\r\n}\r\nSET_NETDEV_DEV(ndev, &ofdev->dev);\r\ndev_set_drvdata(&ofdev->dev, ndev);\r\nfep = netdev_priv(ndev);\r\nfep->dev = &ofdev->dev;\r\nfep->ndev = ndev;\r\nfep->fpi = fpi;\r\nfep->ops = match->data;\r\nret = fep->ops->setup_data(ndev);\r\nif (ret)\r\ngoto out_free_dev;\r\nfep->rx_skbuff = (struct sk_buff **)&fep[1];\r\nfep->tx_skbuff = fep->rx_skbuff + fpi->rx_ring;\r\nspin_lock_init(&fep->lock);\r\nspin_lock_init(&fep->tx_lock);\r\nmac_addr = of_get_mac_address(ofdev->dev.of_node);\r\nif (mac_addr)\r\nmemcpy(ndev->dev_addr, mac_addr, 6);\r\nret = fep->ops->allocate_bd(ndev);\r\nif (ret)\r\ngoto out_cleanup_data;\r\nfep->rx_bd_base = fep->ring_base;\r\nfep->tx_bd_base = fep->rx_bd_base + fpi->rx_ring;\r\nfep->tx_ring = fpi->tx_ring;\r\nfep->rx_ring = fpi->rx_ring;\r\nndev->netdev_ops = &fs_enet_netdev_ops;\r\nndev->watchdog_timeo = 2 * HZ;\r\nif (fpi->use_napi)\r\nnetif_napi_add(ndev, &fep->napi, fs_enet_rx_napi,\r\nfpi->napi_weight);\r\nndev->ethtool_ops = &fs_ethtool_ops;\r\ninit_timer(&fep->phy_timer_list);\r\nnetif_carrier_off(ndev);\r\nret = register_netdev(ndev);\r\nif (ret)\r\ngoto out_free_bd;\r\npr_info("%s: fs_enet: %pM\n", ndev->name, ndev->dev_addr);\r\nreturn 0;\r\nout_free_bd:\r\nfep->ops->free_bd(ndev);\r\nout_cleanup_data:\r\nfep->ops->cleanup_data(ndev);\r\nout_free_dev:\r\nfree_netdev(ndev);\r\ndev_set_drvdata(&ofdev->dev, NULL);\r\nout_put:\r\nof_node_put(fpi->phy_node);\r\nout_free_fpi:\r\nkfree(fpi);\r\nreturn ret;\r\n}\r\nstatic int fs_enet_remove(struct platform_device *ofdev)\r\n{\r\nstruct net_device *ndev = dev_get_drvdata(&ofdev->dev);\r\nstruct fs_enet_private *fep = netdev_priv(ndev);\r\nunregister_netdev(ndev);\r\nfep->ops->free_bd(ndev);\r\nfep->ops->cleanup_data(ndev);\r\ndev_set_drvdata(fep->dev, NULL);\r\nof_node_put(fep->fpi->phy_node);\r\nfree_netdev(ndev);\r\nreturn 0;\r\n}\r\nstatic void fs_enet_netpoll(struct net_device *dev)\r\n{\r\ndisable_irq(dev->irq);\r\nfs_enet_interrupt(dev->irq, dev);\r\nenable_irq(dev->irq);\r\n}
