static int __init ksm_slab_init(void)\r\n{\r\nrmap_item_cache = KSM_KMEM_CACHE(rmap_item, 0);\r\nif (!rmap_item_cache)\r\ngoto out;\r\nstable_node_cache = KSM_KMEM_CACHE(stable_node, 0);\r\nif (!stable_node_cache)\r\ngoto out_free1;\r\nmm_slot_cache = KSM_KMEM_CACHE(mm_slot, 0);\r\nif (!mm_slot_cache)\r\ngoto out_free2;\r\nreturn 0;\r\nout_free2:\r\nkmem_cache_destroy(stable_node_cache);\r\nout_free1:\r\nkmem_cache_destroy(rmap_item_cache);\r\nout:\r\nreturn -ENOMEM;\r\n}\r\nstatic void __init ksm_slab_free(void)\r\n{\r\nkmem_cache_destroy(mm_slot_cache);\r\nkmem_cache_destroy(stable_node_cache);\r\nkmem_cache_destroy(rmap_item_cache);\r\nmm_slot_cache = NULL;\r\n}\r\nstatic inline struct rmap_item *alloc_rmap_item(void)\r\n{\r\nstruct rmap_item *rmap_item;\r\nrmap_item = kmem_cache_zalloc(rmap_item_cache, GFP_KERNEL);\r\nif (rmap_item)\r\nksm_rmap_items++;\r\nreturn rmap_item;\r\n}\r\nstatic inline void free_rmap_item(struct rmap_item *rmap_item)\r\n{\r\nksm_rmap_items--;\r\nrmap_item->mm = NULL;\r\nkmem_cache_free(rmap_item_cache, rmap_item);\r\n}\r\nstatic inline struct stable_node *alloc_stable_node(void)\r\n{\r\nreturn kmem_cache_alloc(stable_node_cache, GFP_KERNEL);\r\n}\r\nstatic inline void free_stable_node(struct stable_node *stable_node)\r\n{\r\nkmem_cache_free(stable_node_cache, stable_node);\r\n}\r\nstatic inline struct mm_slot *alloc_mm_slot(void)\r\n{\r\nif (!mm_slot_cache)\r\nreturn NULL;\r\nreturn kmem_cache_zalloc(mm_slot_cache, GFP_KERNEL);\r\n}\r\nstatic inline void free_mm_slot(struct mm_slot *mm_slot)\r\n{\r\nkmem_cache_free(mm_slot_cache, mm_slot);\r\n}\r\nstatic struct mm_slot *get_mm_slot(struct mm_struct *mm)\r\n{\r\nstruct mm_slot *mm_slot;\r\nstruct hlist_head *bucket;\r\nstruct hlist_node *node;\r\nbucket = &mm_slots_hash[hash_ptr(mm, MM_SLOTS_HASH_SHIFT)];\r\nhlist_for_each_entry(mm_slot, node, bucket, link) {\r\nif (mm == mm_slot->mm)\r\nreturn mm_slot;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void insert_to_mm_slots_hash(struct mm_struct *mm,\r\nstruct mm_slot *mm_slot)\r\n{\r\nstruct hlist_head *bucket;\r\nbucket = &mm_slots_hash[hash_ptr(mm, MM_SLOTS_HASH_SHIFT)];\r\nmm_slot->mm = mm;\r\nhlist_add_head(&mm_slot->link, bucket);\r\n}\r\nstatic inline int in_stable_tree(struct rmap_item *rmap_item)\r\n{\r\nreturn rmap_item->address & STABLE_FLAG;\r\n}\r\nstatic inline bool ksm_test_exit(struct mm_struct *mm)\r\n{\r\nreturn atomic_read(&mm->mm_users) == 0;\r\n}\r\nstatic int break_ksm(struct vm_area_struct *vma, unsigned long addr)\r\n{\r\nstruct page *page;\r\nint ret = 0;\r\ndo {\r\ncond_resched();\r\npage = follow_page(vma, addr, FOLL_GET);\r\nif (IS_ERR_OR_NULL(page))\r\nbreak;\r\nif (PageKsm(page))\r\nret = handle_mm_fault(vma->vm_mm, vma, addr,\r\nFAULT_FLAG_WRITE);\r\nelse\r\nret = VM_FAULT_WRITE;\r\nput_page(page);\r\n} while (!(ret & (VM_FAULT_WRITE | VM_FAULT_SIGBUS | VM_FAULT_OOM)));\r\nreturn (ret & VM_FAULT_OOM) ? -ENOMEM : 0;\r\n}\r\nstatic struct vm_area_struct *find_mergeable_vma(struct mm_struct *mm,\r\nunsigned long addr)\r\n{\r\nstruct vm_area_struct *vma;\r\nif (ksm_test_exit(mm))\r\nreturn NULL;\r\nvma = find_vma(mm, addr);\r\nif (!vma || vma->vm_start > addr)\r\nreturn NULL;\r\nif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\r\nreturn NULL;\r\nreturn vma;\r\n}\r\nstatic void break_cow(struct rmap_item *rmap_item)\r\n{\r\nstruct mm_struct *mm = rmap_item->mm;\r\nunsigned long addr = rmap_item->address;\r\nstruct vm_area_struct *vma;\r\nput_anon_vma(rmap_item->anon_vma);\r\ndown_read(&mm->mmap_sem);\r\nvma = find_mergeable_vma(mm, addr);\r\nif (vma)\r\nbreak_ksm(vma, addr);\r\nup_read(&mm->mmap_sem);\r\n}\r\nstatic struct page *page_trans_compound_anon(struct page *page)\r\n{\r\nif (PageTransCompound(page)) {\r\nstruct page *head = compound_trans_head(page);\r\nif (PageAnon(head))\r\nreturn head;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct page *get_mergeable_page(struct rmap_item *rmap_item)\r\n{\r\nstruct mm_struct *mm = rmap_item->mm;\r\nunsigned long addr = rmap_item->address;\r\nstruct vm_area_struct *vma;\r\nstruct page *page;\r\ndown_read(&mm->mmap_sem);\r\nvma = find_mergeable_vma(mm, addr);\r\nif (!vma)\r\ngoto out;\r\npage = follow_page(vma, addr, FOLL_GET);\r\nif (IS_ERR_OR_NULL(page))\r\ngoto out;\r\nif (PageAnon(page) || page_trans_compound_anon(page)) {\r\nflush_anon_page(vma, page, addr);\r\nflush_dcache_page(page);\r\n} else {\r\nput_page(page);\r\nout: page = NULL;\r\n}\r\nup_read(&mm->mmap_sem);\r\nreturn page;\r\n}\r\nstatic void remove_node_from_stable_tree(struct stable_node *stable_node)\r\n{\r\nstruct rmap_item *rmap_item;\r\nstruct hlist_node *hlist;\r\nhlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\r\nif (rmap_item->hlist.next)\r\nksm_pages_sharing--;\r\nelse\r\nksm_pages_shared--;\r\nput_anon_vma(rmap_item->anon_vma);\r\nrmap_item->address &= PAGE_MASK;\r\ncond_resched();\r\n}\r\nrb_erase(&stable_node->node, &root_stable_tree);\r\nfree_stable_node(stable_node);\r\n}\r\nstatic struct page *get_ksm_page(struct stable_node *stable_node)\r\n{\r\nstruct page *page;\r\nvoid *expected_mapping;\r\npage = pfn_to_page(stable_node->kpfn);\r\nexpected_mapping = (void *)stable_node +\r\n(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM);\r\nrcu_read_lock();\r\nif (page->mapping != expected_mapping)\r\ngoto stale;\r\nif (!get_page_unless_zero(page))\r\ngoto stale;\r\nif (page->mapping != expected_mapping) {\r\nput_page(page);\r\ngoto stale;\r\n}\r\nrcu_read_unlock();\r\nreturn page;\r\nstale:\r\nrcu_read_unlock();\r\nremove_node_from_stable_tree(stable_node);\r\nreturn NULL;\r\n}\r\nstatic void remove_rmap_item_from_tree(struct rmap_item *rmap_item)\r\n{\r\nif (rmap_item->address & STABLE_FLAG) {\r\nstruct stable_node *stable_node;\r\nstruct page *page;\r\nstable_node = rmap_item->head;\r\npage = get_ksm_page(stable_node);\r\nif (!page)\r\ngoto out;\r\nlock_page(page);\r\nhlist_del(&rmap_item->hlist);\r\nunlock_page(page);\r\nput_page(page);\r\nif (stable_node->hlist.first)\r\nksm_pages_sharing--;\r\nelse\r\nksm_pages_shared--;\r\nput_anon_vma(rmap_item->anon_vma);\r\nrmap_item->address &= PAGE_MASK;\r\n} else if (rmap_item->address & UNSTABLE_FLAG) {\r\nunsigned char age;\r\nage = (unsigned char)(ksm_scan.seqnr - rmap_item->address);\r\nBUG_ON(age > 1);\r\nif (!age)\r\nrb_erase(&rmap_item->node, &root_unstable_tree);\r\nksm_pages_unshared--;\r\nrmap_item->address &= PAGE_MASK;\r\n}\r\nout:\r\ncond_resched();\r\n}\r\nstatic void remove_trailing_rmap_items(struct mm_slot *mm_slot,\r\nstruct rmap_item **rmap_list)\r\n{\r\nwhile (*rmap_list) {\r\nstruct rmap_item *rmap_item = *rmap_list;\r\n*rmap_list = rmap_item->rmap_list;\r\nremove_rmap_item_from_tree(rmap_item);\r\nfree_rmap_item(rmap_item);\r\n}\r\n}\r\nstatic int unmerge_ksm_pages(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end)\r\n{\r\nunsigned long addr;\r\nint err = 0;\r\nfor (addr = start; addr < end && !err; addr += PAGE_SIZE) {\r\nif (ksm_test_exit(vma->vm_mm))\r\nbreak;\r\nif (signal_pending(current))\r\nerr = -ERESTARTSYS;\r\nelse\r\nerr = break_ksm(vma, addr);\r\n}\r\nreturn err;\r\n}\r\nstatic int unmerge_and_remove_all_rmap_items(void)\r\n{\r\nstruct mm_slot *mm_slot;\r\nstruct mm_struct *mm;\r\nstruct vm_area_struct *vma;\r\nint err = 0;\r\nspin_lock(&ksm_mmlist_lock);\r\nksm_scan.mm_slot = list_entry(ksm_mm_head.mm_list.next,\r\nstruct mm_slot, mm_list);\r\nspin_unlock(&ksm_mmlist_lock);\r\nfor (mm_slot = ksm_scan.mm_slot;\r\nmm_slot != &ksm_mm_head; mm_slot = ksm_scan.mm_slot) {\r\nmm = mm_slot->mm;\r\ndown_read(&mm->mmap_sem);\r\nfor (vma = mm->mmap; vma; vma = vma->vm_next) {\r\nif (ksm_test_exit(mm))\r\nbreak;\r\nif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\r\ncontinue;\r\nerr = unmerge_ksm_pages(vma,\r\nvma->vm_start, vma->vm_end);\r\nif (err)\r\ngoto error;\r\n}\r\nremove_trailing_rmap_items(mm_slot, &mm_slot->rmap_list);\r\nspin_lock(&ksm_mmlist_lock);\r\nksm_scan.mm_slot = list_entry(mm_slot->mm_list.next,\r\nstruct mm_slot, mm_list);\r\nif (ksm_test_exit(mm)) {\r\nhlist_del(&mm_slot->link);\r\nlist_del(&mm_slot->mm_list);\r\nspin_unlock(&ksm_mmlist_lock);\r\nfree_mm_slot(mm_slot);\r\nclear_bit(MMF_VM_MERGEABLE, &mm->flags);\r\nup_read(&mm->mmap_sem);\r\nmmdrop(mm);\r\n} else {\r\nspin_unlock(&ksm_mmlist_lock);\r\nup_read(&mm->mmap_sem);\r\n}\r\n}\r\nksm_scan.seqnr = 0;\r\nreturn 0;\r\nerror:\r\nup_read(&mm->mmap_sem);\r\nspin_lock(&ksm_mmlist_lock);\r\nksm_scan.mm_slot = &ksm_mm_head;\r\nspin_unlock(&ksm_mmlist_lock);\r\nreturn err;\r\n}\r\nstatic u32 calc_checksum(struct page *page)\r\n{\r\nu32 checksum;\r\nvoid *addr = kmap_atomic(page);\r\nchecksum = jhash2(addr, PAGE_SIZE / 4, 17);\r\nkunmap_atomic(addr);\r\nreturn checksum;\r\n}\r\nstatic int memcmp_pages(struct page *page1, struct page *page2)\r\n{\r\nchar *addr1, *addr2;\r\nint ret;\r\naddr1 = kmap_atomic(page1);\r\naddr2 = kmap_atomic(page2);\r\nret = memcmp(addr1, addr2, PAGE_SIZE);\r\nkunmap_atomic(addr2);\r\nkunmap_atomic(addr1);\r\nreturn ret;\r\n}\r\nstatic inline int pages_identical(struct page *page1, struct page *page2)\r\n{\r\nreturn !memcmp_pages(page1, page2);\r\n}\r\nstatic int write_protect_page(struct vm_area_struct *vma, struct page *page,\r\npte_t *orig_pte)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nunsigned long addr;\r\npte_t *ptep;\r\nspinlock_t *ptl;\r\nint swapped;\r\nint err = -EFAULT;\r\naddr = page_address_in_vma(page, vma);\r\nif (addr == -EFAULT)\r\ngoto out;\r\nBUG_ON(PageTransCompound(page));\r\nptep = page_check_address(page, mm, addr, &ptl, 0);\r\nif (!ptep)\r\ngoto out;\r\nif (pte_write(*ptep) || pte_dirty(*ptep)) {\r\npte_t entry;\r\nswapped = PageSwapCache(page);\r\nflush_cache_page(vma, addr, page_to_pfn(page));\r\nentry = ptep_clear_flush(vma, addr, ptep);\r\nif (page_mapcount(page) + 1 + swapped != page_count(page)) {\r\nset_pte_at(mm, addr, ptep, entry);\r\ngoto out_unlock;\r\n}\r\nif (pte_dirty(entry))\r\nset_page_dirty(page);\r\nentry = pte_mkclean(pte_wrprotect(entry));\r\nset_pte_at_notify(mm, addr, ptep, entry);\r\n}\r\n*orig_pte = *ptep;\r\nerr = 0;\r\nout_unlock:\r\npte_unmap_unlock(ptep, ptl);\r\nout:\r\nreturn err;\r\n}\r\nstatic int replace_page(struct vm_area_struct *vma, struct page *page,\r\nstruct page *kpage, pte_t orig_pte)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *ptep;\r\nspinlock_t *ptl;\r\nunsigned long addr;\r\nint err = -EFAULT;\r\naddr = page_address_in_vma(page, vma);\r\nif (addr == -EFAULT)\r\ngoto out;\r\npgd = pgd_offset(mm, addr);\r\nif (!pgd_present(*pgd))\r\ngoto out;\r\npud = pud_offset(pgd, addr);\r\nif (!pud_present(*pud))\r\ngoto out;\r\npmd = pmd_offset(pud, addr);\r\nBUG_ON(pmd_trans_huge(*pmd));\r\nif (!pmd_present(*pmd))\r\ngoto out;\r\nptep = pte_offset_map_lock(mm, pmd, addr, &ptl);\r\nif (!pte_same(*ptep, orig_pte)) {\r\npte_unmap_unlock(ptep, ptl);\r\ngoto out;\r\n}\r\nget_page(kpage);\r\npage_add_anon_rmap(kpage, vma, addr);\r\nflush_cache_page(vma, addr, pte_pfn(*ptep));\r\nptep_clear_flush(vma, addr, ptep);\r\nset_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma->vm_page_prot));\r\npage_remove_rmap(page);\r\nif (!page_mapped(page))\r\ntry_to_free_swap(page);\r\nput_page(page);\r\npte_unmap_unlock(ptep, ptl);\r\nerr = 0;\r\nout:\r\nreturn err;\r\n}\r\nstatic int page_trans_compound_anon_split(struct page *page)\r\n{\r\nint ret = 0;\r\nstruct page *transhuge_head = page_trans_compound_anon(page);\r\nif (transhuge_head) {\r\nif (get_page_unless_zero(transhuge_head)) {\r\nif (PageAnon(transhuge_head))\r\nret = split_huge_page(transhuge_head);\r\nelse\r\nret = 1;\r\nput_page(transhuge_head);\r\n} else\r\nret = 1;\r\n}\r\nreturn ret;\r\n}\r\nstatic int try_to_merge_one_page(struct vm_area_struct *vma,\r\nstruct page *page, struct page *kpage)\r\n{\r\npte_t orig_pte = __pte(0);\r\nint err = -EFAULT;\r\nif (page == kpage)\r\nreturn 0;\r\nif (!(vma->vm_flags & VM_MERGEABLE))\r\ngoto out;\r\nif (PageTransCompound(page) && page_trans_compound_anon_split(page))\r\ngoto out;\r\nBUG_ON(PageTransCompound(page));\r\nif (!PageAnon(page))\r\ngoto out;\r\nif (!trylock_page(page))\r\ngoto out;\r\nif (write_protect_page(vma, page, &orig_pte) == 0) {\r\nif (!kpage) {\r\nset_page_stable_node(page, NULL);\r\nmark_page_accessed(page);\r\nerr = 0;\r\n} else if (pages_identical(page, kpage))\r\nerr = replace_page(vma, page, kpage, orig_pte);\r\n}\r\nif ((vma->vm_flags & VM_LOCKED) && kpage && !err) {\r\nmunlock_vma_page(page);\r\nif (!PageMlocked(kpage)) {\r\nunlock_page(page);\r\nlock_page(kpage);\r\nmlock_vma_page(kpage);\r\npage = kpage;\r\n}\r\n}\r\nunlock_page(page);\r\nout:\r\nreturn err;\r\n}\r\nstatic int try_to_merge_with_ksm_page(struct rmap_item *rmap_item,\r\nstruct page *page, struct page *kpage)\r\n{\r\nstruct mm_struct *mm = rmap_item->mm;\r\nstruct vm_area_struct *vma;\r\nint err = -EFAULT;\r\ndown_read(&mm->mmap_sem);\r\nif (ksm_test_exit(mm))\r\ngoto out;\r\nvma = find_vma(mm, rmap_item->address);\r\nif (!vma || vma->vm_start > rmap_item->address)\r\ngoto out;\r\nerr = try_to_merge_one_page(vma, page, kpage);\r\nif (err)\r\ngoto out;\r\nrmap_item->anon_vma = vma->anon_vma;\r\nget_anon_vma(vma->anon_vma);\r\nout:\r\nup_read(&mm->mmap_sem);\r\nreturn err;\r\n}\r\nstatic struct page *try_to_merge_two_pages(struct rmap_item *rmap_item,\r\nstruct page *page,\r\nstruct rmap_item *tree_rmap_item,\r\nstruct page *tree_page)\r\n{\r\nint err;\r\nerr = try_to_merge_with_ksm_page(rmap_item, page, NULL);\r\nif (!err) {\r\nerr = try_to_merge_with_ksm_page(tree_rmap_item,\r\ntree_page, page);\r\nif (err)\r\nbreak_cow(rmap_item);\r\n}\r\nreturn err ? NULL : page;\r\n}\r\nstatic struct page *stable_tree_search(struct page *page)\r\n{\r\nstruct rb_node *node = root_stable_tree.rb_node;\r\nstruct stable_node *stable_node;\r\nstable_node = page_stable_node(page);\r\nif (stable_node) {\r\nget_page(page);\r\nreturn page;\r\n}\r\nwhile (node) {\r\nstruct page *tree_page;\r\nint ret;\r\ncond_resched();\r\nstable_node = rb_entry(node, struct stable_node, node);\r\ntree_page = get_ksm_page(stable_node);\r\nif (!tree_page)\r\nreturn NULL;\r\nret = memcmp_pages(page, tree_page);\r\nif (ret < 0) {\r\nput_page(tree_page);\r\nnode = node->rb_left;\r\n} else if (ret > 0) {\r\nput_page(tree_page);\r\nnode = node->rb_right;\r\n} else\r\nreturn tree_page;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct stable_node *stable_tree_insert(struct page *kpage)\r\n{\r\nstruct rb_node **new = &root_stable_tree.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct stable_node *stable_node;\r\nwhile (*new) {\r\nstruct page *tree_page;\r\nint ret;\r\ncond_resched();\r\nstable_node = rb_entry(*new, struct stable_node, node);\r\ntree_page = get_ksm_page(stable_node);\r\nif (!tree_page)\r\nreturn NULL;\r\nret = memcmp_pages(kpage, tree_page);\r\nput_page(tree_page);\r\nparent = *new;\r\nif (ret < 0)\r\nnew = &parent->rb_left;\r\nelse if (ret > 0)\r\nnew = &parent->rb_right;\r\nelse {\r\nreturn NULL;\r\n}\r\n}\r\nstable_node = alloc_stable_node();\r\nif (!stable_node)\r\nreturn NULL;\r\nrb_link_node(&stable_node->node, parent, new);\r\nrb_insert_color(&stable_node->node, &root_stable_tree);\r\nINIT_HLIST_HEAD(&stable_node->hlist);\r\nstable_node->kpfn = page_to_pfn(kpage);\r\nset_page_stable_node(kpage, stable_node);\r\nreturn stable_node;\r\n}\r\nstatic\r\nstruct rmap_item *unstable_tree_search_insert(struct rmap_item *rmap_item,\r\nstruct page *page,\r\nstruct page **tree_pagep)\r\n{\r\nstruct rb_node **new = &root_unstable_tree.rb_node;\r\nstruct rb_node *parent = NULL;\r\nwhile (*new) {\r\nstruct rmap_item *tree_rmap_item;\r\nstruct page *tree_page;\r\nint ret;\r\ncond_resched();\r\ntree_rmap_item = rb_entry(*new, struct rmap_item, node);\r\ntree_page = get_mergeable_page(tree_rmap_item);\r\nif (IS_ERR_OR_NULL(tree_page))\r\nreturn NULL;\r\nif (page == tree_page) {\r\nput_page(tree_page);\r\nreturn NULL;\r\n}\r\nret = memcmp_pages(page, tree_page);\r\nparent = *new;\r\nif (ret < 0) {\r\nput_page(tree_page);\r\nnew = &parent->rb_left;\r\n} else if (ret > 0) {\r\nput_page(tree_page);\r\nnew = &parent->rb_right;\r\n} else {\r\n*tree_pagep = tree_page;\r\nreturn tree_rmap_item;\r\n}\r\n}\r\nrmap_item->address |= UNSTABLE_FLAG;\r\nrmap_item->address |= (ksm_scan.seqnr & SEQNR_MASK);\r\nrb_link_node(&rmap_item->node, parent, new);\r\nrb_insert_color(&rmap_item->node, &root_unstable_tree);\r\nksm_pages_unshared++;\r\nreturn NULL;\r\n}\r\nstatic void stable_tree_append(struct rmap_item *rmap_item,\r\nstruct stable_node *stable_node)\r\n{\r\nrmap_item->head = stable_node;\r\nrmap_item->address |= STABLE_FLAG;\r\nhlist_add_head(&rmap_item->hlist, &stable_node->hlist);\r\nif (rmap_item->hlist.next)\r\nksm_pages_sharing++;\r\nelse\r\nksm_pages_shared++;\r\n}\r\nstatic void cmp_and_merge_page(struct page *page, struct rmap_item *rmap_item)\r\n{\r\nstruct rmap_item *tree_rmap_item;\r\nstruct page *tree_page = NULL;\r\nstruct stable_node *stable_node;\r\nstruct page *kpage;\r\nunsigned int checksum;\r\nint err;\r\nremove_rmap_item_from_tree(rmap_item);\r\nkpage = stable_tree_search(page);\r\nif (kpage) {\r\nerr = try_to_merge_with_ksm_page(rmap_item, page, kpage);\r\nif (!err) {\r\nlock_page(kpage);\r\nstable_tree_append(rmap_item, page_stable_node(kpage));\r\nunlock_page(kpage);\r\n}\r\nput_page(kpage);\r\nreturn;\r\n}\r\nchecksum = calc_checksum(page);\r\nif (rmap_item->oldchecksum != checksum) {\r\nrmap_item->oldchecksum = checksum;\r\nreturn;\r\n}\r\ntree_rmap_item =\r\nunstable_tree_search_insert(rmap_item, page, &tree_page);\r\nif (tree_rmap_item) {\r\nkpage = try_to_merge_two_pages(rmap_item, page,\r\ntree_rmap_item, tree_page);\r\nput_page(tree_page);\r\nif (kpage) {\r\nremove_rmap_item_from_tree(tree_rmap_item);\r\nlock_page(kpage);\r\nstable_node = stable_tree_insert(kpage);\r\nif (stable_node) {\r\nstable_tree_append(tree_rmap_item, stable_node);\r\nstable_tree_append(rmap_item, stable_node);\r\n}\r\nunlock_page(kpage);\r\nif (!stable_node) {\r\nbreak_cow(tree_rmap_item);\r\nbreak_cow(rmap_item);\r\n}\r\n}\r\n}\r\n}\r\nstatic struct rmap_item *get_next_rmap_item(struct mm_slot *mm_slot,\r\nstruct rmap_item **rmap_list,\r\nunsigned long addr)\r\n{\r\nstruct rmap_item *rmap_item;\r\nwhile (*rmap_list) {\r\nrmap_item = *rmap_list;\r\nif ((rmap_item->address & PAGE_MASK) == addr)\r\nreturn rmap_item;\r\nif (rmap_item->address > addr)\r\nbreak;\r\n*rmap_list = rmap_item->rmap_list;\r\nremove_rmap_item_from_tree(rmap_item);\r\nfree_rmap_item(rmap_item);\r\n}\r\nrmap_item = alloc_rmap_item();\r\nif (rmap_item) {\r\nrmap_item->mm = mm_slot->mm;\r\nrmap_item->address = addr;\r\nrmap_item->rmap_list = *rmap_list;\r\n*rmap_list = rmap_item;\r\n}\r\nreturn rmap_item;\r\n}\r\nstatic struct rmap_item *scan_get_next_rmap_item(struct page **page)\r\n{\r\nstruct mm_struct *mm;\r\nstruct mm_slot *slot;\r\nstruct vm_area_struct *vma;\r\nstruct rmap_item *rmap_item;\r\nif (list_empty(&ksm_mm_head.mm_list))\r\nreturn NULL;\r\nslot = ksm_scan.mm_slot;\r\nif (slot == &ksm_mm_head) {\r\nlru_add_drain_all();\r\nroot_unstable_tree = RB_ROOT;\r\nspin_lock(&ksm_mmlist_lock);\r\nslot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);\r\nksm_scan.mm_slot = slot;\r\nspin_unlock(&ksm_mmlist_lock);\r\nif (slot == &ksm_mm_head)\r\nreturn NULL;\r\nnext_mm:\r\nksm_scan.address = 0;\r\nksm_scan.rmap_list = &slot->rmap_list;\r\n}\r\nmm = slot->mm;\r\ndown_read(&mm->mmap_sem);\r\nif (ksm_test_exit(mm))\r\nvma = NULL;\r\nelse\r\nvma = find_vma(mm, ksm_scan.address);\r\nfor (; vma; vma = vma->vm_next) {\r\nif (!(vma->vm_flags & VM_MERGEABLE))\r\ncontinue;\r\nif (ksm_scan.address < vma->vm_start)\r\nksm_scan.address = vma->vm_start;\r\nif (!vma->anon_vma)\r\nksm_scan.address = vma->vm_end;\r\nwhile (ksm_scan.address < vma->vm_end) {\r\nif (ksm_test_exit(mm))\r\nbreak;\r\n*page = follow_page(vma, ksm_scan.address, FOLL_GET);\r\nif (IS_ERR_OR_NULL(*page)) {\r\nksm_scan.address += PAGE_SIZE;\r\ncond_resched();\r\ncontinue;\r\n}\r\nif (PageAnon(*page) ||\r\npage_trans_compound_anon(*page)) {\r\nflush_anon_page(vma, *page, ksm_scan.address);\r\nflush_dcache_page(*page);\r\nrmap_item = get_next_rmap_item(slot,\r\nksm_scan.rmap_list, ksm_scan.address);\r\nif (rmap_item) {\r\nksm_scan.rmap_list =\r\n&rmap_item->rmap_list;\r\nksm_scan.address += PAGE_SIZE;\r\n} else\r\nput_page(*page);\r\nup_read(&mm->mmap_sem);\r\nreturn rmap_item;\r\n}\r\nput_page(*page);\r\nksm_scan.address += PAGE_SIZE;\r\ncond_resched();\r\n}\r\n}\r\nif (ksm_test_exit(mm)) {\r\nksm_scan.address = 0;\r\nksm_scan.rmap_list = &slot->rmap_list;\r\n}\r\nremove_trailing_rmap_items(slot, ksm_scan.rmap_list);\r\nspin_lock(&ksm_mmlist_lock);\r\nksm_scan.mm_slot = list_entry(slot->mm_list.next,\r\nstruct mm_slot, mm_list);\r\nif (ksm_scan.address == 0) {\r\nhlist_del(&slot->link);\r\nlist_del(&slot->mm_list);\r\nspin_unlock(&ksm_mmlist_lock);\r\nfree_mm_slot(slot);\r\nclear_bit(MMF_VM_MERGEABLE, &mm->flags);\r\nup_read(&mm->mmap_sem);\r\nmmdrop(mm);\r\n} else {\r\nspin_unlock(&ksm_mmlist_lock);\r\nup_read(&mm->mmap_sem);\r\n}\r\nslot = ksm_scan.mm_slot;\r\nif (slot != &ksm_mm_head)\r\ngoto next_mm;\r\nksm_scan.seqnr++;\r\nreturn NULL;\r\n}\r\nstatic void ksm_do_scan(unsigned int scan_npages)\r\n{\r\nstruct rmap_item *rmap_item;\r\nstruct page *uninitialized_var(page);\r\nwhile (scan_npages-- && likely(!freezing(current))) {\r\ncond_resched();\r\nrmap_item = scan_get_next_rmap_item(&page);\r\nif (!rmap_item)\r\nreturn;\r\nif (!PageKsm(page) || !in_stable_tree(rmap_item))\r\ncmp_and_merge_page(page, rmap_item);\r\nput_page(page);\r\n}\r\n}\r\nstatic int ksmd_should_run(void)\r\n{\r\nreturn (ksm_run & KSM_RUN_MERGE) && !list_empty(&ksm_mm_head.mm_list);\r\n}\r\nstatic int ksm_scan_thread(void *nothing)\r\n{\r\nset_freezable();\r\nset_user_nice(current, 5);\r\nwhile (!kthread_should_stop()) {\r\nmutex_lock(&ksm_thread_mutex);\r\nif (ksmd_should_run())\r\nksm_do_scan(ksm_thread_pages_to_scan);\r\nmutex_unlock(&ksm_thread_mutex);\r\ntry_to_freeze();\r\nif (ksmd_should_run()) {\r\nschedule_timeout_interruptible(\r\nmsecs_to_jiffies(ksm_thread_sleep_millisecs));\r\n} else {\r\nwait_event_freezable(ksm_thread_wait,\r\nksmd_should_run() || kthread_should_stop());\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint ksm_madvise(struct vm_area_struct *vma, unsigned long start,\r\nunsigned long end, int advice, unsigned long *vm_flags)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nint err;\r\nswitch (advice) {\r\ncase MADV_MERGEABLE:\r\nif (*vm_flags & (VM_MERGEABLE | VM_SHARED | VM_MAYSHARE |\r\nVM_PFNMAP | VM_IO | VM_DONTEXPAND |\r\nVM_RESERVED | VM_HUGETLB | VM_INSERTPAGE |\r\nVM_NONLINEAR | VM_MIXEDMAP | VM_SAO))\r\nreturn 0;\r\nif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {\r\nerr = __ksm_enter(mm);\r\nif (err)\r\nreturn err;\r\n}\r\n*vm_flags |= VM_MERGEABLE;\r\nbreak;\r\ncase MADV_UNMERGEABLE:\r\nif (!(*vm_flags & VM_MERGEABLE))\r\nreturn 0;\r\nif (vma->anon_vma) {\r\nerr = unmerge_ksm_pages(vma, start, end);\r\nif (err)\r\nreturn err;\r\n}\r\n*vm_flags &= ~VM_MERGEABLE;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nint __ksm_enter(struct mm_struct *mm)\r\n{\r\nstruct mm_slot *mm_slot;\r\nint needs_wakeup;\r\nmm_slot = alloc_mm_slot();\r\nif (!mm_slot)\r\nreturn -ENOMEM;\r\nneeds_wakeup = list_empty(&ksm_mm_head.mm_list);\r\nspin_lock(&ksm_mmlist_lock);\r\ninsert_to_mm_slots_hash(mm, mm_slot);\r\nlist_add_tail(&mm_slot->mm_list, &ksm_scan.mm_slot->mm_list);\r\nspin_unlock(&ksm_mmlist_lock);\r\nset_bit(MMF_VM_MERGEABLE, &mm->flags);\r\natomic_inc(&mm->mm_count);\r\nif (needs_wakeup)\r\nwake_up_interruptible(&ksm_thread_wait);\r\nreturn 0;\r\n}\r\nvoid __ksm_exit(struct mm_struct *mm)\r\n{\r\nstruct mm_slot *mm_slot;\r\nint easy_to_free = 0;\r\nspin_lock(&ksm_mmlist_lock);\r\nmm_slot = get_mm_slot(mm);\r\nif (mm_slot && ksm_scan.mm_slot != mm_slot) {\r\nif (!mm_slot->rmap_list) {\r\nhlist_del(&mm_slot->link);\r\nlist_del(&mm_slot->mm_list);\r\neasy_to_free = 1;\r\n} else {\r\nlist_move(&mm_slot->mm_list,\r\n&ksm_scan.mm_slot->mm_list);\r\n}\r\n}\r\nspin_unlock(&ksm_mmlist_lock);\r\nif (easy_to_free) {\r\nfree_mm_slot(mm_slot);\r\nclear_bit(MMF_VM_MERGEABLE, &mm->flags);\r\nmmdrop(mm);\r\n} else if (mm_slot) {\r\ndown_write(&mm->mmap_sem);\r\nup_write(&mm->mmap_sem);\r\n}\r\n}\r\nstruct page *ksm_does_need_to_copy(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\nstruct page *new_page;\r\nnew_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);\r\nif (new_page) {\r\ncopy_user_highpage(new_page, page, address, vma);\r\nSetPageDirty(new_page);\r\n__SetPageUptodate(new_page);\r\nSetPageSwapBacked(new_page);\r\n__set_page_locked(new_page);\r\nif (page_evictable(new_page, vma))\r\nlru_cache_add_lru(new_page, LRU_ACTIVE_ANON);\r\nelse\r\nadd_page_to_unevictable_list(new_page);\r\n}\r\nreturn new_page;\r\n}\r\nint page_referenced_ksm(struct page *page, struct mem_cgroup *memcg,\r\nunsigned long *vm_flags)\r\n{\r\nstruct stable_node *stable_node;\r\nstruct rmap_item *rmap_item;\r\nstruct hlist_node *hlist;\r\nunsigned int mapcount = page_mapcount(page);\r\nint referenced = 0;\r\nint search_new_forks = 0;\r\nVM_BUG_ON(!PageKsm(page));\r\nVM_BUG_ON(!PageLocked(page));\r\nstable_node = page_stable_node(page);\r\nif (!stable_node)\r\nreturn 0;\r\nagain:\r\nhlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\r\nstruct anon_vma *anon_vma = rmap_item->anon_vma;\r\nstruct anon_vma_chain *vmac;\r\nstruct vm_area_struct *vma;\r\nanon_vma_lock(anon_vma);\r\nlist_for_each_entry(vmac, &anon_vma->head, same_anon_vma) {\r\nvma = vmac->vma;\r\nif (rmap_item->address < vma->vm_start ||\r\nrmap_item->address >= vma->vm_end)\r\ncontinue;\r\nif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\r\ncontinue;\r\nif (memcg && !mm_match_cgroup(vma->vm_mm, memcg))\r\ncontinue;\r\nreferenced += page_referenced_one(page, vma,\r\nrmap_item->address, &mapcount, vm_flags);\r\nif (!search_new_forks || !mapcount)\r\nbreak;\r\n}\r\nanon_vma_unlock(anon_vma);\r\nif (!mapcount)\r\ngoto out;\r\n}\r\nif (!search_new_forks++)\r\ngoto again;\r\nout:\r\nreturn referenced;\r\n}\r\nint try_to_unmap_ksm(struct page *page, enum ttu_flags flags)\r\n{\r\nstruct stable_node *stable_node;\r\nstruct hlist_node *hlist;\r\nstruct rmap_item *rmap_item;\r\nint ret = SWAP_AGAIN;\r\nint search_new_forks = 0;\r\nVM_BUG_ON(!PageKsm(page));\r\nVM_BUG_ON(!PageLocked(page));\r\nstable_node = page_stable_node(page);\r\nif (!stable_node)\r\nreturn SWAP_FAIL;\r\nagain:\r\nhlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\r\nstruct anon_vma *anon_vma = rmap_item->anon_vma;\r\nstruct anon_vma_chain *vmac;\r\nstruct vm_area_struct *vma;\r\nanon_vma_lock(anon_vma);\r\nlist_for_each_entry(vmac, &anon_vma->head, same_anon_vma) {\r\nvma = vmac->vma;\r\nif (rmap_item->address < vma->vm_start ||\r\nrmap_item->address >= vma->vm_end)\r\ncontinue;\r\nif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\r\ncontinue;\r\nret = try_to_unmap_one(page, vma,\r\nrmap_item->address, flags);\r\nif (ret != SWAP_AGAIN || !page_mapped(page)) {\r\nanon_vma_unlock(anon_vma);\r\ngoto out;\r\n}\r\n}\r\nanon_vma_unlock(anon_vma);\r\n}\r\nif (!search_new_forks++)\r\ngoto again;\r\nout:\r\nreturn ret;\r\n}\r\nint rmap_walk_ksm(struct page *page, int (*rmap_one)(struct page *,\r\nstruct vm_area_struct *, unsigned long, void *), void *arg)\r\n{\r\nstruct stable_node *stable_node;\r\nstruct hlist_node *hlist;\r\nstruct rmap_item *rmap_item;\r\nint ret = SWAP_AGAIN;\r\nint search_new_forks = 0;\r\nVM_BUG_ON(!PageKsm(page));\r\nVM_BUG_ON(!PageLocked(page));\r\nstable_node = page_stable_node(page);\r\nif (!stable_node)\r\nreturn ret;\r\nagain:\r\nhlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\r\nstruct anon_vma *anon_vma = rmap_item->anon_vma;\r\nstruct anon_vma_chain *vmac;\r\nstruct vm_area_struct *vma;\r\nanon_vma_lock(anon_vma);\r\nlist_for_each_entry(vmac, &anon_vma->head, same_anon_vma) {\r\nvma = vmac->vma;\r\nif (rmap_item->address < vma->vm_start ||\r\nrmap_item->address >= vma->vm_end)\r\ncontinue;\r\nif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\r\ncontinue;\r\nret = rmap_one(page, vma, rmap_item->address, arg);\r\nif (ret != SWAP_AGAIN) {\r\nanon_vma_unlock(anon_vma);\r\ngoto out;\r\n}\r\n}\r\nanon_vma_unlock(anon_vma);\r\n}\r\nif (!search_new_forks++)\r\ngoto again;\r\nout:\r\nreturn ret;\r\n}\r\nvoid ksm_migrate_page(struct page *newpage, struct page *oldpage)\r\n{\r\nstruct stable_node *stable_node;\r\nVM_BUG_ON(!PageLocked(oldpage));\r\nVM_BUG_ON(!PageLocked(newpage));\r\nVM_BUG_ON(newpage->mapping != oldpage->mapping);\r\nstable_node = page_stable_node(newpage);\r\nif (stable_node) {\r\nVM_BUG_ON(stable_node->kpfn != page_to_pfn(oldpage));\r\nstable_node->kpfn = page_to_pfn(newpage);\r\n}\r\n}\r\nstatic struct stable_node *ksm_check_stable_tree(unsigned long start_pfn,\r\nunsigned long end_pfn)\r\n{\r\nstruct rb_node *node;\r\nfor (node = rb_first(&root_stable_tree); node; node = rb_next(node)) {\r\nstruct stable_node *stable_node;\r\nstable_node = rb_entry(node, struct stable_node, node);\r\nif (stable_node->kpfn >= start_pfn &&\r\nstable_node->kpfn < end_pfn)\r\nreturn stable_node;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int ksm_memory_callback(struct notifier_block *self,\r\nunsigned long action, void *arg)\r\n{\r\nstruct memory_notify *mn = arg;\r\nstruct stable_node *stable_node;\r\nswitch (action) {\r\ncase MEM_GOING_OFFLINE:\r\nmutex_lock_nested(&ksm_thread_mutex, SINGLE_DEPTH_NESTING);\r\nbreak;\r\ncase MEM_OFFLINE:\r\nwhile ((stable_node = ksm_check_stable_tree(mn->start_pfn,\r\nmn->start_pfn + mn->nr_pages)) != NULL)\r\nremove_node_from_stable_tree(stable_node);\r\ncase MEM_CANCEL_OFFLINE:\r\nmutex_unlock(&ksm_thread_mutex);\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic ssize_t sleep_millisecs_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", ksm_thread_sleep_millisecs);\r\n}\r\nstatic ssize_t sleep_millisecs_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned long msecs;\r\nint err;\r\nerr = strict_strtoul(buf, 10, &msecs);\r\nif (err || msecs > UINT_MAX)\r\nreturn -EINVAL;\r\nksm_thread_sleep_millisecs = msecs;\r\nreturn count;\r\n}\r\nstatic ssize_t pages_to_scan_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", ksm_thread_pages_to_scan);\r\n}\r\nstatic ssize_t pages_to_scan_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nint err;\r\nunsigned long nr_pages;\r\nerr = strict_strtoul(buf, 10, &nr_pages);\r\nif (err || nr_pages > UINT_MAX)\r\nreturn -EINVAL;\r\nksm_thread_pages_to_scan = nr_pages;\r\nreturn count;\r\n}\r\nstatic ssize_t run_show(struct kobject *kobj, struct kobj_attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", ksm_run);\r\n}\r\nstatic ssize_t run_store(struct kobject *kobj, struct kobj_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nint err;\r\nunsigned long flags;\r\nerr = strict_strtoul(buf, 10, &flags);\r\nif (err || flags > UINT_MAX)\r\nreturn -EINVAL;\r\nif (flags > KSM_RUN_UNMERGE)\r\nreturn -EINVAL;\r\nmutex_lock(&ksm_thread_mutex);\r\nif (ksm_run != flags) {\r\nksm_run = flags;\r\nif (flags & KSM_RUN_UNMERGE) {\r\nint oom_score_adj;\r\noom_score_adj = test_set_oom_score_adj(OOM_SCORE_ADJ_MAX);\r\nerr = unmerge_and_remove_all_rmap_items();\r\ncompare_swap_oom_score_adj(OOM_SCORE_ADJ_MAX,\r\noom_score_adj);\r\nif (err) {\r\nksm_run = KSM_RUN_STOP;\r\ncount = err;\r\n}\r\n}\r\n}\r\nmutex_unlock(&ksm_thread_mutex);\r\nif (flags & KSM_RUN_MERGE)\r\nwake_up_interruptible(&ksm_thread_wait);\r\nreturn count;\r\n}\r\nstatic ssize_t pages_shared_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nreturn sprintf(buf, "%lu\n", ksm_pages_shared);\r\n}\r\nstatic ssize_t pages_sharing_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nreturn sprintf(buf, "%lu\n", ksm_pages_sharing);\r\n}\r\nstatic ssize_t pages_unshared_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nreturn sprintf(buf, "%lu\n", ksm_pages_unshared);\r\n}\r\nstatic ssize_t pages_volatile_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nlong ksm_pages_volatile;\r\nksm_pages_volatile = ksm_rmap_items - ksm_pages_shared\r\n- ksm_pages_sharing - ksm_pages_unshared;\r\nif (ksm_pages_volatile < 0)\r\nksm_pages_volatile = 0;\r\nreturn sprintf(buf, "%ld\n", ksm_pages_volatile);\r\n}\r\nstatic ssize_t full_scans_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nreturn sprintf(buf, "%lu\n", ksm_scan.seqnr);\r\n}\r\nstatic int __init ksm_init(void)\r\n{\r\nstruct task_struct *ksm_thread;\r\nint err;\r\nerr = ksm_slab_init();\r\nif (err)\r\ngoto out;\r\nksm_thread = kthread_run(ksm_scan_thread, NULL, "ksmd");\r\nif (IS_ERR(ksm_thread)) {\r\nprintk(KERN_ERR "ksm: creating kthread failed\n");\r\nerr = PTR_ERR(ksm_thread);\r\ngoto out_free;\r\n}\r\n#ifdef CONFIG_SYSFS\r\nerr = sysfs_create_group(mm_kobj, &ksm_attr_group);\r\nif (err) {\r\nprintk(KERN_ERR "ksm: register sysfs failed\n");\r\nkthread_stop(ksm_thread);\r\ngoto out_free;\r\n}\r\n#else\r\nksm_run = KSM_RUN_MERGE;\r\n#endif\r\n#ifdef CONFIG_MEMORY_HOTREMOVE\r\nhotplug_memory_notifier(ksm_memory_callback, 100);\r\n#endif\r\nreturn 0;\r\nout_free:\r\nksm_slab_free();\r\nout:\r\nreturn err;\r\n}
