static pte_t *lock_pte_protection(struct vm_area_struct *vma, pmd_t *pmd,\r\nunsigned long addr, int prot_numa, spinlock_t **ptl)\r\n{\r\npte_t *pte;\r\nspinlock_t *pmdl;\r\nif (!prot_numa)\r\nreturn pte_offset_map_lock(vma->vm_mm, pmd, addr, ptl);\r\npmdl = pmd_lock(vma->vm_mm, pmd);\r\nif (unlikely(pmd_trans_huge(*pmd) || pmd_none(*pmd))) {\r\nspin_unlock(pmdl);\r\nreturn NULL;\r\n}\r\npte = pte_offset_map_lock(vma->vm_mm, pmd, addr, ptl);\r\nspin_unlock(pmdl);\r\nreturn pte;\r\n}\r\nstatic unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\r\nunsigned long addr, unsigned long end, pgprot_t newprot,\r\nint dirty_accountable, int prot_numa)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npte_t *pte, oldpte;\r\nspinlock_t *ptl;\r\nunsigned long pages = 0;\r\npte = lock_pte_protection(vma, pmd, addr, prot_numa, &ptl);\r\nif (!pte)\r\nreturn 0;\r\narch_enter_lazy_mmu_mode();\r\ndo {\r\noldpte = *pte;\r\nif (pte_present(oldpte)) {\r\npte_t ptent;\r\nbool updated = false;\r\nif (!prot_numa) {\r\nptent = ptep_modify_prot_start(mm, addr, pte);\r\nif (pte_numa(ptent))\r\nptent = pte_mknonnuma(ptent);\r\nptent = pte_modify(ptent, newprot);\r\nif (dirty_accountable && pte_dirty(ptent) &&\r\n(pte_soft_dirty(ptent) ||\r\n!(vma->vm_flags & VM_SOFTDIRTY)))\r\nptent = pte_mkwrite(ptent);\r\nptep_modify_prot_commit(mm, addr, pte, ptent);\r\nupdated = true;\r\n} else {\r\nstruct page *page;\r\npage = vm_normal_page(vma, addr, oldpte);\r\nif (page && !PageKsm(page)) {\r\nif (!pte_numa(oldpte)) {\r\nptep_set_numa(mm, addr, pte);\r\nupdated = true;\r\n}\r\n}\r\n}\r\nif (updated)\r\npages++;\r\n} else if (IS_ENABLED(CONFIG_MIGRATION) && !pte_file(oldpte)) {\r\nswp_entry_t entry = pte_to_swp_entry(oldpte);\r\nif (is_write_migration_entry(entry)) {\r\npte_t newpte;\r\nmake_migration_entry_read(&entry);\r\nnewpte = swp_entry_to_pte(entry);\r\nif (pte_swp_soft_dirty(oldpte))\r\nnewpte = pte_swp_mksoft_dirty(newpte);\r\nset_pte_at(mm, addr, pte, newpte);\r\npages++;\r\n}\r\n}\r\n} while (pte++, addr += PAGE_SIZE, addr != end);\r\narch_leave_lazy_mmu_mode();\r\npte_unmap_unlock(pte - 1, ptl);\r\nreturn pages;\r\n}\r\nstatic inline unsigned long change_pmd_range(struct vm_area_struct *vma,\r\npud_t *pud, unsigned long addr, unsigned long end,\r\npgprot_t newprot, int dirty_accountable, int prot_numa)\r\n{\r\npmd_t *pmd;\r\nstruct mm_struct *mm = vma->vm_mm;\r\nunsigned long next;\r\nunsigned long pages = 0;\r\nunsigned long nr_huge_updates = 0;\r\nunsigned long mni_start = 0;\r\npmd = pmd_offset(pud, addr);\r\ndo {\r\nunsigned long this_pages;\r\nnext = pmd_addr_end(addr, end);\r\nif (!pmd_trans_huge(*pmd) && pmd_none_or_clear_bad(pmd))\r\ncontinue;\r\nif (!mni_start) {\r\nmni_start = addr;\r\nmmu_notifier_invalidate_range_start(mm, mni_start, end);\r\n}\r\nif (pmd_trans_huge(*pmd)) {\r\nif (next - addr != HPAGE_PMD_SIZE)\r\nsplit_huge_page_pmd(vma, addr, pmd);\r\nelse {\r\nint nr_ptes = change_huge_pmd(vma, pmd, addr,\r\nnewprot, prot_numa);\r\nif (nr_ptes) {\r\nif (nr_ptes == HPAGE_PMD_NR) {\r\npages += HPAGE_PMD_NR;\r\nnr_huge_updates++;\r\n}\r\ncontinue;\r\n}\r\n}\r\n}\r\nthis_pages = change_pte_range(vma, pmd, addr, next, newprot,\r\ndirty_accountable, prot_numa);\r\npages += this_pages;\r\n} while (pmd++, addr = next, addr != end);\r\nif (mni_start)\r\nmmu_notifier_invalidate_range_end(mm, mni_start, end);\r\nif (nr_huge_updates)\r\ncount_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);\r\nreturn pages;\r\n}\r\nstatic inline unsigned long change_pud_range(struct vm_area_struct *vma,\r\npgd_t *pgd, unsigned long addr, unsigned long end,\r\npgprot_t newprot, int dirty_accountable, int prot_numa)\r\n{\r\npud_t *pud;\r\nunsigned long next;\r\nunsigned long pages = 0;\r\npud = pud_offset(pgd, addr);\r\ndo {\r\nnext = pud_addr_end(addr, end);\r\nif (pud_none_or_clear_bad(pud))\r\ncontinue;\r\npages += change_pmd_range(vma, pud, addr, next, newprot,\r\ndirty_accountable, prot_numa);\r\n} while (pud++, addr = next, addr != end);\r\nreturn pages;\r\n}\r\nstatic unsigned long change_protection_range(struct vm_area_struct *vma,\r\nunsigned long addr, unsigned long end, pgprot_t newprot,\r\nint dirty_accountable, int prot_numa)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npgd_t *pgd;\r\nunsigned long next;\r\nunsigned long start = addr;\r\nunsigned long pages = 0;\r\nBUG_ON(addr >= end);\r\npgd = pgd_offset(mm, addr);\r\nflush_cache_range(vma, addr, end);\r\nset_tlb_flush_pending(mm);\r\ndo {\r\nnext = pgd_addr_end(addr, end);\r\nif (pgd_none_or_clear_bad(pgd))\r\ncontinue;\r\npages += change_pud_range(vma, pgd, addr, next, newprot,\r\ndirty_accountable, prot_numa);\r\n} while (pgd++, addr = next, addr != end);\r\nif (pages)\r\nflush_tlb_range(vma, start, end);\r\nclear_tlb_flush_pending(mm);\r\nreturn pages;\r\n}\r\nunsigned long change_protection(struct vm_area_struct *vma, unsigned long start,\r\nunsigned long end, pgprot_t newprot,\r\nint dirty_accountable, int prot_numa)\r\n{\r\nunsigned long pages;\r\nif (is_vm_hugetlb_page(vma))\r\npages = hugetlb_change_protection(vma, start, end, newprot);\r\nelse\r\npages = change_protection_range(vma, start, end, newprot, dirty_accountable, prot_numa);\r\nreturn pages;\r\n}\r\nint\r\nmprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,\r\nunsigned long start, unsigned long end, unsigned long newflags)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nunsigned long oldflags = vma->vm_flags;\r\nlong nrpages = (end - start) >> PAGE_SHIFT;\r\nunsigned long charged = 0;\r\npgoff_t pgoff;\r\nint error;\r\nint dirty_accountable = 0;\r\nif (newflags == oldflags) {\r\n*pprev = vma;\r\nreturn 0;\r\n}\r\nif (newflags & VM_WRITE) {\r\nif (!(oldflags & (VM_ACCOUNT|VM_WRITE|VM_HUGETLB|\r\nVM_SHARED|VM_NORESERVE))) {\r\ncharged = nrpages;\r\nif (security_vm_enough_memory_mm(mm, charged))\r\nreturn -ENOMEM;\r\nnewflags |= VM_ACCOUNT;\r\n}\r\n}\r\npgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\r\n*pprev = vma_merge(mm, *pprev, start, end, newflags,\r\nvma->anon_vma, vma->vm_file, pgoff, vma_policy(vma));\r\nif (*pprev) {\r\nvma = *pprev;\r\ngoto success;\r\n}\r\n*pprev = vma;\r\nif (start != vma->vm_start) {\r\nerror = split_vma(mm, vma, start, 1);\r\nif (error)\r\ngoto fail;\r\n}\r\nif (end != vma->vm_end) {\r\nerror = split_vma(mm, vma, end, 0);\r\nif (error)\r\ngoto fail;\r\n}\r\nsuccess:\r\nvma->vm_flags = newflags;\r\ndirty_accountable = vma_wants_writenotify(vma);\r\nvma_set_page_prot(vma);\r\nchange_protection(vma, start, end, vma->vm_page_prot,\r\ndirty_accountable, 0);\r\nvm_stat_account(mm, oldflags, vma->vm_file, -nrpages);\r\nvm_stat_account(mm, newflags, vma->vm_file, nrpages);\r\nperf_event_mmap(vma);\r\nreturn 0;\r\nfail:\r\nvm_unacct_memory(charged);\r\nreturn error;\r\n}
