static struct cxio_rdev *cxio_hal_find_rdev_by_name(char *dev_name)\r\n{\r\nstruct cxio_rdev *rdev;\r\nlist_for_each_entry(rdev, &rdev_list, entry)\r\nif (!strcmp(rdev->dev_name, dev_name))\r\nreturn rdev;\r\nreturn NULL;\r\n}\r\nstatic struct cxio_rdev *cxio_hal_find_rdev_by_t3cdev(struct t3cdev *tdev)\r\n{\r\nstruct cxio_rdev *rdev;\r\nlist_for_each_entry(rdev, &rdev_list, entry)\r\nif (rdev->t3cdev_p == tdev)\r\nreturn rdev;\r\nreturn NULL;\r\n}\r\nint cxio_hal_cq_op(struct cxio_rdev *rdev_p, struct t3_cq *cq,\r\nenum t3_cq_opcode op, u32 credit)\r\n{\r\nint ret;\r\nstruct t3_cqe *cqe;\r\nu32 rptr;\r\nstruct rdma_cq_op setup;\r\nsetup.id = cq->cqid;\r\nsetup.credits = (op == CQ_CREDIT_UPDATE) ? credit : 0;\r\nsetup.op = op;\r\nret = rdev_p->t3cdev_p->ctl(rdev_p->t3cdev_p, RDMA_CQ_OP, &setup);\r\nif ((ret < 0) || (op == CQ_CREDIT_UPDATE))\r\nreturn ret;\r\nif (Q_PTR2IDX((cq->rptr), cq->size_log2) != ret) {\r\nint i=0;\r\nrptr = cq->rptr;\r\nwhile (Q_PTR2IDX((rptr+1), cq->size_log2) != ret)\r\nrptr++;\r\ncqe = cq->queue + Q_PTR2IDX(rptr, cq->size_log2);\r\nwhile (!CQ_VLD_ENTRY(rptr, cq->size_log2, cqe)) {\r\nudelay(1);\r\nif (i++ > 1000000) {\r\nprintk(KERN_ERR "%s: stalled rnic\n",\r\nrdev_p->dev_name);\r\nreturn -EIO;\r\n}\r\n}\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int cxio_hal_clear_cq_ctx(struct cxio_rdev *rdev_p, u32 cqid)\r\n{\r\nstruct rdma_cq_setup setup;\r\nsetup.id = cqid;\r\nsetup.base_addr = 0;\r\nsetup.size = 0;\r\nsetup.credits = 0;\r\nsetup.credit_thres = 0;\r\nsetup.ovfl_mode = 0;\r\nreturn (rdev_p->t3cdev_p->ctl(rdev_p->t3cdev_p, RDMA_CQ_SETUP, &setup));\r\n}\r\nstatic int cxio_hal_clear_qp_ctx(struct cxio_rdev *rdev_p, u32 qpid)\r\n{\r\nu64 sge_cmd;\r\nstruct t3_modify_qp_wr *wqe;\r\nstruct sk_buff *skb = alloc_skb(sizeof(*wqe), GFP_KERNEL);\r\nif (!skb) {\r\nPDBG("%s alloc_skb failed\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nwqe = (struct t3_modify_qp_wr *) skb_put(skb, sizeof(*wqe));\r\nmemset(wqe, 0, sizeof(*wqe));\r\nbuild_fw_riwrh((struct fw_riwrh *) wqe, T3_WR_QP_MOD,\r\nT3_COMPLETION_FLAG | T3_NOTIFY_FLAG, 0, qpid, 7,\r\nT3_SOPEOP);\r\nwqe->flags = cpu_to_be32(MODQP_WRITE_EC);\r\nsge_cmd = qpid << 8 | 3;\r\nwqe->sge_cmd = cpu_to_be64(sge_cmd);\r\nskb->priority = CPL_PRIORITY_CONTROL;\r\nreturn iwch_cxgb3_ofld_send(rdev_p->t3cdev_p, skb);\r\n}\r\nint cxio_create_cq(struct cxio_rdev *rdev_p, struct t3_cq *cq, int kernel)\r\n{\r\nstruct rdma_cq_setup setup;\r\nint size = (1UL << (cq->size_log2)) * sizeof(struct t3_cqe);\r\nsize += 1;\r\ncq->cqid = cxio_hal_get_cqid(rdev_p->rscp);\r\nif (!cq->cqid)\r\nreturn -ENOMEM;\r\nif (kernel) {\r\ncq->sw_queue = kzalloc(size, GFP_KERNEL);\r\nif (!cq->sw_queue)\r\nreturn -ENOMEM;\r\n}\r\ncq->queue = dma_alloc_coherent(&(rdev_p->rnic_info.pdev->dev), size,\r\n&(cq->dma_addr), GFP_KERNEL);\r\nif (!cq->queue) {\r\nkfree(cq->sw_queue);\r\nreturn -ENOMEM;\r\n}\r\ndma_unmap_addr_set(cq, mapping, cq->dma_addr);\r\nmemset(cq->queue, 0, size);\r\nsetup.id = cq->cqid;\r\nsetup.base_addr = (u64) (cq->dma_addr);\r\nsetup.size = 1UL << cq->size_log2;\r\nsetup.credits = 65535;\r\nsetup.credit_thres = 1;\r\nif (rdev_p->t3cdev_p->type != T3A)\r\nsetup.ovfl_mode = 0;\r\nelse\r\nsetup.ovfl_mode = 1;\r\nreturn (rdev_p->t3cdev_p->ctl(rdev_p->t3cdev_p, RDMA_CQ_SETUP, &setup));\r\n}\r\nint cxio_resize_cq(struct cxio_rdev *rdev_p, struct t3_cq *cq)\r\n{\r\nstruct rdma_cq_setup setup;\r\nsetup.id = cq->cqid;\r\nsetup.base_addr = (u64) (cq->dma_addr);\r\nsetup.size = 1UL << cq->size_log2;\r\nsetup.credits = setup.size;\r\nsetup.credit_thres = setup.size;\r\nsetup.ovfl_mode = 1;\r\nreturn (rdev_p->t3cdev_p->ctl(rdev_p->t3cdev_p, RDMA_CQ_SETUP, &setup));\r\n}\r\nstatic u32 get_qpid(struct cxio_rdev *rdev_p, struct cxio_ucontext *uctx)\r\n{\r\nstruct cxio_qpid_list *entry;\r\nu32 qpid;\r\nint i;\r\nmutex_lock(&uctx->lock);\r\nif (!list_empty(&uctx->qpids)) {\r\nentry = list_entry(uctx->qpids.next, struct cxio_qpid_list,\r\nentry);\r\nlist_del(&entry->entry);\r\nqpid = entry->qpid;\r\nkfree(entry);\r\n} else {\r\nqpid = cxio_hal_get_qpid(rdev_p->rscp);\r\nif (!qpid)\r\ngoto out;\r\nfor (i = qpid+1; i & rdev_p->qpmask; i++) {\r\nentry = kmalloc(sizeof *entry, GFP_KERNEL);\r\nif (!entry)\r\nbreak;\r\nentry->qpid = i;\r\nlist_add_tail(&entry->entry, &uctx->qpids);\r\n}\r\n}\r\nout:\r\nmutex_unlock(&uctx->lock);\r\nPDBG("%s qpid 0x%x\n", __func__, qpid);\r\nreturn qpid;\r\n}\r\nstatic void put_qpid(struct cxio_rdev *rdev_p, u32 qpid,\r\nstruct cxio_ucontext *uctx)\r\n{\r\nstruct cxio_qpid_list *entry;\r\nentry = kmalloc(sizeof *entry, GFP_KERNEL);\r\nif (!entry)\r\nreturn;\r\nPDBG("%s qpid 0x%x\n", __func__, qpid);\r\nentry->qpid = qpid;\r\nmutex_lock(&uctx->lock);\r\nlist_add_tail(&entry->entry, &uctx->qpids);\r\nmutex_unlock(&uctx->lock);\r\n}\r\nvoid cxio_release_ucontext(struct cxio_rdev *rdev_p, struct cxio_ucontext *uctx)\r\n{\r\nstruct list_head *pos, *nxt;\r\nstruct cxio_qpid_list *entry;\r\nmutex_lock(&uctx->lock);\r\nlist_for_each_safe(pos, nxt, &uctx->qpids) {\r\nentry = list_entry(pos, struct cxio_qpid_list, entry);\r\nlist_del_init(&entry->entry);\r\nif (!(entry->qpid & rdev_p->qpmask))\r\ncxio_hal_put_qpid(rdev_p->rscp, entry->qpid);\r\nkfree(entry);\r\n}\r\nmutex_unlock(&uctx->lock);\r\n}\r\nvoid cxio_init_ucontext(struct cxio_rdev *rdev_p, struct cxio_ucontext *uctx)\r\n{\r\nINIT_LIST_HEAD(&uctx->qpids);\r\nmutex_init(&uctx->lock);\r\n}\r\nint cxio_create_qp(struct cxio_rdev *rdev_p, u32 kernel_domain,\r\nstruct t3_wq *wq, struct cxio_ucontext *uctx)\r\n{\r\nint depth = 1UL << wq->size_log2;\r\nint rqsize = 1UL << wq->rq_size_log2;\r\nwq->qpid = get_qpid(rdev_p, uctx);\r\nif (!wq->qpid)\r\nreturn -ENOMEM;\r\nwq->rq = kzalloc(depth * sizeof(struct t3_swrq), GFP_KERNEL);\r\nif (!wq->rq)\r\ngoto err1;\r\nwq->rq_addr = cxio_hal_rqtpool_alloc(rdev_p, rqsize);\r\nif (!wq->rq_addr)\r\ngoto err2;\r\nwq->sq = kzalloc(depth * sizeof(struct t3_swsq), GFP_KERNEL);\r\nif (!wq->sq)\r\ngoto err3;\r\nwq->queue = dma_alloc_coherent(&(rdev_p->rnic_info.pdev->dev),\r\ndepth * sizeof(union t3_wr),\r\n&(wq->dma_addr), GFP_KERNEL);\r\nif (!wq->queue)\r\ngoto err4;\r\nmemset(wq->queue, 0, depth * sizeof(union t3_wr));\r\ndma_unmap_addr_set(wq, mapping, wq->dma_addr);\r\nwq->doorbell = (void __iomem *)rdev_p->rnic_info.kdb_addr;\r\nif (!kernel_domain)\r\nwq->udb = (u64)rdev_p->rnic_info.udbell_physbase +\r\n(wq->qpid << rdev_p->qpshift);\r\nwq->rdev = rdev_p;\r\nPDBG("%s qpid 0x%x doorbell 0x%p udb 0x%llx\n", __func__,\r\nwq->qpid, wq->doorbell, (unsigned long long) wq->udb);\r\nreturn 0;\r\nerr4:\r\nkfree(wq->sq);\r\nerr3:\r\ncxio_hal_rqtpool_free(rdev_p, wq->rq_addr, rqsize);\r\nerr2:\r\nkfree(wq->rq);\r\nerr1:\r\nput_qpid(rdev_p, wq->qpid, uctx);\r\nreturn -ENOMEM;\r\n}\r\nint cxio_destroy_cq(struct cxio_rdev *rdev_p, struct t3_cq *cq)\r\n{\r\nint err;\r\nerr = cxio_hal_clear_cq_ctx(rdev_p, cq->cqid);\r\nkfree(cq->sw_queue);\r\ndma_free_coherent(&(rdev_p->rnic_info.pdev->dev),\r\n(1UL << (cq->size_log2))\r\n* sizeof(struct t3_cqe), cq->queue,\r\ndma_unmap_addr(cq, mapping));\r\ncxio_hal_put_cqid(rdev_p->rscp, cq->cqid);\r\nreturn err;\r\n}\r\nint cxio_destroy_qp(struct cxio_rdev *rdev_p, struct t3_wq *wq,\r\nstruct cxio_ucontext *uctx)\r\n{\r\ndma_free_coherent(&(rdev_p->rnic_info.pdev->dev),\r\n(1UL << (wq->size_log2))\r\n* sizeof(union t3_wr), wq->queue,\r\ndma_unmap_addr(wq, mapping));\r\nkfree(wq->sq);\r\ncxio_hal_rqtpool_free(rdev_p, wq->rq_addr, (1UL << wq->rq_size_log2));\r\nkfree(wq->rq);\r\nput_qpid(rdev_p, wq->qpid, uctx);\r\nreturn 0;\r\n}\r\nstatic void insert_recv_cqe(struct t3_wq *wq, struct t3_cq *cq)\r\n{\r\nstruct t3_cqe cqe;\r\nPDBG("%s wq %p cq %p sw_rptr 0x%x sw_wptr 0x%x\n", __func__,\r\nwq, cq, cq->sw_rptr, cq->sw_wptr);\r\nmemset(&cqe, 0, sizeof(cqe));\r\ncqe.header = cpu_to_be32(V_CQE_STATUS(TPT_ERR_SWFLUSH) |\r\nV_CQE_OPCODE(T3_SEND) |\r\nV_CQE_TYPE(0) |\r\nV_CQE_SWCQE(1) |\r\nV_CQE_QPID(wq->qpid) |\r\nV_CQE_GENBIT(Q_GENBIT(cq->sw_wptr,\r\ncq->size_log2)));\r\n*(cq->sw_queue + Q_PTR2IDX(cq->sw_wptr, cq->size_log2)) = cqe;\r\ncq->sw_wptr++;\r\n}\r\nint cxio_flush_rq(struct t3_wq *wq, struct t3_cq *cq, int count)\r\n{\r\nu32 ptr;\r\nint flushed = 0;\r\nPDBG("%s wq %p cq %p\n", __func__, wq, cq);\r\nPDBG("%s rq_rptr %u rq_wptr %u skip count %u\n", __func__,\r\nwq->rq_rptr, wq->rq_wptr, count);\r\nptr = wq->rq_rptr + count;\r\nwhile (ptr++ != wq->rq_wptr) {\r\ninsert_recv_cqe(wq, cq);\r\nflushed++;\r\n}\r\nreturn flushed;\r\n}\r\nstatic void insert_sq_cqe(struct t3_wq *wq, struct t3_cq *cq,\r\nstruct t3_swsq *sqp)\r\n{\r\nstruct t3_cqe cqe;\r\nPDBG("%s wq %p cq %p sw_rptr 0x%x sw_wptr 0x%x\n", __func__,\r\nwq, cq, cq->sw_rptr, cq->sw_wptr);\r\nmemset(&cqe, 0, sizeof(cqe));\r\ncqe.header = cpu_to_be32(V_CQE_STATUS(TPT_ERR_SWFLUSH) |\r\nV_CQE_OPCODE(sqp->opcode) |\r\nV_CQE_TYPE(1) |\r\nV_CQE_SWCQE(1) |\r\nV_CQE_QPID(wq->qpid) |\r\nV_CQE_GENBIT(Q_GENBIT(cq->sw_wptr,\r\ncq->size_log2)));\r\ncqe.u.scqe.wrid_hi = sqp->sq_wptr;\r\n*(cq->sw_queue + Q_PTR2IDX(cq->sw_wptr, cq->size_log2)) = cqe;\r\ncq->sw_wptr++;\r\n}\r\nint cxio_flush_sq(struct t3_wq *wq, struct t3_cq *cq, int count)\r\n{\r\n__u32 ptr;\r\nint flushed = 0;\r\nstruct t3_swsq *sqp = wq->sq + Q_PTR2IDX(wq->sq_rptr, wq->sq_size_log2);\r\nptr = wq->sq_rptr + count;\r\nsqp = wq->sq + Q_PTR2IDX(ptr, wq->sq_size_log2);\r\nwhile (ptr != wq->sq_wptr) {\r\nsqp->signaled = 0;\r\ninsert_sq_cqe(wq, cq, sqp);\r\nptr++;\r\nsqp = wq->sq + Q_PTR2IDX(ptr, wq->sq_size_log2);\r\nflushed++;\r\n}\r\nreturn flushed;\r\n}\r\nvoid cxio_flush_hw_cq(struct t3_cq *cq)\r\n{\r\nstruct t3_cqe *cqe, *swcqe;\r\nPDBG("%s cq %p cqid 0x%x\n", __func__, cq, cq->cqid);\r\ncqe = cxio_next_hw_cqe(cq);\r\nwhile (cqe) {\r\nPDBG("%s flushing hwcq rptr 0x%x to swcq wptr 0x%x\n",\r\n__func__, cq->rptr, cq->sw_wptr);\r\nswcqe = cq->sw_queue + Q_PTR2IDX(cq->sw_wptr, cq->size_log2);\r\n*swcqe = *cqe;\r\nswcqe->header |= cpu_to_be32(V_CQE_SWCQE(1));\r\ncq->sw_wptr++;\r\ncq->rptr++;\r\ncqe = cxio_next_hw_cqe(cq);\r\n}\r\n}\r\nstatic int cqe_completes_wr(struct t3_cqe *cqe, struct t3_wq *wq)\r\n{\r\nif (CQE_OPCODE(*cqe) == T3_TERMINATE)\r\nreturn 0;\r\nif ((CQE_OPCODE(*cqe) == T3_RDMA_WRITE) && RQ_TYPE(*cqe))\r\nreturn 0;\r\nif ((CQE_OPCODE(*cqe) == T3_READ_RESP) && SQ_TYPE(*cqe))\r\nreturn 0;\r\nif (CQE_SEND_OPCODE(*cqe) && RQ_TYPE(*cqe) &&\r\nQ_EMPTY(wq->rq_rptr, wq->rq_wptr))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nvoid cxio_count_scqes(struct t3_cq *cq, struct t3_wq *wq, int *count)\r\n{\r\nstruct t3_cqe *cqe;\r\nu32 ptr;\r\n*count = 0;\r\nptr = cq->sw_rptr;\r\nwhile (!Q_EMPTY(ptr, cq->sw_wptr)) {\r\ncqe = cq->sw_queue + (Q_PTR2IDX(ptr, cq->size_log2));\r\nif ((SQ_TYPE(*cqe) ||\r\n((CQE_OPCODE(*cqe) == T3_READ_RESP) && wq->oldest_read)) &&\r\n(CQE_QPID(*cqe) == wq->qpid))\r\n(*count)++;\r\nptr++;\r\n}\r\nPDBG("%s cq %p count %d\n", __func__, cq, *count);\r\n}\r\nvoid cxio_count_rcqes(struct t3_cq *cq, struct t3_wq *wq, int *count)\r\n{\r\nstruct t3_cqe *cqe;\r\nu32 ptr;\r\n*count = 0;\r\nPDBG("%s count zero %d\n", __func__, *count);\r\nptr = cq->sw_rptr;\r\nwhile (!Q_EMPTY(ptr, cq->sw_wptr)) {\r\ncqe = cq->sw_queue + (Q_PTR2IDX(ptr, cq->size_log2));\r\nif (RQ_TYPE(*cqe) && (CQE_OPCODE(*cqe) != T3_READ_RESP) &&\r\n(CQE_QPID(*cqe) == wq->qpid) && cqe_completes_wr(cqe, wq))\r\n(*count)++;\r\nptr++;\r\n}\r\nPDBG("%s cq %p count %d\n", __func__, cq, *count);\r\n}\r\nstatic int cxio_hal_init_ctrl_cq(struct cxio_rdev *rdev_p)\r\n{\r\nstruct rdma_cq_setup setup;\r\nsetup.id = 0;\r\nsetup.base_addr = 0;\r\nsetup.size = 1;\r\nsetup.credits = 0;\r\nsetup.credit_thres = 0;\r\nsetup.ovfl_mode = 1;\r\nreturn (rdev_p->t3cdev_p->ctl(rdev_p->t3cdev_p, RDMA_CQ_SETUP, &setup));\r\n}\r\nstatic int cxio_hal_init_ctrl_qp(struct cxio_rdev *rdev_p)\r\n{\r\nint err;\r\nu64 sge_cmd, ctx0, ctx1;\r\nu64 base_addr;\r\nstruct t3_modify_qp_wr *wqe;\r\nstruct sk_buff *skb;\r\nskb = alloc_skb(sizeof(*wqe), GFP_KERNEL);\r\nif (!skb) {\r\nPDBG("%s alloc_skb failed\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nerr = cxio_hal_init_ctrl_cq(rdev_p);\r\nif (err) {\r\nPDBG("%s err %d initializing ctrl_cq\n", __func__, err);\r\ngoto err;\r\n}\r\nrdev_p->ctrl_qp.workq = dma_alloc_coherent(\r\n&(rdev_p->rnic_info.pdev->dev),\r\n(1 << T3_CTRL_QP_SIZE_LOG2) *\r\nsizeof(union t3_wr),\r\n&(rdev_p->ctrl_qp.dma_addr),\r\nGFP_KERNEL);\r\nif (!rdev_p->ctrl_qp.workq) {\r\nPDBG("%s dma_alloc_coherent failed\n", __func__);\r\nerr = -ENOMEM;\r\ngoto err;\r\n}\r\ndma_unmap_addr_set(&rdev_p->ctrl_qp, mapping,\r\nrdev_p->ctrl_qp.dma_addr);\r\nrdev_p->ctrl_qp.doorbell = (void __iomem *)rdev_p->rnic_info.kdb_addr;\r\nmemset(rdev_p->ctrl_qp.workq, 0,\r\n(1 << T3_CTRL_QP_SIZE_LOG2) * sizeof(union t3_wr));\r\nmutex_init(&rdev_p->ctrl_qp.lock);\r\ninit_waitqueue_head(&rdev_p->ctrl_qp.waitq);\r\nbase_addr = rdev_p->ctrl_qp.dma_addr;\r\nbase_addr >>= 12;\r\nctx0 = (V_EC_SIZE((1 << T3_CTRL_QP_SIZE_LOG2)) |\r\nV_EC_BASE_LO((u32) base_addr & 0xffff));\r\nctx0 <<= 32;\r\nctx0 |= V_EC_CREDITS(FW_WR_NUM);\r\nbase_addr >>= 16;\r\nctx1 = (u32) base_addr;\r\nbase_addr >>= 32;\r\nctx1 |= ((u64) (V_EC_BASE_HI((u32) base_addr & 0xf) | V_EC_RESPQ(0) |\r\nV_EC_TYPE(0) | V_EC_GEN(1) |\r\nV_EC_UP_TOKEN(T3_CTL_QP_TID) | F_EC_VALID)) << 32;\r\nwqe = (struct t3_modify_qp_wr *) skb_put(skb, sizeof(*wqe));\r\nmemset(wqe, 0, sizeof(*wqe));\r\nbuild_fw_riwrh((struct fw_riwrh *) wqe, T3_WR_QP_MOD, 0, 0,\r\nT3_CTL_QP_TID, 7, T3_SOPEOP);\r\nwqe->flags = cpu_to_be32(MODQP_WRITE_EC);\r\nsge_cmd = (3ULL << 56) | FW_RI_SGEEC_START << 8 | 3;\r\nwqe->sge_cmd = cpu_to_be64(sge_cmd);\r\nwqe->ctx1 = cpu_to_be64(ctx1);\r\nwqe->ctx0 = cpu_to_be64(ctx0);\r\nPDBG("CtrlQP dma_addr 0x%llx workq %p size %d\n",\r\n(unsigned long long) rdev_p->ctrl_qp.dma_addr,\r\nrdev_p->ctrl_qp.workq, 1 << T3_CTRL_QP_SIZE_LOG2);\r\nskb->priority = CPL_PRIORITY_CONTROL;\r\nreturn iwch_cxgb3_ofld_send(rdev_p->t3cdev_p, skb);\r\nerr:\r\nkfree_skb(skb);\r\nreturn err;\r\n}\r\nstatic int cxio_hal_destroy_ctrl_qp(struct cxio_rdev *rdev_p)\r\n{\r\ndma_free_coherent(&(rdev_p->rnic_info.pdev->dev),\r\n(1UL << T3_CTRL_QP_SIZE_LOG2)\r\n* sizeof(union t3_wr), rdev_p->ctrl_qp.workq,\r\ndma_unmap_addr(&rdev_p->ctrl_qp, mapping));\r\nreturn cxio_hal_clear_qp_ctx(rdev_p, T3_CTRL_QP_ID);\r\n}\r\nstatic int cxio_hal_ctrl_qp_write_mem(struct cxio_rdev *rdev_p, u32 addr,\r\nu32 len, void *data)\r\n{\r\nu32 i, nr_wqe, copy_len;\r\nu8 *copy_data;\r\nu8 wr_len, utx_len;\r\nenum t3_wr_flags flag;\r\n__be64 *wqe;\r\nu64 utx_cmd;\r\naddr &= 0x7FFFFFF;\r\nnr_wqe = len % 96 ? len / 96 + 1 : len / 96;\r\nPDBG("%s wptr 0x%x rptr 0x%x len %d, nr_wqe %d data %p addr 0x%0x\n",\r\n__func__, rdev_p->ctrl_qp.wptr, rdev_p->ctrl_qp.rptr, len,\r\nnr_wqe, data, addr);\r\nutx_len = 3;\r\nfor (i = 0; i < nr_wqe; i++) {\r\nif (Q_FULL(rdev_p->ctrl_qp.rptr, rdev_p->ctrl_qp.wptr,\r\nT3_CTRL_QP_SIZE_LOG2)) {\r\nPDBG("%s ctrl_qp full wtpr 0x%0x rptr 0x%0x, "\r\n"wait for more space i %d\n", __func__,\r\nrdev_p->ctrl_qp.wptr, rdev_p->ctrl_qp.rptr, i);\r\nif (wait_event_interruptible(rdev_p->ctrl_qp.waitq,\r\n!Q_FULL(rdev_p->ctrl_qp.rptr,\r\nrdev_p->ctrl_qp.wptr,\r\nT3_CTRL_QP_SIZE_LOG2))) {\r\nPDBG("%s ctrl_qp workq interrupted\n",\r\n__func__);\r\nreturn -ERESTARTSYS;\r\n}\r\nPDBG("%s ctrl_qp wakeup, continue posting work request "\r\n"i %d\n", __func__, i);\r\n}\r\nwqe = (__be64 *)(rdev_p->ctrl_qp.workq + (rdev_p->ctrl_qp.wptr %\r\n(1 << T3_CTRL_QP_SIZE_LOG2)));\r\nflag = 0;\r\nif (i == (nr_wqe - 1)) {\r\nflag = T3_COMPLETION_FLAG;\r\nif (len % 32)\r\nutx_len = len / 32 + 1;\r\nelse\r\nutx_len = len / 32;\r\n}\r\nif ((i != 0) &&\r\n(i % (((1 << T3_CTRL_QP_SIZE_LOG2)) >> 1) == 0)) {\r\nflag = T3_COMPLETION_FLAG;\r\nPDBG("%s force completion at i %d\n", __func__, i);\r\n}\r\nwqe += (sizeof(struct t3_bypass_wr) >> 3);\r\nutx_cmd = (T3_UTX_MEM_WRITE << 28) | (addr + i * 3);\r\nutx_cmd <<= 32;\r\nutx_cmd |= (utx_len << 28) | ((utx_len << 2) + 1);\r\n*wqe = cpu_to_be64(utx_cmd);\r\nwqe++;\r\ncopy_data = (u8 *) data + i * 96;\r\ncopy_len = len > 96 ? 96 : len;\r\nif (data)\r\nmemcpy(wqe, copy_data, copy_len);\r\nelse\r\nmemset(wqe, 0, copy_len);\r\nif (copy_len % 32)\r\nmemset(((u8 *) wqe) + copy_len, 0,\r\n32 - (copy_len % 32));\r\nwr_len = ((sizeof(struct t3_bypass_wr)) >> 3) + 1 +\r\n(utx_len << 2);\r\nwqe = (__be64 *)(rdev_p->ctrl_qp.workq + (rdev_p->ctrl_qp.wptr %\r\n(1 << T3_CTRL_QP_SIZE_LOG2)));\r\n((union t3_wrid *)(wqe+1))->id0.low = rdev_p->ctrl_qp.wptr;\r\nbuild_fw_riwrh((struct fw_riwrh *) wqe, T3_WR_BP, flag,\r\nQ_GENBIT(rdev_p->ctrl_qp.wptr,\r\nT3_CTRL_QP_SIZE_LOG2), T3_CTRL_QP_ID,\r\nwr_len, T3_SOPEOP);\r\nif (flag == T3_COMPLETION_FLAG)\r\nring_doorbell(rdev_p->ctrl_qp.doorbell, T3_CTRL_QP_ID);\r\nlen -= 96;\r\nrdev_p->ctrl_qp.wptr++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __cxio_tpt_op(struct cxio_rdev *rdev_p, u32 reset_tpt_entry,\r\nu32 *stag, u8 stag_state, u32 pdid,\r\nenum tpt_mem_type type, enum tpt_mem_perm perm,\r\nu32 zbva, u64 to, u32 len, u8 page_size,\r\nu32 pbl_size, u32 pbl_addr)\r\n{\r\nint err;\r\nstruct tpt_entry tpt;\r\nu32 stag_idx;\r\nu32 wptr;\r\nif (cxio_fatal_error(rdev_p))\r\nreturn -EIO;\r\nstag_state = stag_state > 0;\r\nstag_idx = (*stag) >> 8;\r\nif ((!reset_tpt_entry) && !(*stag != T3_STAG_UNSET)) {\r\nstag_idx = cxio_hal_get_stag(rdev_p->rscp);\r\nif (!stag_idx)\r\nreturn -ENOMEM;\r\n*stag = (stag_idx << 8) | ((*stag) & 0xFF);\r\n}\r\nPDBG("%s stag_state 0x%0x type 0x%0x pdid 0x%0x, stag_idx 0x%x\n",\r\n__func__, stag_state, type, pdid, stag_idx);\r\nmutex_lock(&rdev_p->ctrl_qp.lock);\r\nif (reset_tpt_entry)\r\nmemset(&tpt, 0, sizeof(tpt));\r\nelse {\r\ntpt.valid_stag_pdid = cpu_to_be32(F_TPT_VALID |\r\nV_TPT_STAG_KEY((*stag) & M_TPT_STAG_KEY) |\r\nV_TPT_STAG_STATE(stag_state) |\r\nV_TPT_STAG_TYPE(type) | V_TPT_PDID(pdid));\r\nBUG_ON(page_size >= 28);\r\ntpt.flags_pagesize_qpid = cpu_to_be32(V_TPT_PERM(perm) |\r\n((perm & TPT_MW_BIND) ? F_TPT_MW_BIND_ENABLE : 0) |\r\nV_TPT_ADDR_TYPE((zbva ? TPT_ZBTO : TPT_VATO)) |\r\nV_TPT_PAGE_SIZE(page_size));\r\ntpt.rsvd_pbl_addr = cpu_to_be32(V_TPT_PBL_ADDR(PBL_OFF(rdev_p, pbl_addr)>>3));\r\ntpt.len = cpu_to_be32(len);\r\ntpt.va_hi = cpu_to_be32((u32) (to >> 32));\r\ntpt.va_low_or_fbo = cpu_to_be32((u32) (to & 0xFFFFFFFFULL));\r\ntpt.rsvd_bind_cnt_or_pstag = 0;\r\ntpt.rsvd_pbl_size = cpu_to_be32(V_TPT_PBL_SIZE(pbl_size >> 2));\r\n}\r\nerr = cxio_hal_ctrl_qp_write_mem(rdev_p,\r\nstag_idx +\r\n(rdev_p->rnic_info.tpt_base >> 5),\r\nsizeof(tpt), &tpt);\r\nif (reset_tpt_entry)\r\ncxio_hal_put_stag(rdev_p->rscp, stag_idx);\r\nwptr = rdev_p->ctrl_qp.wptr;\r\nmutex_unlock(&rdev_p->ctrl_qp.lock);\r\nif (!err)\r\nif (wait_event_interruptible(rdev_p->ctrl_qp.waitq,\r\nSEQ32_GE(rdev_p->ctrl_qp.rptr,\r\nwptr)))\r\nreturn -ERESTARTSYS;\r\nreturn err;\r\n}\r\nint cxio_write_pbl(struct cxio_rdev *rdev_p, __be64 *pbl,\r\nu32 pbl_addr, u32 pbl_size)\r\n{\r\nu32 wptr;\r\nint err;\r\nPDBG("%s *pdb_addr 0x%x, pbl_base 0x%x, pbl_size %d\n",\r\n__func__, pbl_addr, rdev_p->rnic_info.pbl_base,\r\npbl_size);\r\nmutex_lock(&rdev_p->ctrl_qp.lock);\r\nerr = cxio_hal_ctrl_qp_write_mem(rdev_p, pbl_addr >> 5, pbl_size << 3,\r\npbl);\r\nwptr = rdev_p->ctrl_qp.wptr;\r\nmutex_unlock(&rdev_p->ctrl_qp.lock);\r\nif (err)\r\nreturn err;\r\nif (wait_event_interruptible(rdev_p->ctrl_qp.waitq,\r\nSEQ32_GE(rdev_p->ctrl_qp.rptr,\r\nwptr)))\r\nreturn -ERESTARTSYS;\r\nreturn 0;\r\n}\r\nint cxio_register_phys_mem(struct cxio_rdev *rdev_p, u32 *stag, u32 pdid,\r\nenum tpt_mem_perm perm, u32 zbva, u64 to, u32 len,\r\nu8 page_size, u32 pbl_size, u32 pbl_addr)\r\n{\r\n*stag = T3_STAG_UNSET;\r\nreturn __cxio_tpt_op(rdev_p, 0, stag, 1, pdid, TPT_NON_SHARED_MR, perm,\r\nzbva, to, len, page_size, pbl_size, pbl_addr);\r\n}\r\nint cxio_reregister_phys_mem(struct cxio_rdev *rdev_p, u32 *stag, u32 pdid,\r\nenum tpt_mem_perm perm, u32 zbva, u64 to, u32 len,\r\nu8 page_size, u32 pbl_size, u32 pbl_addr)\r\n{\r\nreturn __cxio_tpt_op(rdev_p, 0, stag, 1, pdid, TPT_NON_SHARED_MR, perm,\r\nzbva, to, len, page_size, pbl_size, pbl_addr);\r\n}\r\nint cxio_dereg_mem(struct cxio_rdev *rdev_p, u32 stag, u32 pbl_size,\r\nu32 pbl_addr)\r\n{\r\nreturn __cxio_tpt_op(rdev_p, 1, &stag, 0, 0, 0, 0, 0, 0ULL, 0, 0,\r\npbl_size, pbl_addr);\r\n}\r\nint cxio_allocate_window(struct cxio_rdev *rdev_p, u32 * stag, u32 pdid)\r\n{\r\n*stag = T3_STAG_UNSET;\r\nreturn __cxio_tpt_op(rdev_p, 0, stag, 0, pdid, TPT_MW, 0, 0, 0ULL, 0, 0,\r\n0, 0);\r\n}\r\nint cxio_deallocate_window(struct cxio_rdev *rdev_p, u32 stag)\r\n{\r\nreturn __cxio_tpt_op(rdev_p, 1, &stag, 0, 0, 0, 0, 0, 0ULL, 0, 0,\r\n0, 0);\r\n}\r\nint cxio_allocate_stag(struct cxio_rdev *rdev_p, u32 *stag, u32 pdid, u32 pbl_size, u32 pbl_addr)\r\n{\r\n*stag = T3_STAG_UNSET;\r\nreturn __cxio_tpt_op(rdev_p, 0, stag, 0, pdid, TPT_NON_SHARED_MR,\r\n0, 0, 0ULL, 0, 0, pbl_size, pbl_addr);\r\n}\r\nint cxio_rdma_init(struct cxio_rdev *rdev_p, struct t3_rdma_init_attr *attr)\r\n{\r\nstruct t3_rdma_init_wr *wqe;\r\nstruct sk_buff *skb = alloc_skb(sizeof(*wqe), GFP_ATOMIC);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nPDBG("%s rdev_p %p\n", __func__, rdev_p);\r\nwqe = (struct t3_rdma_init_wr *) __skb_put(skb, sizeof(*wqe));\r\nwqe->wrh.op_seop_flags = cpu_to_be32(V_FW_RIWR_OP(T3_WR_INIT));\r\nwqe->wrh.gen_tid_len = cpu_to_be32(V_FW_RIWR_TID(attr->tid) |\r\nV_FW_RIWR_LEN(sizeof(*wqe) >> 3));\r\nwqe->wrid.id1 = 0;\r\nwqe->qpid = cpu_to_be32(attr->qpid);\r\nwqe->pdid = cpu_to_be32(attr->pdid);\r\nwqe->scqid = cpu_to_be32(attr->scqid);\r\nwqe->rcqid = cpu_to_be32(attr->rcqid);\r\nwqe->rq_addr = cpu_to_be32(attr->rq_addr - rdev_p->rnic_info.rqt_base);\r\nwqe->rq_size = cpu_to_be32(attr->rq_size);\r\nwqe->mpaattrs = attr->mpaattrs;\r\nwqe->qpcaps = attr->qpcaps;\r\nwqe->ulpdu_size = cpu_to_be16(attr->tcp_emss);\r\nwqe->rqe_count = cpu_to_be16(attr->rqe_count);\r\nwqe->flags_rtr_type = cpu_to_be16(attr->flags |\r\nV_RTR_TYPE(attr->rtr_type) |\r\nV_CHAN(attr->chan));\r\nwqe->ord = cpu_to_be32(attr->ord);\r\nwqe->ird = cpu_to_be32(attr->ird);\r\nwqe->qp_dma_addr = cpu_to_be64(attr->qp_dma_addr);\r\nwqe->qp_dma_size = cpu_to_be32(attr->qp_dma_size);\r\nwqe->irs = cpu_to_be32(attr->irs);\r\nskb->priority = 0;\r\nreturn iwch_cxgb3_ofld_send(rdev_p->t3cdev_p, skb);\r\n}\r\nvoid cxio_register_ev_cb(cxio_hal_ev_callback_func_t ev_cb)\r\n{\r\ncxio_ev_cb = ev_cb;\r\n}\r\nvoid cxio_unregister_ev_cb(cxio_hal_ev_callback_func_t ev_cb)\r\n{\r\ncxio_ev_cb = NULL;\r\n}\r\nstatic int cxio_hal_ev_handler(struct t3cdev *t3cdev_p, struct sk_buff *skb)\r\n{\r\nstatic int cnt;\r\nstruct cxio_rdev *rdev_p = NULL;\r\nstruct respQ_msg_t *rsp_msg = (struct respQ_msg_t *) skb->data;\r\nPDBG("%d: %s cq_id 0x%x cq_ptr 0x%x genbit %0x overflow %0x an %0x"\r\n" se %0x notify %0x cqbranch %0x creditth %0x\n",\r\ncnt, __func__, RSPQ_CQID(rsp_msg), RSPQ_CQPTR(rsp_msg),\r\nRSPQ_GENBIT(rsp_msg), RSPQ_OVERFLOW(rsp_msg), RSPQ_AN(rsp_msg),\r\nRSPQ_SE(rsp_msg), RSPQ_NOTIFY(rsp_msg), RSPQ_CQBRANCH(rsp_msg),\r\nRSPQ_CREDIT_THRESH(rsp_msg));\r\nPDBG("CQE: QPID 0x%0x genbit %0x type 0x%0x status 0x%0x opcode %d "\r\n"len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x\n",\r\nCQE_QPID(rsp_msg->cqe), CQE_GENBIT(rsp_msg->cqe),\r\nCQE_TYPE(rsp_msg->cqe), CQE_STATUS(rsp_msg->cqe),\r\nCQE_OPCODE(rsp_msg->cqe), CQE_LEN(rsp_msg->cqe),\r\nCQE_WRID_HI(rsp_msg->cqe), CQE_WRID_LOW(rsp_msg->cqe));\r\nrdev_p = (struct cxio_rdev *)t3cdev_p->ulp;\r\nif (!rdev_p) {\r\nPDBG("%s called by t3cdev %p with null ulp\n", __func__,\r\nt3cdev_p);\r\nreturn 0;\r\n}\r\nif (CQE_QPID(rsp_msg->cqe) == T3_CTRL_QP_ID) {\r\nrdev_p->ctrl_qp.rptr = CQE_WRID_LOW(rsp_msg->cqe) + 1;\r\nwake_up_interruptible(&rdev_p->ctrl_qp.waitq);\r\ndev_kfree_skb_irq(skb);\r\n} else if (CQE_QPID(rsp_msg->cqe) == 0xfff8)\r\ndev_kfree_skb_irq(skb);\r\nelse if (cxio_ev_cb)\r\n(*cxio_ev_cb) (rdev_p, skb);\r\nelse\r\ndev_kfree_skb_irq(skb);\r\ncnt++;\r\nreturn 0;\r\n}\r\nint cxio_rdev_open(struct cxio_rdev *rdev_p)\r\n{\r\nstruct net_device *netdev_p = NULL;\r\nint err = 0;\r\nif (strlen(rdev_p->dev_name)) {\r\nif (cxio_hal_find_rdev_by_name(rdev_p->dev_name)) {\r\nreturn -EBUSY;\r\n}\r\nnetdev_p = dev_get_by_name(&init_net, rdev_p->dev_name);\r\nif (!netdev_p) {\r\nreturn -EINVAL;\r\n}\r\ndev_put(netdev_p);\r\n} else if (rdev_p->t3cdev_p) {\r\nif (cxio_hal_find_rdev_by_t3cdev(rdev_p->t3cdev_p)) {\r\nreturn -EBUSY;\r\n}\r\nnetdev_p = rdev_p->t3cdev_p->lldev;\r\nstrncpy(rdev_p->dev_name, rdev_p->t3cdev_p->name,\r\nT3_MAX_DEV_NAME_LEN);\r\n} else {\r\nPDBG("%s t3cdev_p or dev_name must be set\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nlist_add_tail(&rdev_p->entry, &rdev_list);\r\nPDBG("%s opening rnic dev %s\n", __func__, rdev_p->dev_name);\r\nmemset(&rdev_p->ctrl_qp, 0, sizeof(rdev_p->ctrl_qp));\r\nif (!rdev_p->t3cdev_p)\r\nrdev_p->t3cdev_p = dev2t3cdev(netdev_p);\r\nrdev_p->t3cdev_p->ulp = (void *) rdev_p;\r\nerr = rdev_p->t3cdev_p->ctl(rdev_p->t3cdev_p, GET_EMBEDDED_INFO,\r\n&(rdev_p->fw_info));\r\nif (err) {\r\nprintk(KERN_ERR "%s t3cdev_p(%p)->ctl returned error %d.\n",\r\n__func__, rdev_p->t3cdev_p, err);\r\ngoto err1;\r\n}\r\nif (G_FW_VERSION_MAJOR(rdev_p->fw_info.fw_vers) != CXIO_FW_MAJ) {\r\nprintk(KERN_ERR MOD "fatal firmware version mismatch: "\r\n"need version %u but adapter has version %u\n",\r\nCXIO_FW_MAJ,\r\nG_FW_VERSION_MAJOR(rdev_p->fw_info.fw_vers));\r\nerr = -EINVAL;\r\ngoto err1;\r\n}\r\nerr = rdev_p->t3cdev_p->ctl(rdev_p->t3cdev_p, RDMA_GET_PARAMS,\r\n&(rdev_p->rnic_info));\r\nif (err) {\r\nprintk(KERN_ERR "%s t3cdev_p(%p)->ctl returned error %d.\n",\r\n__func__, rdev_p->t3cdev_p, err);\r\ngoto err1;\r\n}\r\nerr = rdev_p->t3cdev_p->ctl(rdev_p->t3cdev_p, GET_PORTS,\r\n&(rdev_p->port_info));\r\nif (err) {\r\nprintk(KERN_ERR "%s t3cdev_p(%p)->ctl returned error %d.\n",\r\n__func__, rdev_p->t3cdev_p, err);\r\ngoto err1;\r\n}\r\ncxio_init_ucontext(rdev_p, &rdev_p->uctx);\r\nrdev_p->qpshift = PAGE_SHIFT -\r\nilog2(65536 >>\r\nilog2(rdev_p->rnic_info.udbell_len >>\r\nPAGE_SHIFT));\r\nrdev_p->qpnr = rdev_p->rnic_info.udbell_len >> PAGE_SHIFT;\r\nrdev_p->qpmask = (65536 >> ilog2(rdev_p->qpnr)) - 1;\r\nPDBG("%s rnic %s info: tpt_base 0x%0x tpt_top 0x%0x num stags %d "\r\n"pbl_base 0x%0x pbl_top 0x%0x rqt_base 0x%0x, rqt_top 0x%0x\n",\r\n__func__, rdev_p->dev_name, rdev_p->rnic_info.tpt_base,\r\nrdev_p->rnic_info.tpt_top, cxio_num_stags(rdev_p),\r\nrdev_p->rnic_info.pbl_base,\r\nrdev_p->rnic_info.pbl_top, rdev_p->rnic_info.rqt_base,\r\nrdev_p->rnic_info.rqt_top);\r\nPDBG("udbell_len 0x%0x udbell_physbase 0x%lx kdb_addr %p qpshift %lu "\r\n"qpnr %d qpmask 0x%x\n",\r\nrdev_p->rnic_info.udbell_len,\r\nrdev_p->rnic_info.udbell_physbase, rdev_p->rnic_info.kdb_addr,\r\nrdev_p->qpshift, rdev_p->qpnr, rdev_p->qpmask);\r\nerr = cxio_hal_init_ctrl_qp(rdev_p);\r\nif (err) {\r\nprintk(KERN_ERR "%s error %d initializing ctrl_qp.\n",\r\n__func__, err);\r\ngoto err1;\r\n}\r\nerr = cxio_hal_init_resource(rdev_p, cxio_num_stags(rdev_p), 0,\r\n0, T3_MAX_NUM_QP, T3_MAX_NUM_CQ,\r\nT3_MAX_NUM_PD);\r\nif (err) {\r\nprintk(KERN_ERR "%s error %d initializing hal resources.\n",\r\n__func__, err);\r\ngoto err2;\r\n}\r\nerr = cxio_hal_pblpool_create(rdev_p);\r\nif (err) {\r\nprintk(KERN_ERR "%s error %d initializing pbl mem pool.\n",\r\n__func__, err);\r\ngoto err3;\r\n}\r\nerr = cxio_hal_rqtpool_create(rdev_p);\r\nif (err) {\r\nprintk(KERN_ERR "%s error %d initializing rqt mem pool.\n",\r\n__func__, err);\r\ngoto err4;\r\n}\r\nreturn 0;\r\nerr4:\r\ncxio_hal_pblpool_destroy(rdev_p);\r\nerr3:\r\ncxio_hal_destroy_resource(rdev_p->rscp);\r\nerr2:\r\ncxio_hal_destroy_ctrl_qp(rdev_p);\r\nerr1:\r\nrdev_p->t3cdev_p->ulp = NULL;\r\nlist_del(&rdev_p->entry);\r\nreturn err;\r\n}\r\nvoid cxio_rdev_close(struct cxio_rdev *rdev_p)\r\n{\r\nif (rdev_p) {\r\ncxio_hal_pblpool_destroy(rdev_p);\r\ncxio_hal_rqtpool_destroy(rdev_p);\r\nlist_del(&rdev_p->entry);\r\ncxio_hal_destroy_ctrl_qp(rdev_p);\r\ncxio_hal_destroy_resource(rdev_p->rscp);\r\nrdev_p->t3cdev_p->ulp = NULL;\r\n}\r\n}\r\nint __init cxio_hal_init(void)\r\n{\r\nif (cxio_hal_init_rhdl_resource(T3_MAX_NUM_RI))\r\nreturn -ENOMEM;\r\nt3_register_cpl_handler(CPL_ASYNC_NOTIF, cxio_hal_ev_handler);\r\nreturn 0;\r\n}\r\nvoid __exit cxio_hal_exit(void)\r\n{\r\nstruct cxio_rdev *rdev, *tmp;\r\nt3_register_cpl_handler(CPL_ASYNC_NOTIF, NULL);\r\nlist_for_each_entry_safe(rdev, tmp, &rdev_list, entry)\r\ncxio_rdev_close(rdev);\r\ncxio_hal_destroy_rhdl_resource();\r\n}\r\nstatic void flush_completed_wrs(struct t3_wq *wq, struct t3_cq *cq)\r\n{\r\nstruct t3_swsq *sqp;\r\n__u32 ptr = wq->sq_rptr;\r\nint count = Q_COUNT(wq->sq_rptr, wq->sq_wptr);\r\nsqp = wq->sq + Q_PTR2IDX(ptr, wq->sq_size_log2);\r\nwhile (count--)\r\nif (!sqp->signaled) {\r\nptr++;\r\nsqp = wq->sq + Q_PTR2IDX(ptr, wq->sq_size_log2);\r\n} else if (sqp->complete) {\r\nPDBG("%s moving cqe into swcq sq idx %ld cq idx %ld\n",\r\n__func__, Q_PTR2IDX(ptr, wq->sq_size_log2),\r\nQ_PTR2IDX(cq->sw_wptr, cq->size_log2));\r\nsqp->cqe.header |= htonl(V_CQE_SWCQE(1));\r\n*(cq->sw_queue + Q_PTR2IDX(cq->sw_wptr, cq->size_log2))\r\n= sqp->cqe;\r\ncq->sw_wptr++;\r\nsqp->signaled = 0;\r\nbreak;\r\n} else\r\nbreak;\r\n}\r\nstatic void create_read_req_cqe(struct t3_wq *wq, struct t3_cqe *hw_cqe,\r\nstruct t3_cqe *read_cqe)\r\n{\r\nread_cqe->u.scqe.wrid_hi = wq->oldest_read->sq_wptr;\r\nread_cqe->len = wq->oldest_read->read_len;\r\nread_cqe->header = htonl(V_CQE_QPID(CQE_QPID(*hw_cqe)) |\r\nV_CQE_SWCQE(SW_CQE(*hw_cqe)) |\r\nV_CQE_OPCODE(T3_READ_REQ) |\r\nV_CQE_TYPE(1));\r\n}\r\nstatic void advance_oldest_read(struct t3_wq *wq)\r\n{\r\nu32 rptr = wq->oldest_read - wq->sq + 1;\r\nu32 wptr = Q_PTR2IDX(wq->sq_wptr, wq->sq_size_log2);\r\nwhile (Q_PTR2IDX(rptr, wq->sq_size_log2) != wptr) {\r\nwq->oldest_read = wq->sq + Q_PTR2IDX(rptr, wq->sq_size_log2);\r\nif (wq->oldest_read->opcode == T3_READ_REQ)\r\nreturn;\r\nrptr++;\r\n}\r\nwq->oldest_read = NULL;\r\n}\r\nint cxio_poll_cq(struct t3_wq *wq, struct t3_cq *cq, struct t3_cqe *cqe,\r\nu8 *cqe_flushed, u64 *cookie, u32 *credit)\r\n{\r\nint ret = 0;\r\nstruct t3_cqe *hw_cqe, read_cqe;\r\n*cqe_flushed = 0;\r\n*credit = 0;\r\nhw_cqe = cxio_next_cqe(cq);\r\nPDBG("%s CQE OOO %d qpid 0x%0x genbit %d type %d status 0x%0x"\r\n" opcode 0x%0x len 0x%0x wrid_hi_stag 0x%x wrid_low_msn 0x%x\n",\r\n__func__, CQE_OOO(*hw_cqe), CQE_QPID(*hw_cqe),\r\nCQE_GENBIT(*hw_cqe), CQE_TYPE(*hw_cqe), CQE_STATUS(*hw_cqe),\r\nCQE_OPCODE(*hw_cqe), CQE_LEN(*hw_cqe), CQE_WRID_HI(*hw_cqe),\r\nCQE_WRID_LOW(*hw_cqe));\r\nif (wq == NULL) {\r\nret = -1;\r\ngoto skip_cqe;\r\n}\r\nif (RQ_TYPE(*hw_cqe) && (CQE_OPCODE(*hw_cqe) == T3_READ_RESP)) {\r\nif (!wq->oldest_read) {\r\nif (CQE_STATUS(*hw_cqe))\r\nwq->error = 1;\r\nret = -1;\r\ngoto skip_cqe;\r\n}\r\ncreate_read_req_cqe(wq, hw_cqe, &read_cqe);\r\nhw_cqe = &read_cqe;\r\nadvance_oldest_read(wq);\r\n}\r\nif (CQE_OPCODE(*hw_cqe) == T3_TERMINATE) {\r\nret = -1;\r\nwq->error = 1;\r\ngoto skip_cqe;\r\n}\r\nif (CQE_STATUS(*hw_cqe) || wq->error) {\r\n*cqe_flushed = wq->error;\r\nwq->error = 1;\r\nif ((CQE_OPCODE(*hw_cqe) == T3_RDMA_WRITE)\r\n&& RQ_TYPE(*hw_cqe)) {\r\nret = -1;\r\ngoto skip_cqe;\r\n}\r\nif ((CQE_OPCODE(*hw_cqe) == T3_READ_RESP) && SQ_TYPE(*hw_cqe)) {\r\nret = -1;\r\ngoto skip_cqe;\r\n}\r\nif (CQE_SEND_OPCODE(*hw_cqe) && RQ_TYPE(*hw_cqe) &&\r\nQ_EMPTY(wq->rq_rptr, wq->rq_wptr)) {\r\nret = -1;\r\ngoto skip_cqe;\r\n}\r\nBUG_ON((*cqe_flushed == 0) && !SW_CQE(*hw_cqe));\r\ngoto proc_cqe;\r\n}\r\nif (RQ_TYPE(*hw_cqe)) {\r\nif (Q_EMPTY(wq->rq_rptr, wq->rq_wptr)) {\r\nwq->error = 1;\r\nret = -1;\r\ngoto skip_cqe;\r\n}\r\nif (unlikely((CQE_WRID_MSN(*hw_cqe) != (wq->rq_rptr + 1)))) {\r\nwq->error = 1;\r\nhw_cqe->header |= htonl(V_CQE_STATUS(TPT_ERR_MSN));\r\ngoto proc_cqe;\r\n}\r\ngoto proc_cqe;\r\n}\r\nif (!SW_CQE(*hw_cqe) && (CQE_WRID_SQ_WPTR(*hw_cqe) != wq->sq_rptr)) {\r\nstruct t3_swsq *sqp;\r\nPDBG("%s out of order completion going in swsq at idx %ld\n",\r\n__func__,\r\nQ_PTR2IDX(CQE_WRID_SQ_WPTR(*hw_cqe), wq->sq_size_log2));\r\nsqp = wq->sq +\r\nQ_PTR2IDX(CQE_WRID_SQ_WPTR(*hw_cqe), wq->sq_size_log2);\r\nsqp->cqe = *hw_cqe;\r\nsqp->complete = 1;\r\nret = -1;\r\ngoto flush_wq;\r\n}\r\nproc_cqe:\r\n*cqe = *hw_cqe;\r\nif (SQ_TYPE(*hw_cqe)) {\r\nwq->sq_rptr = CQE_WRID_SQ_WPTR(*hw_cqe);\r\nPDBG("%s completing sq idx %ld\n", __func__,\r\nQ_PTR2IDX(wq->sq_rptr, wq->sq_size_log2));\r\n*cookie = wq->sq[Q_PTR2IDX(wq->sq_rptr, wq->sq_size_log2)].wr_id;\r\nwq->sq_rptr++;\r\n} else {\r\nPDBG("%s completing rq idx %ld\n", __func__,\r\nQ_PTR2IDX(wq->rq_rptr, wq->rq_size_log2));\r\n*cookie = wq->rq[Q_PTR2IDX(wq->rq_rptr, wq->rq_size_log2)].wr_id;\r\nif (wq->rq[Q_PTR2IDX(wq->rq_rptr, wq->rq_size_log2)].pbl_addr)\r\ncxio_hal_pblpool_free(wq->rdev,\r\nwq->rq[Q_PTR2IDX(wq->rq_rptr,\r\nwq->rq_size_log2)].pbl_addr, T3_STAG0_PBL_SIZE);\r\nBUG_ON(Q_EMPTY(wq->rq_rptr, wq->rq_wptr));\r\nwq->rq_rptr++;\r\n}\r\nflush_wq:\r\nflush_completed_wrs(wq, cq);\r\nskip_cqe:\r\nif (SW_CQE(*hw_cqe)) {\r\nPDBG("%s cq %p cqid 0x%x skip sw cqe sw_rptr 0x%x\n",\r\n__func__, cq, cq->cqid, cq->sw_rptr);\r\n++cq->sw_rptr;\r\n} else {\r\nPDBG("%s cq %p cqid 0x%x skip hw cqe rptr 0x%x\n",\r\n__func__, cq, cq->cqid, cq->rptr);\r\n++cq->rptr;\r\nif (((cq->rptr - cq->wptr) > (1 << (cq->size_log2 - 1)))\r\n|| ((cq->rptr - cq->wptr) >= 128)) {\r\n*credit = cq->rptr - cq->wptr;\r\ncq->wptr = cq->rptr;\r\n}\r\n}\r\nreturn ret;\r\n}
