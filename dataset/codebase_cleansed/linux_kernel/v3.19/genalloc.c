static inline size_t chunk_size(const struct gen_pool_chunk *chunk)\r\n{\r\nreturn chunk->end_addr - chunk->start_addr + 1;\r\n}\r\nstatic int set_bits_ll(unsigned long *addr, unsigned long mask_to_set)\r\n{\r\nunsigned long val, nval;\r\nnval = *addr;\r\ndo {\r\nval = nval;\r\nif (val & mask_to_set)\r\nreturn -EBUSY;\r\ncpu_relax();\r\n} while ((nval = cmpxchg(addr, val, val | mask_to_set)) != val);\r\nreturn 0;\r\n}\r\nstatic int clear_bits_ll(unsigned long *addr, unsigned long mask_to_clear)\r\n{\r\nunsigned long val, nval;\r\nnval = *addr;\r\ndo {\r\nval = nval;\r\nif ((val & mask_to_clear) != mask_to_clear)\r\nreturn -EBUSY;\r\ncpu_relax();\r\n} while ((nval = cmpxchg(addr, val, val & ~mask_to_clear)) != val);\r\nreturn 0;\r\n}\r\nstatic int bitmap_set_ll(unsigned long *map, int start, int nr)\r\n{\r\nunsigned long *p = map + BIT_WORD(start);\r\nconst int size = start + nr;\r\nint bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);\r\nunsigned long mask_to_set = BITMAP_FIRST_WORD_MASK(start);\r\nwhile (nr - bits_to_set >= 0) {\r\nif (set_bits_ll(p, mask_to_set))\r\nreturn nr;\r\nnr -= bits_to_set;\r\nbits_to_set = BITS_PER_LONG;\r\nmask_to_set = ~0UL;\r\np++;\r\n}\r\nif (nr) {\r\nmask_to_set &= BITMAP_LAST_WORD_MASK(size);\r\nif (set_bits_ll(p, mask_to_set))\r\nreturn nr;\r\n}\r\nreturn 0;\r\n}\r\nstatic int bitmap_clear_ll(unsigned long *map, int start, int nr)\r\n{\r\nunsigned long *p = map + BIT_WORD(start);\r\nconst int size = start + nr;\r\nint bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);\r\nunsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);\r\nwhile (nr - bits_to_clear >= 0) {\r\nif (clear_bits_ll(p, mask_to_clear))\r\nreturn nr;\r\nnr -= bits_to_clear;\r\nbits_to_clear = BITS_PER_LONG;\r\nmask_to_clear = ~0UL;\r\np++;\r\n}\r\nif (nr) {\r\nmask_to_clear &= BITMAP_LAST_WORD_MASK(size);\r\nif (clear_bits_ll(p, mask_to_clear))\r\nreturn nr;\r\n}\r\nreturn 0;\r\n}\r\nstruct gen_pool *gen_pool_create(int min_alloc_order, int nid)\r\n{\r\nstruct gen_pool *pool;\r\npool = kmalloc_node(sizeof(struct gen_pool), GFP_KERNEL, nid);\r\nif (pool != NULL) {\r\nspin_lock_init(&pool->lock);\r\nINIT_LIST_HEAD(&pool->chunks);\r\npool->min_alloc_order = min_alloc_order;\r\npool->algo = gen_pool_first_fit;\r\npool->data = NULL;\r\n}\r\nreturn pool;\r\n}\r\nint gen_pool_add_virt(struct gen_pool *pool, unsigned long virt, phys_addr_t phys,\r\nsize_t size, int nid)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nint nbits = size >> pool->min_alloc_order;\r\nint nbytes = sizeof(struct gen_pool_chunk) +\r\nBITS_TO_LONGS(nbits) * sizeof(long);\r\nchunk = kzalloc_node(nbytes, GFP_KERNEL, nid);\r\nif (unlikely(chunk == NULL))\r\nreturn -ENOMEM;\r\nchunk->phys_addr = phys;\r\nchunk->start_addr = virt;\r\nchunk->end_addr = virt + size - 1;\r\natomic_set(&chunk->avail, size);\r\nspin_lock(&pool->lock);\r\nlist_add_rcu(&chunk->next_chunk, &pool->chunks);\r\nspin_unlock(&pool->lock);\r\nreturn 0;\r\n}\r\nphys_addr_t gen_pool_virt_to_phys(struct gen_pool *pool, unsigned long addr)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nphys_addr_t paddr = -1;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {\r\nif (addr >= chunk->start_addr && addr <= chunk->end_addr) {\r\npaddr = chunk->phys_addr + (addr - chunk->start_addr);\r\nbreak;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn paddr;\r\n}\r\nvoid gen_pool_destroy(struct gen_pool *pool)\r\n{\r\nstruct list_head *_chunk, *_next_chunk;\r\nstruct gen_pool_chunk *chunk;\r\nint order = pool->min_alloc_order;\r\nint bit, end_bit;\r\nlist_for_each_safe(_chunk, _next_chunk, &pool->chunks) {\r\nchunk = list_entry(_chunk, struct gen_pool_chunk, next_chunk);\r\nlist_del(&chunk->next_chunk);\r\nend_bit = chunk_size(chunk) >> order;\r\nbit = find_next_bit(chunk->bits, end_bit, 0);\r\nBUG_ON(bit < end_bit);\r\nkfree(chunk);\r\n}\r\nkfree(pool);\r\nreturn;\r\n}\r\nunsigned long gen_pool_alloc(struct gen_pool *pool, size_t size)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nunsigned long addr = 0;\r\nint order = pool->min_alloc_order;\r\nint nbits, start_bit = 0, end_bit, remain;\r\n#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG\r\nBUG_ON(in_nmi());\r\n#endif\r\nif (size == 0)\r\nreturn 0;\r\nnbits = (size + (1UL << order) - 1) >> order;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {\r\nif (size > atomic_read(&chunk->avail))\r\ncontinue;\r\nend_bit = chunk_size(chunk) >> order;\r\nretry:\r\nstart_bit = pool->algo(chunk->bits, end_bit, start_bit, nbits,\r\npool->data);\r\nif (start_bit >= end_bit)\r\ncontinue;\r\nremain = bitmap_set_ll(chunk->bits, start_bit, nbits);\r\nif (remain) {\r\nremain = bitmap_clear_ll(chunk->bits, start_bit,\r\nnbits - remain);\r\nBUG_ON(remain);\r\ngoto retry;\r\n}\r\naddr = chunk->start_addr + ((unsigned long)start_bit << order);\r\nsize = nbits << order;\r\natomic_sub(size, &chunk->avail);\r\nbreak;\r\n}\r\nrcu_read_unlock();\r\nreturn addr;\r\n}\r\nvoid *gen_pool_dma_alloc(struct gen_pool *pool, size_t size, dma_addr_t *dma)\r\n{\r\nunsigned long vaddr;\r\nif (!pool)\r\nreturn NULL;\r\nvaddr = gen_pool_alloc(pool, size);\r\nif (!vaddr)\r\nreturn NULL;\r\nif (dma)\r\n*dma = gen_pool_virt_to_phys(pool, vaddr);\r\nreturn (void *)vaddr;\r\n}\r\nvoid gen_pool_free(struct gen_pool *pool, unsigned long addr, size_t size)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nint order = pool->min_alloc_order;\r\nint start_bit, nbits, remain;\r\n#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG\r\nBUG_ON(in_nmi());\r\n#endif\r\nnbits = (size + (1UL << order) - 1) >> order;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {\r\nif (addr >= chunk->start_addr && addr <= chunk->end_addr) {\r\nBUG_ON(addr + size - 1 > chunk->end_addr);\r\nstart_bit = (addr - chunk->start_addr) >> order;\r\nremain = bitmap_clear_ll(chunk->bits, start_bit, nbits);\r\nBUG_ON(remain);\r\nsize = nbits << order;\r\natomic_add(size, &chunk->avail);\r\nrcu_read_unlock();\r\nreturn;\r\n}\r\n}\r\nrcu_read_unlock();\r\nBUG();\r\n}\r\nvoid gen_pool_for_each_chunk(struct gen_pool *pool,\r\nvoid (*func)(struct gen_pool *pool, struct gen_pool_chunk *chunk, void *data),\r\nvoid *data)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &(pool)->chunks, next_chunk)\r\nfunc(pool, chunk, data);\r\nrcu_read_unlock();\r\n}\r\nbool addr_in_gen_pool(struct gen_pool *pool, unsigned long start,\r\nsize_t size)\r\n{\r\nbool found = false;\r\nunsigned long end = start + size;\r\nstruct gen_pool_chunk *chunk;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &(pool)->chunks, next_chunk) {\r\nif (start >= chunk->start_addr && start <= chunk->end_addr) {\r\nif (end <= chunk->end_addr) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn found;\r\n}\r\nsize_t gen_pool_avail(struct gen_pool *pool)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nsize_t avail = 0;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk)\r\navail += atomic_read(&chunk->avail);\r\nrcu_read_unlock();\r\nreturn avail;\r\n}\r\nsize_t gen_pool_size(struct gen_pool *pool)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nsize_t size = 0;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk)\r\nsize += chunk_size(chunk);\r\nrcu_read_unlock();\r\nreturn size;\r\n}\r\nvoid gen_pool_set_algo(struct gen_pool *pool, genpool_algo_t algo, void *data)\r\n{\r\nrcu_read_lock();\r\npool->algo = algo;\r\nif (!pool->algo)\r\npool->algo = gen_pool_first_fit;\r\npool->data = data;\r\nrcu_read_unlock();\r\n}\r\nunsigned long gen_pool_first_fit(unsigned long *map, unsigned long size,\r\nunsigned long start, unsigned int nr, void *data)\r\n{\r\nreturn bitmap_find_next_zero_area(map, size, start, nr, 0);\r\n}\r\nunsigned long gen_pool_first_fit_order_align(unsigned long *map,\r\nunsigned long size, unsigned long start,\r\nunsigned int nr, void *data)\r\n{\r\nunsigned long align_mask = roundup_pow_of_two(nr) - 1;\r\nreturn bitmap_find_next_zero_area(map, size, start, nr, align_mask);\r\n}\r\nunsigned long gen_pool_best_fit(unsigned long *map, unsigned long size,\r\nunsigned long start, unsigned int nr, void *data)\r\n{\r\nunsigned long start_bit = size;\r\nunsigned long len = size + 1;\r\nunsigned long index;\r\nindex = bitmap_find_next_zero_area(map, size, start, nr, 0);\r\nwhile (index < size) {\r\nint next_bit = find_next_bit(map, size, index + nr);\r\nif ((next_bit - index) < len) {\r\nlen = next_bit - index;\r\nstart_bit = index;\r\nif (len == nr)\r\nreturn start_bit;\r\n}\r\nindex = bitmap_find_next_zero_area(map, size,\r\nnext_bit + 1, nr, 0);\r\n}\r\nreturn start_bit;\r\n}\r\nstatic void devm_gen_pool_release(struct device *dev, void *res)\r\n{\r\ngen_pool_destroy(*(struct gen_pool **)res);\r\n}\r\nstruct gen_pool *devm_gen_pool_create(struct device *dev, int min_alloc_order,\r\nint nid)\r\n{\r\nstruct gen_pool **ptr, *pool;\r\nptr = devres_alloc(devm_gen_pool_release, sizeof(*ptr), GFP_KERNEL);\r\npool = gen_pool_create(min_alloc_order, nid);\r\nif (pool) {\r\n*ptr = pool;\r\ndevres_add(dev, ptr);\r\n} else {\r\ndevres_free(ptr);\r\n}\r\nreturn pool;\r\n}\r\nstruct gen_pool *dev_get_gen_pool(struct device *dev)\r\n{\r\nstruct gen_pool **p = devres_find(dev, devm_gen_pool_release, NULL,\r\nNULL);\r\nif (!p)\r\nreturn NULL;\r\nreturn *p;\r\n}\r\nstruct gen_pool *of_get_named_gen_pool(struct device_node *np,\r\nconst char *propname, int index)\r\n{\r\nstruct platform_device *pdev;\r\nstruct device_node *np_pool;\r\nnp_pool = of_parse_phandle(np, propname, index);\r\nif (!np_pool)\r\nreturn NULL;\r\npdev = of_find_device_by_node(np_pool);\r\nof_node_put(np_pool);\r\nif (!pdev)\r\nreturn NULL;\r\nreturn dev_get_gen_pool(&pdev->dev);\r\n}
