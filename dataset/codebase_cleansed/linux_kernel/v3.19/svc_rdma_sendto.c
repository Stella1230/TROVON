static int map_xdr(struct svcxprt_rdma *xprt,\r\nstruct xdr_buf *xdr,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nint sge_no;\r\nu32 sge_bytes;\r\nu32 page_bytes;\r\nu32 page_off;\r\nint page_no;\r\nBUG_ON(xdr->len !=\r\n(xdr->head[0].iov_len + xdr->page_len + xdr->tail[0].iov_len));\r\nsge_no = 1;\r\nvec->sge[sge_no].iov_base = xdr->head[0].iov_base;\r\nvec->sge[sge_no].iov_len = xdr->head[0].iov_len;\r\nsge_no++;\r\npage_no = 0;\r\npage_bytes = xdr->page_len;\r\npage_off = xdr->page_base;\r\nwhile (page_bytes) {\r\nvec->sge[sge_no].iov_base =\r\npage_address(xdr->pages[page_no]) + page_off;\r\nsge_bytes = min_t(u32, page_bytes, (PAGE_SIZE - page_off));\r\npage_bytes -= sge_bytes;\r\nvec->sge[sge_no].iov_len = sge_bytes;\r\nsge_no++;\r\npage_no++;\r\npage_off = 0;\r\n}\r\nif (xdr->tail[0].iov_len) {\r\nvec->sge[sge_no].iov_base = xdr->tail[0].iov_base;\r\nvec->sge[sge_no].iov_len = xdr->tail[0].iov_len;\r\nsge_no++;\r\n}\r\ndprintk("svcrdma: map_xdr: sge_no %d page_no %d "\r\n"page_base %u page_len %u head_len %zu tail_len %zu\n",\r\nsge_no, page_no, xdr->page_base, xdr->page_len,\r\nxdr->head[0].iov_len, xdr->tail[0].iov_len);\r\nvec->count = sge_no;\r\nreturn 0;\r\n}\r\nstatic dma_addr_t dma_map_xdr(struct svcxprt_rdma *xprt,\r\nstruct xdr_buf *xdr,\r\nu32 xdr_off, size_t len, int dir)\r\n{\r\nstruct page *page;\r\ndma_addr_t dma_addr;\r\nif (xdr_off < xdr->head[0].iov_len) {\r\nxdr_off += (unsigned long)xdr->head[0].iov_base & ~PAGE_MASK;\r\npage = virt_to_page(xdr->head[0].iov_base);\r\n} else {\r\nxdr_off -= xdr->head[0].iov_len;\r\nif (xdr_off < xdr->page_len) {\r\nxdr_off += xdr->page_base;\r\npage = xdr->pages[xdr_off >> PAGE_SHIFT];\r\nxdr_off &= ~PAGE_MASK;\r\n} else {\r\nxdr_off -= xdr->page_len;\r\nxdr_off += (unsigned long)\r\nxdr->tail[0].iov_base & ~PAGE_MASK;\r\npage = virt_to_page(xdr->tail[0].iov_base);\r\n}\r\n}\r\ndma_addr = ib_dma_map_page(xprt->sc_cm_id->device, page, xdr_off,\r\nmin_t(size_t, PAGE_SIZE, len), dir);\r\nreturn dma_addr;\r\n}\r\nstatic int send_write(struct svcxprt_rdma *xprt, struct svc_rqst *rqstp,\r\nu32 rmr, u64 to,\r\nu32 xdr_off, int write_len,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nstruct ib_send_wr write_wr;\r\nstruct ib_sge *sge;\r\nint xdr_sge_no;\r\nint sge_no;\r\nint sge_bytes;\r\nint sge_off;\r\nint bc;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nBUG_ON(vec->count > RPCSVC_MAXPAGES);\r\ndprintk("svcrdma: RDMA_WRITE rmr=%x, to=%llx, xdr_off=%d, "\r\n"write_len=%d, vec->sge=%p, vec->count=%lu\n",\r\nrmr, (unsigned long long)to, xdr_off,\r\nwrite_len, vec->sge, vec->count);\r\nctxt = svc_rdma_get_context(xprt);\r\nctxt->direction = DMA_TO_DEVICE;\r\nsge = ctxt->sge;\r\nfor (bc = xdr_off, xdr_sge_no = 1; bc && xdr_sge_no < vec->count;\r\nxdr_sge_no++) {\r\nif (vec->sge[xdr_sge_no].iov_len > bc)\r\nbreak;\r\nbc -= vec->sge[xdr_sge_no].iov_len;\r\n}\r\nsge_off = bc;\r\nbc = write_len;\r\nsge_no = 0;\r\nwhile (bc != 0) {\r\nsge_bytes = min_t(size_t,\r\nbc, vec->sge[xdr_sge_no].iov_len-sge_off);\r\nsge[sge_no].length = sge_bytes;\r\nsge[sge_no].addr =\r\ndma_map_xdr(xprt, &rqstp->rq_res, xdr_off,\r\nsge_bytes, DMA_TO_DEVICE);\r\nxdr_off += sge_bytes;\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nsge[sge_no].addr))\r\ngoto err;\r\natomic_inc(&xprt->sc_dma_used);\r\nsge[sge_no].lkey = xprt->sc_dma_lkey;\r\nctxt->count++;\r\nsge_off = 0;\r\nsge_no++;\r\nxdr_sge_no++;\r\nBUG_ON(xdr_sge_no > vec->count);\r\nbc -= sge_bytes;\r\nif (sge_no == xprt->sc_max_sge)\r\nbreak;\r\n}\r\nmemset(&write_wr, 0, sizeof write_wr);\r\nctxt->wr_op = IB_WR_RDMA_WRITE;\r\nwrite_wr.wr_id = (unsigned long)ctxt;\r\nwrite_wr.sg_list = &sge[0];\r\nwrite_wr.num_sge = sge_no;\r\nwrite_wr.opcode = IB_WR_RDMA_WRITE;\r\nwrite_wr.send_flags = IB_SEND_SIGNALED;\r\nwrite_wr.wr.rdma.rkey = rmr;\r\nwrite_wr.wr.rdma.remote_addr = to;\r\natomic_inc(&rdma_stat_write);\r\nif (svc_rdma_send(xprt, &write_wr))\r\ngoto err;\r\nreturn write_len - bc;\r\nerr:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn -EIO;\r\n}\r\nstatic int send_write_chunks(struct svcxprt_rdma *xprt,\r\nstruct rpcrdma_msg *rdma_argp,\r\nstruct rpcrdma_msg *rdma_resp,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nu32 xfer_len = rqstp->rq_res.page_len + rqstp->rq_res.tail[0].iov_len;\r\nint write_len;\r\nu32 xdr_off;\r\nint chunk_off;\r\nint chunk_no;\r\nstruct rpcrdma_write_array *arg_ary;\r\nstruct rpcrdma_write_array *res_ary;\r\nint ret;\r\narg_ary = svc_rdma_get_write_array(rdma_argp);\r\nif (!arg_ary)\r\nreturn 0;\r\nres_ary = (struct rpcrdma_write_array *)\r\n&rdma_resp->rm_body.rm_chunks[1];\r\nfor (xdr_off = rqstp->rq_res.head[0].iov_len, chunk_no = 0;\r\nxfer_len && chunk_no < arg_ary->wc_nchunks;\r\nchunk_no++) {\r\nstruct rpcrdma_segment *arg_ch;\r\nu64 rs_offset;\r\narg_ch = &arg_ary->wc_array[chunk_no].wc_target;\r\nwrite_len = min(xfer_len, ntohl(arg_ch->rs_length));\r\nxdr_decode_hyper((__be32 *)&arg_ch->rs_offset, &rs_offset);\r\nsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\r\narg_ch->rs_handle,\r\narg_ch->rs_offset,\r\nwrite_len);\r\nchunk_off = 0;\r\nwhile (write_len) {\r\nret = send_write(xprt, rqstp,\r\nntohl(arg_ch->rs_handle),\r\nrs_offset + chunk_off,\r\nxdr_off,\r\nwrite_len,\r\nvec);\r\nif (ret <= 0) {\r\ndprintk("svcrdma: RDMA_WRITE failed, ret=%d\n",\r\nret);\r\nreturn -EIO;\r\n}\r\nchunk_off += ret;\r\nxdr_off += ret;\r\nxfer_len -= ret;\r\nwrite_len -= ret;\r\n}\r\n}\r\nsvc_rdma_xdr_encode_write_list(rdma_resp, chunk_no);\r\nreturn rqstp->rq_res.page_len + rqstp->rq_res.tail[0].iov_len;\r\n}\r\nstatic int send_reply_chunks(struct svcxprt_rdma *xprt,\r\nstruct rpcrdma_msg *rdma_argp,\r\nstruct rpcrdma_msg *rdma_resp,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nu32 xfer_len = rqstp->rq_res.len;\r\nint write_len;\r\nu32 xdr_off;\r\nint chunk_no;\r\nint chunk_off;\r\nint nchunks;\r\nstruct rpcrdma_segment *ch;\r\nstruct rpcrdma_write_array *arg_ary;\r\nstruct rpcrdma_write_array *res_ary;\r\nint ret;\r\narg_ary = svc_rdma_get_reply_array(rdma_argp);\r\nif (!arg_ary)\r\nreturn 0;\r\nres_ary = (struct rpcrdma_write_array *)\r\n&rdma_resp->rm_body.rm_chunks[2];\r\nnchunks = ntohl(arg_ary->wc_nchunks);\r\nfor (xdr_off = 0, chunk_no = 0;\r\nxfer_len && chunk_no < nchunks;\r\nchunk_no++) {\r\nu64 rs_offset;\r\nch = &arg_ary->wc_array[chunk_no].wc_target;\r\nwrite_len = min(xfer_len, htonl(ch->rs_length));\r\nxdr_decode_hyper((__be32 *)&ch->rs_offset, &rs_offset);\r\nsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\r\nch->rs_handle, ch->rs_offset,\r\nwrite_len);\r\nchunk_off = 0;\r\nwhile (write_len) {\r\nret = send_write(xprt, rqstp,\r\nntohl(ch->rs_handle),\r\nrs_offset + chunk_off,\r\nxdr_off,\r\nwrite_len,\r\nvec);\r\nif (ret <= 0) {\r\ndprintk("svcrdma: RDMA_WRITE failed, ret=%d\n",\r\nret);\r\nreturn -EIO;\r\n}\r\nchunk_off += ret;\r\nxdr_off += ret;\r\nxfer_len -= ret;\r\nwrite_len -= ret;\r\n}\r\n}\r\nsvc_rdma_xdr_encode_reply_array(res_ary, chunk_no);\r\nreturn rqstp->rq_res.len;\r\n}\r\nstatic int send_reply(struct svcxprt_rdma *rdma,\r\nstruct svc_rqst *rqstp,\r\nstruct page *page,\r\nstruct rpcrdma_msg *rdma_resp,\r\nstruct svc_rdma_op_ctxt *ctxt,\r\nstruct svc_rdma_req_map *vec,\r\nint byte_count)\r\n{\r\nstruct ib_send_wr send_wr;\r\nint sge_no;\r\nint sge_bytes;\r\nint page_no;\r\nint pages;\r\nint ret;\r\nret = svc_rdma_post_recv(rdma);\r\nif (ret) {\r\nprintk(KERN_INFO\r\n"svcrdma: could not post a receive buffer, err=%d."\r\n"Closing transport %p.\n", ret, rdma);\r\nset_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn -ENOTCONN;\r\n}\r\nctxt->pages[0] = page;\r\nctxt->count = 1;\r\nctxt->sge[0].lkey = rdma->sc_dma_lkey;\r\nctxt->sge[0].length = svc_rdma_xdr_get_reply_hdr_len(rdma_resp);\r\nctxt->sge[0].addr =\r\nib_dma_map_page(rdma->sc_cm_id->device, page, 0,\r\nctxt->sge[0].length, DMA_TO_DEVICE);\r\nif (ib_dma_mapping_error(rdma->sc_cm_id->device, ctxt->sge[0].addr))\r\ngoto err;\r\natomic_inc(&rdma->sc_dma_used);\r\nctxt->direction = DMA_TO_DEVICE;\r\nfor (sge_no = 1; byte_count && sge_no < vec->count; sge_no++) {\r\nint xdr_off = 0;\r\nsge_bytes = min_t(size_t, vec->sge[sge_no].iov_len, byte_count);\r\nbyte_count -= sge_bytes;\r\nctxt->sge[sge_no].addr =\r\ndma_map_xdr(rdma, &rqstp->rq_res, xdr_off,\r\nsge_bytes, DMA_TO_DEVICE);\r\nxdr_off += sge_bytes;\r\nif (ib_dma_mapping_error(rdma->sc_cm_id->device,\r\nctxt->sge[sge_no].addr))\r\ngoto err;\r\natomic_inc(&rdma->sc_dma_used);\r\nctxt->sge[sge_no].lkey = rdma->sc_dma_lkey;\r\nctxt->sge[sge_no].length = sge_bytes;\r\n}\r\nBUG_ON(byte_count != 0);\r\npages = rqstp->rq_next_page - rqstp->rq_respages;\r\nfor (page_no = 0; page_no < pages; page_no++) {\r\nctxt->pages[page_no+1] = rqstp->rq_respages[page_no];\r\nctxt->count++;\r\nrqstp->rq_respages[page_no] = NULL;\r\nif (page_no+1 >= sge_no)\r\nctxt->sge[page_no+1].length = 0;\r\n}\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\nBUG_ON(sge_no > rdma->sc_max_sge);\r\nmemset(&send_wr, 0, sizeof send_wr);\r\nctxt->wr_op = IB_WR_SEND;\r\nsend_wr.wr_id = (unsigned long)ctxt;\r\nsend_wr.sg_list = ctxt->sge;\r\nsend_wr.num_sge = sge_no;\r\nsend_wr.opcode = IB_WR_SEND;\r\nsend_wr.send_flags = IB_SEND_SIGNALED;\r\nret = svc_rdma_send(rdma, &send_wr);\r\nif (ret)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn -EIO;\r\n}\r\nvoid svc_rdma_prep_reply_hdr(struct svc_rqst *rqstp)\r\n{\r\n}\r\nstatic void *xdr_start(struct xdr_buf *xdr)\r\n{\r\nreturn xdr->head[0].iov_base -\r\n(xdr->len -\r\nxdr->page_len -\r\nxdr->tail[0].iov_len -\r\nxdr->head[0].iov_len);\r\n}\r\nint svc_rdma_sendto(struct svc_rqst *rqstp)\r\n{\r\nstruct svc_xprt *xprt = rqstp->rq_xprt;\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nstruct rpcrdma_msg *rdma_argp;\r\nstruct rpcrdma_msg *rdma_resp;\r\nstruct rpcrdma_write_array *reply_ary;\r\nenum rpcrdma_proc reply_type;\r\nint ret;\r\nint inline_bytes;\r\nstruct page *res_page;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nstruct svc_rdma_req_map *vec;\r\ndprintk("svcrdma: sending response for rqstp=%p\n", rqstp);\r\nrdma_argp = xdr_start(&rqstp->rq_arg);\r\nctxt = svc_rdma_get_context(rdma);\r\nctxt->direction = DMA_TO_DEVICE;\r\nvec = svc_rdma_get_req_map();\r\nret = map_xdr(rdma, &rqstp->rq_res, vec);\r\nif (ret)\r\ngoto err0;\r\ninline_bytes = rqstp->rq_res.len;\r\nres_page = svc_rdma_get_page();\r\nrdma_resp = page_address(res_page);\r\nreply_ary = svc_rdma_get_reply_array(rdma_argp);\r\nif (reply_ary)\r\nreply_type = RDMA_NOMSG;\r\nelse\r\nreply_type = RDMA_MSG;\r\nsvc_rdma_xdr_encode_reply_header(rdma, rdma_argp,\r\nrdma_resp, reply_type);\r\nret = send_write_chunks(rdma, rdma_argp, rdma_resp,\r\nrqstp, vec);\r\nif (ret < 0) {\r\nprintk(KERN_ERR "svcrdma: failed to send write chunks, rc=%d\n",\r\nret);\r\ngoto err1;\r\n}\r\ninline_bytes -= ret;\r\nret = send_reply_chunks(rdma, rdma_argp, rdma_resp,\r\nrqstp, vec);\r\nif (ret < 0) {\r\nprintk(KERN_ERR "svcrdma: failed to send reply chunks, rc=%d\n",\r\nret);\r\ngoto err1;\r\n}\r\ninline_bytes -= ret;\r\nret = send_reply(rdma, rqstp, res_page, rdma_resp, ctxt, vec,\r\ninline_bytes);\r\nsvc_rdma_put_req_map(vec);\r\ndprintk("svcrdma: send_reply returns %d\n", ret);\r\nreturn ret;\r\nerr1:\r\nput_page(res_page);\r\nerr0:\r\nsvc_rdma_put_req_map(vec);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn ret;\r\n}
