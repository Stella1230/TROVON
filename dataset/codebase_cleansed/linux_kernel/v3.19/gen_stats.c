static inline int\r\ngnet_stats_copy(struct gnet_dump *d, int type, void *buf, int size)\r\n{\r\nif (nla_put(d->skb, type, size, buf))\r\ngoto nla_put_failure;\r\nreturn 0;\r\nnla_put_failure:\r\nspin_unlock_bh(d->lock);\r\nreturn -1;\r\n}\r\nint\r\ngnet_stats_start_copy_compat(struct sk_buff *skb, int type, int tc_stats_type,\r\nint xstats_type, spinlock_t *lock, struct gnet_dump *d)\r\n__acquires(lock)\r\n{\r\nmemset(d, 0, sizeof(*d));\r\nspin_lock_bh(lock);\r\nd->lock = lock;\r\nif (type)\r\nd->tail = (struct nlattr *)skb_tail_pointer(skb);\r\nd->skb = skb;\r\nd->compat_tc_stats = tc_stats_type;\r\nd->compat_xstats = xstats_type;\r\nif (d->tail)\r\nreturn gnet_stats_copy(d, type, NULL, 0);\r\nreturn 0;\r\n}\r\nint\r\ngnet_stats_start_copy(struct sk_buff *skb, int type, spinlock_t *lock,\r\nstruct gnet_dump *d)\r\n{\r\nreturn gnet_stats_start_copy_compat(skb, type, 0, 0, lock, d);\r\n}\r\nstatic void\r\n__gnet_stats_copy_basic_cpu(struct gnet_stats_basic_packed *bstats,\r\nstruct gnet_stats_basic_cpu __percpu *cpu)\r\n{\r\nint i;\r\nfor_each_possible_cpu(i) {\r\nstruct gnet_stats_basic_cpu *bcpu = per_cpu_ptr(cpu, i);\r\nunsigned int start;\r\nu64 bytes;\r\nu32 packets;\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&bcpu->syncp);\r\nbytes = bcpu->bstats.bytes;\r\npackets = bcpu->bstats.packets;\r\n} while (u64_stats_fetch_retry_irq(&bcpu->syncp, start));\r\nbstats->bytes += bytes;\r\nbstats->packets += packets;\r\n}\r\n}\r\nvoid\r\n__gnet_stats_copy_basic(struct gnet_stats_basic_packed *bstats,\r\nstruct gnet_stats_basic_cpu __percpu *cpu,\r\nstruct gnet_stats_basic_packed *b)\r\n{\r\nif (cpu) {\r\n__gnet_stats_copy_basic_cpu(bstats, cpu);\r\n} else {\r\nbstats->bytes = b->bytes;\r\nbstats->packets = b->packets;\r\n}\r\n}\r\nint\r\ngnet_stats_copy_basic(struct gnet_dump *d,\r\nstruct gnet_stats_basic_cpu __percpu *cpu,\r\nstruct gnet_stats_basic_packed *b)\r\n{\r\nstruct gnet_stats_basic_packed bstats = {0};\r\n__gnet_stats_copy_basic(&bstats, cpu, b);\r\nif (d->compat_tc_stats) {\r\nd->tc_stats.bytes = bstats.bytes;\r\nd->tc_stats.packets = bstats.packets;\r\n}\r\nif (d->tail) {\r\nstruct gnet_stats_basic sb;\r\nmemset(&sb, 0, sizeof(sb));\r\nsb.bytes = bstats.bytes;\r\nsb.packets = bstats.packets;\r\nreturn gnet_stats_copy(d, TCA_STATS_BASIC, &sb, sizeof(sb));\r\n}\r\nreturn 0;\r\n}\r\nint\r\ngnet_stats_copy_rate_est(struct gnet_dump *d,\r\nconst struct gnet_stats_basic_packed *b,\r\nstruct gnet_stats_rate_est64 *r)\r\n{\r\nstruct gnet_stats_rate_est est;\r\nint res;\r\nif (b && !gen_estimator_active(b, r))\r\nreturn 0;\r\nest.bps = min_t(u64, UINT_MAX, r->bps);\r\nest.pps = r->pps;\r\nif (d->compat_tc_stats) {\r\nd->tc_stats.bps = est.bps;\r\nd->tc_stats.pps = est.pps;\r\n}\r\nif (d->tail) {\r\nres = gnet_stats_copy(d, TCA_STATS_RATE_EST, &est, sizeof(est));\r\nif (res < 0 || est.bps == r->bps)\r\nreturn res;\r\nreturn gnet_stats_copy(d, TCA_STATS_RATE_EST64, r, sizeof(*r));\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\n__gnet_stats_copy_queue_cpu(struct gnet_stats_queue *qstats,\r\nconst struct gnet_stats_queue __percpu *q)\r\n{\r\nint i;\r\nfor_each_possible_cpu(i) {\r\nconst struct gnet_stats_queue *qcpu = per_cpu_ptr(q, i);\r\nqstats->qlen = 0;\r\nqstats->backlog += qcpu->backlog;\r\nqstats->drops += qcpu->drops;\r\nqstats->requeues += qcpu->requeues;\r\nqstats->overlimits += qcpu->overlimits;\r\n}\r\n}\r\nstatic void __gnet_stats_copy_queue(struct gnet_stats_queue *qstats,\r\nconst struct gnet_stats_queue __percpu *cpu,\r\nconst struct gnet_stats_queue *q,\r\n__u32 qlen)\r\n{\r\nif (cpu) {\r\n__gnet_stats_copy_queue_cpu(qstats, cpu);\r\n} else {\r\nqstats->qlen = q->qlen;\r\nqstats->backlog = q->backlog;\r\nqstats->drops = q->drops;\r\nqstats->requeues = q->requeues;\r\nqstats->overlimits = q->overlimits;\r\n}\r\nqstats->qlen = qlen;\r\n}\r\nint\r\ngnet_stats_copy_queue(struct gnet_dump *d,\r\nstruct gnet_stats_queue __percpu *cpu_q,\r\nstruct gnet_stats_queue *q, __u32 qlen)\r\n{\r\nstruct gnet_stats_queue qstats = {0};\r\n__gnet_stats_copy_queue(&qstats, cpu_q, q, qlen);\r\nif (d->compat_tc_stats) {\r\nd->tc_stats.drops = qstats.drops;\r\nd->tc_stats.qlen = qstats.qlen;\r\nd->tc_stats.backlog = qstats.backlog;\r\nd->tc_stats.overlimits = qstats.overlimits;\r\n}\r\nif (d->tail)\r\nreturn gnet_stats_copy(d, TCA_STATS_QUEUE,\r\n&qstats, sizeof(qstats));\r\nreturn 0;\r\n}\r\nint\r\ngnet_stats_copy_app(struct gnet_dump *d, void *st, int len)\r\n{\r\nif (d->compat_xstats) {\r\nd->xstats = st;\r\nd->xstats_len = len;\r\n}\r\nif (d->tail)\r\nreturn gnet_stats_copy(d, TCA_STATS_APP, st, len);\r\nreturn 0;\r\n}\r\nint\r\ngnet_stats_finish_copy(struct gnet_dump *d)\r\n{\r\nif (d->tail)\r\nd->tail->nla_len = skb_tail_pointer(d->skb) - (u8 *)d->tail;\r\nif (d->compat_tc_stats)\r\nif (gnet_stats_copy(d, d->compat_tc_stats, &d->tc_stats,\r\nsizeof(d->tc_stats)) < 0)\r\nreturn -1;\r\nif (d->compat_xstats && d->xstats) {\r\nif (gnet_stats_copy(d, d->compat_xstats, d->xstats,\r\nd->xstats_len) < 0)\r\nreturn -1;\r\n}\r\nspin_unlock_bh(d->lock);\r\nreturn 0;\r\n}
