void inet_get_local_port_range(struct net *net, int *low, int *high)\r\n{\r\nunsigned int seq;\r\ndo {\r\nseq = read_seqbegin(&net->ipv4.ip_local_ports.lock);\r\n*low = net->ipv4.ip_local_ports.range[0];\r\n*high = net->ipv4.ip_local_ports.range[1];\r\n} while (read_seqretry(&net->ipv4.ip_local_ports.lock, seq));\r\n}\r\nint inet_csk_bind_conflict(const struct sock *sk,\r\nconst struct inet_bind_bucket *tb, bool relax)\r\n{\r\nstruct sock *sk2;\r\nint reuse = sk->sk_reuse;\r\nint reuseport = sk->sk_reuseport;\r\nkuid_t uid = sock_i_uid((struct sock *)sk);\r\nsk_for_each_bound(sk2, &tb->owners) {\r\nif (sk != sk2 &&\r\n!inet_v6_ipv6only(sk2) &&\r\n(!sk->sk_bound_dev_if ||\r\n!sk2->sk_bound_dev_if ||\r\nsk->sk_bound_dev_if == sk2->sk_bound_dev_if)) {\r\nif ((!reuse || !sk2->sk_reuse ||\r\nsk2->sk_state == TCP_LISTEN) &&\r\n(!reuseport || !sk2->sk_reuseport ||\r\n(sk2->sk_state != TCP_TIME_WAIT &&\r\n!uid_eq(uid, sock_i_uid(sk2))))) {\r\nif (!sk2->sk_rcv_saddr || !sk->sk_rcv_saddr ||\r\nsk2->sk_rcv_saddr == sk->sk_rcv_saddr)\r\nbreak;\r\n}\r\nif (!relax && reuse && sk2->sk_reuse &&\r\nsk2->sk_state != TCP_LISTEN) {\r\nif (!sk2->sk_rcv_saddr || !sk->sk_rcv_saddr ||\r\nsk2->sk_rcv_saddr == sk->sk_rcv_saddr)\r\nbreak;\r\n}\r\n}\r\n}\r\nreturn sk2 != NULL;\r\n}\r\nint inet_csk_get_port(struct sock *sk, unsigned short snum)\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\nstruct inet_bind_hashbucket *head;\r\nstruct inet_bind_bucket *tb;\r\nint ret, attempts = 5;\r\nstruct net *net = sock_net(sk);\r\nint smallest_size = -1, smallest_rover;\r\nkuid_t uid = sock_i_uid(sk);\r\nlocal_bh_disable();\r\nif (!snum) {\r\nint remaining, rover, low, high;\r\nagain:\r\ninet_get_local_port_range(net, &low, &high);\r\nremaining = (high - low) + 1;\r\nsmallest_rover = rover = prandom_u32() % remaining + low;\r\nsmallest_size = -1;\r\ndo {\r\nif (inet_is_local_reserved_port(net, rover))\r\ngoto next_nolock;\r\nhead = &hashinfo->bhash[inet_bhashfn(net, rover,\r\nhashinfo->bhash_size)];\r\nspin_lock(&head->lock);\r\ninet_bind_bucket_for_each(tb, &head->chain)\r\nif (net_eq(ib_net(tb), net) && tb->port == rover) {\r\nif (((tb->fastreuse > 0 &&\r\nsk->sk_reuse &&\r\nsk->sk_state != TCP_LISTEN) ||\r\n(tb->fastreuseport > 0 &&\r\nsk->sk_reuseport &&\r\nuid_eq(tb->fastuid, uid))) &&\r\n(tb->num_owners < smallest_size || smallest_size == -1)) {\r\nsmallest_size = tb->num_owners;\r\nsmallest_rover = rover;\r\nif (atomic_read(&hashinfo->bsockets) > (high - low) + 1 &&\r\n!inet_csk(sk)->icsk_af_ops->bind_conflict(sk, tb, false)) {\r\nsnum = smallest_rover;\r\ngoto tb_found;\r\n}\r\n}\r\nif (!inet_csk(sk)->icsk_af_ops->bind_conflict(sk, tb, false)) {\r\nsnum = rover;\r\ngoto tb_found;\r\n}\r\ngoto next;\r\n}\r\nbreak;\r\nnext:\r\nspin_unlock(&head->lock);\r\nnext_nolock:\r\nif (++rover > high)\r\nrover = low;\r\n} while (--remaining > 0);\r\nret = 1;\r\nif (remaining <= 0) {\r\nif (smallest_size != -1) {\r\nsnum = smallest_rover;\r\ngoto have_snum;\r\n}\r\ngoto fail;\r\n}\r\nsnum = rover;\r\n} else {\r\nhave_snum:\r\nhead = &hashinfo->bhash[inet_bhashfn(net, snum,\r\nhashinfo->bhash_size)];\r\nspin_lock(&head->lock);\r\ninet_bind_bucket_for_each(tb, &head->chain)\r\nif (net_eq(ib_net(tb), net) && tb->port == snum)\r\ngoto tb_found;\r\n}\r\ntb = NULL;\r\ngoto tb_not_found;\r\ntb_found:\r\nif (!hlist_empty(&tb->owners)) {\r\nif (sk->sk_reuse == SK_FORCE_REUSE)\r\ngoto success;\r\nif (((tb->fastreuse > 0 &&\r\nsk->sk_reuse && sk->sk_state != TCP_LISTEN) ||\r\n(tb->fastreuseport > 0 &&\r\nsk->sk_reuseport && uid_eq(tb->fastuid, uid))) &&\r\nsmallest_size == -1) {\r\ngoto success;\r\n} else {\r\nret = 1;\r\nif (inet_csk(sk)->icsk_af_ops->bind_conflict(sk, tb, true)) {\r\nif (((sk->sk_reuse && sk->sk_state != TCP_LISTEN) ||\r\n(tb->fastreuseport > 0 &&\r\nsk->sk_reuseport && uid_eq(tb->fastuid, uid))) &&\r\nsmallest_size != -1 && --attempts >= 0) {\r\nspin_unlock(&head->lock);\r\ngoto again;\r\n}\r\ngoto fail_unlock;\r\n}\r\n}\r\n}\r\ntb_not_found:\r\nret = 1;\r\nif (!tb && (tb = inet_bind_bucket_create(hashinfo->bind_bucket_cachep,\r\nnet, head, snum)) == NULL)\r\ngoto fail_unlock;\r\nif (hlist_empty(&tb->owners)) {\r\nif (sk->sk_reuse && sk->sk_state != TCP_LISTEN)\r\ntb->fastreuse = 1;\r\nelse\r\ntb->fastreuse = 0;\r\nif (sk->sk_reuseport) {\r\ntb->fastreuseport = 1;\r\ntb->fastuid = uid;\r\n} else\r\ntb->fastreuseport = 0;\r\n} else {\r\nif (tb->fastreuse &&\r\n(!sk->sk_reuse || sk->sk_state == TCP_LISTEN))\r\ntb->fastreuse = 0;\r\nif (tb->fastreuseport &&\r\n(!sk->sk_reuseport || !uid_eq(tb->fastuid, uid)))\r\ntb->fastreuseport = 0;\r\n}\r\nsuccess:\r\nif (!inet_csk(sk)->icsk_bind_hash)\r\ninet_bind_hash(sk, tb, snum);\r\nWARN_ON(inet_csk(sk)->icsk_bind_hash != tb);\r\nret = 0;\r\nfail_unlock:\r\nspin_unlock(&head->lock);\r\nfail:\r\nlocal_bh_enable();\r\nreturn ret;\r\n}\r\nstatic int inet_csk_wait_for_connect(struct sock *sk, long timeo)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nDEFINE_WAIT(wait);\r\nint err;\r\nfor (;;) {\r\nprepare_to_wait_exclusive(sk_sleep(sk), &wait,\r\nTASK_INTERRUPTIBLE);\r\nrelease_sock(sk);\r\nif (reqsk_queue_empty(&icsk->icsk_accept_queue))\r\ntimeo = schedule_timeout(timeo);\r\nlock_sock(sk);\r\nerr = 0;\r\nif (!reqsk_queue_empty(&icsk->icsk_accept_queue))\r\nbreak;\r\nerr = -EINVAL;\r\nif (sk->sk_state != TCP_LISTEN)\r\nbreak;\r\nerr = sock_intr_errno(timeo);\r\nif (signal_pending(current))\r\nbreak;\r\nerr = -EAGAIN;\r\nif (!timeo)\r\nbreak;\r\n}\r\nfinish_wait(sk_sleep(sk), &wait);\r\nreturn err;\r\n}\r\nstruct sock *inet_csk_accept(struct sock *sk, int flags, int *err)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\r\nstruct sock *newsk;\r\nstruct request_sock *req;\r\nint error;\r\nlock_sock(sk);\r\nerror = -EINVAL;\r\nif (sk->sk_state != TCP_LISTEN)\r\ngoto out_err;\r\nif (reqsk_queue_empty(queue)) {\r\nlong timeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\r\nerror = -EAGAIN;\r\nif (!timeo)\r\ngoto out_err;\r\nerror = inet_csk_wait_for_connect(sk, timeo);\r\nif (error)\r\ngoto out_err;\r\n}\r\nreq = reqsk_queue_remove(queue);\r\nnewsk = req->sk;\r\nsk_acceptq_removed(sk);\r\nif (sk->sk_protocol == IPPROTO_TCP && queue->fastopenq != NULL) {\r\nspin_lock_bh(&queue->fastopenq->lock);\r\nif (tcp_rsk(req)->listener) {\r\nreq->sk = NULL;\r\nreq = NULL;\r\n}\r\nspin_unlock_bh(&queue->fastopenq->lock);\r\n}\r\nout:\r\nrelease_sock(sk);\r\nif (req)\r\n__reqsk_free(req);\r\nreturn newsk;\r\nout_err:\r\nnewsk = NULL;\r\nreq = NULL;\r\n*err = error;\r\ngoto out;\r\n}\r\nvoid inet_csk_init_xmit_timers(struct sock *sk,\r\nvoid (*retransmit_handler)(unsigned long),\r\nvoid (*delack_handler)(unsigned long),\r\nvoid (*keepalive_handler)(unsigned long))\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nsetup_timer(&icsk->icsk_retransmit_timer, retransmit_handler,\r\n(unsigned long)sk);\r\nsetup_timer(&icsk->icsk_delack_timer, delack_handler,\r\n(unsigned long)sk);\r\nsetup_timer(&sk->sk_timer, keepalive_handler, (unsigned long)sk);\r\nicsk->icsk_pending = icsk->icsk_ack.pending = 0;\r\n}\r\nvoid inet_csk_clear_xmit_timers(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nicsk->icsk_pending = icsk->icsk_ack.pending = icsk->icsk_ack.blocked = 0;\r\nsk_stop_timer(sk, &icsk->icsk_retransmit_timer);\r\nsk_stop_timer(sk, &icsk->icsk_delack_timer);\r\nsk_stop_timer(sk, &sk->sk_timer);\r\n}\r\nvoid inet_csk_delete_keepalive_timer(struct sock *sk)\r\n{\r\nsk_stop_timer(sk, &sk->sk_timer);\r\n}\r\nvoid inet_csk_reset_keepalive_timer(struct sock *sk, unsigned long len)\r\n{\r\nsk_reset_timer(sk, &sk->sk_timer, jiffies + len);\r\n}\r\nstruct dst_entry *inet_csk_route_req(struct sock *sk,\r\nstruct flowi4 *fl4,\r\nconst struct request_sock *req)\r\n{\r\nstruct rtable *rt;\r\nconst struct inet_request_sock *ireq = inet_rsk(req);\r\nstruct ip_options_rcu *opt = inet_rsk(req)->opt;\r\nstruct net *net = sock_net(sk);\r\nint flags = inet_sk_flowi_flags(sk);\r\nflowi4_init_output(fl4, sk->sk_bound_dev_if, ireq->ir_mark,\r\nRT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\r\nsk->sk_protocol,\r\nflags,\r\n(opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\r\nireq->ir_loc_addr, ireq->ir_rmt_port, inet_sk(sk)->inet_sport);\r\nsecurity_req_classify_flow(req, flowi4_to_flowi(fl4));\r\nrt = ip_route_output_flow(net, fl4, sk);\r\nif (IS_ERR(rt))\r\ngoto no_route;\r\nif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\r\ngoto route_err;\r\nreturn &rt->dst;\r\nroute_err:\r\nip_rt_put(rt);\r\nno_route:\r\nIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\r\nreturn NULL;\r\n}\r\nstruct dst_entry *inet_csk_route_child_sock(struct sock *sk,\r\nstruct sock *newsk,\r\nconst struct request_sock *req)\r\n{\r\nconst struct inet_request_sock *ireq = inet_rsk(req);\r\nstruct inet_sock *newinet = inet_sk(newsk);\r\nstruct ip_options_rcu *opt;\r\nstruct net *net = sock_net(sk);\r\nstruct flowi4 *fl4;\r\nstruct rtable *rt;\r\nfl4 = &newinet->cork.fl.u.ip4;\r\nrcu_read_lock();\r\nopt = rcu_dereference(newinet->inet_opt);\r\nflowi4_init_output(fl4, sk->sk_bound_dev_if, inet_rsk(req)->ir_mark,\r\nRT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\r\nsk->sk_protocol, inet_sk_flowi_flags(sk),\r\n(opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\r\nireq->ir_loc_addr, ireq->ir_rmt_port, inet_sk(sk)->inet_sport);\r\nsecurity_req_classify_flow(req, flowi4_to_flowi(fl4));\r\nrt = ip_route_output_flow(net, fl4, sk);\r\nif (IS_ERR(rt))\r\ngoto no_route;\r\nif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\r\ngoto route_err;\r\nrcu_read_unlock();\r\nreturn &rt->dst;\r\nroute_err:\r\nip_rt_put(rt);\r\nno_route:\r\nrcu_read_unlock();\r\nIP_INC_STATS_BH(net, IPSTATS_MIB_OUTNOROUTES);\r\nreturn NULL;\r\n}\r\nstatic inline u32 inet_synq_hash(const __be32 raddr, const __be16 rport,\r\nconst u32 rnd, const u32 synq_hsize)\r\n{\r\nreturn jhash_2words((__force u32)raddr, (__force u32)rport, rnd) & (synq_hsize - 1);\r\n}\r\nstruct request_sock *inet_csk_search_req(const struct sock *sk,\r\nstruct request_sock ***prevp,\r\nconst __be16 rport, const __be32 raddr,\r\nconst __be32 laddr)\r\n{\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct listen_sock *lopt = icsk->icsk_accept_queue.listen_opt;\r\nstruct request_sock *req, **prev;\r\nfor (prev = &lopt->syn_table[inet_synq_hash(raddr, rport, lopt->hash_rnd,\r\nlopt->nr_table_entries)];\r\n(req = *prev) != NULL;\r\nprev = &req->dl_next) {\r\nconst struct inet_request_sock *ireq = inet_rsk(req);\r\nif (ireq->ir_rmt_port == rport &&\r\nireq->ir_rmt_addr == raddr &&\r\nireq->ir_loc_addr == laddr &&\r\nAF_INET_FAMILY(req->rsk_ops->family)) {\r\nWARN_ON(req->sk);\r\n*prevp = prev;\r\nbreak;\r\n}\r\n}\r\nreturn req;\r\n}\r\nvoid inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,\r\nunsigned long timeout)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct listen_sock *lopt = icsk->icsk_accept_queue.listen_opt;\r\nconst u32 h = inet_synq_hash(inet_rsk(req)->ir_rmt_addr,\r\ninet_rsk(req)->ir_rmt_port,\r\nlopt->hash_rnd, lopt->nr_table_entries);\r\nreqsk_queue_hash_req(&icsk->icsk_accept_queue, h, req, timeout);\r\ninet_csk_reqsk_queue_added(sk, timeout);\r\n}\r\nstatic inline void syn_ack_recalc(struct request_sock *req, const int thresh,\r\nconst int max_retries,\r\nconst u8 rskq_defer_accept,\r\nint *expire, int *resend)\r\n{\r\nif (!rskq_defer_accept) {\r\n*expire = req->num_timeout >= thresh;\r\n*resend = 1;\r\nreturn;\r\n}\r\n*expire = req->num_timeout >= thresh &&\r\n(!inet_rsk(req)->acked || req->num_timeout >= max_retries);\r\n*resend = !inet_rsk(req)->acked ||\r\nreq->num_timeout >= rskq_defer_accept - 1;\r\n}\r\nint inet_rtx_syn_ack(struct sock *parent, struct request_sock *req)\r\n{\r\nint err = req->rsk_ops->rtx_syn_ack(parent, req);\r\nif (!err)\r\nreq->num_retrans++;\r\nreturn err;\r\n}\r\nvoid inet_csk_reqsk_queue_prune(struct sock *parent,\r\nconst unsigned long interval,\r\nconst unsigned long timeout,\r\nconst unsigned long max_rto)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(parent);\r\nstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\r\nstruct listen_sock *lopt = queue->listen_opt;\r\nint max_retries = icsk->icsk_syn_retries ? : sysctl_tcp_synack_retries;\r\nint thresh = max_retries;\r\nunsigned long now = jiffies;\r\nstruct request_sock **reqp, *req;\r\nint i, budget;\r\nif (lopt == NULL || lopt->qlen == 0)\r\nreturn;\r\nif (lopt->qlen>>(lopt->max_qlen_log-1)) {\r\nint young = (lopt->qlen_young<<1);\r\nwhile (thresh > 2) {\r\nif (lopt->qlen < young)\r\nbreak;\r\nthresh--;\r\nyoung <<= 1;\r\n}\r\n}\r\nif (queue->rskq_defer_accept)\r\nmax_retries = queue->rskq_defer_accept;\r\nbudget = 2 * (lopt->nr_table_entries / (timeout / interval));\r\ni = lopt->clock_hand;\r\ndo {\r\nreqp=&lopt->syn_table[i];\r\nwhile ((req = *reqp) != NULL) {\r\nif (time_after_eq(now, req->expires)) {\r\nint expire = 0, resend = 0;\r\nsyn_ack_recalc(req, thresh, max_retries,\r\nqueue->rskq_defer_accept,\r\n&expire, &resend);\r\nreq->rsk_ops->syn_ack_timeout(parent, req);\r\nif (!expire &&\r\n(!resend ||\r\n!inet_rtx_syn_ack(parent, req) ||\r\ninet_rsk(req)->acked)) {\r\nunsigned long timeo;\r\nif (req->num_timeout++ == 0)\r\nlopt->qlen_young--;\r\ntimeo = min(timeout << req->num_timeout,\r\nmax_rto);\r\nreq->expires = now + timeo;\r\nreqp = &req->dl_next;\r\ncontinue;\r\n}\r\ninet_csk_reqsk_queue_unlink(parent, req, reqp);\r\nreqsk_queue_removed(queue, req);\r\nreqsk_free(req);\r\ncontinue;\r\n}\r\nreqp = &req->dl_next;\r\n}\r\ni = (i + 1) & (lopt->nr_table_entries - 1);\r\n} while (--budget > 0);\r\nlopt->clock_hand = i;\r\nif (lopt->qlen)\r\ninet_csk_reset_keepalive_timer(parent, interval);\r\n}\r\nstruct sock *inet_csk_clone_lock(const struct sock *sk,\r\nconst struct request_sock *req,\r\nconst gfp_t priority)\r\n{\r\nstruct sock *newsk = sk_clone_lock(sk, priority);\r\nif (newsk != NULL) {\r\nstruct inet_connection_sock *newicsk = inet_csk(newsk);\r\nnewsk->sk_state = TCP_SYN_RECV;\r\nnewicsk->icsk_bind_hash = NULL;\r\ninet_sk(newsk)->inet_dport = inet_rsk(req)->ir_rmt_port;\r\ninet_sk(newsk)->inet_num = inet_rsk(req)->ir_num;\r\ninet_sk(newsk)->inet_sport = htons(inet_rsk(req)->ir_num);\r\nnewsk->sk_write_space = sk_stream_write_space;\r\nnewsk->sk_mark = inet_rsk(req)->ir_mark;\r\nnewicsk->icsk_retransmits = 0;\r\nnewicsk->icsk_backoff = 0;\r\nnewicsk->icsk_probes_out = 0;\r\nmemset(&newicsk->icsk_accept_queue, 0, sizeof(newicsk->icsk_accept_queue));\r\nsecurity_inet_csk_clone(newsk, req);\r\n}\r\nreturn newsk;\r\n}\r\nvoid inet_csk_destroy_sock(struct sock *sk)\r\n{\r\nWARN_ON(sk->sk_state != TCP_CLOSE);\r\nWARN_ON(!sock_flag(sk, SOCK_DEAD));\r\nWARN_ON(!sk_unhashed(sk));\r\nWARN_ON(inet_sk(sk)->inet_num && !inet_csk(sk)->icsk_bind_hash);\r\nsk->sk_prot->destroy(sk);\r\nsk_stream_kill_queues(sk);\r\nxfrm_sk_free_policy(sk);\r\nsk_refcnt_debug_release(sk);\r\npercpu_counter_dec(sk->sk_prot->orphan_count);\r\nsock_put(sk);\r\n}\r\nvoid inet_csk_prepare_forced_close(struct sock *sk)\r\n__releases(&sk->sk_lock.slock\r\nint inet_csk_listen_start(struct sock *sk, const int nr_table_entries)\r\n{\r\nstruct inet_sock *inet = inet_sk(sk);\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nint rc = reqsk_queue_alloc(&icsk->icsk_accept_queue, nr_table_entries);\r\nif (rc != 0)\r\nreturn rc;\r\nsk->sk_max_ack_backlog = 0;\r\nsk->sk_ack_backlog = 0;\r\ninet_csk_delack_init(sk);\r\nsk->sk_state = TCP_LISTEN;\r\nif (!sk->sk_prot->get_port(sk, inet->inet_num)) {\r\ninet->inet_sport = htons(inet->inet_num);\r\nsk_dst_reset(sk);\r\nsk->sk_prot->hash(sk);\r\nreturn 0;\r\n}\r\nsk->sk_state = TCP_CLOSE;\r\n__reqsk_queue_destroy(&icsk->icsk_accept_queue);\r\nreturn -EADDRINUSE;\r\n}\r\nvoid inet_csk_listen_stop(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\r\nstruct request_sock *acc_req;\r\nstruct request_sock *req;\r\ninet_csk_delete_keepalive_timer(sk);\r\nacc_req = reqsk_queue_yank_acceptq(queue);\r\nreqsk_queue_destroy(queue);\r\nwhile ((req = acc_req) != NULL) {\r\nstruct sock *child = req->sk;\r\nacc_req = req->dl_next;\r\nlocal_bh_disable();\r\nbh_lock_sock(child);\r\nWARN_ON(sock_owned_by_user(child));\r\nsock_hold(child);\r\nsk->sk_prot->disconnect(child, O_NONBLOCK);\r\nsock_orphan(child);\r\npercpu_counter_inc(sk->sk_prot->orphan_count);\r\nif (sk->sk_protocol == IPPROTO_TCP && tcp_rsk(req)->listener) {\r\nBUG_ON(tcp_sk(child)->fastopen_rsk != req);\r\nBUG_ON(sk != tcp_rsk(req)->listener);\r\ntcp_sk(child)->fastopen_rsk = NULL;\r\nsock_put(sk);\r\n}\r\ninet_csk_destroy_sock(child);\r\nbh_unlock_sock(child);\r\nlocal_bh_enable();\r\nsock_put(child);\r\nsk_acceptq_removed(sk);\r\n__reqsk_free(req);\r\n}\r\nif (queue->fastopenq != NULL) {\r\nspin_lock_bh(&queue->fastopenq->lock);\r\nacc_req = queue->fastopenq->rskq_rst_head;\r\nqueue->fastopenq->rskq_rst_head = NULL;\r\nspin_unlock_bh(&queue->fastopenq->lock);\r\nwhile ((req = acc_req) != NULL) {\r\nacc_req = req->dl_next;\r\n__reqsk_free(req);\r\n}\r\n}\r\nWARN_ON(sk->sk_ack_backlog);\r\n}\r\nvoid inet_csk_addr2sockaddr(struct sock *sk, struct sockaddr *uaddr)\r\n{\r\nstruct sockaddr_in *sin = (struct sockaddr_in *)uaddr;\r\nconst struct inet_sock *inet = inet_sk(sk);\r\nsin->sin_family = AF_INET;\r\nsin->sin_addr.s_addr = inet->inet_daddr;\r\nsin->sin_port = inet->inet_dport;\r\n}\r\nint inet_csk_compat_getsockopt(struct sock *sk, int level, int optname,\r\nchar __user *optval, int __user *optlen)\r\n{\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nif (icsk->icsk_af_ops->compat_getsockopt != NULL)\r\nreturn icsk->icsk_af_ops->compat_getsockopt(sk, level, optname,\r\noptval, optlen);\r\nreturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\r\noptval, optlen);\r\n}\r\nint inet_csk_compat_setsockopt(struct sock *sk, int level, int optname,\r\nchar __user *optval, unsigned int optlen)\r\n{\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nif (icsk->icsk_af_ops->compat_setsockopt != NULL)\r\nreturn icsk->icsk_af_ops->compat_setsockopt(sk, level, optname,\r\noptval, optlen);\r\nreturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\r\noptval, optlen);\r\n}\r\nstatic struct dst_entry *inet_csk_rebuild_route(struct sock *sk, struct flowi *fl)\r\n{\r\nconst struct inet_sock *inet = inet_sk(sk);\r\nconst struct ip_options_rcu *inet_opt;\r\n__be32 daddr = inet->inet_daddr;\r\nstruct flowi4 *fl4;\r\nstruct rtable *rt;\r\nrcu_read_lock();\r\ninet_opt = rcu_dereference(inet->inet_opt);\r\nif (inet_opt && inet_opt->opt.srr)\r\ndaddr = inet_opt->opt.faddr;\r\nfl4 = &fl->u.ip4;\r\nrt = ip_route_output_ports(sock_net(sk), fl4, sk, daddr,\r\ninet->inet_saddr, inet->inet_dport,\r\ninet->inet_sport, sk->sk_protocol,\r\nRT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\r\nif (IS_ERR(rt))\r\nrt = NULL;\r\nif (rt)\r\nsk_setup_caps(sk, &rt->dst);\r\nrcu_read_unlock();\r\nreturn &rt->dst;\r\n}\r\nstruct dst_entry *inet_csk_update_pmtu(struct sock *sk, u32 mtu)\r\n{\r\nstruct dst_entry *dst = __sk_dst_check(sk, 0);\r\nstruct inet_sock *inet = inet_sk(sk);\r\nif (!dst) {\r\ndst = inet_csk_rebuild_route(sk, &inet->cork.fl);\r\nif (!dst)\r\ngoto out;\r\n}\r\ndst->ops->update_pmtu(dst, sk, NULL, mtu);\r\ndst = __sk_dst_check(sk, 0);\r\nif (!dst)\r\ndst = inet_csk_rebuild_route(sk, &inet->cork.fl);\r\nout:\r\nreturn dst;\r\n}
