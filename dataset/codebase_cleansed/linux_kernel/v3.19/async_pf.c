static inline void kvm_async_page_present_sync(struct kvm_vcpu *vcpu,\r\nstruct kvm_async_pf *work)\r\n{\r\n#ifdef CONFIG_KVM_ASYNC_PF_SYNC\r\nkvm_arch_async_page_present(vcpu, work);\r\n#endif\r\n}\r\nstatic inline void kvm_async_page_present_async(struct kvm_vcpu *vcpu,\r\nstruct kvm_async_pf *work)\r\n{\r\n#ifndef CONFIG_KVM_ASYNC_PF_SYNC\r\nkvm_arch_async_page_present(vcpu, work);\r\n#endif\r\n}\r\nint kvm_async_pf_init(void)\r\n{\r\nasync_pf_cache = KMEM_CACHE(kvm_async_pf, 0);\r\nif (!async_pf_cache)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid kvm_async_pf_deinit(void)\r\n{\r\nif (async_pf_cache)\r\nkmem_cache_destroy(async_pf_cache);\r\nasync_pf_cache = NULL;\r\n}\r\nvoid kvm_async_pf_vcpu_init(struct kvm_vcpu *vcpu)\r\n{\r\nINIT_LIST_HEAD(&vcpu->async_pf.done);\r\nINIT_LIST_HEAD(&vcpu->async_pf.queue);\r\nspin_lock_init(&vcpu->async_pf.lock);\r\n}\r\nstatic void async_pf_execute(struct work_struct *work)\r\n{\r\nstruct kvm_async_pf *apf =\r\ncontainer_of(work, struct kvm_async_pf, work);\r\nstruct mm_struct *mm = apf->mm;\r\nstruct kvm_vcpu *vcpu = apf->vcpu;\r\nunsigned long addr = apf->addr;\r\ngva_t gva = apf->gva;\r\nmight_sleep();\r\nkvm_get_user_page_io(NULL, mm, addr, 1, NULL);\r\nkvm_async_page_present_sync(vcpu, apf);\r\nspin_lock(&vcpu->async_pf.lock);\r\nlist_add_tail(&apf->link, &vcpu->async_pf.done);\r\nspin_unlock(&vcpu->async_pf.lock);\r\ntrace_kvm_async_pf_completed(addr, gva);\r\nif (waitqueue_active(&vcpu->wq))\r\nwake_up_interruptible(&vcpu->wq);\r\nmmput(mm);\r\nkvm_put_kvm(vcpu->kvm);\r\n}\r\nvoid kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)\r\n{\r\nwhile (!list_empty(&vcpu->async_pf.queue)) {\r\nstruct kvm_async_pf *work =\r\nlist_entry(vcpu->async_pf.queue.next,\r\ntypeof(*work), queue);\r\nlist_del(&work->queue);\r\n#ifdef CONFIG_KVM_ASYNC_PF_SYNC\r\nflush_work(&work->work);\r\n#else\r\nif (cancel_work_sync(&work->work)) {\r\nmmput(work->mm);\r\nkvm_put_kvm(vcpu->kvm);\r\nkmem_cache_free(async_pf_cache, work);\r\n}\r\n#endif\r\n}\r\nspin_lock(&vcpu->async_pf.lock);\r\nwhile (!list_empty(&vcpu->async_pf.done)) {\r\nstruct kvm_async_pf *work =\r\nlist_entry(vcpu->async_pf.done.next,\r\ntypeof(*work), link);\r\nlist_del(&work->link);\r\nkmem_cache_free(async_pf_cache, work);\r\n}\r\nspin_unlock(&vcpu->async_pf.lock);\r\nvcpu->async_pf.queued = 0;\r\n}\r\nvoid kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_async_pf *work;\r\nwhile (!list_empty_careful(&vcpu->async_pf.done) &&\r\nkvm_arch_can_inject_async_page_present(vcpu)) {\r\nspin_lock(&vcpu->async_pf.lock);\r\nwork = list_first_entry(&vcpu->async_pf.done, typeof(*work),\r\nlink);\r\nlist_del(&work->link);\r\nspin_unlock(&vcpu->async_pf.lock);\r\nkvm_arch_async_page_ready(vcpu, work);\r\nkvm_async_page_present_async(vcpu, work);\r\nlist_del(&work->queue);\r\nvcpu->async_pf.queued--;\r\nkmem_cache_free(async_pf_cache, work);\r\n}\r\n}\r\nint kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, unsigned long hva,\r\nstruct kvm_arch_async_pf *arch)\r\n{\r\nstruct kvm_async_pf *work;\r\nif (vcpu->async_pf.queued >= ASYNC_PF_PER_VCPU)\r\nreturn 0;\r\nwork = kmem_cache_zalloc(async_pf_cache, GFP_NOWAIT);\r\nif (!work)\r\nreturn 0;\r\nwork->wakeup_all = false;\r\nwork->vcpu = vcpu;\r\nwork->gva = gva;\r\nwork->addr = hva;\r\nwork->arch = *arch;\r\nwork->mm = current->mm;\r\natomic_inc(&work->mm->mm_users);\r\nkvm_get_kvm(work->vcpu->kvm);\r\nif (unlikely(kvm_is_error_hva(work->addr)))\r\ngoto retry_sync;\r\nINIT_WORK(&work->work, async_pf_execute);\r\nif (!schedule_work(&work->work))\r\ngoto retry_sync;\r\nlist_add_tail(&work->queue, &vcpu->async_pf.queue);\r\nvcpu->async_pf.queued++;\r\nkvm_arch_async_page_not_present(vcpu, work);\r\nreturn 1;\r\nretry_sync:\r\nkvm_put_kvm(work->vcpu->kvm);\r\nmmput(work->mm);\r\nkmem_cache_free(async_pf_cache, work);\r\nreturn 0;\r\n}\r\nint kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_async_pf *work;\r\nif (!list_empty_careful(&vcpu->async_pf.done))\r\nreturn 0;\r\nwork = kmem_cache_zalloc(async_pf_cache, GFP_ATOMIC);\r\nif (!work)\r\nreturn -ENOMEM;\r\nwork->wakeup_all = true;\r\nINIT_LIST_HEAD(&work->queue);\r\nspin_lock(&vcpu->async_pf.lock);\r\nlist_add_tail(&work->link, &vcpu->async_pf.done);\r\nspin_unlock(&vcpu->async_pf.lock);\r\nvcpu->async_pf.queued++;\r\nreturn 0;\r\n}
