static int tce_iommu_enable(struct tce_container *container)\r\n{\r\nint ret = 0;\r\nunsigned long locked, lock_limit, npages;\r\nstruct iommu_table *tbl = container->tbl;\r\nif (!container->tbl)\r\nreturn -ENXIO;\r\nif (!current->mm)\r\nreturn -ESRCH;\r\nif (container->enabled)\r\nreturn -EBUSY;\r\ndown_write(&current->mm->mmap_sem);\r\nnpages = (tbl->it_size << IOMMU_PAGE_SHIFT_4K) >> PAGE_SHIFT;\r\nlocked = current->mm->locked_vm + npages;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif (locked > lock_limit && !capable(CAP_IPC_LOCK)) {\r\npr_warn("RLIMIT_MEMLOCK (%ld) exceeded\n",\r\nrlimit(RLIMIT_MEMLOCK));\r\nret = -ENOMEM;\r\n} else {\r\ncurrent->mm->locked_vm += npages;\r\ncontainer->enabled = true;\r\n}\r\nup_write(&current->mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic void tce_iommu_disable(struct tce_container *container)\r\n{\r\nif (!container->enabled)\r\nreturn;\r\ncontainer->enabled = false;\r\nif (!container->tbl || !current->mm)\r\nreturn;\r\ndown_write(&current->mm->mmap_sem);\r\ncurrent->mm->locked_vm -= (container->tbl->it_size <<\r\nIOMMU_PAGE_SHIFT_4K) >> PAGE_SHIFT;\r\nup_write(&current->mm->mmap_sem);\r\n}\r\nstatic void *tce_iommu_open(unsigned long arg)\r\n{\r\nstruct tce_container *container;\r\nif (arg != VFIO_SPAPR_TCE_IOMMU) {\r\npr_err("tce_vfio: Wrong IOMMU type\n");\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\ncontainer = kzalloc(sizeof(*container), GFP_KERNEL);\r\nif (!container)\r\nreturn ERR_PTR(-ENOMEM);\r\nmutex_init(&container->lock);\r\nreturn container;\r\n}\r\nstatic void tce_iommu_release(void *iommu_data)\r\n{\r\nstruct tce_container *container = iommu_data;\r\nWARN_ON(container->tbl && !container->tbl->it_group);\r\ntce_iommu_disable(container);\r\nif (container->tbl && container->tbl->it_group)\r\ntce_iommu_detach_group(iommu_data, container->tbl->it_group);\r\nmutex_destroy(&container->lock);\r\nkfree(container);\r\n}\r\nstatic long tce_iommu_ioctl(void *iommu_data,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nstruct tce_container *container = iommu_data;\r\nunsigned long minsz;\r\nlong ret;\r\nswitch (cmd) {\r\ncase VFIO_CHECK_EXTENSION:\r\nswitch (arg) {\r\ncase VFIO_SPAPR_TCE_IOMMU:\r\nret = 1;\r\nbreak;\r\ndefault:\r\nret = vfio_spapr_iommu_eeh_ioctl(NULL, cmd, arg);\r\nbreak;\r\n}\r\nreturn (ret < 0) ? 0 : ret;\r\ncase VFIO_IOMMU_SPAPR_TCE_GET_INFO: {\r\nstruct vfio_iommu_spapr_tce_info info;\r\nstruct iommu_table *tbl = container->tbl;\r\nif (WARN_ON(!tbl))\r\nreturn -ENXIO;\r\nminsz = offsetofend(struct vfio_iommu_spapr_tce_info,\r\ndma32_window_size);\r\nif (copy_from_user(&info, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (info.argsz < minsz)\r\nreturn -EINVAL;\r\ninfo.dma32_window_start = tbl->it_offset << IOMMU_PAGE_SHIFT_4K;\r\ninfo.dma32_window_size = tbl->it_size << IOMMU_PAGE_SHIFT_4K;\r\ninfo.flags = 0;\r\nif (copy_to_user((void __user *)arg, &info, minsz))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\ncase VFIO_IOMMU_MAP_DMA: {\r\nstruct vfio_iommu_type1_dma_map param;\r\nstruct iommu_table *tbl = container->tbl;\r\nunsigned long tce, i;\r\nif (!tbl)\r\nreturn -ENXIO;\r\nBUG_ON(!tbl->it_group);\r\nminsz = offsetofend(struct vfio_iommu_type1_dma_map, size);\r\nif (copy_from_user(&param, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (param.argsz < minsz)\r\nreturn -EINVAL;\r\nif (param.flags & ~(VFIO_DMA_MAP_FLAG_READ |\r\nVFIO_DMA_MAP_FLAG_WRITE))\r\nreturn -EINVAL;\r\nif ((param.size & ~IOMMU_PAGE_MASK_4K) ||\r\n(param.vaddr & ~IOMMU_PAGE_MASK_4K))\r\nreturn -EINVAL;\r\ntce = param.vaddr;\r\nif (param.flags & VFIO_DMA_MAP_FLAG_READ)\r\ntce |= TCE_PCI_READ;\r\nif (param.flags & VFIO_DMA_MAP_FLAG_WRITE)\r\ntce |= TCE_PCI_WRITE;\r\nret = iommu_tce_put_param_check(tbl, param.iova, tce);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < (param.size >> IOMMU_PAGE_SHIFT_4K); ++i) {\r\nret = iommu_put_tce_user_mode(tbl,\r\n(param.iova >> IOMMU_PAGE_SHIFT_4K) + i,\r\ntce);\r\nif (ret)\r\nbreak;\r\ntce += IOMMU_PAGE_SIZE_4K;\r\n}\r\nif (ret)\r\niommu_clear_tces_and_put_pages(tbl,\r\nparam.iova >> IOMMU_PAGE_SHIFT_4K, i);\r\niommu_flush_tce(tbl);\r\nreturn ret;\r\n}\r\ncase VFIO_IOMMU_UNMAP_DMA: {\r\nstruct vfio_iommu_type1_dma_unmap param;\r\nstruct iommu_table *tbl = container->tbl;\r\nif (WARN_ON(!tbl))\r\nreturn -ENXIO;\r\nminsz = offsetofend(struct vfio_iommu_type1_dma_unmap,\r\nsize);\r\nif (copy_from_user(&param, (void __user *)arg, minsz))\r\nreturn -EFAULT;\r\nif (param.argsz < minsz)\r\nreturn -EINVAL;\r\nif (param.flags)\r\nreturn -EINVAL;\r\nif (param.size & ~IOMMU_PAGE_MASK_4K)\r\nreturn -EINVAL;\r\nret = iommu_tce_clear_param_check(tbl, param.iova, 0,\r\nparam.size >> IOMMU_PAGE_SHIFT_4K);\r\nif (ret)\r\nreturn ret;\r\nret = iommu_clear_tces_and_put_pages(tbl,\r\nparam.iova >> IOMMU_PAGE_SHIFT_4K,\r\nparam.size >> IOMMU_PAGE_SHIFT_4K);\r\niommu_flush_tce(tbl);\r\nreturn ret;\r\n}\r\ncase VFIO_IOMMU_ENABLE:\r\nmutex_lock(&container->lock);\r\nret = tce_iommu_enable(container);\r\nmutex_unlock(&container->lock);\r\nreturn ret;\r\ncase VFIO_IOMMU_DISABLE:\r\nmutex_lock(&container->lock);\r\ntce_iommu_disable(container);\r\nmutex_unlock(&container->lock);\r\nreturn 0;\r\ncase VFIO_EEH_PE_OP:\r\nif (!container->tbl || !container->tbl->it_group)\r\nreturn -ENODEV;\r\nreturn vfio_spapr_iommu_eeh_ioctl(container->tbl->it_group,\r\ncmd, arg);\r\n}\r\nreturn -ENOTTY;\r\n}\r\nstatic int tce_iommu_attach_group(void *iommu_data,\r\nstruct iommu_group *iommu_group)\r\n{\r\nint ret;\r\nstruct tce_container *container = iommu_data;\r\nstruct iommu_table *tbl = iommu_group_get_iommudata(iommu_group);\r\nBUG_ON(!tbl);\r\nmutex_lock(&container->lock);\r\nif (container->tbl) {\r\npr_warn("tce_vfio: Only one group per IOMMU container is allowed, existing id=%d, attaching id=%d\n",\r\niommu_group_id(container->tbl->it_group),\r\niommu_group_id(iommu_group));\r\nret = -EBUSY;\r\n} else if (container->enabled) {\r\npr_err("tce_vfio: attaching group #%u to enabled container\n",\r\niommu_group_id(iommu_group));\r\nret = -EBUSY;\r\n} else {\r\nret = iommu_take_ownership(tbl);\r\nif (!ret)\r\ncontainer->tbl = tbl;\r\n}\r\nmutex_unlock(&container->lock);\r\nreturn ret;\r\n}\r\nstatic void tce_iommu_detach_group(void *iommu_data,\r\nstruct iommu_group *iommu_group)\r\n{\r\nstruct tce_container *container = iommu_data;\r\nstruct iommu_table *tbl = iommu_group_get_iommudata(iommu_group);\r\nBUG_ON(!tbl);\r\nmutex_lock(&container->lock);\r\nif (tbl != container->tbl) {\r\npr_warn("tce_vfio: detaching group #%u, expected group is #%u\n",\r\niommu_group_id(iommu_group),\r\niommu_group_id(tbl->it_group));\r\n} else {\r\nif (container->enabled) {\r\npr_warn("tce_vfio: detaching group #%u from enabled container, forcing disable\n",\r\niommu_group_id(tbl->it_group));\r\ntce_iommu_disable(container);\r\n}\r\ncontainer->tbl = NULL;\r\niommu_release_ownership(tbl);\r\n}\r\nmutex_unlock(&container->lock);\r\n}\r\nstatic int __init tce_iommu_init(void)\r\n{\r\nreturn vfio_register_iommu_driver(&tce_iommu_driver_ops);\r\n}\r\nstatic void __exit tce_iommu_cleanup(void)\r\n{\r\nvfio_unregister_iommu_driver(&tce_iommu_driver_ops);\r\n}
