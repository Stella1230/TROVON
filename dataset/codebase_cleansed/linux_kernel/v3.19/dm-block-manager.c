static unsigned __find_holder(struct block_lock *lock,\r\nstruct task_struct *task)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < MAX_HOLDERS; i++)\r\nif (lock->holders[i] == task)\r\nbreak;\r\nBUG_ON(i == MAX_HOLDERS);\r\nreturn i;\r\n}\r\nstatic void __add_holder(struct block_lock *lock, struct task_struct *task)\r\n{\r\nunsigned h = __find_holder(lock, NULL);\r\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\r\nstruct stack_trace *t;\r\n#endif\r\nget_task_struct(task);\r\nlock->holders[h] = task;\r\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\r\nt = lock->traces + h;\r\nt->nr_entries = 0;\r\nt->max_entries = MAX_STACK;\r\nt->entries = lock->entries[h];\r\nt->skip = 2;\r\nsave_stack_trace(t);\r\n#endif\r\n}\r\nstatic void __del_holder(struct block_lock *lock, struct task_struct *task)\r\n{\r\nunsigned h = __find_holder(lock, task);\r\nlock->holders[h] = NULL;\r\nput_task_struct(task);\r\n}\r\nstatic int __check_holder(struct block_lock *lock)\r\n{\r\nunsigned i;\r\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\r\nstatic struct stack_trace t;\r\nstatic stack_entries entries;\r\n#endif\r\nfor (i = 0; i < MAX_HOLDERS; i++) {\r\nif (lock->holders[i] == current) {\r\nDMERR("recursive lock detected in metadata");\r\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\r\nDMERR("previously held here:");\r\nprint_stack_trace(lock->traces + i, 4);\r\nDMERR("subsequent acquisition attempted here:");\r\nt.nr_entries = 0;\r\nt.max_entries = MAX_STACK;\r\nt.entries = entries;\r\nt.skip = 3;\r\nsave_stack_trace(&t);\r\nprint_stack_trace(&t, 4);\r\n#endif\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void __wait(struct waiter *w)\r\n{\r\nfor (;;) {\r\nset_task_state(current, TASK_UNINTERRUPTIBLE);\r\nif (!w->task)\r\nbreak;\r\nschedule();\r\n}\r\nset_task_state(current, TASK_RUNNING);\r\n}\r\nstatic void __wake_waiter(struct waiter *w)\r\n{\r\nstruct task_struct *task;\r\nlist_del(&w->list);\r\ntask = w->task;\r\nsmp_mb();\r\nw->task = NULL;\r\nwake_up_process(task);\r\n}\r\nstatic void __wake_many(struct block_lock *lock)\r\n{\r\nstruct waiter *w, *tmp;\r\nBUG_ON(lock->count < 0);\r\nlist_for_each_entry_safe(w, tmp, &lock->waiters, list) {\r\nif (lock->count >= MAX_HOLDERS)\r\nreturn;\r\nif (w->wants_write) {\r\nif (lock->count > 0)\r\nreturn;\r\nlock->count = -1;\r\n__add_holder(lock, w->task);\r\n__wake_waiter(w);\r\nreturn;\r\n}\r\nlock->count++;\r\n__add_holder(lock, w->task);\r\n__wake_waiter(w);\r\n}\r\n}\r\nstatic void bl_init(struct block_lock *lock)\r\n{\r\nint i;\r\nspin_lock_init(&lock->lock);\r\nlock->count = 0;\r\nINIT_LIST_HEAD(&lock->waiters);\r\nfor (i = 0; i < MAX_HOLDERS; i++)\r\nlock->holders[i] = NULL;\r\n}\r\nstatic int __available_for_read(struct block_lock *lock)\r\n{\r\nreturn lock->count >= 0 &&\r\nlock->count < MAX_HOLDERS &&\r\nlist_empty(&lock->waiters);\r\n}\r\nstatic int bl_down_read(struct block_lock *lock)\r\n{\r\nint r;\r\nstruct waiter w;\r\nspin_lock(&lock->lock);\r\nr = __check_holder(lock);\r\nif (r) {\r\nspin_unlock(&lock->lock);\r\nreturn r;\r\n}\r\nif (__available_for_read(lock)) {\r\nlock->count++;\r\n__add_holder(lock, current);\r\nspin_unlock(&lock->lock);\r\nreturn 0;\r\n}\r\nget_task_struct(current);\r\nw.task = current;\r\nw.wants_write = 0;\r\nlist_add_tail(&w.list, &lock->waiters);\r\nspin_unlock(&lock->lock);\r\n__wait(&w);\r\nput_task_struct(current);\r\nreturn 0;\r\n}\r\nstatic int bl_down_read_nonblock(struct block_lock *lock)\r\n{\r\nint r;\r\nspin_lock(&lock->lock);\r\nr = __check_holder(lock);\r\nif (r)\r\ngoto out;\r\nif (__available_for_read(lock)) {\r\nlock->count++;\r\n__add_holder(lock, current);\r\nr = 0;\r\n} else\r\nr = -EWOULDBLOCK;\r\nout:\r\nspin_unlock(&lock->lock);\r\nreturn r;\r\n}\r\nstatic void bl_up_read(struct block_lock *lock)\r\n{\r\nspin_lock(&lock->lock);\r\nBUG_ON(lock->count <= 0);\r\n__del_holder(lock, current);\r\n--lock->count;\r\nif (!list_empty(&lock->waiters))\r\n__wake_many(lock);\r\nspin_unlock(&lock->lock);\r\n}\r\nstatic int bl_down_write(struct block_lock *lock)\r\n{\r\nint r;\r\nstruct waiter w;\r\nspin_lock(&lock->lock);\r\nr = __check_holder(lock);\r\nif (r) {\r\nspin_unlock(&lock->lock);\r\nreturn r;\r\n}\r\nif (lock->count == 0 && list_empty(&lock->waiters)) {\r\nlock->count = -1;\r\n__add_holder(lock, current);\r\nspin_unlock(&lock->lock);\r\nreturn 0;\r\n}\r\nget_task_struct(current);\r\nw.task = current;\r\nw.wants_write = 1;\r\nlist_add(&w.list, &lock->waiters);\r\nspin_unlock(&lock->lock);\r\n__wait(&w);\r\nput_task_struct(current);\r\nreturn 0;\r\n}\r\nstatic void bl_up_write(struct block_lock *lock)\r\n{\r\nspin_lock(&lock->lock);\r\n__del_holder(lock, current);\r\nlock->count = 0;\r\nif (!list_empty(&lock->waiters))\r\n__wake_many(lock);\r\nspin_unlock(&lock->lock);\r\n}\r\nstatic void report_recursive_bug(dm_block_t b, int r)\r\n{\r\nif (r == -EINVAL)\r\nDMERR("recursive acquisition of block %llu requested.",\r\n(unsigned long long) b);\r\n}\r\nstatic struct dm_buffer *to_buffer(struct dm_block *b)\r\n{\r\nreturn (struct dm_buffer *) b;\r\n}\r\ndm_block_t dm_block_location(struct dm_block *b)\r\n{\r\nreturn dm_bufio_get_block_number(to_buffer(b));\r\n}\r\nvoid *dm_block_data(struct dm_block *b)\r\n{\r\nreturn dm_bufio_get_block_data(to_buffer(b));\r\n}\r\nstatic void dm_block_manager_alloc_callback(struct dm_buffer *buf)\r\n{\r\nstruct buffer_aux *aux = dm_bufio_get_aux_data(buf);\r\naux->validator = NULL;\r\nbl_init(&aux->lock);\r\n}\r\nstatic void dm_block_manager_write_callback(struct dm_buffer *buf)\r\n{\r\nstruct buffer_aux *aux = dm_bufio_get_aux_data(buf);\r\nif (aux->validator) {\r\naux->validator->prepare_for_write(aux->validator, (struct dm_block *) buf,\r\ndm_bufio_get_block_size(dm_bufio_get_client(buf)));\r\n}\r\n}\r\nstruct dm_block_manager *dm_block_manager_create(struct block_device *bdev,\r\nunsigned block_size,\r\nunsigned cache_size,\r\nunsigned max_held_per_thread)\r\n{\r\nint r;\r\nstruct dm_block_manager *bm;\r\nbm = kmalloc(sizeof(*bm), GFP_KERNEL);\r\nif (!bm) {\r\nr = -ENOMEM;\r\ngoto bad;\r\n}\r\nbm->bufio = dm_bufio_client_create(bdev, block_size, max_held_per_thread,\r\nsizeof(struct buffer_aux),\r\ndm_block_manager_alloc_callback,\r\ndm_block_manager_write_callback);\r\nif (IS_ERR(bm->bufio)) {\r\nr = PTR_ERR(bm->bufio);\r\nkfree(bm);\r\ngoto bad;\r\n}\r\nbm->read_only = false;\r\nreturn bm;\r\nbad:\r\nreturn ERR_PTR(r);\r\n}\r\nvoid dm_block_manager_destroy(struct dm_block_manager *bm)\r\n{\r\ndm_bufio_client_destroy(bm->bufio);\r\nkfree(bm);\r\n}\r\nunsigned dm_bm_block_size(struct dm_block_manager *bm)\r\n{\r\nreturn dm_bufio_get_block_size(bm->bufio);\r\n}\r\ndm_block_t dm_bm_nr_blocks(struct dm_block_manager *bm)\r\n{\r\nreturn dm_bufio_get_device_size(bm->bufio);\r\n}\r\nstatic int dm_bm_validate_buffer(struct dm_block_manager *bm,\r\nstruct dm_buffer *buf,\r\nstruct buffer_aux *aux,\r\nstruct dm_block_validator *v)\r\n{\r\nif (unlikely(!aux->validator)) {\r\nint r;\r\nif (!v)\r\nreturn 0;\r\nr = v->check(v, (struct dm_block *) buf, dm_bufio_get_block_size(bm->bufio));\r\nif (unlikely(r)) {\r\nDMERR_LIMIT("%s validator check failed for block %llu", v->name,\r\n(unsigned long long) dm_bufio_get_block_number(buf));\r\nreturn r;\r\n}\r\naux->validator = v;\r\n} else {\r\nif (unlikely(aux->validator != v)) {\r\nDMERR_LIMIT("validator mismatch (old=%s vs new=%s) for block %llu",\r\naux->validator->name, v ? v->name : "NULL",\r\n(unsigned long long) dm_bufio_get_block_number(buf));\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint dm_bm_read_lock(struct dm_block_manager *bm, dm_block_t b,\r\nstruct dm_block_validator *v,\r\nstruct dm_block **result)\r\n{\r\nstruct buffer_aux *aux;\r\nvoid *p;\r\nint r;\r\np = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);\r\nif (unlikely(IS_ERR(p)))\r\nreturn PTR_ERR(p);\r\naux = dm_bufio_get_aux_data(to_buffer(*result));\r\nr = bl_down_read(&aux->lock);\r\nif (unlikely(r)) {\r\ndm_bufio_release(to_buffer(*result));\r\nreport_recursive_bug(b, r);\r\nreturn r;\r\n}\r\naux->write_locked = 0;\r\nr = dm_bm_validate_buffer(bm, to_buffer(*result), aux, v);\r\nif (unlikely(r)) {\r\nbl_up_read(&aux->lock);\r\ndm_bufio_release(to_buffer(*result));\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nint dm_bm_write_lock(struct dm_block_manager *bm,\r\ndm_block_t b, struct dm_block_validator *v,\r\nstruct dm_block **result)\r\n{\r\nstruct buffer_aux *aux;\r\nvoid *p;\r\nint r;\r\nif (bm->read_only)\r\nreturn -EPERM;\r\np = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);\r\nif (unlikely(IS_ERR(p)))\r\nreturn PTR_ERR(p);\r\naux = dm_bufio_get_aux_data(to_buffer(*result));\r\nr = bl_down_write(&aux->lock);\r\nif (r) {\r\ndm_bufio_release(to_buffer(*result));\r\nreport_recursive_bug(b, r);\r\nreturn r;\r\n}\r\naux->write_locked = 1;\r\nr = dm_bm_validate_buffer(bm, to_buffer(*result), aux, v);\r\nif (unlikely(r)) {\r\nbl_up_write(&aux->lock);\r\ndm_bufio_release(to_buffer(*result));\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nint dm_bm_read_try_lock(struct dm_block_manager *bm,\r\ndm_block_t b, struct dm_block_validator *v,\r\nstruct dm_block **result)\r\n{\r\nstruct buffer_aux *aux;\r\nvoid *p;\r\nint r;\r\np = dm_bufio_get(bm->bufio, b, (struct dm_buffer **) result);\r\nif (unlikely(IS_ERR(p)))\r\nreturn PTR_ERR(p);\r\nif (unlikely(!p))\r\nreturn -EWOULDBLOCK;\r\naux = dm_bufio_get_aux_data(to_buffer(*result));\r\nr = bl_down_read_nonblock(&aux->lock);\r\nif (r < 0) {\r\ndm_bufio_release(to_buffer(*result));\r\nreport_recursive_bug(b, r);\r\nreturn r;\r\n}\r\naux->write_locked = 0;\r\nr = dm_bm_validate_buffer(bm, to_buffer(*result), aux, v);\r\nif (unlikely(r)) {\r\nbl_up_read(&aux->lock);\r\ndm_bufio_release(to_buffer(*result));\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nint dm_bm_write_lock_zero(struct dm_block_manager *bm,\r\ndm_block_t b, struct dm_block_validator *v,\r\nstruct dm_block **result)\r\n{\r\nint r;\r\nstruct buffer_aux *aux;\r\nvoid *p;\r\nif (bm->read_only)\r\nreturn -EPERM;\r\np = dm_bufio_new(bm->bufio, b, (struct dm_buffer **) result);\r\nif (unlikely(IS_ERR(p)))\r\nreturn PTR_ERR(p);\r\nmemset(p, 0, dm_bm_block_size(bm));\r\naux = dm_bufio_get_aux_data(to_buffer(*result));\r\nr = bl_down_write(&aux->lock);\r\nif (r) {\r\ndm_bufio_release(to_buffer(*result));\r\nreturn r;\r\n}\r\naux->write_locked = 1;\r\naux->validator = v;\r\nreturn 0;\r\n}\r\nint dm_bm_unlock(struct dm_block *b)\r\n{\r\nstruct buffer_aux *aux;\r\naux = dm_bufio_get_aux_data(to_buffer(b));\r\nif (aux->write_locked) {\r\ndm_bufio_mark_buffer_dirty(to_buffer(b));\r\nbl_up_write(&aux->lock);\r\n} else\r\nbl_up_read(&aux->lock);\r\ndm_bufio_release(to_buffer(b));\r\nreturn 0;\r\n}\r\nint dm_bm_flush(struct dm_block_manager *bm)\r\n{\r\nif (bm->read_only)\r\nreturn -EPERM;\r\nreturn dm_bufio_write_dirty_buffers(bm->bufio);\r\n}\r\nvoid dm_bm_prefetch(struct dm_block_manager *bm, dm_block_t b)\r\n{\r\ndm_bufio_prefetch(bm->bufio, b, 1);\r\n}\r\nvoid dm_bm_set_read_only(struct dm_block_manager *bm)\r\n{\r\nbm->read_only = true;\r\n}\r\nvoid dm_bm_set_read_write(struct dm_block_manager *bm)\r\n{\r\nbm->read_only = false;\r\n}\r\nu32 dm_bm_checksum(const void *data, size_t len, u32 init_xor)\r\n{\r\nreturn crc32c(~(u32) 0, data, len) ^ init_xor;\r\n}
