static inline void dart_tlb_invalidate_all(void)\r\n{\r\nunsigned long l = 0;\r\nunsigned int reg, inv_bit;\r\nunsigned long limit;\r\nunsigned long flags;\r\nspin_lock_irqsave(&invalidate_lock, flags);\r\nDBG("dart: flush\n");\r\nlimit = 0;\r\ninv_bit = dart_is_u4 ? DART_CNTL_U4_FLUSHTLB : DART_CNTL_U3_FLUSHTLB;\r\nretry:\r\nl = 0;\r\nreg = DART_IN(DART_CNTL);\r\nreg |= inv_bit;\r\nDART_OUT(DART_CNTL, reg);\r\nwhile ((DART_IN(DART_CNTL) & inv_bit) && l < (1L << limit))\r\nl++;\r\nif (l == (1L << limit)) {\r\nif (limit < 4) {\r\nlimit++;\r\nreg = DART_IN(DART_CNTL);\r\nreg &= ~inv_bit;\r\nDART_OUT(DART_CNTL, reg);\r\ngoto retry;\r\n} else\r\npanic("DART: TLB did not flush after waiting a long "\r\n"time. Buggy U3 ?");\r\n}\r\nspin_unlock_irqrestore(&invalidate_lock, flags);\r\n}\r\nstatic inline void dart_tlb_invalidate_one(unsigned long bus_rpn)\r\n{\r\nunsigned int reg;\r\nunsigned int l, limit;\r\nunsigned long flags;\r\nspin_lock_irqsave(&invalidate_lock, flags);\r\nreg = DART_CNTL_U4_ENABLE | DART_CNTL_U4_IONE |\r\n(bus_rpn & DART_CNTL_U4_IONE_MASK);\r\nDART_OUT(DART_CNTL, reg);\r\nlimit = 0;\r\nwait_more:\r\nl = 0;\r\nwhile ((DART_IN(DART_CNTL) & DART_CNTL_U4_IONE) && l < (1L << limit)) {\r\nrmb();\r\nl++;\r\n}\r\nif (l == (1L << limit)) {\r\nif (limit < 4) {\r\nlimit++;\r\ngoto wait_more;\r\n} else\r\npanic("DART: TLB did not flush after waiting a long "\r\n"time. Buggy U4 ?");\r\n}\r\nspin_unlock_irqrestore(&invalidate_lock, flags);\r\n}\r\nstatic void dart_flush(struct iommu_table *tbl)\r\n{\r\nmb();\r\nif (dart_dirty) {\r\ndart_tlb_invalidate_all();\r\ndart_dirty = 0;\r\n}\r\n}\r\nstatic int dart_build(struct iommu_table *tbl, long index,\r\nlong npages, unsigned long uaddr,\r\nenum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nunsigned int *dp;\r\nunsigned int rpn;\r\nlong l;\r\nDBG("dart: build at: %lx, %lx, addr: %x\n", index, npages, uaddr);\r\ndp = ((unsigned int*)tbl->it_base) + index;\r\nl = npages;\r\nwhile (l--) {\r\nrpn = __pa(uaddr) >> DART_PAGE_SHIFT;\r\n*(dp++) = DARTMAP_VALID | (rpn & DARTMAP_RPNMASK);\r\nuaddr += DART_PAGE_SIZE;\r\n}\r\nmb();\r\nin_be32((unsigned __iomem *)dp);\r\nmb();\r\nif (dart_is_u4) {\r\nrpn = index;\r\nwhile (npages--)\r\ndart_tlb_invalidate_one(rpn++);\r\n} else {\r\ndart_dirty = 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void dart_free(struct iommu_table *tbl, long index, long npages)\r\n{\r\nunsigned int *dp;\r\nDBG("dart: free at: %lx, %lx\n", index, npages);\r\ndp = ((unsigned int *)tbl->it_base) + index;\r\nwhile (npages--)\r\n*(dp++) = dart_emptyval;\r\n}\r\nstatic int __init dart_init(struct device_node *dart_node)\r\n{\r\nunsigned int i;\r\nunsigned long tmp, base, size;\r\nstruct resource r;\r\nif (dart_tablebase == 0 || dart_tablesize == 0) {\r\nprintk(KERN_INFO "DART: table not allocated, using "\r\n"direct DMA\n");\r\nreturn -ENODEV;\r\n}\r\nif (of_address_to_resource(dart_node, 0, &r))\r\npanic("DART: can't get register base ! ");\r\nflush_dcache_phys_range(dart_tablebase,\r\ndart_tablebase + dart_tablesize);\r\ntmp = memblock_alloc(DART_PAGE_SIZE, DART_PAGE_SIZE);\r\ndart_emptyval = DARTMAP_VALID | ((tmp >> DART_PAGE_SHIFT) &\r\nDARTMAP_RPNMASK);\r\ndart = ioremap(r.start, resource_size(&r));\r\nif (dart == NULL)\r\npanic("DART: Cannot map registers!");\r\ndart_vbase = ioremap(__pa(dart_tablebase), dart_tablesize);\r\nfor (i = 0; i < dart_tablesize/4; i++)\r\ndart_vbase[i] = dart_emptyval;\r\nbase = dart_tablebase >> DART_PAGE_SHIFT;\r\nsize = dart_tablesize >> DART_PAGE_SHIFT;\r\nif (dart_is_u4) {\r\nsize &= DART_SIZE_U4_SIZE_MASK;\r\nDART_OUT(DART_BASE_U4, base);\r\nDART_OUT(DART_SIZE_U4, size);\r\nDART_OUT(DART_CNTL, DART_CNTL_U4_ENABLE);\r\n} else {\r\nsize &= DART_CNTL_U3_SIZE_MASK;\r\nDART_OUT(DART_CNTL,\r\nDART_CNTL_U3_ENABLE |\r\n(base << DART_CNTL_U3_BASE_SHIFT) |\r\n(size << DART_CNTL_U3_SIZE_SHIFT));\r\n}\r\ndart_tlb_invalidate_all();\r\nprintk(KERN_INFO "DART IOMMU initialized for %s type chipset\n",\r\ndart_is_u4 ? "U4" : "U3");\r\nreturn 0;\r\n}\r\nstatic void iommu_table_dart_setup(void)\r\n{\r\niommu_table_dart.it_busno = 0;\r\niommu_table_dart.it_offset = 0;\r\niommu_table_dart.it_size = dart_tablesize / sizeof(u32);\r\niommu_table_dart.it_page_shift = IOMMU_PAGE_SHIFT_4K;\r\niommu_table_dart.it_base = (unsigned long)dart_vbase;\r\niommu_table_dart.it_index = 0;\r\niommu_table_dart.it_blocksize = 1;\r\niommu_init_table(&iommu_table_dart, -1);\r\nset_bit(iommu_table_dart.it_size - 1, iommu_table_dart.it_map);\r\n}\r\nstatic void dma_dev_setup_dart(struct device *dev)\r\n{\r\nif (get_dma_ops(dev) == &dma_direct_ops)\r\nset_dma_offset(dev, DART_U4_BYPASS_BASE);\r\nelse\r\nset_iommu_table_base(dev, &iommu_table_dart);\r\n}\r\nstatic void pci_dma_dev_setup_dart(struct pci_dev *dev)\r\n{\r\ndma_dev_setup_dart(&dev->dev);\r\n}\r\nstatic void pci_dma_bus_setup_dart(struct pci_bus *bus)\r\n{\r\nif (!iommu_table_dart_inited) {\r\niommu_table_dart_inited = 1;\r\niommu_table_dart_setup();\r\n}\r\n}\r\nstatic bool dart_device_on_pcie(struct device *dev)\r\n{\r\nstruct device_node *np = of_node_get(dev->of_node);\r\nwhile(np) {\r\nif (of_device_is_compatible(np, "U4-pcie") ||\r\nof_device_is_compatible(np, "u4-pcie")) {\r\nof_node_put(np);\r\nreturn true;\r\n}\r\nnp = of_get_next_parent(np);\r\n}\r\nreturn false;\r\n}\r\nstatic int dart_dma_set_mask(struct device *dev, u64 dma_mask)\r\n{\r\nif (!dev->dma_mask || !dma_supported(dev, dma_mask))\r\nreturn -EIO;\r\nif (dart_device_on_pcie(dev) && dma_mask >= DMA_BIT_MASK(40)) {\r\ndev_info(dev, "Using 64-bit DMA iommu bypass\n");\r\nset_dma_ops(dev, &dma_direct_ops);\r\n} else {\r\ndev_info(dev, "Using 32-bit DMA via iommu\n");\r\nset_dma_ops(dev, &dma_iommu_ops);\r\n}\r\ndma_dev_setup_dart(dev);\r\n*dev->dma_mask = dma_mask;\r\nreturn 0;\r\n}\r\nvoid __init iommu_init_early_dart(void)\r\n{\r\nstruct device_node *dn;\r\ndn = of_find_compatible_node(NULL, "dart", "u3-dart");\r\nif (dn == NULL) {\r\ndn = of_find_compatible_node(NULL, "dart", "u4-dart");\r\nif (dn == NULL)\r\nreturn;\r\ndart_is_u4 = 1;\r\n}\r\nif (dart_init(dn) != 0)\r\ngoto bail;\r\nppc_md.tce_build = dart_build;\r\nppc_md.tce_free = dart_free;\r\nppc_md.tce_flush = dart_flush;\r\nif (dart_is_u4)\r\nppc_md.dma_set_mask = dart_dma_set_mask;\r\nppc_md.pci_dma_dev_setup = pci_dma_dev_setup_dart;\r\nppc_md.pci_dma_bus_setup = pci_dma_bus_setup_dart;\r\nset_pci_dma_ops(&dma_iommu_ops);\r\nreturn;\r\nbail:\r\nppc_md.pci_dma_dev_setup = NULL;\r\nppc_md.pci_dma_bus_setup = NULL;\r\nset_pci_dma_ops(&dma_direct_ops);\r\n}\r\nstatic void iommu_dart_save(void)\r\n{\r\nmemcpy(dart_copy, dart_vbase, 2*1024*1024);\r\n}\r\nstatic void iommu_dart_restore(void)\r\n{\r\nmemcpy(dart_vbase, dart_copy, 2*1024*1024);\r\ndart_tlb_invalidate_all();\r\n}\r\nstatic int __init iommu_init_late_dart(void)\r\n{\r\nunsigned long tbasepfn;\r\nstruct page *p;\r\nif (!dart_tablebase)\r\nreturn 0;\r\ntbasepfn = __pa(dart_tablebase) >> PAGE_SHIFT;\r\nregister_nosave_region_late(tbasepfn,\r\ntbasepfn + ((1<<24) >> PAGE_SHIFT));\r\np = alloc_pages(GFP_KERNEL, 21 - PAGE_SHIFT);\r\nBUG_ON(!p);\r\ndart_copy = page_address(p);\r\nppc_md.iommu_save = iommu_dart_save;\r\nppc_md.iommu_restore = iommu_dart_restore;\r\nreturn 0;\r\n}\r\nvoid __init alloc_dart_table(void)\r\n{\r\nif (iommu_is_off)\r\nreturn;\r\nif (!iommu_force_on && memblock_end_of_DRAM() <= 0x40000000ull)\r\nreturn;\r\ndart_tablesize = 1UL << 21;\r\ndart_tablebase = (unsigned long)\r\n__va(memblock_alloc_base(1UL<<24, 1UL<<24, 0x80000000L));\r\nkmemleak_no_scan((void *)dart_tablebase);\r\nprintk(KERN_INFO "DART table allocated at: %lx\n", dart_tablebase);\r\n}
