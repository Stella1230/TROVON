static void * r10bio_pool_alloc(gfp_t gfp_flags, void *data)\r\n{\r\nstruct r10conf *conf = data;\r\nint size = offsetof(struct r10bio, devs[conf->copies]);\r\nreturn kzalloc(size, gfp_flags);\r\n}\r\nstatic void r10bio_pool_free(void *r10_bio, void *data)\r\n{\r\nkfree(r10_bio);\r\n}\r\nstatic void * r10buf_pool_alloc(gfp_t gfp_flags, void *data)\r\n{\r\nstruct r10conf *conf = data;\r\nstruct page *page;\r\nstruct r10bio *r10_bio;\r\nstruct bio *bio;\r\nint i, j;\r\nint nalloc;\r\nr10_bio = r10bio_pool_alloc(gfp_flags, conf);\r\nif (!r10_bio)\r\nreturn NULL;\r\nif (test_bit(MD_RECOVERY_SYNC, &conf->mddev->recovery) ||\r\ntest_bit(MD_RECOVERY_RESHAPE, &conf->mddev->recovery))\r\nnalloc = conf->copies;\r\nelse\r\nnalloc = 2;\r\nfor (j = nalloc ; j-- ; ) {\r\nbio = bio_kmalloc(gfp_flags, RESYNC_PAGES);\r\nif (!bio)\r\ngoto out_free_bio;\r\nr10_bio->devs[j].bio = bio;\r\nif (!conf->have_replacement)\r\ncontinue;\r\nbio = bio_kmalloc(gfp_flags, RESYNC_PAGES);\r\nif (!bio)\r\ngoto out_free_bio;\r\nr10_bio->devs[j].repl_bio = bio;\r\n}\r\nfor (j = 0 ; j < nalloc; j++) {\r\nstruct bio *rbio = r10_bio->devs[j].repl_bio;\r\nbio = r10_bio->devs[j].bio;\r\nfor (i = 0; i < RESYNC_PAGES; i++) {\r\nif (j > 0 && !test_bit(MD_RECOVERY_SYNC,\r\n&conf->mddev->recovery)) {\r\nstruct bio *rbio = r10_bio->devs[0].bio;\r\npage = rbio->bi_io_vec[i].bv_page;\r\nget_page(page);\r\n} else\r\npage = alloc_page(gfp_flags);\r\nif (unlikely(!page))\r\ngoto out_free_pages;\r\nbio->bi_io_vec[i].bv_page = page;\r\nif (rbio)\r\nrbio->bi_io_vec[i].bv_page = page;\r\n}\r\n}\r\nreturn r10_bio;\r\nout_free_pages:\r\nfor ( ; i > 0 ; i--)\r\nsafe_put_page(bio->bi_io_vec[i-1].bv_page);\r\nwhile (j--)\r\nfor (i = 0; i < RESYNC_PAGES ; i++)\r\nsafe_put_page(r10_bio->devs[j].bio->bi_io_vec[i].bv_page);\r\nj = 0;\r\nout_free_bio:\r\nfor ( ; j < nalloc; j++) {\r\nif (r10_bio->devs[j].bio)\r\nbio_put(r10_bio->devs[j].bio);\r\nif (r10_bio->devs[j].repl_bio)\r\nbio_put(r10_bio->devs[j].repl_bio);\r\n}\r\nr10bio_pool_free(r10_bio, conf);\r\nreturn NULL;\r\n}\r\nstatic void r10buf_pool_free(void *__r10_bio, void *data)\r\n{\r\nint i;\r\nstruct r10conf *conf = data;\r\nstruct r10bio *r10bio = __r10_bio;\r\nint j;\r\nfor (j=0; j < conf->copies; j++) {\r\nstruct bio *bio = r10bio->devs[j].bio;\r\nif (bio) {\r\nfor (i = 0; i < RESYNC_PAGES; i++) {\r\nsafe_put_page(bio->bi_io_vec[i].bv_page);\r\nbio->bi_io_vec[i].bv_page = NULL;\r\n}\r\nbio_put(bio);\r\n}\r\nbio = r10bio->devs[j].repl_bio;\r\nif (bio)\r\nbio_put(bio);\r\n}\r\nr10bio_pool_free(r10bio, conf);\r\n}\r\nstatic void put_all_bios(struct r10conf *conf, struct r10bio *r10_bio)\r\n{\r\nint i;\r\nfor (i = 0; i < conf->copies; i++) {\r\nstruct bio **bio = & r10_bio->devs[i].bio;\r\nif (!BIO_SPECIAL(*bio))\r\nbio_put(*bio);\r\n*bio = NULL;\r\nbio = &r10_bio->devs[i].repl_bio;\r\nif (r10_bio->read_slot < 0 && !BIO_SPECIAL(*bio))\r\nbio_put(*bio);\r\n*bio = NULL;\r\n}\r\n}\r\nstatic void free_r10bio(struct r10bio *r10_bio)\r\n{\r\nstruct r10conf *conf = r10_bio->mddev->private;\r\nput_all_bios(conf, r10_bio);\r\nmempool_free(r10_bio, conf->r10bio_pool);\r\n}\r\nstatic void put_buf(struct r10bio *r10_bio)\r\n{\r\nstruct r10conf *conf = r10_bio->mddev->private;\r\nmempool_free(r10_bio, conf->r10buf_pool);\r\nlower_barrier(conf);\r\n}\r\nstatic void reschedule_retry(struct r10bio *r10_bio)\r\n{\r\nunsigned long flags;\r\nstruct mddev *mddev = r10_bio->mddev;\r\nstruct r10conf *conf = mddev->private;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nlist_add(&r10_bio->retry_list, &conf->retry_list);\r\nconf->nr_queued ++;\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nwake_up(&conf->wait_barrier);\r\nmd_wakeup_thread(mddev->thread);\r\n}\r\nstatic void raid_end_bio_io(struct r10bio *r10_bio)\r\n{\r\nstruct bio *bio = r10_bio->master_bio;\r\nint done;\r\nstruct r10conf *conf = r10_bio->mddev->private;\r\nif (bio->bi_phys_segments) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nbio->bi_phys_segments--;\r\ndone = (bio->bi_phys_segments == 0);\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\n} else\r\ndone = 1;\r\nif (!test_bit(R10BIO_Uptodate, &r10_bio->state))\r\nclear_bit(BIO_UPTODATE, &bio->bi_flags);\r\nif (done) {\r\nbio_endio(bio, 0);\r\nallow_barrier(conf);\r\n}\r\nfree_r10bio(r10_bio);\r\n}\r\nstatic inline void update_head_pos(int slot, struct r10bio *r10_bio)\r\n{\r\nstruct r10conf *conf = r10_bio->mddev->private;\r\nconf->mirrors[r10_bio->devs[slot].devnum].head_position =\r\nr10_bio->devs[slot].addr + (r10_bio->sectors);\r\n}\r\nstatic int find_bio_disk(struct r10conf *conf, struct r10bio *r10_bio,\r\nstruct bio *bio, int *slotp, int *replp)\r\n{\r\nint slot;\r\nint repl = 0;\r\nfor (slot = 0; slot < conf->copies; slot++) {\r\nif (r10_bio->devs[slot].bio == bio)\r\nbreak;\r\nif (r10_bio->devs[slot].repl_bio == bio) {\r\nrepl = 1;\r\nbreak;\r\n}\r\n}\r\nBUG_ON(slot == conf->copies);\r\nupdate_head_pos(slot, r10_bio);\r\nif (slotp)\r\n*slotp = slot;\r\nif (replp)\r\n*replp = repl;\r\nreturn r10_bio->devs[slot].devnum;\r\n}\r\nstatic void raid10_end_read_request(struct bio *bio, int error)\r\n{\r\nint uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nstruct r10bio *r10_bio = bio->bi_private;\r\nint slot, dev;\r\nstruct md_rdev *rdev;\r\nstruct r10conf *conf = r10_bio->mddev->private;\r\nslot = r10_bio->read_slot;\r\ndev = r10_bio->devs[slot].devnum;\r\nrdev = r10_bio->devs[slot].rdev;\r\nupdate_head_pos(slot, r10_bio);\r\nif (uptodate) {\r\nset_bit(R10BIO_Uptodate, &r10_bio->state);\r\n} else {\r\nif (!_enough(conf, test_bit(R10BIO_Previous, &r10_bio->state),\r\nrdev->raid_disk))\r\nuptodate = 1;\r\n}\r\nif (uptodate) {\r\nraid_end_bio_io(r10_bio);\r\nrdev_dec_pending(rdev, conf->mddev);\r\n} else {\r\nchar b[BDEVNAME_SIZE];\r\nprintk_ratelimited(KERN_ERR\r\n"md/raid10:%s: %s: rescheduling sector %llu\n",\r\nmdname(conf->mddev),\r\nbdevname(rdev->bdev, b),\r\n(unsigned long long)r10_bio->sector);\r\nset_bit(R10BIO_ReadError, &r10_bio->state);\r\nreschedule_retry(r10_bio);\r\n}\r\n}\r\nstatic void close_write(struct r10bio *r10_bio)\r\n{\r\nbitmap_endwrite(r10_bio->mddev->bitmap, r10_bio->sector,\r\nr10_bio->sectors,\r\n!test_bit(R10BIO_Degraded, &r10_bio->state),\r\n0);\r\nmd_write_end(r10_bio->mddev);\r\n}\r\nstatic void one_write_done(struct r10bio *r10_bio)\r\n{\r\nif (atomic_dec_and_test(&r10_bio->remaining)) {\r\nif (test_bit(R10BIO_WriteError, &r10_bio->state))\r\nreschedule_retry(r10_bio);\r\nelse {\r\nclose_write(r10_bio);\r\nif (test_bit(R10BIO_MadeGood, &r10_bio->state))\r\nreschedule_retry(r10_bio);\r\nelse\r\nraid_end_bio_io(r10_bio);\r\n}\r\n}\r\n}\r\nstatic void raid10_end_write_request(struct bio *bio, int error)\r\n{\r\nint uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nstruct r10bio *r10_bio = bio->bi_private;\r\nint dev;\r\nint dec_rdev = 1;\r\nstruct r10conf *conf = r10_bio->mddev->private;\r\nint slot, repl;\r\nstruct md_rdev *rdev = NULL;\r\ndev = find_bio_disk(conf, r10_bio, bio, &slot, &repl);\r\nif (repl)\r\nrdev = conf->mirrors[dev].replacement;\r\nif (!rdev) {\r\nsmp_rmb();\r\nrepl = 0;\r\nrdev = conf->mirrors[dev].rdev;\r\n}\r\nif (!uptodate) {\r\nif (repl)\r\nmd_error(rdev->mddev, rdev);\r\nelse {\r\nset_bit(WriteErrorSeen, &rdev->flags);\r\nif (!test_and_set_bit(WantReplacement, &rdev->flags))\r\nset_bit(MD_RECOVERY_NEEDED,\r\n&rdev->mddev->recovery);\r\nset_bit(R10BIO_WriteError, &r10_bio->state);\r\ndec_rdev = 0;\r\n}\r\n} else {\r\nsector_t first_bad;\r\nint bad_sectors;\r\nif (test_bit(In_sync, &rdev->flags) &&\r\n!test_bit(Faulty, &rdev->flags))\r\nset_bit(R10BIO_Uptodate, &r10_bio->state);\r\nif (is_badblock(rdev,\r\nr10_bio->devs[slot].addr,\r\nr10_bio->sectors,\r\n&first_bad, &bad_sectors)) {\r\nbio_put(bio);\r\nif (repl)\r\nr10_bio->devs[slot].repl_bio = IO_MADE_GOOD;\r\nelse\r\nr10_bio->devs[slot].bio = IO_MADE_GOOD;\r\ndec_rdev = 0;\r\nset_bit(R10BIO_MadeGood, &r10_bio->state);\r\n}\r\n}\r\none_write_done(r10_bio);\r\nif (dec_rdev)\r\nrdev_dec_pending(rdev, conf->mddev);\r\n}\r\nstatic void __raid10_find_phys(struct geom *geo, struct r10bio *r10bio)\r\n{\r\nint n,f;\r\nsector_t sector;\r\nsector_t chunk;\r\nsector_t stripe;\r\nint dev;\r\nint slot = 0;\r\nint last_far_set_start, last_far_set_size;\r\nlast_far_set_start = (geo->raid_disks / geo->far_set_size) - 1;\r\nlast_far_set_start *= geo->far_set_size;\r\nlast_far_set_size = geo->far_set_size;\r\nlast_far_set_size += (geo->raid_disks % geo->far_set_size);\r\nchunk = r10bio->sector >> geo->chunk_shift;\r\nsector = r10bio->sector & geo->chunk_mask;\r\nchunk *= geo->near_copies;\r\nstripe = chunk;\r\ndev = sector_div(stripe, geo->raid_disks);\r\nif (geo->far_offset)\r\nstripe *= geo->far_copies;\r\nsector += stripe << geo->chunk_shift;\r\nfor (n = 0; n < geo->near_copies; n++) {\r\nint d = dev;\r\nint set;\r\nsector_t s = sector;\r\nr10bio->devs[slot].devnum = d;\r\nr10bio->devs[slot].addr = s;\r\nslot++;\r\nfor (f = 1; f < geo->far_copies; f++) {\r\nset = d / geo->far_set_size;\r\nd += geo->near_copies;\r\nif ((geo->raid_disks % geo->far_set_size) &&\r\n(d > last_far_set_start)) {\r\nd -= last_far_set_start;\r\nd %= last_far_set_size;\r\nd += last_far_set_start;\r\n} else {\r\nd %= geo->far_set_size;\r\nd += geo->far_set_size * set;\r\n}\r\ns += geo->stride;\r\nr10bio->devs[slot].devnum = d;\r\nr10bio->devs[slot].addr = s;\r\nslot++;\r\n}\r\ndev++;\r\nif (dev >= geo->raid_disks) {\r\ndev = 0;\r\nsector += (geo->chunk_mask + 1);\r\n}\r\n}\r\n}\r\nstatic void raid10_find_phys(struct r10conf *conf, struct r10bio *r10bio)\r\n{\r\nstruct geom *geo = &conf->geo;\r\nif (conf->reshape_progress != MaxSector &&\r\n((r10bio->sector >= conf->reshape_progress) !=\r\nconf->mddev->reshape_backwards)) {\r\nset_bit(R10BIO_Previous, &r10bio->state);\r\ngeo = &conf->prev;\r\n} else\r\nclear_bit(R10BIO_Previous, &r10bio->state);\r\n__raid10_find_phys(geo, r10bio);\r\n}\r\nstatic sector_t raid10_find_virt(struct r10conf *conf, sector_t sector, int dev)\r\n{\r\nsector_t offset, chunk, vchunk;\r\nstruct geom *geo = &conf->geo;\r\nint far_set_start = (dev / geo->far_set_size) * geo->far_set_size;\r\nint far_set_size = geo->far_set_size;\r\nint last_far_set_start;\r\nif (geo->raid_disks % geo->far_set_size) {\r\nlast_far_set_start = (geo->raid_disks / geo->far_set_size) - 1;\r\nlast_far_set_start *= geo->far_set_size;\r\nif (dev >= last_far_set_start) {\r\nfar_set_size = geo->far_set_size;\r\nfar_set_size += (geo->raid_disks % geo->far_set_size);\r\nfar_set_start = last_far_set_start;\r\n}\r\n}\r\noffset = sector & geo->chunk_mask;\r\nif (geo->far_offset) {\r\nint fc;\r\nchunk = sector >> geo->chunk_shift;\r\nfc = sector_div(chunk, geo->far_copies);\r\ndev -= fc * geo->near_copies;\r\nif (dev < far_set_start)\r\ndev += far_set_size;\r\n} else {\r\nwhile (sector >= geo->stride) {\r\nsector -= geo->stride;\r\nif (dev < (geo->near_copies + far_set_start))\r\ndev += far_set_size - geo->near_copies;\r\nelse\r\ndev -= geo->near_copies;\r\n}\r\nchunk = sector >> geo->chunk_shift;\r\n}\r\nvchunk = chunk * geo->raid_disks + dev;\r\nsector_div(vchunk, geo->near_copies);\r\nreturn (vchunk << geo->chunk_shift) + offset;\r\n}\r\nstatic int raid10_mergeable_bvec(struct request_queue *q,\r\nstruct bvec_merge_data *bvm,\r\nstruct bio_vec *biovec)\r\n{\r\nstruct mddev *mddev = q->queuedata;\r\nstruct r10conf *conf = mddev->private;\r\nsector_t sector = bvm->bi_sector + get_start_sect(bvm->bi_bdev);\r\nint max;\r\nunsigned int chunk_sectors;\r\nunsigned int bio_sectors = bvm->bi_size >> 9;\r\nstruct geom *geo = &conf->geo;\r\nchunk_sectors = (conf->geo.chunk_mask & conf->prev.chunk_mask) + 1;\r\nif (conf->reshape_progress != MaxSector &&\r\n((sector >= conf->reshape_progress) !=\r\nconf->mddev->reshape_backwards))\r\ngeo = &conf->prev;\r\nif (geo->near_copies < geo->raid_disks) {\r\nmax = (chunk_sectors - ((sector & (chunk_sectors - 1))\r\n+ bio_sectors)) << 9;\r\nif (max < 0)\r\nmax = 0;\r\nif (max <= biovec->bv_len && bio_sectors == 0)\r\nreturn biovec->bv_len;\r\n} else\r\nmax = biovec->bv_len;\r\nif (mddev->merge_check_needed) {\r\nstruct {\r\nstruct r10bio r10_bio;\r\nstruct r10dev devs[conf->copies];\r\n} on_stack;\r\nstruct r10bio *r10_bio = &on_stack.r10_bio;\r\nint s;\r\nif (conf->reshape_progress != MaxSector) {\r\nif (max <= biovec->bv_len && bio_sectors == 0)\r\nreturn biovec->bv_len;\r\nreturn 0;\r\n}\r\nr10_bio->sector = sector;\r\nraid10_find_phys(conf, r10_bio);\r\nrcu_read_lock();\r\nfor (s = 0; s < conf->copies; s++) {\r\nint disk = r10_bio->devs[s].devnum;\r\nstruct md_rdev *rdev = rcu_dereference(\r\nconf->mirrors[disk].rdev);\r\nif (rdev && !test_bit(Faulty, &rdev->flags)) {\r\nstruct request_queue *q =\r\nbdev_get_queue(rdev->bdev);\r\nif (q->merge_bvec_fn) {\r\nbvm->bi_sector = r10_bio->devs[s].addr\r\n+ rdev->data_offset;\r\nbvm->bi_bdev = rdev->bdev;\r\nmax = min(max, q->merge_bvec_fn(\r\nq, bvm, biovec));\r\n}\r\n}\r\nrdev = rcu_dereference(conf->mirrors[disk].replacement);\r\nif (rdev && !test_bit(Faulty, &rdev->flags)) {\r\nstruct request_queue *q =\r\nbdev_get_queue(rdev->bdev);\r\nif (q->merge_bvec_fn) {\r\nbvm->bi_sector = r10_bio->devs[s].addr\r\n+ rdev->data_offset;\r\nbvm->bi_bdev = rdev->bdev;\r\nmax = min(max, q->merge_bvec_fn(\r\nq, bvm, biovec));\r\n}\r\n}\r\n}\r\nrcu_read_unlock();\r\n}\r\nreturn max;\r\n}\r\nstatic struct md_rdev *read_balance(struct r10conf *conf,\r\nstruct r10bio *r10_bio,\r\nint *max_sectors)\r\n{\r\nconst sector_t this_sector = r10_bio->sector;\r\nint disk, slot;\r\nint sectors = r10_bio->sectors;\r\nint best_good_sectors;\r\nsector_t new_distance, best_dist;\r\nstruct md_rdev *best_rdev, *rdev = NULL;\r\nint do_balance;\r\nint best_slot;\r\nstruct geom *geo = &conf->geo;\r\nraid10_find_phys(conf, r10_bio);\r\nrcu_read_lock();\r\nretry:\r\nsectors = r10_bio->sectors;\r\nbest_slot = -1;\r\nbest_rdev = NULL;\r\nbest_dist = MaxSector;\r\nbest_good_sectors = 0;\r\ndo_balance = 1;\r\nif (conf->mddev->recovery_cp < MaxSector\r\n&& (this_sector + sectors >= conf->next_resync))\r\ndo_balance = 0;\r\nfor (slot = 0; slot < conf->copies ; slot++) {\r\nsector_t first_bad;\r\nint bad_sectors;\r\nsector_t dev_sector;\r\nif (r10_bio->devs[slot].bio == IO_BLOCKED)\r\ncontinue;\r\ndisk = r10_bio->devs[slot].devnum;\r\nrdev = rcu_dereference(conf->mirrors[disk].replacement);\r\nif (rdev == NULL || test_bit(Faulty, &rdev->flags) ||\r\ntest_bit(Unmerged, &rdev->flags) ||\r\nr10_bio->devs[slot].addr + sectors > rdev->recovery_offset)\r\nrdev = rcu_dereference(conf->mirrors[disk].rdev);\r\nif (rdev == NULL ||\r\ntest_bit(Faulty, &rdev->flags) ||\r\ntest_bit(Unmerged, &rdev->flags))\r\ncontinue;\r\nif (!test_bit(In_sync, &rdev->flags) &&\r\nr10_bio->devs[slot].addr + sectors > rdev->recovery_offset)\r\ncontinue;\r\ndev_sector = r10_bio->devs[slot].addr;\r\nif (is_badblock(rdev, dev_sector, sectors,\r\n&first_bad, &bad_sectors)) {\r\nif (best_dist < MaxSector)\r\ncontinue;\r\nif (first_bad <= dev_sector) {\r\nbad_sectors -= (dev_sector - first_bad);\r\nif (!do_balance && sectors > bad_sectors)\r\nsectors = bad_sectors;\r\nif (best_good_sectors > sectors)\r\nbest_good_sectors = sectors;\r\n} else {\r\nsector_t good_sectors =\r\nfirst_bad - dev_sector;\r\nif (good_sectors > best_good_sectors) {\r\nbest_good_sectors = good_sectors;\r\nbest_slot = slot;\r\nbest_rdev = rdev;\r\n}\r\nif (!do_balance)\r\nbreak;\r\n}\r\ncontinue;\r\n} else\r\nbest_good_sectors = sectors;\r\nif (!do_balance)\r\nbreak;\r\nif (geo->near_copies > 1 && !atomic_read(&rdev->nr_pending))\r\nbreak;\r\nif (geo->far_copies > 1)\r\nnew_distance = r10_bio->devs[slot].addr;\r\nelse\r\nnew_distance = abs(r10_bio->devs[slot].addr -\r\nconf->mirrors[disk].head_position);\r\nif (new_distance < best_dist) {\r\nbest_dist = new_distance;\r\nbest_slot = slot;\r\nbest_rdev = rdev;\r\n}\r\n}\r\nif (slot >= conf->copies) {\r\nslot = best_slot;\r\nrdev = best_rdev;\r\n}\r\nif (slot >= 0) {\r\natomic_inc(&rdev->nr_pending);\r\nif (test_bit(Faulty, &rdev->flags)) {\r\nrdev_dec_pending(rdev, conf->mddev);\r\ngoto retry;\r\n}\r\nr10_bio->read_slot = slot;\r\n} else\r\nrdev = NULL;\r\nrcu_read_unlock();\r\n*max_sectors = best_good_sectors;\r\nreturn rdev;\r\n}\r\nint md_raid10_congested(struct mddev *mddev, int bits)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nint i, ret = 0;\r\nif ((bits & (1 << BDI_async_congested)) &&\r\nconf->pending_count >= max_queued_requests)\r\nreturn 1;\r\nrcu_read_lock();\r\nfor (i = 0;\r\n(i < conf->geo.raid_disks || i < conf->prev.raid_disks)\r\n&& ret == 0;\r\ni++) {\r\nstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\r\nif (rdev && !test_bit(Faulty, &rdev->flags)) {\r\nstruct request_queue *q = bdev_get_queue(rdev->bdev);\r\nret |= bdi_congested(&q->backing_dev_info, bits);\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int raid10_congested(void *data, int bits)\r\n{\r\nstruct mddev *mddev = data;\r\nreturn mddev_congested(mddev, bits) ||\r\nmd_raid10_congested(mddev, bits);\r\n}\r\nstatic void flush_pending_writes(struct r10conf *conf)\r\n{\r\nspin_lock_irq(&conf->device_lock);\r\nif (conf->pending_bio_list.head) {\r\nstruct bio *bio;\r\nbio = bio_list_get(&conf->pending_bio_list);\r\nconf->pending_count = 0;\r\nspin_unlock_irq(&conf->device_lock);\r\nbitmap_unplug(conf->mddev->bitmap);\r\nwake_up(&conf->wait_barrier);\r\nwhile (bio) {\r\nstruct bio *next = bio->bi_next;\r\nbio->bi_next = NULL;\r\nif (unlikely((bio->bi_rw & REQ_DISCARD) &&\r\n!blk_queue_discard(bdev_get_queue(bio->bi_bdev))))\r\nbio_endio(bio, 0);\r\nelse\r\ngeneric_make_request(bio);\r\nbio = next;\r\n}\r\n} else\r\nspin_unlock_irq(&conf->device_lock);\r\n}\r\nstatic void raise_barrier(struct r10conf *conf, int force)\r\n{\r\nBUG_ON(force && !conf->barrier);\r\nspin_lock_irq(&conf->resync_lock);\r\nwait_event_lock_irq(conf->wait_barrier, force || !conf->nr_waiting,\r\nconf->resync_lock);\r\nconf->barrier++;\r\nwait_event_lock_irq(conf->wait_barrier,\r\n!conf->nr_pending && conf->barrier < RESYNC_DEPTH,\r\nconf->resync_lock);\r\nspin_unlock_irq(&conf->resync_lock);\r\n}\r\nstatic void lower_barrier(struct r10conf *conf)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&conf->resync_lock, flags);\r\nconf->barrier--;\r\nspin_unlock_irqrestore(&conf->resync_lock, flags);\r\nwake_up(&conf->wait_barrier);\r\n}\r\nstatic void wait_barrier(struct r10conf *conf)\r\n{\r\nspin_lock_irq(&conf->resync_lock);\r\nif (conf->barrier) {\r\nconf->nr_waiting++;\r\nwait_event_lock_irq(conf->wait_barrier,\r\n!conf->barrier ||\r\n(conf->nr_pending &&\r\ncurrent->bio_list &&\r\n!bio_list_empty(current->bio_list)),\r\nconf->resync_lock);\r\nconf->nr_waiting--;\r\n}\r\nconf->nr_pending++;\r\nspin_unlock_irq(&conf->resync_lock);\r\n}\r\nstatic void allow_barrier(struct r10conf *conf)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&conf->resync_lock, flags);\r\nconf->nr_pending--;\r\nspin_unlock_irqrestore(&conf->resync_lock, flags);\r\nwake_up(&conf->wait_barrier);\r\n}\r\nstatic void freeze_array(struct r10conf *conf, int extra)\r\n{\r\nspin_lock_irq(&conf->resync_lock);\r\nconf->barrier++;\r\nconf->nr_waiting++;\r\nwait_event_lock_irq_cmd(conf->wait_barrier,\r\nconf->nr_pending == conf->nr_queued+extra,\r\nconf->resync_lock,\r\nflush_pending_writes(conf));\r\nspin_unlock_irq(&conf->resync_lock);\r\n}\r\nstatic void unfreeze_array(struct r10conf *conf)\r\n{\r\nspin_lock_irq(&conf->resync_lock);\r\nconf->barrier--;\r\nconf->nr_waiting--;\r\nwake_up(&conf->wait_barrier);\r\nspin_unlock_irq(&conf->resync_lock);\r\n}\r\nstatic sector_t choose_data_offset(struct r10bio *r10_bio,\r\nstruct md_rdev *rdev)\r\n{\r\nif (!test_bit(MD_RECOVERY_RESHAPE, &rdev->mddev->recovery) ||\r\ntest_bit(R10BIO_Previous, &r10_bio->state))\r\nreturn rdev->data_offset;\r\nelse\r\nreturn rdev->new_data_offset;\r\n}\r\nstatic void raid10_unplug(struct blk_plug_cb *cb, bool from_schedule)\r\n{\r\nstruct raid10_plug_cb *plug = container_of(cb, struct raid10_plug_cb,\r\ncb);\r\nstruct mddev *mddev = plug->cb.data;\r\nstruct r10conf *conf = mddev->private;\r\nstruct bio *bio;\r\nif (from_schedule || current->bio_list) {\r\nspin_lock_irq(&conf->device_lock);\r\nbio_list_merge(&conf->pending_bio_list, &plug->pending);\r\nconf->pending_count += plug->pending_cnt;\r\nspin_unlock_irq(&conf->device_lock);\r\nwake_up(&conf->wait_barrier);\r\nmd_wakeup_thread(mddev->thread);\r\nkfree(plug);\r\nreturn;\r\n}\r\nbio = bio_list_get(&plug->pending);\r\nbitmap_unplug(mddev->bitmap);\r\nwake_up(&conf->wait_barrier);\r\nwhile (bio) {\r\nstruct bio *next = bio->bi_next;\r\nbio->bi_next = NULL;\r\nif (unlikely((bio->bi_rw & REQ_DISCARD) &&\r\n!blk_queue_discard(bdev_get_queue(bio->bi_bdev))))\r\nbio_endio(bio, 0);\r\nelse\r\ngeneric_make_request(bio);\r\nbio = next;\r\n}\r\nkfree(plug);\r\n}\r\nstatic void __make_request(struct mddev *mddev, struct bio *bio)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nstruct r10bio *r10_bio;\r\nstruct bio *read_bio;\r\nint i;\r\nconst int rw = bio_data_dir(bio);\r\nconst unsigned long do_sync = (bio->bi_rw & REQ_SYNC);\r\nconst unsigned long do_fua = (bio->bi_rw & REQ_FUA);\r\nconst unsigned long do_discard = (bio->bi_rw\r\n& (REQ_DISCARD | REQ_SECURE));\r\nconst unsigned long do_same = (bio->bi_rw & REQ_WRITE_SAME);\r\nunsigned long flags;\r\nstruct md_rdev *blocked_rdev;\r\nstruct blk_plug_cb *cb;\r\nstruct raid10_plug_cb *plug = NULL;\r\nint sectors_handled;\r\nint max_sectors;\r\nint sectors;\r\nwait_barrier(conf);\r\nsectors = bio_sectors(bio);\r\nwhile (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\r\nbio->bi_iter.bi_sector < conf->reshape_progress &&\r\nbio->bi_iter.bi_sector + sectors > conf->reshape_progress) {\r\nallow_barrier(conf);\r\nwait_event(conf->wait_barrier,\r\nconf->reshape_progress <= bio->bi_iter.bi_sector ||\r\nconf->reshape_progress >= bio->bi_iter.bi_sector +\r\nsectors);\r\nwait_barrier(conf);\r\n}\r\nif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\r\nbio_data_dir(bio) == WRITE &&\r\n(mddev->reshape_backwards\r\n? (bio->bi_iter.bi_sector < conf->reshape_safe &&\r\nbio->bi_iter.bi_sector + sectors > conf->reshape_progress)\r\n: (bio->bi_iter.bi_sector + sectors > conf->reshape_safe &&\r\nbio->bi_iter.bi_sector < conf->reshape_progress))) {\r\nmddev->reshape_position = conf->reshape_progress;\r\nset_bit(MD_CHANGE_DEVS, &mddev->flags);\r\nset_bit(MD_CHANGE_PENDING, &mddev->flags);\r\nmd_wakeup_thread(mddev->thread);\r\nwait_event(mddev->sb_wait,\r\n!test_bit(MD_CHANGE_PENDING, &mddev->flags));\r\nconf->reshape_safe = mddev->reshape_position;\r\n}\r\nr10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);\r\nr10_bio->master_bio = bio;\r\nr10_bio->sectors = sectors;\r\nr10_bio->mddev = mddev;\r\nr10_bio->sector = bio->bi_iter.bi_sector;\r\nr10_bio->state = 0;\r\nbio->bi_phys_segments = 0;\r\nclear_bit(BIO_SEG_VALID, &bio->bi_flags);\r\nif (rw == READ) {\r\nstruct md_rdev *rdev;\r\nint slot;\r\nread_again:\r\nrdev = read_balance(conf, r10_bio, &max_sectors);\r\nif (!rdev) {\r\nraid_end_bio_io(r10_bio);\r\nreturn;\r\n}\r\nslot = r10_bio->read_slot;\r\nread_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);\r\nbio_trim(read_bio, r10_bio->sector - bio->bi_iter.bi_sector,\r\nmax_sectors);\r\nr10_bio->devs[slot].bio = read_bio;\r\nr10_bio->devs[slot].rdev = rdev;\r\nread_bio->bi_iter.bi_sector = r10_bio->devs[slot].addr +\r\nchoose_data_offset(r10_bio, rdev);\r\nread_bio->bi_bdev = rdev->bdev;\r\nread_bio->bi_end_io = raid10_end_read_request;\r\nread_bio->bi_rw = READ | do_sync;\r\nread_bio->bi_private = r10_bio;\r\nif (max_sectors < r10_bio->sectors) {\r\nsectors_handled = (r10_bio->sector + max_sectors\r\n- bio->bi_iter.bi_sector);\r\nr10_bio->sectors = max_sectors;\r\nspin_lock_irq(&conf->device_lock);\r\nif (bio->bi_phys_segments == 0)\r\nbio->bi_phys_segments = 2;\r\nelse\r\nbio->bi_phys_segments++;\r\nspin_unlock_irq(&conf->device_lock);\r\nreschedule_retry(r10_bio);\r\nr10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);\r\nr10_bio->master_bio = bio;\r\nr10_bio->sectors = bio_sectors(bio) - sectors_handled;\r\nr10_bio->state = 0;\r\nr10_bio->mddev = mddev;\r\nr10_bio->sector = bio->bi_iter.bi_sector +\r\nsectors_handled;\r\ngoto read_again;\r\n} else\r\ngeneric_make_request(read_bio);\r\nreturn;\r\n}\r\nif (conf->pending_count >= max_queued_requests) {\r\nmd_wakeup_thread(mddev->thread);\r\nwait_event(conf->wait_barrier,\r\nconf->pending_count < max_queued_requests);\r\n}\r\nr10_bio->read_slot = -1;\r\nraid10_find_phys(conf, r10_bio);\r\nretry_write:\r\nblocked_rdev = NULL;\r\nrcu_read_lock();\r\nmax_sectors = r10_bio->sectors;\r\nfor (i = 0; i < conf->copies; i++) {\r\nint d = r10_bio->devs[i].devnum;\r\nstruct md_rdev *rdev = rcu_dereference(conf->mirrors[d].rdev);\r\nstruct md_rdev *rrdev = rcu_dereference(\r\nconf->mirrors[d].replacement);\r\nif (rdev == rrdev)\r\nrrdev = NULL;\r\nif (rdev && unlikely(test_bit(Blocked, &rdev->flags))) {\r\natomic_inc(&rdev->nr_pending);\r\nblocked_rdev = rdev;\r\nbreak;\r\n}\r\nif (rrdev && unlikely(test_bit(Blocked, &rrdev->flags))) {\r\natomic_inc(&rrdev->nr_pending);\r\nblocked_rdev = rrdev;\r\nbreak;\r\n}\r\nif (rdev && (test_bit(Faulty, &rdev->flags)\r\n|| test_bit(Unmerged, &rdev->flags)))\r\nrdev = NULL;\r\nif (rrdev && (test_bit(Faulty, &rrdev->flags)\r\n|| test_bit(Unmerged, &rrdev->flags)))\r\nrrdev = NULL;\r\nr10_bio->devs[i].bio = NULL;\r\nr10_bio->devs[i].repl_bio = NULL;\r\nif (!rdev && !rrdev) {\r\nset_bit(R10BIO_Degraded, &r10_bio->state);\r\ncontinue;\r\n}\r\nif (rdev && test_bit(WriteErrorSeen, &rdev->flags)) {\r\nsector_t first_bad;\r\nsector_t dev_sector = r10_bio->devs[i].addr;\r\nint bad_sectors;\r\nint is_bad;\r\nis_bad = is_badblock(rdev, dev_sector,\r\nmax_sectors,\r\n&first_bad, &bad_sectors);\r\nif (is_bad < 0) {\r\natomic_inc(&rdev->nr_pending);\r\nset_bit(BlockedBadBlocks, &rdev->flags);\r\nblocked_rdev = rdev;\r\nbreak;\r\n}\r\nif (is_bad && first_bad <= dev_sector) {\r\nbad_sectors -= (dev_sector - first_bad);\r\nif (bad_sectors < max_sectors)\r\nmax_sectors = bad_sectors;\r\ncontinue;\r\n}\r\nif (is_bad) {\r\nint good_sectors = first_bad - dev_sector;\r\nif (good_sectors < max_sectors)\r\nmax_sectors = good_sectors;\r\n}\r\n}\r\nif (rdev) {\r\nr10_bio->devs[i].bio = bio;\r\natomic_inc(&rdev->nr_pending);\r\n}\r\nif (rrdev) {\r\nr10_bio->devs[i].repl_bio = bio;\r\natomic_inc(&rrdev->nr_pending);\r\n}\r\n}\r\nrcu_read_unlock();\r\nif (unlikely(blocked_rdev)) {\r\nint j;\r\nint d;\r\nfor (j = 0; j < i; j++) {\r\nif (r10_bio->devs[j].bio) {\r\nd = r10_bio->devs[j].devnum;\r\nrdev_dec_pending(conf->mirrors[d].rdev, mddev);\r\n}\r\nif (r10_bio->devs[j].repl_bio) {\r\nstruct md_rdev *rdev;\r\nd = r10_bio->devs[j].devnum;\r\nrdev = conf->mirrors[d].replacement;\r\nif (!rdev) {\r\nsmp_mb();\r\nrdev = conf->mirrors[d].rdev;\r\n}\r\nrdev_dec_pending(rdev, mddev);\r\n}\r\n}\r\nallow_barrier(conf);\r\nmd_wait_for_blocked_rdev(blocked_rdev, mddev);\r\nwait_barrier(conf);\r\ngoto retry_write;\r\n}\r\nif (max_sectors < r10_bio->sectors) {\r\nr10_bio->sectors = max_sectors;\r\nspin_lock_irq(&conf->device_lock);\r\nif (bio->bi_phys_segments == 0)\r\nbio->bi_phys_segments = 2;\r\nelse\r\nbio->bi_phys_segments++;\r\nspin_unlock_irq(&conf->device_lock);\r\n}\r\nsectors_handled = r10_bio->sector + max_sectors -\r\nbio->bi_iter.bi_sector;\r\natomic_set(&r10_bio->remaining, 1);\r\nbitmap_startwrite(mddev->bitmap, r10_bio->sector, r10_bio->sectors, 0);\r\nfor (i = 0; i < conf->copies; i++) {\r\nstruct bio *mbio;\r\nint d = r10_bio->devs[i].devnum;\r\nif (r10_bio->devs[i].bio) {\r\nstruct md_rdev *rdev = conf->mirrors[d].rdev;\r\nmbio = bio_clone_mddev(bio, GFP_NOIO, mddev);\r\nbio_trim(mbio, r10_bio->sector - bio->bi_iter.bi_sector,\r\nmax_sectors);\r\nr10_bio->devs[i].bio = mbio;\r\nmbio->bi_iter.bi_sector = (r10_bio->devs[i].addr+\r\nchoose_data_offset(r10_bio,\r\nrdev));\r\nmbio->bi_bdev = rdev->bdev;\r\nmbio->bi_end_io = raid10_end_write_request;\r\nmbio->bi_rw =\r\nWRITE | do_sync | do_fua | do_discard | do_same;\r\nmbio->bi_private = r10_bio;\r\natomic_inc(&r10_bio->remaining);\r\ncb = blk_check_plugged(raid10_unplug, mddev,\r\nsizeof(*plug));\r\nif (cb)\r\nplug = container_of(cb, struct raid10_plug_cb,\r\ncb);\r\nelse\r\nplug = NULL;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nif (plug) {\r\nbio_list_add(&plug->pending, mbio);\r\nplug->pending_cnt++;\r\n} else {\r\nbio_list_add(&conf->pending_bio_list, mbio);\r\nconf->pending_count++;\r\n}\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nif (!plug)\r\nmd_wakeup_thread(mddev->thread);\r\n}\r\nif (r10_bio->devs[i].repl_bio) {\r\nstruct md_rdev *rdev = conf->mirrors[d].replacement;\r\nif (rdev == NULL) {\r\nsmp_mb();\r\nrdev = conf->mirrors[d].rdev;\r\n}\r\nmbio = bio_clone_mddev(bio, GFP_NOIO, mddev);\r\nbio_trim(mbio, r10_bio->sector - bio->bi_iter.bi_sector,\r\nmax_sectors);\r\nr10_bio->devs[i].repl_bio = mbio;\r\nmbio->bi_iter.bi_sector = (r10_bio->devs[i].addr +\r\nchoose_data_offset(\r\nr10_bio, rdev));\r\nmbio->bi_bdev = rdev->bdev;\r\nmbio->bi_end_io = raid10_end_write_request;\r\nmbio->bi_rw =\r\nWRITE | do_sync | do_fua | do_discard | do_same;\r\nmbio->bi_private = r10_bio;\r\natomic_inc(&r10_bio->remaining);\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nbio_list_add(&conf->pending_bio_list, mbio);\r\nconf->pending_count++;\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nif (!mddev_check_plugged(mddev))\r\nmd_wakeup_thread(mddev->thread);\r\n}\r\n}\r\nif (sectors_handled < bio_sectors(bio)) {\r\none_write_done(r10_bio);\r\nr10_bio = mempool_alloc(conf->r10bio_pool, GFP_NOIO);\r\nr10_bio->master_bio = bio;\r\nr10_bio->sectors = bio_sectors(bio) - sectors_handled;\r\nr10_bio->mddev = mddev;\r\nr10_bio->sector = bio->bi_iter.bi_sector + sectors_handled;\r\nr10_bio->state = 0;\r\ngoto retry_write;\r\n}\r\none_write_done(r10_bio);\r\n}\r\nstatic void make_request(struct mddev *mddev, struct bio *bio)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nsector_t chunk_mask = (conf->geo.chunk_mask & conf->prev.chunk_mask);\r\nint chunk_sects = chunk_mask + 1;\r\nstruct bio *split;\r\nif (unlikely(bio->bi_rw & REQ_FLUSH)) {\r\nmd_flush_request(mddev, bio);\r\nreturn;\r\n}\r\nmd_write_start(mddev, bio);\r\ndo {\r\nif (unlikely((bio->bi_iter.bi_sector & chunk_mask) +\r\nbio_sectors(bio) > chunk_sects\r\n&& (conf->geo.near_copies < conf->geo.raid_disks\r\n|| conf->prev.near_copies <\r\nconf->prev.raid_disks))) {\r\nsplit = bio_split(bio, chunk_sects -\r\n(bio->bi_iter.bi_sector &\r\n(chunk_sects - 1)),\r\nGFP_NOIO, fs_bio_set);\r\nbio_chain(split, bio);\r\n} else {\r\nsplit = bio;\r\n}\r\n__make_request(mddev, split);\r\n} while (split != bio);\r\nwake_up(&conf->wait_barrier);\r\n}\r\nstatic void status(struct seq_file *seq, struct mddev *mddev)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nint i;\r\nif (conf->geo.near_copies < conf->geo.raid_disks)\r\nseq_printf(seq, " %dK chunks", mddev->chunk_sectors / 2);\r\nif (conf->geo.near_copies > 1)\r\nseq_printf(seq, " %d near-copies", conf->geo.near_copies);\r\nif (conf->geo.far_copies > 1) {\r\nif (conf->geo.far_offset)\r\nseq_printf(seq, " %d offset-copies", conf->geo.far_copies);\r\nelse\r\nseq_printf(seq, " %d far-copies", conf->geo.far_copies);\r\n}\r\nseq_printf(seq, " [%d/%d] [", conf->geo.raid_disks,\r\nconf->geo.raid_disks - mddev->degraded);\r\nfor (i = 0; i < conf->geo.raid_disks; i++)\r\nseq_printf(seq, "%s",\r\nconf->mirrors[i].rdev &&\r\ntest_bit(In_sync, &conf->mirrors[i].rdev->flags) ? "U" : "_");\r\nseq_printf(seq, "]");\r\n}\r\nstatic int _enough(struct r10conf *conf, int previous, int ignore)\r\n{\r\nint first = 0;\r\nint has_enough = 0;\r\nint disks, ncopies;\r\nif (previous) {\r\ndisks = conf->prev.raid_disks;\r\nncopies = conf->prev.near_copies;\r\n} else {\r\ndisks = conf->geo.raid_disks;\r\nncopies = conf->geo.near_copies;\r\n}\r\nrcu_read_lock();\r\ndo {\r\nint n = conf->copies;\r\nint cnt = 0;\r\nint this = first;\r\nwhile (n--) {\r\nstruct md_rdev *rdev;\r\nif (this != ignore &&\r\n(rdev = rcu_dereference(conf->mirrors[this].rdev)) &&\r\ntest_bit(In_sync, &rdev->flags))\r\ncnt++;\r\nthis = (this+1) % disks;\r\n}\r\nif (cnt == 0)\r\ngoto out;\r\nfirst = (first + ncopies) % disks;\r\n} while (first != 0);\r\nhas_enough = 1;\r\nout:\r\nrcu_read_unlock();\r\nreturn has_enough;\r\n}\r\nstatic int enough(struct r10conf *conf, int ignore)\r\n{\r\nreturn _enough(conf, 0, ignore) &&\r\n_enough(conf, 1, ignore);\r\n}\r\nstatic void error(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nchar b[BDEVNAME_SIZE];\r\nstruct r10conf *conf = mddev->private;\r\nunsigned long flags;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nif (test_bit(In_sync, &rdev->flags)\r\n&& !enough(conf, rdev->raid_disk)) {\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nreturn;\r\n}\r\nif (test_and_clear_bit(In_sync, &rdev->flags))\r\nmddev->degraded++;\r\nset_bit(MD_RECOVERY_INTR, &mddev->recovery);\r\nset_bit(Blocked, &rdev->flags);\r\nset_bit(Faulty, &rdev->flags);\r\nset_bit(MD_CHANGE_DEVS, &mddev->flags);\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nprintk(KERN_ALERT\r\n"md/raid10:%s: Disk failure on %s, disabling device.\n"\r\n"md/raid10:%s: Operation continuing on %d devices.\n",\r\nmdname(mddev), bdevname(rdev->bdev, b),\r\nmdname(mddev), conf->geo.raid_disks - mddev->degraded);\r\n}\r\nstatic void print_conf(struct r10conf *conf)\r\n{\r\nint i;\r\nstruct raid10_info *tmp;\r\nprintk(KERN_DEBUG "RAID10 conf printout:\n");\r\nif (!conf) {\r\nprintk(KERN_DEBUG "(!conf)\n");\r\nreturn;\r\n}\r\nprintk(KERN_DEBUG " --- wd:%d rd:%d\n", conf->geo.raid_disks - conf->mddev->degraded,\r\nconf->geo.raid_disks);\r\nfor (i = 0; i < conf->geo.raid_disks; i++) {\r\nchar b[BDEVNAME_SIZE];\r\ntmp = conf->mirrors + i;\r\nif (tmp->rdev)\r\nprintk(KERN_DEBUG " disk %d, wo:%d, o:%d, dev:%s\n",\r\ni, !test_bit(In_sync, &tmp->rdev->flags),\r\n!test_bit(Faulty, &tmp->rdev->flags),\r\nbdevname(tmp->rdev->bdev,b));\r\n}\r\n}\r\nstatic void close_sync(struct r10conf *conf)\r\n{\r\nwait_barrier(conf);\r\nallow_barrier(conf);\r\nmempool_destroy(conf->r10buf_pool);\r\nconf->r10buf_pool = NULL;\r\n}\r\nstatic int raid10_spare_active(struct mddev *mddev)\r\n{\r\nint i;\r\nstruct r10conf *conf = mddev->private;\r\nstruct raid10_info *tmp;\r\nint count = 0;\r\nunsigned long flags;\r\nfor (i = 0; i < conf->geo.raid_disks; i++) {\r\ntmp = conf->mirrors + i;\r\nif (tmp->replacement\r\n&& tmp->replacement->recovery_offset == MaxSector\r\n&& !test_bit(Faulty, &tmp->replacement->flags)\r\n&& !test_and_set_bit(In_sync, &tmp->replacement->flags)) {\r\nif (!tmp->rdev\r\n|| !test_and_clear_bit(In_sync, &tmp->rdev->flags))\r\ncount++;\r\nif (tmp->rdev) {\r\nset_bit(Faulty, &tmp->rdev->flags);\r\nsysfs_notify_dirent_safe(\r\ntmp->rdev->sysfs_state);\r\n}\r\nsysfs_notify_dirent_safe(tmp->replacement->sysfs_state);\r\n} else if (tmp->rdev\r\n&& tmp->rdev->recovery_offset == MaxSector\r\n&& !test_bit(Faulty, &tmp->rdev->flags)\r\n&& !test_and_set_bit(In_sync, &tmp->rdev->flags)) {\r\ncount++;\r\nsysfs_notify_dirent_safe(tmp->rdev->sysfs_state);\r\n}\r\n}\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nmddev->degraded -= count;\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nprint_conf(conf);\r\nreturn count;\r\n}\r\nstatic int raid10_add_disk(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nint err = -EEXIST;\r\nint mirror;\r\nint first = 0;\r\nint last = conf->geo.raid_disks - 1;\r\nstruct request_queue *q = bdev_get_queue(rdev->bdev);\r\nif (mddev->recovery_cp < MaxSector)\r\nreturn -EBUSY;\r\nif (rdev->saved_raid_disk < 0 && !_enough(conf, 1, -1))\r\nreturn -EINVAL;\r\nif (rdev->raid_disk >= 0)\r\nfirst = last = rdev->raid_disk;\r\nif (q->merge_bvec_fn) {\r\nset_bit(Unmerged, &rdev->flags);\r\nmddev->merge_check_needed = 1;\r\n}\r\nif (rdev->saved_raid_disk >= first &&\r\nconf->mirrors[rdev->saved_raid_disk].rdev == NULL)\r\nmirror = rdev->saved_raid_disk;\r\nelse\r\nmirror = first;\r\nfor ( ; mirror <= last ; mirror++) {\r\nstruct raid10_info *p = &conf->mirrors[mirror];\r\nif (p->recovery_disabled == mddev->recovery_disabled)\r\ncontinue;\r\nif (p->rdev) {\r\nif (!test_bit(WantReplacement, &p->rdev->flags) ||\r\np->replacement != NULL)\r\ncontinue;\r\nclear_bit(In_sync, &rdev->flags);\r\nset_bit(Replacement, &rdev->flags);\r\nrdev->raid_disk = mirror;\r\nerr = 0;\r\nif (mddev->gendisk)\r\ndisk_stack_limits(mddev->gendisk, rdev->bdev,\r\nrdev->data_offset << 9);\r\nconf->fullsync = 1;\r\nrcu_assign_pointer(p->replacement, rdev);\r\nbreak;\r\n}\r\nif (mddev->gendisk)\r\ndisk_stack_limits(mddev->gendisk, rdev->bdev,\r\nrdev->data_offset << 9);\r\np->head_position = 0;\r\np->recovery_disabled = mddev->recovery_disabled - 1;\r\nrdev->raid_disk = mirror;\r\nerr = 0;\r\nif (rdev->saved_raid_disk != mirror)\r\nconf->fullsync = 1;\r\nrcu_assign_pointer(p->rdev, rdev);\r\nbreak;\r\n}\r\nif (err == 0 && test_bit(Unmerged, &rdev->flags)) {\r\nsynchronize_sched();\r\nfreeze_array(conf, 0);\r\nunfreeze_array(conf);\r\nclear_bit(Unmerged, &rdev->flags);\r\n}\r\nmd_integrity_add_rdev(rdev, mddev);\r\nif (mddev->queue && blk_queue_discard(bdev_get_queue(rdev->bdev)))\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, mddev->queue);\r\nprint_conf(conf);\r\nreturn err;\r\n}\r\nstatic int raid10_remove_disk(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nint err = 0;\r\nint number = rdev->raid_disk;\r\nstruct md_rdev **rdevp;\r\nstruct raid10_info *p = conf->mirrors + number;\r\nprint_conf(conf);\r\nif (rdev == p->rdev)\r\nrdevp = &p->rdev;\r\nelse if (rdev == p->replacement)\r\nrdevp = &p->replacement;\r\nelse\r\nreturn 0;\r\nif (test_bit(In_sync, &rdev->flags) ||\r\natomic_read(&rdev->nr_pending)) {\r\nerr = -EBUSY;\r\ngoto abort;\r\n}\r\nif (!test_bit(Faulty, &rdev->flags) &&\r\nmddev->recovery_disabled != p->recovery_disabled &&\r\n(!p->replacement || p->replacement == rdev) &&\r\nnumber < conf->geo.raid_disks &&\r\nenough(conf, -1)) {\r\nerr = -EBUSY;\r\ngoto abort;\r\n}\r\n*rdevp = NULL;\r\nsynchronize_rcu();\r\nif (atomic_read(&rdev->nr_pending)) {\r\nerr = -EBUSY;\r\n*rdevp = rdev;\r\ngoto abort;\r\n} else if (p->replacement) {\r\np->rdev = p->replacement;\r\nclear_bit(Replacement, &p->replacement->flags);\r\nsmp_mb();\r\np->replacement = NULL;\r\nclear_bit(WantReplacement, &rdev->flags);\r\n} else\r\nclear_bit(WantReplacement, &rdev->flags);\r\nerr = md_integrity_register(mddev);\r\nabort:\r\nprint_conf(conf);\r\nreturn err;\r\n}\r\nstatic void end_sync_read(struct bio *bio, int error)\r\n{\r\nstruct r10bio *r10_bio = bio->bi_private;\r\nstruct r10conf *conf = r10_bio->mddev->private;\r\nint d;\r\nif (bio == r10_bio->master_bio) {\r\nd = r10_bio->read_slot;\r\n} else\r\nd = find_bio_disk(conf, r10_bio, bio, NULL, NULL);\r\nif (test_bit(BIO_UPTODATE, &bio->bi_flags))\r\nset_bit(R10BIO_Uptodate, &r10_bio->state);\r\nelse\r\natomic_add(r10_bio->sectors,\r\n&conf->mirrors[d].rdev->corrected_errors);\r\nrdev_dec_pending(conf->mirrors[d].rdev, conf->mddev);\r\nif (test_bit(R10BIO_IsRecover, &r10_bio->state) ||\r\natomic_dec_and_test(&r10_bio->remaining)) {\r\nreschedule_retry(r10_bio);\r\n}\r\n}\r\nstatic void end_sync_request(struct r10bio *r10_bio)\r\n{\r\nstruct mddev *mddev = r10_bio->mddev;\r\nwhile (atomic_dec_and_test(&r10_bio->remaining)) {\r\nif (r10_bio->master_bio == NULL) {\r\nsector_t s = r10_bio->sectors;\r\nif (test_bit(R10BIO_MadeGood, &r10_bio->state) ||\r\ntest_bit(R10BIO_WriteError, &r10_bio->state))\r\nreschedule_retry(r10_bio);\r\nelse\r\nput_buf(r10_bio);\r\nmd_done_sync(mddev, s, 1);\r\nbreak;\r\n} else {\r\nstruct r10bio *r10_bio2 = (struct r10bio *)r10_bio->master_bio;\r\nif (test_bit(R10BIO_MadeGood, &r10_bio->state) ||\r\ntest_bit(R10BIO_WriteError, &r10_bio->state))\r\nreschedule_retry(r10_bio);\r\nelse\r\nput_buf(r10_bio);\r\nr10_bio = r10_bio2;\r\n}\r\n}\r\n}\r\nstatic void end_sync_write(struct bio *bio, int error)\r\n{\r\nint uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nstruct r10bio *r10_bio = bio->bi_private;\r\nstruct mddev *mddev = r10_bio->mddev;\r\nstruct r10conf *conf = mddev->private;\r\nint d;\r\nsector_t first_bad;\r\nint bad_sectors;\r\nint slot;\r\nint repl;\r\nstruct md_rdev *rdev = NULL;\r\nd = find_bio_disk(conf, r10_bio, bio, &slot, &repl);\r\nif (repl)\r\nrdev = conf->mirrors[d].replacement;\r\nelse\r\nrdev = conf->mirrors[d].rdev;\r\nif (!uptodate) {\r\nif (repl)\r\nmd_error(mddev, rdev);\r\nelse {\r\nset_bit(WriteErrorSeen, &rdev->flags);\r\nif (!test_and_set_bit(WantReplacement, &rdev->flags))\r\nset_bit(MD_RECOVERY_NEEDED,\r\n&rdev->mddev->recovery);\r\nset_bit(R10BIO_WriteError, &r10_bio->state);\r\n}\r\n} else if (is_badblock(rdev,\r\nr10_bio->devs[slot].addr,\r\nr10_bio->sectors,\r\n&first_bad, &bad_sectors))\r\nset_bit(R10BIO_MadeGood, &r10_bio->state);\r\nrdev_dec_pending(rdev, mddev);\r\nend_sync_request(r10_bio);\r\n}\r\nstatic void sync_request_write(struct mddev *mddev, struct r10bio *r10_bio)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nint i, first;\r\nstruct bio *tbio, *fbio;\r\nint vcnt;\r\natomic_set(&r10_bio->remaining, 1);\r\nfor (i=0; i<conf->copies; i++)\r\nif (test_bit(BIO_UPTODATE, &r10_bio->devs[i].bio->bi_flags))\r\nbreak;\r\nif (i == conf->copies)\r\ngoto done;\r\nfirst = i;\r\nfbio = r10_bio->devs[i].bio;\r\nvcnt = (r10_bio->sectors + (PAGE_SIZE >> 9) - 1) >> (PAGE_SHIFT - 9);\r\nfor (i=0 ; i < conf->copies ; i++) {\r\nint j, d;\r\ntbio = r10_bio->devs[i].bio;\r\nif (tbio->bi_end_io != end_sync_read)\r\ncontinue;\r\nif (i == first)\r\ncontinue;\r\nif (test_bit(BIO_UPTODATE, &r10_bio->devs[i].bio->bi_flags)) {\r\nint sectors = r10_bio->sectors;\r\nfor (j = 0; j < vcnt; j++) {\r\nint len = PAGE_SIZE;\r\nif (sectors < (len / 512))\r\nlen = sectors * 512;\r\nif (memcmp(page_address(fbio->bi_io_vec[j].bv_page),\r\npage_address(tbio->bi_io_vec[j].bv_page),\r\nlen))\r\nbreak;\r\nsectors -= len/512;\r\n}\r\nif (j == vcnt)\r\ncontinue;\r\natomic64_add(r10_bio->sectors, &mddev->resync_mismatches);\r\nif (test_bit(MD_RECOVERY_CHECK, &mddev->recovery))\r\ncontinue;\r\n}\r\nbio_reset(tbio);\r\ntbio->bi_vcnt = vcnt;\r\ntbio->bi_iter.bi_size = r10_bio->sectors << 9;\r\ntbio->bi_rw = WRITE;\r\ntbio->bi_private = r10_bio;\r\ntbio->bi_iter.bi_sector = r10_bio->devs[i].addr;\r\nfor (j=0; j < vcnt ; j++) {\r\ntbio->bi_io_vec[j].bv_offset = 0;\r\ntbio->bi_io_vec[j].bv_len = PAGE_SIZE;\r\nmemcpy(page_address(tbio->bi_io_vec[j].bv_page),\r\npage_address(fbio->bi_io_vec[j].bv_page),\r\nPAGE_SIZE);\r\n}\r\ntbio->bi_end_io = end_sync_write;\r\nd = r10_bio->devs[i].devnum;\r\natomic_inc(&conf->mirrors[d].rdev->nr_pending);\r\natomic_inc(&r10_bio->remaining);\r\nmd_sync_acct(conf->mirrors[d].rdev->bdev, bio_sectors(tbio));\r\ntbio->bi_iter.bi_sector += conf->mirrors[d].rdev->data_offset;\r\ntbio->bi_bdev = conf->mirrors[d].rdev->bdev;\r\ngeneric_make_request(tbio);\r\n}\r\nfor (i = 0; i < conf->copies; i++) {\r\nint j, d;\r\ntbio = r10_bio->devs[i].repl_bio;\r\nif (!tbio || !tbio->bi_end_io)\r\ncontinue;\r\nif (r10_bio->devs[i].bio->bi_end_io != end_sync_write\r\n&& r10_bio->devs[i].bio != fbio)\r\nfor (j = 0; j < vcnt; j++)\r\nmemcpy(page_address(tbio->bi_io_vec[j].bv_page),\r\npage_address(fbio->bi_io_vec[j].bv_page),\r\nPAGE_SIZE);\r\nd = r10_bio->devs[i].devnum;\r\natomic_inc(&r10_bio->remaining);\r\nmd_sync_acct(conf->mirrors[d].replacement->bdev,\r\nbio_sectors(tbio));\r\ngeneric_make_request(tbio);\r\n}\r\ndone:\r\nif (atomic_dec_and_test(&r10_bio->remaining)) {\r\nmd_done_sync(mddev, r10_bio->sectors, 1);\r\nput_buf(r10_bio);\r\n}\r\n}\r\nstatic void fix_recovery_read_error(struct r10bio *r10_bio)\r\n{\r\nstruct mddev *mddev = r10_bio->mddev;\r\nstruct r10conf *conf = mddev->private;\r\nstruct bio *bio = r10_bio->devs[0].bio;\r\nsector_t sect = 0;\r\nint sectors = r10_bio->sectors;\r\nint idx = 0;\r\nint dr = r10_bio->devs[0].devnum;\r\nint dw = r10_bio->devs[1].devnum;\r\nwhile (sectors) {\r\nint s = sectors;\r\nstruct md_rdev *rdev;\r\nsector_t addr;\r\nint ok;\r\nif (s > (PAGE_SIZE>>9))\r\ns = PAGE_SIZE >> 9;\r\nrdev = conf->mirrors[dr].rdev;\r\naddr = r10_bio->devs[0].addr + sect,\r\nok = sync_page_io(rdev,\r\naddr,\r\ns << 9,\r\nbio->bi_io_vec[idx].bv_page,\r\nREAD, false);\r\nif (ok) {\r\nrdev = conf->mirrors[dw].rdev;\r\naddr = r10_bio->devs[1].addr + sect;\r\nok = sync_page_io(rdev,\r\naddr,\r\ns << 9,\r\nbio->bi_io_vec[idx].bv_page,\r\nWRITE, false);\r\nif (!ok) {\r\nset_bit(WriteErrorSeen, &rdev->flags);\r\nif (!test_and_set_bit(WantReplacement,\r\n&rdev->flags))\r\nset_bit(MD_RECOVERY_NEEDED,\r\n&rdev->mddev->recovery);\r\n}\r\n}\r\nif (!ok) {\r\nrdev_set_badblocks(rdev, addr, s, 0);\r\nif (rdev != conf->mirrors[dw].rdev) {\r\nstruct md_rdev *rdev2 = conf->mirrors[dw].rdev;\r\naddr = r10_bio->devs[1].addr + sect;\r\nok = rdev_set_badblocks(rdev2, addr, s, 0);\r\nif (!ok) {\r\nprintk(KERN_NOTICE\r\n"md/raid10:%s: recovery aborted"\r\n" due to read error\n",\r\nmdname(mddev));\r\nconf->mirrors[dw].recovery_disabled\r\n= mddev->recovery_disabled;\r\nset_bit(MD_RECOVERY_INTR,\r\n&mddev->recovery);\r\nbreak;\r\n}\r\n}\r\n}\r\nsectors -= s;\r\nsect += s;\r\nidx++;\r\n}\r\n}\r\nstatic void recovery_request_write(struct mddev *mddev, struct r10bio *r10_bio)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nint d;\r\nstruct bio *wbio, *wbio2;\r\nif (!test_bit(R10BIO_Uptodate, &r10_bio->state)) {\r\nfix_recovery_read_error(r10_bio);\r\nend_sync_request(r10_bio);\r\nreturn;\r\n}\r\nd = r10_bio->devs[1].devnum;\r\nwbio = r10_bio->devs[1].bio;\r\nwbio2 = r10_bio->devs[1].repl_bio;\r\nif (wbio2 && !wbio2->bi_end_io)\r\nwbio2 = NULL;\r\nif (wbio->bi_end_io) {\r\natomic_inc(&conf->mirrors[d].rdev->nr_pending);\r\nmd_sync_acct(conf->mirrors[d].rdev->bdev, bio_sectors(wbio));\r\ngeneric_make_request(wbio);\r\n}\r\nif (wbio2) {\r\natomic_inc(&conf->mirrors[d].replacement->nr_pending);\r\nmd_sync_acct(conf->mirrors[d].replacement->bdev,\r\nbio_sectors(wbio2));\r\ngeneric_make_request(wbio2);\r\n}\r\n}\r\nstatic void check_decay_read_errors(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nstruct timespec cur_time_mon;\r\nunsigned long hours_since_last;\r\nunsigned int read_errors = atomic_read(&rdev->read_errors);\r\nktime_get_ts(&cur_time_mon);\r\nif (rdev->last_read_error.tv_sec == 0 &&\r\nrdev->last_read_error.tv_nsec == 0) {\r\nrdev->last_read_error = cur_time_mon;\r\nreturn;\r\n}\r\nhours_since_last = (cur_time_mon.tv_sec -\r\nrdev->last_read_error.tv_sec) / 3600;\r\nrdev->last_read_error = cur_time_mon;\r\nif (hours_since_last >= 8 * sizeof(read_errors))\r\natomic_set(&rdev->read_errors, 0);\r\nelse\r\natomic_set(&rdev->read_errors, read_errors >> hours_since_last);\r\n}\r\nstatic int r10_sync_page_io(struct md_rdev *rdev, sector_t sector,\r\nint sectors, struct page *page, int rw)\r\n{\r\nsector_t first_bad;\r\nint bad_sectors;\r\nif (is_badblock(rdev, sector, sectors, &first_bad, &bad_sectors)\r\n&& (rw == READ || test_bit(WriteErrorSeen, &rdev->flags)))\r\nreturn -1;\r\nif (sync_page_io(rdev, sector, sectors << 9, page, rw, false))\r\nreturn 1;\r\nif (rw == WRITE) {\r\nset_bit(WriteErrorSeen, &rdev->flags);\r\nif (!test_and_set_bit(WantReplacement, &rdev->flags))\r\nset_bit(MD_RECOVERY_NEEDED,\r\n&rdev->mddev->recovery);\r\n}\r\nif (!rdev_set_badblocks(rdev, sector, sectors, 0))\r\nmd_error(rdev->mddev, rdev);\r\nreturn 0;\r\n}\r\nstatic void fix_read_error(struct r10conf *conf, struct mddev *mddev, struct r10bio *r10_bio)\r\n{\r\nint sect = 0;\r\nint sectors = r10_bio->sectors;\r\nstruct md_rdev*rdev;\r\nint max_read_errors = atomic_read(&mddev->max_corr_read_errors);\r\nint d = r10_bio->devs[r10_bio->read_slot].devnum;\r\nrdev = conf->mirrors[d].rdev;\r\nif (test_bit(Faulty, &rdev->flags))\r\nreturn;\r\ncheck_decay_read_errors(mddev, rdev);\r\natomic_inc(&rdev->read_errors);\r\nif (atomic_read(&rdev->read_errors) > max_read_errors) {\r\nchar b[BDEVNAME_SIZE];\r\nbdevname(rdev->bdev, b);\r\nprintk(KERN_NOTICE\r\n"md/raid10:%s: %s: Raid device exceeded "\r\n"read_error threshold [cur %d:max %d]\n",\r\nmdname(mddev), b,\r\natomic_read(&rdev->read_errors), max_read_errors);\r\nprintk(KERN_NOTICE\r\n"md/raid10:%s: %s: Failing raid device\n",\r\nmdname(mddev), b);\r\nmd_error(mddev, conf->mirrors[d].rdev);\r\nr10_bio->devs[r10_bio->read_slot].bio = IO_BLOCKED;\r\nreturn;\r\n}\r\nwhile(sectors) {\r\nint s = sectors;\r\nint sl = r10_bio->read_slot;\r\nint success = 0;\r\nint start;\r\nif (s > (PAGE_SIZE>>9))\r\ns = PAGE_SIZE >> 9;\r\nrcu_read_lock();\r\ndo {\r\nsector_t first_bad;\r\nint bad_sectors;\r\nd = r10_bio->devs[sl].devnum;\r\nrdev = rcu_dereference(conf->mirrors[d].rdev);\r\nif (rdev &&\r\n!test_bit(Unmerged, &rdev->flags) &&\r\ntest_bit(In_sync, &rdev->flags) &&\r\nis_badblock(rdev, r10_bio->devs[sl].addr + sect, s,\r\n&first_bad, &bad_sectors) == 0) {\r\natomic_inc(&rdev->nr_pending);\r\nrcu_read_unlock();\r\nsuccess = sync_page_io(rdev,\r\nr10_bio->devs[sl].addr +\r\nsect,\r\ns<<9,\r\nconf->tmppage, READ, false);\r\nrdev_dec_pending(rdev, mddev);\r\nrcu_read_lock();\r\nif (success)\r\nbreak;\r\n}\r\nsl++;\r\nif (sl == conf->copies)\r\nsl = 0;\r\n} while (!success && sl != r10_bio->read_slot);\r\nrcu_read_unlock();\r\nif (!success) {\r\nint dn = r10_bio->devs[r10_bio->read_slot].devnum;\r\nrdev = conf->mirrors[dn].rdev;\r\nif (!rdev_set_badblocks(\r\nrdev,\r\nr10_bio->devs[r10_bio->read_slot].addr\r\n+ sect,\r\ns, 0)) {\r\nmd_error(mddev, rdev);\r\nr10_bio->devs[r10_bio->read_slot].bio\r\n= IO_BLOCKED;\r\n}\r\nbreak;\r\n}\r\nstart = sl;\r\nrcu_read_lock();\r\nwhile (sl != r10_bio->read_slot) {\r\nchar b[BDEVNAME_SIZE];\r\nif (sl==0)\r\nsl = conf->copies;\r\nsl--;\r\nd = r10_bio->devs[sl].devnum;\r\nrdev = rcu_dereference(conf->mirrors[d].rdev);\r\nif (!rdev ||\r\ntest_bit(Unmerged, &rdev->flags) ||\r\n!test_bit(In_sync, &rdev->flags))\r\ncontinue;\r\natomic_inc(&rdev->nr_pending);\r\nrcu_read_unlock();\r\nif (r10_sync_page_io(rdev,\r\nr10_bio->devs[sl].addr +\r\nsect,\r\ns, conf->tmppage, WRITE)\r\n== 0) {\r\nprintk(KERN_NOTICE\r\n"md/raid10:%s: read correction "\r\n"write failed"\r\n" (%d sectors at %llu on %s)\n",\r\nmdname(mddev), s,\r\n(unsigned long long)(\r\nsect +\r\nchoose_data_offset(r10_bio,\r\nrdev)),\r\nbdevname(rdev->bdev, b));\r\nprintk(KERN_NOTICE "md/raid10:%s: %s: failing "\r\n"drive\n",\r\nmdname(mddev),\r\nbdevname(rdev->bdev, b));\r\n}\r\nrdev_dec_pending(rdev, mddev);\r\nrcu_read_lock();\r\n}\r\nsl = start;\r\nwhile (sl != r10_bio->read_slot) {\r\nchar b[BDEVNAME_SIZE];\r\nif (sl==0)\r\nsl = conf->copies;\r\nsl--;\r\nd = r10_bio->devs[sl].devnum;\r\nrdev = rcu_dereference(conf->mirrors[d].rdev);\r\nif (!rdev ||\r\n!test_bit(In_sync, &rdev->flags))\r\ncontinue;\r\natomic_inc(&rdev->nr_pending);\r\nrcu_read_unlock();\r\nswitch (r10_sync_page_io(rdev,\r\nr10_bio->devs[sl].addr +\r\nsect,\r\ns, conf->tmppage,\r\nREAD)) {\r\ncase 0:\r\nprintk(KERN_NOTICE\r\n"md/raid10:%s: unable to read back "\r\n"corrected sectors"\r\n" (%d sectors at %llu on %s)\n",\r\nmdname(mddev), s,\r\n(unsigned long long)(\r\nsect +\r\nchoose_data_offset(r10_bio, rdev)),\r\nbdevname(rdev->bdev, b));\r\nprintk(KERN_NOTICE "md/raid10:%s: %s: failing "\r\n"drive\n",\r\nmdname(mddev),\r\nbdevname(rdev->bdev, b));\r\nbreak;\r\ncase 1:\r\nprintk(KERN_INFO\r\n"md/raid10:%s: read error corrected"\r\n" (%d sectors at %llu on %s)\n",\r\nmdname(mddev), s,\r\n(unsigned long long)(\r\nsect +\r\nchoose_data_offset(r10_bio, rdev)),\r\nbdevname(rdev->bdev, b));\r\natomic_add(s, &rdev->corrected_errors);\r\n}\r\nrdev_dec_pending(rdev, mddev);\r\nrcu_read_lock();\r\n}\r\nrcu_read_unlock();\r\nsectors -= s;\r\nsect += s;\r\n}\r\n}\r\nstatic int narrow_write_error(struct r10bio *r10_bio, int i)\r\n{\r\nstruct bio *bio = r10_bio->master_bio;\r\nstruct mddev *mddev = r10_bio->mddev;\r\nstruct r10conf *conf = mddev->private;\r\nstruct md_rdev *rdev = conf->mirrors[r10_bio->devs[i].devnum].rdev;\r\nint block_sectors;\r\nsector_t sector;\r\nint sectors;\r\nint sect_to_write = r10_bio->sectors;\r\nint ok = 1;\r\nif (rdev->badblocks.shift < 0)\r\nreturn 0;\r\nblock_sectors = 1 << rdev->badblocks.shift;\r\nsector = r10_bio->sector;\r\nsectors = ((r10_bio->sector + block_sectors)\r\n& ~(sector_t)(block_sectors - 1))\r\n- sector;\r\nwhile (sect_to_write) {\r\nstruct bio *wbio;\r\nif (sectors > sect_to_write)\r\nsectors = sect_to_write;\r\nwbio = bio_clone_mddev(bio, GFP_NOIO, mddev);\r\nbio_trim(wbio, sector - bio->bi_iter.bi_sector, sectors);\r\nwbio->bi_iter.bi_sector = (r10_bio->devs[i].addr+\r\nchoose_data_offset(r10_bio, rdev) +\r\n(sector - r10_bio->sector));\r\nwbio->bi_bdev = rdev->bdev;\r\nif (submit_bio_wait(WRITE, wbio) == 0)\r\nok = rdev_set_badblocks(rdev, sector,\r\nsectors, 0)\r\n&& ok;\r\nbio_put(wbio);\r\nsect_to_write -= sectors;\r\nsector += sectors;\r\nsectors = block_sectors;\r\n}\r\nreturn ok;\r\n}\r\nstatic void handle_read_error(struct mddev *mddev, struct r10bio *r10_bio)\r\n{\r\nint slot = r10_bio->read_slot;\r\nstruct bio *bio;\r\nstruct r10conf *conf = mddev->private;\r\nstruct md_rdev *rdev = r10_bio->devs[slot].rdev;\r\nchar b[BDEVNAME_SIZE];\r\nunsigned long do_sync;\r\nint max_sectors;\r\nbio = r10_bio->devs[slot].bio;\r\nbdevname(bio->bi_bdev, b);\r\nbio_put(bio);\r\nr10_bio->devs[slot].bio = NULL;\r\nif (mddev->ro == 0) {\r\nfreeze_array(conf, 1);\r\nfix_read_error(conf, mddev, r10_bio);\r\nunfreeze_array(conf);\r\n} else\r\nr10_bio->devs[slot].bio = IO_BLOCKED;\r\nrdev_dec_pending(rdev, mddev);\r\nread_more:\r\nrdev = read_balance(conf, r10_bio, &max_sectors);\r\nif (rdev == NULL) {\r\nprintk(KERN_ALERT "md/raid10:%s: %s: unrecoverable I/O"\r\n" read error for block %llu\n",\r\nmdname(mddev), b,\r\n(unsigned long long)r10_bio->sector);\r\nraid_end_bio_io(r10_bio);\r\nreturn;\r\n}\r\ndo_sync = (r10_bio->master_bio->bi_rw & REQ_SYNC);\r\nslot = r10_bio->read_slot;\r\nprintk_ratelimited(\r\nKERN_ERR\r\n"md/raid10:%s: %s: redirecting "\r\n"sector %llu to another mirror\n",\r\nmdname(mddev),\r\nbdevname(rdev->bdev, b),\r\n(unsigned long long)r10_bio->sector);\r\nbio = bio_clone_mddev(r10_bio->master_bio,\r\nGFP_NOIO, mddev);\r\nbio_trim(bio, r10_bio->sector - bio->bi_iter.bi_sector, max_sectors);\r\nr10_bio->devs[slot].bio = bio;\r\nr10_bio->devs[slot].rdev = rdev;\r\nbio->bi_iter.bi_sector = r10_bio->devs[slot].addr\r\n+ choose_data_offset(r10_bio, rdev);\r\nbio->bi_bdev = rdev->bdev;\r\nbio->bi_rw = READ | do_sync;\r\nbio->bi_private = r10_bio;\r\nbio->bi_end_io = raid10_end_read_request;\r\nif (max_sectors < r10_bio->sectors) {\r\nstruct bio *mbio = r10_bio->master_bio;\r\nint sectors_handled =\r\nr10_bio->sector + max_sectors\r\n- mbio->bi_iter.bi_sector;\r\nr10_bio->sectors = max_sectors;\r\nspin_lock_irq(&conf->device_lock);\r\nif (mbio->bi_phys_segments == 0)\r\nmbio->bi_phys_segments = 2;\r\nelse\r\nmbio->bi_phys_segments++;\r\nspin_unlock_irq(&conf->device_lock);\r\ngeneric_make_request(bio);\r\nr10_bio = mempool_alloc(conf->r10bio_pool,\r\nGFP_NOIO);\r\nr10_bio->master_bio = mbio;\r\nr10_bio->sectors = bio_sectors(mbio) - sectors_handled;\r\nr10_bio->state = 0;\r\nset_bit(R10BIO_ReadError,\r\n&r10_bio->state);\r\nr10_bio->mddev = mddev;\r\nr10_bio->sector = mbio->bi_iter.bi_sector\r\n+ sectors_handled;\r\ngoto read_more;\r\n} else\r\ngeneric_make_request(bio);\r\n}\r\nstatic void handle_write_completed(struct r10conf *conf, struct r10bio *r10_bio)\r\n{\r\nint m;\r\nstruct md_rdev *rdev;\r\nif (test_bit(R10BIO_IsSync, &r10_bio->state) ||\r\ntest_bit(R10BIO_IsRecover, &r10_bio->state)) {\r\nfor (m = 0; m < conf->copies; m++) {\r\nint dev = r10_bio->devs[m].devnum;\r\nrdev = conf->mirrors[dev].rdev;\r\nif (r10_bio->devs[m].bio == NULL)\r\ncontinue;\r\nif (test_bit(BIO_UPTODATE,\r\n&r10_bio->devs[m].bio->bi_flags)) {\r\nrdev_clear_badblocks(\r\nrdev,\r\nr10_bio->devs[m].addr,\r\nr10_bio->sectors, 0);\r\n} else {\r\nif (!rdev_set_badblocks(\r\nrdev,\r\nr10_bio->devs[m].addr,\r\nr10_bio->sectors, 0))\r\nmd_error(conf->mddev, rdev);\r\n}\r\nrdev = conf->mirrors[dev].replacement;\r\nif (r10_bio->devs[m].repl_bio == NULL)\r\ncontinue;\r\nif (test_bit(BIO_UPTODATE,\r\n&r10_bio->devs[m].repl_bio->bi_flags)) {\r\nrdev_clear_badblocks(\r\nrdev,\r\nr10_bio->devs[m].addr,\r\nr10_bio->sectors, 0);\r\n} else {\r\nif (!rdev_set_badblocks(\r\nrdev,\r\nr10_bio->devs[m].addr,\r\nr10_bio->sectors, 0))\r\nmd_error(conf->mddev, rdev);\r\n}\r\n}\r\nput_buf(r10_bio);\r\n} else {\r\nfor (m = 0; m < conf->copies; m++) {\r\nint dev = r10_bio->devs[m].devnum;\r\nstruct bio *bio = r10_bio->devs[m].bio;\r\nrdev = conf->mirrors[dev].rdev;\r\nif (bio == IO_MADE_GOOD) {\r\nrdev_clear_badblocks(\r\nrdev,\r\nr10_bio->devs[m].addr,\r\nr10_bio->sectors, 0);\r\nrdev_dec_pending(rdev, conf->mddev);\r\n} else if (bio != NULL &&\r\n!test_bit(BIO_UPTODATE, &bio->bi_flags)) {\r\nif (!narrow_write_error(r10_bio, m)) {\r\nmd_error(conf->mddev, rdev);\r\nset_bit(R10BIO_Degraded,\r\n&r10_bio->state);\r\n}\r\nrdev_dec_pending(rdev, conf->mddev);\r\n}\r\nbio = r10_bio->devs[m].repl_bio;\r\nrdev = conf->mirrors[dev].replacement;\r\nif (rdev && bio == IO_MADE_GOOD) {\r\nrdev_clear_badblocks(\r\nrdev,\r\nr10_bio->devs[m].addr,\r\nr10_bio->sectors, 0);\r\nrdev_dec_pending(rdev, conf->mddev);\r\n}\r\n}\r\nif (test_bit(R10BIO_WriteError,\r\n&r10_bio->state))\r\nclose_write(r10_bio);\r\nraid_end_bio_io(r10_bio);\r\n}\r\n}\r\nstatic void raid10d(struct md_thread *thread)\r\n{\r\nstruct mddev *mddev = thread->mddev;\r\nstruct r10bio *r10_bio;\r\nunsigned long flags;\r\nstruct r10conf *conf = mddev->private;\r\nstruct list_head *head = &conf->retry_list;\r\nstruct blk_plug plug;\r\nmd_check_recovery(mddev);\r\nblk_start_plug(&plug);\r\nfor (;;) {\r\nflush_pending_writes(conf);\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nif (list_empty(head)) {\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nbreak;\r\n}\r\nr10_bio = list_entry(head->prev, struct r10bio, retry_list);\r\nlist_del(head->prev);\r\nconf->nr_queued--;\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nmddev = r10_bio->mddev;\r\nconf = mddev->private;\r\nif (test_bit(R10BIO_MadeGood, &r10_bio->state) ||\r\ntest_bit(R10BIO_WriteError, &r10_bio->state))\r\nhandle_write_completed(conf, r10_bio);\r\nelse if (test_bit(R10BIO_IsReshape, &r10_bio->state))\r\nreshape_request_write(mddev, r10_bio);\r\nelse if (test_bit(R10BIO_IsSync, &r10_bio->state))\r\nsync_request_write(mddev, r10_bio);\r\nelse if (test_bit(R10BIO_IsRecover, &r10_bio->state))\r\nrecovery_request_write(mddev, r10_bio);\r\nelse if (test_bit(R10BIO_ReadError, &r10_bio->state))\r\nhandle_read_error(mddev, r10_bio);\r\nelse {\r\nint slot = r10_bio->read_slot;\r\ngeneric_make_request(r10_bio->devs[slot].bio);\r\n}\r\ncond_resched();\r\nif (mddev->flags & ~(1<<MD_CHANGE_PENDING))\r\nmd_check_recovery(mddev);\r\n}\r\nblk_finish_plug(&plug);\r\n}\r\nstatic int init_resync(struct r10conf *conf)\r\n{\r\nint buffs;\r\nint i;\r\nbuffs = RESYNC_WINDOW / RESYNC_BLOCK_SIZE;\r\nBUG_ON(conf->r10buf_pool);\r\nconf->have_replacement = 0;\r\nfor (i = 0; i < conf->geo.raid_disks; i++)\r\nif (conf->mirrors[i].replacement)\r\nconf->have_replacement = 1;\r\nconf->r10buf_pool = mempool_create(buffs, r10buf_pool_alloc, r10buf_pool_free, conf);\r\nif (!conf->r10buf_pool)\r\nreturn -ENOMEM;\r\nconf->next_resync = 0;\r\nreturn 0;\r\n}\r\nstatic sector_t sync_request(struct mddev *mddev, sector_t sector_nr,\r\nint *skipped, int go_faster)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nstruct r10bio *r10_bio;\r\nstruct bio *biolist = NULL, *bio;\r\nsector_t max_sector, nr_sectors;\r\nint i;\r\nint max_sync;\r\nsector_t sync_blocks;\r\nsector_t sectors_skipped = 0;\r\nint chunks_skipped = 0;\r\nsector_t chunk_mask = conf->geo.chunk_mask;\r\nif (!conf->r10buf_pool)\r\nif (init_resync(conf))\r\nreturn 0;\r\nif (mddev->bitmap == NULL &&\r\nmddev->recovery_cp == MaxSector &&\r\nmddev->reshape_position == MaxSector &&\r\n!test_bit(MD_RECOVERY_SYNC, &mddev->recovery) &&\r\n!test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&\r\n!test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\r\nconf->fullsync == 0) {\r\n*skipped = 1;\r\nreturn mddev->dev_sectors - sector_nr;\r\n}\r\nskipped:\r\nmax_sector = mddev->dev_sectors;\r\nif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery) ||\r\ntest_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\r\nmax_sector = mddev->resync_max_sectors;\r\nif (sector_nr >= max_sector) {\r\nif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery)) {\r\nend_reshape(conf);\r\nclose_sync(conf);\r\nreturn 0;\r\n}\r\nif (mddev->curr_resync < max_sector) {\r\nif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery))\r\nbitmap_end_sync(mddev->bitmap, mddev->curr_resync,\r\n&sync_blocks, 1);\r\nelse for (i = 0; i < conf->geo.raid_disks; i++) {\r\nsector_t sect =\r\nraid10_find_virt(conf, mddev->curr_resync, i);\r\nbitmap_end_sync(mddev->bitmap, sect,\r\n&sync_blocks, 1);\r\n}\r\n} else {\r\nif ((!mddev->bitmap || conf->fullsync)\r\n&& conf->have_replacement\r\n&& test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\r\nfor (i = 0; i < conf->geo.raid_disks; i++)\r\nif (conf->mirrors[i].replacement)\r\nconf->mirrors[i].replacement\r\n->recovery_offset\r\n= MaxSector;\r\n}\r\nconf->fullsync = 0;\r\n}\r\nbitmap_close_sync(mddev->bitmap);\r\nclose_sync(conf);\r\n*skipped = 1;\r\nreturn sectors_skipped;\r\n}\r\nif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\r\nreturn reshape_request(mddev, sector_nr, skipped);\r\nif (chunks_skipped >= conf->geo.raid_disks) {\r\n*skipped = 1;\r\nreturn (max_sector - sector_nr) + sectors_skipped;\r\n}\r\nif (max_sector > mddev->resync_max)\r\nmax_sector = mddev->resync_max;\r\nif (conf->geo.near_copies < conf->geo.raid_disks &&\r\nmax_sector > (sector_nr | chunk_mask))\r\nmax_sector = (sector_nr | chunk_mask) + 1;\r\nif (!go_faster && conf->nr_waiting)\r\nmsleep_interruptible(1000);\r\nmax_sync = RESYNC_PAGES << (PAGE_SHIFT-9);\r\nif (!test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\r\nint j;\r\nr10_bio = NULL;\r\nfor (i = 0 ; i < conf->geo.raid_disks; i++) {\r\nint still_degraded;\r\nstruct r10bio *rb2;\r\nsector_t sect;\r\nint must_sync;\r\nint any_working;\r\nstruct raid10_info *mirror = &conf->mirrors[i];\r\nif ((mirror->rdev == NULL ||\r\ntest_bit(In_sync, &mirror->rdev->flags))\r\n&&\r\n(mirror->replacement == NULL ||\r\ntest_bit(Faulty,\r\n&mirror->replacement->flags)))\r\ncontinue;\r\nstill_degraded = 0;\r\nrb2 = r10_bio;\r\nsect = raid10_find_virt(conf, sector_nr, i);\r\nif (sect >= mddev->resync_max_sectors) {\r\ncontinue;\r\n}\r\nmust_sync = bitmap_start_sync(mddev->bitmap, sect,\r\n&sync_blocks, 1);\r\nif (sync_blocks < max_sync)\r\nmax_sync = sync_blocks;\r\nif (!must_sync &&\r\nmirror->replacement == NULL &&\r\n!conf->fullsync) {\r\nchunks_skipped = -1;\r\ncontinue;\r\n}\r\nr10_bio = mempool_alloc(conf->r10buf_pool, GFP_NOIO);\r\nr10_bio->state = 0;\r\nraise_barrier(conf, rb2 != NULL);\r\natomic_set(&r10_bio->remaining, 0);\r\nr10_bio->master_bio = (struct bio*)rb2;\r\nif (rb2)\r\natomic_inc(&rb2->remaining);\r\nr10_bio->mddev = mddev;\r\nset_bit(R10BIO_IsRecover, &r10_bio->state);\r\nr10_bio->sector = sect;\r\nraid10_find_phys(conf, r10_bio);\r\nfor (j = 0; j < conf->geo.raid_disks; j++)\r\nif (conf->mirrors[j].rdev == NULL ||\r\ntest_bit(Faulty, &conf->mirrors[j].rdev->flags)) {\r\nstill_degraded = 1;\r\nbreak;\r\n}\r\nmust_sync = bitmap_start_sync(mddev->bitmap, sect,\r\n&sync_blocks, still_degraded);\r\nany_working = 0;\r\nfor (j=0; j<conf->copies;j++) {\r\nint k;\r\nint d = r10_bio->devs[j].devnum;\r\nsector_t from_addr, to_addr;\r\nstruct md_rdev *rdev;\r\nsector_t sector, first_bad;\r\nint bad_sectors;\r\nif (!conf->mirrors[d].rdev ||\r\n!test_bit(In_sync, &conf->mirrors[d].rdev->flags))\r\ncontinue;\r\nany_working = 1;\r\nrdev = conf->mirrors[d].rdev;\r\nsector = r10_bio->devs[j].addr;\r\nif (is_badblock(rdev, sector, max_sync,\r\n&first_bad, &bad_sectors)) {\r\nif (first_bad > sector)\r\nmax_sync = first_bad - sector;\r\nelse {\r\nbad_sectors -= (sector\r\n- first_bad);\r\nif (max_sync > bad_sectors)\r\nmax_sync = bad_sectors;\r\ncontinue;\r\n}\r\n}\r\nbio = r10_bio->devs[0].bio;\r\nbio_reset(bio);\r\nbio->bi_next = biolist;\r\nbiolist = bio;\r\nbio->bi_private = r10_bio;\r\nbio->bi_end_io = end_sync_read;\r\nbio->bi_rw = READ;\r\nfrom_addr = r10_bio->devs[j].addr;\r\nbio->bi_iter.bi_sector = from_addr +\r\nrdev->data_offset;\r\nbio->bi_bdev = rdev->bdev;\r\natomic_inc(&rdev->nr_pending);\r\nfor (k=0; k<conf->copies; k++)\r\nif (r10_bio->devs[k].devnum == i)\r\nbreak;\r\nBUG_ON(k == conf->copies);\r\nto_addr = r10_bio->devs[k].addr;\r\nr10_bio->devs[0].devnum = d;\r\nr10_bio->devs[0].addr = from_addr;\r\nr10_bio->devs[1].devnum = i;\r\nr10_bio->devs[1].addr = to_addr;\r\nrdev = mirror->rdev;\r\nif (!test_bit(In_sync, &rdev->flags)) {\r\nbio = r10_bio->devs[1].bio;\r\nbio_reset(bio);\r\nbio->bi_next = biolist;\r\nbiolist = bio;\r\nbio->bi_private = r10_bio;\r\nbio->bi_end_io = end_sync_write;\r\nbio->bi_rw = WRITE;\r\nbio->bi_iter.bi_sector = to_addr\r\n+ rdev->data_offset;\r\nbio->bi_bdev = rdev->bdev;\r\natomic_inc(&r10_bio->remaining);\r\n} else\r\nr10_bio->devs[1].bio->bi_end_io = NULL;\r\nbio = r10_bio->devs[1].repl_bio;\r\nif (bio)\r\nbio->bi_end_io = NULL;\r\nrdev = mirror->replacement;\r\nif (rdev == NULL || bio == NULL ||\r\ntest_bit(Faulty, &rdev->flags))\r\nbreak;\r\nbio_reset(bio);\r\nbio->bi_next = biolist;\r\nbiolist = bio;\r\nbio->bi_private = r10_bio;\r\nbio->bi_end_io = end_sync_write;\r\nbio->bi_rw = WRITE;\r\nbio->bi_iter.bi_sector = to_addr +\r\nrdev->data_offset;\r\nbio->bi_bdev = rdev->bdev;\r\natomic_inc(&r10_bio->remaining);\r\nbreak;\r\n}\r\nif (j == conf->copies) {\r\nif (any_working) {\r\nint k;\r\nfor (k = 0; k < conf->copies; k++)\r\nif (r10_bio->devs[k].devnum == i)\r\nbreak;\r\nif (!test_bit(In_sync,\r\n&mirror->rdev->flags)\r\n&& !rdev_set_badblocks(\r\nmirror->rdev,\r\nr10_bio->devs[k].addr,\r\nmax_sync, 0))\r\nany_working = 0;\r\nif (mirror->replacement &&\r\n!rdev_set_badblocks(\r\nmirror->replacement,\r\nr10_bio->devs[k].addr,\r\nmax_sync, 0))\r\nany_working = 0;\r\n}\r\nif (!any_working) {\r\nif (!test_and_set_bit(MD_RECOVERY_INTR,\r\n&mddev->recovery))\r\nprintk(KERN_INFO "md/raid10:%s: insufficient "\r\n"working devices for recovery.\n",\r\nmdname(mddev));\r\nmirror->recovery_disabled\r\n= mddev->recovery_disabled;\r\n}\r\nput_buf(r10_bio);\r\nif (rb2)\r\natomic_dec(&rb2->remaining);\r\nr10_bio = rb2;\r\nbreak;\r\n}\r\n}\r\nif (biolist == NULL) {\r\nwhile (r10_bio) {\r\nstruct r10bio *rb2 = r10_bio;\r\nr10_bio = (struct r10bio*) rb2->master_bio;\r\nrb2->master_bio = NULL;\r\nput_buf(rb2);\r\n}\r\ngoto giveup;\r\n}\r\n} else {\r\nint count = 0;\r\nbitmap_cond_end_sync(mddev->bitmap, sector_nr);\r\nif (!bitmap_start_sync(mddev->bitmap, sector_nr,\r\n&sync_blocks, mddev->degraded) &&\r\n!conf->fullsync && !test_bit(MD_RECOVERY_REQUESTED,\r\n&mddev->recovery)) {\r\n*skipped = 1;\r\nreturn sync_blocks + sectors_skipped;\r\n}\r\nif (sync_blocks < max_sync)\r\nmax_sync = sync_blocks;\r\nr10_bio = mempool_alloc(conf->r10buf_pool, GFP_NOIO);\r\nr10_bio->state = 0;\r\nr10_bio->mddev = mddev;\r\natomic_set(&r10_bio->remaining, 0);\r\nraise_barrier(conf, 0);\r\nconf->next_resync = sector_nr;\r\nr10_bio->master_bio = NULL;\r\nr10_bio->sector = sector_nr;\r\nset_bit(R10BIO_IsSync, &r10_bio->state);\r\nraid10_find_phys(conf, r10_bio);\r\nr10_bio->sectors = (sector_nr | chunk_mask) - sector_nr + 1;\r\nfor (i = 0; i < conf->copies; i++) {\r\nint d = r10_bio->devs[i].devnum;\r\nsector_t first_bad, sector;\r\nint bad_sectors;\r\nif (r10_bio->devs[i].repl_bio)\r\nr10_bio->devs[i].repl_bio->bi_end_io = NULL;\r\nbio = r10_bio->devs[i].bio;\r\nbio_reset(bio);\r\nclear_bit(BIO_UPTODATE, &bio->bi_flags);\r\nif (conf->mirrors[d].rdev == NULL ||\r\ntest_bit(Faulty, &conf->mirrors[d].rdev->flags))\r\ncontinue;\r\nsector = r10_bio->devs[i].addr;\r\nif (is_badblock(conf->mirrors[d].rdev,\r\nsector, max_sync,\r\n&first_bad, &bad_sectors)) {\r\nif (first_bad > sector)\r\nmax_sync = first_bad - sector;\r\nelse {\r\nbad_sectors -= (sector - first_bad);\r\nif (max_sync > bad_sectors)\r\nmax_sync = bad_sectors;\r\ncontinue;\r\n}\r\n}\r\natomic_inc(&conf->mirrors[d].rdev->nr_pending);\r\natomic_inc(&r10_bio->remaining);\r\nbio->bi_next = biolist;\r\nbiolist = bio;\r\nbio->bi_private = r10_bio;\r\nbio->bi_end_io = end_sync_read;\r\nbio->bi_rw = READ;\r\nbio->bi_iter.bi_sector = sector +\r\nconf->mirrors[d].rdev->data_offset;\r\nbio->bi_bdev = conf->mirrors[d].rdev->bdev;\r\ncount++;\r\nif (conf->mirrors[d].replacement == NULL ||\r\ntest_bit(Faulty,\r\n&conf->mirrors[d].replacement->flags))\r\ncontinue;\r\nbio = r10_bio->devs[i].repl_bio;\r\nbio_reset(bio);\r\nclear_bit(BIO_UPTODATE, &bio->bi_flags);\r\nsector = r10_bio->devs[i].addr;\r\natomic_inc(&conf->mirrors[d].rdev->nr_pending);\r\nbio->bi_next = biolist;\r\nbiolist = bio;\r\nbio->bi_private = r10_bio;\r\nbio->bi_end_io = end_sync_write;\r\nbio->bi_rw = WRITE;\r\nbio->bi_iter.bi_sector = sector +\r\nconf->mirrors[d].replacement->data_offset;\r\nbio->bi_bdev = conf->mirrors[d].replacement->bdev;\r\ncount++;\r\n}\r\nif (count < 2) {\r\nfor (i=0; i<conf->copies; i++) {\r\nint d = r10_bio->devs[i].devnum;\r\nif (r10_bio->devs[i].bio->bi_end_io)\r\nrdev_dec_pending(conf->mirrors[d].rdev,\r\nmddev);\r\nif (r10_bio->devs[i].repl_bio &&\r\nr10_bio->devs[i].repl_bio->bi_end_io)\r\nrdev_dec_pending(\r\nconf->mirrors[d].replacement,\r\nmddev);\r\n}\r\nput_buf(r10_bio);\r\nbiolist = NULL;\r\ngoto giveup;\r\n}\r\n}\r\nnr_sectors = 0;\r\nif (sector_nr + max_sync < max_sector)\r\nmax_sector = sector_nr + max_sync;\r\ndo {\r\nstruct page *page;\r\nint len = PAGE_SIZE;\r\nif (sector_nr + (len>>9) > max_sector)\r\nlen = (max_sector - sector_nr) << 9;\r\nif (len == 0)\r\nbreak;\r\nfor (bio= biolist ; bio ; bio=bio->bi_next) {\r\nstruct bio *bio2;\r\npage = bio->bi_io_vec[bio->bi_vcnt].bv_page;\r\nif (bio_add_page(bio, page, len, 0))\r\ncontinue;\r\nbio->bi_io_vec[bio->bi_vcnt].bv_page = page;\r\nfor (bio2 = biolist;\r\nbio2 && bio2 != bio;\r\nbio2 = bio2->bi_next) {\r\nbio2->bi_vcnt--;\r\nbio2->bi_iter.bi_size -= len;\r\n__clear_bit(BIO_SEG_VALID, &bio2->bi_flags);\r\n}\r\ngoto bio_full;\r\n}\r\nnr_sectors += len>>9;\r\nsector_nr += len>>9;\r\n} while (biolist->bi_vcnt < RESYNC_PAGES);\r\nbio_full:\r\nr10_bio->sectors = nr_sectors;\r\nwhile (biolist) {\r\nbio = biolist;\r\nbiolist = biolist->bi_next;\r\nbio->bi_next = NULL;\r\nr10_bio = bio->bi_private;\r\nr10_bio->sectors = nr_sectors;\r\nif (bio->bi_end_io == end_sync_read) {\r\nmd_sync_acct(bio->bi_bdev, nr_sectors);\r\nset_bit(BIO_UPTODATE, &bio->bi_flags);\r\ngeneric_make_request(bio);\r\n}\r\n}\r\nif (sectors_skipped)\r\nmd_done_sync(mddev, sectors_skipped, 1);\r\nreturn sectors_skipped + nr_sectors;\r\ngiveup:\r\nif (sector_nr + max_sync < max_sector)\r\nmax_sector = sector_nr + max_sync;\r\nsectors_skipped += (max_sector - sector_nr);\r\nchunks_skipped ++;\r\nsector_nr = max_sector;\r\ngoto skipped;\r\n}\r\nstatic sector_t\r\nraid10_size(struct mddev *mddev, sector_t sectors, int raid_disks)\r\n{\r\nsector_t size;\r\nstruct r10conf *conf = mddev->private;\r\nif (!raid_disks)\r\nraid_disks = min(conf->geo.raid_disks,\r\nconf->prev.raid_disks);\r\nif (!sectors)\r\nsectors = conf->dev_sectors;\r\nsize = sectors >> conf->geo.chunk_shift;\r\nsector_div(size, conf->geo.far_copies);\r\nsize = size * raid_disks;\r\nsector_div(size, conf->geo.near_copies);\r\nreturn size << conf->geo.chunk_shift;\r\n}\r\nstatic void calc_sectors(struct r10conf *conf, sector_t size)\r\n{\r\nsize = size >> conf->geo.chunk_shift;\r\nsector_div(size, conf->geo.far_copies);\r\nsize = size * conf->geo.raid_disks;\r\nsector_div(size, conf->geo.near_copies);\r\nsize = size * conf->copies;\r\nsize = DIV_ROUND_UP_SECTOR_T(size, conf->geo.raid_disks);\r\nconf->dev_sectors = size << conf->geo.chunk_shift;\r\nif (conf->geo.far_offset)\r\nconf->geo.stride = 1 << conf->geo.chunk_shift;\r\nelse {\r\nsector_div(size, conf->geo.far_copies);\r\nconf->geo.stride = size << conf->geo.chunk_shift;\r\n}\r\n}\r\nstatic int setup_geo(struct geom *geo, struct mddev *mddev, enum geo_type new)\r\n{\r\nint nc, fc, fo;\r\nint layout, chunk, disks;\r\nswitch (new) {\r\ncase geo_old:\r\nlayout = mddev->layout;\r\nchunk = mddev->chunk_sectors;\r\ndisks = mddev->raid_disks - mddev->delta_disks;\r\nbreak;\r\ncase geo_new:\r\nlayout = mddev->new_layout;\r\nchunk = mddev->new_chunk_sectors;\r\ndisks = mddev->raid_disks;\r\nbreak;\r\ndefault:\r\ncase geo_start:\r\nlayout = mddev->new_layout;\r\nchunk = mddev->new_chunk_sectors;\r\ndisks = mddev->raid_disks + mddev->delta_disks;\r\nbreak;\r\n}\r\nif (layout >> 18)\r\nreturn -1;\r\nif (chunk < (PAGE_SIZE >> 9) ||\r\n!is_power_of_2(chunk))\r\nreturn -2;\r\nnc = layout & 255;\r\nfc = (layout >> 8) & 255;\r\nfo = layout & (1<<16);\r\ngeo->raid_disks = disks;\r\ngeo->near_copies = nc;\r\ngeo->far_copies = fc;\r\ngeo->far_offset = fo;\r\ngeo->far_set_size = (layout & (1<<17)) ? disks / fc : disks;\r\ngeo->chunk_mask = chunk - 1;\r\ngeo->chunk_shift = ffz(~chunk);\r\nreturn nc*fc;\r\n}\r\nstatic struct r10conf *setup_conf(struct mddev *mddev)\r\n{\r\nstruct r10conf *conf = NULL;\r\nint err = -EINVAL;\r\nstruct geom geo;\r\nint copies;\r\ncopies = setup_geo(&geo, mddev, geo_new);\r\nif (copies == -2) {\r\nprintk(KERN_ERR "md/raid10:%s: chunk size must be "\r\n"at least PAGE_SIZE(%ld) and be a power of 2.\n",\r\nmdname(mddev), PAGE_SIZE);\r\ngoto out;\r\n}\r\nif (copies < 2 || copies > mddev->raid_disks) {\r\nprintk(KERN_ERR "md/raid10:%s: unsupported raid10 layout: 0x%8x\n",\r\nmdname(mddev), mddev->new_layout);\r\ngoto out;\r\n}\r\nerr = -ENOMEM;\r\nconf = kzalloc(sizeof(struct r10conf), GFP_KERNEL);\r\nif (!conf)\r\ngoto out;\r\nconf->mirrors = kzalloc(sizeof(struct raid10_info)*(mddev->raid_disks +\r\nmax(0,-mddev->delta_disks)),\r\nGFP_KERNEL);\r\nif (!conf->mirrors)\r\ngoto out;\r\nconf->tmppage = alloc_page(GFP_KERNEL);\r\nif (!conf->tmppage)\r\ngoto out;\r\nconf->geo = geo;\r\nconf->copies = copies;\r\nconf->r10bio_pool = mempool_create(NR_RAID10_BIOS, r10bio_pool_alloc,\r\nr10bio_pool_free, conf);\r\nif (!conf->r10bio_pool)\r\ngoto out;\r\ncalc_sectors(conf, mddev->dev_sectors);\r\nif (mddev->reshape_position == MaxSector) {\r\nconf->prev = conf->geo;\r\nconf->reshape_progress = MaxSector;\r\n} else {\r\nif (setup_geo(&conf->prev, mddev, geo_old) != conf->copies) {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nconf->reshape_progress = mddev->reshape_position;\r\nif (conf->prev.far_offset)\r\nconf->prev.stride = 1 << conf->prev.chunk_shift;\r\nelse\r\nconf->prev.stride = conf->dev_sectors;\r\n}\r\nspin_lock_init(&conf->device_lock);\r\nINIT_LIST_HEAD(&conf->retry_list);\r\nspin_lock_init(&conf->resync_lock);\r\ninit_waitqueue_head(&conf->wait_barrier);\r\nconf->thread = md_register_thread(raid10d, mddev, "raid10");\r\nif (!conf->thread)\r\ngoto out;\r\nconf->mddev = mddev;\r\nreturn conf;\r\nout:\r\nif (err == -ENOMEM)\r\nprintk(KERN_ERR "md/raid10:%s: couldn't allocate memory.\n",\r\nmdname(mddev));\r\nif (conf) {\r\nif (conf->r10bio_pool)\r\nmempool_destroy(conf->r10bio_pool);\r\nkfree(conf->mirrors);\r\nsafe_put_page(conf->tmppage);\r\nkfree(conf);\r\n}\r\nreturn ERR_PTR(err);\r\n}\r\nstatic int run(struct mddev *mddev)\r\n{\r\nstruct r10conf *conf;\r\nint i, disk_idx, chunk_size;\r\nstruct raid10_info *disk;\r\nstruct md_rdev *rdev;\r\nsector_t size;\r\nsector_t min_offset_diff = 0;\r\nint first = 1;\r\nbool discard_supported = false;\r\nif (mddev->private == NULL) {\r\nconf = setup_conf(mddev);\r\nif (IS_ERR(conf))\r\nreturn PTR_ERR(conf);\r\nmddev->private = conf;\r\n}\r\nconf = mddev->private;\r\nif (!conf)\r\ngoto out;\r\nmddev->thread = conf->thread;\r\nconf->thread = NULL;\r\nchunk_size = mddev->chunk_sectors << 9;\r\nif (mddev->queue) {\r\nblk_queue_max_discard_sectors(mddev->queue,\r\nmddev->chunk_sectors);\r\nblk_queue_max_write_same_sectors(mddev->queue, 0);\r\nblk_queue_io_min(mddev->queue, chunk_size);\r\nif (conf->geo.raid_disks % conf->geo.near_copies)\r\nblk_queue_io_opt(mddev->queue, chunk_size * conf->geo.raid_disks);\r\nelse\r\nblk_queue_io_opt(mddev->queue, chunk_size *\r\n(conf->geo.raid_disks / conf->geo.near_copies));\r\n}\r\nrdev_for_each(rdev, mddev) {\r\nlong long diff;\r\nstruct request_queue *q;\r\ndisk_idx = rdev->raid_disk;\r\nif (disk_idx < 0)\r\ncontinue;\r\nif (disk_idx >= conf->geo.raid_disks &&\r\ndisk_idx >= conf->prev.raid_disks)\r\ncontinue;\r\ndisk = conf->mirrors + disk_idx;\r\nif (test_bit(Replacement, &rdev->flags)) {\r\nif (disk->replacement)\r\ngoto out_free_conf;\r\ndisk->replacement = rdev;\r\n} else {\r\nif (disk->rdev)\r\ngoto out_free_conf;\r\ndisk->rdev = rdev;\r\n}\r\nq = bdev_get_queue(rdev->bdev);\r\nif (q->merge_bvec_fn)\r\nmddev->merge_check_needed = 1;\r\ndiff = (rdev->new_data_offset - rdev->data_offset);\r\nif (!mddev->reshape_backwards)\r\ndiff = -diff;\r\nif (diff < 0)\r\ndiff = 0;\r\nif (first || diff < min_offset_diff)\r\nmin_offset_diff = diff;\r\nif (mddev->gendisk)\r\ndisk_stack_limits(mddev->gendisk, rdev->bdev,\r\nrdev->data_offset << 9);\r\ndisk->head_position = 0;\r\nif (blk_queue_discard(bdev_get_queue(rdev->bdev)))\r\ndiscard_supported = true;\r\n}\r\nif (mddev->queue) {\r\nif (discard_supported)\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD,\r\nmddev->queue);\r\nelse\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_DISCARD,\r\nmddev->queue);\r\n}\r\nif (!enough(conf, -1)) {\r\nprintk(KERN_ERR "md/raid10:%s: not enough operational mirrors.\n",\r\nmdname(mddev));\r\ngoto out_free_conf;\r\n}\r\nif (conf->reshape_progress != MaxSector) {\r\nif (conf->geo.far_copies != 1 &&\r\nconf->geo.far_offset == 0)\r\ngoto out_free_conf;\r\nif (conf->prev.far_copies != 1 &&\r\nconf->prev.far_offset == 0)\r\ngoto out_free_conf;\r\n}\r\nmddev->degraded = 0;\r\nfor (i = 0;\r\ni < conf->geo.raid_disks\r\n|| i < conf->prev.raid_disks;\r\ni++) {\r\ndisk = conf->mirrors + i;\r\nif (!disk->rdev && disk->replacement) {\r\ndisk->rdev = disk->replacement;\r\ndisk->replacement = NULL;\r\nclear_bit(Replacement, &disk->rdev->flags);\r\n}\r\nif (!disk->rdev ||\r\n!test_bit(In_sync, &disk->rdev->flags)) {\r\ndisk->head_position = 0;\r\nmddev->degraded++;\r\nif (disk->rdev &&\r\ndisk->rdev->saved_raid_disk < 0)\r\nconf->fullsync = 1;\r\n}\r\ndisk->recovery_disabled = mddev->recovery_disabled - 1;\r\n}\r\nif (mddev->recovery_cp != MaxSector)\r\nprintk(KERN_NOTICE "md/raid10:%s: not clean"\r\n" -- starting background reconstruction\n",\r\nmdname(mddev));\r\nprintk(KERN_INFO\r\n"md/raid10:%s: active with %d out of %d devices\n",\r\nmdname(mddev), conf->geo.raid_disks - mddev->degraded,\r\nconf->geo.raid_disks);\r\nmddev->dev_sectors = conf->dev_sectors;\r\nsize = raid10_size(mddev, 0, 0);\r\nmd_set_array_sectors(mddev, size);\r\nmddev->resync_max_sectors = size;\r\nif (mddev->queue) {\r\nint stripe = conf->geo.raid_disks *\r\n((mddev->chunk_sectors << 9) / PAGE_SIZE);\r\nmddev->queue->backing_dev_info.congested_fn = raid10_congested;\r\nmddev->queue->backing_dev_info.congested_data = mddev;\r\nstripe /= conf->geo.near_copies;\r\nif (mddev->queue->backing_dev_info.ra_pages < 2 * stripe)\r\nmddev->queue->backing_dev_info.ra_pages = 2 * stripe;\r\nblk_queue_merge_bvec(mddev->queue, raid10_mergeable_bvec);\r\n}\r\nif (md_integrity_register(mddev))\r\ngoto out_free_conf;\r\nif (conf->reshape_progress != MaxSector) {\r\nunsigned long before_length, after_length;\r\nbefore_length = ((1 << conf->prev.chunk_shift) *\r\nconf->prev.far_copies);\r\nafter_length = ((1 << conf->geo.chunk_shift) *\r\nconf->geo.far_copies);\r\nif (max(before_length, after_length) > min_offset_diff) {\r\nprintk("md/raid10: offset difference not enough to continue reshape\n");\r\ngoto out_free_conf;\r\n}\r\nconf->offset_diff = min_offset_diff;\r\nconf->reshape_safe = conf->reshape_progress;\r\nclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\r\nclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\r\nset_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\r\nset_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\r\nmddev->sync_thread = md_register_thread(md_do_sync, mddev,\r\n"reshape");\r\n}\r\nreturn 0;\r\nout_free_conf:\r\nmd_unregister_thread(&mddev->thread);\r\nif (conf->r10bio_pool)\r\nmempool_destroy(conf->r10bio_pool);\r\nsafe_put_page(conf->tmppage);\r\nkfree(conf->mirrors);\r\nkfree(conf);\r\nmddev->private = NULL;\r\nout:\r\nreturn -EIO;\r\n}\r\nstatic int stop(struct mddev *mddev)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nraise_barrier(conf, 0);\r\nlower_barrier(conf);\r\nmd_unregister_thread(&mddev->thread);\r\nif (mddev->queue)\r\nblk_sync_queue(mddev->queue);\r\nif (conf->r10bio_pool)\r\nmempool_destroy(conf->r10bio_pool);\r\nsafe_put_page(conf->tmppage);\r\nkfree(conf->mirrors);\r\nkfree(conf->mirrors_old);\r\nkfree(conf->mirrors_new);\r\nkfree(conf);\r\nmddev->private = NULL;\r\nreturn 0;\r\n}\r\nstatic void raid10_quiesce(struct mddev *mddev, int state)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nswitch(state) {\r\ncase 1:\r\nraise_barrier(conf, 0);\r\nbreak;\r\ncase 0:\r\nlower_barrier(conf);\r\nbreak;\r\n}\r\n}\r\nstatic int raid10_resize(struct mddev *mddev, sector_t sectors)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nsector_t oldsize, size;\r\nif (mddev->reshape_position != MaxSector)\r\nreturn -EBUSY;\r\nif (conf->geo.far_copies > 1 && !conf->geo.far_offset)\r\nreturn -EINVAL;\r\noldsize = raid10_size(mddev, 0, 0);\r\nsize = raid10_size(mddev, sectors, 0);\r\nif (mddev->external_size &&\r\nmddev->array_sectors > size)\r\nreturn -EINVAL;\r\nif (mddev->bitmap) {\r\nint ret = bitmap_resize(mddev->bitmap, size, 0, 0);\r\nif (ret)\r\nreturn ret;\r\n}\r\nmd_set_array_sectors(mddev, size);\r\nset_capacity(mddev->gendisk, mddev->array_sectors);\r\nrevalidate_disk(mddev->gendisk);\r\nif (sectors > mddev->dev_sectors &&\r\nmddev->recovery_cp > oldsize) {\r\nmddev->recovery_cp = oldsize;\r\nset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\r\n}\r\ncalc_sectors(conf, sectors);\r\nmddev->dev_sectors = conf->dev_sectors;\r\nmddev->resync_max_sectors = size;\r\nreturn 0;\r\n}\r\nstatic void *raid10_takeover_raid0(struct mddev *mddev)\r\n{\r\nstruct md_rdev *rdev;\r\nstruct r10conf *conf;\r\nif (mddev->degraded > 0) {\r\nprintk(KERN_ERR "md/raid10:%s: Error: degraded raid0!\n",\r\nmdname(mddev));\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nmddev->new_level = 10;\r\nmddev->new_layout = (1<<8) + 2;\r\nmddev->new_chunk_sectors = mddev->chunk_sectors;\r\nmddev->delta_disks = mddev->raid_disks;\r\nmddev->raid_disks *= 2;\r\nmddev->recovery_cp = MaxSector;\r\nconf = setup_conf(mddev);\r\nif (!IS_ERR(conf)) {\r\nrdev_for_each(rdev, mddev)\r\nif (rdev->raid_disk >= 0)\r\nrdev->new_raid_disk = rdev->raid_disk * 2;\r\nconf->barrier = 1;\r\n}\r\nreturn conf;\r\n}\r\nstatic void *raid10_takeover(struct mddev *mddev)\r\n{\r\nstruct r0conf *raid0_conf;\r\nif (mddev->level == 0) {\r\nraid0_conf = mddev->private;\r\nif (raid0_conf->nr_strip_zones > 1) {\r\nprintk(KERN_ERR "md/raid10:%s: cannot takeover raid 0"\r\n" with more than one zone.\n",\r\nmdname(mddev));\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nreturn raid10_takeover_raid0(mddev);\r\n}\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nstatic int raid10_check_reshape(struct mddev *mddev)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nstruct geom geo;\r\nif (conf->geo.far_copies != 1 && !conf->geo.far_offset)\r\nreturn -EINVAL;\r\nif (setup_geo(&geo, mddev, geo_start) != conf->copies)\r\nreturn -EINVAL;\r\nif (geo.far_copies > 1 && !geo.far_offset)\r\nreturn -EINVAL;\r\nif (mddev->array_sectors & geo.chunk_mask)\r\nreturn -EINVAL;\r\nif (!enough(conf, -1))\r\nreturn -EINVAL;\r\nkfree(conf->mirrors_new);\r\nconf->mirrors_new = NULL;\r\nif (mddev->delta_disks > 0) {\r\nconf->mirrors_new = kzalloc(\r\nsizeof(struct raid10_info)\r\n*(mddev->raid_disks +\r\nmddev->delta_disks),\r\nGFP_KERNEL);\r\nif (!conf->mirrors_new)\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic int calc_degraded(struct r10conf *conf)\r\n{\r\nint degraded, degraded2;\r\nint i;\r\nrcu_read_lock();\r\ndegraded = 0;\r\nfor (i = 0; i < conf->prev.raid_disks; i++) {\r\nstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\r\nif (!rdev || test_bit(Faulty, &rdev->flags))\r\ndegraded++;\r\nelse if (!test_bit(In_sync, &rdev->flags))\r\ndegraded++;\r\n}\r\nrcu_read_unlock();\r\nif (conf->geo.raid_disks == conf->prev.raid_disks)\r\nreturn degraded;\r\nrcu_read_lock();\r\ndegraded2 = 0;\r\nfor (i = 0; i < conf->geo.raid_disks; i++) {\r\nstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\r\nif (!rdev || test_bit(Faulty, &rdev->flags))\r\ndegraded2++;\r\nelse if (!test_bit(In_sync, &rdev->flags)) {\r\nif (conf->geo.raid_disks <= conf->prev.raid_disks)\r\ndegraded2++;\r\n}\r\n}\r\nrcu_read_unlock();\r\nif (degraded2 > degraded)\r\nreturn degraded2;\r\nreturn degraded;\r\n}\r\nstatic int raid10_start_reshape(struct mddev *mddev)\r\n{\r\nunsigned long before_length, after_length;\r\nsector_t min_offset_diff = 0;\r\nint first = 1;\r\nstruct geom new;\r\nstruct r10conf *conf = mddev->private;\r\nstruct md_rdev *rdev;\r\nint spares = 0;\r\nint ret;\r\nif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\r\nreturn -EBUSY;\r\nif (setup_geo(&new, mddev, geo_start) != conf->copies)\r\nreturn -EINVAL;\r\nbefore_length = ((1 << conf->prev.chunk_shift) *\r\nconf->prev.far_copies);\r\nafter_length = ((1 << conf->geo.chunk_shift) *\r\nconf->geo.far_copies);\r\nrdev_for_each(rdev, mddev) {\r\nif (!test_bit(In_sync, &rdev->flags)\r\n&& !test_bit(Faulty, &rdev->flags))\r\nspares++;\r\nif (rdev->raid_disk >= 0) {\r\nlong long diff = (rdev->new_data_offset\r\n- rdev->data_offset);\r\nif (!mddev->reshape_backwards)\r\ndiff = -diff;\r\nif (diff < 0)\r\ndiff = 0;\r\nif (first || diff < min_offset_diff)\r\nmin_offset_diff = diff;\r\n}\r\n}\r\nif (max(before_length, after_length) > min_offset_diff)\r\nreturn -EINVAL;\r\nif (spares < mddev->delta_disks)\r\nreturn -EINVAL;\r\nconf->offset_diff = min_offset_diff;\r\nspin_lock_irq(&conf->device_lock);\r\nif (conf->mirrors_new) {\r\nmemcpy(conf->mirrors_new, conf->mirrors,\r\nsizeof(struct raid10_info)*conf->prev.raid_disks);\r\nsmp_mb();\r\nkfree(conf->mirrors_old);\r\nconf->mirrors_old = conf->mirrors;\r\nconf->mirrors = conf->mirrors_new;\r\nconf->mirrors_new = NULL;\r\n}\r\nsetup_geo(&conf->geo, mddev, geo_start);\r\nsmp_mb();\r\nif (mddev->reshape_backwards) {\r\nsector_t size = raid10_size(mddev, 0, 0);\r\nif (size < mddev->array_sectors) {\r\nspin_unlock_irq(&conf->device_lock);\r\nprintk(KERN_ERR "md/raid10:%s: array size must be reduce before number of disks\n",\r\nmdname(mddev));\r\nreturn -EINVAL;\r\n}\r\nmddev->resync_max_sectors = size;\r\nconf->reshape_progress = size;\r\n} else\r\nconf->reshape_progress = 0;\r\nspin_unlock_irq(&conf->device_lock);\r\nif (mddev->delta_disks && mddev->bitmap) {\r\nret = bitmap_resize(mddev->bitmap,\r\nraid10_size(mddev, 0,\r\nconf->geo.raid_disks),\r\n0, 0);\r\nif (ret)\r\ngoto abort;\r\n}\r\nif (mddev->delta_disks > 0) {\r\nrdev_for_each(rdev, mddev)\r\nif (rdev->raid_disk < 0 &&\r\n!test_bit(Faulty, &rdev->flags)) {\r\nif (raid10_add_disk(mddev, rdev) == 0) {\r\nif (rdev->raid_disk >=\r\nconf->prev.raid_disks)\r\nset_bit(In_sync, &rdev->flags);\r\nelse\r\nrdev->recovery_offset = 0;\r\nif (sysfs_link_rdev(mddev, rdev))\r\n;\r\n}\r\n} else if (rdev->raid_disk >= conf->prev.raid_disks\r\n&& !test_bit(Faulty, &rdev->flags)) {\r\nset_bit(In_sync, &rdev->flags);\r\n}\r\n}\r\nspin_lock_irq(&conf->device_lock);\r\nmddev->degraded = calc_degraded(conf);\r\nspin_unlock_irq(&conf->device_lock);\r\nmddev->raid_disks = conf->geo.raid_disks;\r\nmddev->reshape_position = conf->reshape_progress;\r\nset_bit(MD_CHANGE_DEVS, &mddev->flags);\r\nclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\r\nclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\r\nset_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\r\nset_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\r\nmddev->sync_thread = md_register_thread(md_do_sync, mddev,\r\n"reshape");\r\nif (!mddev->sync_thread) {\r\nret = -EAGAIN;\r\ngoto abort;\r\n}\r\nconf->reshape_checkpoint = jiffies;\r\nmd_wakeup_thread(mddev->sync_thread);\r\nmd_new_event(mddev);\r\nreturn 0;\r\nabort:\r\nmddev->recovery = 0;\r\nspin_lock_irq(&conf->device_lock);\r\nconf->geo = conf->prev;\r\nmddev->raid_disks = conf->geo.raid_disks;\r\nrdev_for_each(rdev, mddev)\r\nrdev->new_data_offset = rdev->data_offset;\r\nsmp_wmb();\r\nconf->reshape_progress = MaxSector;\r\nmddev->reshape_position = MaxSector;\r\nspin_unlock_irq(&conf->device_lock);\r\nreturn ret;\r\n}\r\nstatic sector_t last_dev_address(sector_t s, struct geom *geo)\r\n{\r\ns = (s | geo->chunk_mask) + 1;\r\ns >>= geo->chunk_shift;\r\ns *= geo->near_copies;\r\ns = DIV_ROUND_UP_SECTOR_T(s, geo->raid_disks);\r\ns *= geo->far_copies;\r\ns <<= geo->chunk_shift;\r\nreturn s;\r\n}\r\nstatic sector_t first_dev_address(sector_t s, struct geom *geo)\r\n{\r\ns >>= geo->chunk_shift;\r\ns *= geo->near_copies;\r\nsector_div(s, geo->raid_disks);\r\ns *= geo->far_copies;\r\ns <<= geo->chunk_shift;\r\nreturn s;\r\n}\r\nstatic sector_t reshape_request(struct mddev *mddev, sector_t sector_nr,\r\nint *skipped)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nstruct r10bio *r10_bio;\r\nsector_t next, safe, last;\r\nint max_sectors;\r\nint nr_sectors;\r\nint s;\r\nstruct md_rdev *rdev;\r\nint need_flush = 0;\r\nstruct bio *blist;\r\nstruct bio *bio, *read_bio;\r\nint sectors_done = 0;\r\nif (sector_nr == 0) {\r\nif (mddev->reshape_backwards &&\r\nconf->reshape_progress < raid10_size(mddev, 0, 0)) {\r\nsector_nr = (raid10_size(mddev, 0, 0)\r\n- conf->reshape_progress);\r\n} else if (!mddev->reshape_backwards &&\r\nconf->reshape_progress > 0)\r\nsector_nr = conf->reshape_progress;\r\nif (sector_nr) {\r\nmddev->curr_resync_completed = sector_nr;\r\nsysfs_notify(&mddev->kobj, NULL, "sync_completed");\r\n*skipped = 1;\r\nreturn sector_nr;\r\n}\r\n}\r\nif (mddev->reshape_backwards) {\r\nnext = first_dev_address(conf->reshape_progress - 1,\r\n&conf->geo);\r\nsafe = last_dev_address(conf->reshape_safe - 1,\r\n&conf->prev);\r\nif (next + conf->offset_diff < safe)\r\nneed_flush = 1;\r\nlast = conf->reshape_progress - 1;\r\nsector_nr = last & ~(sector_t)(conf->geo.chunk_mask\r\n& conf->prev.chunk_mask);\r\nif (sector_nr + RESYNC_BLOCK_SIZE/512 < last)\r\nsector_nr = last + 1 - RESYNC_BLOCK_SIZE/512;\r\n} else {\r\nnext = last_dev_address(conf->reshape_progress, &conf->geo);\r\nsafe = first_dev_address(conf->reshape_safe, &conf->prev);\r\nif (next > safe + conf->offset_diff)\r\nneed_flush = 1;\r\nsector_nr = conf->reshape_progress;\r\nlast = sector_nr | (conf->geo.chunk_mask\r\n& conf->prev.chunk_mask);\r\nif (sector_nr + RESYNC_BLOCK_SIZE/512 <= last)\r\nlast = sector_nr + RESYNC_BLOCK_SIZE/512 - 1;\r\n}\r\nif (need_flush ||\r\ntime_after(jiffies, conf->reshape_checkpoint + 10*HZ)) {\r\nwait_barrier(conf);\r\nmddev->reshape_position = conf->reshape_progress;\r\nif (mddev->reshape_backwards)\r\nmddev->curr_resync_completed = raid10_size(mddev, 0, 0)\r\n- conf->reshape_progress;\r\nelse\r\nmddev->curr_resync_completed = conf->reshape_progress;\r\nconf->reshape_checkpoint = jiffies;\r\nset_bit(MD_CHANGE_DEVS, &mddev->flags);\r\nmd_wakeup_thread(mddev->thread);\r\nwait_event(mddev->sb_wait, mddev->flags == 0 ||\r\ntest_bit(MD_RECOVERY_INTR, &mddev->recovery));\r\nif (test_bit(MD_RECOVERY_INTR, &mddev->recovery)) {\r\nallow_barrier(conf);\r\nreturn sectors_done;\r\n}\r\nconf->reshape_safe = mddev->reshape_position;\r\nallow_barrier(conf);\r\n}\r\nread_more:\r\nr10_bio = mempool_alloc(conf->r10buf_pool, GFP_NOIO);\r\nr10_bio->state = 0;\r\nraise_barrier(conf, sectors_done != 0);\r\natomic_set(&r10_bio->remaining, 0);\r\nr10_bio->mddev = mddev;\r\nr10_bio->sector = sector_nr;\r\nset_bit(R10BIO_IsReshape, &r10_bio->state);\r\nr10_bio->sectors = last - sector_nr + 1;\r\nrdev = read_balance(conf, r10_bio, &max_sectors);\r\nBUG_ON(!test_bit(R10BIO_Previous, &r10_bio->state));\r\nif (!rdev) {\r\nmempool_free(r10_bio, conf->r10buf_pool);\r\nset_bit(MD_RECOVERY_INTR, &mddev->recovery);\r\nreturn sectors_done;\r\n}\r\nread_bio = bio_alloc_mddev(GFP_KERNEL, RESYNC_PAGES, mddev);\r\nread_bio->bi_bdev = rdev->bdev;\r\nread_bio->bi_iter.bi_sector = (r10_bio->devs[r10_bio->read_slot].addr\r\n+ rdev->data_offset);\r\nread_bio->bi_private = r10_bio;\r\nread_bio->bi_end_io = end_sync_read;\r\nread_bio->bi_rw = READ;\r\nread_bio->bi_flags &= (~0UL << BIO_RESET_BITS);\r\n__set_bit(BIO_UPTODATE, &read_bio->bi_flags);\r\nread_bio->bi_vcnt = 0;\r\nread_bio->bi_iter.bi_size = 0;\r\nr10_bio->master_bio = read_bio;\r\nr10_bio->read_slot = r10_bio->devs[r10_bio->read_slot].devnum;\r\n__raid10_find_phys(&conf->geo, r10_bio);\r\nblist = read_bio;\r\nread_bio->bi_next = NULL;\r\nfor (s = 0; s < conf->copies*2; s++) {\r\nstruct bio *b;\r\nint d = r10_bio->devs[s/2].devnum;\r\nstruct md_rdev *rdev2;\r\nif (s&1) {\r\nrdev2 = conf->mirrors[d].replacement;\r\nb = r10_bio->devs[s/2].repl_bio;\r\n} else {\r\nrdev2 = conf->mirrors[d].rdev;\r\nb = r10_bio->devs[s/2].bio;\r\n}\r\nif (!rdev2 || test_bit(Faulty, &rdev2->flags))\r\ncontinue;\r\nbio_reset(b);\r\nb->bi_bdev = rdev2->bdev;\r\nb->bi_iter.bi_sector = r10_bio->devs[s/2].addr +\r\nrdev2->new_data_offset;\r\nb->bi_private = r10_bio;\r\nb->bi_end_io = end_reshape_write;\r\nb->bi_rw = WRITE;\r\nb->bi_next = blist;\r\nblist = b;\r\n}\r\nnr_sectors = 0;\r\nfor (s = 0 ; s < max_sectors; s += PAGE_SIZE >> 9) {\r\nstruct page *page = r10_bio->devs[0].bio->bi_io_vec[s/(PAGE_SIZE>>9)].bv_page;\r\nint len = (max_sectors - s) << 9;\r\nif (len > PAGE_SIZE)\r\nlen = PAGE_SIZE;\r\nfor (bio = blist; bio ; bio = bio->bi_next) {\r\nstruct bio *bio2;\r\nif (bio_add_page(bio, page, len, 0))\r\ncontinue;\r\nfor (bio2 = blist;\r\nbio2 && bio2 != bio;\r\nbio2 = bio2->bi_next) {\r\nbio2->bi_vcnt--;\r\nbio2->bi_iter.bi_size -= len;\r\n__clear_bit(BIO_SEG_VALID, &bio2->bi_flags);\r\n}\r\ngoto bio_full;\r\n}\r\nsector_nr += len >> 9;\r\nnr_sectors += len >> 9;\r\n}\r\nbio_full:\r\nr10_bio->sectors = nr_sectors;\r\nmd_sync_acct(read_bio->bi_bdev, r10_bio->sectors);\r\natomic_inc(&r10_bio->remaining);\r\nread_bio->bi_next = NULL;\r\ngeneric_make_request(read_bio);\r\nsector_nr += nr_sectors;\r\nsectors_done += nr_sectors;\r\nif (sector_nr <= last)\r\ngoto read_more;\r\nif (mddev->reshape_backwards)\r\nconf->reshape_progress -= sectors_done;\r\nelse\r\nconf->reshape_progress += sectors_done;\r\nreturn sectors_done;\r\n}\r\nstatic void reshape_request_write(struct mddev *mddev, struct r10bio *r10_bio)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nint s;\r\nif (!test_bit(R10BIO_Uptodate, &r10_bio->state))\r\nif (handle_reshape_read_error(mddev, r10_bio) < 0) {\r\nmd_done_sync(mddev, r10_bio->sectors, 0);\r\nreturn;\r\n}\r\natomic_set(&r10_bio->remaining, 1);\r\nfor (s = 0; s < conf->copies*2; s++) {\r\nstruct bio *b;\r\nint d = r10_bio->devs[s/2].devnum;\r\nstruct md_rdev *rdev;\r\nif (s&1) {\r\nrdev = conf->mirrors[d].replacement;\r\nb = r10_bio->devs[s/2].repl_bio;\r\n} else {\r\nrdev = conf->mirrors[d].rdev;\r\nb = r10_bio->devs[s/2].bio;\r\n}\r\nif (!rdev || test_bit(Faulty, &rdev->flags))\r\ncontinue;\r\natomic_inc(&rdev->nr_pending);\r\nmd_sync_acct(b->bi_bdev, r10_bio->sectors);\r\natomic_inc(&r10_bio->remaining);\r\nb->bi_next = NULL;\r\ngeneric_make_request(b);\r\n}\r\nend_reshape_request(r10_bio);\r\n}\r\nstatic void end_reshape(struct r10conf *conf)\r\n{\r\nif (test_bit(MD_RECOVERY_INTR, &conf->mddev->recovery))\r\nreturn;\r\nspin_lock_irq(&conf->device_lock);\r\nconf->prev = conf->geo;\r\nmd_finish_reshape(conf->mddev);\r\nsmp_wmb();\r\nconf->reshape_progress = MaxSector;\r\nspin_unlock_irq(&conf->device_lock);\r\nif (conf->mddev->queue) {\r\nint stripe = conf->geo.raid_disks *\r\n((conf->mddev->chunk_sectors << 9) / PAGE_SIZE);\r\nstripe /= conf->geo.near_copies;\r\nif (conf->mddev->queue->backing_dev_info.ra_pages < 2 * stripe)\r\nconf->mddev->queue->backing_dev_info.ra_pages = 2 * stripe;\r\n}\r\nconf->fullsync = 0;\r\n}\r\nstatic int handle_reshape_read_error(struct mddev *mddev,\r\nstruct r10bio *r10_bio)\r\n{\r\nint sectors = r10_bio->sectors;\r\nstruct r10conf *conf = mddev->private;\r\nstruct {\r\nstruct r10bio r10_bio;\r\nstruct r10dev devs[conf->copies];\r\n} on_stack;\r\nstruct r10bio *r10b = &on_stack.r10_bio;\r\nint slot = 0;\r\nint idx = 0;\r\nstruct bio_vec *bvec = r10_bio->master_bio->bi_io_vec;\r\nr10b->sector = r10_bio->sector;\r\n__raid10_find_phys(&conf->prev, r10b);\r\nwhile (sectors) {\r\nint s = sectors;\r\nint success = 0;\r\nint first_slot = slot;\r\nif (s > (PAGE_SIZE >> 9))\r\ns = PAGE_SIZE >> 9;\r\nwhile (!success) {\r\nint d = r10b->devs[slot].devnum;\r\nstruct md_rdev *rdev = conf->mirrors[d].rdev;\r\nsector_t addr;\r\nif (rdev == NULL ||\r\ntest_bit(Faulty, &rdev->flags) ||\r\n!test_bit(In_sync, &rdev->flags))\r\ngoto failed;\r\naddr = r10b->devs[slot].addr + idx * PAGE_SIZE;\r\nsuccess = sync_page_io(rdev,\r\naddr,\r\ns << 9,\r\nbvec[idx].bv_page,\r\nREAD, false);\r\nif (success)\r\nbreak;\r\nfailed:\r\nslot++;\r\nif (slot >= conf->copies)\r\nslot = 0;\r\nif (slot == first_slot)\r\nbreak;\r\n}\r\nif (!success) {\r\nset_bit(MD_RECOVERY_INTR,\r\n&mddev->recovery);\r\nreturn -EIO;\r\n}\r\nsectors -= s;\r\nidx++;\r\n}\r\nreturn 0;\r\n}\r\nstatic void end_reshape_write(struct bio *bio, int error)\r\n{\r\nint uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nstruct r10bio *r10_bio = bio->bi_private;\r\nstruct mddev *mddev = r10_bio->mddev;\r\nstruct r10conf *conf = mddev->private;\r\nint d;\r\nint slot;\r\nint repl;\r\nstruct md_rdev *rdev = NULL;\r\nd = find_bio_disk(conf, r10_bio, bio, &slot, &repl);\r\nif (repl)\r\nrdev = conf->mirrors[d].replacement;\r\nif (!rdev) {\r\nsmp_mb();\r\nrdev = conf->mirrors[d].rdev;\r\n}\r\nif (!uptodate) {\r\nmd_error(mddev, rdev);\r\n}\r\nrdev_dec_pending(rdev, mddev);\r\nend_reshape_request(r10_bio);\r\n}\r\nstatic void end_reshape_request(struct r10bio *r10_bio)\r\n{\r\nif (!atomic_dec_and_test(&r10_bio->remaining))\r\nreturn;\r\nmd_done_sync(r10_bio->mddev, r10_bio->sectors, 1);\r\nbio_put(r10_bio->master_bio);\r\nput_buf(r10_bio);\r\n}\r\nstatic void raid10_finish_reshape(struct mddev *mddev)\r\n{\r\nstruct r10conf *conf = mddev->private;\r\nif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\r\nreturn;\r\nif (mddev->delta_disks > 0) {\r\nsector_t size = raid10_size(mddev, 0, 0);\r\nmd_set_array_sectors(mddev, size);\r\nif (mddev->recovery_cp > mddev->resync_max_sectors) {\r\nmddev->recovery_cp = mddev->resync_max_sectors;\r\nset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\r\n}\r\nmddev->resync_max_sectors = size;\r\nset_capacity(mddev->gendisk, mddev->array_sectors);\r\nrevalidate_disk(mddev->gendisk);\r\n} else {\r\nint d;\r\nfor (d = conf->geo.raid_disks ;\r\nd < conf->geo.raid_disks - mddev->delta_disks;\r\nd++) {\r\nstruct md_rdev *rdev = conf->mirrors[d].rdev;\r\nif (rdev)\r\nclear_bit(In_sync, &rdev->flags);\r\nrdev = conf->mirrors[d].replacement;\r\nif (rdev)\r\nclear_bit(In_sync, &rdev->flags);\r\n}\r\n}\r\nmddev->layout = mddev->new_layout;\r\nmddev->chunk_sectors = 1 << conf->geo.chunk_shift;\r\nmddev->reshape_position = MaxSector;\r\nmddev->delta_disks = 0;\r\nmddev->reshape_backwards = 0;\r\n}\r\nstatic int __init raid_init(void)\r\n{\r\nreturn register_md_personality(&raid10_personality);\r\n}\r\nstatic void raid_exit(void)\r\n{\r\nunregister_md_personality(&raid10_personality);\r\n}
