static void init_query_mad(struct ib_smp *mad)\r\n{\r\nmad->base_version = 1;\r\nmad->mgmt_class = IB_MGMT_CLASS_SUBN_LID_ROUTED;\r\nmad->class_version = 1;\r\nmad->method = IB_MGMT_METHOD_GET;\r\n}\r\nstatic int mthca_query_device(struct ib_device *ibdev,\r\nstruct ib_device_attr *props)\r\n{\r\nstruct ib_smp *in_mad = NULL;\r\nstruct ib_smp *out_mad = NULL;\r\nint err = -ENOMEM;\r\nstruct mthca_dev *mdev = to_mdev(ibdev);\r\nin_mad = kzalloc(sizeof *in_mad, GFP_KERNEL);\r\nout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\r\nif (!in_mad || !out_mad)\r\ngoto out;\r\nmemset(props, 0, sizeof *props);\r\nprops->fw_ver = mdev->fw_ver;\r\ninit_query_mad(in_mad);\r\nin_mad->attr_id = IB_SMP_ATTR_NODE_INFO;\r\nerr = mthca_MAD_IFC(mdev, 1, 1,\r\n1, NULL, NULL, in_mad, out_mad);\r\nif (err)\r\ngoto out;\r\nprops->device_cap_flags = mdev->device_cap_flags;\r\nprops->vendor_id = be32_to_cpup((__be32 *) (out_mad->data + 36)) &\r\n0xffffff;\r\nprops->vendor_part_id = be16_to_cpup((__be16 *) (out_mad->data + 30));\r\nprops->hw_ver = be32_to_cpup((__be32 *) (out_mad->data + 32));\r\nmemcpy(&props->sys_image_guid, out_mad->data + 4, 8);\r\nprops->max_mr_size = ~0ull;\r\nprops->page_size_cap = mdev->limits.page_size_cap;\r\nprops->max_qp = mdev->limits.num_qps - mdev->limits.reserved_qps;\r\nprops->max_qp_wr = mdev->limits.max_wqes;\r\nprops->max_sge = mdev->limits.max_sg;\r\nprops->max_cq = mdev->limits.num_cqs - mdev->limits.reserved_cqs;\r\nprops->max_cqe = mdev->limits.max_cqes;\r\nprops->max_mr = mdev->limits.num_mpts - mdev->limits.reserved_mrws;\r\nprops->max_pd = mdev->limits.num_pds - mdev->limits.reserved_pds;\r\nprops->max_qp_rd_atom = 1 << mdev->qp_table.rdb_shift;\r\nprops->max_qp_init_rd_atom = mdev->limits.max_qp_init_rdma;\r\nprops->max_res_rd_atom = props->max_qp_rd_atom * props->max_qp;\r\nprops->max_srq = mdev->limits.num_srqs - mdev->limits.reserved_srqs;\r\nprops->max_srq_wr = mdev->limits.max_srq_wqes;\r\nprops->max_srq_sge = mdev->limits.max_srq_sge;\r\nprops->local_ca_ack_delay = mdev->limits.local_ca_ack_delay;\r\nprops->atomic_cap = mdev->limits.flags & DEV_LIM_FLAG_ATOMIC ?\r\nIB_ATOMIC_HCA : IB_ATOMIC_NONE;\r\nprops->max_pkeys = mdev->limits.pkey_table_len;\r\nprops->max_mcast_grp = mdev->limits.num_mgms + mdev->limits.num_amgms;\r\nprops->max_mcast_qp_attach = MTHCA_QP_PER_MGM;\r\nprops->max_total_mcast_qp_attach = props->max_mcast_qp_attach *\r\nprops->max_mcast_grp;\r\nif (mdev->mthca_flags & MTHCA_FLAG_SINAI_OPT)\r\nprops->max_map_per_fmr = 255;\r\nelse\r\nprops->max_map_per_fmr =\r\n(1 << (32 - ilog2(mdev->limits.num_mpts))) - 1;\r\nerr = 0;\r\nout:\r\nkfree(in_mad);\r\nkfree(out_mad);\r\nreturn err;\r\n}\r\nstatic int mthca_query_port(struct ib_device *ibdev,\r\nu8 port, struct ib_port_attr *props)\r\n{\r\nstruct ib_smp *in_mad = NULL;\r\nstruct ib_smp *out_mad = NULL;\r\nint err = -ENOMEM;\r\nin_mad = kzalloc(sizeof *in_mad, GFP_KERNEL);\r\nout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\r\nif (!in_mad || !out_mad)\r\ngoto out;\r\nmemset(props, 0, sizeof *props);\r\ninit_query_mad(in_mad);\r\nin_mad->attr_id = IB_SMP_ATTR_PORT_INFO;\r\nin_mad->attr_mod = cpu_to_be32(port);\r\nerr = mthca_MAD_IFC(to_mdev(ibdev), 1, 1,\r\nport, NULL, NULL, in_mad, out_mad);\r\nif (err)\r\ngoto out;\r\nprops->lid = be16_to_cpup((__be16 *) (out_mad->data + 16));\r\nprops->lmc = out_mad->data[34] & 0x7;\r\nprops->sm_lid = be16_to_cpup((__be16 *) (out_mad->data + 18));\r\nprops->sm_sl = out_mad->data[36] & 0xf;\r\nprops->state = out_mad->data[32] & 0xf;\r\nprops->phys_state = out_mad->data[33] >> 4;\r\nprops->port_cap_flags = be32_to_cpup((__be32 *) (out_mad->data + 20));\r\nprops->gid_tbl_len = to_mdev(ibdev)->limits.gid_table_len;\r\nprops->max_msg_sz = 0x80000000;\r\nprops->pkey_tbl_len = to_mdev(ibdev)->limits.pkey_table_len;\r\nprops->bad_pkey_cntr = be16_to_cpup((__be16 *) (out_mad->data + 46));\r\nprops->qkey_viol_cntr = be16_to_cpup((__be16 *) (out_mad->data + 48));\r\nprops->active_width = out_mad->data[31] & 0xf;\r\nprops->active_speed = out_mad->data[35] >> 4;\r\nprops->max_mtu = out_mad->data[41] & 0xf;\r\nprops->active_mtu = out_mad->data[36] >> 4;\r\nprops->subnet_timeout = out_mad->data[51] & 0x1f;\r\nprops->max_vl_num = out_mad->data[37] >> 4;\r\nprops->init_type_reply = out_mad->data[41] >> 4;\r\nout:\r\nkfree(in_mad);\r\nkfree(out_mad);\r\nreturn err;\r\n}\r\nstatic int mthca_modify_device(struct ib_device *ibdev,\r\nint mask,\r\nstruct ib_device_modify *props)\r\n{\r\nif (mask & ~IB_DEVICE_MODIFY_NODE_DESC)\r\nreturn -EOPNOTSUPP;\r\nif (mask & IB_DEVICE_MODIFY_NODE_DESC) {\r\nif (mutex_lock_interruptible(&to_mdev(ibdev)->cap_mask_mutex))\r\nreturn -ERESTARTSYS;\r\nmemcpy(ibdev->node_desc, props->node_desc, 64);\r\nmutex_unlock(&to_mdev(ibdev)->cap_mask_mutex);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mthca_modify_port(struct ib_device *ibdev,\r\nu8 port, int port_modify_mask,\r\nstruct ib_port_modify *props)\r\n{\r\nstruct mthca_set_ib_param set_ib;\r\nstruct ib_port_attr attr;\r\nint err;\r\nif (mutex_lock_interruptible(&to_mdev(ibdev)->cap_mask_mutex))\r\nreturn -ERESTARTSYS;\r\nerr = mthca_query_port(ibdev, port, &attr);\r\nif (err)\r\ngoto out;\r\nset_ib.set_si_guid = 0;\r\nset_ib.reset_qkey_viol = !!(port_modify_mask & IB_PORT_RESET_QKEY_CNTR);\r\nset_ib.cap_mask = (attr.port_cap_flags | props->set_port_cap_mask) &\r\n~props->clr_port_cap_mask;\r\nerr = mthca_SET_IB(to_mdev(ibdev), &set_ib, port);\r\nif (err)\r\ngoto out;\r\nout:\r\nmutex_unlock(&to_mdev(ibdev)->cap_mask_mutex);\r\nreturn err;\r\n}\r\nstatic int mthca_query_pkey(struct ib_device *ibdev,\r\nu8 port, u16 index, u16 *pkey)\r\n{\r\nstruct ib_smp *in_mad = NULL;\r\nstruct ib_smp *out_mad = NULL;\r\nint err = -ENOMEM;\r\nin_mad = kzalloc(sizeof *in_mad, GFP_KERNEL);\r\nout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\r\nif (!in_mad || !out_mad)\r\ngoto out;\r\ninit_query_mad(in_mad);\r\nin_mad->attr_id = IB_SMP_ATTR_PKEY_TABLE;\r\nin_mad->attr_mod = cpu_to_be32(index / 32);\r\nerr = mthca_MAD_IFC(to_mdev(ibdev), 1, 1,\r\nport, NULL, NULL, in_mad, out_mad);\r\nif (err)\r\ngoto out;\r\n*pkey = be16_to_cpu(((__be16 *) out_mad->data)[index % 32]);\r\nout:\r\nkfree(in_mad);\r\nkfree(out_mad);\r\nreturn err;\r\n}\r\nstatic int mthca_query_gid(struct ib_device *ibdev, u8 port,\r\nint index, union ib_gid *gid)\r\n{\r\nstruct ib_smp *in_mad = NULL;\r\nstruct ib_smp *out_mad = NULL;\r\nint err = -ENOMEM;\r\nin_mad = kzalloc(sizeof *in_mad, GFP_KERNEL);\r\nout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\r\nif (!in_mad || !out_mad)\r\ngoto out;\r\ninit_query_mad(in_mad);\r\nin_mad->attr_id = IB_SMP_ATTR_PORT_INFO;\r\nin_mad->attr_mod = cpu_to_be32(port);\r\nerr = mthca_MAD_IFC(to_mdev(ibdev), 1, 1,\r\nport, NULL, NULL, in_mad, out_mad);\r\nif (err)\r\ngoto out;\r\nmemcpy(gid->raw, out_mad->data + 8, 8);\r\ninit_query_mad(in_mad);\r\nin_mad->attr_id = IB_SMP_ATTR_GUID_INFO;\r\nin_mad->attr_mod = cpu_to_be32(index / 8);\r\nerr = mthca_MAD_IFC(to_mdev(ibdev), 1, 1,\r\nport, NULL, NULL, in_mad, out_mad);\r\nif (err)\r\ngoto out;\r\nmemcpy(gid->raw + 8, out_mad->data + (index % 8) * 8, 8);\r\nout:\r\nkfree(in_mad);\r\nkfree(out_mad);\r\nreturn err;\r\n}\r\nstatic struct ib_ucontext *mthca_alloc_ucontext(struct ib_device *ibdev,\r\nstruct ib_udata *udata)\r\n{\r\nstruct mthca_alloc_ucontext_resp uresp;\r\nstruct mthca_ucontext *context;\r\nint err;\r\nif (!(to_mdev(ibdev)->active))\r\nreturn ERR_PTR(-EAGAIN);\r\nmemset(&uresp, 0, sizeof uresp);\r\nuresp.qp_tab_size = to_mdev(ibdev)->limits.num_qps;\r\nif (mthca_is_memfree(to_mdev(ibdev)))\r\nuresp.uarc_size = to_mdev(ibdev)->uar_table.uarc_size;\r\nelse\r\nuresp.uarc_size = 0;\r\ncontext = kmalloc(sizeof *context, GFP_KERNEL);\r\nif (!context)\r\nreturn ERR_PTR(-ENOMEM);\r\nerr = mthca_uar_alloc(to_mdev(ibdev), &context->uar);\r\nif (err) {\r\nkfree(context);\r\nreturn ERR_PTR(err);\r\n}\r\ncontext->db_tab = mthca_init_user_db_tab(to_mdev(ibdev));\r\nif (IS_ERR(context->db_tab)) {\r\nerr = PTR_ERR(context->db_tab);\r\nmthca_uar_free(to_mdev(ibdev), &context->uar);\r\nkfree(context);\r\nreturn ERR_PTR(err);\r\n}\r\nif (ib_copy_to_udata(udata, &uresp, sizeof uresp)) {\r\nmthca_cleanup_user_db_tab(to_mdev(ibdev), &context->uar, context->db_tab);\r\nmthca_uar_free(to_mdev(ibdev), &context->uar);\r\nkfree(context);\r\nreturn ERR_PTR(-EFAULT);\r\n}\r\ncontext->reg_mr_warned = 0;\r\nreturn &context->ibucontext;\r\n}\r\nstatic int mthca_dealloc_ucontext(struct ib_ucontext *context)\r\n{\r\nmthca_cleanup_user_db_tab(to_mdev(context->device), &to_mucontext(context)->uar,\r\nto_mucontext(context)->db_tab);\r\nmthca_uar_free(to_mdev(context->device), &to_mucontext(context)->uar);\r\nkfree(to_mucontext(context));\r\nreturn 0;\r\n}\r\nstatic int mthca_mmap_uar(struct ib_ucontext *context,\r\nstruct vm_area_struct *vma)\r\n{\r\nif (vma->vm_end - vma->vm_start != PAGE_SIZE)\r\nreturn -EINVAL;\r\nvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\r\nif (io_remap_pfn_range(vma, vma->vm_start,\r\nto_mucontext(context)->uar.pfn,\r\nPAGE_SIZE, vma->vm_page_prot))\r\nreturn -EAGAIN;\r\nreturn 0;\r\n}\r\nstatic struct ib_pd *mthca_alloc_pd(struct ib_device *ibdev,\r\nstruct ib_ucontext *context,\r\nstruct ib_udata *udata)\r\n{\r\nstruct mthca_pd *pd;\r\nint err;\r\npd = kmalloc(sizeof *pd, GFP_KERNEL);\r\nif (!pd)\r\nreturn ERR_PTR(-ENOMEM);\r\nerr = mthca_pd_alloc(to_mdev(ibdev), !context, pd);\r\nif (err) {\r\nkfree(pd);\r\nreturn ERR_PTR(err);\r\n}\r\nif (context) {\r\nif (ib_copy_to_udata(udata, &pd->pd_num, sizeof (__u32))) {\r\nmthca_pd_free(to_mdev(ibdev), pd);\r\nkfree(pd);\r\nreturn ERR_PTR(-EFAULT);\r\n}\r\n}\r\nreturn &pd->ibpd;\r\n}\r\nstatic int mthca_dealloc_pd(struct ib_pd *pd)\r\n{\r\nmthca_pd_free(to_mdev(pd->device), to_mpd(pd));\r\nkfree(pd);\r\nreturn 0;\r\n}\r\nstatic struct ib_ah *mthca_ah_create(struct ib_pd *pd,\r\nstruct ib_ah_attr *ah_attr)\r\n{\r\nint err;\r\nstruct mthca_ah *ah;\r\nah = kmalloc(sizeof *ah, GFP_ATOMIC);\r\nif (!ah)\r\nreturn ERR_PTR(-ENOMEM);\r\nerr = mthca_create_ah(to_mdev(pd->device), to_mpd(pd), ah_attr, ah);\r\nif (err) {\r\nkfree(ah);\r\nreturn ERR_PTR(err);\r\n}\r\nreturn &ah->ibah;\r\n}\r\nstatic int mthca_ah_destroy(struct ib_ah *ah)\r\n{\r\nmthca_destroy_ah(to_mdev(ah->device), to_mah(ah));\r\nkfree(ah);\r\nreturn 0;\r\n}\r\nstatic struct ib_srq *mthca_create_srq(struct ib_pd *pd,\r\nstruct ib_srq_init_attr *init_attr,\r\nstruct ib_udata *udata)\r\n{\r\nstruct mthca_create_srq ucmd;\r\nstruct mthca_ucontext *context = NULL;\r\nstruct mthca_srq *srq;\r\nint err;\r\nif (init_attr->srq_type != IB_SRQT_BASIC)\r\nreturn ERR_PTR(-ENOSYS);\r\nsrq = kmalloc(sizeof *srq, GFP_KERNEL);\r\nif (!srq)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (pd->uobject) {\r\ncontext = to_mucontext(pd->uobject->context);\r\nif (ib_copy_from_udata(&ucmd, udata, sizeof ucmd)) {\r\nerr = -EFAULT;\r\ngoto err_free;\r\n}\r\nerr = mthca_map_user_db(to_mdev(pd->device), &context->uar,\r\ncontext->db_tab, ucmd.db_index,\r\nucmd.db_page);\r\nif (err)\r\ngoto err_free;\r\nsrq->mr.ibmr.lkey = ucmd.lkey;\r\nsrq->db_index = ucmd.db_index;\r\n}\r\nerr = mthca_alloc_srq(to_mdev(pd->device), to_mpd(pd),\r\n&init_attr->attr, srq);\r\nif (err && pd->uobject)\r\nmthca_unmap_user_db(to_mdev(pd->device), &context->uar,\r\ncontext->db_tab, ucmd.db_index);\r\nif (err)\r\ngoto err_free;\r\nif (context && ib_copy_to_udata(udata, &srq->srqn, sizeof (__u32))) {\r\nmthca_free_srq(to_mdev(pd->device), srq);\r\nerr = -EFAULT;\r\ngoto err_free;\r\n}\r\nreturn &srq->ibsrq;\r\nerr_free:\r\nkfree(srq);\r\nreturn ERR_PTR(err);\r\n}\r\nstatic int mthca_destroy_srq(struct ib_srq *srq)\r\n{\r\nstruct mthca_ucontext *context;\r\nif (srq->uobject) {\r\ncontext = to_mucontext(srq->uobject->context);\r\nmthca_unmap_user_db(to_mdev(srq->device), &context->uar,\r\ncontext->db_tab, to_msrq(srq)->db_index);\r\n}\r\nmthca_free_srq(to_mdev(srq->device), to_msrq(srq));\r\nkfree(srq);\r\nreturn 0;\r\n}\r\nstatic struct ib_qp *mthca_create_qp(struct ib_pd *pd,\r\nstruct ib_qp_init_attr *init_attr,\r\nstruct ib_udata *udata)\r\n{\r\nstruct mthca_create_qp ucmd;\r\nstruct mthca_qp *qp;\r\nint err;\r\nif (init_attr->create_flags)\r\nreturn ERR_PTR(-EINVAL);\r\nswitch (init_attr->qp_type) {\r\ncase IB_QPT_RC:\r\ncase IB_QPT_UC:\r\ncase IB_QPT_UD:\r\n{\r\nstruct mthca_ucontext *context;\r\nqp = kmalloc(sizeof *qp, GFP_KERNEL);\r\nif (!qp)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (pd->uobject) {\r\ncontext = to_mucontext(pd->uobject->context);\r\nif (ib_copy_from_udata(&ucmd, udata, sizeof ucmd)) {\r\nkfree(qp);\r\nreturn ERR_PTR(-EFAULT);\r\n}\r\nerr = mthca_map_user_db(to_mdev(pd->device), &context->uar,\r\ncontext->db_tab,\r\nucmd.sq_db_index, ucmd.sq_db_page);\r\nif (err) {\r\nkfree(qp);\r\nreturn ERR_PTR(err);\r\n}\r\nerr = mthca_map_user_db(to_mdev(pd->device), &context->uar,\r\ncontext->db_tab,\r\nucmd.rq_db_index, ucmd.rq_db_page);\r\nif (err) {\r\nmthca_unmap_user_db(to_mdev(pd->device),\r\n&context->uar,\r\ncontext->db_tab,\r\nucmd.sq_db_index);\r\nkfree(qp);\r\nreturn ERR_PTR(err);\r\n}\r\nqp->mr.ibmr.lkey = ucmd.lkey;\r\nqp->sq.db_index = ucmd.sq_db_index;\r\nqp->rq.db_index = ucmd.rq_db_index;\r\n}\r\nerr = mthca_alloc_qp(to_mdev(pd->device), to_mpd(pd),\r\nto_mcq(init_attr->send_cq),\r\nto_mcq(init_attr->recv_cq),\r\ninit_attr->qp_type, init_attr->sq_sig_type,\r\n&init_attr->cap, qp);\r\nif (err && pd->uobject) {\r\ncontext = to_mucontext(pd->uobject->context);\r\nmthca_unmap_user_db(to_mdev(pd->device),\r\n&context->uar,\r\ncontext->db_tab,\r\nucmd.sq_db_index);\r\nmthca_unmap_user_db(to_mdev(pd->device),\r\n&context->uar,\r\ncontext->db_tab,\r\nucmd.rq_db_index);\r\n}\r\nqp->ibqp.qp_num = qp->qpn;\r\nbreak;\r\n}\r\ncase IB_QPT_SMI:\r\ncase IB_QPT_GSI:\r\n{\r\nif (pd->uobject)\r\nreturn ERR_PTR(-EINVAL);\r\nqp = kmalloc(sizeof (struct mthca_sqp), GFP_KERNEL);\r\nif (!qp)\r\nreturn ERR_PTR(-ENOMEM);\r\nqp->ibqp.qp_num = init_attr->qp_type == IB_QPT_SMI ? 0 : 1;\r\nerr = mthca_alloc_sqp(to_mdev(pd->device), to_mpd(pd),\r\nto_mcq(init_attr->send_cq),\r\nto_mcq(init_attr->recv_cq),\r\ninit_attr->sq_sig_type, &init_attr->cap,\r\nqp->ibqp.qp_num, init_attr->port_num,\r\nto_msqp(qp));\r\nbreak;\r\n}\r\ndefault:\r\nreturn ERR_PTR(-ENOSYS);\r\n}\r\nif (err) {\r\nkfree(qp);\r\nreturn ERR_PTR(err);\r\n}\r\ninit_attr->cap.max_send_wr = qp->sq.max;\r\ninit_attr->cap.max_recv_wr = qp->rq.max;\r\ninit_attr->cap.max_send_sge = qp->sq.max_gs;\r\ninit_attr->cap.max_recv_sge = qp->rq.max_gs;\r\ninit_attr->cap.max_inline_data = qp->max_inline_data;\r\nreturn &qp->ibqp;\r\n}\r\nstatic int mthca_destroy_qp(struct ib_qp *qp)\r\n{\r\nif (qp->uobject) {\r\nmthca_unmap_user_db(to_mdev(qp->device),\r\n&to_mucontext(qp->uobject->context)->uar,\r\nto_mucontext(qp->uobject->context)->db_tab,\r\nto_mqp(qp)->sq.db_index);\r\nmthca_unmap_user_db(to_mdev(qp->device),\r\n&to_mucontext(qp->uobject->context)->uar,\r\nto_mucontext(qp->uobject->context)->db_tab,\r\nto_mqp(qp)->rq.db_index);\r\n}\r\nmthca_free_qp(to_mdev(qp->device), to_mqp(qp));\r\nkfree(qp);\r\nreturn 0;\r\n}\r\nstatic struct ib_cq *mthca_create_cq(struct ib_device *ibdev, int entries,\r\nint comp_vector,\r\nstruct ib_ucontext *context,\r\nstruct ib_udata *udata)\r\n{\r\nstruct mthca_create_cq ucmd;\r\nstruct mthca_cq *cq;\r\nint nent;\r\nint err;\r\nif (entries < 1 || entries > to_mdev(ibdev)->limits.max_cqes)\r\nreturn ERR_PTR(-EINVAL);\r\nif (context) {\r\nif (ib_copy_from_udata(&ucmd, udata, sizeof ucmd))\r\nreturn ERR_PTR(-EFAULT);\r\nerr = mthca_map_user_db(to_mdev(ibdev), &to_mucontext(context)->uar,\r\nto_mucontext(context)->db_tab,\r\nucmd.set_db_index, ucmd.set_db_page);\r\nif (err)\r\nreturn ERR_PTR(err);\r\nerr = mthca_map_user_db(to_mdev(ibdev), &to_mucontext(context)->uar,\r\nto_mucontext(context)->db_tab,\r\nucmd.arm_db_index, ucmd.arm_db_page);\r\nif (err)\r\ngoto err_unmap_set;\r\n}\r\ncq = kmalloc(sizeof *cq, GFP_KERNEL);\r\nif (!cq) {\r\nerr = -ENOMEM;\r\ngoto err_unmap_arm;\r\n}\r\nif (context) {\r\ncq->buf.mr.ibmr.lkey = ucmd.lkey;\r\ncq->set_ci_db_index = ucmd.set_db_index;\r\ncq->arm_db_index = ucmd.arm_db_index;\r\n}\r\nfor (nent = 1; nent <= entries; nent <<= 1)\r\n;\r\nerr = mthca_init_cq(to_mdev(ibdev), nent,\r\ncontext ? to_mucontext(context) : NULL,\r\ncontext ? ucmd.pdn : to_mdev(ibdev)->driver_pd.pd_num,\r\ncq);\r\nif (err)\r\ngoto err_free;\r\nif (context && ib_copy_to_udata(udata, &cq->cqn, sizeof (__u32))) {\r\nmthca_free_cq(to_mdev(ibdev), cq);\r\nerr = -EFAULT;\r\ngoto err_free;\r\n}\r\ncq->resize_buf = NULL;\r\nreturn &cq->ibcq;\r\nerr_free:\r\nkfree(cq);\r\nerr_unmap_arm:\r\nif (context)\r\nmthca_unmap_user_db(to_mdev(ibdev), &to_mucontext(context)->uar,\r\nto_mucontext(context)->db_tab, ucmd.arm_db_index);\r\nerr_unmap_set:\r\nif (context)\r\nmthca_unmap_user_db(to_mdev(ibdev), &to_mucontext(context)->uar,\r\nto_mucontext(context)->db_tab, ucmd.set_db_index);\r\nreturn ERR_PTR(err);\r\n}\r\nstatic int mthca_alloc_resize_buf(struct mthca_dev *dev, struct mthca_cq *cq,\r\nint entries)\r\n{\r\nint ret;\r\nspin_lock_irq(&cq->lock);\r\nif (cq->resize_buf) {\r\nret = -EBUSY;\r\ngoto unlock;\r\n}\r\ncq->resize_buf = kmalloc(sizeof *cq->resize_buf, GFP_ATOMIC);\r\nif (!cq->resize_buf) {\r\nret = -ENOMEM;\r\ngoto unlock;\r\n}\r\ncq->resize_buf->state = CQ_RESIZE_ALLOC;\r\nret = 0;\r\nunlock:\r\nspin_unlock_irq(&cq->lock);\r\nif (ret)\r\nreturn ret;\r\nret = mthca_alloc_cq_buf(dev, &cq->resize_buf->buf, entries);\r\nif (ret) {\r\nspin_lock_irq(&cq->lock);\r\nkfree(cq->resize_buf);\r\ncq->resize_buf = NULL;\r\nspin_unlock_irq(&cq->lock);\r\nreturn ret;\r\n}\r\ncq->resize_buf->cqe = entries - 1;\r\nspin_lock_irq(&cq->lock);\r\ncq->resize_buf->state = CQ_RESIZE_READY;\r\nspin_unlock_irq(&cq->lock);\r\nreturn 0;\r\n}\r\nstatic int mthca_resize_cq(struct ib_cq *ibcq, int entries, struct ib_udata *udata)\r\n{\r\nstruct mthca_dev *dev = to_mdev(ibcq->device);\r\nstruct mthca_cq *cq = to_mcq(ibcq);\r\nstruct mthca_resize_cq ucmd;\r\nu32 lkey;\r\nint ret;\r\nif (entries < 1 || entries > dev->limits.max_cqes)\r\nreturn -EINVAL;\r\nmutex_lock(&cq->mutex);\r\nentries = roundup_pow_of_two(entries + 1);\r\nif (entries == ibcq->cqe + 1) {\r\nret = 0;\r\ngoto out;\r\n}\r\nif (cq->is_kernel) {\r\nret = mthca_alloc_resize_buf(dev, cq, entries);\r\nif (ret)\r\ngoto out;\r\nlkey = cq->resize_buf->buf.mr.ibmr.lkey;\r\n} else {\r\nif (ib_copy_from_udata(&ucmd, udata, sizeof ucmd)) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\nlkey = ucmd.lkey;\r\n}\r\nret = mthca_RESIZE_CQ(dev, cq->cqn, lkey, ilog2(entries));\r\nif (ret) {\r\nif (cq->resize_buf) {\r\nmthca_free_cq_buf(dev, &cq->resize_buf->buf,\r\ncq->resize_buf->cqe);\r\nkfree(cq->resize_buf);\r\nspin_lock_irq(&cq->lock);\r\ncq->resize_buf = NULL;\r\nspin_unlock_irq(&cq->lock);\r\n}\r\ngoto out;\r\n}\r\nif (cq->is_kernel) {\r\nstruct mthca_cq_buf tbuf;\r\nint tcqe;\r\nspin_lock_irq(&cq->lock);\r\nif (cq->resize_buf->state == CQ_RESIZE_READY) {\r\nmthca_cq_resize_copy_cqes(cq);\r\ntbuf = cq->buf;\r\ntcqe = cq->ibcq.cqe;\r\ncq->buf = cq->resize_buf->buf;\r\ncq->ibcq.cqe = cq->resize_buf->cqe;\r\n} else {\r\ntbuf = cq->resize_buf->buf;\r\ntcqe = cq->resize_buf->cqe;\r\n}\r\nkfree(cq->resize_buf);\r\ncq->resize_buf = NULL;\r\nspin_unlock_irq(&cq->lock);\r\nmthca_free_cq_buf(dev, &tbuf, tcqe);\r\n} else\r\nibcq->cqe = entries - 1;\r\nout:\r\nmutex_unlock(&cq->mutex);\r\nreturn ret;\r\n}\r\nstatic int mthca_destroy_cq(struct ib_cq *cq)\r\n{\r\nif (cq->uobject) {\r\nmthca_unmap_user_db(to_mdev(cq->device),\r\n&to_mucontext(cq->uobject->context)->uar,\r\nto_mucontext(cq->uobject->context)->db_tab,\r\nto_mcq(cq)->arm_db_index);\r\nmthca_unmap_user_db(to_mdev(cq->device),\r\n&to_mucontext(cq->uobject->context)->uar,\r\nto_mucontext(cq->uobject->context)->db_tab,\r\nto_mcq(cq)->set_ci_db_index);\r\n}\r\nmthca_free_cq(to_mdev(cq->device), to_mcq(cq));\r\nkfree(cq);\r\nreturn 0;\r\n}\r\nstatic inline u32 convert_access(int acc)\r\n{\r\nreturn (acc & IB_ACCESS_REMOTE_ATOMIC ? MTHCA_MPT_FLAG_ATOMIC : 0) |\r\n(acc & IB_ACCESS_REMOTE_WRITE ? MTHCA_MPT_FLAG_REMOTE_WRITE : 0) |\r\n(acc & IB_ACCESS_REMOTE_READ ? MTHCA_MPT_FLAG_REMOTE_READ : 0) |\r\n(acc & IB_ACCESS_LOCAL_WRITE ? MTHCA_MPT_FLAG_LOCAL_WRITE : 0) |\r\nMTHCA_MPT_FLAG_LOCAL_READ;\r\n}\r\nstatic struct ib_mr *mthca_get_dma_mr(struct ib_pd *pd, int acc)\r\n{\r\nstruct mthca_mr *mr;\r\nint err;\r\nmr = kmalloc(sizeof *mr, GFP_KERNEL);\r\nif (!mr)\r\nreturn ERR_PTR(-ENOMEM);\r\nerr = mthca_mr_alloc_notrans(to_mdev(pd->device),\r\nto_mpd(pd)->pd_num,\r\nconvert_access(acc), mr);\r\nif (err) {\r\nkfree(mr);\r\nreturn ERR_PTR(err);\r\n}\r\nmr->umem = NULL;\r\nreturn &mr->ibmr;\r\n}\r\nstatic struct ib_mr *mthca_reg_phys_mr(struct ib_pd *pd,\r\nstruct ib_phys_buf *buffer_list,\r\nint num_phys_buf,\r\nint acc,\r\nu64 *iova_start)\r\n{\r\nstruct mthca_mr *mr;\r\nu64 *page_list;\r\nu64 total_size;\r\nunsigned long mask;\r\nint shift;\r\nint npages;\r\nint err;\r\nint i, j, n;\r\nmask = buffer_list[0].addr ^ *iova_start;\r\ntotal_size = 0;\r\nfor (i = 0; i < num_phys_buf; ++i) {\r\nif (i != 0)\r\nmask |= buffer_list[i].addr;\r\nif (i != num_phys_buf - 1)\r\nmask |= buffer_list[i].addr + buffer_list[i].size;\r\ntotal_size += buffer_list[i].size;\r\n}\r\nif (mask & ~PAGE_MASK)\r\nreturn ERR_PTR(-EINVAL);\r\nshift = __ffs(mask | 1 << 31);\r\nbuffer_list[0].size += buffer_list[0].addr & ((1ULL << shift) - 1);\r\nbuffer_list[0].addr &= ~0ull << shift;\r\nmr = kmalloc(sizeof *mr, GFP_KERNEL);\r\nif (!mr)\r\nreturn ERR_PTR(-ENOMEM);\r\nnpages = 0;\r\nfor (i = 0; i < num_phys_buf; ++i)\r\nnpages += (buffer_list[i].size + (1ULL << shift) - 1) >> shift;\r\nif (!npages)\r\nreturn &mr->ibmr;\r\npage_list = kmalloc(npages * sizeof *page_list, GFP_KERNEL);\r\nif (!page_list) {\r\nkfree(mr);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nn = 0;\r\nfor (i = 0; i < num_phys_buf; ++i)\r\nfor (j = 0;\r\nj < (buffer_list[i].size + (1ULL << shift) - 1) >> shift;\r\n++j)\r\npage_list[n++] = buffer_list[i].addr + ((u64) j << shift);\r\nmthca_dbg(to_mdev(pd->device), "Registering memory at %llx (iova %llx) "\r\n"in PD %x; shift %d, npages %d.\n",\r\n(unsigned long long) buffer_list[0].addr,\r\n(unsigned long long) *iova_start,\r\nto_mpd(pd)->pd_num,\r\nshift, npages);\r\nerr = mthca_mr_alloc_phys(to_mdev(pd->device),\r\nto_mpd(pd)->pd_num,\r\npage_list, shift, npages,\r\n*iova_start, total_size,\r\nconvert_access(acc), mr);\r\nif (err) {\r\nkfree(page_list);\r\nkfree(mr);\r\nreturn ERR_PTR(err);\r\n}\r\nkfree(page_list);\r\nmr->umem = NULL;\r\nreturn &mr->ibmr;\r\n}\r\nstatic struct ib_mr *mthca_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\r\nu64 virt, int acc, struct ib_udata *udata)\r\n{\r\nstruct mthca_dev *dev = to_mdev(pd->device);\r\nstruct scatterlist *sg;\r\nstruct mthca_mr *mr;\r\nstruct mthca_reg_mr ucmd;\r\nu64 *pages;\r\nint shift, n, len;\r\nint i, k, entry;\r\nint err = 0;\r\nint write_mtt_size;\r\nif (udata->inlen - sizeof (struct ib_uverbs_cmd_hdr) < sizeof ucmd) {\r\nif (!to_mucontext(pd->uobject->context)->reg_mr_warned) {\r\nmthca_warn(dev, "Process '%s' did not pass in MR attrs.\n",\r\ncurrent->comm);\r\nmthca_warn(dev, " Update libmthca to fix this.\n");\r\n}\r\n++to_mucontext(pd->uobject->context)->reg_mr_warned;\r\nucmd.mr_attrs = 0;\r\n} else if (ib_copy_from_udata(&ucmd, udata, sizeof ucmd))\r\nreturn ERR_PTR(-EFAULT);\r\nmr = kmalloc(sizeof *mr, GFP_KERNEL);\r\nif (!mr)\r\nreturn ERR_PTR(-ENOMEM);\r\nmr->umem = ib_umem_get(pd->uobject->context, start, length, acc,\r\nucmd.mr_attrs & MTHCA_MR_DMASYNC);\r\nif (IS_ERR(mr->umem)) {\r\nerr = PTR_ERR(mr->umem);\r\ngoto err;\r\n}\r\nshift = ffs(mr->umem->page_size) - 1;\r\nn = mr->umem->nmap;\r\nmr->mtt = mthca_alloc_mtt(dev, n);\r\nif (IS_ERR(mr->mtt)) {\r\nerr = PTR_ERR(mr->mtt);\r\ngoto err_umem;\r\n}\r\npages = (u64 *) __get_free_page(GFP_KERNEL);\r\nif (!pages) {\r\nerr = -ENOMEM;\r\ngoto err_mtt;\r\n}\r\ni = n = 0;\r\nwrite_mtt_size = min(mthca_write_mtt_size(dev), (int) (PAGE_SIZE / sizeof *pages));\r\nfor_each_sg(mr->umem->sg_head.sgl, sg, mr->umem->nmap, entry) {\r\nlen = sg_dma_len(sg) >> shift;\r\nfor (k = 0; k < len; ++k) {\r\npages[i++] = sg_dma_address(sg) +\r\nmr->umem->page_size * k;\r\nif (i == write_mtt_size) {\r\nerr = mthca_write_mtt(dev, mr->mtt, n, pages, i);\r\nif (err)\r\ngoto mtt_done;\r\nn += i;\r\ni = 0;\r\n}\r\n}\r\n}\r\nif (i)\r\nerr = mthca_write_mtt(dev, mr->mtt, n, pages, i);\r\nmtt_done:\r\nfree_page((unsigned long) pages);\r\nif (err)\r\ngoto err_mtt;\r\nerr = mthca_mr_alloc(dev, to_mpd(pd)->pd_num, shift, virt, length,\r\nconvert_access(acc), mr);\r\nif (err)\r\ngoto err_mtt;\r\nreturn &mr->ibmr;\r\nerr_mtt:\r\nmthca_free_mtt(dev, mr->mtt);\r\nerr_umem:\r\nib_umem_release(mr->umem);\r\nerr:\r\nkfree(mr);\r\nreturn ERR_PTR(err);\r\n}\r\nstatic int mthca_dereg_mr(struct ib_mr *mr)\r\n{\r\nstruct mthca_mr *mmr = to_mmr(mr);\r\nmthca_free_mr(to_mdev(mr->device), mmr);\r\nif (mmr->umem)\r\nib_umem_release(mmr->umem);\r\nkfree(mmr);\r\nreturn 0;\r\n}\r\nstatic struct ib_fmr *mthca_alloc_fmr(struct ib_pd *pd, int mr_access_flags,\r\nstruct ib_fmr_attr *fmr_attr)\r\n{\r\nstruct mthca_fmr *fmr;\r\nint err;\r\nfmr = kmalloc(sizeof *fmr, GFP_KERNEL);\r\nif (!fmr)\r\nreturn ERR_PTR(-ENOMEM);\r\nmemcpy(&fmr->attr, fmr_attr, sizeof *fmr_attr);\r\nerr = mthca_fmr_alloc(to_mdev(pd->device), to_mpd(pd)->pd_num,\r\nconvert_access(mr_access_flags), fmr);\r\nif (err) {\r\nkfree(fmr);\r\nreturn ERR_PTR(err);\r\n}\r\nreturn &fmr->ibmr;\r\n}\r\nstatic int mthca_dealloc_fmr(struct ib_fmr *fmr)\r\n{\r\nstruct mthca_fmr *mfmr = to_mfmr(fmr);\r\nint err;\r\nerr = mthca_free_fmr(to_mdev(fmr->device), mfmr);\r\nif (err)\r\nreturn err;\r\nkfree(mfmr);\r\nreturn 0;\r\n}\r\nstatic int mthca_unmap_fmr(struct list_head *fmr_list)\r\n{\r\nstruct ib_fmr *fmr;\r\nint err;\r\nstruct mthca_dev *mdev = NULL;\r\nlist_for_each_entry(fmr, fmr_list, list) {\r\nif (mdev && to_mdev(fmr->device) != mdev)\r\nreturn -EINVAL;\r\nmdev = to_mdev(fmr->device);\r\n}\r\nif (!mdev)\r\nreturn 0;\r\nif (mthca_is_memfree(mdev)) {\r\nlist_for_each_entry(fmr, fmr_list, list)\r\nmthca_arbel_fmr_unmap(mdev, to_mfmr(fmr));\r\nwmb();\r\n} else\r\nlist_for_each_entry(fmr, fmr_list, list)\r\nmthca_tavor_fmr_unmap(mdev, to_mfmr(fmr));\r\nerr = mthca_SYNC_TPT(mdev);\r\nreturn err;\r\n}\r\nstatic ssize_t show_rev(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct mthca_dev *dev =\r\ncontainer_of(device, struct mthca_dev, ib_dev.dev);\r\nreturn sprintf(buf, "%x\n", dev->rev_id);\r\n}\r\nstatic ssize_t show_fw_ver(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct mthca_dev *dev =\r\ncontainer_of(device, struct mthca_dev, ib_dev.dev);\r\nreturn sprintf(buf, "%d.%d.%d\n", (int) (dev->fw_ver >> 32),\r\n(int) (dev->fw_ver >> 16) & 0xffff,\r\n(int) dev->fw_ver & 0xffff);\r\n}\r\nstatic ssize_t show_hca(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct mthca_dev *dev =\r\ncontainer_of(device, struct mthca_dev, ib_dev.dev);\r\nswitch (dev->pdev->device) {\r\ncase PCI_DEVICE_ID_MELLANOX_TAVOR:\r\nreturn sprintf(buf, "MT23108\n");\r\ncase PCI_DEVICE_ID_MELLANOX_ARBEL_COMPAT:\r\nreturn sprintf(buf, "MT25208 (MT23108 compat mode)\n");\r\ncase PCI_DEVICE_ID_MELLANOX_ARBEL:\r\nreturn sprintf(buf, "MT25208\n");\r\ncase PCI_DEVICE_ID_MELLANOX_SINAI:\r\ncase PCI_DEVICE_ID_MELLANOX_SINAI_OLD:\r\nreturn sprintf(buf, "MT25204\n");\r\ndefault:\r\nreturn sprintf(buf, "unknown\n");\r\n}\r\n}\r\nstatic ssize_t show_board(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct mthca_dev *dev =\r\ncontainer_of(device, struct mthca_dev, ib_dev.dev);\r\nreturn sprintf(buf, "%.*s\n", MTHCA_BOARD_ID_LEN, dev->board_id);\r\n}\r\nstatic int mthca_init_node_data(struct mthca_dev *dev)\r\n{\r\nstruct ib_smp *in_mad = NULL;\r\nstruct ib_smp *out_mad = NULL;\r\nint err = -ENOMEM;\r\nin_mad = kzalloc(sizeof *in_mad, GFP_KERNEL);\r\nout_mad = kmalloc(sizeof *out_mad, GFP_KERNEL);\r\nif (!in_mad || !out_mad)\r\ngoto out;\r\ninit_query_mad(in_mad);\r\nin_mad->attr_id = IB_SMP_ATTR_NODE_DESC;\r\nerr = mthca_MAD_IFC(dev, 1, 1,\r\n1, NULL, NULL, in_mad, out_mad);\r\nif (err)\r\ngoto out;\r\nmemcpy(dev->ib_dev.node_desc, out_mad->data, 64);\r\nin_mad->attr_id = IB_SMP_ATTR_NODE_INFO;\r\nerr = mthca_MAD_IFC(dev, 1, 1,\r\n1, NULL, NULL, in_mad, out_mad);\r\nif (err)\r\ngoto out;\r\nif (mthca_is_memfree(dev))\r\ndev->rev_id = be32_to_cpup((__be32 *) (out_mad->data + 32));\r\nmemcpy(&dev->ib_dev.node_guid, out_mad->data + 12, 8);\r\nout:\r\nkfree(in_mad);\r\nkfree(out_mad);\r\nreturn err;\r\n}\r\nint mthca_register_device(struct mthca_dev *dev)\r\n{\r\nint ret;\r\nint i;\r\nret = mthca_init_node_data(dev);\r\nif (ret)\r\nreturn ret;\r\nstrlcpy(dev->ib_dev.name, "mthca%d", IB_DEVICE_NAME_MAX);\r\ndev->ib_dev.owner = THIS_MODULE;\r\ndev->ib_dev.uverbs_abi_ver = MTHCA_UVERBS_ABI_VERSION;\r\ndev->ib_dev.uverbs_cmd_mask =\r\n(1ull << IB_USER_VERBS_CMD_GET_CONTEXT) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_DEVICE) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_PORT) |\r\n(1ull << IB_USER_VERBS_CMD_ALLOC_PD) |\r\n(1ull << IB_USER_VERBS_CMD_DEALLOC_PD) |\r\n(1ull << IB_USER_VERBS_CMD_REG_MR) |\r\n(1ull << IB_USER_VERBS_CMD_DEREG_MR) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_RESIZE_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_QP) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_MODIFY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_ATTACH_MCAST) |\r\n(1ull << IB_USER_VERBS_CMD_DETACH_MCAST);\r\ndev->ib_dev.node_type = RDMA_NODE_IB_CA;\r\ndev->ib_dev.phys_port_cnt = dev->limits.num_ports;\r\ndev->ib_dev.num_comp_vectors = 1;\r\ndev->ib_dev.dma_device = &dev->pdev->dev;\r\ndev->ib_dev.query_device = mthca_query_device;\r\ndev->ib_dev.query_port = mthca_query_port;\r\ndev->ib_dev.modify_device = mthca_modify_device;\r\ndev->ib_dev.modify_port = mthca_modify_port;\r\ndev->ib_dev.query_pkey = mthca_query_pkey;\r\ndev->ib_dev.query_gid = mthca_query_gid;\r\ndev->ib_dev.alloc_ucontext = mthca_alloc_ucontext;\r\ndev->ib_dev.dealloc_ucontext = mthca_dealloc_ucontext;\r\ndev->ib_dev.mmap = mthca_mmap_uar;\r\ndev->ib_dev.alloc_pd = mthca_alloc_pd;\r\ndev->ib_dev.dealloc_pd = mthca_dealloc_pd;\r\ndev->ib_dev.create_ah = mthca_ah_create;\r\ndev->ib_dev.query_ah = mthca_ah_query;\r\ndev->ib_dev.destroy_ah = mthca_ah_destroy;\r\nif (dev->mthca_flags & MTHCA_FLAG_SRQ) {\r\ndev->ib_dev.create_srq = mthca_create_srq;\r\ndev->ib_dev.modify_srq = mthca_modify_srq;\r\ndev->ib_dev.query_srq = mthca_query_srq;\r\ndev->ib_dev.destroy_srq = mthca_destroy_srq;\r\ndev->ib_dev.uverbs_cmd_mask |=\r\n(1ull << IB_USER_VERBS_CMD_CREATE_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_MODIFY_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_SRQ);\r\nif (mthca_is_memfree(dev))\r\ndev->ib_dev.post_srq_recv = mthca_arbel_post_srq_recv;\r\nelse\r\ndev->ib_dev.post_srq_recv = mthca_tavor_post_srq_recv;\r\n}\r\ndev->ib_dev.create_qp = mthca_create_qp;\r\ndev->ib_dev.modify_qp = mthca_modify_qp;\r\ndev->ib_dev.query_qp = mthca_query_qp;\r\ndev->ib_dev.destroy_qp = mthca_destroy_qp;\r\ndev->ib_dev.create_cq = mthca_create_cq;\r\ndev->ib_dev.resize_cq = mthca_resize_cq;\r\ndev->ib_dev.destroy_cq = mthca_destroy_cq;\r\ndev->ib_dev.poll_cq = mthca_poll_cq;\r\ndev->ib_dev.get_dma_mr = mthca_get_dma_mr;\r\ndev->ib_dev.reg_phys_mr = mthca_reg_phys_mr;\r\ndev->ib_dev.reg_user_mr = mthca_reg_user_mr;\r\ndev->ib_dev.dereg_mr = mthca_dereg_mr;\r\nif (dev->mthca_flags & MTHCA_FLAG_FMR) {\r\ndev->ib_dev.alloc_fmr = mthca_alloc_fmr;\r\ndev->ib_dev.unmap_fmr = mthca_unmap_fmr;\r\ndev->ib_dev.dealloc_fmr = mthca_dealloc_fmr;\r\nif (mthca_is_memfree(dev))\r\ndev->ib_dev.map_phys_fmr = mthca_arbel_map_phys_fmr;\r\nelse\r\ndev->ib_dev.map_phys_fmr = mthca_tavor_map_phys_fmr;\r\n}\r\ndev->ib_dev.attach_mcast = mthca_multicast_attach;\r\ndev->ib_dev.detach_mcast = mthca_multicast_detach;\r\ndev->ib_dev.process_mad = mthca_process_mad;\r\nif (mthca_is_memfree(dev)) {\r\ndev->ib_dev.req_notify_cq = mthca_arbel_arm_cq;\r\ndev->ib_dev.post_send = mthca_arbel_post_send;\r\ndev->ib_dev.post_recv = mthca_arbel_post_receive;\r\n} else {\r\ndev->ib_dev.req_notify_cq = mthca_tavor_arm_cq;\r\ndev->ib_dev.post_send = mthca_tavor_post_send;\r\ndev->ib_dev.post_recv = mthca_tavor_post_receive;\r\n}\r\nmutex_init(&dev->cap_mask_mutex);\r\nret = ib_register_device(&dev->ib_dev, NULL);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < ARRAY_SIZE(mthca_dev_attributes); ++i) {\r\nret = device_create_file(&dev->ib_dev.dev,\r\nmthca_dev_attributes[i]);\r\nif (ret) {\r\nib_unregister_device(&dev->ib_dev);\r\nreturn ret;\r\n}\r\n}\r\nmthca_start_catas_poll(dev);\r\nreturn 0;\r\n}\r\nvoid mthca_unregister_device(struct mthca_dev *dev)\r\n{\r\nmthca_stop_catas_poll(dev);\r\nib_unregister_device(&dev->ib_dev);\r\n}
