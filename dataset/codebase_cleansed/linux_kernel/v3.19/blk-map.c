int blk_rq_append_bio(struct request_queue *q, struct request *rq,\r\nstruct bio *bio)\r\n{\r\nif (!rq->bio)\r\nblk_rq_bio_prep(q, rq, bio);\r\nelse if (!ll_back_merge_fn(q, rq, bio))\r\nreturn -EINVAL;\r\nelse {\r\nrq->biotail->bi_next = bio;\r\nrq->biotail = bio;\r\nrq->__data_len += bio->bi_iter.bi_size;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __blk_rq_unmap_user(struct bio *bio)\r\n{\r\nint ret = 0;\r\nif (bio) {\r\nif (bio_flagged(bio, BIO_USER_MAPPED))\r\nbio_unmap_user(bio);\r\nelse\r\nret = bio_uncopy_user(bio);\r\n}\r\nreturn ret;\r\n}\r\nstatic int __blk_rq_map_user(struct request_queue *q, struct request *rq,\r\nstruct rq_map_data *map_data, void __user *ubuf,\r\nunsigned int len, gfp_t gfp_mask)\r\n{\r\nunsigned long uaddr;\r\nstruct bio *bio, *orig_bio;\r\nint reading, ret;\r\nreading = rq_data_dir(rq) == READ;\r\nuaddr = (unsigned long) ubuf;\r\nif (blk_rq_aligned(q, uaddr, len) && !map_data)\r\nbio = bio_map_user(q, NULL, uaddr, len, reading, gfp_mask);\r\nelse\r\nbio = bio_copy_user(q, map_data, uaddr, len, reading, gfp_mask);\r\nif (IS_ERR(bio))\r\nreturn PTR_ERR(bio);\r\nif (map_data && map_data->null_mapped)\r\nbio->bi_flags |= (1 << BIO_NULL_MAPPED);\r\norig_bio = bio;\r\nblk_queue_bounce(q, &bio);\r\nbio_get(bio);\r\nret = blk_rq_append_bio(q, rq, bio);\r\nif (!ret)\r\nreturn bio->bi_iter.bi_size;\r\nbio_endio(bio, 0);\r\n__blk_rq_unmap_user(orig_bio);\r\nbio_put(bio);\r\nreturn ret;\r\n}\r\nint blk_rq_map_user(struct request_queue *q, struct request *rq,\r\nstruct rq_map_data *map_data, void __user *ubuf,\r\nunsigned long len, gfp_t gfp_mask)\r\n{\r\nunsigned long bytes_read = 0;\r\nstruct bio *bio = NULL;\r\nint ret;\r\nif (len > (queue_max_hw_sectors(q) << 9))\r\nreturn -EINVAL;\r\nif (!len)\r\nreturn -EINVAL;\r\nif (!ubuf && (!map_data || !map_data->null_mapped))\r\nreturn -EINVAL;\r\nwhile (bytes_read != len) {\r\nunsigned long map_len, end, start;\r\nmap_len = min_t(unsigned long, len - bytes_read, BIO_MAX_SIZE);\r\nend = ((unsigned long)ubuf + map_len + PAGE_SIZE - 1)\r\n>> PAGE_SHIFT;\r\nstart = (unsigned long)ubuf >> PAGE_SHIFT;\r\nif (end - start > BIO_MAX_PAGES)\r\nmap_len -= PAGE_SIZE;\r\nret = __blk_rq_map_user(q, rq, map_data, ubuf, map_len,\r\ngfp_mask);\r\nif (ret < 0)\r\ngoto unmap_rq;\r\nif (!bio)\r\nbio = rq->bio;\r\nbytes_read += ret;\r\nubuf += ret;\r\nif (map_data)\r\nmap_data->offset += ret;\r\n}\r\nif (!bio_flagged(bio, BIO_USER_MAPPED))\r\nrq->cmd_flags |= REQ_COPY_USER;\r\nreturn 0;\r\nunmap_rq:\r\nblk_rq_unmap_user(bio);\r\nrq->bio = NULL;\r\nreturn ret;\r\n}\r\nint blk_rq_map_user_iov(struct request_queue *q, struct request *rq,\r\nstruct rq_map_data *map_data, const struct sg_iovec *iov,\r\nint iov_count, unsigned int len, gfp_t gfp_mask)\r\n{\r\nstruct bio *bio;\r\nint i, read = rq_data_dir(rq) == READ;\r\nint unaligned = 0;\r\nif (!iov || iov_count <= 0)\r\nreturn -EINVAL;\r\nfor (i = 0; i < iov_count; i++) {\r\nunsigned long uaddr = (unsigned long)iov[i].iov_base;\r\nif (!iov[i].iov_len)\r\nreturn -EINVAL;\r\nif (uaddr & queue_dma_alignment(q))\r\nunaligned = 1;\r\n}\r\nif (unaligned || (q->dma_pad_mask & len) || map_data)\r\nbio = bio_copy_user_iov(q, map_data, iov, iov_count, read,\r\ngfp_mask);\r\nelse\r\nbio = bio_map_user_iov(q, NULL, iov, iov_count, read, gfp_mask);\r\nif (IS_ERR(bio))\r\nreturn PTR_ERR(bio);\r\nif (bio->bi_iter.bi_size != len) {\r\nbio_get(bio);\r\nbio_endio(bio, 0);\r\n__blk_rq_unmap_user(bio);\r\nreturn -EINVAL;\r\n}\r\nif (!bio_flagged(bio, BIO_USER_MAPPED))\r\nrq->cmd_flags |= REQ_COPY_USER;\r\nblk_queue_bounce(q, &bio);\r\nbio_get(bio);\r\nblk_rq_bio_prep(q, rq, bio);\r\nreturn 0;\r\n}\r\nint blk_rq_unmap_user(struct bio *bio)\r\n{\r\nstruct bio *mapped_bio;\r\nint ret = 0, ret2;\r\nwhile (bio) {\r\nmapped_bio = bio;\r\nif (unlikely(bio_flagged(bio, BIO_BOUNCED)))\r\nmapped_bio = bio->bi_private;\r\nret2 = __blk_rq_unmap_user(mapped_bio);\r\nif (ret2 && !ret)\r\nret = ret2;\r\nmapped_bio = bio;\r\nbio = bio->bi_next;\r\nbio_put(mapped_bio);\r\n}\r\nreturn ret;\r\n}\r\nint blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,\r\nunsigned int len, gfp_t gfp_mask)\r\n{\r\nint reading = rq_data_dir(rq) == READ;\r\nunsigned long addr = (unsigned long) kbuf;\r\nint do_copy = 0;\r\nstruct bio *bio;\r\nint ret;\r\nif (len > (queue_max_hw_sectors(q) << 9))\r\nreturn -EINVAL;\r\nif (!len || !kbuf)\r\nreturn -EINVAL;\r\ndo_copy = !blk_rq_aligned(q, addr, len) || object_is_on_stack(kbuf);\r\nif (do_copy)\r\nbio = bio_copy_kern(q, kbuf, len, gfp_mask, reading);\r\nelse\r\nbio = bio_map_kern(q, kbuf, len, gfp_mask);\r\nif (IS_ERR(bio))\r\nreturn PTR_ERR(bio);\r\nif (!reading)\r\nbio->bi_rw |= REQ_WRITE;\r\nif (do_copy)\r\nrq->cmd_flags |= REQ_COPY_USER;\r\nret = blk_rq_append_bio(q, rq, bio);\r\nif (unlikely(ret)) {\r\nbio_put(bio);\r\nreturn ret;\r\n}\r\nblk_queue_bounce(q, &rq->bio);\r\nreturn 0;\r\n}
