static inline unsigned long wait_piowc(void)\r\n{\r\nvolatile unsigned long *piows;\r\nunsigned long zeroval, ws;\r\npiows = pda->pio_write_status_addr;\r\nzeroval = pda->pio_write_status_val;\r\ndo {\r\ncpu_relax();\r\n} while (((ws = *piows) & SH_PIO_WRITE_STATUS_PENDING_WRITE_COUNT_MASK) != zeroval);\r\nreturn (ws & SH_PIO_WRITE_STATUS_WRITE_DEADLOCK_MASK) != 0;\r\n}\r\nvoid sn_migrate(struct task_struct *task)\r\n{\r\npda_t *last_pda = pdacpu(task_thread_info(task)->last_cpu);\r\nvolatile unsigned long *adr = last_pda->pio_write_status_addr;\r\nunsigned long val = last_pda->pio_write_status_val;\r\nwhile (unlikely((*adr & SH_PIO_WRITE_STATUS_PENDING_WRITE_COUNT_MASK)\r\n!= val))\r\ncpu_relax();\r\n}\r\nvoid sn_tlb_migrate_finish(struct mm_struct *mm)\r\n{\r\nif (mm == current->mm && mm && atomic_read(&mm->mm_users) == 1)\r\nflush_tlb_mm(mm);\r\n}\r\nstatic void\r\nsn2_ipi_flush_all_tlb(struct mm_struct *mm)\r\n{\r\nunsigned long itc;\r\nitc = ia64_get_itc();\r\nsmp_flush_tlb_cpumask(*mm_cpumask(mm));\r\nitc = ia64_get_itc() - itc;\r\n__this_cpu_add(ptcstats.shub_ipi_flushes_itc_clocks, itc);\r\n__this_cpu_inc(ptcstats.shub_ipi_flushes);\r\n}\r\nvoid\r\nsn2_global_tlb_purge(struct mm_struct *mm, unsigned long start,\r\nunsigned long end, unsigned long nbits)\r\n{\r\nint i, ibegin, shub1, cnode, mynasid, cpu, lcpu = 0, nasid;\r\nint mymm = (mm == current->active_mm && mm == current->mm);\r\nint use_cpu_ptcga;\r\nvolatile unsigned long *ptc0, *ptc1;\r\nunsigned long itc, itc2, flags, data0 = 0, data1 = 0, rr_value, old_rr = 0;\r\nshort nasids[MAX_NUMNODES], nix;\r\nnodemask_t nodes_flushed;\r\nint active, max_active, deadlock, flush_opt = sn2_flush_opt;\r\nif (flush_opt > 2) {\r\nsn2_ipi_flush_all_tlb(mm);\r\nreturn;\r\n}\r\nnodes_clear(nodes_flushed);\r\ni = 0;\r\nfor_each_cpu(cpu, mm_cpumask(mm)) {\r\ncnode = cpu_to_node(cpu);\r\nnode_set(cnode, nodes_flushed);\r\nlcpu = cpu;\r\ni++;\r\n}\r\nif (i == 0)\r\nreturn;\r\npreempt_disable();\r\nif (likely(i == 1 && lcpu == smp_processor_id() && mymm)) {\r\ndo {\r\nia64_ptcl(start, nbits << 2);\r\nstart += (1UL << nbits);\r\n} while (start < end);\r\nia64_srlz_i();\r\n__this_cpu_inc(ptcstats.ptc_l);\r\npreempt_enable();\r\nreturn;\r\n}\r\nif (atomic_read(&mm->mm_users) == 1 && mymm) {\r\nflush_tlb_mm(mm);\r\n__this_cpu_inc(ptcstats.change_rid);\r\npreempt_enable();\r\nreturn;\r\n}\r\nif (flush_opt == 2) {\r\nsn2_ipi_flush_all_tlb(mm);\r\npreempt_enable();\r\nreturn;\r\n}\r\nitc = ia64_get_itc();\r\nnix = 0;\r\nfor_each_node_mask(cnode, nodes_flushed)\r\nnasids[nix++] = cnodeid_to_nasid(cnode);\r\nrr_value = (mm->context << 3) | REGION_NUMBER(start);\r\nshub1 = is_shub1();\r\nif (shub1) {\r\ndata0 = (1UL << SH1_PTC_0_A_SHFT) |\r\n(nbits << SH1_PTC_0_PS_SHFT) |\r\n(rr_value << SH1_PTC_0_RID_SHFT) |\r\n(1UL << SH1_PTC_0_START_SHFT);\r\nptc0 = (long *)GLOBAL_MMR_PHYS_ADDR(0, SH1_PTC_0);\r\nptc1 = (long *)GLOBAL_MMR_PHYS_ADDR(0, SH1_PTC_1);\r\n} else {\r\ndata0 = (1UL << SH2_PTC_A_SHFT) |\r\n(nbits << SH2_PTC_PS_SHFT) |\r\n(1UL << SH2_PTC_START_SHFT);\r\nptc0 = (long *)GLOBAL_MMR_PHYS_ADDR(0, SH2_PTC +\r\n(rr_value << SH2_PTC_RID_SHFT));\r\nptc1 = NULL;\r\n}\r\nmynasid = get_nasid();\r\nuse_cpu_ptcga = local_node_uses_ptc_ga(shub1);\r\nmax_active = max_active_pio(shub1);\r\nitc = ia64_get_itc();\r\nspin_lock_irqsave(PTC_LOCK(shub1), flags);\r\nitc2 = ia64_get_itc();\r\n__this_cpu_add(ptcstats.lock_itc_clocks, itc2 - itc);\r\n__this_cpu_inc(ptcstats.shub_ptc_flushes);\r\n__this_cpu_add(ptcstats.nodes_flushed, nix);\r\nif (!mymm)\r\n__this_cpu_inc(ptcstats.shub_ptc_flushes_not_my_mm);\r\nif (use_cpu_ptcga && !mymm) {\r\nold_rr = ia64_get_rr(start);\r\nia64_set_rr(start, (old_rr & 0xff) | (rr_value << 8));\r\nia64_srlz_d();\r\n}\r\nwait_piowc();\r\ndo {\r\nif (shub1)\r\ndata1 = start | (1UL << SH1_PTC_1_START_SHFT);\r\nelse\r\ndata0 = (data0 & ~SH2_PTC_ADDR_MASK) | (start & SH2_PTC_ADDR_MASK);\r\ndeadlock = 0;\r\nactive = 0;\r\nfor (ibegin = 0, i = 0; i < nix; i++) {\r\nnasid = nasids[i];\r\nif (use_cpu_ptcga && unlikely(nasid == mynasid)) {\r\nia64_ptcga(start, nbits << 2);\r\nia64_srlz_i();\r\n} else {\r\nptc0 = CHANGE_NASID(nasid, ptc0);\r\nif (ptc1)\r\nptc1 = CHANGE_NASID(nasid, ptc1);\r\npio_atomic_phys_write_mmrs(ptc0, data0, ptc1, data1);\r\nactive++;\r\n}\r\nif (active >= max_active || i == (nix - 1)) {\r\nif ((deadlock = wait_piowc())) {\r\nif (flush_opt == 1)\r\ngoto done;\r\nsn2_ptc_deadlock_recovery(nasids, ibegin, i, mynasid, ptc0, data0, ptc1, data1);\r\nif (reset_max_active_on_deadlock())\r\nmax_active = 1;\r\n}\r\nactive = 0;\r\nibegin = i + 1;\r\n}\r\n}\r\nstart += (1UL << nbits);\r\n} while (start < end);\r\ndone:\r\nitc2 = ia64_get_itc() - itc2;\r\n__this_cpu_add(ptcstats.shub_itc_clocks, itc2);\r\nif (itc2 > __this_cpu_read(ptcstats.shub_itc_clocks_max))\r\n__this_cpu_write(ptcstats.shub_itc_clocks_max, itc2);\r\nif (old_rr) {\r\nia64_set_rr(start, old_rr);\r\nia64_srlz_d();\r\n}\r\nspin_unlock_irqrestore(PTC_LOCK(shub1), flags);\r\nif (flush_opt == 1 && deadlock) {\r\n__this_cpu_inc(ptcstats.deadlocks);\r\nsn2_ipi_flush_all_tlb(mm);\r\n}\r\npreempt_enable();\r\n}\r\nvoid\r\nsn2_ptc_deadlock_recovery(short *nasids, short ib, short ie, int mynasid,\r\nvolatile unsigned long *ptc0, unsigned long data0,\r\nvolatile unsigned long *ptc1, unsigned long data1)\r\n{\r\nshort nasid, i;\r\nunsigned long *piows, zeroval, n;\r\n__this_cpu_inc(ptcstats.deadlocks);\r\npiows = (unsigned long *) pda->pio_write_status_addr;\r\nzeroval = pda->pio_write_status_val;\r\nfor (i=ib; i <= ie; i++) {\r\nnasid = nasids[i];\r\nif (local_node_uses_ptc_ga(is_shub1()) && nasid == mynasid)\r\ncontinue;\r\nptc0 = CHANGE_NASID(nasid, ptc0);\r\nif (ptc1)\r\nptc1 = CHANGE_NASID(nasid, ptc1);\r\nn = sn2_ptc_deadlock_recovery_core(ptc0, data0, ptc1, data1, piows, zeroval);\r\n__this_cpu_add(ptcstats.deadlocks2, n);\r\n}\r\n}\r\nvoid sn_send_IPI_phys(int nasid, long physid, int vector, int delivery_mode)\r\n{\r\nlong val;\r\nunsigned long flags = 0;\r\nvolatile long *p;\r\np = (long *)GLOBAL_MMR_PHYS_ADDR(nasid, SH_IPI_INT);\r\nval = (1UL << SH_IPI_INT_SEND_SHFT) |\r\n(physid << SH_IPI_INT_PID_SHFT) |\r\n((long)delivery_mode << SH_IPI_INT_TYPE_SHFT) |\r\n((long)vector << SH_IPI_INT_IDX_SHFT) |\r\n(0x000feeUL << SH_IPI_INT_BASE_SHFT);\r\nmb();\r\nif (enable_shub_wars_1_1()) {\r\nspin_lock_irqsave(&sn2_global_ptc_lock, flags);\r\n}\r\npio_phys_write_mmr(p, val);\r\nif (enable_shub_wars_1_1()) {\r\nwait_piowc();\r\nspin_unlock_irqrestore(&sn2_global_ptc_lock, flags);\r\n}\r\n}\r\nvoid sn2_send_IPI(int cpuid, int vector, int delivery_mode, int redirect)\r\n{\r\nlong physid;\r\nint nasid;\r\nphysid = cpu_physical_id(cpuid);\r\nnasid = cpuid_to_nasid(cpuid);\r\nif (unlikely(nasid == -1))\r\nia64_sn_get_sapic_info(physid, &nasid, NULL, NULL);\r\nsn_send_IPI_phys(nasid, physid, vector, delivery_mode);\r\n}\r\nbool sn_cpu_disable_allowed(int cpu)\r\n{\r\nif (is_shub2() && sn_prom_feature_available(PRF_CPU_DISABLE_SUPPORT)) {\r\nif (cpu != 0)\r\nreturn true;\r\nelse\r\nprintk(KERN_WARNING\r\n"Disabling the boot processor is not allowed.\n");\r\n} else\r\nprintk(KERN_WARNING\r\n"CPU disable is not supported on this system.\n");\r\nreturn false;\r\n}\r\nstatic void *sn2_ptc_seq_start(struct seq_file *file, loff_t * offset)\r\n{\r\nif (*offset < nr_cpu_ids)\r\nreturn offset;\r\nreturn NULL;\r\n}\r\nstatic void *sn2_ptc_seq_next(struct seq_file *file, void *data, loff_t * offset)\r\n{\r\n(*offset)++;\r\nif (*offset < nr_cpu_ids)\r\nreturn offset;\r\nreturn NULL;\r\n}\r\nstatic void sn2_ptc_seq_stop(struct seq_file *file, void *data)\r\n{\r\n}\r\nstatic int sn2_ptc_seq_show(struct seq_file *file, void *data)\r\n{\r\nstruct ptc_stats *stat;\r\nint cpu;\r\ncpu = *(loff_t *) data;\r\nif (!cpu) {\r\nseq_printf(file,\r\n"# cpu ptc_l newrid ptc_flushes nodes_flushed deadlocks lock_nsec shub_nsec shub_nsec_max not_my_mm deadlock2 ipi_fluches ipi_nsec\n");\r\nseq_printf(file, "# ptctest %d, flushopt %d\n", sn2_ptctest, sn2_flush_opt);\r\n}\r\nif (cpu < nr_cpu_ids && cpu_online(cpu)) {\r\nstat = &per_cpu(ptcstats, cpu);\r\nseq_printf(file, "cpu %d %ld %ld %ld %ld %ld %ld %ld %ld %ld %ld %ld %ld\n", cpu, stat->ptc_l,\r\nstat->change_rid, stat->shub_ptc_flushes, stat->nodes_flushed,\r\nstat->deadlocks,\r\n1000 * stat->lock_itc_clocks / per_cpu(ia64_cpu_info, cpu).cyc_per_usec,\r\n1000 * stat->shub_itc_clocks / per_cpu(ia64_cpu_info, cpu).cyc_per_usec,\r\n1000 * stat->shub_itc_clocks_max / per_cpu(ia64_cpu_info, cpu).cyc_per_usec,\r\nstat->shub_ptc_flushes_not_my_mm,\r\nstat->deadlocks2,\r\nstat->shub_ipi_flushes,\r\n1000 * stat->shub_ipi_flushes_itc_clocks / per_cpu(ia64_cpu_info, cpu).cyc_per_usec);\r\n}\r\nreturn 0;\r\n}\r\nstatic ssize_t sn2_ptc_proc_write(struct file *file, const char __user *user, size_t count, loff_t *data)\r\n{\r\nint cpu;\r\nchar optstr[64];\r\nif (count == 0 || count > sizeof(optstr))\r\nreturn -EINVAL;\r\nif (copy_from_user(optstr, user, count))\r\nreturn -EFAULT;\r\noptstr[count - 1] = '\0';\r\nsn2_flush_opt = simple_strtoul(optstr, NULL, 0);\r\nfor_each_online_cpu(cpu)\r\nmemset(&per_cpu(ptcstats, cpu), 0, sizeof(struct ptc_stats));\r\nreturn count;\r\n}\r\nstatic int sn2_ptc_proc_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &sn2_ptc_seq_ops);\r\n}\r\nstatic int __init sn2_ptc_init(void)\r\n{\r\nif (!ia64_platform_is("sn2"))\r\nreturn 0;\r\nproc_sn2_ptc = proc_create(PTC_BASENAME, 0444,\r\nNULL, &proc_sn2_ptc_operations);\r\nif (!proc_sn2_ptc) {\r\nprintk(KERN_ERR "unable to create %s proc entry", PTC_BASENAME);\r\nreturn -EINVAL;\r\n}\r\nspin_lock_init(&sn2_global_ptc_lock);\r\nreturn 0;\r\n}\r\nstatic void __exit sn2_ptc_exit(void)\r\n{\r\nremove_proc_entry(PTC_BASENAME, NULL);\r\n}
