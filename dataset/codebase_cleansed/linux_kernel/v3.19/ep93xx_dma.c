static inline struct device *chan2dev(struct ep93xx_dma_chan *edmac)\r\n{\r\nreturn &edmac->chan.dev->device;\r\n}\r\nstatic struct ep93xx_dma_chan *to_ep93xx_dma_chan(struct dma_chan *chan)\r\n{\r\nreturn container_of(chan, struct ep93xx_dma_chan, chan);\r\n}\r\nstatic void ep93xx_dma_set_active(struct ep93xx_dma_chan *edmac,\r\nstruct ep93xx_dma_desc *desc)\r\n{\r\nBUG_ON(!list_empty(&edmac->active));\r\nlist_add_tail(&desc->node, &edmac->active);\r\nwhile (!list_empty(&desc->tx_list)) {\r\nstruct ep93xx_dma_desc *d = list_first_entry(&desc->tx_list,\r\nstruct ep93xx_dma_desc, node);\r\nd->txd.callback = desc->txd.callback;\r\nd->txd.callback_param = desc->txd.callback_param;\r\nlist_move_tail(&d->node, &edmac->active);\r\n}\r\n}\r\nstatic struct ep93xx_dma_desc *\r\nep93xx_dma_get_active(struct ep93xx_dma_chan *edmac)\r\n{\r\nif (list_empty(&edmac->active))\r\nreturn NULL;\r\nreturn list_first_entry(&edmac->active, struct ep93xx_dma_desc, node);\r\n}\r\nstatic bool ep93xx_dma_advance_active(struct ep93xx_dma_chan *edmac)\r\n{\r\nstruct ep93xx_dma_desc *desc;\r\nlist_rotate_left(&edmac->active);\r\nif (test_bit(EP93XX_DMA_IS_CYCLIC, &edmac->flags))\r\nreturn true;\r\ndesc = ep93xx_dma_get_active(edmac);\r\nif (!desc)\r\nreturn false;\r\nreturn !desc->txd.cookie;\r\n}\r\nstatic void m2p_set_control(struct ep93xx_dma_chan *edmac, u32 control)\r\n{\r\nwritel(control, edmac->regs + M2P_CONTROL);\r\nreadl(edmac->regs + M2P_CONTROL);\r\n}\r\nstatic int m2p_hw_setup(struct ep93xx_dma_chan *edmac)\r\n{\r\nstruct ep93xx_dma_data *data = edmac->chan.private;\r\nu32 control;\r\nwritel(data->port & 0xf, edmac->regs + M2P_PPALLOC);\r\ncontrol = M2P_CONTROL_CH_ERROR_INT | M2P_CONTROL_ICE\r\n| M2P_CONTROL_ENABLE;\r\nm2p_set_control(edmac, control);\r\nreturn 0;\r\n}\r\nstatic inline u32 m2p_channel_state(struct ep93xx_dma_chan *edmac)\r\n{\r\nreturn (readl(edmac->regs + M2P_STATUS) >> 4) & 0x3;\r\n}\r\nstatic void m2p_hw_shutdown(struct ep93xx_dma_chan *edmac)\r\n{\r\nu32 control;\r\ncontrol = readl(edmac->regs + M2P_CONTROL);\r\ncontrol &= ~(M2P_CONTROL_STALLINT | M2P_CONTROL_NFBINT);\r\nm2p_set_control(edmac, control);\r\nwhile (m2p_channel_state(edmac) >= M2P_STATE_ON)\r\ncpu_relax();\r\nm2p_set_control(edmac, 0);\r\nwhile (m2p_channel_state(edmac) == M2P_STATE_STALL)\r\ncpu_relax();\r\n}\r\nstatic void m2p_fill_desc(struct ep93xx_dma_chan *edmac)\r\n{\r\nstruct ep93xx_dma_desc *desc;\r\nu32 bus_addr;\r\ndesc = ep93xx_dma_get_active(edmac);\r\nif (!desc) {\r\ndev_warn(chan2dev(edmac), "M2P: empty descriptor list\n");\r\nreturn;\r\n}\r\nif (ep93xx_dma_chan_direction(&edmac->chan) == DMA_MEM_TO_DEV)\r\nbus_addr = desc->src_addr;\r\nelse\r\nbus_addr = desc->dst_addr;\r\nif (edmac->buffer == 0) {\r\nwritel(desc->size, edmac->regs + M2P_MAXCNT0);\r\nwritel(bus_addr, edmac->regs + M2P_BASE0);\r\n} else {\r\nwritel(desc->size, edmac->regs + M2P_MAXCNT1);\r\nwritel(bus_addr, edmac->regs + M2P_BASE1);\r\n}\r\nedmac->buffer ^= 1;\r\n}\r\nstatic void m2p_hw_submit(struct ep93xx_dma_chan *edmac)\r\n{\r\nu32 control = readl(edmac->regs + M2P_CONTROL);\r\nm2p_fill_desc(edmac);\r\ncontrol |= M2P_CONTROL_STALLINT;\r\nif (ep93xx_dma_advance_active(edmac)) {\r\nm2p_fill_desc(edmac);\r\ncontrol |= M2P_CONTROL_NFBINT;\r\n}\r\nm2p_set_control(edmac, control);\r\n}\r\nstatic int m2p_hw_interrupt(struct ep93xx_dma_chan *edmac)\r\n{\r\nu32 irq_status = readl(edmac->regs + M2P_INTERRUPT);\r\nu32 control;\r\nif (irq_status & M2P_INTERRUPT_ERROR) {\r\nstruct ep93xx_dma_desc *desc = ep93xx_dma_get_active(edmac);\r\nwritel(1, edmac->regs + M2P_INTERRUPT);\r\ndev_err(chan2dev(edmac),\r\n"DMA transfer failed! Details:\n"\r\n"\tcookie : %d\n"\r\n"\tsrc_addr : 0x%08x\n"\r\n"\tdst_addr : 0x%08x\n"\r\n"\tsize : %zu\n",\r\ndesc->txd.cookie, desc->src_addr, desc->dst_addr,\r\ndesc->size);\r\n}\r\nswitch (irq_status & (M2P_INTERRUPT_STALL | M2P_INTERRUPT_NFB)) {\r\ncase M2P_INTERRUPT_STALL:\r\ncontrol = readl(edmac->regs + M2P_CONTROL);\r\ncontrol &= ~(M2P_CONTROL_STALLINT | M2P_CONTROL_NFBINT);\r\nm2p_set_control(edmac, control);\r\nreturn INTERRUPT_DONE;\r\ncase M2P_INTERRUPT_NFB:\r\nif (ep93xx_dma_advance_active(edmac))\r\nm2p_fill_desc(edmac);\r\nreturn INTERRUPT_NEXT_BUFFER;\r\n}\r\nreturn INTERRUPT_UNKNOWN;\r\n}\r\nstatic int m2m_hw_setup(struct ep93xx_dma_chan *edmac)\r\n{\r\nconst struct ep93xx_dma_data *data = edmac->chan.private;\r\nu32 control = 0;\r\nif (!data) {\r\nwritel(control, edmac->regs + M2M_CONTROL);\r\nreturn 0;\r\n}\r\nswitch (data->port) {\r\ncase EP93XX_DMA_SSP:\r\ncontrol = (5 << M2M_CONTROL_PWSC_SHIFT);\r\ncontrol |= M2M_CONTROL_NO_HDSK;\r\nif (data->direction == DMA_MEM_TO_DEV) {\r\ncontrol |= M2M_CONTROL_DAH;\r\ncontrol |= M2M_CONTROL_TM_TX;\r\ncontrol |= M2M_CONTROL_RSS_SSPTX;\r\n} else {\r\ncontrol |= M2M_CONTROL_SAH;\r\ncontrol |= M2M_CONTROL_TM_RX;\r\ncontrol |= M2M_CONTROL_RSS_SSPRX;\r\n}\r\nbreak;\r\ncase EP93XX_DMA_IDE:\r\nif (data->direction == DMA_MEM_TO_DEV) {\r\ncontrol = (3 << M2M_CONTROL_PWSC_SHIFT);\r\ncontrol |= M2M_CONTROL_DAH;\r\ncontrol |= M2M_CONTROL_TM_TX;\r\n} else {\r\ncontrol = (2 << M2M_CONTROL_PWSC_SHIFT);\r\ncontrol |= M2M_CONTROL_SAH;\r\ncontrol |= M2M_CONTROL_TM_RX;\r\n}\r\ncontrol |= M2M_CONTROL_NO_HDSK;\r\ncontrol |= M2M_CONTROL_RSS_IDE;\r\ncontrol |= M2M_CONTROL_PW_16;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nwritel(control, edmac->regs + M2M_CONTROL);\r\nreturn 0;\r\n}\r\nstatic void m2m_hw_shutdown(struct ep93xx_dma_chan *edmac)\r\n{\r\nwritel(0, edmac->regs + M2M_CONTROL);\r\n}\r\nstatic void m2m_fill_desc(struct ep93xx_dma_chan *edmac)\r\n{\r\nstruct ep93xx_dma_desc *desc;\r\ndesc = ep93xx_dma_get_active(edmac);\r\nif (!desc) {\r\ndev_warn(chan2dev(edmac), "M2M: empty descriptor list\n");\r\nreturn;\r\n}\r\nif (edmac->buffer == 0) {\r\nwritel(desc->src_addr, edmac->regs + M2M_SAR_BASE0);\r\nwritel(desc->dst_addr, edmac->regs + M2M_DAR_BASE0);\r\nwritel(desc->size, edmac->regs + M2M_BCR0);\r\n} else {\r\nwritel(desc->src_addr, edmac->regs + M2M_SAR_BASE1);\r\nwritel(desc->dst_addr, edmac->regs + M2M_DAR_BASE1);\r\nwritel(desc->size, edmac->regs + M2M_BCR1);\r\n}\r\nedmac->buffer ^= 1;\r\n}\r\nstatic void m2m_hw_submit(struct ep93xx_dma_chan *edmac)\r\n{\r\nstruct ep93xx_dma_data *data = edmac->chan.private;\r\nu32 control = readl(edmac->regs + M2M_CONTROL);\r\ncontrol &= ~M2M_CONTROL_PW_MASK;\r\ncontrol |= edmac->runtime_ctrl;\r\nm2m_fill_desc(edmac);\r\ncontrol |= M2M_CONTROL_DONEINT;\r\nif (ep93xx_dma_advance_active(edmac)) {\r\nm2m_fill_desc(edmac);\r\ncontrol |= M2M_CONTROL_NFBINT;\r\n}\r\ncontrol |= M2M_CONTROL_ENABLE;\r\nwritel(control, edmac->regs + M2M_CONTROL);\r\nif (!data) {\r\ncontrol |= M2M_CONTROL_START;\r\nwritel(control, edmac->regs + M2M_CONTROL);\r\n}\r\n}\r\nstatic int m2m_hw_interrupt(struct ep93xx_dma_chan *edmac)\r\n{\r\nu32 status = readl(edmac->regs + M2M_STATUS);\r\nu32 ctl_fsm = status & M2M_STATUS_CTL_MASK;\r\nu32 buf_fsm = status & M2M_STATUS_BUF_MASK;\r\nbool done = status & M2M_STATUS_DONE;\r\nbool last_done;\r\nu32 control;\r\nstruct ep93xx_dma_desc *desc;\r\nif (!(readl(edmac->regs + M2M_INTERRUPT) & M2M_INTERRUPT_MASK))\r\nreturn INTERRUPT_UNKNOWN;\r\nif (done) {\r\nwritel(0, edmac->regs + M2M_INTERRUPT);\r\n}\r\ndesc = ep93xx_dma_get_active(edmac);\r\nlast_done = !desc || desc->txd.cookie;\r\nif (!last_done &&\r\n(buf_fsm == M2M_STATUS_BUF_NO ||\r\nbuf_fsm == M2M_STATUS_BUF_ON)) {\r\nif (ep93xx_dma_advance_active(edmac)) {\r\nm2m_fill_desc(edmac);\r\nif (done && !edmac->chan.private) {\r\ncontrol = readl(edmac->regs + M2M_CONTROL);\r\ncontrol |= M2M_CONTROL_START;\r\nwritel(control, edmac->regs + M2M_CONTROL);\r\n}\r\nreturn INTERRUPT_NEXT_BUFFER;\r\n} else {\r\nlast_done = true;\r\n}\r\n}\r\nif (last_done &&\r\nbuf_fsm == M2M_STATUS_BUF_NO &&\r\nctl_fsm == M2M_STATUS_CTL_STALL) {\r\ncontrol = readl(edmac->regs + M2M_CONTROL);\r\ncontrol &= ~(M2M_CONTROL_DONEINT | M2M_CONTROL_NFBINT\r\n| M2M_CONTROL_ENABLE);\r\nwritel(control, edmac->regs + M2M_CONTROL);\r\nreturn INTERRUPT_DONE;\r\n}\r\nreturn INTERRUPT_NEXT_BUFFER;\r\n}\r\nstatic struct ep93xx_dma_desc *\r\nep93xx_dma_desc_get(struct ep93xx_dma_chan *edmac)\r\n{\r\nstruct ep93xx_dma_desc *desc, *_desc;\r\nstruct ep93xx_dma_desc *ret = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&edmac->lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &edmac->free_list, node) {\r\nif (async_tx_test_ack(&desc->txd)) {\r\nlist_del_init(&desc->node);\r\ndesc->src_addr = 0;\r\ndesc->dst_addr = 0;\r\ndesc->size = 0;\r\ndesc->complete = false;\r\ndesc->txd.cookie = 0;\r\ndesc->txd.callback = NULL;\r\ndesc->txd.callback_param = NULL;\r\nret = desc;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&edmac->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void ep93xx_dma_desc_put(struct ep93xx_dma_chan *edmac,\r\nstruct ep93xx_dma_desc *desc)\r\n{\r\nif (desc) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&edmac->lock, flags);\r\nlist_splice_init(&desc->tx_list, &edmac->free_list);\r\nlist_add(&desc->node, &edmac->free_list);\r\nspin_unlock_irqrestore(&edmac->lock, flags);\r\n}\r\n}\r\nstatic void ep93xx_dma_advance_work(struct ep93xx_dma_chan *edmac)\r\n{\r\nstruct ep93xx_dma_desc *new;\r\nunsigned long flags;\r\nspin_lock_irqsave(&edmac->lock, flags);\r\nif (!list_empty(&edmac->active) || list_empty(&edmac->queue)) {\r\nspin_unlock_irqrestore(&edmac->lock, flags);\r\nreturn;\r\n}\r\nnew = list_first_entry(&edmac->queue, struct ep93xx_dma_desc, node);\r\nlist_del_init(&new->node);\r\nep93xx_dma_set_active(edmac, new);\r\nedmac->edma->hw_submit(edmac);\r\nspin_unlock_irqrestore(&edmac->lock, flags);\r\n}\r\nstatic void ep93xx_dma_tasklet(unsigned long data)\r\n{\r\nstruct ep93xx_dma_chan *edmac = (struct ep93xx_dma_chan *)data;\r\nstruct ep93xx_dma_desc *desc, *d;\r\ndma_async_tx_callback callback = NULL;\r\nvoid *callback_param = NULL;\r\nLIST_HEAD(list);\r\nspin_lock_irq(&edmac->lock);\r\ndesc = ep93xx_dma_get_active(edmac);\r\nif (desc) {\r\nif (desc->complete) {\r\nif (!test_bit(EP93XX_DMA_IS_CYCLIC, &edmac->flags))\r\ndma_cookie_complete(&desc->txd);\r\nlist_splice_init(&edmac->active, &list);\r\n}\r\ncallback = desc->txd.callback;\r\ncallback_param = desc->txd.callback_param;\r\n}\r\nspin_unlock_irq(&edmac->lock);\r\nep93xx_dma_advance_work(edmac);\r\nlist_for_each_entry_safe(desc, d, &list, node) {\r\ndma_descriptor_unmap(&desc->txd);\r\nep93xx_dma_desc_put(edmac, desc);\r\n}\r\nif (callback)\r\ncallback(callback_param);\r\n}\r\nstatic irqreturn_t ep93xx_dma_interrupt(int irq, void *dev_id)\r\n{\r\nstruct ep93xx_dma_chan *edmac = dev_id;\r\nstruct ep93xx_dma_desc *desc;\r\nirqreturn_t ret = IRQ_HANDLED;\r\nspin_lock(&edmac->lock);\r\ndesc = ep93xx_dma_get_active(edmac);\r\nif (!desc) {\r\ndev_warn(chan2dev(edmac),\r\n"got interrupt while active list is empty\n");\r\nspin_unlock(&edmac->lock);\r\nreturn IRQ_NONE;\r\n}\r\nswitch (edmac->edma->hw_interrupt(edmac)) {\r\ncase INTERRUPT_DONE:\r\ndesc->complete = true;\r\ntasklet_schedule(&edmac->tasklet);\r\nbreak;\r\ncase INTERRUPT_NEXT_BUFFER:\r\nif (test_bit(EP93XX_DMA_IS_CYCLIC, &edmac->flags))\r\ntasklet_schedule(&edmac->tasklet);\r\nbreak;\r\ndefault:\r\ndev_warn(chan2dev(edmac), "unknown interrupt!\n");\r\nret = IRQ_NONE;\r\nbreak;\r\n}\r\nspin_unlock(&edmac->lock);\r\nreturn ret;\r\n}\r\nstatic dma_cookie_t ep93xx_dma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct ep93xx_dma_chan *edmac = to_ep93xx_dma_chan(tx->chan);\r\nstruct ep93xx_dma_desc *desc;\r\ndma_cookie_t cookie;\r\nunsigned long flags;\r\nspin_lock_irqsave(&edmac->lock, flags);\r\ncookie = dma_cookie_assign(tx);\r\ndesc = container_of(tx, struct ep93xx_dma_desc, txd);\r\nif (list_empty(&edmac->active)) {\r\nep93xx_dma_set_active(edmac, desc);\r\nedmac->edma->hw_submit(edmac);\r\n} else {\r\nlist_add_tail(&desc->node, &edmac->queue);\r\n}\r\nspin_unlock_irqrestore(&edmac->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic int ep93xx_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct ep93xx_dma_chan *edmac = to_ep93xx_dma_chan(chan);\r\nstruct ep93xx_dma_data *data = chan->private;\r\nconst char *name = dma_chan_name(chan);\r\nint ret, i;\r\nif (!edmac->edma->m2m) {\r\nif (!data)\r\nreturn -EINVAL;\r\nif (data->port < EP93XX_DMA_I2S1 ||\r\ndata->port > EP93XX_DMA_IRDA)\r\nreturn -EINVAL;\r\nif (data->direction != ep93xx_dma_chan_direction(chan))\r\nreturn -EINVAL;\r\n} else {\r\nif (data) {\r\nswitch (data->port) {\r\ncase EP93XX_DMA_SSP:\r\ncase EP93XX_DMA_IDE:\r\nif (!is_slave_direction(data->direction))\r\nreturn -EINVAL;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\n}\r\nif (data && data->name)\r\nname = data->name;\r\nret = clk_enable(edmac->clk);\r\nif (ret)\r\nreturn ret;\r\nret = request_irq(edmac->irq, ep93xx_dma_interrupt, 0, name, edmac);\r\nif (ret)\r\ngoto fail_clk_disable;\r\nspin_lock_irq(&edmac->lock);\r\ndma_cookie_init(&edmac->chan);\r\nret = edmac->edma->hw_setup(edmac);\r\nspin_unlock_irq(&edmac->lock);\r\nif (ret)\r\ngoto fail_free_irq;\r\nfor (i = 0; i < DMA_MAX_CHAN_DESCRIPTORS; i++) {\r\nstruct ep93xx_dma_desc *desc;\r\ndesc = kzalloc(sizeof(*desc), GFP_KERNEL);\r\nif (!desc) {\r\ndev_warn(chan2dev(edmac), "not enough descriptors\n");\r\nbreak;\r\n}\r\nINIT_LIST_HEAD(&desc->tx_list);\r\ndma_async_tx_descriptor_init(&desc->txd, chan);\r\ndesc->txd.flags = DMA_CTRL_ACK;\r\ndesc->txd.tx_submit = ep93xx_dma_tx_submit;\r\nep93xx_dma_desc_put(edmac, desc);\r\n}\r\nreturn i;\r\nfail_free_irq:\r\nfree_irq(edmac->irq, edmac);\r\nfail_clk_disable:\r\nclk_disable(edmac->clk);\r\nreturn ret;\r\n}\r\nstatic void ep93xx_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct ep93xx_dma_chan *edmac = to_ep93xx_dma_chan(chan);\r\nstruct ep93xx_dma_desc *desc, *d;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\nBUG_ON(!list_empty(&edmac->active));\r\nBUG_ON(!list_empty(&edmac->queue));\r\nspin_lock_irqsave(&edmac->lock, flags);\r\nedmac->edma->hw_shutdown(edmac);\r\nedmac->runtime_addr = 0;\r\nedmac->runtime_ctrl = 0;\r\nedmac->buffer = 0;\r\nlist_splice_init(&edmac->free_list, &list);\r\nspin_unlock_irqrestore(&edmac->lock, flags);\r\nlist_for_each_entry_safe(desc, d, &list, node)\r\nkfree(desc);\r\nclk_disable(edmac->clk);\r\nfree_irq(edmac->irq, edmac);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nep93xx_dma_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest,\r\ndma_addr_t src, size_t len, unsigned long flags)\r\n{\r\nstruct ep93xx_dma_chan *edmac = to_ep93xx_dma_chan(chan);\r\nstruct ep93xx_dma_desc *desc, *first;\r\nsize_t bytes, offset;\r\nfirst = NULL;\r\nfor (offset = 0; offset < len; offset += bytes) {\r\ndesc = ep93xx_dma_desc_get(edmac);\r\nif (!desc) {\r\ndev_warn(chan2dev(edmac), "couln't get descriptor\n");\r\ngoto fail;\r\n}\r\nbytes = min_t(size_t, len - offset, DMA_MAX_CHAN_BYTES);\r\ndesc->src_addr = src + offset;\r\ndesc->dst_addr = dest + offset;\r\ndesc->size = bytes;\r\nif (!first)\r\nfirst = desc;\r\nelse\r\nlist_add_tail(&desc->node, &first->tx_list);\r\n}\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->txd.flags = flags;\r\nreturn &first->txd;\r\nfail:\r\nep93xx_dma_desc_put(edmac, first);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nep93xx_dma_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction dir,\r\nunsigned long flags, void *context)\r\n{\r\nstruct ep93xx_dma_chan *edmac = to_ep93xx_dma_chan(chan);\r\nstruct ep93xx_dma_desc *desc, *first;\r\nstruct scatterlist *sg;\r\nint i;\r\nif (!edmac->edma->m2m && dir != ep93xx_dma_chan_direction(chan)) {\r\ndev_warn(chan2dev(edmac),\r\n"channel was configured with different direction\n");\r\nreturn NULL;\r\n}\r\nif (test_bit(EP93XX_DMA_IS_CYCLIC, &edmac->flags)) {\r\ndev_warn(chan2dev(edmac),\r\n"channel is already used for cyclic transfers\n");\r\nreturn NULL;\r\n}\r\nfirst = NULL;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nsize_t sg_len = sg_dma_len(sg);\r\nif (sg_len > DMA_MAX_CHAN_BYTES) {\r\ndev_warn(chan2dev(edmac), "too big transfer size %d\n",\r\nsg_len);\r\ngoto fail;\r\n}\r\ndesc = ep93xx_dma_desc_get(edmac);\r\nif (!desc) {\r\ndev_warn(chan2dev(edmac), "couln't get descriptor\n");\r\ngoto fail;\r\n}\r\nif (dir == DMA_MEM_TO_DEV) {\r\ndesc->src_addr = sg_dma_address(sg);\r\ndesc->dst_addr = edmac->runtime_addr;\r\n} else {\r\ndesc->src_addr = edmac->runtime_addr;\r\ndesc->dst_addr = sg_dma_address(sg);\r\n}\r\ndesc->size = sg_len;\r\nif (!first)\r\nfirst = desc;\r\nelse\r\nlist_add_tail(&desc->node, &first->tx_list);\r\n}\r\nfirst->txd.cookie = -EBUSY;\r\nfirst->txd.flags = flags;\r\nreturn &first->txd;\r\nfail:\r\nep93xx_dma_desc_put(edmac, first);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nep93xx_dma_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t dma_addr,\r\nsize_t buf_len, size_t period_len,\r\nenum dma_transfer_direction dir, unsigned long flags)\r\n{\r\nstruct ep93xx_dma_chan *edmac = to_ep93xx_dma_chan(chan);\r\nstruct ep93xx_dma_desc *desc, *first;\r\nsize_t offset = 0;\r\nif (!edmac->edma->m2m && dir != ep93xx_dma_chan_direction(chan)) {\r\ndev_warn(chan2dev(edmac),\r\n"channel was configured with different direction\n");\r\nreturn NULL;\r\n}\r\nif (test_and_set_bit(EP93XX_DMA_IS_CYCLIC, &edmac->flags)) {\r\ndev_warn(chan2dev(edmac),\r\n"channel is already used for cyclic transfers\n");\r\nreturn NULL;\r\n}\r\nif (period_len > DMA_MAX_CHAN_BYTES) {\r\ndev_warn(chan2dev(edmac), "too big period length %d\n",\r\nperiod_len);\r\nreturn NULL;\r\n}\r\nfirst = NULL;\r\nfor (offset = 0; offset < buf_len; offset += period_len) {\r\ndesc = ep93xx_dma_desc_get(edmac);\r\nif (!desc) {\r\ndev_warn(chan2dev(edmac), "couln't get descriptor\n");\r\ngoto fail;\r\n}\r\nif (dir == DMA_MEM_TO_DEV) {\r\ndesc->src_addr = dma_addr + offset;\r\ndesc->dst_addr = edmac->runtime_addr;\r\n} else {\r\ndesc->src_addr = edmac->runtime_addr;\r\ndesc->dst_addr = dma_addr + offset;\r\n}\r\ndesc->size = period_len;\r\nif (!first)\r\nfirst = desc;\r\nelse\r\nlist_add_tail(&desc->node, &first->tx_list);\r\n}\r\nfirst->txd.cookie = -EBUSY;\r\nreturn &first->txd;\r\nfail:\r\nep93xx_dma_desc_put(edmac, first);\r\nreturn NULL;\r\n}\r\nstatic int ep93xx_dma_terminate_all(struct ep93xx_dma_chan *edmac)\r\n{\r\nstruct ep93xx_dma_desc *desc, *_d;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\nspin_lock_irqsave(&edmac->lock, flags);\r\nedmac->edma->hw_shutdown(edmac);\r\nclear_bit(EP93XX_DMA_IS_CYCLIC, &edmac->flags);\r\nlist_splice_init(&edmac->active, &list);\r\nlist_splice_init(&edmac->queue, &list);\r\nedmac->edma->hw_setup(edmac);\r\nspin_unlock_irqrestore(&edmac->lock, flags);\r\nlist_for_each_entry_safe(desc, _d, &list, node)\r\nep93xx_dma_desc_put(edmac, desc);\r\nreturn 0;\r\n}\r\nstatic int ep93xx_dma_slave_config(struct ep93xx_dma_chan *edmac,\r\nstruct dma_slave_config *config)\r\n{\r\nenum dma_slave_buswidth width;\r\nunsigned long flags;\r\nu32 addr, ctrl;\r\nif (!edmac->edma->m2m)\r\nreturn -EINVAL;\r\nswitch (config->direction) {\r\ncase DMA_DEV_TO_MEM:\r\nwidth = config->src_addr_width;\r\naddr = config->src_addr;\r\nbreak;\r\ncase DMA_MEM_TO_DEV:\r\nwidth = config->dst_addr_width;\r\naddr = config->dst_addr;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nswitch (width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nctrl = 0;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nctrl = M2M_CONTROL_PW_16;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nctrl = M2M_CONTROL_PW_32;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&edmac->lock, flags);\r\nedmac->runtime_addr = addr;\r\nedmac->runtime_ctrl = ctrl;\r\nspin_unlock_irqrestore(&edmac->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int ep93xx_dma_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct ep93xx_dma_chan *edmac = to_ep93xx_dma_chan(chan);\r\nstruct dma_slave_config *config;\r\nswitch (cmd) {\r\ncase DMA_TERMINATE_ALL:\r\nreturn ep93xx_dma_terminate_all(edmac);\r\ncase DMA_SLAVE_CONFIG:\r\nconfig = (struct dma_slave_config *)arg;\r\nreturn ep93xx_dma_slave_config(edmac, config);\r\ndefault:\r\nbreak;\r\n}\r\nreturn -ENOSYS;\r\n}\r\nstatic enum dma_status ep93xx_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *state)\r\n{\r\nreturn dma_cookie_status(chan, cookie, state);\r\n}\r\nstatic void ep93xx_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nep93xx_dma_advance_work(to_ep93xx_dma_chan(chan));\r\n}\r\nstatic int __init ep93xx_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct ep93xx_dma_platform_data *pdata = dev_get_platdata(&pdev->dev);\r\nstruct ep93xx_dma_engine *edma;\r\nstruct dma_device *dma_dev;\r\nsize_t edma_size;\r\nint ret, i;\r\nedma_size = pdata->num_channels * sizeof(struct ep93xx_dma_chan);\r\nedma = kzalloc(sizeof(*edma) + edma_size, GFP_KERNEL);\r\nif (!edma)\r\nreturn -ENOMEM;\r\ndma_dev = &edma->dma_dev;\r\nedma->m2m = platform_get_device_id(pdev)->driver_data;\r\nedma->num_channels = pdata->num_channels;\r\nINIT_LIST_HEAD(&dma_dev->channels);\r\nfor (i = 0; i < pdata->num_channels; i++) {\r\nconst struct ep93xx_dma_chan_data *cdata = &pdata->channels[i];\r\nstruct ep93xx_dma_chan *edmac = &edma->channels[i];\r\nedmac->chan.device = dma_dev;\r\nedmac->regs = cdata->base;\r\nedmac->irq = cdata->irq;\r\nedmac->edma = edma;\r\nedmac->clk = clk_get(NULL, cdata->name);\r\nif (IS_ERR(edmac->clk)) {\r\ndev_warn(&pdev->dev, "failed to get clock for %s\n",\r\ncdata->name);\r\ncontinue;\r\n}\r\nspin_lock_init(&edmac->lock);\r\nINIT_LIST_HEAD(&edmac->active);\r\nINIT_LIST_HEAD(&edmac->queue);\r\nINIT_LIST_HEAD(&edmac->free_list);\r\ntasklet_init(&edmac->tasklet, ep93xx_dma_tasklet,\r\n(unsigned long)edmac);\r\nlist_add_tail(&edmac->chan.device_node,\r\n&dma_dev->channels);\r\n}\r\ndma_cap_zero(dma_dev->cap_mask);\r\ndma_cap_set(DMA_SLAVE, dma_dev->cap_mask);\r\ndma_cap_set(DMA_CYCLIC, dma_dev->cap_mask);\r\ndma_dev->dev = &pdev->dev;\r\ndma_dev->device_alloc_chan_resources = ep93xx_dma_alloc_chan_resources;\r\ndma_dev->device_free_chan_resources = ep93xx_dma_free_chan_resources;\r\ndma_dev->device_prep_slave_sg = ep93xx_dma_prep_slave_sg;\r\ndma_dev->device_prep_dma_cyclic = ep93xx_dma_prep_dma_cyclic;\r\ndma_dev->device_control = ep93xx_dma_control;\r\ndma_dev->device_issue_pending = ep93xx_dma_issue_pending;\r\ndma_dev->device_tx_status = ep93xx_dma_tx_status;\r\ndma_set_max_seg_size(dma_dev->dev, DMA_MAX_CHAN_BYTES);\r\nif (edma->m2m) {\r\ndma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);\r\ndma_dev->device_prep_dma_memcpy = ep93xx_dma_prep_dma_memcpy;\r\nedma->hw_setup = m2m_hw_setup;\r\nedma->hw_shutdown = m2m_hw_shutdown;\r\nedma->hw_submit = m2m_hw_submit;\r\nedma->hw_interrupt = m2m_hw_interrupt;\r\n} else {\r\ndma_cap_set(DMA_PRIVATE, dma_dev->cap_mask);\r\nedma->hw_setup = m2p_hw_setup;\r\nedma->hw_shutdown = m2p_hw_shutdown;\r\nedma->hw_submit = m2p_hw_submit;\r\nedma->hw_interrupt = m2p_hw_interrupt;\r\n}\r\nret = dma_async_device_register(dma_dev);\r\nif (unlikely(ret)) {\r\nfor (i = 0; i < edma->num_channels; i++) {\r\nstruct ep93xx_dma_chan *edmac = &edma->channels[i];\r\nif (!IS_ERR_OR_NULL(edmac->clk))\r\nclk_put(edmac->clk);\r\n}\r\nkfree(edma);\r\n} else {\r\ndev_info(dma_dev->dev, "EP93xx M2%s DMA ready\n",\r\nedma->m2m ? "M" : "P");\r\n}\r\nreturn ret;\r\n}\r\nstatic int __init ep93xx_dma_module_init(void)\r\n{\r\nreturn platform_driver_probe(&ep93xx_dma_driver, ep93xx_dma_probe);\r\n}
