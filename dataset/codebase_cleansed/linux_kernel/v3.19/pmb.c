static __always_inline unsigned long mk_pmb_entry(unsigned int entry)\r\n{\r\nreturn (entry & PMB_E_MASK) << PMB_E_SHIFT;\r\n}\r\nstatic __always_inline unsigned long mk_pmb_addr(unsigned int entry)\r\n{\r\nreturn mk_pmb_entry(entry) | PMB_ADDR;\r\n}\r\nstatic __always_inline unsigned long mk_pmb_data(unsigned int entry)\r\n{\r\nreturn mk_pmb_entry(entry) | PMB_DATA;\r\n}\r\nstatic __always_inline unsigned int pmb_ppn_in_range(unsigned long ppn)\r\n{\r\nreturn ppn >= __pa(memory_start) && ppn < __pa(memory_end);\r\n}\r\nstatic __always_inline unsigned long pmb_cache_flags(void)\r\n{\r\nunsigned long flags = 0;\r\n#if defined(CONFIG_CACHE_OFF)\r\nflags |= PMB_WT | PMB_UB;\r\n#elif defined(CONFIG_CACHE_WRITETHROUGH)\r\nflags |= PMB_C | PMB_WT | PMB_UB;\r\n#elif defined(CONFIG_CACHE_WRITEBACK)\r\nflags |= PMB_C;\r\n#endif\r\nreturn flags;\r\n}\r\nstatic inline unsigned long pgprot_to_pmb_flags(pgprot_t prot)\r\n{\r\nunsigned long pmb_flags = 0;\r\nu64 flags = pgprot_val(prot);\r\nif (flags & _PAGE_CACHABLE)\r\npmb_flags |= PMB_C;\r\nif (flags & _PAGE_WT)\r\npmb_flags |= PMB_WT | PMB_UB;\r\nreturn pmb_flags;\r\n}\r\nstatic inline bool pmb_can_merge(struct pmb_entry *a, struct pmb_entry *b)\r\n{\r\nreturn (b->vpn == (a->vpn + a->size)) &&\r\n(b->ppn == (a->ppn + a->size)) &&\r\n(b->flags == a->flags);\r\n}\r\nstatic bool pmb_mapping_exists(unsigned long vaddr, phys_addr_t phys,\r\nunsigned long size)\r\n{\r\nint i;\r\nread_lock(&pmb_rwlock);\r\nfor (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {\r\nstruct pmb_entry *pmbe, *iter;\r\nunsigned long span;\r\nif (!test_bit(i, pmb_map))\r\ncontinue;\r\npmbe = &pmb_entry_list[i];\r\nif ((vaddr < pmbe->vpn) || (vaddr >= (pmbe->vpn + pmbe->size)))\r\ncontinue;\r\nif ((phys < pmbe->ppn) || (phys >= (pmbe->ppn + pmbe->size)))\r\ncontinue;\r\nif (size <= pmbe->size) {\r\nread_unlock(&pmb_rwlock);\r\nreturn true;\r\n}\r\nspan = pmbe->size;\r\nfor (iter = pmbe->link; iter; iter = iter->link)\r\nspan += iter->size;\r\nif (size <= span) {\r\nread_unlock(&pmb_rwlock);\r\nreturn true;\r\n}\r\n}\r\nread_unlock(&pmb_rwlock);\r\nreturn false;\r\n}\r\nstatic bool pmb_size_valid(unsigned long size)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)\r\nif (pmb_sizes[i].size == size)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline bool pmb_addr_valid(unsigned long addr, unsigned long size)\r\n{\r\nreturn (addr >= P1SEG && (addr + size - 1) < P3SEG);\r\n}\r\nstatic inline bool pmb_prot_valid(pgprot_t prot)\r\n{\r\nreturn (pgprot_val(prot) & _PAGE_USER) == 0;\r\n}\r\nstatic int pmb_size_to_flags(unsigned long size)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)\r\nif (pmb_sizes[i].size == size)\r\nreturn pmb_sizes[i].flag;\r\nreturn 0;\r\n}\r\nstatic int pmb_alloc_entry(void)\r\n{\r\nint pos;\r\npos = find_first_zero_bit(pmb_map, NR_PMB_ENTRIES);\r\nif (pos >= 0 && pos < NR_PMB_ENTRIES)\r\n__set_bit(pos, pmb_map);\r\nelse\r\npos = -ENOSPC;\r\nreturn pos;\r\n}\r\nstatic struct pmb_entry *pmb_alloc(unsigned long vpn, unsigned long ppn,\r\nunsigned long flags, int entry)\r\n{\r\nstruct pmb_entry *pmbe;\r\nunsigned long irqflags;\r\nvoid *ret = NULL;\r\nint pos;\r\nwrite_lock_irqsave(&pmb_rwlock, irqflags);\r\nif (entry == PMB_NO_ENTRY) {\r\npos = pmb_alloc_entry();\r\nif (unlikely(pos < 0)) {\r\nret = ERR_PTR(pos);\r\ngoto out;\r\n}\r\n} else {\r\nif (__test_and_set_bit(entry, pmb_map)) {\r\nret = ERR_PTR(-ENOSPC);\r\ngoto out;\r\n}\r\npos = entry;\r\n}\r\nwrite_unlock_irqrestore(&pmb_rwlock, irqflags);\r\npmbe = &pmb_entry_list[pos];\r\nmemset(pmbe, 0, sizeof(struct pmb_entry));\r\nraw_spin_lock_init(&pmbe->lock);\r\npmbe->vpn = vpn;\r\npmbe->ppn = ppn;\r\npmbe->flags = flags;\r\npmbe->entry = pos;\r\nreturn pmbe;\r\nout:\r\nwrite_unlock_irqrestore(&pmb_rwlock, irqflags);\r\nreturn ret;\r\n}\r\nstatic void pmb_free(struct pmb_entry *pmbe)\r\n{\r\n__clear_bit(pmbe->entry, pmb_map);\r\npmbe->entry = PMB_NO_ENTRY;\r\npmbe->link = NULL;\r\n}\r\nstatic void __set_pmb_entry(struct pmb_entry *pmbe)\r\n{\r\nunsigned long addr, data;\r\naddr = mk_pmb_addr(pmbe->entry);\r\ndata = mk_pmb_data(pmbe->entry);\r\njump_to_uncached();\r\n__raw_writel(pmbe->vpn | PMB_V, addr);\r\n__raw_writel(pmbe->ppn | pmbe->flags | PMB_V, data);\r\nback_to_cached();\r\n}\r\nstatic void __clear_pmb_entry(struct pmb_entry *pmbe)\r\n{\r\nunsigned long addr, data;\r\nunsigned long addr_val, data_val;\r\naddr = mk_pmb_addr(pmbe->entry);\r\ndata = mk_pmb_data(pmbe->entry);\r\naddr_val = __raw_readl(addr);\r\ndata_val = __raw_readl(data);\r\nwritel_uncached(addr_val & ~PMB_V, addr);\r\nwritel_uncached(data_val & ~PMB_V, data);\r\n}\r\nstatic void set_pmb_entry(struct pmb_entry *pmbe)\r\n{\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&pmbe->lock, flags);\r\n__set_pmb_entry(pmbe);\r\nraw_spin_unlock_irqrestore(&pmbe->lock, flags);\r\n}\r\nint pmb_bolt_mapping(unsigned long vaddr, phys_addr_t phys,\r\nunsigned long size, pgprot_t prot)\r\n{\r\nstruct pmb_entry *pmbp, *pmbe;\r\nunsigned long orig_addr, orig_size;\r\nunsigned long flags, pmb_flags;\r\nint i, mapped;\r\nif (size < SZ_16M)\r\nreturn -EINVAL;\r\nif (!pmb_addr_valid(vaddr, size))\r\nreturn -EFAULT;\r\nif (pmb_mapping_exists(vaddr, phys, size))\r\nreturn 0;\r\norig_addr = vaddr;\r\norig_size = size;\r\nflush_tlb_kernel_range(vaddr, vaddr + size);\r\npmb_flags = pgprot_to_pmb_flags(prot);\r\npmbp = NULL;\r\ndo {\r\nfor (i = mapped = 0; i < ARRAY_SIZE(pmb_sizes); i++) {\r\nif (size < pmb_sizes[i].size)\r\ncontinue;\r\npmbe = pmb_alloc(vaddr, phys, pmb_flags |\r\npmb_sizes[i].flag, PMB_NO_ENTRY);\r\nif (IS_ERR(pmbe)) {\r\npmb_unmap_entry(pmbp, mapped);\r\nreturn PTR_ERR(pmbe);\r\n}\r\nraw_spin_lock_irqsave(&pmbe->lock, flags);\r\npmbe->size = pmb_sizes[i].size;\r\n__set_pmb_entry(pmbe);\r\nphys += pmbe->size;\r\nvaddr += pmbe->size;\r\nsize -= pmbe->size;\r\nif (likely(pmbp)) {\r\nraw_spin_lock_nested(&pmbp->lock,\r\nSINGLE_DEPTH_NESTING);\r\npmbp->link = pmbe;\r\nraw_spin_unlock(&pmbp->lock);\r\n}\r\npmbp = pmbe;\r\ni--;\r\nmapped++;\r\nraw_spin_unlock_irqrestore(&pmbe->lock, flags);\r\n}\r\n} while (size >= SZ_16M);\r\nflush_cache_vmap(orig_addr, orig_addr + orig_size);\r\nreturn 0;\r\n}\r\nvoid __iomem *pmb_remap_caller(phys_addr_t phys, unsigned long size,\r\npgprot_t prot, void *caller)\r\n{\r\nunsigned long vaddr;\r\nphys_addr_t offset, last_addr;\r\nphys_addr_t align_mask;\r\nunsigned long aligned;\r\nstruct vm_struct *area;\r\nint i, ret;\r\nif (!pmb_iomapping_enabled)\r\nreturn NULL;\r\nif (size < SZ_16M)\r\nreturn ERR_PTR(-EINVAL);\r\nif (!pmb_prot_valid(prot))\r\nreturn ERR_PTR(-EINVAL);\r\nfor (i = 0; i < ARRAY_SIZE(pmb_sizes); i++)\r\nif (size >= pmb_sizes[i].size)\r\nbreak;\r\nlast_addr = phys + size;\r\nalign_mask = ~(pmb_sizes[i].size - 1);\r\noffset = phys & ~align_mask;\r\nphys &= align_mask;\r\naligned = ALIGN(last_addr, pmb_sizes[i].size) - phys;\r\narea = __get_vm_area_caller(aligned, VM_IOREMAP, 0xb0000000,\r\nP3SEG, caller);\r\nif (!area)\r\nreturn NULL;\r\narea->phys_addr = phys;\r\nvaddr = (unsigned long)area->addr;\r\nret = pmb_bolt_mapping(vaddr, phys, size, prot);\r\nif (unlikely(ret != 0))\r\nreturn ERR_PTR(ret);\r\nreturn (void __iomem *)(offset + (char *)vaddr);\r\n}\r\nint pmb_unmap(void __iomem *addr)\r\n{\r\nstruct pmb_entry *pmbe = NULL;\r\nunsigned long vaddr = (unsigned long __force)addr;\r\nint i, found = 0;\r\nread_lock(&pmb_rwlock);\r\nfor (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {\r\nif (test_bit(i, pmb_map)) {\r\npmbe = &pmb_entry_list[i];\r\nif (pmbe->vpn == vaddr) {\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\n}\r\nread_unlock(&pmb_rwlock);\r\nif (found) {\r\npmb_unmap_entry(pmbe, NR_PMB_ENTRIES);\r\nreturn 0;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic void __pmb_unmap_entry(struct pmb_entry *pmbe, int depth)\r\n{\r\ndo {\r\nstruct pmb_entry *pmblink = pmbe;\r\n__clear_pmb_entry(pmbe);\r\nflush_cache_vunmap(pmbe->vpn, pmbe->vpn + pmbe->size);\r\npmbe = pmblink->link;\r\npmb_free(pmblink);\r\n} while (pmbe && --depth);\r\n}\r\nstatic void pmb_unmap_entry(struct pmb_entry *pmbe, int depth)\r\n{\r\nunsigned long flags;\r\nif (unlikely(!pmbe))\r\nreturn;\r\nwrite_lock_irqsave(&pmb_rwlock, flags);\r\n__pmb_unmap_entry(pmbe, depth);\r\nwrite_unlock_irqrestore(&pmb_rwlock, flags);\r\n}\r\nstatic void __init pmb_notify(void)\r\n{\r\nint i;\r\npr_info("PMB: boot mappings:\n");\r\nread_lock(&pmb_rwlock);\r\nfor (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {\r\nstruct pmb_entry *pmbe;\r\nif (!test_bit(i, pmb_map))\r\ncontinue;\r\npmbe = &pmb_entry_list[i];\r\npr_info(" 0x%08lx -> 0x%08lx [ %4ldMB %2scached ]\n",\r\npmbe->vpn >> PAGE_SHIFT, pmbe->ppn >> PAGE_SHIFT,\r\npmbe->size >> 20, (pmbe->flags & PMB_C) ? "" : "un");\r\n}\r\nread_unlock(&pmb_rwlock);\r\n}\r\nstatic void __init pmb_synchronize(void)\r\n{\r\nstruct pmb_entry *pmbp = NULL;\r\nint i, j;\r\nfor (i = 0; i < NR_PMB_ENTRIES; i++) {\r\nunsigned long addr, data;\r\nunsigned long addr_val, data_val;\r\nunsigned long ppn, vpn, flags;\r\nunsigned long irqflags;\r\nunsigned int size;\r\nstruct pmb_entry *pmbe;\r\naddr = mk_pmb_addr(i);\r\ndata = mk_pmb_data(i);\r\naddr_val = __raw_readl(addr);\r\ndata_val = __raw_readl(data);\r\nif (!(data_val & PMB_V) || !(addr_val & PMB_V))\r\ncontinue;\r\nppn = data_val & PMB_PFN_MASK;\r\nvpn = addr_val & PMB_PFN_MASK;\r\nif (!pmb_ppn_in_range(ppn)) {\r\nwritel_uncached(addr_val & ~PMB_V, addr);\r\nwritel_uncached(data_val & ~PMB_V, data);\r\ncontinue;\r\n}\r\nif (data_val & PMB_C) {\r\ndata_val &= ~PMB_CACHE_MASK;\r\ndata_val |= pmb_cache_flags();\r\nwritel_uncached(data_val, data);\r\n}\r\nsize = data_val & PMB_SZ_MASK;\r\nflags = size | (data_val & PMB_CACHE_MASK);\r\npmbe = pmb_alloc(vpn, ppn, flags, i);\r\nif (IS_ERR(pmbe)) {\r\nWARN_ON_ONCE(1);\r\ncontinue;\r\n}\r\nraw_spin_lock_irqsave(&pmbe->lock, irqflags);\r\nfor (j = 0; j < ARRAY_SIZE(pmb_sizes); j++)\r\nif (pmb_sizes[j].flag == size)\r\npmbe->size = pmb_sizes[j].size;\r\nif (pmbp) {\r\nraw_spin_lock_nested(&pmbp->lock, SINGLE_DEPTH_NESTING);\r\nif (pmb_can_merge(pmbp, pmbe))\r\npmbp->link = pmbe;\r\nraw_spin_unlock(&pmbp->lock);\r\n}\r\npmbp = pmbe;\r\nraw_spin_unlock_irqrestore(&pmbe->lock, irqflags);\r\n}\r\n}\r\nstatic void __init pmb_merge(struct pmb_entry *head)\r\n{\r\nunsigned long span, newsize;\r\nstruct pmb_entry *tail;\r\nint i = 1, depth = 0;\r\nspan = newsize = head->size;\r\ntail = head->link;\r\nwhile (tail) {\r\nspan += tail->size;\r\nif (pmb_size_valid(span)) {\r\nnewsize = span;\r\ndepth = i;\r\n}\r\nif (!tail->link)\r\nbreak;\r\ntail = tail->link;\r\ni++;\r\n}\r\nif (!depth || !pmb_size_valid(newsize))\r\nreturn;\r\nhead->flags &= ~PMB_SZ_MASK;\r\nhead->flags |= pmb_size_to_flags(newsize);\r\nhead->size = newsize;\r\n__pmb_unmap_entry(head->link, depth);\r\n__set_pmb_entry(head);\r\n}\r\nstatic void __init pmb_coalesce(void)\r\n{\r\nunsigned long flags;\r\nint i;\r\nwrite_lock_irqsave(&pmb_rwlock, flags);\r\nfor (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {\r\nstruct pmb_entry *pmbe;\r\nif (!test_bit(i, pmb_map))\r\ncontinue;\r\npmbe = &pmb_entry_list[i];\r\nif (!pmbe->link)\r\ncontinue;\r\nif (pmbe->size == SZ_512M)\r\ncontinue;\r\npmb_merge(pmbe);\r\n}\r\nwrite_unlock_irqrestore(&pmb_rwlock, flags);\r\n}\r\nstatic void __init pmb_resize(void)\r\n{\r\nint i;\r\nif (uncached_size == SZ_16M)\r\nreturn;\r\nread_lock(&pmb_rwlock);\r\nfor (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {\r\nstruct pmb_entry *pmbe;\r\nunsigned long flags;\r\nif (!test_bit(i, pmb_map))\r\ncontinue;\r\npmbe = &pmb_entry_list[i];\r\nif (pmbe->vpn != uncached_start)\r\ncontinue;\r\nraw_spin_lock_irqsave(&pmbe->lock, flags);\r\npmbe->size = SZ_16M;\r\npmbe->flags &= ~PMB_SZ_MASK;\r\npmbe->flags |= pmb_size_to_flags(pmbe->size);\r\nuncached_resize(pmbe->size);\r\n__set_pmb_entry(pmbe);\r\nraw_spin_unlock_irqrestore(&pmbe->lock, flags);\r\n}\r\nread_unlock(&pmb_rwlock);\r\n}\r\nstatic int __init early_pmb(char *p)\r\n{\r\nif (!p)\r\nreturn 0;\r\nif (strstr(p, "iomap"))\r\npmb_iomapping_enabled = 1;\r\nreturn 0;\r\n}\r\nvoid __init pmb_init(void)\r\n{\r\npmb_synchronize();\r\npmb_coalesce();\r\n#ifdef CONFIG_UNCACHED_MAPPING\r\npmb_resize();\r\n#endif\r\npmb_notify();\r\nwritel_uncached(0, PMB_IRMCR);\r\nlocal_flush_tlb_all();\r\nctrl_barrier();\r\n}\r\nbool __in_29bit_mode(void)\r\n{\r\nreturn (__raw_readl(PMB_PASCR) & PASCR_SE) == 0;\r\n}\r\nstatic int pmb_seq_show(struct seq_file *file, void *iter)\r\n{\r\nint i;\r\nseq_printf(file, "V: Valid, C: Cacheable, WT: Write-Through\n"\r\n"CB: Copy-Back, B: Buffered, UB: Unbuffered\n");\r\nseq_printf(file, "ety vpn ppn size flags\n");\r\nfor (i = 0; i < NR_PMB_ENTRIES; i++) {\r\nunsigned long addr, data;\r\nunsigned int size;\r\nchar *sz_str = NULL;\r\naddr = __raw_readl(mk_pmb_addr(i));\r\ndata = __raw_readl(mk_pmb_data(i));\r\nsize = data & PMB_SZ_MASK;\r\nsz_str = (size == PMB_SZ_16M) ? " 16MB":\r\n(size == PMB_SZ_64M) ? " 64MB":\r\n(size == PMB_SZ_128M) ? "128MB":\r\n"512MB";\r\nseq_printf(file, "%02d: %c 0x%02lx 0x%02lx %s %c %s %s\n",\r\ni, ((addr & PMB_V) && (data & PMB_V)) ? 'V' : ' ',\r\n(addr >> 24) & 0xff, (data >> 24) & 0xff,\r\nsz_str, (data & PMB_C) ? 'C' : ' ',\r\n(data & PMB_WT) ? "WT" : "CB",\r\n(data & PMB_UB) ? "UB" : " B");\r\n}\r\nreturn 0;\r\n}\r\nstatic int pmb_debugfs_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, pmb_seq_show, NULL);\r\n}\r\nstatic int __init pmb_debugfs_init(void)\r\n{\r\nstruct dentry *dentry;\r\ndentry = debugfs_create_file("pmb", S_IFREG | S_IRUGO,\r\narch_debugfs_dir, NULL, &pmb_debugfs_fops);\r\nif (!dentry)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void pmb_syscore_resume(void)\r\n{\r\nstruct pmb_entry *pmbe;\r\nint i;\r\nread_lock(&pmb_rwlock);\r\nfor (i = 0; i < ARRAY_SIZE(pmb_entry_list); i++) {\r\nif (test_bit(i, pmb_map)) {\r\npmbe = &pmb_entry_list[i];\r\nset_pmb_entry(pmbe);\r\n}\r\n}\r\nread_unlock(&pmb_rwlock);\r\n}\r\nstatic int __init pmb_sysdev_init(void)\r\n{\r\nregister_syscore_ops(&pmb_syscore_ops);\r\nreturn 0;\r\n}
