struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *xprt)\r\n{\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nwhile (1) {\r\nctxt = kmem_cache_alloc(svc_rdma_ctxt_cachep, GFP_KERNEL);\r\nif (ctxt)\r\nbreak;\r\nschedule_timeout_uninterruptible(msecs_to_jiffies(500));\r\n}\r\nctxt->xprt = xprt;\r\nINIT_LIST_HEAD(&ctxt->dto_q);\r\nctxt->count = 0;\r\nctxt->frmr = NULL;\r\natomic_inc(&xprt->sc_ctxt_used);\r\nreturn ctxt;\r\n}\r\nvoid svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt)\r\n{\r\nstruct svcxprt_rdma *xprt = ctxt->xprt;\r\nint i;\r\nfor (i = 0; i < ctxt->count && ctxt->sge[i].length; i++) {\r\nif (ctxt->sge[i].lkey == xprt->sc_dma_lkey) {\r\natomic_dec(&xprt->sc_dma_used);\r\nib_dma_unmap_page(xprt->sc_cm_id->device,\r\nctxt->sge[i].addr,\r\nctxt->sge[i].length,\r\nctxt->direction);\r\n}\r\n}\r\n}\r\nvoid svc_rdma_put_context(struct svc_rdma_op_ctxt *ctxt, int free_pages)\r\n{\r\nstruct svcxprt_rdma *xprt;\r\nint i;\r\nBUG_ON(!ctxt);\r\nxprt = ctxt->xprt;\r\nif (free_pages)\r\nfor (i = 0; i < ctxt->count; i++)\r\nput_page(ctxt->pages[i]);\r\nkmem_cache_free(svc_rdma_ctxt_cachep, ctxt);\r\natomic_dec(&xprt->sc_ctxt_used);\r\n}\r\nstruct svc_rdma_req_map *svc_rdma_get_req_map(void)\r\n{\r\nstruct svc_rdma_req_map *map;\r\nwhile (1) {\r\nmap = kmem_cache_alloc(svc_rdma_map_cachep, GFP_KERNEL);\r\nif (map)\r\nbreak;\r\nschedule_timeout_uninterruptible(msecs_to_jiffies(500));\r\n}\r\nmap->count = 0;\r\nreturn map;\r\n}\r\nvoid svc_rdma_put_req_map(struct svc_rdma_req_map *map)\r\n{\r\nkmem_cache_free(svc_rdma_map_cachep, map);\r\n}\r\nstatic void cq_event_handler(struct ib_event *event, void *context)\r\n{\r\nstruct svc_xprt *xprt = context;\r\ndprintk("svcrdma: received CQ event id=%d, context=%p\n",\r\nevent->event, context);\r\nset_bit(XPT_CLOSE, &xprt->xpt_flags);\r\n}\r\nstatic void qp_event_handler(struct ib_event *event, void *context)\r\n{\r\nstruct svc_xprt *xprt = context;\r\nswitch (event->event) {\r\ncase IB_EVENT_PATH_MIG:\r\ncase IB_EVENT_COMM_EST:\r\ncase IB_EVENT_SQ_DRAINED:\r\ncase IB_EVENT_QP_LAST_WQE_REACHED:\r\ndprintk("svcrdma: QP event %d received for QP=%p\n",\r\nevent->event, event->element.qp);\r\nbreak;\r\ncase IB_EVENT_PATH_MIG_ERR:\r\ncase IB_EVENT_QP_FATAL:\r\ncase IB_EVENT_QP_REQ_ERR:\r\ncase IB_EVENT_QP_ACCESS_ERR:\r\ncase IB_EVENT_DEVICE_FATAL:\r\ndefault:\r\ndprintk("svcrdma: QP ERROR event %d received for QP=%p, "\r\n"closing transport\n",\r\nevent->event, event->element.qp);\r\nset_bit(XPT_CLOSE, &xprt->xpt_flags);\r\nbreak;\r\n}\r\n}\r\nstatic void dto_tasklet_func(unsigned long data)\r\n{\r\nstruct svcxprt_rdma *xprt;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dto_lock, flags);\r\nwhile (!list_empty(&dto_xprt_q)) {\r\nxprt = list_entry(dto_xprt_q.next,\r\nstruct svcxprt_rdma, sc_dto_q);\r\nlist_del_init(&xprt->sc_dto_q);\r\nspin_unlock_irqrestore(&dto_lock, flags);\r\nrq_cq_reap(xprt);\r\nsq_cq_reap(xprt);\r\nsvc_xprt_put(&xprt->sc_xprt);\r\nspin_lock_irqsave(&dto_lock, flags);\r\n}\r\nspin_unlock_irqrestore(&dto_lock, flags);\r\n}\r\nstatic void rq_comp_handler(struct ib_cq *cq, void *cq_context)\r\n{\r\nstruct svcxprt_rdma *xprt = cq_context;\r\nunsigned long flags;\r\nif (atomic_read(&xprt->sc_xprt.xpt_ref.refcount)==0)\r\nreturn;\r\nset_bit(RDMAXPRT_RQ_PENDING, &xprt->sc_flags);\r\nspin_lock_irqsave(&dto_lock, flags);\r\nif (list_empty(&xprt->sc_dto_q)) {\r\nsvc_xprt_get(&xprt->sc_xprt);\r\nlist_add_tail(&xprt->sc_dto_q, &dto_xprt_q);\r\n}\r\nspin_unlock_irqrestore(&dto_lock, flags);\r\ntasklet_schedule(&dto_tasklet);\r\n}\r\nstatic void rq_cq_reap(struct svcxprt_rdma *xprt)\r\n{\r\nint ret;\r\nstruct ib_wc wc;\r\nstruct svc_rdma_op_ctxt *ctxt = NULL;\r\nif (!test_and_clear_bit(RDMAXPRT_RQ_PENDING, &xprt->sc_flags))\r\nreturn;\r\nib_req_notify_cq(xprt->sc_rq_cq, IB_CQ_NEXT_COMP);\r\natomic_inc(&rdma_stat_rq_poll);\r\nwhile ((ret = ib_poll_cq(xprt->sc_rq_cq, 1, &wc)) > 0) {\r\nctxt = (struct svc_rdma_op_ctxt *)(unsigned long)wc.wr_id;\r\nctxt->wc_status = wc.status;\r\nctxt->byte_len = wc.byte_len;\r\nsvc_rdma_unmap_dma(ctxt);\r\nif (wc.status != IB_WC_SUCCESS) {\r\ndprintk("svcrdma: transport closing putting ctxt %p\n", ctxt);\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\nsvc_rdma_put_context(ctxt, 1);\r\nsvc_xprt_put(&xprt->sc_xprt);\r\ncontinue;\r\n}\r\nspin_lock_bh(&xprt->sc_rq_dto_lock);\r\nlist_add_tail(&ctxt->dto_q, &xprt->sc_rq_dto_q);\r\nspin_unlock_bh(&xprt->sc_rq_dto_lock);\r\nsvc_xprt_put(&xprt->sc_xprt);\r\n}\r\nif (ctxt)\r\natomic_inc(&rdma_stat_rq_prod);\r\nset_bit(XPT_DATA, &xprt->sc_xprt.xpt_flags);\r\nif (!test_bit(RDMAXPRT_CONN_PENDING, &xprt->sc_flags))\r\nsvc_xprt_enqueue(&xprt->sc_xprt);\r\n}\r\nstatic void process_context(struct svcxprt_rdma *xprt,\r\nstruct svc_rdma_op_ctxt *ctxt)\r\n{\r\nsvc_rdma_unmap_dma(ctxt);\r\nswitch (ctxt->wr_op) {\r\ncase IB_WR_SEND:\r\nBUG_ON(ctxt->frmr);\r\nsvc_rdma_put_context(ctxt, 1);\r\nbreak;\r\ncase IB_WR_RDMA_WRITE:\r\nBUG_ON(ctxt->frmr);\r\nsvc_rdma_put_context(ctxt, 0);\r\nbreak;\r\ncase IB_WR_RDMA_READ:\r\ncase IB_WR_RDMA_READ_WITH_INV:\r\nsvc_rdma_put_frmr(xprt, ctxt->frmr);\r\nif (test_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags)) {\r\nstruct svc_rdma_op_ctxt *read_hdr = ctxt->read_hdr;\r\nBUG_ON(!read_hdr);\r\nspin_lock_bh(&xprt->sc_rq_dto_lock);\r\nset_bit(XPT_DATA, &xprt->sc_xprt.xpt_flags);\r\nlist_add_tail(&read_hdr->dto_q,\r\n&xprt->sc_read_complete_q);\r\nspin_unlock_bh(&xprt->sc_rq_dto_lock);\r\nsvc_xprt_enqueue(&xprt->sc_xprt);\r\n}\r\nsvc_rdma_put_context(ctxt, 0);\r\nbreak;\r\ndefault:\r\nBUG_ON(1);\r\nprintk(KERN_ERR "svcrdma: unexpected completion type, "\r\n"opcode=%d\n",\r\nctxt->wr_op);\r\nbreak;\r\n}\r\n}\r\nstatic void sq_cq_reap(struct svcxprt_rdma *xprt)\r\n{\r\nstruct svc_rdma_op_ctxt *ctxt = NULL;\r\nstruct ib_wc wc_a[6];\r\nstruct ib_wc *wc;\r\nstruct ib_cq *cq = xprt->sc_sq_cq;\r\nint ret;\r\nmemset(wc_a, 0, sizeof(wc_a));\r\nif (!test_and_clear_bit(RDMAXPRT_SQ_PENDING, &xprt->sc_flags))\r\nreturn;\r\nib_req_notify_cq(xprt->sc_sq_cq, IB_CQ_NEXT_COMP);\r\natomic_inc(&rdma_stat_sq_poll);\r\nwhile ((ret = ib_poll_cq(cq, ARRAY_SIZE(wc_a), wc_a)) > 0) {\r\nint i;\r\nfor (i = 0; i < ret; i++) {\r\nwc = &wc_a[i];\r\nif (wc->status != IB_WC_SUCCESS) {\r\ndprintk("svcrdma: sq wc err status %d\n",\r\nwc->status);\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\n}\r\natomic_dec(&xprt->sc_sq_count);\r\nwake_up(&xprt->sc_send_wait);\r\nctxt = (struct svc_rdma_op_ctxt *)\r\n(unsigned long)wc->wr_id;\r\nif (ctxt)\r\nprocess_context(xprt, ctxt);\r\nsvc_xprt_put(&xprt->sc_xprt);\r\n}\r\n}\r\nif (ctxt)\r\natomic_inc(&rdma_stat_sq_prod);\r\n}\r\nstatic void sq_comp_handler(struct ib_cq *cq, void *cq_context)\r\n{\r\nstruct svcxprt_rdma *xprt = cq_context;\r\nunsigned long flags;\r\nif (atomic_read(&xprt->sc_xprt.xpt_ref.refcount)==0)\r\nreturn;\r\nset_bit(RDMAXPRT_SQ_PENDING, &xprt->sc_flags);\r\nspin_lock_irqsave(&dto_lock, flags);\r\nif (list_empty(&xprt->sc_dto_q)) {\r\nsvc_xprt_get(&xprt->sc_xprt);\r\nlist_add_tail(&xprt->sc_dto_q, &dto_xprt_q);\r\n}\r\nspin_unlock_irqrestore(&dto_lock, flags);\r\ntasklet_schedule(&dto_tasklet);\r\n}\r\nstatic struct svcxprt_rdma *rdma_create_xprt(struct svc_serv *serv,\r\nint listener)\r\n{\r\nstruct svcxprt_rdma *cma_xprt = kzalloc(sizeof *cma_xprt, GFP_KERNEL);\r\nif (!cma_xprt)\r\nreturn NULL;\r\nsvc_xprt_init(&init_net, &svc_rdma_class, &cma_xprt->sc_xprt, serv);\r\nINIT_LIST_HEAD(&cma_xprt->sc_accept_q);\r\nINIT_LIST_HEAD(&cma_xprt->sc_dto_q);\r\nINIT_LIST_HEAD(&cma_xprt->sc_rq_dto_q);\r\nINIT_LIST_HEAD(&cma_xprt->sc_read_complete_q);\r\nINIT_LIST_HEAD(&cma_xprt->sc_frmr_q);\r\ninit_waitqueue_head(&cma_xprt->sc_send_wait);\r\nspin_lock_init(&cma_xprt->sc_lock);\r\nspin_lock_init(&cma_xprt->sc_rq_dto_lock);\r\nspin_lock_init(&cma_xprt->sc_frmr_q_lock);\r\ncma_xprt->sc_ord = svcrdma_ord;\r\ncma_xprt->sc_max_req_size = svcrdma_max_req_size;\r\ncma_xprt->sc_max_requests = svcrdma_max_requests;\r\ncma_xprt->sc_sq_depth = svcrdma_max_requests * RPCRDMA_SQ_DEPTH_MULT;\r\natomic_set(&cma_xprt->sc_sq_count, 0);\r\natomic_set(&cma_xprt->sc_ctxt_used, 0);\r\nif (listener)\r\nset_bit(XPT_LISTENER, &cma_xprt->sc_xprt.xpt_flags);\r\nreturn cma_xprt;\r\n}\r\nstruct page *svc_rdma_get_page(void)\r\n{\r\nstruct page *page;\r\nwhile ((page = alloc_page(GFP_KERNEL)) == NULL) {\r\nprintk(KERN_INFO "svcrdma: out of memory...retrying in 1s\n");\r\nschedule_timeout_uninterruptible(msecs_to_jiffies(1000));\r\n}\r\nreturn page;\r\n}\r\nint svc_rdma_post_recv(struct svcxprt_rdma *xprt)\r\n{\r\nstruct ib_recv_wr recv_wr, *bad_recv_wr;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nstruct page *page;\r\ndma_addr_t pa;\r\nint sge_no;\r\nint buflen;\r\nint ret;\r\nctxt = svc_rdma_get_context(xprt);\r\nbuflen = 0;\r\nctxt->direction = DMA_FROM_DEVICE;\r\nfor (sge_no = 0; buflen < xprt->sc_max_req_size; sge_no++) {\r\nBUG_ON(sge_no >= xprt->sc_max_sge);\r\npage = svc_rdma_get_page();\r\nctxt->pages[sge_no] = page;\r\npa = ib_dma_map_page(xprt->sc_cm_id->device,\r\npage, 0, PAGE_SIZE,\r\nDMA_FROM_DEVICE);\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device, pa))\r\ngoto err_put_ctxt;\r\natomic_inc(&xprt->sc_dma_used);\r\nctxt->sge[sge_no].addr = pa;\r\nctxt->sge[sge_no].length = PAGE_SIZE;\r\nctxt->sge[sge_no].lkey = xprt->sc_dma_lkey;\r\nctxt->count = sge_no + 1;\r\nbuflen += PAGE_SIZE;\r\n}\r\nrecv_wr.next = NULL;\r\nrecv_wr.sg_list = &ctxt->sge[0];\r\nrecv_wr.num_sge = ctxt->count;\r\nrecv_wr.wr_id = (u64)(unsigned long)ctxt;\r\nsvc_xprt_get(&xprt->sc_xprt);\r\nret = ib_post_recv(xprt->sc_qp, &recv_wr, &bad_recv_wr);\r\nif (ret) {\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 1);\r\nsvc_xprt_put(&xprt->sc_xprt);\r\n}\r\nreturn ret;\r\nerr_put_ctxt:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn -ENOMEM;\r\n}\r\nstatic void handle_connect_req(struct rdma_cm_id *new_cma_id, size_t client_ird)\r\n{\r\nstruct svcxprt_rdma *listen_xprt = new_cma_id->context;\r\nstruct svcxprt_rdma *newxprt;\r\nstruct sockaddr *sa;\r\nnewxprt = rdma_create_xprt(listen_xprt->sc_xprt.xpt_server, 0);\r\nif (!newxprt) {\r\ndprintk("svcrdma: failed to create new transport\n");\r\nreturn;\r\n}\r\nnewxprt->sc_cm_id = new_cma_id;\r\nnew_cma_id->context = newxprt;\r\ndprintk("svcrdma: Creating newxprt=%p, cm_id=%p, listenxprt=%p\n",\r\nnewxprt, newxprt->sc_cm_id, listen_xprt);\r\nnewxprt->sc_ord = client_ird;\r\nsa = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.dst_addr;\r\nsvc_xprt_set_remote(&newxprt->sc_xprt, sa, svc_addr_len(sa));\r\nsa = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.src_addr;\r\nsvc_xprt_set_local(&newxprt->sc_xprt, sa, svc_addr_len(sa));\r\nspin_lock_bh(&listen_xprt->sc_lock);\r\nlist_add_tail(&newxprt->sc_accept_q, &listen_xprt->sc_accept_q);\r\nspin_unlock_bh(&listen_xprt->sc_lock);\r\nset_bit(XPT_CONN, &listen_xprt->sc_xprt.xpt_flags);\r\nsvc_xprt_enqueue(&listen_xprt->sc_xprt);\r\n}\r\nstatic int rdma_listen_handler(struct rdma_cm_id *cma_id,\r\nstruct rdma_cm_event *event)\r\n{\r\nstruct svcxprt_rdma *xprt = cma_id->context;\r\nint ret = 0;\r\nswitch (event->event) {\r\ncase RDMA_CM_EVENT_CONNECT_REQUEST:\r\ndprintk("svcrdma: Connect request on cma_id=%p, xprt = %p, "\r\n"event=%d\n", cma_id, cma_id->context, event->event);\r\nhandle_connect_req(cma_id,\r\nevent->param.conn.initiator_depth);\r\nbreak;\r\ncase RDMA_CM_EVENT_ESTABLISHED:\r\ndprintk("svcrdma: Connection completed on LISTEN xprt=%p, "\r\n"cm_id=%p\n", xprt, cma_id);\r\nbreak;\r\ncase RDMA_CM_EVENT_DEVICE_REMOVAL:\r\ndprintk("svcrdma: Device removal xprt=%p, cm_id=%p\n",\r\nxprt, cma_id);\r\nif (xprt)\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\nbreak;\r\ndefault:\r\ndprintk("svcrdma: Unexpected event on listening endpoint %p, "\r\n"event=%d\n", cma_id, event->event);\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int rdma_cma_handler(struct rdma_cm_id *cma_id,\r\nstruct rdma_cm_event *event)\r\n{\r\nstruct svc_xprt *xprt = cma_id->context;\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nswitch (event->event) {\r\ncase RDMA_CM_EVENT_ESTABLISHED:\r\nsvc_xprt_get(xprt);\r\ndprintk("svcrdma: Connection completed on DTO xprt=%p, "\r\n"cm_id=%p\n", xprt, cma_id);\r\nclear_bit(RDMAXPRT_CONN_PENDING, &rdma->sc_flags);\r\nsvc_xprt_enqueue(xprt);\r\nbreak;\r\ncase RDMA_CM_EVENT_DISCONNECTED:\r\ndprintk("svcrdma: Disconnect on DTO xprt=%p, cm_id=%p\n",\r\nxprt, cma_id);\r\nif (xprt) {\r\nset_bit(XPT_CLOSE, &xprt->xpt_flags);\r\nsvc_xprt_enqueue(xprt);\r\nsvc_xprt_put(xprt);\r\n}\r\nbreak;\r\ncase RDMA_CM_EVENT_DEVICE_REMOVAL:\r\ndprintk("svcrdma: Device removal cma_id=%p, xprt = %p, "\r\n"event=%d\n", cma_id, xprt, event->event);\r\nif (xprt) {\r\nset_bit(XPT_CLOSE, &xprt->xpt_flags);\r\nsvc_xprt_enqueue(xprt);\r\n}\r\nbreak;\r\ndefault:\r\ndprintk("svcrdma: Unexpected event on DTO endpoint %p, "\r\n"event=%d\n", cma_id, event->event);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct svc_xprt *svc_rdma_create(struct svc_serv *serv,\r\nstruct net *net,\r\nstruct sockaddr *sa, int salen,\r\nint flags)\r\n{\r\nstruct rdma_cm_id *listen_id;\r\nstruct svcxprt_rdma *cma_xprt;\r\nstruct svc_xprt *xprt;\r\nint ret;\r\ndprintk("svcrdma: Creating RDMA socket\n");\r\nif (sa->sa_family != AF_INET) {\r\ndprintk("svcrdma: Address family %d is not supported.\n", sa->sa_family);\r\nreturn ERR_PTR(-EAFNOSUPPORT);\r\n}\r\ncma_xprt = rdma_create_xprt(serv, 1);\r\nif (!cma_xprt)\r\nreturn ERR_PTR(-ENOMEM);\r\nxprt = &cma_xprt->sc_xprt;\r\nlisten_id = rdma_create_id(rdma_listen_handler, cma_xprt, RDMA_PS_TCP,\r\nIB_QPT_RC);\r\nif (IS_ERR(listen_id)) {\r\nret = PTR_ERR(listen_id);\r\ndprintk("svcrdma: rdma_create_id failed = %d\n", ret);\r\ngoto err0;\r\n}\r\nret = rdma_bind_addr(listen_id, sa);\r\nif (ret) {\r\ndprintk("svcrdma: rdma_bind_addr failed = %d\n", ret);\r\ngoto err1;\r\n}\r\ncma_xprt->sc_cm_id = listen_id;\r\nret = rdma_listen(listen_id, RPCRDMA_LISTEN_BACKLOG);\r\nif (ret) {\r\ndprintk("svcrdma: rdma_listen failed = %d\n", ret);\r\ngoto err1;\r\n}\r\nsa = (struct sockaddr *)&cma_xprt->sc_cm_id->route.addr.src_addr;\r\nsvc_xprt_set_local(&cma_xprt->sc_xprt, sa, salen);\r\nreturn &cma_xprt->sc_xprt;\r\nerr1:\r\nrdma_destroy_id(listen_id);\r\nerr0:\r\nkfree(cma_xprt);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic struct svc_rdma_fastreg_mr *rdma_alloc_frmr(struct svcxprt_rdma *xprt)\r\n{\r\nstruct ib_mr *mr;\r\nstruct ib_fast_reg_page_list *pl;\r\nstruct svc_rdma_fastreg_mr *frmr;\r\nfrmr = kmalloc(sizeof(*frmr), GFP_KERNEL);\r\nif (!frmr)\r\ngoto err;\r\nmr = ib_alloc_fast_reg_mr(xprt->sc_pd, RPCSVC_MAXPAGES);\r\nif (IS_ERR(mr))\r\ngoto err_free_frmr;\r\npl = ib_alloc_fast_reg_page_list(xprt->sc_cm_id->device,\r\nRPCSVC_MAXPAGES);\r\nif (IS_ERR(pl))\r\ngoto err_free_mr;\r\nfrmr->mr = mr;\r\nfrmr->page_list = pl;\r\nINIT_LIST_HEAD(&frmr->frmr_list);\r\nreturn frmr;\r\nerr_free_mr:\r\nib_dereg_mr(mr);\r\nerr_free_frmr:\r\nkfree(frmr);\r\nerr:\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nstatic void rdma_dealloc_frmr_q(struct svcxprt_rdma *xprt)\r\n{\r\nstruct svc_rdma_fastreg_mr *frmr;\r\nwhile (!list_empty(&xprt->sc_frmr_q)) {\r\nfrmr = list_entry(xprt->sc_frmr_q.next,\r\nstruct svc_rdma_fastreg_mr, frmr_list);\r\nlist_del_init(&frmr->frmr_list);\r\nib_dereg_mr(frmr->mr);\r\nib_free_fast_reg_page_list(frmr->page_list);\r\nkfree(frmr);\r\n}\r\n}\r\nstruct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *rdma)\r\n{\r\nstruct svc_rdma_fastreg_mr *frmr = NULL;\r\nspin_lock_bh(&rdma->sc_frmr_q_lock);\r\nif (!list_empty(&rdma->sc_frmr_q)) {\r\nfrmr = list_entry(rdma->sc_frmr_q.next,\r\nstruct svc_rdma_fastreg_mr, frmr_list);\r\nlist_del_init(&frmr->frmr_list);\r\nfrmr->map_len = 0;\r\nfrmr->page_list_len = 0;\r\n}\r\nspin_unlock_bh(&rdma->sc_frmr_q_lock);\r\nif (frmr)\r\nreturn frmr;\r\nreturn rdma_alloc_frmr(rdma);\r\n}\r\nstatic void frmr_unmap_dma(struct svcxprt_rdma *xprt,\r\nstruct svc_rdma_fastreg_mr *frmr)\r\n{\r\nint page_no;\r\nfor (page_no = 0; page_no < frmr->page_list_len; page_no++) {\r\ndma_addr_t addr = frmr->page_list->page_list[page_no];\r\nif (ib_dma_mapping_error(frmr->mr->device, addr))\r\ncontinue;\r\natomic_dec(&xprt->sc_dma_used);\r\nib_dma_unmap_page(frmr->mr->device, addr, PAGE_SIZE,\r\nfrmr->direction);\r\n}\r\n}\r\nvoid svc_rdma_put_frmr(struct svcxprt_rdma *rdma,\r\nstruct svc_rdma_fastreg_mr *frmr)\r\n{\r\nif (frmr) {\r\nfrmr_unmap_dma(rdma, frmr);\r\nspin_lock_bh(&rdma->sc_frmr_q_lock);\r\nBUG_ON(!list_empty(&frmr->frmr_list));\r\nlist_add(&frmr->frmr_list, &rdma->sc_frmr_q);\r\nspin_unlock_bh(&rdma->sc_frmr_q_lock);\r\n}\r\n}\r\nstatic struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt)\r\n{\r\nstruct svcxprt_rdma *listen_rdma;\r\nstruct svcxprt_rdma *newxprt = NULL;\r\nstruct rdma_conn_param conn_param;\r\nstruct ib_qp_init_attr qp_attr;\r\nstruct ib_device_attr devattr;\r\nint uninitialized_var(dma_mr_acc);\r\nint need_dma_mr;\r\nint ret;\r\nint i;\r\nlisten_rdma = container_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nclear_bit(XPT_CONN, &xprt->xpt_flags);\r\nspin_lock_bh(&listen_rdma->sc_lock);\r\nif (!list_empty(&listen_rdma->sc_accept_q)) {\r\nnewxprt = list_entry(listen_rdma->sc_accept_q.next,\r\nstruct svcxprt_rdma, sc_accept_q);\r\nlist_del_init(&newxprt->sc_accept_q);\r\n}\r\nif (!list_empty(&listen_rdma->sc_accept_q))\r\nset_bit(XPT_CONN, &listen_rdma->sc_xprt.xpt_flags);\r\nspin_unlock_bh(&listen_rdma->sc_lock);\r\nif (!newxprt)\r\nreturn NULL;\r\ndprintk("svcrdma: newxprt from accept queue = %p, cm_id=%p\n",\r\nnewxprt, newxprt->sc_cm_id);\r\nret = ib_query_device(newxprt->sc_cm_id->device, &devattr);\r\nif (ret) {\r\ndprintk("svcrdma: could not query device attributes on "\r\n"device %p, rc=%d\n", newxprt->sc_cm_id->device, ret);\r\ngoto errout;\r\n}\r\nnewxprt->sc_max_sge = min((size_t)devattr.max_sge,\r\n(size_t)RPCSVC_MAXPAGES);\r\nnewxprt->sc_max_requests = min((size_t)devattr.max_qp_wr,\r\n(size_t)svcrdma_max_requests);\r\nnewxprt->sc_sq_depth = RPCRDMA_SQ_DEPTH_MULT * newxprt->sc_max_requests;\r\nnewxprt->sc_ord = min_t(size_t, devattr.max_qp_rd_atom, newxprt->sc_ord);\r\nnewxprt->sc_ord = min_t(size_t, svcrdma_ord, newxprt->sc_ord);\r\nnewxprt->sc_pd = ib_alloc_pd(newxprt->sc_cm_id->device);\r\nif (IS_ERR(newxprt->sc_pd)) {\r\ndprintk("svcrdma: error creating PD for connect request\n");\r\ngoto errout;\r\n}\r\nnewxprt->sc_sq_cq = ib_create_cq(newxprt->sc_cm_id->device,\r\nsq_comp_handler,\r\ncq_event_handler,\r\nnewxprt,\r\nnewxprt->sc_sq_depth,\r\n0);\r\nif (IS_ERR(newxprt->sc_sq_cq)) {\r\ndprintk("svcrdma: error creating SQ CQ for connect request\n");\r\ngoto errout;\r\n}\r\nnewxprt->sc_rq_cq = ib_create_cq(newxprt->sc_cm_id->device,\r\nrq_comp_handler,\r\ncq_event_handler,\r\nnewxprt,\r\nnewxprt->sc_max_requests,\r\n0);\r\nif (IS_ERR(newxprt->sc_rq_cq)) {\r\ndprintk("svcrdma: error creating RQ CQ for connect request\n");\r\ngoto errout;\r\n}\r\nmemset(&qp_attr, 0, sizeof qp_attr);\r\nqp_attr.event_handler = qp_event_handler;\r\nqp_attr.qp_context = &newxprt->sc_xprt;\r\nqp_attr.cap.max_send_wr = newxprt->sc_sq_depth;\r\nqp_attr.cap.max_recv_wr = newxprt->sc_max_requests;\r\nqp_attr.cap.max_send_sge = newxprt->sc_max_sge;\r\nqp_attr.cap.max_recv_sge = newxprt->sc_max_sge;\r\nqp_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\r\nqp_attr.qp_type = IB_QPT_RC;\r\nqp_attr.send_cq = newxprt->sc_sq_cq;\r\nqp_attr.recv_cq = newxprt->sc_rq_cq;\r\ndprintk("svcrdma: newxprt->sc_cm_id=%p, newxprt->sc_pd=%p\n"\r\n" cm_id->device=%p, sc_pd->device=%p\n"\r\n" cap.max_send_wr = %d\n"\r\n" cap.max_recv_wr = %d\n"\r\n" cap.max_send_sge = %d\n"\r\n" cap.max_recv_sge = %d\n",\r\nnewxprt->sc_cm_id, newxprt->sc_pd,\r\nnewxprt->sc_cm_id->device, newxprt->sc_pd->device,\r\nqp_attr.cap.max_send_wr,\r\nqp_attr.cap.max_recv_wr,\r\nqp_attr.cap.max_send_sge,\r\nqp_attr.cap.max_recv_sge);\r\nret = rdma_create_qp(newxprt->sc_cm_id, newxprt->sc_pd, &qp_attr);\r\nif (ret) {\r\ndprintk("svcrdma: failed to create QP, ret=%d\n", ret);\r\ngoto errout;\r\n}\r\nnewxprt->sc_qp = newxprt->sc_cm_id->qp;\r\nif (devattr.device_cap_flags & IB_DEVICE_MEM_MGT_EXTENSIONS) {\r\nnewxprt->sc_frmr_pg_list_len =\r\ndevattr.max_fast_reg_page_list_len;\r\nnewxprt->sc_dev_caps |= SVCRDMA_DEVCAP_FAST_REG;\r\n}\r\nswitch (rdma_node_get_transport(newxprt->sc_cm_id->device->node_type)) {\r\ncase RDMA_TRANSPORT_IWARP:\r\nnewxprt->sc_dev_caps |= SVCRDMA_DEVCAP_READ_W_INV;\r\nif (!(newxprt->sc_dev_caps & SVCRDMA_DEVCAP_FAST_REG)) {\r\nneed_dma_mr = 1;\r\ndma_mr_acc =\r\n(IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE);\r\n} else if (!(devattr.device_cap_flags & IB_DEVICE_LOCAL_DMA_LKEY)) {\r\nneed_dma_mr = 1;\r\ndma_mr_acc = IB_ACCESS_LOCAL_WRITE;\r\n} else\r\nneed_dma_mr = 0;\r\nbreak;\r\ncase RDMA_TRANSPORT_IB:\r\nif (!(newxprt->sc_dev_caps & SVCRDMA_DEVCAP_FAST_REG)) {\r\nneed_dma_mr = 1;\r\ndma_mr_acc = IB_ACCESS_LOCAL_WRITE;\r\n} else if (!(devattr.device_cap_flags &\r\nIB_DEVICE_LOCAL_DMA_LKEY)) {\r\nneed_dma_mr = 1;\r\ndma_mr_acc = IB_ACCESS_LOCAL_WRITE;\r\n} else\r\nneed_dma_mr = 0;\r\nbreak;\r\ndefault:\r\ngoto errout;\r\n}\r\nif (need_dma_mr) {\r\nnewxprt->sc_phys_mr =\r\nib_get_dma_mr(newxprt->sc_pd, dma_mr_acc);\r\nif (IS_ERR(newxprt->sc_phys_mr)) {\r\ndprintk("svcrdma: Failed to create DMA MR ret=%d\n",\r\nret);\r\ngoto errout;\r\n}\r\nnewxprt->sc_dma_lkey = newxprt->sc_phys_mr->lkey;\r\n} else\r\nnewxprt->sc_dma_lkey =\r\nnewxprt->sc_cm_id->device->local_dma_lkey;\r\nfor (i = 0; i < newxprt->sc_max_requests; i++) {\r\nret = svc_rdma_post_recv(newxprt);\r\nif (ret) {\r\ndprintk("svcrdma: failure posting receive buffers\n");\r\ngoto errout;\r\n}\r\n}\r\nnewxprt->sc_cm_id->event_handler = rdma_cma_handler;\r\nib_req_notify_cq(newxprt->sc_sq_cq, IB_CQ_NEXT_COMP);\r\nib_req_notify_cq(newxprt->sc_rq_cq, IB_CQ_NEXT_COMP);\r\nset_bit(RDMAXPRT_CONN_PENDING, &newxprt->sc_flags);\r\nmemset(&conn_param, 0, sizeof conn_param);\r\nconn_param.responder_resources = 0;\r\nconn_param.initiator_depth = newxprt->sc_ord;\r\nret = rdma_accept(newxprt->sc_cm_id, &conn_param);\r\nif (ret) {\r\ndprintk("svcrdma: failed to accept new connection, ret=%d\n",\r\nret);\r\ngoto errout;\r\n}\r\ndprintk("svcrdma: new connection %p accepted with the following "\r\n"attributes:\n"\r\n" local_ip : %pI4\n"\r\n" local_port : %d\n"\r\n" remote_ip : %pI4\n"\r\n" remote_port : %d\n"\r\n" max_sge : %d\n"\r\n" sq_depth : %d\n"\r\n" max_requests : %d\n"\r\n" ord : %d\n",\r\nnewxprt,\r\n&((struct sockaddr_in *)&newxprt->sc_cm_id->\r\nroute.addr.src_addr)->sin_addr.s_addr,\r\nntohs(((struct sockaddr_in *)&newxprt->sc_cm_id->\r\nroute.addr.src_addr)->sin_port),\r\n&((struct sockaddr_in *)&newxprt->sc_cm_id->\r\nroute.addr.dst_addr)->sin_addr.s_addr,\r\nntohs(((struct sockaddr_in *)&newxprt->sc_cm_id->\r\nroute.addr.dst_addr)->sin_port),\r\nnewxprt->sc_max_sge,\r\nnewxprt->sc_sq_depth,\r\nnewxprt->sc_max_requests,\r\nnewxprt->sc_ord);\r\nreturn &newxprt->sc_xprt;\r\nerrout:\r\ndprintk("svcrdma: failure accepting new connection rc=%d.\n", ret);\r\nsvc_xprt_get(&newxprt->sc_xprt);\r\nif (newxprt->sc_qp && !IS_ERR(newxprt->sc_qp))\r\nib_destroy_qp(newxprt->sc_qp);\r\nrdma_destroy_id(newxprt->sc_cm_id);\r\nsvc_xprt_put(&newxprt->sc_xprt);\r\nreturn NULL;\r\n}\r\nstatic void svc_rdma_release_rqst(struct svc_rqst *rqstp)\r\n{\r\n}\r\nstatic void svc_rdma_detach(struct svc_xprt *xprt)\r\n{\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\ndprintk("svc: svc_rdma_detach(%p)\n", xprt);\r\nrdma_disconnect(rdma->sc_cm_id);\r\n}\r\nstatic void __svc_rdma_free(struct work_struct *work)\r\n{\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(work, struct svcxprt_rdma, sc_work);\r\ndprintk("svcrdma: svc_rdma_free(%p)\n", rdma);\r\nBUG_ON(atomic_read(&rdma->sc_xprt.xpt_ref.refcount) != 0);\r\nwhile (!list_empty(&rdma->sc_read_complete_q)) {\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nctxt = list_entry(rdma->sc_read_complete_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\nsvc_rdma_put_context(ctxt, 1);\r\n}\r\nwhile (!list_empty(&rdma->sc_rq_dto_q)) {\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nctxt = list_entry(rdma->sc_rq_dto_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\nsvc_rdma_put_context(ctxt, 1);\r\n}\r\nWARN_ON(atomic_read(&rdma->sc_ctxt_used) != 0);\r\nWARN_ON(atomic_read(&rdma->sc_dma_used) != 0);\r\nrdma_dealloc_frmr_q(rdma);\r\nif (rdma->sc_qp && !IS_ERR(rdma->sc_qp))\r\nib_destroy_qp(rdma->sc_qp);\r\nif (rdma->sc_sq_cq && !IS_ERR(rdma->sc_sq_cq))\r\nib_destroy_cq(rdma->sc_sq_cq);\r\nif (rdma->sc_rq_cq && !IS_ERR(rdma->sc_rq_cq))\r\nib_destroy_cq(rdma->sc_rq_cq);\r\nif (rdma->sc_phys_mr && !IS_ERR(rdma->sc_phys_mr))\r\nib_dereg_mr(rdma->sc_phys_mr);\r\nif (rdma->sc_pd && !IS_ERR(rdma->sc_pd))\r\nib_dealloc_pd(rdma->sc_pd);\r\nrdma_destroy_id(rdma->sc_cm_id);\r\nkfree(rdma);\r\n}\r\nstatic void svc_rdma_free(struct svc_xprt *xprt)\r\n{\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nINIT_WORK(&rdma->sc_work, __svc_rdma_free);\r\nqueue_work(svc_rdma_wq, &rdma->sc_work);\r\n}\r\nstatic int svc_rdma_has_wspace(struct svc_xprt *xprt)\r\n{\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nif (waitqueue_active(&rdma->sc_send_wait))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int svc_rdma_secure_port(struct svc_rqst *rqstp)\r\n{\r\nreturn 1;\r\n}\r\nint svc_rdma_fastreg(struct svcxprt_rdma *xprt,\r\nstruct svc_rdma_fastreg_mr *frmr)\r\n{\r\nstruct ib_send_wr fastreg_wr;\r\nu8 key;\r\nkey = (u8)(frmr->mr->lkey & 0x000000FF);\r\nib_update_fast_reg_key(frmr->mr, ++key);\r\nmemset(&fastreg_wr, 0, sizeof fastreg_wr);\r\nfastreg_wr.opcode = IB_WR_FAST_REG_MR;\r\nfastreg_wr.send_flags = IB_SEND_SIGNALED;\r\nfastreg_wr.wr.fast_reg.iova_start = (unsigned long)frmr->kva;\r\nfastreg_wr.wr.fast_reg.page_list = frmr->page_list;\r\nfastreg_wr.wr.fast_reg.page_list_len = frmr->page_list_len;\r\nfastreg_wr.wr.fast_reg.page_shift = PAGE_SHIFT;\r\nfastreg_wr.wr.fast_reg.length = frmr->map_len;\r\nfastreg_wr.wr.fast_reg.access_flags = frmr->access_flags;\r\nfastreg_wr.wr.fast_reg.rkey = frmr->mr->lkey;\r\nreturn svc_rdma_send(xprt, &fastreg_wr);\r\n}\r\nint svc_rdma_send(struct svcxprt_rdma *xprt, struct ib_send_wr *wr)\r\n{\r\nstruct ib_send_wr *bad_wr, *n_wr;\r\nint wr_count;\r\nint i;\r\nint ret;\r\nif (test_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags))\r\nreturn -ENOTCONN;\r\nBUG_ON(wr->send_flags != IB_SEND_SIGNALED);\r\nwr_count = 1;\r\nfor (n_wr = wr->next; n_wr; n_wr = n_wr->next)\r\nwr_count++;\r\nwhile (1) {\r\nspin_lock_bh(&xprt->sc_lock);\r\nif (xprt->sc_sq_depth < atomic_read(&xprt->sc_sq_count) + wr_count) {\r\nspin_unlock_bh(&xprt->sc_lock);\r\natomic_inc(&rdma_stat_sq_starve);\r\nsq_cq_reap(xprt);\r\nwait_event(xprt->sc_send_wait,\r\natomic_read(&xprt->sc_sq_count) <\r\nxprt->sc_sq_depth);\r\nif (test_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags))\r\nreturn -ENOTCONN;\r\ncontinue;\r\n}\r\nfor (i = 0; i < wr_count; i++)\r\nsvc_xprt_get(&xprt->sc_xprt);\r\natomic_add(wr_count, &xprt->sc_sq_count);\r\nret = ib_post_send(xprt->sc_qp, wr, &bad_wr);\r\nif (ret) {\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\natomic_sub(wr_count, &xprt->sc_sq_count);\r\nfor (i = 0; i < wr_count; i ++)\r\nsvc_xprt_put(&xprt->sc_xprt);\r\ndprintk("svcrdma: failed to post SQ WR rc=%d, "\r\n"sc_sq_count=%d, sc_sq_depth=%d\n",\r\nret, atomic_read(&xprt->sc_sq_count),\r\nxprt->sc_sq_depth);\r\n}\r\nspin_unlock_bh(&xprt->sc_lock);\r\nif (ret)\r\nwake_up(&xprt->sc_send_wait);\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nvoid svc_rdma_send_error(struct svcxprt_rdma *xprt, struct rpcrdma_msg *rmsgp,\r\nenum rpcrdma_errcode err)\r\n{\r\nstruct ib_send_wr err_wr;\r\nstruct page *p;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nu32 *va;\r\nint length;\r\nint ret;\r\np = svc_rdma_get_page();\r\nva = page_address(p);\r\nlength = svc_rdma_xdr_encode_error(xprt, rmsgp, err, va);\r\nctxt = svc_rdma_get_context(xprt);\r\nctxt->direction = DMA_FROM_DEVICE;\r\nctxt->count = 1;\r\nctxt->pages[0] = p;\r\nctxt->sge[0].addr = ib_dma_map_page(xprt->sc_cm_id->device,\r\np, 0, length, DMA_FROM_DEVICE);\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device, ctxt->sge[0].addr)) {\r\nput_page(p);\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn;\r\n}\r\natomic_inc(&xprt->sc_dma_used);\r\nctxt->sge[0].lkey = xprt->sc_dma_lkey;\r\nctxt->sge[0].length = length;\r\nmemset(&err_wr, 0, sizeof err_wr);\r\nctxt->wr_op = IB_WR_SEND;\r\nerr_wr.wr_id = (unsigned long)ctxt;\r\nerr_wr.sg_list = ctxt->sge;\r\nerr_wr.num_sge = 1;\r\nerr_wr.opcode = IB_WR_SEND;\r\nerr_wr.send_flags = IB_SEND_SIGNALED;\r\nret = svc_rdma_send(xprt, &err_wr);\r\nif (ret) {\r\ndprintk("svcrdma: Error %d posting send for protocol error\n",\r\nret);\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 1);\r\n}\r\n}
