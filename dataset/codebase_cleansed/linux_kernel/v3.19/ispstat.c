static void __isp_stat_buf_sync_magic(struct ispstat *stat,\r\nstruct ispstat_buffer *buf,\r\nu32 buf_size, enum dma_data_direction dir,\r\nvoid (*dma_sync)(struct device *,\r\ndma_addr_t, unsigned long, size_t,\r\nenum dma_data_direction))\r\n{\r\ndma_sync(stat->isp->dev, buf->dma_addr, 0, MAGIC_SIZE, dir);\r\ndma_sync(stat->isp->dev, buf->dma_addr + (buf_size & PAGE_MASK),\r\nbuf_size & ~PAGE_MASK, MAGIC_SIZE, dir);\r\n}\r\nstatic void isp_stat_buf_sync_magic_for_device(struct ispstat *stat,\r\nstruct ispstat_buffer *buf,\r\nu32 buf_size,\r\nenum dma_data_direction dir)\r\n{\r\nif (ISP_STAT_USES_DMAENGINE(stat))\r\nreturn;\r\n__isp_stat_buf_sync_magic(stat, buf, buf_size, dir,\r\ndma_sync_single_range_for_device);\r\n}\r\nstatic void isp_stat_buf_sync_magic_for_cpu(struct ispstat *stat,\r\nstruct ispstat_buffer *buf,\r\nu32 buf_size,\r\nenum dma_data_direction dir)\r\n{\r\nif (ISP_STAT_USES_DMAENGINE(stat))\r\nreturn;\r\n__isp_stat_buf_sync_magic(stat, buf, buf_size, dir,\r\ndma_sync_single_range_for_cpu);\r\n}\r\nstatic int isp_stat_buf_check_magic(struct ispstat *stat,\r\nstruct ispstat_buffer *buf)\r\n{\r\nconst u32 buf_size = IS_H3A_AF(stat) ?\r\nbuf->buf_size + AF_EXTRA_DATA : buf->buf_size;\r\nu8 *w;\r\nu8 *end;\r\nint ret = -EINVAL;\r\nisp_stat_buf_sync_magic_for_cpu(stat, buf, buf_size, DMA_FROM_DEVICE);\r\nfor (w = buf->virt_addr, end = w + MAGIC_SIZE; w < end; w++)\r\nif (likely(*w != MAGIC_NUM))\r\nret = 0;\r\nif (ret) {\r\ndev_dbg(stat->isp->dev, "%s: beginning magic check does not "\r\n"match.\n", stat->subdev.name);\r\nreturn ret;\r\n}\r\nfor (w = buf->virt_addr + buf_size, end = w + MAGIC_SIZE;\r\nw < end; w++) {\r\nif (unlikely(*w != MAGIC_NUM)) {\r\ndev_dbg(stat->isp->dev, "%s: ending magic check does "\r\n"not match.\n", stat->subdev.name);\r\nreturn -EINVAL;\r\n}\r\n}\r\nisp_stat_buf_sync_magic_for_device(stat, buf, buf_size,\r\nDMA_FROM_DEVICE);\r\nreturn 0;\r\n}\r\nstatic void isp_stat_buf_insert_magic(struct ispstat *stat,\r\nstruct ispstat_buffer *buf)\r\n{\r\nconst u32 buf_size = IS_H3A_AF(stat) ?\r\nstat->buf_size + AF_EXTRA_DATA : stat->buf_size;\r\nisp_stat_buf_sync_magic_for_cpu(stat, buf, buf_size, DMA_FROM_DEVICE);\r\nmemset(buf->virt_addr, MAGIC_NUM, MAGIC_SIZE);\r\nmemset(buf->virt_addr + buf_size, MAGIC_NUM, MAGIC_SIZE);\r\nisp_stat_buf_sync_magic_for_device(stat, buf, buf_size,\r\nDMA_BIDIRECTIONAL);\r\n}\r\nstatic void isp_stat_buf_sync_for_device(struct ispstat *stat,\r\nstruct ispstat_buffer *buf)\r\n{\r\nif (ISP_STAT_USES_DMAENGINE(stat))\r\nreturn;\r\ndma_sync_sg_for_device(stat->isp->dev, buf->sgt.sgl,\r\nbuf->sgt.nents, DMA_FROM_DEVICE);\r\n}\r\nstatic void isp_stat_buf_sync_for_cpu(struct ispstat *stat,\r\nstruct ispstat_buffer *buf)\r\n{\r\nif (ISP_STAT_USES_DMAENGINE(stat))\r\nreturn;\r\ndma_sync_sg_for_cpu(stat->isp->dev, buf->sgt.sgl,\r\nbuf->sgt.nents, DMA_FROM_DEVICE);\r\n}\r\nstatic void isp_stat_buf_clear(struct ispstat *stat)\r\n{\r\nint i;\r\nfor (i = 0; i < STAT_MAX_BUFS; i++)\r\nstat->buf[i].empty = 1;\r\n}\r\nstatic struct ispstat_buffer *\r\n__isp_stat_buf_find(struct ispstat *stat, int look_empty)\r\n{\r\nstruct ispstat_buffer *found = NULL;\r\nint i;\r\nfor (i = 0; i < STAT_MAX_BUFS; i++) {\r\nstruct ispstat_buffer *curr = &stat->buf[i];\r\nif (curr == stat->locked_buf || curr == stat->active_buf)\r\ncontinue;\r\nif (!look_empty && curr->empty)\r\ncontinue;\r\nif (curr->empty) {\r\nfound = curr;\r\nbreak;\r\n}\r\nif (!found ||\r\n(s32)curr->frame_number - (s32)found->frame_number < 0)\r\nfound = curr;\r\n}\r\nreturn found;\r\n}\r\nstatic inline struct ispstat_buffer *\r\nisp_stat_buf_find_oldest(struct ispstat *stat)\r\n{\r\nreturn __isp_stat_buf_find(stat, 0);\r\n}\r\nstatic inline struct ispstat_buffer *\r\nisp_stat_buf_find_oldest_or_empty(struct ispstat *stat)\r\n{\r\nreturn __isp_stat_buf_find(stat, 1);\r\n}\r\nstatic int isp_stat_buf_queue(struct ispstat *stat)\r\n{\r\nif (!stat->active_buf)\r\nreturn STAT_NO_BUF;\r\nktime_get_ts(&stat->active_buf->ts);\r\nstat->active_buf->buf_size = stat->buf_size;\r\nif (isp_stat_buf_check_magic(stat, stat->active_buf)) {\r\ndev_dbg(stat->isp->dev, "%s: data wasn't properly written.\n",\r\nstat->subdev.name);\r\nreturn STAT_NO_BUF;\r\n}\r\nstat->active_buf->config_counter = stat->config_counter;\r\nstat->active_buf->frame_number = stat->frame_number;\r\nstat->active_buf->empty = 0;\r\nstat->active_buf = NULL;\r\nreturn STAT_BUF_DONE;\r\n}\r\nstatic void isp_stat_buf_next(struct ispstat *stat)\r\n{\r\nif (unlikely(stat->active_buf))\r\ndev_dbg(stat->isp->dev, "%s: new buffer requested without "\r\n"queuing active one.\n",\r\nstat->subdev.name);\r\nelse\r\nstat->active_buf = isp_stat_buf_find_oldest_or_empty(stat);\r\n}\r\nstatic void isp_stat_buf_release(struct ispstat *stat)\r\n{\r\nunsigned long flags;\r\nisp_stat_buf_sync_for_device(stat, stat->locked_buf);\r\nspin_lock_irqsave(&stat->isp->stat_lock, flags);\r\nstat->locked_buf = NULL;\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, flags);\r\n}\r\nstatic struct ispstat_buffer *isp_stat_buf_get(struct ispstat *stat,\r\nstruct omap3isp_stat_data *data)\r\n{\r\nint rval = 0;\r\nunsigned long flags;\r\nstruct ispstat_buffer *buf;\r\nspin_lock_irqsave(&stat->isp->stat_lock, flags);\r\nwhile (1) {\r\nbuf = isp_stat_buf_find_oldest(stat);\r\nif (!buf) {\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, flags);\r\ndev_dbg(stat->isp->dev, "%s: cannot find a buffer.\n",\r\nstat->subdev.name);\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\nif (isp_stat_buf_check_magic(stat, buf)) {\r\ndev_dbg(stat->isp->dev, "%s: current buffer has "\r\n"corrupted data\n.", stat->subdev.name);\r\nbuf->empty = 1;\r\n} else {\r\nbreak;\r\n}\r\n}\r\nstat->locked_buf = buf;\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, flags);\r\nif (buf->buf_size > data->buf_size) {\r\ndev_warn(stat->isp->dev, "%s: userspace's buffer size is "\r\n"not enough.\n", stat->subdev.name);\r\nisp_stat_buf_release(stat);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nisp_stat_buf_sync_for_cpu(stat, buf);\r\nrval = copy_to_user(data->buf,\r\nbuf->virt_addr,\r\nbuf->buf_size);\r\nif (rval) {\r\ndev_info(stat->isp->dev,\r\n"%s: failed copying %d bytes of stat data\n",\r\nstat->subdev.name, rval);\r\nbuf = ERR_PTR(-EFAULT);\r\nisp_stat_buf_release(stat);\r\n}\r\nreturn buf;\r\n}\r\nstatic void isp_stat_bufs_free(struct ispstat *stat)\r\n{\r\nstruct device *dev = ISP_STAT_USES_DMAENGINE(stat)\r\n? NULL : stat->isp->dev;\r\nunsigned int i;\r\nfor (i = 0; i < STAT_MAX_BUFS; i++) {\r\nstruct ispstat_buffer *buf = &stat->buf[i];\r\nif (!buf->virt_addr)\r\ncontinue;\r\nsg_free_table(&buf->sgt);\r\ndma_free_coherent(dev, stat->buf_alloc_size, buf->virt_addr,\r\nbuf->dma_addr);\r\nbuf->dma_addr = 0;\r\nbuf->virt_addr = NULL;\r\nbuf->empty = 1;\r\n}\r\ndev_dbg(stat->isp->dev, "%s: all buffers were freed.\n",\r\nstat->subdev.name);\r\nstat->buf_alloc_size = 0;\r\nstat->active_buf = NULL;\r\n}\r\nstatic int isp_stat_bufs_alloc_one(struct device *dev,\r\nstruct ispstat_buffer *buf,\r\nunsigned int size)\r\n{\r\nint ret;\r\nbuf->virt_addr = dma_alloc_coherent(dev, size, &buf->dma_addr,\r\nGFP_KERNEL | GFP_DMA);\r\nif (!buf->virt_addr)\r\nreturn -ENOMEM;\r\nret = dma_get_sgtable(dev, &buf->sgt, buf->virt_addr, buf->dma_addr,\r\nsize);\r\nif (ret < 0) {\r\ndma_free_coherent(dev, size, buf->virt_addr, buf->dma_addr);\r\nbuf->virt_addr = NULL;\r\nbuf->dma_addr = 0;\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int isp_stat_bufs_alloc(struct ispstat *stat, u32 size)\r\n{\r\nstruct device *dev = ISP_STAT_USES_DMAENGINE(stat)\r\n? NULL : stat->isp->dev;\r\nunsigned long flags;\r\nunsigned int i;\r\nspin_lock_irqsave(&stat->isp->stat_lock, flags);\r\nBUG_ON(stat->locked_buf != NULL);\r\nif (stat->buf_alloc_size >= size) {\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, flags);\r\nreturn 0;\r\n}\r\nif (stat->state != ISPSTAT_DISABLED || stat->buf_processing) {\r\ndev_info(stat->isp->dev,\r\n"%s: trying to allocate memory when busy\n",\r\nstat->subdev.name);\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, flags);\r\nreturn -EBUSY;\r\n}\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, flags);\r\nisp_stat_bufs_free(stat);\r\nstat->buf_alloc_size = size;\r\nfor (i = 0; i < STAT_MAX_BUFS; i++) {\r\nstruct ispstat_buffer *buf = &stat->buf[i];\r\nint ret;\r\nret = isp_stat_bufs_alloc_one(dev, buf, size);\r\nif (ret < 0) {\r\ndev_err(stat->isp->dev,\r\n"%s: Failed to allocate DMA buffer %u\n",\r\nstat->subdev.name, i);\r\nisp_stat_bufs_free(stat);\r\nreturn ret;\r\n}\r\nbuf->empty = 1;\r\ndev_dbg(stat->isp->dev,\r\n"%s: buffer[%u] allocated. dma=0x%08lx virt=0x%08lx",\r\nstat->subdev.name, i,\r\n(unsigned long)buf->dma_addr,\r\n(unsigned long)buf->virt_addr);\r\n}\r\nreturn 0;\r\n}\r\nstatic void isp_stat_queue_event(struct ispstat *stat, int err)\r\n{\r\nstruct video_device *vdev = stat->subdev.devnode;\r\nstruct v4l2_event event;\r\nstruct omap3isp_stat_event_status *status = (void *)event.u.data;\r\nmemset(&event, 0, sizeof(event));\r\nif (!err) {\r\nstatus->frame_number = stat->frame_number;\r\nstatus->config_counter = stat->config_counter;\r\n} else {\r\nstatus->buf_err = 1;\r\n}\r\nevent.type = stat->event_type;\r\nv4l2_event_queue(vdev, &event);\r\n}\r\nint omap3isp_stat_request_statistics(struct ispstat *stat,\r\nstruct omap3isp_stat_data *data)\r\n{\r\nstruct ispstat_buffer *buf;\r\nif (stat->state != ISPSTAT_ENABLED) {\r\ndev_dbg(stat->isp->dev, "%s: engine not enabled.\n",\r\nstat->subdev.name);\r\nreturn -EINVAL;\r\n}\r\nmutex_lock(&stat->ioctl_lock);\r\nbuf = isp_stat_buf_get(stat, data);\r\nif (IS_ERR(buf)) {\r\nmutex_unlock(&stat->ioctl_lock);\r\nreturn PTR_ERR(buf);\r\n}\r\ndata->ts.tv_sec = buf->ts.tv_sec;\r\ndata->ts.tv_usec = buf->ts.tv_nsec / NSEC_PER_USEC;\r\ndata->config_counter = buf->config_counter;\r\ndata->frame_number = buf->frame_number;\r\ndata->buf_size = buf->buf_size;\r\nbuf->empty = 1;\r\nisp_stat_buf_release(stat);\r\nmutex_unlock(&stat->ioctl_lock);\r\nreturn 0;\r\n}\r\nint omap3isp_stat_config(struct ispstat *stat, void *new_conf)\r\n{\r\nint ret;\r\nunsigned long irqflags;\r\nstruct ispstat_generic_config *user_cfg = new_conf;\r\nu32 buf_size = user_cfg->buf_size;\r\nif (!new_conf) {\r\ndev_dbg(stat->isp->dev, "%s: configuration is NULL\n",\r\nstat->subdev.name);\r\nreturn -EINVAL;\r\n}\r\nmutex_lock(&stat->ioctl_lock);\r\ndev_dbg(stat->isp->dev, "%s: configuring module with buffer "\r\n"size=0x%08lx\n", stat->subdev.name, (unsigned long)buf_size);\r\nret = stat->ops->validate_params(stat, new_conf);\r\nif (ret) {\r\nmutex_unlock(&stat->ioctl_lock);\r\ndev_dbg(stat->isp->dev, "%s: configuration values are "\r\n"invalid.\n", stat->subdev.name);\r\nreturn ret;\r\n}\r\nif (buf_size != user_cfg->buf_size)\r\ndev_dbg(stat->isp->dev, "%s: driver has corrected buffer size "\r\n"request to 0x%08lx\n", stat->subdev.name,\r\n(unsigned long)user_cfg->buf_size);\r\nif (IS_H3A(stat)) {\r\nbuf_size = user_cfg->buf_size * 2 + MAGIC_SIZE;\r\nif (IS_H3A_AF(stat))\r\nbuf_size += AF_EXTRA_DATA * (NUM_H3A_RECOVER_BUFS + 2);\r\nif (stat->recover_priv) {\r\nstruct ispstat_generic_config *recover_cfg =\r\nstat->recover_priv;\r\nbuf_size += recover_cfg->buf_size *\r\nNUM_H3A_RECOVER_BUFS;\r\n}\r\nbuf_size = PAGE_ALIGN(buf_size);\r\n} else {\r\nbuf_size = PAGE_ALIGN(user_cfg->buf_size + MAGIC_SIZE);\r\n}\r\nret = isp_stat_bufs_alloc(stat, buf_size);\r\nif (ret) {\r\nmutex_unlock(&stat->ioctl_lock);\r\nreturn ret;\r\n}\r\nspin_lock_irqsave(&stat->isp->stat_lock, irqflags);\r\nstat->ops->set_params(stat, new_conf);\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\nuser_cfg->config_counter = stat->config_counter + stat->inc_config;\r\nstat->configured = 1;\r\ndev_dbg(stat->isp->dev, "%s: module has been successfully "\r\n"configured.\n", stat->subdev.name);\r\nmutex_unlock(&stat->ioctl_lock);\r\nreturn 0;\r\n}\r\nstatic int isp_stat_buf_process(struct ispstat *stat, int buf_state)\r\n{\r\nint ret = STAT_NO_BUF;\r\nif (!atomic_add_unless(&stat->buf_err, -1, 0) &&\r\nbuf_state == STAT_BUF_DONE && stat->state == ISPSTAT_ENABLED) {\r\nret = isp_stat_buf_queue(stat);\r\nisp_stat_buf_next(stat);\r\n}\r\nreturn ret;\r\n}\r\nint omap3isp_stat_pcr_busy(struct ispstat *stat)\r\n{\r\nreturn stat->ops->busy(stat);\r\n}\r\nint omap3isp_stat_busy(struct ispstat *stat)\r\n{\r\nreturn omap3isp_stat_pcr_busy(stat) | stat->buf_processing |\r\n(stat->state != ISPSTAT_DISABLED);\r\n}\r\nstatic void isp_stat_pcr_enable(struct ispstat *stat, u8 pcr_enable)\r\n{\r\nif ((stat->state != ISPSTAT_ENABLING &&\r\nstat->state != ISPSTAT_ENABLED) && pcr_enable)\r\nreturn;\r\nstat->ops->enable(stat, pcr_enable);\r\nif (stat->state == ISPSTAT_DISABLING && !pcr_enable)\r\nstat->state = ISPSTAT_DISABLED;\r\nelse if (stat->state == ISPSTAT_ENABLING && pcr_enable)\r\nstat->state = ISPSTAT_ENABLED;\r\n}\r\nvoid omap3isp_stat_suspend(struct ispstat *stat)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&stat->isp->stat_lock, flags);\r\nif (stat->state != ISPSTAT_DISABLED)\r\nstat->ops->enable(stat, 0);\r\nif (stat->state == ISPSTAT_ENABLED)\r\nstat->state = ISPSTAT_SUSPENDED;\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, flags);\r\n}\r\nvoid omap3isp_stat_resume(struct ispstat *stat)\r\n{\r\nif (stat->state == ISPSTAT_SUSPENDED)\r\nstat->state = ISPSTAT_ENABLING;\r\n}\r\nstatic void isp_stat_try_enable(struct ispstat *stat)\r\n{\r\nunsigned long irqflags;\r\nif (stat->priv == NULL)\r\nreturn;\r\nspin_lock_irqsave(&stat->isp->stat_lock, irqflags);\r\nif (stat->state == ISPSTAT_ENABLING && !stat->buf_processing &&\r\nstat->buf_alloc_size) {\r\nstat->update = 1;\r\nisp_stat_buf_next(stat);\r\nstat->ops->setup_regs(stat, stat->priv);\r\nisp_stat_buf_insert_magic(stat, stat->active_buf);\r\nif (!IS_H3A(stat))\r\natomic_set(&stat->buf_err, 0);\r\nisp_stat_pcr_enable(stat, 1);\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\ndev_dbg(stat->isp->dev, "%s: module is enabled.\n",\r\nstat->subdev.name);\r\n} else {\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\n}\r\n}\r\nvoid omap3isp_stat_isr_frame_sync(struct ispstat *stat)\r\n{\r\nisp_stat_try_enable(stat);\r\n}\r\nvoid omap3isp_stat_sbl_overflow(struct ispstat *stat)\r\n{\r\nunsigned long irqflags;\r\nspin_lock_irqsave(&stat->isp->stat_lock, irqflags);\r\natomic_set(&stat->buf_err, 2);\r\nif (stat->recover_priv)\r\nstat->sbl_ovl_recover = 1;\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\n}\r\nint omap3isp_stat_enable(struct ispstat *stat, u8 enable)\r\n{\r\nunsigned long irqflags;\r\ndev_dbg(stat->isp->dev, "%s: user wants to %s module.\n",\r\nstat->subdev.name, enable ? "enable" : "disable");\r\nmutex_lock(&stat->ioctl_lock);\r\nspin_lock_irqsave(&stat->isp->stat_lock, irqflags);\r\nif (!stat->configured && enable) {\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\nmutex_unlock(&stat->ioctl_lock);\r\ndev_dbg(stat->isp->dev, "%s: cannot enable module as it's "\r\n"never been successfully configured so far.\n",\r\nstat->subdev.name);\r\nreturn -EINVAL;\r\n}\r\nif (enable) {\r\nif (stat->state == ISPSTAT_DISABLING)\r\nstat->state = ISPSTAT_ENABLED;\r\nelse if (stat->state == ISPSTAT_DISABLED)\r\nstat->state = ISPSTAT_ENABLING;\r\n} else {\r\nif (stat->state == ISPSTAT_ENABLING) {\r\nstat->state = ISPSTAT_DISABLED;\r\n} else if (stat->state == ISPSTAT_ENABLED) {\r\nstat->state = ISPSTAT_DISABLING;\r\nisp_stat_buf_clear(stat);\r\n}\r\n}\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\nmutex_unlock(&stat->ioctl_lock);\r\nreturn 0;\r\n}\r\nint omap3isp_stat_s_stream(struct v4l2_subdev *subdev, int enable)\r\n{\r\nstruct ispstat *stat = v4l2_get_subdevdata(subdev);\r\nif (enable) {\r\nisp_stat_try_enable(stat);\r\n} else {\r\nunsigned long flags;\r\nomap3isp_stat_enable(stat, 0);\r\nspin_lock_irqsave(&stat->isp->stat_lock, flags);\r\nstat->ops->enable(stat, 0);\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, flags);\r\nif (!omap3isp_stat_pcr_busy(stat))\r\nomap3isp_stat_isr(stat);\r\ndev_dbg(stat->isp->dev, "%s: module is being disabled\n",\r\nstat->subdev.name);\r\n}\r\nreturn 0;\r\n}\r\nstatic void __stat_isr(struct ispstat *stat, int from_dma)\r\n{\r\nint ret = STAT_BUF_DONE;\r\nint buf_processing;\r\nunsigned long irqflags;\r\nstruct isp_pipeline *pipe;\r\nspin_lock_irqsave(&stat->isp->stat_lock, irqflags);\r\nif (stat->state == ISPSTAT_DISABLED) {\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\nreturn;\r\n}\r\nbuf_processing = stat->buf_processing;\r\nstat->buf_processing = 1;\r\nstat->ops->enable(stat, 0);\r\nif (buf_processing && !from_dma) {\r\nif (stat->state == ISPSTAT_ENABLED) {\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\ndev_err(stat->isp->dev,\r\n"%s: interrupt occurred when module was still "\r\n"processing a buffer.\n", stat->subdev.name);\r\nret = STAT_NO_BUF;\r\ngoto out;\r\n} else {\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\nreturn;\r\n}\r\n}\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\nif (!omap3isp_stat_pcr_busy(stat)) {\r\nif (!from_dma && stat->ops->buf_process)\r\nret = stat->ops->buf_process(stat);\r\nif (ret == STAT_BUF_WAITING_DMA)\r\nreturn;\r\nspin_lock_irqsave(&stat->isp->stat_lock, irqflags);\r\nif (stat->state == ISPSTAT_DISABLING) {\r\nstat->state = ISPSTAT_DISABLED;\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\nstat->buf_processing = 0;\r\nreturn;\r\n}\r\npipe = to_isp_pipeline(&stat->subdev.entity);\r\nstat->frame_number = atomic_read(&pipe->frame_number);\r\nret = isp_stat_buf_process(stat, ret);\r\nif (likely(!stat->sbl_ovl_recover)) {\r\nstat->ops->setup_regs(stat, stat->priv);\r\n} else {\r\nstat->update = 1;\r\nstat->ops->setup_regs(stat, stat->recover_priv);\r\nstat->sbl_ovl_recover = 0;\r\nstat->update = 1;\r\n}\r\nisp_stat_buf_insert_magic(stat, stat->active_buf);\r\nisp_stat_pcr_enable(stat, 1);\r\nspin_unlock_irqrestore(&stat->isp->stat_lock, irqflags);\r\n} else {\r\nif (stat->ops->buf_process)\r\natomic_set(&stat->buf_err, 1);\r\nret = STAT_NO_BUF;\r\ndev_dbg(stat->isp->dev, "%s: cannot process buffer, "\r\n"device is busy.\n", stat->subdev.name);\r\n}\r\nout:\r\nstat->buf_processing = 0;\r\nisp_stat_queue_event(stat, ret != STAT_BUF_DONE);\r\n}\r\nvoid omap3isp_stat_isr(struct ispstat *stat)\r\n{\r\n__stat_isr(stat, 0);\r\n}\r\nvoid omap3isp_stat_dma_isr(struct ispstat *stat)\r\n{\r\n__stat_isr(stat, 1);\r\n}\r\nint omap3isp_stat_subscribe_event(struct v4l2_subdev *subdev,\r\nstruct v4l2_fh *fh,\r\nstruct v4l2_event_subscription *sub)\r\n{\r\nstruct ispstat *stat = v4l2_get_subdevdata(subdev);\r\nif (sub->type != stat->event_type)\r\nreturn -EINVAL;\r\nreturn v4l2_event_subscribe(fh, sub, STAT_NEVENTS, NULL);\r\n}\r\nint omap3isp_stat_unsubscribe_event(struct v4l2_subdev *subdev,\r\nstruct v4l2_fh *fh,\r\nstruct v4l2_event_subscription *sub)\r\n{\r\nreturn v4l2_event_unsubscribe(fh, sub);\r\n}\r\nvoid omap3isp_stat_unregister_entities(struct ispstat *stat)\r\n{\r\nv4l2_device_unregister_subdev(&stat->subdev);\r\n}\r\nint omap3isp_stat_register_entities(struct ispstat *stat,\r\nstruct v4l2_device *vdev)\r\n{\r\nreturn v4l2_device_register_subdev(vdev, &stat->subdev);\r\n}\r\nstatic int isp_stat_init_entities(struct ispstat *stat, const char *name,\r\nconst struct v4l2_subdev_ops *sd_ops)\r\n{\r\nstruct v4l2_subdev *subdev = &stat->subdev;\r\nstruct media_entity *me = &subdev->entity;\r\nv4l2_subdev_init(subdev, sd_ops);\r\nsnprintf(subdev->name, V4L2_SUBDEV_NAME_SIZE, "OMAP3 ISP %s", name);\r\nsubdev->grp_id = 1 << 16;\r\nsubdev->flags |= V4L2_SUBDEV_FL_HAS_EVENTS | V4L2_SUBDEV_FL_HAS_DEVNODE;\r\nv4l2_set_subdevdata(subdev, stat);\r\nstat->pad.flags = MEDIA_PAD_FL_SINK | MEDIA_PAD_FL_MUST_CONNECT;\r\nme->ops = NULL;\r\nreturn media_entity_init(me, 1, &stat->pad, 0);\r\n}\r\nint omap3isp_stat_init(struct ispstat *stat, const char *name,\r\nconst struct v4l2_subdev_ops *sd_ops)\r\n{\r\nint ret;\r\nstat->buf = kcalloc(STAT_MAX_BUFS, sizeof(*stat->buf), GFP_KERNEL);\r\nif (!stat->buf)\r\nreturn -ENOMEM;\r\nisp_stat_buf_clear(stat);\r\nmutex_init(&stat->ioctl_lock);\r\natomic_set(&stat->buf_err, 0);\r\nret = isp_stat_init_entities(stat, name, sd_ops);\r\nif (ret < 0) {\r\nmutex_destroy(&stat->ioctl_lock);\r\nkfree(stat->buf);\r\n}\r\nreturn ret;\r\n}\r\nvoid omap3isp_stat_cleanup(struct ispstat *stat)\r\n{\r\nmedia_entity_cleanup(&stat->subdev.entity);\r\nmutex_destroy(&stat->ioctl_lock);\r\nisp_stat_bufs_free(stat);\r\nkfree(stat->buf);\r\n}
