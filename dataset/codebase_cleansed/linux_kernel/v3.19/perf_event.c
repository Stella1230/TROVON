static unsigned long perf_event_encode(const struct perf_event_map *pmap)\r\n{\r\nreturn ((unsigned long) pmap->encoding << 16) | pmap->pic_mask;\r\n}\r\nstatic u8 perf_event_get_msk(unsigned long val)\r\n{\r\nreturn val & 0xff;\r\n}\r\nstatic u64 perf_event_get_enc(unsigned long val)\r\n{\r\nreturn val >> 16;\r\n}\r\nstatic u32 sparc_default_read_pmc(int idx)\r\n{\r\nu64 val;\r\nval = pcr_ops->read_pic(0);\r\nif (idx == PIC_UPPER_INDEX)\r\nval >>= 32;\r\nreturn val & 0xffffffff;\r\n}\r\nstatic void sparc_default_write_pmc(int idx, u64 val)\r\n{\r\nu64 shift, mask, pic;\r\nshift = 0;\r\nif (idx == PIC_UPPER_INDEX)\r\nshift = 32;\r\nmask = ((u64) 0xffffffff) << shift;\r\nval <<= shift;\r\npic = pcr_ops->read_pic(0);\r\npic &= ~mask;\r\npic |= val;\r\npcr_ops->write_pic(0, pic);\r\n}\r\nstatic const struct perf_event_map *ultra3_event_map(int event_id)\r\n{\r\nreturn &ultra3_perfmon_event_map[event_id];\r\n}\r\nstatic const struct perf_event_map *niagara1_event_map(int event_id)\r\n{\r\nreturn &niagara1_perfmon_event_map[event_id];\r\n}\r\nstatic const struct perf_event_map *niagara2_event_map(int event_id)\r\n{\r\nreturn &niagara2_perfmon_event_map[event_id];\r\n}\r\nstatic const struct perf_event_map *niagara4_event_map(int event_id)\r\n{\r\nreturn &niagara4_perfmon_event_map[event_id];\r\n}\r\nstatic u32 sparc_vt_read_pmc(int idx)\r\n{\r\nu64 val = pcr_ops->read_pic(idx);\r\nreturn val & 0xffffffff;\r\n}\r\nstatic void sparc_vt_write_pmc(int idx, u64 val)\r\n{\r\nu64 pcr;\r\npcr = pcr_ops->read_pcr(idx);\r\npcr_ops->write_pcr(idx, PCR_N4_PICNPT);\r\npcr_ops->write_pic(idx, val & 0xffffffff);\r\npcr_ops->write_pcr(idx, pcr);\r\n}\r\nstatic u64 event_encoding(u64 event_id, int idx)\r\n{\r\nif (idx == PIC_UPPER_INDEX)\r\nevent_id <<= sparc_pmu->upper_shift;\r\nelse\r\nevent_id <<= sparc_pmu->lower_shift;\r\nreturn event_id;\r\n}\r\nstatic u64 mask_for_index(int idx)\r\n{\r\nreturn event_encoding(sparc_pmu->event_mask, idx);\r\n}\r\nstatic u64 nop_for_index(int idx)\r\n{\r\nreturn event_encoding(idx == PIC_UPPER_INDEX ?\r\nsparc_pmu->upper_nop :\r\nsparc_pmu->lower_nop, idx);\r\n}\r\nstatic inline void sparc_pmu_enable_event(struct cpu_hw_events *cpuc, struct hw_perf_event *hwc, int idx)\r\n{\r\nu64 enc, val, mask = mask_for_index(idx);\r\nint pcr_index = 0;\r\nif (sparc_pmu->num_pcrs > 1)\r\npcr_index = idx;\r\nenc = perf_event_get_enc(cpuc->events[idx]);\r\nval = cpuc->pcr[pcr_index];\r\nval &= ~mask;\r\nval |= event_encoding(enc, idx);\r\ncpuc->pcr[pcr_index] = val;\r\npcr_ops->write_pcr(pcr_index, cpuc->pcr[pcr_index]);\r\n}\r\nstatic inline void sparc_pmu_disable_event(struct cpu_hw_events *cpuc, struct hw_perf_event *hwc, int idx)\r\n{\r\nu64 mask = mask_for_index(idx);\r\nu64 nop = nop_for_index(idx);\r\nint pcr_index = 0;\r\nu64 val;\r\nif (sparc_pmu->num_pcrs > 1)\r\npcr_index = idx;\r\nval = cpuc->pcr[pcr_index];\r\nval &= ~mask;\r\nval |= nop;\r\ncpuc->pcr[pcr_index] = val;\r\npcr_ops->write_pcr(pcr_index, cpuc->pcr[pcr_index]);\r\n}\r\nstatic u64 sparc_perf_event_update(struct perf_event *event,\r\nstruct hw_perf_event *hwc, int idx)\r\n{\r\nint shift = 64 - 32;\r\nu64 prev_raw_count, new_raw_count;\r\ns64 delta;\r\nagain:\r\nprev_raw_count = local64_read(&hwc->prev_count);\r\nnew_raw_count = sparc_pmu->read_pmc(idx);\r\nif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\r\nnew_raw_count) != prev_raw_count)\r\ngoto again;\r\ndelta = (new_raw_count << shift) - (prev_raw_count << shift);\r\ndelta >>= shift;\r\nlocal64_add(delta, &event->count);\r\nlocal64_sub(delta, &hwc->period_left);\r\nreturn new_raw_count;\r\n}\r\nstatic int sparc_perf_event_set_period(struct perf_event *event,\r\nstruct hw_perf_event *hwc, int idx)\r\n{\r\ns64 left = local64_read(&hwc->period_left);\r\ns64 period = hwc->sample_period;\r\nint ret = 0;\r\nif (unlikely(left <= -period)) {\r\nleft = period;\r\nlocal64_set(&hwc->period_left, left);\r\nhwc->last_period = period;\r\nret = 1;\r\n}\r\nif (unlikely(left <= 0)) {\r\nleft += period;\r\nlocal64_set(&hwc->period_left, left);\r\nhwc->last_period = period;\r\nret = 1;\r\n}\r\nif (left > MAX_PERIOD)\r\nleft = MAX_PERIOD;\r\nlocal64_set(&hwc->prev_count, (u64)-left);\r\nsparc_pmu->write_pmc(idx, (u64)(-left) & 0xffffffff);\r\nperf_event_update_userpage(event);\r\nreturn ret;\r\n}\r\nstatic void read_in_all_counters(struct cpu_hw_events *cpuc)\r\n{\r\nint i;\r\nfor (i = 0; i < cpuc->n_events; i++) {\r\nstruct perf_event *cp = cpuc->event[i];\r\nif (cpuc->current_idx[i] != PIC_NO_INDEX &&\r\ncpuc->current_idx[i] != cp->hw.idx) {\r\nsparc_perf_event_update(cp, &cp->hw,\r\ncpuc->current_idx[i]);\r\ncpuc->current_idx[i] = PIC_NO_INDEX;\r\n}\r\n}\r\n}\r\nstatic void calculate_single_pcr(struct cpu_hw_events *cpuc)\r\n{\r\nint i;\r\nif (!cpuc->n_added)\r\ngoto out;\r\nfor (i = 0; i < cpuc->n_events; i++) {\r\nstruct perf_event *cp = cpuc->event[i];\r\nstruct hw_perf_event *hwc = &cp->hw;\r\nint idx = hwc->idx;\r\nu64 enc;\r\nif (cpuc->current_idx[i] != PIC_NO_INDEX)\r\ncontinue;\r\nsparc_perf_event_set_period(cp, hwc, idx);\r\ncpuc->current_idx[i] = idx;\r\nenc = perf_event_get_enc(cpuc->events[i]);\r\ncpuc->pcr[0] &= ~mask_for_index(idx);\r\nif (hwc->state & PERF_HES_STOPPED)\r\ncpuc->pcr[0] |= nop_for_index(idx);\r\nelse\r\ncpuc->pcr[0] |= event_encoding(enc, idx);\r\n}\r\nout:\r\ncpuc->pcr[0] |= cpuc->event[0]->hw.config_base;\r\n}\r\nstatic void calculate_multiple_pcrs(struct cpu_hw_events *cpuc)\r\n{\r\nint i;\r\nif (!cpuc->n_added)\r\ngoto out;\r\nfor (i = 0; i < cpuc->n_events; i++) {\r\nstruct perf_event *cp = cpuc->event[i];\r\nstruct hw_perf_event *hwc = &cp->hw;\r\nint idx = hwc->idx;\r\nu64 enc;\r\nif (cpuc->current_idx[i] != PIC_NO_INDEX)\r\ncontinue;\r\nsparc_perf_event_set_period(cp, hwc, idx);\r\ncpuc->current_idx[i] = idx;\r\nenc = perf_event_get_enc(cpuc->events[i]);\r\ncpuc->pcr[idx] &= ~mask_for_index(idx);\r\nif (hwc->state & PERF_HES_STOPPED)\r\ncpuc->pcr[idx] |= nop_for_index(idx);\r\nelse\r\ncpuc->pcr[idx] |= event_encoding(enc, idx);\r\n}\r\nout:\r\nfor (i = 0; i < cpuc->n_events; i++) {\r\nstruct perf_event *cp = cpuc->event[i];\r\nint idx = cp->hw.idx;\r\ncpuc->pcr[idx] |= cp->hw.config_base;\r\n}\r\n}\r\nstatic void update_pcrs_for_enable(struct cpu_hw_events *cpuc)\r\n{\r\nif (cpuc->n_added)\r\nread_in_all_counters(cpuc);\r\nif (sparc_pmu->num_pcrs == 1) {\r\ncalculate_single_pcr(cpuc);\r\n} else {\r\ncalculate_multiple_pcrs(cpuc);\r\n}\r\n}\r\nstatic void sparc_pmu_enable(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nint i;\r\nif (cpuc->enabled)\r\nreturn;\r\ncpuc->enabled = 1;\r\nbarrier();\r\nif (cpuc->n_events)\r\nupdate_pcrs_for_enable(cpuc);\r\nfor (i = 0; i < sparc_pmu->num_pcrs; i++)\r\npcr_ops->write_pcr(i, cpuc->pcr[i]);\r\n}\r\nstatic void sparc_pmu_disable(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nint i;\r\nif (!cpuc->enabled)\r\nreturn;\r\ncpuc->enabled = 0;\r\ncpuc->n_added = 0;\r\nfor (i = 0; i < sparc_pmu->num_pcrs; i++) {\r\nu64 val = cpuc->pcr[i];\r\nval &= ~(sparc_pmu->user_bit | sparc_pmu->priv_bit |\r\nsparc_pmu->hv_bit | sparc_pmu->irq_bit);\r\ncpuc->pcr[i] = val;\r\npcr_ops->write_pcr(i, cpuc->pcr[i]);\r\n}\r\n}\r\nstatic int active_event_index(struct cpu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nint i;\r\nfor (i = 0; i < cpuc->n_events; i++) {\r\nif (cpuc->event[i] == event)\r\nbreak;\r\n}\r\nBUG_ON(i == cpuc->n_events);\r\nreturn cpuc->current_idx[i];\r\n}\r\nstatic void sparc_pmu_start(struct perf_event *event, int flags)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nint idx = active_event_index(cpuc, event);\r\nif (flags & PERF_EF_RELOAD) {\r\nWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\r\nsparc_perf_event_set_period(event, &event->hw, idx);\r\n}\r\nevent->hw.state = 0;\r\nsparc_pmu_enable_event(cpuc, &event->hw, idx);\r\n}\r\nstatic void sparc_pmu_stop(struct perf_event *event, int flags)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nint idx = active_event_index(cpuc, event);\r\nif (!(event->hw.state & PERF_HES_STOPPED)) {\r\nsparc_pmu_disable_event(cpuc, &event->hw, idx);\r\nevent->hw.state |= PERF_HES_STOPPED;\r\n}\r\nif (!(event->hw.state & PERF_HES_UPTODATE) && (flags & PERF_EF_UPDATE)) {\r\nsparc_perf_event_update(event, &event->hw, idx);\r\nevent->hw.state |= PERF_HES_UPTODATE;\r\n}\r\n}\r\nstatic void sparc_pmu_del(struct perf_event *event, int _flags)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nunsigned long flags;\r\nint i;\r\nlocal_irq_save(flags);\r\nperf_pmu_disable(event->pmu);\r\nfor (i = 0; i < cpuc->n_events; i++) {\r\nif (event == cpuc->event[i]) {\r\nsparc_pmu_stop(event, PERF_EF_UPDATE);\r\nwhile (++i < cpuc->n_events) {\r\ncpuc->event[i - 1] = cpuc->event[i];\r\ncpuc->events[i - 1] = cpuc->events[i];\r\ncpuc->current_idx[i - 1] =\r\ncpuc->current_idx[i];\r\n}\r\nperf_event_update_userpage(event);\r\ncpuc->n_events--;\r\nbreak;\r\n}\r\n}\r\nperf_pmu_enable(event->pmu);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void sparc_pmu_read(struct perf_event *event)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nint idx = active_event_index(cpuc, event);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nsparc_perf_event_update(event, hwc, idx);\r\n}\r\nstatic void perf_stop_nmi_watchdog(void *unused)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nint i;\r\nstop_nmi_watchdog(NULL);\r\nfor (i = 0; i < sparc_pmu->num_pcrs; i++)\r\ncpuc->pcr[i] = pcr_ops->read_pcr(i);\r\n}\r\nstatic void perf_event_grab_pmc(void)\r\n{\r\nif (atomic_inc_not_zero(&active_events))\r\nreturn;\r\nmutex_lock(&pmc_grab_mutex);\r\nif (atomic_read(&active_events) == 0) {\r\nif (atomic_read(&nmi_active) > 0) {\r\non_each_cpu(perf_stop_nmi_watchdog, NULL, 1);\r\nBUG_ON(atomic_read(&nmi_active) != 0);\r\n}\r\natomic_inc(&active_events);\r\n}\r\nmutex_unlock(&pmc_grab_mutex);\r\n}\r\nstatic void perf_event_release_pmc(void)\r\n{\r\nif (atomic_dec_and_mutex_lock(&active_events, &pmc_grab_mutex)) {\r\nif (atomic_read(&nmi_active) == 0)\r\non_each_cpu(start_nmi_watchdog, NULL, 1);\r\nmutex_unlock(&pmc_grab_mutex);\r\n}\r\n}\r\nstatic const struct perf_event_map *sparc_map_cache_event(u64 config)\r\n{\r\nunsigned int cache_type, cache_op, cache_result;\r\nconst struct perf_event_map *pmap;\r\nif (!sparc_pmu->cache_map)\r\nreturn ERR_PTR(-ENOENT);\r\ncache_type = (config >> 0) & 0xff;\r\nif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\r\nreturn ERR_PTR(-EINVAL);\r\ncache_op = (config >> 8) & 0xff;\r\nif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\r\nreturn ERR_PTR(-EINVAL);\r\ncache_result = (config >> 16) & 0xff;\r\nif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\r\nreturn ERR_PTR(-EINVAL);\r\npmap = &((*sparc_pmu->cache_map)[cache_type][cache_op][cache_result]);\r\nif (pmap->encoding == CACHE_OP_UNSUPPORTED)\r\nreturn ERR_PTR(-ENOENT);\r\nif (pmap->encoding == CACHE_OP_NONSENSE)\r\nreturn ERR_PTR(-EINVAL);\r\nreturn pmap;\r\n}\r\nstatic void hw_perf_event_destroy(struct perf_event *event)\r\n{\r\nperf_event_release_pmc();\r\n}\r\nstatic int sparc_check_constraints(struct perf_event **evts,\r\nunsigned long *events, int n_ev)\r\n{\r\nu8 msk0 = 0, msk1 = 0;\r\nint idx0 = 0;\r\nif (!n_ev)\r\nreturn 0;\r\nif (n_ev > sparc_pmu->max_hw_events)\r\nreturn -1;\r\nif (!(sparc_pmu->flags & SPARC_PMU_HAS_CONFLICTS)) {\r\nint i;\r\nfor (i = 0; i < n_ev; i++)\r\nevts[i]->hw.idx = i;\r\nreturn 0;\r\n}\r\nmsk0 = perf_event_get_msk(events[0]);\r\nif (n_ev == 1) {\r\nif (msk0 & PIC_LOWER)\r\nidx0 = 1;\r\ngoto success;\r\n}\r\nBUG_ON(n_ev != 2);\r\nmsk1 = perf_event_get_msk(events[1]);\r\nif (msk0 == (PIC_UPPER | PIC_LOWER) &&\r\nmsk1 == (PIC_UPPER | PIC_LOWER))\r\ngoto success;\r\nif ((msk0 == PIC_UPPER || msk0 == PIC_LOWER) &&\r\nmsk1 == (PIC_UPPER | PIC_LOWER)) {\r\nif (msk0 & PIC_LOWER)\r\nidx0 = 1;\r\ngoto success;\r\n}\r\nif ((msk1 == PIC_UPPER || msk1 == PIC_LOWER) &&\r\nmsk0 == (PIC_UPPER | PIC_LOWER)) {\r\nif (msk1 & PIC_UPPER)\r\nidx0 = 1;\r\ngoto success;\r\n}\r\nif ((msk0 == PIC_UPPER && msk1 == PIC_LOWER) ||\r\n(msk0 == PIC_LOWER && msk1 == PIC_UPPER)) {\r\nif (msk0 & PIC_LOWER)\r\nidx0 = 1;\r\ngoto success;\r\n}\r\nreturn -1;\r\nsuccess:\r\nevts[0]->hw.idx = idx0;\r\nif (n_ev == 2)\r\nevts[1]->hw.idx = idx0 ^ 1;\r\nreturn 0;\r\n}\r\nstatic int check_excludes(struct perf_event **evts, int n_prev, int n_new)\r\n{\r\nint eu = 0, ek = 0, eh = 0;\r\nstruct perf_event *event;\r\nint i, n, first;\r\nif (!(sparc_pmu->flags & SPARC_PMU_ALL_EXCLUDES_SAME))\r\nreturn 0;\r\nn = n_prev + n_new;\r\nif (n <= 1)\r\nreturn 0;\r\nfirst = 1;\r\nfor (i = 0; i < n; i++) {\r\nevent = evts[i];\r\nif (first) {\r\neu = event->attr.exclude_user;\r\nek = event->attr.exclude_kernel;\r\neh = event->attr.exclude_hv;\r\nfirst = 0;\r\n} else if (event->attr.exclude_user != eu ||\r\nevent->attr.exclude_kernel != ek ||\r\nevent->attr.exclude_hv != eh) {\r\nreturn -EAGAIN;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int collect_events(struct perf_event *group, int max_count,\r\nstruct perf_event *evts[], unsigned long *events,\r\nint *current_idx)\r\n{\r\nstruct perf_event *event;\r\nint n = 0;\r\nif (!is_software_event(group)) {\r\nif (n >= max_count)\r\nreturn -1;\r\nevts[n] = group;\r\nevents[n] = group->hw.event_base;\r\ncurrent_idx[n++] = PIC_NO_INDEX;\r\n}\r\nlist_for_each_entry(event, &group->sibling_list, group_entry) {\r\nif (!is_software_event(event) &&\r\nevent->state != PERF_EVENT_STATE_OFF) {\r\nif (n >= max_count)\r\nreturn -1;\r\nevts[n] = event;\r\nevents[n] = event->hw.event_base;\r\ncurrent_idx[n++] = PIC_NO_INDEX;\r\n}\r\n}\r\nreturn n;\r\n}\r\nstatic int sparc_pmu_add(struct perf_event *event, int ef_flags)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nint n0, ret = -EAGAIN;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nperf_pmu_disable(event->pmu);\r\nn0 = cpuc->n_events;\r\nif (n0 >= sparc_pmu->max_hw_events)\r\ngoto out;\r\ncpuc->event[n0] = event;\r\ncpuc->events[n0] = event->hw.event_base;\r\ncpuc->current_idx[n0] = PIC_NO_INDEX;\r\nevent->hw.state = PERF_HES_UPTODATE;\r\nif (!(ef_flags & PERF_EF_START))\r\nevent->hw.state |= PERF_HES_STOPPED;\r\nif (cpuc->group_flag & PERF_EVENT_TXN)\r\ngoto nocheck;\r\nif (check_excludes(cpuc->event, n0, 1))\r\ngoto out;\r\nif (sparc_check_constraints(cpuc->event, cpuc->events, n0 + 1))\r\ngoto out;\r\nnocheck:\r\ncpuc->n_events++;\r\ncpuc->n_added++;\r\nret = 0;\r\nout:\r\nperf_pmu_enable(event->pmu);\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nstatic int sparc_pmu_event_init(struct perf_event *event)\r\n{\r\nstruct perf_event_attr *attr = &event->attr;\r\nstruct perf_event *evts[MAX_HWEVENTS];\r\nstruct hw_perf_event *hwc = &event->hw;\r\nunsigned long events[MAX_HWEVENTS];\r\nint current_idx_dmy[MAX_HWEVENTS];\r\nconst struct perf_event_map *pmap;\r\nint n;\r\nif (atomic_read(&nmi_active) < 0)\r\nreturn -ENODEV;\r\nif (has_branch_stack(event))\r\nreturn -EOPNOTSUPP;\r\nswitch (attr->type) {\r\ncase PERF_TYPE_HARDWARE:\r\nif (attr->config >= sparc_pmu->max_events)\r\nreturn -EINVAL;\r\npmap = sparc_pmu->event_map(attr->config);\r\nbreak;\r\ncase PERF_TYPE_HW_CACHE:\r\npmap = sparc_map_cache_event(attr->config);\r\nif (IS_ERR(pmap))\r\nreturn PTR_ERR(pmap);\r\nbreak;\r\ncase PERF_TYPE_RAW:\r\npmap = NULL;\r\nbreak;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\nif (pmap) {\r\nhwc->event_base = perf_event_encode(pmap);\r\n} else {\r\nhwc->event_base = attr->config;\r\n}\r\nhwc->config_base = sparc_pmu->irq_bit;\r\nif (!attr->exclude_user)\r\nhwc->config_base |= sparc_pmu->user_bit;\r\nif (!attr->exclude_kernel)\r\nhwc->config_base |= sparc_pmu->priv_bit;\r\nif (!attr->exclude_hv)\r\nhwc->config_base |= sparc_pmu->hv_bit;\r\nn = 0;\r\nif (event->group_leader != event) {\r\nn = collect_events(event->group_leader,\r\nsparc_pmu->max_hw_events - 1,\r\nevts, events, current_idx_dmy);\r\nif (n < 0)\r\nreturn -EINVAL;\r\n}\r\nevents[n] = hwc->event_base;\r\nevts[n] = event;\r\nif (check_excludes(evts, n, 1))\r\nreturn -EINVAL;\r\nif (sparc_check_constraints(evts, events, n + 1))\r\nreturn -EINVAL;\r\nhwc->idx = PIC_NO_INDEX;\r\nperf_event_grab_pmc();\r\nevent->destroy = hw_perf_event_destroy;\r\nif (!hwc->sample_period) {\r\nhwc->sample_period = MAX_PERIOD;\r\nhwc->last_period = hwc->sample_period;\r\nlocal64_set(&hwc->period_left, hwc->sample_period);\r\n}\r\nreturn 0;\r\n}\r\nstatic void sparc_pmu_start_txn(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\r\nperf_pmu_disable(pmu);\r\ncpuhw->group_flag |= PERF_EVENT_TXN;\r\n}\r\nstatic void sparc_pmu_cancel_txn(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\r\ncpuhw->group_flag &= ~PERF_EVENT_TXN;\r\nperf_pmu_enable(pmu);\r\n}\r\nstatic int sparc_pmu_commit_txn(struct pmu *pmu)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nint n;\r\nif (!sparc_pmu)\r\nreturn -EINVAL;\r\ncpuc = this_cpu_ptr(&cpu_hw_events);\r\nn = cpuc->n_events;\r\nif (check_excludes(cpuc->event, 0, n))\r\nreturn -EINVAL;\r\nif (sparc_check_constraints(cpuc->event, cpuc->events, n))\r\nreturn -EAGAIN;\r\ncpuc->group_flag &= ~PERF_EVENT_TXN;\r\nperf_pmu_enable(pmu);\r\nreturn 0;\r\n}\r\nvoid perf_event_print_debug(void)\r\n{\r\nunsigned long flags;\r\nint cpu, i;\r\nif (!sparc_pmu)\r\nreturn;\r\nlocal_irq_save(flags);\r\ncpu = smp_processor_id();\r\npr_info("\n");\r\nfor (i = 0; i < sparc_pmu->num_pcrs; i++)\r\npr_info("CPU#%d: PCR%d[%016llx]\n",\r\ncpu, i, pcr_ops->read_pcr(i));\r\nfor (i = 0; i < sparc_pmu->num_pic_regs; i++)\r\npr_info("CPU#%d: PIC%d[%016llx]\n",\r\ncpu, i, pcr_ops->read_pic(i));\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int __kprobes perf_event_nmi_handler(struct notifier_block *self,\r\nunsigned long cmd, void *__args)\r\n{\r\nstruct die_args *args = __args;\r\nstruct perf_sample_data data;\r\nstruct cpu_hw_events *cpuc;\r\nstruct pt_regs *regs;\r\nint i;\r\nif (!atomic_read(&active_events))\r\nreturn NOTIFY_DONE;\r\nswitch (cmd) {\r\ncase DIE_NMI:\r\nbreak;\r\ndefault:\r\nreturn NOTIFY_DONE;\r\n}\r\nregs = args->regs;\r\ncpuc = this_cpu_ptr(&cpu_hw_events);\r\nif (sparc_pmu->irq_bit &&\r\nsparc_pmu->num_pcrs == 1)\r\npcr_ops->write_pcr(0, cpuc->pcr[0]);\r\nfor (i = 0; i < cpuc->n_events; i++) {\r\nstruct perf_event *event = cpuc->event[i];\r\nint idx = cpuc->current_idx[i];\r\nstruct hw_perf_event *hwc;\r\nu64 val;\r\nif (sparc_pmu->irq_bit &&\r\nsparc_pmu->num_pcrs > 1)\r\npcr_ops->write_pcr(idx, cpuc->pcr[idx]);\r\nhwc = &event->hw;\r\nval = sparc_perf_event_update(event, hwc, idx);\r\nif (val & (1ULL << 31))\r\ncontinue;\r\nperf_sample_data_init(&data, 0, hwc->last_period);\r\nif (!sparc_perf_event_set_period(event, hwc, idx))\r\ncontinue;\r\nif (perf_event_overflow(event, &data, regs))\r\nsparc_pmu_stop(event, 0);\r\n}\r\nreturn NOTIFY_STOP;\r\n}\r\nstatic bool __init supported_pmu(void)\r\n{\r\nif (!strcmp(sparc_pmu_type, "ultra3") ||\r\n!strcmp(sparc_pmu_type, "ultra3+") ||\r\n!strcmp(sparc_pmu_type, "ultra3i") ||\r\n!strcmp(sparc_pmu_type, "ultra4+")) {\r\nsparc_pmu = &ultra3_pmu;\r\nreturn true;\r\n}\r\nif (!strcmp(sparc_pmu_type, "niagara")) {\r\nsparc_pmu = &niagara1_pmu;\r\nreturn true;\r\n}\r\nif (!strcmp(sparc_pmu_type, "niagara2") ||\r\n!strcmp(sparc_pmu_type, "niagara3")) {\r\nsparc_pmu = &niagara2_pmu;\r\nreturn true;\r\n}\r\nif (!strcmp(sparc_pmu_type, "niagara4") ||\r\n!strcmp(sparc_pmu_type, "niagara5")) {\r\nsparc_pmu = &niagara4_pmu;\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int __init init_hw_perf_events(void)\r\n{\r\nint err;\r\npr_info("Performance events: ");\r\nerr = pcr_arch_init();\r\nif (err || !supported_pmu()) {\r\npr_cont("No support for PMU type '%s'\n", sparc_pmu_type);\r\nreturn 0;\r\n}\r\npr_cont("Supported PMU type is '%s'\n", sparc_pmu_type);\r\nperf_pmu_register(&pmu, "cpu", PERF_TYPE_RAW);\r\nregister_die_notifier(&perf_event_nmi_notifier);\r\nreturn 0;\r\n}\r\nvoid perf_callchain_kernel(struct perf_callchain_entry *entry,\r\nstruct pt_regs *regs)\r\n{\r\nunsigned long ksp, fp;\r\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\r\nint graph = 0;\r\n#endif\r\nstack_trace_flush();\r\nperf_callchain_store(entry, regs->tpc);\r\nksp = regs->u_regs[UREG_I6];\r\nfp = ksp + STACK_BIAS;\r\ndo {\r\nstruct sparc_stackf *sf;\r\nstruct pt_regs *regs;\r\nunsigned long pc;\r\nif (!kstack_valid(current_thread_info(), fp))\r\nbreak;\r\nsf = (struct sparc_stackf *) fp;\r\nregs = (struct pt_regs *) (sf + 1);\r\nif (kstack_is_trap_frame(current_thread_info(), regs)) {\r\nif (user_mode(regs))\r\nbreak;\r\npc = regs->tpc;\r\nfp = regs->u_regs[UREG_I6] + STACK_BIAS;\r\n} else {\r\npc = sf->callers_pc;\r\nfp = (unsigned long)sf->fp + STACK_BIAS;\r\n}\r\nperf_callchain_store(entry, pc);\r\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\r\nif ((pc + 8UL) == (unsigned long) &return_to_handler) {\r\nint index = current->curr_ret_stack;\r\nif (current->ret_stack && index >= graph) {\r\npc = current->ret_stack[index - graph].ret;\r\nperf_callchain_store(entry, pc);\r\ngraph++;\r\n}\r\n}\r\n#endif\r\n} while (entry->nr < PERF_MAX_STACK_DEPTH);\r\n}\r\nstatic void perf_callchain_user_64(struct perf_callchain_entry *entry,\r\nstruct pt_regs *regs)\r\n{\r\nunsigned long ufp;\r\nufp = regs->u_regs[UREG_I6] + STACK_BIAS;\r\ndo {\r\nstruct sparc_stackf __user *usf;\r\nstruct sparc_stackf sf;\r\nunsigned long pc;\r\nusf = (struct sparc_stackf __user *)ufp;\r\nif (__copy_from_user_inatomic(&sf, usf, sizeof(sf)))\r\nbreak;\r\npc = sf.callers_pc;\r\nufp = (unsigned long)sf.fp + STACK_BIAS;\r\nperf_callchain_store(entry, pc);\r\n} while (entry->nr < PERF_MAX_STACK_DEPTH);\r\n}\r\nstatic void perf_callchain_user_32(struct perf_callchain_entry *entry,\r\nstruct pt_regs *regs)\r\n{\r\nunsigned long ufp;\r\nufp = regs->u_regs[UREG_I6] & 0xffffffffUL;\r\ndo {\r\nunsigned long pc;\r\nif (thread32_stack_is_64bit(ufp)) {\r\nstruct sparc_stackf __user *usf;\r\nstruct sparc_stackf sf;\r\nufp += STACK_BIAS;\r\nusf = (struct sparc_stackf __user *)ufp;\r\nif (__copy_from_user_inatomic(&sf, usf, sizeof(sf)))\r\nbreak;\r\npc = sf.callers_pc & 0xffffffff;\r\nufp = ((unsigned long) sf.fp) & 0xffffffff;\r\n} else {\r\nstruct sparc_stackf32 __user *usf;\r\nstruct sparc_stackf32 sf;\r\nusf = (struct sparc_stackf32 __user *)ufp;\r\nif (__copy_from_user_inatomic(&sf, usf, sizeof(sf)))\r\nbreak;\r\npc = sf.callers_pc;\r\nufp = (unsigned long)sf.fp;\r\n}\r\nperf_callchain_store(entry, pc);\r\n} while (entry->nr < PERF_MAX_STACK_DEPTH);\r\n}\r\nvoid\r\nperf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs)\r\n{\r\nperf_callchain_store(entry, regs->tpc);\r\nif (!current->mm)\r\nreturn;\r\nflushw_user();\r\nif (test_thread_flag(TIF_32BIT))\r\nperf_callchain_user_32(entry, regs);\r\nelse\r\nperf_callchain_user_64(entry, regs);\r\n}
