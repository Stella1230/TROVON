static void hex_dump_object(struct seq_file *seq,\r\nstruct kmemleak_object *object)\r\n{\r\nconst u8 *ptr = (const u8 *)object->pointer;\r\nint i, len, remaining;\r\nunsigned char linebuf[HEX_ROW_SIZE * 5];\r\nremaining = len =\r\nmin(object->size, (size_t)(HEX_MAX_LINES * HEX_ROW_SIZE));\r\nseq_printf(seq, " hex dump (first %d bytes):\n", len);\r\nfor (i = 0; i < len; i += HEX_ROW_SIZE) {\r\nint linelen = min(remaining, HEX_ROW_SIZE);\r\nremaining -= HEX_ROW_SIZE;\r\nhex_dump_to_buffer(ptr + i, linelen, HEX_ROW_SIZE,\r\nHEX_GROUP_SIZE, linebuf, sizeof(linebuf),\r\nHEX_ASCII);\r\nseq_printf(seq, " %s\n", linebuf);\r\n}\r\n}\r\nstatic bool color_white(const struct kmemleak_object *object)\r\n{\r\nreturn object->count != KMEMLEAK_BLACK &&\r\nobject->count < object->min_count;\r\n}\r\nstatic bool color_gray(const struct kmemleak_object *object)\r\n{\r\nreturn object->min_count != KMEMLEAK_BLACK &&\r\nobject->count >= object->min_count;\r\n}\r\nstatic bool unreferenced_object(struct kmemleak_object *object)\r\n{\r\nreturn (color_white(object) && object->flags & OBJECT_ALLOCATED) &&\r\ntime_before_eq(object->jiffies + jiffies_min_age,\r\njiffies_last_scan);\r\n}\r\nstatic void print_unreferenced(struct seq_file *seq,\r\nstruct kmemleak_object *object)\r\n{\r\nint i;\r\nunsigned int msecs_age = jiffies_to_msecs(jiffies - object->jiffies);\r\nseq_printf(seq, "unreferenced object 0x%08lx (size %zu):\n",\r\nobject->pointer, object->size);\r\nseq_printf(seq, " comm \"%s\", pid %d, jiffies %lu (age %d.%03ds)\n",\r\nobject->comm, object->pid, object->jiffies,\r\nmsecs_age / 1000, msecs_age % 1000);\r\nhex_dump_object(seq, object);\r\nseq_printf(seq, " backtrace:\n");\r\nfor (i = 0; i < object->trace_len; i++) {\r\nvoid *ptr = (void *)object->trace[i];\r\nseq_printf(seq, " [<%p>] %pS\n", ptr, ptr);\r\n}\r\n}\r\nstatic void dump_object_info(struct kmemleak_object *object)\r\n{\r\nstruct stack_trace trace;\r\ntrace.nr_entries = object->trace_len;\r\ntrace.entries = object->trace;\r\npr_notice("Object 0x%08lx (size %zu):\n",\r\nobject->pointer, object->size);\r\npr_notice(" comm \"%s\", pid %d, jiffies %lu\n",\r\nobject->comm, object->pid, object->jiffies);\r\npr_notice(" min_count = %d\n", object->min_count);\r\npr_notice(" count = %d\n", object->count);\r\npr_notice(" flags = 0x%lx\n", object->flags);\r\npr_notice(" checksum = %u\n", object->checksum);\r\npr_notice(" backtrace:\n");\r\nprint_stack_trace(&trace, 4);\r\n}\r\nstatic struct kmemleak_object *lookup_object(unsigned long ptr, int alias)\r\n{\r\nstruct rb_node *rb = object_tree_root.rb_node;\r\nwhile (rb) {\r\nstruct kmemleak_object *object =\r\nrb_entry(rb, struct kmemleak_object, rb_node);\r\nif (ptr < object->pointer)\r\nrb = object->rb_node.rb_left;\r\nelse if (object->pointer + object->size <= ptr)\r\nrb = object->rb_node.rb_right;\r\nelse if (object->pointer == ptr || alias)\r\nreturn object;\r\nelse {\r\nkmemleak_warn("Found object by alias at 0x%08lx\n",\r\nptr);\r\ndump_object_info(object);\r\nbreak;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic int get_object(struct kmemleak_object *object)\r\n{\r\nreturn atomic_inc_not_zero(&object->use_count);\r\n}\r\nstatic void free_object_rcu(struct rcu_head *rcu)\r\n{\r\nstruct hlist_node *tmp;\r\nstruct kmemleak_scan_area *area;\r\nstruct kmemleak_object *object =\r\ncontainer_of(rcu, struct kmemleak_object, rcu);\r\nhlist_for_each_entry_safe(area, tmp, &object->area_list, node) {\r\nhlist_del(&area->node);\r\nkmem_cache_free(scan_area_cache, area);\r\n}\r\nkmem_cache_free(object_cache, object);\r\n}\r\nstatic void put_object(struct kmemleak_object *object)\r\n{\r\nif (!atomic_dec_and_test(&object->use_count))\r\nreturn;\r\nWARN_ON(object->flags & OBJECT_ALLOCATED);\r\ncall_rcu(&object->rcu, free_object_rcu);\r\n}\r\nstatic struct kmemleak_object *find_and_get_object(unsigned long ptr, int alias)\r\n{\r\nunsigned long flags;\r\nstruct kmemleak_object *object = NULL;\r\nrcu_read_lock();\r\nread_lock_irqsave(&kmemleak_lock, flags);\r\nif (ptr >= min_addr && ptr < max_addr)\r\nobject = lookup_object(ptr, alias);\r\nread_unlock_irqrestore(&kmemleak_lock, flags);\r\nif (object && !get_object(object))\r\nobject = NULL;\r\nrcu_read_unlock();\r\nreturn object;\r\n}\r\nstatic int __save_stack_trace(unsigned long *trace)\r\n{\r\nstruct stack_trace stack_trace;\r\nstack_trace.max_entries = MAX_TRACE;\r\nstack_trace.nr_entries = 0;\r\nstack_trace.entries = trace;\r\nstack_trace.skip = 2;\r\nsave_stack_trace(&stack_trace);\r\nreturn stack_trace.nr_entries;\r\n}\r\nstatic struct kmemleak_object *create_object(unsigned long ptr, size_t size,\r\nint min_count, gfp_t gfp)\r\n{\r\nunsigned long flags;\r\nstruct kmemleak_object *object, *parent;\r\nstruct rb_node **link, *rb_parent;\r\nobject = kmem_cache_alloc(object_cache, gfp_kmemleak_mask(gfp));\r\nif (!object) {\r\npr_warning("Cannot allocate a kmemleak_object structure\n");\r\nkmemleak_disable();\r\nreturn NULL;\r\n}\r\nINIT_LIST_HEAD(&object->object_list);\r\nINIT_LIST_HEAD(&object->gray_list);\r\nINIT_HLIST_HEAD(&object->area_list);\r\nspin_lock_init(&object->lock);\r\natomic_set(&object->use_count, 1);\r\nobject->flags = OBJECT_ALLOCATED;\r\nobject->pointer = ptr;\r\nobject->size = size;\r\nobject->min_count = min_count;\r\nobject->count = 0;\r\nobject->jiffies = jiffies;\r\nobject->checksum = 0;\r\nif (in_irq()) {\r\nobject->pid = 0;\r\nstrncpy(object->comm, "hardirq", sizeof(object->comm));\r\n} else if (in_softirq()) {\r\nobject->pid = 0;\r\nstrncpy(object->comm, "softirq", sizeof(object->comm));\r\n} else {\r\nobject->pid = current->pid;\r\nstrncpy(object->comm, current->comm, sizeof(object->comm));\r\n}\r\nobject->trace_len = __save_stack_trace(object->trace);\r\nwrite_lock_irqsave(&kmemleak_lock, flags);\r\nmin_addr = min(min_addr, ptr);\r\nmax_addr = max(max_addr, ptr + size);\r\nlink = &object_tree_root.rb_node;\r\nrb_parent = NULL;\r\nwhile (*link) {\r\nrb_parent = *link;\r\nparent = rb_entry(rb_parent, struct kmemleak_object, rb_node);\r\nif (ptr + size <= parent->pointer)\r\nlink = &parent->rb_node.rb_left;\r\nelse if (parent->pointer + parent->size <= ptr)\r\nlink = &parent->rb_node.rb_right;\r\nelse {\r\nkmemleak_stop("Cannot insert 0x%lx into the object "\r\n"search tree (overlaps existing)\n",\r\nptr);\r\nkmem_cache_free(object_cache, object);\r\nobject = parent;\r\nspin_lock(&object->lock);\r\ndump_object_info(object);\r\nspin_unlock(&object->lock);\r\ngoto out;\r\n}\r\n}\r\nrb_link_node(&object->rb_node, rb_parent, link);\r\nrb_insert_color(&object->rb_node, &object_tree_root);\r\nlist_add_tail_rcu(&object->object_list, &object_list);\r\nout:\r\nwrite_unlock_irqrestore(&kmemleak_lock, flags);\r\nreturn object;\r\n}\r\nstatic void __delete_object(struct kmemleak_object *object)\r\n{\r\nunsigned long flags;\r\nwrite_lock_irqsave(&kmemleak_lock, flags);\r\nrb_erase(&object->rb_node, &object_tree_root);\r\nlist_del_rcu(&object->object_list);\r\nwrite_unlock_irqrestore(&kmemleak_lock, flags);\r\nWARN_ON(!(object->flags & OBJECT_ALLOCATED));\r\nWARN_ON(atomic_read(&object->use_count) < 2);\r\nspin_lock_irqsave(&object->lock, flags);\r\nobject->flags &= ~OBJECT_ALLOCATED;\r\nspin_unlock_irqrestore(&object->lock, flags);\r\nput_object(object);\r\n}\r\nstatic void delete_object_full(unsigned long ptr)\r\n{\r\nstruct kmemleak_object *object;\r\nobject = find_and_get_object(ptr, 0);\r\nif (!object) {\r\n#ifdef DEBUG\r\nkmemleak_warn("Freeing unknown object at 0x%08lx\n",\r\nptr);\r\n#endif\r\nreturn;\r\n}\r\n__delete_object(object);\r\nput_object(object);\r\n}\r\nstatic void delete_object_part(unsigned long ptr, size_t size)\r\n{\r\nstruct kmemleak_object *object;\r\nunsigned long start, end;\r\nobject = find_and_get_object(ptr, 1);\r\nif (!object) {\r\n#ifdef DEBUG\r\nkmemleak_warn("Partially freeing unknown object at 0x%08lx "\r\n"(size %zu)\n", ptr, size);\r\n#endif\r\nreturn;\r\n}\r\n__delete_object(object);\r\nstart = object->pointer;\r\nend = object->pointer + object->size;\r\nif (ptr > start)\r\ncreate_object(start, ptr - start, object->min_count,\r\nGFP_KERNEL);\r\nif (ptr + size < end)\r\ncreate_object(ptr + size, end - ptr - size, object->min_count,\r\nGFP_KERNEL);\r\nput_object(object);\r\n}\r\nstatic void __paint_it(struct kmemleak_object *object, int color)\r\n{\r\nobject->min_count = color;\r\nif (color == KMEMLEAK_BLACK)\r\nobject->flags |= OBJECT_NO_SCAN;\r\n}\r\nstatic void paint_it(struct kmemleak_object *object, int color)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&object->lock, flags);\r\n__paint_it(object, color);\r\nspin_unlock_irqrestore(&object->lock, flags);\r\n}\r\nstatic void paint_ptr(unsigned long ptr, int color)\r\n{\r\nstruct kmemleak_object *object;\r\nobject = find_and_get_object(ptr, 0);\r\nif (!object) {\r\nkmemleak_warn("Trying to color unknown object "\r\n"at 0x%08lx as %s\n", ptr,\r\n(color == KMEMLEAK_GREY) ? "Grey" :\r\n(color == KMEMLEAK_BLACK) ? "Black" : "Unknown");\r\nreturn;\r\n}\r\npaint_it(object, color);\r\nput_object(object);\r\n}\r\nstatic void make_gray_object(unsigned long ptr)\r\n{\r\npaint_ptr(ptr, KMEMLEAK_GREY);\r\n}\r\nstatic void make_black_object(unsigned long ptr)\r\n{\r\npaint_ptr(ptr, KMEMLEAK_BLACK);\r\n}\r\nstatic void add_scan_area(unsigned long ptr, size_t size, gfp_t gfp)\r\n{\r\nunsigned long flags;\r\nstruct kmemleak_object *object;\r\nstruct kmemleak_scan_area *area;\r\nobject = find_and_get_object(ptr, 1);\r\nif (!object) {\r\nkmemleak_warn("Adding scan area to unknown object at 0x%08lx\n",\r\nptr);\r\nreturn;\r\n}\r\narea = kmem_cache_alloc(scan_area_cache, gfp_kmemleak_mask(gfp));\r\nif (!area) {\r\npr_warning("Cannot allocate a scan area\n");\r\ngoto out;\r\n}\r\nspin_lock_irqsave(&object->lock, flags);\r\nif (size == SIZE_MAX) {\r\nsize = object->pointer + object->size - ptr;\r\n} else if (ptr + size > object->pointer + object->size) {\r\nkmemleak_warn("Scan area larger than object 0x%08lx\n", ptr);\r\ndump_object_info(object);\r\nkmem_cache_free(scan_area_cache, area);\r\ngoto out_unlock;\r\n}\r\nINIT_HLIST_NODE(&area->node);\r\narea->start = ptr;\r\narea->size = size;\r\nhlist_add_head(&area->node, &object->area_list);\r\nout_unlock:\r\nspin_unlock_irqrestore(&object->lock, flags);\r\nout:\r\nput_object(object);\r\n}\r\nstatic void object_no_scan(unsigned long ptr)\r\n{\r\nunsigned long flags;\r\nstruct kmemleak_object *object;\r\nobject = find_and_get_object(ptr, 0);\r\nif (!object) {\r\nkmemleak_warn("Not scanning unknown object at 0x%08lx\n", ptr);\r\nreturn;\r\n}\r\nspin_lock_irqsave(&object->lock, flags);\r\nobject->flags |= OBJECT_NO_SCAN;\r\nspin_unlock_irqrestore(&object->lock, flags);\r\nput_object(object);\r\n}\r\nstatic void __init log_early(int op_type, const void *ptr, size_t size,\r\nint min_count)\r\n{\r\nunsigned long flags;\r\nstruct early_log *log;\r\nif (kmemleak_error) {\r\ncrt_early_log++;\r\nreturn;\r\n}\r\nif (crt_early_log >= ARRAY_SIZE(early_log)) {\r\nkmemleak_disable();\r\nreturn;\r\n}\r\nlocal_irq_save(flags);\r\nlog = &early_log[crt_early_log];\r\nlog->op_type = op_type;\r\nlog->ptr = ptr;\r\nlog->size = size;\r\nlog->min_count = min_count;\r\nlog->trace_len = __save_stack_trace(log->trace);\r\ncrt_early_log++;\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void early_alloc(struct early_log *log)\r\n{\r\nstruct kmemleak_object *object;\r\nunsigned long flags;\r\nint i;\r\nif (!kmemleak_enabled || !log->ptr || IS_ERR(log->ptr))\r\nreturn;\r\nrcu_read_lock();\r\nobject = create_object((unsigned long)log->ptr, log->size,\r\nlog->min_count, GFP_ATOMIC);\r\nif (!object)\r\ngoto out;\r\nspin_lock_irqsave(&object->lock, flags);\r\nfor (i = 0; i < log->trace_len; i++)\r\nobject->trace[i] = log->trace[i];\r\nobject->trace_len = log->trace_len;\r\nspin_unlock_irqrestore(&object->lock, flags);\r\nout:\r\nrcu_read_unlock();\r\n}\r\nstatic void early_alloc_percpu(struct early_log *log)\r\n{\r\nunsigned int cpu;\r\nconst void __percpu *ptr = log->ptr;\r\nfor_each_possible_cpu(cpu) {\r\nlog->ptr = per_cpu_ptr(ptr, cpu);\r\nearly_alloc(log);\r\n}\r\n}\r\nvoid __ref kmemleak_alloc(const void *ptr, size_t size, int min_count,\r\ngfp_t gfp)\r\n{\r\npr_debug("%s(0x%p, %zu, %d)\n", __func__, ptr, size, min_count);\r\nif (kmemleak_enabled && ptr && !IS_ERR(ptr))\r\ncreate_object((unsigned long)ptr, size, min_count, gfp);\r\nelse if (kmemleak_early_log)\r\nlog_early(KMEMLEAK_ALLOC, ptr, size, min_count);\r\n}\r\nvoid __ref kmemleak_alloc_percpu(const void __percpu *ptr, size_t size)\r\n{\r\nunsigned int cpu;\r\npr_debug("%s(0x%p, %zu)\n", __func__, ptr, size);\r\nif (kmemleak_enabled && ptr && !IS_ERR(ptr))\r\nfor_each_possible_cpu(cpu)\r\ncreate_object((unsigned long)per_cpu_ptr(ptr, cpu),\r\nsize, 0, GFP_KERNEL);\r\nelse if (kmemleak_early_log)\r\nlog_early(KMEMLEAK_ALLOC_PERCPU, ptr, size, 0);\r\n}\r\nvoid __ref kmemleak_free(const void *ptr)\r\n{\r\npr_debug("%s(0x%p)\n", __func__, ptr);\r\nif (kmemleak_enabled && ptr && !IS_ERR(ptr))\r\ndelete_object_full((unsigned long)ptr);\r\nelse if (kmemleak_early_log)\r\nlog_early(KMEMLEAK_FREE, ptr, 0, 0);\r\n}\r\nvoid __ref kmemleak_free_part(const void *ptr, size_t size)\r\n{\r\npr_debug("%s(0x%p)\n", __func__, ptr);\r\nif (kmemleak_enabled && ptr && !IS_ERR(ptr))\r\ndelete_object_part((unsigned long)ptr, size);\r\nelse if (kmemleak_early_log)\r\nlog_early(KMEMLEAK_FREE_PART, ptr, size, 0);\r\n}\r\nvoid __ref kmemleak_free_percpu(const void __percpu *ptr)\r\n{\r\nunsigned int cpu;\r\npr_debug("%s(0x%p)\n", __func__, ptr);\r\nif (kmemleak_enabled && ptr && !IS_ERR(ptr))\r\nfor_each_possible_cpu(cpu)\r\ndelete_object_full((unsigned long)per_cpu_ptr(ptr,\r\ncpu));\r\nelse if (kmemleak_early_log)\r\nlog_early(KMEMLEAK_FREE_PERCPU, ptr, 0, 0);\r\n}\r\nvoid __ref kmemleak_update_trace(const void *ptr)\r\n{\r\nstruct kmemleak_object *object;\r\nunsigned long flags;\r\npr_debug("%s(0x%p)\n", __func__, ptr);\r\nif (!kmemleak_enabled || IS_ERR_OR_NULL(ptr))\r\nreturn;\r\nobject = find_and_get_object((unsigned long)ptr, 1);\r\nif (!object) {\r\n#ifdef DEBUG\r\nkmemleak_warn("Updating stack trace for unknown object at %p\n",\r\nptr);\r\n#endif\r\nreturn;\r\n}\r\nspin_lock_irqsave(&object->lock, flags);\r\nobject->trace_len = __save_stack_trace(object->trace);\r\nspin_unlock_irqrestore(&object->lock, flags);\r\nput_object(object);\r\n}\r\nvoid __ref kmemleak_not_leak(const void *ptr)\r\n{\r\npr_debug("%s(0x%p)\n", __func__, ptr);\r\nif (kmemleak_enabled && ptr && !IS_ERR(ptr))\r\nmake_gray_object((unsigned long)ptr);\r\nelse if (kmemleak_early_log)\r\nlog_early(KMEMLEAK_NOT_LEAK, ptr, 0, 0);\r\n}\r\nvoid __ref kmemleak_ignore(const void *ptr)\r\n{\r\npr_debug("%s(0x%p)\n", __func__, ptr);\r\nif (kmemleak_enabled && ptr && !IS_ERR(ptr))\r\nmake_black_object((unsigned long)ptr);\r\nelse if (kmemleak_early_log)\r\nlog_early(KMEMLEAK_IGNORE, ptr, 0, 0);\r\n}\r\nvoid __ref kmemleak_scan_area(const void *ptr, size_t size, gfp_t gfp)\r\n{\r\npr_debug("%s(0x%p)\n", __func__, ptr);\r\nif (kmemleak_enabled && ptr && size && !IS_ERR(ptr))\r\nadd_scan_area((unsigned long)ptr, size, gfp);\r\nelse if (kmemleak_early_log)\r\nlog_early(KMEMLEAK_SCAN_AREA, ptr, size, 0);\r\n}\r\nvoid __ref kmemleak_no_scan(const void *ptr)\r\n{\r\npr_debug("%s(0x%p)\n", __func__, ptr);\r\nif (kmemleak_enabled && ptr && !IS_ERR(ptr))\r\nobject_no_scan((unsigned long)ptr);\r\nelse if (kmemleak_early_log)\r\nlog_early(KMEMLEAK_NO_SCAN, ptr, 0, 0);\r\n}\r\nstatic bool update_checksum(struct kmemleak_object *object)\r\n{\r\nu32 old_csum = object->checksum;\r\nif (!kmemcheck_is_obj_initialized(object->pointer, object->size))\r\nreturn false;\r\nobject->checksum = crc32(0, (void *)object->pointer, object->size);\r\nreturn object->checksum != old_csum;\r\n}\r\nstatic int scan_should_stop(void)\r\n{\r\nif (!kmemleak_enabled)\r\nreturn 1;\r\nif (current->mm)\r\nreturn signal_pending(current);\r\nelse\r\nreturn kthread_should_stop();\r\nreturn 0;\r\n}\r\nstatic void scan_block(void *_start, void *_end,\r\nstruct kmemleak_object *scanned, int allow_resched)\r\n{\r\nunsigned long *ptr;\r\nunsigned long *start = PTR_ALIGN(_start, BYTES_PER_POINTER);\r\nunsigned long *end = _end - (BYTES_PER_POINTER - 1);\r\nfor (ptr = start; ptr < end; ptr++) {\r\nstruct kmemleak_object *object;\r\nunsigned long flags;\r\nunsigned long pointer;\r\nif (allow_resched)\r\ncond_resched();\r\nif (scan_should_stop())\r\nbreak;\r\nif (!kmemcheck_is_obj_initialized((unsigned long)ptr,\r\nBYTES_PER_POINTER))\r\ncontinue;\r\npointer = *ptr;\r\nobject = find_and_get_object(pointer, 1);\r\nif (!object)\r\ncontinue;\r\nif (object == scanned) {\r\nput_object(object);\r\ncontinue;\r\n}\r\nspin_lock_irqsave_nested(&object->lock, flags,\r\nSINGLE_DEPTH_NESTING);\r\nif (!color_white(object)) {\r\nspin_unlock_irqrestore(&object->lock, flags);\r\nput_object(object);\r\ncontinue;\r\n}\r\nobject->count++;\r\nif (color_gray(object)) {\r\nlist_add_tail(&object->gray_list, &gray_list);\r\nspin_unlock_irqrestore(&object->lock, flags);\r\ncontinue;\r\n}\r\nspin_unlock_irqrestore(&object->lock, flags);\r\nput_object(object);\r\n}\r\n}\r\nstatic void scan_object(struct kmemleak_object *object)\r\n{\r\nstruct kmemleak_scan_area *area;\r\nunsigned long flags;\r\nspin_lock_irqsave(&object->lock, flags);\r\nif (object->flags & OBJECT_NO_SCAN)\r\ngoto out;\r\nif (!(object->flags & OBJECT_ALLOCATED))\r\ngoto out;\r\nif (hlist_empty(&object->area_list)) {\r\nvoid *start = (void *)object->pointer;\r\nvoid *end = (void *)(object->pointer + object->size);\r\nwhile (start < end && (object->flags & OBJECT_ALLOCATED) &&\r\n!(object->flags & OBJECT_NO_SCAN)) {\r\nscan_block(start, min(start + MAX_SCAN_SIZE, end),\r\nobject, 0);\r\nstart += MAX_SCAN_SIZE;\r\nspin_unlock_irqrestore(&object->lock, flags);\r\ncond_resched();\r\nspin_lock_irqsave(&object->lock, flags);\r\n}\r\n} else\r\nhlist_for_each_entry(area, &object->area_list, node)\r\nscan_block((void *)area->start,\r\n(void *)(area->start + area->size),\r\nobject, 0);\r\nout:\r\nspin_unlock_irqrestore(&object->lock, flags);\r\n}\r\nstatic void scan_gray_list(void)\r\n{\r\nstruct kmemleak_object *object, *tmp;\r\nobject = list_entry(gray_list.next, typeof(*object), gray_list);\r\nwhile (&object->gray_list != &gray_list) {\r\ncond_resched();\r\nif (!scan_should_stop())\r\nscan_object(object);\r\ntmp = list_entry(object->gray_list.next, typeof(*object),\r\ngray_list);\r\nlist_del(&object->gray_list);\r\nput_object(object);\r\nobject = tmp;\r\n}\r\nWARN_ON(!list_empty(&gray_list));\r\n}\r\nstatic void kmemleak_scan(void)\r\n{\r\nunsigned long flags;\r\nstruct kmemleak_object *object;\r\nint i;\r\nint new_leaks = 0;\r\njiffies_last_scan = jiffies;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(object, &object_list, object_list) {\r\nspin_lock_irqsave(&object->lock, flags);\r\n#ifdef DEBUG\r\nif (atomic_read(&object->use_count) > 1) {\r\npr_debug("object->use_count = %d\n",\r\natomic_read(&object->use_count));\r\ndump_object_info(object);\r\n}\r\n#endif\r\nobject->count = 0;\r\nif (color_gray(object) && get_object(object))\r\nlist_add_tail(&object->gray_list, &gray_list);\r\nspin_unlock_irqrestore(&object->lock, flags);\r\n}\r\nrcu_read_unlock();\r\nscan_block(_sdata, _edata, NULL, 1);\r\nscan_block(__bss_start, __bss_stop, NULL, 1);\r\n#ifdef CONFIG_SMP\r\nfor_each_possible_cpu(i)\r\nscan_block(__per_cpu_start + per_cpu_offset(i),\r\n__per_cpu_end + per_cpu_offset(i), NULL, 1);\r\n#endif\r\nget_online_mems();\r\nfor_each_online_node(i) {\r\nunsigned long start_pfn = node_start_pfn(i);\r\nunsigned long end_pfn = node_end_pfn(i);\r\nunsigned long pfn;\r\nfor (pfn = start_pfn; pfn < end_pfn; pfn++) {\r\nstruct page *page;\r\nif (!pfn_valid(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (page_count(page) == 0)\r\ncontinue;\r\nscan_block(page, page + 1, NULL, 1);\r\n}\r\n}\r\nput_online_mems();\r\nif (kmemleak_stack_scan) {\r\nstruct task_struct *p, *g;\r\nread_lock(&tasklist_lock);\r\ndo_each_thread(g, p) {\r\nscan_block(task_stack_page(p), task_stack_page(p) +\r\nTHREAD_SIZE, NULL, 0);\r\n} while_each_thread(g, p);\r\nread_unlock(&tasklist_lock);\r\n}\r\nscan_gray_list();\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(object, &object_list, object_list) {\r\nspin_lock_irqsave(&object->lock, flags);\r\nif (color_white(object) && (object->flags & OBJECT_ALLOCATED)\r\n&& update_checksum(object) && get_object(object)) {\r\nobject->count = object->min_count;\r\nlist_add_tail(&object->gray_list, &gray_list);\r\n}\r\nspin_unlock_irqrestore(&object->lock, flags);\r\n}\r\nrcu_read_unlock();\r\nscan_gray_list();\r\nif (scan_should_stop())\r\nreturn;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(object, &object_list, object_list) {\r\nspin_lock_irqsave(&object->lock, flags);\r\nif (unreferenced_object(object) &&\r\n!(object->flags & OBJECT_REPORTED)) {\r\nobject->flags |= OBJECT_REPORTED;\r\nnew_leaks++;\r\n}\r\nspin_unlock_irqrestore(&object->lock, flags);\r\n}\r\nrcu_read_unlock();\r\nif (new_leaks) {\r\nkmemleak_found_leaks = true;\r\npr_info("%d new suspected memory leaks (see "\r\n"/sys/kernel/debug/kmemleak)\n", new_leaks);\r\n}\r\n}\r\nstatic int kmemleak_scan_thread(void *arg)\r\n{\r\nstatic int first_run = 1;\r\npr_info("Automatic memory scanning thread started\n");\r\nset_user_nice(current, 10);\r\nif (first_run) {\r\nfirst_run = 0;\r\nssleep(SECS_FIRST_SCAN);\r\n}\r\nwhile (!kthread_should_stop()) {\r\nsigned long timeout = jiffies_scan_wait;\r\nmutex_lock(&scan_mutex);\r\nkmemleak_scan();\r\nmutex_unlock(&scan_mutex);\r\nwhile (timeout && !kthread_should_stop())\r\ntimeout = schedule_timeout_interruptible(timeout);\r\n}\r\npr_info("Automatic memory scanning thread ended\n");\r\nreturn 0;\r\n}\r\nstatic void start_scan_thread(void)\r\n{\r\nif (scan_thread)\r\nreturn;\r\nscan_thread = kthread_run(kmemleak_scan_thread, NULL, "kmemleak");\r\nif (IS_ERR(scan_thread)) {\r\npr_warning("Failed to create the scan thread\n");\r\nscan_thread = NULL;\r\n}\r\n}\r\nstatic void stop_scan_thread(void)\r\n{\r\nif (scan_thread) {\r\nkthread_stop(scan_thread);\r\nscan_thread = NULL;\r\n}\r\n}\r\nstatic void *kmemleak_seq_start(struct seq_file *seq, loff_t *pos)\r\n{\r\nstruct kmemleak_object *object;\r\nloff_t n = *pos;\r\nint err;\r\nerr = mutex_lock_interruptible(&scan_mutex);\r\nif (err < 0)\r\nreturn ERR_PTR(err);\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(object, &object_list, object_list) {\r\nif (n-- > 0)\r\ncontinue;\r\nif (get_object(object))\r\ngoto out;\r\n}\r\nobject = NULL;\r\nout:\r\nreturn object;\r\n}\r\nstatic void *kmemleak_seq_next(struct seq_file *seq, void *v, loff_t *pos)\r\n{\r\nstruct kmemleak_object *prev_obj = v;\r\nstruct kmemleak_object *next_obj = NULL;\r\nstruct kmemleak_object *obj = prev_obj;\r\n++(*pos);\r\nlist_for_each_entry_continue_rcu(obj, &object_list, object_list) {\r\nif (get_object(obj)) {\r\nnext_obj = obj;\r\nbreak;\r\n}\r\n}\r\nput_object(prev_obj);\r\nreturn next_obj;\r\n}\r\nstatic void kmemleak_seq_stop(struct seq_file *seq, void *v)\r\n{\r\nif (!IS_ERR(v)) {\r\nrcu_read_unlock();\r\nmutex_unlock(&scan_mutex);\r\nif (v)\r\nput_object(v);\r\n}\r\n}\r\nstatic int kmemleak_seq_show(struct seq_file *seq, void *v)\r\n{\r\nstruct kmemleak_object *object = v;\r\nunsigned long flags;\r\nspin_lock_irqsave(&object->lock, flags);\r\nif ((object->flags & OBJECT_REPORTED) && unreferenced_object(object))\r\nprint_unreferenced(seq, object);\r\nspin_unlock_irqrestore(&object->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int kmemleak_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &kmemleak_seq_ops);\r\n}\r\nstatic int dump_str_object_info(const char *str)\r\n{\r\nunsigned long flags;\r\nstruct kmemleak_object *object;\r\nunsigned long addr;\r\nif (kstrtoul(str, 0, &addr))\r\nreturn -EINVAL;\r\nobject = find_and_get_object(addr, 0);\r\nif (!object) {\r\npr_info("Unknown object at 0x%08lx\n", addr);\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&object->lock, flags);\r\ndump_object_info(object);\r\nspin_unlock_irqrestore(&object->lock, flags);\r\nput_object(object);\r\nreturn 0;\r\n}\r\nstatic void kmemleak_clear(void)\r\n{\r\nstruct kmemleak_object *object;\r\nunsigned long flags;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(object, &object_list, object_list) {\r\nspin_lock_irqsave(&object->lock, flags);\r\nif ((object->flags & OBJECT_REPORTED) &&\r\nunreferenced_object(object))\r\n__paint_it(object, KMEMLEAK_GREY);\r\nspin_unlock_irqrestore(&object->lock, flags);\r\n}\r\nrcu_read_unlock();\r\nkmemleak_found_leaks = false;\r\n}\r\nstatic ssize_t kmemleak_write(struct file *file, const char __user *user_buf,\r\nsize_t size, loff_t *ppos)\r\n{\r\nchar buf[64];\r\nint buf_size;\r\nint ret;\r\nbuf_size = min(size, (sizeof(buf) - 1));\r\nif (strncpy_from_user(buf, user_buf, buf_size) < 0)\r\nreturn -EFAULT;\r\nbuf[buf_size] = 0;\r\nret = mutex_lock_interruptible(&scan_mutex);\r\nif (ret < 0)\r\nreturn ret;\r\nif (strncmp(buf, "clear", 5) == 0) {\r\nif (kmemleak_enabled)\r\nkmemleak_clear();\r\nelse\r\n__kmemleak_do_cleanup();\r\ngoto out;\r\n}\r\nif (!kmemleak_enabled) {\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\nif (strncmp(buf, "off", 3) == 0)\r\nkmemleak_disable();\r\nelse if (strncmp(buf, "stack=on", 8) == 0)\r\nkmemleak_stack_scan = 1;\r\nelse if (strncmp(buf, "stack=off", 9) == 0)\r\nkmemleak_stack_scan = 0;\r\nelse if (strncmp(buf, "scan=on", 7) == 0)\r\nstart_scan_thread();\r\nelse if (strncmp(buf, "scan=off", 8) == 0)\r\nstop_scan_thread();\r\nelse if (strncmp(buf, "scan=", 5) == 0) {\r\nunsigned long secs;\r\nret = kstrtoul(buf + 5, 0, &secs);\r\nif (ret < 0)\r\ngoto out;\r\nstop_scan_thread();\r\nif (secs) {\r\njiffies_scan_wait = msecs_to_jiffies(secs * 1000);\r\nstart_scan_thread();\r\n}\r\n} else if (strncmp(buf, "scan", 4) == 0)\r\nkmemleak_scan();\r\nelse if (strncmp(buf, "dump=", 5) == 0)\r\nret = dump_str_object_info(buf + 5);\r\nelse\r\nret = -EINVAL;\r\nout:\r\nmutex_unlock(&scan_mutex);\r\nif (ret < 0)\r\nreturn ret;\r\n*ppos += size;\r\nreturn size;\r\n}\r\nstatic void __kmemleak_do_cleanup(void)\r\n{\r\nstruct kmemleak_object *object;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(object, &object_list, object_list)\r\ndelete_object_full(object->pointer);\r\nrcu_read_unlock();\r\n}\r\nstatic void kmemleak_do_cleanup(struct work_struct *work)\r\n{\r\nmutex_lock(&scan_mutex);\r\nstop_scan_thread();\r\nif (!kmemleak_found_leaks)\r\n__kmemleak_do_cleanup();\r\nelse\r\npr_info("Kmemleak disabled without freeing internal data. "\r\n"Reclaim the memory with \"echo clear > /sys/kernel/debug/kmemleak\"\n");\r\nmutex_unlock(&scan_mutex);\r\n}\r\nstatic void kmemleak_disable(void)\r\n{\r\nif (cmpxchg(&kmemleak_error, 0, 1))\r\nreturn;\r\nkmemleak_enabled = 0;\r\nif (kmemleak_initialized)\r\nschedule_work(&cleanup_work);\r\npr_info("Kernel memory leak detector disabled\n");\r\n}\r\nstatic int kmemleak_boot_config(char *str)\r\n{\r\nif (!str)\r\nreturn -EINVAL;\r\nif (strcmp(str, "off") == 0)\r\nkmemleak_disable();\r\nelse if (strcmp(str, "on") == 0)\r\nkmemleak_skip_disable = 1;\r\nelse\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic void __init print_log_trace(struct early_log *log)\r\n{\r\nstruct stack_trace trace;\r\ntrace.nr_entries = log->trace_len;\r\ntrace.entries = log->trace;\r\npr_notice("Early log backtrace:\n");\r\nprint_stack_trace(&trace, 2);\r\n}\r\nvoid __init kmemleak_init(void)\r\n{\r\nint i;\r\nunsigned long flags;\r\n#ifdef CONFIG_DEBUG_KMEMLEAK_DEFAULT_OFF\r\nif (!kmemleak_skip_disable) {\r\nkmemleak_early_log = 0;\r\nkmemleak_disable();\r\nreturn;\r\n}\r\n#endif\r\njiffies_min_age = msecs_to_jiffies(MSECS_MIN_AGE);\r\njiffies_scan_wait = msecs_to_jiffies(SECS_SCAN_WAIT * 1000);\r\nobject_cache = KMEM_CACHE(kmemleak_object, SLAB_NOLEAKTRACE);\r\nscan_area_cache = KMEM_CACHE(kmemleak_scan_area, SLAB_NOLEAKTRACE);\r\nif (crt_early_log >= ARRAY_SIZE(early_log))\r\npr_warning("Early log buffer exceeded (%d), please increase "\r\n"DEBUG_KMEMLEAK_EARLY_LOG_SIZE\n", crt_early_log);\r\nlocal_irq_save(flags);\r\nkmemleak_early_log = 0;\r\nif (kmemleak_error) {\r\nlocal_irq_restore(flags);\r\nreturn;\r\n} else\r\nkmemleak_enabled = 1;\r\nlocal_irq_restore(flags);\r\nfor (i = 0; i < crt_early_log; i++) {\r\nstruct early_log *log = &early_log[i];\r\nswitch (log->op_type) {\r\ncase KMEMLEAK_ALLOC:\r\nearly_alloc(log);\r\nbreak;\r\ncase KMEMLEAK_ALLOC_PERCPU:\r\nearly_alloc_percpu(log);\r\nbreak;\r\ncase KMEMLEAK_FREE:\r\nkmemleak_free(log->ptr);\r\nbreak;\r\ncase KMEMLEAK_FREE_PART:\r\nkmemleak_free_part(log->ptr, log->size);\r\nbreak;\r\ncase KMEMLEAK_FREE_PERCPU:\r\nkmemleak_free_percpu(log->ptr);\r\nbreak;\r\ncase KMEMLEAK_NOT_LEAK:\r\nkmemleak_not_leak(log->ptr);\r\nbreak;\r\ncase KMEMLEAK_IGNORE:\r\nkmemleak_ignore(log->ptr);\r\nbreak;\r\ncase KMEMLEAK_SCAN_AREA:\r\nkmemleak_scan_area(log->ptr, log->size, GFP_KERNEL);\r\nbreak;\r\ncase KMEMLEAK_NO_SCAN:\r\nkmemleak_no_scan(log->ptr);\r\nbreak;\r\ndefault:\r\nkmemleak_warn("Unknown early log operation: %d\n",\r\nlog->op_type);\r\n}\r\nif (kmemleak_warning) {\r\nprint_log_trace(log);\r\nkmemleak_warning = 0;\r\n}\r\n}\r\n}\r\nstatic int __init kmemleak_late_init(void)\r\n{\r\nstruct dentry *dentry;\r\nkmemleak_initialized = 1;\r\nif (kmemleak_error) {\r\nschedule_work(&cleanup_work);\r\nreturn -ENOMEM;\r\n}\r\ndentry = debugfs_create_file("kmemleak", S_IRUGO, NULL, NULL,\r\n&kmemleak_fops);\r\nif (!dentry)\r\npr_warning("Failed to create the debugfs kmemleak file\n");\r\nmutex_lock(&scan_mutex);\r\nstart_scan_thread();\r\nmutex_unlock(&scan_mutex);\r\npr_info("Kernel memory leak detector initialized\n");\r\nreturn 0;\r\n}
