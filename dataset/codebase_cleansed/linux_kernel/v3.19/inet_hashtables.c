static unsigned int inet_ehashfn(struct net *net, const __be32 laddr,\r\nconst __u16 lport, const __be32 faddr,\r\nconst __be16 fport)\r\n{\r\nstatic u32 inet_ehash_secret __read_mostly;\r\nnet_get_random_once(&inet_ehash_secret, sizeof(inet_ehash_secret));\r\nreturn __inet_ehashfn(laddr, lport, faddr, fport,\r\ninet_ehash_secret + net_hash_mix(net));\r\n}\r\nstatic unsigned int inet_sk_ehashfn(const struct sock *sk)\r\n{\r\nconst struct inet_sock *inet = inet_sk(sk);\r\nconst __be32 laddr = inet->inet_rcv_saddr;\r\nconst __u16 lport = inet->inet_num;\r\nconst __be32 faddr = inet->inet_daddr;\r\nconst __be16 fport = inet->inet_dport;\r\nstruct net *net = sock_net(sk);\r\nreturn inet_ehashfn(net, laddr, lport, faddr, fport);\r\n}\r\nstruct inet_bind_bucket *inet_bind_bucket_create(struct kmem_cache *cachep,\r\nstruct net *net,\r\nstruct inet_bind_hashbucket *head,\r\nconst unsigned short snum)\r\n{\r\nstruct inet_bind_bucket *tb = kmem_cache_alloc(cachep, GFP_ATOMIC);\r\nif (tb != NULL) {\r\nwrite_pnet(&tb->ib_net, hold_net(net));\r\ntb->port = snum;\r\ntb->fastreuse = 0;\r\ntb->fastreuseport = 0;\r\ntb->num_owners = 0;\r\nINIT_HLIST_HEAD(&tb->owners);\r\nhlist_add_head(&tb->node, &head->chain);\r\n}\r\nreturn tb;\r\n}\r\nvoid inet_bind_bucket_destroy(struct kmem_cache *cachep, struct inet_bind_bucket *tb)\r\n{\r\nif (hlist_empty(&tb->owners)) {\r\n__hlist_del(&tb->node);\r\nrelease_net(ib_net(tb));\r\nkmem_cache_free(cachep, tb);\r\n}\r\n}\r\nvoid inet_bind_hash(struct sock *sk, struct inet_bind_bucket *tb,\r\nconst unsigned short snum)\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\natomic_inc(&hashinfo->bsockets);\r\ninet_sk(sk)->inet_num = snum;\r\nsk_add_bind_node(sk, &tb->owners);\r\ntb->num_owners++;\r\ninet_csk(sk)->icsk_bind_hash = tb;\r\n}\r\nstatic void __inet_put_port(struct sock *sk)\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\nconst int bhash = inet_bhashfn(sock_net(sk), inet_sk(sk)->inet_num,\r\nhashinfo->bhash_size);\r\nstruct inet_bind_hashbucket *head = &hashinfo->bhash[bhash];\r\nstruct inet_bind_bucket *tb;\r\natomic_dec(&hashinfo->bsockets);\r\nspin_lock(&head->lock);\r\ntb = inet_csk(sk)->icsk_bind_hash;\r\n__sk_del_bind_node(sk);\r\ntb->num_owners--;\r\ninet_csk(sk)->icsk_bind_hash = NULL;\r\ninet_sk(sk)->inet_num = 0;\r\ninet_bind_bucket_destroy(hashinfo->bind_bucket_cachep, tb);\r\nspin_unlock(&head->lock);\r\n}\r\nvoid inet_put_port(struct sock *sk)\r\n{\r\nlocal_bh_disable();\r\n__inet_put_port(sk);\r\nlocal_bh_enable();\r\n}\r\nint __inet_inherit_port(struct sock *sk, struct sock *child)\r\n{\r\nstruct inet_hashinfo *table = sk->sk_prot->h.hashinfo;\r\nunsigned short port = inet_sk(child)->inet_num;\r\nconst int bhash = inet_bhashfn(sock_net(sk), port,\r\ntable->bhash_size);\r\nstruct inet_bind_hashbucket *head = &table->bhash[bhash];\r\nstruct inet_bind_bucket *tb;\r\nspin_lock(&head->lock);\r\ntb = inet_csk(sk)->icsk_bind_hash;\r\nif (tb->port != port) {\r\ninet_bind_bucket_for_each(tb, &head->chain) {\r\nif (net_eq(ib_net(tb), sock_net(sk)) &&\r\ntb->port == port)\r\nbreak;\r\n}\r\nif (!tb) {\r\ntb = inet_bind_bucket_create(table->bind_bucket_cachep,\r\nsock_net(sk), head, port);\r\nif (!tb) {\r\nspin_unlock(&head->lock);\r\nreturn -ENOMEM;\r\n}\r\n}\r\n}\r\ninet_bind_hash(child, tb, port);\r\nspin_unlock(&head->lock);\r\nreturn 0;\r\n}\r\nstatic inline int compute_score(struct sock *sk, struct net *net,\r\nconst unsigned short hnum, const __be32 daddr,\r\nconst int dif)\r\n{\r\nint score = -1;\r\nstruct inet_sock *inet = inet_sk(sk);\r\nif (net_eq(sock_net(sk), net) && inet->inet_num == hnum &&\r\n!ipv6_only_sock(sk)) {\r\n__be32 rcv_saddr = inet->inet_rcv_saddr;\r\nscore = sk->sk_family == PF_INET ? 2 : 1;\r\nif (rcv_saddr) {\r\nif (rcv_saddr != daddr)\r\nreturn -1;\r\nscore += 4;\r\n}\r\nif (sk->sk_bound_dev_if) {\r\nif (sk->sk_bound_dev_if != dif)\r\nreturn -1;\r\nscore += 4;\r\n}\r\n}\r\nreturn score;\r\n}\r\nstruct sock *__inet_lookup_listener(struct net *net,\r\nstruct inet_hashinfo *hashinfo,\r\nconst __be32 saddr, __be16 sport,\r\nconst __be32 daddr, const unsigned short hnum,\r\nconst int dif)\r\n{\r\nstruct sock *sk, *result;\r\nstruct hlist_nulls_node *node;\r\nunsigned int hash = inet_lhashfn(net, hnum);\r\nstruct inet_listen_hashbucket *ilb = &hashinfo->listening_hash[hash];\r\nint score, hiscore, matches = 0, reuseport = 0;\r\nu32 phash = 0;\r\nrcu_read_lock();\r\nbegin:\r\nresult = NULL;\r\nhiscore = 0;\r\nsk_nulls_for_each_rcu(sk, node, &ilb->head) {\r\nscore = compute_score(sk, net, hnum, daddr, dif);\r\nif (score > hiscore) {\r\nresult = sk;\r\nhiscore = score;\r\nreuseport = sk->sk_reuseport;\r\nif (reuseport) {\r\nphash = inet_ehashfn(net, daddr, hnum,\r\nsaddr, sport);\r\nmatches = 1;\r\n}\r\n} else if (score == hiscore && reuseport) {\r\nmatches++;\r\nif (reciprocal_scale(phash, matches) == 0)\r\nresult = sk;\r\nphash = next_pseudo_random32(phash);\r\n}\r\n}\r\nif (get_nulls_value(node) != hash + LISTENING_NULLS_BASE)\r\ngoto begin;\r\nif (result) {\r\nif (unlikely(!atomic_inc_not_zero(&result->sk_refcnt)))\r\nresult = NULL;\r\nelse if (unlikely(compute_score(result, net, hnum, daddr,\r\ndif) < hiscore)) {\r\nsock_put(result);\r\ngoto begin;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn result;\r\n}\r\nvoid sock_gen_put(struct sock *sk)\r\n{\r\nif (!atomic_dec_and_test(&sk->sk_refcnt))\r\nreturn;\r\nif (sk->sk_state == TCP_TIME_WAIT)\r\ninet_twsk_free(inet_twsk(sk));\r\nelse\r\nsk_free(sk);\r\n}\r\nstruct sock *__inet_lookup_established(struct net *net,\r\nstruct inet_hashinfo *hashinfo,\r\nconst __be32 saddr, const __be16 sport,\r\nconst __be32 daddr, const u16 hnum,\r\nconst int dif)\r\n{\r\nINET_ADDR_COOKIE(acookie, saddr, daddr);\r\nconst __portpair ports = INET_COMBINED_PORTS(sport, hnum);\r\nstruct sock *sk;\r\nconst struct hlist_nulls_node *node;\r\nunsigned int hash = inet_ehashfn(net, daddr, hnum, saddr, sport);\r\nunsigned int slot = hash & hashinfo->ehash_mask;\r\nstruct inet_ehash_bucket *head = &hashinfo->ehash[slot];\r\nrcu_read_lock();\r\nbegin:\r\nsk_nulls_for_each_rcu(sk, node, &head->chain) {\r\nif (sk->sk_hash != hash)\r\ncontinue;\r\nif (likely(INET_MATCH(sk, net, acookie,\r\nsaddr, daddr, ports, dif))) {\r\nif (unlikely(!atomic_inc_not_zero(&sk->sk_refcnt)))\r\ngoto out;\r\nif (unlikely(!INET_MATCH(sk, net, acookie,\r\nsaddr, daddr, ports, dif))) {\r\nsock_gen_put(sk);\r\ngoto begin;\r\n}\r\ngoto found;\r\n}\r\n}\r\nif (get_nulls_value(node) != slot)\r\ngoto begin;\r\nout:\r\nsk = NULL;\r\nfound:\r\nrcu_read_unlock();\r\nreturn sk;\r\n}\r\nstatic int __inet_check_established(struct inet_timewait_death_row *death_row,\r\nstruct sock *sk, __u16 lport,\r\nstruct inet_timewait_sock **twp)\r\n{\r\nstruct inet_hashinfo *hinfo = death_row->hashinfo;\r\nstruct inet_sock *inet = inet_sk(sk);\r\n__be32 daddr = inet->inet_rcv_saddr;\r\n__be32 saddr = inet->inet_daddr;\r\nint dif = sk->sk_bound_dev_if;\r\nINET_ADDR_COOKIE(acookie, saddr, daddr);\r\nconst __portpair ports = INET_COMBINED_PORTS(inet->inet_dport, lport);\r\nstruct net *net = sock_net(sk);\r\nunsigned int hash = inet_ehashfn(net, daddr, lport,\r\nsaddr, inet->inet_dport);\r\nstruct inet_ehash_bucket *head = inet_ehash_bucket(hinfo, hash);\r\nspinlock_t *lock = inet_ehash_lockp(hinfo, hash);\r\nstruct sock *sk2;\r\nconst struct hlist_nulls_node *node;\r\nstruct inet_timewait_sock *tw = NULL;\r\nint twrefcnt = 0;\r\nspin_lock(lock);\r\nsk_nulls_for_each(sk2, node, &head->chain) {\r\nif (sk2->sk_hash != hash)\r\ncontinue;\r\nif (likely(INET_MATCH(sk2, net, acookie,\r\nsaddr, daddr, ports, dif))) {\r\nif (sk2->sk_state == TCP_TIME_WAIT) {\r\ntw = inet_twsk(sk2);\r\nif (twsk_unique(sk, sk2, twp))\r\nbreak;\r\n}\r\ngoto not_unique;\r\n}\r\n}\r\ninet->inet_num = lport;\r\ninet->inet_sport = htons(lport);\r\nsk->sk_hash = hash;\r\nWARN_ON(!sk_unhashed(sk));\r\n__sk_nulls_add_node_rcu(sk, &head->chain);\r\nif (tw) {\r\ntwrefcnt = inet_twsk_unhash(tw);\r\nNET_INC_STATS_BH(net, LINUX_MIB_TIMEWAITRECYCLED);\r\n}\r\nspin_unlock(lock);\r\nif (twrefcnt)\r\ninet_twsk_put(tw);\r\nsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\r\nif (twp) {\r\n*twp = tw;\r\n} else if (tw) {\r\ninet_twsk_deschedule(tw, death_row);\r\ninet_twsk_put(tw);\r\n}\r\nreturn 0;\r\nnot_unique:\r\nspin_unlock(lock);\r\nreturn -EADDRNOTAVAIL;\r\n}\r\nstatic inline u32 inet_sk_port_offset(const struct sock *sk)\r\n{\r\nconst struct inet_sock *inet = inet_sk(sk);\r\nreturn secure_ipv4_port_ephemeral(inet->inet_rcv_saddr,\r\ninet->inet_daddr,\r\ninet->inet_dport);\r\n}\r\nint __inet_hash_nolisten(struct sock *sk, struct inet_timewait_sock *tw)\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\nstruct hlist_nulls_head *list;\r\nspinlock_t *lock;\r\nstruct inet_ehash_bucket *head;\r\nint twrefcnt = 0;\r\nWARN_ON(!sk_unhashed(sk));\r\nsk->sk_hash = inet_sk_ehashfn(sk);\r\nhead = inet_ehash_bucket(hashinfo, sk->sk_hash);\r\nlist = &head->chain;\r\nlock = inet_ehash_lockp(hashinfo, sk->sk_hash);\r\nspin_lock(lock);\r\n__sk_nulls_add_node_rcu(sk, list);\r\nif (tw) {\r\nWARN_ON(sk->sk_hash != tw->tw_hash);\r\ntwrefcnt = inet_twsk_unhash(tw);\r\n}\r\nspin_unlock(lock);\r\nsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\r\nreturn twrefcnt;\r\n}\r\nstatic void __inet_hash(struct sock *sk)\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\nstruct inet_listen_hashbucket *ilb;\r\nif (sk->sk_state != TCP_LISTEN) {\r\n__inet_hash_nolisten(sk, NULL);\r\nreturn;\r\n}\r\nWARN_ON(!sk_unhashed(sk));\r\nilb = &hashinfo->listening_hash[inet_sk_listen_hashfn(sk)];\r\nspin_lock(&ilb->lock);\r\n__sk_nulls_add_node_rcu(sk, &ilb->head);\r\nsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\r\nspin_unlock(&ilb->lock);\r\n}\r\nvoid inet_hash(struct sock *sk)\r\n{\r\nif (sk->sk_state != TCP_CLOSE) {\r\nlocal_bh_disable();\r\n__inet_hash(sk);\r\nlocal_bh_enable();\r\n}\r\n}\r\nvoid inet_unhash(struct sock *sk)\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\nspinlock_t *lock;\r\nint done;\r\nif (sk_unhashed(sk))\r\nreturn;\r\nif (sk->sk_state == TCP_LISTEN)\r\nlock = &hashinfo->listening_hash[inet_sk_listen_hashfn(sk)].lock;\r\nelse\r\nlock = inet_ehash_lockp(hashinfo, sk->sk_hash);\r\nspin_lock_bh(lock);\r\ndone = __sk_nulls_del_node_init_rcu(sk);\r\nif (done)\r\nsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\r\nspin_unlock_bh(lock);\r\n}\r\nint __inet_hash_connect(struct inet_timewait_death_row *death_row,\r\nstruct sock *sk, u32 port_offset,\r\nint (*check_established)(struct inet_timewait_death_row *,\r\nstruct sock *, __u16, struct inet_timewait_sock **),\r\nint (*hash)(struct sock *sk, struct inet_timewait_sock *twp))\r\n{\r\nstruct inet_hashinfo *hinfo = death_row->hashinfo;\r\nconst unsigned short snum = inet_sk(sk)->inet_num;\r\nstruct inet_bind_hashbucket *head;\r\nstruct inet_bind_bucket *tb;\r\nint ret;\r\nstruct net *net = sock_net(sk);\r\nint twrefcnt = 1;\r\nif (!snum) {\r\nint i, remaining, low, high, port;\r\nstatic u32 hint;\r\nu32 offset = hint + port_offset;\r\nstruct inet_timewait_sock *tw = NULL;\r\ninet_get_local_port_range(net, &low, &high);\r\nremaining = (high - low) + 1;\r\nlocal_bh_disable();\r\nfor (i = 1; i <= remaining; i++) {\r\nport = low + (i + offset) % remaining;\r\nif (inet_is_local_reserved_port(net, port))\r\ncontinue;\r\nhead = &hinfo->bhash[inet_bhashfn(net, port,\r\nhinfo->bhash_size)];\r\nspin_lock(&head->lock);\r\ninet_bind_bucket_for_each(tb, &head->chain) {\r\nif (net_eq(ib_net(tb), net) &&\r\ntb->port == port) {\r\nif (tb->fastreuse >= 0 ||\r\ntb->fastreuseport >= 0)\r\ngoto next_port;\r\nWARN_ON(hlist_empty(&tb->owners));\r\nif (!check_established(death_row, sk,\r\nport, &tw))\r\ngoto ok;\r\ngoto next_port;\r\n}\r\n}\r\ntb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,\r\nnet, head, port);\r\nif (!tb) {\r\nspin_unlock(&head->lock);\r\nbreak;\r\n}\r\ntb->fastreuse = -1;\r\ntb->fastreuseport = -1;\r\ngoto ok;\r\nnext_port:\r\nspin_unlock(&head->lock);\r\n}\r\nlocal_bh_enable();\r\nreturn -EADDRNOTAVAIL;\r\nok:\r\nhint += i;\r\ninet_bind_hash(sk, tb, port);\r\nif (sk_unhashed(sk)) {\r\ninet_sk(sk)->inet_sport = htons(port);\r\ntwrefcnt += hash(sk, tw);\r\n}\r\nif (tw)\r\ntwrefcnt += inet_twsk_bind_unhash(tw, hinfo);\r\nspin_unlock(&head->lock);\r\nif (tw) {\r\ninet_twsk_deschedule(tw, death_row);\r\nwhile (twrefcnt) {\r\ntwrefcnt--;\r\ninet_twsk_put(tw);\r\n}\r\n}\r\nret = 0;\r\ngoto out;\r\n}\r\nhead = &hinfo->bhash[inet_bhashfn(net, snum, hinfo->bhash_size)];\r\ntb = inet_csk(sk)->icsk_bind_hash;\r\nspin_lock_bh(&head->lock);\r\nif (sk_head(&tb->owners) == sk && !sk->sk_bind_node.next) {\r\nhash(sk, NULL);\r\nspin_unlock_bh(&head->lock);\r\nreturn 0;\r\n} else {\r\nspin_unlock(&head->lock);\r\nret = check_established(death_row, sk, snum, NULL);\r\nout:\r\nlocal_bh_enable();\r\nreturn ret;\r\n}\r\n}\r\nint inet_hash_connect(struct inet_timewait_death_row *death_row,\r\nstruct sock *sk)\r\n{\r\nreturn __inet_hash_connect(death_row, sk, inet_sk_port_offset(sk),\r\n__inet_check_established, __inet_hash_nolisten);\r\n}\r\nvoid inet_hashinfo_init(struct inet_hashinfo *h)\r\n{\r\nint i;\r\natomic_set(&h->bsockets, 0);\r\nfor (i = 0; i < INET_LHTABLE_SIZE; i++) {\r\nspin_lock_init(&h->listening_hash[i].lock);\r\nINIT_HLIST_NULLS_HEAD(&h->listening_hash[i].head,\r\ni + LISTENING_NULLS_BASE);\r\n}\r\n}
