static void\r\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\r\n{\r\nunsigned long val = (unsigned long)owner;\r\nif (rt_mutex_has_waiters(lock))\r\nval |= RT_MUTEX_HAS_WAITERS;\r\nlock->owner = (struct task_struct *)val;\r\n}\r\nstatic inline void clear_rt_mutex_waiters(struct rt_mutex *lock)\r\n{\r\nlock->owner = (struct task_struct *)\r\n((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\r\n}\r\nstatic void fixup_rt_mutex_waiters(struct rt_mutex *lock)\r\n{\r\nif (!rt_mutex_has_waiters(lock))\r\nclear_rt_mutex_waiters(lock);\r\n}\r\nstatic inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\r\n{\r\nunsigned long owner, *p = (unsigned long *) &lock->owner;\r\ndo {\r\nowner = *p;\r\n} while (cmpxchg(p, owner, owner | RT_MUTEX_HAS_WAITERS) != owner);\r\n}\r\nstatic inline bool unlock_rt_mutex_safe(struct rt_mutex *lock)\r\n__releases(lock->wait_lock)\r\n{\r\nstruct task_struct *owner = rt_mutex_owner(lock);\r\nclear_rt_mutex_waiters(lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn rt_mutex_cmpxchg(lock, owner, NULL);\r\n}\r\nstatic inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\r\n{\r\nlock->owner = (struct task_struct *)\r\n((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\r\n}\r\nstatic inline bool unlock_rt_mutex_safe(struct rt_mutex *lock)\r\n__releases(lock->wait_lock)\r\n{\r\nlock->owner = NULL;\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn true;\r\n}\r\nstatic inline int\r\nrt_mutex_waiter_less(struct rt_mutex_waiter *left,\r\nstruct rt_mutex_waiter *right)\r\n{\r\nif (left->prio < right->prio)\r\nreturn 1;\r\nif (dl_prio(left->prio))\r\nreturn (left->task->dl.deadline < right->task->dl.deadline);\r\nreturn 0;\r\n}\r\nstatic void\r\nrt_mutex_enqueue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)\r\n{\r\nstruct rb_node **link = &lock->waiters.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct rt_mutex_waiter *entry;\r\nint leftmost = 1;\r\nwhile (*link) {\r\nparent = *link;\r\nentry = rb_entry(parent, struct rt_mutex_waiter, tree_entry);\r\nif (rt_mutex_waiter_less(waiter, entry)) {\r\nlink = &parent->rb_left;\r\n} else {\r\nlink = &parent->rb_right;\r\nleftmost = 0;\r\n}\r\n}\r\nif (leftmost)\r\nlock->waiters_leftmost = &waiter->tree_entry;\r\nrb_link_node(&waiter->tree_entry, parent, link);\r\nrb_insert_color(&waiter->tree_entry, &lock->waiters);\r\n}\r\nstatic void\r\nrt_mutex_dequeue(struct rt_mutex *lock, struct rt_mutex_waiter *waiter)\r\n{\r\nif (RB_EMPTY_NODE(&waiter->tree_entry))\r\nreturn;\r\nif (lock->waiters_leftmost == &waiter->tree_entry)\r\nlock->waiters_leftmost = rb_next(&waiter->tree_entry);\r\nrb_erase(&waiter->tree_entry, &lock->waiters);\r\nRB_CLEAR_NODE(&waiter->tree_entry);\r\n}\r\nstatic void\r\nrt_mutex_enqueue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\r\n{\r\nstruct rb_node **link = &task->pi_waiters.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct rt_mutex_waiter *entry;\r\nint leftmost = 1;\r\nwhile (*link) {\r\nparent = *link;\r\nentry = rb_entry(parent, struct rt_mutex_waiter, pi_tree_entry);\r\nif (rt_mutex_waiter_less(waiter, entry)) {\r\nlink = &parent->rb_left;\r\n} else {\r\nlink = &parent->rb_right;\r\nleftmost = 0;\r\n}\r\n}\r\nif (leftmost)\r\ntask->pi_waiters_leftmost = &waiter->pi_tree_entry;\r\nrb_link_node(&waiter->pi_tree_entry, parent, link);\r\nrb_insert_color(&waiter->pi_tree_entry, &task->pi_waiters);\r\n}\r\nstatic void\r\nrt_mutex_dequeue_pi(struct task_struct *task, struct rt_mutex_waiter *waiter)\r\n{\r\nif (RB_EMPTY_NODE(&waiter->pi_tree_entry))\r\nreturn;\r\nif (task->pi_waiters_leftmost == &waiter->pi_tree_entry)\r\ntask->pi_waiters_leftmost = rb_next(&waiter->pi_tree_entry);\r\nrb_erase(&waiter->pi_tree_entry, &task->pi_waiters);\r\nRB_CLEAR_NODE(&waiter->pi_tree_entry);\r\n}\r\nint rt_mutex_getprio(struct task_struct *task)\r\n{\r\nif (likely(!task_has_pi_waiters(task)))\r\nreturn task->normal_prio;\r\nreturn min(task_top_pi_waiter(task)->prio,\r\ntask->normal_prio);\r\n}\r\nstruct task_struct *rt_mutex_get_top_task(struct task_struct *task)\r\n{\r\nif (likely(!task_has_pi_waiters(task)))\r\nreturn NULL;\r\nreturn task_top_pi_waiter(task)->task;\r\n}\r\nint rt_mutex_check_prio(struct task_struct *task, int newprio)\r\n{\r\nif (!task_has_pi_waiters(task))\r\nreturn 0;\r\nreturn task_top_pi_waiter(task)->task->prio <= newprio;\r\n}\r\nstatic void __rt_mutex_adjust_prio(struct task_struct *task)\r\n{\r\nint prio = rt_mutex_getprio(task);\r\nif (task->prio != prio || dl_prio(prio))\r\nrt_mutex_setprio(task, prio);\r\n}\r\nstatic void rt_mutex_adjust_prio(struct task_struct *task)\r\n{\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\n__rt_mutex_adjust_prio(task);\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\n}\r\nstatic bool rt_mutex_cond_detect_deadlock(struct rt_mutex_waiter *waiter,\r\nenum rtmutex_chainwalk chwalk)\r\n{\r\nreturn debug_rt_mutex_detect_deadlock(waiter, chwalk);\r\n}\r\nstatic inline struct rt_mutex *task_blocked_on_lock(struct task_struct *p)\r\n{\r\nreturn p->pi_blocked_on ? p->pi_blocked_on->lock : NULL;\r\n}\r\nstatic int rt_mutex_adjust_prio_chain(struct task_struct *task,\r\nenum rtmutex_chainwalk chwalk,\r\nstruct rt_mutex *orig_lock,\r\nstruct rt_mutex *next_lock,\r\nstruct rt_mutex_waiter *orig_waiter,\r\nstruct task_struct *top_task)\r\n{\r\nstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\r\nstruct rt_mutex_waiter *prerequeue_top_waiter;\r\nint ret = 0, depth = 0;\r\nstruct rt_mutex *lock;\r\nbool detect_deadlock;\r\nunsigned long flags;\r\nbool requeue = true;\r\ndetect_deadlock = rt_mutex_cond_detect_deadlock(orig_waiter, chwalk);\r\nagain:\r\nif (++depth > max_lock_depth) {\r\nstatic int prev_max;\r\nif (prev_max != max_lock_depth) {\r\nprev_max = max_lock_depth;\r\nprintk(KERN_WARNING "Maximum lock depth %d reached "\r\n"task: %s (%d)\n", max_lock_depth,\r\ntop_task->comm, task_pid_nr(top_task));\r\n}\r\nput_task_struct(task);\r\nreturn -EDEADLK;\r\n}\r\nretry:\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\nwaiter = task->pi_blocked_on;\r\nif (!waiter)\r\ngoto out_unlock_pi;\r\nif (orig_waiter && !rt_mutex_owner(orig_lock))\r\ngoto out_unlock_pi;\r\nif (next_lock != waiter->lock)\r\ngoto out_unlock_pi;\r\nif (top_waiter) {\r\nif (!task_has_pi_waiters(task))\r\ngoto out_unlock_pi;\r\nif (top_waiter != task_top_pi_waiter(task)) {\r\nif (!detect_deadlock)\r\ngoto out_unlock_pi;\r\nelse\r\nrequeue = false;\r\n}\r\n}\r\nif (waiter->prio == task->prio) {\r\nif (!detect_deadlock)\r\ngoto out_unlock_pi;\r\nelse\r\nrequeue = false;\r\n}\r\nlock = waiter->lock;\r\nif (!raw_spin_trylock(&lock->wait_lock)) {\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\ncpu_relax();\r\ngoto retry;\r\n}\r\nif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\r\ndebug_rt_mutex_deadlock(chwalk, orig_waiter, lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nret = -EDEADLK;\r\ngoto out_unlock_pi;\r\n}\r\nif (!requeue) {\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nput_task_struct(task);\r\nif (!rt_mutex_owner(lock)) {\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn 0;\r\n}\r\ntask = rt_mutex_owner(lock);\r\nget_task_struct(task);\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\nnext_lock = task_blocked_on_lock(task);\r\ntop_waiter = rt_mutex_top_waiter(lock);\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nraw_spin_unlock(&lock->wait_lock);\r\nif (!next_lock)\r\ngoto out_put_task;\r\ngoto again;\r\n}\r\nprerequeue_top_waiter = rt_mutex_top_waiter(lock);\r\nrt_mutex_dequeue(lock, waiter);\r\nwaiter->prio = task->prio;\r\nrt_mutex_enqueue(lock, waiter);\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nput_task_struct(task);\r\nif (!rt_mutex_owner(lock)) {\r\nif (prerequeue_top_waiter != rt_mutex_top_waiter(lock))\r\nwake_up_process(rt_mutex_top_waiter(lock)->task);\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn 0;\r\n}\r\ntask = rt_mutex_owner(lock);\r\nget_task_struct(task);\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\nif (waiter == rt_mutex_top_waiter(lock)) {\r\nrt_mutex_dequeue_pi(task, prerequeue_top_waiter);\r\nrt_mutex_enqueue_pi(task, waiter);\r\n__rt_mutex_adjust_prio(task);\r\n} else if (prerequeue_top_waiter == waiter) {\r\nrt_mutex_dequeue_pi(task, waiter);\r\nwaiter = rt_mutex_top_waiter(lock);\r\nrt_mutex_enqueue_pi(task, waiter);\r\n__rt_mutex_adjust_prio(task);\r\n} else {\r\n}\r\nnext_lock = task_blocked_on_lock(task);\r\ntop_waiter = rt_mutex_top_waiter(lock);\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nraw_spin_unlock(&lock->wait_lock);\r\nif (!next_lock)\r\ngoto out_put_task;\r\nif (!detect_deadlock && waiter != top_waiter)\r\ngoto out_put_task;\r\ngoto again;\r\nout_unlock_pi:\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nout_put_task:\r\nput_task_struct(task);\r\nreturn ret;\r\n}\r\nstatic int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\r\nstruct rt_mutex_waiter *waiter)\r\n{\r\nunsigned long flags;\r\nmark_rt_mutex_waiters(lock);\r\nif (rt_mutex_owner(lock))\r\nreturn 0;\r\nif (waiter) {\r\nif (waiter != rt_mutex_top_waiter(lock))\r\nreturn 0;\r\nrt_mutex_dequeue(lock, waiter);\r\n} else {\r\nif (rt_mutex_has_waiters(lock)) {\r\nif (task->prio >= rt_mutex_top_waiter(lock)->prio)\r\nreturn 0;\r\n} else {\r\ngoto takeit;\r\n}\r\n}\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\ntask->pi_blocked_on = NULL;\r\nif (rt_mutex_has_waiters(lock))\r\nrt_mutex_enqueue_pi(task, rt_mutex_top_waiter(lock));\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\ntakeit:\r\ndebug_rt_mutex_lock(lock);\r\nrt_mutex_set_owner(lock, task);\r\nrt_mutex_deadlock_account_lock(lock, task);\r\nreturn 1;\r\n}\r\nstatic int task_blocks_on_rt_mutex(struct rt_mutex *lock,\r\nstruct rt_mutex_waiter *waiter,\r\nstruct task_struct *task,\r\nenum rtmutex_chainwalk chwalk)\r\n{\r\nstruct task_struct *owner = rt_mutex_owner(lock);\r\nstruct rt_mutex_waiter *top_waiter = waiter;\r\nstruct rt_mutex *next_lock;\r\nint chain_walk = 0, res;\r\nunsigned long flags;\r\nif (owner == task)\r\nreturn -EDEADLK;\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\n__rt_mutex_adjust_prio(task);\r\nwaiter->task = task;\r\nwaiter->lock = lock;\r\nwaiter->prio = task->prio;\r\nif (rt_mutex_has_waiters(lock))\r\ntop_waiter = rt_mutex_top_waiter(lock);\r\nrt_mutex_enqueue(lock, waiter);\r\ntask->pi_blocked_on = waiter;\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nif (!owner)\r\nreturn 0;\r\nraw_spin_lock_irqsave(&owner->pi_lock, flags);\r\nif (waiter == rt_mutex_top_waiter(lock)) {\r\nrt_mutex_dequeue_pi(owner, top_waiter);\r\nrt_mutex_enqueue_pi(owner, waiter);\r\n__rt_mutex_adjust_prio(owner);\r\nif (owner->pi_blocked_on)\r\nchain_walk = 1;\r\n} else if (rt_mutex_cond_detect_deadlock(waiter, chwalk)) {\r\nchain_walk = 1;\r\n}\r\nnext_lock = task_blocked_on_lock(owner);\r\nraw_spin_unlock_irqrestore(&owner->pi_lock, flags);\r\nif (!chain_walk || !next_lock)\r\nreturn 0;\r\nget_task_struct(owner);\r\nraw_spin_unlock(&lock->wait_lock);\r\nres = rt_mutex_adjust_prio_chain(owner, chwalk, lock,\r\nnext_lock, waiter, task);\r\nraw_spin_lock(&lock->wait_lock);\r\nreturn res;\r\n}\r\nstatic void wakeup_next_waiter(struct rt_mutex *lock)\r\n{\r\nstruct rt_mutex_waiter *waiter;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&current->pi_lock, flags);\r\nwaiter = rt_mutex_top_waiter(lock);\r\nrt_mutex_dequeue_pi(current, waiter);\r\nlock->owner = (void *) RT_MUTEX_HAS_WAITERS;\r\nraw_spin_unlock_irqrestore(&current->pi_lock, flags);\r\nwake_up_process(waiter->task);\r\n}\r\nstatic void remove_waiter(struct rt_mutex *lock,\r\nstruct rt_mutex_waiter *waiter)\r\n{\r\nbool is_top_waiter = (waiter == rt_mutex_top_waiter(lock));\r\nstruct task_struct *owner = rt_mutex_owner(lock);\r\nstruct rt_mutex *next_lock;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&current->pi_lock, flags);\r\nrt_mutex_dequeue(lock, waiter);\r\ncurrent->pi_blocked_on = NULL;\r\nraw_spin_unlock_irqrestore(&current->pi_lock, flags);\r\nif (!owner || !is_top_waiter)\r\nreturn;\r\nraw_spin_lock_irqsave(&owner->pi_lock, flags);\r\nrt_mutex_dequeue_pi(owner, waiter);\r\nif (rt_mutex_has_waiters(lock))\r\nrt_mutex_enqueue_pi(owner, rt_mutex_top_waiter(lock));\r\n__rt_mutex_adjust_prio(owner);\r\nnext_lock = task_blocked_on_lock(owner);\r\nraw_spin_unlock_irqrestore(&owner->pi_lock, flags);\r\nif (!next_lock)\r\nreturn;\r\nget_task_struct(owner);\r\nraw_spin_unlock(&lock->wait_lock);\r\nrt_mutex_adjust_prio_chain(owner, RT_MUTEX_MIN_CHAINWALK, lock,\r\nnext_lock, NULL, current);\r\nraw_spin_lock(&lock->wait_lock);\r\n}\r\nvoid rt_mutex_adjust_pi(struct task_struct *task)\r\n{\r\nstruct rt_mutex_waiter *waiter;\r\nstruct rt_mutex *next_lock;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\nwaiter = task->pi_blocked_on;\r\nif (!waiter || (waiter->prio == task->prio &&\r\n!dl_prio(task->prio))) {\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nreturn;\r\n}\r\nnext_lock = waiter->lock;\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nget_task_struct(task);\r\nrt_mutex_adjust_prio_chain(task, RT_MUTEX_MIN_CHAINWALK, NULL,\r\nnext_lock, NULL, task);\r\n}\r\nstatic int __sched\r\n__rt_mutex_slowlock(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout,\r\nstruct rt_mutex_waiter *waiter)\r\n{\r\nint ret = 0;\r\nfor (;;) {\r\nif (try_to_take_rt_mutex(lock, current, waiter))\r\nbreak;\r\nif (unlikely(state == TASK_INTERRUPTIBLE)) {\r\nif (signal_pending(current))\r\nret = -EINTR;\r\nif (timeout && !timeout->task)\r\nret = -ETIMEDOUT;\r\nif (ret)\r\nbreak;\r\n}\r\nraw_spin_unlock(&lock->wait_lock);\r\ndebug_rt_mutex_print_deadlock(waiter);\r\nschedule_rt_mutex(lock);\r\nraw_spin_lock(&lock->wait_lock);\r\nset_current_state(state);\r\n}\r\nreturn ret;\r\n}\r\nstatic void rt_mutex_handle_deadlock(int res, int detect_deadlock,\r\nstruct rt_mutex_waiter *w)\r\n{\r\nif (res != -EDEADLOCK || detect_deadlock)\r\nreturn;\r\nrt_mutex_print_deadlock(w);\r\nwhile (1) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nschedule();\r\n}\r\n}\r\nstatic int __sched\r\nrt_mutex_slowlock(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout,\r\nenum rtmutex_chainwalk chwalk)\r\n{\r\nstruct rt_mutex_waiter waiter;\r\nint ret = 0;\r\ndebug_rt_mutex_init_waiter(&waiter);\r\nRB_CLEAR_NODE(&waiter.pi_tree_entry);\r\nRB_CLEAR_NODE(&waiter.tree_entry);\r\nraw_spin_lock(&lock->wait_lock);\r\nif (try_to_take_rt_mutex(lock, current, NULL)) {\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn 0;\r\n}\r\nset_current_state(state);\r\nif (unlikely(timeout)) {\r\nhrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);\r\nif (!hrtimer_active(&timeout->timer))\r\ntimeout->task = NULL;\r\n}\r\nret = task_blocks_on_rt_mutex(lock, &waiter, current, chwalk);\r\nif (likely(!ret))\r\nret = __rt_mutex_slowlock(lock, state, timeout, &waiter);\r\nset_current_state(TASK_RUNNING);\r\nif (unlikely(ret)) {\r\nremove_waiter(lock, &waiter);\r\nrt_mutex_handle_deadlock(ret, chwalk, &waiter);\r\n}\r\nfixup_rt_mutex_waiters(lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nif (unlikely(timeout))\r\nhrtimer_cancel(&timeout->timer);\r\ndebug_rt_mutex_free_waiter(&waiter);\r\nreturn ret;\r\n}\r\nstatic inline int rt_mutex_slowtrylock(struct rt_mutex *lock)\r\n{\r\nint ret;\r\nif (rt_mutex_owner(lock))\r\nreturn 0;\r\nraw_spin_lock(&lock->wait_lock);\r\nret = try_to_take_rt_mutex(lock, current, NULL);\r\nfixup_rt_mutex_waiters(lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn ret;\r\n}\r\nstatic void __sched\r\nrt_mutex_slowunlock(struct rt_mutex *lock)\r\n{\r\nraw_spin_lock(&lock->wait_lock);\r\ndebug_rt_mutex_unlock(lock);\r\nrt_mutex_deadlock_account_unlock(current);\r\nwhile (!rt_mutex_has_waiters(lock)) {\r\nif (unlock_rt_mutex_safe(lock) == true)\r\nreturn;\r\nraw_spin_lock(&lock->wait_lock);\r\n}\r\nwakeup_next_waiter(lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nrt_mutex_adjust_prio(current);\r\n}\r\nstatic inline int\r\nrt_mutex_fastlock(struct rt_mutex *lock, int state,\r\nint (*slowfn)(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout,\r\nenum rtmutex_chainwalk chwalk))\r\n{\r\nif (likely(rt_mutex_cmpxchg(lock, NULL, current))) {\r\nrt_mutex_deadlock_account_lock(lock, current);\r\nreturn 0;\r\n} else\r\nreturn slowfn(lock, state, NULL, RT_MUTEX_MIN_CHAINWALK);\r\n}\r\nstatic inline int\r\nrt_mutex_timed_fastlock(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout,\r\nenum rtmutex_chainwalk chwalk,\r\nint (*slowfn)(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout,\r\nenum rtmutex_chainwalk chwalk))\r\n{\r\nif (chwalk == RT_MUTEX_MIN_CHAINWALK &&\r\nlikely(rt_mutex_cmpxchg(lock, NULL, current))) {\r\nrt_mutex_deadlock_account_lock(lock, current);\r\nreturn 0;\r\n} else\r\nreturn slowfn(lock, state, timeout, chwalk);\r\n}\r\nstatic inline int\r\nrt_mutex_fasttrylock(struct rt_mutex *lock,\r\nint (*slowfn)(struct rt_mutex *lock))\r\n{\r\nif (likely(rt_mutex_cmpxchg(lock, NULL, current))) {\r\nrt_mutex_deadlock_account_lock(lock, current);\r\nreturn 1;\r\n}\r\nreturn slowfn(lock);\r\n}\r\nstatic inline void\r\nrt_mutex_fastunlock(struct rt_mutex *lock,\r\nvoid (*slowfn)(struct rt_mutex *lock))\r\n{\r\nif (likely(rt_mutex_cmpxchg(lock, current, NULL)))\r\nrt_mutex_deadlock_account_unlock(current);\r\nelse\r\nslowfn(lock);\r\n}\r\nvoid __sched rt_mutex_lock(struct rt_mutex *lock)\r\n{\r\nmight_sleep();\r\nrt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, rt_mutex_slowlock);\r\n}\r\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock)\r\n{\r\nmight_sleep();\r\nreturn rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE, rt_mutex_slowlock);\r\n}\r\nint rt_mutex_timed_futex_lock(struct rt_mutex *lock,\r\nstruct hrtimer_sleeper *timeout)\r\n{\r\nmight_sleep();\r\nreturn rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,\r\nRT_MUTEX_FULL_CHAINWALK,\r\nrt_mutex_slowlock);\r\n}\r\nint\r\nrt_mutex_timed_lock(struct rt_mutex *lock, struct hrtimer_sleeper *timeout)\r\n{\r\nmight_sleep();\r\nreturn rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,\r\nRT_MUTEX_MIN_CHAINWALK,\r\nrt_mutex_slowlock);\r\n}\r\nint __sched rt_mutex_trylock(struct rt_mutex *lock)\r\n{\r\nreturn rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);\r\n}\r\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\r\n{\r\nrt_mutex_fastunlock(lock, rt_mutex_slowunlock);\r\n}\r\nvoid rt_mutex_destroy(struct rt_mutex *lock)\r\n{\r\nWARN_ON(rt_mutex_is_locked(lock));\r\n#ifdef CONFIG_DEBUG_RT_MUTEXES\r\nlock->magic = NULL;\r\n#endif\r\n}\r\nvoid __rt_mutex_init(struct rt_mutex *lock, const char *name)\r\n{\r\nlock->owner = NULL;\r\nraw_spin_lock_init(&lock->wait_lock);\r\nlock->waiters = RB_ROOT;\r\nlock->waiters_leftmost = NULL;\r\ndebug_rt_mutex_init(lock, name);\r\n}\r\nvoid rt_mutex_init_proxy_locked(struct rt_mutex *lock,\r\nstruct task_struct *proxy_owner)\r\n{\r\n__rt_mutex_init(lock, NULL);\r\ndebug_rt_mutex_proxy_lock(lock, proxy_owner);\r\nrt_mutex_set_owner(lock, proxy_owner);\r\nrt_mutex_deadlock_account_lock(lock, proxy_owner);\r\n}\r\nvoid rt_mutex_proxy_unlock(struct rt_mutex *lock,\r\nstruct task_struct *proxy_owner)\r\n{\r\ndebug_rt_mutex_proxy_unlock(lock);\r\nrt_mutex_set_owner(lock, NULL);\r\nrt_mutex_deadlock_account_unlock(proxy_owner);\r\n}\r\nint rt_mutex_start_proxy_lock(struct rt_mutex *lock,\r\nstruct rt_mutex_waiter *waiter,\r\nstruct task_struct *task)\r\n{\r\nint ret;\r\nraw_spin_lock(&lock->wait_lock);\r\nif (try_to_take_rt_mutex(lock, task, NULL)) {\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn 1;\r\n}\r\nret = task_blocks_on_rt_mutex(lock, waiter, task,\r\nRT_MUTEX_FULL_CHAINWALK);\r\nif (ret && !rt_mutex_owner(lock)) {\r\nret = 0;\r\n}\r\nif (unlikely(ret))\r\nremove_waiter(lock, waiter);\r\nraw_spin_unlock(&lock->wait_lock);\r\ndebug_rt_mutex_print_deadlock(waiter);\r\nreturn ret;\r\n}\r\nstruct task_struct *rt_mutex_next_owner(struct rt_mutex *lock)\r\n{\r\nif (!rt_mutex_has_waiters(lock))\r\nreturn NULL;\r\nreturn rt_mutex_top_waiter(lock)->task;\r\n}\r\nint rt_mutex_finish_proxy_lock(struct rt_mutex *lock,\r\nstruct hrtimer_sleeper *to,\r\nstruct rt_mutex_waiter *waiter)\r\n{\r\nint ret;\r\nraw_spin_lock(&lock->wait_lock);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter);\r\nset_current_state(TASK_RUNNING);\r\nif (unlikely(ret))\r\nremove_waiter(lock, waiter);\r\nfixup_rt_mutex_waiters(lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn ret;\r\n}
