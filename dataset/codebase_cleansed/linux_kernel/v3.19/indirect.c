static inline void add_chain(Indirect *p, struct buffer_head *bh, __le32 *v)\r\n{\r\np->key = *(p->p = v);\r\np->bh = bh;\r\n}\r\nstatic int ext4_block_to_path(struct inode *inode,\r\next4_lblk_t i_block,\r\next4_lblk_t offsets[4], int *boundary)\r\n{\r\nint ptrs = EXT4_ADDR_PER_BLOCK(inode->i_sb);\r\nint ptrs_bits = EXT4_ADDR_PER_BLOCK_BITS(inode->i_sb);\r\nconst long direct_blocks = EXT4_NDIR_BLOCKS,\r\nindirect_blocks = ptrs,\r\ndouble_blocks = (1 << (ptrs_bits * 2));\r\nint n = 0;\r\nint final = 0;\r\nif (i_block < direct_blocks) {\r\noffsets[n++] = i_block;\r\nfinal = direct_blocks;\r\n} else if ((i_block -= direct_blocks) < indirect_blocks) {\r\noffsets[n++] = EXT4_IND_BLOCK;\r\noffsets[n++] = i_block;\r\nfinal = ptrs;\r\n} else if ((i_block -= indirect_blocks) < double_blocks) {\r\noffsets[n++] = EXT4_DIND_BLOCK;\r\noffsets[n++] = i_block >> ptrs_bits;\r\noffsets[n++] = i_block & (ptrs - 1);\r\nfinal = ptrs;\r\n} else if (((i_block -= double_blocks) >> (ptrs_bits * 2)) < ptrs) {\r\noffsets[n++] = EXT4_TIND_BLOCK;\r\noffsets[n++] = i_block >> (ptrs_bits * 2);\r\noffsets[n++] = (i_block >> ptrs_bits) & (ptrs - 1);\r\noffsets[n++] = i_block & (ptrs - 1);\r\nfinal = ptrs;\r\n} else {\r\next4_warning(inode->i_sb, "block %lu > max in inode %lu",\r\ni_block + direct_blocks +\r\nindirect_blocks + double_blocks, inode->i_ino);\r\n}\r\nif (boundary)\r\n*boundary = final - 1 - (i_block & (ptrs - 1));\r\nreturn n;\r\n}\r\nstatic Indirect *ext4_get_branch(struct inode *inode, int depth,\r\next4_lblk_t *offsets,\r\nIndirect chain[4], int *err)\r\n{\r\nstruct super_block *sb = inode->i_sb;\r\nIndirect *p = chain;\r\nstruct buffer_head *bh;\r\nint ret = -EIO;\r\n*err = 0;\r\nadd_chain(chain, NULL, EXT4_I(inode)->i_data + *offsets);\r\nif (!p->key)\r\ngoto no_block;\r\nwhile (--depth) {\r\nbh = sb_getblk(sb, le32_to_cpu(p->key));\r\nif (unlikely(!bh)) {\r\nret = -ENOMEM;\r\ngoto failure;\r\n}\r\nif (!bh_uptodate_or_lock(bh)) {\r\nif (bh_submit_read(bh) < 0) {\r\nput_bh(bh);\r\ngoto failure;\r\n}\r\nif (ext4_check_indirect_blockref(inode, bh)) {\r\nput_bh(bh);\r\ngoto failure;\r\n}\r\n}\r\nadd_chain(++p, bh, (__le32 *)bh->b_data + *++offsets);\r\nif (!p->key)\r\ngoto no_block;\r\n}\r\nreturn NULL;\r\nfailure:\r\n*err = ret;\r\nno_block:\r\nreturn p;\r\n}\r\nstatic ext4_fsblk_t ext4_find_near(struct inode *inode, Indirect *ind)\r\n{\r\nstruct ext4_inode_info *ei = EXT4_I(inode);\r\n__le32 *start = ind->bh ? (__le32 *) ind->bh->b_data : ei->i_data;\r\n__le32 *p;\r\nfor (p = ind->p - 1; p >= start; p--) {\r\nif (*p)\r\nreturn le32_to_cpu(*p);\r\n}\r\nif (ind->bh)\r\nreturn ind->bh->b_blocknr;\r\nreturn ext4_inode_to_goal_block(inode);\r\n}\r\nstatic ext4_fsblk_t ext4_find_goal(struct inode *inode, ext4_lblk_t block,\r\nIndirect *partial)\r\n{\r\next4_fsblk_t goal;\r\ngoal = ext4_find_near(inode, partial);\r\ngoal = goal & EXT4_MAX_BLOCK_FILE_PHYS;\r\nreturn goal;\r\n}\r\nstatic int ext4_blks_to_allocate(Indirect *branch, int k, unsigned int blks,\r\nint blocks_to_boundary)\r\n{\r\nunsigned int count = 0;\r\nif (k > 0) {\r\nif (blks < blocks_to_boundary + 1)\r\ncount += blks;\r\nelse\r\ncount += blocks_to_boundary + 1;\r\nreturn count;\r\n}\r\ncount++;\r\nwhile (count < blks && count <= blocks_to_boundary &&\r\nle32_to_cpu(*(branch[0].p + count)) == 0) {\r\ncount++;\r\n}\r\nreturn count;\r\n}\r\nstatic int ext4_alloc_branch(handle_t *handle,\r\nstruct ext4_allocation_request *ar,\r\nint indirect_blks, ext4_lblk_t *offsets,\r\nIndirect *branch)\r\n{\r\nstruct buffer_head * bh;\r\next4_fsblk_t b, new_blocks[4];\r\n__le32 *p;\r\nint i, j, err, len = 1;\r\nfor (i = 0; i <= indirect_blks; i++) {\r\nif (i == indirect_blks) {\r\nnew_blocks[i] = ext4_mb_new_blocks(handle, ar, &err);\r\n} else\r\nar->goal = new_blocks[i] = ext4_new_meta_blocks(handle,\r\nar->inode, ar->goal,\r\nar->flags & EXT4_MB_DELALLOC_RESERVED,\r\nNULL, &err);\r\nif (err) {\r\ni--;\r\ngoto failed;\r\n}\r\nbranch[i].key = cpu_to_le32(new_blocks[i]);\r\nif (i == 0)\r\ncontinue;\r\nbh = branch[i].bh = sb_getblk(ar->inode->i_sb, new_blocks[i-1]);\r\nif (unlikely(!bh)) {\r\nerr = -ENOMEM;\r\ngoto failed;\r\n}\r\nlock_buffer(bh);\r\nBUFFER_TRACE(bh, "call get_create_access");\r\nerr = ext4_journal_get_create_access(handle, bh);\r\nif (err) {\r\nunlock_buffer(bh);\r\ngoto failed;\r\n}\r\nmemset(bh->b_data, 0, bh->b_size);\r\np = branch[i].p = (__le32 *) bh->b_data + offsets[i];\r\nb = new_blocks[i];\r\nif (i == indirect_blks)\r\nlen = ar->len;\r\nfor (j = 0; j < len; j++)\r\n*p++ = cpu_to_le32(b++);\r\nBUFFER_TRACE(bh, "marking uptodate");\r\nset_buffer_uptodate(bh);\r\nunlock_buffer(bh);\r\nBUFFER_TRACE(bh, "call ext4_handle_dirty_metadata");\r\nerr = ext4_handle_dirty_metadata(handle, ar->inode, bh);\r\nif (err)\r\ngoto failed;\r\n}\r\nreturn 0;\r\nfailed:\r\nfor (; i >= 0; i--) {\r\nif (i > 0 && i != indirect_blks && branch[i].bh)\r\next4_forget(handle, 1, ar->inode, branch[i].bh,\r\nbranch[i].bh->b_blocknr);\r\next4_free_blocks(handle, ar->inode, NULL, new_blocks[i],\r\n(i == indirect_blks) ? ar->len : 1, 0);\r\n}\r\nreturn err;\r\n}\r\nstatic int ext4_splice_branch(handle_t *handle,\r\nstruct ext4_allocation_request *ar,\r\nIndirect *where, int num)\r\n{\r\nint i;\r\nint err = 0;\r\next4_fsblk_t current_block;\r\nif (where->bh) {\r\nBUFFER_TRACE(where->bh, "get_write_access");\r\nerr = ext4_journal_get_write_access(handle, where->bh);\r\nif (err)\r\ngoto err_out;\r\n}\r\n*where->p = where->key;\r\nif (num == 0 && ar->len > 1) {\r\ncurrent_block = le32_to_cpu(where->key) + 1;\r\nfor (i = 1; i < ar->len; i++)\r\n*(where->p + i) = cpu_to_le32(current_block++);\r\n}\r\nif (where->bh) {\r\njbd_debug(5, "splicing indirect only\n");\r\nBUFFER_TRACE(where->bh, "call ext4_handle_dirty_metadata");\r\nerr = ext4_handle_dirty_metadata(handle, ar->inode, where->bh);\r\nif (err)\r\ngoto err_out;\r\n} else {\r\next4_mark_inode_dirty(handle, ar->inode);\r\njbd_debug(5, "splicing direct\n");\r\n}\r\nreturn err;\r\nerr_out:\r\nfor (i = 1; i <= num; i++) {\r\next4_free_blocks(handle, ar->inode, where[i].bh, 0, 1,\r\nEXT4_FREE_BLOCKS_FORGET);\r\n}\r\next4_free_blocks(handle, ar->inode, NULL, le32_to_cpu(where[num].key),\r\nar->len, 0);\r\nreturn err;\r\n}\r\nint ext4_ind_map_blocks(handle_t *handle, struct inode *inode,\r\nstruct ext4_map_blocks *map,\r\nint flags)\r\n{\r\nstruct ext4_allocation_request ar;\r\nint err = -EIO;\r\next4_lblk_t offsets[4];\r\nIndirect chain[4];\r\nIndirect *partial;\r\nint indirect_blks;\r\nint blocks_to_boundary = 0;\r\nint depth;\r\nint count = 0;\r\next4_fsblk_t first_block = 0;\r\ntrace_ext4_ind_map_blocks_enter(inode, map->m_lblk, map->m_len, flags);\r\nJ_ASSERT(!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)));\r\nJ_ASSERT(handle != NULL || (flags & EXT4_GET_BLOCKS_CREATE) == 0);\r\ndepth = ext4_block_to_path(inode, map->m_lblk, offsets,\r\n&blocks_to_boundary);\r\nif (depth == 0)\r\ngoto out;\r\npartial = ext4_get_branch(inode, depth, offsets, chain, &err);\r\nif (!partial) {\r\nfirst_block = le32_to_cpu(chain[depth - 1].key);\r\ncount++;\r\nwhile (count < map->m_len && count <= blocks_to_boundary) {\r\next4_fsblk_t blk;\r\nblk = le32_to_cpu(*(chain[depth-1].p + count));\r\nif (blk == first_block + count)\r\ncount++;\r\nelse\r\nbreak;\r\n}\r\ngoto got_it;\r\n}\r\nif ((flags & EXT4_GET_BLOCKS_CREATE) == 0 || err == -EIO)\r\ngoto cleanup;\r\nif (EXT4_HAS_RO_COMPAT_FEATURE(inode->i_sb,\r\nEXT4_FEATURE_RO_COMPAT_BIGALLOC)) {\r\nEXT4_ERROR_INODE(inode, "Can't allocate blocks for "\r\n"non-extent mapped inodes with bigalloc");\r\nreturn -ENOSPC;\r\n}\r\nmemset(&ar, 0, sizeof(ar));\r\nar.inode = inode;\r\nar.logical = map->m_lblk;\r\nif (S_ISREG(inode->i_mode))\r\nar.flags = EXT4_MB_HINT_DATA;\r\nif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\r\nar.flags |= EXT4_MB_DELALLOC_RESERVED;\r\nar.goal = ext4_find_goal(inode, map->m_lblk, partial);\r\nindirect_blks = (chain + depth) - partial - 1;\r\nar.len = ext4_blks_to_allocate(partial, indirect_blks,\r\nmap->m_len, blocks_to_boundary);\r\nerr = ext4_alloc_branch(handle, &ar, indirect_blks,\r\noffsets + (partial - chain), partial);\r\nif (!err)\r\nerr = ext4_splice_branch(handle, &ar, partial, indirect_blks);\r\nif (err)\r\ngoto cleanup;\r\nmap->m_flags |= EXT4_MAP_NEW;\r\next4_update_inode_fsync_trans(handle, inode, 1);\r\ncount = ar.len;\r\ngot_it:\r\nmap->m_flags |= EXT4_MAP_MAPPED;\r\nmap->m_pblk = le32_to_cpu(chain[depth-1].key);\r\nmap->m_len = count;\r\nif (count > blocks_to_boundary)\r\nmap->m_flags |= EXT4_MAP_BOUNDARY;\r\nerr = count;\r\npartial = chain + depth - 1;\r\ncleanup:\r\nwhile (partial > chain) {\r\nBUFFER_TRACE(partial->bh, "call brelse");\r\nbrelse(partial->bh);\r\npartial--;\r\n}\r\nout:\r\ntrace_ext4_ind_map_blocks_exit(inode, flags, map, err);\r\nreturn err;\r\n}\r\nssize_t ext4_ind_direct_IO(int rw, struct kiocb *iocb,\r\nstruct iov_iter *iter, loff_t offset)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct inode *inode = file->f_mapping->host;\r\nstruct ext4_inode_info *ei = EXT4_I(inode);\r\nhandle_t *handle;\r\nssize_t ret;\r\nint orphan = 0;\r\nsize_t count = iov_iter_count(iter);\r\nint retries = 0;\r\nif (rw == WRITE) {\r\nloff_t final_size = offset + count;\r\nif (final_size > inode->i_size) {\r\nhandle = ext4_journal_start(inode, EXT4_HT_INODE, 2);\r\nif (IS_ERR(handle)) {\r\nret = PTR_ERR(handle);\r\ngoto out;\r\n}\r\nret = ext4_orphan_add(handle, inode);\r\nif (ret) {\r\next4_journal_stop(handle);\r\ngoto out;\r\n}\r\norphan = 1;\r\nei->i_disksize = inode->i_size;\r\next4_journal_stop(handle);\r\n}\r\n}\r\nretry:\r\nif (rw == READ && ext4_should_dioread_nolock(inode)) {\r\natomic_inc(&inode->i_dio_count);\r\nsmp_mb();\r\nif (unlikely(ext4_test_inode_state(inode,\r\nEXT4_STATE_DIOREAD_LOCK))) {\r\ninode_dio_done(inode);\r\ngoto locked;\r\n}\r\nret = __blockdev_direct_IO(rw, iocb, inode,\r\ninode->i_sb->s_bdev, iter, offset,\r\next4_get_block, NULL, NULL, 0);\r\ninode_dio_done(inode);\r\n} else {\r\nlocked:\r\nret = blockdev_direct_IO(rw, iocb, inode, iter,\r\noffset, ext4_get_block);\r\nif (unlikely((rw & WRITE) && ret < 0)) {\r\nloff_t isize = i_size_read(inode);\r\nloff_t end = offset + count;\r\nif (end > isize)\r\next4_truncate_failed_write(inode);\r\n}\r\n}\r\nif (ret == -ENOSPC && ext4_should_retry_alloc(inode->i_sb, &retries))\r\ngoto retry;\r\nif (orphan) {\r\nint err;\r\nhandle = ext4_journal_start(inode, EXT4_HT_INODE, 2);\r\nif (IS_ERR(handle)) {\r\nret = PTR_ERR(handle);\r\nif (inode->i_nlink)\r\next4_orphan_del(NULL, inode);\r\ngoto out;\r\n}\r\nif (inode->i_nlink)\r\next4_orphan_del(handle, inode);\r\nif (ret > 0) {\r\nloff_t end = offset + ret;\r\nif (end > inode->i_size) {\r\nei->i_disksize = end;\r\ni_size_write(inode, end);\r\next4_mark_inode_dirty(handle, inode);\r\n}\r\n}\r\nerr = ext4_journal_stop(handle);\r\nif (ret == 0)\r\nret = err;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nint ext4_ind_calc_metadata_amount(struct inode *inode, sector_t lblock)\r\n{\r\nstruct ext4_inode_info *ei = EXT4_I(inode);\r\nsector_t dind_mask = ~((sector_t)EXT4_ADDR_PER_BLOCK(inode->i_sb) - 1);\r\nint blk_bits;\r\nif (lblock < EXT4_NDIR_BLOCKS)\r\nreturn 0;\r\nlblock -= EXT4_NDIR_BLOCKS;\r\nif (ei->i_da_metadata_calc_len &&\r\n(lblock & dind_mask) == ei->i_da_metadata_calc_last_lblock) {\r\nei->i_da_metadata_calc_len++;\r\nreturn 0;\r\n}\r\nei->i_da_metadata_calc_last_lblock = lblock & dind_mask;\r\nei->i_da_metadata_calc_len = 1;\r\nblk_bits = order_base_2(lblock);\r\nreturn (blk_bits / EXT4_ADDR_PER_BLOCK_BITS(inode->i_sb)) + 1;\r\n}\r\nint ext4_ind_trans_blocks(struct inode *inode, int nrblocks)\r\n{\r\nreturn DIV_ROUND_UP(nrblocks, EXT4_ADDR_PER_BLOCK(inode->i_sb)) + 4;\r\n}\r\nstatic int try_to_extend_transaction(handle_t *handle, struct inode *inode)\r\n{\r\nif (!ext4_handle_valid(handle))\r\nreturn 0;\r\nif (ext4_handle_has_enough_credits(handle, EXT4_RESERVE_TRANS_BLOCKS+1))\r\nreturn 0;\r\nif (!ext4_journal_extend(handle, ext4_blocks_for_truncate(inode)))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic inline int all_zeroes(__le32 *p, __le32 *q)\r\n{\r\nwhile (p < q)\r\nif (*p++)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic Indirect *ext4_find_shared(struct inode *inode, int depth,\r\next4_lblk_t offsets[4], Indirect chain[4],\r\n__le32 *top)\r\n{\r\nIndirect *partial, *p;\r\nint k, err;\r\n*top = 0;\r\nfor (k = depth; k > 1 && !offsets[k-1]; k--)\r\n;\r\npartial = ext4_get_branch(inode, k, offsets, chain, &err);\r\nif (!partial)\r\npartial = chain + k-1;\r\nif (!partial->key && *partial->p)\r\ngoto no_top;\r\nfor (p = partial; (p > chain) && all_zeroes((__le32 *) p->bh->b_data, p->p); p--)\r\n;\r\nif (p == chain + k - 1 && p > chain) {\r\np->p--;\r\n} else {\r\n*top = *p->p;\r\n#if 0\r\n*p->p = 0;\r\n#endif\r\n}\r\nwhile (partial > p) {\r\nbrelse(partial->bh);\r\npartial--;\r\n}\r\nno_top:\r\nreturn partial;\r\n}\r\nstatic int ext4_clear_blocks(handle_t *handle, struct inode *inode,\r\nstruct buffer_head *bh,\r\next4_fsblk_t block_to_free,\r\nunsigned long count, __le32 *first,\r\n__le32 *last)\r\n{\r\n__le32 *p;\r\nint flags = EXT4_FREE_BLOCKS_VALIDATED;\r\nint err;\r\nif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\r\nflags |= EXT4_FREE_BLOCKS_FORGET | EXT4_FREE_BLOCKS_METADATA;\r\nelse if (ext4_should_journal_data(inode))\r\nflags |= EXT4_FREE_BLOCKS_FORGET;\r\nif (!ext4_data_block_valid(EXT4_SB(inode->i_sb), block_to_free,\r\ncount)) {\r\nEXT4_ERROR_INODE(inode, "attempt to clear invalid "\r\n"blocks %llu len %lu",\r\n(unsigned long long) block_to_free, count);\r\nreturn 1;\r\n}\r\nif (try_to_extend_transaction(handle, inode)) {\r\nif (bh) {\r\nBUFFER_TRACE(bh, "call ext4_handle_dirty_metadata");\r\nerr = ext4_handle_dirty_metadata(handle, inode, bh);\r\nif (unlikely(err))\r\ngoto out_err;\r\n}\r\nerr = ext4_mark_inode_dirty(handle, inode);\r\nif (unlikely(err))\r\ngoto out_err;\r\nerr = ext4_truncate_restart_trans(handle, inode,\r\next4_blocks_for_truncate(inode));\r\nif (unlikely(err))\r\ngoto out_err;\r\nif (bh) {\r\nBUFFER_TRACE(bh, "retaking write access");\r\nerr = ext4_journal_get_write_access(handle, bh);\r\nif (unlikely(err))\r\ngoto out_err;\r\n}\r\n}\r\nfor (p = first; p < last; p++)\r\n*p = 0;\r\next4_free_blocks(handle, inode, NULL, block_to_free, count, flags);\r\nreturn 0;\r\nout_err:\r\next4_std_error(inode->i_sb, err);\r\nreturn err;\r\n}\r\nstatic void ext4_free_data(handle_t *handle, struct inode *inode,\r\nstruct buffer_head *this_bh,\r\n__le32 *first, __le32 *last)\r\n{\r\next4_fsblk_t block_to_free = 0;\r\nunsigned long count = 0;\r\n__le32 *block_to_free_p = NULL;\r\next4_fsblk_t nr;\r\n__le32 *p;\r\nint err = 0;\r\nif (this_bh) {\r\nBUFFER_TRACE(this_bh, "get_write_access");\r\nerr = ext4_journal_get_write_access(handle, this_bh);\r\nif (err)\r\nreturn;\r\n}\r\nfor (p = first; p < last; p++) {\r\nnr = le32_to_cpu(*p);\r\nif (nr) {\r\nif (count == 0) {\r\nblock_to_free = nr;\r\nblock_to_free_p = p;\r\ncount = 1;\r\n} else if (nr == block_to_free + count) {\r\ncount++;\r\n} else {\r\nerr = ext4_clear_blocks(handle, inode, this_bh,\r\nblock_to_free, count,\r\nblock_to_free_p, p);\r\nif (err)\r\nbreak;\r\nblock_to_free = nr;\r\nblock_to_free_p = p;\r\ncount = 1;\r\n}\r\n}\r\n}\r\nif (!err && count > 0)\r\nerr = ext4_clear_blocks(handle, inode, this_bh, block_to_free,\r\ncount, block_to_free_p, p);\r\nif (err < 0)\r\nreturn;\r\nif (this_bh) {\r\nBUFFER_TRACE(this_bh, "call ext4_handle_dirty_metadata");\r\nif ((EXT4_JOURNAL(inode) == NULL) || bh2jh(this_bh))\r\next4_handle_dirty_metadata(handle, inode, this_bh);\r\nelse\r\nEXT4_ERROR_INODE(inode,\r\n"circular indirect block detected at "\r\n"block %llu",\r\n(unsigned long long) this_bh->b_blocknr);\r\n}\r\n}\r\nstatic void ext4_free_branches(handle_t *handle, struct inode *inode,\r\nstruct buffer_head *parent_bh,\r\n__le32 *first, __le32 *last, int depth)\r\n{\r\next4_fsblk_t nr;\r\n__le32 *p;\r\nif (ext4_handle_is_aborted(handle))\r\nreturn;\r\nif (depth--) {\r\nstruct buffer_head *bh;\r\nint addr_per_block = EXT4_ADDR_PER_BLOCK(inode->i_sb);\r\np = last;\r\nwhile (--p >= first) {\r\nnr = le32_to_cpu(*p);\r\nif (!nr)\r\ncontinue;\r\nif (!ext4_data_block_valid(EXT4_SB(inode->i_sb),\r\nnr, 1)) {\r\nEXT4_ERROR_INODE(inode,\r\n"invalid indirect mapped "\r\n"block %lu (level %d)",\r\n(unsigned long) nr, depth);\r\nbreak;\r\n}\r\nbh = sb_bread(inode->i_sb, nr);\r\nif (!bh) {\r\nEXT4_ERROR_INODE_BLOCK(inode, nr,\r\n"Read failure");\r\ncontinue;\r\n}\r\nBUFFER_TRACE(bh, "free child branches");\r\next4_free_branches(handle, inode, bh,\r\n(__le32 *) bh->b_data,\r\n(__le32 *) bh->b_data + addr_per_block,\r\ndepth);\r\nbrelse(bh);\r\nif (ext4_handle_is_aborted(handle))\r\nreturn;\r\nif (try_to_extend_transaction(handle, inode)) {\r\next4_mark_inode_dirty(handle, inode);\r\next4_truncate_restart_trans(handle, inode,\r\next4_blocks_for_truncate(inode));\r\n}\r\next4_free_blocks(handle, inode, NULL, nr, 1,\r\nEXT4_FREE_BLOCKS_METADATA|\r\nEXT4_FREE_BLOCKS_FORGET);\r\nif (parent_bh) {\r\nBUFFER_TRACE(parent_bh, "get_write_access");\r\nif (!ext4_journal_get_write_access(handle,\r\nparent_bh)){\r\n*p = 0;\r\nBUFFER_TRACE(parent_bh,\r\n"call ext4_handle_dirty_metadata");\r\next4_handle_dirty_metadata(handle,\r\ninode,\r\nparent_bh);\r\n}\r\n}\r\n}\r\n} else {\r\nBUFFER_TRACE(parent_bh, "free data blocks");\r\next4_free_data(handle, inode, parent_bh, first, last);\r\n}\r\n}\r\nvoid ext4_ind_truncate(handle_t *handle, struct inode *inode)\r\n{\r\nstruct ext4_inode_info *ei = EXT4_I(inode);\r\n__le32 *i_data = ei->i_data;\r\nint addr_per_block = EXT4_ADDR_PER_BLOCK(inode->i_sb);\r\next4_lblk_t offsets[4];\r\nIndirect chain[4];\r\nIndirect *partial;\r\n__le32 nr = 0;\r\nint n = 0;\r\next4_lblk_t last_block, max_block;\r\nunsigned blocksize = inode->i_sb->s_blocksize;\r\nlast_block = (inode->i_size + blocksize-1)\r\n>> EXT4_BLOCK_SIZE_BITS(inode->i_sb);\r\nmax_block = (EXT4_SB(inode->i_sb)->s_bitmap_maxbytes + blocksize-1)\r\n>> EXT4_BLOCK_SIZE_BITS(inode->i_sb);\r\nif (last_block != max_block) {\r\nn = ext4_block_to_path(inode, last_block, offsets, NULL);\r\nif (n == 0)\r\nreturn;\r\n}\r\next4_es_remove_extent(inode, last_block, EXT_MAX_BLOCKS - last_block);\r\nei->i_disksize = inode->i_size;\r\nif (last_block == max_block) {\r\nreturn;\r\n} else if (n == 1) {\r\next4_free_data(handle, inode, NULL, i_data+offsets[0],\r\ni_data + EXT4_NDIR_BLOCKS);\r\ngoto do_indirects;\r\n}\r\npartial = ext4_find_shared(inode, n, offsets, chain, &nr);\r\nif (nr) {\r\nif (partial == chain) {\r\next4_free_branches(handle, inode, NULL,\r\n&nr, &nr+1, (chain+n-1) - partial);\r\n*partial->p = 0;\r\n} else {\r\nBUFFER_TRACE(partial->bh, "get_write_access");\r\next4_free_branches(handle, inode, partial->bh,\r\npartial->p,\r\npartial->p+1, (chain+n-1) - partial);\r\n}\r\n}\r\nwhile (partial > chain) {\r\next4_free_branches(handle, inode, partial->bh, partial->p + 1,\r\n(__le32*)partial->bh->b_data+addr_per_block,\r\n(chain+n-1) - partial);\r\nBUFFER_TRACE(partial->bh, "call brelse");\r\nbrelse(partial->bh);\r\npartial--;\r\n}\r\ndo_indirects:\r\nswitch (offsets[0]) {\r\ndefault:\r\nnr = i_data[EXT4_IND_BLOCK];\r\nif (nr) {\r\next4_free_branches(handle, inode, NULL, &nr, &nr+1, 1);\r\ni_data[EXT4_IND_BLOCK] = 0;\r\n}\r\ncase EXT4_IND_BLOCK:\r\nnr = i_data[EXT4_DIND_BLOCK];\r\nif (nr) {\r\next4_free_branches(handle, inode, NULL, &nr, &nr+1, 2);\r\ni_data[EXT4_DIND_BLOCK] = 0;\r\n}\r\ncase EXT4_DIND_BLOCK:\r\nnr = i_data[EXT4_TIND_BLOCK];\r\nif (nr) {\r\next4_free_branches(handle, inode, NULL, &nr, &nr+1, 3);\r\ni_data[EXT4_TIND_BLOCK] = 0;\r\n}\r\ncase EXT4_TIND_BLOCK:\r\n;\r\n}\r\n}\r\nint ext4_ind_remove_space(handle_t *handle, struct inode *inode,\r\next4_lblk_t start, ext4_lblk_t end)\r\n{\r\nstruct ext4_inode_info *ei = EXT4_I(inode);\r\n__le32 *i_data = ei->i_data;\r\nint addr_per_block = EXT4_ADDR_PER_BLOCK(inode->i_sb);\r\next4_lblk_t offsets[4], offsets2[4];\r\nIndirect chain[4], chain2[4];\r\nIndirect *partial, *partial2;\r\next4_lblk_t max_block;\r\n__le32 nr = 0, nr2 = 0;\r\nint n = 0, n2 = 0;\r\nunsigned blocksize = inode->i_sb->s_blocksize;\r\nmax_block = (EXT4_SB(inode->i_sb)->s_bitmap_maxbytes + blocksize-1)\r\n>> EXT4_BLOCK_SIZE_BITS(inode->i_sb);\r\nif (end >= max_block)\r\nend = max_block;\r\nif ((start >= end) || (start > max_block))\r\nreturn 0;\r\nn = ext4_block_to_path(inode, start, offsets, NULL);\r\nn2 = ext4_block_to_path(inode, end, offsets2, NULL);\r\nBUG_ON(n > n2);\r\nif ((n == 1) && (n == n2)) {\r\next4_free_data(handle, inode, NULL, i_data + offsets[0],\r\ni_data + offsets2[0]);\r\nreturn 0;\r\n} else if (n2 > n) {\r\nif (n == 1) {\r\next4_free_data(handle, inode, NULL, i_data + offsets[0],\r\ni_data + EXT4_NDIR_BLOCKS);\r\ngoto end_range;\r\n}\r\npartial = ext4_find_shared(inode, n, offsets, chain, &nr);\r\nif (nr) {\r\nif (partial == chain) {\r\next4_free_branches(handle, inode, NULL,\r\n&nr, &nr+1, (chain+n-1) - partial);\r\n*partial->p = 0;\r\n} else {\r\nBUFFER_TRACE(partial->bh, "get_write_access");\r\next4_free_branches(handle, inode, partial->bh,\r\npartial->p,\r\npartial->p+1, (chain+n-1) - partial);\r\n}\r\n}\r\nwhile (partial > chain) {\r\next4_free_branches(handle, inode, partial->bh,\r\npartial->p + 1,\r\n(__le32 *)partial->bh->b_data+addr_per_block,\r\n(chain+n-1) - partial);\r\nBUFFER_TRACE(partial->bh, "call brelse");\r\nbrelse(partial->bh);\r\npartial--;\r\n}\r\nend_range:\r\npartial2 = ext4_find_shared(inode, n2, offsets2, chain2, &nr2);\r\nif (nr2) {\r\nif (partial2 == chain2) {\r\nreturn 0;\r\n} else {\r\npartial2--;\r\n}\r\n} else {\r\npartial2->p++;\r\n}\r\nwhile (partial2 > chain2) {\r\next4_free_branches(handle, inode, partial2->bh,\r\n(__le32 *)partial2->bh->b_data,\r\npartial2->p,\r\n(chain2+n2-1) - partial2);\r\nBUFFER_TRACE(partial2->bh, "call brelse");\r\nbrelse(partial2->bh);\r\npartial2--;\r\n}\r\ngoto do_indirects;\r\n}\r\npartial = ext4_find_shared(inode, n, offsets, chain, &nr);\r\npartial2 = ext4_find_shared(inode, n2, offsets2, chain2, &nr2);\r\npartial2->p++;\r\nwhile ((partial > chain) || (partial2 > chain2)) {\r\nif ((partial->bh && partial2->bh) &&\r\n(partial->bh->b_blocknr == partial2->bh->b_blocknr)) {\r\nif ((partial > chain) && (partial2 > chain2)) {\r\next4_free_branches(handle, inode, partial->bh,\r\npartial->p + 1,\r\npartial2->p,\r\n(chain+n-1) - partial);\r\nBUFFER_TRACE(partial->bh, "call brelse");\r\nbrelse(partial->bh);\r\nBUFFER_TRACE(partial2->bh, "call brelse");\r\nbrelse(partial2->bh);\r\n}\r\nreturn 0;\r\n}\r\nif (partial > chain) {\r\next4_free_branches(handle, inode, partial->bh,\r\npartial->p + 1,\r\n(__le32 *)partial->bh->b_data+addr_per_block,\r\n(chain+n-1) - partial);\r\nBUFFER_TRACE(partial->bh, "call brelse");\r\nbrelse(partial->bh);\r\npartial--;\r\n}\r\nif (partial2 > chain2) {\r\next4_free_branches(handle, inode, partial2->bh,\r\n(__le32 *)partial2->bh->b_data,\r\npartial2->p,\r\n(chain2+n-1) - partial2);\r\nBUFFER_TRACE(partial2->bh, "call brelse");\r\nbrelse(partial2->bh);\r\npartial2--;\r\n}\r\n}\r\ndo_indirects:\r\nswitch (offsets[0]) {\r\ndefault:\r\nif (++n >= n2)\r\nreturn 0;\r\nnr = i_data[EXT4_IND_BLOCK];\r\nif (nr) {\r\next4_free_branches(handle, inode, NULL, &nr, &nr+1, 1);\r\ni_data[EXT4_IND_BLOCK] = 0;\r\n}\r\ncase EXT4_IND_BLOCK:\r\nif (++n >= n2)\r\nreturn 0;\r\nnr = i_data[EXT4_DIND_BLOCK];\r\nif (nr) {\r\next4_free_branches(handle, inode, NULL, &nr, &nr+1, 2);\r\ni_data[EXT4_DIND_BLOCK] = 0;\r\n}\r\ncase EXT4_DIND_BLOCK:\r\nif (++n >= n2)\r\nreturn 0;\r\nnr = i_data[EXT4_TIND_BLOCK];\r\nif (nr) {\r\next4_free_branches(handle, inode, NULL, &nr, &nr+1, 3);\r\ni_data[EXT4_TIND_BLOCK] = 0;\r\n}\r\ncase EXT4_TIND_BLOCK:\r\n;\r\n}\r\nreturn 0;\r\n}
