static int raid6_has_ssse3(void)\r\n{\r\nreturn boot_cpu_has(X86_FEATURE_XMM) &&\r\nboot_cpu_has(X86_FEATURE_XMM2) &&\r\nboot_cpu_has(X86_FEATURE_SSSE3);\r\n}\r\nstatic void raid6_2data_recov_ssse3(int disks, size_t bytes, int faila,\r\nint failb, void **ptrs)\r\n{\r\nu8 *p, *q, *dp, *dq;\r\nconst u8 *pbmul;\r\nconst u8 *qmul;\r\nstatic const u8 __aligned(16) x0f[16] = {\r\n0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f,\r\n0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f};\r\np = (u8 *)ptrs[disks-2];\r\nq = (u8 *)ptrs[disks-1];\r\ndp = (u8 *)ptrs[faila];\r\nptrs[faila] = (void *)raid6_empty_zero_page;\r\nptrs[disks-2] = dp;\r\ndq = (u8 *)ptrs[failb];\r\nptrs[failb] = (void *)raid6_empty_zero_page;\r\nptrs[disks-1] = dq;\r\nraid6_call.gen_syndrome(disks, bytes, ptrs);\r\nptrs[faila] = dp;\r\nptrs[failb] = dq;\r\nptrs[disks-2] = p;\r\nptrs[disks-1] = q;\r\npbmul = raid6_vgfmul[raid6_gfexi[failb-faila]];\r\nqmul = raid6_vgfmul[raid6_gfinv[raid6_gfexp[faila] ^\r\nraid6_gfexp[failb]]];\r\nkernel_fpu_begin();\r\nasm volatile("movdqa %0,%%xmm7" : : "m" (x0f[0]));\r\n#ifdef CONFIG_X86_64\r\nasm volatile("movdqa %0,%%xmm6" : : "m" (qmul[0]));\r\nasm volatile("movdqa %0,%%xmm14" : : "m" (pbmul[0]));\r\nasm volatile("movdqa %0,%%xmm15" : : "m" (pbmul[16]));\r\n#endif\r\nwhile (bytes) {\r\n#ifdef CONFIG_X86_64\r\nasm volatile("movdqa %0,%%xmm1" : : "m" (q[0]));\r\nasm volatile("movdqa %0,%%xmm9" : : "m" (q[16]));\r\nasm volatile("movdqa %0,%%xmm0" : : "m" (p[0]));\r\nasm volatile("movdqa %0,%%xmm8" : : "m" (p[16]));\r\nasm volatile("pxor %0,%%xmm1" : : "m" (dq[0]));\r\nasm volatile("pxor %0,%%xmm9" : : "m" (dq[16]));\r\nasm volatile("pxor %0,%%xmm0" : : "m" (dp[0]));\r\nasm volatile("pxor %0,%%xmm8" : : "m" (dp[16]));\r\nasm volatile("movdqa %xmm6,%xmm4");\r\nasm volatile("movdqa %0,%%xmm5" : : "m" (qmul[16]));\r\nasm volatile("movdqa %xmm6,%xmm12");\r\nasm volatile("movdqa %xmm5,%xmm13");\r\nasm volatile("movdqa %xmm1,%xmm3");\r\nasm volatile("movdqa %xmm9,%xmm11");\r\nasm volatile("movdqa %xmm0,%xmm2");\r\nasm volatile("movdqa %xmm8,%xmm10");\r\nasm volatile("psraw $4,%xmm1");\r\nasm volatile("psraw $4,%xmm9");\r\nasm volatile("pand %xmm7,%xmm3");\r\nasm volatile("pand %xmm7,%xmm11");\r\nasm volatile("pand %xmm7,%xmm1");\r\nasm volatile("pand %xmm7,%xmm9");\r\nasm volatile("pshufb %xmm3,%xmm4");\r\nasm volatile("pshufb %xmm11,%xmm12");\r\nasm volatile("pshufb %xmm1,%xmm5");\r\nasm volatile("pshufb %xmm9,%xmm13");\r\nasm volatile("pxor %xmm4,%xmm5");\r\nasm volatile("pxor %xmm12,%xmm13");\r\nasm volatile("movdqa %xmm14,%xmm4");\r\nasm volatile("movdqa %xmm15,%xmm1");\r\nasm volatile("movdqa %xmm14,%xmm12");\r\nasm volatile("movdqa %xmm15,%xmm9");\r\nasm volatile("movdqa %xmm2,%xmm3");\r\nasm volatile("movdqa %xmm10,%xmm11");\r\nasm volatile("psraw $4,%xmm2");\r\nasm volatile("psraw $4,%xmm10");\r\nasm volatile("pand %xmm7,%xmm3");\r\nasm volatile("pand %xmm7,%xmm11");\r\nasm volatile("pand %xmm7,%xmm2");\r\nasm volatile("pand %xmm7,%xmm10");\r\nasm volatile("pshufb %xmm3,%xmm4");\r\nasm volatile("pshufb %xmm11,%xmm12");\r\nasm volatile("pshufb %xmm2,%xmm1");\r\nasm volatile("pshufb %xmm10,%xmm9");\r\nasm volatile("pxor %xmm4,%xmm1");\r\nasm volatile("pxor %xmm12,%xmm9");\r\nasm volatile("pxor %xmm5,%xmm1");\r\nasm volatile("pxor %xmm13,%xmm9");\r\nasm volatile("movdqa %%xmm1,%0" : "=m" (dq[0]));\r\nasm volatile("movdqa %%xmm9,%0" : "=m" (dq[16]));\r\nasm volatile("pxor %xmm1,%xmm0");\r\nasm volatile("pxor %xmm9,%xmm8");\r\nasm volatile("movdqa %%xmm0,%0" : "=m" (dp[0]));\r\nasm volatile("movdqa %%xmm8,%0" : "=m" (dp[16]));\r\nbytes -= 32;\r\np += 32;\r\nq += 32;\r\ndp += 32;\r\ndq += 32;\r\n#else\r\nasm volatile("movdqa %0,%%xmm1" : : "m" (*q));\r\nasm volatile("movdqa %0,%%xmm0" : : "m" (*p));\r\nasm volatile("pxor %0,%%xmm1" : : "m" (*dq));\r\nasm volatile("pxor %0,%%xmm0" : : "m" (*dp));\r\nasm volatile("movdqa %0,%%xmm4" : : "m" (qmul[0]));\r\nasm volatile("movdqa %0,%%xmm5" : : "m" (qmul[16]));\r\nasm volatile("movdqa %xmm1,%xmm3");\r\nasm volatile("psraw $4,%xmm1");\r\nasm volatile("pand %xmm7,%xmm3");\r\nasm volatile("pand %xmm7,%xmm1");\r\nasm volatile("pshufb %xmm3,%xmm4");\r\nasm volatile("pshufb %xmm1,%xmm5");\r\nasm volatile("pxor %xmm4,%xmm5");\r\nasm volatile("movdqa %xmm0,%xmm2");\r\nasm volatile("movdqa %0,%%xmm4" : : "m" (pbmul[0]));\r\nasm volatile("movdqa %0,%%xmm1" : : "m" (pbmul[16]));\r\nasm volatile("movdqa %xmm2,%xmm3");\r\nasm volatile("psraw $4,%xmm2");\r\nasm volatile("pand %xmm7,%xmm3");\r\nasm volatile("pand %xmm7,%xmm2");\r\nasm volatile("pshufb %xmm3,%xmm4");\r\nasm volatile("pshufb %xmm2,%xmm1");\r\nasm volatile("pxor %xmm4,%xmm1");\r\nasm volatile("pxor %xmm5,%xmm1");\r\nasm volatile("movdqa %%xmm1,%0" : "=m" (*dq));\r\nasm volatile("pxor %xmm1,%xmm0");\r\nasm volatile("movdqa %%xmm0,%0" : "=m" (*dp));\r\nbytes -= 16;\r\np += 16;\r\nq += 16;\r\ndp += 16;\r\ndq += 16;\r\n#endif\r\n}\r\nkernel_fpu_end();\r\n}\r\nstatic void raid6_datap_recov_ssse3(int disks, size_t bytes, int faila,\r\nvoid **ptrs)\r\n{\r\nu8 *p, *q, *dq;\r\nconst u8 *qmul;\r\nstatic const u8 __aligned(16) x0f[16] = {\r\n0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f,\r\n0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f};\r\np = (u8 *)ptrs[disks-2];\r\nq = (u8 *)ptrs[disks-1];\r\ndq = (u8 *)ptrs[faila];\r\nptrs[faila] = (void *)raid6_empty_zero_page;\r\nptrs[disks-1] = dq;\r\nraid6_call.gen_syndrome(disks, bytes, ptrs);\r\nptrs[faila] = dq;\r\nptrs[disks-1] = q;\r\nqmul = raid6_vgfmul[raid6_gfinv[raid6_gfexp[faila]]];\r\nkernel_fpu_begin();\r\nasm volatile("movdqa %0, %%xmm7" : : "m" (x0f[0]));\r\nwhile (bytes) {\r\n#ifdef CONFIG_X86_64\r\nasm volatile("movdqa %0, %%xmm3" : : "m" (dq[0]));\r\nasm volatile("movdqa %0, %%xmm4" : : "m" (dq[16]));\r\nasm volatile("pxor %0, %%xmm3" : : "m" (q[0]));\r\nasm volatile("movdqa %0, %%xmm0" : : "m" (qmul[0]));\r\nasm volatile("pxor %0, %%xmm4" : : "m" (q[16]));\r\nasm volatile("movdqa %0, %%xmm1" : : "m" (qmul[16]));\r\nasm volatile("movdqa %xmm3, %xmm6");\r\nasm volatile("movdqa %xmm4, %xmm8");\r\nasm volatile("psraw $4, %xmm3");\r\nasm volatile("pand %xmm7, %xmm6");\r\nasm volatile("pand %xmm7, %xmm3");\r\nasm volatile("pshufb %xmm6, %xmm0");\r\nasm volatile("pshufb %xmm3, %xmm1");\r\nasm volatile("movdqa %0, %%xmm10" : : "m" (qmul[0]));\r\nasm volatile("pxor %xmm0, %xmm1");\r\nasm volatile("movdqa %0, %%xmm11" : : "m" (qmul[16]));\r\nasm volatile("psraw $4, %xmm4");\r\nasm volatile("pand %xmm7, %xmm8");\r\nasm volatile("pand %xmm7, %xmm4");\r\nasm volatile("pshufb %xmm8, %xmm10");\r\nasm volatile("pshufb %xmm4, %xmm11");\r\nasm volatile("movdqa %0, %%xmm2" : : "m" (p[0]));\r\nasm volatile("pxor %xmm10, %xmm11");\r\nasm volatile("movdqa %0, %%xmm12" : : "m" (p[16]));\r\nasm volatile("pxor %xmm1, %xmm2");\r\nasm volatile("pxor %xmm11, %xmm12");\r\nasm volatile("movdqa %%xmm1, %0" : "=m" (dq[0]));\r\nasm volatile("movdqa %%xmm11, %0" : "=m" (dq[16]));\r\nasm volatile("movdqa %%xmm2, %0" : "=m" (p[0]));\r\nasm volatile("movdqa %%xmm12, %0" : "=m" (p[16]));\r\nbytes -= 32;\r\np += 32;\r\nq += 32;\r\ndq += 32;\r\n#else\r\nasm volatile("movdqa %0, %%xmm3" : : "m" (dq[0]));\r\nasm volatile("movdqa %0, %%xmm0" : : "m" (qmul[0]));\r\nasm volatile("pxor %0, %%xmm3" : : "m" (q[0]));\r\nasm volatile("movdqa %0, %%xmm1" : : "m" (qmul[16]));\r\nasm volatile("movdqa %xmm3, %xmm6");\r\nasm volatile("movdqa %0, %%xmm2" : : "m" (p[0]));\r\nasm volatile("psraw $4, %xmm3");\r\nasm volatile("pand %xmm7, %xmm6");\r\nasm volatile("pand %xmm7, %xmm3");\r\nasm volatile("pshufb %xmm6, %xmm0");\r\nasm volatile("pshufb %xmm3, %xmm1");\r\nasm volatile("pxor %xmm0, %xmm1");\r\nasm volatile("pxor %xmm1, %xmm2");\r\nasm volatile("movdqa %%xmm1, %0" : "=m" (dq[0]));\r\nasm volatile("movdqa %%xmm2, %0" : "=m" (p[0]));\r\nbytes -= 16;\r\np += 16;\r\nq += 16;\r\ndq += 16;\r\n#endif\r\n}\r\nkernel_fpu_end();\r\n}
