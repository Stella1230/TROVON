static void * r1bio_pool_alloc(gfp_t gfp_flags, void *data)\r\n{\r\nstruct pool_info *pi = data;\r\nint size = offsetof(struct r1bio, bios[pi->raid_disks]);\r\nreturn kzalloc(size, gfp_flags);\r\n}\r\nstatic void r1bio_pool_free(void *r1_bio, void *data)\r\n{\r\nkfree(r1_bio);\r\n}\r\nstatic void * r1buf_pool_alloc(gfp_t gfp_flags, void *data)\r\n{\r\nstruct pool_info *pi = data;\r\nstruct r1bio *r1_bio;\r\nstruct bio *bio;\r\nint need_pages;\r\nint i, j;\r\nr1_bio = r1bio_pool_alloc(gfp_flags, pi);\r\nif (!r1_bio)\r\nreturn NULL;\r\nfor (j = pi->raid_disks ; j-- ; ) {\r\nbio = bio_kmalloc(gfp_flags, RESYNC_PAGES);\r\nif (!bio)\r\ngoto out_free_bio;\r\nr1_bio->bios[j] = bio;\r\n}\r\nif (test_bit(MD_RECOVERY_REQUESTED, &pi->mddev->recovery))\r\nneed_pages = pi->raid_disks;\r\nelse\r\nneed_pages = 1;\r\nfor (j = 0; j < need_pages; j++) {\r\nbio = r1_bio->bios[j];\r\nbio->bi_vcnt = RESYNC_PAGES;\r\nif (bio_alloc_pages(bio, gfp_flags))\r\ngoto out_free_pages;\r\n}\r\nif (!test_bit(MD_RECOVERY_REQUESTED, &pi->mddev->recovery)) {\r\nfor (i=0; i<RESYNC_PAGES ; i++)\r\nfor (j=1; j<pi->raid_disks; j++)\r\nr1_bio->bios[j]->bi_io_vec[i].bv_page =\r\nr1_bio->bios[0]->bi_io_vec[i].bv_page;\r\n}\r\nr1_bio->master_bio = NULL;\r\nreturn r1_bio;\r\nout_free_pages:\r\nwhile (--j >= 0) {\r\nstruct bio_vec *bv;\r\nbio_for_each_segment_all(bv, r1_bio->bios[j], i)\r\n__free_page(bv->bv_page);\r\n}\r\nout_free_bio:\r\nwhile (++j < pi->raid_disks)\r\nbio_put(r1_bio->bios[j]);\r\nr1bio_pool_free(r1_bio, data);\r\nreturn NULL;\r\n}\r\nstatic void r1buf_pool_free(void *__r1_bio, void *data)\r\n{\r\nstruct pool_info *pi = data;\r\nint i,j;\r\nstruct r1bio *r1bio = __r1_bio;\r\nfor (i = 0; i < RESYNC_PAGES; i++)\r\nfor (j = pi->raid_disks; j-- ;) {\r\nif (j == 0 ||\r\nr1bio->bios[j]->bi_io_vec[i].bv_page !=\r\nr1bio->bios[0]->bi_io_vec[i].bv_page)\r\nsafe_put_page(r1bio->bios[j]->bi_io_vec[i].bv_page);\r\n}\r\nfor (i=0 ; i < pi->raid_disks; i++)\r\nbio_put(r1bio->bios[i]);\r\nr1bio_pool_free(r1bio, data);\r\n}\r\nstatic void put_all_bios(struct r1conf *conf, struct r1bio *r1_bio)\r\n{\r\nint i;\r\nfor (i = 0; i < conf->raid_disks * 2; i++) {\r\nstruct bio **bio = r1_bio->bios + i;\r\nif (!BIO_SPECIAL(*bio))\r\nbio_put(*bio);\r\n*bio = NULL;\r\n}\r\n}\r\nstatic void free_r1bio(struct r1bio *r1_bio)\r\n{\r\nstruct r1conf *conf = r1_bio->mddev->private;\r\nput_all_bios(conf, r1_bio);\r\nmempool_free(r1_bio, conf->r1bio_pool);\r\n}\r\nstatic void put_buf(struct r1bio *r1_bio)\r\n{\r\nstruct r1conf *conf = r1_bio->mddev->private;\r\nint i;\r\nfor (i = 0; i < conf->raid_disks * 2; i++) {\r\nstruct bio *bio = r1_bio->bios[i];\r\nif (bio->bi_end_io)\r\nrdev_dec_pending(conf->mirrors[i].rdev, r1_bio->mddev);\r\n}\r\nmempool_free(r1_bio, conf->r1buf_pool);\r\nlower_barrier(conf);\r\n}\r\nstatic void reschedule_retry(struct r1bio *r1_bio)\r\n{\r\nunsigned long flags;\r\nstruct mddev *mddev = r1_bio->mddev;\r\nstruct r1conf *conf = mddev->private;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nlist_add(&r1_bio->retry_list, &conf->retry_list);\r\nconf->nr_queued ++;\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nwake_up(&conf->wait_barrier);\r\nmd_wakeup_thread(mddev->thread);\r\n}\r\nstatic void call_bio_endio(struct r1bio *r1_bio)\r\n{\r\nstruct bio *bio = r1_bio->master_bio;\r\nint done;\r\nstruct r1conf *conf = r1_bio->mddev->private;\r\nsector_t start_next_window = r1_bio->start_next_window;\r\nsector_t bi_sector = bio->bi_iter.bi_sector;\r\nif (bio->bi_phys_segments) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nbio->bi_phys_segments--;\r\ndone = (bio->bi_phys_segments == 0);\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nwake_up(&conf->wait_barrier);\r\n} else\r\ndone = 1;\r\nif (!test_bit(R1BIO_Uptodate, &r1_bio->state))\r\nclear_bit(BIO_UPTODATE, &bio->bi_flags);\r\nif (done) {\r\nbio_endio(bio, 0);\r\nallow_barrier(conf, start_next_window, bi_sector);\r\n}\r\n}\r\nstatic void raid_end_bio_io(struct r1bio *r1_bio)\r\n{\r\nstruct bio *bio = r1_bio->master_bio;\r\nif (!test_and_set_bit(R1BIO_Returned, &r1_bio->state)) {\r\npr_debug("raid1: sync end %s on sectors %llu-%llu\n",\r\n(bio_data_dir(bio) == WRITE) ? "write" : "read",\r\n(unsigned long long) bio->bi_iter.bi_sector,\r\n(unsigned long long) bio_end_sector(bio) - 1);\r\ncall_bio_endio(r1_bio);\r\n}\r\nfree_r1bio(r1_bio);\r\n}\r\nstatic inline void update_head_pos(int disk, struct r1bio *r1_bio)\r\n{\r\nstruct r1conf *conf = r1_bio->mddev->private;\r\nconf->mirrors[disk].head_position =\r\nr1_bio->sector + (r1_bio->sectors);\r\n}\r\nstatic int find_bio_disk(struct r1bio *r1_bio, struct bio *bio)\r\n{\r\nint mirror;\r\nstruct r1conf *conf = r1_bio->mddev->private;\r\nint raid_disks = conf->raid_disks;\r\nfor (mirror = 0; mirror < raid_disks * 2; mirror++)\r\nif (r1_bio->bios[mirror] == bio)\r\nbreak;\r\nBUG_ON(mirror == raid_disks * 2);\r\nupdate_head_pos(mirror, r1_bio);\r\nreturn mirror;\r\n}\r\nstatic void raid1_end_read_request(struct bio *bio, int error)\r\n{\r\nint uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nstruct r1bio *r1_bio = bio->bi_private;\r\nint mirror;\r\nstruct r1conf *conf = r1_bio->mddev->private;\r\nmirror = r1_bio->read_disk;\r\nupdate_head_pos(mirror, r1_bio);\r\nif (uptodate)\r\nset_bit(R1BIO_Uptodate, &r1_bio->state);\r\nelse {\r\nunsigned long flags;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nif (r1_bio->mddev->degraded == conf->raid_disks ||\r\n(r1_bio->mddev->degraded == conf->raid_disks-1 &&\r\n!test_bit(Faulty, &conf->mirrors[mirror].rdev->flags)))\r\nuptodate = 1;\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\n}\r\nif (uptodate) {\r\nraid_end_bio_io(r1_bio);\r\nrdev_dec_pending(conf->mirrors[mirror].rdev, conf->mddev);\r\n} else {\r\nchar b[BDEVNAME_SIZE];\r\nprintk_ratelimited(\r\nKERN_ERR "md/raid1:%s: %s: "\r\n"rescheduling sector %llu\n",\r\nmdname(conf->mddev),\r\nbdevname(conf->mirrors[mirror].rdev->bdev,\r\nb),\r\n(unsigned long long)r1_bio->sector);\r\nset_bit(R1BIO_ReadError, &r1_bio->state);\r\nreschedule_retry(r1_bio);\r\n}\r\n}\r\nstatic void close_write(struct r1bio *r1_bio)\r\n{\r\nif (test_bit(R1BIO_BehindIO, &r1_bio->state)) {\r\nint i = r1_bio->behind_page_count;\r\nwhile (i--)\r\nsafe_put_page(r1_bio->behind_bvecs[i].bv_page);\r\nkfree(r1_bio->behind_bvecs);\r\nr1_bio->behind_bvecs = NULL;\r\n}\r\nbitmap_endwrite(r1_bio->mddev->bitmap, r1_bio->sector,\r\nr1_bio->sectors,\r\n!test_bit(R1BIO_Degraded, &r1_bio->state),\r\ntest_bit(R1BIO_BehindIO, &r1_bio->state));\r\nmd_write_end(r1_bio->mddev);\r\n}\r\nstatic void r1_bio_write_done(struct r1bio *r1_bio)\r\n{\r\nif (!atomic_dec_and_test(&r1_bio->remaining))\r\nreturn;\r\nif (test_bit(R1BIO_WriteError, &r1_bio->state))\r\nreschedule_retry(r1_bio);\r\nelse {\r\nclose_write(r1_bio);\r\nif (test_bit(R1BIO_MadeGood, &r1_bio->state))\r\nreschedule_retry(r1_bio);\r\nelse\r\nraid_end_bio_io(r1_bio);\r\n}\r\n}\r\nstatic void raid1_end_write_request(struct bio *bio, int error)\r\n{\r\nint uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nstruct r1bio *r1_bio = bio->bi_private;\r\nint mirror, behind = test_bit(R1BIO_BehindIO, &r1_bio->state);\r\nstruct r1conf *conf = r1_bio->mddev->private;\r\nstruct bio *to_put = NULL;\r\nmirror = find_bio_disk(r1_bio, bio);\r\nif (!uptodate) {\r\nset_bit(WriteErrorSeen,\r\n&conf->mirrors[mirror].rdev->flags);\r\nif (!test_and_set_bit(WantReplacement,\r\n&conf->mirrors[mirror].rdev->flags))\r\nset_bit(MD_RECOVERY_NEEDED, &\r\nconf->mddev->recovery);\r\nset_bit(R1BIO_WriteError, &r1_bio->state);\r\n} else {\r\nsector_t first_bad;\r\nint bad_sectors;\r\nr1_bio->bios[mirror] = NULL;\r\nto_put = bio;\r\nif (test_bit(In_sync, &conf->mirrors[mirror].rdev->flags) &&\r\n!test_bit(Faulty, &conf->mirrors[mirror].rdev->flags))\r\nset_bit(R1BIO_Uptodate, &r1_bio->state);\r\nif (is_badblock(conf->mirrors[mirror].rdev,\r\nr1_bio->sector, r1_bio->sectors,\r\n&first_bad, &bad_sectors)) {\r\nr1_bio->bios[mirror] = IO_MADE_GOOD;\r\nset_bit(R1BIO_MadeGood, &r1_bio->state);\r\n}\r\n}\r\nif (behind) {\r\nif (test_bit(WriteMostly, &conf->mirrors[mirror].rdev->flags))\r\natomic_dec(&r1_bio->behind_remaining);\r\nif (atomic_read(&r1_bio->behind_remaining) >= (atomic_read(&r1_bio->remaining)-1) &&\r\ntest_bit(R1BIO_Uptodate, &r1_bio->state)) {\r\nif (!test_and_set_bit(R1BIO_Returned, &r1_bio->state)) {\r\nstruct bio *mbio = r1_bio->master_bio;\r\npr_debug("raid1: behind end write sectors"\r\n" %llu-%llu\n",\r\n(unsigned long long) mbio->bi_iter.bi_sector,\r\n(unsigned long long) bio_end_sector(mbio) - 1);\r\ncall_bio_endio(r1_bio);\r\n}\r\n}\r\n}\r\nif (r1_bio->bios[mirror] == NULL)\r\nrdev_dec_pending(conf->mirrors[mirror].rdev,\r\nconf->mddev);\r\nr1_bio_write_done(r1_bio);\r\nif (to_put)\r\nbio_put(to_put);\r\n}\r\nstatic int read_balance(struct r1conf *conf, struct r1bio *r1_bio, int *max_sectors)\r\n{\r\nconst sector_t this_sector = r1_bio->sector;\r\nint sectors;\r\nint best_good_sectors;\r\nint best_disk, best_dist_disk, best_pending_disk;\r\nint has_nonrot_disk;\r\nint disk;\r\nsector_t best_dist;\r\nunsigned int min_pending;\r\nstruct md_rdev *rdev;\r\nint choose_first;\r\nint choose_next_idle;\r\nrcu_read_lock();\r\nretry:\r\nsectors = r1_bio->sectors;\r\nbest_disk = -1;\r\nbest_dist_disk = -1;\r\nbest_dist = MaxSector;\r\nbest_pending_disk = -1;\r\nmin_pending = UINT_MAX;\r\nbest_good_sectors = 0;\r\nhas_nonrot_disk = 0;\r\nchoose_next_idle = 0;\r\nchoose_first = (conf->mddev->recovery_cp < this_sector + sectors);\r\nfor (disk = 0 ; disk < conf->raid_disks * 2 ; disk++) {\r\nsector_t dist;\r\nsector_t first_bad;\r\nint bad_sectors;\r\nunsigned int pending;\r\nbool nonrot;\r\nrdev = rcu_dereference(conf->mirrors[disk].rdev);\r\nif (r1_bio->bios[disk] == IO_BLOCKED\r\n|| rdev == NULL\r\n|| test_bit(Unmerged, &rdev->flags)\r\n|| test_bit(Faulty, &rdev->flags))\r\ncontinue;\r\nif (!test_bit(In_sync, &rdev->flags) &&\r\nrdev->recovery_offset < this_sector + sectors)\r\ncontinue;\r\nif (test_bit(WriteMostly, &rdev->flags)) {\r\nif (best_disk < 0) {\r\nif (is_badblock(rdev, this_sector, sectors,\r\n&first_bad, &bad_sectors)) {\r\nif (first_bad < this_sector)\r\ncontinue;\r\nbest_good_sectors = first_bad - this_sector;\r\n} else\r\nbest_good_sectors = sectors;\r\nbest_disk = disk;\r\n}\r\ncontinue;\r\n}\r\nif (is_badblock(rdev, this_sector, sectors,\r\n&first_bad, &bad_sectors)) {\r\nif (best_dist < MaxSector)\r\ncontinue;\r\nif (first_bad <= this_sector) {\r\nbad_sectors -= (this_sector - first_bad);\r\nif (choose_first && sectors > bad_sectors)\r\nsectors = bad_sectors;\r\nif (best_good_sectors > sectors)\r\nbest_good_sectors = sectors;\r\n} else {\r\nsector_t good_sectors = first_bad - this_sector;\r\nif (good_sectors > best_good_sectors) {\r\nbest_good_sectors = good_sectors;\r\nbest_disk = disk;\r\n}\r\nif (choose_first)\r\nbreak;\r\n}\r\ncontinue;\r\n} else\r\nbest_good_sectors = sectors;\r\nnonrot = blk_queue_nonrot(bdev_get_queue(rdev->bdev));\r\nhas_nonrot_disk |= nonrot;\r\npending = atomic_read(&rdev->nr_pending);\r\ndist = abs(this_sector - conf->mirrors[disk].head_position);\r\nif (choose_first) {\r\nbest_disk = disk;\r\nbreak;\r\n}\r\nif (conf->mirrors[disk].next_seq_sect == this_sector\r\n|| dist == 0) {\r\nint opt_iosize = bdev_io_opt(rdev->bdev) >> 9;\r\nstruct raid1_info *mirror = &conf->mirrors[disk];\r\nbest_disk = disk;\r\nif (nonrot && opt_iosize > 0 &&\r\nmirror->seq_start != MaxSector &&\r\nmirror->next_seq_sect > opt_iosize &&\r\nmirror->next_seq_sect - opt_iosize >=\r\nmirror->seq_start) {\r\nchoose_next_idle = 1;\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\nif (pending == 0) {\r\nbest_disk = disk;\r\nbreak;\r\n}\r\nif (choose_next_idle)\r\ncontinue;\r\nif (min_pending > pending) {\r\nmin_pending = pending;\r\nbest_pending_disk = disk;\r\n}\r\nif (dist < best_dist) {\r\nbest_dist = dist;\r\nbest_dist_disk = disk;\r\n}\r\n}\r\nif (best_disk == -1) {\r\nif (has_nonrot_disk)\r\nbest_disk = best_pending_disk;\r\nelse\r\nbest_disk = best_dist_disk;\r\n}\r\nif (best_disk >= 0) {\r\nrdev = rcu_dereference(conf->mirrors[best_disk].rdev);\r\nif (!rdev)\r\ngoto retry;\r\natomic_inc(&rdev->nr_pending);\r\nif (test_bit(Faulty, &rdev->flags)) {\r\nrdev_dec_pending(rdev, conf->mddev);\r\ngoto retry;\r\n}\r\nsectors = best_good_sectors;\r\nif (conf->mirrors[best_disk].next_seq_sect != this_sector)\r\nconf->mirrors[best_disk].seq_start = this_sector;\r\nconf->mirrors[best_disk].next_seq_sect = this_sector + sectors;\r\n}\r\nrcu_read_unlock();\r\n*max_sectors = sectors;\r\nreturn best_disk;\r\n}\r\nstatic int raid1_mergeable_bvec(struct request_queue *q,\r\nstruct bvec_merge_data *bvm,\r\nstruct bio_vec *biovec)\r\n{\r\nstruct mddev *mddev = q->queuedata;\r\nstruct r1conf *conf = mddev->private;\r\nsector_t sector = bvm->bi_sector + get_start_sect(bvm->bi_bdev);\r\nint max = biovec->bv_len;\r\nif (mddev->merge_check_needed) {\r\nint disk;\r\nrcu_read_lock();\r\nfor (disk = 0; disk < conf->raid_disks * 2; disk++) {\r\nstruct md_rdev *rdev = rcu_dereference(\r\nconf->mirrors[disk].rdev);\r\nif (rdev && !test_bit(Faulty, &rdev->flags)) {\r\nstruct request_queue *q =\r\nbdev_get_queue(rdev->bdev);\r\nif (q->merge_bvec_fn) {\r\nbvm->bi_sector = sector +\r\nrdev->data_offset;\r\nbvm->bi_bdev = rdev->bdev;\r\nmax = min(max, q->merge_bvec_fn(\r\nq, bvm, biovec));\r\n}\r\n}\r\n}\r\nrcu_read_unlock();\r\n}\r\nreturn max;\r\n}\r\nint md_raid1_congested(struct mddev *mddev, int bits)\r\n{\r\nstruct r1conf *conf = mddev->private;\r\nint i, ret = 0;\r\nif ((bits & (1 << BDI_async_congested)) &&\r\nconf->pending_count >= max_queued_requests)\r\nreturn 1;\r\nrcu_read_lock();\r\nfor (i = 0; i < conf->raid_disks * 2; i++) {\r\nstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\r\nif (rdev && !test_bit(Faulty, &rdev->flags)) {\r\nstruct request_queue *q = bdev_get_queue(rdev->bdev);\r\nBUG_ON(!q);\r\nif ((bits & (1<<BDI_async_congested)) || 1)\r\nret |= bdi_congested(&q->backing_dev_info, bits);\r\nelse\r\nret &= bdi_congested(&q->backing_dev_info, bits);\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int raid1_congested(void *data, int bits)\r\n{\r\nstruct mddev *mddev = data;\r\nreturn mddev_congested(mddev, bits) ||\r\nmd_raid1_congested(mddev, bits);\r\n}\r\nstatic void flush_pending_writes(struct r1conf *conf)\r\n{\r\nspin_lock_irq(&conf->device_lock);\r\nif (conf->pending_bio_list.head) {\r\nstruct bio *bio;\r\nbio = bio_list_get(&conf->pending_bio_list);\r\nconf->pending_count = 0;\r\nspin_unlock_irq(&conf->device_lock);\r\nbitmap_unplug(conf->mddev->bitmap);\r\nwake_up(&conf->wait_barrier);\r\nwhile (bio) {\r\nstruct bio *next = bio->bi_next;\r\nbio->bi_next = NULL;\r\nif (unlikely((bio->bi_rw & REQ_DISCARD) &&\r\n!blk_queue_discard(bdev_get_queue(bio->bi_bdev))))\r\nbio_endio(bio, 0);\r\nelse\r\ngeneric_make_request(bio);\r\nbio = next;\r\n}\r\n} else\r\nspin_unlock_irq(&conf->device_lock);\r\n}\r\nstatic void raise_barrier(struct r1conf *conf, sector_t sector_nr)\r\n{\r\nspin_lock_irq(&conf->resync_lock);\r\nwait_event_lock_irq(conf->wait_barrier, !conf->nr_waiting,\r\nconf->resync_lock);\r\nconf->barrier++;\r\nconf->next_resync = sector_nr;\r\nwait_event_lock_irq(conf->wait_barrier,\r\n!conf->array_frozen &&\r\nconf->barrier < RESYNC_DEPTH &&\r\nconf->current_window_requests == 0 &&\r\n(conf->start_next_window >=\r\nconf->next_resync + RESYNC_SECTORS),\r\nconf->resync_lock);\r\nconf->nr_pending++;\r\nspin_unlock_irq(&conf->resync_lock);\r\n}\r\nstatic void lower_barrier(struct r1conf *conf)\r\n{\r\nunsigned long flags;\r\nBUG_ON(conf->barrier <= 0);\r\nspin_lock_irqsave(&conf->resync_lock, flags);\r\nconf->barrier--;\r\nconf->nr_pending--;\r\nspin_unlock_irqrestore(&conf->resync_lock, flags);\r\nwake_up(&conf->wait_barrier);\r\n}\r\nstatic bool need_to_wait_for_sync(struct r1conf *conf, struct bio *bio)\r\n{\r\nbool wait = false;\r\nif (conf->array_frozen || !bio)\r\nwait = true;\r\nelse if (conf->barrier && bio_data_dir(bio) == WRITE) {\r\nif ((conf->mddev->curr_resync_completed\r\n>= bio_end_sector(bio)) ||\r\n(conf->next_resync + NEXT_NORMALIO_DISTANCE\r\n<= bio->bi_iter.bi_sector))\r\nwait = false;\r\nelse\r\nwait = true;\r\n}\r\nreturn wait;\r\n}\r\nstatic sector_t wait_barrier(struct r1conf *conf, struct bio *bio)\r\n{\r\nsector_t sector = 0;\r\nspin_lock_irq(&conf->resync_lock);\r\nif (need_to_wait_for_sync(conf, bio)) {\r\nconf->nr_waiting++;\r\nwait_event_lock_irq(conf->wait_barrier,\r\n!conf->array_frozen &&\r\n(!conf->barrier ||\r\n((conf->start_next_window <\r\nconf->next_resync + RESYNC_SECTORS) &&\r\ncurrent->bio_list &&\r\n!bio_list_empty(current->bio_list))),\r\nconf->resync_lock);\r\nconf->nr_waiting--;\r\n}\r\nif (bio && bio_data_dir(bio) == WRITE) {\r\nif (bio->bi_iter.bi_sector >=\r\nconf->mddev->curr_resync_completed) {\r\nif (conf->start_next_window == MaxSector)\r\nconf->start_next_window =\r\nconf->next_resync +\r\nNEXT_NORMALIO_DISTANCE;\r\nif ((conf->start_next_window + NEXT_NORMALIO_DISTANCE)\r\n<= bio->bi_iter.bi_sector)\r\nconf->next_window_requests++;\r\nelse\r\nconf->current_window_requests++;\r\nsector = conf->start_next_window;\r\n}\r\n}\r\nconf->nr_pending++;\r\nspin_unlock_irq(&conf->resync_lock);\r\nreturn sector;\r\n}\r\nstatic void allow_barrier(struct r1conf *conf, sector_t start_next_window,\r\nsector_t bi_sector)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&conf->resync_lock, flags);\r\nconf->nr_pending--;\r\nif (start_next_window) {\r\nif (start_next_window == conf->start_next_window) {\r\nif (conf->start_next_window + NEXT_NORMALIO_DISTANCE\r\n<= bi_sector)\r\nconf->next_window_requests--;\r\nelse\r\nconf->current_window_requests--;\r\n} else\r\nconf->current_window_requests--;\r\nif (!conf->current_window_requests) {\r\nif (conf->next_window_requests) {\r\nconf->current_window_requests =\r\nconf->next_window_requests;\r\nconf->next_window_requests = 0;\r\nconf->start_next_window +=\r\nNEXT_NORMALIO_DISTANCE;\r\n} else\r\nconf->start_next_window = MaxSector;\r\n}\r\n}\r\nspin_unlock_irqrestore(&conf->resync_lock, flags);\r\nwake_up(&conf->wait_barrier);\r\n}\r\nstatic void freeze_array(struct r1conf *conf, int extra)\r\n{\r\nspin_lock_irq(&conf->resync_lock);\r\nconf->array_frozen = 1;\r\nwait_event_lock_irq_cmd(conf->wait_barrier,\r\nconf->nr_pending == conf->nr_queued+extra,\r\nconf->resync_lock,\r\nflush_pending_writes(conf));\r\nspin_unlock_irq(&conf->resync_lock);\r\n}\r\nstatic void unfreeze_array(struct r1conf *conf)\r\n{\r\nspin_lock_irq(&conf->resync_lock);\r\nconf->array_frozen = 0;\r\nwake_up(&conf->wait_barrier);\r\nspin_unlock_irq(&conf->resync_lock);\r\n}\r\nstatic void alloc_behind_pages(struct bio *bio, struct r1bio *r1_bio)\r\n{\r\nint i;\r\nstruct bio_vec *bvec;\r\nstruct bio_vec *bvecs = kzalloc(bio->bi_vcnt * sizeof(struct bio_vec),\r\nGFP_NOIO);\r\nif (unlikely(!bvecs))\r\nreturn;\r\nbio_for_each_segment_all(bvec, bio, i) {\r\nbvecs[i] = *bvec;\r\nbvecs[i].bv_page = alloc_page(GFP_NOIO);\r\nif (unlikely(!bvecs[i].bv_page))\r\ngoto do_sync_io;\r\nmemcpy(kmap(bvecs[i].bv_page) + bvec->bv_offset,\r\nkmap(bvec->bv_page) + bvec->bv_offset, bvec->bv_len);\r\nkunmap(bvecs[i].bv_page);\r\nkunmap(bvec->bv_page);\r\n}\r\nr1_bio->behind_bvecs = bvecs;\r\nr1_bio->behind_page_count = bio->bi_vcnt;\r\nset_bit(R1BIO_BehindIO, &r1_bio->state);\r\nreturn;\r\ndo_sync_io:\r\nfor (i = 0; i < bio->bi_vcnt; i++)\r\nif (bvecs[i].bv_page)\r\nput_page(bvecs[i].bv_page);\r\nkfree(bvecs);\r\npr_debug("%dB behind alloc failed, doing sync I/O\n",\r\nbio->bi_iter.bi_size);\r\n}\r\nstatic void raid1_unplug(struct blk_plug_cb *cb, bool from_schedule)\r\n{\r\nstruct raid1_plug_cb *plug = container_of(cb, struct raid1_plug_cb,\r\ncb);\r\nstruct mddev *mddev = plug->cb.data;\r\nstruct r1conf *conf = mddev->private;\r\nstruct bio *bio;\r\nif (from_schedule || current->bio_list) {\r\nspin_lock_irq(&conf->device_lock);\r\nbio_list_merge(&conf->pending_bio_list, &plug->pending);\r\nconf->pending_count += plug->pending_cnt;\r\nspin_unlock_irq(&conf->device_lock);\r\nwake_up(&conf->wait_barrier);\r\nmd_wakeup_thread(mddev->thread);\r\nkfree(plug);\r\nreturn;\r\n}\r\nbio = bio_list_get(&plug->pending);\r\nbitmap_unplug(mddev->bitmap);\r\nwake_up(&conf->wait_barrier);\r\nwhile (bio) {\r\nstruct bio *next = bio->bi_next;\r\nbio->bi_next = NULL;\r\nif (unlikely((bio->bi_rw & REQ_DISCARD) &&\r\n!blk_queue_discard(bdev_get_queue(bio->bi_bdev))))\r\nbio_endio(bio, 0);\r\nelse\r\ngeneric_make_request(bio);\r\nbio = next;\r\n}\r\nkfree(plug);\r\n}\r\nstatic void make_request(struct mddev *mddev, struct bio * bio)\r\n{\r\nstruct r1conf *conf = mddev->private;\r\nstruct raid1_info *mirror;\r\nstruct r1bio *r1_bio;\r\nstruct bio *read_bio;\r\nint i, disks;\r\nstruct bitmap *bitmap;\r\nunsigned long flags;\r\nconst int rw = bio_data_dir(bio);\r\nconst unsigned long do_sync = (bio->bi_rw & REQ_SYNC);\r\nconst unsigned long do_flush_fua = (bio->bi_rw & (REQ_FLUSH | REQ_FUA));\r\nconst unsigned long do_discard = (bio->bi_rw\r\n& (REQ_DISCARD | REQ_SECURE));\r\nconst unsigned long do_same = (bio->bi_rw & REQ_WRITE_SAME);\r\nstruct md_rdev *blocked_rdev;\r\nstruct blk_plug_cb *cb;\r\nstruct raid1_plug_cb *plug = NULL;\r\nint first_clone;\r\nint sectors_handled;\r\nint max_sectors;\r\nsector_t start_next_window;\r\nmd_write_start(mddev, bio);\r\nif (bio_data_dir(bio) == WRITE &&\r\nbio_end_sector(bio) > mddev->suspend_lo &&\r\nbio->bi_iter.bi_sector < mddev->suspend_hi) {\r\nDEFINE_WAIT(w);\r\nfor (;;) {\r\nflush_signals(current);\r\nprepare_to_wait(&conf->wait_barrier,\r\n&w, TASK_INTERRUPTIBLE);\r\nif (bio_end_sector(bio) <= mddev->suspend_lo ||\r\nbio->bi_iter.bi_sector >= mddev->suspend_hi)\r\nbreak;\r\nschedule();\r\n}\r\nfinish_wait(&conf->wait_barrier, &w);\r\n}\r\nstart_next_window = wait_barrier(conf, bio);\r\nbitmap = mddev->bitmap;\r\nr1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);\r\nr1_bio->master_bio = bio;\r\nr1_bio->sectors = bio_sectors(bio);\r\nr1_bio->state = 0;\r\nr1_bio->mddev = mddev;\r\nr1_bio->sector = bio->bi_iter.bi_sector;\r\nbio->bi_phys_segments = 0;\r\nclear_bit(BIO_SEG_VALID, &bio->bi_flags);\r\nif (rw == READ) {\r\nint rdisk;\r\nread_again:\r\nrdisk = read_balance(conf, r1_bio, &max_sectors);\r\nif (rdisk < 0) {\r\nraid_end_bio_io(r1_bio);\r\nreturn;\r\n}\r\nmirror = conf->mirrors + rdisk;\r\nif (test_bit(WriteMostly, &mirror->rdev->flags) &&\r\nbitmap) {\r\nwait_event(bitmap->behind_wait,\r\natomic_read(&bitmap->behind_writes) == 0);\r\n}\r\nr1_bio->read_disk = rdisk;\r\nr1_bio->start_next_window = 0;\r\nread_bio = bio_clone_mddev(bio, GFP_NOIO, mddev);\r\nbio_trim(read_bio, r1_bio->sector - bio->bi_iter.bi_sector,\r\nmax_sectors);\r\nr1_bio->bios[rdisk] = read_bio;\r\nread_bio->bi_iter.bi_sector = r1_bio->sector +\r\nmirror->rdev->data_offset;\r\nread_bio->bi_bdev = mirror->rdev->bdev;\r\nread_bio->bi_end_io = raid1_end_read_request;\r\nread_bio->bi_rw = READ | do_sync;\r\nread_bio->bi_private = r1_bio;\r\nif (max_sectors < r1_bio->sectors) {\r\nsectors_handled = (r1_bio->sector + max_sectors\r\n- bio->bi_iter.bi_sector);\r\nr1_bio->sectors = max_sectors;\r\nspin_lock_irq(&conf->device_lock);\r\nif (bio->bi_phys_segments == 0)\r\nbio->bi_phys_segments = 2;\r\nelse\r\nbio->bi_phys_segments++;\r\nspin_unlock_irq(&conf->device_lock);\r\nreschedule_retry(r1_bio);\r\nr1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);\r\nr1_bio->master_bio = bio;\r\nr1_bio->sectors = bio_sectors(bio) - sectors_handled;\r\nr1_bio->state = 0;\r\nr1_bio->mddev = mddev;\r\nr1_bio->sector = bio->bi_iter.bi_sector +\r\nsectors_handled;\r\ngoto read_again;\r\n} else\r\ngeneric_make_request(read_bio);\r\nreturn;\r\n}\r\nif (conf->pending_count >= max_queued_requests) {\r\nmd_wakeup_thread(mddev->thread);\r\nwait_event(conf->wait_barrier,\r\nconf->pending_count < max_queued_requests);\r\n}\r\ndisks = conf->raid_disks * 2;\r\nretry_write:\r\nr1_bio->start_next_window = start_next_window;\r\nblocked_rdev = NULL;\r\nrcu_read_lock();\r\nmax_sectors = r1_bio->sectors;\r\nfor (i = 0; i < disks; i++) {\r\nstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\r\nif (rdev && unlikely(test_bit(Blocked, &rdev->flags))) {\r\natomic_inc(&rdev->nr_pending);\r\nblocked_rdev = rdev;\r\nbreak;\r\n}\r\nr1_bio->bios[i] = NULL;\r\nif (!rdev || test_bit(Faulty, &rdev->flags)\r\n|| test_bit(Unmerged, &rdev->flags)) {\r\nif (i < conf->raid_disks)\r\nset_bit(R1BIO_Degraded, &r1_bio->state);\r\ncontinue;\r\n}\r\natomic_inc(&rdev->nr_pending);\r\nif (test_bit(WriteErrorSeen, &rdev->flags)) {\r\nsector_t first_bad;\r\nint bad_sectors;\r\nint is_bad;\r\nis_bad = is_badblock(rdev, r1_bio->sector,\r\nmax_sectors,\r\n&first_bad, &bad_sectors);\r\nif (is_bad < 0) {\r\nset_bit(BlockedBadBlocks, &rdev->flags);\r\nblocked_rdev = rdev;\r\nbreak;\r\n}\r\nif (is_bad && first_bad <= r1_bio->sector) {\r\nbad_sectors -= (r1_bio->sector - first_bad);\r\nif (bad_sectors < max_sectors)\r\nmax_sectors = bad_sectors;\r\nrdev_dec_pending(rdev, mddev);\r\ncontinue;\r\n}\r\nif (is_bad) {\r\nint good_sectors = first_bad - r1_bio->sector;\r\nif (good_sectors < max_sectors)\r\nmax_sectors = good_sectors;\r\n}\r\n}\r\nr1_bio->bios[i] = bio;\r\n}\r\nrcu_read_unlock();\r\nif (unlikely(blocked_rdev)) {\r\nint j;\r\nsector_t old = start_next_window;\r\nfor (j = 0; j < i; j++)\r\nif (r1_bio->bios[j])\r\nrdev_dec_pending(conf->mirrors[j].rdev, mddev);\r\nr1_bio->state = 0;\r\nallow_barrier(conf, start_next_window, bio->bi_iter.bi_sector);\r\nmd_wait_for_blocked_rdev(blocked_rdev, mddev);\r\nstart_next_window = wait_barrier(conf, bio);\r\nif (bio->bi_phys_segments && old &&\r\nold != start_next_window)\r\nwait_event(conf->wait_barrier,\r\nbio->bi_phys_segments == 1);\r\ngoto retry_write;\r\n}\r\nif (max_sectors < r1_bio->sectors) {\r\nr1_bio->sectors = max_sectors;\r\nspin_lock_irq(&conf->device_lock);\r\nif (bio->bi_phys_segments == 0)\r\nbio->bi_phys_segments = 2;\r\nelse\r\nbio->bi_phys_segments++;\r\nspin_unlock_irq(&conf->device_lock);\r\n}\r\nsectors_handled = r1_bio->sector + max_sectors - bio->bi_iter.bi_sector;\r\natomic_set(&r1_bio->remaining, 1);\r\natomic_set(&r1_bio->behind_remaining, 0);\r\nfirst_clone = 1;\r\nfor (i = 0; i < disks; i++) {\r\nstruct bio *mbio;\r\nif (!r1_bio->bios[i])\r\ncontinue;\r\nmbio = bio_clone_mddev(bio, GFP_NOIO, mddev);\r\nbio_trim(mbio, r1_bio->sector - bio->bi_iter.bi_sector, max_sectors);\r\nif (first_clone) {\r\nif (bitmap &&\r\n(atomic_read(&bitmap->behind_writes)\r\n< mddev->bitmap_info.max_write_behind) &&\r\n!waitqueue_active(&bitmap->behind_wait))\r\nalloc_behind_pages(mbio, r1_bio);\r\nbitmap_startwrite(bitmap, r1_bio->sector,\r\nr1_bio->sectors,\r\ntest_bit(R1BIO_BehindIO,\r\n&r1_bio->state));\r\nfirst_clone = 0;\r\n}\r\nif (r1_bio->behind_bvecs) {\r\nstruct bio_vec *bvec;\r\nint j;\r\nbio_for_each_segment_all(bvec, mbio, j)\r\nbvec->bv_page = r1_bio->behind_bvecs[j].bv_page;\r\nif (test_bit(WriteMostly, &conf->mirrors[i].rdev->flags))\r\natomic_inc(&r1_bio->behind_remaining);\r\n}\r\nr1_bio->bios[i] = mbio;\r\nmbio->bi_iter.bi_sector = (r1_bio->sector +\r\nconf->mirrors[i].rdev->data_offset);\r\nmbio->bi_bdev = conf->mirrors[i].rdev->bdev;\r\nmbio->bi_end_io = raid1_end_write_request;\r\nmbio->bi_rw =\r\nWRITE | do_flush_fua | do_sync | do_discard | do_same;\r\nmbio->bi_private = r1_bio;\r\natomic_inc(&r1_bio->remaining);\r\ncb = blk_check_plugged(raid1_unplug, mddev, sizeof(*plug));\r\nif (cb)\r\nplug = container_of(cb, struct raid1_plug_cb, cb);\r\nelse\r\nplug = NULL;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nif (plug) {\r\nbio_list_add(&plug->pending, mbio);\r\nplug->pending_cnt++;\r\n} else {\r\nbio_list_add(&conf->pending_bio_list, mbio);\r\nconf->pending_count++;\r\n}\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nif (!plug)\r\nmd_wakeup_thread(mddev->thread);\r\n}\r\nif (sectors_handled < bio_sectors(bio)) {\r\nr1_bio_write_done(r1_bio);\r\nr1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);\r\nr1_bio->master_bio = bio;\r\nr1_bio->sectors = bio_sectors(bio) - sectors_handled;\r\nr1_bio->state = 0;\r\nr1_bio->mddev = mddev;\r\nr1_bio->sector = bio->bi_iter.bi_sector + sectors_handled;\r\ngoto retry_write;\r\n}\r\nr1_bio_write_done(r1_bio);\r\nwake_up(&conf->wait_barrier);\r\n}\r\nstatic void status(struct seq_file *seq, struct mddev *mddev)\r\n{\r\nstruct r1conf *conf = mddev->private;\r\nint i;\r\nseq_printf(seq, " [%d/%d] [", conf->raid_disks,\r\nconf->raid_disks - mddev->degraded);\r\nrcu_read_lock();\r\nfor (i = 0; i < conf->raid_disks; i++) {\r\nstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\r\nseq_printf(seq, "%s",\r\nrdev && test_bit(In_sync, &rdev->flags) ? "U" : "_");\r\n}\r\nrcu_read_unlock();\r\nseq_printf(seq, "]");\r\n}\r\nstatic void error(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nchar b[BDEVNAME_SIZE];\r\nstruct r1conf *conf = mddev->private;\r\nif (test_bit(In_sync, &rdev->flags)\r\n&& (conf->raid_disks - mddev->degraded) == 1) {\r\nconf->recovery_disabled = mddev->recovery_disabled;\r\nreturn;\r\n}\r\nset_bit(Blocked, &rdev->flags);\r\nif (test_and_clear_bit(In_sync, &rdev->flags)) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nmddev->degraded++;\r\nset_bit(Faulty, &rdev->flags);\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\n} else\r\nset_bit(Faulty, &rdev->flags);\r\nset_bit(MD_RECOVERY_INTR, &mddev->recovery);\r\nset_bit(MD_CHANGE_DEVS, &mddev->flags);\r\nprintk(KERN_ALERT\r\n"md/raid1:%s: Disk failure on %s, disabling device.\n"\r\n"md/raid1:%s: Operation continuing on %d devices.\n",\r\nmdname(mddev), bdevname(rdev->bdev, b),\r\nmdname(mddev), conf->raid_disks - mddev->degraded);\r\n}\r\nstatic void print_conf(struct r1conf *conf)\r\n{\r\nint i;\r\nprintk(KERN_DEBUG "RAID1 conf printout:\n");\r\nif (!conf) {\r\nprintk(KERN_DEBUG "(!conf)\n");\r\nreturn;\r\n}\r\nprintk(KERN_DEBUG " --- wd:%d rd:%d\n", conf->raid_disks - conf->mddev->degraded,\r\nconf->raid_disks);\r\nrcu_read_lock();\r\nfor (i = 0; i < conf->raid_disks; i++) {\r\nchar b[BDEVNAME_SIZE];\r\nstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\r\nif (rdev)\r\nprintk(KERN_DEBUG " disk %d, wo:%d, o:%d, dev:%s\n",\r\ni, !test_bit(In_sync, &rdev->flags),\r\n!test_bit(Faulty, &rdev->flags),\r\nbdevname(rdev->bdev,b));\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic void close_sync(struct r1conf *conf)\r\n{\r\nwait_barrier(conf, NULL);\r\nallow_barrier(conf, 0, 0);\r\nmempool_destroy(conf->r1buf_pool);\r\nconf->r1buf_pool = NULL;\r\nspin_lock_irq(&conf->resync_lock);\r\nconf->next_resync = 0;\r\nconf->start_next_window = MaxSector;\r\nconf->current_window_requests +=\r\nconf->next_window_requests;\r\nconf->next_window_requests = 0;\r\nspin_unlock_irq(&conf->resync_lock);\r\n}\r\nstatic int raid1_spare_active(struct mddev *mddev)\r\n{\r\nint i;\r\nstruct r1conf *conf = mddev->private;\r\nint count = 0;\r\nunsigned long flags;\r\nfor (i = 0; i < conf->raid_disks; i++) {\r\nstruct md_rdev *rdev = conf->mirrors[i].rdev;\r\nstruct md_rdev *repl = conf->mirrors[conf->raid_disks + i].rdev;\r\nif (repl\r\n&& repl->recovery_offset == MaxSector\r\n&& !test_bit(Faulty, &repl->flags)\r\n&& !test_and_set_bit(In_sync, &repl->flags)) {\r\nif (!rdev ||\r\n!test_and_clear_bit(In_sync, &rdev->flags))\r\ncount++;\r\nif (rdev) {\r\nset_bit(Faulty, &rdev->flags);\r\nsysfs_notify_dirent_safe(\r\nrdev->sysfs_state);\r\n}\r\n}\r\nif (rdev\r\n&& rdev->recovery_offset == MaxSector\r\n&& !test_bit(Faulty, &rdev->flags)\r\n&& !test_and_set_bit(In_sync, &rdev->flags)) {\r\ncount++;\r\nsysfs_notify_dirent_safe(rdev->sysfs_state);\r\n}\r\n}\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nmddev->degraded -= count;\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nprint_conf(conf);\r\nreturn count;\r\n}\r\nstatic int raid1_add_disk(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nstruct r1conf *conf = mddev->private;\r\nint err = -EEXIST;\r\nint mirror = 0;\r\nstruct raid1_info *p;\r\nint first = 0;\r\nint last = conf->raid_disks - 1;\r\nstruct request_queue *q = bdev_get_queue(rdev->bdev);\r\nif (mddev->recovery_disabled == conf->recovery_disabled)\r\nreturn -EBUSY;\r\nif (rdev->raid_disk >= 0)\r\nfirst = last = rdev->raid_disk;\r\nif (q->merge_bvec_fn) {\r\nset_bit(Unmerged, &rdev->flags);\r\nmddev->merge_check_needed = 1;\r\n}\r\nfor (mirror = first; mirror <= last; mirror++) {\r\np = conf->mirrors+mirror;\r\nif (!p->rdev) {\r\nif (mddev->gendisk)\r\ndisk_stack_limits(mddev->gendisk, rdev->bdev,\r\nrdev->data_offset << 9);\r\np->head_position = 0;\r\nrdev->raid_disk = mirror;\r\nerr = 0;\r\nif (rdev->saved_raid_disk < 0)\r\nconf->fullsync = 1;\r\nrcu_assign_pointer(p->rdev, rdev);\r\nbreak;\r\n}\r\nif (test_bit(WantReplacement, &p->rdev->flags) &&\r\np[conf->raid_disks].rdev == NULL) {\r\nclear_bit(In_sync, &rdev->flags);\r\nset_bit(Replacement, &rdev->flags);\r\nrdev->raid_disk = mirror;\r\nerr = 0;\r\nconf->fullsync = 1;\r\nrcu_assign_pointer(p[conf->raid_disks].rdev, rdev);\r\nbreak;\r\n}\r\n}\r\nif (err == 0 && test_bit(Unmerged, &rdev->flags)) {\r\nsynchronize_sched();\r\nfreeze_array(conf, 0);\r\nunfreeze_array(conf);\r\nclear_bit(Unmerged, &rdev->flags);\r\n}\r\nmd_integrity_add_rdev(rdev, mddev);\r\nif (mddev->queue && blk_queue_discard(bdev_get_queue(rdev->bdev)))\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, mddev->queue);\r\nprint_conf(conf);\r\nreturn err;\r\n}\r\nstatic int raid1_remove_disk(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nstruct r1conf *conf = mddev->private;\r\nint err = 0;\r\nint number = rdev->raid_disk;\r\nstruct raid1_info *p = conf->mirrors + number;\r\nif (rdev != p->rdev)\r\np = conf->mirrors + conf->raid_disks + number;\r\nprint_conf(conf);\r\nif (rdev == p->rdev) {\r\nif (test_bit(In_sync, &rdev->flags) ||\r\natomic_read(&rdev->nr_pending)) {\r\nerr = -EBUSY;\r\ngoto abort;\r\n}\r\nif (!test_bit(Faulty, &rdev->flags) &&\r\nmddev->recovery_disabled != conf->recovery_disabled &&\r\nmddev->degraded < conf->raid_disks) {\r\nerr = -EBUSY;\r\ngoto abort;\r\n}\r\np->rdev = NULL;\r\nsynchronize_rcu();\r\nif (atomic_read(&rdev->nr_pending)) {\r\nerr = -EBUSY;\r\np->rdev = rdev;\r\ngoto abort;\r\n} else if (conf->mirrors[conf->raid_disks + number].rdev) {\r\nstruct md_rdev *repl =\r\nconf->mirrors[conf->raid_disks + number].rdev;\r\nfreeze_array(conf, 0);\r\nclear_bit(Replacement, &repl->flags);\r\np->rdev = repl;\r\nconf->mirrors[conf->raid_disks + number].rdev = NULL;\r\nunfreeze_array(conf);\r\nclear_bit(WantReplacement, &rdev->flags);\r\n} else\r\nclear_bit(WantReplacement, &rdev->flags);\r\nerr = md_integrity_register(mddev);\r\n}\r\nabort:\r\nprint_conf(conf);\r\nreturn err;\r\n}\r\nstatic void end_sync_read(struct bio *bio, int error)\r\n{\r\nstruct r1bio *r1_bio = bio->bi_private;\r\nupdate_head_pos(r1_bio->read_disk, r1_bio);\r\nif (test_bit(BIO_UPTODATE, &bio->bi_flags))\r\nset_bit(R1BIO_Uptodate, &r1_bio->state);\r\nif (atomic_dec_and_test(&r1_bio->remaining))\r\nreschedule_retry(r1_bio);\r\n}\r\nstatic void end_sync_write(struct bio *bio, int error)\r\n{\r\nint uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nstruct r1bio *r1_bio = bio->bi_private;\r\nstruct mddev *mddev = r1_bio->mddev;\r\nstruct r1conf *conf = mddev->private;\r\nint mirror=0;\r\nsector_t first_bad;\r\nint bad_sectors;\r\nmirror = find_bio_disk(r1_bio, bio);\r\nif (!uptodate) {\r\nsector_t sync_blocks = 0;\r\nsector_t s = r1_bio->sector;\r\nlong sectors_to_go = r1_bio->sectors;\r\ndo {\r\nbitmap_end_sync(mddev->bitmap, s,\r\n&sync_blocks, 1);\r\ns += sync_blocks;\r\nsectors_to_go -= sync_blocks;\r\n} while (sectors_to_go > 0);\r\nset_bit(WriteErrorSeen,\r\n&conf->mirrors[mirror].rdev->flags);\r\nif (!test_and_set_bit(WantReplacement,\r\n&conf->mirrors[mirror].rdev->flags))\r\nset_bit(MD_RECOVERY_NEEDED, &\r\nmddev->recovery);\r\nset_bit(R1BIO_WriteError, &r1_bio->state);\r\n} else if (is_badblock(conf->mirrors[mirror].rdev,\r\nr1_bio->sector,\r\nr1_bio->sectors,\r\n&first_bad, &bad_sectors) &&\r\n!is_badblock(conf->mirrors[r1_bio->read_disk].rdev,\r\nr1_bio->sector,\r\nr1_bio->sectors,\r\n&first_bad, &bad_sectors)\r\n)\r\nset_bit(R1BIO_MadeGood, &r1_bio->state);\r\nif (atomic_dec_and_test(&r1_bio->remaining)) {\r\nint s = r1_bio->sectors;\r\nif (test_bit(R1BIO_MadeGood, &r1_bio->state) ||\r\ntest_bit(R1BIO_WriteError, &r1_bio->state))\r\nreschedule_retry(r1_bio);\r\nelse {\r\nput_buf(r1_bio);\r\nmd_done_sync(mddev, s, uptodate);\r\n}\r\n}\r\n}\r\nstatic int r1_sync_page_io(struct md_rdev *rdev, sector_t sector,\r\nint sectors, struct page *page, int rw)\r\n{\r\nif (sync_page_io(rdev, sector, sectors << 9, page, rw, false))\r\nreturn 1;\r\nif (rw == WRITE) {\r\nset_bit(WriteErrorSeen, &rdev->flags);\r\nif (!test_and_set_bit(WantReplacement,\r\n&rdev->flags))\r\nset_bit(MD_RECOVERY_NEEDED, &\r\nrdev->mddev->recovery);\r\n}\r\nif (!rdev_set_badblocks(rdev, sector, sectors, 0))\r\nmd_error(rdev->mddev, rdev);\r\nreturn 0;\r\n}\r\nstatic int fix_sync_read_error(struct r1bio *r1_bio)\r\n{\r\nstruct mddev *mddev = r1_bio->mddev;\r\nstruct r1conf *conf = mddev->private;\r\nstruct bio *bio = r1_bio->bios[r1_bio->read_disk];\r\nsector_t sect = r1_bio->sector;\r\nint sectors = r1_bio->sectors;\r\nint idx = 0;\r\nwhile(sectors) {\r\nint s = sectors;\r\nint d = r1_bio->read_disk;\r\nint success = 0;\r\nstruct md_rdev *rdev;\r\nint start;\r\nif (s > (PAGE_SIZE>>9))\r\ns = PAGE_SIZE >> 9;\r\ndo {\r\nif (r1_bio->bios[d]->bi_end_io == end_sync_read) {\r\nrdev = conf->mirrors[d].rdev;\r\nif (sync_page_io(rdev, sect, s<<9,\r\nbio->bi_io_vec[idx].bv_page,\r\nREAD, false)) {\r\nsuccess = 1;\r\nbreak;\r\n}\r\n}\r\nd++;\r\nif (d == conf->raid_disks * 2)\r\nd = 0;\r\n} while (!success && d != r1_bio->read_disk);\r\nif (!success) {\r\nchar b[BDEVNAME_SIZE];\r\nint abort = 0;\r\nprintk(KERN_ALERT "md/raid1:%s: %s: unrecoverable I/O read error"\r\n" for block %llu\n",\r\nmdname(mddev),\r\nbdevname(bio->bi_bdev, b),\r\n(unsigned long long)r1_bio->sector);\r\nfor (d = 0; d < conf->raid_disks * 2; d++) {\r\nrdev = conf->mirrors[d].rdev;\r\nif (!rdev || test_bit(Faulty, &rdev->flags))\r\ncontinue;\r\nif (!rdev_set_badblocks(rdev, sect, s, 0))\r\nabort = 1;\r\n}\r\nif (abort) {\r\nconf->recovery_disabled =\r\nmddev->recovery_disabled;\r\nset_bit(MD_RECOVERY_INTR, &mddev->recovery);\r\nmd_done_sync(mddev, r1_bio->sectors, 0);\r\nput_buf(r1_bio);\r\nreturn 0;\r\n}\r\nsectors -= s;\r\nsect += s;\r\nidx++;\r\ncontinue;\r\n}\r\nstart = d;\r\nwhile (d != r1_bio->read_disk) {\r\nif (d == 0)\r\nd = conf->raid_disks * 2;\r\nd--;\r\nif (r1_bio->bios[d]->bi_end_io != end_sync_read)\r\ncontinue;\r\nrdev = conf->mirrors[d].rdev;\r\nif (r1_sync_page_io(rdev, sect, s,\r\nbio->bi_io_vec[idx].bv_page,\r\nWRITE) == 0) {\r\nr1_bio->bios[d]->bi_end_io = NULL;\r\nrdev_dec_pending(rdev, mddev);\r\n}\r\n}\r\nd = start;\r\nwhile (d != r1_bio->read_disk) {\r\nif (d == 0)\r\nd = conf->raid_disks * 2;\r\nd--;\r\nif (r1_bio->bios[d]->bi_end_io != end_sync_read)\r\ncontinue;\r\nrdev = conf->mirrors[d].rdev;\r\nif (r1_sync_page_io(rdev, sect, s,\r\nbio->bi_io_vec[idx].bv_page,\r\nREAD) != 0)\r\natomic_add(s, &rdev->corrected_errors);\r\n}\r\nsectors -= s;\r\nsect += s;\r\nidx ++;\r\n}\r\nset_bit(R1BIO_Uptodate, &r1_bio->state);\r\nset_bit(BIO_UPTODATE, &bio->bi_flags);\r\nreturn 1;\r\n}\r\nstatic void process_checks(struct r1bio *r1_bio)\r\n{\r\nstruct mddev *mddev = r1_bio->mddev;\r\nstruct r1conf *conf = mddev->private;\r\nint primary;\r\nint i;\r\nint vcnt;\r\nvcnt = (r1_bio->sectors + PAGE_SIZE / 512 - 1) >> (PAGE_SHIFT - 9);\r\nfor (i = 0; i < conf->raid_disks * 2; i++) {\r\nint j;\r\nint size;\r\nint uptodate;\r\nstruct bio *b = r1_bio->bios[i];\r\nif (b->bi_end_io != end_sync_read)\r\ncontinue;\r\nuptodate = test_bit(BIO_UPTODATE, &b->bi_flags);\r\nbio_reset(b);\r\nif (!uptodate)\r\nclear_bit(BIO_UPTODATE, &b->bi_flags);\r\nb->bi_vcnt = vcnt;\r\nb->bi_iter.bi_size = r1_bio->sectors << 9;\r\nb->bi_iter.bi_sector = r1_bio->sector +\r\nconf->mirrors[i].rdev->data_offset;\r\nb->bi_bdev = conf->mirrors[i].rdev->bdev;\r\nb->bi_end_io = end_sync_read;\r\nb->bi_private = r1_bio;\r\nsize = b->bi_iter.bi_size;\r\nfor (j = 0; j < vcnt ; j++) {\r\nstruct bio_vec *bi;\r\nbi = &b->bi_io_vec[j];\r\nbi->bv_offset = 0;\r\nif (size > PAGE_SIZE)\r\nbi->bv_len = PAGE_SIZE;\r\nelse\r\nbi->bv_len = size;\r\nsize -= PAGE_SIZE;\r\n}\r\n}\r\nfor (primary = 0; primary < conf->raid_disks * 2; primary++)\r\nif (r1_bio->bios[primary]->bi_end_io == end_sync_read &&\r\ntest_bit(BIO_UPTODATE, &r1_bio->bios[primary]->bi_flags)) {\r\nr1_bio->bios[primary]->bi_end_io = NULL;\r\nrdev_dec_pending(conf->mirrors[primary].rdev, mddev);\r\nbreak;\r\n}\r\nr1_bio->read_disk = primary;\r\nfor (i = 0; i < conf->raid_disks * 2; i++) {\r\nint j;\r\nstruct bio *pbio = r1_bio->bios[primary];\r\nstruct bio *sbio = r1_bio->bios[i];\r\nint uptodate = test_bit(BIO_UPTODATE, &sbio->bi_flags);\r\nif (sbio->bi_end_io != end_sync_read)\r\ncontinue;\r\nset_bit(BIO_UPTODATE, &sbio->bi_flags);\r\nif (uptodate) {\r\nfor (j = vcnt; j-- ; ) {\r\nstruct page *p, *s;\r\np = pbio->bi_io_vec[j].bv_page;\r\ns = sbio->bi_io_vec[j].bv_page;\r\nif (memcmp(page_address(p),\r\npage_address(s),\r\nsbio->bi_io_vec[j].bv_len))\r\nbreak;\r\n}\r\n} else\r\nj = 0;\r\nif (j >= 0)\r\natomic64_add(r1_bio->sectors, &mddev->resync_mismatches);\r\nif (j < 0 || (test_bit(MD_RECOVERY_CHECK, &mddev->recovery)\r\n&& uptodate)) {\r\nsbio->bi_end_io = NULL;\r\nrdev_dec_pending(conf->mirrors[i].rdev, mddev);\r\ncontinue;\r\n}\r\nbio_copy_data(sbio, pbio);\r\n}\r\n}\r\nstatic void sync_request_write(struct mddev *mddev, struct r1bio *r1_bio)\r\n{\r\nstruct r1conf *conf = mddev->private;\r\nint i;\r\nint disks = conf->raid_disks * 2;\r\nstruct bio *bio, *wbio;\r\nbio = r1_bio->bios[r1_bio->read_disk];\r\nif (!test_bit(R1BIO_Uptodate, &r1_bio->state))\r\nif (!fix_sync_read_error(r1_bio))\r\nreturn;\r\nif (test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))\r\nprocess_checks(r1_bio);\r\natomic_set(&r1_bio->remaining, 1);\r\nfor (i = 0; i < disks ; i++) {\r\nwbio = r1_bio->bios[i];\r\nif (wbio->bi_end_io == NULL ||\r\n(wbio->bi_end_io == end_sync_read &&\r\n(i == r1_bio->read_disk ||\r\n!test_bit(MD_RECOVERY_SYNC, &mddev->recovery))))\r\ncontinue;\r\nwbio->bi_rw = WRITE;\r\nwbio->bi_end_io = end_sync_write;\r\natomic_inc(&r1_bio->remaining);\r\nmd_sync_acct(conf->mirrors[i].rdev->bdev, bio_sectors(wbio));\r\ngeneric_make_request(wbio);\r\n}\r\nif (atomic_dec_and_test(&r1_bio->remaining)) {\r\nint s = r1_bio->sectors;\r\nif (test_bit(R1BIO_MadeGood, &r1_bio->state) ||\r\ntest_bit(R1BIO_WriteError, &r1_bio->state))\r\nreschedule_retry(r1_bio);\r\nelse {\r\nput_buf(r1_bio);\r\nmd_done_sync(mddev, s, 1);\r\n}\r\n}\r\n}\r\nstatic void fix_read_error(struct r1conf *conf, int read_disk,\r\nsector_t sect, int sectors)\r\n{\r\nstruct mddev *mddev = conf->mddev;\r\nwhile(sectors) {\r\nint s = sectors;\r\nint d = read_disk;\r\nint success = 0;\r\nint start;\r\nstruct md_rdev *rdev;\r\nif (s > (PAGE_SIZE>>9))\r\ns = PAGE_SIZE >> 9;\r\ndo {\r\nsector_t first_bad;\r\nint bad_sectors;\r\nrdev = conf->mirrors[d].rdev;\r\nif (rdev &&\r\n(test_bit(In_sync, &rdev->flags) ||\r\n(!test_bit(Faulty, &rdev->flags) &&\r\nrdev->recovery_offset >= sect + s)) &&\r\nis_badblock(rdev, sect, s,\r\n&first_bad, &bad_sectors) == 0 &&\r\nsync_page_io(rdev, sect, s<<9,\r\nconf->tmppage, READ, false))\r\nsuccess = 1;\r\nelse {\r\nd++;\r\nif (d == conf->raid_disks * 2)\r\nd = 0;\r\n}\r\n} while (!success && d != read_disk);\r\nif (!success) {\r\nstruct md_rdev *rdev = conf->mirrors[read_disk].rdev;\r\nif (!rdev_set_badblocks(rdev, sect, s, 0))\r\nmd_error(mddev, rdev);\r\nbreak;\r\n}\r\nstart = d;\r\nwhile (d != read_disk) {\r\nif (d==0)\r\nd = conf->raid_disks * 2;\r\nd--;\r\nrdev = conf->mirrors[d].rdev;\r\nif (rdev &&\r\n!test_bit(Faulty, &rdev->flags))\r\nr1_sync_page_io(rdev, sect, s,\r\nconf->tmppage, WRITE);\r\n}\r\nd = start;\r\nwhile (d != read_disk) {\r\nchar b[BDEVNAME_SIZE];\r\nif (d==0)\r\nd = conf->raid_disks * 2;\r\nd--;\r\nrdev = conf->mirrors[d].rdev;\r\nif (rdev &&\r\n!test_bit(Faulty, &rdev->flags)) {\r\nif (r1_sync_page_io(rdev, sect, s,\r\nconf->tmppage, READ)) {\r\natomic_add(s, &rdev->corrected_errors);\r\nprintk(KERN_INFO\r\n"md/raid1:%s: read error corrected "\r\n"(%d sectors at %llu on %s)\n",\r\nmdname(mddev), s,\r\n(unsigned long long)(sect +\r\nrdev->data_offset),\r\nbdevname(rdev->bdev, b));\r\n}\r\n}\r\n}\r\nsectors -= s;\r\nsect += s;\r\n}\r\n}\r\nstatic int narrow_write_error(struct r1bio *r1_bio, int i)\r\n{\r\nstruct mddev *mddev = r1_bio->mddev;\r\nstruct r1conf *conf = mddev->private;\r\nstruct md_rdev *rdev = conf->mirrors[i].rdev;\r\nint block_sectors;\r\nsector_t sector;\r\nint sectors;\r\nint sect_to_write = r1_bio->sectors;\r\nint ok = 1;\r\nif (rdev->badblocks.shift < 0)\r\nreturn 0;\r\nblock_sectors = 1 << rdev->badblocks.shift;\r\nsector = r1_bio->sector;\r\nsectors = ((sector + block_sectors)\r\n& ~(sector_t)(block_sectors - 1))\r\n- sector;\r\nwhile (sect_to_write) {\r\nstruct bio *wbio;\r\nif (sectors > sect_to_write)\r\nsectors = sect_to_write;\r\nif (test_bit(R1BIO_BehindIO, &r1_bio->state)) {\r\nunsigned vcnt = r1_bio->behind_page_count;\r\nstruct bio_vec *vec = r1_bio->behind_bvecs;\r\nwhile (!vec->bv_page) {\r\nvec++;\r\nvcnt--;\r\n}\r\nwbio = bio_alloc_mddev(GFP_NOIO, vcnt, mddev);\r\nmemcpy(wbio->bi_io_vec, vec, vcnt * sizeof(struct bio_vec));\r\nwbio->bi_vcnt = vcnt;\r\n} else {\r\nwbio = bio_clone_mddev(r1_bio->master_bio, GFP_NOIO, mddev);\r\n}\r\nwbio->bi_rw = WRITE;\r\nwbio->bi_iter.bi_sector = r1_bio->sector;\r\nwbio->bi_iter.bi_size = r1_bio->sectors << 9;\r\nbio_trim(wbio, sector - r1_bio->sector, sectors);\r\nwbio->bi_iter.bi_sector += rdev->data_offset;\r\nwbio->bi_bdev = rdev->bdev;\r\nif (submit_bio_wait(WRITE, wbio) == 0)\r\nok = rdev_set_badblocks(rdev, sector,\r\nsectors, 0)\r\n&& ok;\r\nbio_put(wbio);\r\nsect_to_write -= sectors;\r\nsector += sectors;\r\nsectors = block_sectors;\r\n}\r\nreturn ok;\r\n}\r\nstatic void handle_sync_write_finished(struct r1conf *conf, struct r1bio *r1_bio)\r\n{\r\nint m;\r\nint s = r1_bio->sectors;\r\nfor (m = 0; m < conf->raid_disks * 2 ; m++) {\r\nstruct md_rdev *rdev = conf->mirrors[m].rdev;\r\nstruct bio *bio = r1_bio->bios[m];\r\nif (bio->bi_end_io == NULL)\r\ncontinue;\r\nif (test_bit(BIO_UPTODATE, &bio->bi_flags) &&\r\ntest_bit(R1BIO_MadeGood, &r1_bio->state)) {\r\nrdev_clear_badblocks(rdev, r1_bio->sector, s, 0);\r\n}\r\nif (!test_bit(BIO_UPTODATE, &bio->bi_flags) &&\r\ntest_bit(R1BIO_WriteError, &r1_bio->state)) {\r\nif (!rdev_set_badblocks(rdev, r1_bio->sector, s, 0))\r\nmd_error(conf->mddev, rdev);\r\n}\r\n}\r\nput_buf(r1_bio);\r\nmd_done_sync(conf->mddev, s, 1);\r\n}\r\nstatic void handle_write_finished(struct r1conf *conf, struct r1bio *r1_bio)\r\n{\r\nint m;\r\nfor (m = 0; m < conf->raid_disks * 2 ; m++)\r\nif (r1_bio->bios[m] == IO_MADE_GOOD) {\r\nstruct md_rdev *rdev = conf->mirrors[m].rdev;\r\nrdev_clear_badblocks(rdev,\r\nr1_bio->sector,\r\nr1_bio->sectors, 0);\r\nrdev_dec_pending(rdev, conf->mddev);\r\n} else if (r1_bio->bios[m] != NULL) {\r\nif (!narrow_write_error(r1_bio, m)) {\r\nmd_error(conf->mddev,\r\nconf->mirrors[m].rdev);\r\nset_bit(R1BIO_Degraded, &r1_bio->state);\r\n}\r\nrdev_dec_pending(conf->mirrors[m].rdev,\r\nconf->mddev);\r\n}\r\nif (test_bit(R1BIO_WriteError, &r1_bio->state))\r\nclose_write(r1_bio);\r\nraid_end_bio_io(r1_bio);\r\n}\r\nstatic void handle_read_error(struct r1conf *conf, struct r1bio *r1_bio)\r\n{\r\nint disk;\r\nint max_sectors;\r\nstruct mddev *mddev = conf->mddev;\r\nstruct bio *bio;\r\nchar b[BDEVNAME_SIZE];\r\nstruct md_rdev *rdev;\r\nclear_bit(R1BIO_ReadError, &r1_bio->state);\r\nif (mddev->ro == 0) {\r\nfreeze_array(conf, 1);\r\nfix_read_error(conf, r1_bio->read_disk,\r\nr1_bio->sector, r1_bio->sectors);\r\nunfreeze_array(conf);\r\n} else\r\nmd_error(mddev, conf->mirrors[r1_bio->read_disk].rdev);\r\nrdev_dec_pending(conf->mirrors[r1_bio->read_disk].rdev, conf->mddev);\r\nbio = r1_bio->bios[r1_bio->read_disk];\r\nbdevname(bio->bi_bdev, b);\r\nread_more:\r\ndisk = read_balance(conf, r1_bio, &max_sectors);\r\nif (disk == -1) {\r\nprintk(KERN_ALERT "md/raid1:%s: %s: unrecoverable I/O"\r\n" read error for block %llu\n",\r\nmdname(mddev), b, (unsigned long long)r1_bio->sector);\r\nraid_end_bio_io(r1_bio);\r\n} else {\r\nconst unsigned long do_sync\r\n= r1_bio->master_bio->bi_rw & REQ_SYNC;\r\nif (bio) {\r\nr1_bio->bios[r1_bio->read_disk] =\r\nmddev->ro ? IO_BLOCKED : NULL;\r\nbio_put(bio);\r\n}\r\nr1_bio->read_disk = disk;\r\nbio = bio_clone_mddev(r1_bio->master_bio, GFP_NOIO, mddev);\r\nbio_trim(bio, r1_bio->sector - bio->bi_iter.bi_sector,\r\nmax_sectors);\r\nr1_bio->bios[r1_bio->read_disk] = bio;\r\nrdev = conf->mirrors[disk].rdev;\r\nprintk_ratelimited(KERN_ERR\r\n"md/raid1:%s: redirecting sector %llu"\r\n" to other mirror: %s\n",\r\nmdname(mddev),\r\n(unsigned long long)r1_bio->sector,\r\nbdevname(rdev->bdev, b));\r\nbio->bi_iter.bi_sector = r1_bio->sector + rdev->data_offset;\r\nbio->bi_bdev = rdev->bdev;\r\nbio->bi_end_io = raid1_end_read_request;\r\nbio->bi_rw = READ | do_sync;\r\nbio->bi_private = r1_bio;\r\nif (max_sectors < r1_bio->sectors) {\r\nstruct bio *mbio = r1_bio->master_bio;\r\nint sectors_handled = (r1_bio->sector + max_sectors\r\n- mbio->bi_iter.bi_sector);\r\nr1_bio->sectors = max_sectors;\r\nspin_lock_irq(&conf->device_lock);\r\nif (mbio->bi_phys_segments == 0)\r\nmbio->bi_phys_segments = 2;\r\nelse\r\nmbio->bi_phys_segments++;\r\nspin_unlock_irq(&conf->device_lock);\r\ngeneric_make_request(bio);\r\nbio = NULL;\r\nr1_bio = mempool_alloc(conf->r1bio_pool, GFP_NOIO);\r\nr1_bio->master_bio = mbio;\r\nr1_bio->sectors = bio_sectors(mbio) - sectors_handled;\r\nr1_bio->state = 0;\r\nset_bit(R1BIO_ReadError, &r1_bio->state);\r\nr1_bio->mddev = mddev;\r\nr1_bio->sector = mbio->bi_iter.bi_sector +\r\nsectors_handled;\r\ngoto read_more;\r\n} else\r\ngeneric_make_request(bio);\r\n}\r\n}\r\nstatic void raid1d(struct md_thread *thread)\r\n{\r\nstruct mddev *mddev = thread->mddev;\r\nstruct r1bio *r1_bio;\r\nunsigned long flags;\r\nstruct r1conf *conf = mddev->private;\r\nstruct list_head *head = &conf->retry_list;\r\nstruct blk_plug plug;\r\nmd_check_recovery(mddev);\r\nblk_start_plug(&plug);\r\nfor (;;) {\r\nflush_pending_writes(conf);\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nif (list_empty(head)) {\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nbreak;\r\n}\r\nr1_bio = list_entry(head->prev, struct r1bio, retry_list);\r\nlist_del(head->prev);\r\nconf->nr_queued--;\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nmddev = r1_bio->mddev;\r\nconf = mddev->private;\r\nif (test_bit(R1BIO_IsSync, &r1_bio->state)) {\r\nif (test_bit(R1BIO_MadeGood, &r1_bio->state) ||\r\ntest_bit(R1BIO_WriteError, &r1_bio->state))\r\nhandle_sync_write_finished(conf, r1_bio);\r\nelse\r\nsync_request_write(mddev, r1_bio);\r\n} else if (test_bit(R1BIO_MadeGood, &r1_bio->state) ||\r\ntest_bit(R1BIO_WriteError, &r1_bio->state))\r\nhandle_write_finished(conf, r1_bio);\r\nelse if (test_bit(R1BIO_ReadError, &r1_bio->state))\r\nhandle_read_error(conf, r1_bio);\r\nelse\r\ngeneric_make_request(r1_bio->bios[r1_bio->read_disk]);\r\ncond_resched();\r\nif (mddev->flags & ~(1<<MD_CHANGE_PENDING))\r\nmd_check_recovery(mddev);\r\n}\r\nblk_finish_plug(&plug);\r\n}\r\nstatic int init_resync(struct r1conf *conf)\r\n{\r\nint buffs;\r\nbuffs = RESYNC_WINDOW / RESYNC_BLOCK_SIZE;\r\nBUG_ON(conf->r1buf_pool);\r\nconf->r1buf_pool = mempool_create(buffs, r1buf_pool_alloc, r1buf_pool_free,\r\nconf->poolinfo);\r\nif (!conf->r1buf_pool)\r\nreturn -ENOMEM;\r\nconf->next_resync = 0;\r\nreturn 0;\r\n}\r\nstatic sector_t sync_request(struct mddev *mddev, sector_t sector_nr, int *skipped, int go_faster)\r\n{\r\nstruct r1conf *conf = mddev->private;\r\nstruct r1bio *r1_bio;\r\nstruct bio *bio;\r\nsector_t max_sector, nr_sectors;\r\nint disk = -1;\r\nint i;\r\nint wonly = -1;\r\nint write_targets = 0, read_targets = 0;\r\nsector_t sync_blocks;\r\nint still_degraded = 0;\r\nint good_sectors = RESYNC_SECTORS;\r\nint min_bad = 0;\r\nif (!conf->r1buf_pool)\r\nif (init_resync(conf))\r\nreturn 0;\r\nmax_sector = mddev->dev_sectors;\r\nif (sector_nr >= max_sector) {\r\nif (mddev->curr_resync < max_sector)\r\nbitmap_end_sync(mddev->bitmap, mddev->curr_resync,\r\n&sync_blocks, 1);\r\nelse\r\nconf->fullsync = 0;\r\nbitmap_close_sync(mddev->bitmap);\r\nclose_sync(conf);\r\nreturn 0;\r\n}\r\nif (mddev->bitmap == NULL &&\r\nmddev->recovery_cp == MaxSector &&\r\n!test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&\r\nconf->fullsync == 0) {\r\n*skipped = 1;\r\nreturn max_sector - sector_nr;\r\n}\r\nif (!bitmap_start_sync(mddev->bitmap, sector_nr, &sync_blocks, 1) &&\r\n!conf->fullsync && !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {\r\n*skipped = 1;\r\nreturn sync_blocks;\r\n}\r\nif (!go_faster && conf->nr_waiting)\r\nmsleep_interruptible(1000);\r\nbitmap_cond_end_sync(mddev->bitmap, sector_nr);\r\nr1_bio = mempool_alloc(conf->r1buf_pool, GFP_NOIO);\r\nraise_barrier(conf, sector_nr);\r\nrcu_read_lock();\r\nr1_bio->mddev = mddev;\r\nr1_bio->sector = sector_nr;\r\nr1_bio->state = 0;\r\nset_bit(R1BIO_IsSync, &r1_bio->state);\r\nfor (i = 0; i < conf->raid_disks * 2; i++) {\r\nstruct md_rdev *rdev;\r\nbio = r1_bio->bios[i];\r\nbio_reset(bio);\r\nrdev = rcu_dereference(conf->mirrors[i].rdev);\r\nif (rdev == NULL ||\r\ntest_bit(Faulty, &rdev->flags)) {\r\nif (i < conf->raid_disks)\r\nstill_degraded = 1;\r\n} else if (!test_bit(In_sync, &rdev->flags)) {\r\nbio->bi_rw = WRITE;\r\nbio->bi_end_io = end_sync_write;\r\nwrite_targets ++;\r\n} else {\r\nsector_t first_bad = MaxSector;\r\nint bad_sectors;\r\nif (is_badblock(rdev, sector_nr, good_sectors,\r\n&first_bad, &bad_sectors)) {\r\nif (first_bad > sector_nr)\r\ngood_sectors = first_bad - sector_nr;\r\nelse {\r\nbad_sectors -= (sector_nr - first_bad);\r\nif (min_bad == 0 ||\r\nmin_bad > bad_sectors)\r\nmin_bad = bad_sectors;\r\n}\r\n}\r\nif (sector_nr < first_bad) {\r\nif (test_bit(WriteMostly, &rdev->flags)) {\r\nif (wonly < 0)\r\nwonly = i;\r\n} else {\r\nif (disk < 0)\r\ndisk = i;\r\n}\r\nbio->bi_rw = READ;\r\nbio->bi_end_io = end_sync_read;\r\nread_targets++;\r\n} else if (!test_bit(WriteErrorSeen, &rdev->flags) &&\r\ntest_bit(MD_RECOVERY_SYNC, &mddev->recovery) &&\r\n!test_bit(MD_RECOVERY_CHECK, &mddev->recovery)) {\r\nbio->bi_rw = WRITE;\r\nbio->bi_end_io = end_sync_write;\r\nwrite_targets++;\r\n}\r\n}\r\nif (bio->bi_end_io) {\r\natomic_inc(&rdev->nr_pending);\r\nbio->bi_iter.bi_sector = sector_nr + rdev->data_offset;\r\nbio->bi_bdev = rdev->bdev;\r\nbio->bi_private = r1_bio;\r\n}\r\n}\r\nrcu_read_unlock();\r\nif (disk < 0)\r\ndisk = wonly;\r\nr1_bio->read_disk = disk;\r\nif (read_targets == 0 && min_bad > 0) {\r\nint ok = 1;\r\nfor (i = 0 ; i < conf->raid_disks * 2 ; i++)\r\nif (r1_bio->bios[i]->bi_end_io == end_sync_write) {\r\nstruct md_rdev *rdev = conf->mirrors[i].rdev;\r\nok = rdev_set_badblocks(rdev, sector_nr,\r\nmin_bad, 0\r\n) && ok;\r\n}\r\nset_bit(MD_CHANGE_DEVS, &mddev->flags);\r\n*skipped = 1;\r\nput_buf(r1_bio);\r\nif (!ok) {\r\nconf->recovery_disabled = mddev->recovery_disabled;\r\nset_bit(MD_RECOVERY_INTR, &mddev->recovery);\r\nreturn 0;\r\n} else\r\nreturn min_bad;\r\n}\r\nif (min_bad > 0 && min_bad < good_sectors) {\r\ngood_sectors = min_bad;\r\n}\r\nif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery) && read_targets > 0)\r\nwrite_targets += read_targets-1;\r\nif (write_targets == 0 || read_targets == 0) {\r\nsector_t rv;\r\nif (min_bad > 0)\r\nmax_sector = sector_nr + min_bad;\r\nrv = max_sector - sector_nr;\r\n*skipped = 1;\r\nput_buf(r1_bio);\r\nreturn rv;\r\n}\r\nif (max_sector > mddev->resync_max)\r\nmax_sector = mddev->resync_max;\r\nif (max_sector > sector_nr + good_sectors)\r\nmax_sector = sector_nr + good_sectors;\r\nnr_sectors = 0;\r\nsync_blocks = 0;\r\ndo {\r\nstruct page *page;\r\nint len = PAGE_SIZE;\r\nif (sector_nr + (len>>9) > max_sector)\r\nlen = (max_sector - sector_nr) << 9;\r\nif (len == 0)\r\nbreak;\r\nif (sync_blocks == 0) {\r\nif (!bitmap_start_sync(mddev->bitmap, sector_nr,\r\n&sync_blocks, still_degraded) &&\r\n!conf->fullsync &&\r\n!test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))\r\nbreak;\r\nBUG_ON(sync_blocks < (PAGE_SIZE>>9));\r\nif ((len >> 9) > sync_blocks)\r\nlen = sync_blocks<<9;\r\n}\r\nfor (i = 0 ; i < conf->raid_disks * 2; i++) {\r\nbio = r1_bio->bios[i];\r\nif (bio->bi_end_io) {\r\npage = bio->bi_io_vec[bio->bi_vcnt].bv_page;\r\nif (bio_add_page(bio, page, len, 0) == 0) {\r\nbio->bi_io_vec[bio->bi_vcnt].bv_page = page;\r\nwhile (i > 0) {\r\ni--;\r\nbio = r1_bio->bios[i];\r\nif (bio->bi_end_io==NULL)\r\ncontinue;\r\nbio->bi_vcnt--;\r\nbio->bi_iter.bi_size -= len;\r\n__clear_bit(BIO_SEG_VALID, &bio->bi_flags);\r\n}\r\ngoto bio_full;\r\n}\r\n}\r\n}\r\nnr_sectors += len>>9;\r\nsector_nr += len>>9;\r\nsync_blocks -= (len>>9);\r\n} while (r1_bio->bios[disk]->bi_vcnt < RESYNC_PAGES);\r\nbio_full:\r\nr1_bio->sectors = nr_sectors;\r\nif (test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {\r\natomic_set(&r1_bio->remaining, read_targets);\r\nfor (i = 0; i < conf->raid_disks * 2 && read_targets; i++) {\r\nbio = r1_bio->bios[i];\r\nif (bio->bi_end_io == end_sync_read) {\r\nread_targets--;\r\nmd_sync_acct(bio->bi_bdev, nr_sectors);\r\ngeneric_make_request(bio);\r\n}\r\n}\r\n} else {\r\natomic_set(&r1_bio->remaining, 1);\r\nbio = r1_bio->bios[r1_bio->read_disk];\r\nmd_sync_acct(bio->bi_bdev, nr_sectors);\r\ngeneric_make_request(bio);\r\n}\r\nreturn nr_sectors;\r\n}\r\nstatic sector_t raid1_size(struct mddev *mddev, sector_t sectors, int raid_disks)\r\n{\r\nif (sectors)\r\nreturn sectors;\r\nreturn mddev->dev_sectors;\r\n}\r\nstatic struct r1conf *setup_conf(struct mddev *mddev)\r\n{\r\nstruct r1conf *conf;\r\nint i;\r\nstruct raid1_info *disk;\r\nstruct md_rdev *rdev;\r\nint err = -ENOMEM;\r\nconf = kzalloc(sizeof(struct r1conf), GFP_KERNEL);\r\nif (!conf)\r\ngoto abort;\r\nconf->mirrors = kzalloc(sizeof(struct raid1_info)\r\n* mddev->raid_disks * 2,\r\nGFP_KERNEL);\r\nif (!conf->mirrors)\r\ngoto abort;\r\nconf->tmppage = alloc_page(GFP_KERNEL);\r\nif (!conf->tmppage)\r\ngoto abort;\r\nconf->poolinfo = kzalloc(sizeof(*conf->poolinfo), GFP_KERNEL);\r\nif (!conf->poolinfo)\r\ngoto abort;\r\nconf->poolinfo->raid_disks = mddev->raid_disks * 2;\r\nconf->r1bio_pool = mempool_create(NR_RAID1_BIOS, r1bio_pool_alloc,\r\nr1bio_pool_free,\r\nconf->poolinfo);\r\nif (!conf->r1bio_pool)\r\ngoto abort;\r\nconf->poolinfo->mddev = mddev;\r\nerr = -EINVAL;\r\nspin_lock_init(&conf->device_lock);\r\nrdev_for_each(rdev, mddev) {\r\nstruct request_queue *q;\r\nint disk_idx = rdev->raid_disk;\r\nif (disk_idx >= mddev->raid_disks\r\n|| disk_idx < 0)\r\ncontinue;\r\nif (test_bit(Replacement, &rdev->flags))\r\ndisk = conf->mirrors + mddev->raid_disks + disk_idx;\r\nelse\r\ndisk = conf->mirrors + disk_idx;\r\nif (disk->rdev)\r\ngoto abort;\r\ndisk->rdev = rdev;\r\nq = bdev_get_queue(rdev->bdev);\r\nif (q->merge_bvec_fn)\r\nmddev->merge_check_needed = 1;\r\ndisk->head_position = 0;\r\ndisk->seq_start = MaxSector;\r\n}\r\nconf->raid_disks = mddev->raid_disks;\r\nconf->mddev = mddev;\r\nINIT_LIST_HEAD(&conf->retry_list);\r\nspin_lock_init(&conf->resync_lock);\r\ninit_waitqueue_head(&conf->wait_barrier);\r\nbio_list_init(&conf->pending_bio_list);\r\nconf->pending_count = 0;\r\nconf->recovery_disabled = mddev->recovery_disabled - 1;\r\nconf->start_next_window = MaxSector;\r\nconf->current_window_requests = conf->next_window_requests = 0;\r\nerr = -EIO;\r\nfor (i = 0; i < conf->raid_disks * 2; i++) {\r\ndisk = conf->mirrors + i;\r\nif (i < conf->raid_disks &&\r\ndisk[conf->raid_disks].rdev) {\r\nif (!disk->rdev) {\r\ndisk->rdev =\r\ndisk[conf->raid_disks].rdev;\r\ndisk[conf->raid_disks].rdev = NULL;\r\n} else if (!test_bit(In_sync, &disk->rdev->flags))\r\ngoto abort;\r\n}\r\nif (!disk->rdev ||\r\n!test_bit(In_sync, &disk->rdev->flags)) {\r\ndisk->head_position = 0;\r\nif (disk->rdev &&\r\n(disk->rdev->saved_raid_disk < 0))\r\nconf->fullsync = 1;\r\n}\r\n}\r\nerr = -ENOMEM;\r\nconf->thread = md_register_thread(raid1d, mddev, "raid1");\r\nif (!conf->thread) {\r\nprintk(KERN_ERR\r\n"md/raid1:%s: couldn't allocate thread\n",\r\nmdname(mddev));\r\ngoto abort;\r\n}\r\nreturn conf;\r\nabort:\r\nif (conf) {\r\nif (conf->r1bio_pool)\r\nmempool_destroy(conf->r1bio_pool);\r\nkfree(conf->mirrors);\r\nsafe_put_page(conf->tmppage);\r\nkfree(conf->poolinfo);\r\nkfree(conf);\r\n}\r\nreturn ERR_PTR(err);\r\n}\r\nstatic int run(struct mddev *mddev)\r\n{\r\nstruct r1conf *conf;\r\nint i;\r\nstruct md_rdev *rdev;\r\nint ret;\r\nbool discard_supported = false;\r\nif (mddev->level != 1) {\r\nprintk(KERN_ERR "md/raid1:%s: raid level not set to mirroring (%d)\n",\r\nmdname(mddev), mddev->level);\r\nreturn -EIO;\r\n}\r\nif (mddev->reshape_position != MaxSector) {\r\nprintk(KERN_ERR "md/raid1:%s: reshape_position set but not supported\n",\r\nmdname(mddev));\r\nreturn -EIO;\r\n}\r\nif (mddev->private == NULL)\r\nconf = setup_conf(mddev);\r\nelse\r\nconf = mddev->private;\r\nif (IS_ERR(conf))\r\nreturn PTR_ERR(conf);\r\nif (mddev->queue)\r\nblk_queue_max_write_same_sectors(mddev->queue, 0);\r\nrdev_for_each(rdev, mddev) {\r\nif (!mddev->gendisk)\r\ncontinue;\r\ndisk_stack_limits(mddev->gendisk, rdev->bdev,\r\nrdev->data_offset << 9);\r\nif (blk_queue_discard(bdev_get_queue(rdev->bdev)))\r\ndiscard_supported = true;\r\n}\r\nmddev->degraded = 0;\r\nfor (i=0; i < conf->raid_disks; i++)\r\nif (conf->mirrors[i].rdev == NULL ||\r\n!test_bit(In_sync, &conf->mirrors[i].rdev->flags) ||\r\ntest_bit(Faulty, &conf->mirrors[i].rdev->flags))\r\nmddev->degraded++;\r\nif (conf->raid_disks - mddev->degraded == 1)\r\nmddev->recovery_cp = MaxSector;\r\nif (mddev->recovery_cp != MaxSector)\r\nprintk(KERN_NOTICE "md/raid1:%s: not clean"\r\n" -- starting background reconstruction\n",\r\nmdname(mddev));\r\nprintk(KERN_INFO\r\n"md/raid1:%s: active with %d out of %d mirrors\n",\r\nmdname(mddev), mddev->raid_disks - mddev->degraded,\r\nmddev->raid_disks);\r\nmddev->thread = conf->thread;\r\nconf->thread = NULL;\r\nmddev->private = conf;\r\nmd_set_array_sectors(mddev, raid1_size(mddev, 0, 0));\r\nif (mddev->queue) {\r\nmddev->queue->backing_dev_info.congested_fn = raid1_congested;\r\nmddev->queue->backing_dev_info.congested_data = mddev;\r\nblk_queue_merge_bvec(mddev->queue, raid1_mergeable_bvec);\r\nif (discard_supported)\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD,\r\nmddev->queue);\r\nelse\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_DISCARD,\r\nmddev->queue);\r\n}\r\nret = md_integrity_register(mddev);\r\nif (ret)\r\nstop(mddev);\r\nreturn ret;\r\n}\r\nstatic int stop(struct mddev *mddev)\r\n{\r\nstruct r1conf *conf = mddev->private;\r\nstruct bitmap *bitmap = mddev->bitmap;\r\nif (bitmap && atomic_read(&bitmap->behind_writes) > 0) {\r\nprintk(KERN_INFO "md/raid1:%s: behind writes in progress - waiting to stop.\n",\r\nmdname(mddev));\r\nwait_event(bitmap->behind_wait,\r\natomic_read(&bitmap->behind_writes) == 0);\r\n}\r\nfreeze_array(conf, 0);\r\nunfreeze_array(conf);\r\nmd_unregister_thread(&mddev->thread);\r\nif (conf->r1bio_pool)\r\nmempool_destroy(conf->r1bio_pool);\r\nkfree(conf->mirrors);\r\nsafe_put_page(conf->tmppage);\r\nkfree(conf->poolinfo);\r\nkfree(conf);\r\nmddev->private = NULL;\r\nreturn 0;\r\n}\r\nstatic int raid1_resize(struct mddev *mddev, sector_t sectors)\r\n{\r\nsector_t newsize = raid1_size(mddev, sectors, 0);\r\nif (mddev->external_size &&\r\nmddev->array_sectors > newsize)\r\nreturn -EINVAL;\r\nif (mddev->bitmap) {\r\nint ret = bitmap_resize(mddev->bitmap, newsize, 0, 0);\r\nif (ret)\r\nreturn ret;\r\n}\r\nmd_set_array_sectors(mddev, newsize);\r\nset_capacity(mddev->gendisk, mddev->array_sectors);\r\nrevalidate_disk(mddev->gendisk);\r\nif (sectors > mddev->dev_sectors &&\r\nmddev->recovery_cp > mddev->dev_sectors) {\r\nmddev->recovery_cp = mddev->dev_sectors;\r\nset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\r\n}\r\nmddev->dev_sectors = sectors;\r\nmddev->resync_max_sectors = sectors;\r\nreturn 0;\r\n}\r\nstatic int raid1_reshape(struct mddev *mddev)\r\n{\r\nmempool_t *newpool, *oldpool;\r\nstruct pool_info *newpoolinfo;\r\nstruct raid1_info *newmirrors;\r\nstruct r1conf *conf = mddev->private;\r\nint cnt, raid_disks;\r\nunsigned long flags;\r\nint d, d2, err;\r\nif (mddev->chunk_sectors != mddev->new_chunk_sectors ||\r\nmddev->layout != mddev->new_layout ||\r\nmddev->level != mddev->new_level) {\r\nmddev->new_chunk_sectors = mddev->chunk_sectors;\r\nmddev->new_layout = mddev->layout;\r\nmddev->new_level = mddev->level;\r\nreturn -EINVAL;\r\n}\r\nerr = md_allow_write(mddev);\r\nif (err)\r\nreturn err;\r\nraid_disks = mddev->raid_disks + mddev->delta_disks;\r\nif (raid_disks < conf->raid_disks) {\r\ncnt=0;\r\nfor (d= 0; d < conf->raid_disks; d++)\r\nif (conf->mirrors[d].rdev)\r\ncnt++;\r\nif (cnt > raid_disks)\r\nreturn -EBUSY;\r\n}\r\nnewpoolinfo = kmalloc(sizeof(*newpoolinfo), GFP_KERNEL);\r\nif (!newpoolinfo)\r\nreturn -ENOMEM;\r\nnewpoolinfo->mddev = mddev;\r\nnewpoolinfo->raid_disks = raid_disks * 2;\r\nnewpool = mempool_create(NR_RAID1_BIOS, r1bio_pool_alloc,\r\nr1bio_pool_free, newpoolinfo);\r\nif (!newpool) {\r\nkfree(newpoolinfo);\r\nreturn -ENOMEM;\r\n}\r\nnewmirrors = kzalloc(sizeof(struct raid1_info) * raid_disks * 2,\r\nGFP_KERNEL);\r\nif (!newmirrors) {\r\nkfree(newpoolinfo);\r\nmempool_destroy(newpool);\r\nreturn -ENOMEM;\r\n}\r\nfreeze_array(conf, 0);\r\noldpool = conf->r1bio_pool;\r\nconf->r1bio_pool = newpool;\r\nfor (d = d2 = 0; d < conf->raid_disks; d++) {\r\nstruct md_rdev *rdev = conf->mirrors[d].rdev;\r\nif (rdev && rdev->raid_disk != d2) {\r\nsysfs_unlink_rdev(mddev, rdev);\r\nrdev->raid_disk = d2;\r\nsysfs_unlink_rdev(mddev, rdev);\r\nif (sysfs_link_rdev(mddev, rdev))\r\nprintk(KERN_WARNING\r\n"md/raid1:%s: cannot register rd%d\n",\r\nmdname(mddev), rdev->raid_disk);\r\n}\r\nif (rdev)\r\nnewmirrors[d2++].rdev = rdev;\r\n}\r\nkfree(conf->mirrors);\r\nconf->mirrors = newmirrors;\r\nkfree(conf->poolinfo);\r\nconf->poolinfo = newpoolinfo;\r\nspin_lock_irqsave(&conf->device_lock, flags);\r\nmddev->degraded += (raid_disks - conf->raid_disks);\r\nspin_unlock_irqrestore(&conf->device_lock, flags);\r\nconf->raid_disks = mddev->raid_disks = raid_disks;\r\nmddev->delta_disks = 0;\r\nunfreeze_array(conf);\r\nset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\r\nmd_wakeup_thread(mddev->thread);\r\nmempool_destroy(oldpool);\r\nreturn 0;\r\n}\r\nstatic void raid1_quiesce(struct mddev *mddev, int state)\r\n{\r\nstruct r1conf *conf = mddev->private;\r\nswitch(state) {\r\ncase 2:\r\nwake_up(&conf->wait_barrier);\r\nbreak;\r\ncase 1:\r\nfreeze_array(conf, 0);\r\nbreak;\r\ncase 0:\r\nunfreeze_array(conf);\r\nbreak;\r\n}\r\n}\r\nstatic void *raid1_takeover(struct mddev *mddev)\r\n{\r\nif (mddev->level == 5 && mddev->raid_disks == 2) {\r\nstruct r1conf *conf;\r\nmddev->new_level = 1;\r\nmddev->new_layout = 0;\r\nmddev->new_chunk_sectors = 0;\r\nconf = setup_conf(mddev);\r\nif (!IS_ERR(conf))\r\nconf->array_frozen = 1;\r\nreturn conf;\r\n}\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nstatic int __init raid_init(void)\r\n{\r\nreturn register_md_personality(&raid1_personality);\r\n}\r\nstatic void raid_exit(void)\r\n{\r\nunregister_md_personality(&raid1_personality);\r\n}
