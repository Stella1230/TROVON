static inline int mpipe_instance(struct net_device *dev)\r\n{\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nreturn priv->instance;\r\n}\r\nstatic bool network_cpus_init(void)\r\n{\r\nchar buf[1024];\r\nint rc;\r\nif (network_cpus_string == NULL)\r\nreturn false;\r\nrc = cpulist_parse_crop(network_cpus_string, &network_cpus_map);\r\nif (rc != 0) {\r\npr_warn("tile_net.cpus=%s: malformed cpu list\n",\r\nnetwork_cpus_string);\r\nreturn false;\r\n}\r\ncpumask_and(&network_cpus_map, &network_cpus_map, cpu_possible_mask);\r\nif (cpumask_empty(&network_cpus_map)) {\r\npr_warn("Ignoring empty tile_net.cpus='%s'.\n",\r\nnetwork_cpus_string);\r\nreturn false;\r\n}\r\ncpulist_scnprintf(buf, sizeof(buf), &network_cpus_map);\r\npr_info("Linux network CPUs: %s\n", buf);\r\nreturn true;\r\n}\r\nstatic void tile_net_stats_add(unsigned long value, unsigned long *field)\r\n{\r\nBUILD_BUG_ON(sizeof(atomic_long_t) != sizeof(unsigned long));\r\natomic_long_add(value, (atomic_long_t *)field);\r\n}\r\nstatic bool tile_net_provide_buffer(int instance, int kind)\r\n{\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\ngxio_mpipe_buffer_size_enum_t bse = buffer_size_enums[kind];\r\nsize_t bs = gxio_mpipe_buffer_size_enum_to_buffer_size(bse);\r\nconst unsigned long buffer_alignment = 128;\r\nstruct sk_buff *skb;\r\nint len;\r\nlen = sizeof(struct sk_buff **) + buffer_alignment + bs;\r\nskb = dev_alloc_skb(len);\r\nif (skb == NULL)\r\nreturn false;\r\nskb_reserve(skb, sizeof(struct sk_buff **));\r\nskb_reserve(skb, -(long)skb->data & (buffer_alignment - 1));\r\n*(struct sk_buff **)(skb->data - sizeof(struct sk_buff **)) = skb;\r\nwmb();\r\ngxio_mpipe_push_buffer(&md->context, md->first_buffer_stack + kind,\r\n(void *)va_to_tile_io_addr(skb->data));\r\nreturn true;\r\n}\r\nstatic struct sk_buff *mpipe_buf_to_skb(void *va)\r\n{\r\nstruct sk_buff **skb_ptr = va - sizeof(*skb_ptr);\r\nstruct sk_buff *skb = *skb_ptr;\r\nif (skb->data != va) {\r\npanic("Corrupt linux buffer! va=%p, skb=%p, skb->data=%p",\r\nva, skb, skb->data);\r\n}\r\nreturn skb;\r\n}\r\nstatic void tile_net_pop_all_buffers(int instance, int stack)\r\n{\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nfor (;;) {\r\ntile_io_addr_t addr =\r\n(tile_io_addr_t)gxio_mpipe_pop_buffer(&md->context,\r\nstack);\r\nif (addr == 0)\r\nbreak;\r\ndev_kfree_skb_irq(mpipe_buf_to_skb(tile_io_addr_to_va(addr)));\r\n}\r\n}\r\nstatic void tile_net_provide_needed_buffers(void)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nint instance, kind;\r\nfor (instance = 0; instance < NR_MPIPE_MAX &&\r\ninfo->mpipe[instance].has_iqueue; instance++) {\r\nfor (kind = 0; kind < MAX_KINDS; kind++) {\r\nwhile (info->mpipe[instance].num_needed_buffers[kind]\r\n!= 0) {\r\nif (!tile_net_provide_buffer(instance, kind)) {\r\npr_notice("Tile %d still needs"\r\n" some buffers\n",\r\ninfo->my_cpu);\r\nreturn;\r\n}\r\ninfo->mpipe[instance].\r\nnum_needed_buffers[kind]--;\r\n}\r\n}\r\n}\r\n}\r\nstatic void tile_rx_timestamp(struct tile_net_priv *priv, struct sk_buff *skb,\r\ngxio_mpipe_idesc_t *idesc)\r\n{\r\nif (unlikely(priv->stamp_cfg.rx_filter != HWTSTAMP_FILTER_NONE)) {\r\nstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\r\nmemset(shhwtstamps, 0, sizeof(*shhwtstamps));\r\nshhwtstamps->hwtstamp = ktime_set(idesc->time_stamp_sec,\r\nidesc->time_stamp_ns);\r\n}\r\n}\r\nstatic void tile_tx_timestamp(struct sk_buff *skb, int instance)\r\n{\r\nstruct skb_shared_info *shtx = skb_shinfo(skb);\r\nif (unlikely((shtx->tx_flags & SKBTX_HW_TSTAMP) != 0)) {\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nstruct skb_shared_hwtstamps shhwtstamps;\r\nstruct timespec ts;\r\nshtx->tx_flags |= SKBTX_IN_PROGRESS;\r\ngxio_mpipe_get_timestamp(&md->context, &ts);\r\nmemset(&shhwtstamps, 0, sizeof(shhwtstamps));\r\nshhwtstamps.hwtstamp = ktime_set(ts.tv_sec, ts.tv_nsec);\r\nskb_tstamp_tx(skb, &shhwtstamps);\r\n}\r\n}\r\nstatic int tile_hwtstamp_set(struct net_device *dev, struct ifreq *rq)\r\n{\r\nstruct hwtstamp_config config;\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nif (copy_from_user(&config, rq->ifr_data, sizeof(config)))\r\nreturn -EFAULT;\r\nif (config.flags)\r\nreturn -EINVAL;\r\nswitch (config.tx_type) {\r\ncase HWTSTAMP_TX_OFF:\r\ncase HWTSTAMP_TX_ON:\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nswitch (config.rx_filter) {\r\ncase HWTSTAMP_FILTER_NONE:\r\nbreak;\r\ncase HWTSTAMP_FILTER_ALL:\r\ncase HWTSTAMP_FILTER_SOME:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\r\nconfig.rx_filter = HWTSTAMP_FILTER_ALL;\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nif (copy_to_user(rq->ifr_data, &config, sizeof(config)))\r\nreturn -EFAULT;\r\npriv->stamp_cfg = config;\r\nreturn 0;\r\n}\r\nstatic int tile_hwtstamp_get(struct net_device *dev, struct ifreq *rq)\r\n{\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nif (copy_to_user(rq->ifr_data, &priv->stamp_cfg,\r\nsizeof(priv->stamp_cfg)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic inline bool filter_packet(struct net_device *dev, void *buf)\r\n{\r\nif (dev == NULL || !(dev->flags & IFF_UP))\r\nreturn true;\r\nif (!(dev->flags & IFF_PROMISC) &&\r\n!is_multicast_ether_addr(buf) &&\r\n!ether_addr_equal(dev->dev_addr, buf))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void tile_net_receive_skb(struct net_device *dev, struct sk_buff *skb,\r\ngxio_mpipe_idesc_t *idesc, unsigned long len)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nint instance = priv->instance;\r\nskb_put(skb, len);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nif (idesc->cs && idesc->csum_seed_val == 0xFFFF)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\ntile_rx_timestamp(priv, skb, idesc);\r\nnapi_gro_receive(&info->mpipe[instance].napi, skb);\r\ntile_net_stats_add(1, &dev->stats.rx_packets);\r\ntile_net_stats_add(len, &dev->stats.rx_bytes);\r\nif (idesc->size == buffer_size_enums[0])\r\ninfo->mpipe[instance].num_needed_buffers[0]++;\r\nelse if (idesc->size == buffer_size_enums[1])\r\ninfo->mpipe[instance].num_needed_buffers[1]++;\r\nelse\r\ninfo->mpipe[instance].num_needed_buffers[2]++;\r\n}\r\nstatic bool tile_net_handle_packet(int instance, gxio_mpipe_idesc_t *idesc)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nstruct net_device *dev = md->tile_net_devs_for_channel[idesc->channel];\r\nuint8_t l2_offset;\r\nvoid *va;\r\nvoid *buf;\r\nunsigned long len;\r\nbool filter;\r\nif (idesc->be || idesc->me || idesc->tr || idesc->ce) {\r\nif (dev)\r\ntile_net_stats_add(1, &dev->stats.rx_errors);\r\ngoto drop;\r\n}\r\nl2_offset = custom_flag ? 0 : gxio_mpipe_idesc_get_l2_offset(idesc);\r\nva = tile_io_addr_to_va((unsigned long)idesc->va);\r\nbuf = va + l2_offset;\r\nlen = idesc->l2_size - l2_offset;\r\nva -= NET_IP_ALIGN;\r\nfilter = filter_packet(dev, buf);\r\nif (filter) {\r\nif (dev)\r\ntile_net_stats_add(1, &dev->stats.rx_dropped);\r\ndrop:\r\ngxio_mpipe_iqueue_drop(&info->mpipe[instance].iqueue, idesc);\r\n} else {\r\nstruct sk_buff *skb = mpipe_buf_to_skb(va);\r\nskb_reserve(skb, NET_IP_ALIGN + l2_offset);\r\ntile_net_receive_skb(dev, skb, idesc, len);\r\n}\r\ngxio_mpipe_iqueue_consume(&info->mpipe[instance].iqueue, idesc);\r\nreturn !filter;\r\n}\r\nstatic int tile_net_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nunsigned int work = 0;\r\ngxio_mpipe_idesc_t *idesc;\r\nint instance, i, n;\r\nstruct mpipe_data *md;\r\nstruct info_mpipe *info_mpipe =\r\ncontainer_of(napi, struct info_mpipe, napi);\r\nif (budget <= 0)\r\ngoto done;\r\ninstance = info_mpipe->instance;\r\nwhile ((n = gxio_mpipe_iqueue_try_peek(\r\n&info_mpipe->iqueue,\r\n&idesc)) > 0) {\r\nfor (i = 0; i < n; i++) {\r\nif (i == TILE_NET_BATCH)\r\ngoto done;\r\nif (tile_net_handle_packet(instance,\r\nidesc + i)) {\r\nif (++work >= budget)\r\ngoto done;\r\n}\r\n}\r\n}\r\nnapi_complete(&info_mpipe->napi);\r\nmd = &mpipe_data[instance];\r\ngxio_mpipe_enable_notif_ring_interrupt(\r\n&md->context, info->mpipe[instance].iqueue.ring);\r\nif (gxio_mpipe_iqueue_try_peek(&info_mpipe->iqueue, &idesc) > 0)\r\nnapi_schedule(&info_mpipe->napi);\r\ndone:\r\ntile_net_provide_needed_buffers();\r\nreturn work;\r\n}\r\nstatic irqreturn_t tile_net_handle_ingress_irq(int irq, void *id)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nnapi_schedule(&info->mpipe[(uint64_t)id].napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int tile_net_free_comps(gxio_mpipe_equeue_t *equeue,\r\nstruct tile_net_comps *comps,\r\nint limit, bool force_update)\r\n{\r\nint n = 0;\r\nwhile (comps->comp_last < comps->comp_next) {\r\nunsigned int cid = comps->comp_last % TILE_NET_MAX_COMPS;\r\nstruct tile_net_comp *comp = &comps->comp_queue[cid];\r\nif (!gxio_mpipe_equeue_is_complete(equeue, comp->when,\r\nforce_update || n == 0))\r\nbreak;\r\ndev_kfree_skb_irq(comp->skb);\r\ncomps->comp_last++;\r\nif (++n == limit)\r\nbreak;\r\n}\r\nreturn n;\r\n}\r\nstatic void add_comp(gxio_mpipe_equeue_t *equeue,\r\nstruct tile_net_comps *comps,\r\nuint64_t when, struct sk_buff *skb)\r\n{\r\nint cid = comps->comp_next % TILE_NET_MAX_COMPS;\r\ncomps->comp_queue[cid].when = when;\r\ncomps->comp_queue[cid].skb = skb;\r\ncomps->comp_next++;\r\n}\r\nstatic void tile_net_schedule_tx_wake_timer(struct net_device *dev,\r\nint tx_queue_idx)\r\n{\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, tx_queue_idx);\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nint instance = priv->instance;\r\nstruct tile_net_tx_wake *tx_wake =\r\n&info->mpipe[instance].tx_wake[priv->echannel];\r\nhrtimer_start(&tx_wake->timer,\r\nktime_set(0, TX_TIMER_DELAY_USEC * 1000UL),\r\nHRTIMER_MODE_REL_PINNED);\r\n}\r\nstatic enum hrtimer_restart tile_net_handle_tx_wake_timer(struct hrtimer *t)\r\n{\r\nstruct tile_net_tx_wake *tx_wake =\r\ncontainer_of(t, struct tile_net_tx_wake, timer);\r\nnetif_wake_subqueue(tx_wake->dev, tx_wake->tx_queue_idx);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nstatic void tile_net_schedule_egress_timer(void)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nif (!info->egress_timer_scheduled) {\r\nhrtimer_start(&info->egress_timer,\r\nktime_set(0, EGRESS_TIMER_DELAY_USEC * 1000UL),\r\nHRTIMER_MODE_REL_PINNED);\r\ninfo->egress_timer_scheduled = true;\r\n}\r\n}\r\nstatic enum hrtimer_restart tile_net_handle_egress_timer(struct hrtimer *t)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nunsigned long irqflags;\r\nbool pending = false;\r\nint i, instance;\r\nlocal_irq_save(irqflags);\r\ninfo->egress_timer_scheduled = false;\r\nfor (instance = 0; instance < NR_MPIPE_MAX &&\r\ninfo->mpipe[instance].has_iqueue; instance++) {\r\nfor (i = 0; i < TILE_NET_CHANNELS; i++) {\r\nstruct tile_net_egress *egress =\r\n&mpipe_data[instance].egress_for_echannel[i];\r\nstruct tile_net_comps *comps =\r\ninfo->mpipe[instance].comps_for_echannel[i];\r\nif (!egress || comps->comp_last >= comps->comp_next)\r\ncontinue;\r\ntile_net_free_comps(egress->equeue, comps, -1, true);\r\npending = pending ||\r\n(comps->comp_last < comps->comp_next);\r\n}\r\n}\r\nif (pending)\r\ntile_net_schedule_egress_timer();\r\nlocal_irq_restore(irqflags);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nstatic int ptp_mpipe_adjfreq(struct ptp_clock_info *ptp, s32 ppb)\r\n{\r\nint ret = 0;\r\nstruct mpipe_data *md = container_of(ptp, struct mpipe_data, caps);\r\nmutex_lock(&md->ptp_lock);\r\nif (gxio_mpipe_adjust_timestamp_freq(&md->context, ppb))\r\nret = -EINVAL;\r\nmutex_unlock(&md->ptp_lock);\r\nreturn ret;\r\n}\r\nstatic int ptp_mpipe_adjtime(struct ptp_clock_info *ptp, s64 delta)\r\n{\r\nint ret = 0;\r\nstruct mpipe_data *md = container_of(ptp, struct mpipe_data, caps);\r\nmutex_lock(&md->ptp_lock);\r\nif (gxio_mpipe_adjust_timestamp(&md->context, delta))\r\nret = -EBUSY;\r\nmutex_unlock(&md->ptp_lock);\r\nreturn ret;\r\n}\r\nstatic int ptp_mpipe_gettime(struct ptp_clock_info *ptp, struct timespec *ts)\r\n{\r\nint ret = 0;\r\nstruct mpipe_data *md = container_of(ptp, struct mpipe_data, caps);\r\nmutex_lock(&md->ptp_lock);\r\nif (gxio_mpipe_get_timestamp(&md->context, ts))\r\nret = -EBUSY;\r\nmutex_unlock(&md->ptp_lock);\r\nreturn ret;\r\n}\r\nstatic int ptp_mpipe_settime(struct ptp_clock_info *ptp,\r\nconst struct timespec *ts)\r\n{\r\nint ret = 0;\r\nstruct mpipe_data *md = container_of(ptp, struct mpipe_data, caps);\r\nmutex_lock(&md->ptp_lock);\r\nif (gxio_mpipe_set_timestamp(&md->context, ts))\r\nret = -EBUSY;\r\nmutex_unlock(&md->ptp_lock);\r\nreturn ret;\r\n}\r\nstatic int ptp_mpipe_enable(struct ptp_clock_info *ptp,\r\nstruct ptp_clock_request *request, int on)\r\n{\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic void register_ptp_clock(struct net_device *dev, struct mpipe_data *md)\r\n{\r\nstruct timespec ts;\r\ngetnstimeofday(&ts);\r\ngxio_mpipe_set_timestamp(&md->context, &ts);\r\nmutex_init(&md->ptp_lock);\r\nmd->caps = ptp_mpipe_caps;\r\nmd->ptp_clock = ptp_clock_register(&md->caps, NULL);\r\nif (IS_ERR(md->ptp_clock))\r\nnetdev_err(dev, "ptp_clock_register failed %ld\n",\r\nPTR_ERR(md->ptp_clock));\r\n}\r\nstatic void init_ptp_dev(struct tile_net_priv *priv)\r\n{\r\npriv->stamp_cfg.rx_filter = HWTSTAMP_FILTER_NONE;\r\npriv->stamp_cfg.tx_type = HWTSTAMP_TX_OFF;\r\n}\r\nstatic void enable_ingress_irq(void *irq)\r\n{\r\nenable_percpu_irq((long)irq, 0);\r\n}\r\nstatic void disable_ingress_irq(void *irq)\r\n{\r\ndisable_percpu_irq((long)irq);\r\n}\r\nstatic int tile_net_update(struct net_device *dev)\r\n{\r\nstatic gxio_mpipe_rules_t rules;\r\nbool saw_channel = false;\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nint channel;\r\nint rc;\r\nint cpu;\r\nsaw_channel = false;\r\ngxio_mpipe_rules_init(&rules, &md->context);\r\nfor (channel = 0; channel < TILE_NET_CHANNELS; channel++) {\r\nif (md->tile_net_devs_for_channel[channel] == NULL)\r\ncontinue;\r\nif (!saw_channel) {\r\nsaw_channel = true;\r\ngxio_mpipe_rules_begin(&rules, md->first_bucket,\r\nmd->num_buckets, NULL);\r\ngxio_mpipe_rules_set_headroom(&rules, NET_IP_ALIGN);\r\n}\r\ngxio_mpipe_rules_add_channel(&rules, channel);\r\n}\r\nrc = gxio_mpipe_rules_commit(&rules);\r\nif (rc != 0) {\r\nnetdev_warn(dev, "gxio_mpipe_rules_commit: mpipe[%d] %d\n",\r\ninstance, rc);\r\nreturn -EIO;\r\n}\r\nif (!saw_channel)\r\non_each_cpu(disable_ingress_irq,\r\n(void *)(long)(md->ingress_irq), 1);\r\nfor_each_online_cpu(cpu) {\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nif (!info->mpipe[instance].has_iqueue)\r\ncontinue;\r\nif (saw_channel) {\r\nif (!info->mpipe[instance].napi_added) {\r\nnetif_napi_add(dev, &info->mpipe[instance].napi,\r\ntile_net_poll, TILE_NET_WEIGHT);\r\ninfo->mpipe[instance].napi_added = true;\r\n}\r\nif (!info->mpipe[instance].napi_enabled) {\r\nnapi_enable(&info->mpipe[instance].napi);\r\ninfo->mpipe[instance].napi_enabled = true;\r\n}\r\n} else {\r\nif (info->mpipe[instance].napi_enabled) {\r\nnapi_disable(&info->mpipe[instance].napi);\r\ninfo->mpipe[instance].napi_enabled = false;\r\n}\r\n}\r\n}\r\nif (saw_channel)\r\non_each_cpu(enable_ingress_irq,\r\n(void *)(long)(md->ingress_irq), 1);\r\nif (saw_channel)\r\nsim_enable_mpipe_links(instance, -1);\r\nreturn 0;\r\n}\r\nstatic int create_buffer_stack(struct net_device *dev,\r\nint kind, size_t num_buffers)\r\n{\r\npte_t hash_pte = pte_set_home((pte_t) { 0 }, PAGE_HOME_HASH);\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nsize_t needed = gxio_mpipe_calc_buffer_stack_bytes(num_buffers);\r\nint stack_idx = md->first_buffer_stack + kind;\r\nvoid *va;\r\nint i, rc;\r\nmd->buffer_stack_bytes[kind] =\r\nALIGN(needed, 64 * 1024);\r\nva = alloc_pages_exact(md->buffer_stack_bytes[kind], GFP_KERNEL);\r\nif (va == NULL) {\r\nnetdev_err(dev,\r\n"Could not alloc %zd bytes for buffer stack %d\n",\r\nmd->buffer_stack_bytes[kind], kind);\r\nreturn -ENOMEM;\r\n}\r\nrc = gxio_mpipe_init_buffer_stack(&md->context, stack_idx,\r\nbuffer_size_enums[kind], va,\r\nmd->buffer_stack_bytes[kind], 0);\r\nif (rc != 0) {\r\nnetdev_err(dev, "gxio_mpipe_init_buffer_stack: mpipe[%d] %d\n",\r\ninstance, rc);\r\nfree_pages_exact(va, md->buffer_stack_bytes[kind]);\r\nreturn rc;\r\n}\r\nmd->buffer_stack_vas[kind] = va;\r\nrc = gxio_mpipe_register_client_memory(&md->context, stack_idx,\r\nhash_pte, 0);\r\nif (rc != 0) {\r\nnetdev_err(dev,\r\n"gxio_mpipe_register_client_memory: mpipe[%d] %d\n",\r\ninstance, rc);\r\nreturn rc;\r\n}\r\nfor (i = 0; i < num_buffers; i++) {\r\nif (!tile_net_provide_buffer(instance, kind)) {\r\nnetdev_err(dev, "Cannot allocate initial sk_bufs!\n");\r\nreturn -ENOMEM;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int init_buffer_stacks(struct net_device *dev,\r\nint network_cpus_count)\r\n{\r\nint num_kinds = MAX_KINDS - (jumbo_num == 0);\r\nsize_t num_buffers;\r\nint rc;\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nrc = gxio_mpipe_alloc_buffer_stacks(&md->context, num_kinds, 0, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev,\r\n"gxio_mpipe_alloc_buffer_stacks: mpipe[%d] %d\n",\r\ninstance, rc);\r\nreturn rc;\r\n}\r\nmd->first_buffer_stack = rc;\r\nnum_buffers =\r\nnetwork_cpus_count * (IQUEUE_ENTRIES + TILE_NET_BATCH);\r\nif (rc >= 0)\r\nrc = create_buffer_stack(dev, 0, num_buffers);\r\nif (rc >= 0)\r\nrc = create_buffer_stack(dev, 1, num_buffers);\r\nif (rc >= 0 && jumbo_num != 0)\r\nrc = create_buffer_stack(dev, 2, jumbo_num);\r\nreturn rc;\r\n}\r\nstatic int alloc_percpu_mpipe_resources(struct net_device *dev,\r\nint cpu, int ring)\r\n{\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nint order, i, rc;\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nstruct page *page;\r\nvoid *addr;\r\norder = get_order(COMPS_SIZE);\r\npage = homecache_alloc_pages(GFP_KERNEL, order, cpu);\r\nif (page == NULL) {\r\nnetdev_err(dev, "Failed to alloc %zd bytes comps memory\n",\r\nCOMPS_SIZE);\r\nreturn -ENOMEM;\r\n}\r\naddr = pfn_to_kaddr(page_to_pfn(page));\r\nmemset(addr, 0, COMPS_SIZE);\r\nfor (i = 0; i < TILE_NET_CHANNELS; i++)\r\ninfo->mpipe[instance].comps_for_echannel[i] =\r\naddr + i * sizeof(struct tile_net_comps);\r\nif (cpu_isset(cpu, network_cpus_map)) {\r\norder = get_order(NOTIF_RING_SIZE);\r\npage = homecache_alloc_pages(GFP_KERNEL, order, cpu);\r\nif (page == NULL) {\r\nnetdev_err(dev,\r\n"Failed to alloc %zd bytes iqueue memory\n",\r\nNOTIF_RING_SIZE);\r\nreturn -ENOMEM;\r\n}\r\naddr = pfn_to_kaddr(page_to_pfn(page));\r\nrc = gxio_mpipe_iqueue_init(&info->mpipe[instance].iqueue,\r\n&md->context, ring++, addr,\r\nNOTIF_RING_SIZE, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev,\r\n"gxio_mpipe_iqueue_init failed: %d\n", rc);\r\nreturn rc;\r\n}\r\ninfo->mpipe[instance].has_iqueue = true;\r\n}\r\nreturn ring;\r\n}\r\nstatic int init_notif_group_and_buckets(struct net_device *dev,\r\nint ring, int network_cpus_count)\r\n{\r\nint group, rc;\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nrc = gxio_mpipe_alloc_notif_groups(&md->context, 1, 0, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev, "gxio_mpipe_alloc_notif_groups: mpipe[%d] %d\n",\r\ninstance, rc);\r\nreturn rc;\r\n}\r\ngroup = rc;\r\nif (network_cpus_count > 4)\r\nmd->num_buckets = 256;\r\nelse if (network_cpus_count > 1)\r\nmd->num_buckets = 16;\r\nrc = gxio_mpipe_alloc_buckets(&md->context, md->num_buckets, 0, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev, "gxio_mpipe_alloc_buckets: mpipe[%d] %d\n",\r\ninstance, rc);\r\nreturn rc;\r\n}\r\nmd->first_bucket = rc;\r\nrc = gxio_mpipe_init_notif_group_and_buckets(\r\n&md->context, group, ring, network_cpus_count,\r\nmd->first_bucket, md->num_buckets,\r\nGXIO_MPIPE_BUCKET_STICKY_FLOW_LOCALITY);\r\nif (rc != 0) {\r\nnetdev_err(dev, "gxio_mpipe_init_notif_group_and_buckets: "\r\n"mpipe[%d] %d\n", instance, rc);\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nstatic int tile_net_setup_interrupts(struct net_device *dev)\r\n{\r\nint cpu, rc, irq;\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nirq = md->ingress_irq;\r\nif (irq < 0) {\r\nirq = irq_alloc_hwirq(-1);\r\nif (!irq) {\r\nnetdev_err(dev,\r\n"create_irq failed: mpipe[%d] %d\n",\r\ninstance, irq);\r\nreturn irq;\r\n}\r\ntile_irq_activate(irq, TILE_IRQ_PERCPU);\r\nrc = request_irq(irq, tile_net_handle_ingress_irq,\r\n0, "tile_net", (void *)((uint64_t)instance));\r\nif (rc != 0) {\r\nnetdev_err(dev, "request_irq failed: mpipe[%d] %d\n",\r\ninstance, rc);\r\nirq_free_hwirq(irq);\r\nreturn rc;\r\n}\r\nmd->ingress_irq = irq;\r\n}\r\nfor_each_online_cpu(cpu) {\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nif (info->mpipe[instance].has_iqueue) {\r\ngxio_mpipe_request_notif_ring_interrupt(&md->context,\r\ncpu_x(cpu), cpu_y(cpu), KERNEL_PL, irq,\r\ninfo->mpipe[instance].iqueue.ring);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void tile_net_init_mpipe_fail(int instance)\r\n{\r\nint kind, cpu;\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nfor (kind = 0; kind < MAX_KINDS; kind++) {\r\nif (md->buffer_stack_vas[kind] != NULL) {\r\ntile_net_pop_all_buffers(instance,\r\nmd->first_buffer_stack +\r\nkind);\r\n}\r\n}\r\ngxio_mpipe_destroy(&md->context);\r\nfor_each_online_cpu(cpu) {\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nfree_pages(\r\n(unsigned long)(\r\ninfo->mpipe[instance].comps_for_echannel[0]),\r\nget_order(COMPS_SIZE));\r\ninfo->mpipe[instance].comps_for_echannel[0] = NULL;\r\nfree_pages((unsigned long)(info->mpipe[instance].iqueue.idescs),\r\nget_order(NOTIF_RING_SIZE));\r\ninfo->mpipe[instance].iqueue.idescs = NULL;\r\n}\r\nfor (kind = 0; kind < MAX_KINDS; kind++) {\r\nif (md->buffer_stack_vas[kind] != NULL) {\r\nfree_pages_exact(md->buffer_stack_vas[kind],\r\nmd->buffer_stack_bytes[kind]);\r\nmd->buffer_stack_vas[kind] = NULL;\r\n}\r\n}\r\nmd->first_buffer_stack = -1;\r\nmd->first_bucket = -1;\r\n}\r\nstatic int tile_net_init_mpipe(struct net_device *dev)\r\n{\r\nint rc;\r\nint cpu;\r\nint first_ring, ring;\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nint network_cpus_count = cpus_weight(network_cpus_map);\r\nif (!hash_default) {\r\nnetdev_err(dev, "Networking requires hash_default!\n");\r\nreturn -EIO;\r\n}\r\nrc = gxio_mpipe_init(&md->context, instance);\r\nif (rc != 0) {\r\nnetdev_err(dev, "gxio_mpipe_init: mpipe[%d] %d\n",\r\ninstance, rc);\r\nreturn -EIO;\r\n}\r\nrc = init_buffer_stacks(dev, network_cpus_count);\r\nif (rc != 0)\r\ngoto fail;\r\nrc = gxio_mpipe_alloc_notif_rings(&md->context,\r\nnetwork_cpus_count, 0, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev, "gxio_mpipe_alloc_notif_rings failed %d\n",\r\nrc);\r\ngoto fail;\r\n}\r\nfirst_ring = rc;\r\nring = first_ring;\r\nfor_each_online_cpu(cpu) {\r\nrc = alloc_percpu_mpipe_resources(dev, cpu, ring);\r\nif (rc < 0)\r\ngoto fail;\r\nring = rc;\r\n}\r\nrc = init_notif_group_and_buckets(dev, first_ring, network_cpus_count);\r\nif (rc != 0)\r\ngoto fail;\r\nrc = tile_net_setup_interrupts(dev);\r\nif (rc != 0)\r\ngoto fail;\r\nregister_ptp_clock(dev, md);\r\nreturn 0;\r\nfail:\r\ntile_net_init_mpipe_fail(instance);\r\nreturn rc;\r\n}\r\nstatic int tile_net_init_egress(struct net_device *dev, int echannel)\r\n{\r\nstatic int ering = -1;\r\nstruct page *headers_page, *edescs_page, *equeue_page;\r\ngxio_mpipe_edesc_t *edescs;\r\ngxio_mpipe_equeue_t *equeue;\r\nunsigned char *headers;\r\nint headers_order, edescs_order, equeue_order;\r\nsize_t edescs_size;\r\nint rc = -ENOMEM;\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nif (md->egress_for_echannel[echannel].equeue != NULL)\r\nreturn 0;\r\nheaders_order = get_order(EQUEUE_ENTRIES * HEADER_BYTES);\r\nheaders_page = alloc_pages(GFP_KERNEL, headers_order);\r\nif (headers_page == NULL) {\r\nnetdev_warn(dev,\r\n"Could not alloc %zd bytes for TSO headers.\n",\r\nPAGE_SIZE << headers_order);\r\ngoto fail;\r\n}\r\nheaders = pfn_to_kaddr(page_to_pfn(headers_page));\r\nedescs_size = EQUEUE_ENTRIES * sizeof(*edescs);\r\nedescs_order = get_order(edescs_size);\r\nedescs_page = alloc_pages(GFP_KERNEL, edescs_order);\r\nif (edescs_page == NULL) {\r\nnetdev_warn(dev,\r\n"Could not alloc %zd bytes for eDMA ring.\n",\r\nedescs_size);\r\ngoto fail_headers;\r\n}\r\nedescs = pfn_to_kaddr(page_to_pfn(edescs_page));\r\nequeue_order = get_order(sizeof(*equeue));\r\nequeue_page = alloc_pages(GFP_KERNEL, equeue_order);\r\nif (equeue_page == NULL) {\r\nnetdev_warn(dev,\r\n"Could not alloc %zd bytes for equeue info.\n",\r\nPAGE_SIZE << equeue_order);\r\ngoto fail_edescs;\r\n}\r\nequeue = pfn_to_kaddr(page_to_pfn(equeue_page));\r\nif (ering < 0) {\r\nrc = gxio_mpipe_alloc_edma_rings(&md->context, 1, 0, 0);\r\nif (rc < 0) {\r\nnetdev_warn(dev, "gxio_mpipe_alloc_edma_rings: "\r\n"mpipe[%d] %d\n", instance, rc);\r\ngoto fail_equeue;\r\n}\r\nering = rc;\r\n}\r\nrc = gxio_mpipe_equeue_init(equeue, &md->context, ering, echannel,\r\nedescs, edescs_size, 0);\r\nif (rc != 0) {\r\nnetdev_err(dev, "gxio_mpipe_equeue_init: mpipe[%d] %d\n",\r\ninstance, rc);\r\ngoto fail_equeue;\r\n}\r\nering = -1;\r\nif (jumbo_num != 0) {\r\nif (gxio_mpipe_equeue_set_snf_size(equeue, 10368) < 0) {\r\nnetdev_warn(dev, "Jumbo packets may not be egressed"\r\n" properly on channel %d\n", echannel);\r\n}\r\n}\r\nmd->egress_for_echannel[echannel].equeue = equeue;\r\nmd->egress_for_echannel[echannel].headers = headers;\r\nreturn 0;\r\nfail_equeue:\r\n__free_pages(equeue_page, equeue_order);\r\nfail_edescs:\r\n__free_pages(edescs_page, edescs_order);\r\nfail_headers:\r\n__free_pages(headers_page, headers_order);\r\nfail:\r\nreturn rc;\r\n}\r\nstatic int tile_net_link_open(struct net_device *dev, gxio_mpipe_link_t *link,\r\nconst char *link_name)\r\n{\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nint rc = gxio_mpipe_link_open(link, &md->context, link_name, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev, "Failed to open '%s', mpipe[%d], %d\n",\r\nlink_name, instance, rc);\r\nreturn rc;\r\n}\r\nif (jumbo_num != 0) {\r\nu32 attr = GXIO_MPIPE_LINK_RECEIVE_JUMBO;\r\nrc = gxio_mpipe_link_set_attr(link, attr, 1);\r\nif (rc != 0) {\r\nnetdev_err(dev,\r\n"Cannot receive jumbo packets on '%s'\n",\r\nlink_name);\r\ngxio_mpipe_link_close(link);\r\nreturn rc;\r\n}\r\n}\r\nrc = gxio_mpipe_link_channel(link);\r\nif (rc < 0 || rc >= TILE_NET_CHANNELS) {\r\nnetdev_err(dev, "gxio_mpipe_link_channel bad value: %d\n", rc);\r\ngxio_mpipe_link_close(link);\r\nreturn -EINVAL;\r\n}\r\nreturn rc;\r\n}\r\nstatic int tile_net_open(struct net_device *dev)\r\n{\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nint cpu, rc, instance;\r\nmutex_lock(&tile_net_devs_for_channel_mutex);\r\nrc = gxio_mpipe_link_instance(dev->name);\r\nif (rc < 0 || rc >= NR_MPIPE_MAX) {\r\nmutex_unlock(&tile_net_devs_for_channel_mutex);\r\nreturn -EIO;\r\n}\r\npriv->instance = rc;\r\ninstance = rc;\r\nif (!mpipe_data[rc].context.mmio_fast_base) {\r\nrc = tile_net_init_mpipe(dev);\r\nif (rc != 0)\r\ngoto fail;\r\n}\r\nif (unlikely((loopify_link_name != NULL) &&\r\n!strcmp(dev->name, loopify_link_name))) {\r\nrc = tile_net_link_open(dev, &priv->link, "loop0");\r\nif (rc < 0)\r\ngoto fail;\r\npriv->channel = rc;\r\nrc = tile_net_link_open(dev, &priv->loopify_link, "loop1");\r\nif (rc < 0)\r\ngoto fail;\r\npriv->loopify_channel = rc;\r\npriv->echannel = rc;\r\n} else {\r\nrc = tile_net_link_open(dev, &priv->link, dev->name);\r\nif (rc < 0)\r\ngoto fail;\r\npriv->channel = rc;\r\npriv->echannel = rc;\r\n}\r\nrc = tile_net_init_egress(dev, priv->echannel);\r\nif (rc != 0)\r\ngoto fail;\r\nmpipe_data[instance].tile_net_devs_for_channel[priv->channel] = dev;\r\nrc = tile_net_update(dev);\r\nif (rc != 0)\r\ngoto fail;\r\nmutex_unlock(&tile_net_devs_for_channel_mutex);\r\nfor_each_online_cpu(cpu) {\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nstruct tile_net_tx_wake *tx_wake =\r\n&info->mpipe[instance].tx_wake[priv->echannel];\r\nhrtimer_init(&tx_wake->timer, CLOCK_MONOTONIC,\r\nHRTIMER_MODE_REL);\r\ntx_wake->tx_queue_idx = cpu;\r\ntx_wake->timer.function = tile_net_handle_tx_wake_timer;\r\ntx_wake->dev = dev;\r\n}\r\nfor_each_online_cpu(cpu)\r\nnetif_start_subqueue(dev, cpu);\r\nnetif_carrier_on(dev);\r\nreturn 0;\r\nfail:\r\nif (priv->loopify_channel >= 0) {\r\nif (gxio_mpipe_link_close(&priv->loopify_link) != 0)\r\nnetdev_warn(dev, "Failed to close loopify link!\n");\r\npriv->loopify_channel = -1;\r\n}\r\nif (priv->channel >= 0) {\r\nif (gxio_mpipe_link_close(&priv->link) != 0)\r\nnetdev_warn(dev, "Failed to close link!\n");\r\npriv->channel = -1;\r\n}\r\npriv->echannel = -1;\r\nmpipe_data[instance].tile_net_devs_for_channel[priv->channel] = NULL;\r\nmutex_unlock(&tile_net_devs_for_channel_mutex);\r\nreturn (rc > -512) ? rc : -EIO;\r\n}\r\nstatic int tile_net_stop(struct net_device *dev)\r\n{\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nint cpu;\r\nint instance = priv->instance;\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nfor_each_online_cpu(cpu) {\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nstruct tile_net_tx_wake *tx_wake =\r\n&info->mpipe[instance].tx_wake[priv->echannel];\r\nhrtimer_cancel(&tx_wake->timer);\r\nnetif_stop_subqueue(dev, cpu);\r\n}\r\nmutex_lock(&tile_net_devs_for_channel_mutex);\r\nmd->tile_net_devs_for_channel[priv->channel] = NULL;\r\n(void)tile_net_update(dev);\r\nif (priv->loopify_channel >= 0) {\r\nif (gxio_mpipe_link_close(&priv->loopify_link) != 0)\r\nnetdev_warn(dev, "Failed to close loopify link!\n");\r\npriv->loopify_channel = -1;\r\n}\r\nif (priv->channel >= 0) {\r\nif (gxio_mpipe_link_close(&priv->link) != 0)\r\nnetdev_warn(dev, "Failed to close link!\n");\r\npriv->channel = -1;\r\n}\r\npriv->echannel = -1;\r\nmutex_unlock(&tile_net_devs_for_channel_mutex);\r\nreturn 0;\r\n}\r\nstatic inline void *tile_net_frag_buf(skb_frag_t *f)\r\n{\r\nunsigned long pfn = page_to_pfn(skb_frag_page(f));\r\nreturn pfn_to_kaddr(pfn) + f->page_offset;\r\n}\r\nstatic s64 tile_net_equeue_try_reserve(struct net_device *dev,\r\nint tx_queue_idx,\r\nstruct tile_net_comps *comps,\r\ngxio_mpipe_equeue_t *equeue,\r\nint num_edescs)\r\n{\r\nif (comps->comp_next - comps->comp_last < TILE_NET_MAX_COMPS - 1 ||\r\ntile_net_free_comps(equeue, comps, 32, false) != 0) {\r\ns64 slot = gxio_mpipe_equeue_try_reserve(equeue, num_edescs);\r\nif (slot >= 0)\r\nreturn slot;\r\ntile_net_free_comps(equeue, comps, TILE_NET_MAX_COMPS, false);\r\nslot = gxio_mpipe_equeue_try_reserve(equeue, num_edescs);\r\nif (slot >= 0)\r\nreturn slot;\r\n}\r\nnetif_stop_subqueue(dev, tx_queue_idx);\r\ntile_net_schedule_tx_wake_timer(dev, tx_queue_idx);\r\nreturn -1;\r\n}\r\nstatic int tso_count_edescs(struct sk_buff *skb)\r\n{\r\nstruct skb_shared_info *sh = skb_shinfo(skb);\r\nunsigned int sh_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nunsigned int data_len = skb->len - sh_len;\r\nunsigned int p_len = sh->gso_size;\r\nlong f_id = -1;\r\nlong f_size = skb_headlen(skb) - sh_len;\r\nlong f_used = 0;\r\nlong n;\r\nint num_edescs = 0;\r\nint segment;\r\nfor (segment = 0; segment < sh->gso_segs; segment++) {\r\nunsigned int p_used = 0;\r\nfor (num_edescs++; p_used < p_len; num_edescs++) {\r\nwhile (f_used >= f_size) {\r\nf_id++;\r\nf_size = skb_frag_size(&sh->frags[f_id]);\r\nf_used = 0;\r\n}\r\nn = p_len - p_used;\r\nif (n > f_size - f_used)\r\nn = f_size - f_used;\r\nf_used += n;\r\np_used += n;\r\n}\r\ndata_len -= p_len;\r\nif (data_len < p_len)\r\np_len = data_len;\r\n}\r\nreturn num_edescs;\r\n}\r\nstatic void tso_headers_prepare(struct sk_buff *skb, unsigned char *headers,\r\ns64 slot)\r\n{\r\nstruct skb_shared_info *sh = skb_shinfo(skb);\r\nstruct iphdr *ih;\r\nstruct ipv6hdr *ih6;\r\nstruct tcphdr *th;\r\nunsigned int sh_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nunsigned int data_len = skb->len - sh_len;\r\nunsigned char *data = skb->data;\r\nunsigned int ih_off, th_off, p_len;\r\nunsigned int isum_seed, tsum_seed, seq;\r\nunsigned int uninitialized_var(id);\r\nint is_ipv6;\r\nlong f_id = -1;\r\nlong f_size = skb_headlen(skb) - sh_len;\r\nlong f_used = 0;\r\nlong n;\r\nint segment;\r\nis_ipv6 = skb_is_gso_v6(skb);\r\nif (is_ipv6) {\r\nih6 = ipv6_hdr(skb);\r\nih_off = skb_network_offset(skb);\r\n} else {\r\nih = ip_hdr(skb);\r\nih_off = skb_network_offset(skb);\r\nisum_seed = ((0xFFFF - ih->check) +\r\n(0xFFFF - ih->tot_len) +\r\n(0xFFFF - ih->id));\r\nid = ntohs(ih->id);\r\n}\r\nth = tcp_hdr(skb);\r\nth_off = skb_transport_offset(skb);\r\np_len = sh->gso_size;\r\ntsum_seed = th->check + (0xFFFF ^ htons(skb->len));\r\nseq = ntohl(th->seq);\r\nfor (segment = 0; segment < sh->gso_segs; segment++) {\r\nunsigned char *buf;\r\nunsigned int p_used = 0;\r\nbuf = headers + (slot % EQUEUE_ENTRIES) * HEADER_BYTES +\r\nNET_IP_ALIGN;\r\nmemcpy(buf, data, sh_len);\r\nif (is_ipv6) {\r\nih6 = (struct ipv6hdr *)(buf + ih_off);\r\nih6->payload_len = htons(sh_len + p_len - ih_off -\r\nsizeof(*ih6));\r\n} else {\r\nih = (struct iphdr *)(buf + ih_off);\r\nih->tot_len = htons(sh_len + p_len - ih_off);\r\nih->id = htons(id++);\r\nih->check = csum_long(isum_seed + ih->tot_len +\r\nih->id) ^ 0xffff;\r\n}\r\nth = (struct tcphdr *)(buf + th_off);\r\nth->seq = htonl(seq);\r\nth->check = csum_long(tsum_seed + htons(sh_len + p_len));\r\nif (segment != sh->gso_segs - 1) {\r\nth->fin = 0;\r\nth->psh = 0;\r\n}\r\nslot++;\r\nwhile (p_used < p_len) {\r\nwhile (f_used >= f_size) {\r\nf_id++;\r\nf_size = skb_frag_size(&sh->frags[f_id]);\r\nf_used = 0;\r\n}\r\nn = p_len - p_used;\r\nif (n > f_size - f_used)\r\nn = f_size - f_used;\r\nf_used += n;\r\np_used += n;\r\nslot++;\r\n}\r\nseq += p_len;\r\ndata_len -= p_len;\r\nif (data_len < p_len)\r\np_len = data_len;\r\n}\r\nwmb();\r\n}\r\nstatic void tso_egress(struct net_device *dev, gxio_mpipe_equeue_t *equeue,\r\nstruct sk_buff *skb, unsigned char *headers, s64 slot)\r\n{\r\nstruct skb_shared_info *sh = skb_shinfo(skb);\r\nint instance = mpipe_instance(dev);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nunsigned int sh_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nunsigned int data_len = skb->len - sh_len;\r\nunsigned int p_len = sh->gso_size;\r\ngxio_mpipe_edesc_t edesc_head = { { 0 } };\r\ngxio_mpipe_edesc_t edesc_body = { { 0 } };\r\nlong f_id = -1;\r\nlong f_size = skb_headlen(skb) - sh_len;\r\nlong f_used = 0;\r\nvoid *f_data = skb->data + sh_len;\r\nlong n;\r\nunsigned long tx_packets = 0, tx_bytes = 0;\r\nunsigned int csum_start;\r\nint segment;\r\ncsum_start = skb_checksum_start_offset(skb);\r\nedesc_head.csum = 1;\r\nedesc_head.csum_start = csum_start;\r\nedesc_head.csum_dest = csum_start + skb->csum_offset;\r\nedesc_head.xfer_size = sh_len;\r\nedesc_head.stack_idx = md->first_buffer_stack;\r\nedesc_body.stack_idx = md->first_buffer_stack;\r\nfor (segment = 0; segment < sh->gso_segs; segment++) {\r\nunsigned char *buf;\r\nunsigned int p_used = 0;\r\nbuf = headers + (slot % EQUEUE_ENTRIES) * HEADER_BYTES +\r\nNET_IP_ALIGN;\r\nedesc_head.va = va_to_tile_io_addr(buf);\r\ngxio_mpipe_equeue_put_at(equeue, edesc_head, slot);\r\nslot++;\r\nwhile (p_used < p_len) {\r\nvoid *va;\r\nwhile (f_used >= f_size) {\r\nf_id++;\r\nf_size = skb_frag_size(&sh->frags[f_id]);\r\nf_data = tile_net_frag_buf(&sh->frags[f_id]);\r\nf_used = 0;\r\n}\r\nva = f_data + f_used;\r\nn = p_len - p_used;\r\nif (n > f_size - f_used)\r\nn = f_size - f_used;\r\nf_used += n;\r\np_used += n;\r\nedesc_body.va = va_to_tile_io_addr(va);\r\nedesc_body.xfer_size = n;\r\nedesc_body.bound = !(p_used < p_len);\r\ngxio_mpipe_equeue_put_at(equeue, edesc_body, slot);\r\nslot++;\r\n}\r\ntx_packets++;\r\ntx_bytes += sh_len + p_len;\r\ndata_len -= p_len;\r\nif (data_len < p_len)\r\np_len = data_len;\r\n}\r\ntile_net_stats_add(tx_packets, &dev->stats.tx_packets);\r\ntile_net_stats_add(tx_bytes, &dev->stats.tx_bytes);\r\n}\r\nstatic int tile_net_tx_tso(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nint channel = priv->echannel;\r\nint instance = priv->instance;\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nstruct tile_net_egress *egress = &md->egress_for_echannel[channel];\r\nstruct tile_net_comps *comps =\r\ninfo->mpipe[instance].comps_for_echannel[channel];\r\ngxio_mpipe_equeue_t *equeue = egress->equeue;\r\nunsigned long irqflags;\r\nint num_edescs;\r\ns64 slot;\r\nnum_edescs = tso_count_edescs(skb);\r\nlocal_irq_save(irqflags);\r\nslot = tile_net_equeue_try_reserve(dev, skb->queue_mapping, comps,\r\nequeue, num_edescs);\r\nif (slot < 0) {\r\nlocal_irq_restore(irqflags);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\ntso_headers_prepare(skb, egress->headers, slot);\r\ntso_egress(dev, equeue, skb, egress->headers, slot);\r\nadd_comp(equeue, comps, slot + num_edescs - 1, skb);\r\nlocal_irq_restore(irqflags);\r\ntile_net_schedule_egress_timer();\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic unsigned int tile_net_tx_frags(struct frag *frags,\r\nstruct sk_buff *skb,\r\nvoid *b_data, unsigned int b_len)\r\n{\r\nunsigned int i, n = 0;\r\nstruct skb_shared_info *sh = skb_shinfo(skb);\r\nif (b_len != 0) {\r\nfrags[n].buf = b_data;\r\nfrags[n++].length = b_len;\r\n}\r\nfor (i = 0; i < sh->nr_frags; i++) {\r\nskb_frag_t *f = &sh->frags[i];\r\nfrags[n].buf = tile_net_frag_buf(f);\r\nfrags[n++].length = skb_frag_size(f);\r\n}\r\nreturn n;\r\n}\r\nstatic int tile_net_tx(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nint instance = priv->instance;\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\nstruct tile_net_egress *egress =\r\n&md->egress_for_echannel[priv->echannel];\r\ngxio_mpipe_equeue_t *equeue = egress->equeue;\r\nstruct tile_net_comps *comps =\r\ninfo->mpipe[instance].comps_for_echannel[priv->echannel];\r\nunsigned int len = skb->len;\r\nunsigned char *data = skb->data;\r\nunsigned int num_edescs;\r\nstruct frag frags[MAX_FRAGS];\r\ngxio_mpipe_edesc_t edescs[MAX_FRAGS];\r\nunsigned long irqflags;\r\ngxio_mpipe_edesc_t edesc = { { 0 } };\r\nunsigned int i;\r\ns64 slot;\r\nif (skb_is_gso(skb))\r\nreturn tile_net_tx_tso(skb, dev);\r\nnum_edescs = tile_net_tx_frags(frags, skb, data, skb_headlen(skb));\r\nedesc.stack_idx = md->first_buffer_stack;\r\nfor (i = 0; i < num_edescs; i++) {\r\nedesc.xfer_size = frags[i].length;\r\nedesc.va = va_to_tile_io_addr(frags[i].buf);\r\nedescs[i] = edesc;\r\n}\r\nedescs[num_edescs - 1].bound = 1;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nunsigned int csum_start = skb_checksum_start_offset(skb);\r\nedescs[0].csum = 1;\r\nedescs[0].csum_start = csum_start;\r\nedescs[0].csum_dest = csum_start + skb->csum_offset;\r\n}\r\nlocal_irq_save(irqflags);\r\nslot = tile_net_equeue_try_reserve(dev, skb->queue_mapping, comps,\r\nequeue, num_edescs);\r\nif (slot < 0) {\r\nlocal_irq_restore(irqflags);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nfor (i = 0; i < num_edescs; i++)\r\ngxio_mpipe_equeue_put_at(equeue, edescs[i], slot++);\r\ntile_tx_timestamp(skb, instance);\r\nadd_comp(equeue, comps, slot - 1, skb);\r\ntile_net_stats_add(1, &dev->stats.tx_packets);\r\ntile_net_stats_add(max_t(unsigned int, len, ETH_ZLEN),\r\n&dev->stats.tx_bytes);\r\nlocal_irq_restore(irqflags);\r\ntile_net_schedule_egress_timer();\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic u16 tile_net_select_queue(struct net_device *dev, struct sk_buff *skb,\r\nvoid *accel_priv, select_queue_fallback_t fallback)\r\n{\r\nreturn smp_processor_id();\r\n}\r\nstatic void tile_net_tx_timeout(struct net_device *dev)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nnetif_wake_subqueue(dev, cpu);\r\n}\r\nstatic int tile_net_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nif (cmd == SIOCSHWTSTAMP)\r\nreturn tile_hwtstamp_set(dev, rq);\r\nif (cmd == SIOCGHWTSTAMP)\r\nreturn tile_hwtstamp_get(dev, rq);\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic int tile_net_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nif (new_mtu < 68)\r\nreturn -EINVAL;\r\nif (new_mtu > ((jumbo_num != 0) ? 9000 : 1500))\r\nreturn -EINVAL;\r\ndev->mtu = new_mtu;\r\nreturn 0;\r\n}\r\nstatic int tile_net_set_mac_address(struct net_device *dev, void *p)\r\n{\r\nstruct sockaddr *addr = p;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EINVAL;\r\nmemcpy(dev->dev_addr, addr->sa_data, dev->addr_len);\r\nreturn 0;\r\n}\r\nstatic void tile_net_netpoll(struct net_device *dev)\r\n{\r\nint instance = mpipe_instance(dev);\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nstruct mpipe_data *md = &mpipe_data[instance];\r\ndisable_percpu_irq(md->ingress_irq);\r\nnapi_schedule(&info->mpipe[instance].napi);\r\nenable_percpu_irq(md->ingress_irq, 0);\r\n}\r\nstatic void tile_net_setup(struct net_device *dev)\r\n{\r\nnetdev_features_t features = 0;\r\nether_setup(dev);\r\ndev->netdev_ops = &tile_net_ops;\r\ndev->watchdog_timeo = TILE_NET_TIMEOUT;\r\ndev->mtu = 1500;\r\nfeatures |= NETIF_F_HW_CSUM;\r\nfeatures |= NETIF_F_SG;\r\nfeatures |= NETIF_F_TSO;\r\nfeatures |= NETIF_F_TSO6;\r\ndev->hw_features |= features;\r\ndev->vlan_features |= features;\r\ndev->features |= features;\r\n}\r\nstatic void tile_net_dev_init(const char *name, const uint8_t *mac)\r\n{\r\nint ret;\r\nstruct net_device *dev;\r\nstruct tile_net_priv *priv;\r\nif (strncmp(name, "loop", 4) == 0)\r\nreturn;\r\ndev = alloc_netdev_mqs(sizeof(*priv), name, NET_NAME_UNKNOWN,\r\ntile_net_setup, NR_CPUS, 1);\r\nif (!dev) {\r\npr_err("alloc_netdev_mqs(%s) failed\n", name);\r\nreturn;\r\n}\r\npriv = netdev_priv(dev);\r\npriv->dev = dev;\r\npriv->channel = -1;\r\npriv->loopify_channel = -1;\r\npriv->echannel = -1;\r\ninit_ptp_dev(priv);\r\nif (!is_zero_ether_addr(mac))\r\nether_addr_copy(dev->dev_addr, mac);\r\nelse\r\neth_hw_addr_random(dev);\r\nret = register_netdev(dev);\r\nif (ret) {\r\nnetdev_err(dev, "register_netdev failed %d\n", ret);\r\nfree_netdev(dev);\r\nreturn;\r\n}\r\n}\r\nstatic void tile_net_init_module_percpu(void *unused)\r\n{\r\nstruct tile_net_info *info = this_cpu_ptr(&per_cpu_info);\r\nint my_cpu = smp_processor_id();\r\nint instance;\r\nfor (instance = 0; instance < NR_MPIPE_MAX; instance++) {\r\ninfo->mpipe[instance].has_iqueue = false;\r\ninfo->mpipe[instance].instance = instance;\r\n}\r\ninfo->my_cpu = my_cpu;\r\nhrtimer_init(&info->egress_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\ninfo->egress_timer.function = tile_net_handle_egress_timer;\r\n}\r\nstatic int __init tile_net_init_module(void)\r\n{\r\nint i;\r\nchar name[GXIO_MPIPE_LINK_NAME_LEN];\r\nuint8_t mac[6];\r\npr_info("Tilera Network Driver\n");\r\nBUILD_BUG_ON(NR_MPIPE_MAX != 2);\r\nmutex_init(&tile_net_devs_for_channel_mutex);\r\non_each_cpu(tile_net_init_module_percpu, NULL, 1);\r\nfor (i = 0; gxio_mpipe_link_enumerate_mac(i, name, mac) >= 0; i++)\r\ntile_net_dev_init(name, mac);\r\nif (!network_cpus_init())\r\nnetwork_cpus_map = *cpu_online_mask;\r\nreturn 0;\r\n}
