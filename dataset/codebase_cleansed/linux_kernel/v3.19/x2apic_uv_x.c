static unsigned long __init uv_early_read_mmr(unsigned long addr)\r\n{\r\nunsigned long val, *mmr;\r\nmmr = early_ioremap(UV_LOCAL_MMR_BASE | addr, sizeof(*mmr));\r\nval = *mmr;\r\nearly_iounmap(mmr, sizeof(*mmr));\r\nreturn val;\r\n}\r\nstatic inline bool is_GRU_range(u64 start, u64 end)\r\n{\r\nif (gru_dist_base) {\r\nu64 su = start & gru_dist_umask;\r\nu64 sl = start & gru_dist_lmask;\r\nu64 eu = end & gru_dist_umask;\r\nu64 el = end & gru_dist_lmask;\r\nreturn (sl == gru_dist_base && el == gru_dist_base &&\r\nsu >= gru_first_node_paddr &&\r\nsu <= gru_last_node_paddr &&\r\neu == su);\r\n} else {\r\nreturn start >= gru_start_paddr && end <= gru_end_paddr;\r\n}\r\n}\r\nstatic bool uv_is_untracked_pat_range(u64 start, u64 end)\r\n{\r\nreturn is_ISA_range(start, end) || is_GRU_range(start, end);\r\n}\r\nstatic int __init early_get_pnodeid(void)\r\n{\r\nunion uvh_node_id_u node_id;\r\nunion uvh_rh_gam_config_mmr_u m_n_config;\r\nint pnode;\r\nnode_id.v = uv_early_read_mmr(UVH_NODE_ID);\r\nm_n_config.v = uv_early_read_mmr(UVH_RH_GAM_CONFIG_MMR);\r\nuv_min_hub_revision_id = node_id.s.revision;\r\nswitch (node_id.s.part_number) {\r\ncase UV2_HUB_PART_NUMBER:\r\ncase UV2_HUB_PART_NUMBER_X:\r\nuv_min_hub_revision_id += UV2_HUB_REVISION_BASE - 1;\r\nbreak;\r\ncase UV3_HUB_PART_NUMBER:\r\ncase UV3_HUB_PART_NUMBER_X:\r\nuv_min_hub_revision_id += UV3_HUB_REVISION_BASE;\r\nbreak;\r\n}\r\nuv_hub_info->hub_revision = uv_min_hub_revision_id;\r\npnode = (node_id.s.node_id >> 1) & ((1 << m_n_config.s.n_skt) - 1);\r\nreturn pnode;\r\n}\r\nstatic void __init early_get_apic_pnode_shift(void)\r\n{\r\nuvh_apicid.v = uv_early_read_mmr(UVH_APICID);\r\nif (!uvh_apicid.v)\r\nuvh_apicid.s.pnode_shift = UV_APIC_PNODE_SHIFT;\r\n}\r\nstatic void __init uv_set_apicid_hibit(void)\r\n{\r\nunion uv1h_lb_target_physical_apic_id_mask_u apicid_mask;\r\nif (is_uv1_hub()) {\r\napicid_mask.v =\r\nuv_early_read_mmr(UV1H_LB_TARGET_PHYSICAL_APIC_ID_MASK);\r\nuv_apicid_hibits =\r\napicid_mask.s1.bit_enables & UV_APICID_HIBIT_MASK;\r\n}\r\n}\r\nstatic int __init uv_acpi_madt_oem_check(char *oem_id, char *oem_table_id)\r\n{\r\nint pnodeid, is_uv1, is_uv2, is_uv3;\r\nis_uv1 = !strcmp(oem_id, "SGI");\r\nis_uv2 = !strcmp(oem_id, "SGI2");\r\nis_uv3 = !strncmp(oem_id, "SGI3", 4);\r\nif (is_uv1 || is_uv2 || is_uv3) {\r\nuv_hub_info->hub_revision =\r\n(is_uv1 ? UV1_HUB_REVISION_BASE :\r\n(is_uv2 ? UV2_HUB_REVISION_BASE :\r\nUV3_HUB_REVISION_BASE));\r\npnodeid = early_get_pnodeid();\r\nearly_get_apic_pnode_shift();\r\nx86_platform.is_untracked_pat_range = uv_is_untracked_pat_range;\r\nx86_platform.nmi_init = uv_nmi_init;\r\nif (!strcmp(oem_table_id, "UVL"))\r\nuv_system_type = UV_LEGACY_APIC;\r\nelse if (!strcmp(oem_table_id, "UVX"))\r\nuv_system_type = UV_X2APIC;\r\nelse if (!strcmp(oem_table_id, "UVH")) {\r\n__this_cpu_write(x2apic_extra_bits,\r\npnodeid << uvh_apicid.s.pnode_shift);\r\nuv_system_type = UV_NON_UNIQUE_APIC;\r\nuv_set_apicid_hibit();\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nenum uv_system_type get_uv_system_type(void)\r\n{\r\nreturn uv_system_type;\r\n}\r\nint is_uv_system(void)\r\n{\r\nreturn uv_system_type != UV_NONE;\r\n}\r\nstatic int uv_wakeup_secondary(int phys_apicid, unsigned long start_rip)\r\n{\r\nunsigned long val;\r\nint pnode;\r\npnode = uv_apicid_to_pnode(phys_apicid);\r\nphys_apicid |= uv_apicid_hibits;\r\nval = (1UL << UVH_IPI_INT_SEND_SHFT) |\r\n(phys_apicid << UVH_IPI_INT_APIC_ID_SHFT) |\r\n((start_rip << UVH_IPI_INT_VECTOR_SHFT) >> 12) |\r\nAPIC_DM_INIT;\r\nuv_write_global_mmr64(pnode, UVH_IPI_INT, val);\r\nval = (1UL << UVH_IPI_INT_SEND_SHFT) |\r\n(phys_apicid << UVH_IPI_INT_APIC_ID_SHFT) |\r\n((start_rip << UVH_IPI_INT_VECTOR_SHFT) >> 12) |\r\nAPIC_DM_STARTUP;\r\nuv_write_global_mmr64(pnode, UVH_IPI_INT, val);\r\natomic_set(&init_deasserted, 1);\r\nreturn 0;\r\n}\r\nstatic void uv_send_IPI_one(int cpu, int vector)\r\n{\r\nunsigned long apicid;\r\nint pnode;\r\napicid = per_cpu(x86_cpu_to_apicid, cpu);\r\npnode = uv_apicid_to_pnode(apicid);\r\nuv_hub_send_ipi(pnode, apicid, vector);\r\n}\r\nstatic void uv_send_IPI_mask(const struct cpumask *mask, int vector)\r\n{\r\nunsigned int cpu;\r\nfor_each_cpu(cpu, mask)\r\nuv_send_IPI_one(cpu, vector);\r\n}\r\nstatic void uv_send_IPI_mask_allbutself(const struct cpumask *mask, int vector)\r\n{\r\nunsigned int this_cpu = smp_processor_id();\r\nunsigned int cpu;\r\nfor_each_cpu(cpu, mask) {\r\nif (cpu != this_cpu)\r\nuv_send_IPI_one(cpu, vector);\r\n}\r\n}\r\nstatic void uv_send_IPI_allbutself(int vector)\r\n{\r\nunsigned int this_cpu = smp_processor_id();\r\nunsigned int cpu;\r\nfor_each_online_cpu(cpu) {\r\nif (cpu != this_cpu)\r\nuv_send_IPI_one(cpu, vector);\r\n}\r\n}\r\nstatic void uv_send_IPI_all(int vector)\r\n{\r\nuv_send_IPI_mask(cpu_online_mask, vector);\r\n}\r\nstatic int uv_apic_id_valid(int apicid)\r\n{\r\nreturn 1;\r\n}\r\nstatic int uv_apic_id_registered(void)\r\n{\r\nreturn 1;\r\n}\r\nstatic void uv_init_apic_ldr(void)\r\n{\r\n}\r\nstatic int\r\nuv_cpu_mask_to_apicid_and(const struct cpumask *cpumask,\r\nconst struct cpumask *andmask,\r\nunsigned int *apicid)\r\n{\r\nint unsigned cpu;\r\nfor_each_cpu_and(cpu, cpumask, andmask) {\r\nif (cpumask_test_cpu(cpu, cpu_online_mask))\r\nbreak;\r\n}\r\nif (likely(cpu < nr_cpu_ids)) {\r\n*apicid = per_cpu(x86_cpu_to_apicid, cpu) | uv_apicid_hibits;\r\nreturn 0;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic unsigned int x2apic_get_apic_id(unsigned long x)\r\n{\r\nunsigned int id;\r\nWARN_ON(preemptible() && num_online_cpus() > 1);\r\nid = x | __this_cpu_read(x2apic_extra_bits);\r\nreturn id;\r\n}\r\nstatic unsigned long set_apic_id(unsigned int id)\r\n{\r\nunsigned long x;\r\nx = id;\r\nreturn x;\r\n}\r\nstatic unsigned int uv_read_apic_id(void)\r\n{\r\nreturn x2apic_get_apic_id(apic_read(APIC_ID));\r\n}\r\nstatic int uv_phys_pkg_id(int initial_apicid, int index_msb)\r\n{\r\nreturn uv_read_apic_id() >> index_msb;\r\n}\r\nstatic void uv_send_IPI_self(int vector)\r\n{\r\napic_write(APIC_SELF_IPI, vector);\r\n}\r\nstatic int uv_probe(void)\r\n{\r\nreturn apic == &apic_x2apic_uv_x;\r\n}\r\nstatic void set_x2apic_extra_bits(int pnode)\r\n{\r\n__this_cpu_write(x2apic_extra_bits, pnode << uvh_apicid.s.pnode_shift);\r\n}\r\nstatic __init int boot_pnode_to_blade(int pnode)\r\n{\r\nint blade;\r\nfor (blade = 0; blade < uv_num_possible_blades(); blade++)\r\nif (pnode == uv_blade_info[blade].pnode)\r\nreturn blade;\r\nBUG();\r\n}\r\nstatic unsigned char get_n_lshift(int m_val)\r\n{\r\nunion uv3h_gr0_gam_gr_config_u m_gr_config;\r\nif (is_uv1_hub())\r\nreturn m_val;\r\nif (is_uv2_hub())\r\nreturn m_val == 40 ? 40 : 39;\r\nm_gr_config.v = uv_read_local_mmr(UV3H_GR0_GAM_GR_CONFIG);\r\nreturn m_gr_config.s3.m_skt;\r\n}\r\nstatic __init void get_lowmem_redirect(unsigned long *base, unsigned long *size)\r\n{\r\nunion uvh_rh_gam_alias210_overlay_config_2_mmr_u alias;\r\nunion uvh_rh_gam_alias210_redirect_config_2_mmr_u redirect;\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(redir_addrs); i++) {\r\nalias.v = uv_read_local_mmr(redir_addrs[i].alias);\r\nif (alias.s.enable && alias.s.base == 0) {\r\n*size = (1UL << alias.s.m_alias);\r\nredirect.v = uv_read_local_mmr(redir_addrs[i].redirect);\r\n*base = (unsigned long)redirect.s.dest_base << DEST_SHIFT;\r\nreturn;\r\n}\r\n}\r\n*base = *size = 0;\r\n}\r\nstatic __init void map_high(char *id, unsigned long base, int pshift,\r\nint bshift, int max_pnode, enum map_type map_type)\r\n{\r\nunsigned long bytes, paddr;\r\npaddr = base << pshift;\r\nbytes = (1UL << bshift) * (max_pnode + 1);\r\nif (!paddr) {\r\npr_info("UV: Map %s_HI base address NULL\n", id);\r\nreturn;\r\n}\r\npr_debug("UV: Map %s_HI 0x%lx - 0x%lx\n", id, paddr, paddr + bytes);\r\nif (map_type == map_uc)\r\ninit_extra_mapping_uc(paddr, bytes);\r\nelse\r\ninit_extra_mapping_wb(paddr, bytes);\r\n}\r\nstatic __init void map_gru_distributed(unsigned long c)\r\n{\r\nunion uvh_rh_gam_gru_overlay_config_mmr_u gru;\r\nu64 paddr;\r\nunsigned long bytes;\r\nint nid;\r\ngru.v = c;\r\ngru_dist_base = gru.v & 0x000007fff0000000UL;\r\nif (!gru_dist_base) {\r\npr_info("UV: Map GRU_DIST base address NULL\n");\r\nreturn;\r\n}\r\nbytes = 1UL << UVH_RH_GAM_GRU_OVERLAY_CONFIG_MMR_BASE_SHFT;\r\ngru_dist_lmask = ((1UL << uv_hub_info->m_val) - 1) & ~(bytes - 1);\r\ngru_dist_umask = ~((1UL << uv_hub_info->m_val) - 1);\r\ngru_dist_base &= gru_dist_lmask;\r\nfor_each_online_node(nid) {\r\npaddr = ((u64)uv_node_to_pnode(nid) << uv_hub_info->m_val) |\r\ngru_dist_base;\r\ninit_extra_mapping_wb(paddr, bytes);\r\ngru_first_node_paddr = min(paddr, gru_first_node_paddr);\r\ngru_last_node_paddr = max(paddr, gru_last_node_paddr);\r\n}\r\ngru_first_node_paddr &= gru_dist_umask;\r\ngru_last_node_paddr &= gru_dist_umask;\r\npr_debug("UV: Map GRU_DIST base 0x%016llx 0x%016llx - 0x%016llx\n",\r\ngru_dist_base, gru_first_node_paddr, gru_last_node_paddr);\r\n}\r\nstatic __init void map_gru_high(int max_pnode)\r\n{\r\nunion uvh_rh_gam_gru_overlay_config_mmr_u gru;\r\nint shift = UVH_RH_GAM_GRU_OVERLAY_CONFIG_MMR_BASE_SHFT;\r\ngru.v = uv_read_local_mmr(UVH_RH_GAM_GRU_OVERLAY_CONFIG_MMR);\r\nif (!gru.s.enable) {\r\npr_info("UV: GRU disabled\n");\r\nreturn;\r\n}\r\nif (is_uv3_hub() && gru.s3.mode) {\r\nmap_gru_distributed(gru.v);\r\nreturn;\r\n}\r\nmap_high("GRU", gru.s.base, shift, shift, max_pnode, map_wb);\r\ngru_start_paddr = ((u64)gru.s.base << shift);\r\ngru_end_paddr = gru_start_paddr + (1UL << shift) * (max_pnode + 1);\r\n}\r\nstatic __init void map_mmr_high(int max_pnode)\r\n{\r\nunion uvh_rh_gam_mmr_overlay_config_mmr_u mmr;\r\nint shift = UVH_RH_GAM_MMR_OVERLAY_CONFIG_MMR_BASE_SHFT;\r\nmmr.v = uv_read_local_mmr(UVH_RH_GAM_MMR_OVERLAY_CONFIG_MMR);\r\nif (mmr.s.enable)\r\nmap_high("MMR", mmr.s.base, shift, shift, max_pnode, map_uc);\r\nelse\r\npr_info("UV: MMR disabled\n");\r\n}\r\nstatic __init void map_mmioh_high_uv3(int index, int min_pnode, int max_pnode)\r\n{\r\nunion uv3h_rh_gam_mmioh_overlay_config0_mmr_u overlay;\r\nunsigned long mmr;\r\nunsigned long base;\r\nint i, n, shift, m_io, max_io;\r\nint nasid, lnasid, fi, li;\r\nchar *id;\r\nid = mmiohs[index].id;\r\noverlay.v = uv_read_local_mmr(mmiohs[index].overlay);\r\npr_info("UV: %s overlay 0x%lx base:0x%x m_io:%d\n",\r\nid, overlay.v, overlay.s3.base, overlay.s3.m_io);\r\nif (!overlay.s3.enable) {\r\npr_info("UV: %s disabled\n", id);\r\nreturn;\r\n}\r\nshift = UV3H_RH_GAM_MMIOH_OVERLAY_CONFIG0_MMR_BASE_SHFT;\r\nbase = (unsigned long)overlay.s3.base;\r\nm_io = overlay.s3.m_io;\r\nmmr = mmiohs[index].redirect;\r\nn = UV3H_RH_GAM_MMIOH_REDIRECT_CONFIG0_MMR_DEPTH;\r\nmin_pnode *= 2;\r\nmax_pnode *= 2;\r\nmax_io = lnasid = fi = li = -1;\r\nfor (i = 0; i < n; i++) {\r\nunion uv3h_rh_gam_mmioh_redirect_config0_mmr_u redirect;\r\nredirect.v = uv_read_local_mmr(mmr + i * 8);\r\nnasid = redirect.s3.nasid;\r\nif (nasid < min_pnode || max_pnode < nasid)\r\nnasid = -1;\r\nif (nasid == lnasid) {\r\nli = i;\r\nif (i != n-1)\r\ncontinue;\r\n}\r\nif (lnasid != -1 || (i == n-1 && nasid != -1)) {\r\nunsigned long addr1, addr2;\r\nint f, l;\r\nif (lnasid == -1) {\r\nf = l = i;\r\nlnasid = nasid;\r\n} else {\r\nf = fi;\r\nl = li;\r\n}\r\naddr1 = (base << shift) +\r\nf * (unsigned long)(1 << m_io);\r\naddr2 = (base << shift) +\r\n(l + 1) * (unsigned long)(1 << m_io);\r\npr_info("UV: %s[%03d..%03d] NASID 0x%04x ADDR 0x%016lx - 0x%016lx\n",\r\nid, fi, li, lnasid, addr1, addr2);\r\nif (max_io < l)\r\nmax_io = l;\r\n}\r\nfi = li = i;\r\nlnasid = nasid;\r\n}\r\npr_info("UV: %s base:0x%lx shift:%d M_IO:%d MAX_IO:%d\n",\r\nid, base, shift, m_io, max_io);\r\nif (max_io >= 0)\r\nmap_high(id, base, shift, m_io, max_io, map_uc);\r\n}\r\nstatic __init void map_mmioh_high(int min_pnode, int max_pnode)\r\n{\r\nunion uvh_rh_gam_mmioh_overlay_config_mmr_u mmioh;\r\nunsigned long mmr, base;\r\nint shift, enable, m_io, n_io;\r\nif (is_uv3_hub()) {\r\nmap_mmioh_high_uv3(0, min_pnode, max_pnode);\r\nmap_mmioh_high_uv3(1, min_pnode, max_pnode);\r\nreturn;\r\n}\r\nif (is_uv1_hub()) {\r\nmmr = UV1H_RH_GAM_MMIOH_OVERLAY_CONFIG_MMR;\r\nshift = UV1H_RH_GAM_MMIOH_OVERLAY_CONFIG_MMR_BASE_SHFT;\r\nmmioh.v = uv_read_local_mmr(mmr);\r\nenable = !!mmioh.s1.enable;\r\nbase = mmioh.s1.base;\r\nm_io = mmioh.s1.m_io;\r\nn_io = mmioh.s1.n_io;\r\n} else if (is_uv2_hub()) {\r\nmmr = UV2H_RH_GAM_MMIOH_OVERLAY_CONFIG_MMR;\r\nshift = UV2H_RH_GAM_MMIOH_OVERLAY_CONFIG_MMR_BASE_SHFT;\r\nmmioh.v = uv_read_local_mmr(mmr);\r\nenable = !!mmioh.s2.enable;\r\nbase = mmioh.s2.base;\r\nm_io = mmioh.s2.m_io;\r\nn_io = mmioh.s2.n_io;\r\n} else\r\nreturn;\r\nif (enable) {\r\nmax_pnode &= (1 << n_io) - 1;\r\npr_info(\r\n"UV: base:0x%lx shift:%d N_IO:%d M_IO:%d max_pnode:0x%x\n",\r\nbase, shift, m_io, n_io, max_pnode);\r\nmap_high("MMIOH", base, shift, m_io, max_pnode, map_uc);\r\n} else {\r\npr_info("UV: MMIOH disabled\n");\r\n}\r\n}\r\nstatic __init void map_low_mmrs(void)\r\n{\r\ninit_extra_mapping_uc(UV_GLOBAL_MMR32_BASE, UV_GLOBAL_MMR32_SIZE);\r\ninit_extra_mapping_uc(UV_LOCAL_MMR_BASE, UV_LOCAL_MMR_SIZE);\r\n}\r\nstatic __init void uv_rtc_init(void)\r\n{\r\nlong status;\r\nu64 ticks_per_sec;\r\nstatus = uv_bios_freq_base(BIOS_FREQ_BASE_REALTIME_CLOCK,\r\n&ticks_per_sec);\r\nif (status != BIOS_STATUS_SUCCESS || ticks_per_sec < 100000) {\r\nprintk(KERN_WARNING\r\n"unable to determine platform RTC clock frequency, "\r\n"guessing.\n");\r\nsn_rtc_cycles_per_second = 1000000000000UL / 30000UL;\r\n} else\r\nsn_rtc_cycles_per_second = ticks_per_sec;\r\n}\r\nstatic void uv_heartbeat(unsigned long ignored)\r\n{\r\nstruct timer_list *timer = &uv_hub_info->scir.timer;\r\nunsigned char bits = uv_hub_info->scir.state;\r\nbits ^= SCIR_CPU_HEARTBEAT;\r\nif (idle_cpu(raw_smp_processor_id()))\r\nbits &= ~SCIR_CPU_ACTIVITY;\r\nelse\r\nbits |= SCIR_CPU_ACTIVITY;\r\nuv_set_scir_bits(bits);\r\nmod_timer_pinned(timer, jiffies + SCIR_CPU_HB_INTERVAL);\r\n}\r\nstatic void uv_heartbeat_enable(int cpu)\r\n{\r\nwhile (!uv_cpu_hub_info(cpu)->scir.enabled) {\r\nstruct timer_list *timer = &uv_cpu_hub_info(cpu)->scir.timer;\r\nuv_set_cpu_scir_bits(cpu, SCIR_CPU_HEARTBEAT|SCIR_CPU_ACTIVITY);\r\nsetup_timer(timer, uv_heartbeat, cpu);\r\ntimer->expires = jiffies + SCIR_CPU_HB_INTERVAL;\r\nadd_timer_on(timer, cpu);\r\nuv_cpu_hub_info(cpu)->scir.enabled = 1;\r\ncpu = 0;\r\n}\r\n}\r\nstatic void uv_heartbeat_disable(int cpu)\r\n{\r\nif (uv_cpu_hub_info(cpu)->scir.enabled) {\r\nuv_cpu_hub_info(cpu)->scir.enabled = 0;\r\ndel_timer(&uv_cpu_hub_info(cpu)->scir.timer);\r\n}\r\nuv_set_cpu_scir_bits(cpu, 0xff);\r\n}\r\nstatic int uv_scir_cpu_notify(struct notifier_block *self, unsigned long action,\r\nvoid *hcpu)\r\n{\r\nlong cpu = (long)hcpu;\r\nswitch (action) {\r\ncase CPU_ONLINE:\r\nuv_heartbeat_enable(cpu);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\nuv_heartbeat_disable(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic __init void uv_scir_register_cpu_notifier(void)\r\n{\r\nhotcpu_notifier(uv_scir_cpu_notify, 0);\r\n}\r\nstatic __init void uv_scir_register_cpu_notifier(void)\r\n{\r\n}\r\nstatic __init int uv_init_heartbeat(void)\r\n{\r\nint cpu;\r\nif (is_uv_system())\r\nfor_each_online_cpu(cpu)\r\nuv_heartbeat_enable(cpu);\r\nreturn 0;\r\n}\r\nint uv_set_vga_state(struct pci_dev *pdev, bool decode,\r\nunsigned int command_bits, u32 flags)\r\n{\r\nint domain, bus, rc;\r\nPR_DEVEL("devfn %x decode %d cmd %x flags %d\n",\r\npdev->devfn, decode, command_bits, flags);\r\nif (!(flags & PCI_VGA_STATE_CHANGE_BRIDGE))\r\nreturn 0;\r\nif ((command_bits & PCI_COMMAND_IO) == 0)\r\nreturn 0;\r\ndomain = pci_domain_nr(pdev->bus);\r\nbus = pdev->bus->number;\r\nrc = uv_bios_set_legacy_vga_target(decode, domain, bus);\r\nPR_DEVEL("vga decode %d %x:%x, rc: %d\n", decode, domain, bus, rc);\r\nreturn rc;\r\n}\r\nvoid uv_cpu_init(void)\r\n{\r\nif (!uv_blade_info)\r\nreturn;\r\nuv_blade_info[uv_numa_blade_id()].nr_online_cpus++;\r\nif (get_uv_system_type() == UV_NON_UNIQUE_APIC)\r\nset_x2apic_extra_bits(uv_hub_info->pnode);\r\n}\r\nvoid __init uv_system_init(void)\r\n{\r\nunion uvh_rh_gam_config_mmr_u m_n_config;\r\nunion uvh_node_id_u node_id;\r\nunsigned long gnode_upper, lowmem_redir_base, lowmem_redir_size;\r\nint bytes, nid, cpu, lcpu, pnode, blade, i, j, m_val, n_val;\r\nint gnode_extra, min_pnode = 999999, max_pnode = -1;\r\nunsigned long mmr_base, present, paddr;\r\nunsigned short pnode_mask;\r\nunsigned char n_lshift;\r\nchar *hub = (is_uv1_hub() ? "UV1" :\r\n(is_uv2_hub() ? "UV2" :\r\n"UV3"));\r\npr_info("UV: Found %s hub\n", hub);\r\nmap_low_mmrs();\r\nm_n_config.v = uv_read_local_mmr(UVH_RH_GAM_CONFIG_MMR );\r\nm_val = m_n_config.s.m_skt;\r\nn_val = m_n_config.s.n_skt;\r\npnode_mask = (1 << n_val) - 1;\r\nn_lshift = get_n_lshift(m_val);\r\nmmr_base =\r\nuv_read_local_mmr(UVH_RH_GAM_MMR_OVERLAY_CONFIG_MMR) &\r\n~UV_MMR_ENABLE;\r\nnode_id.v = uv_read_local_mmr(UVH_NODE_ID);\r\ngnode_extra = (node_id.s.node_id & ~((1 << n_val) - 1)) >> 1;\r\ngnode_upper = ((unsigned long)gnode_extra << m_val);\r\npr_info("UV: N:%d M:%d pnode_mask:0x%x gnode_upper/extra:0x%lx/0x%x n_lshift 0x%x\n",\r\nn_val, m_val, pnode_mask, gnode_upper, gnode_extra,\r\nn_lshift);\r\npr_info("UV: global MMR base 0x%lx\n", mmr_base);\r\nfor(i = 0; i < UVH_NODE_PRESENT_TABLE_DEPTH; i++)\r\nuv_possible_blades +=\r\nhweight64(uv_read_local_mmr( UVH_NODE_PRESENT_TABLE + i * 8));\r\npr_info("UV: Found %d blades, %d hubs\n",\r\nis_uv1_hub() ? uv_num_possible_blades() :\r\n(uv_num_possible_blades() + 1) / 2,\r\nuv_num_possible_blades());\r\nbytes = sizeof(struct uv_blade_info) * uv_num_possible_blades();\r\nuv_blade_info = kzalloc(bytes, GFP_KERNEL);\r\nBUG_ON(!uv_blade_info);\r\nfor (blade = 0; blade < uv_num_possible_blades(); blade++)\r\nuv_blade_info[blade].memory_nid = -1;\r\nget_lowmem_redirect(&lowmem_redir_base, &lowmem_redir_size);\r\nbytes = sizeof(uv_node_to_blade[0]) * num_possible_nodes();\r\nuv_node_to_blade = kmalloc(bytes, GFP_KERNEL);\r\nBUG_ON(!uv_node_to_blade);\r\nmemset(uv_node_to_blade, 255, bytes);\r\nbytes = sizeof(uv_cpu_to_blade[0]) * num_possible_cpus();\r\nuv_cpu_to_blade = kmalloc(bytes, GFP_KERNEL);\r\nBUG_ON(!uv_cpu_to_blade);\r\nmemset(uv_cpu_to_blade, 255, bytes);\r\nblade = 0;\r\nfor (i = 0; i < UVH_NODE_PRESENT_TABLE_DEPTH; i++) {\r\npresent = uv_read_local_mmr(UVH_NODE_PRESENT_TABLE + i * 8);\r\nfor (j = 0; j < 64; j++) {\r\nif (!test_bit(j, &present))\r\ncontinue;\r\npnode = (i * 64 + j) & pnode_mask;\r\nuv_blade_info[blade].pnode = pnode;\r\nuv_blade_info[blade].nr_possible_cpus = 0;\r\nuv_blade_info[blade].nr_online_cpus = 0;\r\nspin_lock_init(&uv_blade_info[blade].nmi_lock);\r\nmin_pnode = min(pnode, min_pnode);\r\nmax_pnode = max(pnode, max_pnode);\r\nblade++;\r\n}\r\n}\r\nuv_bios_init();\r\nuv_bios_get_sn_info(0, &uv_type, &sn_partition_id, &sn_coherency_id,\r\n&sn_region_size, &system_serial_number);\r\nuv_rtc_init();\r\nfor_each_present_cpu(cpu) {\r\nint apicid = per_cpu(x86_cpu_to_apicid, cpu);\r\nnid = cpu_to_node(cpu);\r\nuv_cpu_hub_info(cpu)->pnode_mask = pnode_mask;\r\nuv_cpu_hub_info(cpu)->apic_pnode_shift = uvh_apicid.s.pnode_shift;\r\nuv_cpu_hub_info(cpu)->hub_revision = uv_hub_info->hub_revision;\r\nuv_cpu_hub_info(cpu)->m_shift = 64 - m_val;\r\nuv_cpu_hub_info(cpu)->n_lshift = n_lshift;\r\npnode = uv_apicid_to_pnode(apicid);\r\nblade = boot_pnode_to_blade(pnode);\r\nlcpu = uv_blade_info[blade].nr_possible_cpus;\r\nuv_blade_info[blade].nr_possible_cpus++;\r\nuv_blade_info[blade].memory_nid = nid;\r\nuv_cpu_hub_info(cpu)->lowmem_remap_base = lowmem_redir_base;\r\nuv_cpu_hub_info(cpu)->lowmem_remap_top = lowmem_redir_size;\r\nuv_cpu_hub_info(cpu)->m_val = m_val;\r\nuv_cpu_hub_info(cpu)->n_val = n_val;\r\nuv_cpu_hub_info(cpu)->numa_blade_id = blade;\r\nuv_cpu_hub_info(cpu)->blade_processor_id = lcpu;\r\nuv_cpu_hub_info(cpu)->pnode = pnode;\r\nuv_cpu_hub_info(cpu)->gpa_mask = (1UL << (m_val + n_val)) - 1;\r\nuv_cpu_hub_info(cpu)->gnode_upper = gnode_upper;\r\nuv_cpu_hub_info(cpu)->gnode_extra = gnode_extra;\r\nuv_cpu_hub_info(cpu)->global_mmr_base = mmr_base;\r\nuv_cpu_hub_info(cpu)->coherency_domain_number = sn_coherency_id;\r\nuv_cpu_hub_info(cpu)->scir.offset = uv_scir_offset(apicid);\r\nuv_node_to_blade[nid] = blade;\r\nuv_cpu_to_blade[cpu] = blade;\r\n}\r\nfor_each_online_node(nid) {\r\nif (uv_node_to_blade[nid] >= 0)\r\ncontinue;\r\npaddr = node_start_pfn(nid) << PAGE_SHIFT;\r\npnode = uv_gpa_to_pnode(uv_soc_phys_ram_to_gpa(paddr));\r\nblade = boot_pnode_to_blade(pnode);\r\nuv_node_to_blade[nid] = blade;\r\n}\r\nmap_gru_high(max_pnode);\r\nmap_mmr_high(max_pnode);\r\nmap_mmioh_high(min_pnode, max_pnode);\r\nuv_nmi_setup();\r\nuv_cpu_init();\r\nuv_scir_register_cpu_notifier();\r\nproc_mkdir("sgi_uv", NULL);\r\npci_register_set_vga_state(uv_set_vga_state);\r\nif (is_kdump_kernel())\r\nreboot_type = BOOT_ACPI;\r\n}
