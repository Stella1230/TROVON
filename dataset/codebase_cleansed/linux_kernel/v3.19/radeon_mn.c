static void radeon_mn_destroy(struct work_struct *work)\r\n{\r\nstruct radeon_mn *rmn = container_of(work, struct radeon_mn, work);\r\nstruct radeon_device *rdev = rmn->rdev;\r\nstruct radeon_bo *bo, *next;\r\nmutex_lock(&rdev->mn_lock);\r\nmutex_lock(&rmn->lock);\r\nhash_del(&rmn->node);\r\nrbtree_postorder_for_each_entry_safe(bo, next, &rmn->objects, mn_it.rb) {\r\ninterval_tree_remove(&bo->mn_it, &rmn->objects);\r\nbo->mn = NULL;\r\n}\r\nmutex_unlock(&rmn->lock);\r\nmutex_unlock(&rdev->mn_lock);\r\nmmu_notifier_unregister(&rmn->mn, rmn->mm);\r\nkfree(rmn);\r\n}\r\nstatic void radeon_mn_release(struct mmu_notifier *mn,\r\nstruct mm_struct *mm)\r\n{\r\nstruct radeon_mn *rmn = container_of(mn, struct radeon_mn, mn);\r\nINIT_WORK(&rmn->work, radeon_mn_destroy);\r\nschedule_work(&rmn->work);\r\n}\r\nstatic void radeon_mn_invalidate_range_start(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start,\r\nunsigned long end)\r\n{\r\nstruct radeon_mn *rmn = container_of(mn, struct radeon_mn, mn);\r\nstruct interval_tree_node *it;\r\nend -= 1;\r\nmutex_lock(&rmn->lock);\r\nit = interval_tree_iter_first(&rmn->objects, start, end);\r\nwhile (it) {\r\nstruct radeon_bo *bo;\r\nstruct fence *fence;\r\nint r;\r\nbo = container_of(it, struct radeon_bo, mn_it);\r\nit = interval_tree_iter_next(it, start, end);\r\nr = radeon_bo_reserve(bo, true);\r\nif (r) {\r\nDRM_ERROR("(%d) failed to reserve user bo\n", r);\r\ncontinue;\r\n}\r\nfence = reservation_object_get_excl(bo->tbo.resv);\r\nif (fence) {\r\nr = radeon_fence_wait((struct radeon_fence *)fence, false);\r\nif (r)\r\nDRM_ERROR("(%d) failed to wait for user bo\n", r);\r\n}\r\nradeon_ttm_placement_from_domain(bo, RADEON_GEM_DOMAIN_CPU);\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (r)\r\nDRM_ERROR("(%d) failed to validate user bo\n", r);\r\nradeon_bo_unreserve(bo);\r\n}\r\nmutex_unlock(&rmn->lock);\r\n}\r\nstatic struct radeon_mn *radeon_mn_get(struct radeon_device *rdev)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct radeon_mn *rmn;\r\nint r;\r\ndown_write(&mm->mmap_sem);\r\nmutex_lock(&rdev->mn_lock);\r\nhash_for_each_possible(rdev->mn_hash, rmn, node, (unsigned long)mm)\r\nif (rmn->mm == mm)\r\ngoto release_locks;\r\nrmn = kzalloc(sizeof(*rmn), GFP_KERNEL);\r\nif (!rmn) {\r\nrmn = ERR_PTR(-ENOMEM);\r\ngoto release_locks;\r\n}\r\nrmn->rdev = rdev;\r\nrmn->mm = mm;\r\nrmn->mn.ops = &radeon_mn_ops;\r\nmutex_init(&rmn->lock);\r\nrmn->objects = RB_ROOT;\r\nr = __mmu_notifier_register(&rmn->mn, mm);\r\nif (r)\r\ngoto free_rmn;\r\nhash_add(rdev->mn_hash, &rmn->node, (unsigned long)mm);\r\nrelease_locks:\r\nmutex_unlock(&rdev->mn_lock);\r\nup_write(&mm->mmap_sem);\r\nreturn rmn;\r\nfree_rmn:\r\nmutex_unlock(&rdev->mn_lock);\r\nup_write(&mm->mmap_sem);\r\nkfree(rmn);\r\nreturn ERR_PTR(r);\r\n}\r\nint radeon_mn_register(struct radeon_bo *bo, unsigned long addr)\r\n{\r\nunsigned long end = addr + radeon_bo_size(bo) - 1;\r\nstruct radeon_device *rdev = bo->rdev;\r\nstruct radeon_mn *rmn;\r\nstruct interval_tree_node *it;\r\nrmn = radeon_mn_get(rdev);\r\nif (IS_ERR(rmn))\r\nreturn PTR_ERR(rmn);\r\nmutex_lock(&rmn->lock);\r\nit = interval_tree_iter_first(&rmn->objects, addr, end);\r\nif (it) {\r\nmutex_unlock(&rmn->lock);\r\nreturn -EEXIST;\r\n}\r\nbo->mn = rmn;\r\nbo->mn_it.start = addr;\r\nbo->mn_it.last = end;\r\ninterval_tree_insert(&bo->mn_it, &rmn->objects);\r\nmutex_unlock(&rmn->lock);\r\nreturn 0;\r\n}\r\nvoid radeon_mn_unregister(struct radeon_bo *bo)\r\n{\r\nstruct radeon_device *rdev = bo->rdev;\r\nstruct radeon_mn *rmn;\r\nmutex_lock(&rdev->mn_lock);\r\nrmn = bo->mn;\r\nif (rmn == NULL) {\r\nmutex_unlock(&rdev->mn_lock);\r\nreturn;\r\n}\r\nmutex_lock(&rmn->lock);\r\ninterval_tree_remove(&bo->mn_it, &rmn->objects);\r\nbo->mn = NULL;\r\nmutex_unlock(&rmn->lock);\r\nmutex_unlock(&rdev->mn_lock);\r\n}
