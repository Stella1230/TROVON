void mcpm_set_entry_vector(unsigned cpu, unsigned cluster, void *ptr)\r\n{\r\nunsigned long val = ptr ? virt_to_phys(ptr) : 0;\r\nmcpm_entry_vectors[cluster][cpu] = val;\r\nsync_cache_w(&mcpm_entry_vectors[cluster][cpu]);\r\n}\r\nvoid mcpm_set_early_poke(unsigned cpu, unsigned cluster,\r\nunsigned long poke_phys_addr, unsigned long poke_val)\r\n{\r\nunsigned long *poke = &mcpm_entry_early_pokes[cluster][cpu][0];\r\npoke[0] = poke_phys_addr;\r\npoke[1] = poke_val;\r\n__sync_cache_range_w(poke, 2 * sizeof(*poke));\r\n}\r\nint __init mcpm_platform_register(const struct mcpm_platform_ops *ops)\r\n{\r\nif (platform_ops)\r\nreturn -EBUSY;\r\nplatform_ops = ops;\r\nreturn 0;\r\n}\r\nbool mcpm_is_available(void)\r\n{\r\nreturn (platform_ops) ? true : false;\r\n}\r\nint mcpm_cpu_power_up(unsigned int cpu, unsigned int cluster)\r\n{\r\nif (!platform_ops)\r\nreturn -EUNATCH;\r\nmight_sleep();\r\nreturn platform_ops->power_up(cpu, cluster);\r\n}\r\nvoid mcpm_cpu_power_down(void)\r\n{\r\nphys_reset_t phys_reset;\r\nif (WARN_ON_ONCE(!platform_ops || !platform_ops->power_down))\r\nreturn;\r\nBUG_ON(!irqs_disabled());\r\nsetup_mm_for_reboot();\r\nplatform_ops->power_down();\r\nphys_reset = (phys_reset_t)(unsigned long)virt_to_phys(cpu_reset);\r\nphys_reset(virt_to_phys(mcpm_entry_point));\r\nBUG();\r\n}\r\nint mcpm_wait_for_cpu_powerdown(unsigned int cpu, unsigned int cluster)\r\n{\r\nint ret;\r\nif (WARN_ON_ONCE(!platform_ops || !platform_ops->wait_for_powerdown))\r\nreturn -EUNATCH;\r\nret = platform_ops->wait_for_powerdown(cpu, cluster);\r\nif (ret)\r\npr_warn("%s: cpu %u, cluster %u failed to power down (%d)\n",\r\n__func__, cpu, cluster, ret);\r\nreturn ret;\r\n}\r\nvoid mcpm_cpu_suspend(u64 expected_residency)\r\n{\r\nphys_reset_t phys_reset;\r\nif (WARN_ON_ONCE(!platform_ops || !platform_ops->suspend))\r\nreturn;\r\nBUG_ON(!irqs_disabled());\r\nsetup_mm_for_reboot();\r\nplatform_ops->suspend(expected_residency);\r\nphys_reset = (phys_reset_t)(unsigned long)virt_to_phys(cpu_reset);\r\nphys_reset(virt_to_phys(mcpm_entry_point));\r\nBUG();\r\n}\r\nint mcpm_cpu_powered_up(void)\r\n{\r\nif (!platform_ops)\r\nreturn -EUNATCH;\r\nif (platform_ops->powered_up)\r\nplatform_ops->powered_up();\r\nreturn 0;\r\n}\r\nstatic int __init nocache_trampoline(unsigned long _arg)\r\n{\r\nvoid (*cache_disable)(void) = (void *)_arg;\r\nunsigned int mpidr = read_cpuid_mpidr();\r\nunsigned int cpu = MPIDR_AFFINITY_LEVEL(mpidr, 0);\r\nunsigned int cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);\r\nphys_reset_t phys_reset;\r\nmcpm_set_entry_vector(cpu, cluster, cpu_resume);\r\nsetup_mm_for_reboot();\r\n__mcpm_cpu_going_down(cpu, cluster);\r\nBUG_ON(!__mcpm_outbound_enter_critical(cpu, cluster));\r\ncache_disable();\r\n__mcpm_outbound_leave_critical(cluster, CLUSTER_DOWN);\r\n__mcpm_cpu_down(cpu, cluster);\r\nphys_reset = (phys_reset_t)(unsigned long)virt_to_phys(cpu_reset);\r\nphys_reset(virt_to_phys(mcpm_entry_point));\r\nBUG();\r\n}\r\nint __init mcpm_loopback(void (*cache_disable)(void))\r\n{\r\nint ret;\r\nlocal_irq_disable();\r\nlocal_fiq_disable();\r\nret = cpu_pm_enter();\r\nif (!ret) {\r\nret = cpu_suspend((unsigned long)cache_disable, nocache_trampoline);\r\ncpu_pm_exit();\r\n}\r\nlocal_fiq_enable();\r\nlocal_irq_enable();\r\nif (ret)\r\npr_err("%s returned %d\n", __func__, ret);\r\nreturn ret;\r\n}\r\nvoid __mcpm_cpu_going_down(unsigned int cpu, unsigned int cluster)\r\n{\r\nmcpm_sync.clusters[cluster].cpus[cpu].cpu = CPU_GOING_DOWN;\r\nsync_cache_w(&mcpm_sync.clusters[cluster].cpus[cpu].cpu);\r\n}\r\nvoid __mcpm_cpu_down(unsigned int cpu, unsigned int cluster)\r\n{\r\ndmb();\r\nmcpm_sync.clusters[cluster].cpus[cpu].cpu = CPU_DOWN;\r\nsync_cache_w(&mcpm_sync.clusters[cluster].cpus[cpu].cpu);\r\nsev();\r\n}\r\nvoid __mcpm_outbound_leave_critical(unsigned int cluster, int state)\r\n{\r\ndmb();\r\nmcpm_sync.clusters[cluster].cluster = state;\r\nsync_cache_w(&mcpm_sync.clusters[cluster].cluster);\r\nsev();\r\n}\r\nbool __mcpm_outbound_enter_critical(unsigned int cpu, unsigned int cluster)\r\n{\r\nunsigned int i;\r\nstruct mcpm_sync_struct *c = &mcpm_sync.clusters[cluster];\r\nc->cluster = CLUSTER_GOING_DOWN;\r\nsync_cache_w(&c->cluster);\r\nsync_cache_r(&c->inbound);\r\nif (c->inbound == INBOUND_COMING_UP)\r\ngoto abort;\r\nsync_cache_r(&c->cpus);\r\nfor (i = 0; i < MAX_CPUS_PER_CLUSTER; i++) {\r\nint cpustate;\r\nif (i == cpu)\r\ncontinue;\r\nwhile (1) {\r\ncpustate = c->cpus[i].cpu;\r\nif (cpustate != CPU_GOING_DOWN)\r\nbreak;\r\nwfe();\r\nsync_cache_r(&c->cpus[i].cpu);\r\n}\r\nswitch (cpustate) {\r\ncase CPU_DOWN:\r\ncontinue;\r\ndefault:\r\ngoto abort;\r\n}\r\n}\r\nreturn true;\r\nabort:\r\n__mcpm_outbound_leave_critical(cluster, CLUSTER_UP);\r\nreturn false;\r\n}\r\nint __mcpm_cluster_state(unsigned int cluster)\r\n{\r\nsync_cache_r(&mcpm_sync.clusters[cluster].cluster);\r\nreturn mcpm_sync.clusters[cluster].cluster;\r\n}\r\nint __init mcpm_sync_init(\r\nvoid (*power_up_setup)(unsigned int affinity_level))\r\n{\r\nunsigned int i, j, mpidr, this_cluster;\r\nBUILD_BUG_ON(MCPM_SYNC_CLUSTER_SIZE * MAX_NR_CLUSTERS != sizeof mcpm_sync);\r\nBUG_ON((unsigned long)&mcpm_sync & (__CACHE_WRITEBACK_GRANULE - 1));\r\nfor (i = 0; i < MAX_NR_CLUSTERS; i++) {\r\nmcpm_sync.clusters[i].cluster = CLUSTER_DOWN;\r\nmcpm_sync.clusters[i].inbound = INBOUND_NOT_COMING_UP;\r\nfor (j = 0; j < MAX_CPUS_PER_CLUSTER; j++)\r\nmcpm_sync.clusters[i].cpus[j].cpu = CPU_DOWN;\r\n}\r\nmpidr = read_cpuid_mpidr();\r\nthis_cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);\r\nfor_each_online_cpu(i)\r\nmcpm_sync.clusters[this_cluster].cpus[i].cpu = CPU_UP;\r\nmcpm_sync.clusters[this_cluster].cluster = CLUSTER_UP;\r\nsync_cache_w(&mcpm_sync);\r\nif (power_up_setup) {\r\nmcpm_power_up_setup_phys = virt_to_phys(power_up_setup);\r\nsync_cache_w(&mcpm_power_up_setup_phys);\r\n}\r\nreturn 0;\r\n}
