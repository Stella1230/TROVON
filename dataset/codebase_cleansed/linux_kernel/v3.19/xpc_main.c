static void\r\nxpc_timeout_partition_disengage(unsigned long data)\r\n{\r\nstruct xpc_partition *part = (struct xpc_partition *)data;\r\nDBUG_ON(time_is_after_jiffies(part->disengage_timeout));\r\n(void)xpc_partition_disengaged(part);\r\nDBUG_ON(part->disengage_timeout != 0);\r\nDBUG_ON(xpc_arch_ops.partition_engaged(XPC_PARTID(part)));\r\n}\r\nstatic void\r\nxpc_hb_beater(unsigned long dummy)\r\n{\r\nxpc_arch_ops.increment_heartbeat();\r\nif (time_is_before_eq_jiffies(xpc_hb_check_timeout))\r\nwake_up_interruptible(&xpc_activate_IRQ_wq);\r\nxpc_hb_timer.expires = jiffies + (xpc_hb_interval * HZ);\r\nadd_timer(&xpc_hb_timer);\r\n}\r\nstatic void\r\nxpc_start_hb_beater(void)\r\n{\r\nxpc_arch_ops.heartbeat_init();\r\ninit_timer(&xpc_hb_timer);\r\nxpc_hb_timer.function = xpc_hb_beater;\r\nxpc_hb_beater(0);\r\n}\r\nstatic void\r\nxpc_stop_hb_beater(void)\r\n{\r\ndel_timer_sync(&xpc_hb_timer);\r\nxpc_arch_ops.heartbeat_exit();\r\n}\r\nstatic void\r\nxpc_check_remote_hb(void)\r\n{\r\nstruct xpc_partition *part;\r\nshort partid;\r\nenum xp_retval ret;\r\nfor (partid = 0; partid < xp_max_npartitions; partid++) {\r\nif (xpc_exiting)\r\nbreak;\r\nif (partid == xp_partition_id)\r\ncontinue;\r\npart = &xpc_partitions[partid];\r\nif (part->act_state == XPC_P_AS_INACTIVE ||\r\npart->act_state == XPC_P_AS_DEACTIVATING) {\r\ncontinue;\r\n}\r\nret = xpc_arch_ops.get_remote_heartbeat(part);\r\nif (ret != xpSuccess)\r\nXPC_DEACTIVATE_PARTITION(part, ret);\r\n}\r\n}\r\nstatic int\r\nxpc_hb_checker(void *ignore)\r\n{\r\nint force_IRQ = 0;\r\nset_cpus_allowed_ptr(current, cpumask_of(XPC_HB_CHECK_CPU));\r\nxpc_hb_check_timeout = jiffies + (xpc_hb_check_interval * HZ);\r\nxpc_start_hb_beater();\r\nwhile (!xpc_exiting) {\r\ndev_dbg(xpc_part, "woke up with %d ticks rem; %d IRQs have "\r\n"been received\n",\r\n(int)(xpc_hb_check_timeout - jiffies),\r\nxpc_activate_IRQ_rcvd);\r\nif (time_is_before_eq_jiffies(xpc_hb_check_timeout)) {\r\nxpc_hb_check_timeout = jiffies +\r\n(xpc_hb_check_interval * HZ);\r\ndev_dbg(xpc_part, "checking remote heartbeats\n");\r\nxpc_check_remote_hb();\r\nif (is_shub())\r\nforce_IRQ = 1;\r\n}\r\nif (xpc_activate_IRQ_rcvd > 0 || force_IRQ != 0) {\r\nforce_IRQ = 0;\r\ndev_dbg(xpc_part, "processing activate IRQs "\r\n"received\n");\r\nxpc_arch_ops.process_activate_IRQ_rcvd();\r\n}\r\n(void)wait_event_interruptible(xpc_activate_IRQ_wq,\r\n(time_is_before_eq_jiffies(\r\nxpc_hb_check_timeout) ||\r\nxpc_activate_IRQ_rcvd > 0 ||\r\nxpc_exiting));\r\n}\r\nxpc_stop_hb_beater();\r\ndev_dbg(xpc_part, "heartbeat checker is exiting\n");\r\ncomplete(&xpc_hb_checker_exited);\r\nreturn 0;\r\n}\r\nstatic int\r\nxpc_initiate_discovery(void *ignore)\r\n{\r\nxpc_discovery();\r\ndev_dbg(xpc_part, "discovery thread is exiting\n");\r\ncomplete(&xpc_discovery_exited);\r\nreturn 0;\r\n}\r\nstatic void\r\nxpc_channel_mgr(struct xpc_partition *part)\r\n{\r\nwhile (part->act_state != XPC_P_AS_DEACTIVATING ||\r\natomic_read(&part->nchannels_active) > 0 ||\r\n!xpc_partition_disengaged(part)) {\r\nxpc_process_sent_chctl_flags(part);\r\natomic_dec(&part->channel_mgr_requests);\r\n(void)wait_event_interruptible(part->channel_mgr_wq,\r\n(atomic_read(&part->channel_mgr_requests) > 0 ||\r\npart->chctl.all_flags != 0 ||\r\n(part->act_state == XPC_P_AS_DEACTIVATING &&\r\natomic_read(&part->nchannels_active) == 0 &&\r\nxpc_partition_disengaged(part))));\r\natomic_set(&part->channel_mgr_requests, 1);\r\n}\r\n}\r\nvoid *\r\nxpc_kzalloc_cacheline_aligned(size_t size, gfp_t flags, void **base)\r\n{\r\n*base = kzalloc(size, flags);\r\nif (*base == NULL)\r\nreturn NULL;\r\nif ((u64)*base == L1_CACHE_ALIGN((u64)*base))\r\nreturn *base;\r\nkfree(*base);\r\n*base = kzalloc(size + L1_CACHE_BYTES, flags);\r\nif (*base == NULL)\r\nreturn NULL;\r\nreturn (void *)L1_CACHE_ALIGN((u64)*base);\r\n}\r\nstatic enum xp_retval\r\nxpc_setup_ch_structures(struct xpc_partition *part)\r\n{\r\nenum xp_retval ret;\r\nint ch_number;\r\nstruct xpc_channel *ch;\r\nshort partid = XPC_PARTID(part);\r\nDBUG_ON(part->channels != NULL);\r\npart->channels = kzalloc(sizeof(struct xpc_channel) * XPC_MAX_NCHANNELS,\r\nGFP_KERNEL);\r\nif (part->channels == NULL) {\r\ndev_err(xpc_chan, "can't get memory for channels\n");\r\nreturn xpNoMemory;\r\n}\r\npart->remote_openclose_args =\r\nxpc_kzalloc_cacheline_aligned(XPC_OPENCLOSE_ARGS_SIZE,\r\nGFP_KERNEL, &part->\r\nremote_openclose_args_base);\r\nif (part->remote_openclose_args == NULL) {\r\ndev_err(xpc_chan, "can't get memory for remote connect args\n");\r\nret = xpNoMemory;\r\ngoto out_1;\r\n}\r\npart->chctl.all_flags = 0;\r\nspin_lock_init(&part->chctl_lock);\r\natomic_set(&part->channel_mgr_requests, 1);\r\ninit_waitqueue_head(&part->channel_mgr_wq);\r\npart->nchannels = XPC_MAX_NCHANNELS;\r\natomic_set(&part->nchannels_active, 0);\r\natomic_set(&part->nchannels_engaged, 0);\r\nfor (ch_number = 0; ch_number < part->nchannels; ch_number++) {\r\nch = &part->channels[ch_number];\r\nch->partid = partid;\r\nch->number = ch_number;\r\nch->flags = XPC_C_DISCONNECTED;\r\natomic_set(&ch->kthreads_assigned, 0);\r\natomic_set(&ch->kthreads_idle, 0);\r\natomic_set(&ch->kthreads_active, 0);\r\natomic_set(&ch->references, 0);\r\natomic_set(&ch->n_to_notify, 0);\r\nspin_lock_init(&ch->lock);\r\ninit_completion(&ch->wdisconnect_wait);\r\natomic_set(&ch->n_on_msg_allocate_wq, 0);\r\ninit_waitqueue_head(&ch->msg_allocate_wq);\r\ninit_waitqueue_head(&ch->idle_wq);\r\n}\r\nret = xpc_arch_ops.setup_ch_structures(part);\r\nif (ret != xpSuccess)\r\ngoto out_2;\r\npart->setup_state = XPC_P_SS_SETUP;\r\nreturn xpSuccess;\r\nout_2:\r\nkfree(part->remote_openclose_args_base);\r\npart->remote_openclose_args = NULL;\r\nout_1:\r\nkfree(part->channels);\r\npart->channels = NULL;\r\nreturn ret;\r\n}\r\nstatic void\r\nxpc_teardown_ch_structures(struct xpc_partition *part)\r\n{\r\nDBUG_ON(atomic_read(&part->nchannels_engaged) != 0);\r\nDBUG_ON(atomic_read(&part->nchannels_active) != 0);\r\nDBUG_ON(part->setup_state != XPC_P_SS_SETUP);\r\npart->setup_state = XPC_P_SS_WTEARDOWN;\r\nwait_event(part->teardown_wq, (atomic_read(&part->references) == 0));\r\nxpc_arch_ops.teardown_ch_structures(part);\r\nkfree(part->remote_openclose_args_base);\r\npart->remote_openclose_args = NULL;\r\nkfree(part->channels);\r\npart->channels = NULL;\r\npart->setup_state = XPC_P_SS_TORNDOWN;\r\n}\r\nstatic int\r\nxpc_activating(void *__partid)\r\n{\r\nshort partid = (u64)__partid;\r\nstruct xpc_partition *part = &xpc_partitions[partid];\r\nunsigned long irq_flags;\r\nDBUG_ON(partid < 0 || partid >= xp_max_npartitions);\r\nspin_lock_irqsave(&part->act_lock, irq_flags);\r\nif (part->act_state == XPC_P_AS_DEACTIVATING) {\r\npart->act_state = XPC_P_AS_INACTIVE;\r\nspin_unlock_irqrestore(&part->act_lock, irq_flags);\r\npart->remote_rp_pa = 0;\r\nreturn 0;\r\n}\r\nDBUG_ON(part->act_state != XPC_P_AS_ACTIVATION_REQ);\r\npart->act_state = XPC_P_AS_ACTIVATING;\r\nXPC_SET_REASON(part, 0, 0);\r\nspin_unlock_irqrestore(&part->act_lock, irq_flags);\r\ndev_dbg(xpc_part, "activating partition %d\n", partid);\r\nxpc_arch_ops.allow_hb(partid);\r\nif (xpc_setup_ch_structures(part) == xpSuccess) {\r\n(void)xpc_part_ref(part);\r\nif (xpc_arch_ops.make_first_contact(part) == xpSuccess) {\r\nxpc_mark_partition_active(part);\r\nxpc_channel_mgr(part);\r\n}\r\nxpc_part_deref(part);\r\nxpc_teardown_ch_structures(part);\r\n}\r\nxpc_arch_ops.disallow_hb(partid);\r\nxpc_mark_partition_inactive(part);\r\nif (part->reason == xpReactivating) {\r\nxpc_arch_ops.request_partition_reactivation(part);\r\n}\r\nreturn 0;\r\n}\r\nvoid\r\nxpc_activate_partition(struct xpc_partition *part)\r\n{\r\nshort partid = XPC_PARTID(part);\r\nunsigned long irq_flags;\r\nstruct task_struct *kthread;\r\nspin_lock_irqsave(&part->act_lock, irq_flags);\r\nDBUG_ON(part->act_state != XPC_P_AS_INACTIVE);\r\npart->act_state = XPC_P_AS_ACTIVATION_REQ;\r\nXPC_SET_REASON(part, xpCloneKThread, __LINE__);\r\nspin_unlock_irqrestore(&part->act_lock, irq_flags);\r\nkthread = kthread_run(xpc_activating, (void *)((u64)partid), "xpc%02d",\r\npartid);\r\nif (IS_ERR(kthread)) {\r\nspin_lock_irqsave(&part->act_lock, irq_flags);\r\npart->act_state = XPC_P_AS_INACTIVE;\r\nXPC_SET_REASON(part, xpCloneKThreadFailed, __LINE__);\r\nspin_unlock_irqrestore(&part->act_lock, irq_flags);\r\n}\r\n}\r\nvoid\r\nxpc_activate_kthreads(struct xpc_channel *ch, int needed)\r\n{\r\nint idle = atomic_read(&ch->kthreads_idle);\r\nint assigned = atomic_read(&ch->kthreads_assigned);\r\nint wakeup;\r\nDBUG_ON(needed <= 0);\r\nif (idle > 0) {\r\nwakeup = (needed > idle) ? idle : needed;\r\nneeded -= wakeup;\r\ndev_dbg(xpc_chan, "wakeup %d idle kthreads, partid=%d, "\r\n"channel=%d\n", wakeup, ch->partid, ch->number);\r\nwake_up_nr(&ch->idle_wq, wakeup);\r\n}\r\nif (needed <= 0)\r\nreturn;\r\nif (needed + assigned > ch->kthreads_assigned_limit) {\r\nneeded = ch->kthreads_assigned_limit - assigned;\r\nif (needed <= 0)\r\nreturn;\r\n}\r\ndev_dbg(xpc_chan, "create %d new kthreads, partid=%d, channel=%d\n",\r\nneeded, ch->partid, ch->number);\r\nxpc_create_kthreads(ch, needed, 0);\r\n}\r\nstatic void\r\nxpc_kthread_waitmsgs(struct xpc_partition *part, struct xpc_channel *ch)\r\n{\r\nint (*n_of_deliverable_payloads) (struct xpc_channel *) =\r\nxpc_arch_ops.n_of_deliverable_payloads;\r\ndo {\r\nwhile (n_of_deliverable_payloads(ch) > 0 &&\r\n!(ch->flags & XPC_C_DISCONNECTING)) {\r\nxpc_deliver_payload(ch);\r\n}\r\nif (atomic_inc_return(&ch->kthreads_idle) >\r\nch->kthreads_idle_limit) {\r\natomic_dec(&ch->kthreads_idle);\r\nbreak;\r\n}\r\ndev_dbg(xpc_chan, "idle kthread calling "\r\n"wait_event_interruptible_exclusive()\n");\r\n(void)wait_event_interruptible_exclusive(ch->idle_wq,\r\n(n_of_deliverable_payloads(ch) > 0 ||\r\n(ch->flags & XPC_C_DISCONNECTING)));\r\natomic_dec(&ch->kthreads_idle);\r\n} while (!(ch->flags & XPC_C_DISCONNECTING));\r\n}\r\nstatic int\r\nxpc_kthread_start(void *args)\r\n{\r\nshort partid = XPC_UNPACK_ARG1(args);\r\nu16 ch_number = XPC_UNPACK_ARG2(args);\r\nstruct xpc_partition *part = &xpc_partitions[partid];\r\nstruct xpc_channel *ch;\r\nint n_needed;\r\nunsigned long irq_flags;\r\nint (*n_of_deliverable_payloads) (struct xpc_channel *) =\r\nxpc_arch_ops.n_of_deliverable_payloads;\r\ndev_dbg(xpc_chan, "kthread starting, partid=%d, channel=%d\n",\r\npartid, ch_number);\r\nch = &part->channels[ch_number];\r\nif (!(ch->flags & XPC_C_DISCONNECTING)) {\r\nspin_lock_irqsave(&ch->lock, irq_flags);\r\nif (!(ch->flags & XPC_C_CONNECTEDCALLOUT)) {\r\nch->flags |= XPC_C_CONNECTEDCALLOUT;\r\nspin_unlock_irqrestore(&ch->lock, irq_flags);\r\nxpc_connected_callout(ch);\r\nspin_lock_irqsave(&ch->lock, irq_flags);\r\nch->flags |= XPC_C_CONNECTEDCALLOUT_MADE;\r\nspin_unlock_irqrestore(&ch->lock, irq_flags);\r\nn_needed = n_of_deliverable_payloads(ch) - 1;\r\nif (n_needed > 0 && !(ch->flags & XPC_C_DISCONNECTING))\r\nxpc_activate_kthreads(ch, n_needed);\r\n} else {\r\nspin_unlock_irqrestore(&ch->lock, irq_flags);\r\n}\r\nxpc_kthread_waitmsgs(part, ch);\r\n}\r\nspin_lock_irqsave(&ch->lock, irq_flags);\r\nif ((ch->flags & XPC_C_CONNECTEDCALLOUT_MADE) &&\r\n!(ch->flags & XPC_C_DISCONNECTINGCALLOUT)) {\r\nch->flags |= XPC_C_DISCONNECTINGCALLOUT;\r\nspin_unlock_irqrestore(&ch->lock, irq_flags);\r\nxpc_disconnect_callout(ch, xpDisconnecting);\r\nspin_lock_irqsave(&ch->lock, irq_flags);\r\nch->flags |= XPC_C_DISCONNECTINGCALLOUT_MADE;\r\n}\r\nspin_unlock_irqrestore(&ch->lock, irq_flags);\r\nif (atomic_dec_return(&ch->kthreads_assigned) == 0 &&\r\natomic_dec_return(&part->nchannels_engaged) == 0) {\r\nxpc_arch_ops.indicate_partition_disengaged(part);\r\n}\r\nxpc_msgqueue_deref(ch);\r\ndev_dbg(xpc_chan, "kthread exiting, partid=%d, channel=%d\n",\r\npartid, ch_number);\r\nxpc_part_deref(part);\r\nreturn 0;\r\n}\r\nvoid\r\nxpc_create_kthreads(struct xpc_channel *ch, int needed,\r\nint ignore_disconnecting)\r\n{\r\nunsigned long irq_flags;\r\nu64 args = XPC_PACK_ARGS(ch->partid, ch->number);\r\nstruct xpc_partition *part = &xpc_partitions[ch->partid];\r\nstruct task_struct *kthread;\r\nvoid (*indicate_partition_disengaged) (struct xpc_partition *) =\r\nxpc_arch_ops.indicate_partition_disengaged;\r\nwhile (needed-- > 0) {\r\nif (ignore_disconnecting) {\r\nif (!atomic_inc_not_zero(&ch->kthreads_assigned)) {\r\nBUG_ON(!(ch->flags &\r\nXPC_C_DISCONNECTINGCALLOUT_MADE));\r\nbreak;\r\n}\r\n} else if (ch->flags & XPC_C_DISCONNECTING) {\r\nbreak;\r\n} else if (atomic_inc_return(&ch->kthreads_assigned) == 1 &&\r\natomic_inc_return(&part->nchannels_engaged) == 1) {\r\nxpc_arch_ops.indicate_partition_engaged(part);\r\n}\r\n(void)xpc_part_ref(part);\r\nxpc_msgqueue_ref(ch);\r\nkthread = kthread_run(xpc_kthread_start, (void *)args,\r\n"xpc%02dc%d", ch->partid, ch->number);\r\nif (IS_ERR(kthread)) {\r\nif (atomic_dec_return(&ch->kthreads_assigned) == 0 &&\r\natomic_dec_return(&part->nchannels_engaged) == 0) {\r\nindicate_partition_disengaged(part);\r\n}\r\nxpc_msgqueue_deref(ch);\r\nxpc_part_deref(part);\r\nif (atomic_read(&ch->kthreads_assigned) <\r\nch->kthreads_idle_limit) {\r\nspin_lock_irqsave(&ch->lock, irq_flags);\r\nXPC_DISCONNECT_CHANNEL(ch, xpLackOfResources,\r\n&irq_flags);\r\nspin_unlock_irqrestore(&ch->lock, irq_flags);\r\n}\r\nbreak;\r\n}\r\n}\r\n}\r\nvoid\r\nxpc_disconnect_wait(int ch_number)\r\n{\r\nunsigned long irq_flags;\r\nshort partid;\r\nstruct xpc_partition *part;\r\nstruct xpc_channel *ch;\r\nint wakeup_channel_mgr;\r\nfor (partid = 0; partid < xp_max_npartitions; partid++) {\r\npart = &xpc_partitions[partid];\r\nif (!xpc_part_ref(part))\r\ncontinue;\r\nch = &part->channels[ch_number];\r\nif (!(ch->flags & XPC_C_WDISCONNECT)) {\r\nxpc_part_deref(part);\r\ncontinue;\r\n}\r\nwait_for_completion(&ch->wdisconnect_wait);\r\nspin_lock_irqsave(&ch->lock, irq_flags);\r\nDBUG_ON(!(ch->flags & XPC_C_DISCONNECTED));\r\nwakeup_channel_mgr = 0;\r\nif (ch->delayed_chctl_flags) {\r\nif (part->act_state != XPC_P_AS_DEACTIVATING) {\r\nspin_lock(&part->chctl_lock);\r\npart->chctl.flags[ch->number] |=\r\nch->delayed_chctl_flags;\r\nspin_unlock(&part->chctl_lock);\r\nwakeup_channel_mgr = 1;\r\n}\r\nch->delayed_chctl_flags = 0;\r\n}\r\nch->flags &= ~XPC_C_WDISCONNECT;\r\nspin_unlock_irqrestore(&ch->lock, irq_flags);\r\nif (wakeup_channel_mgr)\r\nxpc_wakeup_channel_mgr(part);\r\nxpc_part_deref(part);\r\n}\r\n}\r\nstatic int\r\nxpc_setup_partitions(void)\r\n{\r\nshort partid;\r\nstruct xpc_partition *part;\r\nxpc_partitions = kzalloc(sizeof(struct xpc_partition) *\r\nxp_max_npartitions, GFP_KERNEL);\r\nif (xpc_partitions == NULL) {\r\ndev_err(xpc_part, "can't get memory for partition structure\n");\r\nreturn -ENOMEM;\r\n}\r\nfor (partid = 0; partid < xp_max_npartitions; partid++) {\r\npart = &xpc_partitions[partid];\r\nDBUG_ON((u64)part != L1_CACHE_ALIGN((u64)part));\r\npart->activate_IRQ_rcvd = 0;\r\nspin_lock_init(&part->act_lock);\r\npart->act_state = XPC_P_AS_INACTIVE;\r\nXPC_SET_REASON(part, 0, 0);\r\ninit_timer(&part->disengage_timer);\r\npart->disengage_timer.function =\r\nxpc_timeout_partition_disengage;\r\npart->disengage_timer.data = (unsigned long)part;\r\npart->setup_state = XPC_P_SS_UNSET;\r\ninit_waitqueue_head(&part->teardown_wq);\r\natomic_set(&part->references, 0);\r\n}\r\nreturn xpc_arch_ops.setup_partitions();\r\n}\r\nstatic void\r\nxpc_teardown_partitions(void)\r\n{\r\nxpc_arch_ops.teardown_partitions();\r\nkfree(xpc_partitions);\r\n}\r\nstatic void\r\nxpc_do_exit(enum xp_retval reason)\r\n{\r\nshort partid;\r\nint active_part_count, printed_waiting_msg = 0;\r\nstruct xpc_partition *part;\r\nunsigned long printmsg_time, disengage_timeout = 0;\r\nDBUG_ON(xpc_exiting == 1);\r\nxpc_exiting = 1;\r\nwake_up_interruptible(&xpc_activate_IRQ_wq);\r\nwait_for_completion(&xpc_discovery_exited);\r\nwait_for_completion(&xpc_hb_checker_exited);\r\n(void)msleep_interruptible(300);\r\nprintmsg_time = jiffies + (XPC_DEACTIVATE_PRINTMSG_INTERVAL * HZ);\r\nxpc_disengage_timedout = 0;\r\ndo {\r\nactive_part_count = 0;\r\nfor (partid = 0; partid < xp_max_npartitions; partid++) {\r\npart = &xpc_partitions[partid];\r\nif (xpc_partition_disengaged(part) &&\r\npart->act_state == XPC_P_AS_INACTIVE) {\r\ncontinue;\r\n}\r\nactive_part_count++;\r\nXPC_DEACTIVATE_PARTITION(part, reason);\r\nif (part->disengage_timeout > disengage_timeout)\r\ndisengage_timeout = part->disengage_timeout;\r\n}\r\nif (xpc_arch_ops.any_partition_engaged()) {\r\nif (time_is_before_jiffies(printmsg_time)) {\r\ndev_info(xpc_part, "waiting for remote "\r\n"partitions to deactivate, timeout in "\r\n"%ld seconds\n", (disengage_timeout -\r\njiffies) / HZ);\r\nprintmsg_time = jiffies +\r\n(XPC_DEACTIVATE_PRINTMSG_INTERVAL * HZ);\r\nprinted_waiting_msg = 1;\r\n}\r\n} else if (active_part_count > 0) {\r\nif (printed_waiting_msg) {\r\ndev_info(xpc_part, "waiting for local partition"\r\n" to deactivate\n");\r\nprinted_waiting_msg = 0;\r\n}\r\n} else {\r\nif (!xpc_disengage_timedout) {\r\ndev_info(xpc_part, "all partitions have "\r\n"deactivated\n");\r\n}\r\nbreak;\r\n}\r\n(void)msleep_interruptible(300);\r\n} while (1);\r\nDBUG_ON(xpc_arch_ops.any_partition_engaged());\r\nxpc_teardown_rsvd_page();\r\nif (reason == xpUnloading) {\r\n(void)unregister_die_notifier(&xpc_die_notifier);\r\n(void)unregister_reboot_notifier(&xpc_reboot_notifier);\r\n}\r\nxpc_clear_interface();\r\nif (xpc_sysctl)\r\nunregister_sysctl_table(xpc_sysctl);\r\nxpc_teardown_partitions();\r\nif (is_shub())\r\nxpc_exit_sn2();\r\nelse if (is_uv())\r\nxpc_exit_uv();\r\n}\r\nstatic int\r\nxpc_system_reboot(struct notifier_block *nb, unsigned long event, void *unused)\r\n{\r\nenum xp_retval reason;\r\nswitch (event) {\r\ncase SYS_RESTART:\r\nreason = xpSystemReboot;\r\nbreak;\r\ncase SYS_HALT:\r\nreason = xpSystemHalt;\r\nbreak;\r\ncase SYS_POWER_OFF:\r\nreason = xpSystemPoweroff;\r\nbreak;\r\ndefault:\r\nreason = xpSystemGoingDown;\r\n}\r\nxpc_do_exit(reason);\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic void\r\nxpc_die_deactivate(void)\r\n{\r\nstruct xpc_partition *part;\r\nshort partid;\r\nint any_engaged;\r\nlong keep_waiting;\r\nlong wait_to_print;\r\nif (cmpxchg(&xpc_die_disconnecting, 0, 1))\r\nreturn;\r\nxpc_exiting = 1;\r\nxpc_arch_ops.disallow_all_hbs();\r\nfor (partid = 0; partid < xp_max_npartitions; partid++) {\r\npart = &xpc_partitions[partid];\r\nif (xpc_arch_ops.partition_engaged(partid) ||\r\npart->act_state != XPC_P_AS_INACTIVE) {\r\nxpc_arch_ops.request_partition_deactivation(part);\r\nxpc_arch_ops.indicate_partition_disengaged(part);\r\n}\r\n}\r\nkeep_waiting = xpc_disengage_timelimit * 1000 * 5;\r\nwait_to_print = XPC_DEACTIVATE_PRINTMSG_INTERVAL * 1000 * 5;\r\nwhile (1) {\r\nany_engaged = xpc_arch_ops.any_partition_engaged();\r\nif (!any_engaged) {\r\ndev_info(xpc_part, "all partitions have deactivated\n");\r\nbreak;\r\n}\r\nif (!keep_waiting--) {\r\nfor (partid = 0; partid < xp_max_npartitions;\r\npartid++) {\r\nif (xpc_arch_ops.partition_engaged(partid)) {\r\ndev_info(xpc_part, "deactivate from "\r\n"remote partition %d timed "\r\n"out\n", partid);\r\n}\r\n}\r\nbreak;\r\n}\r\nif (!wait_to_print--) {\r\ndev_info(xpc_part, "waiting for remote partitions to "\r\n"deactivate, timeout in %ld seconds\n",\r\nkeep_waiting / (1000 * 5));\r\nwait_to_print = XPC_DEACTIVATE_PRINTMSG_INTERVAL *\r\n1000 * 5;\r\n}\r\nudelay(200);\r\n}\r\n}\r\nstatic int\r\nxpc_system_die(struct notifier_block *nb, unsigned long event, void *_die_args)\r\n{\r\n#ifdef CONFIG_IA64\r\nswitch (event) {\r\ncase DIE_MACHINE_RESTART:\r\ncase DIE_MACHINE_HALT:\r\nxpc_die_deactivate();\r\nbreak;\r\ncase DIE_KDEBUG_ENTER:\r\nif (!xpc_kdebug_ignore)\r\nbreak;\r\ncase DIE_MCA_MONARCH_ENTER:\r\ncase DIE_INIT_MONARCH_ENTER:\r\nxpc_arch_ops.offline_heartbeat();\r\nbreak;\r\ncase DIE_KDEBUG_LEAVE:\r\nif (!xpc_kdebug_ignore)\r\nbreak;\r\ncase DIE_MCA_MONARCH_LEAVE:\r\ncase DIE_INIT_MONARCH_LEAVE:\r\nxpc_arch_ops.online_heartbeat();\r\nbreak;\r\n}\r\n#else\r\nstruct die_args *die_args = _die_args;\r\nswitch (event) {\r\ncase DIE_TRAP:\r\nif (die_args->trapnr == X86_TRAP_DF)\r\nxpc_die_deactivate();\r\nif (((die_args->trapnr == X86_TRAP_MF) ||\r\n(die_args->trapnr == X86_TRAP_XF)) &&\r\n!user_mode_vm(die_args->regs))\r\nxpc_die_deactivate();\r\nbreak;\r\ncase DIE_INT3:\r\ncase DIE_DEBUG:\r\nbreak;\r\ncase DIE_OOPS:\r\ncase DIE_GPF:\r\ndefault:\r\nxpc_die_deactivate();\r\n}\r\n#endif\r\nreturn NOTIFY_DONE;\r\n}\r\nint __init\r\nxpc_init(void)\r\n{\r\nint ret;\r\nstruct task_struct *kthread;\r\ndev_set_name(xpc_part, "part");\r\ndev_set_name(xpc_chan, "chan");\r\nif (is_shub()) {\r\nif (xp_max_npartitions != 64) {\r\ndev_err(xpc_part, "max #of partitions not set to 64\n");\r\nret = -EINVAL;\r\n} else {\r\nret = xpc_init_sn2();\r\n}\r\n} else if (is_uv()) {\r\nret = xpc_init_uv();\r\n} else {\r\nret = -ENODEV;\r\n}\r\nif (ret != 0)\r\nreturn ret;\r\nret = xpc_setup_partitions();\r\nif (ret != 0) {\r\ndev_err(xpc_part, "can't get memory for partition structure\n");\r\ngoto out_1;\r\n}\r\nxpc_sysctl = register_sysctl_table(xpc_sys_dir);\r\nret = xpc_setup_rsvd_page();\r\nif (ret != 0) {\r\ndev_err(xpc_part, "can't setup our reserved page\n");\r\ngoto out_2;\r\n}\r\nret = register_reboot_notifier(&xpc_reboot_notifier);\r\nif (ret != 0)\r\ndev_warn(xpc_part, "can't register reboot notifier\n");\r\nret = register_die_notifier(&xpc_die_notifier);\r\nif (ret != 0)\r\ndev_warn(xpc_part, "can't register die notifier\n");\r\nkthread = kthread_run(xpc_hb_checker, NULL, XPC_HB_CHECK_THREAD_NAME);\r\nif (IS_ERR(kthread)) {\r\ndev_err(xpc_part, "failed while forking hb check thread\n");\r\nret = -EBUSY;\r\ngoto out_3;\r\n}\r\nkthread = kthread_run(xpc_initiate_discovery, NULL,\r\nXPC_DISCOVERY_THREAD_NAME);\r\nif (IS_ERR(kthread)) {\r\ndev_err(xpc_part, "failed while forking discovery thread\n");\r\ncomplete(&xpc_discovery_exited);\r\nxpc_do_exit(xpUnloading);\r\nreturn -EBUSY;\r\n}\r\nxpc_set_interface(xpc_initiate_connect, xpc_initiate_disconnect,\r\nxpc_initiate_send, xpc_initiate_send_notify,\r\nxpc_initiate_received, xpc_initiate_partid_to_nasids);\r\nreturn 0;\r\nout_3:\r\nxpc_teardown_rsvd_page();\r\n(void)unregister_die_notifier(&xpc_die_notifier);\r\n(void)unregister_reboot_notifier(&xpc_reboot_notifier);\r\nout_2:\r\nif (xpc_sysctl)\r\nunregister_sysctl_table(xpc_sysctl);\r\nxpc_teardown_partitions();\r\nout_1:\r\nif (is_shub())\r\nxpc_exit_sn2();\r\nelse if (is_uv())\r\nxpc_exit_uv();\r\nreturn ret;\r\n}\r\nvoid __exit\r\nxpc_exit(void)\r\n{\r\nxpc_do_exit(xpUnloading);\r\n}
