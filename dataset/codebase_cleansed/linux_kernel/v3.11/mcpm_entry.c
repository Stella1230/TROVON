void mcpm_set_entry_vector(unsigned cpu, unsigned cluster, void *ptr)\r\n{\r\nunsigned long val = ptr ? virt_to_phys(ptr) : 0;\r\nmcpm_entry_vectors[cluster][cpu] = val;\r\nsync_cache_w(&mcpm_entry_vectors[cluster][cpu]);\r\n}\r\nint __init mcpm_platform_register(const struct mcpm_platform_ops *ops)\r\n{\r\nif (platform_ops)\r\nreturn -EBUSY;\r\nplatform_ops = ops;\r\nreturn 0;\r\n}\r\nint mcpm_cpu_power_up(unsigned int cpu, unsigned int cluster)\r\n{\r\nif (!platform_ops)\r\nreturn -EUNATCH;\r\nmight_sleep();\r\nreturn platform_ops->power_up(cpu, cluster);\r\n}\r\nvoid mcpm_cpu_power_down(void)\r\n{\r\nphys_reset_t phys_reset;\r\nBUG_ON(!platform_ops);\r\nBUG_ON(!irqs_disabled());\r\nsetup_mm_for_reboot();\r\nplatform_ops->power_down();\r\nphys_reset = (phys_reset_t)(unsigned long)virt_to_phys(cpu_reset);\r\nphys_reset(virt_to_phys(mcpm_entry_point));\r\nBUG();\r\n}\r\nvoid mcpm_cpu_suspend(u64 expected_residency)\r\n{\r\nphys_reset_t phys_reset;\r\nBUG_ON(!platform_ops);\r\nBUG_ON(!irqs_disabled());\r\nsetup_mm_for_reboot();\r\nplatform_ops->suspend(expected_residency);\r\nphys_reset = (phys_reset_t)(unsigned long)virt_to_phys(cpu_reset);\r\nphys_reset(virt_to_phys(mcpm_entry_point));\r\nBUG();\r\n}\r\nint mcpm_cpu_powered_up(void)\r\n{\r\nif (!platform_ops)\r\nreturn -EUNATCH;\r\nif (platform_ops->powered_up)\r\nplatform_ops->powered_up();\r\nreturn 0;\r\n}\r\nvoid __mcpm_cpu_going_down(unsigned int cpu, unsigned int cluster)\r\n{\r\nmcpm_sync.clusters[cluster].cpus[cpu].cpu = CPU_GOING_DOWN;\r\nsync_cache_w(&mcpm_sync.clusters[cluster].cpus[cpu].cpu);\r\n}\r\nvoid __mcpm_cpu_down(unsigned int cpu, unsigned int cluster)\r\n{\r\ndmb();\r\nmcpm_sync.clusters[cluster].cpus[cpu].cpu = CPU_DOWN;\r\nsync_cache_w(&mcpm_sync.clusters[cluster].cpus[cpu].cpu);\r\ndsb_sev();\r\n}\r\nvoid __mcpm_outbound_leave_critical(unsigned int cluster, int state)\r\n{\r\ndmb();\r\nmcpm_sync.clusters[cluster].cluster = state;\r\nsync_cache_w(&mcpm_sync.clusters[cluster].cluster);\r\ndsb_sev();\r\n}\r\nbool __mcpm_outbound_enter_critical(unsigned int cpu, unsigned int cluster)\r\n{\r\nunsigned int i;\r\nstruct mcpm_sync_struct *c = &mcpm_sync.clusters[cluster];\r\nc->cluster = CLUSTER_GOING_DOWN;\r\nsync_cache_w(&c->cluster);\r\nsync_cache_r(&c->inbound);\r\nif (c->inbound == INBOUND_COMING_UP)\r\ngoto abort;\r\nsync_cache_r(&c->cpus);\r\nfor (i = 0; i < MAX_CPUS_PER_CLUSTER; i++) {\r\nint cpustate;\r\nif (i == cpu)\r\ncontinue;\r\nwhile (1) {\r\ncpustate = c->cpus[i].cpu;\r\nif (cpustate != CPU_GOING_DOWN)\r\nbreak;\r\nwfe();\r\nsync_cache_r(&c->cpus[i].cpu);\r\n}\r\nswitch (cpustate) {\r\ncase CPU_DOWN:\r\ncontinue;\r\ndefault:\r\ngoto abort;\r\n}\r\n}\r\nreturn true;\r\nabort:\r\n__mcpm_outbound_leave_critical(cluster, CLUSTER_UP);\r\nreturn false;\r\n}\r\nint __mcpm_cluster_state(unsigned int cluster)\r\n{\r\nsync_cache_r(&mcpm_sync.clusters[cluster].cluster);\r\nreturn mcpm_sync.clusters[cluster].cluster;\r\n}\r\nint __init mcpm_sync_init(\r\nvoid (*power_up_setup)(unsigned int affinity_level))\r\n{\r\nunsigned int i, j, mpidr, this_cluster;\r\nBUILD_BUG_ON(MCPM_SYNC_CLUSTER_SIZE * MAX_NR_CLUSTERS != sizeof mcpm_sync);\r\nBUG_ON((unsigned long)&mcpm_sync & (__CACHE_WRITEBACK_GRANULE - 1));\r\nfor (i = 0; i < MAX_NR_CLUSTERS; i++) {\r\nmcpm_sync.clusters[i].cluster = CLUSTER_DOWN;\r\nmcpm_sync.clusters[i].inbound = INBOUND_NOT_COMING_UP;\r\nfor (j = 0; j < MAX_CPUS_PER_CLUSTER; j++)\r\nmcpm_sync.clusters[i].cpus[j].cpu = CPU_DOWN;\r\n}\r\nmpidr = read_cpuid_mpidr();\r\nthis_cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);\r\nfor_each_online_cpu(i)\r\nmcpm_sync.clusters[this_cluster].cpus[i].cpu = CPU_UP;\r\nmcpm_sync.clusters[this_cluster].cluster = CLUSTER_UP;\r\nsync_cache_w(&mcpm_sync);\r\nif (power_up_setup) {\r\nmcpm_power_up_setup_phys = virt_to_phys(power_up_setup);\r\nsync_cache_w(&mcpm_power_up_setup_phys);\r\n}\r\nreturn 0;\r\n}
