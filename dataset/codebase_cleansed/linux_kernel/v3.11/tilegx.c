static bool network_cpus_init(void)\r\n{\r\nchar buf[1024];\r\nint rc;\r\nif (network_cpus_string == NULL)\r\nreturn false;\r\nrc = cpulist_parse_crop(network_cpus_string, &network_cpus_map);\r\nif (rc != 0) {\r\npr_warn("tile_net.cpus=%s: malformed cpu list\n",\r\nnetwork_cpus_string);\r\nreturn false;\r\n}\r\ncpumask_and(&network_cpus_map, &network_cpus_map, cpu_possible_mask);\r\nif (cpumask_empty(&network_cpus_map)) {\r\npr_warn("Ignoring empty tile_net.cpus='%s'.\n",\r\nnetwork_cpus_string);\r\nreturn false;\r\n}\r\ncpulist_scnprintf(buf, sizeof(buf), &network_cpus_map);\r\npr_info("Linux network CPUs: %s\n", buf);\r\nreturn true;\r\n}\r\nstatic void tile_net_stats_add(unsigned long value, unsigned long *field)\r\n{\r\nBUILD_BUG_ON(sizeof(atomic_long_t) != sizeof(unsigned long));\r\natomic_long_add(value, (atomic_long_t *)field);\r\n}\r\nstatic bool tile_net_provide_buffer(bool small)\r\n{\r\nint stack = small ? small_buffer_stack : large_buffer_stack;\r\nconst unsigned long buffer_alignment = 128;\r\nstruct sk_buff *skb;\r\nint len;\r\nlen = sizeof(struct sk_buff **) + buffer_alignment;\r\nlen += (small ? BUFFER_SIZE_SMALL : BUFFER_SIZE_LARGE);\r\nskb = dev_alloc_skb(len);\r\nif (skb == NULL)\r\nreturn false;\r\nskb_reserve(skb, sizeof(struct sk_buff **));\r\nskb_reserve(skb, -(long)skb->data & (buffer_alignment - 1));\r\n*(struct sk_buff **)(skb->data - sizeof(struct sk_buff **)) = skb;\r\nwmb();\r\ngxio_mpipe_push_buffer(&context, stack,\r\n(void *)va_to_tile_io_addr(skb->data));\r\nreturn true;\r\n}\r\nstatic struct sk_buff *mpipe_buf_to_skb(void *va)\r\n{\r\nstruct sk_buff **skb_ptr = va - sizeof(*skb_ptr);\r\nstruct sk_buff *skb = *skb_ptr;\r\nif (skb->data != va) {\r\npanic("Corrupt linux buffer! va=%p, skb=%p, skb->data=%p",\r\nva, skb, skb->data);\r\n}\r\nreturn skb;\r\n}\r\nstatic void tile_net_pop_all_buffers(int stack)\r\n{\r\nfor (;;) {\r\ntile_io_addr_t addr =\r\n(tile_io_addr_t)gxio_mpipe_pop_buffer(&context, stack);\r\nif (addr == 0)\r\nbreak;\r\ndev_kfree_skb_irq(mpipe_buf_to_skb(tile_io_addr_to_va(addr)));\r\n}\r\n}\r\nstatic void tile_net_provide_needed_buffers(void)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nwhile (info->num_needed_small_buffers != 0) {\r\nif (!tile_net_provide_buffer(true))\r\ngoto oops;\r\ninfo->num_needed_small_buffers--;\r\n}\r\nwhile (info->num_needed_large_buffers != 0) {\r\nif (!tile_net_provide_buffer(false))\r\ngoto oops;\r\ninfo->num_needed_large_buffers--;\r\n}\r\nreturn;\r\noops:\r\npr_notice("Tile %d still needs some buffers\n", info->my_cpu);\r\n}\r\nstatic inline bool filter_packet(struct net_device *dev, void *buf)\r\n{\r\nif (dev == NULL || !(dev->flags & IFF_UP))\r\nreturn true;\r\nif (!(dev->flags & IFF_PROMISC) &&\r\n!is_multicast_ether_addr(buf) &&\r\ncompare_ether_addr(dev->dev_addr, buf) != 0)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void tile_net_receive_skb(struct net_device *dev, struct sk_buff *skb,\r\ngxio_mpipe_idesc_t *idesc, unsigned long len)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nskb_put(skb, len);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nif (idesc->cs && idesc->csum_seed_val == 0xFFFF)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nnetif_receive_skb(skb);\r\ntile_net_stats_add(1, &priv->stats.rx_packets);\r\ntile_net_stats_add(len, &priv->stats.rx_bytes);\r\nif (idesc->size == BUFFER_SIZE_SMALL_ENUM)\r\ninfo->num_needed_small_buffers++;\r\nelse\r\ninfo->num_needed_large_buffers++;\r\n}\r\nstatic bool tile_net_handle_packet(gxio_mpipe_idesc_t *idesc)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nstruct net_device *dev = tile_net_devs_for_channel[idesc->channel];\r\nuint8_t l2_offset;\r\nvoid *va;\r\nvoid *buf;\r\nunsigned long len;\r\nbool filter;\r\nif (idesc->be) {\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\ntile_net_stats_add(1, &priv->stats.rx_dropped);\r\ngxio_mpipe_iqueue_consume(&info->iqueue, idesc);\r\nif (net_ratelimit())\r\npr_info("Dropping packet (insufficient buffers).\n");\r\nreturn false;\r\n}\r\nl2_offset = custom_str ? 0 : gxio_mpipe_idesc_get_l2_offset(idesc);\r\nva = tile_io_addr_to_va((unsigned long)(long)idesc->va);\r\nbuf = va + l2_offset;\r\nlen = idesc->l2_size - l2_offset;\r\nva -= NET_IP_ALIGN;\r\nfilter = filter_packet(dev, buf);\r\nif (filter) {\r\ngxio_mpipe_iqueue_drop(&info->iqueue, idesc);\r\n} else {\r\nstruct sk_buff *skb = mpipe_buf_to_skb(va);\r\nskb_reserve(skb, NET_IP_ALIGN + l2_offset);\r\ntile_net_receive_skb(dev, skb, idesc, len);\r\n}\r\ngxio_mpipe_iqueue_consume(&info->iqueue, idesc);\r\nreturn !filter;\r\n}\r\nstatic int tile_net_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nunsigned int work = 0;\r\ngxio_mpipe_idesc_t *idesc;\r\nint i, n;\r\nwhile ((n = gxio_mpipe_iqueue_try_peek(&info->iqueue, &idesc)) > 0) {\r\nfor (i = 0; i < n; i++) {\r\nif (i == TILE_NET_BATCH)\r\ngoto done;\r\nif (tile_net_handle_packet(idesc + i)) {\r\nif (++work >= budget)\r\ngoto done;\r\n}\r\n}\r\n}\r\nnapi_complete(&info->napi);\r\ngxio_mpipe_enable_notif_ring_interrupt(&context, info->iqueue.ring);\r\nif (gxio_mpipe_iqueue_try_peek(&info->iqueue, &idesc) > 0)\r\nnapi_schedule(&info->napi);\r\ndone:\r\ntile_net_provide_needed_buffers();\r\nreturn work;\r\n}\r\nstatic irqreturn_t tile_net_handle_ingress_irq(int irq, void *unused)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nnapi_schedule(&info->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int tile_net_free_comps(gxio_mpipe_equeue_t *equeue,\r\nstruct tile_net_comps *comps,\r\nint limit, bool force_update)\r\n{\r\nint n = 0;\r\nwhile (comps->comp_last < comps->comp_next) {\r\nunsigned int cid = comps->comp_last % TILE_NET_MAX_COMPS;\r\nstruct tile_net_comp *comp = &comps->comp_queue[cid];\r\nif (!gxio_mpipe_equeue_is_complete(equeue, comp->when,\r\nforce_update || n == 0))\r\nbreak;\r\ndev_kfree_skb_irq(comp->skb);\r\ncomps->comp_last++;\r\nif (++n == limit)\r\nbreak;\r\n}\r\nreturn n;\r\n}\r\nstatic void add_comp(gxio_mpipe_equeue_t *equeue,\r\nstruct tile_net_comps *comps,\r\nuint64_t when, struct sk_buff *skb)\r\n{\r\nint cid = comps->comp_next % TILE_NET_MAX_COMPS;\r\ncomps->comp_queue[cid].when = when;\r\ncomps->comp_queue[cid].skb = skb;\r\ncomps->comp_next++;\r\n}\r\nstatic void tile_net_schedule_tx_wake_timer(struct net_device *dev,\r\nint tx_queue_idx)\r\n{\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, tx_queue_idx);\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nstruct tile_net_tx_wake *tx_wake = &info->tx_wake[priv->echannel];\r\nhrtimer_start(&tx_wake->timer,\r\nktime_set(0, TX_TIMER_DELAY_USEC * 1000UL),\r\nHRTIMER_MODE_REL_PINNED);\r\n}\r\nstatic enum hrtimer_restart tile_net_handle_tx_wake_timer(struct hrtimer *t)\r\n{\r\nstruct tile_net_tx_wake *tx_wake =\r\ncontainer_of(t, struct tile_net_tx_wake, timer);\r\nnetif_wake_subqueue(tx_wake->dev, tx_wake->tx_queue_idx);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nstatic void tile_net_schedule_egress_timer(void)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nif (!info->egress_timer_scheduled) {\r\nhrtimer_start(&info->egress_timer,\r\nktime_set(0, EGRESS_TIMER_DELAY_USEC * 1000UL),\r\nHRTIMER_MODE_REL_PINNED);\r\ninfo->egress_timer_scheduled = true;\r\n}\r\n}\r\nstatic enum hrtimer_restart tile_net_handle_egress_timer(struct hrtimer *t)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nunsigned long irqflags;\r\nbool pending = false;\r\nint i;\r\nlocal_irq_save(irqflags);\r\ninfo->egress_timer_scheduled = false;\r\nfor (i = 0; i < TILE_NET_CHANNELS; i++) {\r\nstruct tile_net_egress *egress = &egress_for_echannel[i];\r\nstruct tile_net_comps *comps = info->comps_for_echannel[i];\r\nif (comps->comp_last >= comps->comp_next)\r\ncontinue;\r\ntile_net_free_comps(egress->equeue, comps, -1, true);\r\npending = pending || (comps->comp_last < comps->comp_next);\r\n}\r\nif (pending)\r\ntile_net_schedule_egress_timer();\r\nlocal_irq_restore(irqflags);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nstatic void tile_net_update_cpu(void *arg)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nstruct net_device *dev = arg;\r\nif (!info->has_iqueue)\r\nreturn;\r\nif (dev != NULL) {\r\nif (!info->napi_added) {\r\nnetif_napi_add(dev, &info->napi,\r\ntile_net_poll, TILE_NET_WEIGHT);\r\ninfo->napi_added = true;\r\n}\r\nif (!info->napi_enabled) {\r\nnapi_enable(&info->napi);\r\ninfo->napi_enabled = true;\r\n}\r\nenable_percpu_irq(ingress_irq, 0);\r\n} else {\r\ndisable_percpu_irq(ingress_irq);\r\nif (info->napi_enabled) {\r\nnapi_disable(&info->napi);\r\ninfo->napi_enabled = false;\r\n}\r\n}\r\n}\r\nstatic int tile_net_update(struct net_device *dev)\r\n{\r\nstatic gxio_mpipe_rules_t rules;\r\nbool saw_channel = false;\r\nint channel;\r\nint rc;\r\nint cpu;\r\ngxio_mpipe_rules_init(&rules, &context);\r\nfor (channel = 0; channel < TILE_NET_CHANNELS; channel++) {\r\nif (tile_net_devs_for_channel[channel] == NULL)\r\ncontinue;\r\nif (!saw_channel) {\r\nsaw_channel = true;\r\ngxio_mpipe_rules_begin(&rules, first_bucket,\r\nnum_buckets, NULL);\r\ngxio_mpipe_rules_set_headroom(&rules, NET_IP_ALIGN);\r\n}\r\ngxio_mpipe_rules_add_channel(&rules, channel);\r\n}\r\nrc = gxio_mpipe_rules_commit(&rules);\r\nif (rc != 0) {\r\nnetdev_warn(dev, "gxio_mpipe_rules_commit failed: %d\n", rc);\r\nreturn -EIO;\r\n}\r\nfor_each_online_cpu(cpu)\r\nsmp_call_function_single(cpu, tile_net_update_cpu,\r\n(saw_channel ? dev : NULL), 1);\r\nif (saw_channel)\r\nsim_enable_mpipe_links(0, -1);\r\nreturn 0;\r\n}\r\nstatic int init_buffer_stacks(struct net_device *dev, int num_buffers)\r\n{\r\npte_t hash_pte = pte_set_home((pte_t) { 0 }, PAGE_HOME_HASH);\r\nint rc;\r\nbuffer_stack_size =\r\nALIGN(gxio_mpipe_calc_buffer_stack_bytes(num_buffers),\r\n64 * 1024);\r\nrc = gxio_mpipe_alloc_buffer_stacks(&context, 2, 0, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev, "gxio_mpipe_alloc_buffer_stacks failed: %d\n",\r\nrc);\r\nreturn rc;\r\n}\r\nsmall_buffer_stack = rc;\r\nlarge_buffer_stack = rc + 1;\r\nsmall_buffer_stack_va =\r\nalloc_pages_exact(buffer_stack_size, GFP_KERNEL);\r\nif (small_buffer_stack_va == NULL) {\r\nnetdev_err(dev,\r\n"Could not alloc %zd bytes for buffer stacks\n",\r\nbuffer_stack_size);\r\nreturn -ENOMEM;\r\n}\r\nrc = gxio_mpipe_init_buffer_stack(&context, small_buffer_stack,\r\nBUFFER_SIZE_SMALL_ENUM,\r\nsmall_buffer_stack_va,\r\nbuffer_stack_size, 0);\r\nif (rc != 0) {\r\nnetdev_err(dev, "gxio_mpipe_init_buffer_stack: %d\n", rc);\r\nreturn rc;\r\n}\r\nrc = gxio_mpipe_register_client_memory(&context, small_buffer_stack,\r\nhash_pte, 0);\r\nif (rc != 0) {\r\nnetdev_err(dev,\r\n"gxio_mpipe_register_buffer_memory failed: %d\n",\r\nrc);\r\nreturn rc;\r\n}\r\nlarge_buffer_stack_va =\r\nalloc_pages_exact(buffer_stack_size, GFP_KERNEL);\r\nif (large_buffer_stack_va == NULL) {\r\nnetdev_err(dev,\r\n"Could not alloc %zd bytes for buffer stacks\n",\r\nbuffer_stack_size);\r\nreturn -ENOMEM;\r\n}\r\nrc = gxio_mpipe_init_buffer_stack(&context, large_buffer_stack,\r\nBUFFER_SIZE_LARGE_ENUM,\r\nlarge_buffer_stack_va,\r\nbuffer_stack_size, 0);\r\nif (rc != 0) {\r\nnetdev_err(dev, "gxio_mpipe_init_buffer_stack failed: %d\n",\r\nrc);\r\nreturn rc;\r\n}\r\nrc = gxio_mpipe_register_client_memory(&context, large_buffer_stack,\r\nhash_pte, 0);\r\nif (rc != 0) {\r\nnetdev_err(dev,\r\n"gxio_mpipe_register_buffer_memory failed: %d\n",\r\nrc);\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nstatic int alloc_percpu_mpipe_resources(struct net_device *dev,\r\nint cpu, int ring)\r\n{\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nint order, i, rc;\r\nstruct page *page;\r\nvoid *addr;\r\norder = get_order(COMPS_SIZE);\r\npage = homecache_alloc_pages(GFP_KERNEL, order, cpu);\r\nif (page == NULL) {\r\nnetdev_err(dev, "Failed to alloc %zd bytes comps memory\n",\r\nCOMPS_SIZE);\r\nreturn -ENOMEM;\r\n}\r\naddr = pfn_to_kaddr(page_to_pfn(page));\r\nmemset(addr, 0, COMPS_SIZE);\r\nfor (i = 0; i < TILE_NET_CHANNELS; i++)\r\ninfo->comps_for_echannel[i] =\r\naddr + i * sizeof(struct tile_net_comps);\r\nif (cpu_isset(cpu, network_cpus_map)) {\r\norder = get_order(NOTIF_RING_SIZE);\r\npage = homecache_alloc_pages(GFP_KERNEL, order, cpu);\r\nif (page == NULL) {\r\nnetdev_err(dev,\r\n"Failed to alloc %zd bytes iqueue memory\n",\r\nNOTIF_RING_SIZE);\r\nreturn -ENOMEM;\r\n}\r\naddr = pfn_to_kaddr(page_to_pfn(page));\r\nrc = gxio_mpipe_iqueue_init(&info->iqueue, &context, ring++,\r\naddr, NOTIF_RING_SIZE, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev,\r\n"gxio_mpipe_iqueue_init failed: %d\n", rc);\r\nreturn rc;\r\n}\r\ninfo->has_iqueue = true;\r\n}\r\nreturn ring;\r\n}\r\nstatic int init_notif_group_and_buckets(struct net_device *dev,\r\nint ring, int network_cpus_count)\r\n{\r\nint group, rc;\r\nrc = gxio_mpipe_alloc_notif_groups(&context, 1, 0, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev, "gxio_mpipe_alloc_notif_groups failed: %d\n",\r\nrc);\r\nreturn rc;\r\n}\r\ngroup = rc;\r\nif (network_cpus_count > 4)\r\nnum_buckets = 256;\r\nelse if (network_cpus_count > 1)\r\nnum_buckets = 16;\r\nrc = gxio_mpipe_alloc_buckets(&context, num_buckets, 0, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev, "gxio_mpipe_alloc_buckets failed: %d\n", rc);\r\nreturn rc;\r\n}\r\nfirst_bucket = rc;\r\nrc = gxio_mpipe_init_notif_group_and_buckets(\r\n&context, group, ring, network_cpus_count,\r\nfirst_bucket, num_buckets,\r\nGXIO_MPIPE_BUCKET_STICKY_FLOW_LOCALITY);\r\nif (rc != 0) {\r\nnetdev_err(\r\ndev,\r\n"gxio_mpipe_init_notif_group_and_buckets failed: %d\n",\r\nrc);\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nstatic int tile_net_setup_interrupts(struct net_device *dev)\r\n{\r\nint cpu, rc;\r\nrc = create_irq();\r\nif (rc < 0) {\r\nnetdev_err(dev, "create_irq failed: %d\n", rc);\r\nreturn rc;\r\n}\r\ningress_irq = rc;\r\ntile_irq_activate(ingress_irq, TILE_IRQ_PERCPU);\r\nrc = request_irq(ingress_irq, tile_net_handle_ingress_irq,\r\n0, "tile_net", NULL);\r\nif (rc != 0) {\r\nnetdev_err(dev, "request_irq failed: %d\n", rc);\r\ndestroy_irq(ingress_irq);\r\ningress_irq = -1;\r\nreturn rc;\r\n}\r\nfor_each_online_cpu(cpu) {\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nif (info->has_iqueue) {\r\ngxio_mpipe_request_notif_ring_interrupt(\r\n&context, cpu_x(cpu), cpu_y(cpu),\r\nKERNEL_PL, ingress_irq, info->iqueue.ring);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void tile_net_init_mpipe_fail(void)\r\n{\r\nint cpu;\r\nif (small_buffer_stack >= 0)\r\ntile_net_pop_all_buffers(small_buffer_stack);\r\nif (large_buffer_stack >= 0)\r\ntile_net_pop_all_buffers(large_buffer_stack);\r\ngxio_mpipe_destroy(&context);\r\nfor_each_online_cpu(cpu) {\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nfree_pages((unsigned long)(info->comps_for_echannel[0]),\r\nget_order(COMPS_SIZE));\r\ninfo->comps_for_echannel[0] = NULL;\r\nfree_pages((unsigned long)(info->iqueue.idescs),\r\nget_order(NOTIF_RING_SIZE));\r\ninfo->iqueue.idescs = NULL;\r\n}\r\nif (small_buffer_stack_va)\r\nfree_pages_exact(small_buffer_stack_va, buffer_stack_size);\r\nif (large_buffer_stack_va)\r\nfree_pages_exact(large_buffer_stack_va, buffer_stack_size);\r\nsmall_buffer_stack_va = NULL;\r\nlarge_buffer_stack_va = NULL;\r\nlarge_buffer_stack = -1;\r\nsmall_buffer_stack = -1;\r\nfirst_bucket = -1;\r\n}\r\nstatic int tile_net_init_mpipe(struct net_device *dev)\r\n{\r\nint i, num_buffers, rc;\r\nint cpu;\r\nint first_ring, ring;\r\nint network_cpus_count = cpus_weight(network_cpus_map);\r\nif (!hash_default) {\r\nnetdev_err(dev, "Networking requires hash_default!\n");\r\nreturn -EIO;\r\n}\r\nrc = gxio_mpipe_init(&context, 0);\r\nif (rc != 0) {\r\nnetdev_err(dev, "gxio_mpipe_init failed: %d\n", rc);\r\nreturn -EIO;\r\n}\r\nnum_buffers =\r\nnetwork_cpus_count * (IQUEUE_ENTRIES + TILE_NET_BATCH);\r\nrc = init_buffer_stacks(dev, num_buffers);\r\nif (rc != 0)\r\ngoto fail;\r\nrc = -ENOMEM;\r\nfor (i = 0; i < num_buffers; i++) {\r\nif (!tile_net_provide_buffer(true)) {\r\nnetdev_err(dev, "Cannot allocate initial sk_bufs!\n");\r\ngoto fail;\r\n}\r\n}\r\nfor (i = 0; i < num_buffers; i++) {\r\nif (!tile_net_provide_buffer(false)) {\r\nnetdev_err(dev, "Cannot allocate initial sk_bufs!\n");\r\ngoto fail;\r\n}\r\n}\r\nrc = gxio_mpipe_alloc_notif_rings(&context, network_cpus_count, 0, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev, "gxio_mpipe_alloc_notif_rings failed %d\n",\r\nrc);\r\ngoto fail;\r\n}\r\nfirst_ring = rc;\r\nring = first_ring;\r\nfor_each_online_cpu(cpu) {\r\nrc = alloc_percpu_mpipe_resources(dev, cpu, ring);\r\nif (rc < 0)\r\ngoto fail;\r\nring = rc;\r\n}\r\nrc = init_notif_group_and_buckets(dev, first_ring, network_cpus_count);\r\nif (rc != 0)\r\ngoto fail;\r\nrc = tile_net_setup_interrupts(dev);\r\nif (rc != 0)\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\ntile_net_init_mpipe_fail();\r\nreturn rc;\r\n}\r\nstatic int tile_net_init_egress(struct net_device *dev, int echannel)\r\n{\r\nstruct page *headers_page, *edescs_page, *equeue_page;\r\ngxio_mpipe_edesc_t *edescs;\r\ngxio_mpipe_equeue_t *equeue;\r\nunsigned char *headers;\r\nint headers_order, edescs_order, equeue_order;\r\nsize_t edescs_size;\r\nint edma;\r\nint rc = -ENOMEM;\r\nif (egress_for_echannel[echannel].equeue != NULL)\r\nreturn 0;\r\nheaders_order = get_order(EQUEUE_ENTRIES * HEADER_BYTES);\r\nheaders_page = alloc_pages(GFP_KERNEL, headers_order);\r\nif (headers_page == NULL) {\r\nnetdev_warn(dev,\r\n"Could not alloc %zd bytes for TSO headers.\n",\r\nPAGE_SIZE << headers_order);\r\ngoto fail;\r\n}\r\nheaders = pfn_to_kaddr(page_to_pfn(headers_page));\r\nedescs_size = EQUEUE_ENTRIES * sizeof(*edescs);\r\nedescs_order = get_order(edescs_size);\r\nedescs_page = alloc_pages(GFP_KERNEL, edescs_order);\r\nif (edescs_page == NULL) {\r\nnetdev_warn(dev,\r\n"Could not alloc %zd bytes for eDMA ring.\n",\r\nedescs_size);\r\ngoto fail_headers;\r\n}\r\nedescs = pfn_to_kaddr(page_to_pfn(edescs_page));\r\nequeue_order = get_order(sizeof(*equeue));\r\nequeue_page = alloc_pages(GFP_KERNEL, equeue_order);\r\nif (equeue_page == NULL) {\r\nnetdev_warn(dev,\r\n"Could not alloc %zd bytes for equeue info.\n",\r\nPAGE_SIZE << equeue_order);\r\ngoto fail_edescs;\r\n}\r\nequeue = pfn_to_kaddr(page_to_pfn(equeue_page));\r\nrc = gxio_mpipe_alloc_edma_rings(&context, 1, 0, 0);\r\nif (rc < 0) {\r\nnetdev_warn(dev, "gxio_mpipe_alloc_edma_rings failed: %d\n",\r\nrc);\r\ngoto fail_equeue;\r\n}\r\nedma = rc;\r\nrc = gxio_mpipe_equeue_init(equeue, &context, edma, echannel,\r\nedescs, edescs_size, 0);\r\nif (rc != 0) {\r\nnetdev_err(dev, "gxio_mpipe_equeue_init failed: %d\n", rc);\r\ngoto fail_equeue;\r\n}\r\negress_for_echannel[echannel].equeue = equeue;\r\negress_for_echannel[echannel].headers = headers;\r\nreturn 0;\r\nfail_equeue:\r\n__free_pages(equeue_page, equeue_order);\r\nfail_edescs:\r\n__free_pages(edescs_page, edescs_order);\r\nfail_headers:\r\n__free_pages(headers_page, headers_order);\r\nfail:\r\nreturn rc;\r\n}\r\nstatic int tile_net_link_open(struct net_device *dev, gxio_mpipe_link_t *link,\r\nconst char *link_name)\r\n{\r\nint rc = gxio_mpipe_link_open(link, &context, link_name, 0);\r\nif (rc < 0) {\r\nnetdev_err(dev, "Failed to open '%s'\n", link_name);\r\nreturn rc;\r\n}\r\nrc = gxio_mpipe_link_channel(link);\r\nif (rc < 0 || rc >= TILE_NET_CHANNELS) {\r\nnetdev_err(dev, "gxio_mpipe_link_channel bad value: %d\n", rc);\r\ngxio_mpipe_link_close(link);\r\nreturn -EINVAL;\r\n}\r\nreturn rc;\r\n}\r\nstatic int tile_net_open(struct net_device *dev)\r\n{\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nint cpu, rc;\r\nmutex_lock(&tile_net_devs_for_channel_mutex);\r\nif (ingress_irq < 0) {\r\nrc = tile_net_init_mpipe(dev);\r\nif (rc != 0)\r\ngoto fail;\r\n}\r\nif (unlikely((loopify_link_name != NULL) &&\r\n!strcmp(dev->name, loopify_link_name))) {\r\nrc = tile_net_link_open(dev, &priv->link, "loop0");\r\nif (rc < 0)\r\ngoto fail;\r\npriv->channel = rc;\r\nrc = tile_net_link_open(dev, &priv->loopify_link, "loop1");\r\nif (rc < 0)\r\ngoto fail;\r\npriv->loopify_channel = rc;\r\npriv->echannel = rc;\r\n} else {\r\nrc = tile_net_link_open(dev, &priv->link, dev->name);\r\nif (rc < 0)\r\ngoto fail;\r\npriv->channel = rc;\r\npriv->echannel = rc;\r\n}\r\nrc = tile_net_init_egress(dev, priv->echannel);\r\nif (rc != 0)\r\ngoto fail;\r\ntile_net_devs_for_channel[priv->channel] = dev;\r\nrc = tile_net_update(dev);\r\nif (rc != 0)\r\ngoto fail;\r\nmutex_unlock(&tile_net_devs_for_channel_mutex);\r\nfor_each_online_cpu(cpu) {\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nstruct tile_net_tx_wake *tx_wake =\r\n&info->tx_wake[priv->echannel];\r\nhrtimer_init(&tx_wake->timer, CLOCK_MONOTONIC,\r\nHRTIMER_MODE_REL);\r\ntx_wake->tx_queue_idx = cpu;\r\ntx_wake->timer.function = tile_net_handle_tx_wake_timer;\r\ntx_wake->dev = dev;\r\n}\r\nfor_each_online_cpu(cpu)\r\nnetif_start_subqueue(dev, cpu);\r\nnetif_carrier_on(dev);\r\nreturn 0;\r\nfail:\r\nif (priv->loopify_channel >= 0) {\r\nif (gxio_mpipe_link_close(&priv->loopify_link) != 0)\r\nnetdev_warn(dev, "Failed to close loopify link!\n");\r\npriv->loopify_channel = -1;\r\n}\r\nif (priv->channel >= 0) {\r\nif (gxio_mpipe_link_close(&priv->link) != 0)\r\nnetdev_warn(dev, "Failed to close link!\n");\r\npriv->channel = -1;\r\n}\r\npriv->echannel = -1;\r\ntile_net_devs_for_channel[priv->channel] = NULL;\r\nmutex_unlock(&tile_net_devs_for_channel_mutex);\r\nreturn (rc > -512) ? rc : -EIO;\r\n}\r\nstatic int tile_net_stop(struct net_device *dev)\r\n{\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nint cpu;\r\nfor_each_online_cpu(cpu) {\r\nstruct tile_net_info *info = &per_cpu(per_cpu_info, cpu);\r\nstruct tile_net_tx_wake *tx_wake =\r\n&info->tx_wake[priv->echannel];\r\nhrtimer_cancel(&tx_wake->timer);\r\nnetif_stop_subqueue(dev, cpu);\r\n}\r\nmutex_lock(&tile_net_devs_for_channel_mutex);\r\ntile_net_devs_for_channel[priv->channel] = NULL;\r\n(void)tile_net_update(dev);\r\nif (priv->loopify_channel >= 0) {\r\nif (gxio_mpipe_link_close(&priv->loopify_link) != 0)\r\nnetdev_warn(dev, "Failed to close loopify link!\n");\r\npriv->loopify_channel = -1;\r\n}\r\nif (priv->channel >= 0) {\r\nif (gxio_mpipe_link_close(&priv->link) != 0)\r\nnetdev_warn(dev, "Failed to close link!\n");\r\npriv->channel = -1;\r\n}\r\npriv->echannel = -1;\r\nmutex_unlock(&tile_net_devs_for_channel_mutex);\r\nreturn 0;\r\n}\r\nstatic inline void *tile_net_frag_buf(skb_frag_t *f)\r\n{\r\nunsigned long pfn = page_to_pfn(skb_frag_page(f));\r\nreturn pfn_to_kaddr(pfn) + f->page_offset;\r\n}\r\nstatic s64 tile_net_equeue_try_reserve(struct net_device *dev,\r\nint tx_queue_idx,\r\nstruct tile_net_comps *comps,\r\ngxio_mpipe_equeue_t *equeue,\r\nint num_edescs)\r\n{\r\nif (comps->comp_next - comps->comp_last < TILE_NET_MAX_COMPS - 1 ||\r\ntile_net_free_comps(equeue, comps, 32, false) != 0) {\r\ns64 slot = gxio_mpipe_equeue_try_reserve(equeue, num_edescs);\r\nif (slot >= 0)\r\nreturn slot;\r\ntile_net_free_comps(equeue, comps, TILE_NET_MAX_COMPS, false);\r\nslot = gxio_mpipe_equeue_try_reserve(equeue, num_edescs);\r\nif (slot >= 0)\r\nreturn slot;\r\n}\r\nnetif_stop_subqueue(dev, tx_queue_idx);\r\ntile_net_schedule_tx_wake_timer(dev, tx_queue_idx);\r\nreturn -1;\r\n}\r\nstatic int tso_count_edescs(struct sk_buff *skb)\r\n{\r\nstruct skb_shared_info *sh = skb_shinfo(skb);\r\nunsigned int sh_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nunsigned int data_len = skb->len - sh_len;\r\nunsigned int p_len = sh->gso_size;\r\nlong f_id = -1;\r\nlong f_size = skb_headlen(skb) - sh_len;\r\nlong f_used = 0;\r\nlong n;\r\nint num_edescs = 0;\r\nint segment;\r\nfor (segment = 0; segment < sh->gso_segs; segment++) {\r\nunsigned int p_used = 0;\r\nfor (num_edescs++; p_used < p_len; num_edescs++) {\r\nwhile (f_used >= f_size) {\r\nf_id++;\r\nf_size = skb_frag_size(&sh->frags[f_id]);\r\nf_used = 0;\r\n}\r\nn = p_len - p_used;\r\nif (n > f_size - f_used)\r\nn = f_size - f_used;\r\nf_used += n;\r\np_used += n;\r\n}\r\ndata_len -= p_len;\r\nif (data_len < p_len)\r\np_len = data_len;\r\n}\r\nreturn num_edescs;\r\n}\r\nstatic void tso_headers_prepare(struct sk_buff *skb, unsigned char *headers,\r\ns64 slot)\r\n{\r\nstruct skb_shared_info *sh = skb_shinfo(skb);\r\nstruct iphdr *ih;\r\nstruct tcphdr *th;\r\nunsigned int sh_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nunsigned int data_len = skb->len - sh_len;\r\nunsigned char *data = skb->data;\r\nunsigned int ih_off, th_off, p_len;\r\nunsigned int isum_seed, tsum_seed, id, seq;\r\nlong f_id = -1;\r\nlong f_size = skb_headlen(skb) - sh_len;\r\nlong f_used = 0;\r\nlong n;\r\nint segment;\r\nih = ip_hdr(skb);\r\nth = tcp_hdr(skb);\r\nih_off = skb_network_offset(skb);\r\nth_off = skb_transport_offset(skb);\r\np_len = sh->gso_size;\r\nisum_seed = ((0xFFFF - ih->check) +\r\n(0xFFFF - ih->tot_len) +\r\n(0xFFFF - ih->id));\r\ntsum_seed = th->check + (0xFFFF ^ htons(skb->len));\r\nid = ntohs(ih->id);\r\nseq = ntohl(th->seq);\r\nfor (segment = 0; segment < sh->gso_segs; segment++) {\r\nunsigned char *buf;\r\nunsigned int p_used = 0;\r\nbuf = headers + (slot % EQUEUE_ENTRIES) * HEADER_BYTES +\r\nNET_IP_ALIGN;\r\nmemcpy(buf, data, sh_len);\r\nih = (struct iphdr *)(buf + ih_off);\r\nih->tot_len = htons(sh_len + p_len - ih_off);\r\nih->id = htons(id);\r\nih->check = csum_long(isum_seed + ih->tot_len +\r\nih->id) ^ 0xffff;\r\nth = (struct tcphdr *)(buf + th_off);\r\nth->seq = htonl(seq);\r\nth->check = csum_long(tsum_seed + htons(sh_len + p_len));\r\nif (segment != sh->gso_segs - 1) {\r\nth->fin = 0;\r\nth->psh = 0;\r\n}\r\nslot++;\r\nwhile (p_used < p_len) {\r\nwhile (f_used >= f_size) {\r\nf_id++;\r\nf_size = skb_frag_size(&sh->frags[f_id]);\r\nf_used = 0;\r\n}\r\nn = p_len - p_used;\r\nif (n > f_size - f_used)\r\nn = f_size - f_used;\r\nf_used += n;\r\np_used += n;\r\nslot++;\r\n}\r\nid++;\r\nseq += p_len;\r\ndata_len -= p_len;\r\nif (data_len < p_len)\r\np_len = data_len;\r\n}\r\nwmb();\r\n}\r\nstatic void tso_egress(struct net_device *dev, gxio_mpipe_equeue_t *equeue,\r\nstruct sk_buff *skb, unsigned char *headers, s64 slot)\r\n{\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nstruct skb_shared_info *sh = skb_shinfo(skb);\r\nunsigned int sh_len = skb_transport_offset(skb) + tcp_hdrlen(skb);\r\nunsigned int data_len = skb->len - sh_len;\r\nunsigned int p_len = sh->gso_size;\r\ngxio_mpipe_edesc_t edesc_head = { { 0 } };\r\ngxio_mpipe_edesc_t edesc_body = { { 0 } };\r\nlong f_id = -1;\r\nlong f_size = skb_headlen(skb) - sh_len;\r\nlong f_used = 0;\r\nvoid *f_data = skb->data + sh_len;\r\nlong n;\r\nunsigned long tx_packets = 0, tx_bytes = 0;\r\nunsigned int csum_start;\r\nint segment;\r\ncsum_start = skb_checksum_start_offset(skb);\r\nedesc_head.csum = 1;\r\nedesc_head.csum_start = csum_start;\r\nedesc_head.csum_dest = csum_start + skb->csum_offset;\r\nedesc_head.xfer_size = sh_len;\r\nedesc_head.stack_idx = large_buffer_stack;\r\nedesc_body.stack_idx = large_buffer_stack;\r\nfor (segment = 0; segment < sh->gso_segs; segment++) {\r\nunsigned char *buf;\r\nunsigned int p_used = 0;\r\nbuf = headers + (slot % EQUEUE_ENTRIES) * HEADER_BYTES +\r\nNET_IP_ALIGN;\r\nedesc_head.va = va_to_tile_io_addr(buf);\r\ngxio_mpipe_equeue_put_at(equeue, edesc_head, slot);\r\nslot++;\r\nwhile (p_used < p_len) {\r\nvoid *va;\r\nwhile (f_used >= f_size) {\r\nf_id++;\r\nf_size = skb_frag_size(&sh->frags[f_id]);\r\nf_data = tile_net_frag_buf(&sh->frags[f_id]);\r\nf_used = 0;\r\n}\r\nva = f_data + f_used;\r\nn = p_len - p_used;\r\nif (n > f_size - f_used)\r\nn = f_size - f_used;\r\nf_used += n;\r\np_used += n;\r\nedesc_body.va = va_to_tile_io_addr(va);\r\nedesc_body.xfer_size = n;\r\nedesc_body.bound = !(p_used < p_len);\r\ngxio_mpipe_equeue_put_at(equeue, edesc_body, slot);\r\nslot++;\r\n}\r\ntx_packets++;\r\ntx_bytes += sh_len + p_len;\r\ndata_len -= p_len;\r\nif (data_len < p_len)\r\np_len = data_len;\r\n}\r\ntile_net_stats_add(tx_packets, &priv->stats.tx_packets);\r\ntile_net_stats_add(tx_bytes, &priv->stats.tx_bytes);\r\n}\r\nstatic int tile_net_tx_tso(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nint channel = priv->echannel;\r\nstruct tile_net_egress *egress = &egress_for_echannel[channel];\r\nstruct tile_net_comps *comps = info->comps_for_echannel[channel];\r\ngxio_mpipe_equeue_t *equeue = egress->equeue;\r\nunsigned long irqflags;\r\nint num_edescs;\r\ns64 slot;\r\nnum_edescs = tso_count_edescs(skb);\r\nlocal_irq_save(irqflags);\r\nslot = tile_net_equeue_try_reserve(dev, skb->queue_mapping, comps,\r\nequeue, num_edescs);\r\nif (slot < 0) {\r\nlocal_irq_restore(irqflags);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\ntso_headers_prepare(skb, egress->headers, slot);\r\ntso_egress(dev, equeue, skb, egress->headers, slot);\r\nadd_comp(equeue, comps, slot + num_edescs - 1, skb);\r\nlocal_irq_restore(irqflags);\r\ntile_net_schedule_egress_timer();\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic unsigned int tile_net_tx_frags(struct frag *frags,\r\nstruct sk_buff *skb,\r\nvoid *b_data, unsigned int b_len)\r\n{\r\nunsigned int i, n = 0;\r\nstruct skb_shared_info *sh = skb_shinfo(skb);\r\nif (b_len != 0) {\r\nfrags[n].buf = b_data;\r\nfrags[n++].length = b_len;\r\n}\r\nfor (i = 0; i < sh->nr_frags; i++) {\r\nskb_frag_t *f = &sh->frags[i];\r\nfrags[n].buf = tile_net_frag_buf(f);\r\nfrags[n++].length = skb_frag_size(f);\r\n}\r\nreturn n;\r\n}\r\nstatic int tile_net_tx(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nstruct tile_net_egress *egress = &egress_for_echannel[priv->echannel];\r\ngxio_mpipe_equeue_t *equeue = egress->equeue;\r\nstruct tile_net_comps *comps =\r\ninfo->comps_for_echannel[priv->echannel];\r\nunsigned int len = skb->len;\r\nunsigned char *data = skb->data;\r\nunsigned int num_edescs;\r\nstruct frag frags[MAX_FRAGS];\r\ngxio_mpipe_edesc_t edescs[MAX_FRAGS];\r\nunsigned long irqflags;\r\ngxio_mpipe_edesc_t edesc = { { 0 } };\r\nunsigned int i;\r\ns64 slot;\r\nif (skb_is_gso(skb))\r\nreturn tile_net_tx_tso(skb, dev);\r\nnum_edescs = tile_net_tx_frags(frags, skb, data, skb_headlen(skb));\r\nedesc.stack_idx = large_buffer_stack;\r\nfor (i = 0; i < num_edescs; i++) {\r\nedesc.xfer_size = frags[i].length;\r\nedesc.va = va_to_tile_io_addr(frags[i].buf);\r\nedescs[i] = edesc;\r\n}\r\nedescs[num_edescs - 1].bound = 1;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nunsigned int csum_start = skb_checksum_start_offset(skb);\r\nedescs[0].csum = 1;\r\nedescs[0].csum_start = csum_start;\r\nedescs[0].csum_dest = csum_start + skb->csum_offset;\r\n}\r\nlocal_irq_save(irqflags);\r\nslot = tile_net_equeue_try_reserve(dev, skb->queue_mapping, comps,\r\nequeue, num_edescs);\r\nif (slot < 0) {\r\nlocal_irq_restore(irqflags);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nfor (i = 0; i < num_edescs; i++)\r\ngxio_mpipe_equeue_put_at(equeue, edescs[i], slot++);\r\nadd_comp(equeue, comps, slot - 1, skb);\r\ntile_net_stats_add(1, &priv->stats.tx_packets);\r\ntile_net_stats_add(max_t(unsigned int, len, ETH_ZLEN),\r\n&priv->stats.tx_bytes);\r\nlocal_irq_restore(irqflags);\r\ntile_net_schedule_egress_timer();\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic u16 tile_net_select_queue(struct net_device *dev, struct sk_buff *skb)\r\n{\r\nreturn smp_processor_id();\r\n}\r\nstatic void tile_net_tx_timeout(struct net_device *dev)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nnetif_wake_subqueue(dev, cpu);\r\n}\r\nstatic int tile_net_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic struct net_device_stats *tile_net_get_stats(struct net_device *dev)\r\n{\r\nstruct tile_net_priv *priv = netdev_priv(dev);\r\nreturn &priv->stats;\r\n}\r\nstatic int tile_net_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nif ((new_mtu < 68) || (new_mtu > 1500))\r\nreturn -EINVAL;\r\ndev->mtu = new_mtu;\r\nreturn 0;\r\n}\r\nstatic int tile_net_set_mac_address(struct net_device *dev, void *p)\r\n{\r\nstruct sockaddr *addr = p;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EINVAL;\r\nmemcpy(dev->dev_addr, addr->sa_data, dev->addr_len);\r\nreturn 0;\r\n}\r\nstatic void tile_net_netpoll(struct net_device *dev)\r\n{\r\ndisable_percpu_irq(ingress_irq);\r\ntile_net_handle_ingress_irq(ingress_irq, NULL);\r\nenable_percpu_irq(ingress_irq, 0);\r\n}\r\nstatic void tile_net_setup(struct net_device *dev)\r\n{\r\nether_setup(dev);\r\ndev->netdev_ops = &tile_net_ops;\r\ndev->watchdog_timeo = TILE_NET_TIMEOUT;\r\ndev->features |= NETIF_F_LLTX;\r\ndev->features |= NETIF_F_HW_CSUM;\r\ndev->features |= NETIF_F_SG;\r\ndev->features |= NETIF_F_TSO;\r\ndev->mtu = 1500;\r\n}\r\nstatic void tile_net_dev_init(const char *name, const uint8_t *mac)\r\n{\r\nint ret;\r\nint i;\r\nint nz_addr = 0;\r\nstruct net_device *dev;\r\nstruct tile_net_priv *priv;\r\nif (strncmp(name, "loop", 4) == 0)\r\nreturn;\r\ndev = alloc_netdev_mqs(sizeof(*priv), name, tile_net_setup,\r\nNR_CPUS, 1);\r\nif (!dev) {\r\npr_err("alloc_netdev_mqs(%s) failed\n", name);\r\nreturn;\r\n}\r\npriv = netdev_priv(dev);\r\nmemset(priv, 0, sizeof(*priv));\r\npriv->dev = dev;\r\npriv->channel = -1;\r\npriv->loopify_channel = -1;\r\npriv->echannel = -1;\r\nfor (i = 0; i < 6; i++)\r\nnz_addr |= mac[i];\r\nif (nz_addr) {\r\nmemcpy(dev->dev_addr, mac, 6);\r\ndev->addr_len = 6;\r\n} else {\r\neth_hw_addr_random(dev);\r\n}\r\nret = register_netdev(dev);\r\nif (ret) {\r\nnetdev_err(dev, "register_netdev failed %d\n", ret);\r\nfree_netdev(dev);\r\nreturn;\r\n}\r\n}\r\nstatic void tile_net_init_module_percpu(void *unused)\r\n{\r\nstruct tile_net_info *info = &__get_cpu_var(per_cpu_info);\r\nint my_cpu = smp_processor_id();\r\ninfo->has_iqueue = false;\r\ninfo->my_cpu = my_cpu;\r\nhrtimer_init(&info->egress_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\ninfo->egress_timer.function = tile_net_handle_egress_timer;\r\n}\r\nstatic int __init tile_net_init_module(void)\r\n{\r\nint i;\r\nchar name[GXIO_MPIPE_LINK_NAME_LEN];\r\nuint8_t mac[6];\r\npr_info("Tilera Network Driver\n");\r\nmutex_init(&tile_net_devs_for_channel_mutex);\r\non_each_cpu(tile_net_init_module_percpu, NULL, 1);\r\nfor (i = 0; gxio_mpipe_link_enumerate_mac(i, name, mac) >= 0; i++)\r\ntile_net_dev_init(name, mac);\r\nif (!network_cpus_init())\r\nnetwork_cpus_map = *cpu_online_mask;\r\nreturn 0;\r\n}
