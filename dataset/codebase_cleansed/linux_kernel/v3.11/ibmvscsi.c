static irqreturn_t ibmvscsi_handle_event(int irq, void *dev_instance)\r\n{\r\nstruct ibmvscsi_host_data *hostdata =\r\n(struct ibmvscsi_host_data *)dev_instance;\r\nvio_disable_interrupts(to_vio_dev(hostdata->dev));\r\ntasklet_schedule(&hostdata->srp_task);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void ibmvscsi_release_crq_queue(struct crq_queue *queue,\r\nstruct ibmvscsi_host_data *hostdata,\r\nint max_requests)\r\n{\r\nlong rc = 0;\r\nstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\r\nfree_irq(vdev->irq, (void *)hostdata);\r\ntasklet_kill(&hostdata->srp_task);\r\ndo {\r\nif (rc)\r\nmsleep(100);\r\nrc = plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);\r\n} while ((rc == H_BUSY) || (H_IS_LONG_BUSY(rc)));\r\ndma_unmap_single(hostdata->dev,\r\nqueue->msg_token,\r\nqueue->size * sizeof(*queue->msgs), DMA_BIDIRECTIONAL);\r\nfree_page((unsigned long)queue->msgs);\r\n}\r\nstatic struct viosrp_crq *crq_queue_next_crq(struct crq_queue *queue)\r\n{\r\nstruct viosrp_crq *crq;\r\nunsigned long flags;\r\nspin_lock_irqsave(&queue->lock, flags);\r\ncrq = &queue->msgs[queue->cur];\r\nif (crq->valid & 0x80) {\r\nif (++queue->cur == queue->size)\r\nqueue->cur = 0;\r\n} else\r\ncrq = NULL;\r\nspin_unlock_irqrestore(&queue->lock, flags);\r\nreturn crq;\r\n}\r\nstatic int ibmvscsi_send_crq(struct ibmvscsi_host_data *hostdata,\r\nu64 word1, u64 word2)\r\n{\r\nstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\r\nreturn plpar_hcall_norets(H_SEND_CRQ, vdev->unit_address, word1, word2);\r\n}\r\nstatic void ibmvscsi_task(void *data)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = (struct ibmvscsi_host_data *)data;\r\nstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\r\nstruct viosrp_crq *crq;\r\nint done = 0;\r\nwhile (!done) {\r\nwhile ((crq = crq_queue_next_crq(&hostdata->queue)) != NULL) {\r\nibmvscsi_handle_crq(crq, hostdata);\r\ncrq->valid = 0x00;\r\n}\r\nvio_enable_interrupts(vdev);\r\ncrq = crq_queue_next_crq(&hostdata->queue);\r\nif (crq != NULL) {\r\nvio_disable_interrupts(vdev);\r\nibmvscsi_handle_crq(crq, hostdata);\r\ncrq->valid = 0x00;\r\n} else {\r\ndone = 1;\r\n}\r\n}\r\n}\r\nstatic void gather_partition_info(void)\r\n{\r\nstruct device_node *rootdn;\r\nconst char *ppartition_name;\r\nconst unsigned int *p_number_ptr;\r\nrootdn = of_find_node_by_path("/");\r\nif (!rootdn) {\r\nreturn;\r\n}\r\nppartition_name = of_get_property(rootdn, "ibm,partition-name", NULL);\r\nif (ppartition_name)\r\nstrncpy(partition_name, ppartition_name,\r\nsizeof(partition_name));\r\np_number_ptr = of_get_property(rootdn, "ibm,partition-no", NULL);\r\nif (p_number_ptr)\r\npartition_number = *p_number_ptr;\r\nof_node_put(rootdn);\r\n}\r\nstatic void set_adapter_info(struct ibmvscsi_host_data *hostdata)\r\n{\r\nmemset(&hostdata->madapter_info, 0x00,\r\nsizeof(hostdata->madapter_info));\r\ndev_info(hostdata->dev, "SRP_VERSION: %s\n", SRP_VERSION);\r\nstrcpy(hostdata->madapter_info.srp_version, SRP_VERSION);\r\nstrncpy(hostdata->madapter_info.partition_name, partition_name,\r\nsizeof(hostdata->madapter_info.partition_name));\r\nhostdata->madapter_info.partition_number = partition_number;\r\nhostdata->madapter_info.mad_version = 1;\r\nhostdata->madapter_info.os_type = 2;\r\n}\r\nstatic int ibmvscsi_reset_crq_queue(struct crq_queue *queue,\r\nstruct ibmvscsi_host_data *hostdata)\r\n{\r\nint rc = 0;\r\nstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\r\ndo {\r\nif (rc)\r\nmsleep(100);\r\nrc = plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);\r\n} while ((rc == H_BUSY) || (H_IS_LONG_BUSY(rc)));\r\nmemset(queue->msgs, 0x00, PAGE_SIZE);\r\nqueue->cur = 0;\r\nset_adapter_info(hostdata);\r\nrc = plpar_hcall_norets(H_REG_CRQ,\r\nvdev->unit_address,\r\nqueue->msg_token, PAGE_SIZE);\r\nif (rc == 2) {\r\ndev_warn(hostdata->dev, "Partner adapter not ready\n");\r\n} else if (rc != 0) {\r\ndev_warn(hostdata->dev, "couldn't register crq--rc 0x%x\n", rc);\r\n}\r\nreturn rc;\r\n}\r\nstatic int ibmvscsi_init_crq_queue(struct crq_queue *queue,\r\nstruct ibmvscsi_host_data *hostdata,\r\nint max_requests)\r\n{\r\nint rc;\r\nint retrc;\r\nstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\r\nqueue->msgs = (struct viosrp_crq *)get_zeroed_page(GFP_KERNEL);\r\nif (!queue->msgs)\r\ngoto malloc_failed;\r\nqueue->size = PAGE_SIZE / sizeof(*queue->msgs);\r\nqueue->msg_token = dma_map_single(hostdata->dev, queue->msgs,\r\nqueue->size * sizeof(*queue->msgs),\r\nDMA_BIDIRECTIONAL);\r\nif (dma_mapping_error(hostdata->dev, queue->msg_token))\r\ngoto map_failed;\r\ngather_partition_info();\r\nset_adapter_info(hostdata);\r\nretrc = rc = plpar_hcall_norets(H_REG_CRQ,\r\nvdev->unit_address,\r\nqueue->msg_token, PAGE_SIZE);\r\nif (rc == H_RESOURCE)\r\nrc = ibmvscsi_reset_crq_queue(queue,\r\nhostdata);\r\nif (rc == 2) {\r\ndev_warn(hostdata->dev, "Partner adapter not ready\n");\r\nretrc = 0;\r\n} else if (rc != 0) {\r\ndev_warn(hostdata->dev, "Error %d opening adapter\n", rc);\r\ngoto reg_crq_failed;\r\n}\r\nqueue->cur = 0;\r\nspin_lock_init(&queue->lock);\r\ntasklet_init(&hostdata->srp_task, (void *)ibmvscsi_task,\r\n(unsigned long)hostdata);\r\nif (request_irq(vdev->irq,\r\nibmvscsi_handle_event,\r\n0, "ibmvscsi", (void *)hostdata) != 0) {\r\ndev_err(hostdata->dev, "couldn't register irq 0x%x\n",\r\nvdev->irq);\r\ngoto req_irq_failed;\r\n}\r\nrc = vio_enable_interrupts(vdev);\r\nif (rc != 0) {\r\ndev_err(hostdata->dev, "Error %d enabling interrupts!!!\n", rc);\r\ngoto req_irq_failed;\r\n}\r\nreturn retrc;\r\nreq_irq_failed:\r\ntasklet_kill(&hostdata->srp_task);\r\nrc = 0;\r\ndo {\r\nif (rc)\r\nmsleep(100);\r\nrc = plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);\r\n} while ((rc == H_BUSY) || (H_IS_LONG_BUSY(rc)));\r\nreg_crq_failed:\r\ndma_unmap_single(hostdata->dev,\r\nqueue->msg_token,\r\nqueue->size * sizeof(*queue->msgs), DMA_BIDIRECTIONAL);\r\nmap_failed:\r\nfree_page((unsigned long)queue->msgs);\r\nmalloc_failed:\r\nreturn -1;\r\n}\r\nstatic int ibmvscsi_reenable_crq_queue(struct crq_queue *queue,\r\nstruct ibmvscsi_host_data *hostdata)\r\n{\r\nint rc = 0;\r\nstruct vio_dev *vdev = to_vio_dev(hostdata->dev);\r\ndo {\r\nif (rc)\r\nmsleep(100);\r\nrc = plpar_hcall_norets(H_ENABLE_CRQ, vdev->unit_address);\r\n} while ((rc == H_IN_PROGRESS) || (rc == H_BUSY) || (H_IS_LONG_BUSY(rc)));\r\nif (rc)\r\ndev_err(hostdata->dev, "Error %d enabling adapter\n", rc);\r\nreturn rc;\r\n}\r\nstatic int initialize_event_pool(struct event_pool *pool,\r\nint size, struct ibmvscsi_host_data *hostdata)\r\n{\r\nint i;\r\npool->size = size;\r\npool->next = 0;\r\npool->events = kcalloc(pool->size, sizeof(*pool->events), GFP_KERNEL);\r\nif (!pool->events)\r\nreturn -ENOMEM;\r\npool->iu_storage =\r\ndma_alloc_coherent(hostdata->dev,\r\npool->size * sizeof(*pool->iu_storage),\r\n&pool->iu_token, 0);\r\nif (!pool->iu_storage) {\r\nkfree(pool->events);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < pool->size; ++i) {\r\nstruct srp_event_struct *evt = &pool->events[i];\r\nmemset(&evt->crq, 0x00, sizeof(evt->crq));\r\natomic_set(&evt->free, 1);\r\nevt->crq.valid = 0x80;\r\nevt->crq.IU_length = sizeof(*evt->xfer_iu);\r\nevt->crq.IU_data_ptr = pool->iu_token +\r\nsizeof(*evt->xfer_iu) * i;\r\nevt->xfer_iu = pool->iu_storage + i;\r\nevt->hostdata = hostdata;\r\nevt->ext_list = NULL;\r\nevt->ext_list_token = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic void release_event_pool(struct event_pool *pool,\r\nstruct ibmvscsi_host_data *hostdata)\r\n{\r\nint i, in_use = 0;\r\nfor (i = 0; i < pool->size; ++i) {\r\nif (atomic_read(&pool->events[i].free) != 1)\r\n++in_use;\r\nif (pool->events[i].ext_list) {\r\ndma_free_coherent(hostdata->dev,\r\nSG_ALL * sizeof(struct srp_direct_buf),\r\npool->events[i].ext_list,\r\npool->events[i].ext_list_token);\r\n}\r\n}\r\nif (in_use)\r\ndev_warn(hostdata->dev, "releasing event pool with %d "\r\n"events still in use?\n", in_use);\r\nkfree(pool->events);\r\ndma_free_coherent(hostdata->dev,\r\npool->size * sizeof(*pool->iu_storage),\r\npool->iu_storage, pool->iu_token);\r\n}\r\nstatic int valid_event_struct(struct event_pool *pool,\r\nstruct srp_event_struct *evt)\r\n{\r\nint index = evt - pool->events;\r\nif (index < 0 || index >= pool->size)\r\nreturn 0;\r\nif (evt != pool->events + index)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void free_event_struct(struct event_pool *pool,\r\nstruct srp_event_struct *evt)\r\n{\r\nif (!valid_event_struct(pool, evt)) {\r\ndev_err(evt->hostdata->dev, "Freeing invalid event_struct %p "\r\n"(not in pool %p)\n", evt, pool->events);\r\nreturn;\r\n}\r\nif (atomic_inc_return(&evt->free) != 1) {\r\ndev_err(evt->hostdata->dev, "Freeing event_struct %p "\r\n"which is not in use!\n", evt);\r\nreturn;\r\n}\r\n}\r\nstatic struct srp_event_struct *get_event_struct(struct event_pool *pool)\r\n{\r\nint i;\r\nint poolsize = pool->size;\r\nint offset = pool->next;\r\nfor (i = 0; i < poolsize; i++) {\r\noffset = (offset + 1) % poolsize;\r\nif (!atomic_dec_if_positive(&pool->events[offset].free)) {\r\npool->next = offset;\r\nreturn &pool->events[offset];\r\n}\r\n}\r\nprintk(KERN_ERR "ibmvscsi: found no event struct in pool!\n");\r\nreturn NULL;\r\n}\r\nstatic void init_event_struct(struct srp_event_struct *evt_struct,\r\nvoid (*done) (struct srp_event_struct *),\r\nu8 format,\r\nint timeout)\r\n{\r\nevt_struct->cmnd = NULL;\r\nevt_struct->cmnd_done = NULL;\r\nevt_struct->sync_srp = NULL;\r\nevt_struct->crq.format = format;\r\nevt_struct->crq.timeout = timeout;\r\nevt_struct->done = done;\r\n}\r\nstatic void set_srp_direction(struct scsi_cmnd *cmd,\r\nstruct srp_cmd *srp_cmd,\r\nint numbuf)\r\n{\r\nu8 fmt;\r\nif (numbuf == 0)\r\nreturn;\r\nif (numbuf == 1)\r\nfmt = SRP_DATA_DESC_DIRECT;\r\nelse {\r\nfmt = SRP_DATA_DESC_INDIRECT;\r\nnumbuf = min(numbuf, MAX_INDIRECT_BUFS);\r\nif (cmd->sc_data_direction == DMA_TO_DEVICE)\r\nsrp_cmd->data_out_desc_cnt = numbuf;\r\nelse\r\nsrp_cmd->data_in_desc_cnt = numbuf;\r\n}\r\nif (cmd->sc_data_direction == DMA_TO_DEVICE)\r\nsrp_cmd->buf_fmt = fmt << 4;\r\nelse\r\nsrp_cmd->buf_fmt = fmt;\r\n}\r\nstatic void unmap_cmd_data(struct srp_cmd *cmd,\r\nstruct srp_event_struct *evt_struct,\r\nstruct device *dev)\r\n{\r\nu8 out_fmt, in_fmt;\r\nout_fmt = cmd->buf_fmt >> 4;\r\nin_fmt = cmd->buf_fmt & ((1U << 4) - 1);\r\nif (out_fmt == SRP_NO_DATA_DESC && in_fmt == SRP_NO_DATA_DESC)\r\nreturn;\r\nif (evt_struct->cmnd)\r\nscsi_dma_unmap(evt_struct->cmnd);\r\n}\r\nstatic int map_sg_list(struct scsi_cmnd *cmd, int nseg,\r\nstruct srp_direct_buf *md)\r\n{\r\nint i;\r\nstruct scatterlist *sg;\r\nu64 total_length = 0;\r\nscsi_for_each_sg(cmd, sg, nseg, i) {\r\nstruct srp_direct_buf *descr = md + i;\r\ndescr->va = sg_dma_address(sg);\r\ndescr->len = sg_dma_len(sg);\r\ndescr->key = 0;\r\ntotal_length += sg_dma_len(sg);\r\n}\r\nreturn total_length;\r\n}\r\nstatic int map_sg_data(struct scsi_cmnd *cmd,\r\nstruct srp_event_struct *evt_struct,\r\nstruct srp_cmd *srp_cmd, struct device *dev)\r\n{\r\nint sg_mapped;\r\nu64 total_length = 0;\r\nstruct srp_direct_buf *data =\r\n(struct srp_direct_buf *) srp_cmd->add_data;\r\nstruct srp_indirect_buf *indirect =\r\n(struct srp_indirect_buf *) data;\r\nsg_mapped = scsi_dma_map(cmd);\r\nif (!sg_mapped)\r\nreturn 1;\r\nelse if (sg_mapped < 0)\r\nreturn 0;\r\nset_srp_direction(cmd, srp_cmd, sg_mapped);\r\nif (sg_mapped == 1) {\r\nmap_sg_list(cmd, sg_mapped, data);\r\nreturn 1;\r\n}\r\nindirect->table_desc.va = 0;\r\nindirect->table_desc.len = sg_mapped * sizeof(struct srp_direct_buf);\r\nindirect->table_desc.key = 0;\r\nif (sg_mapped <= MAX_INDIRECT_BUFS) {\r\ntotal_length = map_sg_list(cmd, sg_mapped,\r\n&indirect->desc_list[0]);\r\nindirect->len = total_length;\r\nreturn 1;\r\n}\r\nif (!evt_struct->ext_list) {\r\nevt_struct->ext_list = (struct srp_direct_buf *)\r\ndma_alloc_coherent(dev,\r\nSG_ALL * sizeof(struct srp_direct_buf),\r\n&evt_struct->ext_list_token, 0);\r\nif (!evt_struct->ext_list) {\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\nsdev_printk(KERN_ERR, cmd->device,\r\n"Can't allocate memory "\r\n"for indirect table\n");\r\nscsi_dma_unmap(cmd);\r\nreturn 0;\r\n}\r\n}\r\ntotal_length = map_sg_list(cmd, sg_mapped, evt_struct->ext_list);\r\nindirect->len = total_length;\r\nindirect->table_desc.va = evt_struct->ext_list_token;\r\nindirect->table_desc.len = sg_mapped * sizeof(indirect->desc_list[0]);\r\nmemcpy(indirect->desc_list, evt_struct->ext_list,\r\nMAX_INDIRECT_BUFS * sizeof(struct srp_direct_buf));\r\nreturn 1;\r\n}\r\nstatic int map_data_for_srp_cmd(struct scsi_cmnd *cmd,\r\nstruct srp_event_struct *evt_struct,\r\nstruct srp_cmd *srp_cmd, struct device *dev)\r\n{\r\nswitch (cmd->sc_data_direction) {\r\ncase DMA_FROM_DEVICE:\r\ncase DMA_TO_DEVICE:\r\nbreak;\r\ncase DMA_NONE:\r\nreturn 1;\r\ncase DMA_BIDIRECTIONAL:\r\nsdev_printk(KERN_ERR, cmd->device,\r\n"Can't map DMA_BIDIRECTIONAL to read/write\n");\r\nreturn 0;\r\ndefault:\r\nsdev_printk(KERN_ERR, cmd->device,\r\n"Unknown data direction 0x%02x; can't map!\n",\r\ncmd->sc_data_direction);\r\nreturn 0;\r\n}\r\nreturn map_sg_data(cmd, evt_struct, srp_cmd, dev);\r\n}\r\nstatic void purge_requests(struct ibmvscsi_host_data *hostdata, int error_code)\r\n{\r\nstruct srp_event_struct *evt;\r\nunsigned long flags;\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\nwhile (!list_empty(&hostdata->sent)) {\r\nevt = list_first_entry(&hostdata->sent, struct srp_event_struct, list);\r\nlist_del(&evt->list);\r\ndel_timer(&evt->timer);\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nif (evt->cmnd) {\r\nevt->cmnd->result = (error_code << 16);\r\nunmap_cmd_data(&evt->iu.srp.cmd, evt,\r\nevt->hostdata->dev);\r\nif (evt->cmnd_done)\r\nevt->cmnd_done(evt->cmnd);\r\n} else if (evt->done)\r\nevt->done(evt);\r\nfree_event_struct(&evt->hostdata->pool, evt);\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\n}\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\n}\r\nstatic void ibmvscsi_reset_host(struct ibmvscsi_host_data *hostdata)\r\n{\r\nscsi_block_requests(hostdata->host);\r\natomic_set(&hostdata->request_limit, 0);\r\npurge_requests(hostdata, DID_ERROR);\r\nhostdata->reset_crq = 1;\r\nwake_up(&hostdata->work_wait_q);\r\n}\r\nstatic void ibmvscsi_timeout(struct srp_event_struct *evt_struct)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\r\ndev_err(hostdata->dev, "Command timed out (%x). Resetting connection\n",\r\nevt_struct->iu.srp.cmd.opcode);\r\nibmvscsi_reset_host(hostdata);\r\n}\r\nstatic int ibmvscsi_send_srp_event(struct srp_event_struct *evt_struct,\r\nstruct ibmvscsi_host_data *hostdata,\r\nunsigned long timeout)\r\n{\r\nu64 *crq_as_u64 = (u64 *) &evt_struct->crq;\r\nint request_status = 0;\r\nint rc;\r\nint srp_req = 0;\r\nif (evt_struct->crq.format == VIOSRP_SRP_FORMAT) {\r\nsrp_req = 1;\r\nrequest_status =\r\natomic_dec_if_positive(&hostdata->request_limit);\r\nif (request_status < -1)\r\ngoto send_error;\r\nelse if (request_status == -1 &&\r\nevt_struct->iu.srp.login_req.opcode != SRP_LOGIN_REQ)\r\ngoto send_busy;\r\nelse if (request_status < 2 &&\r\nevt_struct->iu.srp.cmd.opcode != SRP_TSK_MGMT) {\r\nint server_limit = request_status;\r\nstruct srp_event_struct *tmp_evt;\r\nlist_for_each_entry(tmp_evt, &hostdata->sent, list) {\r\nserver_limit++;\r\n}\r\nif (server_limit > 2)\r\ngoto send_busy;\r\n}\r\n}\r\n*evt_struct->xfer_iu = evt_struct->iu;\r\nevt_struct->xfer_iu->srp.rsp.tag = (u64)evt_struct;\r\nlist_add_tail(&evt_struct->list, &hostdata->sent);\r\ninit_timer(&evt_struct->timer);\r\nif (timeout) {\r\nevt_struct->timer.data = (unsigned long) evt_struct;\r\nevt_struct->timer.expires = jiffies + (timeout * HZ);\r\nevt_struct->timer.function = (void (*)(unsigned long))ibmvscsi_timeout;\r\nadd_timer(&evt_struct->timer);\r\n}\r\nif ((rc =\r\nibmvscsi_send_crq(hostdata, crq_as_u64[0], crq_as_u64[1])) != 0) {\r\nlist_del(&evt_struct->list);\r\ndel_timer(&evt_struct->timer);\r\nif (rc == H_CLOSED) {\r\ndev_warn(hostdata->dev, "send warning. "\r\n"Receive queue closed, will retry.\n");\r\ngoto send_busy;\r\n}\r\ndev_err(hostdata->dev, "send error %d\n", rc);\r\nif (srp_req)\r\natomic_inc(&hostdata->request_limit);\r\ngoto send_error;\r\n}\r\nreturn 0;\r\nsend_busy:\r\nunmap_cmd_data(&evt_struct->iu.srp.cmd, evt_struct, hostdata->dev);\r\nfree_event_struct(&hostdata->pool, evt_struct);\r\nif (srp_req && request_status != -1)\r\natomic_inc(&hostdata->request_limit);\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\nsend_error:\r\nunmap_cmd_data(&evt_struct->iu.srp.cmd, evt_struct, hostdata->dev);\r\nif (evt_struct->cmnd != NULL) {\r\nevt_struct->cmnd->result = DID_ERROR << 16;\r\nevt_struct->cmnd_done(evt_struct->cmnd);\r\n} else if (evt_struct->done)\r\nevt_struct->done(evt_struct);\r\nfree_event_struct(&hostdata->pool, evt_struct);\r\nreturn 0;\r\n}\r\nstatic void handle_cmd_rsp(struct srp_event_struct *evt_struct)\r\n{\r\nstruct srp_rsp *rsp = &evt_struct->xfer_iu->srp.rsp;\r\nstruct scsi_cmnd *cmnd = evt_struct->cmnd;\r\nif (unlikely(rsp->opcode != SRP_RSP)) {\r\nif (printk_ratelimit())\r\ndev_warn(evt_struct->hostdata->dev,\r\n"bad SRP RSP type %d\n", rsp->opcode);\r\n}\r\nif (cmnd) {\r\ncmnd->result |= rsp->status;\r\nif (((cmnd->result >> 1) & 0x1f) == CHECK_CONDITION)\r\nmemcpy(cmnd->sense_buffer,\r\nrsp->data,\r\nrsp->sense_data_len);\r\nunmap_cmd_data(&evt_struct->iu.srp.cmd,\r\nevt_struct,\r\nevt_struct->hostdata->dev);\r\nif (rsp->flags & SRP_RSP_FLAG_DOOVER)\r\nscsi_set_resid(cmnd, rsp->data_out_res_cnt);\r\nelse if (rsp->flags & SRP_RSP_FLAG_DIOVER)\r\nscsi_set_resid(cmnd, rsp->data_in_res_cnt);\r\n}\r\nif (evt_struct->cmnd_done)\r\nevt_struct->cmnd_done(cmnd);\r\n}\r\nstatic inline u16 lun_from_dev(struct scsi_device *dev)\r\n{\r\nreturn (0x2 << 14) | (dev->id << 8) | (dev->channel << 5) | dev->lun;\r\n}\r\nstatic int ibmvscsi_queuecommand_lck(struct scsi_cmnd *cmnd,\r\nvoid (*done) (struct scsi_cmnd *))\r\n{\r\nstruct srp_cmd *srp_cmd;\r\nstruct srp_event_struct *evt_struct;\r\nstruct srp_indirect_buf *indirect;\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(cmnd->device->host);\r\nu16 lun = lun_from_dev(cmnd->device);\r\nu8 out_fmt, in_fmt;\r\ncmnd->result = (DID_OK << 16);\r\nevt_struct = get_event_struct(&hostdata->pool);\r\nif (!evt_struct)\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\nsrp_cmd = &evt_struct->iu.srp.cmd;\r\nmemset(srp_cmd, 0x00, SRP_MAX_IU_LEN);\r\nsrp_cmd->opcode = SRP_CMD;\r\nmemcpy(srp_cmd->cdb, cmnd->cmnd, sizeof(srp_cmd->cdb));\r\nsrp_cmd->lun = ((u64) lun) << 48;\r\nif (!map_data_for_srp_cmd(cmnd, evt_struct, srp_cmd, hostdata->dev)) {\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\nsdev_printk(KERN_ERR, cmnd->device,\r\n"couldn't convert cmd to srp_cmd\n");\r\nfree_event_struct(&hostdata->pool, evt_struct);\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\ninit_event_struct(evt_struct,\r\nhandle_cmd_rsp,\r\nVIOSRP_SRP_FORMAT,\r\ncmnd->request->timeout/HZ);\r\nevt_struct->cmnd = cmnd;\r\nevt_struct->cmnd_done = done;\r\nindirect = (struct srp_indirect_buf *) srp_cmd->add_data;\r\nout_fmt = srp_cmd->buf_fmt >> 4;\r\nin_fmt = srp_cmd->buf_fmt & ((1U << 4) - 1);\r\nif ((in_fmt == SRP_DATA_DESC_INDIRECT ||\r\nout_fmt == SRP_DATA_DESC_INDIRECT) &&\r\nindirect->table_desc.va == 0) {\r\nindirect->table_desc.va = evt_struct->crq.IU_data_ptr +\r\noffsetof(struct srp_cmd, add_data) +\r\noffsetof(struct srp_indirect_buf, desc_list);\r\n}\r\nreturn ibmvscsi_send_srp_event(evt_struct, hostdata, 0);\r\n}\r\nvoid unmap_persist_bufs(struct ibmvscsi_host_data *hostdata)\r\n{\r\ndma_unmap_single(hostdata->dev, hostdata->caps_addr,\r\nsizeof(hostdata->caps), DMA_BIDIRECTIONAL);\r\ndma_unmap_single(hostdata->dev, hostdata->adapter_info_addr,\r\nsizeof(hostdata->madapter_info), DMA_BIDIRECTIONAL);\r\n}\r\nstatic void login_rsp(struct srp_event_struct *evt_struct)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\r\nswitch (evt_struct->xfer_iu->srp.login_rsp.opcode) {\r\ncase SRP_LOGIN_RSP:\r\nbreak;\r\ncase SRP_LOGIN_REJ:\r\ndev_info(hostdata->dev, "SRP_LOGIN_REJ reason %u\n",\r\nevt_struct->xfer_iu->srp.login_rej.reason);\r\natomic_set(&hostdata->request_limit, -1);\r\nreturn;\r\ndefault:\r\ndev_err(hostdata->dev, "Invalid login response typecode 0x%02x!\n",\r\nevt_struct->xfer_iu->srp.login_rsp.opcode);\r\natomic_set(&hostdata->request_limit, -1);\r\nreturn;\r\n}\r\ndev_info(hostdata->dev, "SRP_LOGIN succeeded\n");\r\nhostdata->client_migrated = 0;\r\natomic_set(&hostdata->request_limit,\r\nevt_struct->xfer_iu->srp.login_rsp.req_lim_delta);\r\nscsi_unblock_requests(hostdata->host);\r\n}\r\nstatic int send_srp_login(struct ibmvscsi_host_data *hostdata)\r\n{\r\nint rc;\r\nunsigned long flags;\r\nstruct srp_login_req *login;\r\nstruct srp_event_struct *evt_struct = get_event_struct(&hostdata->pool);\r\nBUG_ON(!evt_struct);\r\ninit_event_struct(evt_struct, login_rsp,\r\nVIOSRP_SRP_FORMAT, login_timeout);\r\nlogin = &evt_struct->iu.srp.login_req;\r\nmemset(login, 0, sizeof(*login));\r\nlogin->opcode = SRP_LOGIN_REQ;\r\nlogin->req_it_iu_len = sizeof(union srp_iu);\r\nlogin->req_buf_fmt = SRP_BUF_FORMAT_DIRECT | SRP_BUF_FORMAT_INDIRECT;\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\natomic_set(&hostdata->request_limit, 0);\r\nrc = ibmvscsi_send_srp_event(evt_struct, hostdata, login_timeout * 2);\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\ndev_info(hostdata->dev, "sent SRP login\n");\r\nreturn rc;\r\n}\r\nstatic void capabilities_rsp(struct srp_event_struct *evt_struct)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\r\nif (evt_struct->xfer_iu->mad.capabilities.common.status) {\r\ndev_err(hostdata->dev, "error 0x%X getting capabilities info\n",\r\nevt_struct->xfer_iu->mad.capabilities.common.status);\r\n} else {\r\nif (hostdata->caps.migration.common.server_support != SERVER_SUPPORTS_CAP)\r\ndev_info(hostdata->dev, "Partition migration not supported\n");\r\nif (client_reserve) {\r\nif (hostdata->caps.reserve.common.server_support ==\r\nSERVER_SUPPORTS_CAP)\r\ndev_info(hostdata->dev, "Client reserve enabled\n");\r\nelse\r\ndev_info(hostdata->dev, "Client reserve not supported\n");\r\n}\r\n}\r\nsend_srp_login(hostdata);\r\n}\r\nstatic void send_mad_capabilities(struct ibmvscsi_host_data *hostdata)\r\n{\r\nstruct viosrp_capabilities *req;\r\nstruct srp_event_struct *evt_struct;\r\nunsigned long flags;\r\nstruct device_node *of_node = hostdata->dev->of_node;\r\nconst char *location;\r\nevt_struct = get_event_struct(&hostdata->pool);\r\nBUG_ON(!evt_struct);\r\ninit_event_struct(evt_struct, capabilities_rsp,\r\nVIOSRP_MAD_FORMAT, info_timeout);\r\nreq = &evt_struct->iu.mad.capabilities;\r\nmemset(req, 0, sizeof(*req));\r\nhostdata->caps.flags = CAP_LIST_SUPPORTED;\r\nif (hostdata->client_migrated)\r\nhostdata->caps.flags |= CLIENT_MIGRATED;\r\nstrncpy(hostdata->caps.name, dev_name(&hostdata->host->shost_gendev),\r\nsizeof(hostdata->caps.name));\r\nhostdata->caps.name[sizeof(hostdata->caps.name) - 1] = '\0';\r\nlocation = of_get_property(of_node, "ibm,loc-code", NULL);\r\nlocation = location ? location : dev_name(hostdata->dev);\r\nstrncpy(hostdata->caps.loc, location, sizeof(hostdata->caps.loc));\r\nhostdata->caps.loc[sizeof(hostdata->caps.loc) - 1] = '\0';\r\nreq->common.type = VIOSRP_CAPABILITIES_TYPE;\r\nreq->buffer = hostdata->caps_addr;\r\nhostdata->caps.migration.common.cap_type = MIGRATION_CAPABILITIES;\r\nhostdata->caps.migration.common.length = sizeof(hostdata->caps.migration);\r\nhostdata->caps.migration.common.server_support = SERVER_SUPPORTS_CAP;\r\nhostdata->caps.migration.ecl = 1;\r\nif (client_reserve) {\r\nhostdata->caps.reserve.common.cap_type = RESERVATION_CAPABILITIES;\r\nhostdata->caps.reserve.common.length = sizeof(hostdata->caps.reserve);\r\nhostdata->caps.reserve.common.server_support = SERVER_SUPPORTS_CAP;\r\nhostdata->caps.reserve.type = CLIENT_RESERVE_SCSI_2;\r\nreq->common.length = sizeof(hostdata->caps);\r\n} else\r\nreq->common.length = sizeof(hostdata->caps) - sizeof(hostdata->caps.reserve);\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\nif (ibmvscsi_send_srp_event(evt_struct, hostdata, info_timeout * 2))\r\ndev_err(hostdata->dev, "couldn't send CAPABILITIES_REQ!\n");\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\n}\r\nstatic void fast_fail_rsp(struct srp_event_struct *evt_struct)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\r\nu8 status = evt_struct->xfer_iu->mad.fast_fail.common.status;\r\nif (status == VIOSRP_MAD_NOT_SUPPORTED)\r\ndev_err(hostdata->dev, "fast_fail not supported in server\n");\r\nelse if (status == VIOSRP_MAD_FAILED)\r\ndev_err(hostdata->dev, "fast_fail request failed\n");\r\nelse if (status != VIOSRP_MAD_SUCCESS)\r\ndev_err(hostdata->dev, "error 0x%X enabling fast_fail\n", status);\r\nsend_mad_capabilities(hostdata);\r\n}\r\nstatic int enable_fast_fail(struct ibmvscsi_host_data *hostdata)\r\n{\r\nint rc;\r\nunsigned long flags;\r\nstruct viosrp_fast_fail *fast_fail_mad;\r\nstruct srp_event_struct *evt_struct;\r\nif (!fast_fail) {\r\nsend_mad_capabilities(hostdata);\r\nreturn 0;\r\n}\r\nevt_struct = get_event_struct(&hostdata->pool);\r\nBUG_ON(!evt_struct);\r\ninit_event_struct(evt_struct, fast_fail_rsp, VIOSRP_MAD_FORMAT, info_timeout);\r\nfast_fail_mad = &evt_struct->iu.mad.fast_fail;\r\nmemset(fast_fail_mad, 0, sizeof(*fast_fail_mad));\r\nfast_fail_mad->common.type = VIOSRP_ENABLE_FAST_FAIL;\r\nfast_fail_mad->common.length = sizeof(*fast_fail_mad);\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\nrc = ibmvscsi_send_srp_event(evt_struct, hostdata, info_timeout * 2);\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nreturn rc;\r\n}\r\nstatic void adapter_info_rsp(struct srp_event_struct *evt_struct)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = evt_struct->hostdata;\r\nif (evt_struct->xfer_iu->mad.adapter_info.common.status) {\r\ndev_err(hostdata->dev, "error %d getting adapter info\n",\r\nevt_struct->xfer_iu->mad.adapter_info.common.status);\r\n} else {\r\ndev_info(hostdata->dev, "host srp version: %s, "\r\n"host partition %s (%d), OS %d, max io %u\n",\r\nhostdata->madapter_info.srp_version,\r\nhostdata->madapter_info.partition_name,\r\nhostdata->madapter_info.partition_number,\r\nhostdata->madapter_info.os_type,\r\nhostdata->madapter_info.port_max_txu[0]);\r\nif (hostdata->madapter_info.port_max_txu[0])\r\nhostdata->host->max_sectors =\r\nhostdata->madapter_info.port_max_txu[0] >> 9;\r\nif (hostdata->madapter_info.os_type == 3 &&\r\nstrcmp(hostdata->madapter_info.srp_version, "1.6a") <= 0) {\r\ndev_err(hostdata->dev, "host (Ver. %s) doesn't support large transfers\n",\r\nhostdata->madapter_info.srp_version);\r\ndev_err(hostdata->dev, "limiting scatterlists to %d\n",\r\nMAX_INDIRECT_BUFS);\r\nhostdata->host->sg_tablesize = MAX_INDIRECT_BUFS;\r\n}\r\nif (hostdata->madapter_info.os_type == 3) {\r\nenable_fast_fail(hostdata);\r\nreturn;\r\n}\r\n}\r\nsend_srp_login(hostdata);\r\n}\r\nstatic void send_mad_adapter_info(struct ibmvscsi_host_data *hostdata)\r\n{\r\nstruct viosrp_adapter_info *req;\r\nstruct srp_event_struct *evt_struct;\r\nunsigned long flags;\r\nevt_struct = get_event_struct(&hostdata->pool);\r\nBUG_ON(!evt_struct);\r\ninit_event_struct(evt_struct,\r\nadapter_info_rsp,\r\nVIOSRP_MAD_FORMAT,\r\ninfo_timeout);\r\nreq = &evt_struct->iu.mad.adapter_info;\r\nmemset(req, 0x00, sizeof(*req));\r\nreq->common.type = VIOSRP_ADAPTER_INFO_TYPE;\r\nreq->common.length = sizeof(hostdata->madapter_info);\r\nreq->buffer = hostdata->adapter_info_addr;\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\nif (ibmvscsi_send_srp_event(evt_struct, hostdata, info_timeout * 2))\r\ndev_err(hostdata->dev, "couldn't send ADAPTER_INFO_REQ!\n");\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\n}\r\nstatic void init_adapter(struct ibmvscsi_host_data *hostdata)\r\n{\r\nsend_mad_adapter_info(hostdata);\r\n}\r\nstatic void sync_completion(struct srp_event_struct *evt_struct)\r\n{\r\nif (evt_struct->sync_srp)\r\n*evt_struct->sync_srp = *evt_struct->xfer_iu;\r\ncomplete(&evt_struct->comp);\r\n}\r\nstatic int ibmvscsi_eh_abort_handler(struct scsi_cmnd *cmd)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(cmd->device->host);\r\nstruct srp_tsk_mgmt *tsk_mgmt;\r\nstruct srp_event_struct *evt;\r\nstruct srp_event_struct *tmp_evt, *found_evt;\r\nunion viosrp_iu srp_rsp;\r\nint rsp_rc;\r\nunsigned long flags;\r\nu16 lun = lun_from_dev(cmd->device);\r\nunsigned long wait_switch = 0;\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\nwait_switch = jiffies + (init_timeout * HZ);\r\ndo {\r\nfound_evt = NULL;\r\nlist_for_each_entry(tmp_evt, &hostdata->sent, list) {\r\nif (tmp_evt->cmnd == cmd) {\r\nfound_evt = tmp_evt;\r\nbreak;\r\n}\r\n}\r\nif (!found_evt) {\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nreturn SUCCESS;\r\n}\r\nevt = get_event_struct(&hostdata->pool);\r\nif (evt == NULL) {\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nsdev_printk(KERN_ERR, cmd->device,\r\n"failed to allocate abort event\n");\r\nreturn FAILED;\r\n}\r\ninit_event_struct(evt,\r\nsync_completion,\r\nVIOSRP_SRP_FORMAT,\r\nabort_timeout);\r\ntsk_mgmt = &evt->iu.srp.tsk_mgmt;\r\nmemset(tsk_mgmt, 0x00, sizeof(*tsk_mgmt));\r\ntsk_mgmt->opcode = SRP_TSK_MGMT;\r\ntsk_mgmt->lun = ((u64) lun) << 48;\r\ntsk_mgmt->tsk_mgmt_func = SRP_TSK_ABORT_TASK;\r\ntsk_mgmt->task_tag = (u64) found_evt;\r\nevt->sync_srp = &srp_rsp;\r\ninit_completion(&evt->comp);\r\nrsp_rc = ibmvscsi_send_srp_event(evt, hostdata, abort_timeout * 2);\r\nif (rsp_rc != SCSI_MLQUEUE_HOST_BUSY)\r\nbreak;\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nmsleep(10);\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\n} while (time_before(jiffies, wait_switch));\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nif (rsp_rc != 0) {\r\nsdev_printk(KERN_ERR, cmd->device,\r\n"failed to send abort() event. rc=%d\n", rsp_rc);\r\nreturn FAILED;\r\n}\r\nsdev_printk(KERN_INFO, cmd->device,\r\n"aborting command. lun 0x%llx, tag 0x%llx\n",\r\n(((u64) lun) << 48), (u64) found_evt);\r\nwait_for_completion(&evt->comp);\r\nif (unlikely(srp_rsp.srp.rsp.opcode != SRP_RSP)) {\r\nif (printk_ratelimit())\r\nsdev_printk(KERN_WARNING, cmd->device, "abort bad SRP RSP type %d\n",\r\nsrp_rsp.srp.rsp.opcode);\r\nreturn FAILED;\r\n}\r\nif (srp_rsp.srp.rsp.flags & SRP_RSP_FLAG_RSPVALID)\r\nrsp_rc = *((int *)srp_rsp.srp.rsp.data);\r\nelse\r\nrsp_rc = srp_rsp.srp.rsp.status;\r\nif (rsp_rc) {\r\nif (printk_ratelimit())\r\nsdev_printk(KERN_WARNING, cmd->device,\r\n"abort code %d for task tag 0x%llx\n",\r\nrsp_rc, tsk_mgmt->task_tag);\r\nreturn FAILED;\r\n}\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\nfound_evt = NULL;\r\nlist_for_each_entry(tmp_evt, &hostdata->sent, list) {\r\nif (tmp_evt->cmnd == cmd) {\r\nfound_evt = tmp_evt;\r\nbreak;\r\n}\r\n}\r\nif (found_evt == NULL) {\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nsdev_printk(KERN_INFO, cmd->device, "aborted task tag 0x%llx completed\n",\r\ntsk_mgmt->task_tag);\r\nreturn SUCCESS;\r\n}\r\nsdev_printk(KERN_INFO, cmd->device, "successfully aborted task tag 0x%llx\n",\r\ntsk_mgmt->task_tag);\r\ncmd->result = (DID_ABORT << 16);\r\nlist_del(&found_evt->list);\r\nunmap_cmd_data(&found_evt->iu.srp.cmd, found_evt,\r\nfound_evt->hostdata->dev);\r\nfree_event_struct(&found_evt->hostdata->pool, found_evt);\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\natomic_inc(&hostdata->request_limit);\r\nreturn SUCCESS;\r\n}\r\nstatic int ibmvscsi_eh_device_reset_handler(struct scsi_cmnd *cmd)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(cmd->device->host);\r\nstruct srp_tsk_mgmt *tsk_mgmt;\r\nstruct srp_event_struct *evt;\r\nstruct srp_event_struct *tmp_evt, *pos;\r\nunion viosrp_iu srp_rsp;\r\nint rsp_rc;\r\nunsigned long flags;\r\nu16 lun = lun_from_dev(cmd->device);\r\nunsigned long wait_switch = 0;\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\nwait_switch = jiffies + (init_timeout * HZ);\r\ndo {\r\nevt = get_event_struct(&hostdata->pool);\r\nif (evt == NULL) {\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nsdev_printk(KERN_ERR, cmd->device,\r\n"failed to allocate reset event\n");\r\nreturn FAILED;\r\n}\r\ninit_event_struct(evt,\r\nsync_completion,\r\nVIOSRP_SRP_FORMAT,\r\nreset_timeout);\r\ntsk_mgmt = &evt->iu.srp.tsk_mgmt;\r\nmemset(tsk_mgmt, 0x00, sizeof(*tsk_mgmt));\r\ntsk_mgmt->opcode = SRP_TSK_MGMT;\r\ntsk_mgmt->lun = ((u64) lun) << 48;\r\ntsk_mgmt->tsk_mgmt_func = SRP_TSK_LUN_RESET;\r\nevt->sync_srp = &srp_rsp;\r\ninit_completion(&evt->comp);\r\nrsp_rc = ibmvscsi_send_srp_event(evt, hostdata, reset_timeout * 2);\r\nif (rsp_rc != SCSI_MLQUEUE_HOST_BUSY)\r\nbreak;\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nmsleep(10);\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\n} while (time_before(jiffies, wait_switch));\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nif (rsp_rc != 0) {\r\nsdev_printk(KERN_ERR, cmd->device,\r\n"failed to send reset event. rc=%d\n", rsp_rc);\r\nreturn FAILED;\r\n}\r\nsdev_printk(KERN_INFO, cmd->device, "resetting device. lun 0x%llx\n",\r\n(((u64) lun) << 48));\r\nwait_for_completion(&evt->comp);\r\nif (unlikely(srp_rsp.srp.rsp.opcode != SRP_RSP)) {\r\nif (printk_ratelimit())\r\nsdev_printk(KERN_WARNING, cmd->device, "reset bad SRP RSP type %d\n",\r\nsrp_rsp.srp.rsp.opcode);\r\nreturn FAILED;\r\n}\r\nif (srp_rsp.srp.rsp.flags & SRP_RSP_FLAG_RSPVALID)\r\nrsp_rc = *((int *)srp_rsp.srp.rsp.data);\r\nelse\r\nrsp_rc = srp_rsp.srp.rsp.status;\r\nif (rsp_rc) {\r\nif (printk_ratelimit())\r\nsdev_printk(KERN_WARNING, cmd->device,\r\n"reset code %d for task tag 0x%llx\n",\r\nrsp_rc, tsk_mgmt->task_tag);\r\nreturn FAILED;\r\n}\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\nlist_for_each_entry_safe(tmp_evt, pos, &hostdata->sent, list) {\r\nif ((tmp_evt->cmnd) && (tmp_evt->cmnd->device == cmd->device)) {\r\nif (tmp_evt->cmnd)\r\ntmp_evt->cmnd->result = (DID_RESET << 16);\r\nlist_del(&tmp_evt->list);\r\nunmap_cmd_data(&tmp_evt->iu.srp.cmd, tmp_evt,\r\ntmp_evt->hostdata->dev);\r\nfree_event_struct(&tmp_evt->hostdata->pool,\r\ntmp_evt);\r\natomic_inc(&hostdata->request_limit);\r\nif (tmp_evt->cmnd_done)\r\ntmp_evt->cmnd_done(tmp_evt->cmnd);\r\nelse if (tmp_evt->done)\r\ntmp_evt->done(tmp_evt);\r\n}\r\n}\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nreturn SUCCESS;\r\n}\r\nstatic int ibmvscsi_eh_host_reset_handler(struct scsi_cmnd *cmd)\r\n{\r\nunsigned long wait_switch = 0;\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(cmd->device->host);\r\ndev_err(hostdata->dev, "Resetting connection due to error recovery\n");\r\nibmvscsi_reset_host(hostdata);\r\nfor (wait_switch = jiffies + (init_timeout * HZ);\r\ntime_before(jiffies, wait_switch) &&\r\natomic_read(&hostdata->request_limit) < 2;) {\r\nmsleep(10);\r\n}\r\nif (atomic_read(&hostdata->request_limit) <= 0)\r\nreturn FAILED;\r\nreturn SUCCESS;\r\n}\r\nstatic void ibmvscsi_handle_crq(struct viosrp_crq *crq,\r\nstruct ibmvscsi_host_data *hostdata)\r\n{\r\nlong rc;\r\nunsigned long flags;\r\nstruct srp_event_struct *evt_struct =\r\n(struct srp_event_struct *)crq->IU_data_ptr;\r\nswitch (crq->valid) {\r\ncase 0xC0:\r\nswitch (crq->format) {\r\ncase 0x01:\r\ndev_info(hostdata->dev, "partner initialized\n");\r\nrc = ibmvscsi_send_crq(hostdata, 0xC002000000000000LL, 0);\r\nif (rc == 0) {\r\ninit_adapter(hostdata);\r\n} else {\r\ndev_err(hostdata->dev, "Unable to send init rsp. rc=%ld\n", rc);\r\n}\r\nbreak;\r\ncase 0x02:\r\ndev_info(hostdata->dev, "partner initialization complete\n");\r\ninit_adapter(hostdata);\r\nbreak;\r\ndefault:\r\ndev_err(hostdata->dev, "unknown crq message type: %d\n", crq->format);\r\n}\r\nreturn;\r\ncase 0xFF:\r\nscsi_block_requests(hostdata->host);\r\natomic_set(&hostdata->request_limit, 0);\r\nif (crq->format == 0x06) {\r\ndev_info(hostdata->dev, "Re-enabling adapter!\n");\r\nhostdata->client_migrated = 1;\r\nhostdata->reenable_crq = 1;\r\npurge_requests(hostdata, DID_REQUEUE);\r\nwake_up(&hostdata->work_wait_q);\r\n} else {\r\ndev_err(hostdata->dev, "Virtual adapter failed rc %d!\n",\r\ncrq->format);\r\nibmvscsi_reset_host(hostdata);\r\n}\r\nreturn;\r\ncase 0x80:\r\nbreak;\r\ndefault:\r\ndev_err(hostdata->dev, "got an invalid message type 0x%02x\n",\r\ncrq->valid);\r\nreturn;\r\n}\r\nif (!valid_event_struct(&hostdata->pool, evt_struct)) {\r\ndev_err(hostdata->dev, "returned correlation_token 0x%p is invalid!\n",\r\n(void *)crq->IU_data_ptr);\r\nreturn;\r\n}\r\nif (atomic_read(&evt_struct->free)) {\r\ndev_err(hostdata->dev, "received duplicate correlation_token 0x%p!\n",\r\n(void *)crq->IU_data_ptr);\r\nreturn;\r\n}\r\nif (crq->format == VIOSRP_SRP_FORMAT)\r\natomic_add(evt_struct->xfer_iu->srp.rsp.req_lim_delta,\r\n&hostdata->request_limit);\r\ndel_timer(&evt_struct->timer);\r\nif ((crq->status != VIOSRP_OK && crq->status != VIOSRP_OK2) && evt_struct->cmnd)\r\nevt_struct->cmnd->result = DID_ERROR << 16;\r\nif (evt_struct->done)\r\nevt_struct->done(evt_struct);\r\nelse\r\ndev_err(hostdata->dev, "returned done() is NULL; not running it!\n");\r\nspin_lock_irqsave(evt_struct->hostdata->host->host_lock, flags);\r\nlist_del(&evt_struct->list);\r\nfree_event_struct(&evt_struct->hostdata->pool, evt_struct);\r\nspin_unlock_irqrestore(evt_struct->hostdata->host->host_lock, flags);\r\n}\r\nstatic int ibmvscsi_do_host_config(struct ibmvscsi_host_data *hostdata,\r\nunsigned char *buffer, int length)\r\n{\r\nstruct viosrp_host_config *host_config;\r\nstruct srp_event_struct *evt_struct;\r\nunsigned long flags;\r\ndma_addr_t addr;\r\nint rc;\r\nevt_struct = get_event_struct(&hostdata->pool);\r\nif (!evt_struct) {\r\ndev_err(hostdata->dev, "couldn't allocate event for HOST_CONFIG!\n");\r\nreturn -1;\r\n}\r\ninit_event_struct(evt_struct,\r\nsync_completion,\r\nVIOSRP_MAD_FORMAT,\r\ninfo_timeout);\r\nhost_config = &evt_struct->iu.mad.host_config;\r\nlength = min(0xffff, length);\r\nmemset(host_config, 0x00, sizeof(*host_config));\r\nhost_config->common.type = VIOSRP_HOST_CONFIG_TYPE;\r\nhost_config->common.length = length;\r\nhost_config->buffer = addr = dma_map_single(hostdata->dev, buffer,\r\nlength,\r\nDMA_BIDIRECTIONAL);\r\nif (dma_mapping_error(hostdata->dev, host_config->buffer)) {\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\ndev_err(hostdata->dev,\r\n"dma_mapping error getting host config\n");\r\nfree_event_struct(&hostdata->pool, evt_struct);\r\nreturn -1;\r\n}\r\ninit_completion(&evt_struct->comp);\r\nspin_lock_irqsave(hostdata->host->host_lock, flags);\r\nrc = ibmvscsi_send_srp_event(evt_struct, hostdata, info_timeout * 2);\r\nspin_unlock_irqrestore(hostdata->host->host_lock, flags);\r\nif (rc == 0)\r\nwait_for_completion(&evt_struct->comp);\r\ndma_unmap_single(hostdata->dev, addr, length, DMA_BIDIRECTIONAL);\r\nreturn rc;\r\n}\r\nstatic int ibmvscsi_slave_configure(struct scsi_device *sdev)\r\n{\r\nstruct Scsi_Host *shost = sdev->host;\r\nunsigned long lock_flags = 0;\r\nspin_lock_irqsave(shost->host_lock, lock_flags);\r\nif (sdev->type == TYPE_DISK) {\r\nsdev->allow_restart = 1;\r\nblk_queue_rq_timeout(sdev->request_queue, 120 * HZ);\r\n}\r\nspin_unlock_irqrestore(shost->host_lock, lock_flags);\r\nscsi_adjust_queue_depth(sdev, 0, shost->cmd_per_lun);\r\nreturn 0;\r\n}\r\nstatic int ibmvscsi_change_queue_depth(struct scsi_device *sdev, int qdepth,\r\nint reason)\r\n{\r\nif (reason != SCSI_QDEPTH_DEFAULT)\r\nreturn -EOPNOTSUPP;\r\nif (qdepth > IBMVSCSI_MAX_CMDS_PER_LUN)\r\nqdepth = IBMVSCSI_MAX_CMDS_PER_LUN;\r\nscsi_adjust_queue_depth(sdev, 0, qdepth);\r\nreturn sdev->queue_depth;\r\n}\r\nstatic ssize_t show_host_vhost_loc(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\r\nint len;\r\nlen = snprintf(buf, sizeof(hostdata->caps.loc), "%s\n",\r\nhostdata->caps.loc);\r\nreturn len;\r\n}\r\nstatic ssize_t show_host_vhost_name(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\r\nint len;\r\nlen = snprintf(buf, sizeof(hostdata->caps.name), "%s\n",\r\nhostdata->caps.name);\r\nreturn len;\r\n}\r\nstatic ssize_t show_host_srp_version(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\r\nint len;\r\nlen = snprintf(buf, PAGE_SIZE, "%s\n",\r\nhostdata->madapter_info.srp_version);\r\nreturn len;\r\n}\r\nstatic ssize_t show_host_partition_name(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\r\nint len;\r\nlen = snprintf(buf, PAGE_SIZE, "%s\n",\r\nhostdata->madapter_info.partition_name);\r\nreturn len;\r\n}\r\nstatic ssize_t show_host_partition_number(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\r\nint len;\r\nlen = snprintf(buf, PAGE_SIZE, "%d\n",\r\nhostdata->madapter_info.partition_number);\r\nreturn len;\r\n}\r\nstatic ssize_t show_host_mad_version(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\r\nint len;\r\nlen = snprintf(buf, PAGE_SIZE, "%d\n",\r\nhostdata->madapter_info.mad_version);\r\nreturn len;\r\n}\r\nstatic ssize_t show_host_os_type(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\r\nint len;\r\nlen = snprintf(buf, PAGE_SIZE, "%d\n", hostdata->madapter_info.os_type);\r\nreturn len;\r\n}\r\nstatic ssize_t show_host_config(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\nstruct ibmvscsi_host_data *hostdata = shost_priv(shost);\r\nif (ibmvscsi_do_host_config(hostdata, buf, PAGE_SIZE) == 0)\r\nreturn strlen(buf);\r\nelse\r\nreturn 0;\r\n}\r\nstatic unsigned long ibmvscsi_get_desired_dma(struct vio_dev *vdev)\r\n{\r\nunsigned long desired_io = max_events * sizeof(union viosrp_iu);\r\ndesired_io += (IBMVSCSI_MAX_SECTORS_DEFAULT * 512 *\r\nIBMVSCSI_CMDS_PER_LUN_DEFAULT);\r\nreturn desired_io;\r\n}\r\nstatic void ibmvscsi_do_work(struct ibmvscsi_host_data *hostdata)\r\n{\r\nint rc;\r\nchar *action = "reset";\r\nif (hostdata->reset_crq) {\r\nsmp_rmb();\r\nhostdata->reset_crq = 0;\r\nrc = ibmvscsi_reset_crq_queue(&hostdata->queue, hostdata);\r\nif (!rc)\r\nrc = ibmvscsi_send_crq(hostdata, 0xC001000000000000LL, 0);\r\nvio_enable_interrupts(to_vio_dev(hostdata->dev));\r\n} else if (hostdata->reenable_crq) {\r\nsmp_rmb();\r\naction = "enable";\r\nrc = ibmvscsi_reenable_crq_queue(&hostdata->queue, hostdata);\r\nhostdata->reenable_crq = 0;\r\nif (!rc)\r\nrc = ibmvscsi_send_crq(hostdata, 0xC001000000000000LL, 0);\r\n} else\r\nreturn;\r\nif (rc) {\r\natomic_set(&hostdata->request_limit, -1);\r\ndev_err(hostdata->dev, "error after %s\n", action);\r\n}\r\nscsi_unblock_requests(hostdata->host);\r\n}\r\nstatic int ibmvscsi_work_to_do(struct ibmvscsi_host_data *hostdata)\r\n{\r\nif (kthread_should_stop())\r\nreturn 1;\r\nelse if (hostdata->reset_crq) {\r\nsmp_rmb();\r\nreturn 1;\r\n} else if (hostdata->reenable_crq) {\r\nsmp_rmb();\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ibmvscsi_work(void *data)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = data;\r\nint rc;\r\nset_user_nice(current, -20);\r\nwhile (1) {\r\nrc = wait_event_interruptible(hostdata->work_wait_q,\r\nibmvscsi_work_to_do(hostdata));\r\nBUG_ON(rc);\r\nif (kthread_should_stop())\r\nbreak;\r\nibmvscsi_do_work(hostdata);\r\n}\r\nreturn 0;\r\n}\r\nstatic int ibmvscsi_probe(struct vio_dev *vdev, const struct vio_device_id *id)\r\n{\r\nstruct ibmvscsi_host_data *hostdata;\r\nstruct Scsi_Host *host;\r\nstruct device *dev = &vdev->dev;\r\nstruct srp_rport_identifiers ids;\r\nstruct srp_rport *rport;\r\nunsigned long wait_switch = 0;\r\nint rc;\r\ndev_set_drvdata(&vdev->dev, NULL);\r\nhost = scsi_host_alloc(&driver_template, sizeof(*hostdata));\r\nif (!host) {\r\ndev_err(&vdev->dev, "couldn't allocate host data\n");\r\ngoto scsi_host_alloc_failed;\r\n}\r\nhost->transportt = ibmvscsi_transport_template;\r\nhostdata = shost_priv(host);\r\nmemset(hostdata, 0x00, sizeof(*hostdata));\r\nINIT_LIST_HEAD(&hostdata->sent);\r\ninit_waitqueue_head(&hostdata->work_wait_q);\r\nhostdata->host = host;\r\nhostdata->dev = dev;\r\natomic_set(&hostdata->request_limit, -1);\r\nhostdata->host->max_sectors = IBMVSCSI_MAX_SECTORS_DEFAULT;\r\nif (map_persist_bufs(hostdata)) {\r\ndev_err(&vdev->dev, "couldn't map persistent buffers\n");\r\ngoto persist_bufs_failed;\r\n}\r\nhostdata->work_thread = kthread_run(ibmvscsi_work, hostdata, "%s_%d",\r\n"ibmvscsi", host->host_no);\r\nif (IS_ERR(hostdata->work_thread)) {\r\ndev_err(&vdev->dev, "couldn't initialize kthread. rc=%ld\n",\r\nPTR_ERR(hostdata->work_thread));\r\ngoto init_crq_failed;\r\n}\r\nrc = ibmvscsi_init_crq_queue(&hostdata->queue, hostdata, max_events);\r\nif (rc != 0 && rc != H_RESOURCE) {\r\ndev_err(&vdev->dev, "couldn't initialize crq. rc=%d\n", rc);\r\ngoto kill_kthread;\r\n}\r\nif (initialize_event_pool(&hostdata->pool, max_events, hostdata) != 0) {\r\ndev_err(&vdev->dev, "couldn't initialize event pool\n");\r\ngoto init_pool_failed;\r\n}\r\nhost->max_lun = 8;\r\nhost->max_id = max_id;\r\nhost->max_channel = max_channel;\r\nhost->max_cmd_len = 16;\r\nif (scsi_add_host(hostdata->host, hostdata->dev))\r\ngoto add_host_failed;\r\nmemcpy(ids.port_id, hostdata->madapter_info.partition_name,\r\nsizeof(ids.port_id));\r\nids.roles = SRP_RPORT_ROLE_TARGET;\r\nrport = srp_rport_add(host, &ids);\r\nif (IS_ERR(rport))\r\ngoto add_srp_port_failed;\r\nif (ibmvscsi_send_crq(hostdata, 0xC001000000000000LL, 0) == 0\r\n|| rc == H_RESOURCE) {\r\nfor (wait_switch = jiffies + (init_timeout * HZ);\r\ntime_before(jiffies, wait_switch) &&\r\natomic_read(&hostdata->request_limit) < 2;) {\r\nmsleep(10);\r\n}\r\nif (atomic_read(&hostdata->request_limit) > 0)\r\nscsi_scan_host(host);\r\n}\r\ndev_set_drvdata(&vdev->dev, hostdata);\r\nreturn 0;\r\nadd_srp_port_failed:\r\nscsi_remove_host(hostdata->host);\r\nadd_host_failed:\r\nrelease_event_pool(&hostdata->pool, hostdata);\r\ninit_pool_failed:\r\nibmvscsi_release_crq_queue(&hostdata->queue, hostdata, max_events);\r\nkill_kthread:\r\nkthread_stop(hostdata->work_thread);\r\ninit_crq_failed:\r\nunmap_persist_bufs(hostdata);\r\npersist_bufs_failed:\r\nscsi_host_put(host);\r\nscsi_host_alloc_failed:\r\nreturn -1;\r\n}\r\nstatic int ibmvscsi_remove(struct vio_dev *vdev)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = dev_get_drvdata(&vdev->dev);\r\nunmap_persist_bufs(hostdata);\r\nrelease_event_pool(&hostdata->pool, hostdata);\r\nibmvscsi_release_crq_queue(&hostdata->queue, hostdata,\r\nmax_events);\r\nkthread_stop(hostdata->work_thread);\r\nsrp_remove_host(hostdata->host);\r\nscsi_remove_host(hostdata->host);\r\nscsi_host_put(hostdata->host);\r\nreturn 0;\r\n}\r\nstatic int ibmvscsi_resume(struct device *dev)\r\n{\r\nstruct ibmvscsi_host_data *hostdata = dev_get_drvdata(dev);\r\nvio_disable_interrupts(to_vio_dev(hostdata->dev));\r\ntasklet_schedule(&hostdata->srp_task);\r\nreturn 0;\r\n}\r\nint __init ibmvscsi_module_init(void)\r\n{\r\nint ret;\r\ndriver_template.can_queue = max_requests;\r\nmax_events = max_requests + 2;\r\nif (!firmware_has_feature(FW_FEATURE_VIO))\r\nreturn -ENODEV;\r\nibmvscsi_transport_template =\r\nsrp_attach_transport(&ibmvscsi_transport_functions);\r\nif (!ibmvscsi_transport_template)\r\nreturn -ENOMEM;\r\nret = vio_register_driver(&ibmvscsi_driver);\r\nif (ret)\r\nsrp_release_transport(ibmvscsi_transport_template);\r\nreturn ret;\r\n}\r\nvoid __exit ibmvscsi_module_exit(void)\r\n{\r\nvio_unregister_driver(&ibmvscsi_driver);\r\nsrp_release_transport(ibmvscsi_transport_template);\r\n}
