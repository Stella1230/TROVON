static u32 random_N(unsigned int N)\r\n{\r\nreturn reciprocal_divide(prandom_u32(), N);\r\n}\r\nstatic unsigned int choke_len(const struct choke_sched_data *q)\r\n{\r\nreturn (q->tail - q->head) & q->tab_mask;\r\n}\r\nstatic int use_ecn(const struct choke_sched_data *q)\r\n{\r\nreturn q->flags & TC_RED_ECN;\r\n}\r\nstatic int use_harddrop(const struct choke_sched_data *q)\r\n{\r\nreturn q->flags & TC_RED_HARDDROP;\r\n}\r\nstatic void choke_zap_head_holes(struct choke_sched_data *q)\r\n{\r\ndo {\r\nq->head = (q->head + 1) & q->tab_mask;\r\nif (q->head == q->tail)\r\nbreak;\r\n} while (q->tab[q->head] == NULL);\r\n}\r\nstatic void choke_zap_tail_holes(struct choke_sched_data *q)\r\n{\r\ndo {\r\nq->tail = (q->tail - 1) & q->tab_mask;\r\nif (q->head == q->tail)\r\nbreak;\r\n} while (q->tab[q->tail] == NULL);\r\n}\r\nstatic void choke_drop_by_idx(struct Qdisc *sch, unsigned int idx)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb = q->tab[idx];\r\nq->tab[idx] = NULL;\r\nif (idx == q->head)\r\nchoke_zap_head_holes(q);\r\nif (idx == q->tail)\r\nchoke_zap_tail_holes(q);\r\nsch->qstats.backlog -= qdisc_pkt_len(skb);\r\nqdisc_drop(skb, sch);\r\nqdisc_tree_decrease_qlen(sch, 1);\r\n--sch->q.qlen;\r\n}\r\nstatic inline struct choke_skb_cb *choke_skb_cb(const struct sk_buff *skb)\r\n{\r\nqdisc_cb_private_validate(skb, sizeof(struct choke_skb_cb));\r\nreturn (struct choke_skb_cb *)qdisc_skb_cb(skb)->data;\r\n}\r\nstatic inline void choke_set_classid(struct sk_buff *skb, u16 classid)\r\n{\r\nchoke_skb_cb(skb)->classid = classid;\r\n}\r\nstatic u16 choke_get_classid(const struct sk_buff *skb)\r\n{\r\nreturn choke_skb_cb(skb)->classid;\r\n}\r\nstatic bool choke_match_flow(struct sk_buff *skb1,\r\nstruct sk_buff *skb2)\r\n{\r\nif (skb1->protocol != skb2->protocol)\r\nreturn false;\r\nif (!choke_skb_cb(skb1)->keys_valid) {\r\nchoke_skb_cb(skb1)->keys_valid = 1;\r\nskb_flow_dissect(skb1, &choke_skb_cb(skb1)->keys);\r\n}\r\nif (!choke_skb_cb(skb2)->keys_valid) {\r\nchoke_skb_cb(skb2)->keys_valid = 1;\r\nskb_flow_dissect(skb2, &choke_skb_cb(skb2)->keys);\r\n}\r\nreturn !memcmp(&choke_skb_cb(skb1)->keys,\r\n&choke_skb_cb(skb2)->keys,\r\nsizeof(struct flow_keys));\r\n}\r\nstatic bool choke_classify(struct sk_buff *skb,\r\nstruct Qdisc *sch, int *qerr)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nstruct tcf_result res;\r\nint result;\r\nresult = tc_classify(skb, q->filter_list, &res);\r\nif (result >= 0) {\r\n#ifdef CONFIG_NET_CLS_ACT\r\nswitch (result) {\r\ncase TC_ACT_STOLEN:\r\ncase TC_ACT_QUEUED:\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\r\ncase TC_ACT_SHOT:\r\nreturn false;\r\n}\r\n#endif\r\nchoke_set_classid(skb, TC_H_MIN(res.classid));\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic struct sk_buff *choke_peek_random(const struct choke_sched_data *q,\r\nunsigned int *pidx)\r\n{\r\nstruct sk_buff *skb;\r\nint retrys = 3;\r\ndo {\r\n*pidx = (q->head + random_N(choke_len(q))) & q->tab_mask;\r\nskb = q->tab[*pidx];\r\nif (skb)\r\nreturn skb;\r\n} while (--retrys > 0);\r\nreturn q->tab[*pidx = q->head];\r\n}\r\nstatic bool choke_match_random(const struct choke_sched_data *q,\r\nstruct sk_buff *nskb,\r\nunsigned int *pidx)\r\n{\r\nstruct sk_buff *oskb;\r\nif (q->head == q->tail)\r\nreturn false;\r\noskb = choke_peek_random(q, pidx);\r\nif (q->filter_list)\r\nreturn choke_get_classid(nskb) == choke_get_classid(oskb);\r\nreturn choke_match_flow(oskb, nskb);\r\n}\r\nstatic int choke_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nconst struct red_parms *p = &q->parms;\r\nint ret = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\r\nif (q->filter_list) {\r\nif (!choke_classify(skb, sch, &ret))\r\ngoto other_drop;\r\n}\r\nchoke_skb_cb(skb)->keys_valid = 0;\r\nq->vars.qavg = red_calc_qavg(p, &q->vars, sch->q.qlen);\r\nif (red_is_idling(&q->vars))\r\nred_end_of_idle_period(&q->vars);\r\nif (q->vars.qavg <= p->qth_min)\r\nq->vars.qcount = -1;\r\nelse {\r\nunsigned int idx;\r\nif (choke_match_random(q, skb, &idx)) {\r\nq->stats.matched++;\r\nchoke_drop_by_idx(sch, idx);\r\ngoto congestion_drop;\r\n}\r\nif (q->vars.qavg > p->qth_max) {\r\nq->vars.qcount = -1;\r\nsch->qstats.overlimits++;\r\nif (use_harddrop(q) || !use_ecn(q) ||\r\n!INET_ECN_set_ce(skb)) {\r\nq->stats.forced_drop++;\r\ngoto congestion_drop;\r\n}\r\nq->stats.forced_mark++;\r\n} else if (++q->vars.qcount) {\r\nif (red_mark_probability(p, &q->vars, q->vars.qavg)) {\r\nq->vars.qcount = 0;\r\nq->vars.qR = red_random(p);\r\nsch->qstats.overlimits++;\r\nif (!use_ecn(q) || !INET_ECN_set_ce(skb)) {\r\nq->stats.prob_drop++;\r\ngoto congestion_drop;\r\n}\r\nq->stats.prob_mark++;\r\n}\r\n} else\r\nq->vars.qR = red_random(p);\r\n}\r\nif (sch->q.qlen < q->limit) {\r\nq->tab[q->tail] = skb;\r\nq->tail = (q->tail + 1) & q->tab_mask;\r\n++sch->q.qlen;\r\nsch->qstats.backlog += qdisc_pkt_len(skb);\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nq->stats.pdrop++;\r\nreturn qdisc_drop(skb, sch);\r\ncongestion_drop:\r\nqdisc_drop(skb, sch);\r\nreturn NET_XMIT_CN;\r\nother_drop:\r\nif (ret & __NET_XMIT_BYPASS)\r\nsch->qstats.drops++;\r\nkfree_skb(skb);\r\nreturn ret;\r\n}\r\nstatic struct sk_buff *choke_dequeue(struct Qdisc *sch)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nif (q->head == q->tail) {\r\nif (!red_is_idling(&q->vars))\r\nred_start_of_idle_period(&q->vars);\r\nreturn NULL;\r\n}\r\nskb = q->tab[q->head];\r\nq->tab[q->head] = NULL;\r\nchoke_zap_head_holes(q);\r\n--sch->q.qlen;\r\nsch->qstats.backlog -= qdisc_pkt_len(skb);\r\nqdisc_bstats_update(sch, skb);\r\nreturn skb;\r\n}\r\nstatic unsigned int choke_drop(struct Qdisc *sch)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nunsigned int len;\r\nlen = qdisc_queue_drop(sch);\r\nif (len > 0)\r\nq->stats.other++;\r\nelse {\r\nif (!red_is_idling(&q->vars))\r\nred_start_of_idle_period(&q->vars);\r\n}\r\nreturn len;\r\n}\r\nstatic void choke_reset(struct Qdisc *sch)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nred_restart(&q->vars);\r\n}\r\nstatic void choke_free(void *addr)\r\n{\r\nif (addr) {\r\nif (is_vmalloc_addr(addr))\r\nvfree(addr);\r\nelse\r\nkfree(addr);\r\n}\r\n}\r\nstatic int choke_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_CHOKE_MAX + 1];\r\nconst struct tc_red_qopt *ctl;\r\nint err;\r\nstruct sk_buff **old = NULL;\r\nunsigned int mask;\r\nu32 max_P;\r\nif (opt == NULL)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_CHOKE_MAX, opt, choke_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (tb[TCA_CHOKE_PARMS] == NULL ||\r\ntb[TCA_CHOKE_STAB] == NULL)\r\nreturn -EINVAL;\r\nmax_P = tb[TCA_CHOKE_MAX_P] ? nla_get_u32(tb[TCA_CHOKE_MAX_P]) : 0;\r\nctl = nla_data(tb[TCA_CHOKE_PARMS]);\r\nif (ctl->limit > CHOKE_MAX_QUEUE)\r\nreturn -EINVAL;\r\nmask = roundup_pow_of_two(ctl->limit + 1) - 1;\r\nif (mask != q->tab_mask) {\r\nstruct sk_buff **ntab;\r\nntab = kcalloc(mask + 1, sizeof(struct sk_buff *), GFP_KERNEL);\r\nif (!ntab)\r\nntab = vzalloc((mask + 1) * sizeof(struct sk_buff *));\r\nif (!ntab)\r\nreturn -ENOMEM;\r\nsch_tree_lock(sch);\r\nold = q->tab;\r\nif (old) {\r\nunsigned int oqlen = sch->q.qlen, tail = 0;\r\nwhile (q->head != q->tail) {\r\nstruct sk_buff *skb = q->tab[q->head];\r\nq->head = (q->head + 1) & q->tab_mask;\r\nif (!skb)\r\ncontinue;\r\nif (tail < mask) {\r\nntab[tail++] = skb;\r\ncontinue;\r\n}\r\nsch->qstats.backlog -= qdisc_pkt_len(skb);\r\n--sch->q.qlen;\r\nqdisc_drop(skb, sch);\r\n}\r\nqdisc_tree_decrease_qlen(sch, oqlen - sch->q.qlen);\r\nq->head = 0;\r\nq->tail = tail;\r\n}\r\nq->tab_mask = mask;\r\nq->tab = ntab;\r\n} else\r\nsch_tree_lock(sch);\r\nq->flags = ctl->flags;\r\nq->limit = ctl->limit;\r\nred_set_parms(&q->parms, ctl->qth_min, ctl->qth_max, ctl->Wlog,\r\nctl->Plog, ctl->Scell_log,\r\nnla_data(tb[TCA_CHOKE_STAB]),\r\nmax_P);\r\nred_set_vars(&q->vars);\r\nif (q->head == q->tail)\r\nred_end_of_idle_period(&q->vars);\r\nsch_tree_unlock(sch);\r\nchoke_free(old);\r\nreturn 0;\r\n}\r\nstatic int choke_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nreturn choke_change(sch, opt);\r\n}\r\nstatic int choke_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *opts = NULL;\r\nstruct tc_red_qopt opt = {\r\n.limit = q->limit,\r\n.flags = q->flags,\r\n.qth_min = q->parms.qth_min >> q->parms.Wlog,\r\n.qth_max = q->parms.qth_max >> q->parms.Wlog,\r\n.Wlog = q->parms.Wlog,\r\n.Plog = q->parms.Plog,\r\n.Scell_log = q->parms.Scell_log,\r\n};\r\nopts = nla_nest_start(skb, TCA_OPTIONS);\r\nif (opts == NULL)\r\ngoto nla_put_failure;\r\nif (nla_put(skb, TCA_CHOKE_PARMS, sizeof(opt), &opt) ||\r\nnla_put_u32(skb, TCA_CHOKE_MAX_P, q->parms.max_P))\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, opts);\r\nnla_put_failure:\r\nnla_nest_cancel(skb, opts);\r\nreturn -EMSGSIZE;\r\n}\r\nstatic int choke_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nstruct tc_choke_xstats st = {\r\n.early = q->stats.prob_drop + q->stats.forced_drop,\r\n.marked = q->stats.prob_mark + q->stats.forced_mark,\r\n.pdrop = q->stats.pdrop,\r\n.other = q->stats.other,\r\n.matched = q->stats.matched,\r\n};\r\nreturn gnet_stats_copy_app(d, &st, sizeof(st));\r\n}\r\nstatic void choke_destroy(struct Qdisc *sch)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\ntcf_destroy_chain(&q->filter_list);\r\nchoke_free(q->tab);\r\n}\r\nstatic struct Qdisc *choke_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nreturn NULL;\r\n}\r\nstatic unsigned long choke_get(struct Qdisc *sch, u32 classid)\r\n{\r\nreturn 0;\r\n}\r\nstatic void choke_put(struct Qdisc *q, unsigned long cl)\r\n{\r\n}\r\nstatic unsigned long choke_bind(struct Qdisc *sch, unsigned long parent,\r\nu32 classid)\r\n{\r\nreturn 0;\r\n}\r\nstatic struct tcf_proto **choke_find_tcf(struct Qdisc *sch, unsigned long cl)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nif (cl)\r\nreturn NULL;\r\nreturn &q->filter_list;\r\n}\r\nstatic int choke_dump_class(struct Qdisc *sch, unsigned long cl,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\ntcm->tcm_handle |= TC_H_MIN(cl);\r\nreturn 0;\r\n}\r\nstatic void choke_walk(struct Qdisc *sch, struct qdisc_walker *arg)\r\n{\r\nif (!arg->stop) {\r\nif (arg->fn(sch, 1, arg) < 0) {\r\narg->stop = 1;\r\nreturn;\r\n}\r\narg->count++;\r\n}\r\n}\r\nstatic struct sk_buff *choke_peek_head(struct Qdisc *sch)\r\n{\r\nstruct choke_sched_data *q = qdisc_priv(sch);\r\nreturn (q->head != q->tail) ? q->tab[q->head] : NULL;\r\n}\r\nstatic int __init choke_module_init(void)\r\n{\r\nreturn register_qdisc(&choke_qdisc_ops);\r\n}\r\nstatic void __exit choke_module_exit(void)\r\n{\r\nunregister_qdisc(&choke_qdisc_ops);\r\n}
