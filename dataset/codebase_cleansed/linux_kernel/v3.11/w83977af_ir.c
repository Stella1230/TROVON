static int __init w83977af_init(void)\r\n{\r\nint i;\r\nIRDA_DEBUG(0, "%s()\n", __func__ );\r\nfor (i=0; i < ARRAY_SIZE(dev_self) && io[i] < 2000; i++) {\r\nif (w83977af_open(i, io[i], irq[i], dma[i]) == 0)\r\nreturn 0;\r\n}\r\nreturn -ENODEV;\r\n}\r\nstatic void __exit w83977af_cleanup(void)\r\n{\r\nint i;\r\nIRDA_DEBUG(4, "%s()\n", __func__ );\r\nfor (i=0; i < ARRAY_SIZE(dev_self); i++) {\r\nif (dev_self[i])\r\nw83977af_close(dev_self[i]);\r\n}\r\n}\r\nstatic int w83977af_open(int i, unsigned int iobase, unsigned int irq,\r\nunsigned int dma)\r\n{\r\nstruct net_device *dev;\r\nstruct w83977af_ir *self;\r\nint err;\r\nIRDA_DEBUG(0, "%s()\n", __func__ );\r\nif (!request_region(iobase, CHIP_IO_EXTENT, driver_name)) {\r\nIRDA_DEBUG(0, "%s(), can't get iobase of 0x%03x\n",\r\n__func__ , iobase);\r\nreturn -ENODEV;\r\n}\r\nif (w83977af_probe(iobase, irq, dma) == -1) {\r\nerr = -1;\r\ngoto err_out;\r\n}\r\ndev = alloc_irdadev(sizeof(struct w83977af_ir));\r\nif (dev == NULL) {\r\nprintk( KERN_ERR "IrDA: Can't allocate memory for "\r\n"IrDA control block!\n");\r\nerr = -ENOMEM;\r\ngoto err_out;\r\n}\r\nself = netdev_priv(dev);\r\nspin_lock_init(&self->lock);\r\nself->io.fir_base = iobase;\r\nself->io.irq = irq;\r\nself->io.fir_ext = CHIP_IO_EXTENT;\r\nself->io.dma = dma;\r\nself->io.fifo_size = 32;\r\nirda_init_max_qos_capabilies(&self->qos);\r\nself->qos.baud_rate.bits = IR_9600|IR_19200|IR_38400|IR_57600|\r\nIR_115200|IR_576000|IR_1152000|(IR_4000000 << 8);\r\nself->qos.min_turn_time.bits = qos_mtt_bits;\r\nirda_qos_bits_to_value(&self->qos);\r\nself->rx_buff.truesize = 14384;\r\nself->tx_buff.truesize = 4000;\r\nself->rx_buff.head =\r\ndma_alloc_coherent(NULL, self->rx_buff.truesize,\r\n&self->rx_buff_dma, GFP_KERNEL | __GFP_ZERO);\r\nif (self->rx_buff.head == NULL) {\r\nerr = -ENOMEM;\r\ngoto err_out1;\r\n}\r\nself->tx_buff.head =\r\ndma_alloc_coherent(NULL, self->tx_buff.truesize,\r\n&self->tx_buff_dma, GFP_KERNEL | __GFP_ZERO);\r\nif (self->tx_buff.head == NULL) {\r\nerr = -ENOMEM;\r\ngoto err_out2;\r\n}\r\nself->rx_buff.in_frame = FALSE;\r\nself->rx_buff.state = OUTSIDE_FRAME;\r\nself->tx_buff.data = self->tx_buff.head;\r\nself->rx_buff.data = self->rx_buff.head;\r\nself->netdev = dev;\r\ndev->netdev_ops = &w83977_netdev_ops;\r\nerr = register_netdev(dev);\r\nif (err) {\r\nIRDA_ERROR("%s(), register_netdevice() failed!\n", __func__);\r\ngoto err_out3;\r\n}\r\nIRDA_MESSAGE("IrDA: Registered device %s\n", dev->name);\r\ndev_self[i] = self;\r\nreturn 0;\r\nerr_out3:\r\ndma_free_coherent(NULL, self->tx_buff.truesize,\r\nself->tx_buff.head, self->tx_buff_dma);\r\nerr_out2:\r\ndma_free_coherent(NULL, self->rx_buff.truesize,\r\nself->rx_buff.head, self->rx_buff_dma);\r\nerr_out1:\r\nfree_netdev(dev);\r\nerr_out:\r\nrelease_region(iobase, CHIP_IO_EXTENT);\r\nreturn err;\r\n}\r\nstatic int w83977af_close(struct w83977af_ir *self)\r\n{\r\nint iobase;\r\nIRDA_DEBUG(0, "%s()\n", __func__ );\r\niobase = self->io.fir_base;\r\n#ifdef CONFIG_USE_W977_PNP\r\nw977_efm_enter(efio);\r\nw977_select_device(W977_DEVICE_IR, efio);\r\nw977_write_reg(0x30, 0x00, efio);\r\nw977_efm_exit(efio);\r\n#endif\r\nunregister_netdev(self->netdev);\r\nIRDA_DEBUG(0 , "%s(), Releasing Region %03x\n",\r\n__func__ , self->io.fir_base);\r\nrelease_region(self->io.fir_base, self->io.fir_ext);\r\nif (self->tx_buff.head)\r\ndma_free_coherent(NULL, self->tx_buff.truesize,\r\nself->tx_buff.head, self->tx_buff_dma);\r\nif (self->rx_buff.head)\r\ndma_free_coherent(NULL, self->rx_buff.truesize,\r\nself->rx_buff.head, self->rx_buff_dma);\r\nfree_netdev(self->netdev);\r\nreturn 0;\r\n}\r\nstatic int w83977af_probe(int iobase, int irq, int dma)\r\n{\r\nint version;\r\nint i;\r\nfor (i=0; i < 2; i++) {\r\nIRDA_DEBUG( 0, "%s()\n", __func__ );\r\n#ifdef CONFIG_USE_W977_PNP\r\nw977_efm_enter(efbase[i]);\r\nw977_select_device(W977_DEVICE_IR, efbase[i]);\r\nw977_write_reg(0x60, (iobase >> 8) & 0xff, efbase[i]);\r\nw977_write_reg(0x61, (iobase) & 0xff, efbase[i]);\r\nw977_write_reg(0x70, irq, efbase[i]);\r\n#ifdef CONFIG_ARCH_NETWINDER\r\nw977_write_reg(0x74, dma+1, efbase[i]);\r\n#else\r\nw977_write_reg(0x74, dma, efbase[i]);\r\n#endif\r\nw977_write_reg(0x75, 0x04, efbase[i]);\r\nw977_write_reg(0xf0, APEDCRC|ENBNKSEL, efbase[i]);\r\nw977_write_reg(0x30, 0x01, efbase[i]);\r\nw977_efm_exit(efbase[i]);\r\n#endif\r\nswitch_bank(iobase, SET2);\r\noutb(iobase+2, 0x00);\r\nswitch_bank(iobase, SET0);\r\noutb(HCR_EN_IRQ, iobase+HCR);\r\nswitch_bank(iobase, SET2);\r\noutb(inb(iobase+ADCR1) | ADCR1_ADV_SL, iobase+ADCR1);\r\nswitch_bank(iobase, SET0);\r\noutb(HCR_SIR, iobase+HCR);\r\nswitch_bank(iobase, SET3);\r\nversion = inb(iobase+AUID);\r\nif (0x10 == (version & 0xf0)) {\r\nefio = efbase[i];\r\nswitch_bank(iobase, SET2);\r\noutb(ADCR2_RXFS32|ADCR2_TXFS32, iobase+ADCR2);\r\nswitch_bank(iobase, SET0);\r\noutb(UFR_RXTL|UFR_TXTL|UFR_TXF_RST|UFR_RXF_RST|\r\nUFR_EN_FIFO,iobase+UFR);\r\nswitch_bank(iobase, SET4);\r\noutb(2048 & 0xff, iobase+6);\r\noutb((2048 >> 8) & 0x1f, iobase+7);\r\nswitch_bank(iobase, SET7);\r\noutb(0x40, iobase+7);\r\nIRDA_MESSAGE("W83977AF (IR) driver loaded. "\r\n"Version: 0x%02x\n", version);\r\nreturn 0;\r\n} else {\r\nIRDA_DEBUG( 0, "%s(), Wrong chip version", __func__ );\r\n}\r\n}\r\nreturn -1;\r\n}\r\nstatic void w83977af_change_speed(struct w83977af_ir *self, __u32 speed)\r\n{\r\nint ir_mode = HCR_SIR;\r\nint iobase;\r\n__u8 set;\r\niobase = self->io.fir_base;\r\nself->io.speed = speed;\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET0);\r\noutb(0, iobase+ICR);\r\nswitch_bank(iobase, SET2);\r\noutb(0x00, iobase+ABHL);\r\nswitch (speed) {\r\ncase 9600: outb(0x0c, iobase+ABLL); break;\r\ncase 19200: outb(0x06, iobase+ABLL); break;\r\ncase 38400: outb(0x03, iobase+ABLL); break;\r\ncase 57600: outb(0x02, iobase+ABLL); break;\r\ncase 115200: outb(0x01, iobase+ABLL); break;\r\ncase 576000:\r\nir_mode = HCR_MIR_576;\r\nIRDA_DEBUG(0, "%s(), handling baud of 576000\n", __func__ );\r\nbreak;\r\ncase 1152000:\r\nir_mode = HCR_MIR_1152;\r\nIRDA_DEBUG(0, "%s(), handling baud of 1152000\n", __func__ );\r\nbreak;\r\ncase 4000000:\r\nir_mode = HCR_FIR;\r\nIRDA_DEBUG(0, "%s(), handling baud of 4000000\n", __func__ );\r\nbreak;\r\ndefault:\r\nir_mode = HCR_FIR;\r\nIRDA_DEBUG(0, "%s(), unknown baud rate of %d\n", __func__ , speed);\r\nbreak;\r\n}\r\nswitch_bank(iobase, SET0);\r\noutb(ir_mode, iobase+HCR);\r\nswitch_bank(iobase, SET2);\r\noutb(ADCR2_RXFS32|ADCR2_TXFS32, iobase+ADCR2);\r\nswitch_bank(iobase, SET0);\r\noutb(0x00, iobase+UFR);\r\noutb(UFR_EN_FIFO, iobase+UFR);\r\noutb(0xa7, iobase+UFR);\r\nnetif_wake_queue(self->netdev);\r\nswitch_bank(iobase, SET0);\r\nif (speed > PIO_MAX_SPEED) {\r\noutb(ICR_EFSFI, iobase+ICR);\r\nw83977af_dma_receive(self);\r\n} else\r\noutb(ICR_ERBRI, iobase+ICR);\r\noutb(set, iobase+SSR);\r\n}\r\nstatic netdev_tx_t w83977af_hard_xmit(struct sk_buff *skb,\r\nstruct net_device *dev)\r\n{\r\nstruct w83977af_ir *self;\r\n__s32 speed;\r\nint iobase;\r\n__u8 set;\r\nint mtt;\r\nself = netdev_priv(dev);\r\niobase = self->io.fir_base;\r\nIRDA_DEBUG(4, "%s(%ld), skb->len=%d\n", __func__ , jiffies,\r\n(int) skb->len);\r\nnetif_stop_queue(dev);\r\nspeed = irda_get_next_speed(skb);\r\nif ((speed != self->io.speed) && (speed != -1)) {\r\nif (!skb->len) {\r\nw83977af_change_speed(self, speed);\r\ndev_kfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\n} else\r\nself->new_speed = speed;\r\n}\r\nset = inb(iobase+SSR);\r\nif (self->io.speed > PIO_MAX_SPEED) {\r\nself->tx_buff.data = self->tx_buff.head;\r\nskb_copy_from_linear_data(skb, self->tx_buff.data, skb->len);\r\nself->tx_buff.len = skb->len;\r\nmtt = irda_get_mtt(skb);\r\nIRDA_DEBUG(4, "%s(%ld), mtt=%d\n", __func__ , jiffies, mtt);\r\nif (mtt)\r\nudelay(mtt);\r\nswitch_bank(iobase, SET0);\r\noutb(ICR_EDMAI, iobase+ICR);\r\nw83977af_dma_write(self, iobase);\r\n} else {\r\nself->tx_buff.data = self->tx_buff.head;\r\nself->tx_buff.len = async_wrap_skb(skb, self->tx_buff.data,\r\nself->tx_buff.truesize);\r\nswitch_bank(iobase, SET0);\r\noutb(ICR_ETXTHI, iobase+ICR);\r\n}\r\ndev_kfree_skb(skb);\r\noutb(set, iobase+SSR);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void w83977af_dma_write(struct w83977af_ir *self, int iobase)\r\n{\r\n__u8 set;\r\n#ifdef CONFIG_NETWINDER_TX_DMA_PROBLEMS\r\nunsigned long flags;\r\n__u8 hcr;\r\n#endif\r\nIRDA_DEBUG(4, "%s(), len=%d\n", __func__ , self->tx_buff.len);\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET0);\r\noutb(inb(iobase+HCR) & ~HCR_EN_DMA, iobase+HCR);\r\nswitch_bank(iobase, SET2);\r\noutb(ADCR1_D_CHSW|ADCR1_ADV_SL, iobase+ADCR1);\r\n#ifdef CONFIG_NETWINDER_TX_DMA_PROBLEMS\r\nspin_lock_irqsave(&self->lock, flags);\r\ndisable_dma(self->io.dma);\r\nclear_dma_ff(self->io.dma);\r\nset_dma_mode(self->io.dma, DMA_MODE_READ);\r\nset_dma_addr(self->io.dma, self->tx_buff_dma);\r\nset_dma_count(self->io.dma, self->tx_buff.len);\r\n#else\r\nirda_setup_dma(self->io.dma, self->tx_buff_dma, self->tx_buff.len,\r\nDMA_MODE_WRITE);\r\n#endif\r\nself->io.direction = IO_XMIT;\r\nswitch_bank(iobase, SET0);\r\n#ifdef CONFIG_NETWINDER_TX_DMA_PROBLEMS\r\nhcr = inb(iobase+HCR);\r\noutb(hcr | HCR_EN_DMA, iobase+HCR);\r\nenable_dma(self->io.dma);\r\nspin_unlock_irqrestore(&self->lock, flags);\r\n#else\r\noutb(inb(iobase+HCR) | HCR_EN_DMA | HCR_TX_WT, iobase+HCR);\r\n#endif\r\noutb(set, iobase+SSR);\r\n}\r\nstatic int w83977af_pio_write(int iobase, __u8 *buf, int len, int fifo_size)\r\n{\r\nint actual = 0;\r\n__u8 set;\r\nIRDA_DEBUG(4, "%s()\n", __func__ );\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET0);\r\nif (!(inb_p(iobase+USR) & USR_TSRE)) {\r\nIRDA_DEBUG(4,\r\n"%s(), warning, FIFO not empty yet!\n", __func__ );\r\nfifo_size -= 17;\r\nIRDA_DEBUG(4, "%s(), %d bytes left in tx fifo\n",\r\n__func__ , fifo_size);\r\n}\r\nwhile ((fifo_size-- > 0) && (actual < len)) {\r\noutb(buf[actual++], iobase+TBR);\r\n}\r\nIRDA_DEBUG(4, "%s(), fifo_size %d ; %d sent of %d\n",\r\n__func__ , fifo_size, actual, len);\r\noutb(set, iobase+SSR);\r\nreturn actual;\r\n}\r\nstatic void w83977af_dma_xmit_complete(struct w83977af_ir *self)\r\n{\r\nint iobase;\r\n__u8 set;\r\nIRDA_DEBUG(4, "%s(%ld)\n", __func__ , jiffies);\r\nIRDA_ASSERT(self != NULL, return;);\r\niobase = self->io.fir_base;\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET0);\r\noutb(inb(iobase+HCR) & ~HCR_EN_DMA, iobase+HCR);\r\nif (inb(iobase+AUDR) & AUDR_UNDR) {\r\nIRDA_DEBUG(0, "%s(), Transmit underrun!\n", __func__ );\r\nself->netdev->stats.tx_errors++;\r\nself->netdev->stats.tx_fifo_errors++;\r\noutb(AUDR_UNDR, iobase+AUDR);\r\n} else\r\nself->netdev->stats.tx_packets++;\r\nif (self->new_speed) {\r\nw83977af_change_speed(self, self->new_speed);\r\nself->new_speed = 0;\r\n}\r\nnetif_wake_queue(self->netdev);\r\noutb(set, iobase+SSR);\r\n}\r\nstatic int w83977af_dma_receive(struct w83977af_ir *self)\r\n{\r\nint iobase;\r\n__u8 set;\r\n#ifdef CONFIG_NETWINDER_RX_DMA_PROBLEMS\r\nunsigned long flags;\r\n__u8 hcr;\r\n#endif\r\nIRDA_ASSERT(self != NULL, return -1;);\r\nIRDA_DEBUG(4, "%s\n", __func__ );\r\niobase= self->io.fir_base;\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET0);\r\noutb(inb(iobase+HCR) & ~HCR_EN_DMA, iobase+HCR);\r\nswitch_bank(iobase, SET2);\r\noutb((inb(iobase+ADCR1) & ~ADCR1_D_CHSW)|ADCR1_ADV_SL,\r\niobase+ADCR1);\r\nself->io.direction = IO_RECV;\r\nself->rx_buff.data = self->rx_buff.head;\r\n#ifdef CONFIG_NETWINDER_RX_DMA_PROBLEMS\r\nspin_lock_irqsave(&self->lock, flags);\r\ndisable_dma(self->io.dma);\r\nclear_dma_ff(self->io.dma);\r\nset_dma_mode(self->io.dma, DMA_MODE_READ);\r\nset_dma_addr(self->io.dma, self->rx_buff_dma);\r\nset_dma_count(self->io.dma, self->rx_buff.truesize);\r\n#else\r\nirda_setup_dma(self->io.dma, self->rx_buff_dma, self->rx_buff.truesize,\r\nDMA_MODE_READ);\r\n#endif\r\nswitch_bank(iobase, SET0);\r\noutb(UFR_RXTL|UFR_TXTL|UFR_RXF_RST|UFR_EN_FIFO, iobase+UFR);\r\nself->st_fifo.len = self->st_fifo.tail = self->st_fifo.head = 0;\r\nswitch_bank(iobase, SET0);\r\n#ifdef CONFIG_NETWINDER_RX_DMA_PROBLEMS\r\nhcr = inb(iobase+HCR);\r\noutb(hcr | HCR_EN_DMA, iobase+HCR);\r\nenable_dma(self->io.dma);\r\nspin_unlock_irqrestore(&self->lock, flags);\r\n#else\r\noutb(inb(iobase+HCR) | HCR_EN_DMA, iobase+HCR);\r\n#endif\r\noutb(set, iobase+SSR);\r\nreturn 0;\r\n}\r\nstatic int w83977af_dma_receive_complete(struct w83977af_ir *self)\r\n{\r\nstruct sk_buff *skb;\r\nstruct st_fifo *st_fifo;\r\nint len;\r\nint iobase;\r\n__u8 set;\r\n__u8 status;\r\nIRDA_DEBUG(4, "%s\n", __func__ );\r\nst_fifo = &self->st_fifo;\r\niobase = self->io.fir_base;\r\nset = inb(iobase+SSR);\r\niobase = self->io.fir_base;\r\nswitch_bank(iobase, SET5);\r\nwhile ((status = inb(iobase+FS_FO)) & FS_FO_FSFDR) {\r\nst_fifo->entries[st_fifo->tail].status = status;\r\nst_fifo->entries[st_fifo->tail].len = inb(iobase+RFLFL);\r\nst_fifo->entries[st_fifo->tail].len |= inb(iobase+RFLFH) << 8;\r\nst_fifo->tail++;\r\nst_fifo->len++;\r\n}\r\nwhile (st_fifo->len) {\r\nstatus = st_fifo->entries[st_fifo->head].status;\r\nlen = st_fifo->entries[st_fifo->head].len;\r\nst_fifo->head++;\r\nst_fifo->len--;\r\nif (status & FS_FO_ERR_MSK) {\r\nif (status & FS_FO_LST_FR) {\r\nself->netdev->stats.rx_errors += len;\r\n} else {\r\nself->netdev->stats.rx_errors++;\r\nself->rx_buff.data += len;\r\nif (status & FS_FO_MX_LEX)\r\nself->netdev->stats.rx_length_errors++;\r\nif (status & FS_FO_PHY_ERR)\r\nself->netdev->stats.rx_frame_errors++;\r\nif (status & FS_FO_CRC_ERR)\r\nself->netdev->stats.rx_crc_errors++;\r\n}\r\nif (status & FS_FO_RX_OV)\r\nself->netdev->stats.rx_fifo_errors++;\r\nif (status & FS_FO_FSF_OV)\r\nself->netdev->stats.rx_fifo_errors++;\r\n} else {\r\nswitch_bank(iobase, SET0);\r\nif (inb(iobase+USR) & USR_RDR) {\r\nudelay(80);\r\n}\r\nskb = dev_alloc_skb(len+1);\r\nif (skb == NULL) {\r\nprintk(KERN_INFO\r\n"%s(), memory squeeze, dropping frame.\n", __func__);\r\noutb(set, iobase+SSR);\r\nreturn FALSE;\r\n}\r\nskb_reserve(skb, 1);\r\nif (self->io.speed < 4000000) {\r\nskb_put(skb, len-2);\r\nskb_copy_to_linear_data(skb,\r\nself->rx_buff.data,\r\nlen - 2);\r\n} else {\r\nskb_put(skb, len-4);\r\nskb_copy_to_linear_data(skb,\r\nself->rx_buff.data,\r\nlen - 4);\r\n}\r\nself->rx_buff.data += len;\r\nself->netdev->stats.rx_packets++;\r\nskb->dev = self->netdev;\r\nskb_reset_mac_header(skb);\r\nskb->protocol = htons(ETH_P_IRDA);\r\nnetif_rx(skb);\r\n}\r\n}\r\noutb(set, iobase+SSR);\r\nreturn TRUE;\r\n}\r\nstatic void w83977af_pio_receive(struct w83977af_ir *self)\r\n{\r\n__u8 byte = 0x00;\r\nint iobase;\r\nIRDA_DEBUG(4, "%s()\n", __func__ );\r\nIRDA_ASSERT(self != NULL, return;);\r\niobase = self->io.fir_base;\r\ndo {\r\nbyte = inb(iobase+RBR);\r\nasync_unwrap_char(self->netdev, &self->netdev->stats, &self->rx_buff,\r\nbyte);\r\n} while (inb(iobase+USR) & USR_RDR);\r\n}\r\nstatic __u8 w83977af_sir_interrupt(struct w83977af_ir *self, int isr)\r\n{\r\nint actual;\r\n__u8 new_icr = 0;\r\n__u8 set;\r\nint iobase;\r\nIRDA_DEBUG(4, "%s(), isr=%#x\n", __func__ , isr);\r\niobase = self->io.fir_base;\r\nif (isr & ISR_TXTH_I) {\r\nactual = w83977af_pio_write(self->io.fir_base,\r\nself->tx_buff.data,\r\nself->tx_buff.len,\r\nself->io.fifo_size);\r\nself->tx_buff.data += actual;\r\nself->tx_buff.len -= actual;\r\nself->io.direction = IO_XMIT;\r\nif (self->tx_buff.len > 0) {\r\nnew_icr |= ICR_ETXTHI;\r\n} else {\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET0);\r\noutb(AUDR_SFEND, iobase+AUDR);\r\noutb(set, iobase+SSR);\r\nself->netdev->stats.tx_packets++;\r\nnetif_wake_queue(self->netdev);\r\nnew_icr |= ICR_ETBREI;\r\n}\r\n}\r\nif (isr & ISR_TXEMP_I) {\r\nif (self->new_speed) {\r\nIRDA_DEBUG(2,\r\n"%s(), Changing speed!\n", __func__ );\r\nw83977af_change_speed(self, self->new_speed);\r\nself->new_speed = 0;\r\n}\r\nself->io.direction = IO_RECV;\r\nnew_icr |= ICR_ERBRI;\r\n}\r\nif (isr & ISR_RXTH_I) {\r\nw83977af_pio_receive(self);\r\nnew_icr |= ICR_ERBRI;\r\n}\r\nreturn new_icr;\r\n}\r\nstatic __u8 w83977af_fir_interrupt(struct w83977af_ir *self, int isr)\r\n{\r\n__u8 new_icr = 0;\r\n__u8 set;\r\nint iobase;\r\niobase = self->io.fir_base;\r\nset = inb(iobase+SSR);\r\nif (isr & (ISR_FEND_I|ISR_FSF_I)) {\r\nif (w83977af_dma_receive_complete(self)) {\r\nnew_icr |= ICR_EFSFI;\r\n} else {\r\nswitch_bank(iobase, SET4);\r\noutb(0x01, iobase+TMRL);\r\noutb(0x00, iobase+TMRH);\r\noutb(IR_MSL_EN_TMR, iobase+IR_MSL);\r\nnew_icr |= ICR_ETMRI;\r\n}\r\n}\r\nif (isr & ISR_TMR_I) {\r\nswitch_bank(iobase, SET4);\r\noutb(0, iobase+IR_MSL);\r\nif (self->io.direction == IO_XMIT) {\r\nw83977af_dma_write(self, iobase);\r\nnew_icr |= ICR_EDMAI;\r\n} else {\r\nw83977af_dma_receive_complete(self);\r\nnew_icr |= ICR_EFSFI;\r\n}\r\n}\r\nif (isr & ISR_DMA_I) {\r\nw83977af_dma_xmit_complete(self);\r\nw83977af_dma_receive(self);\r\nnew_icr = ICR_EFSFI;\r\n}\r\noutb(set, iobase+SSR);\r\nreturn new_icr;\r\n}\r\nstatic irqreturn_t w83977af_interrupt(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev = dev_id;\r\nstruct w83977af_ir *self;\r\n__u8 set, icr, isr;\r\nint iobase;\r\nself = netdev_priv(dev);\r\niobase = self->io.fir_base;\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET0);\r\nicr = inb(iobase+ICR);\r\nisr = inb(iobase+ISR) & icr;\r\noutb(0, iobase+ICR);\r\nif (isr) {\r\nif (self->io.speed > PIO_MAX_SPEED )\r\nicr = w83977af_fir_interrupt(self, isr);\r\nelse\r\nicr = w83977af_sir_interrupt(self, isr);\r\n}\r\noutb(icr, iobase+ICR);\r\noutb(set, iobase+SSR);\r\nreturn IRQ_RETVAL(isr);\r\n}\r\nstatic int w83977af_is_receiving(struct w83977af_ir *self)\r\n{\r\nint status = FALSE;\r\nint iobase;\r\n__u8 set;\r\nIRDA_ASSERT(self != NULL, return FALSE;);\r\nif (self->io.speed > 115200) {\r\niobase = self->io.fir_base;\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET2);\r\nif ((inb(iobase+RXFDTH) & 0x3f) != 0) {\r\nstatus = TRUE;\r\n}\r\noutb(set, iobase+SSR);\r\n} else\r\nstatus = (self->rx_buff.state != OUTSIDE_FRAME);\r\nreturn status;\r\n}\r\nstatic int w83977af_net_open(struct net_device *dev)\r\n{\r\nstruct w83977af_ir *self;\r\nint iobase;\r\nchar hwname[32];\r\n__u8 set;\r\nIRDA_DEBUG(0, "%s()\n", __func__ );\r\nIRDA_ASSERT(dev != NULL, return -1;);\r\nself = netdev_priv(dev);\r\nIRDA_ASSERT(self != NULL, return 0;);\r\niobase = self->io.fir_base;\r\nif (request_irq(self->io.irq, w83977af_interrupt, 0, dev->name,\r\n(void *) dev)) {\r\nreturn -EAGAIN;\r\n}\r\nif (request_dma(self->io.dma, dev->name)) {\r\nfree_irq(self->io.irq, dev);\r\nreturn -EAGAIN;\r\n}\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET0);\r\nif (self->io.speed > 115200) {\r\noutb(ICR_EFSFI, iobase+ICR);\r\nw83977af_dma_receive(self);\r\n} else\r\noutb(ICR_ERBRI, iobase+ICR);\r\noutb(set, iobase+SSR);\r\nnetif_start_queue(dev);\r\nsprintf(hwname, "w83977af @ 0x%03x", self->io.fir_base);\r\nself->irlap = irlap_open(dev, &self->qos, hwname);\r\nreturn 0;\r\n}\r\nstatic int w83977af_net_close(struct net_device *dev)\r\n{\r\nstruct w83977af_ir *self;\r\nint iobase;\r\n__u8 set;\r\nIRDA_DEBUG(0, "%s()\n", __func__ );\r\nIRDA_ASSERT(dev != NULL, return -1;);\r\nself = netdev_priv(dev);\r\nIRDA_ASSERT(self != NULL, return 0;);\r\niobase = self->io.fir_base;\r\nnetif_stop_queue(dev);\r\nif (self->irlap)\r\nirlap_close(self->irlap);\r\nself->irlap = NULL;\r\ndisable_dma(self->io.dma);\r\nset = inb(iobase+SSR);\r\nswitch_bank(iobase, SET0);\r\noutb(0, iobase+ICR);\r\nfree_irq(self->io.irq, dev);\r\nfree_dma(self->io.dma);\r\noutb(set, iobase+SSR);\r\nreturn 0;\r\n}\r\nstatic int w83977af_net_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct if_irda_req *irq = (struct if_irda_req *) rq;\r\nstruct w83977af_ir *self;\r\nunsigned long flags;\r\nint ret = 0;\r\nIRDA_ASSERT(dev != NULL, return -1;);\r\nself = netdev_priv(dev);\r\nIRDA_ASSERT(self != NULL, return -1;);\r\nIRDA_DEBUG(2, "%s(), %s, (cmd=0x%X)\n", __func__ , dev->name, cmd);\r\nspin_lock_irqsave(&self->lock, flags);\r\nswitch (cmd) {\r\ncase SIOCSBANDWIDTH:\r\nif (!capable(CAP_NET_ADMIN)) {\r\nret = -EPERM;\r\ngoto out;\r\n}\r\nw83977af_change_speed(self, irq->ifr_baudrate);\r\nbreak;\r\ncase SIOCSMEDIABUSY:\r\nif (!capable(CAP_NET_ADMIN)) {\r\nret = -EPERM;\r\ngoto out;\r\n}\r\nirda_device_set_media_busy(self->netdev, TRUE);\r\nbreak;\r\ncase SIOCGRECEIVING:\r\nirq->ifr_receiving = w83977af_is_receiving(self);\r\nbreak;\r\ndefault:\r\nret = -EOPNOTSUPP;\r\n}\r\nout:\r\nspin_unlock_irqrestore(&self->lock, flags);\r\nreturn ret;\r\n}
