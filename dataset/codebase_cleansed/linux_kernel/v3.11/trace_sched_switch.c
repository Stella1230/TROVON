void\r\ntracing_sched_switch_trace(struct trace_array *tr,\r\nstruct task_struct *prev,\r\nstruct task_struct *next,\r\nunsigned long flags, int pc)\r\n{\r\nstruct ftrace_event_call *call = &event_context_switch;\r\nstruct ring_buffer *buffer = tr->trace_buffer.buffer;\r\nstruct ring_buffer_event *event;\r\nstruct ctx_switch_entry *entry;\r\nevent = trace_buffer_lock_reserve(buffer, TRACE_CTX,\r\nsizeof(*entry), flags, pc);\r\nif (!event)\r\nreturn;\r\nentry = ring_buffer_event_data(event);\r\nentry->prev_pid = prev->pid;\r\nentry->prev_prio = prev->prio;\r\nentry->prev_state = prev->state;\r\nentry->next_pid = next->pid;\r\nentry->next_prio = next->prio;\r\nentry->next_state = next->state;\r\nentry->next_cpu = task_cpu(next);\r\nif (!filter_check_discard(call, entry, buffer, event))\r\ntrace_buffer_unlock_commit(buffer, event, flags, pc);\r\n}\r\nstatic void\r\nprobe_sched_switch(void *ignore, struct task_struct *prev, struct task_struct *next)\r\n{\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint cpu;\r\nint pc;\r\nif (unlikely(!sched_ref))\r\nreturn;\r\ntracing_record_cmdline(prev);\r\ntracing_record_cmdline(next);\r\nif (!tracer_enabled || sched_stopped)\r\nreturn;\r\npc = preempt_count();\r\nlocal_irq_save(flags);\r\ncpu = raw_smp_processor_id();\r\ndata = per_cpu_ptr(ctx_trace->trace_buffer.data, cpu);\r\nif (likely(!atomic_read(&data->disabled)))\r\ntracing_sched_switch_trace(ctx_trace, prev, next, flags, pc);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid\r\ntracing_sched_wakeup_trace(struct trace_array *tr,\r\nstruct task_struct *wakee,\r\nstruct task_struct *curr,\r\nunsigned long flags, int pc)\r\n{\r\nstruct ftrace_event_call *call = &event_wakeup;\r\nstruct ring_buffer_event *event;\r\nstruct ctx_switch_entry *entry;\r\nstruct ring_buffer *buffer = tr->trace_buffer.buffer;\r\nevent = trace_buffer_lock_reserve(buffer, TRACE_WAKE,\r\nsizeof(*entry), flags, pc);\r\nif (!event)\r\nreturn;\r\nentry = ring_buffer_event_data(event);\r\nentry->prev_pid = curr->pid;\r\nentry->prev_prio = curr->prio;\r\nentry->prev_state = curr->state;\r\nentry->next_pid = wakee->pid;\r\nentry->next_prio = wakee->prio;\r\nentry->next_state = wakee->state;\r\nentry->next_cpu = task_cpu(wakee);\r\nif (!filter_check_discard(call, entry, buffer, event))\r\ntrace_buffer_unlock_commit(buffer, event, flags, pc);\r\n}\r\nstatic void\r\nprobe_sched_wakeup(void *ignore, struct task_struct *wakee, int success)\r\n{\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint cpu, pc;\r\nif (unlikely(!sched_ref))\r\nreturn;\r\ntracing_record_cmdline(current);\r\nif (!tracer_enabled || sched_stopped)\r\nreturn;\r\npc = preempt_count();\r\nlocal_irq_save(flags);\r\ncpu = raw_smp_processor_id();\r\ndata = per_cpu_ptr(ctx_trace->trace_buffer.data, cpu);\r\nif (likely(!atomic_read(&data->disabled)))\r\ntracing_sched_wakeup_trace(ctx_trace, wakee, current,\r\nflags, pc);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int tracing_sched_register(void)\r\n{\r\nint ret;\r\nret = register_trace_sched_wakeup(probe_sched_wakeup, NULL);\r\nif (ret) {\r\npr_info("wakeup trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_wakeup\n");\r\nreturn ret;\r\n}\r\nret = register_trace_sched_wakeup_new(probe_sched_wakeup, NULL);\r\nif (ret) {\r\npr_info("wakeup trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_wakeup_new\n");\r\ngoto fail_deprobe;\r\n}\r\nret = register_trace_sched_switch(probe_sched_switch, NULL);\r\nif (ret) {\r\npr_info("sched trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_switch\n");\r\ngoto fail_deprobe_wake_new;\r\n}\r\nreturn ret;\r\nfail_deprobe_wake_new:\r\nunregister_trace_sched_wakeup_new(probe_sched_wakeup, NULL);\r\nfail_deprobe:\r\nunregister_trace_sched_wakeup(probe_sched_wakeup, NULL);\r\nreturn ret;\r\n}\r\nstatic void tracing_sched_unregister(void)\r\n{\r\nunregister_trace_sched_switch(probe_sched_switch, NULL);\r\nunregister_trace_sched_wakeup_new(probe_sched_wakeup, NULL);\r\nunregister_trace_sched_wakeup(probe_sched_wakeup, NULL);\r\n}\r\nstatic void tracing_start_sched_switch(void)\r\n{\r\nmutex_lock(&sched_register_mutex);\r\nif (!(sched_ref++))\r\ntracing_sched_register();\r\nmutex_unlock(&sched_register_mutex);\r\n}\r\nstatic void tracing_stop_sched_switch(void)\r\n{\r\nmutex_lock(&sched_register_mutex);\r\nif (!(--sched_ref))\r\ntracing_sched_unregister();\r\nmutex_unlock(&sched_register_mutex);\r\n}\r\nvoid tracing_start_cmdline_record(void)\r\n{\r\ntracing_start_sched_switch();\r\n}\r\nvoid tracing_stop_cmdline_record(void)\r\n{\r\ntracing_stop_sched_switch();\r\n}\r\nvoid tracing_start_sched_switch_record(void)\r\n{\r\nif (unlikely(!ctx_trace)) {\r\nWARN_ON(1);\r\nreturn;\r\n}\r\ntracing_start_sched_switch();\r\nmutex_lock(&sched_register_mutex);\r\ntracer_enabled++;\r\nmutex_unlock(&sched_register_mutex);\r\n}\r\nvoid tracing_stop_sched_switch_record(void)\r\n{\r\nmutex_lock(&sched_register_mutex);\r\ntracer_enabled--;\r\nWARN_ON(tracer_enabled < 0);\r\nmutex_unlock(&sched_register_mutex);\r\ntracing_stop_sched_switch();\r\n}\r\nvoid tracing_sched_switch_assign_trace(struct trace_array *tr)\r\n{\r\nctx_trace = tr;\r\n}
