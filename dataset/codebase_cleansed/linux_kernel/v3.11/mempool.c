static void add_element(mempool_t *pool, void *element)\r\n{\r\nBUG_ON(pool->curr_nr >= pool->min_nr);\r\npool->elements[pool->curr_nr++] = element;\r\n}\r\nstatic void *remove_element(mempool_t *pool)\r\n{\r\nBUG_ON(pool->curr_nr <= 0);\r\nreturn pool->elements[--pool->curr_nr];\r\n}\r\nvoid mempool_destroy(mempool_t *pool)\r\n{\r\nwhile (pool->curr_nr) {\r\nvoid *element = remove_element(pool);\r\npool->free(element, pool->pool_data);\r\n}\r\nkfree(pool->elements);\r\nkfree(pool);\r\n}\r\nmempool_t *mempool_create(int min_nr, mempool_alloc_t *alloc_fn,\r\nmempool_free_t *free_fn, void *pool_data)\r\n{\r\nreturn mempool_create_node(min_nr,alloc_fn,free_fn, pool_data,\r\nGFP_KERNEL, NUMA_NO_NODE);\r\n}\r\nmempool_t *mempool_create_node(int min_nr, mempool_alloc_t *alloc_fn,\r\nmempool_free_t *free_fn, void *pool_data,\r\ngfp_t gfp_mask, int node_id)\r\n{\r\nmempool_t *pool;\r\npool = kmalloc_node(sizeof(*pool), gfp_mask | __GFP_ZERO, node_id);\r\nif (!pool)\r\nreturn NULL;\r\npool->elements = kmalloc_node(min_nr * sizeof(void *),\r\ngfp_mask, node_id);\r\nif (!pool->elements) {\r\nkfree(pool);\r\nreturn NULL;\r\n}\r\nspin_lock_init(&pool->lock);\r\npool->min_nr = min_nr;\r\npool->pool_data = pool_data;\r\ninit_waitqueue_head(&pool->wait);\r\npool->alloc = alloc_fn;\r\npool->free = free_fn;\r\nwhile (pool->curr_nr < pool->min_nr) {\r\nvoid *element;\r\nelement = pool->alloc(gfp_mask, pool->pool_data);\r\nif (unlikely(!element)) {\r\nmempool_destroy(pool);\r\nreturn NULL;\r\n}\r\nadd_element(pool, element);\r\n}\r\nreturn pool;\r\n}\r\nint mempool_resize(mempool_t *pool, int new_min_nr, gfp_t gfp_mask)\r\n{\r\nvoid *element;\r\nvoid **new_elements;\r\nunsigned long flags;\r\nBUG_ON(new_min_nr <= 0);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nif (new_min_nr <= pool->min_nr) {\r\nwhile (new_min_nr < pool->curr_nr) {\r\nelement = remove_element(pool);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\npool->free(element, pool->pool_data);\r\nspin_lock_irqsave(&pool->lock, flags);\r\n}\r\npool->min_nr = new_min_nr;\r\ngoto out_unlock;\r\n}\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nnew_elements = kmalloc(new_min_nr * sizeof(*new_elements), gfp_mask);\r\nif (!new_elements)\r\nreturn -ENOMEM;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nif (unlikely(new_min_nr <= pool->min_nr)) {\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nkfree(new_elements);\r\ngoto out;\r\n}\r\nmemcpy(new_elements, pool->elements,\r\npool->curr_nr * sizeof(*new_elements));\r\nkfree(pool->elements);\r\npool->elements = new_elements;\r\npool->min_nr = new_min_nr;\r\nwhile (pool->curr_nr < pool->min_nr) {\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nelement = pool->alloc(gfp_mask, pool->pool_data);\r\nif (!element)\r\ngoto out;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nif (pool->curr_nr < pool->min_nr) {\r\nadd_element(pool, element);\r\n} else {\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\npool->free(element, pool->pool_data);\r\ngoto out;\r\n}\r\n}\r\nout_unlock:\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nout:\r\nreturn 0;\r\n}\r\nvoid * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)\r\n{\r\nvoid *element;\r\nunsigned long flags;\r\nwait_queue_t wait;\r\ngfp_t gfp_temp;\r\nmight_sleep_if(gfp_mask & __GFP_WAIT);\r\ngfp_mask |= __GFP_NOMEMALLOC;\r\ngfp_mask |= __GFP_NORETRY;\r\ngfp_mask |= __GFP_NOWARN;\r\ngfp_temp = gfp_mask & ~(__GFP_WAIT|__GFP_IO);\r\nrepeat_alloc:\r\nelement = pool->alloc(gfp_temp, pool->pool_data);\r\nif (likely(element != NULL))\r\nreturn element;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nif (likely(pool->curr_nr)) {\r\nelement = remove_element(pool);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nsmp_wmb();\r\nreturn element;\r\n}\r\nif (gfp_temp != gfp_mask) {\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\ngfp_temp = gfp_mask;\r\ngoto repeat_alloc;\r\n}\r\nif (!(gfp_mask & __GFP_WAIT)) {\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nreturn NULL;\r\n}\r\ninit_wait(&wait);\r\nprepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nio_schedule_timeout(5*HZ);\r\nfinish_wait(&pool->wait, &wait);\r\ngoto repeat_alloc;\r\n}\r\nvoid mempool_free(void *element, mempool_t *pool)\r\n{\r\nunsigned long flags;\r\nif (unlikely(element == NULL))\r\nreturn;\r\nsmp_rmb();\r\nif (pool->curr_nr < pool->min_nr) {\r\nspin_lock_irqsave(&pool->lock, flags);\r\nif (pool->curr_nr < pool->min_nr) {\r\nadd_element(pool, element);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nwake_up(&pool->wait);\r\nreturn;\r\n}\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\npool->free(element, pool->pool_data);\r\n}\r\nvoid *mempool_alloc_slab(gfp_t gfp_mask, void *pool_data)\r\n{\r\nstruct kmem_cache *mem = pool_data;\r\nreturn kmem_cache_alloc(mem, gfp_mask);\r\n}\r\nvoid mempool_free_slab(void *element, void *pool_data)\r\n{\r\nstruct kmem_cache *mem = pool_data;\r\nkmem_cache_free(mem, element);\r\n}\r\nvoid *mempool_kmalloc(gfp_t gfp_mask, void *pool_data)\r\n{\r\nsize_t size = (size_t)pool_data;\r\nreturn kmalloc(size, gfp_mask);\r\n}\r\nvoid mempool_kfree(void *element, void *pool_data)\r\n{\r\nkfree(element);\r\n}\r\nvoid *mempool_alloc_pages(gfp_t gfp_mask, void *pool_data)\r\n{\r\nint order = (int)(long)pool_data;\r\nreturn alloc_pages(gfp_mask, order);\r\n}\r\nvoid mempool_free_pages(void *element, void *pool_data)\r\n{\r\nint order = (int)(long)pool_data;\r\n__free_pages(element, order);\r\n}
