static inline void count_compact_event(enum vm_event_item item)\r\n{\r\ncount_vm_event(item);\r\n}\r\nstatic inline void count_compact_events(enum vm_event_item item, long delta)\r\n{\r\ncount_vm_events(item, delta);\r\n}\r\nstatic unsigned long release_freepages(struct list_head *freelist)\r\n{\r\nstruct page *page, *next;\r\nunsigned long count = 0;\r\nlist_for_each_entry_safe(page, next, freelist, lru) {\r\nlist_del(&page->lru);\r\n__free_page(page);\r\ncount++;\r\n}\r\nreturn count;\r\n}\r\nstatic void map_pages(struct list_head *list)\r\n{\r\nstruct page *page;\r\nlist_for_each_entry(page, list, lru) {\r\narch_alloc_page(page, 0);\r\nkernel_map_pages(page, 1, 1);\r\n}\r\n}\r\nstatic inline bool migrate_async_suitable(int migratetype)\r\n{\r\nreturn is_migrate_cma(migratetype) || migratetype == MIGRATE_MOVABLE;\r\n}\r\nstatic inline bool isolation_suitable(struct compact_control *cc,\r\nstruct page *page)\r\n{\r\nif (cc->ignore_skip_hint)\r\nreturn true;\r\nreturn !get_pageblock_skip(page);\r\n}\r\nstatic void __reset_isolation_suitable(struct zone *zone)\r\n{\r\nunsigned long start_pfn = zone->zone_start_pfn;\r\nunsigned long end_pfn = zone_end_pfn(zone);\r\nunsigned long pfn;\r\nzone->compact_cached_migrate_pfn = start_pfn;\r\nzone->compact_cached_free_pfn = end_pfn;\r\nzone->compact_blockskip_flush = false;\r\nfor (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {\r\nstruct page *page;\r\ncond_resched();\r\nif (!pfn_valid(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (zone != page_zone(page))\r\ncontinue;\r\nclear_pageblock_skip(page);\r\n}\r\n}\r\nvoid reset_isolation_suitable(pg_data_t *pgdat)\r\n{\r\nint zoneid;\r\nfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\r\nstruct zone *zone = &pgdat->node_zones[zoneid];\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (zone->compact_blockskip_flush)\r\n__reset_isolation_suitable(zone);\r\n}\r\n}\r\nstatic void update_pageblock_skip(struct compact_control *cc,\r\nstruct page *page, unsigned long nr_isolated,\r\nbool migrate_scanner)\r\n{\r\nstruct zone *zone = cc->zone;\r\nif (!page)\r\nreturn;\r\nif (!nr_isolated) {\r\nunsigned long pfn = page_to_pfn(page);\r\nset_pageblock_skip(page);\r\nif (migrate_scanner) {\r\nif (!cc->finished_update_migrate &&\r\npfn > zone->compact_cached_migrate_pfn)\r\nzone->compact_cached_migrate_pfn = pfn;\r\n} else {\r\nif (!cc->finished_update_free &&\r\npfn < zone->compact_cached_free_pfn)\r\nzone->compact_cached_free_pfn = pfn;\r\n}\r\n}\r\n}\r\nstatic inline bool isolation_suitable(struct compact_control *cc,\r\nstruct page *page)\r\n{\r\nreturn true;\r\n}\r\nstatic void update_pageblock_skip(struct compact_control *cc,\r\nstruct page *page, unsigned long nr_isolated,\r\nbool migrate_scanner)\r\n{\r\n}\r\nstatic inline bool should_release_lock(spinlock_t *lock)\r\n{\r\nreturn need_resched() || spin_is_contended(lock);\r\n}\r\nstatic bool compact_checklock_irqsave(spinlock_t *lock, unsigned long *flags,\r\nbool locked, struct compact_control *cc)\r\n{\r\nif (should_release_lock(lock)) {\r\nif (locked) {\r\nspin_unlock_irqrestore(lock, *flags);\r\nlocked = false;\r\n}\r\nif (!cc->sync) {\r\ncc->contended = true;\r\nreturn false;\r\n}\r\ncond_resched();\r\n}\r\nif (!locked)\r\nspin_lock_irqsave(lock, *flags);\r\nreturn true;\r\n}\r\nstatic inline bool compact_trylock_irqsave(spinlock_t *lock,\r\nunsigned long *flags, struct compact_control *cc)\r\n{\r\nreturn compact_checklock_irqsave(lock, flags, false, cc);\r\n}\r\nstatic bool suitable_migration_target(struct page *page)\r\n{\r\nint migratetype = get_pageblock_migratetype(page);\r\nif (migratetype == MIGRATE_RESERVE)\r\nreturn false;\r\nif (is_migrate_isolate(migratetype))\r\nreturn false;\r\nif (PageBuddy(page) && page_order(page) >= pageblock_order)\r\nreturn true;\r\nif (migrate_async_suitable(migratetype))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic unsigned long isolate_freepages_block(struct compact_control *cc,\r\nunsigned long blockpfn,\r\nunsigned long end_pfn,\r\nstruct list_head *freelist,\r\nbool strict)\r\n{\r\nint nr_scanned = 0, total_isolated = 0;\r\nstruct page *cursor, *valid_page = NULL;\r\nunsigned long nr_strict_required = end_pfn - blockpfn;\r\nunsigned long flags;\r\nbool locked = false;\r\ncursor = pfn_to_page(blockpfn);\r\nfor (; blockpfn < end_pfn; blockpfn++, cursor++) {\r\nint isolated, i;\r\nstruct page *page = cursor;\r\nnr_scanned++;\r\nif (!pfn_valid_within(blockpfn))\r\ncontinue;\r\nif (!valid_page)\r\nvalid_page = page;\r\nif (!PageBuddy(page))\r\ncontinue;\r\nlocked = compact_checklock_irqsave(&cc->zone->lock, &flags,\r\nlocked, cc);\r\nif (!locked)\r\nbreak;\r\nif (!strict && !suitable_migration_target(page))\r\nbreak;\r\nif (!PageBuddy(page))\r\ncontinue;\r\nisolated = split_free_page(page);\r\nif (!isolated && strict)\r\nbreak;\r\ntotal_isolated += isolated;\r\nfor (i = 0; i < isolated; i++) {\r\nlist_add(&page->lru, freelist);\r\npage++;\r\n}\r\nif (isolated) {\r\nblockpfn += isolated - 1;\r\ncursor += isolated - 1;\r\n}\r\n}\r\ntrace_mm_compaction_isolate_freepages(nr_scanned, total_isolated);\r\nif (strict && nr_strict_required > total_isolated)\r\ntotal_isolated = 0;\r\nif (locked)\r\nspin_unlock_irqrestore(&cc->zone->lock, flags);\r\nif (blockpfn == end_pfn)\r\nupdate_pageblock_skip(cc, valid_page, total_isolated, false);\r\ncount_compact_events(COMPACTFREE_SCANNED, nr_scanned);\r\nif (total_isolated)\r\ncount_compact_events(COMPACTISOLATED, total_isolated);\r\nreturn total_isolated;\r\n}\r\nunsigned long\r\nisolate_freepages_range(struct compact_control *cc,\r\nunsigned long start_pfn, unsigned long end_pfn)\r\n{\r\nunsigned long isolated, pfn, block_end_pfn;\r\nLIST_HEAD(freelist);\r\nfor (pfn = start_pfn; pfn < end_pfn; pfn += isolated) {\r\nif (!pfn_valid(pfn) || cc->zone != page_zone(pfn_to_page(pfn)))\r\nbreak;\r\nblock_end_pfn = ALIGN(pfn + 1, pageblock_nr_pages);\r\nblock_end_pfn = min(block_end_pfn, end_pfn);\r\nisolated = isolate_freepages_block(cc, pfn, block_end_pfn,\r\n&freelist, true);\r\nif (!isolated)\r\nbreak;\r\n}\r\nmap_pages(&freelist);\r\nif (pfn < end_pfn) {\r\nrelease_freepages(&freelist);\r\nreturn 0;\r\n}\r\nreturn pfn;\r\n}\r\nstatic void acct_isolated(struct zone *zone, bool locked, struct compact_control *cc)\r\n{\r\nstruct page *page;\r\nunsigned int count[2] = { 0, };\r\nlist_for_each_entry(page, &cc->migratepages, lru)\r\ncount[!!page_is_file_cache(page)]++;\r\nif (locked) {\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON, count[0]);\r\n__mod_zone_page_state(zone, NR_ISOLATED_FILE, count[1]);\r\n} else {\r\nmod_zone_page_state(zone, NR_ISOLATED_ANON, count[0]);\r\nmod_zone_page_state(zone, NR_ISOLATED_FILE, count[1]);\r\n}\r\n}\r\nstatic bool too_many_isolated(struct zone *zone)\r\n{\r\nunsigned long active, inactive, isolated;\r\ninactive = zone_page_state(zone, NR_INACTIVE_FILE) +\r\nzone_page_state(zone, NR_INACTIVE_ANON);\r\nactive = zone_page_state(zone, NR_ACTIVE_FILE) +\r\nzone_page_state(zone, NR_ACTIVE_ANON);\r\nisolated = zone_page_state(zone, NR_ISOLATED_FILE) +\r\nzone_page_state(zone, NR_ISOLATED_ANON);\r\nreturn isolated > (inactive + active) / 2;\r\n}\r\nunsigned long\r\nisolate_migratepages_range(struct zone *zone, struct compact_control *cc,\r\nunsigned long low_pfn, unsigned long end_pfn, bool unevictable)\r\n{\r\nunsigned long last_pageblock_nr = 0, pageblock_nr;\r\nunsigned long nr_scanned = 0, nr_isolated = 0;\r\nstruct list_head *migratelist = &cc->migratepages;\r\nisolate_mode_t mode = 0;\r\nstruct lruvec *lruvec;\r\nunsigned long flags;\r\nbool locked = false;\r\nstruct page *page = NULL, *valid_page = NULL;\r\nwhile (unlikely(too_many_isolated(zone))) {\r\nif (!cc->sync)\r\nreturn 0;\r\ncongestion_wait(BLK_RW_ASYNC, HZ/10);\r\nif (fatal_signal_pending(current))\r\nreturn 0;\r\n}\r\ncond_resched();\r\nfor (; low_pfn < end_pfn; low_pfn++) {\r\nif (locked && !((low_pfn+1) % SWAP_CLUSTER_MAX)) {\r\nif (should_release_lock(&zone->lru_lock)) {\r\nspin_unlock_irqrestore(&zone->lru_lock, flags);\r\nlocked = false;\r\n}\r\n}\r\nif ((low_pfn & (MAX_ORDER_NR_PAGES - 1)) == 0) {\r\nif (!pfn_valid(low_pfn)) {\r\nlow_pfn += MAX_ORDER_NR_PAGES - 1;\r\ncontinue;\r\n}\r\n}\r\nif (!pfn_valid_within(low_pfn))\r\ncontinue;\r\nnr_scanned++;\r\npage = pfn_to_page(low_pfn);\r\nif (page_zone(page) != zone)\r\ncontinue;\r\nif (!valid_page)\r\nvalid_page = page;\r\npageblock_nr = low_pfn >> pageblock_order;\r\nif (!isolation_suitable(cc, page))\r\ngoto next_pageblock;\r\nif (PageBuddy(page))\r\ncontinue;\r\nif (!cc->sync && last_pageblock_nr != pageblock_nr &&\r\n!migrate_async_suitable(get_pageblock_migratetype(page))) {\r\ncc->finished_update_migrate = true;\r\ngoto next_pageblock;\r\n}\r\nif (!PageLRU(page)) {\r\nif (unlikely(balloon_page_movable(page))) {\r\nif (locked && balloon_page_isolate(page)) {\r\ncc->finished_update_migrate = true;\r\nlist_add(&page->lru, migratelist);\r\ncc->nr_migratepages++;\r\nnr_isolated++;\r\ngoto check_compact_cluster;\r\n}\r\n}\r\ncontinue;\r\n}\r\nif (PageTransHuge(page)) {\r\nif (!locked)\r\ngoto next_pageblock;\r\nlow_pfn += (1 << compound_order(page)) - 1;\r\ncontinue;\r\n}\r\nlocked = compact_checklock_irqsave(&zone->lru_lock, &flags,\r\nlocked, cc);\r\nif (!locked || fatal_signal_pending(current))\r\nbreak;\r\nif (!PageLRU(page))\r\ncontinue;\r\nif (PageTransHuge(page)) {\r\nlow_pfn += (1 << compound_order(page)) - 1;\r\ncontinue;\r\n}\r\nif (!cc->sync)\r\nmode |= ISOLATE_ASYNC_MIGRATE;\r\nif (unevictable)\r\nmode |= ISOLATE_UNEVICTABLE;\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nif (__isolate_lru_page(page, mode) != 0)\r\ncontinue;\r\nVM_BUG_ON(PageTransCompound(page));\r\ncc->finished_update_migrate = true;\r\ndel_page_from_lru_list(page, lruvec, page_lru(page));\r\nlist_add(&page->lru, migratelist);\r\ncc->nr_migratepages++;\r\nnr_isolated++;\r\ncheck_compact_cluster:\r\nif (cc->nr_migratepages == COMPACT_CLUSTER_MAX) {\r\n++low_pfn;\r\nbreak;\r\n}\r\ncontinue;\r\nnext_pageblock:\r\nlow_pfn = ALIGN(low_pfn + 1, pageblock_nr_pages) - 1;\r\nlast_pageblock_nr = pageblock_nr;\r\n}\r\nacct_isolated(zone, locked, cc);\r\nif (locked)\r\nspin_unlock_irqrestore(&zone->lru_lock, flags);\r\nif (low_pfn == end_pfn)\r\nupdate_pageblock_skip(cc, valid_page, nr_isolated, true);\r\ntrace_mm_compaction_isolate_migratepages(nr_scanned, nr_isolated);\r\ncount_compact_events(COMPACTMIGRATE_SCANNED, nr_scanned);\r\nif (nr_isolated)\r\ncount_compact_events(COMPACTISOLATED, nr_isolated);\r\nreturn low_pfn;\r\n}\r\nstatic void isolate_freepages(struct zone *zone,\r\nstruct compact_control *cc)\r\n{\r\nstruct page *page;\r\nunsigned long high_pfn, low_pfn, pfn, z_end_pfn, end_pfn;\r\nint nr_freepages = cc->nr_freepages;\r\nstruct list_head *freelist = &cc->freepages;\r\npfn = cc->free_pfn;\r\nlow_pfn = cc->migrate_pfn + pageblock_nr_pages;\r\nhigh_pfn = min(low_pfn, pfn);\r\nz_end_pfn = zone_end_pfn(zone);\r\nfor (; pfn > low_pfn && cc->nr_migratepages > nr_freepages;\r\npfn -= pageblock_nr_pages) {\r\nunsigned long isolated;\r\nif (!pfn_valid(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (page_zone(page) != zone)\r\ncontinue;\r\nif (!suitable_migration_target(page))\r\ncontinue;\r\nif (!isolation_suitable(cc, page))\r\ncontinue;\r\nisolated = 0;\r\nend_pfn = ALIGN(pfn + 1, pageblock_nr_pages);\r\nend_pfn = min(end_pfn, z_end_pfn);\r\nisolated = isolate_freepages_block(cc, pfn, end_pfn,\r\nfreelist, false);\r\nnr_freepages += isolated;\r\nif (isolated) {\r\ncc->finished_update_free = true;\r\nhigh_pfn = max(high_pfn, pfn);\r\n}\r\n}\r\nmap_pages(freelist);\r\ncc->free_pfn = high_pfn;\r\ncc->nr_freepages = nr_freepages;\r\n}\r\nstatic struct page *compaction_alloc(struct page *migratepage,\r\nunsigned long data,\r\nint **result)\r\n{\r\nstruct compact_control *cc = (struct compact_control *)data;\r\nstruct page *freepage;\r\nif (list_empty(&cc->freepages)) {\r\nisolate_freepages(cc->zone, cc);\r\nif (list_empty(&cc->freepages))\r\nreturn NULL;\r\n}\r\nfreepage = list_entry(cc->freepages.next, struct page, lru);\r\nlist_del(&freepage->lru);\r\ncc->nr_freepages--;\r\nreturn freepage;\r\n}\r\nstatic void update_nr_listpages(struct compact_control *cc)\r\n{\r\nint nr_migratepages = 0;\r\nint nr_freepages = 0;\r\nstruct page *page;\r\nlist_for_each_entry(page, &cc->migratepages, lru)\r\nnr_migratepages++;\r\nlist_for_each_entry(page, &cc->freepages, lru)\r\nnr_freepages++;\r\ncc->nr_migratepages = nr_migratepages;\r\ncc->nr_freepages = nr_freepages;\r\n}\r\nstatic isolate_migrate_t isolate_migratepages(struct zone *zone,\r\nstruct compact_control *cc)\r\n{\r\nunsigned long low_pfn, end_pfn;\r\nlow_pfn = max(cc->migrate_pfn, zone->zone_start_pfn);\r\nend_pfn = ALIGN(low_pfn + 1, pageblock_nr_pages);\r\nif (end_pfn > cc->free_pfn || !pfn_valid(low_pfn)) {\r\ncc->migrate_pfn = end_pfn;\r\nreturn ISOLATE_NONE;\r\n}\r\nlow_pfn = isolate_migratepages_range(zone, cc, low_pfn, end_pfn, false);\r\nif (!low_pfn || cc->contended)\r\nreturn ISOLATE_ABORT;\r\ncc->migrate_pfn = low_pfn;\r\nreturn ISOLATE_SUCCESS;\r\n}\r\nstatic int compact_finished(struct zone *zone,\r\nstruct compact_control *cc)\r\n{\r\nunsigned int order;\r\nunsigned long watermark;\r\nif (fatal_signal_pending(current))\r\nreturn COMPACT_PARTIAL;\r\nif (cc->free_pfn <= cc->migrate_pfn) {\r\nif (!current_is_kswapd())\r\nzone->compact_blockskip_flush = true;\r\nreturn COMPACT_COMPLETE;\r\n}\r\nif (cc->order == -1)\r\nreturn COMPACT_CONTINUE;\r\nwatermark = low_wmark_pages(zone);\r\nwatermark += (1 << cc->order);\r\nif (!zone_watermark_ok(zone, cc->order, watermark, 0, 0))\r\nreturn COMPACT_CONTINUE;\r\nfor (order = cc->order; order < MAX_ORDER; order++) {\r\nstruct free_area *area = &zone->free_area[order];\r\nif (!list_empty(&area->free_list[cc->migratetype]))\r\nreturn COMPACT_PARTIAL;\r\nif (cc->order >= pageblock_order && area->nr_free)\r\nreturn COMPACT_PARTIAL;\r\n}\r\nreturn COMPACT_CONTINUE;\r\n}\r\nunsigned long compaction_suitable(struct zone *zone, int order)\r\n{\r\nint fragindex;\r\nunsigned long watermark;\r\nif (order == -1)\r\nreturn COMPACT_CONTINUE;\r\nwatermark = low_wmark_pages(zone) + (2UL << order);\r\nif (!zone_watermark_ok(zone, 0, watermark, 0, 0))\r\nreturn COMPACT_SKIPPED;\r\nfragindex = fragmentation_index(zone, order);\r\nif (fragindex >= 0 && fragindex <= sysctl_extfrag_threshold)\r\nreturn COMPACT_SKIPPED;\r\nif (fragindex == -1000 && zone_watermark_ok(zone, order, watermark,\r\n0, 0))\r\nreturn COMPACT_PARTIAL;\r\nreturn COMPACT_CONTINUE;\r\n}\r\nstatic int compact_zone(struct zone *zone, struct compact_control *cc)\r\n{\r\nint ret;\r\nunsigned long start_pfn = zone->zone_start_pfn;\r\nunsigned long end_pfn = zone_end_pfn(zone);\r\nret = compaction_suitable(zone, cc->order);\r\nswitch (ret) {\r\ncase COMPACT_PARTIAL:\r\ncase COMPACT_SKIPPED:\r\nreturn ret;\r\ncase COMPACT_CONTINUE:\r\n;\r\n}\r\ncc->migrate_pfn = zone->compact_cached_migrate_pfn;\r\ncc->free_pfn = zone->compact_cached_free_pfn;\r\nif (cc->free_pfn < start_pfn || cc->free_pfn > end_pfn) {\r\ncc->free_pfn = end_pfn & ~(pageblock_nr_pages-1);\r\nzone->compact_cached_free_pfn = cc->free_pfn;\r\n}\r\nif (cc->migrate_pfn < start_pfn || cc->migrate_pfn > end_pfn) {\r\ncc->migrate_pfn = start_pfn;\r\nzone->compact_cached_migrate_pfn = cc->migrate_pfn;\r\n}\r\nif (compaction_restarting(zone, cc->order) && !current_is_kswapd())\r\n__reset_isolation_suitable(zone);\r\nmigrate_prep_local();\r\nwhile ((ret = compact_finished(zone, cc)) == COMPACT_CONTINUE) {\r\nunsigned long nr_migrate, nr_remaining;\r\nint err;\r\nswitch (isolate_migratepages(zone, cc)) {\r\ncase ISOLATE_ABORT:\r\nret = COMPACT_PARTIAL;\r\nputback_movable_pages(&cc->migratepages);\r\ncc->nr_migratepages = 0;\r\ngoto out;\r\ncase ISOLATE_NONE:\r\ncontinue;\r\ncase ISOLATE_SUCCESS:\r\n;\r\n}\r\nnr_migrate = cc->nr_migratepages;\r\nerr = migrate_pages(&cc->migratepages, compaction_alloc,\r\n(unsigned long)cc,\r\ncc->sync ? MIGRATE_SYNC_LIGHT : MIGRATE_ASYNC,\r\nMR_COMPACTION);\r\nupdate_nr_listpages(cc);\r\nnr_remaining = cc->nr_migratepages;\r\ntrace_mm_compaction_migratepages(nr_migrate - nr_remaining,\r\nnr_remaining);\r\nif (err) {\r\nputback_movable_pages(&cc->migratepages);\r\ncc->nr_migratepages = 0;\r\nif (err == -ENOMEM) {\r\nret = COMPACT_PARTIAL;\r\ngoto out;\r\n}\r\n}\r\n}\r\nout:\r\ncc->nr_freepages -= release_freepages(&cc->freepages);\r\nVM_BUG_ON(cc->nr_freepages != 0);\r\nreturn ret;\r\n}\r\nstatic unsigned long compact_zone_order(struct zone *zone,\r\nint order, gfp_t gfp_mask,\r\nbool sync, bool *contended)\r\n{\r\nunsigned long ret;\r\nstruct compact_control cc = {\r\n.nr_freepages = 0,\r\n.nr_migratepages = 0,\r\n.order = order,\r\n.migratetype = allocflags_to_migratetype(gfp_mask),\r\n.zone = zone,\r\n.sync = sync,\r\n};\r\nINIT_LIST_HEAD(&cc.freepages);\r\nINIT_LIST_HEAD(&cc.migratepages);\r\nret = compact_zone(zone, &cc);\r\nVM_BUG_ON(!list_empty(&cc.freepages));\r\nVM_BUG_ON(!list_empty(&cc.migratepages));\r\n*contended = cc.contended;\r\nreturn ret;\r\n}\r\nunsigned long try_to_compact_pages(struct zonelist *zonelist,\r\nint order, gfp_t gfp_mask, nodemask_t *nodemask,\r\nbool sync, bool *contended)\r\n{\r\nenum zone_type high_zoneidx = gfp_zone(gfp_mask);\r\nint may_enter_fs = gfp_mask & __GFP_FS;\r\nint may_perform_io = gfp_mask & __GFP_IO;\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\nint rc = COMPACT_SKIPPED;\r\nint alloc_flags = 0;\r\nif (!order || !may_enter_fs || !may_perform_io)\r\nreturn rc;\r\ncount_compact_event(COMPACTSTALL);\r\n#ifdef CONFIG_CMA\r\nif (allocflags_to_migratetype(gfp_mask) == MIGRATE_MOVABLE)\r\nalloc_flags |= ALLOC_CMA;\r\n#endif\r\nfor_each_zone_zonelist_nodemask(zone, z, zonelist, high_zoneidx,\r\nnodemask) {\r\nint status;\r\nstatus = compact_zone_order(zone, order, gfp_mask, sync,\r\ncontended);\r\nrc = max(status, rc);\r\nif (zone_watermark_ok(zone, order, low_wmark_pages(zone), 0,\r\nalloc_flags))\r\nbreak;\r\n}\r\nreturn rc;\r\n}\r\nstatic void __compact_pgdat(pg_data_t *pgdat, struct compact_control *cc)\r\n{\r\nint zoneid;\r\nstruct zone *zone;\r\nfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\r\nzone = &pgdat->node_zones[zoneid];\r\nif (!populated_zone(zone))\r\ncontinue;\r\ncc->nr_freepages = 0;\r\ncc->nr_migratepages = 0;\r\ncc->zone = zone;\r\nINIT_LIST_HEAD(&cc->freepages);\r\nINIT_LIST_HEAD(&cc->migratepages);\r\nif (cc->order == -1 || !compaction_deferred(zone, cc->order))\r\ncompact_zone(zone, cc);\r\nif (cc->order > 0) {\r\nint ok = zone_watermark_ok(zone, cc->order,\r\nlow_wmark_pages(zone), 0, 0);\r\nif (ok && cc->order >= zone->compact_order_failed)\r\nzone->compact_order_failed = cc->order + 1;\r\nelse if (!ok && cc->sync)\r\ndefer_compaction(zone, cc->order);\r\n}\r\nVM_BUG_ON(!list_empty(&cc->freepages));\r\nVM_BUG_ON(!list_empty(&cc->migratepages));\r\n}\r\n}\r\nvoid compact_pgdat(pg_data_t *pgdat, int order)\r\n{\r\nstruct compact_control cc = {\r\n.order = order,\r\n.sync = false,\r\n};\r\n__compact_pgdat(pgdat, &cc);\r\n}\r\nstatic void compact_node(int nid)\r\n{\r\nstruct compact_control cc = {\r\n.order = -1,\r\n.sync = true,\r\n};\r\n__compact_pgdat(NODE_DATA(nid), &cc);\r\n}\r\nstatic void compact_nodes(void)\r\n{\r\nint nid;\r\nlru_add_drain_all();\r\nfor_each_online_node(nid)\r\ncompact_node(nid);\r\n}\r\nint sysctl_compaction_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *length, loff_t *ppos)\r\n{\r\nif (write)\r\ncompact_nodes();\r\nreturn 0;\r\n}\r\nint sysctl_extfrag_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *length, loff_t *ppos)\r\n{\r\nproc_dointvec_minmax(table, write, buffer, length, ppos);\r\nreturn 0;\r\n}\r\nssize_t sysfs_compact_node(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nint nid = dev->id;\r\nif (nid >= 0 && nid < nr_node_ids && node_online(nid)) {\r\nlru_add_drain_all();\r\ncompact_node(nid);\r\n}\r\nreturn count;\r\n}\r\nint compaction_register_node(struct node *node)\r\n{\r\nreturn device_create_file(&node->dev, &dev_attr_compact);\r\n}\r\nvoid compaction_unregister_node(struct node *node)\r\n{\r\nreturn device_remove_file(&node->dev, &dev_attr_compact);\r\n}
