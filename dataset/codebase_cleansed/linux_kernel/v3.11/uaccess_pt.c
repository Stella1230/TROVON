static size_t strnlen_kernel(size_t count, const char __user *src)\r\n{\r\nregister unsigned long reg0 asm("0") = 0UL;\r\nunsigned long tmp1, tmp2;\r\nasm volatile(\r\n" la %2,0(%1)\n"\r\n" la %3,0(%0,%1)\n"\r\n" "SLR" %0,%0\n"\r\n"0: srst %3,%2\n"\r\n" jo 0b\n"\r\n" la %0,1(%3)\n"\r\n" "SLR" %0,%1\n"\r\n"1:\n"\r\nEX_TABLE(0b,1b)\r\n: "+a" (count), "+a" (src), "=a" (tmp1), "=a" (tmp2)\r\n: "d" (reg0) : "cc", "memory");\r\nreturn count;\r\n}\r\nstatic size_t copy_in_kernel(size_t count, void __user *to,\r\nconst void __user *from)\r\n{\r\nunsigned long tmp1;\r\nasm volatile(\r\n" "AHI" %0,-1\n"\r\n" jo 5f\n"\r\n" bras %3,3f\n"\r\n"0:"AHI" %0,257\n"\r\n"1: mvc 0(1,%1),0(%2)\n"\r\n" la %1,1(%1)\n"\r\n" la %2,1(%2)\n"\r\n" "AHI" %0,-1\n"\r\n" jnz 1b\n"\r\n" j 5f\n"\r\n"2: mvc 0(256,%1),0(%2)\n"\r\n" la %1,256(%1)\n"\r\n" la %2,256(%2)\n"\r\n"3:"AHI" %0,-256\n"\r\n" jnm 2b\n"\r\n"4: ex %0,1b-0b(%3)\n"\r\n"5:"SLR" %0,%0\n"\r\n"6:\n"\r\nEX_TABLE(1b,6b) EX_TABLE(2b,0b) EX_TABLE(4b,0b)\r\n: "+a" (count), "+a" (to), "+a" (from), "=a" (tmp1)\r\n: : "cc", "memory");\r\nreturn count;\r\n}\r\nstatic unsigned long follow_table(struct mm_struct *mm,\r\nunsigned long address, int write)\r\n{\r\nunsigned long *table = (unsigned long *)__pa(mm->pgd);\r\nswitch (mm->context.asce_bits & _ASCE_TYPE_MASK) {\r\ncase _ASCE_TYPE_REGION1:\r\ntable = table + ((address >> 53) & 0x7ff);\r\nif (unlikely(*table & _REGION_ENTRY_INV))\r\nreturn -0x39UL;\r\ntable = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);\r\ncase _ASCE_TYPE_REGION2:\r\ntable = table + ((address >> 42) & 0x7ff);\r\nif (unlikely(*table & _REGION_ENTRY_INV))\r\nreturn -0x3aUL;\r\ntable = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);\r\ncase _ASCE_TYPE_REGION3:\r\ntable = table + ((address >> 31) & 0x7ff);\r\nif (unlikely(*table & _REGION_ENTRY_INV))\r\nreturn -0x3bUL;\r\ntable = (unsigned long *)(*table & _REGION_ENTRY_ORIGIN);\r\ncase _ASCE_TYPE_SEGMENT:\r\ntable = table + ((address >> 20) & 0x7ff);\r\nif (unlikely(*table & _SEGMENT_ENTRY_INV))\r\nreturn -0x10UL;\r\nif (unlikely(*table & _SEGMENT_ENTRY_LARGE)) {\r\nif (write && (*table & _SEGMENT_ENTRY_RO))\r\nreturn -0x04UL;\r\nreturn (*table & _SEGMENT_ENTRY_ORIGIN_LARGE) +\r\n(address & ~_SEGMENT_ENTRY_ORIGIN_LARGE);\r\n}\r\ntable = (unsigned long *)(*table & _SEGMENT_ENTRY_ORIGIN);\r\n}\r\ntable = table + ((address >> 12) & 0xff);\r\nif (unlikely(*table & _PAGE_INVALID))\r\nreturn -0x11UL;\r\nif (write && (*table & _PAGE_RO))\r\nreturn -0x04UL;\r\nreturn (*table & PAGE_MASK) + (address & ~PAGE_MASK);\r\n}\r\nstatic unsigned long follow_table(struct mm_struct *mm,\r\nunsigned long address, int write)\r\n{\r\nunsigned long *table = (unsigned long *)__pa(mm->pgd);\r\ntable = table + ((address >> 20) & 0x7ff);\r\nif (unlikely(*table & _SEGMENT_ENTRY_INV))\r\nreturn -0x10UL;\r\ntable = (unsigned long *)(*table & _SEGMENT_ENTRY_ORIGIN);\r\ntable = table + ((address >> 12) & 0xff);\r\nif (unlikely(*table & _PAGE_INVALID))\r\nreturn -0x11UL;\r\nif (write && (*table & _PAGE_RO))\r\nreturn -0x04UL;\r\nreturn (*table & PAGE_MASK) + (address & ~PAGE_MASK);\r\n}\r\nstatic __always_inline size_t __user_copy_pt(unsigned long uaddr, void *kptr,\r\nsize_t n, int write_user)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long offset, done, size, kaddr;\r\nvoid *from, *to;\r\ndone = 0;\r\nretry:\r\nspin_lock(&mm->page_table_lock);\r\ndo {\r\nkaddr = follow_table(mm, uaddr, write_user);\r\nif (IS_ERR_VALUE(kaddr))\r\ngoto fault;\r\noffset = uaddr & ~PAGE_MASK;\r\nsize = min(n - done, PAGE_SIZE - offset);\r\nif (write_user) {\r\nto = (void *) kaddr;\r\nfrom = kptr + done;\r\n} else {\r\nfrom = (void *) kaddr;\r\nto = kptr + done;\r\n}\r\nmemcpy(to, from, size);\r\ndone += size;\r\nuaddr += size;\r\n} while (done < n);\r\nspin_unlock(&mm->page_table_lock);\r\nreturn n - done;\r\nfault:\r\nspin_unlock(&mm->page_table_lock);\r\nif (__handle_fault(uaddr, -kaddr, write_user))\r\nreturn n - done;\r\ngoto retry;\r\n}\r\nstatic __always_inline unsigned long __dat_user_addr(unsigned long uaddr,\r\nint write)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long kaddr;\r\nint rc;\r\nretry:\r\nkaddr = follow_table(mm, uaddr, write);\r\nif (IS_ERR_VALUE(kaddr))\r\ngoto fault;\r\nreturn kaddr;\r\nfault:\r\nspin_unlock(&mm->page_table_lock);\r\nrc = __handle_fault(uaddr, -kaddr, write);\r\nspin_lock(&mm->page_table_lock);\r\nif (!rc)\r\ngoto retry;\r\nreturn 0;\r\n}\r\nsize_t copy_from_user_pt(size_t n, const void __user *from, void *to)\r\n{\r\nsize_t rc;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nreturn copy_in_kernel(n, (void __user *) to, from);\r\nrc = __user_copy_pt((unsigned long) from, to, n, 0);\r\nif (unlikely(rc))\r\nmemset(to + n - rc, 0, rc);\r\nreturn rc;\r\n}\r\nsize_t copy_to_user_pt(size_t n, void __user *to, const void *from)\r\n{\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nreturn copy_in_kernel(n, to, (void __user *) from);\r\nreturn __user_copy_pt((unsigned long) to, (void *) from, n, 1);\r\n}\r\nstatic size_t clear_user_pt(size_t n, void __user *to)\r\n{\r\nvoid *zpage = (void *) empty_zero_page;\r\nlong done, size, ret;\r\ndone = 0;\r\ndo {\r\nif (n - done > PAGE_SIZE)\r\nsize = PAGE_SIZE;\r\nelse\r\nsize = n - done;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nret = copy_in_kernel(n, to, (void __user *) zpage);\r\nelse\r\nret = __user_copy_pt((unsigned long) to, zpage, size, 1);\r\ndone += size;\r\nto += size;\r\nif (ret)\r\nreturn ret + n - done;\r\n} while (done < n);\r\nreturn 0;\r\n}\r\nstatic size_t strnlen_user_pt(size_t count, const char __user *src)\r\n{\r\nunsigned long uaddr = (unsigned long) src;\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long offset, done, len, kaddr;\r\nsize_t len_str;\r\nif (unlikely(!count))\r\nreturn 0;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nreturn strnlen_kernel(count, src);\r\ndone = 0;\r\nretry:\r\nspin_lock(&mm->page_table_lock);\r\ndo {\r\nkaddr = follow_table(mm, uaddr, 0);\r\nif (IS_ERR_VALUE(kaddr))\r\ngoto fault;\r\noffset = uaddr & ~PAGE_MASK;\r\nlen = min(count - done, PAGE_SIZE - offset);\r\nlen_str = strnlen((char *) kaddr, len);\r\ndone += len_str;\r\nuaddr += len_str;\r\n} while ((len_str == len) && (done < count));\r\nspin_unlock(&mm->page_table_lock);\r\nreturn done + 1;\r\nfault:\r\nspin_unlock(&mm->page_table_lock);\r\nif (__handle_fault(uaddr, -kaddr, 0))\r\nreturn 0;\r\ngoto retry;\r\n}\r\nstatic size_t strncpy_from_user_pt(size_t count, const char __user *src,\r\nchar *dst)\r\n{\r\nsize_t done, len, offset, len_str;\r\nif (unlikely(!count))\r\nreturn 0;\r\ndone = 0;\r\ndo {\r\noffset = (size_t)src & ~PAGE_MASK;\r\nlen = min(count - done, PAGE_SIZE - offset);\r\nif (segment_eq(get_fs(), KERNEL_DS)) {\r\nif (copy_in_kernel(len, (void __user *) dst, src))\r\nreturn -EFAULT;\r\n} else {\r\nif (__user_copy_pt((unsigned long) src, dst, len, 0))\r\nreturn -EFAULT;\r\n}\r\nlen_str = strnlen(dst, len);\r\ndone += len_str;\r\nsrc += len_str;\r\ndst += len_str;\r\n} while ((len_str == len) && (done < count));\r\nreturn done;\r\n}\r\nstatic size_t copy_in_user_pt(size_t n, void __user *to,\r\nconst void __user *from)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long offset_max, uaddr, done, size, error_code;\r\nunsigned long uaddr_from = (unsigned long) from;\r\nunsigned long uaddr_to = (unsigned long) to;\r\nunsigned long kaddr_to, kaddr_from;\r\nint write_user;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nreturn copy_in_kernel(n, to, from);\r\ndone = 0;\r\nretry:\r\nspin_lock(&mm->page_table_lock);\r\ndo {\r\nwrite_user = 0;\r\nuaddr = uaddr_from;\r\nkaddr_from = follow_table(mm, uaddr_from, 0);\r\nerror_code = kaddr_from;\r\nif (IS_ERR_VALUE(error_code))\r\ngoto fault;\r\nwrite_user = 1;\r\nuaddr = uaddr_to;\r\nkaddr_to = follow_table(mm, uaddr_to, 1);\r\nerror_code = (unsigned long) kaddr_to;\r\nif (IS_ERR_VALUE(error_code))\r\ngoto fault;\r\noffset_max = max(uaddr_from & ~PAGE_MASK,\r\nuaddr_to & ~PAGE_MASK);\r\nsize = min(n - done, PAGE_SIZE - offset_max);\r\nmemcpy((void *) kaddr_to, (void *) kaddr_from, size);\r\ndone += size;\r\nuaddr_from += size;\r\nuaddr_to += size;\r\n} while (done < n);\r\nspin_unlock(&mm->page_table_lock);\r\nreturn n - done;\r\nfault:\r\nspin_unlock(&mm->page_table_lock);\r\nif (__handle_fault(uaddr, -error_code, write_user))\r\nreturn n - done;\r\ngoto retry;\r\n}\r\nstatic int __futex_atomic_op_pt(int op, u32 __user *uaddr, int oparg, int *old)\r\n{\r\nint oldval = 0, newval, ret;\r\nswitch (op) {\r\ncase FUTEX_OP_SET:\r\n__futex_atomic_op("lr %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ncase FUTEX_OP_ADD:\r\n__futex_atomic_op("lr %2,%1\nar %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ncase FUTEX_OP_OR:\r\n__futex_atomic_op("lr %2,%1\nor %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ncase FUTEX_OP_ANDN:\r\n__futex_atomic_op("lr %2,%1\nnr %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ncase FUTEX_OP_XOR:\r\n__futex_atomic_op("lr %2,%1\nxr %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ndefault:\r\nret = -ENOSYS;\r\n}\r\nif (ret == 0)\r\n*old = oldval;\r\nreturn ret;\r\n}\r\nint futex_atomic_op_pt(int op, u32 __user *uaddr, int oparg, int *old)\r\n{\r\nint ret;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nreturn __futex_atomic_op_pt(op, uaddr, oparg, old);\r\nspin_lock(&current->mm->page_table_lock);\r\nuaddr = (u32 __force __user *)\r\n__dat_user_addr((__force unsigned long) uaddr, 1);\r\nif (!uaddr) {\r\nspin_unlock(&current->mm->page_table_lock);\r\nreturn -EFAULT;\r\n}\r\nget_page(virt_to_page(uaddr));\r\nspin_unlock(&current->mm->page_table_lock);\r\nret = __futex_atomic_op_pt(op, uaddr, oparg, old);\r\nput_page(virt_to_page(uaddr));\r\nreturn ret;\r\n}\r\nstatic int __futex_atomic_cmpxchg_pt(u32 *uval, u32 __user *uaddr,\r\nu32 oldval, u32 newval)\r\n{\r\nint ret;\r\nasm volatile("0: cs %1,%4,0(%5)\n"\r\n"1: la %0,0\n"\r\n"2:\n"\r\nEX_TABLE(0b,2b) EX_TABLE(1b,2b)\r\n: "=d" (ret), "+d" (oldval), "=m" (*uaddr)\r\n: "0" (-EFAULT), "d" (newval), "a" (uaddr), "m" (*uaddr)\r\n: "cc", "memory" );\r\n*uval = oldval;\r\nreturn ret;\r\n}\r\nint futex_atomic_cmpxchg_pt(u32 *uval, u32 __user *uaddr,\r\nu32 oldval, u32 newval)\r\n{\r\nint ret;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nreturn __futex_atomic_cmpxchg_pt(uval, uaddr, oldval, newval);\r\nspin_lock(&current->mm->page_table_lock);\r\nuaddr = (u32 __force __user *)\r\n__dat_user_addr((__force unsigned long) uaddr, 1);\r\nif (!uaddr) {\r\nspin_unlock(&current->mm->page_table_lock);\r\nreturn -EFAULT;\r\n}\r\nget_page(virt_to_page(uaddr));\r\nspin_unlock(&current->mm->page_table_lock);\r\nret = __futex_atomic_cmpxchg_pt(uval, uaddr, oldval, newval);\r\nput_page(virt_to_page(uaddr));\r\nreturn ret;\r\n}
