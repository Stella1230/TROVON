static int need_fallback(unsigned int key_len)\r\n{\r\nswitch (key_len) {\r\ncase 16:\r\nif (!(keylen_flag & AES_KEYLEN_128))\r\nreturn 1;\r\nbreak;\r\ncase 24:\r\nif (!(keylen_flag & AES_KEYLEN_192))\r\nreturn 1;\r\nbreak;\r\ncase 32:\r\nif (!(keylen_flag & AES_KEYLEN_256))\r\nreturn 1;\r\nbreak;\r\ndefault:\r\nreturn -1;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int setkey_fallback_cip(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nint ret;\r\nsctx->fallback.cip->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;\r\nsctx->fallback.cip->base.crt_flags |= (tfm->crt_flags &\r\nCRYPTO_TFM_REQ_MASK);\r\nret = crypto_cipher_setkey(sctx->fallback.cip, in_key, key_len);\r\nif (ret) {\r\ntfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm->crt_flags |= (sctx->fallback.cip->base.crt_flags &\r\nCRYPTO_TFM_RES_MASK);\r\n}\r\nreturn ret;\r\n}\r\nstatic int aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nu32 *flags = &tfm->crt_flags;\r\nint ret;\r\nret = need_fallback(key_len);\r\nif (ret < 0) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nsctx->key_len = key_len;\r\nif (!ret) {\r\nmemcpy(sctx->key, in_key, key_len);\r\nreturn 0;\r\n}\r\nreturn setkey_fallback_cip(tfm, in_key, key_len);\r\n}\r\nstatic void aes_encrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\r\n{\r\nconst struct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nif (unlikely(need_fallback(sctx->key_len))) {\r\ncrypto_cipher_encrypt_one(sctx->fallback.cip, out, in);\r\nreturn;\r\n}\r\nswitch (sctx->key_len) {\r\ncase 16:\r\ncrypt_s390_km(KM_AES_128_ENCRYPT, &sctx->key, out, in,\r\nAES_BLOCK_SIZE);\r\nbreak;\r\ncase 24:\r\ncrypt_s390_km(KM_AES_192_ENCRYPT, &sctx->key, out, in,\r\nAES_BLOCK_SIZE);\r\nbreak;\r\ncase 32:\r\ncrypt_s390_km(KM_AES_256_ENCRYPT, &sctx->key, out, in,\r\nAES_BLOCK_SIZE);\r\nbreak;\r\n}\r\n}\r\nstatic void aes_decrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\r\n{\r\nconst struct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nif (unlikely(need_fallback(sctx->key_len))) {\r\ncrypto_cipher_decrypt_one(sctx->fallback.cip, out, in);\r\nreturn;\r\n}\r\nswitch (sctx->key_len) {\r\ncase 16:\r\ncrypt_s390_km(KM_AES_128_DECRYPT, &sctx->key, out, in,\r\nAES_BLOCK_SIZE);\r\nbreak;\r\ncase 24:\r\ncrypt_s390_km(KM_AES_192_DECRYPT, &sctx->key, out, in,\r\nAES_BLOCK_SIZE);\r\nbreak;\r\ncase 32:\r\ncrypt_s390_km(KM_AES_256_DECRYPT, &sctx->key, out, in,\r\nAES_BLOCK_SIZE);\r\nbreak;\r\n}\r\n}\r\nstatic int fallback_init_cip(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = tfm->__crt_alg->cra_name;\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nsctx->fallback.cip = crypto_alloc_cipher(name, 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(sctx->fallback.cip)) {\r\npr_err("Allocating AES fallback algorithm %s failed\n",\r\nname);\r\nreturn PTR_ERR(sctx->fallback.cip);\r\n}\r\nreturn 0;\r\n}\r\nstatic void fallback_exit_cip(struct crypto_tfm *tfm)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_cipher(sctx->fallback.cip);\r\nsctx->fallback.cip = NULL;\r\n}\r\nstatic int setkey_fallback_blk(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nunsigned int ret;\r\nsctx->fallback.blk->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;\r\nsctx->fallback.blk->base.crt_flags |= (tfm->crt_flags &\r\nCRYPTO_TFM_REQ_MASK);\r\nret = crypto_blkcipher_setkey(sctx->fallback.blk, key, len);\r\nif (ret) {\r\ntfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm->crt_flags |= (sctx->fallback.blk->base.crt_flags &\r\nCRYPTO_TFM_RES_MASK);\r\n}\r\nreturn ret;\r\n}\r\nstatic int fallback_blk_dec(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nunsigned int ret;\r\nstruct crypto_blkcipher *tfm;\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\ntfm = desc->tfm;\r\ndesc->tfm = sctx->fallback.blk;\r\nret = crypto_blkcipher_decrypt_iv(desc, dst, src, nbytes);\r\ndesc->tfm = tfm;\r\nreturn ret;\r\n}\r\nstatic int fallback_blk_enc(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nunsigned int ret;\r\nstruct crypto_blkcipher *tfm;\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\ntfm = desc->tfm;\r\ndesc->tfm = sctx->fallback.blk;\r\nret = crypto_blkcipher_encrypt_iv(desc, dst, src, nbytes);\r\ndesc->tfm = tfm;\r\nreturn ret;\r\n}\r\nstatic int ecb_aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nint ret;\r\nret = need_fallback(key_len);\r\nif (ret > 0) {\r\nsctx->key_len = key_len;\r\nreturn setkey_fallback_blk(tfm, in_key, key_len);\r\n}\r\nswitch (key_len) {\r\ncase 16:\r\nsctx->enc = KM_AES_128_ENCRYPT;\r\nsctx->dec = KM_AES_128_DECRYPT;\r\nbreak;\r\ncase 24:\r\nsctx->enc = KM_AES_192_ENCRYPT;\r\nsctx->dec = KM_AES_192_DECRYPT;\r\nbreak;\r\ncase 32:\r\nsctx->enc = KM_AES_256_ENCRYPT;\r\nsctx->dec = KM_AES_256_DECRYPT;\r\nbreak;\r\n}\r\nreturn aes_set_key(tfm, in_key, key_len);\r\n}\r\nstatic int ecb_aes_crypt(struct blkcipher_desc *desc, long func, void *param,\r\nstruct blkcipher_walk *walk)\r\n{\r\nint ret = blkcipher_walk_virt(desc, walk);\r\nunsigned int nbytes;\r\nwhile ((nbytes = walk->nbytes)) {\r\nunsigned int n = nbytes & ~(AES_BLOCK_SIZE - 1);\r\nu8 *out = walk->dst.virt.addr;\r\nu8 *in = walk->src.virt.addr;\r\nret = crypt_s390_km(func, param, out, in, n);\r\nif (ret < 0 || ret != n)\r\nreturn -EIO;\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nret = blkcipher_walk_done(desc, walk, nbytes);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ecb_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(need_fallback(sctx->key_len)))\r\nreturn fallback_blk_enc(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ecb_aes_crypt(desc, sctx->enc, sctx->key, &walk);\r\n}\r\nstatic int ecb_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(need_fallback(sctx->key_len)))\r\nreturn fallback_blk_dec(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ecb_aes_crypt(desc, sctx->dec, sctx->key, &walk);\r\n}\r\nstatic int fallback_init_blk(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = tfm->__crt_alg->cra_name;\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nsctx->fallback.blk = crypto_alloc_blkcipher(name, 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(sctx->fallback.blk)) {\r\npr_err("Allocating AES fallback algorithm %s failed\n",\r\nname);\r\nreturn PTR_ERR(sctx->fallback.blk);\r\n}\r\nreturn 0;\r\n}\r\nstatic void fallback_exit_blk(struct crypto_tfm *tfm)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_blkcipher(sctx->fallback.blk);\r\nsctx->fallback.blk = NULL;\r\n}\r\nstatic int cbc_aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nint ret;\r\nret = need_fallback(key_len);\r\nif (ret > 0) {\r\nsctx->key_len = key_len;\r\nreturn setkey_fallback_blk(tfm, in_key, key_len);\r\n}\r\nswitch (key_len) {\r\ncase 16:\r\nsctx->enc = KMC_AES_128_ENCRYPT;\r\nsctx->dec = KMC_AES_128_DECRYPT;\r\nbreak;\r\ncase 24:\r\nsctx->enc = KMC_AES_192_ENCRYPT;\r\nsctx->dec = KMC_AES_192_DECRYPT;\r\nbreak;\r\ncase 32:\r\nsctx->enc = KMC_AES_256_ENCRYPT;\r\nsctx->dec = KMC_AES_256_DECRYPT;\r\nbreak;\r\n}\r\nreturn aes_set_key(tfm, in_key, key_len);\r\n}\r\nstatic int cbc_aes_crypt(struct blkcipher_desc *desc, long func, void *param,\r\nstruct blkcipher_walk *walk)\r\n{\r\nint ret = blkcipher_walk_virt(desc, walk);\r\nunsigned int nbytes = walk->nbytes;\r\nif (!nbytes)\r\ngoto out;\r\nmemcpy(param, walk->iv, AES_BLOCK_SIZE);\r\ndo {\r\nunsigned int n = nbytes & ~(AES_BLOCK_SIZE - 1);\r\nu8 *out = walk->dst.virt.addr;\r\nu8 *in = walk->src.virt.addr;\r\nret = crypt_s390_kmc(func, param, out, in, n);\r\nif (ret < 0 || ret != n)\r\nreturn -EIO;\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nret = blkcipher_walk_done(desc, walk, nbytes);\r\n} while ((nbytes = walk->nbytes));\r\nmemcpy(walk->iv, param, AES_BLOCK_SIZE);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int cbc_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(need_fallback(sctx->key_len)))\r\nreturn fallback_blk_enc(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn cbc_aes_crypt(desc, sctx->enc, sctx->iv, &walk);\r\n}\r\nstatic int cbc_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(need_fallback(sctx->key_len)))\r\nreturn fallback_blk_dec(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn cbc_aes_crypt(desc, sctx->dec, sctx->iv, &walk);\r\n}\r\nstatic int xts_fallback_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int len)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_tfm_ctx(tfm);\r\nunsigned int ret;\r\nxts_ctx->fallback->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;\r\nxts_ctx->fallback->base.crt_flags |= (tfm->crt_flags &\r\nCRYPTO_TFM_REQ_MASK);\r\nret = crypto_blkcipher_setkey(xts_ctx->fallback, key, len);\r\nif (ret) {\r\ntfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm->crt_flags |= (xts_ctx->fallback->base.crt_flags &\r\nCRYPTO_TFM_RES_MASK);\r\n}\r\nreturn ret;\r\n}\r\nstatic int xts_fallback_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct crypto_blkcipher *tfm;\r\nunsigned int ret;\r\ntfm = desc->tfm;\r\ndesc->tfm = xts_ctx->fallback;\r\nret = crypto_blkcipher_decrypt_iv(desc, dst, src, nbytes);\r\ndesc->tfm = tfm;\r\nreturn ret;\r\n}\r\nstatic int xts_fallback_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct crypto_blkcipher *tfm;\r\nunsigned int ret;\r\ntfm = desc->tfm;\r\ndesc->tfm = xts_ctx->fallback;\r\nret = crypto_blkcipher_encrypt_iv(desc, dst, src, nbytes);\r\ndesc->tfm = tfm;\r\nreturn ret;\r\n}\r\nstatic int xts_aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_tfm_ctx(tfm);\r\nu32 *flags = &tfm->crt_flags;\r\nswitch (key_len) {\r\ncase 32:\r\nxts_ctx->enc = KM_XTS_128_ENCRYPT;\r\nxts_ctx->dec = KM_XTS_128_DECRYPT;\r\nmemcpy(xts_ctx->key + 16, in_key, 16);\r\nmemcpy(xts_ctx->pcc.key + 16, in_key + 16, 16);\r\nbreak;\r\ncase 48:\r\nxts_ctx->enc = 0;\r\nxts_ctx->dec = 0;\r\nxts_fallback_setkey(tfm, in_key, key_len);\r\nbreak;\r\ncase 64:\r\nxts_ctx->enc = KM_XTS_256_ENCRYPT;\r\nxts_ctx->dec = KM_XTS_256_DECRYPT;\r\nmemcpy(xts_ctx->key, in_key, 32);\r\nmemcpy(xts_ctx->pcc.key, in_key + 32, 32);\r\nbreak;\r\ndefault:\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nxts_ctx->key_len = key_len;\r\nreturn 0;\r\n}\r\nstatic int xts_aes_crypt(struct blkcipher_desc *desc, long func,\r\nstruct s390_xts_ctx *xts_ctx,\r\nstruct blkcipher_walk *walk)\r\n{\r\nunsigned int offset = (xts_ctx->key_len >> 1) & 0x10;\r\nint ret = blkcipher_walk_virt(desc, walk);\r\nunsigned int nbytes = walk->nbytes;\r\nunsigned int n;\r\nu8 *in, *out;\r\nvoid *param;\r\nif (!nbytes)\r\ngoto out;\r\nmemset(xts_ctx->pcc.block, 0, sizeof(xts_ctx->pcc.block));\r\nmemset(xts_ctx->pcc.bit, 0, sizeof(xts_ctx->pcc.bit));\r\nmemset(xts_ctx->pcc.xts, 0, sizeof(xts_ctx->pcc.xts));\r\nmemcpy(xts_ctx->pcc.tweak, walk->iv, sizeof(xts_ctx->pcc.tweak));\r\nparam = xts_ctx->pcc.key + offset;\r\nret = crypt_s390_pcc(func, param);\r\nif (ret < 0)\r\nreturn -EIO;\r\nmemcpy(xts_ctx->xts_param, xts_ctx->pcc.xts, 16);\r\nparam = xts_ctx->key + offset;\r\ndo {\r\nn = nbytes & ~(AES_BLOCK_SIZE - 1);\r\nout = walk->dst.virt.addr;\r\nin = walk->src.virt.addr;\r\nret = crypt_s390_km(func, param, out, in, n);\r\nif (ret < 0 || ret != n)\r\nreturn -EIO;\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nret = blkcipher_walk_done(desc, walk, nbytes);\r\n} while ((nbytes = walk->nbytes));\r\nout:\r\nreturn ret;\r\n}\r\nstatic int xts_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(xts_ctx->key_len == 48))\r\nreturn xts_fallback_encrypt(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn xts_aes_crypt(desc, xts_ctx->enc, xts_ctx, &walk);\r\n}\r\nstatic int xts_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nif (unlikely(xts_ctx->key_len == 48))\r\nreturn xts_fallback_decrypt(desc, dst, src, nbytes);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn xts_aes_crypt(desc, xts_ctx->dec, xts_ctx, &walk);\r\n}\r\nstatic int xts_fallback_init(struct crypto_tfm *tfm)\r\n{\r\nconst char *name = tfm->__crt_alg->cra_name;\r\nstruct s390_xts_ctx *xts_ctx = crypto_tfm_ctx(tfm);\r\nxts_ctx->fallback = crypto_alloc_blkcipher(name, 0,\r\nCRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(xts_ctx->fallback)) {\r\npr_err("Allocating XTS fallback algorithm %s failed\n",\r\nname);\r\nreturn PTR_ERR(xts_ctx->fallback);\r\n}\r\nreturn 0;\r\n}\r\nstatic void xts_fallback_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct s390_xts_ctx *xts_ctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_blkcipher(xts_ctx->fallback);\r\nxts_ctx->fallback = NULL;\r\n}\r\nstatic int ctr_aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_tfm_ctx(tfm);\r\nswitch (key_len) {\r\ncase 16:\r\nsctx->enc = KMCTR_AES_128_ENCRYPT;\r\nsctx->dec = KMCTR_AES_128_DECRYPT;\r\nbreak;\r\ncase 24:\r\nsctx->enc = KMCTR_AES_192_ENCRYPT;\r\nsctx->dec = KMCTR_AES_192_DECRYPT;\r\nbreak;\r\ncase 32:\r\nsctx->enc = KMCTR_AES_256_ENCRYPT;\r\nsctx->dec = KMCTR_AES_256_DECRYPT;\r\nbreak;\r\n}\r\nreturn aes_set_key(tfm, in_key, key_len);\r\n}\r\nstatic int ctr_aes_crypt(struct blkcipher_desc *desc, long func,\r\nstruct s390_aes_ctx *sctx, struct blkcipher_walk *walk)\r\n{\r\nint ret = blkcipher_walk_virt_block(desc, walk, AES_BLOCK_SIZE);\r\nunsigned int i, n, nbytes;\r\nu8 buf[AES_BLOCK_SIZE];\r\nu8 *out, *in;\r\nif (!walk->nbytes)\r\nreturn ret;\r\nmemcpy(ctrblk, walk->iv, AES_BLOCK_SIZE);\r\nwhile ((nbytes = walk->nbytes) >= AES_BLOCK_SIZE) {\r\nout = walk->dst.virt.addr;\r\nin = walk->src.virt.addr;\r\nwhile (nbytes >= AES_BLOCK_SIZE) {\r\nn = (nbytes > PAGE_SIZE) ? PAGE_SIZE :\r\nnbytes & ~(AES_BLOCK_SIZE - 1);\r\nfor (i = AES_BLOCK_SIZE; i < n; i += AES_BLOCK_SIZE) {\r\nmemcpy(ctrblk + i, ctrblk + i - AES_BLOCK_SIZE,\r\nAES_BLOCK_SIZE);\r\ncrypto_inc(ctrblk + i, AES_BLOCK_SIZE);\r\n}\r\nret = crypt_s390_kmctr(func, sctx->key, out, in, n, ctrblk);\r\nif (ret < 0 || ret != n)\r\nreturn -EIO;\r\nif (n > AES_BLOCK_SIZE)\r\nmemcpy(ctrblk, ctrblk + n - AES_BLOCK_SIZE,\r\nAES_BLOCK_SIZE);\r\ncrypto_inc(ctrblk, AES_BLOCK_SIZE);\r\nout += n;\r\nin += n;\r\nnbytes -= n;\r\n}\r\nret = blkcipher_walk_done(desc, walk, nbytes);\r\n}\r\nif (nbytes) {\r\nout = walk->dst.virt.addr;\r\nin = walk->src.virt.addr;\r\nret = crypt_s390_kmctr(func, sctx->key, buf, in,\r\nAES_BLOCK_SIZE, ctrblk);\r\nif (ret < 0 || ret != AES_BLOCK_SIZE)\r\nreturn -EIO;\r\nmemcpy(out, buf, nbytes);\r\ncrypto_inc(ctrblk, AES_BLOCK_SIZE);\r\nret = blkcipher_walk_done(desc, walk, 0);\r\n}\r\nmemcpy(walk->iv, ctrblk, AES_BLOCK_SIZE);\r\nreturn ret;\r\n}\r\nstatic int ctr_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ctr_aes_crypt(desc, sctx->enc, sctx, &walk);\r\n}\r\nstatic int ctr_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct s390_aes_ctx *sctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ctr_aes_crypt(desc, sctx->dec, sctx, &walk);\r\n}\r\nstatic int __init aes_s390_init(void)\r\n{\r\nint ret;\r\nif (crypt_s390_func_available(KM_AES_128_ENCRYPT, CRYPT_S390_MSA))\r\nkeylen_flag |= AES_KEYLEN_128;\r\nif (crypt_s390_func_available(KM_AES_192_ENCRYPT, CRYPT_S390_MSA))\r\nkeylen_flag |= AES_KEYLEN_192;\r\nif (crypt_s390_func_available(KM_AES_256_ENCRYPT, CRYPT_S390_MSA))\r\nkeylen_flag |= AES_KEYLEN_256;\r\nif (!keylen_flag)\r\nreturn -EOPNOTSUPP;\r\nif (keylen_flag == AES_KEYLEN_128)\r\npr_info("AES hardware acceleration is only available for"\r\n" 128-bit keys\n");\r\nret = crypto_register_alg(&aes_alg);\r\nif (ret)\r\ngoto aes_err;\r\nret = crypto_register_alg(&ecb_aes_alg);\r\nif (ret)\r\ngoto ecb_aes_err;\r\nret = crypto_register_alg(&cbc_aes_alg);\r\nif (ret)\r\ngoto cbc_aes_err;\r\nif (crypt_s390_func_available(KM_XTS_128_ENCRYPT,\r\nCRYPT_S390_MSA | CRYPT_S390_MSA4) &&\r\ncrypt_s390_func_available(KM_XTS_256_ENCRYPT,\r\nCRYPT_S390_MSA | CRYPT_S390_MSA4)) {\r\nret = crypto_register_alg(&xts_aes_alg);\r\nif (ret)\r\ngoto xts_aes_err;\r\n}\r\nif (crypt_s390_func_available(KMCTR_AES_128_ENCRYPT,\r\nCRYPT_S390_MSA | CRYPT_S390_MSA4) &&\r\ncrypt_s390_func_available(KMCTR_AES_192_ENCRYPT,\r\nCRYPT_S390_MSA | CRYPT_S390_MSA4) &&\r\ncrypt_s390_func_available(KMCTR_AES_256_ENCRYPT,\r\nCRYPT_S390_MSA | CRYPT_S390_MSA4)) {\r\nctrblk = (u8 *) __get_free_page(GFP_KERNEL);\r\nif (!ctrblk) {\r\nret = -ENOMEM;\r\ngoto ctr_aes_err;\r\n}\r\nret = crypto_register_alg(&ctr_aes_alg);\r\nif (ret) {\r\nfree_page((unsigned long) ctrblk);\r\ngoto ctr_aes_err;\r\n}\r\n}\r\nout:\r\nreturn ret;\r\nctr_aes_err:\r\ncrypto_unregister_alg(&xts_aes_alg);\r\nxts_aes_err:\r\ncrypto_unregister_alg(&cbc_aes_alg);\r\ncbc_aes_err:\r\ncrypto_unregister_alg(&ecb_aes_alg);\r\necb_aes_err:\r\ncrypto_unregister_alg(&aes_alg);\r\naes_err:\r\ngoto out;\r\n}\r\nstatic void __exit aes_s390_fini(void)\r\n{\r\ncrypto_unregister_alg(&ctr_aes_alg);\r\nfree_page((unsigned long) ctrblk);\r\ncrypto_unregister_alg(&xts_aes_alg);\r\ncrypto_unregister_alg(&cbc_aes_alg);\r\ncrypto_unregister_alg(&ecb_aes_alg);\r\ncrypto_unregister_alg(&aes_alg);\r\n}
