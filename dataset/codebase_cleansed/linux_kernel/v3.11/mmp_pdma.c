static void set_desc(struct mmp_pdma_phy *phy, dma_addr_t addr)\r\n{\r\nu32 reg = (phy->idx << 4) + DDADR;\r\nwritel(addr, phy->base + reg);\r\n}\r\nstatic void enable_chan(struct mmp_pdma_phy *phy)\r\n{\r\nu32 reg;\r\nif (!phy->vchan)\r\nreturn;\r\nreg = phy->vchan->drcmr;\r\nreg = (((reg) < 64) ? 0x0100 : 0x1100) + (((reg) & 0x3f) << 2);\r\nwritel(DRCMR_MAPVLD | phy->idx, phy->base + reg);\r\nreg = (phy->idx << 2) + DCSR;\r\nwritel(readl(phy->base + reg) | DCSR_RUN,\r\nphy->base + reg);\r\n}\r\nstatic void disable_chan(struct mmp_pdma_phy *phy)\r\n{\r\nu32 reg;\r\nif (phy) {\r\nreg = (phy->idx << 2) + DCSR;\r\nwritel(readl(phy->base + reg) & ~DCSR_RUN,\r\nphy->base + reg);\r\n}\r\n}\r\nstatic int clear_chan_irq(struct mmp_pdma_phy *phy)\r\n{\r\nu32 dcsr;\r\nu32 dint = readl(phy->base + DINT);\r\nu32 reg = (phy->idx << 2) + DCSR;\r\nif (dint & BIT(phy->idx)) {\r\ndcsr = readl(phy->base + reg);\r\nwritel(dcsr, phy->base + reg);\r\nif ((dcsr & DCSR_BUSERR) && (phy->vchan))\r\ndev_warn(phy->vchan->dev, "DCSR_BUSERR\n");\r\nreturn 0;\r\n}\r\nreturn -EAGAIN;\r\n}\r\nstatic irqreturn_t mmp_pdma_chan_handler(int irq, void *dev_id)\r\n{\r\nstruct mmp_pdma_phy *phy = dev_id;\r\nif (clear_chan_irq(phy) == 0) {\r\ntasklet_schedule(&phy->vchan->tasklet);\r\nreturn IRQ_HANDLED;\r\n} else\r\nreturn IRQ_NONE;\r\n}\r\nstatic irqreturn_t mmp_pdma_int_handler(int irq, void *dev_id)\r\n{\r\nstruct mmp_pdma_device *pdev = dev_id;\r\nstruct mmp_pdma_phy *phy;\r\nu32 dint = readl(pdev->base + DINT);\r\nint i, ret;\r\nint irq_num = 0;\r\nwhile (dint) {\r\ni = __ffs(dint);\r\ndint &= (dint - 1);\r\nphy = &pdev->phy[i];\r\nret = mmp_pdma_chan_handler(irq, phy);\r\nif (ret == IRQ_HANDLED)\r\nirq_num++;\r\n}\r\nif (irq_num)\r\nreturn IRQ_HANDLED;\r\nelse\r\nreturn IRQ_NONE;\r\n}\r\nstatic struct mmp_pdma_phy *lookup_phy(struct mmp_pdma_chan *pchan)\r\n{\r\nint prio, i;\r\nstruct mmp_pdma_device *pdev = to_mmp_pdma_dev(pchan->chan.device);\r\nstruct mmp_pdma_phy *phy;\r\nfor (prio = 0; prio <= (((pdev->dma_channels - 1) & 0xf) >> 2); prio++) {\r\nfor (i = 0; i < pdev->dma_channels; i++) {\r\nif (prio != ((i & 0xf) >> 2))\r\ncontinue;\r\nphy = &pdev->phy[i];\r\nif (!phy->vchan) {\r\nphy->vchan = pchan;\r\nreturn phy;\r\n}\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic void append_pending_queue(struct mmp_pdma_chan *chan,\r\nstruct mmp_pdma_desc_sw *desc)\r\n{\r\nstruct mmp_pdma_desc_sw *tail =\r\nto_mmp_pdma_desc(chan->chain_pending.prev);\r\nif (list_empty(&chan->chain_pending))\r\ngoto out_splice;\r\ntail->desc.ddadr = desc->async_tx.phys;\r\ntail->desc.dcmd &= ~DCMD_ENDIRQEN;\r\nout_splice:\r\nlist_splice_tail_init(&desc->tx_list, &chan->chain_pending);\r\n}\r\nstatic void start_pending_queue(struct mmp_pdma_chan *chan)\r\n{\r\nstruct mmp_pdma_desc_sw *desc;\r\nif (!chan->idle) {\r\ndev_dbg(chan->dev, "DMA controller still busy\n");\r\nreturn;\r\n}\r\nif (list_empty(&chan->chain_pending)) {\r\nif (chan->phy) {\r\nchan->phy->vchan = NULL;\r\nchan->phy = NULL;\r\n}\r\ndev_dbg(chan->dev, "no pending list\n");\r\nreturn;\r\n}\r\nif (!chan->phy) {\r\nchan->phy = lookup_phy(chan);\r\nif (!chan->phy) {\r\ndev_dbg(chan->dev, "no free dma channel\n");\r\nreturn;\r\n}\r\n}\r\ndesc = list_first_entry(&chan->chain_pending,\r\nstruct mmp_pdma_desc_sw, node);\r\nlist_splice_tail_init(&chan->chain_pending, &chan->chain_running);\r\nset_desc(chan->phy, desc->async_tx.phys);\r\nenable_chan(chan->phy);\r\nchan->idle = false;\r\n}\r\nstatic dma_cookie_t mmp_pdma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(tx->chan);\r\nstruct mmp_pdma_desc_sw *desc = tx_to_mmp_pdma_desc(tx);\r\nstruct mmp_pdma_desc_sw *child;\r\nunsigned long flags;\r\ndma_cookie_t cookie = -EBUSY;\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nlist_for_each_entry(child, &desc->tx_list, node) {\r\ncookie = dma_cookie_assign(&child->async_tx);\r\n}\r\nappend_pending_queue(chan, desc);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\nreturn cookie;\r\n}\r\nstruct mmp_pdma_desc_sw *mmp_pdma_alloc_descriptor(struct mmp_pdma_chan *chan)\r\n{\r\nstruct mmp_pdma_desc_sw *desc;\r\ndma_addr_t pdesc;\r\ndesc = dma_pool_alloc(chan->desc_pool, GFP_ATOMIC, &pdesc);\r\nif (!desc) {\r\ndev_err(chan->dev, "out of memory for link descriptor\n");\r\nreturn NULL;\r\n}\r\nmemset(desc, 0, sizeof(*desc));\r\nINIT_LIST_HEAD(&desc->tx_list);\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->chan);\r\ndesc->async_tx.tx_submit = mmp_pdma_tx_submit;\r\ndesc->async_tx.phys = pdesc;\r\nreturn desc;\r\n}\r\nstatic int mmp_pdma_alloc_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nif (chan->desc_pool)\r\nreturn 1;\r\nchan->desc_pool =\r\ndma_pool_create(dev_name(&dchan->dev->device), chan->dev,\r\nsizeof(struct mmp_pdma_desc_sw),\r\n__alignof__(struct mmp_pdma_desc_sw), 0);\r\nif (!chan->desc_pool) {\r\ndev_err(chan->dev, "unable to allocate descriptor pool\n");\r\nreturn -ENOMEM;\r\n}\r\nif (chan->phy) {\r\nchan->phy->vchan = NULL;\r\nchan->phy = NULL;\r\n}\r\nchan->idle = true;\r\nchan->dev_addr = 0;\r\nreturn 1;\r\n}\r\nstatic void mmp_pdma_free_desc_list(struct mmp_pdma_chan *chan,\r\nstruct list_head *list)\r\n{\r\nstruct mmp_pdma_desc_sw *desc, *_desc;\r\nlist_for_each_entry_safe(desc, _desc, list, node) {\r\nlist_del(&desc->node);\r\ndma_pool_free(chan->desc_pool, desc, desc->async_tx.phys);\r\n}\r\n}\r\nstatic void mmp_pdma_free_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nmmp_pdma_free_desc_list(chan, &chan->chain_pending);\r\nmmp_pdma_free_desc_list(chan, &chan->chain_running);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\ndma_pool_destroy(chan->desc_pool);\r\nchan->desc_pool = NULL;\r\nchan->idle = true;\r\nchan->dev_addr = 0;\r\nif (chan->phy) {\r\nchan->phy->vchan = NULL;\r\nchan->phy = NULL;\r\n}\r\nreturn;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmmp_pdma_prep_memcpy(struct dma_chan *dchan,\r\ndma_addr_t dma_dst, dma_addr_t dma_src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct mmp_pdma_chan *chan;\r\nstruct mmp_pdma_desc_sw *first = NULL, *prev = NULL, *new;\r\nsize_t copy = 0;\r\nif (!dchan)\r\nreturn NULL;\r\nif (!len)\r\nreturn NULL;\r\nchan = to_mmp_pdma_chan(dchan);\r\nif (!chan->dir) {\r\nchan->dir = DMA_MEM_TO_MEM;\r\nchan->dcmd = DCMD_INCTRGADDR | DCMD_INCSRCADDR;\r\nchan->dcmd |= DCMD_BURST32;\r\n}\r\ndo {\r\nnew = mmp_pdma_alloc_descriptor(chan);\r\nif (!new) {\r\ndev_err(chan->dev, "no memory for desc\n");\r\ngoto fail;\r\n}\r\ncopy = min_t(size_t, len, PDMA_MAX_DESC_BYTES);\r\nnew->desc.dcmd = chan->dcmd | (DCMD_LENGTH & copy);\r\nnew->desc.dsadr = dma_src;\r\nnew->desc.dtadr = dma_dst;\r\nif (!first)\r\nfirst = new;\r\nelse\r\nprev->desc.ddadr = new->async_tx.phys;\r\nnew->async_tx.cookie = 0;\r\nasync_tx_ack(&new->async_tx);\r\nprev = new;\r\nlen -= copy;\r\nif (chan->dir == DMA_MEM_TO_DEV) {\r\ndma_src += copy;\r\n} else if (chan->dir == DMA_DEV_TO_MEM) {\r\ndma_dst += copy;\r\n} else if (chan->dir == DMA_MEM_TO_MEM) {\r\ndma_src += copy;\r\ndma_dst += copy;\r\n}\r\nlist_add_tail(&new->node, &first->tx_list);\r\n} while (len);\r\nfirst->async_tx.flags = flags;\r\nfirst->async_tx.cookie = -EBUSY;\r\nnew->desc.ddadr = DDADR_STOP;\r\nnew->desc.dcmd |= DCMD_ENDIRQEN;\r\nreturn &first->async_tx;\r\nfail:\r\nif (first)\r\nmmp_pdma_free_desc_list(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nmmp_pdma_prep_slave_sg(struct dma_chan *dchan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction dir,\r\nunsigned long flags, void *context)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nstruct mmp_pdma_desc_sw *first = NULL, *prev = NULL, *new = NULL;\r\nsize_t len, avail;\r\nstruct scatterlist *sg;\r\ndma_addr_t addr;\r\nint i;\r\nif ((sgl == NULL) || (sg_len == 0))\r\nreturn NULL;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\naddr = sg_dma_address(sg);\r\navail = sg_dma_len(sgl);\r\ndo {\r\nlen = min_t(size_t, avail, PDMA_MAX_DESC_BYTES);\r\nnew = mmp_pdma_alloc_descriptor(chan);\r\nif (!new) {\r\ndev_err(chan->dev, "no memory for desc\n");\r\ngoto fail;\r\n}\r\nnew->desc.dcmd = chan->dcmd | (DCMD_LENGTH & len);\r\nif (dir == DMA_MEM_TO_DEV) {\r\nnew->desc.dsadr = addr;\r\nnew->desc.dtadr = chan->dev_addr;\r\n} else {\r\nnew->desc.dsadr = chan->dev_addr;\r\nnew->desc.dtadr = addr;\r\n}\r\nif (!first)\r\nfirst = new;\r\nelse\r\nprev->desc.ddadr = new->async_tx.phys;\r\nnew->async_tx.cookie = 0;\r\nasync_tx_ack(&new->async_tx);\r\nprev = new;\r\nlist_add_tail(&new->node, &first->tx_list);\r\naddr += len;\r\navail -= len;\r\n} while (avail);\r\n}\r\nfirst->async_tx.cookie = -EBUSY;\r\nfirst->async_tx.flags = flags;\r\nnew->desc.ddadr = DDADR_STOP;\r\nnew->desc.dcmd |= DCMD_ENDIRQEN;\r\nreturn &first->async_tx;\r\nfail:\r\nif (first)\r\nmmp_pdma_free_desc_list(chan, &first->tx_list);\r\nreturn NULL;\r\n}\r\nstatic int mmp_pdma_control(struct dma_chan *dchan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nstruct dma_slave_config *cfg = (void *)arg;\r\nunsigned long flags;\r\nint ret = 0;\r\nu32 maxburst = 0, addr = 0;\r\nenum dma_slave_buswidth width = DMA_SLAVE_BUSWIDTH_UNDEFINED;\r\nif (!dchan)\r\nreturn -EINVAL;\r\nswitch (cmd) {\r\ncase DMA_TERMINATE_ALL:\r\ndisable_chan(chan->phy);\r\nif (chan->phy) {\r\nchan->phy->vchan = NULL;\r\nchan->phy = NULL;\r\n}\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nmmp_pdma_free_desc_list(chan, &chan->chain_pending);\r\nmmp_pdma_free_desc_list(chan, &chan->chain_running);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\nchan->idle = true;\r\nbreak;\r\ncase DMA_SLAVE_CONFIG:\r\nif (cfg->direction == DMA_DEV_TO_MEM) {\r\nchan->dcmd = DCMD_INCTRGADDR | DCMD_FLOWSRC;\r\nmaxburst = cfg->src_maxburst;\r\nwidth = cfg->src_addr_width;\r\naddr = cfg->src_addr;\r\n} else if (cfg->direction == DMA_MEM_TO_DEV) {\r\nchan->dcmd = DCMD_INCSRCADDR | DCMD_FLOWTRG;\r\nmaxburst = cfg->dst_maxburst;\r\nwidth = cfg->dst_addr_width;\r\naddr = cfg->dst_addr;\r\n}\r\nif (width == DMA_SLAVE_BUSWIDTH_1_BYTE)\r\nchan->dcmd |= DCMD_WIDTH1;\r\nelse if (width == DMA_SLAVE_BUSWIDTH_2_BYTES)\r\nchan->dcmd |= DCMD_WIDTH2;\r\nelse if (width == DMA_SLAVE_BUSWIDTH_4_BYTES)\r\nchan->dcmd |= DCMD_WIDTH4;\r\nif (maxburst == 8)\r\nchan->dcmd |= DCMD_BURST8;\r\nelse if (maxburst == 16)\r\nchan->dcmd |= DCMD_BURST16;\r\nelse if (maxburst == 32)\r\nchan->dcmd |= DCMD_BURST32;\r\nchan->dir = cfg->direction;\r\nchan->drcmr = cfg->slave_id;\r\nchan->dev_addr = addr;\r\nbreak;\r\ndefault:\r\nreturn -ENOSYS;\r\n}\r\nreturn ret;\r\n}\r\nstatic enum dma_status mmp_pdma_tx_status(struct dma_chan *dchan,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nenum dma_status ret;\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nret = dma_cookie_status(dchan, cookie, txstate);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void mmp_pdma_issue_pending(struct dma_chan *dchan)\r\n{\r\nstruct mmp_pdma_chan *chan = to_mmp_pdma_chan(dchan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nstart_pending_queue(chan);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\n}\r\nstatic void dma_do_tasklet(unsigned long data)\r\n{\r\nstruct mmp_pdma_chan *chan = (struct mmp_pdma_chan *)data;\r\nstruct mmp_pdma_desc_sw *desc, *_desc;\r\nLIST_HEAD(chain_cleanup);\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->desc_lock, flags);\r\nif (!list_empty(&chan->chain_running)) {\r\ndma_cookie_t cookie;\r\ndesc = to_mmp_pdma_desc(chan->chain_running.prev);\r\ncookie = desc->async_tx.cookie;\r\ndma_cookie_complete(&desc->async_tx);\r\ndev_dbg(chan->dev, "completed_cookie=%d\n", cookie);\r\n}\r\nlist_splice_tail_init(&chan->chain_running, &chain_cleanup);\r\nchan->idle = true;\r\nstart_pending_queue(chan);\r\nspin_unlock_irqrestore(&chan->desc_lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &chain_cleanup, node) {\r\nstruct dma_async_tx_descriptor *txd = &desc->async_tx;\r\nlist_del(&desc->node);\r\nif (txd->callback)\r\ntxd->callback(txd->callback_param);\r\ndma_pool_free(chan->desc_pool, desc, txd->phys);\r\n}\r\n}\r\nstatic int mmp_pdma_remove(struct platform_device *op)\r\n{\r\nstruct mmp_pdma_device *pdev = platform_get_drvdata(op);\r\ndma_async_device_unregister(&pdev->device);\r\nreturn 0;\r\n}\r\nstatic int mmp_pdma_chan_init(struct mmp_pdma_device *pdev,\r\nint idx, int irq)\r\n{\r\nstruct mmp_pdma_phy *phy = &pdev->phy[idx];\r\nstruct mmp_pdma_chan *chan;\r\nint ret;\r\nchan = devm_kzalloc(pdev->dev,\r\nsizeof(struct mmp_pdma_chan), GFP_KERNEL);\r\nif (chan == NULL)\r\nreturn -ENOMEM;\r\nphy->idx = idx;\r\nphy->base = pdev->base;\r\nif (irq) {\r\nret = devm_request_irq(pdev->dev, irq,\r\nmmp_pdma_chan_handler, IRQF_DISABLED, "pdma", phy);\r\nif (ret) {\r\ndev_err(pdev->dev, "channel request irq fail!\n");\r\nreturn ret;\r\n}\r\n}\r\nspin_lock_init(&chan->desc_lock);\r\nchan->dev = pdev->dev;\r\nchan->chan.device = &pdev->device;\r\ntasklet_init(&chan->tasklet, dma_do_tasklet, (unsigned long)chan);\r\nINIT_LIST_HEAD(&chan->chain_pending);\r\nINIT_LIST_HEAD(&chan->chain_running);\r\nlist_add_tail(&chan->chan.device_node,\r\n&pdev->device.channels);\r\nreturn 0;\r\n}\r\nstatic int mmp_pdma_probe(struct platform_device *op)\r\n{\r\nstruct mmp_pdma_device *pdev;\r\nconst struct of_device_id *of_id;\r\nstruct mmp_dma_platdata *pdata = dev_get_platdata(&op->dev);\r\nstruct resource *iores;\r\nint i, ret, irq = 0;\r\nint dma_channels = 0, irq_num = 0;\r\npdev = devm_kzalloc(&op->dev, sizeof(*pdev), GFP_KERNEL);\r\nif (!pdev)\r\nreturn -ENOMEM;\r\npdev->dev = &op->dev;\r\niores = platform_get_resource(op, IORESOURCE_MEM, 0);\r\nif (!iores)\r\nreturn -EINVAL;\r\npdev->base = devm_ioremap_resource(pdev->dev, iores);\r\nif (IS_ERR(pdev->base))\r\nreturn PTR_ERR(pdev->base);\r\nof_id = of_match_device(mmp_pdma_dt_ids, pdev->dev);\r\nif (of_id)\r\nof_property_read_u32(pdev->dev->of_node,\r\n"#dma-channels", &dma_channels);\r\nelse if (pdata && pdata->dma_channels)\r\ndma_channels = pdata->dma_channels;\r\nelse\r\ndma_channels = 32;\r\npdev->dma_channels = dma_channels;\r\nfor (i = 0; i < dma_channels; i++) {\r\nif (platform_get_irq(op, i) > 0)\r\nirq_num++;\r\n}\r\npdev->phy = devm_kzalloc(pdev->dev,\r\ndma_channels * sizeof(struct mmp_pdma_chan), GFP_KERNEL);\r\nif (pdev->phy == NULL)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&pdev->device.channels);\r\nif (irq_num != dma_channels) {\r\nirq = platform_get_irq(op, 0);\r\nret = devm_request_irq(pdev->dev, irq,\r\nmmp_pdma_int_handler, IRQF_DISABLED, "pdma", pdev);\r\nif (ret)\r\nreturn ret;\r\n}\r\nfor (i = 0; i < dma_channels; i++) {\r\nirq = (irq_num != dma_channels) ? 0 : platform_get_irq(op, i);\r\nret = mmp_pdma_chan_init(pdev, i, irq);\r\nif (ret)\r\nreturn ret;\r\n}\r\ndma_cap_set(DMA_SLAVE, pdev->device.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, pdev->device.cap_mask);\r\ndma_cap_set(DMA_SLAVE, pdev->device.cap_mask);\r\npdev->device.dev = &op->dev;\r\npdev->device.device_alloc_chan_resources = mmp_pdma_alloc_chan_resources;\r\npdev->device.device_free_chan_resources = mmp_pdma_free_chan_resources;\r\npdev->device.device_tx_status = mmp_pdma_tx_status;\r\npdev->device.device_prep_dma_memcpy = mmp_pdma_prep_memcpy;\r\npdev->device.device_prep_slave_sg = mmp_pdma_prep_slave_sg;\r\npdev->device.device_issue_pending = mmp_pdma_issue_pending;\r\npdev->device.device_control = mmp_pdma_control;\r\npdev->device.copy_align = PDMA_ALIGNMENT;\r\nif (pdev->dev->coherent_dma_mask)\r\ndma_set_mask(pdev->dev, pdev->dev->coherent_dma_mask);\r\nelse\r\ndma_set_mask(pdev->dev, DMA_BIT_MASK(64));\r\nret = dma_async_device_register(&pdev->device);\r\nif (ret) {\r\ndev_err(pdev->device.dev, "unable to register\n");\r\nreturn ret;\r\n}\r\ndev_info(pdev->device.dev, "initialized\n");\r\nreturn 0;\r\n}
