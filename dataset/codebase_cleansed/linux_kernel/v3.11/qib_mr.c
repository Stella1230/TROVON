static inline struct qib_fmr *to_ifmr(struct ib_fmr *ibfmr)\r\n{\r\nreturn container_of(ibfmr, struct qib_fmr, ibfmr);\r\n}\r\nstatic int init_qib_mregion(struct qib_mregion *mr, struct ib_pd *pd,\r\nint count)\r\n{\r\nint m, i = 0;\r\nint rval = 0;\r\nm = (count + QIB_SEGSZ - 1) / QIB_SEGSZ;\r\nfor (; i < m; i++) {\r\nmr->map[i] = kzalloc(sizeof *mr->map[0], GFP_KERNEL);\r\nif (!mr->map[i])\r\ngoto bail;\r\n}\r\nmr->mapsz = m;\r\ninit_completion(&mr->comp);\r\natomic_set(&mr->refcount, 1);\r\nmr->pd = pd;\r\nmr->max_segs = count;\r\nout:\r\nreturn rval;\r\nbail:\r\nwhile (i)\r\nkfree(mr->map[--i]);\r\nrval = -ENOMEM;\r\ngoto out;\r\n}\r\nstatic void deinit_qib_mregion(struct qib_mregion *mr)\r\n{\r\nint i = mr->mapsz;\r\nmr->mapsz = 0;\r\nwhile (i)\r\nkfree(mr->map[--i]);\r\n}\r\nstruct ib_mr *qib_get_dma_mr(struct ib_pd *pd, int acc)\r\n{\r\nstruct qib_mr *mr = NULL;\r\nstruct ib_mr *ret;\r\nint rval;\r\nif (to_ipd(pd)->user) {\r\nret = ERR_PTR(-EPERM);\r\ngoto bail;\r\n}\r\nmr = kzalloc(sizeof *mr, GFP_KERNEL);\r\nif (!mr) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nrval = init_qib_mregion(&mr->mr, pd, 0);\r\nif (rval) {\r\nret = ERR_PTR(rval);\r\ngoto bail;\r\n}\r\nrval = qib_alloc_lkey(&mr->mr, 1);\r\nif (rval) {\r\nret = ERR_PTR(rval);\r\ngoto bail_mregion;\r\n}\r\nmr->mr.access_flags = acc;\r\nret = &mr->ibmr;\r\ndone:\r\nreturn ret;\r\nbail_mregion:\r\ndeinit_qib_mregion(&mr->mr);\r\nbail:\r\nkfree(mr);\r\ngoto done;\r\n}\r\nstatic struct qib_mr *alloc_mr(int count, struct ib_pd *pd)\r\n{\r\nstruct qib_mr *mr;\r\nint rval = -ENOMEM;\r\nint m;\r\nm = (count + QIB_SEGSZ - 1) / QIB_SEGSZ;\r\nmr = kzalloc(sizeof *mr + m * sizeof mr->mr.map[0], GFP_KERNEL);\r\nif (!mr)\r\ngoto bail;\r\nrval = init_qib_mregion(&mr->mr, pd, count);\r\nif (rval)\r\ngoto bail;\r\nrval = qib_alloc_lkey(&mr->mr, 0);\r\nif (rval)\r\ngoto bail_mregion;\r\nmr->ibmr.lkey = mr->mr.lkey;\r\nmr->ibmr.rkey = mr->mr.lkey;\r\ndone:\r\nreturn mr;\r\nbail_mregion:\r\ndeinit_qib_mregion(&mr->mr);\r\nbail:\r\nkfree(mr);\r\nmr = ERR_PTR(rval);\r\ngoto done;\r\n}\r\nstruct ib_mr *qib_reg_phys_mr(struct ib_pd *pd,\r\nstruct ib_phys_buf *buffer_list,\r\nint num_phys_buf, int acc, u64 *iova_start)\r\n{\r\nstruct qib_mr *mr;\r\nint n, m, i;\r\nstruct ib_mr *ret;\r\nmr = alloc_mr(num_phys_buf, pd);\r\nif (IS_ERR(mr)) {\r\nret = (struct ib_mr *)mr;\r\ngoto bail;\r\n}\r\nmr->mr.user_base = *iova_start;\r\nmr->mr.iova = *iova_start;\r\nmr->mr.access_flags = acc;\r\nm = 0;\r\nn = 0;\r\nfor (i = 0; i < num_phys_buf; i++) {\r\nmr->mr.map[m]->segs[n].vaddr = (void *) buffer_list[i].addr;\r\nmr->mr.map[m]->segs[n].length = buffer_list[i].size;\r\nmr->mr.length += buffer_list[i].size;\r\nn++;\r\nif (n == QIB_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nret = &mr->ibmr;\r\nbail:\r\nreturn ret;\r\n}\r\nstruct ib_mr *qib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\r\nu64 virt_addr, int mr_access_flags,\r\nstruct ib_udata *udata)\r\n{\r\nstruct qib_mr *mr;\r\nstruct ib_umem *umem;\r\nstruct ib_umem_chunk *chunk;\r\nint n, m, i;\r\nstruct ib_mr *ret;\r\nif (length == 0) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\numem = ib_umem_get(pd->uobject->context, start, length,\r\nmr_access_flags, 0);\r\nif (IS_ERR(umem))\r\nreturn (void *) umem;\r\nn = 0;\r\nlist_for_each_entry(chunk, &umem->chunk_list, list)\r\nn += chunk->nents;\r\nmr = alloc_mr(n, pd);\r\nif (IS_ERR(mr)) {\r\nret = (struct ib_mr *)mr;\r\nib_umem_release(umem);\r\ngoto bail;\r\n}\r\nmr->mr.user_base = start;\r\nmr->mr.iova = virt_addr;\r\nmr->mr.length = length;\r\nmr->mr.offset = umem->offset;\r\nmr->mr.access_flags = mr_access_flags;\r\nmr->umem = umem;\r\nif (is_power_of_2(umem->page_size))\r\nmr->mr.page_shift = ilog2(umem->page_size);\r\nm = 0;\r\nn = 0;\r\nlist_for_each_entry(chunk, &umem->chunk_list, list) {\r\nfor (i = 0; i < chunk->nents; i++) {\r\nvoid *vaddr;\r\nvaddr = page_address(sg_page(&chunk->page_list[i]));\r\nif (!vaddr) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nmr->mr.map[m]->segs[n].vaddr = vaddr;\r\nmr->mr.map[m]->segs[n].length = umem->page_size;\r\nn++;\r\nif (n == QIB_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\n}\r\nret = &mr->ibmr;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_dereg_mr(struct ib_mr *ibmr)\r\n{\r\nstruct qib_mr *mr = to_imr(ibmr);\r\nint ret = 0;\r\nunsigned long timeout;\r\nqib_free_lkey(&mr->mr);\r\nqib_put_mr(&mr->mr);\r\ntimeout = wait_for_completion_timeout(&mr->mr.comp,\r\n5 * HZ);\r\nif (!timeout) {\r\nqib_get_mr(&mr->mr);\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\ndeinit_qib_mregion(&mr->mr);\r\nif (mr->umem)\r\nib_umem_release(mr->umem);\r\nkfree(mr);\r\nout:\r\nreturn ret;\r\n}\r\nstruct ib_mr *qib_alloc_fast_reg_mr(struct ib_pd *pd, int max_page_list_len)\r\n{\r\nstruct qib_mr *mr;\r\nmr = alloc_mr(max_page_list_len, pd);\r\nif (IS_ERR(mr))\r\nreturn (struct ib_mr *)mr;\r\nreturn &mr->ibmr;\r\n}\r\nstruct ib_fast_reg_page_list *\r\nqib_alloc_fast_reg_page_list(struct ib_device *ibdev, int page_list_len)\r\n{\r\nunsigned size = page_list_len * sizeof(u64);\r\nstruct ib_fast_reg_page_list *pl;\r\nif (size > PAGE_SIZE)\r\nreturn ERR_PTR(-EINVAL);\r\npl = kzalloc(sizeof *pl, GFP_KERNEL);\r\nif (!pl)\r\nreturn ERR_PTR(-ENOMEM);\r\npl->page_list = kzalloc(size, GFP_KERNEL);\r\nif (!pl->page_list)\r\ngoto err_free;\r\nreturn pl;\r\nerr_free:\r\nkfree(pl);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nvoid qib_free_fast_reg_page_list(struct ib_fast_reg_page_list *pl)\r\n{\r\nkfree(pl->page_list);\r\nkfree(pl);\r\n}\r\nstruct ib_fmr *qib_alloc_fmr(struct ib_pd *pd, int mr_access_flags,\r\nstruct ib_fmr_attr *fmr_attr)\r\n{\r\nstruct qib_fmr *fmr;\r\nint m;\r\nstruct ib_fmr *ret;\r\nint rval = -ENOMEM;\r\nm = (fmr_attr->max_pages + QIB_SEGSZ - 1) / QIB_SEGSZ;\r\nfmr = kzalloc(sizeof *fmr + m * sizeof fmr->mr.map[0], GFP_KERNEL);\r\nif (!fmr)\r\ngoto bail;\r\nrval = init_qib_mregion(&fmr->mr, pd, fmr_attr->max_pages);\r\nif (rval)\r\ngoto bail;\r\nrval = qib_alloc_lkey(&fmr->mr, 0);\r\nif (rval)\r\ngoto bail_mregion;\r\nfmr->ibfmr.rkey = fmr->mr.lkey;\r\nfmr->ibfmr.lkey = fmr->mr.lkey;\r\nfmr->mr.access_flags = mr_access_flags;\r\nfmr->mr.max_segs = fmr_attr->max_pages;\r\nfmr->mr.page_shift = fmr_attr->page_shift;\r\nret = &fmr->ibfmr;\r\ndone:\r\nreturn ret;\r\nbail_mregion:\r\ndeinit_qib_mregion(&fmr->mr);\r\nbail:\r\nkfree(fmr);\r\nret = ERR_PTR(rval);\r\ngoto done;\r\n}\r\nint qib_map_phys_fmr(struct ib_fmr *ibfmr, u64 *page_list,\r\nint list_len, u64 iova)\r\n{\r\nstruct qib_fmr *fmr = to_ifmr(ibfmr);\r\nstruct qib_lkey_table *rkt;\r\nunsigned long flags;\r\nint m, n, i;\r\nu32 ps;\r\nint ret;\r\ni = atomic_read(&fmr->mr.refcount);\r\nif (i > 2)\r\nreturn -EBUSY;\r\nif (list_len > fmr->mr.max_segs) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nrkt = &to_idev(ibfmr->device)->lk_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nfmr->mr.user_base = iova;\r\nfmr->mr.iova = iova;\r\nps = 1 << fmr->mr.page_shift;\r\nfmr->mr.length = list_len * ps;\r\nm = 0;\r\nn = 0;\r\nfor (i = 0; i < list_len; i++) {\r\nfmr->mr.map[m]->segs[n].vaddr = (void *) page_list[i];\r\nfmr->mr.map[m]->segs[n].length = ps;\r\nif (++n == QIB_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_unmap_fmr(struct list_head *fmr_list)\r\n{\r\nstruct qib_fmr *fmr;\r\nstruct qib_lkey_table *rkt;\r\nunsigned long flags;\r\nlist_for_each_entry(fmr, fmr_list, ibfmr.list) {\r\nrkt = &to_idev(fmr->ibfmr.device)->lk_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nfmr->mr.user_base = 0;\r\nfmr->mr.iova = 0;\r\nfmr->mr.length = 0;\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nint qib_dealloc_fmr(struct ib_fmr *ibfmr)\r\n{\r\nstruct qib_fmr *fmr = to_ifmr(ibfmr);\r\nint ret = 0;\r\nunsigned long timeout;\r\nqib_free_lkey(&fmr->mr);\r\nqib_put_mr(&fmr->mr);\r\ntimeout = wait_for_completion_timeout(&fmr->mr.comp,\r\n5 * HZ);\r\nif (!timeout) {\r\nqib_get_mr(&fmr->mr);\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\ndeinit_qib_mregion(&fmr->mr);\r\nkfree(fmr);\r\nout:\r\nreturn ret;\r\n}\r\nvoid mr_rcu_callback(struct rcu_head *list)\r\n{\r\nstruct qib_mregion *mr = container_of(list, struct qib_mregion, list);\r\ncomplete(&mr->comp);\r\n}
