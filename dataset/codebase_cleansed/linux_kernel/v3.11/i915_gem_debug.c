int\r\ni915_verify_lists(struct drm_device *dev)\r\n{\r\nstatic int warned;\r\ndrm_i915_private_t *dev_priv = dev->dev_private;\r\nstruct drm_i915_gem_object *obj;\r\nint err = 0;\r\nif (warned)\r\nreturn 0;\r\nlist_for_each_entry(obj, &dev_priv->render_ring.active_list, list) {\r\nif (obj->base.dev != dev ||\r\n!atomic_read(&obj->base.refcount.refcount)) {\r\nDRM_ERROR("freed render active %p\n", obj);\r\nerr++;\r\nbreak;\r\n} else if (!obj->active ||\r\n(obj->base.read_domains & I915_GEM_GPU_DOMAINS) == 0) {\r\nDRM_ERROR("invalid render active %p (a %d r %x)\n",\r\nobj,\r\nobj->active,\r\nobj->base.read_domains);\r\nerr++;\r\n} else if (obj->base.write_domain && list_empty(&obj->gpu_write_list)) {\r\nDRM_ERROR("invalid render active %p (w %x, gwl %d)\n",\r\nobj,\r\nobj->base.write_domain,\r\n!list_empty(&obj->gpu_write_list));\r\nerr++;\r\n}\r\n}\r\nlist_for_each_entry(obj, &dev_priv->mm.flushing_list, list) {\r\nif (obj->base.dev != dev ||\r\n!atomic_read(&obj->base.refcount.refcount)) {\r\nDRM_ERROR("freed flushing %p\n", obj);\r\nerr++;\r\nbreak;\r\n} else if (!obj->active ||\r\n(obj->base.write_domain & I915_GEM_GPU_DOMAINS) == 0 ||\r\nlist_empty(&obj->gpu_write_list)) {\r\nDRM_ERROR("invalid flushing %p (a %d w %x gwl %d)\n",\r\nobj,\r\nobj->active,\r\nobj->base.write_domain,\r\n!list_empty(&obj->gpu_write_list));\r\nerr++;\r\n}\r\n}\r\nlist_for_each_entry(obj, &dev_priv->mm.gpu_write_list, gpu_write_list) {\r\nif (obj->base.dev != dev ||\r\n!atomic_read(&obj->base.refcount.refcount)) {\r\nDRM_ERROR("freed gpu write %p\n", obj);\r\nerr++;\r\nbreak;\r\n} else if (!obj->active ||\r\n(obj->base.write_domain & I915_GEM_GPU_DOMAINS) == 0) {\r\nDRM_ERROR("invalid gpu write %p (a %d w %x)\n",\r\nobj,\r\nobj->active,\r\nobj->base.write_domain);\r\nerr++;\r\n}\r\n}\r\nlist_for_each_entry(obj, &dev_priv->mm.inactive_list, list) {\r\nif (obj->base.dev != dev ||\r\n!atomic_read(&obj->base.refcount.refcount)) {\r\nDRM_ERROR("freed inactive %p\n", obj);\r\nerr++;\r\nbreak;\r\n} else if (obj->pin_count || obj->active ||\r\n(obj->base.write_domain & I915_GEM_GPU_DOMAINS)) {\r\nDRM_ERROR("invalid inactive %p (p %d a %d w %x)\n",\r\nobj,\r\nobj->pin_count, obj->active,\r\nobj->base.write_domain);\r\nerr++;\r\n}\r\n}\r\nreturn warned = err;\r\n}\r\nvoid\r\ni915_gem_object_check_coherency(struct drm_i915_gem_object *obj, int handle)\r\n{\r\nstruct drm_device *dev = obj->base.dev;\r\nint page;\r\nuint32_t *gtt_mapping;\r\nuint32_t *backing_map = NULL;\r\nint bad_count = 0;\r\nDRM_INFO("%s: checking coherency of object %p@0x%08x (%d, %zdkb):\n",\r\n__func__, obj, obj->gtt_offset, handle,\r\nobj->size / 1024);\r\ngtt_mapping = ioremap(dev_priv->mm.gtt_base_addr + obj->gtt_offset,\r\nobj->base.size);\r\nif (gtt_mapping == NULL) {\r\nDRM_ERROR("failed to map GTT space\n");\r\nreturn;\r\n}\r\nfor (page = 0; page < obj->size / PAGE_SIZE; page++) {\r\nint i;\r\nbacking_map = kmap_atomic(obj->pages[page]);\r\nif (backing_map == NULL) {\r\nDRM_ERROR("failed to map backing page\n");\r\ngoto out;\r\n}\r\nfor (i = 0; i < PAGE_SIZE / 4; i++) {\r\nuint32_t cpuval = backing_map[i];\r\nuint32_t gttval = readl(gtt_mapping +\r\npage * 1024 + i);\r\nif (cpuval != gttval) {\r\nDRM_INFO("incoherent CPU vs GPU at 0x%08x: "\r\n"0x%08x vs 0x%08x\n",\r\n(int)(obj->gtt_offset +\r\npage * PAGE_SIZE + i * 4),\r\ncpuval, gttval);\r\nif (bad_count++ >= 8) {\r\nDRM_INFO("...\n");\r\ngoto out;\r\n}\r\n}\r\n}\r\nkunmap_atomic(backing_map);\r\nbacking_map = NULL;\r\n}\r\nout:\r\nif (backing_map != NULL)\r\nkunmap_atomic(backing_map);\r\niounmap(gtt_mapping);\r\nmsleep(1);\r\ni915_gem_clflush_object(obj);\r\n}
