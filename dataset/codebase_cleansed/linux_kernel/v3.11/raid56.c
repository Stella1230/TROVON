int btrfs_alloc_stripe_hash_table(struct btrfs_fs_info *info)\r\n{\r\nstruct btrfs_stripe_hash_table *table;\r\nstruct btrfs_stripe_hash_table *x;\r\nstruct btrfs_stripe_hash *cur;\r\nstruct btrfs_stripe_hash *h;\r\nint num_entries = 1 << BTRFS_STRIPE_HASH_TABLE_BITS;\r\nint i;\r\nint table_size;\r\nif (info->stripe_hash_table)\r\nreturn 0;\r\ntable_size = sizeof(*table) + sizeof(*h) * num_entries;\r\ntable = kzalloc(table_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);\r\nif (!table) {\r\ntable = vzalloc(table_size);\r\nif (!table)\r\nreturn -ENOMEM;\r\n}\r\nspin_lock_init(&table->cache_lock);\r\nINIT_LIST_HEAD(&table->stripe_cache);\r\nh = table->table;\r\nfor (i = 0; i < num_entries; i++) {\r\ncur = h + i;\r\nINIT_LIST_HEAD(&cur->hash_list);\r\nspin_lock_init(&cur->lock);\r\ninit_waitqueue_head(&cur->wait);\r\n}\r\nx = cmpxchg(&info->stripe_hash_table, NULL, table);\r\nif (x) {\r\nif (is_vmalloc_addr(x))\r\nvfree(x);\r\nelse\r\nkfree(x);\r\n}\r\nreturn 0;\r\n}\r\nstatic void cache_rbio_pages(struct btrfs_raid_bio *rbio)\r\n{\r\nint i;\r\nchar *s;\r\nchar *d;\r\nint ret;\r\nret = alloc_rbio_pages(rbio);\r\nif (ret)\r\nreturn;\r\nfor (i = 0; i < rbio->nr_pages; i++) {\r\nif (!rbio->bio_pages[i])\r\ncontinue;\r\ns = kmap(rbio->bio_pages[i]);\r\nd = kmap(rbio->stripe_pages[i]);\r\nmemcpy(d, s, PAGE_CACHE_SIZE);\r\nkunmap(rbio->bio_pages[i]);\r\nkunmap(rbio->stripe_pages[i]);\r\nSetPageUptodate(rbio->stripe_pages[i]);\r\n}\r\nset_bit(RBIO_CACHE_READY_BIT, &rbio->flags);\r\n}\r\nstatic int rbio_bucket(struct btrfs_raid_bio *rbio)\r\n{\r\nu64 num = rbio->raid_map[0];\r\nreturn hash_64(num >> 16, BTRFS_STRIPE_HASH_TABLE_BITS);\r\n}\r\nstatic void steal_rbio(struct btrfs_raid_bio *src, struct btrfs_raid_bio *dest)\r\n{\r\nint i;\r\nstruct page *s;\r\nstruct page *d;\r\nif (!test_bit(RBIO_CACHE_READY_BIT, &src->flags))\r\nreturn;\r\nfor (i = 0; i < dest->nr_pages; i++) {\r\ns = src->stripe_pages[i];\r\nif (!s || !PageUptodate(s)) {\r\ncontinue;\r\n}\r\nd = dest->stripe_pages[i];\r\nif (d)\r\n__free_page(d);\r\ndest->stripe_pages[i] = s;\r\nsrc->stripe_pages[i] = NULL;\r\n}\r\n}\r\nstatic void merge_rbio(struct btrfs_raid_bio *dest,\r\nstruct btrfs_raid_bio *victim)\r\n{\r\nbio_list_merge(&dest->bio_list, &victim->bio_list);\r\ndest->bio_list_bytes += victim->bio_list_bytes;\r\nbio_list_init(&victim->bio_list);\r\n}\r\nstatic void __remove_rbio_from_cache(struct btrfs_raid_bio *rbio)\r\n{\r\nint bucket = rbio_bucket(rbio);\r\nstruct btrfs_stripe_hash_table *table;\r\nstruct btrfs_stripe_hash *h;\r\nint freeit = 0;\r\nif (!test_bit(RBIO_CACHE_BIT, &rbio->flags))\r\nreturn;\r\ntable = rbio->fs_info->stripe_hash_table;\r\nh = table->table + bucket;\r\nspin_lock(&h->lock);\r\nspin_lock(&rbio->bio_list_lock);\r\nif (test_and_clear_bit(RBIO_CACHE_BIT, &rbio->flags)) {\r\nlist_del_init(&rbio->stripe_cache);\r\ntable->cache_size -= 1;\r\nfreeit = 1;\r\nif (bio_list_empty(&rbio->bio_list)) {\r\nif (!list_empty(&rbio->hash_list)) {\r\nlist_del_init(&rbio->hash_list);\r\natomic_dec(&rbio->refs);\r\nBUG_ON(!list_empty(&rbio->plug_list));\r\n}\r\n}\r\n}\r\nspin_unlock(&rbio->bio_list_lock);\r\nspin_unlock(&h->lock);\r\nif (freeit)\r\n__free_raid_bio(rbio);\r\n}\r\nstatic void remove_rbio_from_cache(struct btrfs_raid_bio *rbio)\r\n{\r\nstruct btrfs_stripe_hash_table *table;\r\nunsigned long flags;\r\nif (!test_bit(RBIO_CACHE_BIT, &rbio->flags))\r\nreturn;\r\ntable = rbio->fs_info->stripe_hash_table;\r\nspin_lock_irqsave(&table->cache_lock, flags);\r\n__remove_rbio_from_cache(rbio);\r\nspin_unlock_irqrestore(&table->cache_lock, flags);\r\n}\r\nstatic void btrfs_clear_rbio_cache(struct btrfs_fs_info *info)\r\n{\r\nstruct btrfs_stripe_hash_table *table;\r\nunsigned long flags;\r\nstruct btrfs_raid_bio *rbio;\r\ntable = info->stripe_hash_table;\r\nspin_lock_irqsave(&table->cache_lock, flags);\r\nwhile (!list_empty(&table->stripe_cache)) {\r\nrbio = list_entry(table->stripe_cache.next,\r\nstruct btrfs_raid_bio,\r\nstripe_cache);\r\n__remove_rbio_from_cache(rbio);\r\n}\r\nspin_unlock_irqrestore(&table->cache_lock, flags);\r\n}\r\nvoid btrfs_free_stripe_hash_table(struct btrfs_fs_info *info)\r\n{\r\nif (!info->stripe_hash_table)\r\nreturn;\r\nbtrfs_clear_rbio_cache(info);\r\nif (is_vmalloc_addr(info->stripe_hash_table))\r\nvfree(info->stripe_hash_table);\r\nelse\r\nkfree(info->stripe_hash_table);\r\ninfo->stripe_hash_table = NULL;\r\n}\r\nstatic void cache_rbio(struct btrfs_raid_bio *rbio)\r\n{\r\nstruct btrfs_stripe_hash_table *table;\r\nunsigned long flags;\r\nif (!test_bit(RBIO_CACHE_READY_BIT, &rbio->flags))\r\nreturn;\r\ntable = rbio->fs_info->stripe_hash_table;\r\nspin_lock_irqsave(&table->cache_lock, flags);\r\nspin_lock(&rbio->bio_list_lock);\r\nif (!test_and_set_bit(RBIO_CACHE_BIT, &rbio->flags))\r\natomic_inc(&rbio->refs);\r\nif (!list_empty(&rbio->stripe_cache)){\r\nlist_move(&rbio->stripe_cache, &table->stripe_cache);\r\n} else {\r\nlist_add(&rbio->stripe_cache, &table->stripe_cache);\r\ntable->cache_size += 1;\r\n}\r\nspin_unlock(&rbio->bio_list_lock);\r\nif (table->cache_size > RBIO_CACHE_SIZE) {\r\nstruct btrfs_raid_bio *found;\r\nfound = list_entry(table->stripe_cache.prev,\r\nstruct btrfs_raid_bio,\r\nstripe_cache);\r\nif (found != rbio)\r\n__remove_rbio_from_cache(found);\r\n}\r\nspin_unlock_irqrestore(&table->cache_lock, flags);\r\nreturn;\r\n}\r\nstatic void run_xor(void **pages, int src_cnt, ssize_t len)\r\n{\r\nint src_off = 0;\r\nint xor_src_cnt = 0;\r\nvoid *dest = pages[src_cnt];\r\nwhile(src_cnt > 0) {\r\nxor_src_cnt = min(src_cnt, MAX_XOR_BLOCKS);\r\nxor_blocks(xor_src_cnt, len, dest, pages + src_off);\r\nsrc_cnt -= xor_src_cnt;\r\nsrc_off += xor_src_cnt;\r\n}\r\n}\r\nstatic int __rbio_is_full(struct btrfs_raid_bio *rbio)\r\n{\r\nunsigned long size = rbio->bio_list_bytes;\r\nint ret = 1;\r\nif (size != rbio->nr_data * rbio->stripe_len)\r\nret = 0;\r\nBUG_ON(size > rbio->nr_data * rbio->stripe_len);\r\nreturn ret;\r\n}\r\nstatic int rbio_is_full(struct btrfs_raid_bio *rbio)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(&rbio->bio_list_lock, flags);\r\nret = __rbio_is_full(rbio);\r\nspin_unlock_irqrestore(&rbio->bio_list_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int rbio_can_merge(struct btrfs_raid_bio *last,\r\nstruct btrfs_raid_bio *cur)\r\n{\r\nif (test_bit(RBIO_RMW_LOCKED_BIT, &last->flags) ||\r\ntest_bit(RBIO_RMW_LOCKED_BIT, &cur->flags))\r\nreturn 0;\r\nif (test_bit(RBIO_CACHE_BIT, &last->flags) ||\r\ntest_bit(RBIO_CACHE_BIT, &cur->flags))\r\nreturn 0;\r\nif (last->raid_map[0] !=\r\ncur->raid_map[0])\r\nreturn 0;\r\nif (last->read_rebuild !=\r\ncur->read_rebuild) {\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic struct page *rbio_pstripe_page(struct btrfs_raid_bio *rbio, int index)\r\n{\r\nindex += (rbio->nr_data * rbio->stripe_len) >> PAGE_CACHE_SHIFT;\r\nreturn rbio->stripe_pages[index];\r\n}\r\nstatic struct page *rbio_qstripe_page(struct btrfs_raid_bio *rbio, int index)\r\n{\r\nif (rbio->nr_data + 1 == rbio->bbio->num_stripes)\r\nreturn NULL;\r\nindex += ((rbio->nr_data + 1) * rbio->stripe_len) >>\r\nPAGE_CACHE_SHIFT;\r\nreturn rbio->stripe_pages[index];\r\n}\r\nstatic noinline int lock_stripe_add(struct btrfs_raid_bio *rbio)\r\n{\r\nint bucket = rbio_bucket(rbio);\r\nstruct btrfs_stripe_hash *h = rbio->fs_info->stripe_hash_table->table + bucket;\r\nstruct btrfs_raid_bio *cur;\r\nstruct btrfs_raid_bio *pending;\r\nunsigned long flags;\r\nDEFINE_WAIT(wait);\r\nstruct btrfs_raid_bio *freeit = NULL;\r\nstruct btrfs_raid_bio *cache_drop = NULL;\r\nint ret = 0;\r\nint walk = 0;\r\nspin_lock_irqsave(&h->lock, flags);\r\nlist_for_each_entry(cur, &h->hash_list, hash_list) {\r\nwalk++;\r\nif (cur->raid_map[0] == rbio->raid_map[0]) {\r\nspin_lock(&cur->bio_list_lock);\r\nif (bio_list_empty(&cur->bio_list) &&\r\nlist_empty(&cur->plug_list) &&\r\ntest_bit(RBIO_CACHE_BIT, &cur->flags) &&\r\n!test_bit(RBIO_RMW_LOCKED_BIT, &cur->flags)) {\r\nlist_del_init(&cur->hash_list);\r\natomic_dec(&cur->refs);\r\nsteal_rbio(cur, rbio);\r\ncache_drop = cur;\r\nspin_unlock(&cur->bio_list_lock);\r\ngoto lockit;\r\n}\r\nif (rbio_can_merge(cur, rbio)) {\r\nmerge_rbio(cur, rbio);\r\nspin_unlock(&cur->bio_list_lock);\r\nfreeit = rbio;\r\nret = 1;\r\ngoto out;\r\n}\r\nlist_for_each_entry(pending, &cur->plug_list,\r\nplug_list) {\r\nif (rbio_can_merge(pending, rbio)) {\r\nmerge_rbio(pending, rbio);\r\nspin_unlock(&cur->bio_list_lock);\r\nfreeit = rbio;\r\nret = 1;\r\ngoto out;\r\n}\r\n}\r\nlist_add_tail(&rbio->plug_list, &cur->plug_list);\r\nspin_unlock(&cur->bio_list_lock);\r\nret = 1;\r\ngoto out;\r\n}\r\n}\r\nlockit:\r\natomic_inc(&rbio->refs);\r\nlist_add(&rbio->hash_list, &h->hash_list);\r\nout:\r\nspin_unlock_irqrestore(&h->lock, flags);\r\nif (cache_drop)\r\nremove_rbio_from_cache(cache_drop);\r\nif (freeit)\r\n__free_raid_bio(freeit);\r\nreturn ret;\r\n}\r\nstatic noinline void unlock_stripe(struct btrfs_raid_bio *rbio)\r\n{\r\nint bucket;\r\nstruct btrfs_stripe_hash *h;\r\nunsigned long flags;\r\nint keep_cache = 0;\r\nbucket = rbio_bucket(rbio);\r\nh = rbio->fs_info->stripe_hash_table->table + bucket;\r\nif (list_empty(&rbio->plug_list))\r\ncache_rbio(rbio);\r\nspin_lock_irqsave(&h->lock, flags);\r\nspin_lock(&rbio->bio_list_lock);\r\nif (!list_empty(&rbio->hash_list)) {\r\nif (list_empty(&rbio->plug_list) &&\r\ntest_bit(RBIO_CACHE_BIT, &rbio->flags)) {\r\nkeep_cache = 1;\r\nclear_bit(RBIO_RMW_LOCKED_BIT, &rbio->flags);\r\nBUG_ON(!bio_list_empty(&rbio->bio_list));\r\ngoto done;\r\n}\r\nlist_del_init(&rbio->hash_list);\r\natomic_dec(&rbio->refs);\r\nif (!list_empty(&rbio->plug_list)) {\r\nstruct btrfs_raid_bio *next;\r\nstruct list_head *head = rbio->plug_list.next;\r\nnext = list_entry(head, struct btrfs_raid_bio,\r\nplug_list);\r\nlist_del_init(&rbio->plug_list);\r\nlist_add(&next->hash_list, &h->hash_list);\r\natomic_inc(&next->refs);\r\nspin_unlock(&rbio->bio_list_lock);\r\nspin_unlock_irqrestore(&h->lock, flags);\r\nif (next->read_rebuild)\r\nasync_read_rebuild(next);\r\nelse {\r\nsteal_rbio(rbio, next);\r\nasync_rmw_stripe(next);\r\n}\r\ngoto done_nolock;\r\n} else if (waitqueue_active(&h->wait)) {\r\nspin_unlock(&rbio->bio_list_lock);\r\nspin_unlock_irqrestore(&h->lock, flags);\r\nwake_up(&h->wait);\r\ngoto done_nolock;\r\n}\r\n}\r\ndone:\r\nspin_unlock(&rbio->bio_list_lock);\r\nspin_unlock_irqrestore(&h->lock, flags);\r\ndone_nolock:\r\nif (!keep_cache)\r\nremove_rbio_from_cache(rbio);\r\n}\r\nstatic void __free_raid_bio(struct btrfs_raid_bio *rbio)\r\n{\r\nint i;\r\nWARN_ON(atomic_read(&rbio->refs) < 0);\r\nif (!atomic_dec_and_test(&rbio->refs))\r\nreturn;\r\nWARN_ON(!list_empty(&rbio->stripe_cache));\r\nWARN_ON(!list_empty(&rbio->hash_list));\r\nWARN_ON(!bio_list_empty(&rbio->bio_list));\r\nfor (i = 0; i < rbio->nr_pages; i++) {\r\nif (rbio->stripe_pages[i]) {\r\n__free_page(rbio->stripe_pages[i]);\r\nrbio->stripe_pages[i] = NULL;\r\n}\r\n}\r\nkfree(rbio->raid_map);\r\nkfree(rbio->bbio);\r\nkfree(rbio);\r\n}\r\nstatic void free_raid_bio(struct btrfs_raid_bio *rbio)\r\n{\r\nunlock_stripe(rbio);\r\n__free_raid_bio(rbio);\r\n}\r\nstatic void rbio_orig_end_io(struct btrfs_raid_bio *rbio, int err, int uptodate)\r\n{\r\nstruct bio *cur = bio_list_get(&rbio->bio_list);\r\nstruct bio *next;\r\nfree_raid_bio(rbio);\r\nwhile (cur) {\r\nnext = cur->bi_next;\r\ncur->bi_next = NULL;\r\nif (uptodate)\r\nset_bit(BIO_UPTODATE, &cur->bi_flags);\r\nbio_endio(cur, err);\r\ncur = next;\r\n}\r\n}\r\nstatic void raid_write_end_io(struct bio *bio, int err)\r\n{\r\nstruct btrfs_raid_bio *rbio = bio->bi_private;\r\nif (err)\r\nfail_bio_stripe(rbio, bio);\r\nbio_put(bio);\r\nif (!atomic_dec_and_test(&rbio->bbio->stripes_pending))\r\nreturn;\r\nerr = 0;\r\nif (atomic_read(&rbio->bbio->error) > rbio->bbio->max_errors)\r\nerr = -EIO;\r\nrbio_orig_end_io(rbio, err, 0);\r\nreturn;\r\n}\r\nstatic struct page *page_in_rbio(struct btrfs_raid_bio *rbio,\r\nint index, int pagenr, int bio_list_only)\r\n{\r\nint chunk_page;\r\nstruct page *p = NULL;\r\nchunk_page = index * (rbio->stripe_len >> PAGE_SHIFT) + pagenr;\r\nspin_lock_irq(&rbio->bio_list_lock);\r\np = rbio->bio_pages[chunk_page];\r\nspin_unlock_irq(&rbio->bio_list_lock);\r\nif (p || bio_list_only)\r\nreturn p;\r\nreturn rbio->stripe_pages[chunk_page];\r\n}\r\nstatic unsigned long rbio_nr_pages(unsigned long stripe_len, int nr_stripes)\r\n{\r\nunsigned long nr = stripe_len * nr_stripes;\r\nreturn (nr + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\r\n}\r\nstatic struct btrfs_raid_bio *alloc_rbio(struct btrfs_root *root,\r\nstruct btrfs_bio *bbio, u64 *raid_map,\r\nu64 stripe_len)\r\n{\r\nstruct btrfs_raid_bio *rbio;\r\nint nr_data = 0;\r\nint num_pages = rbio_nr_pages(stripe_len, bbio->num_stripes);\r\nvoid *p;\r\nrbio = kzalloc(sizeof(*rbio) + num_pages * sizeof(struct page *) * 2,\r\nGFP_NOFS);\r\nif (!rbio) {\r\nkfree(raid_map);\r\nkfree(bbio);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nbio_list_init(&rbio->bio_list);\r\nINIT_LIST_HEAD(&rbio->plug_list);\r\nspin_lock_init(&rbio->bio_list_lock);\r\nINIT_LIST_HEAD(&rbio->stripe_cache);\r\nINIT_LIST_HEAD(&rbio->hash_list);\r\nrbio->bbio = bbio;\r\nrbio->raid_map = raid_map;\r\nrbio->fs_info = root->fs_info;\r\nrbio->stripe_len = stripe_len;\r\nrbio->nr_pages = num_pages;\r\nrbio->faila = -1;\r\nrbio->failb = -1;\r\natomic_set(&rbio->refs, 1);\r\np = rbio + 1;\r\nrbio->stripe_pages = p;\r\nrbio->bio_pages = p + sizeof(struct page *) * num_pages;\r\nif (raid_map[bbio->num_stripes - 1] == RAID6_Q_STRIPE)\r\nnr_data = bbio->num_stripes - 2;\r\nelse\r\nnr_data = bbio->num_stripes - 1;\r\nrbio->nr_data = nr_data;\r\nreturn rbio;\r\n}\r\nstatic int alloc_rbio_pages(struct btrfs_raid_bio *rbio)\r\n{\r\nint i;\r\nstruct page *page;\r\nfor (i = 0; i < rbio->nr_pages; i++) {\r\nif (rbio->stripe_pages[i])\r\ncontinue;\r\npage = alloc_page(GFP_NOFS | __GFP_HIGHMEM);\r\nif (!page)\r\nreturn -ENOMEM;\r\nrbio->stripe_pages[i] = page;\r\nClearPageUptodate(page);\r\n}\r\nreturn 0;\r\n}\r\nstatic int alloc_rbio_parity_pages(struct btrfs_raid_bio *rbio)\r\n{\r\nint i;\r\nstruct page *page;\r\ni = (rbio->nr_data * rbio->stripe_len) >> PAGE_CACHE_SHIFT;\r\nfor (; i < rbio->nr_pages; i++) {\r\nif (rbio->stripe_pages[i])\r\ncontinue;\r\npage = alloc_page(GFP_NOFS | __GFP_HIGHMEM);\r\nif (!page)\r\nreturn -ENOMEM;\r\nrbio->stripe_pages[i] = page;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rbio_add_io_page(struct btrfs_raid_bio *rbio,\r\nstruct bio_list *bio_list,\r\nstruct page *page,\r\nint stripe_nr,\r\nunsigned long page_index,\r\nunsigned long bio_max_len)\r\n{\r\nstruct bio *last = bio_list->tail;\r\nu64 last_end = 0;\r\nint ret;\r\nstruct bio *bio;\r\nstruct btrfs_bio_stripe *stripe;\r\nu64 disk_start;\r\nstripe = &rbio->bbio->stripes[stripe_nr];\r\ndisk_start = stripe->physical + (page_index << PAGE_CACHE_SHIFT);\r\nif (!stripe->dev->bdev)\r\nreturn fail_rbio_index(rbio, stripe_nr);\r\nif (last) {\r\nlast_end = (u64)last->bi_sector << 9;\r\nlast_end += last->bi_size;\r\nif (last_end == disk_start && stripe->dev->bdev &&\r\ntest_bit(BIO_UPTODATE, &last->bi_flags) &&\r\nlast->bi_bdev == stripe->dev->bdev) {\r\nret = bio_add_page(last, page, PAGE_CACHE_SIZE, 0);\r\nif (ret == PAGE_CACHE_SIZE)\r\nreturn 0;\r\n}\r\n}\r\nbio = btrfs_io_bio_alloc(GFP_NOFS, bio_max_len >> PAGE_SHIFT?:1);\r\nif (!bio)\r\nreturn -ENOMEM;\r\nbio->bi_size = 0;\r\nbio->bi_bdev = stripe->dev->bdev;\r\nbio->bi_sector = disk_start >> 9;\r\nset_bit(BIO_UPTODATE, &bio->bi_flags);\r\nbio_add_page(bio, page, PAGE_CACHE_SIZE, 0);\r\nbio_list_add(bio_list, bio);\r\nreturn 0;\r\n}\r\nstatic void validate_rbio_for_rmw(struct btrfs_raid_bio *rbio)\r\n{\r\nif (rbio->faila >= 0 || rbio->failb >= 0) {\r\nBUG_ON(rbio->faila == rbio->bbio->num_stripes - 1);\r\n__raid56_parity_recover(rbio);\r\n} else {\r\nfinish_rmw(rbio);\r\n}\r\n}\r\nstatic struct page *rbio_stripe_page(struct btrfs_raid_bio *rbio, int stripe, int page)\r\n{\r\nint index;\r\nindex = stripe * (rbio->stripe_len >> PAGE_CACHE_SHIFT);\r\nindex += page;\r\nreturn rbio->stripe_pages[index];\r\n}\r\nstatic void index_rbio_pages(struct btrfs_raid_bio *rbio)\r\n{\r\nstruct bio *bio;\r\nu64 start;\r\nunsigned long stripe_offset;\r\nunsigned long page_index;\r\nstruct page *p;\r\nint i;\r\nspin_lock_irq(&rbio->bio_list_lock);\r\nbio_list_for_each(bio, &rbio->bio_list) {\r\nstart = (u64)bio->bi_sector << 9;\r\nstripe_offset = start - rbio->raid_map[0];\r\npage_index = stripe_offset >> PAGE_CACHE_SHIFT;\r\nfor (i = 0; i < bio->bi_vcnt; i++) {\r\np = bio->bi_io_vec[i].bv_page;\r\nrbio->bio_pages[page_index + i] = p;\r\n}\r\n}\r\nspin_unlock_irq(&rbio->bio_list_lock);\r\n}\r\nstatic noinline void finish_rmw(struct btrfs_raid_bio *rbio)\r\n{\r\nstruct btrfs_bio *bbio = rbio->bbio;\r\nvoid *pointers[bbio->num_stripes];\r\nint stripe_len = rbio->stripe_len;\r\nint nr_data = rbio->nr_data;\r\nint stripe;\r\nint pagenr;\r\nint p_stripe = -1;\r\nint q_stripe = -1;\r\nstruct bio_list bio_list;\r\nstruct bio *bio;\r\nint pages_per_stripe = stripe_len >> PAGE_CACHE_SHIFT;\r\nint ret;\r\nbio_list_init(&bio_list);\r\nif (bbio->num_stripes - rbio->nr_data == 1) {\r\np_stripe = bbio->num_stripes - 1;\r\n} else if (bbio->num_stripes - rbio->nr_data == 2) {\r\np_stripe = bbio->num_stripes - 2;\r\nq_stripe = bbio->num_stripes - 1;\r\n} else {\r\nBUG();\r\n}\r\nspin_lock_irq(&rbio->bio_list_lock);\r\nset_bit(RBIO_RMW_LOCKED_BIT, &rbio->flags);\r\nspin_unlock_irq(&rbio->bio_list_lock);\r\natomic_set(&rbio->bbio->error, 0);\r\nindex_rbio_pages(rbio);\r\nif (!rbio_is_full(rbio))\r\ncache_rbio_pages(rbio);\r\nelse\r\nclear_bit(RBIO_CACHE_READY_BIT, &rbio->flags);\r\nfor (pagenr = 0; pagenr < pages_per_stripe; pagenr++) {\r\nstruct page *p;\r\nfor (stripe = 0; stripe < nr_data; stripe++) {\r\np = page_in_rbio(rbio, stripe, pagenr, 0);\r\npointers[stripe] = kmap(p);\r\n}\r\np = rbio_pstripe_page(rbio, pagenr);\r\nSetPageUptodate(p);\r\npointers[stripe++] = kmap(p);\r\nif (q_stripe != -1) {\r\np = rbio_qstripe_page(rbio, pagenr);\r\nSetPageUptodate(p);\r\npointers[stripe++] = kmap(p);\r\nraid6_call.gen_syndrome(bbio->num_stripes, PAGE_SIZE,\r\npointers);\r\n} else {\r\nmemcpy(pointers[nr_data], pointers[0], PAGE_SIZE);\r\nrun_xor(pointers + 1, nr_data - 1, PAGE_CACHE_SIZE);\r\n}\r\nfor (stripe = 0; stripe < bbio->num_stripes; stripe++)\r\nkunmap(page_in_rbio(rbio, stripe, pagenr, 0));\r\n}\r\nfor (stripe = 0; stripe < bbio->num_stripes; stripe++) {\r\nfor (pagenr = 0; pagenr < pages_per_stripe; pagenr++) {\r\nstruct page *page;\r\nif (stripe < rbio->nr_data) {\r\npage = page_in_rbio(rbio, stripe, pagenr, 1);\r\nif (!page)\r\ncontinue;\r\n} else {\r\npage = rbio_stripe_page(rbio, stripe, pagenr);\r\n}\r\nret = rbio_add_io_page(rbio, &bio_list,\r\npage, stripe, pagenr, rbio->stripe_len);\r\nif (ret)\r\ngoto cleanup;\r\n}\r\n}\r\natomic_set(&bbio->stripes_pending, bio_list_size(&bio_list));\r\nBUG_ON(atomic_read(&bbio->stripes_pending) == 0);\r\nwhile (1) {\r\nbio = bio_list_pop(&bio_list);\r\nif (!bio)\r\nbreak;\r\nbio->bi_private = rbio;\r\nbio->bi_end_io = raid_write_end_io;\r\nBUG_ON(!test_bit(BIO_UPTODATE, &bio->bi_flags));\r\nsubmit_bio(WRITE, bio);\r\n}\r\nreturn;\r\ncleanup:\r\nrbio_orig_end_io(rbio, -EIO, 0);\r\n}\r\nstatic int find_bio_stripe(struct btrfs_raid_bio *rbio,\r\nstruct bio *bio)\r\n{\r\nu64 physical = bio->bi_sector;\r\nu64 stripe_start;\r\nint i;\r\nstruct btrfs_bio_stripe *stripe;\r\nphysical <<= 9;\r\nfor (i = 0; i < rbio->bbio->num_stripes; i++) {\r\nstripe = &rbio->bbio->stripes[i];\r\nstripe_start = stripe->physical;\r\nif (physical >= stripe_start &&\r\nphysical < stripe_start + rbio->stripe_len) {\r\nreturn i;\r\n}\r\n}\r\nreturn -1;\r\n}\r\nstatic int find_logical_bio_stripe(struct btrfs_raid_bio *rbio,\r\nstruct bio *bio)\r\n{\r\nu64 logical = bio->bi_sector;\r\nu64 stripe_start;\r\nint i;\r\nlogical <<= 9;\r\nfor (i = 0; i < rbio->nr_data; i++) {\r\nstripe_start = rbio->raid_map[i];\r\nif (logical >= stripe_start &&\r\nlogical < stripe_start + rbio->stripe_len) {\r\nreturn i;\r\n}\r\n}\r\nreturn -1;\r\n}\r\nstatic int fail_rbio_index(struct btrfs_raid_bio *rbio, int failed)\r\n{\r\nunsigned long flags;\r\nint ret = 0;\r\nspin_lock_irqsave(&rbio->bio_list_lock, flags);\r\nif (rbio->faila == failed || rbio->failb == failed)\r\ngoto out;\r\nif (rbio->faila == -1) {\r\nrbio->faila = failed;\r\natomic_inc(&rbio->bbio->error);\r\n} else if (rbio->failb == -1) {\r\nrbio->failb = failed;\r\natomic_inc(&rbio->bbio->error);\r\n} else {\r\nret = -EIO;\r\n}\r\nout:\r\nspin_unlock_irqrestore(&rbio->bio_list_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int fail_bio_stripe(struct btrfs_raid_bio *rbio,\r\nstruct bio *bio)\r\n{\r\nint failed = find_bio_stripe(rbio, bio);\r\nif (failed < 0)\r\nreturn -EIO;\r\nreturn fail_rbio_index(rbio, failed);\r\n}\r\nstatic void set_bio_pages_uptodate(struct bio *bio)\r\n{\r\nint i;\r\nstruct page *p;\r\nfor (i = 0; i < bio->bi_vcnt; i++) {\r\np = bio->bi_io_vec[i].bv_page;\r\nSetPageUptodate(p);\r\n}\r\n}\r\nstatic void raid_rmw_end_io(struct bio *bio, int err)\r\n{\r\nstruct btrfs_raid_bio *rbio = bio->bi_private;\r\nif (err)\r\nfail_bio_stripe(rbio, bio);\r\nelse\r\nset_bio_pages_uptodate(bio);\r\nbio_put(bio);\r\nif (!atomic_dec_and_test(&rbio->bbio->stripes_pending))\r\nreturn;\r\nerr = 0;\r\nif (atomic_read(&rbio->bbio->error) > rbio->bbio->max_errors)\r\ngoto cleanup;\r\nvalidate_rbio_for_rmw(rbio);\r\nreturn;\r\ncleanup:\r\nrbio_orig_end_io(rbio, -EIO, 0);\r\n}\r\nstatic void async_rmw_stripe(struct btrfs_raid_bio *rbio)\r\n{\r\nrbio->work.flags = 0;\r\nrbio->work.func = rmw_work;\r\nbtrfs_queue_worker(&rbio->fs_info->rmw_workers,\r\n&rbio->work);\r\n}\r\nstatic void async_read_rebuild(struct btrfs_raid_bio *rbio)\r\n{\r\nrbio->work.flags = 0;\r\nrbio->work.func = read_rebuild_work;\r\nbtrfs_queue_worker(&rbio->fs_info->rmw_workers,\r\n&rbio->work);\r\n}\r\nstatic int raid56_rmw_stripe(struct btrfs_raid_bio *rbio)\r\n{\r\nint bios_to_read = 0;\r\nstruct btrfs_bio *bbio = rbio->bbio;\r\nstruct bio_list bio_list;\r\nint ret;\r\nint nr_pages = (rbio->stripe_len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\r\nint pagenr;\r\nint stripe;\r\nstruct bio *bio;\r\nbio_list_init(&bio_list);\r\nret = alloc_rbio_pages(rbio);\r\nif (ret)\r\ngoto cleanup;\r\nindex_rbio_pages(rbio);\r\natomic_set(&rbio->bbio->error, 0);\r\nfor (stripe = 0; stripe < rbio->nr_data; stripe++) {\r\nfor (pagenr = 0; pagenr < nr_pages; pagenr++) {\r\nstruct page *page;\r\npage = page_in_rbio(rbio, stripe, pagenr, 1);\r\nif (page)\r\ncontinue;\r\npage = rbio_stripe_page(rbio, stripe, pagenr);\r\nif (PageUptodate(page))\r\ncontinue;\r\nret = rbio_add_io_page(rbio, &bio_list, page,\r\nstripe, pagenr, rbio->stripe_len);\r\nif (ret)\r\ngoto cleanup;\r\n}\r\n}\r\nbios_to_read = bio_list_size(&bio_list);\r\nif (!bios_to_read) {\r\ngoto finish;\r\n}\r\natomic_set(&bbio->stripes_pending, bios_to_read);\r\nwhile (1) {\r\nbio = bio_list_pop(&bio_list);\r\nif (!bio)\r\nbreak;\r\nbio->bi_private = rbio;\r\nbio->bi_end_io = raid_rmw_end_io;\r\nbtrfs_bio_wq_end_io(rbio->fs_info, bio,\r\nBTRFS_WQ_ENDIO_RAID56);\r\nBUG_ON(!test_bit(BIO_UPTODATE, &bio->bi_flags));\r\nsubmit_bio(READ, bio);\r\n}\r\nreturn 0;\r\ncleanup:\r\nrbio_orig_end_io(rbio, -EIO, 0);\r\nreturn -EIO;\r\nfinish:\r\nvalidate_rbio_for_rmw(rbio);\r\nreturn 0;\r\n}\r\nstatic int full_stripe_write(struct btrfs_raid_bio *rbio)\r\n{\r\nint ret;\r\nret = alloc_rbio_parity_pages(rbio);\r\nif (ret)\r\nreturn ret;\r\nret = lock_stripe_add(rbio);\r\nif (ret == 0)\r\nfinish_rmw(rbio);\r\nreturn 0;\r\n}\r\nstatic int partial_stripe_write(struct btrfs_raid_bio *rbio)\r\n{\r\nint ret;\r\nret = lock_stripe_add(rbio);\r\nif (ret == 0)\r\nasync_rmw_stripe(rbio);\r\nreturn 0;\r\n}\r\nstatic int __raid56_parity_write(struct btrfs_raid_bio *rbio)\r\n{\r\nif (!rbio_is_full(rbio))\r\nreturn partial_stripe_write(rbio);\r\nreturn full_stripe_write(rbio);\r\n}\r\nstatic int plug_cmp(void *priv, struct list_head *a, struct list_head *b)\r\n{\r\nstruct btrfs_raid_bio *ra = container_of(a, struct btrfs_raid_bio,\r\nplug_list);\r\nstruct btrfs_raid_bio *rb = container_of(b, struct btrfs_raid_bio,\r\nplug_list);\r\nu64 a_sector = ra->bio_list.head->bi_sector;\r\nu64 b_sector = rb->bio_list.head->bi_sector;\r\nif (a_sector < b_sector)\r\nreturn -1;\r\nif (a_sector > b_sector)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void run_plug(struct btrfs_plug_cb *plug)\r\n{\r\nstruct btrfs_raid_bio *cur;\r\nstruct btrfs_raid_bio *last = NULL;\r\nlist_sort(NULL, &plug->rbio_list, plug_cmp);\r\nwhile (!list_empty(&plug->rbio_list)) {\r\ncur = list_entry(plug->rbio_list.next,\r\nstruct btrfs_raid_bio, plug_list);\r\nlist_del_init(&cur->plug_list);\r\nif (rbio_is_full(cur)) {\r\nfull_stripe_write(cur);\r\ncontinue;\r\n}\r\nif (last) {\r\nif (rbio_can_merge(last, cur)) {\r\nmerge_rbio(last, cur);\r\n__free_raid_bio(cur);\r\ncontinue;\r\n}\r\n__raid56_parity_write(last);\r\n}\r\nlast = cur;\r\n}\r\nif (last) {\r\n__raid56_parity_write(last);\r\n}\r\nkfree(plug);\r\n}\r\nstatic void unplug_work(struct btrfs_work *work)\r\n{\r\nstruct btrfs_plug_cb *plug;\r\nplug = container_of(work, struct btrfs_plug_cb, work);\r\nrun_plug(plug);\r\n}\r\nstatic void btrfs_raid_unplug(struct blk_plug_cb *cb, bool from_schedule)\r\n{\r\nstruct btrfs_plug_cb *plug;\r\nplug = container_of(cb, struct btrfs_plug_cb, cb);\r\nif (from_schedule) {\r\nplug->work.flags = 0;\r\nplug->work.func = unplug_work;\r\nbtrfs_queue_worker(&plug->info->rmw_workers,\r\n&plug->work);\r\nreturn;\r\n}\r\nrun_plug(plug);\r\n}\r\nint raid56_parity_write(struct btrfs_root *root, struct bio *bio,\r\nstruct btrfs_bio *bbio, u64 *raid_map,\r\nu64 stripe_len)\r\n{\r\nstruct btrfs_raid_bio *rbio;\r\nstruct btrfs_plug_cb *plug = NULL;\r\nstruct blk_plug_cb *cb;\r\nrbio = alloc_rbio(root, bbio, raid_map, stripe_len);\r\nif (IS_ERR(rbio)) {\r\nkfree(raid_map);\r\nkfree(bbio);\r\nreturn PTR_ERR(rbio);\r\n}\r\nbio_list_add(&rbio->bio_list, bio);\r\nrbio->bio_list_bytes = bio->bi_size;\r\nif (rbio_is_full(rbio))\r\nreturn full_stripe_write(rbio);\r\ncb = blk_check_plugged(btrfs_raid_unplug, root->fs_info,\r\nsizeof(*plug));\r\nif (cb) {\r\nplug = container_of(cb, struct btrfs_plug_cb, cb);\r\nif (!plug->info) {\r\nplug->info = root->fs_info;\r\nINIT_LIST_HEAD(&plug->rbio_list);\r\n}\r\nlist_add_tail(&rbio->plug_list, &plug->rbio_list);\r\n} else {\r\nreturn __raid56_parity_write(rbio);\r\n}\r\nreturn 0;\r\n}\r\nstatic void __raid_recover_end_io(struct btrfs_raid_bio *rbio)\r\n{\r\nint pagenr, stripe;\r\nvoid **pointers;\r\nint faila = -1, failb = -1;\r\nint nr_pages = (rbio->stripe_len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\r\nstruct page *page;\r\nint err;\r\nint i;\r\npointers = kzalloc(rbio->bbio->num_stripes * sizeof(void *),\r\nGFP_NOFS);\r\nif (!pointers) {\r\nerr = -ENOMEM;\r\ngoto cleanup_io;\r\n}\r\nfaila = rbio->faila;\r\nfailb = rbio->failb;\r\nif (rbio->read_rebuild) {\r\nspin_lock_irq(&rbio->bio_list_lock);\r\nset_bit(RBIO_RMW_LOCKED_BIT, &rbio->flags);\r\nspin_unlock_irq(&rbio->bio_list_lock);\r\n}\r\nindex_rbio_pages(rbio);\r\nfor (pagenr = 0; pagenr < nr_pages; pagenr++) {\r\nfor (stripe = 0; stripe < rbio->bbio->num_stripes; stripe++) {\r\nif (rbio->read_rebuild &&\r\n(stripe == faila || stripe == failb)) {\r\npage = page_in_rbio(rbio, stripe, pagenr, 0);\r\n} else {\r\npage = rbio_stripe_page(rbio, stripe, pagenr);\r\n}\r\npointers[stripe] = kmap(page);\r\n}\r\nif (rbio->raid_map[rbio->bbio->num_stripes - 1] ==\r\nRAID6_Q_STRIPE) {\r\nif (failb < 0) {\r\nif (faila == rbio->nr_data) {\r\nerr = -EIO;\r\ngoto cleanup;\r\n}\r\ngoto pstripe;\r\n}\r\nif (faila > failb) {\r\nint tmp = failb;\r\nfailb = faila;\r\nfaila = tmp;\r\n}\r\nif (rbio->raid_map[failb] == RAID6_Q_STRIPE) {\r\nif (rbio->raid_map[faila] == RAID5_P_STRIPE) {\r\nerr = -EIO;\r\ngoto cleanup;\r\n}\r\ngoto pstripe;\r\n}\r\nif (rbio->raid_map[failb] == RAID5_P_STRIPE) {\r\nraid6_datap_recov(rbio->bbio->num_stripes,\r\nPAGE_SIZE, faila, pointers);\r\n} else {\r\nraid6_2data_recov(rbio->bbio->num_stripes,\r\nPAGE_SIZE, faila, failb,\r\npointers);\r\n}\r\n} else {\r\nvoid *p;\r\nBUG_ON(failb != -1);\r\npstripe:\r\nmemcpy(pointers[faila],\r\npointers[rbio->nr_data],\r\nPAGE_CACHE_SIZE);\r\np = pointers[faila];\r\nfor (stripe = faila; stripe < rbio->nr_data - 1; stripe++)\r\npointers[stripe] = pointers[stripe + 1];\r\npointers[rbio->nr_data - 1] = p;\r\nrun_xor(pointers, rbio->nr_data - 1, PAGE_CACHE_SIZE);\r\n}\r\nif (!rbio->read_rebuild) {\r\nfor (i = 0; i < nr_pages; i++) {\r\nif (faila != -1) {\r\npage = rbio_stripe_page(rbio, faila, i);\r\nSetPageUptodate(page);\r\n}\r\nif (failb != -1) {\r\npage = rbio_stripe_page(rbio, failb, i);\r\nSetPageUptodate(page);\r\n}\r\n}\r\n}\r\nfor (stripe = 0; stripe < rbio->bbio->num_stripes; stripe++) {\r\nif (rbio->read_rebuild &&\r\n(stripe == faila || stripe == failb)) {\r\npage = page_in_rbio(rbio, stripe, pagenr, 0);\r\n} else {\r\npage = rbio_stripe_page(rbio, stripe, pagenr);\r\n}\r\nkunmap(page);\r\n}\r\n}\r\nerr = 0;\r\ncleanup:\r\nkfree(pointers);\r\ncleanup_io:\r\nif (rbio->read_rebuild) {\r\nif (err == 0)\r\ncache_rbio_pages(rbio);\r\nelse\r\nclear_bit(RBIO_CACHE_READY_BIT, &rbio->flags);\r\nrbio_orig_end_io(rbio, err, err == 0);\r\n} else if (err == 0) {\r\nrbio->faila = -1;\r\nrbio->failb = -1;\r\nfinish_rmw(rbio);\r\n} else {\r\nrbio_orig_end_io(rbio, err, 0);\r\n}\r\n}\r\nstatic void raid_recover_end_io(struct bio *bio, int err)\r\n{\r\nstruct btrfs_raid_bio *rbio = bio->bi_private;\r\nif (err)\r\nfail_bio_stripe(rbio, bio);\r\nelse\r\nset_bio_pages_uptodate(bio);\r\nbio_put(bio);\r\nif (!atomic_dec_and_test(&rbio->bbio->stripes_pending))\r\nreturn;\r\nif (atomic_read(&rbio->bbio->error) > rbio->bbio->max_errors)\r\nrbio_orig_end_io(rbio, -EIO, 0);\r\nelse\r\n__raid_recover_end_io(rbio);\r\n}\r\nstatic int __raid56_parity_recover(struct btrfs_raid_bio *rbio)\r\n{\r\nint bios_to_read = 0;\r\nstruct btrfs_bio *bbio = rbio->bbio;\r\nstruct bio_list bio_list;\r\nint ret;\r\nint nr_pages = (rbio->stripe_len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\r\nint pagenr;\r\nint stripe;\r\nstruct bio *bio;\r\nbio_list_init(&bio_list);\r\nret = alloc_rbio_pages(rbio);\r\nif (ret)\r\ngoto cleanup;\r\natomic_set(&rbio->bbio->error, 0);\r\nfor (stripe = 0; stripe < bbio->num_stripes; stripe++) {\r\nif (rbio->faila == stripe ||\r\nrbio->failb == stripe)\r\ncontinue;\r\nfor (pagenr = 0; pagenr < nr_pages; pagenr++) {\r\nstruct page *p;\r\np = rbio_stripe_page(rbio, stripe, pagenr);\r\nif (PageUptodate(p))\r\ncontinue;\r\nret = rbio_add_io_page(rbio, &bio_list,\r\nrbio_stripe_page(rbio, stripe, pagenr),\r\nstripe, pagenr, rbio->stripe_len);\r\nif (ret < 0)\r\ngoto cleanup;\r\n}\r\n}\r\nbios_to_read = bio_list_size(&bio_list);\r\nif (!bios_to_read) {\r\nif (atomic_read(&rbio->bbio->error) <= rbio->bbio->max_errors) {\r\n__raid_recover_end_io(rbio);\r\ngoto out;\r\n} else {\r\ngoto cleanup;\r\n}\r\n}\r\natomic_set(&bbio->stripes_pending, bios_to_read);\r\nwhile (1) {\r\nbio = bio_list_pop(&bio_list);\r\nif (!bio)\r\nbreak;\r\nbio->bi_private = rbio;\r\nbio->bi_end_io = raid_recover_end_io;\r\nbtrfs_bio_wq_end_io(rbio->fs_info, bio,\r\nBTRFS_WQ_ENDIO_RAID56);\r\nBUG_ON(!test_bit(BIO_UPTODATE, &bio->bi_flags));\r\nsubmit_bio(READ, bio);\r\n}\r\nout:\r\nreturn 0;\r\ncleanup:\r\nif (rbio->read_rebuild)\r\nrbio_orig_end_io(rbio, -EIO, 0);\r\nreturn -EIO;\r\n}\r\nint raid56_parity_recover(struct btrfs_root *root, struct bio *bio,\r\nstruct btrfs_bio *bbio, u64 *raid_map,\r\nu64 stripe_len, int mirror_num)\r\n{\r\nstruct btrfs_raid_bio *rbio;\r\nint ret;\r\nrbio = alloc_rbio(root, bbio, raid_map, stripe_len);\r\nif (IS_ERR(rbio)) {\r\nreturn PTR_ERR(rbio);\r\n}\r\nrbio->read_rebuild = 1;\r\nbio_list_add(&rbio->bio_list, bio);\r\nrbio->bio_list_bytes = bio->bi_size;\r\nrbio->faila = find_logical_bio_stripe(rbio, bio);\r\nif (rbio->faila == -1) {\r\nBUG();\r\nkfree(rbio);\r\nreturn -EIO;\r\n}\r\nif (mirror_num == 3)\r\nrbio->failb = bbio->num_stripes - 2;\r\nret = lock_stripe_add(rbio);\r\nif (ret == 0)\r\n__raid56_parity_recover(rbio);\r\nreturn 0;\r\n}\r\nstatic void rmw_work(struct btrfs_work *work)\r\n{\r\nstruct btrfs_raid_bio *rbio;\r\nrbio = container_of(work, struct btrfs_raid_bio, work);\r\nraid56_rmw_stripe(rbio);\r\n}\r\nstatic void read_rebuild_work(struct btrfs_work *work)\r\n{\r\nstruct btrfs_raid_bio *rbio;\r\nrbio = container_of(work, struct btrfs_raid_bio, work);\r\n__raid56_parity_recover(rbio);\r\n}
