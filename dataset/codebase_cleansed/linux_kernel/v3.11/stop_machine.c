static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)\r\n{\r\nmemset(done, 0, sizeof(*done));\r\natomic_set(&done->nr_todo, nr_todo);\r\ninit_completion(&done->completion);\r\n}\r\nstatic void cpu_stop_signal_done(struct cpu_stop_done *done, bool executed)\r\n{\r\nif (done) {\r\nif (executed)\r\ndone->executed = true;\r\nif (atomic_dec_and_test(&done->nr_todo))\r\ncomplete(&done->completion);\r\n}\r\n}\r\nstatic void cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nstruct task_struct *p = per_cpu(cpu_stopper_task, cpu);\r\nunsigned long flags;\r\nspin_lock_irqsave(&stopper->lock, flags);\r\nif (stopper->enabled) {\r\nlist_add_tail(&work->list, &stopper->works);\r\nwake_up_process(p);\r\n} else\r\ncpu_stop_signal_done(work->done, false);\r\nspin_unlock_irqrestore(&stopper->lock, flags);\r\n}\r\nint stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)\r\n{\r\nstruct cpu_stop_done done;\r\nstruct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done };\r\ncpu_stop_init_done(&done, 1);\r\ncpu_stop_queue_work(cpu, &work);\r\nwait_for_completion(&done.completion);\r\nreturn done.executed ? done.ret : -ENOENT;\r\n}\r\nvoid stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,\r\nstruct cpu_stop_work *work_buf)\r\n{\r\n*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, };\r\ncpu_stop_queue_work(cpu, work_buf);\r\n}\r\nstatic void queue_stop_cpus_work(const struct cpumask *cpumask,\r\ncpu_stop_fn_t fn, void *arg,\r\nstruct cpu_stop_done *done)\r\n{\r\nstruct cpu_stop_work *work;\r\nunsigned int cpu;\r\nfor_each_cpu(cpu, cpumask) {\r\nwork = &per_cpu(stop_cpus_work, cpu);\r\nwork->fn = fn;\r\nwork->arg = arg;\r\nwork->done = done;\r\n}\r\npreempt_disable();\r\nfor_each_cpu(cpu, cpumask)\r\ncpu_stop_queue_work(cpu, &per_cpu(stop_cpus_work, cpu));\r\npreempt_enable();\r\n}\r\nstatic int __stop_cpus(const struct cpumask *cpumask,\r\ncpu_stop_fn_t fn, void *arg)\r\n{\r\nstruct cpu_stop_done done;\r\ncpu_stop_init_done(&done, cpumask_weight(cpumask));\r\nqueue_stop_cpus_work(cpumask, fn, arg, &done);\r\nwait_for_completion(&done.completion);\r\nreturn done.executed ? done.ret : -ENOENT;\r\n}\r\nint stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\r\n{\r\nint ret;\r\nmutex_lock(&stop_cpus_mutex);\r\nret = __stop_cpus(cpumask, fn, arg);\r\nmutex_unlock(&stop_cpus_mutex);\r\nreturn ret;\r\n}\r\nint try_stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\r\n{\r\nint ret;\r\nif (!mutex_trylock(&stop_cpus_mutex))\r\nreturn -EAGAIN;\r\nret = __stop_cpus(cpumask, fn, arg);\r\nmutex_unlock(&stop_cpus_mutex);\r\nreturn ret;\r\n}\r\nstatic int cpu_stop_should_run(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nunsigned long flags;\r\nint run;\r\nspin_lock_irqsave(&stopper->lock, flags);\r\nrun = !list_empty(&stopper->works);\r\nspin_unlock_irqrestore(&stopper->lock, flags);\r\nreturn run;\r\n}\r\nstatic void cpu_stopper_thread(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nstruct cpu_stop_work *work;\r\nint ret;\r\nrepeat:\r\nwork = NULL;\r\nspin_lock_irq(&stopper->lock);\r\nif (!list_empty(&stopper->works)) {\r\nwork = list_first_entry(&stopper->works,\r\nstruct cpu_stop_work, list);\r\nlist_del_init(&work->list);\r\n}\r\nspin_unlock_irq(&stopper->lock);\r\nif (work) {\r\ncpu_stop_fn_t fn = work->fn;\r\nvoid *arg = work->arg;\r\nstruct cpu_stop_done *done = work->done;\r\nchar ksym_buf[KSYM_NAME_LEN] __maybe_unused;\r\npreempt_disable();\r\nret = fn(arg);\r\nif (ret)\r\ndone->ret = ret;\r\npreempt_enable();\r\nWARN_ONCE(preempt_count(),\r\n"cpu_stop: %s(%p) leaked preempt count\n",\r\nkallsyms_lookup((unsigned long)fn, NULL, NULL, NULL,\r\nksym_buf), arg);\r\ncpu_stop_signal_done(done, true);\r\ngoto repeat;\r\n}\r\n}\r\nstatic void cpu_stop_create(unsigned int cpu)\r\n{\r\nsched_set_stop_task(cpu, per_cpu(cpu_stopper_task, cpu));\r\n}\r\nstatic void cpu_stop_park(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nstruct cpu_stop_work *work;\r\nunsigned long flags;\r\nspin_lock_irqsave(&stopper->lock, flags);\r\nlist_for_each_entry(work, &stopper->works, list)\r\ncpu_stop_signal_done(work->done, false);\r\nstopper->enabled = false;\r\nspin_unlock_irqrestore(&stopper->lock, flags);\r\n}\r\nstatic void cpu_stop_unpark(unsigned int cpu)\r\n{\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nspin_lock_irq(&stopper->lock);\r\nstopper->enabled = true;\r\nspin_unlock_irq(&stopper->lock);\r\n}\r\nstatic int __init cpu_stop_init(void)\r\n{\r\nunsigned int cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\r\nspin_lock_init(&stopper->lock);\r\nINIT_LIST_HEAD(&stopper->works);\r\n}\r\nBUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));\r\nstop_machine_initialized = true;\r\nreturn 0;\r\n}\r\nstatic void set_state(struct stop_machine_data *smdata,\r\nenum stopmachine_state newstate)\r\n{\r\natomic_set(&smdata->thread_ack, smdata->num_threads);\r\nsmp_wmb();\r\nsmdata->state = newstate;\r\n}\r\nstatic void ack_state(struct stop_machine_data *smdata)\r\n{\r\nif (atomic_dec_and_test(&smdata->thread_ack))\r\nset_state(smdata, smdata->state + 1);\r\n}\r\nstatic int stop_machine_cpu_stop(void *data)\r\n{\r\nstruct stop_machine_data *smdata = data;\r\nenum stopmachine_state curstate = STOPMACHINE_NONE;\r\nint cpu = smp_processor_id(), err = 0;\r\nunsigned long flags;\r\nbool is_active;\r\nlocal_save_flags(flags);\r\nif (!smdata->active_cpus)\r\nis_active = cpu == cpumask_first(cpu_online_mask);\r\nelse\r\nis_active = cpumask_test_cpu(cpu, smdata->active_cpus);\r\ndo {\r\ncpu_relax();\r\nif (smdata->state != curstate) {\r\ncurstate = smdata->state;\r\nswitch (curstate) {\r\ncase STOPMACHINE_DISABLE_IRQ:\r\nlocal_irq_disable();\r\nhard_irq_disable();\r\nbreak;\r\ncase STOPMACHINE_RUN:\r\nif (is_active)\r\nerr = smdata->fn(smdata->data);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nack_state(smdata);\r\n}\r\n} while (curstate != STOPMACHINE_EXIT);\r\nlocal_irq_restore(flags);\r\nreturn err;\r\n}\r\nint __stop_machine(int (*fn)(void *), void *data, const struct cpumask *cpus)\r\n{\r\nstruct stop_machine_data smdata = { .fn = fn, .data = data,\r\n.num_threads = num_online_cpus(),\r\n.active_cpus = cpus };\r\nif (!stop_machine_initialized) {\r\nunsigned long flags;\r\nint ret;\r\nWARN_ON_ONCE(smdata.num_threads != 1);\r\nlocal_irq_save(flags);\r\nhard_irq_disable();\r\nret = (*fn)(data);\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nset_state(&smdata, STOPMACHINE_PREPARE);\r\nreturn stop_cpus(cpu_online_mask, stop_machine_cpu_stop, &smdata);\r\n}\r\nint stop_machine(int (*fn)(void *), void *data, const struct cpumask *cpus)\r\n{\r\nint ret;\r\nget_online_cpus();\r\nret = __stop_machine(fn, data, cpus);\r\nput_online_cpus();\r\nreturn ret;\r\n}\r\nint stop_machine_from_inactive_cpu(int (*fn)(void *), void *data,\r\nconst struct cpumask *cpus)\r\n{\r\nstruct stop_machine_data smdata = { .fn = fn, .data = data,\r\n.active_cpus = cpus };\r\nstruct cpu_stop_done done;\r\nint ret;\r\nBUG_ON(cpu_active(raw_smp_processor_id()));\r\nsmdata.num_threads = num_active_cpus() + 1;\r\nwhile (!mutex_trylock(&stop_cpus_mutex))\r\ncpu_relax();\r\nset_state(&smdata, STOPMACHINE_PREPARE);\r\ncpu_stop_init_done(&done, num_active_cpus());\r\nqueue_stop_cpus_work(cpu_active_mask, stop_machine_cpu_stop, &smdata,\r\n&done);\r\nret = stop_machine_cpu_stop(&smdata);\r\nwhile (!completion_done(&done.completion))\r\ncpu_relax();\r\nmutex_unlock(&stop_cpus_mutex);\r\nreturn ret ?: done.ret;\r\n}
