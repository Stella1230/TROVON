static __init int init_emergency_pool(void)\r\n{\r\n#if defined(CONFIG_HIGHMEM) && !defined(CONFIG_MEMORY_HOTPLUG)\r\nif (max_pfn <= max_low_pfn)\r\nreturn 0;\r\n#endif\r\npage_pool = mempool_create_page_pool(POOL_SIZE, 0);\r\nBUG_ON(!page_pool);\r\nprintk("bounce pool size: %d pages\n", POOL_SIZE);\r\nreturn 0;\r\n}\r\nstatic void bounce_copy_vec(struct bio_vec *to, unsigned char *vfrom)\r\n{\r\nunsigned long flags;\r\nunsigned char *vto;\r\nlocal_irq_save(flags);\r\nvto = kmap_atomic(to->bv_page);\r\nmemcpy(vto + to->bv_offset, vfrom, to->bv_len);\r\nkunmap_atomic(vto);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void *mempool_alloc_pages_isa(gfp_t gfp_mask, void *data)\r\n{\r\nreturn mempool_alloc_pages(gfp_mask | GFP_DMA, data);\r\n}\r\nint init_emergency_isa_pool(void)\r\n{\r\nif (isa_page_pool)\r\nreturn 0;\r\nisa_page_pool = mempool_create(ISA_POOL_SIZE, mempool_alloc_pages_isa,\r\nmempool_free_pages, (void *) 0);\r\nBUG_ON(!isa_page_pool);\r\nprintk("isa bounce pool size: %d pages\n", ISA_POOL_SIZE);\r\nreturn 0;\r\n}\r\nstatic void copy_to_high_bio_irq(struct bio *to, struct bio *from)\r\n{\r\nunsigned char *vfrom;\r\nstruct bio_vec *tovec, *fromvec;\r\nint i;\r\nbio_for_each_segment(tovec, to, i) {\r\nfromvec = from->bi_io_vec + i;\r\nif (tovec->bv_page == fromvec->bv_page)\r\ncontinue;\r\nvfrom = page_address(fromvec->bv_page) + tovec->bv_offset;\r\nbounce_copy_vec(tovec, vfrom);\r\nflush_dcache_page(tovec->bv_page);\r\n}\r\n}\r\nstatic void bounce_end_io(struct bio *bio, mempool_t *pool, int err)\r\n{\r\nstruct bio *bio_orig = bio->bi_private;\r\nstruct bio_vec *bvec, *org_vec;\r\nint i;\r\nif (test_bit(BIO_EOPNOTSUPP, &bio->bi_flags))\r\nset_bit(BIO_EOPNOTSUPP, &bio_orig->bi_flags);\r\nbio_for_each_segment_all(bvec, bio, i) {\r\norg_vec = bio_orig->bi_io_vec + i;\r\nif (bvec->bv_page == org_vec->bv_page)\r\ncontinue;\r\ndec_zone_page_state(bvec->bv_page, NR_BOUNCE);\r\nmempool_free(bvec->bv_page, pool);\r\n}\r\nbio_endio(bio_orig, err);\r\nbio_put(bio);\r\n}\r\nstatic void bounce_end_io_write(struct bio *bio, int err)\r\n{\r\nbounce_end_io(bio, page_pool, err);\r\n}\r\nstatic void bounce_end_io_write_isa(struct bio *bio, int err)\r\n{\r\nbounce_end_io(bio, isa_page_pool, err);\r\n}\r\nstatic void __bounce_end_io_read(struct bio *bio, mempool_t *pool, int err)\r\n{\r\nstruct bio *bio_orig = bio->bi_private;\r\nif (test_bit(BIO_UPTODATE, &bio->bi_flags))\r\ncopy_to_high_bio_irq(bio_orig, bio);\r\nbounce_end_io(bio, pool, err);\r\n}\r\nstatic void bounce_end_io_read(struct bio *bio, int err)\r\n{\r\n__bounce_end_io_read(bio, page_pool, err);\r\n}\r\nstatic void bounce_end_io_read_isa(struct bio *bio, int err)\r\n{\r\n__bounce_end_io_read(bio, isa_page_pool, err);\r\n}\r\nstatic int must_snapshot_stable_pages(struct request_queue *q, struct bio *bio)\r\n{\r\nif (bio_data_dir(bio) != WRITE)\r\nreturn 0;\r\nif (!bdi_cap_stable_pages_required(&q->backing_dev_info))\r\nreturn 0;\r\nreturn test_bit(BIO_SNAP_STABLE, &bio->bi_flags);\r\n}\r\nstatic int must_snapshot_stable_pages(struct request_queue *q, struct bio *bio)\r\n{\r\nreturn 0;\r\n}\r\nstatic void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,\r\nmempool_t *pool, int force)\r\n{\r\nstruct bio *bio;\r\nint rw = bio_data_dir(*bio_orig);\r\nstruct bio_vec *to, *from;\r\nunsigned i;\r\nbio_for_each_segment(from, *bio_orig, i)\r\nif (page_to_pfn(from->bv_page) > queue_bounce_pfn(q))\r\ngoto bounce;\r\nreturn;\r\nbounce:\r\nbio = bio_clone_bioset(*bio_orig, GFP_NOIO, fs_bio_set);\r\nbio_for_each_segment_all(to, bio, i) {\r\nstruct page *page = to->bv_page;\r\nif (page_to_pfn(page) <= queue_bounce_pfn(q) && !force)\r\ncontinue;\r\ninc_zone_page_state(to->bv_page, NR_BOUNCE);\r\nto->bv_page = mempool_alloc(pool, q->bounce_gfp);\r\nif (rw == WRITE) {\r\nchar *vto, *vfrom;\r\nflush_dcache_page(page);\r\nvto = page_address(to->bv_page) + to->bv_offset;\r\nvfrom = kmap_atomic(page) + to->bv_offset;\r\nmemcpy(vto, vfrom, to->bv_len);\r\nkunmap_atomic(vfrom);\r\n}\r\n}\r\ntrace_block_bio_bounce(q, *bio_orig);\r\nbio->bi_flags |= (1 << BIO_BOUNCED);\r\nif (pool == page_pool) {\r\nbio->bi_end_io = bounce_end_io_write;\r\nif (rw == READ)\r\nbio->bi_end_io = bounce_end_io_read;\r\n} else {\r\nbio->bi_end_io = bounce_end_io_write_isa;\r\nif (rw == READ)\r\nbio->bi_end_io = bounce_end_io_read_isa;\r\n}\r\nbio->bi_private = *bio_orig;\r\n*bio_orig = bio;\r\n}\r\nvoid blk_queue_bounce(struct request_queue *q, struct bio **bio_orig)\r\n{\r\nint must_bounce;\r\nmempool_t *pool;\r\nif (!bio_has_data(*bio_orig))\r\nreturn;\r\nmust_bounce = must_snapshot_stable_pages(q, *bio_orig);\r\nif (!(q->bounce_gfp & GFP_DMA)) {\r\nif (queue_bounce_pfn(q) >= blk_max_pfn && !must_bounce)\r\nreturn;\r\npool = page_pool;\r\n} else {\r\nBUG_ON(!isa_page_pool);\r\npool = isa_page_pool;\r\n}\r\n__blk_queue_bounce(q, bio_orig, pool, must_bounce);\r\n}
