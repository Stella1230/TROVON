int can_do_mlock(void)\r\n{\r\nif (capable(CAP_IPC_LOCK))\r\nreturn 1;\r\nif (rlimit(RLIMIT_MEMLOCK) != 0)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nvoid clear_page_mlock(struct page *page)\r\n{\r\nif (!TestClearPageMlocked(page))\r\nreturn;\r\nmod_zone_page_state(page_zone(page), NR_MLOCK,\r\n-hpage_nr_pages(page));\r\ncount_vm_event(UNEVICTABLE_PGCLEARED);\r\nif (!isolate_lru_page(page)) {\r\nputback_lru_page(page);\r\n} else {\r\nif (PageUnevictable(page))\r\ncount_vm_event(UNEVICTABLE_PGSTRANDED);\r\n}\r\n}\r\nvoid mlock_vma_page(struct page *page)\r\n{\r\nBUG_ON(!PageLocked(page));\r\nif (!TestSetPageMlocked(page)) {\r\nmod_zone_page_state(page_zone(page), NR_MLOCK,\r\nhpage_nr_pages(page));\r\ncount_vm_event(UNEVICTABLE_PGMLOCKED);\r\nif (!isolate_lru_page(page))\r\nputback_lru_page(page);\r\n}\r\n}\r\nunsigned int munlock_vma_page(struct page *page)\r\n{\r\nunsigned int page_mask = 0;\r\nBUG_ON(!PageLocked(page));\r\nif (TestClearPageMlocked(page)) {\r\nunsigned int nr_pages = hpage_nr_pages(page);\r\nmod_zone_page_state(page_zone(page), NR_MLOCK, -nr_pages);\r\npage_mask = nr_pages - 1;\r\nif (!isolate_lru_page(page)) {\r\nint ret = SWAP_AGAIN;\r\nif (page_mapcount(page) > 1)\r\nret = try_to_munlock(page);\r\nif (ret != SWAP_MLOCK)\r\ncount_vm_event(UNEVICTABLE_PGMUNLOCKED);\r\nputback_lru_page(page);\r\n} else {\r\nif (PageUnevictable(page))\r\ncount_vm_event(UNEVICTABLE_PGSTRANDED);\r\nelse\r\ncount_vm_event(UNEVICTABLE_PGMUNLOCKED);\r\n}\r\n}\r\nreturn page_mask;\r\n}\r\nlong __mlock_vma_pages_range(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end, int *nonblocking)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nunsigned long nr_pages = (end - start) / PAGE_SIZE;\r\nint gup_flags;\r\nVM_BUG_ON(start & ~PAGE_MASK);\r\nVM_BUG_ON(end & ~PAGE_MASK);\r\nVM_BUG_ON(start < vma->vm_start);\r\nVM_BUG_ON(end > vma->vm_end);\r\nVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\r\ngup_flags = FOLL_TOUCH | FOLL_MLOCK;\r\nif ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)\r\ngup_flags |= FOLL_WRITE;\r\nif (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC))\r\ngup_flags |= FOLL_FORCE;\r\nreturn __get_user_pages(current, mm, start, nr_pages, gup_flags,\r\nNULL, NULL, nonblocking);\r\n}\r\nstatic int __mlock_posix_error_return(long retval)\r\n{\r\nif (retval == -EFAULT)\r\nretval = -ENOMEM;\r\nelse if (retval == -ENOMEM)\r\nretval = -EAGAIN;\r\nreturn retval;\r\n}\r\nvoid munlock_vma_pages_range(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end)\r\n{\r\nvma->vm_flags &= ~VM_LOCKED;\r\nwhile (start < end) {\r\nstruct page *page;\r\nunsigned int page_mask, page_increm;\r\npage = follow_page_mask(vma, start, FOLL_GET | FOLL_DUMP,\r\n&page_mask);\r\nif (page && !IS_ERR(page)) {\r\nlock_page(page);\r\nlru_add_drain();\r\npage_mask = munlock_vma_page(page);\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\npage_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);\r\nstart += page_increm * PAGE_SIZE;\r\ncond_resched();\r\n}\r\n}\r\nstatic int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,\r\nunsigned long start, unsigned long end, vm_flags_t newflags)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npgoff_t pgoff;\r\nint nr_pages;\r\nint ret = 0;\r\nint lock = !!(newflags & VM_LOCKED);\r\nif (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||\r\nis_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm))\r\ngoto out;\r\npgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\r\n*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,\r\nvma->vm_file, pgoff, vma_policy(vma));\r\nif (*prev) {\r\nvma = *prev;\r\ngoto success;\r\n}\r\nif (start != vma->vm_start) {\r\nret = split_vma(mm, vma, start, 1);\r\nif (ret)\r\ngoto out;\r\n}\r\nif (end != vma->vm_end) {\r\nret = split_vma(mm, vma, end, 0);\r\nif (ret)\r\ngoto out;\r\n}\r\nsuccess:\r\nnr_pages = (end - start) >> PAGE_SHIFT;\r\nif (!lock)\r\nnr_pages = -nr_pages;\r\nmm->locked_vm += nr_pages;\r\nif (lock)\r\nvma->vm_flags = newflags;\r\nelse\r\nmunlock_vma_pages_range(vma, start, end);\r\nout:\r\n*prev = vma;\r\nreturn ret;\r\n}\r\nstatic int do_mlock(unsigned long start, size_t len, int on)\r\n{\r\nunsigned long nstart, end, tmp;\r\nstruct vm_area_struct * vma, * prev;\r\nint error;\r\nVM_BUG_ON(start & ~PAGE_MASK);\r\nVM_BUG_ON(len != PAGE_ALIGN(len));\r\nend = start + len;\r\nif (end < start)\r\nreturn -EINVAL;\r\nif (end == start)\r\nreturn 0;\r\nvma = find_vma(current->mm, start);\r\nif (!vma || vma->vm_start > start)\r\nreturn -ENOMEM;\r\nprev = vma->vm_prev;\r\nif (start > vma->vm_start)\r\nprev = vma;\r\nfor (nstart = start ; ; ) {\r\nvm_flags_t newflags;\r\nnewflags = vma->vm_flags & ~VM_LOCKED;\r\nif (on)\r\nnewflags |= VM_LOCKED;\r\ntmp = vma->vm_end;\r\nif (tmp > end)\r\ntmp = end;\r\nerror = mlock_fixup(vma, &prev, nstart, tmp, newflags);\r\nif (error)\r\nbreak;\r\nnstart = tmp;\r\nif (nstart < prev->vm_end)\r\nnstart = prev->vm_end;\r\nif (nstart >= end)\r\nbreak;\r\nvma = prev->vm_next;\r\nif (!vma || vma->vm_start != nstart) {\r\nerror = -ENOMEM;\r\nbreak;\r\n}\r\n}\r\nreturn error;\r\n}\r\nint __mm_populate(unsigned long start, unsigned long len, int ignore_errors)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long end, nstart, nend;\r\nstruct vm_area_struct *vma = NULL;\r\nint locked = 0;\r\nlong ret = 0;\r\nVM_BUG_ON(start & ~PAGE_MASK);\r\nVM_BUG_ON(len != PAGE_ALIGN(len));\r\nend = start + len;\r\nfor (nstart = start; nstart < end; nstart = nend) {\r\nif (!locked) {\r\nlocked = 1;\r\ndown_read(&mm->mmap_sem);\r\nvma = find_vma(mm, nstart);\r\n} else if (nstart >= vma->vm_end)\r\nvma = vma->vm_next;\r\nif (!vma || vma->vm_start >= end)\r\nbreak;\r\nnend = min(end, vma->vm_end);\r\nif (vma->vm_flags & (VM_IO | VM_PFNMAP))\r\ncontinue;\r\nif (nstart < vma->vm_start)\r\nnstart = vma->vm_start;\r\nret = __mlock_vma_pages_range(vma, nstart, nend, &locked);\r\nif (ret < 0) {\r\nif (ignore_errors) {\r\nret = 0;\r\ncontinue;\r\n}\r\nret = __mlock_posix_error_return(ret);\r\nbreak;\r\n}\r\nnend = nstart + ret * PAGE_SIZE;\r\nret = 0;\r\n}\r\nif (locked)\r\nup_read(&mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic int do_mlockall(int flags)\r\n{\r\nstruct vm_area_struct * vma, * prev = NULL;\r\nif (flags & MCL_FUTURE)\r\ncurrent->mm->def_flags |= VM_LOCKED;\r\nelse\r\ncurrent->mm->def_flags &= ~VM_LOCKED;\r\nif (flags == MCL_FUTURE)\r\ngoto out;\r\nfor (vma = current->mm->mmap; vma ; vma = prev->vm_next) {\r\nvm_flags_t newflags;\r\nnewflags = vma->vm_flags & ~VM_LOCKED;\r\nif (flags & MCL_CURRENT)\r\nnewflags |= VM_LOCKED;\r\nmlock_fixup(vma, &prev, vma->vm_start, vma->vm_end, newflags);\r\n}\r\nout:\r\nreturn 0;\r\n}\r\nint user_shm_lock(size_t size, struct user_struct *user)\r\n{\r\nunsigned long lock_limit, locked;\r\nint allowed = 0;\r\nlocked = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK);\r\nif (lock_limit == RLIM_INFINITY)\r\nallowed = 1;\r\nlock_limit >>= PAGE_SHIFT;\r\nspin_lock(&shmlock_user_lock);\r\nif (!allowed &&\r\nlocked + user->locked_shm > lock_limit && !capable(CAP_IPC_LOCK))\r\ngoto out;\r\nget_uid(user);\r\nuser->locked_shm += locked;\r\nallowed = 1;\r\nout:\r\nspin_unlock(&shmlock_user_lock);\r\nreturn allowed;\r\n}\r\nvoid user_shm_unlock(size_t size, struct user_struct *user)\r\n{\r\nspin_lock(&shmlock_user_lock);\r\nuser->locked_shm -= (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nspin_unlock(&shmlock_user_lock);\r\nfree_uid(user);\r\n}
