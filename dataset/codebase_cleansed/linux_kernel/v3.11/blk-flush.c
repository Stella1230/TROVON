static unsigned int blk_flush_policy(unsigned int fflags, struct request *rq)\r\n{\r\nunsigned int policy = 0;\r\nif (blk_rq_sectors(rq))\r\npolicy |= REQ_FSEQ_DATA;\r\nif (fflags & REQ_FLUSH) {\r\nif (rq->cmd_flags & REQ_FLUSH)\r\npolicy |= REQ_FSEQ_PREFLUSH;\r\nif (!(fflags & REQ_FUA) && (rq->cmd_flags & REQ_FUA))\r\npolicy |= REQ_FSEQ_POSTFLUSH;\r\n}\r\nreturn policy;\r\n}\r\nstatic unsigned int blk_flush_cur_seq(struct request *rq)\r\n{\r\nreturn 1 << ffz(rq->flush.seq);\r\n}\r\nstatic void blk_flush_restore_request(struct request *rq)\r\n{\r\nrq->bio = rq->biotail;\r\nrq->cmd_flags &= ~REQ_FLUSH_SEQ;\r\nrq->end_io = rq->flush.saved_end_io;\r\n}\r\nstatic bool blk_flush_complete_seq(struct request *rq, unsigned int seq,\r\nint error)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct list_head *pending = &q->flush_queue[q->flush_pending_idx];\r\nbool queued = false;\r\nBUG_ON(rq->flush.seq & seq);\r\nrq->flush.seq |= seq;\r\nif (likely(!error))\r\nseq = blk_flush_cur_seq(rq);\r\nelse\r\nseq = REQ_FSEQ_DONE;\r\nswitch (seq) {\r\ncase REQ_FSEQ_PREFLUSH:\r\ncase REQ_FSEQ_POSTFLUSH:\r\nif (list_empty(pending))\r\nq->flush_pending_since = jiffies;\r\nlist_move_tail(&rq->flush.list, pending);\r\nbreak;\r\ncase REQ_FSEQ_DATA:\r\nlist_move_tail(&rq->flush.list, &q->flush_data_in_flight);\r\nlist_add(&rq->queuelist, &q->queue_head);\r\nqueued = true;\r\nbreak;\r\ncase REQ_FSEQ_DONE:\r\nBUG_ON(!list_empty(&rq->queuelist));\r\nlist_del_init(&rq->flush.list);\r\nblk_flush_restore_request(rq);\r\n__blk_end_request_all(rq, error);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn blk_kick_flush(q) | queued;\r\n}\r\nstatic void flush_end_io(struct request *flush_rq, int error)\r\n{\r\nstruct request_queue *q = flush_rq->q;\r\nstruct list_head *running = &q->flush_queue[q->flush_running_idx];\r\nbool queued = false;\r\nstruct request *rq, *n;\r\nBUG_ON(q->flush_pending_idx == q->flush_running_idx);\r\nq->flush_running_idx ^= 1;\r\nelv_completed_request(q, flush_rq);\r\nlist_for_each_entry_safe(rq, n, running, flush.list) {\r\nunsigned int seq = blk_flush_cur_seq(rq);\r\nBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\r\nqueued |= blk_flush_complete_seq(rq, seq, error);\r\n}\r\nif (queued || q->flush_queue_delayed)\r\nblk_run_queue_async(q);\r\nq->flush_queue_delayed = 0;\r\n}\r\nstatic bool blk_kick_flush(struct request_queue *q)\r\n{\r\nstruct list_head *pending = &q->flush_queue[q->flush_pending_idx];\r\nstruct request *first_rq =\r\nlist_first_entry(pending, struct request, flush.list);\r\nif (q->flush_pending_idx != q->flush_running_idx || list_empty(pending))\r\nreturn false;\r\nif (!list_empty(&q->flush_data_in_flight) &&\r\ntime_before(jiffies,\r\nq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\r\nreturn false;\r\nblk_rq_init(q, &q->flush_rq);\r\nq->flush_rq.cmd_type = REQ_TYPE_FS;\r\nq->flush_rq.cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\r\nq->flush_rq.rq_disk = first_rq->rq_disk;\r\nq->flush_rq.end_io = flush_end_io;\r\nq->flush_pending_idx ^= 1;\r\nlist_add_tail(&q->flush_rq.queuelist, &q->queue_head);\r\nreturn true;\r\n}\r\nstatic void flush_data_end_io(struct request *rq, int error)\r\n{\r\nstruct request_queue *q = rq->q;\r\nif (blk_flush_complete_seq(rq, REQ_FSEQ_DATA, error))\r\nblk_run_queue_async(q);\r\n}\r\nvoid blk_insert_flush(struct request *rq)\r\n{\r\nstruct request_queue *q = rq->q;\r\nunsigned int fflags = q->flush_flags;\r\nunsigned int policy = blk_flush_policy(fflags, rq);\r\nrq->cmd_flags &= ~REQ_FLUSH;\r\nif (!(fflags & REQ_FUA))\r\nrq->cmd_flags &= ~REQ_FUA;\r\nif (!policy) {\r\n__blk_end_bidi_request(rq, 0, 0, 0);\r\nreturn;\r\n}\r\nBUG_ON(rq->bio != rq->biotail);\r\nif ((policy & REQ_FSEQ_DATA) &&\r\n!(policy & (REQ_FSEQ_PREFLUSH | REQ_FSEQ_POSTFLUSH))) {\r\nlist_add_tail(&rq->queuelist, &q->queue_head);\r\nreturn;\r\n}\r\nmemset(&rq->flush, 0, sizeof(rq->flush));\r\nINIT_LIST_HEAD(&rq->flush.list);\r\nrq->cmd_flags |= REQ_FLUSH_SEQ;\r\nrq->flush.saved_end_io = rq->end_io;\r\nrq->end_io = flush_data_end_io;\r\nblk_flush_complete_seq(rq, REQ_FSEQ_ACTIONS & ~policy, 0);\r\n}\r\nvoid blk_abort_flushes(struct request_queue *q)\r\n{\r\nstruct request *rq, *n;\r\nint i;\r\nlist_for_each_entry_safe(rq, n, &q->flush_data_in_flight, flush.list) {\r\nlist_del_init(&rq->flush.list);\r\nblk_flush_restore_request(rq);\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(q->flush_queue); i++) {\r\nlist_for_each_entry_safe(rq, n, &q->flush_queue[i],\r\nflush.list) {\r\nlist_del_init(&rq->flush.list);\r\nblk_flush_restore_request(rq);\r\nlist_add_tail(&rq->queuelist, &q->queue_head);\r\n}\r\n}\r\n}\r\nstatic void bio_end_flush(struct bio *bio, int err)\r\n{\r\nif (err)\r\nclear_bit(BIO_UPTODATE, &bio->bi_flags);\r\nif (bio->bi_private)\r\ncomplete(bio->bi_private);\r\nbio_put(bio);\r\n}\r\nint blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,\r\nsector_t *error_sector)\r\n{\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nstruct request_queue *q;\r\nstruct bio *bio;\r\nint ret = 0;\r\nif (bdev->bd_disk == NULL)\r\nreturn -ENXIO;\r\nq = bdev_get_queue(bdev);\r\nif (!q)\r\nreturn -ENXIO;\r\nif (!q->make_request_fn)\r\nreturn -ENXIO;\r\nbio = bio_alloc(gfp_mask, 0);\r\nbio->bi_end_io = bio_end_flush;\r\nbio->bi_bdev = bdev;\r\nbio->bi_private = &wait;\r\nbio_get(bio);\r\nsubmit_bio(WRITE_FLUSH, bio);\r\nwait_for_completion_io(&wait);\r\nif (error_sector)\r\n*error_sector = bio->bi_sector;\r\nif (!bio_flagged(bio, BIO_UPTODATE))\r\nret = -EIO;\r\nbio_put(bio);\r\nreturn ret;\r\n}
