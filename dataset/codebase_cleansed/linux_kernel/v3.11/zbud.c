static inline void zbudpage_spin_lock(struct zbudpage *zbudpage)\r\n{\r\nstruct page *page = (struct page *)zbudpage;\r\nwhile (unlikely(test_and_set_bit_lock(PG_locked, &page->flags))) {\r\ndo {\r\ncpu_relax();\r\n} while (test_bit(PG_locked, &page->flags));\r\n}\r\n}\r\nstatic inline void zbudpage_spin_unlock(struct zbudpage *zbudpage)\r\n{\r\nstruct page *page = (struct page *)zbudpage;\r\nclear_bit(PG_locked, &page->flags);\r\n}\r\nstatic inline int zbudpage_spin_trylock(struct zbudpage *zbudpage)\r\n{\r\nreturn trylock_page((struct page *)zbudpage);\r\n}\r\nstatic inline int zbudpage_is_locked(struct zbudpage *zbudpage)\r\n{\r\nreturn PageLocked((struct page *)zbudpage);\r\n}\r\nstatic inline void *kmap_zbudpage_atomic(struct zbudpage *zbudpage)\r\n{\r\nreturn kmap_atomic((struct page *)zbudpage);\r\n}\r\nstatic inline int zbudpage_is_dying(struct zbudpage *zbudpage)\r\n{\r\nstruct page *page = (struct page *)zbudpage;\r\nreturn test_bit(PG_reclaim, &page->flags);\r\n}\r\nstatic inline void zbudpage_set_dying(struct zbudpage *zbudpage)\r\n{\r\nstruct page *page = (struct page *)zbudpage;\r\nset_bit(PG_reclaim, &page->flags);\r\n}\r\nstatic inline void zbudpage_clear_dying(struct zbudpage *zbudpage)\r\n{\r\nstruct page *page = (struct page *)zbudpage;\r\nclear_bit(PG_reclaim, &page->flags);\r\n}\r\nstatic inline int zbudpage_is_zombie(struct zbudpage *zbudpage)\r\n{\r\nstruct page *page = (struct page *)zbudpage;\r\nreturn test_bit(PG_dirty, &page->flags);\r\n}\r\nstatic inline void zbudpage_set_zombie(struct zbudpage *zbudpage)\r\n{\r\nstruct page *page = (struct page *)zbudpage;\r\nset_bit(PG_dirty, &page->flags);\r\n}\r\nstatic inline void zbudpage_clear_zombie(struct zbudpage *zbudpage)\r\n{\r\nstruct page *page = (struct page *)zbudpage;\r\nclear_bit(PG_dirty, &page->flags);\r\n}\r\nstatic inline void kunmap_zbudpage_atomic(void *zbpg)\r\n{\r\nkunmap_atomic(zbpg);\r\n}\r\nstatic inline struct zbudpage *zbudref_to_zbudpage(struct zbudref *zref)\r\n{\r\nunsigned long zbud = (unsigned long)zref;\r\nzbud &= ~1UL;\r\nreturn (struct zbudpage *)zbud;\r\n}\r\nstatic inline struct zbudref *zbudpage_to_zbudref(struct zbudpage *zbudpage,\r\nunsigned budnum)\r\n{\r\nunsigned long zbud = (unsigned long)zbudpage;\r\nBUG_ON(budnum > 1);\r\nzbud |= budnum;\r\nreturn (struct zbudref *)zbud;\r\n}\r\nstatic inline int zbudref_budnum(struct zbudref *zbudref)\r\n{\r\nunsigned long zbud = (unsigned long)zbudref;\r\nreturn zbud & 1UL;\r\n}\r\nstatic inline unsigned zbud_max_size(void)\r\n{\r\nreturn MAX_CHUNK << CHUNK_SHIFT;\r\n}\r\nstatic inline unsigned zbud_size_to_chunks(unsigned size)\r\n{\r\nBUG_ON(size == 0 || size > zbud_max_size());\r\nreturn (size + CHUNK_SIZE - 1) >> CHUNK_SHIFT;\r\n}\r\nstatic inline char *zbud_data(void *zbpg,\r\nunsigned budnum, unsigned size)\r\n{\r\nchar *p;\r\nBUG_ON(size == 0 || size > zbud_max_size());\r\np = (char *)zbpg;\r\nif (budnum == 1)\r\np += PAGE_SIZE - ((size + CHUNK_SIZE - 1) & CHUNK_MASK);\r\nreturn p;\r\n}\r\nstatic int zbud_debugfs_init(void)\r\n{\r\nstruct dentry *root = debugfs_create_dir("zbud", NULL);\r\nif (root == NULL)\r\nreturn -ENXIO;\r\nzdfs64("eph_zbytes", S_IRUGO, root, &zbud_eph_zbytes);\r\nzdfs64("eph_cumul_zbytes", S_IRUGO, root, &zbud_eph_cumul_zbytes);\r\nzdfs64("pers_zbytes", S_IRUGO, root, &zbud_pers_zbytes);\r\nzdfs64("pers_cumul_zbytes", S_IRUGO, root, &zbud_pers_cumul_zbytes);\r\nzdfs("eph_cumul_zpages", S_IRUGO, root, &zbud_eph_cumul_zpages);\r\nzdfs("eph_evicted_pageframes", S_IRUGO, root,\r\n&zbud_eph_evicted_pageframes);\r\nzdfs("eph_zpages", S_IRUGO, root, &zbud_eph_zpages);\r\nzdfs("eph_pageframes", S_IRUGO, root, &zbud_eph_pageframes);\r\nzdfs("eph_buddied_count", S_IRUGO, root, &zbud_eph_buddied_count);\r\nzdfs("eph_unbuddied_count", S_IRUGO, root, &zbud_eph_unbuddied_count);\r\nzdfs("pers_cumul_zpages", S_IRUGO, root, &zbud_pers_cumul_zpages);\r\nzdfs("pers_evicted_pageframes", S_IRUGO, root,\r\n&zbud_pers_evicted_pageframes);\r\nzdfs("pers_zpages", S_IRUGO, root, &zbud_pers_zpages);\r\nzdfs("pers_pageframes", S_IRUGO, root, &zbud_pers_pageframes);\r\nzdfs("pers_buddied_count", S_IRUGO, root, &zbud_pers_buddied_count);\r\nzdfs("pers_unbuddied_count", S_IRUGO, root, &zbud_pers_unbuddied_count);\r\nzdfs("pers_zombie_count", S_IRUGO, root, &zbud_pers_zombie_count);\r\nreturn 0;\r\n}\r\nstatic inline int zbud_debugfs_init(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline struct zbudpage *zbud_init_zbudpage(struct page *page, bool eph)\r\n{\r\nstruct zbudpage *zbudpage = (struct zbudpage *)page;\r\nBUG_ON(page == NULL);\r\nINIT_LIST_HEAD(&zbudpage->budlist);\r\nINIT_LIST_HEAD(&zbudpage->lru);\r\nzbudpage->zbud0_size = 0;\r\nzbudpage->zbud1_size = 0;\r\nzbudpage->unevictable = 0;\r\nif (eph)\r\nzbud_eph_pageframes++;\r\nelse\r\nzbud_pers_pageframes++;\r\nreturn zbudpage;\r\n}\r\nstatic inline struct page *zbud_unuse_zbudpage(struct zbudpage *zbudpage,\r\nbool eph)\r\n{\r\nstruct page *page = (struct page *)zbudpage;\r\nBUG_ON(!list_empty(&zbudpage->budlist));\r\nBUG_ON(!list_empty(&zbudpage->lru));\r\nBUG_ON(zbudpage->zbud0_size != 0);\r\nBUG_ON(zbudpage->zbud1_size != 0);\r\nBUG_ON(!PageLocked(page));\r\nBUG_ON(zbudpage->unevictable != 0);\r\nBUG_ON(zbudpage_is_dying(zbudpage));\r\nBUG_ON(zbudpage_is_zombie(zbudpage));\r\nif (eph)\r\nzbud_eph_pageframes--;\r\nelse\r\nzbud_pers_pageframes--;\r\nzbudpage_spin_unlock(zbudpage);\r\npage_mapcount_reset(page);\r\ninit_page_count(page);\r\npage->index = 0;\r\nreturn page;\r\n}\r\nstatic inline void zbud_unuse_zbud(struct zbudpage *zbudpage,\r\nint budnum, bool eph)\r\n{\r\nunsigned size;\r\nBUG_ON(!zbudpage_is_locked(zbudpage));\r\nif (budnum == 0) {\r\nsize = zbudpage->zbud0_size;\r\nzbudpage->zbud0_size = 0;\r\n} else {\r\nsize = zbudpage->zbud1_size;\r\nzbudpage->zbud1_size = 0;\r\n}\r\nif (eph) {\r\nzbud_eph_zbytes -= size;\r\nzbud_eph_zpages--;\r\n} else {\r\nzbud_pers_zbytes -= size;\r\nzbud_pers_zpages--;\r\n}\r\n}\r\nstatic void zbud_init_zbud(struct zbudpage *zbudpage, struct tmem_handle *th,\r\nbool eph, void *cdata,\r\nunsigned budnum, unsigned size)\r\n{\r\nchar *to;\r\nvoid *zbpg;\r\nstruct tmem_handle *to_th;\r\nunsigned nchunks = zbud_size_to_chunks(size);\r\nBUG_ON(!zbudpage_is_locked(zbudpage));\r\nzbpg = kmap_zbudpage_atomic(zbudpage);\r\nto = zbud_data(zbpg, budnum, size);\r\nto_th = (struct tmem_handle *)to;\r\nto_th->index = th->index;\r\nto_th->oid = th->oid;\r\nto_th->pool_id = th->pool_id;\r\nto_th->client_id = th->client_id;\r\nto += sizeof(struct tmem_handle);\r\nif (cdata != NULL)\r\nmemcpy(to, cdata, size - sizeof(struct tmem_handle));\r\nkunmap_zbudpage_atomic(zbpg);\r\nif (budnum == 0)\r\nzbudpage->zbud0_size = size;\r\nelse\r\nzbudpage->zbud1_size = size;\r\nif (eph) {\r\nzbud_eph_cumul_chunk_counts[nchunks]++;\r\nzbud_eph_zpages++;\r\nzbud_eph_cumul_zpages++;\r\nzbud_eph_zbytes += size;\r\nzbud_eph_cumul_zbytes += size;\r\n} else {\r\nzbud_pers_cumul_chunk_counts[nchunks]++;\r\nzbud_pers_zpages++;\r\nzbud_pers_cumul_zpages++;\r\nzbud_pers_zbytes += size;\r\nzbud_pers_cumul_zbytes += size;\r\n}\r\n}\r\nstatic void zbud_evict_tmem(struct zbudpage *zbudpage)\r\n{\r\nint i, j;\r\nuint32_t pool_id[2], client_id[2];\r\nuint32_t index[2];\r\nstruct tmem_oid oid[2];\r\nstruct tmem_pool *pool;\r\nvoid *zbpg;\r\nstruct tmem_handle *th;\r\nunsigned size;\r\nzbpg = kmap_zbudpage_atomic(zbudpage);\r\nfor (i = 0, j = 0; i < 2; i++) {\r\nsize = (i == 0) ? zbudpage->zbud0_size : zbudpage->zbud1_size;\r\nif (size) {\r\nth = (struct tmem_handle *)zbud_data(zbpg, i, size);\r\nclient_id[j] = th->client_id;\r\npool_id[j] = th->pool_id;\r\noid[j] = th->oid;\r\nindex[j] = th->index;\r\nj++;\r\nzbud_unuse_zbud(zbudpage, i, true);\r\n}\r\n}\r\nkunmap_zbudpage_atomic(zbpg);\r\nzbudpage_spin_unlock(zbudpage);\r\nfor (i = 0; i < j; i++) {\r\npool = zcache_get_pool_by_id(client_id[i], pool_id[i]);\r\nif (pool != NULL) {\r\ntmem_flush_page(pool, &oid[i], index[i]);\r\nzcache_put_pool(pool);\r\n}\r\n}\r\n}\r\nunsigned int zbud_max_buddy_size(void)\r\n{\r\nreturn zbud_max_size() - sizeof(struct tmem_handle);\r\n}\r\nstruct page *zbud_free_and_delist(struct zbudref *zref, bool eph,\r\nunsigned int *zsize, unsigned int *zpages)\r\n{\r\nunsigned long budnum = zbudref_budnum(zref);\r\nstruct zbudpage *zbudpage = zbudref_to_zbudpage(zref);\r\nstruct page *page = NULL;\r\nunsigned chunks, bud_size, other_bud_size;\r\nspinlock_t *lists_lock =\r\neph ? &zbud_eph_lists_lock : &zbud_pers_lists_lock;\r\nstruct zbud_unbuddied *unbud =\r\neph ? zbud_eph_unbuddied : zbud_pers_unbuddied;\r\nspin_lock(lists_lock);\r\nzbudpage_spin_lock(zbudpage);\r\nif (zbudpage_is_dying(zbudpage)) {\r\nzbudpage_spin_unlock(zbudpage);\r\nspin_unlock(lists_lock);\r\n*zpages = 0;\r\n*zsize = 0;\r\ngoto out;\r\n}\r\nif (budnum == 0) {\r\nbud_size = zbudpage->zbud0_size;\r\nother_bud_size = zbudpage->zbud1_size;\r\n} else {\r\nbud_size = zbudpage->zbud1_size;\r\nother_bud_size = zbudpage->zbud0_size;\r\n}\r\n*zsize = bud_size - sizeof(struct tmem_handle);\r\n*zpages = 1;\r\nzbud_unuse_zbud(zbudpage, budnum, eph);\r\nif (other_bud_size == 0) {\r\nchunks = zbud_size_to_chunks(bud_size) ;\r\nif (zbudpage_is_zombie(zbudpage)) {\r\nif (eph)\r\nzbud_pers_zombie_count =\r\natomic_dec_return(&zbud_eph_zombie_atomic);\r\nelse\r\nzbud_pers_zombie_count =\r\natomic_dec_return(&zbud_pers_zombie_atomic);\r\nzbudpage_clear_zombie(zbudpage);\r\n} else {\r\nBUG_ON(list_empty(&unbud[chunks].list));\r\nlist_del_init(&zbudpage->budlist);\r\nunbud[chunks].count--;\r\n}\r\nlist_del_init(&zbudpage->lru);\r\nspin_unlock(lists_lock);\r\nif (eph)\r\nzbud_eph_unbuddied_count--;\r\nelse\r\nzbud_pers_unbuddied_count--;\r\npage = zbud_unuse_zbudpage(zbudpage, eph);\r\n} else {\r\nchunks = zbud_size_to_chunks(other_bud_size) ;\r\nif (!zbudpage_is_zombie(zbudpage)) {\r\nlist_del_init(&zbudpage->budlist);\r\nlist_add_tail(&zbudpage->budlist, &unbud[chunks].list);\r\nunbud[chunks].count++;\r\n}\r\nif (eph) {\r\nzbud_eph_buddied_count--;\r\nzbud_eph_unbuddied_count++;\r\n} else {\r\nzbud_pers_unbuddied_count++;\r\nzbud_pers_buddied_count--;\r\n}\r\nzbudpage_spin_unlock(zbudpage);\r\nspin_unlock(lists_lock);\r\n}\r\nout:\r\nreturn page;\r\n}\r\nstruct zbudref *zbud_match_prep(struct tmem_handle *th, bool eph,\r\nvoid *cdata, unsigned size)\r\n{\r\nstruct zbudpage *zbudpage = NULL, *zbudpage2;\r\nunsigned long budnum = 0UL;\r\nunsigned nchunks;\r\nint i, found_good_buddy = 0;\r\nspinlock_t *lists_lock =\r\neph ? &zbud_eph_lists_lock : &zbud_pers_lists_lock;\r\nstruct zbud_unbuddied *unbud =\r\neph ? zbud_eph_unbuddied : zbud_pers_unbuddied;\r\nsize += sizeof(struct tmem_handle);\r\nnchunks = zbud_size_to_chunks(size);\r\nfor (i = MAX_CHUNK - nchunks + 1; i > 0; i--) {\r\nspin_lock(lists_lock);\r\nif (!list_empty(&unbud[i].list)) {\r\nlist_for_each_entry_safe(zbudpage, zbudpage2,\r\n&unbud[i].list, budlist) {\r\nif (zbudpage_spin_trylock(zbudpage)) {\r\nfound_good_buddy = i;\r\ngoto found_unbuddied;\r\n}\r\n}\r\n}\r\nspin_unlock(lists_lock);\r\n}\r\nzbudpage = NULL;\r\ngoto out;\r\nfound_unbuddied:\r\nBUG_ON(!zbudpage_is_locked(zbudpage));\r\nBUG_ON(!((zbudpage->zbud0_size == 0) ^ (zbudpage->zbud1_size == 0)));\r\nif (zbudpage->zbud0_size == 0)\r\nbudnum = 0UL;\r\nelse if (zbudpage->zbud1_size == 0)\r\nbudnum = 1UL;\r\nlist_del_init(&zbudpage->budlist);\r\nif (eph) {\r\nlist_add_tail(&zbudpage->budlist, &zbud_eph_buddied_list);\r\nunbud[found_good_buddy].count--;\r\nzbud_eph_unbuddied_count--;\r\nzbud_eph_buddied_count++;\r\nlist_del_init(&zbudpage->lru);\r\nlist_add_tail(&zbudpage->lru, &zbud_eph_lru_list);\r\n} else {\r\nlist_add_tail(&zbudpage->budlist, &zbud_pers_buddied_list);\r\nunbud[found_good_buddy].count--;\r\nzbud_pers_unbuddied_count--;\r\nzbud_pers_buddied_count++;\r\nlist_del_init(&zbudpage->lru);\r\nlist_add_tail(&zbudpage->lru, &zbud_pers_lru_list);\r\n}\r\nzbud_init_zbud(zbudpage, th, eph, cdata, budnum, size);\r\nzbudpage->unevictable++;\r\nBUG_ON(zbudpage->unevictable == 3);\r\nzbudpage_spin_unlock(zbudpage);\r\nspin_unlock(lists_lock);\r\nout:\r\nreturn zbudpage_to_zbudref(zbudpage, budnum);\r\n}\r\nstruct zbudref *zbud_create_prep(struct tmem_handle *th, bool eph,\r\nvoid *cdata, unsigned size,\r\nstruct page *newpage)\r\n{\r\nstruct zbudpage *zbudpage;\r\nunsigned long budnum = 0;\r\nunsigned nchunks;\r\nspinlock_t *lists_lock =\r\neph ? &zbud_eph_lists_lock : &zbud_pers_lists_lock;\r\nstruct zbud_unbuddied *unbud =\r\neph ? zbud_eph_unbuddied : zbud_pers_unbuddied;\r\n#if 0\r\nstatic unsigned long counter;\r\nbudnum = counter++ & 1;\r\n#endif\r\nif (size > zbud_max_buddy_size())\r\nreturn NULL;\r\nif (newpage == NULL)\r\nreturn NULL;\r\nsize += sizeof(struct tmem_handle);\r\nnchunks = zbud_size_to_chunks(size) ;\r\nspin_lock(lists_lock);\r\nzbudpage = zbud_init_zbudpage(newpage, eph);\r\nzbudpage_spin_lock(zbudpage);\r\nlist_add_tail(&zbudpage->budlist, &unbud[nchunks].list);\r\nif (eph) {\r\nlist_add_tail(&zbudpage->lru, &zbud_eph_lru_list);\r\nzbud_eph_unbuddied_count++;\r\n} else {\r\nlist_add_tail(&zbudpage->lru, &zbud_pers_lru_list);\r\nzbud_pers_unbuddied_count++;\r\n}\r\nunbud[nchunks].count++;\r\nzbud_init_zbud(zbudpage, th, eph, cdata, budnum, size);\r\nzbudpage->unevictable++;\r\nBUG_ON(zbudpage->unevictable == 3);\r\nzbudpage_spin_unlock(zbudpage);\r\nspin_unlock(lists_lock);\r\nreturn zbudpage_to_zbudref(zbudpage, budnum);\r\n}\r\nvoid zbud_create_finish(struct zbudref *zref, bool eph)\r\n{\r\nstruct zbudpage *zbudpage = zbudref_to_zbudpage(zref);\r\nspinlock_t *lists_lock =\r\neph ? &zbud_eph_lists_lock : &zbud_pers_lists_lock;\r\nspin_lock(lists_lock);\r\nzbudpage_spin_lock(zbudpage);\r\nBUG_ON(zbudpage_is_dying(zbudpage));\r\nzbudpage->unevictable--;\r\nBUG_ON((int)zbudpage->unevictable < 0);\r\nzbudpage_spin_unlock(zbudpage);\r\nspin_unlock(lists_lock);\r\n}\r\nint zbud_decompress(struct page *data_page, struct zbudref *zref, bool eph,\r\nvoid (*decompress)(char *, unsigned int, char *))\r\n{\r\nstruct zbudpage *zbudpage = zbudref_to_zbudpage(zref);\r\nunsigned long budnum = zbudref_budnum(zref);\r\nvoid *zbpg;\r\nchar *to_va, *from_va;\r\nunsigned size;\r\nint ret = -1;\r\nspinlock_t *lists_lock =\r\neph ? &zbud_eph_lists_lock : &zbud_pers_lists_lock;\r\nspin_lock(lists_lock);\r\nzbudpage_spin_lock(zbudpage);\r\nif (zbudpage_is_dying(zbudpage)) {\r\ngoto out;\r\n}\r\nzbpg = kmap_zbudpage_atomic(zbudpage);\r\nto_va = kmap_atomic(data_page);\r\nif (budnum == 0)\r\nsize = zbudpage->zbud0_size;\r\nelse\r\nsize = zbudpage->zbud1_size;\r\nBUG_ON(size == 0 || size > zbud_max_size());\r\nfrom_va = zbud_data(zbpg, budnum, size);\r\nfrom_va += sizeof(struct tmem_handle);\r\nsize -= sizeof(struct tmem_handle);\r\ndecompress(from_va, size, to_va);\r\nkunmap_atomic(to_va);\r\nkunmap_zbudpage_atomic(zbpg);\r\nret = 0;\r\nout:\r\nzbudpage_spin_unlock(zbudpage);\r\nspin_unlock(lists_lock);\r\nreturn ret;\r\n}\r\nint zbud_copy_from_zbud(char *to_va, struct zbudref *zref,\r\nsize_t *sizep, bool eph)\r\n{\r\nstruct zbudpage *zbudpage = zbudref_to_zbudpage(zref);\r\nunsigned long budnum = zbudref_budnum(zref);\r\nvoid *zbpg;\r\nchar *from_va;\r\nunsigned size;\r\nint ret = -1;\r\nspinlock_t *lists_lock =\r\neph ? &zbud_eph_lists_lock : &zbud_pers_lists_lock;\r\nspin_lock(lists_lock);\r\nzbudpage_spin_lock(zbudpage);\r\nif (zbudpage_is_dying(zbudpage)) {\r\ngoto out;\r\n}\r\nzbpg = kmap_zbudpage_atomic(zbudpage);\r\nif (budnum == 0)\r\nsize = zbudpage->zbud0_size;\r\nelse\r\nsize = zbudpage->zbud1_size;\r\nBUG_ON(size == 0 || size > zbud_max_size());\r\nfrom_va = zbud_data(zbpg, budnum, size);\r\nfrom_va += sizeof(struct tmem_handle);\r\nsize -= sizeof(struct tmem_handle);\r\n*sizep = size;\r\nmemcpy(to_va, from_va, size);\r\nkunmap_zbudpage_atomic(zbpg);\r\nret = 0;\r\nout:\r\nzbudpage_spin_unlock(zbudpage);\r\nspin_unlock(lists_lock);\r\nreturn ret;\r\n}\r\nint zbud_copy_to_zbud(struct zbudref *zref, char *from_va, bool eph)\r\n{\r\nstruct zbudpage *zbudpage = zbudref_to_zbudpage(zref);\r\nunsigned long budnum = zbudref_budnum(zref);\r\nvoid *zbpg;\r\nchar *to_va;\r\nunsigned size;\r\nint ret = -1;\r\nspinlock_t *lists_lock =\r\neph ? &zbud_eph_lists_lock : &zbud_pers_lists_lock;\r\nspin_lock(lists_lock);\r\nzbudpage_spin_lock(zbudpage);\r\nif (zbudpage_is_dying(zbudpage)) {\r\ngoto out;\r\n}\r\nzbpg = kmap_zbudpage_atomic(zbudpage);\r\nif (budnum == 0)\r\nsize = zbudpage->zbud0_size;\r\nelse\r\nsize = zbudpage->zbud1_size;\r\nBUG_ON(size == 0 || size > zbud_max_size());\r\nto_va = zbud_data(zbpg, budnum, size);\r\nto_va += sizeof(struct tmem_handle);\r\nsize -= sizeof(struct tmem_handle);\r\nmemcpy(to_va, from_va, size);\r\nkunmap_zbudpage_atomic(zbpg);\r\nret = 0;\r\nout:\r\nzbudpage_spin_unlock(zbudpage);\r\nspin_unlock(lists_lock);\r\nreturn ret;\r\n}\r\nstruct page *zbud_evict_pageframe_lru(unsigned int *zsize, unsigned int *zpages)\r\n{\r\nstruct zbudpage *zbudpage = NULL, *zbudpage2;\r\nstruct zbud_unbuddied *unbud = zbud_eph_unbuddied;\r\nstruct page *page = NULL;\r\nbool irqs_disabled = irqs_disabled();\r\nif (irqs_disabled)\r\nspin_lock(&zbud_eph_lists_lock);\r\nelse\r\nspin_lock_bh(&zbud_eph_lists_lock);\r\n*zsize = 0;\r\nif (list_empty(&zbud_eph_lru_list))\r\ngoto unlock_out;\r\nlist_for_each_entry_safe(zbudpage, zbudpage2, &zbud_eph_lru_list, lru) {\r\nif (unlikely(!zbudpage_spin_trylock(zbudpage)))\r\ncontinue;\r\nif (unlikely(zbudpage->unevictable != 0)) {\r\nzbudpage_spin_unlock(zbudpage);\r\ncontinue;\r\n}\r\ngoto evict_page;\r\n}\r\nunlock_out:\r\nif (irqs_disabled)\r\nspin_unlock(&zbud_eph_lists_lock);\r\nelse\r\nspin_unlock_bh(&zbud_eph_lists_lock);\r\ngoto out;\r\nevict_page:\r\nlist_del_init(&zbudpage->budlist);\r\nlist_del_init(&zbudpage->lru);\r\nzbudpage_set_dying(zbudpage);\r\nif (zbudpage->zbud0_size != 0 && zbudpage->zbud1_size != 0) {\r\n*zsize = zbudpage->zbud0_size + zbudpage->zbud1_size -\r\n(2 * sizeof(struct tmem_handle));\r\n*zpages = 2;\r\n} else if (zbudpage->zbud0_size != 0) {\r\nunbud[zbud_size_to_chunks(zbudpage->zbud0_size)].count--;\r\n*zsize = zbudpage->zbud0_size - sizeof(struct tmem_handle);\r\n*zpages = 1;\r\n} else if (zbudpage->zbud1_size != 0) {\r\nunbud[zbud_size_to_chunks(zbudpage->zbud1_size)].count--;\r\n*zsize = zbudpage->zbud1_size - sizeof(struct tmem_handle);\r\n*zpages = 1;\r\n} else {\r\nBUG();\r\n}\r\nspin_unlock(&zbud_eph_lists_lock);\r\nzbud_eph_evicted_pageframes++;\r\nif (*zpages == 1)\r\nzbud_eph_unbuddied_count--;\r\nelse\r\nzbud_eph_buddied_count--;\r\nzbud_evict_tmem(zbudpage);\r\nzbudpage_spin_lock(zbudpage);\r\nzbudpage_clear_dying(zbudpage);\r\npage = zbud_unuse_zbudpage(zbudpage, true);\r\nif (!irqs_disabled)\r\nlocal_bh_enable();\r\nout:\r\nreturn page;\r\n}\r\nunsigned int zbud_make_zombie_lru(struct tmem_handle *th, unsigned char **data,\r\nunsigned int *zsize, bool eph)\r\n{\r\nstruct zbudpage *zbudpage = NULL, *zbudpag2;\r\nstruct tmem_handle *thfrom;\r\nchar *from_va;\r\nvoid *zbpg;\r\nunsigned size;\r\nint ret = 0, i;\r\nspinlock_t *lists_lock =\r\neph ? &zbud_eph_lists_lock : &zbud_pers_lists_lock;\r\nstruct list_head *lru_list =\r\neph ? &zbud_eph_lru_list : &zbud_pers_lru_list;\r\nspin_lock_bh(lists_lock);\r\nif (list_empty(lru_list))\r\ngoto out;\r\nlist_for_each_entry_safe(zbudpage, zbudpag2, lru_list, lru) {\r\nif (unlikely(!zbudpage_spin_trylock(zbudpage)))\r\ncontinue;\r\nif (unlikely(zbudpage->unevictable != 0)) {\r\nzbudpage_spin_unlock(zbudpage);\r\ncontinue;\r\n}\r\ngoto zombify_page;\r\n}\r\ngoto out;\r\nzombify_page:\r\nlist_del_init(&zbudpage->budlist);\r\nzbudpage_set_zombie(zbudpage);\r\nlist_del_init(&zbudpage->lru);\r\nif (eph) {\r\nlist_add_tail(&zbudpage->lru, &zbud_eph_zombie_list);\r\nzbud_eph_zombie_count =\r\natomic_inc_return(&zbud_eph_zombie_atomic);\r\n} else {\r\nlist_add_tail(&zbudpage->lru, &zbud_pers_zombie_list);\r\nzbud_pers_zombie_count =\r\natomic_inc_return(&zbud_pers_zombie_atomic);\r\n}\r\nzbpg = kmap_zbudpage_atomic(zbudpage);\r\nfor (i = 0; i < 2; i++) {\r\nsize = (i == 0) ? zbudpage->zbud0_size : zbudpage->zbud1_size;\r\nif (size) {\r\nfrom_va = zbud_data(zbpg, i, size);\r\nthfrom = (struct tmem_handle *)from_va;\r\nfrom_va += sizeof(struct tmem_handle);\r\nsize -= sizeof(struct tmem_handle);\r\nif (th != NULL)\r\nth[ret] = *thfrom;\r\nif (data != NULL)\r\nmemcpy(data[ret], from_va, size);\r\nif (zsize != NULL)\r\n*zsize++ = size;\r\nret++;\r\n}\r\n}\r\nkunmap_zbudpage_atomic(zbpg);\r\nzbudpage_spin_unlock(zbudpage);\r\nout:\r\nspin_unlock_bh(lists_lock);\r\nreturn ret;\r\n}\r\nvoid zbud_init(void)\r\n{\r\nint i;\r\nzbud_debugfs_init();\r\nBUG_ON((sizeof(struct tmem_handle) * 2 > CHUNK_SIZE));\r\nBUG_ON(sizeof(struct zbudpage) > sizeof(struct page));\r\nfor (i = 0; i < NCHUNKS; i++) {\r\nINIT_LIST_HEAD(&zbud_eph_unbuddied[i].list);\r\nINIT_LIST_HEAD(&zbud_pers_unbuddied[i].list);\r\n}\r\n}
