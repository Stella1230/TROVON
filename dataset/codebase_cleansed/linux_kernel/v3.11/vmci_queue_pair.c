static void qp_free_queue(void *q, u64 size)\r\n{\r\nstruct vmci_queue *queue = q;\r\nif (queue) {\r\nu64 i = DIV_ROUND_UP(size, PAGE_SIZE);\r\nif (queue->kernel_if->mapped) {\r\nvunmap(queue->kernel_if->va);\r\nqueue->kernel_if->va = NULL;\r\n}\r\nwhile (i)\r\n__free_page(queue->kernel_if->page[--i]);\r\nvfree(queue->q_header);\r\n}\r\n}\r\nstatic void *qp_alloc_queue(u64 size, u32 flags)\r\n{\r\nu64 i;\r\nstruct vmci_queue *queue;\r\nstruct vmci_queue_header *q_header;\r\nconst u64 num_data_pages = DIV_ROUND_UP(size, PAGE_SIZE);\r\nconst uint queue_size =\r\nPAGE_SIZE +\r\nsizeof(*queue) + sizeof(*(queue->kernel_if)) +\r\nnum_data_pages * sizeof(*(queue->kernel_if->page));\r\nq_header = vmalloc(queue_size);\r\nif (!q_header)\r\nreturn NULL;\r\nqueue = (void *)q_header + PAGE_SIZE;\r\nqueue->q_header = q_header;\r\nqueue->saved_header = NULL;\r\nqueue->kernel_if = (struct vmci_queue_kern_if *)(queue + 1);\r\nqueue->kernel_if->header_page = NULL;\r\nqueue->kernel_if->page = (struct page **)(queue->kernel_if + 1);\r\nqueue->kernel_if->host = false;\r\nqueue->kernel_if->va = NULL;\r\nqueue->kernel_if->mapped = false;\r\nfor (i = 0; i < num_data_pages; i++) {\r\nqueue->kernel_if->page[i] = alloc_pages(GFP_KERNEL, 0);\r\nif (!queue->kernel_if->page[i])\r\ngoto fail;\r\n}\r\nif (vmci_qp_pinned(flags)) {\r\nqueue->kernel_if->va =\r\nvmap(queue->kernel_if->page, num_data_pages, VM_MAP,\r\nPAGE_KERNEL);\r\nif (!queue->kernel_if->va)\r\ngoto fail;\r\nqueue->kernel_if->mapped = true;\r\n}\r\nreturn (void *)queue;\r\nfail:\r\nqp_free_queue(queue, i * PAGE_SIZE);\r\nreturn NULL;\r\n}\r\nstatic int __qp_memcpy_to_queue(struct vmci_queue *queue,\r\nu64 queue_offset,\r\nconst void *src,\r\nsize_t size,\r\nbool is_iovec)\r\n{\r\nstruct vmci_queue_kern_if *kernel_if = queue->kernel_if;\r\nsize_t bytes_copied = 0;\r\nwhile (bytes_copied < size) {\r\nu64 page_index = (queue_offset + bytes_copied) / PAGE_SIZE;\r\nsize_t page_offset =\r\n(queue_offset + bytes_copied) & (PAGE_SIZE - 1);\r\nvoid *va;\r\nsize_t to_copy;\r\nif (!kernel_if->mapped)\r\nva = kmap(kernel_if->page[page_index]);\r\nelse\r\nva = (void *)((u8 *)kernel_if->va +\r\n(page_index * PAGE_SIZE));\r\nif (size - bytes_copied > PAGE_SIZE - page_offset)\r\nto_copy = PAGE_SIZE - page_offset;\r\nelse\r\nto_copy = size - bytes_copied;\r\nif (is_iovec) {\r\nstruct iovec *iov = (struct iovec *)src;\r\nint err;\r\nerr = memcpy_fromiovec((u8 *)va + page_offset,\r\niov, to_copy);\r\nif (err != 0) {\r\nkunmap(kernel_if->page[page_index]);\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\n}\r\n} else {\r\nmemcpy((u8 *)va + page_offset,\r\n(u8 *)src + bytes_copied, to_copy);\r\n}\r\nbytes_copied += to_copy;\r\nif (!kernel_if->mapped)\r\nkunmap(kernel_if->page[page_index]);\r\n}\r\nreturn VMCI_SUCCESS;\r\n}\r\nstatic int __qp_memcpy_from_queue(void *dest,\r\nconst struct vmci_queue *queue,\r\nu64 queue_offset,\r\nsize_t size,\r\nbool is_iovec)\r\n{\r\nstruct vmci_queue_kern_if *kernel_if = queue->kernel_if;\r\nsize_t bytes_copied = 0;\r\nwhile (bytes_copied < size) {\r\nu64 page_index = (queue_offset + bytes_copied) / PAGE_SIZE;\r\nsize_t page_offset =\r\n(queue_offset + bytes_copied) & (PAGE_SIZE - 1);\r\nvoid *va;\r\nsize_t to_copy;\r\nif (!kernel_if->mapped)\r\nva = kmap(kernel_if->page[page_index]);\r\nelse\r\nva = (void *)((u8 *)kernel_if->va +\r\n(page_index * PAGE_SIZE));\r\nif (size - bytes_copied > PAGE_SIZE - page_offset)\r\nto_copy = PAGE_SIZE - page_offset;\r\nelse\r\nto_copy = size - bytes_copied;\r\nif (is_iovec) {\r\nstruct iovec *iov = (struct iovec *)dest;\r\nint err;\r\nerr = memcpy_toiovec(iov, (u8 *)va + page_offset,\r\nto_copy);\r\nif (err != 0) {\r\nkunmap(kernel_if->page[page_index]);\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\n}\r\n} else {\r\nmemcpy((u8 *)dest + bytes_copied,\r\n(u8 *)va + page_offset, to_copy);\r\n}\r\nbytes_copied += to_copy;\r\nif (!kernel_if->mapped)\r\nkunmap(kernel_if->page[page_index]);\r\n}\r\nreturn VMCI_SUCCESS;\r\n}\r\nstatic int qp_alloc_ppn_set(void *prod_q,\r\nu64 num_produce_pages,\r\nvoid *cons_q,\r\nu64 num_consume_pages, struct ppn_set *ppn_set)\r\n{\r\nu32 *produce_ppns;\r\nu32 *consume_ppns;\r\nstruct vmci_queue *produce_q = prod_q;\r\nstruct vmci_queue *consume_q = cons_q;\r\nu64 i;\r\nif (!produce_q || !num_produce_pages || !consume_q ||\r\n!num_consume_pages || !ppn_set)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nif (ppn_set->initialized)\r\nreturn VMCI_ERROR_ALREADY_EXISTS;\r\nproduce_ppns =\r\nkmalloc(num_produce_pages * sizeof(*produce_ppns), GFP_KERNEL);\r\nif (!produce_ppns)\r\nreturn VMCI_ERROR_NO_MEM;\r\nconsume_ppns =\r\nkmalloc(num_consume_pages * sizeof(*consume_ppns), GFP_KERNEL);\r\nif (!consume_ppns) {\r\nkfree(produce_ppns);\r\nreturn VMCI_ERROR_NO_MEM;\r\n}\r\nproduce_ppns[0] = page_to_pfn(vmalloc_to_page(produce_q->q_header));\r\nfor (i = 1; i < num_produce_pages; i++) {\r\nunsigned long pfn;\r\nproduce_ppns[i] =\r\npage_to_pfn(produce_q->kernel_if->page[i - 1]);\r\npfn = produce_ppns[i];\r\nif (sizeof(pfn) > sizeof(*produce_ppns)\r\n&& pfn != produce_ppns[i])\r\ngoto ppn_error;\r\n}\r\nconsume_ppns[0] = page_to_pfn(vmalloc_to_page(consume_q->q_header));\r\nfor (i = 1; i < num_consume_pages; i++) {\r\nunsigned long pfn;\r\nconsume_ppns[i] =\r\npage_to_pfn(consume_q->kernel_if->page[i - 1]);\r\npfn = consume_ppns[i];\r\nif (sizeof(pfn) > sizeof(*consume_ppns)\r\n&& pfn != consume_ppns[i])\r\ngoto ppn_error;\r\n}\r\nppn_set->num_produce_pages = num_produce_pages;\r\nppn_set->num_consume_pages = num_consume_pages;\r\nppn_set->produce_ppns = produce_ppns;\r\nppn_set->consume_ppns = consume_ppns;\r\nppn_set->initialized = true;\r\nreturn VMCI_SUCCESS;\r\nppn_error:\r\nkfree(produce_ppns);\r\nkfree(consume_ppns);\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\n}\r\nstatic void qp_free_ppn_set(struct ppn_set *ppn_set)\r\n{\r\nif (ppn_set->initialized) {\r\nkfree(ppn_set->produce_ppns);\r\nkfree(ppn_set->consume_ppns);\r\n}\r\nmemset(ppn_set, 0, sizeof(*ppn_set));\r\n}\r\nstatic int qp_populate_ppn_set(u8 *call_buf, const struct ppn_set *ppn_set)\r\n{\r\nmemcpy(call_buf, ppn_set->produce_ppns,\r\nppn_set->num_produce_pages * sizeof(*ppn_set->produce_ppns));\r\nmemcpy(call_buf +\r\nppn_set->num_produce_pages * sizeof(*ppn_set->produce_ppns),\r\nppn_set->consume_ppns,\r\nppn_set->num_consume_pages * sizeof(*ppn_set->consume_ppns));\r\nreturn VMCI_SUCCESS;\r\n}\r\nstatic int qp_memcpy_to_queue(struct vmci_queue *queue,\r\nu64 queue_offset,\r\nconst void *src, size_t src_offset, size_t size)\r\n{\r\nreturn __qp_memcpy_to_queue(queue, queue_offset,\r\n(u8 *)src + src_offset, size, false);\r\n}\r\nstatic int qp_memcpy_from_queue(void *dest,\r\nsize_t dest_offset,\r\nconst struct vmci_queue *queue,\r\nu64 queue_offset, size_t size)\r\n{\r\nreturn __qp_memcpy_from_queue((u8 *)dest + dest_offset,\r\nqueue, queue_offset, size, false);\r\n}\r\nstatic int qp_memcpy_to_queue_iov(struct vmci_queue *queue,\r\nu64 queue_offset,\r\nconst void *src,\r\nsize_t src_offset, size_t size)\r\n{\r\nreturn __qp_memcpy_to_queue(queue, queue_offset, src, size, true);\r\n}\r\nstatic int qp_memcpy_from_queue_iov(void *dest,\r\nsize_t dest_offset,\r\nconst struct vmci_queue *queue,\r\nu64 queue_offset, size_t size)\r\n{\r\nreturn __qp_memcpy_from_queue(dest, queue, queue_offset, size, true);\r\n}\r\nstatic struct vmci_queue *qp_host_alloc_queue(u64 size)\r\n{\r\nstruct vmci_queue *queue;\r\nconst size_t num_pages = DIV_ROUND_UP(size, PAGE_SIZE) + 1;\r\nconst size_t queue_size = sizeof(*queue) + sizeof(*(queue->kernel_if));\r\nconst size_t queue_page_size =\r\nnum_pages * sizeof(*queue->kernel_if->page);\r\nqueue = kzalloc(queue_size + queue_page_size, GFP_KERNEL);\r\nif (queue) {\r\nqueue->q_header = NULL;\r\nqueue->saved_header = NULL;\r\nqueue->kernel_if =\r\n(struct vmci_queue_kern_if *)((u8 *)queue +\r\nsizeof(*queue));\r\nqueue->kernel_if->host = true;\r\nqueue->kernel_if->mutex = NULL;\r\nqueue->kernel_if->num_pages = num_pages;\r\nqueue->kernel_if->header_page =\r\n(struct page **)((u8 *)queue + queue_size);\r\nqueue->kernel_if->page = &queue->kernel_if->header_page[1];\r\nqueue->kernel_if->va = NULL;\r\nqueue->kernel_if->mapped = false;\r\n}\r\nreturn queue;\r\n}\r\nstatic void qp_host_free_queue(struct vmci_queue *queue, u64 queue_size)\r\n{\r\nkfree(queue);\r\n}\r\nstatic void qp_init_queue_mutex(struct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q)\r\n{\r\nif (produce_q->kernel_if->host) {\r\nproduce_q->kernel_if->mutex = &produce_q->kernel_if->__mutex;\r\nconsume_q->kernel_if->mutex = &produce_q->kernel_if->__mutex;\r\nmutex_init(produce_q->kernel_if->mutex);\r\n}\r\n}\r\nstatic void qp_cleanup_queue_mutex(struct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q)\r\n{\r\nif (produce_q->kernel_if->host) {\r\nproduce_q->kernel_if->mutex = NULL;\r\nconsume_q->kernel_if->mutex = NULL;\r\n}\r\n}\r\nstatic void qp_acquire_queue_mutex(struct vmci_queue *queue)\r\n{\r\nif (queue->kernel_if->host)\r\nmutex_lock(queue->kernel_if->mutex);\r\n}\r\nstatic void qp_release_queue_mutex(struct vmci_queue *queue)\r\n{\r\nif (queue->kernel_if->host)\r\nmutex_unlock(queue->kernel_if->mutex);\r\n}\r\nstatic void qp_release_pages(struct page **pages,\r\nu64 num_pages, bool dirty)\r\n{\r\nint i;\r\nfor (i = 0; i < num_pages; i++) {\r\nif (dirty)\r\nset_page_dirty(pages[i]);\r\npage_cache_release(pages[i]);\r\npages[i] = NULL;\r\n}\r\n}\r\nstatic int qp_host_get_user_memory(u64 produce_uva,\r\nu64 consume_uva,\r\nstruct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q)\r\n{\r\nint retval;\r\nint err = VMCI_SUCCESS;\r\ndown_write(&current->mm->mmap_sem);\r\nretval = get_user_pages(current,\r\ncurrent->mm,\r\n(uintptr_t) produce_uva,\r\nproduce_q->kernel_if->num_pages,\r\n1, 0, produce_q->kernel_if->header_page, NULL);\r\nif (retval < produce_q->kernel_if->num_pages) {\r\npr_warn("get_user_pages(produce) failed (retval=%d)", retval);\r\nqp_release_pages(produce_q->kernel_if->header_page, retval,\r\nfalse);\r\nerr = VMCI_ERROR_NO_MEM;\r\ngoto out;\r\n}\r\nretval = get_user_pages(current,\r\ncurrent->mm,\r\n(uintptr_t) consume_uva,\r\nconsume_q->kernel_if->num_pages,\r\n1, 0, consume_q->kernel_if->header_page, NULL);\r\nif (retval < consume_q->kernel_if->num_pages) {\r\npr_warn("get_user_pages(consume) failed (retval=%d)", retval);\r\nqp_release_pages(consume_q->kernel_if->header_page, retval,\r\nfalse);\r\nqp_release_pages(produce_q->kernel_if->header_page,\r\nproduce_q->kernel_if->num_pages, false);\r\nerr = VMCI_ERROR_NO_MEM;\r\n}\r\nout:\r\nup_write(&current->mm->mmap_sem);\r\nreturn err;\r\n}\r\nstatic int qp_host_register_user_memory(struct vmci_qp_page_store *page_store,\r\nstruct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q)\r\n{\r\nu64 produce_uva;\r\nu64 consume_uva;\r\nproduce_uva = page_store->pages;\r\nconsume_uva = page_store->pages +\r\nproduce_q->kernel_if->num_pages * PAGE_SIZE;\r\nreturn qp_host_get_user_memory(produce_uva, consume_uva, produce_q,\r\nconsume_q);\r\n}\r\nstatic void qp_host_unregister_user_memory(struct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q)\r\n{\r\nqp_release_pages(produce_q->kernel_if->header_page,\r\nproduce_q->kernel_if->num_pages, true);\r\nmemset(produce_q->kernel_if->header_page, 0,\r\nsizeof(*produce_q->kernel_if->header_page) *\r\nproduce_q->kernel_if->num_pages);\r\nqp_release_pages(consume_q->kernel_if->header_page,\r\nconsume_q->kernel_if->num_pages, true);\r\nmemset(consume_q->kernel_if->header_page, 0,\r\nsizeof(*consume_q->kernel_if->header_page) *\r\nconsume_q->kernel_if->num_pages);\r\n}\r\nstatic int qp_host_map_queues(struct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q)\r\n{\r\nint result;\r\nif (!produce_q->q_header || !consume_q->q_header) {\r\nstruct page *headers[2];\r\nif (produce_q->q_header != consume_q->q_header)\r\nreturn VMCI_ERROR_QUEUEPAIR_MISMATCH;\r\nif (produce_q->kernel_if->header_page == NULL ||\r\n*produce_q->kernel_if->header_page == NULL)\r\nreturn VMCI_ERROR_UNAVAILABLE;\r\nheaders[0] = *produce_q->kernel_if->header_page;\r\nheaders[1] = *consume_q->kernel_if->header_page;\r\nproduce_q->q_header = vmap(headers, 2, VM_MAP, PAGE_KERNEL);\r\nif (produce_q->q_header != NULL) {\r\nconsume_q->q_header =\r\n(struct vmci_queue_header *)((u8 *)\r\nproduce_q->q_header +\r\nPAGE_SIZE);\r\nresult = VMCI_SUCCESS;\r\n} else {\r\npr_warn("vmap failed\n");\r\nresult = VMCI_ERROR_NO_MEM;\r\n}\r\n} else {\r\nresult = VMCI_SUCCESS;\r\n}\r\nreturn result;\r\n}\r\nstatic int qp_host_unmap_queues(u32 gid,\r\nstruct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q)\r\n{\r\nif (produce_q->q_header) {\r\nif (produce_q->q_header < consume_q->q_header)\r\nvunmap(produce_q->q_header);\r\nelse\r\nvunmap(consume_q->q_header);\r\nproduce_q->q_header = NULL;\r\nconsume_q->q_header = NULL;\r\n}\r\nreturn VMCI_SUCCESS;\r\n}\r\nstatic struct qp_entry *qp_list_find(struct qp_list *qp_list,\r\nstruct vmci_handle handle)\r\n{\r\nstruct qp_entry *entry;\r\nif (vmci_handle_is_invalid(handle))\r\nreturn NULL;\r\nlist_for_each_entry(entry, &qp_list->head, list_item) {\r\nif (vmci_handle_is_equal(entry->handle, handle))\r\nreturn entry;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct qp_guest_endpoint *\r\nqp_guest_handle_to_entry(struct vmci_handle handle)\r\n{\r\nstruct qp_guest_endpoint *entry;\r\nstruct qp_entry *qp = qp_list_find(&qp_guest_endpoints, handle);\r\nentry = qp ? container_of(\r\nqp, struct qp_guest_endpoint, qp) : NULL;\r\nreturn entry;\r\n}\r\nstatic struct qp_broker_entry *\r\nqp_broker_handle_to_entry(struct vmci_handle handle)\r\n{\r\nstruct qp_broker_entry *entry;\r\nstruct qp_entry *qp = qp_list_find(&qp_broker_list, handle);\r\nentry = qp ? container_of(\r\nqp, struct qp_broker_entry, qp) : NULL;\r\nreturn entry;\r\n}\r\nstatic int qp_notify_peer_local(bool attach, struct vmci_handle handle)\r\n{\r\nu32 context_id = vmci_get_context_id();\r\nstruct vmci_event_qp ev;\r\nev.msg.hdr.dst = vmci_make_handle(context_id, VMCI_EVENT_HANDLER);\r\nev.msg.hdr.src = vmci_make_handle(VMCI_HYPERVISOR_CONTEXT_ID,\r\nVMCI_CONTEXT_RESOURCE_ID);\r\nev.msg.hdr.payload_size = sizeof(ev) - sizeof(ev.msg.hdr);\r\nev.msg.event_data.event =\r\nattach ? VMCI_EVENT_QP_PEER_ATTACH : VMCI_EVENT_QP_PEER_DETACH;\r\nev.payload.peer_id = context_id;\r\nev.payload.handle = handle;\r\nreturn vmci_event_dispatch(&ev.msg.hdr);\r\n}\r\nstatic struct qp_guest_endpoint *\r\nqp_guest_endpoint_create(struct vmci_handle handle,\r\nu32 peer,\r\nu32 flags,\r\nu64 produce_size,\r\nu64 consume_size,\r\nvoid *produce_q,\r\nvoid *consume_q)\r\n{\r\nint result;\r\nstruct qp_guest_endpoint *entry;\r\nconst u64 num_ppns = DIV_ROUND_UP(produce_size, PAGE_SIZE) +\r\nDIV_ROUND_UP(consume_size, PAGE_SIZE) + 2;\r\nif (vmci_handle_is_invalid(handle)) {\r\nu32 context_id = vmci_get_context_id();\r\nhandle = vmci_make_handle(context_id, VMCI_INVALID_ID);\r\n}\r\nentry = kzalloc(sizeof(*entry), GFP_KERNEL);\r\nif (entry) {\r\nentry->qp.peer = peer;\r\nentry->qp.flags = flags;\r\nentry->qp.produce_size = produce_size;\r\nentry->qp.consume_size = consume_size;\r\nentry->qp.ref_count = 0;\r\nentry->num_ppns = num_ppns;\r\nentry->produce_q = produce_q;\r\nentry->consume_q = consume_q;\r\nINIT_LIST_HEAD(&entry->qp.list_item);\r\nresult = vmci_resource_add(&entry->resource,\r\nVMCI_RESOURCE_TYPE_QPAIR_GUEST,\r\nhandle);\r\nentry->qp.handle = vmci_resource_handle(&entry->resource);\r\nif ((result != VMCI_SUCCESS) ||\r\nqp_list_find(&qp_guest_endpoints, entry->qp.handle)) {\r\npr_warn("Failed to add new resource (handle=0x%x:0x%x), error: %d",\r\nhandle.context, handle.resource, result);\r\nkfree(entry);\r\nentry = NULL;\r\n}\r\n}\r\nreturn entry;\r\n}\r\nstatic void qp_guest_endpoint_destroy(struct qp_guest_endpoint *entry)\r\n{\r\nqp_free_ppn_set(&entry->ppn_set);\r\nqp_cleanup_queue_mutex(entry->produce_q, entry->consume_q);\r\nqp_free_queue(entry->produce_q, entry->qp.produce_size);\r\nqp_free_queue(entry->consume_q, entry->qp.consume_size);\r\nvmci_resource_remove(&entry->resource);\r\nkfree(entry);\r\n}\r\nstatic int qp_alloc_hypercall(const struct qp_guest_endpoint *entry)\r\n{\r\nstruct vmci_qp_alloc_msg *alloc_msg;\r\nsize_t msg_size;\r\nint result;\r\nif (!entry || entry->num_ppns <= 2)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nmsg_size = sizeof(*alloc_msg) +\r\n(size_t) entry->num_ppns * sizeof(u32);\r\nalloc_msg = kmalloc(msg_size, GFP_KERNEL);\r\nif (!alloc_msg)\r\nreturn VMCI_ERROR_NO_MEM;\r\nalloc_msg->hdr.dst = vmci_make_handle(VMCI_HYPERVISOR_CONTEXT_ID,\r\nVMCI_QUEUEPAIR_ALLOC);\r\nalloc_msg->hdr.src = VMCI_ANON_SRC_HANDLE;\r\nalloc_msg->hdr.payload_size = msg_size - VMCI_DG_HEADERSIZE;\r\nalloc_msg->handle = entry->qp.handle;\r\nalloc_msg->peer = entry->qp.peer;\r\nalloc_msg->flags = entry->qp.flags;\r\nalloc_msg->produce_size = entry->qp.produce_size;\r\nalloc_msg->consume_size = entry->qp.consume_size;\r\nalloc_msg->num_ppns = entry->num_ppns;\r\nresult = qp_populate_ppn_set((u8 *)alloc_msg + sizeof(*alloc_msg),\r\n&entry->ppn_set);\r\nif (result == VMCI_SUCCESS)\r\nresult = vmci_send_datagram(&alloc_msg->hdr);\r\nkfree(alloc_msg);\r\nreturn result;\r\n}\r\nstatic int qp_detatch_hypercall(struct vmci_handle handle)\r\n{\r\nstruct vmci_qp_detach_msg detach_msg;\r\ndetach_msg.hdr.dst = vmci_make_handle(VMCI_HYPERVISOR_CONTEXT_ID,\r\nVMCI_QUEUEPAIR_DETACH);\r\ndetach_msg.hdr.src = VMCI_ANON_SRC_HANDLE;\r\ndetach_msg.hdr.payload_size = sizeof(handle);\r\ndetach_msg.handle = handle;\r\nreturn vmci_send_datagram(&detach_msg.hdr);\r\n}\r\nstatic void qp_list_add_entry(struct qp_list *qp_list, struct qp_entry *entry)\r\n{\r\nif (entry)\r\nlist_add(&entry->list_item, &qp_list->head);\r\n}\r\nstatic void qp_list_remove_entry(struct qp_list *qp_list,\r\nstruct qp_entry *entry)\r\n{\r\nif (entry)\r\nlist_del(&entry->list_item);\r\n}\r\nstatic int qp_detatch_guest_work(struct vmci_handle handle)\r\n{\r\nint result;\r\nstruct qp_guest_endpoint *entry;\r\nu32 ref_count = ~0;\r\nmutex_lock(&qp_guest_endpoints.mutex);\r\nentry = qp_guest_handle_to_entry(handle);\r\nif (!entry) {\r\nmutex_unlock(&qp_guest_endpoints.mutex);\r\nreturn VMCI_ERROR_NOT_FOUND;\r\n}\r\nif (entry->qp.flags & VMCI_QPFLAG_LOCAL) {\r\nresult = VMCI_SUCCESS;\r\nif (entry->qp.ref_count > 1) {\r\nresult = qp_notify_peer_local(false, handle);\r\n}\r\n} else {\r\nresult = qp_detatch_hypercall(handle);\r\nif (result < VMCI_SUCCESS) {\r\nmutex_unlock(&qp_guest_endpoints.mutex);\r\nreturn result;\r\n}\r\n}\r\nentry->qp.ref_count--;\r\nif (entry->qp.ref_count == 0)\r\nqp_list_remove_entry(&qp_guest_endpoints, &entry->qp);\r\nif (entry)\r\nref_count = entry->qp.ref_count;\r\nmutex_unlock(&qp_guest_endpoints.mutex);\r\nif (ref_count == 0)\r\nqp_guest_endpoint_destroy(entry);\r\nreturn result;\r\n}\r\nstatic int qp_alloc_guest_work(struct vmci_handle *handle,\r\nstruct vmci_queue **produce_q,\r\nu64 produce_size,\r\nstruct vmci_queue **consume_q,\r\nu64 consume_size,\r\nu32 peer,\r\nu32 flags,\r\nu32 priv_flags)\r\n{\r\nconst u64 num_produce_pages =\r\nDIV_ROUND_UP(produce_size, PAGE_SIZE) + 1;\r\nconst u64 num_consume_pages =\r\nDIV_ROUND_UP(consume_size, PAGE_SIZE) + 1;\r\nvoid *my_produce_q = NULL;\r\nvoid *my_consume_q = NULL;\r\nint result;\r\nstruct qp_guest_endpoint *queue_pair_entry = NULL;\r\nif (priv_flags != VMCI_NO_PRIVILEGE_FLAGS)\r\nreturn VMCI_ERROR_NO_ACCESS;\r\nmutex_lock(&qp_guest_endpoints.mutex);\r\nqueue_pair_entry = qp_guest_handle_to_entry(*handle);\r\nif (queue_pair_entry) {\r\nif (queue_pair_entry->qp.flags & VMCI_QPFLAG_LOCAL) {\r\nif (queue_pair_entry->qp.ref_count > 1) {\r\npr_devel("Error attempting to attach more than once\n");\r\nresult = VMCI_ERROR_UNAVAILABLE;\r\ngoto error_keep_entry;\r\n}\r\nif (queue_pair_entry->qp.produce_size != consume_size ||\r\nqueue_pair_entry->qp.consume_size !=\r\nproduce_size ||\r\nqueue_pair_entry->qp.flags !=\r\n(flags & ~VMCI_QPFLAG_ATTACH_ONLY)) {\r\npr_devel("Error mismatched queue pair in local attach\n");\r\nresult = VMCI_ERROR_QUEUEPAIR_MISMATCH;\r\ngoto error_keep_entry;\r\n}\r\nresult = qp_notify_peer_local(true, *handle);\r\nif (result < VMCI_SUCCESS)\r\ngoto error_keep_entry;\r\nmy_produce_q = queue_pair_entry->consume_q;\r\nmy_consume_q = queue_pair_entry->produce_q;\r\ngoto out;\r\n}\r\nresult = VMCI_ERROR_ALREADY_EXISTS;\r\ngoto error_keep_entry;\r\n}\r\nmy_produce_q = qp_alloc_queue(produce_size, flags);\r\nif (!my_produce_q) {\r\npr_warn("Error allocating pages for produce queue\n");\r\nresult = VMCI_ERROR_NO_MEM;\r\ngoto error;\r\n}\r\nmy_consume_q = qp_alloc_queue(consume_size, flags);\r\nif (!my_consume_q) {\r\npr_warn("Error allocating pages for consume queue\n");\r\nresult = VMCI_ERROR_NO_MEM;\r\ngoto error;\r\n}\r\nqueue_pair_entry = qp_guest_endpoint_create(*handle, peer, flags,\r\nproduce_size, consume_size,\r\nmy_produce_q, my_consume_q);\r\nif (!queue_pair_entry) {\r\npr_warn("Error allocating memory in %s\n", __func__);\r\nresult = VMCI_ERROR_NO_MEM;\r\ngoto error;\r\n}\r\nresult = qp_alloc_ppn_set(my_produce_q, num_produce_pages, my_consume_q,\r\nnum_consume_pages,\r\n&queue_pair_entry->ppn_set);\r\nif (result < VMCI_SUCCESS) {\r\npr_warn("qp_alloc_ppn_set failed\n");\r\ngoto error;\r\n}\r\nif (queue_pair_entry->qp.flags & VMCI_QPFLAG_LOCAL) {\r\nu32 context_id = vmci_get_context_id();\r\nif (queue_pair_entry->qp.handle.context != context_id ||\r\n(queue_pair_entry->qp.peer != VMCI_INVALID_ID &&\r\nqueue_pair_entry->qp.peer != context_id)) {\r\nresult = VMCI_ERROR_NO_ACCESS;\r\ngoto error;\r\n}\r\nif (queue_pair_entry->qp.flags & VMCI_QPFLAG_ATTACH_ONLY) {\r\nresult = VMCI_ERROR_NOT_FOUND;\r\ngoto error;\r\n}\r\n} else {\r\nresult = qp_alloc_hypercall(queue_pair_entry);\r\nif (result < VMCI_SUCCESS) {\r\npr_warn("qp_alloc_hypercall result = %d\n", result);\r\ngoto error;\r\n}\r\n}\r\nqp_init_queue_mutex((struct vmci_queue *)my_produce_q,\r\n(struct vmci_queue *)my_consume_q);\r\nqp_list_add_entry(&qp_guest_endpoints, &queue_pair_entry->qp);\r\nout:\r\nqueue_pair_entry->qp.ref_count++;\r\n*handle = queue_pair_entry->qp.handle;\r\n*produce_q = (struct vmci_queue *)my_produce_q;\r\n*consume_q = (struct vmci_queue *)my_consume_q;\r\nif ((queue_pair_entry->qp.flags & VMCI_QPFLAG_LOCAL) &&\r\nqueue_pair_entry->qp.ref_count == 1) {\r\nvmci_q_header_init((*produce_q)->q_header, *handle);\r\nvmci_q_header_init((*consume_q)->q_header, *handle);\r\n}\r\nmutex_unlock(&qp_guest_endpoints.mutex);\r\nreturn VMCI_SUCCESS;\r\nerror:\r\nmutex_unlock(&qp_guest_endpoints.mutex);\r\nif (queue_pair_entry) {\r\nqp_guest_endpoint_destroy(queue_pair_entry);\r\n} else {\r\nqp_free_queue(my_produce_q, produce_size);\r\nqp_free_queue(my_consume_q, consume_size);\r\n}\r\nreturn result;\r\nerror_keep_entry:\r\nmutex_unlock(&qp_guest_endpoints.mutex);\r\nreturn result;\r\n}\r\nstatic int qp_broker_create(struct vmci_handle handle,\r\nu32 peer,\r\nu32 flags,\r\nu32 priv_flags,\r\nu64 produce_size,\r\nu64 consume_size,\r\nstruct vmci_qp_page_store *page_store,\r\nstruct vmci_ctx *context,\r\nvmci_event_release_cb wakeup_cb,\r\nvoid *client_data, struct qp_broker_entry **ent)\r\n{\r\nstruct qp_broker_entry *entry = NULL;\r\nconst u32 context_id = vmci_ctx_get_id(context);\r\nbool is_local = flags & VMCI_QPFLAG_LOCAL;\r\nint result;\r\nu64 guest_produce_size;\r\nu64 guest_consume_size;\r\nif (flags & VMCI_QPFLAG_ATTACH_ONLY)\r\nreturn VMCI_ERROR_NOT_FOUND;\r\nif (handle.context != context_id && handle.context != peer)\r\nreturn VMCI_ERROR_NO_ACCESS;\r\nif (VMCI_CONTEXT_IS_VM(context_id) && VMCI_CONTEXT_IS_VM(peer))\r\nreturn VMCI_ERROR_DST_UNREACHABLE;\r\nif (is_local && peer != VMCI_INVALID_ID && context_id != peer)\r\nreturn VMCI_ERROR_NO_ACCESS;\r\nentry = kzalloc(sizeof(*entry), GFP_ATOMIC);\r\nif (!entry)\r\nreturn VMCI_ERROR_NO_MEM;\r\nif (vmci_ctx_get_id(context) == VMCI_HOST_CONTEXT_ID && !is_local) {\r\nguest_produce_size = consume_size;\r\nguest_consume_size = produce_size;\r\n} else {\r\nguest_produce_size = produce_size;\r\nguest_consume_size = consume_size;\r\n}\r\nentry->qp.handle = handle;\r\nentry->qp.peer = peer;\r\nentry->qp.flags = flags;\r\nentry->qp.produce_size = guest_produce_size;\r\nentry->qp.consume_size = guest_consume_size;\r\nentry->qp.ref_count = 1;\r\nentry->create_id = context_id;\r\nentry->attach_id = VMCI_INVALID_ID;\r\nentry->state = VMCIQPB_NEW;\r\nentry->require_trusted_attach =\r\n!!(context->priv_flags & VMCI_PRIVILEGE_FLAG_RESTRICTED);\r\nentry->created_by_trusted =\r\n!!(priv_flags & VMCI_PRIVILEGE_FLAG_TRUSTED);\r\nentry->vmci_page_files = false;\r\nentry->wakeup_cb = wakeup_cb;\r\nentry->client_data = client_data;\r\nentry->produce_q = qp_host_alloc_queue(guest_produce_size);\r\nif (entry->produce_q == NULL) {\r\nresult = VMCI_ERROR_NO_MEM;\r\ngoto error;\r\n}\r\nentry->consume_q = qp_host_alloc_queue(guest_consume_size);\r\nif (entry->consume_q == NULL) {\r\nresult = VMCI_ERROR_NO_MEM;\r\ngoto error;\r\n}\r\nqp_init_queue_mutex(entry->produce_q, entry->consume_q);\r\nINIT_LIST_HEAD(&entry->qp.list_item);\r\nif (is_local) {\r\nu8 *tmp;\r\nentry->local_mem = kcalloc(QPE_NUM_PAGES(entry->qp),\r\nPAGE_SIZE, GFP_KERNEL);\r\nif (entry->local_mem == NULL) {\r\nresult = VMCI_ERROR_NO_MEM;\r\ngoto error;\r\n}\r\nentry->state = VMCIQPB_CREATED_MEM;\r\nentry->produce_q->q_header = entry->local_mem;\r\ntmp = (u8 *)entry->local_mem + PAGE_SIZE *\r\n(DIV_ROUND_UP(entry->qp.produce_size, PAGE_SIZE) + 1);\r\nentry->consume_q->q_header = (struct vmci_queue_header *)tmp;\r\n} else if (page_store) {\r\nresult = qp_host_register_user_memory(page_store,\r\nentry->produce_q,\r\nentry->consume_q);\r\nif (result < VMCI_SUCCESS)\r\ngoto error;\r\nentry->state = VMCIQPB_CREATED_MEM;\r\n} else {\r\nentry->state = VMCIQPB_CREATED_NO_MEM;\r\n}\r\nqp_list_add_entry(&qp_broker_list, &entry->qp);\r\nif (ent != NULL)\r\n*ent = entry;\r\nresult = vmci_resource_add(&entry->resource,\r\nVMCI_RESOURCE_TYPE_QPAIR_HOST,\r\nhandle);\r\nif (result != VMCI_SUCCESS) {\r\npr_warn("Failed to add new resource (handle=0x%x:0x%x), error: %d",\r\nhandle.context, handle.resource, result);\r\ngoto error;\r\n}\r\nentry->qp.handle = vmci_resource_handle(&entry->resource);\r\nif (is_local) {\r\nvmci_q_header_init(entry->produce_q->q_header,\r\nentry->qp.handle);\r\nvmci_q_header_init(entry->consume_q->q_header,\r\nentry->qp.handle);\r\n}\r\nvmci_ctx_qp_create(context, entry->qp.handle);\r\nreturn VMCI_SUCCESS;\r\nerror:\r\nif (entry != NULL) {\r\nqp_host_free_queue(entry->produce_q, guest_produce_size);\r\nqp_host_free_queue(entry->consume_q, guest_consume_size);\r\nkfree(entry);\r\n}\r\nreturn result;\r\n}\r\nstatic int qp_notify_peer(bool attach,\r\nstruct vmci_handle handle,\r\nu32 my_id,\r\nu32 peer_id)\r\n{\r\nint rv;\r\nstruct vmci_event_qp ev;\r\nif (vmci_handle_is_invalid(handle) || my_id == VMCI_INVALID_ID ||\r\npeer_id == VMCI_INVALID_ID)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nev.msg.hdr.dst = vmci_make_handle(peer_id, VMCI_EVENT_HANDLER);\r\nev.msg.hdr.src = vmci_make_handle(VMCI_HYPERVISOR_CONTEXT_ID,\r\nVMCI_CONTEXT_RESOURCE_ID);\r\nev.msg.hdr.payload_size = sizeof(ev) - sizeof(ev.msg.hdr);\r\nev.msg.event_data.event = attach ?\r\nVMCI_EVENT_QP_PEER_ATTACH : VMCI_EVENT_QP_PEER_DETACH;\r\nev.payload.handle = handle;\r\nev.payload.peer_id = my_id;\r\nrv = vmci_datagram_dispatch(VMCI_HYPERVISOR_CONTEXT_ID,\r\n&ev.msg.hdr, false);\r\nif (rv < VMCI_SUCCESS)\r\npr_warn("Failed to enqueue queue_pair %s event datagram for context (ID=0x%x)\n",\r\nattach ? "ATTACH" : "DETACH", peer_id);\r\nreturn rv;\r\n}\r\nstatic int qp_broker_attach(struct qp_broker_entry *entry,\r\nu32 peer,\r\nu32 flags,\r\nu32 priv_flags,\r\nu64 produce_size,\r\nu64 consume_size,\r\nstruct vmci_qp_page_store *page_store,\r\nstruct vmci_ctx *context,\r\nvmci_event_release_cb wakeup_cb,\r\nvoid *client_data,\r\nstruct qp_broker_entry **ent)\r\n{\r\nconst u32 context_id = vmci_ctx_get_id(context);\r\nbool is_local = flags & VMCI_QPFLAG_LOCAL;\r\nint result;\r\nif (entry->state != VMCIQPB_CREATED_NO_MEM &&\r\nentry->state != VMCIQPB_CREATED_MEM)\r\nreturn VMCI_ERROR_UNAVAILABLE;\r\nif (is_local) {\r\nif (!(entry->qp.flags & VMCI_QPFLAG_LOCAL) ||\r\ncontext_id != entry->create_id) {\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\n}\r\n} else if (context_id == entry->create_id ||\r\ncontext_id == entry->attach_id) {\r\nreturn VMCI_ERROR_ALREADY_EXISTS;\r\n}\r\nif (VMCI_CONTEXT_IS_VM(context_id) &&\r\nVMCI_CONTEXT_IS_VM(entry->create_id))\r\nreturn VMCI_ERROR_DST_UNREACHABLE;\r\nif ((context->priv_flags & VMCI_PRIVILEGE_FLAG_RESTRICTED) &&\r\n!entry->created_by_trusted)\r\nreturn VMCI_ERROR_NO_ACCESS;\r\nif (entry->require_trusted_attach &&\r\n(!(priv_flags & VMCI_PRIVILEGE_FLAG_TRUSTED)))\r\nreturn VMCI_ERROR_NO_ACCESS;\r\nif (entry->qp.peer != VMCI_INVALID_ID && entry->qp.peer != context_id)\r\nreturn VMCI_ERROR_NO_ACCESS;\r\nif (entry->create_id == VMCI_HOST_CONTEXT_ID) {\r\nif (!vmci_ctx_supports_host_qp(context))\r\nreturn VMCI_ERROR_INVALID_RESOURCE;\r\n} else if (context_id == VMCI_HOST_CONTEXT_ID) {\r\nstruct vmci_ctx *create_context;\r\nbool supports_host_qp;\r\ncreate_context = vmci_ctx_get(entry->create_id);\r\nsupports_host_qp = vmci_ctx_supports_host_qp(create_context);\r\nvmci_ctx_put(create_context);\r\nif (!supports_host_qp)\r\nreturn VMCI_ERROR_INVALID_RESOURCE;\r\n}\r\nif ((entry->qp.flags & ~VMCI_QP_ASYMM) != (flags & ~VMCI_QP_ASYMM_PEER))\r\nreturn VMCI_ERROR_QUEUEPAIR_MISMATCH;\r\nif (context_id != VMCI_HOST_CONTEXT_ID) {\r\nif (entry->qp.produce_size != produce_size ||\r\nentry->qp.consume_size != consume_size) {\r\nreturn VMCI_ERROR_QUEUEPAIR_MISMATCH;\r\n}\r\n} else if (entry->qp.produce_size != consume_size ||\r\nentry->qp.consume_size != produce_size) {\r\nreturn VMCI_ERROR_QUEUEPAIR_MISMATCH;\r\n}\r\nif (context_id != VMCI_HOST_CONTEXT_ID) {\r\nif (entry->state != VMCIQPB_CREATED_NO_MEM)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nif (page_store != NULL) {\r\nresult = qp_host_register_user_memory(page_store,\r\nentry->produce_q,\r\nentry->consume_q);\r\nif (result < VMCI_SUCCESS)\r\nreturn result;\r\nif (entry->qp.flags & VMCI_QPFLAG_NONBLOCK) {\r\nresult = qp_host_map_queues(entry->produce_q,\r\nentry->consume_q);\r\nif (result < VMCI_SUCCESS) {\r\nqp_host_unregister_user_memory(\r\nentry->produce_q,\r\nentry->consume_q);\r\nreturn result;\r\n}\r\n}\r\nentry->state = VMCIQPB_ATTACHED_MEM;\r\n} else {\r\nentry->state = VMCIQPB_ATTACHED_NO_MEM;\r\n}\r\n} else if (entry->state == VMCIQPB_CREATED_NO_MEM) {\r\nreturn VMCI_ERROR_UNAVAILABLE;\r\n} else {\r\nif (flags & VMCI_QPFLAG_NONBLOCK) {\r\nresult =\r\nqp_host_map_queues(entry->produce_q,\r\nentry->consume_q);\r\nif (result < VMCI_SUCCESS)\r\nreturn result;\r\nentry->qp.flags |= flags &\r\n(VMCI_QPFLAG_NONBLOCK | VMCI_QPFLAG_PINNED);\r\n}\r\nentry->state = VMCIQPB_ATTACHED_MEM;\r\n}\r\nif (entry->state == VMCIQPB_ATTACHED_MEM) {\r\nresult =\r\nqp_notify_peer(true, entry->qp.handle, context_id,\r\nentry->create_id);\r\nif (result < VMCI_SUCCESS)\r\npr_warn("Failed to notify peer (ID=0x%x) of attach to queue pair (handle=0x%x:0x%x)\n",\r\nentry->create_id, entry->qp.handle.context,\r\nentry->qp.handle.resource);\r\n}\r\nentry->attach_id = context_id;\r\nentry->qp.ref_count++;\r\nif (wakeup_cb) {\r\nentry->wakeup_cb = wakeup_cb;\r\nentry->client_data = client_data;\r\n}\r\nif (!is_local)\r\nvmci_ctx_qp_create(context, entry->qp.handle);\r\nif (ent != NULL)\r\n*ent = entry;\r\nreturn VMCI_SUCCESS;\r\n}\r\nstatic int qp_broker_alloc(struct vmci_handle handle,\r\nu32 peer,\r\nu32 flags,\r\nu32 priv_flags,\r\nu64 produce_size,\r\nu64 consume_size,\r\nstruct vmci_qp_page_store *page_store,\r\nstruct vmci_ctx *context,\r\nvmci_event_release_cb wakeup_cb,\r\nvoid *client_data,\r\nstruct qp_broker_entry **ent,\r\nbool *swap)\r\n{\r\nconst u32 context_id = vmci_ctx_get_id(context);\r\nbool create;\r\nstruct qp_broker_entry *entry = NULL;\r\nbool is_local = flags & VMCI_QPFLAG_LOCAL;\r\nint result;\r\nif (vmci_handle_is_invalid(handle) ||\r\n(flags & ~VMCI_QP_ALL_FLAGS) || is_local ||\r\n!(produce_size || consume_size) ||\r\n!context || context_id == VMCI_INVALID_ID ||\r\nhandle.context == VMCI_INVALID_ID) {\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\n}\r\nif (page_store && !VMCI_QP_PAGESTORE_IS_WELLFORMED(page_store))\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nmutex_lock(&qp_broker_list.mutex);\r\nif (!is_local && vmci_ctx_qp_exists(context, handle)) {\r\npr_devel("Context (ID=0x%x) already attached to queue pair (handle=0x%x:0x%x)\n",\r\ncontext_id, handle.context, handle.resource);\r\nmutex_unlock(&qp_broker_list.mutex);\r\nreturn VMCI_ERROR_ALREADY_EXISTS;\r\n}\r\nif (handle.resource != VMCI_INVALID_ID)\r\nentry = qp_broker_handle_to_entry(handle);\r\nif (!entry) {\r\ncreate = true;\r\nresult =\r\nqp_broker_create(handle, peer, flags, priv_flags,\r\nproduce_size, consume_size, page_store,\r\ncontext, wakeup_cb, client_data, ent);\r\n} else {\r\ncreate = false;\r\nresult =\r\nqp_broker_attach(entry, peer, flags, priv_flags,\r\nproduce_size, consume_size, page_store,\r\ncontext, wakeup_cb, client_data, ent);\r\n}\r\nmutex_unlock(&qp_broker_list.mutex);\r\nif (swap)\r\n*swap = (context_id == VMCI_HOST_CONTEXT_ID) &&\r\n!(create && is_local);\r\nreturn result;\r\n}\r\nstatic int qp_alloc_host_work(struct vmci_handle *handle,\r\nstruct vmci_queue **produce_q,\r\nu64 produce_size,\r\nstruct vmci_queue **consume_q,\r\nu64 consume_size,\r\nu32 peer,\r\nu32 flags,\r\nu32 priv_flags,\r\nvmci_event_release_cb wakeup_cb,\r\nvoid *client_data)\r\n{\r\nstruct vmci_handle new_handle;\r\nstruct vmci_ctx *context;\r\nstruct qp_broker_entry *entry;\r\nint result;\r\nbool swap;\r\nif (vmci_handle_is_invalid(*handle)) {\r\nnew_handle = vmci_make_handle(\r\nVMCI_HOST_CONTEXT_ID, VMCI_INVALID_ID);\r\n} else\r\nnew_handle = *handle;\r\ncontext = vmci_ctx_get(VMCI_HOST_CONTEXT_ID);\r\nentry = NULL;\r\nresult =\r\nqp_broker_alloc(new_handle, peer, flags, priv_flags,\r\nproduce_size, consume_size, NULL, context,\r\nwakeup_cb, client_data, &entry, &swap);\r\nif (result == VMCI_SUCCESS) {\r\nif (swap) {\r\n*produce_q = entry->consume_q;\r\n*consume_q = entry->produce_q;\r\n} else {\r\n*produce_q = entry->produce_q;\r\n*consume_q = entry->consume_q;\r\n}\r\n*handle = vmci_resource_handle(&entry->resource);\r\n} else {\r\n*handle = VMCI_INVALID_HANDLE;\r\npr_devel("queue pair broker failed to alloc (result=%d)\n",\r\nresult);\r\n}\r\nvmci_ctx_put(context);\r\nreturn result;\r\n}\r\nint vmci_qp_alloc(struct vmci_handle *handle,\r\nstruct vmci_queue **produce_q,\r\nu64 produce_size,\r\nstruct vmci_queue **consume_q,\r\nu64 consume_size,\r\nu32 peer,\r\nu32 flags,\r\nu32 priv_flags,\r\nbool guest_endpoint,\r\nvmci_event_release_cb wakeup_cb,\r\nvoid *client_data)\r\n{\r\nif (!handle || !produce_q || !consume_q ||\r\n(!produce_size && !consume_size) || (flags & ~VMCI_QP_ALL_FLAGS))\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nif (guest_endpoint) {\r\nreturn qp_alloc_guest_work(handle, produce_q,\r\nproduce_size, consume_q,\r\nconsume_size, peer,\r\nflags, priv_flags);\r\n} else {\r\nreturn qp_alloc_host_work(handle, produce_q,\r\nproduce_size, consume_q,\r\nconsume_size, peer, flags,\r\npriv_flags, wakeup_cb, client_data);\r\n}\r\n}\r\nstatic int qp_detatch_host_work(struct vmci_handle handle)\r\n{\r\nint result;\r\nstruct vmci_ctx *context;\r\ncontext = vmci_ctx_get(VMCI_HOST_CONTEXT_ID);\r\nresult = vmci_qp_broker_detach(handle, context);\r\nvmci_ctx_put(context);\r\nreturn result;\r\n}\r\nstatic int qp_detatch(struct vmci_handle handle, bool guest_endpoint)\r\n{\r\nif (vmci_handle_is_invalid(handle))\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nif (guest_endpoint)\r\nreturn qp_detatch_guest_work(handle);\r\nelse\r\nreturn qp_detatch_host_work(handle);\r\n}\r\nstatic struct qp_entry *qp_list_get_head(struct qp_list *qp_list)\r\n{\r\nif (!list_empty(&qp_list->head)) {\r\nstruct qp_entry *entry =\r\nlist_first_entry(&qp_list->head, struct qp_entry,\r\nlist_item);\r\nreturn entry;\r\n}\r\nreturn NULL;\r\n}\r\nvoid vmci_qp_broker_exit(void)\r\n{\r\nstruct qp_entry *entry;\r\nstruct qp_broker_entry *be;\r\nmutex_lock(&qp_broker_list.mutex);\r\nwhile ((entry = qp_list_get_head(&qp_broker_list))) {\r\nbe = (struct qp_broker_entry *)entry;\r\nqp_list_remove_entry(&qp_broker_list, entry);\r\nkfree(be);\r\n}\r\nmutex_unlock(&qp_broker_list.mutex);\r\n}\r\nint vmci_qp_broker_alloc(struct vmci_handle handle,\r\nu32 peer,\r\nu32 flags,\r\nu32 priv_flags,\r\nu64 produce_size,\r\nu64 consume_size,\r\nstruct vmci_qp_page_store *page_store,\r\nstruct vmci_ctx *context)\r\n{\r\nreturn qp_broker_alloc(handle, peer, flags, priv_flags,\r\nproduce_size, consume_size,\r\npage_store, context, NULL, NULL, NULL, NULL);\r\n}\r\nint vmci_qp_broker_set_page_store(struct vmci_handle handle,\r\nu64 produce_uva,\r\nu64 consume_uva,\r\nstruct vmci_ctx *context)\r\n{\r\nstruct qp_broker_entry *entry;\r\nint result;\r\nconst u32 context_id = vmci_ctx_get_id(context);\r\nif (vmci_handle_is_invalid(handle) || !context ||\r\ncontext_id == VMCI_INVALID_ID)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nif (produce_uva == 0 || consume_uva == 0)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nmutex_lock(&qp_broker_list.mutex);\r\nif (!vmci_ctx_qp_exists(context, handle)) {\r\npr_warn("Context (ID=0x%x) not attached to queue pair (handle=0x%x:0x%x)\n",\r\ncontext_id, handle.context, handle.resource);\r\nresult = VMCI_ERROR_NOT_FOUND;\r\ngoto out;\r\n}\r\nentry = qp_broker_handle_to_entry(handle);\r\nif (!entry) {\r\nresult = VMCI_ERROR_NOT_FOUND;\r\ngoto out;\r\n}\r\nif (entry->create_id != context_id &&\r\n(entry->create_id != VMCI_HOST_CONTEXT_ID ||\r\nentry->attach_id != context_id)) {\r\nresult = VMCI_ERROR_QUEUEPAIR_NOTOWNER;\r\ngoto out;\r\n}\r\nif (entry->state != VMCIQPB_CREATED_NO_MEM &&\r\nentry->state != VMCIQPB_ATTACHED_NO_MEM) {\r\nresult = VMCI_ERROR_UNAVAILABLE;\r\ngoto out;\r\n}\r\nresult = qp_host_get_user_memory(produce_uva, consume_uva,\r\nentry->produce_q, entry->consume_q);\r\nif (result < VMCI_SUCCESS)\r\ngoto out;\r\nresult = qp_host_map_queues(entry->produce_q, entry->consume_q);\r\nif (result < VMCI_SUCCESS) {\r\nqp_host_unregister_user_memory(entry->produce_q,\r\nentry->consume_q);\r\ngoto out;\r\n}\r\nif (entry->state == VMCIQPB_CREATED_NO_MEM)\r\nentry->state = VMCIQPB_CREATED_MEM;\r\nelse\r\nentry->state = VMCIQPB_ATTACHED_MEM;\r\nentry->vmci_page_files = true;\r\nif (entry->state == VMCIQPB_ATTACHED_MEM) {\r\nresult =\r\nqp_notify_peer(true, handle, context_id, entry->create_id);\r\nif (result < VMCI_SUCCESS) {\r\npr_warn("Failed to notify peer (ID=0x%x) of attach to queue pair (handle=0x%x:0x%x)\n",\r\nentry->create_id, entry->qp.handle.context,\r\nentry->qp.handle.resource);\r\n}\r\n}\r\nresult = VMCI_SUCCESS;\r\nout:\r\nmutex_unlock(&qp_broker_list.mutex);\r\nreturn result;\r\n}\r\nstatic void qp_reset_saved_headers(struct qp_broker_entry *entry)\r\n{\r\nentry->produce_q->saved_header = NULL;\r\nentry->consume_q->saved_header = NULL;\r\n}\r\nint vmci_qp_broker_detach(struct vmci_handle handle, struct vmci_ctx *context)\r\n{\r\nstruct qp_broker_entry *entry;\r\nconst u32 context_id = vmci_ctx_get_id(context);\r\nu32 peer_id;\r\nbool is_local = false;\r\nint result;\r\nif (vmci_handle_is_invalid(handle) || !context ||\r\ncontext_id == VMCI_INVALID_ID) {\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\n}\r\nmutex_lock(&qp_broker_list.mutex);\r\nif (!vmci_ctx_qp_exists(context, handle)) {\r\npr_devel("Context (ID=0x%x) not attached to queue pair (handle=0x%x:0x%x)\n",\r\ncontext_id, handle.context, handle.resource);\r\nresult = VMCI_ERROR_NOT_FOUND;\r\ngoto out;\r\n}\r\nentry = qp_broker_handle_to_entry(handle);\r\nif (!entry) {\r\npr_devel("Context (ID=0x%x) reports being attached to queue pair(handle=0x%x:0x%x) that isn't present in broker\n",\r\ncontext_id, handle.context, handle.resource);\r\nresult = VMCI_ERROR_NOT_FOUND;\r\ngoto out;\r\n}\r\nif (context_id != entry->create_id && context_id != entry->attach_id) {\r\nresult = VMCI_ERROR_QUEUEPAIR_NOTATTACHED;\r\ngoto out;\r\n}\r\nif (context_id == entry->create_id) {\r\npeer_id = entry->attach_id;\r\nentry->create_id = VMCI_INVALID_ID;\r\n} else {\r\npeer_id = entry->create_id;\r\nentry->attach_id = VMCI_INVALID_ID;\r\n}\r\nentry->qp.ref_count--;\r\nis_local = entry->qp.flags & VMCI_QPFLAG_LOCAL;\r\nif (context_id != VMCI_HOST_CONTEXT_ID) {\r\nbool headers_mapped;\r\nqp_acquire_queue_mutex(entry->produce_q);\r\nheaders_mapped = entry->produce_q->q_header ||\r\nentry->consume_q->q_header;\r\nif (QPBROKERSTATE_HAS_MEM(entry)) {\r\nresult =\r\nqp_host_unmap_queues(INVALID_VMCI_GUEST_MEM_ID,\r\nentry->produce_q,\r\nentry->consume_q);\r\nif (result < VMCI_SUCCESS)\r\npr_warn("Failed to unmap queue headers for queue pair (handle=0x%x:0x%x,result=%d)\n",\r\nhandle.context, handle.resource,\r\nresult);\r\nif (entry->vmci_page_files)\r\nqp_host_unregister_user_memory(entry->produce_q,\r\nentry->\r\nconsume_q);\r\nelse\r\nqp_host_unregister_user_memory(entry->produce_q,\r\nentry->\r\nconsume_q);\r\n}\r\nif (!headers_mapped)\r\nqp_reset_saved_headers(entry);\r\nqp_release_queue_mutex(entry->produce_q);\r\nif (!headers_mapped && entry->wakeup_cb)\r\nentry->wakeup_cb(entry->client_data);\r\n} else {\r\nif (entry->wakeup_cb) {\r\nentry->wakeup_cb = NULL;\r\nentry->client_data = NULL;\r\n}\r\n}\r\nif (entry->qp.ref_count == 0) {\r\nqp_list_remove_entry(&qp_broker_list, &entry->qp);\r\nif (is_local)\r\nkfree(entry->local_mem);\r\nqp_cleanup_queue_mutex(entry->produce_q, entry->consume_q);\r\nqp_host_free_queue(entry->produce_q, entry->qp.produce_size);\r\nqp_host_free_queue(entry->consume_q, entry->qp.consume_size);\r\nvmci_resource_remove(&entry->resource);\r\nkfree(entry);\r\nvmci_ctx_qp_destroy(context, handle);\r\n} else {\r\nqp_notify_peer(false, handle, context_id, peer_id);\r\nif (context_id == VMCI_HOST_CONTEXT_ID &&\r\nQPBROKERSTATE_HAS_MEM(entry)) {\r\nentry->state = VMCIQPB_SHUTDOWN_MEM;\r\n} else {\r\nentry->state = VMCIQPB_SHUTDOWN_NO_MEM;\r\n}\r\nif (!is_local)\r\nvmci_ctx_qp_destroy(context, handle);\r\n}\r\nresult = VMCI_SUCCESS;\r\nout:\r\nmutex_unlock(&qp_broker_list.mutex);\r\nreturn result;\r\n}\r\nint vmci_qp_broker_map(struct vmci_handle handle,\r\nstruct vmci_ctx *context,\r\nu64 guest_mem)\r\n{\r\nstruct qp_broker_entry *entry;\r\nconst u32 context_id = vmci_ctx_get_id(context);\r\nbool is_local = false;\r\nint result;\r\nif (vmci_handle_is_invalid(handle) || !context ||\r\ncontext_id == VMCI_INVALID_ID)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nmutex_lock(&qp_broker_list.mutex);\r\nif (!vmci_ctx_qp_exists(context, handle)) {\r\npr_devel("Context (ID=0x%x) not attached to queue pair (handle=0x%x:0x%x)\n",\r\ncontext_id, handle.context, handle.resource);\r\nresult = VMCI_ERROR_NOT_FOUND;\r\ngoto out;\r\n}\r\nentry = qp_broker_handle_to_entry(handle);\r\nif (!entry) {\r\npr_devel("Context (ID=0x%x) reports being attached to queue pair (handle=0x%x:0x%x) that isn't present in broker\n",\r\ncontext_id, handle.context, handle.resource);\r\nresult = VMCI_ERROR_NOT_FOUND;\r\ngoto out;\r\n}\r\nif (context_id != entry->create_id && context_id != entry->attach_id) {\r\nresult = VMCI_ERROR_QUEUEPAIR_NOTATTACHED;\r\ngoto out;\r\n}\r\nis_local = entry->qp.flags & VMCI_QPFLAG_LOCAL;\r\nresult = VMCI_SUCCESS;\r\nif (context_id != VMCI_HOST_CONTEXT_ID) {\r\nstruct vmci_qp_page_store page_store;\r\npage_store.pages = guest_mem;\r\npage_store.len = QPE_NUM_PAGES(entry->qp);\r\nqp_acquire_queue_mutex(entry->produce_q);\r\nqp_reset_saved_headers(entry);\r\nresult =\r\nqp_host_register_user_memory(&page_store,\r\nentry->produce_q,\r\nentry->consume_q);\r\nqp_release_queue_mutex(entry->produce_q);\r\nif (result == VMCI_SUCCESS) {\r\nentry->state++;\r\nif (entry->wakeup_cb)\r\nentry->wakeup_cb(entry->client_data);\r\n}\r\n}\r\nout:\r\nmutex_unlock(&qp_broker_list.mutex);\r\nreturn result;\r\n}\r\nstatic int qp_save_headers(struct qp_broker_entry *entry)\r\n{\r\nint result;\r\nif (entry->produce_q->saved_header != NULL &&\r\nentry->consume_q->saved_header != NULL) {\r\nreturn VMCI_SUCCESS;\r\n}\r\nif (NULL == entry->produce_q->q_header ||\r\nNULL == entry->consume_q->q_header) {\r\nresult = qp_host_map_queues(entry->produce_q, entry->consume_q);\r\nif (result < VMCI_SUCCESS)\r\nreturn result;\r\n}\r\nmemcpy(&entry->saved_produce_q, entry->produce_q->q_header,\r\nsizeof(entry->saved_produce_q));\r\nentry->produce_q->saved_header = &entry->saved_produce_q;\r\nmemcpy(&entry->saved_consume_q, entry->consume_q->q_header,\r\nsizeof(entry->saved_consume_q));\r\nentry->consume_q->saved_header = &entry->saved_consume_q;\r\nreturn VMCI_SUCCESS;\r\n}\r\nint vmci_qp_broker_unmap(struct vmci_handle handle,\r\nstruct vmci_ctx *context,\r\nu32 gid)\r\n{\r\nstruct qp_broker_entry *entry;\r\nconst u32 context_id = vmci_ctx_get_id(context);\r\nbool is_local = false;\r\nint result;\r\nif (vmci_handle_is_invalid(handle) || !context ||\r\ncontext_id == VMCI_INVALID_ID)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nmutex_lock(&qp_broker_list.mutex);\r\nif (!vmci_ctx_qp_exists(context, handle)) {\r\npr_devel("Context (ID=0x%x) not attached to queue pair (handle=0x%x:0x%x)\n",\r\ncontext_id, handle.context, handle.resource);\r\nresult = VMCI_ERROR_NOT_FOUND;\r\ngoto out;\r\n}\r\nentry = qp_broker_handle_to_entry(handle);\r\nif (!entry) {\r\npr_devel("Context (ID=0x%x) reports being attached to queue pair (handle=0x%x:0x%x) that isn't present in broker\n",\r\ncontext_id, handle.context, handle.resource);\r\nresult = VMCI_ERROR_NOT_FOUND;\r\ngoto out;\r\n}\r\nif (context_id != entry->create_id && context_id != entry->attach_id) {\r\nresult = VMCI_ERROR_QUEUEPAIR_NOTATTACHED;\r\ngoto out;\r\n}\r\nis_local = entry->qp.flags & VMCI_QPFLAG_LOCAL;\r\nif (context_id != VMCI_HOST_CONTEXT_ID) {\r\nqp_acquire_queue_mutex(entry->produce_q);\r\nresult = qp_save_headers(entry);\r\nif (result < VMCI_SUCCESS)\r\npr_warn("Failed to save queue headers for queue pair (handle=0x%x:0x%x,result=%d)\n",\r\nhandle.context, handle.resource, result);\r\nqp_host_unmap_queues(gid, entry->produce_q, entry->consume_q);\r\nqp_host_unregister_user_memory(entry->produce_q,\r\nentry->consume_q);\r\nentry->state--;\r\nqp_release_queue_mutex(entry->produce_q);\r\n}\r\nresult = VMCI_SUCCESS;\r\nout:\r\nmutex_unlock(&qp_broker_list.mutex);\r\nreturn result;\r\n}\r\nvoid vmci_qp_guest_endpoints_exit(void)\r\n{\r\nstruct qp_entry *entry;\r\nstruct qp_guest_endpoint *ep;\r\nmutex_lock(&qp_guest_endpoints.mutex);\r\nwhile ((entry = qp_list_get_head(&qp_guest_endpoints))) {\r\nep = (struct qp_guest_endpoint *)entry;\r\nif (!(entry->flags & VMCI_QPFLAG_LOCAL))\r\nqp_detatch_hypercall(entry->handle);\r\nentry->ref_count = 0;\r\nqp_list_remove_entry(&qp_guest_endpoints, entry);\r\nqp_guest_endpoint_destroy(ep);\r\n}\r\nmutex_unlock(&qp_guest_endpoints.mutex);\r\n}\r\nstatic void qp_lock(const struct vmci_qp *qpair)\r\n{\r\nif (vmci_can_block(qpair->flags))\r\nqp_acquire_queue_mutex(qpair->produce_q);\r\n}\r\nstatic void qp_unlock(const struct vmci_qp *qpair)\r\n{\r\nif (vmci_can_block(qpair->flags))\r\nqp_release_queue_mutex(qpair->produce_q);\r\n}\r\nstatic int qp_map_queue_headers(struct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q,\r\nbool can_block)\r\n{\r\nint result;\r\nif (NULL == produce_q->q_header || NULL == consume_q->q_header) {\r\nif (can_block)\r\nresult = qp_host_map_queues(produce_q, consume_q);\r\nelse\r\nresult = VMCI_ERROR_QUEUEPAIR_NOT_READY;\r\nif (result < VMCI_SUCCESS)\r\nreturn (produce_q->saved_header &&\r\nconsume_q->saved_header) ?\r\nVMCI_ERROR_QUEUEPAIR_NOT_READY :\r\nVMCI_ERROR_QUEUEPAIR_NOTATTACHED;\r\n}\r\nreturn VMCI_SUCCESS;\r\n}\r\nstatic int qp_get_queue_headers(const struct vmci_qp *qpair,\r\nstruct vmci_queue_header **produce_q_header,\r\nstruct vmci_queue_header **consume_q_header)\r\n{\r\nint result;\r\nresult = qp_map_queue_headers(qpair->produce_q, qpair->consume_q,\r\nvmci_can_block(qpair->flags));\r\nif (result == VMCI_SUCCESS) {\r\n*produce_q_header = qpair->produce_q->q_header;\r\n*consume_q_header = qpair->consume_q->q_header;\r\n} else if (qpair->produce_q->saved_header &&\r\nqpair->consume_q->saved_header) {\r\n*produce_q_header = qpair->produce_q->saved_header;\r\n*consume_q_header = qpair->consume_q->saved_header;\r\nresult = VMCI_SUCCESS;\r\n}\r\nreturn result;\r\n}\r\nstatic int qp_wakeup_cb(void *client_data)\r\n{\r\nstruct vmci_qp *qpair = (struct vmci_qp *)client_data;\r\nqp_lock(qpair);\r\nwhile (qpair->blocked > 0) {\r\nqpair->blocked--;\r\nqpair->generation++;\r\nwake_up(&qpair->event);\r\n}\r\nqp_unlock(qpair);\r\nreturn VMCI_SUCCESS;\r\n}\r\nstatic bool qp_wait_for_ready_queue(struct vmci_qp *qpair)\r\n{\r\nunsigned int generation;\r\nif (qpair->flags & VMCI_QPFLAG_NONBLOCK)\r\nreturn false;\r\nqpair->blocked++;\r\ngeneration = qpair->generation;\r\nqp_unlock(qpair);\r\nwait_event(qpair->event, generation != qpair->generation);\r\nqp_lock(qpair);\r\nreturn true;\r\n}\r\nstatic ssize_t qp_enqueue_locked(struct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q,\r\nconst u64 produce_q_size,\r\nconst void *buf,\r\nsize_t buf_size,\r\nvmci_memcpy_to_queue_func memcpy_to_queue,\r\nbool can_block)\r\n{\r\ns64 free_space;\r\nu64 tail;\r\nsize_t written;\r\nssize_t result;\r\nresult = qp_map_queue_headers(produce_q, consume_q, can_block);\r\nif (unlikely(result != VMCI_SUCCESS))\r\nreturn result;\r\nfree_space = vmci_q_header_free_space(produce_q->q_header,\r\nconsume_q->q_header,\r\nproduce_q_size);\r\nif (free_space == 0)\r\nreturn VMCI_ERROR_QUEUEPAIR_NOSPACE;\r\nif (free_space < VMCI_SUCCESS)\r\nreturn (ssize_t) free_space;\r\nwritten = (size_t) (free_space > buf_size ? buf_size : free_space);\r\ntail = vmci_q_header_producer_tail(produce_q->q_header);\r\nif (likely(tail + written < produce_q_size)) {\r\nresult = memcpy_to_queue(produce_q, tail, buf, 0, written);\r\n} else {\r\nconst size_t tmp = (size_t) (produce_q_size - tail);\r\nresult = memcpy_to_queue(produce_q, tail, buf, 0, tmp);\r\nif (result >= VMCI_SUCCESS)\r\nresult = memcpy_to_queue(produce_q, 0, buf, tmp,\r\nwritten - tmp);\r\n}\r\nif (result < VMCI_SUCCESS)\r\nreturn result;\r\nvmci_q_header_add_producer_tail(produce_q->q_header, written,\r\nproduce_q_size);\r\nreturn written;\r\n}\r\nstatic ssize_t qp_dequeue_locked(struct vmci_queue *produce_q,\r\nstruct vmci_queue *consume_q,\r\nconst u64 consume_q_size,\r\nvoid *buf,\r\nsize_t buf_size,\r\nvmci_memcpy_from_queue_func memcpy_from_queue,\r\nbool update_consumer,\r\nbool can_block)\r\n{\r\ns64 buf_ready;\r\nu64 head;\r\nsize_t read;\r\nssize_t result;\r\nresult = qp_map_queue_headers(produce_q, consume_q, can_block);\r\nif (unlikely(result != VMCI_SUCCESS))\r\nreturn result;\r\nbuf_ready = vmci_q_header_buf_ready(consume_q->q_header,\r\nproduce_q->q_header,\r\nconsume_q_size);\r\nif (buf_ready == 0)\r\nreturn VMCI_ERROR_QUEUEPAIR_NODATA;\r\nif (buf_ready < VMCI_SUCCESS)\r\nreturn (ssize_t) buf_ready;\r\nread = (size_t) (buf_ready > buf_size ? buf_size : buf_ready);\r\nhead = vmci_q_header_consumer_head(produce_q->q_header);\r\nif (likely(head + read < consume_q_size)) {\r\nresult = memcpy_from_queue(buf, 0, consume_q, head, read);\r\n} else {\r\nconst size_t tmp = (size_t) (consume_q_size - head);\r\nresult = memcpy_from_queue(buf, 0, consume_q, head, tmp);\r\nif (result >= VMCI_SUCCESS)\r\nresult = memcpy_from_queue(buf, tmp, consume_q, 0,\r\nread - tmp);\r\n}\r\nif (result < VMCI_SUCCESS)\r\nreturn result;\r\nif (update_consumer)\r\nvmci_q_header_add_consumer_head(produce_q->q_header,\r\nread, consume_q_size);\r\nreturn read;\r\n}\r\nint vmci_qpair_alloc(struct vmci_qp **qpair,\r\nstruct vmci_handle *handle,\r\nu64 produce_qsize,\r\nu64 consume_qsize,\r\nu32 peer,\r\nu32 flags,\r\nu32 priv_flags)\r\n{\r\nstruct vmci_qp *my_qpair;\r\nint retval;\r\nstruct vmci_handle src = VMCI_INVALID_HANDLE;\r\nstruct vmci_handle dst = vmci_make_handle(peer, VMCI_INVALID_ID);\r\nenum vmci_route route;\r\nvmci_event_release_cb wakeup_cb;\r\nvoid *client_data;\r\nif (produce_qsize + consume_qsize < max(produce_qsize, consume_qsize) ||\r\nproduce_qsize + consume_qsize > VMCI_MAX_GUEST_QP_MEMORY)\r\nreturn VMCI_ERROR_NO_RESOURCES;\r\nretval = vmci_route(&src, &dst, false, &route);\r\nif (retval < VMCI_SUCCESS)\r\nroute = vmci_guest_code_active() ?\r\nVMCI_ROUTE_AS_GUEST : VMCI_ROUTE_AS_HOST;\r\nif ((!vmci_can_block(flags) || vmci_qp_pinned(flags)) &&\r\nVMCI_ROUTE_AS_GUEST != route) {\r\npr_devel("Not guest personality w/ NONBLOCK OR PINNED set");\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\n}\r\nif (vmci_qp_pinned(flags)) {\r\nif (vmci_can_block(flags)) {\r\npr_err("Attempted to enable pinning w/o non-blocking");\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\n}\r\nif (produce_qsize + consume_qsize > VMCI_MAX_PINNED_QP_MEMORY)\r\nreturn VMCI_ERROR_NO_RESOURCES;\r\n}\r\nmy_qpair = kzalloc(sizeof(*my_qpair), GFP_KERNEL);\r\nif (!my_qpair)\r\nreturn VMCI_ERROR_NO_MEM;\r\nmy_qpair->produce_q_size = produce_qsize;\r\nmy_qpair->consume_q_size = consume_qsize;\r\nmy_qpair->peer = peer;\r\nmy_qpair->flags = flags;\r\nmy_qpair->priv_flags = priv_flags;\r\nwakeup_cb = NULL;\r\nclient_data = NULL;\r\nif (VMCI_ROUTE_AS_HOST == route) {\r\nmy_qpair->guest_endpoint = false;\r\nif (!(flags & VMCI_QPFLAG_LOCAL)) {\r\nmy_qpair->blocked = 0;\r\nmy_qpair->generation = 0;\r\ninit_waitqueue_head(&my_qpair->event);\r\nwakeup_cb = qp_wakeup_cb;\r\nclient_data = (void *)my_qpair;\r\n}\r\n} else {\r\nmy_qpair->guest_endpoint = true;\r\n}\r\nretval = vmci_qp_alloc(handle,\r\n&my_qpair->produce_q,\r\nmy_qpair->produce_q_size,\r\n&my_qpair->consume_q,\r\nmy_qpair->consume_q_size,\r\nmy_qpair->peer,\r\nmy_qpair->flags,\r\nmy_qpair->priv_flags,\r\nmy_qpair->guest_endpoint,\r\nwakeup_cb, client_data);\r\nif (retval < VMCI_SUCCESS) {\r\nkfree(my_qpair);\r\nreturn retval;\r\n}\r\n*qpair = my_qpair;\r\nmy_qpair->handle = *handle;\r\nreturn retval;\r\n}\r\nint vmci_qpair_detach(struct vmci_qp **qpair)\r\n{\r\nint result;\r\nstruct vmci_qp *old_qpair;\r\nif (!qpair || !(*qpair))\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nold_qpair = *qpair;\r\nresult = qp_detatch(old_qpair->handle, old_qpair->guest_endpoint);\r\nmemset(old_qpair, 0, sizeof(*old_qpair));\r\nold_qpair->handle = VMCI_INVALID_HANDLE;\r\nold_qpair->peer = VMCI_INVALID_ID;\r\nkfree(old_qpair);\r\n*qpair = NULL;\r\nreturn result;\r\n}\r\nint vmci_qpair_get_produce_indexes(const struct vmci_qp *qpair,\r\nu64 *producer_tail,\r\nu64 *consumer_head)\r\n{\r\nstruct vmci_queue_header *produce_q_header;\r\nstruct vmci_queue_header *consume_q_header;\r\nint result;\r\nif (!qpair)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\nresult =\r\nqp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\r\nif (result == VMCI_SUCCESS)\r\nvmci_q_header_get_pointers(produce_q_header, consume_q_header,\r\nproducer_tail, consumer_head);\r\nqp_unlock(qpair);\r\nif (result == VMCI_SUCCESS &&\r\n((producer_tail && *producer_tail >= qpair->produce_q_size) ||\r\n(consumer_head && *consumer_head >= qpair->produce_q_size)))\r\nreturn VMCI_ERROR_INVALID_SIZE;\r\nreturn result;\r\n}\r\nint vmci_qpair_get_consume_indexes(const struct vmci_qp *qpair,\r\nu64 *consumer_tail,\r\nu64 *producer_head)\r\n{\r\nstruct vmci_queue_header *produce_q_header;\r\nstruct vmci_queue_header *consume_q_header;\r\nint result;\r\nif (!qpair)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\nresult =\r\nqp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\r\nif (result == VMCI_SUCCESS)\r\nvmci_q_header_get_pointers(consume_q_header, produce_q_header,\r\nconsumer_tail, producer_head);\r\nqp_unlock(qpair);\r\nif (result == VMCI_SUCCESS &&\r\n((consumer_tail && *consumer_tail >= qpair->consume_q_size) ||\r\n(producer_head && *producer_head >= qpair->consume_q_size)))\r\nreturn VMCI_ERROR_INVALID_SIZE;\r\nreturn result;\r\n}\r\ns64 vmci_qpair_produce_free_space(const struct vmci_qp *qpair)\r\n{\r\nstruct vmci_queue_header *produce_q_header;\r\nstruct vmci_queue_header *consume_q_header;\r\ns64 result;\r\nif (!qpair)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\nresult =\r\nqp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\r\nif (result == VMCI_SUCCESS)\r\nresult = vmci_q_header_free_space(produce_q_header,\r\nconsume_q_header,\r\nqpair->produce_q_size);\r\nelse\r\nresult = 0;\r\nqp_unlock(qpair);\r\nreturn result;\r\n}\r\ns64 vmci_qpair_consume_free_space(const struct vmci_qp *qpair)\r\n{\r\nstruct vmci_queue_header *produce_q_header;\r\nstruct vmci_queue_header *consume_q_header;\r\ns64 result;\r\nif (!qpair)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\nresult =\r\nqp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\r\nif (result == VMCI_SUCCESS)\r\nresult = vmci_q_header_free_space(consume_q_header,\r\nproduce_q_header,\r\nqpair->consume_q_size);\r\nelse\r\nresult = 0;\r\nqp_unlock(qpair);\r\nreturn result;\r\n}\r\ns64 vmci_qpair_produce_buf_ready(const struct vmci_qp *qpair)\r\n{\r\nstruct vmci_queue_header *produce_q_header;\r\nstruct vmci_queue_header *consume_q_header;\r\ns64 result;\r\nif (!qpair)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\nresult =\r\nqp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\r\nif (result == VMCI_SUCCESS)\r\nresult = vmci_q_header_buf_ready(produce_q_header,\r\nconsume_q_header,\r\nqpair->produce_q_size);\r\nelse\r\nresult = 0;\r\nqp_unlock(qpair);\r\nreturn result;\r\n}\r\ns64 vmci_qpair_consume_buf_ready(const struct vmci_qp *qpair)\r\n{\r\nstruct vmci_queue_header *produce_q_header;\r\nstruct vmci_queue_header *consume_q_header;\r\ns64 result;\r\nif (!qpair)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\nresult =\r\nqp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\r\nif (result == VMCI_SUCCESS)\r\nresult = vmci_q_header_buf_ready(consume_q_header,\r\nproduce_q_header,\r\nqpair->consume_q_size);\r\nelse\r\nresult = 0;\r\nqp_unlock(qpair);\r\nreturn result;\r\n}\r\nssize_t vmci_qpair_enqueue(struct vmci_qp *qpair,\r\nconst void *buf,\r\nsize_t buf_size,\r\nint buf_type)\r\n{\r\nssize_t result;\r\nif (!qpair || !buf)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\ndo {\r\nresult = qp_enqueue_locked(qpair->produce_q,\r\nqpair->consume_q,\r\nqpair->produce_q_size,\r\nbuf, buf_size,\r\nqp_memcpy_to_queue,\r\nvmci_can_block(qpair->flags));\r\nif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\r\n!qp_wait_for_ready_queue(qpair))\r\nresult = VMCI_ERROR_WOULD_BLOCK;\r\n} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\r\nqp_unlock(qpair);\r\nreturn result;\r\n}\r\nssize_t vmci_qpair_dequeue(struct vmci_qp *qpair,\r\nvoid *buf,\r\nsize_t buf_size,\r\nint buf_type)\r\n{\r\nssize_t result;\r\nif (!qpair || !buf)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\ndo {\r\nresult = qp_dequeue_locked(qpair->produce_q,\r\nqpair->consume_q,\r\nqpair->consume_q_size,\r\nbuf, buf_size,\r\nqp_memcpy_from_queue, true,\r\nvmci_can_block(qpair->flags));\r\nif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\r\n!qp_wait_for_ready_queue(qpair))\r\nresult = VMCI_ERROR_WOULD_BLOCK;\r\n} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\r\nqp_unlock(qpair);\r\nreturn result;\r\n}\r\nssize_t vmci_qpair_peek(struct vmci_qp *qpair,\r\nvoid *buf,\r\nsize_t buf_size,\r\nint buf_type)\r\n{\r\nssize_t result;\r\nif (!qpair || !buf)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\ndo {\r\nresult = qp_dequeue_locked(qpair->produce_q,\r\nqpair->consume_q,\r\nqpair->consume_q_size,\r\nbuf, buf_size,\r\nqp_memcpy_from_queue, false,\r\nvmci_can_block(qpair->flags));\r\nif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\r\n!qp_wait_for_ready_queue(qpair))\r\nresult = VMCI_ERROR_WOULD_BLOCK;\r\n} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\r\nqp_unlock(qpair);\r\nreturn result;\r\n}\r\nssize_t vmci_qpair_enquev(struct vmci_qp *qpair,\r\nvoid *iov,\r\nsize_t iov_size,\r\nint buf_type)\r\n{\r\nssize_t result;\r\nif (!qpair || !iov)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\ndo {\r\nresult = qp_enqueue_locked(qpair->produce_q,\r\nqpair->consume_q,\r\nqpair->produce_q_size,\r\niov, iov_size,\r\nqp_memcpy_to_queue_iov,\r\nvmci_can_block(qpair->flags));\r\nif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\r\n!qp_wait_for_ready_queue(qpair))\r\nresult = VMCI_ERROR_WOULD_BLOCK;\r\n} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\r\nqp_unlock(qpair);\r\nreturn result;\r\n}\r\nssize_t vmci_qpair_dequev(struct vmci_qp *qpair,\r\nvoid *iov,\r\nsize_t iov_size,\r\nint buf_type)\r\n{\r\nssize_t result;\r\nif (!qpair || !iov)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\ndo {\r\nresult = qp_dequeue_locked(qpair->produce_q,\r\nqpair->consume_q,\r\nqpair->consume_q_size,\r\niov, iov_size,\r\nqp_memcpy_from_queue_iov,\r\ntrue, vmci_can_block(qpair->flags));\r\nif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\r\n!qp_wait_for_ready_queue(qpair))\r\nresult = VMCI_ERROR_WOULD_BLOCK;\r\n} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\r\nqp_unlock(qpair);\r\nreturn result;\r\n}\r\nssize_t vmci_qpair_peekv(struct vmci_qp *qpair,\r\nvoid *iov,\r\nsize_t iov_size,\r\nint buf_type)\r\n{\r\nssize_t result;\r\nif (!qpair || !iov)\r\nreturn VMCI_ERROR_INVALID_ARGS;\r\nqp_lock(qpair);\r\ndo {\r\nresult = qp_dequeue_locked(qpair->produce_q,\r\nqpair->consume_q,\r\nqpair->consume_q_size,\r\niov, iov_size,\r\nqp_memcpy_from_queue_iov,\r\nfalse, vmci_can_block(qpair->flags));\r\nif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\r\n!qp_wait_for_ready_queue(qpair))\r\nresult = VMCI_ERROR_WOULD_BLOCK;\r\n} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\r\nqp_unlock(qpair);\r\nreturn result;\r\n}
