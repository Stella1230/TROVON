void user_enter(void)\r\n{\r\nunsigned long flags;\r\nif (in_interrupt())\r\nreturn;\r\nWARN_ON_ONCE(!current->mm);\r\nlocal_irq_save(flags);\r\nif (__this_cpu_read(context_tracking.active) &&\r\n__this_cpu_read(context_tracking.state) != IN_USER) {\r\nvtime_user_enter(current);\r\nrcu_user_enter();\r\n__this_cpu_write(context_tracking.state, IN_USER);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nvoid __sched notrace preempt_schedule_context(void)\r\n{\r\nstruct thread_info *ti = current_thread_info();\r\nenum ctx_state prev_ctx;\r\nif (likely(ti->preempt_count || irqs_disabled()))\r\nreturn;\r\npreempt_disable_notrace();\r\nprev_ctx = exception_enter();\r\npreempt_enable_no_resched_notrace();\r\npreempt_schedule();\r\npreempt_disable_notrace();\r\nexception_exit(prev_ctx);\r\npreempt_enable_notrace();\r\n}\r\nvoid user_exit(void)\r\n{\r\nunsigned long flags;\r\nif (in_interrupt())\r\nreturn;\r\nlocal_irq_save(flags);\r\nif (__this_cpu_read(context_tracking.state) == IN_USER) {\r\nrcu_user_exit();\r\nvtime_user_exit(current);\r\n__this_cpu_write(context_tracking.state, IN_KERNEL);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nvoid guest_enter(void)\r\n{\r\nif (vtime_accounting_enabled())\r\nvtime_guest_enter(current);\r\nelse\r\n__guest_enter();\r\n}\r\nvoid guest_exit(void)\r\n{\r\nif (vtime_accounting_enabled())\r\nvtime_guest_exit(current);\r\nelse\r\n__guest_exit();\r\n}\r\nvoid context_tracking_task_switch(struct task_struct *prev,\r\nstruct task_struct *next)\r\n{\r\nif (__this_cpu_read(context_tracking.active)) {\r\nclear_tsk_thread_flag(prev, TIF_NOHZ);\r\nset_tsk_thread_flag(next, TIF_NOHZ);\r\n}\r\n}
