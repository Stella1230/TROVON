static int kvm_mips_reset_vcpu(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nfor_each_possible_cpu(i) {\r\nvcpu->arch.guest_kernel_asid[i] = 0;\r\nvcpu->arch.guest_user_asid[i] = 0;\r\n}\r\nreturn 0;\r\n}\r\ngfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)\r\n{\r\nreturn gfn;\r\n}\r\nint kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu)\r\n{\r\nreturn !!(vcpu->arch.pending_exceptions);\r\n}\r\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)\r\n{\r\nreturn 1;\r\n}\r\nint kvm_arch_hardware_enable(void *garbage)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_hardware_disable(void *garbage)\r\n{\r\n}\r\nint kvm_arch_hardware_setup(void)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_hardware_unsetup(void)\r\n{\r\n}\r\nvoid kvm_arch_check_processor_compat(void *rtn)\r\n{\r\nint *r = (int *)rtn;\r\n*r = 0;\r\nreturn;\r\n}\r\nstatic void kvm_mips_init_tlbs(struct kvm *kvm)\r\n{\r\nunsigned long wired;\r\nwired = read_c0_wired();\r\nwrite_c0_wired(wired + 1);\r\nmtc0_tlbw_hazard();\r\nkvm->arch.commpage_tlb = wired;\r\nkvm_debug("[%d] commpage TLB: %d\n", smp_processor_id(),\r\nkvm->arch.commpage_tlb);\r\n}\r\nstatic void kvm_mips_init_vm_percpu(void *arg)\r\n{\r\nstruct kvm *kvm = (struct kvm *)arg;\r\nkvm_mips_init_tlbs(kvm);\r\nkvm_mips_callbacks->vm_init(kvm);\r\n}\r\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type)\r\n{\r\nif (atomic_inc_return(&kvm_mips_instance) == 1) {\r\nkvm_info("%s: 1st KVM instance, setup host TLB parameters\n",\r\n__func__);\r\non_each_cpu(kvm_mips_init_vm_percpu, kvm, 1);\r\n}\r\nreturn 0;\r\n}\r\nvoid kvm_mips_free_vcpus(struct kvm *kvm)\r\n{\r\nunsigned int i;\r\nstruct kvm_vcpu *vcpu;\r\nfor (i = 0; i < kvm->arch.guest_pmap_npages; i++) {\r\nif (kvm->arch.guest_pmap[i] != KVM_INVALID_PAGE)\r\nkvm_mips_release_pfn_clean(kvm->arch.guest_pmap[i]);\r\n}\r\nif (kvm->arch.guest_pmap)\r\nkfree(kvm->arch.guest_pmap);\r\nkvm_for_each_vcpu(i, vcpu, kvm) {\r\nkvm_arch_vcpu_free(vcpu);\r\n}\r\nmutex_lock(&kvm->lock);\r\nfor (i = 0; i < atomic_read(&kvm->online_vcpus); i++)\r\nkvm->vcpus[i] = NULL;\r\natomic_set(&kvm->online_vcpus, 0);\r\nmutex_unlock(&kvm->lock);\r\n}\r\nvoid kvm_arch_sync_events(struct kvm *kvm)\r\n{\r\n}\r\nstatic void kvm_mips_uninit_tlbs(void *arg)\r\n{\r\nwrite_c0_wired(0);\r\nmtc0_tlbw_hazard();\r\nkvm_local_flush_tlb_all();\r\n}\r\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\r\n{\r\nkvm_mips_free_vcpus(kvm);\r\nif (atomic_dec_return(&kvm_mips_instance) == 0) {\r\nkvm_info("%s: last KVM instance, restoring TLB parameters\n",\r\n__func__);\r\non_each_cpu(kvm_mips_uninit_tlbs, NULL, 1);\r\n}\r\n}\r\nlong\r\nkvm_arch_dev_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nvoid kvm_arch_free_memslot(struct kvm_memory_slot *free,\r\nstruct kvm_memory_slot *dont)\r\n{\r\n}\r\nint kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)\r\n{\r\nreturn 0;\r\n}\r\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot,\r\nstruct kvm_userspace_memory_region *mem,\r\nenum kvm_mr_change change)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\r\nstruct kvm_userspace_memory_region *mem,\r\nconst struct kvm_memory_slot *old,\r\nenum kvm_mr_change change)\r\n{\r\nunsigned long npages = 0;\r\nint i, err = 0;\r\nkvm_debug("%s: kvm: %p slot: %d, GPA: %llx, size: %llx, QVA: %llx\n",\r\n__func__, kvm, mem->slot, mem->guest_phys_addr,\r\nmem->memory_size, mem->userspace_addr);\r\nif (!kvm->arch.guest_pmap) {\r\nif (mem->slot == 0)\r\nnpages = mem->memory_size >> PAGE_SHIFT;\r\nif (npages) {\r\nkvm->arch.guest_pmap_npages = npages;\r\nkvm->arch.guest_pmap =\r\nkzalloc(npages * sizeof(unsigned long), GFP_KERNEL);\r\nif (!kvm->arch.guest_pmap) {\r\nkvm_err("Failed to allocate guest PMAP");\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nkvm_info\r\n("Allocated space for Guest PMAP Table (%ld pages) @ %p\n",\r\nnpages, kvm->arch.guest_pmap);\r\nfor (i = 0; i < npages; i++) {\r\nkvm->arch.guest_pmap[i] = KVM_INVALID_PAGE;\r\n}\r\n}\r\n}\r\nout:\r\nreturn;\r\n}\r\nvoid kvm_arch_flush_shadow_all(struct kvm *kvm)\r\n{\r\n}\r\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\r\nstruct kvm_memory_slot *slot)\r\n{\r\n}\r\nvoid kvm_arch_flush_shadow(struct kvm *kvm)\r\n{\r\n}\r\nstruct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)\r\n{\r\nextern char mips32_exception[], mips32_exceptionEnd[];\r\nextern char mips32_GuestException[], mips32_GuestExceptionEnd[];\r\nint err, size, offset;\r\nvoid *gebase;\r\nint i;\r\nstruct kvm_vcpu *vcpu = kzalloc(sizeof(struct kvm_vcpu), GFP_KERNEL);\r\nif (!vcpu) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nerr = kvm_vcpu_init(vcpu, kvm, id);\r\nif (err)\r\ngoto out_free_cpu;\r\nkvm_info("kvm @ %p: create cpu %d at %p\n", kvm, id, vcpu);\r\nif (cpu_has_veic || cpu_has_vint) {\r\nsize = 0x200 + VECTORSPACING * 64;\r\n} else {\r\nsize = 0x200;\r\n}\r\nvcpu->arch.host_ebase = (void *)read_c0_ebase();\r\ngebase = kzalloc(ALIGN(size, PAGE_SIZE), GFP_KERNEL);\r\nif (!gebase) {\r\nerr = -ENOMEM;\r\ngoto out_free_cpu;\r\n}\r\nkvm_info("Allocated %d bytes for KVM Exception Handlers @ %p\n",\r\nALIGN(size, PAGE_SIZE), gebase);\r\nvcpu->arch.guest_ebase = gebase;\r\nmemcpy(gebase, mips32_exception,\r\nmips32_exceptionEnd - mips32_exception);\r\nmemcpy(gebase + 0x180, mips32_exception,\r\nmips32_exceptionEnd - mips32_exception);\r\nfor (i = 0; i < 8; i++) {\r\nkvm_debug("L1 Vectored handler @ %p\n",\r\ngebase + 0x200 + (i * VECTORSPACING));\r\nmemcpy(gebase + 0x200 + (i * VECTORSPACING), mips32_exception,\r\nmips32_exceptionEnd - mips32_exception);\r\n}\r\noffset = 0x2000;\r\nkvm_info("Installing KVM Exception handlers @ %p, %#x bytes\n",\r\ngebase + offset,\r\nmips32_GuestExceptionEnd - mips32_GuestException);\r\nmemcpy(gebase + offset, mips32_GuestException,\r\nmips32_GuestExceptionEnd - mips32_GuestException);\r\nmips32_SyncICache((unsigned long) gebase, ALIGN(size, PAGE_SIZE));\r\nvcpu->arch.kseg0_commpage = kzalloc(PAGE_SIZE << 1, GFP_KERNEL);\r\nif (!vcpu->arch.kseg0_commpage) {\r\nerr = -ENOMEM;\r\ngoto out_free_gebase;\r\n}\r\nkvm_info("Allocated COMM page @ %p\n", vcpu->arch.kseg0_commpage);\r\nkvm_mips_commpage_init(vcpu);\r\nvcpu->arch.last_sched_cpu = -1;\r\nkvm_mips_emulate_count(vcpu);\r\nreturn vcpu;\r\nout_free_gebase:\r\nkfree(gebase);\r\nout_free_cpu:\r\nkfree(vcpu);\r\nout:\r\nreturn ERR_PTR(err);\r\n}\r\nvoid kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)\r\n{\r\nhrtimer_cancel(&vcpu->arch.comparecount_timer);\r\nkvm_vcpu_uninit(vcpu);\r\nkvm_mips_dump_stats(vcpu);\r\nif (vcpu->arch.guest_ebase)\r\nkfree(vcpu->arch.guest_ebase);\r\nif (vcpu->arch.kseg0_commpage)\r\nkfree(vcpu->arch.kseg0_commpage);\r\n}\r\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nkvm_arch_vcpu_free(vcpu);\r\n}\r\nint\r\nkvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\r\nstruct kvm_guest_debug *dbg)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nint r = 0;\r\nsigset_t sigsaved;\r\nif (vcpu->sigset_active)\r\nsigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);\r\nif (vcpu->mmio_needed) {\r\nif (!vcpu->mmio_is_write)\r\nkvm_mips_complete_mmio_load(vcpu, run);\r\nvcpu->mmio_needed = 0;\r\n}\r\nkvm_mips_deliver_interrupts(vcpu,\r\nkvm_read_c0_guest_cause(vcpu->arch.cop0));\r\nlocal_irq_disable();\r\nkvm_guest_enter();\r\nr = __kvm_mips_vcpu_run(run, vcpu);\r\nkvm_guest_exit();\r\nlocal_irq_enable();\r\nif (vcpu->sigset_active)\r\nsigprocmask(SIG_SETMASK, &sigsaved, NULL);\r\nreturn r;\r\n}\r\nint\r\nkvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_mips_interrupt *irq)\r\n{\r\nint intr = (int)irq->irq;\r\nstruct kvm_vcpu *dvcpu = NULL;\r\nif (intr == 3 || intr == -3 || intr == 4 || intr == -4)\r\nkvm_debug("%s: CPU: %d, INTR: %d\n", __func__, irq->cpu,\r\n(int)intr);\r\nif (irq->cpu == -1)\r\ndvcpu = vcpu;\r\nelse\r\ndvcpu = vcpu->kvm->vcpus[irq->cpu];\r\nif (intr == 2 || intr == 3 || intr == 4) {\r\nkvm_mips_callbacks->queue_io_int(dvcpu, irq);\r\n} else if (intr == -2 || intr == -3 || intr == -4) {\r\nkvm_mips_callbacks->dequeue_io_int(dvcpu, irq);\r\n} else {\r\nkvm_err("%s: invalid interrupt ioctl (%d:%d)\n", __func__,\r\nirq->cpu, irq->irq);\r\nreturn -EINVAL;\r\n}\r\ndvcpu->arch.wait = 0;\r\nif (waitqueue_active(&dvcpu->wq)) {\r\nwake_up_interruptible(&dvcpu->wq);\r\n}\r\nreturn 0;\r\n}\r\nint\r\nkvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\r\nstruct kvm_mp_state *mp_state)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nint\r\nkvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\r\nstruct kvm_mp_state *mp_state)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nstatic int kvm_mips_get_reg(struct kvm_vcpu *vcpu,\r\nconst struct kvm_one_reg *reg)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\ns64 v;\r\nswitch (reg->id) {\r\ncase KVM_REG_MIPS_R0 ... KVM_REG_MIPS_R31:\r\nv = (long)vcpu->arch.gprs[reg->id - KVM_REG_MIPS_R0];\r\nbreak;\r\ncase KVM_REG_MIPS_HI:\r\nv = (long)vcpu->arch.hi;\r\nbreak;\r\ncase KVM_REG_MIPS_LO:\r\nv = (long)vcpu->arch.lo;\r\nbreak;\r\ncase KVM_REG_MIPS_PC:\r\nv = (long)vcpu->arch.pc;\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_INDEX:\r\nv = (long)kvm_read_c0_guest_index(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONTEXT:\r\nv = (long)kvm_read_c0_guest_context(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PAGEMASK:\r\nv = (long)kvm_read_c0_guest_pagemask(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_WIRED:\r\nv = (long)kvm_read_c0_guest_wired(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADVADDR:\r\nv = (long)kvm_read_c0_guest_badvaddr(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYHI:\r\nv = (long)kvm_read_c0_guest_entryhi(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_STATUS:\r\nv = (long)kvm_read_c0_guest_status(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CAUSE:\r\nv = (long)kvm_read_c0_guest_cause(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ERROREPC:\r\nv = (long)kvm_read_c0_guest_errorepc(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG:\r\nv = (long)kvm_read_c0_guest_config(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG1:\r\nv = (long)kvm_read_c0_guest_config1(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG2:\r\nv = (long)kvm_read_c0_guest_config2(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG3:\r\nv = (long)kvm_read_c0_guest_config3(cop0);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONFIG7:\r\nv = (long)kvm_read_c0_guest_config7(cop0);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif ((reg->id & KVM_REG_SIZE_MASK) == KVM_REG_SIZE_U64) {\r\nu64 __user *uaddr64 = (u64 __user *)(long)reg->addr;\r\nreturn put_user(v, uaddr64);\r\n} else if ((reg->id & KVM_REG_SIZE_MASK) == KVM_REG_SIZE_U32) {\r\nu32 __user *uaddr32 = (u32 __user *)(long)reg->addr;\r\nu32 v32 = (u32)v;\r\nreturn put_user(v32, uaddr32);\r\n} else {\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic int kvm_mips_set_reg(struct kvm_vcpu *vcpu,\r\nconst struct kvm_one_reg *reg)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nu64 v;\r\nif ((reg->id & KVM_REG_SIZE_MASK) == KVM_REG_SIZE_U64) {\r\nu64 __user *uaddr64 = (u64 __user *)(long)reg->addr;\r\nif (get_user(v, uaddr64) != 0)\r\nreturn -EFAULT;\r\n} else if ((reg->id & KVM_REG_SIZE_MASK) == KVM_REG_SIZE_U32) {\r\nu32 __user *uaddr32 = (u32 __user *)(long)reg->addr;\r\ns32 v32;\r\nif (get_user(v32, uaddr32) != 0)\r\nreturn -EFAULT;\r\nv = (s64)v32;\r\n} else {\r\nreturn -EINVAL;\r\n}\r\nswitch (reg->id) {\r\ncase KVM_REG_MIPS_R0:\r\nbreak;\r\ncase KVM_REG_MIPS_R1 ... KVM_REG_MIPS_R31:\r\nvcpu->arch.gprs[reg->id - KVM_REG_MIPS_R0] = v;\r\nbreak;\r\ncase KVM_REG_MIPS_HI:\r\nvcpu->arch.hi = v;\r\nbreak;\r\ncase KVM_REG_MIPS_LO:\r\nvcpu->arch.lo = v;\r\nbreak;\r\ncase KVM_REG_MIPS_PC:\r\nvcpu->arch.pc = v;\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_INDEX:\r\nkvm_write_c0_guest_index(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CONTEXT:\r\nkvm_write_c0_guest_context(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_PAGEMASK:\r\nkvm_write_c0_guest_pagemask(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_WIRED:\r\nkvm_write_c0_guest_wired(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_BADVADDR:\r\nkvm_write_c0_guest_badvaddr(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ENTRYHI:\r\nkvm_write_c0_guest_entryhi(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_STATUS:\r\nkvm_write_c0_guest_status(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_CAUSE:\r\nkvm_write_c0_guest_cause(cop0, v);\r\nbreak;\r\ncase KVM_REG_MIPS_CP0_ERROREPC:\r\nkvm_write_c0_guest_errorepc(cop0, v);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nlong\r\nkvm_arch_vcpu_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)\r\n{\r\nstruct kvm_vcpu *vcpu = filp->private_data;\r\nvoid __user *argp = (void __user *)arg;\r\nlong r;\r\nswitch (ioctl) {\r\ncase KVM_SET_ONE_REG:\r\ncase KVM_GET_ONE_REG: {\r\nstruct kvm_one_reg reg;\r\nif (copy_from_user(&reg, argp, sizeof(reg)))\r\nreturn -EFAULT;\r\nif (ioctl == KVM_SET_ONE_REG)\r\nreturn kvm_mips_set_reg(vcpu, &reg);\r\nelse\r\nreturn kvm_mips_get_reg(vcpu, &reg);\r\n}\r\ncase KVM_GET_REG_LIST: {\r\nstruct kvm_reg_list __user *user_list = argp;\r\nu64 __user *reg_dest;\r\nstruct kvm_reg_list reg_list;\r\nunsigned n;\r\nif (copy_from_user(&reg_list, user_list, sizeof(reg_list)))\r\nreturn -EFAULT;\r\nn = reg_list.n;\r\nreg_list.n = ARRAY_SIZE(kvm_mips_get_one_regs);\r\nif (copy_to_user(user_list, &reg_list, sizeof(reg_list)))\r\nreturn -EFAULT;\r\nif (n < reg_list.n)\r\nreturn -E2BIG;\r\nreg_dest = user_list->reg;\r\nif (copy_to_user(reg_dest, kvm_mips_get_one_regs,\r\nsizeof(kvm_mips_get_one_regs)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\ncase KVM_NMI:\r\nr = kvm_mips_reset_vcpu(vcpu);\r\nbreak;\r\ncase KVM_INTERRUPT:\r\n{\r\nstruct kvm_mips_interrupt irq;\r\nr = -EFAULT;\r\nif (copy_from_user(&irq, argp, sizeof(irq)))\r\ngoto out;\r\nkvm_debug("[%d] %s: irq: %d\n", vcpu->vcpu_id, __func__,\r\nirq.irq);\r\nr = kvm_vcpu_ioctl_interrupt(vcpu, &irq);\r\nbreak;\r\n}\r\ndefault:\r\nr = -ENOIOCTLCMD;\r\n}\r\nout:\r\nreturn r;\r\n}\r\nint kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)\r\n{\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long ga, ga_end;\r\nint is_dirty = 0;\r\nint r;\r\nunsigned long n;\r\nmutex_lock(&kvm->slots_lock);\r\nr = kvm_get_dirty_log(kvm, log, &is_dirty);\r\nif (r)\r\ngoto out;\r\nif (is_dirty) {\r\nmemslot = &kvm->memslots->memslots[log->slot];\r\nga = memslot->base_gfn << PAGE_SHIFT;\r\nga_end = ga + (memslot->npages << PAGE_SHIFT);\r\nprintk("%s: dirty, ga: %#lx, ga_end %#lx\n", __func__, ga,\r\nga_end);\r\nn = kvm_dirty_bitmap_bytes(memslot);\r\nmemset(memslot->dirty_bitmap, 0, n);\r\n}\r\nr = 0;\r\nout:\r\nmutex_unlock(&kvm->slots_lock);\r\nreturn r;\r\n}\r\nlong kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)\r\n{\r\nlong r;\r\nswitch (ioctl) {\r\ndefault:\r\nr = -ENOIOCTLCMD;\r\n}\r\nreturn r;\r\n}\r\nint kvm_arch_init(void *opaque)\r\n{\r\nint ret;\r\nif (kvm_mips_callbacks) {\r\nkvm_err("kvm: module already exists\n");\r\nreturn -EEXIST;\r\n}\r\nret = kvm_mips_emulation_init(&kvm_mips_callbacks);\r\nreturn ret;\r\n}\r\nvoid kvm_arch_exit(void)\r\n{\r\nkvm_mips_callbacks = NULL;\r\n}\r\nint\r\nkvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nint\r\nkvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nint kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)\r\n{\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nint kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)\r\n{\r\nreturn VM_FAULT_SIGBUS;\r\n}\r\nint kvm_dev_ioctl_check_extension(long ext)\r\n{\r\nint r;\r\nswitch (ext) {\r\ncase KVM_CAP_ONE_REG:\r\nr = 1;\r\nbreak;\r\ncase KVM_CAP_COALESCED_MMIO:\r\nr = KVM_COALESCED_MMIO_PAGE_OFFSET;\r\nbreak;\r\ndefault:\r\nr = 0;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nint kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvm_mips_pending_timer(vcpu);\r\n}\r\nint kvm_arch_vcpu_dump_regs(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nstruct mips_coproc *cop0;\r\nif (!vcpu)\r\nreturn -1;\r\nprintk("VCPU Register Dump:\n");\r\nprintk("\tpc = 0x%08lx\n", vcpu->arch.pc);;\r\nprintk("\texceptions: %08lx\n", vcpu->arch.pending_exceptions);\r\nfor (i = 0; i < 32; i += 4) {\r\nprintk("\tgpr%02d: %08lx %08lx %08lx %08lx\n", i,\r\nvcpu->arch.gprs[i],\r\nvcpu->arch.gprs[i + 1],\r\nvcpu->arch.gprs[i + 2], vcpu->arch.gprs[i + 3]);\r\n}\r\nprintk("\thi: 0x%08lx\n", vcpu->arch.hi);\r\nprintk("\tlo: 0x%08lx\n", vcpu->arch.lo);\r\ncop0 = vcpu->arch.cop0;\r\nprintk("\tStatus: 0x%08lx, Cause: 0x%08lx\n",\r\nkvm_read_c0_guest_status(cop0), kvm_read_c0_guest_cause(cop0));\r\nprintk("\tEPC: 0x%08lx\n", kvm_read_c0_guest_epc(cop0));\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\r\n{\r\nint i;\r\nfor (i = 1; i < ARRAY_SIZE(vcpu->arch.gprs); i++)\r\nvcpu->arch.gprs[i] = regs->gpr[i];\r\nvcpu->arch.gprs[0] = 0;\r\nvcpu->arch.hi = regs->hi;\r\nvcpu->arch.lo = regs->lo;\r\nvcpu->arch.pc = regs->pc;\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(vcpu->arch.gprs); i++)\r\nregs->gpr[i] = vcpu->arch.gprs[i];\r\nregs->hi = vcpu->arch.hi;\r\nregs->lo = vcpu->arch.lo;\r\nregs->pc = vcpu->arch.pc;\r\nreturn 0;\r\n}\r\nvoid kvm_mips_comparecount_func(unsigned long data)\r\n{\r\nstruct kvm_vcpu *vcpu = (struct kvm_vcpu *)data;\r\nkvm_mips_callbacks->queue_timer_int(vcpu);\r\nvcpu->arch.wait = 0;\r\nif (waitqueue_active(&vcpu->wq)) {\r\nwake_up_interruptible(&vcpu->wq);\r\n}\r\n}\r\nenum hrtimer_restart kvm_mips_comparecount_wakeup(struct hrtimer *timer)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nvcpu = container_of(timer, struct kvm_vcpu, arch.comparecount_timer);\r\nkvm_mips_comparecount_func((unsigned long) vcpu);\r\nhrtimer_forward_now(&vcpu->arch.comparecount_timer,\r\nktime_set(0, MS_TO_NS(10)));\r\nreturn HRTIMER_RESTART;\r\n}\r\nint kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)\r\n{\r\nkvm_mips_callbacks->vcpu_init(vcpu);\r\nhrtimer_init(&vcpu->arch.comparecount_timer, CLOCK_MONOTONIC,\r\nHRTIMER_MODE_REL);\r\nvcpu->arch.comparecount_timer.function = kvm_mips_comparecount_wakeup;\r\nkvm_mips_init_shadow_tlb(vcpu);\r\nreturn 0;\r\n}\r\nvoid kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)\r\n{\r\nreturn;\r\n}\r\nint\r\nkvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu, struct kvm_translation *tr)\r\n{\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvm_mips_callbacks->vcpu_setup(vcpu);\r\n}\r\nstatic\r\nvoid kvm_mips_set_c0_status(void)\r\n{\r\nuint32_t status = read_c0_status();\r\nif (cpu_has_fpu)\r\nstatus |= (ST0_CU1);\r\nif (cpu_has_dsp)\r\nstatus |= (ST0_MX);\r\nwrite_c0_status(status);\r\nehb();\r\n}\r\nint kvm_mips_handle_exit(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nuint32_t cause = vcpu->arch.host_cp0_cause;\r\nuint32_t exccode = (cause >> CAUSEB_EXCCODE) & 0x1f;\r\nuint32_t __user *opc = (uint32_t __user *) vcpu->arch.pc;\r\nunsigned long badvaddr = vcpu->arch.host_cp0_badvaddr;\r\nenum emulation_result er = EMULATE_DONE;\r\nint ret = RESUME_GUEST;\r\nrun->exit_reason = KVM_EXIT_UNKNOWN;\r\nrun->ready_for_interrupt_injection = 1;\r\nkvm_mips_set_c0_status();\r\nlocal_irq_enable();\r\nkvm_debug("kvm_mips_handle_exit: cause: %#x, PC: %p, kvm_run: %p, kvm_vcpu: %p\n",\r\ncause, opc, run, vcpu);\r\ner = kvm_mips_check_privilege(cause, opc, run, vcpu);\r\nif (er == EMULATE_PRIV_FAIL) {\r\ngoto skip_emul;\r\n} else if (er == EMULATE_FAIL) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\ngoto skip_emul;\r\n}\r\nswitch (exccode) {\r\ncase T_INT:\r\nkvm_debug("[%d]T_INT @ %p\n", vcpu->vcpu_id, opc);\r\n++vcpu->stat.int_exits;\r\ntrace_kvm_exit(vcpu, INT_EXITS);\r\nif (need_resched()) {\r\ncond_resched();\r\n}\r\nret = RESUME_GUEST;\r\nbreak;\r\ncase T_COP_UNUSABLE:\r\nkvm_debug("T_COP_UNUSABLE: @ PC: %p\n", opc);\r\n++vcpu->stat.cop_unusable_exits;\r\ntrace_kvm_exit(vcpu, COP_UNUSABLE_EXITS);\r\nret = kvm_mips_callbacks->handle_cop_unusable(vcpu);\r\nif (run->exit_reason == KVM_EXIT_IRQ_WINDOW_OPEN) {\r\nret = RESUME_HOST;\r\n}\r\nbreak;\r\ncase T_TLB_MOD:\r\n++vcpu->stat.tlbmod_exits;\r\ntrace_kvm_exit(vcpu, TLBMOD_EXITS);\r\nret = kvm_mips_callbacks->handle_tlb_mod(vcpu);\r\nbreak;\r\ncase T_TLB_ST_MISS:\r\nkvm_debug\r\n("TLB ST fault: cause %#x, status %#lx, PC: %p, BadVaddr: %#lx\n",\r\ncause, kvm_read_c0_guest_status(vcpu->arch.cop0), opc,\r\nbadvaddr);\r\n++vcpu->stat.tlbmiss_st_exits;\r\ntrace_kvm_exit(vcpu, TLBMISS_ST_EXITS);\r\nret = kvm_mips_callbacks->handle_tlb_st_miss(vcpu);\r\nbreak;\r\ncase T_TLB_LD_MISS:\r\nkvm_debug("TLB LD fault: cause %#x, PC: %p, BadVaddr: %#lx\n",\r\ncause, opc, badvaddr);\r\n++vcpu->stat.tlbmiss_ld_exits;\r\ntrace_kvm_exit(vcpu, TLBMISS_LD_EXITS);\r\nret = kvm_mips_callbacks->handle_tlb_ld_miss(vcpu);\r\nbreak;\r\ncase T_ADDR_ERR_ST:\r\n++vcpu->stat.addrerr_st_exits;\r\ntrace_kvm_exit(vcpu, ADDRERR_ST_EXITS);\r\nret = kvm_mips_callbacks->handle_addr_err_st(vcpu);\r\nbreak;\r\ncase T_ADDR_ERR_LD:\r\n++vcpu->stat.addrerr_ld_exits;\r\ntrace_kvm_exit(vcpu, ADDRERR_LD_EXITS);\r\nret = kvm_mips_callbacks->handle_addr_err_ld(vcpu);\r\nbreak;\r\ncase T_SYSCALL:\r\n++vcpu->stat.syscall_exits;\r\ntrace_kvm_exit(vcpu, SYSCALL_EXITS);\r\nret = kvm_mips_callbacks->handle_syscall(vcpu);\r\nbreak;\r\ncase T_RES_INST:\r\n++vcpu->stat.resvd_inst_exits;\r\ntrace_kvm_exit(vcpu, RESVD_INST_EXITS);\r\nret = kvm_mips_callbacks->handle_res_inst(vcpu);\r\nbreak;\r\ncase T_BREAK:\r\n++vcpu->stat.break_inst_exits;\r\ntrace_kvm_exit(vcpu, BREAK_INST_EXITS);\r\nret = kvm_mips_callbacks->handle_break(vcpu);\r\nbreak;\r\ndefault:\r\nkvm_err\r\n("Exception Code: %d, not yet handled, @ PC: %p, inst: 0x%08x BadVaddr: %#lx Status: %#lx\n",\r\nexccode, opc, kvm_get_inst(opc, vcpu), badvaddr,\r\nkvm_read_c0_guest_status(vcpu->arch.cop0));\r\nkvm_arch_vcpu_dump_regs(vcpu);\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nret = RESUME_HOST;\r\nbreak;\r\n}\r\nskip_emul:\r\nlocal_irq_disable();\r\nif (er == EMULATE_DONE && !(ret & RESUME_HOST))\r\nkvm_mips_deliver_interrupts(vcpu, cause);\r\nif (!(ret & RESUME_HOST)) {\r\nif (signal_pending(current)) {\r\nrun->exit_reason = KVM_EXIT_INTR;\r\nret = (-EINTR << 2) | RESUME_HOST;\r\n++vcpu->stat.signal_exits;\r\ntrace_kvm_exit(vcpu, SIGNAL_EXITS);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nint __init kvm_mips_init(void)\r\n{\r\nint ret;\r\nret = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);\r\nif (ret)\r\nreturn ret;\r\nkvm_mips_gfn_to_pfn = gfn_to_pfn;\r\nkvm_mips_release_pfn_clean = kvm_release_pfn_clean;\r\nkvm_mips_is_error_pfn = is_error_pfn;\r\npr_info("KVM/MIPS Initialized\n");\r\nreturn 0;\r\n}\r\nvoid __exit kvm_mips_exit(void)\r\n{\r\nkvm_exit();\r\nkvm_mips_gfn_to_pfn = NULL;\r\nkvm_mips_release_pfn_clean = NULL;\r\nkvm_mips_is_error_pfn = NULL;\r\npr_info("KVM/MIPS unloaded\n");\r\n}
