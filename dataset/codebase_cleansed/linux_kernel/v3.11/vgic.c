static u32 *vgic_bitmap_get_reg(struct vgic_bitmap *x,\r\nint cpuid, u32 offset)\r\n{\r\noffset >>= 2;\r\nif (!offset)\r\nreturn x->percpu[cpuid].reg;\r\nelse\r\nreturn x->shared.reg + offset - 1;\r\n}\r\nstatic int vgic_bitmap_get_irq_val(struct vgic_bitmap *x,\r\nint cpuid, int irq)\r\n{\r\nif (irq < VGIC_NR_PRIVATE_IRQS)\r\nreturn test_bit(irq, x->percpu[cpuid].reg_ul);\r\nreturn test_bit(irq - VGIC_NR_PRIVATE_IRQS, x->shared.reg_ul);\r\n}\r\nstatic void vgic_bitmap_set_irq_val(struct vgic_bitmap *x, int cpuid,\r\nint irq, int val)\r\n{\r\nunsigned long *reg;\r\nif (irq < VGIC_NR_PRIVATE_IRQS) {\r\nreg = x->percpu[cpuid].reg_ul;\r\n} else {\r\nreg = x->shared.reg_ul;\r\nirq -= VGIC_NR_PRIVATE_IRQS;\r\n}\r\nif (val)\r\nset_bit(irq, reg);\r\nelse\r\nclear_bit(irq, reg);\r\n}\r\nstatic unsigned long *vgic_bitmap_get_cpu_map(struct vgic_bitmap *x, int cpuid)\r\n{\r\nif (unlikely(cpuid >= VGIC_MAX_CPUS))\r\nreturn NULL;\r\nreturn x->percpu[cpuid].reg_ul;\r\n}\r\nstatic unsigned long *vgic_bitmap_get_shared_map(struct vgic_bitmap *x)\r\n{\r\nreturn x->shared.reg_ul;\r\n}\r\nstatic u32 *vgic_bytemap_get_reg(struct vgic_bytemap *x, int cpuid, u32 offset)\r\n{\r\noffset >>= 2;\r\nBUG_ON(offset > (VGIC_NR_IRQS / 4));\r\nif (offset < 4)\r\nreturn x->percpu[cpuid] + offset;\r\nelse\r\nreturn x->shared + offset - 8;\r\n}\r\nstatic bool vgic_irq_is_edge(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nint irq_val;\r\nirq_val = vgic_bitmap_get_irq_val(&dist->irq_cfg, vcpu->vcpu_id, irq);\r\nreturn irq_val == VGIC_CFG_EDGE;\r\n}\r\nstatic int vgic_irq_is_enabled(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn vgic_bitmap_get_irq_val(&dist->irq_enabled, vcpu->vcpu_id, irq);\r\n}\r\nstatic int vgic_irq_is_active(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn vgic_bitmap_get_irq_val(&dist->irq_active, vcpu->vcpu_id, irq);\r\n}\r\nstatic void vgic_irq_set_active(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_active, vcpu->vcpu_id, irq, 1);\r\n}\r\nstatic void vgic_irq_clear_active(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_active, vcpu->vcpu_id, irq, 0);\r\n}\r\nstatic int vgic_dist_irq_is_pending(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nreturn vgic_bitmap_get_irq_val(&dist->irq_state, vcpu->vcpu_id, irq);\r\n}\r\nstatic void vgic_dist_irq_set(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_state, vcpu->vcpu_id, irq, 1);\r\n}\r\nstatic void vgic_dist_irq_clear(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nvgic_bitmap_set_irq_val(&dist->irq_state, vcpu->vcpu_id, irq, 0);\r\n}\r\nstatic void vgic_cpu_irq_set(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nif (irq < VGIC_NR_PRIVATE_IRQS)\r\nset_bit(irq, vcpu->arch.vgic_cpu.pending_percpu);\r\nelse\r\nset_bit(irq - VGIC_NR_PRIVATE_IRQS,\r\nvcpu->arch.vgic_cpu.pending_shared);\r\n}\r\nstatic void vgic_cpu_irq_clear(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nif (irq < VGIC_NR_PRIVATE_IRQS)\r\nclear_bit(irq, vcpu->arch.vgic_cpu.pending_percpu);\r\nelse\r\nclear_bit(irq - VGIC_NR_PRIVATE_IRQS,\r\nvcpu->arch.vgic_cpu.pending_shared);\r\n}\r\nstatic u32 mmio_data_read(struct kvm_exit_mmio *mmio, u32 mask)\r\n{\r\nreturn *((u32 *)mmio->data) & mask;\r\n}\r\nstatic void mmio_data_write(struct kvm_exit_mmio *mmio, u32 mask, u32 value)\r\n{\r\n*((u32 *)mmio->data) = value & mask;\r\n}\r\nstatic void vgic_reg_access(struct kvm_exit_mmio *mmio, u32 *reg,\r\nphys_addr_t offset, int mode)\r\n{\r\nint word_offset = (offset & 3) * 8;\r\nu32 mask = (1UL << (mmio->len * 8)) - 1;\r\nu32 regval;\r\nif (reg) {\r\nregval = *reg;\r\n} else {\r\nBUG_ON(mode != (ACCESS_READ_RAZ | ACCESS_WRITE_IGNORED));\r\nregval = 0;\r\n}\r\nif (mmio->is_write) {\r\nu32 data = mmio_data_read(mmio, mask) << word_offset;\r\nswitch (ACCESS_WRITE_MASK(mode)) {\r\ncase ACCESS_WRITE_IGNORED:\r\nreturn;\r\ncase ACCESS_WRITE_SETBIT:\r\nregval |= data;\r\nbreak;\r\ncase ACCESS_WRITE_CLEARBIT:\r\nregval &= ~data;\r\nbreak;\r\ncase ACCESS_WRITE_VALUE:\r\nregval = (regval & ~(mask << word_offset)) | data;\r\nbreak;\r\n}\r\n*reg = regval;\r\n} else {\r\nswitch (ACCESS_READ_MASK(mode)) {\r\ncase ACCESS_READ_RAZ:\r\nregval = 0;\r\ncase ACCESS_READ_VALUE:\r\nmmio_data_write(mmio, mask, regval >> word_offset);\r\n}\r\n}\r\n}\r\nstatic bool handle_mmio_misc(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 reg;\r\nu32 word_offset = offset & 3;\r\nswitch (offset & ~3) {\r\ncase 0:\r\nreg = vcpu->kvm->arch.vgic.enabled;\r\nvgic_reg_access(mmio, &reg, word_offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nif (mmio->is_write) {\r\nvcpu->kvm->arch.vgic.enabled = reg & 1;\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nbreak;\r\ncase 4:\r\nreg = (atomic_read(&vcpu->kvm->online_vcpus) - 1) << 5;\r\nreg |= (VGIC_NR_IRQS >> 5) - 1;\r\nvgic_reg_access(mmio, &reg, word_offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nbreak;\r\ncase 8:\r\nreg = 0x4B00043B;\r\nvgic_reg_access(mmio, &reg, word_offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nbreak;\r\n}\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_raz_wi(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_set_enable_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 *reg = vgic_bitmap_get_reg(&vcpu->kvm->arch.vgic.irq_enabled,\r\nvcpu->vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_SETBIT);\r\nif (mmio->is_write) {\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_clear_enable_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 *reg = vgic_bitmap_get_reg(&vcpu->kvm->arch.vgic.irq_enabled,\r\nvcpu->vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_CLEARBIT);\r\nif (mmio->is_write) {\r\nif (offset < 4)\r\n*reg |= 0xffff;\r\nvgic_retire_disabled_irqs(vcpu);\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_set_pending_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 *reg = vgic_bitmap_get_reg(&vcpu->kvm->arch.vgic.irq_state,\r\nvcpu->vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_SETBIT);\r\nif (mmio->is_write) {\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_clear_pending_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 *reg = vgic_bitmap_get_reg(&vcpu->kvm->arch.vgic.irq_state,\r\nvcpu->vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_CLEARBIT);\r\nif (mmio->is_write) {\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_priority_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 *reg = vgic_bytemap_get_reg(&vcpu->kvm->arch.vgic.irq_priority,\r\nvcpu->vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nreturn false;\r\n}\r\nstatic u32 vgic_get_target_reg(struct kvm *kvm, int irq)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct kvm_vcpu *vcpu;\r\nint i, c;\r\nunsigned long *bmap;\r\nu32 val = 0;\r\nirq -= VGIC_NR_PRIVATE_IRQS;\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nbmap = vgic_bitmap_get_shared_map(&dist->irq_spi_target[c]);\r\nfor (i = 0; i < GICD_IRQS_PER_ITARGETSR; i++)\r\nif (test_bit(irq + i, bmap))\r\nval |= 1 << (c + i * 8);\r\n}\r\nreturn val;\r\n}\r\nstatic void vgic_set_target_reg(struct kvm *kvm, u32 val, int irq)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct kvm_vcpu *vcpu;\r\nint i, c;\r\nunsigned long *bmap;\r\nu32 target;\r\nirq -= VGIC_NR_PRIVATE_IRQS;\r\nfor (i = 0; i < GICD_IRQS_PER_ITARGETSR; i++) {\r\nint shift = i * GICD_CPUTARGETS_BITS;\r\ntarget = ffs((val >> shift) & 0xffU);\r\ntarget = target ? (target - 1) : 0;\r\ndist->irq_spi_cpu[irq + i] = target;\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nbmap = vgic_bitmap_get_shared_map(&dist->irq_spi_target[c]);\r\nif (c == target)\r\nset_bit(irq + i, bmap);\r\nelse\r\nclear_bit(irq + i, bmap);\r\n}\r\n}\r\n}\r\nstatic bool handle_mmio_target_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 reg;\r\nif (offset < 32) {\r\nu32 roreg = 1 << vcpu->vcpu_id;\r\nroreg |= roreg << 8;\r\nroreg |= roreg << 16;\r\nvgic_reg_access(mmio, &roreg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nreg = vgic_get_target_reg(vcpu->kvm, offset & ~3U);\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nif (mmio->is_write) {\r\nvgic_set_target_reg(vcpu->kvm, reg, offset & ~3U);\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic u32 vgic_cfg_expand(u16 val)\r\n{\r\nu32 res = 0;\r\nint i;\r\nfor (i = 0; i < 16; i++)\r\nres |= ((val >> i) & VGIC_CFG_EDGE) << (2 * i + 1);\r\nreturn res;\r\n}\r\nstatic u16 vgic_cfg_compress(u32 val)\r\n{\r\nu16 res = 0;\r\nint i;\r\nfor (i = 0; i < 16; i++)\r\nres |= ((val >> (i * 2 + 1)) & VGIC_CFG_EDGE) << i;\r\nreturn res;\r\n}\r\nstatic bool handle_mmio_cfg_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 val;\r\nu32 *reg = vgic_bitmap_get_reg(&vcpu->kvm->arch.vgic.irq_cfg,\r\nvcpu->vcpu_id, offset >> 1);\r\nif (offset & 2)\r\nval = *reg >> 16;\r\nelse\r\nval = *reg & 0xffff;\r\nval = vgic_cfg_expand(val);\r\nvgic_reg_access(mmio, &val, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nif (mmio->is_write) {\r\nif (offset < 4) {\r\n*reg = ~0U;\r\nreturn false;\r\n}\r\nval = vgic_cfg_compress(val);\r\nif (offset & 2) {\r\n*reg &= 0xffff;\r\n*reg |= val << 16;\r\n} else {\r\n*reg &= 0xffff << 16;\r\n*reg |= val;\r\n}\r\n}\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_sgi_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 reg;\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_VALUE);\r\nif (mmio->is_write) {\r\nvgic_dispatch_sgi(vcpu, reg);\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic const\r\nstruct mmio_range *find_matching_range(const struct mmio_range *ranges,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t base)\r\n{\r\nconst struct mmio_range *r = ranges;\r\nphys_addr_t addr = mmio->phys_addr - base;\r\nwhile (r->len) {\r\nif (addr >= r->base &&\r\n(addr + mmio->len) <= (r->base + r->len))\r\nreturn r;\r\nr++;\r\n}\r\nreturn NULL;\r\n}\r\nbool vgic_handle_mmio(struct kvm_vcpu *vcpu, struct kvm_run *run,\r\nstruct kvm_exit_mmio *mmio)\r\n{\r\nconst struct mmio_range *range;\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nunsigned long base = dist->vgic_dist_base;\r\nbool updated_state;\r\nunsigned long offset;\r\nif (!irqchip_in_kernel(vcpu->kvm) ||\r\nmmio->phys_addr < base ||\r\n(mmio->phys_addr + mmio->len) > (base + KVM_VGIC_V2_DIST_SIZE))\r\nreturn false;\r\nif (mmio->len > 4) {\r\nkvm_inject_dabt(vcpu, mmio->phys_addr);\r\nreturn true;\r\n}\r\nrange = find_matching_range(vgic_ranges, mmio, base);\r\nif (unlikely(!range || !range->handle_mmio)) {\r\npr_warn("Unhandled access %d %08llx %d\n",\r\nmmio->is_write, mmio->phys_addr, mmio->len);\r\nreturn false;\r\n}\r\nspin_lock(&vcpu->kvm->arch.vgic.lock);\r\noffset = mmio->phys_addr - range->base - base;\r\nupdated_state = range->handle_mmio(vcpu, mmio, offset);\r\nspin_unlock(&vcpu->kvm->arch.vgic.lock);\r\nkvm_prepare_mmio(run, mmio);\r\nkvm_handle_mmio_return(vcpu, run);\r\nif (updated_state)\r\nvgic_kick_vcpus(vcpu->kvm);\r\nreturn true;\r\n}\r\nstatic void vgic_dispatch_sgi(struct kvm_vcpu *vcpu, u32 reg)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nint nrcpus = atomic_read(&kvm->online_vcpus);\r\nu8 target_cpus;\r\nint sgi, mode, c, vcpu_id;\r\nvcpu_id = vcpu->vcpu_id;\r\nsgi = reg & 0xf;\r\ntarget_cpus = (reg >> 16) & 0xff;\r\nmode = (reg >> 24) & 3;\r\nswitch (mode) {\r\ncase 0:\r\nif (!target_cpus)\r\nreturn;\r\ncase 1:\r\ntarget_cpus = ((1 << nrcpus) - 1) & ~(1 << vcpu_id) & 0xff;\r\nbreak;\r\ncase 2:\r\ntarget_cpus = 1 << vcpu_id;\r\nbreak;\r\n}\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nif (target_cpus & 1) {\r\nvgic_dist_irq_set(vcpu, sgi);\r\ndist->irq_sgi_sources[c][sgi] |= 1 << vcpu_id;\r\nkvm_debug("SGI%d from CPU%d to CPU%d\n", sgi, vcpu_id, c);\r\n}\r\ntarget_cpus >>= 1;\r\n}\r\n}\r\nstatic int compute_pending_for_cpu(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nunsigned long *pending, *enabled, *pend_percpu, *pend_shared;\r\nunsigned long pending_private, pending_shared;\r\nint vcpu_id;\r\nvcpu_id = vcpu->vcpu_id;\r\npend_percpu = vcpu->arch.vgic_cpu.pending_percpu;\r\npend_shared = vcpu->arch.vgic_cpu.pending_shared;\r\npending = vgic_bitmap_get_cpu_map(&dist->irq_state, vcpu_id);\r\nenabled = vgic_bitmap_get_cpu_map(&dist->irq_enabled, vcpu_id);\r\nbitmap_and(pend_percpu, pending, enabled, VGIC_NR_PRIVATE_IRQS);\r\npending = vgic_bitmap_get_shared_map(&dist->irq_state);\r\nenabled = vgic_bitmap_get_shared_map(&dist->irq_enabled);\r\nbitmap_and(pend_shared, pending, enabled, VGIC_NR_SHARED_IRQS);\r\nbitmap_and(pend_shared, pend_shared,\r\nvgic_bitmap_get_shared_map(&dist->irq_spi_target[vcpu_id]),\r\nVGIC_NR_SHARED_IRQS);\r\npending_private = find_first_bit(pend_percpu, VGIC_NR_PRIVATE_IRQS);\r\npending_shared = find_first_bit(pend_shared, VGIC_NR_SHARED_IRQS);\r\nreturn (pending_private < VGIC_NR_PRIVATE_IRQS ||\r\npending_shared < VGIC_NR_SHARED_IRQS);\r\n}\r\nstatic void vgic_update_state(struct kvm *kvm)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct kvm_vcpu *vcpu;\r\nint c;\r\nif (!dist->enabled) {\r\nset_bit(0, &dist->irq_pending_on_cpu);\r\nreturn;\r\n}\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nif (compute_pending_for_cpu(vcpu)) {\r\npr_debug("CPU%d has pending interrupts\n", c);\r\nset_bit(c, &dist->irq_pending_on_cpu);\r\n}\r\n}\r\n}\r\nstatic void vgic_retire_disabled_irqs(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nint lr;\r\nfor_each_set_bit(lr, vgic_cpu->lr_used, vgic_cpu->nr_lr) {\r\nint irq = vgic_cpu->vgic_lr[lr] & GICH_LR_VIRTUALID;\r\nif (!vgic_irq_is_enabled(vcpu, irq)) {\r\nvgic_cpu->vgic_irq_lr_map[irq] = LR_EMPTY;\r\nclear_bit(lr, vgic_cpu->lr_used);\r\nvgic_cpu->vgic_lr[lr] &= ~GICH_LR_STATE;\r\nif (vgic_irq_is_active(vcpu, irq))\r\nvgic_irq_clear_active(vcpu, irq);\r\n}\r\n}\r\n}\r\nstatic bool vgic_queue_irq(struct kvm_vcpu *vcpu, u8 sgi_source_id, int irq)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nint lr;\r\nBUG_ON(sgi_source_id & ~7);\r\nBUG_ON(sgi_source_id && irq >= VGIC_NR_SGIS);\r\nBUG_ON(irq >= VGIC_NR_IRQS);\r\nkvm_debug("Queue IRQ%d\n", irq);\r\nlr = vgic_cpu->vgic_irq_lr_map[irq];\r\nif (lr != LR_EMPTY &&\r\n(LR_CPUID(vgic_cpu->vgic_lr[lr]) == sgi_source_id)) {\r\nkvm_debug("LR%d piggyback for IRQ%d %x\n",\r\nlr, irq, vgic_cpu->vgic_lr[lr]);\r\nBUG_ON(!test_bit(lr, vgic_cpu->lr_used));\r\nvgic_cpu->vgic_lr[lr] |= GICH_LR_PENDING_BIT;\r\nreturn true;\r\n}\r\nlr = find_first_zero_bit((unsigned long *)vgic_cpu->lr_used,\r\nvgic_cpu->nr_lr);\r\nif (lr >= vgic_cpu->nr_lr)\r\nreturn false;\r\nkvm_debug("LR%d allocated for IRQ%d %x\n", lr, irq, sgi_source_id);\r\nvgic_cpu->vgic_lr[lr] = MK_LR_PEND(sgi_source_id, irq);\r\nvgic_cpu->vgic_irq_lr_map[irq] = lr;\r\nset_bit(lr, vgic_cpu->lr_used);\r\nif (!vgic_irq_is_edge(vcpu, irq))\r\nvgic_cpu->vgic_lr[lr] |= GICH_LR_EOI;\r\nreturn true;\r\n}\r\nstatic bool vgic_queue_sgi(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nunsigned long sources;\r\nint vcpu_id = vcpu->vcpu_id;\r\nint c;\r\nsources = dist->irq_sgi_sources[vcpu_id][irq];\r\nfor_each_set_bit(c, &sources, VGIC_MAX_CPUS) {\r\nif (vgic_queue_irq(vcpu, c, irq))\r\nclear_bit(c, &sources);\r\n}\r\ndist->irq_sgi_sources[vcpu_id][irq] = sources;\r\nif (!sources) {\r\nvgic_dist_irq_clear(vcpu, irq);\r\nvgic_cpu_irq_clear(vcpu, irq);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool vgic_queue_hwirq(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nif (vgic_irq_is_active(vcpu, irq))\r\nreturn true;\r\nif (vgic_queue_irq(vcpu, 0, irq)) {\r\nif (vgic_irq_is_edge(vcpu, irq)) {\r\nvgic_dist_irq_clear(vcpu, irq);\r\nvgic_cpu_irq_clear(vcpu, irq);\r\n} else {\r\nvgic_irq_set_active(vcpu, irq);\r\n}\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void __kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nint i, vcpu_id;\r\nint overflow = 0;\r\nvcpu_id = vcpu->vcpu_id;\r\nif (!kvm_vgic_vcpu_pending_irq(vcpu)) {\r\npr_debug("CPU%d has no pending interrupt\n", vcpu_id);\r\ngoto epilog;\r\n}\r\nfor_each_set_bit(i, vgic_cpu->pending_percpu, VGIC_NR_SGIS) {\r\nif (!vgic_queue_sgi(vcpu, i))\r\noverflow = 1;\r\n}\r\nfor_each_set_bit_from(i, vgic_cpu->pending_percpu, VGIC_NR_PRIVATE_IRQS) {\r\nif (!vgic_queue_hwirq(vcpu, i))\r\noverflow = 1;\r\n}\r\nfor_each_set_bit(i, vgic_cpu->pending_shared, VGIC_NR_SHARED_IRQS) {\r\nif (!vgic_queue_hwirq(vcpu, i + VGIC_NR_PRIVATE_IRQS))\r\noverflow = 1;\r\n}\r\nepilog:\r\nif (overflow) {\r\nvgic_cpu->vgic_hcr |= GICH_HCR_UIE;\r\n} else {\r\nvgic_cpu->vgic_hcr &= ~GICH_HCR_UIE;\r\nclear_bit(vcpu_id, &dist->irq_pending_on_cpu);\r\n}\r\n}\r\nstatic bool vgic_process_maintenance(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nbool level_pending = false;\r\nkvm_debug("MISR = %08x\n", vgic_cpu->vgic_misr);\r\nif (vgic_cpu->vgic_misr & GICH_MISR_EOI) {\r\nint lr, irq;\r\nfor_each_set_bit(lr, (unsigned long *)vgic_cpu->vgic_eisr,\r\nvgic_cpu->nr_lr) {\r\nirq = vgic_cpu->vgic_lr[lr] & GICH_LR_VIRTUALID;\r\nvgic_irq_clear_active(vcpu, irq);\r\nvgic_cpu->vgic_lr[lr] &= ~GICH_LR_EOI;\r\nif (vgic_dist_irq_is_pending(vcpu, irq)) {\r\nvgic_cpu_irq_set(vcpu, irq);\r\nlevel_pending = true;\r\n} else {\r\nvgic_cpu_irq_clear(vcpu, irq);\r\n}\r\nset_bit(lr, (unsigned long *)vgic_cpu->vgic_elrsr);\r\nvgic_cpu->vgic_lr[lr] &= ~GICH_LR_ACTIVE_BIT;\r\n}\r\n}\r\nif (vgic_cpu->vgic_misr & GICH_MISR_U)\r\nvgic_cpu->vgic_hcr &= ~GICH_HCR_UIE;\r\nreturn level_pending;\r\n}\r\nstatic void __kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nint lr, pending;\r\nbool level_pending;\r\nlevel_pending = vgic_process_maintenance(vcpu);\r\nfor_each_set_bit(lr, (unsigned long *)vgic_cpu->vgic_elrsr,\r\nvgic_cpu->nr_lr) {\r\nint irq;\r\nif (!test_and_clear_bit(lr, vgic_cpu->lr_used))\r\ncontinue;\r\nirq = vgic_cpu->vgic_lr[lr] & GICH_LR_VIRTUALID;\r\nBUG_ON(irq >= VGIC_NR_IRQS);\r\nvgic_cpu->vgic_irq_lr_map[irq] = LR_EMPTY;\r\n}\r\npending = find_first_zero_bit((unsigned long *)vgic_cpu->vgic_elrsr,\r\nvgic_cpu->nr_lr);\r\nif (level_pending || pending < vgic_cpu->nr_lr)\r\nset_bit(vcpu->vcpu_id, &dist->irq_pending_on_cpu);\r\n}\r\nvoid kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nif (!irqchip_in_kernel(vcpu->kvm))\r\nreturn;\r\nspin_lock(&dist->lock);\r\n__kvm_vgic_flush_hwstate(vcpu);\r\nspin_unlock(&dist->lock);\r\n}\r\nvoid kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nif (!irqchip_in_kernel(vcpu->kvm))\r\nreturn;\r\nspin_lock(&dist->lock);\r\n__kvm_vgic_sync_hwstate(vcpu);\r\nspin_unlock(&dist->lock);\r\n}\r\nint kvm_vgic_vcpu_pending_irq(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nif (!irqchip_in_kernel(vcpu->kvm))\r\nreturn 0;\r\nreturn test_bit(vcpu->vcpu_id, &dist->irq_pending_on_cpu);\r\n}\r\nstatic void vgic_kick_vcpus(struct kvm *kvm)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nint c;\r\nkvm_for_each_vcpu(c, vcpu, kvm) {\r\nif (kvm_vgic_vcpu_pending_irq(vcpu))\r\nkvm_vcpu_kick(vcpu);\r\n}\r\n}\r\nstatic int vgic_validate_injection(struct kvm_vcpu *vcpu, int irq, int level)\r\n{\r\nint is_edge = vgic_irq_is_edge(vcpu, irq);\r\nint state = vgic_dist_irq_is_pending(vcpu, irq);\r\nif (is_edge)\r\nreturn level > state;\r\nelse\r\nreturn level != state;\r\n}\r\nstatic bool vgic_update_irq_state(struct kvm *kvm, int cpuid,\r\nunsigned int irq_num, bool level)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct kvm_vcpu *vcpu;\r\nint is_edge, is_level;\r\nint enabled;\r\nbool ret = true;\r\nspin_lock(&dist->lock);\r\nvcpu = kvm_get_vcpu(kvm, cpuid);\r\nis_edge = vgic_irq_is_edge(vcpu, irq_num);\r\nis_level = !is_edge;\r\nif (!vgic_validate_injection(vcpu, irq_num, level)) {\r\nret = false;\r\ngoto out;\r\n}\r\nif (irq_num >= VGIC_NR_PRIVATE_IRQS) {\r\ncpuid = dist->irq_spi_cpu[irq_num - VGIC_NR_PRIVATE_IRQS];\r\nvcpu = kvm_get_vcpu(kvm, cpuid);\r\n}\r\nkvm_debug("Inject IRQ%d level %d CPU%d\n", irq_num, level, cpuid);\r\nif (level)\r\nvgic_dist_irq_set(vcpu, irq_num);\r\nelse\r\nvgic_dist_irq_clear(vcpu, irq_num);\r\nenabled = vgic_irq_is_enabled(vcpu, irq_num);\r\nif (!enabled) {\r\nret = false;\r\ngoto out;\r\n}\r\nif (is_level && vgic_irq_is_active(vcpu, irq_num)) {\r\nret = false;\r\ngoto out;\r\n}\r\nif (level) {\r\nvgic_cpu_irq_set(vcpu, irq_num);\r\nset_bit(cpuid, &dist->irq_pending_on_cpu);\r\n}\r\nout:\r\nspin_unlock(&dist->lock);\r\nreturn ret;\r\n}\r\nint kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int irq_num,\r\nbool level)\r\n{\r\nif (vgic_update_irq_state(kvm, cpuid, irq_num, level))\r\nvgic_kick_vcpus(kvm);\r\nreturn 0;\r\n}\r\nstatic irqreturn_t vgic_maintenance_handler(int irq, void *data)\r\n{\r\nreturn IRQ_HANDLED;\r\n}\r\nint kvm_vgic_vcpu_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nint i;\r\nif (!irqchip_in_kernel(vcpu->kvm))\r\nreturn 0;\r\nif (vcpu->vcpu_id >= VGIC_MAX_CPUS)\r\nreturn -EBUSY;\r\nfor (i = 0; i < VGIC_NR_IRQS; i++) {\r\nif (i < VGIC_NR_PPIS)\r\nvgic_bitmap_set_irq_val(&dist->irq_enabled,\r\nvcpu->vcpu_id, i, 1);\r\nif (i < VGIC_NR_PRIVATE_IRQS)\r\nvgic_bitmap_set_irq_val(&dist->irq_cfg,\r\nvcpu->vcpu_id, i, VGIC_CFG_EDGE);\r\nvgic_cpu->vgic_irq_lr_map[i] = LR_EMPTY;\r\n}\r\nvgic_cpu->vgic_vmcr = 0;\r\nvgic_cpu->nr_lr = vgic_nr_lr;\r\nvgic_cpu->vgic_hcr = GICH_HCR_EN;\r\nreturn 0;\r\n}\r\nstatic void vgic_init_maintenance_interrupt(void *info)\r\n{\r\nenable_percpu_irq(vgic_maint_irq, 0);\r\n}\r\nstatic int vgic_cpu_notify(struct notifier_block *self,\r\nunsigned long action, void *cpu)\r\n{\r\nswitch (action) {\r\ncase CPU_STARTING:\r\ncase CPU_STARTING_FROZEN:\r\nvgic_init_maintenance_interrupt(NULL);\r\nbreak;\r\ncase CPU_DYING:\r\ncase CPU_DYING_FROZEN:\r\ndisable_percpu_irq(vgic_maint_irq);\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nint kvm_vgic_hyp_init(void)\r\n{\r\nint ret;\r\nstruct resource vctrl_res;\r\nstruct resource vcpu_res;\r\nvgic_node = of_find_compatible_node(NULL, NULL, "arm,cortex-a15-gic");\r\nif (!vgic_node) {\r\nkvm_err("error: no compatible vgic node in DT\n");\r\nreturn -ENODEV;\r\n}\r\nvgic_maint_irq = irq_of_parse_and_map(vgic_node, 0);\r\nif (!vgic_maint_irq) {\r\nkvm_err("error getting vgic maintenance irq from DT\n");\r\nret = -ENXIO;\r\ngoto out;\r\n}\r\nret = request_percpu_irq(vgic_maint_irq, vgic_maintenance_handler,\r\n"vgic", kvm_get_running_vcpus());\r\nif (ret) {\r\nkvm_err("Cannot register interrupt %d\n", vgic_maint_irq);\r\ngoto out;\r\n}\r\nret = register_cpu_notifier(&vgic_cpu_nb);\r\nif (ret) {\r\nkvm_err("Cannot register vgic CPU notifier\n");\r\ngoto out_free_irq;\r\n}\r\nret = of_address_to_resource(vgic_node, 2, &vctrl_res);\r\nif (ret) {\r\nkvm_err("Cannot obtain VCTRL resource\n");\r\ngoto out_free_irq;\r\n}\r\nvgic_vctrl_base = of_iomap(vgic_node, 2);\r\nif (!vgic_vctrl_base) {\r\nkvm_err("Cannot ioremap VCTRL\n");\r\nret = -ENOMEM;\r\ngoto out_free_irq;\r\n}\r\nvgic_nr_lr = readl_relaxed(vgic_vctrl_base + GICH_VTR);\r\nvgic_nr_lr = (vgic_nr_lr & 0x3f) + 1;\r\nret = create_hyp_io_mappings(vgic_vctrl_base,\r\nvgic_vctrl_base + resource_size(&vctrl_res),\r\nvctrl_res.start);\r\nif (ret) {\r\nkvm_err("Cannot map VCTRL into hyp\n");\r\ngoto out_unmap;\r\n}\r\nkvm_info("%s@%llx IRQ%d\n", vgic_node->name,\r\nvctrl_res.start, vgic_maint_irq);\r\non_each_cpu(vgic_init_maintenance_interrupt, NULL, 1);\r\nif (of_address_to_resource(vgic_node, 3, &vcpu_res)) {\r\nkvm_err("Cannot obtain VCPU resource\n");\r\nret = -ENXIO;\r\ngoto out_unmap;\r\n}\r\nvgic_vcpu_base = vcpu_res.start;\r\ngoto out;\r\nout_unmap:\r\niounmap(vgic_vctrl_base);\r\nout_free_irq:\r\nfree_percpu_irq(vgic_maint_irq, kvm_get_running_vcpus());\r\nout:\r\nof_node_put(vgic_node);\r\nreturn ret;\r\n}\r\nint kvm_vgic_init(struct kvm *kvm)\r\n{\r\nint ret = 0, i;\r\nmutex_lock(&kvm->lock);\r\nif (vgic_initialized(kvm))\r\ngoto out;\r\nif (IS_VGIC_ADDR_UNDEF(kvm->arch.vgic.vgic_dist_base) ||\r\nIS_VGIC_ADDR_UNDEF(kvm->arch.vgic.vgic_cpu_base)) {\r\nkvm_err("Need to set vgic cpu and dist addresses first\n");\r\nret = -ENXIO;\r\ngoto out;\r\n}\r\nret = kvm_phys_addr_ioremap(kvm, kvm->arch.vgic.vgic_cpu_base,\r\nvgic_vcpu_base, KVM_VGIC_V2_CPU_SIZE);\r\nif (ret) {\r\nkvm_err("Unable to remap VGIC CPU to VCPU\n");\r\ngoto out;\r\n}\r\nfor (i = VGIC_NR_PRIVATE_IRQS; i < VGIC_NR_IRQS; i += 4)\r\nvgic_set_target_reg(kvm, 0, i);\r\nkvm_timer_init(kvm);\r\nkvm->arch.vgic.ready = true;\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn ret;\r\n}\r\nint kvm_vgic_create(struct kvm *kvm)\r\n{\r\nint ret = 0;\r\nmutex_lock(&kvm->lock);\r\nif (atomic_read(&kvm->online_vcpus) || kvm->arch.vgic.vctrl_base) {\r\nret = -EEXIST;\r\ngoto out;\r\n}\r\nspin_lock_init(&kvm->arch.vgic.lock);\r\nkvm->arch.vgic.vctrl_base = vgic_vctrl_base;\r\nkvm->arch.vgic.vgic_dist_base = VGIC_ADDR_UNDEF;\r\nkvm->arch.vgic.vgic_cpu_base = VGIC_ADDR_UNDEF;\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn ret;\r\n}\r\nstatic bool vgic_ioaddr_overlap(struct kvm *kvm)\r\n{\r\nphys_addr_t dist = kvm->arch.vgic.vgic_dist_base;\r\nphys_addr_t cpu = kvm->arch.vgic.vgic_cpu_base;\r\nif (IS_VGIC_ADDR_UNDEF(dist) || IS_VGIC_ADDR_UNDEF(cpu))\r\nreturn 0;\r\nif ((dist <= cpu && dist + KVM_VGIC_V2_DIST_SIZE > cpu) ||\r\n(cpu <= dist && cpu + KVM_VGIC_V2_CPU_SIZE > dist))\r\nreturn -EBUSY;\r\nreturn 0;\r\n}\r\nstatic int vgic_ioaddr_assign(struct kvm *kvm, phys_addr_t *ioaddr,\r\nphys_addr_t addr, phys_addr_t size)\r\n{\r\nint ret;\r\nif (!IS_VGIC_ADDR_UNDEF(*ioaddr))\r\nreturn -EEXIST;\r\nif (addr + size < addr)\r\nreturn -EINVAL;\r\nret = vgic_ioaddr_overlap(kvm);\r\nif (ret)\r\nreturn ret;\r\n*ioaddr = addr;\r\nreturn ret;\r\n}\r\nint kvm_vgic_set_addr(struct kvm *kvm, unsigned long type, u64 addr)\r\n{\r\nint r = 0;\r\nstruct vgic_dist *vgic = &kvm->arch.vgic;\r\nif (addr & ~KVM_PHYS_MASK)\r\nreturn -E2BIG;\r\nif (addr & (SZ_4K - 1))\r\nreturn -EINVAL;\r\nmutex_lock(&kvm->lock);\r\nswitch (type) {\r\ncase KVM_VGIC_V2_ADDR_TYPE_DIST:\r\nr = vgic_ioaddr_assign(kvm, &vgic->vgic_dist_base,\r\naddr, KVM_VGIC_V2_DIST_SIZE);\r\nbreak;\r\ncase KVM_VGIC_V2_ADDR_TYPE_CPU:\r\nr = vgic_ioaddr_assign(kvm, &vgic->vgic_cpu_base,\r\naddr, KVM_VGIC_V2_CPU_SIZE);\r\nbreak;\r\ndefault:\r\nr = -ENODEV;\r\n}\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\n}
