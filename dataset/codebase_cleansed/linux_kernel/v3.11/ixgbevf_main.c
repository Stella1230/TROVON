static inline void ixgbevf_release_rx_desc(struct ixgbe_hw *hw,\r\nstruct ixgbevf_ring *rx_ring,\r\nu32 val)\r\n{\r\nwmb();\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDT(rx_ring->reg_idx), val);\r\n}\r\nstatic void ixgbevf_set_ivar(struct ixgbevf_adapter *adapter, s8 direction,\r\nu8 queue, u8 msix_vector)\r\n{\r\nu32 ivar, index;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nif (direction == -1) {\r\nmsix_vector |= IXGBE_IVAR_ALLOC_VAL;\r\nivar = IXGBE_READ_REG(hw, IXGBE_VTIVAR_MISC);\r\nivar &= ~0xFF;\r\nivar |= msix_vector;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTIVAR_MISC, ivar);\r\n} else {\r\nmsix_vector |= IXGBE_IVAR_ALLOC_VAL;\r\nindex = ((16 * (queue & 1)) + (8 * direction));\r\nivar = IXGBE_READ_REG(hw, IXGBE_VTIVAR(queue >> 1));\r\nivar &= ~(0xFF << index);\r\nivar |= (msix_vector << index);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTIVAR(queue >> 1), ivar);\r\n}\r\n}\r\nstatic void ixgbevf_unmap_and_free_tx_resource(struct ixgbevf_ring *tx_ring,\r\nstruct ixgbevf_tx_buffer\r\n*tx_buffer_info)\r\n{\r\nif (tx_buffer_info->dma) {\r\nif (tx_buffer_info->mapped_as_page)\r\ndma_unmap_page(tx_ring->dev,\r\ntx_buffer_info->dma,\r\ntx_buffer_info->length,\r\nDMA_TO_DEVICE);\r\nelse\r\ndma_unmap_single(tx_ring->dev,\r\ntx_buffer_info->dma,\r\ntx_buffer_info->length,\r\nDMA_TO_DEVICE);\r\ntx_buffer_info->dma = 0;\r\n}\r\nif (tx_buffer_info->skb) {\r\ndev_kfree_skb_any(tx_buffer_info->skb);\r\ntx_buffer_info->skb = NULL;\r\n}\r\ntx_buffer_info->time_stamp = 0;\r\n}\r\nstatic bool ixgbevf_clean_tx_irq(struct ixgbevf_q_vector *q_vector,\r\nstruct ixgbevf_ring *tx_ring)\r\n{\r\nstruct ixgbevf_adapter *adapter = q_vector->adapter;\r\nunion ixgbe_adv_tx_desc *tx_desc, *eop_desc;\r\nstruct ixgbevf_tx_buffer *tx_buffer_info;\r\nunsigned int i, count = 0;\r\nunsigned int total_bytes = 0, total_packets = 0;\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state))\r\nreturn true;\r\ni = tx_ring->next_to_clean;\r\ntx_buffer_info = &tx_ring->tx_buffer_info[i];\r\neop_desc = tx_buffer_info->next_to_watch;\r\ndo {\r\nbool cleaned = false;\r\nif (!eop_desc)\r\nbreak;\r\nread_barrier_depends();\r\nif (!(eop_desc->wb.status & cpu_to_le32(IXGBE_TXD_STAT_DD)))\r\nbreak;\r\ntx_buffer_info->next_to_watch = NULL;\r\nfor ( ; !cleaned; count++) {\r\nstruct sk_buff *skb;\r\ntx_desc = IXGBEVF_TX_DESC(tx_ring, i);\r\ncleaned = (tx_desc == eop_desc);\r\nskb = tx_buffer_info->skb;\r\nif (cleaned && skb) {\r\nunsigned int segs, bytecount;\r\nsegs = skb_shinfo(skb)->gso_segs ?: 1;\r\nbytecount = ((segs - 1) * skb_headlen(skb)) +\r\nskb->len;\r\ntotal_packets += segs;\r\ntotal_bytes += bytecount;\r\n}\r\nixgbevf_unmap_and_free_tx_resource(tx_ring,\r\ntx_buffer_info);\r\ntx_desc->wb.status = 0;\r\ni++;\r\nif (i == tx_ring->count)\r\ni = 0;\r\ntx_buffer_info = &tx_ring->tx_buffer_info[i];\r\n}\r\neop_desc = tx_buffer_info->next_to_watch;\r\n} while (count < tx_ring->count);\r\ntx_ring->next_to_clean = i;\r\n#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)\r\nif (unlikely(count && netif_carrier_ok(tx_ring->netdev) &&\r\n(IXGBE_DESC_UNUSED(tx_ring) >= TX_WAKE_THRESHOLD))) {\r\nsmp_mb();\r\nif (__netif_subqueue_stopped(tx_ring->netdev,\r\ntx_ring->queue_index) &&\r\n!test_bit(__IXGBEVF_DOWN, &adapter->state)) {\r\nnetif_wake_subqueue(tx_ring->netdev,\r\ntx_ring->queue_index);\r\n++adapter->restart_queue;\r\n}\r\n}\r\nu64_stats_update_begin(&tx_ring->syncp);\r\ntx_ring->total_bytes += total_bytes;\r\ntx_ring->total_packets += total_packets;\r\nu64_stats_update_end(&tx_ring->syncp);\r\nq_vector->tx.total_bytes += total_bytes;\r\nq_vector->tx.total_packets += total_packets;\r\nreturn count < tx_ring->count;\r\n}\r\nstatic void ixgbevf_receive_skb(struct ixgbevf_q_vector *q_vector,\r\nstruct sk_buff *skb, u8 status,\r\nunion ixgbe_adv_rx_desc *rx_desc)\r\n{\r\nstruct ixgbevf_adapter *adapter = q_vector->adapter;\r\nbool is_vlan = (status & IXGBE_RXD_STAT_VP);\r\nu16 tag = le16_to_cpu(rx_desc->wb.upper.vlan);\r\nif (is_vlan && test_bit(tag & VLAN_VID_MASK, adapter->active_vlans))\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tag);\r\nif (!(adapter->flags & IXGBE_FLAG_IN_NETPOLL))\r\nnapi_gro_receive(&q_vector->napi, skb);\r\nelse\r\nnetif_rx(skb);\r\n}\r\nstatic inline void ixgbevf_rx_checksum(struct ixgbevf_ring *ring,\r\nu32 status_err, struct sk_buff *skb)\r\n{\r\nskb_checksum_none_assert(skb);\r\nif (!(ring->netdev->features & NETIF_F_RXCSUM))\r\nreturn;\r\nif ((status_err & IXGBE_RXD_STAT_IPCS) &&\r\n(status_err & IXGBE_RXDADV_ERR_IPE)) {\r\nring->hw_csum_rx_error++;\r\nreturn;\r\n}\r\nif (!(status_err & IXGBE_RXD_STAT_L4CS))\r\nreturn;\r\nif (status_err & IXGBE_RXDADV_ERR_TCPE) {\r\nring->hw_csum_rx_error++;\r\nreturn;\r\n}\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nring->hw_csum_rx_good++;\r\n}\r\nstatic void ixgbevf_alloc_rx_buffers(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *rx_ring,\r\nint cleaned_count)\r\n{\r\nstruct pci_dev *pdev = adapter->pdev;\r\nunion ixgbe_adv_rx_desc *rx_desc;\r\nstruct ixgbevf_rx_buffer *bi;\r\nunsigned int i = rx_ring->next_to_use;\r\nbi = &rx_ring->rx_buffer_info[i];\r\nwhile (cleaned_count--) {\r\nrx_desc = IXGBEVF_RX_DESC(rx_ring, i);\r\nif (!bi->skb) {\r\nstruct sk_buff *skb;\r\nskb = netdev_alloc_skb_ip_align(rx_ring->netdev,\r\nrx_ring->rx_buf_len);\r\nif (!skb) {\r\nadapter->alloc_rx_buff_failed++;\r\ngoto no_buffers;\r\n}\r\nbi->skb = skb;\r\nbi->dma = dma_map_single(&pdev->dev, skb->data,\r\nrx_ring->rx_buf_len,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(&pdev->dev, bi->dma)) {\r\ndev_kfree_skb(skb);\r\nbi->skb = NULL;\r\ndev_err(&pdev->dev, "RX DMA map failed\n");\r\nbreak;\r\n}\r\n}\r\nrx_desc->read.pkt_addr = cpu_to_le64(bi->dma);\r\ni++;\r\nif (i == rx_ring->count)\r\ni = 0;\r\nbi = &rx_ring->rx_buffer_info[i];\r\n}\r\nno_buffers:\r\nif (rx_ring->next_to_use != i) {\r\nrx_ring->next_to_use = i;\r\nixgbevf_release_rx_desc(&adapter->hw, rx_ring, i);\r\n}\r\n}\r\nstatic inline void ixgbevf_irq_enable_queues(struct ixgbevf_adapter *adapter,\r\nu32 qmask)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIMS, qmask);\r\n}\r\nstatic bool ixgbevf_clean_rx_irq(struct ixgbevf_q_vector *q_vector,\r\nstruct ixgbevf_ring *rx_ring,\r\nint budget)\r\n{\r\nstruct ixgbevf_adapter *adapter = q_vector->adapter;\r\nstruct pci_dev *pdev = adapter->pdev;\r\nunion ixgbe_adv_rx_desc *rx_desc, *next_rxd;\r\nstruct ixgbevf_rx_buffer *rx_buffer_info, *next_buffer;\r\nstruct sk_buff *skb;\r\nunsigned int i;\r\nu32 len, staterr;\r\nint cleaned_count = 0;\r\nunsigned int total_rx_bytes = 0, total_rx_packets = 0;\r\ni = rx_ring->next_to_clean;\r\nrx_desc = IXGBEVF_RX_DESC(rx_ring, i);\r\nstaterr = le32_to_cpu(rx_desc->wb.upper.status_error);\r\nrx_buffer_info = &rx_ring->rx_buffer_info[i];\r\nwhile (staterr & IXGBE_RXD_STAT_DD) {\r\nif (!budget)\r\nbreak;\r\nbudget--;\r\nrmb();\r\nlen = le16_to_cpu(rx_desc->wb.upper.length);\r\nskb = rx_buffer_info->skb;\r\nprefetch(skb->data - NET_IP_ALIGN);\r\nrx_buffer_info->skb = NULL;\r\nif (rx_buffer_info->dma) {\r\ndma_unmap_single(&pdev->dev, rx_buffer_info->dma,\r\nrx_ring->rx_buf_len,\r\nDMA_FROM_DEVICE);\r\nrx_buffer_info->dma = 0;\r\nskb_put(skb, len);\r\n}\r\ni++;\r\nif (i == rx_ring->count)\r\ni = 0;\r\nnext_rxd = IXGBEVF_RX_DESC(rx_ring, i);\r\nprefetch(next_rxd);\r\ncleaned_count++;\r\nnext_buffer = &rx_ring->rx_buffer_info[i];\r\nif (!(staterr & IXGBE_RXD_STAT_EOP)) {\r\nskb->next = next_buffer->skb;\r\nIXGBE_CB(skb->next)->prev = skb;\r\nadapter->non_eop_descs++;\r\ngoto next_desc;\r\n}\r\nif (IXGBE_CB(skb)->prev) {\r\ndo {\r\nstruct sk_buff *this = skb;\r\nskb = IXGBE_CB(skb)->prev;\r\ndev_kfree_skb(this);\r\n} while (skb);\r\ngoto next_desc;\r\n}\r\nif (unlikely(staterr & IXGBE_RXDADV_ERR_FRAME_ERR_MASK)) {\r\ndev_kfree_skb_irq(skb);\r\ngoto next_desc;\r\n}\r\nixgbevf_rx_checksum(rx_ring, staterr, skb);\r\ntotal_rx_bytes += skb->len;\r\ntotal_rx_packets++;\r\nif (staterr & IXGBE_RXD_STAT_LB) {\r\nu32 header_fixup_len = skb_headlen(skb);\r\nif (header_fixup_len < 14)\r\nskb_push(skb, header_fixup_len);\r\n}\r\nskb->protocol = eth_type_trans(skb, rx_ring->netdev);\r\nif ((skb->pkt_type & (PACKET_BROADCAST | PACKET_MULTICAST)) &&\r\n!(compare_ether_addr(adapter->netdev->dev_addr,\r\neth_hdr(skb)->h_source))) {\r\ndev_kfree_skb_irq(skb);\r\ngoto next_desc;\r\n}\r\nixgbevf_receive_skb(q_vector, skb, staterr, rx_desc);\r\nnext_desc:\r\nrx_desc->wb.upper.status_error = 0;\r\nif (cleaned_count >= IXGBEVF_RX_BUFFER_WRITE) {\r\nixgbevf_alloc_rx_buffers(adapter, rx_ring,\r\ncleaned_count);\r\ncleaned_count = 0;\r\n}\r\nrx_desc = next_rxd;\r\nrx_buffer_info = &rx_ring->rx_buffer_info[i];\r\nstaterr = le32_to_cpu(rx_desc->wb.upper.status_error);\r\n}\r\nrx_ring->next_to_clean = i;\r\ncleaned_count = IXGBE_DESC_UNUSED(rx_ring);\r\nif (cleaned_count)\r\nixgbevf_alloc_rx_buffers(adapter, rx_ring, cleaned_count);\r\nu64_stats_update_begin(&rx_ring->syncp);\r\nrx_ring->total_packets += total_rx_packets;\r\nrx_ring->total_bytes += total_rx_bytes;\r\nu64_stats_update_end(&rx_ring->syncp);\r\nq_vector->rx.total_packets += total_rx_packets;\r\nq_vector->rx.total_bytes += total_rx_bytes;\r\nreturn !!budget;\r\n}\r\nstatic int ixgbevf_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct ixgbevf_q_vector *q_vector =\r\ncontainer_of(napi, struct ixgbevf_q_vector, napi);\r\nstruct ixgbevf_adapter *adapter = q_vector->adapter;\r\nstruct ixgbevf_ring *ring;\r\nint per_ring_budget;\r\nbool clean_complete = true;\r\nixgbevf_for_each_ring(ring, q_vector->tx)\r\nclean_complete &= ixgbevf_clean_tx_irq(q_vector, ring);\r\nif (q_vector->rx.count > 1)\r\nper_ring_budget = max(budget/q_vector->rx.count, 1);\r\nelse\r\nper_ring_budget = budget;\r\nadapter->flags |= IXGBE_FLAG_IN_NETPOLL;\r\nixgbevf_for_each_ring(ring, q_vector->rx)\r\nclean_complete &= ixgbevf_clean_rx_irq(q_vector, ring,\r\nper_ring_budget);\r\nadapter->flags &= ~IXGBE_FLAG_IN_NETPOLL;\r\nif (!clean_complete)\r\nreturn budget;\r\nnapi_complete(napi);\r\nif (adapter->rx_itr_setting & 1)\r\nixgbevf_set_itr(q_vector);\r\nif (!test_bit(__IXGBEVF_DOWN, &adapter->state))\r\nixgbevf_irq_enable_queues(adapter,\r\n1 << q_vector->v_idx);\r\nreturn 0;\r\n}\r\nstatic void ixgbevf_write_eitr(struct ixgbevf_q_vector *q_vector)\r\n{\r\nstruct ixgbevf_adapter *adapter = q_vector->adapter;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint v_idx = q_vector->v_idx;\r\nu32 itr_reg = q_vector->itr & IXGBE_MAX_EITR;\r\nitr_reg |= IXGBE_EITR_CNT_WDIS;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEITR(v_idx), itr_reg);\r\n}\r\nstatic void ixgbevf_configure_msix(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbevf_q_vector *q_vector;\r\nint q_vectors, v_idx;\r\nq_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nadapter->eims_enable_mask = 0;\r\nfor (v_idx = 0; v_idx < q_vectors; v_idx++) {\r\nstruct ixgbevf_ring *ring;\r\nq_vector = adapter->q_vector[v_idx];\r\nixgbevf_for_each_ring(ring, q_vector->rx)\r\nixgbevf_set_ivar(adapter, 0, ring->reg_idx, v_idx);\r\nixgbevf_for_each_ring(ring, q_vector->tx)\r\nixgbevf_set_ivar(adapter, 1, ring->reg_idx, v_idx);\r\nif (q_vector->tx.ring && !q_vector->rx.ring) {\r\nif (adapter->tx_itr_setting == 1)\r\nq_vector->itr = IXGBE_10K_ITR;\r\nelse\r\nq_vector->itr = adapter->tx_itr_setting;\r\n} else {\r\nif (adapter->rx_itr_setting == 1)\r\nq_vector->itr = IXGBE_20K_ITR;\r\nelse\r\nq_vector->itr = adapter->rx_itr_setting;\r\n}\r\nadapter->eims_enable_mask |= 1 << v_idx;\r\nixgbevf_write_eitr(q_vector);\r\n}\r\nixgbevf_set_ivar(adapter, -1, 1, v_idx);\r\nadapter->eims_other = 1 << v_idx;\r\nadapter->eims_enable_mask |= adapter->eims_other;\r\n}\r\nstatic void ixgbevf_update_itr(struct ixgbevf_q_vector *q_vector,\r\nstruct ixgbevf_ring_container *ring_container)\r\n{\r\nint bytes = ring_container->total_bytes;\r\nint packets = ring_container->total_packets;\r\nu32 timepassed_us;\r\nu64 bytes_perint;\r\nu8 itr_setting = ring_container->itr;\r\nif (packets == 0)\r\nreturn;\r\ntimepassed_us = q_vector->itr >> 2;\r\nbytes_perint = bytes / timepassed_us;\r\nswitch (itr_setting) {\r\ncase lowest_latency:\r\nif (bytes_perint > 10)\r\nitr_setting = low_latency;\r\nbreak;\r\ncase low_latency:\r\nif (bytes_perint > 20)\r\nitr_setting = bulk_latency;\r\nelse if (bytes_perint <= 10)\r\nitr_setting = lowest_latency;\r\nbreak;\r\ncase bulk_latency:\r\nif (bytes_perint <= 20)\r\nitr_setting = low_latency;\r\nbreak;\r\n}\r\nring_container->total_bytes = 0;\r\nring_container->total_packets = 0;\r\nring_container->itr = itr_setting;\r\n}\r\nstatic void ixgbevf_set_itr(struct ixgbevf_q_vector *q_vector)\r\n{\r\nu32 new_itr = q_vector->itr;\r\nu8 current_itr;\r\nixgbevf_update_itr(q_vector, &q_vector->tx);\r\nixgbevf_update_itr(q_vector, &q_vector->rx);\r\ncurrent_itr = max(q_vector->rx.itr, q_vector->tx.itr);\r\nswitch (current_itr) {\r\ncase lowest_latency:\r\nnew_itr = IXGBE_100K_ITR;\r\nbreak;\r\ncase low_latency:\r\nnew_itr = IXGBE_20K_ITR;\r\nbreak;\r\ncase bulk_latency:\r\ndefault:\r\nnew_itr = IXGBE_8K_ITR;\r\nbreak;\r\n}\r\nif (new_itr != q_vector->itr) {\r\nnew_itr = (10 * new_itr * q_vector->itr) /\r\n((9 * new_itr) + q_vector->itr);\r\nq_vector->itr = new_itr;\r\nixgbevf_write_eitr(q_vector);\r\n}\r\n}\r\nstatic irqreturn_t ixgbevf_msix_other(int irq, void *data)\r\n{\r\nstruct ixgbevf_adapter *adapter = data;\r\nstruct pci_dev *pdev = adapter->pdev;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 msg;\r\nbool got_ack = false;\r\nhw->mac.get_link_status = 1;\r\nif (!hw->mbx.ops.check_for_ack(hw))\r\ngot_ack = true;\r\nif (!hw->mbx.ops.check_for_msg(hw)) {\r\nhw->mbx.ops.read(hw, &msg, 1);\r\nif ((msg & IXGBE_MBVFICR_VFREQ_MASK) == IXGBE_PF_CONTROL_MSG) {\r\nmod_timer(&adapter->watchdog_timer,\r\nround_jiffies(jiffies + 1));\r\nadapter->link_up = false;\r\n}\r\nif (msg & IXGBE_VT_MSGTYPE_NACK)\r\ndev_info(&pdev->dev,\r\n"Last Request of type %2.2x to PF Nacked\n",\r\nmsg & 0xFF);\r\nhw->mbx.v2p_mailbox |= IXGBE_VFMAILBOX_PFSTS;\r\n}\r\nif (got_ack)\r\nhw->mbx.v2p_mailbox |= IXGBE_VFMAILBOX_PFACK;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIMS, adapter->eims_other);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t ixgbevf_msix_clean_rings(int irq, void *data)\r\n{\r\nstruct ixgbevf_q_vector *q_vector = data;\r\nif (q_vector->rx.ring || q_vector->tx.ring)\r\nnapi_schedule(&q_vector->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic inline void map_vector_to_rxq(struct ixgbevf_adapter *a, int v_idx,\r\nint r_idx)\r\n{\r\nstruct ixgbevf_q_vector *q_vector = a->q_vector[v_idx];\r\na->rx_ring[r_idx].next = q_vector->rx.ring;\r\nq_vector->rx.ring = &a->rx_ring[r_idx];\r\nq_vector->rx.count++;\r\n}\r\nstatic inline void map_vector_to_txq(struct ixgbevf_adapter *a, int v_idx,\r\nint t_idx)\r\n{\r\nstruct ixgbevf_q_vector *q_vector = a->q_vector[v_idx];\r\na->tx_ring[t_idx].next = q_vector->tx.ring;\r\nq_vector->tx.ring = &a->tx_ring[t_idx];\r\nq_vector->tx.count++;\r\n}\r\nstatic int ixgbevf_map_rings_to_vectors(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_vectors;\r\nint v_start = 0;\r\nint rxr_idx = 0, txr_idx = 0;\r\nint rxr_remaining = adapter->num_rx_queues;\r\nint txr_remaining = adapter->num_tx_queues;\r\nint i, j;\r\nint rqpv, tqpv;\r\nint err = 0;\r\nq_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nif (q_vectors == adapter->num_rx_queues + adapter->num_tx_queues) {\r\nfor (; rxr_idx < rxr_remaining; v_start++, rxr_idx++)\r\nmap_vector_to_rxq(adapter, v_start, rxr_idx);\r\nfor (; txr_idx < txr_remaining; v_start++, txr_idx++)\r\nmap_vector_to_txq(adapter, v_start, txr_idx);\r\ngoto out;\r\n}\r\nfor (i = v_start; i < q_vectors; i++) {\r\nrqpv = DIV_ROUND_UP(rxr_remaining, q_vectors - i);\r\nfor (j = 0; j < rqpv; j++) {\r\nmap_vector_to_rxq(adapter, i, rxr_idx);\r\nrxr_idx++;\r\nrxr_remaining--;\r\n}\r\n}\r\nfor (i = v_start; i < q_vectors; i++) {\r\ntqpv = DIV_ROUND_UP(txr_remaining, q_vectors - i);\r\nfor (j = 0; j < tqpv; j++) {\r\nmap_vector_to_txq(adapter, i, txr_idx);\r\ntxr_idx++;\r\ntxr_remaining--;\r\n}\r\n}\r\nout:\r\nreturn err;\r\n}\r\nstatic int ixgbevf_request_msix_irqs(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nint vector, err;\r\nint ri = 0, ti = 0;\r\nfor (vector = 0; vector < q_vectors; vector++) {\r\nstruct ixgbevf_q_vector *q_vector = adapter->q_vector[vector];\r\nstruct msix_entry *entry = &adapter->msix_entries[vector];\r\nif (q_vector->tx.ring && q_vector->rx.ring) {\r\nsnprintf(q_vector->name, sizeof(q_vector->name) - 1,\r\n"%s-%s-%d", netdev->name, "TxRx", ri++);\r\nti++;\r\n} else if (q_vector->rx.ring) {\r\nsnprintf(q_vector->name, sizeof(q_vector->name) - 1,\r\n"%s-%s-%d", netdev->name, "rx", ri++);\r\n} else if (q_vector->tx.ring) {\r\nsnprintf(q_vector->name, sizeof(q_vector->name) - 1,\r\n"%s-%s-%d", netdev->name, "tx", ti++);\r\n} else {\r\ncontinue;\r\n}\r\nerr = request_irq(entry->vector, &ixgbevf_msix_clean_rings, 0,\r\nq_vector->name, q_vector);\r\nif (err) {\r\nhw_dbg(&adapter->hw,\r\n"request_irq failed for MSIX interrupt "\r\n"Error: %d\n", err);\r\ngoto free_queue_irqs;\r\n}\r\n}\r\nerr = request_irq(adapter->msix_entries[vector].vector,\r\n&ixgbevf_msix_other, 0, netdev->name, adapter);\r\nif (err) {\r\nhw_dbg(&adapter->hw,\r\n"request_irq for msix_other failed: %d\n", err);\r\ngoto free_queue_irqs;\r\n}\r\nreturn 0;\r\nfree_queue_irqs:\r\nwhile (vector) {\r\nvector--;\r\nfree_irq(adapter->msix_entries[vector].vector,\r\nadapter->q_vector[vector]);\r\n}\r\nadapter->num_msix_vectors = 0;\r\nreturn err;\r\n}\r\nstatic inline void ixgbevf_reset_q_vectors(struct ixgbevf_adapter *adapter)\r\n{\r\nint i, q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (i = 0; i < q_vectors; i++) {\r\nstruct ixgbevf_q_vector *q_vector = adapter->q_vector[i];\r\nq_vector->rx.ring = NULL;\r\nq_vector->tx.ring = NULL;\r\nq_vector->rx.count = 0;\r\nq_vector->tx.count = 0;\r\n}\r\n}\r\nstatic int ixgbevf_request_irq(struct ixgbevf_adapter *adapter)\r\n{\r\nint err = 0;\r\nerr = ixgbevf_request_msix_irqs(adapter);\r\nif (err)\r\nhw_dbg(&adapter->hw,\r\n"request_irq failed, Error %d\n", err);\r\nreturn err;\r\n}\r\nstatic void ixgbevf_free_irq(struct ixgbevf_adapter *adapter)\r\n{\r\nint i, q_vectors;\r\nq_vectors = adapter->num_msix_vectors;\r\ni = q_vectors - 1;\r\nfree_irq(adapter->msix_entries[i].vector, adapter);\r\ni--;\r\nfor (; i >= 0; i--) {\r\nif (!adapter->q_vector[i]->rx.ring &&\r\n!adapter->q_vector[i]->tx.ring)\r\ncontinue;\r\nfree_irq(adapter->msix_entries[i].vector,\r\nadapter->q_vector[i]);\r\n}\r\nixgbevf_reset_q_vectors(adapter);\r\n}\r\nstatic inline void ixgbevf_irq_disable(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint i;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIAM, 0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIMC, ~0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIAC, 0);\r\nIXGBE_WRITE_FLUSH(hw);\r\nfor (i = 0; i < adapter->num_msix_vectors; i++)\r\nsynchronize_irq(adapter->msix_entries[i].vector);\r\n}\r\nstatic inline void ixgbevf_irq_enable(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIAM, adapter->eims_enable_mask);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIAC, adapter->eims_enable_mask);\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEIMS, adapter->eims_enable_mask);\r\n}\r\nstatic void ixgbevf_configure_tx(struct ixgbevf_adapter *adapter)\r\n{\r\nu64 tdba;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 i, j, tdlen, txctrl;\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nstruct ixgbevf_ring *ring = &adapter->tx_ring[i];\r\nj = ring->reg_idx;\r\ntdba = ring->dma;\r\ntdlen = ring->count * sizeof(union ixgbe_adv_tx_desc);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDBAL(j),\r\n(tdba & DMA_BIT_MASK(32)));\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDBAH(j), (tdba >> 32));\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDLEN(j), tdlen);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDH(j), 0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTDT(j), 0);\r\nadapter->tx_ring[i].head = IXGBE_VFTDH(j);\r\nadapter->tx_ring[i].tail = IXGBE_VFTDT(j);\r\ntxctrl = IXGBE_READ_REG(hw, IXGBE_VFDCA_TXCTRL(j));\r\ntxctrl &= ~IXGBE_DCA_TXCTRL_TX_WB_RO_EN;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFDCA_TXCTRL(j), txctrl);\r\n}\r\n}\r\nstatic void ixgbevf_configure_srrctl(struct ixgbevf_adapter *adapter, int index)\r\n{\r\nstruct ixgbevf_ring *rx_ring;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 srrctl;\r\nrx_ring = &adapter->rx_ring[index];\r\nsrrctl = IXGBE_SRRCTL_DROP_EN;\r\nsrrctl |= IXGBE_SRRCTL_DESCTYPE_ADV_ONEBUF;\r\nsrrctl |= ALIGN(rx_ring->rx_buf_len, 1024) >>\r\nIXGBE_SRRCTL_BSIZEPKT_SHIFT;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFSRRCTL(index), srrctl);\r\n}\r\nstatic void ixgbevf_set_rx_buffer_len(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct net_device *netdev = adapter->netdev;\r\nint max_frame = netdev->mtu + ETH_HLEN + ETH_FCS_LEN;\r\nint i;\r\nu16 rx_buf_len;\r\nixgbevf_rlpml_set_vf(hw, max_frame);\r\nmax_frame += VLAN_HLEN;\r\nif ((hw->mac.type == ixgbe_mac_X540_vf) &&\r\n(max_frame <= MAXIMUM_ETHERNET_VLAN_SIZE))\r\nrx_buf_len = MAXIMUM_ETHERNET_VLAN_SIZE;\r\nelse if (max_frame <= IXGBEVF_RXBUFFER_2K)\r\nrx_buf_len = IXGBEVF_RXBUFFER_2K;\r\nelse if (max_frame <= IXGBEVF_RXBUFFER_4K)\r\nrx_buf_len = IXGBEVF_RXBUFFER_4K;\r\nelse if (max_frame <= IXGBEVF_RXBUFFER_8K)\r\nrx_buf_len = IXGBEVF_RXBUFFER_8K;\r\nelse\r\nrx_buf_len = IXGBEVF_RXBUFFER_10K;\r\nfor (i = 0; i < adapter->num_rx_queues; i++)\r\nadapter->rx_ring[i].rx_buf_len = rx_buf_len;\r\n}\r\nstatic void ixgbevf_configure_rx(struct ixgbevf_adapter *adapter)\r\n{\r\nu64 rdba;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint i, j;\r\nu32 rdlen;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFPSRTYPE, 0);\r\nixgbevf_set_rx_buffer_len(adapter);\r\nrdlen = adapter->rx_ring[0].count * sizeof(union ixgbe_adv_rx_desc);\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nrdba = adapter->rx_ring[i].dma;\r\nj = adapter->rx_ring[i].reg_idx;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDBAL(j),\r\n(rdba & DMA_BIT_MASK(32)));\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDBAH(j), (rdba >> 32));\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDLEN(j), rdlen);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDH(j), 0);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRDT(j), 0);\r\nadapter->rx_ring[i].head = IXGBE_VFRDH(j);\r\nadapter->rx_ring[i].tail = IXGBE_VFRDT(j);\r\nixgbevf_configure_srrctl(adapter, j);\r\n}\r\n}\r\nstatic int ixgbevf_vlan_rx_add_vid(struct net_device *netdev,\r\n__be16 proto, u16 vid)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint err;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = hw->mac.ops.set_vfta(hw, vid, 0, true);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (err == IXGBE_ERR_MBX)\r\nreturn -EIO;\r\nif (err == IXGBE_ERR_INVALID_ARGUMENT)\r\nreturn -EACCES;\r\nset_bit(vid, adapter->active_vlans);\r\nreturn err;\r\n}\r\nstatic int ixgbevf_vlan_rx_kill_vid(struct net_device *netdev,\r\n__be16 proto, u16 vid)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint err = -EOPNOTSUPP;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = hw->mac.ops.set_vfta(hw, vid, 0, false);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nclear_bit(vid, adapter->active_vlans);\r\nreturn err;\r\n}\r\nstatic void ixgbevf_restore_vlan(struct ixgbevf_adapter *adapter)\r\n{\r\nu16 vid;\r\nfor_each_set_bit(vid, adapter->active_vlans, VLAN_N_VID)\r\nixgbevf_vlan_rx_add_vid(adapter->netdev,\r\nhtons(ETH_P_8021Q), vid);\r\n}\r\nstatic int ixgbevf_write_uc_addr_list(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint count = 0;\r\nif ((netdev_uc_count(netdev)) > 10) {\r\npr_err("Too many unicast filters - No Space\n");\r\nreturn -ENOSPC;\r\n}\r\nif (!netdev_uc_empty(netdev)) {\r\nstruct netdev_hw_addr *ha;\r\nnetdev_for_each_uc_addr(ha, netdev) {\r\nhw->mac.ops.set_uc_addr(hw, ++count, ha->addr);\r\nudelay(200);\r\n}\r\n} else {\r\nhw->mac.ops.set_uc_addr(hw, 0, NULL);\r\n}\r\nreturn count;\r\n}\r\nstatic void ixgbevf_set_rx_mode(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nhw->mac.ops.update_mc_addr_list(hw, netdev);\r\nixgbevf_write_uc_addr_list(netdev);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\n}\r\nstatic void ixgbevf_napi_enable_all(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_idx;\r\nstruct ixgbevf_q_vector *q_vector;\r\nint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (q_idx = 0; q_idx < q_vectors; q_idx++) {\r\nq_vector = adapter->q_vector[q_idx];\r\nnapi_enable(&q_vector->napi);\r\n}\r\n}\r\nstatic void ixgbevf_napi_disable_all(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_idx;\r\nstruct ixgbevf_q_vector *q_vector;\r\nint q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (q_idx = 0; q_idx < q_vectors; q_idx++) {\r\nq_vector = adapter->q_vector[q_idx];\r\nnapi_disable(&q_vector->napi);\r\n}\r\n}\r\nstatic void ixgbevf_configure(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nint i;\r\nixgbevf_set_rx_mode(netdev);\r\nixgbevf_restore_vlan(adapter);\r\nixgbevf_configure_tx(adapter);\r\nixgbevf_configure_rx(adapter);\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nstruct ixgbevf_ring *ring = &adapter->rx_ring[i];\r\nixgbevf_alloc_rx_buffers(adapter, ring,\r\nIXGBE_DESC_UNUSED(ring));\r\n}\r\n}\r\nstatic inline void ixgbevf_rx_desc_queue_enable(struct ixgbevf_adapter *adapter,\r\nint rxr)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint j = adapter->rx_ring[rxr].reg_idx;\r\nint k;\r\nfor (k = 0; k < IXGBE_MAX_RX_DESC_POLL; k++) {\r\nif (IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(j)) & IXGBE_RXDCTL_ENABLE)\r\nbreak;\r\nelse\r\nmsleep(1);\r\n}\r\nif (k >= IXGBE_MAX_RX_DESC_POLL) {\r\nhw_dbg(hw, "RXDCTL.ENABLE on Rx queue %d "\r\n"not set within the polling period\n", rxr);\r\n}\r\nixgbevf_release_rx_desc(hw, &adapter->rx_ring[rxr],\r\nadapter->rx_ring[rxr].count - 1);\r\n}\r\nstatic void ixgbevf_save_reset_stats(struct ixgbevf_adapter *adapter)\r\n{\r\nif (adapter->stats.vfgprc || adapter->stats.vfgptc) {\r\nadapter->stats.saved_reset_vfgprc += adapter->stats.vfgprc -\r\nadapter->stats.base_vfgprc;\r\nadapter->stats.saved_reset_vfgptc += adapter->stats.vfgptc -\r\nadapter->stats.base_vfgptc;\r\nadapter->stats.saved_reset_vfgorc += adapter->stats.vfgorc -\r\nadapter->stats.base_vfgorc;\r\nadapter->stats.saved_reset_vfgotc += adapter->stats.vfgotc -\r\nadapter->stats.base_vfgotc;\r\nadapter->stats.saved_reset_vfmprc += adapter->stats.vfmprc -\r\nadapter->stats.base_vfmprc;\r\n}\r\n}\r\nstatic void ixgbevf_init_last_counter_stats(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nadapter->stats.last_vfgprc = IXGBE_READ_REG(hw, IXGBE_VFGPRC);\r\nadapter->stats.last_vfgorc = IXGBE_READ_REG(hw, IXGBE_VFGORC_LSB);\r\nadapter->stats.last_vfgorc |=\r\n(((u64)(IXGBE_READ_REG(hw, IXGBE_VFGORC_MSB))) << 32);\r\nadapter->stats.last_vfgptc = IXGBE_READ_REG(hw, IXGBE_VFGPTC);\r\nadapter->stats.last_vfgotc = IXGBE_READ_REG(hw, IXGBE_VFGOTC_LSB);\r\nadapter->stats.last_vfgotc |=\r\n(((u64)(IXGBE_READ_REG(hw, IXGBE_VFGOTC_MSB))) << 32);\r\nadapter->stats.last_vfmprc = IXGBE_READ_REG(hw, IXGBE_VFMPRC);\r\nadapter->stats.base_vfgprc = adapter->stats.last_vfgprc;\r\nadapter->stats.base_vfgorc = adapter->stats.last_vfgorc;\r\nadapter->stats.base_vfgptc = adapter->stats.last_vfgptc;\r\nadapter->stats.base_vfgotc = adapter->stats.last_vfgotc;\r\nadapter->stats.base_vfmprc = adapter->stats.last_vfmprc;\r\n}\r\nstatic void ixgbevf_negotiate_api(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint api[] = { ixgbe_mbox_api_11,\r\nixgbe_mbox_api_10,\r\nixgbe_mbox_api_unknown };\r\nint err = 0, idx = 0;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nwhile (api[idx] != ixgbe_mbox_api_unknown) {\r\nerr = ixgbevf_negotiate_api_version(hw, api[idx]);\r\nif (!err)\r\nbreak;\r\nidx++;\r\n}\r\nspin_unlock_bh(&adapter->mbx_lock);\r\n}\r\nstatic void ixgbevf_up_complete(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint i, j = 0;\r\nint num_rx_rings = adapter->num_rx_queues;\r\nu32 txdctl, rxdctl;\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nj = adapter->tx_ring[i].reg_idx;\r\ntxdctl = IXGBE_READ_REG(hw, IXGBE_VFTXDCTL(j));\r\ntxdctl |= (8 << 16);\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(j), txdctl);\r\n}\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nj = adapter->tx_ring[i].reg_idx;\r\ntxdctl = IXGBE_READ_REG(hw, IXGBE_VFTXDCTL(j));\r\ntxdctl |= IXGBE_TXDCTL_ENABLE;\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(j), txdctl);\r\n}\r\nfor (i = 0; i < num_rx_rings; i++) {\r\nj = adapter->rx_ring[i].reg_idx;\r\nrxdctl = IXGBE_READ_REG(hw, IXGBE_VFRXDCTL(j));\r\nrxdctl |= IXGBE_RXDCTL_ENABLE | IXGBE_RXDCTL_VME;\r\nif (hw->mac.type == ixgbe_mac_X540_vf) {\r\nrxdctl &= ~IXGBE_RXDCTL_RLPMLMASK;\r\nrxdctl |= ((netdev->mtu + ETH_HLEN + ETH_FCS_LEN) |\r\nIXGBE_RXDCTL_RLPML_EN);\r\n}\r\nIXGBE_WRITE_REG(hw, IXGBE_VFRXDCTL(j), rxdctl);\r\nixgbevf_rx_desc_queue_enable(adapter, i);\r\n}\r\nixgbevf_configure_msix(adapter);\r\nspin_lock_bh(&adapter->mbx_lock);\r\nif (is_valid_ether_addr(hw->mac.addr))\r\nhw->mac.ops.set_rar(hw, 0, hw->mac.addr, 0);\r\nelse\r\nhw->mac.ops.set_rar(hw, 0, hw->mac.perm_addr, 0);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nclear_bit(__IXGBEVF_DOWN, &adapter->state);\r\nixgbevf_napi_enable_all(adapter);\r\nnetif_tx_start_all_queues(netdev);\r\nixgbevf_save_reset_stats(adapter);\r\nixgbevf_init_last_counter_stats(adapter);\r\nhw->mac.get_link_status = 1;\r\nmod_timer(&adapter->watchdog_timer, jiffies);\r\n}\r\nstatic int ixgbevf_reset_queues(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct ixgbevf_ring *rx_ring;\r\nunsigned int def_q = 0;\r\nunsigned int num_tcs = 0;\r\nunsigned int num_rx_queues = 1;\r\nint err, i;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = ixgbevf_get_queues(hw, &num_tcs, &def_q);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (err)\r\nreturn err;\r\nif (num_tcs > 1) {\r\nadapter->tx_ring[0].reg_idx = def_q;\r\nnum_rx_queues = num_tcs;\r\n}\r\nif (adapter->num_rx_queues == num_rx_queues)\r\nreturn 0;\r\nrx_ring = kcalloc(num_rx_queues,\r\nsizeof(struct ixgbevf_ring), GFP_KERNEL);\r\nif (!rx_ring)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < num_rx_queues; i++) {\r\nrx_ring[i].count = adapter->rx_ring_count;\r\nrx_ring[i].queue_index = i;\r\nrx_ring[i].reg_idx = i;\r\nrx_ring[i].dev = &adapter->pdev->dev;\r\nrx_ring[i].netdev = adapter->netdev;\r\nerr = ixgbevf_setup_rx_resources(adapter, &rx_ring[i]);\r\nif (err) {\r\nwhile (i) {\r\ni--;\r\nixgbevf_free_rx_resources(adapter, &rx_ring[i]);\r\n}\r\nkfree(rx_ring);\r\nreturn err;\r\n}\r\n}\r\nixgbevf_free_all_rx_resources(adapter);\r\nadapter->num_rx_queues = 0;\r\nkfree(adapter->rx_ring);\r\nadapter->rx_ring = rx_ring;\r\nadapter->num_rx_queues = num_rx_queues;\r\nixgbevf_reset_q_vectors(adapter);\r\nixgbevf_map_rings_to_vectors(adapter);\r\nreturn 0;\r\n}\r\nvoid ixgbevf_up(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nixgbevf_negotiate_api(adapter);\r\nixgbevf_reset_queues(adapter);\r\nixgbevf_configure(adapter);\r\nixgbevf_up_complete(adapter);\r\nIXGBE_READ_REG(hw, IXGBE_VTEICR);\r\nixgbevf_irq_enable(adapter);\r\n}\r\nstatic void ixgbevf_clean_rx_ring(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *rx_ring)\r\n{\r\nstruct pci_dev *pdev = adapter->pdev;\r\nunsigned long size;\r\nunsigned int i;\r\nif (!rx_ring->rx_buffer_info)\r\nreturn;\r\nfor (i = 0; i < rx_ring->count; i++) {\r\nstruct ixgbevf_rx_buffer *rx_buffer_info;\r\nrx_buffer_info = &rx_ring->rx_buffer_info[i];\r\nif (rx_buffer_info->dma) {\r\ndma_unmap_single(&pdev->dev, rx_buffer_info->dma,\r\nrx_ring->rx_buf_len,\r\nDMA_FROM_DEVICE);\r\nrx_buffer_info->dma = 0;\r\n}\r\nif (rx_buffer_info->skb) {\r\nstruct sk_buff *skb = rx_buffer_info->skb;\r\nrx_buffer_info->skb = NULL;\r\ndo {\r\nstruct sk_buff *this = skb;\r\nskb = IXGBE_CB(skb)->prev;\r\ndev_kfree_skb(this);\r\n} while (skb);\r\n}\r\n}\r\nsize = sizeof(struct ixgbevf_rx_buffer) * rx_ring->count;\r\nmemset(rx_ring->rx_buffer_info, 0, size);\r\nmemset(rx_ring->desc, 0, rx_ring->size);\r\nrx_ring->next_to_clean = 0;\r\nrx_ring->next_to_use = 0;\r\nif (rx_ring->head)\r\nwritel(0, adapter->hw.hw_addr + rx_ring->head);\r\nif (rx_ring->tail)\r\nwritel(0, adapter->hw.hw_addr + rx_ring->tail);\r\n}\r\nstatic void ixgbevf_clean_tx_ring(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *tx_ring)\r\n{\r\nstruct ixgbevf_tx_buffer *tx_buffer_info;\r\nunsigned long size;\r\nunsigned int i;\r\nif (!tx_ring->tx_buffer_info)\r\nreturn;\r\nfor (i = 0; i < tx_ring->count; i++) {\r\ntx_buffer_info = &tx_ring->tx_buffer_info[i];\r\nixgbevf_unmap_and_free_tx_resource(tx_ring, tx_buffer_info);\r\n}\r\nsize = sizeof(struct ixgbevf_tx_buffer) * tx_ring->count;\r\nmemset(tx_ring->tx_buffer_info, 0, size);\r\nmemset(tx_ring->desc, 0, tx_ring->size);\r\ntx_ring->next_to_use = 0;\r\ntx_ring->next_to_clean = 0;\r\nif (tx_ring->head)\r\nwritel(0, adapter->hw.hw_addr + tx_ring->head);\r\nif (tx_ring->tail)\r\nwritel(0, adapter->hw.hw_addr + tx_ring->tail);\r\n}\r\nstatic void ixgbevf_clean_all_rx_rings(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_rx_queues; i++)\r\nixgbevf_clean_rx_ring(adapter, &adapter->rx_ring[i]);\r\n}\r\nstatic void ixgbevf_clean_all_tx_rings(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_tx_queues; i++)\r\nixgbevf_clean_tx_ring(adapter, &adapter->tx_ring[i]);\r\n}\r\nvoid ixgbevf_down(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 txdctl;\r\nint i, j;\r\nset_bit(__IXGBEVF_DOWN, &adapter->state);\r\nnetif_tx_disable(netdev);\r\nmsleep(10);\r\nnetif_tx_stop_all_queues(netdev);\r\nixgbevf_irq_disable(adapter);\r\nixgbevf_napi_disable_all(adapter);\r\ndel_timer_sync(&adapter->watchdog_timer);\r\nwhile (adapter->flags & IXGBE_FLAG_IN_WATCHDOG_TASK)\r\nmsleep(1);\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nj = adapter->tx_ring[i].reg_idx;\r\ntxdctl = IXGBE_READ_REG(hw, IXGBE_VFTXDCTL(j));\r\nIXGBE_WRITE_REG(hw, IXGBE_VFTXDCTL(j),\r\n(txdctl & ~IXGBE_TXDCTL_ENABLE));\r\n}\r\nnetif_carrier_off(netdev);\r\nif (!pci_channel_offline(adapter->pdev))\r\nixgbevf_reset(adapter);\r\nixgbevf_clean_all_tx_rings(adapter);\r\nixgbevf_clean_all_rx_rings(adapter);\r\n}\r\nvoid ixgbevf_reinit_locked(struct ixgbevf_adapter *adapter)\r\n{\r\nWARN_ON(in_interrupt());\r\nwhile (test_and_set_bit(__IXGBEVF_RESETTING, &adapter->state))\r\nmsleep(1);\r\nixgbevf_down(adapter);\r\nixgbevf_up(adapter);\r\nclear_bit(__IXGBEVF_RESETTING, &adapter->state);\r\n}\r\nvoid ixgbevf_reset(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct net_device *netdev = adapter->netdev;\r\nif (hw->mac.ops.reset_hw(hw))\r\nhw_dbg(hw, "PF still resetting\n");\r\nelse\r\nhw->mac.ops.init_hw(hw);\r\nif (is_valid_ether_addr(adapter->hw.mac.addr)) {\r\nmemcpy(netdev->dev_addr, adapter->hw.mac.addr,\r\nnetdev->addr_len);\r\nmemcpy(netdev->perm_addr, adapter->hw.mac.addr,\r\nnetdev->addr_len);\r\n}\r\n}\r\nstatic int ixgbevf_acquire_msix_vectors(struct ixgbevf_adapter *adapter,\r\nint vectors)\r\n{\r\nint err = 0;\r\nint vector_threshold;\r\nvector_threshold = MIN_MSIX_COUNT;\r\nwhile (vectors >= vector_threshold) {\r\nerr = pci_enable_msix(adapter->pdev, adapter->msix_entries,\r\nvectors);\r\nif (!err || err < 0)\r\nbreak;\r\nelse\r\nvectors = err;\r\n}\r\nif (vectors < vector_threshold)\r\nerr = -ENOMEM;\r\nif (err) {\r\ndev_err(&adapter->pdev->dev,\r\n"Unable to allocate MSI-X interrupts\n");\r\nkfree(adapter->msix_entries);\r\nadapter->msix_entries = NULL;\r\n} else {\r\nadapter->num_msix_vectors = vectors;\r\n}\r\nreturn err;\r\n}\r\nstatic void ixgbevf_set_num_queues(struct ixgbevf_adapter *adapter)\r\n{\r\nadapter->num_rx_queues = 1;\r\nadapter->num_tx_queues = 1;\r\n}\r\nstatic int ixgbevf_alloc_queues(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nadapter->tx_ring = kcalloc(adapter->num_tx_queues,\r\nsizeof(struct ixgbevf_ring), GFP_KERNEL);\r\nif (!adapter->tx_ring)\r\ngoto err_tx_ring_allocation;\r\nadapter->rx_ring = kcalloc(adapter->num_rx_queues,\r\nsizeof(struct ixgbevf_ring), GFP_KERNEL);\r\nif (!adapter->rx_ring)\r\ngoto err_rx_ring_allocation;\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nadapter->tx_ring[i].count = adapter->tx_ring_count;\r\nadapter->tx_ring[i].queue_index = i;\r\nadapter->tx_ring[i].reg_idx = i;\r\nadapter->tx_ring[i].dev = &adapter->pdev->dev;\r\nadapter->tx_ring[i].netdev = adapter->netdev;\r\n}\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nadapter->rx_ring[i].count = adapter->rx_ring_count;\r\nadapter->rx_ring[i].queue_index = i;\r\nadapter->rx_ring[i].reg_idx = i;\r\nadapter->rx_ring[i].dev = &adapter->pdev->dev;\r\nadapter->rx_ring[i].netdev = adapter->netdev;\r\n}\r\nreturn 0;\r\nerr_rx_ring_allocation:\r\nkfree(adapter->tx_ring);\r\nerr_tx_ring_allocation:\r\nreturn -ENOMEM;\r\n}\r\nstatic int ixgbevf_set_interrupt_capability(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct net_device *netdev = adapter->netdev;\r\nint err = 0;\r\nint vector, v_budget;\r\nv_budget = max(adapter->num_rx_queues, adapter->num_tx_queues);\r\nv_budget = min_t(int, v_budget, num_online_cpus());\r\nv_budget += NON_Q_VECTORS;\r\nadapter->msix_entries = kcalloc(v_budget,\r\nsizeof(struct msix_entry), GFP_KERNEL);\r\nif (!adapter->msix_entries) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nfor (vector = 0; vector < v_budget; vector++)\r\nadapter->msix_entries[vector].entry = vector;\r\nerr = ixgbevf_acquire_msix_vectors(adapter, v_budget);\r\nif (err)\r\ngoto out;\r\nerr = netif_set_real_num_tx_queues(netdev, adapter->num_tx_queues);\r\nif (err)\r\ngoto out;\r\nerr = netif_set_real_num_rx_queues(netdev, adapter->num_rx_queues);\r\nout:\r\nreturn err;\r\n}\r\nstatic int ixgbevf_alloc_q_vectors(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_idx, num_q_vectors;\r\nstruct ixgbevf_q_vector *q_vector;\r\nnum_q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (q_idx = 0; q_idx < num_q_vectors; q_idx++) {\r\nq_vector = kzalloc(sizeof(struct ixgbevf_q_vector), GFP_KERNEL);\r\nif (!q_vector)\r\ngoto err_out;\r\nq_vector->adapter = adapter;\r\nq_vector->v_idx = q_idx;\r\nnetif_napi_add(adapter->netdev, &q_vector->napi,\r\nixgbevf_poll, 64);\r\nadapter->q_vector[q_idx] = q_vector;\r\n}\r\nreturn 0;\r\nerr_out:\r\nwhile (q_idx) {\r\nq_idx--;\r\nq_vector = adapter->q_vector[q_idx];\r\nnetif_napi_del(&q_vector->napi);\r\nkfree(q_vector);\r\nadapter->q_vector[q_idx] = NULL;\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void ixgbevf_free_q_vectors(struct ixgbevf_adapter *adapter)\r\n{\r\nint q_idx, num_q_vectors = adapter->num_msix_vectors - NON_Q_VECTORS;\r\nfor (q_idx = 0; q_idx < num_q_vectors; q_idx++) {\r\nstruct ixgbevf_q_vector *q_vector = adapter->q_vector[q_idx];\r\nadapter->q_vector[q_idx] = NULL;\r\nnetif_napi_del(&q_vector->napi);\r\nkfree(q_vector);\r\n}\r\n}\r\nstatic void ixgbevf_reset_interrupt_capability(struct ixgbevf_adapter *adapter)\r\n{\r\npci_disable_msix(adapter->pdev);\r\nkfree(adapter->msix_entries);\r\nadapter->msix_entries = NULL;\r\n}\r\nstatic int ixgbevf_init_interrupt_scheme(struct ixgbevf_adapter *adapter)\r\n{\r\nint err;\r\nixgbevf_set_num_queues(adapter);\r\nerr = ixgbevf_set_interrupt_capability(adapter);\r\nif (err) {\r\nhw_dbg(&adapter->hw,\r\n"Unable to setup interrupt capabilities\n");\r\ngoto err_set_interrupt;\r\n}\r\nerr = ixgbevf_alloc_q_vectors(adapter);\r\nif (err) {\r\nhw_dbg(&adapter->hw, "Unable to allocate memory for queue "\r\n"vectors\n");\r\ngoto err_alloc_q_vectors;\r\n}\r\nerr = ixgbevf_alloc_queues(adapter);\r\nif (err) {\r\npr_err("Unable to allocate memory for queues\n");\r\ngoto err_alloc_queues;\r\n}\r\nhw_dbg(&adapter->hw, "Multiqueue %s: Rx Queue count = %u, "\r\n"Tx Queue count = %u\n",\r\n(adapter->num_rx_queues > 1) ? "Enabled" :\r\n"Disabled", adapter->num_rx_queues, adapter->num_tx_queues);\r\nset_bit(__IXGBEVF_DOWN, &adapter->state);\r\nreturn 0;\r\nerr_alloc_queues:\r\nixgbevf_free_q_vectors(adapter);\r\nerr_alloc_q_vectors:\r\nixgbevf_reset_interrupt_capability(adapter);\r\nerr_set_interrupt:\r\nreturn err;\r\n}\r\nstatic void ixgbevf_clear_interrupt_scheme(struct ixgbevf_adapter *adapter)\r\n{\r\nadapter->num_tx_queues = 0;\r\nadapter->num_rx_queues = 0;\r\nixgbevf_free_q_vectors(adapter);\r\nixgbevf_reset_interrupt_capability(adapter);\r\n}\r\nstatic int ixgbevf_sw_init(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct pci_dev *pdev = adapter->pdev;\r\nstruct net_device *netdev = adapter->netdev;\r\nint err;\r\nhw->vendor_id = pdev->vendor;\r\nhw->device_id = pdev->device;\r\nhw->revision_id = pdev->revision;\r\nhw->subsystem_vendor_id = pdev->subsystem_vendor;\r\nhw->subsystem_device_id = pdev->subsystem_device;\r\nhw->mbx.ops.init_params(hw);\r\nhw->mac.max_tx_queues = 2;\r\nhw->mac.max_rx_queues = 2;\r\nerr = hw->mac.ops.reset_hw(hw);\r\nif (err) {\r\ndev_info(&pdev->dev,\r\n"PF still in reset state. Is the PF interface up?\n");\r\n} else {\r\nerr = hw->mac.ops.init_hw(hw);\r\nif (err) {\r\npr_err("init_shared_code failed: %d\n", err);\r\ngoto out;\r\n}\r\nerr = hw->mac.ops.get_mac_addr(hw, hw->mac.addr);\r\nif (err)\r\ndev_info(&pdev->dev, "Error reading MAC address\n");\r\nelse if (is_zero_ether_addr(adapter->hw.mac.addr))\r\ndev_info(&pdev->dev,\r\n"MAC address not assigned by administrator.\n");\r\nmemcpy(netdev->dev_addr, hw->mac.addr, netdev->addr_len);\r\n}\r\nif (!is_valid_ether_addr(netdev->dev_addr)) {\r\ndev_info(&pdev->dev, "Assigning random MAC address\n");\r\neth_hw_addr_random(netdev);\r\nmemcpy(hw->mac.addr, netdev->dev_addr, netdev->addr_len);\r\n}\r\nspin_lock_init(&adapter->mbx_lock);\r\nadapter->rx_itr_setting = 1;\r\nadapter->tx_itr_setting = 1;\r\nadapter->tx_ring_count = IXGBEVF_DEFAULT_TXD;\r\nadapter->rx_ring_count = IXGBEVF_DEFAULT_RXD;\r\nset_bit(__IXGBEVF_DOWN, &adapter->state);\r\nreturn 0;\r\nout:\r\nreturn err;\r\n}\r\nvoid ixgbevf_update_stats(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint i;\r\nif (!adapter->link_up)\r\nreturn;\r\nUPDATE_VF_COUNTER_32bit(IXGBE_VFGPRC, adapter->stats.last_vfgprc,\r\nadapter->stats.vfgprc);\r\nUPDATE_VF_COUNTER_32bit(IXGBE_VFGPTC, adapter->stats.last_vfgptc,\r\nadapter->stats.vfgptc);\r\nUPDATE_VF_COUNTER_36bit(IXGBE_VFGORC_LSB, IXGBE_VFGORC_MSB,\r\nadapter->stats.last_vfgorc,\r\nadapter->stats.vfgorc);\r\nUPDATE_VF_COUNTER_36bit(IXGBE_VFGOTC_LSB, IXGBE_VFGOTC_MSB,\r\nadapter->stats.last_vfgotc,\r\nadapter->stats.vfgotc);\r\nUPDATE_VF_COUNTER_32bit(IXGBE_VFMPRC, adapter->stats.last_vfmprc,\r\nadapter->stats.vfmprc);\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nadapter->hw_csum_rx_error +=\r\nadapter->rx_ring[i].hw_csum_rx_error;\r\nadapter->hw_csum_rx_good +=\r\nadapter->rx_ring[i].hw_csum_rx_good;\r\nadapter->rx_ring[i].hw_csum_rx_error = 0;\r\nadapter->rx_ring[i].hw_csum_rx_good = 0;\r\n}\r\n}\r\nstatic void ixgbevf_watchdog(unsigned long data)\r\n{\r\nstruct ixgbevf_adapter *adapter = (struct ixgbevf_adapter *)data;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 eics = 0;\r\nint i;\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state))\r\ngoto watchdog_short_circuit;\r\nfor (i = 0; i < adapter->num_msix_vectors - NON_Q_VECTORS; i++) {\r\nstruct ixgbevf_q_vector *qv = adapter->q_vector[i];\r\nif (qv->rx.ring || qv->tx.ring)\r\neics |= 1 << i;\r\n}\r\nIXGBE_WRITE_REG(hw, IXGBE_VTEICS, eics);\r\nwatchdog_short_circuit:\r\nschedule_work(&adapter->watchdog_task);\r\n}\r\nstatic void ixgbevf_tx_timeout(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nschedule_work(&adapter->reset_task);\r\n}\r\nstatic void ixgbevf_reset_task(struct work_struct *work)\r\n{\r\nstruct ixgbevf_adapter *adapter;\r\nadapter = container_of(work, struct ixgbevf_adapter, reset_task);\r\nif (test_bit(__IXGBEVF_DOWN, &adapter->state) ||\r\ntest_bit(__IXGBEVF_RESETTING, &adapter->state))\r\nreturn;\r\nadapter->tx_timeout_count++;\r\nixgbevf_reinit_locked(adapter);\r\n}\r\nstatic void ixgbevf_watchdog_task(struct work_struct *work)\r\n{\r\nstruct ixgbevf_adapter *adapter = container_of(work,\r\nstruct ixgbevf_adapter,\r\nwatchdog_task);\r\nstruct net_device *netdev = adapter->netdev;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu32 link_speed = adapter->link_speed;\r\nbool link_up = adapter->link_up;\r\ns32 need_reset;\r\nadapter->flags |= IXGBE_FLAG_IN_WATCHDOG_TASK;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nneed_reset = hw->mac.ops.check_link(hw, &link_speed, &link_up, false);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (need_reset) {\r\nadapter->link_up = link_up;\r\nadapter->link_speed = link_speed;\r\nnetif_carrier_off(netdev);\r\nnetif_tx_stop_all_queues(netdev);\r\nschedule_work(&adapter->reset_task);\r\ngoto pf_has_reset;\r\n}\r\nadapter->link_up = link_up;\r\nadapter->link_speed = link_speed;\r\nif (link_up) {\r\nif (!netif_carrier_ok(netdev)) {\r\nchar *link_speed_string;\r\nswitch (link_speed) {\r\ncase IXGBE_LINK_SPEED_10GB_FULL:\r\nlink_speed_string = "10 Gbps";\r\nbreak;\r\ncase IXGBE_LINK_SPEED_1GB_FULL:\r\nlink_speed_string = "1 Gbps";\r\nbreak;\r\ncase IXGBE_LINK_SPEED_100_FULL:\r\nlink_speed_string = "100 Mbps";\r\nbreak;\r\ndefault:\r\nlink_speed_string = "unknown speed";\r\nbreak;\r\n}\r\ndev_info(&adapter->pdev->dev,\r\n"NIC Link is Up, %s\n", link_speed_string);\r\nnetif_carrier_on(netdev);\r\nnetif_tx_wake_all_queues(netdev);\r\n}\r\n} else {\r\nadapter->link_up = false;\r\nadapter->link_speed = 0;\r\nif (netif_carrier_ok(netdev)) {\r\ndev_info(&adapter->pdev->dev, "NIC Link is Down\n");\r\nnetif_carrier_off(netdev);\r\nnetif_tx_stop_all_queues(netdev);\r\n}\r\n}\r\nixgbevf_update_stats(adapter);\r\npf_has_reset:\r\nif (!test_bit(__IXGBEVF_DOWN, &adapter->state))\r\nmod_timer(&adapter->watchdog_timer,\r\nround_jiffies(jiffies + (2 * HZ)));\r\nadapter->flags &= ~IXGBE_FLAG_IN_WATCHDOG_TASK;\r\n}\r\nvoid ixgbevf_free_tx_resources(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *tx_ring)\r\n{\r\nstruct pci_dev *pdev = adapter->pdev;\r\nixgbevf_clean_tx_ring(adapter, tx_ring);\r\nvfree(tx_ring->tx_buffer_info);\r\ntx_ring->tx_buffer_info = NULL;\r\ndma_free_coherent(&pdev->dev, tx_ring->size, tx_ring->desc,\r\ntx_ring->dma);\r\ntx_ring->desc = NULL;\r\n}\r\nstatic void ixgbevf_free_all_tx_resources(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_tx_queues; i++)\r\nif (adapter->tx_ring[i].desc)\r\nixgbevf_free_tx_resources(adapter,\r\n&adapter->tx_ring[i]);\r\n}\r\nint ixgbevf_setup_tx_resources(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *tx_ring)\r\n{\r\nstruct pci_dev *pdev = adapter->pdev;\r\nint size;\r\nsize = sizeof(struct ixgbevf_tx_buffer) * tx_ring->count;\r\ntx_ring->tx_buffer_info = vzalloc(size);\r\nif (!tx_ring->tx_buffer_info)\r\ngoto err;\r\ntx_ring->size = tx_ring->count * sizeof(union ixgbe_adv_tx_desc);\r\ntx_ring->size = ALIGN(tx_ring->size, 4096);\r\ntx_ring->desc = dma_alloc_coherent(&pdev->dev, tx_ring->size,\r\n&tx_ring->dma, GFP_KERNEL);\r\nif (!tx_ring->desc)\r\ngoto err;\r\ntx_ring->next_to_use = 0;\r\ntx_ring->next_to_clean = 0;\r\nreturn 0;\r\nerr:\r\nvfree(tx_ring->tx_buffer_info);\r\ntx_ring->tx_buffer_info = NULL;\r\nhw_dbg(&adapter->hw, "Unable to allocate memory for the transmit "\r\n"descriptor ring\n");\r\nreturn -ENOMEM;\r\n}\r\nstatic int ixgbevf_setup_all_tx_resources(struct ixgbevf_adapter *adapter)\r\n{\r\nint i, err = 0;\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nerr = ixgbevf_setup_tx_resources(adapter, &adapter->tx_ring[i]);\r\nif (!err)\r\ncontinue;\r\nhw_dbg(&adapter->hw,\r\n"Allocation for Tx Queue %u failed\n", i);\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nint ixgbevf_setup_rx_resources(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *rx_ring)\r\n{\r\nstruct pci_dev *pdev = adapter->pdev;\r\nint size;\r\nsize = sizeof(struct ixgbevf_rx_buffer) * rx_ring->count;\r\nrx_ring->rx_buffer_info = vzalloc(size);\r\nif (!rx_ring->rx_buffer_info)\r\ngoto alloc_failed;\r\nrx_ring->size = rx_ring->count * sizeof(union ixgbe_adv_rx_desc);\r\nrx_ring->size = ALIGN(rx_ring->size, 4096);\r\nrx_ring->desc = dma_alloc_coherent(&pdev->dev, rx_ring->size,\r\n&rx_ring->dma, GFP_KERNEL);\r\nif (!rx_ring->desc) {\r\nvfree(rx_ring->rx_buffer_info);\r\nrx_ring->rx_buffer_info = NULL;\r\ngoto alloc_failed;\r\n}\r\nrx_ring->next_to_clean = 0;\r\nrx_ring->next_to_use = 0;\r\nreturn 0;\r\nalloc_failed:\r\nreturn -ENOMEM;\r\n}\r\nstatic int ixgbevf_setup_all_rx_resources(struct ixgbevf_adapter *adapter)\r\n{\r\nint i, err = 0;\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nerr = ixgbevf_setup_rx_resources(adapter, &adapter->rx_ring[i]);\r\nif (!err)\r\ncontinue;\r\nhw_dbg(&adapter->hw,\r\n"Allocation for Rx Queue %u failed\n", i);\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nvoid ixgbevf_free_rx_resources(struct ixgbevf_adapter *adapter,\r\nstruct ixgbevf_ring *rx_ring)\r\n{\r\nstruct pci_dev *pdev = adapter->pdev;\r\nixgbevf_clean_rx_ring(adapter, rx_ring);\r\nvfree(rx_ring->rx_buffer_info);\r\nrx_ring->rx_buffer_info = NULL;\r\ndma_free_coherent(&pdev->dev, rx_ring->size, rx_ring->desc,\r\nrx_ring->dma);\r\nrx_ring->desc = NULL;\r\n}\r\nstatic void ixgbevf_free_all_rx_resources(struct ixgbevf_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_rx_queues; i++)\r\nif (adapter->rx_ring[i].desc)\r\nixgbevf_free_rx_resources(adapter,\r\n&adapter->rx_ring[i]);\r\n}\r\nstatic int ixgbevf_setup_queues(struct ixgbevf_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct ixgbevf_ring *rx_ring;\r\nunsigned int def_q = 0;\r\nunsigned int num_tcs = 0;\r\nunsigned int num_rx_queues = 1;\r\nint err, i;\r\nspin_lock_bh(&adapter->mbx_lock);\r\nerr = ixgbevf_get_queues(hw, &num_tcs, &def_q);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nif (err)\r\nreturn err;\r\nif (num_tcs > 1) {\r\nadapter->tx_ring[0].reg_idx = def_q;\r\nnum_rx_queues = num_tcs;\r\n}\r\nif (adapter->num_rx_queues == num_rx_queues)\r\nreturn 0;\r\nrx_ring = kcalloc(num_rx_queues,\r\nsizeof(struct ixgbevf_ring), GFP_KERNEL);\r\nif (!rx_ring)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < num_rx_queues; i++) {\r\nrx_ring[i].count = adapter->rx_ring_count;\r\nrx_ring[i].queue_index = i;\r\nrx_ring[i].reg_idx = i;\r\nrx_ring[i].dev = &adapter->pdev->dev;\r\nrx_ring[i].netdev = adapter->netdev;\r\n}\r\nadapter->num_rx_queues = 0;\r\nkfree(adapter->rx_ring);\r\nadapter->rx_ring = rx_ring;\r\nadapter->num_rx_queues = num_rx_queues;\r\nreturn 0;\r\n}\r\nstatic int ixgbevf_open(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint err;\r\nif (!adapter->num_msix_vectors)\r\nreturn -ENOMEM;\r\nif (test_bit(__IXGBEVF_TESTING, &adapter->state))\r\nreturn -EBUSY;\r\nif (hw->adapter_stopped) {\r\nixgbevf_reset(adapter);\r\nif (hw->adapter_stopped) {\r\nerr = IXGBE_ERR_MBX;\r\npr_err("Unable to start - perhaps the PF Driver isn't "\r\n"up yet\n");\r\ngoto err_setup_reset;\r\n}\r\n}\r\nixgbevf_negotiate_api(adapter);\r\nerr = ixgbevf_setup_queues(adapter);\r\nif (err)\r\ngoto err_setup_queues;\r\nerr = ixgbevf_setup_all_tx_resources(adapter);\r\nif (err)\r\ngoto err_setup_tx;\r\nerr = ixgbevf_setup_all_rx_resources(adapter);\r\nif (err)\r\ngoto err_setup_rx;\r\nixgbevf_configure(adapter);\r\nixgbevf_map_rings_to_vectors(adapter);\r\nixgbevf_up_complete(adapter);\r\nIXGBE_READ_REG(hw, IXGBE_VTEICR);\r\nerr = ixgbevf_request_irq(adapter);\r\nif (err)\r\ngoto err_req_irq;\r\nixgbevf_irq_enable(adapter);\r\nreturn 0;\r\nerr_req_irq:\r\nixgbevf_down(adapter);\r\nerr_setup_rx:\r\nixgbevf_free_all_rx_resources(adapter);\r\nerr_setup_tx:\r\nixgbevf_free_all_tx_resources(adapter);\r\nerr_setup_queues:\r\nixgbevf_reset(adapter);\r\nerr_setup_reset:\r\nreturn err;\r\n}\r\nstatic int ixgbevf_close(struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nixgbevf_down(adapter);\r\nixgbevf_free_irq(adapter);\r\nixgbevf_free_all_tx_resources(adapter);\r\nixgbevf_free_all_rx_resources(adapter);\r\nreturn 0;\r\n}\r\nstatic void ixgbevf_tx_ctxtdesc(struct ixgbevf_ring *tx_ring,\r\nu32 vlan_macip_lens, u32 type_tucmd,\r\nu32 mss_l4len_idx)\r\n{\r\nstruct ixgbe_adv_tx_context_desc *context_desc;\r\nu16 i = tx_ring->next_to_use;\r\ncontext_desc = IXGBEVF_TX_CTXTDESC(tx_ring, i);\r\ni++;\r\ntx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\r\ntype_tucmd |= IXGBE_TXD_CMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;\r\ncontext_desc->vlan_macip_lens = cpu_to_le32(vlan_macip_lens);\r\ncontext_desc->seqnum_seed = 0;\r\ncontext_desc->type_tucmd_mlhl = cpu_to_le32(type_tucmd);\r\ncontext_desc->mss_l4len_idx = cpu_to_le32(mss_l4len_idx);\r\n}\r\nstatic int ixgbevf_tso(struct ixgbevf_ring *tx_ring,\r\nstruct sk_buff *skb, u32 tx_flags, u8 *hdr_len)\r\n{\r\nu32 vlan_macip_lens, type_tucmd;\r\nu32 mss_l4len_idx, l4len;\r\nif (!skb_is_gso(skb))\r\nreturn 0;\r\nif (skb_header_cloned(skb)) {\r\nint err = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\r\nif (err)\r\nreturn err;\r\n}\r\ntype_tucmd = IXGBE_ADVTXD_TUCMD_L4T_TCP;\r\nif (skb->protocol == htons(ETH_P_IP)) {\r\nstruct iphdr *iph = ip_hdr(skb);\r\niph->tot_len = 0;\r\niph->check = 0;\r\ntcp_hdr(skb)->check = ~csum_tcpudp_magic(iph->saddr,\r\niph->daddr, 0,\r\nIPPROTO_TCP,\r\n0);\r\ntype_tucmd |= IXGBE_ADVTXD_TUCMD_IPV4;\r\n} else if (skb_is_gso_v6(skb)) {\r\nipv6_hdr(skb)->payload_len = 0;\r\ntcp_hdr(skb)->check =\r\n~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\r\n&ipv6_hdr(skb)->daddr,\r\n0, IPPROTO_TCP, 0);\r\n}\r\nl4len = tcp_hdrlen(skb);\r\n*hdr_len += l4len;\r\n*hdr_len = skb_transport_offset(skb) + l4len;\r\nmss_l4len_idx = l4len << IXGBE_ADVTXD_L4LEN_SHIFT;\r\nmss_l4len_idx |= skb_shinfo(skb)->gso_size << IXGBE_ADVTXD_MSS_SHIFT;\r\nmss_l4len_idx |= 1 << IXGBE_ADVTXD_IDX_SHIFT;\r\nvlan_macip_lens = skb_network_header_len(skb);\r\nvlan_macip_lens |= skb_network_offset(skb) << IXGBE_ADVTXD_MACLEN_SHIFT;\r\nvlan_macip_lens |= tx_flags & IXGBE_TX_FLAGS_VLAN_MASK;\r\nixgbevf_tx_ctxtdesc(tx_ring, vlan_macip_lens,\r\ntype_tucmd, mss_l4len_idx);\r\nreturn 1;\r\n}\r\nstatic bool ixgbevf_tx_csum(struct ixgbevf_ring *tx_ring,\r\nstruct sk_buff *skb, u32 tx_flags)\r\n{\r\nu32 vlan_macip_lens = 0;\r\nu32 mss_l4len_idx = 0;\r\nu32 type_tucmd = 0;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nu8 l4_hdr = 0;\r\nswitch (skb->protocol) {\r\ncase __constant_htons(ETH_P_IP):\r\nvlan_macip_lens |= skb_network_header_len(skb);\r\ntype_tucmd |= IXGBE_ADVTXD_TUCMD_IPV4;\r\nl4_hdr = ip_hdr(skb)->protocol;\r\nbreak;\r\ncase __constant_htons(ETH_P_IPV6):\r\nvlan_macip_lens |= skb_network_header_len(skb);\r\nl4_hdr = ipv6_hdr(skb)->nexthdr;\r\nbreak;\r\ndefault:\r\nif (unlikely(net_ratelimit())) {\r\ndev_warn(tx_ring->dev,\r\n"partial checksum but proto=%x!\n",\r\nskb->protocol);\r\n}\r\nbreak;\r\n}\r\nswitch (l4_hdr) {\r\ncase IPPROTO_TCP:\r\ntype_tucmd |= IXGBE_ADVTXD_TUCMD_L4T_TCP;\r\nmss_l4len_idx = tcp_hdrlen(skb) <<\r\nIXGBE_ADVTXD_L4LEN_SHIFT;\r\nbreak;\r\ncase IPPROTO_SCTP:\r\ntype_tucmd |= IXGBE_ADVTXD_TUCMD_L4T_SCTP;\r\nmss_l4len_idx = sizeof(struct sctphdr) <<\r\nIXGBE_ADVTXD_L4LEN_SHIFT;\r\nbreak;\r\ncase IPPROTO_UDP:\r\nmss_l4len_idx = sizeof(struct udphdr) <<\r\nIXGBE_ADVTXD_L4LEN_SHIFT;\r\nbreak;\r\ndefault:\r\nif (unlikely(net_ratelimit())) {\r\ndev_warn(tx_ring->dev,\r\n"partial checksum but l4 proto=%x!\n",\r\nl4_hdr);\r\n}\r\nbreak;\r\n}\r\n}\r\nvlan_macip_lens |= skb_network_offset(skb) << IXGBE_ADVTXD_MACLEN_SHIFT;\r\nvlan_macip_lens |= tx_flags & IXGBE_TX_FLAGS_VLAN_MASK;\r\nixgbevf_tx_ctxtdesc(tx_ring, vlan_macip_lens,\r\ntype_tucmd, mss_l4len_idx);\r\nreturn (skb->ip_summed == CHECKSUM_PARTIAL);\r\n}\r\nstatic int ixgbevf_tx_map(struct ixgbevf_ring *tx_ring,\r\nstruct sk_buff *skb, u32 tx_flags)\r\n{\r\nstruct ixgbevf_tx_buffer *tx_buffer_info;\r\nunsigned int len;\r\nunsigned int total = skb->len;\r\nunsigned int offset = 0, size;\r\nint count = 0;\r\nunsigned int nr_frags = skb_shinfo(skb)->nr_frags;\r\nunsigned int f;\r\nint i;\r\ni = tx_ring->next_to_use;\r\nlen = min(skb_headlen(skb), total);\r\nwhile (len) {\r\ntx_buffer_info = &tx_ring->tx_buffer_info[i];\r\nsize = min(len, (unsigned int)IXGBE_MAX_DATA_PER_TXD);\r\ntx_buffer_info->length = size;\r\ntx_buffer_info->mapped_as_page = false;\r\ntx_buffer_info->dma = dma_map_single(tx_ring->dev,\r\nskb->data + offset,\r\nsize, DMA_TO_DEVICE);\r\nif (dma_mapping_error(tx_ring->dev, tx_buffer_info->dma))\r\ngoto dma_error;\r\nlen -= size;\r\ntotal -= size;\r\noffset += size;\r\ncount++;\r\ni++;\r\nif (i == tx_ring->count)\r\ni = 0;\r\n}\r\nfor (f = 0; f < nr_frags; f++) {\r\nconst struct skb_frag_struct *frag;\r\nfrag = &skb_shinfo(skb)->frags[f];\r\nlen = min((unsigned int)skb_frag_size(frag), total);\r\noffset = 0;\r\nwhile (len) {\r\ntx_buffer_info = &tx_ring->tx_buffer_info[i];\r\nsize = min(len, (unsigned int)IXGBE_MAX_DATA_PER_TXD);\r\ntx_buffer_info->length = size;\r\ntx_buffer_info->dma =\r\nskb_frag_dma_map(tx_ring->dev, frag,\r\noffset, size, DMA_TO_DEVICE);\r\nif (dma_mapping_error(tx_ring->dev,\r\ntx_buffer_info->dma))\r\ngoto dma_error;\r\ntx_buffer_info->mapped_as_page = true;\r\nlen -= size;\r\ntotal -= size;\r\noffset += size;\r\ncount++;\r\ni++;\r\nif (i == tx_ring->count)\r\ni = 0;\r\n}\r\nif (total == 0)\r\nbreak;\r\n}\r\nif (i == 0)\r\ni = tx_ring->count - 1;\r\nelse\r\ni = i - 1;\r\ntx_ring->tx_buffer_info[i].skb = skb;\r\nreturn count;\r\ndma_error:\r\ndev_err(tx_ring->dev, "TX DMA map failed\n");\r\ntx_buffer_info->dma = 0;\r\ncount--;\r\nwhile (count >= 0) {\r\ncount--;\r\ni--;\r\nif (i < 0)\r\ni += tx_ring->count;\r\ntx_buffer_info = &tx_ring->tx_buffer_info[i];\r\nixgbevf_unmap_and_free_tx_resource(tx_ring, tx_buffer_info);\r\n}\r\nreturn count;\r\n}\r\nstatic void ixgbevf_tx_queue(struct ixgbevf_ring *tx_ring, int tx_flags,\r\nint count, unsigned int first, u32 paylen,\r\nu8 hdr_len)\r\n{\r\nunion ixgbe_adv_tx_desc *tx_desc = NULL;\r\nstruct ixgbevf_tx_buffer *tx_buffer_info;\r\nu32 olinfo_status = 0, cmd_type_len = 0;\r\nunsigned int i;\r\nu32 txd_cmd = IXGBE_TXD_CMD_EOP | IXGBE_TXD_CMD_RS | IXGBE_TXD_CMD_IFCS;\r\ncmd_type_len |= IXGBE_ADVTXD_DTYP_DATA;\r\ncmd_type_len |= IXGBE_ADVTXD_DCMD_IFCS | IXGBE_ADVTXD_DCMD_DEXT;\r\nif (tx_flags & IXGBE_TX_FLAGS_VLAN)\r\ncmd_type_len |= IXGBE_ADVTXD_DCMD_VLE;\r\nif (tx_flags & IXGBE_TX_FLAGS_CSUM)\r\nolinfo_status |= IXGBE_ADVTXD_POPTS_TXSM;\r\nif (tx_flags & IXGBE_TX_FLAGS_TSO) {\r\ncmd_type_len |= IXGBE_ADVTXD_DCMD_TSE;\r\nolinfo_status |= (1 << IXGBE_ADVTXD_IDX_SHIFT);\r\nif (tx_flags & IXGBE_TX_FLAGS_IPV4)\r\nolinfo_status |= IXGBE_ADVTXD_POPTS_IXSM;\r\n}\r\nolinfo_status |= IXGBE_ADVTXD_CC;\r\nolinfo_status |= ((paylen - hdr_len) << IXGBE_ADVTXD_PAYLEN_SHIFT);\r\ni = tx_ring->next_to_use;\r\nwhile (count--) {\r\ntx_buffer_info = &tx_ring->tx_buffer_info[i];\r\ntx_desc = IXGBEVF_TX_DESC(tx_ring, i);\r\ntx_desc->read.buffer_addr = cpu_to_le64(tx_buffer_info->dma);\r\ntx_desc->read.cmd_type_len =\r\ncpu_to_le32(cmd_type_len | tx_buffer_info->length);\r\ntx_desc->read.olinfo_status = cpu_to_le32(olinfo_status);\r\ni++;\r\nif (i == tx_ring->count)\r\ni = 0;\r\n}\r\ntx_desc->read.cmd_type_len |= cpu_to_le32(txd_cmd);\r\ntx_ring->tx_buffer_info[first].time_stamp = jiffies;\r\nwmb();\r\ntx_ring->tx_buffer_info[first].next_to_watch = tx_desc;\r\ntx_ring->next_to_use = i;\r\n}\r\nstatic int __ixgbevf_maybe_stop_tx(struct ixgbevf_ring *tx_ring, int size)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(tx_ring->netdev);\r\nnetif_stop_subqueue(tx_ring->netdev, tx_ring->queue_index);\r\nsmp_mb();\r\nif (likely(IXGBE_DESC_UNUSED(tx_ring) < size))\r\nreturn -EBUSY;\r\nnetif_start_subqueue(tx_ring->netdev, tx_ring->queue_index);\r\n++adapter->restart_queue;\r\nreturn 0;\r\n}\r\nstatic int ixgbevf_maybe_stop_tx(struct ixgbevf_ring *tx_ring, int size)\r\n{\r\nif (likely(IXGBE_DESC_UNUSED(tx_ring) >= size))\r\nreturn 0;\r\nreturn __ixgbevf_maybe_stop_tx(tx_ring, size);\r\n}\r\nstatic int ixgbevf_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbevf_ring *tx_ring;\r\nunsigned int first;\r\nunsigned int tx_flags = 0;\r\nu8 hdr_len = 0;\r\nint r_idx = 0, tso;\r\nu16 count = TXD_USE_COUNT(skb_headlen(skb));\r\n#if PAGE_SIZE > IXGBE_MAX_DATA_PER_TXD\r\nunsigned short f;\r\n#endif\r\nu8 *dst_mac = skb_header_pointer(skb, 0, 0, NULL);\r\nif (!dst_mac || is_link_local_ether_addr(dst_mac)) {\r\ndev_kfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\ntx_ring = &adapter->tx_ring[r_idx];\r\n#if PAGE_SIZE > IXGBE_MAX_DATA_PER_TXD\r\nfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++)\r\ncount += TXD_USE_COUNT(skb_shinfo(skb)->frags[f].size);\r\n#else\r\ncount += skb_shinfo(skb)->nr_frags;\r\n#endif\r\nif (ixgbevf_maybe_stop_tx(tx_ring, count + 3)) {\r\nadapter->tx_busy++;\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nif (vlan_tx_tag_present(skb)) {\r\ntx_flags |= vlan_tx_tag_get(skb);\r\ntx_flags <<= IXGBE_TX_FLAGS_VLAN_SHIFT;\r\ntx_flags |= IXGBE_TX_FLAGS_VLAN;\r\n}\r\nfirst = tx_ring->next_to_use;\r\nif (skb->protocol == htons(ETH_P_IP))\r\ntx_flags |= IXGBE_TX_FLAGS_IPV4;\r\ntso = ixgbevf_tso(tx_ring, skb, tx_flags, &hdr_len);\r\nif (tso < 0) {\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nif (tso)\r\ntx_flags |= IXGBE_TX_FLAGS_TSO | IXGBE_TX_FLAGS_CSUM;\r\nelse if (ixgbevf_tx_csum(tx_ring, skb, tx_flags))\r\ntx_flags |= IXGBE_TX_FLAGS_CSUM;\r\nixgbevf_tx_queue(tx_ring, tx_flags,\r\nixgbevf_tx_map(tx_ring, skb, tx_flags),\r\nfirst, skb->len, hdr_len);\r\nwritel(tx_ring->next_to_use, adapter->hw.hw_addr + tx_ring->tail);\r\nixgbevf_maybe_stop_tx(tx_ring, DESC_NEEDED);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int ixgbevf_set_mac(struct net_device *netdev, void *p)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nstruct sockaddr *addr = p;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(netdev->dev_addr, addr->sa_data, netdev->addr_len);\r\nmemcpy(hw->mac.addr, addr->sa_data, netdev->addr_len);\r\nspin_lock_bh(&adapter->mbx_lock);\r\nhw->mac.ops.set_rar(hw, 0, hw->mac.addr, 0);\r\nspin_unlock_bh(&adapter->mbx_lock);\r\nreturn 0;\r\n}\r\nstatic int ixgbevf_change_mtu(struct net_device *netdev, int new_mtu)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nint max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN;\r\nint max_possible_frame = MAXIMUM_ETHERNET_VLAN_SIZE;\r\nswitch (adapter->hw.api_version) {\r\ncase ixgbe_mbox_api_11:\r\nmax_possible_frame = IXGBE_MAX_JUMBO_FRAME_SIZE;\r\nbreak;\r\ndefault:\r\nif (adapter->hw.mac.type == ixgbe_mac_X540_vf)\r\nmax_possible_frame = IXGBE_MAX_JUMBO_FRAME_SIZE;\r\nbreak;\r\n}\r\nif ((new_mtu < 68) || (max_frame > max_possible_frame))\r\nreturn -EINVAL;\r\nhw_dbg(&adapter->hw, "changing MTU from %d to %d\n",\r\nnetdev->mtu, new_mtu);\r\nnetdev->mtu = new_mtu;\r\nif (netif_running(netdev))\r\nixgbevf_reinit_locked(adapter);\r\nreturn 0;\r\n}\r\nstatic int ixgbevf_suspend(struct pci_dev *pdev, pm_message_t state)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\n#ifdef CONFIG_PM\r\nint retval = 0;\r\n#endif\r\nnetif_device_detach(netdev);\r\nif (netif_running(netdev)) {\r\nrtnl_lock();\r\nixgbevf_down(adapter);\r\nixgbevf_free_irq(adapter);\r\nixgbevf_free_all_tx_resources(adapter);\r\nixgbevf_free_all_rx_resources(adapter);\r\nrtnl_unlock();\r\n}\r\nixgbevf_clear_interrupt_scheme(adapter);\r\n#ifdef CONFIG_PM\r\nretval = pci_save_state(pdev);\r\nif (retval)\r\nreturn retval;\r\n#endif\r\npci_disable_device(pdev);\r\nreturn 0;\r\n}\r\nstatic int ixgbevf_resume(struct pci_dev *pdev)\r\n{\r\nstruct ixgbevf_adapter *adapter = pci_get_drvdata(pdev);\r\nstruct net_device *netdev = adapter->netdev;\r\nu32 err;\r\npci_set_power_state(pdev, PCI_D0);\r\npci_restore_state(pdev);\r\npci_save_state(pdev);\r\nerr = pci_enable_device_mem(pdev);\r\nif (err) {\r\ndev_err(&pdev->dev, "Cannot enable PCI device from suspend\n");\r\nreturn err;\r\n}\r\npci_set_master(pdev);\r\nrtnl_lock();\r\nerr = ixgbevf_init_interrupt_scheme(adapter);\r\nrtnl_unlock();\r\nif (err) {\r\ndev_err(&pdev->dev, "Cannot initialize interrupts\n");\r\nreturn err;\r\n}\r\nixgbevf_reset(adapter);\r\nif (netif_running(netdev)) {\r\nerr = ixgbevf_open(netdev);\r\nif (err)\r\nreturn err;\r\n}\r\nnetif_device_attach(netdev);\r\nreturn err;\r\n}\r\nstatic void ixgbevf_shutdown(struct pci_dev *pdev)\r\n{\r\nixgbevf_suspend(pdev, PMSG_SUSPEND);\r\n}\r\nstatic struct rtnl_link_stats64 *ixgbevf_get_stats(struct net_device *netdev,\r\nstruct rtnl_link_stats64 *stats)\r\n{\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nunsigned int start;\r\nu64 bytes, packets;\r\nconst struct ixgbevf_ring *ring;\r\nint i;\r\nixgbevf_update_stats(adapter);\r\nstats->multicast = adapter->stats.vfmprc - adapter->stats.base_vfmprc;\r\nfor (i = 0; i < adapter->num_rx_queues; i++) {\r\nring = &adapter->rx_ring[i];\r\ndo {\r\nstart = u64_stats_fetch_begin_bh(&ring->syncp);\r\nbytes = ring->total_bytes;\r\npackets = ring->total_packets;\r\n} while (u64_stats_fetch_retry_bh(&ring->syncp, start));\r\nstats->rx_bytes += bytes;\r\nstats->rx_packets += packets;\r\n}\r\nfor (i = 0; i < adapter->num_tx_queues; i++) {\r\nring = &adapter->tx_ring[i];\r\ndo {\r\nstart = u64_stats_fetch_begin_bh(&ring->syncp);\r\nbytes = ring->total_bytes;\r\npackets = ring->total_packets;\r\n} while (u64_stats_fetch_retry_bh(&ring->syncp, start));\r\nstats->tx_bytes += bytes;\r\nstats->tx_packets += packets;\r\n}\r\nreturn stats;\r\n}\r\nstatic void ixgbevf_assign_netdev_ops(struct net_device *dev)\r\n{\r\ndev->netdev_ops = &ixgbevf_netdev_ops;\r\nixgbevf_set_ethtool_ops(dev);\r\ndev->watchdog_timeo = 5 * HZ;\r\n}\r\nstatic int ixgbevf_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nstruct net_device *netdev;\r\nstruct ixgbevf_adapter *adapter = NULL;\r\nstruct ixgbe_hw *hw = NULL;\r\nconst struct ixgbevf_info *ii = ixgbevf_info_tbl[ent->driver_data];\r\nstatic int cards_found;\r\nint err, pci_using_dac;\r\nerr = pci_enable_device(pdev);\r\nif (err)\r\nreturn err;\r\nif (!dma_set_mask(&pdev->dev, DMA_BIT_MASK(64)) &&\r\n!dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(64))) {\r\npci_using_dac = 1;\r\n} else {\r\nerr = dma_set_mask(&pdev->dev, DMA_BIT_MASK(32));\r\nif (err) {\r\nerr = dma_set_coherent_mask(&pdev->dev,\r\nDMA_BIT_MASK(32));\r\nif (err) {\r\ndev_err(&pdev->dev, "No usable DMA "\r\n"configuration, aborting\n");\r\ngoto err_dma;\r\n}\r\n}\r\npci_using_dac = 0;\r\n}\r\nerr = pci_request_regions(pdev, ixgbevf_driver_name);\r\nif (err) {\r\ndev_err(&pdev->dev, "pci_request_regions failed 0x%x\n", err);\r\ngoto err_pci_reg;\r\n}\r\npci_set_master(pdev);\r\nnetdev = alloc_etherdev_mq(sizeof(struct ixgbevf_adapter),\r\nMAX_TX_QUEUES);\r\nif (!netdev) {\r\nerr = -ENOMEM;\r\ngoto err_alloc_etherdev;\r\n}\r\nSET_NETDEV_DEV(netdev, &pdev->dev);\r\npci_set_drvdata(pdev, netdev);\r\nadapter = netdev_priv(netdev);\r\nadapter->netdev = netdev;\r\nadapter->pdev = pdev;\r\nhw = &adapter->hw;\r\nhw->back = adapter;\r\nadapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);\r\npci_save_state(pdev);\r\nhw->hw_addr = ioremap(pci_resource_start(pdev, 0),\r\npci_resource_len(pdev, 0));\r\nif (!hw->hw_addr) {\r\nerr = -EIO;\r\ngoto err_ioremap;\r\n}\r\nixgbevf_assign_netdev_ops(netdev);\r\nadapter->bd_number = cards_found;\r\nmemcpy(&hw->mac.ops, ii->mac_ops, sizeof(hw->mac.ops));\r\nhw->mac.type = ii->mac;\r\nmemcpy(&hw->mbx.ops, &ixgbevf_mbx_ops,\r\nsizeof(struct ixgbe_mbx_operations));\r\nerr = ixgbevf_sw_init(adapter);\r\nif (err)\r\ngoto err_sw_init;\r\nif (!is_valid_ether_addr(netdev->dev_addr)) {\r\npr_err("invalid MAC address\n");\r\nerr = -EIO;\r\ngoto err_sw_init;\r\n}\r\nnetdev->hw_features = NETIF_F_SG |\r\nNETIF_F_IP_CSUM |\r\nNETIF_F_IPV6_CSUM |\r\nNETIF_F_TSO |\r\nNETIF_F_TSO6 |\r\nNETIF_F_RXCSUM;\r\nnetdev->features = netdev->hw_features |\r\nNETIF_F_HW_VLAN_CTAG_TX |\r\nNETIF_F_HW_VLAN_CTAG_RX |\r\nNETIF_F_HW_VLAN_CTAG_FILTER;\r\nnetdev->vlan_features |= NETIF_F_TSO;\r\nnetdev->vlan_features |= NETIF_F_TSO6;\r\nnetdev->vlan_features |= NETIF_F_IP_CSUM;\r\nnetdev->vlan_features |= NETIF_F_IPV6_CSUM;\r\nnetdev->vlan_features |= NETIF_F_SG;\r\nif (pci_using_dac)\r\nnetdev->features |= NETIF_F_HIGHDMA;\r\nnetdev->priv_flags |= IFF_UNICAST_FLT;\r\ninit_timer(&adapter->watchdog_timer);\r\nadapter->watchdog_timer.function = ixgbevf_watchdog;\r\nadapter->watchdog_timer.data = (unsigned long)adapter;\r\nINIT_WORK(&adapter->reset_task, ixgbevf_reset_task);\r\nINIT_WORK(&adapter->watchdog_task, ixgbevf_watchdog_task);\r\nerr = ixgbevf_init_interrupt_scheme(adapter);\r\nif (err)\r\ngoto err_sw_init;\r\nstrcpy(netdev->name, "eth%d");\r\nerr = register_netdev(netdev);\r\nif (err)\r\ngoto err_register;\r\nnetif_carrier_off(netdev);\r\nixgbevf_init_last_counter_stats(adapter);\r\nhw_dbg(hw, "%pM\n", netdev->dev_addr);\r\nhw_dbg(hw, "MAC: %d\n", hw->mac.type);\r\nhw_dbg(hw, "Intel(R) 82599 Virtual Function\n");\r\ncards_found++;\r\nreturn 0;\r\nerr_register:\r\nixgbevf_clear_interrupt_scheme(adapter);\r\nerr_sw_init:\r\nixgbevf_reset_interrupt_capability(adapter);\r\niounmap(hw->hw_addr);\r\nerr_ioremap:\r\nfree_netdev(netdev);\r\nerr_alloc_etherdev:\r\npci_release_regions(pdev);\r\nerr_pci_reg:\r\nerr_dma:\r\npci_disable_device(pdev);\r\nreturn err;\r\n}\r\nstatic void ixgbevf_remove(struct pci_dev *pdev)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nset_bit(__IXGBEVF_DOWN, &adapter->state);\r\ndel_timer_sync(&adapter->watchdog_timer);\r\ncancel_work_sync(&adapter->reset_task);\r\ncancel_work_sync(&adapter->watchdog_task);\r\nif (netdev->reg_state == NETREG_REGISTERED)\r\nunregister_netdev(netdev);\r\nixgbevf_clear_interrupt_scheme(adapter);\r\nixgbevf_reset_interrupt_capability(adapter);\r\niounmap(adapter->hw.hw_addr);\r\npci_release_regions(pdev);\r\nhw_dbg(&adapter->hw, "Remove complete\n");\r\nkfree(adapter->tx_ring);\r\nkfree(adapter->rx_ring);\r\nfree_netdev(netdev);\r\npci_disable_device(pdev);\r\n}\r\nstatic pci_ers_result_t ixgbevf_io_error_detected(struct pci_dev *pdev,\r\npci_channel_state_t state)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nnetif_device_detach(netdev);\r\nif (state == pci_channel_io_perm_failure)\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\nif (netif_running(netdev))\r\nixgbevf_down(adapter);\r\npci_disable_device(pdev);\r\nreturn PCI_ERS_RESULT_NEED_RESET;\r\n}\r\nstatic pci_ers_result_t ixgbevf_io_slot_reset(struct pci_dev *pdev)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nif (pci_enable_device_mem(pdev)) {\r\ndev_err(&pdev->dev,\r\n"Cannot re-enable PCI device after reset.\n");\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\npci_set_master(pdev);\r\nixgbevf_reset(adapter);\r\nreturn PCI_ERS_RESULT_RECOVERED;\r\n}\r\nstatic void ixgbevf_io_resume(struct pci_dev *pdev)\r\n{\r\nstruct net_device *netdev = pci_get_drvdata(pdev);\r\nstruct ixgbevf_adapter *adapter = netdev_priv(netdev);\r\nif (netif_running(netdev))\r\nixgbevf_up(adapter);\r\nnetif_device_attach(netdev);\r\n}\r\nstatic int __init ixgbevf_init_module(void)\r\n{\r\nint ret;\r\npr_info("%s - version %s\n", ixgbevf_driver_string,\r\nixgbevf_driver_version);\r\npr_info("%s\n", ixgbevf_copyright);\r\nret = pci_register_driver(&ixgbevf_driver);\r\nreturn ret;\r\n}\r\nstatic void __exit ixgbevf_exit_module(void)\r\n{\r\npci_unregister_driver(&ixgbevf_driver);\r\n}\r\nchar *ixgbevf_get_hw_dev_name(struct ixgbe_hw *hw)\r\n{\r\nstruct ixgbevf_adapter *adapter = hw->back;\r\nreturn adapter->netdev->name;\r\n}
