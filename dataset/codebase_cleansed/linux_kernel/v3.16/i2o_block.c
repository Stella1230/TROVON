static void i2o_block_device_free(struct i2o_block_device *dev)\r\n{\r\nblk_cleanup_queue(dev->gd->queue);\r\nput_disk(dev->gd);\r\nkfree(dev);\r\n}\r\nstatic int i2o_block_remove(struct device *dev)\r\n{\r\nstruct i2o_device *i2o_dev = to_i2o_device(dev);\r\nstruct i2o_block_device *i2o_blk_dev = dev_get_drvdata(dev);\r\nosm_info("device removed (TID: %03x): %s\n", i2o_dev->lct_data.tid,\r\ni2o_blk_dev->gd->disk_name);\r\ni2o_event_register(i2o_dev, &i2o_block_driver, 0, 0);\r\ndel_gendisk(i2o_blk_dev->gd);\r\ndev_set_drvdata(dev, NULL);\r\ni2o_device_claim_release(i2o_dev);\r\ni2o_block_device_free(i2o_blk_dev);\r\nreturn 0;\r\n}\r\nstatic int i2o_block_device_flush(struct i2o_device *dev)\r\n{\r\nstruct i2o_message *msg;\r\nmsg = i2o_msg_get_wait(dev->iop, I2O_TIMEOUT_MESSAGE_GET);\r\nif (IS_ERR(msg))\r\nreturn PTR_ERR(msg);\r\nmsg->u.head[0] = cpu_to_le32(FIVE_WORD_MSG_SIZE | SGL_OFFSET_0);\r\nmsg->u.head[1] =\r\ncpu_to_le32(I2O_CMD_BLOCK_CFLUSH << 24 | HOST_TID << 12 | dev->\r\nlct_data.tid);\r\nmsg->body[0] = cpu_to_le32(60 << 16);\r\nosm_debug("Flushing...\n");\r\nreturn i2o_msg_post_wait(dev->iop, msg, 60);\r\n}\r\nstatic int i2o_block_device_mount(struct i2o_device *dev, u32 media_id)\r\n{\r\nstruct i2o_message *msg;\r\nmsg = i2o_msg_get_wait(dev->iop, I2O_TIMEOUT_MESSAGE_GET);\r\nif (IS_ERR(msg))\r\nreturn PTR_ERR(msg);\r\nmsg->u.head[0] = cpu_to_le32(FIVE_WORD_MSG_SIZE | SGL_OFFSET_0);\r\nmsg->u.head[1] =\r\ncpu_to_le32(I2O_CMD_BLOCK_MMOUNT << 24 | HOST_TID << 12 | dev->\r\nlct_data.tid);\r\nmsg->body[0] = cpu_to_le32(-1);\r\nmsg->body[1] = cpu_to_le32(0x00000000);\r\nosm_debug("Mounting...\n");\r\nreturn i2o_msg_post_wait(dev->iop, msg, 2);\r\n}\r\nstatic int i2o_block_device_lock(struct i2o_device *dev, u32 media_id)\r\n{\r\nstruct i2o_message *msg;\r\nmsg = i2o_msg_get_wait(dev->iop, I2O_TIMEOUT_MESSAGE_GET);\r\nif (IS_ERR(msg))\r\nreturn PTR_ERR(msg);\r\nmsg->u.head[0] = cpu_to_le32(FIVE_WORD_MSG_SIZE | SGL_OFFSET_0);\r\nmsg->u.head[1] =\r\ncpu_to_le32(I2O_CMD_BLOCK_MLOCK << 24 | HOST_TID << 12 | dev->\r\nlct_data.tid);\r\nmsg->body[0] = cpu_to_le32(-1);\r\nosm_debug("Locking...\n");\r\nreturn i2o_msg_post_wait(dev->iop, msg, 2);\r\n}\r\nstatic int i2o_block_device_unlock(struct i2o_device *dev, u32 media_id)\r\n{\r\nstruct i2o_message *msg;\r\nmsg = i2o_msg_get_wait(dev->iop, I2O_TIMEOUT_MESSAGE_GET);\r\nif (IS_ERR(msg))\r\nreturn PTR_ERR(msg);\r\nmsg->u.head[0] = cpu_to_le32(FIVE_WORD_MSG_SIZE | SGL_OFFSET_0);\r\nmsg->u.head[1] =\r\ncpu_to_le32(I2O_CMD_BLOCK_MUNLOCK << 24 | HOST_TID << 12 | dev->\r\nlct_data.tid);\r\nmsg->body[0] = cpu_to_le32(media_id);\r\nosm_debug("Unlocking...\n");\r\nreturn i2o_msg_post_wait(dev->iop, msg, 2);\r\n}\r\nstatic int i2o_block_device_power(struct i2o_block_device *dev, u8 op)\r\n{\r\nstruct i2o_device *i2o_dev = dev->i2o_dev;\r\nstruct i2o_controller *c = i2o_dev->iop;\r\nstruct i2o_message *msg;\r\nint rc;\r\nmsg = i2o_msg_get_wait(c, I2O_TIMEOUT_MESSAGE_GET);\r\nif (IS_ERR(msg))\r\nreturn PTR_ERR(msg);\r\nmsg->u.head[0] = cpu_to_le32(FOUR_WORD_MSG_SIZE | SGL_OFFSET_0);\r\nmsg->u.head[1] =\r\ncpu_to_le32(I2O_CMD_BLOCK_POWER << 24 | HOST_TID << 12 | i2o_dev->\r\nlct_data.tid);\r\nmsg->body[0] = cpu_to_le32(op << 24);\r\nosm_debug("Power...\n");\r\nrc = i2o_msg_post_wait(c, msg, 60);\r\nif (!rc)\r\ndev->power = op;\r\nreturn rc;\r\n}\r\nstatic inline struct i2o_block_request *i2o_block_request_alloc(void)\r\n{\r\nstruct i2o_block_request *ireq;\r\nireq = mempool_alloc(i2o_blk_req_pool.pool, GFP_ATOMIC);\r\nif (!ireq)\r\nreturn ERR_PTR(-ENOMEM);\r\nINIT_LIST_HEAD(&ireq->queue);\r\nsg_init_table(ireq->sg_table, I2O_MAX_PHYS_SEGMENTS);\r\nreturn ireq;\r\n}\r\nstatic inline void i2o_block_request_free(struct i2o_block_request *ireq)\r\n{\r\nmempool_free(ireq, i2o_blk_req_pool.pool);\r\n}\r\nstatic inline int i2o_block_sglist_alloc(struct i2o_controller *c,\r\nstruct i2o_block_request *ireq,\r\nu32 ** mptr)\r\n{\r\nint nents;\r\nenum dma_data_direction direction;\r\nireq->dev = &c->pdev->dev;\r\nnents = blk_rq_map_sg(ireq->req->q, ireq->req, ireq->sg_table);\r\nif (rq_data_dir(ireq->req) == READ)\r\ndirection = PCI_DMA_FROMDEVICE;\r\nelse\r\ndirection = PCI_DMA_TODEVICE;\r\nireq->sg_nents = nents;\r\nreturn i2o_dma_map_sg(c, ireq->sg_table, nents, direction, mptr);\r\n}\r\nstatic inline void i2o_block_sglist_free(struct i2o_block_request *ireq)\r\n{\r\nenum dma_data_direction direction;\r\nif (rq_data_dir(ireq->req) == READ)\r\ndirection = PCI_DMA_FROMDEVICE;\r\nelse\r\ndirection = PCI_DMA_TODEVICE;\r\ndma_unmap_sg(ireq->dev, ireq->sg_table, ireq->sg_nents, direction);\r\n}\r\nstatic int i2o_block_prep_req_fn(struct request_queue *q, struct request *req)\r\n{\r\nstruct i2o_block_device *i2o_blk_dev = q->queuedata;\r\nstruct i2o_block_request *ireq;\r\nif (unlikely(!i2o_blk_dev)) {\r\nosm_err("block device already removed\n");\r\nreturn BLKPREP_KILL;\r\n}\r\nif (!req->special) {\r\nireq = i2o_block_request_alloc();\r\nif (IS_ERR(ireq)) {\r\nosm_debug("unable to allocate i2o_block_request!\n");\r\nreturn BLKPREP_DEFER;\r\n}\r\nireq->i2o_blk_dev = i2o_blk_dev;\r\nreq->special = ireq;\r\nireq->req = req;\r\n}\r\nreq->cmd_flags |= REQ_DONTPREP;\r\nreturn BLKPREP_OK;\r\n}\r\nstatic void i2o_block_delayed_request_fn(struct work_struct *work)\r\n{\r\nstruct i2o_block_delayed_request *dreq =\r\ncontainer_of(work, struct i2o_block_delayed_request,\r\nwork.work);\r\nstruct request_queue *q = dreq->queue;\r\nunsigned long flags;\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_start_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\nkfree(dreq);\r\n}\r\nstatic void i2o_block_end_request(struct request *req, int error,\r\nint nr_bytes)\r\n{\r\nstruct i2o_block_request *ireq = req->special;\r\nstruct i2o_block_device *dev = ireq->i2o_blk_dev;\r\nstruct request_queue *q = req->q;\r\nunsigned long flags;\r\nif (blk_end_request(req, error, nr_bytes))\r\nif (error)\r\nblk_end_request_all(req, -EIO);\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nif (likely(dev)) {\r\ndev->open_queue_depth--;\r\nlist_del(&ireq->queue);\r\n}\r\nblk_start_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\ni2o_block_sglist_free(ireq);\r\ni2o_block_request_free(ireq);\r\n}\r\nstatic int i2o_block_reply(struct i2o_controller *c, u32 m,\r\nstruct i2o_message *msg)\r\n{\r\nstruct request *req;\r\nint error = 0;\r\nreq = i2o_cntxt_list_get(c, le32_to_cpu(msg->u.s.tcntxt));\r\nif (unlikely(!req)) {\r\nosm_err("NULL reply received!\n");\r\nreturn -1;\r\n}\r\nif ((le32_to_cpu(msg->body[0]) >> 24) != 0) {\r\nu32 status = le32_to_cpu(msg->body[0]);\r\nosm_err("TID %03x error status: 0x%02x, detailed status: "\r\n"0x%04x\n", (le32_to_cpu(msg->u.head[1]) >> 12 & 0xfff),\r\nstatus >> 24, status & 0xffff);\r\nreq->errors++;\r\nerror = -EIO;\r\n}\r\ni2o_block_end_request(req, error, le32_to_cpu(msg->body[1]));\r\nreturn 1;\r\n}\r\nstatic void i2o_block_event(struct work_struct *work)\r\n{\r\nstruct i2o_event *evt = container_of(work, struct i2o_event, work);\r\nosm_debug("event received\n");\r\nkfree(evt);\r\n}\r\nstatic void i2o_block_biosparam(unsigned long capacity, unsigned short *cyls,\r\nunsigned char *hds, unsigned char *secs)\r\n{\r\nunsigned long heads, sectors, cylinders;\r\nsectors = 63L;\r\nif (capacity <= BLOCK_SIZE_528M)\r\nheads = 16;\r\nelse if (capacity <= BLOCK_SIZE_1G)\r\nheads = 32;\r\nelse if (capacity <= BLOCK_SIZE_21G)\r\nheads = 64;\r\nelse if (capacity <= BLOCK_SIZE_42G)\r\nheads = 128;\r\nelse\r\nheads = 255;\r\ncylinders = (unsigned long)capacity / (heads * sectors);\r\n*cyls = (unsigned short)cylinders;\r\n*secs = (unsigned char)sectors;\r\n*hds = (unsigned char)heads;\r\n}\r\nstatic int i2o_block_open(struct block_device *bdev, fmode_t mode)\r\n{\r\nstruct i2o_block_device *dev = bdev->bd_disk->private_data;\r\nif (!dev->i2o_dev)\r\nreturn -ENODEV;\r\nmutex_lock(&i2o_block_mutex);\r\nif (dev->power > 0x1f)\r\ni2o_block_device_power(dev, 0x02);\r\ni2o_block_device_mount(dev->i2o_dev, -1);\r\ni2o_block_device_lock(dev->i2o_dev, -1);\r\nosm_debug("Ready.\n");\r\nmutex_unlock(&i2o_block_mutex);\r\nreturn 0;\r\n}\r\nstatic void i2o_block_release(struct gendisk *disk, fmode_t mode)\r\n{\r\nstruct i2o_block_device *dev = disk->private_data;\r\nu8 operation;\r\nif (!dev->i2o_dev)\r\nreturn;\r\nmutex_lock(&i2o_block_mutex);\r\ni2o_block_device_flush(dev->i2o_dev);\r\ni2o_block_device_unlock(dev->i2o_dev, -1);\r\nif (dev->flags & (1 << 3 | 1 << 4))\r\noperation = 0x21;\r\nelse\r\noperation = 0x24;\r\ni2o_block_device_power(dev, operation);\r\nmutex_unlock(&i2o_block_mutex);\r\n}\r\nstatic int i2o_block_getgeo(struct block_device *bdev, struct hd_geometry *geo)\r\n{\r\ni2o_block_biosparam(get_capacity(bdev->bd_disk),\r\n&geo->cylinders, &geo->heads, &geo->sectors);\r\nreturn 0;\r\n}\r\nstatic int i2o_block_ioctl(struct block_device *bdev, fmode_t mode,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nstruct gendisk *disk = bdev->bd_disk;\r\nstruct i2o_block_device *dev = disk->private_data;\r\nint ret = -ENOTTY;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nmutex_lock(&i2o_block_mutex);\r\nswitch (cmd) {\r\ncase BLKI2OGRSTRAT:\r\nret = put_user(dev->rcache, (int __user *)arg);\r\nbreak;\r\ncase BLKI2OGWSTRAT:\r\nret = put_user(dev->wcache, (int __user *)arg);\r\nbreak;\r\ncase BLKI2OSRSTRAT:\r\nret = -EINVAL;\r\nif (arg < 0 || arg > CACHE_SMARTFETCH)\r\nbreak;\r\ndev->rcache = arg;\r\nret = 0;\r\nbreak;\r\ncase BLKI2OSWSTRAT:\r\nret = -EINVAL;\r\nif (arg != 0\r\n&& (arg < CACHE_WRITETHROUGH || arg > CACHE_SMARTBACK))\r\nbreak;\r\ndev->wcache = arg;\r\nret = 0;\r\nbreak;\r\n}\r\nmutex_unlock(&i2o_block_mutex);\r\nreturn ret;\r\n}\r\nstatic unsigned int i2o_block_check_events(struct gendisk *disk,\r\nunsigned int clearing)\r\n{\r\nstruct i2o_block_device *p = disk->private_data;\r\nif (p->media_change_flag) {\r\np->media_change_flag = 0;\r\nreturn DISK_EVENT_MEDIA_CHANGE;\r\n}\r\nreturn 0;\r\n}\r\nstatic int i2o_block_transfer(struct request *req)\r\n{\r\nstruct i2o_block_device *dev = req->rq_disk->private_data;\r\nstruct i2o_controller *c;\r\nu32 tid;\r\nstruct i2o_message *msg;\r\nu32 *mptr;\r\nstruct i2o_block_request *ireq = req->special;\r\nu32 tcntxt;\r\nu32 sgl_offset = SGL_OFFSET_8;\r\nu32 ctl_flags = 0x00000000;\r\nint rc;\r\nu32 cmd;\r\nif (unlikely(!dev->i2o_dev)) {\r\nosm_err("transfer to removed drive\n");\r\nrc = -ENODEV;\r\ngoto exit;\r\n}\r\ntid = dev->i2o_dev->lct_data.tid;\r\nc = dev->i2o_dev->iop;\r\nmsg = i2o_msg_get(c);\r\nif (IS_ERR(msg)) {\r\nrc = PTR_ERR(msg);\r\ngoto exit;\r\n}\r\ntcntxt = i2o_cntxt_list_add(c, req);\r\nif (!tcntxt) {\r\nrc = -ENOMEM;\r\ngoto nop_msg;\r\n}\r\nmsg->u.s.icntxt = cpu_to_le32(i2o_block_driver.context);\r\nmsg->u.s.tcntxt = cpu_to_le32(tcntxt);\r\nmptr = &msg->body[0];\r\nif (rq_data_dir(req) == READ) {\r\ncmd = I2O_CMD_BLOCK_READ << 24;\r\nswitch (dev->rcache) {\r\ncase CACHE_PREFETCH:\r\nctl_flags = 0x201F0008;\r\nbreak;\r\ncase CACHE_SMARTFETCH:\r\nif (blk_rq_sectors(req) > 16)\r\nctl_flags = 0x201F0008;\r\nelse\r\nctl_flags = 0x001F0000;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n} else {\r\ncmd = I2O_CMD_BLOCK_WRITE << 24;\r\nswitch (dev->wcache) {\r\ncase CACHE_WRITETHROUGH:\r\nctl_flags = 0x001F0008;\r\nbreak;\r\ncase CACHE_WRITEBACK:\r\nctl_flags = 0x001F0010;\r\nbreak;\r\ncase CACHE_SMARTBACK:\r\nif (blk_rq_sectors(req) > 16)\r\nctl_flags = 0x001F0004;\r\nelse\r\nctl_flags = 0x001F0010;\r\nbreak;\r\ncase CACHE_SMARTTHROUGH:\r\nif (blk_rq_sectors(req) > 16)\r\nctl_flags = 0x001F0004;\r\nelse\r\nctl_flags = 0x001F0010;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\n#ifdef CONFIG_I2O_EXT_ADAPTEC\r\nif (c->adaptec) {\r\nu8 cmd[10];\r\nu32 scsi_flags;\r\nu16 hwsec;\r\nhwsec = queue_logical_block_size(req->q) >> KERNEL_SECTOR_SHIFT;\r\nmemset(cmd, 0, 10);\r\nsgl_offset = SGL_OFFSET_12;\r\nmsg->u.head[1] =\r\ncpu_to_le32(I2O_CMD_PRIVATE << 24 | HOST_TID << 12 | tid);\r\n*mptr++ = cpu_to_le32(I2O_VENDOR_DPT << 16 | I2O_CMD_SCSI_EXEC);\r\n*mptr++ = cpu_to_le32(tid);\r\nif (rq_data_dir(req) == READ) {\r\ncmd[0] = READ_10;\r\nscsi_flags = 0x60a0000a;\r\n} else {\r\ncmd[0] = WRITE_10;\r\nscsi_flags = 0xa0a0000a;\r\n}\r\n*mptr++ = cpu_to_le32(scsi_flags);\r\n*((u32 *) & cmd[2]) = cpu_to_be32(blk_rq_pos(req) * hwsec);\r\n*((u16 *) & cmd[7]) = cpu_to_be16(blk_rq_sectors(req) * hwsec);\r\nmemcpy(mptr, cmd, 10);\r\nmptr += 4;\r\n*mptr++ = cpu_to_le32(blk_rq_bytes(req));\r\n} else\r\n#endif\r\n{\r\nmsg->u.head[1] = cpu_to_le32(cmd | HOST_TID << 12 | tid);\r\n*mptr++ = cpu_to_le32(ctl_flags);\r\n*mptr++ = cpu_to_le32(blk_rq_bytes(req));\r\n*mptr++ =\r\ncpu_to_le32((u32) (blk_rq_pos(req) << KERNEL_SECTOR_SHIFT));\r\n*mptr++ =\r\ncpu_to_le32(blk_rq_pos(req) >> (32 - KERNEL_SECTOR_SHIFT));\r\n}\r\nif (!i2o_block_sglist_alloc(c, ireq, &mptr)) {\r\nrc = -ENOMEM;\r\ngoto context_remove;\r\n}\r\nmsg->u.head[0] =\r\ncpu_to_le32(I2O_MESSAGE_SIZE(mptr - &msg->u.head[0]) | sgl_offset);\r\nlist_add_tail(&ireq->queue, &dev->open_queue);\r\ndev->open_queue_depth++;\r\ni2o_msg_post(c, msg);\r\nreturn 0;\r\ncontext_remove:\r\ni2o_cntxt_list_remove(c, req);\r\nnop_msg:\r\ni2o_msg_nop(c, msg);\r\nexit:\r\nreturn rc;\r\n}\r\nstatic void i2o_block_request_fn(struct request_queue *q)\r\n{\r\nstruct request *req;\r\nwhile ((req = blk_peek_request(q)) != NULL) {\r\nif (req->cmd_type == REQ_TYPE_FS) {\r\nstruct i2o_block_delayed_request *dreq;\r\nstruct i2o_block_request *ireq = req->special;\r\nunsigned int queue_depth;\r\nqueue_depth = ireq->i2o_blk_dev->open_queue_depth;\r\nif (queue_depth < I2O_BLOCK_MAX_OPEN_REQUESTS) {\r\nif (!i2o_block_transfer(req)) {\r\nblk_start_request(req);\r\ncontinue;\r\n} else\r\nosm_info("transfer error\n");\r\n}\r\nif (queue_depth)\r\nbreak;\r\ndreq = kmalloc(sizeof(*dreq), GFP_ATOMIC);\r\nif (!dreq)\r\ncontinue;\r\ndreq->queue = q;\r\nINIT_DELAYED_WORK(&dreq->work,\r\ni2o_block_delayed_request_fn);\r\nif (!queue_delayed_work(i2o_block_driver.event_queue,\r\n&dreq->work,\r\nI2O_BLOCK_RETRY_TIME))\r\nkfree(dreq);\r\nelse {\r\nblk_stop_queue(q);\r\nbreak;\r\n}\r\n} else {\r\nblk_start_request(req);\r\n__blk_end_request_all(req, -EIO);\r\n}\r\n}\r\n}\r\nstatic struct i2o_block_device *i2o_block_device_alloc(void)\r\n{\r\nstruct i2o_block_device *dev;\r\nstruct gendisk *gd;\r\nstruct request_queue *queue;\r\nint rc;\r\ndev = kzalloc(sizeof(*dev), GFP_KERNEL);\r\nif (!dev) {\r\nosm_err("Insufficient memory to allocate I2O Block disk.\n");\r\nrc = -ENOMEM;\r\ngoto exit;\r\n}\r\nINIT_LIST_HEAD(&dev->open_queue);\r\nspin_lock_init(&dev->lock);\r\ndev->rcache = CACHE_PREFETCH;\r\ndev->wcache = CACHE_WRITEBACK;\r\ngd = alloc_disk(16);\r\nif (!gd) {\r\nosm_err("Insufficient memory to allocate gendisk.\n");\r\nrc = -ENOMEM;\r\ngoto cleanup_dev;\r\n}\r\nqueue = blk_init_queue(i2o_block_request_fn, &dev->lock);\r\nif (!queue) {\r\nosm_err("Insufficient memory to allocate request queue.\n");\r\nrc = -ENOMEM;\r\ngoto cleanup_queue;\r\n}\r\nblk_queue_prep_rq(queue, i2o_block_prep_req_fn);\r\ngd->major = I2O_MAJOR;\r\ngd->queue = queue;\r\ngd->fops = &i2o_block_fops;\r\ngd->private_data = dev;\r\ndev->gd = gd;\r\nreturn dev;\r\ncleanup_queue:\r\nput_disk(gd);\r\ncleanup_dev:\r\nkfree(dev);\r\nexit:\r\nreturn ERR_PTR(rc);\r\n}\r\nstatic int i2o_block_probe(struct device *dev)\r\n{\r\nstruct i2o_device *i2o_dev = to_i2o_device(dev);\r\nstruct i2o_controller *c = i2o_dev->iop;\r\nstruct i2o_block_device *i2o_blk_dev;\r\nstruct gendisk *gd;\r\nstruct request_queue *queue;\r\nstatic int unit = 0;\r\nint rc;\r\nu64 size;\r\nu32 blocksize;\r\nu16 body_size = 4;\r\nu16 power;\r\nunsigned short max_sectors;\r\n#ifdef CONFIG_I2O_EXT_ADAPTEC\r\nif (c->adaptec)\r\nbody_size = 8;\r\n#endif\r\nif (c->limit_sectors)\r\nmax_sectors = I2O_MAX_SECTORS_LIMITED;\r\nelse\r\nmax_sectors = I2O_MAX_SECTORS;\r\nif (i2o_dev->lct_data.user_tid != 0xfff) {\r\nosm_debug("skipping used device %03x\n", i2o_dev->lct_data.tid);\r\nreturn -ENODEV;\r\n}\r\nif (i2o_device_claim(i2o_dev)) {\r\nosm_warn("Unable to claim device. Installation aborted\n");\r\nrc = -EFAULT;\r\ngoto exit;\r\n}\r\ni2o_blk_dev = i2o_block_device_alloc();\r\nif (IS_ERR(i2o_blk_dev)) {\r\nosm_err("could not alloc a new I2O block device");\r\nrc = PTR_ERR(i2o_blk_dev);\r\ngoto claim_release;\r\n}\r\ni2o_blk_dev->i2o_dev = i2o_dev;\r\ndev_set_drvdata(dev, i2o_blk_dev);\r\ngd = i2o_blk_dev->gd;\r\ngd->first_minor = unit << 4;\r\nsprintf(gd->disk_name, "i2o/hd%c", 'a' + unit);\r\ngd->driverfs_dev = &i2o_dev->device;\r\nqueue = gd->queue;\r\nqueue->queuedata = i2o_blk_dev;\r\nblk_queue_max_hw_sectors(queue, max_sectors);\r\nblk_queue_max_segments(queue, i2o_sg_tablesize(c, body_size));\r\nosm_debug("max sectors = %d\n", queue->max_sectors);\r\nosm_debug("phys segments = %d\n", queue->max_phys_segments);\r\nosm_debug("max hw segments = %d\n", queue->max_hw_segments);\r\nif (!i2o_parm_field_get(i2o_dev, 0x0004, 1, &blocksize, 4) ||\r\n!i2o_parm_field_get(i2o_dev, 0x0000, 3, &blocksize, 4)) {\r\nblk_queue_logical_block_size(queue, le32_to_cpu(blocksize));\r\n} else\r\nosm_warn("unable to get blocksize of %s\n", gd->disk_name);\r\nif (!i2o_parm_field_get(i2o_dev, 0x0004, 0, &size, 8) ||\r\n!i2o_parm_field_get(i2o_dev, 0x0000, 4, &size, 8)) {\r\nset_capacity(gd, le64_to_cpu(size) >> KERNEL_SECTOR_SHIFT);\r\n} else\r\nosm_warn("could not get size of %s\n", gd->disk_name);\r\nif (!i2o_parm_field_get(i2o_dev, 0x0000, 2, &power, 2))\r\ni2o_blk_dev->power = power;\r\ni2o_event_register(i2o_dev, &i2o_block_driver, 0, 0xffffffff);\r\nadd_disk(gd);\r\nunit++;\r\nosm_info("device added (TID: %03x): %s\n", i2o_dev->lct_data.tid,\r\ni2o_blk_dev->gd->disk_name);\r\nreturn 0;\r\nclaim_release:\r\ni2o_device_claim_release(i2o_dev);\r\nexit:\r\nreturn rc;\r\n}\r\nstatic int __init i2o_block_init(void)\r\n{\r\nint rc;\r\nint size;\r\nprintk(KERN_INFO OSM_DESCRIPTION " v" OSM_VERSION "\n");\r\nsize = sizeof(struct i2o_block_request);\r\ni2o_blk_req_pool.slab = kmem_cache_create("i2o_block_req", size, 0,\r\nSLAB_HWCACHE_ALIGN, NULL);\r\nif (!i2o_blk_req_pool.slab) {\r\nosm_err("can't init request slab\n");\r\nrc = -ENOMEM;\r\ngoto exit;\r\n}\r\ni2o_blk_req_pool.pool =\r\nmempool_create_slab_pool(I2O_BLOCK_REQ_MEMPOOL_SIZE,\r\ni2o_blk_req_pool.slab);\r\nif (!i2o_blk_req_pool.pool) {\r\nosm_err("can't init request mempool\n");\r\nrc = -ENOMEM;\r\ngoto free_slab;\r\n}\r\nrc = register_blkdev(I2O_MAJOR, "i2o_block");\r\nif (rc) {\r\nosm_err("unable to register block device\n");\r\ngoto free_mempool;\r\n}\r\n#ifdef MODULE\r\nosm_info("registered device at major %d\n", I2O_MAJOR);\r\n#endif\r\nrc = i2o_driver_register(&i2o_block_driver);\r\nif (rc) {\r\nosm_err("Could not register Block driver\n");\r\ngoto unregister_blkdev;\r\n}\r\nreturn 0;\r\nunregister_blkdev:\r\nunregister_blkdev(I2O_MAJOR, "i2o_block");\r\nfree_mempool:\r\nmempool_destroy(i2o_blk_req_pool.pool);\r\nfree_slab:\r\nkmem_cache_destroy(i2o_blk_req_pool.slab);\r\nexit:\r\nreturn rc;\r\n}\r\nstatic void __exit i2o_block_exit(void)\r\n{\r\ni2o_driver_unregister(&i2o_block_driver);\r\nunregister_blkdev(I2O_MAJOR, "i2o_block");\r\nmempool_destroy(i2o_blk_req_pool.pool);\r\nkmem_cache_destroy(i2o_blk_req_pool.slab);\r\n}
