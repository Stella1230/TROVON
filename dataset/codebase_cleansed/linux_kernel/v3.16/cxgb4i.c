static inline void set_queue(struct sk_buff *skb, unsigned int queue,\r\nconst struct cxgbi_sock *csk)\r\n{\r\nskb->queue_mapping = queue;\r\n}\r\nstatic inline int is_ofld_imm(const struct sk_buff *skb)\r\n{\r\nreturn skb->len <= (MAX_IMM_TX_PKT_LEN -\r\nsizeof(struct fw_ofld_tx_data_wr));\r\n}\r\nstatic void send_act_open_req(struct cxgbi_sock *csk, struct sk_buff *skb,\r\nstruct l2t_entry *e)\r\n{\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(csk->cdev);\r\nint wscale = cxgbi_sock_compute_wscale(csk->mss_idx);\r\nunsigned long long opt0;\r\nunsigned int opt2;\r\nunsigned int qid_atid = ((unsigned int)csk->atid) |\r\n(((unsigned int)csk->rss_qid) << 14);\r\nopt0 = KEEP_ALIVE(1) |\r\nWND_SCALE(wscale) |\r\nMSS_IDX(csk->mss_idx) |\r\nL2T_IDX(((struct l2t_entry *)csk->l2t)->idx) |\r\nTX_CHAN(csk->tx_chan) |\r\nSMAC_SEL(csk->smac_idx) |\r\nULP_MODE(ULP_MODE_ISCSI) |\r\nRCV_BUFSIZ(cxgb4i_rcv_win >> 10);\r\nopt2 = RX_CHANNEL(0) |\r\nRSS_QUEUE_VALID |\r\n(1 << 20) |\r\nRSS_QUEUE(csk->rss_qid);\r\nif (is_t4(lldi->adapter_type)) {\r\nstruct cpl_act_open_req *req =\r\n(struct cpl_act_open_req *)skb->head;\r\nINIT_TP_WR(req, 0);\r\nOPCODE_TID(req) = cpu_to_be32(MK_OPCODE_TID(CPL_ACT_OPEN_REQ,\r\nqid_atid));\r\nreq->local_port = csk->saddr.sin_port;\r\nreq->peer_port = csk->daddr.sin_port;\r\nreq->local_ip = csk->saddr.sin_addr.s_addr;\r\nreq->peer_ip = csk->daddr.sin_addr.s_addr;\r\nreq->opt0 = cpu_to_be64(opt0);\r\nreq->params = cpu_to_be32(cxgb4_select_ntuple(\r\ncsk->cdev->ports[csk->port_id],\r\ncsk->l2t));\r\nopt2 |= 1 << 22;\r\nreq->opt2 = cpu_to_be32(opt2);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk t4 0x%p, %pI4:%u-%pI4:%u, atid %d, qid %u.\n",\r\ncsk, &req->local_ip, ntohs(req->local_port),\r\n&req->peer_ip, ntohs(req->peer_port),\r\ncsk->atid, csk->rss_qid);\r\n} else {\r\nstruct cpl_t5_act_open_req *req =\r\n(struct cpl_t5_act_open_req *)skb->head;\r\nINIT_TP_WR(req, 0);\r\nOPCODE_TID(req) = cpu_to_be32(MK_OPCODE_TID(CPL_ACT_OPEN_REQ,\r\nqid_atid));\r\nreq->local_port = csk->saddr.sin_port;\r\nreq->peer_port = csk->daddr.sin_port;\r\nreq->local_ip = csk->saddr.sin_addr.s_addr;\r\nreq->peer_ip = csk->daddr.sin_addr.s_addr;\r\nreq->opt0 = cpu_to_be64(opt0);\r\nreq->params = cpu_to_be64(V_FILTER_TUPLE(\r\ncxgb4_select_ntuple(\r\ncsk->cdev->ports[csk->port_id],\r\ncsk->l2t)));\r\nopt2 |= 1 << 31;\r\nreq->opt2 = cpu_to_be32(opt2);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk t5 0x%p, %pI4:%u-%pI4:%u, atid %d, qid %u.\n",\r\ncsk, &req->local_ip, ntohs(req->local_port),\r\n&req->peer_ip, ntohs(req->peer_port),\r\ncsk->atid, csk->rss_qid);\r\n}\r\nset_wr_txq(skb, CPL_PRIORITY_SETUP, csk->port_id);\r\ncxgb4_l2t_send(csk->cdev->ports[csk->port_id], skb, csk->l2t);\r\n}\r\nstatic void send_close_req(struct cxgbi_sock *csk)\r\n{\r\nstruct sk_buff *skb = csk->cpl_close;\r\nstruct cpl_close_con_req *req = (struct cpl_close_con_req *)skb->head;\r\nunsigned int tid = csk->tid;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx, tid %u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncsk->cpl_close = NULL;\r\nset_wr_txq(skb, CPL_PRIORITY_DATA, csk->port_id);\r\nINIT_TP_WR(req, tid);\r\nOPCODE_TID(req) = cpu_to_be32(MK_OPCODE_TID(CPL_CLOSE_CON_REQ, tid));\r\nreq->rsvd = 0;\r\ncxgbi_sock_skb_entail(csk, skb);\r\nif (csk->state >= CTP_ESTABLISHED)\r\npush_tx_frames(csk, 1);\r\n}\r\nstatic void abort_arp_failure(void *handle, struct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk = (struct cxgbi_sock *)handle;\r\nstruct cpl_abort_req *req;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx, tid %u, abort.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\nreq = (struct cpl_abort_req *)skb->data;\r\nreq->cmd = CPL_ABORT_NO_RST;\r\ncxgb4_ofld_send(csk->cdev->ports[csk->port_id], skb);\r\n}\r\nstatic void send_abort_req(struct cxgbi_sock *csk)\r\n{\r\nstruct cpl_abort_req *req;\r\nstruct sk_buff *skb = csk->cpl_abort_req;\r\nif (unlikely(csk->state == CTP_ABORTING) || !skb || !csk->cdev)\r\nreturn;\r\ncxgbi_sock_set_state(csk, CTP_ABORTING);\r\ncxgbi_sock_set_flag(csk, CTPF_ABORT_RPL_PENDING);\r\ncxgbi_sock_purge_write_queue(csk);\r\ncsk->cpl_abort_req = NULL;\r\nreq = (struct cpl_abort_req *)skb->head;\r\nset_queue(skb, CPL_PRIORITY_DATA, csk);\r\nreq->cmd = CPL_ABORT_SEND_RST;\r\nt4_set_arp_err_handler(skb, csk, abort_arp_failure);\r\nINIT_TP_WR(req, csk->tid);\r\nOPCODE_TID(req) = cpu_to_be32(MK_OPCODE_TID(CPL_ABORT_REQ, csk->tid));\r\nreq->rsvd0 = htonl(csk->snd_nxt);\r\nreq->rsvd1 = !cxgbi_sock_flag(csk, CTPF_TX_DATA_SENT);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u, snd_nxt %u, 0x%x.\n",\r\ncsk, csk->state, csk->flags, csk->tid, csk->snd_nxt,\r\nreq->rsvd1);\r\ncxgb4_l2t_send(csk->cdev->ports[csk->port_id], skb, csk->l2t);\r\n}\r\nstatic void send_abort_rpl(struct cxgbi_sock *csk, int rst_status)\r\n{\r\nstruct sk_buff *skb = csk->cpl_abort_rpl;\r\nstruct cpl_abort_rpl *rpl = (struct cpl_abort_rpl *)skb->head;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u, status %d.\n",\r\ncsk, csk->state, csk->flags, csk->tid, rst_status);\r\ncsk->cpl_abort_rpl = NULL;\r\nset_queue(skb, CPL_PRIORITY_DATA, csk);\r\nINIT_TP_WR(rpl, csk->tid);\r\nOPCODE_TID(rpl) = cpu_to_be32(MK_OPCODE_TID(CPL_ABORT_RPL, csk->tid));\r\nrpl->cmd = rst_status;\r\ncxgb4_ofld_send(csk->cdev->ports[csk->port_id], skb);\r\n}\r\nstatic u32 send_rx_credits(struct cxgbi_sock *csk, u32 credits)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cpl_rx_data_ack *req;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p,%u,0x%lx,%u, credit %u.\n",\r\ncsk, csk->state, csk->flags, csk->tid, credits);\r\nskb = alloc_wr(sizeof(*req), 0, GFP_ATOMIC);\r\nif (!skb) {\r\npr_info("csk 0x%p, credit %u, OOM.\n", csk, credits);\r\nreturn 0;\r\n}\r\nreq = (struct cpl_rx_data_ack *)skb->head;\r\nset_wr_txq(skb, CPL_PRIORITY_ACK, csk->port_id);\r\nINIT_TP_WR(req, csk->tid);\r\nOPCODE_TID(req) = cpu_to_be32(MK_OPCODE_TID(CPL_RX_DATA_ACK,\r\ncsk->tid));\r\nreq->credit_dack = cpu_to_be32(RX_CREDITS(credits) | RX_FORCE_ACK(1));\r\ncxgb4_ofld_send(csk->cdev->ports[csk->port_id], skb);\r\nreturn credits;\r\n}\r\nstatic inline unsigned int sgl_len(unsigned int n)\r\n{\r\nn--;\r\nreturn (3 * n) / 2 + (n & 1) + 2;\r\n}\r\nstatic inline unsigned int calc_tx_flits_ofld(const struct sk_buff *skb)\r\n{\r\nunsigned int flits, cnt;\r\nif (is_ofld_imm(skb))\r\nreturn DIV_ROUND_UP(skb->len, 8);\r\nflits = skb_transport_offset(skb) / 8;\r\ncnt = skb_shinfo(skb)->nr_frags;\r\nif (skb_tail_pointer(skb) != skb_transport_header(skb))\r\ncnt++;\r\nreturn flits + sgl_len(cnt);\r\n}\r\nstatic inline void send_tx_flowc_wr(struct cxgbi_sock *csk)\r\n{\r\nstruct sk_buff *skb;\r\nstruct fw_flowc_wr *flowc;\r\nint flowclen, i;\r\nflowclen = 80;\r\nskb = alloc_wr(flowclen, 0, GFP_ATOMIC);\r\nflowc = (struct fw_flowc_wr *)skb->head;\r\nflowc->op_to_nparams =\r\nhtonl(FW_WR_OP(FW_FLOWC_WR) | FW_FLOWC_WR_NPARAMS(8));\r\nflowc->flowid_len16 =\r\nhtonl(FW_WR_LEN16(DIV_ROUND_UP(72, 16)) |\r\nFW_WR_FLOWID(csk->tid));\r\nflowc->mnemval[0].mnemonic = FW_FLOWC_MNEM_PFNVFN;\r\nflowc->mnemval[0].val = htonl(csk->cdev->pfvf);\r\nflowc->mnemval[1].mnemonic = FW_FLOWC_MNEM_CH;\r\nflowc->mnemval[1].val = htonl(csk->tx_chan);\r\nflowc->mnemval[2].mnemonic = FW_FLOWC_MNEM_PORT;\r\nflowc->mnemval[2].val = htonl(csk->tx_chan);\r\nflowc->mnemval[3].mnemonic = FW_FLOWC_MNEM_IQID;\r\nflowc->mnemval[3].val = htonl(csk->rss_qid);\r\nflowc->mnemval[4].mnemonic = FW_FLOWC_MNEM_SNDNXT;\r\nflowc->mnemval[4].val = htonl(csk->snd_nxt);\r\nflowc->mnemval[5].mnemonic = FW_FLOWC_MNEM_RCVNXT;\r\nflowc->mnemval[5].val = htonl(csk->rcv_nxt);\r\nflowc->mnemval[6].mnemonic = FW_FLOWC_MNEM_SNDBUF;\r\nflowc->mnemval[6].val = htonl(cxgb4i_snd_win);\r\nflowc->mnemval[7].mnemonic = FW_FLOWC_MNEM_MSS;\r\nflowc->mnemval[7].val = htonl(csk->advmss);\r\nflowc->mnemval[8].mnemonic = 0;\r\nflowc->mnemval[8].val = 0;\r\nfor (i = 0; i < 9; i++) {\r\nflowc->mnemval[i].r4[0] = 0;\r\nflowc->mnemval[i].r4[1] = 0;\r\nflowc->mnemval[i].r4[2] = 0;\r\n}\r\nset_queue(skb, CPL_PRIORITY_DATA, csk);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p, tid 0x%x, %u,%u,%u,%u,%u,%u,%u.\n",\r\ncsk, csk->tid, 0, csk->tx_chan, csk->rss_qid,\r\ncsk->snd_nxt, csk->rcv_nxt, cxgb4i_snd_win,\r\ncsk->advmss);\r\ncxgb4_ofld_send(csk->cdev->ports[csk->port_id], skb);\r\n}\r\nstatic inline void make_tx_data_wr(struct cxgbi_sock *csk, struct sk_buff *skb,\r\nint dlen, int len, u32 credits, int compl)\r\n{\r\nstruct fw_ofld_tx_data_wr *req;\r\nunsigned int submode = cxgbi_skcb_ulp_mode(skb) & 3;\r\nunsigned int wr_ulp_mode = 0;\r\nreq = (struct fw_ofld_tx_data_wr *)__skb_push(skb, sizeof(*req));\r\nif (is_ofld_imm(skb)) {\r\nreq->op_to_immdlen = htonl(FW_WR_OP(FW_OFLD_TX_DATA_WR) |\r\nFW_WR_COMPL(1) |\r\nFW_WR_IMMDLEN(dlen));\r\nreq->flowid_len16 = htonl(FW_WR_FLOWID(csk->tid) |\r\nFW_WR_LEN16(credits));\r\n} else {\r\nreq->op_to_immdlen =\r\ncpu_to_be32(FW_WR_OP(FW_OFLD_TX_DATA_WR) |\r\nFW_WR_COMPL(1) |\r\nFW_WR_IMMDLEN(0));\r\nreq->flowid_len16 =\r\ncpu_to_be32(FW_WR_FLOWID(csk->tid) |\r\nFW_WR_LEN16(credits));\r\n}\r\nif (submode)\r\nwr_ulp_mode = FW_OFLD_TX_DATA_WR_ULPMODE(ULP2_MODE_ISCSI) |\r\nFW_OFLD_TX_DATA_WR_ULPSUBMODE(submode);\r\nreq->tunnel_to_proxy = htonl(wr_ulp_mode |\r\nFW_OFLD_TX_DATA_WR_SHOVE(skb_peek(&csk->write_queue) ? 0 : 1));\r\nreq->plen = htonl(len);\r\nif (!cxgbi_sock_flag(csk, CTPF_TX_DATA_SENT))\r\ncxgbi_sock_set_flag(csk, CTPF_TX_DATA_SENT);\r\n}\r\nstatic void arp_failure_skb_discard(void *handle, struct sk_buff *skb)\r\n{\r\nkfree_skb(skb);\r\n}\r\nstatic int push_tx_frames(struct cxgbi_sock *csk, int req_completion)\r\n{\r\nint total_size = 0;\r\nstruct sk_buff *skb;\r\nif (unlikely(csk->state < CTP_ESTABLISHED ||\r\ncsk->state == CTP_CLOSE_WAIT_1 || csk->state >= CTP_ABORTING)) {\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK |\r\n1 << CXGBI_DBG_PDU_TX,\r\n"csk 0x%p,%u,0x%lx,%u, in closing state.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\nreturn 0;\r\n}\r\nwhile (csk->wr_cred && (skb = skb_peek(&csk->write_queue)) != NULL) {\r\nint dlen = skb->len;\r\nint len = skb->len;\r\nunsigned int credits_needed;\r\nskb_reset_transport_header(skb);\r\nif (is_ofld_imm(skb))\r\ncredits_needed = DIV_ROUND_UP(dlen +\r\nsizeof(struct fw_ofld_tx_data_wr), 16);\r\nelse\r\ncredits_needed = DIV_ROUND_UP(8*calc_tx_flits_ofld(skb)\r\n+ sizeof(struct fw_ofld_tx_data_wr),\r\n16);\r\nif (csk->wr_cred < credits_needed) {\r\nlog_debug(1 << CXGBI_DBG_PDU_TX,\r\n"csk 0x%p, skb %u/%u, wr %d < %u.\n",\r\ncsk, skb->len, skb->data_len,\r\ncredits_needed, csk->wr_cred);\r\nbreak;\r\n}\r\n__skb_unlink(skb, &csk->write_queue);\r\nset_queue(skb, CPL_PRIORITY_DATA, csk);\r\nskb->csum = credits_needed;\r\ncsk->wr_cred -= credits_needed;\r\ncsk->wr_una_cred += credits_needed;\r\ncxgbi_sock_enqueue_wr(csk, skb);\r\nlog_debug(1 << CXGBI_DBG_PDU_TX,\r\n"csk 0x%p, skb %u/%u, wr %d, left %u, unack %u.\n",\r\ncsk, skb->len, skb->data_len, credits_needed,\r\ncsk->wr_cred, csk->wr_una_cred);\r\nif (likely(cxgbi_skcb_test_flag(skb, SKCBF_TX_NEED_HDR))) {\r\nif (!cxgbi_sock_flag(csk, CTPF_TX_DATA_SENT)) {\r\nsend_tx_flowc_wr(csk);\r\nskb->csum += 5;\r\ncsk->wr_cred -= 5;\r\ncsk->wr_una_cred += 5;\r\n}\r\nlen += cxgbi_ulp_extra_len(cxgbi_skcb_ulp_mode(skb));\r\nmake_tx_data_wr(csk, skb, dlen, len, credits_needed,\r\nreq_completion);\r\ncsk->snd_nxt += len;\r\ncxgbi_skcb_clear_flag(skb, SKCBF_TX_NEED_HDR);\r\n}\r\ntotal_size += skb->truesize;\r\nt4_set_arp_err_handler(skb, csk, arp_failure_skb_discard);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_TX,\r\n"csk 0x%p,%u,0x%lx,%u, skb 0x%p, %u.\n",\r\ncsk, csk->state, csk->flags, csk->tid, skb, len);\r\ncxgb4_l2t_send(csk->cdev->ports[csk->port_id], skb, csk->l2t);\r\n}\r\nreturn total_size;\r\n}\r\nstatic inline void free_atid(struct cxgbi_sock *csk)\r\n{\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(csk->cdev);\r\nif (cxgbi_sock_flag(csk, CTPF_HAS_ATID)) {\r\ncxgb4_free_atid(lldi->tids, csk->atid);\r\ncxgbi_sock_clear_flag(csk, CTPF_HAS_ATID);\r\ncxgbi_sock_put(csk);\r\n}\r\n}\r\nstatic void do_act_establish(struct cxgbi_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk;\r\nstruct cpl_act_establish *req = (struct cpl_act_establish *)skb->data;\r\nunsigned short tcp_opt = ntohs(req->tcp_opt);\r\nunsigned int tid = GET_TID(req);\r\nunsigned int atid = GET_TID_TID(ntohl(req->tos_atid));\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\nu32 rcv_isn = be32_to_cpu(req->rcv_isn);\r\ncsk = lookup_atid(t, atid);\r\nif (unlikely(!csk)) {\r\npr_err("NO conn. for atid %u, cdev 0x%p.\n", atid, cdev);\r\ngoto rel_skb;\r\n}\r\nif (csk->atid != atid) {\r\npr_err("bad conn atid %u, csk 0x%p,%u,0x%lx,tid %u, atid %u.\n",\r\natid, csk, csk->state, csk->flags, csk->tid, csk->atid);\r\ngoto rel_skb;\r\n}\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx, tid %u, atid %u, rseq %u.\n",\r\ncsk, csk->state, csk->flags, tid, atid, rcv_isn);\r\ncxgbi_sock_get(csk);\r\ncsk->tid = tid;\r\ncxgb4_insert_tid(lldi->tids, csk, tid);\r\ncxgbi_sock_set_flag(csk, CTPF_HAS_TID);\r\nfree_atid(csk);\r\nspin_lock_bh(&csk->lock);\r\nif (unlikely(csk->state != CTP_ACTIVE_OPEN))\r\npr_info("csk 0x%p,%u,0x%lx,%u, got EST.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\nif (csk->retry_timer.function) {\r\ndel_timer(&csk->retry_timer);\r\ncsk->retry_timer.function = NULL;\r\n}\r\ncsk->copied_seq = csk->rcv_wup = csk->rcv_nxt = rcv_isn;\r\nif (cxgb4i_rcv_win > (RCV_BUFSIZ_MASK << 10))\r\ncsk->rcv_wup -= cxgb4i_rcv_win - (RCV_BUFSIZ_MASK << 10);\r\ncsk->advmss = lldi->mtus[GET_TCPOPT_MSS(tcp_opt)] - 40;\r\nif (GET_TCPOPT_TSTAMP(tcp_opt))\r\ncsk->advmss -= 12;\r\nif (csk->advmss < 128)\r\ncsk->advmss = 128;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p, mss_idx %u, advmss %u.\n",\r\ncsk, GET_TCPOPT_MSS(tcp_opt), csk->advmss);\r\ncxgbi_sock_established(csk, ntohl(req->snd_isn), ntohs(req->tcp_opt));\r\nif (unlikely(cxgbi_sock_flag(csk, CTPF_ACTIVE_CLOSE_NEEDED)))\r\nsend_abort_req(csk);\r\nelse {\r\nif (skb_queue_len(&csk->write_queue))\r\npush_tx_frames(csk, 0);\r\ncxgbi_conn_tx_open(csk);\r\n}\r\nspin_unlock_bh(&csk->lock);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic int act_open_rpl_status_to_errno(int status)\r\n{\r\nswitch (status) {\r\ncase CPL_ERR_CONN_RESET:\r\nreturn -ECONNREFUSED;\r\ncase CPL_ERR_ARP_MISS:\r\nreturn -EHOSTUNREACH;\r\ncase CPL_ERR_CONN_TIMEDOUT:\r\nreturn -ETIMEDOUT;\r\ncase CPL_ERR_TCAM_FULL:\r\nreturn -ENOMEM;\r\ncase CPL_ERR_CONN_EXIST:\r\nreturn -EADDRINUSE;\r\ndefault:\r\nreturn -EIO;\r\n}\r\n}\r\nstatic void csk_act_open_retry_timer(unsigned long data)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cxgbi_sock *csk = (struct cxgbi_sock *)data;\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(csk->cdev);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncxgbi_sock_get(csk);\r\nspin_lock_bh(&csk->lock);\r\nskb = alloc_wr(is_t4(lldi->adapter_type) ?\r\nsizeof(struct cpl_act_open_req) :\r\nsizeof(struct cpl_t5_act_open_req),\r\n0, GFP_ATOMIC);\r\nif (!skb)\r\ncxgbi_sock_fail_act_open(csk, -ENOMEM);\r\nelse {\r\nskb->sk = (struct sock *)csk;\r\nt4_set_arp_err_handler(skb, csk,\r\ncxgbi_sock_act_open_req_arp_failure);\r\nsend_act_open_req(csk, skb, csk->l2t);\r\n}\r\nspin_unlock_bh(&csk->lock);\r\ncxgbi_sock_put(csk);\r\n}\r\nstatic void do_act_open_rpl(struct cxgbi_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk;\r\nstruct cpl_act_open_rpl *rpl = (struct cpl_act_open_rpl *)skb->data;\r\nunsigned int tid = GET_TID(rpl);\r\nunsigned int atid =\r\nGET_TID_TID(GET_AOPEN_ATID(be32_to_cpu(rpl->atid_status)));\r\nunsigned int status = GET_AOPEN_STATUS(be32_to_cpu(rpl->atid_status));\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\ncsk = lookup_atid(t, atid);\r\nif (unlikely(!csk)) {\r\npr_err("NO matching conn. atid %u, tid %u.\n", atid, tid);\r\ngoto rel_skb;\r\n}\r\npr_info("%pI4:%u-%pI4:%u, atid %u,%u, status %u, csk 0x%p,%u,0x%lx.\n",\r\n&csk->saddr.sin_addr.s_addr, ntohs(csk->saddr.sin_port),\r\n&csk->daddr.sin_addr.s_addr, ntohs(csk->daddr.sin_port),\r\natid, tid, status, csk, csk->state, csk->flags);\r\nif (status == CPL_ERR_RTX_NEG_ADVICE)\r\ngoto rel_skb;\r\nif (status && status != CPL_ERR_TCAM_FULL &&\r\nstatus != CPL_ERR_CONN_EXIST &&\r\nstatus != CPL_ERR_ARP_MISS)\r\ncxgb4_remove_tid(lldi->tids, csk->port_id, GET_TID(rpl));\r\ncxgbi_sock_get(csk);\r\nspin_lock_bh(&csk->lock);\r\nif (status == CPL_ERR_CONN_EXIST &&\r\ncsk->retry_timer.function != csk_act_open_retry_timer) {\r\ncsk->retry_timer.function = csk_act_open_retry_timer;\r\nmod_timer(&csk->retry_timer, jiffies + HZ / 2);\r\n} else\r\ncxgbi_sock_fail_act_open(csk,\r\nact_open_rpl_status_to_errno(status));\r\nspin_unlock_bh(&csk->lock);\r\ncxgbi_sock_put(csk);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void do_peer_close(struct cxgbi_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk;\r\nstruct cpl_peer_close *req = (struct cpl_peer_close *)skb->data;\r\nunsigned int tid = GET_TID(req);\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk)) {\r\npr_err("can't find connection for tid %u.\n", tid);\r\ngoto rel_skb;\r\n}\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncxgbi_sock_rcv_peer_close(csk);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void do_close_con_rpl(struct cxgbi_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk;\r\nstruct cpl_close_con_rpl *rpl = (struct cpl_close_con_rpl *)skb->data;\r\nunsigned int tid = GET_TID(rpl);\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk)) {\r\npr_err("can't find connection for tid %u.\n", tid);\r\ngoto rel_skb;\r\n}\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncxgbi_sock_rcv_close_conn_rpl(csk, ntohl(rpl->snd_nxt));\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic int abort_status_to_errno(struct cxgbi_sock *csk, int abort_reason,\r\nint *need_rst)\r\n{\r\nswitch (abort_reason) {\r\ncase CPL_ERR_BAD_SYN:\r\ncase CPL_ERR_CONN_RESET:\r\nreturn csk->state > CTP_ESTABLISHED ?\r\n-EPIPE : -ECONNRESET;\r\ncase CPL_ERR_XMIT_TIMEDOUT:\r\ncase CPL_ERR_PERSIST_TIMEDOUT:\r\ncase CPL_ERR_FINWAIT2_TIMEDOUT:\r\ncase CPL_ERR_KEEPALIVE_TIMEDOUT:\r\nreturn -ETIMEDOUT;\r\ndefault:\r\nreturn -EIO;\r\n}\r\n}\r\nstatic void do_abort_req_rss(struct cxgbi_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk;\r\nstruct cpl_abort_req_rss *req = (struct cpl_abort_req_rss *)skb->data;\r\nunsigned int tid = GET_TID(req);\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\nint rst_status = CPL_ABORT_NO_RST;\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk)) {\r\npr_err("can't find connection for tid %u.\n", tid);\r\ngoto rel_skb;\r\n}\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx, tid %u, status 0x%x.\n",\r\ncsk, csk->state, csk->flags, csk->tid, req->status);\r\nif (req->status == CPL_ERR_RTX_NEG_ADVICE ||\r\nreq->status == CPL_ERR_PERSIST_NEG_ADVICE)\r\ngoto rel_skb;\r\ncxgbi_sock_get(csk);\r\nspin_lock_bh(&csk->lock);\r\nif (!cxgbi_sock_flag(csk, CTPF_ABORT_REQ_RCVD)) {\r\ncxgbi_sock_set_flag(csk, CTPF_ABORT_REQ_RCVD);\r\ncxgbi_sock_set_state(csk, CTP_ABORTING);\r\ngoto done;\r\n}\r\ncxgbi_sock_clear_flag(csk, CTPF_ABORT_REQ_RCVD);\r\nsend_abort_rpl(csk, rst_status);\r\nif (!cxgbi_sock_flag(csk, CTPF_ABORT_RPL_PENDING)) {\r\ncsk->err = abort_status_to_errno(csk, req->status, &rst_status);\r\ncxgbi_sock_closed(csk);\r\n}\r\ndone:\r\nspin_unlock_bh(&csk->lock);\r\ncxgbi_sock_put(csk);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void do_abort_rpl_rss(struct cxgbi_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk;\r\nstruct cpl_abort_rpl_rss *rpl = (struct cpl_abort_rpl_rss *)skb->data;\r\nunsigned int tid = GET_TID(rpl);\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\ncsk = lookup_tid(t, tid);\r\nif (!csk)\r\ngoto rel_skb;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"status 0x%x, csk 0x%p, s %u, 0x%lx.\n",\r\nrpl->status, csk, csk ? csk->state : 0,\r\ncsk ? csk->flags : 0UL);\r\nif (rpl->status == CPL_ERR_ABORT_FAILED)\r\ngoto rel_skb;\r\ncxgbi_sock_rcv_abort_rpl(csk);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void do_rx_iscsi_hdr(struct cxgbi_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk;\r\nstruct cpl_iscsi_hdr *cpl = (struct cpl_iscsi_hdr *)skb->data;\r\nunsigned short pdu_len_ddp = be16_to_cpu(cpl->pdu_len_ddp);\r\nunsigned int tid = GET_TID(cpl);\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk)) {\r\npr_err("can't find conn. for tid %u.\n", tid);\r\ngoto rel_skb;\r\n}\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p,%u,0x%lx, tid %u, skb 0x%p,%u, 0x%x.\n",\r\ncsk, csk->state, csk->flags, csk->tid, skb, skb->len,\r\npdu_len_ddp);\r\nspin_lock_bh(&csk->lock);\r\nif (unlikely(csk->state >= CTP_PASSIVE_CLOSE)) {\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u, bad state.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\nif (csk->state != CTP_ABORTING)\r\ngoto abort_conn;\r\nelse\r\ngoto discard;\r\n}\r\ncxgbi_skcb_tcp_seq(skb) = ntohl(cpl->seq);\r\ncxgbi_skcb_flags(skb) = 0;\r\nskb_reset_transport_header(skb);\r\n__skb_pull(skb, sizeof(*cpl));\r\n__pskb_trim(skb, ntohs(cpl->len));\r\nif (!csk->skb_ulp_lhdr) {\r\nunsigned char *bhs;\r\nunsigned int hlen, dlen, plen;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p,%u,0x%lx, tid %u, skb 0x%p header.\n",\r\ncsk, csk->state, csk->flags, csk->tid, skb);\r\ncsk->skb_ulp_lhdr = skb;\r\ncxgbi_skcb_set_flag(skb, SKCBF_RX_HDR);\r\nif (cxgbi_skcb_tcp_seq(skb) != csk->rcv_nxt) {\r\npr_info("tid %u, CPL_ISCSI_HDR, bad seq, 0x%x/0x%x.\n",\r\ncsk->tid, cxgbi_skcb_tcp_seq(skb),\r\ncsk->rcv_nxt);\r\ngoto abort_conn;\r\n}\r\nbhs = skb->data;\r\nhlen = ntohs(cpl->len);\r\ndlen = ntohl(*(unsigned int *)(bhs + 4)) & 0xFFFFFF;\r\nplen = ISCSI_PDU_LEN(pdu_len_ddp);\r\nif (is_t4(lldi->adapter_type))\r\nplen -= 40;\r\nif ((hlen + dlen) != plen) {\r\npr_info("tid 0x%x, CPL_ISCSI_HDR, pdu len "\r\n"mismatch %u != %u + %u, seq 0x%x.\n",\r\ncsk->tid, plen, hlen, dlen,\r\ncxgbi_skcb_tcp_seq(skb));\r\ngoto abort_conn;\r\n}\r\ncxgbi_skcb_rx_pdulen(skb) = (hlen + dlen + 3) & (~0x3);\r\nif (dlen)\r\ncxgbi_skcb_rx_pdulen(skb) += csk->dcrc_len;\r\ncsk->rcv_nxt += cxgbi_skcb_rx_pdulen(skb);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p, skb 0x%p, 0x%x,%u+%u,0x%x,0x%x.\n",\r\ncsk, skb, *bhs, hlen, dlen,\r\nntohl(*((unsigned int *)(bhs + 16))),\r\nntohl(*((unsigned int *)(bhs + 24))));\r\n} else {\r\nstruct sk_buff *lskb = csk->skb_ulp_lhdr;\r\ncxgbi_skcb_set_flag(lskb, SKCBF_RX_DATA);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p,%u,0x%lx, skb 0x%p data, 0x%p.\n",\r\ncsk, csk->state, csk->flags, skb, lskb);\r\n}\r\n__skb_queue_tail(&csk->receive_queue, skb);\r\nspin_unlock_bh(&csk->lock);\r\nreturn;\r\nabort_conn:\r\nsend_abort_req(csk);\r\ndiscard:\r\nspin_unlock_bh(&csk->lock);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void do_rx_data_ddp(struct cxgbi_device *cdev,\r\nstruct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk;\r\nstruct sk_buff *lskb;\r\nstruct cpl_rx_data_ddp *rpl = (struct cpl_rx_data_ddp *)skb->data;\r\nunsigned int tid = GET_TID(rpl);\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\nunsigned int status = ntohl(rpl->ddpvld);\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk)) {\r\npr_err("can't find connection for tid %u.\n", tid);\r\ngoto rel_skb;\r\n}\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p,%u,0x%lx, skb 0x%p,0x%x, lhdr 0x%p.\n",\r\ncsk, csk->state, csk->flags, skb, status, csk->skb_ulp_lhdr);\r\nspin_lock_bh(&csk->lock);\r\nif (unlikely(csk->state >= CTP_PASSIVE_CLOSE)) {\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u, bad state.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\nif (csk->state != CTP_ABORTING)\r\ngoto abort_conn;\r\nelse\r\ngoto discard;\r\n}\r\nif (!csk->skb_ulp_lhdr) {\r\npr_err("tid 0x%x, rcv RX_DATA_DDP w/o pdu bhs.\n", csk->tid);\r\ngoto abort_conn;\r\n}\r\nlskb = csk->skb_ulp_lhdr;\r\ncsk->skb_ulp_lhdr = NULL;\r\ncxgbi_skcb_rx_ddigest(lskb) = ntohl(rpl->ulp_crc);\r\nif (ntohs(rpl->len) != cxgbi_skcb_rx_pdulen(lskb))\r\npr_info("tid 0x%x, RX_DATA_DDP pdulen %u != %u.\n",\r\ncsk->tid, ntohs(rpl->len), cxgbi_skcb_rx_pdulen(lskb));\r\nif (status & (1 << CPL_RX_DDP_STATUS_HCRC_SHIFT)) {\r\npr_info("csk 0x%p, lhdr 0x%p, status 0x%x, hcrc bad 0x%lx.\n",\r\ncsk, lskb, status, cxgbi_skcb_flags(lskb));\r\ncxgbi_skcb_set_flag(lskb, SKCBF_RX_HCRC_ERR);\r\n}\r\nif (status & (1 << CPL_RX_DDP_STATUS_DCRC_SHIFT)) {\r\npr_info("csk 0x%p, lhdr 0x%p, status 0x%x, dcrc bad 0x%lx.\n",\r\ncsk, lskb, status, cxgbi_skcb_flags(lskb));\r\ncxgbi_skcb_set_flag(lskb, SKCBF_RX_DCRC_ERR);\r\n}\r\nif (status & (1 << CPL_RX_DDP_STATUS_PAD_SHIFT)) {\r\nlog_debug(1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p, lhdr 0x%p, status 0x%x, pad bad.\n",\r\ncsk, lskb, status);\r\ncxgbi_skcb_set_flag(lskb, SKCBF_RX_PAD_ERR);\r\n}\r\nif ((status & (1 << CPL_RX_DDP_STATUS_DDP_SHIFT)) &&\r\n!cxgbi_skcb_test_flag(lskb, SKCBF_RX_DATA)) {\r\nlog_debug(1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p, lhdr 0x%p, 0x%x, data ddp'ed.\n",\r\ncsk, lskb, status);\r\ncxgbi_skcb_set_flag(lskb, SKCBF_RX_DATA_DDPD);\r\n}\r\nlog_debug(1 << CXGBI_DBG_PDU_RX,\r\n"csk 0x%p, lskb 0x%p, f 0x%lx.\n",\r\ncsk, lskb, cxgbi_skcb_flags(lskb));\r\ncxgbi_skcb_set_flag(lskb, SKCBF_RX_STATUS);\r\ncxgbi_conn_pdu_ready(csk);\r\nspin_unlock_bh(&csk->lock);\r\ngoto rel_skb;\r\nabort_conn:\r\nsend_abort_req(csk);\r\ndiscard:\r\nspin_unlock_bh(&csk->lock);\r\nrel_skb:\r\n__kfree_skb(skb);\r\n}\r\nstatic void do_fw4_ack(struct cxgbi_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cxgbi_sock *csk;\r\nstruct cpl_fw4_ack *rpl = (struct cpl_fw4_ack *)skb->data;\r\nunsigned int tid = GET_TID(rpl);\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\ncsk = lookup_tid(t, tid);\r\nif (unlikely(!csk))\r\npr_err("can't find connection for tid %u.\n", tid);\r\nelse {\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncxgbi_sock_rcv_wr_ack(csk, rpl->credits, ntohl(rpl->snd_una),\r\nrpl->seq_vld);\r\n}\r\n__kfree_skb(skb);\r\n}\r\nstatic void do_set_tcb_rpl(struct cxgbi_device *cdev, struct sk_buff *skb)\r\n{\r\nstruct cpl_set_tcb_rpl *rpl = (struct cpl_set_tcb_rpl *)skb->data;\r\nunsigned int tid = GET_TID(rpl);\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct tid_info *t = lldi->tids;\r\nstruct cxgbi_sock *csk;\r\ncsk = lookup_tid(t, tid);\r\nif (!csk)\r\npr_err("can't find conn. for tid %u.\n", tid);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,%lx,%u, status 0x%x.\n",\r\ncsk, csk->state, csk->flags, csk->tid, rpl->status);\r\nif (rpl->status != CPL_ERR_NONE)\r\npr_err("csk 0x%p,%u, SET_TCB_RPL status %u.\n",\r\ncsk, tid, rpl->status);\r\n__kfree_skb(skb);\r\n}\r\nstatic int alloc_cpls(struct cxgbi_sock *csk)\r\n{\r\ncsk->cpl_close = alloc_wr(sizeof(struct cpl_close_con_req),\r\n0, GFP_KERNEL);\r\nif (!csk->cpl_close)\r\nreturn -ENOMEM;\r\ncsk->cpl_abort_req = alloc_wr(sizeof(struct cpl_abort_req),\r\n0, GFP_KERNEL);\r\nif (!csk->cpl_abort_req)\r\ngoto free_cpls;\r\ncsk->cpl_abort_rpl = alloc_wr(sizeof(struct cpl_abort_rpl),\r\n0, GFP_KERNEL);\r\nif (!csk->cpl_abort_rpl)\r\ngoto free_cpls;\r\nreturn 0;\r\nfree_cpls:\r\ncxgbi_sock_free_cpl_skbs(csk);\r\nreturn -ENOMEM;\r\n}\r\nstatic inline void l2t_put(struct cxgbi_sock *csk)\r\n{\r\nif (csk->l2t) {\r\ncxgb4_l2t_release(csk->l2t);\r\ncsk->l2t = NULL;\r\ncxgbi_sock_put(csk);\r\n}\r\n}\r\nstatic void release_offload_resources(struct cxgbi_sock *csk)\r\n{\r\nstruct cxgb4_lld_info *lldi;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncxgbi_sock_free_cpl_skbs(csk);\r\nif (csk->wr_cred != csk->wr_max_cred) {\r\ncxgbi_sock_purge_wr_queue(csk);\r\ncxgbi_sock_reset_wr_list(csk);\r\n}\r\nl2t_put(csk);\r\nif (cxgbi_sock_flag(csk, CTPF_HAS_ATID))\r\nfree_atid(csk);\r\nelse if (cxgbi_sock_flag(csk, CTPF_HAS_TID)) {\r\nlldi = cxgbi_cdev_priv(csk->cdev);\r\ncxgb4_remove_tid(lldi->tids, 0, csk->tid);\r\ncxgbi_sock_clear_flag(csk, CTPF_HAS_TID);\r\ncxgbi_sock_put(csk);\r\n}\r\ncsk->dst = NULL;\r\ncsk->cdev = NULL;\r\n}\r\nstatic int init_act_open(struct cxgbi_sock *csk)\r\n{\r\nstruct cxgbi_device *cdev = csk->cdev;\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct net_device *ndev = cdev->ports[csk->port_id];\r\nstruct port_info *pi = netdev_priv(ndev);\r\nstruct sk_buff *skb = NULL;\r\nstruct neighbour *n;\r\nunsigned int step;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,%u,0x%lx,%u.\n",\r\ncsk, csk->state, csk->flags, csk->tid);\r\ncsk->atid = cxgb4_alloc_atid(lldi->tids, csk);\r\nif (csk->atid < 0) {\r\npr_err("%s, NO atid available.\n", ndev->name);\r\nreturn -EINVAL;\r\n}\r\ncxgbi_sock_set_flag(csk, CTPF_HAS_ATID);\r\ncxgbi_sock_get(csk);\r\nn = dst_neigh_lookup(csk->dst, &csk->daddr.sin_addr.s_addr);\r\nif (!n) {\r\npr_err("%s, can't get neighbour of csk->dst.\n", ndev->name);\r\ngoto rel_resource;\r\n}\r\ncsk->l2t = cxgb4_l2t_get(lldi->l2t, n, ndev, 0);\r\nif (!csk->l2t) {\r\npr_err("%s, cannot alloc l2t.\n", ndev->name);\r\ngoto rel_resource;\r\n}\r\ncxgbi_sock_get(csk);\r\nskb = alloc_wr(is_t4(lldi->adapter_type) ?\r\nsizeof(struct cpl_act_open_req) :\r\nsizeof(struct cpl_t5_act_open_req),\r\n0, GFP_ATOMIC);\r\nif (!skb)\r\ngoto rel_resource;\r\nskb->sk = (struct sock *)csk;\r\nt4_set_arp_err_handler(skb, csk, cxgbi_sock_act_open_req_arp_failure);\r\nif (!csk->mtu)\r\ncsk->mtu = dst_mtu(csk->dst);\r\ncxgb4_best_mtu(lldi->mtus, csk->mtu, &csk->mss_idx);\r\ncsk->tx_chan = cxgb4_port_chan(ndev);\r\ncsk->smac_idx = ((cxgb4_port_viid(ndev) & 0x7F)) << 1;\r\nstep = lldi->ntxq / lldi->nchan;\r\ncsk->txq_idx = cxgb4_port_idx(ndev) * step;\r\nstep = lldi->nrxq / lldi->nchan;\r\ncsk->rss_qid = lldi->rxq_ids[cxgb4_port_idx(ndev) * step];\r\ncsk->wr_max_cred = csk->wr_cred = lldi->wr_cred;\r\ncsk->wr_una_cred = 0;\r\ncxgbi_sock_reset_wr_list(csk);\r\ncsk->err = 0;\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p,p%d,%s, %u,%u,%u, mss %u,%u, smac %u.\n",\r\ncsk, pi->port_id, ndev->name, csk->tx_chan,\r\ncsk->txq_idx, csk->rss_qid, csk->mtu, csk->mss_idx,\r\ncsk->smac_idx);\r\ncxgbi_sock_set_state(csk, CTP_ACTIVE_OPEN);\r\nsend_act_open_req(csk, skb, csk->l2t);\r\nneigh_release(n);\r\nreturn 0;\r\nrel_resource:\r\nif (n)\r\nneigh_release(n);\r\nif (skb)\r\n__kfree_skb(skb);\r\nreturn -EINVAL;\r\n}\r\nint cxgb4i_ofld_init(struct cxgbi_device *cdev)\r\n{\r\nint rc;\r\nif (cxgb4i_max_connect > CXGB4I_MAX_CONN)\r\ncxgb4i_max_connect = CXGB4I_MAX_CONN;\r\nrc = cxgbi_device_portmap_create(cdev, cxgb4i_sport_base,\r\ncxgb4i_max_connect);\r\nif (rc < 0)\r\nreturn rc;\r\ncdev->csk_release_offload_resources = release_offload_resources;\r\ncdev->csk_push_tx_frames = push_tx_frames;\r\ncdev->csk_send_abort_req = send_abort_req;\r\ncdev->csk_send_close_req = send_close_req;\r\ncdev->csk_send_rx_credits = send_rx_credits;\r\ncdev->csk_alloc_cpls = alloc_cpls;\r\ncdev->csk_init_act_open = init_act_open;\r\npr_info("cdev 0x%p, offload up, added.\n", cdev);\r\nreturn 0;\r\n}\r\nstatic inline void ulp_mem_io_set_hdr(struct cxgb4_lld_info *lldi,\r\nstruct ulp_mem_io *req,\r\nunsigned int wr_len, unsigned int dlen,\r\nunsigned int pm_addr)\r\n{\r\nstruct ulptx_idata *idata = (struct ulptx_idata *)(req + 1);\r\nINIT_ULPTX_WR(req, wr_len, 0, 0);\r\nif (is_t4(lldi->adapter_type))\r\nreq->cmd = htonl(ULPTX_CMD(ULP_TX_MEM_WRITE) |\r\n(ULP_MEMIO_ORDER(1)));\r\nelse\r\nreq->cmd = htonl(ULPTX_CMD(ULP_TX_MEM_WRITE) |\r\n(V_T5_ULP_MEMIO_IMM(1)));\r\nreq->dlen = htonl(ULP_MEMIO_DATA_LEN(dlen >> 5));\r\nreq->lock_addr = htonl(ULP_MEMIO_ADDR(pm_addr >> 5));\r\nreq->len16 = htonl(DIV_ROUND_UP(wr_len - sizeof(req->wr), 16));\r\nidata->cmd_more = htonl(ULPTX_CMD(ULP_TX_SC_IMM));\r\nidata->len = htonl(dlen);\r\n}\r\nstatic int ddp_ppod_write_idata(struct cxgbi_device *cdev, unsigned int port_id,\r\nstruct cxgbi_pagepod_hdr *hdr, unsigned int idx,\r\nunsigned int npods,\r\nstruct cxgbi_gather_list *gl,\r\nunsigned int gl_pidx)\r\n{\r\nstruct cxgbi_ddp_info *ddp = cdev->ddp;\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct sk_buff *skb;\r\nstruct ulp_mem_io *req;\r\nstruct ulptx_idata *idata;\r\nstruct cxgbi_pagepod *ppod;\r\nunsigned int pm_addr = idx * PPOD_SIZE + ddp->llimit;\r\nunsigned int dlen = PPOD_SIZE * npods;\r\nunsigned int wr_len = roundup(sizeof(struct ulp_mem_io) +\r\nsizeof(struct ulptx_idata) + dlen, 16);\r\nunsigned int i;\r\nskb = alloc_wr(wr_len, 0, GFP_ATOMIC);\r\nif (!skb) {\r\npr_err("cdev 0x%p, idx %u, npods %u, OOM.\n",\r\ncdev, idx, npods);\r\nreturn -ENOMEM;\r\n}\r\nreq = (struct ulp_mem_io *)skb->head;\r\nset_queue(skb, CPL_PRIORITY_CONTROL, NULL);\r\nulp_mem_io_set_hdr(lldi, req, wr_len, dlen, pm_addr);\r\nidata = (struct ulptx_idata *)(req + 1);\r\nppod = (struct cxgbi_pagepod *)(idata + 1);\r\nfor (i = 0; i < npods; i++, ppod++, gl_pidx += PPOD_PAGES_MAX) {\r\nif (!hdr && !gl)\r\ncxgbi_ddp_ppod_clear(ppod);\r\nelse\r\ncxgbi_ddp_ppod_set(ppod, hdr, gl, gl_pidx);\r\n}\r\ncxgb4_ofld_send(cdev->ports[port_id], skb);\r\nreturn 0;\r\n}\r\nstatic int ddp_set_map(struct cxgbi_sock *csk, struct cxgbi_pagepod_hdr *hdr,\r\nunsigned int idx, unsigned int npods,\r\nstruct cxgbi_gather_list *gl)\r\n{\r\nunsigned int i, cnt;\r\nint err = 0;\r\nfor (i = 0; i < npods; i += cnt, idx += cnt) {\r\ncnt = npods - i;\r\nif (cnt > ULPMEM_IDATA_MAX_NPPODS)\r\ncnt = ULPMEM_IDATA_MAX_NPPODS;\r\nerr = ddp_ppod_write_idata(csk->cdev, csk->port_id, hdr,\r\nidx, cnt, gl, 4 * i);\r\nif (err < 0)\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic void ddp_clear_map(struct cxgbi_hba *chba, unsigned int tag,\r\nunsigned int idx, unsigned int npods)\r\n{\r\nunsigned int i, cnt;\r\nint err;\r\nfor (i = 0; i < npods; i += cnt, idx += cnt) {\r\ncnt = npods - i;\r\nif (cnt > ULPMEM_IDATA_MAX_NPPODS)\r\ncnt = ULPMEM_IDATA_MAX_NPPODS;\r\nerr = ddp_ppod_write_idata(chba->cdev, chba->port_id, NULL,\r\nidx, cnt, NULL, 0);\r\nif (err < 0)\r\nbreak;\r\n}\r\n}\r\nstatic int ddp_setup_conn_pgidx(struct cxgbi_sock *csk, unsigned int tid,\r\nint pg_idx, bool reply)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cpl_set_tcb_field *req;\r\nif (!pg_idx || pg_idx >= DDP_PGIDX_MAX)\r\nreturn 0;\r\nskb = alloc_wr(sizeof(*req), 0, GFP_KERNEL);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nreq = (struct cpl_set_tcb_field *)skb->head;\r\nINIT_TP_WR(req, csk->tid);\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, csk->tid));\r\nreq->reply_ctrl = htons(NO_REPLY(reply) | QUEUENO(csk->rss_qid));\r\nreq->word_cookie = htons(0);\r\nreq->mask = cpu_to_be64(0x3 << 8);\r\nreq->val = cpu_to_be64(pg_idx << 8);\r\nset_wr_txq(skb, CPL_PRIORITY_CONTROL, csk->port_id);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p, tid 0x%x, pg_idx %u.\n", csk, csk->tid, pg_idx);\r\ncxgb4_ofld_send(csk->cdev->ports[csk->port_id], skb);\r\nreturn 0;\r\n}\r\nstatic int ddp_setup_conn_digest(struct cxgbi_sock *csk, unsigned int tid,\r\nint hcrc, int dcrc, int reply)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cpl_set_tcb_field *req;\r\nif (!hcrc && !dcrc)\r\nreturn 0;\r\nskb = alloc_wr(sizeof(*req), 0, GFP_KERNEL);\r\nif (!skb)\r\nreturn -ENOMEM;\r\ncsk->hcrc_len = (hcrc ? 4 : 0);\r\ncsk->dcrc_len = (dcrc ? 4 : 0);\r\nreq = (struct cpl_set_tcb_field *)skb->head;\r\nINIT_TP_WR(req, tid);\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, tid));\r\nreq->reply_ctrl = htons(NO_REPLY(reply) | QUEUENO(csk->rss_qid));\r\nreq->word_cookie = htons(0);\r\nreq->mask = cpu_to_be64(0x3 << 4);\r\nreq->val = cpu_to_be64(((hcrc ? ULP_CRC_HEADER : 0) |\r\n(dcrc ? ULP_CRC_DATA : 0)) << 4);\r\nset_wr_txq(skb, CPL_PRIORITY_CONTROL, csk->port_id);\r\nlog_debug(1 << CXGBI_DBG_TOE | 1 << CXGBI_DBG_SOCK,\r\n"csk 0x%p, tid 0x%x, crc %d,%d.\n", csk, csk->tid, hcrc, dcrc);\r\ncxgb4_ofld_send(csk->cdev->ports[csk->port_id], skb);\r\nreturn 0;\r\n}\r\nstatic int cxgb4i_ddp_init(struct cxgbi_device *cdev)\r\n{\r\nstruct cxgb4_lld_info *lldi = cxgbi_cdev_priv(cdev);\r\nstruct cxgbi_ddp_info *ddp = cdev->ddp;\r\nunsigned int tagmask, pgsz_factor[4];\r\nint err;\r\nif (ddp) {\r\nkref_get(&ddp->refcnt);\r\npr_warn("cdev 0x%p, ddp 0x%p already set up.\n",\r\ncdev, cdev->ddp);\r\nreturn -EALREADY;\r\n}\r\nerr = cxgbi_ddp_init(cdev, lldi->vr->iscsi.start,\r\nlldi->vr->iscsi.start + lldi->vr->iscsi.size - 1,\r\nlldi->iscsi_iolen, lldi->iscsi_iolen);\r\nif (err < 0)\r\nreturn err;\r\nddp = cdev->ddp;\r\ntagmask = ddp->idx_mask << PPOD_IDX_SHIFT;\r\ncxgbi_ddp_page_size_factor(pgsz_factor);\r\ncxgb4_iscsi_init(lldi->ports[0], tagmask, pgsz_factor);\r\ncdev->csk_ddp_setup_digest = ddp_setup_conn_digest;\r\ncdev->csk_ddp_setup_pgidx = ddp_setup_conn_pgidx;\r\ncdev->csk_ddp_set = ddp_set_map;\r\ncdev->csk_ddp_clear = ddp_clear_map;\r\npr_info("cxgb4i 0x%p tag: sw %u, rsvd %u,%u, mask 0x%x.\n",\r\ncdev, cdev->tag_format.sw_bits, cdev->tag_format.rsvd_bits,\r\ncdev->tag_format.rsvd_shift, cdev->tag_format.rsvd_mask);\r\npr_info("cxgb4i 0x%p, nppods %u, bits %u, mask 0x%x,0x%x pkt %u/%u, "\r\n" %u/%u.\n",\r\ncdev, ddp->nppods, ddp->idx_bits, ddp->idx_mask,\r\nddp->rsvd_tag_mask, ddp->max_txsz, lldi->iscsi_iolen,\r\nddp->max_rxsz, lldi->iscsi_iolen);\r\npr_info("cxgb4i 0x%p max payload size: %u/%u, %u/%u.\n",\r\ncdev, cdev->tx_max_size, ddp->max_txsz, cdev->rx_max_size,\r\nddp->max_rxsz);\r\nreturn 0;\r\n}\r\nstatic void *t4_uld_add(const struct cxgb4_lld_info *lldi)\r\n{\r\nstruct cxgbi_device *cdev;\r\nstruct port_info *pi;\r\nint i, rc;\r\ncdev = cxgbi_device_register(sizeof(*lldi), lldi->nports);\r\nif (!cdev) {\r\npr_info("t4 device 0x%p, register failed.\n", lldi);\r\nreturn NULL;\r\n}\r\npr_info("0x%p,0x%x, ports %u,%s, chan %u, q %u,%u, wr %u.\n",\r\ncdev, lldi->adapter_type, lldi->nports,\r\nlldi->ports[0]->name, lldi->nchan, lldi->ntxq,\r\nlldi->nrxq, lldi->wr_cred);\r\nfor (i = 0; i < lldi->nrxq; i++)\r\nlog_debug(1 << CXGBI_DBG_DEV,\r\n"t4 0x%p, rxq id #%d: %u.\n",\r\ncdev, i, lldi->rxq_ids[i]);\r\nmemcpy(cxgbi_cdev_priv(cdev), lldi, sizeof(*lldi));\r\ncdev->flags = CXGBI_FLAG_DEV_T4;\r\ncdev->pdev = lldi->pdev;\r\ncdev->ports = lldi->ports;\r\ncdev->nports = lldi->nports;\r\ncdev->mtus = lldi->mtus;\r\ncdev->nmtus = NMTUS;\r\ncdev->snd_win = cxgb4i_snd_win;\r\ncdev->rcv_win = cxgb4i_rcv_win;\r\ncdev->rx_credit_thres = cxgb4i_rx_credit_thres;\r\ncdev->skb_tx_rsvd = CXGB4I_TX_HEADER_LEN;\r\ncdev->skb_rx_extra = sizeof(struct cpl_iscsi_hdr);\r\ncdev->itp = &cxgb4i_iscsi_transport;\r\ncdev->pfvf = FW_VIID_PFN_GET(cxgb4_port_viid(lldi->ports[0])) << 8;\r\npr_info("cdev 0x%p,%s, pfvf %u.\n",\r\ncdev, lldi->ports[0]->name, cdev->pfvf);\r\nrc = cxgb4i_ddp_init(cdev);\r\nif (rc) {\r\npr_info("t4 0x%p ddp init failed.\n", cdev);\r\ngoto err_out;\r\n}\r\nrc = cxgb4i_ofld_init(cdev);\r\nif (rc) {\r\npr_info("t4 0x%p ofld init failed.\n", cdev);\r\ngoto err_out;\r\n}\r\nrc = cxgbi_hbas_add(cdev, CXGB4I_MAX_LUN, CXGBI_MAX_CONN,\r\n&cxgb4i_host_template, cxgb4i_stt);\r\nif (rc)\r\ngoto err_out;\r\nfor (i = 0; i < cdev->nports; i++) {\r\npi = netdev_priv(lldi->ports[i]);\r\ncdev->hbas[i]->port_id = pi->port_id;\r\n}\r\nreturn cdev;\r\nerr_out:\r\ncxgbi_device_unregister(cdev);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nstatic int t4_uld_rx_handler(void *handle, const __be64 *rsp,\r\nconst struct pkt_gl *pgl)\r\n{\r\nconst struct cpl_act_establish *rpl;\r\nstruct sk_buff *skb;\r\nunsigned int opc;\r\nstruct cxgbi_device *cdev = handle;\r\nif (pgl == NULL) {\r\nunsigned int len = 64 - sizeof(struct rsp_ctrl) - 8;\r\nskb = alloc_wr(len, 0, GFP_ATOMIC);\r\nif (!skb)\r\ngoto nomem;\r\nskb_copy_to_linear_data(skb, &rsp[1], len);\r\n} else {\r\nif (unlikely(*(u8 *)rsp != *(u8 *)pgl->va)) {\r\npr_info("? FL 0x%p,RSS%#llx,FL %#llx,len %u.\n",\r\npgl->va, be64_to_cpu(*rsp),\r\nbe64_to_cpu(*(u64 *)pgl->va),\r\npgl->tot_len);\r\nreturn 0;\r\n}\r\nskb = cxgb4_pktgl_to_skb(pgl, RX_PULL_LEN, RX_PULL_LEN);\r\nif (unlikely(!skb))\r\ngoto nomem;\r\n}\r\nrpl = (struct cpl_act_establish *)skb->data;\r\nopc = rpl->ot.opcode;\r\nlog_debug(1 << CXGBI_DBG_TOE,\r\n"cdev %p, opcode 0x%x(0x%x,0x%x), skb %p.\n",\r\ncdev, opc, rpl->ot.opcode_tid, ntohl(rpl->ot.opcode_tid), skb);\r\nif (cxgb4i_cplhandlers[opc])\r\ncxgb4i_cplhandlers[opc](cdev, skb);\r\nelse {\r\npr_err("No handler for opcode 0x%x.\n", opc);\r\n__kfree_skb(skb);\r\n}\r\nreturn 0;\r\nnomem:\r\nlog_debug(1 << CXGBI_DBG_TOE, "OOM bailing out.\n");\r\nreturn 1;\r\n}\r\nstatic int t4_uld_state_change(void *handle, enum cxgb4_state state)\r\n{\r\nstruct cxgbi_device *cdev = handle;\r\nswitch (state) {\r\ncase CXGB4_STATE_UP:\r\npr_info("cdev 0x%p, UP.\n", cdev);\r\nbreak;\r\ncase CXGB4_STATE_START_RECOVERY:\r\npr_info("cdev 0x%p, RECOVERY.\n", cdev);\r\nbreak;\r\ncase CXGB4_STATE_DOWN:\r\npr_info("cdev 0x%p, DOWN.\n", cdev);\r\nbreak;\r\ncase CXGB4_STATE_DETACH:\r\npr_info("cdev 0x%p, DETACH.\n", cdev);\r\ncxgbi_device_unregister(cdev);\r\nbreak;\r\ndefault:\r\npr_info("cdev 0x%p, unknown state %d.\n", cdev, state);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init cxgb4i_init_module(void)\r\n{\r\nint rc;\r\nprintk(KERN_INFO "%s", version);\r\nrc = cxgbi_iscsi_init(&cxgb4i_iscsi_transport, &cxgb4i_stt);\r\nif (rc < 0)\r\nreturn rc;\r\ncxgb4_register_uld(CXGB4_ULD_ISCSI, &cxgb4i_uld_info);\r\nreturn 0;\r\n}\r\nstatic void __exit cxgb4i_exit_module(void)\r\n{\r\ncxgb4_unregister_uld(CXGB4_ULD_ISCSI);\r\ncxgbi_device_unregister_all(CXGBI_FLAG_DEV_T4);\r\ncxgbi_iscsi_cleanup(&cxgb4i_iscsi_transport, &cxgb4i_stt);\r\n}
