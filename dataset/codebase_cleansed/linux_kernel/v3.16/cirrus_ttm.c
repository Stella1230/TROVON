static inline struct cirrus_device *\r\ncirrus_bdev(struct ttm_bo_device *bd)\r\n{\r\nreturn container_of(bd, struct cirrus_device, ttm.bdev);\r\n}\r\nstatic int\r\ncirrus_ttm_mem_global_init(struct drm_global_reference *ref)\r\n{\r\nreturn ttm_mem_global_init(ref->object);\r\n}\r\nstatic void\r\ncirrus_ttm_mem_global_release(struct drm_global_reference *ref)\r\n{\r\nttm_mem_global_release(ref->object);\r\n}\r\nstatic int cirrus_ttm_global_init(struct cirrus_device *cirrus)\r\n{\r\nstruct drm_global_reference *global_ref;\r\nint r;\r\nglobal_ref = &cirrus->ttm.mem_global_ref;\r\nglobal_ref->global_type = DRM_GLOBAL_TTM_MEM;\r\nglobal_ref->size = sizeof(struct ttm_mem_global);\r\nglobal_ref->init = &cirrus_ttm_mem_global_init;\r\nglobal_ref->release = &cirrus_ttm_mem_global_release;\r\nr = drm_global_item_ref(global_ref);\r\nif (r != 0) {\r\nDRM_ERROR("Failed setting up TTM memory accounting "\r\n"subsystem.\n");\r\nreturn r;\r\n}\r\ncirrus->ttm.bo_global_ref.mem_glob =\r\ncirrus->ttm.mem_global_ref.object;\r\nglobal_ref = &cirrus->ttm.bo_global_ref.ref;\r\nglobal_ref->global_type = DRM_GLOBAL_TTM_BO;\r\nglobal_ref->size = sizeof(struct ttm_bo_global);\r\nglobal_ref->init = &ttm_bo_global_init;\r\nglobal_ref->release = &ttm_bo_global_release;\r\nr = drm_global_item_ref(global_ref);\r\nif (r != 0) {\r\nDRM_ERROR("Failed setting up TTM BO subsystem.\n");\r\ndrm_global_item_unref(&cirrus->ttm.mem_global_ref);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\ncirrus_ttm_global_release(struct cirrus_device *cirrus)\r\n{\r\nif (cirrus->ttm.mem_global_ref.release == NULL)\r\nreturn;\r\ndrm_global_item_unref(&cirrus->ttm.bo_global_ref.ref);\r\ndrm_global_item_unref(&cirrus->ttm.mem_global_ref);\r\ncirrus->ttm.mem_global_ref.release = NULL;\r\n}\r\nstatic void cirrus_bo_ttm_destroy(struct ttm_buffer_object *tbo)\r\n{\r\nstruct cirrus_bo *bo;\r\nbo = container_of(tbo, struct cirrus_bo, bo);\r\ndrm_gem_object_release(&bo->gem);\r\nkfree(bo);\r\n}\r\nstatic bool cirrus_ttm_bo_is_cirrus_bo(struct ttm_buffer_object *bo)\r\n{\r\nif (bo->destroy == &cirrus_bo_ttm_destroy)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic int\r\ncirrus_bo_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,\r\nstruct ttm_mem_type_manager *man)\r\n{\r\nswitch (type) {\r\ncase TTM_PL_SYSTEM:\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_MASK_CACHING;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ncase TTM_PL_VRAM:\r\nman->func = &ttm_bo_manager_func;\r\nman->flags = TTM_MEMTYPE_FLAG_FIXED |\r\nTTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_WC;\r\nman->default_caching = TTM_PL_FLAG_WC;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Unsupported memory type %u\n", (unsigned)type);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\ncirrus_bo_evict_flags(struct ttm_buffer_object *bo, struct ttm_placement *pl)\r\n{\r\nstruct cirrus_bo *cirrusbo = cirrus_bo(bo);\r\nif (!cirrus_ttm_bo_is_cirrus_bo(bo))\r\nreturn;\r\ncirrus_ttm_placement(cirrusbo, TTM_PL_FLAG_SYSTEM);\r\n*pl = cirrusbo->placement;\r\n}\r\nstatic int cirrus_bo_verify_access(struct ttm_buffer_object *bo, struct file *filp)\r\n{\r\nstruct cirrus_bo *cirrusbo = cirrus_bo(bo);\r\nreturn drm_vma_node_verify_access(&cirrusbo->gem.vma_node, filp);\r\n}\r\nstatic int cirrus_ttm_io_mem_reserve(struct ttm_bo_device *bdev,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nstruct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];\r\nstruct cirrus_device *cirrus = cirrus_bdev(bdev);\r\nmem->bus.addr = NULL;\r\nmem->bus.offset = 0;\r\nmem->bus.size = mem->num_pages << PAGE_SHIFT;\r\nmem->bus.base = 0;\r\nmem->bus.is_iomem = false;\r\nif (!(man->flags & TTM_MEMTYPE_FLAG_MAPPABLE))\r\nreturn -EINVAL;\r\nswitch (mem->mem_type) {\r\ncase TTM_PL_SYSTEM:\r\nreturn 0;\r\ncase TTM_PL_VRAM:\r\nmem->bus.offset = mem->start << PAGE_SHIFT;\r\nmem->bus.base = pci_resource_start(cirrus->dev->pdev, 0);\r\nmem->bus.is_iomem = true;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic void cirrus_ttm_io_mem_free(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)\r\n{\r\n}\r\nstatic int cirrus_bo_move(struct ttm_buffer_object *bo,\r\nbool evict, bool interruptible,\r\nbool no_wait_gpu,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nint r;\r\nr = ttm_bo_move_memcpy(bo, evict, no_wait_gpu, new_mem);\r\nreturn r;\r\n}\r\nstatic void cirrus_ttm_backend_destroy(struct ttm_tt *tt)\r\n{\r\nttm_tt_fini(tt);\r\nkfree(tt);\r\n}\r\nstatic struct ttm_tt *cirrus_ttm_tt_create(struct ttm_bo_device *bdev,\r\nunsigned long size, uint32_t page_flags,\r\nstruct page *dummy_read_page)\r\n{\r\nstruct ttm_tt *tt;\r\ntt = kzalloc(sizeof(struct ttm_tt), GFP_KERNEL);\r\nif (tt == NULL)\r\nreturn NULL;\r\ntt->func = &cirrus_tt_backend_func;\r\nif (ttm_tt_init(tt, bdev, size, page_flags, dummy_read_page)) {\r\nkfree(tt);\r\nreturn NULL;\r\n}\r\nreturn tt;\r\n}\r\nstatic int cirrus_ttm_tt_populate(struct ttm_tt *ttm)\r\n{\r\nreturn ttm_pool_populate(ttm);\r\n}\r\nstatic void cirrus_ttm_tt_unpopulate(struct ttm_tt *ttm)\r\n{\r\nttm_pool_unpopulate(ttm);\r\n}\r\nint cirrus_mm_init(struct cirrus_device *cirrus)\r\n{\r\nint ret;\r\nstruct drm_device *dev = cirrus->dev;\r\nstruct ttm_bo_device *bdev = &cirrus->ttm.bdev;\r\nret = cirrus_ttm_global_init(cirrus);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_bo_device_init(&cirrus->ttm.bdev,\r\ncirrus->ttm.bo_global_ref.ref.object,\r\n&cirrus_bo_driver,\r\ndev->anon_inode->i_mapping,\r\nDRM_FILE_PAGE_OFFSET,\r\ntrue);\r\nif (ret) {\r\nDRM_ERROR("Error initialising bo driver; %d\n", ret);\r\nreturn ret;\r\n}\r\nret = ttm_bo_init_mm(bdev, TTM_PL_VRAM,\r\ncirrus->mc.vram_size >> PAGE_SHIFT);\r\nif (ret) {\r\nDRM_ERROR("Failed ttm VRAM init: %d\n", ret);\r\nreturn ret;\r\n}\r\ncirrus->fb_mtrr = arch_phys_wc_add(pci_resource_start(dev->pdev, 0),\r\npci_resource_len(dev->pdev, 0));\r\ncirrus->mm_inited = true;\r\nreturn 0;\r\n}\r\nvoid cirrus_mm_fini(struct cirrus_device *cirrus)\r\n{\r\nif (!cirrus->mm_inited)\r\nreturn;\r\nttm_bo_device_release(&cirrus->ttm.bdev);\r\ncirrus_ttm_global_release(cirrus);\r\narch_phys_wc_del(cirrus->fb_mtrr);\r\ncirrus->fb_mtrr = 0;\r\n}\r\nvoid cirrus_ttm_placement(struct cirrus_bo *bo, int domain)\r\n{\r\nu32 c = 0;\r\nbo->placement.fpfn = 0;\r\nbo->placement.lpfn = 0;\r\nbo->placement.placement = bo->placements;\r\nbo->placement.busy_placement = bo->placements;\r\nif (domain & TTM_PL_FLAG_VRAM)\r\nbo->placements[c++] = TTM_PL_FLAG_WC | TTM_PL_FLAG_UNCACHED | TTM_PL_FLAG_VRAM;\r\nif (domain & TTM_PL_FLAG_SYSTEM)\r\nbo->placements[c++] = TTM_PL_MASK_CACHING | TTM_PL_FLAG_SYSTEM;\r\nif (!c)\r\nbo->placements[c++] = TTM_PL_MASK_CACHING | TTM_PL_FLAG_SYSTEM;\r\nbo->placement.num_placement = c;\r\nbo->placement.num_busy_placement = c;\r\n}\r\nint cirrus_bo_create(struct drm_device *dev, int size, int align,\r\nuint32_t flags, struct cirrus_bo **pcirrusbo)\r\n{\r\nstruct cirrus_device *cirrus = dev->dev_private;\r\nstruct cirrus_bo *cirrusbo;\r\nsize_t acc_size;\r\nint ret;\r\ncirrusbo = kzalloc(sizeof(struct cirrus_bo), GFP_KERNEL);\r\nif (!cirrusbo)\r\nreturn -ENOMEM;\r\nret = drm_gem_object_init(dev, &cirrusbo->gem, size);\r\nif (ret) {\r\nkfree(cirrusbo);\r\nreturn ret;\r\n}\r\ncirrusbo->bo.bdev = &cirrus->ttm.bdev;\r\ncirrus_ttm_placement(cirrusbo, TTM_PL_FLAG_VRAM | TTM_PL_FLAG_SYSTEM);\r\nacc_size = ttm_bo_dma_acc_size(&cirrus->ttm.bdev, size,\r\nsizeof(struct cirrus_bo));\r\nret = ttm_bo_init(&cirrus->ttm.bdev, &cirrusbo->bo, size,\r\nttm_bo_type_device, &cirrusbo->placement,\r\nalign >> PAGE_SHIFT, false, NULL, acc_size,\r\nNULL, cirrus_bo_ttm_destroy);\r\nif (ret)\r\nreturn ret;\r\n*pcirrusbo = cirrusbo;\r\nreturn 0;\r\n}\r\nstatic inline u64 cirrus_bo_gpu_offset(struct cirrus_bo *bo)\r\n{\r\nreturn bo->bo.offset;\r\n}\r\nint cirrus_bo_pin(struct cirrus_bo *bo, u32 pl_flag, u64 *gpu_addr)\r\n{\r\nint i, ret;\r\nif (bo->pin_count) {\r\nbo->pin_count++;\r\nif (gpu_addr)\r\n*gpu_addr = cirrus_bo_gpu_offset(bo);\r\n}\r\ncirrus_ttm_placement(bo, pl_flag);\r\nfor (i = 0; i < bo->placement.num_placement; i++)\r\nbo->placements[i] |= TTM_PL_FLAG_NO_EVICT;\r\nret = ttm_bo_validate(&bo->bo, &bo->placement, false, false);\r\nif (ret)\r\nreturn ret;\r\nbo->pin_count = 1;\r\nif (gpu_addr)\r\n*gpu_addr = cirrus_bo_gpu_offset(bo);\r\nreturn 0;\r\n}\r\nint cirrus_bo_push_sysram(struct cirrus_bo *bo)\r\n{\r\nint i, ret;\r\nif (!bo->pin_count) {\r\nDRM_ERROR("unpin bad %p\n", bo);\r\nreturn 0;\r\n}\r\nbo->pin_count--;\r\nif (bo->pin_count)\r\nreturn 0;\r\nif (bo->kmap.virtual)\r\nttm_bo_kunmap(&bo->kmap);\r\ncirrus_ttm_placement(bo, TTM_PL_FLAG_SYSTEM);\r\nfor (i = 0; i < bo->placement.num_placement ; i++)\r\nbo->placements[i] |= TTM_PL_FLAG_NO_EVICT;\r\nret = ttm_bo_validate(&bo->bo, &bo->placement, false, false);\r\nif (ret) {\r\nDRM_ERROR("pushing to VRAM failed\n");\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint cirrus_mmap(struct file *filp, struct vm_area_struct *vma)\r\n{\r\nstruct drm_file *file_priv;\r\nstruct cirrus_device *cirrus;\r\nif (unlikely(vma->vm_pgoff < DRM_FILE_PAGE_OFFSET))\r\nreturn drm_mmap(filp, vma);\r\nfile_priv = filp->private_data;\r\ncirrus = file_priv->minor->dev->dev_private;\r\nreturn ttm_bo_mmap(filp, vma, &cirrus->ttm.bdev);\r\n}
