int ocrdma_query_pkey(struct ib_device *ibdev, u8 port, u16 index, u16 *pkey)\r\n{\r\nif (index > 1)\r\nreturn -EINVAL;\r\n*pkey = 0xffff;\r\nreturn 0;\r\n}\r\nint ocrdma_query_gid(struct ib_device *ibdev, u8 port,\r\nint index, union ib_gid *sgid)\r\n{\r\nstruct ocrdma_dev *dev;\r\ndev = get_ocrdma_dev(ibdev);\r\nmemset(sgid, 0, sizeof(*sgid));\r\nif (index > OCRDMA_MAX_SGID)\r\nreturn -EINVAL;\r\nmemcpy(sgid, &dev->sgid_tbl[index], sizeof(*sgid));\r\nreturn 0;\r\n}\r\nint ocrdma_query_device(struct ib_device *ibdev, struct ib_device_attr *attr)\r\n{\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibdev);\r\nmemset(attr, 0, sizeof *attr);\r\nmemcpy(&attr->fw_ver, &dev->attr.fw_ver[0],\r\nmin(sizeof(dev->attr.fw_ver), sizeof(attr->fw_ver)));\r\nocrdma_get_guid(dev, (u8 *)&attr->sys_image_guid);\r\nattr->max_mr_size = ~0ull;\r\nattr->page_size_cap = 0xffff000;\r\nattr->vendor_id = dev->nic_info.pdev->vendor;\r\nattr->vendor_part_id = dev->nic_info.pdev->device;\r\nattr->hw_ver = 0;\r\nattr->max_qp = dev->attr.max_qp;\r\nattr->max_ah = OCRDMA_MAX_AH;\r\nattr->max_qp_wr = dev->attr.max_wqe;\r\nattr->device_cap_flags = IB_DEVICE_CURR_QP_STATE_MOD |\r\nIB_DEVICE_RC_RNR_NAK_GEN |\r\nIB_DEVICE_SHUTDOWN_PORT |\r\nIB_DEVICE_SYS_IMAGE_GUID |\r\nIB_DEVICE_LOCAL_DMA_LKEY |\r\nIB_DEVICE_MEM_MGT_EXTENSIONS;\r\nattr->max_sge = min(dev->attr.max_send_sge, dev->attr.max_srq_sge);\r\nattr->max_sge_rd = 0;\r\nattr->max_cq = dev->attr.max_cq;\r\nattr->max_cqe = dev->attr.max_cqe;\r\nattr->max_mr = dev->attr.max_mr;\r\nattr->max_mw = dev->attr.max_mw;\r\nattr->max_pd = dev->attr.max_pd;\r\nattr->atomic_cap = 0;\r\nattr->max_fmr = 0;\r\nattr->max_map_per_fmr = 0;\r\nattr->max_qp_rd_atom =\r\nmin(dev->attr.max_ord_per_qp, dev->attr.max_ird_per_qp);\r\nattr->max_qp_init_rd_atom = dev->attr.max_ord_per_qp;\r\nattr->max_srq = dev->attr.max_srq;\r\nattr->max_srq_sge = dev->attr.max_srq_sge;\r\nattr->max_srq_wr = dev->attr.max_rqe;\r\nattr->local_ca_ack_delay = dev->attr.local_ca_ack_delay;\r\nattr->max_fast_reg_page_list_len = 0;\r\nattr->max_pkeys = 1;\r\nreturn 0;\r\n}\r\nstatic inline void get_link_speed_and_width(struct ocrdma_dev *dev,\r\nu8 *ib_speed, u8 *ib_width)\r\n{\r\nint status;\r\nu8 speed;\r\nstatus = ocrdma_mbx_get_link_speed(dev, &speed);\r\nif (status)\r\nspeed = OCRDMA_PHYS_LINK_SPEED_ZERO;\r\nswitch (speed) {\r\ncase OCRDMA_PHYS_LINK_SPEED_1GBPS:\r\n*ib_speed = IB_SPEED_SDR;\r\n*ib_width = IB_WIDTH_1X;\r\nbreak;\r\ncase OCRDMA_PHYS_LINK_SPEED_10GBPS:\r\n*ib_speed = IB_SPEED_QDR;\r\n*ib_width = IB_WIDTH_1X;\r\nbreak;\r\ncase OCRDMA_PHYS_LINK_SPEED_20GBPS:\r\n*ib_speed = IB_SPEED_DDR;\r\n*ib_width = IB_WIDTH_4X;\r\nbreak;\r\ncase OCRDMA_PHYS_LINK_SPEED_40GBPS:\r\n*ib_speed = IB_SPEED_QDR;\r\n*ib_width = IB_WIDTH_4X;\r\nbreak;\r\ndefault:\r\n*ib_speed = IB_SPEED_SDR;\r\n*ib_width = IB_WIDTH_1X;\r\n}\r\n}\r\nint ocrdma_query_port(struct ib_device *ibdev,\r\nu8 port, struct ib_port_attr *props)\r\n{\r\nenum ib_port_state port_state;\r\nstruct ocrdma_dev *dev;\r\nstruct net_device *netdev;\r\ndev = get_ocrdma_dev(ibdev);\r\nif (port > 1) {\r\npr_err("%s(%d) invalid_port=0x%x\n", __func__,\r\ndev->id, port);\r\nreturn -EINVAL;\r\n}\r\nnetdev = dev->nic_info.netdev;\r\nif (netif_running(netdev) && netif_oper_up(netdev)) {\r\nport_state = IB_PORT_ACTIVE;\r\nprops->phys_state = 5;\r\n} else {\r\nport_state = IB_PORT_DOWN;\r\nprops->phys_state = 3;\r\n}\r\nprops->max_mtu = IB_MTU_4096;\r\nprops->active_mtu = iboe_get_mtu(netdev->mtu);\r\nprops->lid = 0;\r\nprops->lmc = 0;\r\nprops->sm_lid = 0;\r\nprops->sm_sl = 0;\r\nprops->state = port_state;\r\nprops->port_cap_flags =\r\nIB_PORT_CM_SUP |\r\nIB_PORT_REINIT_SUP |\r\nIB_PORT_DEVICE_MGMT_SUP | IB_PORT_VENDOR_CLASS_SUP | IB_PORT_IP_BASED_GIDS;\r\nprops->gid_tbl_len = OCRDMA_MAX_SGID;\r\nprops->pkey_tbl_len = 1;\r\nprops->bad_pkey_cntr = 0;\r\nprops->qkey_viol_cntr = 0;\r\nget_link_speed_and_width(dev, &props->active_speed,\r\n&props->active_width);\r\nprops->max_msg_sz = 0x80000000;\r\nprops->max_vl_num = 4;\r\nreturn 0;\r\n}\r\nint ocrdma_modify_port(struct ib_device *ibdev, u8 port, int mask,\r\nstruct ib_port_modify *props)\r\n{\r\nstruct ocrdma_dev *dev;\r\ndev = get_ocrdma_dev(ibdev);\r\nif (port > 1) {\r\npr_err("%s(%d) invalid_port=0x%x\n", __func__, dev->id, port);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ocrdma_add_mmap(struct ocrdma_ucontext *uctx, u64 phy_addr,\r\nunsigned long len)\r\n{\r\nstruct ocrdma_mm *mm;\r\nmm = kzalloc(sizeof(*mm), GFP_KERNEL);\r\nif (mm == NULL)\r\nreturn -ENOMEM;\r\nmm->key.phy_addr = phy_addr;\r\nmm->key.len = len;\r\nINIT_LIST_HEAD(&mm->entry);\r\nmutex_lock(&uctx->mm_list_lock);\r\nlist_add_tail(&mm->entry, &uctx->mm_head);\r\nmutex_unlock(&uctx->mm_list_lock);\r\nreturn 0;\r\n}\r\nstatic void ocrdma_del_mmap(struct ocrdma_ucontext *uctx, u64 phy_addr,\r\nunsigned long len)\r\n{\r\nstruct ocrdma_mm *mm, *tmp;\r\nmutex_lock(&uctx->mm_list_lock);\r\nlist_for_each_entry_safe(mm, tmp, &uctx->mm_head, entry) {\r\nif (len != mm->key.len && phy_addr != mm->key.phy_addr)\r\ncontinue;\r\nlist_del(&mm->entry);\r\nkfree(mm);\r\nbreak;\r\n}\r\nmutex_unlock(&uctx->mm_list_lock);\r\n}\r\nstatic bool ocrdma_search_mmap(struct ocrdma_ucontext *uctx, u64 phy_addr,\r\nunsigned long len)\r\n{\r\nbool found = false;\r\nstruct ocrdma_mm *mm;\r\nmutex_lock(&uctx->mm_list_lock);\r\nlist_for_each_entry(mm, &uctx->mm_head, entry) {\r\nif (len != mm->key.len && phy_addr != mm->key.phy_addr)\r\ncontinue;\r\nfound = true;\r\nbreak;\r\n}\r\nmutex_unlock(&uctx->mm_list_lock);\r\nreturn found;\r\n}\r\nstatic struct ocrdma_pd *_ocrdma_alloc_pd(struct ocrdma_dev *dev,\r\nstruct ocrdma_ucontext *uctx,\r\nstruct ib_udata *udata)\r\n{\r\nstruct ocrdma_pd *pd = NULL;\r\nint status = 0;\r\npd = kzalloc(sizeof(*pd), GFP_KERNEL);\r\nif (!pd)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (udata && uctx) {\r\npd->dpp_enabled =\r\nocrdma_get_asic_type(dev) == OCRDMA_ASIC_GEN_SKH_R;\r\npd->num_dpp_qp =\r\npd->dpp_enabled ? OCRDMA_PD_MAX_DPP_ENABLED_QP : 0;\r\n}\r\nretry:\r\nstatus = ocrdma_mbx_alloc_pd(dev, pd);\r\nif (status) {\r\nif (pd->dpp_enabled) {\r\npd->dpp_enabled = false;\r\npd->num_dpp_qp = 0;\r\ngoto retry;\r\n} else {\r\nkfree(pd);\r\nreturn ERR_PTR(status);\r\n}\r\n}\r\nreturn pd;\r\n}\r\nstatic inline int is_ucontext_pd(struct ocrdma_ucontext *uctx,\r\nstruct ocrdma_pd *pd)\r\n{\r\nreturn (uctx->cntxt_pd == pd ? true : false);\r\n}\r\nstatic int _ocrdma_dealloc_pd(struct ocrdma_dev *dev,\r\nstruct ocrdma_pd *pd)\r\n{\r\nint status = 0;\r\nstatus = ocrdma_mbx_dealloc_pd(dev, pd);\r\nkfree(pd);\r\nreturn status;\r\n}\r\nstatic int ocrdma_alloc_ucontext_pd(struct ocrdma_dev *dev,\r\nstruct ocrdma_ucontext *uctx,\r\nstruct ib_udata *udata)\r\n{\r\nint status = 0;\r\nuctx->cntxt_pd = _ocrdma_alloc_pd(dev, uctx, udata);\r\nif (IS_ERR(uctx->cntxt_pd)) {\r\nstatus = PTR_ERR(uctx->cntxt_pd);\r\nuctx->cntxt_pd = NULL;\r\ngoto err;\r\n}\r\nuctx->cntxt_pd->uctx = uctx;\r\nuctx->cntxt_pd->ibpd.device = &dev->ibdev;\r\nerr:\r\nreturn status;\r\n}\r\nstatic int ocrdma_dealloc_ucontext_pd(struct ocrdma_ucontext *uctx)\r\n{\r\nint status = 0;\r\nstruct ocrdma_pd *pd = uctx->cntxt_pd;\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(pd->ibpd.device);\r\nBUG_ON(uctx->pd_in_use);\r\nuctx->cntxt_pd = NULL;\r\nstatus = _ocrdma_dealloc_pd(dev, pd);\r\nreturn status;\r\n}\r\nstatic struct ocrdma_pd *ocrdma_get_ucontext_pd(struct ocrdma_ucontext *uctx)\r\n{\r\nstruct ocrdma_pd *pd = NULL;\r\nmutex_lock(&uctx->mm_list_lock);\r\nif (!uctx->pd_in_use) {\r\nuctx->pd_in_use = true;\r\npd = uctx->cntxt_pd;\r\n}\r\nmutex_unlock(&uctx->mm_list_lock);\r\nreturn pd;\r\n}\r\nstatic void ocrdma_release_ucontext_pd(struct ocrdma_ucontext *uctx)\r\n{\r\nmutex_lock(&uctx->mm_list_lock);\r\nuctx->pd_in_use = false;\r\nmutex_unlock(&uctx->mm_list_lock);\r\n}\r\nstruct ib_ucontext *ocrdma_alloc_ucontext(struct ib_device *ibdev,\r\nstruct ib_udata *udata)\r\n{\r\nint status;\r\nstruct ocrdma_ucontext *ctx;\r\nstruct ocrdma_alloc_ucontext_resp resp;\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibdev);\r\nstruct pci_dev *pdev = dev->nic_info.pdev;\r\nu32 map_len = roundup(sizeof(u32) * 2048, PAGE_SIZE);\r\nif (!udata)\r\nreturn ERR_PTR(-EFAULT);\r\nctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\r\nif (!ctx)\r\nreturn ERR_PTR(-ENOMEM);\r\nINIT_LIST_HEAD(&ctx->mm_head);\r\nmutex_init(&ctx->mm_list_lock);\r\nctx->ah_tbl.va = dma_alloc_coherent(&pdev->dev, map_len,\r\n&ctx->ah_tbl.pa, GFP_KERNEL);\r\nif (!ctx->ah_tbl.va) {\r\nkfree(ctx);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nmemset(ctx->ah_tbl.va, 0, map_len);\r\nctx->ah_tbl.len = map_len;\r\nmemset(&resp, 0, sizeof(resp));\r\nresp.ah_tbl_len = ctx->ah_tbl.len;\r\nresp.ah_tbl_page = ctx->ah_tbl.pa;\r\nstatus = ocrdma_add_mmap(ctx, resp.ah_tbl_page, resp.ah_tbl_len);\r\nif (status)\r\ngoto map_err;\r\nstatus = ocrdma_alloc_ucontext_pd(dev, ctx, udata);\r\nif (status)\r\ngoto pd_err;\r\nresp.dev_id = dev->id;\r\nresp.max_inline_data = dev->attr.max_inline_data;\r\nresp.wqe_size = dev->attr.wqe_size;\r\nresp.rqe_size = dev->attr.rqe_size;\r\nresp.dpp_wqe_size = dev->attr.wqe_size;\r\nmemcpy(resp.fw_ver, dev->attr.fw_ver, sizeof(resp.fw_ver));\r\nstatus = ib_copy_to_udata(udata, &resp, sizeof(resp));\r\nif (status)\r\ngoto cpy_err;\r\nreturn &ctx->ibucontext;\r\ncpy_err:\r\npd_err:\r\nocrdma_del_mmap(ctx, ctx->ah_tbl.pa, ctx->ah_tbl.len);\r\nmap_err:\r\ndma_free_coherent(&pdev->dev, ctx->ah_tbl.len, ctx->ah_tbl.va,\r\nctx->ah_tbl.pa);\r\nkfree(ctx);\r\nreturn ERR_PTR(status);\r\n}\r\nint ocrdma_dealloc_ucontext(struct ib_ucontext *ibctx)\r\n{\r\nint status = 0;\r\nstruct ocrdma_mm *mm, *tmp;\r\nstruct ocrdma_ucontext *uctx = get_ocrdma_ucontext(ibctx);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibctx->device);\r\nstruct pci_dev *pdev = dev->nic_info.pdev;\r\nstatus = ocrdma_dealloc_ucontext_pd(uctx);\r\nocrdma_del_mmap(uctx, uctx->ah_tbl.pa, uctx->ah_tbl.len);\r\ndma_free_coherent(&pdev->dev, uctx->ah_tbl.len, uctx->ah_tbl.va,\r\nuctx->ah_tbl.pa);\r\nlist_for_each_entry_safe(mm, tmp, &uctx->mm_head, entry) {\r\nlist_del(&mm->entry);\r\nkfree(mm);\r\n}\r\nkfree(uctx);\r\nreturn status;\r\n}\r\nint ocrdma_mmap(struct ib_ucontext *context, struct vm_area_struct *vma)\r\n{\r\nstruct ocrdma_ucontext *ucontext = get_ocrdma_ucontext(context);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(context->device);\r\nunsigned long vm_page = vma->vm_pgoff << PAGE_SHIFT;\r\nu64 unmapped_db = (u64) dev->nic_info.unmapped_db;\r\nunsigned long len = (vma->vm_end - vma->vm_start);\r\nint status = 0;\r\nbool found;\r\nif (vma->vm_start & (PAGE_SIZE - 1))\r\nreturn -EINVAL;\r\nfound = ocrdma_search_mmap(ucontext, vma->vm_pgoff << PAGE_SHIFT, len);\r\nif (!found)\r\nreturn -EINVAL;\r\nif ((vm_page >= unmapped_db) && (vm_page <= (unmapped_db +\r\ndev->nic_info.db_total_size)) &&\r\n(len <= dev->nic_info.db_page_size)) {\r\nif (vma->vm_flags & VM_READ)\r\nreturn -EPERM;\r\nvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\r\nstatus = io_remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,\r\nlen, vma->vm_page_prot);\r\n} else if (dev->nic_info.dpp_unmapped_len &&\r\n(vm_page >= (u64) dev->nic_info.dpp_unmapped_addr) &&\r\n(vm_page <= (u64) (dev->nic_info.dpp_unmapped_addr +\r\ndev->nic_info.dpp_unmapped_len)) &&\r\n(len <= dev->nic_info.dpp_unmapped_len)) {\r\nif (vma->vm_flags & VM_READ)\r\nreturn -EPERM;\r\nvma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);\r\nstatus = io_remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,\r\nlen, vma->vm_page_prot);\r\n} else {\r\nstatus = remap_pfn_range(vma, vma->vm_start,\r\nvma->vm_pgoff, len, vma->vm_page_prot);\r\n}\r\nreturn status;\r\n}\r\nstatic int ocrdma_copy_pd_uresp(struct ocrdma_dev *dev, struct ocrdma_pd *pd,\r\nstruct ib_ucontext *ib_ctx,\r\nstruct ib_udata *udata)\r\n{\r\nint status;\r\nu64 db_page_addr;\r\nu64 dpp_page_addr = 0;\r\nu32 db_page_size;\r\nstruct ocrdma_alloc_pd_uresp rsp;\r\nstruct ocrdma_ucontext *uctx = get_ocrdma_ucontext(ib_ctx);\r\nmemset(&rsp, 0, sizeof(rsp));\r\nrsp.id = pd->id;\r\nrsp.dpp_enabled = pd->dpp_enabled;\r\ndb_page_addr = ocrdma_get_db_addr(dev, pd->id);\r\ndb_page_size = dev->nic_info.db_page_size;\r\nstatus = ocrdma_add_mmap(uctx, db_page_addr, db_page_size);\r\nif (status)\r\nreturn status;\r\nif (pd->dpp_enabled) {\r\ndpp_page_addr = dev->nic_info.dpp_unmapped_addr +\r\n(pd->id * PAGE_SIZE);\r\nstatus = ocrdma_add_mmap(uctx, dpp_page_addr,\r\nPAGE_SIZE);\r\nif (status)\r\ngoto dpp_map_err;\r\nrsp.dpp_page_addr_hi = upper_32_bits(dpp_page_addr);\r\nrsp.dpp_page_addr_lo = dpp_page_addr;\r\n}\r\nstatus = ib_copy_to_udata(udata, &rsp, sizeof(rsp));\r\nif (status)\r\ngoto ucopy_err;\r\npd->uctx = uctx;\r\nreturn 0;\r\nucopy_err:\r\nif (pd->dpp_enabled)\r\nocrdma_del_mmap(pd->uctx, dpp_page_addr, PAGE_SIZE);\r\ndpp_map_err:\r\nocrdma_del_mmap(pd->uctx, db_page_addr, db_page_size);\r\nreturn status;\r\n}\r\nstruct ib_pd *ocrdma_alloc_pd(struct ib_device *ibdev,\r\nstruct ib_ucontext *context,\r\nstruct ib_udata *udata)\r\n{\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibdev);\r\nstruct ocrdma_pd *pd;\r\nstruct ocrdma_ucontext *uctx = NULL;\r\nint status;\r\nu8 is_uctx_pd = false;\r\nif (udata && context) {\r\nuctx = get_ocrdma_ucontext(context);\r\npd = ocrdma_get_ucontext_pd(uctx);\r\nif (pd) {\r\nis_uctx_pd = true;\r\ngoto pd_mapping;\r\n}\r\n}\r\npd = _ocrdma_alloc_pd(dev, uctx, udata);\r\nif (IS_ERR(pd)) {\r\nstatus = PTR_ERR(pd);\r\ngoto exit;\r\n}\r\npd_mapping:\r\nif (udata && context) {\r\nstatus = ocrdma_copy_pd_uresp(dev, pd, context, udata);\r\nif (status)\r\ngoto err;\r\n}\r\nreturn &pd->ibpd;\r\nerr:\r\nif (is_uctx_pd) {\r\nocrdma_release_ucontext_pd(uctx);\r\n} else {\r\nstatus = ocrdma_mbx_dealloc_pd(dev, pd);\r\nkfree(pd);\r\n}\r\nexit:\r\nreturn ERR_PTR(status);\r\n}\r\nint ocrdma_dealloc_pd(struct ib_pd *ibpd)\r\n{\r\nstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\r\nstruct ocrdma_ucontext *uctx = NULL;\r\nint status = 0;\r\nu64 usr_db;\r\nuctx = pd->uctx;\r\nif (uctx) {\r\nu64 dpp_db = dev->nic_info.dpp_unmapped_addr +\r\n(pd->id * PAGE_SIZE);\r\nif (pd->dpp_enabled)\r\nocrdma_del_mmap(pd->uctx, dpp_db, PAGE_SIZE);\r\nusr_db = ocrdma_get_db_addr(dev, pd->id);\r\nocrdma_del_mmap(pd->uctx, usr_db, dev->nic_info.db_page_size);\r\nif (is_ucontext_pd(uctx, pd)) {\r\nocrdma_release_ucontext_pd(uctx);\r\nreturn status;\r\n}\r\n}\r\nstatus = _ocrdma_dealloc_pd(dev, pd);\r\nreturn status;\r\n}\r\nstatic int ocrdma_alloc_lkey(struct ocrdma_dev *dev, struct ocrdma_mr *mr,\r\nu32 pdid, int acc, u32 num_pbls, u32 addr_check)\r\n{\r\nint status;\r\nmr->hwmr.fr_mr = 0;\r\nmr->hwmr.local_rd = 1;\r\nmr->hwmr.remote_rd = (acc & IB_ACCESS_REMOTE_READ) ? 1 : 0;\r\nmr->hwmr.remote_wr = (acc & IB_ACCESS_REMOTE_WRITE) ? 1 : 0;\r\nmr->hwmr.local_wr = (acc & IB_ACCESS_LOCAL_WRITE) ? 1 : 0;\r\nmr->hwmr.mw_bind = (acc & IB_ACCESS_MW_BIND) ? 1 : 0;\r\nmr->hwmr.remote_atomic = (acc & IB_ACCESS_REMOTE_ATOMIC) ? 1 : 0;\r\nmr->hwmr.num_pbls = num_pbls;\r\nstatus = ocrdma_mbx_alloc_lkey(dev, &mr->hwmr, pdid, addr_check);\r\nif (status)\r\nreturn status;\r\nmr->ibmr.lkey = mr->hwmr.lkey;\r\nif (mr->hwmr.remote_wr || mr->hwmr.remote_rd)\r\nmr->ibmr.rkey = mr->hwmr.lkey;\r\nreturn 0;\r\n}\r\nstruct ib_mr *ocrdma_get_dma_mr(struct ib_pd *ibpd, int acc)\r\n{\r\nint status;\r\nstruct ocrdma_mr *mr;\r\nstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\r\nif (acc & IB_ACCESS_REMOTE_WRITE && !(acc & IB_ACCESS_LOCAL_WRITE)) {\r\npr_err("%s err, invalid access rights\n", __func__);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nmr = kzalloc(sizeof(*mr), GFP_KERNEL);\r\nif (!mr)\r\nreturn ERR_PTR(-ENOMEM);\r\nstatus = ocrdma_alloc_lkey(dev, mr, pd->id, acc, 0,\r\nOCRDMA_ADDR_CHECK_DISABLE);\r\nif (status) {\r\nkfree(mr);\r\nreturn ERR_PTR(status);\r\n}\r\nreturn &mr->ibmr;\r\n}\r\nstatic void ocrdma_free_mr_pbl_tbl(struct ocrdma_dev *dev,\r\nstruct ocrdma_hw_mr *mr)\r\n{\r\nstruct pci_dev *pdev = dev->nic_info.pdev;\r\nint i = 0;\r\nif (mr->pbl_table) {\r\nfor (i = 0; i < mr->num_pbls; i++) {\r\nif (!mr->pbl_table[i].va)\r\ncontinue;\r\ndma_free_coherent(&pdev->dev, mr->pbl_size,\r\nmr->pbl_table[i].va,\r\nmr->pbl_table[i].pa);\r\n}\r\nkfree(mr->pbl_table);\r\nmr->pbl_table = NULL;\r\n}\r\n}\r\nstatic int ocrdma_get_pbl_info(struct ocrdma_dev *dev, struct ocrdma_mr *mr,\r\nu32 num_pbes)\r\n{\r\nu32 num_pbls = 0;\r\nu32 idx = 0;\r\nint status = 0;\r\nu32 pbl_size;\r\ndo {\r\npbl_size = OCRDMA_MIN_HPAGE_SIZE * (1 << idx);\r\nif (pbl_size > MAX_OCRDMA_PBL_SIZE) {\r\nstatus = -EFAULT;\r\nbreak;\r\n}\r\nnum_pbls = roundup(num_pbes, (pbl_size / sizeof(u64)));\r\nnum_pbls = num_pbls / (pbl_size / sizeof(u64));\r\nidx++;\r\n} while (num_pbls >= dev->attr.max_num_mr_pbl);\r\nmr->hwmr.num_pbes = num_pbes;\r\nmr->hwmr.num_pbls = num_pbls;\r\nmr->hwmr.pbl_size = pbl_size;\r\nreturn status;\r\n}\r\nstatic int ocrdma_build_pbl_tbl(struct ocrdma_dev *dev, struct ocrdma_hw_mr *mr)\r\n{\r\nint status = 0;\r\nint i;\r\nu32 dma_len = mr->pbl_size;\r\nstruct pci_dev *pdev = dev->nic_info.pdev;\r\nvoid *va;\r\ndma_addr_t pa;\r\nmr->pbl_table = kzalloc(sizeof(struct ocrdma_pbl) *\r\nmr->num_pbls, GFP_KERNEL);\r\nif (!mr->pbl_table)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < mr->num_pbls; i++) {\r\nva = dma_alloc_coherent(&pdev->dev, dma_len, &pa, GFP_KERNEL);\r\nif (!va) {\r\nocrdma_free_mr_pbl_tbl(dev, mr);\r\nstatus = -ENOMEM;\r\nbreak;\r\n}\r\nmemset(va, 0, dma_len);\r\nmr->pbl_table[i].va = va;\r\nmr->pbl_table[i].pa = pa;\r\n}\r\nreturn status;\r\n}\r\nstatic void build_user_pbes(struct ocrdma_dev *dev, struct ocrdma_mr *mr,\r\nu32 num_pbes)\r\n{\r\nstruct ocrdma_pbe *pbe;\r\nstruct scatterlist *sg;\r\nstruct ocrdma_pbl *pbl_tbl = mr->hwmr.pbl_table;\r\nstruct ib_umem *umem = mr->umem;\r\nint shift, pg_cnt, pages, pbe_cnt, entry, total_num_pbes = 0;\r\nif (!mr->hwmr.num_pbes)\r\nreturn;\r\npbe = (struct ocrdma_pbe *)pbl_tbl->va;\r\npbe_cnt = 0;\r\nshift = ilog2(umem->page_size);\r\nfor_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {\r\npages = sg_dma_len(sg) >> shift;\r\nfor (pg_cnt = 0; pg_cnt < pages; pg_cnt++) {\r\npbe->pa_lo =\r\ncpu_to_le32(sg_dma_address\r\n(sg) +\r\n(umem->page_size * pg_cnt));\r\npbe->pa_hi =\r\ncpu_to_le32(upper_32_bits\r\n((sg_dma_address\r\n(sg) +\r\numem->page_size * pg_cnt)));\r\npbe_cnt += 1;\r\ntotal_num_pbes += 1;\r\npbe++;\r\nif (total_num_pbes == num_pbes)\r\nreturn;\r\nif (pbe_cnt ==\r\n(mr->hwmr.pbl_size / sizeof(u64))) {\r\npbl_tbl++;\r\npbe = (struct ocrdma_pbe *)pbl_tbl->va;\r\npbe_cnt = 0;\r\n}\r\n}\r\n}\r\n}\r\nstruct ib_mr *ocrdma_reg_user_mr(struct ib_pd *ibpd, u64 start, u64 len,\r\nu64 usr_addr, int acc, struct ib_udata *udata)\r\n{\r\nint status = -ENOMEM;\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\r\nstruct ocrdma_mr *mr;\r\nstruct ocrdma_pd *pd;\r\nu32 num_pbes;\r\npd = get_ocrdma_pd(ibpd);\r\nif (acc & IB_ACCESS_REMOTE_WRITE && !(acc & IB_ACCESS_LOCAL_WRITE))\r\nreturn ERR_PTR(-EINVAL);\r\nmr = kzalloc(sizeof(*mr), GFP_KERNEL);\r\nif (!mr)\r\nreturn ERR_PTR(status);\r\nmr->umem = ib_umem_get(ibpd->uobject->context, start, len, acc, 0);\r\nif (IS_ERR(mr->umem)) {\r\nstatus = -EFAULT;\r\ngoto umem_err;\r\n}\r\nnum_pbes = ib_umem_page_count(mr->umem);\r\nstatus = ocrdma_get_pbl_info(dev, mr, num_pbes);\r\nif (status)\r\ngoto umem_err;\r\nmr->hwmr.pbe_size = mr->umem->page_size;\r\nmr->hwmr.fbo = mr->umem->offset;\r\nmr->hwmr.va = usr_addr;\r\nmr->hwmr.len = len;\r\nmr->hwmr.remote_wr = (acc & IB_ACCESS_REMOTE_WRITE) ? 1 : 0;\r\nmr->hwmr.remote_rd = (acc & IB_ACCESS_REMOTE_READ) ? 1 : 0;\r\nmr->hwmr.local_wr = (acc & IB_ACCESS_LOCAL_WRITE) ? 1 : 0;\r\nmr->hwmr.local_rd = 1;\r\nmr->hwmr.remote_atomic = (acc & IB_ACCESS_REMOTE_ATOMIC) ? 1 : 0;\r\nstatus = ocrdma_build_pbl_tbl(dev, &mr->hwmr);\r\nif (status)\r\ngoto umem_err;\r\nbuild_user_pbes(dev, mr, num_pbes);\r\nstatus = ocrdma_reg_mr(dev, &mr->hwmr, pd->id, acc);\r\nif (status)\r\ngoto mbx_err;\r\nmr->ibmr.lkey = mr->hwmr.lkey;\r\nif (mr->hwmr.remote_wr || mr->hwmr.remote_rd)\r\nmr->ibmr.rkey = mr->hwmr.lkey;\r\nreturn &mr->ibmr;\r\nmbx_err:\r\nocrdma_free_mr_pbl_tbl(dev, &mr->hwmr);\r\numem_err:\r\nkfree(mr);\r\nreturn ERR_PTR(status);\r\n}\r\nint ocrdma_dereg_mr(struct ib_mr *ib_mr)\r\n{\r\nstruct ocrdma_mr *mr = get_ocrdma_mr(ib_mr);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ib_mr->device);\r\nint status;\r\nstatus = ocrdma_mbx_dealloc_lkey(dev, mr->hwmr.fr_mr, mr->hwmr.lkey);\r\nocrdma_free_mr_pbl_tbl(dev, &mr->hwmr);\r\nif (mr->umem)\r\nib_umem_release(mr->umem);\r\nkfree(mr);\r\nreturn status;\r\n}\r\nstatic int ocrdma_copy_cq_uresp(struct ocrdma_dev *dev, struct ocrdma_cq *cq,\r\nstruct ib_udata *udata,\r\nstruct ib_ucontext *ib_ctx)\r\n{\r\nint status;\r\nstruct ocrdma_ucontext *uctx = get_ocrdma_ucontext(ib_ctx);\r\nstruct ocrdma_create_cq_uresp uresp;\r\nmemset(&uresp, 0, sizeof(uresp));\r\nuresp.cq_id = cq->id;\r\nuresp.page_size = PAGE_ALIGN(cq->len);\r\nuresp.num_pages = 1;\r\nuresp.max_hw_cqe = cq->max_hw_cqe;\r\nuresp.page_addr[0] = cq->pa;\r\nuresp.db_page_addr = ocrdma_get_db_addr(dev, uctx->cntxt_pd->id);\r\nuresp.db_page_size = dev->nic_info.db_page_size;\r\nuresp.phase_change = cq->phase_change ? 1 : 0;\r\nstatus = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\r\nif (status) {\r\npr_err("%s(%d) copy error cqid=0x%x.\n",\r\n__func__, dev->id, cq->id);\r\ngoto err;\r\n}\r\nstatus = ocrdma_add_mmap(uctx, uresp.db_page_addr, uresp.db_page_size);\r\nif (status)\r\ngoto err;\r\nstatus = ocrdma_add_mmap(uctx, uresp.page_addr[0], uresp.page_size);\r\nif (status) {\r\nocrdma_del_mmap(uctx, uresp.db_page_addr, uresp.db_page_size);\r\ngoto err;\r\n}\r\ncq->ucontext = uctx;\r\nerr:\r\nreturn status;\r\n}\r\nstruct ib_cq *ocrdma_create_cq(struct ib_device *ibdev, int entries, int vector,\r\nstruct ib_ucontext *ib_ctx,\r\nstruct ib_udata *udata)\r\n{\r\nstruct ocrdma_cq *cq;\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibdev);\r\nstruct ocrdma_ucontext *uctx = NULL;\r\nu16 pd_id = 0;\r\nint status;\r\nstruct ocrdma_create_cq_ureq ureq;\r\nif (udata) {\r\nif (ib_copy_from_udata(&ureq, udata, sizeof(ureq)))\r\nreturn ERR_PTR(-EFAULT);\r\n} else\r\nureq.dpp_cq = 0;\r\ncq = kzalloc(sizeof(*cq), GFP_KERNEL);\r\nif (!cq)\r\nreturn ERR_PTR(-ENOMEM);\r\nspin_lock_init(&cq->cq_lock);\r\nspin_lock_init(&cq->comp_handler_lock);\r\nINIT_LIST_HEAD(&cq->sq_head);\r\nINIT_LIST_HEAD(&cq->rq_head);\r\ncq->first_arm = true;\r\nif (ib_ctx) {\r\nuctx = get_ocrdma_ucontext(ib_ctx);\r\npd_id = uctx->cntxt_pd->id;\r\n}\r\nstatus = ocrdma_mbx_create_cq(dev, cq, entries, ureq.dpp_cq, pd_id);\r\nif (status) {\r\nkfree(cq);\r\nreturn ERR_PTR(status);\r\n}\r\nif (ib_ctx) {\r\nstatus = ocrdma_copy_cq_uresp(dev, cq, udata, ib_ctx);\r\nif (status)\r\ngoto ctx_err;\r\n}\r\ncq->phase = OCRDMA_CQE_VALID;\r\ndev->cq_tbl[cq->id] = cq;\r\nreturn &cq->ibcq;\r\nctx_err:\r\nocrdma_mbx_destroy_cq(dev, cq);\r\nkfree(cq);\r\nreturn ERR_PTR(status);\r\n}\r\nint ocrdma_resize_cq(struct ib_cq *ibcq, int new_cnt,\r\nstruct ib_udata *udata)\r\n{\r\nint status = 0;\r\nstruct ocrdma_cq *cq = get_ocrdma_cq(ibcq);\r\nif (new_cnt < 1 || new_cnt > cq->max_hw_cqe) {\r\nstatus = -EINVAL;\r\nreturn status;\r\n}\r\nibcq->cqe = new_cnt;\r\nreturn status;\r\n}\r\nstatic void ocrdma_flush_cq(struct ocrdma_cq *cq)\r\n{\r\nint cqe_cnt;\r\nint valid_count = 0;\r\nunsigned long flags;\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(cq->ibcq.device);\r\nstruct ocrdma_cqe *cqe = NULL;\r\ncqe = cq->va;\r\ncqe_cnt = cq->cqe_cnt;\r\nspin_lock_irqsave(&cq->cq_lock, flags);\r\nwhile (cqe_cnt) {\r\nif (is_cqe_valid(cq, cqe))\r\nvalid_count++;\r\ncqe++;\r\ncqe_cnt--;\r\n}\r\nocrdma_ring_cq_db(dev, cq->id, false, false, valid_count);\r\nspin_unlock_irqrestore(&cq->cq_lock, flags);\r\n}\r\nint ocrdma_destroy_cq(struct ib_cq *ibcq)\r\n{\r\nint status;\r\nstruct ocrdma_cq *cq = get_ocrdma_cq(ibcq);\r\nstruct ocrdma_eq *eq = NULL;\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibcq->device);\r\nint pdid = 0;\r\nu32 irq, indx;\r\ndev->cq_tbl[cq->id] = NULL;\r\nindx = ocrdma_get_eq_table_index(dev, cq->eqn);\r\nif (indx == -EINVAL)\r\nBUG();\r\neq = &dev->eq_tbl[indx];\r\nirq = ocrdma_get_irq(dev, eq);\r\nsynchronize_irq(irq);\r\nocrdma_flush_cq(cq);\r\nstatus = ocrdma_mbx_destroy_cq(dev, cq);\r\nif (cq->ucontext) {\r\npdid = cq->ucontext->cntxt_pd->id;\r\nocrdma_del_mmap(cq->ucontext, (u64) cq->pa,\r\nPAGE_ALIGN(cq->len));\r\nocrdma_del_mmap(cq->ucontext,\r\nocrdma_get_db_addr(dev, pdid),\r\ndev->nic_info.db_page_size);\r\n}\r\nkfree(cq);\r\nreturn status;\r\n}\r\nstatic int ocrdma_add_qpn_map(struct ocrdma_dev *dev, struct ocrdma_qp *qp)\r\n{\r\nint status = -EINVAL;\r\nif (qp->id < OCRDMA_MAX_QP && dev->qp_tbl[qp->id] == NULL) {\r\ndev->qp_tbl[qp->id] = qp;\r\nstatus = 0;\r\n}\r\nreturn status;\r\n}\r\nstatic void ocrdma_del_qpn_map(struct ocrdma_dev *dev, struct ocrdma_qp *qp)\r\n{\r\ndev->qp_tbl[qp->id] = NULL;\r\n}\r\nstatic int ocrdma_check_qp_params(struct ib_pd *ibpd, struct ocrdma_dev *dev,\r\nstruct ib_qp_init_attr *attrs)\r\n{\r\nif ((attrs->qp_type != IB_QPT_GSI) &&\r\n(attrs->qp_type != IB_QPT_RC) &&\r\n(attrs->qp_type != IB_QPT_UC) &&\r\n(attrs->qp_type != IB_QPT_UD)) {\r\npr_err("%s(%d) unsupported qp type=0x%x requested\n",\r\n__func__, dev->id, attrs->qp_type);\r\nreturn -EINVAL;\r\n}\r\nif ((attrs->qp_type != IB_QPT_GSI) &&\r\n(attrs->cap.max_send_wr > dev->attr.max_wqe)) {\r\npr_err("%s(%d) unsupported send_wr=0x%x requested\n",\r\n__func__, dev->id, attrs->cap.max_send_wr);\r\npr_err("%s(%d) supported send_wr=0x%x\n",\r\n__func__, dev->id, dev->attr.max_wqe);\r\nreturn -EINVAL;\r\n}\r\nif (!attrs->srq && (attrs->cap.max_recv_wr > dev->attr.max_rqe)) {\r\npr_err("%s(%d) unsupported recv_wr=0x%x requested\n",\r\n__func__, dev->id, attrs->cap.max_recv_wr);\r\npr_err("%s(%d) supported recv_wr=0x%x\n",\r\n__func__, dev->id, dev->attr.max_rqe);\r\nreturn -EINVAL;\r\n}\r\nif (attrs->cap.max_inline_data > dev->attr.max_inline_data) {\r\npr_err("%s(%d) unsupported inline data size=0x%x requested\n",\r\n__func__, dev->id, attrs->cap.max_inline_data);\r\npr_err("%s(%d) supported inline data size=0x%x\n",\r\n__func__, dev->id, dev->attr.max_inline_data);\r\nreturn -EINVAL;\r\n}\r\nif (attrs->cap.max_send_sge > dev->attr.max_send_sge) {\r\npr_err("%s(%d) unsupported send_sge=0x%x requested\n",\r\n__func__, dev->id, attrs->cap.max_send_sge);\r\npr_err("%s(%d) supported send_sge=0x%x\n",\r\n__func__, dev->id, dev->attr.max_send_sge);\r\nreturn -EINVAL;\r\n}\r\nif (attrs->cap.max_recv_sge > dev->attr.max_recv_sge) {\r\npr_err("%s(%d) unsupported recv_sge=0x%x requested\n",\r\n__func__, dev->id, attrs->cap.max_recv_sge);\r\npr_err("%s(%d) supported recv_sge=0x%x\n",\r\n__func__, dev->id, dev->attr.max_recv_sge);\r\nreturn -EINVAL;\r\n}\r\nif (ibpd->uobject && attrs->qp_type == IB_QPT_GSI) {\r\npr_err\r\n("%s(%d) Userspace can't create special QPs of type=0x%x\n",\r\n__func__, dev->id, attrs->qp_type);\r\nreturn -EINVAL;\r\n}\r\nif (attrs->qp_type == IB_QPT_GSI && dev->gsi_qp_created) {\r\npr_err("%s(%d) GSI special QPs already created.\n",\r\n__func__, dev->id);\r\nreturn -EINVAL;\r\n}\r\nif ((attrs->qp_type != IB_QPT_GSI) && (dev->gsi_qp_created)) {\r\nif ((dev->gsi_sqcq == get_ocrdma_cq(attrs->send_cq)) ||\r\n(dev->gsi_rqcq == get_ocrdma_cq(attrs->recv_cq))) {\r\npr_err("%s(%d) Consumer QP cannot use GSI CQs.\n",\r\n__func__, dev->id);\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int ocrdma_copy_qp_uresp(struct ocrdma_qp *qp,\r\nstruct ib_udata *udata, int dpp_offset,\r\nint dpp_credit_lmt, int srq)\r\n{\r\nint status = 0;\r\nu64 usr_db;\r\nstruct ocrdma_create_qp_uresp uresp;\r\nstruct ocrdma_dev *dev = qp->dev;\r\nstruct ocrdma_pd *pd = qp->pd;\r\nmemset(&uresp, 0, sizeof(uresp));\r\nusr_db = dev->nic_info.unmapped_db +\r\n(pd->id * dev->nic_info.db_page_size);\r\nuresp.qp_id = qp->id;\r\nuresp.sq_dbid = qp->sq.dbid;\r\nuresp.num_sq_pages = 1;\r\nuresp.sq_page_size = PAGE_ALIGN(qp->sq.len);\r\nuresp.sq_page_addr[0] = qp->sq.pa;\r\nuresp.num_wqe_allocated = qp->sq.max_cnt;\r\nif (!srq) {\r\nuresp.rq_dbid = qp->rq.dbid;\r\nuresp.num_rq_pages = 1;\r\nuresp.rq_page_size = PAGE_ALIGN(qp->rq.len);\r\nuresp.rq_page_addr[0] = qp->rq.pa;\r\nuresp.num_rqe_allocated = qp->rq.max_cnt;\r\n}\r\nuresp.db_page_addr = usr_db;\r\nuresp.db_page_size = dev->nic_info.db_page_size;\r\nuresp.db_sq_offset = OCRDMA_DB_GEN2_SQ_OFFSET;\r\nuresp.db_rq_offset = OCRDMA_DB_GEN2_RQ_OFFSET;\r\nuresp.db_shift = OCRDMA_DB_RQ_SHIFT;\r\nif (qp->dpp_enabled) {\r\nuresp.dpp_credit = dpp_credit_lmt;\r\nuresp.dpp_offset = dpp_offset;\r\n}\r\nstatus = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\r\nif (status) {\r\npr_err("%s(%d) user copy error.\n", __func__, dev->id);\r\ngoto err;\r\n}\r\nstatus = ocrdma_add_mmap(pd->uctx, uresp.sq_page_addr[0],\r\nuresp.sq_page_size);\r\nif (status)\r\ngoto err;\r\nif (!srq) {\r\nstatus = ocrdma_add_mmap(pd->uctx, uresp.rq_page_addr[0],\r\nuresp.rq_page_size);\r\nif (status)\r\ngoto rq_map_err;\r\n}\r\nreturn status;\r\nrq_map_err:\r\nocrdma_del_mmap(pd->uctx, uresp.sq_page_addr[0], uresp.sq_page_size);\r\nerr:\r\nreturn status;\r\n}\r\nstatic void ocrdma_set_qp_db(struct ocrdma_dev *dev, struct ocrdma_qp *qp,\r\nstruct ocrdma_pd *pd)\r\n{\r\nif (ocrdma_get_asic_type(dev) == OCRDMA_ASIC_GEN_SKH_R) {\r\nqp->sq_db = dev->nic_info.db +\r\n(pd->id * dev->nic_info.db_page_size) +\r\nOCRDMA_DB_GEN2_SQ_OFFSET;\r\nqp->rq_db = dev->nic_info.db +\r\n(pd->id * dev->nic_info.db_page_size) +\r\nOCRDMA_DB_GEN2_RQ_OFFSET;\r\n} else {\r\nqp->sq_db = dev->nic_info.db +\r\n(pd->id * dev->nic_info.db_page_size) +\r\nOCRDMA_DB_SQ_OFFSET;\r\nqp->rq_db = dev->nic_info.db +\r\n(pd->id * dev->nic_info.db_page_size) +\r\nOCRDMA_DB_RQ_OFFSET;\r\n}\r\n}\r\nstatic int ocrdma_alloc_wr_id_tbl(struct ocrdma_qp *qp)\r\n{\r\nqp->wqe_wr_id_tbl =\r\nkzalloc(sizeof(*(qp->wqe_wr_id_tbl)) * qp->sq.max_cnt,\r\nGFP_KERNEL);\r\nif (qp->wqe_wr_id_tbl == NULL)\r\nreturn -ENOMEM;\r\nqp->rqe_wr_id_tbl =\r\nkzalloc(sizeof(u64) * qp->rq.max_cnt, GFP_KERNEL);\r\nif (qp->rqe_wr_id_tbl == NULL)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void ocrdma_set_qp_init_params(struct ocrdma_qp *qp,\r\nstruct ocrdma_pd *pd,\r\nstruct ib_qp_init_attr *attrs)\r\n{\r\nqp->pd = pd;\r\nspin_lock_init(&qp->q_lock);\r\nINIT_LIST_HEAD(&qp->sq_entry);\r\nINIT_LIST_HEAD(&qp->rq_entry);\r\nqp->qp_type = attrs->qp_type;\r\nqp->cap_flags = OCRDMA_QP_INB_RD | OCRDMA_QP_INB_WR;\r\nqp->max_inline_data = attrs->cap.max_inline_data;\r\nqp->sq.max_sges = attrs->cap.max_send_sge;\r\nqp->rq.max_sges = attrs->cap.max_recv_sge;\r\nqp->state = OCRDMA_QPS_RST;\r\nqp->signaled = (attrs->sq_sig_type == IB_SIGNAL_ALL_WR) ? true : false;\r\n}\r\nstatic void ocrdma_store_gsi_qp_cq(struct ocrdma_dev *dev,\r\nstruct ib_qp_init_attr *attrs)\r\n{\r\nif (attrs->qp_type == IB_QPT_GSI) {\r\ndev->gsi_qp_created = 1;\r\ndev->gsi_sqcq = get_ocrdma_cq(attrs->send_cq);\r\ndev->gsi_rqcq = get_ocrdma_cq(attrs->recv_cq);\r\n}\r\n}\r\nstruct ib_qp *ocrdma_create_qp(struct ib_pd *ibpd,\r\nstruct ib_qp_init_attr *attrs,\r\nstruct ib_udata *udata)\r\n{\r\nint status;\r\nstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\r\nstruct ocrdma_qp *qp;\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\r\nstruct ocrdma_create_qp_ureq ureq;\r\nu16 dpp_credit_lmt, dpp_offset;\r\nstatus = ocrdma_check_qp_params(ibpd, dev, attrs);\r\nif (status)\r\ngoto gen_err;\r\nmemset(&ureq, 0, sizeof(ureq));\r\nif (udata) {\r\nif (ib_copy_from_udata(&ureq, udata, sizeof(ureq)))\r\nreturn ERR_PTR(-EFAULT);\r\n}\r\nqp = kzalloc(sizeof(*qp), GFP_KERNEL);\r\nif (!qp) {\r\nstatus = -ENOMEM;\r\ngoto gen_err;\r\n}\r\nqp->dev = dev;\r\nocrdma_set_qp_init_params(qp, pd, attrs);\r\nif (udata == NULL)\r\nqp->cap_flags |= (OCRDMA_QP_MW_BIND | OCRDMA_QP_LKEY0 |\r\nOCRDMA_QP_FAST_REG);\r\nmutex_lock(&dev->dev_lock);\r\nstatus = ocrdma_mbx_create_qp(qp, attrs, ureq.enable_dpp_cq,\r\nureq.dpp_cq_id,\r\n&dpp_offset, &dpp_credit_lmt);\r\nif (status)\r\ngoto mbx_err;\r\nif (udata == NULL) {\r\nstatus = ocrdma_alloc_wr_id_tbl(qp);\r\nif (status)\r\ngoto map_err;\r\n}\r\nstatus = ocrdma_add_qpn_map(dev, qp);\r\nif (status)\r\ngoto map_err;\r\nocrdma_set_qp_db(dev, qp, pd);\r\nif (udata) {\r\nstatus = ocrdma_copy_qp_uresp(qp, udata, dpp_offset,\r\ndpp_credit_lmt,\r\n(attrs->srq != NULL));\r\nif (status)\r\ngoto cpy_err;\r\n}\r\nocrdma_store_gsi_qp_cq(dev, attrs);\r\nqp->ibqp.qp_num = qp->id;\r\nmutex_unlock(&dev->dev_lock);\r\nreturn &qp->ibqp;\r\ncpy_err:\r\nocrdma_del_qpn_map(dev, qp);\r\nmap_err:\r\nocrdma_mbx_destroy_qp(dev, qp);\r\nmbx_err:\r\nmutex_unlock(&dev->dev_lock);\r\nkfree(qp->wqe_wr_id_tbl);\r\nkfree(qp->rqe_wr_id_tbl);\r\nkfree(qp);\r\npr_err("%s(%d) error=%d\n", __func__, dev->id, status);\r\ngen_err:\r\nreturn ERR_PTR(status);\r\n}\r\nint _ocrdma_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\r\nint attr_mask)\r\n{\r\nint status = 0;\r\nstruct ocrdma_qp *qp;\r\nstruct ocrdma_dev *dev;\r\nenum ib_qp_state old_qps;\r\nqp = get_ocrdma_qp(ibqp);\r\ndev = qp->dev;\r\nif (attr_mask & IB_QP_STATE)\r\nstatus = ocrdma_qp_state_change(qp, attr->qp_state, &old_qps);\r\nif (status < 0)\r\nreturn status;\r\nstatus = ocrdma_mbx_modify_qp(dev, qp, attr, attr_mask);\r\nreturn status;\r\n}\r\nint ocrdma_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\r\nint attr_mask, struct ib_udata *udata)\r\n{\r\nunsigned long flags;\r\nint status = -EINVAL;\r\nstruct ocrdma_qp *qp;\r\nstruct ocrdma_dev *dev;\r\nenum ib_qp_state old_qps, new_qps;\r\nqp = get_ocrdma_qp(ibqp);\r\ndev = qp->dev;\r\nmutex_lock(&dev->dev_lock);\r\nspin_lock_irqsave(&qp->q_lock, flags);\r\nold_qps = get_ibqp_state(qp->state);\r\nif (attr_mask & IB_QP_STATE)\r\nnew_qps = attr->qp_state;\r\nelse\r\nnew_qps = old_qps;\r\nspin_unlock_irqrestore(&qp->q_lock, flags);\r\nif (!ib_modify_qp_is_ok(old_qps, new_qps, ibqp->qp_type, attr_mask,\r\nIB_LINK_LAYER_ETHERNET)) {\r\npr_err("%s(%d) invalid attribute mask=0x%x specified for\n"\r\n"qpn=0x%x of type=0x%x old_qps=0x%x, new_qps=0x%x\n",\r\n__func__, dev->id, attr_mask, qp->id, ibqp->qp_type,\r\nold_qps, new_qps);\r\ngoto param_err;\r\n}\r\nstatus = _ocrdma_modify_qp(ibqp, attr, attr_mask);\r\nif (status > 0)\r\nstatus = 0;\r\nparam_err:\r\nmutex_unlock(&dev->dev_lock);\r\nreturn status;\r\n}\r\nstatic enum ib_mtu ocrdma_mtu_int_to_enum(u16 mtu)\r\n{\r\nswitch (mtu) {\r\ncase 256:\r\nreturn IB_MTU_256;\r\ncase 512:\r\nreturn IB_MTU_512;\r\ncase 1024:\r\nreturn IB_MTU_1024;\r\ncase 2048:\r\nreturn IB_MTU_2048;\r\ncase 4096:\r\nreturn IB_MTU_4096;\r\ndefault:\r\nreturn IB_MTU_1024;\r\n}\r\n}\r\nstatic int ocrdma_to_ib_qp_acc_flags(int qp_cap_flags)\r\n{\r\nint ib_qp_acc_flags = 0;\r\nif (qp_cap_flags & OCRDMA_QP_INB_WR)\r\nib_qp_acc_flags |= IB_ACCESS_REMOTE_WRITE;\r\nif (qp_cap_flags & OCRDMA_QP_INB_RD)\r\nib_qp_acc_flags |= IB_ACCESS_LOCAL_WRITE;\r\nreturn ib_qp_acc_flags;\r\n}\r\nint ocrdma_query_qp(struct ib_qp *ibqp,\r\nstruct ib_qp_attr *qp_attr,\r\nint attr_mask, struct ib_qp_init_attr *qp_init_attr)\r\n{\r\nint status;\r\nu32 qp_state;\r\nstruct ocrdma_qp_params params;\r\nstruct ocrdma_qp *qp = get_ocrdma_qp(ibqp);\r\nstruct ocrdma_dev *dev = qp->dev;\r\nmemset(&params, 0, sizeof(params));\r\nmutex_lock(&dev->dev_lock);\r\nstatus = ocrdma_mbx_query_qp(dev, qp, &params);\r\nmutex_unlock(&dev->dev_lock);\r\nif (status)\r\ngoto mbx_err;\r\nqp_attr->qp_state = get_ibqp_state(IB_QPS_INIT);\r\nqp_attr->cur_qp_state = get_ibqp_state(IB_QPS_INIT);\r\nqp_attr->path_mtu =\r\nocrdma_mtu_int_to_enum(params.path_mtu_pkey_indx &\r\nOCRDMA_QP_PARAMS_PATH_MTU_MASK) >>\r\nOCRDMA_QP_PARAMS_PATH_MTU_SHIFT;\r\nqp_attr->path_mig_state = IB_MIG_MIGRATED;\r\nqp_attr->rq_psn = params.hop_lmt_rq_psn & OCRDMA_QP_PARAMS_RQ_PSN_MASK;\r\nqp_attr->sq_psn = params.tclass_sq_psn & OCRDMA_QP_PARAMS_SQ_PSN_MASK;\r\nqp_attr->dest_qp_num =\r\nparams.ack_to_rnr_rtc_dest_qpn & OCRDMA_QP_PARAMS_DEST_QPN_MASK;\r\nqp_attr->qp_access_flags = ocrdma_to_ib_qp_acc_flags(qp->cap_flags);\r\nqp_attr->cap.max_send_wr = qp->sq.max_cnt - 1;\r\nqp_attr->cap.max_recv_wr = qp->rq.max_cnt - 1;\r\nqp_attr->cap.max_send_sge = qp->sq.max_sges;\r\nqp_attr->cap.max_recv_sge = qp->rq.max_sges;\r\nqp_attr->cap.max_inline_data = qp->max_inline_data;\r\nqp_init_attr->cap = qp_attr->cap;\r\nmemcpy(&qp_attr->ah_attr.grh.dgid, &params.dgid[0],\r\nsizeof(params.dgid));\r\nqp_attr->ah_attr.grh.flow_label = params.rnt_rc_sl_fl &\r\nOCRDMA_QP_PARAMS_FLOW_LABEL_MASK;\r\nqp_attr->ah_attr.grh.sgid_index = qp->sgid_idx;\r\nqp_attr->ah_attr.grh.hop_limit = (params.hop_lmt_rq_psn &\r\nOCRDMA_QP_PARAMS_HOP_LMT_MASK) >>\r\nOCRDMA_QP_PARAMS_HOP_LMT_SHIFT;\r\nqp_attr->ah_attr.grh.traffic_class = (params.tclass_sq_psn &\r\nOCRDMA_QP_PARAMS_TCLASS_MASK) >>\r\nOCRDMA_QP_PARAMS_TCLASS_SHIFT;\r\nqp_attr->ah_attr.ah_flags = IB_AH_GRH;\r\nqp_attr->ah_attr.port_num = 1;\r\nqp_attr->ah_attr.sl = (params.rnt_rc_sl_fl &\r\nOCRDMA_QP_PARAMS_SL_MASK) >>\r\nOCRDMA_QP_PARAMS_SL_SHIFT;\r\nqp_attr->timeout = (params.ack_to_rnr_rtc_dest_qpn &\r\nOCRDMA_QP_PARAMS_ACK_TIMEOUT_MASK) >>\r\nOCRDMA_QP_PARAMS_ACK_TIMEOUT_SHIFT;\r\nqp_attr->rnr_retry = (params.ack_to_rnr_rtc_dest_qpn &\r\nOCRDMA_QP_PARAMS_RNR_RETRY_CNT_MASK) >>\r\nOCRDMA_QP_PARAMS_RNR_RETRY_CNT_SHIFT;\r\nqp_attr->retry_cnt =\r\n(params.rnt_rc_sl_fl & OCRDMA_QP_PARAMS_RETRY_CNT_MASK) >>\r\nOCRDMA_QP_PARAMS_RETRY_CNT_SHIFT;\r\nqp_attr->min_rnr_timer = 0;\r\nqp_attr->pkey_index = 0;\r\nqp_attr->port_num = 1;\r\nqp_attr->ah_attr.src_path_bits = 0;\r\nqp_attr->ah_attr.static_rate = 0;\r\nqp_attr->alt_pkey_index = 0;\r\nqp_attr->alt_port_num = 0;\r\nqp_attr->alt_timeout = 0;\r\nmemset(&qp_attr->alt_ah_attr, 0, sizeof(qp_attr->alt_ah_attr));\r\nqp_state = (params.max_sge_recv_flags & OCRDMA_QP_PARAMS_STATE_MASK) >>\r\nOCRDMA_QP_PARAMS_STATE_SHIFT;\r\nqp_attr->sq_draining = (qp_state == OCRDMA_QPS_SQ_DRAINING) ? 1 : 0;\r\nqp_attr->max_dest_rd_atomic =\r\nparams.max_ord_ird >> OCRDMA_QP_PARAMS_MAX_ORD_SHIFT;\r\nqp_attr->max_rd_atomic =\r\nparams.max_ord_ird & OCRDMA_QP_PARAMS_MAX_IRD_MASK;\r\nqp_attr->en_sqd_async_notify = (params.max_sge_recv_flags &\r\nOCRDMA_QP_PARAMS_FLAGS_SQD_ASYNC) ? 1 : 0;\r\nmbx_err:\r\nreturn status;\r\n}\r\nstatic void ocrdma_srq_toggle_bit(struct ocrdma_srq *srq, int idx)\r\n{\r\nint i = idx / 32;\r\nunsigned int mask = (1 << (idx % 32));\r\nif (srq->idx_bit_fields[i] & mask)\r\nsrq->idx_bit_fields[i] &= ~mask;\r\nelse\r\nsrq->idx_bit_fields[i] |= mask;\r\n}\r\nstatic int ocrdma_hwq_free_cnt(struct ocrdma_qp_hwq_info *q)\r\n{\r\nreturn ((q->max_wqe_idx - q->head) + q->tail) % q->max_cnt;\r\n}\r\nstatic int is_hw_sq_empty(struct ocrdma_qp *qp)\r\n{\r\nreturn (qp->sq.tail == qp->sq.head);\r\n}\r\nstatic int is_hw_rq_empty(struct ocrdma_qp *qp)\r\n{\r\nreturn (qp->rq.tail == qp->rq.head);\r\n}\r\nstatic void *ocrdma_hwq_head(struct ocrdma_qp_hwq_info *q)\r\n{\r\nreturn q->va + (q->head * q->entry_size);\r\n}\r\nstatic void *ocrdma_hwq_head_from_idx(struct ocrdma_qp_hwq_info *q,\r\nu32 idx)\r\n{\r\nreturn q->va + (idx * q->entry_size);\r\n}\r\nstatic void ocrdma_hwq_inc_head(struct ocrdma_qp_hwq_info *q)\r\n{\r\nq->head = (q->head + 1) & q->max_wqe_idx;\r\n}\r\nstatic void ocrdma_hwq_inc_tail(struct ocrdma_qp_hwq_info *q)\r\n{\r\nq->tail = (q->tail + 1) & q->max_wqe_idx;\r\n}\r\nstatic void ocrdma_discard_cqes(struct ocrdma_qp *qp, struct ocrdma_cq *cq)\r\n{\r\nunsigned long cq_flags;\r\nunsigned long flags;\r\nint discard_cnt = 0;\r\nu32 cur_getp, stop_getp;\r\nstruct ocrdma_cqe *cqe;\r\nu32 qpn = 0, wqe_idx = 0;\r\nspin_lock_irqsave(&cq->cq_lock, cq_flags);\r\ncur_getp = cq->getp;\r\nstop_getp = cur_getp;\r\ndo {\r\nif (is_hw_sq_empty(qp) && (!qp->srq && is_hw_rq_empty(qp)))\r\nbreak;\r\ncqe = cq->va + cur_getp;\r\nqpn = cqe->cmn.qpn & OCRDMA_CQE_QPN_MASK;\r\nif (qpn == 0 || qpn != qp->id)\r\ngoto skip_cqe;\r\nif (is_cqe_for_sq(cqe)) {\r\nocrdma_hwq_inc_tail(&qp->sq);\r\n} else {\r\nif (qp->srq) {\r\nwqe_idx = (le32_to_cpu(cqe->rq.buftag_qpn) >>\r\nOCRDMA_CQE_BUFTAG_SHIFT) &\r\nqp->srq->rq.max_wqe_idx;\r\nif (wqe_idx < 1)\r\nBUG();\r\nspin_lock_irqsave(&qp->srq->q_lock, flags);\r\nocrdma_hwq_inc_tail(&qp->srq->rq);\r\nocrdma_srq_toggle_bit(qp->srq, wqe_idx - 1);\r\nspin_unlock_irqrestore(&qp->srq->q_lock, flags);\r\n} else {\r\nocrdma_hwq_inc_tail(&qp->rq);\r\n}\r\n}\r\ndiscard_cnt += 1;\r\ncqe->cmn.qpn = 0;\r\nskip_cqe:\r\ncur_getp = (cur_getp + 1) % cq->max_hw_cqe;\r\n} while (cur_getp != stop_getp);\r\nspin_unlock_irqrestore(&cq->cq_lock, cq_flags);\r\n}\r\nvoid ocrdma_del_flush_qp(struct ocrdma_qp *qp)\r\n{\r\nint found = false;\r\nunsigned long flags;\r\nstruct ocrdma_dev *dev = qp->dev;\r\nspin_lock_irqsave(&dev->flush_q_lock, flags);\r\nfound = ocrdma_is_qp_in_sq_flushlist(qp->sq_cq, qp);\r\nif (found)\r\nlist_del(&qp->sq_entry);\r\nif (!qp->srq) {\r\nfound = ocrdma_is_qp_in_rq_flushlist(qp->rq_cq, qp);\r\nif (found)\r\nlist_del(&qp->rq_entry);\r\n}\r\nspin_unlock_irqrestore(&dev->flush_q_lock, flags);\r\n}\r\nint ocrdma_destroy_qp(struct ib_qp *ibqp)\r\n{\r\nint status;\r\nstruct ocrdma_pd *pd;\r\nstruct ocrdma_qp *qp;\r\nstruct ocrdma_dev *dev;\r\nstruct ib_qp_attr attrs;\r\nint attr_mask = IB_QP_STATE;\r\nunsigned long flags;\r\nqp = get_ocrdma_qp(ibqp);\r\ndev = qp->dev;\r\nattrs.qp_state = IB_QPS_ERR;\r\npd = qp->pd;\r\n_ocrdma_modify_qp(ibqp, &attrs, attr_mask);\r\nmutex_lock(&dev->dev_lock);\r\nstatus = ocrdma_mbx_destroy_qp(dev, qp);\r\nspin_lock_irqsave(&qp->sq_cq->cq_lock, flags);\r\nif (qp->rq_cq && (qp->rq_cq != qp->sq_cq))\r\nspin_lock(&qp->rq_cq->cq_lock);\r\nocrdma_del_qpn_map(dev, qp);\r\nif (qp->rq_cq && (qp->rq_cq != qp->sq_cq))\r\nspin_unlock(&qp->rq_cq->cq_lock);\r\nspin_unlock_irqrestore(&qp->sq_cq->cq_lock, flags);\r\nif (!pd->uctx) {\r\nocrdma_discard_cqes(qp, qp->sq_cq);\r\nocrdma_discard_cqes(qp, qp->rq_cq);\r\n}\r\nmutex_unlock(&dev->dev_lock);\r\nif (pd->uctx) {\r\nocrdma_del_mmap(pd->uctx, (u64) qp->sq.pa,\r\nPAGE_ALIGN(qp->sq.len));\r\nif (!qp->srq)\r\nocrdma_del_mmap(pd->uctx, (u64) qp->rq.pa,\r\nPAGE_ALIGN(qp->rq.len));\r\n}\r\nocrdma_del_flush_qp(qp);\r\nkfree(qp->wqe_wr_id_tbl);\r\nkfree(qp->rqe_wr_id_tbl);\r\nkfree(qp);\r\nreturn status;\r\n}\r\nstatic int ocrdma_copy_srq_uresp(struct ocrdma_dev *dev, struct ocrdma_srq *srq,\r\nstruct ib_udata *udata)\r\n{\r\nint status;\r\nstruct ocrdma_create_srq_uresp uresp;\r\nmemset(&uresp, 0, sizeof(uresp));\r\nuresp.rq_dbid = srq->rq.dbid;\r\nuresp.num_rq_pages = 1;\r\nuresp.rq_page_addr[0] = srq->rq.pa;\r\nuresp.rq_page_size = srq->rq.len;\r\nuresp.db_page_addr = dev->nic_info.unmapped_db +\r\n(srq->pd->id * dev->nic_info.db_page_size);\r\nuresp.db_page_size = dev->nic_info.db_page_size;\r\nuresp.num_rqe_allocated = srq->rq.max_cnt;\r\nif (ocrdma_get_asic_type(dev) == OCRDMA_ASIC_GEN_SKH_R) {\r\nuresp.db_rq_offset = OCRDMA_DB_GEN2_RQ_OFFSET;\r\nuresp.db_shift = 24;\r\n} else {\r\nuresp.db_rq_offset = OCRDMA_DB_RQ_OFFSET;\r\nuresp.db_shift = 16;\r\n}\r\nstatus = ib_copy_to_udata(udata, &uresp, sizeof(uresp));\r\nif (status)\r\nreturn status;\r\nstatus = ocrdma_add_mmap(srq->pd->uctx, uresp.rq_page_addr[0],\r\nuresp.rq_page_size);\r\nif (status)\r\nreturn status;\r\nreturn status;\r\n}\r\nstruct ib_srq *ocrdma_create_srq(struct ib_pd *ibpd,\r\nstruct ib_srq_init_attr *init_attr,\r\nstruct ib_udata *udata)\r\n{\r\nint status = -ENOMEM;\r\nstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\r\nstruct ocrdma_srq *srq;\r\nif (init_attr->attr.max_sge > dev->attr.max_recv_sge)\r\nreturn ERR_PTR(-EINVAL);\r\nif (init_attr->attr.max_wr > dev->attr.max_rqe)\r\nreturn ERR_PTR(-EINVAL);\r\nsrq = kzalloc(sizeof(*srq), GFP_KERNEL);\r\nif (!srq)\r\nreturn ERR_PTR(status);\r\nspin_lock_init(&srq->q_lock);\r\nsrq->pd = pd;\r\nsrq->db = dev->nic_info.db + (pd->id * dev->nic_info.db_page_size);\r\nstatus = ocrdma_mbx_create_srq(dev, srq, init_attr, pd);\r\nif (status)\r\ngoto err;\r\nif (udata == NULL) {\r\nsrq->rqe_wr_id_tbl = kzalloc(sizeof(u64) * srq->rq.max_cnt,\r\nGFP_KERNEL);\r\nif (srq->rqe_wr_id_tbl == NULL)\r\ngoto arm_err;\r\nsrq->bit_fields_len = (srq->rq.max_cnt / 32) +\r\n(srq->rq.max_cnt % 32 ? 1 : 0);\r\nsrq->idx_bit_fields =\r\nkmalloc(srq->bit_fields_len * sizeof(u32), GFP_KERNEL);\r\nif (srq->idx_bit_fields == NULL)\r\ngoto arm_err;\r\nmemset(srq->idx_bit_fields, 0xff,\r\nsrq->bit_fields_len * sizeof(u32));\r\n}\r\nif (init_attr->attr.srq_limit) {\r\nstatus = ocrdma_mbx_modify_srq(srq, &init_attr->attr);\r\nif (status)\r\ngoto arm_err;\r\n}\r\nif (udata) {\r\nstatus = ocrdma_copy_srq_uresp(dev, srq, udata);\r\nif (status)\r\ngoto arm_err;\r\n}\r\nreturn &srq->ibsrq;\r\narm_err:\r\nocrdma_mbx_destroy_srq(dev, srq);\r\nerr:\r\nkfree(srq->rqe_wr_id_tbl);\r\nkfree(srq->idx_bit_fields);\r\nkfree(srq);\r\nreturn ERR_PTR(status);\r\n}\r\nint ocrdma_modify_srq(struct ib_srq *ibsrq,\r\nstruct ib_srq_attr *srq_attr,\r\nenum ib_srq_attr_mask srq_attr_mask,\r\nstruct ib_udata *udata)\r\n{\r\nint status = 0;\r\nstruct ocrdma_srq *srq;\r\nsrq = get_ocrdma_srq(ibsrq);\r\nif (srq_attr_mask & IB_SRQ_MAX_WR)\r\nstatus = -EINVAL;\r\nelse\r\nstatus = ocrdma_mbx_modify_srq(srq, srq_attr);\r\nreturn status;\r\n}\r\nint ocrdma_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr)\r\n{\r\nint status;\r\nstruct ocrdma_srq *srq;\r\nsrq = get_ocrdma_srq(ibsrq);\r\nstatus = ocrdma_mbx_query_srq(srq, srq_attr);\r\nreturn status;\r\n}\r\nint ocrdma_destroy_srq(struct ib_srq *ibsrq)\r\n{\r\nint status;\r\nstruct ocrdma_srq *srq;\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibsrq->device);\r\nsrq = get_ocrdma_srq(ibsrq);\r\nstatus = ocrdma_mbx_destroy_srq(dev, srq);\r\nif (srq->pd->uctx)\r\nocrdma_del_mmap(srq->pd->uctx, (u64) srq->rq.pa,\r\nPAGE_ALIGN(srq->rq.len));\r\nkfree(srq->idx_bit_fields);\r\nkfree(srq->rqe_wr_id_tbl);\r\nkfree(srq);\r\nreturn status;\r\n}\r\nstatic void ocrdma_build_ud_hdr(struct ocrdma_qp *qp,\r\nstruct ocrdma_hdr_wqe *hdr,\r\nstruct ib_send_wr *wr)\r\n{\r\nstruct ocrdma_ewqe_ud_hdr *ud_hdr =\r\n(struct ocrdma_ewqe_ud_hdr *)(hdr + 1);\r\nstruct ocrdma_ah *ah = get_ocrdma_ah(wr->wr.ud.ah);\r\nud_hdr->rsvd_dest_qpn = wr->wr.ud.remote_qpn;\r\nif (qp->qp_type == IB_QPT_GSI)\r\nud_hdr->qkey = qp->qkey;\r\nelse\r\nud_hdr->qkey = wr->wr.ud.remote_qkey;\r\nud_hdr->rsvd_ahid = ah->id;\r\n}\r\nstatic void ocrdma_build_sges(struct ocrdma_hdr_wqe *hdr,\r\nstruct ocrdma_sge *sge, int num_sge,\r\nstruct ib_sge *sg_list)\r\n{\r\nint i;\r\nfor (i = 0; i < num_sge; i++) {\r\nsge[i].lrkey = sg_list[i].lkey;\r\nsge[i].addr_lo = sg_list[i].addr;\r\nsge[i].addr_hi = upper_32_bits(sg_list[i].addr);\r\nsge[i].len = sg_list[i].length;\r\nhdr->total_len += sg_list[i].length;\r\n}\r\nif (num_sge == 0)\r\nmemset(sge, 0, sizeof(*sge));\r\n}\r\nstatic inline uint32_t ocrdma_sglist_len(struct ib_sge *sg_list, int num_sge)\r\n{\r\nuint32_t total_len = 0, i;\r\nfor (i = 0; i < num_sge; i++)\r\ntotal_len += sg_list[i].length;\r\nreturn total_len;\r\n}\r\nstatic int ocrdma_build_inline_sges(struct ocrdma_qp *qp,\r\nstruct ocrdma_hdr_wqe *hdr,\r\nstruct ocrdma_sge *sge,\r\nstruct ib_send_wr *wr, u32 wqe_size)\r\n{\r\nint i;\r\nchar *dpp_addr;\r\nif (wr->send_flags & IB_SEND_INLINE && qp->qp_type != IB_QPT_UD) {\r\nhdr->total_len = ocrdma_sglist_len(wr->sg_list, wr->num_sge);\r\nif (unlikely(hdr->total_len > qp->max_inline_data)) {\r\npr_err("%s() supported_len=0x%x,\n"\r\n" unspported len req=0x%x\n", __func__,\r\nqp->max_inline_data, hdr->total_len);\r\nreturn -EINVAL;\r\n}\r\ndpp_addr = (char *)sge;\r\nfor (i = 0; i < wr->num_sge; i++) {\r\nmemcpy(dpp_addr,\r\n(void *)(unsigned long)wr->sg_list[i].addr,\r\nwr->sg_list[i].length);\r\ndpp_addr += wr->sg_list[i].length;\r\n}\r\nwqe_size += roundup(hdr->total_len, OCRDMA_WQE_ALIGN_BYTES);\r\nif (0 == hdr->total_len)\r\nwqe_size += sizeof(struct ocrdma_sge);\r\nhdr->cw |= (OCRDMA_TYPE_INLINE << OCRDMA_WQE_TYPE_SHIFT);\r\n} else {\r\nocrdma_build_sges(hdr, sge, wr->num_sge, wr->sg_list);\r\nif (wr->num_sge)\r\nwqe_size += (wr->num_sge * sizeof(struct ocrdma_sge));\r\nelse\r\nwqe_size += sizeof(struct ocrdma_sge);\r\nhdr->cw |= (OCRDMA_TYPE_LKEY << OCRDMA_WQE_TYPE_SHIFT);\r\n}\r\nhdr->cw |= ((wqe_size / OCRDMA_WQE_STRIDE) << OCRDMA_WQE_SIZE_SHIFT);\r\nreturn 0;\r\n}\r\nstatic int ocrdma_build_send(struct ocrdma_qp *qp, struct ocrdma_hdr_wqe *hdr,\r\nstruct ib_send_wr *wr)\r\n{\r\nint status;\r\nstruct ocrdma_sge *sge;\r\nu32 wqe_size = sizeof(*hdr);\r\nif (qp->qp_type == IB_QPT_UD || qp->qp_type == IB_QPT_GSI) {\r\nocrdma_build_ud_hdr(qp, hdr, wr);\r\nsge = (struct ocrdma_sge *)(hdr + 2);\r\nwqe_size += sizeof(struct ocrdma_ewqe_ud_hdr);\r\n} else {\r\nsge = (struct ocrdma_sge *)(hdr + 1);\r\n}\r\nstatus = ocrdma_build_inline_sges(qp, hdr, sge, wr, wqe_size);\r\nreturn status;\r\n}\r\nstatic int ocrdma_build_write(struct ocrdma_qp *qp, struct ocrdma_hdr_wqe *hdr,\r\nstruct ib_send_wr *wr)\r\n{\r\nint status;\r\nstruct ocrdma_sge *ext_rw = (struct ocrdma_sge *)(hdr + 1);\r\nstruct ocrdma_sge *sge = ext_rw + 1;\r\nu32 wqe_size = sizeof(*hdr) + sizeof(*ext_rw);\r\nstatus = ocrdma_build_inline_sges(qp, hdr, sge, wr, wqe_size);\r\nif (status)\r\nreturn status;\r\next_rw->addr_lo = wr->wr.rdma.remote_addr;\r\next_rw->addr_hi = upper_32_bits(wr->wr.rdma.remote_addr);\r\next_rw->lrkey = wr->wr.rdma.rkey;\r\next_rw->len = hdr->total_len;\r\nreturn 0;\r\n}\r\nstatic void ocrdma_build_read(struct ocrdma_qp *qp, struct ocrdma_hdr_wqe *hdr,\r\nstruct ib_send_wr *wr)\r\n{\r\nstruct ocrdma_sge *ext_rw = (struct ocrdma_sge *)(hdr + 1);\r\nstruct ocrdma_sge *sge = ext_rw + 1;\r\nu32 wqe_size = ((wr->num_sge + 1) * sizeof(struct ocrdma_sge)) +\r\nsizeof(struct ocrdma_hdr_wqe);\r\nocrdma_build_sges(hdr, sge, wr->num_sge, wr->sg_list);\r\nhdr->cw |= ((wqe_size / OCRDMA_WQE_STRIDE) << OCRDMA_WQE_SIZE_SHIFT);\r\nhdr->cw |= (OCRDMA_READ << OCRDMA_WQE_OPCODE_SHIFT);\r\nhdr->cw |= (OCRDMA_TYPE_LKEY << OCRDMA_WQE_TYPE_SHIFT);\r\next_rw->addr_lo = wr->wr.rdma.remote_addr;\r\next_rw->addr_hi = upper_32_bits(wr->wr.rdma.remote_addr);\r\next_rw->lrkey = wr->wr.rdma.rkey;\r\next_rw->len = hdr->total_len;\r\n}\r\nstatic void build_frmr_pbes(struct ib_send_wr *wr, struct ocrdma_pbl *pbl_tbl,\r\nstruct ocrdma_hw_mr *hwmr)\r\n{\r\nint i;\r\nu64 buf_addr = 0;\r\nint num_pbes;\r\nstruct ocrdma_pbe *pbe;\r\npbe = (struct ocrdma_pbe *)pbl_tbl->va;\r\nnum_pbes = 0;\r\nfor (i = 0; i < wr->wr.fast_reg.page_list_len; i++) {\r\nbuf_addr = wr->wr.fast_reg.page_list->page_list[i];\r\npbe->pa_lo = cpu_to_le32((u32) (buf_addr & PAGE_MASK));\r\npbe->pa_hi = cpu_to_le32((u32) upper_32_bits(buf_addr));\r\nnum_pbes += 1;\r\npbe++;\r\nif (num_pbes == (hwmr->pbl_size/sizeof(u64))) {\r\npbl_tbl++;\r\npbe = (struct ocrdma_pbe *)pbl_tbl->va;\r\n}\r\n}\r\nreturn;\r\n}\r\nstatic int get_encoded_page_size(int pg_sz)\r\n{\r\nint i = 0;\r\nfor (; i < 17; i++)\r\nif (pg_sz == (4096 << i))\r\nbreak;\r\nreturn i;\r\n}\r\nstatic int ocrdma_build_fr(struct ocrdma_qp *qp, struct ocrdma_hdr_wqe *hdr,\r\nstruct ib_send_wr *wr)\r\n{\r\nu64 fbo;\r\nstruct ocrdma_ewqe_fr *fast_reg = (struct ocrdma_ewqe_fr *)(hdr + 1);\r\nstruct ocrdma_mr *mr;\r\nu32 wqe_size = sizeof(*fast_reg) + sizeof(*hdr);\r\nwqe_size = roundup(wqe_size, OCRDMA_WQE_ALIGN_BYTES);\r\nif (wr->wr.fast_reg.page_list_len > qp->dev->attr.max_pages_per_frmr)\r\nreturn -EINVAL;\r\nhdr->cw |= (OCRDMA_FR_MR << OCRDMA_WQE_OPCODE_SHIFT);\r\nhdr->cw |= ((wqe_size / OCRDMA_WQE_STRIDE) << OCRDMA_WQE_SIZE_SHIFT);\r\nif (wr->wr.fast_reg.page_list_len == 0)\r\nBUG();\r\nif (wr->wr.fast_reg.access_flags & IB_ACCESS_LOCAL_WRITE)\r\nhdr->rsvd_lkey_flags |= OCRDMA_LKEY_FLAG_LOCAL_WR;\r\nif (wr->wr.fast_reg.access_flags & IB_ACCESS_REMOTE_WRITE)\r\nhdr->rsvd_lkey_flags |= OCRDMA_LKEY_FLAG_REMOTE_WR;\r\nif (wr->wr.fast_reg.access_flags & IB_ACCESS_REMOTE_READ)\r\nhdr->rsvd_lkey_flags |= OCRDMA_LKEY_FLAG_REMOTE_RD;\r\nhdr->lkey = wr->wr.fast_reg.rkey;\r\nhdr->total_len = wr->wr.fast_reg.length;\r\nfbo = wr->wr.fast_reg.iova_start -\r\n(wr->wr.fast_reg.page_list->page_list[0] & PAGE_MASK);\r\nfast_reg->va_hi = upper_32_bits(wr->wr.fast_reg.iova_start);\r\nfast_reg->va_lo = (u32) (wr->wr.fast_reg.iova_start & 0xffffffff);\r\nfast_reg->fbo_hi = upper_32_bits(fbo);\r\nfast_reg->fbo_lo = (u32) fbo & 0xffffffff;\r\nfast_reg->num_sges = wr->wr.fast_reg.page_list_len;\r\nfast_reg->size_sge =\r\nget_encoded_page_size(1 << wr->wr.fast_reg.page_shift);\r\nmr = (struct ocrdma_mr *) (unsigned long)\r\nqp->dev->stag_arr[(hdr->lkey >> 8) & (OCRDMA_MAX_STAG - 1)];\r\nbuild_frmr_pbes(wr, mr->hwmr.pbl_table, &mr->hwmr);\r\nreturn 0;\r\n}\r\nstatic void ocrdma_ring_sq_db(struct ocrdma_qp *qp)\r\n{\r\nu32 val = qp->sq.dbid | (1 << OCRDMA_DB_SQ_SHIFT);\r\niowrite32(val, qp->sq_db);\r\n}\r\nint ocrdma_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,\r\nstruct ib_send_wr **bad_wr)\r\n{\r\nint status = 0;\r\nstruct ocrdma_qp *qp = get_ocrdma_qp(ibqp);\r\nstruct ocrdma_hdr_wqe *hdr;\r\nunsigned long flags;\r\nspin_lock_irqsave(&qp->q_lock, flags);\r\nif (qp->state != OCRDMA_QPS_RTS && qp->state != OCRDMA_QPS_SQD) {\r\nspin_unlock_irqrestore(&qp->q_lock, flags);\r\n*bad_wr = wr;\r\nreturn -EINVAL;\r\n}\r\nwhile (wr) {\r\nif (ocrdma_hwq_free_cnt(&qp->sq) == 0 ||\r\nwr->num_sge > qp->sq.max_sges) {\r\n*bad_wr = wr;\r\nstatus = -ENOMEM;\r\nbreak;\r\n}\r\nhdr = ocrdma_hwq_head(&qp->sq);\r\nhdr->cw = 0;\r\nif (wr->send_flags & IB_SEND_SIGNALED || qp->signaled)\r\nhdr->cw |= (OCRDMA_FLAG_SIG << OCRDMA_WQE_FLAGS_SHIFT);\r\nif (wr->send_flags & IB_SEND_FENCE)\r\nhdr->cw |=\r\n(OCRDMA_FLAG_FENCE_L << OCRDMA_WQE_FLAGS_SHIFT);\r\nif (wr->send_flags & IB_SEND_SOLICITED)\r\nhdr->cw |=\r\n(OCRDMA_FLAG_SOLICIT << OCRDMA_WQE_FLAGS_SHIFT);\r\nhdr->total_len = 0;\r\nswitch (wr->opcode) {\r\ncase IB_WR_SEND_WITH_IMM:\r\nhdr->cw |= (OCRDMA_FLAG_IMM << OCRDMA_WQE_FLAGS_SHIFT);\r\nhdr->immdt = ntohl(wr->ex.imm_data);\r\ncase IB_WR_SEND:\r\nhdr->cw |= (OCRDMA_SEND << OCRDMA_WQE_OPCODE_SHIFT);\r\nocrdma_build_send(qp, hdr, wr);\r\nbreak;\r\ncase IB_WR_SEND_WITH_INV:\r\nhdr->cw |= (OCRDMA_FLAG_INV << OCRDMA_WQE_FLAGS_SHIFT);\r\nhdr->cw |= (OCRDMA_SEND << OCRDMA_WQE_OPCODE_SHIFT);\r\nhdr->lkey = wr->ex.invalidate_rkey;\r\nstatus = ocrdma_build_send(qp, hdr, wr);\r\nbreak;\r\ncase IB_WR_RDMA_WRITE_WITH_IMM:\r\nhdr->cw |= (OCRDMA_FLAG_IMM << OCRDMA_WQE_FLAGS_SHIFT);\r\nhdr->immdt = ntohl(wr->ex.imm_data);\r\ncase IB_WR_RDMA_WRITE:\r\nhdr->cw |= (OCRDMA_WRITE << OCRDMA_WQE_OPCODE_SHIFT);\r\nstatus = ocrdma_build_write(qp, hdr, wr);\r\nbreak;\r\ncase IB_WR_RDMA_READ_WITH_INV:\r\nhdr->cw |= (OCRDMA_FLAG_INV << OCRDMA_WQE_FLAGS_SHIFT);\r\ncase IB_WR_RDMA_READ:\r\nocrdma_build_read(qp, hdr, wr);\r\nbreak;\r\ncase IB_WR_LOCAL_INV:\r\nhdr->cw |=\r\n(OCRDMA_LKEY_INV << OCRDMA_WQE_OPCODE_SHIFT);\r\nhdr->cw |= ((sizeof(struct ocrdma_hdr_wqe) +\r\nsizeof(struct ocrdma_sge)) /\r\nOCRDMA_WQE_STRIDE) << OCRDMA_WQE_SIZE_SHIFT;\r\nhdr->lkey = wr->ex.invalidate_rkey;\r\nbreak;\r\ncase IB_WR_FAST_REG_MR:\r\nstatus = ocrdma_build_fr(qp, hdr, wr);\r\nbreak;\r\ndefault:\r\nstatus = -EINVAL;\r\nbreak;\r\n}\r\nif (status) {\r\n*bad_wr = wr;\r\nbreak;\r\n}\r\nif (wr->send_flags & IB_SEND_SIGNALED || qp->signaled)\r\nqp->wqe_wr_id_tbl[qp->sq.head].signaled = 1;\r\nelse\r\nqp->wqe_wr_id_tbl[qp->sq.head].signaled = 0;\r\nqp->wqe_wr_id_tbl[qp->sq.head].wrid = wr->wr_id;\r\nocrdma_cpu_to_le32(hdr, ((hdr->cw >> OCRDMA_WQE_SIZE_SHIFT) &\r\nOCRDMA_WQE_SIZE_MASK) * OCRDMA_WQE_STRIDE);\r\nwmb();\r\nocrdma_ring_sq_db(qp);\r\nocrdma_hwq_inc_head(&qp->sq);\r\nwr = wr->next;\r\n}\r\nspin_unlock_irqrestore(&qp->q_lock, flags);\r\nreturn status;\r\n}\r\nstatic void ocrdma_ring_rq_db(struct ocrdma_qp *qp)\r\n{\r\nu32 val = qp->rq.dbid | (1 << OCRDMA_DB_RQ_SHIFT);\r\niowrite32(val, qp->rq_db);\r\n}\r\nstatic void ocrdma_build_rqe(struct ocrdma_hdr_wqe *rqe, struct ib_recv_wr *wr,\r\nu16 tag)\r\n{\r\nu32 wqe_size = 0;\r\nstruct ocrdma_sge *sge;\r\nif (wr->num_sge)\r\nwqe_size = (wr->num_sge * sizeof(*sge)) + sizeof(*rqe);\r\nelse\r\nwqe_size = sizeof(*sge) + sizeof(*rqe);\r\nrqe->cw = ((wqe_size / OCRDMA_WQE_STRIDE) <<\r\nOCRDMA_WQE_SIZE_SHIFT);\r\nrqe->cw |= (OCRDMA_FLAG_SIG << OCRDMA_WQE_FLAGS_SHIFT);\r\nrqe->cw |= (OCRDMA_TYPE_LKEY << OCRDMA_WQE_TYPE_SHIFT);\r\nrqe->total_len = 0;\r\nrqe->rsvd_tag = tag;\r\nsge = (struct ocrdma_sge *)(rqe + 1);\r\nocrdma_build_sges(rqe, sge, wr->num_sge, wr->sg_list);\r\nocrdma_cpu_to_le32(rqe, wqe_size);\r\n}\r\nint ocrdma_post_recv(struct ib_qp *ibqp, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nint status = 0;\r\nunsigned long flags;\r\nstruct ocrdma_qp *qp = get_ocrdma_qp(ibqp);\r\nstruct ocrdma_hdr_wqe *rqe;\r\nspin_lock_irqsave(&qp->q_lock, flags);\r\nif (qp->state == OCRDMA_QPS_RST || qp->state == OCRDMA_QPS_ERR) {\r\nspin_unlock_irqrestore(&qp->q_lock, flags);\r\n*bad_wr = wr;\r\nreturn -EINVAL;\r\n}\r\nwhile (wr) {\r\nif (ocrdma_hwq_free_cnt(&qp->rq) == 0 ||\r\nwr->num_sge > qp->rq.max_sges) {\r\n*bad_wr = wr;\r\nstatus = -ENOMEM;\r\nbreak;\r\n}\r\nrqe = ocrdma_hwq_head(&qp->rq);\r\nocrdma_build_rqe(rqe, wr, 0);\r\nqp->rqe_wr_id_tbl[qp->rq.head] = wr->wr_id;\r\nwmb();\r\nocrdma_ring_rq_db(qp);\r\nocrdma_hwq_inc_head(&qp->rq);\r\nwr = wr->next;\r\n}\r\nspin_unlock_irqrestore(&qp->q_lock, flags);\r\nreturn status;\r\n}\r\nstatic int ocrdma_srq_get_idx(struct ocrdma_srq *srq)\r\n{\r\nint row = 0;\r\nint indx = 0;\r\nfor (row = 0; row < srq->bit_fields_len; row++) {\r\nif (srq->idx_bit_fields[row]) {\r\nindx = ffs(srq->idx_bit_fields[row]);\r\nindx = (row * 32) + (indx - 1);\r\nif (indx >= srq->rq.max_cnt)\r\nBUG();\r\nocrdma_srq_toggle_bit(srq, indx);\r\nbreak;\r\n}\r\n}\r\nif (row == srq->bit_fields_len)\r\nBUG();\r\nreturn indx + 1;\r\n}\r\nstatic void ocrdma_ring_srq_db(struct ocrdma_srq *srq)\r\n{\r\nu32 val = srq->rq.dbid | (1 << 16);\r\niowrite32(val, srq->db + OCRDMA_DB_GEN2_SRQ_OFFSET);\r\n}\r\nint ocrdma_post_srq_recv(struct ib_srq *ibsrq, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nint status = 0;\r\nunsigned long flags;\r\nstruct ocrdma_srq *srq;\r\nstruct ocrdma_hdr_wqe *rqe;\r\nu16 tag;\r\nsrq = get_ocrdma_srq(ibsrq);\r\nspin_lock_irqsave(&srq->q_lock, flags);\r\nwhile (wr) {\r\nif (ocrdma_hwq_free_cnt(&srq->rq) == 0 ||\r\nwr->num_sge > srq->rq.max_sges) {\r\nstatus = -ENOMEM;\r\n*bad_wr = wr;\r\nbreak;\r\n}\r\ntag = ocrdma_srq_get_idx(srq);\r\nrqe = ocrdma_hwq_head(&srq->rq);\r\nocrdma_build_rqe(rqe, wr, tag);\r\nsrq->rqe_wr_id_tbl[tag] = wr->wr_id;\r\nwmb();\r\nocrdma_ring_srq_db(srq);\r\nocrdma_hwq_inc_head(&srq->rq);\r\nwr = wr->next;\r\n}\r\nspin_unlock_irqrestore(&srq->q_lock, flags);\r\nreturn status;\r\n}\r\nstatic enum ib_wc_status ocrdma_to_ibwc_err(u16 status)\r\n{\r\nenum ib_wc_status ibwc_status;\r\nswitch (status) {\r\ncase OCRDMA_CQE_GENERAL_ERR:\r\nibwc_status = IB_WC_GENERAL_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_LOC_LEN_ERR:\r\nibwc_status = IB_WC_LOC_LEN_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_LOC_QP_OP_ERR:\r\nibwc_status = IB_WC_LOC_QP_OP_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_LOC_EEC_OP_ERR:\r\nibwc_status = IB_WC_LOC_EEC_OP_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_LOC_PROT_ERR:\r\nibwc_status = IB_WC_LOC_PROT_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_WR_FLUSH_ERR:\r\nibwc_status = IB_WC_WR_FLUSH_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_MW_BIND_ERR:\r\nibwc_status = IB_WC_MW_BIND_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_BAD_RESP_ERR:\r\nibwc_status = IB_WC_BAD_RESP_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_LOC_ACCESS_ERR:\r\nibwc_status = IB_WC_LOC_ACCESS_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_REM_INV_REQ_ERR:\r\nibwc_status = IB_WC_REM_INV_REQ_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_REM_ACCESS_ERR:\r\nibwc_status = IB_WC_REM_ACCESS_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_REM_OP_ERR:\r\nibwc_status = IB_WC_REM_OP_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_RETRY_EXC_ERR:\r\nibwc_status = IB_WC_RETRY_EXC_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_RNR_RETRY_EXC_ERR:\r\nibwc_status = IB_WC_RNR_RETRY_EXC_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_LOC_RDD_VIOL_ERR:\r\nibwc_status = IB_WC_LOC_RDD_VIOL_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_REM_INV_RD_REQ_ERR:\r\nibwc_status = IB_WC_REM_INV_RD_REQ_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_REM_ABORT_ERR:\r\nibwc_status = IB_WC_REM_ABORT_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_INV_EECN_ERR:\r\nibwc_status = IB_WC_INV_EECN_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_INV_EEC_STATE_ERR:\r\nibwc_status = IB_WC_INV_EEC_STATE_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_FATAL_ERR:\r\nibwc_status = IB_WC_FATAL_ERR;\r\nbreak;\r\ncase OCRDMA_CQE_RESP_TIMEOUT_ERR:\r\nibwc_status = IB_WC_RESP_TIMEOUT_ERR;\r\nbreak;\r\ndefault:\r\nibwc_status = IB_WC_GENERAL_ERR;\r\nbreak;\r\n}\r\nreturn ibwc_status;\r\n}\r\nstatic void ocrdma_update_wc(struct ocrdma_qp *qp, struct ib_wc *ibwc,\r\nu32 wqe_idx)\r\n{\r\nstruct ocrdma_hdr_wqe *hdr;\r\nstruct ocrdma_sge *rw;\r\nint opcode;\r\nhdr = ocrdma_hwq_head_from_idx(&qp->sq, wqe_idx);\r\nibwc->wr_id = qp->wqe_wr_id_tbl[wqe_idx].wrid;\r\nopcode = le32_to_cpu(hdr->cw) & OCRDMA_WQE_OPCODE_MASK;\r\nswitch (opcode) {\r\ncase OCRDMA_WRITE:\r\nibwc->opcode = IB_WC_RDMA_WRITE;\r\nbreak;\r\ncase OCRDMA_READ:\r\nrw = (struct ocrdma_sge *)(hdr + 1);\r\nibwc->opcode = IB_WC_RDMA_READ;\r\nibwc->byte_len = rw->len;\r\nbreak;\r\ncase OCRDMA_SEND:\r\nibwc->opcode = IB_WC_SEND;\r\nbreak;\r\ncase OCRDMA_FR_MR:\r\nibwc->opcode = IB_WC_FAST_REG_MR;\r\nbreak;\r\ncase OCRDMA_LKEY_INV:\r\nibwc->opcode = IB_WC_LOCAL_INV;\r\nbreak;\r\ndefault:\r\nibwc->status = IB_WC_GENERAL_ERR;\r\npr_err("%s() invalid opcode received = 0x%x\n",\r\n__func__, hdr->cw & OCRDMA_WQE_OPCODE_MASK);\r\nbreak;\r\n}\r\n}\r\nstatic void ocrdma_set_cqe_status_flushed(struct ocrdma_qp *qp,\r\nstruct ocrdma_cqe *cqe)\r\n{\r\nif (is_cqe_for_sq(cqe)) {\r\ncqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\r\ncqe->flags_status_srcqpn) &\r\n~OCRDMA_CQE_STATUS_MASK);\r\ncqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\r\ncqe->flags_status_srcqpn) |\r\n(OCRDMA_CQE_WR_FLUSH_ERR <<\r\nOCRDMA_CQE_STATUS_SHIFT));\r\n} else {\r\nif (qp->qp_type == IB_QPT_UD || qp->qp_type == IB_QPT_GSI) {\r\ncqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\r\ncqe->flags_status_srcqpn) &\r\n~OCRDMA_CQE_UD_STATUS_MASK);\r\ncqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\r\ncqe->flags_status_srcqpn) |\r\n(OCRDMA_CQE_WR_FLUSH_ERR <<\r\nOCRDMA_CQE_UD_STATUS_SHIFT));\r\n} else {\r\ncqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\r\ncqe->flags_status_srcqpn) &\r\n~OCRDMA_CQE_STATUS_MASK);\r\ncqe->flags_status_srcqpn = cpu_to_le32(le32_to_cpu(\r\ncqe->flags_status_srcqpn) |\r\n(OCRDMA_CQE_WR_FLUSH_ERR <<\r\nOCRDMA_CQE_STATUS_SHIFT));\r\n}\r\n}\r\n}\r\nstatic bool ocrdma_update_err_cqe(struct ib_wc *ibwc, struct ocrdma_cqe *cqe,\r\nstruct ocrdma_qp *qp, int status)\r\n{\r\nbool expand = false;\r\nibwc->byte_len = 0;\r\nibwc->qp = &qp->ibqp;\r\nibwc->status = ocrdma_to_ibwc_err(status);\r\nocrdma_flush_qp(qp);\r\nocrdma_qp_state_change(qp, IB_QPS_ERR, NULL);\r\nif (!is_hw_rq_empty(qp) || !is_hw_sq_empty(qp)) {\r\nexpand = true;\r\nocrdma_set_cqe_status_flushed(qp, cqe);\r\n}\r\nreturn expand;\r\n}\r\nstatic int ocrdma_update_err_rcqe(struct ib_wc *ibwc, struct ocrdma_cqe *cqe,\r\nstruct ocrdma_qp *qp, int status)\r\n{\r\nibwc->opcode = IB_WC_RECV;\r\nibwc->wr_id = qp->rqe_wr_id_tbl[qp->rq.tail];\r\nocrdma_hwq_inc_tail(&qp->rq);\r\nreturn ocrdma_update_err_cqe(ibwc, cqe, qp, status);\r\n}\r\nstatic int ocrdma_update_err_scqe(struct ib_wc *ibwc, struct ocrdma_cqe *cqe,\r\nstruct ocrdma_qp *qp, int status)\r\n{\r\nocrdma_update_wc(qp, ibwc, qp->sq.tail);\r\nocrdma_hwq_inc_tail(&qp->sq);\r\nreturn ocrdma_update_err_cqe(ibwc, cqe, qp, status);\r\n}\r\nstatic bool ocrdma_poll_err_scqe(struct ocrdma_qp *qp,\r\nstruct ocrdma_cqe *cqe, struct ib_wc *ibwc,\r\nbool *polled, bool *stop)\r\n{\r\nbool expand;\r\nint status = (le32_to_cpu(cqe->flags_status_srcqpn) &\r\nOCRDMA_CQE_STATUS_MASK) >> OCRDMA_CQE_STATUS_SHIFT;\r\nif (is_hw_sq_empty(qp) && !is_hw_rq_empty(qp)) {\r\nif (!qp->srq && (qp->sq_cq == qp->rq_cq)) {\r\n*polled = true;\r\nstatus = OCRDMA_CQE_WR_FLUSH_ERR;\r\nexpand = ocrdma_update_err_rcqe(ibwc, cqe, qp, status);\r\n} else {\r\n*polled = false;\r\n*stop = true;\r\nexpand = false;\r\n}\r\n} else {\r\n*polled = true;\r\nexpand = ocrdma_update_err_scqe(ibwc, cqe, qp, status);\r\n}\r\nreturn expand;\r\n}\r\nstatic bool ocrdma_poll_success_scqe(struct ocrdma_qp *qp,\r\nstruct ocrdma_cqe *cqe,\r\nstruct ib_wc *ibwc, bool *polled)\r\n{\r\nbool expand = false;\r\nint tail = qp->sq.tail;\r\nu32 wqe_idx;\r\nif (!qp->wqe_wr_id_tbl[tail].signaled) {\r\n*polled = false;\r\n} else {\r\nibwc->status = IB_WC_SUCCESS;\r\nibwc->wc_flags = 0;\r\nibwc->qp = &qp->ibqp;\r\nocrdma_update_wc(qp, ibwc, tail);\r\n*polled = true;\r\n}\r\nwqe_idx = (le32_to_cpu(cqe->wq.wqeidx) &\r\nOCRDMA_CQE_WQEIDX_MASK) & qp->sq.max_wqe_idx;\r\nif (tail != wqe_idx)\r\nexpand = true;\r\nocrdma_hwq_inc_tail(&qp->sq);\r\nreturn expand;\r\n}\r\nstatic bool ocrdma_poll_scqe(struct ocrdma_qp *qp, struct ocrdma_cqe *cqe,\r\nstruct ib_wc *ibwc, bool *polled, bool *stop)\r\n{\r\nint status;\r\nbool expand;\r\nstatus = (le32_to_cpu(cqe->flags_status_srcqpn) &\r\nOCRDMA_CQE_STATUS_MASK) >> OCRDMA_CQE_STATUS_SHIFT;\r\nif (status == OCRDMA_CQE_SUCCESS)\r\nexpand = ocrdma_poll_success_scqe(qp, cqe, ibwc, polled);\r\nelse\r\nexpand = ocrdma_poll_err_scqe(qp, cqe, ibwc, polled, stop);\r\nreturn expand;\r\n}\r\nstatic int ocrdma_update_ud_rcqe(struct ib_wc *ibwc, struct ocrdma_cqe *cqe)\r\n{\r\nint status;\r\nstatus = (le32_to_cpu(cqe->flags_status_srcqpn) &\r\nOCRDMA_CQE_UD_STATUS_MASK) >> OCRDMA_CQE_UD_STATUS_SHIFT;\r\nibwc->src_qp = le32_to_cpu(cqe->flags_status_srcqpn) &\r\nOCRDMA_CQE_SRCQP_MASK;\r\nibwc->pkey_index = le32_to_cpu(cqe->ud.rxlen_pkey) &\r\nOCRDMA_CQE_PKEY_MASK;\r\nibwc->wc_flags = IB_WC_GRH;\r\nibwc->byte_len = (le32_to_cpu(cqe->ud.rxlen_pkey) >>\r\nOCRDMA_CQE_UD_XFER_LEN_SHIFT);\r\nreturn status;\r\n}\r\nstatic void ocrdma_update_free_srq_cqe(struct ib_wc *ibwc,\r\nstruct ocrdma_cqe *cqe,\r\nstruct ocrdma_qp *qp)\r\n{\r\nunsigned long flags;\r\nstruct ocrdma_srq *srq;\r\nu32 wqe_idx;\r\nsrq = get_ocrdma_srq(qp->ibqp.srq);\r\nwqe_idx = (le32_to_cpu(cqe->rq.buftag_qpn) >>\r\nOCRDMA_CQE_BUFTAG_SHIFT) & srq->rq.max_wqe_idx;\r\nif (wqe_idx < 1)\r\nBUG();\r\nibwc->wr_id = srq->rqe_wr_id_tbl[wqe_idx];\r\nspin_lock_irqsave(&srq->q_lock, flags);\r\nocrdma_srq_toggle_bit(srq, wqe_idx - 1);\r\nspin_unlock_irqrestore(&srq->q_lock, flags);\r\nocrdma_hwq_inc_tail(&srq->rq);\r\n}\r\nstatic bool ocrdma_poll_err_rcqe(struct ocrdma_qp *qp, struct ocrdma_cqe *cqe,\r\nstruct ib_wc *ibwc, bool *polled, bool *stop,\r\nint status)\r\n{\r\nbool expand;\r\nif (is_hw_rq_empty(qp) && !is_hw_sq_empty(qp)) {\r\nif (!qp->srq && (qp->sq_cq == qp->rq_cq)) {\r\n*polled = true;\r\nstatus = OCRDMA_CQE_WR_FLUSH_ERR;\r\nexpand = ocrdma_update_err_scqe(ibwc, cqe, qp, status);\r\n} else {\r\n*polled = false;\r\n*stop = true;\r\nexpand = false;\r\n}\r\n} else {\r\n*polled = true;\r\nexpand = ocrdma_update_err_rcqe(ibwc, cqe, qp, status);\r\n}\r\nreturn expand;\r\n}\r\nstatic void ocrdma_poll_success_rcqe(struct ocrdma_qp *qp,\r\nstruct ocrdma_cqe *cqe, struct ib_wc *ibwc)\r\n{\r\nibwc->opcode = IB_WC_RECV;\r\nibwc->qp = &qp->ibqp;\r\nibwc->status = IB_WC_SUCCESS;\r\nif (qp->qp_type == IB_QPT_UD || qp->qp_type == IB_QPT_GSI)\r\nocrdma_update_ud_rcqe(ibwc, cqe);\r\nelse\r\nibwc->byte_len = le32_to_cpu(cqe->rq.rxlen);\r\nif (is_cqe_imm(cqe)) {\r\nibwc->ex.imm_data = htonl(le32_to_cpu(cqe->rq.lkey_immdt));\r\nibwc->wc_flags |= IB_WC_WITH_IMM;\r\n} else if (is_cqe_wr_imm(cqe)) {\r\nibwc->opcode = IB_WC_RECV_RDMA_WITH_IMM;\r\nibwc->ex.imm_data = htonl(le32_to_cpu(cqe->rq.lkey_immdt));\r\nibwc->wc_flags |= IB_WC_WITH_IMM;\r\n} else if (is_cqe_invalidated(cqe)) {\r\nibwc->ex.invalidate_rkey = le32_to_cpu(cqe->rq.lkey_immdt);\r\nibwc->wc_flags |= IB_WC_WITH_INVALIDATE;\r\n}\r\nif (qp->ibqp.srq) {\r\nocrdma_update_free_srq_cqe(ibwc, cqe, qp);\r\n} else {\r\nibwc->wr_id = qp->rqe_wr_id_tbl[qp->rq.tail];\r\nocrdma_hwq_inc_tail(&qp->rq);\r\n}\r\n}\r\nstatic bool ocrdma_poll_rcqe(struct ocrdma_qp *qp, struct ocrdma_cqe *cqe,\r\nstruct ib_wc *ibwc, bool *polled, bool *stop)\r\n{\r\nint status;\r\nbool expand = false;\r\nibwc->wc_flags = 0;\r\nif (qp->qp_type == IB_QPT_UD || qp->qp_type == IB_QPT_GSI) {\r\nstatus = (le32_to_cpu(cqe->flags_status_srcqpn) &\r\nOCRDMA_CQE_UD_STATUS_MASK) >>\r\nOCRDMA_CQE_UD_STATUS_SHIFT;\r\n} else {\r\nstatus = (le32_to_cpu(cqe->flags_status_srcqpn) &\r\nOCRDMA_CQE_STATUS_MASK) >> OCRDMA_CQE_STATUS_SHIFT;\r\n}\r\nif (status == OCRDMA_CQE_SUCCESS) {\r\n*polled = true;\r\nocrdma_poll_success_rcqe(qp, cqe, ibwc);\r\n} else {\r\nexpand = ocrdma_poll_err_rcqe(qp, cqe, ibwc, polled, stop,\r\nstatus);\r\n}\r\nreturn expand;\r\n}\r\nstatic void ocrdma_change_cq_phase(struct ocrdma_cq *cq, struct ocrdma_cqe *cqe,\r\nu16 cur_getp)\r\n{\r\nif (cq->phase_change) {\r\nif (cur_getp == 0)\r\ncq->phase = (~cq->phase & OCRDMA_CQE_VALID);\r\n} else {\r\ncqe->flags_status_srcqpn = 0;\r\n}\r\n}\r\nstatic int ocrdma_poll_hwcq(struct ocrdma_cq *cq, int num_entries,\r\nstruct ib_wc *ibwc)\r\n{\r\nu16 qpn = 0;\r\nint i = 0;\r\nbool expand = false;\r\nint polled_hw_cqes = 0;\r\nstruct ocrdma_qp *qp = NULL;\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(cq->ibcq.device);\r\nstruct ocrdma_cqe *cqe;\r\nu16 cur_getp; bool polled = false; bool stop = false;\r\ncur_getp = cq->getp;\r\nwhile (num_entries) {\r\ncqe = cq->va + cur_getp;\r\nif (!is_cqe_valid(cq, cqe))\r\nbreak;\r\nqpn = (le32_to_cpu(cqe->cmn.qpn) & OCRDMA_CQE_QPN_MASK);\r\nif (qpn == 0)\r\ngoto skip_cqe;\r\nqp = dev->qp_tbl[qpn];\r\nBUG_ON(qp == NULL);\r\nif (is_cqe_for_sq(cqe)) {\r\nexpand = ocrdma_poll_scqe(qp, cqe, ibwc, &polled,\r\n&stop);\r\n} else {\r\nexpand = ocrdma_poll_rcqe(qp, cqe, ibwc, &polled,\r\n&stop);\r\n}\r\nif (expand)\r\ngoto expand_cqe;\r\nif (stop)\r\ngoto stop_cqe;\r\ncqe->cmn.qpn = 0;\r\nskip_cqe:\r\npolled_hw_cqes += 1;\r\ncur_getp = (cur_getp + 1) % cq->max_hw_cqe;\r\nocrdma_change_cq_phase(cq, cqe, cur_getp);\r\nexpand_cqe:\r\nif (polled) {\r\nnum_entries -= 1;\r\ni += 1;\r\nibwc = ibwc + 1;\r\npolled = false;\r\n}\r\n}\r\nstop_cqe:\r\ncq->getp = cur_getp;\r\nif (cq->deferred_arm) {\r\nocrdma_ring_cq_db(dev, cq->id, true, cq->deferred_sol,\r\npolled_hw_cqes);\r\ncq->deferred_arm = false;\r\ncq->deferred_sol = false;\r\n} else {\r\nocrdma_ring_cq_db(dev, cq->id, false, cq->deferred_sol,\r\npolled_hw_cqes);\r\ncq->deferred_sol = false;\r\n}\r\nreturn i;\r\n}\r\nstatic int ocrdma_add_err_cqe(struct ocrdma_cq *cq, int num_entries,\r\nstruct ocrdma_qp *qp, struct ib_wc *ibwc)\r\n{\r\nint err_cqes = 0;\r\nwhile (num_entries) {\r\nif (is_hw_sq_empty(qp) && is_hw_rq_empty(qp))\r\nbreak;\r\nif (!is_hw_sq_empty(qp) && qp->sq_cq == cq) {\r\nocrdma_update_wc(qp, ibwc, qp->sq.tail);\r\nocrdma_hwq_inc_tail(&qp->sq);\r\n} else if (!is_hw_rq_empty(qp) && qp->rq_cq == cq) {\r\nibwc->wr_id = qp->rqe_wr_id_tbl[qp->rq.tail];\r\nocrdma_hwq_inc_tail(&qp->rq);\r\n} else {\r\nreturn err_cqes;\r\n}\r\nibwc->byte_len = 0;\r\nibwc->status = IB_WC_WR_FLUSH_ERR;\r\nibwc = ibwc + 1;\r\nerr_cqes += 1;\r\nnum_entries -= 1;\r\n}\r\nreturn err_cqes;\r\n}\r\nint ocrdma_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc)\r\n{\r\nint cqes_to_poll = num_entries;\r\nstruct ocrdma_cq *cq = get_ocrdma_cq(ibcq);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibcq->device);\r\nint num_os_cqe = 0, err_cqes = 0;\r\nstruct ocrdma_qp *qp;\r\nunsigned long flags;\r\nspin_lock_irqsave(&cq->cq_lock, flags);\r\nnum_os_cqe = ocrdma_poll_hwcq(cq, cqes_to_poll, wc);\r\nspin_unlock_irqrestore(&cq->cq_lock, flags);\r\ncqes_to_poll -= num_os_cqe;\r\nif (cqes_to_poll) {\r\nwc = wc + num_os_cqe;\r\nspin_lock_irqsave(&dev->flush_q_lock, flags);\r\nlist_for_each_entry(qp, &cq->sq_head, sq_entry) {\r\nif (cqes_to_poll == 0)\r\nbreak;\r\nerr_cqes = ocrdma_add_err_cqe(cq, cqes_to_poll, qp, wc);\r\ncqes_to_poll -= err_cqes;\r\nnum_os_cqe += err_cqes;\r\nwc = wc + err_cqes;\r\n}\r\nspin_unlock_irqrestore(&dev->flush_q_lock, flags);\r\n}\r\nreturn num_os_cqe;\r\n}\r\nint ocrdma_arm_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags cq_flags)\r\n{\r\nstruct ocrdma_cq *cq = get_ocrdma_cq(ibcq);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibcq->device);\r\nu16 cq_id;\r\nunsigned long flags;\r\nbool arm_needed = false, sol_needed = false;\r\ncq_id = cq->id;\r\nspin_lock_irqsave(&cq->cq_lock, flags);\r\nif (cq_flags & IB_CQ_NEXT_COMP || cq_flags & IB_CQ_SOLICITED)\r\narm_needed = true;\r\nif (cq_flags & IB_CQ_SOLICITED)\r\nsol_needed = true;\r\nif (cq->first_arm) {\r\nocrdma_ring_cq_db(dev, cq_id, arm_needed, sol_needed, 0);\r\ncq->first_arm = false;\r\ngoto skip_defer;\r\n}\r\ncq->deferred_arm = true;\r\nskip_defer:\r\ncq->deferred_sol = sol_needed;\r\nspin_unlock_irqrestore(&cq->cq_lock, flags);\r\nreturn 0;\r\n}\r\nstruct ib_mr *ocrdma_alloc_frmr(struct ib_pd *ibpd, int max_page_list_len)\r\n{\r\nint status;\r\nstruct ocrdma_mr *mr;\r\nstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\r\nif (max_page_list_len > dev->attr.max_pages_per_frmr)\r\nreturn ERR_PTR(-EINVAL);\r\nmr = kzalloc(sizeof(*mr), GFP_KERNEL);\r\nif (!mr)\r\nreturn ERR_PTR(-ENOMEM);\r\nstatus = ocrdma_get_pbl_info(dev, mr, max_page_list_len);\r\nif (status)\r\ngoto pbl_err;\r\nmr->hwmr.fr_mr = 1;\r\nmr->hwmr.remote_rd = 0;\r\nmr->hwmr.remote_wr = 0;\r\nmr->hwmr.local_rd = 0;\r\nmr->hwmr.local_wr = 0;\r\nmr->hwmr.mw_bind = 0;\r\nstatus = ocrdma_build_pbl_tbl(dev, &mr->hwmr);\r\nif (status)\r\ngoto pbl_err;\r\nstatus = ocrdma_reg_mr(dev, &mr->hwmr, pd->id, 0);\r\nif (status)\r\ngoto mbx_err;\r\nmr->ibmr.rkey = mr->hwmr.lkey;\r\nmr->ibmr.lkey = mr->hwmr.lkey;\r\ndev->stag_arr[(mr->hwmr.lkey >> 8) & (OCRDMA_MAX_STAG - 1)] =\r\n(unsigned long) mr;\r\nreturn &mr->ibmr;\r\nmbx_err:\r\nocrdma_free_mr_pbl_tbl(dev, &mr->hwmr);\r\npbl_err:\r\nkfree(mr);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nstruct ib_fast_reg_page_list *ocrdma_alloc_frmr_page_list(struct ib_device\r\n*ibdev,\r\nint page_list_len)\r\n{\r\nstruct ib_fast_reg_page_list *frmr_list;\r\nint size;\r\nsize = sizeof(*frmr_list) + (page_list_len * sizeof(u64));\r\nfrmr_list = kzalloc(size, GFP_KERNEL);\r\nif (!frmr_list)\r\nreturn ERR_PTR(-ENOMEM);\r\nfrmr_list->page_list = (u64 *)(frmr_list + 1);\r\nreturn frmr_list;\r\n}\r\nvoid ocrdma_free_frmr_page_list(struct ib_fast_reg_page_list *page_list)\r\n{\r\nkfree(page_list);\r\n}\r\nstatic inline int count_kernel_pbes(struct ib_phys_buf *buf_list,\r\nint buf_cnt, u32 *pbe_size)\r\n{\r\nu64 total_size = 0;\r\nu64 buf_size = 0;\r\nint i;\r\n*pbe_size = roundup(buf_list[0].size, PAGE_SIZE);\r\n*pbe_size = roundup_pow_of_two(*pbe_size);\r\nfor (i = 0; i < buf_cnt; i++) {\r\nif ((i != 0) && ((buf_list[i].addr & ~PAGE_MASK) ||\r\n(buf_list[i].size & ~PAGE_MASK))) {\r\nreturn 0;\r\n}\r\nbuf_size = roundup(buf_list[i].size, PAGE_SIZE);\r\nbuf_size = roundup_pow_of_two(buf_size);\r\nif (*pbe_size > buf_size)\r\n*pbe_size = buf_size;\r\ntotal_size += buf_size;\r\n}\r\n*pbe_size = *pbe_size > MAX_KERNEL_PBE_SIZE ?\r\n(MAX_KERNEL_PBE_SIZE) : (*pbe_size);\r\nreturn total_size >> ilog2(*pbe_size);\r\n}\r\nstatic void build_kernel_pbes(struct ib_phys_buf *buf_list, int ib_buf_cnt,\r\nu32 pbe_size, struct ocrdma_pbl *pbl_tbl,\r\nstruct ocrdma_hw_mr *hwmr)\r\n{\r\nint i;\r\nint idx;\r\nint pbes_per_buf = 0;\r\nu64 buf_addr = 0;\r\nint num_pbes;\r\nstruct ocrdma_pbe *pbe;\r\nint total_num_pbes = 0;\r\nif (!hwmr->num_pbes)\r\nreturn;\r\npbe = (struct ocrdma_pbe *)pbl_tbl->va;\r\nnum_pbes = 0;\r\nfor (i = 0; i < ib_buf_cnt; i++) {\r\nbuf_addr = buf_list[i].addr;\r\npbes_per_buf =\r\nroundup_pow_of_two(roundup(buf_list[i].size, PAGE_SIZE)) /\r\npbe_size;\r\nhwmr->len += buf_list[i].size;\r\nfor (idx = 0; idx < pbes_per_buf; idx++) {\r\nif (i == 0) {\r\npbe->pa_lo =\r\ncpu_to_le32((u32) (buf_addr & PAGE_MASK));\r\npbe->pa_hi =\r\ncpu_to_le32((u32) upper_32_bits(buf_addr));\r\n} else {\r\npbe->pa_lo =\r\ncpu_to_le32((u32) (buf_addr & 0xffffffff));\r\npbe->pa_hi =\r\ncpu_to_le32((u32) upper_32_bits(buf_addr));\r\n}\r\nbuf_addr += pbe_size;\r\nnum_pbes += 1;\r\ntotal_num_pbes += 1;\r\npbe++;\r\nif (total_num_pbes == hwmr->num_pbes)\r\ngoto mr_tbl_done;\r\nif (num_pbes == (hwmr->pbl_size/sizeof(u64))) {\r\npbl_tbl++;\r\npbe = (struct ocrdma_pbe *)pbl_tbl->va;\r\nnum_pbes = 0;\r\n}\r\n}\r\n}\r\nmr_tbl_done:\r\nreturn;\r\n}\r\nstruct ib_mr *ocrdma_reg_kernel_mr(struct ib_pd *ibpd,\r\nstruct ib_phys_buf *buf_list,\r\nint buf_cnt, int acc, u64 *iova_start)\r\n{\r\nint status = -ENOMEM;\r\nstruct ocrdma_mr *mr;\r\nstruct ocrdma_pd *pd = get_ocrdma_pd(ibpd);\r\nstruct ocrdma_dev *dev = get_ocrdma_dev(ibpd->device);\r\nu32 num_pbes;\r\nu32 pbe_size = 0;\r\nif ((acc & IB_ACCESS_REMOTE_WRITE) && !(acc & IB_ACCESS_LOCAL_WRITE))\r\nreturn ERR_PTR(-EINVAL);\r\nmr = kzalloc(sizeof(*mr), GFP_KERNEL);\r\nif (!mr)\r\nreturn ERR_PTR(status);\r\nnum_pbes = count_kernel_pbes(buf_list, buf_cnt, &pbe_size);\r\nif (num_pbes == 0) {\r\nstatus = -EINVAL;\r\ngoto pbl_err;\r\n}\r\nstatus = ocrdma_get_pbl_info(dev, mr, num_pbes);\r\nif (status)\r\ngoto pbl_err;\r\nmr->hwmr.pbe_size = pbe_size;\r\nmr->hwmr.fbo = *iova_start - (buf_list[0].addr & PAGE_MASK);\r\nmr->hwmr.va = *iova_start;\r\nmr->hwmr.local_rd = 1;\r\nmr->hwmr.remote_wr = (acc & IB_ACCESS_REMOTE_WRITE) ? 1 : 0;\r\nmr->hwmr.remote_rd = (acc & IB_ACCESS_REMOTE_READ) ? 1 : 0;\r\nmr->hwmr.local_wr = (acc & IB_ACCESS_LOCAL_WRITE) ? 1 : 0;\r\nmr->hwmr.remote_atomic = (acc & IB_ACCESS_REMOTE_ATOMIC) ? 1 : 0;\r\nmr->hwmr.mw_bind = (acc & IB_ACCESS_MW_BIND) ? 1 : 0;\r\nstatus = ocrdma_build_pbl_tbl(dev, &mr->hwmr);\r\nif (status)\r\ngoto pbl_err;\r\nbuild_kernel_pbes(buf_list, buf_cnt, pbe_size, mr->hwmr.pbl_table,\r\n&mr->hwmr);\r\nstatus = ocrdma_reg_mr(dev, &mr->hwmr, pd->id, acc);\r\nif (status)\r\ngoto mbx_err;\r\nmr->ibmr.lkey = mr->hwmr.lkey;\r\nif (mr->hwmr.remote_wr || mr->hwmr.remote_rd)\r\nmr->ibmr.rkey = mr->hwmr.lkey;\r\nreturn &mr->ibmr;\r\nmbx_err:\r\nocrdma_free_mr_pbl_tbl(dev, &mr->hwmr);\r\npbl_err:\r\nkfree(mr);\r\nreturn ERR_PTR(status);\r\n}
