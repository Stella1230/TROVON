static long privcmd_ioctl_hypercall(void __user *udata)\r\n{\r\nstruct privcmd_hypercall hypercall;\r\nlong ret;\r\nif (copy_from_user(&hypercall, udata, sizeof(hypercall)))\r\nreturn -EFAULT;\r\nret = privcmd_call(hypercall.op,\r\nhypercall.arg[0], hypercall.arg[1],\r\nhypercall.arg[2], hypercall.arg[3],\r\nhypercall.arg[4]);\r\nreturn ret;\r\n}\r\nstatic void free_page_list(struct list_head *pages)\r\n{\r\nstruct page *p, *n;\r\nlist_for_each_entry_safe(p, n, pages, lru)\r\n__free_page(p);\r\nINIT_LIST_HEAD(pages);\r\n}\r\nstatic int gather_array(struct list_head *pagelist,\r\nunsigned nelem, size_t size,\r\nconst void __user *data)\r\n{\r\nunsigned pageidx;\r\nvoid *pagedata;\r\nint ret;\r\nif (size > PAGE_SIZE)\r\nreturn 0;\r\npageidx = PAGE_SIZE;\r\npagedata = NULL;\r\nwhile (nelem--) {\r\nif (pageidx > PAGE_SIZE-size) {\r\nstruct page *page = alloc_page(GFP_KERNEL);\r\nret = -ENOMEM;\r\nif (page == NULL)\r\ngoto fail;\r\npagedata = page_address(page);\r\nlist_add_tail(&page->lru, pagelist);\r\npageidx = 0;\r\n}\r\nret = -EFAULT;\r\nif (copy_from_user(pagedata + pageidx, data, size))\r\ngoto fail;\r\ndata += size;\r\npageidx += size;\r\n}\r\nret = 0;\r\nfail:\r\nreturn ret;\r\n}\r\nstatic int traverse_pages(unsigned nelem, size_t size,\r\nstruct list_head *pos,\r\nint (*fn)(void *data, void *state),\r\nvoid *state)\r\n{\r\nvoid *pagedata;\r\nunsigned pageidx;\r\nint ret = 0;\r\nBUG_ON(size > PAGE_SIZE);\r\npageidx = PAGE_SIZE;\r\npagedata = NULL;\r\nwhile (nelem--) {\r\nif (pageidx > PAGE_SIZE-size) {\r\nstruct page *page;\r\npos = pos->next;\r\npage = list_entry(pos, struct page, lru);\r\npagedata = page_address(page);\r\npageidx = 0;\r\n}\r\nret = (*fn)(pagedata + pageidx, state);\r\nif (ret)\r\nbreak;\r\npageidx += size;\r\n}\r\nreturn ret;\r\n}\r\nstatic int mmap_mfn_range(void *data, void *state)\r\n{\r\nstruct privcmd_mmap_entry *msg = data;\r\nstruct mmap_mfn_state *st = state;\r\nstruct vm_area_struct *vma = st->vma;\r\nint rc;\r\nif ((msg->npages > (LONG_MAX >> PAGE_SHIFT)) ||\r\n((unsigned long)(msg->npages << PAGE_SHIFT) >= -st->va))\r\nreturn -EINVAL;\r\nif ((msg->va != st->va) ||\r\n((msg->va+(msg->npages<<PAGE_SHIFT)) > vma->vm_end))\r\nreturn -EINVAL;\r\nrc = xen_remap_domain_mfn_range(vma,\r\nmsg->va & PAGE_MASK,\r\nmsg->mfn, msg->npages,\r\nvma->vm_page_prot,\r\nst->domain, NULL);\r\nif (rc < 0)\r\nreturn rc;\r\nst->va += msg->npages << PAGE_SHIFT;\r\nreturn 0;\r\n}\r\nstatic long privcmd_ioctl_mmap(void __user *udata)\r\n{\r\nstruct privcmd_mmap mmapcmd;\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nint rc;\r\nLIST_HEAD(pagelist);\r\nstruct mmap_mfn_state state;\r\nif (xen_feature(XENFEAT_auto_translated_physmap))\r\nreturn -ENOSYS;\r\nif (copy_from_user(&mmapcmd, udata, sizeof(mmapcmd)))\r\nreturn -EFAULT;\r\nrc = gather_array(&pagelist,\r\nmmapcmd.num, sizeof(struct privcmd_mmap_entry),\r\nmmapcmd.entry);\r\nif (rc || list_empty(&pagelist))\r\ngoto out;\r\ndown_write(&mm->mmap_sem);\r\n{\r\nstruct page *page = list_first_entry(&pagelist,\r\nstruct page, lru);\r\nstruct privcmd_mmap_entry *msg = page_address(page);\r\nvma = find_vma(mm, msg->va);\r\nrc = -EINVAL;\r\nif (!vma || (msg->va != vma->vm_start) || vma->vm_private_data)\r\ngoto out_up;\r\nvma->vm_private_data = PRIV_VMA_LOCKED;\r\n}\r\nstate.va = vma->vm_start;\r\nstate.vma = vma;\r\nstate.domain = mmapcmd.dom;\r\nrc = traverse_pages(mmapcmd.num, sizeof(struct privcmd_mmap_entry),\r\n&pagelist,\r\nmmap_mfn_range, &state);\r\nout_up:\r\nup_write(&mm->mmap_sem);\r\nout:\r\nfree_page_list(&pagelist);\r\nreturn rc;\r\n}\r\nstatic int mmap_batch_fn(void *data, void *state)\r\n{\r\nxen_pfn_t *mfnp = data;\r\nstruct mmap_batch_state *st = state;\r\nstruct vm_area_struct *vma = st->vma;\r\nstruct page **pages = vma->vm_private_data;\r\nstruct page *cur_page = NULL;\r\nint ret;\r\nif (xen_feature(XENFEAT_auto_translated_physmap))\r\ncur_page = pages[st->index++];\r\nret = xen_remap_domain_mfn_range(st->vma, st->va & PAGE_MASK, *mfnp, 1,\r\nst->vma->vm_page_prot, st->domain,\r\n&cur_page);\r\nif (st->version == 1) {\r\nif (ret < 0) {\r\n*mfnp |= (ret == -ENOENT) ?\r\nPRIVCMD_MMAPBATCH_PAGED_ERROR :\r\nPRIVCMD_MMAPBATCH_MFN_ERROR;\r\n}\r\n} else {\r\n*((int *) mfnp) = ret;\r\n}\r\nif (ret < 0) {\r\nif (ret == -ENOENT)\r\nst->global_error = -ENOENT;\r\nelse {\r\nif (st->global_error == 0)\r\nst->global_error = 1;\r\n}\r\n}\r\nst->va += PAGE_SIZE;\r\nreturn 0;\r\n}\r\nstatic int mmap_return_errors(void *data, void *state)\r\n{\r\nstruct mmap_batch_state *st = state;\r\nif (st->version == 1) {\r\nxen_pfn_t mfnp = *((xen_pfn_t *) data);\r\nif (mfnp & PRIVCMD_MMAPBATCH_MFN_ERROR)\r\nreturn __put_user(mfnp, st->user_mfn++);\r\nelse\r\nst->user_mfn++;\r\n} else {\r\nint err = *((int *) data);\r\nif (err)\r\nreturn __put_user(err, st->user_err++);\r\nelse\r\nst->user_err++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int alloc_empty_pages(struct vm_area_struct *vma, int numpgs)\r\n{\r\nint rc;\r\nstruct page **pages;\r\npages = kcalloc(numpgs, sizeof(pages[0]), GFP_KERNEL);\r\nif (pages == NULL)\r\nreturn -ENOMEM;\r\nrc = alloc_xenballooned_pages(numpgs, pages, 0);\r\nif (rc != 0) {\r\npr_warn("%s Could not alloc %d pfns rc:%d\n", __func__,\r\nnumpgs, rc);\r\nkfree(pages);\r\nreturn -ENOMEM;\r\n}\r\nBUG_ON(vma->vm_private_data != NULL);\r\nvma->vm_private_data = pages;\r\nreturn 0;\r\n}\r\nstatic long privcmd_ioctl_mmap_batch(void __user *udata, int version)\r\n{\r\nint ret;\r\nstruct privcmd_mmapbatch_v2 m;\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nunsigned long nr_pages;\r\nLIST_HEAD(pagelist);\r\nstruct mmap_batch_state state;\r\nswitch (version) {\r\ncase 1:\r\nif (copy_from_user(&m, udata, sizeof(struct privcmd_mmapbatch)))\r\nreturn -EFAULT;\r\nm.err = NULL;\r\nif (!access_ok(VERIFY_WRITE, m.arr, m.num * sizeof(*m.arr)))\r\nreturn -EFAULT;\r\nbreak;\r\ncase 2:\r\nif (copy_from_user(&m, udata, sizeof(struct privcmd_mmapbatch_v2)))\r\nreturn -EFAULT;\r\nif (!access_ok(VERIFY_WRITE, m.err, m.num * (sizeof(*m.err))))\r\nreturn -EFAULT;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nnr_pages = m.num;\r\nif ((m.num <= 0) || (nr_pages > (LONG_MAX >> PAGE_SHIFT)))\r\nreturn -EINVAL;\r\nret = gather_array(&pagelist, m.num, sizeof(xen_pfn_t), m.arr);\r\nif (ret)\r\ngoto out;\r\nif (list_empty(&pagelist)) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (version == 2) {\r\nif (clear_user(m.err, sizeof(int) * m.num)) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\n}\r\ndown_write(&mm->mmap_sem);\r\nvma = find_vma(mm, m.addr);\r\nif (!vma ||\r\nvma->vm_ops != &privcmd_vm_ops) {\r\nret = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nif (vma->vm_private_data == NULL) {\r\nif (m.addr != vma->vm_start ||\r\nm.addr + (nr_pages << PAGE_SHIFT) != vma->vm_end) {\r\nret = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nif (xen_feature(XENFEAT_auto_translated_physmap)) {\r\nret = alloc_empty_pages(vma, m.num);\r\nif (ret < 0)\r\ngoto out_unlock;\r\n} else\r\nvma->vm_private_data = PRIV_VMA_LOCKED;\r\n} else {\r\nif (m.addr < vma->vm_start ||\r\nm.addr + (nr_pages << PAGE_SHIFT) > vma->vm_end) {\r\nret = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nif (privcmd_vma_range_is_mapped(vma, m.addr, nr_pages)) {\r\nret = -EINVAL;\r\ngoto out_unlock;\r\n}\r\n}\r\nstate.domain = m.dom;\r\nstate.vma = vma;\r\nstate.va = m.addr;\r\nstate.index = 0;\r\nstate.global_error = 0;\r\nstate.version = version;\r\nBUG_ON(traverse_pages(m.num, sizeof(xen_pfn_t),\r\n&pagelist, mmap_batch_fn, &state));\r\nup_write(&mm->mmap_sem);\r\nif (state.global_error) {\r\nstate.user_mfn = (xen_pfn_t *)m.arr;\r\nstate.user_err = m.err;\r\nret = traverse_pages(m.num, sizeof(xen_pfn_t),\r\n&pagelist, mmap_return_errors, &state);\r\n} else\r\nret = 0;\r\nif ((ret == 0) && (state.global_error == -ENOENT))\r\nret = -ENOENT;\r\nout:\r\nfree_page_list(&pagelist);\r\nreturn ret;\r\nout_unlock:\r\nup_write(&mm->mmap_sem);\r\ngoto out;\r\n}\r\nstatic long privcmd_ioctl(struct file *file,\r\nunsigned int cmd, unsigned long data)\r\n{\r\nint ret = -ENOSYS;\r\nvoid __user *udata = (void __user *) data;\r\nswitch (cmd) {\r\ncase IOCTL_PRIVCMD_HYPERCALL:\r\nret = privcmd_ioctl_hypercall(udata);\r\nbreak;\r\ncase IOCTL_PRIVCMD_MMAP:\r\nret = privcmd_ioctl_mmap(udata);\r\nbreak;\r\ncase IOCTL_PRIVCMD_MMAPBATCH:\r\nret = privcmd_ioctl_mmap_batch(udata, 1);\r\nbreak;\r\ncase IOCTL_PRIVCMD_MMAPBATCH_V2:\r\nret = privcmd_ioctl_mmap_batch(udata, 2);\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic void privcmd_close(struct vm_area_struct *vma)\r\n{\r\nstruct page **pages = vma->vm_private_data;\r\nint numpgs = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\r\nint rc;\r\nif (!xen_feature(XENFEAT_auto_translated_physmap) || !numpgs || !pages)\r\nreturn;\r\nrc = xen_unmap_domain_mfn_range(vma, numpgs, pages);\r\nif (rc == 0)\r\nfree_xenballooned_pages(numpgs, pages);\r\nelse\r\npr_crit("unable to unmap MFN range: leaking %d pages. rc=%d\n",\r\nnumpgs, rc);\r\nkfree(pages);\r\n}\r\nstatic int privcmd_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nprintk(KERN_DEBUG "privcmd_fault: vma=%p %lx-%lx, pgoff=%lx, uv=%p\n",\r\nvma, vma->vm_start, vma->vm_end,\r\nvmf->pgoff, vmf->virtual_address);\r\nreturn VM_FAULT_SIGBUS;\r\n}\r\nstatic int privcmd_mmap(struct file *file, struct vm_area_struct *vma)\r\n{\r\nvma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTCOPY |\r\nVM_DONTEXPAND | VM_DONTDUMP;\r\nvma->vm_ops = &privcmd_vm_ops;\r\nvma->vm_private_data = NULL;\r\nreturn 0;\r\n}\r\nstatic int is_mapped_fn(pte_t *pte, struct page *pmd_page,\r\nunsigned long addr, void *data)\r\n{\r\nreturn pte_none(*pte) ? 0 : -EBUSY;\r\n}\r\nstatic int privcmd_vma_range_is_mapped(\r\nstruct vm_area_struct *vma,\r\nunsigned long addr,\r\nunsigned long nr_pages)\r\n{\r\nreturn apply_to_page_range(vma->vm_mm, addr, nr_pages << PAGE_SHIFT,\r\nis_mapped_fn, NULL) != 0;\r\n}\r\nstatic int __init privcmd_init(void)\r\n{\r\nint err;\r\nif (!xen_domain())\r\nreturn -ENODEV;\r\nerr = misc_register(&privcmd_dev);\r\nif (err != 0) {\r\npr_err("Could not register Xen privcmd device\n");\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __exit privcmd_exit(void)\r\n{\r\nmisc_deregister(&privcmd_dev);\r\n}
