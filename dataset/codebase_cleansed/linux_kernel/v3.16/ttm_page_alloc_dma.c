static void ttm_pool_kobj_release(struct kobject *kobj)\r\n{\r\nstruct ttm_pool_manager *m =\r\ncontainer_of(kobj, struct ttm_pool_manager, kobj);\r\nkfree(m);\r\n}\r\nstatic ssize_t ttm_pool_store(struct kobject *kobj, struct attribute *attr,\r\nconst char *buffer, size_t size)\r\n{\r\nstruct ttm_pool_manager *m =\r\ncontainer_of(kobj, struct ttm_pool_manager, kobj);\r\nint chars;\r\nunsigned val;\r\nchars = sscanf(buffer, "%u", &val);\r\nif (chars == 0)\r\nreturn size;\r\nval = val / (PAGE_SIZE >> 10);\r\nif (attr == &ttm_page_pool_max)\r\nm->options.max_size = val;\r\nelse if (attr == &ttm_page_pool_small)\r\nm->options.small = val;\r\nelse if (attr == &ttm_page_pool_alloc_size) {\r\nif (val > NUM_PAGES_TO_ALLOC*8) {\r\npr_err("Setting allocation size to %lu is not allowed. Recommended size is %lu\n",\r\nNUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 7),\r\nNUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 10));\r\nreturn size;\r\n} else if (val > NUM_PAGES_TO_ALLOC) {\r\npr_warn("Setting allocation size to larger than %lu is not recommended\n",\r\nNUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 10));\r\n}\r\nm->options.alloc_size = val;\r\n}\r\nreturn size;\r\n}\r\nstatic ssize_t ttm_pool_show(struct kobject *kobj, struct attribute *attr,\r\nchar *buffer)\r\n{\r\nstruct ttm_pool_manager *m =\r\ncontainer_of(kobj, struct ttm_pool_manager, kobj);\r\nunsigned val = 0;\r\nif (attr == &ttm_page_pool_max)\r\nval = m->options.max_size;\r\nelse if (attr == &ttm_page_pool_small)\r\nval = m->options.small;\r\nelse if (attr == &ttm_page_pool_alloc_size)\r\nval = m->options.alloc_size;\r\nval = val * (PAGE_SIZE >> 10);\r\nreturn snprintf(buffer, PAGE_SIZE, "%u\n", val);\r\n}\r\nstatic int set_pages_array_wb(struct page **pages, int addrinarray)\r\n{\r\n#ifdef TTM_HAS_AGP\r\nint i;\r\nfor (i = 0; i < addrinarray; i++)\r\nunmap_page_from_agp(pages[i]);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int set_pages_array_wc(struct page **pages, int addrinarray)\r\n{\r\n#ifdef TTM_HAS_AGP\r\nint i;\r\nfor (i = 0; i < addrinarray; i++)\r\nmap_page_into_agp(pages[i]);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int set_pages_array_uc(struct page **pages, int addrinarray)\r\n{\r\n#ifdef TTM_HAS_AGP\r\nint i;\r\nfor (i = 0; i < addrinarray; i++)\r\nmap_page_into_agp(pages[i]);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int ttm_set_pages_caching(struct dma_pool *pool,\r\nstruct page **pages, unsigned cpages)\r\n{\r\nint r = 0;\r\nif (pool->type & IS_UC) {\r\nr = set_pages_array_uc(pages, cpages);\r\nif (r)\r\npr_err("%s: Failed to set %d pages to uc!\n",\r\npool->dev_name, cpages);\r\n}\r\nif (pool->type & IS_WC) {\r\nr = set_pages_array_wc(pages, cpages);\r\nif (r)\r\npr_err("%s: Failed to set %d pages to wc!\n",\r\npool->dev_name, cpages);\r\n}\r\nreturn r;\r\n}\r\nstatic void __ttm_dma_free_page(struct dma_pool *pool, struct dma_page *d_page)\r\n{\r\ndma_addr_t dma = d_page->dma;\r\ndma_free_coherent(pool->dev, pool->size, d_page->vaddr, dma);\r\nkfree(d_page);\r\nd_page = NULL;\r\n}\r\nstatic struct dma_page *__ttm_dma_alloc_page(struct dma_pool *pool)\r\n{\r\nstruct dma_page *d_page;\r\nd_page = kmalloc(sizeof(struct dma_page), GFP_KERNEL);\r\nif (!d_page)\r\nreturn NULL;\r\nd_page->vaddr = dma_alloc_coherent(pool->dev, pool->size,\r\n&d_page->dma,\r\npool->gfp_flags);\r\nif (d_page->vaddr)\r\nd_page->p = virt_to_page(d_page->vaddr);\r\nelse {\r\nkfree(d_page);\r\nd_page = NULL;\r\n}\r\nreturn d_page;\r\n}\r\nstatic enum pool_type ttm_to_type(int flags, enum ttm_caching_state cstate)\r\n{\r\nenum pool_type type = IS_UNDEFINED;\r\nif (flags & TTM_PAGE_FLAG_DMA32)\r\ntype |= IS_DMA32;\r\nif (cstate == tt_cached)\r\ntype |= IS_CACHED;\r\nelse if (cstate == tt_uncached)\r\ntype |= IS_UC;\r\nelse\r\ntype |= IS_WC;\r\nreturn type;\r\n}\r\nstatic void ttm_pool_update_free_locked(struct dma_pool *pool,\r\nunsigned freed_pages)\r\n{\r\npool->npages_free -= freed_pages;\r\npool->nfrees += freed_pages;\r\n}\r\nstatic void ttm_dma_pages_put(struct dma_pool *pool, struct list_head *d_pages,\r\nstruct page *pages[], unsigned npages)\r\n{\r\nstruct dma_page *d_page, *tmp;\r\nif (npages && !(pool->type & IS_CACHED) &&\r\nset_pages_array_wb(pages, npages))\r\npr_err("%s: Failed to set %d pages to wb!\n",\r\npool->dev_name, npages);\r\nlist_for_each_entry_safe(d_page, tmp, d_pages, page_list) {\r\nlist_del(&d_page->page_list);\r\n__ttm_dma_free_page(pool, d_page);\r\n}\r\n}\r\nstatic void ttm_dma_page_put(struct dma_pool *pool, struct dma_page *d_page)\r\n{\r\nif (!(pool->type & IS_CACHED) && set_pages_array_wb(&d_page->p, 1))\r\npr_err("%s: Failed to set %d pages to wb!\n",\r\npool->dev_name, 1);\r\nlist_del(&d_page->page_list);\r\n__ttm_dma_free_page(pool, d_page);\r\n}\r\nstatic unsigned ttm_dma_page_pool_free(struct dma_pool *pool, unsigned nr_free)\r\n{\r\nunsigned long irq_flags;\r\nstruct dma_page *dma_p, *tmp;\r\nstruct page **pages_to_free;\r\nstruct list_head d_pages;\r\nunsigned freed_pages = 0,\r\nnpages_to_free = nr_free;\r\nif (NUM_PAGES_TO_ALLOC < nr_free)\r\nnpages_to_free = NUM_PAGES_TO_ALLOC;\r\n#if 0\r\nif (nr_free > 1) {\r\npr_debug("%s: (%s:%d) Attempting to free %d (%d) pages\n",\r\npool->dev_name, pool->name, current->pid,\r\nnpages_to_free, nr_free);\r\n}\r\n#endif\r\npages_to_free = kmalloc(npages_to_free * sizeof(struct page *),\r\nGFP_KERNEL);\r\nif (!pages_to_free) {\r\npr_err("%s: Failed to allocate memory for pool free operation\n",\r\npool->dev_name);\r\nreturn 0;\r\n}\r\nINIT_LIST_HEAD(&d_pages);\r\nrestart:\r\nspin_lock_irqsave(&pool->lock, irq_flags);\r\nlist_for_each_entry_safe_reverse(dma_p, tmp, &pool->free_list,\r\npage_list) {\r\nif (freed_pages >= npages_to_free)\r\nbreak;\r\nlist_move(&dma_p->page_list, &d_pages);\r\npages_to_free[freed_pages++] = dma_p->p;\r\nif (freed_pages >= NUM_PAGES_TO_ALLOC) {\r\nttm_pool_update_free_locked(pool, freed_pages);\r\nspin_unlock_irqrestore(&pool->lock, irq_flags);\r\nttm_dma_pages_put(pool, &d_pages, pages_to_free,\r\nfreed_pages);\r\nINIT_LIST_HEAD(&d_pages);\r\nif (likely(nr_free != FREE_ALL_PAGES))\r\nnr_free -= freed_pages;\r\nif (NUM_PAGES_TO_ALLOC >= nr_free)\r\nnpages_to_free = nr_free;\r\nelse\r\nnpages_to_free = NUM_PAGES_TO_ALLOC;\r\nfreed_pages = 0;\r\nif (nr_free)\r\ngoto restart;\r\ngoto out;\r\n}\r\n}\r\nif (freed_pages) {\r\nttm_pool_update_free_locked(pool, freed_pages);\r\nnr_free -= freed_pages;\r\n}\r\nspin_unlock_irqrestore(&pool->lock, irq_flags);\r\nif (freed_pages)\r\nttm_dma_pages_put(pool, &d_pages, pages_to_free, freed_pages);\r\nout:\r\nkfree(pages_to_free);\r\nreturn nr_free;\r\n}\r\nstatic void ttm_dma_free_pool(struct device *dev, enum pool_type type)\r\n{\r\nstruct device_pools *p;\r\nstruct dma_pool *pool;\r\nif (!dev)\r\nreturn;\r\nmutex_lock(&_manager->lock);\r\nlist_for_each_entry_reverse(p, &_manager->pools, pools) {\r\nif (p->dev != dev)\r\ncontinue;\r\npool = p->pool;\r\nif (pool->type != type)\r\ncontinue;\r\nlist_del(&p->pools);\r\nkfree(p);\r\n_manager->npools--;\r\nbreak;\r\n}\r\nlist_for_each_entry_reverse(pool, &dev->dma_pools, pools) {\r\nif (pool->type != type)\r\ncontinue;\r\nttm_dma_page_pool_free(pool, FREE_ALL_PAGES);\r\nWARN_ON(((pool->npages_in_use + pool->npages_free) != 0));\r\nlist_del(&pool->pools);\r\nkfree(pool);\r\nbreak;\r\n}\r\nmutex_unlock(&_manager->lock);\r\n}\r\nstatic void ttm_dma_pool_release(struct device *dev, void *res)\r\n{\r\nstruct dma_pool *pool = *(struct dma_pool **)res;\r\nif (pool)\r\nttm_dma_free_pool(dev, pool->type);\r\n}\r\nstatic int ttm_dma_pool_match(struct device *dev, void *res, void *match_data)\r\n{\r\nreturn *(struct dma_pool **)res == match_data;\r\n}\r\nstatic struct dma_pool *ttm_dma_pool_init(struct device *dev, gfp_t flags,\r\nenum pool_type type)\r\n{\r\nchar *n[] = {"wc", "uc", "cached", " dma32", "unknown",};\r\nenum pool_type t[] = {IS_WC, IS_UC, IS_CACHED, IS_DMA32, IS_UNDEFINED};\r\nstruct device_pools *sec_pool = NULL;\r\nstruct dma_pool *pool = NULL, **ptr;\r\nunsigned i;\r\nint ret = -ENODEV;\r\nchar *p;\r\nif (!dev)\r\nreturn NULL;\r\nptr = devres_alloc(ttm_dma_pool_release, sizeof(*ptr), GFP_KERNEL);\r\nif (!ptr)\r\nreturn NULL;\r\nret = -ENOMEM;\r\npool = kmalloc_node(sizeof(struct dma_pool), GFP_KERNEL,\r\ndev_to_node(dev));\r\nif (!pool)\r\ngoto err_mem;\r\nsec_pool = kmalloc_node(sizeof(struct device_pools), GFP_KERNEL,\r\ndev_to_node(dev));\r\nif (!sec_pool)\r\ngoto err_mem;\r\nINIT_LIST_HEAD(&sec_pool->pools);\r\nsec_pool->dev = dev;\r\nsec_pool->pool = pool;\r\nINIT_LIST_HEAD(&pool->free_list);\r\nINIT_LIST_HEAD(&pool->inuse_list);\r\nINIT_LIST_HEAD(&pool->pools);\r\nspin_lock_init(&pool->lock);\r\npool->dev = dev;\r\npool->npages_free = pool->npages_in_use = 0;\r\npool->nfrees = 0;\r\npool->gfp_flags = flags;\r\npool->size = PAGE_SIZE;\r\npool->type = type;\r\npool->nrefills = 0;\r\np = pool->name;\r\nfor (i = 0; i < 5; i++) {\r\nif (type & t[i]) {\r\np += snprintf(p, sizeof(pool->name) - (p - pool->name),\r\n"%s", n[i]);\r\n}\r\n}\r\n*p = 0;\r\nsnprintf(pool->dev_name, sizeof(pool->dev_name), "%s %s",\r\ndev_driver_string(dev), dev_name(dev));\r\nmutex_lock(&_manager->lock);\r\nlist_add(&sec_pool->pools, &_manager->pools);\r\n_manager->npools++;\r\nlist_add(&pool->pools, &dev->dma_pools);\r\nmutex_unlock(&_manager->lock);\r\n*ptr = pool;\r\ndevres_add(dev, ptr);\r\nreturn pool;\r\nerr_mem:\r\ndevres_free(ptr);\r\nkfree(sec_pool);\r\nkfree(pool);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic struct dma_pool *ttm_dma_find_pool(struct device *dev,\r\nenum pool_type type)\r\n{\r\nstruct dma_pool *pool, *tmp, *found = NULL;\r\nif (type == IS_UNDEFINED)\r\nreturn found;\r\nlist_for_each_entry_safe(pool, tmp, &dev->dma_pools, pools) {\r\nif (pool->type != type)\r\ncontinue;\r\nfound = pool;\r\nbreak;\r\n}\r\nreturn found;\r\n}\r\nstatic void ttm_dma_handle_caching_state_failure(struct dma_pool *pool,\r\nstruct list_head *d_pages,\r\nstruct page **failed_pages,\r\nunsigned cpages)\r\n{\r\nstruct dma_page *d_page, *tmp;\r\nstruct page *p;\r\nunsigned i = 0;\r\np = failed_pages[0];\r\nif (!p)\r\nreturn;\r\nlist_for_each_entry_safe(d_page, tmp, d_pages, page_list) {\r\nif (d_page->p != p)\r\ncontinue;\r\nlist_del(&d_page->page_list);\r\n__ttm_dma_free_page(pool, d_page);\r\nif (++i < cpages)\r\np = failed_pages[i];\r\nelse\r\nbreak;\r\n}\r\n}\r\nstatic int ttm_dma_pool_alloc_new_pages(struct dma_pool *pool,\r\nstruct list_head *d_pages,\r\nunsigned count)\r\n{\r\nstruct page **caching_array;\r\nstruct dma_page *dma_p;\r\nstruct page *p;\r\nint r = 0;\r\nunsigned i, cpages;\r\nunsigned max_cpages = min(count,\r\n(unsigned)(PAGE_SIZE/sizeof(struct page *)));\r\ncaching_array = kmalloc(max_cpages*sizeof(struct page *), GFP_KERNEL);\r\nif (!caching_array) {\r\npr_err("%s: Unable to allocate table for new pages\n",\r\npool->dev_name);\r\nreturn -ENOMEM;\r\n}\r\nif (count > 1) {\r\npr_debug("%s: (%s:%d) Getting %d pages\n",\r\npool->dev_name, pool->name, current->pid, count);\r\n}\r\nfor (i = 0, cpages = 0; i < count; ++i) {\r\ndma_p = __ttm_dma_alloc_page(pool);\r\nif (!dma_p) {\r\npr_err("%s: Unable to get page %u\n",\r\npool->dev_name, i);\r\nif (cpages) {\r\nr = ttm_set_pages_caching(pool, caching_array,\r\ncpages);\r\nif (r)\r\nttm_dma_handle_caching_state_failure(\r\npool, d_pages, caching_array,\r\ncpages);\r\n}\r\nr = -ENOMEM;\r\ngoto out;\r\n}\r\np = dma_p->p;\r\n#ifdef CONFIG_HIGHMEM\r\nif (!PageHighMem(p))\r\n#endif\r\n{\r\ncaching_array[cpages++] = p;\r\nif (cpages == max_cpages) {\r\nr = ttm_set_pages_caching(pool, caching_array,\r\ncpages);\r\nif (r) {\r\nttm_dma_handle_caching_state_failure(\r\npool, d_pages, caching_array,\r\ncpages);\r\ngoto out;\r\n}\r\ncpages = 0;\r\n}\r\n}\r\nlist_add(&dma_p->page_list, d_pages);\r\n}\r\nif (cpages) {\r\nr = ttm_set_pages_caching(pool, caching_array, cpages);\r\nif (r)\r\nttm_dma_handle_caching_state_failure(pool, d_pages,\r\ncaching_array, cpages);\r\n}\r\nout:\r\nkfree(caching_array);\r\nreturn r;\r\n}\r\nstatic int ttm_dma_page_pool_fill_locked(struct dma_pool *pool,\r\nunsigned long *irq_flags)\r\n{\r\nunsigned count = _manager->options.small;\r\nint r = pool->npages_free;\r\nif (count > pool->npages_free) {\r\nstruct list_head d_pages;\r\nINIT_LIST_HEAD(&d_pages);\r\nspin_unlock_irqrestore(&pool->lock, *irq_flags);\r\nr = ttm_dma_pool_alloc_new_pages(pool, &d_pages, count);\r\nspin_lock_irqsave(&pool->lock, *irq_flags);\r\nif (!r) {\r\nlist_splice(&d_pages, &pool->free_list);\r\n++pool->nrefills;\r\npool->npages_free += count;\r\nr = count;\r\n} else {\r\nstruct dma_page *d_page;\r\nunsigned cpages = 0;\r\npr_err("%s: Failed to fill %s pool (r:%d)!\n",\r\npool->dev_name, pool->name, r);\r\nlist_for_each_entry(d_page, &d_pages, page_list) {\r\ncpages++;\r\n}\r\nlist_splice_tail(&d_pages, &pool->free_list);\r\npool->npages_free += cpages;\r\nr = cpages;\r\n}\r\n}\r\nreturn r;\r\n}\r\nstatic int ttm_dma_pool_get_pages(struct dma_pool *pool,\r\nstruct ttm_dma_tt *ttm_dma,\r\nunsigned index)\r\n{\r\nstruct dma_page *d_page;\r\nstruct ttm_tt *ttm = &ttm_dma->ttm;\r\nunsigned long irq_flags;\r\nint count, r = -ENOMEM;\r\nspin_lock_irqsave(&pool->lock, irq_flags);\r\ncount = ttm_dma_page_pool_fill_locked(pool, &irq_flags);\r\nif (count) {\r\nd_page = list_first_entry(&pool->free_list, struct dma_page, page_list);\r\nttm->pages[index] = d_page->p;\r\nttm_dma->dma_address[index] = d_page->dma;\r\nlist_move_tail(&d_page->page_list, &ttm_dma->pages_list);\r\nr = 0;\r\npool->npages_in_use += 1;\r\npool->npages_free -= 1;\r\n}\r\nspin_unlock_irqrestore(&pool->lock, irq_flags);\r\nreturn r;\r\n}\r\nint ttm_dma_populate(struct ttm_dma_tt *ttm_dma, struct device *dev)\r\n{\r\nstruct ttm_tt *ttm = &ttm_dma->ttm;\r\nstruct ttm_mem_global *mem_glob = ttm->glob->mem_glob;\r\nstruct dma_pool *pool;\r\nenum pool_type type;\r\nunsigned i;\r\ngfp_t gfp_flags;\r\nint ret;\r\nif (ttm->state != tt_unpopulated)\r\nreturn 0;\r\ntype = ttm_to_type(ttm->page_flags, ttm->caching_state);\r\nif (ttm->page_flags & TTM_PAGE_FLAG_DMA32)\r\ngfp_flags = GFP_USER | GFP_DMA32;\r\nelse\r\ngfp_flags = GFP_HIGHUSER;\r\nif (ttm->page_flags & TTM_PAGE_FLAG_ZERO_ALLOC)\r\ngfp_flags |= __GFP_ZERO;\r\npool = ttm_dma_find_pool(dev, type);\r\nif (!pool) {\r\npool = ttm_dma_pool_init(dev, gfp_flags, type);\r\nif (IS_ERR_OR_NULL(pool)) {\r\nreturn -ENOMEM;\r\n}\r\n}\r\nINIT_LIST_HEAD(&ttm_dma->pages_list);\r\nfor (i = 0; i < ttm->num_pages; ++i) {\r\nret = ttm_dma_pool_get_pages(pool, ttm_dma, i);\r\nif (ret != 0) {\r\nttm_dma_unpopulate(ttm_dma, dev);\r\nreturn -ENOMEM;\r\n}\r\nret = ttm_mem_global_alloc_page(mem_glob, ttm->pages[i],\r\nfalse, false);\r\nif (unlikely(ret != 0)) {\r\nttm_dma_unpopulate(ttm_dma, dev);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nif (unlikely(ttm->page_flags & TTM_PAGE_FLAG_SWAPPED)) {\r\nret = ttm_tt_swapin(ttm);\r\nif (unlikely(ret != 0)) {\r\nttm_dma_unpopulate(ttm_dma, dev);\r\nreturn ret;\r\n}\r\n}\r\nttm->state = tt_unbound;\r\nreturn 0;\r\n}\r\nvoid ttm_dma_unpopulate(struct ttm_dma_tt *ttm_dma, struct device *dev)\r\n{\r\nstruct ttm_tt *ttm = &ttm_dma->ttm;\r\nstruct dma_pool *pool;\r\nstruct dma_page *d_page, *next;\r\nenum pool_type type;\r\nbool is_cached = false;\r\nunsigned count = 0, i, npages = 0;\r\nunsigned long irq_flags;\r\ntype = ttm_to_type(ttm->page_flags, ttm->caching_state);\r\npool = ttm_dma_find_pool(dev, type);\r\nif (!pool)\r\nreturn;\r\nis_cached = (ttm_dma_find_pool(pool->dev,\r\nttm_to_type(ttm->page_flags, tt_cached)) == pool);\r\nlist_for_each_entry(d_page, &ttm_dma->pages_list, page_list) {\r\nttm->pages[count] = d_page->p;\r\ncount++;\r\n}\r\nspin_lock_irqsave(&pool->lock, irq_flags);\r\npool->npages_in_use -= count;\r\nif (is_cached) {\r\npool->nfrees += count;\r\n} else {\r\npool->npages_free += count;\r\nlist_splice(&ttm_dma->pages_list, &pool->free_list);\r\nnpages = count;\r\nif (pool->npages_free > _manager->options.max_size) {\r\nnpages = pool->npages_free - _manager->options.max_size;\r\nif (npages < NUM_PAGES_TO_ALLOC)\r\nnpages = NUM_PAGES_TO_ALLOC;\r\n}\r\n}\r\nspin_unlock_irqrestore(&pool->lock, irq_flags);\r\nif (is_cached) {\r\nlist_for_each_entry_safe(d_page, next, &ttm_dma->pages_list, page_list) {\r\nttm_mem_global_free_page(ttm->glob->mem_glob,\r\nd_page->p);\r\nttm_dma_page_put(pool, d_page);\r\n}\r\n} else {\r\nfor (i = 0; i < count; i++) {\r\nttm_mem_global_free_page(ttm->glob->mem_glob,\r\nttm->pages[i]);\r\n}\r\n}\r\nINIT_LIST_HEAD(&ttm_dma->pages_list);\r\nfor (i = 0; i < ttm->num_pages; i++) {\r\nttm->pages[i] = NULL;\r\nttm_dma->dma_address[i] = 0;\r\n}\r\nif (npages)\r\nttm_dma_page_pool_free(pool, npages);\r\nttm->state = tt_unpopulated;\r\n}\r\nstatic unsigned long\r\nttm_dma_pool_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nstatic atomic_t start_pool = ATOMIC_INIT(0);\r\nunsigned idx = 0;\r\nunsigned pool_offset = atomic_add_return(1, &start_pool);\r\nunsigned shrink_pages = sc->nr_to_scan;\r\nstruct device_pools *p;\r\nunsigned long freed = 0;\r\nif (list_empty(&_manager->pools))\r\nreturn SHRINK_STOP;\r\nmutex_lock(&_manager->lock);\r\npool_offset = pool_offset % _manager->npools;\r\nlist_for_each_entry(p, &_manager->pools, pools) {\r\nunsigned nr_free;\r\nif (!p->dev)\r\ncontinue;\r\nif (shrink_pages == 0)\r\nbreak;\r\nif (++idx < pool_offset)\r\ncontinue;\r\nnr_free = shrink_pages;\r\nshrink_pages = ttm_dma_page_pool_free(p->pool, nr_free);\r\nfreed += nr_free - shrink_pages;\r\npr_debug("%s: (%s:%d) Asked to shrink %d, have %d more to go\n",\r\np->pool->dev_name, p->pool->name, current->pid,\r\nnr_free, shrink_pages);\r\n}\r\nmutex_unlock(&_manager->lock);\r\nreturn freed;\r\n}\r\nstatic unsigned long\r\nttm_dma_pool_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nstruct device_pools *p;\r\nunsigned long count = 0;\r\nmutex_lock(&_manager->lock);\r\nlist_for_each_entry(p, &_manager->pools, pools)\r\ncount += p->pool->npages_free;\r\nmutex_unlock(&_manager->lock);\r\nreturn count;\r\n}\r\nstatic void ttm_dma_pool_mm_shrink_init(struct ttm_pool_manager *manager)\r\n{\r\nmanager->mm_shrink.count_objects = ttm_dma_pool_shrink_count;\r\nmanager->mm_shrink.scan_objects = &ttm_dma_pool_shrink_scan;\r\nmanager->mm_shrink.seeks = 1;\r\nregister_shrinker(&manager->mm_shrink);\r\n}\r\nstatic void ttm_dma_pool_mm_shrink_fini(struct ttm_pool_manager *manager)\r\n{\r\nunregister_shrinker(&manager->mm_shrink);\r\n}\r\nint ttm_dma_page_alloc_init(struct ttm_mem_global *glob, unsigned max_pages)\r\n{\r\nint ret = -ENOMEM;\r\nWARN_ON(_manager);\r\npr_info("Initializing DMA pool allocator\n");\r\n_manager = kzalloc(sizeof(*_manager), GFP_KERNEL);\r\nif (!_manager)\r\ngoto err;\r\nmutex_init(&_manager->lock);\r\nINIT_LIST_HEAD(&_manager->pools);\r\n_manager->options.max_size = max_pages;\r\n_manager->options.small = SMALL_ALLOCATION;\r\n_manager->options.alloc_size = NUM_PAGES_TO_ALLOC;\r\nret = kobject_init_and_add(&_manager->kobj, &ttm_pool_kobj_type,\r\n&glob->kobj, "dma_pool");\r\nif (unlikely(ret != 0)) {\r\nkobject_put(&_manager->kobj);\r\ngoto err;\r\n}\r\nttm_dma_pool_mm_shrink_init(_manager);\r\nreturn 0;\r\nerr:\r\nreturn ret;\r\n}\r\nvoid ttm_dma_page_alloc_fini(void)\r\n{\r\nstruct device_pools *p, *t;\r\npr_info("Finalizing DMA pool allocator\n");\r\nttm_dma_pool_mm_shrink_fini(_manager);\r\nlist_for_each_entry_safe_reverse(p, t, &_manager->pools, pools) {\r\ndev_dbg(p->dev, "(%s:%d) Freeing.\n", p->pool->name,\r\ncurrent->pid);\r\nWARN_ON(devres_destroy(p->dev, ttm_dma_pool_release,\r\nttm_dma_pool_match, p->pool));\r\nttm_dma_free_pool(p->dev, p->pool->type);\r\n}\r\nkobject_put(&_manager->kobj);\r\n_manager = NULL;\r\n}\r\nint ttm_dma_page_alloc_debugfs(struct seq_file *m, void *data)\r\n{\r\nstruct device_pools *p;\r\nstruct dma_pool *pool = NULL;\r\nchar *h[] = {"pool", "refills", "pages freed", "inuse", "available",\r\n"name", "virt", "busaddr"};\r\nif (!_manager) {\r\nseq_printf(m, "No pool allocator running.\n");\r\nreturn 0;\r\n}\r\nseq_printf(m, "%13s %12s %13s %8s %8s %8s\n",\r\nh[0], h[1], h[2], h[3], h[4], h[5]);\r\nmutex_lock(&_manager->lock);\r\nlist_for_each_entry(p, &_manager->pools, pools) {\r\nstruct device *dev = p->dev;\r\nif (!dev)\r\ncontinue;\r\npool = p->pool;\r\nseq_printf(m, "%13s %12ld %13ld %8d %8d %8s\n",\r\npool->name, pool->nrefills,\r\npool->nfrees, pool->npages_in_use,\r\npool->npages_free,\r\npool->dev_name);\r\n}\r\nmutex_unlock(&_manager->lock);\r\nreturn 0;\r\n}
