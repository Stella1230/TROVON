static unsigned int vpe_shift(void)\r\n{\r\nif (num_possible_cpus() > 1)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic unsigned int counters_total_to_per_cpu(unsigned int counters)\r\n{\r\nreturn counters >> vpe_shift();\r\n}\r\nstatic unsigned int mipsxx_pmu_swizzle_perf_idx(unsigned int idx)\r\n{\r\nif (vpe_id() == 1)\r\nidx = (idx + 2) & 3;\r\nreturn idx;\r\n}\r\nstatic u64 mipsxx_pmu_read_counter(unsigned int idx)\r\n{\r\nidx = mipsxx_pmu_swizzle_perf_idx(idx);\r\nswitch (idx) {\r\ncase 0:\r\nreturn (u32)read_c0_perfcntr0();\r\ncase 1:\r\nreturn (u32)read_c0_perfcntr1();\r\ncase 2:\r\nreturn (u32)read_c0_perfcntr2();\r\ncase 3:\r\nreturn (u32)read_c0_perfcntr3();\r\ndefault:\r\nWARN_ONCE(1, "Invalid performance counter number (%d)\n", idx);\r\nreturn 0;\r\n}\r\n}\r\nstatic u64 mipsxx_pmu_read_counter_64(unsigned int idx)\r\n{\r\nidx = mipsxx_pmu_swizzle_perf_idx(idx);\r\nswitch (idx) {\r\ncase 0:\r\nreturn read_c0_perfcntr0_64();\r\ncase 1:\r\nreturn read_c0_perfcntr1_64();\r\ncase 2:\r\nreturn read_c0_perfcntr2_64();\r\ncase 3:\r\nreturn read_c0_perfcntr3_64();\r\ndefault:\r\nWARN_ONCE(1, "Invalid performance counter number (%d)\n", idx);\r\nreturn 0;\r\n}\r\n}\r\nstatic void mipsxx_pmu_write_counter(unsigned int idx, u64 val)\r\n{\r\nidx = mipsxx_pmu_swizzle_perf_idx(idx);\r\nswitch (idx) {\r\ncase 0:\r\nwrite_c0_perfcntr0(val);\r\nreturn;\r\ncase 1:\r\nwrite_c0_perfcntr1(val);\r\nreturn;\r\ncase 2:\r\nwrite_c0_perfcntr2(val);\r\nreturn;\r\ncase 3:\r\nwrite_c0_perfcntr3(val);\r\nreturn;\r\n}\r\n}\r\nstatic void mipsxx_pmu_write_counter_64(unsigned int idx, u64 val)\r\n{\r\nidx = mipsxx_pmu_swizzle_perf_idx(idx);\r\nswitch (idx) {\r\ncase 0:\r\nwrite_c0_perfcntr0_64(val);\r\nreturn;\r\ncase 1:\r\nwrite_c0_perfcntr1_64(val);\r\nreturn;\r\ncase 2:\r\nwrite_c0_perfcntr2_64(val);\r\nreturn;\r\ncase 3:\r\nwrite_c0_perfcntr3_64(val);\r\nreturn;\r\n}\r\n}\r\nstatic unsigned int mipsxx_pmu_read_control(unsigned int idx)\r\n{\r\nidx = mipsxx_pmu_swizzle_perf_idx(idx);\r\nswitch (idx) {\r\ncase 0:\r\nreturn read_c0_perfctrl0();\r\ncase 1:\r\nreturn read_c0_perfctrl1();\r\ncase 2:\r\nreturn read_c0_perfctrl2();\r\ncase 3:\r\nreturn read_c0_perfctrl3();\r\ndefault:\r\nWARN_ONCE(1, "Invalid performance counter number (%d)\n", idx);\r\nreturn 0;\r\n}\r\n}\r\nstatic void mipsxx_pmu_write_control(unsigned int idx, unsigned int val)\r\n{\r\nidx = mipsxx_pmu_swizzle_perf_idx(idx);\r\nswitch (idx) {\r\ncase 0:\r\nwrite_c0_perfctrl0(val);\r\nreturn;\r\ncase 1:\r\nwrite_c0_perfctrl1(val);\r\nreturn;\r\ncase 2:\r\nwrite_c0_perfctrl2(val);\r\nreturn;\r\ncase 3:\r\nwrite_c0_perfctrl3(val);\r\nreturn;\r\n}\r\n}\r\nstatic int mipsxx_pmu_alloc_counter(struct cpu_hw_events *cpuc,\r\nstruct hw_perf_event *hwc)\r\n{\r\nint i;\r\nunsigned long cntr_mask = (hwc->event_base >> 8) & 0xffff;\r\nfor (i = mipspmu.num_counters - 1; i >= 0; i--) {\r\nif (test_bit(i, &cntr_mask) &&\r\n!test_and_set_bit(i, cpuc->used_mask))\r\nreturn i;\r\n}\r\nreturn -EAGAIN;\r\n}\r\nstatic void mipsxx_pmu_enable_event(struct hw_perf_event *evt, int idx)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nWARN_ON(idx < 0 || idx >= mipspmu.num_counters);\r\ncpuc->saved_ctrl[idx] = M_PERFCTL_EVENT(evt->event_base & 0xff) |\r\n(evt->config_base & M_PERFCTL_CONFIG_MASK) |\r\nM_PERFCTL_INTERRUPT_ENABLE;\r\nif (IS_ENABLED(CONFIG_CPU_BMIPS5000))\r\ncpuc->saved_ctrl[idx] |=\r\n(1 << (12 + vpe_id())) | M_PERFCTL_TC;\r\n}\r\nstatic void mipsxx_pmu_disable_event(int idx)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nunsigned long flags;\r\nWARN_ON(idx < 0 || idx >= mipspmu.num_counters);\r\nlocal_irq_save(flags);\r\ncpuc->saved_ctrl[idx] = mipsxx_pmu_read_control(idx) &\r\n~M_PERFCTL_COUNT_EVENT_WHENEVER;\r\nmipsxx_pmu_write_control(idx, cpuc->saved_ctrl[idx]);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int mipspmu_event_set_period(struct perf_event *event,\r\nstruct hw_perf_event *hwc,\r\nint idx)\r\n{\r\nu64 left = local64_read(&hwc->period_left);\r\nu64 period = hwc->sample_period;\r\nint ret = 0;\r\nif (unlikely((left + period) & (1ULL << 63))) {\r\nleft = period;\r\nlocal64_set(&hwc->period_left, left);\r\nhwc->last_period = period;\r\nret = 1;\r\n} else if (unlikely((left + period) <= period)) {\r\nleft += period;\r\nlocal64_set(&hwc->period_left, left);\r\nhwc->last_period = period;\r\nret = 1;\r\n}\r\nif (left > mipspmu.max_period) {\r\nleft = mipspmu.max_period;\r\nlocal64_set(&hwc->period_left, left);\r\n}\r\nlocal64_set(&hwc->prev_count, mipspmu.overflow - left);\r\nmipspmu.write_counter(idx, mipspmu.overflow - left);\r\nperf_event_update_userpage(event);\r\nreturn ret;\r\n}\r\nstatic void mipspmu_event_update(struct perf_event *event,\r\nstruct hw_perf_event *hwc,\r\nint idx)\r\n{\r\nu64 prev_raw_count, new_raw_count;\r\nu64 delta;\r\nagain:\r\nprev_raw_count = local64_read(&hwc->prev_count);\r\nnew_raw_count = mipspmu.read_counter(idx);\r\nif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\r\nnew_raw_count) != prev_raw_count)\r\ngoto again;\r\ndelta = new_raw_count - prev_raw_count;\r\nlocal64_add(delta, &event->count);\r\nlocal64_sub(delta, &hwc->period_left);\r\n}\r\nstatic void mipspmu_start(struct perf_event *event, int flags)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (flags & PERF_EF_RELOAD)\r\nWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\r\nhwc->state = 0;\r\nmipspmu_event_set_period(event, hwc, hwc->idx);\r\nmipsxx_pmu_enable_event(hwc, hwc->idx);\r\n}\r\nstatic void mipspmu_stop(struct perf_event *event, int flags)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (!(hwc->state & PERF_HES_STOPPED)) {\r\nmipsxx_pmu_disable_event(hwc->idx);\r\nbarrier();\r\nmipspmu_event_update(event, hwc, hwc->idx);\r\nhwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\n}\r\n}\r\nstatic int mipspmu_add(struct perf_event *event, int flags)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx;\r\nint err = 0;\r\nperf_pmu_disable(event->pmu);\r\nidx = mipsxx_pmu_alloc_counter(cpuc, hwc);\r\nif (idx < 0) {\r\nerr = idx;\r\ngoto out;\r\n}\r\nevent->hw.idx = idx;\r\nmipsxx_pmu_disable_event(idx);\r\ncpuc->events[idx] = event;\r\nhwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\r\nif (flags & PERF_EF_START)\r\nmipspmu_start(event, PERF_EF_RELOAD);\r\nperf_event_update_userpage(event);\r\nout:\r\nperf_pmu_enable(event->pmu);\r\nreturn err;\r\n}\r\nstatic void mipspmu_del(struct perf_event *event, int flags)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint idx = hwc->idx;\r\nWARN_ON(idx < 0 || idx >= mipspmu.num_counters);\r\nmipspmu_stop(event, PERF_EF_UPDATE);\r\ncpuc->events[idx] = NULL;\r\nclear_bit(idx, cpuc->used_mask);\r\nperf_event_update_userpage(event);\r\n}\r\nstatic void mipspmu_read(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (hwc->idx < 0)\r\nreturn;\r\nmipspmu_event_update(event, hwc, hwc->idx);\r\n}\r\nstatic void mipspmu_enable(struct pmu *pmu)\r\n{\r\n#ifdef CONFIG_MIPS_PERF_SHARED_TC_COUNTERS\r\nwrite_unlock(&pmuint_rwlock);\r\n#endif\r\nresume_local_counters();\r\n}\r\nstatic void mipspmu_disable(struct pmu *pmu)\r\n{\r\npause_local_counters();\r\n#ifdef CONFIG_MIPS_PERF_SHARED_TC_COUNTERS\r\nwrite_lock(&pmuint_rwlock);\r\n#endif\r\n}\r\nstatic int mipspmu_get_irq(void)\r\n{\r\nint err;\r\nif (mipspmu.irq >= 0) {\r\nerr = request_irq(mipspmu.irq, mipsxx_pmu_handle_irq,\r\nIRQF_PERCPU | IRQF_NOBALANCING,\r\n"mips_perf_pmu", NULL);\r\nif (err) {\r\npr_warning("Unable to request IRQ%d for MIPS "\r\n"performance counters!\n", mipspmu.irq);\r\n}\r\n} else if (cp0_perfcount_irq < 0) {\r\nsave_perf_irq = perf_irq;\r\nperf_irq = mipsxx_pmu_handle_shared_irq;\r\nerr = 0;\r\n} else {\r\npr_warning("The platform hasn't properly defined its "\r\n"interrupt controller.\n");\r\nerr = -ENOENT;\r\n}\r\nreturn err;\r\n}\r\nstatic void mipspmu_free_irq(void)\r\n{\r\nif (mipspmu.irq >= 0)\r\nfree_irq(mipspmu.irq, NULL);\r\nelse if (cp0_perfcount_irq < 0)\r\nperf_irq = save_perf_irq;\r\n}\r\nstatic void hw_perf_event_destroy(struct perf_event *event)\r\n{\r\nif (atomic_dec_and_mutex_lock(&active_events,\r\n&pmu_reserve_mutex)) {\r\non_each_cpu(reset_counters,\r\n(void *)(long)mipspmu.num_counters, 1);\r\nmipspmu_free_irq();\r\nmutex_unlock(&pmu_reserve_mutex);\r\n}\r\n}\r\nstatic int mipspmu_event_init(struct perf_event *event)\r\n{\r\nint err = 0;\r\nif (has_branch_stack(event))\r\nreturn -EOPNOTSUPP;\r\nswitch (event->attr.type) {\r\ncase PERF_TYPE_RAW:\r\ncase PERF_TYPE_HARDWARE:\r\ncase PERF_TYPE_HW_CACHE:\r\nbreak;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\nif (event->cpu >= nr_cpumask_bits ||\r\n(event->cpu >= 0 && !cpu_online(event->cpu)))\r\nreturn -ENODEV;\r\nif (!atomic_inc_not_zero(&active_events)) {\r\nmutex_lock(&pmu_reserve_mutex);\r\nif (atomic_read(&active_events) == 0)\r\nerr = mipspmu_get_irq();\r\nif (!err)\r\natomic_inc(&active_events);\r\nmutex_unlock(&pmu_reserve_mutex);\r\n}\r\nif (err)\r\nreturn err;\r\nreturn __hw_perf_event_init(event);\r\n}\r\nstatic unsigned int mipspmu_perf_event_encode(const struct mips_perf_event *pev)\r\n{\r\n#ifdef CONFIG_MIPS_MT_SMP\r\nreturn ((unsigned int)pev->range << 24) |\r\n(pev->cntr_mask & 0xffff00) |\r\n(pev->event_id & 0xff);\r\n#else\r\nreturn (pev->cntr_mask & 0xffff00) |\r\n(pev->event_id & 0xff);\r\n#endif\r\n}\r\nstatic const struct mips_perf_event *mipspmu_map_general_event(int idx)\r\n{\r\nif ((*mipspmu.general_event_map)[idx].cntr_mask == 0)\r\nreturn ERR_PTR(-EOPNOTSUPP);\r\nreturn &(*mipspmu.general_event_map)[idx];\r\n}\r\nstatic const struct mips_perf_event *mipspmu_map_cache_event(u64 config)\r\n{\r\nunsigned int cache_type, cache_op, cache_result;\r\nconst struct mips_perf_event *pev;\r\ncache_type = (config >> 0) & 0xff;\r\nif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\r\nreturn ERR_PTR(-EINVAL);\r\ncache_op = (config >> 8) & 0xff;\r\nif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\r\nreturn ERR_PTR(-EINVAL);\r\ncache_result = (config >> 16) & 0xff;\r\nif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\r\nreturn ERR_PTR(-EINVAL);\r\npev = &((*mipspmu.cache_event_map)\r\n[cache_type]\r\n[cache_op]\r\n[cache_result]);\r\nif (pev->cntr_mask == 0)\r\nreturn ERR_PTR(-EOPNOTSUPP);\r\nreturn pev;\r\n}\r\nstatic int validate_group(struct perf_event *event)\r\n{\r\nstruct perf_event *sibling, *leader = event->group_leader;\r\nstruct cpu_hw_events fake_cpuc;\r\nmemset(&fake_cpuc, 0, sizeof(fake_cpuc));\r\nif (mipsxx_pmu_alloc_counter(&fake_cpuc, &leader->hw) < 0)\r\nreturn -EINVAL;\r\nlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\r\nif (mipsxx_pmu_alloc_counter(&fake_cpuc, &sibling->hw) < 0)\r\nreturn -EINVAL;\r\n}\r\nif (mipsxx_pmu_alloc_counter(&fake_cpuc, &event->hw) < 0)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic void handle_associated_event(struct cpu_hw_events *cpuc,\r\nint idx, struct perf_sample_data *data,\r\nstruct pt_regs *regs)\r\n{\r\nstruct perf_event *event = cpuc->events[idx];\r\nstruct hw_perf_event *hwc = &event->hw;\r\nmipspmu_event_update(event, hwc, idx);\r\ndata->period = event->hw.last_period;\r\nif (!mipspmu_event_set_period(event, hwc, idx))\r\nreturn;\r\nif (perf_event_overflow(event, data, regs))\r\nmipsxx_pmu_disable_event(idx);\r\n}\r\nstatic int __n_counters(void)\r\n{\r\nif (!(read_c0_config1() & M_CONFIG1_PC))\r\nreturn 0;\r\nif (!(read_c0_perfctrl0() & M_PERFCTL_MORE))\r\nreturn 1;\r\nif (!(read_c0_perfctrl1() & M_PERFCTL_MORE))\r\nreturn 2;\r\nif (!(read_c0_perfctrl2() & M_PERFCTL_MORE))\r\nreturn 3;\r\nreturn 4;\r\n}\r\nstatic int n_counters(void)\r\n{\r\nint counters;\r\nswitch (current_cpu_type()) {\r\ncase CPU_R10000:\r\ncounters = 2;\r\nbreak;\r\ncase CPU_R12000:\r\ncase CPU_R14000:\r\ncounters = 4;\r\nbreak;\r\ndefault:\r\ncounters = __n_counters();\r\n}\r\nreturn counters;\r\n}\r\nstatic void reset_counters(void *arg)\r\n{\r\nint counters = (int)(long)arg;\r\nswitch (counters) {\r\ncase 4:\r\nmipsxx_pmu_write_control(3, 0);\r\nmipspmu.write_counter(3, 0);\r\ncase 3:\r\nmipsxx_pmu_write_control(2, 0);\r\nmipspmu.write_counter(2, 0);\r\ncase 2:\r\nmipsxx_pmu_write_control(1, 0);\r\nmipspmu.write_counter(1, 0);\r\ncase 1:\r\nmipsxx_pmu_write_control(0, 0);\r\nmipspmu.write_counter(0, 0);\r\n}\r\n}\r\nstatic void check_and_calc_range(struct perf_event *event,\r\nconst struct mips_perf_event *pev)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (event->cpu >= 0) {\r\nif (pev->range > V) {\r\nhwc->config_base |= M_TC_EN_ALL;\r\n} else {\r\nhwc->config_base |= M_PERFCTL_VPEID(event->cpu);\r\nhwc->config_base |= M_TC_EN_VPE;\r\n}\r\n} else\r\nhwc->config_base |= M_TC_EN_ALL;\r\n}\r\nstatic void check_and_calc_range(struct perf_event *event,\r\nconst struct mips_perf_event *pev)\r\n{\r\n}\r\nstatic int __hw_perf_event_init(struct perf_event *event)\r\n{\r\nstruct perf_event_attr *attr = &event->attr;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nconst struct mips_perf_event *pev;\r\nint err;\r\nif (PERF_TYPE_HARDWARE == event->attr.type) {\r\nif (event->attr.config >= PERF_COUNT_HW_MAX)\r\nreturn -EINVAL;\r\npev = mipspmu_map_general_event(event->attr.config);\r\n} else if (PERF_TYPE_HW_CACHE == event->attr.type) {\r\npev = mipspmu_map_cache_event(event->attr.config);\r\n} else if (PERF_TYPE_RAW == event->attr.type) {\r\nmutex_lock(&raw_event_mutex);\r\npev = mipspmu.map_raw_event(event->attr.config);\r\n} else {\r\nreturn -EOPNOTSUPP;\r\n}\r\nif (IS_ERR(pev)) {\r\nif (PERF_TYPE_RAW == event->attr.type)\r\nmutex_unlock(&raw_event_mutex);\r\nreturn PTR_ERR(pev);\r\n}\r\nhwc->config_base = M_PERFCTL_INTERRUPT_ENABLE;\r\nif (num_possible_cpus() > 1)\r\ncheck_and_calc_range(event, pev);\r\nhwc->event_base = mipspmu_perf_event_encode(pev);\r\nif (PERF_TYPE_RAW == event->attr.type)\r\nmutex_unlock(&raw_event_mutex);\r\nif (!attr->exclude_user)\r\nhwc->config_base |= M_PERFCTL_USER;\r\nif (!attr->exclude_kernel) {\r\nhwc->config_base |= M_PERFCTL_KERNEL;\r\nhwc->config_base |= M_PERFCTL_EXL;\r\n}\r\nif (!attr->exclude_hv)\r\nhwc->config_base |= M_PERFCTL_SUPERVISOR;\r\nhwc->config_base &= M_PERFCTL_CONFIG_MASK;\r\nhwc->idx = -1;\r\nhwc->config = 0;\r\nif (!hwc->sample_period) {\r\nhwc->sample_period = mipspmu.max_period;\r\nhwc->last_period = hwc->sample_period;\r\nlocal64_set(&hwc->period_left, hwc->sample_period);\r\n}\r\nerr = 0;\r\nif (event->group_leader != event)\r\nerr = validate_group(event);\r\nevent->destroy = hw_perf_event_destroy;\r\nif (err)\r\nevent->destroy(event);\r\nreturn err;\r\n}\r\nstatic void pause_local_counters(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nint ctr = mipspmu.num_counters;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\ndo {\r\nctr--;\r\ncpuc->saved_ctrl[ctr] = mipsxx_pmu_read_control(ctr);\r\nmipsxx_pmu_write_control(ctr, cpuc->saved_ctrl[ctr] &\r\n~M_PERFCTL_COUNT_EVENT_WHENEVER);\r\n} while (ctr > 0);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void resume_local_counters(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nint ctr = mipspmu.num_counters;\r\ndo {\r\nctr--;\r\nmipsxx_pmu_write_control(ctr, cpuc->saved_ctrl[ctr]);\r\n} while (ctr > 0);\r\n}\r\nstatic int mipsxx_pmu_handle_shared_irq(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct perf_sample_data data;\r\nunsigned int counters = mipspmu.num_counters;\r\nu64 counter;\r\nint handled = IRQ_NONE;\r\nstruct pt_regs *regs;\r\nif (cpu_has_perf_cntr_intr_bit && !(read_c0_cause() & CAUSEF_PCI))\r\nreturn handled;\r\npause_local_counters();\r\n#ifdef CONFIG_MIPS_PERF_SHARED_TC_COUNTERS\r\nread_lock(&pmuint_rwlock);\r\n#endif\r\nregs = get_irq_regs();\r\nperf_sample_data_init(&data, 0, 0);\r\nswitch (counters) {\r\n#define HANDLE_COUNTER(n) \\r\ncase n + 1: \\r\nif (test_bit(n, cpuc->used_mask)) { \\r\ncounter = mipspmu.read_counter(n); \\r\nif (counter & mipspmu.overflow) { \\r\nhandle_associated_event(cpuc, n, &data, regs); \\r\nhandled = IRQ_HANDLED; \\r\n} \\r\n}\r\nHANDLE_COUNTER(3)\r\nHANDLE_COUNTER(2)\r\nHANDLE_COUNTER(1)\r\nHANDLE_COUNTER(0)\r\n}\r\nif (handled == IRQ_HANDLED)\r\nirq_work_run();\r\n#ifdef CONFIG_MIPS_PERF_SHARED_TC_COUNTERS\r\nread_unlock(&pmuint_rwlock);\r\n#endif\r\nresume_local_counters();\r\nreturn handled;\r\n}\r\nstatic irqreturn_t mipsxx_pmu_handle_irq(int irq, void *dev)\r\n{\r\nreturn mipsxx_pmu_handle_shared_irq();\r\n}\r\nstatic const struct mips_perf_event *mipsxx_pmu_map_raw_event(u64 config)\r\n{\r\nunsigned int raw_id = config & 0xff;\r\nunsigned int base_id = raw_id & 0x7f;\r\nraw_event.event_id = base_id;\r\nswitch (current_cpu_type()) {\r\ncase CPU_24K:\r\nif (IS_BOTH_COUNTERS_24K_EVENT(base_id))\r\nraw_event.cntr_mask = CNTR_EVEN | CNTR_ODD;\r\nelse\r\nraw_event.cntr_mask =\r\nraw_id > 127 ? CNTR_ODD : CNTR_EVEN;\r\n#ifdef CONFIG_MIPS_MT_SMP\r\nraw_event.range = P;\r\n#endif\r\nbreak;\r\ncase CPU_34K:\r\nif (IS_BOTH_COUNTERS_34K_EVENT(base_id))\r\nraw_event.cntr_mask = CNTR_EVEN | CNTR_ODD;\r\nelse\r\nraw_event.cntr_mask =\r\nraw_id > 127 ? CNTR_ODD : CNTR_EVEN;\r\n#ifdef CONFIG_MIPS_MT_SMP\r\nif (IS_RANGE_P_34K_EVENT(raw_id, base_id))\r\nraw_event.range = P;\r\nelse if (unlikely(IS_RANGE_V_34K_EVENT(raw_id)))\r\nraw_event.range = V;\r\nelse\r\nraw_event.range = T;\r\n#endif\r\nbreak;\r\ncase CPU_74K:\r\ncase CPU_1074K:\r\nif (IS_BOTH_COUNTERS_74K_EVENT(base_id))\r\nraw_event.cntr_mask = CNTR_EVEN | CNTR_ODD;\r\nelse\r\nraw_event.cntr_mask =\r\nraw_id > 127 ? CNTR_ODD : CNTR_EVEN;\r\n#ifdef CONFIG_MIPS_MT_SMP\r\nraw_event.range = P;\r\n#endif\r\nbreak;\r\ncase CPU_PROAPTIV:\r\nif (IS_BOTH_COUNTERS_PROAPTIV_EVENT(base_id))\r\nraw_event.cntr_mask = CNTR_EVEN | CNTR_ODD;\r\nelse\r\nraw_event.cntr_mask =\r\nraw_id > 127 ? CNTR_ODD : CNTR_EVEN;\r\n#ifdef CONFIG_MIPS_MT_SMP\r\nraw_event.range = P;\r\n#endif\r\nbreak;\r\ncase CPU_1004K:\r\nif (IS_BOTH_COUNTERS_1004K_EVENT(base_id))\r\nraw_event.cntr_mask = CNTR_EVEN | CNTR_ODD;\r\nelse\r\nraw_event.cntr_mask =\r\nraw_id > 127 ? CNTR_ODD : CNTR_EVEN;\r\n#ifdef CONFIG_MIPS_MT_SMP\r\nif (IS_RANGE_P_1004K_EVENT(raw_id, base_id))\r\nraw_event.range = P;\r\nelse if (unlikely(IS_RANGE_V_1004K_EVENT(raw_id)))\r\nraw_event.range = V;\r\nelse\r\nraw_event.range = T;\r\n#endif\r\nbreak;\r\ncase CPU_INTERAPTIV:\r\nif (IS_BOTH_COUNTERS_INTERAPTIV_EVENT(base_id))\r\nraw_event.cntr_mask = CNTR_EVEN | CNTR_ODD;\r\nelse\r\nraw_event.cntr_mask =\r\nraw_id > 127 ? CNTR_ODD : CNTR_EVEN;\r\n#ifdef CONFIG_MIPS_MT_SMP\r\nif (IS_RANGE_P_INTERAPTIV_EVENT(raw_id, base_id))\r\nraw_event.range = P;\r\nelse if (unlikely(IS_RANGE_V_INTERAPTIV_EVENT(raw_id)))\r\nraw_event.range = V;\r\nelse\r\nraw_event.range = T;\r\n#endif\r\nbreak;\r\ncase CPU_BMIPS5000:\r\nif (IS_BOTH_COUNTERS_BMIPS5000_EVENT(base_id))\r\nraw_event.cntr_mask = CNTR_EVEN | CNTR_ODD;\r\nelse\r\nraw_event.cntr_mask =\r\nraw_id > 127 ? CNTR_ODD : CNTR_EVEN;\r\n}\r\nreturn &raw_event;\r\n}\r\nstatic const struct mips_perf_event *octeon_pmu_map_raw_event(u64 config)\r\n{\r\nunsigned int raw_id = config & 0xff;\r\nunsigned int base_id = raw_id & 0x7f;\r\nraw_event.cntr_mask = CNTR_ALL;\r\nraw_event.event_id = base_id;\r\nif (current_cpu_type() == CPU_CAVIUM_OCTEON2) {\r\nif (base_id > 0x42)\r\nreturn ERR_PTR(-EOPNOTSUPP);\r\n} else {\r\nif (base_id > 0x3a)\r\nreturn ERR_PTR(-EOPNOTSUPP);\r\n}\r\nswitch (base_id) {\r\ncase 0x00:\r\ncase 0x0f:\r\ncase 0x1e:\r\ncase 0x1f:\r\ncase 0x2f:\r\ncase 0x34:\r\ncase 0x3b ... 0x3f:\r\nreturn ERR_PTR(-EOPNOTSUPP);\r\ndefault:\r\nbreak;\r\n}\r\nreturn &raw_event;\r\n}\r\nstatic const struct mips_perf_event *xlp_pmu_map_raw_event(u64 config)\r\n{\r\nunsigned int raw_id = config & 0xff;\r\nif ((raw_id < 0x01) || (raw_id > 0x3f))\r\nreturn ERR_PTR(-EOPNOTSUPP);\r\nraw_event.cntr_mask = CNTR_ALL;\r\nraw_event.event_id = raw_id;\r\nreturn &raw_event;\r\n}\r\nstatic int __init\r\ninit_hw_perf_events(void)\r\n{\r\nint counters, irq;\r\nint counter_bits;\r\npr_info("Performance counters: ");\r\ncounters = n_counters();\r\nif (counters == 0) {\r\npr_cont("No available PMU.\n");\r\nreturn -ENODEV;\r\n}\r\n#ifdef CONFIG_MIPS_PERF_SHARED_TC_COUNTERS\r\ncpu_has_mipsmt_pertccounters = read_c0_config7() & (1<<19);\r\nif (!cpu_has_mipsmt_pertccounters)\r\ncounters = counters_total_to_per_cpu(counters);\r\n#endif\r\n#ifdef MSC01E_INT_BASE\r\nif (cpu_has_veic) {\r\nirq = MSC01E_INT_BASE + MSC01E_INT_PERFCTR;\r\n} else {\r\n#endif\r\nif ((cp0_perfcount_irq >= 0) &&\r\n(cp0_compare_irq != cp0_perfcount_irq))\r\nirq = MIPS_CPU_IRQ_BASE + cp0_perfcount_irq;\r\nelse\r\nirq = -1;\r\n#ifdef MSC01E_INT_BASE\r\n}\r\n#endif\r\nmipspmu.map_raw_event = mipsxx_pmu_map_raw_event;\r\nswitch (current_cpu_type()) {\r\ncase CPU_24K:\r\nmipspmu.name = "mips/24K";\r\nmipspmu.general_event_map = &mipsxxcore_event_map;\r\nmipspmu.cache_event_map = &mipsxxcore_cache_map;\r\nbreak;\r\ncase CPU_34K:\r\nmipspmu.name = "mips/34K";\r\nmipspmu.general_event_map = &mipsxxcore_event_map;\r\nmipspmu.cache_event_map = &mipsxxcore_cache_map;\r\nbreak;\r\ncase CPU_74K:\r\nmipspmu.name = "mips/74K";\r\nmipspmu.general_event_map = &mipsxxcore_event_map2;\r\nmipspmu.cache_event_map = &mipsxxcore_cache_map2;\r\nbreak;\r\ncase CPU_PROAPTIV:\r\nmipspmu.name = "mips/proAptiv";\r\nmipspmu.general_event_map = &mipsxxcore_event_map2;\r\nmipspmu.cache_event_map = &mipsxxcore_cache_map2;\r\nbreak;\r\ncase CPU_1004K:\r\nmipspmu.name = "mips/1004K";\r\nmipspmu.general_event_map = &mipsxxcore_event_map;\r\nmipspmu.cache_event_map = &mipsxxcore_cache_map;\r\nbreak;\r\ncase CPU_1074K:\r\nmipspmu.name = "mips/1074K";\r\nmipspmu.general_event_map = &mipsxxcore_event_map;\r\nmipspmu.cache_event_map = &mipsxxcore_cache_map;\r\nbreak;\r\ncase CPU_INTERAPTIV:\r\nmipspmu.name = "mips/interAptiv";\r\nmipspmu.general_event_map = &mipsxxcore_event_map;\r\nmipspmu.cache_event_map = &mipsxxcore_cache_map;\r\nbreak;\r\ncase CPU_LOONGSON1:\r\nmipspmu.name = "mips/loongson1";\r\nmipspmu.general_event_map = &mipsxxcore_event_map;\r\nmipspmu.cache_event_map = &mipsxxcore_cache_map;\r\nbreak;\r\ncase CPU_CAVIUM_OCTEON:\r\ncase CPU_CAVIUM_OCTEON_PLUS:\r\ncase CPU_CAVIUM_OCTEON2:\r\nmipspmu.name = "octeon";\r\nmipspmu.general_event_map = &octeon_event_map;\r\nmipspmu.cache_event_map = &octeon_cache_map;\r\nmipspmu.map_raw_event = octeon_pmu_map_raw_event;\r\nbreak;\r\ncase CPU_BMIPS5000:\r\nmipspmu.name = "BMIPS5000";\r\nmipspmu.general_event_map = &bmips5000_event_map;\r\nmipspmu.cache_event_map = &bmips5000_cache_map;\r\nbreak;\r\ncase CPU_XLP:\r\nmipspmu.name = "xlp";\r\nmipspmu.general_event_map = &xlp_event_map;\r\nmipspmu.cache_event_map = &xlp_cache_map;\r\nmipspmu.map_raw_event = xlp_pmu_map_raw_event;\r\nbreak;\r\ndefault:\r\npr_cont("Either hardware does not support performance "\r\n"counters, or not yet implemented.\n");\r\nreturn -ENODEV;\r\n}\r\nmipspmu.num_counters = counters;\r\nmipspmu.irq = irq;\r\nif (read_c0_perfctrl0() & M_PERFCTL_WIDE) {\r\nmipspmu.max_period = (1ULL << 63) - 1;\r\nmipspmu.valid_count = (1ULL << 63) - 1;\r\nmipspmu.overflow = 1ULL << 63;\r\nmipspmu.read_counter = mipsxx_pmu_read_counter_64;\r\nmipspmu.write_counter = mipsxx_pmu_write_counter_64;\r\ncounter_bits = 64;\r\n} else {\r\nmipspmu.max_period = (1ULL << 31) - 1;\r\nmipspmu.valid_count = (1ULL << 31) - 1;\r\nmipspmu.overflow = 1ULL << 31;\r\nmipspmu.read_counter = mipsxx_pmu_read_counter;\r\nmipspmu.write_counter = mipsxx_pmu_write_counter;\r\ncounter_bits = 32;\r\n}\r\non_each_cpu(reset_counters, (void *)(long)counters, 1);\r\npr_cont("%s PMU enabled, %d %d-bit counters available to each "\r\n"CPU, irq %d%s\n", mipspmu.name, counters, counter_bits, irq,\r\nirq < 0 ? " (share with timer interrupt)" : "");\r\nperf_pmu_register(&pmu, "cpu", PERF_TYPE_RAW);\r\nreturn 0;\r\n}
