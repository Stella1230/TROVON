void perf_evlist__init(struct perf_evlist *evlist, struct cpu_map *cpus,\r\nstruct thread_map *threads)\r\n{\r\nint i;\r\nfor (i = 0; i < PERF_EVLIST__HLIST_SIZE; ++i)\r\nINIT_HLIST_HEAD(&evlist->heads[i]);\r\nINIT_LIST_HEAD(&evlist->entries);\r\nperf_evlist__set_maps(evlist, cpus, threads);\r\nevlist->workload.pid = -1;\r\n}\r\nstruct perf_evlist *perf_evlist__new(void)\r\n{\r\nstruct perf_evlist *evlist = zalloc(sizeof(*evlist));\r\nif (evlist != NULL)\r\nperf_evlist__init(evlist, NULL, NULL);\r\nreturn evlist;\r\n}\r\nstruct perf_evlist *perf_evlist__new_default(void)\r\n{\r\nstruct perf_evlist *evlist = perf_evlist__new();\r\nif (evlist && perf_evlist__add_default(evlist)) {\r\nperf_evlist__delete(evlist);\r\nevlist = NULL;\r\n}\r\nreturn evlist;\r\n}\r\nvoid perf_evlist__set_id_pos(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *first = perf_evlist__first(evlist);\r\nevlist->id_pos = first->id_pos;\r\nevlist->is_pos = first->is_pos;\r\n}\r\nstatic void perf_evlist__update_id_pos(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each(evlist, evsel)\r\nperf_evsel__calc_id_pos(evsel);\r\nperf_evlist__set_id_pos(evlist);\r\n}\r\nstatic void perf_evlist__purge(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *pos, *n;\r\nevlist__for_each_safe(evlist, n, pos) {\r\nlist_del_init(&pos->node);\r\nperf_evsel__delete(pos);\r\n}\r\nevlist->nr_entries = 0;\r\n}\r\nvoid perf_evlist__exit(struct perf_evlist *evlist)\r\n{\r\nzfree(&evlist->mmap);\r\nzfree(&evlist->pollfd);\r\n}\r\nvoid perf_evlist__delete(struct perf_evlist *evlist)\r\n{\r\nperf_evlist__munmap(evlist);\r\nperf_evlist__close(evlist);\r\ncpu_map__delete(evlist->cpus);\r\nthread_map__delete(evlist->threads);\r\nevlist->cpus = NULL;\r\nevlist->threads = NULL;\r\nperf_evlist__purge(evlist);\r\nperf_evlist__exit(evlist);\r\nfree(evlist);\r\n}\r\nvoid perf_evlist__add(struct perf_evlist *evlist, struct perf_evsel *entry)\r\n{\r\nlist_add_tail(&entry->node, &evlist->entries);\r\nentry->idx = evlist->nr_entries;\r\nif (!evlist->nr_entries++)\r\nperf_evlist__set_id_pos(evlist);\r\n}\r\nvoid perf_evlist__splice_list_tail(struct perf_evlist *evlist,\r\nstruct list_head *list,\r\nint nr_entries)\r\n{\r\nbool set_id_pos = !evlist->nr_entries;\r\nlist_splice_tail(list, &evlist->entries);\r\nevlist->nr_entries += nr_entries;\r\nif (set_id_pos)\r\nperf_evlist__set_id_pos(evlist);\r\n}\r\nvoid __perf_evlist__set_leader(struct list_head *list)\r\n{\r\nstruct perf_evsel *evsel, *leader;\r\nleader = list_entry(list->next, struct perf_evsel, node);\r\nevsel = list_entry(list->prev, struct perf_evsel, node);\r\nleader->nr_members = evsel->idx - leader->idx + 1;\r\n__evlist__for_each(list, evsel) {\r\nevsel->leader = leader;\r\n}\r\n}\r\nvoid perf_evlist__set_leader(struct perf_evlist *evlist)\r\n{\r\nif (evlist->nr_entries) {\r\nevlist->nr_groups = evlist->nr_entries > 1 ? 1 : 0;\r\n__perf_evlist__set_leader(&evlist->entries);\r\n}\r\n}\r\nint perf_evlist__add_default(struct perf_evlist *evlist)\r\n{\r\nstruct perf_event_attr attr = {\r\n.type = PERF_TYPE_HARDWARE,\r\n.config = PERF_COUNT_HW_CPU_CYCLES,\r\n};\r\nstruct perf_evsel *evsel;\r\nevent_attr_init(&attr);\r\nevsel = perf_evsel__new(&attr);\r\nif (evsel == NULL)\r\ngoto error;\r\nevsel->name = strdup("cycles");\r\nif (!evsel->name)\r\ngoto error_free;\r\nperf_evlist__add(evlist, evsel);\r\nreturn 0;\r\nerror_free:\r\nperf_evsel__delete(evsel);\r\nerror:\r\nreturn -ENOMEM;\r\n}\r\nstatic int perf_evlist__add_attrs(struct perf_evlist *evlist,\r\nstruct perf_event_attr *attrs, size_t nr_attrs)\r\n{\r\nstruct perf_evsel *evsel, *n;\r\nLIST_HEAD(head);\r\nsize_t i;\r\nfor (i = 0; i < nr_attrs; i++) {\r\nevsel = perf_evsel__new_idx(attrs + i, evlist->nr_entries + i);\r\nif (evsel == NULL)\r\ngoto out_delete_partial_list;\r\nlist_add_tail(&evsel->node, &head);\r\n}\r\nperf_evlist__splice_list_tail(evlist, &head, nr_attrs);\r\nreturn 0;\r\nout_delete_partial_list:\r\n__evlist__for_each_safe(&head, n, evsel)\r\nperf_evsel__delete(evsel);\r\nreturn -1;\r\n}\r\nint __perf_evlist__add_default_attrs(struct perf_evlist *evlist,\r\nstruct perf_event_attr *attrs, size_t nr_attrs)\r\n{\r\nsize_t i;\r\nfor (i = 0; i < nr_attrs; i++)\r\nevent_attr_init(attrs + i);\r\nreturn perf_evlist__add_attrs(evlist, attrs, nr_attrs);\r\n}\r\nstruct perf_evsel *\r\nperf_evlist__find_tracepoint_by_id(struct perf_evlist *evlist, int id)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each(evlist, evsel) {\r\nif (evsel->attr.type == PERF_TYPE_TRACEPOINT &&\r\n(int)evsel->attr.config == id)\r\nreturn evsel;\r\n}\r\nreturn NULL;\r\n}\r\nstruct perf_evsel *\r\nperf_evlist__find_tracepoint_by_name(struct perf_evlist *evlist,\r\nconst char *name)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each(evlist, evsel) {\r\nif ((evsel->attr.type == PERF_TYPE_TRACEPOINT) &&\r\n(strcmp(evsel->name, name) == 0))\r\nreturn evsel;\r\n}\r\nreturn NULL;\r\n}\r\nint perf_evlist__add_newtp(struct perf_evlist *evlist,\r\nconst char *sys, const char *name, void *handler)\r\n{\r\nstruct perf_evsel *evsel = perf_evsel__newtp(sys, name);\r\nif (evsel == NULL)\r\nreturn -1;\r\nevsel->handler = handler;\r\nperf_evlist__add(evlist, evsel);\r\nreturn 0;\r\n}\r\nvoid perf_evlist__disable(struct perf_evlist *evlist)\r\n{\r\nint cpu, thread;\r\nstruct perf_evsel *pos;\r\nint nr_cpus = cpu_map__nr(evlist->cpus);\r\nint nr_threads = thread_map__nr(evlist->threads);\r\nfor (cpu = 0; cpu < nr_cpus; cpu++) {\r\nevlist__for_each(evlist, pos) {\r\nif (!perf_evsel__is_group_leader(pos) || !pos->fd)\r\ncontinue;\r\nfor (thread = 0; thread < nr_threads; thread++)\r\nioctl(FD(pos, cpu, thread),\r\nPERF_EVENT_IOC_DISABLE, 0);\r\n}\r\n}\r\n}\r\nvoid perf_evlist__enable(struct perf_evlist *evlist)\r\n{\r\nint cpu, thread;\r\nstruct perf_evsel *pos;\r\nint nr_cpus = cpu_map__nr(evlist->cpus);\r\nint nr_threads = thread_map__nr(evlist->threads);\r\nfor (cpu = 0; cpu < nr_cpus; cpu++) {\r\nevlist__for_each(evlist, pos) {\r\nif (!perf_evsel__is_group_leader(pos) || !pos->fd)\r\ncontinue;\r\nfor (thread = 0; thread < nr_threads; thread++)\r\nioctl(FD(pos, cpu, thread),\r\nPERF_EVENT_IOC_ENABLE, 0);\r\n}\r\n}\r\n}\r\nint perf_evlist__disable_event(struct perf_evlist *evlist,\r\nstruct perf_evsel *evsel)\r\n{\r\nint cpu, thread, err;\r\nif (!evsel->fd)\r\nreturn 0;\r\nfor (cpu = 0; cpu < evlist->cpus->nr; cpu++) {\r\nfor (thread = 0; thread < evlist->threads->nr; thread++) {\r\nerr = ioctl(FD(evsel, cpu, thread),\r\nPERF_EVENT_IOC_DISABLE, 0);\r\nif (err)\r\nreturn err;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint perf_evlist__enable_event(struct perf_evlist *evlist,\r\nstruct perf_evsel *evsel)\r\n{\r\nint cpu, thread, err;\r\nif (!evsel->fd)\r\nreturn -EINVAL;\r\nfor (cpu = 0; cpu < evlist->cpus->nr; cpu++) {\r\nfor (thread = 0; thread < evlist->threads->nr; thread++) {\r\nerr = ioctl(FD(evsel, cpu, thread),\r\nPERF_EVENT_IOC_ENABLE, 0);\r\nif (err)\r\nreturn err;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int perf_evlist__alloc_pollfd(struct perf_evlist *evlist)\r\n{\r\nint nr_cpus = cpu_map__nr(evlist->cpus);\r\nint nr_threads = thread_map__nr(evlist->threads);\r\nint nfds = nr_cpus * nr_threads * evlist->nr_entries;\r\nevlist->pollfd = malloc(sizeof(struct pollfd) * nfds);\r\nreturn evlist->pollfd != NULL ? 0 : -ENOMEM;\r\n}\r\nvoid perf_evlist__add_pollfd(struct perf_evlist *evlist, int fd)\r\n{\r\nfcntl(fd, F_SETFL, O_NONBLOCK);\r\nevlist->pollfd[evlist->nr_fds].fd = fd;\r\nevlist->pollfd[evlist->nr_fds].events = POLLIN;\r\nevlist->nr_fds++;\r\n}\r\nstatic void perf_evlist__id_hash(struct perf_evlist *evlist,\r\nstruct perf_evsel *evsel,\r\nint cpu, int thread, u64 id)\r\n{\r\nint hash;\r\nstruct perf_sample_id *sid = SID(evsel, cpu, thread);\r\nsid->id = id;\r\nsid->evsel = evsel;\r\nhash = hash_64(sid->id, PERF_EVLIST__HLIST_BITS);\r\nhlist_add_head(&sid->node, &evlist->heads[hash]);\r\n}\r\nvoid perf_evlist__id_add(struct perf_evlist *evlist, struct perf_evsel *evsel,\r\nint cpu, int thread, u64 id)\r\n{\r\nperf_evlist__id_hash(evlist, evsel, cpu, thread, id);\r\nevsel->id[evsel->ids++] = id;\r\n}\r\nstatic int perf_evlist__id_add_fd(struct perf_evlist *evlist,\r\nstruct perf_evsel *evsel,\r\nint cpu, int thread, int fd)\r\n{\r\nu64 read_data[4] = { 0, };\r\nint id_idx = 1;\r\nu64 id;\r\nint ret;\r\nret = ioctl(fd, PERF_EVENT_IOC_ID, &id);\r\nif (!ret)\r\ngoto add;\r\nif (errno != ENOTTY)\r\nreturn -1;\r\nif (perf_evlist__read_format(evlist) & PERF_FORMAT_GROUP)\r\nreturn -1;\r\nif (!(evsel->attr.read_format & PERF_FORMAT_ID) ||\r\nread(fd, &read_data, sizeof(read_data)) == -1)\r\nreturn -1;\r\nif (evsel->attr.read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\r\n++id_idx;\r\nif (evsel->attr.read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\r\n++id_idx;\r\nid = read_data[id_idx];\r\nadd:\r\nperf_evlist__id_add(evlist, evsel, cpu, thread, id);\r\nreturn 0;\r\n}\r\nstruct perf_sample_id *perf_evlist__id2sid(struct perf_evlist *evlist, u64 id)\r\n{\r\nstruct hlist_head *head;\r\nstruct perf_sample_id *sid;\r\nint hash;\r\nhash = hash_64(id, PERF_EVLIST__HLIST_BITS);\r\nhead = &evlist->heads[hash];\r\nhlist_for_each_entry(sid, head, node)\r\nif (sid->id == id)\r\nreturn sid;\r\nreturn NULL;\r\n}\r\nstruct perf_evsel *perf_evlist__id2evsel(struct perf_evlist *evlist, u64 id)\r\n{\r\nstruct perf_sample_id *sid;\r\nif (evlist->nr_entries == 1)\r\nreturn perf_evlist__first(evlist);\r\nsid = perf_evlist__id2sid(evlist, id);\r\nif (sid)\r\nreturn sid->evsel;\r\nif (!perf_evlist__sample_id_all(evlist))\r\nreturn perf_evlist__first(evlist);\r\nreturn NULL;\r\n}\r\nstatic int perf_evlist__event2id(struct perf_evlist *evlist,\r\nunion perf_event *event, u64 *id)\r\n{\r\nconst u64 *array = event->sample.array;\r\nssize_t n;\r\nn = (event->header.size - sizeof(event->header)) >> 3;\r\nif (event->header.type == PERF_RECORD_SAMPLE) {\r\nif (evlist->id_pos >= n)\r\nreturn -1;\r\n*id = array[evlist->id_pos];\r\n} else {\r\nif (evlist->is_pos > n)\r\nreturn -1;\r\nn -= evlist->is_pos;\r\n*id = array[n];\r\n}\r\nreturn 0;\r\n}\r\nstatic struct perf_evsel *perf_evlist__event2evsel(struct perf_evlist *evlist,\r\nunion perf_event *event)\r\n{\r\nstruct perf_evsel *first = perf_evlist__first(evlist);\r\nstruct hlist_head *head;\r\nstruct perf_sample_id *sid;\r\nint hash;\r\nu64 id;\r\nif (evlist->nr_entries == 1)\r\nreturn first;\r\nif (!first->attr.sample_id_all &&\r\nevent->header.type != PERF_RECORD_SAMPLE)\r\nreturn first;\r\nif (perf_evlist__event2id(evlist, event, &id))\r\nreturn NULL;\r\nif (!id)\r\nreturn first;\r\nhash = hash_64(id, PERF_EVLIST__HLIST_BITS);\r\nhead = &evlist->heads[hash];\r\nhlist_for_each_entry(sid, head, node) {\r\nif (sid->id == id)\r\nreturn sid->evsel;\r\n}\r\nreturn NULL;\r\n}\r\nunion perf_event *perf_evlist__mmap_read(struct perf_evlist *evlist, int idx)\r\n{\r\nstruct perf_mmap *md = &evlist->mmap[idx];\r\nunsigned int head = perf_mmap__read_head(md);\r\nunsigned int old = md->prev;\r\nunsigned char *data = md->base + page_size;\r\nunion perf_event *event = NULL;\r\nif (evlist->overwrite) {\r\nint diff = head - old;\r\nif (diff > md->mask / 2 || diff < 0) {\r\nfprintf(stderr, "WARNING: failed to keep up with mmap data.\n");\r\nold = head;\r\n}\r\n}\r\nif (old != head) {\r\nsize_t size;\r\nevent = (union perf_event *)&data[old & md->mask];\r\nsize = event->header.size;\r\nif ((old & md->mask) + size != ((old + size) & md->mask)) {\r\nunsigned int offset = old;\r\nunsigned int len = min(sizeof(*event), size), cpy;\r\nvoid *dst = md->event_copy;\r\ndo {\r\ncpy = min(md->mask + 1 - (offset & md->mask), len);\r\nmemcpy(dst, &data[offset & md->mask], cpy);\r\noffset += cpy;\r\ndst += cpy;\r\nlen -= cpy;\r\n} while (len);\r\nevent = (union perf_event *) md->event_copy;\r\n}\r\nold += size;\r\n}\r\nmd->prev = old;\r\nreturn event;\r\n}\r\nvoid perf_evlist__mmap_consume(struct perf_evlist *evlist, int idx)\r\n{\r\nif (!evlist->overwrite) {\r\nstruct perf_mmap *md = &evlist->mmap[idx];\r\nunsigned int old = md->prev;\r\nperf_mmap__write_tail(md, old);\r\n}\r\n}\r\nstatic void __perf_evlist__munmap(struct perf_evlist *evlist, int idx)\r\n{\r\nif (evlist->mmap[idx].base != NULL) {\r\nmunmap(evlist->mmap[idx].base, evlist->mmap_len);\r\nevlist->mmap[idx].base = NULL;\r\n}\r\n}\r\nvoid perf_evlist__munmap(struct perf_evlist *evlist)\r\n{\r\nint i;\r\nif (evlist->mmap == NULL)\r\nreturn;\r\nfor (i = 0; i < evlist->nr_mmaps; i++)\r\n__perf_evlist__munmap(evlist, i);\r\nzfree(&evlist->mmap);\r\n}\r\nstatic int perf_evlist__alloc_mmap(struct perf_evlist *evlist)\r\n{\r\nevlist->nr_mmaps = cpu_map__nr(evlist->cpus);\r\nif (cpu_map__empty(evlist->cpus))\r\nevlist->nr_mmaps = thread_map__nr(evlist->threads);\r\nevlist->mmap = zalloc(evlist->nr_mmaps * sizeof(struct perf_mmap));\r\nreturn evlist->mmap != NULL ? 0 : -ENOMEM;\r\n}\r\nstatic int __perf_evlist__mmap(struct perf_evlist *evlist,\r\nint idx, int prot, int mask, int fd)\r\n{\r\nevlist->mmap[idx].prev = 0;\r\nevlist->mmap[idx].mask = mask;\r\nevlist->mmap[idx].base = mmap(NULL, evlist->mmap_len, prot,\r\nMAP_SHARED, fd, 0);\r\nif (evlist->mmap[idx].base == MAP_FAILED) {\r\npr_debug2("failed to mmap perf event ring buffer, error %d\n",\r\nerrno);\r\nevlist->mmap[idx].base = NULL;\r\nreturn -1;\r\n}\r\nperf_evlist__add_pollfd(evlist, fd);\r\nreturn 0;\r\n}\r\nstatic int perf_evlist__mmap_per_evsel(struct perf_evlist *evlist, int idx,\r\nint prot, int mask, int cpu, int thread,\r\nint *output)\r\n{\r\nstruct perf_evsel *evsel;\r\nevlist__for_each(evlist, evsel) {\r\nint fd = FD(evsel, cpu, thread);\r\nif (*output == -1) {\r\n*output = fd;\r\nif (__perf_evlist__mmap(evlist, idx, prot, mask,\r\n*output) < 0)\r\nreturn -1;\r\n} else {\r\nif (ioctl(fd, PERF_EVENT_IOC_SET_OUTPUT, *output) != 0)\r\nreturn -1;\r\n}\r\nif ((evsel->attr.read_format & PERF_FORMAT_ID) &&\r\nperf_evlist__id_add_fd(evlist, evsel, cpu, thread, fd) < 0)\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int perf_evlist__mmap_per_cpu(struct perf_evlist *evlist, int prot,\r\nint mask)\r\n{\r\nint cpu, thread;\r\nint nr_cpus = cpu_map__nr(evlist->cpus);\r\nint nr_threads = thread_map__nr(evlist->threads);\r\npr_debug2("perf event ring buffer mmapped per cpu\n");\r\nfor (cpu = 0; cpu < nr_cpus; cpu++) {\r\nint output = -1;\r\nfor (thread = 0; thread < nr_threads; thread++) {\r\nif (perf_evlist__mmap_per_evsel(evlist, cpu, prot, mask,\r\ncpu, thread, &output))\r\ngoto out_unmap;\r\n}\r\n}\r\nreturn 0;\r\nout_unmap:\r\nfor (cpu = 0; cpu < nr_cpus; cpu++)\r\n__perf_evlist__munmap(evlist, cpu);\r\nreturn -1;\r\n}\r\nstatic int perf_evlist__mmap_per_thread(struct perf_evlist *evlist, int prot,\r\nint mask)\r\n{\r\nint thread;\r\nint nr_threads = thread_map__nr(evlist->threads);\r\npr_debug2("perf event ring buffer mmapped per thread\n");\r\nfor (thread = 0; thread < nr_threads; thread++) {\r\nint output = -1;\r\nif (perf_evlist__mmap_per_evsel(evlist, thread, prot, mask, 0,\r\nthread, &output))\r\ngoto out_unmap;\r\n}\r\nreturn 0;\r\nout_unmap:\r\nfor (thread = 0; thread < nr_threads; thread++)\r\n__perf_evlist__munmap(evlist, thread);\r\nreturn -1;\r\n}\r\nstatic size_t perf_evlist__mmap_size(unsigned long pages)\r\n{\r\nif (pages == UINT_MAX)\r\npages = (512 * 1024) / page_size;\r\nelse if (!is_power_of_2(pages))\r\nreturn 0;\r\nreturn (pages + 1) * page_size;\r\n}\r\nstatic long parse_pages_arg(const char *str, unsigned long min,\r\nunsigned long max)\r\n{\r\nunsigned long pages, val;\r\nstatic struct parse_tag tags[] = {\r\n{ .tag = 'B', .mult = 1 },\r\n{ .tag = 'K', .mult = 1 << 10 },\r\n{ .tag = 'M', .mult = 1 << 20 },\r\n{ .tag = 'G', .mult = 1 << 30 },\r\n{ .tag = 0 },\r\n};\r\nif (str == NULL)\r\nreturn -EINVAL;\r\nval = parse_tag_value(str, tags);\r\nif (val != (unsigned long) -1) {\r\npages = PERF_ALIGN(val, page_size) / page_size;\r\n} else {\r\nchar *eptr;\r\npages = strtoul(str, &eptr, 10);\r\nif (*eptr != '\0')\r\nreturn -EINVAL;\r\n}\r\nif (pages == 0 && min == 0) {\r\n} else if (!is_power_of_2(pages)) {\r\npages = next_pow2_l(pages);\r\nif (!pages)\r\nreturn -EINVAL;\r\npr_info("rounding mmap pages size to %lu bytes (%lu pages)\n",\r\npages * page_size, pages);\r\n}\r\nif (pages > max)\r\nreturn -EINVAL;\r\nreturn pages;\r\n}\r\nint perf_evlist__parse_mmap_pages(const struct option *opt, const char *str,\r\nint unset __maybe_unused)\r\n{\r\nunsigned int *mmap_pages = opt->value;\r\nunsigned long max = UINT_MAX;\r\nlong pages;\r\nif (max > SIZE_MAX / page_size)\r\nmax = SIZE_MAX / page_size;\r\npages = parse_pages_arg(str, 1, max);\r\nif (pages < 0) {\r\npr_err("Invalid argument for --mmap_pages/-m\n");\r\nreturn -1;\r\n}\r\n*mmap_pages = pages;\r\nreturn 0;\r\n}\r\nint perf_evlist__mmap(struct perf_evlist *evlist, unsigned int pages,\r\nbool overwrite)\r\n{\r\nstruct perf_evsel *evsel;\r\nconst struct cpu_map *cpus = evlist->cpus;\r\nconst struct thread_map *threads = evlist->threads;\r\nint prot = PROT_READ | (overwrite ? 0 : PROT_WRITE), mask;\r\nif (evlist->mmap == NULL && perf_evlist__alloc_mmap(evlist) < 0)\r\nreturn -ENOMEM;\r\nif (evlist->pollfd == NULL && perf_evlist__alloc_pollfd(evlist) < 0)\r\nreturn -ENOMEM;\r\nevlist->overwrite = overwrite;\r\nevlist->mmap_len = perf_evlist__mmap_size(pages);\r\npr_debug("mmap size %zuB\n", evlist->mmap_len);\r\nmask = evlist->mmap_len - page_size - 1;\r\nevlist__for_each(evlist, evsel) {\r\nif ((evsel->attr.read_format & PERF_FORMAT_ID) &&\r\nevsel->sample_id == NULL &&\r\nperf_evsel__alloc_id(evsel, cpu_map__nr(cpus), threads->nr) < 0)\r\nreturn -ENOMEM;\r\n}\r\nif (cpu_map__empty(cpus))\r\nreturn perf_evlist__mmap_per_thread(evlist, prot, mask);\r\nreturn perf_evlist__mmap_per_cpu(evlist, prot, mask);\r\n}\r\nint perf_evlist__create_maps(struct perf_evlist *evlist, struct target *target)\r\n{\r\nevlist->threads = thread_map__new_str(target->pid, target->tid,\r\ntarget->uid);\r\nif (evlist->threads == NULL)\r\nreturn -1;\r\nif (target__uses_dummy_map(target))\r\nevlist->cpus = cpu_map__dummy_new();\r\nelse\r\nevlist->cpus = cpu_map__new(target->cpu_list);\r\nif (evlist->cpus == NULL)\r\ngoto out_delete_threads;\r\nreturn 0;\r\nout_delete_threads:\r\nthread_map__delete(evlist->threads);\r\nreturn -1;\r\n}\r\nint perf_evlist__apply_filters(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *evsel;\r\nint err = 0;\r\nconst int ncpus = cpu_map__nr(evlist->cpus),\r\nnthreads = thread_map__nr(evlist->threads);\r\nevlist__for_each(evlist, evsel) {\r\nif (evsel->filter == NULL)\r\ncontinue;\r\nerr = perf_evsel__set_filter(evsel, ncpus, nthreads, evsel->filter);\r\nif (err)\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nint perf_evlist__set_filter(struct perf_evlist *evlist, const char *filter)\r\n{\r\nstruct perf_evsel *evsel;\r\nint err = 0;\r\nconst int ncpus = cpu_map__nr(evlist->cpus),\r\nnthreads = thread_map__nr(evlist->threads);\r\nevlist__for_each(evlist, evsel) {\r\nerr = perf_evsel__set_filter(evsel, ncpus, nthreads, filter);\r\nif (err)\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nbool perf_evlist__valid_sample_type(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *pos;\r\nif (evlist->nr_entries == 1)\r\nreturn true;\r\nif (evlist->id_pos < 0 || evlist->is_pos < 0)\r\nreturn false;\r\nevlist__for_each(evlist, pos) {\r\nif (pos->id_pos != evlist->id_pos ||\r\npos->is_pos != evlist->is_pos)\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nu64 __perf_evlist__combined_sample_type(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *evsel;\r\nif (evlist->combined_sample_type)\r\nreturn evlist->combined_sample_type;\r\nevlist__for_each(evlist, evsel)\r\nevlist->combined_sample_type |= evsel->attr.sample_type;\r\nreturn evlist->combined_sample_type;\r\n}\r\nu64 perf_evlist__combined_sample_type(struct perf_evlist *evlist)\r\n{\r\nevlist->combined_sample_type = 0;\r\nreturn __perf_evlist__combined_sample_type(evlist);\r\n}\r\nbool perf_evlist__valid_read_format(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *first = perf_evlist__first(evlist), *pos = first;\r\nu64 read_format = first->attr.read_format;\r\nu64 sample_type = first->attr.sample_type;\r\nevlist__for_each(evlist, pos) {\r\nif (read_format != pos->attr.read_format)\r\nreturn false;\r\n}\r\nif ((sample_type & PERF_SAMPLE_READ) &&\r\n!(read_format & PERF_FORMAT_ID)) {\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nu64 perf_evlist__read_format(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *first = perf_evlist__first(evlist);\r\nreturn first->attr.read_format;\r\n}\r\nu16 perf_evlist__id_hdr_size(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *first = perf_evlist__first(evlist);\r\nstruct perf_sample *data;\r\nu64 sample_type;\r\nu16 size = 0;\r\nif (!first->attr.sample_id_all)\r\ngoto out;\r\nsample_type = first->attr.sample_type;\r\nif (sample_type & PERF_SAMPLE_TID)\r\nsize += sizeof(data->tid) * 2;\r\nif (sample_type & PERF_SAMPLE_TIME)\r\nsize += sizeof(data->time);\r\nif (sample_type & PERF_SAMPLE_ID)\r\nsize += sizeof(data->id);\r\nif (sample_type & PERF_SAMPLE_STREAM_ID)\r\nsize += sizeof(data->stream_id);\r\nif (sample_type & PERF_SAMPLE_CPU)\r\nsize += sizeof(data->cpu) * 2;\r\nif (sample_type & PERF_SAMPLE_IDENTIFIER)\r\nsize += sizeof(data->id);\r\nout:\r\nreturn size;\r\n}\r\nbool perf_evlist__valid_sample_id_all(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *first = perf_evlist__first(evlist), *pos = first;\r\nevlist__for_each_continue(evlist, pos) {\r\nif (first->attr.sample_id_all != pos->attr.sample_id_all)\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nbool perf_evlist__sample_id_all(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *first = perf_evlist__first(evlist);\r\nreturn first->attr.sample_id_all;\r\n}\r\nvoid perf_evlist__set_selected(struct perf_evlist *evlist,\r\nstruct perf_evsel *evsel)\r\n{\r\nevlist->selected = evsel;\r\n}\r\nvoid perf_evlist__close(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *evsel;\r\nint ncpus = cpu_map__nr(evlist->cpus);\r\nint nthreads = thread_map__nr(evlist->threads);\r\nint n;\r\nevlist__for_each_reverse(evlist, evsel) {\r\nn = evsel->cpus ? evsel->cpus->nr : ncpus;\r\nperf_evsel__close(evsel, n, nthreads);\r\n}\r\n}\r\nint perf_evlist__open(struct perf_evlist *evlist)\r\n{\r\nstruct perf_evsel *evsel;\r\nint err;\r\nperf_evlist__update_id_pos(evlist);\r\nevlist__for_each(evlist, evsel) {\r\nerr = perf_evsel__open(evsel, evlist->cpus, evlist->threads);\r\nif (err < 0)\r\ngoto out_err;\r\n}\r\nreturn 0;\r\nout_err:\r\nperf_evlist__close(evlist);\r\nerrno = -err;\r\nreturn err;\r\n}\r\nint perf_evlist__prepare_workload(struct perf_evlist *evlist, struct target *target,\r\nconst char *argv[], bool pipe_output,\r\nvoid (*exec_error)(int signo, siginfo_t *info, void *ucontext))\r\n{\r\nint child_ready_pipe[2], go_pipe[2];\r\nchar bf;\r\nif (pipe(child_ready_pipe) < 0) {\r\nperror("failed to create 'ready' pipe");\r\nreturn -1;\r\n}\r\nif (pipe(go_pipe) < 0) {\r\nperror("failed to create 'go' pipe");\r\ngoto out_close_ready_pipe;\r\n}\r\nevlist->workload.pid = fork();\r\nif (evlist->workload.pid < 0) {\r\nperror("failed to fork");\r\ngoto out_close_pipes;\r\n}\r\nif (!evlist->workload.pid) {\r\nif (pipe_output)\r\ndup2(2, 1);\r\nsignal(SIGTERM, SIG_DFL);\r\nclose(child_ready_pipe[0]);\r\nclose(go_pipe[1]);\r\nfcntl(go_pipe[0], F_SETFD, FD_CLOEXEC);\r\nclose(child_ready_pipe[1]);\r\nif (read(go_pipe[0], &bf, 1) == -1)\r\nperror("unable to read pipe");\r\nexecvp(argv[0], (char **)argv);\r\nif (exec_error) {\r\nunion sigval val;\r\nval.sival_int = errno;\r\nif (sigqueue(getppid(), SIGUSR1, val))\r\nperror(argv[0]);\r\n} else\r\nperror(argv[0]);\r\nexit(-1);\r\n}\r\nif (exec_error) {\r\nstruct sigaction act = {\r\n.sa_flags = SA_SIGINFO,\r\n.sa_sigaction = exec_error,\r\n};\r\nsigaction(SIGUSR1, &act, NULL);\r\n}\r\nif (target__none(target))\r\nevlist->threads->map[0] = evlist->workload.pid;\r\nclose(child_ready_pipe[1]);\r\nclose(go_pipe[0]);\r\nif (read(child_ready_pipe[0], &bf, 1) == -1) {\r\nperror("unable to read pipe");\r\ngoto out_close_pipes;\r\n}\r\nfcntl(go_pipe[1], F_SETFD, FD_CLOEXEC);\r\nevlist->workload.cork_fd = go_pipe[1];\r\nclose(child_ready_pipe[0]);\r\nreturn 0;\r\nout_close_pipes:\r\nclose(go_pipe[0]);\r\nclose(go_pipe[1]);\r\nout_close_ready_pipe:\r\nclose(child_ready_pipe[0]);\r\nclose(child_ready_pipe[1]);\r\nreturn -1;\r\n}\r\nint perf_evlist__start_workload(struct perf_evlist *evlist)\r\n{\r\nif (evlist->workload.cork_fd > 0) {\r\nchar bf = 0;\r\nint ret;\r\nret = write(evlist->workload.cork_fd, &bf, 1);\r\nif (ret < 0)\r\nperror("enable to write to pipe");\r\nclose(evlist->workload.cork_fd);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint perf_evlist__parse_sample(struct perf_evlist *evlist, union perf_event *event,\r\nstruct perf_sample *sample)\r\n{\r\nstruct perf_evsel *evsel = perf_evlist__event2evsel(evlist, event);\r\nif (!evsel)\r\nreturn -EFAULT;\r\nreturn perf_evsel__parse_sample(evsel, event, sample);\r\n}\r\nsize_t perf_evlist__fprintf(struct perf_evlist *evlist, FILE *fp)\r\n{\r\nstruct perf_evsel *evsel;\r\nsize_t printed = 0;\r\nevlist__for_each(evlist, evsel) {\r\nprinted += fprintf(fp, "%s%s", evsel->idx ? ", " : "",\r\nperf_evsel__name(evsel));\r\n}\r\nreturn printed + fprintf(fp, "\n");\r\n}\r\nint perf_evlist__strerror_tp(struct perf_evlist *evlist __maybe_unused,\r\nint err, char *buf, size_t size)\r\n{\r\nchar sbuf[128];\r\nswitch (err) {\r\ncase ENOENT:\r\nscnprintf(buf, size, "%s",\r\n"Error:\tUnable to find debugfs\n"\r\n"Hint:\tWas your kernel was compiled with debugfs support?\n"\r\n"Hint:\tIs the debugfs filesystem mounted?\n"\r\n"Hint:\tTry 'sudo mount -t debugfs nodev /sys/kernel/debug'");\r\nbreak;\r\ncase EACCES:\r\nscnprintf(buf, size,\r\n"Error:\tNo permissions to read %s/tracing/events/raw_syscalls\n"\r\n"Hint:\tTry 'sudo mount -o remount,mode=755 %s'\n",\r\ndebugfs_mountpoint, debugfs_mountpoint);\r\nbreak;\r\ndefault:\r\nscnprintf(buf, size, "%s", strerror_r(err, sbuf, sizeof(sbuf)));\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nint perf_evlist__strerror_open(struct perf_evlist *evlist __maybe_unused,\r\nint err, char *buf, size_t size)\r\n{\r\nint printed, value;\r\nchar sbuf[128], *emsg = strerror_r(err, sbuf, sizeof(sbuf));\r\nswitch (err) {\r\ncase EACCES:\r\ncase EPERM:\r\nprinted = scnprintf(buf, size,\r\n"Error:\t%s.\n"\r\n"Hint:\tCheck /proc/sys/kernel/perf_event_paranoid setting.", emsg);\r\nvalue = perf_event_paranoid();\r\nprinted += scnprintf(buf + printed, size - printed, "\nHint:\t");\r\nif (value >= 2) {\r\nprinted += scnprintf(buf + printed, size - printed,\r\n"For your workloads it needs to be <= 1\nHint:\t");\r\n}\r\nprinted += scnprintf(buf + printed, size - printed,\r\n"For system wide tracing it needs to be set to -1");\r\nprinted += scnprintf(buf + printed, size - printed,\r\n".\nHint:\tThe current value is %d.", value);\r\nbreak;\r\ndefault:\r\nscnprintf(buf, size, "%s", emsg);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nvoid perf_evlist__to_front(struct perf_evlist *evlist,\r\nstruct perf_evsel *move_evsel)\r\n{\r\nstruct perf_evsel *evsel, *n;\r\nLIST_HEAD(move);\r\nif (move_evsel == perf_evlist__first(evlist))\r\nreturn;\r\nevlist__for_each_safe(evlist, n, evsel) {\r\nif (evsel->leader == move_evsel->leader)\r\nlist_move_tail(&evsel->node, &move);\r\n}\r\nlist_splice(&move, &evlist->entries);\r\n}
