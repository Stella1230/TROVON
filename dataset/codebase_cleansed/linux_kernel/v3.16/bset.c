void bch_dump_bset(struct btree_keys *b, struct bset *i, unsigned set)\r\n{\r\nstruct bkey *k, *next;\r\nfor (k = i->start; k < bset_bkey_last(i); k = next) {\r\nnext = bkey_next(k);\r\nprintk(KERN_ERR "block %u key %u/%u: ", set,\r\n(unsigned) ((u64 *) k - i->d), i->keys);\r\nif (b->ops->key_dump)\r\nb->ops->key_dump(b, k);\r\nelse\r\nprintk("%llu:%llu\n", KEY_INODE(k), KEY_OFFSET(k));\r\nif (next < bset_bkey_last(i) &&\r\nbkey_cmp(k, b->ops->is_extents ?\r\n&START_KEY(next) : next) > 0)\r\nprintk(KERN_ERR "Key skipped backwards\n");\r\n}\r\n}\r\nvoid bch_dump_bucket(struct btree_keys *b)\r\n{\r\nunsigned i;\r\nconsole_lock();\r\nfor (i = 0; i <= b->nsets; i++)\r\nbch_dump_bset(b, b->set[i].data,\r\nbset_sector_offset(b, b->set[i].data));\r\nconsole_unlock();\r\n}\r\nint __bch_count_data(struct btree_keys *b)\r\n{\r\nunsigned ret = 0;\r\nstruct btree_iter iter;\r\nstruct bkey *k;\r\nif (b->ops->is_extents)\r\nfor_each_key(b, k, &iter)\r\nret += KEY_SIZE(k);\r\nreturn ret;\r\n}\r\nvoid __bch_check_keys(struct btree_keys *b, const char *fmt, ...)\r\n{\r\nva_list args;\r\nstruct bkey *k, *p = NULL;\r\nstruct btree_iter iter;\r\nconst char *err;\r\nfor_each_key(b, k, &iter) {\r\nif (b->ops->is_extents) {\r\nerr = "Keys out of order";\r\nif (p && bkey_cmp(&START_KEY(p), &START_KEY(k)) > 0)\r\ngoto bug;\r\nif (bch_ptr_invalid(b, k))\r\ncontinue;\r\nerr = "Overlapping keys";\r\nif (p && bkey_cmp(p, &START_KEY(k)) > 0)\r\ngoto bug;\r\n} else {\r\nif (bch_ptr_bad(b, k))\r\ncontinue;\r\nerr = "Duplicate keys";\r\nif (p && !bkey_cmp(p, k))\r\ngoto bug;\r\n}\r\np = k;\r\n}\r\n#if 0\r\nerr = "Key larger than btree node key";\r\nif (p && bkey_cmp(p, &b->key) > 0)\r\ngoto bug;\r\n#endif\r\nreturn;\r\nbug:\r\nbch_dump_bucket(b);\r\nva_start(args, fmt);\r\nvprintk(fmt, args);\r\nva_end(args);\r\npanic("bch_check_keys error: %s:\n", err);\r\n}\r\nstatic void bch_btree_iter_next_check(struct btree_iter *iter)\r\n{\r\nstruct bkey *k = iter->data->k, *next = bkey_next(k);\r\nif (next < iter->data->end &&\r\nbkey_cmp(k, iter->b->ops->is_extents ?\r\n&START_KEY(next) : next) > 0) {\r\nbch_dump_bucket(iter->b);\r\npanic("Key skipped backwards\n");\r\n}\r\n}\r\nstatic inline void bch_btree_iter_next_check(struct btree_iter *iter) {}\r\nint __bch_keylist_realloc(struct keylist *l, unsigned u64s)\r\n{\r\nsize_t oldsize = bch_keylist_nkeys(l);\r\nsize_t newsize = oldsize + u64s;\r\nuint64_t *old_keys = l->keys_p == l->inline_keys ? NULL : l->keys_p;\r\nuint64_t *new_keys;\r\nnewsize = roundup_pow_of_two(newsize);\r\nif (newsize <= KEYLIST_INLINE ||\r\nroundup_pow_of_two(oldsize) == newsize)\r\nreturn 0;\r\nnew_keys = krealloc(old_keys, sizeof(uint64_t) * newsize, GFP_NOIO);\r\nif (!new_keys)\r\nreturn -ENOMEM;\r\nif (!old_keys)\r\nmemcpy(new_keys, l->inline_keys, sizeof(uint64_t) * oldsize);\r\nl->keys_p = new_keys;\r\nl->top_p = new_keys + oldsize;\r\nreturn 0;\r\n}\r\nstruct bkey *bch_keylist_pop(struct keylist *l)\r\n{\r\nstruct bkey *k = l->keys;\r\nif (k == l->top)\r\nreturn NULL;\r\nwhile (bkey_next(k) != l->top)\r\nk = bkey_next(k);\r\nreturn l->top = k;\r\n}\r\nvoid bch_keylist_pop_front(struct keylist *l)\r\n{\r\nl->top_p -= bkey_u64s(l->keys);\r\nmemmove(l->keys,\r\nbkey_next(l->keys),\r\nbch_keylist_bytes(l));\r\n}\r\nvoid bch_bkey_copy_single_ptr(struct bkey *dest, const struct bkey *src,\r\nunsigned i)\r\n{\r\nBUG_ON(i > KEY_PTRS(src));\r\nmemcpy(dest, src, 2 * sizeof(uint64_t));\r\ndest->ptr[0] = src->ptr[i];\r\nSET_KEY_PTRS(dest, 1);\r\nSET_KEY_CSUM(dest, 0);\r\n}\r\nbool __bch_cut_front(const struct bkey *where, struct bkey *k)\r\n{\r\nunsigned i, len = 0;\r\nif (bkey_cmp(where, &START_KEY(k)) <= 0)\r\nreturn false;\r\nif (bkey_cmp(where, k) < 0)\r\nlen = KEY_OFFSET(k) - KEY_OFFSET(where);\r\nelse\r\nbkey_copy_key(k, where);\r\nfor (i = 0; i < KEY_PTRS(k); i++)\r\nSET_PTR_OFFSET(k, i, PTR_OFFSET(k, i) + KEY_SIZE(k) - len);\r\nBUG_ON(len > KEY_SIZE(k));\r\nSET_KEY_SIZE(k, len);\r\nreturn true;\r\n}\r\nbool __bch_cut_back(const struct bkey *where, struct bkey *k)\r\n{\r\nunsigned len = 0;\r\nif (bkey_cmp(where, k) >= 0)\r\nreturn false;\r\nBUG_ON(KEY_INODE(where) != KEY_INODE(k));\r\nif (bkey_cmp(where, &START_KEY(k)) > 0)\r\nlen = KEY_OFFSET(where) - KEY_START(k);\r\nbkey_copy_key(k, where);\r\nBUG_ON(len > KEY_SIZE(k));\r\nSET_KEY_SIZE(k, len);\r\nreturn true;\r\n}\r\nstatic inline size_t btree_keys_bytes(struct btree_keys *b)\r\n{\r\nreturn PAGE_SIZE << b->page_order;\r\n}\r\nstatic inline size_t btree_keys_cachelines(struct btree_keys *b)\r\n{\r\nreturn btree_keys_bytes(b) / BSET_CACHELINE;\r\n}\r\nstatic inline size_t bset_tree_bytes(struct btree_keys *b)\r\n{\r\nreturn btree_keys_cachelines(b) * sizeof(struct bkey_float);\r\n}\r\nstatic inline size_t bset_prev_bytes(struct btree_keys *b)\r\n{\r\nreturn btree_keys_cachelines(b) * sizeof(uint8_t);\r\n}\r\nvoid bch_btree_keys_free(struct btree_keys *b)\r\n{\r\nstruct bset_tree *t = b->set;\r\nif (bset_prev_bytes(b) < PAGE_SIZE)\r\nkfree(t->prev);\r\nelse\r\nfree_pages((unsigned long) t->prev,\r\nget_order(bset_prev_bytes(b)));\r\nif (bset_tree_bytes(b) < PAGE_SIZE)\r\nkfree(t->tree);\r\nelse\r\nfree_pages((unsigned long) t->tree,\r\nget_order(bset_tree_bytes(b)));\r\nfree_pages((unsigned long) t->data, b->page_order);\r\nt->prev = NULL;\r\nt->tree = NULL;\r\nt->data = NULL;\r\n}\r\nint bch_btree_keys_alloc(struct btree_keys *b, unsigned page_order, gfp_t gfp)\r\n{\r\nstruct bset_tree *t = b->set;\r\nBUG_ON(t->data);\r\nb->page_order = page_order;\r\nt->data = (void *) __get_free_pages(gfp, b->page_order);\r\nif (!t->data)\r\ngoto err;\r\nt->tree = bset_tree_bytes(b) < PAGE_SIZE\r\n? kmalloc(bset_tree_bytes(b), gfp)\r\n: (void *) __get_free_pages(gfp, get_order(bset_tree_bytes(b)));\r\nif (!t->tree)\r\ngoto err;\r\nt->prev = bset_prev_bytes(b) < PAGE_SIZE\r\n? kmalloc(bset_prev_bytes(b), gfp)\r\n: (void *) __get_free_pages(gfp, get_order(bset_prev_bytes(b)));\r\nif (!t->prev)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\nbch_btree_keys_free(b);\r\nreturn -ENOMEM;\r\n}\r\nvoid bch_btree_keys_init(struct btree_keys *b, const struct btree_keys_ops *ops,\r\nbool *expensive_debug_checks)\r\n{\r\nunsigned i;\r\nb->ops = ops;\r\nb->expensive_debug_checks = expensive_debug_checks;\r\nb->nsets = 0;\r\nb->last_set_unwritten = 0;\r\nfor (i = 0; i < MAX_BSETS; i++)\r\nb->set[i].size = 0;\r\nfor (i = 1; i < MAX_BSETS; i++)\r\nb->set[i].data = NULL;\r\n}\r\nstatic unsigned inorder_next(unsigned j, unsigned size)\r\n{\r\nif (j * 2 + 1 < size) {\r\nj = j * 2 + 1;\r\nwhile (j * 2 < size)\r\nj *= 2;\r\n} else\r\nj >>= ffz(j) + 1;\r\nreturn j;\r\n}\r\nstatic unsigned inorder_prev(unsigned j, unsigned size)\r\n{\r\nif (j * 2 < size) {\r\nj = j * 2;\r\nwhile (j * 2 + 1 < size)\r\nj = j * 2 + 1;\r\n} else\r\nj >>= ffs(j);\r\nreturn j;\r\n}\r\nstatic unsigned __to_inorder(unsigned j, unsigned size, unsigned extra)\r\n{\r\nunsigned b = fls(j);\r\nunsigned shift = fls(size - 1) - b;\r\nj ^= 1U << (b - 1);\r\nj <<= 1;\r\nj |= 1;\r\nj <<= shift;\r\nif (j > extra)\r\nj -= (j - extra) >> 1;\r\nreturn j;\r\n}\r\nstatic unsigned to_inorder(unsigned j, struct bset_tree *t)\r\n{\r\nreturn __to_inorder(j, t->size, t->extra);\r\n}\r\nstatic unsigned __inorder_to_tree(unsigned j, unsigned size, unsigned extra)\r\n{\r\nunsigned shift;\r\nif (j > extra)\r\nj += j - extra;\r\nshift = ffs(j);\r\nj >>= shift;\r\nj |= roundup_pow_of_two(size) >> shift;\r\nreturn j;\r\n}\r\nstatic unsigned inorder_to_tree(unsigned j, struct bset_tree *t)\r\n{\r\nreturn __inorder_to_tree(j, t->size, t->extra);\r\n}\r\nstatic struct bkey *cacheline_to_bkey(struct bset_tree *t, unsigned cacheline,\r\nunsigned offset)\r\n{\r\nreturn ((void *) t->data) + cacheline * BSET_CACHELINE + offset * 8;\r\n}\r\nstatic unsigned bkey_to_cacheline(struct bset_tree *t, struct bkey *k)\r\n{\r\nreturn ((void *) k - (void *) t->data) / BSET_CACHELINE;\r\n}\r\nstatic unsigned bkey_to_cacheline_offset(struct bset_tree *t,\r\nunsigned cacheline,\r\nstruct bkey *k)\r\n{\r\nreturn (u64 *) k - (u64 *) cacheline_to_bkey(t, cacheline, 0);\r\n}\r\nstatic struct bkey *tree_to_bkey(struct bset_tree *t, unsigned j)\r\n{\r\nreturn cacheline_to_bkey(t, to_inorder(j, t), t->tree[j].m);\r\n}\r\nstatic struct bkey *tree_to_prev_bkey(struct bset_tree *t, unsigned j)\r\n{\r\nreturn (void *) (((uint64_t *) tree_to_bkey(t, j)) - t->prev[j]);\r\n}\r\nstatic struct bkey *table_to_bkey(struct bset_tree *t, unsigned cacheline)\r\n{\r\nreturn cacheline_to_bkey(t, cacheline, t->prev[cacheline]);\r\n}\r\nstatic inline uint64_t shrd128(uint64_t high, uint64_t low, uint8_t shift)\r\n{\r\nlow >>= shift;\r\nlow |= (high << 1) << (63U - shift);\r\nreturn low;\r\n}\r\nstatic inline unsigned bfloat_mantissa(const struct bkey *k,\r\nstruct bkey_float *f)\r\n{\r\nconst uint64_t *p = &k->low - (f->exponent >> 6);\r\nreturn shrd128(p[-1], p[0], f->exponent & 63) & BKEY_MANTISSA_MASK;\r\n}\r\nstatic void make_bfloat(struct bset_tree *t, unsigned j)\r\n{\r\nstruct bkey_float *f = &t->tree[j];\r\nstruct bkey *m = tree_to_bkey(t, j);\r\nstruct bkey *p = tree_to_prev_bkey(t, j);\r\nstruct bkey *l = is_power_of_2(j)\r\n? t->data->start\r\n: tree_to_prev_bkey(t, j >> ffs(j));\r\nstruct bkey *r = is_power_of_2(j + 1)\r\n? bset_bkey_idx(t->data, t->data->keys - bkey_u64s(&t->end))\r\n: tree_to_bkey(t, j >> (ffz(j) + 1));\r\nBUG_ON(m < l || m > r);\r\nBUG_ON(bkey_next(p) != m);\r\nif (KEY_INODE(l) != KEY_INODE(r))\r\nf->exponent = fls64(KEY_INODE(r) ^ KEY_INODE(l)) + 64;\r\nelse\r\nf->exponent = fls64(r->low ^ l->low);\r\nf->exponent = max_t(int, f->exponent - BKEY_MANTISSA_BITS, 0);\r\nif (bfloat_mantissa(m, f) != bfloat_mantissa(p, f))\r\nf->mantissa = bfloat_mantissa(m, f) - 1;\r\nelse\r\nf->exponent = 127;\r\n}\r\nstatic void bset_alloc_tree(struct btree_keys *b, struct bset_tree *t)\r\n{\r\nif (t != b->set) {\r\nunsigned j = roundup(t[-1].size,\r\n64 / sizeof(struct bkey_float));\r\nt->tree = t[-1].tree + j;\r\nt->prev = t[-1].prev + j;\r\n}\r\nwhile (t < b->set + MAX_BSETS)\r\nt++->size = 0;\r\n}\r\nstatic void bch_bset_build_unwritten_tree(struct btree_keys *b)\r\n{\r\nstruct bset_tree *t = bset_tree_last(b);\r\nBUG_ON(b->last_set_unwritten);\r\nb->last_set_unwritten = 1;\r\nbset_alloc_tree(b, t);\r\nif (t->tree != b->set->tree + btree_keys_cachelines(b)) {\r\nt->prev[0] = bkey_to_cacheline_offset(t, 0, t->data->start);\r\nt->size = 1;\r\n}\r\n}\r\nvoid bch_bset_init_next(struct btree_keys *b, struct bset *i, uint64_t magic)\r\n{\r\nif (i != b->set->data) {\r\nb->set[++b->nsets].data = i;\r\ni->seq = b->set->data->seq;\r\n} else\r\nget_random_bytes(&i->seq, sizeof(uint64_t));\r\ni->magic = magic;\r\ni->version = 0;\r\ni->keys = 0;\r\nbch_bset_build_unwritten_tree(b);\r\n}\r\nvoid bch_bset_build_written_tree(struct btree_keys *b)\r\n{\r\nstruct bset_tree *t = bset_tree_last(b);\r\nstruct bkey *prev = NULL, *k = t->data->start;\r\nunsigned j, cacheline = 1;\r\nb->last_set_unwritten = 0;\r\nbset_alloc_tree(b, t);\r\nt->size = min_t(unsigned,\r\nbkey_to_cacheline(t, bset_bkey_last(t->data)),\r\nb->set->tree + btree_keys_cachelines(b) - t->tree);\r\nif (t->size < 2) {\r\nt->size = 0;\r\nreturn;\r\n}\r\nt->extra = (t->size - rounddown_pow_of_two(t->size - 1)) << 1;\r\nfor (j = inorder_next(0, t->size);\r\nj;\r\nj = inorder_next(j, t->size)) {\r\nwhile (bkey_to_cacheline(t, k) < cacheline)\r\nprev = k, k = bkey_next(k);\r\nt->prev[j] = bkey_u64s(prev);\r\nt->tree[j].m = bkey_to_cacheline_offset(t, cacheline++, k);\r\n}\r\nwhile (bkey_next(k) != bset_bkey_last(t->data))\r\nk = bkey_next(k);\r\nt->end = *k;\r\nfor (j = inorder_next(0, t->size);\r\nj;\r\nj = inorder_next(j, t->size))\r\nmake_bfloat(t, j);\r\n}\r\nvoid bch_bset_fix_invalidated_key(struct btree_keys *b, struct bkey *k)\r\n{\r\nstruct bset_tree *t;\r\nunsigned inorder, j = 1;\r\nfor (t = b->set; t <= bset_tree_last(b); t++)\r\nif (k < bset_bkey_last(t->data))\r\ngoto found_set;\r\nBUG();\r\nfound_set:\r\nif (!t->size || !bset_written(b, t))\r\nreturn;\r\ninorder = bkey_to_cacheline(t, k);\r\nif (k == t->data->start)\r\ngoto fix_left;\r\nif (bkey_next(k) == bset_bkey_last(t->data)) {\r\nt->end = *k;\r\ngoto fix_right;\r\n}\r\nj = inorder_to_tree(inorder, t);\r\nif (j &&\r\nj < t->size &&\r\nk == tree_to_bkey(t, j))\r\nfix_left: do {\r\nmake_bfloat(t, j);\r\nj = j * 2;\r\n} while (j < t->size);\r\nj = inorder_to_tree(inorder + 1, t);\r\nif (j &&\r\nj < t->size &&\r\nk == tree_to_prev_bkey(t, j))\r\nfix_right: do {\r\nmake_bfloat(t, j);\r\nj = j * 2 + 1;\r\n} while (j < t->size);\r\n}\r\nstatic void bch_bset_fix_lookup_table(struct btree_keys *b,\r\nstruct bset_tree *t,\r\nstruct bkey *k)\r\n{\r\nunsigned shift = bkey_u64s(k);\r\nunsigned j = bkey_to_cacheline(t, k);\r\nif (!t->size)\r\nreturn;\r\nwhile (j < t->size &&\r\ntable_to_bkey(t, j) <= k)\r\nj++;\r\nfor (; j < t->size; j++) {\r\nt->prev[j] += shift;\r\nif (t->prev[j] > 7) {\r\nk = table_to_bkey(t, j - 1);\r\nwhile (k < cacheline_to_bkey(t, j, 0))\r\nk = bkey_next(k);\r\nt->prev[j] = bkey_to_cacheline_offset(t, j, k);\r\n}\r\n}\r\nif (t->size == b->set->tree + btree_keys_cachelines(b) - t->tree)\r\nreturn;\r\nfor (k = table_to_bkey(t, t->size - 1);\r\nk != bset_bkey_last(t->data);\r\nk = bkey_next(k))\r\nif (t->size == bkey_to_cacheline(t, k)) {\r\nt->prev[t->size] = bkey_to_cacheline_offset(t, t->size, k);\r\nt->size++;\r\n}\r\n}\r\nbool bch_bkey_try_merge(struct btree_keys *b, struct bkey *l, struct bkey *r)\r\n{\r\nif (!b->ops->key_merge)\r\nreturn false;\r\nif (!bch_bkey_equal_header(l, r) ||\r\nbkey_cmp(l, &START_KEY(r)))\r\nreturn false;\r\nreturn b->ops->key_merge(b, l, r);\r\n}\r\nvoid bch_bset_insert(struct btree_keys *b, struct bkey *where,\r\nstruct bkey *insert)\r\n{\r\nstruct bset_tree *t = bset_tree_last(b);\r\nBUG_ON(!b->last_set_unwritten);\r\nBUG_ON(bset_byte_offset(b, t->data) +\r\n__set_bytes(t->data, t->data->keys + bkey_u64s(insert)) >\r\nPAGE_SIZE << b->page_order);\r\nmemmove((uint64_t *) where + bkey_u64s(insert),\r\nwhere,\r\n(void *) bset_bkey_last(t->data) - (void *) where);\r\nt->data->keys += bkey_u64s(insert);\r\nbkey_copy(where, insert);\r\nbch_bset_fix_lookup_table(b, t, where);\r\n}\r\nunsigned bch_btree_insert_key(struct btree_keys *b, struct bkey *k,\r\nstruct bkey *replace_key)\r\n{\r\nunsigned status = BTREE_INSERT_STATUS_NO_INSERT;\r\nstruct bset *i = bset_tree_last(b)->data;\r\nstruct bkey *m, *prev = NULL;\r\nstruct btree_iter iter;\r\nBUG_ON(b->ops->is_extents && !KEY_SIZE(k));\r\nm = bch_btree_iter_init(b, &iter, b->ops->is_extents\r\n? PRECEDING_KEY(&START_KEY(k))\r\n: PRECEDING_KEY(k));\r\nif (b->ops->insert_fixup(b, k, &iter, replace_key))\r\nreturn status;\r\nstatus = BTREE_INSERT_STATUS_INSERT;\r\nwhile (m != bset_bkey_last(i) &&\r\nbkey_cmp(k, b->ops->is_extents ? &START_KEY(m) : m) > 0)\r\nprev = m, m = bkey_next(m);\r\nstatus = BTREE_INSERT_STATUS_BACK_MERGE;\r\nif (prev &&\r\nbch_bkey_try_merge(b, prev, k))\r\ngoto merged;\r\n#if 0\r\nstatus = BTREE_INSERT_STATUS_OVERWROTE;\r\nif (m != bset_bkey_last(i) &&\r\nKEY_PTRS(m) == KEY_PTRS(k) && !KEY_SIZE(m))\r\ngoto copy;\r\n#endif\r\nstatus = BTREE_INSERT_STATUS_FRONT_MERGE;\r\nif (m != bset_bkey_last(i) &&\r\nbch_bkey_try_merge(b, k, m))\r\ngoto copy;\r\nbch_bset_insert(b, m, k);\r\ncopy: bkey_copy(m, k);\r\nmerged:\r\nreturn status;\r\n}\r\nstatic struct bset_search_iter bset_search_write_set(struct bset_tree *t,\r\nconst struct bkey *search)\r\n{\r\nunsigned li = 0, ri = t->size;\r\nwhile (li + 1 != ri) {\r\nunsigned m = (li + ri) >> 1;\r\nif (bkey_cmp(table_to_bkey(t, m), search) > 0)\r\nri = m;\r\nelse\r\nli = m;\r\n}\r\nreturn (struct bset_search_iter) {\r\ntable_to_bkey(t, li),\r\nri < t->size ? table_to_bkey(t, ri) : bset_bkey_last(t->data)\r\n};\r\n}\r\nstatic struct bset_search_iter bset_search_tree(struct bset_tree *t,\r\nconst struct bkey *search)\r\n{\r\nstruct bkey *l, *r;\r\nstruct bkey_float *f;\r\nunsigned inorder, j, n = 1;\r\ndo {\r\nunsigned p = n << 4;\r\np &= ((int) (p - t->size)) >> 31;\r\nprefetch(&t->tree[p]);\r\nj = n;\r\nf = &t->tree[j];\r\nif (likely(f->exponent != 127))\r\nn = j * 2 + (((unsigned)\r\n(f->mantissa -\r\nbfloat_mantissa(search, f))) >> 31);\r\nelse\r\nn = (bkey_cmp(tree_to_bkey(t, j), search) > 0)\r\n? j * 2\r\n: j * 2 + 1;\r\n} while (n < t->size);\r\ninorder = to_inorder(j, t);\r\nif (n & 1) {\r\nl = cacheline_to_bkey(t, inorder, f->m);\r\nif (++inorder != t->size) {\r\nf = &t->tree[inorder_next(j, t->size)];\r\nr = cacheline_to_bkey(t, inorder, f->m);\r\n} else\r\nr = bset_bkey_last(t->data);\r\n} else {\r\nr = cacheline_to_bkey(t, inorder, f->m);\r\nif (--inorder) {\r\nf = &t->tree[inorder_prev(j, t->size)];\r\nl = cacheline_to_bkey(t, inorder, f->m);\r\n} else\r\nl = t->data->start;\r\n}\r\nreturn (struct bset_search_iter) {l, r};\r\n}\r\nstruct bkey *__bch_bset_search(struct btree_keys *b, struct bset_tree *t,\r\nconst struct bkey *search)\r\n{\r\nstruct bset_search_iter i;\r\nif (unlikely(!t->size)) {\r\ni.l = t->data->start;\r\ni.r = bset_bkey_last(t->data);\r\n} else if (bset_written(b, t)) {\r\nif (unlikely(bkey_cmp(search, &t->end) >= 0))\r\nreturn bset_bkey_last(t->data);\r\nif (unlikely(bkey_cmp(search, t->data->start) < 0))\r\nreturn t->data->start;\r\ni = bset_search_tree(t, search);\r\n} else {\r\nBUG_ON(!b->nsets &&\r\nt->size < bkey_to_cacheline(t, bset_bkey_last(t->data)));\r\ni = bset_search_write_set(t, search);\r\n}\r\nif (btree_keys_expensive_checks(b)) {\r\nBUG_ON(bset_written(b, t) &&\r\ni.l != t->data->start &&\r\nbkey_cmp(tree_to_prev_bkey(t,\r\ninorder_to_tree(bkey_to_cacheline(t, i.l), t)),\r\nsearch) > 0);\r\nBUG_ON(i.r != bset_bkey_last(t->data) &&\r\nbkey_cmp(i.r, search) <= 0);\r\n}\r\nwhile (likely(i.l != i.r) &&\r\nbkey_cmp(i.l, search) <= 0)\r\ni.l = bkey_next(i.l);\r\nreturn i.l;\r\n}\r\nstatic inline bool btree_iter_cmp(struct btree_iter_set l,\r\nstruct btree_iter_set r)\r\n{\r\nreturn bkey_cmp(l.k, r.k) > 0;\r\n}\r\nstatic inline bool btree_iter_end(struct btree_iter *iter)\r\n{\r\nreturn !iter->used;\r\n}\r\nvoid bch_btree_iter_push(struct btree_iter *iter, struct bkey *k,\r\nstruct bkey *end)\r\n{\r\nif (k != end)\r\nBUG_ON(!heap_add(iter,\r\n((struct btree_iter_set) { k, end }),\r\nbtree_iter_cmp));\r\n}\r\nstatic struct bkey *__bch_btree_iter_init(struct btree_keys *b,\r\nstruct btree_iter *iter,\r\nstruct bkey *search,\r\nstruct bset_tree *start)\r\n{\r\nstruct bkey *ret = NULL;\r\niter->size = ARRAY_SIZE(iter->data);\r\niter->used = 0;\r\n#ifdef CONFIG_BCACHE_DEBUG\r\niter->b = b;\r\n#endif\r\nfor (; start <= bset_tree_last(b); start++) {\r\nret = bch_bset_search(b, start, search);\r\nbch_btree_iter_push(iter, ret, bset_bkey_last(start->data));\r\n}\r\nreturn ret;\r\n}\r\nstruct bkey *bch_btree_iter_init(struct btree_keys *b,\r\nstruct btree_iter *iter,\r\nstruct bkey *search)\r\n{\r\nreturn __bch_btree_iter_init(b, iter, search, b->set);\r\n}\r\nstatic inline struct bkey *__bch_btree_iter_next(struct btree_iter *iter,\r\nbtree_iter_cmp_fn *cmp)\r\n{\r\nstruct btree_iter_set unused;\r\nstruct bkey *ret = NULL;\r\nif (!btree_iter_end(iter)) {\r\nbch_btree_iter_next_check(iter);\r\nret = iter->data->k;\r\niter->data->k = bkey_next(iter->data->k);\r\nif (iter->data->k > iter->data->end) {\r\nWARN_ONCE(1, "bset was corrupt!\n");\r\niter->data->k = iter->data->end;\r\n}\r\nif (iter->data->k == iter->data->end)\r\nheap_pop(iter, unused, cmp);\r\nelse\r\nheap_sift(iter, 0, cmp);\r\n}\r\nreturn ret;\r\n}\r\nstruct bkey *bch_btree_iter_next(struct btree_iter *iter)\r\n{\r\nreturn __bch_btree_iter_next(iter, btree_iter_cmp);\r\n}\r\nstruct bkey *bch_btree_iter_next_filter(struct btree_iter *iter,\r\nstruct btree_keys *b, ptr_filter_fn fn)\r\n{\r\nstruct bkey *ret;\r\ndo {\r\nret = bch_btree_iter_next(iter);\r\n} while (ret && fn(b, ret));\r\nreturn ret;\r\n}\r\nvoid bch_bset_sort_state_free(struct bset_sort_state *state)\r\n{\r\nif (state->pool)\r\nmempool_destroy(state->pool);\r\n}\r\nint bch_bset_sort_state_init(struct bset_sort_state *state, unsigned page_order)\r\n{\r\nspin_lock_init(&state->time.lock);\r\nstate->page_order = page_order;\r\nstate->crit_factor = int_sqrt(1 << page_order);\r\nstate->pool = mempool_create_page_pool(1, page_order);\r\nif (!state->pool)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void btree_mergesort(struct btree_keys *b, struct bset *out,\r\nstruct btree_iter *iter,\r\nbool fixup, bool remove_stale)\r\n{\r\nint i;\r\nstruct bkey *k, *last = NULL;\r\nBKEY_PADDED(k) tmp;\r\nbool (*bad)(struct btree_keys *, const struct bkey *) = remove_stale\r\n? bch_ptr_bad\r\n: bch_ptr_invalid;\r\nfor (i = iter->used / 2 - 1; i >= 0; --i)\r\nheap_sift(iter, i, b->ops->sort_cmp);\r\nwhile (!btree_iter_end(iter)) {\r\nif (b->ops->sort_fixup && fixup)\r\nk = b->ops->sort_fixup(iter, &tmp.k);\r\nelse\r\nk = NULL;\r\nif (!k)\r\nk = __bch_btree_iter_next(iter, b->ops->sort_cmp);\r\nif (bad(b, k))\r\ncontinue;\r\nif (!last) {\r\nlast = out->start;\r\nbkey_copy(last, k);\r\n} else if (!bch_bkey_try_merge(b, last, k)) {\r\nlast = bkey_next(last);\r\nbkey_copy(last, k);\r\n}\r\n}\r\nout->keys = last ? (uint64_t *) bkey_next(last) - out->d : 0;\r\npr_debug("sorted %i keys", out->keys);\r\n}\r\nstatic void __btree_sort(struct btree_keys *b, struct btree_iter *iter,\r\nunsigned start, unsigned order, bool fixup,\r\nstruct bset_sort_state *state)\r\n{\r\nuint64_t start_time;\r\nbool used_mempool = false;\r\nstruct bset *out = (void *) __get_free_pages(__GFP_NOWARN|GFP_NOIO,\r\norder);\r\nif (!out) {\r\nstruct page *outp;\r\nBUG_ON(order > state->page_order);\r\noutp = mempool_alloc(state->pool, GFP_NOIO);\r\nout = page_address(outp);\r\nused_mempool = true;\r\norder = state->page_order;\r\n}\r\nstart_time = local_clock();\r\nbtree_mergesort(b, out, iter, fixup, false);\r\nb->nsets = start;\r\nif (!start && order == b->page_order) {\r\nout->magic = b->set->data->magic;\r\nout->seq = b->set->data->seq;\r\nout->version = b->set->data->version;\r\nswap(out, b->set->data);\r\n} else {\r\nb->set[start].data->keys = out->keys;\r\nmemcpy(b->set[start].data->start, out->start,\r\n(void *) bset_bkey_last(out) - (void *) out->start);\r\n}\r\nif (used_mempool)\r\nmempool_free(virt_to_page(out), state->pool);\r\nelse\r\nfree_pages((unsigned long) out, order);\r\nbch_bset_build_written_tree(b);\r\nif (!start)\r\nbch_time_stats_update(&state->time, start_time);\r\n}\r\nvoid bch_btree_sort_partial(struct btree_keys *b, unsigned start,\r\nstruct bset_sort_state *state)\r\n{\r\nsize_t order = b->page_order, keys = 0;\r\nstruct btree_iter iter;\r\nint oldsize = bch_count_data(b);\r\n__bch_btree_iter_init(b, &iter, NULL, &b->set[start]);\r\nif (start) {\r\nunsigned i;\r\nfor (i = start; i <= b->nsets; i++)\r\nkeys += b->set[i].data->keys;\r\norder = get_order(__set_bytes(b->set->data, keys));\r\n}\r\n__btree_sort(b, &iter, start, order, false, state);\r\nEBUG_ON(oldsize >= 0 && bch_count_data(b) != oldsize);\r\n}\r\nvoid bch_btree_sort_and_fix_extents(struct btree_keys *b,\r\nstruct btree_iter *iter,\r\nstruct bset_sort_state *state)\r\n{\r\n__btree_sort(b, iter, 0, b->page_order, true, state);\r\n}\r\nvoid bch_btree_sort_into(struct btree_keys *b, struct btree_keys *new,\r\nstruct bset_sort_state *state)\r\n{\r\nuint64_t start_time = local_clock();\r\nstruct btree_iter iter;\r\nbch_btree_iter_init(b, &iter, NULL);\r\nbtree_mergesort(b, new->set->data, &iter, false, true);\r\nbch_time_stats_update(&state->time, start_time);\r\nnew->set->size = 0;\r\n}\r\nvoid bch_btree_sort_lazy(struct btree_keys *b, struct bset_sort_state *state)\r\n{\r\nunsigned crit = SORT_CRIT;\r\nint i;\r\nif (!b->nsets)\r\ngoto out;\r\nfor (i = b->nsets - 1; i >= 0; --i) {\r\ncrit *= state->crit_factor;\r\nif (b->set[i].data->keys < crit) {\r\nbch_btree_sort_partial(b, i, state);\r\nreturn;\r\n}\r\n}\r\nif (b->nsets + 1 == MAX_BSETS) {\r\nbch_btree_sort(b, state);\r\nreturn;\r\n}\r\nout:\r\nbch_bset_build_written_tree(b);\r\n}\r\nvoid bch_btree_keys_stats(struct btree_keys *b, struct bset_stats *stats)\r\n{\r\nunsigned i;\r\nfor (i = 0; i <= b->nsets; i++) {\r\nstruct bset_tree *t = &b->set[i];\r\nsize_t bytes = t->data->keys * sizeof(uint64_t);\r\nsize_t j;\r\nif (bset_written(b, t)) {\r\nstats->sets_written++;\r\nstats->bytes_written += bytes;\r\nstats->floats += t->size - 1;\r\nfor (j = 1; j < t->size; j++)\r\nif (t->tree[j].exponent == 127)\r\nstats->failed++;\r\n} else {\r\nstats->sets_unwritten++;\r\nstats->bytes_unwritten += bytes;\r\n}\r\n}\r\n}
