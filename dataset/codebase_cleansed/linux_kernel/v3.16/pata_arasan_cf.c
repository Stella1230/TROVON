static void cf_dumpregs(struct arasan_cf_dev *acdev)\r\n{\r\nstruct device *dev = acdev->host->dev;\r\ndev_dbg(dev, ": =========== REGISTER DUMP ===========");\r\ndev_dbg(dev, ": CFI_STS: %x", readl(acdev->vbase + CFI_STS));\r\ndev_dbg(dev, ": IRQ_STS: %x", readl(acdev->vbase + IRQ_STS));\r\ndev_dbg(dev, ": IRQ_EN: %x", readl(acdev->vbase + IRQ_EN));\r\ndev_dbg(dev, ": OP_MODE: %x", readl(acdev->vbase + OP_MODE));\r\ndev_dbg(dev, ": CLK_CFG: %x", readl(acdev->vbase + CLK_CFG));\r\ndev_dbg(dev, ": TM_CFG: %x", readl(acdev->vbase + TM_CFG));\r\ndev_dbg(dev, ": XFER_CTR: %x", readl(acdev->vbase + XFER_CTR));\r\ndev_dbg(dev, ": GIRQ_STS: %x", readl(acdev->vbase + GIRQ_STS));\r\ndev_dbg(dev, ": GIRQ_STS_EN: %x", readl(acdev->vbase + GIRQ_STS_EN));\r\ndev_dbg(dev, ": GIRQ_SGN_EN: %x", readl(acdev->vbase + GIRQ_SGN_EN));\r\ndev_dbg(dev, ": =====================================");\r\n}\r\nstatic void cf_ginterrupt_enable(struct arasan_cf_dev *acdev, bool enable)\r\n{\r\nwritel(enable, acdev->vbase + GIRQ_STS_EN);\r\nwritel(enable, acdev->vbase + GIRQ_SGN_EN);\r\n}\r\nstatic inline void\r\ncf_interrupt_enable(struct arasan_cf_dev *acdev, u32 mask, bool enable)\r\n{\r\nu32 val = readl(acdev->vbase + IRQ_EN);\r\nif (enable) {\r\nwritel(mask, acdev->vbase + IRQ_STS);\r\nwritel(val | mask, acdev->vbase + IRQ_EN);\r\n} else\r\nwritel(val & ~mask, acdev->vbase + IRQ_EN);\r\n}\r\nstatic inline void cf_card_reset(struct arasan_cf_dev *acdev)\r\n{\r\nu32 val = readl(acdev->vbase + OP_MODE);\r\nwritel(val | CARD_RESET, acdev->vbase + OP_MODE);\r\nudelay(200);\r\nwritel(val & ~CARD_RESET, acdev->vbase + OP_MODE);\r\n}\r\nstatic inline void cf_ctrl_reset(struct arasan_cf_dev *acdev)\r\n{\r\nwritel(readl(acdev->vbase + OP_MODE) & ~CFHOST_ENB,\r\nacdev->vbase + OP_MODE);\r\nwritel(readl(acdev->vbase + OP_MODE) | CFHOST_ENB,\r\nacdev->vbase + OP_MODE);\r\n}\r\nstatic void cf_card_detect(struct arasan_cf_dev *acdev, bool hotplugged)\r\n{\r\nstruct ata_port *ap = acdev->host->ports[0];\r\nstruct ata_eh_info *ehi = &ap->link.eh_info;\r\nu32 val = readl(acdev->vbase + CFI_STS);\r\nif (!(val & (CARD_DETECT1 | CARD_DETECT2))) {\r\nif (acdev->card_present)\r\nreturn;\r\nacdev->card_present = 1;\r\ncf_card_reset(acdev);\r\n} else {\r\nif (!acdev->card_present)\r\nreturn;\r\nacdev->card_present = 0;\r\n}\r\nif (hotplugged) {\r\nata_ehi_hotplugged(ehi);\r\nata_port_freeze(ap);\r\n}\r\n}\r\nstatic int cf_init(struct arasan_cf_dev *acdev)\r\n{\r\nstruct arasan_cf_pdata *pdata = dev_get_platdata(acdev->host->dev);\r\nunsigned int if_clk;\r\nunsigned long flags;\r\nint ret = 0;\r\nret = clk_prepare_enable(acdev->clk);\r\nif (ret) {\r\ndev_dbg(acdev->host->dev, "clock enable failed");\r\nreturn ret;\r\n}\r\nret = clk_set_rate(acdev->clk, 166000000);\r\nif (ret) {\r\ndev_warn(acdev->host->dev, "clock set rate failed");\r\nclk_disable_unprepare(acdev->clk);\r\nreturn ret;\r\n}\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nif_clk = CF_IF_CLK_166M;\r\nif (pdata && pdata->cf_if_clk <= CF_IF_CLK_200M)\r\nif_clk = pdata->cf_if_clk;\r\nwritel(if_clk, acdev->vbase + CLK_CFG);\r\nwritel(TRUE_IDE_MODE | CFHOST_ENB, acdev->vbase + OP_MODE);\r\ncf_interrupt_enable(acdev, CARD_DETECT_IRQ, 1);\r\ncf_ginterrupt_enable(acdev, 1);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void cf_exit(struct arasan_cf_dev *acdev)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\ncf_ginterrupt_enable(acdev, 0);\r\ncf_interrupt_enable(acdev, TRUE_IDE_IRQS, 0);\r\ncf_card_reset(acdev);\r\nwritel(readl(acdev->vbase + OP_MODE) & ~CFHOST_ENB,\r\nacdev->vbase + OP_MODE);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\nclk_disable_unprepare(acdev->clk);\r\n}\r\nstatic void dma_callback(void *dev)\r\n{\r\nstruct arasan_cf_dev *acdev = dev;\r\ncomplete(&acdev->dma_completion);\r\n}\r\nstatic inline void dma_complete(struct arasan_cf_dev *acdev)\r\n{\r\nstruct ata_queued_cmd *qc = acdev->qc;\r\nunsigned long flags;\r\nacdev->qc = NULL;\r\nata_sff_interrupt(acdev->irq, acdev->host);\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nif (unlikely(qc->err_mask) && ata_is_dma(qc->tf.protocol))\r\nata_ehi_push_desc(&qc->ap->link.eh_info, "DMA Failed: Timeout");\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\n}\r\nstatic inline int wait4buf(struct arasan_cf_dev *acdev)\r\n{\r\nif (!wait_for_completion_timeout(&acdev->cf_completion, TIMEOUT)) {\r\nu32 rw = acdev->qc->tf.flags & ATA_TFLAG_WRITE;\r\ndev_err(acdev->host->dev, "%s TimeOut", rw ? "write" : "read");\r\nreturn -ETIMEDOUT;\r\n}\r\nif (acdev->dma_status & ATA_DMA_ERR)\r\nreturn -EAGAIN;\r\nreturn 0;\r\n}\r\nstatic int\r\ndma_xfer(struct arasan_cf_dev *acdev, dma_addr_t src, dma_addr_t dest, u32 len)\r\n{\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct dma_chan *chan = acdev->dma_chan;\r\ndma_cookie_t cookie;\r\nunsigned long flags = DMA_PREP_INTERRUPT;\r\nint ret = 0;\r\ntx = chan->device->device_prep_dma_memcpy(chan, dest, src, len, flags);\r\nif (!tx) {\r\ndev_err(acdev->host->dev, "device_prep_dma_memcpy failed\n");\r\nreturn -EAGAIN;\r\n}\r\ntx->callback = dma_callback;\r\ntx->callback_param = acdev;\r\ncookie = tx->tx_submit(tx);\r\nret = dma_submit_error(cookie);\r\nif (ret) {\r\ndev_err(acdev->host->dev, "dma_submit_error\n");\r\nreturn ret;\r\n}\r\nchan->device->device_issue_pending(chan);\r\nif (!wait_for_completion_timeout(&acdev->dma_completion, TIMEOUT)) {\r\nchan->device->device_control(chan, DMA_TERMINATE_ALL, 0);\r\ndev_err(acdev->host->dev, "wait_for_completion_timeout\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nreturn ret;\r\n}\r\nstatic int sg_xfer(struct arasan_cf_dev *acdev, struct scatterlist *sg)\r\n{\r\ndma_addr_t dest = 0, src = 0;\r\nu32 xfer_cnt, sglen, dma_len, xfer_ctr;\r\nu32 write = acdev->qc->tf.flags & ATA_TFLAG_WRITE;\r\nunsigned long flags;\r\nint ret = 0;\r\nsglen = sg_dma_len(sg);\r\nif (write) {\r\nsrc = sg_dma_address(sg);\r\ndest = acdev->pbase + EXT_WRITE_PORT;\r\n} else {\r\ndest = sg_dma_address(sg);\r\nsrc = acdev->pbase + EXT_READ_PORT;\r\n}\r\nwhile (sglen) {\r\nxfer_cnt = min(sglen, MAX_XFER_COUNT);\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nxfer_ctr = readl(acdev->vbase + XFER_CTR) &\r\n~XFER_COUNT_MASK;\r\nwritel(xfer_ctr | xfer_cnt | XFER_START,\r\nacdev->vbase + XFER_CTR);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\nwhile (xfer_cnt) {\r\nif (!write) {\r\nret = wait4buf(acdev);\r\nif (ret)\r\ngoto fail;\r\n}\r\ndma_len = min(xfer_cnt, FIFO_SIZE);\r\nret = dma_xfer(acdev, src, dest, dma_len);\r\nif (ret) {\r\ndev_err(acdev->host->dev, "dma failed");\r\ngoto fail;\r\n}\r\nif (write)\r\nsrc += dma_len;\r\nelse\r\ndest += dma_len;\r\nsglen -= dma_len;\r\nxfer_cnt -= dma_len;\r\nif (write) {\r\nret = wait4buf(acdev);\r\nif (ret)\r\ngoto fail;\r\n}\r\n}\r\n}\r\nfail:\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nwritel(readl(acdev->vbase + XFER_CTR) & ~XFER_START,\r\nacdev->vbase + XFER_CTR);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void data_xfer(struct work_struct *work)\r\n{\r\nstruct arasan_cf_dev *acdev = container_of(work, struct arasan_cf_dev,\r\nwork);\r\nstruct ata_queued_cmd *qc = acdev->qc;\r\nstruct scatterlist *sg;\r\nunsigned long flags;\r\nu32 temp;\r\nint ret = 0;\r\nacdev->dma_chan = dma_request_slave_channel(acdev->host->dev, "data");\r\nif (!acdev->dma_chan) {\r\ndev_err(acdev->host->dev, "Unable to get dma_chan\n");\r\ngoto chan_request_fail;\r\n}\r\nfor_each_sg(qc->sg, sg, qc->n_elem, temp) {\r\nret = sg_xfer(acdev, sg);\r\nif (ret)\r\nbreak;\r\n}\r\ndma_release_channel(acdev->dma_chan);\r\nif (!ret) {\r\nu32 status;\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nstatus = ioread8(qc->ap->ioaddr.altstatus_addr);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\nif (status & (ATA_BUSY | ATA_DRQ)) {\r\nata_sff_queue_delayed_work(&acdev->dwork, 1);\r\nreturn;\r\n}\r\ngoto sff_intr;\r\n}\r\ncf_dumpregs(acdev);\r\nchan_request_fail:\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nqc->err_mask |= AC_ERR_HOST_BUS;\r\nqc->ap->hsm_task_state = HSM_ST_ERR;\r\ncf_ctrl_reset(acdev);\r\nspin_unlock_irqrestore(qc->ap->lock, flags);\r\nsff_intr:\r\ndma_complete(acdev);\r\n}\r\nstatic void delayed_finish(struct work_struct *work)\r\n{\r\nstruct arasan_cf_dev *acdev = container_of(work, struct arasan_cf_dev,\r\ndwork.work);\r\nstruct ata_queued_cmd *qc = acdev->qc;\r\nunsigned long flags;\r\nu8 status;\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nstatus = ioread8(qc->ap->ioaddr.altstatus_addr);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\nif (status & (ATA_BUSY | ATA_DRQ))\r\nata_sff_queue_delayed_work(&acdev->dwork, 1);\r\nelse\r\ndma_complete(acdev);\r\n}\r\nstatic irqreturn_t arasan_cf_interrupt(int irq, void *dev)\r\n{\r\nstruct arasan_cf_dev *acdev = ((struct ata_host *)dev)->private_data;\r\nunsigned long flags;\r\nu32 irqsts;\r\nirqsts = readl(acdev->vbase + GIRQ_STS);\r\nif (!(irqsts & GIRQ_CF))\r\nreturn IRQ_NONE;\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nirqsts = readl(acdev->vbase + IRQ_STS);\r\nwritel(irqsts, acdev->vbase + IRQ_STS);\r\nwritel(GIRQ_CF, acdev->vbase + GIRQ_STS);\r\nirqsts &= ~IGNORED_IRQS;\r\nif (irqsts & CARD_DETECT_IRQ) {\r\ncf_card_detect(acdev, 1);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nif (irqsts & PIO_XFER_ERR_IRQ) {\r\nacdev->dma_status = ATA_DMA_ERR;\r\nwritel(readl(acdev->vbase + XFER_CTR) & ~XFER_START,\r\nacdev->vbase + XFER_CTR);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\ncomplete(&acdev->cf_completion);\r\ndev_err(acdev->host->dev, "pio xfer err irq\n");\r\nreturn IRQ_HANDLED;\r\n}\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\nif (irqsts & BUF_AVAIL_IRQ) {\r\ncomplete(&acdev->cf_completion);\r\nreturn IRQ_HANDLED;\r\n}\r\nif (irqsts & XFER_DONE_IRQ) {\r\nstruct ata_queued_cmd *qc = acdev->qc;\r\nif (qc->tf.flags & ATA_TFLAG_WRITE)\r\ncomplete(&acdev->cf_completion);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void arasan_cf_freeze(struct ata_port *ap)\r\n{\r\nstruct arasan_cf_dev *acdev = ap->host->private_data;\r\nwritel(readl(acdev->vbase + XFER_CTR) & ~XFER_START,\r\nacdev->vbase + XFER_CTR);\r\ncf_ctrl_reset(acdev);\r\nacdev->dma_status = ATA_DMA_ERR;\r\nata_sff_dma_pause(ap);\r\nata_sff_freeze(ap);\r\n}\r\nstatic void arasan_cf_error_handler(struct ata_port *ap)\r\n{\r\nstruct arasan_cf_dev *acdev = ap->host->private_data;\r\ncancel_work_sync(&acdev->work);\r\ncancel_delayed_work_sync(&acdev->dwork);\r\nreturn ata_sff_error_handler(ap);\r\n}\r\nstatic void arasan_cf_dma_start(struct arasan_cf_dev *acdev)\r\n{\r\nstruct ata_queued_cmd *qc = acdev->qc;\r\nstruct ata_port *ap = qc->ap;\r\nstruct ata_taskfile *tf = &qc->tf;\r\nu32 xfer_ctr = readl(acdev->vbase + XFER_CTR) & ~XFER_DIR_MASK;\r\nu32 write = tf->flags & ATA_TFLAG_WRITE;\r\nxfer_ctr |= write ? XFER_WRITE : XFER_READ;\r\nwritel(xfer_ctr, acdev->vbase + XFER_CTR);\r\nap->ops->sff_exec_command(ap, tf);\r\nata_sff_queue_work(&acdev->work);\r\n}\r\nstatic unsigned int arasan_cf_qc_issue(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct arasan_cf_dev *acdev = ap->host->private_data;\r\nif (!ata_is_dma(qc->tf.protocol))\r\nreturn ata_sff_qc_issue(qc);\r\nata_wait_idle(ap);\r\nata_sff_dev_select(ap, qc->dev->devno);\r\nata_wait_idle(ap);\r\nswitch (qc->tf.protocol) {\r\ncase ATA_PROT_DMA:\r\nWARN_ON_ONCE(qc->tf.flags & ATA_TFLAG_POLLING);\r\nap->ops->sff_tf_load(ap, &qc->tf);\r\nacdev->dma_status = 0;\r\nacdev->qc = qc;\r\narasan_cf_dma_start(acdev);\r\nap->hsm_task_state = HSM_ST_LAST;\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\nreturn AC_ERR_SYSTEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void arasan_cf_set_piomode(struct ata_port *ap, struct ata_device *adev)\r\n{\r\nstruct arasan_cf_dev *acdev = ap->host->private_data;\r\nu8 pio = adev->pio_mode - XFER_PIO_0;\r\nunsigned long flags;\r\nu32 val;\r\nif (pio > 6) {\r\ndev_err(ap->dev, "Unknown PIO mode\n");\r\nreturn;\r\n}\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nval = readl(acdev->vbase + OP_MODE) &\r\n~(ULTRA_DMA_ENB | MULTI_WORD_DMA_ENB | DRQ_BLOCK_SIZE_MASK);\r\nwritel(val, acdev->vbase + OP_MODE);\r\nval = readl(acdev->vbase + TM_CFG) & ~TRUEIDE_PIO_TIMING_MASK;\r\nval |= pio << TRUEIDE_PIO_TIMING_SHIFT;\r\nwritel(val, acdev->vbase + TM_CFG);\r\ncf_interrupt_enable(acdev, BUF_AVAIL_IRQ | XFER_DONE_IRQ, 0);\r\ncf_interrupt_enable(acdev, PIO_XFER_ERR_IRQ, 1);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\n}\r\nstatic void arasan_cf_set_dmamode(struct ata_port *ap, struct ata_device *adev)\r\n{\r\nstruct arasan_cf_dev *acdev = ap->host->private_data;\r\nu32 opmode, tmcfg, dma_mode = adev->dma_mode;\r\nunsigned long flags;\r\nspin_lock_irqsave(&acdev->host->lock, flags);\r\nopmode = readl(acdev->vbase + OP_MODE) &\r\n~(MULTI_WORD_DMA_ENB | ULTRA_DMA_ENB);\r\ntmcfg = readl(acdev->vbase + TM_CFG);\r\nif ((dma_mode >= XFER_UDMA_0) && (dma_mode <= XFER_UDMA_6)) {\r\nopmode |= ULTRA_DMA_ENB;\r\ntmcfg &= ~ULTRA_DMA_TIMING_MASK;\r\ntmcfg |= (dma_mode - XFER_UDMA_0) << ULTRA_DMA_TIMING_SHIFT;\r\n} else if ((dma_mode >= XFER_MW_DMA_0) && (dma_mode <= XFER_MW_DMA_4)) {\r\nopmode |= MULTI_WORD_DMA_ENB;\r\ntmcfg &= ~TRUEIDE_MWORD_DMA_TIMING_MASK;\r\ntmcfg |= (dma_mode - XFER_MW_DMA_0) <<\r\nTRUEIDE_MWORD_DMA_TIMING_SHIFT;\r\n} else {\r\ndev_err(ap->dev, "Unknown DMA mode\n");\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\nreturn;\r\n}\r\nwritel(opmode, acdev->vbase + OP_MODE);\r\nwritel(tmcfg, acdev->vbase + TM_CFG);\r\nwritel(DMA_XFER_MODE, acdev->vbase + XFER_CTR);\r\ncf_interrupt_enable(acdev, PIO_XFER_ERR_IRQ, 0);\r\ncf_interrupt_enable(acdev, BUF_AVAIL_IRQ | XFER_DONE_IRQ, 1);\r\nspin_unlock_irqrestore(&acdev->host->lock, flags);\r\n}\r\nstatic int arasan_cf_probe(struct platform_device *pdev)\r\n{\r\nstruct arasan_cf_dev *acdev;\r\nstruct arasan_cf_pdata *pdata = dev_get_platdata(&pdev->dev);\r\nstruct ata_host *host;\r\nstruct ata_port *ap;\r\nstruct resource *res;\r\nu32 quirk;\r\nirq_handler_t irq_handler = NULL;\r\nint ret = 0;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res)\r\nreturn -EINVAL;\r\nif (!devm_request_mem_region(&pdev->dev, res->start, resource_size(res),\r\nDRIVER_NAME)) {\r\ndev_warn(&pdev->dev, "Failed to get memory region resource\n");\r\nreturn -ENOENT;\r\n}\r\nacdev = devm_kzalloc(&pdev->dev, sizeof(*acdev), GFP_KERNEL);\r\nif (!acdev) {\r\ndev_warn(&pdev->dev, "kzalloc fail\n");\r\nreturn -ENOMEM;\r\n}\r\nif (pdata)\r\nquirk = pdata->quirk;\r\nelse\r\nquirk = CF_BROKEN_UDMA;\r\nacdev->irq = platform_get_irq(pdev, 0);\r\nif (acdev->irq)\r\nirq_handler = arasan_cf_interrupt;\r\nelse\r\nquirk |= CF_BROKEN_MWDMA | CF_BROKEN_UDMA;\r\nacdev->pbase = res->start;\r\nacdev->vbase = devm_ioremap_nocache(&pdev->dev, res->start,\r\nresource_size(res));\r\nif (!acdev->vbase) {\r\ndev_warn(&pdev->dev, "ioremap fail\n");\r\nreturn -ENOMEM;\r\n}\r\nacdev->clk = clk_get(&pdev->dev, NULL);\r\nif (IS_ERR(acdev->clk)) {\r\ndev_warn(&pdev->dev, "Clock not found\n");\r\nreturn PTR_ERR(acdev->clk);\r\n}\r\nhost = ata_host_alloc(&pdev->dev, 1);\r\nif (!host) {\r\nret = -ENOMEM;\r\ndev_warn(&pdev->dev, "alloc host fail\n");\r\ngoto free_clk;\r\n}\r\nap = host->ports[0];\r\nhost->private_data = acdev;\r\nacdev->host = host;\r\nap->ops = &arasan_cf_ops;\r\nap->pio_mask = ATA_PIO6;\r\nap->mwdma_mask = ATA_MWDMA4;\r\nap->udma_mask = ATA_UDMA6;\r\ninit_completion(&acdev->cf_completion);\r\ninit_completion(&acdev->dma_completion);\r\nINIT_WORK(&acdev->work, data_xfer);\r\nINIT_DELAYED_WORK(&acdev->dwork, delayed_finish);\r\ndma_cap_set(DMA_MEMCPY, acdev->mask);\r\nif (quirk) {\r\nif (quirk & CF_BROKEN_PIO) {\r\nap->ops->set_piomode = NULL;\r\nap->pio_mask = 0;\r\n}\r\nif (quirk & CF_BROKEN_MWDMA)\r\nap->mwdma_mask = 0;\r\nif (quirk & CF_BROKEN_UDMA)\r\nap->udma_mask = 0;\r\n}\r\nap->flags |= ATA_FLAG_PIO_POLLING | ATA_FLAG_NO_ATAPI;\r\nap->ioaddr.cmd_addr = acdev->vbase + ATA_DATA_PORT;\r\nap->ioaddr.data_addr = acdev->vbase + ATA_DATA_PORT;\r\nap->ioaddr.error_addr = acdev->vbase + ATA_ERR_FTR;\r\nap->ioaddr.feature_addr = acdev->vbase + ATA_ERR_FTR;\r\nap->ioaddr.nsect_addr = acdev->vbase + ATA_SC;\r\nap->ioaddr.lbal_addr = acdev->vbase + ATA_SN;\r\nap->ioaddr.lbam_addr = acdev->vbase + ATA_CL;\r\nap->ioaddr.lbah_addr = acdev->vbase + ATA_CH;\r\nap->ioaddr.device_addr = acdev->vbase + ATA_SH;\r\nap->ioaddr.status_addr = acdev->vbase + ATA_STS_CMD;\r\nap->ioaddr.command_addr = acdev->vbase + ATA_STS_CMD;\r\nap->ioaddr.altstatus_addr = acdev->vbase + ATA_ASTS_DCTR;\r\nap->ioaddr.ctl_addr = acdev->vbase + ATA_ASTS_DCTR;\r\nata_port_desc(ap, "phy_addr %llx virt_addr %p",\r\n(unsigned long long) res->start, acdev->vbase);\r\nret = cf_init(acdev);\r\nif (ret)\r\ngoto free_clk;\r\ncf_card_detect(acdev, 0);\r\nret = ata_host_activate(host, acdev->irq, irq_handler, 0,\r\n&arasan_cf_sht);\r\nif (!ret)\r\nreturn 0;\r\ncf_exit(acdev);\r\nfree_clk:\r\nclk_put(acdev->clk);\r\nreturn ret;\r\n}\r\nstatic int arasan_cf_remove(struct platform_device *pdev)\r\n{\r\nstruct ata_host *host = platform_get_drvdata(pdev);\r\nstruct arasan_cf_dev *acdev = host->ports[0]->private_data;\r\nata_host_detach(host);\r\ncf_exit(acdev);\r\nclk_put(acdev->clk);\r\nreturn 0;\r\n}\r\nstatic int arasan_cf_suspend(struct device *dev)\r\n{\r\nstruct ata_host *host = dev_get_drvdata(dev);\r\nstruct arasan_cf_dev *acdev = host->ports[0]->private_data;\r\nif (acdev->dma_chan)\r\nacdev->dma_chan->device->device_control(acdev->dma_chan,\r\nDMA_TERMINATE_ALL, 0);\r\ncf_exit(acdev);\r\nreturn ata_host_suspend(host, PMSG_SUSPEND);\r\n}\r\nstatic int arasan_cf_resume(struct device *dev)\r\n{\r\nstruct ata_host *host = dev_get_drvdata(dev);\r\nstruct arasan_cf_dev *acdev = host->ports[0]->private_data;\r\ncf_init(acdev);\r\nata_host_resume(host);\r\nreturn 0;\r\n}
