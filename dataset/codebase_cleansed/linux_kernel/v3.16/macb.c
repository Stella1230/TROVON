static unsigned int macb_tx_ring_wrap(unsigned int index)\r\n{\r\nreturn index & (TX_RING_SIZE - 1);\r\n}\r\nstatic struct macb_dma_desc *macb_tx_desc(struct macb *bp, unsigned int index)\r\n{\r\nreturn &bp->tx_ring[macb_tx_ring_wrap(index)];\r\n}\r\nstatic struct macb_tx_skb *macb_tx_skb(struct macb *bp, unsigned int index)\r\n{\r\nreturn &bp->tx_skb[macb_tx_ring_wrap(index)];\r\n}\r\nstatic dma_addr_t macb_tx_dma(struct macb *bp, unsigned int index)\r\n{\r\ndma_addr_t offset;\r\noffset = macb_tx_ring_wrap(index) * sizeof(struct macb_dma_desc);\r\nreturn bp->tx_ring_dma + offset;\r\n}\r\nstatic unsigned int macb_rx_ring_wrap(unsigned int index)\r\n{\r\nreturn index & (RX_RING_SIZE - 1);\r\n}\r\nstatic struct macb_dma_desc *macb_rx_desc(struct macb *bp, unsigned int index)\r\n{\r\nreturn &bp->rx_ring[macb_rx_ring_wrap(index)];\r\n}\r\nstatic void *macb_rx_buffer(struct macb *bp, unsigned int index)\r\n{\r\nreturn bp->rx_buffers + bp->rx_buffer_size * macb_rx_ring_wrap(index);\r\n}\r\nvoid macb_set_hwaddr(struct macb *bp)\r\n{\r\nu32 bottom;\r\nu16 top;\r\nbottom = cpu_to_le32(*((u32 *)bp->dev->dev_addr));\r\nmacb_or_gem_writel(bp, SA1B, bottom);\r\ntop = cpu_to_le16(*((u16 *)(bp->dev->dev_addr + 4)));\r\nmacb_or_gem_writel(bp, SA1T, top);\r\nmacb_or_gem_writel(bp, SA2B, 0);\r\nmacb_or_gem_writel(bp, SA2T, 0);\r\nmacb_or_gem_writel(bp, SA3B, 0);\r\nmacb_or_gem_writel(bp, SA3T, 0);\r\nmacb_or_gem_writel(bp, SA4B, 0);\r\nmacb_or_gem_writel(bp, SA4T, 0);\r\n}\r\nvoid macb_get_hwaddr(struct macb *bp)\r\n{\r\nstruct macb_platform_data *pdata;\r\nu32 bottom;\r\nu16 top;\r\nu8 addr[6];\r\nint i;\r\npdata = dev_get_platdata(&bp->pdev->dev);\r\nfor (i = 0; i < 4; i++) {\r\nbottom = macb_or_gem_readl(bp, SA1B + i * 8);\r\ntop = macb_or_gem_readl(bp, SA1T + i * 8);\r\nif (pdata && pdata->rev_eth_addr) {\r\naddr[5] = bottom & 0xff;\r\naddr[4] = (bottom >> 8) & 0xff;\r\naddr[3] = (bottom >> 16) & 0xff;\r\naddr[2] = (bottom >> 24) & 0xff;\r\naddr[1] = top & 0xff;\r\naddr[0] = (top & 0xff00) >> 8;\r\n} else {\r\naddr[0] = bottom & 0xff;\r\naddr[1] = (bottom >> 8) & 0xff;\r\naddr[2] = (bottom >> 16) & 0xff;\r\naddr[3] = (bottom >> 24) & 0xff;\r\naddr[4] = top & 0xff;\r\naddr[5] = (top >> 8) & 0xff;\r\n}\r\nif (is_valid_ether_addr(addr)) {\r\nmemcpy(bp->dev->dev_addr, addr, sizeof(addr));\r\nreturn;\r\n}\r\n}\r\nnetdev_info(bp->dev, "invalid hw address, using random\n");\r\neth_hw_addr_random(bp->dev);\r\n}\r\nstatic int macb_mdio_read(struct mii_bus *bus, int mii_id, int regnum)\r\n{\r\nstruct macb *bp = bus->priv;\r\nint value;\r\nmacb_writel(bp, MAN, (MACB_BF(SOF, MACB_MAN_SOF)\r\n| MACB_BF(RW, MACB_MAN_READ)\r\n| MACB_BF(PHYA, mii_id)\r\n| MACB_BF(REGA, regnum)\r\n| MACB_BF(CODE, MACB_MAN_CODE)));\r\nwhile (!MACB_BFEXT(IDLE, macb_readl(bp, NSR)))\r\ncpu_relax();\r\nvalue = MACB_BFEXT(DATA, macb_readl(bp, MAN));\r\nreturn value;\r\n}\r\nstatic int macb_mdio_write(struct mii_bus *bus, int mii_id, int regnum,\r\nu16 value)\r\n{\r\nstruct macb *bp = bus->priv;\r\nmacb_writel(bp, MAN, (MACB_BF(SOF, MACB_MAN_SOF)\r\n| MACB_BF(RW, MACB_MAN_WRITE)\r\n| MACB_BF(PHYA, mii_id)\r\n| MACB_BF(REGA, regnum)\r\n| MACB_BF(CODE, MACB_MAN_CODE)\r\n| MACB_BF(DATA, value)));\r\nwhile (!MACB_BFEXT(IDLE, macb_readl(bp, NSR)))\r\ncpu_relax();\r\nreturn 0;\r\n}\r\nstatic void macb_set_tx_clk(struct clk *clk, int speed, struct net_device *dev)\r\n{\r\nlong ferr, rate, rate_rounded;\r\nswitch (speed) {\r\ncase SPEED_10:\r\nrate = 2500000;\r\nbreak;\r\ncase SPEED_100:\r\nrate = 25000000;\r\nbreak;\r\ncase SPEED_1000:\r\nrate = 125000000;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\nrate_rounded = clk_round_rate(clk, rate);\r\nif (rate_rounded < 0)\r\nreturn;\r\nferr = abs(rate_rounded - rate);\r\nferr = DIV_ROUND_UP(ferr, rate / 100000);\r\nif (ferr > 5)\r\nnetdev_warn(dev, "unable to generate target frequency: %ld Hz\n",\r\nrate);\r\nif (clk_set_rate(clk, rate_rounded))\r\nnetdev_err(dev, "adjusting tx_clk failed.\n");\r\n}\r\nstatic void macb_handle_link_change(struct net_device *dev)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\nstruct phy_device *phydev = bp->phy_dev;\r\nunsigned long flags;\r\nint status_change = 0;\r\nspin_lock_irqsave(&bp->lock, flags);\r\nif (phydev->link) {\r\nif ((bp->speed != phydev->speed) ||\r\n(bp->duplex != phydev->duplex)) {\r\nu32 reg;\r\nreg = macb_readl(bp, NCFGR);\r\nreg &= ~(MACB_BIT(SPD) | MACB_BIT(FD));\r\nif (macb_is_gem(bp))\r\nreg &= ~GEM_BIT(GBE);\r\nif (phydev->duplex)\r\nreg |= MACB_BIT(FD);\r\nif (phydev->speed == SPEED_100)\r\nreg |= MACB_BIT(SPD);\r\nif (phydev->speed == SPEED_1000)\r\nreg |= GEM_BIT(GBE);\r\nmacb_or_gem_writel(bp, NCFGR, reg);\r\nbp->speed = phydev->speed;\r\nbp->duplex = phydev->duplex;\r\nstatus_change = 1;\r\n}\r\n}\r\nif (phydev->link != bp->link) {\r\nif (!phydev->link) {\r\nbp->speed = 0;\r\nbp->duplex = -1;\r\n}\r\nbp->link = phydev->link;\r\nstatus_change = 1;\r\n}\r\nspin_unlock_irqrestore(&bp->lock, flags);\r\nif (!IS_ERR(bp->tx_clk))\r\nmacb_set_tx_clk(bp->tx_clk, phydev->speed, dev);\r\nif (status_change) {\r\nif (phydev->link) {\r\nnetif_carrier_on(dev);\r\nnetdev_info(dev, "link up (%d/%s)\n",\r\nphydev->speed,\r\nphydev->duplex == DUPLEX_FULL ?\r\n"Full" : "Half");\r\n} else {\r\nnetif_carrier_off(dev);\r\nnetdev_info(dev, "link down\n");\r\n}\r\n}\r\n}\r\nstatic int macb_mii_probe(struct net_device *dev)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\nstruct macb_platform_data *pdata;\r\nstruct phy_device *phydev;\r\nint phy_irq;\r\nint ret;\r\nphydev = phy_find_first(bp->mii_bus);\r\nif (!phydev) {\r\nnetdev_err(dev, "no PHY found\n");\r\nreturn -ENXIO;\r\n}\r\npdata = dev_get_platdata(&bp->pdev->dev);\r\nif (pdata && gpio_is_valid(pdata->phy_irq_pin)) {\r\nret = devm_gpio_request(&bp->pdev->dev, pdata->phy_irq_pin, "phy int");\r\nif (!ret) {\r\nphy_irq = gpio_to_irq(pdata->phy_irq_pin);\r\nphydev->irq = (phy_irq < 0) ? PHY_POLL : phy_irq;\r\n}\r\n}\r\nret = phy_connect_direct(dev, phydev, &macb_handle_link_change,\r\nbp->phy_interface);\r\nif (ret) {\r\nnetdev_err(dev, "Could not attach to PHY\n");\r\nreturn ret;\r\n}\r\nif (macb_is_gem(bp))\r\nphydev->supported &= PHY_GBIT_FEATURES;\r\nelse\r\nphydev->supported &= PHY_BASIC_FEATURES;\r\nphydev->advertising = phydev->supported;\r\nbp->link = 0;\r\nbp->speed = 0;\r\nbp->duplex = -1;\r\nbp->phy_dev = phydev;\r\nreturn 0;\r\n}\r\nint macb_mii_init(struct macb *bp)\r\n{\r\nstruct macb_platform_data *pdata;\r\nstruct device_node *np;\r\nint err = -ENXIO, i;\r\nmacb_writel(bp, NCR, MACB_BIT(MPE));\r\nbp->mii_bus = mdiobus_alloc();\r\nif (bp->mii_bus == NULL) {\r\nerr = -ENOMEM;\r\ngoto err_out;\r\n}\r\nbp->mii_bus->name = "MACB_mii_bus";\r\nbp->mii_bus->read = &macb_mdio_read;\r\nbp->mii_bus->write = &macb_mdio_write;\r\nsnprintf(bp->mii_bus->id, MII_BUS_ID_SIZE, "%s-%x",\r\nbp->pdev->name, bp->pdev->id);\r\nbp->mii_bus->priv = bp;\r\nbp->mii_bus->parent = &bp->dev->dev;\r\npdata = dev_get_platdata(&bp->pdev->dev);\r\nbp->mii_bus->irq = kmalloc(sizeof(int)*PHY_MAX_ADDR, GFP_KERNEL);\r\nif (!bp->mii_bus->irq) {\r\nerr = -ENOMEM;\r\ngoto err_out_free_mdiobus;\r\n}\r\ndev_set_drvdata(&bp->dev->dev, bp->mii_bus);\r\nnp = bp->pdev->dev.of_node;\r\nif (np) {\r\nerr = of_mdiobus_register(bp->mii_bus, np);\r\nif (!err && !phy_find_first(bp->mii_bus)) {\r\nfor (i = 0; i < PHY_MAX_ADDR; i++) {\r\nstruct phy_device *phydev;\r\nphydev = mdiobus_scan(bp->mii_bus, i);\r\nif (IS_ERR(phydev)) {\r\nerr = PTR_ERR(phydev);\r\nbreak;\r\n}\r\n}\r\nif (err)\r\ngoto err_out_unregister_bus;\r\n}\r\n} else {\r\nfor (i = 0; i < PHY_MAX_ADDR; i++)\r\nbp->mii_bus->irq[i] = PHY_POLL;\r\nif (pdata)\r\nbp->mii_bus->phy_mask = pdata->phy_mask;\r\nerr = mdiobus_register(bp->mii_bus);\r\n}\r\nif (err)\r\ngoto err_out_free_mdio_irq;\r\nerr = macb_mii_probe(bp->dev);\r\nif (err)\r\ngoto err_out_unregister_bus;\r\nreturn 0;\r\nerr_out_unregister_bus:\r\nmdiobus_unregister(bp->mii_bus);\r\nerr_out_free_mdio_irq:\r\nkfree(bp->mii_bus->irq);\r\nerr_out_free_mdiobus:\r\nmdiobus_free(bp->mii_bus);\r\nerr_out:\r\nreturn err;\r\n}\r\nstatic void macb_update_stats(struct macb *bp)\r\n{\r\nu32 __iomem *reg = bp->regs + MACB_PFR;\r\nu32 *p = &bp->hw_stats.macb.rx_pause_frames;\r\nu32 *end = &bp->hw_stats.macb.tx_pause_frames + 1;\r\nWARN_ON((unsigned long)(end - p - 1) != (MACB_TPF - MACB_PFR) / 4);\r\nfor(; p < end; p++, reg++)\r\n*p += __raw_readl(reg);\r\n}\r\nstatic int macb_halt_tx(struct macb *bp)\r\n{\r\nunsigned long halt_time, timeout;\r\nu32 status;\r\nmacb_writel(bp, NCR, macb_readl(bp, NCR) | MACB_BIT(THALT));\r\ntimeout = jiffies + usecs_to_jiffies(MACB_HALT_TIMEOUT);\r\ndo {\r\nhalt_time = jiffies;\r\nstatus = macb_readl(bp, TSR);\r\nif (!(status & MACB_BIT(TGO)))\r\nreturn 0;\r\nusleep_range(10, 250);\r\n} while (time_before(halt_time, timeout));\r\nreturn -ETIMEDOUT;\r\n}\r\nstatic void macb_tx_error_task(struct work_struct *work)\r\n{\r\nstruct macb *bp = container_of(work, struct macb, tx_error_task);\r\nstruct macb_tx_skb *tx_skb;\r\nstruct sk_buff *skb;\r\nunsigned int tail;\r\nnetdev_vdbg(bp->dev, "macb_tx_error_task: t = %u, h = %u\n",\r\nbp->tx_tail, bp->tx_head);\r\nnetif_stop_queue(bp->dev);\r\nif (macb_halt_tx(bp))\r\nnetdev_err(bp->dev, "BUG: halt tx timed out\n");\r\nfor (tail = bp->tx_tail; tail != bp->tx_head; tail++) {\r\nstruct macb_dma_desc *desc;\r\nu32 ctrl;\r\ndesc = macb_tx_desc(bp, tail);\r\nctrl = desc->ctrl;\r\ntx_skb = macb_tx_skb(bp, tail);\r\nskb = tx_skb->skb;\r\nif (ctrl & MACB_BIT(TX_USED)) {\r\nnetdev_vdbg(bp->dev, "txerr skb %u (data %p) TX complete\n",\r\nmacb_tx_ring_wrap(tail), skb->data);\r\nbp->stats.tx_packets++;\r\nbp->stats.tx_bytes += skb->len;\r\n} else {\r\nif (ctrl & MACB_BIT(TX_BUF_EXHAUSTED))\r\nnetdev_err(bp->dev,\r\n"BUG: TX buffers exhausted mid-frame\n");\r\ndesc->ctrl = ctrl | MACB_BIT(TX_USED);\r\n}\r\ndma_unmap_single(&bp->pdev->dev, tx_skb->mapping, skb->len,\r\nDMA_TO_DEVICE);\r\ntx_skb->skb = NULL;\r\ndev_kfree_skb(skb);\r\n}\r\nwmb();\r\nmacb_writel(bp, TBQP, bp->tx_ring_dma);\r\nbp->tx_head = bp->tx_tail = 0;\r\nnetif_wake_queue(bp->dev);\r\nmacb_writel(bp, TSR, macb_readl(bp, TSR));\r\nmacb_writel(bp, IER, MACB_TX_INT_FLAGS);\r\n}\r\nstatic void macb_tx_interrupt(struct macb *bp)\r\n{\r\nunsigned int tail;\r\nunsigned int head;\r\nu32 status;\r\nstatus = macb_readl(bp, TSR);\r\nmacb_writel(bp, TSR, status);\r\nif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\r\nmacb_writel(bp, ISR, MACB_BIT(TCOMP));\r\nnetdev_vdbg(bp->dev, "macb_tx_interrupt status = 0x%03lx\n",\r\n(unsigned long)status);\r\nhead = bp->tx_head;\r\nfor (tail = bp->tx_tail; tail != head; tail++) {\r\nstruct macb_tx_skb *tx_skb;\r\nstruct sk_buff *skb;\r\nstruct macb_dma_desc *desc;\r\nu32 ctrl;\r\ndesc = macb_tx_desc(bp, tail);\r\nrmb();\r\nctrl = desc->ctrl;\r\nif (!(ctrl & MACB_BIT(TX_USED)))\r\nbreak;\r\ntx_skb = macb_tx_skb(bp, tail);\r\nskb = tx_skb->skb;\r\nnetdev_vdbg(bp->dev, "skb %u (data %p) TX complete\n",\r\nmacb_tx_ring_wrap(tail), skb->data);\r\ndma_unmap_single(&bp->pdev->dev, tx_skb->mapping, skb->len,\r\nDMA_TO_DEVICE);\r\nbp->stats.tx_packets++;\r\nbp->stats.tx_bytes += skb->len;\r\ntx_skb->skb = NULL;\r\ndev_kfree_skb_irq(skb);\r\n}\r\nbp->tx_tail = tail;\r\nif (netif_queue_stopped(bp->dev)\r\n&& CIRC_CNT(bp->tx_head, bp->tx_tail,\r\nTX_RING_SIZE) <= MACB_TX_WAKEUP_THRESH)\r\nnetif_wake_queue(bp->dev);\r\n}\r\nstatic void gem_rx_refill(struct macb *bp)\r\n{\r\nunsigned int entry;\r\nstruct sk_buff *skb;\r\ndma_addr_t paddr;\r\nwhile (CIRC_SPACE(bp->rx_prepared_head, bp->rx_tail, RX_RING_SIZE) > 0) {\r\nentry = macb_rx_ring_wrap(bp->rx_prepared_head);\r\nrmb();\r\nbp->rx_prepared_head++;\r\nif (bp->rx_skbuff[entry] == NULL) {\r\nskb = netdev_alloc_skb(bp->dev, bp->rx_buffer_size);\r\nif (unlikely(skb == NULL)) {\r\nnetdev_err(bp->dev,\r\n"Unable to allocate sk_buff\n");\r\nbreak;\r\n}\r\npaddr = dma_map_single(&bp->pdev->dev, skb->data,\r\nbp->rx_buffer_size, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(&bp->pdev->dev, paddr)) {\r\ndev_kfree_skb(skb);\r\nbreak;\r\n}\r\nbp->rx_skbuff[entry] = skb;\r\nif (entry == RX_RING_SIZE - 1)\r\npaddr |= MACB_BIT(RX_WRAP);\r\nbp->rx_ring[entry].addr = paddr;\r\nbp->rx_ring[entry].ctrl = 0;\r\nskb_reserve(skb, NET_IP_ALIGN);\r\n}\r\n}\r\nwmb();\r\nnetdev_vdbg(bp->dev, "rx ring: prepared head %d, tail %d\n",\r\nbp->rx_prepared_head, bp->rx_tail);\r\n}\r\nstatic void discard_partial_frame(struct macb *bp, unsigned int begin,\r\nunsigned int end)\r\n{\r\nunsigned int frag;\r\nfor (frag = begin; frag != end; frag++) {\r\nstruct macb_dma_desc *desc = macb_rx_desc(bp, frag);\r\ndesc->addr &= ~MACB_BIT(RX_USED);\r\n}\r\nwmb();\r\n}\r\nstatic int gem_rx(struct macb *bp, int budget)\r\n{\r\nunsigned int len;\r\nunsigned int entry;\r\nstruct sk_buff *skb;\r\nstruct macb_dma_desc *desc;\r\nint count = 0;\r\nwhile (count < budget) {\r\nu32 addr, ctrl;\r\nentry = macb_rx_ring_wrap(bp->rx_tail);\r\ndesc = &bp->rx_ring[entry];\r\nrmb();\r\naddr = desc->addr;\r\nctrl = desc->ctrl;\r\nif (!(addr & MACB_BIT(RX_USED)))\r\nbreak;\r\nbp->rx_tail++;\r\ncount++;\r\nif (!(ctrl & MACB_BIT(RX_SOF) && ctrl & MACB_BIT(RX_EOF))) {\r\nnetdev_err(bp->dev,\r\n"not whole frame pointed by descriptor\n");\r\nbp->stats.rx_dropped++;\r\nbreak;\r\n}\r\nskb = bp->rx_skbuff[entry];\r\nif (unlikely(!skb)) {\r\nnetdev_err(bp->dev,\r\n"inconsistent Rx descriptor chain\n");\r\nbp->stats.rx_dropped++;\r\nbreak;\r\n}\r\nbp->rx_skbuff[entry] = NULL;\r\nlen = MACB_BFEXT(RX_FRMLEN, ctrl);\r\nnetdev_vdbg(bp->dev, "gem_rx %u (len %u)\n", entry, len);\r\nskb_put(skb, len);\r\naddr = MACB_BF(RX_WADDR, MACB_BFEXT(RX_WADDR, addr));\r\ndma_unmap_single(&bp->pdev->dev, addr,\r\nbp->rx_buffer_size, DMA_FROM_DEVICE);\r\nskb->protocol = eth_type_trans(skb, bp->dev);\r\nskb_checksum_none_assert(skb);\r\nbp->stats.rx_packets++;\r\nbp->stats.rx_bytes += skb->len;\r\n#if defined(DEBUG) && defined(VERBOSE_DEBUG)\r\nnetdev_vdbg(bp->dev, "received skb of length %u, csum: %08x\n",\r\nskb->len, skb->csum);\r\nprint_hex_dump(KERN_DEBUG, " mac: ", DUMP_PREFIX_ADDRESS, 16, 1,\r\nskb->mac_header, 16, true);\r\nprint_hex_dump(KERN_DEBUG, "data: ", DUMP_PREFIX_ADDRESS, 16, 1,\r\nskb->data, 32, true);\r\n#endif\r\nnetif_receive_skb(skb);\r\n}\r\ngem_rx_refill(bp);\r\nreturn count;\r\n}\r\nstatic int macb_rx_frame(struct macb *bp, unsigned int first_frag,\r\nunsigned int last_frag)\r\n{\r\nunsigned int len;\r\nunsigned int frag;\r\nunsigned int offset;\r\nstruct sk_buff *skb;\r\nstruct macb_dma_desc *desc;\r\ndesc = macb_rx_desc(bp, last_frag);\r\nlen = MACB_BFEXT(RX_FRMLEN, desc->ctrl);\r\nnetdev_vdbg(bp->dev, "macb_rx_frame frags %u - %u (len %u)\n",\r\nmacb_rx_ring_wrap(first_frag),\r\nmacb_rx_ring_wrap(last_frag), len);\r\nskb = netdev_alloc_skb(bp->dev, len + NET_IP_ALIGN);\r\nif (!skb) {\r\nbp->stats.rx_dropped++;\r\nfor (frag = first_frag; ; frag++) {\r\ndesc = macb_rx_desc(bp, frag);\r\ndesc->addr &= ~MACB_BIT(RX_USED);\r\nif (frag == last_frag)\r\nbreak;\r\n}\r\nwmb();\r\nreturn 1;\r\n}\r\noffset = 0;\r\nlen += NET_IP_ALIGN;\r\nskb_checksum_none_assert(skb);\r\nskb_put(skb, len);\r\nfor (frag = first_frag; ; frag++) {\r\nunsigned int frag_len = bp->rx_buffer_size;\r\nif (offset + frag_len > len) {\r\nBUG_ON(frag != last_frag);\r\nfrag_len = len - offset;\r\n}\r\nskb_copy_to_linear_data_offset(skb, offset,\r\nmacb_rx_buffer(bp, frag), frag_len);\r\noffset += bp->rx_buffer_size;\r\ndesc = macb_rx_desc(bp, frag);\r\ndesc->addr &= ~MACB_BIT(RX_USED);\r\nif (frag == last_frag)\r\nbreak;\r\n}\r\nwmb();\r\n__skb_pull(skb, NET_IP_ALIGN);\r\nskb->protocol = eth_type_trans(skb, bp->dev);\r\nbp->stats.rx_packets++;\r\nbp->stats.rx_bytes += skb->len;\r\nnetdev_vdbg(bp->dev, "received skb of length %u, csum: %08x\n",\r\nskb->len, skb->csum);\r\nnetif_receive_skb(skb);\r\nreturn 0;\r\n}\r\nstatic int macb_rx(struct macb *bp, int budget)\r\n{\r\nint received = 0;\r\nunsigned int tail;\r\nint first_frag = -1;\r\nfor (tail = bp->rx_tail; budget > 0; tail++) {\r\nstruct macb_dma_desc *desc = macb_rx_desc(bp, tail);\r\nu32 addr, ctrl;\r\nrmb();\r\naddr = desc->addr;\r\nctrl = desc->ctrl;\r\nif (!(addr & MACB_BIT(RX_USED)))\r\nbreak;\r\nif (ctrl & MACB_BIT(RX_SOF)) {\r\nif (first_frag != -1)\r\ndiscard_partial_frame(bp, first_frag, tail);\r\nfirst_frag = tail;\r\n}\r\nif (ctrl & MACB_BIT(RX_EOF)) {\r\nint dropped;\r\nBUG_ON(first_frag == -1);\r\ndropped = macb_rx_frame(bp, first_frag, tail);\r\nfirst_frag = -1;\r\nif (!dropped) {\r\nreceived++;\r\nbudget--;\r\n}\r\n}\r\n}\r\nif (first_frag != -1)\r\nbp->rx_tail = first_frag;\r\nelse\r\nbp->rx_tail = tail;\r\nreturn received;\r\n}\r\nstatic int macb_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct macb *bp = container_of(napi, struct macb, napi);\r\nint work_done;\r\nu32 status;\r\nstatus = macb_readl(bp, RSR);\r\nmacb_writel(bp, RSR, status);\r\nwork_done = 0;\r\nnetdev_vdbg(bp->dev, "poll: status = %08lx, budget = %d\n",\r\n(unsigned long)status, budget);\r\nwork_done = bp->macbgem_ops.mog_rx(bp, budget);\r\nif (work_done < budget) {\r\nnapi_complete(napi);\r\nstatus = macb_readl(bp, RSR);\r\nif (status) {\r\nif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\r\nmacb_writel(bp, ISR, MACB_BIT(RCOMP));\r\nnapi_reschedule(napi);\r\n} else {\r\nmacb_writel(bp, IER, MACB_RX_INT_FLAGS);\r\n}\r\n}\r\nreturn work_done;\r\n}\r\nstatic irqreturn_t macb_interrupt(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev = dev_id;\r\nstruct macb *bp = netdev_priv(dev);\r\nu32 status;\r\nstatus = macb_readl(bp, ISR);\r\nif (unlikely(!status))\r\nreturn IRQ_NONE;\r\nspin_lock(&bp->lock);\r\nwhile (status) {\r\nif (unlikely(!netif_running(dev))) {\r\nmacb_writel(bp, IDR, -1);\r\nbreak;\r\n}\r\nnetdev_vdbg(bp->dev, "isr = 0x%08lx\n", (unsigned long)status);\r\nif (status & MACB_RX_INT_FLAGS) {\r\nmacb_writel(bp, IDR, MACB_RX_INT_FLAGS);\r\nif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\r\nmacb_writel(bp, ISR, MACB_BIT(RCOMP));\r\nif (napi_schedule_prep(&bp->napi)) {\r\nnetdev_vdbg(bp->dev, "scheduling RX softirq\n");\r\n__napi_schedule(&bp->napi);\r\n}\r\n}\r\nif (unlikely(status & (MACB_TX_ERR_FLAGS))) {\r\nmacb_writel(bp, IDR, MACB_TX_INT_FLAGS);\r\nschedule_work(&bp->tx_error_task);\r\nif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\r\nmacb_writel(bp, ISR, MACB_TX_ERR_FLAGS);\r\nbreak;\r\n}\r\nif (status & MACB_BIT(TCOMP))\r\nmacb_tx_interrupt(bp);\r\nif (status & MACB_BIT(ISR_ROVR)) {\r\nif (macb_is_gem(bp))\r\nbp->hw_stats.gem.rx_overruns++;\r\nelse\r\nbp->hw_stats.macb.rx_overruns++;\r\nif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\r\nmacb_writel(bp, ISR, MACB_BIT(ISR_ROVR));\r\n}\r\nif (status & MACB_BIT(HRESP)) {\r\nnetdev_err(dev, "DMA bus error: HRESP not OK\n");\r\nif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\r\nmacb_writel(bp, ISR, MACB_BIT(HRESP));\r\n}\r\nstatus = macb_readl(bp, ISR);\r\n}\r\nspin_unlock(&bp->lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void macb_poll_controller(struct net_device *dev)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nmacb_interrupt(dev->irq, dev);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int macb_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\ndma_addr_t mapping;\r\nunsigned int len, entry;\r\nstruct macb_dma_desc *desc;\r\nstruct macb_tx_skb *tx_skb;\r\nu32 ctrl;\r\nunsigned long flags;\r\n#if defined(DEBUG) && defined(VERBOSE_DEBUG)\r\nnetdev_vdbg(bp->dev,\r\n"start_xmit: len %u head %p data %p tail %p end %p\n",\r\nskb->len, skb->head, skb->data,\r\nskb_tail_pointer(skb), skb_end_pointer(skb));\r\nprint_hex_dump(KERN_DEBUG, "data: ", DUMP_PREFIX_OFFSET, 16, 1,\r\nskb->data, 16, true);\r\n#endif\r\nlen = skb->len;\r\nspin_lock_irqsave(&bp->lock, flags);\r\nif (CIRC_SPACE(bp->tx_head, bp->tx_tail, TX_RING_SIZE) < 1) {\r\nnetif_stop_queue(dev);\r\nspin_unlock_irqrestore(&bp->lock, flags);\r\nnetdev_err(bp->dev, "BUG! Tx Ring full when queue awake!\n");\r\nnetdev_dbg(bp->dev, "tx_head = %u, tx_tail = %u\n",\r\nbp->tx_head, bp->tx_tail);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nentry = macb_tx_ring_wrap(bp->tx_head);\r\nnetdev_vdbg(bp->dev, "Allocated ring entry %u\n", entry);\r\nmapping = dma_map_single(&bp->pdev->dev, skb->data,\r\nlen, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&bp->pdev->dev, mapping)) {\r\ndev_kfree_skb_any(skb);\r\ngoto unlock;\r\n}\r\nbp->tx_head++;\r\ntx_skb = &bp->tx_skb[entry];\r\ntx_skb->skb = skb;\r\ntx_skb->mapping = mapping;\r\nnetdev_vdbg(bp->dev, "Mapped skb data %p to DMA addr %08lx\n",\r\nskb->data, (unsigned long)mapping);\r\nctrl = MACB_BF(TX_FRMLEN, len);\r\nctrl |= MACB_BIT(TX_LAST);\r\nif (entry == (TX_RING_SIZE - 1))\r\nctrl |= MACB_BIT(TX_WRAP);\r\ndesc = &bp->tx_ring[entry];\r\ndesc->addr = mapping;\r\ndesc->ctrl = ctrl;\r\nwmb();\r\nskb_tx_timestamp(skb);\r\nmacb_writel(bp, NCR, macb_readl(bp, NCR) | MACB_BIT(TSTART));\r\nif (CIRC_SPACE(bp->tx_head, bp->tx_tail, TX_RING_SIZE) < 1)\r\nnetif_stop_queue(dev);\r\nunlock:\r\nspin_unlock_irqrestore(&bp->lock, flags);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void macb_init_rx_buffer_size(struct macb *bp, size_t size)\r\n{\r\nif (!macb_is_gem(bp)) {\r\nbp->rx_buffer_size = MACB_RX_BUFFER_SIZE;\r\n} else {\r\nbp->rx_buffer_size = size;\r\nif (bp->rx_buffer_size % RX_BUFFER_MULTIPLE) {\r\nnetdev_dbg(bp->dev,\r\n"RX buffer must be multiple of %d bytes, expanding\n",\r\nRX_BUFFER_MULTIPLE);\r\nbp->rx_buffer_size =\r\nroundup(bp->rx_buffer_size, RX_BUFFER_MULTIPLE);\r\n}\r\n}\r\nnetdev_dbg(bp->dev, "mtu [%u] rx_buffer_size [%Zu]\n",\r\nbp->dev->mtu, bp->rx_buffer_size);\r\n}\r\nstatic void gem_free_rx_buffers(struct macb *bp)\r\n{\r\nstruct sk_buff *skb;\r\nstruct macb_dma_desc *desc;\r\ndma_addr_t addr;\r\nint i;\r\nif (!bp->rx_skbuff)\r\nreturn;\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nskb = bp->rx_skbuff[i];\r\nif (skb == NULL)\r\ncontinue;\r\ndesc = &bp->rx_ring[i];\r\naddr = MACB_BF(RX_WADDR, MACB_BFEXT(RX_WADDR, desc->addr));\r\ndma_unmap_single(&bp->pdev->dev, addr, bp->rx_buffer_size,\r\nDMA_FROM_DEVICE);\r\ndev_kfree_skb_any(skb);\r\nskb = NULL;\r\n}\r\nkfree(bp->rx_skbuff);\r\nbp->rx_skbuff = NULL;\r\n}\r\nstatic void macb_free_rx_buffers(struct macb *bp)\r\n{\r\nif (bp->rx_buffers) {\r\ndma_free_coherent(&bp->pdev->dev,\r\nRX_RING_SIZE * bp->rx_buffer_size,\r\nbp->rx_buffers, bp->rx_buffers_dma);\r\nbp->rx_buffers = NULL;\r\n}\r\n}\r\nstatic void macb_free_consistent(struct macb *bp)\r\n{\r\nif (bp->tx_skb) {\r\nkfree(bp->tx_skb);\r\nbp->tx_skb = NULL;\r\n}\r\nbp->macbgem_ops.mog_free_rx_buffers(bp);\r\nif (bp->rx_ring) {\r\ndma_free_coherent(&bp->pdev->dev, RX_RING_BYTES,\r\nbp->rx_ring, bp->rx_ring_dma);\r\nbp->rx_ring = NULL;\r\n}\r\nif (bp->tx_ring) {\r\ndma_free_coherent(&bp->pdev->dev, TX_RING_BYTES,\r\nbp->tx_ring, bp->tx_ring_dma);\r\nbp->tx_ring = NULL;\r\n}\r\n}\r\nstatic int gem_alloc_rx_buffers(struct macb *bp)\r\n{\r\nint size;\r\nsize = RX_RING_SIZE * sizeof(struct sk_buff *);\r\nbp->rx_skbuff = kzalloc(size, GFP_KERNEL);\r\nif (!bp->rx_skbuff)\r\nreturn -ENOMEM;\r\nelse\r\nnetdev_dbg(bp->dev,\r\n"Allocated %d RX struct sk_buff entries at %p\n",\r\nRX_RING_SIZE, bp->rx_skbuff);\r\nreturn 0;\r\n}\r\nstatic int macb_alloc_rx_buffers(struct macb *bp)\r\n{\r\nint size;\r\nsize = RX_RING_SIZE * bp->rx_buffer_size;\r\nbp->rx_buffers = dma_alloc_coherent(&bp->pdev->dev, size,\r\n&bp->rx_buffers_dma, GFP_KERNEL);\r\nif (!bp->rx_buffers)\r\nreturn -ENOMEM;\r\nelse\r\nnetdev_dbg(bp->dev,\r\n"Allocated RX buffers of %d bytes at %08lx (mapped %p)\n",\r\nsize, (unsigned long)bp->rx_buffers_dma, bp->rx_buffers);\r\nreturn 0;\r\n}\r\nstatic int macb_alloc_consistent(struct macb *bp)\r\n{\r\nint size;\r\nsize = TX_RING_SIZE * sizeof(struct macb_tx_skb);\r\nbp->tx_skb = kmalloc(size, GFP_KERNEL);\r\nif (!bp->tx_skb)\r\ngoto out_err;\r\nsize = RX_RING_BYTES;\r\nbp->rx_ring = dma_alloc_coherent(&bp->pdev->dev, size,\r\n&bp->rx_ring_dma, GFP_KERNEL);\r\nif (!bp->rx_ring)\r\ngoto out_err;\r\nnetdev_dbg(bp->dev,\r\n"Allocated RX ring of %d bytes at %08lx (mapped %p)\n",\r\nsize, (unsigned long)bp->rx_ring_dma, bp->rx_ring);\r\nsize = TX_RING_BYTES;\r\nbp->tx_ring = dma_alloc_coherent(&bp->pdev->dev, size,\r\n&bp->tx_ring_dma, GFP_KERNEL);\r\nif (!bp->tx_ring)\r\ngoto out_err;\r\nnetdev_dbg(bp->dev,\r\n"Allocated TX ring of %d bytes at %08lx (mapped %p)\n",\r\nsize, (unsigned long)bp->tx_ring_dma, bp->tx_ring);\r\nif (bp->macbgem_ops.mog_alloc_rx_buffers(bp))\r\ngoto out_err;\r\nreturn 0;\r\nout_err:\r\nmacb_free_consistent(bp);\r\nreturn -ENOMEM;\r\n}\r\nstatic void gem_init_rings(struct macb *bp)\r\n{\r\nint i;\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nbp->tx_ring[i].addr = 0;\r\nbp->tx_ring[i].ctrl = MACB_BIT(TX_USED);\r\n}\r\nbp->tx_ring[TX_RING_SIZE - 1].ctrl |= MACB_BIT(TX_WRAP);\r\nbp->rx_tail = bp->rx_prepared_head = bp->tx_head = bp->tx_tail = 0;\r\ngem_rx_refill(bp);\r\n}\r\nstatic void macb_init_rings(struct macb *bp)\r\n{\r\nint i;\r\ndma_addr_t addr;\r\naddr = bp->rx_buffers_dma;\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nbp->rx_ring[i].addr = addr;\r\nbp->rx_ring[i].ctrl = 0;\r\naddr += bp->rx_buffer_size;\r\n}\r\nbp->rx_ring[RX_RING_SIZE - 1].addr |= MACB_BIT(RX_WRAP);\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nbp->tx_ring[i].addr = 0;\r\nbp->tx_ring[i].ctrl = MACB_BIT(TX_USED);\r\n}\r\nbp->tx_ring[TX_RING_SIZE - 1].ctrl |= MACB_BIT(TX_WRAP);\r\nbp->rx_tail = bp->tx_head = bp->tx_tail = 0;\r\n}\r\nstatic void macb_reset_hw(struct macb *bp)\r\n{\r\nmacb_writel(bp, NCR, 0);\r\nmacb_writel(bp, NCR, MACB_BIT(CLRSTAT));\r\nmacb_writel(bp, TSR, -1);\r\nmacb_writel(bp, RSR, -1);\r\nmacb_writel(bp, IDR, -1);\r\nmacb_readl(bp, ISR);\r\n}\r\nstatic u32 gem_mdc_clk_div(struct macb *bp)\r\n{\r\nu32 config;\r\nunsigned long pclk_hz = clk_get_rate(bp->pclk);\r\nif (pclk_hz <= 20000000)\r\nconfig = GEM_BF(CLK, GEM_CLK_DIV8);\r\nelse if (pclk_hz <= 40000000)\r\nconfig = GEM_BF(CLK, GEM_CLK_DIV16);\r\nelse if (pclk_hz <= 80000000)\r\nconfig = GEM_BF(CLK, GEM_CLK_DIV32);\r\nelse if (pclk_hz <= 120000000)\r\nconfig = GEM_BF(CLK, GEM_CLK_DIV48);\r\nelse if (pclk_hz <= 160000000)\r\nconfig = GEM_BF(CLK, GEM_CLK_DIV64);\r\nelse\r\nconfig = GEM_BF(CLK, GEM_CLK_DIV96);\r\nreturn config;\r\n}\r\nstatic u32 macb_mdc_clk_div(struct macb *bp)\r\n{\r\nu32 config;\r\nunsigned long pclk_hz;\r\nif (macb_is_gem(bp))\r\nreturn gem_mdc_clk_div(bp);\r\npclk_hz = clk_get_rate(bp->pclk);\r\nif (pclk_hz <= 20000000)\r\nconfig = MACB_BF(CLK, MACB_CLK_DIV8);\r\nelse if (pclk_hz <= 40000000)\r\nconfig = MACB_BF(CLK, MACB_CLK_DIV16);\r\nelse if (pclk_hz <= 80000000)\r\nconfig = MACB_BF(CLK, MACB_CLK_DIV32);\r\nelse\r\nconfig = MACB_BF(CLK, MACB_CLK_DIV64);\r\nreturn config;\r\n}\r\nstatic u32 macb_dbw(struct macb *bp)\r\n{\r\nif (!macb_is_gem(bp))\r\nreturn 0;\r\nswitch (GEM_BFEXT(DBWDEF, gem_readl(bp, DCFG1))) {\r\ncase 4:\r\nreturn GEM_BF(DBW, GEM_DBW128);\r\ncase 2:\r\nreturn GEM_BF(DBW, GEM_DBW64);\r\ncase 1:\r\ndefault:\r\nreturn GEM_BF(DBW, GEM_DBW32);\r\n}\r\n}\r\nstatic void macb_configure_dma(struct macb *bp)\r\n{\r\nu32 dmacfg;\r\nif (macb_is_gem(bp)) {\r\ndmacfg = gem_readl(bp, DMACFG) & ~GEM_BF(RXBS, -1L);\r\ndmacfg |= GEM_BF(RXBS, bp->rx_buffer_size / RX_BUFFER_MULTIPLE);\r\ndmacfg |= GEM_BF(FBLDO, 16);\r\ndmacfg |= GEM_BIT(TXPBMS) | GEM_BF(RXBMS, -1L);\r\ndmacfg &= ~GEM_BIT(ENDIA);\r\ngem_writel(bp, DMACFG, dmacfg);\r\n}\r\n}\r\nstatic void macb_configure_caps(struct macb *bp)\r\n{\r\nif (macb_is_gem(bp)) {\r\nif (GEM_BFEXT(IRQCOR, gem_readl(bp, DCFG1)) == 0)\r\nbp->caps |= MACB_CAPS_ISR_CLEAR_ON_WRITE;\r\n}\r\n}\r\nstatic void macb_init_hw(struct macb *bp)\r\n{\r\nu32 config;\r\nmacb_reset_hw(bp);\r\nmacb_set_hwaddr(bp);\r\nconfig = macb_mdc_clk_div(bp);\r\nconfig |= MACB_BF(RBOF, NET_IP_ALIGN);\r\nconfig |= MACB_BIT(PAE);\r\nconfig |= MACB_BIT(DRFCS);\r\nconfig |= MACB_BIT(BIG);\r\nif (bp->dev->flags & IFF_PROMISC)\r\nconfig |= MACB_BIT(CAF);\r\nif (!(bp->dev->flags & IFF_BROADCAST))\r\nconfig |= MACB_BIT(NBC);\r\nconfig |= macb_dbw(bp);\r\nmacb_writel(bp, NCFGR, config);\r\nbp->speed = SPEED_10;\r\nbp->duplex = DUPLEX_HALF;\r\nmacb_configure_dma(bp);\r\nmacb_configure_caps(bp);\r\nmacb_writel(bp, RBQP, bp->rx_ring_dma);\r\nmacb_writel(bp, TBQP, bp->tx_ring_dma);\r\nmacb_writel(bp, NCR, MACB_BIT(RE) | MACB_BIT(TE) | MACB_BIT(MPE));\r\nmacb_writel(bp, IER, (MACB_RX_INT_FLAGS\r\n| MACB_TX_INT_FLAGS\r\n| MACB_BIT(HRESP)));\r\n}\r\nstatic inline int hash_bit_value(int bitnr, __u8 *addr)\r\n{\r\nif (addr[bitnr / 8] & (1 << (bitnr % 8)))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int hash_get_index(__u8 *addr)\r\n{\r\nint i, j, bitval;\r\nint hash_index = 0;\r\nfor (j = 0; j < 6; j++) {\r\nfor (i = 0, bitval = 0; i < 8; i++)\r\nbitval ^= hash_bit_value(i*6 + j, addr);\r\nhash_index |= (bitval << j);\r\n}\r\nreturn hash_index;\r\n}\r\nstatic void macb_sethashtable(struct net_device *dev)\r\n{\r\nstruct netdev_hw_addr *ha;\r\nunsigned long mc_filter[2];\r\nunsigned int bitnr;\r\nstruct macb *bp = netdev_priv(dev);\r\nmc_filter[0] = mc_filter[1] = 0;\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nbitnr = hash_get_index(ha->addr);\r\nmc_filter[bitnr >> 5] |= 1 << (bitnr & 31);\r\n}\r\nmacb_or_gem_writel(bp, HRB, mc_filter[0]);\r\nmacb_or_gem_writel(bp, HRT, mc_filter[1]);\r\n}\r\nvoid macb_set_rx_mode(struct net_device *dev)\r\n{\r\nunsigned long cfg;\r\nstruct macb *bp = netdev_priv(dev);\r\ncfg = macb_readl(bp, NCFGR);\r\nif (dev->flags & IFF_PROMISC)\r\ncfg |= MACB_BIT(CAF);\r\nelse if (dev->flags & (~IFF_PROMISC))\r\ncfg &= ~MACB_BIT(CAF);\r\nif (dev->flags & IFF_ALLMULTI) {\r\nmacb_or_gem_writel(bp, HRB, -1);\r\nmacb_or_gem_writel(bp, HRT, -1);\r\ncfg |= MACB_BIT(NCFGR_MTI);\r\n} else if (!netdev_mc_empty(dev)) {\r\nmacb_sethashtable(dev);\r\ncfg |= MACB_BIT(NCFGR_MTI);\r\n} else if (dev->flags & (~IFF_ALLMULTI)) {\r\nmacb_or_gem_writel(bp, HRB, 0);\r\nmacb_or_gem_writel(bp, HRT, 0);\r\ncfg &= ~MACB_BIT(NCFGR_MTI);\r\n}\r\nmacb_writel(bp, NCFGR, cfg);\r\n}\r\nstatic int macb_open(struct net_device *dev)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\nsize_t bufsz = dev->mtu + ETH_HLEN + ETH_FCS_LEN + NET_IP_ALIGN;\r\nint err;\r\nnetdev_dbg(bp->dev, "open\n");\r\nnetif_carrier_off(dev);\r\nif (!bp->phy_dev)\r\nreturn -EAGAIN;\r\nmacb_init_rx_buffer_size(bp, bufsz);\r\nerr = macb_alloc_consistent(bp);\r\nif (err) {\r\nnetdev_err(dev, "Unable to allocate DMA memory (error %d)\n",\r\nerr);\r\nreturn err;\r\n}\r\nnapi_enable(&bp->napi);\r\nbp->macbgem_ops.mog_init_rings(bp);\r\nmacb_init_hw(bp);\r\nphy_start(bp->phy_dev);\r\nnetif_start_queue(dev);\r\nreturn 0;\r\n}\r\nstatic int macb_close(struct net_device *dev)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\nunsigned long flags;\r\nnetif_stop_queue(dev);\r\nnapi_disable(&bp->napi);\r\nif (bp->phy_dev)\r\nphy_stop(bp->phy_dev);\r\nspin_lock_irqsave(&bp->lock, flags);\r\nmacb_reset_hw(bp);\r\nnetif_carrier_off(dev);\r\nspin_unlock_irqrestore(&bp->lock, flags);\r\nmacb_free_consistent(bp);\r\nreturn 0;\r\n}\r\nstatic void gem_update_stats(struct macb *bp)\r\n{\r\nu32 __iomem *reg = bp->regs + GEM_OTX;\r\nu32 *p = &bp->hw_stats.gem.tx_octets_31_0;\r\nu32 *end = &bp->hw_stats.gem.rx_udp_checksum_errors + 1;\r\nfor (; p < end; p++, reg++)\r\n*p += __raw_readl(reg);\r\n}\r\nstatic struct net_device_stats *gem_get_stats(struct macb *bp)\r\n{\r\nstruct gem_stats *hwstat = &bp->hw_stats.gem;\r\nstruct net_device_stats *nstat = &bp->stats;\r\ngem_update_stats(bp);\r\nnstat->rx_errors = (hwstat->rx_frame_check_sequence_errors +\r\nhwstat->rx_alignment_errors +\r\nhwstat->rx_resource_errors +\r\nhwstat->rx_overruns +\r\nhwstat->rx_oversize_frames +\r\nhwstat->rx_jabbers +\r\nhwstat->rx_undersized_frames +\r\nhwstat->rx_length_field_frame_errors);\r\nnstat->tx_errors = (hwstat->tx_late_collisions +\r\nhwstat->tx_excessive_collisions +\r\nhwstat->tx_underrun +\r\nhwstat->tx_carrier_sense_errors);\r\nnstat->multicast = hwstat->rx_multicast_frames;\r\nnstat->collisions = (hwstat->tx_single_collision_frames +\r\nhwstat->tx_multiple_collision_frames +\r\nhwstat->tx_excessive_collisions);\r\nnstat->rx_length_errors = (hwstat->rx_oversize_frames +\r\nhwstat->rx_jabbers +\r\nhwstat->rx_undersized_frames +\r\nhwstat->rx_length_field_frame_errors);\r\nnstat->rx_over_errors = hwstat->rx_resource_errors;\r\nnstat->rx_crc_errors = hwstat->rx_frame_check_sequence_errors;\r\nnstat->rx_frame_errors = hwstat->rx_alignment_errors;\r\nnstat->rx_fifo_errors = hwstat->rx_overruns;\r\nnstat->tx_aborted_errors = hwstat->tx_excessive_collisions;\r\nnstat->tx_carrier_errors = hwstat->tx_carrier_sense_errors;\r\nnstat->tx_fifo_errors = hwstat->tx_underrun;\r\nreturn nstat;\r\n}\r\nstruct net_device_stats *macb_get_stats(struct net_device *dev)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\nstruct net_device_stats *nstat = &bp->stats;\r\nstruct macb_stats *hwstat = &bp->hw_stats.macb;\r\nif (macb_is_gem(bp))\r\nreturn gem_get_stats(bp);\r\nmacb_update_stats(bp);\r\nnstat->rx_errors = (hwstat->rx_fcs_errors +\r\nhwstat->rx_align_errors +\r\nhwstat->rx_resource_errors +\r\nhwstat->rx_overruns +\r\nhwstat->rx_oversize_pkts +\r\nhwstat->rx_jabbers +\r\nhwstat->rx_undersize_pkts +\r\nhwstat->sqe_test_errors +\r\nhwstat->rx_length_mismatch);\r\nnstat->tx_errors = (hwstat->tx_late_cols +\r\nhwstat->tx_excessive_cols +\r\nhwstat->tx_underruns +\r\nhwstat->tx_carrier_errors);\r\nnstat->collisions = (hwstat->tx_single_cols +\r\nhwstat->tx_multiple_cols +\r\nhwstat->tx_excessive_cols);\r\nnstat->rx_length_errors = (hwstat->rx_oversize_pkts +\r\nhwstat->rx_jabbers +\r\nhwstat->rx_undersize_pkts +\r\nhwstat->rx_length_mismatch);\r\nnstat->rx_over_errors = hwstat->rx_resource_errors +\r\nhwstat->rx_overruns;\r\nnstat->rx_crc_errors = hwstat->rx_fcs_errors;\r\nnstat->rx_frame_errors = hwstat->rx_align_errors;\r\nnstat->rx_fifo_errors = hwstat->rx_overruns;\r\nnstat->tx_aborted_errors = hwstat->tx_excessive_cols;\r\nnstat->tx_carrier_errors = hwstat->tx_carrier_errors;\r\nnstat->tx_fifo_errors = hwstat->tx_underruns;\r\nreturn nstat;\r\n}\r\nstatic int macb_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\nstruct phy_device *phydev = bp->phy_dev;\r\nif (!phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_gset(phydev, cmd);\r\n}\r\nstatic int macb_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\nstruct phy_device *phydev = bp->phy_dev;\r\nif (!phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_sset(phydev, cmd);\r\n}\r\nstatic int macb_get_regs_len(struct net_device *netdev)\r\n{\r\nreturn MACB_GREGS_NBR * sizeof(u32);\r\n}\r\nstatic void macb_get_regs(struct net_device *dev, struct ethtool_regs *regs,\r\nvoid *p)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\nunsigned int tail, head;\r\nu32 *regs_buff = p;\r\nregs->version = (macb_readl(bp, MID) & ((1 << MACB_REV_SIZE) - 1))\r\n| MACB_GREGS_VERSION;\r\ntail = macb_tx_ring_wrap(bp->tx_tail);\r\nhead = macb_tx_ring_wrap(bp->tx_head);\r\nregs_buff[0] = macb_readl(bp, NCR);\r\nregs_buff[1] = macb_or_gem_readl(bp, NCFGR);\r\nregs_buff[2] = macb_readl(bp, NSR);\r\nregs_buff[3] = macb_readl(bp, TSR);\r\nregs_buff[4] = macb_readl(bp, RBQP);\r\nregs_buff[5] = macb_readl(bp, TBQP);\r\nregs_buff[6] = macb_readl(bp, RSR);\r\nregs_buff[7] = macb_readl(bp, IMR);\r\nregs_buff[8] = tail;\r\nregs_buff[9] = head;\r\nregs_buff[10] = macb_tx_dma(bp, tail);\r\nregs_buff[11] = macb_tx_dma(bp, head);\r\nif (macb_is_gem(bp)) {\r\nregs_buff[12] = gem_readl(bp, USRIO);\r\nregs_buff[13] = gem_readl(bp, DMACFG);\r\n}\r\n}\r\nint macb_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct macb *bp = netdev_priv(dev);\r\nstruct phy_device *phydev = bp->phy_dev;\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nif (!phydev)\r\nreturn -ENODEV;\r\nreturn phy_mii_ioctl(phydev, rq, cmd);\r\n}\r\nstatic int __init macb_probe(struct platform_device *pdev)\r\n{\r\nstruct macb_platform_data *pdata;\r\nstruct resource *regs;\r\nstruct net_device *dev;\r\nstruct macb *bp;\r\nstruct phy_device *phydev;\r\nu32 config;\r\nint err = -ENXIO;\r\nstruct pinctrl *pinctrl;\r\nconst char *mac;\r\nregs = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!regs) {\r\ndev_err(&pdev->dev, "no mmio resource defined\n");\r\ngoto err_out;\r\n}\r\npinctrl = devm_pinctrl_get_select_default(&pdev->dev);\r\nif (IS_ERR(pinctrl)) {\r\nerr = PTR_ERR(pinctrl);\r\nif (err == -EPROBE_DEFER)\r\ngoto err_out;\r\ndev_warn(&pdev->dev, "No pinctrl provided\n");\r\n}\r\nerr = -ENOMEM;\r\ndev = alloc_etherdev(sizeof(*bp));\r\nif (!dev)\r\ngoto err_out;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\ndev->features |= 0;\r\nbp = netdev_priv(dev);\r\nbp->pdev = pdev;\r\nbp->dev = dev;\r\nspin_lock_init(&bp->lock);\r\nINIT_WORK(&bp->tx_error_task, macb_tx_error_task);\r\nbp->pclk = devm_clk_get(&pdev->dev, "pclk");\r\nif (IS_ERR(bp->pclk)) {\r\nerr = PTR_ERR(bp->pclk);\r\ndev_err(&pdev->dev, "failed to get macb_clk (%u)\n", err);\r\ngoto err_out_free_dev;\r\n}\r\nbp->hclk = devm_clk_get(&pdev->dev, "hclk");\r\nif (IS_ERR(bp->hclk)) {\r\nerr = PTR_ERR(bp->hclk);\r\ndev_err(&pdev->dev, "failed to get hclk (%u)\n", err);\r\ngoto err_out_free_dev;\r\n}\r\nbp->tx_clk = devm_clk_get(&pdev->dev, "tx_clk");\r\nerr = clk_prepare_enable(bp->pclk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable pclk (%u)\n", err);\r\ngoto err_out_free_dev;\r\n}\r\nerr = clk_prepare_enable(bp->hclk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable hclk (%u)\n", err);\r\ngoto err_out_disable_pclk;\r\n}\r\nif (!IS_ERR(bp->tx_clk)) {\r\nerr = clk_prepare_enable(bp->tx_clk);\r\nif (err) {\r\ndev_err(&pdev->dev, "failed to enable tx_clk (%u)\n",\r\nerr);\r\ngoto err_out_disable_hclk;\r\n}\r\n}\r\nbp->regs = devm_ioremap(&pdev->dev, regs->start, resource_size(regs));\r\nif (!bp->regs) {\r\ndev_err(&pdev->dev, "failed to map registers, aborting.\n");\r\nerr = -ENOMEM;\r\ngoto err_out_disable_clocks;\r\n}\r\ndev->irq = platform_get_irq(pdev, 0);\r\nerr = devm_request_irq(&pdev->dev, dev->irq, macb_interrupt, 0,\r\ndev->name, dev);\r\nif (err) {\r\ndev_err(&pdev->dev, "Unable to request IRQ %d (error %d)\n",\r\ndev->irq, err);\r\ngoto err_out_disable_clocks;\r\n}\r\ndev->netdev_ops = &macb_netdev_ops;\r\nnetif_napi_add(dev, &bp->napi, macb_poll, 64);\r\ndev->ethtool_ops = &macb_ethtool_ops;\r\ndev->base_addr = regs->start;\r\nif (macb_is_gem(bp)) {\r\nbp->macbgem_ops.mog_alloc_rx_buffers = gem_alloc_rx_buffers;\r\nbp->macbgem_ops.mog_free_rx_buffers = gem_free_rx_buffers;\r\nbp->macbgem_ops.mog_init_rings = gem_init_rings;\r\nbp->macbgem_ops.mog_rx = gem_rx;\r\n} else {\r\nbp->macbgem_ops.mog_alloc_rx_buffers = macb_alloc_rx_buffers;\r\nbp->macbgem_ops.mog_free_rx_buffers = macb_free_rx_buffers;\r\nbp->macbgem_ops.mog_init_rings = macb_init_rings;\r\nbp->macbgem_ops.mog_rx = macb_rx;\r\n}\r\nconfig = macb_mdc_clk_div(bp);\r\nconfig |= macb_dbw(bp);\r\nmacb_writel(bp, NCFGR, config);\r\nmac = of_get_mac_address(pdev->dev.of_node);\r\nif (mac)\r\nmemcpy(bp->dev->dev_addr, mac, ETH_ALEN);\r\nelse\r\nmacb_get_hwaddr(bp);\r\nerr = of_get_phy_mode(pdev->dev.of_node);\r\nif (err < 0) {\r\npdata = dev_get_platdata(&pdev->dev);\r\nif (pdata && pdata->is_rmii)\r\nbp->phy_interface = PHY_INTERFACE_MODE_RMII;\r\nelse\r\nbp->phy_interface = PHY_INTERFACE_MODE_MII;\r\n} else {\r\nbp->phy_interface = err;\r\n}\r\nif (bp->phy_interface == PHY_INTERFACE_MODE_RGMII)\r\nmacb_or_gem_writel(bp, USRIO, GEM_BIT(RGMII));\r\nelse if (bp->phy_interface == PHY_INTERFACE_MODE_RMII)\r\n#if defined(CONFIG_ARCH_AT91)\r\nmacb_or_gem_writel(bp, USRIO, (MACB_BIT(RMII) |\r\nMACB_BIT(CLKEN)));\r\n#else\r\nmacb_or_gem_writel(bp, USRIO, 0);\r\n#endif\r\nelse\r\n#if defined(CONFIG_ARCH_AT91)\r\nmacb_or_gem_writel(bp, USRIO, MACB_BIT(CLKEN));\r\n#else\r\nmacb_or_gem_writel(bp, USRIO, MACB_BIT(MII));\r\n#endif\r\nerr = register_netdev(dev);\r\nif (err) {\r\ndev_err(&pdev->dev, "Cannot register net device, aborting.\n");\r\ngoto err_out_disable_clocks;\r\n}\r\nerr = macb_mii_init(bp);\r\nif (err)\r\ngoto err_out_unregister_netdev;\r\nplatform_set_drvdata(pdev, dev);\r\nnetif_carrier_off(dev);\r\nnetdev_info(dev, "Cadence %s at 0x%08lx irq %d (%pM)\n",\r\nmacb_is_gem(bp) ? "GEM" : "MACB", dev->base_addr,\r\ndev->irq, dev->dev_addr);\r\nphydev = bp->phy_dev;\r\nnetdev_info(dev, "attached PHY driver [%s] (mii_bus:phy_addr=%s, irq=%d)\n",\r\nphydev->drv->name, dev_name(&phydev->dev), phydev->irq);\r\nreturn 0;\r\nerr_out_unregister_netdev:\r\nunregister_netdev(dev);\r\nerr_out_disable_clocks:\r\nif (!IS_ERR(bp->tx_clk))\r\nclk_disable_unprepare(bp->tx_clk);\r\nerr_out_disable_hclk:\r\nclk_disable_unprepare(bp->hclk);\r\nerr_out_disable_pclk:\r\nclk_disable_unprepare(bp->pclk);\r\nerr_out_free_dev:\r\nfree_netdev(dev);\r\nerr_out:\r\nreturn err;\r\n}\r\nstatic int __exit macb_remove(struct platform_device *pdev)\r\n{\r\nstruct net_device *dev;\r\nstruct macb *bp;\r\ndev = platform_get_drvdata(pdev);\r\nif (dev) {\r\nbp = netdev_priv(dev);\r\nif (bp->phy_dev)\r\nphy_disconnect(bp->phy_dev);\r\nmdiobus_unregister(bp->mii_bus);\r\nkfree(bp->mii_bus->irq);\r\nmdiobus_free(bp->mii_bus);\r\nunregister_netdev(dev);\r\nif (!IS_ERR(bp->tx_clk))\r\nclk_disable_unprepare(bp->tx_clk);\r\nclk_disable_unprepare(bp->hclk);\r\nclk_disable_unprepare(bp->pclk);\r\nfree_netdev(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic int macb_suspend(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct net_device *netdev = platform_get_drvdata(pdev);\r\nstruct macb *bp = netdev_priv(netdev);\r\nnetif_carrier_off(netdev);\r\nnetif_device_detach(netdev);\r\nif (!IS_ERR(bp->tx_clk))\r\nclk_disable_unprepare(bp->tx_clk);\r\nclk_disable_unprepare(bp->hclk);\r\nclk_disable_unprepare(bp->pclk);\r\nreturn 0;\r\n}\r\nstatic int macb_resume(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct net_device *netdev = platform_get_drvdata(pdev);\r\nstruct macb *bp = netdev_priv(netdev);\r\nclk_prepare_enable(bp->pclk);\r\nclk_prepare_enable(bp->hclk);\r\nif (!IS_ERR(bp->tx_clk))\r\nclk_prepare_enable(bp->tx_clk);\r\nnetif_device_attach(netdev);\r\nreturn 0;\r\n}
