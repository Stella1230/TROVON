static int read_mpidr(void)\r\n{\r\nunsigned int id;\r\nasm volatile ("mrc p15, 0, %0, c0, c0, 5" : "=r" (id));\r\nreturn id & MPIDR_HWID_BITMASK;\r\n}\r\nstatic s64 get_ns(void)\r\n{\r\nstruct timespec ts;\r\ngetnstimeofday(&ts);\r\nreturn timespec_to_ns(&ts);\r\n}\r\nstatic void bL_do_switch(void *_arg)\r\n{\r\nunsigned ib_mpidr, ib_cpu, ib_cluster;\r\nlong volatile handshake, **handshake_ptr = _arg;\r\npr_debug("%s\n", __func__);\r\nib_mpidr = cpu_logical_map(smp_processor_id());\r\nib_cpu = MPIDR_AFFINITY_LEVEL(ib_mpidr, 0);\r\nib_cluster = MPIDR_AFFINITY_LEVEL(ib_mpidr, 1);\r\nif (handshake_ptr) {\r\nhandshake = 0;\r\n*handshake_ptr = &handshake;\r\n} else\r\nhandshake = -1;\r\nmcpm_set_entry_vector(ib_cpu, ib_cluster, cpu_resume);\r\nsev();\r\nwhile (!handshake) {\r\nwfe();\r\nsmp_mb();\r\n}\r\nmcpm_cpu_power_down();\r\nBUG();\r\n}\r\nstatic int bL_switchpoint(unsigned long _arg)\r\n{\r\nunsigned int mpidr = read_mpidr();\r\nunsigned int clusterid = MPIDR_AFFINITY_LEVEL(mpidr, 1);\r\nvoid *stack = current_thread_info() + 1;\r\nstack = PTR_ALIGN(stack, L1_CACHE_BYTES);\r\nstack += clusterid * STACK_SIZE + STACK_SIZE;\r\ncall_with_stack(bL_do_switch, (void *)_arg, stack);\r\nBUG();\r\n}\r\nstatic int bL_switch_to(unsigned int new_cluster_id)\r\n{\r\nunsigned int mpidr, this_cpu, that_cpu;\r\nunsigned int ob_mpidr, ob_cpu, ob_cluster, ib_mpidr, ib_cpu, ib_cluster;\r\nstruct completion inbound_alive;\r\nstruct tick_device *tdev;\r\nenum clock_event_mode tdev_mode;\r\nlong volatile *handshake_ptr;\r\nint ipi_nr, ret;\r\nthis_cpu = smp_processor_id();\r\nob_mpidr = read_mpidr();\r\nob_cpu = MPIDR_AFFINITY_LEVEL(ob_mpidr, 0);\r\nob_cluster = MPIDR_AFFINITY_LEVEL(ob_mpidr, 1);\r\nBUG_ON(cpu_logical_map(this_cpu) != ob_mpidr);\r\nif (new_cluster_id == ob_cluster)\r\nreturn 0;\r\nthat_cpu = bL_switcher_cpu_pairing[this_cpu];\r\nib_mpidr = cpu_logical_map(that_cpu);\r\nib_cpu = MPIDR_AFFINITY_LEVEL(ib_mpidr, 0);\r\nib_cluster = MPIDR_AFFINITY_LEVEL(ib_mpidr, 1);\r\npr_debug("before switch: CPU %d MPIDR %#x -> %#x\n",\r\nthis_cpu, ob_mpidr, ib_mpidr);\r\nthis_cpu = smp_processor_id();\r\nmcpm_set_entry_vector(ob_cpu, ob_cluster, NULL);\r\nmcpm_set_entry_vector(ib_cpu, ib_cluster, NULL);\r\ninit_completion(&inbound_alive);\r\nipi_nr = register_ipi_completion(&inbound_alive, this_cpu);\r\nipi_nr |= ((1 << 16) << bL_gic_id[ob_cpu][ob_cluster]);\r\nmcpm_set_early_poke(ib_cpu, ib_cluster, gic_get_sgir_physaddr(), ipi_nr);\r\nret = mcpm_cpu_power_up(ib_cpu, ib_cluster);\r\nif (ret) {\r\npr_err("%s: mcpm_cpu_power_up() returned %d\n", __func__, ret);\r\nreturn ret;\r\n}\r\ngic_send_sgi(bL_gic_id[ib_cpu][ib_cluster], 0);\r\nwait_for_completion(&inbound_alive);\r\nmcpm_set_early_poke(ib_cpu, ib_cluster, 0, 0);\r\nlocal_irq_disable();\r\nlocal_fiq_disable();\r\ntrace_cpu_migrate_begin(get_ns(), ob_mpidr);\r\ngic_migrate_target(bL_gic_id[ib_cpu][ib_cluster]);\r\ntdev = tick_get_device(this_cpu);\r\nif (tdev && !cpumask_equal(tdev->evtdev->cpumask, cpumask_of(this_cpu)))\r\ntdev = NULL;\r\nif (tdev) {\r\ntdev_mode = tdev->evtdev->mode;\r\nclockevents_set_mode(tdev->evtdev, CLOCK_EVT_MODE_SHUTDOWN);\r\n}\r\nret = cpu_pm_enter();\r\nif (ret)\r\npanic("%s: cpu_pm_enter() returned %d\n", __func__, ret);\r\ncpu_logical_map(this_cpu) = ib_mpidr;\r\ncpu_logical_map(that_cpu) = ob_mpidr;\r\nret = cpu_suspend((unsigned long)&handshake_ptr, bL_switchpoint);\r\nif (ret > 0)\r\npanic("%s: cpu_suspend() returned %d\n", __func__, ret);\r\nmpidr = read_mpidr();\r\npr_debug("after switch: CPU %d MPIDR %#x\n", this_cpu, mpidr);\r\nBUG_ON(mpidr != ib_mpidr);\r\nmcpm_cpu_powered_up();\r\nret = cpu_pm_exit();\r\nif (tdev) {\r\nclockevents_set_mode(tdev->evtdev, tdev_mode);\r\nclockevents_program_event(tdev->evtdev,\r\ntdev->evtdev->next_event, 1);\r\n}\r\ntrace_cpu_migrate_finish(get_ns(), ib_mpidr);\r\nlocal_fiq_enable();\r\nlocal_irq_enable();\r\n*handshake_ptr = 1;\r\ndsb_sev();\r\nif (ret)\r\npr_err("%s exiting with error %d\n", __func__, ret);\r\nreturn ret;\r\n}\r\nstatic int bL_switcher_thread(void *arg)\r\n{\r\nstruct bL_thread *t = arg;\r\nstruct sched_param param = { .sched_priority = 1 };\r\nint cluster;\r\nbL_switch_completion_handler completer;\r\nvoid *completer_cookie;\r\nsched_setscheduler_nocheck(current, SCHED_FIFO, &param);\r\ncomplete(&t->started);\r\ndo {\r\nif (signal_pending(current))\r\nflush_signals(current);\r\nwait_event_interruptible(t->wq,\r\nt->wanted_cluster != -1 ||\r\nkthread_should_stop());\r\nspin_lock(&t->lock);\r\ncluster = t->wanted_cluster;\r\ncompleter = t->completer;\r\ncompleter_cookie = t->completer_cookie;\r\nt->wanted_cluster = -1;\r\nt->completer = NULL;\r\nspin_unlock(&t->lock);\r\nif (cluster != -1) {\r\nbL_switch_to(cluster);\r\nif (completer)\r\ncompleter(completer_cookie);\r\n}\r\n} while (!kthread_should_stop());\r\nreturn 0;\r\n}\r\nstatic struct task_struct *bL_switcher_thread_create(int cpu, void *arg)\r\n{\r\nstruct task_struct *task;\r\ntask = kthread_create_on_node(bL_switcher_thread, arg,\r\ncpu_to_node(cpu), "kswitcher_%d", cpu);\r\nif (!IS_ERR(task)) {\r\nkthread_bind(task, cpu);\r\nwake_up_process(task);\r\n} else\r\npr_err("%s failed for CPU %d\n", __func__, cpu);\r\nreturn task;\r\n}\r\nint bL_switch_request_cb(unsigned int cpu, unsigned int new_cluster_id,\r\nbL_switch_completion_handler completer,\r\nvoid *completer_cookie)\r\n{\r\nstruct bL_thread *t;\r\nif (cpu >= ARRAY_SIZE(bL_threads)) {\r\npr_err("%s: cpu %d out of bounds\n", __func__, cpu);\r\nreturn -EINVAL;\r\n}\r\nt = &bL_threads[cpu];\r\nif (IS_ERR(t->task))\r\nreturn PTR_ERR(t->task);\r\nif (!t->task)\r\nreturn -ESRCH;\r\nspin_lock(&t->lock);\r\nif (t->completer) {\r\nspin_unlock(&t->lock);\r\nreturn -EBUSY;\r\n}\r\nt->completer = completer;\r\nt->completer_cookie = completer_cookie;\r\nt->wanted_cluster = new_cluster_id;\r\nspin_unlock(&t->lock);\r\nwake_up(&t->wq);\r\nreturn 0;\r\n}\r\nint bL_switcher_register_notifier(struct notifier_block *nb)\r\n{\r\nreturn blocking_notifier_chain_register(&bL_activation_notifier, nb);\r\n}\r\nint bL_switcher_unregister_notifier(struct notifier_block *nb)\r\n{\r\nreturn blocking_notifier_chain_unregister(&bL_activation_notifier, nb);\r\n}\r\nstatic int bL_activation_notify(unsigned long val)\r\n{\r\nint ret;\r\nret = blocking_notifier_call_chain(&bL_activation_notifier, val, NULL);\r\nif (ret & NOTIFY_STOP_MASK)\r\npr_err("%s: notifier chain failed with status 0x%x\n",\r\n__func__, ret);\r\nreturn notifier_to_errno(ret);\r\n}\r\nstatic void bL_switcher_restore_cpus(void)\r\n{\r\nint i;\r\nfor_each_cpu(i, &bL_switcher_removed_logical_cpus) {\r\nstruct device *cpu_dev = get_cpu_device(i);\r\nint ret = device_online(cpu_dev);\r\nif (ret)\r\ndev_err(cpu_dev, "switcher: unable to restore CPU\n");\r\n}\r\n}\r\nstatic int bL_switcher_halve_cpus(void)\r\n{\r\nint i, j, cluster_0, gic_id, ret;\r\nunsigned int cpu, cluster, mask;\r\ncpumask_t available_cpus;\r\nmask = 0;\r\nfor_each_online_cpu(i) {\r\ncpu = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 0);\r\ncluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 1);\r\nif (cluster >= 2) {\r\npr_err("%s: only dual cluster systems are supported\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nif (WARN_ON(cpu >= MAX_CPUS_PER_CLUSTER))\r\nreturn -EINVAL;\r\nmask |= (1 << cluster);\r\n}\r\nif (mask != 3) {\r\npr_err("%s: no CPU pairing possible\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nmemset(bL_switcher_cpu_pairing, -1, sizeof(bL_switcher_cpu_pairing));\r\ncpumask_copy(&available_cpus, cpu_online_mask);\r\ncluster_0 = -1;\r\nfor_each_cpu(i, &available_cpus) {\r\nint match = -1;\r\ncluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 1);\r\nif (cluster_0 == -1)\r\ncluster_0 = cluster;\r\nif (cluster != cluster_0)\r\ncontinue;\r\ncpumask_clear_cpu(i, &available_cpus);\r\nfor_each_cpu(j, &available_cpus) {\r\ncluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(j), 1);\r\nif (cluster != cluster_0)\r\nmatch = j;\r\n}\r\nif (match != -1) {\r\nbL_switcher_cpu_pairing[i] = match;\r\ncpumask_clear_cpu(match, &available_cpus);\r\npr_info("CPU%d paired with CPU%d\n", i, match);\r\n}\r\n}\r\ncpumask_clear(&bL_switcher_removed_logical_cpus);\r\nfor_each_online_cpu(i) {\r\ncpu = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 0);\r\ncluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 1);\r\ngic_id = gic_get_cpu_id(i);\r\nif (gic_id < 0) {\r\npr_err("%s: bad GIC ID for CPU %d\n", __func__, i);\r\nbL_switcher_restore_cpus();\r\nreturn -EINVAL;\r\n}\r\nbL_gic_id[cpu][cluster] = gic_id;\r\npr_info("GIC ID for CPU %u cluster %u is %u\n",\r\ncpu, cluster, gic_id);\r\nif (bL_switcher_cpu_pairing[i] != -1) {\r\nbL_switcher_cpu_original_cluster[i] = cluster;\r\ncontinue;\r\n}\r\nret = device_offline(get_cpu_device(i));\r\nif (ret) {\r\nbL_switcher_restore_cpus();\r\nreturn ret;\r\n}\r\ncpumask_set_cpu(i, &bL_switcher_removed_logical_cpus);\r\n}\r\nreturn 0;\r\n}\r\nint bL_switcher_get_logical_index(u32 mpidr)\r\n{\r\nint cpu;\r\nif (!bL_switcher_active)\r\nreturn -EUNATCH;\r\nmpidr &= MPIDR_HWID_BITMASK;\r\nfor_each_online_cpu(cpu) {\r\nint pairing = bL_switcher_cpu_pairing[cpu];\r\nif (pairing == -1)\r\ncontinue;\r\nif ((mpidr == cpu_logical_map(cpu)) ||\r\n(mpidr == cpu_logical_map(pairing)))\r\nreturn cpu;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic void bL_switcher_trace_trigger_cpu(void *__always_unused info)\r\n{\r\ntrace_cpu_migrate_current(get_ns(), read_mpidr());\r\n}\r\nint bL_switcher_trace_trigger(void)\r\n{\r\nint ret;\r\npreempt_disable();\r\nbL_switcher_trace_trigger_cpu(NULL);\r\nret = smp_call_function(bL_switcher_trace_trigger_cpu, NULL, true);\r\npreempt_enable();\r\nreturn ret;\r\n}\r\nstatic int bL_switcher_enable(void)\r\n{\r\nint cpu, ret;\r\nmutex_lock(&bL_switcher_activation_lock);\r\nlock_device_hotplug();\r\nif (bL_switcher_active) {\r\nunlock_device_hotplug();\r\nmutex_unlock(&bL_switcher_activation_lock);\r\nreturn 0;\r\n}\r\npr_info("big.LITTLE switcher initializing\n");\r\nret = bL_activation_notify(BL_NOTIFY_PRE_ENABLE);\r\nif (ret)\r\ngoto error;\r\nret = bL_switcher_halve_cpus();\r\nif (ret)\r\ngoto error;\r\nbL_switcher_trace_trigger();\r\nfor_each_online_cpu(cpu) {\r\nstruct bL_thread *t = &bL_threads[cpu];\r\nspin_lock_init(&t->lock);\r\ninit_waitqueue_head(&t->wq);\r\ninit_completion(&t->started);\r\nt->wanted_cluster = -1;\r\nt->task = bL_switcher_thread_create(cpu, t);\r\n}\r\nbL_switcher_active = 1;\r\nbL_activation_notify(BL_NOTIFY_POST_ENABLE);\r\npr_info("big.LITTLE switcher initialized\n");\r\ngoto out;\r\nerror:\r\npr_warn("big.LITTLE switcher initialization failed\n");\r\nbL_activation_notify(BL_NOTIFY_POST_DISABLE);\r\nout:\r\nunlock_device_hotplug();\r\nmutex_unlock(&bL_switcher_activation_lock);\r\nreturn ret;\r\n}\r\nstatic void bL_switcher_disable(void)\r\n{\r\nunsigned int cpu, cluster;\r\nstruct bL_thread *t;\r\nstruct task_struct *task;\r\nmutex_lock(&bL_switcher_activation_lock);\r\nlock_device_hotplug();\r\nif (!bL_switcher_active)\r\ngoto out;\r\nif (bL_activation_notify(BL_NOTIFY_PRE_DISABLE) != 0) {\r\nbL_activation_notify(BL_NOTIFY_POST_ENABLE);\r\ngoto out;\r\n}\r\nbL_switcher_active = 0;\r\nfor_each_online_cpu(cpu) {\r\nt = &bL_threads[cpu];\r\ntask = t->task;\r\nt->task = NULL;\r\nif (!task || IS_ERR(task))\r\ncontinue;\r\nkthread_stop(task);\r\ncluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(cpu), 1);\r\nif (cluster == bL_switcher_cpu_original_cluster[cpu])\r\ncontinue;\r\ninit_completion(&t->started);\r\nt->wanted_cluster = bL_switcher_cpu_original_cluster[cpu];\r\ntask = bL_switcher_thread_create(cpu, t);\r\nif (!IS_ERR(task)) {\r\nwait_for_completion(&t->started);\r\nkthread_stop(task);\r\ncluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(cpu), 1);\r\nif (cluster == bL_switcher_cpu_original_cluster[cpu])\r\ncontinue;\r\n}\r\npr_crit("%s: unable to restore original cluster for CPU %d\n",\r\n__func__, cpu);\r\npr_crit("%s: CPU %d can't be restored\n",\r\n__func__, bL_switcher_cpu_pairing[cpu]);\r\ncpumask_clear_cpu(bL_switcher_cpu_pairing[cpu],\r\n&bL_switcher_removed_logical_cpus);\r\n}\r\nbL_switcher_restore_cpus();\r\nbL_switcher_trace_trigger();\r\nbL_activation_notify(BL_NOTIFY_POST_DISABLE);\r\nout:\r\nunlock_device_hotplug();\r\nmutex_unlock(&bL_switcher_activation_lock);\r\n}\r\nstatic ssize_t bL_switcher_active_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", bL_switcher_active);\r\n}\r\nstatic ssize_t bL_switcher_active_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr, const char *buf, size_t count)\r\n{\r\nint ret;\r\nswitch (buf[0]) {\r\ncase '0':\r\nbL_switcher_disable();\r\nret = 0;\r\nbreak;\r\ncase '1':\r\nret = bL_switcher_enable();\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\n}\r\nreturn (ret >= 0) ? count : ret;\r\n}\r\nstatic ssize_t bL_switcher_trace_trigger_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr, const char *buf, size_t count)\r\n{\r\nint ret = bL_switcher_trace_trigger();\r\nreturn ret ? ret : count;\r\n}\r\nstatic int __init bL_switcher_sysfs_init(void)\r\n{\r\nint ret;\r\nbL_switcher_kobj = kobject_create_and_add("bL_switcher", kernel_kobj);\r\nif (!bL_switcher_kobj)\r\nreturn -ENOMEM;\r\nret = sysfs_create_group(bL_switcher_kobj, &bL_switcher_attr_group);\r\nif (ret)\r\nkobject_put(bL_switcher_kobj);\r\nreturn ret;\r\n}\r\nbool bL_switcher_get_enabled(void)\r\n{\r\nmutex_lock(&bL_switcher_activation_lock);\r\nreturn bL_switcher_active;\r\n}\r\nvoid bL_switcher_put_enabled(void)\r\n{\r\nmutex_unlock(&bL_switcher_activation_lock);\r\n}\r\nstatic int bL_switcher_hotplug_callback(struct notifier_block *nfb,\r\nunsigned long action, void *hcpu)\r\n{\r\nif (bL_switcher_active) {\r\nint pairing = bL_switcher_cpu_pairing[(unsigned long)hcpu];\r\nswitch (action & 0xf) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_DOWN_PREPARE:\r\nif (pairing == -1)\r\nreturn NOTIFY_BAD;\r\n}\r\n}\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic int __init bL_switcher_init(void)\r\n{\r\nint ret;\r\nif (!mcpm_is_available())\r\nreturn -ENODEV;\r\ncpu_notifier(bL_switcher_hotplug_callback, 0);\r\nif (!no_bL_switcher) {\r\nret = bL_switcher_enable();\r\nif (ret)\r\nreturn ret;\r\n}\r\n#ifdef CONFIG_SYSFS\r\nret = bL_switcher_sysfs_init();\r\nif (ret)\r\npr_err("%s: unable to create sysfs entry\n", __func__);\r\n#endif\r\nreturn 0;\r\n}
