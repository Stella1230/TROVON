int __amd64_read_pci_cfg_dword(struct pci_dev *pdev, int offset,\r\nu32 *val, const char *func)\r\n{\r\nint err = 0;\r\nerr = pci_read_config_dword(pdev, offset, val);\r\nif (err)\r\namd64_warn("%s: error reading F%dx%03x.\n",\r\nfunc, PCI_FUNC(pdev->devfn), offset);\r\nreturn err;\r\n}\r\nint __amd64_write_pci_cfg_dword(struct pci_dev *pdev, int offset,\r\nu32 val, const char *func)\r\n{\r\nint err = 0;\r\nerr = pci_write_config_dword(pdev, offset, val);\r\nif (err)\r\namd64_warn("%s: error writing to F%dx%03x.\n",\r\nfunc, PCI_FUNC(pdev->devfn), offset);\r\nreturn err;\r\n}\r\nstatic int k8_read_dct_pci_cfg(struct amd64_pvt *pvt, int addr, u32 *val,\r\nconst char *func)\r\n{\r\nif (addr >= 0x100)\r\nreturn -EINVAL;\r\nreturn __amd64_read_pci_cfg_dword(pvt->F2, addr, val, func);\r\n}\r\nstatic int f10_read_dct_pci_cfg(struct amd64_pvt *pvt, int addr, u32 *val,\r\nconst char *func)\r\n{\r\nreturn __amd64_read_pci_cfg_dword(pvt->F2, addr, val, func);\r\n}\r\nstatic void f15h_select_dct(struct amd64_pvt *pvt, u8 dct)\r\n{\r\nu32 reg = 0;\r\namd64_read_pci_cfg(pvt->F1, DCT_CFG_SEL, &reg);\r\nreg &= (pvt->model >= 0x30) ? ~3 : ~1;\r\nreg |= dct;\r\namd64_write_pci_cfg(pvt->F1, DCT_CFG_SEL, reg);\r\n}\r\nstatic int f15_read_dct_pci_cfg(struct amd64_pvt *pvt, int addr, u32 *val,\r\nconst char *func)\r\n{\r\nu8 dct = 0;\r\nif (addr >= 0x140 && addr <= 0x1a0) {\r\ndct = (pvt->model >= 0x30) ? 3 : 1;\r\naddr -= 0x100;\r\n}\r\nf15h_select_dct(pvt, dct);\r\nreturn __amd64_read_pci_cfg_dword(pvt->F2, addr, val, func);\r\n}\r\nstatic int __set_scrub_rate(struct pci_dev *ctl, u32 new_bw, u32 min_rate)\r\n{\r\nu32 scrubval;\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(scrubrates) - 1; i++) {\r\nif (scrubrates[i].scrubval < min_rate)\r\ncontinue;\r\nif (scrubrates[i].bandwidth <= new_bw)\r\nbreak;\r\n}\r\nscrubval = scrubrates[i].scrubval;\r\npci_write_bits32(ctl, SCRCTRL, scrubval, 0x001F);\r\nif (scrubval)\r\nreturn scrubrates[i].bandwidth;\r\nreturn 0;\r\n}\r\nstatic int set_scrub_rate(struct mem_ctl_info *mci, u32 bw)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu32 min_scrubrate = 0x5;\r\nif (pvt->fam == 0xf)\r\nmin_scrubrate = 0x0;\r\nif (pvt->fam == 0x15 && pvt->model < 0x10)\r\nf15h_select_dct(pvt, 0);\r\nreturn __set_scrub_rate(pvt->F3, bw, min_scrubrate);\r\n}\r\nstatic int get_scrub_rate(struct mem_ctl_info *mci)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu32 scrubval = 0;\r\nint i, retval = -EINVAL;\r\nif (pvt->fam == 0x15 && pvt->model < 0x10)\r\nf15h_select_dct(pvt, 0);\r\namd64_read_pci_cfg(pvt->F3, SCRCTRL, &scrubval);\r\nscrubval = scrubval & 0x001F;\r\nfor (i = 0; i < ARRAY_SIZE(scrubrates); i++) {\r\nif (scrubrates[i].scrubval == scrubval) {\r\nretval = scrubrates[i].bandwidth;\r\nbreak;\r\n}\r\n}\r\nreturn retval;\r\n}\r\nstatic bool base_limit_match(struct amd64_pvt *pvt, u64 sys_addr, u8 nid)\r\n{\r\nu64 addr;\r\naddr = sys_addr & 0x000000ffffffffffull;\r\nreturn ((addr >= get_dram_base(pvt, nid)) &&\r\n(addr <= get_dram_limit(pvt, nid)));\r\n}\r\nstatic struct mem_ctl_info *find_mc_by_sys_addr(struct mem_ctl_info *mci,\r\nu64 sys_addr)\r\n{\r\nstruct amd64_pvt *pvt;\r\nu8 node_id;\r\nu32 intlv_en, bits;\r\npvt = mci->pvt_info;\r\nintlv_en = dram_intlv_en(pvt, 0);\r\nif (intlv_en == 0) {\r\nfor (node_id = 0; node_id < DRAM_RANGES; node_id++) {\r\nif (base_limit_match(pvt, sys_addr, node_id))\r\ngoto found;\r\n}\r\ngoto err_no_match;\r\n}\r\nif (unlikely((intlv_en != 0x01) &&\r\n(intlv_en != 0x03) &&\r\n(intlv_en != 0x07))) {\r\namd64_warn("DRAM Base[IntlvEn] junk value: 0x%x, BIOS bug?\n", intlv_en);\r\nreturn NULL;\r\n}\r\nbits = (((u32) sys_addr) >> 12) & intlv_en;\r\nfor (node_id = 0; ; ) {\r\nif ((dram_intlv_sel(pvt, node_id) & intlv_en) == bits)\r\nbreak;\r\nif (++node_id >= DRAM_RANGES)\r\ngoto err_no_match;\r\n}\r\nif (unlikely(!base_limit_match(pvt, sys_addr, node_id))) {\r\namd64_warn("%s: sys_addr 0x%llx falls outside base/limit address"\r\n"range for node %d with node interleaving enabled.\n",\r\n__func__, sys_addr, node_id);\r\nreturn NULL;\r\n}\r\nfound:\r\nreturn edac_mc_find((int)node_id);\r\nerr_no_match:\r\nedac_dbg(2, "sys_addr 0x%lx doesn't match any node\n",\r\n(unsigned long)sys_addr);\r\nreturn NULL;\r\n}\r\nstatic void get_cs_base_and_mask(struct amd64_pvt *pvt, int csrow, u8 dct,\r\nu64 *base, u64 *mask)\r\n{\r\nu64 csbase, csmask, base_bits, mask_bits;\r\nu8 addr_shift;\r\nif (pvt->fam == 0xf && pvt->ext_model < K8_REV_F) {\r\ncsbase = pvt->csels[dct].csbases[csrow];\r\ncsmask = pvt->csels[dct].csmasks[csrow];\r\nbase_bits = GENMASK_ULL(31, 21) | GENMASK_ULL(15, 9);\r\nmask_bits = GENMASK_ULL(29, 21) | GENMASK_ULL(15, 9);\r\naddr_shift = 4;\r\n} else if (pvt->fam == 0x16 ||\r\n(pvt->fam == 0x15 && pvt->model >= 0x30)) {\r\ncsbase = pvt->csels[dct].csbases[csrow];\r\ncsmask = pvt->csels[dct].csmasks[csrow >> 1];\r\n*base = (csbase & GENMASK_ULL(15, 5)) << 6;\r\n*base |= (csbase & GENMASK_ULL(30, 19)) << 8;\r\n*mask = ~0ULL;\r\n*mask &= ~((GENMASK_ULL(15, 5) << 6) |\r\n(GENMASK_ULL(30, 19) << 8));\r\n*mask |= (csmask & GENMASK_ULL(15, 5)) << 6;\r\n*mask |= (csmask & GENMASK_ULL(30, 19)) << 8;\r\nreturn;\r\n} else {\r\ncsbase = pvt->csels[dct].csbases[csrow];\r\ncsmask = pvt->csels[dct].csmasks[csrow >> 1];\r\naddr_shift = 8;\r\nif (pvt->fam == 0x15)\r\nbase_bits = mask_bits =\r\nGENMASK_ULL(30,19) | GENMASK_ULL(13,5);\r\nelse\r\nbase_bits = mask_bits =\r\nGENMASK_ULL(28,19) | GENMASK_ULL(13,5);\r\n}\r\n*base = (csbase & base_bits) << addr_shift;\r\n*mask = ~0ULL;\r\n*mask &= ~(mask_bits << addr_shift);\r\n*mask |= (csmask & mask_bits) << addr_shift;\r\n}\r\nstatic int input_addr_to_csrow(struct mem_ctl_info *mci, u64 input_addr)\r\n{\r\nstruct amd64_pvt *pvt;\r\nint csrow;\r\nu64 base, mask;\r\npvt = mci->pvt_info;\r\nfor_each_chip_select(csrow, 0, pvt) {\r\nif (!csrow_enabled(csrow, 0, pvt))\r\ncontinue;\r\nget_cs_base_and_mask(pvt, csrow, 0, &base, &mask);\r\nmask = ~mask;\r\nif ((input_addr & mask) == (base & mask)) {\r\nedac_dbg(2, "InputAddr 0x%lx matches csrow %d (node %d)\n",\r\n(unsigned long)input_addr, csrow,\r\npvt->mc_node_id);\r\nreturn csrow;\r\n}\r\n}\r\nedac_dbg(2, "no matching csrow for InputAddr 0x%lx (MC node %d)\n",\r\n(unsigned long)input_addr, pvt->mc_node_id);\r\nreturn -1;\r\n}\r\nint amd64_get_dram_hole_info(struct mem_ctl_info *mci, u64 *hole_base,\r\nu64 *hole_offset, u64 *hole_size)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nif (pvt->fam == 0xf && pvt->ext_model < K8_REV_E) {\r\nedac_dbg(1, " revision %d for node %d does not support DHAR\n",\r\npvt->ext_model, pvt->mc_node_id);\r\nreturn 1;\r\n}\r\nif (pvt->fam >= 0x10 && !dhar_mem_hoist_valid(pvt)) {\r\nedac_dbg(1, " Dram Memory Hoisting is DISABLED on this system\n");\r\nreturn 1;\r\n}\r\nif (!dhar_valid(pvt)) {\r\nedac_dbg(1, " Dram Memory Hoisting is DISABLED on this node %d\n",\r\npvt->mc_node_id);\r\nreturn 1;\r\n}\r\n*hole_base = dhar_base(pvt);\r\n*hole_size = (1ULL << 32) - *hole_base;\r\n*hole_offset = (pvt->fam > 0xf) ? f10_dhar_offset(pvt)\r\n: k8_dhar_offset(pvt);\r\nedac_dbg(1, " DHAR info for node %d base 0x%lx offset 0x%lx size 0x%lx\n",\r\npvt->mc_node_id, (unsigned long)*hole_base,\r\n(unsigned long)*hole_offset, (unsigned long)*hole_size);\r\nreturn 0;\r\n}\r\nstatic u64 sys_addr_to_dram_addr(struct mem_ctl_info *mci, u64 sys_addr)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu64 dram_base, hole_base, hole_offset, hole_size, dram_addr;\r\nint ret;\r\ndram_base = get_dram_base(pvt, pvt->mc_node_id);\r\nret = amd64_get_dram_hole_info(mci, &hole_base, &hole_offset,\r\n&hole_size);\r\nif (!ret) {\r\nif ((sys_addr >= (1ULL << 32)) &&\r\n(sys_addr < ((1ULL << 32) + hole_size))) {\r\ndram_addr = sys_addr - hole_offset;\r\nedac_dbg(2, "using DHAR to translate SysAddr 0x%lx to DramAddr 0x%lx\n",\r\n(unsigned long)sys_addr,\r\n(unsigned long)dram_addr);\r\nreturn dram_addr;\r\n}\r\n}\r\ndram_addr = (sys_addr & GENMASK_ULL(39, 0)) - dram_base;\r\nedac_dbg(2, "using DRAM Base register to translate SysAddr 0x%lx to DramAddr 0x%lx\n",\r\n(unsigned long)sys_addr, (unsigned long)dram_addr);\r\nreturn dram_addr;\r\n}\r\nstatic int num_node_interleave_bits(unsigned intlv_en)\r\n{\r\nstatic const int intlv_shift_table[] = { 0, 1, 0, 2, 0, 0, 0, 3 };\r\nint n;\r\nBUG_ON(intlv_en > 7);\r\nn = intlv_shift_table[intlv_en];\r\nreturn n;\r\n}\r\nstatic u64 dram_addr_to_input_addr(struct mem_ctl_info *mci, u64 dram_addr)\r\n{\r\nstruct amd64_pvt *pvt;\r\nint intlv_shift;\r\nu64 input_addr;\r\npvt = mci->pvt_info;\r\nintlv_shift = num_node_interleave_bits(dram_intlv_en(pvt, 0));\r\ninput_addr = ((dram_addr >> intlv_shift) & GENMASK_ULL(35, 12)) +\r\n(dram_addr & 0xfff);\r\nedac_dbg(2, " Intlv Shift=%d DramAddr=0x%lx maps to InputAddr=0x%lx\n",\r\nintlv_shift, (unsigned long)dram_addr,\r\n(unsigned long)input_addr);\r\nreturn input_addr;\r\n}\r\nstatic u64 sys_addr_to_input_addr(struct mem_ctl_info *mci, u64 sys_addr)\r\n{\r\nu64 input_addr;\r\ninput_addr =\r\ndram_addr_to_input_addr(mci, sys_addr_to_dram_addr(mci, sys_addr));\r\nedac_dbg(2, "SysAdddr 0x%lx translates to InputAddr 0x%lx\n",\r\n(unsigned long)sys_addr, (unsigned long)input_addr);\r\nreturn input_addr;\r\n}\r\nstatic inline void error_address_to_page_and_offset(u64 error_address,\r\nstruct err_info *err)\r\n{\r\nerr->page = (u32) (error_address >> PAGE_SHIFT);\r\nerr->offset = ((u32) error_address) & ~PAGE_MASK;\r\n}\r\nstatic int sys_addr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr)\r\n{\r\nint csrow;\r\ncsrow = input_addr_to_csrow(mci, sys_addr_to_input_addr(mci, sys_addr));\r\nif (csrow == -1)\r\namd64_mc_err(mci, "Failed to translate InputAddr to csrow for "\r\n"address 0x%lx\n", (unsigned long)sys_addr);\r\nreturn csrow;\r\n}\r\nstatic unsigned long determine_edac_cap(struct amd64_pvt *pvt)\r\n{\r\nu8 bit;\r\nunsigned long edac_cap = EDAC_FLAG_NONE;\r\nbit = (pvt->fam > 0xf || pvt->ext_model >= K8_REV_F)\r\n? 19\r\n: 17;\r\nif (pvt->dclr0 & BIT(bit))\r\nedac_cap = EDAC_FLAG_SECDED;\r\nreturn edac_cap;\r\n}\r\nstatic void debug_dump_dramcfg_low(struct amd64_pvt *pvt, u32 dclr, int chan)\r\n{\r\nedac_dbg(1, "F2x%d90 (DRAM Cfg Low): 0x%08x\n", chan, dclr);\r\nedac_dbg(1, " DIMM type: %sbuffered; all DIMMs support ECC: %s\n",\r\n(dclr & BIT(16)) ? "un" : "",\r\n(dclr & BIT(19)) ? "yes" : "no");\r\nedac_dbg(1, " PAR/ERR parity: %s\n",\r\n(dclr & BIT(8)) ? "enabled" : "disabled");\r\nif (pvt->fam == 0x10)\r\nedac_dbg(1, " DCT 128bit mode width: %s\n",\r\n(dclr & BIT(11)) ? "128b" : "64b");\r\nedac_dbg(1, " x4 logical DIMMs present: L0: %s L1: %s L2: %s L3: %s\n",\r\n(dclr & BIT(12)) ? "yes" : "no",\r\n(dclr & BIT(13)) ? "yes" : "no",\r\n(dclr & BIT(14)) ? "yes" : "no",\r\n(dclr & BIT(15)) ? "yes" : "no");\r\n}\r\nstatic void dump_misc_regs(struct amd64_pvt *pvt)\r\n{\r\nedac_dbg(1, "F3xE8 (NB Cap): 0x%08x\n", pvt->nbcap);\r\nedac_dbg(1, " NB two channel DRAM capable: %s\n",\r\n(pvt->nbcap & NBCAP_DCT_DUAL) ? "yes" : "no");\r\nedac_dbg(1, " ECC capable: %s, ChipKill ECC capable: %s\n",\r\n(pvt->nbcap & NBCAP_SECDED) ? "yes" : "no",\r\n(pvt->nbcap & NBCAP_CHIPKILL) ? "yes" : "no");\r\ndebug_dump_dramcfg_low(pvt, pvt->dclr0, 0);\r\nedac_dbg(1, "F3xB0 (Online Spare): 0x%08x\n", pvt->online_spare);\r\nedac_dbg(1, "F1xF0 (DRAM Hole Address): 0x%08x, base: 0x%08x, offset: 0x%08x\n",\r\npvt->dhar, dhar_base(pvt),\r\n(pvt->fam == 0xf) ? k8_dhar_offset(pvt)\r\n: f10_dhar_offset(pvt));\r\nedac_dbg(1, " DramHoleValid: %s\n", dhar_valid(pvt) ? "yes" : "no");\r\ndebug_display_dimm_sizes(pvt, 0);\r\nif (pvt->fam == 0xf)\r\nreturn;\r\ndebug_display_dimm_sizes(pvt, 1);\r\namd64_info("using %s syndromes.\n", ((pvt->ecc_sym_sz == 8) ? "x8" : "x4"));\r\nif (!dct_ganging_enabled(pvt))\r\ndebug_dump_dramcfg_low(pvt, pvt->dclr1, 1);\r\n}\r\nstatic void prep_chip_selects(struct amd64_pvt *pvt)\r\n{\r\nif (pvt->fam == 0xf && pvt->ext_model < K8_REV_F) {\r\npvt->csels[0].b_cnt = pvt->csels[1].b_cnt = 8;\r\npvt->csels[0].m_cnt = pvt->csels[1].m_cnt = 8;\r\n} else if (pvt->fam == 0x15 && pvt->model >= 0x30) {\r\npvt->csels[0].b_cnt = pvt->csels[1].b_cnt = 4;\r\npvt->csels[0].m_cnt = pvt->csels[1].m_cnt = 2;\r\n} else {\r\npvt->csels[0].b_cnt = pvt->csels[1].b_cnt = 8;\r\npvt->csels[0].m_cnt = pvt->csels[1].m_cnt = 4;\r\n}\r\n}\r\nstatic void read_dct_base_mask(struct amd64_pvt *pvt)\r\n{\r\nint cs;\r\nprep_chip_selects(pvt);\r\nfor_each_chip_select(cs, 0, pvt) {\r\nint reg0 = DCSB0 + (cs * 4);\r\nint reg1 = DCSB1 + (cs * 4);\r\nu32 *base0 = &pvt->csels[0].csbases[cs];\r\nu32 *base1 = &pvt->csels[1].csbases[cs];\r\nif (!amd64_read_dct_pci_cfg(pvt, reg0, base0))\r\nedac_dbg(0, " DCSB0[%d]=0x%08x reg: F2x%x\n",\r\ncs, *base0, reg0);\r\nif (pvt->fam == 0xf || dct_ganging_enabled(pvt))\r\ncontinue;\r\nif (!amd64_read_dct_pci_cfg(pvt, reg1, base1))\r\nedac_dbg(0, " DCSB1[%d]=0x%08x reg: F2x%x\n",\r\ncs, *base1, reg1);\r\n}\r\nfor_each_chip_select_mask(cs, 0, pvt) {\r\nint reg0 = DCSM0 + (cs * 4);\r\nint reg1 = DCSM1 + (cs * 4);\r\nu32 *mask0 = &pvt->csels[0].csmasks[cs];\r\nu32 *mask1 = &pvt->csels[1].csmasks[cs];\r\nif (!amd64_read_dct_pci_cfg(pvt, reg0, mask0))\r\nedac_dbg(0, " DCSM0[%d]=0x%08x reg: F2x%x\n",\r\ncs, *mask0, reg0);\r\nif (pvt->fam == 0xf || dct_ganging_enabled(pvt))\r\ncontinue;\r\nif (!amd64_read_dct_pci_cfg(pvt, reg1, mask1))\r\nedac_dbg(0, " DCSM1[%d]=0x%08x reg: F2x%x\n",\r\ncs, *mask1, reg1);\r\n}\r\n}\r\nstatic enum mem_type determine_memory_type(struct amd64_pvt *pvt, int cs)\r\n{\r\nenum mem_type type;\r\nif (pvt->fam >= 0x15)\r\ntype = (pvt->dclr0 & BIT(16)) ? MEM_DDR3 : MEM_RDDR3;\r\nelse if (pvt->fam == 0x10 || pvt->ext_model >= K8_REV_F) {\r\nif (pvt->dchr0 & DDR3_MODE)\r\ntype = (pvt->dclr0 & BIT(16)) ? MEM_DDR3 : MEM_RDDR3;\r\nelse\r\ntype = (pvt->dclr0 & BIT(16)) ? MEM_DDR2 : MEM_RDDR2;\r\n} else {\r\ntype = (pvt->dclr0 & BIT(18)) ? MEM_DDR : MEM_RDDR;\r\n}\r\namd64_info("CS%d: %s\n", cs, edac_mem_types[type]);\r\nreturn type;\r\n}\r\nstatic int k8_early_channel_count(struct amd64_pvt *pvt)\r\n{\r\nint flag;\r\nif (pvt->ext_model >= K8_REV_F)\r\nflag = pvt->dclr0 & WIDTH_128;\r\nelse\r\nflag = pvt->dclr0 & REVE_WIDTH_128;\r\npvt->dclr1 = 0;\r\nreturn (flag) ? 2 : 1;\r\n}\r\nstatic u64 get_error_address(struct amd64_pvt *pvt, struct mce *m)\r\n{\r\nu64 addr;\r\nu8 start_bit = 1;\r\nu8 end_bit = 47;\r\nif (pvt->fam == 0xf) {\r\nstart_bit = 3;\r\nend_bit = 39;\r\n}\r\naddr = m->addr & GENMASK_ULL(end_bit, start_bit);\r\nif (pvt->fam == 0x15) {\r\nstruct amd64_pvt *pvt;\r\nu64 cc6_base, tmp_addr;\r\nu32 tmp;\r\nu16 mce_nid;\r\nu8 intlv_en;\r\nif ((addr & GENMASK_ULL(47, 24)) >> 24 != 0x00fdf7)\r\nreturn addr;\r\nmce_nid = amd_get_nb_id(m->extcpu);\r\npvt = mcis[mce_nid]->pvt_info;\r\namd64_read_pci_cfg(pvt->F1, DRAM_LOCAL_NODE_LIM, &tmp);\r\nintlv_en = tmp >> 21 & 0x7;\r\ncc6_base = (tmp & GENMASK_ULL(20, 0)) << 3;\r\ncc6_base |= intlv_en ^ 0x7;\r\ncc6_base <<= 24;\r\nif (!intlv_en)\r\nreturn cc6_base | (addr & GENMASK_ULL(23, 0));\r\namd64_read_pci_cfg(pvt->F1, DRAM_LOCAL_NODE_BASE, &tmp);\r\ntmp_addr = (addr & GENMASK_ULL(23, 12)) << __fls(intlv_en + 1);\r\ntmp_addr |= (tmp & GENMASK_ULL(23, 21)) >> 9;\r\ntmp_addr |= addr & GENMASK_ULL(11, 0);\r\nreturn cc6_base | tmp_addr;\r\n}\r\nreturn addr;\r\n}\r\nstatic struct pci_dev *pci_get_related_function(unsigned int vendor,\r\nunsigned int device,\r\nstruct pci_dev *related)\r\n{\r\nstruct pci_dev *dev = NULL;\r\nwhile ((dev = pci_get_device(vendor, device, dev))) {\r\nif (pci_domain_nr(dev->bus) == pci_domain_nr(related->bus) &&\r\n(dev->bus->number == related->bus->number) &&\r\n(PCI_SLOT(dev->devfn) == PCI_SLOT(related->devfn)))\r\nbreak;\r\n}\r\nreturn dev;\r\n}\r\nstatic void read_dram_base_limit_regs(struct amd64_pvt *pvt, unsigned range)\r\n{\r\nstruct amd_northbridge *nb;\r\nstruct pci_dev *f1 = NULL;\r\nunsigned int pci_func;\r\nint off = range << 3;\r\nu32 llim;\r\namd64_read_pci_cfg(pvt->F1, DRAM_BASE_LO + off, &pvt->ranges[range].base.lo);\r\namd64_read_pci_cfg(pvt->F1, DRAM_LIMIT_LO + off, &pvt->ranges[range].lim.lo);\r\nif (pvt->fam == 0xf)\r\nreturn;\r\nif (!dram_rw(pvt, range))\r\nreturn;\r\namd64_read_pci_cfg(pvt->F1, DRAM_BASE_HI + off, &pvt->ranges[range].base.hi);\r\namd64_read_pci_cfg(pvt->F1, DRAM_LIMIT_HI + off, &pvt->ranges[range].lim.hi);\r\nif (pvt->fam != 0x15)\r\nreturn;\r\nnb = node_to_amd_nb(dram_dst_node(pvt, range));\r\nif (WARN_ON(!nb))\r\nreturn;\r\npci_func = (pvt->model == 0x30) ? PCI_DEVICE_ID_AMD_15H_M30H_NB_F1\r\n: PCI_DEVICE_ID_AMD_15H_NB_F1;\r\nf1 = pci_get_related_function(nb->misc->vendor, pci_func, nb->misc);\r\nif (WARN_ON(!f1))\r\nreturn;\r\namd64_read_pci_cfg(f1, DRAM_LOCAL_NODE_LIM, &llim);\r\npvt->ranges[range].lim.lo &= GENMASK_ULL(15, 0);\r\npvt->ranges[range].lim.lo |= ((llim & 0x1fff) << 3 | 0x7) << 16;\r\npvt->ranges[range].lim.hi &= GENMASK_ULL(7, 0);\r\npvt->ranges[range].lim.hi |= llim >> 13;\r\npci_dev_put(f1);\r\n}\r\nstatic void k8_map_sysaddr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr,\r\nstruct err_info *err)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nerror_address_to_page_and_offset(sys_addr, err);\r\nerr->src_mci = find_mc_by_sys_addr(mci, sys_addr);\r\nif (!err->src_mci) {\r\namd64_mc_err(mci, "failed to map error addr 0x%lx to a node\n",\r\n(unsigned long)sys_addr);\r\nerr->err_code = ERR_NODE;\r\nreturn;\r\n}\r\nerr->csrow = sys_addr_to_csrow(err->src_mci, sys_addr);\r\nif (err->csrow < 0) {\r\nerr->err_code = ERR_CSROW;\r\nreturn;\r\n}\r\nif (pvt->nbcfg & NBCFG_CHIPKILL) {\r\nerr->channel = get_channel_from_ecc_syndrome(mci, err->syndrome);\r\nif (err->channel < 0) {\r\namd64_mc_warn(err->src_mci, "unknown syndrome 0x%04x - "\r\n"possible error reporting race\n",\r\nerr->syndrome);\r\nerr->err_code = ERR_CHANNEL;\r\nreturn;\r\n}\r\n} else {\r\nerr->channel = ((sys_addr & BIT(3)) != 0);\r\n}\r\n}\r\nstatic int ddr2_cs_size(unsigned i, bool dct_width)\r\n{\r\nunsigned shift = 0;\r\nif (i <= 2)\r\nshift = i;\r\nelse if (!(i & 0x1))\r\nshift = i >> 1;\r\nelse\r\nshift = (i + 1) >> 1;\r\nreturn 128 << (shift + !!dct_width);\r\n}\r\nstatic int k8_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\r\nunsigned cs_mode)\r\n{\r\nu32 dclr = dct ? pvt->dclr1 : pvt->dclr0;\r\nif (pvt->ext_model >= K8_REV_F) {\r\nWARN_ON(cs_mode > 11);\r\nreturn ddr2_cs_size(cs_mode, dclr & WIDTH_128);\r\n}\r\nelse if (pvt->ext_model >= K8_REV_D) {\r\nunsigned diff;\r\nWARN_ON(cs_mode > 10);\r\ndiff = cs_mode/3 + (unsigned)(cs_mode > 5);\r\nreturn 32 << (cs_mode - diff);\r\n}\r\nelse {\r\nWARN_ON(cs_mode > 6);\r\nreturn 32 << cs_mode;\r\n}\r\n}\r\nstatic int f1x_early_channel_count(struct amd64_pvt *pvt)\r\n{\r\nint i, j, channels = 0;\r\nif (pvt->fam == 0x10 && (pvt->dclr0 & WIDTH_128))\r\nreturn 2;\r\nedac_dbg(0, "Data width is not 128 bits - need more decoding\n");\r\nfor (i = 0; i < 2; i++) {\r\nu32 dbam = (i ? pvt->dbam1 : pvt->dbam0);\r\nfor (j = 0; j < 4; j++) {\r\nif (DBAM_DIMM(j, dbam) > 0) {\r\nchannels++;\r\nbreak;\r\n}\r\n}\r\n}\r\nif (channels > 2)\r\nchannels = 2;\r\namd64_info("MCT channel count: %d\n", channels);\r\nreturn channels;\r\n}\r\nstatic int ddr3_cs_size(unsigned i, bool dct_width)\r\n{\r\nunsigned shift = 0;\r\nint cs_size = 0;\r\nif (i == 0 || i == 3 || i == 4)\r\ncs_size = -1;\r\nelse if (i <= 2)\r\nshift = i;\r\nelse if (i == 12)\r\nshift = 7;\r\nelse if (!(i & 0x1))\r\nshift = i >> 1;\r\nelse\r\nshift = (i + 1) >> 1;\r\nif (cs_size != -1)\r\ncs_size = (128 * (1 << !!dct_width)) << shift;\r\nreturn cs_size;\r\n}\r\nstatic int f10_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\r\nunsigned cs_mode)\r\n{\r\nu32 dclr = dct ? pvt->dclr1 : pvt->dclr0;\r\nWARN_ON(cs_mode > 11);\r\nif (pvt->dchr0 & DDR3_MODE || pvt->dchr1 & DDR3_MODE)\r\nreturn ddr3_cs_size(cs_mode, dclr & WIDTH_128);\r\nelse\r\nreturn ddr2_cs_size(cs_mode, dclr & WIDTH_128);\r\n}\r\nstatic int f15_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\r\nunsigned cs_mode)\r\n{\r\nWARN_ON(cs_mode > 12);\r\nreturn ddr3_cs_size(cs_mode, false);\r\n}\r\nstatic int f16_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\r\nunsigned cs_mode)\r\n{\r\nWARN_ON(cs_mode > 12);\r\nif (cs_mode == 6 || cs_mode == 8 ||\r\ncs_mode == 9 || cs_mode == 12)\r\nreturn -1;\r\nelse\r\nreturn ddr3_cs_size(cs_mode, false);\r\n}\r\nstatic void read_dram_ctl_register(struct amd64_pvt *pvt)\r\n{\r\nif (pvt->fam == 0xf)\r\nreturn;\r\nif (!amd64_read_dct_pci_cfg(pvt, DCT_SEL_LO, &pvt->dct_sel_lo)) {\r\nedac_dbg(0, "F2x110 (DCTSelLow): 0x%08x, High range addrs at: 0x%x\n",\r\npvt->dct_sel_lo, dct_sel_baseaddr(pvt));\r\nedac_dbg(0, " DCTs operate in %s mode\n",\r\n(dct_ganging_enabled(pvt) ? "ganged" : "unganged"));\r\nif (!dct_ganging_enabled(pvt))\r\nedac_dbg(0, " Address range split per DCT: %s\n",\r\n(dct_high_range_enabled(pvt) ? "yes" : "no"));\r\nedac_dbg(0, " data interleave for ECC: %s, DRAM cleared since last warm reset: %s\n",\r\n(dct_data_intlv_enabled(pvt) ? "enabled" : "disabled"),\r\n(dct_memory_cleared(pvt) ? "yes" : "no"));\r\nedac_dbg(0, " channel interleave: %s, "\r\n"interleave bits selector: 0x%x\n",\r\n(dct_interleave_enabled(pvt) ? "enabled" : "disabled"),\r\ndct_sel_interleave_addr(pvt));\r\n}\r\namd64_read_dct_pci_cfg(pvt, DCT_SEL_HI, &pvt->dct_sel_hi);\r\n}\r\nstatic u8 f15_m30h_determine_channel(struct amd64_pvt *pvt, u64 sys_addr,\r\nu8 intlv_en, int num_dcts_intlv,\r\nu32 dct_sel)\r\n{\r\nu8 channel = 0;\r\nu8 select;\r\nif (!(intlv_en))\r\nreturn (u8)(dct_sel);\r\nif (num_dcts_intlv == 2) {\r\nselect = (sys_addr >> 8) & 0x3;\r\nchannel = select ? 0x3 : 0;\r\n} else if (num_dcts_intlv == 4) {\r\nu8 intlv_addr = dct_sel_interleave_addr(pvt);\r\nswitch (intlv_addr) {\r\ncase 0x4:\r\nchannel = (sys_addr >> 8) & 0x3;\r\nbreak;\r\ncase 0x5:\r\nchannel = (sys_addr >> 9) & 0x3;\r\nbreak;\r\n}\r\n}\r\nreturn channel;\r\n}\r\nstatic u8 f1x_determine_channel(struct amd64_pvt *pvt, u64 sys_addr,\r\nbool hi_range_sel, u8 intlv_en)\r\n{\r\nu8 dct_sel_high = (pvt->dct_sel_lo >> 1) & 1;\r\nif (dct_ganging_enabled(pvt))\r\nreturn 0;\r\nif (hi_range_sel)\r\nreturn dct_sel_high;\r\nif (dct_interleave_enabled(pvt)) {\r\nu8 intlv_addr = dct_sel_interleave_addr(pvt);\r\nif (!intlv_addr)\r\nreturn sys_addr >> 6 & 1;\r\nif (intlv_addr & 0x2) {\r\nu8 shift = intlv_addr & 0x1 ? 9 : 6;\r\nu32 temp = hweight_long((u32) ((sys_addr >> 16) & 0x1F)) % 2;\r\nreturn ((sys_addr >> shift) & 1) ^ temp;\r\n}\r\nreturn (sys_addr >> (12 + hweight8(intlv_en))) & 1;\r\n}\r\nif (dct_high_range_enabled(pvt))\r\nreturn ~dct_sel_high & 1;\r\nreturn 0;\r\n}\r\nstatic u64 f1x_get_norm_dct_addr(struct amd64_pvt *pvt, u8 range,\r\nu64 sys_addr, bool hi_rng,\r\nu32 dct_sel_base_addr)\r\n{\r\nu64 chan_off;\r\nu64 dram_base = get_dram_base(pvt, range);\r\nu64 hole_off = f10_dhar_offset(pvt);\r\nu64 dct_sel_base_off = (pvt->dct_sel_hi & 0xFFFFFC00) << 16;\r\nif (hi_rng) {\r\nif ((!(dct_sel_base_addr >> 16) ||\r\ndct_sel_base_addr < dhar_base(pvt)) &&\r\ndhar_valid(pvt) &&\r\n(sys_addr >= BIT_64(32)))\r\nchan_off = hole_off;\r\nelse\r\nchan_off = dct_sel_base_off;\r\n} else {\r\nif (dhar_valid(pvt) && (sys_addr >= BIT_64(32)))\r\nchan_off = hole_off;\r\nelse\r\nchan_off = dram_base;\r\n}\r\nreturn (sys_addr & GENMASK_ULL(47,6)) - (chan_off & GENMASK_ULL(47,23));\r\n}\r\nstatic int f10_process_possible_spare(struct amd64_pvt *pvt, u8 dct, int csrow)\r\n{\r\nint tmp_cs;\r\nif (online_spare_swap_done(pvt, dct) &&\r\ncsrow == online_spare_bad_dramcs(pvt, dct)) {\r\nfor_each_chip_select(tmp_cs, dct, pvt) {\r\nif (chip_select_base(tmp_cs, dct, pvt) & 0x2) {\r\ncsrow = tmp_cs;\r\nbreak;\r\n}\r\n}\r\n}\r\nreturn csrow;\r\n}\r\nstatic int f1x_lookup_addr_in_dct(u64 in_addr, u8 nid, u8 dct)\r\n{\r\nstruct mem_ctl_info *mci;\r\nstruct amd64_pvt *pvt;\r\nu64 cs_base, cs_mask;\r\nint cs_found = -EINVAL;\r\nint csrow;\r\nmci = mcis[nid];\r\nif (!mci)\r\nreturn cs_found;\r\npvt = mci->pvt_info;\r\nedac_dbg(1, "input addr: 0x%llx, DCT: %d\n", in_addr, dct);\r\nfor_each_chip_select(csrow, dct, pvt) {\r\nif (!csrow_enabled(csrow, dct, pvt))\r\ncontinue;\r\nget_cs_base_and_mask(pvt, csrow, dct, &cs_base, &cs_mask);\r\nedac_dbg(1, " CSROW=%d CSBase=0x%llx CSMask=0x%llx\n",\r\ncsrow, cs_base, cs_mask);\r\ncs_mask = ~cs_mask;\r\nedac_dbg(1, " (InputAddr & ~CSMask)=0x%llx (CSBase & ~CSMask)=0x%llx\n",\r\n(in_addr & cs_mask), (cs_base & cs_mask));\r\nif ((in_addr & cs_mask) == (cs_base & cs_mask)) {\r\nif (pvt->fam == 0x15 && pvt->model >= 0x30) {\r\ncs_found = csrow;\r\nbreak;\r\n}\r\ncs_found = f10_process_possible_spare(pvt, dct, csrow);\r\nedac_dbg(1, " MATCH csrow=%d\n", cs_found);\r\nbreak;\r\n}\r\n}\r\nreturn cs_found;\r\n}\r\nstatic u64 f1x_swap_interleaved_region(struct amd64_pvt *pvt, u64 sys_addr)\r\n{\r\nu32 swap_reg, swap_base, swap_limit, rgn_size, tmp_addr;\r\nif (pvt->fam == 0x10) {\r\nif (pvt->model < 4 || (pvt->model < 0xa && pvt->stepping < 3))\r\nreturn sys_addr;\r\n}\r\namd64_read_dct_pci_cfg(pvt, SWAP_INTLV_REG, &swap_reg);\r\nif (!(swap_reg & 0x1))\r\nreturn sys_addr;\r\nswap_base = (swap_reg >> 3) & 0x7f;\r\nswap_limit = (swap_reg >> 11) & 0x7f;\r\nrgn_size = (swap_reg >> 20) & 0x7f;\r\ntmp_addr = sys_addr >> 27;\r\nif (!(sys_addr >> 34) &&\r\n(((tmp_addr >= swap_base) &&\r\n(tmp_addr <= swap_limit)) ||\r\n(tmp_addr < rgn_size)))\r\nreturn sys_addr ^ (u64)swap_base << 27;\r\nreturn sys_addr;\r\n}\r\nstatic int f1x_match_to_this_node(struct amd64_pvt *pvt, unsigned range,\r\nu64 sys_addr, int *chan_sel)\r\n{\r\nint cs_found = -EINVAL;\r\nu64 chan_addr;\r\nu32 dct_sel_base;\r\nu8 channel;\r\nbool high_range = false;\r\nu8 node_id = dram_dst_node(pvt, range);\r\nu8 intlv_en = dram_intlv_en(pvt, range);\r\nu32 intlv_sel = dram_intlv_sel(pvt, range);\r\nedac_dbg(1, "(range %d) SystemAddr= 0x%llx Limit=0x%llx\n",\r\nrange, sys_addr, get_dram_limit(pvt, range));\r\nif (dhar_valid(pvt) &&\r\ndhar_base(pvt) <= sys_addr &&\r\nsys_addr < BIT_64(32)) {\r\namd64_warn("Huh? Address is in the MMIO hole: 0x%016llx\n",\r\nsys_addr);\r\nreturn -EINVAL;\r\n}\r\nif (intlv_en && (intlv_sel != ((sys_addr >> 12) & intlv_en)))\r\nreturn -EINVAL;\r\nsys_addr = f1x_swap_interleaved_region(pvt, sys_addr);\r\ndct_sel_base = dct_sel_baseaddr(pvt);\r\nif (dct_high_range_enabled(pvt) &&\r\n!dct_ganging_enabled(pvt) &&\r\n((sys_addr >> 27) >= (dct_sel_base >> 11)))\r\nhigh_range = true;\r\nchannel = f1x_determine_channel(pvt, sys_addr, high_range, intlv_en);\r\nchan_addr = f1x_get_norm_dct_addr(pvt, range, sys_addr,\r\nhigh_range, dct_sel_base);\r\nif (intlv_en)\r\nchan_addr = ((chan_addr >> (12 + hweight8(intlv_en))) << 12) |\r\n(chan_addr & 0xfff);\r\nif (dct_interleave_enabled(pvt) &&\r\n!dct_high_range_enabled(pvt) &&\r\n!dct_ganging_enabled(pvt)) {\r\nif (dct_sel_interleave_addr(pvt) != 1) {\r\nif (dct_sel_interleave_addr(pvt) == 0x3)\r\nchan_addr = ((chan_addr >> 10) << 9) |\r\n(chan_addr & 0x1ff);\r\nelse\r\nchan_addr = ((chan_addr >> 7) << 6) |\r\n(chan_addr & 0x3f);\r\n} else\r\nchan_addr = ((chan_addr >> 13) << 12) |\r\n(chan_addr & 0xfff);\r\n}\r\nedac_dbg(1, " Normalized DCT addr: 0x%llx\n", chan_addr);\r\ncs_found = f1x_lookup_addr_in_dct(chan_addr, node_id, channel);\r\nif (cs_found >= 0)\r\n*chan_sel = channel;\r\nreturn cs_found;\r\n}\r\nstatic int f15_m30h_match_to_this_node(struct amd64_pvt *pvt, unsigned range,\r\nu64 sys_addr, int *chan_sel)\r\n{\r\nint cs_found = -EINVAL;\r\nint num_dcts_intlv = 0;\r\nu64 chan_addr, chan_offset;\r\nu64 dct_base, dct_limit;\r\nu32 dct_cont_base_reg, dct_cont_limit_reg, tmp;\r\nu8 channel, alias_channel, leg_mmio_hole, dct_sel, dct_offset_en;\r\nu64 dhar_offset = f10_dhar_offset(pvt);\r\nu8 intlv_addr = dct_sel_interleave_addr(pvt);\r\nu8 node_id = dram_dst_node(pvt, range);\r\nu8 intlv_en = dram_intlv_en(pvt, range);\r\namd64_read_pci_cfg(pvt->F1, DRAM_CONT_BASE, &dct_cont_base_reg);\r\namd64_read_pci_cfg(pvt->F1, DRAM_CONT_LIMIT, &dct_cont_limit_reg);\r\ndct_offset_en = (u8) ((dct_cont_base_reg >> 3) & BIT(0));\r\ndct_sel = (u8) ((dct_cont_base_reg >> 4) & 0x7);\r\nedac_dbg(1, "(range %d) SystemAddr= 0x%llx Limit=0x%llx\n",\r\nrange, sys_addr, get_dram_limit(pvt, range));\r\nif (!(get_dram_base(pvt, range) <= sys_addr) &&\r\n!(get_dram_limit(pvt, range) >= sys_addr))\r\nreturn -EINVAL;\r\nif (dhar_valid(pvt) &&\r\ndhar_base(pvt) <= sys_addr &&\r\nsys_addr < BIT_64(32)) {\r\namd64_warn("Huh? Address is in the MMIO hole: 0x%016llx\n",\r\nsys_addr);\r\nreturn -EINVAL;\r\n}\r\ndct_base = (u64) dct_sel_baseaddr(pvt);\r\ndct_limit = (dct_cont_limit_reg >> 11) & 0x1FFF;\r\nif (!(dct_cont_base_reg & BIT(0)) &&\r\n!(dct_base <= (sys_addr >> 27) &&\r\ndct_limit >= (sys_addr >> 27)))\r\nreturn -EINVAL;\r\nnum_dcts_intlv = (int) hweight8(intlv_en);\r\nif (!(num_dcts_intlv % 2 == 0) || (num_dcts_intlv > 4))\r\nreturn -EINVAL;\r\nchannel = f15_m30h_determine_channel(pvt, sys_addr, intlv_en,\r\nnum_dcts_intlv, dct_sel);\r\nif (channel > 3)\r\nreturn -EINVAL;\r\nleg_mmio_hole = (u8) (dct_cont_base_reg >> 1 & BIT(0));\r\nif (leg_mmio_hole && (sys_addr >= BIT_64(32)))\r\nchan_offset = dhar_offset;\r\nelse\r\nchan_offset = dct_base << 27;\r\nchan_addr = sys_addr - chan_offset;\r\nif (num_dcts_intlv == 2) {\r\nif (intlv_addr == 0x4)\r\nchan_addr = ((chan_addr >> 9) << 8) |\r\n(chan_addr & 0xff);\r\nelse if (intlv_addr == 0x5)\r\nchan_addr = ((chan_addr >> 10) << 9) |\r\n(chan_addr & 0x1ff);\r\nelse\r\nreturn -EINVAL;\r\n} else if (num_dcts_intlv == 4) {\r\nif (intlv_addr == 0x4)\r\nchan_addr = ((chan_addr >> 10) << 8) |\r\n(chan_addr & 0xff);\r\nelse if (intlv_addr == 0x5)\r\nchan_addr = ((chan_addr >> 11) << 9) |\r\n(chan_addr & 0x1ff);\r\nelse\r\nreturn -EINVAL;\r\n}\r\nif (dct_offset_en) {\r\namd64_read_pci_cfg(pvt->F1,\r\nDRAM_CONT_HIGH_OFF + (int) channel * 4,\r\n&tmp);\r\nchan_addr += (u64) ((tmp >> 11) & 0xfff) << 27;\r\n}\r\nf15h_select_dct(pvt, channel);\r\nedac_dbg(1, " Normalized DCT addr: 0x%llx\n", chan_addr);\r\nalias_channel = (channel == 3) ? 1 : channel;\r\ncs_found = f1x_lookup_addr_in_dct(chan_addr, node_id, alias_channel);\r\nif (cs_found >= 0)\r\n*chan_sel = alias_channel;\r\nreturn cs_found;\r\n}\r\nstatic int f1x_translate_sysaddr_to_cs(struct amd64_pvt *pvt,\r\nu64 sys_addr,\r\nint *chan_sel)\r\n{\r\nint cs_found = -EINVAL;\r\nunsigned range;\r\nfor (range = 0; range < DRAM_RANGES; range++) {\r\nif (!dram_rw(pvt, range))\r\ncontinue;\r\nif (pvt->fam == 0x15 && pvt->model >= 0x30)\r\ncs_found = f15_m30h_match_to_this_node(pvt, range,\r\nsys_addr,\r\nchan_sel);\r\nelse if ((get_dram_base(pvt, range) <= sys_addr) &&\r\n(get_dram_limit(pvt, range) >= sys_addr)) {\r\ncs_found = f1x_match_to_this_node(pvt, range,\r\nsys_addr, chan_sel);\r\nif (cs_found >= 0)\r\nbreak;\r\n}\r\n}\r\nreturn cs_found;\r\n}\r\nstatic void f1x_map_sysaddr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr,\r\nstruct err_info *err)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nerror_address_to_page_and_offset(sys_addr, err);\r\nerr->csrow = f1x_translate_sysaddr_to_cs(pvt, sys_addr, &err->channel);\r\nif (err->csrow < 0) {\r\nerr->err_code = ERR_CSROW;\r\nreturn;\r\n}\r\nif (dct_ganging_enabled(pvt))\r\nerr->channel = get_channel_from_ecc_syndrome(mci, err->syndrome);\r\n}\r\nstatic void debug_display_dimm_sizes(struct amd64_pvt *pvt, u8 ctrl)\r\n{\r\nint dimm, size0, size1;\r\nu32 *dcsb = ctrl ? pvt->csels[1].csbases : pvt->csels[0].csbases;\r\nu32 dbam = ctrl ? pvt->dbam1 : pvt->dbam0;\r\nif (pvt->fam == 0xf) {\r\nif (pvt->ext_model < K8_REV_F)\r\nreturn;\r\nelse\r\nWARN_ON(ctrl != 0);\r\n}\r\ndbam = (ctrl && !dct_ganging_enabled(pvt)) ? pvt->dbam1 : pvt->dbam0;\r\ndcsb = (ctrl && !dct_ganging_enabled(pvt)) ? pvt->csels[1].csbases\r\n: pvt->csels[0].csbases;\r\nedac_dbg(1, "F2x%d80 (DRAM Bank Address Mapping): 0x%08x\n",\r\nctrl, dbam);\r\nedac_printk(KERN_DEBUG, EDAC_MC, "DCT%d chip selects:\n", ctrl);\r\nfor (dimm = 0; dimm < 4; dimm++) {\r\nsize0 = 0;\r\nif (dcsb[dimm*2] & DCSB_CS_ENABLE)\r\nsize0 = pvt->ops->dbam_to_cs(pvt, ctrl,\r\nDBAM_DIMM(dimm, dbam));\r\nsize1 = 0;\r\nif (dcsb[dimm*2 + 1] & DCSB_CS_ENABLE)\r\nsize1 = pvt->ops->dbam_to_cs(pvt, ctrl,\r\nDBAM_DIMM(dimm, dbam));\r\namd64_info(EDAC_MC ": %d: %5dMB %d: %5dMB\n",\r\ndimm * 2, size0,\r\ndimm * 2 + 1, size1);\r\n}\r\n}\r\nstatic int decode_syndrome(u16 syndrome, const u16 *vectors, unsigned num_vecs,\r\nunsigned v_dim)\r\n{\r\nunsigned int i, err_sym;\r\nfor (err_sym = 0; err_sym < num_vecs / v_dim; err_sym++) {\r\nu16 s = syndrome;\r\nunsigned v_idx = err_sym * v_dim;\r\nunsigned v_end = (err_sym + 1) * v_dim;\r\nfor (i = 1; i < (1U << 16); i <<= 1) {\r\nif (v_idx < v_end && vectors[v_idx] & i) {\r\nu16 ev_comp = vectors[v_idx++];\r\nif (s & i) {\r\ns ^= ev_comp;\r\nif (!s)\r\nreturn err_sym;\r\n}\r\n} else if (s & i)\r\nbreak;\r\n}\r\n}\r\nedac_dbg(0, "syndrome(%x) not found\n", syndrome);\r\nreturn -1;\r\n}\r\nstatic int map_err_sym_to_channel(int err_sym, int sym_size)\r\n{\r\nif (sym_size == 4)\r\nswitch (err_sym) {\r\ncase 0x20:\r\ncase 0x21:\r\nreturn 0;\r\nbreak;\r\ncase 0x22:\r\ncase 0x23:\r\nreturn 1;\r\nbreak;\r\ndefault:\r\nreturn err_sym >> 4;\r\nbreak;\r\n}\r\nelse\r\nswitch (err_sym) {\r\ncase 0x10:\r\nWARN(1, KERN_ERR "Invalid error symbol: 0x%x\n",\r\nerr_sym);\r\nreturn -1;\r\nbreak;\r\ncase 0x11:\r\nreturn 0;\r\nbreak;\r\ncase 0x12:\r\nreturn 1;\r\nbreak;\r\ndefault:\r\nreturn err_sym >> 3;\r\nbreak;\r\n}\r\nreturn -1;\r\n}\r\nstatic int get_channel_from_ecc_syndrome(struct mem_ctl_info *mci, u16 syndrome)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nint err_sym = -1;\r\nif (pvt->ecc_sym_sz == 8)\r\nerr_sym = decode_syndrome(syndrome, x8_vectors,\r\nARRAY_SIZE(x8_vectors),\r\npvt->ecc_sym_sz);\r\nelse if (pvt->ecc_sym_sz == 4)\r\nerr_sym = decode_syndrome(syndrome, x4_vectors,\r\nARRAY_SIZE(x4_vectors),\r\npvt->ecc_sym_sz);\r\nelse {\r\namd64_warn("Illegal syndrome type: %u\n", pvt->ecc_sym_sz);\r\nreturn err_sym;\r\n}\r\nreturn map_err_sym_to_channel(err_sym, pvt->ecc_sym_sz);\r\n}\r\nstatic void __log_bus_error(struct mem_ctl_info *mci, struct err_info *err,\r\nu8 ecc_type)\r\n{\r\nenum hw_event_mc_err_type err_type;\r\nconst char *string;\r\nif (ecc_type == 2)\r\nerr_type = HW_EVENT_ERR_CORRECTED;\r\nelse if (ecc_type == 1)\r\nerr_type = HW_EVENT_ERR_UNCORRECTED;\r\nelse {\r\nWARN(1, "Something is rotten in the state of Denmark.\n");\r\nreturn;\r\n}\r\nswitch (err->err_code) {\r\ncase DECODE_OK:\r\nstring = "";\r\nbreak;\r\ncase ERR_NODE:\r\nstring = "Failed to map error addr to a node";\r\nbreak;\r\ncase ERR_CSROW:\r\nstring = "Failed to map error addr to a csrow";\r\nbreak;\r\ncase ERR_CHANNEL:\r\nstring = "unknown syndrome - possible error reporting race";\r\nbreak;\r\ndefault:\r\nstring = "WTF error";\r\nbreak;\r\n}\r\nedac_mc_handle_error(err_type, mci, 1,\r\nerr->page, err->offset, err->syndrome,\r\nerr->csrow, err->channel, -1,\r\nstring, "");\r\n}\r\nstatic inline void decode_bus_error(int node_id, struct mce *m)\r\n{\r\nstruct mem_ctl_info *mci = mcis[node_id];\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu8 ecc_type = (m->status >> 45) & 0x3;\r\nu8 xec = XEC(m->status, 0x1f);\r\nu16 ec = EC(m->status);\r\nu64 sys_addr;\r\nstruct err_info err;\r\nif (PP(ec) == NBSL_PP_OBS)\r\nreturn;\r\nif (xec && xec != F10_NBSL_EXT_ERR_ECC)\r\nreturn;\r\nmemset(&err, 0, sizeof(err));\r\nsys_addr = get_error_address(pvt, m);\r\nif (ecc_type == 2)\r\nerr.syndrome = extract_syndrome(m->status);\r\npvt->ops->map_sysaddr_to_csrow(mci, sys_addr, &err);\r\n__log_bus_error(mci, &err, ecc_type);\r\n}\r\nstatic int reserve_mc_sibling_devs(struct amd64_pvt *pvt, u16 f1_id, u16 f3_id)\r\n{\r\npvt->F1 = pci_get_related_function(pvt->F2->vendor, f1_id, pvt->F2);\r\nif (!pvt->F1) {\r\namd64_err("error address map device not found: "\r\n"vendor %x device 0x%x (broken BIOS?)\n",\r\nPCI_VENDOR_ID_AMD, f1_id);\r\nreturn -ENODEV;\r\n}\r\npvt->F3 = pci_get_related_function(pvt->F2->vendor, f3_id, pvt->F2);\r\nif (!pvt->F3) {\r\npci_dev_put(pvt->F1);\r\npvt->F1 = NULL;\r\namd64_err("error F3 device not found: "\r\n"vendor %x device 0x%x (broken BIOS?)\n",\r\nPCI_VENDOR_ID_AMD, f3_id);\r\nreturn -ENODEV;\r\n}\r\nedac_dbg(1, "F1: %s\n", pci_name(pvt->F1));\r\nedac_dbg(1, "F2: %s\n", pci_name(pvt->F2));\r\nedac_dbg(1, "F3: %s\n", pci_name(pvt->F3));\r\nreturn 0;\r\n}\r\nstatic void free_mc_sibling_devs(struct amd64_pvt *pvt)\r\n{\r\npci_dev_put(pvt->F1);\r\npci_dev_put(pvt->F3);\r\n}\r\nstatic void read_mc_regs(struct amd64_pvt *pvt)\r\n{\r\nunsigned range;\r\nu64 msr_val;\r\nu32 tmp;\r\nrdmsrl(MSR_K8_TOP_MEM1, pvt->top_mem);\r\nedac_dbg(0, " TOP_MEM: 0x%016llx\n", pvt->top_mem);\r\nrdmsrl(MSR_K8_SYSCFG, msr_val);\r\nif (msr_val & (1U << 21)) {\r\nrdmsrl(MSR_K8_TOP_MEM2, pvt->top_mem2);\r\nedac_dbg(0, " TOP_MEM2: 0x%016llx\n", pvt->top_mem2);\r\n} else\r\nedac_dbg(0, " TOP_MEM2 disabled\n");\r\namd64_read_pci_cfg(pvt->F3, NBCAP, &pvt->nbcap);\r\nread_dram_ctl_register(pvt);\r\nfor (range = 0; range < DRAM_RANGES; range++) {\r\nu8 rw;\r\nread_dram_base_limit_regs(pvt, range);\r\nrw = dram_rw(pvt, range);\r\nif (!rw)\r\ncontinue;\r\nedac_dbg(1, " DRAM range[%d], base: 0x%016llx; limit: 0x%016llx\n",\r\nrange,\r\nget_dram_base(pvt, range),\r\nget_dram_limit(pvt, range));\r\nedac_dbg(1, " IntlvEn=%s; Range access: %s%s IntlvSel=%d DstNode=%d\n",\r\ndram_intlv_en(pvt, range) ? "Enabled" : "Disabled",\r\n(rw & 0x1) ? "R" : "-",\r\n(rw & 0x2) ? "W" : "-",\r\ndram_intlv_sel(pvt, range),\r\ndram_dst_node(pvt, range));\r\n}\r\nread_dct_base_mask(pvt);\r\namd64_read_pci_cfg(pvt->F1, DHAR, &pvt->dhar);\r\namd64_read_dct_pci_cfg(pvt, DBAM0, &pvt->dbam0);\r\namd64_read_pci_cfg(pvt->F3, F10_ONLINE_SPARE, &pvt->online_spare);\r\namd64_read_dct_pci_cfg(pvt, DCLR0, &pvt->dclr0);\r\namd64_read_dct_pci_cfg(pvt, DCHR0, &pvt->dchr0);\r\nif (!dct_ganging_enabled(pvt)) {\r\namd64_read_dct_pci_cfg(pvt, DCLR1, &pvt->dclr1);\r\namd64_read_dct_pci_cfg(pvt, DCHR1, &pvt->dchr1);\r\n}\r\npvt->ecc_sym_sz = 4;\r\nif (pvt->fam >= 0x10) {\r\namd64_read_pci_cfg(pvt->F3, EXT_NB_MCA_CFG, &tmp);\r\nif (pvt->fam != 0x16)\r\namd64_read_dct_pci_cfg(pvt, DBAM1, &pvt->dbam1);\r\nif ((pvt->fam > 0x10 || pvt->model > 7) && tmp & BIT(25))\r\npvt->ecc_sym_sz = 8;\r\n}\r\ndump_misc_regs(pvt);\r\n}\r\nstatic u32 get_csrow_nr_pages(struct amd64_pvt *pvt, u8 dct, int csrow_nr)\r\n{\r\nu32 cs_mode, nr_pages;\r\nu32 dbam = dct ? pvt->dbam1 : pvt->dbam0;\r\ncs_mode = DBAM_DIMM(csrow_nr / 2, dbam);\r\nnr_pages = pvt->ops->dbam_to_cs(pvt, dct, cs_mode) << (20 - PAGE_SHIFT);\r\nedac_dbg(0, "csrow: %d, channel: %d, DBAM idx: %d\n",\r\ncsrow_nr, dct, cs_mode);\r\nedac_dbg(0, "nr_pages/channel: %u\n", nr_pages);\r\nreturn nr_pages;\r\n}\r\nstatic int init_csrows(struct mem_ctl_info *mci)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nstruct csrow_info *csrow;\r\nstruct dimm_info *dimm;\r\nenum edac_type edac_mode;\r\nenum mem_type mtype;\r\nint i, j, empty = 1;\r\nint nr_pages = 0;\r\nu32 val;\r\namd64_read_pci_cfg(pvt->F3, NBCFG, &val);\r\npvt->nbcfg = val;\r\nedac_dbg(0, "node %d, NBCFG=0x%08x[ChipKillEccCap: %d|DramEccEn: %d]\n",\r\npvt->mc_node_id, val,\r\n!!(val & NBCFG_CHIPKILL), !!(val & NBCFG_ECC_ENABLE));\r\nfor_each_chip_select(i, 0, pvt) {\r\nbool row_dct0 = !!csrow_enabled(i, 0, pvt);\r\nbool row_dct1 = false;\r\nif (pvt->fam != 0xf)\r\nrow_dct1 = !!csrow_enabled(i, 1, pvt);\r\nif (!row_dct0 && !row_dct1)\r\ncontinue;\r\ncsrow = mci->csrows[i];\r\nempty = 0;\r\nedac_dbg(1, "MC node: %d, csrow: %d\n",\r\npvt->mc_node_id, i);\r\nif (row_dct0) {\r\nnr_pages = get_csrow_nr_pages(pvt, 0, i);\r\ncsrow->channels[0]->dimm->nr_pages = nr_pages;\r\n}\r\nif (pvt->fam != 0xf && row_dct1) {\r\nint row_dct1_pages = get_csrow_nr_pages(pvt, 1, i);\r\ncsrow->channels[1]->dimm->nr_pages = row_dct1_pages;\r\nnr_pages += row_dct1_pages;\r\n}\r\nmtype = determine_memory_type(pvt, i);\r\nedac_dbg(1, "Total csrow%d pages: %u\n", i, nr_pages);\r\nif (pvt->nbcfg & NBCFG_ECC_ENABLE)\r\nedac_mode = (pvt->nbcfg & NBCFG_CHIPKILL) ?\r\nEDAC_S4ECD4ED : EDAC_SECDED;\r\nelse\r\nedac_mode = EDAC_NONE;\r\nfor (j = 0; j < pvt->channel_count; j++) {\r\ndimm = csrow->channels[j]->dimm;\r\ndimm->mtype = mtype;\r\ndimm->edac_mode = edac_mode;\r\n}\r\n}\r\nreturn empty;\r\n}\r\nstatic void get_cpus_on_this_dct_cpumask(struct cpumask *mask, u16 nid)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nif (amd_get_nb_id(cpu) == nid)\r\ncpumask_set_cpu(cpu, mask);\r\n}\r\nstatic bool nb_mce_bank_enabled_on_node(u16 nid)\r\n{\r\ncpumask_var_t mask;\r\nint cpu, nbe;\r\nbool ret = false;\r\nif (!zalloc_cpumask_var(&mask, GFP_KERNEL)) {\r\namd64_warn("%s: Error allocating mask\n", __func__);\r\nreturn false;\r\n}\r\nget_cpus_on_this_dct_cpumask(mask, nid);\r\nrdmsr_on_cpus(mask, MSR_IA32_MCG_CTL, msrs);\r\nfor_each_cpu(cpu, mask) {\r\nstruct msr *reg = per_cpu_ptr(msrs, cpu);\r\nnbe = reg->l & MSR_MCGCTL_NBE;\r\nedac_dbg(0, "core: %u, MCG_CTL: 0x%llx, NB MSR is %s\n",\r\ncpu, reg->q,\r\n(nbe ? "enabled" : "disabled"));\r\nif (!nbe)\r\ngoto out;\r\n}\r\nret = true;\r\nout:\r\nfree_cpumask_var(mask);\r\nreturn ret;\r\n}\r\nstatic int toggle_ecc_err_reporting(struct ecc_settings *s, u16 nid, bool on)\r\n{\r\ncpumask_var_t cmask;\r\nint cpu;\r\nif (!zalloc_cpumask_var(&cmask, GFP_KERNEL)) {\r\namd64_warn("%s: error allocating mask\n", __func__);\r\nreturn false;\r\n}\r\nget_cpus_on_this_dct_cpumask(cmask, nid);\r\nrdmsr_on_cpus(cmask, MSR_IA32_MCG_CTL, msrs);\r\nfor_each_cpu(cpu, cmask) {\r\nstruct msr *reg = per_cpu_ptr(msrs, cpu);\r\nif (on) {\r\nif (reg->l & MSR_MCGCTL_NBE)\r\ns->flags.nb_mce_enable = 1;\r\nreg->l |= MSR_MCGCTL_NBE;\r\n} else {\r\nif (!s->flags.nb_mce_enable)\r\nreg->l &= ~MSR_MCGCTL_NBE;\r\n}\r\n}\r\nwrmsr_on_cpus(cmask, MSR_IA32_MCG_CTL, msrs);\r\nfree_cpumask_var(cmask);\r\nreturn 0;\r\n}\r\nstatic bool enable_ecc_error_reporting(struct ecc_settings *s, u16 nid,\r\nstruct pci_dev *F3)\r\n{\r\nbool ret = true;\r\nu32 value, mask = 0x3;\r\nif (toggle_ecc_err_reporting(s, nid, ON)) {\r\namd64_warn("Error enabling ECC reporting over MCGCTL!\n");\r\nreturn false;\r\n}\r\namd64_read_pci_cfg(F3, NBCTL, &value);\r\ns->old_nbctl = value & mask;\r\ns->nbctl_valid = true;\r\nvalue |= mask;\r\namd64_write_pci_cfg(F3, NBCTL, value);\r\namd64_read_pci_cfg(F3, NBCFG, &value);\r\nedac_dbg(0, "1: node %d, NBCFG=0x%08x[DramEccEn: %d]\n",\r\nnid, value, !!(value & NBCFG_ECC_ENABLE));\r\nif (!(value & NBCFG_ECC_ENABLE)) {\r\namd64_warn("DRAM ECC disabled on this node, enabling...\n");\r\ns->flags.nb_ecc_prev = 0;\r\nvalue |= NBCFG_ECC_ENABLE;\r\namd64_write_pci_cfg(F3, NBCFG, value);\r\namd64_read_pci_cfg(F3, NBCFG, &value);\r\nif (!(value & NBCFG_ECC_ENABLE)) {\r\namd64_warn("Hardware rejected DRAM ECC enable,"\r\n"check memory DIMM configuration.\n");\r\nret = false;\r\n} else {\r\namd64_info("Hardware accepted DRAM ECC Enable\n");\r\n}\r\n} else {\r\ns->flags.nb_ecc_prev = 1;\r\n}\r\nedac_dbg(0, "2: node %d, NBCFG=0x%08x[DramEccEn: %d]\n",\r\nnid, value, !!(value & NBCFG_ECC_ENABLE));\r\nreturn ret;\r\n}\r\nstatic void restore_ecc_error_reporting(struct ecc_settings *s, u16 nid,\r\nstruct pci_dev *F3)\r\n{\r\nu32 value, mask = 0x3;\r\nif (!s->nbctl_valid)\r\nreturn;\r\namd64_read_pci_cfg(F3, NBCTL, &value);\r\nvalue &= ~mask;\r\nvalue |= s->old_nbctl;\r\namd64_write_pci_cfg(F3, NBCTL, value);\r\nif (!s->flags.nb_ecc_prev) {\r\namd64_read_pci_cfg(F3, NBCFG, &value);\r\nvalue &= ~NBCFG_ECC_ENABLE;\r\namd64_write_pci_cfg(F3, NBCFG, value);\r\n}\r\nif (toggle_ecc_err_reporting(s, nid, OFF))\r\namd64_warn("Error restoring NB MCGCTL settings!\n");\r\n}\r\nstatic bool ecc_enabled(struct pci_dev *F3, u16 nid)\r\n{\r\nu32 value;\r\nu8 ecc_en = 0;\r\nbool nb_mce_en = false;\r\namd64_read_pci_cfg(F3, NBCFG, &value);\r\necc_en = !!(value & NBCFG_ECC_ENABLE);\r\namd64_info("DRAM ECC %s.\n", (ecc_en ? "enabled" : "disabled"));\r\nnb_mce_en = nb_mce_bank_enabled_on_node(nid);\r\nif (!nb_mce_en)\r\namd64_notice("NB MCE bank disabled, set MSR "\r\n"0x%08x[4] on node %d to enable.\n",\r\nMSR_IA32_MCG_CTL, nid);\r\nif (!ecc_en || !nb_mce_en) {\r\namd64_notice("%s", ecc_msg);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int set_mc_sysfs_attrs(struct mem_ctl_info *mci)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nint rc;\r\nrc = amd64_create_sysfs_dbg_files(mci);\r\nif (rc < 0)\r\nreturn rc;\r\nif (pvt->fam >= 0x10) {\r\nrc = amd64_create_sysfs_inject_files(mci);\r\nif (rc < 0)\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nstatic void del_mc_sysfs_attrs(struct mem_ctl_info *mci)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\namd64_remove_sysfs_dbg_files(mci);\r\nif (pvt->fam >= 0x10)\r\namd64_remove_sysfs_inject_files(mci);\r\n}\r\nstatic void setup_mci_misc_attrs(struct mem_ctl_info *mci,\r\nstruct amd64_family_type *fam)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nmci->mtype_cap = MEM_FLAG_DDR2 | MEM_FLAG_RDDR2;\r\nmci->edac_ctl_cap = EDAC_FLAG_NONE;\r\nif (pvt->nbcap & NBCAP_SECDED)\r\nmci->edac_ctl_cap |= EDAC_FLAG_SECDED;\r\nif (pvt->nbcap & NBCAP_CHIPKILL)\r\nmci->edac_ctl_cap |= EDAC_FLAG_S4ECD4ED;\r\nmci->edac_cap = determine_edac_cap(pvt);\r\nmci->mod_name = EDAC_MOD_STR;\r\nmci->mod_ver = EDAC_AMD64_VERSION;\r\nmci->ctl_name = fam->ctl_name;\r\nmci->dev_name = pci_name(pvt->F2);\r\nmci->ctl_page_to_phys = NULL;\r\nmci->set_sdram_scrub_rate = set_scrub_rate;\r\nmci->get_sdram_scrub_rate = get_scrub_rate;\r\n}\r\nstatic struct amd64_family_type *per_family_init(struct amd64_pvt *pvt)\r\n{\r\nstruct amd64_family_type *fam_type = NULL;\r\npvt->ext_model = boot_cpu_data.x86_model >> 4;\r\npvt->stepping = boot_cpu_data.x86_mask;\r\npvt->model = boot_cpu_data.x86_model;\r\npvt->fam = boot_cpu_data.x86;\r\nswitch (pvt->fam) {\r\ncase 0xf:\r\nfam_type = &family_types[K8_CPUS];\r\npvt->ops = &family_types[K8_CPUS].ops;\r\nbreak;\r\ncase 0x10:\r\nfam_type = &family_types[F10_CPUS];\r\npvt->ops = &family_types[F10_CPUS].ops;\r\nbreak;\r\ncase 0x15:\r\nif (pvt->model == 0x30) {\r\nfam_type = &family_types[F15_M30H_CPUS];\r\npvt->ops = &family_types[F15_M30H_CPUS].ops;\r\nbreak;\r\n}\r\nfam_type = &family_types[F15_CPUS];\r\npvt->ops = &family_types[F15_CPUS].ops;\r\nbreak;\r\ncase 0x16:\r\nif (pvt->model == 0x30) {\r\nfam_type = &family_types[F16_M30H_CPUS];\r\npvt->ops = &family_types[F16_M30H_CPUS].ops;\r\nbreak;\r\n}\r\nfam_type = &family_types[F16_CPUS];\r\npvt->ops = &family_types[F16_CPUS].ops;\r\nbreak;\r\ndefault:\r\namd64_err("Unsupported family!\n");\r\nreturn NULL;\r\n}\r\namd64_info("%s %sdetected (node %d).\n", fam_type->ctl_name,\r\n(pvt->fam == 0xf ?\r\n(pvt->ext_model >= K8_REV_F ? "revF or later "\r\n: "revE or earlier ")\r\n: ""), pvt->mc_node_id);\r\nreturn fam_type;\r\n}\r\nstatic int init_one_instance(struct pci_dev *F2)\r\n{\r\nstruct amd64_pvt *pvt = NULL;\r\nstruct amd64_family_type *fam_type = NULL;\r\nstruct mem_ctl_info *mci = NULL;\r\nstruct edac_mc_layer layers[2];\r\nint err = 0, ret;\r\nu16 nid = amd_get_node_id(F2);\r\nret = -ENOMEM;\r\npvt = kzalloc(sizeof(struct amd64_pvt), GFP_KERNEL);\r\nif (!pvt)\r\ngoto err_ret;\r\npvt->mc_node_id = nid;\r\npvt->F2 = F2;\r\nret = -EINVAL;\r\nfam_type = per_family_init(pvt);\r\nif (!fam_type)\r\ngoto err_free;\r\nret = -ENODEV;\r\nerr = reserve_mc_sibling_devs(pvt, fam_type->f1_id, fam_type->f3_id);\r\nif (err)\r\ngoto err_free;\r\nread_mc_regs(pvt);\r\nret = -EINVAL;\r\npvt->channel_count = pvt->ops->early_channel_count(pvt);\r\nif (pvt->channel_count < 0)\r\ngoto err_siblings;\r\nret = -ENOMEM;\r\nlayers[0].type = EDAC_MC_LAYER_CHIP_SELECT;\r\nlayers[0].size = pvt->csels[0].b_cnt;\r\nlayers[0].is_virt_csrow = true;\r\nlayers[1].type = EDAC_MC_LAYER_CHANNEL;\r\nlayers[1].size = 2;\r\nlayers[1].is_virt_csrow = false;\r\nmci = edac_mc_alloc(nid, ARRAY_SIZE(layers), layers, 0);\r\nif (!mci)\r\ngoto err_siblings;\r\nmci->pvt_info = pvt;\r\nmci->pdev = &pvt->F2->dev;\r\nsetup_mci_misc_attrs(mci, fam_type);\r\nif (init_csrows(mci))\r\nmci->edac_cap = EDAC_FLAG_NONE;\r\nret = -ENODEV;\r\nif (edac_mc_add_mc(mci)) {\r\nedac_dbg(1, "failed edac_mc_add_mc()\n");\r\ngoto err_add_mc;\r\n}\r\nif (set_mc_sysfs_attrs(mci)) {\r\nedac_dbg(1, "failed edac_mc_add_mc()\n");\r\ngoto err_add_sysfs;\r\n}\r\nif (report_gart_errors)\r\namd_report_gart_errors(true);\r\namd_register_ecc_decoder(decode_bus_error);\r\nmcis[nid] = mci;\r\natomic_inc(&drv_instances);\r\nreturn 0;\r\nerr_add_sysfs:\r\nedac_mc_del_mc(mci->pdev);\r\nerr_add_mc:\r\nedac_mc_free(mci);\r\nerr_siblings:\r\nfree_mc_sibling_devs(pvt);\r\nerr_free:\r\nkfree(pvt);\r\nerr_ret:\r\nreturn ret;\r\n}\r\nstatic int probe_one_instance(struct pci_dev *pdev,\r\nconst struct pci_device_id *mc_type)\r\n{\r\nu16 nid = amd_get_node_id(pdev);\r\nstruct pci_dev *F3 = node_to_amd_nb(nid)->misc;\r\nstruct ecc_settings *s;\r\nint ret = 0;\r\nret = pci_enable_device(pdev);\r\nif (ret < 0) {\r\nedac_dbg(0, "ret=%d\n", ret);\r\nreturn -EIO;\r\n}\r\nret = -ENOMEM;\r\ns = kzalloc(sizeof(struct ecc_settings), GFP_KERNEL);\r\nif (!s)\r\ngoto err_out;\r\necc_stngs[nid] = s;\r\nif (!ecc_enabled(F3, nid)) {\r\nret = -ENODEV;\r\nif (!ecc_enable_override)\r\ngoto err_enable;\r\namd64_warn("Forcing ECC on!\n");\r\nif (!enable_ecc_error_reporting(s, nid, F3))\r\ngoto err_enable;\r\n}\r\nret = init_one_instance(pdev);\r\nif (ret < 0) {\r\namd64_err("Error probing instance: %d\n", nid);\r\nrestore_ecc_error_reporting(s, nid, F3);\r\n}\r\nreturn ret;\r\nerr_enable:\r\nkfree(s);\r\necc_stngs[nid] = NULL;\r\nerr_out:\r\nreturn ret;\r\n}\r\nstatic void remove_one_instance(struct pci_dev *pdev)\r\n{\r\nstruct mem_ctl_info *mci;\r\nstruct amd64_pvt *pvt;\r\nu16 nid = amd_get_node_id(pdev);\r\nstruct pci_dev *F3 = node_to_amd_nb(nid)->misc;\r\nstruct ecc_settings *s = ecc_stngs[nid];\r\nmci = find_mci_by_dev(&pdev->dev);\r\nWARN_ON(!mci);\r\ndel_mc_sysfs_attrs(mci);\r\nmci = edac_mc_del_mc(&pdev->dev);\r\nif (!mci)\r\nreturn;\r\npvt = mci->pvt_info;\r\nrestore_ecc_error_reporting(s, nid, F3);\r\nfree_mc_sibling_devs(pvt);\r\namd_report_gart_errors(false);\r\namd_unregister_ecc_decoder(decode_bus_error);\r\nkfree(ecc_stngs[nid]);\r\necc_stngs[nid] = NULL;\r\nmci->pvt_info = NULL;\r\nmcis[nid] = NULL;\r\nkfree(pvt);\r\nedac_mc_free(mci);\r\n}\r\nstatic void setup_pci_device(void)\r\n{\r\nstruct mem_ctl_info *mci;\r\nstruct amd64_pvt *pvt;\r\nif (pci_ctl)\r\nreturn;\r\nmci = mcis[0];\r\nif (!mci)\r\nreturn;\r\npvt = mci->pvt_info;\r\npci_ctl = edac_pci_create_generic_ctl(&pvt->F2->dev, EDAC_MOD_STR);\r\nif (!pci_ctl) {\r\npr_warn("%s(): Unable to create PCI control\n", __func__);\r\npr_warn("%s(): PCI error report via EDAC not set\n", __func__);\r\n}\r\n}\r\nstatic int __init amd64_edac_init(void)\r\n{\r\nint err = -ENODEV;\r\nprintk(KERN_INFO "AMD64 EDAC driver v%s\n", EDAC_AMD64_VERSION);\r\nopstate_init();\r\nif (amd_cache_northbridges() < 0)\r\ngoto err_ret;\r\nerr = -ENOMEM;\r\nmcis = kzalloc(amd_nb_num() * sizeof(mcis[0]), GFP_KERNEL);\r\necc_stngs = kzalloc(amd_nb_num() * sizeof(ecc_stngs[0]), GFP_KERNEL);\r\nif (!(mcis && ecc_stngs))\r\ngoto err_free;\r\nmsrs = msrs_alloc();\r\nif (!msrs)\r\ngoto err_free;\r\nerr = pci_register_driver(&amd64_pci_driver);\r\nif (err)\r\ngoto err_pci;\r\nerr = -ENODEV;\r\nif (!atomic_read(&drv_instances))\r\ngoto err_no_instances;\r\nsetup_pci_device();\r\nreturn 0;\r\nerr_no_instances:\r\npci_unregister_driver(&amd64_pci_driver);\r\nerr_pci:\r\nmsrs_free(msrs);\r\nmsrs = NULL;\r\nerr_free:\r\nkfree(mcis);\r\nmcis = NULL;\r\nkfree(ecc_stngs);\r\necc_stngs = NULL;\r\nerr_ret:\r\nreturn err;\r\n}\r\nstatic void __exit amd64_edac_exit(void)\r\n{\r\nif (pci_ctl)\r\nedac_pci_release_generic_ctl(pci_ctl);\r\npci_unregister_driver(&amd64_pci_driver);\r\nkfree(ecc_stngs);\r\necc_stngs = NULL;\r\nkfree(mcis);\r\nmcis = NULL;\r\nmsrs_free(msrs);\r\nmsrs = NULL;\r\n}
