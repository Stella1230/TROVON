void\r\nnouveau_gem_object_del(struct drm_gem_object *gem)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(gem);\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nif (gem->import_attach)\r\ndrm_prime_gem_destroy(gem, nvbo->bo.sg);\r\ndrm_gem_object_release(gem);\r\ngem->filp = NULL;\r\nttm_bo_unref(&bo);\r\n}\r\nint\r\nnouveau_gem_object_open(struct drm_gem_object *gem, struct drm_file *file_priv)\r\n{\r\nstruct nouveau_cli *cli = nouveau_cli(file_priv);\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(gem);\r\nstruct nouveau_vma *vma;\r\nint ret;\r\nif (!cli->base.vm)\r\nreturn 0;\r\nret = ttm_bo_reserve(&nvbo->bo, false, false, false, 0);\r\nif (ret)\r\nreturn ret;\r\nvma = nouveau_bo_vma_find(nvbo, cli->base.vm);\r\nif (!vma) {\r\nvma = kzalloc(sizeof(*vma), GFP_KERNEL);\r\nif (!vma) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = nouveau_bo_vma_add(nvbo, cli->base.vm, vma);\r\nif (ret) {\r\nkfree(vma);\r\ngoto out;\r\n}\r\n} else {\r\nvma->refcount++;\r\n}\r\nout:\r\nttm_bo_unreserve(&nvbo->bo);\r\nreturn ret;\r\n}\r\nstatic void\r\nnouveau_gem_object_delete(void *data)\r\n{\r\nstruct nouveau_vma *vma = data;\r\nnouveau_vm_unmap(vma);\r\nnouveau_vm_put(vma);\r\nkfree(vma);\r\n}\r\nstatic void\r\nnouveau_gem_object_unmap(struct nouveau_bo *nvbo, struct nouveau_vma *vma)\r\n{\r\nconst bool mapped = nvbo->bo.mem.mem_type != TTM_PL_SYSTEM;\r\nstruct nouveau_fence *fence = NULL;\r\nlist_del(&vma->head);\r\nif (mapped) {\r\nspin_lock(&nvbo->bo.bdev->fence_lock);\r\nfence = nouveau_fence_ref(nvbo->bo.sync_obj);\r\nspin_unlock(&nvbo->bo.bdev->fence_lock);\r\n}\r\nif (fence) {\r\nnouveau_fence_work(fence, nouveau_gem_object_delete, vma);\r\n} else {\r\nif (mapped)\r\nnouveau_vm_unmap(vma);\r\nnouveau_vm_put(vma);\r\nkfree(vma);\r\n}\r\nnouveau_fence_unref(&fence);\r\n}\r\nvoid\r\nnouveau_gem_object_close(struct drm_gem_object *gem, struct drm_file *file_priv)\r\n{\r\nstruct nouveau_cli *cli = nouveau_cli(file_priv);\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(gem);\r\nstruct nouveau_vma *vma;\r\nint ret;\r\nif (!cli->base.vm)\r\nreturn;\r\nret = ttm_bo_reserve(&nvbo->bo, false, false, false, 0);\r\nif (ret)\r\nreturn;\r\nvma = nouveau_bo_vma_find(nvbo, cli->base.vm);\r\nif (vma) {\r\nif (--vma->refcount == 0)\r\nnouveau_gem_object_unmap(nvbo, vma);\r\n}\r\nttm_bo_unreserve(&nvbo->bo);\r\n}\r\nint\r\nnouveau_gem_new(struct drm_device *dev, int size, int align, uint32_t domain,\r\nuint32_t tile_mode, uint32_t tile_flags,\r\nstruct nouveau_bo **pnvbo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nstruct nouveau_bo *nvbo;\r\nu32 flags = 0;\r\nint ret;\r\nif (domain & NOUVEAU_GEM_DOMAIN_VRAM)\r\nflags |= TTM_PL_FLAG_VRAM;\r\nif (domain & NOUVEAU_GEM_DOMAIN_GART)\r\nflags |= TTM_PL_FLAG_TT;\r\nif (!flags || domain & NOUVEAU_GEM_DOMAIN_CPU)\r\nflags |= TTM_PL_FLAG_SYSTEM;\r\nret = nouveau_bo_new(dev, size, align, flags, tile_mode,\r\ntile_flags, NULL, pnvbo);\r\nif (ret)\r\nreturn ret;\r\nnvbo = *pnvbo;\r\nnvbo->valid_domains = NOUVEAU_GEM_DOMAIN_VRAM |\r\nNOUVEAU_GEM_DOMAIN_GART;\r\nif (nv_device(drm->device)->card_type >= NV_50)\r\nnvbo->valid_domains &= domain;\r\nret = drm_gem_object_init(dev, &nvbo->gem, nvbo->bo.mem.size);\r\nif (ret) {\r\nnouveau_bo_ref(NULL, pnvbo);\r\nreturn -ENOMEM;\r\n}\r\nnvbo->bo.persistent_swap_storage = nvbo->gem.filp;\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_gem_info(struct drm_file *file_priv, struct drm_gem_object *gem,\r\nstruct drm_nouveau_gem_info *rep)\r\n{\r\nstruct nouveau_cli *cli = nouveau_cli(file_priv);\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(gem);\r\nstruct nouveau_vma *vma;\r\nif (nvbo->bo.mem.mem_type == TTM_PL_TT)\r\nrep->domain = NOUVEAU_GEM_DOMAIN_GART;\r\nelse\r\nrep->domain = NOUVEAU_GEM_DOMAIN_VRAM;\r\nrep->offset = nvbo->bo.offset;\r\nif (cli->base.vm) {\r\nvma = nouveau_bo_vma_find(nvbo, cli->base.vm);\r\nif (!vma)\r\nreturn -EINVAL;\r\nrep->offset = vma->offset;\r\n}\r\nrep->size = nvbo->bo.mem.num_pages << PAGE_SHIFT;\r\nrep->map_handle = drm_vma_node_offset_addr(&nvbo->bo.vma_node);\r\nrep->tile_mode = nvbo->tile_mode;\r\nrep->tile_flags = nvbo->tile_flags;\r\nreturn 0;\r\n}\r\nint\r\nnouveau_gem_ioctl_new(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nstruct nouveau_cli *cli = nouveau_cli(file_priv);\r\nstruct nouveau_fb *pfb = nouveau_fb(drm->device);\r\nstruct drm_nouveau_gem_new *req = data;\r\nstruct nouveau_bo *nvbo = NULL;\r\nint ret = 0;\r\nif (!pfb->memtype_valid(pfb, req->info.tile_flags)) {\r\nNV_ERROR(cli, "bad page flags: 0x%08x\n", req->info.tile_flags);\r\nreturn -EINVAL;\r\n}\r\nret = nouveau_gem_new(dev, req->info.size, req->align,\r\nreq->info.domain, req->info.tile_mode,\r\nreq->info.tile_flags, &nvbo);\r\nif (ret)\r\nreturn ret;\r\nret = drm_gem_handle_create(file_priv, &nvbo->gem, &req->info.handle);\r\nif (ret == 0) {\r\nret = nouveau_gem_info(file_priv, &nvbo->gem, &req->info);\r\nif (ret)\r\ndrm_gem_handle_delete(file_priv, req->info.handle);\r\n}\r\ndrm_gem_object_unreference_unlocked(&nvbo->gem);\r\nreturn ret;\r\n}\r\nstatic int\r\nnouveau_gem_set_domain(struct drm_gem_object *gem, uint32_t read_domains,\r\nuint32_t write_domains, uint32_t valid_domains)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(gem);\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nuint32_t domains = valid_domains & nvbo->valid_domains &\r\n(write_domains ? write_domains : read_domains);\r\nuint32_t pref_flags = 0, valid_flags = 0;\r\nif (!domains)\r\nreturn -EINVAL;\r\nif (valid_domains & NOUVEAU_GEM_DOMAIN_VRAM)\r\nvalid_flags |= TTM_PL_FLAG_VRAM;\r\nif (valid_domains & NOUVEAU_GEM_DOMAIN_GART)\r\nvalid_flags |= TTM_PL_FLAG_TT;\r\nif ((domains & NOUVEAU_GEM_DOMAIN_VRAM) &&\r\nbo->mem.mem_type == TTM_PL_VRAM)\r\npref_flags |= TTM_PL_FLAG_VRAM;\r\nelse if ((domains & NOUVEAU_GEM_DOMAIN_GART) &&\r\nbo->mem.mem_type == TTM_PL_TT)\r\npref_flags |= TTM_PL_FLAG_TT;\r\nelse if (domains & NOUVEAU_GEM_DOMAIN_VRAM)\r\npref_flags |= TTM_PL_FLAG_VRAM;\r\nelse\r\npref_flags |= TTM_PL_FLAG_TT;\r\nnouveau_bo_placement_set(nvbo, pref_flags, valid_flags);\r\nreturn 0;\r\n}\r\nstatic void\r\nvalidate_fini_list(struct list_head *list, struct nouveau_fence *fence,\r\nstruct ww_acquire_ctx *ticket)\r\n{\r\nstruct list_head *entry, *tmp;\r\nstruct nouveau_bo *nvbo;\r\nlist_for_each_safe(entry, tmp, list) {\r\nnvbo = list_entry(entry, struct nouveau_bo, entry);\r\nif (likely(fence))\r\nnouveau_bo_fence(nvbo, fence);\r\nif (unlikely(nvbo->validate_mapped)) {\r\nttm_bo_kunmap(&nvbo->kmap);\r\nnvbo->validate_mapped = false;\r\n}\r\nlist_del(&nvbo->entry);\r\nnvbo->reserved_by = NULL;\r\nttm_bo_unreserve_ticket(&nvbo->bo, ticket);\r\ndrm_gem_object_unreference_unlocked(&nvbo->gem);\r\n}\r\n}\r\nstatic void\r\nvalidate_fini_no_ticket(struct validate_op *op, struct nouveau_fence *fence)\r\n{\r\nvalidate_fini_list(&op->vram_list, fence, &op->ticket);\r\nvalidate_fini_list(&op->gart_list, fence, &op->ticket);\r\nvalidate_fini_list(&op->both_list, fence, &op->ticket);\r\n}\r\nstatic void\r\nvalidate_fini(struct validate_op *op, struct nouveau_fence *fence)\r\n{\r\nvalidate_fini_no_ticket(op, fence);\r\nww_acquire_fini(&op->ticket);\r\n}\r\nstatic int\r\nvalidate_init(struct nouveau_channel *chan, struct drm_file *file_priv,\r\nstruct drm_nouveau_gem_pushbuf_bo *pbbo,\r\nint nr_buffers, struct validate_op *op)\r\n{\r\nstruct nouveau_cli *cli = nouveau_cli(file_priv);\r\nstruct drm_device *dev = chan->drm->dev;\r\nint trycnt = 0;\r\nint ret, i;\r\nstruct nouveau_bo *res_bo = NULL;\r\nww_acquire_init(&op->ticket, &reservation_ww_class);\r\nretry:\r\nif (++trycnt > 100000) {\r\nNV_ERROR(cli, "%s failed and gave up.\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < nr_buffers; i++) {\r\nstruct drm_nouveau_gem_pushbuf_bo *b = &pbbo[i];\r\nstruct drm_gem_object *gem;\r\nstruct nouveau_bo *nvbo;\r\ngem = drm_gem_object_lookup(dev, file_priv, b->handle);\r\nif (!gem) {\r\nNV_ERROR(cli, "Unknown handle 0x%08x\n", b->handle);\r\nww_acquire_done(&op->ticket);\r\nvalidate_fini(op, NULL);\r\nreturn -ENOENT;\r\n}\r\nnvbo = nouveau_gem_object(gem);\r\nif (nvbo == res_bo) {\r\nres_bo = NULL;\r\ndrm_gem_object_unreference_unlocked(gem);\r\ncontinue;\r\n}\r\nif (nvbo->reserved_by && nvbo->reserved_by == file_priv) {\r\nNV_ERROR(cli, "multiple instances of buffer %d on "\r\n"validation list\n", b->handle);\r\ndrm_gem_object_unreference_unlocked(gem);\r\nww_acquire_done(&op->ticket);\r\nvalidate_fini(op, NULL);\r\nreturn -EINVAL;\r\n}\r\nret = ttm_bo_reserve(&nvbo->bo, true, false, true, &op->ticket);\r\nif (ret) {\r\nvalidate_fini_no_ticket(op, NULL);\r\nif (unlikely(ret == -EDEADLK)) {\r\nret = ttm_bo_reserve_slowpath(&nvbo->bo, true,\r\n&op->ticket);\r\nif (!ret)\r\nres_bo = nvbo;\r\n}\r\nif (unlikely(ret)) {\r\nww_acquire_done(&op->ticket);\r\nww_acquire_fini(&op->ticket);\r\ndrm_gem_object_unreference_unlocked(gem);\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(cli, "fail reserve\n");\r\nreturn ret;\r\n}\r\n}\r\nb->user_priv = (uint64_t)(unsigned long)nvbo;\r\nnvbo->reserved_by = file_priv;\r\nnvbo->pbbo_index = i;\r\nif ((b->valid_domains & NOUVEAU_GEM_DOMAIN_VRAM) &&\r\n(b->valid_domains & NOUVEAU_GEM_DOMAIN_GART))\r\nlist_add_tail(&nvbo->entry, &op->both_list);\r\nelse\r\nif (b->valid_domains & NOUVEAU_GEM_DOMAIN_VRAM)\r\nlist_add_tail(&nvbo->entry, &op->vram_list);\r\nelse\r\nif (b->valid_domains & NOUVEAU_GEM_DOMAIN_GART)\r\nlist_add_tail(&nvbo->entry, &op->gart_list);\r\nelse {\r\nNV_ERROR(cli, "invalid valid domains: 0x%08x\n",\r\nb->valid_domains);\r\nlist_add_tail(&nvbo->entry, &op->both_list);\r\nww_acquire_done(&op->ticket);\r\nvalidate_fini(op, NULL);\r\nreturn -EINVAL;\r\n}\r\nif (nvbo == res_bo)\r\ngoto retry;\r\n}\r\nww_acquire_done(&op->ticket);\r\nreturn 0;\r\n}\r\nstatic int\r\nvalidate_sync(struct nouveau_channel *chan, struct nouveau_bo *nvbo)\r\n{\r\nstruct nouveau_fence *fence = NULL;\r\nint ret = 0;\r\nspin_lock(&nvbo->bo.bdev->fence_lock);\r\nfence = nouveau_fence_ref(nvbo->bo.sync_obj);\r\nspin_unlock(&nvbo->bo.bdev->fence_lock);\r\nif (fence) {\r\nret = nouveau_fence_sync(fence, chan);\r\nnouveau_fence_unref(&fence);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nvalidate_list(struct nouveau_channel *chan, struct nouveau_cli *cli,\r\nstruct list_head *list, struct drm_nouveau_gem_pushbuf_bo *pbbo,\r\nuint64_t user_pbbo_ptr)\r\n{\r\nstruct nouveau_drm *drm = chan->drm;\r\nstruct drm_nouveau_gem_pushbuf_bo __user *upbbo =\r\n(void __force __user *)(uintptr_t)user_pbbo_ptr;\r\nstruct nouveau_bo *nvbo;\r\nint ret, relocs = 0;\r\nlist_for_each_entry(nvbo, list, entry) {\r\nstruct drm_nouveau_gem_pushbuf_bo *b = &pbbo[nvbo->pbbo_index];\r\nret = nouveau_gem_set_domain(&nvbo->gem, b->read_domains,\r\nb->write_domains,\r\nb->valid_domains);\r\nif (unlikely(ret)) {\r\nNV_ERROR(cli, "fail set_domain\n");\r\nreturn ret;\r\n}\r\nret = nouveau_bo_validate(nvbo, true, false);\r\nif (unlikely(ret)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(cli, "fail ttm_validate\n");\r\nreturn ret;\r\n}\r\nret = validate_sync(chan, nvbo);\r\nif (unlikely(ret)) {\r\nNV_ERROR(cli, "fail post-validate sync\n");\r\nreturn ret;\r\n}\r\nif (nv_device(drm->device)->card_type < NV_50) {\r\nif (nvbo->bo.offset == b->presumed.offset &&\r\n((nvbo->bo.mem.mem_type == TTM_PL_VRAM &&\r\nb->presumed.domain & NOUVEAU_GEM_DOMAIN_VRAM) ||\r\n(nvbo->bo.mem.mem_type == TTM_PL_TT &&\r\nb->presumed.domain & NOUVEAU_GEM_DOMAIN_GART)))\r\ncontinue;\r\nif (nvbo->bo.mem.mem_type == TTM_PL_TT)\r\nb->presumed.domain = NOUVEAU_GEM_DOMAIN_GART;\r\nelse\r\nb->presumed.domain = NOUVEAU_GEM_DOMAIN_VRAM;\r\nb->presumed.offset = nvbo->bo.offset;\r\nb->presumed.valid = 0;\r\nrelocs++;\r\nif (copy_to_user(&upbbo[nvbo->pbbo_index].presumed,\r\n&b->presumed, sizeof(b->presumed)))\r\nreturn -EFAULT;\r\n}\r\n}\r\nreturn relocs;\r\n}\r\nstatic int\r\nnouveau_gem_pushbuf_validate(struct nouveau_channel *chan,\r\nstruct drm_file *file_priv,\r\nstruct drm_nouveau_gem_pushbuf_bo *pbbo,\r\nuint64_t user_buffers, int nr_buffers,\r\nstruct validate_op *op, int *apply_relocs)\r\n{\r\nstruct nouveau_cli *cli = nouveau_cli(file_priv);\r\nint ret, relocs = 0;\r\nINIT_LIST_HEAD(&op->vram_list);\r\nINIT_LIST_HEAD(&op->gart_list);\r\nINIT_LIST_HEAD(&op->both_list);\r\nif (nr_buffers == 0)\r\nreturn 0;\r\nret = validate_init(chan, file_priv, pbbo, nr_buffers, op);\r\nif (unlikely(ret)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(cli, "validate_init\n");\r\nreturn ret;\r\n}\r\nret = validate_list(chan, cli, &op->vram_list, pbbo, user_buffers);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(cli, "validate vram_list\n");\r\nvalidate_fini(op, NULL);\r\nreturn ret;\r\n}\r\nrelocs += ret;\r\nret = validate_list(chan, cli, &op->gart_list, pbbo, user_buffers);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(cli, "validate gart_list\n");\r\nvalidate_fini(op, NULL);\r\nreturn ret;\r\n}\r\nrelocs += ret;\r\nret = validate_list(chan, cli, &op->both_list, pbbo, user_buffers);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(cli, "validate both_list\n");\r\nvalidate_fini(op, NULL);\r\nreturn ret;\r\n}\r\nrelocs += ret;\r\n*apply_relocs = relocs;\r\nreturn 0;\r\n}\r\nstatic inline void\r\nu_free(void *addr)\r\n{\r\nif (!is_vmalloc_addr(addr))\r\nkfree(addr);\r\nelse\r\nvfree(addr);\r\n}\r\nstatic inline void *\r\nu_memcpya(uint64_t user, unsigned nmemb, unsigned size)\r\n{\r\nvoid *mem;\r\nvoid __user *userptr = (void __force __user *)(uintptr_t)user;\r\nsize *= nmemb;\r\nmem = kmalloc(size, GFP_KERNEL | __GFP_NOWARN);\r\nif (!mem)\r\nmem = vmalloc(size);\r\nif (!mem)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (copy_from_user(mem, userptr, size)) {\r\nu_free(mem);\r\nreturn ERR_PTR(-EFAULT);\r\n}\r\nreturn mem;\r\n}\r\nstatic int\r\nnouveau_gem_pushbuf_reloc_apply(struct nouveau_cli *cli,\r\nstruct drm_nouveau_gem_pushbuf *req,\r\nstruct drm_nouveau_gem_pushbuf_bo *bo)\r\n{\r\nstruct drm_nouveau_gem_pushbuf_reloc *reloc = NULL;\r\nint ret = 0;\r\nunsigned i;\r\nreloc = u_memcpya(req->relocs, req->nr_relocs, sizeof(*reloc));\r\nif (IS_ERR(reloc))\r\nreturn PTR_ERR(reloc);\r\nfor (i = 0; i < req->nr_relocs; i++) {\r\nstruct drm_nouveau_gem_pushbuf_reloc *r = &reloc[i];\r\nstruct drm_nouveau_gem_pushbuf_bo *b;\r\nstruct nouveau_bo *nvbo;\r\nuint32_t data;\r\nif (unlikely(r->bo_index > req->nr_buffers)) {\r\nNV_ERROR(cli, "reloc bo index invalid\n");\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nb = &bo[r->bo_index];\r\nif (b->presumed.valid)\r\ncontinue;\r\nif (unlikely(r->reloc_bo_index > req->nr_buffers)) {\r\nNV_ERROR(cli, "reloc container bo index invalid\n");\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nnvbo = (void *)(unsigned long)bo[r->reloc_bo_index].user_priv;\r\nif (unlikely(r->reloc_bo_offset + 4 >\r\nnvbo->bo.mem.num_pages << PAGE_SHIFT)) {\r\nNV_ERROR(cli, "reloc outside of bo\n");\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nif (!nvbo->kmap.virtual) {\r\nret = ttm_bo_kmap(&nvbo->bo, 0, nvbo->bo.mem.num_pages,\r\n&nvbo->kmap);\r\nif (ret) {\r\nNV_ERROR(cli, "failed kmap for reloc\n");\r\nbreak;\r\n}\r\nnvbo->validate_mapped = true;\r\n}\r\nif (r->flags & NOUVEAU_GEM_RELOC_LOW)\r\ndata = b->presumed.offset + r->data;\r\nelse\r\nif (r->flags & NOUVEAU_GEM_RELOC_HIGH)\r\ndata = (b->presumed.offset + r->data) >> 32;\r\nelse\r\ndata = r->data;\r\nif (r->flags & NOUVEAU_GEM_RELOC_OR) {\r\nif (b->presumed.domain == NOUVEAU_GEM_DOMAIN_GART)\r\ndata |= r->tor;\r\nelse\r\ndata |= r->vor;\r\n}\r\nspin_lock(&nvbo->bo.bdev->fence_lock);\r\nret = ttm_bo_wait(&nvbo->bo, false, false, false);\r\nspin_unlock(&nvbo->bo.bdev->fence_lock);\r\nif (ret) {\r\nNV_ERROR(cli, "reloc wait_idle failed: %d\n", ret);\r\nbreak;\r\n}\r\nnouveau_bo_wr32(nvbo, r->reloc_bo_offset >> 2, data);\r\n}\r\nu_free(reloc);\r\nreturn ret;\r\n}\r\nint\r\nnouveau_gem_ioctl_pushbuf(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct nouveau_abi16 *abi16 = nouveau_abi16_get(file_priv, dev);\r\nstruct nouveau_cli *cli = nouveau_cli(file_priv);\r\nstruct nouveau_abi16_chan *temp;\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nstruct drm_nouveau_gem_pushbuf *req = data;\r\nstruct drm_nouveau_gem_pushbuf_push *push;\r\nstruct drm_nouveau_gem_pushbuf_bo *bo;\r\nstruct nouveau_channel *chan = NULL;\r\nstruct validate_op op;\r\nstruct nouveau_fence *fence = NULL;\r\nint i, j, ret = 0, do_reloc = 0;\r\nif (unlikely(!abi16))\r\nreturn -ENOMEM;\r\nlist_for_each_entry(temp, &abi16->channels, head) {\r\nif (temp->chan->handle == (NVDRM_CHAN | req->channel)) {\r\nchan = temp->chan;\r\nbreak;\r\n}\r\n}\r\nif (!chan)\r\nreturn nouveau_abi16_put(abi16, -ENOENT);\r\nreq->vram_available = drm->gem.vram_available;\r\nreq->gart_available = drm->gem.gart_available;\r\nif (unlikely(req->nr_push == 0))\r\ngoto out_next;\r\nif (unlikely(req->nr_push > NOUVEAU_GEM_MAX_PUSH)) {\r\nNV_ERROR(cli, "pushbuf push count exceeds limit: %d max %d\n",\r\nreq->nr_push, NOUVEAU_GEM_MAX_PUSH);\r\nreturn nouveau_abi16_put(abi16, -EINVAL);\r\n}\r\nif (unlikely(req->nr_buffers > NOUVEAU_GEM_MAX_BUFFERS)) {\r\nNV_ERROR(cli, "pushbuf bo count exceeds limit: %d max %d\n",\r\nreq->nr_buffers, NOUVEAU_GEM_MAX_BUFFERS);\r\nreturn nouveau_abi16_put(abi16, -EINVAL);\r\n}\r\nif (unlikely(req->nr_relocs > NOUVEAU_GEM_MAX_RELOCS)) {\r\nNV_ERROR(cli, "pushbuf reloc count exceeds limit: %d max %d\n",\r\nreq->nr_relocs, NOUVEAU_GEM_MAX_RELOCS);\r\nreturn nouveau_abi16_put(abi16, -EINVAL);\r\n}\r\npush = u_memcpya(req->push, req->nr_push, sizeof(*push));\r\nif (IS_ERR(push))\r\nreturn nouveau_abi16_put(abi16, PTR_ERR(push));\r\nbo = u_memcpya(req->buffers, req->nr_buffers, sizeof(*bo));\r\nif (IS_ERR(bo)) {\r\nu_free(push);\r\nreturn nouveau_abi16_put(abi16, PTR_ERR(bo));\r\n}\r\nfor (i = 0; i < req->nr_push; i++) {\r\nif (push[i].bo_index >= req->nr_buffers) {\r\nNV_ERROR(cli, "push %d buffer not in list\n", i);\r\nret = -EINVAL;\r\ngoto out_prevalid;\r\n}\r\n}\r\nret = nouveau_gem_pushbuf_validate(chan, file_priv, bo, req->buffers,\r\nreq->nr_buffers, &op, &do_reloc);\r\nif (ret) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(cli, "validate: %d\n", ret);\r\ngoto out_prevalid;\r\n}\r\nif (do_reloc) {\r\nret = nouveau_gem_pushbuf_reloc_apply(cli, req, bo);\r\nif (ret) {\r\nNV_ERROR(cli, "reloc apply: %d\n", ret);\r\ngoto out;\r\n}\r\n}\r\nif (chan->dma.ib_max) {\r\nret = nouveau_dma_wait(chan, req->nr_push + 1, 16);\r\nif (ret) {\r\nNV_ERROR(cli, "nv50cal_space: %d\n", ret);\r\ngoto out;\r\n}\r\nfor (i = 0; i < req->nr_push; i++) {\r\nstruct nouveau_bo *nvbo = (void *)(unsigned long)\r\nbo[push[i].bo_index].user_priv;\r\nnv50_dma_push(chan, nvbo, push[i].offset,\r\npush[i].length);\r\n}\r\n} else\r\nif (nv_device(drm->device)->chipset >= 0x25) {\r\nret = RING_SPACE(chan, req->nr_push * 2);\r\nif (ret) {\r\nNV_ERROR(cli, "cal_space: %d\n", ret);\r\ngoto out;\r\n}\r\nfor (i = 0; i < req->nr_push; i++) {\r\nstruct nouveau_bo *nvbo = (void *)(unsigned long)\r\nbo[push[i].bo_index].user_priv;\r\nOUT_RING(chan, (nvbo->bo.offset + push[i].offset) | 2);\r\nOUT_RING(chan, 0);\r\n}\r\n} else {\r\nret = RING_SPACE(chan, req->nr_push * (2 + NOUVEAU_DMA_SKIPS));\r\nif (ret) {\r\nNV_ERROR(cli, "jmp_space: %d\n", ret);\r\ngoto out;\r\n}\r\nfor (i = 0; i < req->nr_push; i++) {\r\nstruct nouveau_bo *nvbo = (void *)(unsigned long)\r\nbo[push[i].bo_index].user_priv;\r\nuint32_t cmd;\r\ncmd = chan->push.vma.offset + ((chan->dma.cur + 2) << 2);\r\ncmd |= 0x20000000;\r\nif (unlikely(cmd != req->suffix0)) {\r\nif (!nvbo->kmap.virtual) {\r\nret = ttm_bo_kmap(&nvbo->bo, 0,\r\nnvbo->bo.mem.\r\nnum_pages,\r\n&nvbo->kmap);\r\nif (ret) {\r\nWIND_RING(chan);\r\ngoto out;\r\n}\r\nnvbo->validate_mapped = true;\r\n}\r\nnouveau_bo_wr32(nvbo, (push[i].offset +\r\npush[i].length - 8) / 4, cmd);\r\n}\r\nOUT_RING(chan, 0x20000000 |\r\n(nvbo->bo.offset + push[i].offset));\r\nOUT_RING(chan, 0);\r\nfor (j = 0; j < NOUVEAU_DMA_SKIPS; j++)\r\nOUT_RING(chan, 0);\r\n}\r\n}\r\nret = nouveau_fence_new(chan, false, &fence);\r\nif (ret) {\r\nNV_ERROR(cli, "error fencing pushbuf: %d\n", ret);\r\nWIND_RING(chan);\r\ngoto out;\r\n}\r\nout:\r\nvalidate_fini(&op, fence);\r\nnouveau_fence_unref(&fence);\r\nout_prevalid:\r\nu_free(bo);\r\nu_free(push);\r\nout_next:\r\nif (chan->dma.ib_max) {\r\nreq->suffix0 = 0x00000000;\r\nreq->suffix1 = 0x00000000;\r\n} else\r\nif (nv_device(drm->device)->chipset >= 0x25) {\r\nreq->suffix0 = 0x00020000;\r\nreq->suffix1 = 0x00000000;\r\n} else {\r\nreq->suffix0 = 0x20000000 |\r\n(chan->push.vma.offset + ((chan->dma.cur + 2) << 2));\r\nreq->suffix1 = 0x00000000;\r\n}\r\nreturn nouveau_abi16_put(abi16, ret);\r\n}\r\nstatic inline uint32_t\r\ndomain_to_ttm(struct nouveau_bo *nvbo, uint32_t domain)\r\n{\r\nuint32_t flags = 0;\r\nif (domain & NOUVEAU_GEM_DOMAIN_VRAM)\r\nflags |= TTM_PL_FLAG_VRAM;\r\nif (domain & NOUVEAU_GEM_DOMAIN_GART)\r\nflags |= TTM_PL_FLAG_TT;\r\nreturn flags;\r\n}\r\nint\r\nnouveau_gem_ioctl_cpu_prep(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_nouveau_gem_cpu_prep *req = data;\r\nstruct drm_gem_object *gem;\r\nstruct nouveau_bo *nvbo;\r\nbool no_wait = !!(req->flags & NOUVEAU_GEM_CPU_PREP_NOWAIT);\r\nint ret = -EINVAL;\r\ngem = drm_gem_object_lookup(dev, file_priv, req->handle);\r\nif (!gem)\r\nreturn -ENOENT;\r\nnvbo = nouveau_gem_object(gem);\r\nspin_lock(&nvbo->bo.bdev->fence_lock);\r\nret = ttm_bo_wait(&nvbo->bo, true, true, no_wait);\r\nspin_unlock(&nvbo->bo.bdev->fence_lock);\r\ndrm_gem_object_unreference_unlocked(gem);\r\nreturn ret;\r\n}\r\nint\r\nnouveau_gem_ioctl_cpu_fini(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nreturn 0;\r\n}\r\nint\r\nnouveau_gem_ioctl_info(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_nouveau_gem_info *req = data;\r\nstruct drm_gem_object *gem;\r\nint ret;\r\ngem = drm_gem_object_lookup(dev, file_priv, req->handle);\r\nif (!gem)\r\nreturn -ENOENT;\r\nret = nouveau_gem_info(file_priv, gem, req);\r\ndrm_gem_object_unreference_unlocked(gem);\r\nreturn ret;\r\n}
