static void sort_key_next(struct btree_iter *iter,\r\nstruct btree_iter_set *i)\r\n{\r\ni->k = bkey_next(i->k);\r\nif (i->k == i->end)\r\n*i = iter->data[--iter->used];\r\n}\r\nstatic bool bch_key_sort_cmp(struct btree_iter_set l,\r\nstruct btree_iter_set r)\r\n{\r\nint64_t c = bkey_cmp(l.k, r.k);\r\nreturn c ? c > 0 : l.k < r.k;\r\n}\r\nstatic bool __ptr_invalid(struct cache_set *c, const struct bkey *k)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < KEY_PTRS(k); i++)\r\nif (ptr_available(c, k, i)) {\r\nstruct cache *ca = PTR_CACHE(c, k, i);\r\nsize_t bucket = PTR_BUCKET_NR(c, k, i);\r\nsize_t r = bucket_remainder(c, PTR_OFFSET(k, i));\r\nif (KEY_SIZE(k) + r > c->sb.bucket_size ||\r\nbucket < ca->sb.first_bucket ||\r\nbucket >= ca->sb.nbuckets)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic const char *bch_ptr_status(struct cache_set *c, const struct bkey *k)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < KEY_PTRS(k); i++)\r\nif (ptr_available(c, k, i)) {\r\nstruct cache *ca = PTR_CACHE(c, k, i);\r\nsize_t bucket = PTR_BUCKET_NR(c, k, i);\r\nsize_t r = bucket_remainder(c, PTR_OFFSET(k, i));\r\nif (KEY_SIZE(k) + r > c->sb.bucket_size)\r\nreturn "bad, length too big";\r\nif (bucket < ca->sb.first_bucket)\r\nreturn "bad, short offset";\r\nif (bucket >= ca->sb.nbuckets)\r\nreturn "bad, offset past end of device";\r\nif (ptr_stale(c, k, i))\r\nreturn "stale";\r\n}\r\nif (!bkey_cmp(k, &ZERO_KEY))\r\nreturn "bad, null key";\r\nif (!KEY_PTRS(k))\r\nreturn "bad, no pointers";\r\nif (!KEY_SIZE(k))\r\nreturn "zeroed key";\r\nreturn "";\r\n}\r\nvoid bch_extent_to_text(char *buf, size_t size, const struct bkey *k)\r\n{\r\nunsigned i = 0;\r\nchar *out = buf, *end = buf + size;\r\n#define p(...) (out += scnprintf(out, end - out, __VA_ARGS__))\r\np("%llu:%llu len %llu -> [", KEY_INODE(k), KEY_START(k), KEY_SIZE(k));\r\nfor (i = 0; i < KEY_PTRS(k); i++) {\r\nif (i)\r\np(", ");\r\nif (PTR_DEV(k, i) == PTR_CHECK_DEV)\r\np("check dev");\r\nelse\r\np("%llu:%llu gen %llu", PTR_DEV(k, i),\r\nPTR_OFFSET(k, i), PTR_GEN(k, i));\r\n}\r\np("]");\r\nif (KEY_DIRTY(k))\r\np(" dirty");\r\nif (KEY_CSUM(k))\r\np(" cs%llu %llx", KEY_CSUM(k), k->ptr[1]);\r\n#undef p\r\n}\r\nstatic void bch_bkey_dump(struct btree_keys *keys, const struct bkey *k)\r\n{\r\nstruct btree *b = container_of(keys, struct btree, keys);\r\nunsigned j;\r\nchar buf[80];\r\nbch_extent_to_text(buf, sizeof(buf), k);\r\nprintk(" %s", buf);\r\nfor (j = 0; j < KEY_PTRS(k); j++) {\r\nsize_t n = PTR_BUCKET_NR(b->c, k, j);\r\nprintk(" bucket %zu", n);\r\nif (n >= b->c->sb.first_bucket && n < b->c->sb.nbuckets)\r\nprintk(" prio %i",\r\nPTR_BUCKET(b->c, k, j)->prio);\r\n}\r\nprintk(" %s\n", bch_ptr_status(b->c, k));\r\n}\r\nbool __bch_btree_ptr_invalid(struct cache_set *c, const struct bkey *k)\r\n{\r\nchar buf[80];\r\nif (!KEY_PTRS(k) || !KEY_SIZE(k) || KEY_DIRTY(k))\r\ngoto bad;\r\nif (__ptr_invalid(c, k))\r\ngoto bad;\r\nreturn false;\r\nbad:\r\nbch_extent_to_text(buf, sizeof(buf), k);\r\ncache_bug(c, "spotted btree ptr %s: %s", buf, bch_ptr_status(c, k));\r\nreturn true;\r\n}\r\nstatic bool bch_btree_ptr_invalid(struct btree_keys *bk, const struct bkey *k)\r\n{\r\nstruct btree *b = container_of(bk, struct btree, keys);\r\nreturn __bch_btree_ptr_invalid(b->c, k);\r\n}\r\nstatic bool btree_ptr_bad_expensive(struct btree *b, const struct bkey *k)\r\n{\r\nunsigned i;\r\nchar buf[80];\r\nstruct bucket *g;\r\nif (mutex_trylock(&b->c->bucket_lock)) {\r\nfor (i = 0; i < KEY_PTRS(k); i++)\r\nif (ptr_available(b->c, k, i)) {\r\ng = PTR_BUCKET(b->c, k, i);\r\nif (KEY_DIRTY(k) ||\r\ng->prio != BTREE_PRIO ||\r\n(b->c->gc_mark_valid &&\r\nGC_MARK(g) != GC_MARK_METADATA))\r\ngoto err;\r\n}\r\nmutex_unlock(&b->c->bucket_lock);\r\n}\r\nreturn false;\r\nerr:\r\nmutex_unlock(&b->c->bucket_lock);\r\nbch_extent_to_text(buf, sizeof(buf), k);\r\nbtree_bug(b,\r\n"inconsistent btree pointer %s: bucket %zi pin %i prio %i gen %i last_gc %i mark %llu",\r\nbuf, PTR_BUCKET_NR(b->c, k, i), atomic_read(&g->pin),\r\ng->prio, g->gen, g->last_gc, GC_MARK(g));\r\nreturn true;\r\n}\r\nstatic bool bch_btree_ptr_bad(struct btree_keys *bk, const struct bkey *k)\r\n{\r\nstruct btree *b = container_of(bk, struct btree, keys);\r\nunsigned i;\r\nif (!bkey_cmp(k, &ZERO_KEY) ||\r\n!KEY_PTRS(k) ||\r\nbch_ptr_invalid(bk, k))\r\nreturn true;\r\nfor (i = 0; i < KEY_PTRS(k); i++)\r\nif (!ptr_available(b->c, k, i) ||\r\nptr_stale(b->c, k, i))\r\nreturn true;\r\nif (expensive_debug_checks(b->c) &&\r\nbtree_ptr_bad_expensive(b, k))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool bch_btree_ptr_insert_fixup(struct btree_keys *bk,\r\nstruct bkey *insert,\r\nstruct btree_iter *iter,\r\nstruct bkey *replace_key)\r\n{\r\nstruct btree *b = container_of(bk, struct btree, keys);\r\nif (!KEY_OFFSET(insert))\r\nbtree_current_write(b)->prio_blocked++;\r\nreturn false;\r\n}\r\nstatic bool bch_extent_sort_cmp(struct btree_iter_set l,\r\nstruct btree_iter_set r)\r\n{\r\nint64_t c = bkey_cmp(&START_KEY(l.k), &START_KEY(r.k));\r\nreturn c ? c > 0 : l.k < r.k;\r\n}\r\nstatic struct bkey *bch_extent_sort_fixup(struct btree_iter *iter,\r\nstruct bkey *tmp)\r\n{\r\nwhile (iter->used > 1) {\r\nstruct btree_iter_set *top = iter->data, *i = top + 1;\r\nif (iter->used > 2 &&\r\nbch_extent_sort_cmp(i[0], i[1]))\r\ni++;\r\nif (bkey_cmp(top->k, &START_KEY(i->k)) <= 0)\r\nbreak;\r\nif (!KEY_SIZE(i->k)) {\r\nsort_key_next(iter, i);\r\nheap_sift(iter, i - top, bch_extent_sort_cmp);\r\ncontinue;\r\n}\r\nif (top->k > i->k) {\r\nif (bkey_cmp(top->k, i->k) >= 0)\r\nsort_key_next(iter, i);\r\nelse\r\nbch_cut_front(top->k, i->k);\r\nheap_sift(iter, i - top, bch_extent_sort_cmp);\r\n} else {\r\nBUG_ON(!bkey_cmp(&START_KEY(top->k), &START_KEY(i->k)));\r\nif (bkey_cmp(i->k, top->k) < 0) {\r\nbkey_copy(tmp, top->k);\r\nbch_cut_back(&START_KEY(i->k), tmp);\r\nbch_cut_front(i->k, top->k);\r\nheap_sift(iter, 0, bch_extent_sort_cmp);\r\nreturn tmp;\r\n} else {\r\nbch_cut_back(&START_KEY(i->k), top->k);\r\n}\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic void bch_subtract_dirty(struct bkey *k,\r\nstruct cache_set *c,\r\nuint64_t offset,\r\nint sectors)\r\n{\r\nif (KEY_DIRTY(k))\r\nbcache_dev_sectors_dirty_add(c, KEY_INODE(k),\r\noffset, -sectors);\r\n}\r\nstatic bool bch_extent_insert_fixup(struct btree_keys *b,\r\nstruct bkey *insert,\r\nstruct btree_iter *iter,\r\nstruct bkey *replace_key)\r\n{\r\nstruct cache_set *c = container_of(b, struct btree, keys)->c;\r\nuint64_t old_offset;\r\nunsigned old_size, sectors_found = 0;\r\nBUG_ON(!KEY_OFFSET(insert));\r\nBUG_ON(!KEY_SIZE(insert));\r\nwhile (1) {\r\nstruct bkey *k = bch_btree_iter_next(iter);\r\nif (!k)\r\nbreak;\r\nif (bkey_cmp(&START_KEY(k), insert) >= 0) {\r\nif (KEY_SIZE(k))\r\nbreak;\r\nelse\r\ncontinue;\r\n}\r\nif (bkey_cmp(k, &START_KEY(insert)) <= 0)\r\ncontinue;\r\nold_offset = KEY_START(k);\r\nold_size = KEY_SIZE(k);\r\nif (replace_key && KEY_SIZE(k)) {\r\nunsigned i;\r\nuint64_t offset = KEY_START(k) -\r\nKEY_START(replace_key);\r\nif (KEY_START(k) < KEY_START(replace_key) ||\r\nKEY_OFFSET(k) > KEY_OFFSET(replace_key))\r\ngoto check_failed;\r\nif (KEY_START(k) > KEY_START(insert) + sectors_found)\r\ngoto check_failed;\r\nif (!bch_bkey_equal_header(k, replace_key))\r\ngoto check_failed;\r\noffset <<= 8;\r\nBUG_ON(!KEY_PTRS(replace_key));\r\nfor (i = 0; i < KEY_PTRS(replace_key); i++)\r\nif (k->ptr[i] != replace_key->ptr[i] + offset)\r\ngoto check_failed;\r\nsectors_found = KEY_OFFSET(k) - KEY_START(insert);\r\n}\r\nif (bkey_cmp(insert, k) < 0 &&\r\nbkey_cmp(&START_KEY(insert), &START_KEY(k)) > 0) {\r\nstruct bkey *top;\r\nbch_subtract_dirty(k, c, KEY_START(insert),\r\nKEY_SIZE(insert));\r\nif (bkey_written(b, k)) {\r\ntop = bch_bset_search(b, bset_tree_last(b),\r\ninsert);\r\nbch_bset_insert(b, top, k);\r\n} else {\r\nBKEY_PADDED(key) temp;\r\nbkey_copy(&temp.key, k);\r\nbch_bset_insert(b, k, &temp.key);\r\ntop = bkey_next(k);\r\n}\r\nbch_cut_front(insert, top);\r\nbch_cut_back(&START_KEY(insert), k);\r\nbch_bset_fix_invalidated_key(b, k);\r\ngoto out;\r\n}\r\nif (bkey_cmp(insert, k) < 0) {\r\nbch_cut_front(insert, k);\r\n} else {\r\nif (bkey_cmp(&START_KEY(insert), &START_KEY(k)) > 0)\r\nold_offset = KEY_START(insert);\r\nif (bkey_written(b, k) &&\r\nbkey_cmp(&START_KEY(insert), &START_KEY(k)) <= 0) {\r\nbch_cut_front(k, k);\r\n} else {\r\n__bch_cut_back(&START_KEY(insert), k);\r\nbch_bset_fix_invalidated_key(b, k);\r\n}\r\n}\r\nbch_subtract_dirty(k, c, old_offset, old_size - KEY_SIZE(k));\r\n}\r\ncheck_failed:\r\nif (replace_key) {\r\nif (!sectors_found) {\r\nreturn true;\r\n} else if (sectors_found < KEY_SIZE(insert)) {\r\nSET_KEY_OFFSET(insert, KEY_OFFSET(insert) -\r\n(KEY_SIZE(insert) - sectors_found));\r\nSET_KEY_SIZE(insert, sectors_found);\r\n}\r\n}\r\nout:\r\nif (KEY_DIRTY(insert))\r\nbcache_dev_sectors_dirty_add(c, KEY_INODE(insert),\r\nKEY_START(insert),\r\nKEY_SIZE(insert));\r\nreturn false;\r\n}\r\nstatic bool bch_extent_invalid(struct btree_keys *bk, const struct bkey *k)\r\n{\r\nstruct btree *b = container_of(bk, struct btree, keys);\r\nchar buf[80];\r\nif (!KEY_SIZE(k))\r\nreturn true;\r\nif (KEY_SIZE(k) > KEY_OFFSET(k))\r\ngoto bad;\r\nif (__ptr_invalid(b->c, k))\r\ngoto bad;\r\nreturn false;\r\nbad:\r\nbch_extent_to_text(buf, sizeof(buf), k);\r\ncache_bug(b->c, "spotted extent %s: %s", buf, bch_ptr_status(b->c, k));\r\nreturn true;\r\n}\r\nstatic bool bch_extent_bad_expensive(struct btree *b, const struct bkey *k,\r\nunsigned ptr)\r\n{\r\nstruct bucket *g = PTR_BUCKET(b->c, k, ptr);\r\nchar buf[80];\r\nif (mutex_trylock(&b->c->bucket_lock)) {\r\nif (b->c->gc_mark_valid &&\r\n(!GC_MARK(g) ||\r\nGC_MARK(g) == GC_MARK_METADATA ||\r\n(GC_MARK(g) != GC_MARK_DIRTY && KEY_DIRTY(k))))\r\ngoto err;\r\nif (g->prio == BTREE_PRIO)\r\ngoto err;\r\nmutex_unlock(&b->c->bucket_lock);\r\n}\r\nreturn false;\r\nerr:\r\nmutex_unlock(&b->c->bucket_lock);\r\nbch_extent_to_text(buf, sizeof(buf), k);\r\nbtree_bug(b,\r\n"inconsistent extent pointer %s:\nbucket %zu pin %i prio %i gen %i last_gc %i mark %llu",\r\nbuf, PTR_BUCKET_NR(b->c, k, ptr), atomic_read(&g->pin),\r\ng->prio, g->gen, g->last_gc, GC_MARK(g));\r\nreturn true;\r\n}\r\nstatic bool bch_extent_bad(struct btree_keys *bk, const struct bkey *k)\r\n{\r\nstruct btree *b = container_of(bk, struct btree, keys);\r\nstruct bucket *g;\r\nunsigned i, stale;\r\nif (!KEY_PTRS(k) ||\r\nbch_extent_invalid(bk, k))\r\nreturn true;\r\nfor (i = 0; i < KEY_PTRS(k); i++)\r\nif (!ptr_available(b->c, k, i))\r\nreturn true;\r\nif (!expensive_debug_checks(b->c) && KEY_DIRTY(k))\r\nreturn false;\r\nfor (i = 0; i < KEY_PTRS(k); i++) {\r\ng = PTR_BUCKET(b->c, k, i);\r\nstale = ptr_stale(b->c, k, i);\r\nbtree_bug_on(stale > 96, b,\r\n"key too stale: %i, need_gc %u",\r\nstale, b->c->need_gc);\r\nbtree_bug_on(stale && KEY_DIRTY(k) && KEY_SIZE(k),\r\nb, "stale dirty pointer");\r\nif (stale)\r\nreturn true;\r\nif (expensive_debug_checks(b->c) &&\r\nbch_extent_bad_expensive(b, k, i))\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic uint64_t merge_chksums(struct bkey *l, struct bkey *r)\r\n{\r\nreturn (l->ptr[KEY_PTRS(l)] + r->ptr[KEY_PTRS(r)]) &\r\n~((uint64_t)1 << 63);\r\n}\r\nstatic bool bch_extent_merge(struct btree_keys *bk, struct bkey *l, struct bkey *r)\r\n{\r\nstruct btree *b = container_of(bk, struct btree, keys);\r\nunsigned i;\r\nif (key_merging_disabled(b->c))\r\nreturn false;\r\nfor (i = 0; i < KEY_PTRS(l); i++)\r\nif (l->ptr[i] + PTR(0, KEY_SIZE(l), 0) != r->ptr[i] ||\r\nPTR_BUCKET_NR(b->c, l, i) != PTR_BUCKET_NR(b->c, r, i))\r\nreturn false;\r\nif (KEY_SIZE(l) + KEY_SIZE(r) > USHRT_MAX) {\r\nSET_KEY_OFFSET(l, KEY_OFFSET(l) + USHRT_MAX - KEY_SIZE(l));\r\nSET_KEY_SIZE(l, USHRT_MAX);\r\nbch_cut_front(l, r);\r\nreturn false;\r\n}\r\nif (KEY_CSUM(l)) {\r\nif (KEY_CSUM(r))\r\nl->ptr[KEY_PTRS(l)] = merge_chksums(l, r);\r\nelse\r\nSET_KEY_CSUM(l, 0);\r\n}\r\nSET_KEY_OFFSET(l, KEY_OFFSET(l) + KEY_SIZE(r));\r\nSET_KEY_SIZE(l, KEY_SIZE(l) + KEY_SIZE(r));\r\nreturn true;\r\n}
