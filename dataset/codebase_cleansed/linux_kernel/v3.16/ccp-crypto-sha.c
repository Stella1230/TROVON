static int ccp_sha_complete(struct crypto_async_request *async_req, int ret)\r\n{\r\nstruct ahash_request *req = ahash_request_cast(async_req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct ccp_sha_req_ctx *rctx = ahash_request_ctx(req);\r\nunsigned int digest_size = crypto_ahash_digestsize(tfm);\r\nif (ret)\r\ngoto e_free;\r\nif (rctx->hash_rem) {\r\nunsigned int offset = rctx->nbytes - rctx->hash_rem;\r\nscatterwalk_map_and_copy(rctx->buf, rctx->src,\r\noffset, rctx->hash_rem, 0);\r\nrctx->buf_count = rctx->hash_rem;\r\n} else\r\nrctx->buf_count = 0;\r\nif (req->result)\r\nmemcpy(req->result, rctx->ctx, digest_size);\r\ne_free:\r\nsg_free_table(&rctx->data_sg);\r\nreturn ret;\r\n}\r\nstatic int ccp_do_sha_update(struct ahash_request *req, unsigned int nbytes,\r\nunsigned int final)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct ccp_ctx *ctx = crypto_ahash_ctx(tfm);\r\nstruct ccp_sha_req_ctx *rctx = ahash_request_ctx(req);\r\nstruct scatterlist *sg;\r\nunsigned int block_size =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nunsigned int sg_count;\r\ngfp_t gfp;\r\nu64 len;\r\nint ret;\r\nlen = (u64)rctx->buf_count + (u64)nbytes;\r\nif (!final && (len <= block_size)) {\r\nscatterwalk_map_and_copy(rctx->buf + rctx->buf_count, req->src,\r\n0, nbytes, 0);\r\nrctx->buf_count += nbytes;\r\nreturn 0;\r\n}\r\nrctx->src = req->src;\r\nrctx->nbytes = nbytes;\r\nrctx->final = final;\r\nrctx->hash_rem = final ? 0 : len & (block_size - 1);\r\nrctx->hash_cnt = len - rctx->hash_rem;\r\nif (!final && !rctx->hash_rem) {\r\nrctx->hash_cnt -= block_size;\r\nrctx->hash_rem = block_size;\r\n}\r\nsg_init_one(&rctx->ctx_sg, rctx->ctx, sizeof(rctx->ctx));\r\nsg = NULL;\r\nif (rctx->buf_count && nbytes) {\r\ngfp = req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ?\r\nGFP_KERNEL : GFP_ATOMIC;\r\nsg_count = sg_nents(req->src) + 1;\r\nret = sg_alloc_table(&rctx->data_sg, sg_count, gfp);\r\nif (ret)\r\nreturn ret;\r\nsg_init_one(&rctx->buf_sg, rctx->buf, rctx->buf_count);\r\nsg = ccp_crypto_sg_table_add(&rctx->data_sg, &rctx->buf_sg);\r\nsg = ccp_crypto_sg_table_add(&rctx->data_sg, req->src);\r\nsg_mark_end(sg);\r\nsg = rctx->data_sg.sgl;\r\n} else if (rctx->buf_count) {\r\nsg_init_one(&rctx->buf_sg, rctx->buf, rctx->buf_count);\r\nsg = &rctx->buf_sg;\r\n} else if (nbytes) {\r\nsg = req->src;\r\n}\r\nrctx->msg_bits += (rctx->hash_cnt << 3);\r\nmemset(&rctx->cmd, 0, sizeof(rctx->cmd));\r\nINIT_LIST_HEAD(&rctx->cmd.entry);\r\nrctx->cmd.engine = CCP_ENGINE_SHA;\r\nrctx->cmd.u.sha.type = rctx->type;\r\nrctx->cmd.u.sha.ctx = &rctx->ctx_sg;\r\nrctx->cmd.u.sha.ctx_len = sizeof(rctx->ctx);\r\nrctx->cmd.u.sha.src = sg;\r\nrctx->cmd.u.sha.src_len = rctx->hash_cnt;\r\nrctx->cmd.u.sha.opad = ctx->u.sha.key_len ?\r\n&ctx->u.sha.opad_sg : NULL;\r\nrctx->cmd.u.sha.opad_len = ctx->u.sha.key_len ?\r\nctx->u.sha.opad_count : 0;\r\nrctx->cmd.u.sha.first = rctx->first;\r\nrctx->cmd.u.sha.final = rctx->final;\r\nrctx->cmd.u.sha.msg_bits = rctx->msg_bits;\r\nrctx->first = 0;\r\nret = ccp_crypto_enqueue_request(&req->base, &rctx->cmd);\r\nreturn ret;\r\n}\r\nstatic int ccp_sha_init(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct ccp_ctx *ctx = crypto_ahash_ctx(tfm);\r\nstruct ccp_sha_req_ctx *rctx = ahash_request_ctx(req);\r\nstruct ccp_crypto_ahash_alg *alg =\r\nccp_crypto_ahash_alg(crypto_ahash_tfm(tfm));\r\nunsigned int block_size =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nmemset(rctx, 0, sizeof(*rctx));\r\nrctx->type = alg->type;\r\nrctx->first = 1;\r\nif (ctx->u.sha.key_len) {\r\nmemcpy(rctx->buf, ctx->u.sha.ipad, block_size);\r\nrctx->buf_count = block_size;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ccp_sha_update(struct ahash_request *req)\r\n{\r\nreturn ccp_do_sha_update(req, req->nbytes, 0);\r\n}\r\nstatic int ccp_sha_final(struct ahash_request *req)\r\n{\r\nreturn ccp_do_sha_update(req, 0, 1);\r\n}\r\nstatic int ccp_sha_finup(struct ahash_request *req)\r\n{\r\nreturn ccp_do_sha_update(req, req->nbytes, 1);\r\n}\r\nstatic int ccp_sha_digest(struct ahash_request *req)\r\n{\r\nint ret;\r\nret = ccp_sha_init(req);\r\nif (ret)\r\nreturn ret;\r\nreturn ccp_sha_finup(req);\r\n}\r\nstatic int ccp_sha_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int key_len)\r\n{\r\nstruct ccp_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));\r\nstruct crypto_shash *shash = ctx->u.sha.hmac_tfm;\r\nstruct {\r\nstruct shash_desc sdesc;\r\nchar ctx[crypto_shash_descsize(shash)];\r\n} desc;\r\nunsigned int block_size = crypto_shash_blocksize(shash);\r\nunsigned int digest_size = crypto_shash_digestsize(shash);\r\nint i, ret;\r\nctx->u.sha.key_len = 0;\r\nmemset(ctx->u.sha.key, 0, sizeof(ctx->u.sha.key));\r\nif (key_len > block_size) {\r\ndesc.sdesc.tfm = shash;\r\ndesc.sdesc.flags = crypto_ahash_get_flags(tfm) &\r\nCRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = crypto_shash_digest(&desc.sdesc, key, key_len,\r\nctx->u.sha.key);\r\nif (ret) {\r\ncrypto_ahash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nkey_len = digest_size;\r\n} else\r\nmemcpy(ctx->u.sha.key, key, key_len);\r\nfor (i = 0; i < block_size; i++) {\r\nctx->u.sha.ipad[i] = ctx->u.sha.key[i] ^ 0x36;\r\nctx->u.sha.opad[i] = ctx->u.sha.key[i] ^ 0x5c;\r\n}\r\nsg_init_one(&ctx->u.sha.opad_sg, ctx->u.sha.opad, block_size);\r\nctx->u.sha.opad_count = block_size;\r\nctx->u.sha.key_len = key_len;\r\nreturn 0;\r\n}\r\nstatic int ccp_sha_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct ccp_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\r\nctx->complete = ccp_sha_complete;\r\nctx->u.sha.key_len = 0;\r\ncrypto_ahash_set_reqsize(ahash, sizeof(struct ccp_sha_req_ctx));\r\nreturn 0;\r\n}\r\nstatic void ccp_sha_cra_exit(struct crypto_tfm *tfm)\r\n{\r\n}\r\nstatic int ccp_hmac_sha_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct ccp_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct ccp_crypto_ahash_alg *alg = ccp_crypto_ahash_alg(tfm);\r\nstruct crypto_shash *hmac_tfm;\r\nhmac_tfm = crypto_alloc_shash(alg->child_alg, 0, 0);\r\nif (IS_ERR(hmac_tfm)) {\r\npr_warn("could not load driver %s need for HMAC support\n",\r\nalg->child_alg);\r\nreturn PTR_ERR(hmac_tfm);\r\n}\r\nctx->u.sha.hmac_tfm = hmac_tfm;\r\nreturn ccp_sha_cra_init(tfm);\r\n}\r\nstatic void ccp_hmac_sha_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct ccp_ctx *ctx = crypto_tfm_ctx(tfm);\r\nif (ctx->u.sha.hmac_tfm)\r\ncrypto_free_shash(ctx->u.sha.hmac_tfm);\r\nccp_sha_cra_exit(tfm);\r\n}\r\nstatic int ccp_register_hmac_alg(struct list_head *head,\r\nconst struct ccp_sha_def *def,\r\nconst struct ccp_crypto_ahash_alg *base_alg)\r\n{\r\nstruct ccp_crypto_ahash_alg *ccp_alg;\r\nstruct ahash_alg *alg;\r\nstruct hash_alg_common *halg;\r\nstruct crypto_alg *base;\r\nint ret;\r\nccp_alg = kzalloc(sizeof(*ccp_alg), GFP_KERNEL);\r\nif (!ccp_alg)\r\nreturn -ENOMEM;\r\n*ccp_alg = *base_alg;\r\nINIT_LIST_HEAD(&ccp_alg->entry);\r\nstrncpy(ccp_alg->child_alg, def->name, CRYPTO_MAX_ALG_NAME);\r\nalg = &ccp_alg->alg;\r\nalg->setkey = ccp_sha_setkey;\r\nhalg = &alg->halg;\r\nbase = &halg->base;\r\nsnprintf(base->cra_name, CRYPTO_MAX_ALG_NAME, "hmac(%s)", def->name);\r\nsnprintf(base->cra_driver_name, CRYPTO_MAX_ALG_NAME, "hmac-%s",\r\ndef->drv_name);\r\nbase->cra_init = ccp_hmac_sha_cra_init;\r\nbase->cra_exit = ccp_hmac_sha_cra_exit;\r\nret = crypto_register_ahash(alg);\r\nif (ret) {\r\npr_err("%s ahash algorithm registration error (%d)\n",\r\nbase->cra_name, ret);\r\nkfree(ccp_alg);\r\nreturn ret;\r\n}\r\nlist_add(&ccp_alg->entry, head);\r\nreturn ret;\r\n}\r\nstatic int ccp_register_sha_alg(struct list_head *head,\r\nconst struct ccp_sha_def *def)\r\n{\r\nstruct ccp_crypto_ahash_alg *ccp_alg;\r\nstruct ahash_alg *alg;\r\nstruct hash_alg_common *halg;\r\nstruct crypto_alg *base;\r\nint ret;\r\nccp_alg = kzalloc(sizeof(*ccp_alg), GFP_KERNEL);\r\nif (!ccp_alg)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&ccp_alg->entry);\r\nccp_alg->type = def->type;\r\nalg = &ccp_alg->alg;\r\nalg->init = ccp_sha_init;\r\nalg->update = ccp_sha_update;\r\nalg->final = ccp_sha_final;\r\nalg->finup = ccp_sha_finup;\r\nalg->digest = ccp_sha_digest;\r\nhalg = &alg->halg;\r\nhalg->digestsize = def->digest_size;\r\nbase = &halg->base;\r\nsnprintf(base->cra_name, CRYPTO_MAX_ALG_NAME, "%s", def->name);\r\nsnprintf(base->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ndef->drv_name);\r\nbase->cra_flags = CRYPTO_ALG_TYPE_AHASH | CRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_KERN_DRIVER_ONLY |\r\nCRYPTO_ALG_NEED_FALLBACK;\r\nbase->cra_blocksize = def->block_size;\r\nbase->cra_ctxsize = sizeof(struct ccp_ctx);\r\nbase->cra_priority = CCP_CRA_PRIORITY;\r\nbase->cra_type = &crypto_ahash_type;\r\nbase->cra_init = ccp_sha_cra_init;\r\nbase->cra_exit = ccp_sha_cra_exit;\r\nbase->cra_module = THIS_MODULE;\r\nret = crypto_register_ahash(alg);\r\nif (ret) {\r\npr_err("%s ahash algorithm registration error (%d)\n",\r\nbase->cra_name, ret);\r\nkfree(ccp_alg);\r\nreturn ret;\r\n}\r\nlist_add(&ccp_alg->entry, head);\r\nret = ccp_register_hmac_alg(head, def, ccp_alg);\r\nreturn ret;\r\n}\r\nint ccp_register_sha_algs(struct list_head *head)\r\n{\r\nint i, ret;\r\nfor (i = 0; i < ARRAY_SIZE(sha_algs); i++) {\r\nret = ccp_register_sha_alg(head, &sha_algs[i]);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}
