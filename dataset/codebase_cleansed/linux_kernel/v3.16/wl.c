static void update_fastmap_work_fn(struct work_struct *wrk)\r\n{\r\nstruct ubi_device *ubi = container_of(wrk, struct ubi_device, fm_work);\r\nubi_update_fastmap(ubi);\r\n}\r\nstatic int ubi_is_fm_block(struct ubi_device *ubi, int pnum)\r\n{\r\nint i;\r\nif (!ubi->fm)\r\nreturn 0;\r\nfor (i = 0; i < ubi->fm->used_blocks; i++)\r\nif (ubi->fm->e[i]->pnum == pnum)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int ubi_is_fm_block(struct ubi_device *ubi, int pnum)\r\n{\r\nreturn 0;\r\n}\r\nstatic void wl_tree_add(struct ubi_wl_entry *e, struct rb_root *root)\r\n{\r\nstruct rb_node **p, *parent = NULL;\r\np = &root->rb_node;\r\nwhile (*p) {\r\nstruct ubi_wl_entry *e1;\r\nparent = *p;\r\ne1 = rb_entry(parent, struct ubi_wl_entry, u.rb);\r\nif (e->ec < e1->ec)\r\np = &(*p)->rb_left;\r\nelse if (e->ec > e1->ec)\r\np = &(*p)->rb_right;\r\nelse {\r\nubi_assert(e->pnum != e1->pnum);\r\nif (e->pnum < e1->pnum)\r\np = &(*p)->rb_left;\r\nelse\r\np = &(*p)->rb_right;\r\n}\r\n}\r\nrb_link_node(&e->u.rb, parent, p);\r\nrb_insert_color(&e->u.rb, root);\r\n}\r\nstatic int do_work(struct ubi_device *ubi)\r\n{\r\nint err;\r\nstruct ubi_work *wrk;\r\ncond_resched();\r\ndown_read(&ubi->work_sem);\r\nspin_lock(&ubi->wl_lock);\r\nif (list_empty(&ubi->works)) {\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->work_sem);\r\nreturn 0;\r\n}\r\nwrk = list_entry(ubi->works.next, struct ubi_work, list);\r\nlist_del(&wrk->list);\r\nubi->works_count -= 1;\r\nubi_assert(ubi->works_count >= 0);\r\nspin_unlock(&ubi->wl_lock);\r\nerr = wrk->func(ubi, wrk, 0);\r\nif (err)\r\nubi_err("work failed with error code %d", err);\r\nup_read(&ubi->work_sem);\r\nreturn err;\r\n}\r\nstatic int produce_free_peb(struct ubi_device *ubi)\r\n{\r\nint err;\r\nwhile (!ubi->free.rb_node) {\r\nspin_unlock(&ubi->wl_lock);\r\ndbg_wl("do one work synchronously");\r\nerr = do_work(ubi);\r\nspin_lock(&ubi->wl_lock);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic int in_wl_tree(struct ubi_wl_entry *e, struct rb_root *root)\r\n{\r\nstruct rb_node *p;\r\np = root->rb_node;\r\nwhile (p) {\r\nstruct ubi_wl_entry *e1;\r\ne1 = rb_entry(p, struct ubi_wl_entry, u.rb);\r\nif (e->pnum == e1->pnum) {\r\nubi_assert(e == e1);\r\nreturn 1;\r\n}\r\nif (e->ec < e1->ec)\r\np = p->rb_left;\r\nelse if (e->ec > e1->ec)\r\np = p->rb_right;\r\nelse {\r\nubi_assert(e->pnum != e1->pnum);\r\nif (e->pnum < e1->pnum)\r\np = p->rb_left;\r\nelse\r\np = p->rb_right;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void prot_queue_add(struct ubi_device *ubi, struct ubi_wl_entry *e)\r\n{\r\nint pq_tail = ubi->pq_head - 1;\r\nif (pq_tail < 0)\r\npq_tail = UBI_PROT_QUEUE_LEN - 1;\r\nubi_assert(pq_tail >= 0 && pq_tail < UBI_PROT_QUEUE_LEN);\r\nlist_add_tail(&e->u.list, &ubi->pq[pq_tail]);\r\ndbg_wl("added PEB %d EC %d to the protection queue", e->pnum, e->ec);\r\n}\r\nstatic struct ubi_wl_entry *find_wl_entry(struct ubi_device *ubi,\r\nstruct rb_root *root, int diff)\r\n{\r\nstruct rb_node *p;\r\nstruct ubi_wl_entry *e, *prev_e = NULL;\r\nint max;\r\ne = rb_entry(rb_first(root), struct ubi_wl_entry, u.rb);\r\nmax = e->ec + diff;\r\np = root->rb_node;\r\nwhile (p) {\r\nstruct ubi_wl_entry *e1;\r\ne1 = rb_entry(p, struct ubi_wl_entry, u.rb);\r\nif (e1->ec >= max)\r\np = p->rb_left;\r\nelse {\r\np = p->rb_right;\r\nprev_e = e;\r\ne = e1;\r\n}\r\n}\r\nif (prev_e && !ubi->fm_disabled &&\r\n!ubi->fm && e->pnum < UBI_FM_MAX_START)\r\nreturn prev_e;\r\nreturn e;\r\n}\r\nstatic struct ubi_wl_entry *find_mean_wl_entry(struct ubi_device *ubi,\r\nstruct rb_root *root)\r\n{\r\nstruct ubi_wl_entry *e, *first, *last;\r\nfirst = rb_entry(rb_first(root), struct ubi_wl_entry, u.rb);\r\nlast = rb_entry(rb_last(root), struct ubi_wl_entry, u.rb);\r\nif (last->ec - first->ec < WL_FREE_MAX_DIFF) {\r\ne = rb_entry(root->rb_node, struct ubi_wl_entry, u.rb);\r\n#ifdef CONFIG_MTD_UBI_FASTMAP\r\nif (e && !ubi->fm_disabled && !ubi->fm &&\r\ne->pnum < UBI_FM_MAX_START)\r\ne = rb_entry(rb_next(root->rb_node),\r\nstruct ubi_wl_entry, u.rb);\r\n#endif\r\n} else\r\ne = find_wl_entry(ubi, root, WL_FREE_MAX_DIFF/2);\r\nreturn e;\r\n}\r\nstatic struct ubi_wl_entry *find_anchor_wl_entry(struct rb_root *root)\r\n{\r\nstruct rb_node *p;\r\nstruct ubi_wl_entry *e, *victim = NULL;\r\nint max_ec = UBI_MAX_ERASECOUNTER;\r\nubi_rb_for_each_entry(p, e, root, u.rb) {\r\nif (e->pnum < UBI_FM_MAX_START && e->ec < max_ec) {\r\nvictim = e;\r\nmax_ec = e->ec;\r\n}\r\n}\r\nreturn victim;\r\n}\r\nstatic int anchor_pebs_avalible(struct rb_root *root)\r\n{\r\nstruct rb_node *p;\r\nstruct ubi_wl_entry *e;\r\nubi_rb_for_each_entry(p, e, root, u.rb)\r\nif (e->pnum < UBI_FM_MAX_START)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstruct ubi_wl_entry *ubi_wl_get_fm_peb(struct ubi_device *ubi, int anchor)\r\n{\r\nstruct ubi_wl_entry *e = NULL;\r\nif (!ubi->free.rb_node || (ubi->free_count - ubi->beb_rsvd_pebs < 1))\r\ngoto out;\r\nif (anchor)\r\ne = find_anchor_wl_entry(&ubi->free);\r\nelse\r\ne = find_mean_wl_entry(ubi, &ubi->free);\r\nif (!e)\r\ngoto out;\r\nself_check_in_wl_tree(ubi, e, &ubi->free);\r\nrb_erase(&e->u.rb, &ubi->free);\r\nubi->free_count--;\r\nout:\r\nreturn e;\r\n}\r\nstatic int __wl_get_peb(struct ubi_device *ubi)\r\n{\r\nint err;\r\nstruct ubi_wl_entry *e;\r\nretry:\r\nif (!ubi->free.rb_node) {\r\nif (ubi->works_count == 0) {\r\nubi_err("no free eraseblocks");\r\nubi_assert(list_empty(&ubi->works));\r\nreturn -ENOSPC;\r\n}\r\nerr = produce_free_peb(ubi);\r\nif (err < 0)\r\nreturn err;\r\ngoto retry;\r\n}\r\ne = find_mean_wl_entry(ubi, &ubi->free);\r\nif (!e) {\r\nubi_err("no free eraseblocks");\r\nreturn -ENOSPC;\r\n}\r\nself_check_in_wl_tree(ubi, e, &ubi->free);\r\nrb_erase(&e->u.rb, &ubi->free);\r\nubi->free_count--;\r\ndbg_wl("PEB %d EC %d", e->pnum, e->ec);\r\n#ifndef CONFIG_MTD_UBI_FASTMAP\r\nprot_queue_add(ubi, e);\r\n#endif\r\nreturn e->pnum;\r\n}\r\nstatic void return_unused_pool_pebs(struct ubi_device *ubi,\r\nstruct ubi_fm_pool *pool)\r\n{\r\nint i;\r\nstruct ubi_wl_entry *e;\r\nfor (i = pool->used; i < pool->size; i++) {\r\ne = ubi->lookuptbl[pool->pebs[i]];\r\nwl_tree_add(e, &ubi->free);\r\nubi->free_count++;\r\n}\r\n}\r\nstatic void refill_wl_pool(struct ubi_device *ubi)\r\n{\r\nstruct ubi_wl_entry *e;\r\nstruct ubi_fm_pool *pool = &ubi->fm_wl_pool;\r\nreturn_unused_pool_pebs(ubi, pool);\r\nfor (pool->size = 0; pool->size < pool->max_size; pool->size++) {\r\nif (!ubi->free.rb_node ||\r\n(ubi->free_count - ubi->beb_rsvd_pebs < 5))\r\nbreak;\r\ne = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\r\nself_check_in_wl_tree(ubi, e, &ubi->free);\r\nrb_erase(&e->u.rb, &ubi->free);\r\nubi->free_count--;\r\npool->pebs[pool->size] = e->pnum;\r\n}\r\npool->used = 0;\r\n}\r\nstatic void refill_wl_user_pool(struct ubi_device *ubi)\r\n{\r\nstruct ubi_fm_pool *pool = &ubi->fm_pool;\r\nreturn_unused_pool_pebs(ubi, pool);\r\nfor (pool->size = 0; pool->size < pool->max_size; pool->size++) {\r\npool->pebs[pool->size] = __wl_get_peb(ubi);\r\nif (pool->pebs[pool->size] < 0)\r\nbreak;\r\n}\r\npool->used = 0;\r\n}\r\nvoid ubi_refill_pools(struct ubi_device *ubi)\r\n{\r\nspin_lock(&ubi->wl_lock);\r\nrefill_wl_pool(ubi);\r\nrefill_wl_user_pool(ubi);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nint ubi_wl_get_peb(struct ubi_device *ubi)\r\n{\r\nint ret;\r\nstruct ubi_fm_pool *pool = &ubi->fm_pool;\r\nstruct ubi_fm_pool *wl_pool = &ubi->fm_wl_pool;\r\nif (!pool->size || !wl_pool->size || pool->used == pool->size ||\r\nwl_pool->used == wl_pool->size)\r\nubi_update_fastmap(ubi);\r\nif (!pool->size)\r\nret = -ENOSPC;\r\nelse {\r\nspin_lock(&ubi->wl_lock);\r\nret = pool->pebs[pool->used++];\r\nprot_queue_add(ubi, ubi->lookuptbl[ret]);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nreturn ret;\r\n}\r\nstatic struct ubi_wl_entry *get_peb_for_wl(struct ubi_device *ubi)\r\n{\r\nstruct ubi_fm_pool *pool = &ubi->fm_wl_pool;\r\nint pnum;\r\nif (pool->used == pool->size || !pool->size) {\r\nschedule_work(&ubi->fm_work);\r\nreturn NULL;\r\n} else {\r\npnum = pool->pebs[pool->used++];\r\nreturn ubi->lookuptbl[pnum];\r\n}\r\n}\r\nstatic struct ubi_wl_entry *get_peb_for_wl(struct ubi_device *ubi)\r\n{\r\nstruct ubi_wl_entry *e;\r\ne = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\r\nself_check_in_wl_tree(ubi, e, &ubi->free);\r\nubi->free_count--;\r\nubi_assert(ubi->free_count >= 0);\r\nrb_erase(&e->u.rb, &ubi->free);\r\nreturn e;\r\n}\r\nint ubi_wl_get_peb(struct ubi_device *ubi)\r\n{\r\nint peb, err;\r\nspin_lock(&ubi->wl_lock);\r\npeb = __wl_get_peb(ubi);\r\nspin_unlock(&ubi->wl_lock);\r\nif (peb < 0)\r\nreturn peb;\r\nerr = ubi_self_check_all_ff(ubi, peb, ubi->vid_hdr_aloffset,\r\nubi->peb_size - ubi->vid_hdr_aloffset);\r\nif (err) {\r\nubi_err("new PEB %d does not contain all 0xFF bytes", peb);\r\nreturn err;\r\n}\r\nreturn peb;\r\n}\r\nstatic int prot_queue_del(struct ubi_device *ubi, int pnum)\r\n{\r\nstruct ubi_wl_entry *e;\r\ne = ubi->lookuptbl[pnum];\r\nif (!e)\r\nreturn -ENODEV;\r\nif (self_check_in_pq(ubi, e))\r\nreturn -ENODEV;\r\nlist_del(&e->u.list);\r\ndbg_wl("deleted PEB %d from the protection queue", e->pnum);\r\nreturn 0;\r\n}\r\nstatic int sync_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\r\nint torture)\r\n{\r\nint err;\r\nstruct ubi_ec_hdr *ec_hdr;\r\nunsigned long long ec = e->ec;\r\ndbg_wl("erase PEB %d, old EC %llu", e->pnum, ec);\r\nerr = self_check_ec(ubi, e->pnum, e->ec);\r\nif (err)\r\nreturn -EINVAL;\r\nec_hdr = kzalloc(ubi->ec_hdr_alsize, GFP_NOFS);\r\nif (!ec_hdr)\r\nreturn -ENOMEM;\r\nerr = ubi_io_sync_erase(ubi, e->pnum, torture);\r\nif (err < 0)\r\ngoto out_free;\r\nec += err;\r\nif (ec > UBI_MAX_ERASECOUNTER) {\r\nubi_err("erase counter overflow at PEB %d, EC %llu",\r\ne->pnum, ec);\r\nerr = -EINVAL;\r\ngoto out_free;\r\n}\r\ndbg_wl("erased PEB %d, new EC %llu", e->pnum, ec);\r\nec_hdr->ec = cpu_to_be64(ec);\r\nerr = ubi_io_write_ec_hdr(ubi, e->pnum, ec_hdr);\r\nif (err)\r\ngoto out_free;\r\ne->ec = ec;\r\nspin_lock(&ubi->wl_lock);\r\nif (e->ec > ubi->max_ec)\r\nubi->max_ec = e->ec;\r\nspin_unlock(&ubi->wl_lock);\r\nout_free:\r\nkfree(ec_hdr);\r\nreturn err;\r\n}\r\nstatic void serve_prot_queue(struct ubi_device *ubi)\r\n{\r\nstruct ubi_wl_entry *e, *tmp;\r\nint count;\r\nrepeat:\r\ncount = 0;\r\nspin_lock(&ubi->wl_lock);\r\nlist_for_each_entry_safe(e, tmp, &ubi->pq[ubi->pq_head], u.list) {\r\ndbg_wl("PEB %d EC %d protection over, move to used tree",\r\ne->pnum, e->ec);\r\nlist_del(&e->u.list);\r\nwl_tree_add(e, &ubi->used);\r\nif (count++ > 32) {\r\nspin_unlock(&ubi->wl_lock);\r\ncond_resched();\r\ngoto repeat;\r\n}\r\n}\r\nubi->pq_head += 1;\r\nif (ubi->pq_head == UBI_PROT_QUEUE_LEN)\r\nubi->pq_head = 0;\r\nubi_assert(ubi->pq_head >= 0 && ubi->pq_head < UBI_PROT_QUEUE_LEN);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nstatic void __schedule_ubi_work(struct ubi_device *ubi, struct ubi_work *wrk)\r\n{\r\nspin_lock(&ubi->wl_lock);\r\nlist_add_tail(&wrk->list, &ubi->works);\r\nubi_assert(ubi->works_count >= 0);\r\nubi->works_count += 1;\r\nif (ubi->thread_enabled && !ubi_dbg_is_bgt_disabled(ubi))\r\nwake_up_process(ubi->bgt_thread);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nstatic void schedule_ubi_work(struct ubi_device *ubi, struct ubi_work *wrk)\r\n{\r\ndown_read(&ubi->work_sem);\r\n__schedule_ubi_work(ubi, wrk);\r\nup_read(&ubi->work_sem);\r\n}\r\nint ubi_is_erase_work(struct ubi_work *wrk)\r\n{\r\nreturn wrk->func == erase_worker;\r\n}\r\nstatic int schedule_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\r\nint vol_id, int lnum, int torture)\r\n{\r\nstruct ubi_work *wl_wrk;\r\nubi_assert(e);\r\nubi_assert(!ubi_is_fm_block(ubi, e->pnum));\r\ndbg_wl("schedule erasure of PEB %d, EC %d, torture %d",\r\ne->pnum, e->ec, torture);\r\nwl_wrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\r\nif (!wl_wrk)\r\nreturn -ENOMEM;\r\nwl_wrk->func = &erase_worker;\r\nwl_wrk->e = e;\r\nwl_wrk->vol_id = vol_id;\r\nwl_wrk->lnum = lnum;\r\nwl_wrk->torture = torture;\r\nschedule_ubi_work(ubi, wl_wrk);\r\nreturn 0;\r\n}\r\nstatic int do_sync_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\r\nint vol_id, int lnum, int torture)\r\n{\r\nstruct ubi_work *wl_wrk;\r\ndbg_wl("sync erase of PEB %i", e->pnum);\r\nwl_wrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\r\nif (!wl_wrk)\r\nreturn -ENOMEM;\r\nwl_wrk->e = e;\r\nwl_wrk->vol_id = vol_id;\r\nwl_wrk->lnum = lnum;\r\nwl_wrk->torture = torture;\r\nreturn erase_worker(ubi, wl_wrk, 0);\r\n}\r\nint ubi_wl_put_fm_peb(struct ubi_device *ubi, struct ubi_wl_entry *fm_e,\r\nint lnum, int torture)\r\n{\r\nstruct ubi_wl_entry *e;\r\nint vol_id, pnum = fm_e->pnum;\r\ndbg_wl("PEB %d", pnum);\r\nubi_assert(pnum >= 0);\r\nubi_assert(pnum < ubi->peb_count);\r\nspin_lock(&ubi->wl_lock);\r\ne = ubi->lookuptbl[pnum];\r\nif (!e) {\r\ne = fm_e;\r\nubi_assert(e->ec >= 0);\r\nubi->lookuptbl[pnum] = e;\r\n} else {\r\ne->ec = fm_e->ec;\r\nkfree(fm_e);\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nvol_id = lnum ? UBI_FM_DATA_VOLUME_ID : UBI_FM_SB_VOLUME_ID;\r\nreturn schedule_erase(ubi, e, vol_id, lnum, torture);\r\n}\r\nstatic int ensure_wear_leveling(struct ubi_device *ubi, int nested)\r\n{\r\nint err = 0;\r\nstruct ubi_wl_entry *e1;\r\nstruct ubi_wl_entry *e2;\r\nstruct ubi_work *wrk;\r\nspin_lock(&ubi->wl_lock);\r\nif (ubi->wl_scheduled)\r\ngoto out_unlock;\r\nif (!ubi->scrub.rb_node) {\r\nif (!ubi->used.rb_node || !ubi->free.rb_node)\r\ngoto out_unlock;\r\ne1 = rb_entry(rb_first(&ubi->used), struct ubi_wl_entry, u.rb);\r\ne2 = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\r\nif (!(e2->ec - e1->ec >= UBI_WL_THRESHOLD))\r\ngoto out_unlock;\r\ndbg_wl("schedule wear-leveling");\r\n} else\r\ndbg_wl("schedule scrubbing");\r\nubi->wl_scheduled = 1;\r\nspin_unlock(&ubi->wl_lock);\r\nwrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\r\nif (!wrk) {\r\nerr = -ENOMEM;\r\ngoto out_cancel;\r\n}\r\nwrk->anchor = 0;\r\nwrk->func = &wear_leveling_worker;\r\nif (nested)\r\n__schedule_ubi_work(ubi, wrk);\r\nelse\r\nschedule_ubi_work(ubi, wrk);\r\nreturn err;\r\nout_cancel:\r\nspin_lock(&ubi->wl_lock);\r\nubi->wl_scheduled = 0;\r\nout_unlock:\r\nspin_unlock(&ubi->wl_lock);\r\nreturn err;\r\n}\r\nint ubi_ensure_anchor_pebs(struct ubi_device *ubi)\r\n{\r\nstruct ubi_work *wrk;\r\nspin_lock(&ubi->wl_lock);\r\nif (ubi->wl_scheduled) {\r\nspin_unlock(&ubi->wl_lock);\r\nreturn 0;\r\n}\r\nubi->wl_scheduled = 1;\r\nspin_unlock(&ubi->wl_lock);\r\nwrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\r\nif (!wrk) {\r\nspin_lock(&ubi->wl_lock);\r\nubi->wl_scheduled = 0;\r\nspin_unlock(&ubi->wl_lock);\r\nreturn -ENOMEM;\r\n}\r\nwrk->anchor = 1;\r\nwrk->func = &wear_leveling_worker;\r\nschedule_ubi_work(ubi, wrk);\r\nreturn 0;\r\n}\r\nstatic int erase_worker(struct ubi_device *ubi, struct ubi_work *wl_wrk,\r\nint cancel)\r\n{\r\nstruct ubi_wl_entry *e = wl_wrk->e;\r\nint pnum = e->pnum;\r\nint vol_id = wl_wrk->vol_id;\r\nint lnum = wl_wrk->lnum;\r\nint err, available_consumed = 0;\r\nif (cancel) {\r\ndbg_wl("cancel erasure of PEB %d EC %d", pnum, e->ec);\r\nkfree(wl_wrk);\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\nreturn 0;\r\n}\r\ndbg_wl("erase PEB %d EC %d LEB %d:%d",\r\npnum, e->ec, wl_wrk->vol_id, wl_wrk->lnum);\r\nubi_assert(!ubi_is_fm_block(ubi, e->pnum));\r\nerr = sync_erase(ubi, e, wl_wrk->torture);\r\nif (!err) {\r\nkfree(wl_wrk);\r\nspin_lock(&ubi->wl_lock);\r\nwl_tree_add(e, &ubi->free);\r\nubi->free_count++;\r\nspin_unlock(&ubi->wl_lock);\r\nserve_prot_queue(ubi);\r\nerr = ensure_wear_leveling(ubi, 1);\r\nreturn err;\r\n}\r\nubi_err("failed to erase PEB %d, error %d", pnum, err);\r\nkfree(wl_wrk);\r\nif (err == -EINTR || err == -ENOMEM || err == -EAGAIN ||\r\nerr == -EBUSY) {\r\nint err1;\r\nerr1 = schedule_erase(ubi, e, vol_id, lnum, 0);\r\nif (err1) {\r\nerr = err1;\r\ngoto out_ro;\r\n}\r\nreturn err;\r\n}\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\nif (err != -EIO)\r\ngoto out_ro;\r\nif (!ubi->bad_allowed) {\r\nubi_err("bad physical eraseblock %d detected", pnum);\r\ngoto out_ro;\r\n}\r\nspin_lock(&ubi->volumes_lock);\r\nif (ubi->beb_rsvd_pebs == 0) {\r\nif (ubi->avail_pebs == 0) {\r\nspin_unlock(&ubi->volumes_lock);\r\nubi_err("no reserved/available physical eraseblocks");\r\ngoto out_ro;\r\n}\r\nubi->avail_pebs -= 1;\r\navailable_consumed = 1;\r\n}\r\nspin_unlock(&ubi->volumes_lock);\r\nubi_msg("mark PEB %d as bad", pnum);\r\nerr = ubi_io_mark_bad(ubi, pnum);\r\nif (err)\r\ngoto out_ro;\r\nspin_lock(&ubi->volumes_lock);\r\nif (ubi->beb_rsvd_pebs > 0) {\r\nif (available_consumed) {\r\nubi->avail_pebs += 1;\r\navailable_consumed = 0;\r\n}\r\nubi->beb_rsvd_pebs -= 1;\r\n}\r\nubi->bad_peb_count += 1;\r\nubi->good_peb_count -= 1;\r\nubi_calculate_reserved(ubi);\r\nif (available_consumed)\r\nubi_warn("no PEBs in the reserved pool, used an available PEB");\r\nelse if (ubi->beb_rsvd_pebs)\r\nubi_msg("%d PEBs left in the reserve", ubi->beb_rsvd_pebs);\r\nelse\r\nubi_warn("last PEB from the reserve was used");\r\nspin_unlock(&ubi->volumes_lock);\r\nreturn err;\r\nout_ro:\r\nif (available_consumed) {\r\nspin_lock(&ubi->volumes_lock);\r\nubi->avail_pebs += 1;\r\nspin_unlock(&ubi->volumes_lock);\r\n}\r\nubi_ro_mode(ubi);\r\nreturn err;\r\n}\r\nint ubi_wl_put_peb(struct ubi_device *ubi, int vol_id, int lnum,\r\nint pnum, int torture)\r\n{\r\nint err;\r\nstruct ubi_wl_entry *e;\r\ndbg_wl("PEB %d", pnum);\r\nubi_assert(pnum >= 0);\r\nubi_assert(pnum < ubi->peb_count);\r\nretry:\r\nspin_lock(&ubi->wl_lock);\r\ne = ubi->lookuptbl[pnum];\r\nif (e == ubi->move_from) {\r\ndbg_wl("PEB %d is being moved, wait", pnum);\r\nspin_unlock(&ubi->wl_lock);\r\nmutex_lock(&ubi->move_mutex);\r\nmutex_unlock(&ubi->move_mutex);\r\ngoto retry;\r\n} else if (e == ubi->move_to) {\r\ndbg_wl("PEB %d is the target of data moving", pnum);\r\nubi_assert(!ubi->move_to_put);\r\nubi->move_to_put = 1;\r\nspin_unlock(&ubi->wl_lock);\r\nreturn 0;\r\n} else {\r\nif (in_wl_tree(e, &ubi->used)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->used);\r\nrb_erase(&e->u.rb, &ubi->used);\r\n} else if (in_wl_tree(e, &ubi->scrub)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->scrub);\r\nrb_erase(&e->u.rb, &ubi->scrub);\r\n} else if (in_wl_tree(e, &ubi->erroneous)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->erroneous);\r\nrb_erase(&e->u.rb, &ubi->erroneous);\r\nubi->erroneous_peb_count -= 1;\r\nubi_assert(ubi->erroneous_peb_count >= 0);\r\ntorture = 1;\r\n} else {\r\nerr = prot_queue_del(ubi, e->pnum);\r\nif (err) {\r\nubi_err("PEB %d not found", pnum);\r\nubi_ro_mode(ubi);\r\nspin_unlock(&ubi->wl_lock);\r\nreturn err;\r\n}\r\n}\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nerr = schedule_erase(ubi, e, vol_id, lnum, torture);\r\nif (err) {\r\nspin_lock(&ubi->wl_lock);\r\nwl_tree_add(e, &ubi->used);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nreturn err;\r\n}\r\nint ubi_wl_scrub_peb(struct ubi_device *ubi, int pnum)\r\n{\r\nstruct ubi_wl_entry *e;\r\nubi_msg("schedule PEB %d for scrubbing", pnum);\r\nretry:\r\nspin_lock(&ubi->wl_lock);\r\ne = ubi->lookuptbl[pnum];\r\nif (e == ubi->move_from || in_wl_tree(e, &ubi->scrub) ||\r\nin_wl_tree(e, &ubi->erroneous)) {\r\nspin_unlock(&ubi->wl_lock);\r\nreturn 0;\r\n}\r\nif (e == ubi->move_to) {\r\nspin_unlock(&ubi->wl_lock);\r\ndbg_wl("the PEB %d is not in proper tree, retry", pnum);\r\nyield();\r\ngoto retry;\r\n}\r\nif (in_wl_tree(e, &ubi->used)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->used);\r\nrb_erase(&e->u.rb, &ubi->used);\r\n} else {\r\nint err;\r\nerr = prot_queue_del(ubi, e->pnum);\r\nif (err) {\r\nubi_err("PEB %d not found", pnum);\r\nubi_ro_mode(ubi);\r\nspin_unlock(&ubi->wl_lock);\r\nreturn err;\r\n}\r\n}\r\nwl_tree_add(e, &ubi->scrub);\r\nspin_unlock(&ubi->wl_lock);\r\nreturn ensure_wear_leveling(ubi, 0);\r\n}\r\nint ubi_wl_flush(struct ubi_device *ubi, int vol_id, int lnum)\r\n{\r\nint err = 0;\r\nint found = 1;\r\ndbg_wl("flush pending work for LEB %d:%d (%d pending works)",\r\nvol_id, lnum, ubi->works_count);\r\nwhile (found) {\r\nstruct ubi_work *wrk;\r\nfound = 0;\r\ndown_read(&ubi->work_sem);\r\nspin_lock(&ubi->wl_lock);\r\nlist_for_each_entry(wrk, &ubi->works, list) {\r\nif ((vol_id == UBI_ALL || wrk->vol_id == vol_id) &&\r\n(lnum == UBI_ALL || wrk->lnum == lnum)) {\r\nlist_del(&wrk->list);\r\nubi->works_count -= 1;\r\nubi_assert(ubi->works_count >= 0);\r\nspin_unlock(&ubi->wl_lock);\r\nerr = wrk->func(ubi, wrk, 0);\r\nif (err) {\r\nup_read(&ubi->work_sem);\r\nreturn err;\r\n}\r\nspin_lock(&ubi->wl_lock);\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->work_sem);\r\n}\r\ndown_write(&ubi->work_sem);\r\nup_write(&ubi->work_sem);\r\nreturn err;\r\n}\r\nstatic void tree_destroy(struct rb_root *root)\r\n{\r\nstruct rb_node *rb;\r\nstruct ubi_wl_entry *e;\r\nrb = root->rb_node;\r\nwhile (rb) {\r\nif (rb->rb_left)\r\nrb = rb->rb_left;\r\nelse if (rb->rb_right)\r\nrb = rb->rb_right;\r\nelse {\r\ne = rb_entry(rb, struct ubi_wl_entry, u.rb);\r\nrb = rb_parent(rb);\r\nif (rb) {\r\nif (rb->rb_left == &e->u.rb)\r\nrb->rb_left = NULL;\r\nelse\r\nrb->rb_right = NULL;\r\n}\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\n}\r\n}\r\n}\r\nint ubi_thread(void *u)\r\n{\r\nint failures = 0;\r\nstruct ubi_device *ubi = u;\r\nubi_msg("background thread \"%s\" started, PID %d",\r\nubi->bgt_name, task_pid_nr(current));\r\nset_freezable();\r\nfor (;;) {\r\nint err;\r\nif (kthread_should_stop())\r\nbreak;\r\nif (try_to_freeze())\r\ncontinue;\r\nspin_lock(&ubi->wl_lock);\r\nif (list_empty(&ubi->works) || ubi->ro_mode ||\r\n!ubi->thread_enabled || ubi_dbg_is_bgt_disabled(ubi)) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nspin_unlock(&ubi->wl_lock);\r\nschedule();\r\ncontinue;\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nerr = do_work(ubi);\r\nif (err) {\r\nubi_err("%s: work failed with error code %d",\r\nubi->bgt_name, err);\r\nif (failures++ > WL_MAX_FAILURES) {\r\nubi_msg("%s: %d consecutive failures",\r\nubi->bgt_name, WL_MAX_FAILURES);\r\nubi_ro_mode(ubi);\r\nubi->thread_enabled = 0;\r\ncontinue;\r\n}\r\n} else\r\nfailures = 0;\r\ncond_resched();\r\n}\r\ndbg_wl("background thread \"%s\" is killed", ubi->bgt_name);\r\nreturn 0;\r\n}\r\nstatic void cancel_pending(struct ubi_device *ubi)\r\n{\r\nwhile (!list_empty(&ubi->works)) {\r\nstruct ubi_work *wrk;\r\nwrk = list_entry(ubi->works.next, struct ubi_work, list);\r\nlist_del(&wrk->list);\r\nwrk->func(ubi, wrk, 1);\r\nubi->works_count -= 1;\r\nubi_assert(ubi->works_count >= 0);\r\n}\r\n}\r\nint ubi_wl_init(struct ubi_device *ubi, struct ubi_attach_info *ai)\r\n{\r\nint err, i, reserved_pebs, found_pebs = 0;\r\nstruct rb_node *rb1, *rb2;\r\nstruct ubi_ainf_volume *av;\r\nstruct ubi_ainf_peb *aeb, *tmp;\r\nstruct ubi_wl_entry *e;\r\nubi->used = ubi->erroneous = ubi->free = ubi->scrub = RB_ROOT;\r\nspin_lock_init(&ubi->wl_lock);\r\nmutex_init(&ubi->move_mutex);\r\ninit_rwsem(&ubi->work_sem);\r\nubi->max_ec = ai->max_ec;\r\nINIT_LIST_HEAD(&ubi->works);\r\n#ifdef CONFIG_MTD_UBI_FASTMAP\r\nINIT_WORK(&ubi->fm_work, update_fastmap_work_fn);\r\n#endif\r\nsprintf(ubi->bgt_name, UBI_BGT_NAME_PATTERN, ubi->ubi_num);\r\nerr = -ENOMEM;\r\nubi->lookuptbl = kzalloc(ubi->peb_count * sizeof(void *), GFP_KERNEL);\r\nif (!ubi->lookuptbl)\r\nreturn err;\r\nfor (i = 0; i < UBI_PROT_QUEUE_LEN; i++)\r\nINIT_LIST_HEAD(&ubi->pq[i]);\r\nubi->pq_head = 0;\r\nlist_for_each_entry_safe(aeb, tmp, &ai->erase, u.list) {\r\ncond_resched();\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi_assert(!ubi_is_fm_block(ubi, e->pnum));\r\nubi->lookuptbl[e->pnum] = e;\r\nif (schedule_erase(ubi, e, aeb->vol_id, aeb->lnum, 0)) {\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\ngoto out_free;\r\n}\r\nfound_pebs++;\r\n}\r\nubi->free_count = 0;\r\nlist_for_each_entry(aeb, &ai->free, u.list) {\r\ncond_resched();\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi_assert(e->ec >= 0);\r\nubi_assert(!ubi_is_fm_block(ubi, e->pnum));\r\nwl_tree_add(e, &ubi->free);\r\nubi->free_count++;\r\nubi->lookuptbl[e->pnum] = e;\r\nfound_pebs++;\r\n}\r\nubi_rb_for_each_entry(rb1, av, &ai->volumes, rb) {\r\nubi_rb_for_each_entry(rb2, aeb, &av->root, u.rb) {\r\ncond_resched();\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi->lookuptbl[e->pnum] = e;\r\nif (!aeb->scrub) {\r\ndbg_wl("add PEB %d EC %d to the used tree",\r\ne->pnum, e->ec);\r\nwl_tree_add(e, &ubi->used);\r\n} else {\r\ndbg_wl("add PEB %d EC %d to the scrub tree",\r\ne->pnum, e->ec);\r\nwl_tree_add(e, &ubi->scrub);\r\n}\r\nfound_pebs++;\r\n}\r\n}\r\ndbg_wl("found %i PEBs", found_pebs);\r\nif (ubi->fm)\r\nubi_assert(ubi->good_peb_count == \\r\nfound_pebs + ubi->fm->used_blocks);\r\nelse\r\nubi_assert(ubi->good_peb_count == found_pebs);\r\nreserved_pebs = WL_RESERVED_PEBS;\r\n#ifdef CONFIG_MTD_UBI_FASTMAP\r\nreserved_pebs += (ubi->fm_size / ubi->leb_size) * 2;\r\n#endif\r\nif (ubi->avail_pebs < reserved_pebs) {\r\nubi_err("no enough physical eraseblocks (%d, need %d)",\r\nubi->avail_pebs, reserved_pebs);\r\nif (ubi->corr_peb_count)\r\nubi_err("%d PEBs are corrupted and not used",\r\nubi->corr_peb_count);\r\ngoto out_free;\r\n}\r\nubi->avail_pebs -= reserved_pebs;\r\nubi->rsvd_pebs += reserved_pebs;\r\nerr = ensure_wear_leveling(ubi, 0);\r\nif (err)\r\ngoto out_free;\r\nreturn 0;\r\nout_free:\r\ncancel_pending(ubi);\r\ntree_destroy(&ubi->used);\r\ntree_destroy(&ubi->free);\r\ntree_destroy(&ubi->scrub);\r\nkfree(ubi->lookuptbl);\r\nreturn err;\r\n}\r\nstatic void protection_queue_destroy(struct ubi_device *ubi)\r\n{\r\nint i;\r\nstruct ubi_wl_entry *e, *tmp;\r\nfor (i = 0; i < UBI_PROT_QUEUE_LEN; ++i) {\r\nlist_for_each_entry_safe(e, tmp, &ubi->pq[i], u.list) {\r\nlist_del(&e->u.list);\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\n}\r\n}\r\n}\r\nvoid ubi_wl_close(struct ubi_device *ubi)\r\n{\r\ndbg_wl("close the WL sub-system");\r\ncancel_pending(ubi);\r\nprotection_queue_destroy(ubi);\r\ntree_destroy(&ubi->used);\r\ntree_destroy(&ubi->erroneous);\r\ntree_destroy(&ubi->free);\r\ntree_destroy(&ubi->scrub);\r\nkfree(ubi->lookuptbl);\r\n}\r\nstatic int self_check_ec(struct ubi_device *ubi, int pnum, int ec)\r\n{\r\nint err;\r\nlong long read_ec;\r\nstruct ubi_ec_hdr *ec_hdr;\r\nif (!ubi_dbg_chk_gen(ubi))\r\nreturn 0;\r\nec_hdr = kzalloc(ubi->ec_hdr_alsize, GFP_NOFS);\r\nif (!ec_hdr)\r\nreturn -ENOMEM;\r\nerr = ubi_io_read_ec_hdr(ubi, pnum, ec_hdr, 0);\r\nif (err && err != UBI_IO_BITFLIPS) {\r\nerr = 0;\r\ngoto out_free;\r\n}\r\nread_ec = be64_to_cpu(ec_hdr->ec);\r\nif (ec != read_ec && read_ec - ec > 1) {\r\nubi_err("self-check failed for PEB %d", pnum);\r\nubi_err("read EC is %lld, should be %d", read_ec, ec);\r\ndump_stack();\r\nerr = 1;\r\n} else\r\nerr = 0;\r\nout_free:\r\nkfree(ec_hdr);\r\nreturn err;\r\n}\r\nstatic int self_check_in_wl_tree(const struct ubi_device *ubi,\r\nstruct ubi_wl_entry *e, struct rb_root *root)\r\n{\r\nif (!ubi_dbg_chk_gen(ubi))\r\nreturn 0;\r\nif (in_wl_tree(e, root))\r\nreturn 0;\r\nubi_err("self-check failed for PEB %d, EC %d, RB-tree %p ",\r\ne->pnum, e->ec, root);\r\ndump_stack();\r\nreturn -EINVAL;\r\n}\r\nstatic int self_check_in_pq(const struct ubi_device *ubi,\r\nstruct ubi_wl_entry *e)\r\n{\r\nstruct ubi_wl_entry *p;\r\nint i;\r\nif (!ubi_dbg_chk_gen(ubi))\r\nreturn 0;\r\nfor (i = 0; i < UBI_PROT_QUEUE_LEN; ++i)\r\nlist_for_each_entry(p, &ubi->pq[i], u.list)\r\nif (p == e)\r\nreturn 0;\r\nubi_err("self-check failed for PEB %d, EC %d, Protect queue",\r\ne->pnum, e->ec);\r\ndump_stack();\r\nreturn -EINVAL;\r\n}
