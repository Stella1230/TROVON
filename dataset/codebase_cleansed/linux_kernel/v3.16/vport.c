int ovs_vport_init(void)\r\n{\r\ndev_table = kzalloc(VPORT_HASH_BUCKETS * sizeof(struct hlist_head),\r\nGFP_KERNEL);\r\nif (!dev_table)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid ovs_vport_exit(void)\r\n{\r\nkfree(dev_table);\r\n}\r\nstatic struct hlist_head *hash_bucket(struct net *net, const char *name)\r\n{\r\nunsigned int hash = jhash(name, strlen(name), (unsigned long) net);\r\nreturn &dev_table[hash & (VPORT_HASH_BUCKETS - 1)];\r\n}\r\nstruct vport *ovs_vport_locate(struct net *net, const char *name)\r\n{\r\nstruct hlist_head *bucket = hash_bucket(net, name);\r\nstruct vport *vport;\r\nhlist_for_each_entry_rcu(vport, bucket, hash_node)\r\nif (!strcmp(name, vport->ops->get_name(vport)) &&\r\nnet_eq(ovs_dp_get_net(vport->dp), net))\r\nreturn vport;\r\nreturn NULL;\r\n}\r\nstruct vport *ovs_vport_alloc(int priv_size, const struct vport_ops *ops,\r\nconst struct vport_parms *parms)\r\n{\r\nstruct vport *vport;\r\nsize_t alloc_size;\r\nalloc_size = sizeof(struct vport);\r\nif (priv_size) {\r\nalloc_size = ALIGN(alloc_size, VPORT_ALIGN);\r\nalloc_size += priv_size;\r\n}\r\nvport = kzalloc(alloc_size, GFP_KERNEL);\r\nif (!vport)\r\nreturn ERR_PTR(-ENOMEM);\r\nvport->dp = parms->dp;\r\nvport->port_no = parms->port_no;\r\nvport->upcall_portid = parms->upcall_portid;\r\nvport->ops = ops;\r\nINIT_HLIST_NODE(&vport->dp_hash_node);\r\nvport->percpu_stats = netdev_alloc_pcpu_stats(struct pcpu_sw_netstats);\r\nif (!vport->percpu_stats) {\r\nkfree(vport);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nspin_lock_init(&vport->stats_lock);\r\nreturn vport;\r\n}\r\nvoid ovs_vport_free(struct vport *vport)\r\n{\r\nfree_percpu(vport->percpu_stats);\r\nkfree(vport);\r\n}\r\nstruct vport *ovs_vport_add(const struct vport_parms *parms)\r\n{\r\nstruct vport *vport;\r\nint err = 0;\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(vport_ops_list); i++) {\r\nif (vport_ops_list[i]->type == parms->type) {\r\nstruct hlist_head *bucket;\r\nvport = vport_ops_list[i]->create(parms);\r\nif (IS_ERR(vport)) {\r\nerr = PTR_ERR(vport);\r\ngoto out;\r\n}\r\nbucket = hash_bucket(ovs_dp_get_net(vport->dp),\r\nvport->ops->get_name(vport));\r\nhlist_add_head_rcu(&vport->hash_node, bucket);\r\nreturn vport;\r\n}\r\n}\r\nerr = -EAFNOSUPPORT;\r\nout:\r\nreturn ERR_PTR(err);\r\n}\r\nint ovs_vport_set_options(struct vport *vport, struct nlattr *options)\r\n{\r\nif (!vport->ops->set_options)\r\nreturn -EOPNOTSUPP;\r\nreturn vport->ops->set_options(vport, options);\r\n}\r\nvoid ovs_vport_del(struct vport *vport)\r\n{\r\nASSERT_OVSL();\r\nhlist_del_rcu(&vport->hash_node);\r\nvport->ops->destroy(vport);\r\n}\r\nvoid ovs_vport_get_stats(struct vport *vport, struct ovs_vport_stats *stats)\r\n{\r\nint i;\r\nmemset(stats, 0, sizeof(*stats));\r\nspin_lock_bh(&vport->stats_lock);\r\nstats->rx_errors = vport->err_stats.rx_errors;\r\nstats->tx_errors = vport->err_stats.tx_errors;\r\nstats->tx_dropped = vport->err_stats.tx_dropped;\r\nstats->rx_dropped = vport->err_stats.rx_dropped;\r\nspin_unlock_bh(&vport->stats_lock);\r\nfor_each_possible_cpu(i) {\r\nconst struct pcpu_sw_netstats *percpu_stats;\r\nstruct pcpu_sw_netstats local_stats;\r\nunsigned int start;\r\npercpu_stats = per_cpu_ptr(vport->percpu_stats, i);\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&percpu_stats->syncp);\r\nlocal_stats = *percpu_stats;\r\n} while (u64_stats_fetch_retry_irq(&percpu_stats->syncp, start));\r\nstats->rx_bytes += local_stats.rx_bytes;\r\nstats->rx_packets += local_stats.rx_packets;\r\nstats->tx_bytes += local_stats.tx_bytes;\r\nstats->tx_packets += local_stats.tx_packets;\r\n}\r\n}\r\nint ovs_vport_get_options(const struct vport *vport, struct sk_buff *skb)\r\n{\r\nstruct nlattr *nla;\r\nint err;\r\nif (!vport->ops->get_options)\r\nreturn 0;\r\nnla = nla_nest_start(skb, OVS_VPORT_ATTR_OPTIONS);\r\nif (!nla)\r\nreturn -EMSGSIZE;\r\nerr = vport->ops->get_options(vport, skb);\r\nif (err) {\r\nnla_nest_cancel(skb, nla);\r\nreturn err;\r\n}\r\nnla_nest_end(skb, nla);\r\nreturn 0;\r\n}\r\nvoid ovs_vport_receive(struct vport *vport, struct sk_buff *skb,\r\nstruct ovs_key_ipv4_tunnel *tun_key)\r\n{\r\nstruct pcpu_sw_netstats *stats;\r\nstats = this_cpu_ptr(vport->percpu_stats);\r\nu64_stats_update_begin(&stats->syncp);\r\nstats->rx_packets++;\r\nstats->rx_bytes += skb->len;\r\nu64_stats_update_end(&stats->syncp);\r\nOVS_CB(skb)->tun_key = tun_key;\r\novs_dp_process_received_packet(vport, skb);\r\n}\r\nint ovs_vport_send(struct vport *vport, struct sk_buff *skb)\r\n{\r\nint sent = vport->ops->send(vport, skb);\r\nif (likely(sent > 0)) {\r\nstruct pcpu_sw_netstats *stats;\r\nstats = this_cpu_ptr(vport->percpu_stats);\r\nu64_stats_update_begin(&stats->syncp);\r\nstats->tx_packets++;\r\nstats->tx_bytes += sent;\r\nu64_stats_update_end(&stats->syncp);\r\n} else if (sent < 0) {\r\novs_vport_record_error(vport, VPORT_E_TX_ERROR);\r\nkfree_skb(skb);\r\n} else\r\novs_vport_record_error(vport, VPORT_E_TX_DROPPED);\r\nreturn sent;\r\n}\r\nstatic void ovs_vport_record_error(struct vport *vport,\r\nenum vport_err_type err_type)\r\n{\r\nspin_lock(&vport->stats_lock);\r\nswitch (err_type) {\r\ncase VPORT_E_RX_DROPPED:\r\nvport->err_stats.rx_dropped++;\r\nbreak;\r\ncase VPORT_E_RX_ERROR:\r\nvport->err_stats.rx_errors++;\r\nbreak;\r\ncase VPORT_E_TX_DROPPED:\r\nvport->err_stats.tx_dropped++;\r\nbreak;\r\ncase VPORT_E_TX_ERROR:\r\nvport->err_stats.tx_errors++;\r\nbreak;\r\n}\r\nspin_unlock(&vport->stats_lock);\r\n}\r\nstatic void free_vport_rcu(struct rcu_head *rcu)\r\n{\r\nstruct vport *vport = container_of(rcu, struct vport, rcu);\r\novs_vport_free(vport);\r\n}\r\nvoid ovs_vport_deferred_free(struct vport *vport)\r\n{\r\nif (!vport)\r\nreturn;\r\ncall_rcu(&vport->rcu, free_vport_rcu);\r\n}
