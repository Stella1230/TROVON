static inline int mk_pid(struct pid_namespace *pid_ns,\r\nstruct pidmap *map, int off)\r\n{\r\nreturn (map - pid_ns->pidmap)*BITS_PER_PAGE + off;\r\n}\r\nstatic void free_pidmap(struct upid *upid)\r\n{\r\nint nr = upid->nr;\r\nstruct pidmap *map = upid->ns->pidmap + nr / BITS_PER_PAGE;\r\nint offset = nr & BITS_PER_PAGE_MASK;\r\nclear_bit(offset, map->page);\r\natomic_inc(&map->nr_free);\r\n}\r\nstatic int pid_before(int base, int a, int b)\r\n{\r\nreturn (unsigned)(a - base) < (unsigned)(b - base);\r\n}\r\nstatic void set_last_pid(struct pid_namespace *pid_ns, int base, int pid)\r\n{\r\nint prev;\r\nint last_write = base;\r\ndo {\r\nprev = last_write;\r\nlast_write = cmpxchg(&pid_ns->last_pid, prev, pid);\r\n} while ((prev != last_write) && (pid_before(base, last_write, pid)));\r\n}\r\nstatic int alloc_pidmap(struct pid_namespace *pid_ns)\r\n{\r\nint i, offset, max_scan, pid, last = pid_ns->last_pid;\r\nstruct pidmap *map;\r\npid = last + 1;\r\nif (pid >= pid_max)\r\npid = RESERVED_PIDS;\r\noffset = pid & BITS_PER_PAGE_MASK;\r\nmap = &pid_ns->pidmap[pid/BITS_PER_PAGE];\r\nmax_scan = DIV_ROUND_UP(pid_max, BITS_PER_PAGE) - !offset;\r\nfor (i = 0; i <= max_scan; ++i) {\r\nif (unlikely(!map->page)) {\r\nvoid *page = kzalloc(PAGE_SIZE, GFP_KERNEL);\r\nspin_lock_irq(&pidmap_lock);\r\nif (!map->page) {\r\nmap->page = page;\r\npage = NULL;\r\n}\r\nspin_unlock_irq(&pidmap_lock);\r\nkfree(page);\r\nif (unlikely(!map->page))\r\nbreak;\r\n}\r\nif (likely(atomic_read(&map->nr_free))) {\r\nfor ( ; ; ) {\r\nif (!test_and_set_bit(offset, map->page)) {\r\natomic_dec(&map->nr_free);\r\nset_last_pid(pid_ns, last, pid);\r\nreturn pid;\r\n}\r\noffset = find_next_offset(map, offset);\r\nif (offset >= BITS_PER_PAGE)\r\nbreak;\r\npid = mk_pid(pid_ns, map, offset);\r\nif (pid >= pid_max)\r\nbreak;\r\n}\r\n}\r\nif (map < &pid_ns->pidmap[(pid_max-1)/BITS_PER_PAGE]) {\r\n++map;\r\noffset = 0;\r\n} else {\r\nmap = &pid_ns->pidmap[0];\r\noffset = RESERVED_PIDS;\r\nif (unlikely(last == offset))\r\nbreak;\r\n}\r\npid = mk_pid(pid_ns, map, offset);\r\n}\r\nreturn -1;\r\n}\r\nint next_pidmap(struct pid_namespace *pid_ns, unsigned int last)\r\n{\r\nint offset;\r\nstruct pidmap *map, *end;\r\nif (last >= PID_MAX_LIMIT)\r\nreturn -1;\r\noffset = (last + 1) & BITS_PER_PAGE_MASK;\r\nmap = &pid_ns->pidmap[(last + 1)/BITS_PER_PAGE];\r\nend = &pid_ns->pidmap[PIDMAP_ENTRIES];\r\nfor (; map < end; map++, offset = 0) {\r\nif (unlikely(!map->page))\r\ncontinue;\r\noffset = find_next_bit((map)->page, BITS_PER_PAGE, offset);\r\nif (offset < BITS_PER_PAGE)\r\nreturn mk_pid(pid_ns, map, offset);\r\n}\r\nreturn -1;\r\n}\r\nvoid put_pid(struct pid *pid)\r\n{\r\nstruct pid_namespace *ns;\r\nif (!pid)\r\nreturn;\r\nns = pid->numbers[pid->level].ns;\r\nif ((atomic_read(&pid->count) == 1) ||\r\natomic_dec_and_test(&pid->count)) {\r\nkmem_cache_free(ns->pid_cachep, pid);\r\nput_pid_ns(ns);\r\n}\r\n}\r\nstatic void delayed_put_pid(struct rcu_head *rhp)\r\n{\r\nstruct pid *pid = container_of(rhp, struct pid, rcu);\r\nput_pid(pid);\r\n}\r\nvoid free_pid(struct pid *pid)\r\n{\r\nint i;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pidmap_lock, flags);\r\nfor (i = 0; i <= pid->level; i++) {\r\nstruct upid *upid = pid->numbers + i;\r\nstruct pid_namespace *ns = upid->ns;\r\nhlist_del_rcu(&upid->pid_chain);\r\nswitch(--ns->nr_hashed) {\r\ncase 2:\r\ncase 1:\r\nwake_up_process(ns->child_reaper);\r\nbreak;\r\ncase PIDNS_HASH_ADDING:\r\nWARN_ON(ns->child_reaper);\r\nns->nr_hashed = 0;\r\ncase 0:\r\nschedule_work(&ns->proc_work);\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&pidmap_lock, flags);\r\nfor (i = 0; i <= pid->level; i++)\r\nfree_pidmap(pid->numbers + i);\r\ncall_rcu(&pid->rcu, delayed_put_pid);\r\n}\r\nstruct pid *alloc_pid(struct pid_namespace *ns)\r\n{\r\nstruct pid *pid;\r\nenum pid_type type;\r\nint i, nr;\r\nstruct pid_namespace *tmp;\r\nstruct upid *upid;\r\npid = kmem_cache_alloc(ns->pid_cachep, GFP_KERNEL);\r\nif (!pid)\r\ngoto out;\r\ntmp = ns;\r\npid->level = ns->level;\r\nfor (i = ns->level; i >= 0; i--) {\r\nnr = alloc_pidmap(tmp);\r\nif (nr < 0)\r\ngoto out_free;\r\npid->numbers[i].nr = nr;\r\npid->numbers[i].ns = tmp;\r\ntmp = tmp->parent;\r\n}\r\nif (unlikely(is_child_reaper(pid))) {\r\nif (pid_ns_prepare_proc(ns))\r\ngoto out_free;\r\n}\r\nget_pid_ns(ns);\r\natomic_set(&pid->count, 1);\r\nfor (type = 0; type < PIDTYPE_MAX; ++type)\r\nINIT_HLIST_HEAD(&pid->tasks[type]);\r\nupid = pid->numbers + ns->level;\r\nspin_lock_irq(&pidmap_lock);\r\nif (!(ns->nr_hashed & PIDNS_HASH_ADDING))\r\ngoto out_unlock;\r\nfor ( ; upid >= pid->numbers; --upid) {\r\nhlist_add_head_rcu(&upid->pid_chain,\r\n&pid_hash[pid_hashfn(upid->nr, upid->ns)]);\r\nupid->ns->nr_hashed++;\r\n}\r\nspin_unlock_irq(&pidmap_lock);\r\nout:\r\nreturn pid;\r\nout_unlock:\r\nspin_unlock_irq(&pidmap_lock);\r\nout_free:\r\nwhile (++i <= ns->level)\r\nfree_pidmap(pid->numbers + i);\r\nkmem_cache_free(ns->pid_cachep, pid);\r\npid = NULL;\r\ngoto out;\r\n}\r\nvoid disable_pid_allocation(struct pid_namespace *ns)\r\n{\r\nspin_lock_irq(&pidmap_lock);\r\nns->nr_hashed &= ~PIDNS_HASH_ADDING;\r\nspin_unlock_irq(&pidmap_lock);\r\n}\r\nstruct pid *find_pid_ns(int nr, struct pid_namespace *ns)\r\n{\r\nstruct upid *pnr;\r\nhlist_for_each_entry_rcu(pnr,\r\n&pid_hash[pid_hashfn(nr, ns)], pid_chain)\r\nif (pnr->nr == nr && pnr->ns == ns)\r\nreturn container_of(pnr, struct pid,\r\nnumbers[ns->level]);\r\nreturn NULL;\r\n}\r\nstruct pid *find_vpid(int nr)\r\n{\r\nreturn find_pid_ns(nr, task_active_pid_ns(current));\r\n}\r\nvoid attach_pid(struct task_struct *task, enum pid_type type)\r\n{\r\nstruct pid_link *link = &task->pids[type];\r\nhlist_add_head_rcu(&link->node, &link->pid->tasks[type]);\r\n}\r\nstatic void __change_pid(struct task_struct *task, enum pid_type type,\r\nstruct pid *new)\r\n{\r\nstruct pid_link *link;\r\nstruct pid *pid;\r\nint tmp;\r\nlink = &task->pids[type];\r\npid = link->pid;\r\nhlist_del_rcu(&link->node);\r\nlink->pid = new;\r\nfor (tmp = PIDTYPE_MAX; --tmp >= 0; )\r\nif (!hlist_empty(&pid->tasks[tmp]))\r\nreturn;\r\nfree_pid(pid);\r\n}\r\nvoid detach_pid(struct task_struct *task, enum pid_type type)\r\n{\r\n__change_pid(task, type, NULL);\r\n}\r\nvoid change_pid(struct task_struct *task, enum pid_type type,\r\nstruct pid *pid)\r\n{\r\n__change_pid(task, type, pid);\r\nattach_pid(task, type);\r\n}\r\nvoid transfer_pid(struct task_struct *old, struct task_struct *new,\r\nenum pid_type type)\r\n{\r\nnew->pids[type].pid = old->pids[type].pid;\r\nhlist_replace_rcu(&old->pids[type].node, &new->pids[type].node);\r\n}\r\nstruct task_struct *pid_task(struct pid *pid, enum pid_type type)\r\n{\r\nstruct task_struct *result = NULL;\r\nif (pid) {\r\nstruct hlist_node *first;\r\nfirst = rcu_dereference_check(hlist_first_rcu(&pid->tasks[type]),\r\nlockdep_tasklist_lock_is_held());\r\nif (first)\r\nresult = hlist_entry(first, struct task_struct, pids[(type)].node);\r\n}\r\nreturn result;\r\n}\r\nstruct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns)\r\n{\r\nrcu_lockdep_assert(rcu_read_lock_held(),\r\n"find_task_by_pid_ns() needs rcu_read_lock()"\r\n" protection");\r\nreturn pid_task(find_pid_ns(nr, ns), PIDTYPE_PID);\r\n}\r\nstruct task_struct *find_task_by_vpid(pid_t vnr)\r\n{\r\nreturn find_task_by_pid_ns(vnr, task_active_pid_ns(current));\r\n}\r\nstruct pid *get_task_pid(struct task_struct *task, enum pid_type type)\r\n{\r\nstruct pid *pid;\r\nrcu_read_lock();\r\nif (type != PIDTYPE_PID)\r\ntask = task->group_leader;\r\npid = get_pid(task->pids[type].pid);\r\nrcu_read_unlock();\r\nreturn pid;\r\n}\r\nstruct task_struct *get_pid_task(struct pid *pid, enum pid_type type)\r\n{\r\nstruct task_struct *result;\r\nrcu_read_lock();\r\nresult = pid_task(pid, type);\r\nif (result)\r\nget_task_struct(result);\r\nrcu_read_unlock();\r\nreturn result;\r\n}\r\nstruct pid *find_get_pid(pid_t nr)\r\n{\r\nstruct pid *pid;\r\nrcu_read_lock();\r\npid = get_pid(find_vpid(nr));\r\nrcu_read_unlock();\r\nreturn pid;\r\n}\r\npid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns)\r\n{\r\nstruct upid *upid;\r\npid_t nr = 0;\r\nif (pid && ns->level <= pid->level) {\r\nupid = &pid->numbers[ns->level];\r\nif (upid->ns == ns)\r\nnr = upid->nr;\r\n}\r\nreturn nr;\r\n}\r\npid_t pid_vnr(struct pid *pid)\r\n{\r\nreturn pid_nr_ns(pid, task_active_pid_ns(current));\r\n}\r\npid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,\r\nstruct pid_namespace *ns)\r\n{\r\npid_t nr = 0;\r\nrcu_read_lock();\r\nif (!ns)\r\nns = task_active_pid_ns(current);\r\nif (likely(pid_alive(task))) {\r\nif (type != PIDTYPE_PID)\r\ntask = task->group_leader;\r\nnr = pid_nr_ns(task->pids[type].pid, ns);\r\n}\r\nrcu_read_unlock();\r\nreturn nr;\r\n}\r\npid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\r\n{\r\nreturn pid_nr_ns(task_tgid(tsk), ns);\r\n}\r\nstruct pid_namespace *task_active_pid_ns(struct task_struct *tsk)\r\n{\r\nreturn ns_of_pid(task_pid(tsk));\r\n}\r\nstruct pid *find_ge_pid(int nr, struct pid_namespace *ns)\r\n{\r\nstruct pid *pid;\r\ndo {\r\npid = find_pid_ns(nr, ns);\r\nif (pid)\r\nbreak;\r\nnr = next_pidmap(ns, nr);\r\n} while (nr > 0);\r\nreturn pid;\r\n}\r\nvoid __init pidhash_init(void)\r\n{\r\nunsigned int i, pidhash_size;\r\npid_hash = alloc_large_system_hash("PID", sizeof(*pid_hash), 0, 18,\r\nHASH_EARLY | HASH_SMALL,\r\n&pidhash_shift, NULL,\r\n0, 4096);\r\npidhash_size = 1U << pidhash_shift;\r\nfor (i = 0; i < pidhash_size; i++)\r\nINIT_HLIST_HEAD(&pid_hash[i]);\r\n}\r\nvoid __init pidmap_init(void)\r\n{\r\nBUILD_BUG_ON(PID_MAX_LIMIT >= PIDNS_HASH_ADDING);\r\npid_max = min(pid_max_max, max_t(int, pid_max,\r\nPIDS_PER_CPU_DEFAULT * num_possible_cpus()));\r\npid_max_min = max_t(int, pid_max_min,\r\nPIDS_PER_CPU_MIN * num_possible_cpus());\r\npr_info("pid_max: default: %u minimum: %u\n", pid_max, pid_max_min);\r\ninit_pid_ns.pidmap[0].page = kzalloc(PAGE_SIZE, GFP_KERNEL);\r\nset_bit(0, init_pid_ns.pidmap[0].page);\r\natomic_dec(&init_pid_ns.pidmap[0].nr_free);\r\ninit_pid_ns.pid_cachep = KMEM_CACHE(pid,\r\nSLAB_HWCACHE_ALIGN | SLAB_PANIC);\r\n}
