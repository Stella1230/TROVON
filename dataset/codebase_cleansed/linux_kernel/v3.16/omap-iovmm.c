static unsigned int sgtable_offset(const struct sg_table *sgt)\r\n{\r\nif (!sgt || !sgt->nents)\r\nreturn 0;\r\nreturn sgt->sgl->offset;\r\n}\r\nstatic size_t sgtable_len(const struct sg_table *sgt)\r\n{\r\nunsigned int i, total = 0;\r\nstruct scatterlist *sg;\r\nif (!sgt)\r\nreturn 0;\r\nfor_each_sg(sgt->sgl, sg, sgt->nents, i) {\r\nsize_t bytes;\r\nbytes = sg->length + sg->offset;\r\nif (!iopgsz_ok(bytes)) {\r\npr_err("%s: sg[%d] not iommu pagesize(%u %u)\n",\r\n__func__, i, bytes, sg->offset);\r\nreturn 0;\r\n}\r\nif (i && sg->offset) {\r\npr_err("%s: sg[%d] offset not allowed in internal entries\n",\r\n__func__, i);\r\nreturn 0;\r\n}\r\ntotal += bytes;\r\n}\r\nreturn total;\r\n}\r\nstatic unsigned max_alignment(u32 addr)\r\n{\r\nint i;\r\nunsigned pagesize[] = { SZ_16M, SZ_1M, SZ_64K, SZ_4K, };\r\nfor (i = 0; i < ARRAY_SIZE(pagesize) && addr & (pagesize[i] - 1); i++)\r\n;\r\nreturn (i < ARRAY_SIZE(pagesize)) ? pagesize[i] : 0;\r\n}\r\nstatic unsigned sgtable_nents(size_t bytes, u32 da, u32 pa)\r\n{\r\nunsigned nr_entries = 0, ent_sz;\r\nif (!IS_ALIGNED(bytes, PAGE_SIZE)) {\r\npr_err("%s: wrong size %08x\n", __func__, bytes);\r\nreturn 0;\r\n}\r\nwhile (bytes) {\r\nent_sz = max_alignment(da | pa);\r\nent_sz = min_t(unsigned, ent_sz, iopgsz_max(bytes));\r\nnr_entries++;\r\nda += ent_sz;\r\npa += ent_sz;\r\nbytes -= ent_sz;\r\n}\r\nreturn nr_entries;\r\n}\r\nstatic struct sg_table *sgtable_alloc(const size_t bytes, u32 flags,\r\nu32 da, u32 pa)\r\n{\r\nunsigned int nr_entries;\r\nint err;\r\nstruct sg_table *sgt;\r\nif (!bytes)\r\nreturn ERR_PTR(-EINVAL);\r\nif (!IS_ALIGNED(bytes, PAGE_SIZE))\r\nreturn ERR_PTR(-EINVAL);\r\nif (flags & IOVMF_LINEAR) {\r\nnr_entries = sgtable_nents(bytes, da, pa);\r\nif (!nr_entries)\r\nreturn ERR_PTR(-EINVAL);\r\n} else\r\nnr_entries = bytes / PAGE_SIZE;\r\nsgt = kzalloc(sizeof(*sgt), GFP_KERNEL);\r\nif (!sgt)\r\nreturn ERR_PTR(-ENOMEM);\r\nerr = sg_alloc_table(sgt, nr_entries, GFP_KERNEL);\r\nif (err) {\r\nkfree(sgt);\r\nreturn ERR_PTR(err);\r\n}\r\npr_debug("%s: sgt:%p(%d entries)\n", __func__, sgt, nr_entries);\r\nreturn sgt;\r\n}\r\nstatic void sgtable_free(struct sg_table *sgt)\r\n{\r\nif (!sgt)\r\nreturn;\r\nsg_free_table(sgt);\r\nkfree(sgt);\r\npr_debug("%s: sgt:%p\n", __func__, sgt);\r\n}\r\nstatic void *vmap_sg(const struct sg_table *sgt)\r\n{\r\nu32 va;\r\nsize_t total;\r\nunsigned int i;\r\nstruct scatterlist *sg;\r\nstruct vm_struct *new;\r\nconst struct mem_type *mtype;\r\nmtype = get_mem_type(MT_DEVICE);\r\nif (!mtype)\r\nreturn ERR_PTR(-EINVAL);\r\ntotal = sgtable_len(sgt);\r\nif (!total)\r\nreturn ERR_PTR(-EINVAL);\r\nnew = __get_vm_area(total, VM_IOREMAP, VMALLOC_START, VMALLOC_END);\r\nif (!new)\r\nreturn ERR_PTR(-ENOMEM);\r\nva = (u32)new->addr;\r\nfor_each_sg(sgt->sgl, sg, sgt->nents, i) {\r\nsize_t bytes;\r\nu32 pa;\r\nint err;\r\npa = sg_phys(sg) - sg->offset;\r\nbytes = sg->length + sg->offset;\r\nBUG_ON(bytes != PAGE_SIZE);\r\nerr = ioremap_page(va, pa, mtype);\r\nif (err)\r\ngoto err_out;\r\nva += bytes;\r\n}\r\nflush_cache_vmap((unsigned long)new->addr,\r\n(unsigned long)(new->addr + total));\r\nreturn new->addr;\r\nerr_out:\r\nWARN_ON(1);\r\nvunmap(new->addr);\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nstatic inline void vunmap_sg(const void *va)\r\n{\r\nvunmap(va);\r\n}\r\nstatic struct iovm_struct *__find_iovm_area(struct omap_iommu *obj,\r\nconst u32 da)\r\n{\r\nstruct iovm_struct *tmp;\r\nlist_for_each_entry(tmp, &obj->mmap, list) {\r\nif ((da >= tmp->da_start) && (da < tmp->da_end)) {\r\nsize_t len;\r\nlen = tmp->da_end - tmp->da_start;\r\ndev_dbg(obj->dev, "%s: %08x-%08x-%08x(%x) %08x\n",\r\n__func__, tmp->da_start, da, tmp->da_end, len,\r\ntmp->flags);\r\nreturn tmp;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstruct iovm_struct *omap_find_iovm_area(struct device *dev, u32 da)\r\n{\r\nstruct omap_iommu *obj = dev_to_omap_iommu(dev);\r\nstruct iovm_struct *area;\r\nmutex_lock(&obj->mmap_lock);\r\narea = __find_iovm_area(obj, da);\r\nmutex_unlock(&obj->mmap_lock);\r\nreturn area;\r\n}\r\nstatic struct iovm_struct *alloc_iovm_area(struct omap_iommu *obj, u32 da,\r\nsize_t bytes, u32 flags)\r\n{\r\nstruct iovm_struct *new, *tmp;\r\nu32 start, prev_end, alignment;\r\nif (!obj || !bytes)\r\nreturn ERR_PTR(-EINVAL);\r\nstart = da;\r\nalignment = PAGE_SIZE;\r\nif (~flags & IOVMF_DA_FIXED) {\r\nstart = obj->da_start ? obj->da_start : alignment;\r\nif (flags & IOVMF_LINEAR)\r\nalignment = iopgsz_max(bytes);\r\nstart = roundup(start, alignment);\r\n} else if (start < obj->da_start || start > obj->da_end ||\r\nobj->da_end - start < bytes) {\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\ntmp = NULL;\r\nif (list_empty(&obj->mmap))\r\ngoto found;\r\nprev_end = 0;\r\nlist_for_each_entry(tmp, &obj->mmap, list) {\r\nif (prev_end > start)\r\nbreak;\r\nif (tmp->da_start > start && (tmp->da_start - start) >= bytes)\r\ngoto found;\r\nif (tmp->da_end >= start && ~flags & IOVMF_DA_FIXED)\r\nstart = roundup(tmp->da_end + 1, alignment);\r\nprev_end = tmp->da_end;\r\n}\r\nif ((start >= prev_end) && (obj->da_end - start >= bytes))\r\ngoto found;\r\ndev_dbg(obj->dev, "%s: no space to fit %08x(%x) flags: %08x\n",\r\n__func__, da, bytes, flags);\r\nreturn ERR_PTR(-EINVAL);\r\nfound:\r\nnew = kmem_cache_zalloc(iovm_area_cachep, GFP_KERNEL);\r\nif (!new)\r\nreturn ERR_PTR(-ENOMEM);\r\nnew->iommu = obj;\r\nnew->da_start = start;\r\nnew->da_end = start + bytes;\r\nnew->flags = flags;\r\nif (tmp)\r\nlist_add_tail(&new->list, &tmp->list);\r\nelse\r\nlist_add(&new->list, &obj->mmap);\r\ndev_dbg(obj->dev, "%s: found %08x-%08x-%08x(%x) %08x\n",\r\n__func__, new->da_start, start, new->da_end, bytes, flags);\r\nreturn new;\r\n}\r\nstatic void free_iovm_area(struct omap_iommu *obj, struct iovm_struct *area)\r\n{\r\nsize_t bytes;\r\nBUG_ON(!obj || !area);\r\nbytes = area->da_end - area->da_start;\r\ndev_dbg(obj->dev, "%s: %08x-%08x(%x) %08x\n",\r\n__func__, area->da_start, area->da_end, bytes, area->flags);\r\nlist_del(&area->list);\r\nkmem_cache_free(iovm_area_cachep, area);\r\n}\r\nvoid *omap_da_to_va(struct device *dev, u32 da)\r\n{\r\nstruct omap_iommu *obj = dev_to_omap_iommu(dev);\r\nvoid *va = NULL;\r\nstruct iovm_struct *area;\r\nmutex_lock(&obj->mmap_lock);\r\narea = __find_iovm_area(obj, da);\r\nif (!area) {\r\ndev_dbg(obj->dev, "%s: no da area(%08x)\n", __func__, da);\r\ngoto out;\r\n}\r\nva = area->va;\r\nout:\r\nmutex_unlock(&obj->mmap_lock);\r\nreturn va;\r\n}\r\nstatic void sgtable_fill_vmalloc(struct sg_table *sgt, void *_va)\r\n{\r\nunsigned int i;\r\nstruct scatterlist *sg;\r\nvoid *va = _va;\r\nvoid *va_end;\r\nfor_each_sg(sgt->sgl, sg, sgt->nents, i) {\r\nstruct page *pg;\r\nconst size_t bytes = PAGE_SIZE;\r\npg = vmalloc_to_page(va);\r\nBUG_ON(!pg);\r\nsg_set_page(sg, pg, bytes, 0);\r\nva += bytes;\r\n}\r\nva_end = _va + PAGE_SIZE * i;\r\n}\r\nstatic inline void sgtable_drain_vmalloc(struct sg_table *sgt)\r\n{\r\nBUG_ON(!sgt);\r\n}\r\nstatic int map_iovm_area(struct iommu_domain *domain, struct iovm_struct *new,\r\nconst struct sg_table *sgt, u32 flags)\r\n{\r\nint err;\r\nunsigned int i, j;\r\nstruct scatterlist *sg;\r\nu32 da = new->da_start;\r\nif (!domain || !sgt)\r\nreturn -EINVAL;\r\nBUG_ON(!sgtable_ok(sgt));\r\nfor_each_sg(sgt->sgl, sg, sgt->nents, i) {\r\nu32 pa;\r\nsize_t bytes;\r\npa = sg_phys(sg) - sg->offset;\r\nbytes = sg->length + sg->offset;\r\nflags &= ~IOVMF_PGSZ_MASK;\r\nif (bytes_to_iopgsz(bytes) < 0)\r\ngoto err_out;\r\npr_debug("%s: [%d] %08x %08x(%x)\n", __func__,\r\ni, da, pa, bytes);\r\nerr = iommu_map(domain, da, pa, bytes, flags);\r\nif (err)\r\ngoto err_out;\r\nda += bytes;\r\n}\r\nreturn 0;\r\nerr_out:\r\nda = new->da_start;\r\nfor_each_sg(sgt->sgl, sg, i, j) {\r\nsize_t bytes;\r\nbytes = sg->length + sg->offset;\r\niommu_unmap(domain, da, bytes);\r\nda += bytes;\r\n}\r\nreturn err;\r\n}\r\nstatic void unmap_iovm_area(struct iommu_domain *domain, struct omap_iommu *obj,\r\nstruct iovm_struct *area)\r\n{\r\nu32 start;\r\nsize_t total = area->da_end - area->da_start;\r\nconst struct sg_table *sgt = area->sgt;\r\nstruct scatterlist *sg;\r\nint i;\r\nsize_t unmapped;\r\nBUG_ON(!sgtable_ok(sgt));\r\nBUG_ON((!total) || !IS_ALIGNED(total, PAGE_SIZE));\r\nstart = area->da_start;\r\nfor_each_sg(sgt->sgl, sg, sgt->nents, i) {\r\nsize_t bytes;\r\nbytes = sg->length + sg->offset;\r\nunmapped = iommu_unmap(domain, start, bytes);\r\nif (unmapped < bytes)\r\nbreak;\r\ndev_dbg(obj->dev, "%s: unmap %08x(%x) %08x\n",\r\n__func__, start, bytes, area->flags);\r\nBUG_ON(!IS_ALIGNED(bytes, PAGE_SIZE));\r\ntotal -= bytes;\r\nstart += bytes;\r\n}\r\nBUG_ON(total);\r\n}\r\nstatic struct sg_table *unmap_vm_area(struct iommu_domain *domain,\r\nstruct omap_iommu *obj, const u32 da,\r\nvoid (*fn)(const void *), u32 flags)\r\n{\r\nstruct sg_table *sgt = NULL;\r\nstruct iovm_struct *area;\r\nif (!IS_ALIGNED(da, PAGE_SIZE)) {\r\ndev_err(obj->dev, "%s: alignment err(%08x)\n", __func__, da);\r\nreturn NULL;\r\n}\r\nmutex_lock(&obj->mmap_lock);\r\narea = __find_iovm_area(obj, da);\r\nif (!area) {\r\ndev_dbg(obj->dev, "%s: no da area(%08x)\n", __func__, da);\r\ngoto out;\r\n}\r\nif ((area->flags & flags) != flags) {\r\ndev_err(obj->dev, "%s: wrong flags(%08x)\n", __func__,\r\narea->flags);\r\ngoto out;\r\n}\r\nsgt = (struct sg_table *)area->sgt;\r\nunmap_iovm_area(domain, obj, area);\r\nfn(area->va);\r\ndev_dbg(obj->dev, "%s: %08x-%08x-%08x(%x) %08x\n", __func__,\r\narea->da_start, da, area->da_end,\r\narea->da_end - area->da_start, area->flags);\r\nfree_iovm_area(obj, area);\r\nout:\r\nmutex_unlock(&obj->mmap_lock);\r\nreturn sgt;\r\n}\r\nstatic u32 map_iommu_region(struct iommu_domain *domain, struct omap_iommu *obj,\r\nu32 da, const struct sg_table *sgt, void *va,\r\nsize_t bytes, u32 flags)\r\n{\r\nint err = -ENOMEM;\r\nstruct iovm_struct *new;\r\nmutex_lock(&obj->mmap_lock);\r\nnew = alloc_iovm_area(obj, da, bytes, flags);\r\nif (IS_ERR(new)) {\r\nerr = PTR_ERR(new);\r\ngoto err_alloc_iovma;\r\n}\r\nnew->va = va;\r\nnew->sgt = sgt;\r\nif (map_iovm_area(domain, new, sgt, new->flags))\r\ngoto err_map;\r\nmutex_unlock(&obj->mmap_lock);\r\ndev_dbg(obj->dev, "%s: da:%08x(%x) flags:%08x va:%p\n",\r\n__func__, new->da_start, bytes, new->flags, va);\r\nreturn new->da_start;\r\nerr_map:\r\nfree_iovm_area(obj, new);\r\nerr_alloc_iovma:\r\nmutex_unlock(&obj->mmap_lock);\r\nreturn err;\r\n}\r\nstatic inline u32\r\n__iommu_vmap(struct iommu_domain *domain, struct omap_iommu *obj,\r\nu32 da, const struct sg_table *sgt,\r\nvoid *va, size_t bytes, u32 flags)\r\n{\r\nreturn map_iommu_region(domain, obj, da, sgt, va, bytes, flags);\r\n}\r\nu32 omap_iommu_vmap(struct iommu_domain *domain, struct device *dev, u32 da,\r\nconst struct sg_table *sgt, u32 flags)\r\n{\r\nstruct omap_iommu *obj = dev_to_omap_iommu(dev);\r\nsize_t bytes;\r\nvoid *va = NULL;\r\nif (!obj || !obj->dev || !sgt)\r\nreturn -EINVAL;\r\nbytes = sgtable_len(sgt);\r\nif (!bytes)\r\nreturn -EINVAL;\r\nbytes = PAGE_ALIGN(bytes);\r\nif (flags & IOVMF_MMIO) {\r\nva = vmap_sg(sgt);\r\nif (IS_ERR(va))\r\nreturn PTR_ERR(va);\r\n}\r\nflags |= IOVMF_DISCONT;\r\nflags |= IOVMF_MMIO;\r\nda = __iommu_vmap(domain, obj, da, sgt, va, bytes, flags);\r\nif (IS_ERR_VALUE(da))\r\nvunmap_sg(va);\r\nreturn da + sgtable_offset(sgt);\r\n}\r\nstruct sg_table *\r\nomap_iommu_vunmap(struct iommu_domain *domain, struct device *dev, u32 da)\r\n{\r\nstruct omap_iommu *obj = dev_to_omap_iommu(dev);\r\nstruct sg_table *sgt;\r\nda &= PAGE_MASK;\r\nsgt = unmap_vm_area(domain, obj, da, vunmap_sg,\r\nIOVMF_DISCONT | IOVMF_MMIO);\r\nif (!sgt)\r\ndev_dbg(obj->dev, "%s: No sgt\n", __func__);\r\nreturn sgt;\r\n}\r\nu32\r\nomap_iommu_vmalloc(struct iommu_domain *domain, struct device *dev, u32 da,\r\nsize_t bytes, u32 flags)\r\n{\r\nstruct omap_iommu *obj = dev_to_omap_iommu(dev);\r\nvoid *va;\r\nstruct sg_table *sgt;\r\nif (!obj || !obj->dev || !bytes)\r\nreturn -EINVAL;\r\nbytes = PAGE_ALIGN(bytes);\r\nva = vmalloc(bytes);\r\nif (!va)\r\nreturn -ENOMEM;\r\nflags |= IOVMF_DISCONT;\r\nflags |= IOVMF_ALLOC;\r\nsgt = sgtable_alloc(bytes, flags, da, 0);\r\nif (IS_ERR(sgt)) {\r\nda = PTR_ERR(sgt);\r\ngoto err_sgt_alloc;\r\n}\r\nsgtable_fill_vmalloc(sgt, va);\r\nda = __iommu_vmap(domain, obj, da, sgt, va, bytes, flags);\r\nif (IS_ERR_VALUE(da))\r\ngoto err_iommu_vmap;\r\nreturn da;\r\nerr_iommu_vmap:\r\nsgtable_drain_vmalloc(sgt);\r\nsgtable_free(sgt);\r\nerr_sgt_alloc:\r\nvfree(va);\r\nreturn da;\r\n}\r\nvoid omap_iommu_vfree(struct iommu_domain *domain, struct device *dev,\r\nconst u32 da)\r\n{\r\nstruct omap_iommu *obj = dev_to_omap_iommu(dev);\r\nstruct sg_table *sgt;\r\nsgt = unmap_vm_area(domain, obj, da, vfree,\r\nIOVMF_DISCONT | IOVMF_ALLOC);\r\nif (!sgt)\r\ndev_dbg(obj->dev, "%s: No sgt\n", __func__);\r\nsgtable_free(sgt);\r\n}\r\nstatic int __init iovmm_init(void)\r\n{\r\nconst unsigned long flags = SLAB_HWCACHE_ALIGN;\r\nstruct kmem_cache *p;\r\np = kmem_cache_create("iovm_area_cache", sizeof(struct iovm_struct), 0,\r\nflags, NULL);\r\nif (!p)\r\nreturn -ENOMEM;\r\niovm_area_cachep = p;\r\nreturn 0;\r\n}\r\nstatic void __exit iovmm_exit(void)\r\n{\r\nkmem_cache_destroy(iovm_area_cachep);\r\n}
