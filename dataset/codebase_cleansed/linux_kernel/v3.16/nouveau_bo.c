static void\r\nnv10_bo_update_tile_region(struct drm_device *dev, struct nouveau_drm_tile *reg,\r\nu32 addr, u32 size, u32 pitch, u32 flags)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nint i = reg - drm->tile.reg;\r\nstruct nouveau_fb *pfb = nouveau_fb(drm->device);\r\nstruct nouveau_fb_tile *tile = &pfb->tile.region[i];\r\nstruct nouveau_engine *engine;\r\nnouveau_fence_unref(&reg->fence);\r\nif (tile->pitch)\r\npfb->tile.fini(pfb, i, tile);\r\nif (pitch)\r\npfb->tile.init(pfb, i, addr, size, pitch, flags, tile);\r\npfb->tile.prog(pfb, i, tile);\r\nif ((engine = nouveau_engine(pfb, NVDEV_ENGINE_GR)))\r\nengine->tile_prog(engine, i);\r\nif ((engine = nouveau_engine(pfb, NVDEV_ENGINE_MPEG)))\r\nengine->tile_prog(engine, i);\r\n}\r\nstatic struct nouveau_drm_tile *\r\nnv10_bo_get_tile_region(struct drm_device *dev, int i)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nstruct nouveau_drm_tile *tile = &drm->tile.reg[i];\r\nspin_lock(&drm->tile.lock);\r\nif (!tile->used &&\r\n(!tile->fence || nouveau_fence_done(tile->fence)))\r\ntile->used = true;\r\nelse\r\ntile = NULL;\r\nspin_unlock(&drm->tile.lock);\r\nreturn tile;\r\n}\r\nstatic void\r\nnv10_bo_put_tile_region(struct drm_device *dev, struct nouveau_drm_tile *tile,\r\nstruct nouveau_fence *fence)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nif (tile) {\r\nspin_lock(&drm->tile.lock);\r\ntile->fence = nouveau_fence_ref(fence);\r\ntile->used = false;\r\nspin_unlock(&drm->tile.lock);\r\n}\r\n}\r\nstatic struct nouveau_drm_tile *\r\nnv10_bo_set_tiling(struct drm_device *dev, u32 addr,\r\nu32 size, u32 pitch, u32 flags)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nstruct nouveau_fb *pfb = nouveau_fb(drm->device);\r\nstruct nouveau_drm_tile *tile, *found = NULL;\r\nint i;\r\nfor (i = 0; i < pfb->tile.regions; i++) {\r\ntile = nv10_bo_get_tile_region(dev, i);\r\nif (pitch && !found) {\r\nfound = tile;\r\ncontinue;\r\n} else if (tile && pfb->tile.region[i].pitch) {\r\nnv10_bo_update_tile_region(dev, tile, 0, 0, 0, 0);\r\n}\r\nnv10_bo_put_tile_region(dev, tile, NULL);\r\n}\r\nif (found)\r\nnv10_bo_update_tile_region(dev, found, addr, size,\r\npitch, flags);\r\nreturn found;\r\n}\r\nstatic void\r\nnouveau_bo_del_ttm(struct ttm_buffer_object *bo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct drm_device *dev = drm->dev;\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nif (unlikely(nvbo->gem.filp))\r\nDRM_ERROR("bo %p still attached to GEM object\n", bo);\r\nWARN_ON(nvbo->pin_refcnt > 0);\r\nnv10_bo_put_tile_region(dev, nvbo->tile, NULL);\r\nkfree(nvbo);\r\n}\r\nstatic void\r\nnouveau_bo_fixup_align(struct nouveau_bo *nvbo, u32 flags,\r\nint *align, int *size)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nstruct nouveau_device *device = nv_device(drm->device);\r\nif (device->card_type < NV_50) {\r\nif (nvbo->tile_mode) {\r\nif (device->chipset >= 0x40) {\r\n*align = 65536;\r\n*size = roundup(*size, 64 * nvbo->tile_mode);\r\n} else if (device->chipset >= 0x30) {\r\n*align = 32768;\r\n*size = roundup(*size, 64 * nvbo->tile_mode);\r\n} else if (device->chipset >= 0x20) {\r\n*align = 16384;\r\n*size = roundup(*size, 64 * nvbo->tile_mode);\r\n} else if (device->chipset >= 0x10) {\r\n*align = 16384;\r\n*size = roundup(*size, 32 * nvbo->tile_mode);\r\n}\r\n}\r\n} else {\r\n*size = roundup(*size, (1 << nvbo->page_shift));\r\n*align = max((1 << nvbo->page_shift), *align);\r\n}\r\n*size = roundup(*size, PAGE_SIZE);\r\n}\r\nint\r\nnouveau_bo_new(struct drm_device *dev, int size, int align,\r\nuint32_t flags, uint32_t tile_mode, uint32_t tile_flags,\r\nstruct sg_table *sg,\r\nstruct nouveau_bo **pnvbo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_drm(dev);\r\nstruct nouveau_bo *nvbo;\r\nsize_t acc_size;\r\nint ret;\r\nint type = ttm_bo_type_device;\r\nint lpg_shift = 12;\r\nint max_size;\r\nif (drm->client.base.vm)\r\nlpg_shift = drm->client.base.vm->vmm->lpg_shift;\r\nmax_size = INT_MAX & ~((1 << lpg_shift) - 1);\r\nif (size <= 0 || size > max_size) {\r\nnv_warn(drm, "skipped size %x\n", (u32)size);\r\nreturn -EINVAL;\r\n}\r\nif (sg)\r\ntype = ttm_bo_type_sg;\r\nnvbo = kzalloc(sizeof(struct nouveau_bo), GFP_KERNEL);\r\nif (!nvbo)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&nvbo->head);\r\nINIT_LIST_HEAD(&nvbo->entry);\r\nINIT_LIST_HEAD(&nvbo->vma_list);\r\nnvbo->tile_mode = tile_mode;\r\nnvbo->tile_flags = tile_flags;\r\nnvbo->bo.bdev = &drm->ttm.bdev;\r\nnvbo->page_shift = 12;\r\nif (drm->client.base.vm) {\r\nif (!(flags & TTM_PL_FLAG_TT) && size > 256 * 1024)\r\nnvbo->page_shift = drm->client.base.vm->vmm->lpg_shift;\r\n}\r\nnouveau_bo_fixup_align(nvbo, flags, &align, &size);\r\nnvbo->bo.mem.num_pages = size >> PAGE_SHIFT;\r\nnouveau_bo_placement_set(nvbo, flags, 0);\r\nacc_size = ttm_bo_dma_acc_size(&drm->ttm.bdev, size,\r\nsizeof(struct nouveau_bo));\r\nret = ttm_bo_init(&drm->ttm.bdev, &nvbo->bo, size,\r\ntype, &nvbo->placement,\r\nalign >> PAGE_SHIFT, false, NULL, acc_size, sg,\r\nnouveau_bo_del_ttm);\r\nif (ret) {\r\nreturn ret;\r\n}\r\n*pnvbo = nvbo;\r\nreturn 0;\r\n}\r\nstatic void\r\nset_placement_list(uint32_t *pl, unsigned *n, uint32_t type, uint32_t flags)\r\n{\r\n*n = 0;\r\nif (type & TTM_PL_FLAG_VRAM)\r\npl[(*n)++] = TTM_PL_FLAG_VRAM | flags;\r\nif (type & TTM_PL_FLAG_TT)\r\npl[(*n)++] = TTM_PL_FLAG_TT | flags;\r\nif (type & TTM_PL_FLAG_SYSTEM)\r\npl[(*n)++] = TTM_PL_FLAG_SYSTEM | flags;\r\n}\r\nstatic void\r\nset_placement_range(struct nouveau_bo *nvbo, uint32_t type)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nstruct nouveau_fb *pfb = nouveau_fb(drm->device);\r\nu32 vram_pages = pfb->ram->size >> PAGE_SHIFT;\r\nif ((nv_device(drm->device)->card_type == NV_10 ||\r\nnv_device(drm->device)->card_type == NV_11) &&\r\nnvbo->tile_mode && (type & TTM_PL_FLAG_VRAM) &&\r\nnvbo->bo.mem.num_pages < vram_pages / 4) {\r\nif (nvbo->tile_flags & NOUVEAU_GEM_TILE_ZETA) {\r\nnvbo->placement.fpfn = vram_pages / 2;\r\nnvbo->placement.lpfn = ~0;\r\n} else {\r\nnvbo->placement.fpfn = 0;\r\nnvbo->placement.lpfn = vram_pages / 2;\r\n}\r\n}\r\n}\r\nvoid\r\nnouveau_bo_placement_set(struct nouveau_bo *nvbo, uint32_t type, uint32_t busy)\r\n{\r\nstruct ttm_placement *pl = &nvbo->placement;\r\nuint32_t flags = TTM_PL_MASK_CACHING |\r\n(nvbo->pin_refcnt ? TTM_PL_FLAG_NO_EVICT : 0);\r\npl->placement = nvbo->placements;\r\nset_placement_list(nvbo->placements, &pl->num_placement,\r\ntype, flags);\r\npl->busy_placement = nvbo->busy_placements;\r\nset_placement_list(nvbo->busy_placements, &pl->num_busy_placement,\r\ntype | busy, flags);\r\nset_placement_range(nvbo, type);\r\n}\r\nint\r\nnouveau_bo_pin(struct nouveau_bo *nvbo, uint32_t memtype)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nint ret;\r\nret = ttm_bo_reserve(bo, false, false, false, 0);\r\nif (ret)\r\ngoto out;\r\nif (nvbo->pin_refcnt && !(memtype & (1 << bo->mem.mem_type))) {\r\nNV_ERROR(drm, "bo %p pinned elsewhere: 0x%08x vs 0x%08x\n", bo,\r\n1 << bo->mem.mem_type, memtype);\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (nvbo->pin_refcnt++)\r\ngoto out;\r\nnouveau_bo_placement_set(nvbo, memtype, 0);\r\nret = nouveau_bo_validate(nvbo, false, false);\r\nif (ret == 0) {\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\ndrm->gem.vram_available -= bo->mem.size;\r\nbreak;\r\ncase TTM_PL_TT:\r\ndrm->gem.gart_available -= bo->mem.size;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nout:\r\nttm_bo_unreserve(bo);\r\nreturn ret;\r\n}\r\nint\r\nnouveau_bo_unpin(struct nouveau_bo *nvbo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nint ret, ref;\r\nret = ttm_bo_reserve(bo, false, false, false, 0);\r\nif (ret)\r\nreturn ret;\r\nref = --nvbo->pin_refcnt;\r\nWARN_ON_ONCE(ref < 0);\r\nif (ref)\r\ngoto out;\r\nnouveau_bo_placement_set(nvbo, bo->mem.placement, 0);\r\nret = nouveau_bo_validate(nvbo, false, false);\r\nif (ret == 0) {\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\ndrm->gem.vram_available += bo->mem.size;\r\nbreak;\r\ncase TTM_PL_TT:\r\ndrm->gem.gart_available += bo->mem.size;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nout:\r\nttm_bo_unreserve(bo);\r\nreturn ret;\r\n}\r\nint\r\nnouveau_bo_map(struct nouveau_bo *nvbo)\r\n{\r\nint ret;\r\nret = ttm_bo_reserve(&nvbo->bo, false, false, false, 0);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_bo_kmap(&nvbo->bo, 0, nvbo->bo.mem.num_pages, &nvbo->kmap);\r\nttm_bo_unreserve(&nvbo->bo);\r\nreturn ret;\r\n}\r\nvoid\r\nnouveau_bo_unmap(struct nouveau_bo *nvbo)\r\n{\r\nif (nvbo)\r\nttm_bo_kunmap(&nvbo->kmap);\r\n}\r\nint\r\nnouveau_bo_validate(struct nouveau_bo *nvbo, bool interruptible,\r\nbool no_wait_gpu)\r\n{\r\nint ret;\r\nret = ttm_bo_validate(&nvbo->bo, &nvbo->placement,\r\ninterruptible, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nu16\r\nnouveau_bo_rd16(struct nouveau_bo *nvbo, unsigned index)\r\n{\r\nbool is_iomem;\r\nu16 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem = &mem[index];\r\nif (is_iomem)\r\nreturn ioread16_native((void __force __iomem *)mem);\r\nelse\r\nreturn *mem;\r\n}\r\nvoid\r\nnouveau_bo_wr16(struct nouveau_bo *nvbo, unsigned index, u16 val)\r\n{\r\nbool is_iomem;\r\nu16 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem = &mem[index];\r\nif (is_iomem)\r\niowrite16_native(val, (void __force __iomem *)mem);\r\nelse\r\n*mem = val;\r\n}\r\nu32\r\nnouveau_bo_rd32(struct nouveau_bo *nvbo, unsigned index)\r\n{\r\nbool is_iomem;\r\nu32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem = &mem[index];\r\nif (is_iomem)\r\nreturn ioread32_native((void __force __iomem *)mem);\r\nelse\r\nreturn *mem;\r\n}\r\nvoid\r\nnouveau_bo_wr32(struct nouveau_bo *nvbo, unsigned index, u32 val)\r\n{\r\nbool is_iomem;\r\nu32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\r\nmem = &mem[index];\r\nif (is_iomem)\r\niowrite32_native(val, (void __force __iomem *)mem);\r\nelse\r\n*mem = val;\r\n}\r\nstatic struct ttm_tt *\r\nnouveau_ttm_tt_create(struct ttm_bo_device *bdev, unsigned long size,\r\nuint32_t page_flags, struct page *dummy_read)\r\n{\r\n#if __OS_HAS_AGP\r\nstruct nouveau_drm *drm = nouveau_bdev(bdev);\r\nstruct drm_device *dev = drm->dev;\r\nif (drm->agp.stat == ENABLED) {\r\nreturn ttm_agp_tt_create(bdev, dev->agp->bridge, size,\r\npage_flags, dummy_read);\r\n}\r\n#endif\r\nreturn nouveau_sgdma_create_ttm(bdev, size, page_flags, dummy_read);\r\n}\r\nstatic int\r\nnouveau_bo_invalidate_caches(struct ttm_bo_device *bdev, uint32_t flags)\r\n{\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_bo_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,\r\nstruct ttm_mem_type_manager *man)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bdev);\r\nswitch (type) {\r\ncase TTM_PL_SYSTEM:\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_MASK_CACHING;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ncase TTM_PL_VRAM:\r\nif (nv_device(drm->device)->card_type >= NV_50) {\r\nman->func = &nouveau_vram_manager;\r\nman->io_reserve_fastpath = false;\r\nman->use_io_reserve_lru = true;\r\n} else {\r\nman->func = &ttm_bo_manager_func;\r\n}\r\nman->flags = TTM_MEMTYPE_FLAG_FIXED |\r\nTTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_WC;\r\nman->default_caching = TTM_PL_FLAG_WC;\r\nbreak;\r\ncase TTM_PL_TT:\r\nif (nv_device(drm->device)->card_type >= NV_50)\r\nman->func = &nouveau_gart_manager;\r\nelse\r\nif (drm->agp.stat != ENABLED)\r\nman->func = &nv04_gart_manager;\r\nelse\r\nman->func = &ttm_bo_manager_func;\r\nif (drm->agp.stat == ENABLED) {\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_WC;\r\nman->default_caching = TTM_PL_FLAG_WC;\r\n} else {\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE |\r\nTTM_MEMTYPE_FLAG_CMA;\r\nman->available_caching = TTM_PL_MASK_CACHING;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\n}\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_bo_evict_flags(struct ttm_buffer_object *bo, struct ttm_placement *pl)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_TT,\r\nTTM_PL_FLAG_SYSTEM);\r\nbreak;\r\ndefault:\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_SYSTEM, 0);\r\nbreak;\r\n}\r\n*pl = nvbo->placement;\r\n}\r\nstatic int\r\nnve0_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = RING_SPACE(chan, 2);\r\nif (ret == 0) {\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle & 0x0000ffff);\r\nFIRE_RING (chan);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnve0_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nint ret = RING_SPACE(chan, 10);\r\nif (ret == 0) {\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0400, 8);\r\nOUT_RING (chan, upper_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, upper_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, new_mem->num_pages);\r\nBEGIN_IMC0(chan, NvSubCopy, 0x0300, 0x0386);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnvc0_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = RING_SPACE(chan, 2);\r\nif (ret == 0) {\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnvc0_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nu64 src_offset = node->vma[0].offset;\r\nu64 dst_offset = node->vma[1].offset;\r\nu32 page_count = new_mem->num_pages;\r\nint ret;\r\npage_count = new_mem->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 8191) ? 8191 : page_count;\r\nret = RING_SPACE(chan, 11);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NVC0(chan, NvSubCopy, 0x030c, 8);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0300, 1);\r\nOUT_RING (chan, 0x00000110);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnvc0_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nu64 src_offset = node->vma[0].offset;\r\nu64 dst_offset = node->vma[1].offset;\r\nu32 page_count = new_mem->num_pages;\r\nint ret;\r\npage_count = new_mem->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 2047) ? 2047 : page_count;\r\nret = RING_SPACE(chan, 12);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0238, 2);\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nBEGIN_NVC0(chan, NvSubCopy, 0x030c, 6);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nBEGIN_NVC0(chan, NvSubCopy, 0x0300, 1);\r\nOUT_RING (chan, 0x00100110);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnva3_bo_move_copy(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nu64 src_offset = node->vma[0].offset;\r\nu64 dst_offset = node->vma[1].offset;\r\nu32 page_count = new_mem->num_pages;\r\nint ret;\r\npage_count = new_mem->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 8191) ? 8191 : page_count;\r\nret = RING_SPACE(chan, 11);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, 0x030c, 8);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nBEGIN_NV04(chan, NvSubCopy, 0x0300, 1);\r\nOUT_RING (chan, 0x00000110);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnv98_bo_move_exec(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nint ret = RING_SPACE(chan, 7);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0320, 6);\r\nOUT_RING (chan, upper_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, upper_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, 0x00000000 );\r\nOUT_RING (chan, new_mem->num_pages << PAGE_SHIFT);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnv84_bo_move_exec(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nint ret = RING_SPACE(chan, 7);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0304, 6);\r\nOUT_RING (chan, new_mem->num_pages << PAGE_SHIFT);\r\nOUT_RING (chan, upper_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[0].offset));\r\nOUT_RING (chan, upper_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, lower_32_bits(node->vma[1].offset));\r\nOUT_RING (chan, 0x00000000 );\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnv50_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = RING_SPACE(chan, 6);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle);\r\nBEGIN_NV04(chan, NvSubCopy, 0x0180, 3);\r\nOUT_RING (chan, NvNotify0);\r\nOUT_RING (chan, NvDmaFB);\r\nOUT_RING (chan, NvDmaFB);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnv50_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_mem *node = old_mem->mm_node;\r\nu64 length = (new_mem->num_pages << PAGE_SHIFT);\r\nu64 src_offset = node->vma[0].offset;\r\nu64 dst_offset = node->vma[1].offset;\r\nint src_tiled = !!node->memtype;\r\nint dst_tiled = !!((struct nouveau_mem *)new_mem->mm_node)->memtype;\r\nint ret;\r\nwhile (length) {\r\nu32 amount, stride, height;\r\nret = RING_SPACE(chan, 18 + 6 * (src_tiled + dst_tiled));\r\nif (ret)\r\nreturn ret;\r\namount = min(length, (u64)(4 * 1024 * 1024));\r\nstride = 16 * 4;\r\nheight = amount / stride;\r\nif (src_tiled) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0200, 7);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, height);\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\n} else {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0200, 1);\r\nOUT_RING (chan, 1);\r\n}\r\nif (dst_tiled) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x021c, 7);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, height);\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 0);\r\nOUT_RING (chan, 0);\r\n} else {\r\nBEGIN_NV04(chan, NvSubCopy, 0x021c, 1);\r\nOUT_RING (chan, 1);\r\n}\r\nBEGIN_NV04(chan, NvSubCopy, 0x0238, 2);\r\nOUT_RING (chan, upper_32_bits(src_offset));\r\nOUT_RING (chan, upper_32_bits(dst_offset));\r\nBEGIN_NV04(chan, NvSubCopy, 0x030c, 8);\r\nOUT_RING (chan, lower_32_bits(src_offset));\r\nOUT_RING (chan, lower_32_bits(dst_offset));\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, stride);\r\nOUT_RING (chan, height);\r\nOUT_RING (chan, 0x00000101);\r\nOUT_RING (chan, 0x00000000);\r\nBEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_NOP, 1);\r\nOUT_RING (chan, 0);\r\nlength -= amount;\r\nsrc_offset += amount;\r\ndst_offset += amount;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnv04_bo_move_init(struct nouveau_channel *chan, u32 handle)\r\n{\r\nint ret = RING_SPACE(chan, 4);\r\nif (ret == 0) {\r\nBEGIN_NV04(chan, NvSubCopy, 0x0000, 1);\r\nOUT_RING (chan, handle);\r\nBEGIN_NV04(chan, NvSubCopy, 0x0180, 1);\r\nOUT_RING (chan, NvNotify0);\r\n}\r\nreturn ret;\r\n}\r\nstatic inline uint32_t\r\nnouveau_bo_mem_ctxdma(struct ttm_buffer_object *bo,\r\nstruct nouveau_channel *chan, struct ttm_mem_reg *mem)\r\n{\r\nif (mem->mem_type == TTM_PL_TT)\r\nreturn NvDmaTT;\r\nreturn NvDmaFB;\r\n}\r\nstatic int\r\nnv04_bo_move_m2mf(struct nouveau_channel *chan, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *old_mem, struct ttm_mem_reg *new_mem)\r\n{\r\nu32 src_offset = old_mem->start << PAGE_SHIFT;\r\nu32 dst_offset = new_mem->start << PAGE_SHIFT;\r\nu32 page_count = new_mem->num_pages;\r\nint ret;\r\nret = RING_SPACE(chan, 3);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_DMA_SOURCE, 2);\r\nOUT_RING (chan, nouveau_bo_mem_ctxdma(bo, chan, old_mem));\r\nOUT_RING (chan, nouveau_bo_mem_ctxdma(bo, chan, new_mem));\r\npage_count = new_mem->num_pages;\r\nwhile (page_count) {\r\nint line_count = (page_count > 2047) ? 2047 : page_count;\r\nret = RING_SPACE(chan, 11);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NV04(chan, NvSubCopy,\r\nNV_MEMORY_TO_MEMORY_FORMAT_OFFSET_IN, 8);\r\nOUT_RING (chan, src_offset);\r\nOUT_RING (chan, dst_offset);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, PAGE_SIZE);\r\nOUT_RING (chan, line_count);\r\nOUT_RING (chan, 0x00000101);\r\nOUT_RING (chan, 0x00000000);\r\nBEGIN_NV04(chan, NvSubCopy, NV_MEMORY_TO_MEMORY_FORMAT_NOP, 1);\r\nOUT_RING (chan, 0);\r\npage_count -= line_count;\r\nsrc_offset += (PAGE_SIZE * line_count);\r\ndst_offset += (PAGE_SIZE * line_count);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_bo_move_prep(struct nouveau_drm *drm, struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nstruct nouveau_mem *old_node = bo->mem.mm_node;\r\nstruct nouveau_mem *new_node = mem->mm_node;\r\nu64 size = (u64)mem->num_pages << PAGE_SHIFT;\r\nint ret;\r\nret = nouveau_vm_get(nv_client(drm)->vm, size, old_node->page_shift,\r\nNV_MEM_ACCESS_RW, &old_node->vma[0]);\r\nif (ret)\r\nreturn ret;\r\nret = nouveau_vm_get(nv_client(drm)->vm, size, new_node->page_shift,\r\nNV_MEM_ACCESS_RW, &old_node->vma[1]);\r\nif (ret) {\r\nnouveau_vm_put(&old_node->vma[0]);\r\nreturn ret;\r\n}\r\nnouveau_vm_map(&old_node->vma[0], old_node);\r\nnouveau_vm_map(&old_node->vma[1], new_node);\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_bo_move_m2mf(struct ttm_buffer_object *bo, int evict, bool intr,\r\nbool no_wait_gpu, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct nouveau_channel *chan = drm->ttm.chan;\r\nstruct nouveau_fence *fence;\r\nint ret;\r\nif (nv_device(drm->device)->card_type >= NV_50) {\r\nret = nouveau_bo_move_prep(drm, bo, new_mem);\r\nif (ret)\r\nreturn ret;\r\n}\r\nmutex_lock_nested(&chan->cli->mutex, SINGLE_DEPTH_NESTING);\r\nret = nouveau_fence_sync(bo->sync_obj, chan);\r\nif (ret == 0) {\r\nret = drm->ttm.move(chan, bo, &bo->mem, new_mem);\r\nif (ret == 0) {\r\nret = nouveau_fence_new(chan, false, &fence);\r\nif (ret == 0) {\r\nret = ttm_bo_move_accel_cleanup(bo, fence,\r\nevict,\r\nno_wait_gpu,\r\nnew_mem);\r\nnouveau_fence_unref(&fence);\r\n}\r\n}\r\n}\r\nmutex_unlock(&chan->cli->mutex);\r\nreturn ret;\r\n}\r\nvoid\r\nnouveau_bo_move_init(struct nouveau_drm *drm)\r\n{\r\nstatic const struct {\r\nconst char *name;\r\nint engine;\r\nu32 oclass;\r\nint (*exec)(struct nouveau_channel *,\r\nstruct ttm_buffer_object *,\r\nstruct ttm_mem_reg *, struct ttm_mem_reg *);\r\nint (*init)(struct nouveau_channel *, u32 handle);\r\n} _methods[] = {\r\n{ "COPY", 4, 0xa0b5, nve0_bo_move_copy, nve0_bo_move_init },\r\n{ "GRCE", 0, 0xa0b5, nve0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY1", 5, 0x90b8, nvc0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY0", 4, 0x90b5, nvc0_bo_move_copy, nvc0_bo_move_init },\r\n{ "COPY", 0, 0x85b5, nva3_bo_move_copy, nv50_bo_move_init },\r\n{ "CRYPT", 0, 0x74c1, nv84_bo_move_exec, nv50_bo_move_init },\r\n{ "M2MF", 0, 0x9039, nvc0_bo_move_m2mf, nvc0_bo_move_init },\r\n{ "M2MF", 0, 0x5039, nv50_bo_move_m2mf, nv50_bo_move_init },\r\n{ "M2MF", 0, 0x0039, nv04_bo_move_m2mf, nv04_bo_move_init },\r\n{},\r\n{ "CRYPT", 0, 0x88b4, nv98_bo_move_exec, nv50_bo_move_init },\r\n}, *mthd = _methods;\r\nconst char *name = "CPU";\r\nint ret;\r\ndo {\r\nstruct nouveau_object *object;\r\nstruct nouveau_channel *chan;\r\nu32 handle = (mthd->engine << 16) | mthd->oclass;\r\nif (mthd->engine)\r\nchan = drm->cechan;\r\nelse\r\nchan = drm->channel;\r\nif (chan == NULL)\r\ncontinue;\r\nret = nouveau_object_new(nv_object(drm), chan->handle, handle,\r\nmthd->oclass, NULL, 0, &object);\r\nif (ret == 0) {\r\nret = mthd->init(chan, handle);\r\nif (ret) {\r\nnouveau_object_del(nv_object(drm),\r\nchan->handle, handle);\r\ncontinue;\r\n}\r\ndrm->ttm.move = mthd->exec;\r\ndrm->ttm.chan = chan;\r\nname = mthd->name;\r\nbreak;\r\n}\r\n} while ((++mthd)->exec);\r\nNV_INFO(drm, "MM: using %s for buffer copies\n", name);\r\n}\r\nstatic int\r\nnouveau_bo_move_flipd(struct ttm_buffer_object *bo, bool evict, bool intr,\r\nbool no_wait_gpu, struct ttm_mem_reg *new_mem)\r\n{\r\nu32 placement_memtype = TTM_PL_FLAG_TT | TTM_PL_MASK_CACHING;\r\nstruct ttm_placement placement;\r\nstruct ttm_mem_reg tmp_mem;\r\nint ret;\r\nplacement.fpfn = placement.lpfn = 0;\r\nplacement.num_placement = placement.num_busy_placement = 1;\r\nplacement.placement = placement.busy_placement = &placement_memtype;\r\ntmp_mem = *new_mem;\r\ntmp_mem.mm_node = NULL;\r\nret = ttm_bo_mem_space(bo, &placement, &tmp_mem, intr, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_tt_bind(bo->ttm, &tmp_mem);\r\nif (ret)\r\ngoto out;\r\nret = nouveau_bo_move_m2mf(bo, true, intr, no_wait_gpu, &tmp_mem);\r\nif (ret)\r\ngoto out;\r\nret = ttm_bo_move_ttm(bo, true, no_wait_gpu, new_mem);\r\nout:\r\nttm_bo_mem_put(bo, &tmp_mem);\r\nreturn ret;\r\n}\r\nstatic int\r\nnouveau_bo_move_flips(struct ttm_buffer_object *bo, bool evict, bool intr,\r\nbool no_wait_gpu, struct ttm_mem_reg *new_mem)\r\n{\r\nu32 placement_memtype = TTM_PL_FLAG_TT | TTM_PL_MASK_CACHING;\r\nstruct ttm_placement placement;\r\nstruct ttm_mem_reg tmp_mem;\r\nint ret;\r\nplacement.fpfn = placement.lpfn = 0;\r\nplacement.num_placement = placement.num_busy_placement = 1;\r\nplacement.placement = placement.busy_placement = &placement_memtype;\r\ntmp_mem = *new_mem;\r\ntmp_mem.mm_node = NULL;\r\nret = ttm_bo_mem_space(bo, &placement, &tmp_mem, intr, no_wait_gpu);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_bo_move_ttm(bo, true, no_wait_gpu, &tmp_mem);\r\nif (ret)\r\ngoto out;\r\nret = nouveau_bo_move_m2mf(bo, true, intr, no_wait_gpu, new_mem);\r\nif (ret)\r\ngoto out;\r\nout:\r\nttm_bo_mem_put(bo, &tmp_mem);\r\nreturn ret;\r\n}\r\nstatic void\r\nnouveau_bo_move_ntfy(struct ttm_buffer_object *bo, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nstruct nouveau_vma *vma;\r\nif (bo->destroy != nouveau_bo_del_ttm)\r\nreturn;\r\nlist_for_each_entry(vma, &nvbo->vma_list, head) {\r\nif (new_mem && new_mem->mem_type != TTM_PL_SYSTEM &&\r\n(new_mem->mem_type == TTM_PL_VRAM ||\r\nnvbo->page_shift != vma->vm->vmm->lpg_shift)) {\r\nnouveau_vm_map(vma, new_mem->mm_node);\r\n} else {\r\nnouveau_vm_unmap(vma);\r\n}\r\n}\r\n}\r\nstatic int\r\nnouveau_bo_vm_bind(struct ttm_buffer_object *bo, struct ttm_mem_reg *new_mem,\r\nstruct nouveau_drm_tile **new_tile)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct drm_device *dev = drm->dev;\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nu64 offset = new_mem->start << PAGE_SHIFT;\r\n*new_tile = NULL;\r\nif (new_mem->mem_type != TTM_PL_VRAM)\r\nreturn 0;\r\nif (nv_device(drm->device)->card_type >= NV_10) {\r\n*new_tile = nv10_bo_set_tiling(dev, offset, new_mem->size,\r\nnvbo->tile_mode,\r\nnvbo->tile_flags);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_bo_vm_cleanup(struct ttm_buffer_object *bo,\r\nstruct nouveau_drm_tile *new_tile,\r\nstruct nouveau_drm_tile **old_tile)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct drm_device *dev = drm->dev;\r\nnv10_bo_put_tile_region(dev, *old_tile, bo->sync_obj);\r\n*old_tile = new_tile;\r\n}\r\nstatic int\r\nnouveau_bo_move(struct ttm_buffer_object *bo, bool evict, bool intr,\r\nbool no_wait_gpu, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nstruct ttm_mem_reg *old_mem = &bo->mem;\r\nstruct nouveau_drm_tile *new_tile = NULL;\r\nint ret = 0;\r\nif (nv_device(drm->device)->card_type < NV_50) {\r\nret = nouveau_bo_vm_bind(bo, new_mem, &new_tile);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (old_mem->mem_type == TTM_PL_SYSTEM && !bo->ttm) {\r\nBUG_ON(bo->mem.mm_node != NULL);\r\nbo->mem = *new_mem;\r\nnew_mem->mm_node = NULL;\r\ngoto out;\r\n}\r\nif (drm->ttm.move) {\r\nif (new_mem->mem_type == TTM_PL_SYSTEM)\r\nret = nouveau_bo_move_flipd(bo, evict, intr,\r\nno_wait_gpu, new_mem);\r\nelse if (old_mem->mem_type == TTM_PL_SYSTEM)\r\nret = nouveau_bo_move_flips(bo, evict, intr,\r\nno_wait_gpu, new_mem);\r\nelse\r\nret = nouveau_bo_move_m2mf(bo, evict, intr,\r\nno_wait_gpu, new_mem);\r\nif (!ret)\r\ngoto out;\r\n}\r\nspin_lock(&bo->bdev->fence_lock);\r\nret = ttm_bo_wait(bo, true, intr, no_wait_gpu);\r\nspin_unlock(&bo->bdev->fence_lock);\r\nif (ret == 0)\r\nret = ttm_bo_move_memcpy(bo, evict, no_wait_gpu, new_mem);\r\nout:\r\nif (nv_device(drm->device)->card_type < NV_50) {\r\nif (ret)\r\nnouveau_bo_vm_cleanup(bo, NULL, &new_tile);\r\nelse\r\nnouveau_bo_vm_cleanup(bo, new_tile, &nvbo->tile);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nnouveau_bo_verify_access(struct ttm_buffer_object *bo, struct file *filp)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nreturn drm_vma_node_verify_access(&nvbo->gem.vma_node, filp);\r\n}\r\nstatic int\r\nnouveau_ttm_io_mem_reserve(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)\r\n{\r\nstruct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];\r\nstruct nouveau_drm *drm = nouveau_bdev(bdev);\r\nstruct nouveau_mem *node = mem->mm_node;\r\nstruct drm_device *dev = drm->dev;\r\nint ret;\r\nmem->bus.addr = NULL;\r\nmem->bus.offset = 0;\r\nmem->bus.size = mem->num_pages << PAGE_SHIFT;\r\nmem->bus.base = 0;\r\nmem->bus.is_iomem = false;\r\nif (!(man->flags & TTM_MEMTYPE_FLAG_MAPPABLE))\r\nreturn -EINVAL;\r\nswitch (mem->mem_type) {\r\ncase TTM_PL_SYSTEM:\r\nreturn 0;\r\ncase TTM_PL_TT:\r\n#if __OS_HAS_AGP\r\nif (drm->agp.stat == ENABLED) {\r\nmem->bus.offset = mem->start << PAGE_SHIFT;\r\nmem->bus.base = drm->agp.base;\r\nmem->bus.is_iomem = !dev->agp->cant_use_aperture;\r\n}\r\n#endif\r\nif (nv_device(drm->device)->card_type < NV_50 || !node->memtype)\r\nbreak;\r\ncase TTM_PL_VRAM:\r\nmem->bus.offset = mem->start << PAGE_SHIFT;\r\nmem->bus.base = nv_device_resource_start(nouveau_dev(dev), 1);\r\nmem->bus.is_iomem = true;\r\nif (nv_device(drm->device)->card_type >= NV_50) {\r\nstruct nouveau_bar *bar = nouveau_bar(drm->device);\r\nret = bar->umap(bar, node, NV_MEM_ACCESS_RW,\r\n&node->bar_vma);\r\nif (ret)\r\nreturn ret;\r\nmem->bus.offset = node->bar_vma.offset;\r\n}\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_ttm_io_mem_free(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bdev);\r\nstruct nouveau_bar *bar = nouveau_bar(drm->device);\r\nstruct nouveau_mem *node = mem->mm_node;\r\nif (!node->bar_vma.node)\r\nreturn;\r\nbar->unmap(bar, &node->bar_vma);\r\n}\r\nstatic int\r\nnouveau_ttm_fault_reserve_notify(struct ttm_buffer_object *bo)\r\n{\r\nstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\r\nstruct nouveau_bo *nvbo = nouveau_bo(bo);\r\nstruct nouveau_device *device = nv_device(drm->device);\r\nu32 mappable = nv_device_resource_len(device, 1) >> PAGE_SHIFT;\r\nint ret;\r\nif (bo->mem.mem_type != TTM_PL_VRAM) {\r\nif (nv_device(drm->device)->card_type < NV_50 ||\r\n!nouveau_bo_tile_layout(nvbo))\r\nreturn 0;\r\nif (bo->mem.mem_type == TTM_PL_SYSTEM) {\r\nnouveau_bo_placement_set(nvbo, TTM_PL_TT, 0);\r\nret = nouveau_bo_validate(nvbo, false, false);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nif (nv_device(drm->device)->card_type >= NV_50 ||\r\nbo->mem.start + bo->mem.num_pages < mappable)\r\nreturn 0;\r\nnvbo->placement.fpfn = 0;\r\nnvbo->placement.lpfn = mappable;\r\nnouveau_bo_placement_set(nvbo, TTM_PL_FLAG_VRAM, 0);\r\nreturn nouveau_bo_validate(nvbo, false, false);\r\n}\r\nstatic int\r\nnouveau_ttm_tt_populate(struct ttm_tt *ttm)\r\n{\r\nstruct ttm_dma_tt *ttm_dma = (void *)ttm;\r\nstruct nouveau_drm *drm;\r\nstruct nouveau_device *device;\r\nstruct drm_device *dev;\r\nunsigned i;\r\nint r;\r\nbool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);\r\nif (ttm->state != tt_unpopulated)\r\nreturn 0;\r\nif (slave && ttm->sg) {\r\ndrm_prime_sg_to_page_addr_arrays(ttm->sg, ttm->pages,\r\nttm_dma->dma_address, ttm->num_pages);\r\nttm->state = tt_unbound;\r\nreturn 0;\r\n}\r\ndrm = nouveau_bdev(ttm->bdev);\r\ndevice = nv_device(drm->device);\r\ndev = drm->dev;\r\n#if __OS_HAS_AGP\r\nif (drm->agp.stat == ENABLED) {\r\nreturn ttm_agp_tt_populate(ttm);\r\n}\r\n#endif\r\n#ifdef CONFIG_SWIOTLB\r\nif (swiotlb_nr_tbl()) {\r\nreturn ttm_dma_populate((void *)ttm, dev->dev);\r\n}\r\n#endif\r\nr = ttm_pool_populate(ttm);\r\nif (r) {\r\nreturn r;\r\n}\r\nfor (i = 0; i < ttm->num_pages; i++) {\r\nttm_dma->dma_address[i] = nv_device_map_page(device,\r\nttm->pages[i]);\r\nif (!ttm_dma->dma_address[i]) {\r\nwhile (--i) {\r\nnv_device_unmap_page(device,\r\nttm_dma->dma_address[i]);\r\nttm_dma->dma_address[i] = 0;\r\n}\r\nttm_pool_unpopulate(ttm);\r\nreturn -EFAULT;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnouveau_ttm_tt_unpopulate(struct ttm_tt *ttm)\r\n{\r\nstruct ttm_dma_tt *ttm_dma = (void *)ttm;\r\nstruct nouveau_drm *drm;\r\nstruct nouveau_device *device;\r\nstruct drm_device *dev;\r\nunsigned i;\r\nbool slave = !!(ttm->page_flags & TTM_PAGE_FLAG_SG);\r\nif (slave)\r\nreturn;\r\ndrm = nouveau_bdev(ttm->bdev);\r\ndevice = nv_device(drm->device);\r\ndev = drm->dev;\r\n#if __OS_HAS_AGP\r\nif (drm->agp.stat == ENABLED) {\r\nttm_agp_tt_unpopulate(ttm);\r\nreturn;\r\n}\r\n#endif\r\n#ifdef CONFIG_SWIOTLB\r\nif (swiotlb_nr_tbl()) {\r\nttm_dma_unpopulate((void *)ttm, dev->dev);\r\nreturn;\r\n}\r\n#endif\r\nfor (i = 0; i < ttm->num_pages; i++) {\r\nif (ttm_dma->dma_address[i]) {\r\nnv_device_unmap_page(device, ttm_dma->dma_address[i]);\r\n}\r\n}\r\nttm_pool_unpopulate(ttm);\r\n}\r\nvoid\r\nnouveau_bo_fence(struct nouveau_bo *nvbo, struct nouveau_fence *fence)\r\n{\r\nstruct nouveau_fence *new_fence = nouveau_fence_ref(fence);\r\nstruct nouveau_fence *old_fence = NULL;\r\nspin_lock(&nvbo->bo.bdev->fence_lock);\r\nold_fence = nvbo->bo.sync_obj;\r\nnvbo->bo.sync_obj = new_fence;\r\nspin_unlock(&nvbo->bo.bdev->fence_lock);\r\nnouveau_fence_unref(&old_fence);\r\n}\r\nstatic void\r\nnouveau_bo_fence_unref(void **sync_obj)\r\n{\r\nnouveau_fence_unref((struct nouveau_fence **)sync_obj);\r\n}\r\nstatic void *\r\nnouveau_bo_fence_ref(void *sync_obj)\r\n{\r\nreturn nouveau_fence_ref(sync_obj);\r\n}\r\nstatic bool\r\nnouveau_bo_fence_signalled(void *sync_obj)\r\n{\r\nreturn nouveau_fence_done(sync_obj);\r\n}\r\nstatic int\r\nnouveau_bo_fence_wait(void *sync_obj, bool lazy, bool intr)\r\n{\r\nreturn nouveau_fence_wait(sync_obj, lazy, intr);\r\n}\r\nstatic int\r\nnouveau_bo_fence_flush(void *sync_obj)\r\n{\r\nreturn 0;\r\n}\r\nstruct nouveau_vma *\r\nnouveau_bo_vma_find(struct nouveau_bo *nvbo, struct nouveau_vm *vm)\r\n{\r\nstruct nouveau_vma *vma;\r\nlist_for_each_entry(vma, &nvbo->vma_list, head) {\r\nif (vma->vm == vm)\r\nreturn vma;\r\n}\r\nreturn NULL;\r\n}\r\nint\r\nnouveau_bo_vma_add(struct nouveau_bo *nvbo, struct nouveau_vm *vm,\r\nstruct nouveau_vma *vma)\r\n{\r\nconst u32 size = nvbo->bo.mem.num_pages << PAGE_SHIFT;\r\nint ret;\r\nret = nouveau_vm_get(vm, size, nvbo->page_shift,\r\nNV_MEM_ACCESS_RW, vma);\r\nif (ret)\r\nreturn ret;\r\nif ( nvbo->bo.mem.mem_type != TTM_PL_SYSTEM &&\r\n(nvbo->bo.mem.mem_type == TTM_PL_VRAM ||\r\nnvbo->page_shift != vma->vm->vmm->lpg_shift))\r\nnouveau_vm_map(vma, nvbo->bo.mem.mm_node);\r\nlist_add_tail(&vma->head, &nvbo->vma_list);\r\nvma->refcount = 1;\r\nreturn 0;\r\n}\r\nvoid\r\nnouveau_bo_vma_del(struct nouveau_bo *nvbo, struct nouveau_vma *vma)\r\n{\r\nif (vma->node) {\r\nif (nvbo->bo.mem.mem_type != TTM_PL_SYSTEM)\r\nnouveau_vm_unmap(vma);\r\nnouveau_vm_put(vma);\r\nlist_del(&vma->head);\r\n}\r\n}
