static bool is_duplicate_irq(int irq, int *irqs, int nr_irqs)\r\n{\r\nint i;\r\nfor (i = 0; i < nr_irqs; i++)\r\nif (irq == irqs[i])\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic int probe_cci_revision(void)\r\n{\r\nint rev;\r\nrev = readl_relaxed(cci_ctrl_base + CCI_PID2) & CCI_PID2_REV_MASK;\r\nrev >>= CCI_PID2_REV_SHIFT;\r\nif (rev < CCI_REV_R1_PX)\r\nreturn CCI_REV_R0;\r\nelse\r\nreturn CCI_REV_R1;\r\n}\r\nstatic struct pmu_port_event_ranges *port_range_by_rev(void)\r\n{\r\nint rev = probe_cci_revision();\r\nreturn &port_event_range[rev];\r\n}\r\nstatic int pmu_is_valid_slave_event(u8 ev_code)\r\n{\r\nreturn pmu->port_ranges->slave_min <= ev_code &&\r\nev_code <= pmu->port_ranges->slave_max;\r\n}\r\nstatic int pmu_is_valid_master_event(u8 ev_code)\r\n{\r\nreturn pmu->port_ranges->master_min <= ev_code &&\r\nev_code <= pmu->port_ranges->master_max;\r\n}\r\nstatic int pmu_validate_hw_event(u8 hw_event)\r\n{\r\nu8 ev_source = CCI_PMU_EVENT_SOURCE(hw_event);\r\nu8 ev_code = CCI_PMU_EVENT_CODE(hw_event);\r\nswitch (ev_source) {\r\ncase CCI_PORT_S0:\r\ncase CCI_PORT_S1:\r\ncase CCI_PORT_S2:\r\ncase CCI_PORT_S3:\r\ncase CCI_PORT_S4:\r\nif (pmu_is_valid_slave_event(ev_code))\r\nreturn hw_event;\r\nbreak;\r\ncase CCI_PORT_M0:\r\ncase CCI_PORT_M1:\r\ncase CCI_PORT_M2:\r\nif (pmu_is_valid_master_event(ev_code))\r\nreturn hw_event;\r\nbreak;\r\n}\r\nreturn -ENOENT;\r\n}\r\nstatic int pmu_is_valid_counter(struct arm_pmu *cci_pmu, int idx)\r\n{\r\nreturn CCI_PMU_CYCLE_CNTR_IDX <= idx &&\r\nidx <= CCI_PMU_CNTR_LAST(cci_pmu);\r\n}\r\nstatic u32 pmu_read_register(int idx, unsigned int offset)\r\n{\r\nreturn readl_relaxed(pmu->base + CCI_PMU_CNTR_BASE(idx) + offset);\r\n}\r\nstatic void pmu_write_register(u32 value, int idx, unsigned int offset)\r\n{\r\nreturn writel_relaxed(value, pmu->base + CCI_PMU_CNTR_BASE(idx) + offset);\r\n}\r\nstatic void pmu_disable_counter(int idx)\r\n{\r\npmu_write_register(0, idx, CCI_PMU_CNTR_CTRL);\r\n}\r\nstatic void pmu_enable_counter(int idx)\r\n{\r\npmu_write_register(1, idx, CCI_PMU_CNTR_CTRL);\r\n}\r\nstatic void pmu_set_event(int idx, unsigned long event)\r\n{\r\nevent &= CCI_PMU_EVENT_MASK;\r\npmu_write_register(event, idx, CCI_PMU_EVT_SEL);\r\n}\r\nstatic u32 pmu_get_max_counters(void)\r\n{\r\nu32 n_cnts = (readl_relaxed(cci_ctrl_base + CCI_PMCR) &\r\nCCI_PMCR_NCNT_MASK) >> CCI_PMCR_NCNT_SHIFT;\r\nreturn n_cnts + 1;\r\n}\r\nstatic struct pmu_hw_events *pmu_get_hw_events(void)\r\n{\r\nreturn &pmu->hw_events;\r\n}\r\nstatic int pmu_get_event_idx(struct pmu_hw_events *hw, struct perf_event *event)\r\n{\r\nstruct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hw_event = &event->hw;\r\nunsigned long cci_event = hw_event->config_base & CCI_PMU_EVENT_MASK;\r\nint idx;\r\nif (cci_event == CCI_PMU_CYCLES) {\r\nif (test_and_set_bit(CCI_PMU_CYCLE_CNTR_IDX, hw->used_mask))\r\nreturn -EAGAIN;\r\nreturn CCI_PMU_CYCLE_CNTR_IDX;\r\n}\r\nfor (idx = CCI_PMU_CNTR0_IDX; idx <= CCI_PMU_CNTR_LAST(cci_pmu); ++idx)\r\nif (!test_and_set_bit(idx, hw->used_mask))\r\nreturn idx;\r\nreturn -EAGAIN;\r\n}\r\nstatic int pmu_map_event(struct perf_event *event)\r\n{\r\nint mapping;\r\nu8 config = event->attr.config & CCI_PMU_EVENT_MASK;\r\nif (event->attr.type < PERF_TYPE_MAX)\r\nreturn -ENOENT;\r\nif (config == CCI_PMU_CYCLES)\r\nmapping = config;\r\nelse\r\nmapping = pmu_validate_hw_event(config);\r\nreturn mapping;\r\n}\r\nstatic int pmu_request_irq(struct arm_pmu *cci_pmu, irq_handler_t handler)\r\n{\r\nint i;\r\nstruct platform_device *pmu_device = cci_pmu->plat_device;\r\nif (unlikely(!pmu_device))\r\nreturn -ENODEV;\r\nif (pmu->nr_irqs < 1) {\r\ndev_err(&pmu_device->dev, "no irqs for CCI PMUs defined\n");\r\nreturn -ENODEV;\r\n}\r\nfor (i = 0; i < pmu->nr_irqs; i++) {\r\nint err = request_irq(pmu->irqs[i], handler, IRQF_SHARED,\r\n"arm-cci-pmu", cci_pmu);\r\nif (err) {\r\ndev_err(&pmu_device->dev, "unable to request IRQ%d for ARM CCI PMU counters\n",\r\npmu->irqs[i]);\r\nreturn err;\r\n}\r\nset_bit(i, &pmu->active_irqs);\r\n}\r\nreturn 0;\r\n}\r\nstatic irqreturn_t pmu_handle_irq(int irq_num, void *dev)\r\n{\r\nunsigned long flags;\r\nstruct arm_pmu *cci_pmu = (struct arm_pmu *)dev;\r\nstruct pmu_hw_events *events = cci_pmu->get_hw_events();\r\nstruct perf_sample_data data;\r\nstruct pt_regs *regs;\r\nint idx, handled = IRQ_NONE;\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nregs = get_irq_regs();\r\nfor (idx = CCI_PMU_CYCLE_CNTR_IDX; idx <= CCI_PMU_CNTR_LAST(cci_pmu); idx++) {\r\nstruct perf_event *event = events->events[idx];\r\nstruct hw_perf_event *hw_counter;\r\nif (!event)\r\ncontinue;\r\nhw_counter = &event->hw;\r\nif (!pmu_read_register(idx, CCI_PMU_OVRFLW) & CCI_PMU_OVRFLW_FLAG)\r\ncontinue;\r\npmu_write_register(CCI_PMU_OVRFLW_FLAG, idx, CCI_PMU_OVRFLW);\r\nhandled = IRQ_HANDLED;\r\narmpmu_event_update(event);\r\nperf_sample_data_init(&data, 0, hw_counter->last_period);\r\nif (!armpmu_event_set_period(event))\r\ncontinue;\r\nif (perf_event_overflow(event, &data, regs))\r\ncci_pmu->disable(event);\r\n}\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic void pmu_free_irq(struct arm_pmu *cci_pmu)\r\n{\r\nint i;\r\nfor (i = 0; i < pmu->nr_irqs; i++) {\r\nif (!test_and_clear_bit(i, &pmu->active_irqs))\r\ncontinue;\r\nfree_irq(pmu->irqs[i], cci_pmu);\r\n}\r\n}\r\nstatic void pmu_enable_event(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nstruct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);\r\nstruct pmu_hw_events *events = cci_pmu->get_hw_events();\r\nstruct hw_perf_event *hw_counter = &event->hw;\r\nint idx = hw_counter->idx;\r\nif (unlikely(!pmu_is_valid_counter(cci_pmu, idx))) {\r\ndev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nif (idx != CCI_PMU_CYCLE_CNTR_IDX)\r\npmu_set_event(idx, hw_counter->config_base);\r\npmu_enable_counter(idx);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void pmu_disable_event(struct perf_event *event)\r\n{\r\nstruct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hw_counter = &event->hw;\r\nint idx = hw_counter->idx;\r\nif (unlikely(!pmu_is_valid_counter(cci_pmu, idx))) {\r\ndev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);\r\nreturn;\r\n}\r\npmu_disable_counter(idx);\r\n}\r\nstatic void pmu_start(struct arm_pmu *cci_pmu)\r\n{\r\nu32 val;\r\nunsigned long flags;\r\nstruct pmu_hw_events *events = cci_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = readl_relaxed(cci_ctrl_base + CCI_PMCR) | CCI_PMCR_CEN;\r\nwritel(val, cci_ctrl_base + CCI_PMCR);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void pmu_stop(struct arm_pmu *cci_pmu)\r\n{\r\nu32 val;\r\nunsigned long flags;\r\nstruct pmu_hw_events *events = cci_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = readl_relaxed(cci_ctrl_base + CCI_PMCR) & ~CCI_PMCR_CEN;\r\nwritel(val, cci_ctrl_base + CCI_PMCR);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic u32 pmu_read_counter(struct perf_event *event)\r\n{\r\nstruct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hw_counter = &event->hw;\r\nint idx = hw_counter->idx;\r\nu32 value;\r\nif (unlikely(!pmu_is_valid_counter(cci_pmu, idx))) {\r\ndev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);\r\nreturn 0;\r\n}\r\nvalue = pmu_read_register(idx, CCI_PMU_CNTR);\r\nreturn value;\r\n}\r\nstatic void pmu_write_counter(struct perf_event *event, u32 value)\r\n{\r\nstruct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);\r\nstruct hw_perf_event *hw_counter = &event->hw;\r\nint idx = hw_counter->idx;\r\nif (unlikely(!pmu_is_valid_counter(cci_pmu, idx)))\r\ndev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);\r\nelse\r\npmu_write_register(value, idx, CCI_PMU_CNTR);\r\n}\r\nstatic int cci_pmu_init(struct arm_pmu *cci_pmu, struct platform_device *pdev)\r\n{\r\n*cci_pmu = (struct arm_pmu){\r\n.name = pmu_names[probe_cci_revision()],\r\n.max_period = (1LLU << 32) - 1,\r\n.get_hw_events = pmu_get_hw_events,\r\n.get_event_idx = pmu_get_event_idx,\r\n.map_event = pmu_map_event,\r\n.request_irq = pmu_request_irq,\r\n.handle_irq = pmu_handle_irq,\r\n.free_irq = pmu_free_irq,\r\n.enable = pmu_enable_event,\r\n.disable = pmu_disable_event,\r\n.start = pmu_start,\r\n.stop = pmu_stop,\r\n.read_counter = pmu_read_counter,\r\n.write_counter = pmu_write_counter,\r\n};\r\ncci_pmu->plat_device = pdev;\r\ncci_pmu->num_events = pmu_get_max_counters();\r\nreturn armpmu_register(cci_pmu, -1);\r\n}\r\nstatic int cci_pmu_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *res;\r\nint i, ret, irq;\r\npmu = devm_kzalloc(&pdev->dev, sizeof(*pmu), GFP_KERNEL);\r\nif (!pmu)\r\nreturn -ENOMEM;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\npmu->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(pmu->base))\r\nreturn -ENOMEM;\r\npmu->nr_irqs = 0;\r\nfor (i = 0; i < CCI_PMU_MAX_HW_EVENTS; i++) {\r\nirq = platform_get_irq(pdev, i);\r\nif (irq < 0)\r\nbreak;\r\nif (is_duplicate_irq(irq, pmu->irqs, pmu->nr_irqs))\r\ncontinue;\r\npmu->irqs[pmu->nr_irqs++] = irq;\r\n}\r\nif (i < CCI_PMU_MAX_HW_EVENTS) {\r\ndev_warn(&pdev->dev, "In-correct number of interrupts: %d, should be %d\n",\r\ni, CCI_PMU_MAX_HW_EVENTS);\r\nreturn -EINVAL;\r\n}\r\npmu->port_ranges = port_range_by_rev();\r\nif (!pmu->port_ranges) {\r\ndev_warn(&pdev->dev, "CCI PMU version not supported\n");\r\nreturn -EINVAL;\r\n}\r\npmu->cci_pmu = devm_kzalloc(&pdev->dev, sizeof(*(pmu->cci_pmu)), GFP_KERNEL);\r\nif (!pmu->cci_pmu)\r\nreturn -ENOMEM;\r\npmu->hw_events.events = pmu->events;\r\npmu->hw_events.used_mask = pmu->used_mask;\r\nraw_spin_lock_init(&pmu->hw_events.pmu_lock);\r\nret = cci_pmu_init(pmu->cci_pmu, pdev);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nstatic int cci_platform_probe(struct platform_device *pdev)\r\n{\r\nif (!cci_probed())\r\nreturn -ENODEV;\r\nreturn of_platform_populate(pdev->dev.of_node, NULL, NULL, &pdev->dev);\r\n}\r\nstatic inline void init_cpu_port(struct cpu_port *port, u32 index, u64 mpidr)\r\n{\r\nport->port = PORT_VALID | index;\r\nport->mpidr = mpidr;\r\n}\r\nstatic inline bool cpu_port_is_valid(struct cpu_port *port)\r\n{\r\nreturn !!(port->port & PORT_VALID);\r\n}\r\nstatic inline bool cpu_port_match(struct cpu_port *port, u64 mpidr)\r\n{\r\nreturn port->mpidr == (mpidr & MPIDR_HWID_BITMASK);\r\n}\r\nstatic int __cci_ace_get_port(struct device_node *dn, int type)\r\n{\r\nint i;\r\nbool ace_match;\r\nstruct device_node *cci_portn;\r\ncci_portn = of_parse_phandle(dn, "cci-control-port", 0);\r\nfor (i = 0; i < nb_cci_ports; i++) {\r\nace_match = ports[i].type == type;\r\nif (ace_match && cci_portn == ports[i].dn)\r\nreturn i;\r\n}\r\nreturn -ENODEV;\r\n}\r\nint cci_ace_get_port(struct device_node *dn)\r\n{\r\nreturn __cci_ace_get_port(dn, ACE_LITE_PORT);\r\n}\r\nstatic void cci_ace_init_ports(void)\r\n{\r\nint port, cpu;\r\nstruct device_node *cpun;\r\nfor_each_possible_cpu(cpu) {\r\ncpun = of_get_cpu_node(cpu, NULL);\r\nif (WARN(!cpun, "Missing cpu device node\n"))\r\ncontinue;\r\nport = __cci_ace_get_port(cpun, ACE_PORT);\r\nif (port < 0)\r\ncontinue;\r\ninit_cpu_port(&cpu_port[cpu], port, cpu_logical_map(cpu));\r\n}\r\nfor_each_possible_cpu(cpu) {\r\nWARN(!cpu_port_is_valid(&cpu_port[cpu]),\r\n"CPU %u does not have an associated CCI port\n",\r\ncpu);\r\n}\r\n}\r\nstatic void notrace cci_port_control(unsigned int port, bool enable)\r\n{\r\nvoid __iomem *base = ports[port].base;\r\nwritel_relaxed(enable ? CCI_ENABLE_REQ : 0, base + CCI_PORT_CTRL);\r\nwhile (readl_relaxed(cci_ctrl_base + CCI_CTRL_STATUS) & 0x1)\r\n;\r\n}\r\nint notrace cci_disable_port_by_cpu(u64 mpidr)\r\n{\r\nint cpu;\r\nbool is_valid;\r\nfor (cpu = 0; cpu < nr_cpu_ids; cpu++) {\r\nis_valid = cpu_port_is_valid(&cpu_port[cpu]);\r\nif (is_valid && cpu_port_match(&cpu_port[cpu], mpidr)) {\r\ncci_port_control(cpu_port[cpu].port, false);\r\nreturn 0;\r\n}\r\n}\r\nreturn -ENODEV;\r\n}\r\nasmlinkage void __naked cci_enable_port_for_self(void)\r\n{\r\nasm volatile ("\n"\r\n" .arch armv7-a\n"\r\n" mrc p15, 0, r0, c0, c0, 5 @ get MPIDR value \n"\r\n" and r0, r0, #"__stringify(MPIDR_HWID_BITMASK)" \n"\r\n" adr r1, 5f \n"\r\n" ldr r2, [r1] \n"\r\n" add r1, r1, r2 @ &cpu_port \n"\r\n" add ip, r1, %[sizeof_cpu_port] \n"\r\n"1: ldr r2, [r1, %[offsetof_cpu_port_mpidr_lsb]] \n"\r\n" cmp r2, r0 @ compare MPIDR \n"\r\n" bne 2f \n"\r\n" ldr r3, [r1, %[offsetof_cpu_port_port]] \n"\r\n" tst r3, #"__stringify(PORT_VALID)" \n"\r\n" bne 3f \n"\r\n"2: add r1, r1, %[sizeof_struct_cpu_port] \n"\r\n" cmp r1, ip @ done? \n"\r\n" blo 1b \n"\r\n"cci_port_not_found: \n"\r\n" wfi \n"\r\n" wfe \n"\r\n" b cci_port_not_found \n"\r\n"3: bic r3, r3, #"__stringify(PORT_VALID)" \n"\r\n" adr r0, 6f \n"\r\n" ldmia r0, {r1, r2} \n"\r\n" sub r1, r1, r0 @ virt - phys \n"\r\n" ldr r0, [r0, r2] @ *(&ports) \n"\r\n" mov r2, %[sizeof_struct_ace_port] \n"\r\n" mla r0, r2, r3, r0 @ &ports[index] \n"\r\n" sub r0, r0, r1 @ virt_to_phys() \n"\r\n" ldr r0, [r0, %[offsetof_port_phys]] \n"\r\n" mov r3, %[cci_enable_req]\n"\r\n" str r3, [r0, #"__stringify(CCI_PORT_CTRL)"] \n"\r\n" adr r1, 7f \n"\r\n" ldr r0, [r1] \n"\r\n" ldr r0, [r0, r1] @ cci_ctrl_base \n"\r\n"4: ldr r1, [r0, #"__stringify(CCI_CTRL_STATUS)"] \n"\r\n" tst r1, %[cci_control_status_bits] \n"\r\n" bne 4b \n"\r\n" mov r0, #0 \n"\r\n" bx lr \n"\r\n" .align 2 \n"\r\n"5: .word cpu_port - . \n"\r\n"6: .word . \n"\r\n" .word ports - 6b \n"\r\n"7: .word cci_ctrl_phys - . \n"\r\n: :\r\n[sizeof_cpu_port] "i" (sizeof(cpu_port)),\r\n[cci_enable_req] "i" cpu_to_le32(CCI_ENABLE_REQ),\r\n[cci_control_status_bits] "i" cpu_to_le32(1),\r\n#ifndef __ARMEB__\r\n[offsetof_cpu_port_mpidr_lsb] "i" (offsetof(struct cpu_port, mpidr)),\r\n#else\r\n[offsetof_cpu_port_mpidr_lsb] "i" (offsetof(struct cpu_port, mpidr)+4),\r\n#endif\r\n[offsetof_cpu_port_port] "i" (offsetof(struct cpu_port, port)),\r\n[sizeof_struct_cpu_port] "i" (sizeof(struct cpu_port)),\r\n[sizeof_struct_ace_port] "i" (sizeof(struct cci_ace_port)),\r\n[offsetof_port_phys] "i" (offsetof(struct cci_ace_port, phys)) );\r\nunreachable();\r\n}\r\nint notrace __cci_control_port_by_device(struct device_node *dn, bool enable)\r\n{\r\nint port;\r\nif (!dn)\r\nreturn -ENODEV;\r\nport = __cci_ace_get_port(dn, ACE_LITE_PORT);\r\nif (WARN_ONCE(port < 0, "node %s ACE lite port look-up failure\n",\r\ndn->full_name))\r\nreturn -ENODEV;\r\ncci_port_control(port, enable);\r\nreturn 0;\r\n}\r\nint notrace __cci_control_port_by_index(u32 port, bool enable)\r\n{\r\nif (port >= nb_cci_ports || ports[port].type == ACE_INVALID_PORT)\r\nreturn -ENODEV;\r\nif (ports[port].type == ACE_PORT)\r\nreturn -EPERM;\r\ncci_port_control(port, enable);\r\nreturn 0;\r\n}\r\nstatic int cci_probe(void)\r\n{\r\nstruct cci_nb_ports const *cci_config;\r\nint ret, i, nb_ace = 0, nb_ace_lite = 0;\r\nstruct device_node *np, *cp;\r\nstruct resource res;\r\nconst char *match_str;\r\nbool is_ace;\r\nnp = of_find_matching_node(NULL, arm_cci_matches);\r\nif (!np)\r\nreturn -ENODEV;\r\ncci_config = of_match_node(arm_cci_matches, np)->data;\r\nif (!cci_config)\r\nreturn -ENODEV;\r\nnb_cci_ports = cci_config->nb_ace + cci_config->nb_ace_lite;\r\nports = kcalloc(nb_cci_ports, sizeof(*ports), GFP_KERNEL);\r\nif (!ports)\r\nreturn -ENOMEM;\r\nret = of_address_to_resource(np, 0, &res);\r\nif (!ret) {\r\ncci_ctrl_base = ioremap(res.start, resource_size(&res));\r\ncci_ctrl_phys = res.start;\r\n}\r\nif (ret || !cci_ctrl_base) {\r\nWARN(1, "unable to ioremap CCI ctrl\n");\r\nret = -ENXIO;\r\ngoto memalloc_err;\r\n}\r\nfor_each_child_of_node(np, cp) {\r\nif (!of_match_node(arm_cci_ctrl_if_matches, cp))\r\ncontinue;\r\ni = nb_ace + nb_ace_lite;\r\nif (i >= nb_cci_ports)\r\nbreak;\r\nif (of_property_read_string(cp, "interface-type",\r\n&match_str)) {\r\nWARN(1, "node %s missing interface-type property\n",\r\ncp->full_name);\r\ncontinue;\r\n}\r\nis_ace = strcmp(match_str, "ace") == 0;\r\nif (!is_ace && strcmp(match_str, "ace-lite")) {\r\nWARN(1, "node %s containing invalid interface-type property, skipping it\n",\r\ncp->full_name);\r\ncontinue;\r\n}\r\nret = of_address_to_resource(cp, 0, &res);\r\nif (!ret) {\r\nports[i].base = ioremap(res.start, resource_size(&res));\r\nports[i].phys = res.start;\r\n}\r\nif (ret || !ports[i].base) {\r\nWARN(1, "unable to ioremap CCI port %d\n", i);\r\ncontinue;\r\n}\r\nif (is_ace) {\r\nif (WARN_ON(nb_ace >= cci_config->nb_ace))\r\ncontinue;\r\nports[i].type = ACE_PORT;\r\n++nb_ace;\r\n} else {\r\nif (WARN_ON(nb_ace_lite >= cci_config->nb_ace_lite))\r\ncontinue;\r\nports[i].type = ACE_LITE_PORT;\r\n++nb_ace_lite;\r\n}\r\nports[i].dn = cp;\r\n}\r\ncci_ace_init_ports();\r\nsync_cache_w(&cci_ctrl_base);\r\nsync_cache_w(&cci_ctrl_phys);\r\nsync_cache_w(&ports);\r\nsync_cache_w(&cpu_port);\r\n__sync_cache_range_w(ports, sizeof(*ports) * nb_cci_ports);\r\npr_info("ARM CCI driver probed\n");\r\nreturn 0;\r\nmemalloc_err:\r\nkfree(ports);\r\nreturn ret;\r\n}\r\nstatic int cci_init(void)\r\n{\r\nif (cci_init_status != -EAGAIN)\r\nreturn cci_init_status;\r\nmutex_lock(&cci_probing);\r\nif (cci_init_status == -EAGAIN)\r\ncci_init_status = cci_probe();\r\nmutex_unlock(&cci_probing);\r\nreturn cci_init_status;\r\n}\r\nstatic int __init cci_platform_init(void)\r\n{\r\nint ret;\r\nret = platform_driver_register(&cci_pmu_driver);\r\nif (ret)\r\nreturn ret;\r\nreturn platform_driver_register(&cci_platform_driver);\r\n}\r\nstatic int __init cci_platform_init(void)\r\n{\r\nreturn 0;\r\n}\r\nbool cci_probed(void)\r\n{\r\nreturn cci_init() == 0;\r\n}
