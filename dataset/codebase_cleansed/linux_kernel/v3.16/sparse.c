int page_to_nid(const struct page *page)\r\n{\r\nreturn section_to_node_table[page_to_section(page)];\r\n}\r\nstatic void set_section_nid(unsigned long section_nr, int nid)\r\n{\r\nsection_to_node_table[section_nr] = nid;\r\n}\r\nstatic inline void set_section_nid(unsigned long section_nr, int nid)\r\n{\r\n}\r\nstatic struct mem_section noinline __init_refok *sparse_index_alloc(int nid)\r\n{\r\nstruct mem_section *section = NULL;\r\nunsigned long array_size = SECTIONS_PER_ROOT *\r\nsizeof(struct mem_section);\r\nif (slab_is_available()) {\r\nif (node_state(nid, N_HIGH_MEMORY))\r\nsection = kzalloc_node(array_size, GFP_KERNEL, nid);\r\nelse\r\nsection = kzalloc(array_size, GFP_KERNEL);\r\n} else {\r\nsection = memblock_virt_alloc_node(array_size, nid);\r\n}\r\nreturn section;\r\n}\r\nstatic int __meminit sparse_index_init(unsigned long section_nr, int nid)\r\n{\r\nunsigned long root = SECTION_NR_TO_ROOT(section_nr);\r\nstruct mem_section *section;\r\nif (mem_section[root])\r\nreturn -EEXIST;\r\nsection = sparse_index_alloc(nid);\r\nif (!section)\r\nreturn -ENOMEM;\r\nmem_section[root] = section;\r\nreturn 0;\r\n}\r\nstatic inline int sparse_index_init(unsigned long section_nr, int nid)\r\n{\r\nreturn 0;\r\n}\r\nint __section_nr(struct mem_section* ms)\r\n{\r\nunsigned long root_nr;\r\nstruct mem_section* root;\r\nfor (root_nr = 0; root_nr < NR_SECTION_ROOTS; root_nr++) {\r\nroot = __nr_to_section(root_nr * SECTIONS_PER_ROOT);\r\nif (!root)\r\ncontinue;\r\nif ((ms >= root) && (ms < (root + SECTIONS_PER_ROOT)))\r\nbreak;\r\n}\r\nVM_BUG_ON(root_nr == NR_SECTION_ROOTS);\r\nreturn (root_nr * SECTIONS_PER_ROOT) + (ms - root);\r\n}\r\nstatic inline unsigned long sparse_encode_early_nid(int nid)\r\n{\r\nreturn (nid << SECTION_NID_SHIFT);\r\n}\r\nstatic inline int sparse_early_nid(struct mem_section *section)\r\n{\r\nreturn (section->section_mem_map >> SECTION_NID_SHIFT);\r\n}\r\nvoid __meminit mminit_validate_memmodel_limits(unsigned long *start_pfn,\r\nunsigned long *end_pfn)\r\n{\r\nunsigned long max_sparsemem_pfn = 1UL << (MAX_PHYSMEM_BITS-PAGE_SHIFT);\r\nif (*start_pfn > max_sparsemem_pfn) {\r\nmminit_dprintk(MMINIT_WARNING, "pfnvalidation",\r\n"Start of range %lu -> %lu exceeds SPARSEMEM max %lu\n",\r\n*start_pfn, *end_pfn, max_sparsemem_pfn);\r\nWARN_ON_ONCE(1);\r\n*start_pfn = max_sparsemem_pfn;\r\n*end_pfn = max_sparsemem_pfn;\r\n} else if (*end_pfn > max_sparsemem_pfn) {\r\nmminit_dprintk(MMINIT_WARNING, "pfnvalidation",\r\n"End of range %lu -> %lu exceeds SPARSEMEM max %lu\n",\r\n*start_pfn, *end_pfn, max_sparsemem_pfn);\r\nWARN_ON_ONCE(1);\r\n*end_pfn = max_sparsemem_pfn;\r\n}\r\n}\r\nvoid __init memory_present(int nid, unsigned long start, unsigned long end)\r\n{\r\nunsigned long pfn;\r\nstart &= PAGE_SECTION_MASK;\r\nmminit_validate_memmodel_limits(&start, &end);\r\nfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION) {\r\nunsigned long section = pfn_to_section_nr(pfn);\r\nstruct mem_section *ms;\r\nsparse_index_init(section, nid);\r\nset_section_nid(section, nid);\r\nms = __nr_to_section(section);\r\nif (!ms->section_mem_map)\r\nms->section_mem_map = sparse_encode_early_nid(nid) |\r\nSECTION_MARKED_PRESENT;\r\n}\r\n}\r\nunsigned long __init node_memmap_size_bytes(int nid, unsigned long start_pfn,\r\nunsigned long end_pfn)\r\n{\r\nunsigned long pfn;\r\nunsigned long nr_pages = 0;\r\nmminit_validate_memmodel_limits(&start_pfn, &end_pfn);\r\nfor (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {\r\nif (nid != early_pfn_to_nid(pfn))\r\ncontinue;\r\nif (pfn_present(pfn))\r\nnr_pages += PAGES_PER_SECTION;\r\n}\r\nreturn nr_pages * sizeof(struct page);\r\n}\r\nstatic unsigned long sparse_encode_mem_map(struct page *mem_map, unsigned long pnum)\r\n{\r\nreturn (unsigned long)(mem_map - (section_nr_to_pfn(pnum)));\r\n}\r\nstruct page *sparse_decode_mem_map(unsigned long coded_mem_map, unsigned long pnum)\r\n{\r\ncoded_mem_map &= SECTION_MAP_MASK;\r\nreturn ((struct page *)coded_mem_map) + section_nr_to_pfn(pnum);\r\n}\r\nstatic int __meminit sparse_init_one_section(struct mem_section *ms,\r\nunsigned long pnum, struct page *mem_map,\r\nunsigned long *pageblock_bitmap)\r\n{\r\nif (!present_section(ms))\r\nreturn -EINVAL;\r\nms->section_mem_map &= ~SECTION_MAP_MASK;\r\nms->section_mem_map |= sparse_encode_mem_map(mem_map, pnum) |\r\nSECTION_HAS_MEM_MAP;\r\nms->pageblock_flags = pageblock_bitmap;\r\nreturn 1;\r\n}\r\nunsigned long usemap_size(void)\r\n{\r\nunsigned long size_bytes;\r\nsize_bytes = roundup(SECTION_BLOCKFLAGS_BITS, 8) / 8;\r\nsize_bytes = roundup(size_bytes, sizeof(unsigned long));\r\nreturn size_bytes;\r\n}\r\nstatic unsigned long *__kmalloc_section_usemap(void)\r\n{\r\nreturn kmalloc(usemap_size(), GFP_KERNEL);\r\n}\r\nstatic unsigned long * __init\r\nsparse_early_usemaps_alloc_pgdat_section(struct pglist_data *pgdat,\r\nunsigned long size)\r\n{\r\nunsigned long goal, limit;\r\nunsigned long *p;\r\nint nid;\r\ngoal = __pa(pgdat) & (PAGE_SECTION_MASK << PAGE_SHIFT);\r\nlimit = goal + (1UL << PA_SECTION_SHIFT);\r\nnid = early_pfn_to_nid(goal >> PAGE_SHIFT);\r\nagain:\r\np = memblock_virt_alloc_try_nid_nopanic(size,\r\nSMP_CACHE_BYTES, goal, limit,\r\nnid);\r\nif (!p && limit) {\r\nlimit = 0;\r\ngoto again;\r\n}\r\nreturn p;\r\n}\r\nstatic void __init check_usemap_section_nr(int nid, unsigned long *usemap)\r\n{\r\nunsigned long usemap_snr, pgdat_snr;\r\nstatic unsigned long old_usemap_snr = NR_MEM_SECTIONS;\r\nstatic unsigned long old_pgdat_snr = NR_MEM_SECTIONS;\r\nstruct pglist_data *pgdat = NODE_DATA(nid);\r\nint usemap_nid;\r\nusemap_snr = pfn_to_section_nr(__pa(usemap) >> PAGE_SHIFT);\r\npgdat_snr = pfn_to_section_nr(__pa(pgdat) >> PAGE_SHIFT);\r\nif (usemap_snr == pgdat_snr)\r\nreturn;\r\nif (old_usemap_snr == usemap_snr && old_pgdat_snr == pgdat_snr)\r\nreturn;\r\nold_usemap_snr = usemap_snr;\r\nold_pgdat_snr = pgdat_snr;\r\nusemap_nid = sparse_early_nid(__nr_to_section(usemap_snr));\r\nif (usemap_nid != nid) {\r\nprintk(KERN_INFO\r\n"node %d must be removed before remove section %ld\n",\r\nnid, usemap_snr);\r\nreturn;\r\n}\r\nprintk(KERN_INFO "Section %ld and %ld (node %d)", usemap_snr,\r\npgdat_snr, nid);\r\nprintk(KERN_CONT\r\n" have a circular dependency on usemap and pgdat allocations\n");\r\n}\r\nstatic unsigned long * __init\r\nsparse_early_usemaps_alloc_pgdat_section(struct pglist_data *pgdat,\r\nunsigned long size)\r\n{\r\nreturn memblock_virt_alloc_node_nopanic(size, pgdat->node_id);\r\n}\r\nstatic void __init check_usemap_section_nr(int nid, unsigned long *usemap)\r\n{\r\n}\r\nstatic void __init sparse_early_usemaps_alloc_node(void *data,\r\nunsigned long pnum_begin,\r\nunsigned long pnum_end,\r\nunsigned long usemap_count, int nodeid)\r\n{\r\nvoid *usemap;\r\nunsigned long pnum;\r\nunsigned long **usemap_map = (unsigned long **)data;\r\nint size = usemap_size();\r\nusemap = sparse_early_usemaps_alloc_pgdat_section(NODE_DATA(nodeid),\r\nsize * usemap_count);\r\nif (!usemap) {\r\nprintk(KERN_WARNING "%s: allocation failed\n", __func__);\r\nreturn;\r\n}\r\nfor (pnum = pnum_begin; pnum < pnum_end; pnum++) {\r\nif (!present_section_nr(pnum))\r\ncontinue;\r\nusemap_map[pnum] = usemap;\r\nusemap += size;\r\ncheck_usemap_section_nr(nodeid, usemap_map[pnum]);\r\n}\r\n}\r\nstruct page __init *sparse_mem_map_populate(unsigned long pnum, int nid)\r\n{\r\nstruct page *map;\r\nunsigned long size;\r\nmap = alloc_remap(nid, sizeof(struct page) * PAGES_PER_SECTION);\r\nif (map)\r\nreturn map;\r\nsize = PAGE_ALIGN(sizeof(struct page) * PAGES_PER_SECTION);\r\nmap = memblock_virt_alloc_try_nid(size,\r\nPAGE_SIZE, __pa(MAX_DMA_ADDRESS),\r\nBOOTMEM_ALLOC_ACCESSIBLE, nid);\r\nreturn map;\r\n}\r\nvoid __init sparse_mem_maps_populate_node(struct page **map_map,\r\nunsigned long pnum_begin,\r\nunsigned long pnum_end,\r\nunsigned long map_count, int nodeid)\r\n{\r\nvoid *map;\r\nunsigned long pnum;\r\nunsigned long size = sizeof(struct page) * PAGES_PER_SECTION;\r\nmap = alloc_remap(nodeid, size * map_count);\r\nif (map) {\r\nfor (pnum = pnum_begin; pnum < pnum_end; pnum++) {\r\nif (!present_section_nr(pnum))\r\ncontinue;\r\nmap_map[pnum] = map;\r\nmap += size;\r\n}\r\nreturn;\r\n}\r\nsize = PAGE_ALIGN(size);\r\nmap = memblock_virt_alloc_try_nid(size * map_count,\r\nPAGE_SIZE, __pa(MAX_DMA_ADDRESS),\r\nBOOTMEM_ALLOC_ACCESSIBLE, nodeid);\r\nif (map) {\r\nfor (pnum = pnum_begin; pnum < pnum_end; pnum++) {\r\nif (!present_section_nr(pnum))\r\ncontinue;\r\nmap_map[pnum] = map;\r\nmap += size;\r\n}\r\nreturn;\r\n}\r\nfor (pnum = pnum_begin; pnum < pnum_end; pnum++) {\r\nstruct mem_section *ms;\r\nif (!present_section_nr(pnum))\r\ncontinue;\r\nmap_map[pnum] = sparse_mem_map_populate(pnum, nodeid);\r\nif (map_map[pnum])\r\ncontinue;\r\nms = __nr_to_section(pnum);\r\nprintk(KERN_ERR "%s: sparsemem memory map backing failed "\r\n"some memory will not be available.\n", __func__);\r\nms->section_mem_map = 0;\r\n}\r\n}\r\nstatic void __init sparse_early_mem_maps_alloc_node(void *data,\r\nunsigned long pnum_begin,\r\nunsigned long pnum_end,\r\nunsigned long map_count, int nodeid)\r\n{\r\nstruct page **map_map = (struct page **)data;\r\nsparse_mem_maps_populate_node(map_map, pnum_begin, pnum_end,\r\nmap_count, nodeid);\r\n}\r\nstatic struct page __init *sparse_early_mem_map_alloc(unsigned long pnum)\r\n{\r\nstruct page *map;\r\nstruct mem_section *ms = __nr_to_section(pnum);\r\nint nid = sparse_early_nid(ms);\r\nmap = sparse_mem_map_populate(pnum, nid);\r\nif (map)\r\nreturn map;\r\nprintk(KERN_ERR "%s: sparsemem memory map backing failed "\r\n"some memory will not be available.\n", __func__);\r\nms->section_mem_map = 0;\r\nreturn NULL;\r\n}\r\nvoid __weak __meminit vmemmap_populate_print_last(void)\r\n{\r\n}\r\nstatic void __init alloc_usemap_and_memmap(void (*alloc_func)\r\n(void *, unsigned long, unsigned long,\r\nunsigned long, int), void *data)\r\n{\r\nunsigned long pnum;\r\nunsigned long map_count;\r\nint nodeid_begin = 0;\r\nunsigned long pnum_begin = 0;\r\nfor (pnum = 0; pnum < NR_MEM_SECTIONS; pnum++) {\r\nstruct mem_section *ms;\r\nif (!present_section_nr(pnum))\r\ncontinue;\r\nms = __nr_to_section(pnum);\r\nnodeid_begin = sparse_early_nid(ms);\r\npnum_begin = pnum;\r\nbreak;\r\n}\r\nmap_count = 1;\r\nfor (pnum = pnum_begin + 1; pnum < NR_MEM_SECTIONS; pnum++) {\r\nstruct mem_section *ms;\r\nint nodeid;\r\nif (!present_section_nr(pnum))\r\ncontinue;\r\nms = __nr_to_section(pnum);\r\nnodeid = sparse_early_nid(ms);\r\nif (nodeid == nodeid_begin) {\r\nmap_count++;\r\ncontinue;\r\n}\r\nalloc_func(data, pnum_begin, pnum,\r\nmap_count, nodeid_begin);\r\nnodeid_begin = nodeid;\r\npnum_begin = pnum;\r\nmap_count = 1;\r\n}\r\nalloc_func(data, pnum_begin, NR_MEM_SECTIONS,\r\nmap_count, nodeid_begin);\r\n}\r\nvoid __init sparse_init(void)\r\n{\r\nunsigned long pnum;\r\nstruct page *map;\r\nunsigned long *usemap;\r\nunsigned long **usemap_map;\r\nint size;\r\n#ifdef CONFIG_SPARSEMEM_ALLOC_MEM_MAP_TOGETHER\r\nint size2;\r\nstruct page **map_map;\r\n#endif\r\nBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\r\nset_pageblock_order();\r\nsize = sizeof(unsigned long *) * NR_MEM_SECTIONS;\r\nusemap_map = memblock_virt_alloc(size, 0);\r\nif (!usemap_map)\r\npanic("can not allocate usemap_map\n");\r\nalloc_usemap_and_memmap(sparse_early_usemaps_alloc_node,\r\n(void *)usemap_map);\r\n#ifdef CONFIG_SPARSEMEM_ALLOC_MEM_MAP_TOGETHER\r\nsize2 = sizeof(struct page *) * NR_MEM_SECTIONS;\r\nmap_map = memblock_virt_alloc(size2, 0);\r\nif (!map_map)\r\npanic("can not allocate map_map\n");\r\nalloc_usemap_and_memmap(sparse_early_mem_maps_alloc_node,\r\n(void *)map_map);\r\n#endif\r\nfor (pnum = 0; pnum < NR_MEM_SECTIONS; pnum++) {\r\nif (!present_section_nr(pnum))\r\ncontinue;\r\nusemap = usemap_map[pnum];\r\nif (!usemap)\r\ncontinue;\r\n#ifdef CONFIG_SPARSEMEM_ALLOC_MEM_MAP_TOGETHER\r\nmap = map_map[pnum];\r\n#else\r\nmap = sparse_early_mem_map_alloc(pnum);\r\n#endif\r\nif (!map)\r\ncontinue;\r\nsparse_init_one_section(__nr_to_section(pnum), pnum, map,\r\nusemap);\r\n}\r\nvmemmap_populate_print_last();\r\n#ifdef CONFIG_SPARSEMEM_ALLOC_MEM_MAP_TOGETHER\r\nmemblock_free_early(__pa(map_map), size2);\r\n#endif\r\nmemblock_free_early(__pa(usemap_map), size);\r\n}\r\nstatic inline struct page *kmalloc_section_memmap(unsigned long pnum, int nid)\r\n{\r\nreturn sparse_mem_map_populate(pnum, nid);\r\n}\r\nstatic void __kfree_section_memmap(struct page *memmap)\r\n{\r\nunsigned long start = (unsigned long)memmap;\r\nunsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);\r\nvmemmap_free(start, end);\r\n}\r\nstatic void free_map_bootmem(struct page *memmap)\r\n{\r\nunsigned long start = (unsigned long)memmap;\r\nunsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);\r\nvmemmap_free(start, end);\r\n}\r\nstatic struct page *__kmalloc_section_memmap(void)\r\n{\r\nstruct page *page, *ret;\r\nunsigned long memmap_size = sizeof(struct page) * PAGES_PER_SECTION;\r\npage = alloc_pages(GFP_KERNEL|__GFP_NOWARN, get_order(memmap_size));\r\nif (page)\r\ngoto got_map_page;\r\nret = vmalloc(memmap_size);\r\nif (ret)\r\ngoto got_map_ptr;\r\nreturn NULL;\r\ngot_map_page:\r\nret = (struct page *)pfn_to_kaddr(page_to_pfn(page));\r\ngot_map_ptr:\r\nreturn ret;\r\n}\r\nstatic inline struct page *kmalloc_section_memmap(unsigned long pnum, int nid)\r\n{\r\nreturn __kmalloc_section_memmap();\r\n}\r\nstatic void __kfree_section_memmap(struct page *memmap)\r\n{\r\nif (is_vmalloc_addr(memmap))\r\nvfree(memmap);\r\nelse\r\nfree_pages((unsigned long)memmap,\r\nget_order(sizeof(struct page) * PAGES_PER_SECTION));\r\n}\r\nstatic void free_map_bootmem(struct page *memmap)\r\n{\r\nunsigned long maps_section_nr, removing_section_nr, i;\r\nunsigned long magic, nr_pages;\r\nstruct page *page = virt_to_page(memmap);\r\nnr_pages = PAGE_ALIGN(PAGES_PER_SECTION * sizeof(struct page))\r\n>> PAGE_SHIFT;\r\nfor (i = 0; i < nr_pages; i++, page++) {\r\nmagic = (unsigned long) page->lru.next;\r\nBUG_ON(magic == NODE_INFO);\r\nmaps_section_nr = pfn_to_section_nr(page_to_pfn(page));\r\nremoving_section_nr = page->private;\r\nif (maps_section_nr != removing_section_nr)\r\nput_page_bootmem(page);\r\n}\r\n}\r\nint __meminit sparse_add_one_section(struct zone *zone, unsigned long start_pfn)\r\n{\r\nunsigned long section_nr = pfn_to_section_nr(start_pfn);\r\nstruct pglist_data *pgdat = zone->zone_pgdat;\r\nstruct mem_section *ms;\r\nstruct page *memmap;\r\nunsigned long *usemap;\r\nunsigned long flags;\r\nint ret;\r\nret = sparse_index_init(section_nr, pgdat->node_id);\r\nif (ret < 0 && ret != -EEXIST)\r\nreturn ret;\r\nmemmap = kmalloc_section_memmap(section_nr, pgdat->node_id);\r\nif (!memmap)\r\nreturn -ENOMEM;\r\nusemap = __kmalloc_section_usemap();\r\nif (!usemap) {\r\n__kfree_section_memmap(memmap);\r\nreturn -ENOMEM;\r\n}\r\npgdat_resize_lock(pgdat, &flags);\r\nms = __pfn_to_section(start_pfn);\r\nif (ms->section_mem_map & SECTION_MARKED_PRESENT) {\r\nret = -EEXIST;\r\ngoto out;\r\n}\r\nmemset(memmap, 0, sizeof(struct page) * PAGES_PER_SECTION);\r\nms->section_mem_map |= SECTION_MARKED_PRESENT;\r\nret = sparse_init_one_section(ms, section_nr, memmap, usemap);\r\nout:\r\npgdat_resize_unlock(pgdat, &flags);\r\nif (ret <= 0) {\r\nkfree(usemap);\r\n__kfree_section_memmap(memmap);\r\n}\r\nreturn ret;\r\n}\r\nstatic void clear_hwpoisoned_pages(struct page *memmap, int nr_pages)\r\n{\r\nint i;\r\nif (!memmap)\r\nreturn;\r\nfor (i = 0; i < PAGES_PER_SECTION; i++) {\r\nif (PageHWPoison(&memmap[i])) {\r\natomic_long_sub(1, &num_poisoned_pages);\r\nClearPageHWPoison(&memmap[i]);\r\n}\r\n}\r\n}\r\nstatic inline void clear_hwpoisoned_pages(struct page *memmap, int nr_pages)\r\n{\r\n}\r\nstatic void free_section_usemap(struct page *memmap, unsigned long *usemap)\r\n{\r\nstruct page *usemap_page;\r\nif (!usemap)\r\nreturn;\r\nusemap_page = virt_to_page(usemap);\r\nif (PageSlab(usemap_page) || PageCompound(usemap_page)) {\r\nkfree(usemap);\r\nif (memmap)\r\n__kfree_section_memmap(memmap);\r\nreturn;\r\n}\r\nif (memmap)\r\nfree_map_bootmem(memmap);\r\n}\r\nvoid sparse_remove_one_section(struct zone *zone, struct mem_section *ms)\r\n{\r\nstruct page *memmap = NULL;\r\nunsigned long *usemap = NULL, flags;\r\nstruct pglist_data *pgdat = zone->zone_pgdat;\r\npgdat_resize_lock(pgdat, &flags);\r\nif (ms->section_mem_map) {\r\nusemap = ms->pageblock_flags;\r\nmemmap = sparse_decode_mem_map(ms->section_mem_map,\r\n__section_nr(ms));\r\nms->section_mem_map = 0;\r\nms->pageblock_flags = NULL;\r\n}\r\npgdat_resize_unlock(pgdat, &flags);\r\nclear_hwpoisoned_pages(memmap, PAGES_PER_SECTION);\r\nfree_section_usemap(memmap, usemap);\r\n}
