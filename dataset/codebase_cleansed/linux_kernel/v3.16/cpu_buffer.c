unsigned long oprofile_get_cpu_buffer_size(void)\r\n{\r\nreturn oprofile_cpu_buffer_size;\r\n}\r\nvoid oprofile_cpu_buffer_inc_smpl_lost(void)\r\n{\r\nstruct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);\r\ncpu_buf->sample_lost_overflow++;\r\n}\r\nvoid free_cpu_buffers(void)\r\n{\r\nif (op_ring_buffer)\r\nring_buffer_free(op_ring_buffer);\r\nop_ring_buffer = NULL;\r\n}\r\nint alloc_cpu_buffers(void)\r\n{\r\nint i;\r\nunsigned long buffer_size = oprofile_cpu_buffer_size;\r\nunsigned long byte_size = buffer_size * (sizeof(struct op_sample) +\r\nRB_EVENT_HDR_SIZE);\r\nop_ring_buffer = ring_buffer_alloc(byte_size, OP_BUFFER_FLAGS);\r\nif (!op_ring_buffer)\r\ngoto fail;\r\nfor_each_possible_cpu(i) {\r\nstruct oprofile_cpu_buffer *b = &per_cpu(op_cpu_buffer, i);\r\nb->last_task = NULL;\r\nb->last_is_kernel = -1;\r\nb->tracing = 0;\r\nb->buffer_size = buffer_size;\r\nb->sample_received = 0;\r\nb->sample_lost_overflow = 0;\r\nb->backtrace_aborted = 0;\r\nb->sample_invalid_eip = 0;\r\nb->cpu = i;\r\nINIT_DELAYED_WORK(&b->work, wq_sync_buffer);\r\n}\r\nreturn 0;\r\nfail:\r\nfree_cpu_buffers();\r\nreturn -ENOMEM;\r\n}\r\nvoid start_cpu_work(void)\r\n{\r\nint i;\r\nwork_enabled = 1;\r\nfor_each_online_cpu(i) {\r\nstruct oprofile_cpu_buffer *b = &per_cpu(op_cpu_buffer, i);\r\nschedule_delayed_work_on(i, &b->work, DEFAULT_TIMER_EXPIRE + i);\r\n}\r\n}\r\nvoid end_cpu_work(void)\r\n{\r\nwork_enabled = 0;\r\n}\r\nvoid flush_cpu_work(void)\r\n{\r\nint i;\r\nfor_each_online_cpu(i) {\r\nstruct oprofile_cpu_buffer *b = &per_cpu(op_cpu_buffer, i);\r\nflush_delayed_work(&b->work);\r\n}\r\n}\r\nstruct op_sample\r\n*op_cpu_buffer_write_reserve(struct op_entry *entry, unsigned long size)\r\n{\r\nentry->event = ring_buffer_lock_reserve\r\n(op_ring_buffer, sizeof(struct op_sample) +\r\nsize * sizeof(entry->sample->data[0]));\r\nif (!entry->event)\r\nreturn NULL;\r\nentry->sample = ring_buffer_event_data(entry->event);\r\nentry->size = size;\r\nentry->data = entry->sample->data;\r\nreturn entry->sample;\r\n}\r\nint op_cpu_buffer_write_commit(struct op_entry *entry)\r\n{\r\nreturn ring_buffer_unlock_commit(op_ring_buffer, entry->event);\r\n}\r\nstruct op_sample *op_cpu_buffer_read_entry(struct op_entry *entry, int cpu)\r\n{\r\nstruct ring_buffer_event *e;\r\ne = ring_buffer_consume(op_ring_buffer, cpu, NULL, NULL);\r\nif (!e)\r\nreturn NULL;\r\nentry->event = e;\r\nentry->sample = ring_buffer_event_data(e);\r\nentry->size = (ring_buffer_event_length(e) - sizeof(struct op_sample))\r\n/ sizeof(entry->sample->data[0]);\r\nentry->data = entry->sample->data;\r\nreturn entry->sample;\r\n}\r\nunsigned long op_cpu_buffer_entries(int cpu)\r\n{\r\nreturn ring_buffer_entries_cpu(op_ring_buffer, cpu);\r\n}\r\nstatic int\r\nop_add_code(struct oprofile_cpu_buffer *cpu_buf, unsigned long backtrace,\r\nint is_kernel, struct task_struct *task)\r\n{\r\nstruct op_entry entry;\r\nstruct op_sample *sample;\r\nunsigned long flags;\r\nint size;\r\nflags = 0;\r\nif (backtrace)\r\nflags |= TRACE_BEGIN;\r\nis_kernel = !!is_kernel;\r\nif (cpu_buf->last_is_kernel != is_kernel) {\r\ncpu_buf->last_is_kernel = is_kernel;\r\nflags |= KERNEL_CTX_SWITCH;\r\nif (is_kernel)\r\nflags |= IS_KERNEL;\r\n}\r\nif (cpu_buf->last_task != task) {\r\ncpu_buf->last_task = task;\r\nflags |= USER_CTX_SWITCH;\r\n}\r\nif (!flags)\r\nreturn 0;\r\nif (flags & USER_CTX_SWITCH)\r\nsize = 1;\r\nelse\r\nsize = 0;\r\nsample = op_cpu_buffer_write_reserve(&entry, size);\r\nif (!sample)\r\nreturn -ENOMEM;\r\nsample->eip = ESCAPE_CODE;\r\nsample->event = flags;\r\nif (size)\r\nop_cpu_buffer_add_data(&entry, (unsigned long)task);\r\nop_cpu_buffer_write_commit(&entry);\r\nreturn 0;\r\n}\r\nstatic inline int\r\nop_add_sample(struct oprofile_cpu_buffer *cpu_buf,\r\nunsigned long pc, unsigned long event)\r\n{\r\nstruct op_entry entry;\r\nstruct op_sample *sample;\r\nsample = op_cpu_buffer_write_reserve(&entry, 0);\r\nif (!sample)\r\nreturn -ENOMEM;\r\nsample->eip = pc;\r\nsample->event = event;\r\nreturn op_cpu_buffer_write_commit(&entry);\r\n}\r\nstatic int\r\nlog_sample(struct oprofile_cpu_buffer *cpu_buf, unsigned long pc,\r\nunsigned long backtrace, int is_kernel, unsigned long event,\r\nstruct task_struct *task)\r\n{\r\nstruct task_struct *tsk = task ? task : current;\r\ncpu_buf->sample_received++;\r\nif (pc == ESCAPE_CODE) {\r\ncpu_buf->sample_invalid_eip++;\r\nreturn 0;\r\n}\r\nif (op_add_code(cpu_buf, backtrace, is_kernel, tsk))\r\ngoto fail;\r\nif (op_add_sample(cpu_buf, pc, event))\r\ngoto fail;\r\nreturn 1;\r\nfail:\r\ncpu_buf->sample_lost_overflow++;\r\nreturn 0;\r\n}\r\nstatic inline void oprofile_begin_trace(struct oprofile_cpu_buffer *cpu_buf)\r\n{\r\ncpu_buf->tracing = 1;\r\n}\r\nstatic inline void oprofile_end_trace(struct oprofile_cpu_buffer *cpu_buf)\r\n{\r\ncpu_buf->tracing = 0;\r\n}\r\nstatic inline void\r\n__oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,\r\nunsigned long event, int is_kernel,\r\nstruct task_struct *task)\r\n{\r\nstruct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);\r\nunsigned long backtrace = oprofile_backtrace_depth;\r\nif (!log_sample(cpu_buf, pc, backtrace, is_kernel, event, task))\r\nreturn;\r\nif (!backtrace)\r\nreturn;\r\noprofile_begin_trace(cpu_buf);\r\noprofile_ops.backtrace(regs, backtrace);\r\noprofile_end_trace(cpu_buf);\r\n}\r\nvoid oprofile_add_ext_hw_sample(unsigned long pc, struct pt_regs * const regs,\r\nunsigned long event, int is_kernel,\r\nstruct task_struct *task)\r\n{\r\n__oprofile_add_ext_sample(pc, regs, event, is_kernel, task);\r\n}\r\nvoid oprofile_add_ext_sample(unsigned long pc, struct pt_regs * const regs,\r\nunsigned long event, int is_kernel)\r\n{\r\n__oprofile_add_ext_sample(pc, regs, event, is_kernel, NULL);\r\n}\r\nvoid oprofile_add_sample(struct pt_regs * const regs, unsigned long event)\r\n{\r\nint is_kernel;\r\nunsigned long pc;\r\nif (likely(regs)) {\r\nis_kernel = !user_mode(regs);\r\npc = profile_pc(regs);\r\n} else {\r\nis_kernel = 0;\r\npc = ESCAPE_CODE;\r\n}\r\n__oprofile_add_ext_sample(pc, regs, event, is_kernel, NULL);\r\n}\r\nvoid\r\noprofile_write_reserve(struct op_entry *entry, struct pt_regs * const regs,\r\nunsigned long pc, int code, int size)\r\n{\r\nstruct op_sample *sample;\r\nint is_kernel = !user_mode(regs);\r\nstruct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);\r\ncpu_buf->sample_received++;\r\nif (op_add_code(cpu_buf, 0, is_kernel, current))\r\ngoto fail;\r\nsample = op_cpu_buffer_write_reserve(entry, size + 2);\r\nif (!sample)\r\ngoto fail;\r\nsample->eip = ESCAPE_CODE;\r\nsample->event = 0;\r\nop_cpu_buffer_add_data(entry, code);\r\nop_cpu_buffer_add_data(entry, pc);\r\nreturn;\r\nfail:\r\nentry->event = NULL;\r\ncpu_buf->sample_lost_overflow++;\r\n}\r\nint oprofile_add_data(struct op_entry *entry, unsigned long val)\r\n{\r\nif (!entry->event)\r\nreturn 0;\r\nreturn op_cpu_buffer_add_data(entry, val);\r\n}\r\nint oprofile_add_data64(struct op_entry *entry, u64 val)\r\n{\r\nif (!entry->event)\r\nreturn 0;\r\nif (op_cpu_buffer_get_size(entry) < 2)\r\nreturn 0;\r\nif (!op_cpu_buffer_add_data(entry, (u32)val))\r\nreturn 0;\r\nreturn op_cpu_buffer_add_data(entry, (u32)(val >> 32));\r\n}\r\nint oprofile_write_commit(struct op_entry *entry)\r\n{\r\nif (!entry->event)\r\nreturn -EINVAL;\r\nreturn op_cpu_buffer_write_commit(entry);\r\n}\r\nvoid oprofile_add_pc(unsigned long pc, int is_kernel, unsigned long event)\r\n{\r\nstruct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);\r\nlog_sample(cpu_buf, pc, 0, is_kernel, event, NULL);\r\n}\r\nvoid oprofile_add_trace(unsigned long pc)\r\n{\r\nstruct oprofile_cpu_buffer *cpu_buf = &__get_cpu_var(op_cpu_buffer);\r\nif (!cpu_buf->tracing)\r\nreturn;\r\nif (pc == ESCAPE_CODE)\r\ngoto fail;\r\nif (op_add_sample(cpu_buf, pc, 0))\r\ngoto fail;\r\nreturn;\r\nfail:\r\ncpu_buf->tracing = 0;\r\ncpu_buf->backtrace_aborted++;\r\nreturn;\r\n}\r\nstatic void wq_sync_buffer(struct work_struct *work)\r\n{\r\nstruct oprofile_cpu_buffer *b =\r\ncontainer_of(work, struct oprofile_cpu_buffer, work.work);\r\nif (b->cpu != smp_processor_id() && !cpu_online(b->cpu)) {\r\ncancel_delayed_work(&b->work);\r\nreturn;\r\n}\r\nsync_buffer(b->cpu);\r\nif (work_enabled)\r\nschedule_delayed_work(&b->work, DEFAULT_TIMER_EXPIRE);\r\n}
