static int ir_raw_event_thread(void *data)\r\n{\r\nstruct ir_raw_event ev;\r\nstruct ir_raw_handler *handler;\r\nstruct ir_raw_event_ctrl *raw = (struct ir_raw_event_ctrl *)data;\r\nint retval;\r\nwhile (!kthread_should_stop()) {\r\nspin_lock_irq(&raw->lock);\r\nretval = kfifo_len(&raw->kfifo);\r\nif (retval < sizeof(ev)) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (kthread_should_stop())\r\nset_current_state(TASK_RUNNING);\r\nspin_unlock_irq(&raw->lock);\r\nschedule();\r\ncontinue;\r\n}\r\nretval = kfifo_out(&raw->kfifo, &ev, sizeof(ev));\r\nspin_unlock_irq(&raw->lock);\r\nmutex_lock(&ir_raw_handler_lock);\r\nlist_for_each_entry(handler, &ir_raw_handler_list, list)\r\nhandler->decode(raw->dev, ev);\r\nraw->prev_ev = ev;\r\nmutex_unlock(&ir_raw_handler_lock);\r\n}\r\nreturn 0;\r\n}\r\nint ir_raw_event_store(struct rc_dev *dev, struct ir_raw_event *ev)\r\n{\r\nif (!dev->raw)\r\nreturn -EINVAL;\r\nIR_dprintk(2, "sample: (%05dus %s)\n",\r\nTO_US(ev->duration), TO_STR(ev->pulse));\r\nif (kfifo_in(&dev->raw->kfifo, ev, sizeof(*ev)) != sizeof(*ev))\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nint ir_raw_event_store_edge(struct rc_dev *dev, enum raw_event_type type)\r\n{\r\nktime_t now;\r\ns64 delta;\r\nDEFINE_IR_RAW_EVENT(ev);\r\nint rc = 0;\r\nint delay;\r\nif (!dev->raw)\r\nreturn -EINVAL;\r\nnow = ktime_get();\r\ndelta = ktime_to_ns(ktime_sub(now, dev->raw->last_event));\r\ndelay = MS_TO_NS(dev->input_dev->rep[REP_DELAY]);\r\nif (delta > delay || !dev->raw->last_type)\r\ntype |= IR_START_EVENT;\r\nelse\r\nev.duration = delta;\r\nif (type & IR_START_EVENT)\r\nir_raw_event_reset(dev);\r\nelse if (dev->raw->last_type & IR_SPACE) {\r\nev.pulse = false;\r\nrc = ir_raw_event_store(dev, &ev);\r\n} else if (dev->raw->last_type & IR_PULSE) {\r\nev.pulse = true;\r\nrc = ir_raw_event_store(dev, &ev);\r\n} else\r\nreturn 0;\r\ndev->raw->last_event = now;\r\ndev->raw->last_type = type;\r\nreturn rc;\r\n}\r\nint ir_raw_event_store_with_filter(struct rc_dev *dev, struct ir_raw_event *ev)\r\n{\r\nif (!dev->raw)\r\nreturn -EINVAL;\r\nif (dev->idle && !ev->pulse)\r\nreturn 0;\r\nelse if (dev->idle)\r\nir_raw_event_set_idle(dev, false);\r\nif (!dev->raw->this_ev.duration)\r\ndev->raw->this_ev = *ev;\r\nelse if (ev->pulse == dev->raw->this_ev.pulse)\r\ndev->raw->this_ev.duration += ev->duration;\r\nelse {\r\nir_raw_event_store(dev, &dev->raw->this_ev);\r\ndev->raw->this_ev = *ev;\r\n}\r\nif (!ev->pulse && dev->timeout &&\r\ndev->raw->this_ev.duration >= dev->timeout)\r\nir_raw_event_set_idle(dev, true);\r\nreturn 1;\r\n}\r\nvoid ir_raw_event_set_idle(struct rc_dev *dev, bool idle)\r\n{\r\nif (!dev->raw)\r\nreturn;\r\nIR_dprintk(2, "%s idle mode\n", idle ? "enter" : "leave");\r\nif (idle) {\r\ndev->raw->this_ev.timeout = true;\r\nir_raw_event_store(dev, &dev->raw->this_ev);\r\ninit_ir_raw_event(&dev->raw->this_ev);\r\n}\r\nif (dev->s_idle)\r\ndev->s_idle(dev, idle);\r\ndev->idle = idle;\r\n}\r\nvoid ir_raw_event_handle(struct rc_dev *dev)\r\n{\r\nunsigned long flags;\r\nif (!dev->raw)\r\nreturn;\r\nspin_lock_irqsave(&dev->raw->lock, flags);\r\nwake_up_process(dev->raw->thread);\r\nspin_unlock_irqrestore(&dev->raw->lock, flags);\r\n}\r\nu64\r\nir_raw_get_allowed_protocols(void)\r\n{\r\nu64 protocols;\r\nmutex_lock(&ir_raw_handler_lock);\r\nprotocols = available_protocols;\r\nmutex_unlock(&ir_raw_handler_lock);\r\nreturn protocols;\r\n}\r\nint ir_raw_event_register(struct rc_dev *dev)\r\n{\r\nint rc;\r\nstruct ir_raw_handler *handler;\r\nif (!dev)\r\nreturn -EINVAL;\r\ndev->raw = kzalloc(sizeof(*dev->raw), GFP_KERNEL);\r\nif (!dev->raw)\r\nreturn -ENOMEM;\r\ndev->raw->dev = dev;\r\nrc_set_enabled_protocols(dev, ~0);\r\nrc = kfifo_alloc(&dev->raw->kfifo,\r\nsizeof(struct ir_raw_event) * MAX_IR_EVENT_SIZE,\r\nGFP_KERNEL);\r\nif (rc < 0)\r\ngoto out;\r\nspin_lock_init(&dev->raw->lock);\r\ndev->raw->thread = kthread_run(ir_raw_event_thread, dev->raw,\r\n"rc%ld", dev->devno);\r\nif (IS_ERR(dev->raw->thread)) {\r\nrc = PTR_ERR(dev->raw->thread);\r\ngoto out;\r\n}\r\nmutex_lock(&ir_raw_handler_lock);\r\nlist_add_tail(&dev->raw->list, &ir_raw_client_list);\r\nlist_for_each_entry(handler, &ir_raw_handler_list, list)\r\nif (handler->raw_register)\r\nhandler->raw_register(dev);\r\nmutex_unlock(&ir_raw_handler_lock);\r\nreturn 0;\r\nout:\r\nkfree(dev->raw);\r\ndev->raw = NULL;\r\nreturn rc;\r\n}\r\nvoid ir_raw_event_unregister(struct rc_dev *dev)\r\n{\r\nstruct ir_raw_handler *handler;\r\nif (!dev || !dev->raw)\r\nreturn;\r\nkthread_stop(dev->raw->thread);\r\nmutex_lock(&ir_raw_handler_lock);\r\nlist_del(&dev->raw->list);\r\nlist_for_each_entry(handler, &ir_raw_handler_list, list)\r\nif (handler->raw_unregister)\r\nhandler->raw_unregister(dev);\r\nmutex_unlock(&ir_raw_handler_lock);\r\nkfifo_free(&dev->raw->kfifo);\r\nkfree(dev->raw);\r\ndev->raw = NULL;\r\n}\r\nint ir_raw_handler_register(struct ir_raw_handler *ir_raw_handler)\r\n{\r\nstruct ir_raw_event_ctrl *raw;\r\nmutex_lock(&ir_raw_handler_lock);\r\nlist_add_tail(&ir_raw_handler->list, &ir_raw_handler_list);\r\nif (ir_raw_handler->raw_register)\r\nlist_for_each_entry(raw, &ir_raw_client_list, list)\r\nir_raw_handler->raw_register(raw->dev);\r\navailable_protocols |= ir_raw_handler->protocols;\r\nmutex_unlock(&ir_raw_handler_lock);\r\nreturn 0;\r\n}\r\nvoid ir_raw_handler_unregister(struct ir_raw_handler *ir_raw_handler)\r\n{\r\nstruct ir_raw_event_ctrl *raw;\r\nmutex_lock(&ir_raw_handler_lock);\r\nlist_del(&ir_raw_handler->list);\r\nif (ir_raw_handler->raw_unregister)\r\nlist_for_each_entry(raw, &ir_raw_client_list, list)\r\nir_raw_handler->raw_unregister(raw->dev);\r\navailable_protocols &= ~ir_raw_handler->protocols;\r\nmutex_unlock(&ir_raw_handler_lock);\r\n}\r\nvoid ir_raw_init(void)\r\n{\r\nload_nec_decode();\r\nload_rc5_decode();\r\nload_rc6_decode();\r\nload_jvc_decode();\r\nload_sony_decode();\r\nload_sanyo_decode();\r\nload_sharp_decode();\r\nload_mce_kbd_decode();\r\nload_lirc_codec();\r\n}
