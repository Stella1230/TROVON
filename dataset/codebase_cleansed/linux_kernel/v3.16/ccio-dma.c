static int\r\nccio_alloc_range(struct ioc *ioc, struct device *dev, size_t size)\r\n{\r\nunsigned int pages_needed = size >> IOVP_SHIFT;\r\nunsigned int res_idx;\r\nunsigned long boundary_size;\r\n#ifdef CCIO_COLLECT_STATS\r\nunsigned long cr_start = mfctl(16);\r\n#endif\r\nBUG_ON(pages_needed == 0);\r\nBUG_ON((pages_needed * IOVP_SIZE) > DMA_CHUNK_SIZE);\r\nDBG_RES("%s() size: %d pages_needed %d\n",\r\n__func__, size, pages_needed);\r\nboundary_size = ALIGN((unsigned long long)dma_get_seg_boundary(dev) + 1,\r\n1ULL << IOVP_SHIFT) >> IOVP_SHIFT;\r\nif (pages_needed <= 8) {\r\n#if 0\r\nunsigned long mask = ~(~0UL >> pages_needed);\r\nCCIO_FIND_FREE_MAPPING(ioc, res_idx, mask, 8);\r\n#else\r\nCCIO_FIND_FREE_MAPPING(ioc, res_idx, 0xff, 8);\r\n#endif\r\n} else if (pages_needed <= 16) {\r\nCCIO_FIND_FREE_MAPPING(ioc, res_idx, 0xffff, 16);\r\n} else if (pages_needed <= 32) {\r\nCCIO_FIND_FREE_MAPPING(ioc, res_idx, ~(unsigned int)0, 32);\r\n#ifdef __LP64__\r\n} else if (pages_needed <= 64) {\r\nCCIO_FIND_FREE_MAPPING(ioc, res_idx, ~0UL, 64);\r\n#endif\r\n} else {\r\npanic("%s: %s() Too many pages to map. pages_needed: %u\n",\r\n__FILE__, __func__, pages_needed);\r\n}\r\npanic("%s: %s() I/O MMU is out of mapping resources.\n", __FILE__,\r\n__func__);\r\nresource_found:\r\nDBG_RES("%s() res_idx %d res_hint: %d\n",\r\n__func__, res_idx, ioc->res_hint);\r\n#ifdef CCIO_COLLECT_STATS\r\n{\r\nunsigned long cr_end = mfctl(16);\r\nunsigned long tmp = cr_end - cr_start;\r\ncr_start = (cr_end < cr_start) ? -(tmp) : (tmp);\r\n}\r\nioc->avg_search[ioc->avg_idx++] = cr_start;\r\nioc->avg_idx &= CCIO_SEARCH_SAMPLE - 1;\r\nioc->used_pages += pages_needed;\r\n#endif\r\nreturn res_idx << 3;\r\n}\r\nstatic void\r\nccio_free_range(struct ioc *ioc, dma_addr_t iova, unsigned long pages_mapped)\r\n{\r\nunsigned long iovp = CCIO_IOVP(iova);\r\nunsigned int res_idx = PDIR_INDEX(iovp) >> 3;\r\nBUG_ON(pages_mapped == 0);\r\nBUG_ON((pages_mapped * IOVP_SIZE) > DMA_CHUNK_SIZE);\r\nBUG_ON(pages_mapped > BITS_PER_LONG);\r\nDBG_RES("%s(): res_idx: %d pages_mapped %d\n",\r\n__func__, res_idx, pages_mapped);\r\n#ifdef CCIO_COLLECT_STATS\r\nioc->used_pages -= pages_mapped;\r\n#endif\r\nif(pages_mapped <= 8) {\r\n#if 0\r\nunsigned long mask = ~(~0UL >> pages_mapped);\r\nCCIO_FREE_MAPPINGS(ioc, res_idx, mask, 8);\r\n#else\r\nCCIO_FREE_MAPPINGS(ioc, res_idx, 0xffUL, 8);\r\n#endif\r\n} else if(pages_mapped <= 16) {\r\nCCIO_FREE_MAPPINGS(ioc, res_idx, 0xffffUL, 16);\r\n} else if(pages_mapped <= 32) {\r\nCCIO_FREE_MAPPINGS(ioc, res_idx, ~(unsigned int)0, 32);\r\n#ifdef __LP64__\r\n} else if(pages_mapped <= 64) {\r\nCCIO_FREE_MAPPINGS(ioc, res_idx, ~0UL, 64);\r\n#endif\r\n} else {\r\npanic("%s:%s() Too many pages to unmap.\n", __FILE__,\r\n__func__);\r\n}\r\n}\r\nstatic void CCIO_INLINE\r\nccio_io_pdir_entry(u64 *pdir_ptr, space_t sid, unsigned long vba,\r\nunsigned long hints)\r\n{\r\nregister unsigned long pa;\r\nregister unsigned long ci;\r\nBUG_ON(sid != KERNEL_SPACE);\r\nmtsp(sid,1);\r\npa = virt_to_phys(vba);\r\nasm volatile("depw %1,31,12,%0" : "+r" (pa) : "r" (hints));\r\n((u32 *)pdir_ptr)[1] = (u32) pa;\r\n#ifdef __LP64__\r\nasm volatile ("extrd,u %1,15,4,%0" : "=r" (ci) : "r" (pa));\r\nasm volatile ("extrd,u %1,31,16,%0" : "+r" (pa) : "r" (pa));\r\nasm volatile ("depd %1,35,4,%0" : "+r" (pa) : "r" (ci));\r\n#else\r\npa = 0;\r\n#endif\r\nasm volatile ("lci %%r0(%%sr1, %1), %0" : "=r" (ci) : "r" (vba));\r\nasm volatile ("extru %1,19,12,%0" : "+r" (ci) : "r" (ci));\r\nasm volatile ("depw %1,15,12,%0" : "+r" (pa) : "r" (ci));\r\n((u32 *)pdir_ptr)[0] = (u32) pa;\r\nasm volatile("fdc %%r0(%0)" : : "r" (pdir_ptr));\r\nasm volatile("sync");\r\n}\r\nstatic CCIO_INLINE void\r\nccio_clear_io_tlb(struct ioc *ioc, dma_addr_t iovp, size_t byte_cnt)\r\n{\r\nu32 chain_size = 1 << ioc->chainid_shift;\r\niovp &= IOVP_MASK;\r\nbyte_cnt += chain_size;\r\nwhile(byte_cnt > chain_size) {\r\nWRITE_U32(CMD_TLB_PURGE | iovp, &ioc->ioc_regs->io_command);\r\niovp += chain_size;\r\nbyte_cnt -= chain_size;\r\n}\r\n}\r\nstatic CCIO_INLINE void\r\nccio_mark_invalid(struct ioc *ioc, dma_addr_t iova, size_t byte_cnt)\r\n{\r\nu32 iovp = (u32)CCIO_IOVP(iova);\r\nsize_t saved_byte_cnt;\r\nsaved_byte_cnt = byte_cnt = ALIGN(byte_cnt, IOVP_SIZE);\r\nwhile(byte_cnt > 0) {\r\nunsigned int idx = PDIR_INDEX(iovp);\r\nchar *pdir_ptr = (char *) &(ioc->pdir_base[idx]);\r\nBUG_ON(idx >= (ioc->pdir_size / sizeof(u64)));\r\npdir_ptr[7] = 0;\r\nasm volatile("fdc %%r0(%0)" : : "r" (pdir_ptr[7]));\r\niovp += IOVP_SIZE;\r\nbyte_cnt -= IOVP_SIZE;\r\n}\r\nasm volatile("sync");\r\nccio_clear_io_tlb(ioc, CCIO_IOVP(iova), saved_byte_cnt);\r\n}\r\nstatic int\r\nccio_dma_supported(struct device *dev, u64 mask)\r\n{\r\nif(dev == NULL) {\r\nprintk(KERN_ERR MODULE_NAME ": EISA/ISA/et al not supported\n");\r\nBUG();\r\nreturn 0;\r\n}\r\nreturn (int)(mask == 0xffffffffUL);\r\n}\r\nstatic dma_addr_t\r\nccio_map_single(struct device *dev, void *addr, size_t size,\r\nenum dma_data_direction direction)\r\n{\r\nint idx;\r\nstruct ioc *ioc;\r\nunsigned long flags;\r\ndma_addr_t iovp;\r\ndma_addr_t offset;\r\nu64 *pdir_start;\r\nunsigned long hint = hint_lookup[(int)direction];\r\nBUG_ON(!dev);\r\nioc = GET_IOC(dev);\r\nBUG_ON(size <= 0);\r\noffset = ((unsigned long) addr) & ~IOVP_MASK;\r\nsize = ALIGN(size + offset, IOVP_SIZE);\r\nspin_lock_irqsave(&ioc->res_lock, flags);\r\n#ifdef CCIO_COLLECT_STATS\r\nioc->msingle_calls++;\r\nioc->msingle_pages += size >> IOVP_SHIFT;\r\n#endif\r\nidx = ccio_alloc_range(ioc, dev, size);\r\niovp = (dma_addr_t)MKIOVP(idx);\r\npdir_start = &(ioc->pdir_base[idx]);\r\nDBG_RUN("%s() 0x%p -> 0x%lx size: %0x%x\n",\r\n__func__, addr, (long)iovp | offset, size);\r\nif((size % L1_CACHE_BYTES) || ((unsigned long)addr % L1_CACHE_BYTES))\r\nhint |= HINT_SAFE_DMA;\r\nwhile(size > 0) {\r\nccio_io_pdir_entry(pdir_start, KERNEL_SPACE, (unsigned long)addr, hint);\r\nDBG_RUN(" pdir %p %08x%08x\n",\r\npdir_start,\r\n(u32) (((u32 *) pdir_start)[0]),\r\n(u32) (((u32 *) pdir_start)[1]));\r\n++pdir_start;\r\naddr += IOVP_SIZE;\r\nsize -= IOVP_SIZE;\r\n}\r\nspin_unlock_irqrestore(&ioc->res_lock, flags);\r\nreturn CCIO_IOVA(iovp, offset);\r\n}\r\nstatic void\r\nccio_unmap_single(struct device *dev, dma_addr_t iova, size_t size,\r\nenum dma_data_direction direction)\r\n{\r\nstruct ioc *ioc;\r\nunsigned long flags;\r\ndma_addr_t offset = iova & ~IOVP_MASK;\r\nBUG_ON(!dev);\r\nioc = GET_IOC(dev);\r\nDBG_RUN("%s() iovp 0x%lx/%x\n",\r\n__func__, (long)iova, size);\r\niova ^= offset;\r\nsize += offset;\r\nsize = ALIGN(size, IOVP_SIZE);\r\nspin_lock_irqsave(&ioc->res_lock, flags);\r\n#ifdef CCIO_COLLECT_STATS\r\nioc->usingle_calls++;\r\nioc->usingle_pages += size >> IOVP_SHIFT;\r\n#endif\r\nccio_mark_invalid(ioc, iova, size);\r\nccio_free_range(ioc, iova, (size >> IOVP_SHIFT));\r\nspin_unlock_irqrestore(&ioc->res_lock, flags);\r\n}\r\nstatic void *\r\nccio_alloc_consistent(struct device *dev, size_t size, dma_addr_t *dma_handle, gfp_t flag)\r\n{\r\nvoid *ret;\r\n#if 0\r\nif(!hwdev) {\r\n*dma_handle = 0;\r\nreturn 0;\r\n}\r\n#endif\r\nret = (void *) __get_free_pages(flag, get_order(size));\r\nif (ret) {\r\nmemset(ret, 0, size);\r\n*dma_handle = ccio_map_single(dev, ret, size, PCI_DMA_BIDIRECTIONAL);\r\n}\r\nreturn ret;\r\n}\r\nstatic void\r\nccio_free_consistent(struct device *dev, size_t size, void *cpu_addr,\r\ndma_addr_t dma_handle)\r\n{\r\nccio_unmap_single(dev, dma_handle, size, 0);\r\nfree_pages((unsigned long)cpu_addr, get_order(size));\r\n}\r\nstatic int\r\nccio_map_sg(struct device *dev, struct scatterlist *sglist, int nents,\r\nenum dma_data_direction direction)\r\n{\r\nstruct ioc *ioc;\r\nint coalesced, filled = 0;\r\nunsigned long flags;\r\nunsigned long hint = hint_lookup[(int)direction];\r\nunsigned long prev_len = 0, current_len = 0;\r\nint i;\r\nBUG_ON(!dev);\r\nioc = GET_IOC(dev);\r\nDBG_RUN_SG("%s() START %d entries\n", __func__, nents);\r\nif (nents == 1) {\r\nsg_dma_address(sglist) = ccio_map_single(dev,\r\n(void *)sg_virt_addr(sglist), sglist->length,\r\ndirection);\r\nsg_dma_len(sglist) = sglist->length;\r\nreturn 1;\r\n}\r\nfor(i = 0; i < nents; i++)\r\nprev_len += sglist[i].length;\r\nspin_lock_irqsave(&ioc->res_lock, flags);\r\n#ifdef CCIO_COLLECT_STATS\r\nioc->msg_calls++;\r\n#endif\r\ncoalesced = iommu_coalesce_chunks(ioc, dev, sglist, nents, ccio_alloc_range);\r\nfilled = iommu_fill_pdir(ioc, sglist, nents, hint, ccio_io_pdir_entry);\r\nspin_unlock_irqrestore(&ioc->res_lock, flags);\r\nBUG_ON(coalesced != filled);\r\nDBG_RUN_SG("%s() DONE %d mappings\n", __func__, filled);\r\nfor (i = 0; i < filled; i++)\r\ncurrent_len += sg_dma_len(sglist + i);\r\nBUG_ON(current_len != prev_len);\r\nreturn filled;\r\n}\r\nstatic void\r\nccio_unmap_sg(struct device *dev, struct scatterlist *sglist, int nents,\r\nenum dma_data_direction direction)\r\n{\r\nstruct ioc *ioc;\r\nBUG_ON(!dev);\r\nioc = GET_IOC(dev);\r\nDBG_RUN_SG("%s() START %d entries, %08lx,%x\n",\r\n__func__, nents, sg_virt_addr(sglist), sglist->length);\r\n#ifdef CCIO_COLLECT_STATS\r\nioc->usg_calls++;\r\n#endif\r\nwhile(sg_dma_len(sglist) && nents--) {\r\n#ifdef CCIO_COLLECT_STATS\r\nioc->usg_pages += sg_dma_len(sglist) >> PAGE_SHIFT;\r\n#endif\r\nccio_unmap_single(dev, sg_dma_address(sglist),\r\nsg_dma_len(sglist), direction);\r\n++sglist;\r\n}\r\nDBG_RUN_SG("%s() DONE (nents %d)\n", __func__, nents);\r\n}\r\nstatic int ccio_proc_info(struct seq_file *m, void *p)\r\n{\r\nint len = 0;\r\nstruct ioc *ioc = ioc_list;\r\nwhile (ioc != NULL) {\r\nunsigned int total_pages = ioc->res_size << 3;\r\n#ifdef CCIO_COLLECT_STATS\r\nunsigned long avg = 0, min, max;\r\nint j;\r\n#endif\r\nlen += seq_printf(m, "%s\n", ioc->name);\r\nlen += seq_printf(m, "Cujo 2.0 bug : %s\n",\r\n(ioc->cujo20_bug ? "yes" : "no"));\r\nlen += seq_printf(m, "IO PDIR size : %d bytes (%d entries)\n",\r\ntotal_pages * 8, total_pages);\r\n#ifdef CCIO_COLLECT_STATS\r\nlen += seq_printf(m, "IO PDIR entries : %ld free %ld used (%d%%)\n",\r\ntotal_pages - ioc->used_pages, ioc->used_pages,\r\n(int)(ioc->used_pages * 100 / total_pages));\r\n#endif\r\nlen += seq_printf(m, "Resource bitmap : %d bytes (%d pages)\n",\r\nioc->res_size, total_pages);\r\n#ifdef CCIO_COLLECT_STATS\r\nmin = max = ioc->avg_search[0];\r\nfor(j = 0; j < CCIO_SEARCH_SAMPLE; ++j) {\r\navg += ioc->avg_search[j];\r\nif(ioc->avg_search[j] > max)\r\nmax = ioc->avg_search[j];\r\nif(ioc->avg_search[j] < min)\r\nmin = ioc->avg_search[j];\r\n}\r\navg /= CCIO_SEARCH_SAMPLE;\r\nlen += seq_printf(m, " Bitmap search : %ld/%ld/%ld (min/avg/max CPU Cycles)\n",\r\nmin, avg, max);\r\nlen += seq_printf(m, "pci_map_single(): %8ld calls %8ld pages (avg %d/1000)\n",\r\nioc->msingle_calls, ioc->msingle_pages,\r\n(int)((ioc->msingle_pages * 1000)/ioc->msingle_calls));\r\nmin = ioc->usingle_calls - ioc->usg_calls;\r\nmax = ioc->usingle_pages - ioc->usg_pages;\r\nlen += seq_printf(m, "pci_unmap_single: %8ld calls %8ld pages (avg %d/1000)\n",\r\nmin, max, (int)((max * 1000)/min));\r\nlen += seq_printf(m, "pci_map_sg() : %8ld calls %8ld pages (avg %d/1000)\n",\r\nioc->msg_calls, ioc->msg_pages,\r\n(int)((ioc->msg_pages * 1000)/ioc->msg_calls));\r\nlen += seq_printf(m, "pci_unmap_sg() : %8ld calls %8ld pages (avg %d/1000)\n\n\n",\r\nioc->usg_calls, ioc->usg_pages,\r\n(int)((ioc->usg_pages * 1000)/ioc->usg_calls));\r\n#endif\r\nioc = ioc->next;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ccio_proc_info_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, &ccio_proc_info, NULL);\r\n}\r\nstatic int ccio_proc_bitmap_info(struct seq_file *m, void *p)\r\n{\r\nint len = 0;\r\nstruct ioc *ioc = ioc_list;\r\nwhile (ioc != NULL) {\r\nu32 *res_ptr = (u32 *)ioc->res_map;\r\nint j;\r\nfor (j = 0; j < (ioc->res_size / sizeof(u32)); j++) {\r\nif ((j & 7) == 0)\r\nlen += seq_puts(m, "\n ");\r\nlen += seq_printf(m, "%08x", *res_ptr);\r\nres_ptr++;\r\n}\r\nlen += seq_puts(m, "\n\n");\r\nioc = ioc->next;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ccio_proc_bitmap_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, &ccio_proc_bitmap_info, NULL);\r\n}\r\nstatic struct ioc * ccio_find_ioc(int hw_path)\r\n{\r\nint i;\r\nstruct ioc *ioc;\r\nioc = ioc_list;\r\nfor (i = 0; i < ioc_count; i++) {\r\nif (ioc->hw_path == hw_path)\r\nreturn ioc;\r\nioc = ioc->next;\r\n}\r\nreturn NULL;\r\n}\r\nvoid * ccio_get_iommu(const struct parisc_device *dev)\r\n{\r\ndev = find_pa_parent_type(dev, HPHW_IOA);\r\nif (!dev)\r\nreturn NULL;\r\nreturn ccio_find_ioc(dev->hw_path);\r\n}\r\nvoid ccio_cujo20_fixup(struct parisc_device *cujo, u32 iovp)\r\n{\r\nunsigned int idx;\r\nstruct parisc_device *dev = parisc_parent(cujo);\r\nstruct ioc *ioc = ccio_get_iommu(dev);\r\nu8 *res_ptr;\r\nioc->cujo20_bug = 1;\r\nres_ptr = ioc->res_map;\r\nidx = PDIR_INDEX(iovp) >> 3;\r\nwhile (idx < ioc->res_size) {\r\nres_ptr[idx] |= 0xff;\r\nidx += PDIR_INDEX(CUJO_20_STEP) >> 3;\r\n}\r\n}\r\nstatic void\r\nccio_ioc_init(struct ioc *ioc)\r\n{\r\nint i;\r\nunsigned int iov_order;\r\nu32 iova_space_size;\r\niova_space_size = (u32) (totalram_pages / count_parisc_driver(&ccio_driver));\r\nif (iova_space_size < (1 << (20 - PAGE_SHIFT))) {\r\niova_space_size = 1 << (20 - PAGE_SHIFT);\r\n#ifdef __LP64__\r\n} else if (iova_space_size > (1 << (30 - PAGE_SHIFT))) {\r\niova_space_size = 1 << (30 - PAGE_SHIFT);\r\n#endif\r\n}\r\niov_order = get_order(iova_space_size << PAGE_SHIFT);\r\niova_space_size = 1 << (iov_order + PAGE_SHIFT);\r\nioc->pdir_size = (iova_space_size / IOVP_SIZE) * sizeof(u64);\r\nBUG_ON(ioc->pdir_size > 8 * 1024 * 1024);\r\nBUG_ON((1 << get_order(ioc->pdir_size)) != (ioc->pdir_size >> PAGE_SHIFT));\r\nDBG_INIT("%s() hpa 0x%p mem %luMB IOV %dMB (%d bits)\n",\r\n__func__, ioc->ioc_regs,\r\n(unsigned long) totalram_pages >> (20 - PAGE_SHIFT),\r\niova_space_size>>20,\r\niov_order + PAGE_SHIFT);\r\nioc->pdir_base = (u64 *)__get_free_pages(GFP_KERNEL,\r\nget_order(ioc->pdir_size));\r\nif(NULL == ioc->pdir_base) {\r\npanic("%s() could not allocate I/O Page Table\n", __func__);\r\n}\r\nmemset(ioc->pdir_base, 0, ioc->pdir_size);\r\nBUG_ON((((unsigned long)ioc->pdir_base) & PAGE_MASK) != (unsigned long)ioc->pdir_base);\r\nDBG_INIT(" base %p\n", ioc->pdir_base);\r\nioc->res_size = (ioc->pdir_size / sizeof(u64)) >> 3;\r\nDBG_INIT("%s() res_size 0x%x\n", __func__, ioc->res_size);\r\nioc->res_map = (u8 *)__get_free_pages(GFP_KERNEL,\r\nget_order(ioc->res_size));\r\nif(NULL == ioc->res_map) {\r\npanic("%s() could not allocate resource map\n", __func__);\r\n}\r\nmemset(ioc->res_map, 0, ioc->res_size);\r\nioc->res_hint = 16;\r\nspin_lock_init(&ioc->res_lock);\r\nioc->chainid_shift = get_order(iova_space_size) + PAGE_SHIFT - CCIO_CHAINID_SHIFT;\r\nDBG_INIT(" chainid_shift 0x%x\n", ioc->chainid_shift);\r\nWRITE_U32(CCIO_CHAINID_MASK << ioc->chainid_shift,\r\n&ioc->ioc_regs->io_chain_id_mask);\r\nWRITE_U32(virt_to_phys(ioc->pdir_base),\r\n&ioc->ioc_regs->io_pdir_base);\r\nWRITE_U32(IOA_NORMAL_MODE, &ioc->ioc_regs->io_control);\r\nWRITE_U32(0, &ioc->ioc_regs->io_tlb_entry_m);\r\nWRITE_U32(0, &ioc->ioc_regs->io_tlb_entry_l);\r\nfor(i = 1 << CCIO_CHAINID_SHIFT; i ; i--) {\r\nWRITE_U32((CMD_TLB_DIRECT_WRITE | (i << ioc->chainid_shift)),\r\n&ioc->ioc_regs->io_command);\r\n}\r\n}\r\nstatic void __init\r\nccio_init_resource(struct resource *res, char *name, void __iomem *ioaddr)\r\n{\r\nint result;\r\nres->parent = NULL;\r\nres->flags = IORESOURCE_MEM;\r\nres->start = (unsigned long)((signed) READ_U32(ioaddr) << 16);\r\nres->end = (unsigned long)((signed) (READ_U32(ioaddr + 4) << 16) - 1);\r\nres->name = name;\r\nif (res->end + 1 == res->start)\r\nreturn;\r\nresult = insert_resource(&iomem_resource, res);\r\nif (result < 0) {\r\nprintk(KERN_ERR "%s() failed to claim CCIO bus address space (%08lx,%08lx)\n",\r\n__func__, (unsigned long)res->start, (unsigned long)res->end);\r\n}\r\n}\r\nstatic void __init ccio_init_resources(struct ioc *ioc)\r\n{\r\nstruct resource *res = ioc->mmio_region;\r\nchar *name = kmalloc(14, GFP_KERNEL);\r\nsnprintf(name, 14, "GSC Bus [%d/]", ioc->hw_path);\r\nccio_init_resource(res, name, &ioc->ioc_regs->io_io_low);\r\nccio_init_resource(res + 1, name, &ioc->ioc_regs->io_io_low_hv);\r\n}\r\nstatic int new_ioc_area(struct resource *res, unsigned long size,\r\nunsigned long min, unsigned long max, unsigned long align)\r\n{\r\nif (max <= min)\r\nreturn -EBUSY;\r\nres->start = (max - size + 1) &~ (align - 1);\r\nres->end = res->start + size;\r\nif (!insert_resource(&iomem_resource, res))\r\nreturn 0;\r\nreturn new_ioc_area(res, size, min, max - size, align);\r\n}\r\nstatic int expand_ioc_area(struct resource *res, unsigned long size,\r\nunsigned long min, unsigned long max, unsigned long align)\r\n{\r\nunsigned long start, len;\r\nif (!res->parent)\r\nreturn new_ioc_area(res, size, min, max, align);\r\nstart = (res->start - size) &~ (align - 1);\r\nlen = res->end - start + 1;\r\nif (start >= min) {\r\nif (!adjust_resource(res, start, len))\r\nreturn 0;\r\n}\r\nstart = res->start;\r\nlen = ((size + res->end + align) &~ (align - 1)) - start;\r\nif (start + len <= max) {\r\nif (!adjust_resource(res, start, len))\r\nreturn 0;\r\n}\r\nreturn -EBUSY;\r\n}\r\nint ccio_allocate_resource(const struct parisc_device *dev,\r\nstruct resource *res, unsigned long size,\r\nunsigned long min, unsigned long max, unsigned long align)\r\n{\r\nstruct resource *parent = &iomem_resource;\r\nstruct ioc *ioc = ccio_get_iommu(dev);\r\nif (!ioc)\r\ngoto out;\r\nparent = ioc->mmio_region;\r\nif (parent->parent &&\r\n!allocate_resource(parent, res, size, min, max, align, NULL, NULL))\r\nreturn 0;\r\nif ((parent + 1)->parent &&\r\n!allocate_resource(parent + 1, res, size, min, max, align,\r\nNULL, NULL))\r\nreturn 0;\r\nif (!expand_ioc_area(parent, size, min, max, align)) {\r\n__raw_writel(((parent->start)>>16) | 0xffff0000,\r\n&ioc->ioc_regs->io_io_low);\r\n__raw_writel(((parent->end)>>16) | 0xffff0000,\r\n&ioc->ioc_regs->io_io_high);\r\n} else if (!expand_ioc_area(parent + 1, size, min, max, align)) {\r\nparent++;\r\n__raw_writel(((parent->start)>>16) | 0xffff0000,\r\n&ioc->ioc_regs->io_io_low_hv);\r\n__raw_writel(((parent->end)>>16) | 0xffff0000,\r\n&ioc->ioc_regs->io_io_high_hv);\r\n} else {\r\nreturn -EBUSY;\r\n}\r\nout:\r\nreturn allocate_resource(parent, res, size, min, max, align, NULL,NULL);\r\n}\r\nint ccio_request_resource(const struct parisc_device *dev,\r\nstruct resource *res)\r\n{\r\nstruct resource *parent;\r\nstruct ioc *ioc = ccio_get_iommu(dev);\r\nif (!ioc) {\r\nparent = &iomem_resource;\r\n} else if ((ioc->mmio_region->start <= res->start) &&\r\n(res->end <= ioc->mmio_region->end)) {\r\nparent = ioc->mmio_region;\r\n} else if (((ioc->mmio_region + 1)->start <= res->start) &&\r\n(res->end <= (ioc->mmio_region + 1)->end)) {\r\nparent = ioc->mmio_region + 1;\r\n} else {\r\nreturn -EBUSY;\r\n}\r\nreturn insert_resource(parent, res);\r\n}\r\nstatic int __init ccio_probe(struct parisc_device *dev)\r\n{\r\nint i;\r\nstruct ioc *ioc, **ioc_p = &ioc_list;\r\nioc = kzalloc(sizeof(struct ioc), GFP_KERNEL);\r\nif (ioc == NULL) {\r\nprintk(KERN_ERR MODULE_NAME ": memory allocation failure\n");\r\nreturn 1;\r\n}\r\nioc->name = dev->id.hversion == U2_IOA_RUNWAY ? "U2" : "UTurn";\r\nprintk(KERN_INFO "Found %s at 0x%lx\n", ioc->name,\r\n(unsigned long)dev->hpa.start);\r\nfor (i = 0; i < ioc_count; i++) {\r\nioc_p = &(*ioc_p)->next;\r\n}\r\n*ioc_p = ioc;\r\nioc->hw_path = dev->hw_path;\r\nioc->ioc_regs = ioremap_nocache(dev->hpa.start, 4096);\r\nccio_ioc_init(ioc);\r\nccio_init_resources(ioc);\r\nhppa_dma_ops = &ccio_ops;\r\ndev->dev.platform_data = kzalloc(sizeof(struct pci_hba_data), GFP_KERNEL);\r\nBUG_ON(dev->dev.platform_data == NULL);\r\nHBA_DATA(dev->dev.platform_data)->iommu = ioc;\r\n#ifdef CONFIG_PROC_FS\r\nif (ioc_count == 0) {\r\nproc_create(MODULE_NAME, 0, proc_runway_root,\r\n&ccio_proc_info_fops);\r\nproc_create(MODULE_NAME"-bitmap", 0, proc_runway_root,\r\n&ccio_proc_bitmap_fops);\r\n}\r\n#endif\r\nioc_count++;\r\nparisc_has_iommu();\r\nreturn 0;\r\n}\r\nvoid __init ccio_init(void)\r\n{\r\nregister_parisc_driver(&ccio_driver);\r\n}
