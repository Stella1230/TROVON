int kvm_handle_cp10_id(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nkvm_inject_undefined(vcpu);\r\nreturn 1;\r\n}\r\nint kvm_handle_cp_0_13_access(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nkvm_inject_undefined(vcpu);\r\nreturn 1;\r\n}\r\nint kvm_handle_cp14_load_store(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nkvm_inject_undefined(vcpu);\r\nreturn 1;\r\n}\r\nint kvm_handle_cp14_access(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nkvm_inject_undefined(vcpu);\r\nreturn 1;\r\n}\r\nstatic void reset_mpidr(struct kvm_vcpu *vcpu, const struct coproc_reg *r)\r\n{\r\nvcpu->arch.cp15[c0_MPIDR] = ((read_cpuid_mpidr() & MPIDR_SMP_BITMASK) |\r\n((vcpu->vcpu_id >> 2) << MPIDR_LEVEL_BITS) |\r\n(vcpu->vcpu_id & 3));\r\n}\r\nstatic bool access_actlr(struct kvm_vcpu *vcpu,\r\nconst struct coproc_params *p,\r\nconst struct coproc_reg *r)\r\n{\r\nif (p->is_write)\r\nreturn ignore_write(vcpu, p);\r\n*vcpu_reg(vcpu, p->Rt1) = vcpu->arch.cp15[c1_ACTLR];\r\nreturn true;\r\n}\r\nstatic bool access_cbar(struct kvm_vcpu *vcpu,\r\nconst struct coproc_params *p,\r\nconst struct coproc_reg *r)\r\n{\r\nif (p->is_write)\r\nreturn write_to_read_only(vcpu, p);\r\nreturn read_zero(vcpu, p);\r\n}\r\nstatic bool access_l2ctlr(struct kvm_vcpu *vcpu,\r\nconst struct coproc_params *p,\r\nconst struct coproc_reg *r)\r\n{\r\nif (p->is_write)\r\nreturn ignore_write(vcpu, p);\r\n*vcpu_reg(vcpu, p->Rt1) = vcpu->arch.cp15[c9_L2CTLR];\r\nreturn true;\r\n}\r\nstatic void reset_l2ctlr(struct kvm_vcpu *vcpu, const struct coproc_reg *r)\r\n{\r\nu32 l2ctlr, ncores;\r\nasm volatile("mrc p15, 1, %0, c9, c0, 2\n" : "=r" (l2ctlr));\r\nl2ctlr &= ~(3 << 24);\r\nncores = atomic_read(&vcpu->kvm->online_vcpus) - 1;\r\nncores -= (vcpu->vcpu_id & ~3);\r\nncores = min(ncores, 3U);\r\nl2ctlr |= (ncores & 3) << 24;\r\nvcpu->arch.cp15[c9_L2CTLR] = l2ctlr;\r\n}\r\nstatic void reset_actlr(struct kvm_vcpu *vcpu, const struct coproc_reg *r)\r\n{\r\nu32 actlr;\r\nasm volatile("mrc p15, 0, %0, c1, c0, 1\n" : "=r" (actlr));\r\nif (atomic_read(&vcpu->kvm->online_vcpus) > 1)\r\nactlr |= 1U << 6;\r\nelse\r\nactlr &= ~(1U << 6);\r\nvcpu->arch.cp15[c1_ACTLR] = actlr;\r\n}\r\nstatic bool access_l2ectlr(struct kvm_vcpu *vcpu,\r\nconst struct coproc_params *p,\r\nconst struct coproc_reg *r)\r\n{\r\nif (p->is_write)\r\nreturn ignore_write(vcpu, p);\r\n*vcpu_reg(vcpu, p->Rt1) = 0;\r\nreturn true;\r\n}\r\nstatic bool access_dcsw(struct kvm_vcpu *vcpu,\r\nconst struct coproc_params *p,\r\nconst struct coproc_reg *r)\r\n{\r\nunsigned long val;\r\nint cpu;\r\nif (!p->is_write)\r\nreturn read_from_write_only(vcpu, p);\r\ncpu = get_cpu();\r\ncpumask_setall(&vcpu->arch.require_dcache_flush);\r\ncpumask_clear_cpu(cpu, &vcpu->arch.require_dcache_flush);\r\nif (cpu != vcpu->arch.last_pcpu) {\r\nflush_cache_all();\r\ngoto done;\r\n}\r\nval = *vcpu_reg(vcpu, p->Rt1);\r\nswitch (p->CRm) {\r\ncase 6:\r\ncase 14:\r\nasm volatile("mcr p15, 0, %0, c7, c14, 2" : : "r" (val));\r\nbreak;\r\ncase 10:\r\nasm volatile("mcr p15, 0, %0, c7, c10, 2" : : "r" (val));\r\nbreak;\r\n}\r\ndone:\r\nput_cpu();\r\nreturn true;\r\n}\r\nstatic bool access_vm_reg(struct kvm_vcpu *vcpu,\r\nconst struct coproc_params *p,\r\nconst struct coproc_reg *r)\r\n{\r\nBUG_ON(!p->is_write);\r\nvcpu->arch.cp15[r->reg] = *vcpu_reg(vcpu, p->Rt1);\r\nif (p->is_64bit)\r\nvcpu->arch.cp15[r->reg + 1] = *vcpu_reg(vcpu, p->Rt2);\r\nreturn true;\r\n}\r\nbool access_sctlr(struct kvm_vcpu *vcpu,\r\nconst struct coproc_params *p,\r\nconst struct coproc_reg *r)\r\n{\r\naccess_vm_reg(vcpu, p, r);\r\nif (vcpu_has_cache_enabled(vcpu)) {\r\nvcpu->arch.hcr &= ~HCR_TVM;\r\nstage2_flush_vm(vcpu->kvm);\r\n}\r\nreturn true;\r\n}\r\nstatic bool pm_fake(struct kvm_vcpu *vcpu,\r\nconst struct coproc_params *p,\r\nconst struct coproc_reg *r)\r\n{\r\nif (p->is_write)\r\nreturn ignore_write(vcpu, p);\r\nelse\r\nreturn read_zero(vcpu, p);\r\n}\r\nvoid kvm_register_target_coproc_table(struct kvm_coproc_target_table *table)\r\n{\r\nunsigned int i;\r\nfor (i = 1; i < table->num; i++)\r\nBUG_ON(cmp_reg(&table->table[i-1],\r\n&table->table[i]) >= 0);\r\ntarget_tables[table->target] = table;\r\n}\r\nstatic const struct coproc_reg *get_target_table(unsigned target, size_t *num)\r\n{\r\nstruct kvm_coproc_target_table *table;\r\ntable = target_tables[target];\r\n*num = table->num;\r\nreturn table->table;\r\n}\r\nstatic const struct coproc_reg *find_reg(const struct coproc_params *params,\r\nconst struct coproc_reg table[],\r\nunsigned int num)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < num; i++) {\r\nconst struct coproc_reg *r = &table[i];\r\nif (params->is_64bit != r->is_64)\r\ncontinue;\r\nif (params->CRn != r->CRn)\r\ncontinue;\r\nif (params->CRm != r->CRm)\r\ncontinue;\r\nif (params->Op1 != r->Op1)\r\ncontinue;\r\nif (params->Op2 != r->Op2)\r\ncontinue;\r\nreturn r;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int emulate_cp15(struct kvm_vcpu *vcpu,\r\nconst struct coproc_params *params)\r\n{\r\nsize_t num;\r\nconst struct coproc_reg *table, *r;\r\ntrace_kvm_emulate_cp15_imp(params->Op1, params->Rt1, params->CRn,\r\nparams->CRm, params->Op2, params->is_write);\r\ntable = get_target_table(vcpu->arch.target, &num);\r\nr = find_reg(params, table, num);\r\nif (!r)\r\nr = find_reg(params, cp15_regs, ARRAY_SIZE(cp15_regs));\r\nif (likely(r)) {\r\nBUG_ON(!r->access);\r\nif (likely(r->access(vcpu, params, r))) {\r\nkvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));\r\nreturn 1;\r\n}\r\n} else {\r\nkvm_err("Unsupported guest CP15 access at: %08lx\n",\r\n*vcpu_pc(vcpu));\r\nprint_cp_instr(params);\r\n}\r\nkvm_inject_undefined(vcpu);\r\nreturn 1;\r\n}\r\nint kvm_handle_cp15_64(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nstruct coproc_params params;\r\nparams.CRn = (kvm_vcpu_get_hsr(vcpu) >> 1) & 0xf;\r\nparams.Rt1 = (kvm_vcpu_get_hsr(vcpu) >> 5) & 0xf;\r\nparams.is_write = ((kvm_vcpu_get_hsr(vcpu) & 1) == 0);\r\nparams.is_64bit = true;\r\nparams.Op1 = (kvm_vcpu_get_hsr(vcpu) >> 16) & 0xf;\r\nparams.Op2 = 0;\r\nparams.Rt2 = (kvm_vcpu_get_hsr(vcpu) >> 10) & 0xf;\r\nparams.CRm = 0;\r\nreturn emulate_cp15(vcpu, &params);\r\n}\r\nstatic void reset_coproc_regs(struct kvm_vcpu *vcpu,\r\nconst struct coproc_reg *table, size_t num)\r\n{\r\nunsigned long i;\r\nfor (i = 0; i < num; i++)\r\nif (table[i].reset)\r\ntable[i].reset(vcpu, &table[i]);\r\n}\r\nint kvm_handle_cp15_32(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nstruct coproc_params params;\r\nparams.CRm = (kvm_vcpu_get_hsr(vcpu) >> 1) & 0xf;\r\nparams.Rt1 = (kvm_vcpu_get_hsr(vcpu) >> 5) & 0xf;\r\nparams.is_write = ((kvm_vcpu_get_hsr(vcpu) & 1) == 0);\r\nparams.is_64bit = false;\r\nparams.CRn = (kvm_vcpu_get_hsr(vcpu) >> 10) & 0xf;\r\nparams.Op1 = (kvm_vcpu_get_hsr(vcpu) >> 14) & 0x7;\r\nparams.Op2 = (kvm_vcpu_get_hsr(vcpu) >> 17) & 0x7;\r\nparams.Rt2 = 0;\r\nreturn emulate_cp15(vcpu, &params);\r\n}\r\nstatic bool index_to_params(u64 id, struct coproc_params *params)\r\n{\r\nswitch (id & KVM_REG_SIZE_MASK) {\r\ncase KVM_REG_SIZE_U32:\r\nif (id & ~(KVM_REG_ARCH_MASK | KVM_REG_SIZE_MASK\r\n| KVM_REG_ARM_COPROC_MASK\r\n| KVM_REG_ARM_32_CRN_MASK\r\n| KVM_REG_ARM_CRM_MASK\r\n| KVM_REG_ARM_OPC1_MASK\r\n| KVM_REG_ARM_32_OPC2_MASK))\r\nreturn false;\r\nparams->is_64bit = false;\r\nparams->CRn = ((id & KVM_REG_ARM_32_CRN_MASK)\r\n>> KVM_REG_ARM_32_CRN_SHIFT);\r\nparams->CRm = ((id & KVM_REG_ARM_CRM_MASK)\r\n>> KVM_REG_ARM_CRM_SHIFT);\r\nparams->Op1 = ((id & KVM_REG_ARM_OPC1_MASK)\r\n>> KVM_REG_ARM_OPC1_SHIFT);\r\nparams->Op2 = ((id & KVM_REG_ARM_32_OPC2_MASK)\r\n>> KVM_REG_ARM_32_OPC2_SHIFT);\r\nreturn true;\r\ncase KVM_REG_SIZE_U64:\r\nif (id & ~(KVM_REG_ARCH_MASK | KVM_REG_SIZE_MASK\r\n| KVM_REG_ARM_COPROC_MASK\r\n| KVM_REG_ARM_CRM_MASK\r\n| KVM_REG_ARM_OPC1_MASK))\r\nreturn false;\r\nparams->is_64bit = true;\r\nparams->CRn = ((id & KVM_REG_ARM_CRM_MASK)\r\n>> KVM_REG_ARM_CRM_SHIFT);\r\nparams->Op1 = ((id & KVM_REG_ARM_OPC1_MASK)\r\n>> KVM_REG_ARM_OPC1_SHIFT);\r\nparams->Op2 = 0;\r\nparams->CRm = 0;\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic const struct coproc_reg *index_to_coproc_reg(struct kvm_vcpu *vcpu,\r\nu64 id)\r\n{\r\nsize_t num;\r\nconst struct coproc_reg *table, *r;\r\nstruct coproc_params params;\r\nif ((id & KVM_REG_ARM_COPROC_MASK) >> KVM_REG_ARM_COPROC_SHIFT != 15)\r\nreturn NULL;\r\nif (!index_to_params(id, &params))\r\nreturn NULL;\r\ntable = get_target_table(vcpu->arch.target, &num);\r\nr = find_reg(&params, table, num);\r\nif (!r)\r\nr = find_reg(&params, cp15_regs, ARRAY_SIZE(cp15_regs));\r\nif (r && !r->reg)\r\nr = NULL;\r\nreturn r;\r\n}\r\nstatic int reg_from_user(void *val, const void __user *uaddr, u64 id)\r\n{\r\nif (copy_from_user(val, uaddr, KVM_REG_SIZE(id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int reg_to_user(void __user *uaddr, const void *val, u64 id)\r\n{\r\nif (copy_to_user(uaddr, val, KVM_REG_SIZE(id)) != 0)\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int get_invariant_cp15(u64 id, void __user *uaddr)\r\n{\r\nstruct coproc_params params;\r\nconst struct coproc_reg *r;\r\nif (!index_to_params(id, &params))\r\nreturn -ENOENT;\r\nr = find_reg(&params, invariant_cp15, ARRAY_SIZE(invariant_cp15));\r\nif (!r)\r\nreturn -ENOENT;\r\nreturn reg_to_user(uaddr, &r->val, id);\r\n}\r\nstatic int set_invariant_cp15(u64 id, void __user *uaddr)\r\n{\r\nstruct coproc_params params;\r\nconst struct coproc_reg *r;\r\nint err;\r\nu64 val = 0;\r\nif (!index_to_params(id, &params))\r\nreturn -ENOENT;\r\nr = find_reg(&params, invariant_cp15, ARRAY_SIZE(invariant_cp15));\r\nif (!r)\r\nreturn -ENOENT;\r\nerr = reg_from_user(&val, uaddr, id);\r\nif (err)\r\nreturn err;\r\nif (r->val != val)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic bool is_valid_cache(u32 val)\r\n{\r\nu32 level, ctype;\r\nif (val >= CSSELR_MAX)\r\nreturn -ENOENT;\r\nlevel = (val >> 1);\r\nctype = (cache_levels >> (level * 3)) & 7;\r\nswitch (ctype) {\r\ncase 0:\r\nreturn false;\r\ncase 1:\r\nreturn (val & 1);\r\ncase 2:\r\ncase 4:\r\nreturn !(val & 1);\r\ncase 3:\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic u32 get_ccsidr(u32 csselr)\r\n{\r\nu32 ccsidr;\r\nlocal_irq_disable();\r\nasm volatile("mcr p15, 2, %0, c0, c0, 0" : : "r" (csselr));\r\nisb();\r\nasm volatile("mrc p15, 1, %0, c0, c0, 0" : "=r" (ccsidr));\r\nlocal_irq_enable();\r\nreturn ccsidr;\r\n}\r\nstatic int demux_c15_get(u64 id, void __user *uaddr)\r\n{\r\nu32 val;\r\nu32 __user *uval = uaddr;\r\nif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\r\n| ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\r\nreturn -ENOENT;\r\nswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\r\ncase KVM_REG_ARM_DEMUX_ID_CCSIDR:\r\nif (KVM_REG_SIZE(id) != 4)\r\nreturn -ENOENT;\r\nval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\r\n>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\r\nif (!is_valid_cache(val))\r\nreturn -ENOENT;\r\nreturn put_user(get_ccsidr(val), uval);\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\n}\r\nstatic int demux_c15_set(u64 id, void __user *uaddr)\r\n{\r\nu32 val, newval;\r\nu32 __user *uval = uaddr;\r\nif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\r\n| ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\r\nreturn -ENOENT;\r\nswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\r\ncase KVM_REG_ARM_DEMUX_ID_CCSIDR:\r\nif (KVM_REG_SIZE(id) != 4)\r\nreturn -ENOENT;\r\nval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\r\n>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\r\nif (!is_valid_cache(val))\r\nreturn -ENOENT;\r\nif (get_user(newval, uval))\r\nreturn -EFAULT;\r\nif (newval != get_ccsidr(val))\r\nreturn -EINVAL;\r\nreturn 0;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\n}\r\nstatic unsigned int num_fp_regs(void)\r\n{\r\nif (((fmrx(MVFR0) & MVFR0_A_SIMD_MASK) >> MVFR0_A_SIMD_BIT) == 2)\r\nreturn 32;\r\nelse\r\nreturn 16;\r\n}\r\nstatic unsigned int num_vfp_regs(void)\r\n{\r\nreturn num_fp_regs() + ARRAY_SIZE(vfp_sysregs);\r\n}\r\nstatic int copy_vfp_regids(u64 __user *uindices)\r\n{\r\nunsigned int i;\r\nconst u64 u32reg = KVM_REG_ARM | KVM_REG_SIZE_U32 | KVM_REG_ARM_VFP;\r\nconst u64 u64reg = KVM_REG_ARM | KVM_REG_SIZE_U64 | KVM_REG_ARM_VFP;\r\nfor (i = 0; i < num_fp_regs(); i++) {\r\nif (put_user((u64reg | KVM_REG_ARM_VFP_BASE_REG) + i,\r\nuindices))\r\nreturn -EFAULT;\r\nuindices++;\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(vfp_sysregs); i++) {\r\nif (put_user(u32reg | vfp_sysregs[i], uindices))\r\nreturn -EFAULT;\r\nuindices++;\r\n}\r\nreturn num_vfp_regs();\r\n}\r\nstatic int vfp_get_reg(const struct kvm_vcpu *vcpu, u64 id, void __user *uaddr)\r\n{\r\nu32 vfpid = (id & KVM_REG_ARM_VFP_MASK);\r\nu32 val;\r\nif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\r\n| ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\r\nreturn -ENOENT;\r\nif (vfpid < num_fp_regs()) {\r\nif (KVM_REG_SIZE(id) != 8)\r\nreturn -ENOENT;\r\nreturn reg_to_user(uaddr, &vcpu->arch.vfp_guest.fpregs[vfpid],\r\nid);\r\n}\r\nif (KVM_REG_SIZE(id) != 4)\r\nreturn -ENOENT;\r\nswitch (vfpid) {\r\ncase KVM_REG_ARM_VFP_FPEXC:\r\nreturn reg_to_user(uaddr, &vcpu->arch.vfp_guest.fpexc, id);\r\ncase KVM_REG_ARM_VFP_FPSCR:\r\nreturn reg_to_user(uaddr, &vcpu->arch.vfp_guest.fpscr, id);\r\ncase KVM_REG_ARM_VFP_FPINST:\r\nreturn reg_to_user(uaddr, &vcpu->arch.vfp_guest.fpinst, id);\r\ncase KVM_REG_ARM_VFP_FPINST2:\r\nreturn reg_to_user(uaddr, &vcpu->arch.vfp_guest.fpinst2, id);\r\ncase KVM_REG_ARM_VFP_MVFR0:\r\nval = fmrx(MVFR0);\r\nreturn reg_to_user(uaddr, &val, id);\r\ncase KVM_REG_ARM_VFP_MVFR1:\r\nval = fmrx(MVFR1);\r\nreturn reg_to_user(uaddr, &val, id);\r\ncase KVM_REG_ARM_VFP_FPSID:\r\nval = fmrx(FPSID);\r\nreturn reg_to_user(uaddr, &val, id);\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\n}\r\nstatic int vfp_set_reg(struct kvm_vcpu *vcpu, u64 id, const void __user *uaddr)\r\n{\r\nu32 vfpid = (id & KVM_REG_ARM_VFP_MASK);\r\nu32 val;\r\nif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\r\n| ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\r\nreturn -ENOENT;\r\nif (vfpid < num_fp_regs()) {\r\nif (KVM_REG_SIZE(id) != 8)\r\nreturn -ENOENT;\r\nreturn reg_from_user(&vcpu->arch.vfp_guest.fpregs[vfpid],\r\nuaddr, id);\r\n}\r\nif (KVM_REG_SIZE(id) != 4)\r\nreturn -ENOENT;\r\nswitch (vfpid) {\r\ncase KVM_REG_ARM_VFP_FPEXC:\r\nreturn reg_from_user(&vcpu->arch.vfp_guest.fpexc, uaddr, id);\r\ncase KVM_REG_ARM_VFP_FPSCR:\r\nreturn reg_from_user(&vcpu->arch.vfp_guest.fpscr, uaddr, id);\r\ncase KVM_REG_ARM_VFP_FPINST:\r\nreturn reg_from_user(&vcpu->arch.vfp_guest.fpinst, uaddr, id);\r\ncase KVM_REG_ARM_VFP_FPINST2:\r\nreturn reg_from_user(&vcpu->arch.vfp_guest.fpinst2, uaddr, id);\r\ncase KVM_REG_ARM_VFP_MVFR0:\r\nif (reg_from_user(&val, uaddr, id))\r\nreturn -EFAULT;\r\nif (val != fmrx(MVFR0))\r\nreturn -EINVAL;\r\nreturn 0;\r\ncase KVM_REG_ARM_VFP_MVFR1:\r\nif (reg_from_user(&val, uaddr, id))\r\nreturn -EFAULT;\r\nif (val != fmrx(MVFR1))\r\nreturn -EINVAL;\r\nreturn 0;\r\ncase KVM_REG_ARM_VFP_FPSID:\r\nif (reg_from_user(&val, uaddr, id))\r\nreturn -EFAULT;\r\nif (val != fmrx(FPSID))\r\nreturn -EINVAL;\r\nreturn 0;\r\ndefault:\r\nreturn -ENOENT;\r\n}\r\n}\r\nstatic unsigned int num_vfp_regs(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic int copy_vfp_regids(u64 __user *uindices)\r\n{\r\nreturn 0;\r\n}\r\nstatic int vfp_get_reg(const struct kvm_vcpu *vcpu, u64 id, void __user *uaddr)\r\n{\r\nreturn -ENOENT;\r\n}\r\nstatic int vfp_set_reg(struct kvm_vcpu *vcpu, u64 id, const void __user *uaddr)\r\n{\r\nreturn -ENOENT;\r\n}\r\nint kvm_arm_coproc_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\r\n{\r\nconst struct coproc_reg *r;\r\nvoid __user *uaddr = (void __user *)(long)reg->addr;\r\nif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\r\nreturn demux_c15_get(reg->id, uaddr);\r\nif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_VFP)\r\nreturn vfp_get_reg(vcpu, reg->id, uaddr);\r\nr = index_to_coproc_reg(vcpu, reg->id);\r\nif (!r)\r\nreturn get_invariant_cp15(reg->id, uaddr);\r\nreturn reg_to_user(uaddr, &vcpu->arch.cp15[r->reg], reg->id);\r\n}\r\nint kvm_arm_coproc_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\r\n{\r\nconst struct coproc_reg *r;\r\nvoid __user *uaddr = (void __user *)(long)reg->addr;\r\nif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\r\nreturn demux_c15_set(reg->id, uaddr);\r\nif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_VFP)\r\nreturn vfp_set_reg(vcpu, reg->id, uaddr);\r\nr = index_to_coproc_reg(vcpu, reg->id);\r\nif (!r)\r\nreturn set_invariant_cp15(reg->id, uaddr);\r\nreturn reg_from_user(&vcpu->arch.cp15[r->reg], uaddr, reg->id);\r\n}\r\nstatic unsigned int num_demux_regs(void)\r\n{\r\nunsigned int i, count = 0;\r\nfor (i = 0; i < CSSELR_MAX; i++)\r\nif (is_valid_cache(i))\r\ncount++;\r\nreturn count;\r\n}\r\nstatic int write_demux_regids(u64 __user *uindices)\r\n{\r\nu64 val = KVM_REG_ARM | KVM_REG_SIZE_U32 | KVM_REG_ARM_DEMUX;\r\nunsigned int i;\r\nval |= KVM_REG_ARM_DEMUX_ID_CCSIDR;\r\nfor (i = 0; i < CSSELR_MAX; i++) {\r\nif (!is_valid_cache(i))\r\ncontinue;\r\nif (put_user(val | i, uindices))\r\nreturn -EFAULT;\r\nuindices++;\r\n}\r\nreturn 0;\r\n}\r\nstatic u64 cp15_to_index(const struct coproc_reg *reg)\r\n{\r\nu64 val = KVM_REG_ARM | (15 << KVM_REG_ARM_COPROC_SHIFT);\r\nif (reg->is_64) {\r\nval |= KVM_REG_SIZE_U64;\r\nval |= (reg->Op1 << KVM_REG_ARM_OPC1_SHIFT);\r\nval |= (reg->CRn << KVM_REG_ARM_CRM_SHIFT);\r\n} else {\r\nval |= KVM_REG_SIZE_U32;\r\nval |= (reg->Op1 << KVM_REG_ARM_OPC1_SHIFT);\r\nval |= (reg->Op2 << KVM_REG_ARM_32_OPC2_SHIFT);\r\nval |= (reg->CRm << KVM_REG_ARM_CRM_SHIFT);\r\nval |= (reg->CRn << KVM_REG_ARM_32_CRN_SHIFT);\r\n}\r\nreturn val;\r\n}\r\nstatic bool copy_reg_to_user(const struct coproc_reg *reg, u64 __user **uind)\r\n{\r\nif (!*uind)\r\nreturn true;\r\nif (put_user(cp15_to_index(reg), *uind))\r\nreturn false;\r\n(*uind)++;\r\nreturn true;\r\n}\r\nstatic int walk_cp15(struct kvm_vcpu *vcpu, u64 __user *uind)\r\n{\r\nconst struct coproc_reg *i1, *i2, *end1, *end2;\r\nunsigned int total = 0;\r\nsize_t num;\r\ni1 = get_target_table(vcpu->arch.target, &num);\r\nend1 = i1 + num;\r\ni2 = cp15_regs;\r\nend2 = cp15_regs + ARRAY_SIZE(cp15_regs);\r\nBUG_ON(i1 == end1 || i2 == end2);\r\nwhile (i1 || i2) {\r\nint cmp = cmp_reg(i1, i2);\r\nif (cmp <= 0) {\r\nif (i1->reg) {\r\nif (!copy_reg_to_user(i1, &uind))\r\nreturn -EFAULT;\r\ntotal++;\r\n}\r\n} else {\r\nif (i2->reg) {\r\nif (!copy_reg_to_user(i2, &uind))\r\nreturn -EFAULT;\r\ntotal++;\r\n}\r\n}\r\nif (cmp <= 0 && ++i1 == end1)\r\ni1 = NULL;\r\nif (cmp >= 0 && ++i2 == end2)\r\ni2 = NULL;\r\n}\r\nreturn total;\r\n}\r\nunsigned long kvm_arm_num_coproc_regs(struct kvm_vcpu *vcpu)\r\n{\r\nreturn ARRAY_SIZE(invariant_cp15)\r\n+ num_demux_regs()\r\n+ num_vfp_regs()\r\n+ walk_cp15(vcpu, (u64 __user *)NULL);\r\n}\r\nint kvm_arm_copy_coproc_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)\r\n{\r\nunsigned int i;\r\nint err;\r\nfor (i = 0; i < ARRAY_SIZE(invariant_cp15); i++) {\r\nif (put_user(cp15_to_index(&invariant_cp15[i]), uindices))\r\nreturn -EFAULT;\r\nuindices++;\r\n}\r\nerr = walk_cp15(vcpu, uindices);\r\nif (err < 0)\r\nreturn err;\r\nuindices += err;\r\nerr = copy_vfp_regids(uindices);\r\nif (err < 0)\r\nreturn err;\r\nuindices += err;\r\nreturn write_demux_regids(uindices);\r\n}\r\nvoid kvm_coproc_table_init(void)\r\n{\r\nunsigned int i;\r\nfor (i = 1; i < ARRAY_SIZE(cp15_regs); i++)\r\nBUG_ON(cmp_reg(&cp15_regs[i-1], &cp15_regs[i]) >= 0);\r\nfor (i = 0; i < ARRAY_SIZE(invariant_cp15); i++)\r\ninvariant_cp15[i].reset(NULL, &invariant_cp15[i]);\r\nasm volatile("mrc p15, 1, %0, c0, c0, 1" : "=r" (cache_levels));\r\nfor (i = 0; i < 7; i++)\r\nif (((cache_levels >> (i*3)) & 7) == 0)\r\nbreak;\r\ncache_levels &= (1 << (i*3))-1;\r\n}\r\nvoid kvm_reset_coprocs(struct kvm_vcpu *vcpu)\r\n{\r\nsize_t num;\r\nconst struct coproc_reg *table;\r\nmemset(vcpu->arch.cp15, 0x42, sizeof(vcpu->arch.cp15));\r\nreset_coproc_regs(vcpu, cp15_regs, ARRAY_SIZE(cp15_regs));\r\ntable = get_target_table(vcpu->arch.target, &num);\r\nreset_coproc_regs(vcpu, table, num);\r\nfor (num = 1; num < NR_CP15_REGS; num++)\r\nif (vcpu->arch.cp15[num] == 0x42424242)\r\npanic("Didn't reset vcpu->arch.cp15[%zi]", num);\r\n}
