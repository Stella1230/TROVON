static int srpt_get_u64_x(char *buffer, struct kernel_param *kp)\r\n{\r\nreturn sprintf(buffer, "0x%016llx", *(u64 *)kp->arg);\r\n}\r\nstatic inline\r\nenum dma_data_direction opposite_dma_dir(enum dma_data_direction dir)\r\n{\r\nswitch (dir) {\r\ncase DMA_TO_DEVICE: return DMA_FROM_DEVICE;\r\ncase DMA_FROM_DEVICE: return DMA_TO_DEVICE;\r\ndefault: return dir;\r\n}\r\n}\r\nstatic inline const char *srpt_sdev_name(struct srpt_device *sdev)\r\n{\r\nreturn sdev->device->name;\r\n}\r\nstatic enum rdma_ch_state srpt_get_ch_state(struct srpt_rdma_ch *ch)\r\n{\r\nunsigned long flags;\r\nenum rdma_ch_state state;\r\nspin_lock_irqsave(&ch->spinlock, flags);\r\nstate = ch->state;\r\nspin_unlock_irqrestore(&ch->spinlock, flags);\r\nreturn state;\r\n}\r\nstatic enum rdma_ch_state\r\nsrpt_set_ch_state(struct srpt_rdma_ch *ch, enum rdma_ch_state new_state)\r\n{\r\nunsigned long flags;\r\nenum rdma_ch_state prev;\r\nspin_lock_irqsave(&ch->spinlock, flags);\r\nprev = ch->state;\r\nch->state = new_state;\r\nspin_unlock_irqrestore(&ch->spinlock, flags);\r\nreturn prev;\r\n}\r\nstatic bool\r\nsrpt_test_and_set_ch_state(struct srpt_rdma_ch *ch, enum rdma_ch_state old,\r\nenum rdma_ch_state new)\r\n{\r\nunsigned long flags;\r\nenum rdma_ch_state prev;\r\nspin_lock_irqsave(&ch->spinlock, flags);\r\nprev = ch->state;\r\nif (prev == old)\r\nch->state = new;\r\nspin_unlock_irqrestore(&ch->spinlock, flags);\r\nreturn prev == old;\r\n}\r\nstatic void srpt_event_handler(struct ib_event_handler *handler,\r\nstruct ib_event *event)\r\n{\r\nstruct srpt_device *sdev;\r\nstruct srpt_port *sport;\r\nsdev = ib_get_client_data(event->device, &srpt_client);\r\nif (!sdev || sdev->device != event->device)\r\nreturn;\r\npr_debug("ASYNC event= %d on device= %s\n", event->event,\r\nsrpt_sdev_name(sdev));\r\nswitch (event->event) {\r\ncase IB_EVENT_PORT_ERR:\r\nif (event->element.port_num <= sdev->device->phys_port_cnt) {\r\nsport = &sdev->port[event->element.port_num - 1];\r\nsport->lid = 0;\r\nsport->sm_lid = 0;\r\n}\r\nbreak;\r\ncase IB_EVENT_PORT_ACTIVE:\r\ncase IB_EVENT_LID_CHANGE:\r\ncase IB_EVENT_PKEY_CHANGE:\r\ncase IB_EVENT_SM_CHANGE:\r\ncase IB_EVENT_CLIENT_REREGISTER:\r\nif (event->element.port_num <= sdev->device->phys_port_cnt) {\r\nsport = &sdev->port[event->element.port_num - 1];\r\nif (!sport->lid && !sport->sm_lid)\r\nschedule_work(&sport->work);\r\n}\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "received unrecognized IB event %d\n",\r\nevent->event);\r\nbreak;\r\n}\r\n}\r\nstatic void srpt_srq_event(struct ib_event *event, void *ctx)\r\n{\r\nprintk(KERN_INFO "SRQ event %d\n", event->event);\r\n}\r\nstatic void srpt_qp_event(struct ib_event *event, struct srpt_rdma_ch *ch)\r\n{\r\npr_debug("QP event %d on cm_id=%p sess_name=%s state=%d\n",\r\nevent->event, ch->cm_id, ch->sess_name, srpt_get_ch_state(ch));\r\nswitch (event->event) {\r\ncase IB_EVENT_COMM_EST:\r\nib_cm_notify(ch->cm_id, event->event);\r\nbreak;\r\ncase IB_EVENT_QP_LAST_WQE_REACHED:\r\nif (srpt_test_and_set_ch_state(ch, CH_DRAINING,\r\nCH_RELEASING))\r\nsrpt_release_channel(ch);\r\nelse\r\npr_debug("%s: state %d - ignored LAST_WQE.\n",\r\nch->sess_name, srpt_get_ch_state(ch));\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "received unrecognized IB QP event %d\n",\r\nevent->event);\r\nbreak;\r\n}\r\n}\r\nstatic void srpt_set_ioc(u8 *c_list, u32 slot, u8 value)\r\n{\r\nu16 id;\r\nu8 tmp;\r\nid = (slot - 1) / 2;\r\nif (slot & 0x1) {\r\ntmp = c_list[id] & 0xf;\r\nc_list[id] = (value << 4) | tmp;\r\n} else {\r\ntmp = c_list[id] & 0xf0;\r\nc_list[id] = (value & 0xf) | tmp;\r\n}\r\n}\r\nstatic void srpt_get_class_port_info(struct ib_dm_mad *mad)\r\n{\r\nstruct ib_class_port_info *cif;\r\ncif = (struct ib_class_port_info *)mad->data;\r\nmemset(cif, 0, sizeof *cif);\r\ncif->base_version = 1;\r\ncif->class_version = 1;\r\ncif->resp_time_value = 20;\r\nmad->mad_hdr.status = 0;\r\n}\r\nstatic void srpt_get_iou(struct ib_dm_mad *mad)\r\n{\r\nstruct ib_dm_iou_info *ioui;\r\nu8 slot;\r\nint i;\r\nioui = (struct ib_dm_iou_info *)mad->data;\r\nioui->change_id = __constant_cpu_to_be16(1);\r\nioui->max_controllers = 16;\r\nsrpt_set_ioc(ioui->controller_list, 1, 1);\r\nfor (i = 1, slot = 2; i < 16; i++, slot++)\r\nsrpt_set_ioc(ioui->controller_list, slot, 0);\r\nmad->mad_hdr.status = 0;\r\n}\r\nstatic void srpt_get_ioc(struct srpt_port *sport, u32 slot,\r\nstruct ib_dm_mad *mad)\r\n{\r\nstruct srpt_device *sdev = sport->sdev;\r\nstruct ib_dm_ioc_profile *iocp;\r\niocp = (struct ib_dm_ioc_profile *)mad->data;\r\nif (!slot || slot > 16) {\r\nmad->mad_hdr.status\r\n= __constant_cpu_to_be16(DM_MAD_STATUS_INVALID_FIELD);\r\nreturn;\r\n}\r\nif (slot > 2) {\r\nmad->mad_hdr.status\r\n= __constant_cpu_to_be16(DM_MAD_STATUS_NO_IOC);\r\nreturn;\r\n}\r\nmemset(iocp, 0, sizeof *iocp);\r\nstrcpy(iocp->id_string, SRPT_ID_STRING);\r\niocp->guid = cpu_to_be64(srpt_service_guid);\r\niocp->vendor_id = cpu_to_be32(sdev->dev_attr.vendor_id);\r\niocp->device_id = cpu_to_be32(sdev->dev_attr.vendor_part_id);\r\niocp->device_version = cpu_to_be16(sdev->dev_attr.hw_ver);\r\niocp->subsys_vendor_id = cpu_to_be32(sdev->dev_attr.vendor_id);\r\niocp->subsys_device_id = 0x0;\r\niocp->io_class = __constant_cpu_to_be16(SRP_REV16A_IB_IO_CLASS);\r\niocp->io_subclass = __constant_cpu_to_be16(SRP_IO_SUBCLASS);\r\niocp->protocol = __constant_cpu_to_be16(SRP_PROTOCOL);\r\niocp->protocol_version = __constant_cpu_to_be16(SRP_PROTOCOL_VERSION);\r\niocp->send_queue_depth = cpu_to_be16(sdev->srq_size);\r\niocp->rdma_read_depth = 4;\r\niocp->send_size = cpu_to_be32(srp_max_req_size);\r\niocp->rdma_size = cpu_to_be32(min(sport->port_attrib.srp_max_rdma_size,\r\n1U << 24));\r\niocp->num_svc_entries = 1;\r\niocp->op_cap_mask = SRP_SEND_TO_IOC | SRP_SEND_FROM_IOC |\r\nSRP_RDMA_READ_FROM_IOC | SRP_RDMA_WRITE_FROM_IOC;\r\nmad->mad_hdr.status = 0;\r\n}\r\nstatic void srpt_get_svc_entries(u64 ioc_guid,\r\nu16 slot, u8 hi, u8 lo, struct ib_dm_mad *mad)\r\n{\r\nstruct ib_dm_svc_entries *svc_entries;\r\nWARN_ON(!ioc_guid);\r\nif (!slot || slot > 16) {\r\nmad->mad_hdr.status\r\n= __constant_cpu_to_be16(DM_MAD_STATUS_INVALID_FIELD);\r\nreturn;\r\n}\r\nif (slot > 2 || lo > hi || hi > 1) {\r\nmad->mad_hdr.status\r\n= __constant_cpu_to_be16(DM_MAD_STATUS_NO_IOC);\r\nreturn;\r\n}\r\nsvc_entries = (struct ib_dm_svc_entries *)mad->data;\r\nmemset(svc_entries, 0, sizeof *svc_entries);\r\nsvc_entries->service_entries[0].id = cpu_to_be64(ioc_guid);\r\nsnprintf(svc_entries->service_entries[0].name,\r\nsizeof(svc_entries->service_entries[0].name),\r\n"%s%016llx",\r\nSRP_SERVICE_NAME_PREFIX,\r\nioc_guid);\r\nmad->mad_hdr.status = 0;\r\n}\r\nstatic void srpt_mgmt_method_get(struct srpt_port *sp, struct ib_mad *rq_mad,\r\nstruct ib_dm_mad *rsp_mad)\r\n{\r\nu16 attr_id;\r\nu32 slot;\r\nu8 hi, lo;\r\nattr_id = be16_to_cpu(rq_mad->mad_hdr.attr_id);\r\nswitch (attr_id) {\r\ncase DM_ATTR_CLASS_PORT_INFO:\r\nsrpt_get_class_port_info(rsp_mad);\r\nbreak;\r\ncase DM_ATTR_IOU_INFO:\r\nsrpt_get_iou(rsp_mad);\r\nbreak;\r\ncase DM_ATTR_IOC_PROFILE:\r\nslot = be32_to_cpu(rq_mad->mad_hdr.attr_mod);\r\nsrpt_get_ioc(sp, slot, rsp_mad);\r\nbreak;\r\ncase DM_ATTR_SVC_ENTRIES:\r\nslot = be32_to_cpu(rq_mad->mad_hdr.attr_mod);\r\nhi = (u8) ((slot >> 8) & 0xff);\r\nlo = (u8) (slot & 0xff);\r\nslot = (u16) ((slot >> 16) & 0xffff);\r\nsrpt_get_svc_entries(srpt_service_guid,\r\nslot, hi, lo, rsp_mad);\r\nbreak;\r\ndefault:\r\nrsp_mad->mad_hdr.status =\r\n__constant_cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD_ATTR);\r\nbreak;\r\n}\r\n}\r\nstatic void srpt_mad_send_handler(struct ib_mad_agent *mad_agent,\r\nstruct ib_mad_send_wc *mad_wc)\r\n{\r\nib_destroy_ah(mad_wc->send_buf->ah);\r\nib_free_send_mad(mad_wc->send_buf);\r\n}\r\nstatic void srpt_mad_recv_handler(struct ib_mad_agent *mad_agent,\r\nstruct ib_mad_recv_wc *mad_wc)\r\n{\r\nstruct srpt_port *sport = (struct srpt_port *)mad_agent->context;\r\nstruct ib_ah *ah;\r\nstruct ib_mad_send_buf *rsp;\r\nstruct ib_dm_mad *dm_mad;\r\nif (!mad_wc || !mad_wc->recv_buf.mad)\r\nreturn;\r\nah = ib_create_ah_from_wc(mad_agent->qp->pd, mad_wc->wc,\r\nmad_wc->recv_buf.grh, mad_agent->port_num);\r\nif (IS_ERR(ah))\r\ngoto err;\r\nBUILD_BUG_ON(offsetof(struct ib_dm_mad, data) != IB_MGMT_DEVICE_HDR);\r\nrsp = ib_create_send_mad(mad_agent, mad_wc->wc->src_qp,\r\nmad_wc->wc->pkey_index, 0,\r\nIB_MGMT_DEVICE_HDR, IB_MGMT_DEVICE_DATA,\r\nGFP_KERNEL);\r\nif (IS_ERR(rsp))\r\ngoto err_rsp;\r\nrsp->ah = ah;\r\ndm_mad = rsp->mad;\r\nmemcpy(dm_mad, mad_wc->recv_buf.mad, sizeof *dm_mad);\r\ndm_mad->mad_hdr.method = IB_MGMT_METHOD_GET_RESP;\r\ndm_mad->mad_hdr.status = 0;\r\nswitch (mad_wc->recv_buf.mad->mad_hdr.method) {\r\ncase IB_MGMT_METHOD_GET:\r\nsrpt_mgmt_method_get(sport, mad_wc->recv_buf.mad, dm_mad);\r\nbreak;\r\ncase IB_MGMT_METHOD_SET:\r\ndm_mad->mad_hdr.status =\r\n__constant_cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD_ATTR);\r\nbreak;\r\ndefault:\r\ndm_mad->mad_hdr.status =\r\n__constant_cpu_to_be16(DM_MAD_STATUS_UNSUP_METHOD);\r\nbreak;\r\n}\r\nif (!ib_post_send_mad(rsp, NULL)) {\r\nib_free_recv_mad(mad_wc);\r\nreturn;\r\n}\r\nib_free_send_mad(rsp);\r\nerr_rsp:\r\nib_destroy_ah(ah);\r\nerr:\r\nib_free_recv_mad(mad_wc);\r\n}\r\nstatic int srpt_refresh_port(struct srpt_port *sport)\r\n{\r\nstruct ib_mad_reg_req reg_req;\r\nstruct ib_port_modify port_modify;\r\nstruct ib_port_attr port_attr;\r\nint ret;\r\nmemset(&port_modify, 0, sizeof port_modify);\r\nport_modify.set_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP;\r\nport_modify.clr_port_cap_mask = 0;\r\nret = ib_modify_port(sport->sdev->device, sport->port, 0, &port_modify);\r\nif (ret)\r\ngoto err_mod_port;\r\nret = ib_query_port(sport->sdev->device, sport->port, &port_attr);\r\nif (ret)\r\ngoto err_query_port;\r\nsport->sm_lid = port_attr.sm_lid;\r\nsport->lid = port_attr.lid;\r\nret = ib_query_gid(sport->sdev->device, sport->port, 0, &sport->gid);\r\nif (ret)\r\ngoto err_query_port;\r\nif (!sport->mad_agent) {\r\nmemset(&reg_req, 0, sizeof reg_req);\r\nreg_req.mgmt_class = IB_MGMT_CLASS_DEVICE_MGMT;\r\nreg_req.mgmt_class_version = IB_MGMT_BASE_VERSION;\r\nset_bit(IB_MGMT_METHOD_GET, reg_req.method_mask);\r\nset_bit(IB_MGMT_METHOD_SET, reg_req.method_mask);\r\nsport->mad_agent = ib_register_mad_agent(sport->sdev->device,\r\nsport->port,\r\nIB_QPT_GSI,\r\n&reg_req, 0,\r\nsrpt_mad_send_handler,\r\nsrpt_mad_recv_handler,\r\nsport);\r\nif (IS_ERR(sport->mad_agent)) {\r\nret = PTR_ERR(sport->mad_agent);\r\nsport->mad_agent = NULL;\r\ngoto err_query_port;\r\n}\r\n}\r\nreturn 0;\r\nerr_query_port:\r\nport_modify.set_port_cap_mask = 0;\r\nport_modify.clr_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP;\r\nib_modify_port(sport->sdev->device, sport->port, 0, &port_modify);\r\nerr_mod_port:\r\nreturn ret;\r\n}\r\nstatic void srpt_unregister_mad_agent(struct srpt_device *sdev)\r\n{\r\nstruct ib_port_modify port_modify = {\r\n.clr_port_cap_mask = IB_PORT_DEVICE_MGMT_SUP,\r\n};\r\nstruct srpt_port *sport;\r\nint i;\r\nfor (i = 1; i <= sdev->device->phys_port_cnt; i++) {\r\nsport = &sdev->port[i - 1];\r\nWARN_ON(sport->port != i);\r\nif (ib_modify_port(sdev->device, i, 0, &port_modify) < 0)\r\nprintk(KERN_ERR "disabling MAD processing failed.\n");\r\nif (sport->mad_agent) {\r\nib_unregister_mad_agent(sport->mad_agent);\r\nsport->mad_agent = NULL;\r\n}\r\n}\r\n}\r\nstatic struct srpt_ioctx *srpt_alloc_ioctx(struct srpt_device *sdev,\r\nint ioctx_size, int dma_size,\r\nenum dma_data_direction dir)\r\n{\r\nstruct srpt_ioctx *ioctx;\r\nioctx = kmalloc(ioctx_size, GFP_KERNEL);\r\nif (!ioctx)\r\ngoto err;\r\nioctx->buf = kmalloc(dma_size, GFP_KERNEL);\r\nif (!ioctx->buf)\r\ngoto err_free_ioctx;\r\nioctx->dma = ib_dma_map_single(sdev->device, ioctx->buf, dma_size, dir);\r\nif (ib_dma_mapping_error(sdev->device, ioctx->dma))\r\ngoto err_free_buf;\r\nreturn ioctx;\r\nerr_free_buf:\r\nkfree(ioctx->buf);\r\nerr_free_ioctx:\r\nkfree(ioctx);\r\nerr:\r\nreturn NULL;\r\n}\r\nstatic void srpt_free_ioctx(struct srpt_device *sdev, struct srpt_ioctx *ioctx,\r\nint dma_size, enum dma_data_direction dir)\r\n{\r\nif (!ioctx)\r\nreturn;\r\nib_dma_unmap_single(sdev->device, ioctx->dma, dma_size, dir);\r\nkfree(ioctx->buf);\r\nkfree(ioctx);\r\n}\r\nstatic struct srpt_ioctx **srpt_alloc_ioctx_ring(struct srpt_device *sdev,\r\nint ring_size, int ioctx_size,\r\nint dma_size, enum dma_data_direction dir)\r\n{\r\nstruct srpt_ioctx **ring;\r\nint i;\r\nWARN_ON(ioctx_size != sizeof(struct srpt_recv_ioctx)\r\n&& ioctx_size != sizeof(struct srpt_send_ioctx));\r\nring = kmalloc(ring_size * sizeof(ring[0]), GFP_KERNEL);\r\nif (!ring)\r\ngoto out;\r\nfor (i = 0; i < ring_size; ++i) {\r\nring[i] = srpt_alloc_ioctx(sdev, ioctx_size, dma_size, dir);\r\nif (!ring[i])\r\ngoto err;\r\nring[i]->index = i;\r\n}\r\ngoto out;\r\nerr:\r\nwhile (--i >= 0)\r\nsrpt_free_ioctx(sdev, ring[i], dma_size, dir);\r\nkfree(ring);\r\nring = NULL;\r\nout:\r\nreturn ring;\r\n}\r\nstatic void srpt_free_ioctx_ring(struct srpt_ioctx **ioctx_ring,\r\nstruct srpt_device *sdev, int ring_size,\r\nint dma_size, enum dma_data_direction dir)\r\n{\r\nint i;\r\nfor (i = 0; i < ring_size; ++i)\r\nsrpt_free_ioctx(sdev, ioctx_ring[i], dma_size, dir);\r\nkfree(ioctx_ring);\r\n}\r\nstatic enum srpt_command_state srpt_get_cmd_state(struct srpt_send_ioctx *ioctx)\r\n{\r\nenum srpt_command_state state;\r\nunsigned long flags;\r\nBUG_ON(!ioctx);\r\nspin_lock_irqsave(&ioctx->spinlock, flags);\r\nstate = ioctx->state;\r\nspin_unlock_irqrestore(&ioctx->spinlock, flags);\r\nreturn state;\r\n}\r\nstatic enum srpt_command_state srpt_set_cmd_state(struct srpt_send_ioctx *ioctx,\r\nenum srpt_command_state new)\r\n{\r\nenum srpt_command_state previous;\r\nunsigned long flags;\r\nBUG_ON(!ioctx);\r\nspin_lock_irqsave(&ioctx->spinlock, flags);\r\nprevious = ioctx->state;\r\nif (previous != SRPT_STATE_DONE)\r\nioctx->state = new;\r\nspin_unlock_irqrestore(&ioctx->spinlock, flags);\r\nreturn previous;\r\n}\r\nstatic bool srpt_test_and_set_cmd_state(struct srpt_send_ioctx *ioctx,\r\nenum srpt_command_state old,\r\nenum srpt_command_state new)\r\n{\r\nenum srpt_command_state previous;\r\nunsigned long flags;\r\nWARN_ON(!ioctx);\r\nWARN_ON(old == SRPT_STATE_DONE);\r\nWARN_ON(new == SRPT_STATE_NEW);\r\nspin_lock_irqsave(&ioctx->spinlock, flags);\r\nprevious = ioctx->state;\r\nif (previous == old)\r\nioctx->state = new;\r\nspin_unlock_irqrestore(&ioctx->spinlock, flags);\r\nreturn previous == old;\r\n}\r\nstatic int srpt_post_recv(struct srpt_device *sdev,\r\nstruct srpt_recv_ioctx *ioctx)\r\n{\r\nstruct ib_sge list;\r\nstruct ib_recv_wr wr, *bad_wr;\r\nBUG_ON(!sdev);\r\nwr.wr_id = encode_wr_id(SRPT_RECV, ioctx->ioctx.index);\r\nlist.addr = ioctx->ioctx.dma;\r\nlist.length = srp_max_req_size;\r\nlist.lkey = sdev->mr->lkey;\r\nwr.next = NULL;\r\nwr.sg_list = &list;\r\nwr.num_sge = 1;\r\nreturn ib_post_srq_recv(sdev->srq, &wr, &bad_wr);\r\n}\r\nstatic int srpt_post_send(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx, int len)\r\n{\r\nstruct ib_sge list;\r\nstruct ib_send_wr wr, *bad_wr;\r\nstruct srpt_device *sdev = ch->sport->sdev;\r\nint ret;\r\natomic_inc(&ch->req_lim);\r\nret = -ENOMEM;\r\nif (unlikely(atomic_dec_return(&ch->sq_wr_avail) < 0)) {\r\nprintk(KERN_WARNING "IB send queue full (needed 1)\n");\r\ngoto out;\r\n}\r\nib_dma_sync_single_for_device(sdev->device, ioctx->ioctx.dma, len,\r\nDMA_TO_DEVICE);\r\nlist.addr = ioctx->ioctx.dma;\r\nlist.length = len;\r\nlist.lkey = sdev->mr->lkey;\r\nwr.next = NULL;\r\nwr.wr_id = encode_wr_id(SRPT_SEND, ioctx->ioctx.index);\r\nwr.sg_list = &list;\r\nwr.num_sge = 1;\r\nwr.opcode = IB_WR_SEND;\r\nwr.send_flags = IB_SEND_SIGNALED;\r\nret = ib_post_send(ch->qp, &wr, &bad_wr);\r\nout:\r\nif (ret < 0) {\r\natomic_inc(&ch->sq_wr_avail);\r\natomic_dec(&ch->req_lim);\r\n}\r\nreturn ret;\r\n}\r\nstatic int srpt_get_desc_tbl(struct srpt_send_ioctx *ioctx,\r\nstruct srp_cmd *srp_cmd,\r\nenum dma_data_direction *dir, u64 *data_len)\r\n{\r\nstruct srp_indirect_buf *idb;\r\nstruct srp_direct_buf *db;\r\nunsigned add_cdb_offset;\r\nint ret;\r\nBUILD_BUG_ON(!__same_type(srp_cmd->add_data[0], (s8)0)\r\n&& !__same_type(srp_cmd->add_data[0], (u8)0));\r\nBUG_ON(!dir);\r\nBUG_ON(!data_len);\r\nret = 0;\r\n*data_len = 0;\r\n*dir = DMA_NONE;\r\nif (srp_cmd->buf_fmt & 0xf)\r\n*dir = DMA_FROM_DEVICE;\r\nelse if (srp_cmd->buf_fmt >> 4)\r\n*dir = DMA_TO_DEVICE;\r\nadd_cdb_offset = srp_cmd->add_cdb_len & ~3;\r\nif (((srp_cmd->buf_fmt & 0xf) == SRP_DATA_DESC_DIRECT) ||\r\n((srp_cmd->buf_fmt >> 4) == SRP_DATA_DESC_DIRECT)) {\r\nioctx->n_rbuf = 1;\r\nioctx->rbufs = &ioctx->single_rbuf;\r\ndb = (struct srp_direct_buf *)(srp_cmd->add_data\r\n+ add_cdb_offset);\r\nmemcpy(ioctx->rbufs, db, sizeof *db);\r\n*data_len = be32_to_cpu(db->len);\r\n} else if (((srp_cmd->buf_fmt & 0xf) == SRP_DATA_DESC_INDIRECT) ||\r\n((srp_cmd->buf_fmt >> 4) == SRP_DATA_DESC_INDIRECT)) {\r\nidb = (struct srp_indirect_buf *)(srp_cmd->add_data\r\n+ add_cdb_offset);\r\nioctx->n_rbuf = be32_to_cpu(idb->table_desc.len) / sizeof *db;\r\nif (ioctx->n_rbuf >\r\n(srp_cmd->data_out_desc_cnt + srp_cmd->data_in_desc_cnt)) {\r\nprintk(KERN_ERR "received unsupported SRP_CMD request"\r\n" type (%u out + %u in != %u / %zu)\n",\r\nsrp_cmd->data_out_desc_cnt,\r\nsrp_cmd->data_in_desc_cnt,\r\nbe32_to_cpu(idb->table_desc.len),\r\nsizeof(*db));\r\nioctx->n_rbuf = 0;\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (ioctx->n_rbuf == 1)\r\nioctx->rbufs = &ioctx->single_rbuf;\r\nelse {\r\nioctx->rbufs =\r\nkmalloc(ioctx->n_rbuf * sizeof *db, GFP_ATOMIC);\r\nif (!ioctx->rbufs) {\r\nioctx->n_rbuf = 0;\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\n}\r\ndb = idb->desc_list;\r\nmemcpy(ioctx->rbufs, db, ioctx->n_rbuf * sizeof *db);\r\n*data_len = be32_to_cpu(idb->len);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int srpt_init_ch_qp(struct srpt_rdma_ch *ch, struct ib_qp *qp)\r\n{\r\nstruct ib_qp_attr *attr;\r\nint ret;\r\nattr = kzalloc(sizeof *attr, GFP_KERNEL);\r\nif (!attr)\r\nreturn -ENOMEM;\r\nattr->qp_state = IB_QPS_INIT;\r\nattr->qp_access_flags = IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE;\r\nattr->port_num = ch->sport->port;\r\nattr->pkey_index = 0;\r\nret = ib_modify_qp(qp, attr,\r\nIB_QP_STATE | IB_QP_ACCESS_FLAGS | IB_QP_PORT |\r\nIB_QP_PKEY_INDEX);\r\nkfree(attr);\r\nreturn ret;\r\n}\r\nstatic int srpt_ch_qp_rtr(struct srpt_rdma_ch *ch, struct ib_qp *qp)\r\n{\r\nstruct ib_qp_attr qp_attr;\r\nint attr_mask;\r\nint ret;\r\nqp_attr.qp_state = IB_QPS_RTR;\r\nret = ib_cm_init_qp_attr(ch->cm_id, &qp_attr, &attr_mask);\r\nif (ret)\r\ngoto out;\r\nqp_attr.max_dest_rd_atomic = 4;\r\nret = ib_modify_qp(qp, &qp_attr, attr_mask);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int srpt_ch_qp_rts(struct srpt_rdma_ch *ch, struct ib_qp *qp)\r\n{\r\nstruct ib_qp_attr qp_attr;\r\nint attr_mask;\r\nint ret;\r\nqp_attr.qp_state = IB_QPS_RTS;\r\nret = ib_cm_init_qp_attr(ch->cm_id, &qp_attr, &attr_mask);\r\nif (ret)\r\ngoto out;\r\nqp_attr.max_rd_atomic = 4;\r\nret = ib_modify_qp(qp, &qp_attr, attr_mask);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int srpt_ch_qp_err(struct srpt_rdma_ch *ch)\r\n{\r\nstruct ib_qp_attr qp_attr;\r\nqp_attr.qp_state = IB_QPS_ERR;\r\nreturn ib_modify_qp(ch->qp, &qp_attr, IB_QP_STATE);\r\n}\r\nstatic void srpt_unmap_sg_to_ib_sge(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx)\r\n{\r\nstruct scatterlist *sg;\r\nenum dma_data_direction dir;\r\nBUG_ON(!ch);\r\nBUG_ON(!ioctx);\r\nBUG_ON(ioctx->n_rdma && !ioctx->rdma_ius);\r\nwhile (ioctx->n_rdma)\r\nkfree(ioctx->rdma_ius[--ioctx->n_rdma].sge);\r\nkfree(ioctx->rdma_ius);\r\nioctx->rdma_ius = NULL;\r\nif (ioctx->mapped_sg_count) {\r\nsg = ioctx->sg;\r\nWARN_ON(!sg);\r\ndir = ioctx->cmd.data_direction;\r\nBUG_ON(dir == DMA_NONE);\r\nib_dma_unmap_sg(ch->sport->sdev->device, sg, ioctx->sg_cnt,\r\nopposite_dma_dir(dir));\r\nioctx->mapped_sg_count = 0;\r\n}\r\n}\r\nstatic int srpt_map_sg_to_ib_sge(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx)\r\n{\r\nstruct ib_device *dev = ch->sport->sdev->device;\r\nstruct se_cmd *cmd;\r\nstruct scatterlist *sg, *sg_orig;\r\nint sg_cnt;\r\nenum dma_data_direction dir;\r\nstruct rdma_iu *riu;\r\nstruct srp_direct_buf *db;\r\ndma_addr_t dma_addr;\r\nstruct ib_sge *sge;\r\nu64 raddr;\r\nu32 rsize;\r\nu32 tsize;\r\nu32 dma_len;\r\nint count, nrdma;\r\nint i, j, k;\r\nBUG_ON(!ch);\r\nBUG_ON(!ioctx);\r\ncmd = &ioctx->cmd;\r\ndir = cmd->data_direction;\r\nBUG_ON(dir == DMA_NONE);\r\nioctx->sg = sg = sg_orig = cmd->t_data_sg;\r\nioctx->sg_cnt = sg_cnt = cmd->t_data_nents;\r\ncount = ib_dma_map_sg(ch->sport->sdev->device, sg, sg_cnt,\r\nopposite_dma_dir(dir));\r\nif (unlikely(!count))\r\nreturn -EAGAIN;\r\nioctx->mapped_sg_count = count;\r\nif (ioctx->rdma_ius && ioctx->n_rdma_ius)\r\nnrdma = ioctx->n_rdma_ius;\r\nelse {\r\nnrdma = (count + SRPT_DEF_SG_PER_WQE - 1) / SRPT_DEF_SG_PER_WQE\r\n+ ioctx->n_rbuf;\r\nioctx->rdma_ius = kzalloc(nrdma * sizeof *riu, GFP_KERNEL);\r\nif (!ioctx->rdma_ius)\r\ngoto free_mem;\r\nioctx->n_rdma_ius = nrdma;\r\n}\r\ndb = ioctx->rbufs;\r\ntsize = cmd->data_length;\r\ndma_len = ib_sg_dma_len(dev, &sg[0]);\r\nriu = ioctx->rdma_ius;\r\nfor (i = 0, j = 0;\r\nj < count && i < ioctx->n_rbuf && tsize > 0; ++i, ++riu, ++db) {\r\nrsize = be32_to_cpu(db->len);\r\nraddr = be64_to_cpu(db->va);\r\nriu->raddr = raddr;\r\nriu->rkey = be32_to_cpu(db->key);\r\nriu->sge_cnt = 0;\r\nwhile (rsize > 0 && tsize > 0) {\r\nif (rsize >= dma_len) {\r\ntsize -= dma_len;\r\nrsize -= dma_len;\r\nraddr += dma_len;\r\nif (tsize > 0) {\r\n++j;\r\nif (j < count) {\r\nsg = sg_next(sg);\r\ndma_len = ib_sg_dma_len(\r\ndev, sg);\r\n}\r\n}\r\n} else {\r\ntsize -= rsize;\r\ndma_len -= rsize;\r\nrsize = 0;\r\n}\r\n++riu->sge_cnt;\r\nif (rsize > 0 && riu->sge_cnt == SRPT_DEF_SG_PER_WQE) {\r\n++ioctx->n_rdma;\r\nriu->sge =\r\nkmalloc(riu->sge_cnt * sizeof *riu->sge,\r\nGFP_KERNEL);\r\nif (!riu->sge)\r\ngoto free_mem;\r\n++riu;\r\nriu->sge_cnt = 0;\r\nriu->raddr = raddr;\r\nriu->rkey = be32_to_cpu(db->key);\r\n}\r\n}\r\n++ioctx->n_rdma;\r\nriu->sge = kmalloc(riu->sge_cnt * sizeof *riu->sge,\r\nGFP_KERNEL);\r\nif (!riu->sge)\r\ngoto free_mem;\r\n}\r\ndb = ioctx->rbufs;\r\ntsize = cmd->data_length;\r\nriu = ioctx->rdma_ius;\r\nsg = sg_orig;\r\ndma_len = ib_sg_dma_len(dev, &sg[0]);\r\ndma_addr = ib_sg_dma_address(dev, &sg[0]);\r\nfor (i = 0, j = 0;\r\nj < count && i < ioctx->n_rbuf && tsize > 0; ++i, ++riu, ++db) {\r\nrsize = be32_to_cpu(db->len);\r\nsge = riu->sge;\r\nk = 0;\r\nwhile (rsize > 0 && tsize > 0) {\r\nsge->addr = dma_addr;\r\nsge->lkey = ch->sport->sdev->mr->lkey;\r\nif (rsize >= dma_len) {\r\nsge->length =\r\n(tsize < dma_len) ? tsize : dma_len;\r\ntsize -= dma_len;\r\nrsize -= dma_len;\r\nif (tsize > 0) {\r\n++j;\r\nif (j < count) {\r\nsg = sg_next(sg);\r\ndma_len = ib_sg_dma_len(\r\ndev, sg);\r\ndma_addr = ib_sg_dma_address(\r\ndev, sg);\r\n}\r\n}\r\n} else {\r\nsge->length = (tsize < rsize) ? tsize : rsize;\r\ntsize -= rsize;\r\ndma_len -= rsize;\r\ndma_addr += rsize;\r\nrsize = 0;\r\n}\r\n++k;\r\nif (k == riu->sge_cnt && rsize > 0 && tsize > 0) {\r\n++riu;\r\nsge = riu->sge;\r\nk = 0;\r\n} else if (rsize > 0 && tsize > 0)\r\n++sge;\r\n}\r\n}\r\nreturn 0;\r\nfree_mem:\r\nsrpt_unmap_sg_to_ib_sge(ch, ioctx);\r\nreturn -ENOMEM;\r\n}\r\nstatic struct srpt_send_ioctx *srpt_get_send_ioctx(struct srpt_rdma_ch *ch)\r\n{\r\nstruct srpt_send_ioctx *ioctx;\r\nunsigned long flags;\r\nBUG_ON(!ch);\r\nioctx = NULL;\r\nspin_lock_irqsave(&ch->spinlock, flags);\r\nif (!list_empty(&ch->free_list)) {\r\nioctx = list_first_entry(&ch->free_list,\r\nstruct srpt_send_ioctx, free_list);\r\nlist_del(&ioctx->free_list);\r\n}\r\nspin_unlock_irqrestore(&ch->spinlock, flags);\r\nif (!ioctx)\r\nreturn ioctx;\r\nBUG_ON(ioctx->ch != ch);\r\nspin_lock_init(&ioctx->spinlock);\r\nioctx->state = SRPT_STATE_NEW;\r\nioctx->n_rbuf = 0;\r\nioctx->rbufs = NULL;\r\nioctx->n_rdma = 0;\r\nioctx->n_rdma_ius = 0;\r\nioctx->rdma_ius = NULL;\r\nioctx->mapped_sg_count = 0;\r\ninit_completion(&ioctx->tx_done);\r\nioctx->queue_status_only = false;\r\nmemset(&ioctx->cmd, 0, sizeof(ioctx->cmd));\r\nmemset(&ioctx->sense_data, 0, sizeof(ioctx->sense_data));\r\nreturn ioctx;\r\n}\r\nstatic int srpt_abort_cmd(struct srpt_send_ioctx *ioctx)\r\n{\r\nenum srpt_command_state state;\r\nunsigned long flags;\r\nBUG_ON(!ioctx);\r\nspin_lock_irqsave(&ioctx->spinlock, flags);\r\nstate = ioctx->state;\r\nswitch (state) {\r\ncase SRPT_STATE_NEED_DATA:\r\nioctx->state = SRPT_STATE_DATA_IN;\r\nbreak;\r\ncase SRPT_STATE_DATA_IN:\r\ncase SRPT_STATE_CMD_RSP_SENT:\r\ncase SRPT_STATE_MGMT_RSP_SENT:\r\nioctx->state = SRPT_STATE_DONE;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&ioctx->spinlock, flags);\r\nif (state == SRPT_STATE_DONE) {\r\nstruct srpt_rdma_ch *ch = ioctx->ch;\r\nBUG_ON(ch->sess == NULL);\r\ntarget_put_sess_cmd(ch->sess, &ioctx->cmd);\r\ngoto out;\r\n}\r\npr_debug("Aborting cmd with state %d and tag %lld\n", state,\r\nioctx->tag);\r\nswitch (state) {\r\ncase SRPT_STATE_NEW:\r\ncase SRPT_STATE_DATA_IN:\r\ncase SRPT_STATE_MGMT:\r\nWARN_ON(!transport_check_aborted_status(&ioctx->cmd, false));\r\nbreak;\r\ncase SRPT_STATE_NEED_DATA:\r\nspin_lock_irqsave(&ioctx->cmd.t_state_lock, flags);\r\nioctx->cmd.transport_state &= ~CMD_T_ACTIVE;\r\nspin_unlock_irqrestore(&ioctx->cmd.t_state_lock, flags);\r\nbreak;\r\ncase SRPT_STATE_CMD_RSP_SENT:\r\nsrpt_unmap_sg_to_ib_sge(ioctx->ch, ioctx);\r\ntarget_put_sess_cmd(ioctx->ch->sess, &ioctx->cmd);\r\nbreak;\r\ncase SRPT_STATE_MGMT_RSP_SENT:\r\nsrpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\r\ntarget_put_sess_cmd(ioctx->ch->sess, &ioctx->cmd);\r\nbreak;\r\ndefault:\r\nWARN(1, "Unexpected command state (%d)", state);\r\nbreak;\r\n}\r\nout:\r\nreturn state;\r\n}\r\nstatic void srpt_handle_send_err_comp(struct srpt_rdma_ch *ch, u64 wr_id)\r\n{\r\nstruct srpt_send_ioctx *ioctx;\r\nenum srpt_command_state state;\r\nstruct se_cmd *cmd;\r\nu32 index;\r\natomic_inc(&ch->sq_wr_avail);\r\nindex = idx_from_wr_id(wr_id);\r\nioctx = ch->ioctx_ring[index];\r\nstate = srpt_get_cmd_state(ioctx);\r\ncmd = &ioctx->cmd;\r\nWARN_ON(state != SRPT_STATE_CMD_RSP_SENT\r\n&& state != SRPT_STATE_MGMT_RSP_SENT\r\n&& state != SRPT_STATE_NEED_DATA\r\n&& state != SRPT_STATE_DONE);\r\nif (state == SRPT_STATE_CMD_RSP_SENT\r\n|| state == SRPT_STATE_MGMT_RSP_SENT)\r\natomic_dec(&ch->req_lim);\r\nsrpt_abort_cmd(ioctx);\r\n}\r\nstatic void srpt_handle_send_comp(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx)\r\n{\r\nenum srpt_command_state state;\r\natomic_inc(&ch->sq_wr_avail);\r\nstate = srpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\r\nif (WARN_ON(state != SRPT_STATE_CMD_RSP_SENT\r\n&& state != SRPT_STATE_MGMT_RSP_SENT\r\n&& state != SRPT_STATE_DONE))\r\npr_debug("state = %d\n", state);\r\nif (state != SRPT_STATE_DONE) {\r\nsrpt_unmap_sg_to_ib_sge(ch, ioctx);\r\ntransport_generic_free_cmd(&ioctx->cmd, 0);\r\n} else {\r\nprintk(KERN_ERR "IB completion has been received too late for"\r\n" wr_id = %u.\n", ioctx->ioctx.index);\r\n}\r\n}\r\nstatic void srpt_handle_rdma_comp(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx,\r\nenum srpt_opcode opcode)\r\n{\r\nWARN_ON(ioctx->n_rdma <= 0);\r\natomic_add(ioctx->n_rdma, &ch->sq_wr_avail);\r\nif (opcode == SRPT_RDMA_READ_LAST) {\r\nif (srpt_test_and_set_cmd_state(ioctx, SRPT_STATE_NEED_DATA,\r\nSRPT_STATE_DATA_IN))\r\ntarget_execute_cmd(&ioctx->cmd);\r\nelse\r\nprintk(KERN_ERR "%s[%d]: wrong state = %d\n", __func__,\r\n__LINE__, srpt_get_cmd_state(ioctx));\r\n} else if (opcode == SRPT_RDMA_ABORT) {\r\nioctx->rdma_aborted = true;\r\n} else {\r\nWARN(true, "unexpected opcode %d\n", opcode);\r\n}\r\n}\r\nstatic void srpt_handle_rdma_err_comp(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx,\r\nenum srpt_opcode opcode)\r\n{\r\nstruct se_cmd *cmd;\r\nenum srpt_command_state state;\r\ncmd = &ioctx->cmd;\r\nstate = srpt_get_cmd_state(ioctx);\r\nswitch (opcode) {\r\ncase SRPT_RDMA_READ_LAST:\r\nif (ioctx->n_rdma <= 0) {\r\nprintk(KERN_ERR "Received invalid RDMA read"\r\n" error completion with idx %d\n",\r\nioctx->ioctx.index);\r\nbreak;\r\n}\r\natomic_add(ioctx->n_rdma, &ch->sq_wr_avail);\r\nif (state == SRPT_STATE_NEED_DATA)\r\nsrpt_abort_cmd(ioctx);\r\nelse\r\nprintk(KERN_ERR "%s[%d]: wrong state = %d\n",\r\n__func__, __LINE__, state);\r\nbreak;\r\ncase SRPT_RDMA_WRITE_LAST:\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "%s[%d]: opcode = %u\n", __func__,\r\n__LINE__, opcode);\r\nbreak;\r\n}\r\n}\r\nstatic int srpt_build_cmd_rsp(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx, u64 tag,\r\nint status)\r\n{\r\nstruct srp_rsp *srp_rsp;\r\nconst u8 *sense_data;\r\nint sense_data_len, max_sense_len;\r\nWARN_ON(status & 1);\r\nsrp_rsp = ioctx->ioctx.buf;\r\nBUG_ON(!srp_rsp);\r\nsense_data = ioctx->sense_data;\r\nsense_data_len = ioctx->cmd.scsi_sense_length;\r\nWARN_ON(sense_data_len > sizeof(ioctx->sense_data));\r\nmemset(srp_rsp, 0, sizeof *srp_rsp);\r\nsrp_rsp->opcode = SRP_RSP;\r\nsrp_rsp->req_lim_delta =\r\n__constant_cpu_to_be32(1 + atomic_xchg(&ch->req_lim_delta, 0));\r\nsrp_rsp->tag = tag;\r\nsrp_rsp->status = status;\r\nif (sense_data_len) {\r\nBUILD_BUG_ON(MIN_MAX_RSP_SIZE <= sizeof(*srp_rsp));\r\nmax_sense_len = ch->max_ti_iu_len - sizeof(*srp_rsp);\r\nif (sense_data_len > max_sense_len) {\r\nprintk(KERN_WARNING "truncated sense data from %d to %d"\r\n" bytes\n", sense_data_len, max_sense_len);\r\nsense_data_len = max_sense_len;\r\n}\r\nsrp_rsp->flags |= SRP_RSP_FLAG_SNSVALID;\r\nsrp_rsp->sense_data_len = cpu_to_be32(sense_data_len);\r\nmemcpy(srp_rsp + 1, sense_data, sense_data_len);\r\n}\r\nreturn sizeof(*srp_rsp) + sense_data_len;\r\n}\r\nstatic int srpt_build_tskmgmt_rsp(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx,\r\nu8 rsp_code, u64 tag)\r\n{\r\nstruct srp_rsp *srp_rsp;\r\nint resp_data_len;\r\nint resp_len;\r\nresp_data_len = 4;\r\nresp_len = sizeof(*srp_rsp) + resp_data_len;\r\nsrp_rsp = ioctx->ioctx.buf;\r\nBUG_ON(!srp_rsp);\r\nmemset(srp_rsp, 0, sizeof *srp_rsp);\r\nsrp_rsp->opcode = SRP_RSP;\r\nsrp_rsp->req_lim_delta = __constant_cpu_to_be32(1\r\n+ atomic_xchg(&ch->req_lim_delta, 0));\r\nsrp_rsp->tag = tag;\r\nsrp_rsp->flags |= SRP_RSP_FLAG_RSPVALID;\r\nsrp_rsp->resp_data_len = cpu_to_be32(resp_data_len);\r\nsrp_rsp->data[3] = rsp_code;\r\nreturn resp_len;\r\n}\r\nstatic uint64_t srpt_unpack_lun(const uint8_t *lun, int len)\r\n{\r\nuint64_t res = NO_SUCH_LUN;\r\nint addressing_method;\r\nif (unlikely(len < 2)) {\r\nprintk(KERN_ERR "Illegal LUN length %d, expected 2 bytes or "\r\n"more", len);\r\ngoto out;\r\n}\r\nswitch (len) {\r\ncase 8:\r\nif ((*((__be64 *)lun) &\r\n__constant_cpu_to_be64(0x0000FFFFFFFFFFFFLL)) != 0)\r\ngoto out_err;\r\nbreak;\r\ncase 4:\r\nif (*((__be16 *)&lun[2]) != 0)\r\ngoto out_err;\r\nbreak;\r\ncase 6:\r\nif (*((__be32 *)&lun[2]) != 0)\r\ngoto out_err;\r\nbreak;\r\ncase 2:\r\nbreak;\r\ndefault:\r\ngoto out_err;\r\n}\r\naddressing_method = (*lun) >> 6;\r\nswitch (addressing_method) {\r\ncase SCSI_LUN_ADDR_METHOD_PERIPHERAL:\r\ncase SCSI_LUN_ADDR_METHOD_FLAT:\r\ncase SCSI_LUN_ADDR_METHOD_LUN:\r\nres = *(lun + 1) | (((*lun) & 0x3f) << 8);\r\nbreak;\r\ncase SCSI_LUN_ADDR_METHOD_EXTENDED_LUN:\r\ndefault:\r\nprintk(KERN_ERR "Unimplemented LUN addressing method %u",\r\naddressing_method);\r\nbreak;\r\n}\r\nout:\r\nreturn res;\r\nout_err:\r\nprintk(KERN_ERR "Support for multi-level LUNs has not yet been"\r\n" implemented");\r\ngoto out;\r\n}\r\nstatic int srpt_check_stop_free(struct se_cmd *cmd)\r\n{\r\nstruct srpt_send_ioctx *ioctx = container_of(cmd,\r\nstruct srpt_send_ioctx, cmd);\r\nreturn target_put_sess_cmd(ioctx->ch->sess, &ioctx->cmd);\r\n}\r\nstatic int srpt_handle_cmd(struct srpt_rdma_ch *ch,\r\nstruct srpt_recv_ioctx *recv_ioctx,\r\nstruct srpt_send_ioctx *send_ioctx)\r\n{\r\nstruct se_cmd *cmd;\r\nstruct srp_cmd *srp_cmd;\r\nuint64_t unpacked_lun;\r\nu64 data_len;\r\nenum dma_data_direction dir;\r\nsense_reason_t ret;\r\nint rc;\r\nBUG_ON(!send_ioctx);\r\nsrp_cmd = recv_ioctx->ioctx.buf;\r\ncmd = &send_ioctx->cmd;\r\nsend_ioctx->tag = srp_cmd->tag;\r\nswitch (srp_cmd->task_attr) {\r\ncase SRP_CMD_SIMPLE_Q:\r\ncmd->sam_task_attr = MSG_SIMPLE_TAG;\r\nbreak;\r\ncase SRP_CMD_ORDERED_Q:\r\ndefault:\r\ncmd->sam_task_attr = MSG_ORDERED_TAG;\r\nbreak;\r\ncase SRP_CMD_HEAD_OF_Q:\r\ncmd->sam_task_attr = MSG_HEAD_TAG;\r\nbreak;\r\ncase SRP_CMD_ACA:\r\ncmd->sam_task_attr = MSG_ACA_TAG;\r\nbreak;\r\n}\r\nif (srpt_get_desc_tbl(send_ioctx, srp_cmd, &dir, &data_len)) {\r\nprintk(KERN_ERR "0x%llx: parsing SRP descriptor table failed.\n",\r\nsrp_cmd->tag);\r\nret = TCM_INVALID_CDB_FIELD;\r\ngoto send_sense;\r\n}\r\nunpacked_lun = srpt_unpack_lun((uint8_t *)&srp_cmd->lun,\r\nsizeof(srp_cmd->lun));\r\nrc = target_submit_cmd(cmd, ch->sess, srp_cmd->cdb,\r\n&send_ioctx->sense_data[0], unpacked_lun, data_len,\r\nMSG_SIMPLE_TAG, dir, TARGET_SCF_ACK_KREF);\r\nif (rc != 0) {\r\nret = TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\ngoto send_sense;\r\n}\r\nreturn 0;\r\nsend_sense:\r\ntransport_send_check_condition_and_sense(cmd, ret, 0);\r\nreturn -1;\r\n}\r\nstatic int srpt_rx_mgmt_fn_tag(struct srpt_send_ioctx *ioctx, u64 tag)\r\n{\r\nstruct srpt_device *sdev;\r\nstruct srpt_rdma_ch *ch;\r\nstruct srpt_send_ioctx *target;\r\nint ret, i;\r\nret = -EINVAL;\r\nch = ioctx->ch;\r\nBUG_ON(!ch);\r\nBUG_ON(!ch->sport);\r\nsdev = ch->sport->sdev;\r\nBUG_ON(!sdev);\r\nspin_lock_irq(&sdev->spinlock);\r\nfor (i = 0; i < ch->rq_size; ++i) {\r\ntarget = ch->ioctx_ring[i];\r\nif (target->cmd.se_lun == ioctx->cmd.se_lun &&\r\ntarget->tag == tag &&\r\nsrpt_get_cmd_state(target) != SRPT_STATE_DONE) {\r\nret = 0;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irq(&sdev->spinlock);\r\nreturn ret;\r\n}\r\nstatic int srp_tmr_to_tcm(int fn)\r\n{\r\nswitch (fn) {\r\ncase SRP_TSK_ABORT_TASK:\r\nreturn TMR_ABORT_TASK;\r\ncase SRP_TSK_ABORT_TASK_SET:\r\nreturn TMR_ABORT_TASK_SET;\r\ncase SRP_TSK_CLEAR_TASK_SET:\r\nreturn TMR_CLEAR_TASK_SET;\r\ncase SRP_TSK_LUN_RESET:\r\nreturn TMR_LUN_RESET;\r\ncase SRP_TSK_CLEAR_ACA:\r\nreturn TMR_CLEAR_ACA;\r\ndefault:\r\nreturn -1;\r\n}\r\n}\r\nstatic void srpt_handle_tsk_mgmt(struct srpt_rdma_ch *ch,\r\nstruct srpt_recv_ioctx *recv_ioctx,\r\nstruct srpt_send_ioctx *send_ioctx)\r\n{\r\nstruct srp_tsk_mgmt *srp_tsk;\r\nstruct se_cmd *cmd;\r\nstruct se_session *sess = ch->sess;\r\nuint64_t unpacked_lun;\r\nuint32_t tag = 0;\r\nint tcm_tmr;\r\nint rc;\r\nBUG_ON(!send_ioctx);\r\nsrp_tsk = recv_ioctx->ioctx.buf;\r\ncmd = &send_ioctx->cmd;\r\npr_debug("recv tsk_mgmt fn %d for task_tag %lld and cmd tag %lld"\r\n" cm_id %p sess %p\n", srp_tsk->tsk_mgmt_func,\r\nsrp_tsk->task_tag, srp_tsk->tag, ch->cm_id, ch->sess);\r\nsrpt_set_cmd_state(send_ioctx, SRPT_STATE_MGMT);\r\nsend_ioctx->tag = srp_tsk->tag;\r\ntcm_tmr = srp_tmr_to_tcm(srp_tsk->tsk_mgmt_func);\r\nif (tcm_tmr < 0) {\r\nsend_ioctx->cmd.se_tmr_req->response =\r\nTMR_TASK_MGMT_FUNCTION_NOT_SUPPORTED;\r\ngoto fail;\r\n}\r\nunpacked_lun = srpt_unpack_lun((uint8_t *)&srp_tsk->lun,\r\nsizeof(srp_tsk->lun));\r\nif (srp_tsk->tsk_mgmt_func == SRP_TSK_ABORT_TASK) {\r\nrc = srpt_rx_mgmt_fn_tag(send_ioctx, srp_tsk->task_tag);\r\nif (rc < 0) {\r\nsend_ioctx->cmd.se_tmr_req->response =\r\nTMR_TASK_DOES_NOT_EXIST;\r\ngoto fail;\r\n}\r\ntag = srp_tsk->task_tag;\r\n}\r\nrc = target_submit_tmr(&send_ioctx->cmd, sess, NULL, unpacked_lun,\r\nsrp_tsk, tcm_tmr, GFP_KERNEL, tag,\r\nTARGET_SCF_ACK_KREF);\r\nif (rc != 0) {\r\nsend_ioctx->cmd.se_tmr_req->response = TMR_FUNCTION_REJECTED;\r\ngoto fail;\r\n}\r\nreturn;\r\nfail:\r\ntransport_send_check_condition_and_sense(cmd, 0, 0);\r\n}\r\nstatic void srpt_handle_new_iu(struct srpt_rdma_ch *ch,\r\nstruct srpt_recv_ioctx *recv_ioctx,\r\nstruct srpt_send_ioctx *send_ioctx)\r\n{\r\nstruct srp_cmd *srp_cmd;\r\nenum rdma_ch_state ch_state;\r\nBUG_ON(!ch);\r\nBUG_ON(!recv_ioctx);\r\nib_dma_sync_single_for_cpu(ch->sport->sdev->device,\r\nrecv_ioctx->ioctx.dma, srp_max_req_size,\r\nDMA_FROM_DEVICE);\r\nch_state = srpt_get_ch_state(ch);\r\nif (unlikely(ch_state == CH_CONNECTING)) {\r\nlist_add_tail(&recv_ioctx->wait_list, &ch->cmd_wait_list);\r\ngoto out;\r\n}\r\nif (unlikely(ch_state != CH_LIVE))\r\ngoto out;\r\nsrp_cmd = recv_ioctx->ioctx.buf;\r\nif (srp_cmd->opcode == SRP_CMD || srp_cmd->opcode == SRP_TSK_MGMT) {\r\nif (!send_ioctx)\r\nsend_ioctx = srpt_get_send_ioctx(ch);\r\nif (unlikely(!send_ioctx)) {\r\nlist_add_tail(&recv_ioctx->wait_list,\r\n&ch->cmd_wait_list);\r\ngoto out;\r\n}\r\n}\r\nswitch (srp_cmd->opcode) {\r\ncase SRP_CMD:\r\nsrpt_handle_cmd(ch, recv_ioctx, send_ioctx);\r\nbreak;\r\ncase SRP_TSK_MGMT:\r\nsrpt_handle_tsk_mgmt(ch, recv_ioctx, send_ioctx);\r\nbreak;\r\ncase SRP_I_LOGOUT:\r\nprintk(KERN_ERR "Not yet implemented: SRP_I_LOGOUT\n");\r\nbreak;\r\ncase SRP_CRED_RSP:\r\npr_debug("received SRP_CRED_RSP\n");\r\nbreak;\r\ncase SRP_AER_RSP:\r\npr_debug("received SRP_AER_RSP\n");\r\nbreak;\r\ncase SRP_RSP:\r\nprintk(KERN_ERR "Received SRP_RSP\n");\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "received IU with unknown opcode 0x%x\n",\r\nsrp_cmd->opcode);\r\nbreak;\r\n}\r\nsrpt_post_recv(ch->sport->sdev, recv_ioctx);\r\nout:\r\nreturn;\r\n}\r\nstatic void srpt_process_rcv_completion(struct ib_cq *cq,\r\nstruct srpt_rdma_ch *ch,\r\nstruct ib_wc *wc)\r\n{\r\nstruct srpt_device *sdev = ch->sport->sdev;\r\nstruct srpt_recv_ioctx *ioctx;\r\nu32 index;\r\nindex = idx_from_wr_id(wc->wr_id);\r\nif (wc->status == IB_WC_SUCCESS) {\r\nint req_lim;\r\nreq_lim = atomic_dec_return(&ch->req_lim);\r\nif (unlikely(req_lim < 0))\r\nprintk(KERN_ERR "req_lim = %d < 0\n", req_lim);\r\nioctx = sdev->ioctx_ring[index];\r\nsrpt_handle_new_iu(ch, ioctx, NULL);\r\n} else {\r\nprintk(KERN_INFO "receiving failed for idx %u with status %d\n",\r\nindex, wc->status);\r\n}\r\n}\r\nstatic void srpt_process_send_completion(struct ib_cq *cq,\r\nstruct srpt_rdma_ch *ch,\r\nstruct ib_wc *wc)\r\n{\r\nstruct srpt_send_ioctx *send_ioctx;\r\nuint32_t index;\r\nenum srpt_opcode opcode;\r\nindex = idx_from_wr_id(wc->wr_id);\r\nopcode = opcode_from_wr_id(wc->wr_id);\r\nsend_ioctx = ch->ioctx_ring[index];\r\nif (wc->status == IB_WC_SUCCESS) {\r\nif (opcode == SRPT_SEND)\r\nsrpt_handle_send_comp(ch, send_ioctx);\r\nelse {\r\nWARN_ON(opcode != SRPT_RDMA_ABORT &&\r\nwc->opcode != IB_WC_RDMA_READ);\r\nsrpt_handle_rdma_comp(ch, send_ioctx, opcode);\r\n}\r\n} else {\r\nif (opcode == SRPT_SEND) {\r\nprintk(KERN_INFO "sending response for idx %u failed"\r\n" with status %d\n", index, wc->status);\r\nsrpt_handle_send_err_comp(ch, wc->wr_id);\r\n} else if (opcode != SRPT_RDMA_MID) {\r\nprintk(KERN_INFO "RDMA t %d for idx %u failed with"\r\n" status %d", opcode, index, wc->status);\r\nsrpt_handle_rdma_err_comp(ch, send_ioctx, opcode);\r\n}\r\n}\r\nwhile (unlikely(opcode == SRPT_SEND\r\n&& !list_empty(&ch->cmd_wait_list)\r\n&& srpt_get_ch_state(ch) == CH_LIVE\r\n&& (send_ioctx = srpt_get_send_ioctx(ch)) != NULL)) {\r\nstruct srpt_recv_ioctx *recv_ioctx;\r\nrecv_ioctx = list_first_entry(&ch->cmd_wait_list,\r\nstruct srpt_recv_ioctx,\r\nwait_list);\r\nlist_del(&recv_ioctx->wait_list);\r\nsrpt_handle_new_iu(ch, recv_ioctx, send_ioctx);\r\n}\r\n}\r\nstatic void srpt_process_completion(struct ib_cq *cq, struct srpt_rdma_ch *ch)\r\n{\r\nstruct ib_wc *const wc = ch->wc;\r\nint i, n;\r\nWARN_ON(cq != ch->cq);\r\nib_req_notify_cq(cq, IB_CQ_NEXT_COMP);\r\nwhile ((n = ib_poll_cq(cq, ARRAY_SIZE(ch->wc), wc)) > 0) {\r\nfor (i = 0; i < n; i++) {\r\nif (opcode_from_wr_id(wc[i].wr_id) == SRPT_RECV)\r\nsrpt_process_rcv_completion(cq, ch, &wc[i]);\r\nelse\r\nsrpt_process_send_completion(cq, ch, &wc[i]);\r\n}\r\n}\r\n}\r\nstatic void srpt_completion(struct ib_cq *cq, void *ctx)\r\n{\r\nstruct srpt_rdma_ch *ch = ctx;\r\nwake_up_interruptible(&ch->wait_queue);\r\n}\r\nstatic int srpt_compl_thread(void *arg)\r\n{\r\nstruct srpt_rdma_ch *ch;\r\ncurrent->flags |= PF_NOFREEZE;\r\nch = arg;\r\nBUG_ON(!ch);\r\nprintk(KERN_INFO "Session %s: kernel thread %s (PID %d) started\n",\r\nch->sess_name, ch->thread->comm, current->pid);\r\nwhile (!kthread_should_stop()) {\r\nwait_event_interruptible(ch->wait_queue,\r\n(srpt_process_completion(ch->cq, ch),\r\nkthread_should_stop()));\r\n}\r\nprintk(KERN_INFO "Session %s: kernel thread %s (PID %d) stopped\n",\r\nch->sess_name, ch->thread->comm, current->pid);\r\nreturn 0;\r\n}\r\nstatic int srpt_create_ch_ib(struct srpt_rdma_ch *ch)\r\n{\r\nstruct ib_qp_init_attr *qp_init;\r\nstruct srpt_port *sport = ch->sport;\r\nstruct srpt_device *sdev = sport->sdev;\r\nu32 srp_sq_size = sport->port_attrib.srp_sq_size;\r\nint ret;\r\nWARN_ON(ch->rq_size < 1);\r\nret = -ENOMEM;\r\nqp_init = kzalloc(sizeof *qp_init, GFP_KERNEL);\r\nif (!qp_init)\r\ngoto out;\r\nch->cq = ib_create_cq(sdev->device, srpt_completion, NULL, ch,\r\nch->rq_size + srp_sq_size, 0);\r\nif (IS_ERR(ch->cq)) {\r\nret = PTR_ERR(ch->cq);\r\nprintk(KERN_ERR "failed to create CQ cqe= %d ret= %d\n",\r\nch->rq_size + srp_sq_size, ret);\r\ngoto out;\r\n}\r\nqp_init->qp_context = (void *)ch;\r\nqp_init->event_handler\r\n= (void(*)(struct ib_event *, void*))srpt_qp_event;\r\nqp_init->send_cq = ch->cq;\r\nqp_init->recv_cq = ch->cq;\r\nqp_init->srq = sdev->srq;\r\nqp_init->sq_sig_type = IB_SIGNAL_REQ_WR;\r\nqp_init->qp_type = IB_QPT_RC;\r\nqp_init->cap.max_send_wr = srp_sq_size;\r\nqp_init->cap.max_send_sge = SRPT_DEF_SG_PER_WQE;\r\nch->qp = ib_create_qp(sdev->pd, qp_init);\r\nif (IS_ERR(ch->qp)) {\r\nret = PTR_ERR(ch->qp);\r\nprintk(KERN_ERR "failed to create_qp ret= %d\n", ret);\r\ngoto err_destroy_cq;\r\n}\r\natomic_set(&ch->sq_wr_avail, qp_init->cap.max_send_wr);\r\npr_debug("%s: max_cqe= %d max_sge= %d sq_size = %d cm_id= %p\n",\r\n__func__, ch->cq->cqe, qp_init->cap.max_send_sge,\r\nqp_init->cap.max_send_wr, ch->cm_id);\r\nret = srpt_init_ch_qp(ch, ch->qp);\r\nif (ret)\r\ngoto err_destroy_qp;\r\ninit_waitqueue_head(&ch->wait_queue);\r\npr_debug("creating thread for session %s\n", ch->sess_name);\r\nch->thread = kthread_run(srpt_compl_thread, ch, "ib_srpt_compl");\r\nif (IS_ERR(ch->thread)) {\r\nprintk(KERN_ERR "failed to create kernel thread %ld\n",\r\nPTR_ERR(ch->thread));\r\nch->thread = NULL;\r\ngoto err_destroy_qp;\r\n}\r\nout:\r\nkfree(qp_init);\r\nreturn ret;\r\nerr_destroy_qp:\r\nib_destroy_qp(ch->qp);\r\nerr_destroy_cq:\r\nib_destroy_cq(ch->cq);\r\ngoto out;\r\n}\r\nstatic void srpt_destroy_ch_ib(struct srpt_rdma_ch *ch)\r\n{\r\nif (ch->thread)\r\nkthread_stop(ch->thread);\r\nib_destroy_qp(ch->qp);\r\nib_destroy_cq(ch->cq);\r\n}\r\nstatic void __srpt_close_ch(struct srpt_rdma_ch *ch)\r\n{\r\nstruct srpt_device *sdev;\r\nenum rdma_ch_state prev_state;\r\nunsigned long flags;\r\nsdev = ch->sport->sdev;\r\nspin_lock_irqsave(&ch->spinlock, flags);\r\nprev_state = ch->state;\r\nswitch (prev_state) {\r\ncase CH_CONNECTING:\r\ncase CH_LIVE:\r\nch->state = CH_DISCONNECTING;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&ch->spinlock, flags);\r\nswitch (prev_state) {\r\ncase CH_CONNECTING:\r\nib_send_cm_rej(ch->cm_id, IB_CM_REJ_NO_RESOURCES, NULL, 0,\r\nNULL, 0);\r\ncase CH_LIVE:\r\nif (ib_send_cm_dreq(ch->cm_id, NULL, 0) < 0)\r\nprintk(KERN_ERR "sending CM DREQ failed.\n");\r\nbreak;\r\ncase CH_DISCONNECTING:\r\nbreak;\r\ncase CH_DRAINING:\r\ncase CH_RELEASING:\r\nbreak;\r\n}\r\n}\r\nstatic void srpt_close_ch(struct srpt_rdma_ch *ch)\r\n{\r\nstruct srpt_device *sdev;\r\nsdev = ch->sport->sdev;\r\nspin_lock_irq(&sdev->spinlock);\r\n__srpt_close_ch(ch);\r\nspin_unlock_irq(&sdev->spinlock);\r\n}\r\nstatic int srpt_shutdown_session(struct se_session *se_sess)\r\n{\r\nstruct srpt_rdma_ch *ch = se_sess->fabric_sess_ptr;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ch->spinlock, flags);\r\nif (ch->in_shutdown) {\r\nspin_unlock_irqrestore(&ch->spinlock, flags);\r\nreturn true;\r\n}\r\nch->in_shutdown = true;\r\ntarget_sess_cmd_list_set_waiting(se_sess);\r\nspin_unlock_irqrestore(&ch->spinlock, flags);\r\nreturn true;\r\n}\r\nstatic void srpt_drain_channel(struct ib_cm_id *cm_id)\r\n{\r\nstruct srpt_device *sdev;\r\nstruct srpt_rdma_ch *ch;\r\nint ret;\r\nbool do_reset = false;\r\nWARN_ON_ONCE(irqs_disabled());\r\nsdev = cm_id->context;\r\nBUG_ON(!sdev);\r\nspin_lock_irq(&sdev->spinlock);\r\nlist_for_each_entry(ch, &sdev->rch_list, list) {\r\nif (ch->cm_id == cm_id) {\r\ndo_reset = srpt_test_and_set_ch_state(ch,\r\nCH_CONNECTING, CH_DRAINING) ||\r\nsrpt_test_and_set_ch_state(ch,\r\nCH_LIVE, CH_DRAINING) ||\r\nsrpt_test_and_set_ch_state(ch,\r\nCH_DISCONNECTING, CH_DRAINING);\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irq(&sdev->spinlock);\r\nif (do_reset) {\r\nif (ch->sess)\r\nsrpt_shutdown_session(ch->sess);\r\nret = srpt_ch_qp_err(ch);\r\nif (ret < 0)\r\nprintk(KERN_ERR "Setting queue pair in error state"\r\n" failed: %d\n", ret);\r\n}\r\n}\r\nstatic struct srpt_rdma_ch *srpt_find_channel(struct srpt_device *sdev,\r\nstruct ib_cm_id *cm_id)\r\n{\r\nstruct srpt_rdma_ch *ch;\r\nbool found;\r\nWARN_ON_ONCE(irqs_disabled());\r\nBUG_ON(!sdev);\r\nfound = false;\r\nspin_lock_irq(&sdev->spinlock);\r\nlist_for_each_entry(ch, &sdev->rch_list, list) {\r\nif (ch->cm_id == cm_id) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irq(&sdev->spinlock);\r\nreturn found ? ch : NULL;\r\n}\r\nstatic void srpt_release_channel(struct srpt_rdma_ch *ch)\r\n{\r\nschedule_work(&ch->release_work);\r\n}\r\nstatic void srpt_release_channel_work(struct work_struct *w)\r\n{\r\nstruct srpt_rdma_ch *ch;\r\nstruct srpt_device *sdev;\r\nstruct se_session *se_sess;\r\nch = container_of(w, struct srpt_rdma_ch, release_work);\r\npr_debug("ch = %p; ch->sess = %p; release_done = %p\n", ch, ch->sess,\r\nch->release_done);\r\nsdev = ch->sport->sdev;\r\nBUG_ON(!sdev);\r\nse_sess = ch->sess;\r\nBUG_ON(!se_sess);\r\ntarget_wait_for_sess_cmds(se_sess);\r\ntransport_deregister_session_configfs(se_sess);\r\ntransport_deregister_session(se_sess);\r\nch->sess = NULL;\r\nib_destroy_cm_id(ch->cm_id);\r\nsrpt_destroy_ch_ib(ch);\r\nsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_ring,\r\nch->sport->sdev, ch->rq_size,\r\nch->rsp_size, DMA_TO_DEVICE);\r\nspin_lock_irq(&sdev->spinlock);\r\nlist_del(&ch->list);\r\nspin_unlock_irq(&sdev->spinlock);\r\nif (ch->release_done)\r\ncomplete(ch->release_done);\r\nwake_up(&sdev->ch_releaseQ);\r\nkfree(ch);\r\n}\r\nstatic struct srpt_node_acl *__srpt_lookup_acl(struct srpt_port *sport,\r\nu8 i_port_id[16])\r\n{\r\nstruct srpt_node_acl *nacl;\r\nlist_for_each_entry(nacl, &sport->port_acl_list, list)\r\nif (memcmp(nacl->i_port_id, i_port_id,\r\nsizeof(nacl->i_port_id)) == 0)\r\nreturn nacl;\r\nreturn NULL;\r\n}\r\nstatic struct srpt_node_acl *srpt_lookup_acl(struct srpt_port *sport,\r\nu8 i_port_id[16])\r\n{\r\nstruct srpt_node_acl *nacl;\r\nspin_lock_irq(&sport->port_acl_lock);\r\nnacl = __srpt_lookup_acl(sport, i_port_id);\r\nspin_unlock_irq(&sport->port_acl_lock);\r\nreturn nacl;\r\n}\r\nstatic int srpt_cm_req_recv(struct ib_cm_id *cm_id,\r\nstruct ib_cm_req_event_param *param,\r\nvoid *private_data)\r\n{\r\nstruct srpt_device *sdev = cm_id->context;\r\nstruct srpt_port *sport = &sdev->port[param->port - 1];\r\nstruct srp_login_req *req;\r\nstruct srp_login_rsp *rsp;\r\nstruct srp_login_rej *rej;\r\nstruct ib_cm_rep_param *rep_param;\r\nstruct srpt_rdma_ch *ch, *tmp_ch;\r\nstruct srpt_node_acl *nacl;\r\nu32 it_iu_len;\r\nint i;\r\nint ret = 0;\r\nWARN_ON_ONCE(irqs_disabled());\r\nif (WARN_ON(!sdev || !private_data))\r\nreturn -EINVAL;\r\nreq = (struct srp_login_req *)private_data;\r\nit_iu_len = be32_to_cpu(req->req_it_iu_len);\r\nprintk(KERN_INFO "Received SRP_LOGIN_REQ with i_port_id 0x%llx:0x%llx,"\r\n" t_port_id 0x%llx:0x%llx and it_iu_len %d on port %d"\r\n" (guid=0x%llx:0x%llx)\n",\r\nbe64_to_cpu(*(__be64 *)&req->initiator_port_id[0]),\r\nbe64_to_cpu(*(__be64 *)&req->initiator_port_id[8]),\r\nbe64_to_cpu(*(__be64 *)&req->target_port_id[0]),\r\nbe64_to_cpu(*(__be64 *)&req->target_port_id[8]),\r\nit_iu_len,\r\nparam->port,\r\nbe64_to_cpu(*(__be64 *)&sdev->port[param->port - 1].gid.raw[0]),\r\nbe64_to_cpu(*(__be64 *)&sdev->port[param->port - 1].gid.raw[8]));\r\nrsp = kzalloc(sizeof *rsp, GFP_KERNEL);\r\nrej = kzalloc(sizeof *rej, GFP_KERNEL);\r\nrep_param = kzalloc(sizeof *rep_param, GFP_KERNEL);\r\nif (!rsp || !rej || !rep_param) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (it_iu_len > srp_max_req_size || it_iu_len < 64) {\r\nrej->reason = __constant_cpu_to_be32(\r\nSRP_LOGIN_REJ_REQ_IT_IU_LENGTH_TOO_LARGE);\r\nret = -EINVAL;\r\nprintk(KERN_ERR "rejected SRP_LOGIN_REQ because its"\r\n" length (%d bytes) is out of range (%d .. %d)\n",\r\nit_iu_len, 64, srp_max_req_size);\r\ngoto reject;\r\n}\r\nif (!sport->enabled) {\r\nrej->reason = __constant_cpu_to_be32(\r\nSRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\r\nret = -EINVAL;\r\nprintk(KERN_ERR "rejected SRP_LOGIN_REQ because the target port"\r\n" has not yet been enabled\n");\r\ngoto reject;\r\n}\r\nif ((req->req_flags & SRP_MTCH_ACTION) == SRP_MULTICHAN_SINGLE) {\r\nrsp->rsp_flags = SRP_LOGIN_RSP_MULTICHAN_NO_CHAN;\r\nspin_lock_irq(&sdev->spinlock);\r\nlist_for_each_entry_safe(ch, tmp_ch, &sdev->rch_list, list) {\r\nif (!memcmp(ch->i_port_id, req->initiator_port_id, 16)\r\n&& !memcmp(ch->t_port_id, req->target_port_id, 16)\r\n&& param->port == ch->sport->port\r\n&& param->listen_id == ch->sport->sdev->cm_id\r\n&& ch->cm_id) {\r\nenum rdma_ch_state ch_state;\r\nch_state = srpt_get_ch_state(ch);\r\nif (ch_state != CH_CONNECTING\r\n&& ch_state != CH_LIVE)\r\ncontinue;\r\npr_debug("Found existing channel %s"\r\n" cm_id= %p state= %d\n",\r\nch->sess_name, ch->cm_id, ch_state);\r\n__srpt_close_ch(ch);\r\nrsp->rsp_flags =\r\nSRP_LOGIN_RSP_MULTICHAN_TERMINATED;\r\n}\r\n}\r\nspin_unlock_irq(&sdev->spinlock);\r\n} else\r\nrsp->rsp_flags = SRP_LOGIN_RSP_MULTICHAN_MAINTAINED;\r\nif (*(__be64 *)req->target_port_id != cpu_to_be64(srpt_service_guid)\r\n|| *(__be64 *)(req->target_port_id + 8) !=\r\ncpu_to_be64(srpt_service_guid)) {\r\nrej->reason = __constant_cpu_to_be32(\r\nSRP_LOGIN_REJ_UNABLE_ASSOCIATE_CHANNEL);\r\nret = -ENOMEM;\r\nprintk(KERN_ERR "rejected SRP_LOGIN_REQ because it"\r\n" has an invalid target port identifier.\n");\r\ngoto reject;\r\n}\r\nch = kzalloc(sizeof *ch, GFP_KERNEL);\r\nif (!ch) {\r\nrej->reason = __constant_cpu_to_be32(\r\nSRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\r\nprintk(KERN_ERR "rejected SRP_LOGIN_REQ because no memory.\n");\r\nret = -ENOMEM;\r\ngoto reject;\r\n}\r\nINIT_WORK(&ch->release_work, srpt_release_channel_work);\r\nmemcpy(ch->i_port_id, req->initiator_port_id, 16);\r\nmemcpy(ch->t_port_id, req->target_port_id, 16);\r\nch->sport = &sdev->port[param->port - 1];\r\nch->cm_id = cm_id;\r\nch->rq_size = SRPT_RQ_SIZE;\r\nspin_lock_init(&ch->spinlock);\r\nch->state = CH_CONNECTING;\r\nINIT_LIST_HEAD(&ch->cmd_wait_list);\r\nch->rsp_size = ch->sport->port_attrib.srp_max_rsp_size;\r\nch->ioctx_ring = (struct srpt_send_ioctx **)\r\nsrpt_alloc_ioctx_ring(ch->sport->sdev, ch->rq_size,\r\nsizeof(*ch->ioctx_ring[0]),\r\nch->rsp_size, DMA_TO_DEVICE);\r\nif (!ch->ioctx_ring)\r\ngoto free_ch;\r\nINIT_LIST_HEAD(&ch->free_list);\r\nfor (i = 0; i < ch->rq_size; i++) {\r\nch->ioctx_ring[i]->ch = ch;\r\nlist_add_tail(&ch->ioctx_ring[i]->free_list, &ch->free_list);\r\n}\r\nret = srpt_create_ch_ib(ch);\r\nif (ret) {\r\nrej->reason = __constant_cpu_to_be32(\r\nSRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\r\nprintk(KERN_ERR "rejected SRP_LOGIN_REQ because creating"\r\n" a new RDMA channel failed.\n");\r\ngoto free_ring;\r\n}\r\nret = srpt_ch_qp_rtr(ch, ch->qp);\r\nif (ret) {\r\nrej->reason = __constant_cpu_to_be32(\r\nSRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\r\nprintk(KERN_ERR "rejected SRP_LOGIN_REQ because enabling"\r\n" RTR failed (error code = %d)\n", ret);\r\ngoto destroy_ib;\r\n}\r\nsnprintf(ch->sess_name, sizeof(ch->sess_name), "0x%016llx%016llx",\r\nbe64_to_cpu(*(__be64 *)ch->i_port_id),\r\nbe64_to_cpu(*(__be64 *)(ch->i_port_id + 8)));\r\npr_debug("registering session %s\n", ch->sess_name);\r\nnacl = srpt_lookup_acl(sport, ch->i_port_id);\r\nif (!nacl) {\r\nprintk(KERN_INFO "Rejected login because no ACL has been"\r\n" configured yet for initiator %s.\n", ch->sess_name);\r\nrej->reason = __constant_cpu_to_be32(\r\nSRP_LOGIN_REJ_CHANNEL_LIMIT_REACHED);\r\ngoto destroy_ib;\r\n}\r\nch->sess = transport_init_session(TARGET_PROT_NORMAL);\r\nif (IS_ERR(ch->sess)) {\r\nrej->reason = __constant_cpu_to_be32(\r\nSRP_LOGIN_REJ_INSUFFICIENT_RESOURCES);\r\npr_debug("Failed to create session\n");\r\ngoto deregister_session;\r\n}\r\nch->sess->se_node_acl = &nacl->nacl;\r\ntransport_register_session(&sport->port_tpg_1, &nacl->nacl, ch->sess, ch);\r\npr_debug("Establish connection sess=%p name=%s cm_id=%p\n", ch->sess,\r\nch->sess_name, ch->cm_id);\r\nrsp->opcode = SRP_LOGIN_RSP;\r\nrsp->tag = req->tag;\r\nrsp->max_it_iu_len = req->req_it_iu_len;\r\nrsp->max_ti_iu_len = req->req_it_iu_len;\r\nch->max_ti_iu_len = it_iu_len;\r\nrsp->buf_fmt = __constant_cpu_to_be16(SRP_BUF_FORMAT_DIRECT\r\n| SRP_BUF_FORMAT_INDIRECT);\r\nrsp->req_lim_delta = cpu_to_be32(ch->rq_size);\r\natomic_set(&ch->req_lim, ch->rq_size);\r\natomic_set(&ch->req_lim_delta, 0);\r\nrep_param->qp_num = ch->qp->qp_num;\r\nrep_param->private_data = (void *)rsp;\r\nrep_param->private_data_len = sizeof *rsp;\r\nrep_param->rnr_retry_count = 7;\r\nrep_param->flow_control = 1;\r\nrep_param->failover_accepted = 0;\r\nrep_param->srq = 1;\r\nrep_param->responder_resources = 4;\r\nrep_param->initiator_depth = 4;\r\nret = ib_send_cm_rep(cm_id, rep_param);\r\nif (ret) {\r\nprintk(KERN_ERR "sending SRP_LOGIN_REQ response failed"\r\n" (error code = %d)\n", ret);\r\ngoto release_channel;\r\n}\r\nspin_lock_irq(&sdev->spinlock);\r\nlist_add_tail(&ch->list, &sdev->rch_list);\r\nspin_unlock_irq(&sdev->spinlock);\r\ngoto out;\r\nrelease_channel:\r\nsrpt_set_ch_state(ch, CH_RELEASING);\r\ntransport_deregister_session_configfs(ch->sess);\r\nderegister_session:\r\ntransport_deregister_session(ch->sess);\r\nch->sess = NULL;\r\ndestroy_ib:\r\nsrpt_destroy_ch_ib(ch);\r\nfree_ring:\r\nsrpt_free_ioctx_ring((struct srpt_ioctx **)ch->ioctx_ring,\r\nch->sport->sdev, ch->rq_size,\r\nch->rsp_size, DMA_TO_DEVICE);\r\nfree_ch:\r\nkfree(ch);\r\nreject:\r\nrej->opcode = SRP_LOGIN_REJ;\r\nrej->tag = req->tag;\r\nrej->buf_fmt = __constant_cpu_to_be16(SRP_BUF_FORMAT_DIRECT\r\n| SRP_BUF_FORMAT_INDIRECT);\r\nib_send_cm_rej(cm_id, IB_CM_REJ_CONSUMER_DEFINED, NULL, 0,\r\n(void *)rej, sizeof *rej);\r\nout:\r\nkfree(rep_param);\r\nkfree(rsp);\r\nkfree(rej);\r\nreturn ret;\r\n}\r\nstatic void srpt_cm_rej_recv(struct ib_cm_id *cm_id)\r\n{\r\nprintk(KERN_INFO "Received IB REJ for cm_id %p.\n", cm_id);\r\nsrpt_drain_channel(cm_id);\r\n}\r\nstatic void srpt_cm_rtu_recv(struct ib_cm_id *cm_id)\r\n{\r\nstruct srpt_rdma_ch *ch;\r\nint ret;\r\nch = srpt_find_channel(cm_id->context, cm_id);\r\nBUG_ON(!ch);\r\nif (srpt_test_and_set_ch_state(ch, CH_CONNECTING, CH_LIVE)) {\r\nstruct srpt_recv_ioctx *ioctx, *ioctx_tmp;\r\nret = srpt_ch_qp_rts(ch, ch->qp);\r\nlist_for_each_entry_safe(ioctx, ioctx_tmp, &ch->cmd_wait_list,\r\nwait_list) {\r\nlist_del(&ioctx->wait_list);\r\nsrpt_handle_new_iu(ch, ioctx, NULL);\r\n}\r\nif (ret)\r\nsrpt_close_ch(ch);\r\n}\r\n}\r\nstatic void srpt_cm_timewait_exit(struct ib_cm_id *cm_id)\r\n{\r\nprintk(KERN_INFO "Received IB TimeWait exit for cm_id %p.\n", cm_id);\r\nsrpt_drain_channel(cm_id);\r\n}\r\nstatic void srpt_cm_rep_error(struct ib_cm_id *cm_id)\r\n{\r\nprintk(KERN_INFO "Received IB REP error for cm_id %p.\n", cm_id);\r\nsrpt_drain_channel(cm_id);\r\n}\r\nstatic void srpt_cm_dreq_recv(struct ib_cm_id *cm_id)\r\n{\r\nstruct srpt_rdma_ch *ch;\r\nunsigned long flags;\r\nbool send_drep = false;\r\nch = srpt_find_channel(cm_id->context, cm_id);\r\nBUG_ON(!ch);\r\npr_debug("cm_id= %p ch->state= %d\n", cm_id, srpt_get_ch_state(ch));\r\nspin_lock_irqsave(&ch->spinlock, flags);\r\nswitch (ch->state) {\r\ncase CH_CONNECTING:\r\ncase CH_LIVE:\r\nsend_drep = true;\r\nch->state = CH_DISCONNECTING;\r\nbreak;\r\ncase CH_DISCONNECTING:\r\ncase CH_DRAINING:\r\ncase CH_RELEASING:\r\nWARN(true, "unexpected channel state %d\n", ch->state);\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&ch->spinlock, flags);\r\nif (send_drep) {\r\nif (ib_send_cm_drep(ch->cm_id, NULL, 0) < 0)\r\nprintk(KERN_ERR "Sending IB DREP failed.\n");\r\nprintk(KERN_INFO "Received DREQ and sent DREP for session %s.\n",\r\nch->sess_name);\r\n}\r\n}\r\nstatic void srpt_cm_drep_recv(struct ib_cm_id *cm_id)\r\n{\r\nprintk(KERN_INFO "Received InfiniBand DREP message for cm_id %p.\n",\r\ncm_id);\r\nsrpt_drain_channel(cm_id);\r\n}\r\nstatic int srpt_cm_handler(struct ib_cm_id *cm_id, struct ib_cm_event *event)\r\n{\r\nint ret;\r\nret = 0;\r\nswitch (event->event) {\r\ncase IB_CM_REQ_RECEIVED:\r\nret = srpt_cm_req_recv(cm_id, &event->param.req_rcvd,\r\nevent->private_data);\r\nbreak;\r\ncase IB_CM_REJ_RECEIVED:\r\nsrpt_cm_rej_recv(cm_id);\r\nbreak;\r\ncase IB_CM_RTU_RECEIVED:\r\ncase IB_CM_USER_ESTABLISHED:\r\nsrpt_cm_rtu_recv(cm_id);\r\nbreak;\r\ncase IB_CM_DREQ_RECEIVED:\r\nsrpt_cm_dreq_recv(cm_id);\r\nbreak;\r\ncase IB_CM_DREP_RECEIVED:\r\nsrpt_cm_drep_recv(cm_id);\r\nbreak;\r\ncase IB_CM_TIMEWAIT_EXIT:\r\nsrpt_cm_timewait_exit(cm_id);\r\nbreak;\r\ncase IB_CM_REP_ERROR:\r\nsrpt_cm_rep_error(cm_id);\r\nbreak;\r\ncase IB_CM_DREQ_ERROR:\r\nprintk(KERN_INFO "Received IB DREQ ERROR event.\n");\r\nbreak;\r\ncase IB_CM_MRA_RECEIVED:\r\nprintk(KERN_INFO "Received IB MRA event\n");\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "received unrecognized IB CM event %d\n",\r\nevent->event);\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int srpt_perform_rdmas(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx)\r\n{\r\nstruct ib_send_wr wr;\r\nstruct ib_send_wr *bad_wr;\r\nstruct rdma_iu *riu;\r\nint i;\r\nint ret;\r\nint sq_wr_avail;\r\nenum dma_data_direction dir;\r\nconst int n_rdma = ioctx->n_rdma;\r\ndir = ioctx->cmd.data_direction;\r\nif (dir == DMA_TO_DEVICE) {\r\nret = -ENOMEM;\r\nsq_wr_avail = atomic_sub_return(n_rdma, &ch->sq_wr_avail);\r\nif (sq_wr_avail < 0) {\r\nprintk(KERN_WARNING "IB send queue full (needed %d)\n",\r\nn_rdma);\r\ngoto out;\r\n}\r\n}\r\nioctx->rdma_aborted = false;\r\nret = 0;\r\nriu = ioctx->rdma_ius;\r\nmemset(&wr, 0, sizeof wr);\r\nfor (i = 0; i < n_rdma; ++i, ++riu) {\r\nif (dir == DMA_FROM_DEVICE) {\r\nwr.opcode = IB_WR_RDMA_WRITE;\r\nwr.wr_id = encode_wr_id(i == n_rdma - 1 ?\r\nSRPT_RDMA_WRITE_LAST :\r\nSRPT_RDMA_MID,\r\nioctx->ioctx.index);\r\n} else {\r\nwr.opcode = IB_WR_RDMA_READ;\r\nwr.wr_id = encode_wr_id(i == n_rdma - 1 ?\r\nSRPT_RDMA_READ_LAST :\r\nSRPT_RDMA_MID,\r\nioctx->ioctx.index);\r\n}\r\nwr.next = NULL;\r\nwr.wr.rdma.remote_addr = riu->raddr;\r\nwr.wr.rdma.rkey = riu->rkey;\r\nwr.num_sge = riu->sge_cnt;\r\nwr.sg_list = riu->sge;\r\nif (i == (n_rdma - 1) && dir == DMA_TO_DEVICE)\r\nwr.send_flags = IB_SEND_SIGNALED;\r\nret = ib_post_send(ch->qp, &wr, &bad_wr);\r\nif (ret)\r\nbreak;\r\n}\r\nif (ret)\r\nprintk(KERN_ERR "%s[%d]: ib_post_send() returned %d for %d/%d",\r\n__func__, __LINE__, ret, i, n_rdma);\r\nif (ret && i > 0) {\r\nwr.num_sge = 0;\r\nwr.wr_id = encode_wr_id(SRPT_RDMA_ABORT, ioctx->ioctx.index);\r\nwr.send_flags = IB_SEND_SIGNALED;\r\nwhile (ch->state == CH_LIVE &&\r\nib_post_send(ch->qp, &wr, &bad_wr) != 0) {\r\nprintk(KERN_INFO "Trying to abort failed RDMA transfer [%d]",\r\nioctx->ioctx.index);\r\nmsleep(1000);\r\n}\r\nwhile (ch->state != CH_RELEASING && !ioctx->rdma_aborted) {\r\nprintk(KERN_INFO "Waiting until RDMA abort finished [%d]",\r\nioctx->ioctx.index);\r\nmsleep(1000);\r\n}\r\n}\r\nout:\r\nif (unlikely(dir == DMA_TO_DEVICE && ret < 0))\r\natomic_add(n_rdma, &ch->sq_wr_avail);\r\nreturn ret;\r\n}\r\nstatic int srpt_xfer_data(struct srpt_rdma_ch *ch,\r\nstruct srpt_send_ioctx *ioctx)\r\n{\r\nint ret;\r\nret = srpt_map_sg_to_ib_sge(ch, ioctx);\r\nif (ret) {\r\nprintk(KERN_ERR "%s[%d] ret=%d\n", __func__, __LINE__, ret);\r\ngoto out;\r\n}\r\nret = srpt_perform_rdmas(ch, ioctx);\r\nif (ret) {\r\nif (ret == -EAGAIN || ret == -ENOMEM)\r\nprintk(KERN_INFO "%s[%d] queue full -- ret=%d\n",\r\n__func__, __LINE__, ret);\r\nelse\r\nprintk(KERN_ERR "%s[%d] fatal error -- ret=%d\n",\r\n__func__, __LINE__, ret);\r\ngoto out_unmap;\r\n}\r\nout:\r\nreturn ret;\r\nout_unmap:\r\nsrpt_unmap_sg_to_ib_sge(ch, ioctx);\r\ngoto out;\r\n}\r\nstatic int srpt_write_pending_status(struct se_cmd *se_cmd)\r\n{\r\nstruct srpt_send_ioctx *ioctx;\r\nioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\r\nreturn srpt_get_cmd_state(ioctx) == SRPT_STATE_NEED_DATA;\r\n}\r\nstatic int srpt_write_pending(struct se_cmd *se_cmd)\r\n{\r\nstruct srpt_rdma_ch *ch;\r\nstruct srpt_send_ioctx *ioctx;\r\nenum srpt_command_state new_state;\r\nenum rdma_ch_state ch_state;\r\nint ret;\r\nioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\r\nnew_state = srpt_set_cmd_state(ioctx, SRPT_STATE_NEED_DATA);\r\nWARN_ON(new_state == SRPT_STATE_DONE);\r\nch = ioctx->ch;\r\nBUG_ON(!ch);\r\nch_state = srpt_get_ch_state(ch);\r\nswitch (ch_state) {\r\ncase CH_CONNECTING:\r\nWARN(true, "unexpected channel state %d\n", ch_state);\r\nret = -EINVAL;\r\ngoto out;\r\ncase CH_LIVE:\r\nbreak;\r\ncase CH_DISCONNECTING:\r\ncase CH_DRAINING:\r\ncase CH_RELEASING:\r\npr_debug("cmd with tag %lld: channel disconnecting\n",\r\nioctx->tag);\r\nsrpt_set_cmd_state(ioctx, SRPT_STATE_DATA_IN);\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nret = srpt_xfer_data(ch, ioctx);\r\nout:\r\nreturn ret;\r\n}\r\nstatic u8 tcm_to_srp_tsk_mgmt_status(const int tcm_mgmt_status)\r\n{\r\nswitch (tcm_mgmt_status) {\r\ncase TMR_FUNCTION_COMPLETE:\r\nreturn SRP_TSK_MGMT_SUCCESS;\r\ncase TMR_FUNCTION_REJECTED:\r\nreturn SRP_TSK_MGMT_FUNC_NOT_SUPP;\r\n}\r\nreturn SRP_TSK_MGMT_FAILED;\r\n}\r\nstatic void srpt_queue_response(struct se_cmd *cmd)\r\n{\r\nstruct srpt_rdma_ch *ch;\r\nstruct srpt_send_ioctx *ioctx;\r\nenum srpt_command_state state;\r\nunsigned long flags;\r\nint ret;\r\nenum dma_data_direction dir;\r\nint resp_len;\r\nu8 srp_tm_status;\r\nioctx = container_of(cmd, struct srpt_send_ioctx, cmd);\r\nch = ioctx->ch;\r\nBUG_ON(!ch);\r\nspin_lock_irqsave(&ioctx->spinlock, flags);\r\nstate = ioctx->state;\r\nswitch (state) {\r\ncase SRPT_STATE_NEW:\r\ncase SRPT_STATE_DATA_IN:\r\nioctx->state = SRPT_STATE_CMD_RSP_SENT;\r\nbreak;\r\ncase SRPT_STATE_MGMT:\r\nioctx->state = SRPT_STATE_MGMT_RSP_SENT;\r\nbreak;\r\ndefault:\r\nWARN(true, "ch %p; cmd %d: unexpected command state %d\n",\r\nch, ioctx->ioctx.index, ioctx->state);\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&ioctx->spinlock, flags);\r\nif (unlikely(transport_check_aborted_status(&ioctx->cmd, false)\r\n|| WARN_ON_ONCE(state == SRPT_STATE_CMD_RSP_SENT))) {\r\natomic_inc(&ch->req_lim_delta);\r\nsrpt_abort_cmd(ioctx);\r\nreturn;\r\n}\r\ndir = ioctx->cmd.data_direction;\r\nif (dir == DMA_FROM_DEVICE && ioctx->cmd.data_length &&\r\n!ioctx->queue_status_only) {\r\nret = srpt_xfer_data(ch, ioctx);\r\nif (ret) {\r\nprintk(KERN_ERR "xfer_data failed for tag %llu\n",\r\nioctx->tag);\r\nreturn;\r\n}\r\n}\r\nif (state != SRPT_STATE_MGMT)\r\nresp_len = srpt_build_cmd_rsp(ch, ioctx, ioctx->tag,\r\ncmd->scsi_status);\r\nelse {\r\nsrp_tm_status\r\n= tcm_to_srp_tsk_mgmt_status(cmd->se_tmr_req->response);\r\nresp_len = srpt_build_tskmgmt_rsp(ch, ioctx, srp_tm_status,\r\nioctx->tag);\r\n}\r\nret = srpt_post_send(ch, ioctx, resp_len);\r\nif (ret) {\r\nprintk(KERN_ERR "sending cmd response failed for tag %llu\n",\r\nioctx->tag);\r\nsrpt_unmap_sg_to_ib_sge(ch, ioctx);\r\nsrpt_set_cmd_state(ioctx, SRPT_STATE_DONE);\r\ntarget_put_sess_cmd(ioctx->ch->sess, &ioctx->cmd);\r\n}\r\n}\r\nstatic int srpt_queue_data_in(struct se_cmd *cmd)\r\n{\r\nsrpt_queue_response(cmd);\r\nreturn 0;\r\n}\r\nstatic void srpt_queue_tm_rsp(struct se_cmd *cmd)\r\n{\r\nsrpt_queue_response(cmd);\r\n}\r\nstatic void srpt_aborted_task(struct se_cmd *cmd)\r\n{\r\nstruct srpt_send_ioctx *ioctx = container_of(cmd,\r\nstruct srpt_send_ioctx, cmd);\r\nsrpt_unmap_sg_to_ib_sge(ioctx->ch, ioctx);\r\n}\r\nstatic int srpt_queue_status(struct se_cmd *cmd)\r\n{\r\nstruct srpt_send_ioctx *ioctx;\r\nioctx = container_of(cmd, struct srpt_send_ioctx, cmd);\r\nBUG_ON(ioctx->sense_data != cmd->sense_buffer);\r\nif (cmd->se_cmd_flags &\r\n(SCF_TRANSPORT_TASK_SENSE | SCF_EMULATED_TASK_SENSE))\r\nWARN_ON(cmd->scsi_status != SAM_STAT_CHECK_CONDITION);\r\nioctx->queue_status_only = true;\r\nsrpt_queue_response(cmd);\r\nreturn 0;\r\n}\r\nstatic void srpt_refresh_port_work(struct work_struct *work)\r\n{\r\nstruct srpt_port *sport = container_of(work, struct srpt_port, work);\r\nsrpt_refresh_port(sport);\r\n}\r\nstatic int srpt_ch_list_empty(struct srpt_device *sdev)\r\n{\r\nint res;\r\nspin_lock_irq(&sdev->spinlock);\r\nres = list_empty(&sdev->rch_list);\r\nspin_unlock_irq(&sdev->spinlock);\r\nreturn res;\r\n}\r\nstatic int srpt_release_sdev(struct srpt_device *sdev)\r\n{\r\nstruct srpt_rdma_ch *ch, *tmp_ch;\r\nint res;\r\nWARN_ON_ONCE(irqs_disabled());\r\nBUG_ON(!sdev);\r\nspin_lock_irq(&sdev->spinlock);\r\nlist_for_each_entry_safe(ch, tmp_ch, &sdev->rch_list, list)\r\n__srpt_close_ch(ch);\r\nspin_unlock_irq(&sdev->spinlock);\r\nres = wait_event_interruptible(sdev->ch_releaseQ,\r\nsrpt_ch_list_empty(sdev));\r\nif (res)\r\nprintk(KERN_ERR "%s: interrupted.\n", __func__);\r\nreturn 0;\r\n}\r\nstatic struct srpt_port *__srpt_lookup_port(const char *name)\r\n{\r\nstruct ib_device *dev;\r\nstruct srpt_device *sdev;\r\nstruct srpt_port *sport;\r\nint i;\r\nlist_for_each_entry(sdev, &srpt_dev_list, list) {\r\ndev = sdev->device;\r\nif (!dev)\r\ncontinue;\r\nfor (i = 0; i < dev->phys_port_cnt; i++) {\r\nsport = &sdev->port[i];\r\nif (!strcmp(sport->port_guid, name))\r\nreturn sport;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct srpt_port *srpt_lookup_port(const char *name)\r\n{\r\nstruct srpt_port *sport;\r\nspin_lock(&srpt_dev_lock);\r\nsport = __srpt_lookup_port(name);\r\nspin_unlock(&srpt_dev_lock);\r\nreturn sport;\r\n}\r\nstatic void srpt_add_one(struct ib_device *device)\r\n{\r\nstruct srpt_device *sdev;\r\nstruct srpt_port *sport;\r\nstruct ib_srq_init_attr srq_attr;\r\nint i;\r\npr_debug("device = %p, device->dma_ops = %p\n", device,\r\ndevice->dma_ops);\r\nsdev = kzalloc(sizeof *sdev, GFP_KERNEL);\r\nif (!sdev)\r\ngoto err;\r\nsdev->device = device;\r\nINIT_LIST_HEAD(&sdev->rch_list);\r\ninit_waitqueue_head(&sdev->ch_releaseQ);\r\nspin_lock_init(&sdev->spinlock);\r\nif (ib_query_device(device, &sdev->dev_attr))\r\ngoto free_dev;\r\nsdev->pd = ib_alloc_pd(device);\r\nif (IS_ERR(sdev->pd))\r\ngoto free_dev;\r\nsdev->mr = ib_get_dma_mr(sdev->pd, IB_ACCESS_LOCAL_WRITE);\r\nif (IS_ERR(sdev->mr))\r\ngoto err_pd;\r\nsdev->srq_size = min(srpt_srq_size, sdev->dev_attr.max_srq_wr);\r\nsrq_attr.event_handler = srpt_srq_event;\r\nsrq_attr.srq_context = (void *)sdev;\r\nsrq_attr.attr.max_wr = sdev->srq_size;\r\nsrq_attr.attr.max_sge = 1;\r\nsrq_attr.attr.srq_limit = 0;\r\nsrq_attr.srq_type = IB_SRQT_BASIC;\r\nsdev->srq = ib_create_srq(sdev->pd, &srq_attr);\r\nif (IS_ERR(sdev->srq))\r\ngoto err_mr;\r\npr_debug("%s: create SRQ #wr= %d max_allow=%d dev= %s\n",\r\n__func__, sdev->srq_size, sdev->dev_attr.max_srq_wr,\r\ndevice->name);\r\nif (!srpt_service_guid)\r\nsrpt_service_guid = be64_to_cpu(device->node_guid);\r\nsdev->cm_id = ib_create_cm_id(device, srpt_cm_handler, sdev);\r\nif (IS_ERR(sdev->cm_id))\r\ngoto err_srq;\r\npr_debug("Target login info: id_ext=%016llx,ioc_guid=%016llx,"\r\n"pkey=ffff,service_id=%016llx\n", srpt_service_guid,\r\nsrpt_service_guid, srpt_service_guid);\r\nif (ib_cm_listen(sdev->cm_id, cpu_to_be64(srpt_service_guid), 0, NULL))\r\ngoto err_cm;\r\nINIT_IB_EVENT_HANDLER(&sdev->event_handler, sdev->device,\r\nsrpt_event_handler);\r\nif (ib_register_event_handler(&sdev->event_handler))\r\ngoto err_cm;\r\nsdev->ioctx_ring = (struct srpt_recv_ioctx **)\r\nsrpt_alloc_ioctx_ring(sdev, sdev->srq_size,\r\nsizeof(*sdev->ioctx_ring[0]),\r\nsrp_max_req_size, DMA_FROM_DEVICE);\r\nif (!sdev->ioctx_ring)\r\ngoto err_event;\r\nfor (i = 0; i < sdev->srq_size; ++i)\r\nsrpt_post_recv(sdev, sdev->ioctx_ring[i]);\r\nWARN_ON(sdev->device->phys_port_cnt > ARRAY_SIZE(sdev->port));\r\nfor (i = 1; i <= sdev->device->phys_port_cnt; i++) {\r\nsport = &sdev->port[i - 1];\r\nsport->sdev = sdev;\r\nsport->port = i;\r\nsport->port_attrib.srp_max_rdma_size = DEFAULT_MAX_RDMA_SIZE;\r\nsport->port_attrib.srp_max_rsp_size = DEFAULT_MAX_RSP_SIZE;\r\nsport->port_attrib.srp_sq_size = DEF_SRPT_SQ_SIZE;\r\nINIT_WORK(&sport->work, srpt_refresh_port_work);\r\nINIT_LIST_HEAD(&sport->port_acl_list);\r\nspin_lock_init(&sport->port_acl_lock);\r\nif (srpt_refresh_port(sport)) {\r\nprintk(KERN_ERR "MAD registration failed for %s-%d.\n",\r\nsrpt_sdev_name(sdev), i);\r\ngoto err_ring;\r\n}\r\nsnprintf(sport->port_guid, sizeof(sport->port_guid),\r\n"0x%016llx%016llx",\r\nbe64_to_cpu(sport->gid.global.subnet_prefix),\r\nbe64_to_cpu(sport->gid.global.interface_id));\r\n}\r\nspin_lock(&srpt_dev_lock);\r\nlist_add_tail(&sdev->list, &srpt_dev_list);\r\nspin_unlock(&srpt_dev_lock);\r\nout:\r\nib_set_client_data(device, &srpt_client, sdev);\r\npr_debug("added %s.\n", device->name);\r\nreturn;\r\nerr_ring:\r\nsrpt_free_ioctx_ring((struct srpt_ioctx **)sdev->ioctx_ring, sdev,\r\nsdev->srq_size, srp_max_req_size,\r\nDMA_FROM_DEVICE);\r\nerr_event:\r\nib_unregister_event_handler(&sdev->event_handler);\r\nerr_cm:\r\nib_destroy_cm_id(sdev->cm_id);\r\nerr_srq:\r\nib_destroy_srq(sdev->srq);\r\nerr_mr:\r\nib_dereg_mr(sdev->mr);\r\nerr_pd:\r\nib_dealloc_pd(sdev->pd);\r\nfree_dev:\r\nkfree(sdev);\r\nerr:\r\nsdev = NULL;\r\nprintk(KERN_INFO "%s(%s) failed.\n", __func__, device->name);\r\ngoto out;\r\n}\r\nstatic void srpt_remove_one(struct ib_device *device)\r\n{\r\nstruct srpt_device *sdev;\r\nint i;\r\nsdev = ib_get_client_data(device, &srpt_client);\r\nif (!sdev) {\r\nprintk(KERN_INFO "%s(%s): nothing to do.\n", __func__,\r\ndevice->name);\r\nreturn;\r\n}\r\nsrpt_unregister_mad_agent(sdev);\r\nib_unregister_event_handler(&sdev->event_handler);\r\nfor (i = 0; i < sdev->device->phys_port_cnt; i++)\r\ncancel_work_sync(&sdev->port[i].work);\r\nib_destroy_cm_id(sdev->cm_id);\r\nspin_lock(&srpt_dev_lock);\r\nlist_del(&sdev->list);\r\nspin_unlock(&srpt_dev_lock);\r\nsrpt_release_sdev(sdev);\r\nib_destroy_srq(sdev->srq);\r\nib_dereg_mr(sdev->mr);\r\nib_dealloc_pd(sdev->pd);\r\nsrpt_free_ioctx_ring((struct srpt_ioctx **)sdev->ioctx_ring, sdev,\r\nsdev->srq_size, srp_max_req_size, DMA_FROM_DEVICE);\r\nsdev->ioctx_ring = NULL;\r\nkfree(sdev);\r\n}\r\nstatic int srpt_check_true(struct se_portal_group *se_tpg)\r\n{\r\nreturn 1;\r\n}\r\nstatic int srpt_check_false(struct se_portal_group *se_tpg)\r\n{\r\nreturn 0;\r\n}\r\nstatic char *srpt_get_fabric_name(void)\r\n{\r\nreturn "srpt";\r\n}\r\nstatic u8 srpt_get_fabric_proto_ident(struct se_portal_group *se_tpg)\r\n{\r\nreturn SCSI_TRANSPORTID_PROTOCOLID_SRP;\r\n}\r\nstatic char *srpt_get_fabric_wwn(struct se_portal_group *tpg)\r\n{\r\nstruct srpt_port *sport = container_of(tpg, struct srpt_port, port_tpg_1);\r\nreturn sport->port_guid;\r\n}\r\nstatic u16 srpt_get_tag(struct se_portal_group *tpg)\r\n{\r\nreturn 1;\r\n}\r\nstatic u32 srpt_get_default_depth(struct se_portal_group *se_tpg)\r\n{\r\nreturn 1;\r\n}\r\nstatic u32 srpt_get_pr_transport_id(struct se_portal_group *se_tpg,\r\nstruct se_node_acl *se_nacl,\r\nstruct t10_pr_registration *pr_reg,\r\nint *format_code, unsigned char *buf)\r\n{\r\nstruct srpt_node_acl *nacl;\r\nstruct spc_rdma_transport_id *tr_id;\r\nnacl = container_of(se_nacl, struct srpt_node_acl, nacl);\r\ntr_id = (void *)buf;\r\ntr_id->protocol_identifier = SCSI_TRANSPORTID_PROTOCOLID_SRP;\r\nmemcpy(tr_id->i_port_id, nacl->i_port_id, sizeof(tr_id->i_port_id));\r\nreturn sizeof(*tr_id);\r\n}\r\nstatic u32 srpt_get_pr_transport_id_len(struct se_portal_group *se_tpg,\r\nstruct se_node_acl *se_nacl,\r\nstruct t10_pr_registration *pr_reg,\r\nint *format_code)\r\n{\r\n*format_code = 0;\r\nreturn sizeof(struct spc_rdma_transport_id);\r\n}\r\nstatic char *srpt_parse_pr_out_transport_id(struct se_portal_group *se_tpg,\r\nconst char *buf, u32 *out_tid_len,\r\nchar **port_nexus_ptr)\r\n{\r\nstruct spc_rdma_transport_id *tr_id;\r\n*port_nexus_ptr = NULL;\r\n*out_tid_len = sizeof(struct spc_rdma_transport_id);\r\ntr_id = (void *)buf;\r\nreturn (char *)tr_id->i_port_id;\r\n}\r\nstatic struct se_node_acl *srpt_alloc_fabric_acl(struct se_portal_group *se_tpg)\r\n{\r\nstruct srpt_node_acl *nacl;\r\nnacl = kzalloc(sizeof(struct srpt_node_acl), GFP_KERNEL);\r\nif (!nacl) {\r\nprintk(KERN_ERR "Unable to allocate struct srpt_node_acl\n");\r\nreturn NULL;\r\n}\r\nreturn &nacl->nacl;\r\n}\r\nstatic void srpt_release_fabric_acl(struct se_portal_group *se_tpg,\r\nstruct se_node_acl *se_nacl)\r\n{\r\nstruct srpt_node_acl *nacl;\r\nnacl = container_of(se_nacl, struct srpt_node_acl, nacl);\r\nkfree(nacl);\r\n}\r\nstatic u32 srpt_tpg_get_inst_index(struct se_portal_group *se_tpg)\r\n{\r\nreturn 1;\r\n}\r\nstatic void srpt_release_cmd(struct se_cmd *se_cmd)\r\n{\r\nstruct srpt_send_ioctx *ioctx = container_of(se_cmd,\r\nstruct srpt_send_ioctx, cmd);\r\nstruct srpt_rdma_ch *ch = ioctx->ch;\r\nunsigned long flags;\r\nWARN_ON(ioctx->state != SRPT_STATE_DONE);\r\nWARN_ON(ioctx->mapped_sg_count != 0);\r\nif (ioctx->n_rbuf > 1) {\r\nkfree(ioctx->rbufs);\r\nioctx->rbufs = NULL;\r\nioctx->n_rbuf = 0;\r\n}\r\nspin_lock_irqsave(&ch->spinlock, flags);\r\nlist_add(&ioctx->free_list, &ch->free_list);\r\nspin_unlock_irqrestore(&ch->spinlock, flags);\r\n}\r\nstatic void srpt_close_session(struct se_session *se_sess)\r\n{\r\nDECLARE_COMPLETION_ONSTACK(release_done);\r\nstruct srpt_rdma_ch *ch;\r\nstruct srpt_device *sdev;\r\nint res;\r\nch = se_sess->fabric_sess_ptr;\r\nWARN_ON(ch->sess != se_sess);\r\npr_debug("ch %p state %d\n", ch, srpt_get_ch_state(ch));\r\nsdev = ch->sport->sdev;\r\nspin_lock_irq(&sdev->spinlock);\r\nBUG_ON(ch->release_done);\r\nch->release_done = &release_done;\r\n__srpt_close_ch(ch);\r\nspin_unlock_irq(&sdev->spinlock);\r\nres = wait_for_completion_timeout(&release_done, 60 * HZ);\r\nWARN_ON(res <= 0);\r\n}\r\nstatic u32 srpt_sess_get_index(struct se_session *se_sess)\r\n{\r\nreturn 0;\r\n}\r\nstatic void srpt_set_default_node_attrs(struct se_node_acl *nacl)\r\n{\r\n}\r\nstatic u32 srpt_get_task_tag(struct se_cmd *se_cmd)\r\n{\r\nstruct srpt_send_ioctx *ioctx;\r\nioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\r\nreturn ioctx->tag;\r\n}\r\nstatic int srpt_get_tcm_cmd_state(struct se_cmd *se_cmd)\r\n{\r\nstruct srpt_send_ioctx *ioctx;\r\nioctx = container_of(se_cmd, struct srpt_send_ioctx, cmd);\r\nreturn srpt_get_cmd_state(ioctx);\r\n}\r\nstatic int srpt_parse_i_port_id(u8 i_port_id[16], const char *name)\r\n{\r\nconst char *p;\r\nunsigned len, count, leading_zero_bytes;\r\nint ret, rc;\r\np = name;\r\nif (strnicmp(p, "0x", 2) == 0)\r\np += 2;\r\nret = -EINVAL;\r\nlen = strlen(p);\r\nif (len % 2)\r\ngoto out;\r\ncount = min(len / 2, 16U);\r\nleading_zero_bytes = 16 - count;\r\nmemset(i_port_id, 0, leading_zero_bytes);\r\nrc = hex2bin(i_port_id + leading_zero_bytes, p, count);\r\nif (rc < 0)\r\npr_debug("hex2bin failed for srpt_parse_i_port_id: %d\n", rc);\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic struct se_node_acl *srpt_make_nodeacl(struct se_portal_group *tpg,\r\nstruct config_group *group,\r\nconst char *name)\r\n{\r\nstruct srpt_port *sport = container_of(tpg, struct srpt_port, port_tpg_1);\r\nstruct se_node_acl *se_nacl, *se_nacl_new;\r\nstruct srpt_node_acl *nacl;\r\nint ret = 0;\r\nu32 nexus_depth = 1;\r\nu8 i_port_id[16];\r\nif (srpt_parse_i_port_id(i_port_id, name) < 0) {\r\nprintk(KERN_ERR "invalid initiator port ID %s\n", name);\r\nret = -EINVAL;\r\ngoto err;\r\n}\r\nse_nacl_new = srpt_alloc_fabric_acl(tpg);\r\nif (!se_nacl_new) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nse_nacl = core_tpg_add_initiator_node_acl(tpg, se_nacl_new, name,\r\nnexus_depth);\r\nif (IS_ERR(se_nacl)) {\r\nret = PTR_ERR(se_nacl);\r\ngoto err;\r\n}\r\nnacl = container_of(se_nacl, struct srpt_node_acl, nacl);\r\nmemcpy(&nacl->i_port_id[0], &i_port_id[0], 16);\r\nnacl->sport = sport;\r\nspin_lock_irq(&sport->port_acl_lock);\r\nlist_add_tail(&nacl->list, &sport->port_acl_list);\r\nspin_unlock_irq(&sport->port_acl_lock);\r\nreturn se_nacl;\r\nerr:\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void srpt_drop_nodeacl(struct se_node_acl *se_nacl)\r\n{\r\nstruct srpt_node_acl *nacl;\r\nstruct srpt_device *sdev;\r\nstruct srpt_port *sport;\r\nnacl = container_of(se_nacl, struct srpt_node_acl, nacl);\r\nsport = nacl->sport;\r\nsdev = sport->sdev;\r\nspin_lock_irq(&sport->port_acl_lock);\r\nlist_del(&nacl->list);\r\nspin_unlock_irq(&sport->port_acl_lock);\r\ncore_tpg_del_initiator_node_acl(&sport->port_tpg_1, se_nacl, 1);\r\nsrpt_release_fabric_acl(NULL, se_nacl);\r\n}\r\nstatic ssize_t srpt_tpg_attrib_show_srp_max_rdma_size(\r\nstruct se_portal_group *se_tpg,\r\nchar *page)\r\n{\r\nstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\r\nreturn sprintf(page, "%u\n", sport->port_attrib.srp_max_rdma_size);\r\n}\r\nstatic ssize_t srpt_tpg_attrib_store_srp_max_rdma_size(\r\nstruct se_portal_group *se_tpg,\r\nconst char *page,\r\nsize_t count)\r\n{\r\nstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\r\nunsigned long val;\r\nint ret;\r\nret = kstrtoul(page, 0, &val);\r\nif (ret < 0) {\r\npr_err("kstrtoul() failed with ret: %d\n", ret);\r\nreturn -EINVAL;\r\n}\r\nif (val > MAX_SRPT_RDMA_SIZE) {\r\npr_err("val: %lu exceeds MAX_SRPT_RDMA_SIZE: %d\n", val,\r\nMAX_SRPT_RDMA_SIZE);\r\nreturn -EINVAL;\r\n}\r\nif (val < DEFAULT_MAX_RDMA_SIZE) {\r\npr_err("val: %lu smaller than DEFAULT_MAX_RDMA_SIZE: %d\n",\r\nval, DEFAULT_MAX_RDMA_SIZE);\r\nreturn -EINVAL;\r\n}\r\nsport->port_attrib.srp_max_rdma_size = val;\r\nreturn count;\r\n}\r\nstatic ssize_t srpt_tpg_attrib_show_srp_max_rsp_size(\r\nstruct se_portal_group *se_tpg,\r\nchar *page)\r\n{\r\nstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\r\nreturn sprintf(page, "%u\n", sport->port_attrib.srp_max_rsp_size);\r\n}\r\nstatic ssize_t srpt_tpg_attrib_store_srp_max_rsp_size(\r\nstruct se_portal_group *se_tpg,\r\nconst char *page,\r\nsize_t count)\r\n{\r\nstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\r\nunsigned long val;\r\nint ret;\r\nret = kstrtoul(page, 0, &val);\r\nif (ret < 0) {\r\npr_err("kstrtoul() failed with ret: %d\n", ret);\r\nreturn -EINVAL;\r\n}\r\nif (val > MAX_SRPT_RSP_SIZE) {\r\npr_err("val: %lu exceeds MAX_SRPT_RSP_SIZE: %d\n", val,\r\nMAX_SRPT_RSP_SIZE);\r\nreturn -EINVAL;\r\n}\r\nif (val < MIN_MAX_RSP_SIZE) {\r\npr_err("val: %lu smaller than MIN_MAX_RSP_SIZE: %d\n", val,\r\nMIN_MAX_RSP_SIZE);\r\nreturn -EINVAL;\r\n}\r\nsport->port_attrib.srp_max_rsp_size = val;\r\nreturn count;\r\n}\r\nstatic ssize_t srpt_tpg_attrib_show_srp_sq_size(\r\nstruct se_portal_group *se_tpg,\r\nchar *page)\r\n{\r\nstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\r\nreturn sprintf(page, "%u\n", sport->port_attrib.srp_sq_size);\r\n}\r\nstatic ssize_t srpt_tpg_attrib_store_srp_sq_size(\r\nstruct se_portal_group *se_tpg,\r\nconst char *page,\r\nsize_t count)\r\n{\r\nstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\r\nunsigned long val;\r\nint ret;\r\nret = kstrtoul(page, 0, &val);\r\nif (ret < 0) {\r\npr_err("kstrtoul() failed with ret: %d\n", ret);\r\nreturn -EINVAL;\r\n}\r\nif (val > MAX_SRPT_SRQ_SIZE) {\r\npr_err("val: %lu exceeds MAX_SRPT_SRQ_SIZE: %d\n", val,\r\nMAX_SRPT_SRQ_SIZE);\r\nreturn -EINVAL;\r\n}\r\nif (val < MIN_SRPT_SRQ_SIZE) {\r\npr_err("val: %lu smaller than MIN_SRPT_SRQ_SIZE: %d\n", val,\r\nMIN_SRPT_SRQ_SIZE);\r\nreturn -EINVAL;\r\n}\r\nsport->port_attrib.srp_sq_size = val;\r\nreturn count;\r\n}\r\nstatic ssize_t srpt_tpg_show_enable(\r\nstruct se_portal_group *se_tpg,\r\nchar *page)\r\n{\r\nstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\r\nreturn snprintf(page, PAGE_SIZE, "%d\n", (sport->enabled) ? 1: 0);\r\n}\r\nstatic ssize_t srpt_tpg_store_enable(\r\nstruct se_portal_group *se_tpg,\r\nconst char *page,\r\nsize_t count)\r\n{\r\nstruct srpt_port *sport = container_of(se_tpg, struct srpt_port, port_tpg_1);\r\nunsigned long tmp;\r\nint ret;\r\nret = kstrtoul(page, 0, &tmp);\r\nif (ret < 0) {\r\nprintk(KERN_ERR "Unable to extract srpt_tpg_store_enable\n");\r\nreturn -EINVAL;\r\n}\r\nif ((tmp != 0) && (tmp != 1)) {\r\nprintk(KERN_ERR "Illegal value for srpt_tpg_store_enable: %lu\n", tmp);\r\nreturn -EINVAL;\r\n}\r\nif (tmp == 1)\r\nsport->enabled = true;\r\nelse\r\nsport->enabled = false;\r\nreturn count;\r\n}\r\nstatic struct se_portal_group *srpt_make_tpg(struct se_wwn *wwn,\r\nstruct config_group *group,\r\nconst char *name)\r\n{\r\nstruct srpt_port *sport = container_of(wwn, struct srpt_port, port_wwn);\r\nint res;\r\nres = core_tpg_register(&srpt_target->tf_ops, &sport->port_wwn,\r\n&sport->port_tpg_1, sport, TRANSPORT_TPG_TYPE_NORMAL);\r\nif (res)\r\nreturn ERR_PTR(res);\r\nreturn &sport->port_tpg_1;\r\n}\r\nstatic void srpt_drop_tpg(struct se_portal_group *tpg)\r\n{\r\nstruct srpt_port *sport = container_of(tpg,\r\nstruct srpt_port, port_tpg_1);\r\nsport->enabled = false;\r\ncore_tpg_deregister(&sport->port_tpg_1);\r\n}\r\nstatic struct se_wwn *srpt_make_tport(struct target_fabric_configfs *tf,\r\nstruct config_group *group,\r\nconst char *name)\r\n{\r\nstruct srpt_port *sport;\r\nint ret;\r\nsport = srpt_lookup_port(name);\r\npr_debug("make_tport(%s)\n", name);\r\nret = -EINVAL;\r\nif (!sport)\r\ngoto err;\r\nreturn &sport->port_wwn;\r\nerr:\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void srpt_drop_tport(struct se_wwn *wwn)\r\n{\r\nstruct srpt_port *sport = container_of(wwn, struct srpt_port, port_wwn);\r\npr_debug("drop_tport(%s\n", config_item_name(&sport->port_wwn.wwn_group.cg_item));\r\n}\r\nstatic ssize_t srpt_wwn_show_attr_version(struct target_fabric_configfs *tf,\r\nchar *buf)\r\n{\r\nreturn scnprintf(buf, PAGE_SIZE, "%s\n", DRV_VERSION);\r\n}\r\nstatic int __init srpt_init_module(void)\r\n{\r\nint ret;\r\nret = -EINVAL;\r\nif (srp_max_req_size < MIN_MAX_REQ_SIZE) {\r\nprintk(KERN_ERR "invalid value %d for kernel module parameter"\r\n" srp_max_req_size -- must be at least %d.\n",\r\nsrp_max_req_size, MIN_MAX_REQ_SIZE);\r\ngoto out;\r\n}\r\nif (srpt_srq_size < MIN_SRPT_SRQ_SIZE\r\n|| srpt_srq_size > MAX_SRPT_SRQ_SIZE) {\r\nprintk(KERN_ERR "invalid value %d for kernel module parameter"\r\n" srpt_srq_size -- must be in the range [%d..%d].\n",\r\nsrpt_srq_size, MIN_SRPT_SRQ_SIZE, MAX_SRPT_SRQ_SIZE);\r\ngoto out;\r\n}\r\nsrpt_target = target_fabric_configfs_init(THIS_MODULE, "srpt");\r\nif (IS_ERR(srpt_target)) {\r\nprintk(KERN_ERR "couldn't register\n");\r\nret = PTR_ERR(srpt_target);\r\ngoto out;\r\n}\r\nsrpt_target->tf_ops = srpt_template;\r\nsrpt_target->tf_cit_tmpl.tfc_wwn_cit.ct_attrs = srpt_wwn_attrs;\r\nsrpt_target->tf_cit_tmpl.tfc_tpg_base_cit.ct_attrs = srpt_tpg_attrs;\r\nsrpt_target->tf_cit_tmpl.tfc_tpg_attrib_cit.ct_attrs = srpt_tpg_attrib_attrs;\r\nsrpt_target->tf_cit_tmpl.tfc_tpg_param_cit.ct_attrs = NULL;\r\nsrpt_target->tf_cit_tmpl.tfc_tpg_np_base_cit.ct_attrs = NULL;\r\nsrpt_target->tf_cit_tmpl.tfc_tpg_nacl_base_cit.ct_attrs = NULL;\r\nsrpt_target->tf_cit_tmpl.tfc_tpg_nacl_attrib_cit.ct_attrs = NULL;\r\nsrpt_target->tf_cit_tmpl.tfc_tpg_nacl_auth_cit.ct_attrs = NULL;\r\nsrpt_target->tf_cit_tmpl.tfc_tpg_nacl_param_cit.ct_attrs = NULL;\r\nret = target_fabric_configfs_register(srpt_target);\r\nif (ret < 0) {\r\nprintk(KERN_ERR "couldn't register\n");\r\ngoto out_free_target;\r\n}\r\nret = ib_register_client(&srpt_client);\r\nif (ret) {\r\nprintk(KERN_ERR "couldn't register IB client\n");\r\ngoto out_unregister_target;\r\n}\r\nreturn 0;\r\nout_unregister_target:\r\ntarget_fabric_configfs_deregister(srpt_target);\r\nsrpt_target = NULL;\r\nout_free_target:\r\nif (srpt_target)\r\ntarget_fabric_configfs_free(srpt_target);\r\nout:\r\nreturn ret;\r\n}\r\nstatic void __exit srpt_cleanup_module(void)\r\n{\r\nib_unregister_client(&srpt_client);\r\ntarget_fabric_configfs_deregister(srpt_target);\r\nsrpt_target = NULL;\r\n}
