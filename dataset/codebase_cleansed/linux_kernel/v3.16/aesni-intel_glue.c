static void aesni_gcm_enc_avx(void *ctx, u8 *out,\r\nconst u8 *in, unsigned long plaintext_len, u8 *iv,\r\nu8 *hash_subkey, const u8 *aad, unsigned long aad_len,\r\nu8 *auth_tag, unsigned long auth_tag_len)\r\n{\r\nif (plaintext_len < AVX_GEN2_OPTSIZE) {\r\naesni_gcm_enc(ctx, out, in, plaintext_len, iv, hash_subkey, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else {\r\naesni_gcm_precomp_avx_gen2(ctx, hash_subkey);\r\naesni_gcm_enc_avx_gen2(ctx, out, in, plaintext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n}\r\n}\r\nstatic void aesni_gcm_dec_avx(void *ctx, u8 *out,\r\nconst u8 *in, unsigned long ciphertext_len, u8 *iv,\r\nu8 *hash_subkey, const u8 *aad, unsigned long aad_len,\r\nu8 *auth_tag, unsigned long auth_tag_len)\r\n{\r\nif (ciphertext_len < AVX_GEN2_OPTSIZE) {\r\naesni_gcm_dec(ctx, out, in, ciphertext_len, iv, hash_subkey, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else {\r\naesni_gcm_precomp_avx_gen2(ctx, hash_subkey);\r\naesni_gcm_dec_avx_gen2(ctx, out, in, ciphertext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n}\r\n}\r\nstatic void aesni_gcm_enc_avx2(void *ctx, u8 *out,\r\nconst u8 *in, unsigned long plaintext_len, u8 *iv,\r\nu8 *hash_subkey, const u8 *aad, unsigned long aad_len,\r\nu8 *auth_tag, unsigned long auth_tag_len)\r\n{\r\nif (plaintext_len < AVX_GEN2_OPTSIZE) {\r\naesni_gcm_enc(ctx, out, in, plaintext_len, iv, hash_subkey, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else if (plaintext_len < AVX_GEN4_OPTSIZE) {\r\naesni_gcm_precomp_avx_gen2(ctx, hash_subkey);\r\naesni_gcm_enc_avx_gen2(ctx, out, in, plaintext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else {\r\naesni_gcm_precomp_avx_gen4(ctx, hash_subkey);\r\naesni_gcm_enc_avx_gen4(ctx, out, in, plaintext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n}\r\n}\r\nstatic void aesni_gcm_dec_avx2(void *ctx, u8 *out,\r\nconst u8 *in, unsigned long ciphertext_len, u8 *iv,\r\nu8 *hash_subkey, const u8 *aad, unsigned long aad_len,\r\nu8 *auth_tag, unsigned long auth_tag_len)\r\n{\r\nif (ciphertext_len < AVX_GEN2_OPTSIZE) {\r\naesni_gcm_dec(ctx, out, in, ciphertext_len, iv, hash_subkey,\r\naad, aad_len, auth_tag, auth_tag_len);\r\n} else if (ciphertext_len < AVX_GEN4_OPTSIZE) {\r\naesni_gcm_precomp_avx_gen2(ctx, hash_subkey);\r\naesni_gcm_dec_avx_gen2(ctx, out, in, ciphertext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n} else {\r\naesni_gcm_precomp_avx_gen4(ctx, hash_subkey);\r\naesni_gcm_dec_avx_gen4(ctx, out, in, ciphertext_len, iv, aad,\r\naad_len, auth_tag, auth_tag_len);\r\n}\r\n}\r\nstatic inline struct\r\naesni_rfc4106_gcm_ctx *aesni_rfc4106_gcm_ctx_get(struct crypto_aead *tfm)\r\n{\r\nreturn\r\n(struct aesni_rfc4106_gcm_ctx *)\r\nPTR_ALIGN((u8 *)\r\ncrypto_tfm_ctx(crypto_aead_tfm(tfm)), AESNI_ALIGN);\r\n}\r\nstatic inline struct crypto_aes_ctx *aes_ctx(void *raw_ctx)\r\n{\r\nunsigned long addr = (unsigned long)raw_ctx;\r\nunsigned long align = AESNI_ALIGN;\r\nif (align <= crypto_tfm_ctx_alignment())\r\nalign = 1;\r\nreturn (struct crypto_aes_ctx *)ALIGN(addr, align);\r\n}\r\nstatic int aes_set_key_common(struct crypto_tfm *tfm, void *raw_ctx,\r\nconst u8 *in_key, unsigned int key_len)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(raw_ctx);\r\nu32 *flags = &tfm->crt_flags;\r\nint err;\r\nif (key_len != AES_KEYSIZE_128 && key_len != AES_KEYSIZE_192 &&\r\nkey_len != AES_KEYSIZE_256) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nif (!irq_fpu_usable())\r\nerr = crypto_aes_expand_key(ctx, in_key, key_len);\r\nelse {\r\nkernel_fpu_begin();\r\nerr = aesni_set_key(ctx, in_key, key_len);\r\nkernel_fpu_end();\r\n}\r\nreturn err;\r\n}\r\nstatic int aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nreturn aes_set_key_common(tfm, crypto_tfm_ctx(tfm), in_key, key_len);\r\n}\r\nstatic void aes_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\r\nif (!irq_fpu_usable())\r\ncrypto_aes_encrypt_x86(ctx, dst, src);\r\nelse {\r\nkernel_fpu_begin();\r\naesni_enc(ctx, dst, src);\r\nkernel_fpu_end();\r\n}\r\n}\r\nstatic void aes_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\r\nif (!irq_fpu_usable())\r\ncrypto_aes_decrypt_x86(ctx, dst, src);\r\nelse {\r\nkernel_fpu_begin();\r\naesni_dec(ctx, dst, src);\r\nkernel_fpu_end();\r\n}\r\n}\r\nstatic void __aes_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\r\naesni_enc(ctx, dst, src);\r\n}\r\nstatic void __aes_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\r\naesni_dec(ctx, dst, src);\r\n}\r\nstatic int ecb_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes)) {\r\naesni_ecb_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic int ecb_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes)) {\r\naesni_ecb_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic int cbc_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes)) {\r\naesni_cbc_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK, walk.iv);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic int cbc_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes)) {\r\naesni_cbc_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK, walk.iv);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic void ctr_crypt_final(struct crypto_aes_ctx *ctx,\r\nstruct blkcipher_walk *walk)\r\n{\r\nu8 *ctrblk = walk->iv;\r\nu8 keystream[AES_BLOCK_SIZE];\r\nu8 *src = walk->src.virt.addr;\r\nu8 *dst = walk->dst.virt.addr;\r\nunsigned int nbytes = walk->nbytes;\r\naesni_enc(ctx, keystream, ctrblk);\r\ncrypto_xor(keystream, src, nbytes);\r\nmemcpy(dst, keystream, nbytes);\r\ncrypto_inc(ctrblk, AES_BLOCK_SIZE);\r\n}\r\nstatic int ctr_crypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = aes_ctx(crypto_blkcipher_ctx(desc->tfm));\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nwhile ((nbytes = walk.nbytes) >= AES_BLOCK_SIZE) {\r\naesni_ctr_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes & AES_BLOCK_MASK, walk.iv);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nif (walk.nbytes) {\r\nctr_crypt_final(ctx, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, 0);\r\n}\r\nkernel_fpu_end();\r\nreturn err;\r\n}\r\nstatic int ablk_ecb_init(struct crypto_tfm *tfm)\r\n{\r\nreturn ablk_init_common(tfm, "__driver-ecb-aes-aesni");\r\n}\r\nstatic int ablk_cbc_init(struct crypto_tfm *tfm)\r\n{\r\nreturn ablk_init_common(tfm, "__driver-cbc-aes-aesni");\r\n}\r\nstatic int ablk_ctr_init(struct crypto_tfm *tfm)\r\n{\r\nreturn ablk_init_common(tfm, "__driver-ctr-aes-aesni");\r\n}\r\nstatic int ablk_pcbc_init(struct crypto_tfm *tfm)\r\n{\r\nreturn ablk_init_common(tfm, "fpu(pcbc(__driver-aes-aesni))");\r\n}\r\nstatic void lrw_xts_encrypt_callback(void *ctx, u8 *blks, unsigned int nbytes)\r\n{\r\naesni_ecb_enc(ctx, blks, blks, nbytes);\r\n}\r\nstatic void lrw_xts_decrypt_callback(void *ctx, u8 *blks, unsigned int nbytes)\r\n{\r\naesni_ecb_dec(ctx, blks, blks, nbytes);\r\n}\r\nstatic int lrw_aesni_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct aesni_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint err;\r\nerr = aes_set_key_common(tfm, ctx->raw_aes_ctx, key,\r\nkeylen - AES_BLOCK_SIZE);\r\nif (err)\r\nreturn err;\r\nreturn lrw_init_table(&ctx->lrw_table, key + keylen - AES_BLOCK_SIZE);\r\n}\r\nstatic void lrw_aesni_exit_tfm(struct crypto_tfm *tfm)\r\n{\r\nstruct aesni_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nlrw_free_table(&ctx->lrw_table);\r\n}\r\nstatic int lrw_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[8];\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = aes_ctx(ctx->raw_aes_ctx),\r\n.crypt_fn = lrw_xts_encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\nstatic int lrw_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[8];\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = aes_ctx(ctx->raw_aes_ctx),\r\n.crypt_fn = lrw_xts_decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\nstatic int xts_aesni_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_tfm_ctx(tfm);\r\nu32 *flags = &tfm->crt_flags;\r\nint err;\r\nif (keylen % 2) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nerr = aes_set_key_common(tfm, ctx->raw_crypt_ctx, key, keylen / 2);\r\nif (err)\r\nreturn err;\r\nreturn aes_set_key_common(tfm, ctx->raw_tweak_ctx, key + keylen / 2,\r\nkeylen / 2);\r\n}\r\nstatic void aesni_xts_tweak(void *ctx, u8 *out, const u8 *in)\r\n{\r\naesni_enc(ctx, out, in);\r\n}\r\nstatic void aesni_xts_enc(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\nglue_xts_crypt_128bit_one(ctx, dst, src, iv, GLUE_FUNC_CAST(aesni_enc));\r\n}\r\nstatic void aesni_xts_dec(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\nglue_xts_crypt_128bit_one(ctx, dst, src, iv, GLUE_FUNC_CAST(aesni_dec));\r\n}\r\nstatic void aesni_xts_enc8(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\naesni_xts_crypt8(ctx, (u8 *)dst, (const u8 *)src, true, (u8 *)iv);\r\n}\r\nstatic void aesni_xts_dec8(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\naesni_xts_crypt8(ctx, (u8 *)dst, (const u8 *)src, false, (u8 *)iv);\r\n}\r\nstatic int xts_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nreturn glue_xts_crypt_128bit(&aesni_enc_xts, desc, dst, src, nbytes,\r\nXTS_TWEAK_CAST(aesni_xts_tweak),\r\naes_ctx(ctx->raw_tweak_ctx),\r\naes_ctx(ctx->raw_crypt_ctx));\r\n}\r\nstatic int xts_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nreturn glue_xts_crypt_128bit(&aesni_dec_xts, desc, dst, src, nbytes,\r\nXTS_TWEAK_CAST(aesni_xts_tweak),\r\naes_ctx(ctx->raw_tweak_ctx),\r\naes_ctx(ctx->raw_crypt_ctx));\r\n}\r\nstatic int xts_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[8];\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = aes_ctx(ctx->raw_tweak_ctx),\r\n.tweak_fn = aesni_xts_tweak,\r\n.crypt_ctx = aes_ctx(ctx->raw_crypt_ctx),\r\n.crypt_fn = lrw_xts_encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\nstatic int xts_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct aesni_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[8];\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = aes_ctx(ctx->raw_tweak_ctx),\r\n.tweak_fn = aesni_xts_tweak,\r\n.crypt_ctx = aes_ctx(ctx->raw_crypt_ctx),\r\n.crypt_fn = lrw_xts_decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nkernel_fpu_begin();\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\nstatic int rfc4106_init(struct crypto_tfm *tfm)\r\n{\r\nstruct cryptd_aead *cryptd_tfm;\r\nstruct aesni_rfc4106_gcm_ctx *ctx = (struct aesni_rfc4106_gcm_ctx *)\r\nPTR_ALIGN((u8 *)crypto_tfm_ctx(tfm), AESNI_ALIGN);\r\nstruct crypto_aead *cryptd_child;\r\nstruct aesni_rfc4106_gcm_ctx *child_ctx;\r\ncryptd_tfm = cryptd_alloc_aead("__driver-gcm-aes-aesni", 0, 0);\r\nif (IS_ERR(cryptd_tfm))\r\nreturn PTR_ERR(cryptd_tfm);\r\ncryptd_child = cryptd_aead_child(cryptd_tfm);\r\nchild_ctx = aesni_rfc4106_gcm_ctx_get(cryptd_child);\r\nmemcpy(child_ctx, ctx, sizeof(*ctx));\r\nctx->cryptd_tfm = cryptd_tfm;\r\ntfm->crt_aead.reqsize = sizeof(struct aead_request)\r\n+ crypto_aead_reqsize(&cryptd_tfm->base);\r\nreturn 0;\r\n}\r\nstatic void rfc4106_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct aesni_rfc4106_gcm_ctx *ctx =\r\n(struct aesni_rfc4106_gcm_ctx *)\r\nPTR_ALIGN((u8 *)crypto_tfm_ctx(tfm), AESNI_ALIGN);\r\nif (!IS_ERR(ctx->cryptd_tfm))\r\ncryptd_free_aead(ctx->cryptd_tfm);\r\nreturn;\r\n}\r\nstatic void\r\nrfc4106_set_hash_subkey_done(struct crypto_async_request *req, int err)\r\n{\r\nstruct aesni_gcm_set_hash_subkey_result *result = req->data;\r\nif (err == -EINPROGRESS)\r\nreturn;\r\nresult->err = err;\r\ncomplete(&result->completion);\r\n}\r\nstatic int\r\nrfc4106_set_hash_subkey(u8 *hash_subkey, const u8 *key, unsigned int key_len)\r\n{\r\nstruct crypto_ablkcipher *ctr_tfm;\r\nstruct ablkcipher_request *req;\r\nint ret = -EINVAL;\r\nstruct aesni_hash_subkey_req_data *req_data;\r\nctr_tfm = crypto_alloc_ablkcipher("ctr(aes)", 0, 0);\r\nif (IS_ERR(ctr_tfm))\r\nreturn PTR_ERR(ctr_tfm);\r\ncrypto_ablkcipher_clear_flags(ctr_tfm, ~0);\r\nret = crypto_ablkcipher_setkey(ctr_tfm, key, key_len);\r\nif (ret)\r\ngoto out_free_ablkcipher;\r\nret = -ENOMEM;\r\nreq = ablkcipher_request_alloc(ctr_tfm, GFP_KERNEL);\r\nif (!req)\r\ngoto out_free_ablkcipher;\r\nreq_data = kmalloc(sizeof(*req_data), GFP_KERNEL);\r\nif (!req_data)\r\ngoto out_free_request;\r\nmemset(req_data->iv, 0, sizeof(req_data->iv));\r\nmemset(hash_subkey, 0, RFC4106_HASH_SUBKEY_SIZE);\r\ninit_completion(&req_data->result.completion);\r\nsg_init_one(&req_data->sg, hash_subkey, RFC4106_HASH_SUBKEY_SIZE);\r\nablkcipher_request_set_tfm(req, ctr_tfm);\r\nablkcipher_request_set_callback(req, CRYPTO_TFM_REQ_MAY_SLEEP |\r\nCRYPTO_TFM_REQ_MAY_BACKLOG,\r\nrfc4106_set_hash_subkey_done,\r\n&req_data->result);\r\nablkcipher_request_set_crypt(req, &req_data->sg,\r\n&req_data->sg, RFC4106_HASH_SUBKEY_SIZE, req_data->iv);\r\nret = crypto_ablkcipher_encrypt(req);\r\nif (ret == -EINPROGRESS || ret == -EBUSY) {\r\nret = wait_for_completion_interruptible\r\n(&req_data->result.completion);\r\nif (!ret)\r\nret = req_data->result.err;\r\n}\r\nkfree(req_data);\r\nout_free_request:\r\nablkcipher_request_free(req);\r\nout_free_ablkcipher:\r\ncrypto_free_ablkcipher(ctr_tfm);\r\nreturn ret;\r\n}\r\nstatic int rfc4106_set_key(struct crypto_aead *parent, const u8 *key,\r\nunsigned int key_len)\r\n{\r\nint ret = 0;\r\nstruct crypto_tfm *tfm = crypto_aead_tfm(parent);\r\nstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(parent);\r\nstruct crypto_aead *cryptd_child = cryptd_aead_child(ctx->cryptd_tfm);\r\nstruct aesni_rfc4106_gcm_ctx *child_ctx =\r\naesni_rfc4106_gcm_ctx_get(cryptd_child);\r\nu8 *new_key_align, *new_key_mem = NULL;\r\nif (key_len < 4) {\r\ncrypto_tfm_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nkey_len -= 4;\r\nif (key_len != AES_KEYSIZE_128) {\r\ncrypto_tfm_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nmemcpy(ctx->nonce, key + key_len, sizeof(ctx->nonce));\r\nif ((unsigned long)(&(ctx->aes_key_expanded.key_enc[0])) % AESNI_ALIGN)\r\nreturn -EINVAL;\r\nif ((unsigned long)key % AESNI_ALIGN) {\r\nnew_key_mem = kmalloc(key_len+AESNI_ALIGN, GFP_KERNEL);\r\nif (!new_key_mem)\r\nreturn -ENOMEM;\r\nnew_key_align = PTR_ALIGN(new_key_mem, AESNI_ALIGN);\r\nmemcpy(new_key_align, key, key_len);\r\nkey = new_key_align;\r\n}\r\nif (!irq_fpu_usable())\r\nret = crypto_aes_expand_key(&(ctx->aes_key_expanded),\r\nkey, key_len);\r\nelse {\r\nkernel_fpu_begin();\r\nret = aesni_set_key(&(ctx->aes_key_expanded), key, key_len);\r\nkernel_fpu_end();\r\n}\r\nif ((unsigned long)(&(ctx->hash_subkey[0])) % AESNI_ALIGN) {\r\nret = -EINVAL;\r\ngoto exit;\r\n}\r\nret = rfc4106_set_hash_subkey(ctx->hash_subkey, key, key_len);\r\nmemcpy(child_ctx, ctx, sizeof(*ctx));\r\nexit:\r\nkfree(new_key_mem);\r\nreturn ret;\r\n}\r\nstatic int rfc4106_set_authsize(struct crypto_aead *parent,\r\nunsigned int authsize)\r\n{\r\nstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(parent);\r\nstruct crypto_aead *cryptd_child = cryptd_aead_child(ctx->cryptd_tfm);\r\nswitch (authsize) {\r\ncase 8:\r\ncase 12:\r\ncase 16:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ncrypto_aead_crt(parent)->authsize = authsize;\r\ncrypto_aead_crt(cryptd_child)->authsize = authsize;\r\nreturn 0;\r\n}\r\nstatic int rfc4106_encrypt(struct aead_request *req)\r\n{\r\nint ret;\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(tfm);\r\nif (!irq_fpu_usable()) {\r\nstruct aead_request *cryptd_req =\r\n(struct aead_request *) aead_request_ctx(req);\r\nmemcpy(cryptd_req, req, sizeof(*req));\r\naead_request_set_tfm(cryptd_req, &ctx->cryptd_tfm->base);\r\nreturn crypto_aead_encrypt(cryptd_req);\r\n} else {\r\nstruct crypto_aead *cryptd_child = cryptd_aead_child(ctx->cryptd_tfm);\r\nkernel_fpu_begin();\r\nret = cryptd_child->base.crt_aead.encrypt(req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\n}\r\nstatic int rfc4106_decrypt(struct aead_request *req)\r\n{\r\nint ret;\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(tfm);\r\nif (!irq_fpu_usable()) {\r\nstruct aead_request *cryptd_req =\r\n(struct aead_request *) aead_request_ctx(req);\r\nmemcpy(cryptd_req, req, sizeof(*req));\r\naead_request_set_tfm(cryptd_req, &ctx->cryptd_tfm->base);\r\nreturn crypto_aead_decrypt(cryptd_req);\r\n} else {\r\nstruct crypto_aead *cryptd_child = cryptd_aead_child(ctx->cryptd_tfm);\r\nkernel_fpu_begin();\r\nret = cryptd_child->base.crt_aead.decrypt(req);\r\nkernel_fpu_end();\r\nreturn ret;\r\n}\r\n}\r\nstatic int __driver_rfc4106_encrypt(struct aead_request *req)\r\n{\r\nu8 one_entry_in_sg = 0;\r\nu8 *src, *dst, *assoc;\r\n__be32 counter = cpu_to_be32(1);\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(tfm);\r\nvoid *aes_ctx = &(ctx->aes_key_expanded);\r\nunsigned long auth_tag_len = crypto_aead_authsize(tfm);\r\nu8 iv_tab[16+AESNI_ALIGN];\r\nu8* iv = (u8 *) PTR_ALIGN((u8 *)iv_tab, AESNI_ALIGN);\r\nstruct scatter_walk src_sg_walk;\r\nstruct scatter_walk assoc_sg_walk;\r\nstruct scatter_walk dst_sg_walk;\r\nunsigned int i;\r\nif (unlikely(req->assoclen != 8 && req->assoclen != 12))\r\nreturn -EINVAL;\r\nfor (i = 0; i < 4; i++)\r\n*(iv+i) = ctx->nonce[i];\r\nfor (i = 0; i < 8; i++)\r\n*(iv+4+i) = req->iv[i];\r\n*((__be32 *)(iv+12)) = counter;\r\nif ((sg_is_last(req->src)) && (sg_is_last(req->assoc))) {\r\none_entry_in_sg = 1;\r\nscatterwalk_start(&src_sg_walk, req->src);\r\nscatterwalk_start(&assoc_sg_walk, req->assoc);\r\nsrc = scatterwalk_map(&src_sg_walk);\r\nassoc = scatterwalk_map(&assoc_sg_walk);\r\ndst = src;\r\nif (unlikely(req->src != req->dst)) {\r\nscatterwalk_start(&dst_sg_walk, req->dst);\r\ndst = scatterwalk_map(&dst_sg_walk);\r\n}\r\n} else {\r\nsrc = kmalloc(req->cryptlen + auth_tag_len + req->assoclen,\r\nGFP_ATOMIC);\r\nif (unlikely(!src))\r\nreturn -ENOMEM;\r\nassoc = (src + req->cryptlen + auth_tag_len);\r\nscatterwalk_map_and_copy(src, req->src, 0, req->cryptlen, 0);\r\nscatterwalk_map_and_copy(assoc, req->assoc, 0,\r\nreq->assoclen, 0);\r\ndst = src;\r\n}\r\naesni_gcm_enc_tfm(aes_ctx, dst, src, (unsigned long)req->cryptlen, iv,\r\nctx->hash_subkey, assoc, (unsigned long)req->assoclen, dst\r\n+ ((unsigned long)req->cryptlen), auth_tag_len);\r\nif (one_entry_in_sg) {\r\nif (unlikely(req->src != req->dst)) {\r\nscatterwalk_unmap(dst);\r\nscatterwalk_done(&dst_sg_walk, 0, 0);\r\n}\r\nscatterwalk_unmap(src);\r\nscatterwalk_unmap(assoc);\r\nscatterwalk_done(&src_sg_walk, 0, 0);\r\nscatterwalk_done(&assoc_sg_walk, 0, 0);\r\n} else {\r\nscatterwalk_map_and_copy(dst, req->dst, 0,\r\nreq->cryptlen + auth_tag_len, 1);\r\nkfree(src);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __driver_rfc4106_decrypt(struct aead_request *req)\r\n{\r\nu8 one_entry_in_sg = 0;\r\nu8 *src, *dst, *assoc;\r\nunsigned long tempCipherLen = 0;\r\n__be32 counter = cpu_to_be32(1);\r\nint retval = 0;\r\nstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\r\nstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(tfm);\r\nvoid *aes_ctx = &(ctx->aes_key_expanded);\r\nunsigned long auth_tag_len = crypto_aead_authsize(tfm);\r\nu8 iv_and_authTag[32+AESNI_ALIGN];\r\nu8 *iv = (u8 *) PTR_ALIGN((u8 *)iv_and_authTag, AESNI_ALIGN);\r\nu8 *authTag = iv + 16;\r\nstruct scatter_walk src_sg_walk;\r\nstruct scatter_walk assoc_sg_walk;\r\nstruct scatter_walk dst_sg_walk;\r\nunsigned int i;\r\nif (unlikely((req->cryptlen < auth_tag_len) ||\r\n(req->assoclen != 8 && req->assoclen != 12)))\r\nreturn -EINVAL;\r\ntempCipherLen = (unsigned long)(req->cryptlen - auth_tag_len);\r\nfor (i = 0; i < 4; i++)\r\n*(iv+i) = ctx->nonce[i];\r\nfor (i = 0; i < 8; i++)\r\n*(iv+4+i) = req->iv[i];\r\n*((__be32 *)(iv+12)) = counter;\r\nif ((sg_is_last(req->src)) && (sg_is_last(req->assoc))) {\r\none_entry_in_sg = 1;\r\nscatterwalk_start(&src_sg_walk, req->src);\r\nscatterwalk_start(&assoc_sg_walk, req->assoc);\r\nsrc = scatterwalk_map(&src_sg_walk);\r\nassoc = scatterwalk_map(&assoc_sg_walk);\r\ndst = src;\r\nif (unlikely(req->src != req->dst)) {\r\nscatterwalk_start(&dst_sg_walk, req->dst);\r\ndst = scatterwalk_map(&dst_sg_walk);\r\n}\r\n} else {\r\nsrc = kmalloc(req->cryptlen + req->assoclen, GFP_ATOMIC);\r\nif (!src)\r\nreturn -ENOMEM;\r\nassoc = (src + req->cryptlen + auth_tag_len);\r\nscatterwalk_map_and_copy(src, req->src, 0, req->cryptlen, 0);\r\nscatterwalk_map_and_copy(assoc, req->assoc, 0,\r\nreq->assoclen, 0);\r\ndst = src;\r\n}\r\naesni_gcm_dec_tfm(aes_ctx, dst, src, tempCipherLen, iv,\r\nctx->hash_subkey, assoc, (unsigned long)req->assoclen,\r\nauthTag, auth_tag_len);\r\nretval = crypto_memneq(src + tempCipherLen, authTag, auth_tag_len) ?\r\n-EBADMSG : 0;\r\nif (one_entry_in_sg) {\r\nif (unlikely(req->src != req->dst)) {\r\nscatterwalk_unmap(dst);\r\nscatterwalk_done(&dst_sg_walk, 0, 0);\r\n}\r\nscatterwalk_unmap(src);\r\nscatterwalk_unmap(assoc);\r\nscatterwalk_done(&src_sg_walk, 0, 0);\r\nscatterwalk_done(&assoc_sg_walk, 0, 0);\r\n} else {\r\nscatterwalk_map_and_copy(dst, req->dst, 0, req->cryptlen, 1);\r\nkfree(src);\r\n}\r\nreturn retval;\r\n}\r\nstatic int __init aesni_init(void)\r\n{\r\nint err;\r\nif (!x86_match_cpu(aesni_cpu_id))\r\nreturn -ENODEV;\r\n#ifdef CONFIG_X86_64\r\n#ifdef CONFIG_AS_AVX2\r\nif (boot_cpu_has(X86_FEATURE_AVX2)) {\r\npr_info("AVX2 version of gcm_enc/dec engaged.\n");\r\naesni_gcm_enc_tfm = aesni_gcm_enc_avx2;\r\naesni_gcm_dec_tfm = aesni_gcm_dec_avx2;\r\n} else\r\n#endif\r\n#ifdef CONFIG_AS_AVX\r\nif (boot_cpu_has(X86_FEATURE_AVX)) {\r\npr_info("AVX version of gcm_enc/dec engaged.\n");\r\naesni_gcm_enc_tfm = aesni_gcm_enc_avx;\r\naesni_gcm_dec_tfm = aesni_gcm_dec_avx;\r\n} else\r\n#endif\r\n{\r\npr_info("SSE version of gcm_enc/dec engaged.\n");\r\naesni_gcm_enc_tfm = aesni_gcm_enc;\r\naesni_gcm_dec_tfm = aesni_gcm_dec;\r\n}\r\n#endif\r\nerr = crypto_fpu_init();\r\nif (err)\r\nreturn err;\r\nreturn crypto_register_algs(aesni_algs, ARRAY_SIZE(aesni_algs));\r\n}\r\nstatic void __exit aesni_exit(void)\r\n{\r\ncrypto_unregister_algs(aesni_algs, ARRAY_SIZE(aesni_algs));\r\ncrypto_fpu_exit();\r\n}
