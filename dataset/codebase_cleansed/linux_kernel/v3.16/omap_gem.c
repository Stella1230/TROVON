static void evict_entry(struct drm_gem_object *obj,\r\nenum tiler_fmt fmt, struct usergart_entry *entry)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint n = usergart[fmt].height;\r\nsize_t size = PAGE_SIZE * n;\r\nloff_t off = mmap_offset(obj) +\r\n(entry->obj_pgoff << PAGE_SHIFT);\r\nconst int m = 1 + ((omap_obj->width << fmt) / PAGE_SIZE);\r\nif (m > 1) {\r\nint i;\r\nfor (i = n; i > 0; i--) {\r\nunmap_mapping_range(obj->dev->anon_inode->i_mapping,\r\noff, PAGE_SIZE, 1);\r\noff += PAGE_SIZE * m;\r\n}\r\n} else {\r\nunmap_mapping_range(obj->dev->anon_inode->i_mapping,\r\noff, size, 1);\r\n}\r\nentry->obj = NULL;\r\n}\r\nstatic void evict(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nif (omap_obj->flags & OMAP_BO_TILED) {\r\nenum tiler_fmt fmt = gem2fmt(omap_obj->flags);\r\nint i;\r\nif (!usergart)\r\nreturn;\r\nfor (i = 0; i < NUM_USERGART_ENTRIES; i++) {\r\nstruct usergart_entry *entry = &usergart[fmt].entry[i];\r\nif (entry->obj == obj)\r\nevict_entry(obj, fmt, entry);\r\n}\r\n}\r\n}\r\nstatic inline bool is_shmem(struct drm_gem_object *obj)\r\n{\r\nreturn obj->filp != NULL;\r\n}\r\nstatic inline bool is_cached_coherent(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nreturn is_shmem(obj) &&\r\n((omap_obj->flags & OMAP_BO_CACHE_MASK) == OMAP_BO_CACHED);\r\n}\r\nstatic int omap_gem_attach_pages(struct drm_gem_object *obj)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nstruct page **pages;\r\nint npages = obj->size >> PAGE_SHIFT;\r\nint i, ret;\r\ndma_addr_t *addrs;\r\nWARN_ON(omap_obj->pages);\r\npages = drm_gem_get_pages(obj, GFP_KERNEL);\r\nif (IS_ERR(pages)) {\r\ndev_err(obj->dev->dev, "could not get pages: %ld\n", PTR_ERR(pages));\r\nreturn PTR_ERR(pages);\r\n}\r\nif (omap_obj->flags & (OMAP_BO_WC|OMAP_BO_UNCACHED)) {\r\naddrs = kmalloc(npages * sizeof(*addrs), GFP_KERNEL);\r\nif (!addrs) {\r\nret = -ENOMEM;\r\ngoto free_pages;\r\n}\r\nfor (i = 0; i < npages; i++) {\r\naddrs[i] = dma_map_page(dev->dev, pages[i],\r\n0, PAGE_SIZE, DMA_BIDIRECTIONAL);\r\n}\r\n} else {\r\naddrs = kzalloc(npages * sizeof(*addrs), GFP_KERNEL);\r\nif (!addrs) {\r\nret = -ENOMEM;\r\ngoto free_pages;\r\n}\r\n}\r\nomap_obj->addrs = addrs;\r\nomap_obj->pages = pages;\r\nreturn 0;\r\nfree_pages:\r\ndrm_gem_put_pages(obj, pages, true, false);\r\nreturn ret;\r\n}\r\nstatic void omap_gem_detach_pages(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nif (omap_obj->flags & (OMAP_BO_WC|OMAP_BO_UNCACHED)) {\r\nint i, npages = obj->size >> PAGE_SHIFT;\r\nfor (i = 0; i < npages; i++) {\r\ndma_unmap_page(obj->dev->dev, omap_obj->addrs[i],\r\nPAGE_SIZE, DMA_BIDIRECTIONAL);\r\n}\r\n}\r\nkfree(omap_obj->addrs);\r\nomap_obj->addrs = NULL;\r\ndrm_gem_put_pages(obj, omap_obj->pages, true, false);\r\nomap_obj->pages = NULL;\r\n}\r\nuint32_t omap_gem_flags(struct drm_gem_object *obj)\r\n{\r\nreturn to_omap_bo(obj)->flags;\r\n}\r\nstatic uint64_t mmap_offset(struct drm_gem_object *obj)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nint ret;\r\nsize_t size;\r\nWARN_ON(!mutex_is_locked(&dev->struct_mutex));\r\nsize = omap_gem_mmap_size(obj);\r\nret = drm_gem_create_mmap_offset_size(obj, size);\r\nif (ret) {\r\ndev_err(dev->dev, "could not allocate mmap offset\n");\r\nreturn 0;\r\n}\r\nreturn drm_vma_node_offset_addr(&obj->vma_node);\r\n}\r\nuint64_t omap_gem_mmap_offset(struct drm_gem_object *obj)\r\n{\r\nuint64_t offset;\r\nmutex_lock(&obj->dev->struct_mutex);\r\noffset = mmap_offset(obj);\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn offset;\r\n}\r\nsize_t omap_gem_mmap_size(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nsize_t size = obj->size;\r\nif (omap_obj->flags & OMAP_BO_TILED) {\r\nsize = tiler_vsize(gem2fmt(omap_obj->flags),\r\nomap_obj->width, omap_obj->height);\r\n}\r\nreturn size;\r\n}\r\nint omap_gem_tiled_size(struct drm_gem_object *obj, uint16_t *w, uint16_t *h)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nif (omap_obj->flags & OMAP_BO_TILED) {\r\n*w = omap_obj->width;\r\n*h = omap_obj->height;\r\nreturn 0;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic int fault_1d(struct drm_gem_object *obj,\r\nstruct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nunsigned long pfn;\r\npgoff_t pgoff;\r\npgoff = ((unsigned long)vmf->virtual_address -\r\nvma->vm_start) >> PAGE_SHIFT;\r\nif (omap_obj->pages) {\r\nomap_gem_cpu_sync(obj, pgoff);\r\npfn = page_to_pfn(omap_obj->pages[pgoff]);\r\n} else {\r\nBUG_ON(!(omap_obj->flags & OMAP_BO_DMA));\r\npfn = (omap_obj->paddr >> PAGE_SHIFT) + pgoff;\r\n}\r\nVERB("Inserting %p pfn %lx, pa %lx", vmf->virtual_address,\r\npfn, pfn << PAGE_SHIFT);\r\nreturn vm_insert_mixed(vma, (unsigned long)vmf->virtual_address, pfn);\r\n}\r\nstatic int fault_2d(struct drm_gem_object *obj,\r\nstruct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nstruct usergart_entry *entry;\r\nenum tiler_fmt fmt = gem2fmt(omap_obj->flags);\r\nstruct page *pages[64];\r\nunsigned long pfn;\r\npgoff_t pgoff, base_pgoff;\r\nvoid __user *vaddr;\r\nint i, ret, slots;\r\nconst int n = usergart[fmt].height;\r\nconst int n_shift = usergart[fmt].height_shift;\r\nconst int m = 1 + ((omap_obj->width << fmt) / PAGE_SIZE);\r\npgoff = ((unsigned long)vmf->virtual_address -\r\nvma->vm_start) >> PAGE_SHIFT;\r\nbase_pgoff = round_down(pgoff, m << n_shift);\r\nslots = omap_obj->width >> usergart[fmt].slot_shift;\r\nvaddr = vmf->virtual_address - ((pgoff - base_pgoff) << PAGE_SHIFT);\r\nentry = &usergart[fmt].entry[usergart[fmt].last];\r\nif (entry->obj)\r\nevict_entry(entry->obj, fmt, entry);\r\nentry->obj = obj;\r\nentry->obj_pgoff = base_pgoff;\r\nbase_pgoff = (base_pgoff >> n_shift) * slots;\r\nif (m > 1) {\r\nint off = pgoff % m;\r\nentry->obj_pgoff += off;\r\nbase_pgoff /= m;\r\nslots = min(slots - (off << n_shift), n);\r\nbase_pgoff += off << n_shift;\r\nvaddr += off << PAGE_SHIFT;\r\n}\r\nmemcpy(pages, &omap_obj->pages[base_pgoff],\r\nsizeof(struct page *) * slots);\r\nmemset(pages + slots, 0,\r\nsizeof(struct page *) * (n - slots));\r\nret = tiler_pin(entry->block, pages, ARRAY_SIZE(pages), 0, true);\r\nif (ret) {\r\ndev_err(obj->dev->dev, "failed to pin: %d\n", ret);\r\nreturn ret;\r\n}\r\npfn = entry->paddr >> PAGE_SHIFT;\r\nVERB("Inserting %p pfn %lx, pa %lx", vmf->virtual_address,\r\npfn, pfn << PAGE_SHIFT);\r\nfor (i = n; i > 0; i--) {\r\nvm_insert_mixed(vma, (unsigned long)vaddr, pfn);\r\npfn += usergart[fmt].stride_pfn;\r\nvaddr += PAGE_SIZE * m;\r\n}\r\nusergart[fmt].last = (usergart[fmt].last + 1) % NUM_USERGART_ENTRIES;\r\nreturn 0;\r\n}\r\nint omap_gem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct drm_gem_object *obj = vma->vm_private_data;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nstruct drm_device *dev = obj->dev;\r\nstruct page **pages;\r\nint ret;\r\nmutex_lock(&dev->struct_mutex);\r\nret = get_pages(obj, &pages);\r\nif (ret)\r\ngoto fail;\r\nif (omap_obj->flags & OMAP_BO_TILED)\r\nret = fault_2d(obj, vma, vmf);\r\nelse\r\nret = fault_1d(obj, vma, vmf);\r\nfail:\r\nmutex_unlock(&dev->struct_mutex);\r\nswitch (ret) {\r\ncase 0:\r\ncase -ERESTARTSYS:\r\ncase -EINTR:\r\nreturn VM_FAULT_NOPAGE;\r\ncase -ENOMEM:\r\nreturn VM_FAULT_OOM;\r\ndefault:\r\nreturn VM_FAULT_SIGBUS;\r\n}\r\n}\r\nint omap_gem_mmap(struct file *filp, struct vm_area_struct *vma)\r\n{\r\nint ret;\r\nret = drm_gem_mmap(filp, vma);\r\nif (ret) {\r\nDBG("mmap failed: %d", ret);\r\nreturn ret;\r\n}\r\nreturn omap_gem_mmap_obj(vma->vm_private_data, vma);\r\n}\r\nint omap_gem_mmap_obj(struct drm_gem_object *obj,\r\nstruct vm_area_struct *vma)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nvma->vm_flags &= ~VM_PFNMAP;\r\nvma->vm_flags |= VM_MIXEDMAP;\r\nif (omap_obj->flags & OMAP_BO_WC) {\r\nvma->vm_page_prot = pgprot_writecombine(vm_get_page_prot(vma->vm_flags));\r\n} else if (omap_obj->flags & OMAP_BO_UNCACHED) {\r\nvma->vm_page_prot = pgprot_noncached(vm_get_page_prot(vma->vm_flags));\r\n} else {\r\nif (WARN_ON(!obj->filp))\r\nreturn -EINVAL;\r\nfput(vma->vm_file);\r\nvma->vm_pgoff = 0;\r\nvma->vm_file = get_file(obj->filp);\r\nvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\r\n}\r\nreturn 0;\r\n}\r\nint omap_gem_dumb_create(struct drm_file *file, struct drm_device *dev,\r\nstruct drm_mode_create_dumb *args)\r\n{\r\nunion omap_gem_size gsize;\r\nargs->pitch = align_pitch(args->pitch, args->width, args->bpp);\r\nargs->size = PAGE_ALIGN(args->pitch * args->height);\r\ngsize = (union omap_gem_size){\r\n.bytes = args->size,\r\n};\r\nreturn omap_gem_new_handle(dev, file, gsize,\r\nOMAP_BO_SCANOUT | OMAP_BO_WC, &args->handle);\r\n}\r\nint omap_gem_dumb_map_offset(struct drm_file *file, struct drm_device *dev,\r\nuint32_t handle, uint64_t *offset)\r\n{\r\nstruct drm_gem_object *obj;\r\nint ret = 0;\r\nobj = drm_gem_object_lookup(dev, file, handle);\r\nif (obj == NULL) {\r\nret = -ENOENT;\r\ngoto fail;\r\n}\r\n*offset = omap_gem_mmap_offset(obj);\r\ndrm_gem_object_unreference_unlocked(obj);\r\nfail:\r\nreturn ret;\r\n}\r\nint omap_gem_roll(struct drm_gem_object *obj, uint32_t roll)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nuint32_t npages = obj->size >> PAGE_SHIFT;\r\nint ret = 0;\r\nif (roll > npages) {\r\ndev_err(obj->dev->dev, "invalid roll: %d\n", roll);\r\nreturn -EINVAL;\r\n}\r\nomap_obj->roll = roll;\r\nmutex_lock(&obj->dev->struct_mutex);\r\nif (omap_obj->block) {\r\nstruct page **pages;\r\nret = get_pages(obj, &pages);\r\nif (ret)\r\ngoto fail;\r\nret = tiler_pin(omap_obj->block, pages, npages, roll, true);\r\nif (ret)\r\ndev_err(obj->dev->dev, "could not repin: %d\n", ret);\r\n}\r\nfail:\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn ret;\r\n}\r\nvoid omap_gem_cpu_sync(struct drm_gem_object *obj, int pgoff)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nif (is_cached_coherent(obj) && omap_obj->addrs[pgoff]) {\r\ndma_unmap_page(dev->dev, omap_obj->addrs[pgoff],\r\nPAGE_SIZE, DMA_BIDIRECTIONAL);\r\nomap_obj->addrs[pgoff] = 0;\r\n}\r\n}\r\nvoid omap_gem_dma_sync(struct drm_gem_object *obj,\r\nenum dma_data_direction dir)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nif (is_cached_coherent(obj)) {\r\nint i, npages = obj->size >> PAGE_SHIFT;\r\nstruct page **pages = omap_obj->pages;\r\nbool dirty = false;\r\nfor (i = 0; i < npages; i++) {\r\nif (!omap_obj->addrs[i]) {\r\nomap_obj->addrs[i] = dma_map_page(dev->dev, pages[i], 0,\r\nPAGE_SIZE, DMA_BIDIRECTIONAL);\r\ndirty = true;\r\n}\r\n}\r\nif (dirty) {\r\nunmap_mapping_range(obj->filp->f_mapping, 0,\r\nomap_gem_mmap_size(obj), 1);\r\n}\r\n}\r\n}\r\nint omap_gem_get_paddr(struct drm_gem_object *obj,\r\ndma_addr_t *paddr, bool remap)\r\n{\r\nstruct omap_drm_private *priv = obj->dev->dev_private;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = 0;\r\nmutex_lock(&obj->dev->struct_mutex);\r\nif (remap && is_shmem(obj) && priv->has_dmm) {\r\nif (omap_obj->paddr_cnt == 0) {\r\nstruct page **pages;\r\nuint32_t npages = obj->size >> PAGE_SHIFT;\r\nenum tiler_fmt fmt = gem2fmt(omap_obj->flags);\r\nstruct tiler_block *block;\r\nBUG_ON(omap_obj->block);\r\nret = get_pages(obj, &pages);\r\nif (ret)\r\ngoto fail;\r\nif (omap_obj->flags & OMAP_BO_TILED) {\r\nblock = tiler_reserve_2d(fmt,\r\nomap_obj->width,\r\nomap_obj->height, 0);\r\n} else {\r\nblock = tiler_reserve_1d(obj->size);\r\n}\r\nif (IS_ERR(block)) {\r\nret = PTR_ERR(block);\r\ndev_err(obj->dev->dev,\r\n"could not remap: %d (%d)\n", ret, fmt);\r\ngoto fail;\r\n}\r\nret = tiler_pin(block, pages, npages,\r\nomap_obj->roll, true);\r\nif (ret) {\r\ntiler_release(block);\r\ndev_err(obj->dev->dev,\r\n"could not pin: %d\n", ret);\r\ngoto fail;\r\n}\r\nomap_obj->paddr = tiler_ssptr(block);\r\nomap_obj->block = block;\r\nDBG("got paddr: %08x", omap_obj->paddr);\r\n}\r\nomap_obj->paddr_cnt++;\r\n*paddr = omap_obj->paddr;\r\n} else if (omap_obj->flags & OMAP_BO_DMA) {\r\n*paddr = omap_obj->paddr;\r\n} else {\r\nret = -EINVAL;\r\ngoto fail;\r\n}\r\nfail:\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn ret;\r\n}\r\nint omap_gem_put_paddr(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = 0;\r\nmutex_lock(&obj->dev->struct_mutex);\r\nif (omap_obj->paddr_cnt > 0) {\r\nomap_obj->paddr_cnt--;\r\nif (omap_obj->paddr_cnt == 0) {\r\nret = tiler_unpin(omap_obj->block);\r\nif (ret) {\r\ndev_err(obj->dev->dev,\r\n"could not unpin pages: %d\n", ret);\r\ngoto fail;\r\n}\r\nret = tiler_release(omap_obj->block);\r\nif (ret) {\r\ndev_err(obj->dev->dev,\r\n"could not release unmap: %d\n", ret);\r\n}\r\nomap_obj->block = NULL;\r\n}\r\n}\r\nfail:\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn ret;\r\n}\r\nint omap_gem_rotated_paddr(struct drm_gem_object *obj, uint32_t orient,\r\nint x, int y, dma_addr_t *paddr)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = -EINVAL;\r\nmutex_lock(&obj->dev->struct_mutex);\r\nif ((omap_obj->paddr_cnt > 0) && omap_obj->block &&\r\n(omap_obj->flags & OMAP_BO_TILED)) {\r\n*paddr = tiler_tsptr(omap_obj->block, orient, x, y);\r\nret = 0;\r\n}\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn ret;\r\n}\r\nint omap_gem_tiled_stride(struct drm_gem_object *obj, uint32_t orient)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = -EINVAL;\r\nif (omap_obj->flags & OMAP_BO_TILED)\r\nret = tiler_stride(gem2fmt(omap_obj->flags), orient);\r\nreturn ret;\r\n}\r\nstatic int get_pages(struct drm_gem_object *obj, struct page ***pages)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = 0;\r\nif (is_shmem(obj) && !omap_obj->pages) {\r\nret = omap_gem_attach_pages(obj);\r\nif (ret) {\r\ndev_err(obj->dev->dev, "could not attach pages\n");\r\nreturn ret;\r\n}\r\n}\r\n*pages = omap_obj->pages;\r\nreturn 0;\r\n}\r\nint omap_gem_get_pages(struct drm_gem_object *obj, struct page ***pages,\r\nbool remap)\r\n{\r\nint ret;\r\nif (!remap) {\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nif (!omap_obj->pages)\r\nreturn -ENOMEM;\r\n*pages = omap_obj->pages;\r\nreturn 0;\r\n}\r\nmutex_lock(&obj->dev->struct_mutex);\r\nret = get_pages(obj, pages);\r\nmutex_unlock(&obj->dev->struct_mutex);\r\nreturn ret;\r\n}\r\nint omap_gem_put_pages(struct drm_gem_object *obj)\r\n{\r\nreturn 0;\r\n}\r\nvoid *omap_gem_vaddr(struct drm_gem_object *obj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nWARN_ON(!mutex_is_locked(&obj->dev->struct_mutex));\r\nif (!omap_obj->vaddr) {\r\nstruct page **pages;\r\nint ret = get_pages(obj, &pages);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nomap_obj->vaddr = vmap(pages, obj->size >> PAGE_SHIFT,\r\nVM_MAP, pgprot_writecombine(PAGE_KERNEL));\r\n}\r\nreturn omap_obj->vaddr;\r\n}\r\nint omap_gem_resume(struct device *dev)\r\n{\r\nstruct drm_device *drm_dev = dev_get_drvdata(dev);\r\nstruct omap_drm_private *priv = drm_dev->dev_private;\r\nstruct omap_gem_object *omap_obj;\r\nint ret = 0;\r\nlist_for_each_entry(omap_obj, &priv->obj_list, mm_list) {\r\nif (omap_obj->block) {\r\nstruct drm_gem_object *obj = &omap_obj->base;\r\nuint32_t npages = obj->size >> PAGE_SHIFT;\r\nWARN_ON(!omap_obj->pages);\r\nret = tiler_pin(omap_obj->block,\r\nomap_obj->pages, npages,\r\nomap_obj->roll, true);\r\nif (ret) {\r\ndev_err(dev, "could not repin: %d\n", ret);\r\nreturn ret;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nvoid omap_gem_describe(struct drm_gem_object *obj, struct seq_file *m)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nuint64_t off;\r\noff = drm_vma_node_start(&obj->vma_node);\r\nseq_printf(m, "%08x: %2d (%2d) %08llx %08Zx (%2d) %p %4d",\r\nomap_obj->flags, obj->name, obj->refcount.refcount.counter,\r\noff, omap_obj->paddr, omap_obj->paddr_cnt,\r\nomap_obj->vaddr, omap_obj->roll);\r\nif (omap_obj->flags & OMAP_BO_TILED) {\r\nseq_printf(m, " %dx%d", omap_obj->width, omap_obj->height);\r\nif (omap_obj->block) {\r\nstruct tcm_area *area = &omap_obj->block->area;\r\nseq_printf(m, " (%dx%d, %dx%d)",\r\narea->p0.x, area->p0.y,\r\narea->p1.x, area->p1.y);\r\n}\r\n} else {\r\nseq_printf(m, " %d", obj->size);\r\n}\r\nseq_printf(m, "\n");\r\n}\r\nvoid omap_gem_describe_objects(struct list_head *list, struct seq_file *m)\r\n{\r\nstruct omap_gem_object *omap_obj;\r\nint count = 0;\r\nsize_t size = 0;\r\nlist_for_each_entry(omap_obj, list, mm_list) {\r\nstruct drm_gem_object *obj = &omap_obj->base;\r\nseq_printf(m, " ");\r\nomap_gem_describe(obj, m);\r\ncount++;\r\nsize += obj->size;\r\n}\r\nseq_printf(m, "Total %d objects, %zu bytes\n", count, size);\r\n}\r\nstatic inline bool is_waiting(struct omap_gem_sync_waiter *waiter)\r\n{\r\nstruct omap_gem_object *omap_obj = waiter->omap_obj;\r\nif ((waiter->op & OMAP_GEM_READ) &&\r\n(omap_obj->sync->write_complete < waiter->write_target))\r\nreturn true;\r\nif ((waiter->op & OMAP_GEM_WRITE) &&\r\n(omap_obj->sync->read_complete < waiter->read_target))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void sync_op_update(void)\r\n{\r\nstruct omap_gem_sync_waiter *waiter, *n;\r\nlist_for_each_entry_safe(waiter, n, &waiters, list) {\r\nif (!is_waiting(waiter)) {\r\nlist_del(&waiter->list);\r\nSYNC("notify: %p", waiter);\r\nwaiter->notify(waiter->arg);\r\nkfree(waiter);\r\n}\r\n}\r\n}\r\nstatic inline int sync_op(struct drm_gem_object *obj,\r\nenum omap_gem_op op, bool start)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = 0;\r\nspin_lock(&sync_lock);\r\nif (!omap_obj->sync) {\r\nomap_obj->sync = kzalloc(sizeof(*omap_obj->sync), GFP_ATOMIC);\r\nif (!omap_obj->sync) {\r\nret = -ENOMEM;\r\ngoto unlock;\r\n}\r\n}\r\nif (start) {\r\nif (op & OMAP_GEM_READ)\r\nomap_obj->sync->read_pending++;\r\nif (op & OMAP_GEM_WRITE)\r\nomap_obj->sync->write_pending++;\r\n} else {\r\nif (op & OMAP_GEM_READ)\r\nomap_obj->sync->read_complete++;\r\nif (op & OMAP_GEM_WRITE)\r\nomap_obj->sync->write_complete++;\r\nsync_op_update();\r\n}\r\nunlock:\r\nspin_unlock(&sync_lock);\r\nreturn ret;\r\n}\r\nvoid omap_gem_op_update(void)\r\n{\r\nspin_lock(&sync_lock);\r\nsync_op_update();\r\nspin_unlock(&sync_lock);\r\n}\r\nint omap_gem_op_start(struct drm_gem_object *obj, enum omap_gem_op op)\r\n{\r\nreturn sync_op(obj, op, true);\r\n}\r\nint omap_gem_op_finish(struct drm_gem_object *obj, enum omap_gem_op op)\r\n{\r\nreturn sync_op(obj, op, false);\r\n}\r\nstatic void sync_notify(void *arg)\r\n{\r\nstruct task_struct **waiter_task = arg;\r\n*waiter_task = NULL;\r\nwake_up_all(&sync_event);\r\n}\r\nint omap_gem_op_sync(struct drm_gem_object *obj, enum omap_gem_op op)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = 0;\r\nif (omap_obj->sync) {\r\nstruct task_struct *waiter_task = current;\r\nstruct omap_gem_sync_waiter *waiter =\r\nkzalloc(sizeof(*waiter), GFP_KERNEL);\r\nif (!waiter)\r\nreturn -ENOMEM;\r\nwaiter->omap_obj = omap_obj;\r\nwaiter->op = op;\r\nwaiter->read_target = omap_obj->sync->read_pending;\r\nwaiter->write_target = omap_obj->sync->write_pending;\r\nwaiter->notify = sync_notify;\r\nwaiter->arg = &waiter_task;\r\nspin_lock(&sync_lock);\r\nif (is_waiting(waiter)) {\r\nSYNC("waited: %p", waiter);\r\nlist_add_tail(&waiter->list, &waiters);\r\nspin_unlock(&sync_lock);\r\nret = wait_event_interruptible(sync_event,\r\n(waiter_task == NULL));\r\nspin_lock(&sync_lock);\r\nif (waiter_task) {\r\nSYNC("interrupted: %p", waiter);\r\nlist_del(&waiter->list);\r\nwaiter_task = NULL;\r\n} else {\r\nwaiter = NULL;\r\n}\r\n}\r\nspin_unlock(&sync_lock);\r\nif (waiter)\r\nkfree(waiter);\r\n}\r\nreturn ret;\r\n}\r\nint omap_gem_op_async(struct drm_gem_object *obj, enum omap_gem_op op,\r\nvoid (*fxn)(void *arg), void *arg)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nif (omap_obj->sync) {\r\nstruct omap_gem_sync_waiter *waiter =\r\nkzalloc(sizeof(*waiter), GFP_ATOMIC);\r\nif (!waiter)\r\nreturn -ENOMEM;\r\nwaiter->omap_obj = omap_obj;\r\nwaiter->op = op;\r\nwaiter->read_target = omap_obj->sync->read_pending;\r\nwaiter->write_target = omap_obj->sync->write_pending;\r\nwaiter->notify = fxn;\r\nwaiter->arg = arg;\r\nspin_lock(&sync_lock);\r\nif (is_waiting(waiter)) {\r\nSYNC("waited: %p", waiter);\r\nlist_add_tail(&waiter->list, &waiters);\r\nspin_unlock(&sync_lock);\r\nreturn 0;\r\n}\r\nspin_unlock(&sync_lock);\r\nkfree(waiter);\r\n}\r\nfxn(arg);\r\nreturn 0;\r\n}\r\nint omap_gem_set_sync_object(struct drm_gem_object *obj, void *syncobj)\r\n{\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nint ret = 0;\r\nspin_lock(&sync_lock);\r\nif ((omap_obj->flags & OMAP_BO_EXT_SYNC) && !syncobj) {\r\nsyncobj = kmemdup(omap_obj->sync, sizeof(*omap_obj->sync),\r\nGFP_ATOMIC);\r\nif (!syncobj) {\r\nret = -ENOMEM;\r\ngoto unlock;\r\n}\r\nomap_obj->flags &= ~OMAP_BO_EXT_SYNC;\r\nomap_obj->sync = syncobj;\r\n} else if (syncobj && !(omap_obj->flags & OMAP_BO_EXT_SYNC)) {\r\nif (omap_obj->sync) {\r\nmemcpy(syncobj, omap_obj->sync, sizeof(*omap_obj->sync));\r\nkfree(omap_obj->sync);\r\n}\r\nomap_obj->flags |= OMAP_BO_EXT_SYNC;\r\nomap_obj->sync = syncobj;\r\n}\r\nunlock:\r\nspin_unlock(&sync_lock);\r\nreturn ret;\r\n}\r\nvoid omap_gem_free_object(struct drm_gem_object *obj)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nstruct omap_gem_object *omap_obj = to_omap_bo(obj);\r\nevict(obj);\r\nWARN_ON(!mutex_is_locked(&dev->struct_mutex));\r\nlist_del(&omap_obj->mm_list);\r\ndrm_gem_free_mmap_offset(obj);\r\nWARN_ON(omap_obj->paddr_cnt > 0);\r\nif (!(omap_obj->flags & OMAP_BO_EXT_MEM)) {\r\nif (omap_obj->pages)\r\nomap_gem_detach_pages(obj);\r\nif (!is_shmem(obj)) {\r\ndma_free_writecombine(dev->dev, obj->size,\r\nomap_obj->vaddr, omap_obj->paddr);\r\n} else if (omap_obj->vaddr) {\r\nvunmap(omap_obj->vaddr);\r\n}\r\n}\r\nif (!(omap_obj->flags & OMAP_BO_EXT_SYNC))\r\nkfree(omap_obj->sync);\r\ndrm_gem_object_release(obj);\r\nkfree(obj);\r\n}\r\nint omap_gem_new_handle(struct drm_device *dev, struct drm_file *file,\r\nunion omap_gem_size gsize, uint32_t flags, uint32_t *handle)\r\n{\r\nstruct drm_gem_object *obj;\r\nint ret;\r\nobj = omap_gem_new(dev, gsize, flags);\r\nif (!obj)\r\nreturn -ENOMEM;\r\nret = drm_gem_handle_create(file, obj, handle);\r\nif (ret) {\r\ndrm_gem_object_release(obj);\r\nkfree(obj);\r\nreturn ret;\r\n}\r\ndrm_gem_object_unreference_unlocked(obj);\r\nreturn 0;\r\n}\r\nstruct drm_gem_object *omap_gem_new(struct drm_device *dev,\r\nunion omap_gem_size gsize, uint32_t flags)\r\n{\r\nstruct omap_drm_private *priv = dev->dev_private;\r\nstruct omap_gem_object *omap_obj;\r\nstruct drm_gem_object *obj = NULL;\r\nsize_t size;\r\nint ret;\r\nif (flags & OMAP_BO_TILED) {\r\nif (!usergart) {\r\ndev_err(dev->dev, "Tiled buffers require DMM\n");\r\ngoto fail;\r\n}\r\nflags &= ~OMAP_BO_SCANOUT;\r\nflags &= ~(OMAP_BO_CACHED|OMAP_BO_UNCACHED);\r\nflags |= OMAP_BO_WC;\r\ntiler_align(gem2fmt(flags),\r\n&gsize.tiled.width, &gsize.tiled.height);\r\nsize = tiler_size(gem2fmt(flags),\r\ngsize.tiled.width, gsize.tiled.height);\r\n} else {\r\nsize = PAGE_ALIGN(gsize.bytes);\r\n}\r\nomap_obj = kzalloc(sizeof(*omap_obj), GFP_KERNEL);\r\nif (!omap_obj)\r\ngoto fail;\r\nlist_add(&omap_obj->mm_list, &priv->obj_list);\r\nobj = &omap_obj->base;\r\nif ((flags & OMAP_BO_SCANOUT) && !priv->has_dmm) {\r\nomap_obj->vaddr = dma_alloc_writecombine(dev->dev, size,\r\n&omap_obj->paddr, GFP_KERNEL);\r\nif (omap_obj->vaddr)\r\nflags |= OMAP_BO_DMA;\r\n}\r\nomap_obj->flags = flags;\r\nif (flags & OMAP_BO_TILED) {\r\nomap_obj->width = gsize.tiled.width;\r\nomap_obj->height = gsize.tiled.height;\r\n}\r\nret = 0;\r\nif (flags & (OMAP_BO_DMA|OMAP_BO_EXT_MEM))\r\ndrm_gem_private_object_init(dev, obj, size);\r\nelse\r\nret = drm_gem_object_init(dev, obj, size);\r\nif (ret)\r\ngoto fail;\r\nreturn obj;\r\nfail:\r\nif (obj)\r\nomap_gem_free_object(obj);\r\nreturn NULL;\r\n}\r\nvoid omap_gem_init(struct drm_device *dev)\r\n{\r\nstruct omap_drm_private *priv = dev->dev_private;\r\nconst enum tiler_fmt fmts[] = {\r\nTILFMT_8BIT, TILFMT_16BIT, TILFMT_32BIT\r\n};\r\nint i, j;\r\nif (!dmm_is_available()) {\r\ndev_warn(dev->dev, "DMM not available, disable DMM support\n");\r\nreturn;\r\n}\r\nusergart = kcalloc(3, sizeof(*usergart), GFP_KERNEL);\r\nif (!usergart)\r\nreturn;\r\nfor (i = 0; i < ARRAY_SIZE(fmts); i++) {\r\nuint16_t h = 1, w = PAGE_SIZE >> i;\r\ntiler_align(fmts[i], &w, &h);\r\nusergart[i].height = h;\r\nusergart[i].height_shift = ilog2(h);\r\nusergart[i].stride_pfn = tiler_stride(fmts[i], 0) >> PAGE_SHIFT;\r\nusergart[i].slot_shift = ilog2((PAGE_SIZE / h) >> i);\r\nfor (j = 0; j < NUM_USERGART_ENTRIES; j++) {\r\nstruct usergart_entry *entry = &usergart[i].entry[j];\r\nstruct tiler_block *block =\r\ntiler_reserve_2d(fmts[i], w, h,\r\nPAGE_SIZE);\r\nif (IS_ERR(block)) {\r\ndev_err(dev->dev,\r\n"reserve failed: %d, %d, %ld\n",\r\ni, j, PTR_ERR(block));\r\nreturn;\r\n}\r\nentry->paddr = tiler_ssptr(block);\r\nentry->block = block;\r\nDBG("%d:%d: %dx%d: paddr=%08x stride=%d", i, j, w, h,\r\nentry->paddr,\r\nusergart[i].stride_pfn << PAGE_SHIFT);\r\n}\r\n}\r\npriv->has_dmm = true;\r\n}\r\nvoid omap_gem_deinit(struct drm_device *dev)\r\n{\r\nkfree(usergart);\r\n}
