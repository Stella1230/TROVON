static void ttm_pool_kobj_release(struct kobject *kobj)\r\n{\r\nstruct ttm_pool_manager *m =\r\ncontainer_of(kobj, struct ttm_pool_manager, kobj);\r\nkfree(m);\r\n}\r\nstatic ssize_t ttm_pool_store(struct kobject *kobj,\r\nstruct attribute *attr, const char *buffer, size_t size)\r\n{\r\nstruct ttm_pool_manager *m =\r\ncontainer_of(kobj, struct ttm_pool_manager, kobj);\r\nint chars;\r\nunsigned val;\r\nchars = sscanf(buffer, "%u", &val);\r\nif (chars == 0)\r\nreturn size;\r\nval = val / (PAGE_SIZE >> 10);\r\nif (attr == &ttm_page_pool_max)\r\nm->options.max_size = val;\r\nelse if (attr == &ttm_page_pool_small)\r\nm->options.small = val;\r\nelse if (attr == &ttm_page_pool_alloc_size) {\r\nif (val > NUM_PAGES_TO_ALLOC*8) {\r\npr_err("Setting allocation size to %lu is not allowed. Recommended size is %lu\n",\r\nNUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 7),\r\nNUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 10));\r\nreturn size;\r\n} else if (val > NUM_PAGES_TO_ALLOC) {\r\npr_warn("Setting allocation size to larger than %lu is not recommended\n",\r\nNUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 10));\r\n}\r\nm->options.alloc_size = val;\r\n}\r\nreturn size;\r\n}\r\nstatic ssize_t ttm_pool_show(struct kobject *kobj,\r\nstruct attribute *attr, char *buffer)\r\n{\r\nstruct ttm_pool_manager *m =\r\ncontainer_of(kobj, struct ttm_pool_manager, kobj);\r\nunsigned val = 0;\r\nif (attr == &ttm_page_pool_max)\r\nval = m->options.max_size;\r\nelse if (attr == &ttm_page_pool_small)\r\nval = m->options.small;\r\nelse if (attr == &ttm_page_pool_alloc_size)\r\nval = m->options.alloc_size;\r\nval = val * (PAGE_SIZE >> 10);\r\nreturn snprintf(buffer, PAGE_SIZE, "%u\n", val);\r\n}\r\nstatic int set_pages_array_wb(struct page **pages, int addrinarray)\r\n{\r\n#ifdef TTM_HAS_AGP\r\nint i;\r\nfor (i = 0; i < addrinarray; i++)\r\nunmap_page_from_agp(pages[i]);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int set_pages_array_wc(struct page **pages, int addrinarray)\r\n{\r\n#ifdef TTM_HAS_AGP\r\nint i;\r\nfor (i = 0; i < addrinarray; i++)\r\nmap_page_into_agp(pages[i]);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int set_pages_array_uc(struct page **pages, int addrinarray)\r\n{\r\n#ifdef TTM_HAS_AGP\r\nint i;\r\nfor (i = 0; i < addrinarray; i++)\r\nmap_page_into_agp(pages[i]);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic struct ttm_page_pool *ttm_get_pool(int flags,\r\nenum ttm_caching_state cstate)\r\n{\r\nint pool_index;\r\nif (cstate == tt_cached)\r\nreturn NULL;\r\nif (cstate == tt_wc)\r\npool_index = 0x0;\r\nelse\r\npool_index = 0x1;\r\nif (flags & TTM_PAGE_FLAG_DMA32)\r\npool_index |= 0x2;\r\nreturn &_manager->pools[pool_index];\r\n}\r\nstatic void ttm_pages_put(struct page *pages[], unsigned npages)\r\n{\r\nunsigned i;\r\nif (set_pages_array_wb(pages, npages))\r\npr_err("Failed to set %d pages to wb!\n", npages);\r\nfor (i = 0; i < npages; ++i)\r\n__free_page(pages[i]);\r\n}\r\nstatic void ttm_pool_update_free_locked(struct ttm_page_pool *pool,\r\nunsigned freed_pages)\r\n{\r\npool->npages -= freed_pages;\r\npool->nfrees += freed_pages;\r\n}\r\nstatic int ttm_page_pool_free(struct ttm_page_pool *pool, unsigned nr_free)\r\n{\r\nunsigned long irq_flags;\r\nstruct page *p;\r\nstruct page **pages_to_free;\r\nunsigned freed_pages = 0,\r\nnpages_to_free = nr_free;\r\nif (NUM_PAGES_TO_ALLOC < nr_free)\r\nnpages_to_free = NUM_PAGES_TO_ALLOC;\r\npages_to_free = kmalloc(npages_to_free * sizeof(struct page *),\r\nGFP_KERNEL);\r\nif (!pages_to_free) {\r\npr_err("Failed to allocate memory for pool free operation\n");\r\nreturn 0;\r\n}\r\nrestart:\r\nspin_lock_irqsave(&pool->lock, irq_flags);\r\nlist_for_each_entry_reverse(p, &pool->list, lru) {\r\nif (freed_pages >= npages_to_free)\r\nbreak;\r\npages_to_free[freed_pages++] = p;\r\nif (freed_pages >= NUM_PAGES_TO_ALLOC) {\r\n__list_del(p->lru.prev, &pool->list);\r\nttm_pool_update_free_locked(pool, freed_pages);\r\nspin_unlock_irqrestore(&pool->lock, irq_flags);\r\nttm_pages_put(pages_to_free, freed_pages);\r\nif (likely(nr_free != FREE_ALL_PAGES))\r\nnr_free -= freed_pages;\r\nif (NUM_PAGES_TO_ALLOC >= nr_free)\r\nnpages_to_free = nr_free;\r\nelse\r\nnpages_to_free = NUM_PAGES_TO_ALLOC;\r\nfreed_pages = 0;\r\nif (nr_free)\r\ngoto restart;\r\ngoto out;\r\n}\r\n}\r\nif (freed_pages) {\r\n__list_del(&p->lru, &pool->list);\r\nttm_pool_update_free_locked(pool, freed_pages);\r\nnr_free -= freed_pages;\r\n}\r\nspin_unlock_irqrestore(&pool->lock, irq_flags);\r\nif (freed_pages)\r\nttm_pages_put(pages_to_free, freed_pages);\r\nout:\r\nkfree(pages_to_free);\r\nreturn nr_free;\r\n}\r\nstatic unsigned long\r\nttm_pool_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nstatic atomic_t start_pool = ATOMIC_INIT(0);\r\nunsigned i;\r\nunsigned pool_offset = atomic_add_return(1, &start_pool);\r\nstruct ttm_page_pool *pool;\r\nint shrink_pages = sc->nr_to_scan;\r\nunsigned long freed = 0;\r\npool_offset = pool_offset % NUM_POOLS;\r\nfor (i = 0; i < NUM_POOLS; ++i) {\r\nunsigned nr_free = shrink_pages;\r\nif (shrink_pages == 0)\r\nbreak;\r\npool = &_manager->pools[(i + pool_offset)%NUM_POOLS];\r\nshrink_pages = ttm_page_pool_free(pool, nr_free);\r\nfreed += nr_free - shrink_pages;\r\n}\r\nreturn freed;\r\n}\r\nstatic unsigned long\r\nttm_pool_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nunsigned i;\r\nunsigned long count = 0;\r\nfor (i = 0; i < NUM_POOLS; ++i)\r\ncount += _manager->pools[i].npages;\r\nreturn count;\r\n}\r\nstatic void ttm_pool_mm_shrink_init(struct ttm_pool_manager *manager)\r\n{\r\nmanager->mm_shrink.count_objects = ttm_pool_shrink_count;\r\nmanager->mm_shrink.scan_objects = ttm_pool_shrink_scan;\r\nmanager->mm_shrink.seeks = 1;\r\nregister_shrinker(&manager->mm_shrink);\r\n}\r\nstatic void ttm_pool_mm_shrink_fini(struct ttm_pool_manager *manager)\r\n{\r\nunregister_shrinker(&manager->mm_shrink);\r\n}\r\nstatic int ttm_set_pages_caching(struct page **pages,\r\nenum ttm_caching_state cstate, unsigned cpages)\r\n{\r\nint r = 0;\r\nswitch (cstate) {\r\ncase tt_uncached:\r\nr = set_pages_array_uc(pages, cpages);\r\nif (r)\r\npr_err("Failed to set %d pages to uc!\n", cpages);\r\nbreak;\r\ncase tt_wc:\r\nr = set_pages_array_wc(pages, cpages);\r\nif (r)\r\npr_err("Failed to set %d pages to wc!\n", cpages);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstatic void ttm_handle_caching_state_failure(struct list_head *pages,\r\nint ttm_flags, enum ttm_caching_state cstate,\r\nstruct page **failed_pages, unsigned cpages)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < cpages; ++i) {\r\nlist_del(&failed_pages[i]->lru);\r\n__free_page(failed_pages[i]);\r\n}\r\n}\r\nstatic int ttm_alloc_new_pages(struct list_head *pages, gfp_t gfp_flags,\r\nint ttm_flags, enum ttm_caching_state cstate, unsigned count)\r\n{\r\nstruct page **caching_array;\r\nstruct page *p;\r\nint r = 0;\r\nunsigned i, cpages;\r\nunsigned max_cpages = min(count,\r\n(unsigned)(PAGE_SIZE/sizeof(struct page *)));\r\ncaching_array = kmalloc(max_cpages*sizeof(struct page *), GFP_KERNEL);\r\nif (!caching_array) {\r\npr_err("Unable to allocate table for new pages\n");\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0, cpages = 0; i < count; ++i) {\r\np = alloc_page(gfp_flags);\r\nif (!p) {\r\npr_err("Unable to get page %u\n", i);\r\nif (cpages) {\r\nr = ttm_set_pages_caching(caching_array,\r\ncstate, cpages);\r\nif (r)\r\nttm_handle_caching_state_failure(pages,\r\nttm_flags, cstate,\r\ncaching_array, cpages);\r\n}\r\nr = -ENOMEM;\r\ngoto out;\r\n}\r\n#ifdef CONFIG_HIGHMEM\r\nif (!PageHighMem(p))\r\n#endif\r\n{\r\ncaching_array[cpages++] = p;\r\nif (cpages == max_cpages) {\r\nr = ttm_set_pages_caching(caching_array,\r\ncstate, cpages);\r\nif (r) {\r\nttm_handle_caching_state_failure(pages,\r\nttm_flags, cstate,\r\ncaching_array, cpages);\r\ngoto out;\r\n}\r\ncpages = 0;\r\n}\r\n}\r\nlist_add(&p->lru, pages);\r\n}\r\nif (cpages) {\r\nr = ttm_set_pages_caching(caching_array, cstate, cpages);\r\nif (r)\r\nttm_handle_caching_state_failure(pages,\r\nttm_flags, cstate,\r\ncaching_array, cpages);\r\n}\r\nout:\r\nkfree(caching_array);\r\nreturn r;\r\n}\r\nstatic void ttm_page_pool_fill_locked(struct ttm_page_pool *pool,\r\nint ttm_flags, enum ttm_caching_state cstate, unsigned count,\r\nunsigned long *irq_flags)\r\n{\r\nstruct page *p;\r\nint r;\r\nunsigned cpages = 0;\r\nif (pool->fill_lock)\r\nreturn;\r\npool->fill_lock = true;\r\nif (count < _manager->options.small\r\n&& count > pool->npages) {\r\nstruct list_head new_pages;\r\nunsigned alloc_size = _manager->options.alloc_size;\r\nspin_unlock_irqrestore(&pool->lock, *irq_flags);\r\nINIT_LIST_HEAD(&new_pages);\r\nr = ttm_alloc_new_pages(&new_pages, pool->gfp_flags, ttm_flags,\r\ncstate, alloc_size);\r\nspin_lock_irqsave(&pool->lock, *irq_flags);\r\nif (!r) {\r\nlist_splice(&new_pages, &pool->list);\r\n++pool->nrefills;\r\npool->npages += alloc_size;\r\n} else {\r\npr_err("Failed to fill pool (%p)\n", pool);\r\nlist_for_each_entry(p, &pool->list, lru) {\r\n++cpages;\r\n}\r\nlist_splice(&new_pages, &pool->list);\r\npool->npages += cpages;\r\n}\r\n}\r\npool->fill_lock = false;\r\n}\r\nstatic unsigned ttm_page_pool_get_pages(struct ttm_page_pool *pool,\r\nstruct list_head *pages,\r\nint ttm_flags,\r\nenum ttm_caching_state cstate,\r\nunsigned count)\r\n{\r\nunsigned long irq_flags;\r\nstruct list_head *p;\r\nunsigned i;\r\nspin_lock_irqsave(&pool->lock, irq_flags);\r\nttm_page_pool_fill_locked(pool, ttm_flags, cstate, count, &irq_flags);\r\nif (count >= pool->npages) {\r\nlist_splice_init(&pool->list, pages);\r\ncount -= pool->npages;\r\npool->npages = 0;\r\ngoto out;\r\n}\r\nif (count <= pool->npages/2) {\r\ni = 0;\r\nlist_for_each(p, &pool->list) {\r\nif (++i == count)\r\nbreak;\r\n}\r\n} else {\r\ni = pool->npages + 1;\r\nlist_for_each_prev(p, &pool->list) {\r\nif (--i == count)\r\nbreak;\r\n}\r\n}\r\nlist_cut_position(pages, &pool->list, p);\r\npool->npages -= count;\r\ncount = 0;\r\nout:\r\nspin_unlock_irqrestore(&pool->lock, irq_flags);\r\nreturn count;\r\n}\r\nstatic void ttm_put_pages(struct page **pages, unsigned npages, int flags,\r\nenum ttm_caching_state cstate)\r\n{\r\nunsigned long irq_flags;\r\nstruct ttm_page_pool *pool = ttm_get_pool(flags, cstate);\r\nunsigned i;\r\nif (pool == NULL) {\r\nfor (i = 0; i < npages; i++) {\r\nif (pages[i]) {\r\nif (page_count(pages[i]) != 1)\r\npr_err("Erroneous page count. Leaking pages.\n");\r\n__free_page(pages[i]);\r\npages[i] = NULL;\r\n}\r\n}\r\nreturn;\r\n}\r\nspin_lock_irqsave(&pool->lock, irq_flags);\r\nfor (i = 0; i < npages; i++) {\r\nif (pages[i]) {\r\nif (page_count(pages[i]) != 1)\r\npr_err("Erroneous page count. Leaking pages.\n");\r\nlist_add_tail(&pages[i]->lru, &pool->list);\r\npages[i] = NULL;\r\npool->npages++;\r\n}\r\n}\r\nnpages = 0;\r\nif (pool->npages > _manager->options.max_size) {\r\nnpages = pool->npages - _manager->options.max_size;\r\nif (npages < NUM_PAGES_TO_ALLOC)\r\nnpages = NUM_PAGES_TO_ALLOC;\r\n}\r\nspin_unlock_irqrestore(&pool->lock, irq_flags);\r\nif (npages)\r\nttm_page_pool_free(pool, npages);\r\n}\r\nstatic int ttm_get_pages(struct page **pages, unsigned npages, int flags,\r\nenum ttm_caching_state cstate)\r\n{\r\nstruct ttm_page_pool *pool = ttm_get_pool(flags, cstate);\r\nstruct list_head plist;\r\nstruct page *p = NULL;\r\ngfp_t gfp_flags = GFP_USER;\r\nunsigned count;\r\nint r;\r\nif (flags & TTM_PAGE_FLAG_ZERO_ALLOC)\r\ngfp_flags |= __GFP_ZERO;\r\nif (pool == NULL) {\r\nif (flags & TTM_PAGE_FLAG_DMA32)\r\ngfp_flags |= GFP_DMA32;\r\nelse\r\ngfp_flags |= GFP_HIGHUSER;\r\nfor (r = 0; r < npages; ++r) {\r\np = alloc_page(gfp_flags);\r\nif (!p) {\r\npr_err("Unable to allocate page\n");\r\nreturn -ENOMEM;\r\n}\r\npages[r] = p;\r\n}\r\nreturn 0;\r\n}\r\ngfp_flags |= pool->gfp_flags;\r\nINIT_LIST_HEAD(&plist);\r\nnpages = ttm_page_pool_get_pages(pool, &plist, flags, cstate, npages);\r\ncount = 0;\r\nlist_for_each_entry(p, &plist, lru) {\r\npages[count++] = p;\r\n}\r\nif (flags & TTM_PAGE_FLAG_ZERO_ALLOC) {\r\nlist_for_each_entry(p, &plist, lru) {\r\nif (PageHighMem(p))\r\nclear_highpage(p);\r\nelse\r\nclear_page(page_address(p));\r\n}\r\n}\r\nif (npages > 0) {\r\nINIT_LIST_HEAD(&plist);\r\nr = ttm_alloc_new_pages(&plist, gfp_flags, flags, cstate, npages);\r\nlist_for_each_entry(p, &plist, lru) {\r\npages[count++] = p;\r\n}\r\nif (r) {\r\npr_err("Failed to allocate extra pages for large request\n");\r\nttm_put_pages(pages, count, flags, cstate);\r\nreturn r;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void ttm_page_pool_init_locked(struct ttm_page_pool *pool, int flags,\r\nchar *name)\r\n{\r\nspin_lock_init(&pool->lock);\r\npool->fill_lock = false;\r\nINIT_LIST_HEAD(&pool->list);\r\npool->npages = pool->nfrees = 0;\r\npool->gfp_flags = flags;\r\npool->name = name;\r\n}\r\nint ttm_page_alloc_init(struct ttm_mem_global *glob, unsigned max_pages)\r\n{\r\nint ret;\r\nWARN_ON(_manager);\r\npr_info("Initializing pool allocator\n");\r\n_manager = kzalloc(sizeof(*_manager), GFP_KERNEL);\r\nttm_page_pool_init_locked(&_manager->wc_pool, GFP_HIGHUSER, "wc");\r\nttm_page_pool_init_locked(&_manager->uc_pool, GFP_HIGHUSER, "uc");\r\nttm_page_pool_init_locked(&_manager->wc_pool_dma32,\r\nGFP_USER | GFP_DMA32, "wc dma");\r\nttm_page_pool_init_locked(&_manager->uc_pool_dma32,\r\nGFP_USER | GFP_DMA32, "uc dma");\r\n_manager->options.max_size = max_pages;\r\n_manager->options.small = SMALL_ALLOCATION;\r\n_manager->options.alloc_size = NUM_PAGES_TO_ALLOC;\r\nret = kobject_init_and_add(&_manager->kobj, &ttm_pool_kobj_type,\r\n&glob->kobj, "pool");\r\nif (unlikely(ret != 0)) {\r\nkobject_put(&_manager->kobj);\r\n_manager = NULL;\r\nreturn ret;\r\n}\r\nttm_pool_mm_shrink_init(_manager);\r\nreturn 0;\r\n}\r\nvoid ttm_page_alloc_fini(void)\r\n{\r\nint i;\r\npr_info("Finalizing pool allocator\n");\r\nttm_pool_mm_shrink_fini(_manager);\r\nfor (i = 0; i < NUM_POOLS; ++i)\r\nttm_page_pool_free(&_manager->pools[i], FREE_ALL_PAGES);\r\nkobject_put(&_manager->kobj);\r\n_manager = NULL;\r\n}\r\nint ttm_pool_populate(struct ttm_tt *ttm)\r\n{\r\nstruct ttm_mem_global *mem_glob = ttm->glob->mem_glob;\r\nunsigned i;\r\nint ret;\r\nif (ttm->state != tt_unpopulated)\r\nreturn 0;\r\nfor (i = 0; i < ttm->num_pages; ++i) {\r\nret = ttm_get_pages(&ttm->pages[i], 1,\r\nttm->page_flags,\r\nttm->caching_state);\r\nif (ret != 0) {\r\nttm_pool_unpopulate(ttm);\r\nreturn -ENOMEM;\r\n}\r\nret = ttm_mem_global_alloc_page(mem_glob, ttm->pages[i],\r\nfalse, false);\r\nif (unlikely(ret != 0)) {\r\nttm_pool_unpopulate(ttm);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nif (unlikely(ttm->page_flags & TTM_PAGE_FLAG_SWAPPED)) {\r\nret = ttm_tt_swapin(ttm);\r\nif (unlikely(ret != 0)) {\r\nttm_pool_unpopulate(ttm);\r\nreturn ret;\r\n}\r\n}\r\nttm->state = tt_unbound;\r\nreturn 0;\r\n}\r\nvoid ttm_pool_unpopulate(struct ttm_tt *ttm)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < ttm->num_pages; ++i) {\r\nif (ttm->pages[i]) {\r\nttm_mem_global_free_page(ttm->glob->mem_glob,\r\nttm->pages[i]);\r\nttm_put_pages(&ttm->pages[i], 1,\r\nttm->page_flags,\r\nttm->caching_state);\r\n}\r\n}\r\nttm->state = tt_unpopulated;\r\n}\r\nint ttm_page_alloc_debugfs(struct seq_file *m, void *data)\r\n{\r\nstruct ttm_page_pool *p;\r\nunsigned i;\r\nchar *h[] = {"pool", "refills", "pages freed", "size"};\r\nif (!_manager) {\r\nseq_printf(m, "No pool allocator running.\n");\r\nreturn 0;\r\n}\r\nseq_printf(m, "%6s %12s %13s %8s\n",\r\nh[0], h[1], h[2], h[3]);\r\nfor (i = 0; i < NUM_POOLS; ++i) {\r\np = &_manager->pools[i];\r\nseq_printf(m, "%6s %12ld %13ld %8d\n",\r\np->name, p->nrefills,\r\np->nfrees, p->npages);\r\n}\r\nreturn 0;\r\n}
