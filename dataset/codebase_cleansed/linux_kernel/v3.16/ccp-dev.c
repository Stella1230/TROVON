static inline struct ccp_device *ccp_get_device(void)\r\n{\r\nreturn ccp_dev;\r\n}\r\nstatic inline void ccp_add_device(struct ccp_device *ccp)\r\n{\r\nccp_dev = ccp;\r\n}\r\nstatic inline void ccp_del_device(struct ccp_device *ccp)\r\n{\r\nccp_dev = NULL;\r\n}\r\nint ccp_enqueue_cmd(struct ccp_cmd *cmd)\r\n{\r\nstruct ccp_device *ccp = ccp_get_device();\r\nunsigned long flags;\r\nunsigned int i;\r\nint ret;\r\nif (!ccp)\r\nreturn -ENODEV;\r\nif (!cmd->callback)\r\nreturn -EINVAL;\r\ncmd->ccp = ccp;\r\nspin_lock_irqsave(&ccp->cmd_lock, flags);\r\ni = ccp->cmd_q_count;\r\nif (ccp->cmd_count >= MAX_CMD_QLEN) {\r\nret = -EBUSY;\r\nif (cmd->flags & CCP_CMD_MAY_BACKLOG)\r\nlist_add_tail(&cmd->entry, &ccp->backlog);\r\n} else {\r\nret = -EINPROGRESS;\r\nccp->cmd_count++;\r\nlist_add_tail(&cmd->entry, &ccp->cmd);\r\nif (!ccp->suspending) {\r\nfor (i = 0; i < ccp->cmd_q_count; i++) {\r\nif (ccp->cmd_q[i].active)\r\ncontinue;\r\nbreak;\r\n}\r\n}\r\n}\r\nspin_unlock_irqrestore(&ccp->cmd_lock, flags);\r\nif (i < ccp->cmd_q_count)\r\nwake_up_process(ccp->cmd_q[i].kthread);\r\nreturn ret;\r\n}\r\nstatic void ccp_do_cmd_backlog(struct work_struct *work)\r\n{\r\nstruct ccp_cmd *cmd = container_of(work, struct ccp_cmd, work);\r\nstruct ccp_device *ccp = cmd->ccp;\r\nunsigned long flags;\r\nunsigned int i;\r\ncmd->callback(cmd->data, -EINPROGRESS);\r\nspin_lock_irqsave(&ccp->cmd_lock, flags);\r\nccp->cmd_count++;\r\nlist_add_tail(&cmd->entry, &ccp->cmd);\r\nfor (i = 0; i < ccp->cmd_q_count; i++) {\r\nif (ccp->cmd_q[i].active)\r\ncontinue;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&ccp->cmd_lock, flags);\r\nif (i < ccp->cmd_q_count)\r\nwake_up_process(ccp->cmd_q[i].kthread);\r\n}\r\nstatic struct ccp_cmd *ccp_dequeue_cmd(struct ccp_cmd_queue *cmd_q)\r\n{\r\nstruct ccp_device *ccp = cmd_q->ccp;\r\nstruct ccp_cmd *cmd = NULL;\r\nstruct ccp_cmd *backlog = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ccp->cmd_lock, flags);\r\ncmd_q->active = 0;\r\nif (ccp->suspending) {\r\ncmd_q->suspended = 1;\r\nspin_unlock_irqrestore(&ccp->cmd_lock, flags);\r\nwake_up_interruptible(&ccp->suspend_queue);\r\nreturn NULL;\r\n}\r\nif (ccp->cmd_count) {\r\ncmd_q->active = 1;\r\ncmd = list_first_entry(&ccp->cmd, struct ccp_cmd, entry);\r\nlist_del(&cmd->entry);\r\nccp->cmd_count--;\r\n}\r\nif (!list_empty(&ccp->backlog)) {\r\nbacklog = list_first_entry(&ccp->backlog, struct ccp_cmd,\r\nentry);\r\nlist_del(&backlog->entry);\r\n}\r\nspin_unlock_irqrestore(&ccp->cmd_lock, flags);\r\nif (backlog) {\r\nINIT_WORK(&backlog->work, ccp_do_cmd_backlog);\r\nschedule_work(&backlog->work);\r\n}\r\nreturn cmd;\r\n}\r\nstatic void ccp_do_cmd_complete(unsigned long data)\r\n{\r\nstruct ccp_tasklet_data *tdata = (struct ccp_tasklet_data *)data;\r\nstruct ccp_cmd *cmd = tdata->cmd;\r\ncmd->callback(cmd->data, cmd->ret);\r\ncomplete(&tdata->completion);\r\n}\r\nstatic int ccp_cmd_queue_thread(void *data)\r\n{\r\nstruct ccp_cmd_queue *cmd_q = (struct ccp_cmd_queue *)data;\r\nstruct ccp_cmd *cmd;\r\nstruct ccp_tasklet_data tdata;\r\nstruct tasklet_struct tasklet;\r\ntasklet_init(&tasklet, ccp_do_cmd_complete, (unsigned long)&tdata);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nwhile (!kthread_should_stop()) {\r\nschedule();\r\nset_current_state(TASK_INTERRUPTIBLE);\r\ncmd = ccp_dequeue_cmd(cmd_q);\r\nif (!cmd)\r\ncontinue;\r\n__set_current_state(TASK_RUNNING);\r\ncmd->ret = ccp_run_cmd(cmd_q, cmd);\r\ntdata.cmd = cmd;\r\ninit_completion(&tdata.completion);\r\ntasklet_schedule(&tasklet);\r\nwait_for_completion(&tdata.completion);\r\n}\r\n__set_current_state(TASK_RUNNING);\r\nreturn 0;\r\n}\r\nstatic int ccp_trng_read(struct hwrng *rng, void *data, size_t max, bool wait)\r\n{\r\nstruct ccp_device *ccp = container_of(rng, struct ccp_device, hwrng);\r\nu32 trng_value;\r\nint len = min_t(int, sizeof(trng_value), max);\r\ntrng_value = ioread32(ccp->io_regs + TRNG_OUT_REG);\r\nif (!trng_value) {\r\nif (ccp->hwrng_retries++ > TRNG_RETRIES)\r\nreturn -EIO;\r\nreturn 0;\r\n}\r\nccp->hwrng_retries = 0;\r\nmemcpy(data, &trng_value, len);\r\nreturn len;\r\n}\r\nstruct ccp_device *ccp_alloc_struct(struct device *dev)\r\n{\r\nstruct ccp_device *ccp;\r\nccp = kzalloc(sizeof(*ccp), GFP_KERNEL);\r\nif (ccp == NULL) {\r\ndev_err(dev, "unable to allocate device struct\n");\r\nreturn NULL;\r\n}\r\nccp->dev = dev;\r\nINIT_LIST_HEAD(&ccp->cmd);\r\nINIT_LIST_HEAD(&ccp->backlog);\r\nspin_lock_init(&ccp->cmd_lock);\r\nmutex_init(&ccp->req_mutex);\r\nmutex_init(&ccp->ksb_mutex);\r\nccp->ksb_count = KSB_COUNT;\r\nccp->ksb_start = 0;\r\nreturn ccp;\r\n}\r\nint ccp_init(struct ccp_device *ccp)\r\n{\r\nstruct device *dev = ccp->dev;\r\nstruct ccp_cmd_queue *cmd_q;\r\nstruct dma_pool *dma_pool;\r\nchar dma_pool_name[MAX_DMAPOOL_NAME_LEN];\r\nunsigned int qmr, qim, i;\r\nint ret;\r\nqim = 0;\r\nqmr = ioread32(ccp->io_regs + Q_MASK_REG);\r\nfor (i = 0; i < MAX_HW_QUEUES; i++) {\r\nif (!(qmr & (1 << i)))\r\ncontinue;\r\nsnprintf(dma_pool_name, sizeof(dma_pool_name), "ccp_q%d", i);\r\ndma_pool = dma_pool_create(dma_pool_name, dev,\r\nCCP_DMAPOOL_MAX_SIZE,\r\nCCP_DMAPOOL_ALIGN, 0);\r\nif (!dma_pool) {\r\ndev_err(dev, "unable to allocate dma pool\n");\r\nret = -ENOMEM;\r\ngoto e_pool;\r\n}\r\ncmd_q = &ccp->cmd_q[ccp->cmd_q_count];\r\nccp->cmd_q_count++;\r\ncmd_q->ccp = ccp;\r\ncmd_q->id = i;\r\ncmd_q->dma_pool = dma_pool;\r\ncmd_q->ksb_key = KSB_START + ccp->ksb_start++;\r\ncmd_q->ksb_ctx = KSB_START + ccp->ksb_start++;\r\nccp->ksb_count -= 2;\r\ncmd_q->reg_status = ccp->io_regs + CMD_Q_STATUS_BASE +\r\n(CMD_Q_STATUS_INCR * i);\r\ncmd_q->reg_int_status = ccp->io_regs + CMD_Q_INT_STATUS_BASE +\r\n(CMD_Q_STATUS_INCR * i);\r\ncmd_q->int_ok = 1 << (i * 2);\r\ncmd_q->int_err = 1 << ((i * 2) + 1);\r\ncmd_q->free_slots = CMD_Q_DEPTH(ioread32(cmd_q->reg_status));\r\ninit_waitqueue_head(&cmd_q->int_queue);\r\nqim |= cmd_q->int_ok | cmd_q->int_err;\r\ndev_dbg(dev, "queue #%u available\n", i);\r\n}\r\nif (ccp->cmd_q_count == 0) {\r\ndev_notice(dev, "no command queues available\n");\r\nret = -EIO;\r\ngoto e_pool;\r\n}\r\ndev_notice(dev, "%u command queues available\n", ccp->cmd_q_count);\r\niowrite32(0x00, ccp->io_regs + IRQ_MASK_REG);\r\nfor (i = 0; i < ccp->cmd_q_count; i++) {\r\ncmd_q = &ccp->cmd_q[i];\r\nioread32(cmd_q->reg_int_status);\r\nioread32(cmd_q->reg_status);\r\n}\r\niowrite32(qim, ccp->io_regs + IRQ_STATUS_REG);\r\nret = ccp->get_irq(ccp);\r\nif (ret) {\r\ndev_err(dev, "unable to allocate an IRQ\n");\r\ngoto e_pool;\r\n}\r\ninit_waitqueue_head(&ccp->ksb_queue);\r\ninit_waitqueue_head(&ccp->suspend_queue);\r\nfor (i = 0; i < ccp->cmd_q_count; i++) {\r\nstruct task_struct *kthread;\r\ncmd_q = &ccp->cmd_q[i];\r\nkthread = kthread_create(ccp_cmd_queue_thread, cmd_q,\r\n"ccp-q%u", cmd_q->id);\r\nif (IS_ERR(kthread)) {\r\ndev_err(dev, "error creating queue thread (%ld)\n",\r\nPTR_ERR(kthread));\r\nret = PTR_ERR(kthread);\r\ngoto e_kthread;\r\n}\r\ncmd_q->kthread = kthread;\r\nwake_up_process(kthread);\r\n}\r\nccp->hwrng.name = "ccp-rng";\r\nccp->hwrng.read = ccp_trng_read;\r\nret = hwrng_register(&ccp->hwrng);\r\nif (ret) {\r\ndev_err(dev, "error registering hwrng (%d)\n", ret);\r\ngoto e_kthread;\r\n}\r\nccp_add_device(ccp);\r\niowrite32(qim, ccp->io_regs + IRQ_MASK_REG);\r\nreturn 0;\r\ne_kthread:\r\nfor (i = 0; i < ccp->cmd_q_count; i++)\r\nif (ccp->cmd_q[i].kthread)\r\nkthread_stop(ccp->cmd_q[i].kthread);\r\nccp->free_irq(ccp);\r\ne_pool:\r\nfor (i = 0; i < ccp->cmd_q_count; i++)\r\ndma_pool_destroy(ccp->cmd_q[i].dma_pool);\r\nreturn ret;\r\n}\r\nvoid ccp_destroy(struct ccp_device *ccp)\r\n{\r\nstruct ccp_cmd_queue *cmd_q;\r\nstruct ccp_cmd *cmd;\r\nunsigned int qim, i;\r\nccp_del_device(ccp);\r\nhwrng_unregister(&ccp->hwrng);\r\nfor (i = 0; i < ccp->cmd_q_count; i++)\r\nif (ccp->cmd_q[i].kthread)\r\nkthread_stop(ccp->cmd_q[i].kthread);\r\nqim = 0;\r\nfor (i = 0; i < ccp->cmd_q_count; i++) {\r\ncmd_q = &ccp->cmd_q[i];\r\nqim |= cmd_q->int_ok | cmd_q->int_err;\r\n}\r\niowrite32(0x00, ccp->io_regs + IRQ_MASK_REG);\r\nfor (i = 0; i < ccp->cmd_q_count; i++) {\r\ncmd_q = &ccp->cmd_q[i];\r\nioread32(cmd_q->reg_int_status);\r\nioread32(cmd_q->reg_status);\r\n}\r\niowrite32(qim, ccp->io_regs + IRQ_STATUS_REG);\r\nccp->free_irq(ccp);\r\nfor (i = 0; i < ccp->cmd_q_count; i++)\r\ndma_pool_destroy(ccp->cmd_q[i].dma_pool);\r\nwhile (!list_empty(&ccp->cmd)) {\r\ncmd = list_first_entry(&ccp->cmd, struct ccp_cmd, entry);\r\nlist_del(&cmd->entry);\r\ncmd->callback(cmd->data, -ENODEV);\r\n}\r\nwhile (!list_empty(&ccp->backlog)) {\r\ncmd = list_first_entry(&ccp->backlog, struct ccp_cmd, entry);\r\nlist_del(&cmd->entry);\r\ncmd->callback(cmd->data, -ENODEV);\r\n}\r\n}\r\nirqreturn_t ccp_irq_handler(int irq, void *data)\r\n{\r\nstruct device *dev = data;\r\nstruct ccp_device *ccp = dev_get_drvdata(dev);\r\nstruct ccp_cmd_queue *cmd_q;\r\nu32 q_int, status;\r\nunsigned int i;\r\nstatus = ioread32(ccp->io_regs + IRQ_STATUS_REG);\r\nfor (i = 0; i < ccp->cmd_q_count; i++) {\r\ncmd_q = &ccp->cmd_q[i];\r\nq_int = status & (cmd_q->int_ok | cmd_q->int_err);\r\nif (q_int) {\r\ncmd_q->int_status = status;\r\ncmd_q->q_status = ioread32(cmd_q->reg_status);\r\ncmd_q->q_int_status = ioread32(cmd_q->reg_int_status);\r\nif ((q_int & cmd_q->int_err) && !cmd_q->cmd_error)\r\ncmd_q->cmd_error = CMD_Q_ERROR(cmd_q->q_status);\r\ncmd_q->int_rcvd = 1;\r\niowrite32(q_int, ccp->io_regs + IRQ_STATUS_REG);\r\nwake_up_interruptible(&cmd_q->int_queue);\r\n}\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nbool ccp_queues_suspended(struct ccp_device *ccp)\r\n{\r\nunsigned int suspended = 0;\r\nunsigned long flags;\r\nunsigned int i;\r\nspin_lock_irqsave(&ccp->cmd_lock, flags);\r\nfor (i = 0; i < ccp->cmd_q_count; i++)\r\nif (ccp->cmd_q[i].suspended)\r\nsuspended++;\r\nspin_unlock_irqrestore(&ccp->cmd_lock, flags);\r\nreturn ccp->cmd_q_count == suspended;\r\n}\r\nstatic int __init ccp_mod_init(void)\r\n{\r\nstruct cpuinfo_x86 *cpuinfo = &boot_cpu_data;\r\nint ret;\r\nif (!x86_match_cpu(ccp_support))\r\nreturn -ENODEV;\r\nswitch (cpuinfo->x86) {\r\ncase 22:\r\nif ((cpuinfo->x86_model < 48) || (cpuinfo->x86_model > 63))\r\nreturn -ENODEV;\r\nret = ccp_pci_init();\r\nif (ret)\r\nreturn ret;\r\nif (!ccp_get_device()) {\r\nccp_pci_exit();\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\nbreak;\r\n}\r\nreturn -ENODEV;\r\n}\r\nstatic void __exit ccp_mod_exit(void)\r\n{\r\nstruct cpuinfo_x86 *cpuinfo = &boot_cpu_data;\r\nswitch (cpuinfo->x86) {\r\ncase 22:\r\nccp_pci_exit();\r\nbreak;\r\n}\r\n}
