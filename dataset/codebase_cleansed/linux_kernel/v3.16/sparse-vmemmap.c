static void * __init_refok __earlyonly_bootmem_alloc(int node,\r\nunsigned long size,\r\nunsigned long align,\r\nunsigned long goal)\r\n{\r\nreturn memblock_virt_alloc_try_nid(size, align, goal,\r\nBOOTMEM_ALLOC_ACCESSIBLE, node);\r\n}\r\nvoid * __meminit vmemmap_alloc_block(unsigned long size, int node)\r\n{\r\nif (slab_is_available()) {\r\nstruct page *page;\r\nif (node_state(node, N_HIGH_MEMORY))\r\npage = alloc_pages_node(\r\nnode, GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,\r\nget_order(size));\r\nelse\r\npage = alloc_pages(\r\nGFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,\r\nget_order(size));\r\nif (page)\r\nreturn page_address(page);\r\nreturn NULL;\r\n} else\r\nreturn __earlyonly_bootmem_alloc(node, size, size,\r\n__pa(MAX_DMA_ADDRESS));\r\n}\r\nvoid * __meminit vmemmap_alloc_block_buf(unsigned long size, int node)\r\n{\r\nvoid *ptr;\r\nif (!vmemmap_buf)\r\nreturn vmemmap_alloc_block(size, node);\r\nptr = (void *)ALIGN((unsigned long)vmemmap_buf, size);\r\nif (ptr + size > vmemmap_buf_end)\r\nreturn vmemmap_alloc_block(size, node);\r\nvmemmap_buf = ptr + size;\r\nreturn ptr;\r\n}\r\nvoid __meminit vmemmap_verify(pte_t *pte, int node,\r\nunsigned long start, unsigned long end)\r\n{\r\nunsigned long pfn = pte_pfn(*pte);\r\nint actual_node = early_pfn_to_nid(pfn);\r\nif (node_distance(actual_node, node) > LOCAL_DISTANCE)\r\nprintk(KERN_WARNING "[%lx-%lx] potential offnode "\r\n"page_structs\n", start, end - 1);\r\n}\r\npte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node)\r\n{\r\npte_t *pte = pte_offset_kernel(pmd, addr);\r\nif (pte_none(*pte)) {\r\npte_t entry;\r\nvoid *p = vmemmap_alloc_block_buf(PAGE_SIZE, node);\r\nif (!p)\r\nreturn NULL;\r\nentry = pfn_pte(__pa(p) >> PAGE_SHIFT, PAGE_KERNEL);\r\nset_pte_at(&init_mm, addr, pte, entry);\r\n}\r\nreturn pte;\r\n}\r\npmd_t * __meminit vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node)\r\n{\r\npmd_t *pmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd)) {\r\nvoid *p = vmemmap_alloc_block(PAGE_SIZE, node);\r\nif (!p)\r\nreturn NULL;\r\npmd_populate_kernel(&init_mm, pmd, p);\r\n}\r\nreturn pmd;\r\n}\r\npud_t * __meminit vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node)\r\n{\r\npud_t *pud = pud_offset(pgd, addr);\r\nif (pud_none(*pud)) {\r\nvoid *p = vmemmap_alloc_block(PAGE_SIZE, node);\r\nif (!p)\r\nreturn NULL;\r\npud_populate(&init_mm, pud, p);\r\n}\r\nreturn pud;\r\n}\r\npgd_t * __meminit vmemmap_pgd_populate(unsigned long addr, int node)\r\n{\r\npgd_t *pgd = pgd_offset_k(addr);\r\nif (pgd_none(*pgd)) {\r\nvoid *p = vmemmap_alloc_block(PAGE_SIZE, node);\r\nif (!p)\r\nreturn NULL;\r\npgd_populate(&init_mm, pgd, p);\r\n}\r\nreturn pgd;\r\n}\r\nint __meminit vmemmap_populate_basepages(unsigned long start,\r\nunsigned long end, int node)\r\n{\r\nunsigned long addr = start;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nfor (; addr < end; addr += PAGE_SIZE) {\r\npgd = vmemmap_pgd_populate(addr, node);\r\nif (!pgd)\r\nreturn -ENOMEM;\r\npud = vmemmap_pud_populate(pgd, addr, node);\r\nif (!pud)\r\nreturn -ENOMEM;\r\npmd = vmemmap_pmd_populate(pud, addr, node);\r\nif (!pmd)\r\nreturn -ENOMEM;\r\npte = vmemmap_pte_populate(pmd, addr, node);\r\nif (!pte)\r\nreturn -ENOMEM;\r\nvmemmap_verify(pte, node, addr, addr + PAGE_SIZE);\r\n}\r\nreturn 0;\r\n}\r\nstruct page * __meminit sparse_mem_map_populate(unsigned long pnum, int nid)\r\n{\r\nunsigned long start;\r\nunsigned long end;\r\nstruct page *map;\r\nmap = pfn_to_page(pnum * PAGES_PER_SECTION);\r\nstart = (unsigned long)map;\r\nend = (unsigned long)(map + PAGES_PER_SECTION);\r\nif (vmemmap_populate(start, end, nid))\r\nreturn NULL;\r\nreturn map;\r\n}\r\nvoid __init sparse_mem_maps_populate_node(struct page **map_map,\r\nunsigned long pnum_begin,\r\nunsigned long pnum_end,\r\nunsigned long map_count, int nodeid)\r\n{\r\nunsigned long pnum;\r\nunsigned long size = sizeof(struct page) * PAGES_PER_SECTION;\r\nvoid *vmemmap_buf_start;\r\nsize = ALIGN(size, PMD_SIZE);\r\nvmemmap_buf_start = __earlyonly_bootmem_alloc(nodeid, size * map_count,\r\nPMD_SIZE, __pa(MAX_DMA_ADDRESS));\r\nif (vmemmap_buf_start) {\r\nvmemmap_buf = vmemmap_buf_start;\r\nvmemmap_buf_end = vmemmap_buf_start + size * map_count;\r\n}\r\nfor (pnum = pnum_begin; pnum < pnum_end; pnum++) {\r\nstruct mem_section *ms;\r\nif (!present_section_nr(pnum))\r\ncontinue;\r\nmap_map[pnum] = sparse_mem_map_populate(pnum, nodeid);\r\nif (map_map[pnum])\r\ncontinue;\r\nms = __nr_to_section(pnum);\r\nprintk(KERN_ERR "%s: sparsemem memory map backing failed "\r\n"some memory will not be available.\n", __func__);\r\nms->section_mem_map = 0;\r\n}\r\nif (vmemmap_buf_start) {\r\nmemblock_free_early(__pa(vmemmap_buf),\r\nvmemmap_buf_end - vmemmap_buf);\r\nvmemmap_buf = NULL;\r\nvmemmap_buf_end = NULL;\r\n}\r\n}
