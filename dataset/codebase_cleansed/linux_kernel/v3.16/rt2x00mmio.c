int rt2x00mmio_regbusy_read(struct rt2x00_dev *rt2x00dev,\r\nconst unsigned int offset,\r\nconst struct rt2x00_field32 field,\r\nu32 *reg)\r\n{\r\nunsigned int i;\r\nif (!test_bit(DEVICE_STATE_PRESENT, &rt2x00dev->flags))\r\nreturn 0;\r\nfor (i = 0; i < REGISTER_BUSY_COUNT; i++) {\r\nrt2x00mmio_register_read(rt2x00dev, offset, reg);\r\nif (!rt2x00_get_field32(*reg, field))\r\nreturn 1;\r\nudelay(REGISTER_BUSY_DELAY);\r\n}\r\nprintk_once(KERN_ERR "%s() Indirect register access failed: "\r\n"offset=0x%.08x, value=0x%.08x\n", __func__, offset, *reg);\r\n*reg = ~0;\r\nreturn 0;\r\n}\r\nbool rt2x00mmio_rxdone(struct rt2x00_dev *rt2x00dev)\r\n{\r\nstruct data_queue *queue = rt2x00dev->rx;\r\nstruct queue_entry *entry;\r\nstruct queue_entry_priv_mmio *entry_priv;\r\nstruct skb_frame_desc *skbdesc;\r\nint max_rx = 16;\r\nwhile (--max_rx) {\r\nentry = rt2x00queue_get_entry(queue, Q_INDEX);\r\nentry_priv = entry->priv_data;\r\nif (rt2x00dev->ops->lib->get_entry_state(entry))\r\nbreak;\r\nskbdesc = get_skb_frame_desc(entry->skb);\r\nskbdesc->desc = entry_priv->desc;\r\nskbdesc->desc_len = entry->queue->desc_size;\r\nrt2x00lib_dmastart(entry);\r\nrt2x00lib_dmadone(entry);\r\nrt2x00lib_rxdone(entry, GFP_ATOMIC);\r\n}\r\nreturn !max_rx;\r\n}\r\nvoid rt2x00mmio_flush_queue(struct data_queue *queue, bool drop)\r\n{\r\nunsigned int i;\r\nfor (i = 0; !rt2x00queue_empty(queue) && i < 10; i++)\r\nmsleep(10);\r\n}\r\nstatic int rt2x00mmio_alloc_queue_dma(struct rt2x00_dev *rt2x00dev,\r\nstruct data_queue *queue)\r\n{\r\nstruct queue_entry_priv_mmio *entry_priv;\r\nvoid *addr;\r\ndma_addr_t dma;\r\nunsigned int i;\r\naddr = dma_alloc_coherent(rt2x00dev->dev,\r\nqueue->limit * queue->desc_size,\r\n&dma, GFP_KERNEL);\r\nif (!addr)\r\nreturn -ENOMEM;\r\nmemset(addr, 0, queue->limit * queue->desc_size);\r\nfor (i = 0; i < queue->limit; i++) {\r\nentry_priv = queue->entries[i].priv_data;\r\nentry_priv->desc = addr + i * queue->desc_size;\r\nentry_priv->desc_dma = dma + i * queue->desc_size;\r\n}\r\nreturn 0;\r\n}\r\nstatic void rt2x00mmio_free_queue_dma(struct rt2x00_dev *rt2x00dev,\r\nstruct data_queue *queue)\r\n{\r\nstruct queue_entry_priv_mmio *entry_priv =\r\nqueue->entries[0].priv_data;\r\nif (entry_priv->desc)\r\ndma_free_coherent(rt2x00dev->dev,\r\nqueue->limit * queue->desc_size,\r\nentry_priv->desc, entry_priv->desc_dma);\r\nentry_priv->desc = NULL;\r\n}\r\nint rt2x00mmio_initialize(struct rt2x00_dev *rt2x00dev)\r\n{\r\nstruct data_queue *queue;\r\nint status;\r\nqueue_for_each(rt2x00dev, queue) {\r\nstatus = rt2x00mmio_alloc_queue_dma(rt2x00dev, queue);\r\nif (status)\r\ngoto exit;\r\n}\r\nstatus = request_irq(rt2x00dev->irq,\r\nrt2x00dev->ops->lib->irq_handler,\r\nIRQF_SHARED, rt2x00dev->name, rt2x00dev);\r\nif (status) {\r\nrt2x00_err(rt2x00dev, "IRQ %d allocation failed (error %d)\n",\r\nrt2x00dev->irq, status);\r\ngoto exit;\r\n}\r\nreturn 0;\r\nexit:\r\nqueue_for_each(rt2x00dev, queue)\r\nrt2x00mmio_free_queue_dma(rt2x00dev, queue);\r\nreturn status;\r\n}\r\nvoid rt2x00mmio_uninitialize(struct rt2x00_dev *rt2x00dev)\r\n{\r\nstruct data_queue *queue;\r\nfree_irq(rt2x00dev->irq, rt2x00dev);\r\nqueue_for_each(rt2x00dev, queue)\r\nrt2x00mmio_free_queue_dma(rt2x00dev, queue);\r\n}
