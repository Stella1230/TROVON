void rcu_enter_nohz(void)\r\n{\r\nif (--rcu_dynticks_nesting == 0)\r\nrcu_sched_qs(0);\r\n}\r\nvoid rcu_exit_nohz(void)\r\n{\r\nrcu_dynticks_nesting++;\r\n}\r\nstatic int rcu_qsctr_help(struct rcu_ctrlblk *rcp)\r\n{\r\nif (rcp->rcucblist != NULL &&\r\nrcp->donetail != rcp->curtail) {\r\nrcp->donetail = rcp->curtail;\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void invoke_rcu_kthread(void)\r\n{\r\nhave_rcu_kthread_work = 1;\r\nwake_up(&rcu_kthread_wq);\r\n}\r\nvoid rcu_sched_qs(int cpu)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nif (rcu_qsctr_help(&rcu_sched_ctrlblk) +\r\nrcu_qsctr_help(&rcu_bh_ctrlblk))\r\ninvoke_rcu_kthread();\r\nlocal_irq_restore(flags);\r\n}\r\nvoid rcu_bh_qs(int cpu)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nif (rcu_qsctr_help(&rcu_bh_ctrlblk))\r\ninvoke_rcu_kthread();\r\nlocal_irq_restore(flags);\r\n}\r\nvoid rcu_check_callbacks(int cpu, int user)\r\n{\r\nif (user ||\r\n(idle_cpu(cpu) &&\r\n!in_softirq() &&\r\nhardirq_count() <= (1 << HARDIRQ_SHIFT)))\r\nrcu_sched_qs(cpu);\r\nelse if (!in_softirq())\r\nrcu_bh_qs(cpu);\r\nrcu_preempt_check_callbacks();\r\n}\r\nstatic void rcu_process_callbacks(struct rcu_ctrlblk *rcp)\r\n{\r\nstruct rcu_head *next, *list;\r\nunsigned long flags;\r\nRCU_TRACE(int cb_count = 0);\r\nif (&rcp->rcucblist == rcp->donetail)\r\nreturn;\r\nlocal_irq_save(flags);\r\nlist = rcp->rcucblist;\r\nrcp->rcucblist = *rcp->donetail;\r\n*rcp->donetail = NULL;\r\nif (rcp->curtail == rcp->donetail)\r\nrcp->curtail = &rcp->rcucblist;\r\nrcu_preempt_remove_callbacks(rcp);\r\nrcp->donetail = &rcp->rcucblist;\r\nlocal_irq_restore(flags);\r\nwhile (list) {\r\nnext = list->next;\r\nprefetch(next);\r\ndebug_rcu_head_unqueue(list);\r\nlocal_bh_disable();\r\n__rcu_reclaim(list);\r\nlocal_bh_enable();\r\nlist = next;\r\nRCU_TRACE(cb_count++);\r\n}\r\nRCU_TRACE(rcu_trace_sub_qlen(rcp, cb_count));\r\n}\r\nstatic int rcu_kthread(void *arg)\r\n{\r\nunsigned long work;\r\nunsigned long morework;\r\nunsigned long flags;\r\nfor (;;) {\r\nwait_event_interruptible(rcu_kthread_wq,\r\nhave_rcu_kthread_work != 0);\r\nmorework = rcu_boost();\r\nlocal_irq_save(flags);\r\nwork = have_rcu_kthread_work;\r\nhave_rcu_kthread_work = morework;\r\nlocal_irq_restore(flags);\r\nif (work) {\r\nrcu_process_callbacks(&rcu_sched_ctrlblk);\r\nrcu_process_callbacks(&rcu_bh_ctrlblk);\r\nrcu_preempt_process_callbacks();\r\n}\r\nschedule_timeout_interruptible(1);\r\n}\r\nreturn 0;\r\n}\r\nvoid synchronize_sched(void)\r\n{\r\ncond_resched();\r\n}\r\nstatic void __call_rcu(struct rcu_head *head,\r\nvoid (*func)(struct rcu_head *rcu),\r\nstruct rcu_ctrlblk *rcp)\r\n{\r\nunsigned long flags;\r\ndebug_rcu_head_queue(head);\r\nhead->func = func;\r\nhead->next = NULL;\r\nlocal_irq_save(flags);\r\n*rcp->curtail = head;\r\nrcp->curtail = &head->next;\r\nRCU_TRACE(rcp->qlen++);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))\r\n{\r\n__call_rcu(head, func, &rcu_sched_ctrlblk);\r\n}\r\nvoid call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))\r\n{\r\n__call_rcu(head, func, &rcu_bh_ctrlblk);\r\n}\r\nvoid rcu_barrier_bh(void)\r\n{\r\nstruct rcu_synchronize rcu;\r\ninit_rcu_head_on_stack(&rcu.head);\r\ninit_completion(&rcu.completion);\r\ncall_rcu_bh(&rcu.head, wakeme_after_rcu);\r\nwait_for_completion(&rcu.completion);\r\ndestroy_rcu_head_on_stack(&rcu.head);\r\n}\r\nvoid rcu_barrier_sched(void)\r\n{\r\nstruct rcu_synchronize rcu;\r\ninit_rcu_head_on_stack(&rcu.head);\r\ninit_completion(&rcu.completion);\r\ncall_rcu_sched(&rcu.head, wakeme_after_rcu);\r\nwait_for_completion(&rcu.completion);\r\ndestroy_rcu_head_on_stack(&rcu.head);\r\n}\r\nstatic int __init rcu_spawn_kthreads(void)\r\n{\r\nstruct sched_param sp;\r\nrcu_kthread_task = kthread_run(rcu_kthread, NULL, "rcu_kthread");\r\nsp.sched_priority = RCU_BOOST_PRIO;\r\nsched_setscheduler_nocheck(rcu_kthread_task, SCHED_FIFO, &sp);\r\nreturn 0;\r\n}
