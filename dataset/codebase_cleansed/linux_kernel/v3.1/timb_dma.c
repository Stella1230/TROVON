static struct device *chan2dev(struct dma_chan *chan)\r\n{\r\nreturn &chan->dev->device;\r\n}\r\nstatic struct device *chan2dmadev(struct dma_chan *chan)\r\n{\r\nreturn chan2dev(chan)->parent->parent;\r\n}\r\nstatic struct timb_dma *tdchantotd(struct timb_dma_chan *td_chan)\r\n{\r\nint id = td_chan->chan.chan_id;\r\nreturn (struct timb_dma *)((u8 *)td_chan -\r\nid * sizeof(struct timb_dma_chan) - sizeof(struct timb_dma));\r\n}\r\nstatic void __td_enable_chan_irq(struct timb_dma_chan *td_chan)\r\n{\r\nint id = td_chan->chan.chan_id;\r\nstruct timb_dma *td = tdchantotd(td_chan);\r\nu32 ier;\r\nier = ioread32(td->membase + TIMBDMA_IER);\r\nier |= 1 << id;\r\ndev_dbg(chan2dev(&td_chan->chan), "Enabling irq: %d, IER: 0x%x\n", id,\r\nier);\r\niowrite32(ier, td->membase + TIMBDMA_IER);\r\n}\r\nstatic bool __td_dma_done_ack(struct timb_dma_chan *td_chan)\r\n{\r\nint id = td_chan->chan.chan_id;\r\nstruct timb_dma *td = (struct timb_dma *)((u8 *)td_chan -\r\nid * sizeof(struct timb_dma_chan) - sizeof(struct timb_dma));\r\nu32 isr;\r\nbool done = false;\r\ndev_dbg(chan2dev(&td_chan->chan), "Checking irq: %d, td: %p\n", id, td);\r\nisr = ioread32(td->membase + TIMBDMA_ISR) & (1 << id);\r\nif (isr) {\r\niowrite32(isr, td->membase + TIMBDMA_ISR);\r\ndone = true;\r\n}\r\nreturn done;\r\n}\r\nstatic void __td_unmap_desc(struct timb_dma_chan *td_chan, const u8 *dma_desc,\r\nbool single)\r\n{\r\ndma_addr_t addr;\r\nint len;\r\naddr = (dma_desc[7] << 24) | (dma_desc[6] << 16) | (dma_desc[5] << 8) |\r\ndma_desc[4];\r\nlen = (dma_desc[3] << 8) | dma_desc[2];\r\nif (single)\r\ndma_unmap_single(chan2dev(&td_chan->chan), addr, len,\r\ntd_chan->direction);\r\nelse\r\ndma_unmap_page(chan2dev(&td_chan->chan), addr, len,\r\ntd_chan->direction);\r\n}\r\nstatic void __td_unmap_descs(struct timb_dma_desc *td_desc, bool single)\r\n{\r\nstruct timb_dma_chan *td_chan = container_of(td_desc->txd.chan,\r\nstruct timb_dma_chan, chan);\r\nu8 *descs;\r\nfor (descs = td_desc->desc_list; ; descs += TIMB_DMA_DESC_SIZE) {\r\n__td_unmap_desc(td_chan, descs, single);\r\nif (descs[0] & 0x02)\r\nbreak;\r\n}\r\n}\r\nstatic int td_fill_desc(struct timb_dma_chan *td_chan, u8 *dma_desc,\r\nstruct scatterlist *sg, bool last)\r\n{\r\nif (sg_dma_len(sg) > USHRT_MAX) {\r\ndev_err(chan2dev(&td_chan->chan), "Too big sg element\n");\r\nreturn -EINVAL;\r\n}\r\nif (sg_dma_len(sg) % sizeof(u32)) {\r\ndev_err(chan2dev(&td_chan->chan), "Incorrect length: %d\n",\r\nsg_dma_len(sg));\r\nreturn -EINVAL;\r\n}\r\ndev_dbg(chan2dev(&td_chan->chan), "desc: %p, addr: 0x%llx\n",\r\ndma_desc, (unsigned long long)sg_dma_address(sg));\r\ndma_desc[7] = (sg_dma_address(sg) >> 24) & 0xff;\r\ndma_desc[6] = (sg_dma_address(sg) >> 16) & 0xff;\r\ndma_desc[5] = (sg_dma_address(sg) >> 8) & 0xff;\r\ndma_desc[4] = (sg_dma_address(sg) >> 0) & 0xff;\r\ndma_desc[3] = (sg_dma_len(sg) >> 8) & 0xff;\r\ndma_desc[2] = (sg_dma_len(sg) >> 0) & 0xff;\r\ndma_desc[1] = 0x00;\r\ndma_desc[0] = 0x21 | (last ? 0x02 : 0);\r\nreturn 0;\r\n}\r\nstatic void __td_start_dma(struct timb_dma_chan *td_chan)\r\n{\r\nstruct timb_dma_desc *td_desc;\r\nif (td_chan->ongoing) {\r\ndev_err(chan2dev(&td_chan->chan),\r\n"Transfer already ongoing\n");\r\nreturn;\r\n}\r\ntd_desc = list_entry(td_chan->active_list.next, struct timb_dma_desc,\r\ndesc_node);\r\ndev_dbg(chan2dev(&td_chan->chan),\r\n"td_chan: %p, chan: %d, membase: %p\n",\r\ntd_chan, td_chan->chan.chan_id, td_chan->membase);\r\nif (td_chan->direction == DMA_FROM_DEVICE) {\r\niowrite32(0, td_chan->membase + TIMBDMA_OFFS_RX_DHAR);\r\niowrite32(td_desc->txd.phys, td_chan->membase +\r\nTIMBDMA_OFFS_RX_DLAR);\r\niowrite32(td_chan->bytes_per_line, td_chan->membase +\r\nTIMBDMA_OFFS_RX_BPRR);\r\niowrite32(TIMBDMA_RX_EN, td_chan->membase + TIMBDMA_OFFS_RX_ER);\r\n} else {\r\niowrite32(0, td_chan->membase + TIMBDMA_OFFS_TX_DHAR);\r\niowrite32(td_desc->txd.phys, td_chan->membase +\r\nTIMBDMA_OFFS_TX_DLAR);\r\n}\r\ntd_chan->ongoing = true;\r\nif (td_desc->interrupt)\r\n__td_enable_chan_irq(td_chan);\r\n}\r\nstatic void __td_finish(struct timb_dma_chan *td_chan)\r\n{\r\ndma_async_tx_callback callback;\r\nvoid *param;\r\nstruct dma_async_tx_descriptor *txd;\r\nstruct timb_dma_desc *td_desc;\r\nif (list_empty(&td_chan->active_list))\r\nreturn;\r\ntd_desc = list_entry(td_chan->active_list.next, struct timb_dma_desc,\r\ndesc_node);\r\ntxd = &td_desc->txd;\r\ndev_dbg(chan2dev(&td_chan->chan), "descriptor %u complete\n",\r\ntxd->cookie);\r\nif (td_chan->direction == DMA_FROM_DEVICE)\r\niowrite32(0, td_chan->membase + TIMBDMA_OFFS_RX_ER);\r\ntd_chan->last_completed_cookie = txd->cookie;\r\ntd_chan->ongoing = false;\r\ncallback = txd->callback;\r\nparam = txd->callback_param;\r\nlist_move(&td_desc->desc_node, &td_chan->free_list);\r\nif (!(txd->flags & DMA_COMPL_SKIP_SRC_UNMAP))\r\n__td_unmap_descs(td_desc,\r\ntxd->flags & DMA_COMPL_SRC_UNMAP_SINGLE);\r\nif (callback)\r\ncallback(param);\r\n}\r\nstatic u32 __td_ier_mask(struct timb_dma *td)\r\n{\r\nint i;\r\nu32 ret = 0;\r\nfor (i = 0; i < td->dma.chancnt; i++) {\r\nstruct timb_dma_chan *td_chan = td->channels + i;\r\nif (td_chan->ongoing) {\r\nstruct timb_dma_desc *td_desc =\r\nlist_entry(td_chan->active_list.next,\r\nstruct timb_dma_desc, desc_node);\r\nif (td_desc->interrupt)\r\nret |= 1 << i;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void __td_start_next(struct timb_dma_chan *td_chan)\r\n{\r\nstruct timb_dma_desc *td_desc;\r\nBUG_ON(list_empty(&td_chan->queue));\r\nBUG_ON(td_chan->ongoing);\r\ntd_desc = list_entry(td_chan->queue.next, struct timb_dma_desc,\r\ndesc_node);\r\ndev_dbg(chan2dev(&td_chan->chan), "%s: started %u\n",\r\n__func__, td_desc->txd.cookie);\r\nlist_move(&td_desc->desc_node, &td_chan->active_list);\r\n__td_start_dma(td_chan);\r\n}\r\nstatic dma_cookie_t td_tx_submit(struct dma_async_tx_descriptor *txd)\r\n{\r\nstruct timb_dma_desc *td_desc = container_of(txd, struct timb_dma_desc,\r\ntxd);\r\nstruct timb_dma_chan *td_chan = container_of(txd->chan,\r\nstruct timb_dma_chan, chan);\r\ndma_cookie_t cookie;\r\nspin_lock_bh(&td_chan->lock);\r\ncookie = txd->chan->cookie;\r\nif (++cookie < 0)\r\ncookie = 1;\r\ntxd->chan->cookie = cookie;\r\ntxd->cookie = cookie;\r\nif (list_empty(&td_chan->active_list)) {\r\ndev_dbg(chan2dev(txd->chan), "%s: started %u\n", __func__,\r\ntxd->cookie);\r\nlist_add_tail(&td_desc->desc_node, &td_chan->active_list);\r\n__td_start_dma(td_chan);\r\n} else {\r\ndev_dbg(chan2dev(txd->chan), "tx_submit: queued %u\n",\r\ntxd->cookie);\r\nlist_add_tail(&td_desc->desc_node, &td_chan->queue);\r\n}\r\nspin_unlock_bh(&td_chan->lock);\r\nreturn cookie;\r\n}\r\nstatic struct timb_dma_desc *td_alloc_init_desc(struct timb_dma_chan *td_chan)\r\n{\r\nstruct dma_chan *chan = &td_chan->chan;\r\nstruct timb_dma_desc *td_desc;\r\nint err;\r\ntd_desc = kzalloc(sizeof(struct timb_dma_desc), GFP_KERNEL);\r\nif (!td_desc) {\r\ndev_err(chan2dev(chan), "Failed to alloc descriptor\n");\r\ngoto out;\r\n}\r\ntd_desc->desc_list_len = td_chan->desc_elems * TIMB_DMA_DESC_SIZE;\r\ntd_desc->desc_list = kzalloc(td_desc->desc_list_len, GFP_KERNEL);\r\nif (!td_desc->desc_list) {\r\ndev_err(chan2dev(chan), "Failed to alloc descriptor\n");\r\ngoto err;\r\n}\r\ndma_async_tx_descriptor_init(&td_desc->txd, chan);\r\ntd_desc->txd.tx_submit = td_tx_submit;\r\ntd_desc->txd.flags = DMA_CTRL_ACK;\r\ntd_desc->txd.phys = dma_map_single(chan2dmadev(chan),\r\ntd_desc->desc_list, td_desc->desc_list_len, DMA_TO_DEVICE);\r\nerr = dma_mapping_error(chan2dmadev(chan), td_desc->txd.phys);\r\nif (err) {\r\ndev_err(chan2dev(chan), "DMA mapping error: %d\n", err);\r\ngoto err;\r\n}\r\nreturn td_desc;\r\nerr:\r\nkfree(td_desc->desc_list);\r\nkfree(td_desc);\r\nout:\r\nreturn NULL;\r\n}\r\nstatic void td_free_desc(struct timb_dma_desc *td_desc)\r\n{\r\ndev_dbg(chan2dev(td_desc->txd.chan), "Freeing desc: %p\n", td_desc);\r\ndma_unmap_single(chan2dmadev(td_desc->txd.chan), td_desc->txd.phys,\r\ntd_desc->desc_list_len, DMA_TO_DEVICE);\r\nkfree(td_desc->desc_list);\r\nkfree(td_desc);\r\n}\r\nstatic void td_desc_put(struct timb_dma_chan *td_chan,\r\nstruct timb_dma_desc *td_desc)\r\n{\r\ndev_dbg(chan2dev(&td_chan->chan), "Putting desc: %p\n", td_desc);\r\nspin_lock_bh(&td_chan->lock);\r\nlist_add(&td_desc->desc_node, &td_chan->free_list);\r\nspin_unlock_bh(&td_chan->lock);\r\n}\r\nstatic struct timb_dma_desc *td_desc_get(struct timb_dma_chan *td_chan)\r\n{\r\nstruct timb_dma_desc *td_desc, *_td_desc;\r\nstruct timb_dma_desc *ret = NULL;\r\nspin_lock_bh(&td_chan->lock);\r\nlist_for_each_entry_safe(td_desc, _td_desc, &td_chan->free_list,\r\ndesc_node) {\r\nif (async_tx_test_ack(&td_desc->txd)) {\r\nlist_del(&td_desc->desc_node);\r\nret = td_desc;\r\nbreak;\r\n}\r\ndev_dbg(chan2dev(&td_chan->chan), "desc %p not ACKed\n",\r\ntd_desc);\r\n}\r\nspin_unlock_bh(&td_chan->lock);\r\nreturn ret;\r\n}\r\nstatic int td_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct timb_dma_chan *td_chan =\r\ncontainer_of(chan, struct timb_dma_chan, chan);\r\nint i;\r\ndev_dbg(chan2dev(chan), "%s: entry\n", __func__);\r\nBUG_ON(!list_empty(&td_chan->free_list));\r\nfor (i = 0; i < td_chan->descs; i++) {\r\nstruct timb_dma_desc *td_desc = td_alloc_init_desc(td_chan);\r\nif (!td_desc) {\r\nif (i)\r\nbreak;\r\nelse {\r\ndev_err(chan2dev(chan),\r\n"Couldnt allocate any descriptors\n");\r\nreturn -ENOMEM;\r\n}\r\n}\r\ntd_desc_put(td_chan, td_desc);\r\n}\r\nspin_lock_bh(&td_chan->lock);\r\ntd_chan->last_completed_cookie = 1;\r\nchan->cookie = 1;\r\nspin_unlock_bh(&td_chan->lock);\r\nreturn 0;\r\n}\r\nstatic void td_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct timb_dma_chan *td_chan =\r\ncontainer_of(chan, struct timb_dma_chan, chan);\r\nstruct timb_dma_desc *td_desc, *_td_desc;\r\nLIST_HEAD(list);\r\ndev_dbg(chan2dev(chan), "%s: Entry\n", __func__);\r\nBUG_ON(!list_empty(&td_chan->active_list));\r\nBUG_ON(!list_empty(&td_chan->queue));\r\nspin_lock_bh(&td_chan->lock);\r\nlist_splice_init(&td_chan->free_list, &list);\r\nspin_unlock_bh(&td_chan->lock);\r\nlist_for_each_entry_safe(td_desc, _td_desc, &list, desc_node) {\r\ndev_dbg(chan2dev(chan), "%s: Freeing desc: %p\n", __func__,\r\ntd_desc);\r\ntd_free_desc(td_desc);\r\n}\r\n}\r\nstatic enum dma_status td_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct timb_dma_chan *td_chan =\r\ncontainer_of(chan, struct timb_dma_chan, chan);\r\ndma_cookie_t last_used;\r\ndma_cookie_t last_complete;\r\nint ret;\r\ndev_dbg(chan2dev(chan), "%s: Entry\n", __func__);\r\nlast_complete = td_chan->last_completed_cookie;\r\nlast_used = chan->cookie;\r\nret = dma_async_is_complete(cookie, last_complete, last_used);\r\ndma_set_tx_state(txstate, last_complete, last_used, 0);\r\ndev_dbg(chan2dev(chan),\r\n"%s: exit, ret: %d, last_complete: %d, last_used: %d\n",\r\n__func__, ret, last_complete, last_used);\r\nreturn ret;\r\n}\r\nstatic void td_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct timb_dma_chan *td_chan =\r\ncontainer_of(chan, struct timb_dma_chan, chan);\r\ndev_dbg(chan2dev(chan), "%s: Entry\n", __func__);\r\nspin_lock_bh(&td_chan->lock);\r\nif (!list_empty(&td_chan->active_list))\r\nif (__td_dma_done_ack(td_chan))\r\n__td_finish(td_chan);\r\nif (list_empty(&td_chan->active_list) && !list_empty(&td_chan->queue))\r\n__td_start_next(td_chan);\r\nspin_unlock_bh(&td_chan->lock);\r\n}\r\nstatic struct dma_async_tx_descriptor *td_prep_slave_sg(struct dma_chan *chan,\r\nstruct scatterlist *sgl, unsigned int sg_len,\r\nenum dma_data_direction direction, unsigned long flags)\r\n{\r\nstruct timb_dma_chan *td_chan =\r\ncontainer_of(chan, struct timb_dma_chan, chan);\r\nstruct timb_dma_desc *td_desc;\r\nstruct scatterlist *sg;\r\nunsigned int i;\r\nunsigned int desc_usage = 0;\r\nif (!sgl || !sg_len) {\r\ndev_err(chan2dev(chan), "%s: No SG list\n", __func__);\r\nreturn NULL;\r\n}\r\nif (td_chan->direction != direction) {\r\ndev_err(chan2dev(chan),\r\n"Requesting channel in wrong direction\n");\r\nreturn NULL;\r\n}\r\ntd_desc = td_desc_get(td_chan);\r\nif (!td_desc) {\r\ndev_err(chan2dev(chan), "Not enough descriptors available\n");\r\nreturn NULL;\r\n}\r\ntd_desc->interrupt = (flags & DMA_PREP_INTERRUPT) != 0;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nint err;\r\nif (desc_usage > td_desc->desc_list_len) {\r\ndev_err(chan2dev(chan), "No descriptor space\n");\r\nreturn NULL;\r\n}\r\nerr = td_fill_desc(td_chan, td_desc->desc_list + desc_usage, sg,\r\ni == (sg_len - 1));\r\nif (err) {\r\ndev_err(chan2dev(chan), "Failed to update desc: %d\n",\r\nerr);\r\ntd_desc_put(td_chan, td_desc);\r\nreturn NULL;\r\n}\r\ndesc_usage += TIMB_DMA_DESC_SIZE;\r\n}\r\ndma_sync_single_for_device(chan2dmadev(chan), td_desc->txd.phys,\r\ntd_desc->desc_list_len, DMA_TO_DEVICE);\r\nreturn &td_desc->txd;\r\n}\r\nstatic int td_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct timb_dma_chan *td_chan =\r\ncontainer_of(chan, struct timb_dma_chan, chan);\r\nstruct timb_dma_desc *td_desc, *_td_desc;\r\ndev_dbg(chan2dev(chan), "%s: Entry\n", __func__);\r\nif (cmd != DMA_TERMINATE_ALL)\r\nreturn -ENXIO;\r\nspin_lock_bh(&td_chan->lock);\r\nlist_for_each_entry_safe(td_desc, _td_desc, &td_chan->queue,\r\ndesc_node)\r\nlist_move(&td_desc->desc_node, &td_chan->free_list);\r\n__td_finish(td_chan);\r\nspin_unlock_bh(&td_chan->lock);\r\nreturn 0;\r\n}\r\nstatic void td_tasklet(unsigned long data)\r\n{\r\nstruct timb_dma *td = (struct timb_dma *)data;\r\nu32 isr;\r\nu32 ipr;\r\nu32 ier;\r\nint i;\r\nisr = ioread32(td->membase + TIMBDMA_ISR);\r\nipr = isr & __td_ier_mask(td);\r\niowrite32(ipr, td->membase + TIMBDMA_ISR);\r\nfor (i = 0; i < td->dma.chancnt; i++)\r\nif (ipr & (1 << i)) {\r\nstruct timb_dma_chan *td_chan = td->channels + i;\r\nspin_lock(&td_chan->lock);\r\n__td_finish(td_chan);\r\nif (!list_empty(&td_chan->queue))\r\n__td_start_next(td_chan);\r\nspin_unlock(&td_chan->lock);\r\n}\r\nier = __td_ier_mask(td);\r\niowrite32(ier, td->membase + TIMBDMA_IER);\r\n}\r\nstatic irqreturn_t td_irq(int irq, void *devid)\r\n{\r\nstruct timb_dma *td = devid;\r\nu32 ipr = ioread32(td->membase + TIMBDMA_IPR);\r\nif (ipr) {\r\niowrite32(0, td->membase + TIMBDMA_IER);\r\ntasklet_schedule(&td->tasklet);\r\nreturn IRQ_HANDLED;\r\n} else\r\nreturn IRQ_NONE;\r\n}\r\nstatic int __devinit td_probe(struct platform_device *pdev)\r\n{\r\nstruct timb_dma_platform_data *pdata = pdev->dev.platform_data;\r\nstruct timb_dma *td;\r\nstruct resource *iomem;\r\nint irq;\r\nint err;\r\nint i;\r\nif (!pdata) {\r\ndev_err(&pdev->dev, "No platform data\n");\r\nreturn -EINVAL;\r\n}\r\niomem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!iomem)\r\nreturn -EINVAL;\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0)\r\nreturn irq;\r\nif (!request_mem_region(iomem->start, resource_size(iomem),\r\nDRIVER_NAME))\r\nreturn -EBUSY;\r\ntd = kzalloc(sizeof(struct timb_dma) +\r\nsizeof(struct timb_dma_chan) * pdata->nr_channels, GFP_KERNEL);\r\nif (!td) {\r\nerr = -ENOMEM;\r\ngoto err_release_region;\r\n}\r\ndev_dbg(&pdev->dev, "Allocated TD: %p\n", td);\r\ntd->membase = ioremap(iomem->start, resource_size(iomem));\r\nif (!td->membase) {\r\ndev_err(&pdev->dev, "Failed to remap I/O memory\n");\r\nerr = -ENOMEM;\r\ngoto err_free_mem;\r\n}\r\niowrite32(TIMBDMA_32BIT_ADDR, td->membase + TIMBDMA_ACR);\r\niowrite32(0x0, td->membase + TIMBDMA_IER);\r\niowrite32(0xFFFFFFFF, td->membase + TIMBDMA_ISR);\r\ntasklet_init(&td->tasklet, td_tasklet, (unsigned long)td);\r\nerr = request_irq(irq, td_irq, IRQF_SHARED, DRIVER_NAME, td);\r\nif (err) {\r\ndev_err(&pdev->dev, "Failed to request IRQ\n");\r\ngoto err_tasklet_kill;\r\n}\r\ntd->dma.device_alloc_chan_resources = td_alloc_chan_resources;\r\ntd->dma.device_free_chan_resources = td_free_chan_resources;\r\ntd->dma.device_tx_status = td_tx_status;\r\ntd->dma.device_issue_pending = td_issue_pending;\r\ndma_cap_set(DMA_SLAVE, td->dma.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, td->dma.cap_mask);\r\ntd->dma.device_prep_slave_sg = td_prep_slave_sg;\r\ntd->dma.device_control = td_control;\r\ntd->dma.dev = &pdev->dev;\r\nINIT_LIST_HEAD(&td->dma.channels);\r\nfor (i = 0; i < pdata->nr_channels; i++, td->dma.chancnt++) {\r\nstruct timb_dma_chan *td_chan = &td->channels[i];\r\nstruct timb_dma_platform_data_channel *pchan =\r\npdata->channels + i;\r\nif ((i % 2) == pchan->rx) {\r\ndev_err(&pdev->dev, "Wrong channel configuration\n");\r\nerr = -EINVAL;\r\ngoto err_tasklet_kill;\r\n}\r\ntd_chan->chan.device = &td->dma;\r\ntd_chan->chan.cookie = 1;\r\ntd_chan->chan.chan_id = i;\r\nspin_lock_init(&td_chan->lock);\r\nINIT_LIST_HEAD(&td_chan->active_list);\r\nINIT_LIST_HEAD(&td_chan->queue);\r\nINIT_LIST_HEAD(&td_chan->free_list);\r\ntd_chan->descs = pchan->descriptors;\r\ntd_chan->desc_elems = pchan->descriptor_elements;\r\ntd_chan->bytes_per_line = pchan->bytes_per_line;\r\ntd_chan->direction = pchan->rx ? DMA_FROM_DEVICE :\r\nDMA_TO_DEVICE;\r\ntd_chan->membase = td->membase +\r\n(i / 2) * TIMBDMA_INSTANCE_OFFSET +\r\n(pchan->rx ? 0 : TIMBDMA_INSTANCE_TX_OFFSET);\r\ndev_dbg(&pdev->dev, "Chan: %d, membase: %p\n",\r\ni, td_chan->membase);\r\nlist_add_tail(&td_chan->chan.device_node, &td->dma.channels);\r\n}\r\nerr = dma_async_device_register(&td->dma);\r\nif (err) {\r\ndev_err(&pdev->dev, "Failed to register async device\n");\r\ngoto err_free_irq;\r\n}\r\nplatform_set_drvdata(pdev, td);\r\ndev_dbg(&pdev->dev, "Probe result: %d\n", err);\r\nreturn err;\r\nerr_free_irq:\r\nfree_irq(irq, td);\r\nerr_tasklet_kill:\r\ntasklet_kill(&td->tasklet);\r\niounmap(td->membase);\r\nerr_free_mem:\r\nkfree(td);\r\nerr_release_region:\r\nrelease_mem_region(iomem->start, resource_size(iomem));\r\nreturn err;\r\n}\r\nstatic int __devexit td_remove(struct platform_device *pdev)\r\n{\r\nstruct timb_dma *td = platform_get_drvdata(pdev);\r\nstruct resource *iomem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nint irq = platform_get_irq(pdev, 0);\r\ndma_async_device_unregister(&td->dma);\r\nfree_irq(irq, td);\r\ntasklet_kill(&td->tasklet);\r\niounmap(td->membase);\r\nkfree(td);\r\nrelease_mem_region(iomem->start, resource_size(iomem));\r\nplatform_set_drvdata(pdev, NULL);\r\ndev_dbg(&pdev->dev, "Removed...\n");\r\nreturn 0;\r\n}\r\nstatic int __init td_init(void)\r\n{\r\nreturn platform_driver_register(&td_driver);\r\n}\r\nstatic void __exit td_exit(void)\r\n{\r\nplatform_driver_unregister(&td_driver);\r\n}
