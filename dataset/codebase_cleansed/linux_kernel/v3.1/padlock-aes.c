static inline int\r\naes_hw_extkey_available(uint8_t key_len)\r\n{\r\nif (key_len == 16)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline struct aes_ctx *aes_ctx_common(void *ctx)\r\n{\r\nunsigned long addr = (unsigned long)ctx;\r\nunsigned long align = PADLOCK_ALIGNMENT;\r\nif (align <= crypto_tfm_ctx_alignment())\r\nalign = 1;\r\nreturn (struct aes_ctx *)ALIGN(addr, align);\r\n}\r\nstatic inline struct aes_ctx *aes_ctx(struct crypto_tfm *tfm)\r\n{\r\nreturn aes_ctx_common(crypto_tfm_ctx(tfm));\r\n}\r\nstatic inline struct aes_ctx *blk_aes_ctx(struct crypto_blkcipher *tfm)\r\n{\r\nreturn aes_ctx_common(crypto_blkcipher_ctx(tfm));\r\n}\r\nstatic int aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct aes_ctx *ctx = aes_ctx(tfm);\r\nconst __le32 *key = (const __le32 *)in_key;\r\nu32 *flags = &tfm->crt_flags;\r\nstruct crypto_aes_ctx gen_aes;\r\nint cpu;\r\nif (key_len % 8) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nctx->D = ctx->E;\r\nctx->E[0] = le32_to_cpu(key[0]);\r\nctx->E[1] = le32_to_cpu(key[1]);\r\nctx->E[2] = le32_to_cpu(key[2]);\r\nctx->E[3] = le32_to_cpu(key[3]);\r\nmemset(&ctx->cword, 0, sizeof(ctx->cword));\r\nctx->cword.decrypt.encdec = 1;\r\nctx->cword.encrypt.rounds = 10 + (key_len - 16) / 4;\r\nctx->cword.decrypt.rounds = ctx->cword.encrypt.rounds;\r\nctx->cword.encrypt.ksize = (key_len - 16) / 8;\r\nctx->cword.decrypt.ksize = ctx->cword.encrypt.ksize;\r\nif (aes_hw_extkey_available(key_len))\r\ngoto ok;\r\nctx->D = ctx->d_data;\r\nctx->cword.encrypt.keygen = 1;\r\nctx->cword.decrypt.keygen = 1;\r\nif (crypto_aes_expand_key(&gen_aes, in_key, key_len)) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nmemcpy(ctx->E, gen_aes.key_enc, AES_MAX_KEYLENGTH);\r\nmemcpy(ctx->D, gen_aes.key_dec, AES_MAX_KEYLENGTH);\r\nok:\r\nfor_each_online_cpu(cpu)\r\nif (&ctx->cword.encrypt == per_cpu(paes_last_cword, cpu) ||\r\n&ctx->cword.decrypt == per_cpu(paes_last_cword, cpu))\r\nper_cpu(paes_last_cword, cpu) = NULL;\r\nreturn 0;\r\n}\r\nstatic inline void padlock_reset_key(struct cword *cword)\r\n{\r\nint cpu = raw_smp_processor_id();\r\nif (cword != per_cpu(paes_last_cword, cpu))\r\n#ifndef CONFIG_X86_64\r\nasm volatile ("pushfl; popfl");\r\n#else\r\nasm volatile ("pushfq; popfq");\r\n#endif\r\n}\r\nstatic inline void padlock_store_cword(struct cword *cword)\r\n{\r\nper_cpu(paes_last_cword, raw_smp_processor_id()) = cword;\r\n}\r\nstatic inline void rep_xcrypt_ecb(const u8 *input, u8 *output, void *key,\r\nstruct cword *control_word, int count)\r\n{\r\nasm volatile (".byte 0xf3,0x0f,0xa7,0xc8"\r\n: "+S"(input), "+D"(output)\r\n: "d"(control_word), "b"(key), "c"(count));\r\n}\r\nstatic inline u8 *rep_xcrypt_cbc(const u8 *input, u8 *output, void *key,\r\nu8 *iv, struct cword *control_word, int count)\r\n{\r\nasm volatile (".byte 0xf3,0x0f,0xa7,0xd0"\r\n: "+S" (input), "+D" (output), "+a" (iv)\r\n: "d" (control_word), "b" (key), "c" (count));\r\nreturn iv;\r\n}\r\nstatic void ecb_crypt_copy(const u8 *in, u8 *out, u32 *key,\r\nstruct cword *cword, int count)\r\n{\r\nu8 buf[AES_BLOCK_SIZE * (MAX_ECB_FETCH_BLOCKS - 1) + PADLOCK_ALIGNMENT - 1];\r\nu8 *tmp = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\r\nmemcpy(tmp, in, count * AES_BLOCK_SIZE);\r\nrep_xcrypt_ecb(tmp, out, key, cword, count);\r\n}\r\nstatic u8 *cbc_crypt_copy(const u8 *in, u8 *out, u32 *key,\r\nu8 *iv, struct cword *cword, int count)\r\n{\r\nu8 buf[AES_BLOCK_SIZE * (MAX_CBC_FETCH_BLOCKS - 1) + PADLOCK_ALIGNMENT - 1];\r\nu8 *tmp = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\r\nmemcpy(tmp, in, count * AES_BLOCK_SIZE);\r\nreturn rep_xcrypt_cbc(tmp, out, key, iv, cword, count);\r\n}\r\nstatic inline void ecb_crypt(const u8 *in, u8 *out, u32 *key,\r\nstruct cword *cword, int count)\r\n{\r\nif (unlikely(((unsigned long)in & ~PAGE_MASK) + ecb_fetch_bytes > PAGE_SIZE)) {\r\necb_crypt_copy(in, out, key, cword, count);\r\nreturn;\r\n}\r\nrep_xcrypt_ecb(in, out, key, cword, count);\r\n}\r\nstatic inline u8 *cbc_crypt(const u8 *in, u8 *out, u32 *key,\r\nu8 *iv, struct cword *cword, int count)\r\n{\r\nif (unlikely(((unsigned long)in & ~PAGE_MASK) + cbc_fetch_bytes > PAGE_SIZE))\r\nreturn cbc_crypt_copy(in, out, key, iv, cword, count);\r\nreturn rep_xcrypt_cbc(in, out, key, iv, cword, count);\r\n}\r\nstatic inline void padlock_xcrypt_ecb(const u8 *input, u8 *output, void *key,\r\nvoid *control_word, u32 count)\r\n{\r\nu32 initial = count & (ecb_fetch_blocks - 1);\r\nif (count < ecb_fetch_blocks) {\r\necb_crypt(input, output, key, control_word, count);\r\nreturn;\r\n}\r\nif (initial)\r\nasm volatile (".byte 0xf3,0x0f,0xa7,0xc8"\r\n: "+S"(input), "+D"(output)\r\n: "d"(control_word), "b"(key), "c"(initial));\r\nasm volatile (".byte 0xf3,0x0f,0xa7,0xc8"\r\n: "+S"(input), "+D"(output)\r\n: "d"(control_word), "b"(key), "c"(count - initial));\r\n}\r\nstatic inline u8 *padlock_xcrypt_cbc(const u8 *input, u8 *output, void *key,\r\nu8 *iv, void *control_word, u32 count)\r\n{\r\nu32 initial = count & (cbc_fetch_blocks - 1);\r\nif (count < cbc_fetch_blocks)\r\nreturn cbc_crypt(input, output, key, iv, control_word, count);\r\nif (initial)\r\nasm volatile (".byte 0xf3,0x0f,0xa7,0xd0"\r\n: "+S" (input), "+D" (output), "+a" (iv)\r\n: "d" (control_word), "b" (key), "c" (initial));\r\nasm volatile (".byte 0xf3,0x0f,0xa7,0xd0"\r\n: "+S" (input), "+D" (output), "+a" (iv)\r\n: "d" (control_word), "b" (key), "c" (count-initial));\r\nreturn iv;\r\n}\r\nstatic void aes_encrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\r\n{\r\nstruct aes_ctx *ctx = aes_ctx(tfm);\r\nint ts_state;\r\npadlock_reset_key(&ctx->cword.encrypt);\r\nts_state = irq_ts_save();\r\necb_crypt(in, out, ctx->E, &ctx->cword.encrypt, 1);\r\nirq_ts_restore(ts_state);\r\npadlock_store_cword(&ctx->cword.encrypt);\r\n}\r\nstatic void aes_decrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\r\n{\r\nstruct aes_ctx *ctx = aes_ctx(tfm);\r\nint ts_state;\r\npadlock_reset_key(&ctx->cword.encrypt);\r\nts_state = irq_ts_save();\r\necb_crypt(in, out, ctx->D, &ctx->cword.decrypt, 1);\r\nirq_ts_restore(ts_state);\r\npadlock_store_cword(&ctx->cword.encrypt);\r\n}\r\nstatic int ecb_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct aes_ctx *ctx = blk_aes_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nint err;\r\nint ts_state;\r\npadlock_reset_key(&ctx->cword.encrypt);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nts_state = irq_ts_save();\r\nwhile ((nbytes = walk.nbytes)) {\r\npadlock_xcrypt_ecb(walk.src.virt.addr, walk.dst.virt.addr,\r\nctx->E, &ctx->cword.encrypt,\r\nnbytes / AES_BLOCK_SIZE);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nirq_ts_restore(ts_state);\r\npadlock_store_cword(&ctx->cword.encrypt);\r\nreturn err;\r\n}\r\nstatic int ecb_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct aes_ctx *ctx = blk_aes_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nint err;\r\nint ts_state;\r\npadlock_reset_key(&ctx->cword.decrypt);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nts_state = irq_ts_save();\r\nwhile ((nbytes = walk.nbytes)) {\r\npadlock_xcrypt_ecb(walk.src.virt.addr, walk.dst.virt.addr,\r\nctx->D, &ctx->cword.decrypt,\r\nnbytes / AES_BLOCK_SIZE);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nirq_ts_restore(ts_state);\r\npadlock_store_cword(&ctx->cword.encrypt);\r\nreturn err;\r\n}\r\nstatic int cbc_aes_encrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct aes_ctx *ctx = blk_aes_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nint err;\r\nint ts_state;\r\npadlock_reset_key(&ctx->cword.encrypt);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nts_state = irq_ts_save();\r\nwhile ((nbytes = walk.nbytes)) {\r\nu8 *iv = padlock_xcrypt_cbc(walk.src.virt.addr,\r\nwalk.dst.virt.addr, ctx->E,\r\nwalk.iv, &ctx->cword.encrypt,\r\nnbytes / AES_BLOCK_SIZE);\r\nmemcpy(walk.iv, iv, AES_BLOCK_SIZE);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nirq_ts_restore(ts_state);\r\npadlock_store_cword(&ctx->cword.decrypt);\r\nreturn err;\r\n}\r\nstatic int cbc_aes_decrypt(struct blkcipher_desc *desc,\r\nstruct scatterlist *dst, struct scatterlist *src,\r\nunsigned int nbytes)\r\n{\r\nstruct aes_ctx *ctx = blk_aes_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nint err;\r\nint ts_state;\r\npadlock_reset_key(&ctx->cword.encrypt);\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nts_state = irq_ts_save();\r\nwhile ((nbytes = walk.nbytes)) {\r\npadlock_xcrypt_cbc(walk.src.virt.addr, walk.dst.virt.addr,\r\nctx->D, walk.iv, &ctx->cword.decrypt,\r\nnbytes / AES_BLOCK_SIZE);\r\nnbytes &= AES_BLOCK_SIZE - 1;\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nirq_ts_restore(ts_state);\r\npadlock_store_cword(&ctx->cword.encrypt);\r\nreturn err;\r\n}\r\nstatic int __init padlock_init(void)\r\n{\r\nint ret;\r\nstruct cpuinfo_x86 *c = &cpu_data(0);\r\nif (!cpu_has_xcrypt) {\r\nprintk(KERN_NOTICE PFX "VIA PadLock not detected.\n");\r\nreturn -ENODEV;\r\n}\r\nif (!cpu_has_xcrypt_enabled) {\r\nprintk(KERN_NOTICE PFX "VIA PadLock detected, but not enabled. Hmm, strange...\n");\r\nreturn -ENODEV;\r\n}\r\nif ((ret = crypto_register_alg(&aes_alg)))\r\ngoto aes_err;\r\nif ((ret = crypto_register_alg(&ecb_aes_alg)))\r\ngoto ecb_aes_err;\r\nif ((ret = crypto_register_alg(&cbc_aes_alg)))\r\ngoto cbc_aes_err;\r\nprintk(KERN_NOTICE PFX "Using VIA PadLock ACE for AES algorithm.\n");\r\nif (c->x86 == 6 && c->x86_model == 15 && c->x86_mask == 2) {\r\necb_fetch_blocks = MAX_ECB_FETCH_BLOCKS;\r\ncbc_fetch_blocks = MAX_CBC_FETCH_BLOCKS;\r\nprintk(KERN_NOTICE PFX "VIA Nano stepping 2 detected: enabling workaround.\n");\r\n}\r\nout:\r\nreturn ret;\r\ncbc_aes_err:\r\ncrypto_unregister_alg(&ecb_aes_alg);\r\necb_aes_err:\r\ncrypto_unregister_alg(&aes_alg);\r\naes_err:\r\nprintk(KERN_ERR PFX "VIA PadLock AES initialization failed.\n");\r\ngoto out;\r\n}\r\nstatic void __exit padlock_fini(void)\r\n{\r\ncrypto_unregister_alg(&cbc_aes_alg);\r\ncrypto_unregister_alg(&ecb_aes_alg);\r\ncrypto_unregister_alg(&aes_alg);\r\n}
