static inline void io_remap_pte_range(struct mm_struct *mm, pte_t * pte,\r\nunsigned long address,\r\nunsigned long size,\r\nunsigned long offset, pgprot_t prot,\r\nint space)\r\n{\r\nunsigned long end;\r\noffset &= ~0x1UL;\r\naddress &= ~PMD_MASK;\r\nend = address + size;\r\nif (end > PMD_SIZE)\r\nend = PMD_SIZE;\r\ndo {\r\npte_t entry;\r\nunsigned long curend = address + PAGE_SIZE;\r\nentry = mk_pte_io(offset, prot, space, PAGE_SIZE);\r\nif (!(address & 0xffff)) {\r\nif (PAGE_SIZE < (4 * 1024 * 1024) &&\r\n!(address & 0x3fffff) &&\r\n!(offset & 0x3ffffe) &&\r\nend >= address + 0x400000) {\r\nentry = mk_pte_io(offset, prot, space,\r\n4 * 1024 * 1024);\r\ncurend = address + 0x400000;\r\noffset += 0x400000;\r\n} else if (PAGE_SIZE < (512 * 1024) &&\r\n!(address & 0x7ffff) &&\r\n!(offset & 0x7fffe) &&\r\nend >= address + 0x80000) {\r\nentry = mk_pte_io(offset, prot, space,\r\n512 * 1024 * 1024);\r\ncurend = address + 0x80000;\r\noffset += 0x80000;\r\n} else if (PAGE_SIZE < (64 * 1024) &&\r\n!(offset & 0xfffe) &&\r\nend >= address + 0x10000) {\r\nentry = mk_pte_io(offset, prot, space,\r\n64 * 1024);\r\ncurend = address + 0x10000;\r\noffset += 0x10000;\r\n} else\r\noffset += PAGE_SIZE;\r\n} else\r\noffset += PAGE_SIZE;\r\nif (pte_write(entry))\r\nentry = pte_mkdirty(entry);\r\ndo {\r\nBUG_ON(!pte_none(*pte));\r\nset_pte_at(mm, address, pte, entry);\r\naddress += PAGE_SIZE;\r\npte_val(entry) += PAGE_SIZE;\r\npte++;\r\n} while (address < curend);\r\n} while (address < end);\r\n}\r\nstatic inline int io_remap_pmd_range(struct mm_struct *mm, pmd_t * pmd, unsigned long address, unsigned long size,\r\nunsigned long offset, pgprot_t prot, int space)\r\n{\r\nunsigned long end;\r\naddress &= ~PGDIR_MASK;\r\nend = address + size;\r\nif (end > PGDIR_SIZE)\r\nend = PGDIR_SIZE;\r\noffset -= address;\r\ndo {\r\npte_t *pte = pte_alloc_map(mm, NULL, pmd, address);\r\nif (!pte)\r\nreturn -ENOMEM;\r\nio_remap_pte_range(mm, pte, address, end - address, address + offset, prot, space);\r\npte_unmap(pte);\r\naddress = (address + PMD_SIZE) & PMD_MASK;\r\npmd++;\r\n} while (address < end);\r\nreturn 0;\r\n}\r\nstatic inline int io_remap_pud_range(struct mm_struct *mm, pud_t * pud, unsigned long address, unsigned long size,\r\nunsigned long offset, pgprot_t prot, int space)\r\n{\r\nunsigned long end;\r\naddress &= ~PUD_MASK;\r\nend = address + size;\r\nif (end > PUD_SIZE)\r\nend = PUD_SIZE;\r\noffset -= address;\r\ndo {\r\npmd_t *pmd = pmd_alloc(mm, pud, address);\r\nif (!pud)\r\nreturn -ENOMEM;\r\nio_remap_pmd_range(mm, pmd, address, end - address, address + offset, prot, space);\r\naddress = (address + PUD_SIZE) & PUD_MASK;\r\npud++;\r\n} while (address < end);\r\nreturn 0;\r\n}\r\nint io_remap_pfn_range(struct vm_area_struct *vma, unsigned long from,\r\nunsigned long pfn, unsigned long size, pgprot_t prot)\r\n{\r\nint error = 0;\r\npgd_t * dir;\r\nunsigned long beg = from;\r\nunsigned long end = from + size;\r\nstruct mm_struct *mm = vma->vm_mm;\r\nint space = GET_IOSPACE(pfn);\r\nunsigned long offset = GET_PFN(pfn) << PAGE_SHIFT;\r\nunsigned long phys_base;\r\nphys_base = offset | (((unsigned long) space) << 32UL);\r\nvma->vm_flags |= VM_IO | VM_RESERVED | VM_PFNMAP;\r\nvma->vm_pgoff = phys_base >> PAGE_SHIFT;\r\noffset -= from;\r\ndir = pgd_offset(mm, from);\r\nflush_cache_range(vma, beg, end);\r\nwhile (from < end) {\r\npud_t *pud = pud_alloc(mm, dir, from);\r\nerror = -ENOMEM;\r\nif (!pud)\r\nbreak;\r\nerror = io_remap_pud_range(mm, pud, from, end - from, offset + from, prot, space);\r\nif (error)\r\nbreak;\r\nfrom = (from + PGDIR_SIZE) & PGDIR_MASK;\r\ndir++;\r\n}\r\nflush_tlb_range(vma, beg, end);\r\nreturn error;\r\n}
