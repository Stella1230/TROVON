void hfs_bnode_read(struct hfs_bnode *node, void *buf,\r\nint off, int len)\r\n{\r\nstruct page *page;\r\noff += node->page_offset;\r\npage = node->page[0];\r\nmemcpy(buf, kmap(page) + off, len);\r\nkunmap(page);\r\n}\r\nu16 hfs_bnode_read_u16(struct hfs_bnode *node, int off)\r\n{\r\n__be16 data;\r\nhfs_bnode_read(node, &data, off, 2);\r\nreturn be16_to_cpu(data);\r\n}\r\nu8 hfs_bnode_read_u8(struct hfs_bnode *node, int off)\r\n{\r\nu8 data;\r\nhfs_bnode_read(node, &data, off, 1);\r\nreturn data;\r\n}\r\nvoid hfs_bnode_read_key(struct hfs_bnode *node, void *key, int off)\r\n{\r\nstruct hfs_btree *tree;\r\nint key_len;\r\ntree = node->tree;\r\nif (node->type == HFS_NODE_LEAF ||\r\ntree->attributes & HFS_TREE_VARIDXKEYS)\r\nkey_len = hfs_bnode_read_u8(node, off) + 1;\r\nelse\r\nkey_len = tree->max_key_len + 1;\r\nhfs_bnode_read(node, key, off, key_len);\r\n}\r\nvoid hfs_bnode_write(struct hfs_bnode *node, void *buf, int off, int len)\r\n{\r\nstruct page *page;\r\noff += node->page_offset;\r\npage = node->page[0];\r\nmemcpy(kmap(page) + off, buf, len);\r\nkunmap(page);\r\nset_page_dirty(page);\r\n}\r\nvoid hfs_bnode_write_u16(struct hfs_bnode *node, int off, u16 data)\r\n{\r\n__be16 v = cpu_to_be16(data);\r\nhfs_bnode_write(node, &v, off, 2);\r\n}\r\nvoid hfs_bnode_write_u8(struct hfs_bnode *node, int off, u8 data)\r\n{\r\nhfs_bnode_write(node, &data, off, 1);\r\n}\r\nvoid hfs_bnode_clear(struct hfs_bnode *node, int off, int len)\r\n{\r\nstruct page *page;\r\noff += node->page_offset;\r\npage = node->page[0];\r\nmemset(kmap(page) + off, 0, len);\r\nkunmap(page);\r\nset_page_dirty(page);\r\n}\r\nvoid hfs_bnode_copy(struct hfs_bnode *dst_node, int dst,\r\nstruct hfs_bnode *src_node, int src, int len)\r\n{\r\nstruct hfs_btree *tree;\r\nstruct page *src_page, *dst_page;\r\ndprint(DBG_BNODE_MOD, "copybytes: %u,%u,%u\n", dst, src, len);\r\nif (!len)\r\nreturn;\r\ntree = src_node->tree;\r\nsrc += src_node->page_offset;\r\ndst += dst_node->page_offset;\r\nsrc_page = src_node->page[0];\r\ndst_page = dst_node->page[0];\r\nmemcpy(kmap(dst_page) + dst, kmap(src_page) + src, len);\r\nkunmap(src_page);\r\nkunmap(dst_page);\r\nset_page_dirty(dst_page);\r\n}\r\nvoid hfs_bnode_move(struct hfs_bnode *node, int dst, int src, int len)\r\n{\r\nstruct page *page;\r\nvoid *ptr;\r\ndprint(DBG_BNODE_MOD, "movebytes: %u,%u,%u\n", dst, src, len);\r\nif (!len)\r\nreturn;\r\nsrc += node->page_offset;\r\ndst += node->page_offset;\r\npage = node->page[0];\r\nptr = kmap(page);\r\nmemmove(ptr + dst, ptr + src, len);\r\nkunmap(page);\r\nset_page_dirty(page);\r\n}\r\nvoid hfs_bnode_dump(struct hfs_bnode *node)\r\n{\r\nstruct hfs_bnode_desc desc;\r\n__be32 cnid;\r\nint i, off, key_off;\r\ndprint(DBG_BNODE_MOD, "bnode: %d\n", node->this);\r\nhfs_bnode_read(node, &desc, 0, sizeof(desc));\r\ndprint(DBG_BNODE_MOD, "%d, %d, %d, %d, %d\n",\r\nbe32_to_cpu(desc.next), be32_to_cpu(desc.prev),\r\ndesc.type, desc.height, be16_to_cpu(desc.num_recs));\r\noff = node->tree->node_size - 2;\r\nfor (i = be16_to_cpu(desc.num_recs); i >= 0; off -= 2, i--) {\r\nkey_off = hfs_bnode_read_u16(node, off);\r\ndprint(DBG_BNODE_MOD, " %d", key_off);\r\nif (i && node->type == HFS_NODE_INDEX) {\r\nint tmp;\r\nif (node->tree->attributes & HFS_TREE_VARIDXKEYS)\r\ntmp = (hfs_bnode_read_u8(node, key_off) | 1) + 1;\r\nelse\r\ntmp = node->tree->max_key_len + 1;\r\ndprint(DBG_BNODE_MOD, " (%d,%d", tmp, hfs_bnode_read_u8(node, key_off));\r\nhfs_bnode_read(node, &cnid, key_off + tmp, 4);\r\ndprint(DBG_BNODE_MOD, ",%d)", be32_to_cpu(cnid));\r\n} else if (i && node->type == HFS_NODE_LEAF) {\r\nint tmp;\r\ntmp = hfs_bnode_read_u8(node, key_off);\r\ndprint(DBG_BNODE_MOD, " (%d)", tmp);\r\n}\r\n}\r\ndprint(DBG_BNODE_MOD, "\n");\r\n}\r\nvoid hfs_bnode_unlink(struct hfs_bnode *node)\r\n{\r\nstruct hfs_btree *tree;\r\nstruct hfs_bnode *tmp;\r\n__be32 cnid;\r\ntree = node->tree;\r\nif (node->prev) {\r\ntmp = hfs_bnode_find(tree, node->prev);\r\nif (IS_ERR(tmp))\r\nreturn;\r\ntmp->next = node->next;\r\ncnid = cpu_to_be32(tmp->next);\r\nhfs_bnode_write(tmp, &cnid, offsetof(struct hfs_bnode_desc, next), 4);\r\nhfs_bnode_put(tmp);\r\n} else if (node->type == HFS_NODE_LEAF)\r\ntree->leaf_head = node->next;\r\nif (node->next) {\r\ntmp = hfs_bnode_find(tree, node->next);\r\nif (IS_ERR(tmp))\r\nreturn;\r\ntmp->prev = node->prev;\r\ncnid = cpu_to_be32(tmp->prev);\r\nhfs_bnode_write(tmp, &cnid, offsetof(struct hfs_bnode_desc, prev), 4);\r\nhfs_bnode_put(tmp);\r\n} else if (node->type == HFS_NODE_LEAF)\r\ntree->leaf_tail = node->prev;\r\nif (!node->prev && !node->next) {\r\nprintk(KERN_DEBUG "hfs_btree_del_level\n");\r\n}\r\nif (!node->parent) {\r\ntree->root = 0;\r\ntree->depth = 0;\r\n}\r\nset_bit(HFS_BNODE_DELETED, &node->flags);\r\n}\r\nstatic inline int hfs_bnode_hash(u32 num)\r\n{\r\nnum = (num >> 16) + num;\r\nnum += num >> 8;\r\nreturn num & (NODE_HASH_SIZE - 1);\r\n}\r\nstruct hfs_bnode *hfs_bnode_findhash(struct hfs_btree *tree, u32 cnid)\r\n{\r\nstruct hfs_bnode *node;\r\nif (cnid >= tree->node_count) {\r\nprintk(KERN_ERR "hfs: request for non-existent node %d in B*Tree\n", cnid);\r\nreturn NULL;\r\n}\r\nfor (node = tree->node_hash[hfs_bnode_hash(cnid)];\r\nnode; node = node->next_hash) {\r\nif (node->this == cnid) {\r\nreturn node;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct hfs_bnode *__hfs_bnode_create(struct hfs_btree *tree, u32 cnid)\r\n{\r\nstruct super_block *sb;\r\nstruct hfs_bnode *node, *node2;\r\nstruct address_space *mapping;\r\nstruct page *page;\r\nint size, block, i, hash;\r\nloff_t off;\r\nif (cnid >= tree->node_count) {\r\nprintk(KERN_ERR "hfs: request for non-existent node %d in B*Tree\n", cnid);\r\nreturn NULL;\r\n}\r\nsb = tree->inode->i_sb;\r\nsize = sizeof(struct hfs_bnode) + tree->pages_per_bnode *\r\nsizeof(struct page *);\r\nnode = kzalloc(size, GFP_KERNEL);\r\nif (!node)\r\nreturn NULL;\r\nnode->tree = tree;\r\nnode->this = cnid;\r\nset_bit(HFS_BNODE_NEW, &node->flags);\r\natomic_set(&node->refcnt, 1);\r\ndprint(DBG_BNODE_REFS, "new_node(%d:%d): 1\n",\r\nnode->tree->cnid, node->this);\r\ninit_waitqueue_head(&node->lock_wq);\r\nspin_lock(&tree->hash_lock);\r\nnode2 = hfs_bnode_findhash(tree, cnid);\r\nif (!node2) {\r\nhash = hfs_bnode_hash(cnid);\r\nnode->next_hash = tree->node_hash[hash];\r\ntree->node_hash[hash] = node;\r\ntree->node_hash_cnt++;\r\n} else {\r\nspin_unlock(&tree->hash_lock);\r\nkfree(node);\r\nwait_event(node2->lock_wq, !test_bit(HFS_BNODE_NEW, &node2->flags));\r\nreturn node2;\r\n}\r\nspin_unlock(&tree->hash_lock);\r\nmapping = tree->inode->i_mapping;\r\noff = (loff_t)cnid * tree->node_size;\r\nblock = off >> PAGE_CACHE_SHIFT;\r\nnode->page_offset = off & ~PAGE_CACHE_MASK;\r\nfor (i = 0; i < tree->pages_per_bnode; i++) {\r\npage = read_mapping_page(mapping, block++, NULL);\r\nif (IS_ERR(page))\r\ngoto fail;\r\nif (PageError(page)) {\r\npage_cache_release(page);\r\ngoto fail;\r\n}\r\npage_cache_release(page);\r\nnode->page[i] = page;\r\n}\r\nreturn node;\r\nfail:\r\nset_bit(HFS_BNODE_ERROR, &node->flags);\r\nreturn node;\r\n}\r\nvoid hfs_bnode_unhash(struct hfs_bnode *node)\r\n{\r\nstruct hfs_bnode **p;\r\ndprint(DBG_BNODE_REFS, "remove_node(%d:%d): %d\n",\r\nnode->tree->cnid, node->this, atomic_read(&node->refcnt));\r\nfor (p = &node->tree->node_hash[hfs_bnode_hash(node->this)];\r\n*p && *p != node; p = &(*p)->next_hash)\r\n;\r\nBUG_ON(!*p);\r\n*p = node->next_hash;\r\nnode->tree->node_hash_cnt--;\r\n}\r\nstruct hfs_bnode *hfs_bnode_find(struct hfs_btree *tree, u32 num)\r\n{\r\nstruct hfs_bnode *node;\r\nstruct hfs_bnode_desc *desc;\r\nint i, rec_off, off, next_off;\r\nint entry_size, key_size;\r\nspin_lock(&tree->hash_lock);\r\nnode = hfs_bnode_findhash(tree, num);\r\nif (node) {\r\nhfs_bnode_get(node);\r\nspin_unlock(&tree->hash_lock);\r\nwait_event(node->lock_wq, !test_bit(HFS_BNODE_NEW, &node->flags));\r\nif (test_bit(HFS_BNODE_ERROR, &node->flags))\r\ngoto node_error;\r\nreturn node;\r\n}\r\nspin_unlock(&tree->hash_lock);\r\nnode = __hfs_bnode_create(tree, num);\r\nif (!node)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (test_bit(HFS_BNODE_ERROR, &node->flags))\r\ngoto node_error;\r\nif (!test_bit(HFS_BNODE_NEW, &node->flags))\r\nreturn node;\r\ndesc = (struct hfs_bnode_desc *)(kmap(node->page[0]) + node->page_offset);\r\nnode->prev = be32_to_cpu(desc->prev);\r\nnode->next = be32_to_cpu(desc->next);\r\nnode->num_recs = be16_to_cpu(desc->num_recs);\r\nnode->type = desc->type;\r\nnode->height = desc->height;\r\nkunmap(node->page[0]);\r\nswitch (node->type) {\r\ncase HFS_NODE_HEADER:\r\ncase HFS_NODE_MAP:\r\nif (node->height != 0)\r\ngoto node_error;\r\nbreak;\r\ncase HFS_NODE_LEAF:\r\nif (node->height != 1)\r\ngoto node_error;\r\nbreak;\r\ncase HFS_NODE_INDEX:\r\nif (node->height <= 1 || node->height > tree->depth)\r\ngoto node_error;\r\nbreak;\r\ndefault:\r\ngoto node_error;\r\n}\r\nrec_off = tree->node_size - 2;\r\noff = hfs_bnode_read_u16(node, rec_off);\r\nif (off != sizeof(struct hfs_bnode_desc))\r\ngoto node_error;\r\nfor (i = 1; i <= node->num_recs; off = next_off, i++) {\r\nrec_off -= 2;\r\nnext_off = hfs_bnode_read_u16(node, rec_off);\r\nif (next_off <= off ||\r\nnext_off > tree->node_size ||\r\nnext_off & 1)\r\ngoto node_error;\r\nentry_size = next_off - off;\r\nif (node->type != HFS_NODE_INDEX &&\r\nnode->type != HFS_NODE_LEAF)\r\ncontinue;\r\nkey_size = hfs_bnode_read_u8(node, off) + 1;\r\nif (key_size >= entry_size )\r\ngoto node_error;\r\n}\r\nclear_bit(HFS_BNODE_NEW, &node->flags);\r\nwake_up(&node->lock_wq);\r\nreturn node;\r\nnode_error:\r\nset_bit(HFS_BNODE_ERROR, &node->flags);\r\nclear_bit(HFS_BNODE_NEW, &node->flags);\r\nwake_up(&node->lock_wq);\r\nhfs_bnode_put(node);\r\nreturn ERR_PTR(-EIO);\r\n}\r\nvoid hfs_bnode_free(struct hfs_bnode *node)\r\n{\r\nkfree(node);\r\n}\r\nstruct hfs_bnode *hfs_bnode_create(struct hfs_btree *tree, u32 num)\r\n{\r\nstruct hfs_bnode *node;\r\nstruct page **pagep;\r\nint i;\r\nspin_lock(&tree->hash_lock);\r\nnode = hfs_bnode_findhash(tree, num);\r\nspin_unlock(&tree->hash_lock);\r\nBUG_ON(node);\r\nnode = __hfs_bnode_create(tree, num);\r\nif (!node)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (test_bit(HFS_BNODE_ERROR, &node->flags)) {\r\nhfs_bnode_put(node);\r\nreturn ERR_PTR(-EIO);\r\n}\r\npagep = node->page;\r\nmemset(kmap(*pagep) + node->page_offset, 0,\r\nmin((int)PAGE_CACHE_SIZE, (int)tree->node_size));\r\nset_page_dirty(*pagep);\r\nkunmap(*pagep);\r\nfor (i = 1; i < tree->pages_per_bnode; i++) {\r\nmemset(kmap(*++pagep), 0, PAGE_CACHE_SIZE);\r\nset_page_dirty(*pagep);\r\nkunmap(*pagep);\r\n}\r\nclear_bit(HFS_BNODE_NEW, &node->flags);\r\nwake_up(&node->lock_wq);\r\nreturn node;\r\n}\r\nvoid hfs_bnode_get(struct hfs_bnode *node)\r\n{\r\nif (node) {\r\natomic_inc(&node->refcnt);\r\ndprint(DBG_BNODE_REFS, "get_node(%d:%d): %d\n",\r\nnode->tree->cnid, node->this, atomic_read(&node->refcnt));\r\n}\r\n}\r\nvoid hfs_bnode_put(struct hfs_bnode *node)\r\n{\r\nif (node) {\r\nstruct hfs_btree *tree = node->tree;\r\nint i;\r\ndprint(DBG_BNODE_REFS, "put_node(%d:%d): %d\n",\r\nnode->tree->cnid, node->this, atomic_read(&node->refcnt));\r\nBUG_ON(!atomic_read(&node->refcnt));\r\nif (!atomic_dec_and_lock(&node->refcnt, &tree->hash_lock))\r\nreturn;\r\nfor (i = 0; i < tree->pages_per_bnode; i++) {\r\nif (!node->page[i])\r\ncontinue;\r\nmark_page_accessed(node->page[i]);\r\n}\r\nif (test_bit(HFS_BNODE_DELETED, &node->flags)) {\r\nhfs_bnode_unhash(node);\r\nspin_unlock(&tree->hash_lock);\r\nhfs_bmap_free(node);\r\nhfs_bnode_free(node);\r\nreturn;\r\n}\r\nspin_unlock(&tree->hash_lock);\r\n}\r\n}
