static void scrub_free_csums(struct scrub_dev *sdev)\r\n{\r\nwhile (!list_empty(&sdev->csum_list)) {\r\nstruct btrfs_ordered_sum *sum;\r\nsum = list_first_entry(&sdev->csum_list,\r\nstruct btrfs_ordered_sum, list);\r\nlist_del(&sum->list);\r\nkfree(sum);\r\n}\r\n}\r\nstatic void scrub_free_bio(struct bio *bio)\r\n{\r\nint i;\r\nstruct page *last_page = NULL;\r\nif (!bio)\r\nreturn;\r\nfor (i = 0; i < bio->bi_vcnt; ++i) {\r\nif (bio->bi_io_vec[i].bv_page == last_page)\r\ncontinue;\r\nlast_page = bio->bi_io_vec[i].bv_page;\r\n__free_page(last_page);\r\n}\r\nbio_put(bio);\r\n}\r\nstatic noinline_for_stack void scrub_free_dev(struct scrub_dev *sdev)\r\n{\r\nint i;\r\nif (!sdev)\r\nreturn;\r\nfor (i = 0; i < SCRUB_BIOS_PER_DEV; ++i) {\r\nstruct scrub_bio *sbio = sdev->bios[i];\r\nif (!sbio)\r\nbreak;\r\nscrub_free_bio(sbio->bio);\r\nkfree(sbio);\r\n}\r\nscrub_free_csums(sdev);\r\nkfree(sdev);\r\n}\r\nvoid scrub_recheck_error(struct scrub_bio *sbio, int ix)\r\n{\r\nif (sbio->err) {\r\nif (scrub_fixup_io(READ, sbio->sdev->dev->bdev,\r\n(sbio->physical + ix * PAGE_SIZE) >> 9,\r\nsbio->bio->bi_io_vec[ix].bv_page) == 0) {\r\nif (scrub_fixup_check(sbio, ix) == 0)\r\nreturn;\r\n}\r\n}\r\nscrub_fixup(sbio, ix);\r\n}\r\nstatic int scrub_fixup_check(struct scrub_bio *sbio, int ix)\r\n{\r\nint ret = 1;\r\nstruct page *page;\r\nvoid *buffer;\r\nu64 flags = sbio->spag[ix].flags;\r\npage = sbio->bio->bi_io_vec[ix].bv_page;\r\nbuffer = kmap_atomic(page, KM_USER0);\r\nif (flags & BTRFS_EXTENT_FLAG_DATA) {\r\nret = scrub_checksum_data(sbio->sdev,\r\nsbio->spag + ix, buffer);\r\n} else if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\r\nret = scrub_checksum_tree_block(sbio->sdev,\r\nsbio->spag + ix,\r\nsbio->logical + ix * PAGE_SIZE,\r\nbuffer);\r\n} else {\r\nWARN_ON(1);\r\n}\r\nkunmap_atomic(buffer, KM_USER0);\r\nreturn ret;\r\n}\r\nstatic void scrub_fixup_end_io(struct bio *bio, int err)\r\n{\r\ncomplete((struct completion *)bio->bi_private);\r\n}\r\nstatic void scrub_fixup(struct scrub_bio *sbio, int ix)\r\n{\r\nstruct scrub_dev *sdev = sbio->sdev;\r\nstruct btrfs_fs_info *fs_info = sdev->dev->dev_root->fs_info;\r\nstruct btrfs_mapping_tree *map_tree = &fs_info->mapping_tree;\r\nstruct btrfs_multi_bio *multi = NULL;\r\nu64 logical = sbio->logical + ix * PAGE_SIZE;\r\nu64 length;\r\nint i;\r\nint ret;\r\nDECLARE_COMPLETION_ONSTACK(complete);\r\nif ((sbio->spag[ix].flags & BTRFS_EXTENT_FLAG_DATA) &&\r\n(sbio->spag[ix].have_csum == 0)) {\r\ngoto uncorrectable;\r\n}\r\nlength = PAGE_SIZE;\r\nret = btrfs_map_block(map_tree, REQ_WRITE, logical, &length,\r\n&multi, 0);\r\nif (ret || !multi || length < PAGE_SIZE) {\r\nprintk(KERN_ERR\r\n"scrub_fixup: btrfs_map_block failed us for %llu\n",\r\n(unsigned long long)logical);\r\nWARN_ON(1);\r\nreturn;\r\n}\r\nif (multi->num_stripes == 1)\r\ngoto uncorrectable;\r\nfor (i = 0; i < multi->num_stripes; ++i) {\r\nif (i == sbio->spag[ix].mirror_num)\r\ncontinue;\r\nif (scrub_fixup_io(READ, multi->stripes[i].dev->bdev,\r\nmulti->stripes[i].physical >> 9,\r\nsbio->bio->bi_io_vec[ix].bv_page)) {\r\ncontinue;\r\n}\r\nif (scrub_fixup_check(sbio, ix) == 0)\r\nbreak;\r\n}\r\nif (i == multi->num_stripes)\r\ngoto uncorrectable;\r\nif (!sdev->readonly) {\r\nif (scrub_fixup_io(WRITE, sdev->dev->bdev,\r\n(sbio->physical + ix * PAGE_SIZE) >> 9,\r\nsbio->bio->bi_io_vec[ix].bv_page)) {\r\ngoto uncorrectable;\r\n}\r\n}\r\nkfree(multi);\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.corrected_errors;\r\nspin_unlock(&sdev->stat_lock);\r\nif (printk_ratelimit())\r\nprintk(KERN_ERR "btrfs: fixed up at %llu\n",\r\n(unsigned long long)logical);\r\nreturn;\r\nuncorrectable:\r\nkfree(multi);\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.uncorrectable_errors;\r\nspin_unlock(&sdev->stat_lock);\r\nif (printk_ratelimit())\r\nprintk(KERN_ERR "btrfs: unable to fixup at %llu\n",\r\n(unsigned long long)logical);\r\n}\r\nstatic int scrub_fixup_io(int rw, struct block_device *bdev, sector_t sector,\r\nstruct page *page)\r\n{\r\nstruct bio *bio = NULL;\r\nint ret;\r\nDECLARE_COMPLETION_ONSTACK(complete);\r\nbio = bio_alloc(GFP_NOFS, 1);\r\nbio->bi_bdev = bdev;\r\nbio->bi_sector = sector;\r\nbio_add_page(bio, page, PAGE_SIZE, 0);\r\nbio->bi_end_io = scrub_fixup_end_io;\r\nbio->bi_private = &complete;\r\nsubmit_bio(rw, bio);\r\nwait_for_completion(&complete);\r\nret = !test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nbio_put(bio);\r\nreturn ret;\r\n}\r\nstatic void scrub_bio_end_io(struct bio *bio, int err)\r\n{\r\nstruct scrub_bio *sbio = bio->bi_private;\r\nstruct scrub_dev *sdev = sbio->sdev;\r\nstruct btrfs_fs_info *fs_info = sdev->dev->dev_root->fs_info;\r\nsbio->err = err;\r\nsbio->bio = bio;\r\nbtrfs_queue_worker(&fs_info->scrub_workers, &sbio->work);\r\n}\r\nstatic void scrub_checksum(struct btrfs_work *work)\r\n{\r\nstruct scrub_bio *sbio = container_of(work, struct scrub_bio, work);\r\nstruct scrub_dev *sdev = sbio->sdev;\r\nstruct page *page;\r\nvoid *buffer;\r\nint i;\r\nu64 flags;\r\nu64 logical;\r\nint ret;\r\nif (sbio->err) {\r\nfor (i = 0; i < sbio->count; ++i)\r\nscrub_recheck_error(sbio, i);\r\nsbio->bio->bi_flags &= ~(BIO_POOL_MASK - 1);\r\nsbio->bio->bi_flags |= 1 << BIO_UPTODATE;\r\nsbio->bio->bi_phys_segments = 0;\r\nsbio->bio->bi_idx = 0;\r\nfor (i = 0; i < sbio->count; i++) {\r\nstruct bio_vec *bi;\r\nbi = &sbio->bio->bi_io_vec[i];\r\nbi->bv_offset = 0;\r\nbi->bv_len = PAGE_SIZE;\r\n}\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.read_errors;\r\nspin_unlock(&sdev->stat_lock);\r\ngoto out;\r\n}\r\nfor (i = 0; i < sbio->count; ++i) {\r\npage = sbio->bio->bi_io_vec[i].bv_page;\r\nbuffer = kmap_atomic(page, KM_USER0);\r\nflags = sbio->spag[i].flags;\r\nlogical = sbio->logical + i * PAGE_SIZE;\r\nret = 0;\r\nif (flags & BTRFS_EXTENT_FLAG_DATA) {\r\nret = scrub_checksum_data(sdev, sbio->spag + i, buffer);\r\n} else if (flags & BTRFS_EXTENT_FLAG_TREE_BLOCK) {\r\nret = scrub_checksum_tree_block(sdev, sbio->spag + i,\r\nlogical, buffer);\r\n} else if (flags & BTRFS_EXTENT_FLAG_SUPER) {\r\nBUG_ON(i);\r\n(void)scrub_checksum_super(sbio, buffer);\r\n} else {\r\nWARN_ON(1);\r\n}\r\nkunmap_atomic(buffer, KM_USER0);\r\nif (ret)\r\nscrub_recheck_error(sbio, i);\r\n}\r\nout:\r\nscrub_free_bio(sbio->bio);\r\nsbio->bio = NULL;\r\nspin_lock(&sdev->list_lock);\r\nsbio->next_free = sdev->first_free;\r\nsdev->first_free = sbio->index;\r\nspin_unlock(&sdev->list_lock);\r\natomic_dec(&sdev->in_flight);\r\nwake_up(&sdev->list_wait);\r\n}\r\nstatic int scrub_checksum_data(struct scrub_dev *sdev,\r\nstruct scrub_page *spag, void *buffer)\r\n{\r\nu8 csum[BTRFS_CSUM_SIZE];\r\nu32 crc = ~(u32)0;\r\nint fail = 0;\r\nstruct btrfs_root *root = sdev->dev->dev_root;\r\nif (!spag->have_csum)\r\nreturn 0;\r\ncrc = btrfs_csum_data(root, buffer, crc, PAGE_SIZE);\r\nbtrfs_csum_final(crc, csum);\r\nif (memcmp(csum, spag->csum, sdev->csum_size))\r\nfail = 1;\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.data_extents_scrubbed;\r\nsdev->stat.data_bytes_scrubbed += PAGE_SIZE;\r\nif (fail)\r\n++sdev->stat.csum_errors;\r\nspin_unlock(&sdev->stat_lock);\r\nreturn fail;\r\n}\r\nstatic int scrub_checksum_tree_block(struct scrub_dev *sdev,\r\nstruct scrub_page *spag, u64 logical,\r\nvoid *buffer)\r\n{\r\nstruct btrfs_header *h;\r\nstruct btrfs_root *root = sdev->dev->dev_root;\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nu8 csum[BTRFS_CSUM_SIZE];\r\nu32 crc = ~(u32)0;\r\nint fail = 0;\r\nint crc_fail = 0;\r\nh = (struct btrfs_header *)buffer;\r\nif (logical != le64_to_cpu(h->bytenr))\r\n++fail;\r\nif (spag->generation != le64_to_cpu(h->generation))\r\n++fail;\r\nif (memcmp(h->fsid, fs_info->fsid, BTRFS_UUID_SIZE))\r\n++fail;\r\nif (memcmp(h->chunk_tree_uuid, fs_info->chunk_tree_uuid,\r\nBTRFS_UUID_SIZE))\r\n++fail;\r\ncrc = btrfs_csum_data(root, buffer + BTRFS_CSUM_SIZE, crc,\r\nPAGE_SIZE - BTRFS_CSUM_SIZE);\r\nbtrfs_csum_final(crc, csum);\r\nif (memcmp(csum, h->csum, sdev->csum_size))\r\n++crc_fail;\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.tree_extents_scrubbed;\r\nsdev->stat.tree_bytes_scrubbed += PAGE_SIZE;\r\nif (crc_fail)\r\n++sdev->stat.csum_errors;\r\nif (fail)\r\n++sdev->stat.verify_errors;\r\nspin_unlock(&sdev->stat_lock);\r\nreturn fail || crc_fail;\r\n}\r\nstatic int scrub_checksum_super(struct scrub_bio *sbio, void *buffer)\r\n{\r\nstruct btrfs_super_block *s;\r\nu64 logical;\r\nstruct scrub_dev *sdev = sbio->sdev;\r\nstruct btrfs_root *root = sdev->dev->dev_root;\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nu8 csum[BTRFS_CSUM_SIZE];\r\nu32 crc = ~(u32)0;\r\nint fail = 0;\r\ns = (struct btrfs_super_block *)buffer;\r\nlogical = sbio->logical;\r\nif (logical != le64_to_cpu(s->bytenr))\r\n++fail;\r\nif (sbio->spag[0].generation != le64_to_cpu(s->generation))\r\n++fail;\r\nif (memcmp(s->fsid, fs_info->fsid, BTRFS_UUID_SIZE))\r\n++fail;\r\ncrc = btrfs_csum_data(root, buffer + BTRFS_CSUM_SIZE, crc,\r\nPAGE_SIZE - BTRFS_CSUM_SIZE);\r\nbtrfs_csum_final(crc, csum);\r\nif (memcmp(csum, s->csum, sbio->sdev->csum_size))\r\n++fail;\r\nif (fail) {\r\nspin_lock(&sdev->stat_lock);\r\n++sdev->stat.super_errors;\r\nspin_unlock(&sdev->stat_lock);\r\n}\r\nreturn fail;\r\n}\r\nstatic int scrub_submit(struct scrub_dev *sdev)\r\n{\r\nstruct scrub_bio *sbio;\r\nstruct bio *bio;\r\nint i;\r\nif (sdev->curr == -1)\r\nreturn 0;\r\nsbio = sdev->bios[sdev->curr];\r\nbio = bio_alloc(GFP_NOFS, sbio->count);\r\nif (!bio)\r\ngoto nomem;\r\nbio->bi_private = sbio;\r\nbio->bi_end_io = scrub_bio_end_io;\r\nbio->bi_bdev = sdev->dev->bdev;\r\nbio->bi_sector = sbio->physical >> 9;\r\nfor (i = 0; i < sbio->count; ++i) {\r\nstruct page *page;\r\nint ret;\r\npage = alloc_page(GFP_NOFS);\r\nif (!page)\r\ngoto nomem;\r\nret = bio_add_page(bio, page, PAGE_SIZE, 0);\r\nif (!ret) {\r\n__free_page(page);\r\ngoto nomem;\r\n}\r\n}\r\nsbio->err = 0;\r\nsdev->curr = -1;\r\natomic_inc(&sdev->in_flight);\r\nsubmit_bio(READ, bio);\r\nreturn 0;\r\nnomem:\r\nscrub_free_bio(bio);\r\nreturn -ENOMEM;\r\n}\r\nstatic int scrub_page(struct scrub_dev *sdev, u64 logical, u64 len,\r\nu64 physical, u64 flags, u64 gen, u64 mirror_num,\r\nu8 *csum, int force)\r\n{\r\nstruct scrub_bio *sbio;\r\nagain:\r\nwhile (sdev->curr == -1) {\r\nspin_lock(&sdev->list_lock);\r\nsdev->curr = sdev->first_free;\r\nif (sdev->curr != -1) {\r\nsdev->first_free = sdev->bios[sdev->curr]->next_free;\r\nsdev->bios[sdev->curr]->next_free = -1;\r\nsdev->bios[sdev->curr]->count = 0;\r\nspin_unlock(&sdev->list_lock);\r\n} else {\r\nspin_unlock(&sdev->list_lock);\r\nwait_event(sdev->list_wait, sdev->first_free != -1);\r\n}\r\n}\r\nsbio = sdev->bios[sdev->curr];\r\nif (sbio->count == 0) {\r\nsbio->physical = physical;\r\nsbio->logical = logical;\r\n} else if (sbio->physical + sbio->count * PAGE_SIZE != physical ||\r\nsbio->logical + sbio->count * PAGE_SIZE != logical) {\r\nint ret;\r\nret = scrub_submit(sdev);\r\nif (ret)\r\nreturn ret;\r\ngoto again;\r\n}\r\nsbio->spag[sbio->count].flags = flags;\r\nsbio->spag[sbio->count].generation = gen;\r\nsbio->spag[sbio->count].have_csum = 0;\r\nsbio->spag[sbio->count].mirror_num = mirror_num;\r\nif (csum) {\r\nsbio->spag[sbio->count].have_csum = 1;\r\nmemcpy(sbio->spag[sbio->count].csum, csum, sdev->csum_size);\r\n}\r\n++sbio->count;\r\nif (sbio->count == SCRUB_PAGES_PER_BIO || force) {\r\nint ret;\r\nret = scrub_submit(sdev);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int scrub_find_csum(struct scrub_dev *sdev, u64 logical, u64 len,\r\nu8 *csum)\r\n{\r\nstruct btrfs_ordered_sum *sum = NULL;\r\nint ret = 0;\r\nunsigned long i;\r\nunsigned long num_sectors;\r\nu32 sectorsize = sdev->dev->dev_root->sectorsize;\r\nwhile (!list_empty(&sdev->csum_list)) {\r\nsum = list_first_entry(&sdev->csum_list,\r\nstruct btrfs_ordered_sum, list);\r\nif (sum->bytenr > logical)\r\nreturn 0;\r\nif (sum->bytenr + sum->len > logical)\r\nbreak;\r\n++sdev->stat.csum_discards;\r\nlist_del(&sum->list);\r\nkfree(sum);\r\nsum = NULL;\r\n}\r\nif (!sum)\r\nreturn 0;\r\nnum_sectors = sum->len / sectorsize;\r\nfor (i = 0; i < num_sectors; ++i) {\r\nif (sum->sums[i].bytenr == logical) {\r\nmemcpy(csum, &sum->sums[i].sum, sdev->csum_size);\r\nret = 1;\r\nbreak;\r\n}\r\n}\r\nif (ret && i == num_sectors - 1) {\r\nlist_del(&sum->list);\r\nkfree(sum);\r\n}\r\nreturn ret;\r\n}\r\nstatic int scrub_extent(struct scrub_dev *sdev, u64 logical, u64 len,\r\nu64 physical, u64 flags, u64 gen, u64 mirror_num)\r\n{\r\nint ret;\r\nu8 csum[BTRFS_CSUM_SIZE];\r\nwhile (len) {\r\nu64 l = min_t(u64, len, PAGE_SIZE);\r\nint have_csum = 0;\r\nif (flags & BTRFS_EXTENT_FLAG_DATA) {\r\nhave_csum = scrub_find_csum(sdev, logical, l, csum);\r\nif (have_csum == 0)\r\n++sdev->stat.no_csum;\r\n}\r\nret = scrub_page(sdev, logical, l, physical, flags, gen,\r\nmirror_num, have_csum ? csum : NULL, 0);\r\nif (ret)\r\nreturn ret;\r\nlen -= l;\r\nlogical += l;\r\nphysical += l;\r\n}\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack int scrub_stripe(struct scrub_dev *sdev,\r\nstruct map_lookup *map, int num, u64 base, u64 length)\r\n{\r\nstruct btrfs_path *path;\r\nstruct btrfs_fs_info *fs_info = sdev->dev->dev_root->fs_info;\r\nstruct btrfs_root *root = fs_info->extent_root;\r\nstruct btrfs_root *csum_root = fs_info->csum_root;\r\nstruct btrfs_extent_item *extent;\r\nstruct blk_plug plug;\r\nu64 flags;\r\nint ret;\r\nint slot;\r\nint i;\r\nu64 nstripes;\r\nint start_stripe;\r\nstruct extent_buffer *l;\r\nstruct btrfs_key key;\r\nu64 physical;\r\nu64 logical;\r\nu64 generation;\r\nu64 mirror_num;\r\nu64 increment = map->stripe_len;\r\nu64 offset;\r\nnstripes = length;\r\noffset = 0;\r\ndo_div(nstripes, map->stripe_len);\r\nif (map->type & BTRFS_BLOCK_GROUP_RAID0) {\r\noffset = map->stripe_len * num;\r\nincrement = map->stripe_len * map->num_stripes;\r\nmirror_num = 0;\r\n} else if (map->type & BTRFS_BLOCK_GROUP_RAID10) {\r\nint factor = map->num_stripes / map->sub_stripes;\r\noffset = map->stripe_len * (num / map->sub_stripes);\r\nincrement = map->stripe_len * factor;\r\nmirror_num = num % map->sub_stripes;\r\n} else if (map->type & BTRFS_BLOCK_GROUP_RAID1) {\r\nincrement = map->stripe_len;\r\nmirror_num = num % map->num_stripes;\r\n} else if (map->type & BTRFS_BLOCK_GROUP_DUP) {\r\nincrement = map->stripe_len;\r\nmirror_num = num % map->num_stripes;\r\n} else {\r\nincrement = map->stripe_len;\r\nmirror_num = 0;\r\n}\r\npath = btrfs_alloc_path();\r\nif (!path)\r\nreturn -ENOMEM;\r\npath->reada = 2;\r\npath->search_commit_root = 1;\r\npath->skip_locking = 1;\r\nlogical = base + offset;\r\nphysical = map->stripes[num].physical;\r\nret = 0;\r\nfor (i = 0; i < nstripes; ++i) {\r\nkey.objectid = logical;\r\nkey.type = BTRFS_EXTENT_ITEM_KEY;\r\nkey.offset = (u64)0;\r\nret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\r\nif (ret < 0)\r\ngoto out_noplug;\r\nwhile (1) {\r\nl = path->nodes[0];\r\nslot = path->slots[0];\r\nif (slot >= btrfs_header_nritems(l)) {\r\nret = btrfs_next_leaf(root, path);\r\nif (ret == 0)\r\ncontinue;\r\nif (ret < 0)\r\ngoto out_noplug;\r\nbreak;\r\n}\r\nbtrfs_item_key_to_cpu(l, &key, slot);\r\nif (key.objectid >= logical + map->stripe_len)\r\nbreak;\r\npath->slots[0]++;\r\n}\r\nbtrfs_release_path(path);\r\nlogical += increment;\r\nphysical += map->stripe_len;\r\ncond_resched();\r\n}\r\nstart_stripe = 0;\r\nblk_start_plug(&plug);\r\nagain:\r\nlogical = base + offset + start_stripe * increment;\r\nfor (i = start_stripe; i < nstripes; ++i) {\r\nret = btrfs_lookup_csums_range(csum_root, logical,\r\nlogical + map->stripe_len - 1,\r\n&sdev->csum_list, 1);\r\nif (ret)\r\ngoto out;\r\nlogical += increment;\r\ncond_resched();\r\n}\r\nlogical = base + offset + start_stripe * increment;\r\nphysical = map->stripes[num].physical + start_stripe * map->stripe_len;\r\nret = 0;\r\nfor (i = start_stripe; i < nstripes; ++i) {\r\nif (atomic_read(&fs_info->scrub_cancel_req) ||\r\natomic_read(&sdev->cancel_req)) {\r\nret = -ECANCELED;\r\ngoto out;\r\n}\r\nif (atomic_read(&fs_info->scrub_pause_req)) {\r\nscrub_submit(sdev);\r\nwait_event(sdev->list_wait,\r\natomic_read(&sdev->in_flight) == 0);\r\natomic_inc(&fs_info->scrubs_paused);\r\nwake_up(&fs_info->scrub_pause_wait);\r\nmutex_lock(&fs_info->scrub_lock);\r\nwhile (atomic_read(&fs_info->scrub_pause_req)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwait_event(fs_info->scrub_pause_wait,\r\natomic_read(&fs_info->scrub_pause_req) == 0);\r\nmutex_lock(&fs_info->scrub_lock);\r\n}\r\natomic_dec(&fs_info->scrubs_paused);\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwake_up(&fs_info->scrub_pause_wait);\r\nscrub_free_csums(sdev);\r\nstart_stripe = i;\r\ngoto again;\r\n}\r\nkey.objectid = logical;\r\nkey.type = BTRFS_EXTENT_ITEM_KEY;\r\nkey.offset = (u64)0;\r\nret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\r\nif (ret < 0)\r\ngoto out;\r\nif (ret > 0) {\r\nret = btrfs_previous_item(root, path, 0,\r\nBTRFS_EXTENT_ITEM_KEY);\r\nif (ret < 0)\r\ngoto out;\r\nif (ret > 0) {\r\nbtrfs_release_path(path);\r\nret = btrfs_search_slot(NULL, root, &key,\r\npath, 0, 0);\r\nif (ret < 0)\r\ngoto out;\r\n}\r\n}\r\nwhile (1) {\r\nl = path->nodes[0];\r\nslot = path->slots[0];\r\nif (slot >= btrfs_header_nritems(l)) {\r\nret = btrfs_next_leaf(root, path);\r\nif (ret == 0)\r\ncontinue;\r\nif (ret < 0)\r\ngoto out;\r\nbreak;\r\n}\r\nbtrfs_item_key_to_cpu(l, &key, slot);\r\nif (key.objectid + key.offset <= logical)\r\ngoto next;\r\nif (key.objectid >= logical + map->stripe_len)\r\nbreak;\r\nif (btrfs_key_type(&key) != BTRFS_EXTENT_ITEM_KEY)\r\ngoto next;\r\nextent = btrfs_item_ptr(l, slot,\r\nstruct btrfs_extent_item);\r\nflags = btrfs_extent_flags(l, extent);\r\ngeneration = btrfs_extent_generation(l, extent);\r\nif (key.objectid < logical &&\r\n(flags & BTRFS_EXTENT_FLAG_TREE_BLOCK)) {\r\nprintk(KERN_ERR\r\n"btrfs scrub: tree block %llu spanning "\r\n"stripes, ignored. logical=%llu\n",\r\n(unsigned long long)key.objectid,\r\n(unsigned long long)logical);\r\ngoto next;\r\n}\r\nif (key.objectid < logical) {\r\nkey.offset -= logical - key.objectid;\r\nkey.objectid = logical;\r\n}\r\nif (key.objectid + key.offset >\r\nlogical + map->stripe_len) {\r\nkey.offset = logical + map->stripe_len -\r\nkey.objectid;\r\n}\r\nret = scrub_extent(sdev, key.objectid, key.offset,\r\nkey.objectid - logical + physical,\r\nflags, generation, mirror_num);\r\nif (ret)\r\ngoto out;\r\nnext:\r\npath->slots[0]++;\r\n}\r\nbtrfs_release_path(path);\r\nlogical += increment;\r\nphysical += map->stripe_len;\r\nspin_lock(&sdev->stat_lock);\r\nsdev->stat.last_physical = physical;\r\nspin_unlock(&sdev->stat_lock);\r\n}\r\nscrub_submit(sdev);\r\nout:\r\nblk_finish_plug(&plug);\r\nout_noplug:\r\nbtrfs_free_path(path);\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic noinline_for_stack int scrub_chunk(struct scrub_dev *sdev,\r\nu64 chunk_tree, u64 chunk_objectid, u64 chunk_offset, u64 length)\r\n{\r\nstruct btrfs_mapping_tree *map_tree =\r\n&sdev->dev->dev_root->fs_info->mapping_tree;\r\nstruct map_lookup *map;\r\nstruct extent_map *em;\r\nint i;\r\nint ret = -EINVAL;\r\nread_lock(&map_tree->map_tree.lock);\r\nem = lookup_extent_mapping(&map_tree->map_tree, chunk_offset, 1);\r\nread_unlock(&map_tree->map_tree.lock);\r\nif (!em)\r\nreturn -EINVAL;\r\nmap = (struct map_lookup *)em->bdev;\r\nif (em->start != chunk_offset)\r\ngoto out;\r\nif (em->len < length)\r\ngoto out;\r\nfor (i = 0; i < map->num_stripes; ++i) {\r\nif (map->stripes[i].dev == sdev->dev) {\r\nret = scrub_stripe(sdev, map, i, chunk_offset, length);\r\nif (ret)\r\ngoto out;\r\n}\r\n}\r\nout:\r\nfree_extent_map(em);\r\nreturn ret;\r\n}\r\nstatic noinline_for_stack\r\nint scrub_enumerate_chunks(struct scrub_dev *sdev, u64 start, u64 end)\r\n{\r\nstruct btrfs_dev_extent *dev_extent = NULL;\r\nstruct btrfs_path *path;\r\nstruct btrfs_root *root = sdev->dev->dev_root;\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nu64 length;\r\nu64 chunk_tree;\r\nu64 chunk_objectid;\r\nu64 chunk_offset;\r\nint ret;\r\nint slot;\r\nstruct extent_buffer *l;\r\nstruct btrfs_key key;\r\nstruct btrfs_key found_key;\r\nstruct btrfs_block_group_cache *cache;\r\npath = btrfs_alloc_path();\r\nif (!path)\r\nreturn -ENOMEM;\r\npath->reada = 2;\r\npath->search_commit_root = 1;\r\npath->skip_locking = 1;\r\nkey.objectid = sdev->dev->devid;\r\nkey.offset = 0ull;\r\nkey.type = BTRFS_DEV_EXTENT_KEY;\r\nwhile (1) {\r\nret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\r\nif (ret < 0)\r\nbreak;\r\nif (ret > 0) {\r\nif (path->slots[0] >=\r\nbtrfs_header_nritems(path->nodes[0])) {\r\nret = btrfs_next_leaf(root, path);\r\nif (ret)\r\nbreak;\r\n}\r\n}\r\nl = path->nodes[0];\r\nslot = path->slots[0];\r\nbtrfs_item_key_to_cpu(l, &found_key, slot);\r\nif (found_key.objectid != sdev->dev->devid)\r\nbreak;\r\nif (btrfs_key_type(&found_key) != BTRFS_DEV_EXTENT_KEY)\r\nbreak;\r\nif (found_key.offset >= end)\r\nbreak;\r\nif (found_key.offset < key.offset)\r\nbreak;\r\ndev_extent = btrfs_item_ptr(l, slot, struct btrfs_dev_extent);\r\nlength = btrfs_dev_extent_length(l, dev_extent);\r\nif (found_key.offset + length <= start) {\r\nkey.offset = found_key.offset + length;\r\nbtrfs_release_path(path);\r\ncontinue;\r\n}\r\nchunk_tree = btrfs_dev_extent_chunk_tree(l, dev_extent);\r\nchunk_objectid = btrfs_dev_extent_chunk_objectid(l, dev_extent);\r\nchunk_offset = btrfs_dev_extent_chunk_offset(l, dev_extent);\r\ncache = btrfs_lookup_block_group(fs_info, chunk_offset);\r\nif (!cache) {\r\nret = -ENOENT;\r\nbreak;\r\n}\r\nret = scrub_chunk(sdev, chunk_tree, chunk_objectid,\r\nchunk_offset, length);\r\nbtrfs_put_block_group(cache);\r\nif (ret)\r\nbreak;\r\nkey.offset = found_key.offset + length;\r\nbtrfs_release_path(path);\r\n}\r\nbtrfs_free_path(path);\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic noinline_for_stack int scrub_supers(struct scrub_dev *sdev)\r\n{\r\nint i;\r\nu64 bytenr;\r\nu64 gen;\r\nint ret;\r\nstruct btrfs_device *device = sdev->dev;\r\nstruct btrfs_root *root = device->dev_root;\r\ngen = root->fs_info->last_trans_committed;\r\nfor (i = 0; i < BTRFS_SUPER_MIRROR_MAX; i++) {\r\nbytenr = btrfs_sb_offset(i);\r\nif (bytenr + BTRFS_SUPER_INFO_SIZE >= device->total_bytes)\r\nbreak;\r\nret = scrub_page(sdev, bytenr, PAGE_SIZE, bytenr,\r\nBTRFS_EXTENT_FLAG_SUPER, gen, i, NULL, 1);\r\nif (ret)\r\nreturn ret;\r\n}\r\nwait_event(sdev->list_wait, atomic_read(&sdev->in_flight) == 0);\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack int scrub_workers_get(struct btrfs_root *root)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nmutex_lock(&fs_info->scrub_lock);\r\nif (fs_info->scrub_workers_refcnt == 0) {\r\nbtrfs_init_workers(&fs_info->scrub_workers, "scrub",\r\nfs_info->thread_pool_size, &fs_info->generic_worker);\r\nfs_info->scrub_workers.idle_thresh = 4;\r\nbtrfs_start_workers(&fs_info->scrub_workers, 1);\r\n}\r\n++fs_info->scrub_workers_refcnt;\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack void scrub_workers_put(struct btrfs_root *root)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nmutex_lock(&fs_info->scrub_lock);\r\nif (--fs_info->scrub_workers_refcnt == 0)\r\nbtrfs_stop_workers(&fs_info->scrub_workers);\r\nWARN_ON(fs_info->scrub_workers_refcnt < 0);\r\nmutex_unlock(&fs_info->scrub_lock);\r\n}\r\nint btrfs_scrub_dev(struct btrfs_root *root, u64 devid, u64 start, u64 end,\r\nstruct btrfs_scrub_progress *progress, int readonly)\r\n{\r\nstruct scrub_dev *sdev;\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nint ret;\r\nstruct btrfs_device *dev;\r\nif (btrfs_fs_closing(root->fs_info))\r\nreturn -EINVAL;\r\nif (root->sectorsize != PAGE_SIZE ||\r\nroot->sectorsize != root->leafsize ||\r\nroot->sectorsize != root->nodesize) {\r\nprintk(KERN_ERR "btrfs_scrub: size assumptions fail\n");\r\nreturn -EINVAL;\r\n}\r\nret = scrub_workers_get(root);\r\nif (ret)\r\nreturn ret;\r\nmutex_lock(&root->fs_info->fs_devices->device_list_mutex);\r\ndev = btrfs_find_device(root, devid, NULL, NULL);\r\nif (!dev || dev->missing) {\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nscrub_workers_put(root);\r\nreturn -ENODEV;\r\n}\r\nmutex_lock(&fs_info->scrub_lock);\r\nif (!dev->in_fs_metadata) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nscrub_workers_put(root);\r\nreturn -ENODEV;\r\n}\r\nif (dev->scrub_device) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nscrub_workers_put(root);\r\nreturn -EINPROGRESS;\r\n}\r\nsdev = scrub_setup_dev(dev);\r\nif (IS_ERR(sdev)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nscrub_workers_put(root);\r\nreturn PTR_ERR(sdev);\r\n}\r\nsdev->readonly = readonly;\r\ndev->scrub_device = sdev;\r\natomic_inc(&fs_info->scrubs_running);\r\nmutex_unlock(&fs_info->scrub_lock);\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\ndown_read(&fs_info->scrub_super_lock);\r\nret = scrub_supers(sdev);\r\nup_read(&fs_info->scrub_super_lock);\r\nif (!ret)\r\nret = scrub_enumerate_chunks(sdev, start, end);\r\nwait_event(sdev->list_wait, atomic_read(&sdev->in_flight) == 0);\r\natomic_dec(&fs_info->scrubs_running);\r\nwake_up(&fs_info->scrub_pause_wait);\r\nif (progress)\r\nmemcpy(progress, &sdev->stat, sizeof(*progress));\r\nmutex_lock(&fs_info->scrub_lock);\r\ndev->scrub_device = NULL;\r\nmutex_unlock(&fs_info->scrub_lock);\r\nscrub_free_dev(sdev);\r\nscrub_workers_put(root);\r\nreturn ret;\r\n}\r\nint btrfs_scrub_pause(struct btrfs_root *root)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nmutex_lock(&fs_info->scrub_lock);\r\natomic_inc(&fs_info->scrub_pause_req);\r\nwhile (atomic_read(&fs_info->scrubs_paused) !=\r\natomic_read(&fs_info->scrubs_running)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwait_event(fs_info->scrub_pause_wait,\r\natomic_read(&fs_info->scrubs_paused) ==\r\natomic_read(&fs_info->scrubs_running));\r\nmutex_lock(&fs_info->scrub_lock);\r\n}\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn 0;\r\n}\r\nint btrfs_scrub_continue(struct btrfs_root *root)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\natomic_dec(&fs_info->scrub_pause_req);\r\nwake_up(&fs_info->scrub_pause_wait);\r\nreturn 0;\r\n}\r\nint btrfs_scrub_pause_super(struct btrfs_root *root)\r\n{\r\ndown_write(&root->fs_info->scrub_super_lock);\r\nreturn 0;\r\n}\r\nint btrfs_scrub_continue_super(struct btrfs_root *root)\r\n{\r\nup_write(&root->fs_info->scrub_super_lock);\r\nreturn 0;\r\n}\r\nint btrfs_scrub_cancel(struct btrfs_root *root)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nmutex_lock(&fs_info->scrub_lock);\r\nif (!atomic_read(&fs_info->scrubs_running)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn -ENOTCONN;\r\n}\r\natomic_inc(&fs_info->scrub_cancel_req);\r\nwhile (atomic_read(&fs_info->scrubs_running)) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwait_event(fs_info->scrub_pause_wait,\r\natomic_read(&fs_info->scrubs_running) == 0);\r\nmutex_lock(&fs_info->scrub_lock);\r\n}\r\natomic_dec(&fs_info->scrub_cancel_req);\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn 0;\r\n}\r\nint btrfs_scrub_cancel_dev(struct btrfs_root *root, struct btrfs_device *dev)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nstruct scrub_dev *sdev;\r\nmutex_lock(&fs_info->scrub_lock);\r\nsdev = dev->scrub_device;\r\nif (!sdev) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn -ENOTCONN;\r\n}\r\natomic_inc(&sdev->cancel_req);\r\nwhile (dev->scrub_device) {\r\nmutex_unlock(&fs_info->scrub_lock);\r\nwait_event(fs_info->scrub_pause_wait,\r\ndev->scrub_device == NULL);\r\nmutex_lock(&fs_info->scrub_lock);\r\n}\r\nmutex_unlock(&fs_info->scrub_lock);\r\nreturn 0;\r\n}\r\nint btrfs_scrub_cancel_devid(struct btrfs_root *root, u64 devid)\r\n{\r\nstruct btrfs_fs_info *fs_info = root->fs_info;\r\nstruct btrfs_device *dev;\r\nint ret;\r\nmutex_lock(&fs_info->fs_devices->device_list_mutex);\r\ndev = btrfs_find_device(root, devid, NULL, NULL);\r\nif (!dev) {\r\nmutex_unlock(&fs_info->fs_devices->device_list_mutex);\r\nreturn -ENODEV;\r\n}\r\nret = btrfs_scrub_cancel_dev(root, dev);\r\nmutex_unlock(&fs_info->fs_devices->device_list_mutex);\r\nreturn ret;\r\n}\r\nint btrfs_scrub_progress(struct btrfs_root *root, u64 devid,\r\nstruct btrfs_scrub_progress *progress)\r\n{\r\nstruct btrfs_device *dev;\r\nstruct scrub_dev *sdev = NULL;\r\nmutex_lock(&root->fs_info->fs_devices->device_list_mutex);\r\ndev = btrfs_find_device(root, devid, NULL, NULL);\r\nif (dev)\r\nsdev = dev->scrub_device;\r\nif (sdev)\r\nmemcpy(progress, &sdev->stat, sizeof(*progress));\r\nmutex_unlock(&root->fs_info->fs_devices->device_list_mutex);\r\nreturn dev ? (sdev ? 0 : -ENOTCONN) : -ENODEV;\r\n}
