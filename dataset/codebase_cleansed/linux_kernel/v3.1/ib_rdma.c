static struct rds_ib_device *rds_ib_get_device(__be32 ipaddr)\r\n{\r\nstruct rds_ib_device *rds_ibdev;\r\nstruct rds_ib_ipaddr *i_ipaddr;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(rds_ibdev, &rds_ib_devices, list) {\r\nlist_for_each_entry_rcu(i_ipaddr, &rds_ibdev->ipaddr_list, list) {\r\nif (i_ipaddr->ipaddr == ipaddr) {\r\natomic_inc(&rds_ibdev->refcount);\r\nrcu_read_unlock();\r\nreturn rds_ibdev;\r\n}\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn NULL;\r\n}\r\nstatic int rds_ib_add_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr)\r\n{\r\nstruct rds_ib_ipaddr *i_ipaddr;\r\ni_ipaddr = kmalloc(sizeof *i_ipaddr, GFP_KERNEL);\r\nif (!i_ipaddr)\r\nreturn -ENOMEM;\r\ni_ipaddr->ipaddr = ipaddr;\r\nspin_lock_irq(&rds_ibdev->spinlock);\r\nlist_add_tail_rcu(&i_ipaddr->list, &rds_ibdev->ipaddr_list);\r\nspin_unlock_irq(&rds_ibdev->spinlock);\r\nreturn 0;\r\n}\r\nstatic void rds_ib_remove_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr)\r\n{\r\nstruct rds_ib_ipaddr *i_ipaddr;\r\nstruct rds_ib_ipaddr *to_free = NULL;\r\nspin_lock_irq(&rds_ibdev->spinlock);\r\nlist_for_each_entry_rcu(i_ipaddr, &rds_ibdev->ipaddr_list, list) {\r\nif (i_ipaddr->ipaddr == ipaddr) {\r\nlist_del_rcu(&i_ipaddr->list);\r\nto_free = i_ipaddr;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irq(&rds_ibdev->spinlock);\r\nif (to_free) {\r\nsynchronize_rcu();\r\nkfree(to_free);\r\n}\r\n}\r\nint rds_ib_update_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr)\r\n{\r\nstruct rds_ib_device *rds_ibdev_old;\r\nrds_ibdev_old = rds_ib_get_device(ipaddr);\r\nif (rds_ibdev_old) {\r\nrds_ib_remove_ipaddr(rds_ibdev_old, ipaddr);\r\nrds_ib_dev_put(rds_ibdev_old);\r\n}\r\nreturn rds_ib_add_ipaddr(rds_ibdev, ipaddr);\r\n}\r\nvoid rds_ib_add_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn)\r\n{\r\nstruct rds_ib_connection *ic = conn->c_transport_data;\r\nspin_lock_irq(&ib_nodev_conns_lock);\r\nBUG_ON(list_empty(&ib_nodev_conns));\r\nBUG_ON(list_empty(&ic->ib_node));\r\nlist_del(&ic->ib_node);\r\nspin_lock(&rds_ibdev->spinlock);\r\nlist_add_tail(&ic->ib_node, &rds_ibdev->conn_list);\r\nspin_unlock(&rds_ibdev->spinlock);\r\nspin_unlock_irq(&ib_nodev_conns_lock);\r\nic->rds_ibdev = rds_ibdev;\r\natomic_inc(&rds_ibdev->refcount);\r\n}\r\nvoid rds_ib_remove_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn)\r\n{\r\nstruct rds_ib_connection *ic = conn->c_transport_data;\r\nspin_lock(&ib_nodev_conns_lock);\r\nspin_lock_irq(&rds_ibdev->spinlock);\r\nBUG_ON(list_empty(&ic->ib_node));\r\nlist_del(&ic->ib_node);\r\nspin_unlock_irq(&rds_ibdev->spinlock);\r\nlist_add_tail(&ic->ib_node, &ib_nodev_conns);\r\nspin_unlock(&ib_nodev_conns_lock);\r\nic->rds_ibdev = NULL;\r\nrds_ib_dev_put(rds_ibdev);\r\n}\r\nvoid rds_ib_destroy_nodev_conns(void)\r\n{\r\nstruct rds_ib_connection *ic, *_ic;\r\nLIST_HEAD(tmp_list);\r\nspin_lock_irq(&ib_nodev_conns_lock);\r\nlist_splice(&ib_nodev_conns, &tmp_list);\r\nspin_unlock_irq(&ib_nodev_conns_lock);\r\nlist_for_each_entry_safe(ic, _ic, &tmp_list, ib_node)\r\nrds_conn_destroy(ic->conn);\r\n}\r\nstruct rds_ib_mr_pool *rds_ib_create_mr_pool(struct rds_ib_device *rds_ibdev)\r\n{\r\nstruct rds_ib_mr_pool *pool;\r\npool = kzalloc(sizeof(*pool), GFP_KERNEL);\r\nif (!pool)\r\nreturn ERR_PTR(-ENOMEM);\r\nINIT_XLIST_HEAD(&pool->free_list);\r\nINIT_XLIST_HEAD(&pool->drop_list);\r\nINIT_XLIST_HEAD(&pool->clean_list);\r\nmutex_init(&pool->flush_lock);\r\ninit_waitqueue_head(&pool->flush_wait);\r\nINIT_DELAYED_WORK(&pool->flush_worker, rds_ib_mr_pool_flush_worker);\r\npool->fmr_attr.max_pages = fmr_message_size;\r\npool->fmr_attr.max_maps = rds_ibdev->fmr_max_remaps;\r\npool->fmr_attr.page_shift = PAGE_SHIFT;\r\npool->max_free_pinned = rds_ibdev->max_fmrs * fmr_message_size / 4;\r\npool->max_items_soft = rds_ibdev->max_fmrs * 3 / 4;\r\npool->max_items = rds_ibdev->max_fmrs;\r\nreturn pool;\r\n}\r\nvoid rds_ib_get_mr_info(struct rds_ib_device *rds_ibdev, struct rds_info_rdma_connection *iinfo)\r\n{\r\nstruct rds_ib_mr_pool *pool = rds_ibdev->mr_pool;\r\niinfo->rdma_mr_max = pool->max_items;\r\niinfo->rdma_mr_size = pool->fmr_attr.max_pages;\r\n}\r\nvoid rds_ib_destroy_mr_pool(struct rds_ib_mr_pool *pool)\r\n{\r\ncancel_delayed_work_sync(&pool->flush_worker);\r\nrds_ib_flush_mr_pool(pool, 1, NULL);\r\nWARN_ON(atomic_read(&pool->item_count));\r\nWARN_ON(atomic_read(&pool->free_pinned));\r\nkfree(pool);\r\n}\r\nstatic void refill_local(struct rds_ib_mr_pool *pool, struct xlist_head *xl,\r\nstruct rds_ib_mr **ibmr_ret)\r\n{\r\nstruct xlist_head *ibmr_xl;\r\nibmr_xl = xlist_del_head_fast(xl);\r\n*ibmr_ret = list_entry(ibmr_xl, struct rds_ib_mr, xlist);\r\n}\r\nstatic inline struct rds_ib_mr *rds_ib_reuse_fmr(struct rds_ib_mr_pool *pool)\r\n{\r\nstruct rds_ib_mr *ibmr = NULL;\r\nstruct xlist_head *ret;\r\nunsigned long *flag;\r\npreempt_disable();\r\nflag = &__get_cpu_var(clean_list_grace);\r\nset_bit(CLEAN_LIST_BUSY_BIT, flag);\r\nret = xlist_del_head(&pool->clean_list);\r\nif (ret)\r\nibmr = list_entry(ret, struct rds_ib_mr, xlist);\r\nclear_bit(CLEAN_LIST_BUSY_BIT, flag);\r\npreempt_enable();\r\nreturn ibmr;\r\n}\r\nstatic inline void wait_clean_list_grace(void)\r\n{\r\nint cpu;\r\nunsigned long *flag;\r\nfor_each_online_cpu(cpu) {\r\nflag = &per_cpu(clean_list_grace, cpu);\r\nwhile (test_bit(CLEAN_LIST_BUSY_BIT, flag))\r\ncpu_relax();\r\n}\r\n}\r\nstatic struct rds_ib_mr *rds_ib_alloc_fmr(struct rds_ib_device *rds_ibdev)\r\n{\r\nstruct rds_ib_mr_pool *pool = rds_ibdev->mr_pool;\r\nstruct rds_ib_mr *ibmr = NULL;\r\nint err = 0, iter = 0;\r\nif (atomic_read(&pool->dirty_count) >= pool->max_items / 10)\r\nschedule_delayed_work(&pool->flush_worker, 10);\r\nwhile (1) {\r\nibmr = rds_ib_reuse_fmr(pool);\r\nif (ibmr)\r\nreturn ibmr;\r\nif (atomic_inc_return(&pool->item_count) <= pool->max_items)\r\nbreak;\r\natomic_dec(&pool->item_count);\r\nif (++iter > 2) {\r\nrds_ib_stats_inc(s_ib_rdma_mr_pool_depleted);\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nrds_ib_stats_inc(s_ib_rdma_mr_pool_wait);\r\nrds_ib_flush_mr_pool(pool, 0, &ibmr);\r\nif (ibmr)\r\nreturn ibmr;\r\n}\r\nibmr = kzalloc_node(sizeof(*ibmr), GFP_KERNEL, rdsibdev_to_node(rds_ibdev));\r\nif (!ibmr) {\r\nerr = -ENOMEM;\r\ngoto out_no_cigar;\r\n}\r\nmemset(ibmr, 0, sizeof(*ibmr));\r\nibmr->fmr = ib_alloc_fmr(rds_ibdev->pd,\r\n(IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE|\r\nIB_ACCESS_REMOTE_ATOMIC),\r\n&pool->fmr_attr);\r\nif (IS_ERR(ibmr->fmr)) {\r\nerr = PTR_ERR(ibmr->fmr);\r\nibmr->fmr = NULL;\r\nprintk(KERN_WARNING "RDS/IB: ib_alloc_fmr failed (err=%d)\n", err);\r\ngoto out_no_cigar;\r\n}\r\nrds_ib_stats_inc(s_ib_rdma_mr_alloc);\r\nreturn ibmr;\r\nout_no_cigar:\r\nif (ibmr) {\r\nif (ibmr->fmr)\r\nib_dealloc_fmr(ibmr->fmr);\r\nkfree(ibmr);\r\n}\r\natomic_dec(&pool->item_count);\r\nreturn ERR_PTR(err);\r\n}\r\nstatic int rds_ib_map_fmr(struct rds_ib_device *rds_ibdev, struct rds_ib_mr *ibmr,\r\nstruct scatterlist *sg, unsigned int nents)\r\n{\r\nstruct ib_device *dev = rds_ibdev->dev;\r\nstruct scatterlist *scat = sg;\r\nu64 io_addr = 0;\r\nu64 *dma_pages;\r\nu32 len;\r\nint page_cnt, sg_dma_len;\r\nint i, j;\r\nint ret;\r\nsg_dma_len = ib_dma_map_sg(dev, sg, nents,\r\nDMA_BIDIRECTIONAL);\r\nif (unlikely(!sg_dma_len)) {\r\nprintk(KERN_WARNING "RDS/IB: dma_map_sg failed!\n");\r\nreturn -EBUSY;\r\n}\r\nlen = 0;\r\npage_cnt = 0;\r\nfor (i = 0; i < sg_dma_len; ++i) {\r\nunsigned int dma_len = ib_sg_dma_len(dev, &scat[i]);\r\nu64 dma_addr = ib_sg_dma_address(dev, &scat[i]);\r\nif (dma_addr & ~PAGE_MASK) {\r\nif (i > 0)\r\nreturn -EINVAL;\r\nelse\r\n++page_cnt;\r\n}\r\nif ((dma_addr + dma_len) & ~PAGE_MASK) {\r\nif (i < sg_dma_len - 1)\r\nreturn -EINVAL;\r\nelse\r\n++page_cnt;\r\n}\r\nlen += dma_len;\r\n}\r\npage_cnt += len >> PAGE_SHIFT;\r\nif (page_cnt > fmr_message_size)\r\nreturn -EINVAL;\r\ndma_pages = kmalloc_node(sizeof(u64) * page_cnt, GFP_ATOMIC,\r\nrdsibdev_to_node(rds_ibdev));\r\nif (!dma_pages)\r\nreturn -ENOMEM;\r\npage_cnt = 0;\r\nfor (i = 0; i < sg_dma_len; ++i) {\r\nunsigned int dma_len = ib_sg_dma_len(dev, &scat[i]);\r\nu64 dma_addr = ib_sg_dma_address(dev, &scat[i]);\r\nfor (j = 0; j < dma_len; j += PAGE_SIZE)\r\ndma_pages[page_cnt++] =\r\n(dma_addr & PAGE_MASK) + j;\r\n}\r\nret = ib_map_phys_fmr(ibmr->fmr,\r\ndma_pages, page_cnt, io_addr);\r\nif (ret)\r\ngoto out;\r\nrds_ib_teardown_mr(ibmr);\r\nibmr->sg = scat;\r\nibmr->sg_len = nents;\r\nibmr->sg_dma_len = sg_dma_len;\r\nibmr->remap_count++;\r\nrds_ib_stats_inc(s_ib_rdma_mr_used);\r\nret = 0;\r\nout:\r\nkfree(dma_pages);\r\nreturn ret;\r\n}\r\nvoid rds_ib_sync_mr(void *trans_private, int direction)\r\n{\r\nstruct rds_ib_mr *ibmr = trans_private;\r\nstruct rds_ib_device *rds_ibdev = ibmr->device;\r\nswitch (direction) {\r\ncase DMA_FROM_DEVICE:\r\nib_dma_sync_sg_for_cpu(rds_ibdev->dev, ibmr->sg,\r\nibmr->sg_dma_len, DMA_BIDIRECTIONAL);\r\nbreak;\r\ncase DMA_TO_DEVICE:\r\nib_dma_sync_sg_for_device(rds_ibdev->dev, ibmr->sg,\r\nibmr->sg_dma_len, DMA_BIDIRECTIONAL);\r\nbreak;\r\n}\r\n}\r\nstatic void __rds_ib_teardown_mr(struct rds_ib_mr *ibmr)\r\n{\r\nstruct rds_ib_device *rds_ibdev = ibmr->device;\r\nif (ibmr->sg_dma_len) {\r\nib_dma_unmap_sg(rds_ibdev->dev,\r\nibmr->sg, ibmr->sg_len,\r\nDMA_BIDIRECTIONAL);\r\nibmr->sg_dma_len = 0;\r\n}\r\nif (ibmr->sg_len) {\r\nunsigned int i;\r\nfor (i = 0; i < ibmr->sg_len; ++i) {\r\nstruct page *page = sg_page(&ibmr->sg[i]);\r\nBUG_ON(irqs_disabled());\r\nset_page_dirty(page);\r\nput_page(page);\r\n}\r\nkfree(ibmr->sg);\r\nibmr->sg = NULL;\r\nibmr->sg_len = 0;\r\n}\r\n}\r\nstatic void rds_ib_teardown_mr(struct rds_ib_mr *ibmr)\r\n{\r\nunsigned int pinned = ibmr->sg_len;\r\n__rds_ib_teardown_mr(ibmr);\r\nif (pinned) {\r\nstruct rds_ib_device *rds_ibdev = ibmr->device;\r\nstruct rds_ib_mr_pool *pool = rds_ibdev->mr_pool;\r\natomic_sub(pinned, &pool->free_pinned);\r\n}\r\n}\r\nstatic inline unsigned int rds_ib_flush_goal(struct rds_ib_mr_pool *pool, int free_all)\r\n{\r\nunsigned int item_count;\r\nitem_count = atomic_read(&pool->item_count);\r\nif (free_all)\r\nreturn item_count;\r\nreturn 0;\r\n}\r\nstatic void xlist_append_to_list(struct xlist_head *xlist, struct list_head *list)\r\n{\r\nstruct rds_ib_mr *ibmr;\r\nstruct xlist_head splice;\r\nstruct xlist_head *cur;\r\nstruct xlist_head *next;\r\nsplice.next = NULL;\r\nxlist_splice(xlist, &splice);\r\ncur = splice.next;\r\nwhile (cur) {\r\nnext = cur->next;\r\nibmr = list_entry(cur, struct rds_ib_mr, xlist);\r\nlist_add_tail(&ibmr->unmap_list, list);\r\ncur = next;\r\n}\r\n}\r\nstatic void list_append_to_xlist(struct rds_ib_mr_pool *pool,\r\nstruct list_head *list, struct xlist_head *xlist,\r\nstruct xlist_head **tail_ret)\r\n{\r\nstruct rds_ib_mr *ibmr;\r\nstruct xlist_head *cur_mr = xlist;\r\nstruct xlist_head *tail_mr = NULL;\r\nlist_for_each_entry(ibmr, list, unmap_list) {\r\ntail_mr = &ibmr->xlist;\r\ntail_mr->next = NULL;\r\ncur_mr->next = tail_mr;\r\ncur_mr = tail_mr;\r\n}\r\n*tail_ret = tail_mr;\r\n}\r\nstatic int rds_ib_flush_mr_pool(struct rds_ib_mr_pool *pool,\r\nint free_all, struct rds_ib_mr **ibmr_ret)\r\n{\r\nstruct rds_ib_mr *ibmr, *next;\r\nstruct xlist_head clean_xlist;\r\nstruct xlist_head *clean_tail;\r\nLIST_HEAD(unmap_list);\r\nLIST_HEAD(fmr_list);\r\nunsigned long unpinned = 0;\r\nunsigned int nfreed = 0, ncleaned = 0, free_goal;\r\nint ret = 0;\r\nrds_ib_stats_inc(s_ib_rdma_mr_pool_flush);\r\nif (ibmr_ret) {\r\nDEFINE_WAIT(wait);\r\nwhile(!mutex_trylock(&pool->flush_lock)) {\r\nibmr = rds_ib_reuse_fmr(pool);\r\nif (ibmr) {\r\n*ibmr_ret = ibmr;\r\nfinish_wait(&pool->flush_wait, &wait);\r\ngoto out_nolock;\r\n}\r\nprepare_to_wait(&pool->flush_wait, &wait,\r\nTASK_UNINTERRUPTIBLE);\r\nif (xlist_empty(&pool->clean_list))\r\nschedule();\r\nibmr = rds_ib_reuse_fmr(pool);\r\nif (ibmr) {\r\n*ibmr_ret = ibmr;\r\nfinish_wait(&pool->flush_wait, &wait);\r\ngoto out_nolock;\r\n}\r\n}\r\nfinish_wait(&pool->flush_wait, &wait);\r\n} else\r\nmutex_lock(&pool->flush_lock);\r\nif (ibmr_ret) {\r\nibmr = rds_ib_reuse_fmr(pool);\r\nif (ibmr) {\r\n*ibmr_ret = ibmr;\r\ngoto out;\r\n}\r\n}\r\nxlist_append_to_list(&pool->drop_list, &unmap_list);\r\nxlist_append_to_list(&pool->free_list, &unmap_list);\r\nif (free_all)\r\nxlist_append_to_list(&pool->clean_list, &unmap_list);\r\nfree_goal = rds_ib_flush_goal(pool, free_all);\r\nif (list_empty(&unmap_list))\r\ngoto out;\r\nlist_for_each_entry(ibmr, &unmap_list, unmap_list)\r\nlist_add(&ibmr->fmr->list, &fmr_list);\r\nret = ib_unmap_fmr(&fmr_list);\r\nif (ret)\r\nprintk(KERN_WARNING "RDS/IB: ib_unmap_fmr failed (err=%d)\n", ret);\r\nlist_for_each_entry_safe(ibmr, next, &unmap_list, unmap_list) {\r\nunpinned += ibmr->sg_len;\r\n__rds_ib_teardown_mr(ibmr);\r\nif (nfreed < free_goal || ibmr->remap_count >= pool->fmr_attr.max_maps) {\r\nrds_ib_stats_inc(s_ib_rdma_mr_free);\r\nlist_del(&ibmr->unmap_list);\r\nib_dealloc_fmr(ibmr->fmr);\r\nkfree(ibmr);\r\nnfreed++;\r\n}\r\nncleaned++;\r\n}\r\nif (!list_empty(&unmap_list)) {\r\nwait_clean_list_grace();\r\nlist_append_to_xlist(pool, &unmap_list, &clean_xlist, &clean_tail);\r\nif (ibmr_ret)\r\nrefill_local(pool, &clean_xlist, ibmr_ret);\r\nif (!xlist_empty(&clean_xlist))\r\nxlist_add(clean_xlist.next, clean_tail, &pool->clean_list);\r\n}\r\natomic_sub(unpinned, &pool->free_pinned);\r\natomic_sub(ncleaned, &pool->dirty_count);\r\natomic_sub(nfreed, &pool->item_count);\r\nout:\r\nmutex_unlock(&pool->flush_lock);\r\nif (waitqueue_active(&pool->flush_wait))\r\nwake_up(&pool->flush_wait);\r\nout_nolock:\r\nreturn ret;\r\n}\r\nstatic void rds_ib_mr_pool_flush_worker(struct work_struct *work)\r\n{\r\nstruct rds_ib_mr_pool *pool = container_of(work, struct rds_ib_mr_pool, flush_worker.work);\r\nrds_ib_flush_mr_pool(pool, 0, NULL);\r\n}\r\nvoid rds_ib_free_mr(void *trans_private, int invalidate)\r\n{\r\nstruct rds_ib_mr *ibmr = trans_private;\r\nstruct rds_ib_device *rds_ibdev = ibmr->device;\r\nstruct rds_ib_mr_pool *pool = rds_ibdev->mr_pool;\r\nrdsdebug("RDS/IB: free_mr nents %u\n", ibmr->sg_len);\r\nif (ibmr->remap_count >= pool->fmr_attr.max_maps)\r\nxlist_add(&ibmr->xlist, &ibmr->xlist, &pool->drop_list);\r\nelse\r\nxlist_add(&ibmr->xlist, &ibmr->xlist, &pool->free_list);\r\natomic_add(ibmr->sg_len, &pool->free_pinned);\r\natomic_inc(&pool->dirty_count);\r\nif (atomic_read(&pool->free_pinned) >= pool->max_free_pinned ||\r\natomic_read(&pool->dirty_count) >= pool->max_items / 10)\r\nschedule_delayed_work(&pool->flush_worker, 10);\r\nif (invalidate) {\r\nif (likely(!in_interrupt())) {\r\nrds_ib_flush_mr_pool(pool, 0, NULL);\r\n} else {\r\nschedule_delayed_work(&pool->flush_worker, 10);\r\n}\r\n}\r\nrds_ib_dev_put(rds_ibdev);\r\n}\r\nvoid rds_ib_flush_mrs(void)\r\n{\r\nstruct rds_ib_device *rds_ibdev;\r\ndown_read(&rds_ib_devices_lock);\r\nlist_for_each_entry(rds_ibdev, &rds_ib_devices, list) {\r\nstruct rds_ib_mr_pool *pool = rds_ibdev->mr_pool;\r\nif (pool)\r\nrds_ib_flush_mr_pool(pool, 0, NULL);\r\n}\r\nup_read(&rds_ib_devices_lock);\r\n}\r\nvoid *rds_ib_get_mr(struct scatterlist *sg, unsigned long nents,\r\nstruct rds_sock *rs, u32 *key_ret)\r\n{\r\nstruct rds_ib_device *rds_ibdev;\r\nstruct rds_ib_mr *ibmr = NULL;\r\nint ret;\r\nrds_ibdev = rds_ib_get_device(rs->rs_bound_addr);\r\nif (!rds_ibdev) {\r\nret = -ENODEV;\r\ngoto out;\r\n}\r\nif (!rds_ibdev->mr_pool) {\r\nret = -ENODEV;\r\ngoto out;\r\n}\r\nibmr = rds_ib_alloc_fmr(rds_ibdev);\r\nif (IS_ERR(ibmr))\r\nreturn ibmr;\r\nret = rds_ib_map_fmr(rds_ibdev, ibmr, sg, nents);\r\nif (ret == 0)\r\n*key_ret = ibmr->fmr->rkey;\r\nelse\r\nprintk(KERN_WARNING "RDS/IB: map_fmr failed (errno=%d)\n", ret);\r\nibmr->device = rds_ibdev;\r\nrds_ibdev = NULL;\r\nout:\r\nif (ret) {\r\nif (ibmr)\r\nrds_ib_free_mr(ibmr, 0);\r\nibmr = ERR_PTR(ret);\r\n}\r\nif (rds_ibdev)\r\nrds_ib_dev_put(rds_ibdev);\r\nreturn ibmr;\r\n}
