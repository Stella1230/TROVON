static u32 hash_conntrack_raw(const struct nf_conntrack_tuple *tuple, u16 zone)\r\n{\r\nunsigned int n;\r\nn = (sizeof(tuple->src) + sizeof(tuple->dst.u3)) / sizeof(u32);\r\nreturn jhash2((u32 *)tuple, n, zone ^ nf_conntrack_hash_rnd ^\r\n(((__force __u16)tuple->dst.u.all << 16) |\r\ntuple->dst.protonum));\r\n}\r\nstatic u32 __hash_bucket(u32 hash, unsigned int size)\r\n{\r\nreturn ((u64)hash * size) >> 32;\r\n}\r\nstatic u32 hash_bucket(u32 hash, const struct net *net)\r\n{\r\nreturn __hash_bucket(hash, net->ct.htable_size);\r\n}\r\nstatic u_int32_t __hash_conntrack(const struct nf_conntrack_tuple *tuple,\r\nu16 zone, unsigned int size)\r\n{\r\nreturn __hash_bucket(hash_conntrack_raw(tuple, zone), size);\r\n}\r\nstatic inline u_int32_t hash_conntrack(const struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *tuple)\r\n{\r\nreturn __hash_conntrack(tuple, zone, net->ct.htable_size);\r\n}\r\nbool\r\nnf_ct_get_tuple(const struct sk_buff *skb,\r\nunsigned int nhoff,\r\nunsigned int dataoff,\r\nu_int16_t l3num,\r\nu_int8_t protonum,\r\nstruct nf_conntrack_tuple *tuple,\r\nconst struct nf_conntrack_l3proto *l3proto,\r\nconst struct nf_conntrack_l4proto *l4proto)\r\n{\r\nmemset(tuple, 0, sizeof(*tuple));\r\ntuple->src.l3num = l3num;\r\nif (l3proto->pkt_to_tuple(skb, nhoff, tuple) == 0)\r\nreturn false;\r\ntuple->dst.protonum = protonum;\r\ntuple->dst.dir = IP_CT_DIR_ORIGINAL;\r\nreturn l4proto->pkt_to_tuple(skb, dataoff, tuple);\r\n}\r\nbool nf_ct_get_tuplepr(const struct sk_buff *skb, unsigned int nhoff,\r\nu_int16_t l3num, struct nf_conntrack_tuple *tuple)\r\n{\r\nstruct nf_conntrack_l3proto *l3proto;\r\nstruct nf_conntrack_l4proto *l4proto;\r\nunsigned int protoff;\r\nu_int8_t protonum;\r\nint ret;\r\nrcu_read_lock();\r\nl3proto = __nf_ct_l3proto_find(l3num);\r\nret = l3proto->get_l4proto(skb, nhoff, &protoff, &protonum);\r\nif (ret != NF_ACCEPT) {\r\nrcu_read_unlock();\r\nreturn false;\r\n}\r\nl4proto = __nf_ct_l4proto_find(l3num, protonum);\r\nret = nf_ct_get_tuple(skb, nhoff, protoff, l3num, protonum, tuple,\r\nl3proto, l4proto);\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nbool\r\nnf_ct_invert_tuple(struct nf_conntrack_tuple *inverse,\r\nconst struct nf_conntrack_tuple *orig,\r\nconst struct nf_conntrack_l3proto *l3proto,\r\nconst struct nf_conntrack_l4proto *l4proto)\r\n{\r\nmemset(inverse, 0, sizeof(*inverse));\r\ninverse->src.l3num = orig->src.l3num;\r\nif (l3proto->invert_tuple(inverse, orig) == 0)\r\nreturn false;\r\ninverse->dst.dir = !orig->dst.dir;\r\ninverse->dst.protonum = orig->dst.protonum;\r\nreturn l4proto->invert_tuple(inverse, orig);\r\n}\r\nstatic void\r\nclean_from_lists(struct nf_conn *ct)\r\n{\r\npr_debug("clean_from_lists(%p)\n", ct);\r\nhlist_nulls_del_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode);\r\nhlist_nulls_del_rcu(&ct->tuplehash[IP_CT_DIR_REPLY].hnnode);\r\nnf_ct_remove_expectations(ct);\r\n}\r\nstatic void\r\ndestroy_conntrack(struct nf_conntrack *nfct)\r\n{\r\nstruct nf_conn *ct = (struct nf_conn *)nfct;\r\nstruct net *net = nf_ct_net(ct);\r\nstruct nf_conntrack_l4proto *l4proto;\r\npr_debug("destroy_conntrack(%p)\n", ct);\r\nNF_CT_ASSERT(atomic_read(&nfct->use) == 0);\r\nNF_CT_ASSERT(!timer_pending(&ct->timeout));\r\nrcu_read_lock();\r\nl4proto = __nf_ct_l4proto_find(nf_ct_l3num(ct), nf_ct_protonum(ct));\r\nif (l4proto && l4proto->destroy)\r\nl4proto->destroy(ct);\r\nrcu_read_unlock();\r\nspin_lock_bh(&nf_conntrack_lock);\r\nnf_ct_remove_expectations(ct);\r\nif (!nf_ct_is_confirmed(ct)) {\r\nBUG_ON(hlist_nulls_unhashed(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode));\r\nhlist_nulls_del_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode);\r\n}\r\nNF_CT_STAT_INC(net, delete);\r\nspin_unlock_bh(&nf_conntrack_lock);\r\nif (ct->master)\r\nnf_ct_put(ct->master);\r\npr_debug("destroy_conntrack: returning ct=%p to slab\n", ct);\r\nnf_conntrack_free(ct);\r\n}\r\nvoid nf_ct_delete_from_lists(struct nf_conn *ct)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nnf_ct_helper_destroy(ct);\r\nspin_lock_bh(&nf_conntrack_lock);\r\nNF_CT_STAT_INC(net, delete_list);\r\nclean_from_lists(ct);\r\nspin_unlock_bh(&nf_conntrack_lock);\r\n}\r\nstatic void death_by_event(unsigned long ul_conntrack)\r\n{\r\nstruct nf_conn *ct = (void *)ul_conntrack;\r\nstruct net *net = nf_ct_net(ct);\r\nif (nf_conntrack_event(IPCT_DESTROY, ct) < 0) {\r\nct->timeout.expires = jiffies +\r\n(random32() % net->ct.sysctl_events_retry_timeout);\r\nadd_timer(&ct->timeout);\r\nreturn;\r\n}\r\nset_bit(IPS_DYING_BIT, &ct->status);\r\nspin_lock(&nf_conntrack_lock);\r\nhlist_nulls_del(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode);\r\nspin_unlock(&nf_conntrack_lock);\r\nnf_ct_put(ct);\r\n}\r\nvoid nf_ct_insert_dying_list(struct nf_conn *ct)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nspin_lock_bh(&nf_conntrack_lock);\r\nhlist_nulls_add_head(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode,\r\n&net->ct.dying);\r\nspin_unlock_bh(&nf_conntrack_lock);\r\nsetup_timer(&ct->timeout, death_by_event, (unsigned long)ct);\r\nct->timeout.expires = jiffies +\r\n(random32() % net->ct.sysctl_events_retry_timeout);\r\nadd_timer(&ct->timeout);\r\n}\r\nstatic void death_by_timeout(unsigned long ul_conntrack)\r\n{\r\nstruct nf_conn *ct = (void *)ul_conntrack;\r\nstruct nf_conn_tstamp *tstamp;\r\ntstamp = nf_conn_tstamp_find(ct);\r\nif (tstamp && tstamp->stop == 0)\r\ntstamp->stop = ktime_to_ns(ktime_get_real());\r\nif (!test_bit(IPS_DYING_BIT, &ct->status) &&\r\nunlikely(nf_conntrack_event(IPCT_DESTROY, ct) < 0)) {\r\nnf_ct_delete_from_lists(ct);\r\nnf_ct_insert_dying_list(ct);\r\nreturn;\r\n}\r\nset_bit(IPS_DYING_BIT, &ct->status);\r\nnf_ct_delete_from_lists(ct);\r\nnf_ct_put(ct);\r\n}\r\nstatic struct nf_conntrack_tuple_hash *\r\n____nf_conntrack_find(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *tuple, u32 hash)\r\n{\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct hlist_nulls_node *n;\r\nunsigned int bucket = hash_bucket(hash, net);\r\nlocal_bh_disable();\r\nbegin:\r\nhlist_nulls_for_each_entry_rcu(h, n, &net->ct.hash[bucket], hnnode) {\r\nif (nf_ct_tuple_equal(tuple, &h->tuple) &&\r\nnf_ct_zone(nf_ct_tuplehash_to_ctrack(h)) == zone) {\r\nNF_CT_STAT_INC(net, found);\r\nlocal_bh_enable();\r\nreturn h;\r\n}\r\nNF_CT_STAT_INC(net, searched);\r\n}\r\nif (get_nulls_value(n) != bucket) {\r\nNF_CT_STAT_INC(net, search_restart);\r\ngoto begin;\r\n}\r\nlocal_bh_enable();\r\nreturn NULL;\r\n}\r\nstruct nf_conntrack_tuple_hash *\r\n__nf_conntrack_find(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *tuple)\r\n{\r\nreturn ____nf_conntrack_find(net, zone, tuple,\r\nhash_conntrack_raw(tuple, zone));\r\n}\r\nstatic struct nf_conntrack_tuple_hash *\r\n__nf_conntrack_find_get(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *tuple, u32 hash)\r\n{\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nrcu_read_lock();\r\nbegin:\r\nh = ____nf_conntrack_find(net, zone, tuple, hash);\r\nif (h) {\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (unlikely(nf_ct_is_dying(ct) ||\r\n!atomic_inc_not_zero(&ct->ct_general.use)))\r\nh = NULL;\r\nelse {\r\nif (unlikely(!nf_ct_tuple_equal(tuple, &h->tuple) ||\r\nnf_ct_zone(ct) != zone)) {\r\nnf_ct_put(ct);\r\ngoto begin;\r\n}\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn h;\r\n}\r\nstruct nf_conntrack_tuple_hash *\r\nnf_conntrack_find_get(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *tuple)\r\n{\r\nreturn __nf_conntrack_find_get(net, zone, tuple,\r\nhash_conntrack_raw(tuple, zone));\r\n}\r\nstatic void __nf_conntrack_hash_insert(struct nf_conn *ct,\r\nunsigned int hash,\r\nunsigned int repl_hash)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nhlist_nulls_add_head_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode,\r\n&net->ct.hash[hash]);\r\nhlist_nulls_add_head_rcu(&ct->tuplehash[IP_CT_DIR_REPLY].hnnode,\r\n&net->ct.hash[repl_hash]);\r\n}\r\nvoid nf_conntrack_hash_insert(struct nf_conn *ct)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nunsigned int hash, repl_hash;\r\nu16 zone;\r\nzone = nf_ct_zone(ct);\r\nhash = hash_conntrack(net, zone, &ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple);\r\nrepl_hash = hash_conntrack(net, zone, &ct->tuplehash[IP_CT_DIR_REPLY].tuple);\r\n__nf_conntrack_hash_insert(ct, hash, repl_hash);\r\n}\r\nint\r\n__nf_conntrack_confirm(struct sk_buff *skb)\r\n{\r\nunsigned int hash, repl_hash;\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nstruct nf_conn_help *help;\r\nstruct nf_conn_tstamp *tstamp;\r\nstruct hlist_nulls_node *n;\r\nenum ip_conntrack_info ctinfo;\r\nstruct net *net;\r\nu16 zone;\r\nct = nf_ct_get(skb, &ctinfo);\r\nnet = nf_ct_net(ct);\r\nif (CTINFO2DIR(ctinfo) != IP_CT_DIR_ORIGINAL)\r\nreturn NF_ACCEPT;\r\nzone = nf_ct_zone(ct);\r\nhash = *(unsigned long *)&ct->tuplehash[IP_CT_DIR_REPLY].hnnode.pprev;\r\nhash = hash_bucket(hash, net);\r\nrepl_hash = hash_conntrack(net, zone,\r\n&ct->tuplehash[IP_CT_DIR_REPLY].tuple);\r\nNF_CT_ASSERT(!nf_ct_is_confirmed(ct));\r\npr_debug("Confirming conntrack %p\n", ct);\r\nspin_lock_bh(&nf_conntrack_lock);\r\nif (unlikely(nf_ct_is_dying(ct))) {\r\nspin_unlock_bh(&nf_conntrack_lock);\r\nreturn NF_ACCEPT;\r\n}\r\nhlist_nulls_for_each_entry(h, n, &net->ct.hash[hash], hnnode)\r\nif (nf_ct_tuple_equal(&ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\r\n&h->tuple) &&\r\nzone == nf_ct_zone(nf_ct_tuplehash_to_ctrack(h)))\r\ngoto out;\r\nhlist_nulls_for_each_entry(h, n, &net->ct.hash[repl_hash], hnnode)\r\nif (nf_ct_tuple_equal(&ct->tuplehash[IP_CT_DIR_REPLY].tuple,\r\n&h->tuple) &&\r\nzone == nf_ct_zone(nf_ct_tuplehash_to_ctrack(h)))\r\ngoto out;\r\nhlist_nulls_del_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode);\r\nct->timeout.expires += jiffies;\r\nadd_timer(&ct->timeout);\r\natomic_inc(&ct->ct_general.use);\r\nct->status |= IPS_CONFIRMED;\r\ntstamp = nf_conn_tstamp_find(ct);\r\nif (tstamp) {\r\nif (skb->tstamp.tv64 == 0)\r\n__net_timestamp((struct sk_buff *)skb);\r\ntstamp->start = ktime_to_ns(skb->tstamp);\r\n}\r\n__nf_conntrack_hash_insert(ct, hash, repl_hash);\r\nNF_CT_STAT_INC(net, insert);\r\nspin_unlock_bh(&nf_conntrack_lock);\r\nhelp = nfct_help(ct);\r\nif (help && help->helper)\r\nnf_conntrack_event_cache(IPCT_HELPER, ct);\r\nnf_conntrack_event_cache(master_ct(ct) ?\r\nIPCT_RELATED : IPCT_NEW, ct);\r\nreturn NF_ACCEPT;\r\nout:\r\nNF_CT_STAT_INC(net, insert_failed);\r\nspin_unlock_bh(&nf_conntrack_lock);\r\nreturn NF_DROP;\r\n}\r\nint\r\nnf_conntrack_tuple_taken(const struct nf_conntrack_tuple *tuple,\r\nconst struct nf_conn *ignored_conntrack)\r\n{\r\nstruct net *net = nf_ct_net(ignored_conntrack);\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct hlist_nulls_node *n;\r\nstruct nf_conn *ct;\r\nu16 zone = nf_ct_zone(ignored_conntrack);\r\nunsigned int hash = hash_conntrack(net, zone, tuple);\r\nrcu_read_lock_bh();\r\nhlist_nulls_for_each_entry_rcu(h, n, &net->ct.hash[hash], hnnode) {\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (ct != ignored_conntrack &&\r\nnf_ct_tuple_equal(tuple, &h->tuple) &&\r\nnf_ct_zone(ct) == zone) {\r\nNF_CT_STAT_INC(net, found);\r\nrcu_read_unlock_bh();\r\nreturn 1;\r\n}\r\nNF_CT_STAT_INC(net, searched);\r\n}\r\nrcu_read_unlock_bh();\r\nreturn 0;\r\n}\r\nstatic noinline int early_drop(struct net *net, unsigned int hash)\r\n{\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct = NULL, *tmp;\r\nstruct hlist_nulls_node *n;\r\nunsigned int i, cnt = 0;\r\nint dropped = 0;\r\nrcu_read_lock();\r\nfor (i = 0; i < net->ct.htable_size; i++) {\r\nhlist_nulls_for_each_entry_rcu(h, n, &net->ct.hash[hash],\r\nhnnode) {\r\ntmp = nf_ct_tuplehash_to_ctrack(h);\r\nif (!test_bit(IPS_ASSURED_BIT, &tmp->status))\r\nct = tmp;\r\ncnt++;\r\n}\r\nif (ct != NULL) {\r\nif (likely(!nf_ct_is_dying(ct) &&\r\natomic_inc_not_zero(&ct->ct_general.use)))\r\nbreak;\r\nelse\r\nct = NULL;\r\n}\r\nif (cnt >= NF_CT_EVICTION_RANGE)\r\nbreak;\r\nhash = (hash + 1) % net->ct.htable_size;\r\n}\r\nrcu_read_unlock();\r\nif (!ct)\r\nreturn dropped;\r\nif (del_timer(&ct->timeout)) {\r\ndeath_by_timeout((unsigned long)ct);\r\ndropped = 1;\r\nNF_CT_STAT_INC_ATOMIC(net, early_drop);\r\n}\r\nnf_ct_put(ct);\r\nreturn dropped;\r\n}\r\nvoid init_nf_conntrack_hash_rnd(void)\r\n{\r\nunsigned int rand;\r\ndo {\r\nget_random_bytes(&rand, sizeof(rand));\r\n} while (!rand);\r\ncmpxchg(&nf_conntrack_hash_rnd, 0, rand);\r\n}\r\nstatic struct nf_conn *\r\n__nf_conntrack_alloc(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *orig,\r\nconst struct nf_conntrack_tuple *repl,\r\ngfp_t gfp, u32 hash)\r\n{\r\nstruct nf_conn *ct;\r\nif (unlikely(!nf_conntrack_hash_rnd)) {\r\ninit_nf_conntrack_hash_rnd();\r\nhash = hash_conntrack_raw(orig, zone);\r\n}\r\natomic_inc(&net->ct.count);\r\nif (nf_conntrack_max &&\r\nunlikely(atomic_read(&net->ct.count) > nf_conntrack_max)) {\r\nif (!early_drop(net, hash_bucket(hash, net))) {\r\natomic_dec(&net->ct.count);\r\nif (net_ratelimit())\r\nprintk(KERN_WARNING\r\n"nf_conntrack: table full, dropping"\r\n" packet.\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\n}\r\nct = kmem_cache_alloc(net->ct.nf_conntrack_cachep, gfp);\r\nif (ct == NULL) {\r\npr_debug("nf_conntrack_alloc: Can't alloc conntrack.\n");\r\natomic_dec(&net->ct.count);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nmemset(&ct->tuplehash[IP_CT_DIR_MAX], 0,\r\noffsetof(struct nf_conn, proto) -\r\noffsetof(struct nf_conn, tuplehash[IP_CT_DIR_MAX]));\r\nspin_lock_init(&ct->lock);\r\nct->tuplehash[IP_CT_DIR_ORIGINAL].tuple = *orig;\r\nct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode.pprev = NULL;\r\nct->tuplehash[IP_CT_DIR_REPLY].tuple = *repl;\r\n*(unsigned long *)(&ct->tuplehash[IP_CT_DIR_REPLY].hnnode.pprev) = hash;\r\nsetup_timer(&ct->timeout, death_by_timeout, (unsigned long)ct);\r\nwrite_pnet(&ct->ct_net, net);\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nif (zone) {\r\nstruct nf_conntrack_zone *nf_ct_zone;\r\nnf_ct_zone = nf_ct_ext_add(ct, NF_CT_EXT_ZONE, GFP_ATOMIC);\r\nif (!nf_ct_zone)\r\ngoto out_free;\r\nnf_ct_zone->id = zone;\r\n}\r\n#endif\r\nsmp_wmb();\r\natomic_set(&ct->ct_general.use, 1);\r\nreturn ct;\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nout_free:\r\nkmem_cache_free(net->ct.nf_conntrack_cachep, ct);\r\nreturn ERR_PTR(-ENOMEM);\r\n#endif\r\n}\r\nstruct nf_conn *nf_conntrack_alloc(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *orig,\r\nconst struct nf_conntrack_tuple *repl,\r\ngfp_t gfp)\r\n{\r\nreturn __nf_conntrack_alloc(net, zone, orig, repl, gfp, 0);\r\n}\r\nvoid nf_conntrack_free(struct nf_conn *ct)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nnf_ct_ext_destroy(ct);\r\natomic_dec(&net->ct.count);\r\nnf_ct_ext_free(ct);\r\nkmem_cache_free(net->ct.nf_conntrack_cachep, ct);\r\n}\r\nstatic struct nf_conntrack_tuple_hash *\r\ninit_conntrack(struct net *net, struct nf_conn *tmpl,\r\nconst struct nf_conntrack_tuple *tuple,\r\nstruct nf_conntrack_l3proto *l3proto,\r\nstruct nf_conntrack_l4proto *l4proto,\r\nstruct sk_buff *skb,\r\nunsigned int dataoff, u32 hash)\r\n{\r\nstruct nf_conn *ct;\r\nstruct nf_conn_help *help;\r\nstruct nf_conntrack_tuple repl_tuple;\r\nstruct nf_conntrack_ecache *ecache;\r\nstruct nf_conntrack_expect *exp;\r\nu16 zone = tmpl ? nf_ct_zone(tmpl) : NF_CT_DEFAULT_ZONE;\r\nif (!nf_ct_invert_tuple(&repl_tuple, tuple, l3proto, l4proto)) {\r\npr_debug("Can't invert tuple.\n");\r\nreturn NULL;\r\n}\r\nct = __nf_conntrack_alloc(net, zone, tuple, &repl_tuple, GFP_ATOMIC,\r\nhash);\r\nif (IS_ERR(ct)) {\r\npr_debug("Can't allocate conntrack.\n");\r\nreturn (struct nf_conntrack_tuple_hash *)ct;\r\n}\r\nif (!l4proto->new(ct, skb, dataoff)) {\r\nnf_conntrack_free(ct);\r\npr_debug("init conntrack: can't track with proto module\n");\r\nreturn NULL;\r\n}\r\nnf_ct_acct_ext_add(ct, GFP_ATOMIC);\r\nnf_ct_tstamp_ext_add(ct, GFP_ATOMIC);\r\necache = tmpl ? nf_ct_ecache_find(tmpl) : NULL;\r\nnf_ct_ecache_ext_add(ct, ecache ? ecache->ctmask : 0,\r\necache ? ecache->expmask : 0,\r\nGFP_ATOMIC);\r\nspin_lock_bh(&nf_conntrack_lock);\r\nexp = nf_ct_find_expectation(net, zone, tuple);\r\nif (exp) {\r\npr_debug("conntrack: expectation arrives ct=%p exp=%p\n",\r\nct, exp);\r\n__set_bit(IPS_EXPECTED_BIT, &ct->status);\r\nct->master = exp->master;\r\nif (exp->helper) {\r\nhelp = nf_ct_helper_ext_add(ct, GFP_ATOMIC);\r\nif (help)\r\nrcu_assign_pointer(help->helper, exp->helper);\r\n}\r\n#ifdef CONFIG_NF_CONNTRACK_MARK\r\nct->mark = exp->master->mark;\r\n#endif\r\n#ifdef CONFIG_NF_CONNTRACK_SECMARK\r\nct->secmark = exp->master->secmark;\r\n#endif\r\nnf_conntrack_get(&ct->master->ct_general);\r\nNF_CT_STAT_INC(net, expect_new);\r\n} else {\r\n__nf_ct_try_assign_helper(ct, tmpl, GFP_ATOMIC);\r\nNF_CT_STAT_INC(net, new);\r\n}\r\nhlist_nulls_add_head_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode,\r\n&net->ct.unconfirmed);\r\nspin_unlock_bh(&nf_conntrack_lock);\r\nif (exp) {\r\nif (exp->expectfn)\r\nexp->expectfn(ct, exp);\r\nnf_ct_expect_put(exp);\r\n}\r\nreturn &ct->tuplehash[IP_CT_DIR_ORIGINAL];\r\n}\r\nstatic inline struct nf_conn *\r\nresolve_normal_ct(struct net *net, struct nf_conn *tmpl,\r\nstruct sk_buff *skb,\r\nunsigned int dataoff,\r\nu_int16_t l3num,\r\nu_int8_t protonum,\r\nstruct nf_conntrack_l3proto *l3proto,\r\nstruct nf_conntrack_l4proto *l4proto,\r\nint *set_reply,\r\nenum ip_conntrack_info *ctinfo)\r\n{\r\nstruct nf_conntrack_tuple tuple;\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nu16 zone = tmpl ? nf_ct_zone(tmpl) : NF_CT_DEFAULT_ZONE;\r\nu32 hash;\r\nif (!nf_ct_get_tuple(skb, skb_network_offset(skb),\r\ndataoff, l3num, protonum, &tuple, l3proto,\r\nl4proto)) {\r\npr_debug("resolve_normal_ct: Can't get tuple\n");\r\nreturn NULL;\r\n}\r\nhash = hash_conntrack_raw(&tuple, zone);\r\nh = __nf_conntrack_find_get(net, zone, &tuple, hash);\r\nif (!h) {\r\nh = init_conntrack(net, tmpl, &tuple, l3proto, l4proto,\r\nskb, dataoff, hash);\r\nif (!h)\r\nreturn NULL;\r\nif (IS_ERR(h))\r\nreturn (void *)h;\r\n}\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (NF_CT_DIRECTION(h) == IP_CT_DIR_REPLY) {\r\n*ctinfo = IP_CT_ESTABLISHED_REPLY;\r\n*set_reply = 1;\r\n} else {\r\nif (test_bit(IPS_SEEN_REPLY_BIT, &ct->status)) {\r\npr_debug("nf_conntrack_in: normal packet for %p\n", ct);\r\n*ctinfo = IP_CT_ESTABLISHED;\r\n} else if (test_bit(IPS_EXPECTED_BIT, &ct->status)) {\r\npr_debug("nf_conntrack_in: related packet for %p\n",\r\nct);\r\n*ctinfo = IP_CT_RELATED;\r\n} else {\r\npr_debug("nf_conntrack_in: new packet for %p\n", ct);\r\n*ctinfo = IP_CT_NEW;\r\n}\r\n*set_reply = 0;\r\n}\r\nskb->nfct = &ct->ct_general;\r\nskb->nfctinfo = *ctinfo;\r\nreturn ct;\r\n}\r\nunsigned int\r\nnf_conntrack_in(struct net *net, u_int8_t pf, unsigned int hooknum,\r\nstruct sk_buff *skb)\r\n{\r\nstruct nf_conn *ct, *tmpl = NULL;\r\nenum ip_conntrack_info ctinfo;\r\nstruct nf_conntrack_l3proto *l3proto;\r\nstruct nf_conntrack_l4proto *l4proto;\r\nunsigned int dataoff;\r\nu_int8_t protonum;\r\nint set_reply = 0;\r\nint ret;\r\nif (skb->nfct) {\r\ntmpl = (struct nf_conn *)skb->nfct;\r\nif (!nf_ct_is_template(tmpl)) {\r\nNF_CT_STAT_INC_ATOMIC(net, ignore);\r\nreturn NF_ACCEPT;\r\n}\r\nskb->nfct = NULL;\r\n}\r\nl3proto = __nf_ct_l3proto_find(pf);\r\nret = l3proto->get_l4proto(skb, skb_network_offset(skb),\r\n&dataoff, &protonum);\r\nif (ret <= 0) {\r\npr_debug("not prepared to track yet or error occurred\n");\r\nNF_CT_STAT_INC_ATOMIC(net, error);\r\nNF_CT_STAT_INC_ATOMIC(net, invalid);\r\nret = -ret;\r\ngoto out;\r\n}\r\nl4proto = __nf_ct_l4proto_find(pf, protonum);\r\nif (l4proto->error != NULL) {\r\nret = l4proto->error(net, tmpl, skb, dataoff, &ctinfo,\r\npf, hooknum);\r\nif (ret <= 0) {\r\nNF_CT_STAT_INC_ATOMIC(net, error);\r\nNF_CT_STAT_INC_ATOMIC(net, invalid);\r\nret = -ret;\r\ngoto out;\r\n}\r\nif (skb->nfct)\r\ngoto out;\r\n}\r\nct = resolve_normal_ct(net, tmpl, skb, dataoff, pf, protonum,\r\nl3proto, l4proto, &set_reply, &ctinfo);\r\nif (!ct) {\r\nNF_CT_STAT_INC_ATOMIC(net, invalid);\r\nret = NF_ACCEPT;\r\ngoto out;\r\n}\r\nif (IS_ERR(ct)) {\r\nNF_CT_STAT_INC_ATOMIC(net, drop);\r\nret = NF_DROP;\r\ngoto out;\r\n}\r\nNF_CT_ASSERT(skb->nfct);\r\nret = l4proto->packet(ct, skb, dataoff, ctinfo, pf, hooknum);\r\nif (ret <= 0) {\r\npr_debug("nf_conntrack_in: Can't track with proto module\n");\r\nnf_conntrack_put(skb->nfct);\r\nskb->nfct = NULL;\r\nNF_CT_STAT_INC_ATOMIC(net, invalid);\r\nif (ret == -NF_DROP)\r\nNF_CT_STAT_INC_ATOMIC(net, drop);\r\nret = -ret;\r\ngoto out;\r\n}\r\nif (set_reply && !test_and_set_bit(IPS_SEEN_REPLY_BIT, &ct->status))\r\nnf_conntrack_event_cache(IPCT_REPLY, ct);\r\nout:\r\nif (tmpl) {\r\nif (ret == NF_REPEAT)\r\nskb->nfct = (struct nf_conntrack *)tmpl;\r\nelse\r\nnf_ct_put(tmpl);\r\n}\r\nreturn ret;\r\n}\r\nbool nf_ct_invert_tuplepr(struct nf_conntrack_tuple *inverse,\r\nconst struct nf_conntrack_tuple *orig)\r\n{\r\nbool ret;\r\nrcu_read_lock();\r\nret = nf_ct_invert_tuple(inverse, orig,\r\n__nf_ct_l3proto_find(orig->src.l3num),\r\n__nf_ct_l4proto_find(orig->src.l3num,\r\norig->dst.protonum));\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nvoid nf_conntrack_alter_reply(struct nf_conn *ct,\r\nconst struct nf_conntrack_tuple *newreply)\r\n{\r\nstruct nf_conn_help *help = nfct_help(ct);\r\nNF_CT_ASSERT(!nf_ct_is_confirmed(ct));\r\npr_debug("Altering reply tuple of %p to ", ct);\r\nnf_ct_dump_tuple(newreply);\r\nct->tuplehash[IP_CT_DIR_REPLY].tuple = *newreply;\r\nif (ct->master || (help && !hlist_empty(&help->expectations)))\r\nreturn;\r\nrcu_read_lock();\r\n__nf_ct_try_assign_helper(ct, NULL, GFP_ATOMIC);\r\nrcu_read_unlock();\r\n}\r\nvoid __nf_ct_refresh_acct(struct nf_conn *ct,\r\nenum ip_conntrack_info ctinfo,\r\nconst struct sk_buff *skb,\r\nunsigned long extra_jiffies,\r\nint do_acct)\r\n{\r\nNF_CT_ASSERT(ct->timeout.data == (unsigned long)ct);\r\nNF_CT_ASSERT(skb);\r\nif (test_bit(IPS_FIXED_TIMEOUT_BIT, &ct->status))\r\ngoto acct;\r\nif (!nf_ct_is_confirmed(ct)) {\r\nct->timeout.expires = extra_jiffies;\r\n} else {\r\nunsigned long newtime = jiffies + extra_jiffies;\r\nif (newtime - ct->timeout.expires >= HZ)\r\nmod_timer_pending(&ct->timeout, newtime);\r\n}\r\nacct:\r\nif (do_acct) {\r\nstruct nf_conn_counter *acct;\r\nacct = nf_conn_acct_find(ct);\r\nif (acct) {\r\nspin_lock_bh(&ct->lock);\r\nacct[CTINFO2DIR(ctinfo)].packets++;\r\nacct[CTINFO2DIR(ctinfo)].bytes += skb->len;\r\nspin_unlock_bh(&ct->lock);\r\n}\r\n}\r\n}\r\nbool __nf_ct_kill_acct(struct nf_conn *ct,\r\nenum ip_conntrack_info ctinfo,\r\nconst struct sk_buff *skb,\r\nint do_acct)\r\n{\r\nif (do_acct) {\r\nstruct nf_conn_counter *acct;\r\nacct = nf_conn_acct_find(ct);\r\nif (acct) {\r\nspin_lock_bh(&ct->lock);\r\nacct[CTINFO2DIR(ctinfo)].packets++;\r\nacct[CTINFO2DIR(ctinfo)].bytes +=\r\nskb->len - skb_network_offset(skb);\r\nspin_unlock_bh(&ct->lock);\r\n}\r\n}\r\nif (del_timer(&ct->timeout)) {\r\nct->timeout.function((unsigned long)ct);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nint nf_ct_port_tuple_to_nlattr(struct sk_buff *skb,\r\nconst struct nf_conntrack_tuple *tuple)\r\n{\r\nNLA_PUT_BE16(skb, CTA_PROTO_SRC_PORT, tuple->src.u.tcp.port);\r\nNLA_PUT_BE16(skb, CTA_PROTO_DST_PORT, tuple->dst.u.tcp.port);\r\nreturn 0;\r\nnla_put_failure:\r\nreturn -1;\r\n}\r\nint nf_ct_port_nlattr_to_tuple(struct nlattr *tb[],\r\nstruct nf_conntrack_tuple *t)\r\n{\r\nif (!tb[CTA_PROTO_SRC_PORT] || !tb[CTA_PROTO_DST_PORT])\r\nreturn -EINVAL;\r\nt->src.u.tcp.port = nla_get_be16(tb[CTA_PROTO_SRC_PORT]);\r\nt->dst.u.tcp.port = nla_get_be16(tb[CTA_PROTO_DST_PORT]);\r\nreturn 0;\r\n}\r\nint nf_ct_port_nlattr_tuple_size(void)\r\n{\r\nreturn nla_policy_len(nf_ct_port_nla_policy, CTA_PROTO_MAX + 1);\r\n}\r\nstatic void nf_conntrack_attach(struct sk_buff *nskb, struct sk_buff *skb)\r\n{\r\nstruct nf_conn *ct;\r\nenum ip_conntrack_info ctinfo;\r\nct = nf_ct_get(skb, &ctinfo);\r\nif (CTINFO2DIR(ctinfo) == IP_CT_DIR_ORIGINAL)\r\nctinfo = IP_CT_RELATED_REPLY;\r\nelse\r\nctinfo = IP_CT_RELATED;\r\nnskb->nfct = &ct->ct_general;\r\nnskb->nfctinfo = ctinfo;\r\nnf_conntrack_get(nskb->nfct);\r\n}\r\nstatic struct nf_conn *\r\nget_next_corpse(struct net *net, int (*iter)(struct nf_conn *i, void *data),\r\nvoid *data, unsigned int *bucket)\r\n{\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nstruct hlist_nulls_node *n;\r\nspin_lock_bh(&nf_conntrack_lock);\r\nfor (; *bucket < net->ct.htable_size; (*bucket)++) {\r\nhlist_nulls_for_each_entry(h, n, &net->ct.hash[*bucket], hnnode) {\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (iter(ct, data))\r\ngoto found;\r\n}\r\n}\r\nhlist_nulls_for_each_entry(h, n, &net->ct.unconfirmed, hnnode) {\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (iter(ct, data))\r\nset_bit(IPS_DYING_BIT, &ct->status);\r\n}\r\nspin_unlock_bh(&nf_conntrack_lock);\r\nreturn NULL;\r\nfound:\r\natomic_inc(&ct->ct_general.use);\r\nspin_unlock_bh(&nf_conntrack_lock);\r\nreturn ct;\r\n}\r\nvoid nf_ct_iterate_cleanup(struct net *net,\r\nint (*iter)(struct nf_conn *i, void *data),\r\nvoid *data)\r\n{\r\nstruct nf_conn *ct;\r\nunsigned int bucket = 0;\r\nwhile ((ct = get_next_corpse(net, iter, data, &bucket)) != NULL) {\r\nif (del_timer(&ct->timeout))\r\ndeath_by_timeout((unsigned long)ct);\r\nnf_ct_put(ct);\r\n}\r\n}\r\nstatic int kill_report(struct nf_conn *i, void *data)\r\n{\r\nstruct __nf_ct_flush_report *fr = (struct __nf_ct_flush_report *)data;\r\nstruct nf_conn_tstamp *tstamp;\r\ntstamp = nf_conn_tstamp_find(i);\r\nif (tstamp && tstamp->stop == 0)\r\ntstamp->stop = ktime_to_ns(ktime_get_real());\r\nif (nf_conntrack_event_report(IPCT_DESTROY, i,\r\nfr->pid, fr->report) < 0)\r\nreturn 1;\r\nset_bit(IPS_DYING_BIT, &i->status);\r\nreturn 1;\r\n}\r\nstatic int kill_all(struct nf_conn *i, void *data)\r\n{\r\nreturn 1;\r\n}\r\nvoid nf_ct_free_hashtable(void *hash, unsigned int size)\r\n{\r\nif (is_vmalloc_addr(hash))\r\nvfree(hash);\r\nelse\r\nfree_pages((unsigned long)hash,\r\nget_order(sizeof(struct hlist_head) * size));\r\n}\r\nvoid nf_conntrack_flush_report(struct net *net, u32 pid, int report)\r\n{\r\nstruct __nf_ct_flush_report fr = {\r\n.pid = pid,\r\n.report = report,\r\n};\r\nnf_ct_iterate_cleanup(net, kill_report, &fr);\r\n}\r\nstatic void nf_ct_release_dying_list(struct net *net)\r\n{\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nstruct hlist_nulls_node *n;\r\nspin_lock_bh(&nf_conntrack_lock);\r\nhlist_nulls_for_each_entry(h, n, &net->ct.dying, hnnode) {\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nnf_ct_kill(ct);\r\n}\r\nspin_unlock_bh(&nf_conntrack_lock);\r\n}\r\nstatic int untrack_refs(void)\r\n{\r\nint cnt = 0, cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct nf_conn *ct = &per_cpu(nf_conntrack_untracked, cpu);\r\ncnt += atomic_read(&ct->ct_general.use) - 1;\r\n}\r\nreturn cnt;\r\n}\r\nstatic void nf_conntrack_cleanup_init_net(void)\r\n{\r\nwhile (untrack_refs() > 0)\r\nschedule();\r\nnf_conntrack_helper_fini();\r\nnf_conntrack_proto_fini();\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nnf_ct_extend_unregister(&nf_ct_zone_extend);\r\n#endif\r\n}\r\nstatic void nf_conntrack_cleanup_net(struct net *net)\r\n{\r\ni_see_dead_people:\r\nnf_ct_iterate_cleanup(net, kill_all, NULL);\r\nnf_ct_release_dying_list(net);\r\nif (atomic_read(&net->ct.count) != 0) {\r\nschedule();\r\ngoto i_see_dead_people;\r\n}\r\nnf_ct_free_hashtable(net->ct.hash, net->ct.htable_size);\r\nnf_conntrack_ecache_fini(net);\r\nnf_conntrack_tstamp_fini(net);\r\nnf_conntrack_acct_fini(net);\r\nnf_conntrack_expect_fini(net);\r\nkmem_cache_destroy(net->ct.nf_conntrack_cachep);\r\nkfree(net->ct.slabname);\r\nfree_percpu(net->ct.stat);\r\n}\r\nvoid nf_conntrack_cleanup(struct net *net)\r\n{\r\nif (net_eq(net, &init_net))\r\nrcu_assign_pointer(ip_ct_attach, NULL);\r\nsynchronize_net();\r\nnf_conntrack_cleanup_net(net);\r\nif (net_eq(net, &init_net)) {\r\nrcu_assign_pointer(nf_ct_destroy, NULL);\r\nnf_conntrack_cleanup_init_net();\r\n}\r\n}\r\nvoid *nf_ct_alloc_hashtable(unsigned int *sizep, int nulls)\r\n{\r\nstruct hlist_nulls_head *hash;\r\nunsigned int nr_slots, i;\r\nsize_t sz;\r\nBUILD_BUG_ON(sizeof(struct hlist_nulls_head) != sizeof(struct hlist_head));\r\nnr_slots = *sizep = roundup(*sizep, PAGE_SIZE / sizeof(struct hlist_nulls_head));\r\nsz = nr_slots * sizeof(struct hlist_nulls_head);\r\nhash = (void *)__get_free_pages(GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO,\r\nget_order(sz));\r\nif (!hash) {\r\nprintk(KERN_WARNING "nf_conntrack: falling back to vmalloc.\n");\r\nhash = __vmalloc(sz, GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO,\r\nPAGE_KERNEL);\r\n}\r\nif (hash && nulls)\r\nfor (i = 0; i < nr_slots; i++)\r\nINIT_HLIST_NULLS_HEAD(&hash[i], i);\r\nreturn hash;\r\n}\r\nint nf_conntrack_set_hashsize(const char *val, struct kernel_param *kp)\r\n{\r\nint i, bucket;\r\nunsigned int hashsize, old_size;\r\nstruct hlist_nulls_head *hash, *old_hash;\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nif (current->nsproxy->net_ns != &init_net)\r\nreturn -EOPNOTSUPP;\r\nif (!nf_conntrack_htable_size)\r\nreturn param_set_uint(val, kp);\r\nhashsize = simple_strtoul(val, NULL, 0);\r\nif (!hashsize)\r\nreturn -EINVAL;\r\nhash = nf_ct_alloc_hashtable(&hashsize, 1);\r\nif (!hash)\r\nreturn -ENOMEM;\r\nspin_lock_bh(&nf_conntrack_lock);\r\nfor (i = 0; i < init_net.ct.htable_size; i++) {\r\nwhile (!hlist_nulls_empty(&init_net.ct.hash[i])) {\r\nh = hlist_nulls_entry(init_net.ct.hash[i].first,\r\nstruct nf_conntrack_tuple_hash, hnnode);\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nhlist_nulls_del_rcu(&h->hnnode);\r\nbucket = __hash_conntrack(&h->tuple, nf_ct_zone(ct),\r\nhashsize);\r\nhlist_nulls_add_head_rcu(&h->hnnode, &hash[bucket]);\r\n}\r\n}\r\nold_size = init_net.ct.htable_size;\r\nold_hash = init_net.ct.hash;\r\ninit_net.ct.htable_size = nf_conntrack_htable_size = hashsize;\r\ninit_net.ct.hash = hash;\r\nspin_unlock_bh(&nf_conntrack_lock);\r\nnf_ct_free_hashtable(old_hash, old_size);\r\nreturn 0;\r\n}\r\nvoid nf_ct_untracked_status_or(unsigned long bits)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu)\r\nper_cpu(nf_conntrack_untracked, cpu).status |= bits;\r\n}\r\nstatic int nf_conntrack_init_init_net(void)\r\n{\r\nint max_factor = 8;\r\nint ret, cpu;\r\nif (!nf_conntrack_htable_size) {\r\nnf_conntrack_htable_size\r\n= (((totalram_pages << PAGE_SHIFT) / 16384)\r\n/ sizeof(struct hlist_head));\r\nif (totalram_pages > (1024 * 1024 * 1024 / PAGE_SIZE))\r\nnf_conntrack_htable_size = 16384;\r\nif (nf_conntrack_htable_size < 32)\r\nnf_conntrack_htable_size = 32;\r\nmax_factor = 4;\r\n}\r\nnf_conntrack_max = max_factor * nf_conntrack_htable_size;\r\nprintk(KERN_INFO "nf_conntrack version %s (%u buckets, %d max)\n",\r\nNF_CONNTRACK_VERSION, nf_conntrack_htable_size,\r\nnf_conntrack_max);\r\nret = nf_conntrack_proto_init();\r\nif (ret < 0)\r\ngoto err_proto;\r\nret = nf_conntrack_helper_init();\r\nif (ret < 0)\r\ngoto err_helper;\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nret = nf_ct_extend_register(&nf_ct_zone_extend);\r\nif (ret < 0)\r\ngoto err_extend;\r\n#endif\r\nfor_each_possible_cpu(cpu) {\r\nstruct nf_conn *ct = &per_cpu(nf_conntrack_untracked, cpu);\r\nwrite_pnet(&ct->ct_net, &init_net);\r\natomic_set(&ct->ct_general.use, 1);\r\n}\r\nnf_ct_untracked_status_or(IPS_CONFIRMED | IPS_UNTRACKED);\r\nreturn 0;\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nerr_extend:\r\nnf_conntrack_helper_fini();\r\n#endif\r\nerr_helper:\r\nnf_conntrack_proto_fini();\r\nerr_proto:\r\nreturn ret;\r\n}\r\nstatic int nf_conntrack_init_net(struct net *net)\r\n{\r\nint ret;\r\natomic_set(&net->ct.count, 0);\r\nINIT_HLIST_NULLS_HEAD(&net->ct.unconfirmed, UNCONFIRMED_NULLS_VAL);\r\nINIT_HLIST_NULLS_HEAD(&net->ct.dying, DYING_NULLS_VAL);\r\nnet->ct.stat = alloc_percpu(struct ip_conntrack_stat);\r\nif (!net->ct.stat) {\r\nret = -ENOMEM;\r\ngoto err_stat;\r\n}\r\nnet->ct.slabname = kasprintf(GFP_KERNEL, "nf_conntrack_%p", net);\r\nif (!net->ct.slabname) {\r\nret = -ENOMEM;\r\ngoto err_slabname;\r\n}\r\nnet->ct.nf_conntrack_cachep = kmem_cache_create(net->ct.slabname,\r\nsizeof(struct nf_conn), 0,\r\nSLAB_DESTROY_BY_RCU, NULL);\r\nif (!net->ct.nf_conntrack_cachep) {\r\nprintk(KERN_ERR "Unable to create nf_conn slab cache\n");\r\nret = -ENOMEM;\r\ngoto err_cache;\r\n}\r\nnet->ct.htable_size = nf_conntrack_htable_size;\r\nnet->ct.hash = nf_ct_alloc_hashtable(&net->ct.htable_size, 1);\r\nif (!net->ct.hash) {\r\nret = -ENOMEM;\r\nprintk(KERN_ERR "Unable to create nf_conntrack_hash\n");\r\ngoto err_hash;\r\n}\r\nret = nf_conntrack_expect_init(net);\r\nif (ret < 0)\r\ngoto err_expect;\r\nret = nf_conntrack_acct_init(net);\r\nif (ret < 0)\r\ngoto err_acct;\r\nret = nf_conntrack_tstamp_init(net);\r\nif (ret < 0)\r\ngoto err_tstamp;\r\nret = nf_conntrack_ecache_init(net);\r\nif (ret < 0)\r\ngoto err_ecache;\r\nreturn 0;\r\nerr_ecache:\r\nnf_conntrack_tstamp_fini(net);\r\nerr_tstamp:\r\nnf_conntrack_acct_fini(net);\r\nerr_acct:\r\nnf_conntrack_expect_fini(net);\r\nerr_expect:\r\nnf_ct_free_hashtable(net->ct.hash, net->ct.htable_size);\r\nerr_hash:\r\nkmem_cache_destroy(net->ct.nf_conntrack_cachep);\r\nerr_cache:\r\nkfree(net->ct.slabname);\r\nerr_slabname:\r\nfree_percpu(net->ct.stat);\r\nerr_stat:\r\nreturn ret;\r\n}\r\nint nf_conntrack_init(struct net *net)\r\n{\r\nint ret;\r\nif (net_eq(net, &init_net)) {\r\nret = nf_conntrack_init_init_net();\r\nif (ret < 0)\r\ngoto out_init_net;\r\n}\r\nret = nf_conntrack_init_net(net);\r\nif (ret < 0)\r\ngoto out_net;\r\nif (net_eq(net, &init_net)) {\r\nrcu_assign_pointer(ip_ct_attach, nf_conntrack_attach);\r\nrcu_assign_pointer(nf_ct_destroy, destroy_conntrack);\r\nrcu_assign_pointer(nf_ct_nat_offset, NULL);\r\n}\r\nreturn 0;\r\nout_net:\r\nif (net_eq(net, &init_net))\r\nnf_conntrack_cleanup_init_net();\r\nout_init_net:\r\nreturn ret;\r\n}
