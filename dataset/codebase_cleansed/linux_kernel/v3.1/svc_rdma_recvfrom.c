static void rdma_build_arg_xdr(struct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *ctxt,\r\nu32 byte_count)\r\n{\r\nstruct page *page;\r\nu32 bc;\r\nint sge_no;\r\npage = ctxt->pages[0];\r\nput_page(rqstp->rq_pages[0]);\r\nrqstp->rq_pages[0] = page;\r\nrqstp->rq_arg.head[0].iov_base = page_address(page);\r\nrqstp->rq_arg.head[0].iov_len = min(byte_count, ctxt->sge[0].length);\r\nrqstp->rq_arg.len = byte_count;\r\nrqstp->rq_arg.buflen = byte_count;\r\nbc = byte_count - rqstp->rq_arg.head[0].iov_len;\r\nrqstp->rq_arg.page_len = bc;\r\nrqstp->rq_arg.page_base = 0;\r\nrqstp->rq_arg.pages = &rqstp->rq_pages[1];\r\nsge_no = 1;\r\nwhile (bc && sge_no < ctxt->count) {\r\npage = ctxt->pages[sge_no];\r\nput_page(rqstp->rq_pages[sge_no]);\r\nrqstp->rq_pages[sge_no] = page;\r\nbc -= min(bc, ctxt->sge[sge_no].length);\r\nrqstp->rq_arg.buflen += ctxt->sge[sge_no].length;\r\nsge_no++;\r\n}\r\nrqstp->rq_respages = &rqstp->rq_pages[sge_no];\r\nBUG_ON(bc && (sge_no == ctxt->count));\r\nBUG_ON((rqstp->rq_arg.head[0].iov_len + rqstp->rq_arg.page_len)\r\n!= byte_count);\r\nBUG_ON(rqstp->rq_arg.len != byte_count);\r\nbc = sge_no;\r\nwhile (sge_no < ctxt->count) {\r\npage = ctxt->pages[sge_no++];\r\nput_page(page);\r\n}\r\nctxt->count = bc;\r\nrqstp->rq_arg.tail[0].iov_base = NULL;\r\nrqstp->rq_arg.tail[0].iov_len = 0;\r\n}\r\nstatic int map_read_chunks(struct svcxprt_rdma *xprt,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head,\r\nstruct rpcrdma_msg *rmsgp,\r\nstruct svc_rdma_req_map *rpl_map,\r\nstruct svc_rdma_req_map *chl_map,\r\nint ch_count,\r\nint byte_count)\r\n{\r\nint sge_no;\r\nint sge_bytes;\r\nint page_off;\r\nint page_no;\r\nint ch_bytes;\r\nint ch_no;\r\nstruct rpcrdma_read_chunk *ch;\r\nsge_no = 0;\r\npage_no = 0;\r\npage_off = 0;\r\nch = (struct rpcrdma_read_chunk *)&rmsgp->rm_body.rm_chunks[0];\r\nch_no = 0;\r\nch_bytes = ch->rc_target.rs_length;\r\nhead->arg.head[0] = rqstp->rq_arg.head[0];\r\nhead->arg.tail[0] = rqstp->rq_arg.tail[0];\r\nhead->arg.pages = &head->pages[head->count];\r\nhead->hdr_count = head->count;\r\nhead->arg.page_base = 0;\r\nhead->arg.page_len = ch_bytes;\r\nhead->arg.len = rqstp->rq_arg.len + ch_bytes;\r\nhead->arg.buflen = rqstp->rq_arg.buflen + ch_bytes;\r\nhead->count++;\r\nchl_map->ch[0].start = 0;\r\nwhile (byte_count) {\r\nrpl_map->sge[sge_no].iov_base =\r\npage_address(rqstp->rq_arg.pages[page_no]) + page_off;\r\nsge_bytes = min_t(int, PAGE_SIZE-page_off, ch_bytes);\r\nrpl_map->sge[sge_no].iov_len = sge_bytes;\r\nhead->arg.pages[page_no] = rqstp->rq_arg.pages[page_no];\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[page_no+1];\r\nbyte_count -= sge_bytes;\r\nch_bytes -= sge_bytes;\r\nsge_no++;\r\nif (ch_bytes == 0) {\r\nchl_map->ch[ch_no].count =\r\nsge_no - chl_map->ch[ch_no].start;\r\nch_no++;\r\nch++;\r\nchl_map->ch[ch_no].start = sge_no;\r\nch_bytes = ch->rc_target.rs_length;\r\nif (byte_count) {\r\nhead->arg.page_len += ch_bytes;\r\nhead->arg.len += ch_bytes;\r\nhead->arg.buflen += ch_bytes;\r\n}\r\n}\r\nif ((sge_bytes + page_off) == PAGE_SIZE) {\r\npage_no++;\r\npage_off = 0;\r\nif (byte_count)\r\nhead->count++;\r\n} else\r\npage_off += sge_bytes;\r\n}\r\nBUG_ON(byte_count != 0);\r\nreturn sge_no;\r\n}\r\nstatic int fast_reg_read_chunks(struct svcxprt_rdma *xprt,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head,\r\nstruct rpcrdma_msg *rmsgp,\r\nstruct svc_rdma_req_map *rpl_map,\r\nstruct svc_rdma_req_map *chl_map,\r\nint ch_count,\r\nint byte_count)\r\n{\r\nint page_no;\r\nint ch_no;\r\nu32 offset;\r\nstruct rpcrdma_read_chunk *ch;\r\nstruct svc_rdma_fastreg_mr *frmr;\r\nint ret = 0;\r\nfrmr = svc_rdma_get_frmr(xprt);\r\nif (IS_ERR(frmr))\r\nreturn -ENOMEM;\r\nhead->frmr = frmr;\r\nhead->arg.head[0] = rqstp->rq_arg.head[0];\r\nhead->arg.tail[0] = rqstp->rq_arg.tail[0];\r\nhead->arg.pages = &head->pages[head->count];\r\nhead->hdr_count = head->count;\r\nhead->arg.page_base = 0;\r\nhead->arg.page_len = byte_count;\r\nhead->arg.len = rqstp->rq_arg.len + byte_count;\r\nhead->arg.buflen = rqstp->rq_arg.buflen + byte_count;\r\nfrmr->kva = page_address(rqstp->rq_arg.pages[0]);\r\nfrmr->direction = DMA_FROM_DEVICE;\r\nfrmr->access_flags = (IB_ACCESS_LOCAL_WRITE|IB_ACCESS_REMOTE_WRITE);\r\nfrmr->map_len = byte_count;\r\nfrmr->page_list_len = PAGE_ALIGN(byte_count) >> PAGE_SHIFT;\r\nfor (page_no = 0; page_no < frmr->page_list_len; page_no++) {\r\nfrmr->page_list->page_list[page_no] =\r\nib_dma_map_page(xprt->sc_cm_id->device,\r\nrqstp->rq_arg.pages[page_no], 0,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nfrmr->page_list->page_list[page_no]))\r\ngoto fatal_err;\r\natomic_inc(&xprt->sc_dma_used);\r\nhead->arg.pages[page_no] = rqstp->rq_arg.pages[page_no];\r\n}\r\nhead->count += page_no;\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[page_no];\r\noffset = 0;\r\nch = (struct rpcrdma_read_chunk *)&rmsgp->rm_body.rm_chunks[0];\r\nfor (ch_no = 0; ch_no < ch_count; ch_no++) {\r\nrpl_map->sge[ch_no].iov_base = frmr->kva + offset;\r\nrpl_map->sge[ch_no].iov_len = ch->rc_target.rs_length;\r\nchl_map->ch[ch_no].count = 1;\r\nchl_map->ch[ch_no].start = ch_no;\r\noffset += ch->rc_target.rs_length;\r\nch++;\r\n}\r\nret = svc_rdma_fastreg(xprt, frmr);\r\nif (ret)\r\ngoto fatal_err;\r\nreturn ch_no;\r\nfatal_err:\r\nprintk("svcrdma: error fast registering xdr for xprt %p", xprt);\r\nsvc_rdma_put_frmr(xprt, frmr);\r\nreturn -EIO;\r\n}\r\nstatic int rdma_set_ctxt_sge(struct svcxprt_rdma *xprt,\r\nstruct svc_rdma_op_ctxt *ctxt,\r\nstruct svc_rdma_fastreg_mr *frmr,\r\nstruct kvec *vec,\r\nu64 *sgl_offset,\r\nint count)\r\n{\r\nint i;\r\nunsigned long off;\r\nctxt->count = count;\r\nctxt->direction = DMA_FROM_DEVICE;\r\nfor (i = 0; i < count; i++) {\r\nctxt->sge[i].length = 0;\r\nif (!frmr) {\r\nBUG_ON(0 == virt_to_page(vec[i].iov_base));\r\noff = (unsigned long)vec[i].iov_base & ~PAGE_MASK;\r\nctxt->sge[i].addr =\r\nib_dma_map_page(xprt->sc_cm_id->device,\r\nvirt_to_page(vec[i].iov_base),\r\noff,\r\nvec[i].iov_len,\r\nDMA_FROM_DEVICE);\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nctxt->sge[i].addr))\r\nreturn -EINVAL;\r\nctxt->sge[i].lkey = xprt->sc_dma_lkey;\r\natomic_inc(&xprt->sc_dma_used);\r\n} else {\r\nctxt->sge[i].addr = (unsigned long)vec[i].iov_base;\r\nctxt->sge[i].lkey = frmr->mr->lkey;\r\n}\r\nctxt->sge[i].length = vec[i].iov_len;\r\n*sgl_offset = *sgl_offset + vec[i].iov_len;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rdma_read_max_sge(struct svcxprt_rdma *xprt, int sge_count)\r\n{\r\nif ((rdma_node_get_transport(xprt->sc_cm_id->device->node_type) ==\r\nRDMA_TRANSPORT_IWARP) &&\r\nsge_count > 1)\r\nreturn 1;\r\nelse\r\nreturn min_t(int, sge_count, xprt->sc_max_sge);\r\n}\r\nstatic int rdma_read_xdr(struct svcxprt_rdma *xprt,\r\nstruct rpcrdma_msg *rmsgp,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *hdr_ctxt)\r\n{\r\nstruct ib_send_wr read_wr;\r\nstruct ib_send_wr inv_wr;\r\nint err = 0;\r\nint ch_no;\r\nint ch_count;\r\nint byte_count;\r\nint sge_count;\r\nu64 sgl_offset;\r\nstruct rpcrdma_read_chunk *ch;\r\nstruct svc_rdma_op_ctxt *ctxt = NULL;\r\nstruct svc_rdma_req_map *rpl_map;\r\nstruct svc_rdma_req_map *chl_map;\r\nch = svc_rdma_get_read_chunk(rmsgp);\r\nif (!ch)\r\nreturn 0;\r\nsvc_rdma_rcl_chunk_counts(ch, &ch_count, &byte_count);\r\nif (ch_count > RPCSVC_MAXPAGES)\r\nreturn -EINVAL;\r\nrpl_map = svc_rdma_get_req_map();\r\nchl_map = svc_rdma_get_req_map();\r\nif (!xprt->sc_frmr_pg_list_len)\r\nsge_count = map_read_chunks(xprt, rqstp, hdr_ctxt, rmsgp,\r\nrpl_map, chl_map, ch_count,\r\nbyte_count);\r\nelse\r\nsge_count = fast_reg_read_chunks(xprt, rqstp, hdr_ctxt, rmsgp,\r\nrpl_map, chl_map, ch_count,\r\nbyte_count);\r\nif (sge_count < 0) {\r\nerr = -EIO;\r\ngoto out;\r\n}\r\nsgl_offset = 0;\r\nch_no = 0;\r\nfor (ch = (struct rpcrdma_read_chunk *)&rmsgp->rm_body.rm_chunks[0];\r\nch->rc_discrim != 0; ch++, ch_no++) {\r\nnext_sge:\r\nctxt = svc_rdma_get_context(xprt);\r\nctxt->direction = DMA_FROM_DEVICE;\r\nctxt->frmr = hdr_ctxt->frmr;\r\nctxt->read_hdr = NULL;\r\nclear_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\nclear_bit(RDMACTXT_F_FAST_UNREG, &ctxt->flags);\r\nmemset(&read_wr, 0, sizeof read_wr);\r\nread_wr.wr_id = (unsigned long)ctxt;\r\nread_wr.opcode = IB_WR_RDMA_READ;\r\nctxt->wr_op = read_wr.opcode;\r\nread_wr.send_flags = IB_SEND_SIGNALED;\r\nread_wr.wr.rdma.rkey = ch->rc_target.rs_handle;\r\nread_wr.wr.rdma.remote_addr =\r\nget_unaligned(&(ch->rc_target.rs_offset)) +\r\nsgl_offset;\r\nread_wr.sg_list = ctxt->sge;\r\nread_wr.num_sge =\r\nrdma_read_max_sge(xprt, chl_map->ch[ch_no].count);\r\nerr = rdma_set_ctxt_sge(xprt, ctxt, hdr_ctxt->frmr,\r\n&rpl_map->sge[chl_map->ch[ch_no].start],\r\n&sgl_offset,\r\nread_wr.num_sge);\r\nif (err) {\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 0);\r\ngoto out;\r\n}\r\nif (((ch+1)->rc_discrim == 0) &&\r\n(read_wr.num_sge == chl_map->ch[ch_no].count)) {\r\nset_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\nif (hdr_ctxt->frmr) {\r\nset_bit(RDMACTXT_F_FAST_UNREG, &ctxt->flags);\r\nif (xprt->sc_dev_caps &\r\nSVCRDMA_DEVCAP_READ_W_INV) {\r\nread_wr.opcode =\r\nIB_WR_RDMA_READ_WITH_INV;\r\nctxt->wr_op = read_wr.opcode;\r\nread_wr.ex.invalidate_rkey =\r\nctxt->frmr->mr->lkey;\r\n} else {\r\nmemset(&inv_wr, 0, sizeof inv_wr);\r\ninv_wr.opcode = IB_WR_LOCAL_INV;\r\ninv_wr.send_flags = IB_SEND_SIGNALED;\r\ninv_wr.ex.invalidate_rkey =\r\nhdr_ctxt->frmr->mr->lkey;\r\nread_wr.next = &inv_wr;\r\n}\r\n}\r\nctxt->read_hdr = hdr_ctxt;\r\n}\r\nerr = svc_rdma_send(xprt, &read_wr);\r\nif (err) {\r\nprintk(KERN_ERR "svcrdma: Error %d posting RDMA_READ\n",\r\nerr);\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 0);\r\ngoto out;\r\n}\r\natomic_inc(&rdma_stat_read);\r\nif (read_wr.num_sge < chl_map->ch[ch_no].count) {\r\nchl_map->ch[ch_no].count -= read_wr.num_sge;\r\nchl_map->ch[ch_no].start += read_wr.num_sge;\r\ngoto next_sge;\r\n}\r\nsgl_offset = 0;\r\nerr = 1;\r\n}\r\nout:\r\nsvc_rdma_put_req_map(rpl_map);\r\nsvc_rdma_put_req_map(chl_map);\r\nfor (ch_no = 0; &rqstp->rq_pages[ch_no] < rqstp->rq_respages; ch_no++)\r\nrqstp->rq_pages[ch_no] = NULL;\r\nwhile (rqstp->rq_resused)\r\nrqstp->rq_respages[--rqstp->rq_resused] = NULL;\r\nreturn err;\r\n}\r\nstatic int rdma_read_complete(struct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head)\r\n{\r\nint page_no;\r\nint ret;\r\nBUG_ON(!head);\r\nfor (page_no = 0; page_no < head->count; page_no++) {\r\nput_page(rqstp->rq_pages[page_no]);\r\nrqstp->rq_pages[page_no] = head->pages[page_no];\r\n}\r\nrqstp->rq_arg.pages = &rqstp->rq_pages[head->hdr_count];\r\nrqstp->rq_arg.page_len = head->arg.page_len;\r\nrqstp->rq_arg.page_base = head->arg.page_base;\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[page_no];\r\nrqstp->rq_resused = 0;\r\nrqstp->rq_arg.head[0] = head->arg.head[0];\r\nrqstp->rq_arg.tail[0] = head->arg.tail[0];\r\nrqstp->rq_arg.len = head->arg.len;\r\nrqstp->rq_arg.buflen = head->arg.buflen;\r\nsvc_rdma_put_context(head, 0);\r\nrqstp->rq_prot = IPPROTO_MAX;\r\nsvc_xprt_copy_addrs(rqstp, rqstp->rq_xprt);\r\nret = rqstp->rq_arg.head[0].iov_len\r\n+ rqstp->rq_arg.page_len\r\n+ rqstp->rq_arg.tail[0].iov_len;\r\ndprintk("svcrdma: deferred read ret=%d, rq_arg.len =%d, "\r\n"rq_arg.head[0].iov_base=%p, rq_arg.head[0].iov_len = %zd\n",\r\nret, rqstp->rq_arg.len, rqstp->rq_arg.head[0].iov_base,\r\nrqstp->rq_arg.head[0].iov_len);\r\nreturn ret;\r\n}\r\nint svc_rdma_recvfrom(struct svc_rqst *rqstp)\r\n{\r\nstruct svc_xprt *xprt = rqstp->rq_xprt;\r\nstruct svcxprt_rdma *rdma_xprt =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nstruct svc_rdma_op_ctxt *ctxt = NULL;\r\nstruct rpcrdma_msg *rmsgp;\r\nint ret = 0;\r\nint len;\r\ndprintk("svcrdma: rqstp=%p\n", rqstp);\r\nspin_lock_bh(&rdma_xprt->sc_rq_dto_lock);\r\nif (!list_empty(&rdma_xprt->sc_read_complete_q)) {\r\nctxt = list_entry(rdma_xprt->sc_read_complete_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\n}\r\nif (ctxt) {\r\nspin_unlock_bh(&rdma_xprt->sc_rq_dto_lock);\r\nreturn rdma_read_complete(rqstp, ctxt);\r\n}\r\nif (!list_empty(&rdma_xprt->sc_rq_dto_q)) {\r\nctxt = list_entry(rdma_xprt->sc_rq_dto_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\n} else {\r\natomic_inc(&rdma_stat_rq_starve);\r\nclear_bit(XPT_DATA, &xprt->xpt_flags);\r\nctxt = NULL;\r\n}\r\nspin_unlock_bh(&rdma_xprt->sc_rq_dto_lock);\r\nif (!ctxt) {\r\nif (test_bit(XPT_CLOSE, &xprt->xpt_flags))\r\ngoto close_out;\r\nBUG_ON(ret);\r\ngoto out;\r\n}\r\ndprintk("svcrdma: processing ctxt=%p on xprt=%p, rqstp=%p, status=%d\n",\r\nctxt, rdma_xprt, rqstp, ctxt->wc_status);\r\nBUG_ON(ctxt->wc_status != IB_WC_SUCCESS);\r\natomic_inc(&rdma_stat_recv);\r\nrdma_build_arg_xdr(rqstp, ctxt, ctxt->byte_len);\r\nlen = svc_rdma_xdr_decode_req(&rmsgp, rqstp);\r\nrqstp->rq_xprt_hlen = len;\r\nif (len < 0) {\r\nif (len == -ENOSYS)\r\nsvc_rdma_send_error(rdma_xprt, rmsgp, ERR_VERS);\r\ngoto close_out;\r\n}\r\nret = rdma_read_xdr(rdma_xprt, rmsgp, rqstp, ctxt);\r\nif (ret > 0) {\r\ngoto defer;\r\n}\r\nif (ret < 0) {\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn 0;\r\n}\r\nret = rqstp->rq_arg.head[0].iov_len\r\n+ rqstp->rq_arg.page_len\r\n+ rqstp->rq_arg.tail[0].iov_len;\r\nsvc_rdma_put_context(ctxt, 0);\r\nout:\r\ndprintk("svcrdma: ret = %d, rq_arg.len =%d, "\r\n"rq_arg.head[0].iov_base=%p, rq_arg.head[0].iov_len = %zd\n",\r\nret, rqstp->rq_arg.len,\r\nrqstp->rq_arg.head[0].iov_base,\r\nrqstp->rq_arg.head[0].iov_len);\r\nrqstp->rq_prot = IPPROTO_MAX;\r\nsvc_xprt_copy_addrs(rqstp, xprt);\r\nreturn ret;\r\nclose_out:\r\nif (ctxt)\r\nsvc_rdma_put_context(ctxt, 1);\r\ndprintk("svcrdma: transport %p is closing\n", xprt);\r\nset_bit(XPT_CLOSE, &xprt->xpt_flags);\r\ndefer:\r\nreturn 0;\r\n}
