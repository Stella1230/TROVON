static int comp_tree_refs(struct btrfs_delayed_tree_ref *ref2,\r\nstruct btrfs_delayed_tree_ref *ref1)\r\n{\r\nif (ref1->node.type == BTRFS_TREE_BLOCK_REF_KEY) {\r\nif (ref1->root < ref2->root)\r\nreturn -1;\r\nif (ref1->root > ref2->root)\r\nreturn 1;\r\n} else {\r\nif (ref1->parent < ref2->parent)\r\nreturn -1;\r\nif (ref1->parent > ref2->parent)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int comp_data_refs(struct btrfs_delayed_data_ref *ref2,\r\nstruct btrfs_delayed_data_ref *ref1)\r\n{\r\nif (ref1->node.type == BTRFS_EXTENT_DATA_REF_KEY) {\r\nif (ref1->root < ref2->root)\r\nreturn -1;\r\nif (ref1->root > ref2->root)\r\nreturn 1;\r\nif (ref1->objectid < ref2->objectid)\r\nreturn -1;\r\nif (ref1->objectid > ref2->objectid)\r\nreturn 1;\r\nif (ref1->offset < ref2->offset)\r\nreturn -1;\r\nif (ref1->offset > ref2->offset)\r\nreturn 1;\r\n} else {\r\nif (ref1->parent < ref2->parent)\r\nreturn -1;\r\nif (ref1->parent > ref2->parent)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int comp_entry(struct btrfs_delayed_ref_node *ref2,\r\nstruct btrfs_delayed_ref_node *ref1)\r\n{\r\nif (ref1->bytenr < ref2->bytenr)\r\nreturn -1;\r\nif (ref1->bytenr > ref2->bytenr)\r\nreturn 1;\r\nif (ref1->is_head && ref2->is_head)\r\nreturn 0;\r\nif (ref2->is_head)\r\nreturn -1;\r\nif (ref1->is_head)\r\nreturn 1;\r\nif (ref1->type < ref2->type)\r\nreturn -1;\r\nif (ref1->type > ref2->type)\r\nreturn 1;\r\nif (ref1->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nref1->type == BTRFS_SHARED_BLOCK_REF_KEY) {\r\nreturn comp_tree_refs(btrfs_delayed_node_to_tree_ref(ref2),\r\nbtrfs_delayed_node_to_tree_ref(ref1));\r\n} else if (ref1->type == BTRFS_EXTENT_DATA_REF_KEY ||\r\nref1->type == BTRFS_SHARED_DATA_REF_KEY) {\r\nreturn comp_data_refs(btrfs_delayed_node_to_data_ref(ref2),\r\nbtrfs_delayed_node_to_data_ref(ref1));\r\n}\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic struct btrfs_delayed_ref_node *tree_insert(struct rb_root *root,\r\nstruct rb_node *node)\r\n{\r\nstruct rb_node **p = &root->rb_node;\r\nstruct rb_node *parent_node = NULL;\r\nstruct btrfs_delayed_ref_node *entry;\r\nstruct btrfs_delayed_ref_node *ins;\r\nint cmp;\r\nins = rb_entry(node, struct btrfs_delayed_ref_node, rb_node);\r\nwhile (*p) {\r\nparent_node = *p;\r\nentry = rb_entry(parent_node, struct btrfs_delayed_ref_node,\r\nrb_node);\r\ncmp = comp_entry(entry, ins);\r\nif (cmp < 0)\r\np = &(*p)->rb_left;\r\nelse if (cmp > 0)\r\np = &(*p)->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nrb_link_node(node, parent_node, p);\r\nrb_insert_color(node, root);\r\nreturn NULL;\r\n}\r\nstatic struct btrfs_delayed_ref_node *find_ref_head(struct rb_root *root,\r\nu64 bytenr,\r\nstruct btrfs_delayed_ref_node **last)\r\n{\r\nstruct rb_node *n = root->rb_node;\r\nstruct btrfs_delayed_ref_node *entry;\r\nint cmp;\r\nwhile (n) {\r\nentry = rb_entry(n, struct btrfs_delayed_ref_node, rb_node);\r\nWARN_ON(!entry->in_tree);\r\nif (last)\r\n*last = entry;\r\nif (bytenr < entry->bytenr)\r\ncmp = -1;\r\nelse if (bytenr > entry->bytenr)\r\ncmp = 1;\r\nelse if (!btrfs_delayed_ref_is_head(entry))\r\ncmp = 1;\r\nelse\r\ncmp = 0;\r\nif (cmp < 0)\r\nn = n->rb_left;\r\nelse if (cmp > 0)\r\nn = n->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nreturn NULL;\r\n}\r\nint btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_head *head)\r\n{\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nassert_spin_locked(&delayed_refs->lock);\r\nif (mutex_trylock(&head->mutex))\r\nreturn 0;\r\natomic_inc(&head->node.refs);\r\nspin_unlock(&delayed_refs->lock);\r\nmutex_lock(&head->mutex);\r\nspin_lock(&delayed_refs->lock);\r\nif (!head->node.in_tree) {\r\nmutex_unlock(&head->mutex);\r\nbtrfs_put_delayed_ref(&head->node);\r\nreturn -EAGAIN;\r\n}\r\nbtrfs_put_delayed_ref(&head->node);\r\nreturn 0;\r\n}\r\nint btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,\r\nstruct list_head *cluster, u64 start)\r\n{\r\nint count = 0;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nstruct rb_node *node;\r\nstruct btrfs_delayed_ref_node *ref;\r\nstruct btrfs_delayed_ref_head *head;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nif (start == 0) {\r\nnode = rb_first(&delayed_refs->root);\r\n} else {\r\nref = NULL;\r\nfind_ref_head(&delayed_refs->root, start, &ref);\r\nif (ref) {\r\nstruct btrfs_delayed_ref_node *tmp;\r\nnode = rb_prev(&ref->rb_node);\r\nwhile (node) {\r\ntmp = rb_entry(node,\r\nstruct btrfs_delayed_ref_node,\r\nrb_node);\r\nif (tmp->bytenr < start)\r\nbreak;\r\nref = tmp;\r\nnode = rb_prev(&ref->rb_node);\r\n}\r\nnode = &ref->rb_node;\r\n} else\r\nnode = rb_first(&delayed_refs->root);\r\n}\r\nagain:\r\nwhile (node && count < 32) {\r\nref = rb_entry(node, struct btrfs_delayed_ref_node, rb_node);\r\nif (btrfs_delayed_ref_is_head(ref)) {\r\nhead = btrfs_delayed_node_to_head(ref);\r\nif (list_empty(&head->cluster)) {\r\nlist_add_tail(&head->cluster, cluster);\r\ndelayed_refs->run_delayed_start =\r\nhead->node.bytenr;\r\ncount++;\r\nWARN_ON(delayed_refs->num_heads_ready == 0);\r\ndelayed_refs->num_heads_ready--;\r\n} else if (count) {\r\nbreak;\r\n}\r\n}\r\nnode = rb_next(node);\r\n}\r\nif (count) {\r\nreturn 0;\r\n} else if (start) {\r\nstart = 0;\r\nnode = rb_first(&delayed_refs->root);\r\ngoto again;\r\n}\r\nreturn 1;\r\n}\r\nstatic noinline void\r\nupdate_existing_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_node *existing,\r\nstruct btrfs_delayed_ref_node *update)\r\n{\r\nif (update->action != existing->action) {\r\nexisting->ref_mod--;\r\nif (existing->ref_mod == 0) {\r\nrb_erase(&existing->rb_node,\r\n&delayed_refs->root);\r\nexisting->in_tree = 0;\r\nbtrfs_put_delayed_ref(existing);\r\ndelayed_refs->num_entries--;\r\nif (trans->delayed_ref_updates)\r\ntrans->delayed_ref_updates--;\r\n} else {\r\nWARN_ON(existing->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nexisting->type == BTRFS_SHARED_BLOCK_REF_KEY);\r\n}\r\n} else {\r\nWARN_ON(existing->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nexisting->type == BTRFS_SHARED_BLOCK_REF_KEY);\r\nexisting->ref_mod += update->ref_mod;\r\n}\r\n}\r\nstatic noinline void\r\nupdate_existing_head_ref(struct btrfs_delayed_ref_node *existing,\r\nstruct btrfs_delayed_ref_node *update)\r\n{\r\nstruct btrfs_delayed_ref_head *existing_ref;\r\nstruct btrfs_delayed_ref_head *ref;\r\nexisting_ref = btrfs_delayed_node_to_head(existing);\r\nref = btrfs_delayed_node_to_head(update);\r\nBUG_ON(existing_ref->is_data != ref->is_data);\r\nif (ref->must_insert_reserved) {\r\nexisting_ref->must_insert_reserved = ref->must_insert_reserved;\r\nexisting->num_bytes = update->num_bytes;\r\n}\r\nif (ref->extent_op) {\r\nif (!existing_ref->extent_op) {\r\nexisting_ref->extent_op = ref->extent_op;\r\n} else {\r\nif (ref->extent_op->update_key) {\r\nmemcpy(&existing_ref->extent_op->key,\r\n&ref->extent_op->key,\r\nsizeof(ref->extent_op->key));\r\nexisting_ref->extent_op->update_key = 1;\r\n}\r\nif (ref->extent_op->update_flags) {\r\nexisting_ref->extent_op->flags_to_set |=\r\nref->extent_op->flags_to_set;\r\nexisting_ref->extent_op->update_flags = 1;\r\n}\r\nkfree(ref->extent_op);\r\n}\r\n}\r\nexisting->ref_mod += update->ref_mod;\r\n}\r\nstatic noinline int add_delayed_ref_head(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_node *ref,\r\nu64 bytenr, u64 num_bytes,\r\nint action, int is_data)\r\n{\r\nstruct btrfs_delayed_ref_node *existing;\r\nstruct btrfs_delayed_ref_head *head_ref = NULL;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nint count_mod = 1;\r\nint must_insert_reserved = 0;\r\nif (action == BTRFS_UPDATE_DELAYED_HEAD)\r\ncount_mod = 0;\r\nelse if (action == BTRFS_DROP_DELAYED_REF)\r\ncount_mod = -1;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\nmust_insert_reserved = 1;\r\nelse\r\nmust_insert_reserved = 0;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = count_mod;\r\nref->type = 0;\r\nref->action = 0;\r\nref->is_head = 1;\r\nref->in_tree = 1;\r\nhead_ref = btrfs_delayed_node_to_head(ref);\r\nhead_ref->must_insert_reserved = must_insert_reserved;\r\nhead_ref->is_data = is_data;\r\nINIT_LIST_HEAD(&head_ref->cluster);\r\nmutex_init(&head_ref->mutex);\r\ntrace_btrfs_delayed_ref_head(ref, head_ref, action);\r\nexisting = tree_insert(&delayed_refs->root, &ref->rb_node);\r\nif (existing) {\r\nupdate_existing_head_ref(existing, ref);\r\nkfree(ref);\r\n} else {\r\ndelayed_refs->num_heads++;\r\ndelayed_refs->num_heads_ready++;\r\ndelayed_refs->num_entries++;\r\ntrans->delayed_ref_updates++;\r\n}\r\nreturn 0;\r\n}\r\nstatic noinline int add_delayed_tree_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_node *ref,\r\nu64 bytenr, u64 num_bytes, u64 parent,\r\nu64 ref_root, int level, int action)\r\n{\r\nstruct btrfs_delayed_ref_node *existing;\r\nstruct btrfs_delayed_tree_ref *full_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\naction = BTRFS_ADD_DELAYED_REF;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = 1;\r\nref->action = action;\r\nref->is_head = 0;\r\nref->in_tree = 1;\r\nfull_ref = btrfs_delayed_node_to_tree_ref(ref);\r\nif (parent) {\r\nfull_ref->parent = parent;\r\nref->type = BTRFS_SHARED_BLOCK_REF_KEY;\r\n} else {\r\nfull_ref->root = ref_root;\r\nref->type = BTRFS_TREE_BLOCK_REF_KEY;\r\n}\r\nfull_ref->level = level;\r\ntrace_btrfs_delayed_tree_ref(ref, full_ref, action);\r\nexisting = tree_insert(&delayed_refs->root, &ref->rb_node);\r\nif (existing) {\r\nupdate_existing_ref(trans, delayed_refs, existing, ref);\r\nkfree(ref);\r\n} else {\r\ndelayed_refs->num_entries++;\r\ntrans->delayed_ref_updates++;\r\n}\r\nreturn 0;\r\n}\r\nstatic noinline int add_delayed_data_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_node *ref,\r\nu64 bytenr, u64 num_bytes, u64 parent,\r\nu64 ref_root, u64 owner, u64 offset,\r\nint action)\r\n{\r\nstruct btrfs_delayed_ref_node *existing;\r\nstruct btrfs_delayed_data_ref *full_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\naction = BTRFS_ADD_DELAYED_REF;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = 1;\r\nref->action = action;\r\nref->is_head = 0;\r\nref->in_tree = 1;\r\nfull_ref = btrfs_delayed_node_to_data_ref(ref);\r\nif (parent) {\r\nfull_ref->parent = parent;\r\nref->type = BTRFS_SHARED_DATA_REF_KEY;\r\n} else {\r\nfull_ref->root = ref_root;\r\nref->type = BTRFS_EXTENT_DATA_REF_KEY;\r\n}\r\nfull_ref->objectid = owner;\r\nfull_ref->offset = offset;\r\ntrace_btrfs_delayed_data_ref(ref, full_ref, action);\r\nexisting = tree_insert(&delayed_refs->root, &ref->rb_node);\r\nif (existing) {\r\nupdate_existing_ref(trans, delayed_refs, existing, ref);\r\nkfree(ref);\r\n} else {\r\ndelayed_refs->num_entries++;\r\ntrans->delayed_ref_updates++;\r\n}\r\nreturn 0;\r\n}\r\nint btrfs_add_delayed_tree_ref(struct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes, u64 parent,\r\nu64 ref_root, int level, int action,\r\nstruct btrfs_delayed_extent_op *extent_op)\r\n{\r\nstruct btrfs_delayed_tree_ref *ref;\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nint ret;\r\nBUG_ON(extent_op && extent_op->is_data);\r\nref = kmalloc(sizeof(*ref), GFP_NOFS);\r\nif (!ref)\r\nreturn -ENOMEM;\r\nhead_ref = kmalloc(sizeof(*head_ref), GFP_NOFS);\r\nif (!head_ref) {\r\nkfree(ref);\r\nreturn -ENOMEM;\r\n}\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nret = add_delayed_ref_head(trans, &head_ref->node, bytenr, num_bytes,\r\naction, 0);\r\nBUG_ON(ret);\r\nret = add_delayed_tree_ref(trans, &ref->node, bytenr, num_bytes,\r\nparent, ref_root, level, action);\r\nBUG_ON(ret);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\n}\r\nint btrfs_add_delayed_data_ref(struct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes,\r\nu64 parent, u64 ref_root,\r\nu64 owner, u64 offset, int action,\r\nstruct btrfs_delayed_extent_op *extent_op)\r\n{\r\nstruct btrfs_delayed_data_ref *ref;\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nint ret;\r\nBUG_ON(extent_op && !extent_op->is_data);\r\nref = kmalloc(sizeof(*ref), GFP_NOFS);\r\nif (!ref)\r\nreturn -ENOMEM;\r\nhead_ref = kmalloc(sizeof(*head_ref), GFP_NOFS);\r\nif (!head_ref) {\r\nkfree(ref);\r\nreturn -ENOMEM;\r\n}\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nret = add_delayed_ref_head(trans, &head_ref->node, bytenr, num_bytes,\r\naction, 1);\r\nBUG_ON(ret);\r\nret = add_delayed_data_ref(trans, &ref->node, bytenr, num_bytes,\r\nparent, ref_root, owner, offset, action);\r\nBUG_ON(ret);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\n}\r\nint btrfs_add_delayed_extent_op(struct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes,\r\nstruct btrfs_delayed_extent_op *extent_op)\r\n{\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nint ret;\r\nhead_ref = kmalloc(sizeof(*head_ref), GFP_NOFS);\r\nif (!head_ref)\r\nreturn -ENOMEM;\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nret = add_delayed_ref_head(trans, &head_ref->node, bytenr,\r\nnum_bytes, BTRFS_UPDATE_DELAYED_HEAD,\r\nextent_op->is_data);\r\nBUG_ON(ret);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\n}\r\nstruct btrfs_delayed_ref_head *\r\nbtrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr)\r\n{\r\nstruct btrfs_delayed_ref_node *ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nref = find_ref_head(&delayed_refs->root, bytenr, NULL);\r\nif (ref)\r\nreturn btrfs_delayed_node_to_head(ref);\r\nreturn NULL;\r\n}
