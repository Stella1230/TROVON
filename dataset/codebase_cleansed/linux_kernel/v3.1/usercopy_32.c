static inline int __movsl_is_ok(unsigned long a1, unsigned long a2, unsigned long n)\r\n{\r\n#ifdef CONFIG_X86_INTEL_USERCOPY\r\nif (n >= 64 && ((a1 ^ a2) & movsl_mask.mask))\r\nreturn 0;\r\n#endif\r\nreturn 1;\r\n}\r\nlong\r\n__strncpy_from_user(char *dst, const char __user *src, long count)\r\n{\r\nlong res;\r\n__do_strncpy_from_user(dst, src, count, res);\r\nreturn res;\r\n}\r\nlong\r\nstrncpy_from_user(char *dst, const char __user *src, long count)\r\n{\r\nlong res = -EFAULT;\r\nif (access_ok(VERIFY_READ, src, 1))\r\n__do_strncpy_from_user(dst, src, count, res);\r\nreturn res;\r\n}\r\nunsigned long\r\nclear_user(void __user *to, unsigned long n)\r\n{\r\nmight_fault();\r\nif (access_ok(VERIFY_WRITE, to, n))\r\n__do_clear_user(to, n);\r\nreturn n;\r\n}\r\nunsigned long\r\n__clear_user(void __user *to, unsigned long n)\r\n{\r\n__do_clear_user(to, n);\r\nreturn n;\r\n}\r\nlong strnlen_user(const char __user *s, long n)\r\n{\r\nunsigned long mask = -__addr_ok(s);\r\nunsigned long res, tmp;\r\nmight_fault();\r\n__asm__ __volatile__(\r\n" testl %0, %0\n"\r\n" jz 3f\n"\r\n" andl %0,%%ecx\n"\r\n"0: repne; scasb\n"\r\n" setne %%al\n"\r\n" subl %%ecx,%0\n"\r\n" addl %0,%%eax\n"\r\n"1:\n"\r\n".section .fixup,\"ax\"\n"\r\n"2: xorl %%eax,%%eax\n"\r\n" jmp 1b\n"\r\n"3: movb $1,%%al\n"\r\n" jmp 1b\n"\r\n".previous\n"\r\n".section __ex_table,\"a\"\n"\r\n" .align 4\n"\r\n" .long 0b,2b\n"\r\n".previous"\r\n:"=&r" (n), "=&D" (s), "=&a" (res), "=&c" (tmp)\r\n:"0" (n), "1" (s), "2" (0), "3" (mask)\r\n:"cc");\r\nreturn res & mask;\r\n}\r\nstatic unsigned long\r\n__copy_user_intel(void __user *to, const void *from, unsigned long size)\r\n{\r\nint d0, d1;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"1: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 3f\n"\r\n"2: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"3: movl 0(%4), %%eax\n"\r\n"4: movl 4(%4), %%edx\n"\r\n"5: movl %%eax, 0(%3)\n"\r\n"6: movl %%edx, 4(%3)\n"\r\n"7: movl 8(%4), %%eax\n"\r\n"8: movl 12(%4),%%edx\n"\r\n"9: movl %%eax, 8(%3)\n"\r\n"10: movl %%edx, 12(%3)\n"\r\n"11: movl 16(%4), %%eax\n"\r\n"12: movl 20(%4), %%edx\n"\r\n"13: movl %%eax, 16(%3)\n"\r\n"14: movl %%edx, 20(%3)\n"\r\n"15: movl 24(%4), %%eax\n"\r\n"16: movl 28(%4), %%edx\n"\r\n"17: movl %%eax, 24(%3)\n"\r\n"18: movl %%edx, 28(%3)\n"\r\n"19: movl 32(%4), %%eax\n"\r\n"20: movl 36(%4), %%edx\n"\r\n"21: movl %%eax, 32(%3)\n"\r\n"22: movl %%edx, 36(%3)\n"\r\n"23: movl 40(%4), %%eax\n"\r\n"24: movl 44(%4), %%edx\n"\r\n"25: movl %%eax, 40(%3)\n"\r\n"26: movl %%edx, 44(%3)\n"\r\n"27: movl 48(%4), %%eax\n"\r\n"28: movl 52(%4), %%edx\n"\r\n"29: movl %%eax, 48(%3)\n"\r\n"30: movl %%edx, 52(%3)\n"\r\n"31: movl 56(%4), %%eax\n"\r\n"32: movl 60(%4), %%edx\n"\r\n"33: movl %%eax, 56(%3)\n"\r\n"34: movl %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 1b\n"\r\n"35: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"99: rep; movsl\n"\r\n"36: movl %%eax, %0\n"\r\n"37: rep; movsb\n"\r\n"100:\n"\r\n".section .fixup,\"ax\"\n"\r\n"101: lea 0(%%eax,%0,4),%0\n"\r\n" jmp 100b\n"\r\n".previous\n"\r\n".section __ex_table,\"a\"\n"\r\n" .align 4\n"\r\n" .long 1b,100b\n"\r\n" .long 2b,100b\n"\r\n" .long 3b,100b\n"\r\n" .long 4b,100b\n"\r\n" .long 5b,100b\n"\r\n" .long 6b,100b\n"\r\n" .long 7b,100b\n"\r\n" .long 8b,100b\n"\r\n" .long 9b,100b\n"\r\n" .long 10b,100b\n"\r\n" .long 11b,100b\n"\r\n" .long 12b,100b\n"\r\n" .long 13b,100b\n"\r\n" .long 14b,100b\n"\r\n" .long 15b,100b\n"\r\n" .long 16b,100b\n"\r\n" .long 17b,100b\n"\r\n" .long 18b,100b\n"\r\n" .long 19b,100b\n"\r\n" .long 20b,100b\n"\r\n" .long 21b,100b\n"\r\n" .long 22b,100b\n"\r\n" .long 23b,100b\n"\r\n" .long 24b,100b\n"\r\n" .long 25b,100b\n"\r\n" .long 26b,100b\n"\r\n" .long 27b,100b\n"\r\n" .long 28b,100b\n"\r\n" .long 29b,100b\n"\r\n" .long 30b,100b\n"\r\n" .long 31b,100b\n"\r\n" .long 32b,100b\n"\r\n" .long 33b,100b\n"\r\n" .long 34b,100b\n"\r\n" .long 35b,100b\n"\r\n" .long 36b,100b\n"\r\n" .long 37b,100b\n"\r\n" .long 99b,101b\n"\r\n".previous"\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn size;\r\n}\r\nstatic unsigned long\r\n__copy_user_zeroing_intel(void *to, const void __user *from, unsigned long size)\r\n{\r\nint d0, d1;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"0: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 2f\n"\r\n"1: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"2: movl 0(%4), %%eax\n"\r\n"21: movl 4(%4), %%edx\n"\r\n" movl %%eax, 0(%3)\n"\r\n" movl %%edx, 4(%3)\n"\r\n"3: movl 8(%4), %%eax\n"\r\n"31: movl 12(%4),%%edx\n"\r\n" movl %%eax, 8(%3)\n"\r\n" movl %%edx, 12(%3)\n"\r\n"4: movl 16(%4), %%eax\n"\r\n"41: movl 20(%4), %%edx\n"\r\n" movl %%eax, 16(%3)\n"\r\n" movl %%edx, 20(%3)\n"\r\n"10: movl 24(%4), %%eax\n"\r\n"51: movl 28(%4), %%edx\n"\r\n" movl %%eax, 24(%3)\n"\r\n" movl %%edx, 28(%3)\n"\r\n"11: movl 32(%4), %%eax\n"\r\n"61: movl 36(%4), %%edx\n"\r\n" movl %%eax, 32(%3)\n"\r\n" movl %%edx, 36(%3)\n"\r\n"12: movl 40(%4), %%eax\n"\r\n"71: movl 44(%4), %%edx\n"\r\n" movl %%eax, 40(%3)\n"\r\n" movl %%edx, 44(%3)\n"\r\n"13: movl 48(%4), %%eax\n"\r\n"81: movl 52(%4), %%edx\n"\r\n" movl %%eax, 48(%3)\n"\r\n" movl %%edx, 52(%3)\n"\r\n"14: movl 56(%4), %%eax\n"\r\n"91: movl 60(%4), %%edx\n"\r\n" movl %%eax, 56(%3)\n"\r\n" movl %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 0b\n"\r\n"5: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"6: rep; movsl\n"\r\n" movl %%eax,%0\n"\r\n"7: rep; movsb\n"\r\n"8:\n"\r\n".section .fixup,\"ax\"\n"\r\n"9: lea 0(%%eax,%0,4),%0\n"\r\n"16: pushl %0\n"\r\n" pushl %%eax\n"\r\n" xorl %%eax,%%eax\n"\r\n" rep; stosb\n"\r\n" popl %%eax\n"\r\n" popl %0\n"\r\n" jmp 8b\n"\r\n".previous\n"\r\n".section __ex_table,\"a\"\n"\r\n" .align 4\n"\r\n" .long 0b,16b\n"\r\n" .long 1b,16b\n"\r\n" .long 2b,16b\n"\r\n" .long 21b,16b\n"\r\n" .long 3b,16b\n"\r\n" .long 31b,16b\n"\r\n" .long 4b,16b\n"\r\n" .long 41b,16b\n"\r\n" .long 10b,16b\n"\r\n" .long 51b,16b\n"\r\n" .long 11b,16b\n"\r\n" .long 61b,16b\n"\r\n" .long 12b,16b\n"\r\n" .long 71b,16b\n"\r\n" .long 13b,16b\n"\r\n" .long 81b,16b\n"\r\n" .long 14b,16b\n"\r\n" .long 91b,16b\n"\r\n" .long 6b,9b\n"\r\n" .long 7b,16b\n"\r\n".previous"\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn size;\r\n}\r\nstatic unsigned long __copy_user_zeroing_intel_nocache(void *to,\r\nconst void __user *from, unsigned long size)\r\n{\r\nint d0, d1;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"0: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 2f\n"\r\n"1: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"2: movl 0(%4), %%eax\n"\r\n"21: movl 4(%4), %%edx\n"\r\n" movnti %%eax, 0(%3)\n"\r\n" movnti %%edx, 4(%3)\n"\r\n"3: movl 8(%4), %%eax\n"\r\n"31: movl 12(%4),%%edx\n"\r\n" movnti %%eax, 8(%3)\n"\r\n" movnti %%edx, 12(%3)\n"\r\n"4: movl 16(%4), %%eax\n"\r\n"41: movl 20(%4), %%edx\n"\r\n" movnti %%eax, 16(%3)\n"\r\n" movnti %%edx, 20(%3)\n"\r\n"10: movl 24(%4), %%eax\n"\r\n"51: movl 28(%4), %%edx\n"\r\n" movnti %%eax, 24(%3)\n"\r\n" movnti %%edx, 28(%3)\n"\r\n"11: movl 32(%4), %%eax\n"\r\n"61: movl 36(%4), %%edx\n"\r\n" movnti %%eax, 32(%3)\n"\r\n" movnti %%edx, 36(%3)\n"\r\n"12: movl 40(%4), %%eax\n"\r\n"71: movl 44(%4), %%edx\n"\r\n" movnti %%eax, 40(%3)\n"\r\n" movnti %%edx, 44(%3)\n"\r\n"13: movl 48(%4), %%eax\n"\r\n"81: movl 52(%4), %%edx\n"\r\n" movnti %%eax, 48(%3)\n"\r\n" movnti %%edx, 52(%3)\n"\r\n"14: movl 56(%4), %%eax\n"\r\n"91: movl 60(%4), %%edx\n"\r\n" movnti %%eax, 56(%3)\n"\r\n" movnti %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 0b\n"\r\n" sfence \n"\r\n"5: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"6: rep; movsl\n"\r\n" movl %%eax,%0\n"\r\n"7: rep; movsb\n"\r\n"8:\n"\r\n".section .fixup,\"ax\"\n"\r\n"9: lea 0(%%eax,%0,4),%0\n"\r\n"16: pushl %0\n"\r\n" pushl %%eax\n"\r\n" xorl %%eax,%%eax\n"\r\n" rep; stosb\n"\r\n" popl %%eax\n"\r\n" popl %0\n"\r\n" jmp 8b\n"\r\n".previous\n"\r\n".section __ex_table,\"a\"\n"\r\n" .align 4\n"\r\n" .long 0b,16b\n"\r\n" .long 1b,16b\n"\r\n" .long 2b,16b\n"\r\n" .long 21b,16b\n"\r\n" .long 3b,16b\n"\r\n" .long 31b,16b\n"\r\n" .long 4b,16b\n"\r\n" .long 41b,16b\n"\r\n" .long 10b,16b\n"\r\n" .long 51b,16b\n"\r\n" .long 11b,16b\n"\r\n" .long 61b,16b\n"\r\n" .long 12b,16b\n"\r\n" .long 71b,16b\n"\r\n" .long 13b,16b\n"\r\n" .long 81b,16b\n"\r\n" .long 14b,16b\n"\r\n" .long 91b,16b\n"\r\n" .long 6b,9b\n"\r\n" .long 7b,16b\n"\r\n".previous"\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn size;\r\n}\r\nstatic unsigned long __copy_user_intel_nocache(void *to,\r\nconst void __user *from, unsigned long size)\r\n{\r\nint d0, d1;\r\n__asm__ __volatile__(\r\n" .align 2,0x90\n"\r\n"0: movl 32(%4), %%eax\n"\r\n" cmpl $67, %0\n"\r\n" jbe 2f\n"\r\n"1: movl 64(%4), %%eax\n"\r\n" .align 2,0x90\n"\r\n"2: movl 0(%4), %%eax\n"\r\n"21: movl 4(%4), %%edx\n"\r\n" movnti %%eax, 0(%3)\n"\r\n" movnti %%edx, 4(%3)\n"\r\n"3: movl 8(%4), %%eax\n"\r\n"31: movl 12(%4),%%edx\n"\r\n" movnti %%eax, 8(%3)\n"\r\n" movnti %%edx, 12(%3)\n"\r\n"4: movl 16(%4), %%eax\n"\r\n"41: movl 20(%4), %%edx\n"\r\n" movnti %%eax, 16(%3)\n"\r\n" movnti %%edx, 20(%3)\n"\r\n"10: movl 24(%4), %%eax\n"\r\n"51: movl 28(%4), %%edx\n"\r\n" movnti %%eax, 24(%3)\n"\r\n" movnti %%edx, 28(%3)\n"\r\n"11: movl 32(%4), %%eax\n"\r\n"61: movl 36(%4), %%edx\n"\r\n" movnti %%eax, 32(%3)\n"\r\n" movnti %%edx, 36(%3)\n"\r\n"12: movl 40(%4), %%eax\n"\r\n"71: movl 44(%4), %%edx\n"\r\n" movnti %%eax, 40(%3)\n"\r\n" movnti %%edx, 44(%3)\n"\r\n"13: movl 48(%4), %%eax\n"\r\n"81: movl 52(%4), %%edx\n"\r\n" movnti %%eax, 48(%3)\n"\r\n" movnti %%edx, 52(%3)\n"\r\n"14: movl 56(%4), %%eax\n"\r\n"91: movl 60(%4), %%edx\n"\r\n" movnti %%eax, 56(%3)\n"\r\n" movnti %%edx, 60(%3)\n"\r\n" addl $-64, %0\n"\r\n" addl $64, %4\n"\r\n" addl $64, %3\n"\r\n" cmpl $63, %0\n"\r\n" ja 0b\n"\r\n" sfence \n"\r\n"5: movl %0, %%eax\n"\r\n" shrl $2, %0\n"\r\n" andl $3, %%eax\n"\r\n" cld\n"\r\n"6: rep; movsl\n"\r\n" movl %%eax,%0\n"\r\n"7: rep; movsb\n"\r\n"8:\n"\r\n".section .fixup,\"ax\"\n"\r\n"9: lea 0(%%eax,%0,4),%0\n"\r\n"16: jmp 8b\n"\r\n".previous\n"\r\n".section __ex_table,\"a\"\n"\r\n" .align 4\n"\r\n" .long 0b,16b\n"\r\n" .long 1b,16b\n"\r\n" .long 2b,16b\n"\r\n" .long 21b,16b\n"\r\n" .long 3b,16b\n"\r\n" .long 31b,16b\n"\r\n" .long 4b,16b\n"\r\n" .long 41b,16b\n"\r\n" .long 10b,16b\n"\r\n" .long 51b,16b\n"\r\n" .long 11b,16b\n"\r\n" .long 61b,16b\n"\r\n" .long 12b,16b\n"\r\n" .long 71b,16b\n"\r\n" .long 13b,16b\n"\r\n" .long 81b,16b\n"\r\n" .long 14b,16b\n"\r\n" .long 91b,16b\n"\r\n" .long 6b,9b\n"\r\n" .long 7b,16b\n"\r\n".previous"\r\n: "=&c"(size), "=&D" (d0), "=&S" (d1)\r\n: "1"(to), "2"(from), "0"(size)\r\n: "eax", "edx", "memory");\r\nreturn size;\r\n}\r\nunsigned long __copy_to_user_ll(void __user *to, const void *from,\r\nunsigned long n)\r\n{\r\n#ifndef CONFIG_X86_WP_WORKS_OK\r\nif (unlikely(boot_cpu_data.wp_works_ok == 0) &&\r\n((unsigned long)to) < TASK_SIZE) {\r\nif (in_atomic())\r\nreturn n;\r\nwhile (n) {\r\nunsigned long offset = ((unsigned long)to)%PAGE_SIZE;\r\nunsigned long len = PAGE_SIZE - offset;\r\nint retval;\r\nstruct page *pg;\r\nvoid *maddr;\r\nif (len > n)\r\nlen = n;\r\nsurvive:\r\ndown_read(&current->mm->mmap_sem);\r\nretval = get_user_pages(current, current->mm,\r\n(unsigned long)to, 1, 1, 0, &pg, NULL);\r\nif (retval == -ENOMEM && is_global_init(current)) {\r\nup_read(&current->mm->mmap_sem);\r\ncongestion_wait(BLK_RW_ASYNC, HZ/50);\r\ngoto survive;\r\n}\r\nif (retval != 1) {\r\nup_read(&current->mm->mmap_sem);\r\nbreak;\r\n}\r\nmaddr = kmap_atomic(pg, KM_USER0);\r\nmemcpy(maddr + offset, from, len);\r\nkunmap_atomic(maddr, KM_USER0);\r\nset_page_dirty_lock(pg);\r\nput_page(pg);\r\nup_read(&current->mm->mmap_sem);\r\nfrom += len;\r\nto += len;\r\nn -= len;\r\n}\r\nreturn n;\r\n}\r\n#endif\r\nif (movsl_is_ok(to, from, n))\r\n__copy_user(to, from, n);\r\nelse\r\nn = __copy_user_intel(to, from, n);\r\nreturn n;\r\n}\r\nunsigned long __copy_from_user_ll(void *to, const void __user *from,\r\nunsigned long n)\r\n{\r\nif (movsl_is_ok(to, from, n))\r\n__copy_user_zeroing(to, from, n);\r\nelse\r\nn = __copy_user_zeroing_intel(to, from, n);\r\nreturn n;\r\n}\r\nunsigned long __copy_from_user_ll_nozero(void *to, const void __user *from,\r\nunsigned long n)\r\n{\r\nif (movsl_is_ok(to, from, n))\r\n__copy_user(to, from, n);\r\nelse\r\nn = __copy_user_intel((void __user *)to,\r\n(const void *)from, n);\r\nreturn n;\r\n}\r\nunsigned long __copy_from_user_ll_nocache(void *to, const void __user *from,\r\nunsigned long n)\r\n{\r\n#ifdef CONFIG_X86_INTEL_USERCOPY\r\nif (n > 64 && cpu_has_xmm2)\r\nn = __copy_user_zeroing_intel_nocache(to, from, n);\r\nelse\r\n__copy_user_zeroing(to, from, n);\r\n#else\r\n__copy_user_zeroing(to, from, n);\r\n#endif\r\nreturn n;\r\n}\r\nunsigned long __copy_from_user_ll_nocache_nozero(void *to, const void __user *from,\r\nunsigned long n)\r\n{\r\n#ifdef CONFIG_X86_INTEL_USERCOPY\r\nif (n > 64 && cpu_has_xmm2)\r\nn = __copy_user_intel_nocache(to, from, n);\r\nelse\r\n__copy_user(to, from, n);\r\n#else\r\n__copy_user(to, from, n);\r\n#endif\r\nreturn n;\r\n}\r\nunsigned long\r\ncopy_to_user(void __user *to, const void *from, unsigned long n)\r\n{\r\nif (access_ok(VERIFY_WRITE, to, n))\r\nn = __copy_to_user(to, from, n);\r\nreturn n;\r\n}\r\nunsigned long\r\n_copy_from_user(void *to, const void __user *from, unsigned long n)\r\n{\r\nif (access_ok(VERIFY_READ, from, n))\r\nn = __copy_from_user(to, from, n);\r\nelse\r\nmemset(to, 0, n);\r\nreturn n;\r\n}\r\nvoid copy_from_user_overflow(void)\r\n{\r\nWARN(1, "Buffer overflow detected!\n");\r\n}
