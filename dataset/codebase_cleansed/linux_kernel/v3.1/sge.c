static inline dma_addr_t get_buf_addr(const struct rx_sw_desc *d)\r\n{\r\nreturn d->dma_addr & ~(dma_addr_t)(RX_LARGE_BUF | RX_UNMAPPED_BUF);\r\n}\r\nstatic inline bool is_buf_mapped(const struct rx_sw_desc *d)\r\n{\r\nreturn !(d->dma_addr & RX_UNMAPPED_BUF);\r\n}\r\nstatic inline unsigned int txq_avail(const struct sge_txq *q)\r\n{\r\nreturn q->size - 1 - q->in_use;\r\n}\r\nstatic inline unsigned int fl_cap(const struct sge_fl *fl)\r\n{\r\nreturn fl->size - 8;\r\n}\r\nstatic inline bool fl_starving(const struct sge_fl *fl)\r\n{\r\nreturn fl->avail - fl->pend_cred <= FL_STARVE_THRES;\r\n}\r\nstatic int map_skb(struct device *dev, const struct sk_buff *skb,\r\ndma_addr_t *addr)\r\n{\r\nconst skb_frag_t *fp, *end;\r\nconst struct skb_shared_info *si;\r\n*addr = dma_map_single(dev, skb->data, skb_headlen(skb), DMA_TO_DEVICE);\r\nif (dma_mapping_error(dev, *addr))\r\ngoto out_err;\r\nsi = skb_shinfo(skb);\r\nend = &si->frags[si->nr_frags];\r\nfor (fp = si->frags; fp < end; fp++) {\r\n*++addr = dma_map_page(dev, fp->page, fp->page_offset, fp->size,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(dev, *addr))\r\ngoto unwind;\r\n}\r\nreturn 0;\r\nunwind:\r\nwhile (fp-- > si->frags)\r\ndma_unmap_page(dev, *--addr, fp->size, DMA_TO_DEVICE);\r\ndma_unmap_single(dev, addr[-1], skb_headlen(skb), DMA_TO_DEVICE);\r\nout_err:\r\nreturn -ENOMEM;\r\n}\r\nstatic void unmap_skb(struct device *dev, const struct sk_buff *skb,\r\nconst dma_addr_t *addr)\r\n{\r\nconst skb_frag_t *fp, *end;\r\nconst struct skb_shared_info *si;\r\ndma_unmap_single(dev, *addr++, skb_headlen(skb), DMA_TO_DEVICE);\r\nsi = skb_shinfo(skb);\r\nend = &si->frags[si->nr_frags];\r\nfor (fp = si->frags; fp < end; fp++)\r\ndma_unmap_page(dev, *addr++, fp->size, DMA_TO_DEVICE);\r\n}\r\nstatic void deferred_unmap_destructor(struct sk_buff *skb)\r\n{\r\nunmap_skb(skb->dev->dev.parent, skb, (dma_addr_t *)skb->head);\r\n}\r\nstatic void unmap_sgl(struct device *dev, const struct sk_buff *skb,\r\nconst struct ulptx_sgl *sgl, const struct sge_txq *q)\r\n{\r\nconst struct ulptx_sge_pair *p;\r\nunsigned int nfrags = skb_shinfo(skb)->nr_frags;\r\nif (likely(skb_headlen(skb)))\r\ndma_unmap_single(dev, be64_to_cpu(sgl->addr0), ntohl(sgl->len0),\r\nDMA_TO_DEVICE);\r\nelse {\r\ndma_unmap_page(dev, be64_to_cpu(sgl->addr0), ntohl(sgl->len0),\r\nDMA_TO_DEVICE);\r\nnfrags--;\r\n}\r\nfor (p = sgl->sge; nfrags >= 2; nfrags -= 2) {\r\nif (likely((u8 *)(p + 1) <= (u8 *)q->stat)) {\r\nunmap: dma_unmap_page(dev, be64_to_cpu(p->addr[0]),\r\nntohl(p->len[0]), DMA_TO_DEVICE);\r\ndma_unmap_page(dev, be64_to_cpu(p->addr[1]),\r\nntohl(p->len[1]), DMA_TO_DEVICE);\r\np++;\r\n} else if ((u8 *)p == (u8 *)q->stat) {\r\np = (const struct ulptx_sge_pair *)q->desc;\r\ngoto unmap;\r\n} else if ((u8 *)p + 8 == (u8 *)q->stat) {\r\nconst __be64 *addr = (const __be64 *)q->desc;\r\ndma_unmap_page(dev, be64_to_cpu(addr[0]),\r\nntohl(p->len[0]), DMA_TO_DEVICE);\r\ndma_unmap_page(dev, be64_to_cpu(addr[1]),\r\nntohl(p->len[1]), DMA_TO_DEVICE);\r\np = (const struct ulptx_sge_pair *)&addr[2];\r\n} else {\r\nconst __be64 *addr = (const __be64 *)q->desc;\r\ndma_unmap_page(dev, be64_to_cpu(p->addr[0]),\r\nntohl(p->len[0]), DMA_TO_DEVICE);\r\ndma_unmap_page(dev, be64_to_cpu(addr[0]),\r\nntohl(p->len[1]), DMA_TO_DEVICE);\r\np = (const struct ulptx_sge_pair *)&addr[1];\r\n}\r\n}\r\nif (nfrags) {\r\n__be64 addr;\r\nif ((u8 *)p == (u8 *)q->stat)\r\np = (const struct ulptx_sge_pair *)q->desc;\r\naddr = (u8 *)p + 16 <= (u8 *)q->stat ? p->addr[0] :\r\n*(const __be64 *)q->desc;\r\ndma_unmap_page(dev, be64_to_cpu(addr), ntohl(p->len[0]),\r\nDMA_TO_DEVICE);\r\n}\r\n}\r\nstatic void free_tx_desc(struct adapter *adap, struct sge_txq *q,\r\nunsigned int n, bool unmap)\r\n{\r\nstruct tx_sw_desc *d;\r\nunsigned int cidx = q->cidx;\r\nstruct device *dev = adap->pdev_dev;\r\nd = &q->sdesc[cidx];\r\nwhile (n--) {\r\nif (d->skb) {\r\nif (unmap)\r\nunmap_sgl(dev, d->skb, d->sgl, q);\r\nkfree_skb(d->skb);\r\nd->skb = NULL;\r\n}\r\n++d;\r\nif (++cidx == q->size) {\r\ncidx = 0;\r\nd = q->sdesc;\r\n}\r\n}\r\nq->cidx = cidx;\r\n}\r\nstatic inline int reclaimable(const struct sge_txq *q)\r\n{\r\nint hw_cidx = ntohs(q->stat->cidx);\r\nhw_cidx -= q->cidx;\r\nreturn hw_cidx < 0 ? hw_cidx + q->size : hw_cidx;\r\n}\r\nstatic inline void reclaim_completed_tx(struct adapter *adap, struct sge_txq *q,\r\nbool unmap)\r\n{\r\nint avail = reclaimable(q);\r\nif (avail) {\r\nif (avail > MAX_TX_RECLAIM)\r\navail = MAX_TX_RECLAIM;\r\nfree_tx_desc(adap, q, avail, unmap);\r\nq->in_use -= avail;\r\n}\r\n}\r\nstatic inline int get_buf_size(const struct rx_sw_desc *d)\r\n{\r\n#if FL_PG_ORDER > 0\r\nreturn (d->dma_addr & RX_LARGE_BUF) ? (PAGE_SIZE << FL_PG_ORDER) :\r\nPAGE_SIZE;\r\n#else\r\nreturn PAGE_SIZE;\r\n#endif\r\n}\r\nstatic void free_rx_bufs(struct adapter *adap, struct sge_fl *q, int n)\r\n{\r\nwhile (n--) {\r\nstruct rx_sw_desc *d = &q->sdesc[q->cidx];\r\nif (is_buf_mapped(d))\r\ndma_unmap_page(adap->pdev_dev, get_buf_addr(d),\r\nget_buf_size(d), PCI_DMA_FROMDEVICE);\r\nput_page(d->page);\r\nd->page = NULL;\r\nif (++q->cidx == q->size)\r\nq->cidx = 0;\r\nq->avail--;\r\n}\r\n}\r\nstatic void unmap_rx_buf(struct adapter *adap, struct sge_fl *q)\r\n{\r\nstruct rx_sw_desc *d = &q->sdesc[q->cidx];\r\nif (is_buf_mapped(d))\r\ndma_unmap_page(adap->pdev_dev, get_buf_addr(d),\r\nget_buf_size(d), PCI_DMA_FROMDEVICE);\r\nd->page = NULL;\r\nif (++q->cidx == q->size)\r\nq->cidx = 0;\r\nq->avail--;\r\n}\r\nstatic inline void ring_fl_db(struct adapter *adap, struct sge_fl *q)\r\n{\r\nif (q->pend_cred >= 8) {\r\nwmb();\r\nt4_write_reg(adap, MYPF_REG(SGE_PF_KDOORBELL), DBPRIO |\r\nQID(q->cntxt_id) | PIDX(q->pend_cred / 8));\r\nq->pend_cred &= 7;\r\n}\r\n}\r\nstatic inline void set_rx_sw_desc(struct rx_sw_desc *sd, struct page *pg,\r\ndma_addr_t mapping)\r\n{\r\nsd->page = pg;\r\nsd->dma_addr = mapping;\r\n}\r\nstatic unsigned int refill_fl(struct adapter *adap, struct sge_fl *q, int n,\r\ngfp_t gfp)\r\n{\r\nstruct page *pg;\r\ndma_addr_t mapping;\r\nunsigned int cred = q->avail;\r\n__be64 *d = &q->desc[q->pidx];\r\nstruct rx_sw_desc *sd = &q->sdesc[q->pidx];\r\ngfp |= __GFP_NOWARN;\r\n#if FL_PG_ORDER > 0\r\nwhile (n) {\r\npg = alloc_pages(gfp | __GFP_COMP, FL_PG_ORDER);\r\nif (unlikely(!pg)) {\r\nq->large_alloc_failed++;\r\nbreak;\r\n}\r\nmapping = dma_map_page(adap->pdev_dev, pg, 0,\r\nPAGE_SIZE << FL_PG_ORDER,\r\nPCI_DMA_FROMDEVICE);\r\nif (unlikely(dma_mapping_error(adap->pdev_dev, mapping))) {\r\n__free_pages(pg, FL_PG_ORDER);\r\ngoto out;\r\n}\r\nmapping |= RX_LARGE_BUF;\r\n*d++ = cpu_to_be64(mapping);\r\nset_rx_sw_desc(sd, pg, mapping);\r\nsd++;\r\nq->avail++;\r\nif (++q->pidx == q->size) {\r\nq->pidx = 0;\r\nsd = q->sdesc;\r\nd = q->desc;\r\n}\r\nn--;\r\n}\r\n#endif\r\nwhile (n--) {\r\npg = __netdev_alloc_page(adap->port[0], gfp);\r\nif (unlikely(!pg)) {\r\nq->alloc_failed++;\r\nbreak;\r\n}\r\nmapping = dma_map_page(adap->pdev_dev, pg, 0, PAGE_SIZE,\r\nPCI_DMA_FROMDEVICE);\r\nif (unlikely(dma_mapping_error(adap->pdev_dev, mapping))) {\r\nnetdev_free_page(adap->port[0], pg);\r\ngoto out;\r\n}\r\n*d++ = cpu_to_be64(mapping);\r\nset_rx_sw_desc(sd, pg, mapping);\r\nsd++;\r\nq->avail++;\r\nif (++q->pidx == q->size) {\r\nq->pidx = 0;\r\nsd = q->sdesc;\r\nd = q->desc;\r\n}\r\n}\r\nout: cred = q->avail - cred;\r\nq->pend_cred += cred;\r\nring_fl_db(adap, q);\r\nif (unlikely(fl_starving(q))) {\r\nsmp_wmb();\r\nset_bit(q->cntxt_id - adap->sge.egr_start,\r\nadap->sge.starving_fl);\r\n}\r\nreturn cred;\r\n}\r\nstatic inline void __refill_fl(struct adapter *adap, struct sge_fl *fl)\r\n{\r\nrefill_fl(adap, fl, min(MAX_RX_REFILL, fl_cap(fl) - fl->avail),\r\nGFP_ATOMIC);\r\n}\r\nstatic void *alloc_ring(struct device *dev, size_t nelem, size_t elem_size,\r\nsize_t sw_size, dma_addr_t *phys, void *metadata,\r\nsize_t stat_size, int node)\r\n{\r\nsize_t len = nelem * elem_size + stat_size;\r\nvoid *s = NULL;\r\nvoid *p = dma_alloc_coherent(dev, len, phys, GFP_KERNEL);\r\nif (!p)\r\nreturn NULL;\r\nif (sw_size) {\r\ns = kzalloc_node(nelem * sw_size, GFP_KERNEL, node);\r\nif (!s) {\r\ndma_free_coherent(dev, len, p, *phys);\r\nreturn NULL;\r\n}\r\n}\r\nif (metadata)\r\n*(void **)metadata = s;\r\nmemset(p, 0, len);\r\nreturn p;\r\n}\r\nstatic inline unsigned int sgl_len(unsigned int n)\r\n{\r\nn--;\r\nreturn (3 * n) / 2 + (n & 1) + 2;\r\n}\r\nstatic inline unsigned int flits_to_desc(unsigned int n)\r\n{\r\nBUG_ON(n > SGE_MAX_WR_LEN / 8);\r\nreturn DIV_ROUND_UP(n, 8);\r\n}\r\nstatic inline int is_eth_imm(const struct sk_buff *skb)\r\n{\r\nreturn skb->len <= MAX_IMM_TX_PKT_LEN - sizeof(struct cpl_tx_pkt);\r\n}\r\nstatic inline unsigned int calc_tx_flits(const struct sk_buff *skb)\r\n{\r\nunsigned int flits;\r\nif (is_eth_imm(skb))\r\nreturn DIV_ROUND_UP(skb->len + sizeof(struct cpl_tx_pkt), 8);\r\nflits = sgl_len(skb_shinfo(skb)->nr_frags + 1) + 4;\r\nif (skb_shinfo(skb)->gso_size)\r\nflits += 2;\r\nreturn flits;\r\n}\r\nstatic inline unsigned int calc_tx_descs(const struct sk_buff *skb)\r\n{\r\nreturn flits_to_desc(calc_tx_flits(skb));\r\n}\r\nstatic void write_sgl(const struct sk_buff *skb, struct sge_txq *q,\r\nstruct ulptx_sgl *sgl, u64 *end, unsigned int start,\r\nconst dma_addr_t *addr)\r\n{\r\nunsigned int i, len;\r\nstruct ulptx_sge_pair *to;\r\nconst struct skb_shared_info *si = skb_shinfo(skb);\r\nunsigned int nfrags = si->nr_frags;\r\nstruct ulptx_sge_pair buf[MAX_SKB_FRAGS / 2 + 1];\r\nlen = skb_headlen(skb) - start;\r\nif (likely(len)) {\r\nsgl->len0 = htonl(len);\r\nsgl->addr0 = cpu_to_be64(addr[0] + start);\r\nnfrags++;\r\n} else {\r\nsgl->len0 = htonl(si->frags[0].size);\r\nsgl->addr0 = cpu_to_be64(addr[1]);\r\n}\r\nsgl->cmd_nsge = htonl(ULPTX_CMD(ULP_TX_SC_DSGL) | ULPTX_NSGE(nfrags));\r\nif (likely(--nfrags == 0))\r\nreturn;\r\nto = (u8 *)end > (u8 *)q->stat ? buf : sgl->sge;\r\nfor (i = (nfrags != si->nr_frags); nfrags >= 2; nfrags -= 2, to++) {\r\nto->len[0] = cpu_to_be32(si->frags[i].size);\r\nto->len[1] = cpu_to_be32(si->frags[++i].size);\r\nto->addr[0] = cpu_to_be64(addr[i]);\r\nto->addr[1] = cpu_to_be64(addr[++i]);\r\n}\r\nif (nfrags) {\r\nto->len[0] = cpu_to_be32(si->frags[i].size);\r\nto->len[1] = cpu_to_be32(0);\r\nto->addr[0] = cpu_to_be64(addr[i + 1]);\r\n}\r\nif (unlikely((u8 *)end > (u8 *)q->stat)) {\r\nunsigned int part0 = (u8 *)q->stat - (u8 *)sgl->sge, part1;\r\nif (likely(part0))\r\nmemcpy(sgl->sge, buf, part0);\r\npart1 = (u8 *)end - (u8 *)q->stat;\r\nmemcpy(q->desc, (u8 *)buf + part0, part1);\r\nend = (void *)q->desc + part1;\r\n}\r\nif ((uintptr_t)end & 8)\r\n*(u64 *)end = 0;\r\n}\r\nstatic inline void ring_tx_db(struct adapter *adap, struct sge_txq *q, int n)\r\n{\r\nwmb();\r\nt4_write_reg(adap, MYPF_REG(SGE_PF_KDOORBELL),\r\nQID(q->cntxt_id) | PIDX(n));\r\n}\r\nstatic void inline_tx_skb(const struct sk_buff *skb, const struct sge_txq *q,\r\nvoid *pos)\r\n{\r\nu64 *p;\r\nint left = (void *)q->stat - pos;\r\nif (likely(skb->len <= left)) {\r\nif (likely(!skb->data_len))\r\nskb_copy_from_linear_data(skb, pos, skb->len);\r\nelse\r\nskb_copy_bits(skb, 0, pos, skb->len);\r\npos += skb->len;\r\n} else {\r\nskb_copy_bits(skb, 0, pos, left);\r\nskb_copy_bits(skb, left, q->desc, skb->len - left);\r\npos = (void *)q->desc + (skb->len - left);\r\n}\r\np = PTR_ALIGN(pos, 8);\r\nif ((uintptr_t)p & 8)\r\n*p = 0;\r\n}\r\nstatic u64 hwcsum(const struct sk_buff *skb)\r\n{\r\nint csum_type;\r\nconst struct iphdr *iph = ip_hdr(skb);\r\nif (iph->version == 4) {\r\nif (iph->protocol == IPPROTO_TCP)\r\ncsum_type = TX_CSUM_TCPIP;\r\nelse if (iph->protocol == IPPROTO_UDP)\r\ncsum_type = TX_CSUM_UDPIP;\r\nelse {\r\nnocsum:\r\nreturn TXPKT_L4CSUM_DIS;\r\n}\r\n} else {\r\nconst struct ipv6hdr *ip6h = (const struct ipv6hdr *)iph;\r\nif (ip6h->nexthdr == IPPROTO_TCP)\r\ncsum_type = TX_CSUM_TCPIP6;\r\nelse if (ip6h->nexthdr == IPPROTO_UDP)\r\ncsum_type = TX_CSUM_UDPIP6;\r\nelse\r\ngoto nocsum;\r\n}\r\nif (likely(csum_type >= TX_CSUM_TCPIP))\r\nreturn TXPKT_CSUM_TYPE(csum_type) |\r\nTXPKT_IPHDR_LEN(skb_network_header_len(skb)) |\r\nTXPKT_ETHHDR_LEN(skb_network_offset(skb) - ETH_HLEN);\r\nelse {\r\nint start = skb_transport_offset(skb);\r\nreturn TXPKT_CSUM_TYPE(csum_type) | TXPKT_CSUM_START(start) |\r\nTXPKT_CSUM_LOC(start + skb->csum_offset);\r\n}\r\n}\r\nstatic void eth_txq_stop(struct sge_eth_txq *q)\r\n{\r\nnetif_tx_stop_queue(q->txq);\r\nq->q.stops++;\r\n}\r\nstatic inline void txq_advance(struct sge_txq *q, unsigned int n)\r\n{\r\nq->in_use += n;\r\nq->pidx += n;\r\nif (q->pidx >= q->size)\r\nq->pidx -= q->size;\r\n}\r\nnetdev_tx_t t4_eth_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nu32 wr_mid;\r\nu64 cntrl, *end;\r\nint qidx, credits;\r\nunsigned int flits, ndesc;\r\nstruct adapter *adap;\r\nstruct sge_eth_txq *q;\r\nconst struct port_info *pi;\r\nstruct fw_eth_tx_pkt_wr *wr;\r\nstruct cpl_tx_pkt_core *cpl;\r\nconst struct skb_shared_info *ssi;\r\ndma_addr_t addr[MAX_SKB_FRAGS + 1];\r\nif (unlikely(skb->len < ETH_HLEN)) {\r\nout_free: dev_kfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\npi = netdev_priv(dev);\r\nadap = pi->adapter;\r\nqidx = skb_get_queue_mapping(skb);\r\nq = &adap->sge.ethtxq[qidx + pi->first_qset];\r\nreclaim_completed_tx(adap, &q->q, true);\r\nflits = calc_tx_flits(skb);\r\nndesc = flits_to_desc(flits);\r\ncredits = txq_avail(&q->q) - ndesc;\r\nif (unlikely(credits < 0)) {\r\neth_txq_stop(q);\r\ndev_err(adap->pdev_dev,\r\n"%s: Tx ring %u full while queue awake!\n",\r\ndev->name, qidx);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nif (!is_eth_imm(skb) &&\r\nunlikely(map_skb(adap->pdev_dev, skb, addr) < 0)) {\r\nq->mapping_err++;\r\ngoto out_free;\r\n}\r\nwr_mid = FW_WR_LEN16(DIV_ROUND_UP(flits, 2));\r\nif (unlikely(credits < ETHTXQ_STOP_THRES)) {\r\neth_txq_stop(q);\r\nwr_mid |= FW_WR_EQUEQ | FW_WR_EQUIQ;\r\n}\r\nwr = (void *)&q->q.desc[q->q.pidx];\r\nwr->equiq_to_len16 = htonl(wr_mid);\r\nwr->r3 = cpu_to_be64(0);\r\nend = (u64 *)wr + flits;\r\nssi = skb_shinfo(skb);\r\nif (ssi->gso_size) {\r\nstruct cpl_tx_pkt_lso *lso = (void *)wr;\r\nbool v6 = (ssi->gso_type & SKB_GSO_TCPV6) != 0;\r\nint l3hdr_len = skb_network_header_len(skb);\r\nint eth_xtra_len = skb_network_offset(skb) - ETH_HLEN;\r\nwr->op_immdlen = htonl(FW_WR_OP(FW_ETH_TX_PKT_WR) |\r\nFW_WR_IMMDLEN(sizeof(*lso)));\r\nlso->c.lso_ctrl = htonl(LSO_OPCODE(CPL_TX_PKT_LSO) |\r\nLSO_FIRST_SLICE | LSO_LAST_SLICE |\r\nLSO_IPV6(v6) |\r\nLSO_ETHHDR_LEN(eth_xtra_len / 4) |\r\nLSO_IPHDR_LEN(l3hdr_len / 4) |\r\nLSO_TCPHDR_LEN(tcp_hdr(skb)->doff));\r\nlso->c.ipid_ofst = htons(0);\r\nlso->c.mss = htons(ssi->gso_size);\r\nlso->c.seqno_offset = htonl(0);\r\nlso->c.len = htonl(skb->len);\r\ncpl = (void *)(lso + 1);\r\ncntrl = TXPKT_CSUM_TYPE(v6 ? TX_CSUM_TCPIP6 : TX_CSUM_TCPIP) |\r\nTXPKT_IPHDR_LEN(l3hdr_len) |\r\nTXPKT_ETHHDR_LEN(eth_xtra_len);\r\nq->tso++;\r\nq->tx_cso += ssi->gso_segs;\r\n} else {\r\nint len;\r\nlen = is_eth_imm(skb) ? skb->len + sizeof(*cpl) : sizeof(*cpl);\r\nwr->op_immdlen = htonl(FW_WR_OP(FW_ETH_TX_PKT_WR) |\r\nFW_WR_IMMDLEN(len));\r\ncpl = (void *)(wr + 1);\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\ncntrl = hwcsum(skb) | TXPKT_IPCSUM_DIS;\r\nq->tx_cso++;\r\n} else\r\ncntrl = TXPKT_L4CSUM_DIS | TXPKT_IPCSUM_DIS;\r\n}\r\nif (vlan_tx_tag_present(skb)) {\r\nq->vlan_ins++;\r\ncntrl |= TXPKT_VLAN_VLD | TXPKT_VLAN(vlan_tx_tag_get(skb));\r\n}\r\ncpl->ctrl0 = htonl(TXPKT_OPCODE(CPL_TX_PKT_XT) |\r\nTXPKT_INTF(pi->tx_chan) | TXPKT_PF(adap->fn));\r\ncpl->pack = htons(0);\r\ncpl->len = htons(skb->len);\r\ncpl->ctrl1 = cpu_to_be64(cntrl);\r\nif (is_eth_imm(skb)) {\r\ninline_tx_skb(skb, &q->q, cpl + 1);\r\ndev_kfree_skb(skb);\r\n} else {\r\nint last_desc;\r\nwrite_sgl(skb, &q->q, (struct ulptx_sgl *)(cpl + 1), end, 0,\r\naddr);\r\nskb_orphan(skb);\r\nlast_desc = q->q.pidx + ndesc - 1;\r\nif (last_desc >= q->q.size)\r\nlast_desc -= q->q.size;\r\nq->q.sdesc[last_desc].skb = skb;\r\nq->q.sdesc[last_desc].sgl = (struct ulptx_sgl *)(cpl + 1);\r\n}\r\ntxq_advance(&q->q, ndesc);\r\nring_tx_db(adap, &q->q, ndesc);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic inline void reclaim_completed_tx_imm(struct sge_txq *q)\r\n{\r\nint hw_cidx = ntohs(q->stat->cidx);\r\nint reclaim = hw_cidx - q->cidx;\r\nif (reclaim < 0)\r\nreclaim += q->size;\r\nq->in_use -= reclaim;\r\nq->cidx = hw_cidx;\r\n}\r\nstatic inline int is_imm(const struct sk_buff *skb)\r\n{\r\nreturn skb->len <= MAX_CTRL_WR_LEN;\r\n}\r\nstatic void ctrlq_check_stop(struct sge_ctrl_txq *q, struct fw_wr_hdr *wr)\r\n{\r\nreclaim_completed_tx_imm(&q->q);\r\nif (unlikely(txq_avail(&q->q) < TXQ_STOP_THRES)) {\r\nwr->lo |= htonl(FW_WR_EQUEQ | FW_WR_EQUIQ);\r\nq->q.stops++;\r\nq->full = 1;\r\n}\r\n}\r\nstatic int ctrl_xmit(struct sge_ctrl_txq *q, struct sk_buff *skb)\r\n{\r\nunsigned int ndesc;\r\nstruct fw_wr_hdr *wr;\r\nif (unlikely(!is_imm(skb))) {\r\nWARN_ON(1);\r\ndev_kfree_skb(skb);\r\nreturn NET_XMIT_DROP;\r\n}\r\nndesc = DIV_ROUND_UP(skb->len, sizeof(struct tx_desc));\r\nspin_lock(&q->sendq.lock);\r\nif (unlikely(q->full)) {\r\nskb->priority = ndesc;\r\n__skb_queue_tail(&q->sendq, skb);\r\nspin_unlock(&q->sendq.lock);\r\nreturn NET_XMIT_CN;\r\n}\r\nwr = (struct fw_wr_hdr *)&q->q.desc[q->q.pidx];\r\ninline_tx_skb(skb, &q->q, wr);\r\ntxq_advance(&q->q, ndesc);\r\nif (unlikely(txq_avail(&q->q) < TXQ_STOP_THRES))\r\nctrlq_check_stop(q, wr);\r\nring_tx_db(q->adap, &q->q, ndesc);\r\nspin_unlock(&q->sendq.lock);\r\nkfree_skb(skb);\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic void restart_ctrlq(unsigned long data)\r\n{\r\nstruct sk_buff *skb;\r\nunsigned int written = 0;\r\nstruct sge_ctrl_txq *q = (struct sge_ctrl_txq *)data;\r\nspin_lock(&q->sendq.lock);\r\nreclaim_completed_tx_imm(&q->q);\r\nBUG_ON(txq_avail(&q->q) < TXQ_STOP_THRES);\r\nwhile ((skb = __skb_dequeue(&q->sendq)) != NULL) {\r\nstruct fw_wr_hdr *wr;\r\nunsigned int ndesc = skb->priority;\r\nspin_unlock(&q->sendq.lock);\r\nwr = (struct fw_wr_hdr *)&q->q.desc[q->q.pidx];\r\ninline_tx_skb(skb, &q->q, wr);\r\nkfree_skb(skb);\r\nwritten += ndesc;\r\ntxq_advance(&q->q, ndesc);\r\nif (unlikely(txq_avail(&q->q) < TXQ_STOP_THRES)) {\r\nunsigned long old = q->q.stops;\r\nctrlq_check_stop(q, wr);\r\nif (q->q.stops != old) {\r\nspin_lock(&q->sendq.lock);\r\ngoto ringdb;\r\n}\r\n}\r\nif (written > 16) {\r\nring_tx_db(q->adap, &q->q, written);\r\nwritten = 0;\r\n}\r\nspin_lock(&q->sendq.lock);\r\n}\r\nq->full = 0;\r\nringdb: if (written)\r\nring_tx_db(q->adap, &q->q, written);\r\nspin_unlock(&q->sendq.lock);\r\n}\r\nint t4_mgmt_tx(struct adapter *adap, struct sk_buff *skb)\r\n{\r\nint ret;\r\nlocal_bh_disable();\r\nret = ctrl_xmit(&adap->sge.ctrlq[0], skb);\r\nlocal_bh_enable();\r\nreturn ret;\r\n}\r\nstatic inline int is_ofld_imm(const struct sk_buff *skb)\r\n{\r\nreturn skb->len <= MAX_IMM_TX_PKT_LEN;\r\n}\r\nstatic inline unsigned int calc_tx_flits_ofld(const struct sk_buff *skb)\r\n{\r\nunsigned int flits, cnt;\r\nif (is_ofld_imm(skb))\r\nreturn DIV_ROUND_UP(skb->len, 8);\r\nflits = skb_transport_offset(skb) / 8U;\r\ncnt = skb_shinfo(skb)->nr_frags;\r\nif (skb->tail != skb->transport_header)\r\ncnt++;\r\nreturn flits + sgl_len(cnt);\r\n}\r\nstatic void txq_stop_maperr(struct sge_ofld_txq *q)\r\n{\r\nq->mapping_err++;\r\nq->q.stops++;\r\nset_bit(q->q.cntxt_id - q->adap->sge.egr_start,\r\nq->adap->sge.txq_maperr);\r\n}\r\nstatic void ofldtxq_stop(struct sge_ofld_txq *q, struct sk_buff *skb)\r\n{\r\nstruct fw_wr_hdr *wr = (struct fw_wr_hdr *)skb->data;\r\nwr->lo |= htonl(FW_WR_EQUEQ | FW_WR_EQUIQ);\r\nq->q.stops++;\r\nq->full = 1;\r\n}\r\nstatic void service_ofldq(struct sge_ofld_txq *q)\r\n{\r\nu64 *pos;\r\nint credits;\r\nstruct sk_buff *skb;\r\nunsigned int written = 0;\r\nunsigned int flits, ndesc;\r\nwhile ((skb = skb_peek(&q->sendq)) != NULL && !q->full) {\r\nspin_unlock(&q->sendq.lock);\r\nreclaim_completed_tx(q->adap, &q->q, false);\r\nflits = skb->priority;\r\nndesc = flits_to_desc(flits);\r\ncredits = txq_avail(&q->q) - ndesc;\r\nBUG_ON(credits < 0);\r\nif (unlikely(credits < TXQ_STOP_THRES))\r\nofldtxq_stop(q, skb);\r\npos = (u64 *)&q->q.desc[q->q.pidx];\r\nif (is_ofld_imm(skb))\r\ninline_tx_skb(skb, &q->q, pos);\r\nelse if (map_skb(q->adap->pdev_dev, skb,\r\n(dma_addr_t *)skb->head)) {\r\ntxq_stop_maperr(q);\r\nspin_lock(&q->sendq.lock);\r\nbreak;\r\n} else {\r\nint last_desc, hdr_len = skb_transport_offset(skb);\r\nmemcpy(pos, skb->data, hdr_len);\r\nwrite_sgl(skb, &q->q, (void *)pos + hdr_len,\r\npos + flits, hdr_len,\r\n(dma_addr_t *)skb->head);\r\n#ifdef CONFIG_NEED_DMA_MAP_STATE\r\nskb->dev = q->adap->port[0];\r\nskb->destructor = deferred_unmap_destructor;\r\n#endif\r\nlast_desc = q->q.pidx + ndesc - 1;\r\nif (last_desc >= q->q.size)\r\nlast_desc -= q->q.size;\r\nq->q.sdesc[last_desc].skb = skb;\r\n}\r\ntxq_advance(&q->q, ndesc);\r\nwritten += ndesc;\r\nif (unlikely(written > 32)) {\r\nring_tx_db(q->adap, &q->q, written);\r\nwritten = 0;\r\n}\r\nspin_lock(&q->sendq.lock);\r\n__skb_unlink(skb, &q->sendq);\r\nif (is_ofld_imm(skb))\r\nkfree_skb(skb);\r\n}\r\nif (likely(written))\r\nring_tx_db(q->adap, &q->q, written);\r\n}\r\nstatic int ofld_xmit(struct sge_ofld_txq *q, struct sk_buff *skb)\r\n{\r\nskb->priority = calc_tx_flits_ofld(skb);\r\nspin_lock(&q->sendq.lock);\r\n__skb_queue_tail(&q->sendq, skb);\r\nif (q->sendq.qlen == 1)\r\nservice_ofldq(q);\r\nspin_unlock(&q->sendq.lock);\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic void restart_ofldq(unsigned long data)\r\n{\r\nstruct sge_ofld_txq *q = (struct sge_ofld_txq *)data;\r\nspin_lock(&q->sendq.lock);\r\nq->full = 0;\r\nservice_ofldq(q);\r\nspin_unlock(&q->sendq.lock);\r\n}\r\nstatic inline unsigned int skb_txq(const struct sk_buff *skb)\r\n{\r\nreturn skb->queue_mapping >> 1;\r\n}\r\nstatic inline unsigned int is_ctrl_pkt(const struct sk_buff *skb)\r\n{\r\nreturn skb->queue_mapping & 1;\r\n}\r\nstatic inline int ofld_send(struct adapter *adap, struct sk_buff *skb)\r\n{\r\nunsigned int idx = skb_txq(skb);\r\nif (unlikely(is_ctrl_pkt(skb)))\r\nreturn ctrl_xmit(&adap->sge.ctrlq[idx], skb);\r\nreturn ofld_xmit(&adap->sge.ofldtxq[idx], skb);\r\n}\r\nint t4_ofld_send(struct adapter *adap, struct sk_buff *skb)\r\n{\r\nint ret;\r\nlocal_bh_disable();\r\nret = ofld_send(adap, skb);\r\nlocal_bh_enable();\r\nreturn ret;\r\n}\r\nint cxgb4_ofld_send(struct net_device *dev, struct sk_buff *skb)\r\n{\r\nreturn t4_ofld_send(netdev2adap(dev), skb);\r\n}\r\nstatic inline void copy_frags(struct skb_shared_info *ssi,\r\nconst struct pkt_gl *gl, unsigned int offset)\r\n{\r\nunsigned int n;\r\nssi->frags[0].page = gl->frags[0].page;\r\nssi->frags[0].page_offset = gl->frags[0].page_offset + offset;\r\nssi->frags[0].size = gl->frags[0].size - offset;\r\nssi->nr_frags = gl->nfrags;\r\nn = gl->nfrags - 1;\r\nif (n)\r\nmemcpy(&ssi->frags[1], &gl->frags[1], n * sizeof(skb_frag_t));\r\nget_page(gl->frags[n].page);\r\n}\r\nstruct sk_buff *cxgb4_pktgl_to_skb(const struct pkt_gl *gl,\r\nunsigned int skb_len, unsigned int pull_len)\r\n{\r\nstruct sk_buff *skb;\r\nif (gl->tot_len <= RX_COPY_THRES) {\r\nskb = dev_alloc_skb(gl->tot_len);\r\nif (unlikely(!skb))\r\ngoto out;\r\n__skb_put(skb, gl->tot_len);\r\nskb_copy_to_linear_data(skb, gl->va, gl->tot_len);\r\n} else {\r\nskb = dev_alloc_skb(skb_len);\r\nif (unlikely(!skb))\r\ngoto out;\r\n__skb_put(skb, pull_len);\r\nskb_copy_to_linear_data(skb, gl->va, pull_len);\r\ncopy_frags(skb_shinfo(skb), gl, pull_len);\r\nskb->len = gl->tot_len;\r\nskb->data_len = skb->len - pull_len;\r\nskb->truesize += skb->data_len;\r\n}\r\nout: return skb;\r\n}\r\nstatic void t4_pktgl_free(const struct pkt_gl *gl)\r\n{\r\nint n;\r\nconst skb_frag_t *p;\r\nfor (p = gl->frags, n = gl->nfrags - 1; n--; p++)\r\nput_page(p->page);\r\n}\r\nstatic noinline int handle_trace_pkt(struct adapter *adap,\r\nconst struct pkt_gl *gl)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cpl_trace_pkt *p;\r\nskb = cxgb4_pktgl_to_skb(gl, RX_PULL_LEN, RX_PULL_LEN);\r\nif (unlikely(!skb)) {\r\nt4_pktgl_free(gl);\r\nreturn 0;\r\n}\r\np = (struct cpl_trace_pkt *)skb->data;\r\n__skb_pull(skb, sizeof(*p));\r\nskb_reset_mac_header(skb);\r\nskb->protocol = htons(0xffff);\r\nskb->dev = adap->port[0];\r\nnetif_receive_skb(skb);\r\nreturn 0;\r\n}\r\nstatic void do_gro(struct sge_eth_rxq *rxq, const struct pkt_gl *gl,\r\nconst struct cpl_rx_pkt *pkt)\r\n{\r\nint ret;\r\nstruct sk_buff *skb;\r\nskb = napi_get_frags(&rxq->rspq.napi);\r\nif (unlikely(!skb)) {\r\nt4_pktgl_free(gl);\r\nrxq->stats.rx_drops++;\r\nreturn;\r\n}\r\ncopy_frags(skb_shinfo(skb), gl, RX_PKT_PAD);\r\nskb->len = gl->tot_len - RX_PKT_PAD;\r\nskb->data_len = skb->len;\r\nskb->truesize += skb->data_len;\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nskb_record_rx_queue(skb, rxq->rspq.idx);\r\nif (rxq->rspq.netdev->features & NETIF_F_RXHASH)\r\nskb->rxhash = (__force u32)pkt->rsshdr.hash_val;\r\nif (unlikely(pkt->vlan_ex)) {\r\n__vlan_hwaccel_put_tag(skb, ntohs(pkt->vlan));\r\nrxq->stats.vlan_ex++;\r\n}\r\nret = napi_gro_frags(&rxq->rspq.napi);\r\nif (ret == GRO_HELD)\r\nrxq->stats.lro_pkts++;\r\nelse if (ret == GRO_MERGED || ret == GRO_MERGED_FREE)\r\nrxq->stats.lro_merged++;\r\nrxq->stats.pkts++;\r\nrxq->stats.rx_cso++;\r\n}\r\nint t4_ethrx_handler(struct sge_rspq *q, const __be64 *rsp,\r\nconst struct pkt_gl *si)\r\n{\r\nbool csum_ok;\r\nstruct sk_buff *skb;\r\nconst struct cpl_rx_pkt *pkt;\r\nstruct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, rspq);\r\nif (unlikely(*(u8 *)rsp == CPL_TRACE_PKT))\r\nreturn handle_trace_pkt(q->adap, si);\r\npkt = (const struct cpl_rx_pkt *)rsp;\r\ncsum_ok = pkt->csum_calc && !pkt->err_vec;\r\nif ((pkt->l2info & htonl(RXF_TCP)) &&\r\n(q->netdev->features & NETIF_F_GRO) && csum_ok && !pkt->ip_frag) {\r\ndo_gro(rxq, si, pkt);\r\nreturn 0;\r\n}\r\nskb = cxgb4_pktgl_to_skb(si, RX_PKT_SKB_LEN, RX_PULL_LEN);\r\nif (unlikely(!skb)) {\r\nt4_pktgl_free(si);\r\nrxq->stats.rx_drops++;\r\nreturn 0;\r\n}\r\n__skb_pull(skb, RX_PKT_PAD);\r\nskb->protocol = eth_type_trans(skb, q->netdev);\r\nskb_record_rx_queue(skb, q->idx);\r\nif (skb->dev->features & NETIF_F_RXHASH)\r\nskb->rxhash = (__force u32)pkt->rsshdr.hash_val;\r\nrxq->stats.pkts++;\r\nif (csum_ok && (q->netdev->features & NETIF_F_RXCSUM) &&\r\n(pkt->l2info & htonl(RXF_UDP | RXF_TCP))) {\r\nif (!pkt->ip_frag) {\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nrxq->stats.rx_cso++;\r\n} else if (pkt->l2info & htonl(RXF_IP)) {\r\n__sum16 c = (__force __sum16)pkt->csum;\r\nskb->csum = csum_unfold(c);\r\nskb->ip_summed = CHECKSUM_COMPLETE;\r\nrxq->stats.rx_cso++;\r\n}\r\n} else\r\nskb_checksum_none_assert(skb);\r\nif (unlikely(pkt->vlan_ex)) {\r\n__vlan_hwaccel_put_tag(skb, ntohs(pkt->vlan));\r\nrxq->stats.vlan_ex++;\r\n}\r\nnetif_receive_skb(skb);\r\nreturn 0;\r\n}\r\nstatic void restore_rx_bufs(const struct pkt_gl *si, struct sge_fl *q,\r\nint frags)\r\n{\r\nstruct rx_sw_desc *d;\r\nwhile (frags--) {\r\nif (q->cidx == 0)\r\nq->cidx = q->size - 1;\r\nelse\r\nq->cidx--;\r\nd = &q->sdesc[q->cidx];\r\nd->page = si->frags[frags].page;\r\nd->dma_addr |= RX_UNMAPPED_BUF;\r\nq->avail++;\r\n}\r\n}\r\nstatic inline bool is_new_response(const struct rsp_ctrl *r,\r\nconst struct sge_rspq *q)\r\n{\r\nreturn RSPD_GEN(r->type_gen) == q->gen;\r\n}\r\nstatic inline void rspq_next(struct sge_rspq *q)\r\n{\r\nq->cur_desc = (void *)q->cur_desc + q->iqe_len;\r\nif (unlikely(++q->cidx == q->size)) {\r\nq->cidx = 0;\r\nq->gen ^= 1;\r\nq->cur_desc = q->desc;\r\n}\r\n}\r\nstatic int process_responses(struct sge_rspq *q, int budget)\r\n{\r\nint ret, rsp_type;\r\nint budget_left = budget;\r\nconst struct rsp_ctrl *rc;\r\nstruct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, rspq);\r\nwhile (likely(budget_left)) {\r\nrc = (void *)q->cur_desc + (q->iqe_len - sizeof(*rc));\r\nif (!is_new_response(rc, q))\r\nbreak;\r\nrmb();\r\nrsp_type = RSPD_TYPE(rc->type_gen);\r\nif (likely(rsp_type == RSP_TYPE_FLBUF)) {\r\nskb_frag_t *fp;\r\nstruct pkt_gl si;\r\nconst struct rx_sw_desc *rsd;\r\nu32 len = ntohl(rc->pldbuflen_qid), bufsz, frags;\r\nif (len & RSPD_NEWBUF) {\r\nif (likely(q->offset > 0)) {\r\nfree_rx_bufs(q->adap, &rxq->fl, 1);\r\nq->offset = 0;\r\n}\r\nlen = RSPD_LEN(len);\r\n}\r\nsi.tot_len = len;\r\nfor (frags = 0, fp = si.frags; ; frags++, fp++) {\r\nrsd = &rxq->fl.sdesc[rxq->fl.cidx];\r\nbufsz = get_buf_size(rsd);\r\nfp->page = rsd->page;\r\nfp->page_offset = q->offset;\r\nfp->size = min(bufsz, len);\r\nlen -= fp->size;\r\nif (!len)\r\nbreak;\r\nunmap_rx_buf(q->adap, &rxq->fl);\r\n}\r\ndma_sync_single_for_cpu(q->adap->pdev_dev,\r\nget_buf_addr(rsd),\r\nfp->size, DMA_FROM_DEVICE);\r\nsi.va = page_address(si.frags[0].page) +\r\nsi.frags[0].page_offset;\r\nprefetch(si.va);\r\nsi.nfrags = frags + 1;\r\nret = q->handler(q, q->cur_desc, &si);\r\nif (likely(ret == 0))\r\nq->offset += ALIGN(fp->size, FL_ALIGN);\r\nelse\r\nrestore_rx_bufs(&si, &rxq->fl, frags);\r\n} else if (likely(rsp_type == RSP_TYPE_CPL)) {\r\nret = q->handler(q, q->cur_desc, NULL);\r\n} else {\r\nret = q->handler(q, (const __be64 *)rc, CXGB4_MSG_AN);\r\n}\r\nif (unlikely(ret)) {\r\nq->next_intr_params = QINTR_TIMER_IDX(NOMEM_TMR_IDX);\r\nbreak;\r\n}\r\nrspq_next(q);\r\nbudget_left--;\r\n}\r\nif (q->offset >= 0 && rxq->fl.size - rxq->fl.avail >= 16)\r\n__refill_fl(q->adap, &rxq->fl);\r\nreturn budget - budget_left;\r\n}\r\nstatic int napi_rx_handler(struct napi_struct *napi, int budget)\r\n{\r\nunsigned int params;\r\nstruct sge_rspq *q = container_of(napi, struct sge_rspq, napi);\r\nint work_done = process_responses(q, budget);\r\nif (likely(work_done < budget)) {\r\nnapi_complete(napi);\r\nparams = q->next_intr_params;\r\nq->next_intr_params = q->intr_params;\r\n} else\r\nparams = QINTR_TIMER_IDX(7);\r\nt4_write_reg(q->adap, MYPF_REG(SGE_PF_GTS), CIDXINC(work_done) |\r\nINGRESSQID((u32)q->cntxt_id) | SEINTARM(params));\r\nreturn work_done;\r\n}\r\nirqreturn_t t4_sge_intr_msix(int irq, void *cookie)\r\n{\r\nstruct sge_rspq *q = cookie;\r\nnapi_schedule(&q->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic unsigned int process_intrq(struct adapter *adap)\r\n{\r\nunsigned int credits;\r\nconst struct rsp_ctrl *rc;\r\nstruct sge_rspq *q = &adap->sge.intrq;\r\nspin_lock(&adap->sge.intrq_lock);\r\nfor (credits = 0; ; credits++) {\r\nrc = (void *)q->cur_desc + (q->iqe_len - sizeof(*rc));\r\nif (!is_new_response(rc, q))\r\nbreak;\r\nrmb();\r\nif (RSPD_TYPE(rc->type_gen) == RSP_TYPE_INTR) {\r\nunsigned int qid = ntohl(rc->pldbuflen_qid);\r\nqid -= adap->sge.ingr_start;\r\nnapi_schedule(&adap->sge.ingr_map[qid]->napi);\r\n}\r\nrspq_next(q);\r\n}\r\nt4_write_reg(adap, MYPF_REG(SGE_PF_GTS), CIDXINC(credits) |\r\nINGRESSQID(q->cntxt_id) | SEINTARM(q->intr_params));\r\nspin_unlock(&adap->sge.intrq_lock);\r\nreturn credits;\r\n}\r\nstatic irqreturn_t t4_intr_msi(int irq, void *cookie)\r\n{\r\nstruct adapter *adap = cookie;\r\nt4_slow_intr_handler(adap);\r\nprocess_intrq(adap);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t t4_intr_intx(int irq, void *cookie)\r\n{\r\nstruct adapter *adap = cookie;\r\nt4_write_reg(adap, MYPF_REG(PCIE_PF_CLI), 0);\r\nif (t4_slow_intr_handler(adap) | process_intrq(adap))\r\nreturn IRQ_HANDLED;\r\nreturn IRQ_NONE;\r\n}\r\nirq_handler_t t4_intr_handler(struct adapter *adap)\r\n{\r\nif (adap->flags & USING_MSIX)\r\nreturn t4_sge_intr_msix;\r\nif (adap->flags & USING_MSI)\r\nreturn t4_intr_msi;\r\nreturn t4_intr_intx;\r\n}\r\nstatic void sge_rx_timer_cb(unsigned long data)\r\n{\r\nunsigned long m;\r\nunsigned int i, cnt[2];\r\nstruct adapter *adap = (struct adapter *)data;\r\nstruct sge *s = &adap->sge;\r\nfor (i = 0; i < ARRAY_SIZE(s->starving_fl); i++)\r\nfor (m = s->starving_fl[i]; m; m &= m - 1) {\r\nstruct sge_eth_rxq *rxq;\r\nunsigned int id = __ffs(m) + i * BITS_PER_LONG;\r\nstruct sge_fl *fl = s->egr_map[id];\r\nclear_bit(id, s->starving_fl);\r\nsmp_mb__after_clear_bit();\r\nif (fl_starving(fl)) {\r\nrxq = container_of(fl, struct sge_eth_rxq, fl);\r\nif (napi_reschedule(&rxq->rspq.napi))\r\nfl->starving++;\r\nelse\r\nset_bit(id, s->starving_fl);\r\n}\r\n}\r\nt4_write_reg(adap, SGE_DEBUG_INDEX, 13);\r\ncnt[0] = t4_read_reg(adap, SGE_DEBUG_DATA_HIGH);\r\ncnt[1] = t4_read_reg(adap, SGE_DEBUG_DATA_LOW);\r\nfor (i = 0; i < 2; i++)\r\nif (cnt[i] >= s->starve_thres) {\r\nif (s->idma_state[i] || cnt[i] == 0xffffffff)\r\ncontinue;\r\ns->idma_state[i] = 1;\r\nt4_write_reg(adap, SGE_DEBUG_INDEX, 11);\r\nm = t4_read_reg(adap, SGE_DEBUG_DATA_LOW) >> (i * 16);\r\ndev_warn(adap->pdev_dev,\r\n"SGE idma%u starvation detected for "\r\n"queue %lu\n", i, m & 0xffff);\r\n} else if (s->idma_state[i])\r\ns->idma_state[i] = 0;\r\nmod_timer(&s->rx_timer, jiffies + RX_QCHECK_PERIOD);\r\n}\r\nstatic void sge_tx_timer_cb(unsigned long data)\r\n{\r\nunsigned long m;\r\nunsigned int i, budget;\r\nstruct adapter *adap = (struct adapter *)data;\r\nstruct sge *s = &adap->sge;\r\nfor (i = 0; i < ARRAY_SIZE(s->txq_maperr); i++)\r\nfor (m = s->txq_maperr[i]; m; m &= m - 1) {\r\nunsigned long id = __ffs(m) + i * BITS_PER_LONG;\r\nstruct sge_ofld_txq *txq = s->egr_map[id];\r\nclear_bit(id, s->txq_maperr);\r\ntasklet_schedule(&txq->qresume_tsk);\r\n}\r\nbudget = MAX_TIMER_TX_RECLAIM;\r\ni = s->ethtxq_rover;\r\ndo {\r\nstruct sge_eth_txq *q = &s->ethtxq[i];\r\nif (q->q.in_use &&\r\ntime_after_eq(jiffies, q->txq->trans_start + HZ / 100) &&\r\n__netif_tx_trylock(q->txq)) {\r\nint avail = reclaimable(&q->q);\r\nif (avail) {\r\nif (avail > budget)\r\navail = budget;\r\nfree_tx_desc(adap, &q->q, avail, true);\r\nq->q.in_use -= avail;\r\nbudget -= avail;\r\n}\r\n__netif_tx_unlock(q->txq);\r\n}\r\nif (++i >= s->ethqsets)\r\ni = 0;\r\n} while (budget && i != s->ethtxq_rover);\r\ns->ethtxq_rover = i;\r\nmod_timer(&s->tx_timer, jiffies + (budget ? TX_QCHECK_PERIOD : 2));\r\n}\r\nint t4_sge_alloc_rxq(struct adapter *adap, struct sge_rspq *iq, bool fwevtq,\r\nstruct net_device *dev, int intr_idx,\r\nstruct sge_fl *fl, rspq_handler_t hnd)\r\n{\r\nint ret, flsz = 0;\r\nstruct fw_iq_cmd c;\r\nstruct port_info *pi = netdev_priv(dev);\r\niq->size = roundup(iq->size, 16);\r\niq->desc = alloc_ring(adap->pdev_dev, iq->size, iq->iqe_len, 0,\r\n&iq->phys_addr, NULL, 0, NUMA_NO_NODE);\r\nif (!iq->desc)\r\nreturn -ENOMEM;\r\nmemset(&c, 0, sizeof(c));\r\nc.op_to_vfn = htonl(FW_CMD_OP(FW_IQ_CMD) | FW_CMD_REQUEST |\r\nFW_CMD_WRITE | FW_CMD_EXEC |\r\nFW_IQ_CMD_PFN(adap->fn) | FW_IQ_CMD_VFN(0));\r\nc.alloc_to_len16 = htonl(FW_IQ_CMD_ALLOC | FW_IQ_CMD_IQSTART(1) |\r\nFW_LEN16(c));\r\nc.type_to_iqandstindex = htonl(FW_IQ_CMD_TYPE(FW_IQ_TYPE_FL_INT_CAP) |\r\nFW_IQ_CMD_IQASYNCH(fwevtq) | FW_IQ_CMD_VIID(pi->viid) |\r\nFW_IQ_CMD_IQANDST(intr_idx < 0) | FW_IQ_CMD_IQANUD(1) |\r\nFW_IQ_CMD_IQANDSTINDEX(intr_idx >= 0 ? intr_idx :\r\n-intr_idx - 1));\r\nc.iqdroprss_to_iqesize = htons(FW_IQ_CMD_IQPCIECH(pi->tx_chan) |\r\nFW_IQ_CMD_IQGTSMODE |\r\nFW_IQ_CMD_IQINTCNTTHRESH(iq->pktcnt_idx) |\r\nFW_IQ_CMD_IQESIZE(ilog2(iq->iqe_len) - 4));\r\nc.iqsize = htons(iq->size);\r\nc.iqaddr = cpu_to_be64(iq->phys_addr);\r\nif (fl) {\r\nfl->size = roundup(fl->size, 8);\r\nfl->desc = alloc_ring(adap->pdev_dev, fl->size, sizeof(__be64),\r\nsizeof(struct rx_sw_desc), &fl->addr,\r\n&fl->sdesc, STAT_LEN, NUMA_NO_NODE);\r\nif (!fl->desc)\r\ngoto fl_nomem;\r\nflsz = fl->size / 8 + STAT_LEN / sizeof(struct tx_desc);\r\nc.iqns_to_fl0congen = htonl(FW_IQ_CMD_FL0PACKEN |\r\nFW_IQ_CMD_FL0FETCHRO(1) |\r\nFW_IQ_CMD_FL0DATARO(1) |\r\nFW_IQ_CMD_FL0PADEN);\r\nc.fl0dcaen_to_fl0cidxfthresh = htons(FW_IQ_CMD_FL0FBMIN(2) |\r\nFW_IQ_CMD_FL0FBMAX(3));\r\nc.fl0size = htons(flsz);\r\nc.fl0addr = cpu_to_be64(fl->addr);\r\n}\r\nret = t4_wr_mbox(adap, adap->fn, &c, sizeof(c), &c);\r\nif (ret)\r\ngoto err;\r\nnetif_napi_add(dev, &iq->napi, napi_rx_handler, 64);\r\niq->cur_desc = iq->desc;\r\niq->cidx = 0;\r\niq->gen = 1;\r\niq->next_intr_params = iq->intr_params;\r\niq->cntxt_id = ntohs(c.iqid);\r\niq->abs_id = ntohs(c.physiqid);\r\niq->size--;\r\niq->adap = adap;\r\niq->netdev = dev;\r\niq->handler = hnd;\r\niq->offset = fl ? 0 : -1;\r\nadap->sge.ingr_map[iq->cntxt_id - adap->sge.ingr_start] = iq;\r\nif (fl) {\r\nfl->cntxt_id = ntohs(c.fl0id);\r\nfl->avail = fl->pend_cred = 0;\r\nfl->pidx = fl->cidx = 0;\r\nfl->alloc_failed = fl->large_alloc_failed = fl->starving = 0;\r\nadap->sge.egr_map[fl->cntxt_id - adap->sge.egr_start] = fl;\r\nrefill_fl(adap, fl, fl_cap(fl), GFP_KERNEL);\r\n}\r\nreturn 0;\r\nfl_nomem:\r\nret = -ENOMEM;\r\nerr:\r\nif (iq->desc) {\r\ndma_free_coherent(adap->pdev_dev, iq->size * iq->iqe_len,\r\niq->desc, iq->phys_addr);\r\niq->desc = NULL;\r\n}\r\nif (fl && fl->desc) {\r\nkfree(fl->sdesc);\r\nfl->sdesc = NULL;\r\ndma_free_coherent(adap->pdev_dev, flsz * sizeof(struct tx_desc),\r\nfl->desc, fl->addr);\r\nfl->desc = NULL;\r\n}\r\nreturn ret;\r\n}\r\nstatic void init_txq(struct adapter *adap, struct sge_txq *q, unsigned int id)\r\n{\r\nq->in_use = 0;\r\nq->cidx = q->pidx = 0;\r\nq->stops = q->restarts = 0;\r\nq->stat = (void *)&q->desc[q->size];\r\nq->cntxt_id = id;\r\nadap->sge.egr_map[id - adap->sge.egr_start] = q;\r\n}\r\nint t4_sge_alloc_eth_txq(struct adapter *adap, struct sge_eth_txq *txq,\r\nstruct net_device *dev, struct netdev_queue *netdevq,\r\nunsigned int iqid)\r\n{\r\nint ret, nentries;\r\nstruct fw_eq_eth_cmd c;\r\nstruct port_info *pi = netdev_priv(dev);\r\nnentries = txq->q.size + STAT_LEN / sizeof(struct tx_desc);\r\ntxq->q.desc = alloc_ring(adap->pdev_dev, txq->q.size,\r\nsizeof(struct tx_desc), sizeof(struct tx_sw_desc),\r\n&txq->q.phys_addr, &txq->q.sdesc, STAT_LEN,\r\nnetdev_queue_numa_node_read(netdevq));\r\nif (!txq->q.desc)\r\nreturn -ENOMEM;\r\nmemset(&c, 0, sizeof(c));\r\nc.op_to_vfn = htonl(FW_CMD_OP(FW_EQ_ETH_CMD) | FW_CMD_REQUEST |\r\nFW_CMD_WRITE | FW_CMD_EXEC |\r\nFW_EQ_ETH_CMD_PFN(adap->fn) | FW_EQ_ETH_CMD_VFN(0));\r\nc.alloc_to_len16 = htonl(FW_EQ_ETH_CMD_ALLOC |\r\nFW_EQ_ETH_CMD_EQSTART | FW_LEN16(c));\r\nc.viid_pkd = htonl(FW_EQ_ETH_CMD_VIID(pi->viid));\r\nc.fetchszm_to_iqid = htonl(FW_EQ_ETH_CMD_HOSTFCMODE(2) |\r\nFW_EQ_ETH_CMD_PCIECHN(pi->tx_chan) |\r\nFW_EQ_ETH_CMD_FETCHRO(1) |\r\nFW_EQ_ETH_CMD_IQID(iqid));\r\nc.dcaen_to_eqsize = htonl(FW_EQ_ETH_CMD_FBMIN(2) |\r\nFW_EQ_ETH_CMD_FBMAX(3) |\r\nFW_EQ_ETH_CMD_CIDXFTHRESH(5) |\r\nFW_EQ_ETH_CMD_EQSIZE(nentries));\r\nc.eqaddr = cpu_to_be64(txq->q.phys_addr);\r\nret = t4_wr_mbox(adap, adap->fn, &c, sizeof(c), &c);\r\nif (ret) {\r\nkfree(txq->q.sdesc);\r\ntxq->q.sdesc = NULL;\r\ndma_free_coherent(adap->pdev_dev,\r\nnentries * sizeof(struct tx_desc),\r\ntxq->q.desc, txq->q.phys_addr);\r\ntxq->q.desc = NULL;\r\nreturn ret;\r\n}\r\ninit_txq(adap, &txq->q, FW_EQ_ETH_CMD_EQID_GET(ntohl(c.eqid_pkd)));\r\ntxq->txq = netdevq;\r\ntxq->tso = txq->tx_cso = txq->vlan_ins = 0;\r\ntxq->mapping_err = 0;\r\nreturn 0;\r\n}\r\nint t4_sge_alloc_ctrl_txq(struct adapter *adap, struct sge_ctrl_txq *txq,\r\nstruct net_device *dev, unsigned int iqid,\r\nunsigned int cmplqid)\r\n{\r\nint ret, nentries;\r\nstruct fw_eq_ctrl_cmd c;\r\nstruct port_info *pi = netdev_priv(dev);\r\nnentries = txq->q.size + STAT_LEN / sizeof(struct tx_desc);\r\ntxq->q.desc = alloc_ring(adap->pdev_dev, nentries,\r\nsizeof(struct tx_desc), 0, &txq->q.phys_addr,\r\nNULL, 0, NUMA_NO_NODE);\r\nif (!txq->q.desc)\r\nreturn -ENOMEM;\r\nc.op_to_vfn = htonl(FW_CMD_OP(FW_EQ_CTRL_CMD) | FW_CMD_REQUEST |\r\nFW_CMD_WRITE | FW_CMD_EXEC |\r\nFW_EQ_CTRL_CMD_PFN(adap->fn) |\r\nFW_EQ_CTRL_CMD_VFN(0));\r\nc.alloc_to_len16 = htonl(FW_EQ_CTRL_CMD_ALLOC |\r\nFW_EQ_CTRL_CMD_EQSTART | FW_LEN16(c));\r\nc.cmpliqid_eqid = htonl(FW_EQ_CTRL_CMD_CMPLIQID(cmplqid));\r\nc.physeqid_pkd = htonl(0);\r\nc.fetchszm_to_iqid = htonl(FW_EQ_CTRL_CMD_HOSTFCMODE(2) |\r\nFW_EQ_CTRL_CMD_PCIECHN(pi->tx_chan) |\r\nFW_EQ_CTRL_CMD_FETCHRO |\r\nFW_EQ_CTRL_CMD_IQID(iqid));\r\nc.dcaen_to_eqsize = htonl(FW_EQ_CTRL_CMD_FBMIN(2) |\r\nFW_EQ_CTRL_CMD_FBMAX(3) |\r\nFW_EQ_CTRL_CMD_CIDXFTHRESH(5) |\r\nFW_EQ_CTRL_CMD_EQSIZE(nentries));\r\nc.eqaddr = cpu_to_be64(txq->q.phys_addr);\r\nret = t4_wr_mbox(adap, adap->fn, &c, sizeof(c), &c);\r\nif (ret) {\r\ndma_free_coherent(adap->pdev_dev,\r\nnentries * sizeof(struct tx_desc),\r\ntxq->q.desc, txq->q.phys_addr);\r\ntxq->q.desc = NULL;\r\nreturn ret;\r\n}\r\ninit_txq(adap, &txq->q, FW_EQ_CTRL_CMD_EQID_GET(ntohl(c.cmpliqid_eqid)));\r\ntxq->adap = adap;\r\nskb_queue_head_init(&txq->sendq);\r\ntasklet_init(&txq->qresume_tsk, restart_ctrlq, (unsigned long)txq);\r\ntxq->full = 0;\r\nreturn 0;\r\n}\r\nint t4_sge_alloc_ofld_txq(struct adapter *adap, struct sge_ofld_txq *txq,\r\nstruct net_device *dev, unsigned int iqid)\r\n{\r\nint ret, nentries;\r\nstruct fw_eq_ofld_cmd c;\r\nstruct port_info *pi = netdev_priv(dev);\r\nnentries = txq->q.size + STAT_LEN / sizeof(struct tx_desc);\r\ntxq->q.desc = alloc_ring(adap->pdev_dev, txq->q.size,\r\nsizeof(struct tx_desc), sizeof(struct tx_sw_desc),\r\n&txq->q.phys_addr, &txq->q.sdesc, STAT_LEN,\r\nNUMA_NO_NODE);\r\nif (!txq->q.desc)\r\nreturn -ENOMEM;\r\nmemset(&c, 0, sizeof(c));\r\nc.op_to_vfn = htonl(FW_CMD_OP(FW_EQ_OFLD_CMD) | FW_CMD_REQUEST |\r\nFW_CMD_WRITE | FW_CMD_EXEC |\r\nFW_EQ_OFLD_CMD_PFN(adap->fn) |\r\nFW_EQ_OFLD_CMD_VFN(0));\r\nc.alloc_to_len16 = htonl(FW_EQ_OFLD_CMD_ALLOC |\r\nFW_EQ_OFLD_CMD_EQSTART | FW_LEN16(c));\r\nc.fetchszm_to_iqid = htonl(FW_EQ_OFLD_CMD_HOSTFCMODE(2) |\r\nFW_EQ_OFLD_CMD_PCIECHN(pi->tx_chan) |\r\nFW_EQ_OFLD_CMD_FETCHRO(1) |\r\nFW_EQ_OFLD_CMD_IQID(iqid));\r\nc.dcaen_to_eqsize = htonl(FW_EQ_OFLD_CMD_FBMIN(2) |\r\nFW_EQ_OFLD_CMD_FBMAX(3) |\r\nFW_EQ_OFLD_CMD_CIDXFTHRESH(5) |\r\nFW_EQ_OFLD_CMD_EQSIZE(nentries));\r\nc.eqaddr = cpu_to_be64(txq->q.phys_addr);\r\nret = t4_wr_mbox(adap, adap->fn, &c, sizeof(c), &c);\r\nif (ret) {\r\nkfree(txq->q.sdesc);\r\ntxq->q.sdesc = NULL;\r\ndma_free_coherent(adap->pdev_dev,\r\nnentries * sizeof(struct tx_desc),\r\ntxq->q.desc, txq->q.phys_addr);\r\ntxq->q.desc = NULL;\r\nreturn ret;\r\n}\r\ninit_txq(adap, &txq->q, FW_EQ_OFLD_CMD_EQID_GET(ntohl(c.eqid_pkd)));\r\ntxq->adap = adap;\r\nskb_queue_head_init(&txq->sendq);\r\ntasklet_init(&txq->qresume_tsk, restart_ofldq, (unsigned long)txq);\r\ntxq->full = 0;\r\ntxq->mapping_err = 0;\r\nreturn 0;\r\n}\r\nstatic void free_txq(struct adapter *adap, struct sge_txq *q)\r\n{\r\ndma_free_coherent(adap->pdev_dev,\r\nq->size * sizeof(struct tx_desc) + STAT_LEN,\r\nq->desc, q->phys_addr);\r\nq->cntxt_id = 0;\r\nq->sdesc = NULL;\r\nq->desc = NULL;\r\n}\r\nstatic void free_rspq_fl(struct adapter *adap, struct sge_rspq *rq,\r\nstruct sge_fl *fl)\r\n{\r\nunsigned int fl_id = fl ? fl->cntxt_id : 0xffff;\r\nadap->sge.ingr_map[rq->cntxt_id - adap->sge.ingr_start] = NULL;\r\nt4_iq_free(adap, adap->fn, adap->fn, 0, FW_IQ_TYPE_FL_INT_CAP,\r\nrq->cntxt_id, fl_id, 0xffff);\r\ndma_free_coherent(adap->pdev_dev, (rq->size + 1) * rq->iqe_len,\r\nrq->desc, rq->phys_addr);\r\nnetif_napi_del(&rq->napi);\r\nrq->netdev = NULL;\r\nrq->cntxt_id = rq->abs_id = 0;\r\nrq->desc = NULL;\r\nif (fl) {\r\nfree_rx_bufs(adap, fl, fl->avail);\r\ndma_free_coherent(adap->pdev_dev, fl->size * 8 + STAT_LEN,\r\nfl->desc, fl->addr);\r\nkfree(fl->sdesc);\r\nfl->sdesc = NULL;\r\nfl->cntxt_id = 0;\r\nfl->desc = NULL;\r\n}\r\n}\r\nvoid t4_free_sge_resources(struct adapter *adap)\r\n{\r\nint i;\r\nstruct sge_eth_rxq *eq = adap->sge.ethrxq;\r\nstruct sge_eth_txq *etq = adap->sge.ethtxq;\r\nstruct sge_ofld_rxq *oq = adap->sge.ofldrxq;\r\nfor (i = 0; i < adap->sge.ethqsets; i++, eq++, etq++) {\r\nif (eq->rspq.desc)\r\nfree_rspq_fl(adap, &eq->rspq, &eq->fl);\r\nif (etq->q.desc) {\r\nt4_eth_eq_free(adap, adap->fn, adap->fn, 0,\r\netq->q.cntxt_id);\r\nfree_tx_desc(adap, &etq->q, etq->q.in_use, true);\r\nkfree(etq->q.sdesc);\r\nfree_txq(adap, &etq->q);\r\n}\r\n}\r\nfor (i = 0; i < adap->sge.ofldqsets; i++, oq++) {\r\nif (oq->rspq.desc)\r\nfree_rspq_fl(adap, &oq->rspq, &oq->fl);\r\n}\r\nfor (i = 0, oq = adap->sge.rdmarxq; i < adap->sge.rdmaqs; i++, oq++) {\r\nif (oq->rspq.desc)\r\nfree_rspq_fl(adap, &oq->rspq, &oq->fl);\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(adap->sge.ofldtxq); i++) {\r\nstruct sge_ofld_txq *q = &adap->sge.ofldtxq[i];\r\nif (q->q.desc) {\r\ntasklet_kill(&q->qresume_tsk);\r\nt4_ofld_eq_free(adap, adap->fn, adap->fn, 0,\r\nq->q.cntxt_id);\r\nfree_tx_desc(adap, &q->q, q->q.in_use, false);\r\nkfree(q->q.sdesc);\r\n__skb_queue_purge(&q->sendq);\r\nfree_txq(adap, &q->q);\r\n}\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(adap->sge.ctrlq); i++) {\r\nstruct sge_ctrl_txq *cq = &adap->sge.ctrlq[i];\r\nif (cq->q.desc) {\r\ntasklet_kill(&cq->qresume_tsk);\r\nt4_ctrl_eq_free(adap, adap->fn, adap->fn, 0,\r\ncq->q.cntxt_id);\r\n__skb_queue_purge(&cq->sendq);\r\nfree_txq(adap, &cq->q);\r\n}\r\n}\r\nif (adap->sge.fw_evtq.desc)\r\nfree_rspq_fl(adap, &adap->sge.fw_evtq, NULL);\r\nif (adap->sge.intrq.desc)\r\nfree_rspq_fl(adap, &adap->sge.intrq, NULL);\r\nmemset(adap->sge.egr_map, 0, sizeof(adap->sge.egr_map));\r\n}\r\nvoid t4_sge_start(struct adapter *adap)\r\n{\r\nadap->sge.ethtxq_rover = 0;\r\nmod_timer(&adap->sge.rx_timer, jiffies + RX_QCHECK_PERIOD);\r\nmod_timer(&adap->sge.tx_timer, jiffies + TX_QCHECK_PERIOD);\r\n}\r\nvoid t4_sge_stop(struct adapter *adap)\r\n{\r\nint i;\r\nstruct sge *s = &adap->sge;\r\nif (in_interrupt())\r\nreturn;\r\nif (s->rx_timer.function)\r\ndel_timer_sync(&s->rx_timer);\r\nif (s->tx_timer.function)\r\ndel_timer_sync(&s->tx_timer);\r\nfor (i = 0; i < ARRAY_SIZE(s->ofldtxq); i++) {\r\nstruct sge_ofld_txq *q = &s->ofldtxq[i];\r\nif (q->q.desc)\r\ntasklet_kill(&q->qresume_tsk);\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(s->ctrlq); i++) {\r\nstruct sge_ctrl_txq *cq = &s->ctrlq[i];\r\nif (cq->q.desc)\r\ntasklet_kill(&cq->qresume_tsk);\r\n}\r\n}\r\nvoid t4_sge_init(struct adapter *adap)\r\n{\r\nunsigned int i, v;\r\nstruct sge *s = &adap->sge;\r\nunsigned int fl_align_log = ilog2(FL_ALIGN);\r\nt4_set_reg_field(adap, SGE_CONTROL, PKTSHIFT_MASK |\r\nINGPADBOUNDARY_MASK | EGRSTATUSPAGESIZE,\r\nINGPADBOUNDARY(fl_align_log - 5) | PKTSHIFT(2) |\r\nRXPKTCPLMODE |\r\n(STAT_LEN == 128 ? EGRSTATUSPAGESIZE : 0));\r\nfor (i = v = 0; i < 32; i += 4)\r\nv |= (PAGE_SHIFT - 10) << i;\r\nt4_write_reg(adap, SGE_HOST_PAGE_SIZE, v);\r\nt4_write_reg(adap, SGE_FL_BUFFER_SIZE0, PAGE_SIZE);\r\n#if FL_PG_ORDER > 0\r\nt4_write_reg(adap, SGE_FL_BUFFER_SIZE1, PAGE_SIZE << FL_PG_ORDER);\r\n#endif\r\nt4_write_reg(adap, SGE_INGRESS_RX_THRESHOLD,\r\nTHRESHOLD_0(s->counter_val[0]) |\r\nTHRESHOLD_1(s->counter_val[1]) |\r\nTHRESHOLD_2(s->counter_val[2]) |\r\nTHRESHOLD_3(s->counter_val[3]));\r\nt4_write_reg(adap, SGE_TIMER_VALUE_0_AND_1,\r\nTIMERVALUE0(us_to_core_ticks(adap, s->timer_val[0])) |\r\nTIMERVALUE1(us_to_core_ticks(adap, s->timer_val[1])));\r\nt4_write_reg(adap, SGE_TIMER_VALUE_2_AND_3,\r\nTIMERVALUE0(us_to_core_ticks(adap, s->timer_val[2])) |\r\nTIMERVALUE1(us_to_core_ticks(adap, s->timer_val[3])));\r\nt4_write_reg(adap, SGE_TIMER_VALUE_4_AND_5,\r\nTIMERVALUE0(us_to_core_ticks(adap, s->timer_val[4])) |\r\nTIMERVALUE1(us_to_core_ticks(adap, s->timer_val[5])));\r\nsetup_timer(&s->rx_timer, sge_rx_timer_cb, (unsigned long)adap);\r\nsetup_timer(&s->tx_timer, sge_tx_timer_cb, (unsigned long)adap);\r\ns->starve_thres = core_ticks_per_usec(adap) * 1000000;\r\ns->idma_state[0] = s->idma_state[1] = 0;\r\nspin_lock_init(&s->intrq_lock);\r\n}
