static int __init blackfin_dma_init(void)\r\n{\r\nint i;\r\nprintk(KERN_INFO "Blackfin DMA Controller\n");\r\n#if ANOMALY_05000480\r\nbfin_write_DMAC_TC_PER(0x0111);\r\n#endif\r\nfor (i = 0; i < MAX_DMA_CHANNELS; i++) {\r\natomic_set(&dma_ch[i].chan_status, 0);\r\ndma_ch[i].regs = dma_io_base_addr[i];\r\n}\r\nrequest_dma(CH_MEM_STREAM0_DEST, "Blackfin dma_memcpy");\r\nrequest_dma(CH_MEM_STREAM0_SRC, "Blackfin dma_memcpy");\r\n#if defined(CONFIG_DEB_DMA_URGENT)\r\nbfin_write_EBIU_DDRQUE(bfin_read_EBIU_DDRQUE()\r\n| DEB1_URGENT | DEB2_URGENT | DEB3_URGENT);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int proc_dma_show(struct seq_file *m, void *v)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_DMA_CHANNELS; ++i)\r\nif (dma_channel_active(i))\r\nseq_printf(m, "%2d: %s\n", i, dma_ch[i].device_id);\r\nreturn 0;\r\n}\r\nstatic int proc_dma_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, proc_dma_show, NULL);\r\n}\r\nstatic int __init proc_dma_init(void)\r\n{\r\nreturn proc_create("dma", 0, NULL, &proc_dma_operations) != NULL;\r\n}\r\nstatic void set_dma_peripheral_map(unsigned int channel, const char *device_id)\r\n{\r\n#ifdef CONFIG_BF54x\r\nunsigned int per_map;\r\nswitch (channel) {\r\ncase CH_UART2_RX: per_map = 0xC << 12; break;\r\ncase CH_UART2_TX: per_map = 0xD << 12; break;\r\ncase CH_UART3_RX: per_map = 0xE << 12; break;\r\ncase CH_UART3_TX: per_map = 0xF << 12; break;\r\ndefault: return;\r\n}\r\nif (strncmp(device_id, "BFIN_UART", 9) == 0)\r\ndma_ch[channel].regs->peripheral_map = per_map;\r\n#endif\r\n}\r\nint request_dma(unsigned int channel, const char *device_id)\r\n{\r\npr_debug("request_dma() : BEGIN\n");\r\nif (device_id == NULL)\r\nprintk(KERN_WARNING "request_dma(%u): no device_id given\n", channel);\r\n#if defined(CONFIG_BF561) && ANOMALY_05000182\r\nif (channel >= CH_IMEM_STREAM0_DEST && channel <= CH_IMEM_STREAM1_DEST) {\r\nif (get_cclk() > 500000000) {\r\nprintk(KERN_WARNING\r\n"Request IMDMA failed due to ANOMALY 05000182\n");\r\nreturn -EFAULT;\r\n}\r\n}\r\n#endif\r\nif (atomic_cmpxchg(&dma_ch[channel].chan_status, 0, 1)) {\r\npr_debug("DMA CHANNEL IN USE\n");\r\nreturn -EBUSY;\r\n}\r\nset_dma_peripheral_map(channel, device_id);\r\ndma_ch[channel].device_id = device_id;\r\ndma_ch[channel].irq = 0;\r\npr_debug("request_dma() : END\n");\r\nreturn 0;\r\n}\r\nint set_dma_callback(unsigned int channel, irq_handler_t callback, void *data)\r\n{\r\nint ret;\r\nunsigned int irq;\r\nBUG_ON(channel >= MAX_DMA_CHANNELS || !callback ||\r\n!atomic_read(&dma_ch[channel].chan_status));\r\nirq = channel2irq(channel);\r\nret = request_irq(irq, callback, 0, dma_ch[channel].device_id, data);\r\nif (ret)\r\nreturn ret;\r\ndma_ch[channel].irq = irq;\r\ndma_ch[channel].data = data;\r\nreturn 0;\r\n}\r\nstatic void clear_dma_buffer(unsigned int channel)\r\n{\r\ndma_ch[channel].regs->cfg |= RESTART;\r\nSSYNC();\r\ndma_ch[channel].regs->cfg &= ~RESTART;\r\n}\r\nvoid free_dma(unsigned int channel)\r\n{\r\npr_debug("freedma() : BEGIN\n");\r\nBUG_ON(channel >= MAX_DMA_CHANNELS ||\r\n!atomic_read(&dma_ch[channel].chan_status));\r\ndisable_dma(channel);\r\nclear_dma_buffer(channel);\r\nif (dma_ch[channel].irq)\r\nfree_irq(dma_ch[channel].irq, dma_ch[channel].data);\r\natomic_set(&dma_ch[channel].chan_status, 0);\r\npr_debug("freedma() : END\n");\r\n}\r\nint blackfin_dma_suspend(void)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_DMA_CHANNELS; ++i) {\r\nif (dma_ch[i].regs->cfg & DMAEN) {\r\nprintk(KERN_ERR "DMA Channel %d failed to suspend\n", i);\r\nreturn -EBUSY;\r\n}\r\nif (i < MAX_DMA_SUSPEND_CHANNELS)\r\ndma_ch[i].saved_peripheral_map = dma_ch[i].regs->peripheral_map;\r\n}\r\nreturn 0;\r\n}\r\nvoid blackfin_dma_resume(void)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_DMA_CHANNELS; ++i) {\r\ndma_ch[i].regs->cfg = 0;\r\nif (i < MAX_DMA_SUSPEND_CHANNELS)\r\ndma_ch[i].regs->peripheral_map = dma_ch[i].saved_peripheral_map;\r\n}\r\n}\r\nvoid __init blackfin_dma_early_init(void)\r\n{\r\nearly_shadow_stamp();\r\nbfin_write_MDMA_S0_CONFIG(0);\r\nbfin_write_MDMA_S1_CONFIG(0);\r\n}\r\nvoid __init early_dma_memcpy(void *pdst, const void *psrc, size_t size)\r\n{\r\nunsigned long dst = (unsigned long)pdst;\r\nunsigned long src = (unsigned long)psrc;\r\nstruct dma_register *dst_ch, *src_ch;\r\nearly_shadow_stamp();\r\nBUG_ON(dst % 4);\r\nBUG_ON(src % 4);\r\nBUG_ON(size % 4);\r\nsrc_ch = 0;\r\nwhile (1) {\r\nif (src_ch == (struct dma_register *)MDMA_S0_NEXT_DESC_PTR) {\r\ndst_ch = (struct dma_register *)MDMA_D1_NEXT_DESC_PTR;\r\nsrc_ch = (struct dma_register *)MDMA_S1_NEXT_DESC_PTR;\r\n} else {\r\ndst_ch = (struct dma_register *)MDMA_D0_NEXT_DESC_PTR;\r\nsrc_ch = (struct dma_register *)MDMA_S0_NEXT_DESC_PTR;\r\n}\r\nif (!bfin_read16(&src_ch->cfg))\r\nbreak;\r\nelse if (bfin_read16(&dst_ch->irq_status) & DMA_DONE) {\r\nbfin_write16(&src_ch->cfg, 0);\r\nbreak;\r\n}\r\n}\r\n__builtin_bfin_ssync();\r\nbfin_write32(&dst_ch->start_addr, dst);\r\nbfin_write16(&dst_ch->x_count, size >> 2);\r\nbfin_write16(&dst_ch->x_modify, 1 << 2);\r\nbfin_write16(&dst_ch->irq_status, DMA_DONE | DMA_ERR);\r\nbfin_write32(&src_ch->start_addr, src);\r\nbfin_write16(&src_ch->x_count, size >> 2);\r\nbfin_write16(&src_ch->x_modify, 1 << 2);\r\nbfin_write16(&src_ch->irq_status, DMA_DONE | DMA_ERR);\r\nbfin_write16(&src_ch->cfg, DMAEN | WDSIZE_32);\r\nbfin_write16(&dst_ch->cfg, WNR | DI_EN | DMAEN | WDSIZE_32);\r\n__builtin_bfin_ssync();\r\n}\r\nvoid __init early_dma_memcpy_done(void)\r\n{\r\nearly_shadow_stamp();\r\nwhile ((bfin_read_MDMA_S0_CONFIG() && !(bfin_read_MDMA_D0_IRQ_STATUS() & DMA_DONE)) ||\r\n(bfin_read_MDMA_S1_CONFIG() && !(bfin_read_MDMA_D1_IRQ_STATUS() & DMA_DONE)))\r\ncontinue;\r\nbfin_write_MDMA_D0_IRQ_STATUS(DMA_DONE | DMA_ERR);\r\nbfin_write_MDMA_D1_IRQ_STATUS(DMA_DONE | DMA_ERR);\r\nbfin_write_MDMA_S0_CONFIG(0);\r\nbfin_write_MDMA_S1_CONFIG(0);\r\nbfin_write_MDMA_D0_CONFIG(0);\r\nbfin_write_MDMA_D1_CONFIG(0);\r\n__builtin_bfin_ssync();\r\n}\r\nstatic void __dma_memcpy(u32 daddr, s16 dmod, u32 saddr, s16 smod, size_t cnt, u32 conf)\r\n{\r\nstatic DEFINE_SPINLOCK(mdma_lock);\r\nunsigned long flags;\r\nspin_lock_irqsave(&mdma_lock, flags);\r\n__builtin_bfin_ssync();\r\nif (bfin_read_MDMA_S0_CONFIG())\r\nwhile (!(bfin_read_MDMA_D0_IRQ_STATUS() & DMA_DONE))\r\ncontinue;\r\nif (conf & DMA2D) {\r\nu32 shift = abs(dmod) >> 1;\r\nsize_t ycnt = cnt >> (16 - shift);\r\ncnt = 1 << (16 - shift);\r\nbfin_write_MDMA_D0_Y_COUNT(ycnt);\r\nbfin_write_MDMA_S0_Y_COUNT(ycnt);\r\nbfin_write_MDMA_D0_Y_MODIFY(dmod);\r\nbfin_write_MDMA_S0_Y_MODIFY(smod);\r\n}\r\nbfin_write_MDMA_D0_START_ADDR(daddr);\r\nbfin_write_MDMA_D0_X_COUNT(cnt);\r\nbfin_write_MDMA_D0_X_MODIFY(dmod);\r\nbfin_write_MDMA_D0_IRQ_STATUS(DMA_DONE | DMA_ERR);\r\nbfin_write_MDMA_S0_START_ADDR(saddr);\r\nbfin_write_MDMA_S0_X_COUNT(cnt);\r\nbfin_write_MDMA_S0_X_MODIFY(smod);\r\nbfin_write_MDMA_S0_IRQ_STATUS(DMA_DONE | DMA_ERR);\r\nbfin_write_MDMA_S0_CONFIG(DMAEN | conf);\r\nbfin_write_MDMA_D0_CONFIG(WNR | DI_EN | DMAEN | conf);\r\nspin_unlock_irqrestore(&mdma_lock, flags);\r\nSSYNC();\r\nwhile (!(bfin_read_MDMA_D0_IRQ_STATUS() & DMA_DONE))\r\nif (bfin_read_MDMA_S0_CONFIG())\r\ncontinue;\r\nelse\r\nreturn;\r\nbfin_write_MDMA_D0_IRQ_STATUS(DMA_DONE | DMA_ERR);\r\nbfin_write_MDMA_S0_CONFIG(0);\r\nbfin_write_MDMA_D0_CONFIG(0);\r\n}\r\nstatic void *_dma_memcpy(void *pdst, const void *psrc, size_t size)\r\n{\r\nu32 conf, shift;\r\ns16 mod;\r\nunsigned long dst = (unsigned long)pdst;\r\nunsigned long src = (unsigned long)psrc;\r\nif (size == 0)\r\nreturn NULL;\r\nif (dst % 4 == 0 && src % 4 == 0 && size % 4 == 0) {\r\nconf = WDSIZE_32;\r\nshift = 2;\r\n} else if (dst % 2 == 0 && src % 2 == 0 && size % 2 == 0) {\r\nconf = WDSIZE_16;\r\nshift = 1;\r\n} else {\r\nconf = WDSIZE_8;\r\nshift = 0;\r\n}\r\nmod = 1 << shift;\r\nif (src < dst) {\r\nmod *= -1;\r\ndst += size + mod;\r\nsrc += size + mod;\r\n}\r\nsize >>= shift;\r\nif (size > 0x10000)\r\nconf |= DMA2D;\r\n__dma_memcpy(dst, mod, src, mod, size, conf);\r\nreturn pdst;\r\n}\r\nvoid *dma_memcpy(void *pdst, const void *psrc, size_t size)\r\n{\r\nunsigned long dst = (unsigned long)pdst;\r\nunsigned long src = (unsigned long)psrc;\r\nif (bfin_addr_dcacheable(src))\r\nblackfin_dcache_flush_range(src, src + size);\r\nif (bfin_addr_dcacheable(dst))\r\nblackfin_dcache_invalidate_range(dst, dst + size);\r\nreturn dma_memcpy_nocache(pdst, psrc, size);\r\n}\r\nvoid *dma_memcpy_nocache(void *pdst, const void *psrc, size_t size)\r\n{\r\nsize_t bulk, rest;\r\nbulk = size & ~0xffff;\r\nrest = size - bulk;\r\nif (bulk)\r\n_dma_memcpy(pdst, psrc, bulk);\r\n_dma_memcpy(pdst + bulk, psrc + bulk, rest);\r\nreturn pdst;\r\n}\r\nvoid *safe_dma_memcpy(void *dst, const void *src, size_t size)\r\n{\r\nif (!access_ok(VERIFY_WRITE, dst, size))\r\nreturn NULL;\r\nif (!access_ok(VERIFY_READ, src, size))\r\nreturn NULL;\r\nreturn dma_memcpy(dst, src, size);\r\n}\r\nstatic void _dma_out(unsigned long addr, unsigned long buf, unsigned short len,\r\nu16 size, u16 dma_size)\r\n{\r\nblackfin_dcache_flush_range(buf, buf + len * size);\r\n__dma_memcpy(addr, 0, buf, size, len, dma_size);\r\n}\r\nstatic void _dma_in(unsigned long addr, unsigned long buf, unsigned short len,\r\nu16 size, u16 dma_size)\r\n{\r\nblackfin_dcache_invalidate_range(buf, buf + len * size);\r\n__dma_memcpy(buf, size, addr, 0, len, dma_size);\r\n}
