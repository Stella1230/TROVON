static inline cputime64_t get_cpu_idle_time_jiffy(unsigned int cpu,\r\ncputime64_t *wall)\r\n{\r\ncputime64_t idle_time;\r\ncputime64_t cur_wall_time;\r\ncputime64_t busy_time;\r\ncur_wall_time = jiffies64_to_cputime64(get_jiffies_64());\r\nbusy_time = cputime64_add(kstat_cpu(cpu).cpustat.user,\r\nkstat_cpu(cpu).cpustat.system);\r\nbusy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.irq);\r\nbusy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.softirq);\r\nbusy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.steal);\r\nbusy_time = cputime64_add(busy_time, kstat_cpu(cpu).cpustat.nice);\r\nidle_time = cputime64_sub(cur_wall_time, busy_time);\r\nif (wall)\r\n*wall = (cputime64_t)jiffies_to_usecs(cur_wall_time);\r\nreturn (cputime64_t)jiffies_to_usecs(idle_time);\r\n}\r\nstatic inline cputime64_t get_cpu_idle_time(unsigned int cpu, cputime64_t *wall)\r\n{\r\nu64 idle_time = get_cpu_idle_time_us(cpu, wall);\r\nif (idle_time == -1ULL)\r\nreturn get_cpu_idle_time_jiffy(cpu, wall);\r\nreturn idle_time;\r\n}\r\nstatic int\r\ndbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,\r\nvoid *data)\r\n{\r\nstruct cpufreq_freqs *freq = data;\r\nstruct cpu_dbs_info_s *this_dbs_info = &per_cpu(cs_cpu_dbs_info,\r\nfreq->cpu);\r\nstruct cpufreq_policy *policy;\r\nif (!this_dbs_info->enable)\r\nreturn 0;\r\npolicy = this_dbs_info->cur_policy;\r\nif (this_dbs_info->requested_freq > policy->max\r\n|| this_dbs_info->requested_freq < policy->min)\r\nthis_dbs_info->requested_freq = freq->new;\r\nreturn 0;\r\n}\r\nstatic ssize_t show_sampling_rate_min(struct kobject *kobj,\r\nstruct attribute *attr, char *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", min_sampling_rate);\r\n}\r\nstatic ssize_t store_sampling_down_factor(struct kobject *a,\r\nstruct attribute *b,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned int input;\r\nint ret;\r\nret = sscanf(buf, "%u", &input);\r\nif (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)\r\nreturn -EINVAL;\r\ndbs_tuners_ins.sampling_down_factor = input;\r\nreturn count;\r\n}\r\nstatic ssize_t store_sampling_rate(struct kobject *a, struct attribute *b,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned int input;\r\nint ret;\r\nret = sscanf(buf, "%u", &input);\r\nif (ret != 1)\r\nreturn -EINVAL;\r\ndbs_tuners_ins.sampling_rate = max(input, min_sampling_rate);\r\nreturn count;\r\n}\r\nstatic ssize_t store_up_threshold(struct kobject *a, struct attribute *b,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned int input;\r\nint ret;\r\nret = sscanf(buf, "%u", &input);\r\nif (ret != 1 || input > 100 ||\r\ninput <= dbs_tuners_ins.down_threshold)\r\nreturn -EINVAL;\r\ndbs_tuners_ins.up_threshold = input;\r\nreturn count;\r\n}\r\nstatic ssize_t store_down_threshold(struct kobject *a, struct attribute *b,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned int input;\r\nint ret;\r\nret = sscanf(buf, "%u", &input);\r\nif (ret != 1 || input < 11 || input > 100 ||\r\ninput >= dbs_tuners_ins.up_threshold)\r\nreturn -EINVAL;\r\ndbs_tuners_ins.down_threshold = input;\r\nreturn count;\r\n}\r\nstatic ssize_t store_ignore_nice_load(struct kobject *a, struct attribute *b,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned int input;\r\nint ret;\r\nunsigned int j;\r\nret = sscanf(buf, "%u", &input);\r\nif (ret != 1)\r\nreturn -EINVAL;\r\nif (input > 1)\r\ninput = 1;\r\nif (input == dbs_tuners_ins.ignore_nice)\r\nreturn count;\r\ndbs_tuners_ins.ignore_nice = input;\r\nfor_each_online_cpu(j) {\r\nstruct cpu_dbs_info_s *dbs_info;\r\ndbs_info = &per_cpu(cs_cpu_dbs_info, j);\r\ndbs_info->prev_cpu_idle = get_cpu_idle_time(j,\r\n&dbs_info->prev_cpu_wall);\r\nif (dbs_tuners_ins.ignore_nice)\r\ndbs_info->prev_cpu_nice = kstat_cpu(j).cpustat.nice;\r\n}\r\nreturn count;\r\n}\r\nstatic ssize_t store_freq_step(struct kobject *a, struct attribute *b,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned int input;\r\nint ret;\r\nret = sscanf(buf, "%u", &input);\r\nif (ret != 1)\r\nreturn -EINVAL;\r\nif (input > 100)\r\ninput = 100;\r\ndbs_tuners_ins.freq_step = input;\r\nreturn count;\r\n}\r\nstatic void dbs_check_cpu(struct cpu_dbs_info_s *this_dbs_info)\r\n{\r\nunsigned int load = 0;\r\nunsigned int max_load = 0;\r\nunsigned int freq_target;\r\nstruct cpufreq_policy *policy;\r\nunsigned int j;\r\npolicy = this_dbs_info->cur_policy;\r\nfor_each_cpu(j, policy->cpus) {\r\nstruct cpu_dbs_info_s *j_dbs_info;\r\ncputime64_t cur_wall_time, cur_idle_time;\r\nunsigned int idle_time, wall_time;\r\nj_dbs_info = &per_cpu(cs_cpu_dbs_info, j);\r\ncur_idle_time = get_cpu_idle_time(j, &cur_wall_time);\r\nwall_time = (unsigned int) cputime64_sub(cur_wall_time,\r\nj_dbs_info->prev_cpu_wall);\r\nj_dbs_info->prev_cpu_wall = cur_wall_time;\r\nidle_time = (unsigned int) cputime64_sub(cur_idle_time,\r\nj_dbs_info->prev_cpu_idle);\r\nj_dbs_info->prev_cpu_idle = cur_idle_time;\r\nif (dbs_tuners_ins.ignore_nice) {\r\ncputime64_t cur_nice;\r\nunsigned long cur_nice_jiffies;\r\ncur_nice = cputime64_sub(kstat_cpu(j).cpustat.nice,\r\nj_dbs_info->prev_cpu_nice);\r\ncur_nice_jiffies = (unsigned long)\r\ncputime64_to_jiffies64(cur_nice);\r\nj_dbs_info->prev_cpu_nice = kstat_cpu(j).cpustat.nice;\r\nidle_time += jiffies_to_usecs(cur_nice_jiffies);\r\n}\r\nif (unlikely(!wall_time || wall_time < idle_time))\r\ncontinue;\r\nload = 100 * (wall_time - idle_time) / wall_time;\r\nif (load > max_load)\r\nmax_load = load;\r\n}\r\nif (dbs_tuners_ins.freq_step == 0)\r\nreturn;\r\nif (max_load > dbs_tuners_ins.up_threshold) {\r\nthis_dbs_info->down_skip = 0;\r\nif (this_dbs_info->requested_freq == policy->max)\r\nreturn;\r\nfreq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;\r\nif (unlikely(freq_target == 0))\r\nfreq_target = 5;\r\nthis_dbs_info->requested_freq += freq_target;\r\nif (this_dbs_info->requested_freq > policy->max)\r\nthis_dbs_info->requested_freq = policy->max;\r\n__cpufreq_driver_target(policy, this_dbs_info->requested_freq,\r\nCPUFREQ_RELATION_H);\r\nreturn;\r\n}\r\nif (max_load < (dbs_tuners_ins.down_threshold - 10)) {\r\nfreq_target = (dbs_tuners_ins.freq_step * policy->max) / 100;\r\nthis_dbs_info->requested_freq -= freq_target;\r\nif (this_dbs_info->requested_freq < policy->min)\r\nthis_dbs_info->requested_freq = policy->min;\r\nif (policy->cur == policy->min)\r\nreturn;\r\n__cpufreq_driver_target(policy, this_dbs_info->requested_freq,\r\nCPUFREQ_RELATION_H);\r\nreturn;\r\n}\r\n}\r\nstatic void do_dbs_timer(struct work_struct *work)\r\n{\r\nstruct cpu_dbs_info_s *dbs_info =\r\ncontainer_of(work, struct cpu_dbs_info_s, work.work);\r\nunsigned int cpu = dbs_info->cpu;\r\nint delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);\r\ndelay -= jiffies % delay;\r\nmutex_lock(&dbs_info->timer_mutex);\r\ndbs_check_cpu(dbs_info);\r\nschedule_delayed_work_on(cpu, &dbs_info->work, delay);\r\nmutex_unlock(&dbs_info->timer_mutex);\r\n}\r\nstatic inline void dbs_timer_init(struct cpu_dbs_info_s *dbs_info)\r\n{\r\nint delay = usecs_to_jiffies(dbs_tuners_ins.sampling_rate);\r\ndelay -= jiffies % delay;\r\ndbs_info->enable = 1;\r\nINIT_DELAYED_WORK_DEFERRABLE(&dbs_info->work, do_dbs_timer);\r\nschedule_delayed_work_on(dbs_info->cpu, &dbs_info->work, delay);\r\n}\r\nstatic inline void dbs_timer_exit(struct cpu_dbs_info_s *dbs_info)\r\n{\r\ndbs_info->enable = 0;\r\ncancel_delayed_work_sync(&dbs_info->work);\r\n}\r\nstatic int cpufreq_governor_dbs(struct cpufreq_policy *policy,\r\nunsigned int event)\r\n{\r\nunsigned int cpu = policy->cpu;\r\nstruct cpu_dbs_info_s *this_dbs_info;\r\nunsigned int j;\r\nint rc;\r\nthis_dbs_info = &per_cpu(cs_cpu_dbs_info, cpu);\r\nswitch (event) {\r\ncase CPUFREQ_GOV_START:\r\nif ((!cpu_online(cpu)) || (!policy->cur))\r\nreturn -EINVAL;\r\nmutex_lock(&dbs_mutex);\r\nfor_each_cpu(j, policy->cpus) {\r\nstruct cpu_dbs_info_s *j_dbs_info;\r\nj_dbs_info = &per_cpu(cs_cpu_dbs_info, j);\r\nj_dbs_info->cur_policy = policy;\r\nj_dbs_info->prev_cpu_idle = get_cpu_idle_time(j,\r\n&j_dbs_info->prev_cpu_wall);\r\nif (dbs_tuners_ins.ignore_nice) {\r\nj_dbs_info->prev_cpu_nice =\r\nkstat_cpu(j).cpustat.nice;\r\n}\r\n}\r\nthis_dbs_info->down_skip = 0;\r\nthis_dbs_info->requested_freq = policy->cur;\r\nmutex_init(&this_dbs_info->timer_mutex);\r\ndbs_enable++;\r\nif (dbs_enable == 1) {\r\nunsigned int latency;\r\nlatency = policy->cpuinfo.transition_latency / 1000;\r\nif (latency == 0)\r\nlatency = 1;\r\nrc = sysfs_create_group(cpufreq_global_kobject,\r\n&dbs_attr_group);\r\nif (rc) {\r\nmutex_unlock(&dbs_mutex);\r\nreturn rc;\r\n}\r\nmin_sampling_rate =\r\nMIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(10);\r\nmin_sampling_rate = max(min_sampling_rate,\r\nMIN_LATENCY_MULTIPLIER * latency);\r\ndbs_tuners_ins.sampling_rate =\r\nmax(min_sampling_rate,\r\nlatency * LATENCY_MULTIPLIER);\r\ncpufreq_register_notifier(\r\n&dbs_cpufreq_notifier_block,\r\nCPUFREQ_TRANSITION_NOTIFIER);\r\n}\r\nmutex_unlock(&dbs_mutex);\r\ndbs_timer_init(this_dbs_info);\r\nbreak;\r\ncase CPUFREQ_GOV_STOP:\r\ndbs_timer_exit(this_dbs_info);\r\nmutex_lock(&dbs_mutex);\r\ndbs_enable--;\r\nmutex_destroy(&this_dbs_info->timer_mutex);\r\nif (dbs_enable == 0)\r\ncpufreq_unregister_notifier(\r\n&dbs_cpufreq_notifier_block,\r\nCPUFREQ_TRANSITION_NOTIFIER);\r\nmutex_unlock(&dbs_mutex);\r\nif (!dbs_enable)\r\nsysfs_remove_group(cpufreq_global_kobject,\r\n&dbs_attr_group);\r\nbreak;\r\ncase CPUFREQ_GOV_LIMITS:\r\nmutex_lock(&this_dbs_info->timer_mutex);\r\nif (policy->max < this_dbs_info->cur_policy->cur)\r\n__cpufreq_driver_target(\r\nthis_dbs_info->cur_policy,\r\npolicy->max, CPUFREQ_RELATION_H);\r\nelse if (policy->min > this_dbs_info->cur_policy->cur)\r\n__cpufreq_driver_target(\r\nthis_dbs_info->cur_policy,\r\npolicy->min, CPUFREQ_RELATION_L);\r\nmutex_unlock(&this_dbs_info->timer_mutex);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init cpufreq_gov_dbs_init(void)\r\n{\r\nreturn cpufreq_register_governor(&cpufreq_gov_conservative);\r\n}\r\nstatic void __exit cpufreq_gov_dbs_exit(void)\r\n{\r\ncpufreq_unregister_governor(&cpufreq_gov_conservative);\r\n}
