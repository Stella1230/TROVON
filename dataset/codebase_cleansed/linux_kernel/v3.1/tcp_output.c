static void tcp_event_new_data_sent(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nunsigned int prior_packets = tp->packets_out;\r\ntcp_advance_send_head(sk, skb);\r\ntp->snd_nxt = TCP_SKB_CB(skb)->end_seq;\r\nif (tp->frto_counter == 2)\r\ntp->frto_counter = 3;\r\ntp->packets_out += tcp_skb_pcount(skb);\r\nif (!prior_packets)\r\ninet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\r\ninet_csk(sk)->icsk_rto, TCP_RTO_MAX);\r\n}\r\nstatic inline __u32 tcp_acceptable_seq(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nif (!before(tcp_wnd_end(tp), tp->snd_nxt))\r\nreturn tp->snd_nxt;\r\nelse\r\nreturn tcp_wnd_end(tp);\r\n}\r\nstatic __u16 tcp_advertise_mss(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct dst_entry *dst = __sk_dst_get(sk);\r\nint mss = tp->advmss;\r\nif (dst) {\r\nunsigned int metric = dst_metric_advmss(dst);\r\nif (metric < mss) {\r\nmss = metric;\r\ntp->advmss = mss;\r\n}\r\n}\r\nreturn (__u16)mss;\r\n}\r\nstatic void tcp_cwnd_restart(struct sock *sk, struct dst_entry *dst)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\ns32 delta = tcp_time_stamp - tp->lsndtime;\r\nu32 restart_cwnd = tcp_init_cwnd(tp, dst);\r\nu32 cwnd = tp->snd_cwnd;\r\ntcp_ca_event(sk, CA_EVENT_CWND_RESTART);\r\ntp->snd_ssthresh = tcp_current_ssthresh(sk);\r\nrestart_cwnd = min(restart_cwnd, cwnd);\r\nwhile ((delta -= inet_csk(sk)->icsk_rto) > 0 && cwnd > restart_cwnd)\r\ncwnd >>= 1;\r\ntp->snd_cwnd = max(cwnd, restart_cwnd);\r\ntp->snd_cwnd_stamp = tcp_time_stamp;\r\ntp->snd_cwnd_used = 0;\r\n}\r\nstatic void tcp_event_data_sent(struct tcp_sock *tp,\r\nstruct sk_buff *skb, struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nconst u32 now = tcp_time_stamp;\r\nif (sysctl_tcp_slow_start_after_idle &&\r\n(!tp->packets_out && (s32)(now - tp->lsndtime) > icsk->icsk_rto))\r\ntcp_cwnd_restart(sk, __sk_dst_get(sk));\r\ntp->lsndtime = now;\r\nif ((u32)(now - icsk->icsk_ack.lrcvtime) < icsk->icsk_ack.ato)\r\nicsk->icsk_ack.pingpong = 1;\r\n}\r\nstatic inline void tcp_event_ack_sent(struct sock *sk, unsigned int pkts)\r\n{\r\ntcp_dec_quickack_mode(sk, pkts);\r\ninet_csk_clear_xmit_timer(sk, ICSK_TIME_DACK);\r\n}\r\nvoid tcp_select_initial_window(int __space, __u32 mss,\r\n__u32 *rcv_wnd, __u32 *window_clamp,\r\nint wscale_ok, __u8 *rcv_wscale,\r\n__u32 init_rcv_wnd)\r\n{\r\nunsigned int space = (__space < 0 ? 0 : __space);\r\nif (*window_clamp == 0)\r\n(*window_clamp) = (65535 << 14);\r\nspace = min(*window_clamp, space);\r\nif (space > mss)\r\nspace = (space / mss) * mss;\r\nif (sysctl_tcp_workaround_signed_windows)\r\n(*rcv_wnd) = min(space, MAX_TCP_WINDOW);\r\nelse\r\n(*rcv_wnd) = space;\r\n(*rcv_wscale) = 0;\r\nif (wscale_ok) {\r\nspace = max_t(u32, sysctl_tcp_rmem[2], sysctl_rmem_max);\r\nspace = min_t(u32, space, *window_clamp);\r\nwhile (space > 65535 && (*rcv_wscale) < 14) {\r\nspace >>= 1;\r\n(*rcv_wscale)++;\r\n}\r\n}\r\nif (mss > (1 << *rcv_wscale)) {\r\nint init_cwnd = TCP_DEFAULT_INIT_RCVWND;\r\nif (mss > 1460)\r\ninit_cwnd =\r\nmax_t(u32, (1460 * TCP_DEFAULT_INIT_RCVWND) / mss, 2);\r\nif (init_rcv_wnd)\r\n*rcv_wnd = min(*rcv_wnd, init_rcv_wnd * mss);\r\nelse\r\n*rcv_wnd = min(*rcv_wnd, init_cwnd * mss);\r\n}\r\n(*window_clamp) = min(65535U << (*rcv_wscale), *window_clamp);\r\n}\r\nstatic u16 tcp_select_window(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nu32 cur_win = tcp_receive_window(tp);\r\nu32 new_win = __tcp_select_window(sk);\r\nif (new_win < cur_win) {\r\nnew_win = ALIGN(cur_win, 1 << tp->rx_opt.rcv_wscale);\r\n}\r\ntp->rcv_wnd = new_win;\r\ntp->rcv_wup = tp->rcv_nxt;\r\nif (!tp->rx_opt.rcv_wscale && sysctl_tcp_workaround_signed_windows)\r\nnew_win = min(new_win, MAX_TCP_WINDOW);\r\nelse\r\nnew_win = min(new_win, (65535U << tp->rx_opt.rcv_wscale));\r\nnew_win >>= tp->rx_opt.rcv_wscale;\r\nif (new_win == 0)\r\ntp->pred_flags = 0;\r\nreturn new_win;\r\n}\r\nstatic inline void TCP_ECN_send_synack(struct tcp_sock *tp, struct sk_buff *skb)\r\n{\r\nTCP_SKB_CB(skb)->flags &= ~TCPHDR_CWR;\r\nif (!(tp->ecn_flags & TCP_ECN_OK))\r\nTCP_SKB_CB(skb)->flags &= ~TCPHDR_ECE;\r\n}\r\nstatic inline void TCP_ECN_send_syn(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\ntp->ecn_flags = 0;\r\nif (sysctl_tcp_ecn == 1) {\r\nTCP_SKB_CB(skb)->flags |= TCPHDR_ECE | TCPHDR_CWR;\r\ntp->ecn_flags = TCP_ECN_OK;\r\n}\r\n}\r\nstatic __inline__ void\r\nTCP_ECN_make_synack(struct request_sock *req, struct tcphdr *th)\r\n{\r\nif (inet_rsk(req)->ecn_ok)\r\nth->ece = 1;\r\n}\r\nstatic inline void TCP_ECN_send(struct sock *sk, struct sk_buff *skb,\r\nint tcp_header_len)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nif (tp->ecn_flags & TCP_ECN_OK) {\r\nif (skb->len != tcp_header_len &&\r\n!before(TCP_SKB_CB(skb)->seq, tp->snd_nxt)) {\r\nINET_ECN_xmit(sk);\r\nif (tp->ecn_flags & TCP_ECN_QUEUE_CWR) {\r\ntp->ecn_flags &= ~TCP_ECN_QUEUE_CWR;\r\ntcp_hdr(skb)->cwr = 1;\r\nskb_shinfo(skb)->gso_type |= SKB_GSO_TCP_ECN;\r\n}\r\n} else {\r\nINET_ECN_dontxmit(sk);\r\n}\r\nif (tp->ecn_flags & TCP_ECN_DEMAND_CWR)\r\ntcp_hdr(skb)->ece = 1;\r\n}\r\n}\r\nstatic void tcp_init_nondata_skb(struct sk_buff *skb, u32 seq, u8 flags)\r\n{\r\nskb->ip_summed = CHECKSUM_PARTIAL;\r\nskb->csum = 0;\r\nTCP_SKB_CB(skb)->flags = flags;\r\nTCP_SKB_CB(skb)->sacked = 0;\r\nskb_shinfo(skb)->gso_segs = 1;\r\nskb_shinfo(skb)->gso_size = 0;\r\nskb_shinfo(skb)->gso_type = 0;\r\nTCP_SKB_CB(skb)->seq = seq;\r\nif (flags & (TCPHDR_SYN | TCPHDR_FIN))\r\nseq++;\r\nTCP_SKB_CB(skb)->end_seq = seq;\r\n}\r\nstatic inline int tcp_urg_mode(const struct tcp_sock *tp)\r\n{\r\nreturn tp->snd_una != tp->snd_up;\r\n}\r\nstatic u8 tcp_cookie_size_check(u8 desired)\r\n{\r\nint cookie_size;\r\nif (desired > 0)\r\nreturn desired;\r\ncookie_size = ACCESS_ONCE(sysctl_tcp_cookie_size);\r\nif (cookie_size <= 0)\r\nreturn 0;\r\nif (cookie_size <= TCP_COOKIE_MIN)\r\nreturn TCP_COOKIE_MIN;\r\nif (cookie_size >= TCP_COOKIE_MAX)\r\nreturn TCP_COOKIE_MAX;\r\nif (cookie_size & 1)\r\ncookie_size++;\r\nreturn (u8)cookie_size;\r\n}\r\nstatic void tcp_options_write(__be32 *ptr, struct tcp_sock *tp,\r\nstruct tcp_out_options *opts)\r\n{\r\nu8 options = opts->options;\r\nif (unlikely(OPTION_MD5 & options)) {\r\nif (unlikely(OPTION_COOKIE_EXTENSION & options)) {\r\n*ptr++ = htonl((TCPOPT_COOKIE << 24) |\r\n(TCPOLEN_COOKIE_BASE << 16) |\r\n(TCPOPT_MD5SIG << 8) |\r\nTCPOLEN_MD5SIG);\r\n} else {\r\n*ptr++ = htonl((TCPOPT_NOP << 24) |\r\n(TCPOPT_NOP << 16) |\r\n(TCPOPT_MD5SIG << 8) |\r\nTCPOLEN_MD5SIG);\r\n}\r\noptions &= ~OPTION_COOKIE_EXTENSION;\r\nopts->hash_location = (__u8 *)ptr;\r\nptr += 4;\r\n}\r\nif (unlikely(opts->mss)) {\r\n*ptr++ = htonl((TCPOPT_MSS << 24) |\r\n(TCPOLEN_MSS << 16) |\r\nopts->mss);\r\n}\r\nif (likely(OPTION_TS & options)) {\r\nif (unlikely(OPTION_SACK_ADVERTISE & options)) {\r\n*ptr++ = htonl((TCPOPT_SACK_PERM << 24) |\r\n(TCPOLEN_SACK_PERM << 16) |\r\n(TCPOPT_TIMESTAMP << 8) |\r\nTCPOLEN_TIMESTAMP);\r\noptions &= ~OPTION_SACK_ADVERTISE;\r\n} else {\r\n*ptr++ = htonl((TCPOPT_NOP << 24) |\r\n(TCPOPT_NOP << 16) |\r\n(TCPOPT_TIMESTAMP << 8) |\r\nTCPOLEN_TIMESTAMP);\r\n}\r\n*ptr++ = htonl(opts->tsval);\r\n*ptr++ = htonl(opts->tsecr);\r\n}\r\nif (unlikely(OPTION_COOKIE_EXTENSION & options)) {\r\n__u8 *cookie_copy = opts->hash_location;\r\nu8 cookie_size = opts->hash_size;\r\nif (0x2 & cookie_size) {\r\n__u8 *p = (__u8 *)ptr;\r\n*p++ = TCPOPT_COOKIE;\r\n*p++ = TCPOLEN_COOKIE_BASE + cookie_size;\r\n*p++ = *cookie_copy++;\r\n*p++ = *cookie_copy++;\r\nptr++;\r\ncookie_size -= 2;\r\n} else {\r\n*ptr++ = htonl(((TCPOPT_NOP << 24) |\r\n(TCPOPT_NOP << 16) |\r\n(TCPOPT_COOKIE << 8) |\r\nTCPOLEN_COOKIE_BASE) +\r\ncookie_size);\r\n}\r\nif (cookie_size > 0) {\r\nmemcpy(ptr, cookie_copy, cookie_size);\r\nptr += (cookie_size / 4);\r\n}\r\n}\r\nif (unlikely(OPTION_SACK_ADVERTISE & options)) {\r\n*ptr++ = htonl((TCPOPT_NOP << 24) |\r\n(TCPOPT_NOP << 16) |\r\n(TCPOPT_SACK_PERM << 8) |\r\nTCPOLEN_SACK_PERM);\r\n}\r\nif (unlikely(OPTION_WSCALE & options)) {\r\n*ptr++ = htonl((TCPOPT_NOP << 24) |\r\n(TCPOPT_WINDOW << 16) |\r\n(TCPOLEN_WINDOW << 8) |\r\nopts->ws);\r\n}\r\nif (unlikely(opts->num_sack_blocks)) {\r\nstruct tcp_sack_block *sp = tp->rx_opt.dsack ?\r\ntp->duplicate_sack : tp->selective_acks;\r\nint this_sack;\r\n*ptr++ = htonl((TCPOPT_NOP << 24) |\r\n(TCPOPT_NOP << 16) |\r\n(TCPOPT_SACK << 8) |\r\n(TCPOLEN_SACK_BASE + (opts->num_sack_blocks *\r\nTCPOLEN_SACK_PERBLOCK)));\r\nfor (this_sack = 0; this_sack < opts->num_sack_blocks;\r\n++this_sack) {\r\n*ptr++ = htonl(sp[this_sack].start_seq);\r\n*ptr++ = htonl(sp[this_sack].end_seq);\r\n}\r\ntp->rx_opt.dsack = 0;\r\n}\r\n}\r\nstatic unsigned tcp_syn_options(struct sock *sk, struct sk_buff *skb,\r\nstruct tcp_out_options *opts,\r\nstruct tcp_md5sig_key **md5) {\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct tcp_cookie_values *cvp = tp->cookie_values;\r\nunsigned remaining = MAX_TCP_OPTION_SPACE;\r\nu8 cookie_size = (!tp->rx_opt.cookie_out_never && cvp != NULL) ?\r\ntcp_cookie_size_check(cvp->cookie_desired) :\r\n0;\r\n#ifdef CONFIG_TCP_MD5SIG\r\n*md5 = tp->af_specific->md5_lookup(sk, sk);\r\nif (*md5) {\r\nopts->options |= OPTION_MD5;\r\nremaining -= TCPOLEN_MD5SIG_ALIGNED;\r\n}\r\n#else\r\n*md5 = NULL;\r\n#endif\r\nopts->mss = tcp_advertise_mss(sk);\r\nremaining -= TCPOLEN_MSS_ALIGNED;\r\nif (likely(sysctl_tcp_timestamps && *md5 == NULL)) {\r\nopts->options |= OPTION_TS;\r\nopts->tsval = TCP_SKB_CB(skb)->when;\r\nopts->tsecr = tp->rx_opt.ts_recent;\r\nremaining -= TCPOLEN_TSTAMP_ALIGNED;\r\n}\r\nif (likely(sysctl_tcp_window_scaling)) {\r\nopts->ws = tp->rx_opt.rcv_wscale;\r\nopts->options |= OPTION_WSCALE;\r\nremaining -= TCPOLEN_WSCALE_ALIGNED;\r\n}\r\nif (likely(sysctl_tcp_sack)) {\r\nopts->options |= OPTION_SACK_ADVERTISE;\r\nif (unlikely(!(OPTION_TS & opts->options)))\r\nremaining -= TCPOLEN_SACKPERM_ALIGNED;\r\n}\r\nif (*md5 == NULL &&\r\n(OPTION_TS & opts->options) &&\r\ncookie_size > 0) {\r\nint need = TCPOLEN_COOKIE_BASE + cookie_size;\r\nif (0x2 & need) {\r\nneed += 2;\r\nif (need > remaining) {\r\ncookie_size -= 2;\r\nneed -= 4;\r\n}\r\n}\r\nwhile (need > remaining && TCP_COOKIE_MIN <= cookie_size) {\r\ncookie_size -= 4;\r\nneed -= 4;\r\n}\r\nif (TCP_COOKIE_MIN <= cookie_size) {\r\nopts->options |= OPTION_COOKIE_EXTENSION;\r\nopts->hash_location = (__u8 *)&cvp->cookie_pair[0];\r\nopts->hash_size = cookie_size;\r\ncvp->cookie_desired = cookie_size;\r\nif (cvp->cookie_desired != cvp->cookie_pair_size) {\r\nget_random_bytes(&cvp->cookie_pair[0],\r\ncookie_size);\r\ncvp->cookie_pair_size = cookie_size;\r\n}\r\nremaining -= need;\r\n}\r\n}\r\nreturn MAX_TCP_OPTION_SPACE - remaining;\r\n}\r\nstatic unsigned tcp_synack_options(struct sock *sk,\r\nstruct request_sock *req,\r\nunsigned mss, struct sk_buff *skb,\r\nstruct tcp_out_options *opts,\r\nstruct tcp_md5sig_key **md5,\r\nstruct tcp_extend_values *xvp)\r\n{\r\nstruct inet_request_sock *ireq = inet_rsk(req);\r\nunsigned remaining = MAX_TCP_OPTION_SPACE;\r\nu8 cookie_plus = (xvp != NULL && !xvp->cookie_out_never) ?\r\nxvp->cookie_plus :\r\n0;\r\n#ifdef CONFIG_TCP_MD5SIG\r\n*md5 = tcp_rsk(req)->af_specific->md5_lookup(sk, req);\r\nif (*md5) {\r\nopts->options |= OPTION_MD5;\r\nremaining -= TCPOLEN_MD5SIG_ALIGNED;\r\nireq->tstamp_ok &= !ireq->sack_ok;\r\n}\r\n#else\r\n*md5 = NULL;\r\n#endif\r\nopts->mss = mss;\r\nremaining -= TCPOLEN_MSS_ALIGNED;\r\nif (likely(ireq->wscale_ok)) {\r\nopts->ws = ireq->rcv_wscale;\r\nopts->options |= OPTION_WSCALE;\r\nremaining -= TCPOLEN_WSCALE_ALIGNED;\r\n}\r\nif (likely(ireq->tstamp_ok)) {\r\nopts->options |= OPTION_TS;\r\nopts->tsval = TCP_SKB_CB(skb)->when;\r\nopts->tsecr = req->ts_recent;\r\nremaining -= TCPOLEN_TSTAMP_ALIGNED;\r\n}\r\nif (likely(ireq->sack_ok)) {\r\nopts->options |= OPTION_SACK_ADVERTISE;\r\nif (unlikely(!ireq->tstamp_ok))\r\nremaining -= TCPOLEN_SACKPERM_ALIGNED;\r\n}\r\nif (*md5 == NULL &&\r\nireq->tstamp_ok &&\r\ncookie_plus > TCPOLEN_COOKIE_BASE) {\r\nint need = cookie_plus;\r\nif (0x2 & need) {\r\nneed += 2;\r\n}\r\nif (need <= remaining) {\r\nopts->options |= OPTION_COOKIE_EXTENSION;\r\nopts->hash_size = cookie_plus - TCPOLEN_COOKIE_BASE;\r\nremaining -= need;\r\n} else {\r\nxvp->cookie_out_never = 1;\r\nopts->hash_size = 0;\r\n}\r\n}\r\nreturn MAX_TCP_OPTION_SPACE - remaining;\r\n}\r\nstatic unsigned tcp_established_options(struct sock *sk, struct sk_buff *skb,\r\nstruct tcp_out_options *opts,\r\nstruct tcp_md5sig_key **md5) {\r\nstruct tcp_skb_cb *tcb = skb ? TCP_SKB_CB(skb) : NULL;\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nunsigned size = 0;\r\nunsigned int eff_sacks;\r\n#ifdef CONFIG_TCP_MD5SIG\r\n*md5 = tp->af_specific->md5_lookup(sk, sk);\r\nif (unlikely(*md5)) {\r\nopts->options |= OPTION_MD5;\r\nsize += TCPOLEN_MD5SIG_ALIGNED;\r\n}\r\n#else\r\n*md5 = NULL;\r\n#endif\r\nif (likely(tp->rx_opt.tstamp_ok)) {\r\nopts->options |= OPTION_TS;\r\nopts->tsval = tcb ? tcb->when : 0;\r\nopts->tsecr = tp->rx_opt.ts_recent;\r\nsize += TCPOLEN_TSTAMP_ALIGNED;\r\n}\r\neff_sacks = tp->rx_opt.num_sacks + tp->rx_opt.dsack;\r\nif (unlikely(eff_sacks)) {\r\nconst unsigned remaining = MAX_TCP_OPTION_SPACE - size;\r\nopts->num_sack_blocks =\r\nmin_t(unsigned, eff_sacks,\r\n(remaining - TCPOLEN_SACK_BASE_ALIGNED) /\r\nTCPOLEN_SACK_PERBLOCK);\r\nsize += TCPOLEN_SACK_BASE_ALIGNED +\r\nopts->num_sack_blocks * TCPOLEN_SACK_PERBLOCK;\r\n}\r\nreturn size;\r\n}\r\nstatic int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb, int clone_it,\r\ngfp_t gfp_mask)\r\n{\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct inet_sock *inet;\r\nstruct tcp_sock *tp;\r\nstruct tcp_skb_cb *tcb;\r\nstruct tcp_out_options opts;\r\nunsigned tcp_options_size, tcp_header_size;\r\nstruct tcp_md5sig_key *md5;\r\nstruct tcphdr *th;\r\nint err;\r\nBUG_ON(!skb || !tcp_skb_pcount(skb));\r\nif (icsk->icsk_ca_ops->flags & TCP_CONG_RTT_STAMP)\r\n__net_timestamp(skb);\r\nif (likely(clone_it)) {\r\nif (unlikely(skb_cloned(skb)))\r\nskb = pskb_copy(skb, gfp_mask);\r\nelse\r\nskb = skb_clone(skb, gfp_mask);\r\nif (unlikely(!skb))\r\nreturn -ENOBUFS;\r\n}\r\ninet = inet_sk(sk);\r\ntp = tcp_sk(sk);\r\ntcb = TCP_SKB_CB(skb);\r\nmemset(&opts, 0, sizeof(opts));\r\nif (unlikely(tcb->flags & TCPHDR_SYN))\r\ntcp_options_size = tcp_syn_options(sk, skb, &opts, &md5);\r\nelse\r\ntcp_options_size = tcp_established_options(sk, skb, &opts,\r\n&md5);\r\ntcp_header_size = tcp_options_size + sizeof(struct tcphdr);\r\nif (tcp_packets_in_flight(tp) == 0) {\r\ntcp_ca_event(sk, CA_EVENT_TX_START);\r\nskb->ooo_okay = 1;\r\n} else\r\nskb->ooo_okay = 0;\r\nskb_push(skb, tcp_header_size);\r\nskb_reset_transport_header(skb);\r\nskb_set_owner_w(skb, sk);\r\nth = tcp_hdr(skb);\r\nth->source = inet->inet_sport;\r\nth->dest = inet->inet_dport;\r\nth->seq = htonl(tcb->seq);\r\nth->ack_seq = htonl(tp->rcv_nxt);\r\n*(((__be16 *)th) + 6) = htons(((tcp_header_size >> 2) << 12) |\r\ntcb->flags);\r\nif (unlikely(tcb->flags & TCPHDR_SYN)) {\r\nth->window = htons(min(tp->rcv_wnd, 65535U));\r\n} else {\r\nth->window = htons(tcp_select_window(sk));\r\n}\r\nth->check = 0;\r\nth->urg_ptr = 0;\r\nif (unlikely(tcp_urg_mode(tp) && before(tcb->seq, tp->snd_up))) {\r\nif (before(tp->snd_up, tcb->seq + 0x10000)) {\r\nth->urg_ptr = htons(tp->snd_up - tcb->seq);\r\nth->urg = 1;\r\n} else if (after(tcb->seq + 0xFFFF, tp->snd_nxt)) {\r\nth->urg_ptr = htons(0xFFFF);\r\nth->urg = 1;\r\n}\r\n}\r\ntcp_options_write((__be32 *)(th + 1), tp, &opts);\r\nif (likely((tcb->flags & TCPHDR_SYN) == 0))\r\nTCP_ECN_send(sk, skb, tcp_header_size);\r\n#ifdef CONFIG_TCP_MD5SIG\r\nif (md5) {\r\nsk_nocaps_add(sk, NETIF_F_GSO_MASK);\r\ntp->af_specific->calc_md5_hash(opts.hash_location,\r\nmd5, sk, NULL, skb);\r\n}\r\n#endif\r\nicsk->icsk_af_ops->send_check(sk, skb);\r\nif (likely(tcb->flags & TCPHDR_ACK))\r\ntcp_event_ack_sent(sk, tcp_skb_pcount(skb));\r\nif (skb->len != tcp_header_size)\r\ntcp_event_data_sent(tp, skb, sk);\r\nif (after(tcb->end_seq, tp->snd_nxt) || tcb->seq == tcb->end_seq)\r\nTCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS,\r\ntcp_skb_pcount(skb));\r\nerr = icsk->icsk_af_ops->queue_xmit(skb, &inet->cork.fl);\r\nif (likely(err <= 0))\r\nreturn err;\r\ntcp_enter_cwr(sk, 1);\r\nreturn net_xmit_eval(err);\r\n}\r\nstatic void tcp_queue_skb(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\ntp->write_seq = TCP_SKB_CB(skb)->end_seq;\r\nskb_header_release(skb);\r\ntcp_add_write_queue_tail(sk, skb);\r\nsk->sk_wmem_queued += skb->truesize;\r\nsk_mem_charge(sk, skb->truesize);\r\n}\r\nstatic void tcp_set_skb_tso_segs(struct sock *sk, struct sk_buff *skb,\r\nunsigned int mss_now)\r\n{\r\nif (skb->len <= mss_now || !sk_can_gso(sk) ||\r\nskb->ip_summed == CHECKSUM_NONE) {\r\nskb_shinfo(skb)->gso_segs = 1;\r\nskb_shinfo(skb)->gso_size = 0;\r\nskb_shinfo(skb)->gso_type = 0;\r\n} else {\r\nskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss_now);\r\nskb_shinfo(skb)->gso_size = mss_now;\r\nskb_shinfo(skb)->gso_type = sk->sk_gso_type;\r\n}\r\n}\r\nstatic void tcp_adjust_fackets_out(struct sock *sk, struct sk_buff *skb,\r\nint decr)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nif (!tp->sacked_out || tcp_is_reno(tp))\r\nreturn;\r\nif (after(tcp_highest_sack_seq(tp), TCP_SKB_CB(skb)->seq))\r\ntp->fackets_out -= decr;\r\n}\r\nstatic void tcp_adjust_pcount(struct sock *sk, struct sk_buff *skb, int decr)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\ntp->packets_out -= decr;\r\nif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\r\ntp->sacked_out -= decr;\r\nif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS)\r\ntp->retrans_out -= decr;\r\nif (TCP_SKB_CB(skb)->sacked & TCPCB_LOST)\r\ntp->lost_out -= decr;\r\nif (tcp_is_reno(tp) && decr > 0)\r\ntp->sacked_out -= min_t(u32, tp->sacked_out, decr);\r\ntcp_adjust_fackets_out(sk, skb, decr);\r\nif (tp->lost_skb_hint &&\r\nbefore(TCP_SKB_CB(skb)->seq, TCP_SKB_CB(tp->lost_skb_hint)->seq) &&\r\n(tcp_is_fack(tp) || (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)))\r\ntp->lost_cnt_hint -= decr;\r\ntcp_verify_left_out(tp);\r\n}\r\nint tcp_fragment(struct sock *sk, struct sk_buff *skb, u32 len,\r\nunsigned int mss_now)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *buff;\r\nint nsize, old_factor;\r\nint nlen;\r\nu8 flags;\r\nif (WARN_ON(len > skb->len))\r\nreturn -EINVAL;\r\nnsize = skb_headlen(skb) - len;\r\nif (nsize < 0)\r\nnsize = 0;\r\nif (skb_cloned(skb) &&\r\nskb_is_nonlinear(skb) &&\r\npskb_expand_head(skb, 0, 0, GFP_ATOMIC))\r\nreturn -ENOMEM;\r\nbuff = sk_stream_alloc_skb(sk, nsize, GFP_ATOMIC);\r\nif (buff == NULL)\r\nreturn -ENOMEM;\r\nsk->sk_wmem_queued += buff->truesize;\r\nsk_mem_charge(sk, buff->truesize);\r\nnlen = skb->len - len - nsize;\r\nbuff->truesize += nlen;\r\nskb->truesize -= nlen;\r\nTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\r\nTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\r\nTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\r\nflags = TCP_SKB_CB(skb)->flags;\r\nTCP_SKB_CB(skb)->flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\r\nTCP_SKB_CB(buff)->flags = flags;\r\nTCP_SKB_CB(buff)->sacked = TCP_SKB_CB(skb)->sacked;\r\nif (!skb_shinfo(skb)->nr_frags && skb->ip_summed != CHECKSUM_PARTIAL) {\r\nbuff->csum = csum_partial_copy_nocheck(skb->data + len,\r\nskb_put(buff, nsize),\r\nnsize, 0);\r\nskb_trim(skb, len);\r\nskb->csum = csum_block_sub(skb->csum, buff->csum, len);\r\n} else {\r\nskb->ip_summed = CHECKSUM_PARTIAL;\r\nskb_split(skb, buff, len);\r\n}\r\nbuff->ip_summed = skb->ip_summed;\r\nTCP_SKB_CB(buff)->when = TCP_SKB_CB(skb)->when;\r\nbuff->tstamp = skb->tstamp;\r\nold_factor = tcp_skb_pcount(skb);\r\ntcp_set_skb_tso_segs(sk, skb, mss_now);\r\ntcp_set_skb_tso_segs(sk, buff, mss_now);\r\nif (!before(tp->snd_nxt, TCP_SKB_CB(buff)->end_seq)) {\r\nint diff = old_factor - tcp_skb_pcount(skb) -\r\ntcp_skb_pcount(buff);\r\nif (diff)\r\ntcp_adjust_pcount(sk, skb, diff);\r\n}\r\nskb_header_release(buff);\r\ntcp_insert_write_queue_after(skb, buff, sk);\r\nreturn 0;\r\n}\r\nstatic void __pskb_trim_head(struct sk_buff *skb, int len)\r\n{\r\nint i, k, eat;\r\neat = len;\r\nk = 0;\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nif (skb_shinfo(skb)->frags[i].size <= eat) {\r\nput_page(skb_shinfo(skb)->frags[i].page);\r\neat -= skb_shinfo(skb)->frags[i].size;\r\n} else {\r\nskb_shinfo(skb)->frags[k] = skb_shinfo(skb)->frags[i];\r\nif (eat) {\r\nskb_shinfo(skb)->frags[k].page_offset += eat;\r\nskb_shinfo(skb)->frags[k].size -= eat;\r\neat = 0;\r\n}\r\nk++;\r\n}\r\n}\r\nskb_shinfo(skb)->nr_frags = k;\r\nskb_reset_tail_pointer(skb);\r\nskb->data_len -= len;\r\nskb->len = skb->data_len;\r\n}\r\nint tcp_trim_head(struct sock *sk, struct sk_buff *skb, u32 len)\r\n{\r\nif (skb_cloned(skb) && pskb_expand_head(skb, 0, 0, GFP_ATOMIC))\r\nreturn -ENOMEM;\r\nif (unlikely(len < skb_headlen(skb)))\r\n__skb_pull(skb, len);\r\nelse\r\n__pskb_trim_head(skb, len - skb_headlen(skb));\r\nTCP_SKB_CB(skb)->seq += len;\r\nskb->ip_summed = CHECKSUM_PARTIAL;\r\nskb->truesize -= len;\r\nsk->sk_wmem_queued -= len;\r\nsk_mem_uncharge(sk, len);\r\nsock_set_flag(sk, SOCK_QUEUE_SHRUNK);\r\nif (tcp_skb_pcount(skb) > 1)\r\ntcp_set_skb_tso_segs(sk, skb, tcp_current_mss(sk));\r\nreturn 0;\r\n}\r\nint tcp_mtu_to_mss(struct sock *sk, int pmtu)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nint mss_now;\r\nmss_now = pmtu - icsk->icsk_af_ops->net_header_len - sizeof(struct tcphdr);\r\nif (mss_now > tp->rx_opt.mss_clamp)\r\nmss_now = tp->rx_opt.mss_clamp;\r\nmss_now -= icsk->icsk_ext_hdr_len;\r\nif (mss_now < 48)\r\nmss_now = 48;\r\nmss_now -= tp->tcp_header_len - sizeof(struct tcphdr);\r\nreturn mss_now;\r\n}\r\nint tcp_mss_to_mtu(struct sock *sk, int mss)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nint mtu;\r\nmtu = mss +\r\ntp->tcp_header_len +\r\nicsk->icsk_ext_hdr_len +\r\nicsk->icsk_af_ops->net_header_len;\r\nreturn mtu;\r\n}\r\nvoid tcp_mtup_init(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nicsk->icsk_mtup.enabled = sysctl_tcp_mtu_probing > 1;\r\nicsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp + sizeof(struct tcphdr) +\r\nicsk->icsk_af_ops->net_header_len;\r\nicsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, sysctl_tcp_base_mss);\r\nicsk->icsk_mtup.probe_size = 0;\r\n}\r\nunsigned int tcp_sync_mss(struct sock *sk, u32 pmtu)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nint mss_now;\r\nif (icsk->icsk_mtup.search_high > pmtu)\r\nicsk->icsk_mtup.search_high = pmtu;\r\nmss_now = tcp_mtu_to_mss(sk, pmtu);\r\nmss_now = tcp_bound_to_half_wnd(tp, mss_now);\r\nicsk->icsk_pmtu_cookie = pmtu;\r\nif (icsk->icsk_mtup.enabled)\r\nmss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_low));\r\ntp->mss_cache = mss_now;\r\nreturn mss_now;\r\n}\r\nunsigned int tcp_current_mss(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct dst_entry *dst = __sk_dst_get(sk);\r\nu32 mss_now;\r\nunsigned header_len;\r\nstruct tcp_out_options opts;\r\nstruct tcp_md5sig_key *md5;\r\nmss_now = tp->mss_cache;\r\nif (dst) {\r\nu32 mtu = dst_mtu(dst);\r\nif (mtu != inet_csk(sk)->icsk_pmtu_cookie)\r\nmss_now = tcp_sync_mss(sk, mtu);\r\n}\r\nheader_len = tcp_established_options(sk, NULL, &opts, &md5) +\r\nsizeof(struct tcphdr);\r\nif (header_len != tp->tcp_header_len) {\r\nint delta = (int) header_len - tp->tcp_header_len;\r\nmss_now -= delta;\r\n}\r\nreturn mss_now;\r\n}\r\nstatic void tcp_cwnd_validate(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nif (tp->packets_out >= tp->snd_cwnd) {\r\ntp->snd_cwnd_used = 0;\r\ntp->snd_cwnd_stamp = tcp_time_stamp;\r\n} else {\r\nif (tp->packets_out > tp->snd_cwnd_used)\r\ntp->snd_cwnd_used = tp->packets_out;\r\nif (sysctl_tcp_slow_start_after_idle &&\r\n(s32)(tcp_time_stamp - tp->snd_cwnd_stamp) >= inet_csk(sk)->icsk_rto)\r\ntcp_cwnd_application_limited(sk);\r\n}\r\n}\r\nstatic unsigned int tcp_mss_split_point(struct sock *sk, struct sk_buff *skb,\r\nunsigned int mss_now, unsigned int cwnd)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nu32 needed, window, cwnd_len;\r\nwindow = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\r\ncwnd_len = mss_now * cwnd;\r\nif (likely(cwnd_len <= window && skb != tcp_write_queue_tail(sk)))\r\nreturn cwnd_len;\r\nneeded = min(skb->len, window);\r\nif (cwnd_len <= needed)\r\nreturn cwnd_len;\r\nreturn needed - needed % mss_now;\r\n}\r\nstatic inline unsigned int tcp_cwnd_test(struct tcp_sock *tp,\r\nstruct sk_buff *skb)\r\n{\r\nu32 in_flight, cwnd;\r\nif ((TCP_SKB_CB(skb)->flags & TCPHDR_FIN) && tcp_skb_pcount(skb) == 1)\r\nreturn 1;\r\nin_flight = tcp_packets_in_flight(tp);\r\ncwnd = tp->snd_cwnd;\r\nif (in_flight < cwnd)\r\nreturn (cwnd - in_flight);\r\nreturn 0;\r\n}\r\nstatic int tcp_init_tso_segs(struct sock *sk, struct sk_buff *skb,\r\nunsigned int mss_now)\r\n{\r\nint tso_segs = tcp_skb_pcount(skb);\r\nif (!tso_segs || (tso_segs > 1 && tcp_skb_mss(skb) != mss_now)) {\r\ntcp_set_skb_tso_segs(sk, skb, mss_now);\r\ntso_segs = tcp_skb_pcount(skb);\r\n}\r\nreturn tso_segs;\r\n}\r\nstatic inline int tcp_minshall_check(const struct tcp_sock *tp)\r\n{\r\nreturn after(tp->snd_sml, tp->snd_una) &&\r\n!after(tp->snd_sml, tp->snd_nxt);\r\n}\r\nstatic inline int tcp_nagle_check(const struct tcp_sock *tp,\r\nconst struct sk_buff *skb,\r\nunsigned mss_now, int nonagle)\r\n{\r\nreturn skb->len < mss_now &&\r\n((nonagle & TCP_NAGLE_CORK) ||\r\n(!nonagle && tp->packets_out && tcp_minshall_check(tp)));\r\n}\r\nstatic inline int tcp_nagle_test(struct tcp_sock *tp, struct sk_buff *skb,\r\nunsigned int cur_mss, int nonagle)\r\n{\r\nif (nonagle & TCP_NAGLE_PUSH)\r\nreturn 1;\r\nif (tcp_urg_mode(tp) || (tp->frto_counter == 2) ||\r\n(TCP_SKB_CB(skb)->flags & TCPHDR_FIN))\r\nreturn 1;\r\nif (!tcp_nagle_check(tp, skb, cur_mss, nonagle))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline int tcp_snd_wnd_test(struct tcp_sock *tp, struct sk_buff *skb,\r\nunsigned int cur_mss)\r\n{\r\nu32 end_seq = TCP_SKB_CB(skb)->end_seq;\r\nif (skb->len > cur_mss)\r\nend_seq = TCP_SKB_CB(skb)->seq + cur_mss;\r\nreturn !after(end_seq, tcp_wnd_end(tp));\r\n}\r\nstatic unsigned int tcp_snd_test(struct sock *sk, struct sk_buff *skb,\r\nunsigned int cur_mss, int nonagle)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nunsigned int cwnd_quota;\r\ntcp_init_tso_segs(sk, skb, cur_mss);\r\nif (!tcp_nagle_test(tp, skb, cur_mss, nonagle))\r\nreturn 0;\r\ncwnd_quota = tcp_cwnd_test(tp, skb);\r\nif (cwnd_quota && !tcp_snd_wnd_test(tp, skb, cur_mss))\r\ncwnd_quota = 0;\r\nreturn cwnd_quota;\r\n}\r\nint tcp_may_send_now(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *skb = tcp_send_head(sk);\r\nreturn skb &&\r\ntcp_snd_test(sk, skb, tcp_current_mss(sk),\r\n(tcp_skb_is_last(sk, skb) ?\r\ntp->nonagle : TCP_NAGLE_PUSH));\r\n}\r\nstatic int tso_fragment(struct sock *sk, struct sk_buff *skb, unsigned int len,\r\nunsigned int mss_now, gfp_t gfp)\r\n{\r\nstruct sk_buff *buff;\r\nint nlen = skb->len - len;\r\nu8 flags;\r\nif (skb->len != skb->data_len)\r\nreturn tcp_fragment(sk, skb, len, mss_now);\r\nbuff = sk_stream_alloc_skb(sk, 0, gfp);\r\nif (unlikely(buff == NULL))\r\nreturn -ENOMEM;\r\nsk->sk_wmem_queued += buff->truesize;\r\nsk_mem_charge(sk, buff->truesize);\r\nbuff->truesize += nlen;\r\nskb->truesize -= nlen;\r\nTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\r\nTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\r\nTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\r\nflags = TCP_SKB_CB(skb)->flags;\r\nTCP_SKB_CB(skb)->flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\r\nTCP_SKB_CB(buff)->flags = flags;\r\nTCP_SKB_CB(buff)->sacked = 0;\r\nbuff->ip_summed = skb->ip_summed = CHECKSUM_PARTIAL;\r\nskb_split(skb, buff, len);\r\ntcp_set_skb_tso_segs(sk, skb, mss_now);\r\ntcp_set_skb_tso_segs(sk, buff, mss_now);\r\nskb_header_release(buff);\r\ntcp_insert_write_queue_after(skb, buff, sk);\r\nreturn 0;\r\n}\r\nstatic int tcp_tso_should_defer(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nu32 send_win, cong_win, limit, in_flight;\r\nint win_divisor;\r\nif (TCP_SKB_CB(skb)->flags & TCPHDR_FIN)\r\ngoto send_now;\r\nif (icsk->icsk_ca_state != TCP_CA_Open)\r\ngoto send_now;\r\nif (tp->tso_deferred &&\r\n(((u32)jiffies << 1) >> 1) - (tp->tso_deferred >> 1) > 1)\r\ngoto send_now;\r\nin_flight = tcp_packets_in_flight(tp);\r\nBUG_ON(tcp_skb_pcount(skb) <= 1 || (tp->snd_cwnd <= in_flight));\r\nsend_win = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\r\ncong_win = (tp->snd_cwnd - in_flight) * tp->mss_cache;\r\nlimit = min(send_win, cong_win);\r\nif (limit >= sk->sk_gso_max_size)\r\ngoto send_now;\r\nif ((skb != tcp_write_queue_tail(sk)) && (limit >= skb->len))\r\ngoto send_now;\r\nwin_divisor = ACCESS_ONCE(sysctl_tcp_tso_win_divisor);\r\nif (win_divisor) {\r\nu32 chunk = min(tp->snd_wnd, tp->snd_cwnd * tp->mss_cache);\r\nchunk /= win_divisor;\r\nif (limit >= chunk)\r\ngoto send_now;\r\n} else {\r\nif (limit > tcp_max_burst(tp) * tp->mss_cache)\r\ngoto send_now;\r\n}\r\ntp->tso_deferred = 1 | (jiffies << 1);\r\nreturn 1;\r\nsend_now:\r\ntp->tso_deferred = 0;\r\nreturn 0;\r\n}\r\nstatic int tcp_mtu_probe(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct sk_buff *skb, *nskb, *next;\r\nint len;\r\nint probe_size;\r\nint size_needed;\r\nint copy;\r\nint mss_now;\r\nif (!icsk->icsk_mtup.enabled ||\r\nicsk->icsk_mtup.probe_size ||\r\ninet_csk(sk)->icsk_ca_state != TCP_CA_Open ||\r\ntp->snd_cwnd < 11 ||\r\ntp->rx_opt.num_sacks || tp->rx_opt.dsack)\r\nreturn -1;\r\nmss_now = tcp_current_mss(sk);\r\nprobe_size = 2 * tp->mss_cache;\r\nsize_needed = probe_size + (tp->reordering + 1) * tp->mss_cache;\r\nif (probe_size > tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_high)) {\r\nreturn -1;\r\n}\r\nif (tp->write_seq - tp->snd_nxt < size_needed)\r\nreturn -1;\r\nif (tp->snd_wnd < size_needed)\r\nreturn -1;\r\nif (after(tp->snd_nxt + size_needed, tcp_wnd_end(tp)))\r\nreturn 0;\r\nif (tcp_packets_in_flight(tp) + 2 > tp->snd_cwnd) {\r\nif (!tcp_packets_in_flight(tp))\r\nreturn -1;\r\nelse\r\nreturn 0;\r\n}\r\nif ((nskb = sk_stream_alloc_skb(sk, probe_size, GFP_ATOMIC)) == NULL)\r\nreturn -1;\r\nsk->sk_wmem_queued += nskb->truesize;\r\nsk_mem_charge(sk, nskb->truesize);\r\nskb = tcp_send_head(sk);\r\nTCP_SKB_CB(nskb)->seq = TCP_SKB_CB(skb)->seq;\r\nTCP_SKB_CB(nskb)->end_seq = TCP_SKB_CB(skb)->seq + probe_size;\r\nTCP_SKB_CB(nskb)->flags = TCPHDR_ACK;\r\nTCP_SKB_CB(nskb)->sacked = 0;\r\nnskb->csum = 0;\r\nnskb->ip_summed = skb->ip_summed;\r\ntcp_insert_write_queue_before(nskb, skb, sk);\r\nlen = 0;\r\ntcp_for_write_queue_from_safe(skb, next, sk) {\r\ncopy = min_t(int, skb->len, probe_size - len);\r\nif (nskb->ip_summed)\r\nskb_copy_bits(skb, 0, skb_put(nskb, copy), copy);\r\nelse\r\nnskb->csum = skb_copy_and_csum_bits(skb, 0,\r\nskb_put(nskb, copy),\r\ncopy, nskb->csum);\r\nif (skb->len <= copy) {\r\nTCP_SKB_CB(nskb)->flags |= TCP_SKB_CB(skb)->flags;\r\ntcp_unlink_write_queue(skb, sk);\r\nsk_wmem_free_skb(sk, skb);\r\n} else {\r\nTCP_SKB_CB(nskb)->flags |= TCP_SKB_CB(skb)->flags &\r\n~(TCPHDR_FIN|TCPHDR_PSH);\r\nif (!skb_shinfo(skb)->nr_frags) {\r\nskb_pull(skb, copy);\r\nif (skb->ip_summed != CHECKSUM_PARTIAL)\r\nskb->csum = csum_partial(skb->data,\r\nskb->len, 0);\r\n} else {\r\n__pskb_trim_head(skb, copy);\r\ntcp_set_skb_tso_segs(sk, skb, mss_now);\r\n}\r\nTCP_SKB_CB(skb)->seq += copy;\r\n}\r\nlen += copy;\r\nif (len >= probe_size)\r\nbreak;\r\n}\r\ntcp_init_tso_segs(sk, nskb, nskb->len);\r\nTCP_SKB_CB(nskb)->when = tcp_time_stamp;\r\nif (!tcp_transmit_skb(sk, nskb, 1, GFP_ATOMIC)) {\r\ntp->snd_cwnd--;\r\ntcp_event_new_data_sent(sk, nskb);\r\nicsk->icsk_mtup.probe_size = tcp_mss_to_mtu(sk, nskb->len);\r\ntp->mtu_probe.probe_seq_start = TCP_SKB_CB(nskb)->seq;\r\ntp->mtu_probe.probe_seq_end = TCP_SKB_CB(nskb)->end_seq;\r\nreturn 1;\r\n}\r\nreturn -1;\r\n}\r\nstatic int tcp_write_xmit(struct sock *sk, unsigned int mss_now, int nonagle,\r\nint push_one, gfp_t gfp)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *skb;\r\nunsigned int tso_segs, sent_pkts;\r\nint cwnd_quota;\r\nint result;\r\nsent_pkts = 0;\r\nif (!push_one) {\r\nresult = tcp_mtu_probe(sk);\r\nif (!result) {\r\nreturn 0;\r\n} else if (result > 0) {\r\nsent_pkts = 1;\r\n}\r\n}\r\nwhile ((skb = tcp_send_head(sk))) {\r\nunsigned int limit;\r\ntso_segs = tcp_init_tso_segs(sk, skb, mss_now);\r\nBUG_ON(!tso_segs);\r\ncwnd_quota = tcp_cwnd_test(tp, skb);\r\nif (!cwnd_quota)\r\nbreak;\r\nif (unlikely(!tcp_snd_wnd_test(tp, skb, mss_now)))\r\nbreak;\r\nif (tso_segs == 1) {\r\nif (unlikely(!tcp_nagle_test(tp, skb, mss_now,\r\n(tcp_skb_is_last(sk, skb) ?\r\nnonagle : TCP_NAGLE_PUSH))))\r\nbreak;\r\n} else {\r\nif (!push_one && tcp_tso_should_defer(sk, skb))\r\nbreak;\r\n}\r\nlimit = mss_now;\r\nif (tso_segs > 1 && !tcp_urg_mode(tp))\r\nlimit = tcp_mss_split_point(sk, skb, mss_now,\r\ncwnd_quota);\r\nif (skb->len > limit &&\r\nunlikely(tso_fragment(sk, skb, limit, mss_now, gfp)))\r\nbreak;\r\nTCP_SKB_CB(skb)->when = tcp_time_stamp;\r\nif (unlikely(tcp_transmit_skb(sk, skb, 1, gfp)))\r\nbreak;\r\ntcp_event_new_data_sent(sk, skb);\r\ntcp_minshall_update(tp, mss_now, skb);\r\nsent_pkts++;\r\nif (push_one)\r\nbreak;\r\n}\r\nif (likely(sent_pkts)) {\r\ntcp_cwnd_validate(sk);\r\nreturn 0;\r\n}\r\nreturn !tp->packets_out && tcp_send_head(sk);\r\n}\r\nvoid __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,\r\nint nonagle)\r\n{\r\nif (unlikely(sk->sk_state == TCP_CLOSE))\r\nreturn;\r\nif (tcp_write_xmit(sk, cur_mss, nonagle, 0, GFP_ATOMIC))\r\ntcp_check_probe_timer(sk);\r\n}\r\nvoid tcp_push_one(struct sock *sk, unsigned int mss_now)\r\n{\r\nstruct sk_buff *skb = tcp_send_head(sk);\r\nBUG_ON(!skb || skb->len < mss_now);\r\ntcp_write_xmit(sk, mss_now, TCP_NAGLE_PUSH, 1, sk->sk_allocation);\r\n}\r\nu32 __tcp_select_window(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nint mss = icsk->icsk_ack.rcv_mss;\r\nint free_space = tcp_space(sk);\r\nint full_space = min_t(int, tp->window_clamp, tcp_full_space(sk));\r\nint window;\r\nif (mss > full_space)\r\nmss = full_space;\r\nif (free_space < (full_space >> 1)) {\r\nicsk->icsk_ack.quick = 0;\r\nif (tcp_memory_pressure)\r\ntp->rcv_ssthresh = min(tp->rcv_ssthresh,\r\n4U * tp->advmss);\r\nif (free_space < mss)\r\nreturn 0;\r\n}\r\nif (free_space > tp->rcv_ssthresh)\r\nfree_space = tp->rcv_ssthresh;\r\nwindow = tp->rcv_wnd;\r\nif (tp->rx_opt.rcv_wscale) {\r\nwindow = free_space;\r\nif (((window >> tp->rx_opt.rcv_wscale) << tp->rx_opt.rcv_wscale) != window)\r\nwindow = (((window >> tp->rx_opt.rcv_wscale) + 1)\r\n<< tp->rx_opt.rcv_wscale);\r\n} else {\r\nif (window <= free_space - mss || window > free_space)\r\nwindow = (free_space / mss) * mss;\r\nelse if (mss == full_space &&\r\nfree_space > window + (full_space >> 1))\r\nwindow = free_space;\r\n}\r\nreturn window;\r\n}\r\nstatic void tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *next_skb = tcp_write_queue_next(sk, skb);\r\nint skb_size, next_skb_size;\r\nskb_size = skb->len;\r\nnext_skb_size = next_skb->len;\r\nBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\r\ntcp_highest_sack_combine(sk, next_skb, skb);\r\ntcp_unlink_write_queue(next_skb, sk);\r\nskb_copy_from_linear_data(next_skb, skb_put(skb, next_skb_size),\r\nnext_skb_size);\r\nif (next_skb->ip_summed == CHECKSUM_PARTIAL)\r\nskb->ip_summed = CHECKSUM_PARTIAL;\r\nif (skb->ip_summed != CHECKSUM_PARTIAL)\r\nskb->csum = csum_block_add(skb->csum, next_skb->csum, skb_size);\r\nTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(next_skb)->end_seq;\r\nTCP_SKB_CB(skb)->flags |= TCP_SKB_CB(next_skb)->flags;\r\nTCP_SKB_CB(skb)->sacked |= TCP_SKB_CB(next_skb)->sacked & TCPCB_EVER_RETRANS;\r\ntcp_clear_retrans_hints_partial(tp);\r\nif (next_skb == tp->retransmit_skb_hint)\r\ntp->retransmit_skb_hint = skb;\r\ntcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\r\nsk_wmem_free_skb(sk, next_skb);\r\n}\r\nstatic int tcp_can_collapse(struct sock *sk, struct sk_buff *skb)\r\n{\r\nif (tcp_skb_pcount(skb) > 1)\r\nreturn 0;\r\nif (skb_shinfo(skb)->nr_frags != 0)\r\nreturn 0;\r\nif (skb_cloned(skb))\r\nreturn 0;\r\nif (skb == tcp_send_head(sk))\r\nreturn 0;\r\nif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *to,\r\nint space)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *skb = to, *tmp;\r\nint first = 1;\r\nif (!sysctl_tcp_retrans_collapse)\r\nreturn;\r\nif (TCP_SKB_CB(skb)->flags & TCPHDR_SYN)\r\nreturn;\r\ntcp_for_write_queue_from_safe(skb, tmp, sk) {\r\nif (!tcp_can_collapse(sk, skb))\r\nbreak;\r\nspace -= skb->len;\r\nif (first) {\r\nfirst = 0;\r\ncontinue;\r\n}\r\nif (space < 0)\r\nbreak;\r\nif (skb->len > skb_tailroom(to))\r\nbreak;\r\nif (after(TCP_SKB_CB(skb)->end_seq, tcp_wnd_end(tp)))\r\nbreak;\r\ntcp_collapse_retrans(sk, to);\r\n}\r\n}\r\nint tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nunsigned int cur_mss;\r\nint err;\r\nif (icsk->icsk_mtup.probe_size) {\r\nicsk->icsk_mtup.probe_size = 0;\r\n}\r\nif (atomic_read(&sk->sk_wmem_alloc) >\r\nmin(sk->sk_wmem_queued + (sk->sk_wmem_queued >> 2), sk->sk_sndbuf))\r\nreturn -EAGAIN;\r\nif (before(TCP_SKB_CB(skb)->seq, tp->snd_una)) {\r\nif (before(TCP_SKB_CB(skb)->end_seq, tp->snd_una))\r\nBUG();\r\nif (tcp_trim_head(sk, skb, tp->snd_una - TCP_SKB_CB(skb)->seq))\r\nreturn -ENOMEM;\r\n}\r\nif (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))\r\nreturn -EHOSTUNREACH;\r\ncur_mss = tcp_current_mss(sk);\r\nif (!before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp)) &&\r\nTCP_SKB_CB(skb)->seq != tp->snd_una)\r\nreturn -EAGAIN;\r\nif (skb->len > cur_mss) {\r\nif (tcp_fragment(sk, skb, cur_mss, cur_mss))\r\nreturn -ENOMEM;\r\n} else {\r\nint oldpcount = tcp_skb_pcount(skb);\r\nif (unlikely(oldpcount > 1)) {\r\ntcp_init_tso_segs(sk, skb, cur_mss);\r\ntcp_adjust_pcount(sk, skb, oldpcount - tcp_skb_pcount(skb));\r\n}\r\n}\r\ntcp_retrans_try_collapse(sk, skb, cur_mss);\r\nif (skb->len > 0 &&\r\n(TCP_SKB_CB(skb)->flags & TCPHDR_FIN) &&\r\ntp->snd_una == (TCP_SKB_CB(skb)->end_seq - 1)) {\r\nif (!pskb_trim(skb, 0)) {\r\ntcp_init_nondata_skb(skb, TCP_SKB_CB(skb)->end_seq - 1,\r\nTCP_SKB_CB(skb)->flags);\r\nskb->ip_summed = CHECKSUM_NONE;\r\n}\r\n}\r\nTCP_SKB_CB(skb)->when = tcp_time_stamp;\r\nerr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\r\nif (err == 0) {\r\nTCP_INC_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS);\r\ntp->total_retrans++;\r\n#if FASTRETRANS_DEBUG > 0\r\nif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_RETRANS) {\r\nif (net_ratelimit())\r\nprintk(KERN_DEBUG "retrans_out leaked.\n");\r\n}\r\n#endif\r\nif (!tp->retrans_out)\r\ntp->lost_retrans_low = tp->snd_nxt;\r\nTCP_SKB_CB(skb)->sacked |= TCPCB_RETRANS;\r\ntp->retrans_out += tcp_skb_pcount(skb);\r\nif (!tp->retrans_stamp)\r\ntp->retrans_stamp = TCP_SKB_CB(skb)->when;\r\ntp->undo_retrans += tcp_skb_pcount(skb);\r\nTCP_SKB_CB(skb)->ack_seq = tp->snd_nxt;\r\n}\r\nreturn err;\r\n}\r\nstatic int tcp_can_forward_retransmit(struct sock *sk)\r\n{\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nif (icsk->icsk_ca_state != TCP_CA_Recovery)\r\nreturn 0;\r\nif (tcp_is_reno(tp))\r\nreturn 0;\r\nif (tcp_may_send_now(sk))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nvoid tcp_xmit_retransmit_queue(struct sock *sk)\r\n{\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *skb;\r\nstruct sk_buff *hole = NULL;\r\nu32 last_lost;\r\nint mib_idx;\r\nint fwd_rexmitting = 0;\r\nif (!tp->packets_out)\r\nreturn;\r\nif (!tp->lost_out)\r\ntp->retransmit_high = tp->snd_una;\r\nif (tp->retransmit_skb_hint) {\r\nskb = tp->retransmit_skb_hint;\r\nlast_lost = TCP_SKB_CB(skb)->end_seq;\r\nif (after(last_lost, tp->retransmit_high))\r\nlast_lost = tp->retransmit_high;\r\n} else {\r\nskb = tcp_write_queue_head(sk);\r\nlast_lost = tp->snd_una;\r\n}\r\ntcp_for_write_queue_from(skb, sk) {\r\n__u8 sacked = TCP_SKB_CB(skb)->sacked;\r\nif (skb == tcp_send_head(sk))\r\nbreak;\r\nif (hole == NULL)\r\ntp->retransmit_skb_hint = skb;\r\nif (tcp_packets_in_flight(tp) >= tp->snd_cwnd)\r\nreturn;\r\nif (fwd_rexmitting) {\r\nbegin_fwd:\r\nif (!before(TCP_SKB_CB(skb)->seq, tcp_highest_sack_seq(tp)))\r\nbreak;\r\nmib_idx = LINUX_MIB_TCPFORWARDRETRANS;\r\n} else if (!before(TCP_SKB_CB(skb)->seq, tp->retransmit_high)) {\r\ntp->retransmit_high = last_lost;\r\nif (!tcp_can_forward_retransmit(sk))\r\nbreak;\r\nif (hole != NULL) {\r\nskb = hole;\r\nhole = NULL;\r\n}\r\nfwd_rexmitting = 1;\r\ngoto begin_fwd;\r\n} else if (!(sacked & TCPCB_LOST)) {\r\nif (hole == NULL && !(sacked & (TCPCB_SACKED_RETRANS|TCPCB_SACKED_ACKED)))\r\nhole = skb;\r\ncontinue;\r\n} else {\r\nlast_lost = TCP_SKB_CB(skb)->end_seq;\r\nif (icsk->icsk_ca_state != TCP_CA_Loss)\r\nmib_idx = LINUX_MIB_TCPFASTRETRANS;\r\nelse\r\nmib_idx = LINUX_MIB_TCPSLOWSTARTRETRANS;\r\n}\r\nif (sacked & (TCPCB_SACKED_ACKED|TCPCB_SACKED_RETRANS))\r\ncontinue;\r\nif (tcp_retransmit_skb(sk, skb))\r\nreturn;\r\nNET_INC_STATS_BH(sock_net(sk), mib_idx);\r\nif (skb == tcp_write_queue_head(sk))\r\ninet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\r\ninet_csk(sk)->icsk_rto,\r\nTCP_RTO_MAX);\r\n}\r\n}\r\nvoid tcp_send_fin(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *skb = tcp_write_queue_tail(sk);\r\nint mss_now;\r\nmss_now = tcp_current_mss(sk);\r\nif (tcp_send_head(sk) != NULL) {\r\nTCP_SKB_CB(skb)->flags |= TCPHDR_FIN;\r\nTCP_SKB_CB(skb)->end_seq++;\r\ntp->write_seq++;\r\n} else {\r\nfor (;;) {\r\nskb = alloc_skb_fclone(MAX_TCP_HEADER,\r\nsk->sk_allocation);\r\nif (skb)\r\nbreak;\r\nyield();\r\n}\r\nskb_reserve(skb, MAX_TCP_HEADER);\r\ntcp_init_nondata_skb(skb, tp->write_seq,\r\nTCPHDR_ACK | TCPHDR_FIN);\r\ntcp_queue_skb(sk, skb);\r\n}\r\n__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_OFF);\r\n}\r\nvoid tcp_send_active_reset(struct sock *sk, gfp_t priority)\r\n{\r\nstruct sk_buff *skb;\r\nskb = alloc_skb(MAX_TCP_HEADER, priority);\r\nif (!skb) {\r\nNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);\r\nreturn;\r\n}\r\nskb_reserve(skb, MAX_TCP_HEADER);\r\ntcp_init_nondata_skb(skb, tcp_acceptable_seq(sk),\r\nTCPHDR_ACK | TCPHDR_RST);\r\nTCP_SKB_CB(skb)->when = tcp_time_stamp;\r\nif (tcp_transmit_skb(sk, skb, 0, priority))\r\nNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTFAILED);\r\nTCP_INC_STATS(sock_net(sk), TCP_MIB_OUTRSTS);\r\n}\r\nint tcp_send_synack(struct sock *sk)\r\n{\r\nstruct sk_buff *skb;\r\nskb = tcp_write_queue_head(sk);\r\nif (skb == NULL || !(TCP_SKB_CB(skb)->flags & TCPHDR_SYN)) {\r\nprintk(KERN_DEBUG "tcp_send_synack: wrong queue state\n");\r\nreturn -EFAULT;\r\n}\r\nif (!(TCP_SKB_CB(skb)->flags & TCPHDR_ACK)) {\r\nif (skb_cloned(skb)) {\r\nstruct sk_buff *nskb = skb_copy(skb, GFP_ATOMIC);\r\nif (nskb == NULL)\r\nreturn -ENOMEM;\r\ntcp_unlink_write_queue(skb, sk);\r\nskb_header_release(nskb);\r\n__tcp_add_write_queue_head(sk, nskb);\r\nsk_wmem_free_skb(sk, skb);\r\nsk->sk_wmem_queued += nskb->truesize;\r\nsk_mem_charge(sk, nskb->truesize);\r\nskb = nskb;\r\n}\r\nTCP_SKB_CB(skb)->flags |= TCPHDR_ACK;\r\nTCP_ECN_send_synack(tcp_sk(sk), skb);\r\n}\r\nTCP_SKB_CB(skb)->when = tcp_time_stamp;\r\nreturn tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\r\n}\r\nstruct sk_buff *tcp_make_synack(struct sock *sk, struct dst_entry *dst,\r\nstruct request_sock *req,\r\nstruct request_values *rvp)\r\n{\r\nstruct tcp_out_options opts;\r\nstruct tcp_extend_values *xvp = tcp_xv(rvp);\r\nstruct inet_request_sock *ireq = inet_rsk(req);\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nconst struct tcp_cookie_values *cvp = tp->cookie_values;\r\nstruct tcphdr *th;\r\nstruct sk_buff *skb;\r\nstruct tcp_md5sig_key *md5;\r\nint tcp_header_size;\r\nint mss;\r\nint s_data_desired = 0;\r\nif (cvp != NULL && cvp->s_data_constant && cvp->s_data_desired)\r\ns_data_desired = cvp->s_data_desired;\r\nskb = sock_wmalloc(sk, MAX_TCP_HEADER + 15 + s_data_desired, 1, GFP_ATOMIC);\r\nif (skb == NULL)\r\nreturn NULL;\r\nskb_reserve(skb, MAX_TCP_HEADER);\r\nskb_dst_set(skb, dst_clone(dst));\r\nmss = dst_metric_advmss(dst);\r\nif (tp->rx_opt.user_mss && tp->rx_opt.user_mss < mss)\r\nmss = tp->rx_opt.user_mss;\r\nif (req->rcv_wnd == 0) {\r\n__u8 rcv_wscale;\r\nreq->window_clamp = tp->window_clamp ? : dst_metric(dst, RTAX_WINDOW);\r\nif (sk->sk_userlocks & SOCK_RCVBUF_LOCK &&\r\n(req->window_clamp > tcp_full_space(sk) || req->window_clamp == 0))\r\nreq->window_clamp = tcp_full_space(sk);\r\ntcp_select_initial_window(tcp_full_space(sk),\r\nmss - (ireq->tstamp_ok ? TCPOLEN_TSTAMP_ALIGNED : 0),\r\n&req->rcv_wnd,\r\n&req->window_clamp,\r\nireq->wscale_ok,\r\n&rcv_wscale,\r\ndst_metric(dst, RTAX_INITRWND));\r\nireq->rcv_wscale = rcv_wscale;\r\n}\r\nmemset(&opts, 0, sizeof(opts));\r\n#ifdef CONFIG_SYN_COOKIES\r\nif (unlikely(req->cookie_ts))\r\nTCP_SKB_CB(skb)->when = cookie_init_timestamp(req);\r\nelse\r\n#endif\r\nTCP_SKB_CB(skb)->when = tcp_time_stamp;\r\ntcp_header_size = tcp_synack_options(sk, req, mss,\r\nskb, &opts, &md5, xvp)\r\n+ sizeof(*th);\r\nskb_push(skb, tcp_header_size);\r\nskb_reset_transport_header(skb);\r\nth = tcp_hdr(skb);\r\nmemset(th, 0, sizeof(struct tcphdr));\r\nth->syn = 1;\r\nth->ack = 1;\r\nTCP_ECN_make_synack(req, th);\r\nth->source = ireq->loc_port;\r\nth->dest = ireq->rmt_port;\r\ntcp_init_nondata_skb(skb, tcp_rsk(req)->snt_isn,\r\nTCPHDR_SYN | TCPHDR_ACK);\r\nif (OPTION_COOKIE_EXTENSION & opts.options) {\r\nif (s_data_desired) {\r\nu8 *buf = skb_put(skb, s_data_desired);\r\nmemcpy(buf, cvp->s_data_payload, s_data_desired);\r\nTCP_SKB_CB(skb)->end_seq += s_data_desired;\r\n}\r\nif (opts.hash_size > 0) {\r\n__u32 workspace[SHA_WORKSPACE_WORDS];\r\nu32 *mess = &xvp->cookie_bakery[COOKIE_DIGEST_WORDS];\r\nu32 *tail = &mess[COOKIE_MESSAGE_WORDS-1];\r\n*tail-- ^= opts.tsval;\r\n*tail-- ^= tcp_rsk(req)->rcv_isn + 1;\r\n*tail-- ^= TCP_SKB_CB(skb)->seq + 1;\r\n*tail-- ^= (((__force u32)th->dest << 16) | (__force u32)th->source);\r\n*tail-- ^= (u32)(unsigned long)cvp;\r\nsha_transform((__u32 *)&xvp->cookie_bakery[0],\r\n(char *)mess,\r\n&workspace[0]);\r\nopts.hash_location =\r\n(__u8 *)&xvp->cookie_bakery[0];\r\n}\r\n}\r\nth->seq = htonl(TCP_SKB_CB(skb)->seq);\r\nth->ack_seq = htonl(tcp_rsk(req)->rcv_isn + 1);\r\nth->window = htons(min(req->rcv_wnd, 65535U));\r\ntcp_options_write((__be32 *)(th + 1), tp, &opts);\r\nth->doff = (tcp_header_size >> 2);\r\nTCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS, tcp_skb_pcount(skb));\r\n#ifdef CONFIG_TCP_MD5SIG\r\nif (md5) {\r\ntcp_rsk(req)->af_specific->calc_md5_hash(opts.hash_location,\r\nmd5, NULL, req, skb);\r\n}\r\n#endif\r\nreturn skb;\r\n}\r\nstatic void tcp_connect_init(struct sock *sk)\r\n{\r\nstruct dst_entry *dst = __sk_dst_get(sk);\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\n__u8 rcv_wscale;\r\ntp->tcp_header_len = sizeof(struct tcphdr) +\r\n(sysctl_tcp_timestamps ? TCPOLEN_TSTAMP_ALIGNED : 0);\r\n#ifdef CONFIG_TCP_MD5SIG\r\nif (tp->af_specific->md5_lookup(sk, sk) != NULL)\r\ntp->tcp_header_len += TCPOLEN_MD5SIG_ALIGNED;\r\n#endif\r\nif (tp->rx_opt.user_mss)\r\ntp->rx_opt.mss_clamp = tp->rx_opt.user_mss;\r\ntp->max_window = 0;\r\ntcp_mtup_init(sk);\r\ntcp_sync_mss(sk, dst_mtu(dst));\r\nif (!tp->window_clamp)\r\ntp->window_clamp = dst_metric(dst, RTAX_WINDOW);\r\ntp->advmss = dst_metric_advmss(dst);\r\nif (tp->rx_opt.user_mss && tp->rx_opt.user_mss < tp->advmss)\r\ntp->advmss = tp->rx_opt.user_mss;\r\ntcp_initialize_rcv_mss(sk);\r\nif (sk->sk_userlocks & SOCK_RCVBUF_LOCK &&\r\n(tp->window_clamp > tcp_full_space(sk) || tp->window_clamp == 0))\r\ntp->window_clamp = tcp_full_space(sk);\r\ntcp_select_initial_window(tcp_full_space(sk),\r\ntp->advmss - (tp->rx_opt.ts_recent_stamp ? tp->tcp_header_len - sizeof(struct tcphdr) : 0),\r\n&tp->rcv_wnd,\r\n&tp->window_clamp,\r\nsysctl_tcp_window_scaling,\r\n&rcv_wscale,\r\ndst_metric(dst, RTAX_INITRWND));\r\ntp->rx_opt.rcv_wscale = rcv_wscale;\r\ntp->rcv_ssthresh = tp->rcv_wnd;\r\nsk->sk_err = 0;\r\nsock_reset_flag(sk, SOCK_DONE);\r\ntp->snd_wnd = 0;\r\ntcp_init_wl(tp, 0);\r\ntp->snd_una = tp->write_seq;\r\ntp->snd_sml = tp->write_seq;\r\ntp->snd_up = tp->write_seq;\r\ntp->rcv_nxt = 0;\r\ntp->rcv_wup = 0;\r\ntp->copied_seq = 0;\r\ninet_csk(sk)->icsk_rto = TCP_TIMEOUT_INIT;\r\ninet_csk(sk)->icsk_retransmits = 0;\r\ntcp_clear_retrans(tp);\r\n}\r\nint tcp_connect(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *buff;\r\nint err;\r\ntcp_connect_init(sk);\r\nbuff = alloc_skb_fclone(MAX_TCP_HEADER + 15, sk->sk_allocation);\r\nif (unlikely(buff == NULL))\r\nreturn -ENOBUFS;\r\nskb_reserve(buff, MAX_TCP_HEADER);\r\ntp->snd_nxt = tp->write_seq;\r\ntcp_init_nondata_skb(buff, tp->write_seq++, TCPHDR_SYN);\r\nTCP_ECN_send_syn(sk, buff);\r\nTCP_SKB_CB(buff)->when = tcp_time_stamp;\r\ntp->retrans_stamp = TCP_SKB_CB(buff)->when;\r\nskb_header_release(buff);\r\n__tcp_add_write_queue_tail(sk, buff);\r\nsk->sk_wmem_queued += buff->truesize;\r\nsk_mem_charge(sk, buff->truesize);\r\ntp->packets_out += tcp_skb_pcount(buff);\r\nerr = tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);\r\nif (err == -ECONNREFUSED)\r\nreturn err;\r\ntp->snd_nxt = tp->write_seq;\r\ntp->pushed_seq = tp->write_seq;\r\nTCP_INC_STATS(sock_net(sk), TCP_MIB_ACTIVEOPENS);\r\ninet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\r\ninet_csk(sk)->icsk_rto, TCP_RTO_MAX);\r\nreturn 0;\r\n}\r\nvoid tcp_send_delayed_ack(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nint ato = icsk->icsk_ack.ato;\r\nunsigned long timeout;\r\nif (ato > TCP_DELACK_MIN) {\r\nconst struct tcp_sock *tp = tcp_sk(sk);\r\nint max_ato = HZ / 2;\r\nif (icsk->icsk_ack.pingpong ||\r\n(icsk->icsk_ack.pending & ICSK_ACK_PUSHED))\r\nmax_ato = TCP_DELACK_MAX;\r\nif (tp->srtt) {\r\nint rtt = max(tp->srtt >> 3, TCP_DELACK_MIN);\r\nif (rtt < max_ato)\r\nmax_ato = rtt;\r\n}\r\nato = min(ato, max_ato);\r\n}\r\ntimeout = jiffies + ato;\r\nif (icsk->icsk_ack.pending & ICSK_ACK_TIMER) {\r\nif (icsk->icsk_ack.blocked ||\r\ntime_before_eq(icsk->icsk_ack.timeout, jiffies + (ato >> 2))) {\r\ntcp_send_ack(sk);\r\nreturn;\r\n}\r\nif (!time_before(timeout, icsk->icsk_ack.timeout))\r\ntimeout = icsk->icsk_ack.timeout;\r\n}\r\nicsk->icsk_ack.pending |= ICSK_ACK_SCHED | ICSK_ACK_TIMER;\r\nicsk->icsk_ack.timeout = timeout;\r\nsk_reset_timer(sk, &icsk->icsk_delack_timer, timeout);\r\n}\r\nvoid tcp_send_ack(struct sock *sk)\r\n{\r\nstruct sk_buff *buff;\r\nif (sk->sk_state == TCP_CLOSE)\r\nreturn;\r\nbuff = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);\r\nif (buff == NULL) {\r\ninet_csk_schedule_ack(sk);\r\ninet_csk(sk)->icsk_ack.ato = TCP_ATO_MIN;\r\ninet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,\r\nTCP_DELACK_MAX, TCP_RTO_MAX);\r\nreturn;\r\n}\r\nskb_reserve(buff, MAX_TCP_HEADER);\r\ntcp_init_nondata_skb(buff, tcp_acceptable_seq(sk), TCPHDR_ACK);\r\nTCP_SKB_CB(buff)->when = tcp_time_stamp;\r\ntcp_transmit_skb(sk, buff, 0, GFP_ATOMIC);\r\n}\r\nstatic int tcp_xmit_probe_skb(struct sock *sk, int urgent)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *skb;\r\nskb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);\r\nif (skb == NULL)\r\nreturn -1;\r\nskb_reserve(skb, MAX_TCP_HEADER);\r\ntcp_init_nondata_skb(skb, tp->snd_una - !urgent, TCPHDR_ACK);\r\nTCP_SKB_CB(skb)->when = tcp_time_stamp;\r\nreturn tcp_transmit_skb(sk, skb, 0, GFP_ATOMIC);\r\n}\r\nint tcp_write_wakeup(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *skb;\r\nif (sk->sk_state == TCP_CLOSE)\r\nreturn -1;\r\nif ((skb = tcp_send_head(sk)) != NULL &&\r\nbefore(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp))) {\r\nint err;\r\nunsigned int mss = tcp_current_mss(sk);\r\nunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\r\nif (before(tp->pushed_seq, TCP_SKB_CB(skb)->end_seq))\r\ntp->pushed_seq = TCP_SKB_CB(skb)->end_seq;\r\nif (seg_size < TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq ||\r\nskb->len > mss) {\r\nseg_size = min(seg_size, mss);\r\nTCP_SKB_CB(skb)->flags |= TCPHDR_PSH;\r\nif (tcp_fragment(sk, skb, seg_size, mss))\r\nreturn -1;\r\n} else if (!tcp_skb_pcount(skb))\r\ntcp_set_skb_tso_segs(sk, skb, mss);\r\nTCP_SKB_CB(skb)->flags |= TCPHDR_PSH;\r\nTCP_SKB_CB(skb)->when = tcp_time_stamp;\r\nerr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\r\nif (!err)\r\ntcp_event_new_data_sent(sk, skb);\r\nreturn err;\r\n} else {\r\nif (between(tp->snd_up, tp->snd_una + 1, tp->snd_una + 0xFFFF))\r\ntcp_xmit_probe_skb(sk, 1);\r\nreturn tcp_xmit_probe_skb(sk, 0);\r\n}\r\n}\r\nvoid tcp_send_probe0(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nint err;\r\nerr = tcp_write_wakeup(sk);\r\nif (tp->packets_out || !tcp_send_head(sk)) {\r\nicsk->icsk_probes_out = 0;\r\nicsk->icsk_backoff = 0;\r\nreturn;\r\n}\r\nif (err <= 0) {\r\nif (icsk->icsk_backoff < sysctl_tcp_retries2)\r\nicsk->icsk_backoff++;\r\nicsk->icsk_probes_out++;\r\ninet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0,\r\nmin(icsk->icsk_rto << icsk->icsk_backoff, TCP_RTO_MAX),\r\nTCP_RTO_MAX);\r\n} else {\r\nif (!icsk->icsk_probes_out)\r\nicsk->icsk_probes_out = 1;\r\ninet_csk_reset_xmit_timer(sk, ICSK_TIME_PROBE0,\r\nmin(icsk->icsk_rto << icsk->icsk_backoff,\r\nTCP_RESOURCE_PROBE_INTERVAL),\r\nTCP_RTO_MAX);\r\n}\r\n}
