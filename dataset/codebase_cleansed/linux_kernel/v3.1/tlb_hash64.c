void hpte_need_flush(struct mm_struct *mm, unsigned long addr,\r\npte_t *ptep, unsigned long pte, int huge)\r\n{\r\nstruct ppc64_tlb_batch *batch = &get_cpu_var(ppc64_tlb_batch);\r\nunsigned long vsid, vaddr;\r\nunsigned int psize;\r\nint ssize;\r\nreal_pte_t rpte;\r\nint i;\r\ni = batch->index;\r\nif (huge) {\r\n#ifdef CONFIG_HUGETLB_PAGE\r\npsize = get_slice_psize(mm, addr);\r\naddr &= ~((1UL << mmu_psize_defs[psize].shift) - 1);\r\n#else\r\nBUG();\r\npsize = pte_pagesize_index(mm, addr, pte);\r\n#endif\r\n} else {\r\npsize = pte_pagesize_index(mm, addr, pte);\r\naddr &= PAGE_MASK;\r\n}\r\nif (!is_kernel_addr(addr)) {\r\nssize = user_segment_size(addr);\r\nvsid = get_vsid(mm->context.id, addr, ssize);\r\nWARN_ON(vsid == 0);\r\n} else {\r\nvsid = get_kernel_vsid(addr, mmu_kernel_ssize);\r\nssize = mmu_kernel_ssize;\r\n}\r\nvaddr = hpt_va(addr, vsid, ssize);\r\nrpte = __real_pte(__pte(pte), ptep);\r\nif (!batch->active) {\r\nflush_hash_page(vaddr, rpte, psize, ssize, 0);\r\nput_cpu_var(ppc64_tlb_batch);\r\nreturn;\r\n}\r\nif (i != 0 && (mm != batch->mm || batch->psize != psize ||\r\nbatch->ssize != ssize)) {\r\n__flush_tlb_pending(batch);\r\ni = 0;\r\n}\r\nif (i == 0) {\r\nbatch->mm = mm;\r\nbatch->psize = psize;\r\nbatch->ssize = ssize;\r\n}\r\nbatch->pte[i] = rpte;\r\nbatch->vaddr[i] = vaddr;\r\nbatch->index = ++i;\r\nif (i >= PPC64_TLB_BATCH_NR)\r\n__flush_tlb_pending(batch);\r\nput_cpu_var(ppc64_tlb_batch);\r\n}\r\nvoid __flush_tlb_pending(struct ppc64_tlb_batch *batch)\r\n{\r\nconst struct cpumask *tmp;\r\nint i, local = 0;\r\ni = batch->index;\r\ntmp = cpumask_of(smp_processor_id());\r\nif (cpumask_equal(mm_cpumask(batch->mm), tmp))\r\nlocal = 1;\r\nif (i == 1)\r\nflush_hash_page(batch->vaddr[0], batch->pte[0],\r\nbatch->psize, batch->ssize, local);\r\nelse\r\nflush_hash_range(i, local);\r\nbatch->index = 0;\r\n}\r\nvoid tlb_flush(struct mmu_gather *tlb)\r\n{\r\nstruct ppc64_tlb_batch *tlbbatch = &get_cpu_var(ppc64_tlb_batch);\r\nif (tlbbatch->index)\r\n__flush_tlb_pending(tlbbatch);\r\nput_cpu_var(ppc64_tlb_batch);\r\n}\r\nvoid __flush_hash_table_range(struct mm_struct *mm, unsigned long start,\r\nunsigned long end)\r\n{\r\nunsigned long flags;\r\nstart = _ALIGN_DOWN(start, PAGE_SIZE);\r\nend = _ALIGN_UP(end, PAGE_SIZE);\r\nBUG_ON(!mm->pgd);\r\nlocal_irq_save(flags);\r\narch_enter_lazy_mmu_mode();\r\nfor (; start < end; start += PAGE_SIZE) {\r\npte_t *ptep = find_linux_pte(mm->pgd, start);\r\nunsigned long pte;\r\nif (ptep == NULL)\r\ncontinue;\r\npte = pte_val(*ptep);\r\nif (!(pte & _PAGE_HASHPTE))\r\ncontinue;\r\nhpte_need_flush(mm, start, ptep, pte, 0);\r\n}\r\narch_leave_lazy_mmu_mode();\r\nlocal_irq_restore(flags);\r\n}
