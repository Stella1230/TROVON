static inline int red_use_ecn(struct red_sched_data *q)\r\n{\r\nreturn q->flags & TC_RED_ECN;\r\n}\r\nstatic inline int red_use_harddrop(struct red_sched_data *q)\r\n{\r\nreturn q->flags & TC_RED_HARDDROP;\r\n}\r\nstatic int red_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nstruct Qdisc *child = q->qdisc;\r\nint ret;\r\nq->parms.qavg = red_calc_qavg(&q->parms, child->qstats.backlog);\r\nif (red_is_idling(&q->parms))\r\nred_end_of_idle_period(&q->parms);\r\nswitch (red_action(&q->parms, q->parms.qavg)) {\r\ncase RED_DONT_MARK:\r\nbreak;\r\ncase RED_PROB_MARK:\r\nsch->qstats.overlimits++;\r\nif (!red_use_ecn(q) || !INET_ECN_set_ce(skb)) {\r\nq->stats.prob_drop++;\r\ngoto congestion_drop;\r\n}\r\nq->stats.prob_mark++;\r\nbreak;\r\ncase RED_HARD_MARK:\r\nsch->qstats.overlimits++;\r\nif (red_use_harddrop(q) || !red_use_ecn(q) ||\r\n!INET_ECN_set_ce(skb)) {\r\nq->stats.forced_drop++;\r\ngoto congestion_drop;\r\n}\r\nq->stats.forced_mark++;\r\nbreak;\r\n}\r\nret = qdisc_enqueue(skb, child);\r\nif (likely(ret == NET_XMIT_SUCCESS)) {\r\nsch->q.qlen++;\r\n} else if (net_xmit_drop_count(ret)) {\r\nq->stats.pdrop++;\r\nsch->qstats.drops++;\r\n}\r\nreturn ret;\r\ncongestion_drop:\r\nqdisc_drop(skb, sch);\r\nreturn NET_XMIT_CN;\r\n}\r\nstatic struct sk_buff *red_dequeue(struct Qdisc *sch)\r\n{\r\nstruct sk_buff *skb;\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nstruct Qdisc *child = q->qdisc;\r\nskb = child->dequeue(child);\r\nif (skb) {\r\nqdisc_bstats_update(sch, skb);\r\nsch->q.qlen--;\r\n} else {\r\nif (!red_is_idling(&q->parms))\r\nred_start_of_idle_period(&q->parms);\r\n}\r\nreturn skb;\r\n}\r\nstatic struct sk_buff *red_peek(struct Qdisc *sch)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nstruct Qdisc *child = q->qdisc;\r\nreturn child->ops->peek(child);\r\n}\r\nstatic unsigned int red_drop(struct Qdisc *sch)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nstruct Qdisc *child = q->qdisc;\r\nunsigned int len;\r\nif (child->ops->drop && (len = child->ops->drop(child)) > 0) {\r\nq->stats.other++;\r\nsch->qstats.drops++;\r\nsch->q.qlen--;\r\nreturn len;\r\n}\r\nif (!red_is_idling(&q->parms))\r\nred_start_of_idle_period(&q->parms);\r\nreturn 0;\r\n}\r\nstatic void red_reset(struct Qdisc *sch)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nqdisc_reset(q->qdisc);\r\nsch->q.qlen = 0;\r\nred_restart(&q->parms);\r\n}\r\nstatic void red_destroy(struct Qdisc *sch)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nqdisc_destroy(q->qdisc);\r\n}\r\nstatic int red_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_RED_MAX + 1];\r\nstruct tc_red_qopt *ctl;\r\nstruct Qdisc *child = NULL;\r\nint err;\r\nif (opt == NULL)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_RED_MAX, opt, red_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (tb[TCA_RED_PARMS] == NULL ||\r\ntb[TCA_RED_STAB] == NULL)\r\nreturn -EINVAL;\r\nctl = nla_data(tb[TCA_RED_PARMS]);\r\nif (ctl->limit > 0) {\r\nchild = fifo_create_dflt(sch, &bfifo_qdisc_ops, ctl->limit);\r\nif (IS_ERR(child))\r\nreturn PTR_ERR(child);\r\n}\r\nsch_tree_lock(sch);\r\nq->flags = ctl->flags;\r\nq->limit = ctl->limit;\r\nif (child) {\r\nqdisc_tree_decrease_qlen(q->qdisc, q->qdisc->q.qlen);\r\nqdisc_destroy(q->qdisc);\r\nq->qdisc = child;\r\n}\r\nred_set_parms(&q->parms, ctl->qth_min, ctl->qth_max, ctl->Wlog,\r\nctl->Plog, ctl->Scell_log,\r\nnla_data(tb[TCA_RED_STAB]));\r\nif (skb_queue_empty(&sch->q))\r\nred_end_of_idle_period(&q->parms);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic int red_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nq->qdisc = &noop_qdisc;\r\nreturn red_change(sch, opt);\r\n}\r\nstatic int red_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *opts = NULL;\r\nstruct tc_red_qopt opt = {\r\n.limit = q->limit,\r\n.flags = q->flags,\r\n.qth_min = q->parms.qth_min >> q->parms.Wlog,\r\n.qth_max = q->parms.qth_max >> q->parms.Wlog,\r\n.Wlog = q->parms.Wlog,\r\n.Plog = q->parms.Plog,\r\n.Scell_log = q->parms.Scell_log,\r\n};\r\nsch->qstats.backlog = q->qdisc->qstats.backlog;\r\nopts = nla_nest_start(skb, TCA_OPTIONS);\r\nif (opts == NULL)\r\ngoto nla_put_failure;\r\nNLA_PUT(skb, TCA_RED_PARMS, sizeof(opt), &opt);\r\nreturn nla_nest_end(skb, opts);\r\nnla_put_failure:\r\nnla_nest_cancel(skb, opts);\r\nreturn -EMSGSIZE;\r\n}\r\nstatic int red_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nstruct tc_red_xstats st = {\r\n.early = q->stats.prob_drop + q->stats.forced_drop,\r\n.pdrop = q->stats.pdrop,\r\n.other = q->stats.other,\r\n.marked = q->stats.prob_mark + q->stats.forced_mark,\r\n};\r\nreturn gnet_stats_copy_app(d, &st, sizeof(st));\r\n}\r\nstatic int red_dump_class(struct Qdisc *sch, unsigned long cl,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\ntcm->tcm_handle |= TC_H_MIN(1);\r\ntcm->tcm_info = q->qdisc->handle;\r\nreturn 0;\r\n}\r\nstatic int red_graft(struct Qdisc *sch, unsigned long arg, struct Qdisc *new,\r\nstruct Qdisc **old)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nif (new == NULL)\r\nnew = &noop_qdisc;\r\nsch_tree_lock(sch);\r\n*old = q->qdisc;\r\nq->qdisc = new;\r\nqdisc_tree_decrease_qlen(*old, (*old)->q.qlen);\r\nqdisc_reset(*old);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic struct Qdisc *red_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct red_sched_data *q = qdisc_priv(sch);\r\nreturn q->qdisc;\r\n}\r\nstatic unsigned long red_get(struct Qdisc *sch, u32 classid)\r\n{\r\nreturn 1;\r\n}\r\nstatic void red_put(struct Qdisc *sch, unsigned long arg)\r\n{\r\n}\r\nstatic void red_walk(struct Qdisc *sch, struct qdisc_walker *walker)\r\n{\r\nif (!walker->stop) {\r\nif (walker->count >= walker->skip)\r\nif (walker->fn(sch, 1, walker) < 0) {\r\nwalker->stop = 1;\r\nreturn;\r\n}\r\nwalker->count++;\r\n}\r\n}\r\nstatic int __init red_module_init(void)\r\n{\r\nreturn register_qdisc(&red_qdisc_ops);\r\n}\r\nstatic void __exit red_module_exit(void)\r\n{\r\nunregister_qdisc(&red_qdisc_ops);\r\n}
