static void\r\ni915_gem_object_set_to_gpu_domain(struct drm_i915_gem_object *obj,\r\nstruct intel_ring_buffer *ring,\r\nstruct change_domains *cd)\r\n{\r\nuint32_t invalidate_domains = 0, flush_domains = 0;\r\nif (obj->base.pending_write_domain == 0)\r\nobj->base.pending_read_domains |= obj->base.read_domains;\r\nif (obj->base.write_domain &&\r\n(((obj->base.write_domain != obj->base.pending_read_domains ||\r\nobj->ring != ring)) ||\r\n(obj->fenced_gpu_access && !obj->pending_fenced_gpu_access))) {\r\nflush_domains |= obj->base.write_domain;\r\ninvalidate_domains |=\r\nobj->base.pending_read_domains & ~obj->base.write_domain;\r\n}\r\ninvalidate_domains |= obj->base.pending_read_domains & ~obj->base.read_domains;\r\nif ((flush_domains | invalidate_domains) & I915_GEM_DOMAIN_CPU)\r\ni915_gem_clflush_object(obj);\r\nif (obj->base.pending_write_domain)\r\ncd->flips |= atomic_read(&obj->pending_flip);\r\nif (flush_domains == 0 && obj->base.pending_write_domain == 0)\r\nobj->base.pending_write_domain = obj->base.write_domain;\r\ncd->invalidate_domains |= invalidate_domains;\r\ncd->flush_domains |= flush_domains;\r\nif (flush_domains & I915_GEM_GPU_DOMAINS)\r\ncd->flush_rings |= obj->ring->id;\r\nif (invalidate_domains & I915_GEM_GPU_DOMAINS)\r\ncd->flush_rings |= ring->id;\r\n}\r\nstatic struct eb_objects *\r\neb_create(int size)\r\n{\r\nstruct eb_objects *eb;\r\nint count = PAGE_SIZE / sizeof(struct hlist_head) / 2;\r\nwhile (count > size)\r\ncount >>= 1;\r\neb = kzalloc(count*sizeof(struct hlist_head) +\r\nsizeof(struct eb_objects),\r\nGFP_KERNEL);\r\nif (eb == NULL)\r\nreturn eb;\r\neb->and = count - 1;\r\nreturn eb;\r\n}\r\nstatic void\r\neb_reset(struct eb_objects *eb)\r\n{\r\nmemset(eb->buckets, 0, (eb->and+1)*sizeof(struct hlist_head));\r\n}\r\nstatic void\r\neb_add_object(struct eb_objects *eb, struct drm_i915_gem_object *obj)\r\n{\r\nhlist_add_head(&obj->exec_node,\r\n&eb->buckets[obj->exec_handle & eb->and]);\r\n}\r\nstatic struct drm_i915_gem_object *\r\neb_get_object(struct eb_objects *eb, unsigned long handle)\r\n{\r\nstruct hlist_head *head;\r\nstruct hlist_node *node;\r\nstruct drm_i915_gem_object *obj;\r\nhead = &eb->buckets[handle & eb->and];\r\nhlist_for_each(node, head) {\r\nobj = hlist_entry(node, struct drm_i915_gem_object, exec_node);\r\nif (obj->exec_handle == handle)\r\nreturn obj;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void\r\neb_destroy(struct eb_objects *eb)\r\n{\r\nkfree(eb);\r\n}\r\nstatic int\r\ni915_gem_execbuffer_relocate_entry(struct drm_i915_gem_object *obj,\r\nstruct eb_objects *eb,\r\nstruct drm_i915_gem_relocation_entry *reloc)\r\n{\r\nstruct drm_device *dev = obj->base.dev;\r\nstruct drm_gem_object *target_obj;\r\nuint32_t target_offset;\r\nint ret = -EINVAL;\r\ntarget_obj = &eb_get_object(eb, reloc->target_handle)->base;\r\nif (unlikely(target_obj == NULL))\r\nreturn -ENOENT;\r\ntarget_offset = to_intel_bo(target_obj)->gtt_offset;\r\nif (unlikely(target_offset == 0)) {\r\nDRM_ERROR("No GTT space found for object %d\n",\r\nreloc->target_handle);\r\nreturn ret;\r\n}\r\nif (unlikely(reloc->write_domain & (reloc->write_domain - 1))) {\r\nDRM_ERROR("reloc with multiple write domains: "\r\n"obj %p target %d offset %d "\r\n"read %08x write %08x",\r\nobj, reloc->target_handle,\r\n(int) reloc->offset,\r\nreloc->read_domains,\r\nreloc->write_domain);\r\nreturn ret;\r\n}\r\nif (unlikely((reloc->write_domain | reloc->read_domains) & I915_GEM_DOMAIN_CPU)) {\r\nDRM_ERROR("reloc with read/write CPU domains: "\r\n"obj %p target %d offset %d "\r\n"read %08x write %08x",\r\nobj, reloc->target_handle,\r\n(int) reloc->offset,\r\nreloc->read_domains,\r\nreloc->write_domain);\r\nreturn ret;\r\n}\r\nif (unlikely(reloc->write_domain && target_obj->pending_write_domain &&\r\nreloc->write_domain != target_obj->pending_write_domain)) {\r\nDRM_ERROR("Write domain conflict: "\r\n"obj %p target %d offset %d "\r\n"new %08x old %08x\n",\r\nobj, reloc->target_handle,\r\n(int) reloc->offset,\r\nreloc->write_domain,\r\ntarget_obj->pending_write_domain);\r\nreturn ret;\r\n}\r\ntarget_obj->pending_read_domains |= reloc->read_domains;\r\ntarget_obj->pending_write_domain |= reloc->write_domain;\r\nif (target_offset == reloc->presumed_offset)\r\nreturn 0;\r\nif (unlikely(reloc->offset > obj->base.size - 4)) {\r\nDRM_ERROR("Relocation beyond object bounds: "\r\n"obj %p target %d offset %d size %d.\n",\r\nobj, reloc->target_handle,\r\n(int) reloc->offset,\r\n(int) obj->base.size);\r\nreturn ret;\r\n}\r\nif (unlikely(reloc->offset & 3)) {\r\nDRM_ERROR("Relocation not 4-byte aligned: "\r\n"obj %p target %d offset %d.\n",\r\nobj, reloc->target_handle,\r\n(int) reloc->offset);\r\nreturn ret;\r\n}\r\nreloc->delta += target_offset;\r\nif (obj->base.write_domain == I915_GEM_DOMAIN_CPU) {\r\nuint32_t page_offset = reloc->offset & ~PAGE_MASK;\r\nchar *vaddr;\r\nvaddr = kmap_atomic(obj->pages[reloc->offset >> PAGE_SHIFT]);\r\n*(uint32_t *)(vaddr + page_offset) = reloc->delta;\r\nkunmap_atomic(vaddr);\r\n} else {\r\nstruct drm_i915_private *dev_priv = dev->dev_private;\r\nuint32_t __iomem *reloc_entry;\r\nvoid __iomem *reloc_page;\r\nif (obj->active && in_atomic())\r\nreturn -EFAULT;\r\nret = i915_gem_object_set_to_gtt_domain(obj, 1);\r\nif (ret)\r\nreturn ret;\r\nreloc->offset += obj->gtt_offset;\r\nreloc_page = io_mapping_map_atomic_wc(dev_priv->mm.gtt_mapping,\r\nreloc->offset & PAGE_MASK);\r\nreloc_entry = (uint32_t __iomem *)\r\n(reloc_page + (reloc->offset & ~PAGE_MASK));\r\niowrite32(reloc->delta, reloc_entry);\r\nio_mapping_unmap_atomic(reloc_page);\r\n}\r\nreloc->presumed_offset = target_offset;\r\nreturn 0;\r\n}\r\nstatic int\r\ni915_gem_execbuffer_relocate_object(struct drm_i915_gem_object *obj,\r\nstruct eb_objects *eb)\r\n{\r\nstruct drm_i915_gem_relocation_entry __user *user_relocs;\r\nstruct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\r\nint i, ret;\r\nuser_relocs = (void __user *)(uintptr_t)entry->relocs_ptr;\r\nfor (i = 0; i < entry->relocation_count; i++) {\r\nstruct drm_i915_gem_relocation_entry reloc;\r\nif (__copy_from_user_inatomic(&reloc,\r\nuser_relocs+i,\r\nsizeof(reloc)))\r\nreturn -EFAULT;\r\nret = i915_gem_execbuffer_relocate_entry(obj, eb, &reloc);\r\nif (ret)\r\nreturn ret;\r\nif (__copy_to_user_inatomic(&user_relocs[i].presumed_offset,\r\n&reloc.presumed_offset,\r\nsizeof(reloc.presumed_offset)))\r\nreturn -EFAULT;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\ni915_gem_execbuffer_relocate_object_slow(struct drm_i915_gem_object *obj,\r\nstruct eb_objects *eb,\r\nstruct drm_i915_gem_relocation_entry *relocs)\r\n{\r\nconst struct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\r\nint i, ret;\r\nfor (i = 0; i < entry->relocation_count; i++) {\r\nret = i915_gem_execbuffer_relocate_entry(obj, eb, &relocs[i]);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\ni915_gem_execbuffer_relocate(struct drm_device *dev,\r\nstruct eb_objects *eb,\r\nstruct list_head *objects)\r\n{\r\nstruct drm_i915_gem_object *obj;\r\nint ret = 0;\r\npagefault_disable();\r\nlist_for_each_entry(obj, objects, exec_list) {\r\nret = i915_gem_execbuffer_relocate_object(obj, eb);\r\nif (ret)\r\nbreak;\r\n}\r\npagefault_enable();\r\nreturn ret;\r\n}\r\nstatic int\r\ni915_gem_execbuffer_reserve(struct intel_ring_buffer *ring,\r\nstruct drm_file *file,\r\nstruct list_head *objects)\r\n{\r\nstruct drm_i915_gem_object *obj;\r\nint ret, retry;\r\nbool has_fenced_gpu_access = INTEL_INFO(ring->dev)->gen < 4;\r\nstruct list_head ordered_objects;\r\nINIT_LIST_HEAD(&ordered_objects);\r\nwhile (!list_empty(objects)) {\r\nstruct drm_i915_gem_exec_object2 *entry;\r\nbool need_fence, need_mappable;\r\nobj = list_first_entry(objects,\r\nstruct drm_i915_gem_object,\r\nexec_list);\r\nentry = obj->exec_entry;\r\nneed_fence =\r\nhas_fenced_gpu_access &&\r\nentry->flags & EXEC_OBJECT_NEEDS_FENCE &&\r\nobj->tiling_mode != I915_TILING_NONE;\r\nneed_mappable =\r\nentry->relocation_count ? true : need_fence;\r\nif (need_mappable)\r\nlist_move(&obj->exec_list, &ordered_objects);\r\nelse\r\nlist_move_tail(&obj->exec_list, &ordered_objects);\r\nobj->base.pending_read_domains = 0;\r\nobj->base.pending_write_domain = 0;\r\n}\r\nlist_splice(&ordered_objects, objects);\r\nretry = 0;\r\ndo {\r\nret = 0;\r\nlist_for_each_entry(obj, objects, exec_list) {\r\nstruct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\r\nbool need_fence, need_mappable;\r\nif (!obj->gtt_space)\r\ncontinue;\r\nneed_fence =\r\nhas_fenced_gpu_access &&\r\nentry->flags & EXEC_OBJECT_NEEDS_FENCE &&\r\nobj->tiling_mode != I915_TILING_NONE;\r\nneed_mappable =\r\nentry->relocation_count ? true : need_fence;\r\nif ((entry->alignment && obj->gtt_offset & (entry->alignment - 1)) ||\r\n(need_mappable && !obj->map_and_fenceable))\r\nret = i915_gem_object_unbind(obj);\r\nelse\r\nret = i915_gem_object_pin(obj,\r\nentry->alignment,\r\nneed_mappable);\r\nif (ret)\r\ngoto err;\r\nentry++;\r\n}\r\nlist_for_each_entry(obj, objects, exec_list) {\r\nstruct drm_i915_gem_exec_object2 *entry = obj->exec_entry;\r\nbool need_fence;\r\nneed_fence =\r\nhas_fenced_gpu_access &&\r\nentry->flags & EXEC_OBJECT_NEEDS_FENCE &&\r\nobj->tiling_mode != I915_TILING_NONE;\r\nif (!obj->gtt_space) {\r\nbool need_mappable =\r\nentry->relocation_count ? true : need_fence;\r\nret = i915_gem_object_pin(obj,\r\nentry->alignment,\r\nneed_mappable);\r\nif (ret)\r\nbreak;\r\n}\r\nif (has_fenced_gpu_access) {\r\nif (need_fence) {\r\nret = i915_gem_object_get_fence(obj, ring);\r\nif (ret)\r\nbreak;\r\n} else if (entry->flags & EXEC_OBJECT_NEEDS_FENCE &&\r\nobj->tiling_mode == I915_TILING_NONE) {\r\nret = i915_gem_object_put_fence(obj);\r\nif (ret)\r\nbreak;\r\n}\r\nobj->pending_fenced_gpu_access = need_fence;\r\n}\r\nentry->offset = obj->gtt_offset;\r\n}\r\nlist_for_each_entry(obj, objects, exec_list) {\r\nif (obj->gtt_space)\r\ni915_gem_object_unpin(obj);\r\n}\r\nif (ret != -ENOSPC || retry > 1)\r\nreturn ret;\r\nret = i915_gem_evict_everything(ring->dev, retry == 0);\r\nif (ret)\r\nreturn ret;\r\nretry++;\r\n} while (1);\r\nerr:\r\nobj = list_entry(obj->exec_list.prev,\r\nstruct drm_i915_gem_object,\r\nexec_list);\r\nwhile (objects != &obj->exec_list) {\r\nif (obj->gtt_space)\r\ni915_gem_object_unpin(obj);\r\nobj = list_entry(obj->exec_list.prev,\r\nstruct drm_i915_gem_object,\r\nexec_list);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\ni915_gem_execbuffer_relocate_slow(struct drm_device *dev,\r\nstruct drm_file *file,\r\nstruct intel_ring_buffer *ring,\r\nstruct list_head *objects,\r\nstruct eb_objects *eb,\r\nstruct drm_i915_gem_exec_object2 *exec,\r\nint count)\r\n{\r\nstruct drm_i915_gem_relocation_entry *reloc;\r\nstruct drm_i915_gem_object *obj;\r\nint *reloc_offset;\r\nint i, total, ret;\r\nwhile (!list_empty(objects)) {\r\nobj = list_first_entry(objects,\r\nstruct drm_i915_gem_object,\r\nexec_list);\r\nlist_del_init(&obj->exec_list);\r\ndrm_gem_object_unreference(&obj->base);\r\n}\r\nmutex_unlock(&dev->struct_mutex);\r\ntotal = 0;\r\nfor (i = 0; i < count; i++)\r\ntotal += exec[i].relocation_count;\r\nreloc_offset = drm_malloc_ab(count, sizeof(*reloc_offset));\r\nreloc = drm_malloc_ab(total, sizeof(*reloc));\r\nif (reloc == NULL || reloc_offset == NULL) {\r\ndrm_free_large(reloc);\r\ndrm_free_large(reloc_offset);\r\nmutex_lock(&dev->struct_mutex);\r\nreturn -ENOMEM;\r\n}\r\ntotal = 0;\r\nfor (i = 0; i < count; i++) {\r\nstruct drm_i915_gem_relocation_entry __user *user_relocs;\r\nuser_relocs = (void __user *)(uintptr_t)exec[i].relocs_ptr;\r\nif (copy_from_user(reloc+total, user_relocs,\r\nexec[i].relocation_count * sizeof(*reloc))) {\r\nret = -EFAULT;\r\nmutex_lock(&dev->struct_mutex);\r\ngoto err;\r\n}\r\nreloc_offset[i] = total;\r\ntotal += exec[i].relocation_count;\r\n}\r\nret = i915_mutex_lock_interruptible(dev);\r\nif (ret) {\r\nmutex_lock(&dev->struct_mutex);\r\ngoto err;\r\n}\r\neb_reset(eb);\r\nfor (i = 0; i < count; i++) {\r\nobj = to_intel_bo(drm_gem_object_lookup(dev, file,\r\nexec[i].handle));\r\nif (&obj->base == NULL) {\r\nDRM_ERROR("Invalid object handle %d at index %d\n",\r\nexec[i].handle, i);\r\nret = -ENOENT;\r\ngoto err;\r\n}\r\nlist_add_tail(&obj->exec_list, objects);\r\nobj->exec_handle = exec[i].handle;\r\nobj->exec_entry = &exec[i];\r\neb_add_object(eb, obj);\r\n}\r\nret = i915_gem_execbuffer_reserve(ring, file, objects);\r\nif (ret)\r\ngoto err;\r\nlist_for_each_entry(obj, objects, exec_list) {\r\nint offset = obj->exec_entry - exec;\r\nret = i915_gem_execbuffer_relocate_object_slow(obj, eb,\r\nreloc + reloc_offset[offset]);\r\nif (ret)\r\ngoto err;\r\n}\r\nerr:\r\ndrm_free_large(reloc);\r\ndrm_free_large(reloc_offset);\r\nreturn ret;\r\n}\r\nstatic int\r\ni915_gem_execbuffer_flush(struct drm_device *dev,\r\nuint32_t invalidate_domains,\r\nuint32_t flush_domains,\r\nuint32_t flush_rings)\r\n{\r\ndrm_i915_private_t *dev_priv = dev->dev_private;\r\nint i, ret;\r\nif (flush_domains & I915_GEM_DOMAIN_CPU)\r\nintel_gtt_chipset_flush();\r\nif (flush_domains & I915_GEM_DOMAIN_GTT)\r\nwmb();\r\nif ((flush_domains | invalidate_domains) & I915_GEM_GPU_DOMAINS) {\r\nfor (i = 0; i < I915_NUM_RINGS; i++)\r\nif (flush_rings & (1 << i)) {\r\nret = i915_gem_flush_ring(&dev_priv->ring[i],\r\ninvalidate_domains,\r\nflush_domains);\r\nif (ret)\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\ni915_gem_execbuffer_sync_rings(struct drm_i915_gem_object *obj,\r\nstruct intel_ring_buffer *to)\r\n{\r\nstruct intel_ring_buffer *from = obj->ring;\r\nu32 seqno;\r\nint ret, idx;\r\nif (from == NULL || to == from)\r\nreturn 0;\r\nif (INTEL_INFO(obj->base.dev)->gen < 6 || !i915_semaphores)\r\nreturn i915_gem_object_wait_rendering(obj);\r\nidx = intel_ring_sync_index(from, to);\r\nseqno = obj->last_rendering_seqno;\r\nif (seqno <= from->sync_seqno[idx])\r\nreturn 0;\r\nif (seqno == from->outstanding_lazy_request) {\r\nstruct drm_i915_gem_request *request;\r\nrequest = kzalloc(sizeof(*request), GFP_KERNEL);\r\nif (request == NULL)\r\nreturn -ENOMEM;\r\nret = i915_add_request(from, NULL, request);\r\nif (ret) {\r\nkfree(request);\r\nreturn ret;\r\n}\r\nseqno = request->seqno;\r\n}\r\nfrom->sync_seqno[idx] = seqno;\r\nreturn intel_ring_sync(to, from, seqno - 1);\r\n}\r\nstatic int\r\ni915_gem_execbuffer_wait_for_flips(struct intel_ring_buffer *ring, u32 flips)\r\n{\r\nu32 plane, flip_mask;\r\nint ret;\r\nfor (plane = 0; flips >> plane; plane++) {\r\nif (((flips >> plane) & 1) == 0)\r\ncontinue;\r\nif (plane)\r\nflip_mask = MI_WAIT_FOR_PLANE_B_FLIP;\r\nelse\r\nflip_mask = MI_WAIT_FOR_PLANE_A_FLIP;\r\nret = intel_ring_begin(ring, 2);\r\nif (ret)\r\nreturn ret;\r\nintel_ring_emit(ring, MI_WAIT_FOR_EVENT | flip_mask);\r\nintel_ring_emit(ring, MI_NOOP);\r\nintel_ring_advance(ring);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\ni915_gem_execbuffer_move_to_gpu(struct intel_ring_buffer *ring,\r\nstruct list_head *objects)\r\n{\r\nstruct drm_i915_gem_object *obj;\r\nstruct change_domains cd;\r\nint ret;\r\nmemset(&cd, 0, sizeof(cd));\r\nlist_for_each_entry(obj, objects, exec_list)\r\ni915_gem_object_set_to_gpu_domain(obj, ring, &cd);\r\nif (cd.invalidate_domains | cd.flush_domains) {\r\nret = i915_gem_execbuffer_flush(ring->dev,\r\ncd.invalidate_domains,\r\ncd.flush_domains,\r\ncd.flush_rings);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (cd.flips) {\r\nret = i915_gem_execbuffer_wait_for_flips(ring, cd.flips);\r\nif (ret)\r\nreturn ret;\r\n}\r\nlist_for_each_entry(obj, objects, exec_list) {\r\nret = i915_gem_execbuffer_sync_rings(obj, ring);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic bool\r\ni915_gem_check_execbuffer(struct drm_i915_gem_execbuffer2 *exec)\r\n{\r\nreturn ((exec->batch_start_offset | exec->batch_len) & 0x7) == 0;\r\n}\r\nstatic int\r\nvalidate_exec_list(struct drm_i915_gem_exec_object2 *exec,\r\nint count)\r\n{\r\nint i;\r\nfor (i = 0; i < count; i++) {\r\nchar __user *ptr = (char __user *)(uintptr_t)exec[i].relocs_ptr;\r\nint length;\r\nif (exec[i].relocation_count >\r\nINT_MAX / sizeof(struct drm_i915_gem_relocation_entry))\r\nreturn -EINVAL;\r\nlength = exec[i].relocation_count *\r\nsizeof(struct drm_i915_gem_relocation_entry);\r\nif (!access_ok(VERIFY_READ, ptr, length))\r\nreturn -EFAULT;\r\nif (!access_ok(VERIFY_WRITE, ptr, length))\r\nreturn -EFAULT;\r\nif (fault_in_pages_readable(ptr, length))\r\nreturn -EFAULT;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\ni915_gem_execbuffer_move_to_active(struct list_head *objects,\r\nstruct intel_ring_buffer *ring,\r\nu32 seqno)\r\n{\r\nstruct drm_i915_gem_object *obj;\r\nlist_for_each_entry(obj, objects, exec_list) {\r\nu32 old_read = obj->base.read_domains;\r\nu32 old_write = obj->base.write_domain;\r\nobj->base.read_domains = obj->base.pending_read_domains;\r\nobj->base.write_domain = obj->base.pending_write_domain;\r\nobj->fenced_gpu_access = obj->pending_fenced_gpu_access;\r\ni915_gem_object_move_to_active(obj, ring, seqno);\r\nif (obj->base.write_domain) {\r\nobj->dirty = 1;\r\nobj->pending_gpu_write = true;\r\nlist_move_tail(&obj->gpu_write_list,\r\n&ring->gpu_write_list);\r\nintel_mark_busy(ring->dev, obj);\r\n}\r\ntrace_i915_gem_object_change_domain(obj, old_read, old_write);\r\n}\r\n}\r\nstatic void\r\ni915_gem_execbuffer_retire_commands(struct drm_device *dev,\r\nstruct drm_file *file,\r\nstruct intel_ring_buffer *ring)\r\n{\r\nstruct drm_i915_gem_request *request;\r\nu32 invalidate;\r\ninvalidate = I915_GEM_DOMAIN_COMMAND;\r\nif (INTEL_INFO(dev)->gen >= 4)\r\ninvalidate |= I915_GEM_DOMAIN_SAMPLER;\r\nif (ring->flush(ring, invalidate, 0)) {\r\ni915_gem_next_request_seqno(ring);\r\nreturn;\r\n}\r\nrequest = kzalloc(sizeof(*request), GFP_KERNEL);\r\nif (request == NULL || i915_add_request(ring, file, request)) {\r\ni915_gem_next_request_seqno(ring);\r\nkfree(request);\r\n}\r\n}\r\nstatic int\r\ni915_gem_do_execbuffer(struct drm_device *dev, void *data,\r\nstruct drm_file *file,\r\nstruct drm_i915_gem_execbuffer2 *args,\r\nstruct drm_i915_gem_exec_object2 *exec)\r\n{\r\ndrm_i915_private_t *dev_priv = dev->dev_private;\r\nstruct list_head objects;\r\nstruct eb_objects *eb;\r\nstruct drm_i915_gem_object *batch_obj;\r\nstruct drm_clip_rect *cliprects = NULL;\r\nstruct intel_ring_buffer *ring;\r\nu32 exec_start, exec_len;\r\nu32 seqno;\r\nint ret, mode, i;\r\nif (!i915_gem_check_execbuffer(args)) {\r\nDRM_ERROR("execbuf with invalid offset/length\n");\r\nreturn -EINVAL;\r\n}\r\nret = validate_exec_list(exec, args->buffer_count);\r\nif (ret)\r\nreturn ret;\r\nswitch (args->flags & I915_EXEC_RING_MASK) {\r\ncase I915_EXEC_DEFAULT:\r\ncase I915_EXEC_RENDER:\r\nring = &dev_priv->ring[RCS];\r\nbreak;\r\ncase I915_EXEC_BSD:\r\nif (!HAS_BSD(dev)) {\r\nDRM_ERROR("execbuf with invalid ring (BSD)\n");\r\nreturn -EINVAL;\r\n}\r\nring = &dev_priv->ring[VCS];\r\nbreak;\r\ncase I915_EXEC_BLT:\r\nif (!HAS_BLT(dev)) {\r\nDRM_ERROR("execbuf with invalid ring (BLT)\n");\r\nreturn -EINVAL;\r\n}\r\nring = &dev_priv->ring[BCS];\r\nbreak;\r\ndefault:\r\nDRM_ERROR("execbuf with unknown ring: %d\n",\r\n(int)(args->flags & I915_EXEC_RING_MASK));\r\nreturn -EINVAL;\r\n}\r\nmode = args->flags & I915_EXEC_CONSTANTS_MASK;\r\nswitch (mode) {\r\ncase I915_EXEC_CONSTANTS_REL_GENERAL:\r\ncase I915_EXEC_CONSTANTS_ABSOLUTE:\r\ncase I915_EXEC_CONSTANTS_REL_SURFACE:\r\nif (ring == &dev_priv->ring[RCS] &&\r\nmode != dev_priv->relative_constants_mode) {\r\nif (INTEL_INFO(dev)->gen < 4)\r\nreturn -EINVAL;\r\nif (INTEL_INFO(dev)->gen > 5 &&\r\nmode == I915_EXEC_CONSTANTS_REL_SURFACE)\r\nreturn -EINVAL;\r\nret = intel_ring_begin(ring, 4);\r\nif (ret)\r\nreturn ret;\r\nintel_ring_emit(ring, MI_NOOP);\r\nintel_ring_emit(ring, MI_LOAD_REGISTER_IMM(1));\r\nintel_ring_emit(ring, INSTPM);\r\nintel_ring_emit(ring,\r\nI915_EXEC_CONSTANTS_MASK << 16 | mode);\r\nintel_ring_advance(ring);\r\ndev_priv->relative_constants_mode = mode;\r\n}\r\nbreak;\r\ndefault:\r\nDRM_ERROR("execbuf with unknown constants: %d\n", mode);\r\nreturn -EINVAL;\r\n}\r\nif (args->buffer_count < 1) {\r\nDRM_ERROR("execbuf with %d buffers\n", args->buffer_count);\r\nreturn -EINVAL;\r\n}\r\nif (args->num_cliprects != 0) {\r\nif (ring != &dev_priv->ring[RCS]) {\r\nDRM_ERROR("clip rectangles are only valid with the render ring\n");\r\nreturn -EINVAL;\r\n}\r\ncliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),\r\nGFP_KERNEL);\r\nif (cliprects == NULL) {\r\nret = -ENOMEM;\r\ngoto pre_mutex_err;\r\n}\r\nif (copy_from_user(cliprects,\r\n(struct drm_clip_rect __user *)(uintptr_t)\r\nargs->cliprects_ptr,\r\nsizeof(*cliprects)*args->num_cliprects)) {\r\nret = -EFAULT;\r\ngoto pre_mutex_err;\r\n}\r\n}\r\nret = i915_mutex_lock_interruptible(dev);\r\nif (ret)\r\ngoto pre_mutex_err;\r\nif (dev_priv->mm.suspended) {\r\nmutex_unlock(&dev->struct_mutex);\r\nret = -EBUSY;\r\ngoto pre_mutex_err;\r\n}\r\neb = eb_create(args->buffer_count);\r\nif (eb == NULL) {\r\nmutex_unlock(&dev->struct_mutex);\r\nret = -ENOMEM;\r\ngoto pre_mutex_err;\r\n}\r\nINIT_LIST_HEAD(&objects);\r\nfor (i = 0; i < args->buffer_count; i++) {\r\nstruct drm_i915_gem_object *obj;\r\nobj = to_intel_bo(drm_gem_object_lookup(dev, file,\r\nexec[i].handle));\r\nif (&obj->base == NULL) {\r\nDRM_ERROR("Invalid object handle %d at index %d\n",\r\nexec[i].handle, i);\r\nret = -ENOENT;\r\ngoto err;\r\n}\r\nif (!list_empty(&obj->exec_list)) {\r\nDRM_ERROR("Object %p [handle %d, index %d] appears more than once in object list\n",\r\nobj, exec[i].handle, i);\r\nret = -EINVAL;\r\ngoto err;\r\n}\r\nlist_add_tail(&obj->exec_list, &objects);\r\nobj->exec_handle = exec[i].handle;\r\nobj->exec_entry = &exec[i];\r\neb_add_object(eb, obj);\r\n}\r\nbatch_obj = list_entry(objects.prev,\r\nstruct drm_i915_gem_object,\r\nexec_list);\r\nret = i915_gem_execbuffer_reserve(ring, file, &objects);\r\nif (ret)\r\ngoto err;\r\nret = i915_gem_execbuffer_relocate(dev, eb, &objects);\r\nif (ret) {\r\nif (ret == -EFAULT) {\r\nret = i915_gem_execbuffer_relocate_slow(dev, file, ring,\r\n&objects, eb,\r\nexec,\r\nargs->buffer_count);\r\nBUG_ON(!mutex_is_locked(&dev->struct_mutex));\r\n}\r\nif (ret)\r\ngoto err;\r\n}\r\nif (batch_obj->base.pending_write_domain) {\r\nDRM_ERROR("Attempting to use self-modifying batch buffer\n");\r\nret = -EINVAL;\r\ngoto err;\r\n}\r\nbatch_obj->base.pending_read_domains |= I915_GEM_DOMAIN_COMMAND;\r\nret = i915_gem_execbuffer_move_to_gpu(ring, &objects);\r\nif (ret)\r\ngoto err;\r\nseqno = i915_gem_next_request_seqno(ring);\r\nfor (i = 0; i < ARRAY_SIZE(ring->sync_seqno); i++) {\r\nif (seqno < ring->sync_seqno[i]) {\r\nret = i915_gpu_idle(dev);\r\nif (ret)\r\ngoto err;\r\nBUG_ON(ring->sync_seqno[i]);\r\n}\r\n}\r\ntrace_i915_gem_ring_dispatch(ring, seqno);\r\nexec_start = batch_obj->gtt_offset + args->batch_start_offset;\r\nexec_len = args->batch_len;\r\nif (cliprects) {\r\nfor (i = 0; i < args->num_cliprects; i++) {\r\nret = i915_emit_box(dev, &cliprects[i],\r\nargs->DR1, args->DR4);\r\nif (ret)\r\ngoto err;\r\nret = ring->dispatch_execbuffer(ring,\r\nexec_start, exec_len);\r\nif (ret)\r\ngoto err;\r\n}\r\n} else {\r\nret = ring->dispatch_execbuffer(ring, exec_start, exec_len);\r\nif (ret)\r\ngoto err;\r\n}\r\ni915_gem_execbuffer_move_to_active(&objects, ring, seqno);\r\ni915_gem_execbuffer_retire_commands(dev, file, ring);\r\nerr:\r\neb_destroy(eb);\r\nwhile (!list_empty(&objects)) {\r\nstruct drm_i915_gem_object *obj;\r\nobj = list_first_entry(&objects,\r\nstruct drm_i915_gem_object,\r\nexec_list);\r\nlist_del_init(&obj->exec_list);\r\ndrm_gem_object_unreference(&obj->base);\r\n}\r\nmutex_unlock(&dev->struct_mutex);\r\npre_mutex_err:\r\nkfree(cliprects);\r\nreturn ret;\r\n}\r\nint\r\ni915_gem_execbuffer(struct drm_device *dev, void *data,\r\nstruct drm_file *file)\r\n{\r\nstruct drm_i915_gem_execbuffer *args = data;\r\nstruct drm_i915_gem_execbuffer2 exec2;\r\nstruct drm_i915_gem_exec_object *exec_list = NULL;\r\nstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\r\nint ret, i;\r\nif (args->buffer_count < 1) {\r\nDRM_ERROR("execbuf with %d buffers\n", args->buffer_count);\r\nreturn -EINVAL;\r\n}\r\nexec_list = drm_malloc_ab(sizeof(*exec_list), args->buffer_count);\r\nexec2_list = drm_malloc_ab(sizeof(*exec2_list), args->buffer_count);\r\nif (exec_list == NULL || exec2_list == NULL) {\r\nDRM_ERROR("Failed to allocate exec list for %d buffers\n",\r\nargs->buffer_count);\r\ndrm_free_large(exec_list);\r\ndrm_free_large(exec2_list);\r\nreturn -ENOMEM;\r\n}\r\nret = copy_from_user(exec_list,\r\n(struct drm_i915_relocation_entry __user *)\r\n(uintptr_t) args->buffers_ptr,\r\nsizeof(*exec_list) * args->buffer_count);\r\nif (ret != 0) {\r\nDRM_ERROR("copy %d exec entries failed %d\n",\r\nargs->buffer_count, ret);\r\ndrm_free_large(exec_list);\r\ndrm_free_large(exec2_list);\r\nreturn -EFAULT;\r\n}\r\nfor (i = 0; i < args->buffer_count; i++) {\r\nexec2_list[i].handle = exec_list[i].handle;\r\nexec2_list[i].relocation_count = exec_list[i].relocation_count;\r\nexec2_list[i].relocs_ptr = exec_list[i].relocs_ptr;\r\nexec2_list[i].alignment = exec_list[i].alignment;\r\nexec2_list[i].offset = exec_list[i].offset;\r\nif (INTEL_INFO(dev)->gen < 4)\r\nexec2_list[i].flags = EXEC_OBJECT_NEEDS_FENCE;\r\nelse\r\nexec2_list[i].flags = 0;\r\n}\r\nexec2.buffers_ptr = args->buffers_ptr;\r\nexec2.buffer_count = args->buffer_count;\r\nexec2.batch_start_offset = args->batch_start_offset;\r\nexec2.batch_len = args->batch_len;\r\nexec2.DR1 = args->DR1;\r\nexec2.DR4 = args->DR4;\r\nexec2.num_cliprects = args->num_cliprects;\r\nexec2.cliprects_ptr = args->cliprects_ptr;\r\nexec2.flags = I915_EXEC_RENDER;\r\nret = i915_gem_do_execbuffer(dev, data, file, &exec2, exec2_list);\r\nif (!ret) {\r\nfor (i = 0; i < args->buffer_count; i++)\r\nexec_list[i].offset = exec2_list[i].offset;\r\nret = copy_to_user((struct drm_i915_relocation_entry __user *)\r\n(uintptr_t) args->buffers_ptr,\r\nexec_list,\r\nsizeof(*exec_list) * args->buffer_count);\r\nif (ret) {\r\nret = -EFAULT;\r\nDRM_ERROR("failed to copy %d exec entries "\r\n"back to user (%d)\n",\r\nargs->buffer_count, ret);\r\n}\r\n}\r\ndrm_free_large(exec_list);\r\ndrm_free_large(exec2_list);\r\nreturn ret;\r\n}\r\nint\r\ni915_gem_execbuffer2(struct drm_device *dev, void *data,\r\nstruct drm_file *file)\r\n{\r\nstruct drm_i915_gem_execbuffer2 *args = data;\r\nstruct drm_i915_gem_exec_object2 *exec2_list = NULL;\r\nint ret;\r\nif (args->buffer_count < 1) {\r\nDRM_ERROR("execbuf2 with %d buffers\n", args->buffer_count);\r\nreturn -EINVAL;\r\n}\r\nexec2_list = kmalloc(sizeof(*exec2_list)*args->buffer_count,\r\nGFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY);\r\nif (exec2_list == NULL)\r\nexec2_list = drm_malloc_ab(sizeof(*exec2_list),\r\nargs->buffer_count);\r\nif (exec2_list == NULL) {\r\nDRM_ERROR("Failed to allocate exec list for %d buffers\n",\r\nargs->buffer_count);\r\nreturn -ENOMEM;\r\n}\r\nret = copy_from_user(exec2_list,\r\n(struct drm_i915_relocation_entry __user *)\r\n(uintptr_t) args->buffers_ptr,\r\nsizeof(*exec2_list) * args->buffer_count);\r\nif (ret != 0) {\r\nDRM_ERROR("copy %d exec entries failed %d\n",\r\nargs->buffer_count, ret);\r\ndrm_free_large(exec2_list);\r\nreturn -EFAULT;\r\n}\r\nret = i915_gem_do_execbuffer(dev, data, file, args, exec2_list);\r\nif (!ret) {\r\nret = copy_to_user((struct drm_i915_relocation_entry __user *)\r\n(uintptr_t) args->buffers_ptr,\r\nexec2_list,\r\nsizeof(*exec2_list) * args->buffer_count);\r\nif (ret) {\r\nret = -EFAULT;\r\nDRM_ERROR("failed to copy %d exec entries "\r\n"back to user (%d)\n",\r\nargs->buffer_count, ret);\r\n}\r\n}\r\ndrm_free_large(exec2_list);\r\nreturn ret;\r\n}
