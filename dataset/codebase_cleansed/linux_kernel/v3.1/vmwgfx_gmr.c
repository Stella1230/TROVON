static int vmw_gmr_build_descriptors(struct list_head *desc_pages,\r\nstruct page *pages[],\r\nunsigned long num_pages)\r\n{\r\nstruct page *page, *next;\r\nstruct svga_guest_mem_descriptor *page_virtual = NULL;\r\nstruct svga_guest_mem_descriptor *desc_virtual = NULL;\r\nunsigned int desc_per_page;\r\nunsigned long prev_pfn;\r\nunsigned long pfn;\r\nint ret;\r\ndesc_per_page = PAGE_SIZE /\r\nsizeof(struct svga_guest_mem_descriptor) - 1;\r\nwhile (likely(num_pages != 0)) {\r\npage = alloc_page(__GFP_HIGHMEM);\r\nif (unlikely(page == NULL)) {\r\nret = -ENOMEM;\r\ngoto out_err;\r\n}\r\nlist_add_tail(&page->lru, desc_pages);\r\nif (likely(page_virtual != NULL)) {\r\ndesc_virtual->ppn = page_to_pfn(page);\r\nkunmap_atomic(page_virtual, KM_USER0);\r\n}\r\npage_virtual = kmap_atomic(page, KM_USER0);\r\ndesc_virtual = page_virtual - 1;\r\nprev_pfn = ~(0UL);\r\nwhile (likely(num_pages != 0)) {\r\npfn = page_to_pfn(*pages);\r\nif (pfn != prev_pfn + 1) {\r\nif (desc_virtual - page_virtual ==\r\ndesc_per_page - 1)\r\nbreak;\r\n(++desc_virtual)->ppn = cpu_to_le32(pfn);\r\ndesc_virtual->num_pages = cpu_to_le32(1);\r\n} else {\r\nuint32_t tmp =\r\nle32_to_cpu(desc_virtual->num_pages);\r\ndesc_virtual->num_pages = cpu_to_le32(tmp + 1);\r\n}\r\nprev_pfn = pfn;\r\n--num_pages;\r\n++pages;\r\n}\r\n(++desc_virtual)->ppn = cpu_to_le32(0);\r\ndesc_virtual->num_pages = cpu_to_le32(0);\r\n}\r\nif (likely(page_virtual != NULL))\r\nkunmap_atomic(page_virtual, KM_USER0);\r\nreturn 0;\r\nout_err:\r\nlist_for_each_entry_safe(page, next, desc_pages, lru) {\r\nlist_del_init(&page->lru);\r\n__free_page(page);\r\n}\r\nreturn ret;\r\n}\r\nstatic inline void vmw_gmr_free_descriptors(struct list_head *desc_pages)\r\n{\r\nstruct page *page, *next;\r\nlist_for_each_entry_safe(page, next, desc_pages, lru) {\r\nlist_del_init(&page->lru);\r\n__free_page(page);\r\n}\r\n}\r\nstatic void vmw_gmr_fire_descriptors(struct vmw_private *dev_priv,\r\nint gmr_id, struct list_head *desc_pages)\r\n{\r\nstruct page *page;\r\nif (unlikely(list_empty(desc_pages)))\r\nreturn;\r\npage = list_entry(desc_pages->next, struct page, lru);\r\nmutex_lock(&dev_priv->hw_mutex);\r\nvmw_write(dev_priv, SVGA_REG_GMR_ID, gmr_id);\r\nwmb();\r\nvmw_write(dev_priv, SVGA_REG_GMR_DESCRIPTOR, page_to_pfn(page));\r\nmb();\r\nmutex_unlock(&dev_priv->hw_mutex);\r\n}\r\nstatic unsigned long vmw_gmr_count_descriptors(struct page *pages[],\r\nunsigned long num_pages)\r\n{\r\nunsigned long prev_pfn = ~(0UL);\r\nunsigned long pfn;\r\nunsigned long descriptors = 0;\r\nwhile (num_pages--) {\r\npfn = page_to_pfn(*pages++);\r\nif (prev_pfn + 1 != pfn)\r\n++descriptors;\r\nprev_pfn = pfn;\r\n}\r\nreturn descriptors;\r\n}\r\nint vmw_gmr_bind(struct vmw_private *dev_priv,\r\nstruct page *pages[],\r\nunsigned long num_pages,\r\nint gmr_id)\r\n{\r\nstruct list_head desc_pages;\r\nint ret;\r\nif (unlikely(!(dev_priv->capabilities & SVGA_CAP_GMR)))\r\nreturn -EINVAL;\r\nif (vmw_gmr_count_descriptors(pages, num_pages) >\r\ndev_priv->max_gmr_descriptors)\r\nreturn -EINVAL;\r\nINIT_LIST_HEAD(&desc_pages);\r\nret = vmw_gmr_build_descriptors(&desc_pages, pages, num_pages);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nvmw_gmr_fire_descriptors(dev_priv, gmr_id, &desc_pages);\r\nvmw_gmr_free_descriptors(&desc_pages);\r\nreturn 0;\r\n}\r\nvoid vmw_gmr_unbind(struct vmw_private *dev_priv, int gmr_id)\r\n{\r\nmutex_lock(&dev_priv->hw_mutex);\r\nvmw_write(dev_priv, SVGA_REG_GMR_ID, gmr_id);\r\nwmb();\r\nvmw_write(dev_priv, SVGA_REG_GMR_DESCRIPTOR, 0);\r\nmb();\r\nmutex_unlock(&dev_priv->hw_mutex);\r\n}
