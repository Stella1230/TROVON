static int vmw_ttm_populate(struct ttm_backend *backend,\r\nunsigned long num_pages, struct page **pages,\r\nstruct page *dummy_read_page,\r\ndma_addr_t *dma_addrs)\r\n{\r\nstruct vmw_ttm_backend *vmw_be =\r\ncontainer_of(backend, struct vmw_ttm_backend, backend);\r\nvmw_be->pages = pages;\r\nvmw_be->num_pages = num_pages;\r\nreturn 0;\r\n}\r\nstatic int vmw_ttm_bind(struct ttm_backend *backend, struct ttm_mem_reg *bo_mem)\r\n{\r\nstruct vmw_ttm_backend *vmw_be =\r\ncontainer_of(backend, struct vmw_ttm_backend, backend);\r\nvmw_be->gmr_id = bo_mem->start;\r\nreturn vmw_gmr_bind(vmw_be->dev_priv, vmw_be->pages,\r\nvmw_be->num_pages, vmw_be->gmr_id);\r\n}\r\nstatic int vmw_ttm_unbind(struct ttm_backend *backend)\r\n{\r\nstruct vmw_ttm_backend *vmw_be =\r\ncontainer_of(backend, struct vmw_ttm_backend, backend);\r\nvmw_gmr_unbind(vmw_be->dev_priv, vmw_be->gmr_id);\r\nreturn 0;\r\n}\r\nstatic void vmw_ttm_clear(struct ttm_backend *backend)\r\n{\r\nstruct vmw_ttm_backend *vmw_be =\r\ncontainer_of(backend, struct vmw_ttm_backend, backend);\r\nvmw_be->pages = NULL;\r\nvmw_be->num_pages = 0;\r\n}\r\nstatic void vmw_ttm_destroy(struct ttm_backend *backend)\r\n{\r\nstruct vmw_ttm_backend *vmw_be =\r\ncontainer_of(backend, struct vmw_ttm_backend, backend);\r\nkfree(vmw_be);\r\n}\r\nstruct ttm_backend *vmw_ttm_backend_init(struct ttm_bo_device *bdev)\r\n{\r\nstruct vmw_ttm_backend *vmw_be;\r\nvmw_be = kmalloc(sizeof(*vmw_be), GFP_KERNEL);\r\nif (!vmw_be)\r\nreturn NULL;\r\nvmw_be->backend.func = &vmw_ttm_func;\r\nvmw_be->dev_priv = container_of(bdev, struct vmw_private, bdev);\r\nreturn &vmw_be->backend;\r\n}\r\nint vmw_invalidate_caches(struct ttm_bo_device *bdev, uint32_t flags)\r\n{\r\nreturn 0;\r\n}\r\nint vmw_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,\r\nstruct ttm_mem_type_manager *man)\r\n{\r\nswitch (type) {\r\ncase TTM_PL_SYSTEM:\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_CACHED;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ncase TTM_PL_VRAM:\r\nman->func = &ttm_bo_manager_func;\r\nman->gpu_offset = 0;\r\nman->flags = TTM_MEMTYPE_FLAG_FIXED | TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_CACHED;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ncase VMW_PL_GMR:\r\nman->func = &vmw_gmrid_manager_func;\r\nman->gpu_offset = 0;\r\nman->flags = TTM_MEMTYPE_FLAG_CMA | TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_CACHED;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Unsupported memory type %u\n", (unsigned)type);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nvoid vmw_evict_flags(struct ttm_buffer_object *bo,\r\nstruct ttm_placement *placement)\r\n{\r\n*placement = vmw_sys_placement;\r\n}\r\nstatic int vmw_verify_access(struct ttm_buffer_object *bo, struct file *filp)\r\n{\r\nreturn 0;\r\n}\r\nstatic int vmw_ttm_io_mem_reserve(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)\r\n{\r\nstruct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];\r\nstruct vmw_private *dev_priv = container_of(bdev, struct vmw_private, bdev);\r\nmem->bus.addr = NULL;\r\nmem->bus.is_iomem = false;\r\nmem->bus.offset = 0;\r\nmem->bus.size = mem->num_pages << PAGE_SHIFT;\r\nmem->bus.base = 0;\r\nif (!(man->flags & TTM_MEMTYPE_FLAG_MAPPABLE))\r\nreturn -EINVAL;\r\nswitch (mem->mem_type) {\r\ncase TTM_PL_SYSTEM:\r\ncase VMW_PL_GMR:\r\nreturn 0;\r\ncase TTM_PL_VRAM:\r\nmem->bus.offset = mem->start << PAGE_SHIFT;\r\nmem->bus.base = dev_priv->vram_start;\r\nmem->bus.is_iomem = true;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void vmw_ttm_io_mem_free(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)\r\n{\r\n}\r\nstatic int vmw_ttm_fault_reserve_notify(struct ttm_buffer_object *bo)\r\n{\r\nreturn 0;\r\n}\r\nstatic void *vmw_sync_obj_ref(void *sync_obj)\r\n{\r\nreturn sync_obj;\r\n}\r\nstatic void vmw_sync_obj_unref(void **sync_obj)\r\n{\r\n*sync_obj = NULL;\r\n}\r\nstatic int vmw_sync_obj_flush(void *sync_obj, void *sync_arg)\r\n{\r\nstruct vmw_private *dev_priv = (struct vmw_private *)sync_arg;\r\nmutex_lock(&dev_priv->hw_mutex);\r\nvmw_write(dev_priv, SVGA_REG_SYNC, SVGA_SYNC_GENERIC);\r\nmutex_unlock(&dev_priv->hw_mutex);\r\nreturn 0;\r\n}\r\nstatic bool vmw_sync_obj_signaled(void *sync_obj, void *sync_arg)\r\n{\r\nstruct vmw_private *dev_priv = (struct vmw_private *)sync_arg;\r\nuint32_t sequence = (unsigned long) sync_obj;\r\nreturn vmw_fence_signaled(dev_priv, sequence);\r\n}\r\nstatic int vmw_sync_obj_wait(void *sync_obj, void *sync_arg,\r\nbool lazy, bool interruptible)\r\n{\r\nstruct vmw_private *dev_priv = (struct vmw_private *)sync_arg;\r\nuint32_t sequence = (unsigned long) sync_obj;\r\nreturn vmw_wait_fence(dev_priv, false, sequence, false, 3*HZ);\r\n}
