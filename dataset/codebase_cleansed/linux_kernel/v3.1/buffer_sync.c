static int\r\ntask_free_notify(struct notifier_block *self, unsigned long val, void *data)\r\n{\r\nunsigned long flags;\r\nstruct task_struct *task = data;\r\nspin_lock_irqsave(&task_mortuary, flags);\r\nlist_add(&task->tasks, &dying_tasks);\r\nspin_unlock_irqrestore(&task_mortuary, flags);\r\nreturn NOTIFY_OK;\r\n}\r\nstatic int\r\ntask_exit_notify(struct notifier_block *self, unsigned long val, void *data)\r\n{\r\nsync_buffer(raw_smp_processor_id());\r\nreturn 0;\r\n}\r\nstatic int\r\nmunmap_notify(struct notifier_block *self, unsigned long val, void *data)\r\n{\r\nunsigned long addr = (unsigned long)data;\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *mpnt;\r\ndown_read(&mm->mmap_sem);\r\nmpnt = find_vma(mm, addr);\r\nif (mpnt && mpnt->vm_file && (mpnt->vm_flags & VM_EXEC)) {\r\nup_read(&mm->mmap_sem);\r\nsync_buffer(raw_smp_processor_id());\r\nreturn 0;\r\n}\r\nup_read(&mm->mmap_sem);\r\nreturn 0;\r\n}\r\nstatic int\r\nmodule_load_notify(struct notifier_block *self, unsigned long val, void *data)\r\n{\r\n#ifdef CONFIG_MODULES\r\nif (val != MODULE_STATE_COMING)\r\nreturn 0;\r\nmutex_lock(&buffer_mutex);\r\nadd_event_entry(ESCAPE_CODE);\r\nadd_event_entry(MODULE_LOADED_CODE);\r\nmutex_unlock(&buffer_mutex);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void free_all_tasks(void)\r\n{\r\nprocess_task_mortuary();\r\nprocess_task_mortuary();\r\n}\r\nint sync_start(void)\r\n{\r\nint err;\r\nif (!zalloc_cpumask_var(&marked_cpus, GFP_KERNEL))\r\nreturn -ENOMEM;\r\nerr = task_handoff_register(&task_free_nb);\r\nif (err)\r\ngoto out1;\r\nerr = profile_event_register(PROFILE_TASK_EXIT, &task_exit_nb);\r\nif (err)\r\ngoto out2;\r\nerr = profile_event_register(PROFILE_MUNMAP, &munmap_nb);\r\nif (err)\r\ngoto out3;\r\nerr = register_module_notifier(&module_load_nb);\r\nif (err)\r\ngoto out4;\r\nstart_cpu_work();\r\nout:\r\nreturn err;\r\nout4:\r\nprofile_event_unregister(PROFILE_MUNMAP, &munmap_nb);\r\nout3:\r\nprofile_event_unregister(PROFILE_TASK_EXIT, &task_exit_nb);\r\nout2:\r\ntask_handoff_unregister(&task_free_nb);\r\nfree_all_tasks();\r\nout1:\r\nfree_cpumask_var(marked_cpus);\r\ngoto out;\r\n}\r\nvoid sync_stop(void)\r\n{\r\nend_cpu_work();\r\nunregister_module_notifier(&module_load_nb);\r\nprofile_event_unregister(PROFILE_MUNMAP, &munmap_nb);\r\nprofile_event_unregister(PROFILE_TASK_EXIT, &task_exit_nb);\r\ntask_handoff_unregister(&task_free_nb);\r\nbarrier();\r\nflush_cpu_work();\r\nfree_all_tasks();\r\nfree_cpumask_var(marked_cpus);\r\n}\r\nstatic inline unsigned long fast_get_dcookie(struct path *path)\r\n{\r\nunsigned long cookie;\r\nif (path->dentry->d_flags & DCACHE_COOKIE)\r\nreturn (unsigned long)path->dentry;\r\nget_dcookie(path, &cookie);\r\nreturn cookie;\r\n}\r\nstatic unsigned long get_exec_dcookie(struct mm_struct *mm)\r\n{\r\nunsigned long cookie = NO_COOKIE;\r\nstruct vm_area_struct *vma;\r\nif (!mm)\r\ngoto out;\r\nfor (vma = mm->mmap; vma; vma = vma->vm_next) {\r\nif (!vma->vm_file)\r\ncontinue;\r\nif (!(vma->vm_flags & VM_EXECUTABLE))\r\ncontinue;\r\ncookie = fast_get_dcookie(&vma->vm_file->f_path);\r\nbreak;\r\n}\r\nout:\r\nreturn cookie;\r\n}\r\nstatic unsigned long\r\nlookup_dcookie(struct mm_struct *mm, unsigned long addr, off_t *offset)\r\n{\r\nunsigned long cookie = NO_COOKIE;\r\nstruct vm_area_struct *vma;\r\nfor (vma = find_vma(mm, addr); vma; vma = vma->vm_next) {\r\nif (addr < vma->vm_start || addr >= vma->vm_end)\r\ncontinue;\r\nif (vma->vm_file) {\r\ncookie = fast_get_dcookie(&vma->vm_file->f_path);\r\n*offset = (vma->vm_pgoff << PAGE_SHIFT) + addr -\r\nvma->vm_start;\r\n} else {\r\n*offset = addr;\r\n}\r\nbreak;\r\n}\r\nif (!vma)\r\ncookie = INVALID_COOKIE;\r\nreturn cookie;\r\n}\r\nstatic void add_cpu_switch(int i)\r\n{\r\nadd_event_entry(ESCAPE_CODE);\r\nadd_event_entry(CPU_SWITCH_CODE);\r\nadd_event_entry(i);\r\nlast_cookie = INVALID_COOKIE;\r\n}\r\nstatic void add_kernel_ctx_switch(unsigned int in_kernel)\r\n{\r\nadd_event_entry(ESCAPE_CODE);\r\nif (in_kernel)\r\nadd_event_entry(KERNEL_ENTER_SWITCH_CODE);\r\nelse\r\nadd_event_entry(KERNEL_EXIT_SWITCH_CODE);\r\n}\r\nstatic void\r\nadd_user_ctx_switch(struct task_struct const *task, unsigned long cookie)\r\n{\r\nadd_event_entry(ESCAPE_CODE);\r\nadd_event_entry(CTX_SWITCH_CODE);\r\nadd_event_entry(task->pid);\r\nadd_event_entry(cookie);\r\nadd_event_entry(ESCAPE_CODE);\r\nadd_event_entry(CTX_TGID_CODE);\r\nadd_event_entry(task->tgid);\r\n}\r\nstatic void add_cookie_switch(unsigned long cookie)\r\n{\r\nadd_event_entry(ESCAPE_CODE);\r\nadd_event_entry(COOKIE_SWITCH_CODE);\r\nadd_event_entry(cookie);\r\n}\r\nstatic void add_trace_begin(void)\r\n{\r\nadd_event_entry(ESCAPE_CODE);\r\nadd_event_entry(TRACE_BEGIN_CODE);\r\n}\r\nstatic void add_data(struct op_entry *entry, struct mm_struct *mm)\r\n{\r\nunsigned long code, pc, val;\r\nunsigned long cookie;\r\noff_t offset;\r\nif (!op_cpu_buffer_get_data(entry, &code))\r\nreturn;\r\nif (!op_cpu_buffer_get_data(entry, &pc))\r\nreturn;\r\nif (!op_cpu_buffer_get_size(entry))\r\nreturn;\r\nif (mm) {\r\ncookie = lookup_dcookie(mm, pc, &offset);\r\nif (cookie == NO_COOKIE)\r\noffset = pc;\r\nif (cookie == INVALID_COOKIE) {\r\natomic_inc(&oprofile_stats.sample_lost_no_mapping);\r\noffset = pc;\r\n}\r\nif (cookie != last_cookie) {\r\nadd_cookie_switch(cookie);\r\nlast_cookie = cookie;\r\n}\r\n} else\r\noffset = pc;\r\nadd_event_entry(ESCAPE_CODE);\r\nadd_event_entry(code);\r\nadd_event_entry(offset);\r\nwhile (op_cpu_buffer_get_data(entry, &val))\r\nadd_event_entry(val);\r\n}\r\nstatic inline void add_sample_entry(unsigned long offset, unsigned long event)\r\n{\r\nadd_event_entry(offset);\r\nadd_event_entry(event);\r\n}\r\nstatic int\r\nadd_sample(struct mm_struct *mm, struct op_sample *s, int in_kernel)\r\n{\r\nunsigned long cookie;\r\noff_t offset;\r\nif (in_kernel) {\r\nadd_sample_entry(s->eip, s->event);\r\nreturn 1;\r\n}\r\nif (!mm) {\r\natomic_inc(&oprofile_stats.sample_lost_no_mm);\r\nreturn 0;\r\n}\r\ncookie = lookup_dcookie(mm, s->eip, &offset);\r\nif (cookie == INVALID_COOKIE) {\r\natomic_inc(&oprofile_stats.sample_lost_no_mapping);\r\nreturn 0;\r\n}\r\nif (cookie != last_cookie) {\r\nadd_cookie_switch(cookie);\r\nlast_cookie = cookie;\r\n}\r\nadd_sample_entry(offset, s->event);\r\nreturn 1;\r\n}\r\nstatic void release_mm(struct mm_struct *mm)\r\n{\r\nif (!mm)\r\nreturn;\r\nup_read(&mm->mmap_sem);\r\nmmput(mm);\r\n}\r\nstatic struct mm_struct *take_tasks_mm(struct task_struct *task)\r\n{\r\nstruct mm_struct *mm = get_task_mm(task);\r\nif (mm)\r\ndown_read(&mm->mmap_sem);\r\nreturn mm;\r\n}\r\nstatic inline int is_code(unsigned long val)\r\n{\r\nreturn val == ESCAPE_CODE;\r\n}\r\nstatic void process_task_mortuary(void)\r\n{\r\nunsigned long flags;\r\nLIST_HEAD(local_dead_tasks);\r\nstruct task_struct *task;\r\nstruct task_struct *ttask;\r\nspin_lock_irqsave(&task_mortuary, flags);\r\nlist_splice_init(&dead_tasks, &local_dead_tasks);\r\nlist_splice_init(&dying_tasks, &dead_tasks);\r\nspin_unlock_irqrestore(&task_mortuary, flags);\r\nlist_for_each_entry_safe(task, ttask, &local_dead_tasks, tasks) {\r\nlist_del(&task->tasks);\r\nfree_task(task);\r\n}\r\n}\r\nstatic void mark_done(int cpu)\r\n{\r\nint i;\r\ncpumask_set_cpu(cpu, marked_cpus);\r\nfor_each_online_cpu(i) {\r\nif (!cpumask_test_cpu(i, marked_cpus))\r\nreturn;\r\n}\r\nprocess_task_mortuary();\r\ncpumask_clear(marked_cpus);\r\n}\r\nvoid sync_buffer(int cpu)\r\n{\r\nstruct mm_struct *mm = NULL;\r\nstruct mm_struct *oldmm;\r\nunsigned long val;\r\nstruct task_struct *new;\r\nunsigned long cookie = 0;\r\nint in_kernel = 1;\r\nsync_buffer_state state = sb_buffer_start;\r\nunsigned int i;\r\nunsigned long available;\r\nunsigned long flags;\r\nstruct op_entry entry;\r\nstruct op_sample *sample;\r\nmutex_lock(&buffer_mutex);\r\nadd_cpu_switch(cpu);\r\nop_cpu_buffer_reset(cpu);\r\navailable = op_cpu_buffer_entries(cpu);\r\nfor (i = 0; i < available; ++i) {\r\nsample = op_cpu_buffer_read_entry(&entry, cpu);\r\nif (!sample)\r\nbreak;\r\nif (is_code(sample->eip)) {\r\nflags = sample->event;\r\nif (flags & TRACE_BEGIN) {\r\nstate = sb_bt_start;\r\nadd_trace_begin();\r\n}\r\nif (flags & KERNEL_CTX_SWITCH) {\r\nin_kernel = flags & IS_KERNEL;\r\nif (state == sb_buffer_start)\r\nstate = sb_sample_start;\r\nadd_kernel_ctx_switch(flags & IS_KERNEL);\r\n}\r\nif (flags & USER_CTX_SWITCH\r\n&& op_cpu_buffer_get_data(&entry, &val)) {\r\nnew = (struct task_struct *)val;\r\noldmm = mm;\r\nrelease_mm(oldmm);\r\nmm = take_tasks_mm(new);\r\nif (mm != oldmm)\r\ncookie = get_exec_dcookie(mm);\r\nadd_user_ctx_switch(new, cookie);\r\n}\r\nif (op_cpu_buffer_get_size(&entry))\r\nadd_data(&entry, mm);\r\ncontinue;\r\n}\r\nif (state < sb_bt_start)\r\ncontinue;\r\nif (add_sample(mm, sample, in_kernel))\r\ncontinue;\r\nif (state == sb_bt_start) {\r\nstate = sb_bt_ignore;\r\natomic_inc(&oprofile_stats.bt_lost_no_mapping);\r\n}\r\n}\r\nrelease_mm(mm);\r\nmark_done(cpu);\r\nmutex_unlock(&buffer_mutex);\r\n}\r\nvoid oprofile_put_buff(unsigned long *buf, unsigned int start,\r\nunsigned int stop, unsigned int max)\r\n{\r\nint i;\r\ni = start;\r\nmutex_lock(&buffer_mutex);\r\nwhile (i != stop) {\r\nadd_event_entry(buf[i++]);\r\nif (i >= max)\r\ni = 0;\r\n}\r\nmutex_unlock(&buffer_mutex);\r\n}
