void kvmppc_mmu_invalidate_pte(struct kvm_vcpu *vcpu, struct hpte_cache *pte)\r\n{\r\nvolatile u32 *pteg;\r\npteg = (u32*)pte->slot;\r\npteg[0] = 0;\r\nasm volatile ("sync");\r\nasm volatile ("tlbie %0" : : "r" (pte->pte.eaddr) : "memory");\r\nasm volatile ("sync");\r\nasm volatile ("tlbsync");\r\n}\r\nstatic u16 kvmppc_sid_hash(struct kvm_vcpu *vcpu, u64 gvsid)\r\n{\r\nreturn (u16)(((gvsid >> (SID_MAP_BITS * 7)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 6)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 5)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 4)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 3)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 2)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 1)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 0)) & SID_MAP_MASK));\r\n}\r\nstatic struct kvmppc_sid_map *find_sid_vsid(struct kvm_vcpu *vcpu, u64 gvsid)\r\n{\r\nstruct kvmppc_sid_map *map;\r\nu16 sid_map_mask;\r\nif (vcpu->arch.shared->msr & MSR_PR)\r\ngvsid |= VSID_PR;\r\nsid_map_mask = kvmppc_sid_hash(vcpu, gvsid);\r\nmap = &to_book3s(vcpu)->sid_map[sid_map_mask];\r\nif (map->guest_vsid == gvsid) {\r\ndprintk_sr("SR: Searching 0x%llx -> 0x%llx\n",\r\ngvsid, map->host_vsid);\r\nreturn map;\r\n}\r\nmap = &to_book3s(vcpu)->sid_map[SID_MAP_MASK - sid_map_mask];\r\nif (map->guest_vsid == gvsid) {\r\ndprintk_sr("SR: Searching 0x%llx -> 0x%llx\n",\r\ngvsid, map->host_vsid);\r\nreturn map;\r\n}\r\ndprintk_sr("SR: Searching 0x%llx -> not found\n", gvsid);\r\nreturn NULL;\r\n}\r\nstatic u32 *kvmppc_mmu_get_pteg(struct kvm_vcpu *vcpu, u32 vsid, u32 eaddr,\r\nbool primary)\r\n{\r\nu32 page, hash;\r\nulong pteg = htab;\r\npage = (eaddr & ~ESID_MASK) >> 12;\r\nhash = ((vsid ^ page) << 6);\r\nif (!primary)\r\nhash = ~hash;\r\nhash &= htabmask;\r\npteg |= hash;\r\ndprintk_mmu("htab: %lx | hash: %x | htabmask: %x | pteg: %lx\n",\r\nhtab, hash, htabmask, pteg);\r\nreturn (u32*)pteg;\r\n}\r\nint kvmppc_mmu_map_page(struct kvm_vcpu *vcpu, struct kvmppc_pte *orig_pte)\r\n{\r\npfn_t hpaddr;\r\nu64 va;\r\nu64 vsid;\r\nstruct kvmppc_sid_map *map;\r\nvolatile u32 *pteg;\r\nu32 eaddr = orig_pte->eaddr;\r\nu32 pteg0, pteg1;\r\nregister int rr = 0;\r\nbool primary = false;\r\nbool evict = false;\r\nstruct hpte_cache *pte;\r\nhpaddr = kvmppc_gfn_to_pfn(vcpu, orig_pte->raddr >> PAGE_SHIFT);\r\nif (is_error_pfn(hpaddr)) {\r\nprintk(KERN_INFO "Couldn't get guest page for gfn %lx!\n",\r\norig_pte->eaddr);\r\nreturn -EINVAL;\r\n}\r\nhpaddr <<= PAGE_SHIFT;\r\nvcpu->arch.mmu.esid_to_vsid(vcpu, orig_pte->eaddr >> SID_SHIFT, &vsid);\r\nmap = find_sid_vsid(vcpu, vsid);\r\nif (!map) {\r\nkvmppc_mmu_map_segment(vcpu, eaddr);\r\nmap = find_sid_vsid(vcpu, vsid);\r\n}\r\nBUG_ON(!map);\r\nvsid = map->host_vsid;\r\nva = (vsid << SID_SHIFT) | (eaddr & ~ESID_MASK);\r\nnext_pteg:\r\nif (rr == 16) {\r\nprimary = !primary;\r\nevict = true;\r\nrr = 0;\r\n}\r\npteg = kvmppc_mmu_get_pteg(vcpu, vsid, eaddr, primary);\r\nif (!evict && (pteg[rr] & PTE_V)) {\r\nrr += 2;\r\ngoto next_pteg;\r\n}\r\ndprintk_mmu("KVM: old PTEG: %p (%d)\n", pteg, rr);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[0], pteg[1]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[2], pteg[3]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[4], pteg[5]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[6], pteg[7]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[8], pteg[9]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[10], pteg[11]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[12], pteg[13]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[14], pteg[15]);\r\npteg0 = ((eaddr & 0x0fffffff) >> 22) | (vsid << 7) | PTE_V |\r\n(primary ? 0 : PTE_SEC);\r\npteg1 = hpaddr | PTE_M | PTE_R | PTE_C;\r\nif (orig_pte->may_write) {\r\npteg1 |= PP_RWRW;\r\nmark_page_dirty(vcpu->kvm, orig_pte->raddr >> PAGE_SHIFT);\r\n} else {\r\npteg1 |= PP_RWRX;\r\n}\r\nlocal_irq_disable();\r\nif (pteg[rr]) {\r\npteg[rr] = 0;\r\nasm volatile ("sync");\r\n}\r\npteg[rr + 1] = pteg1;\r\npteg[rr] = pteg0;\r\nasm volatile ("sync");\r\nlocal_irq_enable();\r\ndprintk_mmu("KVM: new PTEG: %p\n", pteg);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[0], pteg[1]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[2], pteg[3]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[4], pteg[5]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[6], pteg[7]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[8], pteg[9]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[10], pteg[11]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[12], pteg[13]);\r\ndprintk_mmu("KVM: %08x - %08x\n", pteg[14], pteg[15]);\r\npte = kvmppc_mmu_hpte_cache_next(vcpu);\r\ndprintk_mmu("KVM: %c%c Map 0x%llx: [%lx] 0x%llx (0x%llx) -> %lx\n",\r\norig_pte->may_write ? 'w' : '-',\r\norig_pte->may_execute ? 'x' : '-',\r\norig_pte->eaddr, (ulong)pteg, va,\r\norig_pte->vpage, hpaddr);\r\npte->slot = (ulong)&pteg[rr];\r\npte->host_va = va;\r\npte->pte = *orig_pte;\r\npte->pfn = hpaddr >> PAGE_SHIFT;\r\nkvmppc_mmu_hpte_cache_map(vcpu, pte);\r\nreturn 0;\r\n}\r\nstatic struct kvmppc_sid_map *create_sid_map(struct kvm_vcpu *vcpu, u64 gvsid)\r\n{\r\nstruct kvmppc_sid_map *map;\r\nstruct kvmppc_vcpu_book3s *vcpu_book3s = to_book3s(vcpu);\r\nu16 sid_map_mask;\r\nstatic int backwards_map = 0;\r\nif (vcpu->arch.shared->msr & MSR_PR)\r\ngvsid |= VSID_PR;\r\nsid_map_mask = kvmppc_sid_hash(vcpu, gvsid);\r\nif (backwards_map)\r\nsid_map_mask = SID_MAP_MASK - sid_map_mask;\r\nmap = &to_book3s(vcpu)->sid_map[sid_map_mask];\r\nbackwards_map = !backwards_map;\r\nif (vcpu_book3s->vsid_next >= VSID_POOL_SIZE) {\r\nvcpu_book3s->vsid_next = 0;\r\nmemset(vcpu_book3s->sid_map, 0,\r\nsizeof(struct kvmppc_sid_map) * SID_MAP_NUM);\r\nkvmppc_mmu_pte_flush(vcpu, 0, 0);\r\nkvmppc_mmu_flush_segments(vcpu);\r\n}\r\nmap->host_vsid = vcpu_book3s->vsid_pool[vcpu_book3s->vsid_next];\r\nvcpu_book3s->vsid_next++;\r\nmap->guest_vsid = gvsid;\r\nmap->valid = true;\r\nreturn map;\r\n}\r\nint kvmppc_mmu_map_segment(struct kvm_vcpu *vcpu, ulong eaddr)\r\n{\r\nu32 esid = eaddr >> SID_SHIFT;\r\nu64 gvsid;\r\nu32 sr;\r\nstruct kvmppc_sid_map *map;\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = to_svcpu(vcpu);\r\nif (vcpu->arch.mmu.esid_to_vsid(vcpu, esid, &gvsid)) {\r\nsvcpu->sr[esid] = SR_INVALID;\r\nreturn -ENOENT;\r\n}\r\nmap = find_sid_vsid(vcpu, gvsid);\r\nif (!map)\r\nmap = create_sid_map(vcpu, gvsid);\r\nmap->guest_esid = esid;\r\nsr = map->host_vsid | SR_KP;\r\nsvcpu->sr[esid] = sr;\r\ndprintk_sr("MMU: mtsr %d, 0x%x\n", esid, sr);\r\nreturn 0;\r\n}\r\nvoid kvmppc_mmu_flush_segments(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = to_svcpu(vcpu);\r\ndprintk_sr("MMU: flushing all segments (%d)\n", ARRAY_SIZE(svcpu->sr));\r\nfor (i = 0; i < ARRAY_SIZE(svcpu->sr); i++)\r\nsvcpu->sr[i] = SR_INVALID;\r\n}\r\nvoid kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nkvmppc_mmu_hpte_destroy(vcpu);\r\npreempt_disable();\r\nfor (i = 0; i < SID_CONTEXTS; i++)\r\n__destroy_context(to_book3s(vcpu)->context_id[i]);\r\npreempt_enable();\r\n}\r\nint kvmppc_mmu_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nint err;\r\nulong sdr1;\r\nint i;\r\nint j;\r\nfor (i = 0; i < SID_CONTEXTS; i++) {\r\nerr = __init_new_context();\r\nif (err < 0)\r\ngoto init_fail;\r\nvcpu3s->context_id[i] = err;\r\nfor (j = 0; j < 16; j++)\r\nvcpu3s->vsid_pool[(i * 16) + j] = CTX_TO_VSID(err, j);\r\n}\r\nvcpu3s->vsid_next = 0;\r\nasm ( "mfsdr1 %0" : "=r"(sdr1) );\r\nhtabmask = ((sdr1 & 0x1FF) << 16) | 0xFFC0;\r\nhtab = (ulong)__va(sdr1 & 0xffff0000);\r\nkvmppc_mmu_hpte_init(vcpu);\r\nreturn 0;\r\ninit_fail:\r\nfor (j = 0; j < i; j++) {\r\nif (!vcpu3s->context_id[j])\r\ncontinue;\r\n__destroy_context(to_book3s(vcpu)->context_id[j]);\r\n}\r\nreturn -1;\r\n}
