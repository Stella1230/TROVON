static int __amd64_read_pci_cfg_dword(struct pci_dev *pdev, int offset,\r\nu32 *val, const char *func)\r\n{\r\nint err = 0;\r\nerr = pci_read_config_dword(pdev, offset, val);\r\nif (err)\r\namd64_warn("%s: error reading F%dx%03x.\n",\r\nfunc, PCI_FUNC(pdev->devfn), offset);\r\nreturn err;\r\n}\r\nint __amd64_write_pci_cfg_dword(struct pci_dev *pdev, int offset,\r\nu32 val, const char *func)\r\n{\r\nint err = 0;\r\nerr = pci_write_config_dword(pdev, offset, val);\r\nif (err)\r\namd64_warn("%s: error writing to F%dx%03x.\n",\r\nfunc, PCI_FUNC(pdev->devfn), offset);\r\nreturn err;\r\n}\r\nstatic int k8_read_dct_pci_cfg(struct amd64_pvt *pvt, int addr, u32 *val,\r\nconst char *func)\r\n{\r\nif (addr >= 0x100)\r\nreturn -EINVAL;\r\nreturn __amd64_read_pci_cfg_dword(pvt->F2, addr, val, func);\r\n}\r\nstatic int f10_read_dct_pci_cfg(struct amd64_pvt *pvt, int addr, u32 *val,\r\nconst char *func)\r\n{\r\nreturn __amd64_read_pci_cfg_dword(pvt->F2, addr, val, func);\r\n}\r\nstatic int f15_read_dct_pci_cfg(struct amd64_pvt *pvt, int addr, u32 *val,\r\nconst char *func)\r\n{\r\nu32 reg = 0;\r\nu8 dct = 0;\r\nif (addr >= 0x140 && addr <= 0x1a0) {\r\ndct = 1;\r\naddr -= 0x100;\r\n}\r\namd64_read_pci_cfg(pvt->F1, DCT_CFG_SEL, &reg);\r\nreg &= 0xfffffffe;\r\nreg |= dct;\r\namd64_write_pci_cfg(pvt->F1, DCT_CFG_SEL, reg);\r\nreturn __amd64_read_pci_cfg_dword(pvt->F2, addr, val, func);\r\n}\r\nstatic int __amd64_set_scrub_rate(struct pci_dev *ctl, u32 new_bw, u32 min_rate)\r\n{\r\nu32 scrubval;\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(scrubrates); i++) {\r\nif (scrubrates[i].scrubval < min_rate)\r\ncontinue;\r\nif (scrubrates[i].bandwidth <= new_bw)\r\nbreak;\r\n}\r\nscrubval = scrubrates[i].scrubval;\r\npci_write_bits32(ctl, SCRCTRL, scrubval, 0x001F);\r\nif (scrubval)\r\nreturn scrubrates[i].bandwidth;\r\nreturn 0;\r\n}\r\nstatic int amd64_set_scrub_rate(struct mem_ctl_info *mci, u32 bw)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu32 min_scrubrate = 0x5;\r\nif (boot_cpu_data.x86 == 0xf)\r\nmin_scrubrate = 0x0;\r\nreturn __amd64_set_scrub_rate(pvt->F3, bw, min_scrubrate);\r\n}\r\nstatic int amd64_get_scrub_rate(struct mem_ctl_info *mci)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu32 scrubval = 0;\r\nint i, retval = -EINVAL;\r\namd64_read_pci_cfg(pvt->F3, SCRCTRL, &scrubval);\r\nscrubval = scrubval & 0x001F;\r\nfor (i = 0; i < ARRAY_SIZE(scrubrates); i++) {\r\nif (scrubrates[i].scrubval == scrubval) {\r\nretval = scrubrates[i].bandwidth;\r\nbreak;\r\n}\r\n}\r\nreturn retval;\r\n}\r\nstatic bool amd64_base_limit_match(struct amd64_pvt *pvt, u64 sys_addr,\r\nunsigned nid)\r\n{\r\nu64 addr;\r\naddr = sys_addr & 0x000000ffffffffffull;\r\nreturn ((addr >= get_dram_base(pvt, nid)) &&\r\n(addr <= get_dram_limit(pvt, nid)));\r\n}\r\nstatic struct mem_ctl_info *find_mc_by_sys_addr(struct mem_ctl_info *mci,\r\nu64 sys_addr)\r\n{\r\nstruct amd64_pvt *pvt;\r\nunsigned node_id;\r\nu32 intlv_en, bits;\r\npvt = mci->pvt_info;\r\nintlv_en = dram_intlv_en(pvt, 0);\r\nif (intlv_en == 0) {\r\nfor (node_id = 0; node_id < DRAM_RANGES; node_id++) {\r\nif (amd64_base_limit_match(pvt, sys_addr, node_id))\r\ngoto found;\r\n}\r\ngoto err_no_match;\r\n}\r\nif (unlikely((intlv_en != 0x01) &&\r\n(intlv_en != 0x03) &&\r\n(intlv_en != 0x07))) {\r\namd64_warn("DRAM Base[IntlvEn] junk value: 0x%x, BIOS bug?\n", intlv_en);\r\nreturn NULL;\r\n}\r\nbits = (((u32) sys_addr) >> 12) & intlv_en;\r\nfor (node_id = 0; ; ) {\r\nif ((dram_intlv_sel(pvt, node_id) & intlv_en) == bits)\r\nbreak;\r\nif (++node_id >= DRAM_RANGES)\r\ngoto err_no_match;\r\n}\r\nif (unlikely(!amd64_base_limit_match(pvt, sys_addr, node_id))) {\r\namd64_warn("%s: sys_addr 0x%llx falls outside base/limit address"\r\n"range for node %d with node interleaving enabled.\n",\r\n__func__, sys_addr, node_id);\r\nreturn NULL;\r\n}\r\nfound:\r\nreturn edac_mc_find((int)node_id);\r\nerr_no_match:\r\ndebugf2("sys_addr 0x%lx doesn't match any node\n",\r\n(unsigned long)sys_addr);\r\nreturn NULL;\r\n}\r\nstatic void get_cs_base_and_mask(struct amd64_pvt *pvt, int csrow, u8 dct,\r\nu64 *base, u64 *mask)\r\n{\r\nu64 csbase, csmask, base_bits, mask_bits;\r\nu8 addr_shift;\r\nif (boot_cpu_data.x86 == 0xf && pvt->ext_model < K8_REV_F) {\r\ncsbase = pvt->csels[dct].csbases[csrow];\r\ncsmask = pvt->csels[dct].csmasks[csrow];\r\nbase_bits = GENMASK(21, 31) | GENMASK(9, 15);\r\nmask_bits = GENMASK(21, 29) | GENMASK(9, 15);\r\naddr_shift = 4;\r\n} else {\r\ncsbase = pvt->csels[dct].csbases[csrow];\r\ncsmask = pvt->csels[dct].csmasks[csrow >> 1];\r\naddr_shift = 8;\r\nif (boot_cpu_data.x86 == 0x15)\r\nbase_bits = mask_bits = GENMASK(19,30) | GENMASK(5,13);\r\nelse\r\nbase_bits = mask_bits = GENMASK(19,28) | GENMASK(5,13);\r\n}\r\n*base = (csbase & base_bits) << addr_shift;\r\n*mask = ~0ULL;\r\n*mask &= ~(mask_bits << addr_shift);\r\n*mask |= (csmask & mask_bits) << addr_shift;\r\n}\r\nstatic int input_addr_to_csrow(struct mem_ctl_info *mci, u64 input_addr)\r\n{\r\nstruct amd64_pvt *pvt;\r\nint csrow;\r\nu64 base, mask;\r\npvt = mci->pvt_info;\r\nfor_each_chip_select(csrow, 0, pvt) {\r\nif (!csrow_enabled(csrow, 0, pvt))\r\ncontinue;\r\nget_cs_base_and_mask(pvt, csrow, 0, &base, &mask);\r\nmask = ~mask;\r\nif ((input_addr & mask) == (base & mask)) {\r\ndebugf2("InputAddr 0x%lx matches csrow %d (node %d)\n",\r\n(unsigned long)input_addr, csrow,\r\npvt->mc_node_id);\r\nreturn csrow;\r\n}\r\n}\r\ndebugf2("no matching csrow for InputAddr 0x%lx (MC node %d)\n",\r\n(unsigned long)input_addr, pvt->mc_node_id);\r\nreturn -1;\r\n}\r\nint amd64_get_dram_hole_info(struct mem_ctl_info *mci, u64 *hole_base,\r\nu64 *hole_offset, u64 *hole_size)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu64 base;\r\nif (boot_cpu_data.x86 == 0xf && pvt->ext_model < K8_REV_E) {\r\ndebugf1(" revision %d for node %d does not support DHAR\n",\r\npvt->ext_model, pvt->mc_node_id);\r\nreturn 1;\r\n}\r\nif (boot_cpu_data.x86 >= 0x10 && !dhar_mem_hoist_valid(pvt)) {\r\ndebugf1(" Dram Memory Hoisting is DISABLED on this system\n");\r\nreturn 1;\r\n}\r\nif (!dhar_valid(pvt)) {\r\ndebugf1(" Dram Memory Hoisting is DISABLED on this node %d\n",\r\npvt->mc_node_id);\r\nreturn 1;\r\n}\r\nbase = dhar_base(pvt);\r\n*hole_base = base;\r\n*hole_size = (0x1ull << 32) - base;\r\nif (boot_cpu_data.x86 > 0xf)\r\n*hole_offset = f10_dhar_offset(pvt);\r\nelse\r\n*hole_offset = k8_dhar_offset(pvt);\r\ndebugf1(" DHAR info for node %d base 0x%lx offset 0x%lx size 0x%lx\n",\r\npvt->mc_node_id, (unsigned long)*hole_base,\r\n(unsigned long)*hole_offset, (unsigned long)*hole_size);\r\nreturn 0;\r\n}\r\nstatic u64 sys_addr_to_dram_addr(struct mem_ctl_info *mci, u64 sys_addr)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu64 dram_base, hole_base, hole_offset, hole_size, dram_addr;\r\nint ret = 0;\r\ndram_base = get_dram_base(pvt, pvt->mc_node_id);\r\nret = amd64_get_dram_hole_info(mci, &hole_base, &hole_offset,\r\n&hole_size);\r\nif (!ret) {\r\nif ((sys_addr >= (1ull << 32)) &&\r\n(sys_addr < ((1ull << 32) + hole_size))) {\r\ndram_addr = sys_addr - hole_offset;\r\ndebugf2("using DHAR to translate SysAddr 0x%lx to "\r\n"DramAddr 0x%lx\n",\r\n(unsigned long)sys_addr,\r\n(unsigned long)dram_addr);\r\nreturn dram_addr;\r\n}\r\n}\r\ndram_addr = (sys_addr & GENMASK(0, 39)) - dram_base;\r\ndebugf2("using DRAM Base register to translate SysAddr 0x%lx to "\r\n"DramAddr 0x%lx\n", (unsigned long)sys_addr,\r\n(unsigned long)dram_addr);\r\nreturn dram_addr;\r\n}\r\nstatic int num_node_interleave_bits(unsigned intlv_en)\r\n{\r\nstatic const int intlv_shift_table[] = { 0, 1, 0, 2, 0, 0, 0, 3 };\r\nint n;\r\nBUG_ON(intlv_en > 7);\r\nn = intlv_shift_table[intlv_en];\r\nreturn n;\r\n}\r\nstatic u64 dram_addr_to_input_addr(struct mem_ctl_info *mci, u64 dram_addr)\r\n{\r\nstruct amd64_pvt *pvt;\r\nint intlv_shift;\r\nu64 input_addr;\r\npvt = mci->pvt_info;\r\nintlv_shift = num_node_interleave_bits(dram_intlv_en(pvt, 0));\r\ninput_addr = ((dram_addr >> intlv_shift) & GENMASK(12, 35)) +\r\n(dram_addr & 0xfff);\r\ndebugf2(" Intlv Shift=%d DramAddr=0x%lx maps to InputAddr=0x%lx\n",\r\nintlv_shift, (unsigned long)dram_addr,\r\n(unsigned long)input_addr);\r\nreturn input_addr;\r\n}\r\nstatic u64 sys_addr_to_input_addr(struct mem_ctl_info *mci, u64 sys_addr)\r\n{\r\nu64 input_addr;\r\ninput_addr =\r\ndram_addr_to_input_addr(mci, sys_addr_to_dram_addr(mci, sys_addr));\r\ndebugf2("SysAdddr 0x%lx translates to InputAddr 0x%lx\n",\r\n(unsigned long)sys_addr, (unsigned long)input_addr);\r\nreturn input_addr;\r\n}\r\nstatic u64 input_addr_to_dram_addr(struct mem_ctl_info *mci, u64 input_addr)\r\n{\r\nstruct amd64_pvt *pvt;\r\nunsigned node_id, intlv_shift;\r\nu64 bits, dram_addr;\r\nu32 intlv_sel;\r\npvt = mci->pvt_info;\r\nnode_id = pvt->mc_node_id;\r\nBUG_ON(node_id > 7);\r\nintlv_shift = num_node_interleave_bits(dram_intlv_en(pvt, 0));\r\nif (intlv_shift == 0) {\r\ndebugf1(" InputAddr 0x%lx translates to DramAddr of "\r\n"same value\n", (unsigned long)input_addr);\r\nreturn input_addr;\r\n}\r\nbits = ((input_addr & GENMASK(12, 35)) << intlv_shift) +\r\n(input_addr & 0xfff);\r\nintlv_sel = dram_intlv_sel(pvt, node_id) & ((1 << intlv_shift) - 1);\r\ndram_addr = bits + (intlv_sel << 12);\r\ndebugf1("InputAddr 0x%lx translates to DramAddr 0x%lx "\r\n"(%d node interleave bits)\n", (unsigned long)input_addr,\r\n(unsigned long)dram_addr, intlv_shift);\r\nreturn dram_addr;\r\n}\r\nstatic u64 dram_addr_to_sys_addr(struct mem_ctl_info *mci, u64 dram_addr)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu64 hole_base, hole_offset, hole_size, base, sys_addr;\r\nint ret = 0;\r\nret = amd64_get_dram_hole_info(mci, &hole_base, &hole_offset,\r\n&hole_size);\r\nif (!ret) {\r\nif ((dram_addr >= hole_base) &&\r\n(dram_addr < (hole_base + hole_size))) {\r\nsys_addr = dram_addr + hole_offset;\r\ndebugf1("using DHAR to translate DramAddr 0x%lx to "\r\n"SysAddr 0x%lx\n", (unsigned long)dram_addr,\r\n(unsigned long)sys_addr);\r\nreturn sys_addr;\r\n}\r\n}\r\nbase = get_dram_base(pvt, pvt->mc_node_id);\r\nsys_addr = dram_addr + base;\r\nsys_addr |= ~((sys_addr & (1ull << 39)) - 1);\r\ndebugf1(" Node %d, DramAddr 0x%lx to SysAddr 0x%lx\n",\r\npvt->mc_node_id, (unsigned long)dram_addr,\r\n(unsigned long)sys_addr);\r\nreturn sys_addr;\r\n}\r\nstatic inline u64 input_addr_to_sys_addr(struct mem_ctl_info *mci,\r\nu64 input_addr)\r\n{\r\nreturn dram_addr_to_sys_addr(mci,\r\ninput_addr_to_dram_addr(mci, input_addr));\r\n}\r\nstatic void find_csrow_limits(struct mem_ctl_info *mci, int csrow,\r\nu64 *input_addr_min, u64 *input_addr_max)\r\n{\r\nstruct amd64_pvt *pvt;\r\nu64 base, mask;\r\npvt = mci->pvt_info;\r\nBUG_ON((csrow < 0) || (csrow >= pvt->csels[0].b_cnt));\r\nget_cs_base_and_mask(pvt, csrow, 0, &base, &mask);\r\n*input_addr_min = base & ~mask;\r\n*input_addr_max = base | mask;\r\n}\r\nstatic inline void error_address_to_page_and_offset(u64 error_address,\r\nu32 *page, u32 *offset)\r\n{\r\n*page = (u32) (error_address >> PAGE_SHIFT);\r\n*offset = ((u32) error_address) & ~PAGE_MASK;\r\n}\r\nstatic int sys_addr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr)\r\n{\r\nint csrow;\r\ncsrow = input_addr_to_csrow(mci, sys_addr_to_input_addr(mci, sys_addr));\r\nif (csrow == -1)\r\namd64_mc_err(mci, "Failed to translate InputAddr to csrow for "\r\n"address 0x%lx\n", (unsigned long)sys_addr);\r\nreturn csrow;\r\n}\r\nstatic enum edac_type amd64_determine_edac_cap(struct amd64_pvt *pvt)\r\n{\r\nu8 bit;\r\nenum dev_type edac_cap = EDAC_FLAG_NONE;\r\nbit = (boot_cpu_data.x86 > 0xf || pvt->ext_model >= K8_REV_F)\r\n? 19\r\n: 17;\r\nif (pvt->dclr0 & BIT(bit))\r\nedac_cap = EDAC_FLAG_SECDED;\r\nreturn edac_cap;\r\n}\r\nstatic void amd64_dump_dramcfg_low(u32 dclr, int chan)\r\n{\r\ndebugf1("F2x%d90 (DRAM Cfg Low): 0x%08x\n", chan, dclr);\r\ndebugf1(" DIMM type: %sbuffered; all DIMMs support ECC: %s\n",\r\n(dclr & BIT(16)) ? "un" : "",\r\n(dclr & BIT(19)) ? "yes" : "no");\r\ndebugf1(" PAR/ERR parity: %s\n",\r\n(dclr & BIT(8)) ? "enabled" : "disabled");\r\nif (boot_cpu_data.x86 == 0x10)\r\ndebugf1(" DCT 128bit mode width: %s\n",\r\n(dclr & BIT(11)) ? "128b" : "64b");\r\ndebugf1(" x4 logical DIMMs present: L0: %s L1: %s L2: %s L3: %s\n",\r\n(dclr & BIT(12)) ? "yes" : "no",\r\n(dclr & BIT(13)) ? "yes" : "no",\r\n(dclr & BIT(14)) ? "yes" : "no",\r\n(dclr & BIT(15)) ? "yes" : "no");\r\n}\r\nstatic void dump_misc_regs(struct amd64_pvt *pvt)\r\n{\r\ndebugf1("F3xE8 (NB Cap): 0x%08x\n", pvt->nbcap);\r\ndebugf1(" NB two channel DRAM capable: %s\n",\r\n(pvt->nbcap & NBCAP_DCT_DUAL) ? "yes" : "no");\r\ndebugf1(" ECC capable: %s, ChipKill ECC capable: %s\n",\r\n(pvt->nbcap & NBCAP_SECDED) ? "yes" : "no",\r\n(pvt->nbcap & NBCAP_CHIPKILL) ? "yes" : "no");\r\namd64_dump_dramcfg_low(pvt->dclr0, 0);\r\ndebugf1("F3xB0 (Online Spare): 0x%08x\n", pvt->online_spare);\r\ndebugf1("F1xF0 (DRAM Hole Address): 0x%08x, base: 0x%08x, "\r\n"offset: 0x%08x\n",\r\npvt->dhar, dhar_base(pvt),\r\n(boot_cpu_data.x86 == 0xf) ? k8_dhar_offset(pvt)\r\n: f10_dhar_offset(pvt));\r\ndebugf1(" DramHoleValid: %s\n", dhar_valid(pvt) ? "yes" : "no");\r\namd64_debug_display_dimm_sizes(pvt, 0);\r\nif (boot_cpu_data.x86 == 0xf)\r\nreturn;\r\namd64_debug_display_dimm_sizes(pvt, 1);\r\namd64_info("using %s syndromes.\n", ((pvt->ecc_sym_sz == 8) ? "x8" : "x4"));\r\nif (!dct_ganging_enabled(pvt))\r\namd64_dump_dramcfg_low(pvt->dclr1, 1);\r\n}\r\nstatic void prep_chip_selects(struct amd64_pvt *pvt)\r\n{\r\nif (boot_cpu_data.x86 == 0xf && pvt->ext_model < K8_REV_F) {\r\npvt->csels[0].b_cnt = pvt->csels[1].b_cnt = 8;\r\npvt->csels[0].m_cnt = pvt->csels[1].m_cnt = 8;\r\n} else {\r\npvt->csels[0].b_cnt = pvt->csels[1].b_cnt = 8;\r\npvt->csels[0].m_cnt = pvt->csels[1].m_cnt = 4;\r\n}\r\n}\r\nstatic void read_dct_base_mask(struct amd64_pvt *pvt)\r\n{\r\nint cs;\r\nprep_chip_selects(pvt);\r\nfor_each_chip_select(cs, 0, pvt) {\r\nint reg0 = DCSB0 + (cs * 4);\r\nint reg1 = DCSB1 + (cs * 4);\r\nu32 *base0 = &pvt->csels[0].csbases[cs];\r\nu32 *base1 = &pvt->csels[1].csbases[cs];\r\nif (!amd64_read_dct_pci_cfg(pvt, reg0, base0))\r\ndebugf0(" DCSB0[%d]=0x%08x reg: F2x%x\n",\r\ncs, *base0, reg0);\r\nif (boot_cpu_data.x86 == 0xf || dct_ganging_enabled(pvt))\r\ncontinue;\r\nif (!amd64_read_dct_pci_cfg(pvt, reg1, base1))\r\ndebugf0(" DCSB1[%d]=0x%08x reg: F2x%x\n",\r\ncs, *base1, reg1);\r\n}\r\nfor_each_chip_select_mask(cs, 0, pvt) {\r\nint reg0 = DCSM0 + (cs * 4);\r\nint reg1 = DCSM1 + (cs * 4);\r\nu32 *mask0 = &pvt->csels[0].csmasks[cs];\r\nu32 *mask1 = &pvt->csels[1].csmasks[cs];\r\nif (!amd64_read_dct_pci_cfg(pvt, reg0, mask0))\r\ndebugf0(" DCSM0[%d]=0x%08x reg: F2x%x\n",\r\ncs, *mask0, reg0);\r\nif (boot_cpu_data.x86 == 0xf || dct_ganging_enabled(pvt))\r\ncontinue;\r\nif (!amd64_read_dct_pci_cfg(pvt, reg1, mask1))\r\ndebugf0(" DCSM1[%d]=0x%08x reg: F2x%x\n",\r\ncs, *mask1, reg1);\r\n}\r\n}\r\nstatic enum mem_type amd64_determine_memory_type(struct amd64_pvt *pvt, int cs)\r\n{\r\nenum mem_type type;\r\nif (boot_cpu_data.x86 >= 0x15)\r\ntype = (pvt->dclr0 & BIT(16)) ? MEM_DDR3 : MEM_RDDR3;\r\nelse if (boot_cpu_data.x86 == 0x10 || pvt->ext_model >= K8_REV_F) {\r\nif (pvt->dchr0 & DDR3_MODE)\r\ntype = (pvt->dclr0 & BIT(16)) ? MEM_DDR3 : MEM_RDDR3;\r\nelse\r\ntype = (pvt->dclr0 & BIT(16)) ? MEM_DDR2 : MEM_RDDR2;\r\n} else {\r\ntype = (pvt->dclr0 & BIT(18)) ? MEM_DDR : MEM_RDDR;\r\n}\r\namd64_info("CS%d: %s\n", cs, edac_mem_types[type]);\r\nreturn type;\r\n}\r\nstatic int k8_early_channel_count(struct amd64_pvt *pvt)\r\n{\r\nint flag;\r\nif (pvt->ext_model >= K8_REV_F)\r\nflag = pvt->dclr0 & WIDTH_128;\r\nelse\r\nflag = pvt->dclr0 & REVE_WIDTH_128;\r\npvt->dclr1 = 0;\r\nreturn (flag) ? 2 : 1;\r\n}\r\nstatic u64 get_error_address(struct mce *m)\r\n{\r\nstruct cpuinfo_x86 *c = &boot_cpu_data;\r\nu64 addr;\r\nu8 start_bit = 1;\r\nu8 end_bit = 47;\r\nif (c->x86 == 0xf) {\r\nstart_bit = 3;\r\nend_bit = 39;\r\n}\r\naddr = m->addr & GENMASK(start_bit, end_bit);\r\nif (c->x86 == 0x15) {\r\nstruct amd64_pvt *pvt;\r\nu64 cc6_base, tmp_addr;\r\nu32 tmp;\r\nu8 mce_nid, intlv_en;\r\nif ((addr & GENMASK(24, 47)) >> 24 != 0x00fdf7)\r\nreturn addr;\r\nmce_nid = amd_get_nb_id(m->extcpu);\r\npvt = mcis[mce_nid]->pvt_info;\r\namd64_read_pci_cfg(pvt->F1, DRAM_LOCAL_NODE_LIM, &tmp);\r\nintlv_en = tmp >> 21 & 0x7;\r\ncc6_base = (tmp & GENMASK(0, 20)) << 3;\r\ncc6_base |= intlv_en ^ 0x7;\r\ncc6_base <<= 24;\r\nif (!intlv_en)\r\nreturn cc6_base | (addr & GENMASK(0, 23));\r\namd64_read_pci_cfg(pvt->F1, DRAM_LOCAL_NODE_BASE, &tmp);\r\ntmp_addr = (addr & GENMASK(12, 23)) << __fls(intlv_en + 1);\r\ntmp_addr |= (tmp & GENMASK(21, 23)) >> 9;\r\ntmp_addr |= addr & GENMASK(0, 11);\r\nreturn cc6_base | tmp_addr;\r\n}\r\nreturn addr;\r\n}\r\nstatic void read_dram_base_limit_regs(struct amd64_pvt *pvt, unsigned range)\r\n{\r\nstruct cpuinfo_x86 *c = &boot_cpu_data;\r\nint off = range << 3;\r\namd64_read_pci_cfg(pvt->F1, DRAM_BASE_LO + off, &pvt->ranges[range].base.lo);\r\namd64_read_pci_cfg(pvt->F1, DRAM_LIMIT_LO + off, &pvt->ranges[range].lim.lo);\r\nif (c->x86 == 0xf)\r\nreturn;\r\nif (!dram_rw(pvt, range))\r\nreturn;\r\namd64_read_pci_cfg(pvt->F1, DRAM_BASE_HI + off, &pvt->ranges[range].base.hi);\r\namd64_read_pci_cfg(pvt->F1, DRAM_LIMIT_HI + off, &pvt->ranges[range].lim.hi);\r\nif (c->x86 == 0x15) {\r\nstruct pci_dev *f1 = NULL;\r\nu8 nid = dram_dst_node(pvt, range);\r\nu32 llim;\r\nf1 = pci_get_domain_bus_and_slot(0, 0, PCI_DEVFN(0x18 + nid, 1));\r\nif (WARN_ON(!f1))\r\nreturn;\r\namd64_read_pci_cfg(f1, DRAM_LOCAL_NODE_LIM, &llim);\r\npvt->ranges[range].lim.lo &= GENMASK(0, 15);\r\npvt->ranges[range].lim.lo |= ((llim & 0x1fff) << 3 | 0x7) << 16;\r\npvt->ranges[range].lim.hi &= GENMASK(0, 7);\r\npvt->ranges[range].lim.hi |= llim >> 13;\r\npci_dev_put(f1);\r\n}\r\n}\r\nstatic void k8_map_sysaddr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr,\r\nu16 syndrome)\r\n{\r\nstruct mem_ctl_info *src_mci;\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nint channel, csrow;\r\nu32 page, offset;\r\nif (pvt->nbcfg & NBCFG_CHIPKILL) {\r\nchannel = get_channel_from_ecc_syndrome(mci, syndrome);\r\nif (channel < 0) {\r\namd64_mc_warn(mci, "unknown syndrome 0x%04x - possible "\r\n"error reporting race\n", syndrome);\r\nedac_mc_handle_ce_no_info(mci, EDAC_MOD_STR);\r\nreturn;\r\n}\r\n} else {\r\nchannel = ((sys_addr & BIT(3)) != 0);\r\n}\r\nsrc_mci = find_mc_by_sys_addr(mci, sys_addr);\r\nif (!src_mci) {\r\namd64_mc_err(mci, "failed to map error addr 0x%lx to a node\n",\r\n(unsigned long)sys_addr);\r\nedac_mc_handle_ce_no_info(mci, EDAC_MOD_STR);\r\nreturn;\r\n}\r\ncsrow = sys_addr_to_csrow(src_mci, sys_addr);\r\nif (csrow < 0) {\r\nedac_mc_handle_ce_no_info(src_mci, EDAC_MOD_STR);\r\n} else {\r\nerror_address_to_page_and_offset(sys_addr, &page, &offset);\r\nedac_mc_handle_ce(src_mci, page, offset, syndrome, csrow,\r\nchannel, EDAC_MOD_STR);\r\n}\r\n}\r\nstatic int ddr2_cs_size(unsigned i, bool dct_width)\r\n{\r\nunsigned shift = 0;\r\nif (i <= 2)\r\nshift = i;\r\nelse if (!(i & 0x1))\r\nshift = i >> 1;\r\nelse\r\nshift = (i + 1) >> 1;\r\nreturn 128 << (shift + !!dct_width);\r\n}\r\nstatic int k8_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\r\nunsigned cs_mode)\r\n{\r\nu32 dclr = dct ? pvt->dclr1 : pvt->dclr0;\r\nif (pvt->ext_model >= K8_REV_F) {\r\nWARN_ON(cs_mode > 11);\r\nreturn ddr2_cs_size(cs_mode, dclr & WIDTH_128);\r\n}\r\nelse if (pvt->ext_model >= K8_REV_D) {\r\nWARN_ON(cs_mode > 10);\r\nif (cs_mode == 3 || cs_mode == 8)\r\nreturn 32 << (cs_mode - 1);\r\nelse\r\nreturn 32 << cs_mode;\r\n}\r\nelse {\r\nWARN_ON(cs_mode > 6);\r\nreturn 32 << cs_mode;\r\n}\r\n}\r\nstatic int f1x_early_channel_count(struct amd64_pvt *pvt)\r\n{\r\nint i, j, channels = 0;\r\nif (boot_cpu_data.x86 == 0x10 && (pvt->dclr0 & WIDTH_128))\r\nreturn 2;\r\ndebugf0("Data width is not 128 bits - need more decoding\n");\r\nfor (i = 0; i < 2; i++) {\r\nu32 dbam = (i ? pvt->dbam1 : pvt->dbam0);\r\nfor (j = 0; j < 4; j++) {\r\nif (DBAM_DIMM(j, dbam) > 0) {\r\nchannels++;\r\nbreak;\r\n}\r\n}\r\n}\r\nif (channels > 2)\r\nchannels = 2;\r\namd64_info("MCT channel count: %d\n", channels);\r\nreturn channels;\r\n}\r\nstatic int ddr3_cs_size(unsigned i, bool dct_width)\r\n{\r\nunsigned shift = 0;\r\nint cs_size = 0;\r\nif (i == 0 || i == 3 || i == 4)\r\ncs_size = -1;\r\nelse if (i <= 2)\r\nshift = i;\r\nelse if (i == 12)\r\nshift = 7;\r\nelse if (!(i & 0x1))\r\nshift = i >> 1;\r\nelse\r\nshift = (i + 1) >> 1;\r\nif (cs_size != -1)\r\ncs_size = (128 * (1 << !!dct_width)) << shift;\r\nreturn cs_size;\r\n}\r\nstatic int f10_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\r\nunsigned cs_mode)\r\n{\r\nu32 dclr = dct ? pvt->dclr1 : pvt->dclr0;\r\nWARN_ON(cs_mode > 11);\r\nif (pvt->dchr0 & DDR3_MODE || pvt->dchr1 & DDR3_MODE)\r\nreturn ddr3_cs_size(cs_mode, dclr & WIDTH_128);\r\nelse\r\nreturn ddr2_cs_size(cs_mode, dclr & WIDTH_128);\r\n}\r\nstatic int f15_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\r\nunsigned cs_mode)\r\n{\r\nWARN_ON(cs_mode > 12);\r\nreturn ddr3_cs_size(cs_mode, false);\r\n}\r\nstatic void read_dram_ctl_register(struct amd64_pvt *pvt)\r\n{\r\nif (boot_cpu_data.x86 == 0xf)\r\nreturn;\r\nif (!amd64_read_dct_pci_cfg(pvt, DCT_SEL_LO, &pvt->dct_sel_lo)) {\r\ndebugf0("F2x110 (DCTSelLow): 0x%08x, High range addrs at: 0x%x\n",\r\npvt->dct_sel_lo, dct_sel_baseaddr(pvt));\r\ndebugf0(" DCTs operate in %s mode.\n",\r\n(dct_ganging_enabled(pvt) ? "ganged" : "unganged"));\r\nif (!dct_ganging_enabled(pvt))\r\ndebugf0(" Address range split per DCT: %s\n",\r\n(dct_high_range_enabled(pvt) ? "yes" : "no"));\r\ndebugf0(" data interleave for ECC: %s, "\r\n"DRAM cleared since last warm reset: %s\n",\r\n(dct_data_intlv_enabled(pvt) ? "enabled" : "disabled"),\r\n(dct_memory_cleared(pvt) ? "yes" : "no"));\r\ndebugf0(" channel interleave: %s, "\r\n"interleave bits selector: 0x%x\n",\r\n(dct_interleave_enabled(pvt) ? "enabled" : "disabled"),\r\ndct_sel_interleave_addr(pvt));\r\n}\r\namd64_read_dct_pci_cfg(pvt, DCT_SEL_HI, &pvt->dct_sel_hi);\r\n}\r\nstatic u8 f1x_determine_channel(struct amd64_pvt *pvt, u64 sys_addr,\r\nbool hi_range_sel, u8 intlv_en)\r\n{\r\nu8 dct_sel_high = (pvt->dct_sel_lo >> 1) & 1;\r\nif (dct_ganging_enabled(pvt))\r\nreturn 0;\r\nif (hi_range_sel)\r\nreturn dct_sel_high;\r\nif (dct_interleave_enabled(pvt)) {\r\nu8 intlv_addr = dct_sel_interleave_addr(pvt);\r\nif (!intlv_addr)\r\nreturn sys_addr >> 6 & 1;\r\nif (intlv_addr & 0x2) {\r\nu8 shift = intlv_addr & 0x1 ? 9 : 6;\r\nu32 temp = hweight_long((u32) ((sys_addr >> 16) & 0x1F)) % 2;\r\nreturn ((sys_addr >> shift) & 1) ^ temp;\r\n}\r\nreturn (sys_addr >> (12 + hweight8(intlv_en))) & 1;\r\n}\r\nif (dct_high_range_enabled(pvt))\r\nreturn ~dct_sel_high & 1;\r\nreturn 0;\r\n}\r\nstatic u64 f1x_get_norm_dct_addr(struct amd64_pvt *pvt, unsigned range,\r\nu64 sys_addr, bool hi_rng,\r\nu32 dct_sel_base_addr)\r\n{\r\nu64 chan_off;\r\nu64 dram_base = get_dram_base(pvt, range);\r\nu64 hole_off = f10_dhar_offset(pvt);\r\nu64 dct_sel_base_off = (pvt->dct_sel_hi & 0xFFFFFC00) << 16;\r\nif (hi_rng) {\r\nif ((!(dct_sel_base_addr >> 16) ||\r\ndct_sel_base_addr < dhar_base(pvt)) &&\r\ndhar_valid(pvt) &&\r\n(sys_addr >= BIT_64(32)))\r\nchan_off = hole_off;\r\nelse\r\nchan_off = dct_sel_base_off;\r\n} else {\r\nif (dhar_valid(pvt) && (sys_addr >= BIT_64(32)))\r\nchan_off = hole_off;\r\nelse\r\nchan_off = dram_base;\r\n}\r\nreturn (sys_addr & GENMASK(6,47)) - (chan_off & GENMASK(23,47));\r\n}\r\nstatic int f10_process_possible_spare(struct amd64_pvt *pvt, u8 dct, int csrow)\r\n{\r\nint tmp_cs;\r\nif (online_spare_swap_done(pvt, dct) &&\r\ncsrow == online_spare_bad_dramcs(pvt, dct)) {\r\nfor_each_chip_select(tmp_cs, dct, pvt) {\r\nif (chip_select_base(tmp_cs, dct, pvt) & 0x2) {\r\ncsrow = tmp_cs;\r\nbreak;\r\n}\r\n}\r\n}\r\nreturn csrow;\r\n}\r\nstatic int f1x_lookup_addr_in_dct(u64 in_addr, u32 nid, u8 dct)\r\n{\r\nstruct mem_ctl_info *mci;\r\nstruct amd64_pvt *pvt;\r\nu64 cs_base, cs_mask;\r\nint cs_found = -EINVAL;\r\nint csrow;\r\nmci = mcis[nid];\r\nif (!mci)\r\nreturn cs_found;\r\npvt = mci->pvt_info;\r\ndebugf1("input addr: 0x%llx, DCT: %d\n", in_addr, dct);\r\nfor_each_chip_select(csrow, dct, pvt) {\r\nif (!csrow_enabled(csrow, dct, pvt))\r\ncontinue;\r\nget_cs_base_and_mask(pvt, csrow, dct, &cs_base, &cs_mask);\r\ndebugf1(" CSROW=%d CSBase=0x%llx CSMask=0x%llx\n",\r\ncsrow, cs_base, cs_mask);\r\ncs_mask = ~cs_mask;\r\ndebugf1(" (InputAddr & ~CSMask)=0x%llx "\r\n"(CSBase & ~CSMask)=0x%llx\n",\r\n(in_addr & cs_mask), (cs_base & cs_mask));\r\nif ((in_addr & cs_mask) == (cs_base & cs_mask)) {\r\ncs_found = f10_process_possible_spare(pvt, dct, csrow);\r\ndebugf1(" MATCH csrow=%d\n", cs_found);\r\nbreak;\r\n}\r\n}\r\nreturn cs_found;\r\n}\r\nstatic u64 f1x_swap_interleaved_region(struct amd64_pvt *pvt, u64 sys_addr)\r\n{\r\nu32 swap_reg, swap_base, swap_limit, rgn_size, tmp_addr;\r\nif (boot_cpu_data.x86 == 0x10) {\r\nif (boot_cpu_data.x86_model < 4 ||\r\n(boot_cpu_data.x86_model < 0xa &&\r\nboot_cpu_data.x86_mask < 3))\r\nreturn sys_addr;\r\n}\r\namd64_read_dct_pci_cfg(pvt, SWAP_INTLV_REG, &swap_reg);\r\nif (!(swap_reg & 0x1))\r\nreturn sys_addr;\r\nswap_base = (swap_reg >> 3) & 0x7f;\r\nswap_limit = (swap_reg >> 11) & 0x7f;\r\nrgn_size = (swap_reg >> 20) & 0x7f;\r\ntmp_addr = sys_addr >> 27;\r\nif (!(sys_addr >> 34) &&\r\n(((tmp_addr >= swap_base) &&\r\n(tmp_addr <= swap_limit)) ||\r\n(tmp_addr < rgn_size)))\r\nreturn sys_addr ^ (u64)swap_base << 27;\r\nreturn sys_addr;\r\n}\r\nstatic int f1x_match_to_this_node(struct amd64_pvt *pvt, unsigned range,\r\nu64 sys_addr, int *nid, int *chan_sel)\r\n{\r\nint cs_found = -EINVAL;\r\nu64 chan_addr;\r\nu32 dct_sel_base;\r\nu8 channel;\r\nbool high_range = false;\r\nu8 node_id = dram_dst_node(pvt, range);\r\nu8 intlv_en = dram_intlv_en(pvt, range);\r\nu32 intlv_sel = dram_intlv_sel(pvt, range);\r\ndebugf1("(range %d) SystemAddr= 0x%llx Limit=0x%llx\n",\r\nrange, sys_addr, get_dram_limit(pvt, range));\r\nif (dhar_valid(pvt) &&\r\ndhar_base(pvt) <= sys_addr &&\r\nsys_addr < BIT_64(32)) {\r\namd64_warn("Huh? Address is in the MMIO hole: 0x%016llx\n",\r\nsys_addr);\r\nreturn -EINVAL;\r\n}\r\nif (intlv_en && (intlv_sel != ((sys_addr >> 12) & intlv_en)))\r\nreturn -EINVAL;\r\nsys_addr = f1x_swap_interleaved_region(pvt, sys_addr);\r\ndct_sel_base = dct_sel_baseaddr(pvt);\r\nif (dct_high_range_enabled(pvt) &&\r\n!dct_ganging_enabled(pvt) &&\r\n((sys_addr >> 27) >= (dct_sel_base >> 11)))\r\nhigh_range = true;\r\nchannel = f1x_determine_channel(pvt, sys_addr, high_range, intlv_en);\r\nchan_addr = f1x_get_norm_dct_addr(pvt, range, sys_addr,\r\nhigh_range, dct_sel_base);\r\nif (intlv_en)\r\nchan_addr = ((chan_addr >> (12 + hweight8(intlv_en))) << 12) |\r\n(chan_addr & 0xfff);\r\nif (dct_interleave_enabled(pvt) &&\r\n!dct_high_range_enabled(pvt) &&\r\n!dct_ganging_enabled(pvt)) {\r\nif (dct_sel_interleave_addr(pvt) != 1) {\r\nif (dct_sel_interleave_addr(pvt) == 0x3)\r\nchan_addr = ((chan_addr >> 10) << 9) |\r\n(chan_addr & 0x1ff);\r\nelse\r\nchan_addr = ((chan_addr >> 7) << 6) |\r\n(chan_addr & 0x3f);\r\n} else\r\nchan_addr = ((chan_addr >> 13) << 12) |\r\n(chan_addr & 0xfff);\r\n}\r\ndebugf1(" Normalized DCT addr: 0x%llx\n", chan_addr);\r\ncs_found = f1x_lookup_addr_in_dct(chan_addr, node_id, channel);\r\nif (cs_found >= 0) {\r\n*nid = node_id;\r\n*chan_sel = channel;\r\n}\r\nreturn cs_found;\r\n}\r\nstatic int f1x_translate_sysaddr_to_cs(struct amd64_pvt *pvt, u64 sys_addr,\r\nint *node, int *chan_sel)\r\n{\r\nint cs_found = -EINVAL;\r\nunsigned range;\r\nfor (range = 0; range < DRAM_RANGES; range++) {\r\nif (!dram_rw(pvt, range))\r\ncontinue;\r\nif ((get_dram_base(pvt, range) <= sys_addr) &&\r\n(get_dram_limit(pvt, range) >= sys_addr)) {\r\ncs_found = f1x_match_to_this_node(pvt, range,\r\nsys_addr, node,\r\nchan_sel);\r\nif (cs_found >= 0)\r\nbreak;\r\n}\r\n}\r\nreturn cs_found;\r\n}\r\nstatic void f1x_map_sysaddr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr,\r\nu16 syndrome)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu32 page, offset;\r\nint nid, csrow, chan = 0;\r\ncsrow = f1x_translate_sysaddr_to_cs(pvt, sys_addr, &nid, &chan);\r\nif (csrow < 0) {\r\nedac_mc_handle_ce_no_info(mci, EDAC_MOD_STR);\r\nreturn;\r\n}\r\nerror_address_to_page_and_offset(sys_addr, &page, &offset);\r\nif (dct_ganging_enabled(pvt))\r\nchan = get_channel_from_ecc_syndrome(mci, syndrome);\r\nif (chan >= 0)\r\nedac_mc_handle_ce(mci, page, offset, syndrome, csrow, chan,\r\nEDAC_MOD_STR);\r\nelse\r\nfor (chan = 0; chan < mci->csrows[csrow].nr_channels; chan++)\r\nedac_mc_handle_ce(mci, page, offset, syndrome,\r\ncsrow, chan, EDAC_MOD_STR);\r\n}\r\nstatic void amd64_debug_display_dimm_sizes(struct amd64_pvt *pvt, u8 ctrl)\r\n{\r\nint dimm, size0, size1, factor = 0;\r\nu32 *dcsb = ctrl ? pvt->csels[1].csbases : pvt->csels[0].csbases;\r\nu32 dbam = ctrl ? pvt->dbam1 : pvt->dbam0;\r\nif (boot_cpu_data.x86 == 0xf) {\r\nif (pvt->dclr0 & WIDTH_128)\r\nfactor = 1;\r\nif (pvt->ext_model < K8_REV_F)\r\nreturn;\r\nelse\r\nWARN_ON(ctrl != 0);\r\n}\r\ndbam = (ctrl && !dct_ganging_enabled(pvt)) ? pvt->dbam1 : pvt->dbam0;\r\ndcsb = (ctrl && !dct_ganging_enabled(pvt)) ? pvt->csels[1].csbases\r\n: pvt->csels[0].csbases;\r\ndebugf1("F2x%d80 (DRAM Bank Address Mapping): 0x%08x\n", ctrl, dbam);\r\nedac_printk(KERN_DEBUG, EDAC_MC, "DCT%d chip selects:\n", ctrl);\r\nfor (dimm = 0; dimm < 4; dimm++) {\r\nsize0 = 0;\r\nif (dcsb[dimm*2] & DCSB_CS_ENABLE)\r\nsize0 = pvt->ops->dbam_to_cs(pvt, ctrl,\r\nDBAM_DIMM(dimm, dbam));\r\nsize1 = 0;\r\nif (dcsb[dimm*2 + 1] & DCSB_CS_ENABLE)\r\nsize1 = pvt->ops->dbam_to_cs(pvt, ctrl,\r\nDBAM_DIMM(dimm, dbam));\r\namd64_info(EDAC_MC ": %d: %5dMB %d: %5dMB\n",\r\ndimm * 2, size0 << factor,\r\ndimm * 2 + 1, size1 << factor);\r\n}\r\n}\r\nstatic struct pci_dev *pci_get_related_function(unsigned int vendor,\r\nunsigned int device,\r\nstruct pci_dev *related)\r\n{\r\nstruct pci_dev *dev = NULL;\r\ndev = pci_get_device(vendor, device, dev);\r\nwhile (dev) {\r\nif ((dev->bus->number == related->bus->number) &&\r\n(PCI_SLOT(dev->devfn) == PCI_SLOT(related->devfn)))\r\nbreak;\r\ndev = pci_get_device(vendor, device, dev);\r\n}\r\nreturn dev;\r\n}\r\nstatic int decode_syndrome(u16 syndrome, u16 *vectors, unsigned num_vecs,\r\nunsigned v_dim)\r\n{\r\nunsigned int i, err_sym;\r\nfor (err_sym = 0; err_sym < num_vecs / v_dim; err_sym++) {\r\nu16 s = syndrome;\r\nunsigned v_idx = err_sym * v_dim;\r\nunsigned v_end = (err_sym + 1) * v_dim;\r\nfor (i = 1; i < (1U << 16); i <<= 1) {\r\nif (v_idx < v_end && vectors[v_idx] & i) {\r\nu16 ev_comp = vectors[v_idx++];\r\nif (s & i) {\r\ns ^= ev_comp;\r\nif (!s)\r\nreturn err_sym;\r\n}\r\n} else if (s & i)\r\nbreak;\r\n}\r\n}\r\ndebugf0("syndrome(%x) not found\n", syndrome);\r\nreturn -1;\r\n}\r\nstatic int map_err_sym_to_channel(int err_sym, int sym_size)\r\n{\r\nif (sym_size == 4)\r\nswitch (err_sym) {\r\ncase 0x20:\r\ncase 0x21:\r\nreturn 0;\r\nbreak;\r\ncase 0x22:\r\ncase 0x23:\r\nreturn 1;\r\nbreak;\r\ndefault:\r\nreturn err_sym >> 4;\r\nbreak;\r\n}\r\nelse\r\nswitch (err_sym) {\r\ncase 0x10:\r\nWARN(1, KERN_ERR "Invalid error symbol: 0x%x\n",\r\nerr_sym);\r\nreturn -1;\r\nbreak;\r\ncase 0x11:\r\nreturn 0;\r\nbreak;\r\ncase 0x12:\r\nreturn 1;\r\nbreak;\r\ndefault:\r\nreturn err_sym >> 3;\r\nbreak;\r\n}\r\nreturn -1;\r\n}\r\nstatic int get_channel_from_ecc_syndrome(struct mem_ctl_info *mci, u16 syndrome)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nint err_sym = -1;\r\nif (pvt->ecc_sym_sz == 8)\r\nerr_sym = decode_syndrome(syndrome, x8_vectors,\r\nARRAY_SIZE(x8_vectors),\r\npvt->ecc_sym_sz);\r\nelse if (pvt->ecc_sym_sz == 4)\r\nerr_sym = decode_syndrome(syndrome, x4_vectors,\r\nARRAY_SIZE(x4_vectors),\r\npvt->ecc_sym_sz);\r\nelse {\r\namd64_warn("Illegal syndrome type: %u\n", pvt->ecc_sym_sz);\r\nreturn err_sym;\r\n}\r\nreturn map_err_sym_to_channel(err_sym, pvt->ecc_sym_sz);\r\n}\r\nstatic void amd64_handle_ce(struct mem_ctl_info *mci, struct mce *m)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu64 sys_addr;\r\nu16 syndrome;\r\nif (!(m->status & MCI_STATUS_ADDRV)) {\r\namd64_mc_err(mci, "HW has no ERROR_ADDRESS available\n");\r\nedac_mc_handle_ce_no_info(mci, EDAC_MOD_STR);\r\nreturn;\r\n}\r\nsys_addr = get_error_address(m);\r\nsyndrome = extract_syndrome(m->status);\r\namd64_mc_err(mci, "CE ERROR_ADDRESS= 0x%llx\n", sys_addr);\r\npvt->ops->map_sysaddr_to_csrow(mci, sys_addr, syndrome);\r\n}\r\nstatic void amd64_handle_ue(struct mem_ctl_info *mci, struct mce *m)\r\n{\r\nstruct mem_ctl_info *log_mci, *src_mci = NULL;\r\nint csrow;\r\nu64 sys_addr;\r\nu32 page, offset;\r\nlog_mci = mci;\r\nif (!(m->status & MCI_STATUS_ADDRV)) {\r\namd64_mc_err(mci, "HW has no ERROR_ADDRESS available\n");\r\nedac_mc_handle_ue_no_info(log_mci, EDAC_MOD_STR);\r\nreturn;\r\n}\r\nsys_addr = get_error_address(m);\r\nsrc_mci = find_mc_by_sys_addr(mci, sys_addr);\r\nif (!src_mci) {\r\namd64_mc_err(mci, "ERROR ADDRESS (0x%lx) NOT mapped to a MC\n",\r\n(unsigned long)sys_addr);\r\nedac_mc_handle_ue_no_info(log_mci, EDAC_MOD_STR);\r\nreturn;\r\n}\r\nlog_mci = src_mci;\r\ncsrow = sys_addr_to_csrow(log_mci, sys_addr);\r\nif (csrow < 0) {\r\namd64_mc_err(mci, "ERROR_ADDRESS (0x%lx) NOT mapped to CS\n",\r\n(unsigned long)sys_addr);\r\nedac_mc_handle_ue_no_info(log_mci, EDAC_MOD_STR);\r\n} else {\r\nerror_address_to_page_and_offset(sys_addr, &page, &offset);\r\nedac_mc_handle_ue(log_mci, page, offset, csrow, EDAC_MOD_STR);\r\n}\r\n}\r\nstatic inline void __amd64_decode_bus_error(struct mem_ctl_info *mci,\r\nstruct mce *m)\r\n{\r\nu16 ec = EC(m->status);\r\nu8 xec = XEC(m->status, 0x1f);\r\nu8 ecc_type = (m->status >> 45) & 0x3;\r\nif (PP(ec) == NBSL_PP_OBS)\r\nreturn;\r\nif (xec && xec != F10_NBSL_EXT_ERR_ECC)\r\nreturn;\r\nif (ecc_type == 2)\r\namd64_handle_ce(mci, m);\r\nelse if (ecc_type == 1)\r\namd64_handle_ue(mci, m);\r\n}\r\nvoid amd64_decode_bus_error(int node_id, struct mce *m, u32 nbcfg)\r\n{\r\nstruct mem_ctl_info *mci = mcis[node_id];\r\n__amd64_decode_bus_error(mci, m);\r\n}\r\nstatic int reserve_mc_sibling_devs(struct amd64_pvt *pvt, u16 f1_id, u16 f3_id)\r\n{\r\npvt->F1 = pci_get_related_function(pvt->F2->vendor, f1_id, pvt->F2);\r\nif (!pvt->F1) {\r\namd64_err("error address map device not found: "\r\n"vendor %x device 0x%x (broken BIOS?)\n",\r\nPCI_VENDOR_ID_AMD, f1_id);\r\nreturn -ENODEV;\r\n}\r\npvt->F3 = pci_get_related_function(pvt->F2->vendor, f3_id, pvt->F2);\r\nif (!pvt->F3) {\r\npci_dev_put(pvt->F1);\r\npvt->F1 = NULL;\r\namd64_err("error F3 device not found: "\r\n"vendor %x device 0x%x (broken BIOS?)\n",\r\nPCI_VENDOR_ID_AMD, f3_id);\r\nreturn -ENODEV;\r\n}\r\ndebugf1("F1: %s\n", pci_name(pvt->F1));\r\ndebugf1("F2: %s\n", pci_name(pvt->F2));\r\ndebugf1("F3: %s\n", pci_name(pvt->F3));\r\nreturn 0;\r\n}\r\nstatic void free_mc_sibling_devs(struct amd64_pvt *pvt)\r\n{\r\npci_dev_put(pvt->F1);\r\npci_dev_put(pvt->F3);\r\n}\r\nstatic void read_mc_regs(struct amd64_pvt *pvt)\r\n{\r\nstruct cpuinfo_x86 *c = &boot_cpu_data;\r\nu64 msr_val;\r\nu32 tmp;\r\nunsigned range;\r\nrdmsrl(MSR_K8_TOP_MEM1, pvt->top_mem);\r\ndebugf0(" TOP_MEM: 0x%016llx\n", pvt->top_mem);\r\nrdmsrl(MSR_K8_SYSCFG, msr_val);\r\nif (msr_val & (1U << 21)) {\r\nrdmsrl(MSR_K8_TOP_MEM2, pvt->top_mem2);\r\ndebugf0(" TOP_MEM2: 0x%016llx\n", pvt->top_mem2);\r\n} else\r\ndebugf0(" TOP_MEM2 disabled.\n");\r\namd64_read_pci_cfg(pvt->F3, NBCAP, &pvt->nbcap);\r\nread_dram_ctl_register(pvt);\r\nfor (range = 0; range < DRAM_RANGES; range++) {\r\nu8 rw;\r\nread_dram_base_limit_regs(pvt, range);\r\nrw = dram_rw(pvt, range);\r\nif (!rw)\r\ncontinue;\r\ndebugf1(" DRAM range[%d], base: 0x%016llx; limit: 0x%016llx\n",\r\nrange,\r\nget_dram_base(pvt, range),\r\nget_dram_limit(pvt, range));\r\ndebugf1(" IntlvEn=%s; Range access: %s%s IntlvSel=%d DstNode=%d\n",\r\ndram_intlv_en(pvt, range) ? "Enabled" : "Disabled",\r\n(rw & 0x1) ? "R" : "-",\r\n(rw & 0x2) ? "W" : "-",\r\ndram_intlv_sel(pvt, range),\r\ndram_dst_node(pvt, range));\r\n}\r\nread_dct_base_mask(pvt);\r\namd64_read_pci_cfg(pvt->F1, DHAR, &pvt->dhar);\r\namd64_read_dct_pci_cfg(pvt, DBAM0, &pvt->dbam0);\r\namd64_read_pci_cfg(pvt->F3, F10_ONLINE_SPARE, &pvt->online_spare);\r\namd64_read_dct_pci_cfg(pvt, DCLR0, &pvt->dclr0);\r\namd64_read_dct_pci_cfg(pvt, DCHR0, &pvt->dchr0);\r\nif (!dct_ganging_enabled(pvt)) {\r\namd64_read_dct_pci_cfg(pvt, DCLR1, &pvt->dclr1);\r\namd64_read_dct_pci_cfg(pvt, DCHR1, &pvt->dchr1);\r\n}\r\npvt->ecc_sym_sz = 4;\r\nif (c->x86 >= 0x10) {\r\namd64_read_pci_cfg(pvt->F3, EXT_NB_MCA_CFG, &tmp);\r\namd64_read_dct_pci_cfg(pvt, DBAM1, &pvt->dbam1);\r\nif ((c->x86 > 0x10 || c->x86_model > 7) && tmp & BIT(25))\r\npvt->ecc_sym_sz = 8;\r\n}\r\ndump_misc_regs(pvt);\r\n}\r\nstatic u32 amd64_csrow_nr_pages(struct amd64_pvt *pvt, u8 dct, int csrow_nr)\r\n{\r\nu32 cs_mode, nr_pages;\r\ncs_mode = (pvt->dbam0 >> ((csrow_nr / 2) * 4)) & 0xF;\r\nnr_pages = pvt->ops->dbam_to_cs(pvt, dct, cs_mode) << (20 - PAGE_SHIFT);\r\nnr_pages <<= (pvt->channel_count - 1);\r\ndebugf0(" (csrow=%d) DBAM map index= %d\n", csrow_nr, cs_mode);\r\ndebugf0(" nr_pages= %u channel-count = %d\n",\r\nnr_pages, pvt->channel_count);\r\nreturn nr_pages;\r\n}\r\nstatic int init_csrows(struct mem_ctl_info *mci)\r\n{\r\nstruct csrow_info *csrow;\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nu64 input_addr_min, input_addr_max, sys_addr, base, mask;\r\nu32 val;\r\nint i, empty = 1;\r\namd64_read_pci_cfg(pvt->F3, NBCFG, &val);\r\npvt->nbcfg = val;\r\ndebugf0("node %d, NBCFG=0x%08x[ChipKillEccCap: %d|DramEccEn: %d]\n",\r\npvt->mc_node_id, val,\r\n!!(val & NBCFG_CHIPKILL), !!(val & NBCFG_ECC_ENABLE));\r\nfor_each_chip_select(i, 0, pvt) {\r\ncsrow = &mci->csrows[i];\r\nif (!csrow_enabled(i, 0, pvt)) {\r\ndebugf1("----CSROW %d EMPTY for node %d\n", i,\r\npvt->mc_node_id);\r\ncontinue;\r\n}\r\ndebugf1("----CSROW %d VALID for MC node %d\n",\r\ni, pvt->mc_node_id);\r\nempty = 0;\r\ncsrow->nr_pages = amd64_csrow_nr_pages(pvt, 0, i);\r\nfind_csrow_limits(mci, i, &input_addr_min, &input_addr_max);\r\nsys_addr = input_addr_to_sys_addr(mci, input_addr_min);\r\ncsrow->first_page = (u32) (sys_addr >> PAGE_SHIFT);\r\nsys_addr = input_addr_to_sys_addr(mci, input_addr_max);\r\ncsrow->last_page = (u32) (sys_addr >> PAGE_SHIFT);\r\nget_cs_base_and_mask(pvt, i, 0, &base, &mask);\r\ncsrow->page_mask = ~mask;\r\ncsrow->mtype = amd64_determine_memory_type(pvt, i);\r\ndebugf1(" for MC node %d csrow %d:\n", pvt->mc_node_id, i);\r\ndebugf1(" input_addr_min: 0x%lx input_addr_max: 0x%lx\n",\r\n(unsigned long)input_addr_min,\r\n(unsigned long)input_addr_max);\r\ndebugf1(" sys_addr: 0x%lx page_mask: 0x%lx\n",\r\n(unsigned long)sys_addr, csrow->page_mask);\r\ndebugf1(" nr_pages: %u first_page: 0x%lx "\r\n"last_page: 0x%lx\n",\r\n(unsigned)csrow->nr_pages,\r\ncsrow->first_page, csrow->last_page);\r\nif (pvt->nbcfg & NBCFG_ECC_ENABLE)\r\ncsrow->edac_mode =\r\n(pvt->nbcfg & NBCFG_CHIPKILL) ?\r\nEDAC_S4ECD4ED : EDAC_SECDED;\r\nelse\r\ncsrow->edac_mode = EDAC_NONE;\r\n}\r\nreturn empty;\r\n}\r\nstatic void get_cpus_on_this_dct_cpumask(struct cpumask *mask, unsigned nid)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nif (amd_get_nb_id(cpu) == nid)\r\ncpumask_set_cpu(cpu, mask);\r\n}\r\nstatic bool amd64_nb_mce_bank_enabled_on_node(unsigned nid)\r\n{\r\ncpumask_var_t mask;\r\nint cpu, nbe;\r\nbool ret = false;\r\nif (!zalloc_cpumask_var(&mask, GFP_KERNEL)) {\r\namd64_warn("%s: Error allocating mask\n", __func__);\r\nreturn false;\r\n}\r\nget_cpus_on_this_dct_cpumask(mask, nid);\r\nrdmsr_on_cpus(mask, MSR_IA32_MCG_CTL, msrs);\r\nfor_each_cpu(cpu, mask) {\r\nstruct msr *reg = per_cpu_ptr(msrs, cpu);\r\nnbe = reg->l & MSR_MCGCTL_NBE;\r\ndebugf0("core: %u, MCG_CTL: 0x%llx, NB MSR is %s\n",\r\ncpu, reg->q,\r\n(nbe ? "enabled" : "disabled"));\r\nif (!nbe)\r\ngoto out;\r\n}\r\nret = true;\r\nout:\r\nfree_cpumask_var(mask);\r\nreturn ret;\r\n}\r\nstatic int toggle_ecc_err_reporting(struct ecc_settings *s, u8 nid, bool on)\r\n{\r\ncpumask_var_t cmask;\r\nint cpu;\r\nif (!zalloc_cpumask_var(&cmask, GFP_KERNEL)) {\r\namd64_warn("%s: error allocating mask\n", __func__);\r\nreturn false;\r\n}\r\nget_cpus_on_this_dct_cpumask(cmask, nid);\r\nrdmsr_on_cpus(cmask, MSR_IA32_MCG_CTL, msrs);\r\nfor_each_cpu(cpu, cmask) {\r\nstruct msr *reg = per_cpu_ptr(msrs, cpu);\r\nif (on) {\r\nif (reg->l & MSR_MCGCTL_NBE)\r\ns->flags.nb_mce_enable = 1;\r\nreg->l |= MSR_MCGCTL_NBE;\r\n} else {\r\nif (!s->flags.nb_mce_enable)\r\nreg->l &= ~MSR_MCGCTL_NBE;\r\n}\r\n}\r\nwrmsr_on_cpus(cmask, MSR_IA32_MCG_CTL, msrs);\r\nfree_cpumask_var(cmask);\r\nreturn 0;\r\n}\r\nstatic bool enable_ecc_error_reporting(struct ecc_settings *s, u8 nid,\r\nstruct pci_dev *F3)\r\n{\r\nbool ret = true;\r\nu32 value, mask = 0x3;\r\nif (toggle_ecc_err_reporting(s, nid, ON)) {\r\namd64_warn("Error enabling ECC reporting over MCGCTL!\n");\r\nreturn false;\r\n}\r\namd64_read_pci_cfg(F3, NBCTL, &value);\r\ns->old_nbctl = value & mask;\r\ns->nbctl_valid = true;\r\nvalue |= mask;\r\namd64_write_pci_cfg(F3, NBCTL, value);\r\namd64_read_pci_cfg(F3, NBCFG, &value);\r\ndebugf0("1: node %d, NBCFG=0x%08x[DramEccEn: %d]\n",\r\nnid, value, !!(value & NBCFG_ECC_ENABLE));\r\nif (!(value & NBCFG_ECC_ENABLE)) {\r\namd64_warn("DRAM ECC disabled on this node, enabling...\n");\r\ns->flags.nb_ecc_prev = 0;\r\nvalue |= NBCFG_ECC_ENABLE;\r\namd64_write_pci_cfg(F3, NBCFG, value);\r\namd64_read_pci_cfg(F3, NBCFG, &value);\r\nif (!(value & NBCFG_ECC_ENABLE)) {\r\namd64_warn("Hardware rejected DRAM ECC enable,"\r\n"check memory DIMM configuration.\n");\r\nret = false;\r\n} else {\r\namd64_info("Hardware accepted DRAM ECC Enable\n");\r\n}\r\n} else {\r\ns->flags.nb_ecc_prev = 1;\r\n}\r\ndebugf0("2: node %d, NBCFG=0x%08x[DramEccEn: %d]\n",\r\nnid, value, !!(value & NBCFG_ECC_ENABLE));\r\nreturn ret;\r\n}\r\nstatic void restore_ecc_error_reporting(struct ecc_settings *s, u8 nid,\r\nstruct pci_dev *F3)\r\n{\r\nu32 value, mask = 0x3;\r\nif (!s->nbctl_valid)\r\nreturn;\r\namd64_read_pci_cfg(F3, NBCTL, &value);\r\nvalue &= ~mask;\r\nvalue |= s->old_nbctl;\r\namd64_write_pci_cfg(F3, NBCTL, value);\r\nif (!s->flags.nb_ecc_prev) {\r\namd64_read_pci_cfg(F3, NBCFG, &value);\r\nvalue &= ~NBCFG_ECC_ENABLE;\r\namd64_write_pci_cfg(F3, NBCFG, value);\r\n}\r\nif (toggle_ecc_err_reporting(s, nid, OFF))\r\namd64_warn("Error restoring NB MCGCTL settings!\n");\r\n}\r\nstatic bool ecc_enabled(struct pci_dev *F3, u8 nid)\r\n{\r\nu32 value;\r\nu8 ecc_en = 0;\r\nbool nb_mce_en = false;\r\namd64_read_pci_cfg(F3, NBCFG, &value);\r\necc_en = !!(value & NBCFG_ECC_ENABLE);\r\namd64_info("DRAM ECC %s.\n", (ecc_en ? "enabled" : "disabled"));\r\nnb_mce_en = amd64_nb_mce_bank_enabled_on_node(nid);\r\nif (!nb_mce_en)\r\namd64_notice("NB MCE bank disabled, set MSR "\r\n"0x%08x[4] on node %d to enable.\n",\r\nMSR_IA32_MCG_CTL, nid);\r\nif (!ecc_en || !nb_mce_en) {\r\namd64_notice("%s", ecc_msg);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic void set_mc_sysfs_attrs(struct mem_ctl_info *mci)\r\n{\r\nunsigned int i = 0, j = 0;\r\nfor (; i < ARRAY_SIZE(amd64_dbg_attrs); i++)\r\nsysfs_attrs[i] = amd64_dbg_attrs[i];\r\nif (boot_cpu_data.x86 >= 0x10)\r\nfor (j = 0; j < ARRAY_SIZE(amd64_inj_attrs); j++, i++)\r\nsysfs_attrs[i] = amd64_inj_attrs[j];\r\nsysfs_attrs[i] = terminator;\r\nmci->mc_driver_sysfs_attributes = sysfs_attrs;\r\n}\r\nstatic void setup_mci_misc_attrs(struct mem_ctl_info *mci,\r\nstruct amd64_family_type *fam)\r\n{\r\nstruct amd64_pvt *pvt = mci->pvt_info;\r\nmci->mtype_cap = MEM_FLAG_DDR2 | MEM_FLAG_RDDR2;\r\nmci->edac_ctl_cap = EDAC_FLAG_NONE;\r\nif (pvt->nbcap & NBCAP_SECDED)\r\nmci->edac_ctl_cap |= EDAC_FLAG_SECDED;\r\nif (pvt->nbcap & NBCAP_CHIPKILL)\r\nmci->edac_ctl_cap |= EDAC_FLAG_S4ECD4ED;\r\nmci->edac_cap = amd64_determine_edac_cap(pvt);\r\nmci->mod_name = EDAC_MOD_STR;\r\nmci->mod_ver = EDAC_AMD64_VERSION;\r\nmci->ctl_name = fam->ctl_name;\r\nmci->dev_name = pci_name(pvt->F2);\r\nmci->ctl_page_to_phys = NULL;\r\nmci->set_sdram_scrub_rate = amd64_set_scrub_rate;\r\nmci->get_sdram_scrub_rate = amd64_get_scrub_rate;\r\n}\r\nstatic struct amd64_family_type *amd64_per_family_init(struct amd64_pvt *pvt)\r\n{\r\nu8 fam = boot_cpu_data.x86;\r\nstruct amd64_family_type *fam_type = NULL;\r\nswitch (fam) {\r\ncase 0xf:\r\nfam_type = &amd64_family_types[K8_CPUS];\r\npvt->ops = &amd64_family_types[K8_CPUS].ops;\r\nbreak;\r\ncase 0x10:\r\nfam_type = &amd64_family_types[F10_CPUS];\r\npvt->ops = &amd64_family_types[F10_CPUS].ops;\r\nbreak;\r\ncase 0x15:\r\nfam_type = &amd64_family_types[F15_CPUS];\r\npvt->ops = &amd64_family_types[F15_CPUS].ops;\r\nbreak;\r\ndefault:\r\namd64_err("Unsupported family!\n");\r\nreturn NULL;\r\n}\r\npvt->ext_model = boot_cpu_data.x86_model >> 4;\r\namd64_info("%s %sdetected (node %d).\n", fam_type->ctl_name,\r\n(fam == 0xf ?\r\n(pvt->ext_model >= K8_REV_F ? "revF or later "\r\n: "revE or earlier ")\r\n: ""), pvt->mc_node_id);\r\nreturn fam_type;\r\n}\r\nstatic int amd64_init_one_instance(struct pci_dev *F2)\r\n{\r\nstruct amd64_pvt *pvt = NULL;\r\nstruct amd64_family_type *fam_type = NULL;\r\nstruct mem_ctl_info *mci = NULL;\r\nint err = 0, ret;\r\nu8 nid = get_node_id(F2);\r\nret = -ENOMEM;\r\npvt = kzalloc(sizeof(struct amd64_pvt), GFP_KERNEL);\r\nif (!pvt)\r\ngoto err_ret;\r\npvt->mc_node_id = nid;\r\npvt->F2 = F2;\r\nret = -EINVAL;\r\nfam_type = amd64_per_family_init(pvt);\r\nif (!fam_type)\r\ngoto err_free;\r\nret = -ENODEV;\r\nerr = reserve_mc_sibling_devs(pvt, fam_type->f1_id, fam_type->f3_id);\r\nif (err)\r\ngoto err_free;\r\nread_mc_regs(pvt);\r\nret = -EINVAL;\r\npvt->channel_count = pvt->ops->early_channel_count(pvt);\r\nif (pvt->channel_count < 0)\r\ngoto err_siblings;\r\nret = -ENOMEM;\r\nmci = edac_mc_alloc(0, pvt->csels[0].b_cnt, pvt->channel_count, nid);\r\nif (!mci)\r\ngoto err_siblings;\r\nmci->pvt_info = pvt;\r\nmci->dev = &pvt->F2->dev;\r\nsetup_mci_misc_attrs(mci, fam_type);\r\nif (init_csrows(mci))\r\nmci->edac_cap = EDAC_FLAG_NONE;\r\nset_mc_sysfs_attrs(mci);\r\nret = -ENODEV;\r\nif (edac_mc_add_mc(mci)) {\r\ndebugf1("failed edac_mc_add_mc()\n");\r\ngoto err_add_mc;\r\n}\r\nif (report_gart_errors)\r\namd_report_gart_errors(true);\r\namd_register_ecc_decoder(amd64_decode_bus_error);\r\nmcis[nid] = mci;\r\natomic_inc(&drv_instances);\r\nreturn 0;\r\nerr_add_mc:\r\nedac_mc_free(mci);\r\nerr_siblings:\r\nfree_mc_sibling_devs(pvt);\r\nerr_free:\r\nkfree(pvt);\r\nerr_ret:\r\nreturn ret;\r\n}\r\nstatic int __devinit amd64_probe_one_instance(struct pci_dev *pdev,\r\nconst struct pci_device_id *mc_type)\r\n{\r\nu8 nid = get_node_id(pdev);\r\nstruct pci_dev *F3 = node_to_amd_nb(nid)->misc;\r\nstruct ecc_settings *s;\r\nint ret = 0;\r\nret = pci_enable_device(pdev);\r\nif (ret < 0) {\r\ndebugf0("ret=%d\n", ret);\r\nreturn -EIO;\r\n}\r\nret = -ENOMEM;\r\ns = kzalloc(sizeof(struct ecc_settings), GFP_KERNEL);\r\nif (!s)\r\ngoto err_out;\r\necc_stngs[nid] = s;\r\nif (!ecc_enabled(F3, nid)) {\r\nret = -ENODEV;\r\nif (!ecc_enable_override)\r\ngoto err_enable;\r\namd64_warn("Forcing ECC on!\n");\r\nif (!enable_ecc_error_reporting(s, nid, F3))\r\ngoto err_enable;\r\n}\r\nret = amd64_init_one_instance(pdev);\r\nif (ret < 0) {\r\namd64_err("Error probing instance: %d\n", nid);\r\nrestore_ecc_error_reporting(s, nid, F3);\r\n}\r\nreturn ret;\r\nerr_enable:\r\nkfree(s);\r\necc_stngs[nid] = NULL;\r\nerr_out:\r\nreturn ret;\r\n}\r\nstatic void __devexit amd64_remove_one_instance(struct pci_dev *pdev)\r\n{\r\nstruct mem_ctl_info *mci;\r\nstruct amd64_pvt *pvt;\r\nu8 nid = get_node_id(pdev);\r\nstruct pci_dev *F3 = node_to_amd_nb(nid)->misc;\r\nstruct ecc_settings *s = ecc_stngs[nid];\r\nmci = edac_mc_del_mc(&pdev->dev);\r\nif (!mci)\r\nreturn;\r\npvt = mci->pvt_info;\r\nrestore_ecc_error_reporting(s, nid, F3);\r\nfree_mc_sibling_devs(pvt);\r\namd_report_gart_errors(false);\r\namd_unregister_ecc_decoder(amd64_decode_bus_error);\r\nkfree(ecc_stngs[nid]);\r\necc_stngs[nid] = NULL;\r\nmci->pvt_info = NULL;\r\nmcis[nid] = NULL;\r\nkfree(pvt);\r\nedac_mc_free(mci);\r\n}\r\nstatic void setup_pci_device(void)\r\n{\r\nstruct mem_ctl_info *mci;\r\nstruct amd64_pvt *pvt;\r\nif (amd64_ctl_pci)\r\nreturn;\r\nmci = mcis[0];\r\nif (mci) {\r\npvt = mci->pvt_info;\r\namd64_ctl_pci =\r\nedac_pci_create_generic_ctl(&pvt->F2->dev, EDAC_MOD_STR);\r\nif (!amd64_ctl_pci) {\r\npr_warning("%s(): Unable to create PCI control\n",\r\n__func__);\r\npr_warning("%s(): PCI error report via EDAC not set\n",\r\n__func__);\r\n}\r\n}\r\n}\r\nstatic int __init amd64_edac_init(void)\r\n{\r\nint err = -ENODEV;\r\nprintk(KERN_INFO "AMD64 EDAC driver v%s\n", EDAC_AMD64_VERSION);\r\nopstate_init();\r\nif (amd_cache_northbridges() < 0)\r\ngoto err_ret;\r\nerr = -ENOMEM;\r\nmcis = kzalloc(amd_nb_num() * sizeof(mcis[0]), GFP_KERNEL);\r\necc_stngs = kzalloc(amd_nb_num() * sizeof(ecc_stngs[0]), GFP_KERNEL);\r\nif (!(mcis && ecc_stngs))\r\ngoto err_free;\r\nmsrs = msrs_alloc();\r\nif (!msrs)\r\ngoto err_free;\r\nerr = pci_register_driver(&amd64_pci_driver);\r\nif (err)\r\ngoto err_pci;\r\nerr = -ENODEV;\r\nif (!atomic_read(&drv_instances))\r\ngoto err_no_instances;\r\nsetup_pci_device();\r\nreturn 0;\r\nerr_no_instances:\r\npci_unregister_driver(&amd64_pci_driver);\r\nerr_pci:\r\nmsrs_free(msrs);\r\nmsrs = NULL;\r\nerr_free:\r\nkfree(mcis);\r\nmcis = NULL;\r\nkfree(ecc_stngs);\r\necc_stngs = NULL;\r\nerr_ret:\r\nreturn err;\r\n}\r\nstatic void __exit amd64_edac_exit(void)\r\n{\r\nif (amd64_ctl_pci)\r\nedac_pci_release_generic_ctl(amd64_ctl_pci);\r\npci_unregister_driver(&amd64_pci_driver);\r\nkfree(ecc_stngs);\r\necc_stngs = NULL;\r\nkfree(mcis);\r\nmcis = NULL;\r\nmsrs_free(msrs);\r\nmsrs = NULL;\r\n}
