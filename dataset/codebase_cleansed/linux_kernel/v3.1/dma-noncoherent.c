static struct ppc_vm_region *\r\nppc_vm_region_alloc(struct ppc_vm_region *head, size_t size, gfp_t gfp)\r\n{\r\nunsigned long addr = head->vm_start, end = head->vm_end - size;\r\nunsigned long flags;\r\nstruct ppc_vm_region *c, *new;\r\nnew = kmalloc(sizeof(struct ppc_vm_region), gfp);\r\nif (!new)\r\ngoto out;\r\nspin_lock_irqsave(&consistent_lock, flags);\r\nlist_for_each_entry(c, &head->vm_list, vm_list) {\r\nif ((addr + size) < addr)\r\ngoto nospc;\r\nif ((addr + size) <= c->vm_start)\r\ngoto found;\r\naddr = c->vm_end;\r\nif (addr > end)\r\ngoto nospc;\r\n}\r\nfound:\r\nlist_add_tail(&new->vm_list, &c->vm_list);\r\nnew->vm_start = addr;\r\nnew->vm_end = addr + size;\r\nspin_unlock_irqrestore(&consistent_lock, flags);\r\nreturn new;\r\nnospc:\r\nspin_unlock_irqrestore(&consistent_lock, flags);\r\nkfree(new);\r\nout:\r\nreturn NULL;\r\n}\r\nstatic struct ppc_vm_region *ppc_vm_region_find(struct ppc_vm_region *head, unsigned long addr)\r\n{\r\nstruct ppc_vm_region *c;\r\nlist_for_each_entry(c, &head->vm_list, vm_list) {\r\nif (c->vm_start == addr)\r\ngoto out;\r\n}\r\nc = NULL;\r\nout:\r\nreturn c;\r\n}\r\nvoid *\r\n__dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp)\r\n{\r\nstruct page *page;\r\nstruct ppc_vm_region *c;\r\nunsigned long order;\r\nu64 mask = ISA_DMA_THRESHOLD, limit;\r\nif (dev) {\r\nmask = dev->coherent_dma_mask;\r\nif (mask == 0) {\r\ndev_warn(dev, "coherent DMA mask is unset\n");\r\ngoto no_page;\r\n}\r\nif ((~mask) & ISA_DMA_THRESHOLD) {\r\ndev_warn(dev, "coherent DMA mask %#llx is smaller "\r\n"than system GFP_DMA mask %#llx\n",\r\nmask, (unsigned long long)ISA_DMA_THRESHOLD);\r\ngoto no_page;\r\n}\r\n}\r\nsize = PAGE_ALIGN(size);\r\nlimit = (mask + 1) & ~mask;\r\nif ((limit && size >= limit) ||\r\nsize >= (CONSISTENT_END - CONSISTENT_BASE)) {\r\nprintk(KERN_WARNING "coherent allocation too big (requested %#x mask %#Lx)\n",\r\nsize, mask);\r\nreturn NULL;\r\n}\r\norder = get_order(size);\r\nif (mask != 0xffffffff)\r\ngfp |= GFP_DMA;\r\npage = alloc_pages(gfp, order);\r\nif (!page)\r\ngoto no_page;\r\n{\r\nunsigned long kaddr = (unsigned long)page_address(page);\r\nmemset(page_address(page), 0, size);\r\nflush_dcache_range(kaddr, kaddr + size);\r\n}\r\nc = ppc_vm_region_alloc(&consistent_head, size,\r\ngfp & ~(__GFP_DMA | __GFP_HIGHMEM));\r\nif (c) {\r\nunsigned long vaddr = c->vm_start;\r\nstruct page *end = page + (1 << order);\r\nsplit_page(page, order);\r\n*handle = page_to_phys(page);\r\ndo {\r\nSetPageReserved(page);\r\nmap_page(vaddr, page_to_phys(page),\r\npgprot_noncached(PAGE_KERNEL));\r\npage++;\r\nvaddr += PAGE_SIZE;\r\n} while (size -= PAGE_SIZE);\r\nwhile (page < end) {\r\n__free_page(page);\r\npage++;\r\n}\r\nreturn (void *)c->vm_start;\r\n}\r\nif (page)\r\n__free_pages(page, order);\r\nno_page:\r\nreturn NULL;\r\n}\r\nvoid __dma_free_coherent(size_t size, void *vaddr)\r\n{\r\nstruct ppc_vm_region *c;\r\nunsigned long flags, addr;\r\nsize = PAGE_ALIGN(size);\r\nspin_lock_irqsave(&consistent_lock, flags);\r\nc = ppc_vm_region_find(&consistent_head, (unsigned long)vaddr);\r\nif (!c)\r\ngoto no_area;\r\nif ((c->vm_end - c->vm_start) != size) {\r\nprintk(KERN_ERR "%s: freeing wrong coherent size (%ld != %d)\n",\r\n__func__, c->vm_end - c->vm_start, size);\r\ndump_stack();\r\nsize = c->vm_end - c->vm_start;\r\n}\r\naddr = c->vm_start;\r\ndo {\r\npte_t *ptep;\r\nunsigned long pfn;\r\nptep = pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k(addr),\r\naddr),\r\naddr),\r\naddr);\r\nif (!pte_none(*ptep) && pte_present(*ptep)) {\r\npfn = pte_pfn(*ptep);\r\npte_clear(&init_mm, addr, ptep);\r\nif (pfn_valid(pfn)) {\r\nstruct page *page = pfn_to_page(pfn);\r\nClearPageReserved(page);\r\n__free_page(page);\r\n}\r\n}\r\naddr += PAGE_SIZE;\r\n} while (size -= PAGE_SIZE);\r\nflush_tlb_kernel_range(c->vm_start, c->vm_end);\r\nlist_del(&c->vm_list);\r\nspin_unlock_irqrestore(&consistent_lock, flags);\r\nkfree(c);\r\nreturn;\r\nno_area:\r\nspin_unlock_irqrestore(&consistent_lock, flags);\r\nprintk(KERN_ERR "%s: trying to free invalid coherent area: %p\n",\r\n__func__, vaddr);\r\ndump_stack();\r\n}\r\nvoid __dma_sync(void *vaddr, size_t size, int direction)\r\n{\r\nunsigned long start = (unsigned long)vaddr;\r\nunsigned long end = start + size;\r\nswitch (direction) {\r\ncase DMA_NONE:\r\nBUG();\r\ncase DMA_FROM_DEVICE:\r\nif ((start & (L1_CACHE_BYTES - 1)) || (size & (L1_CACHE_BYTES - 1)))\r\nflush_dcache_range(start, end);\r\nelse\r\ninvalidate_dcache_range(start, end);\r\nbreak;\r\ncase DMA_TO_DEVICE:\r\nclean_dcache_range(start, end);\r\nbreak;\r\ncase DMA_BIDIRECTIONAL:\r\nflush_dcache_range(start, end);\r\nbreak;\r\n}\r\n}\r\nstatic inline void __dma_sync_page_highmem(struct page *page,\r\nunsigned long offset, size_t size, int direction)\r\n{\r\nsize_t seg_size = min((size_t)(PAGE_SIZE - offset), size);\r\nsize_t cur_size = seg_size;\r\nunsigned long flags, start, seg_offset = offset;\r\nint nr_segs = 1 + ((size - seg_size) + PAGE_SIZE - 1)/PAGE_SIZE;\r\nint seg_nr = 0;\r\nlocal_irq_save(flags);\r\ndo {\r\nstart = (unsigned long)kmap_atomic(page + seg_nr,\r\nKM_PPC_SYNC_PAGE) + seg_offset;\r\n__dma_sync((void *)start, seg_size, direction);\r\nkunmap_atomic((void *)start, KM_PPC_SYNC_PAGE);\r\nseg_nr++;\r\nseg_size = min((size_t)PAGE_SIZE, size - cur_size);\r\ncur_size += seg_size;\r\nseg_offset = 0;\r\n} while (seg_nr < nr_segs);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid __dma_sync_page(struct page *page, unsigned long offset,\r\nsize_t size, int direction)\r\n{\r\n#ifdef CONFIG_HIGHMEM\r\n__dma_sync_page_highmem(page, offset, size, direction);\r\n#else\r\nunsigned long start = (unsigned long)page_address(page) + offset;\r\n__dma_sync((void *)start, size, direction);\r\n#endif\r\n}\r\nunsigned long __dma_get_coherent_pfn(unsigned long cpu_addr)\r\n{\r\npgd_t *pgd = pgd_offset_k(cpu_addr);\r\npud_t *pud = pud_offset(pgd, cpu_addr);\r\npmd_t *pmd = pmd_offset(pud, cpu_addr);\r\npte_t *ptep = pte_offset_kernel(pmd, cpu_addr);\r\nif (pte_none(*ptep) || !pte_present(*ptep))\r\nreturn 0;\r\nreturn pte_pfn(*ptep);\r\n}
