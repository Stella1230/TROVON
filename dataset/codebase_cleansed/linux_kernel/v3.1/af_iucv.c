static inline void high_nmcpy(unsigned char *dst, char *src)\r\n{\r\nmemcpy(dst, src, 8);\r\n}\r\nstatic inline void low_nmcpy(unsigned char *dst, char *src)\r\n{\r\nmemcpy(&dst[8], src, 8);\r\n}\r\nstatic int afiucv_pm_prepare(struct device *dev)\r\n{\r\n#ifdef CONFIG_PM_DEBUG\r\nprintk(KERN_WARNING "afiucv_pm_prepare\n");\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void afiucv_pm_complete(struct device *dev)\r\n{\r\n#ifdef CONFIG_PM_DEBUG\r\nprintk(KERN_WARNING "afiucv_pm_complete\n");\r\n#endif\r\n}\r\nstatic int afiucv_pm_freeze(struct device *dev)\r\n{\r\nstruct iucv_sock *iucv;\r\nstruct sock *sk;\r\nstruct hlist_node *node;\r\nint err = 0;\r\n#ifdef CONFIG_PM_DEBUG\r\nprintk(KERN_WARNING "afiucv_pm_freeze\n");\r\n#endif\r\nread_lock(&iucv_sk_list.lock);\r\nsk_for_each(sk, node, &iucv_sk_list.head) {\r\niucv = iucv_sk(sk);\r\nskb_queue_purge(&iucv->send_skb_q);\r\nskb_queue_purge(&iucv->backlog_skb_q);\r\nswitch (sk->sk_state) {\r\ncase IUCV_SEVERED:\r\ncase IUCV_DISCONN:\r\ncase IUCV_CLOSING:\r\ncase IUCV_CONNECTED:\r\nif (iucv->path) {\r\nerr = iucv_path_sever(iucv->path, NULL);\r\niucv_path_free(iucv->path);\r\niucv->path = NULL;\r\n}\r\nbreak;\r\ncase IUCV_OPEN:\r\ncase IUCV_BOUND:\r\ncase IUCV_LISTEN:\r\ncase IUCV_CLOSED:\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nread_unlock(&iucv_sk_list.lock);\r\nreturn err;\r\n}\r\nstatic int afiucv_pm_restore_thaw(struct device *dev)\r\n{\r\nstruct sock *sk;\r\nstruct hlist_node *node;\r\n#ifdef CONFIG_PM_DEBUG\r\nprintk(KERN_WARNING "afiucv_pm_restore_thaw\n");\r\n#endif\r\nread_lock(&iucv_sk_list.lock);\r\nsk_for_each(sk, node, &iucv_sk_list.head) {\r\nswitch (sk->sk_state) {\r\ncase IUCV_CONNECTED:\r\nsk->sk_err = EPIPE;\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\nbreak;\r\ncase IUCV_DISCONN:\r\ncase IUCV_SEVERED:\r\ncase IUCV_CLOSING:\r\ncase IUCV_LISTEN:\r\ncase IUCV_BOUND:\r\ncase IUCV_OPEN:\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nread_unlock(&iucv_sk_list.lock);\r\nreturn 0;\r\n}\r\nstatic inline size_t iucv_msg_length(struct iucv_message *msg)\r\n{\r\nsize_t datalen;\r\nif (msg->flags & IUCV_IPRMDATA) {\r\ndatalen = 0xff - msg->rmmsg[7];\r\nreturn (datalen < 8) ? datalen : 8;\r\n}\r\nreturn msg->length;\r\n}\r\nstatic int iucv_sock_in_state(struct sock *sk, int state, int state2)\r\n{\r\nreturn (sk->sk_state == state || sk->sk_state == state2);\r\n}\r\nstatic inline int iucv_below_msglim(struct sock *sk)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nif (sk->sk_state != IUCV_CONNECTED)\r\nreturn 1;\r\nreturn (skb_queue_len(&iucv->send_skb_q) < iucv->path->msglim);\r\n}\r\nstatic void iucv_sock_wake_msglim(struct sock *sk)\r\n{\r\nstruct socket_wq *wq;\r\nrcu_read_lock();\r\nwq = rcu_dereference(sk->sk_wq);\r\nif (wq_has_sleeper(wq))\r\nwake_up_interruptible_all(&wq->wait);\r\nsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\r\nrcu_read_unlock();\r\n}\r\nstatic void iucv_sock_timeout(unsigned long arg)\r\n{\r\nstruct sock *sk = (struct sock *)arg;\r\nbh_lock_sock(sk);\r\nsk->sk_err = ETIMEDOUT;\r\nsk->sk_state_change(sk);\r\nbh_unlock_sock(sk);\r\niucv_sock_kill(sk);\r\nsock_put(sk);\r\n}\r\nstatic void iucv_sock_clear_timer(struct sock *sk)\r\n{\r\nsk_stop_timer(sk, &sk->sk_timer);\r\n}\r\nstatic struct sock *__iucv_get_sock_by_name(char *nm)\r\n{\r\nstruct sock *sk;\r\nstruct hlist_node *node;\r\nsk_for_each(sk, node, &iucv_sk_list.head)\r\nif (!memcmp(&iucv_sk(sk)->src_name, nm, 8))\r\nreturn sk;\r\nreturn NULL;\r\n}\r\nstatic void iucv_sock_destruct(struct sock *sk)\r\n{\r\nskb_queue_purge(&sk->sk_receive_queue);\r\nskb_queue_purge(&sk->sk_write_queue);\r\n}\r\nstatic void iucv_sock_cleanup_listen(struct sock *parent)\r\n{\r\nstruct sock *sk;\r\nwhile ((sk = iucv_accept_dequeue(parent, NULL))) {\r\niucv_sock_close(sk);\r\niucv_sock_kill(sk);\r\n}\r\nparent->sk_state = IUCV_CLOSED;\r\n}\r\nstatic void iucv_sock_kill(struct sock *sk)\r\n{\r\nif (!sock_flag(sk, SOCK_ZAPPED) || sk->sk_socket)\r\nreturn;\r\niucv_sock_unlink(&iucv_sk_list, sk);\r\nsock_set_flag(sk, SOCK_DEAD);\r\nsock_put(sk);\r\n}\r\nstatic void iucv_sock_close(struct sock *sk)\r\n{\r\nunsigned char user_data[16];\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nunsigned long timeo;\r\niucv_sock_clear_timer(sk);\r\nlock_sock(sk);\r\nswitch (sk->sk_state) {\r\ncase IUCV_LISTEN:\r\niucv_sock_cleanup_listen(sk);\r\nbreak;\r\ncase IUCV_CONNECTED:\r\ncase IUCV_DISCONN:\r\nsk->sk_state = IUCV_CLOSING;\r\nsk->sk_state_change(sk);\r\nif (!skb_queue_empty(&iucv->send_skb_q)) {\r\nif (sock_flag(sk, SOCK_LINGER) && sk->sk_lingertime)\r\ntimeo = sk->sk_lingertime;\r\nelse\r\ntimeo = IUCV_DISCONN_TIMEOUT;\r\niucv_sock_wait(sk,\r\niucv_sock_in_state(sk, IUCV_CLOSED, 0),\r\ntimeo);\r\n}\r\ncase IUCV_CLOSING:\r\nsk->sk_state = IUCV_CLOSED;\r\nsk->sk_state_change(sk);\r\nif (iucv->path) {\r\nlow_nmcpy(user_data, iucv->src_name);\r\nhigh_nmcpy(user_data, iucv->dst_name);\r\nASCEBC(user_data, sizeof(user_data));\r\niucv_path_sever(iucv->path, user_data);\r\niucv_path_free(iucv->path);\r\niucv->path = NULL;\r\n}\r\nsk->sk_err = ECONNRESET;\r\nsk->sk_state_change(sk);\r\nskb_queue_purge(&iucv->send_skb_q);\r\nskb_queue_purge(&iucv->backlog_skb_q);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nsock_set_flag(sk, SOCK_ZAPPED);\r\nrelease_sock(sk);\r\n}\r\nstatic void iucv_sock_init(struct sock *sk, struct sock *parent)\r\n{\r\nif (parent)\r\nsk->sk_type = parent->sk_type;\r\n}\r\nstatic struct sock *iucv_sock_alloc(struct socket *sock, int proto, gfp_t prio)\r\n{\r\nstruct sock *sk;\r\nsk = sk_alloc(&init_net, PF_IUCV, prio, &iucv_proto);\r\nif (!sk)\r\nreturn NULL;\r\nsock_init_data(sock, sk);\r\nINIT_LIST_HEAD(&iucv_sk(sk)->accept_q);\r\nspin_lock_init(&iucv_sk(sk)->accept_q_lock);\r\nskb_queue_head_init(&iucv_sk(sk)->send_skb_q);\r\nINIT_LIST_HEAD(&iucv_sk(sk)->message_q.list);\r\nspin_lock_init(&iucv_sk(sk)->message_q.lock);\r\nskb_queue_head_init(&iucv_sk(sk)->backlog_skb_q);\r\niucv_sk(sk)->send_tag = 0;\r\niucv_sk(sk)->flags = 0;\r\niucv_sk(sk)->msglimit = IUCV_QUEUELEN_DEFAULT;\r\niucv_sk(sk)->path = NULL;\r\nmemset(&iucv_sk(sk)->src_user_id , 0, 32);\r\nsk->sk_destruct = iucv_sock_destruct;\r\nsk->sk_sndtimeo = IUCV_CONN_TIMEOUT;\r\nsk->sk_allocation = GFP_DMA;\r\nsock_reset_flag(sk, SOCK_ZAPPED);\r\nsk->sk_protocol = proto;\r\nsk->sk_state = IUCV_OPEN;\r\nsetup_timer(&sk->sk_timer, iucv_sock_timeout, (unsigned long)sk);\r\niucv_sock_link(&iucv_sk_list, sk);\r\nreturn sk;\r\n}\r\nstatic int iucv_sock_create(struct net *net, struct socket *sock, int protocol,\r\nint kern)\r\n{\r\nstruct sock *sk;\r\nif (protocol && protocol != PF_IUCV)\r\nreturn -EPROTONOSUPPORT;\r\nsock->state = SS_UNCONNECTED;\r\nswitch (sock->type) {\r\ncase SOCK_STREAM:\r\nsock->ops = &iucv_sock_ops;\r\nbreak;\r\ncase SOCK_SEQPACKET:\r\nsock->ops = &iucv_sock_ops;\r\nbreak;\r\ndefault:\r\nreturn -ESOCKTNOSUPPORT;\r\n}\r\nsk = iucv_sock_alloc(sock, protocol, GFP_KERNEL);\r\nif (!sk)\r\nreturn -ENOMEM;\r\niucv_sock_init(sk, NULL);\r\nreturn 0;\r\n}\r\nvoid iucv_sock_link(struct iucv_sock_list *l, struct sock *sk)\r\n{\r\nwrite_lock_bh(&l->lock);\r\nsk_add_node(sk, &l->head);\r\nwrite_unlock_bh(&l->lock);\r\n}\r\nvoid iucv_sock_unlink(struct iucv_sock_list *l, struct sock *sk)\r\n{\r\nwrite_lock_bh(&l->lock);\r\nsk_del_node_init(sk);\r\nwrite_unlock_bh(&l->lock);\r\n}\r\nvoid iucv_accept_enqueue(struct sock *parent, struct sock *sk)\r\n{\r\nunsigned long flags;\r\nstruct iucv_sock *par = iucv_sk(parent);\r\nsock_hold(sk);\r\nspin_lock_irqsave(&par->accept_q_lock, flags);\r\nlist_add_tail(&iucv_sk(sk)->accept_q, &par->accept_q);\r\nspin_unlock_irqrestore(&par->accept_q_lock, flags);\r\niucv_sk(sk)->parent = parent;\r\nsk_acceptq_added(parent);\r\n}\r\nvoid iucv_accept_unlink(struct sock *sk)\r\n{\r\nunsigned long flags;\r\nstruct iucv_sock *par = iucv_sk(iucv_sk(sk)->parent);\r\nspin_lock_irqsave(&par->accept_q_lock, flags);\r\nlist_del_init(&iucv_sk(sk)->accept_q);\r\nspin_unlock_irqrestore(&par->accept_q_lock, flags);\r\nsk_acceptq_removed(iucv_sk(sk)->parent);\r\niucv_sk(sk)->parent = NULL;\r\nsock_put(sk);\r\n}\r\nstruct sock *iucv_accept_dequeue(struct sock *parent, struct socket *newsock)\r\n{\r\nstruct iucv_sock *isk, *n;\r\nstruct sock *sk;\r\nlist_for_each_entry_safe(isk, n, &iucv_sk(parent)->accept_q, accept_q) {\r\nsk = (struct sock *) isk;\r\nlock_sock(sk);\r\nif (sk->sk_state == IUCV_CLOSED) {\r\niucv_accept_unlink(sk);\r\nrelease_sock(sk);\r\ncontinue;\r\n}\r\nif (sk->sk_state == IUCV_CONNECTED ||\r\nsk->sk_state == IUCV_SEVERED ||\r\nsk->sk_state == IUCV_DISCONN ||\r\n!newsock) {\r\niucv_accept_unlink(sk);\r\nif (newsock)\r\nsock_graft(sk, newsock);\r\nif (sk->sk_state == IUCV_SEVERED)\r\nsk->sk_state = IUCV_DISCONN;\r\nrelease_sock(sk);\r\nreturn sk;\r\n}\r\nrelease_sock(sk);\r\n}\r\nreturn NULL;\r\n}\r\nstatic int iucv_sock_bind(struct socket *sock, struct sockaddr *addr,\r\nint addr_len)\r\n{\r\nstruct sockaddr_iucv *sa = (struct sockaddr_iucv *) addr;\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv;\r\nint err;\r\nif (!addr || addr->sa_family != AF_IUCV)\r\nreturn -EINVAL;\r\nlock_sock(sk);\r\nif (sk->sk_state != IUCV_OPEN) {\r\nerr = -EBADFD;\r\ngoto done;\r\n}\r\nwrite_lock_bh(&iucv_sk_list.lock);\r\niucv = iucv_sk(sk);\r\nif (__iucv_get_sock_by_name(sa->siucv_name)) {\r\nerr = -EADDRINUSE;\r\ngoto done_unlock;\r\n}\r\nif (iucv->path) {\r\nerr = 0;\r\ngoto done_unlock;\r\n}\r\nmemcpy(iucv->src_name, sa->siucv_name, 8);\r\nmemcpy(iucv->src_user_id, iucv_userid, 8);\r\nsk->sk_state = IUCV_BOUND;\r\nerr = 0;\r\ndone_unlock:\r\nwrite_unlock_bh(&iucv_sk_list.lock);\r\ndone:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_autobind(struct sock *sk)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nchar query_buffer[80];\r\nchar name[12];\r\nint err = 0;\r\ncpcmd("QUERY USERID", query_buffer, sizeof(query_buffer), &err);\r\nif (unlikely(err))\r\nreturn -EPROTO;\r\nmemcpy(iucv->src_user_id, query_buffer, 8);\r\nwrite_lock_bh(&iucv_sk_list.lock);\r\nsprintf(name, "%08x", atomic_inc_return(&iucv_sk_list.autobind_name));\r\nwhile (__iucv_get_sock_by_name(name)) {\r\nsprintf(name, "%08x",\r\natomic_inc_return(&iucv_sk_list.autobind_name));\r\n}\r\nwrite_unlock_bh(&iucv_sk_list.lock);\r\nmemcpy(&iucv->src_name, name, 8);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_connect(struct socket *sock, struct sockaddr *addr,\r\nint alen, int flags)\r\n{\r\nstruct sockaddr_iucv *sa = (struct sockaddr_iucv *) addr;\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv;\r\nunsigned char user_data[16];\r\nint err;\r\nif (addr->sa_family != AF_IUCV || alen < sizeof(struct sockaddr_iucv))\r\nreturn -EINVAL;\r\nif (sk->sk_state != IUCV_OPEN && sk->sk_state != IUCV_BOUND)\r\nreturn -EBADFD;\r\nif (sk->sk_type != SOCK_STREAM && sk->sk_type != SOCK_SEQPACKET)\r\nreturn -EINVAL;\r\nif (sk->sk_state == IUCV_OPEN) {\r\nerr = iucv_sock_autobind(sk);\r\nif (unlikely(err))\r\nreturn err;\r\n}\r\nlock_sock(sk);\r\nmemcpy(iucv_sk(sk)->dst_user_id, sa->siucv_user_id, 8);\r\nmemcpy(iucv_sk(sk)->dst_name, sa->siucv_name, 8);\r\nhigh_nmcpy(user_data, sa->siucv_name);\r\nlow_nmcpy(user_data, iucv_sk(sk)->src_name);\r\nASCEBC(user_data, sizeof(user_data));\r\niucv = iucv_sk(sk);\r\niucv->path = iucv_path_alloc(iucv->msglimit,\r\nIUCV_IPRMDATA, GFP_KERNEL);\r\nif (!iucv->path) {\r\nerr = -ENOMEM;\r\ngoto done;\r\n}\r\nerr = iucv_path_connect(iucv->path, &af_iucv_handler,\r\nsa->siucv_user_id, NULL, user_data, sk);\r\nif (err) {\r\niucv_path_free(iucv->path);\r\niucv->path = NULL;\r\nswitch (err) {\r\ncase 0x0b:\r\nerr = -ENETUNREACH;\r\nbreak;\r\ncase 0x0d:\r\ncase 0x0e:\r\nerr = -EAGAIN;\r\nbreak;\r\ncase 0x0f:\r\nerr = -EACCES;\r\nbreak;\r\ndefault:\r\nerr = -ECONNREFUSED;\r\nbreak;\r\n}\r\ngoto done;\r\n}\r\nif (sk->sk_state != IUCV_CONNECTED) {\r\nerr = iucv_sock_wait(sk, iucv_sock_in_state(sk, IUCV_CONNECTED,\r\nIUCV_DISCONN),\r\nsock_sndtimeo(sk, flags & O_NONBLOCK));\r\n}\r\nif (sk->sk_state == IUCV_DISCONN) {\r\nerr = -ECONNREFUSED;\r\n}\r\nif (err) {\r\niucv_path_sever(iucv->path, NULL);\r\niucv_path_free(iucv->path);\r\niucv->path = NULL;\r\n}\r\ndone:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_listen(struct socket *sock, int backlog)\r\n{\r\nstruct sock *sk = sock->sk;\r\nint err;\r\nlock_sock(sk);\r\nerr = -EINVAL;\r\nif (sk->sk_state != IUCV_BOUND)\r\ngoto done;\r\nif (sock->type != SOCK_STREAM && sock->type != SOCK_SEQPACKET)\r\ngoto done;\r\nsk->sk_max_ack_backlog = backlog;\r\nsk->sk_ack_backlog = 0;\r\nsk->sk_state = IUCV_LISTEN;\r\nerr = 0;\r\ndone:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_accept(struct socket *sock, struct socket *newsock,\r\nint flags)\r\n{\r\nDECLARE_WAITQUEUE(wait, current);\r\nstruct sock *sk = sock->sk, *nsk;\r\nlong timeo;\r\nint err = 0;\r\nlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\r\nif (sk->sk_state != IUCV_LISTEN) {\r\nerr = -EBADFD;\r\ngoto done;\r\n}\r\ntimeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\r\nadd_wait_queue_exclusive(sk_sleep(sk), &wait);\r\nwhile (!(nsk = iucv_accept_dequeue(sk, newsock))) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (!timeo) {\r\nerr = -EAGAIN;\r\nbreak;\r\n}\r\nrelease_sock(sk);\r\ntimeo = schedule_timeout(timeo);\r\nlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\r\nif (sk->sk_state != IUCV_LISTEN) {\r\nerr = -EBADFD;\r\nbreak;\r\n}\r\nif (signal_pending(current)) {\r\nerr = sock_intr_errno(timeo);\r\nbreak;\r\n}\r\n}\r\nset_current_state(TASK_RUNNING);\r\nremove_wait_queue(sk_sleep(sk), &wait);\r\nif (err)\r\ngoto done;\r\nnewsock->state = SS_CONNECTED;\r\ndone:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_getname(struct socket *sock, struct sockaddr *addr,\r\nint *len, int peer)\r\n{\r\nstruct sockaddr_iucv *siucv = (struct sockaddr_iucv *) addr;\r\nstruct sock *sk = sock->sk;\r\naddr->sa_family = AF_IUCV;\r\n*len = sizeof(struct sockaddr_iucv);\r\nif (peer) {\r\nmemcpy(siucv->siucv_user_id, iucv_sk(sk)->dst_user_id, 8);\r\nmemcpy(siucv->siucv_name, &iucv_sk(sk)->dst_name, 8);\r\n} else {\r\nmemcpy(siucv->siucv_user_id, iucv_sk(sk)->src_user_id, 8);\r\nmemcpy(siucv->siucv_name, iucv_sk(sk)->src_name, 8);\r\n}\r\nmemset(&siucv->siucv_port, 0, sizeof(siucv->siucv_port));\r\nmemset(&siucv->siucv_addr, 0, sizeof(siucv->siucv_addr));\r\nmemset(siucv->siucv_nodeid, 0, sizeof(siucv->siucv_nodeid));\r\nreturn 0;\r\n}\r\nstatic int iucv_send_iprm(struct iucv_path *path, struct iucv_message *msg,\r\nstruct sk_buff *skb)\r\n{\r\nu8 prmdata[8];\r\nmemcpy(prmdata, (void *) skb->data, skb->len);\r\nprmdata[7] = 0xff - (u8) skb->len;\r\nreturn iucv_message_send(path, msg, IUCV_IPRMDATA, 0,\r\n(void *) prmdata, 8);\r\n}\r\nstatic int iucv_sock_sendmsg(struct kiocb *iocb, struct socket *sock,\r\nstruct msghdr *msg, size_t len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct sk_buff *skb;\r\nstruct iucv_message txmsg;\r\nstruct cmsghdr *cmsg;\r\nint cmsg_done;\r\nlong timeo;\r\nchar user_id[9];\r\nchar appl_id[9];\r\nint err;\r\nint noblock = msg->msg_flags & MSG_DONTWAIT;\r\nerr = sock_error(sk);\r\nif (err)\r\nreturn err;\r\nif (msg->msg_flags & MSG_OOB)\r\nreturn -EOPNOTSUPP;\r\nif (sk->sk_type == SOCK_SEQPACKET && !(msg->msg_flags & MSG_EOR))\r\nreturn -EOPNOTSUPP;\r\nlock_sock(sk);\r\nif (sk->sk_shutdown & SEND_SHUTDOWN) {\r\nerr = -EPIPE;\r\ngoto out;\r\n}\r\nif (sk->sk_state != IUCV_CONNECTED) {\r\nerr = -ENOTCONN;\r\ngoto out;\r\n}\r\ncmsg_done = 0;\r\ntxmsg.class = 0;\r\nfor (cmsg = CMSG_FIRSTHDR(msg); cmsg;\r\ncmsg = CMSG_NXTHDR(msg, cmsg)) {\r\nif (!CMSG_OK(msg, cmsg)) {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nif (cmsg->cmsg_level != SOL_IUCV)\r\ncontinue;\r\nif (cmsg->cmsg_type & cmsg_done) {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\ncmsg_done |= cmsg->cmsg_type;\r\nswitch (cmsg->cmsg_type) {\r\ncase SCM_IUCV_TRGCLS:\r\nif (cmsg->cmsg_len != CMSG_LEN(TRGCLS_SIZE)) {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nmemcpy(&txmsg.class,\r\n(void *) CMSG_DATA(cmsg), TRGCLS_SIZE);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\ngoto out;\r\nbreak;\r\n}\r\n}\r\nskb = sock_alloc_send_skb(sk, len, noblock, &err);\r\nif (!skb)\r\ngoto out;\r\nif (memcpy_fromiovec(skb_put(skb, len), msg->msg_iov, len)) {\r\nerr = -EFAULT;\r\ngoto fail;\r\n}\r\ntimeo = sock_sndtimeo(sk, noblock);\r\nerr = iucv_sock_wait(sk, iucv_below_msglim(sk), timeo);\r\nif (err)\r\ngoto fail;\r\nif (sk->sk_state != IUCV_CONNECTED) {\r\nerr = -ECONNRESET;\r\ngoto fail;\r\n}\r\ntxmsg.tag = iucv->send_tag++;\r\nmemcpy(CB_TAG(skb), &txmsg.tag, CB_TAG_LEN);\r\nskb_queue_tail(&iucv->send_skb_q, skb);\r\nif (((iucv->path->flags & IUCV_IPRMDATA) & iucv->flags)\r\n&& skb->len <= 7) {\r\nerr = iucv_send_iprm(iucv->path, &txmsg, skb);\r\nif (err == 0) {\r\nskb_unlink(skb, &iucv->send_skb_q);\r\nkfree_skb(skb);\r\n}\r\nif (err == 0x15) {\r\niucv_path_sever(iucv->path, NULL);\r\nskb_unlink(skb, &iucv->send_skb_q);\r\nerr = -EPIPE;\r\ngoto fail;\r\n}\r\n} else\r\nerr = iucv_message_send(iucv->path, &txmsg, 0, 0,\r\n(void *) skb->data, skb->len);\r\nif (err) {\r\nif (err == 3) {\r\nuser_id[8] = 0;\r\nmemcpy(user_id, iucv->dst_user_id, 8);\r\nappl_id[8] = 0;\r\nmemcpy(appl_id, iucv->dst_name, 8);\r\npr_err("Application %s on z/VM guest %s"\r\n" exceeds message limit\n",\r\nappl_id, user_id);\r\nerr = -EAGAIN;\r\n} else\r\nerr = -EPIPE;\r\nskb_unlink(skb, &iucv->send_skb_q);\r\ngoto fail;\r\n}\r\nrelease_sock(sk);\r\nreturn len;\r\nfail:\r\nkfree_skb(skb);\r\nout:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_fragment_skb(struct sock *sk, struct sk_buff *skb, int len)\r\n{\r\nint dataleft, size, copied = 0;\r\nstruct sk_buff *nskb;\r\ndataleft = len;\r\nwhile (dataleft) {\r\nif (dataleft >= sk->sk_rcvbuf / 4)\r\nsize = sk->sk_rcvbuf / 4;\r\nelse\r\nsize = dataleft;\r\nnskb = alloc_skb(size, GFP_ATOMIC | GFP_DMA);\r\nif (!nskb)\r\nreturn -ENOMEM;\r\nmemcpy(CB_TRGCLS(nskb), CB_TRGCLS(skb), CB_TRGCLS_LEN);\r\nmemcpy(nskb->data, skb->data + copied, size);\r\ncopied += size;\r\ndataleft -= size;\r\nskb_reset_transport_header(nskb);\r\nskb_reset_network_header(nskb);\r\nnskb->len = size;\r\nskb_queue_tail(&iucv_sk(sk)->backlog_skb_q, nskb);\r\n}\r\nreturn 0;\r\n}\r\nstatic void iucv_process_message(struct sock *sk, struct sk_buff *skb,\r\nstruct iucv_path *path,\r\nstruct iucv_message *msg)\r\n{\r\nint rc;\r\nunsigned int len;\r\nlen = iucv_msg_length(msg);\r\nmemcpy(CB_TRGCLS(skb), &msg->class, CB_TRGCLS_LEN);\r\nif ((msg->flags & IUCV_IPRMDATA) && len > 7) {\r\nif (memcmp(msg->rmmsg, iprm_shutdown, 8) == 0) {\r\nskb->data = NULL;\r\nskb->len = 0;\r\n}\r\n} else {\r\nrc = iucv_message_receive(path, msg, msg->flags & IUCV_IPRMDATA,\r\nskb->data, len, NULL);\r\nif (rc) {\r\nkfree_skb(skb);\r\nreturn;\r\n}\r\nif (sk->sk_type == SOCK_STREAM &&\r\nskb->truesize >= sk->sk_rcvbuf / 4) {\r\nrc = iucv_fragment_skb(sk, skb, len);\r\nkfree_skb(skb);\r\nskb = NULL;\r\nif (rc) {\r\niucv_path_sever(path, NULL);\r\nreturn;\r\n}\r\nskb = skb_dequeue(&iucv_sk(sk)->backlog_skb_q);\r\n} else {\r\nskb_reset_transport_header(skb);\r\nskb_reset_network_header(skb);\r\nskb->len = len;\r\n}\r\n}\r\nif (sock_queue_rcv_skb(sk, skb))\r\nskb_queue_head(&iucv_sk(sk)->backlog_skb_q, skb);\r\n}\r\nstatic void iucv_process_message_q(struct sock *sk)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct sk_buff *skb;\r\nstruct sock_msg_q *p, *n;\r\nlist_for_each_entry_safe(p, n, &iucv->message_q.list, list) {\r\nskb = alloc_skb(iucv_msg_length(&p->msg), GFP_ATOMIC | GFP_DMA);\r\nif (!skb)\r\nbreak;\r\niucv_process_message(sk, skb, p->path, &p->msg);\r\nlist_del(&p->list);\r\nkfree(p);\r\nif (!skb_queue_empty(&iucv->backlog_skb_q))\r\nbreak;\r\n}\r\n}\r\nstatic int iucv_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\r\nstruct msghdr *msg, size_t len, int flags)\r\n{\r\nint noblock = flags & MSG_DONTWAIT;\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nunsigned int copied, rlen;\r\nstruct sk_buff *skb, *rskb, *cskb;\r\nint err = 0;\r\nif ((sk->sk_state == IUCV_DISCONN || sk->sk_state == IUCV_SEVERED) &&\r\nskb_queue_empty(&iucv->backlog_skb_q) &&\r\nskb_queue_empty(&sk->sk_receive_queue) &&\r\nlist_empty(&iucv->message_q.list))\r\nreturn 0;\r\nif (flags & (MSG_OOB))\r\nreturn -EOPNOTSUPP;\r\nskb = skb_recv_datagram(sk, flags, noblock, &err);\r\nif (!skb) {\r\nif (sk->sk_shutdown & RCV_SHUTDOWN)\r\nreturn 0;\r\nreturn err;\r\n}\r\nrlen = skb->len;\r\ncopied = min_t(unsigned int, rlen, len);\r\ncskb = skb;\r\nif (memcpy_toiovec(msg->msg_iov, cskb->data, copied)) {\r\nif (!(flags & MSG_PEEK))\r\nskb_queue_head(&sk->sk_receive_queue, skb);\r\nreturn -EFAULT;\r\n}\r\nif (sk->sk_type == SOCK_SEQPACKET) {\r\nif (copied < rlen)\r\nmsg->msg_flags |= MSG_TRUNC;\r\nmsg->msg_flags |= MSG_EOR;\r\n}\r\nerr = put_cmsg(msg, SOL_IUCV, SCM_IUCV_TRGCLS,\r\nCB_TRGCLS_LEN, CB_TRGCLS(skb));\r\nif (err) {\r\nif (!(flags & MSG_PEEK))\r\nskb_queue_head(&sk->sk_receive_queue, skb);\r\nreturn err;\r\n}\r\nif (!(flags & MSG_PEEK)) {\r\nif (sk->sk_type == SOCK_STREAM) {\r\nskb_pull(skb, copied);\r\nif (skb->len) {\r\nskb_queue_head(&sk->sk_receive_queue, skb);\r\ngoto done;\r\n}\r\n}\r\nkfree_skb(skb);\r\nspin_lock_bh(&iucv->message_q.lock);\r\nrskb = skb_dequeue(&iucv->backlog_skb_q);\r\nwhile (rskb) {\r\nif (sock_queue_rcv_skb(sk, rskb)) {\r\nskb_queue_head(&iucv->backlog_skb_q,\r\nrskb);\r\nbreak;\r\n} else {\r\nrskb = skb_dequeue(&iucv->backlog_skb_q);\r\n}\r\n}\r\nif (skb_queue_empty(&iucv->backlog_skb_q)) {\r\nif (!list_empty(&iucv->message_q.list))\r\niucv_process_message_q(sk);\r\n}\r\nspin_unlock_bh(&iucv->message_q.lock);\r\n}\r\ndone:\r\nif (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC))\r\ncopied = rlen;\r\nreturn copied;\r\n}\r\nstatic inline unsigned int iucv_accept_poll(struct sock *parent)\r\n{\r\nstruct iucv_sock *isk, *n;\r\nstruct sock *sk;\r\nlist_for_each_entry_safe(isk, n, &iucv_sk(parent)->accept_q, accept_q) {\r\nsk = (struct sock *) isk;\r\nif (sk->sk_state == IUCV_CONNECTED)\r\nreturn POLLIN | POLLRDNORM;\r\n}\r\nreturn 0;\r\n}\r\nunsigned int iucv_sock_poll(struct file *file, struct socket *sock,\r\npoll_table *wait)\r\n{\r\nstruct sock *sk = sock->sk;\r\nunsigned int mask = 0;\r\nsock_poll_wait(file, sk_sleep(sk), wait);\r\nif (sk->sk_state == IUCV_LISTEN)\r\nreturn iucv_accept_poll(sk);\r\nif (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue))\r\nmask |= POLLERR;\r\nif (sk->sk_shutdown & RCV_SHUTDOWN)\r\nmask |= POLLRDHUP;\r\nif (sk->sk_shutdown == SHUTDOWN_MASK)\r\nmask |= POLLHUP;\r\nif (!skb_queue_empty(&sk->sk_receive_queue) ||\r\n(sk->sk_shutdown & RCV_SHUTDOWN))\r\nmask |= POLLIN | POLLRDNORM;\r\nif (sk->sk_state == IUCV_CLOSED)\r\nmask |= POLLHUP;\r\nif (sk->sk_state == IUCV_DISCONN || sk->sk_state == IUCV_SEVERED)\r\nmask |= POLLIN;\r\nif (sock_writeable(sk))\r\nmask |= POLLOUT | POLLWRNORM | POLLWRBAND;\r\nelse\r\nset_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\r\nreturn mask;\r\n}\r\nstatic int iucv_sock_shutdown(struct socket *sock, int how)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct iucv_message txmsg;\r\nint err = 0;\r\nhow++;\r\nif ((how & ~SHUTDOWN_MASK) || !how)\r\nreturn -EINVAL;\r\nlock_sock(sk);\r\nswitch (sk->sk_state) {\r\ncase IUCV_DISCONN:\r\ncase IUCV_CLOSING:\r\ncase IUCV_SEVERED:\r\ncase IUCV_CLOSED:\r\nerr = -ENOTCONN;\r\ngoto fail;\r\ndefault:\r\nsk->sk_shutdown |= how;\r\nbreak;\r\n}\r\nif (how == SEND_SHUTDOWN || how == SHUTDOWN_MASK) {\r\ntxmsg.class = 0;\r\ntxmsg.tag = 0;\r\nerr = iucv_message_send(iucv->path, &txmsg, IUCV_IPRMDATA, 0,\r\n(void *) iprm_shutdown, 8);\r\nif (err) {\r\nswitch (err) {\r\ncase 1:\r\nerr = -ENOTCONN;\r\nbreak;\r\ncase 2:\r\nerr = -ECONNRESET;\r\nbreak;\r\ndefault:\r\nerr = -ENOTCONN;\r\nbreak;\r\n}\r\n}\r\n}\r\nif (how == RCV_SHUTDOWN || how == SHUTDOWN_MASK) {\r\nerr = iucv_path_quiesce(iucv_sk(sk)->path, NULL);\r\nif (err)\r\nerr = -ENOTCONN;\r\nskb_queue_purge(&sk->sk_receive_queue);\r\n}\r\nsk->sk_state_change(sk);\r\nfail:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_release(struct socket *sock)\r\n{\r\nstruct sock *sk = sock->sk;\r\nint err = 0;\r\nif (!sk)\r\nreturn 0;\r\niucv_sock_close(sk);\r\nif (iucv_sk(sk)->path) {\r\niucv_path_sever(iucv_sk(sk)->path, NULL);\r\niucv_path_free(iucv_sk(sk)->path);\r\niucv_sk(sk)->path = NULL;\r\n}\r\nsock_orphan(sk);\r\niucv_sock_kill(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_setsockopt(struct socket *sock, int level, int optname,\r\nchar __user *optval, unsigned int optlen)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nint val;\r\nint rc;\r\nif (level != SOL_IUCV)\r\nreturn -ENOPROTOOPT;\r\nif (optlen < sizeof(int))\r\nreturn -EINVAL;\r\nif (get_user(val, (int __user *) optval))\r\nreturn -EFAULT;\r\nrc = 0;\r\nlock_sock(sk);\r\nswitch (optname) {\r\ncase SO_IPRMDATA_MSG:\r\nif (val)\r\niucv->flags |= IUCV_IPRMDATA;\r\nelse\r\niucv->flags &= ~IUCV_IPRMDATA;\r\nbreak;\r\ncase SO_MSGLIMIT:\r\nswitch (sk->sk_state) {\r\ncase IUCV_OPEN:\r\ncase IUCV_BOUND:\r\nif (val < 1 || val > (u16)(~0))\r\nrc = -EINVAL;\r\nelse\r\niucv->msglimit = val;\r\nbreak;\r\ndefault:\r\nrc = -EINVAL;\r\nbreak;\r\n}\r\nbreak;\r\ndefault:\r\nrc = -ENOPROTOOPT;\r\nbreak;\r\n}\r\nrelease_sock(sk);\r\nreturn rc;\r\n}\r\nstatic int iucv_sock_getsockopt(struct socket *sock, int level, int optname,\r\nchar __user *optval, int __user *optlen)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nint val, len;\r\nif (level != SOL_IUCV)\r\nreturn -ENOPROTOOPT;\r\nif (get_user(len, optlen))\r\nreturn -EFAULT;\r\nif (len < 0)\r\nreturn -EINVAL;\r\nlen = min_t(unsigned int, len, sizeof(int));\r\nswitch (optname) {\r\ncase SO_IPRMDATA_MSG:\r\nval = (iucv->flags & IUCV_IPRMDATA) ? 1 : 0;\r\nbreak;\r\ncase SO_MSGLIMIT:\r\nlock_sock(sk);\r\nval = (iucv->path != NULL) ? iucv->path->msglim\r\n: iucv->msglimit;\r\nrelease_sock(sk);\r\nbreak;\r\ndefault:\r\nreturn -ENOPROTOOPT;\r\n}\r\nif (put_user(len, optlen))\r\nreturn -EFAULT;\r\nif (copy_to_user(optval, &val, len))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int iucv_callback_connreq(struct iucv_path *path,\r\nu8 ipvmid[8], u8 ipuser[16])\r\n{\r\nunsigned char user_data[16];\r\nunsigned char nuser_data[16];\r\nunsigned char src_name[8];\r\nstruct hlist_node *node;\r\nstruct sock *sk, *nsk;\r\nstruct iucv_sock *iucv, *niucv;\r\nint err;\r\nmemcpy(src_name, ipuser, 8);\r\nEBCASC(src_name, 8);\r\nread_lock(&iucv_sk_list.lock);\r\niucv = NULL;\r\nsk = NULL;\r\nsk_for_each(sk, node, &iucv_sk_list.head)\r\nif (sk->sk_state == IUCV_LISTEN &&\r\n!memcmp(&iucv_sk(sk)->src_name, src_name, 8)) {\r\niucv = iucv_sk(sk);\r\nbreak;\r\n}\r\nread_unlock(&iucv_sk_list.lock);\r\nif (!iucv)\r\nreturn -EINVAL;\r\nbh_lock_sock(sk);\r\nlow_nmcpy(user_data, iucv->src_name);\r\nhigh_nmcpy(user_data, iucv->dst_name);\r\nASCEBC(user_data, sizeof(user_data));\r\nif (sk->sk_state != IUCV_LISTEN) {\r\nerr = iucv_path_sever(path, user_data);\r\niucv_path_free(path);\r\ngoto fail;\r\n}\r\nif (sk_acceptq_is_full(sk)) {\r\nerr = iucv_path_sever(path, user_data);\r\niucv_path_free(path);\r\ngoto fail;\r\n}\r\nnsk = iucv_sock_alloc(NULL, sk->sk_type, GFP_ATOMIC);\r\nif (!nsk) {\r\nerr = iucv_path_sever(path, user_data);\r\niucv_path_free(path);\r\ngoto fail;\r\n}\r\nniucv = iucv_sk(nsk);\r\niucv_sock_init(nsk, sk);\r\nmemcpy(niucv->dst_name, ipuser + 8, 8);\r\nEBCASC(niucv->dst_name, 8);\r\nmemcpy(niucv->dst_user_id, ipvmid, 8);\r\nmemcpy(niucv->src_name, iucv->src_name, 8);\r\nmemcpy(niucv->src_user_id, iucv->src_user_id, 8);\r\nniucv->path = path;\r\nhigh_nmcpy(nuser_data, ipuser + 8);\r\nmemcpy(nuser_data + 8, niucv->src_name, 8);\r\nASCEBC(nuser_data + 8, 8);\r\nniucv->msglimit = iucv->msglimit;\r\npath->msglim = iucv->msglimit;\r\nerr = iucv_path_accept(path, &af_iucv_handler, nuser_data, nsk);\r\nif (err) {\r\nerr = iucv_path_sever(path, user_data);\r\niucv_path_free(path);\r\niucv_sock_kill(nsk);\r\ngoto fail;\r\n}\r\niucv_accept_enqueue(sk, nsk);\r\nnsk->sk_state = IUCV_CONNECTED;\r\nsk->sk_data_ready(sk, 1);\r\nerr = 0;\r\nfail:\r\nbh_unlock_sock(sk);\r\nreturn 0;\r\n}\r\nstatic void iucv_callback_connack(struct iucv_path *path, u8 ipuser[16])\r\n{\r\nstruct sock *sk = path->private;\r\nsk->sk_state = IUCV_CONNECTED;\r\nsk->sk_state_change(sk);\r\n}\r\nstatic void iucv_callback_rx(struct iucv_path *path, struct iucv_message *msg)\r\n{\r\nstruct sock *sk = path->private;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct sk_buff *skb;\r\nstruct sock_msg_q *save_msg;\r\nint len;\r\nif (sk->sk_shutdown & RCV_SHUTDOWN) {\r\niucv_message_reject(path, msg);\r\nreturn;\r\n}\r\nspin_lock(&iucv->message_q.lock);\r\nif (!list_empty(&iucv->message_q.list) ||\r\n!skb_queue_empty(&iucv->backlog_skb_q))\r\ngoto save_message;\r\nlen = atomic_read(&sk->sk_rmem_alloc);\r\nlen += iucv_msg_length(msg) + sizeof(struct sk_buff);\r\nif (len > sk->sk_rcvbuf)\r\ngoto save_message;\r\nskb = alloc_skb(iucv_msg_length(msg), GFP_ATOMIC | GFP_DMA);\r\nif (!skb)\r\ngoto save_message;\r\niucv_process_message(sk, skb, path, msg);\r\ngoto out_unlock;\r\nsave_message:\r\nsave_msg = kzalloc(sizeof(struct sock_msg_q), GFP_ATOMIC | GFP_DMA);\r\nif (!save_msg)\r\ngoto out_unlock;\r\nsave_msg->path = path;\r\nsave_msg->msg = *msg;\r\nlist_add_tail(&save_msg->list, &iucv->message_q.list);\r\nout_unlock:\r\nspin_unlock(&iucv->message_q.lock);\r\n}\r\nstatic void iucv_callback_txdone(struct iucv_path *path,\r\nstruct iucv_message *msg)\r\n{\r\nstruct sock *sk = path->private;\r\nstruct sk_buff *this = NULL;\r\nstruct sk_buff_head *list = &iucv_sk(sk)->send_skb_q;\r\nstruct sk_buff *list_skb = list->next;\r\nunsigned long flags;\r\nif (!skb_queue_empty(list)) {\r\nspin_lock_irqsave(&list->lock, flags);\r\nwhile (list_skb != (struct sk_buff *)list) {\r\nif (!memcmp(&msg->tag, CB_TAG(list_skb), CB_TAG_LEN)) {\r\nthis = list_skb;\r\nbreak;\r\n}\r\nlist_skb = list_skb->next;\r\n}\r\nif (this)\r\n__skb_unlink(this, list);\r\nspin_unlock_irqrestore(&list->lock, flags);\r\nif (this) {\r\nkfree_skb(this);\r\niucv_sock_wake_msglim(sk);\r\n}\r\n}\r\nBUG_ON(!this);\r\nif (sk->sk_state == IUCV_CLOSING) {\r\nif (skb_queue_empty(&iucv_sk(sk)->send_skb_q)) {\r\nsk->sk_state = IUCV_CLOSED;\r\nsk->sk_state_change(sk);\r\n}\r\n}\r\n}\r\nstatic void iucv_callback_connrej(struct iucv_path *path, u8 ipuser[16])\r\n{\r\nstruct sock *sk = path->private;\r\nif (!list_empty(&iucv_sk(sk)->accept_q))\r\nsk->sk_state = IUCV_SEVERED;\r\nelse\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\n}\r\nstatic void iucv_callback_shutdown(struct iucv_path *path, u8 ipuser[16])\r\n{\r\nstruct sock *sk = path->private;\r\nbh_lock_sock(sk);\r\nif (sk->sk_state != IUCV_CLOSED) {\r\nsk->sk_shutdown |= SEND_SHUTDOWN;\r\nsk->sk_state_change(sk);\r\n}\r\nbh_unlock_sock(sk);\r\n}\r\nstatic int __init afiucv_init(void)\r\n{\r\nint err;\r\nif (!MACHINE_IS_VM) {\r\npr_err("The af_iucv module cannot be loaded"\r\n" without z/VM\n");\r\nerr = -EPROTONOSUPPORT;\r\ngoto out;\r\n}\r\ncpcmd("QUERY USERID", iucv_userid, sizeof(iucv_userid), &err);\r\nif (unlikely(err)) {\r\nWARN_ON(err);\r\nerr = -EPROTONOSUPPORT;\r\ngoto out;\r\n}\r\nerr = iucv_register(&af_iucv_handler, 0);\r\nif (err)\r\ngoto out;\r\nerr = proto_register(&iucv_proto, 0);\r\nif (err)\r\ngoto out_iucv;\r\nerr = sock_register(&iucv_sock_family_ops);\r\nif (err)\r\ngoto out_proto;\r\nerr = driver_register(&af_iucv_driver);\r\nif (err)\r\ngoto out_sock;\r\naf_iucv_dev = kzalloc(sizeof(struct device), GFP_KERNEL);\r\nif (!af_iucv_dev) {\r\nerr = -ENOMEM;\r\ngoto out_driver;\r\n}\r\ndev_set_name(af_iucv_dev, "af_iucv");\r\naf_iucv_dev->bus = &iucv_bus;\r\naf_iucv_dev->parent = iucv_root;\r\naf_iucv_dev->release = (void (*)(struct device *))kfree;\r\naf_iucv_dev->driver = &af_iucv_driver;\r\nerr = device_register(af_iucv_dev);\r\nif (err)\r\ngoto out_driver;\r\nreturn 0;\r\nout_driver:\r\ndriver_unregister(&af_iucv_driver);\r\nout_sock:\r\nsock_unregister(PF_IUCV);\r\nout_proto:\r\nproto_unregister(&iucv_proto);\r\nout_iucv:\r\niucv_unregister(&af_iucv_handler, 0);\r\nout:\r\nreturn err;\r\n}\r\nstatic void __exit afiucv_exit(void)\r\n{\r\ndevice_unregister(af_iucv_dev);\r\ndriver_unregister(&af_iucv_driver);\r\nsock_unregister(PF_IUCV);\r\nproto_unregister(&iucv_proto);\r\niucv_unregister(&af_iucv_handler, 0);\r\n}
