static int rcu_gp_in_progress(struct rcu_state *rsp)\r\n{\r\nreturn ACCESS_ONCE(rsp->completed) != ACCESS_ONCE(rsp->gpnum);\r\n}\r\nvoid rcu_sched_qs(int cpu)\r\n{\r\nstruct rcu_data *rdp = &per_cpu(rcu_sched_data, cpu);\r\nrdp->passed_quiesc_completed = rdp->gpnum - 1;\r\nbarrier();\r\nrdp->passed_quiesc = 1;\r\n}\r\nvoid rcu_bh_qs(int cpu)\r\n{\r\nstruct rcu_data *rdp = &per_cpu(rcu_bh_data, cpu);\r\nrdp->passed_quiesc_completed = rdp->gpnum - 1;\r\nbarrier();\r\nrdp->passed_quiesc = 1;\r\n}\r\nvoid rcu_note_context_switch(int cpu)\r\n{\r\nrcu_sched_qs(cpu);\r\nrcu_preempt_note_context_switch(cpu);\r\n}\r\nlong rcu_batches_completed_sched(void)\r\n{\r\nreturn rcu_sched_state.completed;\r\n}\r\nlong rcu_batches_completed_bh(void)\r\n{\r\nreturn rcu_bh_state.completed;\r\n}\r\nvoid rcu_bh_force_quiescent_state(void)\r\n{\r\nforce_quiescent_state(&rcu_bh_state, 0);\r\n}\r\nvoid rcutorture_record_test_transition(void)\r\n{\r\nrcutorture_testseq++;\r\nrcutorture_vernum = 0;\r\n}\r\nvoid rcutorture_record_progress(unsigned long vernum)\r\n{\r\nrcutorture_vernum++;\r\n}\r\nvoid rcu_sched_force_quiescent_state(void)\r\n{\r\nforce_quiescent_state(&rcu_sched_state, 0);\r\n}\r\nstatic int\r\ncpu_has_callbacks_ready_to_invoke(struct rcu_data *rdp)\r\n{\r\nreturn &rdp->nxtlist != rdp->nxttail[RCU_DONE_TAIL];\r\n}\r\nstatic int\r\ncpu_needs_another_gp(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nreturn *rdp->nxttail[RCU_DONE_TAIL] && !rcu_gp_in_progress(rsp);\r\n}\r\nstatic struct rcu_node *rcu_get_root(struct rcu_state *rsp)\r\n{\r\nreturn &rsp->node[0];\r\n}\r\nstatic int rcu_implicit_offline_qs(struct rcu_data *rdp)\r\n{\r\nif (cpu_is_offline(rdp->cpu)) {\r\nrdp->offline_fqs++;\r\nreturn 1;\r\n}\r\nif (rdp->preemptible)\r\nreturn 0;\r\nif (rdp->cpu != smp_processor_id())\r\nsmp_send_reschedule(rdp->cpu);\r\nelse\r\nset_need_resched();\r\nrdp->resched_ipi++;\r\nreturn 0;\r\n}\r\nvoid rcu_enter_nohz(void)\r\n{\r\nunsigned long flags;\r\nstruct rcu_dynticks *rdtp;\r\nlocal_irq_save(flags);\r\nrdtp = &__get_cpu_var(rcu_dynticks);\r\nif (--rdtp->dynticks_nesting) {\r\nlocal_irq_restore(flags);\r\nreturn;\r\n}\r\nsmp_mb__before_atomic_inc();\r\natomic_inc(&rdtp->dynticks);\r\nsmp_mb__after_atomic_inc();\r\nWARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);\r\nlocal_irq_restore(flags);\r\nif (in_irq() &&\r\n(__get_cpu_var(rcu_sched_data).nxtlist ||\r\n__get_cpu_var(rcu_bh_data).nxtlist ||\r\nrcu_preempt_needs_cpu(smp_processor_id())))\r\nset_need_resched();\r\n}\r\nvoid rcu_exit_nohz(void)\r\n{\r\nunsigned long flags;\r\nstruct rcu_dynticks *rdtp;\r\nlocal_irq_save(flags);\r\nrdtp = &__get_cpu_var(rcu_dynticks);\r\nif (rdtp->dynticks_nesting++) {\r\nlocal_irq_restore(flags);\r\nreturn;\r\n}\r\nsmp_mb__before_atomic_inc();\r\natomic_inc(&rdtp->dynticks);\r\nsmp_mb__after_atomic_inc();\r\nWARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));\r\nlocal_irq_restore(flags);\r\n}\r\nvoid rcu_nmi_enter(void)\r\n{\r\nstruct rcu_dynticks *rdtp = &__get_cpu_var(rcu_dynticks);\r\nif (rdtp->dynticks_nmi_nesting == 0 &&\r\n(atomic_read(&rdtp->dynticks) & 0x1))\r\nreturn;\r\nrdtp->dynticks_nmi_nesting++;\r\nsmp_mb__before_atomic_inc();\r\natomic_inc(&rdtp->dynticks);\r\nsmp_mb__after_atomic_inc();\r\nWARN_ON_ONCE(!(atomic_read(&rdtp->dynticks) & 0x1));\r\n}\r\nvoid rcu_nmi_exit(void)\r\n{\r\nstruct rcu_dynticks *rdtp = &__get_cpu_var(rcu_dynticks);\r\nif (rdtp->dynticks_nmi_nesting == 0 ||\r\n--rdtp->dynticks_nmi_nesting != 0)\r\nreturn;\r\nsmp_mb__before_atomic_inc();\r\natomic_inc(&rdtp->dynticks);\r\nsmp_mb__after_atomic_inc();\r\nWARN_ON_ONCE(atomic_read(&rdtp->dynticks) & 0x1);\r\n}\r\nvoid rcu_irq_enter(void)\r\n{\r\nrcu_exit_nohz();\r\n}\r\nvoid rcu_irq_exit(void)\r\n{\r\nrcu_enter_nohz();\r\n}\r\nstatic int dyntick_save_progress_counter(struct rcu_data *rdp)\r\n{\r\nrdp->dynticks_snap = atomic_add_return(0, &rdp->dynticks->dynticks);\r\nreturn 0;\r\n}\r\nstatic int rcu_implicit_dynticks_qs(struct rcu_data *rdp)\r\n{\r\nunsigned long curr;\r\nunsigned long snap;\r\ncurr = (unsigned long)atomic_add_return(0, &rdp->dynticks->dynticks);\r\nsnap = (unsigned long)rdp->dynticks_snap;\r\nif ((curr & 0x1) == 0 || ULONG_CMP_GE(curr, snap + 2)) {\r\nrdp->dynticks_fqs++;\r\nreturn 1;\r\n}\r\nreturn rcu_implicit_offline_qs(rdp);\r\n}\r\nstatic int dyntick_save_progress_counter(struct rcu_data *rdp)\r\n{\r\nreturn 0;\r\n}\r\nstatic int rcu_implicit_dynticks_qs(struct rcu_data *rdp)\r\n{\r\nreturn rcu_implicit_offline_qs(rdp);\r\n}\r\nstatic void record_gp_stall_check_time(struct rcu_state *rsp)\r\n{\r\nrsp->gp_start = jiffies;\r\nrsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_CHECK;\r\n}\r\nstatic void print_other_cpu_stall(struct rcu_state *rsp)\r\n{\r\nint cpu;\r\nlong delta;\r\nunsigned long flags;\r\nstruct rcu_node *rnp = rcu_get_root(rsp);\r\nraw_spin_lock_irqsave(&rnp->lock, flags);\r\ndelta = jiffies - rsp->jiffies_stall;\r\nif (delta < RCU_STALL_RAT_DELAY || !rcu_gp_in_progress(rsp)) {\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nreturn;\r\n}\r\nrsp->jiffies_stall = jiffies + RCU_SECONDS_TILL_STALL_RECHECK;\r\nrcu_print_task_stall(rnp);\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nprintk(KERN_ERR "INFO: %s detected stalls on CPUs/tasks: {",\r\nrsp->name);\r\nrcu_for_each_leaf_node(rsp, rnp) {\r\nraw_spin_lock_irqsave(&rnp->lock, flags);\r\nrcu_print_task_stall(rnp);\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nif (rnp->qsmask == 0)\r\ncontinue;\r\nfor (cpu = 0; cpu <= rnp->grphi - rnp->grplo; cpu++)\r\nif (rnp->qsmask & (1UL << cpu))\r\nprintk(" %d", rnp->grplo + cpu);\r\n}\r\nprintk("} (detected by %d, t=%ld jiffies)\n",\r\nsmp_processor_id(), (long)(jiffies - rsp->gp_start));\r\ntrigger_all_cpu_backtrace();\r\nrcu_print_detail_task_stall(rsp);\r\nforce_quiescent_state(rsp, 0);\r\n}\r\nstatic void print_cpu_stall(struct rcu_state *rsp)\r\n{\r\nunsigned long flags;\r\nstruct rcu_node *rnp = rcu_get_root(rsp);\r\nprintk(KERN_ERR "INFO: %s detected stall on CPU %d (t=%lu jiffies)\n",\r\nrsp->name, smp_processor_id(), jiffies - rsp->gp_start);\r\ntrigger_all_cpu_backtrace();\r\nraw_spin_lock_irqsave(&rnp->lock, flags);\r\nif (ULONG_CMP_GE(jiffies, rsp->jiffies_stall))\r\nrsp->jiffies_stall =\r\njiffies + RCU_SECONDS_TILL_STALL_RECHECK;\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nset_need_resched();\r\n}\r\nstatic void check_cpu_stall(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nunsigned long j;\r\nunsigned long js;\r\nstruct rcu_node *rnp;\r\nif (rcu_cpu_stall_suppress)\r\nreturn;\r\nj = ACCESS_ONCE(jiffies);\r\njs = ACCESS_ONCE(rsp->jiffies_stall);\r\nrnp = rdp->mynode;\r\nif ((ACCESS_ONCE(rnp->qsmask) & rdp->grpmask) && ULONG_CMP_GE(j, js)) {\r\nprint_cpu_stall(rsp);\r\n} else if (rcu_gp_in_progress(rsp) &&\r\nULONG_CMP_GE(j, js + RCU_STALL_RAT_DELAY)) {\r\nprint_other_cpu_stall(rsp);\r\n}\r\n}\r\nstatic int rcu_panic(struct notifier_block *this, unsigned long ev, void *ptr)\r\n{\r\nrcu_cpu_stall_suppress = 1;\r\nreturn NOTIFY_DONE;\r\n}\r\nvoid rcu_cpu_stall_reset(void)\r\n{\r\nrcu_sched_state.jiffies_stall = jiffies + ULONG_MAX / 2;\r\nrcu_bh_state.jiffies_stall = jiffies + ULONG_MAX / 2;\r\nrcu_preempt_stall_reset();\r\n}\r\nstatic void __init check_cpu_stall_init(void)\r\n{\r\natomic_notifier_chain_register(&panic_notifier_list, &rcu_panic_block);\r\n}\r\nstatic void __note_new_gpnum(struct rcu_state *rsp, struct rcu_node *rnp, struct rcu_data *rdp)\r\n{\r\nif (rdp->gpnum != rnp->gpnum) {\r\nrdp->gpnum = rnp->gpnum;\r\nif (rnp->qsmask & rdp->grpmask) {\r\nrdp->qs_pending = 1;\r\nrdp->passed_quiesc = 0;\r\n} else\r\nrdp->qs_pending = 0;\r\n}\r\n}\r\nstatic void note_new_gpnum(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nunsigned long flags;\r\nstruct rcu_node *rnp;\r\nlocal_irq_save(flags);\r\nrnp = rdp->mynode;\r\nif (rdp->gpnum == ACCESS_ONCE(rnp->gpnum) ||\r\n!raw_spin_trylock(&rnp->lock)) {\r\nlocal_irq_restore(flags);\r\nreturn;\r\n}\r\n__note_new_gpnum(rsp, rnp, rdp);\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\n}\r\nstatic int\r\ncheck_for_new_grace_period(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nunsigned long flags;\r\nint ret = 0;\r\nlocal_irq_save(flags);\r\nif (rdp->gpnum != rsp->gpnum) {\r\nnote_new_gpnum(rsp, rdp);\r\nret = 1;\r\n}\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nstatic void\r\n__rcu_process_gp_end(struct rcu_state *rsp, struct rcu_node *rnp, struct rcu_data *rdp)\r\n{\r\nif (rdp->completed != rnp->completed) {\r\nrdp->nxttail[RCU_DONE_TAIL] = rdp->nxttail[RCU_WAIT_TAIL];\r\nrdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_READY_TAIL];\r\nrdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];\r\nrdp->completed = rnp->completed;\r\nif (ULONG_CMP_LT(rdp->gpnum, rdp->completed))\r\nrdp->gpnum = rdp->completed;\r\nif ((rnp->qsmask & rdp->grpmask) == 0)\r\nrdp->qs_pending = 0;\r\n}\r\n}\r\nstatic void\r\nrcu_process_gp_end(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nunsigned long flags;\r\nstruct rcu_node *rnp;\r\nlocal_irq_save(flags);\r\nrnp = rdp->mynode;\r\nif (rdp->completed == ACCESS_ONCE(rnp->completed) ||\r\n!raw_spin_trylock(&rnp->lock)) {\r\nlocal_irq_restore(flags);\r\nreturn;\r\n}\r\n__rcu_process_gp_end(rsp, rnp, rdp);\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\n}\r\nstatic void\r\nrcu_start_gp_per_cpu(struct rcu_state *rsp, struct rcu_node *rnp, struct rcu_data *rdp)\r\n{\r\n__rcu_process_gp_end(rsp, rnp, rdp);\r\nrdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];\r\nrdp->nxttail[RCU_WAIT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];\r\n__note_new_gpnum(rsp, rnp, rdp);\r\n}\r\nstatic void\r\nrcu_start_gp(struct rcu_state *rsp, unsigned long flags)\r\n__releases(rcu_get_root(rsp)->lock\r\nstatic void rcu_report_qs_rsp(struct rcu_state *rsp, unsigned long flags)\r\n__releases(rcu_get_root(rsp)->lock\r\nstatic void\r\nrcu_report_qs_rnp(unsigned long mask, struct rcu_state *rsp,\r\nstruct rcu_node *rnp, unsigned long flags)\r\n__releases(rnp->lock)\r\n{\r\nstruct rcu_node *rnp_c;\r\nfor (;;) {\r\nif (!(rnp->qsmask & mask)) {\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nreturn;\r\n}\r\nrnp->qsmask &= ~mask;\r\nif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nreturn;\r\n}\r\nmask = rnp->grpmask;\r\nif (rnp->parent == NULL) {\r\nbreak;\r\n}\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nrnp_c = rnp;\r\nrnp = rnp->parent;\r\nraw_spin_lock_irqsave(&rnp->lock, flags);\r\nWARN_ON_ONCE(rnp_c->qsmask);\r\n}\r\nrcu_report_qs_rsp(rsp, flags);\r\n}\r\nstatic void\r\nrcu_report_qs_rdp(int cpu, struct rcu_state *rsp, struct rcu_data *rdp, long lastcomp)\r\n{\r\nunsigned long flags;\r\nunsigned long mask;\r\nstruct rcu_node *rnp;\r\nrnp = rdp->mynode;\r\nraw_spin_lock_irqsave(&rnp->lock, flags);\r\nif (lastcomp != rnp->completed) {\r\nrdp->passed_quiesc = 0;\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nreturn;\r\n}\r\nmask = rdp->grpmask;\r\nif ((rnp->qsmask & mask) == 0) {\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\n} else {\r\nrdp->qs_pending = 0;\r\nrdp->nxttail[RCU_NEXT_READY_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];\r\nrcu_report_qs_rnp(mask, rsp, rnp, flags);\r\n}\r\n}\r\nstatic void\r\nrcu_check_quiescent_state(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nif (check_for_new_grace_period(rsp, rdp))\r\nreturn;\r\nif (!rdp->qs_pending)\r\nreturn;\r\nif (!rdp->passed_quiesc)\r\nreturn;\r\nrcu_report_qs_rdp(rdp->cpu, rsp, rdp, rdp->passed_quiesc_completed);\r\n}\r\nstatic void rcu_send_cbs_to_online(struct rcu_state *rsp)\r\n{\r\nint i;\r\nint receive_cpu = cpumask_any(cpu_online_mask);\r\nstruct rcu_data *rdp = this_cpu_ptr(rsp->rda);\r\nstruct rcu_data *receive_rdp = per_cpu_ptr(rsp->rda, receive_cpu);\r\nif (rdp->nxtlist == NULL)\r\nreturn;\r\n*receive_rdp->nxttail[RCU_NEXT_TAIL] = rdp->nxtlist;\r\nreceive_rdp->nxttail[RCU_NEXT_TAIL] = rdp->nxttail[RCU_NEXT_TAIL];\r\nreceive_rdp->qlen += rdp->qlen;\r\nreceive_rdp->n_cbs_adopted += rdp->qlen;\r\nrdp->n_cbs_orphaned += rdp->qlen;\r\nrdp->nxtlist = NULL;\r\nfor (i = 0; i < RCU_NEXT_SIZE; i++)\r\nrdp->nxttail[i] = &rdp->nxtlist;\r\nrdp->qlen = 0;\r\n}\r\nstatic void __rcu_offline_cpu(int cpu, struct rcu_state *rsp)\r\n{\r\nunsigned long flags;\r\nunsigned long mask;\r\nint need_report = 0;\r\nstruct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);\r\nstruct rcu_node *rnp;\r\nrcu_stop_cpu_kthread(cpu);\r\nraw_spin_lock_irqsave(&rsp->onofflock, flags);\r\nrnp = rdp->mynode;\r\nmask = rdp->grpmask;\r\ndo {\r\nraw_spin_lock(&rnp->lock);\r\nrnp->qsmaskinit &= ~mask;\r\nif (rnp->qsmaskinit != 0) {\r\nif (rnp != rdp->mynode)\r\nraw_spin_unlock(&rnp->lock);\r\nbreak;\r\n}\r\nif (rnp == rdp->mynode)\r\nneed_report = rcu_preempt_offline_tasks(rsp, rnp, rdp);\r\nelse\r\nraw_spin_unlock(&rnp->lock);\r\nmask = rnp->grpmask;\r\nrnp = rnp->parent;\r\n} while (rnp != NULL);\r\nraw_spin_unlock(&rsp->onofflock);\r\nrnp = rdp->mynode;\r\nif (need_report & RCU_OFL_TASKS_NORM_GP)\r\nrcu_report_unblock_qs_rnp(rnp, flags);\r\nelse\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nif (need_report & RCU_OFL_TASKS_EXP_GP)\r\nrcu_report_exp_rnp(rsp, rnp);\r\nrcu_node_kthread_setaffinity(rnp, -1);\r\n}\r\nstatic void rcu_offline_cpu(int cpu)\r\n{\r\n__rcu_offline_cpu(cpu, &rcu_sched_state);\r\n__rcu_offline_cpu(cpu, &rcu_bh_state);\r\nrcu_preempt_offline_cpu(cpu);\r\n}\r\nstatic void rcu_send_cbs_to_online(struct rcu_state *rsp)\r\n{\r\n}\r\nstatic void rcu_offline_cpu(int cpu)\r\n{\r\n}\r\nstatic void rcu_do_batch(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nunsigned long flags;\r\nstruct rcu_head *next, *list, **tail;\r\nint count;\r\nif (!cpu_has_callbacks_ready_to_invoke(rdp))\r\nreturn;\r\nlocal_irq_save(flags);\r\nlist = rdp->nxtlist;\r\nrdp->nxtlist = *rdp->nxttail[RCU_DONE_TAIL];\r\n*rdp->nxttail[RCU_DONE_TAIL] = NULL;\r\ntail = rdp->nxttail[RCU_DONE_TAIL];\r\nfor (count = RCU_NEXT_SIZE - 1; count >= 0; count--)\r\nif (rdp->nxttail[count] == rdp->nxttail[RCU_DONE_TAIL])\r\nrdp->nxttail[count] = &rdp->nxtlist;\r\nlocal_irq_restore(flags);\r\ncount = 0;\r\nwhile (list) {\r\nnext = list->next;\r\nprefetch(next);\r\ndebug_rcu_head_unqueue(list);\r\n__rcu_reclaim(list);\r\nlist = next;\r\nif (++count >= rdp->blimit)\r\nbreak;\r\n}\r\nlocal_irq_save(flags);\r\nrdp->qlen -= count;\r\nrdp->n_cbs_invoked += count;\r\nif (list != NULL) {\r\n*tail = rdp->nxtlist;\r\nrdp->nxtlist = list;\r\nfor (count = 0; count < RCU_NEXT_SIZE; count++)\r\nif (&rdp->nxtlist == rdp->nxttail[count])\r\nrdp->nxttail[count] = tail;\r\nelse\r\nbreak;\r\n}\r\nif (rdp->blimit == LONG_MAX && rdp->qlen <= qlowmark)\r\nrdp->blimit = blimit;\r\nif (rdp->qlen == 0 && rdp->qlen_last_fqs_check != 0) {\r\nrdp->qlen_last_fqs_check = 0;\r\nrdp->n_force_qs_snap = rsp->n_force_qs;\r\n} else if (rdp->qlen < rdp->qlen_last_fqs_check - qhimark)\r\nrdp->qlen_last_fqs_check = rdp->qlen;\r\nlocal_irq_restore(flags);\r\nif (cpu_has_callbacks_ready_to_invoke(rdp))\r\ninvoke_rcu_core();\r\n}\r\nvoid rcu_check_callbacks(int cpu, int user)\r\n{\r\nif (user ||\r\n(idle_cpu(cpu) && rcu_scheduler_active &&\r\n!in_softirq() && hardirq_count() <= (1 << HARDIRQ_SHIFT))) {\r\nrcu_sched_qs(cpu);\r\nrcu_bh_qs(cpu);\r\n} else if (!in_softirq()) {\r\nrcu_bh_qs(cpu);\r\n}\r\nrcu_preempt_check_callbacks(cpu);\r\nif (rcu_pending(cpu))\r\ninvoke_rcu_core();\r\n}\r\nstatic void force_qs_rnp(struct rcu_state *rsp, int (*f)(struct rcu_data *))\r\n{\r\nunsigned long bit;\r\nint cpu;\r\nunsigned long flags;\r\nunsigned long mask;\r\nstruct rcu_node *rnp;\r\nrcu_for_each_leaf_node(rsp, rnp) {\r\nmask = 0;\r\nraw_spin_lock_irqsave(&rnp->lock, flags);\r\nif (!rcu_gp_in_progress(rsp)) {\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\nreturn;\r\n}\r\nif (rnp->qsmask == 0) {\r\nrcu_initiate_boost(rnp, flags);\r\ncontinue;\r\n}\r\ncpu = rnp->grplo;\r\nbit = 1;\r\nfor (; cpu <= rnp->grphi; cpu++, bit <<= 1) {\r\nif ((rnp->qsmask & bit) != 0 &&\r\nf(per_cpu_ptr(rsp->rda, cpu)))\r\nmask |= bit;\r\n}\r\nif (mask != 0) {\r\nrcu_report_qs_rnp(mask, rsp, rnp, flags);\r\ncontinue;\r\n}\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\n}\r\nrnp = rcu_get_root(rsp);\r\nif (rnp->qsmask == 0) {\r\nraw_spin_lock_irqsave(&rnp->lock, flags);\r\nrcu_initiate_boost(rnp, flags);\r\n}\r\n}\r\nstatic void force_quiescent_state(struct rcu_state *rsp, int relaxed)\r\n{\r\nunsigned long flags;\r\nstruct rcu_node *rnp = rcu_get_root(rsp);\r\nif (!rcu_gp_in_progress(rsp))\r\nreturn;\r\nif (!raw_spin_trylock_irqsave(&rsp->fqslock, flags)) {\r\nrsp->n_force_qs_lh++;\r\nreturn;\r\n}\r\nif (relaxed && ULONG_CMP_GE(rsp->jiffies_force_qs, jiffies))\r\ngoto unlock_fqs_ret;\r\nrsp->n_force_qs++;\r\nraw_spin_lock(&rnp->lock);\r\nrsp->jiffies_force_qs = jiffies + RCU_JIFFIES_TILL_FORCE_QS;\r\nif(!rcu_gp_in_progress(rsp)) {\r\nrsp->n_force_qs_ngp++;\r\nraw_spin_unlock(&rnp->lock);\r\ngoto unlock_fqs_ret;\r\n}\r\nrsp->fqs_active = 1;\r\nswitch (rsp->signaled) {\r\ncase RCU_GP_IDLE:\r\ncase RCU_GP_INIT:\r\nbreak;\r\ncase RCU_SAVE_DYNTICK:\r\nif (RCU_SIGNAL_INIT != RCU_SAVE_DYNTICK)\r\nbreak;\r\nraw_spin_unlock(&rnp->lock);\r\nforce_qs_rnp(rsp, dyntick_save_progress_counter);\r\nraw_spin_lock(&rnp->lock);\r\nif (rcu_gp_in_progress(rsp))\r\nrsp->signaled = RCU_FORCE_QS;\r\nbreak;\r\ncase RCU_FORCE_QS:\r\nraw_spin_unlock(&rnp->lock);\r\nforce_qs_rnp(rsp, rcu_implicit_dynticks_qs);\r\nraw_spin_lock(&rnp->lock);\r\nbreak;\r\n}\r\nrsp->fqs_active = 0;\r\nif (rsp->fqs_need_gp) {\r\nraw_spin_unlock(&rsp->fqslock);\r\nrsp->fqs_need_gp = 0;\r\nrcu_start_gp(rsp, flags);\r\nreturn;\r\n}\r\nraw_spin_unlock(&rnp->lock);\r\nunlock_fqs_ret:\r\nraw_spin_unlock_irqrestore(&rsp->fqslock, flags);\r\n}\r\nstatic void force_quiescent_state(struct rcu_state *rsp, int relaxed)\r\n{\r\nset_need_resched();\r\n}\r\nstatic void\r\n__rcu_process_callbacks(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nunsigned long flags;\r\nWARN_ON_ONCE(rdp->beenonline == 0);\r\nif (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))\r\nforce_quiescent_state(rsp, 1);\r\nrcu_process_gp_end(rsp, rdp);\r\nrcu_check_quiescent_state(rsp, rdp);\r\nif (cpu_needs_another_gp(rsp, rdp)) {\r\nraw_spin_lock_irqsave(&rcu_get_root(rsp)->lock, flags);\r\nrcu_start_gp(rsp, flags);\r\n}\r\nif (cpu_has_callbacks_ready_to_invoke(rdp))\r\ninvoke_rcu_callbacks(rsp, rdp);\r\n}\r\nstatic void rcu_process_callbacks(struct softirq_action *unused)\r\n{\r\n__rcu_process_callbacks(&rcu_sched_state,\r\n&__get_cpu_var(rcu_sched_data));\r\n__rcu_process_callbacks(&rcu_bh_state, &__get_cpu_var(rcu_bh_data));\r\nrcu_preempt_process_callbacks();\r\nrcu_needs_cpu_flush();\r\n}\r\nstatic void invoke_rcu_callbacks(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nif (unlikely(!ACCESS_ONCE(rcu_scheduler_fully_active)))\r\nreturn;\r\nif (likely(!rsp->boost)) {\r\nrcu_do_batch(rsp, rdp);\r\nreturn;\r\n}\r\ninvoke_rcu_callbacks_kthread();\r\n}\r\nstatic void invoke_rcu_core(void)\r\n{\r\nraise_softirq(RCU_SOFTIRQ);\r\n}\r\nstatic void\r\n__call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu),\r\nstruct rcu_state *rsp)\r\n{\r\nunsigned long flags;\r\nstruct rcu_data *rdp;\r\ndebug_rcu_head_queue(head);\r\nhead->func = func;\r\nhead->next = NULL;\r\nsmp_mb();\r\nlocal_irq_save(flags);\r\nrdp = this_cpu_ptr(rsp->rda);\r\n*rdp->nxttail[RCU_NEXT_TAIL] = head;\r\nrdp->nxttail[RCU_NEXT_TAIL] = &head->next;\r\nrdp->qlen++;\r\nif (irqs_disabled_flags(flags)) {\r\nlocal_irq_restore(flags);\r\nreturn;\r\n}\r\nif (unlikely(rdp->qlen > rdp->qlen_last_fqs_check + qhimark)) {\r\nrcu_process_gp_end(rsp, rdp);\r\ncheck_for_new_grace_period(rsp, rdp);\r\nif (!rcu_gp_in_progress(rsp)) {\r\nunsigned long nestflag;\r\nstruct rcu_node *rnp_root = rcu_get_root(rsp);\r\nraw_spin_lock_irqsave(&rnp_root->lock, nestflag);\r\nrcu_start_gp(rsp, nestflag);\r\n} else {\r\nrdp->blimit = LONG_MAX;\r\nif (rsp->n_force_qs == rdp->n_force_qs_snap &&\r\n*rdp->nxttail[RCU_DONE_TAIL] != head)\r\nforce_quiescent_state(rsp, 0);\r\nrdp->n_force_qs_snap = rsp->n_force_qs;\r\nrdp->qlen_last_fqs_check = rdp->qlen;\r\n}\r\n} else if (ULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies))\r\nforce_quiescent_state(rsp, 1);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))\r\n{\r\n__call_rcu(head, func, &rcu_sched_state);\r\n}\r\nvoid call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))\r\n{\r\n__call_rcu(head, func, &rcu_bh_state);\r\n}\r\nvoid synchronize_sched(void)\r\n{\r\nstruct rcu_synchronize rcu;\r\nif (rcu_blocking_is_gp())\r\nreturn;\r\ninit_rcu_head_on_stack(&rcu.head);\r\ninit_completion(&rcu.completion);\r\ncall_rcu_sched(&rcu.head, wakeme_after_rcu);\r\nwait_for_completion(&rcu.completion);\r\ndestroy_rcu_head_on_stack(&rcu.head);\r\n}\r\nvoid synchronize_rcu_bh(void)\r\n{\r\nstruct rcu_synchronize rcu;\r\nif (rcu_blocking_is_gp())\r\nreturn;\r\ninit_rcu_head_on_stack(&rcu.head);\r\ninit_completion(&rcu.completion);\r\ncall_rcu_bh(&rcu.head, wakeme_after_rcu);\r\nwait_for_completion(&rcu.completion);\r\ndestroy_rcu_head_on_stack(&rcu.head);\r\n}\r\nstatic int __rcu_pending(struct rcu_state *rsp, struct rcu_data *rdp)\r\n{\r\nstruct rcu_node *rnp = rdp->mynode;\r\nrdp->n_rcu_pending++;\r\ncheck_cpu_stall(rsp, rdp);\r\nif (rdp->qs_pending && !rdp->passed_quiesc) {\r\nrdp->n_rp_qs_pending++;\r\nif (!rdp->preemptible &&\r\nULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs) - 1,\r\njiffies))\r\nset_need_resched();\r\n} else if (rdp->qs_pending && rdp->passed_quiesc) {\r\nrdp->n_rp_report_qs++;\r\nreturn 1;\r\n}\r\nif (cpu_has_callbacks_ready_to_invoke(rdp)) {\r\nrdp->n_rp_cb_ready++;\r\nreturn 1;\r\n}\r\nif (cpu_needs_another_gp(rsp, rdp)) {\r\nrdp->n_rp_cpu_needs_gp++;\r\nreturn 1;\r\n}\r\nif (ACCESS_ONCE(rnp->completed) != rdp->completed) {\r\nrdp->n_rp_gp_completed++;\r\nreturn 1;\r\n}\r\nif (ACCESS_ONCE(rnp->gpnum) != rdp->gpnum) {\r\nrdp->n_rp_gp_started++;\r\nreturn 1;\r\n}\r\nif (rcu_gp_in_progress(rsp) &&\r\nULONG_CMP_LT(ACCESS_ONCE(rsp->jiffies_force_qs), jiffies)) {\r\nrdp->n_rp_need_fqs++;\r\nreturn 1;\r\n}\r\nrdp->n_rp_need_nothing++;\r\nreturn 0;\r\n}\r\nstatic int rcu_pending(int cpu)\r\n{\r\nreturn __rcu_pending(&rcu_sched_state, &per_cpu(rcu_sched_data, cpu)) ||\r\n__rcu_pending(&rcu_bh_state, &per_cpu(rcu_bh_data, cpu)) ||\r\nrcu_preempt_pending(cpu);\r\n}\r\nstatic int rcu_needs_cpu_quick_check(int cpu)\r\n{\r\nreturn per_cpu(rcu_sched_data, cpu).nxtlist ||\r\nper_cpu(rcu_bh_data, cpu).nxtlist ||\r\nrcu_preempt_needs_cpu(cpu);\r\n}\r\nstatic void rcu_barrier_callback(struct rcu_head *notused)\r\n{\r\nif (atomic_dec_and_test(&rcu_barrier_cpu_count))\r\ncomplete(&rcu_barrier_completion);\r\n}\r\nstatic void rcu_barrier_func(void *type)\r\n{\r\nint cpu = smp_processor_id();\r\nstruct rcu_head *head = &per_cpu(rcu_barrier_head, cpu);\r\nvoid (*call_rcu_func)(struct rcu_head *head,\r\nvoid (*func)(struct rcu_head *head));\r\natomic_inc(&rcu_barrier_cpu_count);\r\ncall_rcu_func = type;\r\ncall_rcu_func(head, rcu_barrier_callback);\r\n}\r\nstatic void _rcu_barrier(struct rcu_state *rsp,\r\nvoid (*call_rcu_func)(struct rcu_head *head,\r\nvoid (*func)(struct rcu_head *head)))\r\n{\r\nBUG_ON(in_interrupt());\r\nmutex_lock(&rcu_barrier_mutex);\r\ninit_completion(&rcu_barrier_completion);\r\natomic_set(&rcu_barrier_cpu_count, 1);\r\non_each_cpu(rcu_barrier_func, (void *)call_rcu_func, 1);\r\nif (atomic_dec_and_test(&rcu_barrier_cpu_count))\r\ncomplete(&rcu_barrier_completion);\r\nwait_for_completion(&rcu_barrier_completion);\r\nmutex_unlock(&rcu_barrier_mutex);\r\n}\r\nvoid rcu_barrier_bh(void)\r\n{\r\n_rcu_barrier(&rcu_bh_state, call_rcu_bh);\r\n}\r\nvoid rcu_barrier_sched(void)\r\n{\r\n_rcu_barrier(&rcu_sched_state, call_rcu_sched);\r\n}\r\nstatic void __init\r\nrcu_boot_init_percpu_data(int cpu, struct rcu_state *rsp)\r\n{\r\nunsigned long flags;\r\nint i;\r\nstruct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);\r\nstruct rcu_node *rnp = rcu_get_root(rsp);\r\nraw_spin_lock_irqsave(&rnp->lock, flags);\r\nrdp->grpmask = 1UL << (cpu - rdp->mynode->grplo);\r\nrdp->nxtlist = NULL;\r\nfor (i = 0; i < RCU_NEXT_SIZE; i++)\r\nrdp->nxttail[i] = &rdp->nxtlist;\r\nrdp->qlen = 0;\r\n#ifdef CONFIG_NO_HZ\r\nrdp->dynticks = &per_cpu(rcu_dynticks, cpu);\r\n#endif\r\nrdp->cpu = cpu;\r\nraw_spin_unlock_irqrestore(&rnp->lock, flags);\r\n}\r\nstatic void __cpuinit\r\nrcu_init_percpu_data(int cpu, struct rcu_state *rsp, int preemptible)\r\n{\r\nunsigned long flags;\r\nunsigned long mask;\r\nstruct rcu_data *rdp = per_cpu_ptr(rsp->rda, cpu);\r\nstruct rcu_node *rnp = rcu_get_root(rsp);\r\nraw_spin_lock_irqsave(&rnp->lock, flags);\r\nrdp->passed_quiesc = 0;\r\nrdp->qs_pending = 1;\r\nrdp->beenonline = 1;\r\nrdp->preemptible = preemptible;\r\nrdp->qlen_last_fqs_check = 0;\r\nrdp->n_force_qs_snap = rsp->n_force_qs;\r\nrdp->blimit = blimit;\r\nraw_spin_unlock(&rnp->lock);\r\nraw_spin_lock(&rsp->onofflock);\r\nrnp = rdp->mynode;\r\nmask = rdp->grpmask;\r\ndo {\r\nraw_spin_lock(&rnp->lock);\r\nrnp->qsmaskinit |= mask;\r\nmask = rnp->grpmask;\r\nif (rnp == rdp->mynode) {\r\nrdp->gpnum = rnp->completed;\r\nrdp->completed = rnp->completed;\r\nrdp->passed_quiesc_completed = rnp->completed - 1;\r\n}\r\nraw_spin_unlock(&rnp->lock);\r\nrnp = rnp->parent;\r\n} while (rnp != NULL && !(rnp->qsmaskinit & mask));\r\nraw_spin_unlock_irqrestore(&rsp->onofflock, flags);\r\n}\r\nstatic void __cpuinit rcu_prepare_cpu(int cpu)\r\n{\r\nrcu_init_percpu_data(cpu, &rcu_sched_state, 0);\r\nrcu_init_percpu_data(cpu, &rcu_bh_state, 0);\r\nrcu_preempt_init_percpu_data(cpu);\r\n}\r\nstatic int __cpuinit rcu_cpu_notify(struct notifier_block *self,\r\nunsigned long action, void *hcpu)\r\n{\r\nlong cpu = (long)hcpu;\r\nstruct rcu_data *rdp = per_cpu_ptr(rcu_state->rda, cpu);\r\nstruct rcu_node *rnp = rdp->mynode;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_UP_PREPARE_FROZEN:\r\nrcu_prepare_cpu(cpu);\r\nrcu_prepare_kthreads(cpu);\r\nbreak;\r\ncase CPU_ONLINE:\r\ncase CPU_DOWN_FAILED:\r\nrcu_node_kthread_setaffinity(rnp, -1);\r\nrcu_cpu_kthread_setrt(cpu, 1);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\nrcu_node_kthread_setaffinity(rnp, cpu);\r\nrcu_cpu_kthread_setrt(cpu, 0);\r\nbreak;\r\ncase CPU_DYING:\r\ncase CPU_DYING_FROZEN:\r\nrcu_send_cbs_to_online(&rcu_bh_state);\r\nrcu_send_cbs_to_online(&rcu_sched_state);\r\nrcu_preempt_send_cbs_to_online();\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\ncase CPU_UP_CANCELED:\r\ncase CPU_UP_CANCELED_FROZEN:\r\nrcu_offline_cpu(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nvoid rcu_scheduler_starting(void)\r\n{\r\nWARN_ON(num_online_cpus() != 1);\r\nWARN_ON(nr_context_switches() > 0);\r\nrcu_scheduler_active = 1;\r\n}\r\nstatic void __init rcu_init_levelspread(struct rcu_state *rsp)\r\n{\r\nint i;\r\nfor (i = NUM_RCU_LVLS - 1; i > 0; i--)\r\nrsp->levelspread[i] = CONFIG_RCU_FANOUT;\r\nrsp->levelspread[0] = RCU_FANOUT_LEAF;\r\n}\r\nstatic void __init rcu_init_levelspread(struct rcu_state *rsp)\r\n{\r\nint ccur;\r\nint cprv;\r\nint i;\r\ncprv = NR_CPUS;\r\nfor (i = NUM_RCU_LVLS - 1; i >= 0; i--) {\r\nccur = rsp->levelcnt[i];\r\nrsp->levelspread[i] = (cprv + ccur - 1) / ccur;\r\ncprv = ccur;\r\n}\r\n}\r\nstatic void __init rcu_init_one(struct rcu_state *rsp,\r\nstruct rcu_data __percpu *rda)\r\n{\r\nstatic char *buf[] = { "rcu_node_level_0",\r\n"rcu_node_level_1",\r\n"rcu_node_level_2",\r\n"rcu_node_level_3" };\r\nint cpustride = 1;\r\nint i;\r\nint j;\r\nstruct rcu_node *rnp;\r\nBUILD_BUG_ON(MAX_RCU_LVLS > ARRAY_SIZE(buf));\r\nfor (i = 1; i < NUM_RCU_LVLS; i++)\r\nrsp->level[i] = rsp->level[i - 1] + rsp->levelcnt[i - 1];\r\nrcu_init_levelspread(rsp);\r\nfor (i = NUM_RCU_LVLS - 1; i >= 0; i--) {\r\ncpustride *= rsp->levelspread[i];\r\nrnp = rsp->level[i];\r\nfor (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {\r\nraw_spin_lock_init(&rnp->lock);\r\nlockdep_set_class_and_name(&rnp->lock,\r\n&rcu_node_class[i], buf[i]);\r\nrnp->gpnum = 0;\r\nrnp->qsmask = 0;\r\nrnp->qsmaskinit = 0;\r\nrnp->grplo = j * cpustride;\r\nrnp->grphi = (j + 1) * cpustride - 1;\r\nif (rnp->grphi >= NR_CPUS)\r\nrnp->grphi = NR_CPUS - 1;\r\nif (i == 0) {\r\nrnp->grpnum = 0;\r\nrnp->grpmask = 0;\r\nrnp->parent = NULL;\r\n} else {\r\nrnp->grpnum = j % rsp->levelspread[i - 1];\r\nrnp->grpmask = 1UL << rnp->grpnum;\r\nrnp->parent = rsp->level[i - 1] +\r\nj / rsp->levelspread[i - 1];\r\n}\r\nrnp->level = i;\r\nINIT_LIST_HEAD(&rnp->blkd_tasks);\r\n}\r\n}\r\nrsp->rda = rda;\r\nrnp = rsp->level[NUM_RCU_LVLS - 1];\r\nfor_each_possible_cpu(i) {\r\nwhile (i > rnp->grphi)\r\nrnp++;\r\nper_cpu_ptr(rsp->rda, i)->mynode = rnp;\r\nrcu_boot_init_percpu_data(i, rsp);\r\n}\r\n}\r\nvoid __init rcu_init(void)\r\n{\r\nint cpu;\r\nrcu_bootup_announce();\r\nrcu_init_one(&rcu_sched_state, &rcu_sched_data);\r\nrcu_init_one(&rcu_bh_state, &rcu_bh_data);\r\n__rcu_init_preempt();\r\nopen_softirq(RCU_SOFTIRQ, rcu_process_callbacks);\r\ncpu_notifier(rcu_cpu_notify, 0);\r\nfor_each_online_cpu(cpu)\r\nrcu_cpu_notify(NULL, CPU_UP_PREPARE, (void *)(long)cpu);\r\ncheck_cpu_stall_init();\r\n}
