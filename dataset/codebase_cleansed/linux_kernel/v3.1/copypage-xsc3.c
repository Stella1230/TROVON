static void __naked\r\nxsc3_mc_copy_user_page(void *kto, const void *kfrom)\r\n{\r\nasm("\\r\nstmfd sp!, {r4, r5, lr} \n\\r\nmov lr, %2 \n\\r\n\n\\r\npld [r1, #0] \n\\r\npld [r1, #32] \n\\r\n1: pld [r1, #64] \n\\r\npld [r1, #96] \n\\r\n\n\\r\n2: ldrd r2, [r1], #8 \n\\r\nmov ip, r0 \n\\r\nldrd r4, [r1], #8 \n\\r\nmcr p15, 0, ip, c7, c6, 1 @ invalidate\n\\r\nstrd r2, [r0], #8 \n\\r\nldrd r2, [r1], #8 \n\\r\nstrd r4, [r0], #8 \n\\r\nldrd r4, [r1], #8 \n\\r\nstrd r2, [r0], #8 \n\\r\nstrd r4, [r0], #8 \n\\r\nldrd r2, [r1], #8 \n\\r\nmov ip, r0 \n\\r\nldrd r4, [r1], #8 \n\\r\nmcr p15, 0, ip, c7, c6, 1 @ invalidate\n\\r\nstrd r2, [r0], #8 \n\\r\nldrd r2, [r1], #8 \n\\r\nsubs lr, lr, #1 \n\\r\nstrd r4, [r0], #8 \n\\r\nldrd r4, [r1], #8 \n\\r\nstrd r2, [r0], #8 \n\\r\nstrd r4, [r0], #8 \n\\r\nbgt 1b \n\\r\nbeq 2b \n\\r\n\n\\r\nldmfd sp!, {r4, r5, pc}"\r\n:\r\n: "r" (kto), "r" (kfrom), "I" (PAGE_SIZE / 64 - 1));\r\n}\r\nvoid xsc3_mc_copy_user_highpage(struct page *to, struct page *from,\r\nunsigned long vaddr, struct vm_area_struct *vma)\r\n{\r\nvoid *kto, *kfrom;\r\nkto = kmap_atomic(to, KM_USER0);\r\nkfrom = kmap_atomic(from, KM_USER1);\r\nflush_cache_page(vma, vaddr, page_to_pfn(from));\r\nxsc3_mc_copy_user_page(kto, kfrom);\r\nkunmap_atomic(kfrom, KM_USER1);\r\nkunmap_atomic(kto, KM_USER0);\r\n}\r\nvoid xsc3_mc_clear_user_highpage(struct page *page, unsigned long vaddr)\r\n{\r\nvoid *ptr, *kaddr = kmap_atomic(page, KM_USER0);\r\nasm volatile ("\\r\nmov r1, %2 \n\\r\nmov r2, #0 \n\\r\nmov r3, #0 \n\\r\n1: mcr p15, 0, %0, c7, c6, 1 @ invalidate line\n\\r\nstrd r2, [%0], #8 \n\\r\nstrd r2, [%0], #8 \n\\r\nstrd r2, [%0], #8 \n\\r\nstrd r2, [%0], #8 \n\\r\nsubs r1, r1, #1 \n\\r\nbne 1b"\r\n: "=r" (ptr)\r\n: "0" (kaddr), "I" (PAGE_SIZE / 32)\r\n: "r1", "r2", "r3");\r\nkunmap_atomic(kaddr, KM_USER0);\r\n}
