static inline struct cbq_class *\r\ncbq_class_lookup(struct cbq_sched_data *q, u32 classid)\r\n{\r\nstruct Qdisc_class_common *clc;\r\nclc = qdisc_class_find(&q->clhash, classid);\r\nif (clc == NULL)\r\nreturn NULL;\r\nreturn container_of(clc, struct cbq_class, common);\r\n}\r\nstatic struct cbq_class *\r\ncbq_reclassify(struct sk_buff *skb, struct cbq_class *this)\r\n{\r\nstruct cbq_class *cl;\r\nfor (cl = this->tparent; cl; cl = cl->tparent) {\r\nstruct cbq_class *new = cl->defaults[TC_PRIO_BESTEFFORT];\r\nif (new != NULL && new != this)\r\nreturn new;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct cbq_class *\r\ncbq_classify(struct sk_buff *skb, struct Qdisc *sch, int *qerr)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *head = &q->link;\r\nstruct cbq_class **defmap;\r\nstruct cbq_class *cl = NULL;\r\nu32 prio = skb->priority;\r\nstruct tcf_result res;\r\nif (TC_H_MAJ(prio ^ sch->handle) == 0 &&\r\n(cl = cbq_class_lookup(q, prio)) != NULL)\r\nreturn cl;\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\r\nfor (;;) {\r\nint result = 0;\r\ndefmap = head->defaults;\r\nif (!head->filter_list ||\r\n(result = tc_classify_compat(skb, head->filter_list, &res)) < 0)\r\ngoto fallback;\r\ncl = (void *)res.class;\r\nif (!cl) {\r\nif (TC_H_MAJ(res.classid))\r\ncl = cbq_class_lookup(q, res.classid);\r\nelse if ((cl = defmap[res.classid & TC_PRIO_MAX]) == NULL)\r\ncl = defmap[TC_PRIO_BESTEFFORT];\r\nif (cl == NULL || cl->level >= head->level)\r\ngoto fallback;\r\n}\r\n#ifdef CONFIG_NET_CLS_ACT\r\nswitch (result) {\r\ncase TC_ACT_QUEUED:\r\ncase TC_ACT_STOLEN:\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\r\ncase TC_ACT_SHOT:\r\nreturn NULL;\r\ncase TC_ACT_RECLASSIFY:\r\nreturn cbq_reclassify(skb, cl);\r\n}\r\n#endif\r\nif (cl->level == 0)\r\nreturn cl;\r\nhead = cl;\r\n}\r\nfallback:\r\ncl = head;\r\nif (TC_H_MAJ(prio) == 0 &&\r\n!(cl = head->defaults[prio & TC_PRIO_MAX]) &&\r\n!(cl = head->defaults[TC_PRIO_BESTEFFORT]))\r\nreturn head;\r\nreturn cl;\r\n}\r\nstatic inline void cbq_activate_class(struct cbq_class *cl)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(cl->qdisc);\r\nint prio = cl->cpriority;\r\nstruct cbq_class *cl_tail;\r\ncl_tail = q->active[prio];\r\nq->active[prio] = cl;\r\nif (cl_tail != NULL) {\r\ncl->next_alive = cl_tail->next_alive;\r\ncl_tail->next_alive = cl;\r\n} else {\r\ncl->next_alive = cl;\r\nq->activemask |= (1<<prio);\r\n}\r\n}\r\nstatic void cbq_deactivate_class(struct cbq_class *this)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(this->qdisc);\r\nint prio = this->cpriority;\r\nstruct cbq_class *cl;\r\nstruct cbq_class *cl_prev = q->active[prio];\r\ndo {\r\ncl = cl_prev->next_alive;\r\nif (cl == this) {\r\ncl_prev->next_alive = cl->next_alive;\r\ncl->next_alive = NULL;\r\nif (cl == q->active[prio]) {\r\nq->active[prio] = cl_prev;\r\nif (cl == q->active[prio]) {\r\nq->active[prio] = NULL;\r\nq->activemask &= ~(1<<prio);\r\nreturn;\r\n}\r\n}\r\nreturn;\r\n}\r\n} while ((cl_prev = cl) != q->active[prio]);\r\n}\r\nstatic void\r\ncbq_mark_toplevel(struct cbq_sched_data *q, struct cbq_class *cl)\r\n{\r\nint toplevel = q->toplevel;\r\nif (toplevel > cl->level && !(qdisc_is_throttled(cl->q))) {\r\npsched_time_t now;\r\npsched_tdiff_t incr;\r\nnow = psched_get_time();\r\nincr = now - q->now_rt;\r\nnow = q->now + incr;\r\ndo {\r\nif (cl->undertime < now) {\r\nq->toplevel = cl->level;\r\nreturn;\r\n}\r\n} while ((cl = cl->borrow) != NULL && toplevel > cl->level);\r\n}\r\n}\r\nstatic int\r\ncbq_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nint uninitialized_var(ret);\r\nstruct cbq_class *cl = cbq_classify(skb, sch, &ret);\r\n#ifdef CONFIG_NET_CLS_ACT\r\nq->rx_class = cl;\r\n#endif\r\nif (cl == NULL) {\r\nif (ret & __NET_XMIT_BYPASS)\r\nsch->qstats.drops++;\r\nkfree_skb(skb);\r\nreturn ret;\r\n}\r\n#ifdef CONFIG_NET_CLS_ACT\r\ncl->q->__parent = sch;\r\n#endif\r\nret = qdisc_enqueue(skb, cl->q);\r\nif (ret == NET_XMIT_SUCCESS) {\r\nsch->q.qlen++;\r\ncbq_mark_toplevel(q, cl);\r\nif (!cl->next_alive)\r\ncbq_activate_class(cl);\r\nreturn ret;\r\n}\r\nif (net_xmit_drop_count(ret)) {\r\nsch->qstats.drops++;\r\ncbq_mark_toplevel(q, cl);\r\ncl->qstats.drops++;\r\n}\r\nreturn ret;\r\n}\r\nstatic void cbq_ovl_classic(struct cbq_class *cl)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(cl->qdisc);\r\npsched_tdiff_t delay = cl->undertime - q->now;\r\nif (!cl->delayed) {\r\ndelay += cl->offtime;\r\nif (cl->avgidle < 0)\r\ndelay -= (-cl->avgidle) - ((-cl->avgidle) >> cl->ewma_log);\r\nif (cl->avgidle < cl->minidle)\r\ncl->avgidle = cl->minidle;\r\nif (delay <= 0)\r\ndelay = 1;\r\ncl->undertime = q->now + delay;\r\ncl->xstats.overactions++;\r\ncl->delayed = 1;\r\n}\r\nif (q->wd_expires == 0 || q->wd_expires > delay)\r\nq->wd_expires = delay;\r\nif (q->toplevel == TC_CBQ_MAXLEVEL) {\r\nstruct cbq_class *b;\r\npsched_tdiff_t base_delay = q->wd_expires;\r\nfor (b = cl->borrow; b; b = b->borrow) {\r\ndelay = b->undertime - q->now;\r\nif (delay < base_delay) {\r\nif (delay <= 0)\r\ndelay = 1;\r\nbase_delay = delay;\r\n}\r\n}\r\nq->wd_expires = base_delay;\r\n}\r\n}\r\nstatic void cbq_ovl_rclassic(struct cbq_class *cl)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(cl->qdisc);\r\nstruct cbq_class *this = cl;\r\ndo {\r\nif (cl->level > q->toplevel) {\r\ncl = NULL;\r\nbreak;\r\n}\r\n} while ((cl = cl->borrow) != NULL);\r\nif (cl == NULL)\r\ncl = this;\r\ncbq_ovl_classic(cl);\r\n}\r\nstatic void cbq_ovl_delay(struct cbq_class *cl)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(cl->qdisc);\r\npsched_tdiff_t delay = cl->undertime - q->now;\r\nif (test_bit(__QDISC_STATE_DEACTIVATED,\r\n&qdisc_root_sleeping(cl->qdisc)->state))\r\nreturn;\r\nif (!cl->delayed) {\r\npsched_time_t sched = q->now;\r\nktime_t expires;\r\ndelay += cl->offtime;\r\nif (cl->avgidle < 0)\r\ndelay -= (-cl->avgidle) - ((-cl->avgidle) >> cl->ewma_log);\r\nif (cl->avgidle < cl->minidle)\r\ncl->avgidle = cl->minidle;\r\ncl->undertime = q->now + delay;\r\nif (delay > 0) {\r\nsched += delay + cl->penalty;\r\ncl->penalized = sched;\r\ncl->cpriority = TC_CBQ_MAXPRIO;\r\nq->pmask |= (1<<TC_CBQ_MAXPRIO);\r\nexpires = ktime_set(0, 0);\r\nexpires = ktime_add_ns(expires, PSCHED_TICKS2NS(sched));\r\nif (hrtimer_try_to_cancel(&q->delay_timer) &&\r\nktime_to_ns(ktime_sub(\r\nhrtimer_get_expires(&q->delay_timer),\r\nexpires)) > 0)\r\nhrtimer_set_expires(&q->delay_timer, expires);\r\nhrtimer_restart(&q->delay_timer);\r\ncl->delayed = 1;\r\ncl->xstats.overactions++;\r\nreturn;\r\n}\r\ndelay = 1;\r\n}\r\nif (q->wd_expires == 0 || q->wd_expires > delay)\r\nq->wd_expires = delay;\r\n}\r\nstatic void cbq_ovl_lowprio(struct cbq_class *cl)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(cl->qdisc);\r\ncl->penalized = q->now + cl->penalty;\r\nif (cl->cpriority != cl->priority2) {\r\ncl->cpriority = cl->priority2;\r\nq->pmask |= (1<<cl->cpriority);\r\ncl->xstats.overactions++;\r\n}\r\ncbq_ovl_classic(cl);\r\n}\r\nstatic void cbq_ovl_drop(struct cbq_class *cl)\r\n{\r\nif (cl->q->ops->drop)\r\nif (cl->q->ops->drop(cl->q))\r\ncl->qdisc->q.qlen--;\r\ncl->xstats.overactions++;\r\ncbq_ovl_classic(cl);\r\n}\r\nstatic psched_tdiff_t cbq_undelay_prio(struct cbq_sched_data *q, int prio,\r\npsched_time_t now)\r\n{\r\nstruct cbq_class *cl;\r\nstruct cbq_class *cl_prev = q->active[prio];\r\npsched_time_t sched = now;\r\nif (cl_prev == NULL)\r\nreturn 0;\r\ndo {\r\ncl = cl_prev->next_alive;\r\nif (now - cl->penalized > 0) {\r\ncl_prev->next_alive = cl->next_alive;\r\ncl->next_alive = NULL;\r\ncl->cpriority = cl->priority;\r\ncl->delayed = 0;\r\ncbq_activate_class(cl);\r\nif (cl == q->active[prio]) {\r\nq->active[prio] = cl_prev;\r\nif (cl == q->active[prio]) {\r\nq->active[prio] = NULL;\r\nreturn 0;\r\n}\r\n}\r\ncl = cl_prev->next_alive;\r\n} else if (sched - cl->penalized > 0)\r\nsched = cl->penalized;\r\n} while ((cl_prev = cl) != q->active[prio]);\r\nreturn sched - now;\r\n}\r\nstatic enum hrtimer_restart cbq_undelay(struct hrtimer *timer)\r\n{\r\nstruct cbq_sched_data *q = container_of(timer, struct cbq_sched_data,\r\ndelay_timer);\r\nstruct Qdisc *sch = q->watchdog.qdisc;\r\npsched_time_t now;\r\npsched_tdiff_t delay = 0;\r\nunsigned int pmask;\r\nnow = psched_get_time();\r\npmask = q->pmask;\r\nq->pmask = 0;\r\nwhile (pmask) {\r\nint prio = ffz(~pmask);\r\npsched_tdiff_t tmp;\r\npmask &= ~(1<<prio);\r\ntmp = cbq_undelay_prio(q, prio, now);\r\nif (tmp > 0) {\r\nq->pmask |= 1<<prio;\r\nif (tmp < delay || delay == 0)\r\ndelay = tmp;\r\n}\r\n}\r\nif (delay) {\r\nktime_t time;\r\ntime = ktime_set(0, 0);\r\ntime = ktime_add_ns(time, PSCHED_TICKS2NS(now + delay));\r\nhrtimer_start(&q->delay_timer, time, HRTIMER_MODE_ABS);\r\n}\r\nqdisc_unthrottled(sch);\r\n__netif_schedule(qdisc_root(sch));\r\nreturn HRTIMER_NORESTART;\r\n}\r\nstatic int cbq_reshape_fail(struct sk_buff *skb, struct Qdisc *child)\r\n{\r\nstruct Qdisc *sch = child->__parent;\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl = q->rx_class;\r\nq->rx_class = NULL;\r\nif (cl && (cl = cbq_reclassify(skb, cl)) != NULL) {\r\nint ret;\r\ncbq_mark_toplevel(q, cl);\r\nq->rx_class = cl;\r\ncl->q->__parent = sch;\r\nret = qdisc_enqueue(skb, cl->q);\r\nif (ret == NET_XMIT_SUCCESS) {\r\nsch->q.qlen++;\r\nif (!cl->next_alive)\r\ncbq_activate_class(cl);\r\nreturn 0;\r\n}\r\nif (net_xmit_drop_count(ret))\r\nsch->qstats.drops++;\r\nreturn 0;\r\n}\r\nsch->qstats.drops++;\r\nreturn -1;\r\n}\r\nstatic inline void\r\ncbq_update_toplevel(struct cbq_sched_data *q, struct cbq_class *cl,\r\nstruct cbq_class *borrowed)\r\n{\r\nif (cl && q->toplevel >= borrowed->level) {\r\nif (cl->q->q.qlen > 1) {\r\ndo {\r\nif (borrowed->undertime == PSCHED_PASTPERFECT) {\r\nq->toplevel = borrowed->level;\r\nreturn;\r\n}\r\n} while ((borrowed = borrowed->borrow) != NULL);\r\n}\r\n#if 0\r\nq->toplevel = TC_CBQ_MAXLEVEL;\r\n#endif\r\n}\r\n}\r\nstatic void\r\ncbq_update(struct cbq_sched_data *q)\r\n{\r\nstruct cbq_class *this = q->tx_class;\r\nstruct cbq_class *cl = this;\r\nint len = q->tx_len;\r\nq->tx_class = NULL;\r\nfor ( ; cl; cl = cl->share) {\r\nlong avgidle = cl->avgidle;\r\nlong idle;\r\ncl->bstats.packets++;\r\ncl->bstats.bytes += len;\r\nidle = q->now - cl->last;\r\nif ((unsigned long)idle > 128*1024*1024) {\r\navgidle = cl->maxidle;\r\n} else {\r\nidle -= L2T(cl, len);\r\navgidle += idle - (avgidle>>cl->ewma_log);\r\n}\r\nif (avgidle <= 0) {\r\nif (avgidle < cl->minidle)\r\navgidle = cl->minidle;\r\ncl->avgidle = avgidle;\r\nidle = (-avgidle) - ((-avgidle) >> cl->ewma_log);\r\nidle -= L2T(&q->link, len);\r\nidle += L2T(cl, len);\r\ncl->undertime = q->now + idle;\r\n} else {\r\ncl->undertime = PSCHED_PASTPERFECT;\r\nif (avgidle > cl->maxidle)\r\ncl->avgidle = cl->maxidle;\r\nelse\r\ncl->avgidle = avgidle;\r\n}\r\ncl->last = q->now;\r\n}\r\ncbq_update_toplevel(q, this, q->tx_borrowed);\r\n}\r\nstatic inline struct cbq_class *\r\ncbq_under_limit(struct cbq_class *cl)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(cl->qdisc);\r\nstruct cbq_class *this_cl = cl;\r\nif (cl->tparent == NULL)\r\nreturn cl;\r\nif (cl->undertime == PSCHED_PASTPERFECT || q->now >= cl->undertime) {\r\ncl->delayed = 0;\r\nreturn cl;\r\n}\r\ndo {\r\ncl = cl->borrow;\r\nif (!cl) {\r\nthis_cl->qstats.overlimits++;\r\nthis_cl->overlimit(this_cl);\r\nreturn NULL;\r\n}\r\nif (cl->level > q->toplevel)\r\nreturn NULL;\r\n} while (cl->undertime != PSCHED_PASTPERFECT && q->now < cl->undertime);\r\ncl->delayed = 0;\r\nreturn cl;\r\n}\r\nstatic inline struct sk_buff *\r\ncbq_dequeue_prio(struct Qdisc *sch, int prio)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl_tail, *cl_prev, *cl;\r\nstruct sk_buff *skb;\r\nint deficit;\r\ncl_tail = cl_prev = q->active[prio];\r\ncl = cl_prev->next_alive;\r\ndo {\r\ndeficit = 0;\r\ndo {\r\nstruct cbq_class *borrow = cl;\r\nif (cl->q->q.qlen &&\r\n(borrow = cbq_under_limit(cl)) == NULL)\r\ngoto skip_class;\r\nif (cl->deficit <= 0) {\r\ndeficit = 1;\r\ncl->deficit += cl->quantum;\r\ngoto next_class;\r\n}\r\nskb = cl->q->dequeue(cl->q);\r\nif (skb == NULL)\r\ngoto skip_class;\r\ncl->deficit -= qdisc_pkt_len(skb);\r\nq->tx_class = cl;\r\nq->tx_borrowed = borrow;\r\nif (borrow != cl) {\r\n#ifndef CBQ_XSTATS_BORROWS_BYTES\r\nborrow->xstats.borrows++;\r\ncl->xstats.borrows++;\r\n#else\r\nborrow->xstats.borrows += qdisc_pkt_len(skb);\r\ncl->xstats.borrows += qdisc_pkt_len(skb);\r\n#endif\r\n}\r\nq->tx_len = qdisc_pkt_len(skb);\r\nif (cl->deficit <= 0) {\r\nq->active[prio] = cl;\r\ncl = cl->next_alive;\r\ncl->deficit += cl->quantum;\r\n}\r\nreturn skb;\r\nskip_class:\r\nif (cl->q->q.qlen == 0 || prio != cl->cpriority) {\r\ncl_prev->next_alive = cl->next_alive;\r\ncl->next_alive = NULL;\r\nif (cl == cl_tail) {\r\ncl_tail = cl_prev;\r\nif (cl == cl_tail) {\r\nq->active[prio] = NULL;\r\nq->activemask &= ~(1<<prio);\r\nif (cl->q->q.qlen)\r\ncbq_activate_class(cl);\r\nreturn NULL;\r\n}\r\nq->active[prio] = cl_tail;\r\n}\r\nif (cl->q->q.qlen)\r\ncbq_activate_class(cl);\r\ncl = cl_prev;\r\n}\r\nnext_class:\r\ncl_prev = cl;\r\ncl = cl->next_alive;\r\n} while (cl_prev != cl_tail);\r\n} while (deficit);\r\nq->active[prio] = cl_prev;\r\nreturn NULL;\r\n}\r\nstatic inline struct sk_buff *\r\ncbq_dequeue_1(struct Qdisc *sch)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nunsigned int activemask;\r\nactivemask = q->activemask & 0xFF;\r\nwhile (activemask) {\r\nint prio = ffz(~activemask);\r\nactivemask &= ~(1<<prio);\r\nskb = cbq_dequeue_prio(sch, prio);\r\nif (skb)\r\nreturn skb;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct sk_buff *\r\ncbq_dequeue(struct Qdisc *sch)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\npsched_time_t now;\r\npsched_tdiff_t incr;\r\nnow = psched_get_time();\r\nincr = now - q->now_rt;\r\nif (q->tx_class) {\r\npsched_tdiff_t incr2;\r\nincr2 = L2T(&q->link, q->tx_len);\r\nq->now += incr2;\r\ncbq_update(q);\r\nif ((incr -= incr2) < 0)\r\nincr = 0;\r\n}\r\nq->now += incr;\r\nq->now_rt = now;\r\nfor (;;) {\r\nq->wd_expires = 0;\r\nskb = cbq_dequeue_1(sch);\r\nif (skb) {\r\nqdisc_bstats_update(sch, skb);\r\nsch->q.qlen--;\r\nqdisc_unthrottled(sch);\r\nreturn skb;\r\n}\r\nif (q->toplevel == TC_CBQ_MAXLEVEL &&\r\nq->link.undertime == PSCHED_PASTPERFECT)\r\nbreak;\r\nq->toplevel = TC_CBQ_MAXLEVEL;\r\nq->link.undertime = PSCHED_PASTPERFECT;\r\n}\r\nif (sch->q.qlen) {\r\nsch->qstats.overlimits++;\r\nif (q->wd_expires)\r\nqdisc_watchdog_schedule(&q->watchdog,\r\nnow + q->wd_expires);\r\n}\r\nreturn NULL;\r\n}\r\nstatic void cbq_adjust_levels(struct cbq_class *this)\r\n{\r\nif (this == NULL)\r\nreturn;\r\ndo {\r\nint level = 0;\r\nstruct cbq_class *cl;\r\ncl = this->children;\r\nif (cl) {\r\ndo {\r\nif (cl->level > level)\r\nlevel = cl->level;\r\n} while ((cl = cl->sibling) != this->children);\r\n}\r\nthis->level = level + 1;\r\n} while ((this = this->tparent) != NULL);\r\n}\r\nstatic void cbq_normalize_quanta(struct cbq_sched_data *q, int prio)\r\n{\r\nstruct cbq_class *cl;\r\nstruct hlist_node *n;\r\nunsigned int h;\r\nif (q->quanta[prio] == 0)\r\nreturn;\r\nfor (h = 0; h < q->clhash.hashsize; h++) {\r\nhlist_for_each_entry(cl, n, &q->clhash.hash[h], common.hnode) {\r\nif (cl->priority == prio) {\r\ncl->quantum = (cl->weight*cl->allot*q->nclasses[prio])/\r\nq->quanta[prio];\r\n}\r\nif (cl->quantum <= 0 || cl->quantum>32*qdisc_dev(cl->qdisc)->mtu) {\r\npr_warning("CBQ: class %08x has bad quantum==%ld, repaired.\n",\r\ncl->common.classid, cl->quantum);\r\ncl->quantum = qdisc_dev(cl->qdisc)->mtu/2 + 1;\r\n}\r\n}\r\n}\r\n}\r\nstatic void cbq_sync_defmap(struct cbq_class *cl)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(cl->qdisc);\r\nstruct cbq_class *split = cl->split;\r\nunsigned int h;\r\nint i;\r\nif (split == NULL)\r\nreturn;\r\nfor (i = 0; i <= TC_PRIO_MAX; i++) {\r\nif (split->defaults[i] == cl && !(cl->defmap & (1<<i)))\r\nsplit->defaults[i] = NULL;\r\n}\r\nfor (i = 0; i <= TC_PRIO_MAX; i++) {\r\nint level = split->level;\r\nif (split->defaults[i])\r\ncontinue;\r\nfor (h = 0; h < q->clhash.hashsize; h++) {\r\nstruct hlist_node *n;\r\nstruct cbq_class *c;\r\nhlist_for_each_entry(c, n, &q->clhash.hash[h],\r\ncommon.hnode) {\r\nif (c->split == split && c->level < level &&\r\nc->defmap & (1<<i)) {\r\nsplit->defaults[i] = c;\r\nlevel = c->level;\r\n}\r\n}\r\n}\r\n}\r\n}\r\nstatic void cbq_change_defmap(struct cbq_class *cl, u32 splitid, u32 def, u32 mask)\r\n{\r\nstruct cbq_class *split = NULL;\r\nif (splitid == 0) {\r\nsplit = cl->split;\r\nif (!split)\r\nreturn;\r\nsplitid = split->common.classid;\r\n}\r\nif (split == NULL || split->common.classid != splitid) {\r\nfor (split = cl->tparent; split; split = split->tparent)\r\nif (split->common.classid == splitid)\r\nbreak;\r\n}\r\nif (split == NULL)\r\nreturn;\r\nif (cl->split != split) {\r\ncl->defmap = 0;\r\ncbq_sync_defmap(cl);\r\ncl->split = split;\r\ncl->defmap = def & mask;\r\n} else\r\ncl->defmap = (cl->defmap & ~mask) | (def & mask);\r\ncbq_sync_defmap(cl);\r\n}\r\nstatic void cbq_unlink_class(struct cbq_class *this)\r\n{\r\nstruct cbq_class *cl, **clp;\r\nstruct cbq_sched_data *q = qdisc_priv(this->qdisc);\r\nqdisc_class_hash_remove(&q->clhash, &this->common);\r\nif (this->tparent) {\r\nclp = &this->sibling;\r\ncl = *clp;\r\ndo {\r\nif (cl == this) {\r\n*clp = cl->sibling;\r\nbreak;\r\n}\r\nclp = &cl->sibling;\r\n} while ((cl = *clp) != this->sibling);\r\nif (this->tparent->children == this) {\r\nthis->tparent->children = this->sibling;\r\nif (this->sibling == this)\r\nthis->tparent->children = NULL;\r\n}\r\n} else {\r\nWARN_ON(this->sibling != this);\r\n}\r\n}\r\nstatic void cbq_link_class(struct cbq_class *this)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(this->qdisc);\r\nstruct cbq_class *parent = this->tparent;\r\nthis->sibling = this;\r\nqdisc_class_hash_insert(&q->clhash, &this->common);\r\nif (parent == NULL)\r\nreturn;\r\nif (parent->children == NULL) {\r\nparent->children = this;\r\n} else {\r\nthis->sibling = parent->children->sibling;\r\nparent->children->sibling = this;\r\n}\r\n}\r\nstatic unsigned int cbq_drop(struct Qdisc *sch)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl, *cl_head;\r\nint prio;\r\nunsigned int len;\r\nfor (prio = TC_CBQ_MAXPRIO; prio >= 0; prio--) {\r\ncl_head = q->active[prio];\r\nif (!cl_head)\r\ncontinue;\r\ncl = cl_head;\r\ndo {\r\nif (cl->q->ops->drop && (len = cl->q->ops->drop(cl->q))) {\r\nsch->q.qlen--;\r\nif (!cl->q->q.qlen)\r\ncbq_deactivate_class(cl);\r\nreturn len;\r\n}\r\n} while ((cl = cl->next_alive) != cl_head);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\ncbq_reset(struct Qdisc *sch)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl;\r\nstruct hlist_node *n;\r\nint prio;\r\nunsigned int h;\r\nq->activemask = 0;\r\nq->pmask = 0;\r\nq->tx_class = NULL;\r\nq->tx_borrowed = NULL;\r\nqdisc_watchdog_cancel(&q->watchdog);\r\nhrtimer_cancel(&q->delay_timer);\r\nq->toplevel = TC_CBQ_MAXLEVEL;\r\nq->now = psched_get_time();\r\nq->now_rt = q->now;\r\nfor (prio = 0; prio <= TC_CBQ_MAXPRIO; prio++)\r\nq->active[prio] = NULL;\r\nfor (h = 0; h < q->clhash.hashsize; h++) {\r\nhlist_for_each_entry(cl, n, &q->clhash.hash[h], common.hnode) {\r\nqdisc_reset(cl->q);\r\ncl->next_alive = NULL;\r\ncl->undertime = PSCHED_PASTPERFECT;\r\ncl->avgidle = cl->maxidle;\r\ncl->deficit = cl->quantum;\r\ncl->cpriority = cl->priority;\r\n}\r\n}\r\nsch->q.qlen = 0;\r\n}\r\nstatic int cbq_set_lss(struct cbq_class *cl, struct tc_cbq_lssopt *lss)\r\n{\r\nif (lss->change & TCF_CBQ_LSS_FLAGS) {\r\ncl->share = (lss->flags & TCF_CBQ_LSS_ISOLATED) ? NULL : cl->tparent;\r\ncl->borrow = (lss->flags & TCF_CBQ_LSS_BOUNDED) ? NULL : cl->tparent;\r\n}\r\nif (lss->change & TCF_CBQ_LSS_EWMA)\r\ncl->ewma_log = lss->ewma_log;\r\nif (lss->change & TCF_CBQ_LSS_AVPKT)\r\ncl->avpkt = lss->avpkt;\r\nif (lss->change & TCF_CBQ_LSS_MINIDLE)\r\ncl->minidle = -(long)lss->minidle;\r\nif (lss->change & TCF_CBQ_LSS_MAXIDLE) {\r\ncl->maxidle = lss->maxidle;\r\ncl->avgidle = lss->maxidle;\r\n}\r\nif (lss->change & TCF_CBQ_LSS_OFFTIME)\r\ncl->offtime = lss->offtime;\r\nreturn 0;\r\n}\r\nstatic void cbq_rmprio(struct cbq_sched_data *q, struct cbq_class *cl)\r\n{\r\nq->nclasses[cl->priority]--;\r\nq->quanta[cl->priority] -= cl->weight;\r\ncbq_normalize_quanta(q, cl->priority);\r\n}\r\nstatic void cbq_addprio(struct cbq_sched_data *q, struct cbq_class *cl)\r\n{\r\nq->nclasses[cl->priority]++;\r\nq->quanta[cl->priority] += cl->weight;\r\ncbq_normalize_quanta(q, cl->priority);\r\n}\r\nstatic int cbq_set_wrr(struct cbq_class *cl, struct tc_cbq_wrropt *wrr)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(cl->qdisc);\r\nif (wrr->allot)\r\ncl->allot = wrr->allot;\r\nif (wrr->weight)\r\ncl->weight = wrr->weight;\r\nif (wrr->priority) {\r\ncl->priority = wrr->priority - 1;\r\ncl->cpriority = cl->priority;\r\nif (cl->priority >= cl->priority2)\r\ncl->priority2 = TC_CBQ_MAXPRIO - 1;\r\n}\r\ncbq_addprio(q, cl);\r\nreturn 0;\r\n}\r\nstatic int cbq_set_overlimit(struct cbq_class *cl, struct tc_cbq_ovl *ovl)\r\n{\r\nswitch (ovl->strategy) {\r\ncase TC_CBQ_OVL_CLASSIC:\r\ncl->overlimit = cbq_ovl_classic;\r\nbreak;\r\ncase TC_CBQ_OVL_DELAY:\r\ncl->overlimit = cbq_ovl_delay;\r\nbreak;\r\ncase TC_CBQ_OVL_LOWPRIO:\r\nif (ovl->priority2 - 1 >= TC_CBQ_MAXPRIO ||\r\novl->priority2 - 1 <= cl->priority)\r\nreturn -EINVAL;\r\ncl->priority2 = ovl->priority2 - 1;\r\ncl->overlimit = cbq_ovl_lowprio;\r\nbreak;\r\ncase TC_CBQ_OVL_DROP:\r\ncl->overlimit = cbq_ovl_drop;\r\nbreak;\r\ncase TC_CBQ_OVL_RCLASSIC:\r\ncl->overlimit = cbq_ovl_rclassic;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ncl->penalty = ovl->penalty;\r\nreturn 0;\r\n}\r\nstatic int cbq_set_police(struct cbq_class *cl, struct tc_cbq_police *p)\r\n{\r\ncl->police = p->police;\r\nif (cl->q->handle) {\r\nif (p->police == TC_POLICE_RECLASSIFY)\r\ncl->q->reshape_fail = cbq_reshape_fail;\r\nelse\r\ncl->q->reshape_fail = NULL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int cbq_set_fopt(struct cbq_class *cl, struct tc_cbq_fopt *fopt)\r\n{\r\ncbq_change_defmap(cl, fopt->split, fopt->defmap, fopt->defchange);\r\nreturn 0;\r\n}\r\nstatic int cbq_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_CBQ_MAX + 1];\r\nstruct tc_ratespec *r;\r\nint err;\r\nerr = nla_parse_nested(tb, TCA_CBQ_MAX, opt, cbq_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (tb[TCA_CBQ_RTAB] == NULL || tb[TCA_CBQ_RATE] == NULL)\r\nreturn -EINVAL;\r\nr = nla_data(tb[TCA_CBQ_RATE]);\r\nif ((q->link.R_tab = qdisc_get_rtab(r, tb[TCA_CBQ_RTAB])) == NULL)\r\nreturn -EINVAL;\r\nerr = qdisc_class_hash_init(&q->clhash);\r\nif (err < 0)\r\ngoto put_rtab;\r\nq->link.refcnt = 1;\r\nq->link.sibling = &q->link;\r\nq->link.common.classid = sch->handle;\r\nq->link.qdisc = sch;\r\nq->link.q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\r\nsch->handle);\r\nif (!q->link.q)\r\nq->link.q = &noop_qdisc;\r\nq->link.priority = TC_CBQ_MAXPRIO - 1;\r\nq->link.priority2 = TC_CBQ_MAXPRIO - 1;\r\nq->link.cpriority = TC_CBQ_MAXPRIO - 1;\r\nq->link.ovl_strategy = TC_CBQ_OVL_CLASSIC;\r\nq->link.overlimit = cbq_ovl_classic;\r\nq->link.allot = psched_mtu(qdisc_dev(sch));\r\nq->link.quantum = q->link.allot;\r\nq->link.weight = q->link.R_tab->rate.rate;\r\nq->link.ewma_log = TC_CBQ_DEF_EWMA;\r\nq->link.avpkt = q->link.allot/2;\r\nq->link.minidle = -0x7FFFFFFF;\r\nqdisc_watchdog_init(&q->watchdog, sch);\r\nhrtimer_init(&q->delay_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);\r\nq->delay_timer.function = cbq_undelay;\r\nq->toplevel = TC_CBQ_MAXLEVEL;\r\nq->now = psched_get_time();\r\nq->now_rt = q->now;\r\ncbq_link_class(&q->link);\r\nif (tb[TCA_CBQ_LSSOPT])\r\ncbq_set_lss(&q->link, nla_data(tb[TCA_CBQ_LSSOPT]));\r\ncbq_addprio(q, &q->link);\r\nreturn 0;\r\nput_rtab:\r\nqdisc_put_rtab(q->link.R_tab);\r\nreturn err;\r\n}\r\nstatic int cbq_dump_rate(struct sk_buff *skb, struct cbq_class *cl)\r\n{\r\nunsigned char *b = skb_tail_pointer(skb);\r\nNLA_PUT(skb, TCA_CBQ_RATE, sizeof(cl->R_tab->rate), &cl->R_tab->rate);\r\nreturn skb->len;\r\nnla_put_failure:\r\nnlmsg_trim(skb, b);\r\nreturn -1;\r\n}\r\nstatic int cbq_dump_lss(struct sk_buff *skb, struct cbq_class *cl)\r\n{\r\nunsigned char *b = skb_tail_pointer(skb);\r\nstruct tc_cbq_lssopt opt;\r\nopt.flags = 0;\r\nif (cl->borrow == NULL)\r\nopt.flags |= TCF_CBQ_LSS_BOUNDED;\r\nif (cl->share == NULL)\r\nopt.flags |= TCF_CBQ_LSS_ISOLATED;\r\nopt.ewma_log = cl->ewma_log;\r\nopt.level = cl->level;\r\nopt.avpkt = cl->avpkt;\r\nopt.maxidle = cl->maxidle;\r\nopt.minidle = (u32)(-cl->minidle);\r\nopt.offtime = cl->offtime;\r\nopt.change = ~0;\r\nNLA_PUT(skb, TCA_CBQ_LSSOPT, sizeof(opt), &opt);\r\nreturn skb->len;\r\nnla_put_failure:\r\nnlmsg_trim(skb, b);\r\nreturn -1;\r\n}\r\nstatic int cbq_dump_wrr(struct sk_buff *skb, struct cbq_class *cl)\r\n{\r\nunsigned char *b = skb_tail_pointer(skb);\r\nstruct tc_cbq_wrropt opt;\r\nopt.flags = 0;\r\nopt.allot = cl->allot;\r\nopt.priority = cl->priority + 1;\r\nopt.cpriority = cl->cpriority + 1;\r\nopt.weight = cl->weight;\r\nNLA_PUT(skb, TCA_CBQ_WRROPT, sizeof(opt), &opt);\r\nreturn skb->len;\r\nnla_put_failure:\r\nnlmsg_trim(skb, b);\r\nreturn -1;\r\n}\r\nstatic int cbq_dump_ovl(struct sk_buff *skb, struct cbq_class *cl)\r\n{\r\nunsigned char *b = skb_tail_pointer(skb);\r\nstruct tc_cbq_ovl opt;\r\nopt.strategy = cl->ovl_strategy;\r\nopt.priority2 = cl->priority2 + 1;\r\nopt.pad = 0;\r\nopt.penalty = cl->penalty;\r\nNLA_PUT(skb, TCA_CBQ_OVL_STRATEGY, sizeof(opt), &opt);\r\nreturn skb->len;\r\nnla_put_failure:\r\nnlmsg_trim(skb, b);\r\nreturn -1;\r\n}\r\nstatic int cbq_dump_fopt(struct sk_buff *skb, struct cbq_class *cl)\r\n{\r\nunsigned char *b = skb_tail_pointer(skb);\r\nstruct tc_cbq_fopt opt;\r\nif (cl->split || cl->defmap) {\r\nopt.split = cl->split ? cl->split->common.classid : 0;\r\nopt.defmap = cl->defmap;\r\nopt.defchange = ~0;\r\nNLA_PUT(skb, TCA_CBQ_FOPT, sizeof(opt), &opt);\r\n}\r\nreturn skb->len;\r\nnla_put_failure:\r\nnlmsg_trim(skb, b);\r\nreturn -1;\r\n}\r\nstatic int cbq_dump_police(struct sk_buff *skb, struct cbq_class *cl)\r\n{\r\nunsigned char *b = skb_tail_pointer(skb);\r\nstruct tc_cbq_police opt;\r\nif (cl->police) {\r\nopt.police = cl->police;\r\nopt.__res1 = 0;\r\nopt.__res2 = 0;\r\nNLA_PUT(skb, TCA_CBQ_POLICE, sizeof(opt), &opt);\r\n}\r\nreturn skb->len;\r\nnla_put_failure:\r\nnlmsg_trim(skb, b);\r\nreturn -1;\r\n}\r\nstatic int cbq_dump_attr(struct sk_buff *skb, struct cbq_class *cl)\r\n{\r\nif (cbq_dump_lss(skb, cl) < 0 ||\r\ncbq_dump_rate(skb, cl) < 0 ||\r\ncbq_dump_wrr(skb, cl) < 0 ||\r\ncbq_dump_ovl(skb, cl) < 0 ||\r\n#ifdef CONFIG_NET_CLS_ACT\r\ncbq_dump_police(skb, cl) < 0 ||\r\n#endif\r\ncbq_dump_fopt(skb, cl) < 0)\r\nreturn -1;\r\nreturn 0;\r\n}\r\nstatic int cbq_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *nest;\r\nnest = nla_nest_start(skb, TCA_OPTIONS);\r\nif (nest == NULL)\r\ngoto nla_put_failure;\r\nif (cbq_dump_attr(skb, &q->link) < 0)\r\ngoto nla_put_failure;\r\nnla_nest_end(skb, nest);\r\nreturn skb->len;\r\nnla_put_failure:\r\nnla_nest_cancel(skb, nest);\r\nreturn -1;\r\n}\r\nstatic int\r\ncbq_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nq->link.xstats.avgidle = q->link.avgidle;\r\nreturn gnet_stats_copy_app(d, &q->link.xstats, sizeof(q->link.xstats));\r\n}\r\nstatic int\r\ncbq_dump_class(struct Qdisc *sch, unsigned long arg,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\nstruct cbq_class *cl = (struct cbq_class *)arg;\r\nstruct nlattr *nest;\r\nif (cl->tparent)\r\ntcm->tcm_parent = cl->tparent->common.classid;\r\nelse\r\ntcm->tcm_parent = TC_H_ROOT;\r\ntcm->tcm_handle = cl->common.classid;\r\ntcm->tcm_info = cl->q->handle;\r\nnest = nla_nest_start(skb, TCA_OPTIONS);\r\nif (nest == NULL)\r\ngoto nla_put_failure;\r\nif (cbq_dump_attr(skb, cl) < 0)\r\ngoto nla_put_failure;\r\nnla_nest_end(skb, nest);\r\nreturn skb->len;\r\nnla_put_failure:\r\nnla_nest_cancel(skb, nest);\r\nreturn -1;\r\n}\r\nstatic int\r\ncbq_dump_class_stats(struct Qdisc *sch, unsigned long arg,\r\nstruct gnet_dump *d)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl = (struct cbq_class *)arg;\r\ncl->qstats.qlen = cl->q->q.qlen;\r\ncl->xstats.avgidle = cl->avgidle;\r\ncl->xstats.undertime = 0;\r\nif (cl->undertime != PSCHED_PASTPERFECT)\r\ncl->xstats.undertime = cl->undertime - q->now;\r\nif (gnet_stats_copy_basic(d, &cl->bstats) < 0 ||\r\ngnet_stats_copy_rate_est(d, &cl->bstats, &cl->rate_est) < 0 ||\r\ngnet_stats_copy_queue(d, &cl->qstats) < 0)\r\nreturn -1;\r\nreturn gnet_stats_copy_app(d, &cl->xstats, sizeof(cl->xstats));\r\n}\r\nstatic int cbq_graft(struct Qdisc *sch, unsigned long arg, struct Qdisc *new,\r\nstruct Qdisc **old)\r\n{\r\nstruct cbq_class *cl = (struct cbq_class *)arg;\r\nif (new == NULL) {\r\nnew = qdisc_create_dflt(sch->dev_queue,\r\n&pfifo_qdisc_ops, cl->common.classid);\r\nif (new == NULL)\r\nreturn -ENOBUFS;\r\n} else {\r\n#ifdef CONFIG_NET_CLS_ACT\r\nif (cl->police == TC_POLICE_RECLASSIFY)\r\nnew->reshape_fail = cbq_reshape_fail;\r\n#endif\r\n}\r\nsch_tree_lock(sch);\r\n*old = cl->q;\r\ncl->q = new;\r\nqdisc_tree_decrease_qlen(*old, (*old)->q.qlen);\r\nqdisc_reset(*old);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic struct Qdisc *cbq_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct cbq_class *cl = (struct cbq_class *)arg;\r\nreturn cl->q;\r\n}\r\nstatic void cbq_qlen_notify(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct cbq_class *cl = (struct cbq_class *)arg;\r\nif (cl->q->q.qlen == 0)\r\ncbq_deactivate_class(cl);\r\n}\r\nstatic unsigned long cbq_get(struct Qdisc *sch, u32 classid)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl = cbq_class_lookup(q, classid);\r\nif (cl) {\r\ncl->refcnt++;\r\nreturn (unsigned long)cl;\r\n}\r\nreturn 0;\r\n}\r\nstatic void cbq_destroy_class(struct Qdisc *sch, struct cbq_class *cl)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nWARN_ON(cl->filters);\r\ntcf_destroy_chain(&cl->filter_list);\r\nqdisc_destroy(cl->q);\r\nqdisc_put_rtab(cl->R_tab);\r\ngen_kill_estimator(&cl->bstats, &cl->rate_est);\r\nif (cl != &q->link)\r\nkfree(cl);\r\n}\r\nstatic void cbq_destroy(struct Qdisc *sch)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct hlist_node *n, *next;\r\nstruct cbq_class *cl;\r\nunsigned int h;\r\n#ifdef CONFIG_NET_CLS_ACT\r\nq->rx_class = NULL;\r\n#endif\r\nfor (h = 0; h < q->clhash.hashsize; h++) {\r\nhlist_for_each_entry(cl, n, &q->clhash.hash[h], common.hnode)\r\ntcf_destroy_chain(&cl->filter_list);\r\n}\r\nfor (h = 0; h < q->clhash.hashsize; h++) {\r\nhlist_for_each_entry_safe(cl, n, next, &q->clhash.hash[h],\r\ncommon.hnode)\r\ncbq_destroy_class(sch, cl);\r\n}\r\nqdisc_class_hash_destroy(&q->clhash);\r\n}\r\nstatic void cbq_put(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct cbq_class *cl = (struct cbq_class *)arg;\r\nif (--cl->refcnt == 0) {\r\n#ifdef CONFIG_NET_CLS_ACT\r\nspinlock_t *root_lock = qdisc_root_sleeping_lock(sch);\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nspin_lock_bh(root_lock);\r\nif (q->rx_class == cl)\r\nq->rx_class = NULL;\r\nspin_unlock_bh(root_lock);\r\n#endif\r\ncbq_destroy_class(sch, cl);\r\n}\r\n}\r\nstatic int\r\ncbq_change_class(struct Qdisc *sch, u32 classid, u32 parentid, struct nlattr **tca,\r\nunsigned long *arg)\r\n{\r\nint err;\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl = (struct cbq_class *)*arg;\r\nstruct nlattr *opt = tca[TCA_OPTIONS];\r\nstruct nlattr *tb[TCA_CBQ_MAX + 1];\r\nstruct cbq_class *parent;\r\nstruct qdisc_rate_table *rtab = NULL;\r\nif (opt == NULL)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_CBQ_MAX, opt, cbq_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (cl) {\r\nif (parentid) {\r\nif (cl->tparent &&\r\ncl->tparent->common.classid != parentid)\r\nreturn -EINVAL;\r\nif (!cl->tparent && parentid != TC_H_ROOT)\r\nreturn -EINVAL;\r\n}\r\nif (tb[TCA_CBQ_RATE]) {\r\nrtab = qdisc_get_rtab(nla_data(tb[TCA_CBQ_RATE]),\r\ntb[TCA_CBQ_RTAB]);\r\nif (rtab == NULL)\r\nreturn -EINVAL;\r\n}\r\nif (tca[TCA_RATE]) {\r\nerr = gen_replace_estimator(&cl->bstats, &cl->rate_est,\r\nqdisc_root_sleeping_lock(sch),\r\ntca[TCA_RATE]);\r\nif (err) {\r\nif (rtab)\r\nqdisc_put_rtab(rtab);\r\nreturn err;\r\n}\r\n}\r\nsch_tree_lock(sch);\r\nif (cl->next_alive != NULL)\r\ncbq_deactivate_class(cl);\r\nif (rtab) {\r\nqdisc_put_rtab(cl->R_tab);\r\ncl->R_tab = rtab;\r\n}\r\nif (tb[TCA_CBQ_LSSOPT])\r\ncbq_set_lss(cl, nla_data(tb[TCA_CBQ_LSSOPT]));\r\nif (tb[TCA_CBQ_WRROPT]) {\r\ncbq_rmprio(q, cl);\r\ncbq_set_wrr(cl, nla_data(tb[TCA_CBQ_WRROPT]));\r\n}\r\nif (tb[TCA_CBQ_OVL_STRATEGY])\r\ncbq_set_overlimit(cl, nla_data(tb[TCA_CBQ_OVL_STRATEGY]));\r\n#ifdef CONFIG_NET_CLS_ACT\r\nif (tb[TCA_CBQ_POLICE])\r\ncbq_set_police(cl, nla_data(tb[TCA_CBQ_POLICE]));\r\n#endif\r\nif (tb[TCA_CBQ_FOPT])\r\ncbq_set_fopt(cl, nla_data(tb[TCA_CBQ_FOPT]));\r\nif (cl->q->q.qlen)\r\ncbq_activate_class(cl);\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nif (parentid == TC_H_ROOT)\r\nreturn -EINVAL;\r\nif (tb[TCA_CBQ_WRROPT] == NULL || tb[TCA_CBQ_RATE] == NULL ||\r\ntb[TCA_CBQ_LSSOPT] == NULL)\r\nreturn -EINVAL;\r\nrtab = qdisc_get_rtab(nla_data(tb[TCA_CBQ_RATE]), tb[TCA_CBQ_RTAB]);\r\nif (rtab == NULL)\r\nreturn -EINVAL;\r\nif (classid) {\r\nerr = -EINVAL;\r\nif (TC_H_MAJ(classid ^ sch->handle) ||\r\ncbq_class_lookup(q, classid))\r\ngoto failure;\r\n} else {\r\nint i;\r\nclassid = TC_H_MAKE(sch->handle, 0x8000);\r\nfor (i = 0; i < 0x8000; i++) {\r\nif (++q->hgenerator >= 0x8000)\r\nq->hgenerator = 1;\r\nif (cbq_class_lookup(q, classid|q->hgenerator) == NULL)\r\nbreak;\r\n}\r\nerr = -ENOSR;\r\nif (i >= 0x8000)\r\ngoto failure;\r\nclassid = classid|q->hgenerator;\r\n}\r\nparent = &q->link;\r\nif (parentid) {\r\nparent = cbq_class_lookup(q, parentid);\r\nerr = -EINVAL;\r\nif (parent == NULL)\r\ngoto failure;\r\n}\r\nerr = -ENOBUFS;\r\ncl = kzalloc(sizeof(*cl), GFP_KERNEL);\r\nif (cl == NULL)\r\ngoto failure;\r\nif (tca[TCA_RATE]) {\r\nerr = gen_new_estimator(&cl->bstats, &cl->rate_est,\r\nqdisc_root_sleeping_lock(sch),\r\ntca[TCA_RATE]);\r\nif (err) {\r\nkfree(cl);\r\ngoto failure;\r\n}\r\n}\r\ncl->R_tab = rtab;\r\nrtab = NULL;\r\ncl->refcnt = 1;\r\ncl->q = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops, classid);\r\nif (!cl->q)\r\ncl->q = &noop_qdisc;\r\ncl->common.classid = classid;\r\ncl->tparent = parent;\r\ncl->qdisc = sch;\r\ncl->allot = parent->allot;\r\ncl->quantum = cl->allot;\r\ncl->weight = cl->R_tab->rate.rate;\r\nsch_tree_lock(sch);\r\ncbq_link_class(cl);\r\ncl->borrow = cl->tparent;\r\nif (cl->tparent != &q->link)\r\ncl->share = cl->tparent;\r\ncbq_adjust_levels(parent);\r\ncl->minidle = -0x7FFFFFFF;\r\ncbq_set_lss(cl, nla_data(tb[TCA_CBQ_LSSOPT]));\r\ncbq_set_wrr(cl, nla_data(tb[TCA_CBQ_WRROPT]));\r\nif (cl->ewma_log == 0)\r\ncl->ewma_log = q->link.ewma_log;\r\nif (cl->maxidle == 0)\r\ncl->maxidle = q->link.maxidle;\r\nif (cl->avpkt == 0)\r\ncl->avpkt = q->link.avpkt;\r\ncl->overlimit = cbq_ovl_classic;\r\nif (tb[TCA_CBQ_OVL_STRATEGY])\r\ncbq_set_overlimit(cl, nla_data(tb[TCA_CBQ_OVL_STRATEGY]));\r\n#ifdef CONFIG_NET_CLS_ACT\r\nif (tb[TCA_CBQ_POLICE])\r\ncbq_set_police(cl, nla_data(tb[TCA_CBQ_POLICE]));\r\n#endif\r\nif (tb[TCA_CBQ_FOPT])\r\ncbq_set_fopt(cl, nla_data(tb[TCA_CBQ_FOPT]));\r\nsch_tree_unlock(sch);\r\nqdisc_class_hash_grow(sch, &q->clhash);\r\n*arg = (unsigned long)cl;\r\nreturn 0;\r\nfailure:\r\nqdisc_put_rtab(rtab);\r\nreturn err;\r\n}\r\nstatic int cbq_delete(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl = (struct cbq_class *)arg;\r\nunsigned int qlen;\r\nif (cl->filters || cl->children || cl == &q->link)\r\nreturn -EBUSY;\r\nsch_tree_lock(sch);\r\nqlen = cl->q->q.qlen;\r\nqdisc_reset(cl->q);\r\nqdisc_tree_decrease_qlen(cl->q, qlen);\r\nif (cl->next_alive)\r\ncbq_deactivate_class(cl);\r\nif (q->tx_borrowed == cl)\r\nq->tx_borrowed = q->tx_class;\r\nif (q->tx_class == cl) {\r\nq->tx_class = NULL;\r\nq->tx_borrowed = NULL;\r\n}\r\n#ifdef CONFIG_NET_CLS_ACT\r\nif (q->rx_class == cl)\r\nq->rx_class = NULL;\r\n#endif\r\ncbq_unlink_class(cl);\r\ncbq_adjust_levels(cl->tparent);\r\ncl->defmap = 0;\r\ncbq_sync_defmap(cl);\r\ncbq_rmprio(q, cl);\r\nsch_tree_unlock(sch);\r\nBUG_ON(--cl->refcnt == 0);\r\nreturn 0;\r\n}\r\nstatic struct tcf_proto **cbq_find_tcf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl = (struct cbq_class *)arg;\r\nif (cl == NULL)\r\ncl = &q->link;\r\nreturn &cl->filter_list;\r\n}\r\nstatic unsigned long cbq_bind_filter(struct Qdisc *sch, unsigned long parent,\r\nu32 classid)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *p = (struct cbq_class *)parent;\r\nstruct cbq_class *cl = cbq_class_lookup(q, classid);\r\nif (cl) {\r\nif (p && p->level <= cl->level)\r\nreturn 0;\r\ncl->filters++;\r\nreturn (unsigned long)cl;\r\n}\r\nreturn 0;\r\n}\r\nstatic void cbq_unbind_filter(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct cbq_class *cl = (struct cbq_class *)arg;\r\ncl->filters--;\r\n}\r\nstatic void cbq_walk(struct Qdisc *sch, struct qdisc_walker *arg)\r\n{\r\nstruct cbq_sched_data *q = qdisc_priv(sch);\r\nstruct cbq_class *cl;\r\nstruct hlist_node *n;\r\nunsigned int h;\r\nif (arg->stop)\r\nreturn;\r\nfor (h = 0; h < q->clhash.hashsize; h++) {\r\nhlist_for_each_entry(cl, n, &q->clhash.hash[h], common.hnode) {\r\nif (arg->count < arg->skip) {\r\narg->count++;\r\ncontinue;\r\n}\r\nif (arg->fn(sch, (unsigned long)cl, arg) < 0) {\r\narg->stop = 1;\r\nreturn;\r\n}\r\narg->count++;\r\n}\r\n}\r\n}\r\nstatic int __init cbq_module_init(void)\r\n{\r\nreturn register_qdisc(&cbq_qdisc_ops);\r\n}\r\nstatic void __exit cbq_module_exit(void)\r\n{\r\nunregister_qdisc(&cbq_qdisc_ops);\r\n}
