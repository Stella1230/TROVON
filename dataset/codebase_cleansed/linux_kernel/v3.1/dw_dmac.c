static struct device *chan2dev(struct dma_chan *chan)\r\n{\r\nreturn &chan->dev->device;\r\n}\r\nstatic struct device *chan2parent(struct dma_chan *chan)\r\n{\r\nreturn chan->dev->device.parent;\r\n}\r\nstatic struct dw_desc *dwc_first_active(struct dw_dma_chan *dwc)\r\n{\r\nreturn list_entry(dwc->active_list.next, struct dw_desc, desc_node);\r\n}\r\nstatic struct dw_desc *dwc_desc_get(struct dw_dma_chan *dwc)\r\n{\r\nstruct dw_desc *desc, *_desc;\r\nstruct dw_desc *ret = NULL;\r\nunsigned int i = 0;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &dwc->free_list, desc_node) {\r\nif (async_tx_test_ack(&desc->txd)) {\r\nlist_del(&desc->desc_node);\r\nret = desc;\r\nbreak;\r\n}\r\ndev_dbg(chan2dev(&dwc->chan), "desc %p not ACKed\n", desc);\r\ni++;\r\n}\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\ndev_vdbg(chan2dev(&dwc->chan), "scanned %u descriptors on freelist\n", i);\r\nreturn ret;\r\n}\r\nstatic void dwc_sync_desc_for_cpu(struct dw_dma_chan *dwc, struct dw_desc *desc)\r\n{\r\nstruct dw_desc *child;\r\nlist_for_each_entry(child, &desc->tx_list, desc_node)\r\ndma_sync_single_for_cpu(chan2parent(&dwc->chan),\r\nchild->txd.phys, sizeof(child->lli),\r\nDMA_TO_DEVICE);\r\ndma_sync_single_for_cpu(chan2parent(&dwc->chan),\r\ndesc->txd.phys, sizeof(desc->lli),\r\nDMA_TO_DEVICE);\r\n}\r\nstatic void dwc_desc_put(struct dw_dma_chan *dwc, struct dw_desc *desc)\r\n{\r\nunsigned long flags;\r\nif (desc) {\r\nstruct dw_desc *child;\r\ndwc_sync_desc_for_cpu(dwc, desc);\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nlist_for_each_entry(child, &desc->tx_list, desc_node)\r\ndev_vdbg(chan2dev(&dwc->chan),\r\n"moving child desc %p to freelist\n",\r\nchild);\r\nlist_splice_init(&desc->tx_list, &dwc->free_list);\r\ndev_vdbg(chan2dev(&dwc->chan), "moving desc %p to freelist\n", desc);\r\nlist_add(&desc->desc_node, &dwc->free_list);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\n}\r\n}\r\nstatic dma_cookie_t\r\ndwc_assign_cookie(struct dw_dma_chan *dwc, struct dw_desc *desc)\r\n{\r\ndma_cookie_t cookie = dwc->chan.cookie;\r\nif (++cookie < 0)\r\ncookie = 1;\r\ndwc->chan.cookie = cookie;\r\ndesc->txd.cookie = cookie;\r\nreturn cookie;\r\n}\r\nstatic void dwc_dostart(struct dw_dma_chan *dwc, struct dw_desc *first)\r\n{\r\nstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\r\nif (dma_readl(dw, CH_EN) & dwc->mask) {\r\ndev_err(chan2dev(&dwc->chan),\r\n"BUG: Attempted to start non-idle channel\n");\r\ndev_err(chan2dev(&dwc->chan),\r\n" SAR: 0x%x DAR: 0x%x LLP: 0x%x CTL: 0x%x:%08x\n",\r\nchannel_readl(dwc, SAR),\r\nchannel_readl(dwc, DAR),\r\nchannel_readl(dwc, LLP),\r\nchannel_readl(dwc, CTL_HI),\r\nchannel_readl(dwc, CTL_LO));\r\nreturn;\r\n}\r\nchannel_writel(dwc, LLP, first->txd.phys);\r\nchannel_writel(dwc, CTL_LO,\r\nDWC_CTLL_LLP_D_EN | DWC_CTLL_LLP_S_EN);\r\nchannel_writel(dwc, CTL_HI, 0);\r\nchannel_set_bit(dw, CH_EN, dwc->mask);\r\n}\r\nstatic void\r\ndwc_descriptor_complete(struct dw_dma_chan *dwc, struct dw_desc *desc,\r\nbool callback_required)\r\n{\r\ndma_async_tx_callback callback = NULL;\r\nvoid *param = NULL;\r\nstruct dma_async_tx_descriptor *txd = &desc->txd;\r\nstruct dw_desc *child;\r\nunsigned long flags;\r\ndev_vdbg(chan2dev(&dwc->chan), "descriptor %u complete\n", txd->cookie);\r\nspin_lock_irqsave(&dwc->lock, flags);\r\ndwc->completed = txd->cookie;\r\nif (callback_required) {\r\ncallback = txd->callback;\r\nparam = txd->callback_param;\r\n}\r\ndwc_sync_desc_for_cpu(dwc, desc);\r\nlist_for_each_entry(child, &desc->tx_list, desc_node)\r\nasync_tx_ack(&child->txd);\r\nasync_tx_ack(&desc->txd);\r\nlist_splice_init(&desc->tx_list, &dwc->free_list);\r\nlist_move(&desc->desc_node, &dwc->free_list);\r\nif (!dwc->chan.private) {\r\nstruct device *parent = chan2parent(&dwc->chan);\r\nif (!(txd->flags & DMA_COMPL_SKIP_DEST_UNMAP)) {\r\nif (txd->flags & DMA_COMPL_DEST_UNMAP_SINGLE)\r\ndma_unmap_single(parent, desc->lli.dar,\r\ndesc->len, DMA_FROM_DEVICE);\r\nelse\r\ndma_unmap_page(parent, desc->lli.dar,\r\ndesc->len, DMA_FROM_DEVICE);\r\n}\r\nif (!(txd->flags & DMA_COMPL_SKIP_SRC_UNMAP)) {\r\nif (txd->flags & DMA_COMPL_SRC_UNMAP_SINGLE)\r\ndma_unmap_single(parent, desc->lli.sar,\r\ndesc->len, DMA_TO_DEVICE);\r\nelse\r\ndma_unmap_page(parent, desc->lli.sar,\r\ndesc->len, DMA_TO_DEVICE);\r\n}\r\n}\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nif (callback_required && callback)\r\ncallback(param);\r\n}\r\nstatic void dwc_complete_all(struct dw_dma *dw, struct dw_dma_chan *dwc)\r\n{\r\nstruct dw_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\nunsigned long flags;\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nif (dma_readl(dw, CH_EN) & dwc->mask) {\r\ndev_err(chan2dev(&dwc->chan),\r\n"BUG: XFER bit set, but channel not idle!\n");\r\nchannel_clear_bit(dw, CH_EN, dwc->mask);\r\nwhile (dma_readl(dw, CH_EN) & dwc->mask)\r\ncpu_relax();\r\n}\r\nlist_splice_init(&dwc->active_list, &list);\r\nif (!list_empty(&dwc->queue)) {\r\nlist_move(dwc->queue.next, &dwc->active_list);\r\ndwc_dostart(dwc, dwc_first_active(dwc));\r\n}\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node)\r\ndwc_descriptor_complete(dwc, desc, true);\r\n}\r\nstatic void dwc_scan_descriptors(struct dw_dma *dw, struct dw_dma_chan *dwc)\r\n{\r\ndma_addr_t llp;\r\nstruct dw_desc *desc, *_desc;\r\nstruct dw_desc *child;\r\nu32 status_xfer;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dwc->lock, flags);\r\ndma_writel(dw, CLEAR.BLOCK, dwc->mask);\r\nllp = channel_readl(dwc, LLP);\r\nstatus_xfer = dma_readl(dw, RAW.XFER);\r\nif (status_xfer & dwc->mask) {\r\ndma_writel(dw, CLEAR.XFER, dwc->mask);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\ndwc_complete_all(dw, dwc);\r\nreturn;\r\n}\r\nif (list_empty(&dwc->active_list)) {\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nreturn;\r\n}\r\ndev_vdbg(chan2dev(&dwc->chan), "scan_descriptors: llp=0x%x\n", llp);\r\nlist_for_each_entry_safe(desc, _desc, &dwc->active_list, desc_node) {\r\nif (desc->txd.phys == llp) {\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nreturn;\r\n}\r\nif (desc->lli.llp == llp) {\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nreturn;\r\n}\r\nlist_for_each_entry(child, &desc->tx_list, desc_node)\r\nif (child->lli.llp == llp) {\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nreturn;\r\n}\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\ndwc_descriptor_complete(dwc, desc, true);\r\nspin_lock_irqsave(&dwc->lock, flags);\r\n}\r\ndev_err(chan2dev(&dwc->chan),\r\n"BUG: All descriptors done, but channel not idle!\n");\r\nchannel_clear_bit(dw, CH_EN, dwc->mask);\r\nwhile (dma_readl(dw, CH_EN) & dwc->mask)\r\ncpu_relax();\r\nif (!list_empty(&dwc->queue)) {\r\nlist_move(dwc->queue.next, &dwc->active_list);\r\ndwc_dostart(dwc, dwc_first_active(dwc));\r\n}\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\n}\r\nstatic void dwc_dump_lli(struct dw_dma_chan *dwc, struct dw_lli *lli)\r\n{\r\ndev_printk(KERN_CRIT, chan2dev(&dwc->chan),\r\n" desc: s0x%x d0x%x l0x%x c0x%x:%x\n",\r\nlli->sar, lli->dar, lli->llp,\r\nlli->ctlhi, lli->ctllo);\r\n}\r\nstatic void dwc_handle_error(struct dw_dma *dw, struct dw_dma_chan *dwc)\r\n{\r\nstruct dw_desc *bad_desc;\r\nstruct dw_desc *child;\r\nunsigned long flags;\r\ndwc_scan_descriptors(dw, dwc);\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nbad_desc = dwc_first_active(dwc);\r\nlist_del_init(&bad_desc->desc_node);\r\nlist_move(dwc->queue.next, dwc->active_list.prev);\r\ndma_writel(dw, CLEAR.ERROR, dwc->mask);\r\nif (!list_empty(&dwc->active_list))\r\ndwc_dostart(dwc, dwc_first_active(dwc));\r\ndev_printk(KERN_CRIT, chan2dev(&dwc->chan),\r\n"Bad descriptor submitted for DMA!\n");\r\ndev_printk(KERN_CRIT, chan2dev(&dwc->chan),\r\n" cookie: %d\n", bad_desc->txd.cookie);\r\ndwc_dump_lli(dwc, &bad_desc->lli);\r\nlist_for_each_entry(child, &bad_desc->tx_list, desc_node)\r\ndwc_dump_lli(dwc, &child->lli);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\ndwc_descriptor_complete(dwc, bad_desc, true);\r\n}\r\ninline dma_addr_t dw_dma_get_src_addr(struct dma_chan *chan)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nreturn channel_readl(dwc, SAR);\r\n}\r\ninline dma_addr_t dw_dma_get_dst_addr(struct dma_chan *chan)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nreturn channel_readl(dwc, DAR);\r\n}\r\nstatic void dwc_handle_cyclic(struct dw_dma *dw, struct dw_dma_chan *dwc,\r\nu32 status_block, u32 status_err, u32 status_xfer)\r\n{\r\nunsigned long flags;\r\nif (status_block & dwc->mask) {\r\nvoid (*callback)(void *param);\r\nvoid *callback_param;\r\ndev_vdbg(chan2dev(&dwc->chan), "new cyclic period llp 0x%08x\n",\r\nchannel_readl(dwc, LLP));\r\ndma_writel(dw, CLEAR.BLOCK, dwc->mask);\r\ncallback = dwc->cdesc->period_callback;\r\ncallback_param = dwc->cdesc->period_callback_param;\r\nif (callback)\r\ncallback(callback_param);\r\n}\r\nif (unlikely(status_err & dwc->mask) ||\r\nunlikely(status_xfer & dwc->mask)) {\r\nint i;\r\ndev_err(chan2dev(&dwc->chan), "cyclic DMA unexpected %s "\r\n"interrupt, stopping DMA transfer\n",\r\nstatus_xfer ? "xfer" : "error");\r\nspin_lock_irqsave(&dwc->lock, flags);\r\ndev_err(chan2dev(&dwc->chan),\r\n" SAR: 0x%x DAR: 0x%x LLP: 0x%x CTL: 0x%x:%08x\n",\r\nchannel_readl(dwc, SAR),\r\nchannel_readl(dwc, DAR),\r\nchannel_readl(dwc, LLP),\r\nchannel_readl(dwc, CTL_HI),\r\nchannel_readl(dwc, CTL_LO));\r\nchannel_clear_bit(dw, CH_EN, dwc->mask);\r\nwhile (dma_readl(dw, CH_EN) & dwc->mask)\r\ncpu_relax();\r\nchannel_writel(dwc, LLP, 0);\r\nchannel_writel(dwc, CTL_LO, 0);\r\nchannel_writel(dwc, CTL_HI, 0);\r\ndma_writel(dw, CLEAR.BLOCK, dwc->mask);\r\ndma_writel(dw, CLEAR.ERROR, dwc->mask);\r\ndma_writel(dw, CLEAR.XFER, dwc->mask);\r\nfor (i = 0; i < dwc->cdesc->periods; i++)\r\ndwc_dump_lli(dwc, &dwc->cdesc->desc[i]->lli);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\n}\r\n}\r\nstatic void dw_dma_tasklet(unsigned long data)\r\n{\r\nstruct dw_dma *dw = (struct dw_dma *)data;\r\nstruct dw_dma_chan *dwc;\r\nu32 status_block;\r\nu32 status_xfer;\r\nu32 status_err;\r\nint i;\r\nstatus_block = dma_readl(dw, RAW.BLOCK);\r\nstatus_xfer = dma_readl(dw, RAW.XFER);\r\nstatus_err = dma_readl(dw, RAW.ERROR);\r\ndev_vdbg(dw->dma.dev, "tasklet: status_block=%x status_err=%x\n",\r\nstatus_block, status_err);\r\nfor (i = 0; i < dw->dma.chancnt; i++) {\r\ndwc = &dw->chan[i];\r\nif (test_bit(DW_DMA_IS_CYCLIC, &dwc->flags))\r\ndwc_handle_cyclic(dw, dwc, status_block, status_err,\r\nstatus_xfer);\r\nelse if (status_err & (1 << i))\r\ndwc_handle_error(dw, dwc);\r\nelse if ((status_block | status_xfer) & (1 << i))\r\ndwc_scan_descriptors(dw, dwc);\r\n}\r\nchannel_set_bit(dw, MASK.XFER, dw->all_chan_mask);\r\nchannel_set_bit(dw, MASK.BLOCK, dw->all_chan_mask);\r\nchannel_set_bit(dw, MASK.ERROR, dw->all_chan_mask);\r\n}\r\nstatic irqreturn_t dw_dma_interrupt(int irq, void *dev_id)\r\n{\r\nstruct dw_dma *dw = dev_id;\r\nu32 status;\r\ndev_vdbg(dw->dma.dev, "interrupt: status=0x%x\n",\r\ndma_readl(dw, STATUS_INT));\r\nchannel_clear_bit(dw, MASK.XFER, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.BLOCK, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.ERROR, dw->all_chan_mask);\r\nstatus = dma_readl(dw, STATUS_INT);\r\nif (status) {\r\ndev_err(dw->dma.dev,\r\n"BUG: Unexpected interrupts pending: 0x%x\n",\r\nstatus);\r\nchannel_clear_bit(dw, MASK.XFER, (1 << 8) - 1);\r\nchannel_clear_bit(dw, MASK.BLOCK, (1 << 8) - 1);\r\nchannel_clear_bit(dw, MASK.SRC_TRAN, (1 << 8) - 1);\r\nchannel_clear_bit(dw, MASK.DST_TRAN, (1 << 8) - 1);\r\nchannel_clear_bit(dw, MASK.ERROR, (1 << 8) - 1);\r\n}\r\ntasklet_schedule(&dw->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic dma_cookie_t dwc_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct dw_desc *desc = txd_to_dw_desc(tx);\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(tx->chan);\r\ndma_cookie_t cookie;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dwc->lock, flags);\r\ncookie = dwc_assign_cookie(dwc, desc);\r\nif (list_empty(&dwc->active_list)) {\r\ndev_vdbg(chan2dev(tx->chan), "tx_submit: started %u\n",\r\ndesc->txd.cookie);\r\nlist_add_tail(&desc->desc_node, &dwc->active_list);\r\ndwc_dostart(dwc, dwc_first_active(dwc));\r\n} else {\r\ndev_vdbg(chan2dev(tx->chan), "tx_submit: queued %u\n",\r\ndesc->txd.cookie);\r\nlist_add_tail(&desc->desc_node, &dwc->queue);\r\n}\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\ndwc_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nstruct dw_desc *desc;\r\nstruct dw_desc *first;\r\nstruct dw_desc *prev;\r\nsize_t xfer_count;\r\nsize_t offset;\r\nunsigned int src_width;\r\nunsigned int dst_width;\r\nu32 ctllo;\r\ndev_vdbg(chan2dev(chan), "prep_dma_memcpy d0x%x s0x%x l0x%zx f0x%lx\n",\r\ndest, src, len, flags);\r\nif (unlikely(!len)) {\r\ndev_dbg(chan2dev(chan), "prep_dma_memcpy: length is zero!\n");\r\nreturn NULL;\r\n}\r\nif (!((src | dest | len) & 7))\r\nsrc_width = dst_width = 3;\r\nelse if (!((src | dest | len) & 3))\r\nsrc_width = dst_width = 2;\r\nelse if (!((src | dest | len) & 1))\r\nsrc_width = dst_width = 1;\r\nelse\r\nsrc_width = dst_width = 0;\r\nctllo = DWC_DEFAULT_CTLLO(chan->private)\r\n| DWC_CTLL_DST_WIDTH(dst_width)\r\n| DWC_CTLL_SRC_WIDTH(src_width)\r\n| DWC_CTLL_DST_INC\r\n| DWC_CTLL_SRC_INC\r\n| DWC_CTLL_FC_M2M;\r\nprev = first = NULL;\r\nfor (offset = 0; offset < len; offset += xfer_count << src_width) {\r\nxfer_count = min_t(size_t, (len - offset) >> src_width,\r\nDWC_MAX_COUNT);\r\ndesc = dwc_desc_get(dwc);\r\nif (!desc)\r\ngoto err_desc_get;\r\ndesc->lli.sar = src + offset;\r\ndesc->lli.dar = dest + offset;\r\ndesc->lli.ctllo = ctllo;\r\ndesc->lli.ctlhi = xfer_count;\r\nif (!first) {\r\nfirst = desc;\r\n} else {\r\nprev->lli.llp = desc->txd.phys;\r\ndma_sync_single_for_device(chan2parent(chan),\r\nprev->txd.phys, sizeof(prev->lli),\r\nDMA_TO_DEVICE);\r\nlist_add_tail(&desc->desc_node,\r\n&first->tx_list);\r\n}\r\nprev = desc;\r\n}\r\nif (flags & DMA_PREP_INTERRUPT)\r\nprev->lli.ctllo |= DWC_CTLL_INT_EN;\r\nprev->lli.llp = 0;\r\ndma_sync_single_for_device(chan2parent(chan),\r\nprev->txd.phys, sizeof(prev->lli),\r\nDMA_TO_DEVICE);\r\nfirst->txd.flags = flags;\r\nfirst->len = len;\r\nreturn &first->txd;\r\nerr_desc_get:\r\ndwc_desc_put(dwc, first);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\ndwc_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_data_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nstruct dw_dma_slave *dws = chan->private;\r\nstruct dw_desc *prev;\r\nstruct dw_desc *first;\r\nu32 ctllo;\r\ndma_addr_t reg;\r\nunsigned int reg_width;\r\nunsigned int mem_width;\r\nunsigned int i;\r\nstruct scatterlist *sg;\r\nsize_t total_len = 0;\r\ndev_vdbg(chan2dev(chan), "prep_dma_slave\n");\r\nif (unlikely(!dws || !sg_len))\r\nreturn NULL;\r\nreg_width = dws->reg_width;\r\nprev = first = NULL;\r\nswitch (direction) {\r\ncase DMA_TO_DEVICE:\r\nctllo = (DWC_DEFAULT_CTLLO(chan->private)\r\n| DWC_CTLL_DST_WIDTH(reg_width)\r\n| DWC_CTLL_DST_FIX\r\n| DWC_CTLL_SRC_INC\r\n| DWC_CTLL_FC(dws->fc));\r\nreg = dws->tx_reg;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nstruct dw_desc *desc;\r\nu32 len, dlen, mem;\r\nmem = sg_phys(sg);\r\nlen = sg_dma_len(sg);\r\nmem_width = 2;\r\nif (unlikely(mem & 3 || len & 3))\r\nmem_width = 0;\r\nslave_sg_todev_fill_desc:\r\ndesc = dwc_desc_get(dwc);\r\nif (!desc) {\r\ndev_err(chan2dev(chan),\r\n"not enough descriptors available\n");\r\ngoto err_desc_get;\r\n}\r\ndesc->lli.sar = mem;\r\ndesc->lli.dar = reg;\r\ndesc->lli.ctllo = ctllo | DWC_CTLL_SRC_WIDTH(mem_width);\r\nif ((len >> mem_width) > DWC_MAX_COUNT) {\r\ndlen = DWC_MAX_COUNT << mem_width;\r\nmem += dlen;\r\nlen -= dlen;\r\n} else {\r\ndlen = len;\r\nlen = 0;\r\n}\r\ndesc->lli.ctlhi = dlen >> mem_width;\r\nif (!first) {\r\nfirst = desc;\r\n} else {\r\nprev->lli.llp = desc->txd.phys;\r\ndma_sync_single_for_device(chan2parent(chan),\r\nprev->txd.phys,\r\nsizeof(prev->lli),\r\nDMA_TO_DEVICE);\r\nlist_add_tail(&desc->desc_node,\r\n&first->tx_list);\r\n}\r\nprev = desc;\r\ntotal_len += dlen;\r\nif (len)\r\ngoto slave_sg_todev_fill_desc;\r\n}\r\nbreak;\r\ncase DMA_FROM_DEVICE:\r\nctllo = (DWC_DEFAULT_CTLLO(chan->private)\r\n| DWC_CTLL_SRC_WIDTH(reg_width)\r\n| DWC_CTLL_DST_INC\r\n| DWC_CTLL_SRC_FIX\r\n| DWC_CTLL_FC(dws->fc));\r\nreg = dws->rx_reg;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nstruct dw_desc *desc;\r\nu32 len, dlen, mem;\r\nmem = sg_phys(sg);\r\nlen = sg_dma_len(sg);\r\nmem_width = 2;\r\nif (unlikely(mem & 3 || len & 3))\r\nmem_width = 0;\r\nslave_sg_fromdev_fill_desc:\r\ndesc = dwc_desc_get(dwc);\r\nif (!desc) {\r\ndev_err(chan2dev(chan),\r\n"not enough descriptors available\n");\r\ngoto err_desc_get;\r\n}\r\ndesc->lli.sar = reg;\r\ndesc->lli.dar = mem;\r\ndesc->lli.ctllo = ctllo | DWC_CTLL_DST_WIDTH(mem_width);\r\nif ((len >> reg_width) > DWC_MAX_COUNT) {\r\ndlen = DWC_MAX_COUNT << reg_width;\r\nmem += dlen;\r\nlen -= dlen;\r\n} else {\r\ndlen = len;\r\nlen = 0;\r\n}\r\ndesc->lli.ctlhi = dlen >> reg_width;\r\nif (!first) {\r\nfirst = desc;\r\n} else {\r\nprev->lli.llp = desc->txd.phys;\r\ndma_sync_single_for_device(chan2parent(chan),\r\nprev->txd.phys,\r\nsizeof(prev->lli),\r\nDMA_TO_DEVICE);\r\nlist_add_tail(&desc->desc_node,\r\n&first->tx_list);\r\n}\r\nprev = desc;\r\ntotal_len += dlen;\r\nif (len)\r\ngoto slave_sg_fromdev_fill_desc;\r\n}\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nif (flags & DMA_PREP_INTERRUPT)\r\nprev->lli.ctllo |= DWC_CTLL_INT_EN;\r\nprev->lli.llp = 0;\r\ndma_sync_single_for_device(chan2parent(chan),\r\nprev->txd.phys, sizeof(prev->lli),\r\nDMA_TO_DEVICE);\r\nfirst->len = total_len;\r\nreturn &first->txd;\r\nerr_desc_get:\r\ndwc_desc_put(dwc, first);\r\nreturn NULL;\r\n}\r\nstatic int dwc_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nstruct dw_dma *dw = to_dw_dma(chan->device);\r\nstruct dw_desc *desc, *_desc;\r\nunsigned long flags;\r\nu32 cfglo;\r\nLIST_HEAD(list);\r\nif (cmd == DMA_PAUSE) {\r\nspin_lock_irqsave(&dwc->lock, flags);\r\ncfglo = channel_readl(dwc, CFG_LO);\r\nchannel_writel(dwc, CFG_LO, cfglo | DWC_CFGL_CH_SUSP);\r\nwhile (!(channel_readl(dwc, CFG_LO) & DWC_CFGL_FIFO_EMPTY))\r\ncpu_relax();\r\ndwc->paused = true;\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\n} else if (cmd == DMA_RESUME) {\r\nif (!dwc->paused)\r\nreturn 0;\r\nspin_lock_irqsave(&dwc->lock, flags);\r\ncfglo = channel_readl(dwc, CFG_LO);\r\nchannel_writel(dwc, CFG_LO, cfglo & ~DWC_CFGL_CH_SUSP);\r\ndwc->paused = false;\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\n} else if (cmd == DMA_TERMINATE_ALL) {\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nchannel_clear_bit(dw, CH_EN, dwc->mask);\r\nwhile (dma_readl(dw, CH_EN) & dwc->mask)\r\ncpu_relax();\r\ndwc->paused = false;\r\nlist_splice_init(&dwc->queue, &list);\r\nlist_splice_init(&dwc->active_list, &list);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node)\r\ndwc_descriptor_complete(dwc, desc, false);\r\n} else\r\nreturn -ENXIO;\r\nreturn 0;\r\n}\r\nstatic enum dma_status\r\ndwc_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\ndma_cookie_t last_used;\r\ndma_cookie_t last_complete;\r\nint ret;\r\nlast_complete = dwc->completed;\r\nlast_used = chan->cookie;\r\nret = dma_async_is_complete(cookie, last_complete, last_used);\r\nif (ret != DMA_SUCCESS) {\r\ndwc_scan_descriptors(to_dw_dma(chan->device), dwc);\r\nlast_complete = dwc->completed;\r\nlast_used = chan->cookie;\r\nret = dma_async_is_complete(cookie, last_complete, last_used);\r\n}\r\nif (ret != DMA_SUCCESS)\r\ndma_set_tx_state(txstate, last_complete, last_used,\r\ndwc_first_active(dwc)->len);\r\nelse\r\ndma_set_tx_state(txstate, last_complete, last_used, 0);\r\nif (dwc->paused)\r\nreturn DMA_PAUSED;\r\nreturn ret;\r\n}\r\nstatic void dwc_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nif (!list_empty(&dwc->queue))\r\ndwc_scan_descriptors(to_dw_dma(chan->device), dwc);\r\n}\r\nstatic int dwc_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nstruct dw_dma *dw = to_dw_dma(chan->device);\r\nstruct dw_desc *desc;\r\nstruct dw_dma_slave *dws;\r\nint i;\r\nu32 cfghi;\r\nu32 cfglo;\r\nunsigned long flags;\r\ndev_vdbg(chan2dev(chan), "alloc_chan_resources\n");\r\nif (dma_readl(dw, CH_EN) & dwc->mask) {\r\ndev_dbg(chan2dev(chan), "DMA channel not idle?\n");\r\nreturn -EIO;\r\n}\r\ndwc->completed = chan->cookie = 1;\r\ncfghi = DWC_CFGH_FIFO_MODE;\r\ncfglo = 0;\r\ndws = chan->private;\r\nif (dws) {\r\nBUG_ON(!dws->dma_dev || dws->dma_dev != dw->dma.dev);\r\ncfghi = dws->cfg_hi;\r\ncfglo = dws->cfg_lo & ~DWC_CFGL_CH_PRIOR_MASK;\r\n}\r\ncfglo |= DWC_CFGL_CH_PRIOR(dwc->priority);\r\nchannel_writel(dwc, CFG_LO, cfglo);\r\nchannel_writel(dwc, CFG_HI, cfghi);\r\nspin_lock_irqsave(&dwc->lock, flags);\r\ni = dwc->descs_allocated;\r\nwhile (dwc->descs_allocated < NR_DESCS_PER_CHANNEL) {\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\ndesc = kzalloc(sizeof(struct dw_desc), GFP_KERNEL);\r\nif (!desc) {\r\ndev_info(chan2dev(chan),\r\n"only allocated %d descriptors\n", i);\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nbreak;\r\n}\r\nINIT_LIST_HEAD(&desc->tx_list);\r\ndma_async_tx_descriptor_init(&desc->txd, chan);\r\ndesc->txd.tx_submit = dwc_tx_submit;\r\ndesc->txd.flags = DMA_CTRL_ACK;\r\ndesc->txd.phys = dma_map_single(chan2parent(chan), &desc->lli,\r\nsizeof(desc->lli), DMA_TO_DEVICE);\r\ndwc_desc_put(dwc, desc);\r\nspin_lock_irqsave(&dwc->lock, flags);\r\ni = ++dwc->descs_allocated;\r\n}\r\nchannel_set_bit(dw, MASK.XFER, dwc->mask);\r\nchannel_set_bit(dw, MASK.BLOCK, dwc->mask);\r\nchannel_set_bit(dw, MASK.ERROR, dwc->mask);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\ndev_dbg(chan2dev(chan),\r\n"alloc_chan_resources allocated %d descriptors\n", i);\r\nreturn i;\r\n}\r\nstatic void dwc_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nstruct dw_dma *dw = to_dw_dma(chan->device);\r\nstruct dw_desc *desc, *_desc;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\ndev_dbg(chan2dev(chan), "free_chan_resources (descs allocated=%u)\n",\r\ndwc->descs_allocated);\r\nBUG_ON(!list_empty(&dwc->active_list));\r\nBUG_ON(!list_empty(&dwc->queue));\r\nBUG_ON(dma_readl(to_dw_dma(chan->device), CH_EN) & dwc->mask);\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nlist_splice_init(&dwc->free_list, &list);\r\ndwc->descs_allocated = 0;\r\nchannel_clear_bit(dw, MASK.XFER, dwc->mask);\r\nchannel_clear_bit(dw, MASK.BLOCK, dwc->mask);\r\nchannel_clear_bit(dw, MASK.ERROR, dwc->mask);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &list, desc_node) {\r\ndev_vdbg(chan2dev(chan), " freeing descriptor %p\n", desc);\r\ndma_unmap_single(chan2parent(chan), desc->txd.phys,\r\nsizeof(desc->lli), DMA_TO_DEVICE);\r\nkfree(desc);\r\n}\r\ndev_vdbg(chan2dev(chan), "free_chan_resources done\n");\r\n}\r\nint dw_dma_cyclic_start(struct dma_chan *chan)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\r\nunsigned long flags;\r\nif (!test_bit(DW_DMA_IS_CYCLIC, &dwc->flags)) {\r\ndev_err(chan2dev(&dwc->chan), "missing prep for cyclic DMA\n");\r\nreturn -ENODEV;\r\n}\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nif (dma_readl(dw, CH_EN) & dwc->mask) {\r\ndev_err(chan2dev(&dwc->chan),\r\n"BUG: Attempted to start non-idle channel\n");\r\ndev_err(chan2dev(&dwc->chan),\r\n" SAR: 0x%x DAR: 0x%x LLP: 0x%x CTL: 0x%x:%08x\n",\r\nchannel_readl(dwc, SAR),\r\nchannel_readl(dwc, DAR),\r\nchannel_readl(dwc, LLP),\r\nchannel_readl(dwc, CTL_HI),\r\nchannel_readl(dwc, CTL_LO));\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nreturn -EBUSY;\r\n}\r\ndma_writel(dw, CLEAR.BLOCK, dwc->mask);\r\ndma_writel(dw, CLEAR.ERROR, dwc->mask);\r\ndma_writel(dw, CLEAR.XFER, dwc->mask);\r\nchannel_writel(dwc, LLP, dwc->cdesc->desc[0]->txd.phys);\r\nchannel_writel(dwc, CTL_LO, DWC_CTLL_LLP_D_EN | DWC_CTLL_LLP_S_EN);\r\nchannel_writel(dwc, CTL_HI, 0);\r\nchannel_set_bit(dw, CH_EN, dwc->mask);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nreturn 0;\r\n}\r\nvoid dw_dma_cyclic_stop(struct dma_chan *chan)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\r\nunsigned long flags;\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nchannel_clear_bit(dw, CH_EN, dwc->mask);\r\nwhile (dma_readl(dw, CH_EN) & dwc->mask)\r\ncpu_relax();\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\n}\r\nstruct dw_cyclic_desc *dw_dma_cyclic_prep(struct dma_chan *chan,\r\ndma_addr_t buf_addr, size_t buf_len, size_t period_len,\r\nenum dma_data_direction direction)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nstruct dw_cyclic_desc *cdesc;\r\nstruct dw_cyclic_desc *retval = NULL;\r\nstruct dw_desc *desc;\r\nstruct dw_desc *last = NULL;\r\nstruct dw_dma_slave *dws = chan->private;\r\nunsigned long was_cyclic;\r\nunsigned int reg_width;\r\nunsigned int periods;\r\nunsigned int i;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nif (!list_empty(&dwc->queue) || !list_empty(&dwc->active_list)) {\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\ndev_dbg(chan2dev(&dwc->chan),\r\n"queue and/or active list are not empty\n");\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\nwas_cyclic = test_and_set_bit(DW_DMA_IS_CYCLIC, &dwc->flags);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nif (was_cyclic) {\r\ndev_dbg(chan2dev(&dwc->chan),\r\n"channel already prepared for cyclic DMA\n");\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\nretval = ERR_PTR(-EINVAL);\r\nreg_width = dws->reg_width;\r\nperiods = buf_len / period_len;\r\nif (period_len > (DWC_MAX_COUNT << reg_width))\r\ngoto out_err;\r\nif (unlikely(period_len & ((1 << reg_width) - 1)))\r\ngoto out_err;\r\nif (unlikely(buf_addr & ((1 << reg_width) - 1)))\r\ngoto out_err;\r\nif (unlikely(!(direction & (DMA_TO_DEVICE | DMA_FROM_DEVICE))))\r\ngoto out_err;\r\nretval = ERR_PTR(-ENOMEM);\r\nif (periods > NR_DESCS_PER_CHANNEL)\r\ngoto out_err;\r\ncdesc = kzalloc(sizeof(struct dw_cyclic_desc), GFP_KERNEL);\r\nif (!cdesc)\r\ngoto out_err;\r\ncdesc->desc = kzalloc(sizeof(struct dw_desc *) * periods, GFP_KERNEL);\r\nif (!cdesc->desc)\r\ngoto out_err_alloc;\r\nfor (i = 0; i < periods; i++) {\r\ndesc = dwc_desc_get(dwc);\r\nif (!desc)\r\ngoto out_err_desc_get;\r\nswitch (direction) {\r\ncase DMA_TO_DEVICE:\r\ndesc->lli.dar = dws->tx_reg;\r\ndesc->lli.sar = buf_addr + (period_len * i);\r\ndesc->lli.ctllo = (DWC_DEFAULT_CTLLO(chan->private)\r\n| DWC_CTLL_DST_WIDTH(reg_width)\r\n| DWC_CTLL_SRC_WIDTH(reg_width)\r\n| DWC_CTLL_DST_FIX\r\n| DWC_CTLL_SRC_INC\r\n| DWC_CTLL_FC(dws->fc)\r\n| DWC_CTLL_INT_EN);\r\nbreak;\r\ncase DMA_FROM_DEVICE:\r\ndesc->lli.dar = buf_addr + (period_len * i);\r\ndesc->lli.sar = dws->rx_reg;\r\ndesc->lli.ctllo = (DWC_DEFAULT_CTLLO(chan->private)\r\n| DWC_CTLL_SRC_WIDTH(reg_width)\r\n| DWC_CTLL_DST_WIDTH(reg_width)\r\n| DWC_CTLL_DST_INC\r\n| DWC_CTLL_SRC_FIX\r\n| DWC_CTLL_FC(dws->fc)\r\n| DWC_CTLL_INT_EN);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\ndesc->lli.ctlhi = (period_len >> reg_width);\r\ncdesc->desc[i] = desc;\r\nif (last) {\r\nlast->lli.llp = desc->txd.phys;\r\ndma_sync_single_for_device(chan2parent(chan),\r\nlast->txd.phys, sizeof(last->lli),\r\nDMA_TO_DEVICE);\r\n}\r\nlast = desc;\r\n}\r\nlast->lli.llp = cdesc->desc[0]->txd.phys;\r\ndma_sync_single_for_device(chan2parent(chan), last->txd.phys,\r\nsizeof(last->lli), DMA_TO_DEVICE);\r\ndev_dbg(chan2dev(&dwc->chan), "cyclic prepared buf 0x%08x len %zu "\r\n"period %zu periods %d\n", buf_addr, buf_len,\r\nperiod_len, periods);\r\ncdesc->periods = periods;\r\ndwc->cdesc = cdesc;\r\nreturn cdesc;\r\nout_err_desc_get:\r\nwhile (i--)\r\ndwc_desc_put(dwc, cdesc->desc[i]);\r\nout_err_alloc:\r\nkfree(cdesc);\r\nout_err:\r\nclear_bit(DW_DMA_IS_CYCLIC, &dwc->flags);\r\nreturn (struct dw_cyclic_desc *)retval;\r\n}\r\nvoid dw_dma_cyclic_free(struct dma_chan *chan)\r\n{\r\nstruct dw_dma_chan *dwc = to_dw_dma_chan(chan);\r\nstruct dw_dma *dw = to_dw_dma(dwc->chan.device);\r\nstruct dw_cyclic_desc *cdesc = dwc->cdesc;\r\nint i;\r\nunsigned long flags;\r\ndev_dbg(chan2dev(&dwc->chan), "cyclic free\n");\r\nif (!cdesc)\r\nreturn;\r\nspin_lock_irqsave(&dwc->lock, flags);\r\nchannel_clear_bit(dw, CH_EN, dwc->mask);\r\nwhile (dma_readl(dw, CH_EN) & dwc->mask)\r\ncpu_relax();\r\ndma_writel(dw, CLEAR.BLOCK, dwc->mask);\r\ndma_writel(dw, CLEAR.ERROR, dwc->mask);\r\ndma_writel(dw, CLEAR.XFER, dwc->mask);\r\nspin_unlock_irqrestore(&dwc->lock, flags);\r\nfor (i = 0; i < cdesc->periods; i++)\r\ndwc_desc_put(dwc, cdesc->desc[i]);\r\nkfree(cdesc->desc);\r\nkfree(cdesc);\r\nclear_bit(DW_DMA_IS_CYCLIC, &dwc->flags);\r\n}\r\nstatic void dw_dma_off(struct dw_dma *dw)\r\n{\r\ndma_writel(dw, CFG, 0);\r\nchannel_clear_bit(dw, MASK.XFER, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.BLOCK, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.SRC_TRAN, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.DST_TRAN, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.ERROR, dw->all_chan_mask);\r\nwhile (dma_readl(dw, CFG) & DW_CFG_DMA_EN)\r\ncpu_relax();\r\n}\r\nstatic int __init dw_probe(struct platform_device *pdev)\r\n{\r\nstruct dw_dma_platform_data *pdata;\r\nstruct resource *io;\r\nstruct dw_dma *dw;\r\nsize_t size;\r\nint irq;\r\nint err;\r\nint i;\r\npdata = pdev->dev.platform_data;\r\nif (!pdata || pdata->nr_channels > DW_DMA_MAX_NR_CHANNELS)\r\nreturn -EINVAL;\r\nio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!io)\r\nreturn -EINVAL;\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0)\r\nreturn irq;\r\nsize = sizeof(struct dw_dma);\r\nsize += pdata->nr_channels * sizeof(struct dw_dma_chan);\r\ndw = kzalloc(size, GFP_KERNEL);\r\nif (!dw)\r\nreturn -ENOMEM;\r\nif (!request_mem_region(io->start, DW_REGLEN, pdev->dev.driver->name)) {\r\nerr = -EBUSY;\r\ngoto err_kfree;\r\n}\r\ndw->regs = ioremap(io->start, DW_REGLEN);\r\nif (!dw->regs) {\r\nerr = -ENOMEM;\r\ngoto err_release_r;\r\n}\r\ndw->clk = clk_get(&pdev->dev, "hclk");\r\nif (IS_ERR(dw->clk)) {\r\nerr = PTR_ERR(dw->clk);\r\ngoto err_clk;\r\n}\r\nclk_enable(dw->clk);\r\ndw_dma_off(dw);\r\nerr = request_irq(irq, dw_dma_interrupt, 0, "dw_dmac", dw);\r\nif (err)\r\ngoto err_irq;\r\nplatform_set_drvdata(pdev, dw);\r\ntasklet_init(&dw->tasklet, dw_dma_tasklet, (unsigned long)dw);\r\ndw->all_chan_mask = (1 << pdata->nr_channels) - 1;\r\nINIT_LIST_HEAD(&dw->dma.channels);\r\nfor (i = 0; i < pdata->nr_channels; i++, dw->dma.chancnt++) {\r\nstruct dw_dma_chan *dwc = &dw->chan[i];\r\ndwc->chan.device = &dw->dma;\r\ndwc->chan.cookie = dwc->completed = 1;\r\ndwc->chan.chan_id = i;\r\nif (pdata->chan_allocation_order == CHAN_ALLOCATION_ASCENDING)\r\nlist_add_tail(&dwc->chan.device_node,\r\n&dw->dma.channels);\r\nelse\r\nlist_add(&dwc->chan.device_node, &dw->dma.channels);\r\nif (pdata->chan_priority == CHAN_PRIORITY_ASCENDING)\r\ndwc->priority = 7 - i;\r\nelse\r\ndwc->priority = i;\r\ndwc->ch_regs = &__dw_regs(dw)->CHAN[i];\r\nspin_lock_init(&dwc->lock);\r\ndwc->mask = 1 << i;\r\nINIT_LIST_HEAD(&dwc->active_list);\r\nINIT_LIST_HEAD(&dwc->queue);\r\nINIT_LIST_HEAD(&dwc->free_list);\r\nchannel_clear_bit(dw, CH_EN, dwc->mask);\r\n}\r\ndma_writel(dw, CLEAR.XFER, dw->all_chan_mask);\r\ndma_writel(dw, CLEAR.BLOCK, dw->all_chan_mask);\r\ndma_writel(dw, CLEAR.SRC_TRAN, dw->all_chan_mask);\r\ndma_writel(dw, CLEAR.DST_TRAN, dw->all_chan_mask);\r\ndma_writel(dw, CLEAR.ERROR, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.XFER, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.BLOCK, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.SRC_TRAN, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.DST_TRAN, dw->all_chan_mask);\r\nchannel_clear_bit(dw, MASK.ERROR, dw->all_chan_mask);\r\ndma_cap_set(DMA_MEMCPY, dw->dma.cap_mask);\r\ndma_cap_set(DMA_SLAVE, dw->dma.cap_mask);\r\nif (pdata->is_private)\r\ndma_cap_set(DMA_PRIVATE, dw->dma.cap_mask);\r\ndw->dma.dev = &pdev->dev;\r\ndw->dma.device_alloc_chan_resources = dwc_alloc_chan_resources;\r\ndw->dma.device_free_chan_resources = dwc_free_chan_resources;\r\ndw->dma.device_prep_dma_memcpy = dwc_prep_dma_memcpy;\r\ndw->dma.device_prep_slave_sg = dwc_prep_slave_sg;\r\ndw->dma.device_control = dwc_control;\r\ndw->dma.device_tx_status = dwc_tx_status;\r\ndw->dma.device_issue_pending = dwc_issue_pending;\r\ndma_writel(dw, CFG, DW_CFG_DMA_EN);\r\nprintk(KERN_INFO "%s: DesignWare DMA Controller, %d channels\n",\r\ndev_name(&pdev->dev), dw->dma.chancnt);\r\ndma_async_device_register(&dw->dma);\r\nreturn 0;\r\nerr_irq:\r\nclk_disable(dw->clk);\r\nclk_put(dw->clk);\r\nerr_clk:\r\niounmap(dw->regs);\r\ndw->regs = NULL;\r\nerr_release_r:\r\nrelease_resource(io);\r\nerr_kfree:\r\nkfree(dw);\r\nreturn err;\r\n}\r\nstatic int __exit dw_remove(struct platform_device *pdev)\r\n{\r\nstruct dw_dma *dw = platform_get_drvdata(pdev);\r\nstruct dw_dma_chan *dwc, *_dwc;\r\nstruct resource *io;\r\ndw_dma_off(dw);\r\ndma_async_device_unregister(&dw->dma);\r\nfree_irq(platform_get_irq(pdev, 0), dw);\r\ntasklet_kill(&dw->tasklet);\r\nlist_for_each_entry_safe(dwc, _dwc, &dw->dma.channels,\r\nchan.device_node) {\r\nlist_del(&dwc->chan.device_node);\r\nchannel_clear_bit(dw, CH_EN, dwc->mask);\r\n}\r\nclk_disable(dw->clk);\r\nclk_put(dw->clk);\r\niounmap(dw->regs);\r\ndw->regs = NULL;\r\nio = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nrelease_mem_region(io->start, DW_REGLEN);\r\nkfree(dw);\r\nreturn 0;\r\n}\r\nstatic void dw_shutdown(struct platform_device *pdev)\r\n{\r\nstruct dw_dma *dw = platform_get_drvdata(pdev);\r\ndw_dma_off(platform_get_drvdata(pdev));\r\nclk_disable(dw->clk);\r\n}\r\nstatic int dw_suspend_noirq(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct dw_dma *dw = platform_get_drvdata(pdev);\r\ndw_dma_off(platform_get_drvdata(pdev));\r\nclk_disable(dw->clk);\r\nreturn 0;\r\n}\r\nstatic int dw_resume_noirq(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct dw_dma *dw = platform_get_drvdata(pdev);\r\nclk_enable(dw->clk);\r\ndma_writel(dw, CFG, DW_CFG_DMA_EN);\r\nreturn 0;\r\n}\r\nstatic int __init dw_init(void)\r\n{\r\nreturn platform_driver_probe(&dw_driver, dw_probe);\r\n}\r\nstatic void __exit dw_exit(void)\r\n{\r\nplatform_driver_unregister(&dw_driver);\r\n}
