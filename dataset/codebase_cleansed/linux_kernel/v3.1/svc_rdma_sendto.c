static int fast_reg_xdr(struct svcxprt_rdma *xprt,\r\nstruct xdr_buf *xdr,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nint sge_no;\r\nu32 sge_bytes;\r\nu32 page_bytes;\r\nu32 page_off;\r\nint page_no = 0;\r\nu8 *frva;\r\nstruct svc_rdma_fastreg_mr *frmr;\r\nfrmr = svc_rdma_get_frmr(xprt);\r\nif (IS_ERR(frmr))\r\nreturn -ENOMEM;\r\nvec->frmr = frmr;\r\nsge_no = 1;\r\nfrva = (void *)((unsigned long)(xdr->head[0].iov_base) & PAGE_MASK);\r\nvec->sge[sge_no].iov_base = xdr->head[0].iov_base;\r\nvec->sge[sge_no].iov_len = xdr->head[0].iov_len;\r\nvec->count = 2;\r\nsge_no++;\r\nfrmr->kva = frva;\r\nfrmr->direction = DMA_TO_DEVICE;\r\nfrmr->access_flags = 0;\r\nfrmr->map_len = PAGE_SIZE;\r\nfrmr->page_list_len = 1;\r\npage_off = (unsigned long)xdr->head[0].iov_base & ~PAGE_MASK;\r\nfrmr->page_list->page_list[page_no] =\r\nib_dma_map_page(xprt->sc_cm_id->device,\r\nvirt_to_page(xdr->head[0].iov_base),\r\npage_off,\r\nPAGE_SIZE - page_off,\r\nDMA_TO_DEVICE);\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nfrmr->page_list->page_list[page_no]))\r\ngoto fatal_err;\r\natomic_inc(&xprt->sc_dma_used);\r\npage_off = xdr->page_base;\r\npage_bytes = xdr->page_len + page_off;\r\nif (!page_bytes)\r\ngoto encode_tail;\r\nvec->sge[sge_no].iov_base = frva + frmr->map_len + page_off;\r\nvec->sge[sge_no].iov_len = page_bytes;\r\nsge_no++;\r\nwhile (page_bytes) {\r\nstruct page *page;\r\npage = xdr->pages[page_no++];\r\nsge_bytes = min_t(u32, page_bytes, (PAGE_SIZE - page_off));\r\npage_bytes -= sge_bytes;\r\nfrmr->page_list->page_list[page_no] =\r\nib_dma_map_page(xprt->sc_cm_id->device,\r\npage, page_off,\r\nsge_bytes, DMA_TO_DEVICE);\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nfrmr->page_list->page_list[page_no]))\r\ngoto fatal_err;\r\natomic_inc(&xprt->sc_dma_used);\r\npage_off = 0;\r\nfrmr->map_len += PAGE_SIZE;\r\nfrmr->page_list_len++;\r\n}\r\nvec->count++;\r\nencode_tail:\r\nif (0 == xdr->tail[0].iov_len)\r\ngoto done;\r\nvec->count++;\r\nvec->sge[sge_no].iov_len = xdr->tail[0].iov_len;\r\nif (((unsigned long)xdr->tail[0].iov_base & PAGE_MASK) ==\r\n((unsigned long)xdr->head[0].iov_base & PAGE_MASK)) {\r\nvec->sge[sge_no].iov_base = xdr->tail[0].iov_base;\r\n} else {\r\nvoid *va;\r\npage_off = (unsigned long)xdr->tail[0].iov_base & ~PAGE_MASK;\r\nva = (void *)((unsigned long)xdr->tail[0].iov_base & PAGE_MASK);\r\nvec->sge[sge_no].iov_base = frva + frmr->map_len + page_off;\r\nfrmr->page_list->page_list[page_no] =\r\nib_dma_map_page(xprt->sc_cm_id->device, virt_to_page(va),\r\npage_off,\r\nPAGE_SIZE,\r\nDMA_TO_DEVICE);\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nfrmr->page_list->page_list[page_no]))\r\ngoto fatal_err;\r\natomic_inc(&xprt->sc_dma_used);\r\nfrmr->map_len += PAGE_SIZE;\r\nfrmr->page_list_len++;\r\n}\r\ndone:\r\nif (svc_rdma_fastreg(xprt, frmr))\r\ngoto fatal_err;\r\nreturn 0;\r\nfatal_err:\r\nprintk("svcrdma: Error fast registering memory for xprt %p\n", xprt);\r\nvec->frmr = NULL;\r\nsvc_rdma_put_frmr(xprt, frmr);\r\nreturn -EIO;\r\n}\r\nstatic int map_xdr(struct svcxprt_rdma *xprt,\r\nstruct xdr_buf *xdr,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nint sge_no;\r\nu32 sge_bytes;\r\nu32 page_bytes;\r\nu32 page_off;\r\nint page_no;\r\nBUG_ON(xdr->len !=\r\n(xdr->head[0].iov_len + xdr->page_len + xdr->tail[0].iov_len));\r\nif (xprt->sc_frmr_pg_list_len)\r\nreturn fast_reg_xdr(xprt, xdr, vec);\r\nsge_no = 1;\r\nvec->sge[sge_no].iov_base = xdr->head[0].iov_base;\r\nvec->sge[sge_no].iov_len = xdr->head[0].iov_len;\r\nsge_no++;\r\npage_no = 0;\r\npage_bytes = xdr->page_len;\r\npage_off = xdr->page_base;\r\nwhile (page_bytes) {\r\nvec->sge[sge_no].iov_base =\r\npage_address(xdr->pages[page_no]) + page_off;\r\nsge_bytes = min_t(u32, page_bytes, (PAGE_SIZE - page_off));\r\npage_bytes -= sge_bytes;\r\nvec->sge[sge_no].iov_len = sge_bytes;\r\nsge_no++;\r\npage_no++;\r\npage_off = 0;\r\n}\r\nif (xdr->tail[0].iov_len) {\r\nvec->sge[sge_no].iov_base = xdr->tail[0].iov_base;\r\nvec->sge[sge_no].iov_len = xdr->tail[0].iov_len;\r\nsge_no++;\r\n}\r\ndprintk("svcrdma: map_xdr: sge_no %d page_no %d "\r\n"page_base %u page_len %u head_len %zu tail_len %zu\n",\r\nsge_no, page_no, xdr->page_base, xdr->page_len,\r\nxdr->head[0].iov_len, xdr->tail[0].iov_len);\r\nvec->count = sge_no;\r\nreturn 0;\r\n}\r\nstatic dma_addr_t dma_map_xdr(struct svcxprt_rdma *xprt,\r\nstruct xdr_buf *xdr,\r\nu32 xdr_off, size_t len, int dir)\r\n{\r\nstruct page *page;\r\ndma_addr_t dma_addr;\r\nif (xdr_off < xdr->head[0].iov_len) {\r\nxdr_off += (unsigned long)xdr->head[0].iov_base & ~PAGE_MASK;\r\npage = virt_to_page(xdr->head[0].iov_base);\r\n} else {\r\nxdr_off -= xdr->head[0].iov_len;\r\nif (xdr_off < xdr->page_len) {\r\npage = xdr->pages[xdr_off >> PAGE_SHIFT];\r\nxdr_off &= ~PAGE_MASK;\r\n} else {\r\nxdr_off -= xdr->page_len;\r\nxdr_off += (unsigned long)\r\nxdr->tail[0].iov_base & ~PAGE_MASK;\r\npage = virt_to_page(xdr->tail[0].iov_base);\r\n}\r\n}\r\ndma_addr = ib_dma_map_page(xprt->sc_cm_id->device, page, xdr_off,\r\nmin_t(size_t, PAGE_SIZE, len), dir);\r\nreturn dma_addr;\r\n}\r\nstatic int send_write(struct svcxprt_rdma *xprt, struct svc_rqst *rqstp,\r\nu32 rmr, u64 to,\r\nu32 xdr_off, int write_len,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nstruct ib_send_wr write_wr;\r\nstruct ib_sge *sge;\r\nint xdr_sge_no;\r\nint sge_no;\r\nint sge_bytes;\r\nint sge_off;\r\nint bc;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nBUG_ON(vec->count > RPCSVC_MAXPAGES);\r\ndprintk("svcrdma: RDMA_WRITE rmr=%x, to=%llx, xdr_off=%d, "\r\n"write_len=%d, vec->sge=%p, vec->count=%lu\n",\r\nrmr, (unsigned long long)to, xdr_off,\r\nwrite_len, vec->sge, vec->count);\r\nctxt = svc_rdma_get_context(xprt);\r\nctxt->direction = DMA_TO_DEVICE;\r\nsge = ctxt->sge;\r\nfor (bc = xdr_off, xdr_sge_no = 1; bc && xdr_sge_no < vec->count;\r\nxdr_sge_no++) {\r\nif (vec->sge[xdr_sge_no].iov_len > bc)\r\nbreak;\r\nbc -= vec->sge[xdr_sge_no].iov_len;\r\n}\r\nsge_off = bc;\r\nbc = write_len;\r\nsge_no = 0;\r\nwhile (bc != 0) {\r\nsge_bytes = min_t(size_t,\r\nbc, vec->sge[xdr_sge_no].iov_len-sge_off);\r\nsge[sge_no].length = sge_bytes;\r\nif (!vec->frmr) {\r\nsge[sge_no].addr =\r\ndma_map_xdr(xprt, &rqstp->rq_res, xdr_off,\r\nsge_bytes, DMA_TO_DEVICE);\r\nxdr_off += sge_bytes;\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nsge[sge_no].addr))\r\ngoto err;\r\natomic_inc(&xprt->sc_dma_used);\r\nsge[sge_no].lkey = xprt->sc_dma_lkey;\r\n} else {\r\nsge[sge_no].addr = (unsigned long)\r\nvec->sge[xdr_sge_no].iov_base + sge_off;\r\nsge[sge_no].lkey = vec->frmr->mr->lkey;\r\n}\r\nctxt->count++;\r\nctxt->frmr = vec->frmr;\r\nsge_off = 0;\r\nsge_no++;\r\nxdr_sge_no++;\r\nBUG_ON(xdr_sge_no > vec->count);\r\nbc -= sge_bytes;\r\n}\r\nmemset(&write_wr, 0, sizeof write_wr);\r\nctxt->wr_op = IB_WR_RDMA_WRITE;\r\nwrite_wr.wr_id = (unsigned long)ctxt;\r\nwrite_wr.sg_list = &sge[0];\r\nwrite_wr.num_sge = sge_no;\r\nwrite_wr.opcode = IB_WR_RDMA_WRITE;\r\nwrite_wr.send_flags = IB_SEND_SIGNALED;\r\nwrite_wr.wr.rdma.rkey = rmr;\r\nwrite_wr.wr.rdma.remote_addr = to;\r\natomic_inc(&rdma_stat_write);\r\nif (svc_rdma_send(xprt, &write_wr))\r\ngoto err;\r\nreturn 0;\r\nerr:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_frmr(xprt, vec->frmr);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn -EIO;\r\n}\r\nstatic int send_write_chunks(struct svcxprt_rdma *xprt,\r\nstruct rpcrdma_msg *rdma_argp,\r\nstruct rpcrdma_msg *rdma_resp,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nu32 xfer_len = rqstp->rq_res.page_len + rqstp->rq_res.tail[0].iov_len;\r\nint write_len;\r\nint max_write;\r\nu32 xdr_off;\r\nint chunk_off;\r\nint chunk_no;\r\nstruct rpcrdma_write_array *arg_ary;\r\nstruct rpcrdma_write_array *res_ary;\r\nint ret;\r\narg_ary = svc_rdma_get_write_array(rdma_argp);\r\nif (!arg_ary)\r\nreturn 0;\r\nres_ary = (struct rpcrdma_write_array *)\r\n&rdma_resp->rm_body.rm_chunks[1];\r\nif (vec->frmr)\r\nmax_write = vec->frmr->map_len;\r\nelse\r\nmax_write = xprt->sc_max_sge * PAGE_SIZE;\r\nfor (xdr_off = rqstp->rq_res.head[0].iov_len, chunk_no = 0;\r\nxfer_len && chunk_no < arg_ary->wc_nchunks;\r\nchunk_no++) {\r\nstruct rpcrdma_segment *arg_ch;\r\nu64 rs_offset;\r\narg_ch = &arg_ary->wc_array[chunk_no].wc_target;\r\nwrite_len = min(xfer_len, arg_ch->rs_length);\r\nrs_offset = get_unaligned(&(arg_ch->rs_offset));\r\nsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\r\narg_ch->rs_handle,\r\nrs_offset,\r\nwrite_len);\r\nchunk_off = 0;\r\nwhile (write_len) {\r\nint this_write;\r\nthis_write = min(write_len, max_write);\r\nret = send_write(xprt, rqstp,\r\narg_ch->rs_handle,\r\nrs_offset + chunk_off,\r\nxdr_off,\r\nthis_write,\r\nvec);\r\nif (ret) {\r\ndprintk("svcrdma: RDMA_WRITE failed, ret=%d\n",\r\nret);\r\nreturn -EIO;\r\n}\r\nchunk_off += this_write;\r\nxdr_off += this_write;\r\nxfer_len -= this_write;\r\nwrite_len -= this_write;\r\n}\r\n}\r\nsvc_rdma_xdr_encode_write_list(rdma_resp, chunk_no);\r\nreturn rqstp->rq_res.page_len + rqstp->rq_res.tail[0].iov_len;\r\n}\r\nstatic int send_reply_chunks(struct svcxprt_rdma *xprt,\r\nstruct rpcrdma_msg *rdma_argp,\r\nstruct rpcrdma_msg *rdma_resp,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nu32 xfer_len = rqstp->rq_res.len;\r\nint write_len;\r\nint max_write;\r\nu32 xdr_off;\r\nint chunk_no;\r\nint chunk_off;\r\nstruct rpcrdma_segment *ch;\r\nstruct rpcrdma_write_array *arg_ary;\r\nstruct rpcrdma_write_array *res_ary;\r\nint ret;\r\narg_ary = svc_rdma_get_reply_array(rdma_argp);\r\nif (!arg_ary)\r\nreturn 0;\r\nres_ary = (struct rpcrdma_write_array *)\r\n&rdma_resp->rm_body.rm_chunks[2];\r\nif (vec->frmr)\r\nmax_write = vec->frmr->map_len;\r\nelse\r\nmax_write = xprt->sc_max_sge * PAGE_SIZE;\r\nfor (xdr_off = 0, chunk_no = 0;\r\nxfer_len && chunk_no < arg_ary->wc_nchunks;\r\nchunk_no++) {\r\nu64 rs_offset;\r\nch = &arg_ary->wc_array[chunk_no].wc_target;\r\nwrite_len = min(xfer_len, ch->rs_length);\r\nrs_offset = get_unaligned(&(ch->rs_offset));\r\nsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\r\nch->rs_handle, rs_offset,\r\nwrite_len);\r\nchunk_off = 0;\r\nwhile (write_len) {\r\nint this_write;\r\nthis_write = min(write_len, max_write);\r\nret = send_write(xprt, rqstp,\r\nch->rs_handle,\r\nrs_offset + chunk_off,\r\nxdr_off,\r\nthis_write,\r\nvec);\r\nif (ret) {\r\ndprintk("svcrdma: RDMA_WRITE failed, ret=%d\n",\r\nret);\r\nreturn -EIO;\r\n}\r\nchunk_off += this_write;\r\nxdr_off += this_write;\r\nxfer_len -= this_write;\r\nwrite_len -= this_write;\r\n}\r\n}\r\nsvc_rdma_xdr_encode_reply_array(res_ary, chunk_no);\r\nreturn rqstp->rq_res.len;\r\n}\r\nstatic int send_reply(struct svcxprt_rdma *rdma,\r\nstruct svc_rqst *rqstp,\r\nstruct page *page,\r\nstruct rpcrdma_msg *rdma_resp,\r\nstruct svc_rdma_op_ctxt *ctxt,\r\nstruct svc_rdma_req_map *vec,\r\nint byte_count)\r\n{\r\nstruct ib_send_wr send_wr;\r\nstruct ib_send_wr inv_wr;\r\nint sge_no;\r\nint sge_bytes;\r\nint page_no;\r\nint ret;\r\nret = svc_rdma_post_recv(rdma);\r\nif (ret) {\r\nprintk(KERN_INFO\r\n"svcrdma: could not post a receive buffer, err=%d."\r\n"Closing transport %p.\n", ret, rdma);\r\nset_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);\r\nsvc_rdma_put_frmr(rdma, vec->frmr);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn -ENOTCONN;\r\n}\r\nctxt->pages[0] = page;\r\nctxt->count = 1;\r\nctxt->frmr = vec->frmr;\r\nif (vec->frmr)\r\nset_bit(RDMACTXT_F_FAST_UNREG, &ctxt->flags);\r\nelse\r\nclear_bit(RDMACTXT_F_FAST_UNREG, &ctxt->flags);\r\nctxt->sge[0].lkey = rdma->sc_dma_lkey;\r\nctxt->sge[0].length = svc_rdma_xdr_get_reply_hdr_len(rdma_resp);\r\nctxt->sge[0].addr =\r\nib_dma_map_page(rdma->sc_cm_id->device, page, 0,\r\nctxt->sge[0].length, DMA_TO_DEVICE);\r\nif (ib_dma_mapping_error(rdma->sc_cm_id->device, ctxt->sge[0].addr))\r\ngoto err;\r\natomic_inc(&rdma->sc_dma_used);\r\nctxt->direction = DMA_TO_DEVICE;\r\nfor (sge_no = 1; byte_count && sge_no < vec->count; sge_no++) {\r\nint xdr_off = 0;\r\nsge_bytes = min_t(size_t, vec->sge[sge_no].iov_len, byte_count);\r\nbyte_count -= sge_bytes;\r\nif (!vec->frmr) {\r\nctxt->sge[sge_no].addr =\r\ndma_map_xdr(rdma, &rqstp->rq_res, xdr_off,\r\nsge_bytes, DMA_TO_DEVICE);\r\nxdr_off += sge_bytes;\r\nif (ib_dma_mapping_error(rdma->sc_cm_id->device,\r\nctxt->sge[sge_no].addr))\r\ngoto err;\r\natomic_inc(&rdma->sc_dma_used);\r\nctxt->sge[sge_no].lkey = rdma->sc_dma_lkey;\r\n} else {\r\nctxt->sge[sge_no].addr = (unsigned long)\r\nvec->sge[sge_no].iov_base;\r\nctxt->sge[sge_no].lkey = vec->frmr->mr->lkey;\r\n}\r\nctxt->sge[sge_no].length = sge_bytes;\r\n}\r\nBUG_ON(byte_count != 0);\r\nfor (page_no = 0; page_no < rqstp->rq_resused; page_no++) {\r\nctxt->pages[page_no+1] = rqstp->rq_respages[page_no];\r\nctxt->count++;\r\nrqstp->rq_respages[page_no] = NULL;\r\nif (page_no+1 >= sge_no)\r\nctxt->sge[page_no+1].length = 0;\r\n}\r\nBUG_ON(sge_no > rdma->sc_max_sge);\r\nmemset(&send_wr, 0, sizeof send_wr);\r\nctxt->wr_op = IB_WR_SEND;\r\nsend_wr.wr_id = (unsigned long)ctxt;\r\nsend_wr.sg_list = ctxt->sge;\r\nsend_wr.num_sge = sge_no;\r\nsend_wr.opcode = IB_WR_SEND;\r\nsend_wr.send_flags = IB_SEND_SIGNALED;\r\nif (vec->frmr) {\r\nmemset(&inv_wr, 0, sizeof inv_wr);\r\ninv_wr.opcode = IB_WR_LOCAL_INV;\r\ninv_wr.send_flags = IB_SEND_SIGNALED;\r\ninv_wr.ex.invalidate_rkey =\r\nvec->frmr->mr->lkey;\r\nsend_wr.next = &inv_wr;\r\n}\r\nret = svc_rdma_send(rdma, &send_wr);\r\nif (ret)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_frmr(rdma, vec->frmr);\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn -EIO;\r\n}\r\nvoid svc_rdma_prep_reply_hdr(struct svc_rqst *rqstp)\r\n{\r\n}\r\nstatic void *xdr_start(struct xdr_buf *xdr)\r\n{\r\nreturn xdr->head[0].iov_base -\r\n(xdr->len -\r\nxdr->page_len -\r\nxdr->tail[0].iov_len -\r\nxdr->head[0].iov_len);\r\n}\r\nint svc_rdma_sendto(struct svc_rqst *rqstp)\r\n{\r\nstruct svc_xprt *xprt = rqstp->rq_xprt;\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nstruct rpcrdma_msg *rdma_argp;\r\nstruct rpcrdma_msg *rdma_resp;\r\nstruct rpcrdma_write_array *reply_ary;\r\nenum rpcrdma_proc reply_type;\r\nint ret;\r\nint inline_bytes;\r\nstruct page *res_page;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nstruct svc_rdma_req_map *vec;\r\ndprintk("svcrdma: sending response for rqstp=%p\n", rqstp);\r\nrdma_argp = xdr_start(&rqstp->rq_arg);\r\nctxt = svc_rdma_get_context(rdma);\r\nctxt->direction = DMA_TO_DEVICE;\r\nvec = svc_rdma_get_req_map();\r\nret = map_xdr(rdma, &rqstp->rq_res, vec);\r\nif (ret)\r\ngoto err0;\r\ninline_bytes = rqstp->rq_res.len;\r\nres_page = svc_rdma_get_page();\r\nrdma_resp = page_address(res_page);\r\nreply_ary = svc_rdma_get_reply_array(rdma_argp);\r\nif (reply_ary)\r\nreply_type = RDMA_NOMSG;\r\nelse\r\nreply_type = RDMA_MSG;\r\nsvc_rdma_xdr_encode_reply_header(rdma, rdma_argp,\r\nrdma_resp, reply_type);\r\nret = send_write_chunks(rdma, rdma_argp, rdma_resp,\r\nrqstp, vec);\r\nif (ret < 0) {\r\nprintk(KERN_ERR "svcrdma: failed to send write chunks, rc=%d\n",\r\nret);\r\ngoto err1;\r\n}\r\ninline_bytes -= ret;\r\nret = send_reply_chunks(rdma, rdma_argp, rdma_resp,\r\nrqstp, vec);\r\nif (ret < 0) {\r\nprintk(KERN_ERR "svcrdma: failed to send reply chunks, rc=%d\n",\r\nret);\r\ngoto err1;\r\n}\r\ninline_bytes -= ret;\r\nret = send_reply(rdma, rqstp, res_page, rdma_resp, ctxt, vec,\r\ninline_bytes);\r\nsvc_rdma_put_req_map(vec);\r\ndprintk("svcrdma: send_reply returns %d\n", ret);\r\nreturn ret;\r\nerr1:\r\nput_page(res_page);\r\nerr0:\r\nsvc_rdma_put_req_map(vec);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn ret;\r\n}
