void mlx4_srq_event(struct mlx4_dev *dev, u32 srqn, int event_type)\r\n{\r\nstruct mlx4_srq_table *srq_table = &mlx4_priv(dev)->srq_table;\r\nstruct mlx4_srq *srq;\r\nspin_lock(&srq_table->lock);\r\nsrq = radix_tree_lookup(&srq_table->tree, srqn & (dev->caps.num_srqs - 1));\r\nif (srq)\r\natomic_inc(&srq->refcount);\r\nspin_unlock(&srq_table->lock);\r\nif (!srq) {\r\nmlx4_warn(dev, "Async event for bogus SRQ %08x\n", srqn);\r\nreturn;\r\n}\r\nsrq->event(srq, event_type);\r\nif (atomic_dec_and_test(&srq->refcount))\r\ncomplete(&srq->free);\r\n}\r\nstatic int mlx4_SW2HW_SRQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,\r\nint srq_num)\r\n{\r\nreturn mlx4_cmd(dev, mailbox->dma, srq_num, 0, MLX4_CMD_SW2HW_SRQ,\r\nMLX4_CMD_TIME_CLASS_A);\r\n}\r\nstatic int mlx4_HW2SW_SRQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,\r\nint srq_num)\r\n{\r\nreturn mlx4_cmd_box(dev, 0, mailbox ? mailbox->dma : 0, srq_num,\r\nmailbox ? 0 : 1, MLX4_CMD_HW2SW_SRQ,\r\nMLX4_CMD_TIME_CLASS_A);\r\n}\r\nstatic int mlx4_ARM_SRQ(struct mlx4_dev *dev, int srq_num, int limit_watermark)\r\n{\r\nreturn mlx4_cmd(dev, limit_watermark, srq_num, 0, MLX4_CMD_ARM_SRQ,\r\nMLX4_CMD_TIME_CLASS_B);\r\n}\r\nstatic int mlx4_QUERY_SRQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,\r\nint srq_num)\r\n{\r\nreturn mlx4_cmd_box(dev, 0, mailbox->dma, srq_num, 0, MLX4_CMD_QUERY_SRQ,\r\nMLX4_CMD_TIME_CLASS_A);\r\n}\r\nint mlx4_srq_alloc(struct mlx4_dev *dev, u32 pdn, struct mlx4_mtt *mtt,\r\nu64 db_rec, struct mlx4_srq *srq)\r\n{\r\nstruct mlx4_srq_table *srq_table = &mlx4_priv(dev)->srq_table;\r\nstruct mlx4_cmd_mailbox *mailbox;\r\nstruct mlx4_srq_context *srq_context;\r\nu64 mtt_addr;\r\nint err;\r\nsrq->srqn = mlx4_bitmap_alloc(&srq_table->bitmap);\r\nif (srq->srqn == -1)\r\nreturn -ENOMEM;\r\nerr = mlx4_table_get(dev, &srq_table->table, srq->srqn);\r\nif (err)\r\ngoto err_out;\r\nerr = mlx4_table_get(dev, &srq_table->cmpt_table, srq->srqn);\r\nif (err)\r\ngoto err_put;\r\nspin_lock_irq(&srq_table->lock);\r\nerr = radix_tree_insert(&srq_table->tree, srq->srqn, srq);\r\nspin_unlock_irq(&srq_table->lock);\r\nif (err)\r\ngoto err_cmpt_put;\r\nmailbox = mlx4_alloc_cmd_mailbox(dev);\r\nif (IS_ERR(mailbox)) {\r\nerr = PTR_ERR(mailbox);\r\ngoto err_radix;\r\n}\r\nsrq_context = mailbox->buf;\r\nmemset(srq_context, 0, sizeof *srq_context);\r\nsrq_context->state_logsize_srqn = cpu_to_be32((ilog2(srq->max) << 24) |\r\nsrq->srqn);\r\nsrq_context->logstride = srq->wqe_shift - 4;\r\nsrq_context->log_page_size = mtt->page_shift - MLX4_ICM_PAGE_SHIFT;\r\nmtt_addr = mlx4_mtt_addr(dev, mtt);\r\nsrq_context->mtt_base_addr_h = mtt_addr >> 32;\r\nsrq_context->mtt_base_addr_l = cpu_to_be32(mtt_addr & 0xffffffff);\r\nsrq_context->pd = cpu_to_be32(pdn);\r\nsrq_context->db_rec_addr = cpu_to_be64(db_rec);\r\nerr = mlx4_SW2HW_SRQ(dev, mailbox, srq->srqn);\r\nmlx4_free_cmd_mailbox(dev, mailbox);\r\nif (err)\r\ngoto err_radix;\r\natomic_set(&srq->refcount, 1);\r\ninit_completion(&srq->free);\r\nreturn 0;\r\nerr_radix:\r\nspin_lock_irq(&srq_table->lock);\r\nradix_tree_delete(&srq_table->tree, srq->srqn);\r\nspin_unlock_irq(&srq_table->lock);\r\nerr_cmpt_put:\r\nmlx4_table_put(dev, &srq_table->cmpt_table, srq->srqn);\r\nerr_put:\r\nmlx4_table_put(dev, &srq_table->table, srq->srqn);\r\nerr_out:\r\nmlx4_bitmap_free(&srq_table->bitmap, srq->srqn);\r\nreturn err;\r\n}\r\nvoid mlx4_srq_free(struct mlx4_dev *dev, struct mlx4_srq *srq)\r\n{\r\nstruct mlx4_srq_table *srq_table = &mlx4_priv(dev)->srq_table;\r\nint err;\r\nerr = mlx4_HW2SW_SRQ(dev, NULL, srq->srqn);\r\nif (err)\r\nmlx4_warn(dev, "HW2SW_SRQ failed (%d) for SRQN %06x\n", err, srq->srqn);\r\nspin_lock_irq(&srq_table->lock);\r\nradix_tree_delete(&srq_table->tree, srq->srqn);\r\nspin_unlock_irq(&srq_table->lock);\r\nif (atomic_dec_and_test(&srq->refcount))\r\ncomplete(&srq->free);\r\nwait_for_completion(&srq->free);\r\nmlx4_table_put(dev, &srq_table->table, srq->srqn);\r\nmlx4_bitmap_free(&srq_table->bitmap, srq->srqn);\r\n}\r\nint mlx4_srq_arm(struct mlx4_dev *dev, struct mlx4_srq *srq, int limit_watermark)\r\n{\r\nreturn mlx4_ARM_SRQ(dev, srq->srqn, limit_watermark);\r\n}\r\nint mlx4_srq_query(struct mlx4_dev *dev, struct mlx4_srq *srq, int *limit_watermark)\r\n{\r\nstruct mlx4_cmd_mailbox *mailbox;\r\nstruct mlx4_srq_context *srq_context;\r\nint err;\r\nmailbox = mlx4_alloc_cmd_mailbox(dev);\r\nif (IS_ERR(mailbox))\r\nreturn PTR_ERR(mailbox);\r\nsrq_context = mailbox->buf;\r\nerr = mlx4_QUERY_SRQ(dev, mailbox, srq->srqn);\r\nif (err)\r\ngoto err_out;\r\n*limit_watermark = be16_to_cpu(srq_context->limit_watermark);\r\nerr_out:\r\nmlx4_free_cmd_mailbox(dev, mailbox);\r\nreturn err;\r\n}\r\nint mlx4_init_srq_table(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_srq_table *srq_table = &mlx4_priv(dev)->srq_table;\r\nint err;\r\nspin_lock_init(&srq_table->lock);\r\nINIT_RADIX_TREE(&srq_table->tree, GFP_ATOMIC);\r\nerr = mlx4_bitmap_init(&srq_table->bitmap, dev->caps.num_srqs,\r\ndev->caps.num_srqs - 1, dev->caps.reserved_srqs, 0);\r\nif (err)\r\nreturn err;\r\nreturn 0;\r\n}\r\nvoid mlx4_cleanup_srq_table(struct mlx4_dev *dev)\r\n{\r\nmlx4_bitmap_cleanup(&mlx4_priv(dev)->srq_table.bitmap);\r\n}
