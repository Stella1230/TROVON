static struct ip_vs_dest_set_elem *\r\nip_vs_dest_set_insert(struct ip_vs_dest_set *set, struct ip_vs_dest *dest)\r\n{\r\nstruct ip_vs_dest_set_elem *e;\r\nlist_for_each_entry(e, &set->list, list) {\r\nif (e->dest == dest)\r\nreturn NULL;\r\n}\r\ne = kmalloc(sizeof(*e), GFP_ATOMIC);\r\nif (e == NULL) {\r\npr_err("%s(): no memory\n", __func__);\r\nreturn NULL;\r\n}\r\natomic_inc(&dest->refcnt);\r\ne->dest = dest;\r\nlist_add(&e->list, &set->list);\r\natomic_inc(&set->size);\r\nset->lastmod = jiffies;\r\nreturn e;\r\n}\r\nstatic void\r\nip_vs_dest_set_erase(struct ip_vs_dest_set *set, struct ip_vs_dest *dest)\r\n{\r\nstruct ip_vs_dest_set_elem *e;\r\nlist_for_each_entry(e, &set->list, list) {\r\nif (e->dest == dest) {\r\natomic_dec(&set->size);\r\nset->lastmod = jiffies;\r\natomic_dec(&e->dest->refcnt);\r\nlist_del(&e->list);\r\nkfree(e);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void ip_vs_dest_set_eraseall(struct ip_vs_dest_set *set)\r\n{\r\nstruct ip_vs_dest_set_elem *e, *ep;\r\nwrite_lock(&set->lock);\r\nlist_for_each_entry_safe(e, ep, &set->list, list) {\r\natomic_dec(&e->dest->refcnt);\r\nlist_del(&e->list);\r\nkfree(e);\r\n}\r\nwrite_unlock(&set->lock);\r\n}\r\nstatic inline struct ip_vs_dest *ip_vs_dest_set_min(struct ip_vs_dest_set *set)\r\n{\r\nregister struct ip_vs_dest_set_elem *e;\r\nstruct ip_vs_dest *dest, *least;\r\nint loh, doh;\r\nif (set == NULL)\r\nreturn NULL;\r\nlist_for_each_entry(e, &set->list, list) {\r\nleast = e->dest;\r\nif (least->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\nif ((atomic_read(&least->weight) > 0)\r\n&& (least->flags & IP_VS_DEST_F_AVAILABLE)) {\r\nloh = ip_vs_dest_conn_overhead(least);\r\ngoto nextstage;\r\n}\r\n}\r\nreturn NULL;\r\nnextstage:\r\nlist_for_each_entry(e, &set->list, list) {\r\ndest = e->dest;\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\ndoh = ip_vs_dest_conn_overhead(dest);\r\nif ((loh * atomic_read(&dest->weight) >\r\ndoh * atomic_read(&least->weight))\r\n&& (dest->flags & IP_VS_DEST_F_AVAILABLE)) {\r\nleast = dest;\r\nloh = doh;\r\n}\r\n}\r\nIP_VS_DBG_BUF(6, "%s(): server %s:%d "\r\n"activeconns %d refcnt %d weight %d overhead %d\n",\r\n__func__,\r\nIP_VS_DBG_ADDR(least->af, &least->addr),\r\nntohs(least->port),\r\natomic_read(&least->activeconns),\r\natomic_read(&least->refcnt),\r\natomic_read(&least->weight), loh);\r\nreturn least;\r\n}\r\nstatic inline struct ip_vs_dest *ip_vs_dest_set_max(struct ip_vs_dest_set *set)\r\n{\r\nregister struct ip_vs_dest_set_elem *e;\r\nstruct ip_vs_dest *dest, *most;\r\nint moh, doh;\r\nif (set == NULL)\r\nreturn NULL;\r\nlist_for_each_entry(e, &set->list, list) {\r\nmost = e->dest;\r\nif (atomic_read(&most->weight) > 0) {\r\nmoh = ip_vs_dest_conn_overhead(most);\r\ngoto nextstage;\r\n}\r\n}\r\nreturn NULL;\r\nnextstage:\r\nlist_for_each_entry(e, &set->list, list) {\r\ndest = e->dest;\r\ndoh = ip_vs_dest_conn_overhead(dest);\r\nif ((moh * atomic_read(&dest->weight) <\r\ndoh * atomic_read(&most->weight))\r\n&& (atomic_read(&dest->weight) > 0)) {\r\nmost = dest;\r\nmoh = doh;\r\n}\r\n}\r\nIP_VS_DBG_BUF(6, "%s(): server %s:%d "\r\n"activeconns %d refcnt %d weight %d overhead %d\n",\r\n__func__,\r\nIP_VS_DBG_ADDR(most->af, &most->addr), ntohs(most->port),\r\natomic_read(&most->activeconns),\r\natomic_read(&most->refcnt),\r\natomic_read(&most->weight), moh);\r\nreturn most;\r\n}\r\nstatic inline void ip_vs_lblcr_free(struct ip_vs_lblcr_entry *en)\r\n{\r\nlist_del(&en->list);\r\nip_vs_dest_set_eraseall(&en->set);\r\nkfree(en);\r\n}\r\nstatic inline unsigned\r\nip_vs_lblcr_hashkey(int af, const union nf_inet_addr *addr)\r\n{\r\n__be32 addr_fold = addr->ip;\r\n#ifdef CONFIG_IP_VS_IPV6\r\nif (af == AF_INET6)\r\naddr_fold = addr->ip6[0]^addr->ip6[1]^\r\naddr->ip6[2]^addr->ip6[3];\r\n#endif\r\nreturn (ntohl(addr_fold)*2654435761UL) & IP_VS_LBLCR_TAB_MASK;\r\n}\r\nstatic void\r\nip_vs_lblcr_hash(struct ip_vs_lblcr_table *tbl, struct ip_vs_lblcr_entry *en)\r\n{\r\nunsigned hash = ip_vs_lblcr_hashkey(en->af, &en->addr);\r\nlist_add(&en->list, &tbl->bucket[hash]);\r\natomic_inc(&tbl->entries);\r\n}\r\nstatic inline struct ip_vs_lblcr_entry *\r\nip_vs_lblcr_get(int af, struct ip_vs_lblcr_table *tbl,\r\nconst union nf_inet_addr *addr)\r\n{\r\nunsigned hash = ip_vs_lblcr_hashkey(af, addr);\r\nstruct ip_vs_lblcr_entry *en;\r\nlist_for_each_entry(en, &tbl->bucket[hash], list)\r\nif (ip_vs_addr_equal(af, &en->addr, addr))\r\nreturn en;\r\nreturn NULL;\r\n}\r\nstatic inline struct ip_vs_lblcr_entry *\r\nip_vs_lblcr_new(struct ip_vs_lblcr_table *tbl, const union nf_inet_addr *daddr,\r\nstruct ip_vs_dest *dest)\r\n{\r\nstruct ip_vs_lblcr_entry *en;\r\nen = ip_vs_lblcr_get(dest->af, tbl, daddr);\r\nif (!en) {\r\nen = kmalloc(sizeof(*en), GFP_ATOMIC);\r\nif (!en) {\r\npr_err("%s(): no memory\n", __func__);\r\nreturn NULL;\r\n}\r\nen->af = dest->af;\r\nip_vs_addr_copy(dest->af, &en->addr, daddr);\r\nen->lastuse = jiffies;\r\natomic_set(&(en->set.size), 0);\r\nINIT_LIST_HEAD(&en->set.list);\r\nrwlock_init(&en->set.lock);\r\nip_vs_lblcr_hash(tbl, en);\r\n}\r\nwrite_lock(&en->set.lock);\r\nip_vs_dest_set_insert(&en->set, dest);\r\nwrite_unlock(&en->set.lock);\r\nreturn en;\r\n}\r\nstatic void ip_vs_lblcr_flush(struct ip_vs_lblcr_table *tbl)\r\n{\r\nint i;\r\nstruct ip_vs_lblcr_entry *en, *nxt;\r\nfor (i=0; i<IP_VS_LBLCR_TAB_SIZE; i++) {\r\nlist_for_each_entry_safe(en, nxt, &tbl->bucket[i], list) {\r\nip_vs_lblcr_free(en);\r\n}\r\n}\r\n}\r\nstatic int sysctl_lblcr_expiration(struct ip_vs_service *svc)\r\n{\r\n#ifdef CONFIG_SYSCTL\r\nstruct netns_ipvs *ipvs = net_ipvs(svc->net);\r\nreturn ipvs->sysctl_lblcr_expiration;\r\n#else\r\nreturn DEFAULT_EXPIRATION;\r\n#endif\r\n}\r\nstatic inline void ip_vs_lblcr_full_check(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblcr_table *tbl = svc->sched_data;\r\nunsigned long now = jiffies;\r\nint i, j;\r\nstruct ip_vs_lblcr_entry *en, *nxt;\r\nfor (i=0, j=tbl->rover; i<IP_VS_LBLCR_TAB_SIZE; i++) {\r\nj = (j + 1) & IP_VS_LBLCR_TAB_MASK;\r\nwrite_lock(&svc->sched_lock);\r\nlist_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {\r\nif (time_after(en->lastuse +\r\nsysctl_lblcr_expiration(svc), now))\r\ncontinue;\r\nip_vs_lblcr_free(en);\r\natomic_dec(&tbl->entries);\r\n}\r\nwrite_unlock(&svc->sched_lock);\r\n}\r\ntbl->rover = j;\r\n}\r\nstatic void ip_vs_lblcr_check_expire(unsigned long data)\r\n{\r\nstruct ip_vs_service *svc = (struct ip_vs_service *) data;\r\nstruct ip_vs_lblcr_table *tbl = svc->sched_data;\r\nunsigned long now = jiffies;\r\nint goal;\r\nint i, j;\r\nstruct ip_vs_lblcr_entry *en, *nxt;\r\nif ((tbl->counter % COUNT_FOR_FULL_EXPIRATION) == 0) {\r\nip_vs_lblcr_full_check(svc);\r\ntbl->counter = 1;\r\ngoto out;\r\n}\r\nif (atomic_read(&tbl->entries) <= tbl->max_size) {\r\ntbl->counter++;\r\ngoto out;\r\n}\r\ngoal = (atomic_read(&tbl->entries) - tbl->max_size)*4/3;\r\nif (goal > tbl->max_size/2)\r\ngoal = tbl->max_size/2;\r\nfor (i=0, j=tbl->rover; i<IP_VS_LBLCR_TAB_SIZE; i++) {\r\nj = (j + 1) & IP_VS_LBLCR_TAB_MASK;\r\nwrite_lock(&svc->sched_lock);\r\nlist_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {\r\nif (time_before(now, en->lastuse+ENTRY_TIMEOUT))\r\ncontinue;\r\nip_vs_lblcr_free(en);\r\natomic_dec(&tbl->entries);\r\ngoal--;\r\n}\r\nwrite_unlock(&svc->sched_lock);\r\nif (goal <= 0)\r\nbreak;\r\n}\r\ntbl->rover = j;\r\nout:\r\nmod_timer(&tbl->periodic_timer, jiffies+CHECK_EXPIRE_INTERVAL);\r\n}\r\nstatic int ip_vs_lblcr_init_svc(struct ip_vs_service *svc)\r\n{\r\nint i;\r\nstruct ip_vs_lblcr_table *tbl;\r\ntbl = kmalloc(sizeof(*tbl), GFP_ATOMIC);\r\nif (tbl == NULL) {\r\npr_err("%s(): no memory\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nsvc->sched_data = tbl;\r\nIP_VS_DBG(6, "LBLCR hash table (memory=%Zdbytes) allocated for "\r\n"current service\n", sizeof(*tbl));\r\nfor (i=0; i<IP_VS_LBLCR_TAB_SIZE; i++) {\r\nINIT_LIST_HEAD(&tbl->bucket[i]);\r\n}\r\ntbl->max_size = IP_VS_LBLCR_TAB_SIZE*16;\r\ntbl->rover = 0;\r\ntbl->counter = 1;\r\nsetup_timer(&tbl->periodic_timer, ip_vs_lblcr_check_expire,\r\n(unsigned long)svc);\r\nmod_timer(&tbl->periodic_timer, jiffies + CHECK_EXPIRE_INTERVAL);\r\nreturn 0;\r\n}\r\nstatic int ip_vs_lblcr_done_svc(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_lblcr_table *tbl = svc->sched_data;\r\ndel_timer_sync(&tbl->periodic_timer);\r\nip_vs_lblcr_flush(tbl);\r\nkfree(tbl);\r\nIP_VS_DBG(6, "LBLCR hash table (memory=%Zdbytes) released\n",\r\nsizeof(*tbl));\r\nreturn 0;\r\n}\r\nstatic inline struct ip_vs_dest *\r\n__ip_vs_lblcr_schedule(struct ip_vs_service *svc)\r\n{\r\nstruct ip_vs_dest *dest, *least;\r\nint loh, doh;\r\nlist_for_each_entry(dest, &svc->destinations, n_list) {\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\nif (atomic_read(&dest->weight) > 0) {\r\nleast = dest;\r\nloh = ip_vs_dest_conn_overhead(least);\r\ngoto nextstage;\r\n}\r\n}\r\nreturn NULL;\r\nnextstage:\r\nlist_for_each_entry_continue(dest, &svc->destinations, n_list) {\r\nif (dest->flags & IP_VS_DEST_F_OVERLOAD)\r\ncontinue;\r\ndoh = ip_vs_dest_conn_overhead(dest);\r\nif (loh * atomic_read(&dest->weight) >\r\ndoh * atomic_read(&least->weight)) {\r\nleast = dest;\r\nloh = doh;\r\n}\r\n}\r\nIP_VS_DBG_BUF(6, "LBLCR: server %s:%d "\r\n"activeconns %d refcnt %d weight %d overhead %d\n",\r\nIP_VS_DBG_ADDR(least->af, &least->addr),\r\nntohs(least->port),\r\natomic_read(&least->activeconns),\r\natomic_read(&least->refcnt),\r\natomic_read(&least->weight), loh);\r\nreturn least;\r\n}\r\nstatic inline int\r\nis_overloaded(struct ip_vs_dest *dest, struct ip_vs_service *svc)\r\n{\r\nif (atomic_read(&dest->activeconns) > atomic_read(&dest->weight)) {\r\nstruct ip_vs_dest *d;\r\nlist_for_each_entry(d, &svc->destinations, n_list) {\r\nif (atomic_read(&d->activeconns)*2\r\n< atomic_read(&d->weight)) {\r\nreturn 1;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic struct ip_vs_dest *\r\nip_vs_lblcr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)\r\n{\r\nstruct ip_vs_lblcr_table *tbl = svc->sched_data;\r\nstruct ip_vs_iphdr iph;\r\nstruct ip_vs_dest *dest = NULL;\r\nstruct ip_vs_lblcr_entry *en;\r\nip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);\r\nIP_VS_DBG(6, "%s(): Scheduling...\n", __func__);\r\nread_lock(&svc->sched_lock);\r\nen = ip_vs_lblcr_get(svc->af, tbl, &iph.daddr);\r\nif (en) {\r\nen->lastuse = jiffies;\r\nread_lock(&en->set.lock);\r\ndest = ip_vs_dest_set_min(&en->set);\r\nread_unlock(&en->set.lock);\r\nif (atomic_read(&en->set.size) > 1 &&\r\ntime_after(jiffies, en->set.lastmod +\r\nsysctl_lblcr_expiration(svc))) {\r\nstruct ip_vs_dest *m;\r\nwrite_lock(&en->set.lock);\r\nm = ip_vs_dest_set_max(&en->set);\r\nif (m)\r\nip_vs_dest_set_erase(&en->set, m);\r\nwrite_unlock(&en->set.lock);\r\n}\r\nif (dest && !is_overloaded(dest, svc)) {\r\nread_unlock(&svc->sched_lock);\r\ngoto out;\r\n}\r\ndest = __ip_vs_lblcr_schedule(svc);\r\nif (!dest) {\r\nip_vs_scheduler_err(svc, "no destination available");\r\nread_unlock(&svc->sched_lock);\r\nreturn NULL;\r\n}\r\nwrite_lock(&en->set.lock);\r\nip_vs_dest_set_insert(&en->set, dest);\r\nwrite_unlock(&en->set.lock);\r\n}\r\nread_unlock(&svc->sched_lock);\r\nif (dest)\r\ngoto out;\r\ndest = __ip_vs_lblcr_schedule(svc);\r\nif (!dest) {\r\nIP_VS_DBG(1, "no destination available\n");\r\nreturn NULL;\r\n}\r\nwrite_lock(&svc->sched_lock);\r\nip_vs_lblcr_new(tbl, &iph.daddr, dest);\r\nwrite_unlock(&svc->sched_lock);\r\nout:\r\nIP_VS_DBG_BUF(6, "LBLCR: destination IP address %s --> server %s:%d\n",\r\nIP_VS_DBG_ADDR(svc->af, &iph.daddr),\r\nIP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port));\r\nreturn dest;\r\n}\r\nstatic int __net_init __ip_vs_lblcr_init(struct net *net)\r\n{\r\nstruct netns_ipvs *ipvs = net_ipvs(net);\r\nif (!net_eq(net, &init_net)) {\r\nipvs->lblcr_ctl_table = kmemdup(vs_vars_table,\r\nsizeof(vs_vars_table),\r\nGFP_KERNEL);\r\nif (ipvs->lblcr_ctl_table == NULL)\r\nreturn -ENOMEM;\r\n} else\r\nipvs->lblcr_ctl_table = vs_vars_table;\r\nipvs->sysctl_lblcr_expiration = DEFAULT_EXPIRATION;\r\nipvs->lblcr_ctl_table[0].data = &ipvs->sysctl_lblcr_expiration;\r\nipvs->lblcr_ctl_header =\r\nregister_net_sysctl_table(net, net_vs_ctl_path,\r\nipvs->lblcr_ctl_table);\r\nif (!ipvs->lblcr_ctl_header) {\r\nif (!net_eq(net, &init_net))\r\nkfree(ipvs->lblcr_ctl_table);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __net_exit __ip_vs_lblcr_exit(struct net *net)\r\n{\r\nstruct netns_ipvs *ipvs = net_ipvs(net);\r\nunregister_net_sysctl_table(ipvs->lblcr_ctl_header);\r\nif (!net_eq(net, &init_net))\r\nkfree(ipvs->lblcr_ctl_table);\r\n}\r\nstatic int __net_init __ip_vs_lblcr_init(struct net *net) { return 0; }\r\nstatic void __net_exit __ip_vs_lblcr_exit(struct net *net) { }\r\nstatic int __init ip_vs_lblcr_init(void)\r\n{\r\nint ret;\r\nret = register_pernet_subsys(&ip_vs_lblcr_ops);\r\nif (ret)\r\nreturn ret;\r\nret = register_ip_vs_scheduler(&ip_vs_lblcr_scheduler);\r\nif (ret)\r\nunregister_pernet_subsys(&ip_vs_lblcr_ops);\r\nreturn ret;\r\n}\r\nstatic void __exit ip_vs_lblcr_cleanup(void)\r\n{\r\nunregister_ip_vs_scheduler(&ip_vs_lblcr_scheduler);\r\nunregister_pernet_subsys(&ip_vs_lblcr_ops);\r\n}
