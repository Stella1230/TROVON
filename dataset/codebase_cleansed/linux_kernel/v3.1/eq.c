static void eq_set_ci(struct mlx4_eq *eq, int req_not)\r\n{\r\n__raw_writel((__force u32) cpu_to_be32((eq->cons_index & 0xffffff) |\r\nreq_not << 31),\r\neq->doorbell);\r\nmb();\r\n}\r\nstatic struct mlx4_eqe *get_eqe(struct mlx4_eq *eq, u32 entry)\r\n{\r\nunsigned long off = (entry & (eq->nent - 1)) * MLX4_EQ_ENTRY_SIZE;\r\nreturn eq->page_list[off / PAGE_SIZE].buf + off % PAGE_SIZE;\r\n}\r\nstatic struct mlx4_eqe *next_eqe_sw(struct mlx4_eq *eq)\r\n{\r\nstruct mlx4_eqe *eqe = get_eqe(eq, eq->cons_index);\r\nreturn !!(eqe->owner & 0x80) ^ !!(eq->cons_index & eq->nent) ? NULL : eqe;\r\n}\r\nstatic int mlx4_eq_int(struct mlx4_dev *dev, struct mlx4_eq *eq)\r\n{\r\nstruct mlx4_eqe *eqe;\r\nint cqn;\r\nint eqes_found = 0;\r\nint set_ci = 0;\r\nint port;\r\nwhile ((eqe = next_eqe_sw(eq))) {\r\nrmb();\r\nswitch (eqe->type) {\r\ncase MLX4_EVENT_TYPE_COMP:\r\ncqn = be32_to_cpu(eqe->event.comp.cqn) & 0xffffff;\r\nmlx4_cq_completion(dev, cqn);\r\nbreak;\r\ncase MLX4_EVENT_TYPE_PATH_MIG:\r\ncase MLX4_EVENT_TYPE_COMM_EST:\r\ncase MLX4_EVENT_TYPE_SQ_DRAINED:\r\ncase MLX4_EVENT_TYPE_SRQ_QP_LAST_WQE:\r\ncase MLX4_EVENT_TYPE_WQ_CATAS_ERROR:\r\ncase MLX4_EVENT_TYPE_PATH_MIG_FAILED:\r\ncase MLX4_EVENT_TYPE_WQ_INVAL_REQ_ERROR:\r\ncase MLX4_EVENT_TYPE_WQ_ACCESS_ERROR:\r\nmlx4_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,\r\neqe->type);\r\nbreak;\r\ncase MLX4_EVENT_TYPE_SRQ_LIMIT:\r\ncase MLX4_EVENT_TYPE_SRQ_CATAS_ERROR:\r\nmlx4_srq_event(dev, be32_to_cpu(eqe->event.srq.srqn) & 0xffffff,\r\neqe->type);\r\nbreak;\r\ncase MLX4_EVENT_TYPE_CMD:\r\nmlx4_cmd_event(dev,\r\nbe16_to_cpu(eqe->event.cmd.token),\r\neqe->event.cmd.status,\r\nbe64_to_cpu(eqe->event.cmd.out_param));\r\nbreak;\r\ncase MLX4_EVENT_TYPE_PORT_CHANGE:\r\nport = be32_to_cpu(eqe->event.port_change.port) >> 28;\r\nif (eqe->subtype == MLX4_PORT_CHANGE_SUBTYPE_DOWN) {\r\nmlx4_dispatch_event(dev, MLX4_DEV_EVENT_PORT_DOWN,\r\nport);\r\nmlx4_priv(dev)->sense.do_sense_port[port] = 1;\r\n} else {\r\nmlx4_dispatch_event(dev, MLX4_DEV_EVENT_PORT_UP,\r\nport);\r\nmlx4_priv(dev)->sense.do_sense_port[port] = 0;\r\n}\r\nbreak;\r\ncase MLX4_EVENT_TYPE_CQ_ERROR:\r\nmlx4_warn(dev, "CQ %s on CQN %06x\n",\r\neqe->event.cq_err.syndrome == 1 ?\r\n"overrun" : "access violation",\r\nbe32_to_cpu(eqe->event.cq_err.cqn) & 0xffffff);\r\nmlx4_cq_event(dev, be32_to_cpu(eqe->event.cq_err.cqn),\r\neqe->type);\r\nbreak;\r\ncase MLX4_EVENT_TYPE_EQ_OVERFLOW:\r\nmlx4_warn(dev, "EQ overrun on EQN %d\n", eq->eqn);\r\nbreak;\r\ncase MLX4_EVENT_TYPE_EEC_CATAS_ERROR:\r\ncase MLX4_EVENT_TYPE_ECC_DETECT:\r\ndefault:\r\nmlx4_warn(dev, "Unhandled event %02x(%02x) on EQ %d at index %u\n",\r\neqe->type, eqe->subtype, eq->eqn, eq->cons_index);\r\nbreak;\r\n}\r\n++eq->cons_index;\r\neqes_found = 1;\r\n++set_ci;\r\nif (unlikely(set_ci >= MLX4_NUM_SPARE_EQE)) {\r\neq_set_ci(eq, 0);\r\nset_ci = 0;\r\n}\r\n}\r\neq_set_ci(eq, 1);\r\nreturn eqes_found;\r\n}\r\nstatic irqreturn_t mlx4_interrupt(int irq, void *dev_ptr)\r\n{\r\nstruct mlx4_dev *dev = dev_ptr;\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint work = 0;\r\nint i;\r\nwritel(priv->eq_table.clr_mask, priv->eq_table.clr_int);\r\nfor (i = 0; i < dev->caps.num_comp_vectors + 1; ++i)\r\nwork |= mlx4_eq_int(dev, &priv->eq_table.eq[i]);\r\nreturn IRQ_RETVAL(work);\r\n}\r\nstatic irqreturn_t mlx4_msi_x_interrupt(int irq, void *eq_ptr)\r\n{\r\nstruct mlx4_eq *eq = eq_ptr;\r\nstruct mlx4_dev *dev = eq->dev;\r\nmlx4_eq_int(dev, eq);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int mlx4_MAP_EQ(struct mlx4_dev *dev, u64 event_mask, int unmap,\r\nint eq_num)\r\n{\r\nreturn mlx4_cmd(dev, event_mask, (unmap << 31) | eq_num,\r\n0, MLX4_CMD_MAP_EQ, MLX4_CMD_TIME_CLASS_B);\r\n}\r\nstatic int mlx4_SW2HW_EQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,\r\nint eq_num)\r\n{\r\nreturn mlx4_cmd(dev, mailbox->dma, eq_num, 0, MLX4_CMD_SW2HW_EQ,\r\nMLX4_CMD_TIME_CLASS_A);\r\n}\r\nstatic int mlx4_HW2SW_EQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,\r\nint eq_num)\r\n{\r\nreturn mlx4_cmd_box(dev, 0, mailbox->dma, eq_num, 0, MLX4_CMD_HW2SW_EQ,\r\nMLX4_CMD_TIME_CLASS_A);\r\n}\r\nstatic int mlx4_num_eq_uar(struct mlx4_dev *dev)\r\n{\r\nreturn (dev->caps.num_comp_vectors + 1 + dev->caps.reserved_eqs +\r\ndev->caps.comp_pool)/4 - dev->caps.reserved_eqs/4 + 1;\r\n}\r\nstatic void __iomem *mlx4_get_eq_uar(struct mlx4_dev *dev, struct mlx4_eq *eq)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint index;\r\nindex = eq->eqn / 4 - dev->caps.reserved_eqs / 4;\r\nif (!priv->eq_table.uar_map[index]) {\r\npriv->eq_table.uar_map[index] =\r\nioremap(pci_resource_start(dev->pdev, 2) +\r\n((eq->eqn / 4) << PAGE_SHIFT),\r\nPAGE_SIZE);\r\nif (!priv->eq_table.uar_map[index]) {\r\nmlx4_err(dev, "Couldn't map EQ doorbell for EQN 0x%06x\n",\r\neq->eqn);\r\nreturn NULL;\r\n}\r\n}\r\nreturn priv->eq_table.uar_map[index] + 0x800 + 8 * (eq->eqn % 4);\r\n}\r\nstatic int mlx4_create_eq(struct mlx4_dev *dev, int nent,\r\nu8 intr, struct mlx4_eq *eq)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_cmd_mailbox *mailbox;\r\nstruct mlx4_eq_context *eq_context;\r\nint npages;\r\nu64 *dma_list = NULL;\r\ndma_addr_t t;\r\nu64 mtt_addr;\r\nint err = -ENOMEM;\r\nint i;\r\neq->dev = dev;\r\neq->nent = roundup_pow_of_two(max(nent, 2));\r\nnpages = PAGE_ALIGN(eq->nent * MLX4_EQ_ENTRY_SIZE) / PAGE_SIZE;\r\neq->page_list = kmalloc(npages * sizeof *eq->page_list,\r\nGFP_KERNEL);\r\nif (!eq->page_list)\r\ngoto err_out;\r\nfor (i = 0; i < npages; ++i)\r\neq->page_list[i].buf = NULL;\r\ndma_list = kmalloc(npages * sizeof *dma_list, GFP_KERNEL);\r\nif (!dma_list)\r\ngoto err_out_free;\r\nmailbox = mlx4_alloc_cmd_mailbox(dev);\r\nif (IS_ERR(mailbox))\r\ngoto err_out_free;\r\neq_context = mailbox->buf;\r\nfor (i = 0; i < npages; ++i) {\r\neq->page_list[i].buf = dma_alloc_coherent(&dev->pdev->dev,\r\nPAGE_SIZE, &t, GFP_KERNEL);\r\nif (!eq->page_list[i].buf)\r\ngoto err_out_free_pages;\r\ndma_list[i] = t;\r\neq->page_list[i].map = t;\r\nmemset(eq->page_list[i].buf, 0, PAGE_SIZE);\r\n}\r\neq->eqn = mlx4_bitmap_alloc(&priv->eq_table.bitmap);\r\nif (eq->eqn == -1)\r\ngoto err_out_free_pages;\r\neq->doorbell = mlx4_get_eq_uar(dev, eq);\r\nif (!eq->doorbell) {\r\nerr = -ENOMEM;\r\ngoto err_out_free_eq;\r\n}\r\nerr = mlx4_mtt_init(dev, npages, PAGE_SHIFT, &eq->mtt);\r\nif (err)\r\ngoto err_out_free_eq;\r\nerr = mlx4_write_mtt(dev, &eq->mtt, 0, npages, dma_list);\r\nif (err)\r\ngoto err_out_free_mtt;\r\nmemset(eq_context, 0, sizeof *eq_context);\r\neq_context->flags = cpu_to_be32(MLX4_EQ_STATUS_OK |\r\nMLX4_EQ_STATE_ARMED);\r\neq_context->log_eq_size = ilog2(eq->nent);\r\neq_context->intr = intr;\r\neq_context->log_page_size = PAGE_SHIFT - MLX4_ICM_PAGE_SHIFT;\r\nmtt_addr = mlx4_mtt_addr(dev, &eq->mtt);\r\neq_context->mtt_base_addr_h = mtt_addr >> 32;\r\neq_context->mtt_base_addr_l = cpu_to_be32(mtt_addr & 0xffffffff);\r\nerr = mlx4_SW2HW_EQ(dev, mailbox, eq->eqn);\r\nif (err) {\r\nmlx4_warn(dev, "SW2HW_EQ failed (%d)\n", err);\r\ngoto err_out_free_mtt;\r\n}\r\nkfree(dma_list);\r\nmlx4_free_cmd_mailbox(dev, mailbox);\r\neq->cons_index = 0;\r\nreturn err;\r\nerr_out_free_mtt:\r\nmlx4_mtt_cleanup(dev, &eq->mtt);\r\nerr_out_free_eq:\r\nmlx4_bitmap_free(&priv->eq_table.bitmap, eq->eqn);\r\nerr_out_free_pages:\r\nfor (i = 0; i < npages; ++i)\r\nif (eq->page_list[i].buf)\r\ndma_free_coherent(&dev->pdev->dev, PAGE_SIZE,\r\neq->page_list[i].buf,\r\neq->page_list[i].map);\r\nmlx4_free_cmd_mailbox(dev, mailbox);\r\nerr_out_free:\r\nkfree(eq->page_list);\r\nkfree(dma_list);\r\nerr_out:\r\nreturn err;\r\n}\r\nstatic void mlx4_free_eq(struct mlx4_dev *dev,\r\nstruct mlx4_eq *eq)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_cmd_mailbox *mailbox;\r\nint err;\r\nint npages = PAGE_ALIGN(MLX4_EQ_ENTRY_SIZE * eq->nent) / PAGE_SIZE;\r\nint i;\r\nmailbox = mlx4_alloc_cmd_mailbox(dev);\r\nif (IS_ERR(mailbox))\r\nreturn;\r\nerr = mlx4_HW2SW_EQ(dev, mailbox, eq->eqn);\r\nif (err)\r\nmlx4_warn(dev, "HW2SW_EQ failed (%d)\n", err);\r\nif (0) {\r\nmlx4_dbg(dev, "Dumping EQ context %02x:\n", eq->eqn);\r\nfor (i = 0; i < sizeof (struct mlx4_eq_context) / 4; ++i) {\r\nif (i % 4 == 0)\r\npr_cont("[%02x] ", i * 4);\r\npr_cont(" %08x", be32_to_cpup(mailbox->buf + i * 4));\r\nif ((i + 1) % 4 == 0)\r\npr_cont("\n");\r\n}\r\n}\r\nmlx4_mtt_cleanup(dev, &eq->mtt);\r\nfor (i = 0; i < npages; ++i)\r\npci_free_consistent(dev->pdev, PAGE_SIZE,\r\neq->page_list[i].buf,\r\neq->page_list[i].map);\r\nkfree(eq->page_list);\r\nmlx4_bitmap_free(&priv->eq_table.bitmap, eq->eqn);\r\nmlx4_free_cmd_mailbox(dev, mailbox);\r\n}\r\nstatic void mlx4_free_irqs(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_eq_table *eq_table = &mlx4_priv(dev)->eq_table;\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint i, vec;\r\nif (eq_table->have_irq)\r\nfree_irq(dev->pdev->irq, dev);\r\nfor (i = 0; i < dev->caps.num_comp_vectors + 1; ++i)\r\nif (eq_table->eq[i].have_irq) {\r\nfree_irq(eq_table->eq[i].irq, eq_table->eq + i);\r\neq_table->eq[i].have_irq = 0;\r\n}\r\nfor (i = 0; i < dev->caps.comp_pool; i++) {\r\nif (priv->msix_ctl.pool_bm & 1ULL << i) {\r\nvec = dev->caps.num_comp_vectors + 1 + i;\r\nfree_irq(priv->eq_table.eq[vec].irq,\r\n&priv->eq_table.eq[vec]);\r\n}\r\n}\r\nkfree(eq_table->irq_names);\r\n}\r\nstatic int mlx4_map_clr_int(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\npriv->clr_base = ioremap(pci_resource_start(dev->pdev, priv->fw.clr_int_bar) +\r\npriv->fw.clr_int_base, MLX4_CLR_INT_SIZE);\r\nif (!priv->clr_base) {\r\nmlx4_err(dev, "Couldn't map interrupt clear register, aborting.\n");\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void mlx4_unmap_clr_int(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\niounmap(priv->clr_base);\r\n}\r\nint mlx4_alloc_eq_table(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\npriv->eq_table.eq = kcalloc(dev->caps.num_eqs - dev->caps.reserved_eqs,\r\nsizeof *priv->eq_table.eq, GFP_KERNEL);\r\nif (!priv->eq_table.eq)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid mlx4_free_eq_table(struct mlx4_dev *dev)\r\n{\r\nkfree(mlx4_priv(dev)->eq_table.eq);\r\n}\r\nint mlx4_init_eq_table(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint err;\r\nint i;\r\npriv->eq_table.uar_map = kcalloc(sizeof *priv->eq_table.uar_map,\r\nmlx4_num_eq_uar(dev), GFP_KERNEL);\r\nif (!priv->eq_table.uar_map) {\r\nerr = -ENOMEM;\r\ngoto err_out_free;\r\n}\r\nerr = mlx4_bitmap_init(&priv->eq_table.bitmap, dev->caps.num_eqs,\r\ndev->caps.num_eqs - 1, dev->caps.reserved_eqs, 0);\r\nif (err)\r\ngoto err_out_free;\r\nfor (i = 0; i < mlx4_num_eq_uar(dev); ++i)\r\npriv->eq_table.uar_map[i] = NULL;\r\nerr = mlx4_map_clr_int(dev);\r\nif (err)\r\ngoto err_out_bitmap;\r\npriv->eq_table.clr_mask =\r\nswab32(1 << (priv->eq_table.inta_pin & 31));\r\npriv->eq_table.clr_int = priv->clr_base +\r\n(priv->eq_table.inta_pin < 32 ? 4 : 0);\r\npriv->eq_table.irq_names =\r\nkmalloc(MLX4_IRQNAME_SIZE * (dev->caps.num_comp_vectors + 1 +\r\ndev->caps.comp_pool),\r\nGFP_KERNEL);\r\nif (!priv->eq_table.irq_names) {\r\nerr = -ENOMEM;\r\ngoto err_out_bitmap;\r\n}\r\nfor (i = 0; i < dev->caps.num_comp_vectors; ++i) {\r\nerr = mlx4_create_eq(dev, dev->caps.num_cqs -\r\ndev->caps.reserved_cqs +\r\nMLX4_NUM_SPARE_EQE,\r\n(dev->flags & MLX4_FLAG_MSI_X) ? i : 0,\r\n&priv->eq_table.eq[i]);\r\nif (err) {\r\n--i;\r\ngoto err_out_unmap;\r\n}\r\n}\r\nerr = mlx4_create_eq(dev, MLX4_NUM_ASYNC_EQE + MLX4_NUM_SPARE_EQE,\r\n(dev->flags & MLX4_FLAG_MSI_X) ? dev->caps.num_comp_vectors : 0,\r\n&priv->eq_table.eq[dev->caps.num_comp_vectors]);\r\nif (err)\r\ngoto err_out_comp;\r\nfor (i = dev->caps.num_comp_vectors + 1;\r\ni < dev->caps.num_comp_vectors + dev->caps.comp_pool + 1; ++i) {\r\nerr = mlx4_create_eq(dev, dev->caps.num_cqs -\r\ndev->caps.reserved_cqs +\r\nMLX4_NUM_SPARE_EQE,\r\n(dev->flags & MLX4_FLAG_MSI_X) ? i : 0,\r\n&priv->eq_table.eq[i]);\r\nif (err) {\r\n--i;\r\ngoto err_out_unmap;\r\n}\r\n}\r\nif (dev->flags & MLX4_FLAG_MSI_X) {\r\nconst char *eq_name;\r\nfor (i = 0; i < dev->caps.num_comp_vectors + 1; ++i) {\r\nif (i < dev->caps.num_comp_vectors) {\r\nsnprintf(priv->eq_table.irq_names +\r\ni * MLX4_IRQNAME_SIZE,\r\nMLX4_IRQNAME_SIZE,\r\n"mlx4-comp-%d@pci:%s", i,\r\npci_name(dev->pdev));\r\n} else {\r\nsnprintf(priv->eq_table.irq_names +\r\ni * MLX4_IRQNAME_SIZE,\r\nMLX4_IRQNAME_SIZE,\r\n"mlx4-async@pci:%s",\r\npci_name(dev->pdev));\r\n}\r\neq_name = priv->eq_table.irq_names +\r\ni * MLX4_IRQNAME_SIZE;\r\nerr = request_irq(priv->eq_table.eq[i].irq,\r\nmlx4_msi_x_interrupt, 0, eq_name,\r\npriv->eq_table.eq + i);\r\nif (err)\r\ngoto err_out_async;\r\npriv->eq_table.eq[i].have_irq = 1;\r\n}\r\n} else {\r\nsnprintf(priv->eq_table.irq_names,\r\nMLX4_IRQNAME_SIZE,\r\nDRV_NAME "@pci:%s",\r\npci_name(dev->pdev));\r\nerr = request_irq(dev->pdev->irq, mlx4_interrupt,\r\nIRQF_SHARED, priv->eq_table.irq_names, dev);\r\nif (err)\r\ngoto err_out_async;\r\npriv->eq_table.have_irq = 1;\r\n}\r\nerr = mlx4_MAP_EQ(dev, MLX4_ASYNC_EVENT_MASK, 0,\r\npriv->eq_table.eq[dev->caps.num_comp_vectors].eqn);\r\nif (err)\r\nmlx4_warn(dev, "MAP_EQ for async EQ %d failed (%d)\n",\r\npriv->eq_table.eq[dev->caps.num_comp_vectors].eqn, err);\r\nfor (i = 0; i < dev->caps.num_comp_vectors + 1; ++i)\r\neq_set_ci(&priv->eq_table.eq[i], 1);\r\nreturn 0;\r\nerr_out_async:\r\nmlx4_free_eq(dev, &priv->eq_table.eq[dev->caps.num_comp_vectors]);\r\nerr_out_comp:\r\ni = dev->caps.num_comp_vectors - 1;\r\nerr_out_unmap:\r\nwhile (i >= 0) {\r\nmlx4_free_eq(dev, &priv->eq_table.eq[i]);\r\n--i;\r\n}\r\nmlx4_unmap_clr_int(dev);\r\nmlx4_free_irqs(dev);\r\nerr_out_bitmap:\r\nmlx4_bitmap_cleanup(&priv->eq_table.bitmap);\r\nerr_out_free:\r\nkfree(priv->eq_table.uar_map);\r\nreturn err;\r\n}\r\nvoid mlx4_cleanup_eq_table(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint i;\r\nmlx4_MAP_EQ(dev, MLX4_ASYNC_EVENT_MASK, 1,\r\npriv->eq_table.eq[dev->caps.num_comp_vectors].eqn);\r\nmlx4_free_irqs(dev);\r\nfor (i = 0; i < dev->caps.num_comp_vectors + dev->caps.comp_pool + 1; ++i)\r\nmlx4_free_eq(dev, &priv->eq_table.eq[i]);\r\nmlx4_unmap_clr_int(dev);\r\nfor (i = 0; i < mlx4_num_eq_uar(dev); ++i)\r\nif (priv->eq_table.uar_map[i])\r\niounmap(priv->eq_table.uar_map[i]);\r\nmlx4_bitmap_cleanup(&priv->eq_table.bitmap);\r\nkfree(priv->eq_table.uar_map);\r\n}\r\nint mlx4_test_interrupts(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint i;\r\nint err;\r\nerr = mlx4_NOP(dev);\r\nif (!(dev->flags & MLX4_FLAG_MSI_X))\r\nreturn err;\r\nfor(i = 0; !err && (i < dev->caps.num_comp_vectors); ++i) {\r\nmlx4_cmd_use_polling(dev);\r\nerr = mlx4_MAP_EQ(dev, MLX4_ASYNC_EVENT_MASK, 0,\r\npriv->eq_table.eq[i].eqn);\r\nif (err) {\r\nmlx4_warn(dev, "Failed mapping eq for interrupt test\n");\r\nmlx4_cmd_use_events(dev);\r\nbreak;\r\n}\r\nmlx4_cmd_use_events(dev);\r\nerr = mlx4_NOP(dev);\r\n}\r\nmlx4_MAP_EQ(dev, MLX4_ASYNC_EVENT_MASK, 0,\r\npriv->eq_table.eq[dev->caps.num_comp_vectors].eqn);\r\nreturn err;\r\n}\r\nint mlx4_assign_eq(struct mlx4_dev *dev, char* name, int * vector)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint vec = 0, err = 0, i;\r\nspin_lock(&priv->msix_ctl.pool_lock);\r\nfor (i = 0; !vec && i < dev->caps.comp_pool; i++) {\r\nif (~priv->msix_ctl.pool_bm & 1ULL << i) {\r\npriv->msix_ctl.pool_bm |= 1ULL << i;\r\nvec = dev->caps.num_comp_vectors + 1 + i;\r\nsnprintf(priv->eq_table.irq_names +\r\nvec * MLX4_IRQNAME_SIZE,\r\nMLX4_IRQNAME_SIZE, "%s", name);\r\nerr = request_irq(priv->eq_table.eq[vec].irq,\r\nmlx4_msi_x_interrupt, 0,\r\n&priv->eq_table.irq_names[vec<<5],\r\npriv->eq_table.eq + vec);\r\nif (err) {\r\npriv->msix_ctl.pool_bm ^= 1 << i;\r\nvec = 0;\r\ncontinue;\r\n}\r\neq_set_ci(&priv->eq_table.eq[vec], 1);\r\n}\r\n}\r\nspin_unlock(&priv->msix_ctl.pool_lock);\r\nif (vec) {\r\n*vector = vec;\r\n} else {\r\n*vector = 0;\r\nerr = (i == dev->caps.comp_pool) ? -ENOSPC : err;\r\n}\r\nreturn err;\r\n}\r\nvoid mlx4_release_eq(struct mlx4_dev *dev, int vec)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint i = vec - dev->caps.num_comp_vectors - 1;\r\nif (likely(i >= 0)) {\r\nspin_lock(&priv->msix_ctl.pool_lock);\r\nif (priv->msix_ctl.pool_bm & 1ULL << i) {\r\nfree_irq(priv->eq_table.eq[vec].irq,\r\n&priv->eq_table.eq[vec]);\r\npriv->msix_ctl.pool_bm &= ~(1ULL << i);\r\n}\r\nspin_unlock(&priv->msix_ctl.pool_lock);\r\n}\r\n}
