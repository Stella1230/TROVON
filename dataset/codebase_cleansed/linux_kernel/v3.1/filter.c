static void *__load_pointer(const struct sk_buff *skb, int k, unsigned int size)\r\n{\r\nu8 *ptr = NULL;\r\nif (k >= SKF_NET_OFF)\r\nptr = skb_network_header(skb) + k - SKF_NET_OFF;\r\nelse if (k >= SKF_LL_OFF)\r\nptr = skb_mac_header(skb) + k - SKF_LL_OFF;\r\nif (ptr >= skb->head && ptr + size <= skb_tail_pointer(skb))\r\nreturn ptr;\r\nreturn NULL;\r\n}\r\nstatic inline void *load_pointer(const struct sk_buff *skb, int k,\r\nunsigned int size, void *buffer)\r\n{\r\nif (k >= 0)\r\nreturn skb_header_pointer(skb, k, size, buffer);\r\nreturn __load_pointer(skb, k, size);\r\n}\r\nint sk_filter(struct sock *sk, struct sk_buff *skb)\r\n{\r\nint err;\r\nstruct sk_filter *filter;\r\nerr = security_sock_rcv_skb(sk, skb);\r\nif (err)\r\nreturn err;\r\nrcu_read_lock();\r\nfilter = rcu_dereference(sk->sk_filter);\r\nif (filter) {\r\nunsigned int pkt_len = SK_RUN_FILTER(filter, skb);\r\nerr = pkt_len ? pskb_trim(skb, pkt_len) : -EPERM;\r\n}\r\nrcu_read_unlock();\r\nreturn err;\r\n}\r\nunsigned int sk_run_filter(const struct sk_buff *skb,\r\nconst struct sock_filter *fentry)\r\n{\r\nvoid *ptr;\r\nu32 A = 0;\r\nu32 X = 0;\r\nu32 mem[BPF_MEMWORDS];\r\nu32 tmp;\r\nint k;\r\nfor (;; fentry++) {\r\n#if defined(CONFIG_X86_32)\r\n#define K (fentry->k)\r\n#else\r\nconst u32 K = fentry->k;\r\n#endif\r\nswitch (fentry->code) {\r\ncase BPF_S_ALU_ADD_X:\r\nA += X;\r\ncontinue;\r\ncase BPF_S_ALU_ADD_K:\r\nA += K;\r\ncontinue;\r\ncase BPF_S_ALU_SUB_X:\r\nA -= X;\r\ncontinue;\r\ncase BPF_S_ALU_SUB_K:\r\nA -= K;\r\ncontinue;\r\ncase BPF_S_ALU_MUL_X:\r\nA *= X;\r\ncontinue;\r\ncase BPF_S_ALU_MUL_K:\r\nA *= K;\r\ncontinue;\r\ncase BPF_S_ALU_DIV_X:\r\nif (X == 0)\r\nreturn 0;\r\nA /= X;\r\ncontinue;\r\ncase BPF_S_ALU_DIV_K:\r\nA = reciprocal_divide(A, K);\r\ncontinue;\r\ncase BPF_S_ALU_AND_X:\r\nA &= X;\r\ncontinue;\r\ncase BPF_S_ALU_AND_K:\r\nA &= K;\r\ncontinue;\r\ncase BPF_S_ALU_OR_X:\r\nA |= X;\r\ncontinue;\r\ncase BPF_S_ALU_OR_K:\r\nA |= K;\r\ncontinue;\r\ncase BPF_S_ALU_LSH_X:\r\nA <<= X;\r\ncontinue;\r\ncase BPF_S_ALU_LSH_K:\r\nA <<= K;\r\ncontinue;\r\ncase BPF_S_ALU_RSH_X:\r\nA >>= X;\r\ncontinue;\r\ncase BPF_S_ALU_RSH_K:\r\nA >>= K;\r\ncontinue;\r\ncase BPF_S_ALU_NEG:\r\nA = -A;\r\ncontinue;\r\ncase BPF_S_JMP_JA:\r\nfentry += K;\r\ncontinue;\r\ncase BPF_S_JMP_JGT_K:\r\nfentry += (A > K) ? fentry->jt : fentry->jf;\r\ncontinue;\r\ncase BPF_S_JMP_JGE_K:\r\nfentry += (A >= K) ? fentry->jt : fentry->jf;\r\ncontinue;\r\ncase BPF_S_JMP_JEQ_K:\r\nfentry += (A == K) ? fentry->jt : fentry->jf;\r\ncontinue;\r\ncase BPF_S_JMP_JSET_K:\r\nfentry += (A & K) ? fentry->jt : fentry->jf;\r\ncontinue;\r\ncase BPF_S_JMP_JGT_X:\r\nfentry += (A > X) ? fentry->jt : fentry->jf;\r\ncontinue;\r\ncase BPF_S_JMP_JGE_X:\r\nfentry += (A >= X) ? fentry->jt : fentry->jf;\r\ncontinue;\r\ncase BPF_S_JMP_JEQ_X:\r\nfentry += (A == X) ? fentry->jt : fentry->jf;\r\ncontinue;\r\ncase BPF_S_JMP_JSET_X:\r\nfentry += (A & X) ? fentry->jt : fentry->jf;\r\ncontinue;\r\ncase BPF_S_LD_W_ABS:\r\nk = K;\r\nload_w:\r\nptr = load_pointer(skb, k, 4, &tmp);\r\nif (ptr != NULL) {\r\nA = get_unaligned_be32(ptr);\r\ncontinue;\r\n}\r\nreturn 0;\r\ncase BPF_S_LD_H_ABS:\r\nk = K;\r\nload_h:\r\nptr = load_pointer(skb, k, 2, &tmp);\r\nif (ptr != NULL) {\r\nA = get_unaligned_be16(ptr);\r\ncontinue;\r\n}\r\nreturn 0;\r\ncase BPF_S_LD_B_ABS:\r\nk = K;\r\nload_b:\r\nptr = load_pointer(skb, k, 1, &tmp);\r\nif (ptr != NULL) {\r\nA = *(u8 *)ptr;\r\ncontinue;\r\n}\r\nreturn 0;\r\ncase BPF_S_LD_W_LEN:\r\nA = skb->len;\r\ncontinue;\r\ncase BPF_S_LDX_W_LEN:\r\nX = skb->len;\r\ncontinue;\r\ncase BPF_S_LD_W_IND:\r\nk = X + K;\r\ngoto load_w;\r\ncase BPF_S_LD_H_IND:\r\nk = X + K;\r\ngoto load_h;\r\ncase BPF_S_LD_B_IND:\r\nk = X + K;\r\ngoto load_b;\r\ncase BPF_S_LDX_B_MSH:\r\nptr = load_pointer(skb, K, 1, &tmp);\r\nif (ptr != NULL) {\r\nX = (*(u8 *)ptr & 0xf) << 2;\r\ncontinue;\r\n}\r\nreturn 0;\r\ncase BPF_S_LD_IMM:\r\nA = K;\r\ncontinue;\r\ncase BPF_S_LDX_IMM:\r\nX = K;\r\ncontinue;\r\ncase BPF_S_LD_MEM:\r\nA = mem[K];\r\ncontinue;\r\ncase BPF_S_LDX_MEM:\r\nX = mem[K];\r\ncontinue;\r\ncase BPF_S_MISC_TAX:\r\nX = A;\r\ncontinue;\r\ncase BPF_S_MISC_TXA:\r\nA = X;\r\ncontinue;\r\ncase BPF_S_RET_K:\r\nreturn K;\r\ncase BPF_S_RET_A:\r\nreturn A;\r\ncase BPF_S_ST:\r\nmem[K] = A;\r\ncontinue;\r\ncase BPF_S_STX:\r\nmem[K] = X;\r\ncontinue;\r\ncase BPF_S_ANC_PROTOCOL:\r\nA = ntohs(skb->protocol);\r\ncontinue;\r\ncase BPF_S_ANC_PKTTYPE:\r\nA = skb->pkt_type;\r\ncontinue;\r\ncase BPF_S_ANC_IFINDEX:\r\nif (!skb->dev)\r\nreturn 0;\r\nA = skb->dev->ifindex;\r\ncontinue;\r\ncase BPF_S_ANC_MARK:\r\nA = skb->mark;\r\ncontinue;\r\ncase BPF_S_ANC_QUEUE:\r\nA = skb->queue_mapping;\r\ncontinue;\r\ncase BPF_S_ANC_HATYPE:\r\nif (!skb->dev)\r\nreturn 0;\r\nA = skb->dev->type;\r\ncontinue;\r\ncase BPF_S_ANC_RXHASH:\r\nA = skb->rxhash;\r\ncontinue;\r\ncase BPF_S_ANC_CPU:\r\nA = raw_smp_processor_id();\r\ncontinue;\r\ncase BPF_S_ANC_NLATTR: {\r\nstruct nlattr *nla;\r\nif (skb_is_nonlinear(skb))\r\nreturn 0;\r\nif (A > skb->len - sizeof(struct nlattr))\r\nreturn 0;\r\nnla = nla_find((struct nlattr *)&skb->data[A],\r\nskb->len - A, X);\r\nif (nla)\r\nA = (void *)nla - (void *)skb->data;\r\nelse\r\nA = 0;\r\ncontinue;\r\n}\r\ncase BPF_S_ANC_NLATTR_NEST: {\r\nstruct nlattr *nla;\r\nif (skb_is_nonlinear(skb))\r\nreturn 0;\r\nif (A > skb->len - sizeof(struct nlattr))\r\nreturn 0;\r\nnla = (struct nlattr *)&skb->data[A];\r\nif (nla->nla_len > A - skb->len)\r\nreturn 0;\r\nnla = nla_find_nested(nla, X);\r\nif (nla)\r\nA = (void *)nla - (void *)skb->data;\r\nelse\r\nA = 0;\r\ncontinue;\r\n}\r\ndefault:\r\nWARN_RATELIMIT(1, "Unknown code:%u jt:%u tf:%u k:%u\n",\r\nfentry->code, fentry->jt,\r\nfentry->jf, fentry->k);\r\nreturn 0;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int check_load_and_stores(struct sock_filter *filter, int flen)\r\n{\r\nu16 *masks, memvalid = 0;\r\nint pc, ret = 0;\r\nBUILD_BUG_ON(BPF_MEMWORDS > 16);\r\nmasks = kmalloc(flen * sizeof(*masks), GFP_KERNEL);\r\nif (!masks)\r\nreturn -ENOMEM;\r\nmemset(masks, 0xff, flen * sizeof(*masks));\r\nfor (pc = 0; pc < flen; pc++) {\r\nmemvalid &= masks[pc];\r\nswitch (filter[pc].code) {\r\ncase BPF_S_ST:\r\ncase BPF_S_STX:\r\nmemvalid |= (1 << filter[pc].k);\r\nbreak;\r\ncase BPF_S_LD_MEM:\r\ncase BPF_S_LDX_MEM:\r\nif (!(memvalid & (1 << filter[pc].k))) {\r\nret = -EINVAL;\r\ngoto error;\r\n}\r\nbreak;\r\ncase BPF_S_JMP_JA:\r\nmasks[pc + 1 + filter[pc].k] &= memvalid;\r\nmemvalid = ~0;\r\nbreak;\r\ncase BPF_S_JMP_JEQ_K:\r\ncase BPF_S_JMP_JEQ_X:\r\ncase BPF_S_JMP_JGE_K:\r\ncase BPF_S_JMP_JGE_X:\r\ncase BPF_S_JMP_JGT_K:\r\ncase BPF_S_JMP_JGT_X:\r\ncase BPF_S_JMP_JSET_X:\r\ncase BPF_S_JMP_JSET_K:\r\nmasks[pc + 1 + filter[pc].jt] &= memvalid;\r\nmasks[pc + 1 + filter[pc].jf] &= memvalid;\r\nmemvalid = ~0;\r\nbreak;\r\n}\r\n}\r\nerror:\r\nkfree(masks);\r\nreturn ret;\r\n}\r\nint sk_chk_filter(struct sock_filter *filter, int flen)\r\n{\r\nstatic const u8 codes[] = {\r\n[BPF_ALU|BPF_ADD|BPF_K] = BPF_S_ALU_ADD_K,\r\n[BPF_ALU|BPF_ADD|BPF_X] = BPF_S_ALU_ADD_X,\r\n[BPF_ALU|BPF_SUB|BPF_K] = BPF_S_ALU_SUB_K,\r\n[BPF_ALU|BPF_SUB|BPF_X] = BPF_S_ALU_SUB_X,\r\n[BPF_ALU|BPF_MUL|BPF_K] = BPF_S_ALU_MUL_K,\r\n[BPF_ALU|BPF_MUL|BPF_X] = BPF_S_ALU_MUL_X,\r\n[BPF_ALU|BPF_DIV|BPF_X] = BPF_S_ALU_DIV_X,\r\n[BPF_ALU|BPF_AND|BPF_K] = BPF_S_ALU_AND_K,\r\n[BPF_ALU|BPF_AND|BPF_X] = BPF_S_ALU_AND_X,\r\n[BPF_ALU|BPF_OR|BPF_K] = BPF_S_ALU_OR_K,\r\n[BPF_ALU|BPF_OR|BPF_X] = BPF_S_ALU_OR_X,\r\n[BPF_ALU|BPF_LSH|BPF_K] = BPF_S_ALU_LSH_K,\r\n[BPF_ALU|BPF_LSH|BPF_X] = BPF_S_ALU_LSH_X,\r\n[BPF_ALU|BPF_RSH|BPF_K] = BPF_S_ALU_RSH_K,\r\n[BPF_ALU|BPF_RSH|BPF_X] = BPF_S_ALU_RSH_X,\r\n[BPF_ALU|BPF_NEG] = BPF_S_ALU_NEG,\r\n[BPF_LD|BPF_W|BPF_ABS] = BPF_S_LD_W_ABS,\r\n[BPF_LD|BPF_H|BPF_ABS] = BPF_S_LD_H_ABS,\r\n[BPF_LD|BPF_B|BPF_ABS] = BPF_S_LD_B_ABS,\r\n[BPF_LD|BPF_W|BPF_LEN] = BPF_S_LD_W_LEN,\r\n[BPF_LD|BPF_W|BPF_IND] = BPF_S_LD_W_IND,\r\n[BPF_LD|BPF_H|BPF_IND] = BPF_S_LD_H_IND,\r\n[BPF_LD|BPF_B|BPF_IND] = BPF_S_LD_B_IND,\r\n[BPF_LD|BPF_IMM] = BPF_S_LD_IMM,\r\n[BPF_LDX|BPF_W|BPF_LEN] = BPF_S_LDX_W_LEN,\r\n[BPF_LDX|BPF_B|BPF_MSH] = BPF_S_LDX_B_MSH,\r\n[BPF_LDX|BPF_IMM] = BPF_S_LDX_IMM,\r\n[BPF_MISC|BPF_TAX] = BPF_S_MISC_TAX,\r\n[BPF_MISC|BPF_TXA] = BPF_S_MISC_TXA,\r\n[BPF_RET|BPF_K] = BPF_S_RET_K,\r\n[BPF_RET|BPF_A] = BPF_S_RET_A,\r\n[BPF_ALU|BPF_DIV|BPF_K] = BPF_S_ALU_DIV_K,\r\n[BPF_LD|BPF_MEM] = BPF_S_LD_MEM,\r\n[BPF_LDX|BPF_MEM] = BPF_S_LDX_MEM,\r\n[BPF_ST] = BPF_S_ST,\r\n[BPF_STX] = BPF_S_STX,\r\n[BPF_JMP|BPF_JA] = BPF_S_JMP_JA,\r\n[BPF_JMP|BPF_JEQ|BPF_K] = BPF_S_JMP_JEQ_K,\r\n[BPF_JMP|BPF_JEQ|BPF_X] = BPF_S_JMP_JEQ_X,\r\n[BPF_JMP|BPF_JGE|BPF_K] = BPF_S_JMP_JGE_K,\r\n[BPF_JMP|BPF_JGE|BPF_X] = BPF_S_JMP_JGE_X,\r\n[BPF_JMP|BPF_JGT|BPF_K] = BPF_S_JMP_JGT_K,\r\n[BPF_JMP|BPF_JGT|BPF_X] = BPF_S_JMP_JGT_X,\r\n[BPF_JMP|BPF_JSET|BPF_K] = BPF_S_JMP_JSET_K,\r\n[BPF_JMP|BPF_JSET|BPF_X] = BPF_S_JMP_JSET_X,\r\n};\r\nint pc;\r\nif (flen == 0 || flen > BPF_MAXINSNS)\r\nreturn -EINVAL;\r\nfor (pc = 0; pc < flen; pc++) {\r\nstruct sock_filter *ftest = &filter[pc];\r\nu16 code = ftest->code;\r\nif (code >= ARRAY_SIZE(codes))\r\nreturn -EINVAL;\r\ncode = codes[code];\r\nif (!code)\r\nreturn -EINVAL;\r\nswitch (code) {\r\ncase BPF_S_ALU_DIV_K:\r\nif (ftest->k == 0)\r\nreturn -EINVAL;\r\nftest->k = reciprocal_value(ftest->k);\r\nbreak;\r\ncase BPF_S_LD_MEM:\r\ncase BPF_S_LDX_MEM:\r\ncase BPF_S_ST:\r\ncase BPF_S_STX:\r\nif (ftest->k >= BPF_MEMWORDS)\r\nreturn -EINVAL;\r\nbreak;\r\ncase BPF_S_JMP_JA:\r\nif (ftest->k >= (unsigned)(flen-pc-1))\r\nreturn -EINVAL;\r\nbreak;\r\ncase BPF_S_JMP_JEQ_K:\r\ncase BPF_S_JMP_JEQ_X:\r\ncase BPF_S_JMP_JGE_K:\r\ncase BPF_S_JMP_JGE_X:\r\ncase BPF_S_JMP_JGT_K:\r\ncase BPF_S_JMP_JGT_X:\r\ncase BPF_S_JMP_JSET_X:\r\ncase BPF_S_JMP_JSET_K:\r\nif (pc + ftest->jt + 1 >= flen ||\r\npc + ftest->jf + 1 >= flen)\r\nreturn -EINVAL;\r\nbreak;\r\ncase BPF_S_LD_W_ABS:\r\ncase BPF_S_LD_H_ABS:\r\ncase BPF_S_LD_B_ABS:\r\n#define ANCILLARY(CODE) case SKF_AD_OFF + SKF_AD_##CODE: \\r\ncode = BPF_S_ANC_##CODE; \\r\nbreak\r\nswitch (ftest->k) {\r\nANCILLARY(PROTOCOL);\r\nANCILLARY(PKTTYPE);\r\nANCILLARY(IFINDEX);\r\nANCILLARY(NLATTR);\r\nANCILLARY(NLATTR_NEST);\r\nANCILLARY(MARK);\r\nANCILLARY(QUEUE);\r\nANCILLARY(HATYPE);\r\nANCILLARY(RXHASH);\r\nANCILLARY(CPU);\r\n}\r\n}\r\nftest->code = code;\r\n}\r\nswitch (filter[flen - 1].code) {\r\ncase BPF_S_RET_K:\r\ncase BPF_S_RET_A:\r\nreturn check_load_and_stores(filter, flen);\r\n}\r\nreturn -EINVAL;\r\n}\r\nvoid sk_filter_release_rcu(struct rcu_head *rcu)\r\n{\r\nstruct sk_filter *fp = container_of(rcu, struct sk_filter, rcu);\r\nbpf_jit_free(fp);\r\nkfree(fp);\r\n}\r\nint sk_attach_filter(struct sock_fprog *fprog, struct sock *sk)\r\n{\r\nstruct sk_filter *fp, *old_fp;\r\nunsigned int fsize = sizeof(struct sock_filter) * fprog->len;\r\nint err;\r\nif (fprog->filter == NULL)\r\nreturn -EINVAL;\r\nfp = sock_kmalloc(sk, fsize+sizeof(*fp), GFP_KERNEL);\r\nif (!fp)\r\nreturn -ENOMEM;\r\nif (copy_from_user(fp->insns, fprog->filter, fsize)) {\r\nsock_kfree_s(sk, fp, fsize+sizeof(*fp));\r\nreturn -EFAULT;\r\n}\r\natomic_set(&fp->refcnt, 1);\r\nfp->len = fprog->len;\r\nfp->bpf_func = sk_run_filter;\r\nerr = sk_chk_filter(fp->insns, fp->len);\r\nif (err) {\r\nsk_filter_uncharge(sk, fp);\r\nreturn err;\r\n}\r\nbpf_jit_compile(fp);\r\nold_fp = rcu_dereference_protected(sk->sk_filter,\r\nsock_owned_by_user(sk));\r\nrcu_assign_pointer(sk->sk_filter, fp);\r\nif (old_fp)\r\nsk_filter_uncharge(sk, old_fp);\r\nreturn 0;\r\n}\r\nint sk_detach_filter(struct sock *sk)\r\n{\r\nint ret = -ENOENT;\r\nstruct sk_filter *filter;\r\nfilter = rcu_dereference_protected(sk->sk_filter,\r\nsock_owned_by_user(sk));\r\nif (filter) {\r\nrcu_assign_pointer(sk->sk_filter, NULL);\r\nsk_filter_uncharge(sk, filter);\r\nret = 0;\r\n}\r\nreturn ret;\r\n}
