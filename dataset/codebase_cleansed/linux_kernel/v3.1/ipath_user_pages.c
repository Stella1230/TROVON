static void __ipath_release_user_pages(struct page **p, size_t num_pages,\r\nint dirty)\r\n{\r\nsize_t i;\r\nfor (i = 0; i < num_pages; i++) {\r\nipath_cdbg(MM, "%lu/%lu put_page %p\n", (unsigned long) i,\r\n(unsigned long) num_pages, p[i]);\r\nif (dirty)\r\nset_page_dirty_lock(p[i]);\r\nput_page(p[i]);\r\n}\r\n}\r\nstatic int __ipath_get_user_pages(unsigned long start_page, size_t num_pages,\r\nstruct page **p, struct vm_area_struct **vma)\r\n{\r\nunsigned long lock_limit;\r\nsize_t got;\r\nint ret;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif (num_pages > lock_limit) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nipath_cdbg(VERBOSE, "pin %lx pages from vaddr %lx\n",\r\n(unsigned long) num_pages, start_page);\r\nfor (got = 0; got < num_pages; got += ret) {\r\nret = get_user_pages(current, current->mm,\r\nstart_page + got * PAGE_SIZE,\r\nnum_pages - got, 1, 1,\r\np + got, vma);\r\nif (ret < 0)\r\ngoto bail_release;\r\n}\r\ncurrent->mm->locked_vm += num_pages;\r\nret = 0;\r\ngoto bail;\r\nbail_release:\r\n__ipath_release_user_pages(p, got, 0);\r\nbail:\r\nreturn ret;\r\n}\r\ndma_addr_t ipath_map_page(struct pci_dev *hwdev, struct page *page,\r\nunsigned long offset, size_t size, int direction)\r\n{\r\ndma_addr_t phys;\r\nphys = pci_map_page(hwdev, page, offset, size, direction);\r\nif (phys == 0) {\r\npci_unmap_page(hwdev, phys, size, direction);\r\nphys = pci_map_page(hwdev, page, offset, size, direction);\r\n}\r\nreturn phys;\r\n}\r\ndma_addr_t ipath_map_single(struct pci_dev *hwdev, void *ptr, size_t size,\r\nint direction)\r\n{\r\ndma_addr_t phys;\r\nphys = pci_map_single(hwdev, ptr, size, direction);\r\nif (phys == 0) {\r\npci_unmap_single(hwdev, phys, size, direction);\r\nphys = pci_map_single(hwdev, ptr, size, direction);\r\n}\r\nreturn phys;\r\n}\r\nint ipath_get_user_pages(unsigned long start_page, size_t num_pages,\r\nstruct page **p)\r\n{\r\nint ret;\r\ndown_write(&current->mm->mmap_sem);\r\nret = __ipath_get_user_pages(start_page, num_pages, p, NULL);\r\nup_write(&current->mm->mmap_sem);\r\nreturn ret;\r\n}\r\nvoid ipath_release_user_pages(struct page **p, size_t num_pages)\r\n{\r\ndown_write(&current->mm->mmap_sem);\r\n__ipath_release_user_pages(p, num_pages, 1);\r\ncurrent->mm->locked_vm -= num_pages;\r\nup_write(&current->mm->mmap_sem);\r\n}\r\nstatic void user_pages_account(struct work_struct *_work)\r\n{\r\nstruct ipath_user_pages_work *work =\r\ncontainer_of(_work, struct ipath_user_pages_work, work);\r\ndown_write(&work->mm->mmap_sem);\r\nwork->mm->locked_vm -= work->num_pages;\r\nup_write(&work->mm->mmap_sem);\r\nmmput(work->mm);\r\nkfree(work);\r\n}\r\nvoid ipath_release_user_pages_on_close(struct page **p, size_t num_pages)\r\n{\r\nstruct ipath_user_pages_work *work;\r\nstruct mm_struct *mm;\r\n__ipath_release_user_pages(p, num_pages, 1);\r\nmm = get_task_mm(current);\r\nif (!mm)\r\nreturn;\r\nwork = kmalloc(sizeof(*work), GFP_KERNEL);\r\nif (!work)\r\ngoto bail_mm;\r\nINIT_WORK(&work->work, user_pages_account);\r\nwork->mm = mm;\r\nwork->num_pages = num_pages;\r\nqueue_work(ib_wq, &work->work);\r\nreturn;\r\nbail_mm:\r\nmmput(mm);\r\nreturn;\r\n}
