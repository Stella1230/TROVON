static void sh4_flush_icache_range(void *args)\r\n{\r\nstruct flusher_data *data = args;\r\nunsigned long start, end;\r\nunsigned long flags, v;\r\nint i;\r\nstart = data->addr1;\r\nend = data->addr2;\r\nif (((end - start) >> PAGE_SHIFT) >= MAX_ICACHE_PAGES) {\r\nlocal_flush_cache_all(NULL);\r\nreturn;\r\n}\r\nstart &= ~(L1_CACHE_BYTES-1);\r\nend += L1_CACHE_BYTES-1;\r\nend &= ~(L1_CACHE_BYTES-1);\r\nlocal_irq_save(flags);\r\njump_to_uncached();\r\nfor (v = start; v < end; v += L1_CACHE_BYTES) {\r\nunsigned long icacheaddr;\r\nint j, n;\r\n__ocbwb(v);\r\nicacheaddr = CACHE_IC_ADDRESS_ARRAY | (v &\r\ncpu_data->icache.entry_mask);\r\nn = boot_cpu_data.icache.n_aliases;\r\nfor (i = 0; i < cpu_data->icache.ways; i++) {\r\nfor (j = 0; j < n; j++)\r\n__raw_writel(0, icacheaddr + (j * PAGE_SIZE));\r\nicacheaddr += cpu_data->icache.way_incr;\r\n}\r\n}\r\nback_to_cached();\r\nlocal_irq_restore(flags);\r\n}\r\nstatic inline void flush_cache_one(unsigned long start, unsigned long phys)\r\n{\r\nunsigned long flags, exec_offset = 0;\r\nif ((boot_cpu_data.flags & CPU_HAS_P2_FLUSH_BUG) ||\r\n(start < CACHE_OC_ADDRESS_ARRAY))\r\nexec_offset = cached_to_uncached;\r\nlocal_irq_save(flags);\r\n__flush_cache_one(start, phys, exec_offset);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void sh4_flush_dcache_page(void *arg)\r\n{\r\nstruct page *page = arg;\r\nunsigned long addr = (unsigned long)page_address(page);\r\n#ifndef CONFIG_SMP\r\nstruct address_space *mapping = page_mapping(page);\r\nif (mapping && !mapping_mapped(mapping))\r\nclear_bit(PG_dcache_clean, &page->flags);\r\nelse\r\n#endif\r\nflush_cache_one(CACHE_OC_ADDRESS_ARRAY |\r\n(addr & shm_align_mask), page_to_phys(page));\r\nwmb();\r\n}\r\nstatic void flush_icache_all(void)\r\n{\r\nunsigned long flags, ccr;\r\nlocal_irq_save(flags);\r\njump_to_uncached();\r\nccr = __raw_readl(CCR);\r\nccr |= CCR_CACHE_ICI;\r\n__raw_writel(ccr, CCR);\r\nback_to_cached();\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void flush_dcache_all(void)\r\n{\r\nunsigned long addr, end_addr, entry_offset;\r\nend_addr = CACHE_OC_ADDRESS_ARRAY +\r\n(current_cpu_data.dcache.sets <<\r\ncurrent_cpu_data.dcache.entry_shift) *\r\ncurrent_cpu_data.dcache.ways;\r\nentry_offset = 1 << current_cpu_data.dcache.entry_shift;\r\nfor (addr = CACHE_OC_ADDRESS_ARRAY; addr < end_addr; ) {\r\n__raw_writel(0, addr); addr += entry_offset;\r\n__raw_writel(0, addr); addr += entry_offset;\r\n__raw_writel(0, addr); addr += entry_offset;\r\n__raw_writel(0, addr); addr += entry_offset;\r\n__raw_writel(0, addr); addr += entry_offset;\r\n__raw_writel(0, addr); addr += entry_offset;\r\n__raw_writel(0, addr); addr += entry_offset;\r\n__raw_writel(0, addr); addr += entry_offset;\r\n}\r\n}\r\nstatic void sh4_flush_cache_all(void *unused)\r\n{\r\nflush_dcache_all();\r\nflush_icache_all();\r\n}\r\nstatic void sh4_flush_cache_mm(void *arg)\r\n{\r\nstruct mm_struct *mm = arg;\r\nif (cpu_context(smp_processor_id(), mm) == NO_CONTEXT)\r\nreturn;\r\nflush_dcache_all();\r\n}\r\nstatic void sh4_flush_cache_page(void *args)\r\n{\r\nstruct flusher_data *data = args;\r\nstruct vm_area_struct *vma;\r\nstruct page *page;\r\nunsigned long address, pfn, phys;\r\nint map_coherent = 0;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nvoid *vaddr;\r\nvma = data->vma;\r\naddress = data->addr1 & PAGE_MASK;\r\npfn = data->addr2;\r\nphys = pfn << PAGE_SHIFT;\r\npage = pfn_to_page(pfn);\r\nif (cpu_context(smp_processor_id(), vma->vm_mm) == NO_CONTEXT)\r\nreturn;\r\npgd = pgd_offset(vma->vm_mm, address);\r\npud = pud_offset(pgd, address);\r\npmd = pmd_offset(pud, address);\r\npte = pte_offset_kernel(pmd, address);\r\nif (!(pte_val(*pte) & _PAGE_PRESENT))\r\nreturn;\r\nif ((vma->vm_mm == current->active_mm))\r\nvaddr = NULL;\r\nelse {\r\nmap_coherent = (current_cpu_data.dcache.n_aliases &&\r\ntest_bit(PG_dcache_clean, &page->flags) &&\r\npage_mapped(page));\r\nif (map_coherent)\r\nvaddr = kmap_coherent(page, address);\r\nelse\r\nvaddr = kmap_atomic(page, KM_USER0);\r\naddress = (unsigned long)vaddr;\r\n}\r\nflush_cache_one(CACHE_OC_ADDRESS_ARRAY |\r\n(address & shm_align_mask), phys);\r\nif (vma->vm_flags & VM_EXEC)\r\nflush_icache_all();\r\nif (vaddr) {\r\nif (map_coherent)\r\nkunmap_coherent(vaddr);\r\nelse\r\nkunmap_atomic(vaddr, KM_USER0);\r\n}\r\n}\r\nstatic void sh4_flush_cache_range(void *args)\r\n{\r\nstruct flusher_data *data = args;\r\nstruct vm_area_struct *vma;\r\nunsigned long start, end;\r\nvma = data->vma;\r\nstart = data->addr1;\r\nend = data->addr2;\r\nif (cpu_context(smp_processor_id(), vma->vm_mm) == NO_CONTEXT)\r\nreturn;\r\nif (boot_cpu_data.dcache.n_aliases == 0)\r\nreturn;\r\nflush_dcache_all();\r\nif (vma->vm_flags & VM_EXEC)\r\nflush_icache_all();\r\n}\r\nstatic void __flush_cache_one(unsigned long addr, unsigned long phys,\r\nunsigned long exec_offset)\r\n{\r\nint way_count;\r\nunsigned long base_addr = addr;\r\nstruct cache_info *dcache;\r\nunsigned long way_incr;\r\nunsigned long a, ea, p;\r\nunsigned long temp_pc;\r\ndcache = &boot_cpu_data.dcache;\r\nway_count = dcache->ways;\r\nway_incr = dcache->way_incr;\r\nasm volatile("mov.l 1f, %0\n\t"\r\n"add %1, %0\n\t"\r\n"jmp @%0\n\t"\r\n"nop\n\t"\r\n".balign 4\n\t"\r\n"1: .long 2f\n\t"\r\n"2:\n" : "=&r" (temp_pc) : "r" (exec_offset));\r\ndo {\r\nea = base_addr + PAGE_SIZE;\r\na = base_addr;\r\np = phys;\r\ndo {\r\n*(volatile unsigned long *)a = p;\r\n*(volatile unsigned long *)(a+32) = p;\r\na += 64;\r\np += 64;\r\n} while (a < ea);\r\nbase_addr += way_incr;\r\n} while (--way_count != 0);\r\n}\r\nvoid __init sh4_cache_init(void)\r\n{\r\nprintk("PVR=%08x CVR=%08x PRR=%08x\n",\r\n__raw_readl(CCN_PVR),\r\n__raw_readl(CCN_CVR),\r\n__raw_readl(CCN_PRR));\r\nlocal_flush_icache_range = sh4_flush_icache_range;\r\nlocal_flush_dcache_page = sh4_flush_dcache_page;\r\nlocal_flush_cache_all = sh4_flush_cache_all;\r\nlocal_flush_cache_mm = sh4_flush_cache_mm;\r\nlocal_flush_cache_dup_mm = sh4_flush_cache_mm;\r\nlocal_flush_cache_page = sh4_flush_cache_page;\r\nlocal_flush_cache_range = sh4_flush_cache_range;\r\nsh4__flush_region_init();\r\n}
