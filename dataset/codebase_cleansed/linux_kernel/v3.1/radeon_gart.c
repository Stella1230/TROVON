int radeon_gart_table_ram_alloc(struct radeon_device *rdev)\r\n{\r\nvoid *ptr;\r\nptr = pci_alloc_consistent(rdev->pdev, rdev->gart.table_size,\r\n&rdev->gart.table_addr);\r\nif (ptr == NULL) {\r\nreturn -ENOMEM;\r\n}\r\n#ifdef CONFIG_X86\r\nif (rdev->family == CHIP_RS400 || rdev->family == CHIP_RS480 ||\r\nrdev->family == CHIP_RS690 || rdev->family == CHIP_RS740) {\r\nset_memory_uc((unsigned long)ptr,\r\nrdev->gart.table_size >> PAGE_SHIFT);\r\n}\r\n#endif\r\nrdev->gart.table.ram.ptr = ptr;\r\nmemset((void *)rdev->gart.table.ram.ptr, 0, rdev->gart.table_size);\r\nreturn 0;\r\n}\r\nvoid radeon_gart_table_ram_free(struct radeon_device *rdev)\r\n{\r\nif (rdev->gart.table.ram.ptr == NULL) {\r\nreturn;\r\n}\r\n#ifdef CONFIG_X86\r\nif (rdev->family == CHIP_RS400 || rdev->family == CHIP_RS480 ||\r\nrdev->family == CHIP_RS690 || rdev->family == CHIP_RS740) {\r\nset_memory_wb((unsigned long)rdev->gart.table.ram.ptr,\r\nrdev->gart.table_size >> PAGE_SHIFT);\r\n}\r\n#endif\r\npci_free_consistent(rdev->pdev, rdev->gart.table_size,\r\n(void *)rdev->gart.table.ram.ptr,\r\nrdev->gart.table_addr);\r\nrdev->gart.table.ram.ptr = NULL;\r\nrdev->gart.table_addr = 0;\r\n}\r\nint radeon_gart_table_vram_alloc(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (rdev->gart.table.vram.robj == NULL) {\r\nr = radeon_bo_create(rdev, rdev->gart.table_size,\r\nPAGE_SIZE, true, RADEON_GEM_DOMAIN_VRAM,\r\n&rdev->gart.table.vram.robj);\r\nif (r) {\r\nreturn r;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint radeon_gart_table_vram_pin(struct radeon_device *rdev)\r\n{\r\nuint64_t gpu_addr;\r\nint r;\r\nr = radeon_bo_reserve(rdev->gart.table.vram.robj, false);\r\nif (unlikely(r != 0))\r\nreturn r;\r\nr = radeon_bo_pin(rdev->gart.table.vram.robj,\r\nRADEON_GEM_DOMAIN_VRAM, &gpu_addr);\r\nif (r) {\r\nradeon_bo_unreserve(rdev->gart.table.vram.robj);\r\nreturn r;\r\n}\r\nr = radeon_bo_kmap(rdev->gart.table.vram.robj,\r\n(void **)&rdev->gart.table.vram.ptr);\r\nif (r)\r\nradeon_bo_unpin(rdev->gart.table.vram.robj);\r\nradeon_bo_unreserve(rdev->gart.table.vram.robj);\r\nrdev->gart.table_addr = gpu_addr;\r\nreturn r;\r\n}\r\nvoid radeon_gart_table_vram_free(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (rdev->gart.table.vram.robj == NULL) {\r\nreturn;\r\n}\r\nr = radeon_bo_reserve(rdev->gart.table.vram.robj, false);\r\nif (likely(r == 0)) {\r\nradeon_bo_kunmap(rdev->gart.table.vram.robj);\r\nradeon_bo_unpin(rdev->gart.table.vram.robj);\r\nradeon_bo_unreserve(rdev->gart.table.vram.robj);\r\n}\r\nradeon_bo_unref(&rdev->gart.table.vram.robj);\r\n}\r\nvoid radeon_gart_unbind(struct radeon_device *rdev, unsigned offset,\r\nint pages)\r\n{\r\nunsigned t;\r\nunsigned p;\r\nint i, j;\r\nu64 page_base;\r\nif (!rdev->gart.ready) {\r\nWARN(1, "trying to unbind memory to unitialized GART !\n");\r\nreturn;\r\n}\r\nt = offset / RADEON_GPU_PAGE_SIZE;\r\np = t / (PAGE_SIZE / RADEON_GPU_PAGE_SIZE);\r\nfor (i = 0; i < pages; i++, p++) {\r\nif (rdev->gart.pages[p]) {\r\nif (!rdev->gart.ttm_alloced[p])\r\npci_unmap_page(rdev->pdev, rdev->gart.pages_addr[p],\r\nPAGE_SIZE, PCI_DMA_BIDIRECTIONAL);\r\nrdev->gart.pages[p] = NULL;\r\nrdev->gart.pages_addr[p] = rdev->dummy_page.addr;\r\npage_base = rdev->gart.pages_addr[p];\r\nfor (j = 0; j < (PAGE_SIZE / RADEON_GPU_PAGE_SIZE); j++, t++) {\r\nradeon_gart_set_page(rdev, t, page_base);\r\npage_base += RADEON_GPU_PAGE_SIZE;\r\n}\r\n}\r\n}\r\nmb();\r\nradeon_gart_tlb_flush(rdev);\r\n}\r\nint radeon_gart_bind(struct radeon_device *rdev, unsigned offset,\r\nint pages, struct page **pagelist, dma_addr_t *dma_addr)\r\n{\r\nunsigned t;\r\nunsigned p;\r\nuint64_t page_base;\r\nint i, j;\r\nif (!rdev->gart.ready) {\r\nWARN(1, "trying to bind memory to unitialized GART !\n");\r\nreturn -EINVAL;\r\n}\r\nt = offset / RADEON_GPU_PAGE_SIZE;\r\np = t / (PAGE_SIZE / RADEON_GPU_PAGE_SIZE);\r\nfor (i = 0; i < pages; i++, p++) {\r\nif (0) {\r\nrdev->gart.ttm_alloced[p] = true;\r\nrdev->gart.pages_addr[p] = dma_addr[i];\r\n} else {\r\nrdev->gart.pages_addr[p] = pci_map_page(rdev->pdev, pagelist[i],\r\n0, PAGE_SIZE,\r\nPCI_DMA_BIDIRECTIONAL);\r\nif (pci_dma_mapping_error(rdev->pdev, rdev->gart.pages_addr[p])) {\r\nradeon_gart_unbind(rdev, offset, pages);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nrdev->gart.pages[p] = pagelist[i];\r\npage_base = rdev->gart.pages_addr[p];\r\nfor (j = 0; j < (PAGE_SIZE / RADEON_GPU_PAGE_SIZE); j++, t++) {\r\nradeon_gart_set_page(rdev, t, page_base);\r\npage_base += RADEON_GPU_PAGE_SIZE;\r\n}\r\n}\r\nmb();\r\nradeon_gart_tlb_flush(rdev);\r\nreturn 0;\r\n}\r\nvoid radeon_gart_restore(struct radeon_device *rdev)\r\n{\r\nint i, j, t;\r\nu64 page_base;\r\nfor (i = 0, t = 0; i < rdev->gart.num_cpu_pages; i++) {\r\npage_base = rdev->gart.pages_addr[i];\r\nfor (j = 0; j < (PAGE_SIZE / RADEON_GPU_PAGE_SIZE); j++, t++) {\r\nradeon_gart_set_page(rdev, t, page_base);\r\npage_base += RADEON_GPU_PAGE_SIZE;\r\n}\r\n}\r\nmb();\r\nradeon_gart_tlb_flush(rdev);\r\n}\r\nint radeon_gart_init(struct radeon_device *rdev)\r\n{\r\nint r, i;\r\nif (rdev->gart.pages) {\r\nreturn 0;\r\n}\r\nif (PAGE_SIZE < RADEON_GPU_PAGE_SIZE) {\r\nDRM_ERROR("Page size is smaller than GPU page size!\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_dummy_page_init(rdev);\r\nif (r)\r\nreturn r;\r\nrdev->gart.num_cpu_pages = rdev->mc.gtt_size / PAGE_SIZE;\r\nrdev->gart.num_gpu_pages = rdev->mc.gtt_size / RADEON_GPU_PAGE_SIZE;\r\nDRM_INFO("GART: num cpu pages %u, num gpu pages %u\n",\r\nrdev->gart.num_cpu_pages, rdev->gart.num_gpu_pages);\r\nrdev->gart.pages = kzalloc(sizeof(void *) * rdev->gart.num_cpu_pages,\r\nGFP_KERNEL);\r\nif (rdev->gart.pages == NULL) {\r\nradeon_gart_fini(rdev);\r\nreturn -ENOMEM;\r\n}\r\nrdev->gart.pages_addr = kzalloc(sizeof(dma_addr_t) *\r\nrdev->gart.num_cpu_pages, GFP_KERNEL);\r\nif (rdev->gart.pages_addr == NULL) {\r\nradeon_gart_fini(rdev);\r\nreturn -ENOMEM;\r\n}\r\nrdev->gart.ttm_alloced = kzalloc(sizeof(bool) *\r\nrdev->gart.num_cpu_pages, GFP_KERNEL);\r\nif (rdev->gart.ttm_alloced == NULL) {\r\nradeon_gart_fini(rdev);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < rdev->gart.num_cpu_pages; i++) {\r\nrdev->gart.pages_addr[i] = rdev->dummy_page.addr;\r\n}\r\nreturn 0;\r\n}\r\nvoid radeon_gart_fini(struct radeon_device *rdev)\r\n{\r\nif (rdev->gart.pages && rdev->gart.pages_addr && rdev->gart.ready) {\r\nradeon_gart_unbind(rdev, 0, rdev->gart.num_cpu_pages);\r\n}\r\nrdev->gart.ready = false;\r\nkfree(rdev->gart.pages);\r\nkfree(rdev->gart.pages_addr);\r\nkfree(rdev->gart.ttm_alloced);\r\nrdev->gart.pages = NULL;\r\nrdev->gart.pages_addr = NULL;\r\nrdev->gart.ttm_alloced = NULL;\r\nradeon_dummy_page_fini(rdev);\r\n}
