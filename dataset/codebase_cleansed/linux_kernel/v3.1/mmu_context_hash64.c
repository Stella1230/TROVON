void switch_cop(struct mm_struct *next)\r\n{\r\nmtspr(SPRN_PID, next->context.cop_pid);\r\nmtspr(SPRN_ACOP, next->context.acop);\r\n}\r\nstatic int new_cop_pid(struct ida *ida, int min_id, int max_id,\r\nspinlock_t *lock)\r\n{\r\nint index;\r\nint err;\r\nagain:\r\nif (!ida_pre_get(ida, GFP_KERNEL))\r\nreturn -ENOMEM;\r\nspin_lock(lock);\r\nerr = ida_get_new_above(ida, min_id, &index);\r\nspin_unlock(lock);\r\nif (err == -EAGAIN)\r\ngoto again;\r\nelse if (err)\r\nreturn err;\r\nif (index > max_id) {\r\nspin_lock(lock);\r\nida_remove(ida, index);\r\nspin_unlock(lock);\r\nreturn -ENOMEM;\r\n}\r\nreturn index;\r\n}\r\nstatic void sync_cop(void *arg)\r\n{\r\nstruct mm_struct *mm = arg;\r\nif (mm == current->active_mm)\r\nswitch_cop(current->active_mm);\r\n}\r\nint use_cop(unsigned long acop, struct mm_struct *mm)\r\n{\r\nint ret;\r\nif (!cpu_has_feature(CPU_FTR_ICSWX))\r\nreturn -ENODEV;\r\nif (!mm || !acop)\r\nreturn -EINVAL;\r\ndown_read(&mm->mmap_sem);\r\nspin_lock(mm->context.cop_lockp);\r\nif (mm->context.cop_pid == COP_PID_NONE) {\r\nret = new_cop_pid(&cop_ida, COP_PID_MIN, COP_PID_MAX,\r\n&mmu_context_acop_lock);\r\nif (ret < 0)\r\ngoto out;\r\nmm->context.cop_pid = ret;\r\n}\r\nmm->context.acop |= acop;\r\nsync_cop(mm);\r\nif (atomic_read(&mm->mm_users) > 1)\r\nsmp_call_function(sync_cop, mm, 1);\r\nret = mm->context.cop_pid;\r\nout:\r\nspin_unlock(mm->context.cop_lockp);\r\nup_read(&mm->mmap_sem);\r\nreturn ret;\r\n}\r\nvoid drop_cop(unsigned long acop, struct mm_struct *mm)\r\n{\r\nint free_pid = COP_PID_NONE;\r\nif (!cpu_has_feature(CPU_FTR_ICSWX))\r\nreturn;\r\nif (WARN_ON_ONCE(!mm))\r\nreturn;\r\ndown_read(&mm->mmap_sem);\r\nspin_lock(mm->context.cop_lockp);\r\nmm->context.acop &= ~acop;\r\nif ((!mm->context.acop) && (mm->context.cop_pid != COP_PID_NONE)) {\r\nfree_pid = mm->context.cop_pid;\r\nmm->context.cop_pid = COP_PID_NONE;\r\n}\r\nsync_cop(mm);\r\nif (atomic_read(&mm->mm_users) > 1)\r\nsmp_call_function(sync_cop, mm, 1);\r\nif (free_pid != COP_PID_NONE) {\r\nspin_lock(&mmu_context_acop_lock);\r\nida_remove(&cop_ida, free_pid);\r\nspin_unlock(&mmu_context_acop_lock);\r\n}\r\nspin_unlock(mm->context.cop_lockp);\r\nup_read(&mm->mmap_sem);\r\n}\r\nint __init_new_context(void)\r\n{\r\nint index;\r\nint err;\r\nagain:\r\nif (!ida_pre_get(&mmu_context_ida, GFP_KERNEL))\r\nreturn -ENOMEM;\r\nspin_lock(&mmu_context_lock);\r\nerr = ida_get_new_above(&mmu_context_ida, 1, &index);\r\nspin_unlock(&mmu_context_lock);\r\nif (err == -EAGAIN)\r\ngoto again;\r\nelse if (err)\r\nreturn err;\r\nif (index > MAX_CONTEXT) {\r\nspin_lock(&mmu_context_lock);\r\nida_remove(&mmu_context_ida, index);\r\nspin_unlock(&mmu_context_lock);\r\nreturn -ENOMEM;\r\n}\r\nreturn index;\r\n}\r\nint init_new_context(struct task_struct *tsk, struct mm_struct *mm)\r\n{\r\nint index;\r\nindex = __init_new_context();\r\nif (index < 0)\r\nreturn index;\r\nif (slice_mm_new_context(mm))\r\nslice_set_user_psize(mm, mmu_virtual_psize);\r\nsubpage_prot_init_new_context(mm);\r\nmm->context.id = index;\r\n#ifdef CONFIG_PPC_ICSWX\r\nmm->context.cop_lockp = kmalloc(sizeof(spinlock_t), GFP_KERNEL);\r\nif (!mm->context.cop_lockp) {\r\n__destroy_context(index);\r\nsubpage_prot_free(mm);\r\nmm->context.id = MMU_NO_CONTEXT;\r\nreturn -ENOMEM;\r\n}\r\nspin_lock_init(mm->context.cop_lockp);\r\n#endif\r\nreturn 0;\r\n}\r\nvoid __destroy_context(int context_id)\r\n{\r\nspin_lock(&mmu_context_lock);\r\nida_remove(&mmu_context_ida, context_id);\r\nspin_unlock(&mmu_context_lock);\r\n}\r\nvoid destroy_context(struct mm_struct *mm)\r\n{\r\n#ifdef CONFIG_PPC_ICSWX\r\ndrop_cop(mm->context.acop, mm);\r\nkfree(mm->context.cop_lockp);\r\nmm->context.cop_lockp = NULL;\r\n#endif\r\n__destroy_context(mm->context.id);\r\nsubpage_prot_free(mm);\r\nmm->context.id = MMU_NO_CONTEXT;\r\n}
