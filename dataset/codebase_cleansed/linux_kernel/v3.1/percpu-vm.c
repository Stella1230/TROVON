static struct page *pcpu_chunk_page(struct pcpu_chunk *chunk,\r\nunsigned int cpu, int page_idx)\r\n{\r\nWARN_ON(chunk->immutable);\r\nreturn vmalloc_to_page((void *)pcpu_chunk_addr(chunk, cpu, page_idx));\r\n}\r\nstatic struct page **pcpu_get_pages_and_bitmap(struct pcpu_chunk *chunk,\r\nunsigned long **bitmapp,\r\nbool may_alloc)\r\n{\r\nstatic struct page **pages;\r\nstatic unsigned long *bitmap;\r\nsize_t pages_size = pcpu_nr_units * pcpu_unit_pages * sizeof(pages[0]);\r\nsize_t bitmap_size = BITS_TO_LONGS(pcpu_unit_pages) *\r\nsizeof(unsigned long);\r\nif (!pages || !bitmap) {\r\nif (may_alloc && !pages)\r\npages = pcpu_mem_alloc(pages_size);\r\nif (may_alloc && !bitmap)\r\nbitmap = pcpu_mem_alloc(bitmap_size);\r\nif (!pages || !bitmap)\r\nreturn NULL;\r\n}\r\nmemset(pages, 0, pages_size);\r\nbitmap_copy(bitmap, chunk->populated, pcpu_unit_pages);\r\n*bitmapp = bitmap;\r\nreturn pages;\r\n}\r\nstatic void pcpu_free_pages(struct pcpu_chunk *chunk,\r\nstruct page **pages, unsigned long *populated,\r\nint page_start, int page_end)\r\n{\r\nunsigned int cpu;\r\nint i;\r\nfor_each_possible_cpu(cpu) {\r\nfor (i = page_start; i < page_end; i++) {\r\nstruct page *page = pages[pcpu_page_idx(cpu, i)];\r\nif (page)\r\n__free_page(page);\r\n}\r\n}\r\n}\r\nstatic int pcpu_alloc_pages(struct pcpu_chunk *chunk,\r\nstruct page **pages, unsigned long *populated,\r\nint page_start, int page_end)\r\n{\r\nconst gfp_t gfp = GFP_KERNEL | __GFP_HIGHMEM | __GFP_COLD;\r\nunsigned int cpu;\r\nint i;\r\nfor_each_possible_cpu(cpu) {\r\nfor (i = page_start; i < page_end; i++) {\r\nstruct page **pagep = &pages[pcpu_page_idx(cpu, i)];\r\n*pagep = alloc_pages_node(cpu_to_node(cpu), gfp, 0);\r\nif (!*pagep) {\r\npcpu_free_pages(chunk, pages, populated,\r\npage_start, page_end);\r\nreturn -ENOMEM;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void pcpu_pre_unmap_flush(struct pcpu_chunk *chunk,\r\nint page_start, int page_end)\r\n{\r\nflush_cache_vunmap(\r\npcpu_chunk_addr(chunk, pcpu_first_unit_cpu, page_start),\r\npcpu_chunk_addr(chunk, pcpu_last_unit_cpu, page_end));\r\n}\r\nstatic void __pcpu_unmap_pages(unsigned long addr, int nr_pages)\r\n{\r\nunmap_kernel_range_noflush(addr, nr_pages << PAGE_SHIFT);\r\n}\r\nstatic void pcpu_unmap_pages(struct pcpu_chunk *chunk,\r\nstruct page **pages, unsigned long *populated,\r\nint page_start, int page_end)\r\n{\r\nunsigned int cpu;\r\nint i;\r\nfor_each_possible_cpu(cpu) {\r\nfor (i = page_start; i < page_end; i++) {\r\nstruct page *page;\r\npage = pcpu_chunk_page(chunk, cpu, i);\r\nWARN_ON(!page);\r\npages[pcpu_page_idx(cpu, i)] = page;\r\n}\r\n__pcpu_unmap_pages(pcpu_chunk_addr(chunk, cpu, page_start),\r\npage_end - page_start);\r\n}\r\nfor (i = page_start; i < page_end; i++)\r\n__clear_bit(i, populated);\r\n}\r\nstatic void pcpu_post_unmap_tlb_flush(struct pcpu_chunk *chunk,\r\nint page_start, int page_end)\r\n{\r\nflush_tlb_kernel_range(\r\npcpu_chunk_addr(chunk, pcpu_first_unit_cpu, page_start),\r\npcpu_chunk_addr(chunk, pcpu_last_unit_cpu, page_end));\r\n}\r\nstatic int __pcpu_map_pages(unsigned long addr, struct page **pages,\r\nint nr_pages)\r\n{\r\nreturn map_kernel_range_noflush(addr, nr_pages << PAGE_SHIFT,\r\nPAGE_KERNEL, pages);\r\n}\r\nstatic int pcpu_map_pages(struct pcpu_chunk *chunk,\r\nstruct page **pages, unsigned long *populated,\r\nint page_start, int page_end)\r\n{\r\nunsigned int cpu, tcpu;\r\nint i, err;\r\nfor_each_possible_cpu(cpu) {\r\nerr = __pcpu_map_pages(pcpu_chunk_addr(chunk, cpu, page_start),\r\n&pages[pcpu_page_idx(cpu, page_start)],\r\npage_end - page_start);\r\nif (err < 0)\r\ngoto err;\r\n}\r\nfor (i = page_start; i < page_end; i++) {\r\nfor_each_possible_cpu(cpu)\r\npcpu_set_page_chunk(pages[pcpu_page_idx(cpu, i)],\r\nchunk);\r\n__set_bit(i, populated);\r\n}\r\nreturn 0;\r\nerr:\r\nfor_each_possible_cpu(tcpu) {\r\nif (tcpu == cpu)\r\nbreak;\r\n__pcpu_unmap_pages(pcpu_chunk_addr(chunk, tcpu, page_start),\r\npage_end - page_start);\r\n}\r\nreturn err;\r\n}\r\nstatic void pcpu_post_map_flush(struct pcpu_chunk *chunk,\r\nint page_start, int page_end)\r\n{\r\nflush_cache_vmap(\r\npcpu_chunk_addr(chunk, pcpu_first_unit_cpu, page_start),\r\npcpu_chunk_addr(chunk, pcpu_last_unit_cpu, page_end));\r\n}\r\nstatic int pcpu_populate_chunk(struct pcpu_chunk *chunk, int off, int size)\r\n{\r\nint page_start = PFN_DOWN(off);\r\nint page_end = PFN_UP(off + size);\r\nint free_end = page_start, unmap_end = page_start;\r\nstruct page **pages;\r\nunsigned long *populated;\r\nunsigned int cpu;\r\nint rs, re, rc;\r\nrs = page_start;\r\npcpu_next_pop(chunk, &rs, &re, page_end);\r\nif (rs == page_start && re == page_end)\r\ngoto clear;\r\nWARN_ON(chunk->immutable);\r\npages = pcpu_get_pages_and_bitmap(chunk, &populated, true);\r\nif (!pages)\r\nreturn -ENOMEM;\r\npcpu_for_each_unpop_region(chunk, rs, re, page_start, page_end) {\r\nrc = pcpu_alloc_pages(chunk, pages, populated, rs, re);\r\nif (rc)\r\ngoto err_free;\r\nfree_end = re;\r\n}\r\npcpu_for_each_unpop_region(chunk, rs, re, page_start, page_end) {\r\nrc = pcpu_map_pages(chunk, pages, populated, rs, re);\r\nif (rc)\r\ngoto err_unmap;\r\nunmap_end = re;\r\n}\r\npcpu_post_map_flush(chunk, page_start, page_end);\r\nbitmap_copy(chunk->populated, populated, pcpu_unit_pages);\r\nclear:\r\nfor_each_possible_cpu(cpu)\r\nmemset((void *)pcpu_chunk_addr(chunk, cpu, 0) + off, 0, size);\r\nreturn 0;\r\nerr_unmap:\r\npcpu_pre_unmap_flush(chunk, page_start, unmap_end);\r\npcpu_for_each_unpop_region(chunk, rs, re, page_start, unmap_end)\r\npcpu_unmap_pages(chunk, pages, populated, rs, re);\r\npcpu_post_unmap_tlb_flush(chunk, page_start, unmap_end);\r\nerr_free:\r\npcpu_for_each_unpop_region(chunk, rs, re, page_start, free_end)\r\npcpu_free_pages(chunk, pages, populated, rs, re);\r\nreturn rc;\r\n}\r\nstatic void pcpu_depopulate_chunk(struct pcpu_chunk *chunk, int off, int size)\r\n{\r\nint page_start = PFN_DOWN(off);\r\nint page_end = PFN_UP(off + size);\r\nstruct page **pages;\r\nunsigned long *populated;\r\nint rs, re;\r\nrs = page_start;\r\npcpu_next_unpop(chunk, &rs, &re, page_end);\r\nif (rs == page_start && re == page_end)\r\nreturn;\r\nWARN_ON(chunk->immutable);\r\npages = pcpu_get_pages_and_bitmap(chunk, &populated, false);\r\nBUG_ON(!pages);\r\npcpu_pre_unmap_flush(chunk, page_start, page_end);\r\npcpu_for_each_pop_region(chunk, rs, re, page_start, page_end)\r\npcpu_unmap_pages(chunk, pages, populated, rs, re);\r\npcpu_for_each_pop_region(chunk, rs, re, page_start, page_end)\r\npcpu_free_pages(chunk, pages, populated, rs, re);\r\nbitmap_copy(chunk->populated, populated, pcpu_unit_pages);\r\n}\r\nstatic struct pcpu_chunk *pcpu_create_chunk(void)\r\n{\r\nstruct pcpu_chunk *chunk;\r\nstruct vm_struct **vms;\r\nchunk = pcpu_alloc_chunk();\r\nif (!chunk)\r\nreturn NULL;\r\nvms = pcpu_get_vm_areas(pcpu_group_offsets, pcpu_group_sizes,\r\npcpu_nr_groups, pcpu_atom_size);\r\nif (!vms) {\r\npcpu_free_chunk(chunk);\r\nreturn NULL;\r\n}\r\nchunk->data = vms;\r\nchunk->base_addr = vms[0]->addr - pcpu_group_offsets[0];\r\nreturn chunk;\r\n}\r\nstatic void pcpu_destroy_chunk(struct pcpu_chunk *chunk)\r\n{\r\nif (chunk && chunk->data)\r\npcpu_free_vm_areas(chunk->data, pcpu_nr_groups);\r\npcpu_free_chunk(chunk);\r\n}\r\nstatic struct page *pcpu_addr_to_page(void *addr)\r\n{\r\nreturn vmalloc_to_page(addr);\r\n}\r\nstatic int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)\r\n{\r\nreturn 0;\r\n}
