static void mark_page_used(struct mtdoops_context *cxt, int page)\r\n{\r\nset_bit(page, cxt->oops_page_used);\r\n}\r\nstatic void mark_page_unused(struct mtdoops_context *cxt, int page)\r\n{\r\nclear_bit(page, cxt->oops_page_used);\r\n}\r\nstatic int page_is_used(struct mtdoops_context *cxt, int page)\r\n{\r\nreturn test_bit(page, cxt->oops_page_used);\r\n}\r\nstatic void mtdoops_erase_callback(struct erase_info *done)\r\n{\r\nwait_queue_head_t *wait_q = (wait_queue_head_t *)done->priv;\r\nwake_up(wait_q);\r\n}\r\nstatic int mtdoops_erase_block(struct mtdoops_context *cxt, int offset)\r\n{\r\nstruct mtd_info *mtd = cxt->mtd;\r\nu32 start_page_offset = mtd_div_by_eb(offset, mtd) * mtd->erasesize;\r\nu32 start_page = start_page_offset / record_size;\r\nu32 erase_pages = mtd->erasesize / record_size;\r\nstruct erase_info erase;\r\nDECLARE_WAITQUEUE(wait, current);\r\nwait_queue_head_t wait_q;\r\nint ret;\r\nint page;\r\ninit_waitqueue_head(&wait_q);\r\nerase.mtd = mtd;\r\nerase.callback = mtdoops_erase_callback;\r\nerase.addr = offset;\r\nerase.len = mtd->erasesize;\r\nerase.priv = (u_long)&wait_q;\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nadd_wait_queue(&wait_q, &wait);\r\nret = mtd->erase(mtd, &erase);\r\nif (ret) {\r\nset_current_state(TASK_RUNNING);\r\nremove_wait_queue(&wait_q, &wait);\r\nprintk(KERN_WARNING "mtdoops: erase of region [0x%llx, 0x%llx] on \"%s\" failed\n",\r\n(unsigned long long)erase.addr,\r\n(unsigned long long)erase.len, mtddev);\r\nreturn ret;\r\n}\r\nschedule();\r\nremove_wait_queue(&wait_q, &wait);\r\nfor (page = start_page; page < start_page + erase_pages; page++)\r\nmark_page_unused(cxt, page);\r\nreturn 0;\r\n}\r\nstatic void mtdoops_inc_counter(struct mtdoops_context *cxt)\r\n{\r\ncxt->nextpage++;\r\nif (cxt->nextpage >= cxt->oops_pages)\r\ncxt->nextpage = 0;\r\ncxt->nextcount++;\r\nif (cxt->nextcount == 0xffffffff)\r\ncxt->nextcount = 0;\r\nif (page_is_used(cxt, cxt->nextpage)) {\r\nschedule_work(&cxt->work_erase);\r\nreturn;\r\n}\r\nprintk(KERN_DEBUG "mtdoops: ready %d, %d (no erase)\n",\r\ncxt->nextpage, cxt->nextcount);\r\n}\r\nstatic void mtdoops_workfunc_erase(struct work_struct *work)\r\n{\r\nstruct mtdoops_context *cxt =\r\ncontainer_of(work, struct mtdoops_context, work_erase);\r\nstruct mtd_info *mtd = cxt->mtd;\r\nint i = 0, j, ret, mod;\r\nif (!mtd)\r\nreturn;\r\nmod = (cxt->nextpage * record_size) % mtd->erasesize;\r\nif (mod != 0) {\r\ncxt->nextpage = cxt->nextpage + ((mtd->erasesize - mod) / record_size);\r\nif (cxt->nextpage >= cxt->oops_pages)\r\ncxt->nextpage = 0;\r\n}\r\nwhile (mtd->block_isbad) {\r\nret = mtd->block_isbad(mtd, cxt->nextpage * record_size);\r\nif (!ret)\r\nbreak;\r\nif (ret < 0) {\r\nprintk(KERN_ERR "mtdoops: block_isbad failed, aborting\n");\r\nreturn;\r\n}\r\nbadblock:\r\nprintk(KERN_WARNING "mtdoops: bad block at %08lx\n",\r\ncxt->nextpage * record_size);\r\ni++;\r\ncxt->nextpage = cxt->nextpage + (mtd->erasesize / record_size);\r\nif (cxt->nextpage >= cxt->oops_pages)\r\ncxt->nextpage = 0;\r\nif (i == cxt->oops_pages / (mtd->erasesize / record_size)) {\r\nprintk(KERN_ERR "mtdoops: all blocks bad!\n");\r\nreturn;\r\n}\r\n}\r\nfor (j = 0, ret = -1; (j < 3) && (ret < 0); j++)\r\nret = mtdoops_erase_block(cxt, cxt->nextpage * record_size);\r\nif (ret >= 0) {\r\nprintk(KERN_DEBUG "mtdoops: ready %d, %d\n",\r\ncxt->nextpage, cxt->nextcount);\r\nreturn;\r\n}\r\nif (mtd->block_markbad && ret == -EIO) {\r\nret = mtd->block_markbad(mtd, cxt->nextpage * record_size);\r\nif (ret < 0) {\r\nprintk(KERN_ERR "mtdoops: block_markbad failed, aborting\n");\r\nreturn;\r\n}\r\n}\r\ngoto badblock;\r\n}\r\nstatic void mtdoops_write(struct mtdoops_context *cxt, int panic)\r\n{\r\nstruct mtd_info *mtd = cxt->mtd;\r\nsize_t retlen;\r\nu32 *hdr;\r\nint ret;\r\nhdr = cxt->oops_buf;\r\nhdr[0] = cxt->nextcount;\r\nhdr[1] = MTDOOPS_KERNMSG_MAGIC;\r\nif (panic)\r\nret = mtd->panic_write(mtd, cxt->nextpage * record_size,\r\nrecord_size, &retlen, cxt->oops_buf);\r\nelse\r\nret = mtd->write(mtd, cxt->nextpage * record_size,\r\nrecord_size, &retlen, cxt->oops_buf);\r\nif (retlen != record_size || ret < 0)\r\nprintk(KERN_ERR "mtdoops: write failure at %ld (%td of %ld written), error %d\n",\r\ncxt->nextpage * record_size, retlen, record_size, ret);\r\nmark_page_used(cxt, cxt->nextpage);\r\nmemset(cxt->oops_buf, 0xff, record_size);\r\nmtdoops_inc_counter(cxt);\r\n}\r\nstatic void mtdoops_workfunc_write(struct work_struct *work)\r\n{\r\nstruct mtdoops_context *cxt =\r\ncontainer_of(work, struct mtdoops_context, work_write);\r\nmtdoops_write(cxt, 0);\r\n}\r\nstatic void find_next_position(struct mtdoops_context *cxt)\r\n{\r\nstruct mtd_info *mtd = cxt->mtd;\r\nint ret, page, maxpos = 0;\r\nu32 count[2], maxcount = 0xffffffff;\r\nsize_t retlen;\r\nfor (page = 0; page < cxt->oops_pages; page++) {\r\nmark_page_used(cxt, page);\r\nret = mtd->read(mtd, page * record_size, MTDOOPS_HEADER_SIZE,\r\n&retlen, (u_char *) &count[0]);\r\nif (retlen != MTDOOPS_HEADER_SIZE ||\r\n(ret < 0 && ret != -EUCLEAN)) {\r\nprintk(KERN_ERR "mtdoops: read failure at %ld (%td of %d read), err %d\n",\r\npage * record_size, retlen,\r\nMTDOOPS_HEADER_SIZE, ret);\r\ncontinue;\r\n}\r\nif (count[0] == 0xffffffff && count[1] == 0xffffffff)\r\nmark_page_unused(cxt, page);\r\nif (count[0] == 0xffffffff)\r\ncontinue;\r\nif (maxcount == 0xffffffff) {\r\nmaxcount = count[0];\r\nmaxpos = page;\r\n} else if (count[0] < 0x40000000 && maxcount > 0xc0000000) {\r\nmaxcount = count[0];\r\nmaxpos = page;\r\n} else if (count[0] > maxcount && count[0] < 0xc0000000) {\r\nmaxcount = count[0];\r\nmaxpos = page;\r\n} else if (count[0] > maxcount && count[0] > 0xc0000000\r\n&& maxcount > 0x80000000) {\r\nmaxcount = count[0];\r\nmaxpos = page;\r\n}\r\n}\r\nif (maxcount == 0xffffffff) {\r\ncxt->nextpage = 0;\r\ncxt->nextcount = 1;\r\nschedule_work(&cxt->work_erase);\r\nreturn;\r\n}\r\ncxt->nextpage = maxpos;\r\ncxt->nextcount = maxcount;\r\nmtdoops_inc_counter(cxt);\r\n}\r\nstatic void mtdoops_do_dump(struct kmsg_dumper *dumper,\r\nenum kmsg_dump_reason reason, const char *s1, unsigned long l1,\r\nconst char *s2, unsigned long l2)\r\n{\r\nstruct mtdoops_context *cxt = container_of(dumper,\r\nstruct mtdoops_context, dump);\r\nunsigned long s1_start, s2_start;\r\nunsigned long l1_cpy, l2_cpy;\r\nchar *dst;\r\nif (reason != KMSG_DUMP_OOPS &&\r\nreason != KMSG_DUMP_PANIC &&\r\nreason != KMSG_DUMP_KEXEC)\r\nreturn;\r\nif (reason == KMSG_DUMP_OOPS && !dump_oops)\r\nreturn;\r\ndst = cxt->oops_buf + MTDOOPS_HEADER_SIZE;\r\nl2_cpy = min(l2, record_size - MTDOOPS_HEADER_SIZE);\r\nl1_cpy = min(l1, record_size - MTDOOPS_HEADER_SIZE - l2_cpy);\r\ns2_start = l2 - l2_cpy;\r\ns1_start = l1 - l1_cpy;\r\nmemcpy(dst, s1 + s1_start, l1_cpy);\r\nmemcpy(dst + l1_cpy, s2 + s2_start, l2_cpy);\r\nif (reason != KMSG_DUMP_OOPS) {\r\nif (!cxt->mtd->panic_write)\r\nprintk(KERN_ERR "mtdoops: Cannot write from panic without panic_write\n");\r\nelse\r\nmtdoops_write(cxt, 1);\r\nreturn;\r\n}\r\nschedule_work(&cxt->work_write);\r\n}\r\nstatic void mtdoops_notify_add(struct mtd_info *mtd)\r\n{\r\nstruct mtdoops_context *cxt = &oops_cxt;\r\nu64 mtdoops_pages = div_u64(mtd->size, record_size);\r\nint err;\r\nif (!strcmp(mtd->name, mtddev))\r\ncxt->mtd_index = mtd->index;\r\nif (mtd->index != cxt->mtd_index || cxt->mtd_index < 0)\r\nreturn;\r\nif (mtd->size < mtd->erasesize * 2) {\r\nprintk(KERN_ERR "mtdoops: MTD partition %d not big enough for mtdoops\n",\r\nmtd->index);\r\nreturn;\r\n}\r\nif (mtd->erasesize < record_size) {\r\nprintk(KERN_ERR "mtdoops: eraseblock size of MTD partition %d too small\n",\r\nmtd->index);\r\nreturn;\r\n}\r\nif (mtd->size > MTDOOPS_MAX_MTD_SIZE) {\r\nprintk(KERN_ERR "mtdoops: mtd%d is too large (limit is %d MiB)\n",\r\nmtd->index, MTDOOPS_MAX_MTD_SIZE / 1024 / 1024);\r\nreturn;\r\n}\r\ncxt->oops_page_used = vmalloc(DIV_ROUND_UP(mtdoops_pages,\r\nBITS_PER_LONG));\r\nif (!cxt->oops_page_used) {\r\nprintk(KERN_ERR "mtdoops: could not allocate page array\n");\r\nreturn;\r\n}\r\ncxt->dump.dump = mtdoops_do_dump;\r\nerr = kmsg_dump_register(&cxt->dump);\r\nif (err) {\r\nprintk(KERN_ERR "mtdoops: registering kmsg dumper failed, error %d\n", err);\r\nvfree(cxt->oops_page_used);\r\ncxt->oops_page_used = NULL;\r\nreturn;\r\n}\r\ncxt->mtd = mtd;\r\ncxt->oops_pages = (int)mtd->size / record_size;\r\nfind_next_position(cxt);\r\nprintk(KERN_INFO "mtdoops: Attached to MTD device %d\n", mtd->index);\r\n}\r\nstatic void mtdoops_notify_remove(struct mtd_info *mtd)\r\n{\r\nstruct mtdoops_context *cxt = &oops_cxt;\r\nif (mtd->index != cxt->mtd_index || cxt->mtd_index < 0)\r\nreturn;\r\nif (kmsg_dump_unregister(&cxt->dump) < 0)\r\nprintk(KERN_WARNING "mtdoops: could not unregister kmsg_dumper\n");\r\ncxt->mtd = NULL;\r\nflush_work_sync(&cxt->work_erase);\r\nflush_work_sync(&cxt->work_write);\r\n}\r\nstatic int __init mtdoops_init(void)\r\n{\r\nstruct mtdoops_context *cxt = &oops_cxt;\r\nint mtd_index;\r\nchar *endp;\r\nif (strlen(mtddev) == 0) {\r\nprintk(KERN_ERR "mtdoops: mtd device (mtddev=name/number) must be supplied\n");\r\nreturn -EINVAL;\r\n}\r\nif ((record_size & 4095) != 0) {\r\nprintk(KERN_ERR "mtdoops: record_size must be a multiple of 4096\n");\r\nreturn -EINVAL;\r\n}\r\nif (record_size < 4096) {\r\nprintk(KERN_ERR "mtdoops: record_size must be over 4096 bytes\n");\r\nreturn -EINVAL;\r\n}\r\ncxt->mtd_index = -1;\r\nmtd_index = simple_strtoul(mtddev, &endp, 0);\r\nif (*endp == '\0')\r\ncxt->mtd_index = mtd_index;\r\ncxt->oops_buf = vmalloc(record_size);\r\nif (!cxt->oops_buf) {\r\nprintk(KERN_ERR "mtdoops: failed to allocate buffer workspace\n");\r\nreturn -ENOMEM;\r\n}\r\nmemset(cxt->oops_buf, 0xff, record_size);\r\nINIT_WORK(&cxt->work_erase, mtdoops_workfunc_erase);\r\nINIT_WORK(&cxt->work_write, mtdoops_workfunc_write);\r\nregister_mtd_user(&mtdoops_notifier);\r\nreturn 0;\r\n}\r\nstatic void __exit mtdoops_exit(void)\r\n{\r\nstruct mtdoops_context *cxt = &oops_cxt;\r\nunregister_mtd_user(&mtdoops_notifier);\r\nvfree(cxt->oops_buf);\r\nvfree(cxt->oops_page_used);\r\n}
