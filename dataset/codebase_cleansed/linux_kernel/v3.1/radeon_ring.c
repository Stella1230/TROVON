void radeon_ib_bogus_cleanup(struct radeon_device *rdev)\r\n{\r\nstruct radeon_ib *ib, *n;\r\nlist_for_each_entry_safe(ib, n, &rdev->ib_pool.bogus_ib, list) {\r\nlist_del(&ib->list);\r\nvfree(ib->ptr);\r\nkfree(ib);\r\n}\r\n}\r\nvoid radeon_ib_bogus_add(struct radeon_device *rdev, struct radeon_ib *ib)\r\n{\r\nstruct radeon_ib *bib;\r\nbib = kmalloc(sizeof(*bib), GFP_KERNEL);\r\nif (bib == NULL)\r\nreturn;\r\nbib->ptr = vmalloc(ib->length_dw * 4);\r\nif (bib->ptr == NULL) {\r\nkfree(bib);\r\nreturn;\r\n}\r\nmemcpy(bib->ptr, ib->ptr, ib->length_dw * 4);\r\nbib->length_dw = ib->length_dw;\r\nmutex_lock(&rdev->ib_pool.mutex);\r\nlist_add_tail(&bib->list, &rdev->ib_pool.bogus_ib);\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\n}\r\nint radeon_ib_get(struct radeon_device *rdev, struct radeon_ib **ib)\r\n{\r\nstruct radeon_fence *fence;\r\nstruct radeon_ib *nib;\r\nint r = 0, i, c;\r\n*ib = NULL;\r\nr = radeon_fence_create(rdev, &fence);\r\nif (r) {\r\ndev_err(rdev->dev, "failed to create fence for new IB\n");\r\nreturn r;\r\n}\r\nmutex_lock(&rdev->ib_pool.mutex);\r\nfor (i = rdev->ib_pool.head_id, c = 0, nib = NULL; c < RADEON_IB_POOL_SIZE; c++, i++) {\r\ni &= (RADEON_IB_POOL_SIZE - 1);\r\nif (rdev->ib_pool.ibs[i].free) {\r\nnib = &rdev->ib_pool.ibs[i];\r\nbreak;\r\n}\r\n}\r\nif (nib == NULL) {\r\ndev_err(rdev->dev, "no free indirect buffer !\n");\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\nradeon_fence_unref(&fence);\r\nreturn -EBUSY;\r\n}\r\nrdev->ib_pool.head_id = (nib->idx + 1) & (RADEON_IB_POOL_SIZE - 1);\r\nnib->free = false;\r\nif (nib->fence) {\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\nr = radeon_fence_wait(nib->fence, false);\r\nif (r) {\r\ndev_err(rdev->dev, "error waiting fence of IB(%u:0x%016lX:%u)\n",\r\nnib->idx, (unsigned long)nib->gpu_addr, nib->length_dw);\r\nmutex_lock(&rdev->ib_pool.mutex);\r\nnib->free = true;\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\nradeon_fence_unref(&fence);\r\nreturn r;\r\n}\r\nmutex_lock(&rdev->ib_pool.mutex);\r\n}\r\nradeon_fence_unref(&nib->fence);\r\nnib->fence = fence;\r\nnib->length_dw = 0;\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\n*ib = nib;\r\nreturn 0;\r\n}\r\nvoid radeon_ib_free(struct radeon_device *rdev, struct radeon_ib **ib)\r\n{\r\nstruct radeon_ib *tmp = *ib;\r\n*ib = NULL;\r\nif (tmp == NULL) {\r\nreturn;\r\n}\r\nif (!tmp->fence->emited)\r\nradeon_fence_unref(&tmp->fence);\r\nmutex_lock(&rdev->ib_pool.mutex);\r\ntmp->free = true;\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\n}\r\nint radeon_ib_schedule(struct radeon_device *rdev, struct radeon_ib *ib)\r\n{\r\nint r = 0;\r\nif (!ib->length_dw || !rdev->cp.ready) {\r\nDRM_ERROR("radeon: couldn't schedule IB(%u).\n", ib->idx);\r\nreturn -EINVAL;\r\n}\r\nr = radeon_ring_lock(rdev, 64);\r\nif (r) {\r\nDRM_ERROR("radeon: scheduling IB failed (%d).\n", r);\r\nreturn r;\r\n}\r\nradeon_ring_ib_execute(rdev, ib);\r\nradeon_fence_emit(rdev, ib->fence);\r\nmutex_lock(&rdev->ib_pool.mutex);\r\nib->free = true;\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\nradeon_ring_unlock_commit(rdev);\r\nreturn 0;\r\n}\r\nint radeon_ib_pool_init(struct radeon_device *rdev)\r\n{\r\nvoid *ptr;\r\nuint64_t gpu_addr;\r\nint i;\r\nint r = 0;\r\nif (rdev->ib_pool.robj)\r\nreturn 0;\r\nINIT_LIST_HEAD(&rdev->ib_pool.bogus_ib);\r\nr = radeon_bo_create(rdev, RADEON_IB_POOL_SIZE*64*1024,\r\nPAGE_SIZE, true, RADEON_GEM_DOMAIN_GTT,\r\n&rdev->ib_pool.robj);\r\nif (r) {\r\nDRM_ERROR("radeon: failed to ib pool (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_bo_reserve(rdev->ib_pool.robj, false);\r\nif (unlikely(r != 0))\r\nreturn r;\r\nr = radeon_bo_pin(rdev->ib_pool.robj, RADEON_GEM_DOMAIN_GTT, &gpu_addr);\r\nif (r) {\r\nradeon_bo_unreserve(rdev->ib_pool.robj);\r\nDRM_ERROR("radeon: failed to pin ib pool (%d).\n", r);\r\nreturn r;\r\n}\r\nr = radeon_bo_kmap(rdev->ib_pool.robj, &ptr);\r\nradeon_bo_unreserve(rdev->ib_pool.robj);\r\nif (r) {\r\nDRM_ERROR("radeon: failed to map ib pool (%d).\n", r);\r\nreturn r;\r\n}\r\nfor (i = 0; i < RADEON_IB_POOL_SIZE; i++) {\r\nunsigned offset;\r\noffset = i * 64 * 1024;\r\nrdev->ib_pool.ibs[i].gpu_addr = gpu_addr + offset;\r\nrdev->ib_pool.ibs[i].ptr = ptr + offset;\r\nrdev->ib_pool.ibs[i].idx = i;\r\nrdev->ib_pool.ibs[i].length_dw = 0;\r\nrdev->ib_pool.ibs[i].free = true;\r\n}\r\nrdev->ib_pool.head_id = 0;\r\nrdev->ib_pool.ready = true;\r\nDRM_INFO("radeon: ib pool ready.\n");\r\nif (radeon_debugfs_ib_init(rdev)) {\r\nDRM_ERROR("Failed to register debugfs file for IB !\n");\r\n}\r\nreturn r;\r\n}\r\nvoid radeon_ib_pool_fini(struct radeon_device *rdev)\r\n{\r\nint r;\r\nstruct radeon_bo *robj;\r\nif (!rdev->ib_pool.ready) {\r\nreturn;\r\n}\r\nmutex_lock(&rdev->ib_pool.mutex);\r\nradeon_ib_bogus_cleanup(rdev);\r\nrobj = rdev->ib_pool.robj;\r\nrdev->ib_pool.robj = NULL;\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\nif (robj) {\r\nr = radeon_bo_reserve(robj, false);\r\nif (likely(r == 0)) {\r\nradeon_bo_kunmap(robj);\r\nradeon_bo_unpin(robj);\r\nradeon_bo_unreserve(robj);\r\n}\r\nradeon_bo_unref(&robj);\r\n}\r\n}\r\nvoid radeon_ring_free_size(struct radeon_device *rdev)\r\n{\r\nif (rdev->wb.enabled)\r\nrdev->cp.rptr = le32_to_cpu(rdev->wb.wb[RADEON_WB_CP_RPTR_OFFSET/4]);\r\nelse {\r\nif (rdev->family >= CHIP_R600)\r\nrdev->cp.rptr = RREG32(R600_CP_RB_RPTR);\r\nelse\r\nrdev->cp.rptr = RREG32(RADEON_CP_RB_RPTR);\r\n}\r\nrdev->cp.ring_free_dw = (rdev->cp.rptr + (rdev->cp.ring_size / 4));\r\nrdev->cp.ring_free_dw -= rdev->cp.wptr;\r\nrdev->cp.ring_free_dw &= rdev->cp.ptr_mask;\r\nif (!rdev->cp.ring_free_dw) {\r\nrdev->cp.ring_free_dw = rdev->cp.ring_size / 4;\r\n}\r\n}\r\nint radeon_ring_alloc(struct radeon_device *rdev, unsigned ndw)\r\n{\r\nint r;\r\nndw = (ndw + rdev->cp.align_mask) & ~rdev->cp.align_mask;\r\nwhile (ndw > (rdev->cp.ring_free_dw - 1)) {\r\nradeon_ring_free_size(rdev);\r\nif (ndw < rdev->cp.ring_free_dw) {\r\nbreak;\r\n}\r\nr = radeon_fence_wait_next(rdev);\r\nif (r)\r\nreturn r;\r\n}\r\nrdev->cp.count_dw = ndw;\r\nrdev->cp.wptr_old = rdev->cp.wptr;\r\nreturn 0;\r\n}\r\nint radeon_ring_lock(struct radeon_device *rdev, unsigned ndw)\r\n{\r\nint r;\r\nmutex_lock(&rdev->cp.mutex);\r\nr = radeon_ring_alloc(rdev, ndw);\r\nif (r) {\r\nmutex_unlock(&rdev->cp.mutex);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nvoid radeon_ring_commit(struct radeon_device *rdev)\r\n{\r\nunsigned count_dw_pad;\r\nunsigned i;\r\ncount_dw_pad = (rdev->cp.align_mask + 1) -\r\n(rdev->cp.wptr & rdev->cp.align_mask);\r\nfor (i = 0; i < count_dw_pad; i++) {\r\nradeon_ring_write(rdev, 2 << 30);\r\n}\r\nDRM_MEMORYBARRIER();\r\nradeon_cp_commit(rdev);\r\n}\r\nvoid radeon_ring_unlock_commit(struct radeon_device *rdev)\r\n{\r\nradeon_ring_commit(rdev);\r\nmutex_unlock(&rdev->cp.mutex);\r\n}\r\nvoid radeon_ring_unlock_undo(struct radeon_device *rdev)\r\n{\r\nrdev->cp.wptr = rdev->cp.wptr_old;\r\nmutex_unlock(&rdev->cp.mutex);\r\n}\r\nint radeon_ring_init(struct radeon_device *rdev, unsigned ring_size)\r\n{\r\nint r;\r\nrdev->cp.ring_size = ring_size;\r\nif (rdev->cp.ring_obj == NULL) {\r\nr = radeon_bo_create(rdev, rdev->cp.ring_size, PAGE_SIZE, true,\r\nRADEON_GEM_DOMAIN_GTT,\r\n&rdev->cp.ring_obj);\r\nif (r) {\r\ndev_err(rdev->dev, "(%d) ring create failed\n", r);\r\nreturn r;\r\n}\r\nr = radeon_bo_reserve(rdev->cp.ring_obj, false);\r\nif (unlikely(r != 0))\r\nreturn r;\r\nr = radeon_bo_pin(rdev->cp.ring_obj, RADEON_GEM_DOMAIN_GTT,\r\n&rdev->cp.gpu_addr);\r\nif (r) {\r\nradeon_bo_unreserve(rdev->cp.ring_obj);\r\ndev_err(rdev->dev, "(%d) ring pin failed\n", r);\r\nreturn r;\r\n}\r\nr = radeon_bo_kmap(rdev->cp.ring_obj,\r\n(void **)&rdev->cp.ring);\r\nradeon_bo_unreserve(rdev->cp.ring_obj);\r\nif (r) {\r\ndev_err(rdev->dev, "(%d) ring map failed\n", r);\r\nreturn r;\r\n}\r\n}\r\nrdev->cp.ptr_mask = (rdev->cp.ring_size / 4) - 1;\r\nrdev->cp.ring_free_dw = rdev->cp.ring_size / 4;\r\nreturn 0;\r\n}\r\nvoid radeon_ring_fini(struct radeon_device *rdev)\r\n{\r\nint r;\r\nstruct radeon_bo *ring_obj;\r\nmutex_lock(&rdev->cp.mutex);\r\nring_obj = rdev->cp.ring_obj;\r\nrdev->cp.ring = NULL;\r\nrdev->cp.ring_obj = NULL;\r\nmutex_unlock(&rdev->cp.mutex);\r\nif (ring_obj) {\r\nr = radeon_bo_reserve(ring_obj, false);\r\nif (likely(r == 0)) {\r\nradeon_bo_kunmap(ring_obj);\r\nradeon_bo_unpin(ring_obj);\r\nradeon_bo_unreserve(ring_obj);\r\n}\r\nradeon_bo_unref(&ring_obj);\r\n}\r\n}\r\nstatic int radeon_debugfs_ib_info(struct seq_file *m, void *data)\r\n{\r\nstruct drm_info_node *node = (struct drm_info_node *) m->private;\r\nstruct radeon_ib *ib = node->info_ent->data;\r\nunsigned i;\r\nif (ib == NULL) {\r\nreturn 0;\r\n}\r\nseq_printf(m, "IB %04u\n", ib->idx);\r\nseq_printf(m, "IB fence %p\n", ib->fence);\r\nseq_printf(m, "IB size %05u dwords\n", ib->length_dw);\r\nfor (i = 0; i < ib->length_dw; i++) {\r\nseq_printf(m, "[%05u]=0x%08X\n", i, ib->ptr[i]);\r\n}\r\nreturn 0;\r\n}\r\nstatic int radeon_debugfs_ib_bogus_info(struct seq_file *m, void *data)\r\n{\r\nstruct drm_info_node *node = (struct drm_info_node *) m->private;\r\nstruct radeon_device *rdev = node->info_ent->data;\r\nstruct radeon_ib *ib;\r\nunsigned i;\r\nmutex_lock(&rdev->ib_pool.mutex);\r\nif (list_empty(&rdev->ib_pool.bogus_ib)) {\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\nseq_printf(m, "no bogus IB recorded\n");\r\nreturn 0;\r\n}\r\nib = list_first_entry(&rdev->ib_pool.bogus_ib, struct radeon_ib, list);\r\nlist_del_init(&ib->list);\r\nmutex_unlock(&rdev->ib_pool.mutex);\r\nseq_printf(m, "IB size %05u dwords\n", ib->length_dw);\r\nfor (i = 0; i < ib->length_dw; i++) {\r\nseq_printf(m, "[%05u]=0x%08X\n", i, ib->ptr[i]);\r\n}\r\nvfree(ib->ptr);\r\nkfree(ib);\r\nreturn 0;\r\n}\r\nint radeon_debugfs_ib_init(struct radeon_device *rdev)\r\n{\r\n#if defined(CONFIG_DEBUG_FS)\r\nunsigned i;\r\nint r;\r\nradeon_debugfs_ib_bogus_info_list[0].data = rdev;\r\nr = radeon_debugfs_add_files(rdev, radeon_debugfs_ib_bogus_info_list, 1);\r\nif (r)\r\nreturn r;\r\nfor (i = 0; i < RADEON_IB_POOL_SIZE; i++) {\r\nsprintf(radeon_debugfs_ib_names[i], "radeon_ib_%04u", i);\r\nradeon_debugfs_ib_list[i].name = radeon_debugfs_ib_names[i];\r\nradeon_debugfs_ib_list[i].show = &radeon_debugfs_ib_info;\r\nradeon_debugfs_ib_list[i].driver_features = 0;\r\nradeon_debugfs_ib_list[i].data = &rdev->ib_pool.ibs[i];\r\n}\r\nreturn radeon_debugfs_add_files(rdev, radeon_debugfs_ib_list,\r\nRADEON_IB_POOL_SIZE);\r\n#else\r\nreturn 0;\r\n#endif\r\n}
