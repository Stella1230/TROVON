static inline struct qib_fmr *to_ifmr(struct ib_fmr *ibfmr)\r\n{\r\nreturn container_of(ibfmr, struct qib_fmr, ibfmr);\r\n}\r\nstruct ib_mr *qib_get_dma_mr(struct ib_pd *pd, int acc)\r\n{\r\nstruct qib_ibdev *dev = to_idev(pd->device);\r\nstruct qib_mr *mr;\r\nstruct ib_mr *ret;\r\nunsigned long flags;\r\nif (to_ipd(pd)->user) {\r\nret = ERR_PTR(-EPERM);\r\ngoto bail;\r\n}\r\nmr = kzalloc(sizeof *mr, GFP_KERNEL);\r\nif (!mr) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nmr->mr.access_flags = acc;\r\natomic_set(&mr->mr.refcount, 0);\r\nspin_lock_irqsave(&dev->lk_table.lock, flags);\r\nif (!dev->dma_mr)\r\ndev->dma_mr = &mr->mr;\r\nspin_unlock_irqrestore(&dev->lk_table.lock, flags);\r\nret = &mr->ibmr;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic struct qib_mr *alloc_mr(int count, struct qib_lkey_table *lk_table)\r\n{\r\nstruct qib_mr *mr;\r\nint m, i = 0;\r\nm = (count + QIB_SEGSZ - 1) / QIB_SEGSZ;\r\nmr = kmalloc(sizeof *mr + m * sizeof mr->mr.map[0], GFP_KERNEL);\r\nif (!mr)\r\ngoto done;\r\nfor (; i < m; i++) {\r\nmr->mr.map[i] = kmalloc(sizeof *mr->mr.map[0], GFP_KERNEL);\r\nif (!mr->mr.map[i])\r\ngoto bail;\r\n}\r\nmr->mr.mapsz = m;\r\nmr->mr.page_shift = 0;\r\nmr->mr.max_segs = count;\r\nif (!qib_alloc_lkey(lk_table, &mr->mr))\r\ngoto bail;\r\nmr->ibmr.lkey = mr->mr.lkey;\r\nmr->ibmr.rkey = mr->mr.lkey;\r\natomic_set(&mr->mr.refcount, 0);\r\ngoto done;\r\nbail:\r\nwhile (i)\r\nkfree(mr->mr.map[--i]);\r\nkfree(mr);\r\nmr = NULL;\r\ndone:\r\nreturn mr;\r\n}\r\nstruct ib_mr *qib_reg_phys_mr(struct ib_pd *pd,\r\nstruct ib_phys_buf *buffer_list,\r\nint num_phys_buf, int acc, u64 *iova_start)\r\n{\r\nstruct qib_mr *mr;\r\nint n, m, i;\r\nstruct ib_mr *ret;\r\nmr = alloc_mr(num_phys_buf, &to_idev(pd->device)->lk_table);\r\nif (mr == NULL) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nmr->mr.pd = pd;\r\nmr->mr.user_base = *iova_start;\r\nmr->mr.iova = *iova_start;\r\nmr->mr.length = 0;\r\nmr->mr.offset = 0;\r\nmr->mr.access_flags = acc;\r\nmr->umem = NULL;\r\nm = 0;\r\nn = 0;\r\nfor (i = 0; i < num_phys_buf; i++) {\r\nmr->mr.map[m]->segs[n].vaddr = (void *) buffer_list[i].addr;\r\nmr->mr.map[m]->segs[n].length = buffer_list[i].size;\r\nmr->mr.length += buffer_list[i].size;\r\nn++;\r\nif (n == QIB_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nret = &mr->ibmr;\r\nbail:\r\nreturn ret;\r\n}\r\nstruct ib_mr *qib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\r\nu64 virt_addr, int mr_access_flags,\r\nstruct ib_udata *udata)\r\n{\r\nstruct qib_mr *mr;\r\nstruct ib_umem *umem;\r\nstruct ib_umem_chunk *chunk;\r\nint n, m, i;\r\nstruct ib_mr *ret;\r\nif (length == 0) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\numem = ib_umem_get(pd->uobject->context, start, length,\r\nmr_access_flags, 0);\r\nif (IS_ERR(umem))\r\nreturn (void *) umem;\r\nn = 0;\r\nlist_for_each_entry(chunk, &umem->chunk_list, list)\r\nn += chunk->nents;\r\nmr = alloc_mr(n, &to_idev(pd->device)->lk_table);\r\nif (!mr) {\r\nret = ERR_PTR(-ENOMEM);\r\nib_umem_release(umem);\r\ngoto bail;\r\n}\r\nmr->mr.pd = pd;\r\nmr->mr.user_base = start;\r\nmr->mr.iova = virt_addr;\r\nmr->mr.length = length;\r\nmr->mr.offset = umem->offset;\r\nmr->mr.access_flags = mr_access_flags;\r\nmr->umem = umem;\r\nif (is_power_of_2(umem->page_size))\r\nmr->mr.page_shift = ilog2(umem->page_size);\r\nm = 0;\r\nn = 0;\r\nlist_for_each_entry(chunk, &umem->chunk_list, list) {\r\nfor (i = 0; i < chunk->nents; i++) {\r\nvoid *vaddr;\r\nvaddr = page_address(sg_page(&chunk->page_list[i]));\r\nif (!vaddr) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nmr->mr.map[m]->segs[n].vaddr = vaddr;\r\nmr->mr.map[m]->segs[n].length = umem->page_size;\r\nn++;\r\nif (n == QIB_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\n}\r\nret = &mr->ibmr;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_dereg_mr(struct ib_mr *ibmr)\r\n{\r\nstruct qib_mr *mr = to_imr(ibmr);\r\nstruct qib_ibdev *dev = to_idev(ibmr->device);\r\nint ret;\r\nint i;\r\nret = qib_free_lkey(dev, &mr->mr);\r\nif (ret)\r\nreturn ret;\r\ni = mr->mr.mapsz;\r\nwhile (i)\r\nkfree(mr->mr.map[--i]);\r\nif (mr->umem)\r\nib_umem_release(mr->umem);\r\nkfree(mr);\r\nreturn 0;\r\n}\r\nstruct ib_mr *qib_alloc_fast_reg_mr(struct ib_pd *pd, int max_page_list_len)\r\n{\r\nstruct qib_mr *mr;\r\nmr = alloc_mr(max_page_list_len, &to_idev(pd->device)->lk_table);\r\nif (mr == NULL)\r\nreturn ERR_PTR(-ENOMEM);\r\nmr->mr.pd = pd;\r\nmr->mr.user_base = 0;\r\nmr->mr.iova = 0;\r\nmr->mr.length = 0;\r\nmr->mr.offset = 0;\r\nmr->mr.access_flags = 0;\r\nmr->umem = NULL;\r\nreturn &mr->ibmr;\r\n}\r\nstruct ib_fast_reg_page_list *\r\nqib_alloc_fast_reg_page_list(struct ib_device *ibdev, int page_list_len)\r\n{\r\nunsigned size = page_list_len * sizeof(u64);\r\nstruct ib_fast_reg_page_list *pl;\r\nif (size > PAGE_SIZE)\r\nreturn ERR_PTR(-EINVAL);\r\npl = kmalloc(sizeof *pl, GFP_KERNEL);\r\nif (!pl)\r\nreturn ERR_PTR(-ENOMEM);\r\npl->page_list = kmalloc(size, GFP_KERNEL);\r\nif (!pl->page_list)\r\ngoto err_free;\r\nreturn pl;\r\nerr_free:\r\nkfree(pl);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nvoid qib_free_fast_reg_page_list(struct ib_fast_reg_page_list *pl)\r\n{\r\nkfree(pl->page_list);\r\nkfree(pl);\r\n}\r\nstruct ib_fmr *qib_alloc_fmr(struct ib_pd *pd, int mr_access_flags,\r\nstruct ib_fmr_attr *fmr_attr)\r\n{\r\nstruct qib_fmr *fmr;\r\nint m, i = 0;\r\nstruct ib_fmr *ret;\r\nm = (fmr_attr->max_pages + QIB_SEGSZ - 1) / QIB_SEGSZ;\r\nfmr = kmalloc(sizeof *fmr + m * sizeof fmr->mr.map[0], GFP_KERNEL);\r\nif (!fmr)\r\ngoto bail;\r\nfor (; i < m; i++) {\r\nfmr->mr.map[i] = kmalloc(sizeof *fmr->mr.map[0],\r\nGFP_KERNEL);\r\nif (!fmr->mr.map[i])\r\ngoto bail;\r\n}\r\nfmr->mr.mapsz = m;\r\nif (!qib_alloc_lkey(&to_idev(pd->device)->lk_table, &fmr->mr))\r\ngoto bail;\r\nfmr->ibfmr.rkey = fmr->mr.lkey;\r\nfmr->ibfmr.lkey = fmr->mr.lkey;\r\nfmr->mr.pd = pd;\r\nfmr->mr.user_base = 0;\r\nfmr->mr.iova = 0;\r\nfmr->mr.length = 0;\r\nfmr->mr.offset = 0;\r\nfmr->mr.access_flags = mr_access_flags;\r\nfmr->mr.max_segs = fmr_attr->max_pages;\r\nfmr->mr.page_shift = fmr_attr->page_shift;\r\natomic_set(&fmr->mr.refcount, 0);\r\nret = &fmr->ibfmr;\r\ngoto done;\r\nbail:\r\nwhile (i)\r\nkfree(fmr->mr.map[--i]);\r\nkfree(fmr);\r\nret = ERR_PTR(-ENOMEM);\r\ndone:\r\nreturn ret;\r\n}\r\nint qib_map_phys_fmr(struct ib_fmr *ibfmr, u64 *page_list,\r\nint list_len, u64 iova)\r\n{\r\nstruct qib_fmr *fmr = to_ifmr(ibfmr);\r\nstruct qib_lkey_table *rkt;\r\nunsigned long flags;\r\nint m, n, i;\r\nu32 ps;\r\nint ret;\r\nif (atomic_read(&fmr->mr.refcount))\r\nreturn -EBUSY;\r\nif (list_len > fmr->mr.max_segs) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nrkt = &to_idev(ibfmr->device)->lk_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nfmr->mr.user_base = iova;\r\nfmr->mr.iova = iova;\r\nps = 1 << fmr->mr.page_shift;\r\nfmr->mr.length = list_len * ps;\r\nm = 0;\r\nn = 0;\r\nfor (i = 0; i < list_len; i++) {\r\nfmr->mr.map[m]->segs[n].vaddr = (void *) page_list[i];\r\nfmr->mr.map[m]->segs[n].length = ps;\r\nif (++n == QIB_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_unmap_fmr(struct list_head *fmr_list)\r\n{\r\nstruct qib_fmr *fmr;\r\nstruct qib_lkey_table *rkt;\r\nunsigned long flags;\r\nlist_for_each_entry(fmr, fmr_list, ibfmr.list) {\r\nrkt = &to_idev(fmr->ibfmr.device)->lk_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nfmr->mr.user_base = 0;\r\nfmr->mr.iova = 0;\r\nfmr->mr.length = 0;\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nint qib_dealloc_fmr(struct ib_fmr *ibfmr)\r\n{\r\nstruct qib_fmr *fmr = to_ifmr(ibfmr);\r\nint ret;\r\nint i;\r\nret = qib_free_lkey(to_idev(ibfmr->device), &fmr->mr);\r\nif (ret)\r\nreturn ret;\r\ni = fmr->mr.mapsz;\r\nwhile (i)\r\nkfree(fmr->mr.map[--i]);\r\nkfree(fmr);\r\nreturn 0;\r\n}
