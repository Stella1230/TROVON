static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)\r\n{\r\nstruct rtable *rt;\r\nstruct flowi4 fl4;\r\npeer->if_mtu = 1500;\r\nrt = ip_route_output_ports(&init_net, &fl4, NULL,\r\npeer->srx.transport.sin.sin_addr.s_addr, 0,\r\nhtons(7000), htons(7001),\r\nIPPROTO_UDP, 0, 0);\r\nif (IS_ERR(rt)) {\r\n_leave(" [route err %ld]", PTR_ERR(rt));\r\nreturn;\r\n}\r\npeer->if_mtu = dst_mtu(&rt->dst);\r\ndst_release(&rt->dst);\r\n_leave(" [if_mtu %u]", peer->if_mtu);\r\n}\r\nstatic struct rxrpc_peer *rxrpc_alloc_peer(struct sockaddr_rxrpc *srx,\r\ngfp_t gfp)\r\n{\r\nstruct rxrpc_peer *peer;\r\n_enter("");\r\npeer = kzalloc(sizeof(struct rxrpc_peer), gfp);\r\nif (peer) {\r\nINIT_WORK(&peer->destroyer, &rxrpc_destroy_peer);\r\nINIT_LIST_HEAD(&peer->link);\r\nINIT_LIST_HEAD(&peer->error_targets);\r\nspin_lock_init(&peer->lock);\r\natomic_set(&peer->usage, 1);\r\npeer->debug_id = atomic_inc_return(&rxrpc_debug_id);\r\nmemcpy(&peer->srx, srx, sizeof(*srx));\r\nrxrpc_assess_MTU_size(peer);\r\npeer->mtu = peer->if_mtu;\r\nif (srx->transport.family == AF_INET) {\r\npeer->hdrsize = sizeof(struct iphdr);\r\nswitch (srx->transport_type) {\r\ncase SOCK_DGRAM:\r\npeer->hdrsize += sizeof(struct udphdr);\r\nbreak;\r\ndefault:\r\nBUG();\r\nbreak;\r\n}\r\n} else {\r\nBUG();\r\n}\r\npeer->hdrsize += sizeof(struct rxrpc_header);\r\npeer->maxdata = peer->mtu - peer->hdrsize;\r\n}\r\n_leave(" = %p", peer);\r\nreturn peer;\r\n}\r\nstruct rxrpc_peer *rxrpc_get_peer(struct sockaddr_rxrpc *srx, gfp_t gfp)\r\n{\r\nstruct rxrpc_peer *peer, *candidate;\r\nconst char *new = "old";\r\nint usage;\r\n_enter("{%d,%d,%pI4+%hu}",\r\nsrx->transport_type,\r\nsrx->transport_len,\r\n&srx->transport.sin.sin_addr,\r\nntohs(srx->transport.sin.sin_port));\r\nread_lock_bh(&rxrpc_peer_lock);\r\nlist_for_each_entry(peer, &rxrpc_peers, link) {\r\n_debug("check PEER %d { u=%d t=%d l=%d }",\r\npeer->debug_id,\r\natomic_read(&peer->usage),\r\npeer->srx.transport_type,\r\npeer->srx.transport_len);\r\nif (atomic_read(&peer->usage) > 0 &&\r\npeer->srx.transport_type == srx->transport_type &&\r\npeer->srx.transport_len == srx->transport_len &&\r\nmemcmp(&peer->srx.transport,\r\n&srx->transport,\r\nsrx->transport_len) == 0)\r\ngoto found_extant_peer;\r\n}\r\nread_unlock_bh(&rxrpc_peer_lock);\r\ncandidate = rxrpc_alloc_peer(srx, gfp);\r\nif (!candidate) {\r\n_leave(" = -ENOMEM");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nwrite_lock_bh(&rxrpc_peer_lock);\r\nlist_for_each_entry(peer, &rxrpc_peers, link) {\r\nif (atomic_read(&peer->usage) > 0 &&\r\npeer->srx.transport_type == srx->transport_type &&\r\npeer->srx.transport_len == srx->transport_len &&\r\nmemcmp(&peer->srx.transport,\r\n&srx->transport,\r\nsrx->transport_len) == 0)\r\ngoto found_extant_second;\r\n}\r\npeer = candidate;\r\ncandidate = NULL;\r\nusage = atomic_read(&peer->usage);\r\nlist_add_tail(&peer->link, &rxrpc_peers);\r\nwrite_unlock_bh(&rxrpc_peer_lock);\r\nnew = "new";\r\nsuccess:\r\n_net("PEER %s %d {%d,%u,%pI4+%hu}",\r\nnew,\r\npeer->debug_id,\r\npeer->srx.transport_type,\r\npeer->srx.transport.family,\r\n&peer->srx.transport.sin.sin_addr,\r\nntohs(peer->srx.transport.sin.sin_port));\r\n_leave(" = %p {u=%d}", peer, usage);\r\nreturn peer;\r\nfound_extant_peer:\r\nusage = atomic_inc_return(&peer->usage);\r\nread_unlock_bh(&rxrpc_peer_lock);\r\ngoto success;\r\nfound_extant_second:\r\nusage = atomic_inc_return(&peer->usage);\r\nwrite_unlock_bh(&rxrpc_peer_lock);\r\nkfree(candidate);\r\ngoto success;\r\n}\r\nstruct rxrpc_peer *rxrpc_find_peer(struct rxrpc_local *local,\r\n__be32 addr, __be16 port)\r\n{\r\nstruct rxrpc_peer *peer;\r\n_enter("");\r\nread_lock_bh(&rxrpc_peer_lock);\r\nif (local->srx.transport.family == AF_INET &&\r\nlocal->srx.transport_type == SOCK_DGRAM\r\n) {\r\nlist_for_each_entry(peer, &rxrpc_peers, link) {\r\nif (atomic_read(&peer->usage) > 0 &&\r\npeer->srx.transport_type == SOCK_DGRAM &&\r\npeer->srx.transport.family == AF_INET &&\r\npeer->srx.transport.sin.sin_port == port &&\r\npeer->srx.transport.sin.sin_addr.s_addr == addr)\r\ngoto found_UDP_peer;\r\n}\r\ngoto new_UDP_peer;\r\n}\r\nread_unlock_bh(&rxrpc_peer_lock);\r\n_leave(" = -EAFNOSUPPORT");\r\nreturn ERR_PTR(-EAFNOSUPPORT);\r\nfound_UDP_peer:\r\n_net("Rx UDP DGRAM from peer %d", peer->debug_id);\r\natomic_inc(&peer->usage);\r\nread_unlock_bh(&rxrpc_peer_lock);\r\n_leave(" = %p", peer);\r\nreturn peer;\r\nnew_UDP_peer:\r\n_net("Rx UDP DGRAM from NEW peer %d", peer->debug_id);\r\nread_unlock_bh(&rxrpc_peer_lock);\r\n_leave(" = -EBUSY [new]");\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\nvoid rxrpc_put_peer(struct rxrpc_peer *peer)\r\n{\r\n_enter("%p{u=%d}", peer, atomic_read(&peer->usage));\r\nASSERTCMP(atomic_read(&peer->usage), >, 0);\r\nif (likely(!atomic_dec_and_test(&peer->usage))) {\r\n_leave(" [in use]");\r\nreturn;\r\n}\r\nrxrpc_queue_work(&peer->destroyer);\r\n_leave("");\r\n}\r\nstatic void rxrpc_destroy_peer(struct work_struct *work)\r\n{\r\nstruct rxrpc_peer *peer =\r\ncontainer_of(work, struct rxrpc_peer, destroyer);\r\n_enter("%p{%d}", peer, atomic_read(&peer->usage));\r\nwrite_lock_bh(&rxrpc_peer_lock);\r\nlist_del(&peer->link);\r\nwrite_unlock_bh(&rxrpc_peer_lock);\r\n_net("DESTROY PEER %d", peer->debug_id);\r\nkfree(peer);\r\nif (list_empty(&rxrpc_peers))\r\nwake_up_all(&rxrpc_peer_wq);\r\n_leave("");\r\n}\r\nvoid __exit rxrpc_destroy_all_peers(void)\r\n{\r\nDECLARE_WAITQUEUE(myself,current);\r\n_enter("");\r\nif (!list_empty(&rxrpc_peers)) {\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nadd_wait_queue(&rxrpc_peer_wq, &myself);\r\nwhile (!list_empty(&rxrpc_peers)) {\r\nschedule();\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\n}\r\nremove_wait_queue(&rxrpc_peer_wq, &myself);\r\nset_current_state(TASK_RUNNING);\r\n}\r\n_leave("");\r\n}
