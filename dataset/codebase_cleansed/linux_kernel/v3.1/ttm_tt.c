static void ttm_tt_alloc_page_directory(struct ttm_tt *ttm)\r\n{\r\nttm->pages = drm_calloc_large(ttm->num_pages, sizeof(*ttm->pages));\r\nttm->dma_address = drm_calloc_large(ttm->num_pages,\r\nsizeof(*ttm->dma_address));\r\n}\r\nstatic void ttm_tt_free_page_directory(struct ttm_tt *ttm)\r\n{\r\ndrm_free_large(ttm->pages);\r\nttm->pages = NULL;\r\ndrm_free_large(ttm->dma_address);\r\nttm->dma_address = NULL;\r\n}\r\nstatic void ttm_tt_free_user_pages(struct ttm_tt *ttm)\r\n{\r\nint write;\r\nint dirty;\r\nstruct page *page;\r\nint i;\r\nstruct ttm_backend *be = ttm->be;\r\nBUG_ON(!(ttm->page_flags & TTM_PAGE_FLAG_USER));\r\nwrite = ((ttm->page_flags & TTM_PAGE_FLAG_WRITE) != 0);\r\ndirty = ((ttm->page_flags & TTM_PAGE_FLAG_USER_DIRTY) != 0);\r\nif (be)\r\nbe->func->clear(be);\r\nfor (i = 0; i < ttm->num_pages; ++i) {\r\npage = ttm->pages[i];\r\nif (page == NULL)\r\ncontinue;\r\nif (page == ttm->dummy_read_page) {\r\nBUG_ON(write);\r\ncontinue;\r\n}\r\nif (write && dirty && !PageReserved(page))\r\nset_page_dirty_lock(page);\r\nttm->pages[i] = NULL;\r\nttm_mem_global_free(ttm->glob->mem_glob, PAGE_SIZE);\r\nput_page(page);\r\n}\r\nttm->state = tt_unpopulated;\r\nttm->first_himem_page = ttm->num_pages;\r\nttm->last_lomem_page = -1;\r\n}\r\nstatic struct page *__ttm_tt_get_page(struct ttm_tt *ttm, int index)\r\n{\r\nstruct page *p;\r\nstruct list_head h;\r\nstruct ttm_mem_global *mem_glob = ttm->glob->mem_glob;\r\nint ret;\r\nwhile (NULL == (p = ttm->pages[index])) {\r\nINIT_LIST_HEAD(&h);\r\nret = ttm_get_pages(&h, ttm->page_flags, ttm->caching_state, 1,\r\n&ttm->dma_address[index]);\r\nif (ret != 0)\r\nreturn NULL;\r\np = list_first_entry(&h, struct page, lru);\r\nret = ttm_mem_global_alloc_page(mem_glob, p, false, false);\r\nif (unlikely(ret != 0))\r\ngoto out_err;\r\nif (PageHighMem(p))\r\nttm->pages[--ttm->first_himem_page] = p;\r\nelse\r\nttm->pages[++ttm->last_lomem_page] = p;\r\n}\r\nreturn p;\r\nout_err:\r\nput_page(p);\r\nreturn NULL;\r\n}\r\nstruct page *ttm_tt_get_page(struct ttm_tt *ttm, int index)\r\n{\r\nint ret;\r\nif (unlikely(ttm->page_flags & TTM_PAGE_FLAG_SWAPPED)) {\r\nret = ttm_tt_swapin(ttm);\r\nif (unlikely(ret != 0))\r\nreturn NULL;\r\n}\r\nreturn __ttm_tt_get_page(ttm, index);\r\n}\r\nint ttm_tt_populate(struct ttm_tt *ttm)\r\n{\r\nstruct page *page;\r\nunsigned long i;\r\nstruct ttm_backend *be;\r\nint ret;\r\nif (ttm->state != tt_unpopulated)\r\nreturn 0;\r\nif (unlikely(ttm->page_flags & TTM_PAGE_FLAG_SWAPPED)) {\r\nret = ttm_tt_swapin(ttm);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\nbe = ttm->be;\r\nfor (i = 0; i < ttm->num_pages; ++i) {\r\npage = __ttm_tt_get_page(ttm, i);\r\nif (!page)\r\nreturn -ENOMEM;\r\n}\r\nbe->func->populate(be, ttm->num_pages, ttm->pages,\r\nttm->dummy_read_page, ttm->dma_address);\r\nttm->state = tt_unbound;\r\nreturn 0;\r\n}\r\nstatic inline int ttm_tt_set_page_caching(struct page *p,\r\nenum ttm_caching_state c_old,\r\nenum ttm_caching_state c_new)\r\n{\r\nint ret = 0;\r\nif (PageHighMem(p))\r\nreturn 0;\r\nif (c_old != tt_cached) {\r\nret = set_pages_wb(p, 1);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (c_new == tt_wc)\r\nret = set_memory_wc((unsigned long) page_address(p), 1);\r\nelse if (c_new == tt_uncached)\r\nret = set_pages_uc(p, 1);\r\nreturn ret;\r\n}\r\nstatic inline int ttm_tt_set_page_caching(struct page *p,\r\nenum ttm_caching_state c_old,\r\nenum ttm_caching_state c_new)\r\n{\r\nreturn 0;\r\n}\r\nstatic int ttm_tt_set_caching(struct ttm_tt *ttm,\r\nenum ttm_caching_state c_state)\r\n{\r\nint i, j;\r\nstruct page *cur_page;\r\nint ret;\r\nif (ttm->caching_state == c_state)\r\nreturn 0;\r\nif (ttm->state == tt_unpopulated) {\r\nttm->caching_state = c_state;\r\nreturn 0;\r\n}\r\nif (ttm->caching_state == tt_cached)\r\ndrm_clflush_pages(ttm->pages, ttm->num_pages);\r\nfor (i = 0; i < ttm->num_pages; ++i) {\r\ncur_page = ttm->pages[i];\r\nif (likely(cur_page != NULL)) {\r\nret = ttm_tt_set_page_caching(cur_page,\r\nttm->caching_state,\r\nc_state);\r\nif (unlikely(ret != 0))\r\ngoto out_err;\r\n}\r\n}\r\nttm->caching_state = c_state;\r\nreturn 0;\r\nout_err:\r\nfor (j = 0; j < i; ++j) {\r\ncur_page = ttm->pages[j];\r\nif (likely(cur_page != NULL)) {\r\n(void)ttm_tt_set_page_caching(cur_page, c_state,\r\nttm->caching_state);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nint ttm_tt_set_placement_caching(struct ttm_tt *ttm, uint32_t placement)\r\n{\r\nenum ttm_caching_state state;\r\nif (placement & TTM_PL_FLAG_WC)\r\nstate = tt_wc;\r\nelse if (placement & TTM_PL_FLAG_UNCACHED)\r\nstate = tt_uncached;\r\nelse\r\nstate = tt_cached;\r\nreturn ttm_tt_set_caching(ttm, state);\r\n}\r\nstatic void ttm_tt_free_alloced_pages(struct ttm_tt *ttm)\r\n{\r\nint i;\r\nunsigned count = 0;\r\nstruct list_head h;\r\nstruct page *cur_page;\r\nstruct ttm_backend *be = ttm->be;\r\nINIT_LIST_HEAD(&h);\r\nif (be)\r\nbe->func->clear(be);\r\nfor (i = 0; i < ttm->num_pages; ++i) {\r\ncur_page = ttm->pages[i];\r\nttm->pages[i] = NULL;\r\nif (cur_page) {\r\nif (page_count(cur_page) != 1)\r\nprintk(KERN_ERR TTM_PFX\r\n"Erroneous page count. "\r\n"Leaking pages.\n");\r\nttm_mem_global_free_page(ttm->glob->mem_glob,\r\ncur_page);\r\nlist_add(&cur_page->lru, &h);\r\ncount++;\r\n}\r\n}\r\nttm_put_pages(&h, count, ttm->page_flags, ttm->caching_state,\r\nttm->dma_address);\r\nttm->state = tt_unpopulated;\r\nttm->first_himem_page = ttm->num_pages;\r\nttm->last_lomem_page = -1;\r\n}\r\nvoid ttm_tt_destroy(struct ttm_tt *ttm)\r\n{\r\nstruct ttm_backend *be;\r\nif (unlikely(ttm == NULL))\r\nreturn;\r\nbe = ttm->be;\r\nif (likely(be != NULL)) {\r\nbe->func->destroy(be);\r\nttm->be = NULL;\r\n}\r\nif (likely(ttm->pages != NULL)) {\r\nif (ttm->page_flags & TTM_PAGE_FLAG_USER)\r\nttm_tt_free_user_pages(ttm);\r\nelse\r\nttm_tt_free_alloced_pages(ttm);\r\nttm_tt_free_page_directory(ttm);\r\n}\r\nif (!(ttm->page_flags & TTM_PAGE_FLAG_PERSISTENT_SWAP) &&\r\nttm->swap_storage)\r\nfput(ttm->swap_storage);\r\nkfree(ttm);\r\n}\r\nint ttm_tt_set_user(struct ttm_tt *ttm,\r\nstruct task_struct *tsk,\r\nunsigned long start, unsigned long num_pages)\r\n{\r\nstruct mm_struct *mm = tsk->mm;\r\nint ret;\r\nint write = (ttm->page_flags & TTM_PAGE_FLAG_WRITE) != 0;\r\nstruct ttm_mem_global *mem_glob = ttm->glob->mem_glob;\r\nBUG_ON(num_pages != ttm->num_pages);\r\nBUG_ON((ttm->page_flags & TTM_PAGE_FLAG_USER) == 0);\r\nret = ttm_mem_global_alloc(mem_glob, num_pages * PAGE_SIZE,\r\nfalse, false);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\ndown_read(&mm->mmap_sem);\r\nret = get_user_pages(tsk, mm, start, num_pages,\r\nwrite, 0, ttm->pages, NULL);\r\nup_read(&mm->mmap_sem);\r\nif (ret != num_pages && write) {\r\nttm_tt_free_user_pages(ttm);\r\nttm_mem_global_free(mem_glob, num_pages * PAGE_SIZE);\r\nreturn -ENOMEM;\r\n}\r\nttm->tsk = tsk;\r\nttm->start = start;\r\nttm->state = tt_unbound;\r\nreturn 0;\r\n}\r\nstruct ttm_tt *ttm_tt_create(struct ttm_bo_device *bdev, unsigned long size,\r\nuint32_t page_flags, struct page *dummy_read_page)\r\n{\r\nstruct ttm_bo_driver *bo_driver = bdev->driver;\r\nstruct ttm_tt *ttm;\r\nif (!bo_driver)\r\nreturn NULL;\r\nttm = kzalloc(sizeof(*ttm), GFP_KERNEL);\r\nif (!ttm)\r\nreturn NULL;\r\nttm->glob = bdev->glob;\r\nttm->num_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nttm->first_himem_page = ttm->num_pages;\r\nttm->last_lomem_page = -1;\r\nttm->caching_state = tt_cached;\r\nttm->page_flags = page_flags;\r\nttm->dummy_read_page = dummy_read_page;\r\nttm_tt_alloc_page_directory(ttm);\r\nif (!ttm->pages) {\r\nttm_tt_destroy(ttm);\r\nprintk(KERN_ERR TTM_PFX "Failed allocating page table\n");\r\nreturn NULL;\r\n}\r\nttm->be = bo_driver->create_ttm_backend_entry(bdev);\r\nif (!ttm->be) {\r\nttm_tt_destroy(ttm);\r\nprintk(KERN_ERR TTM_PFX "Failed creating ttm backend entry\n");\r\nreturn NULL;\r\n}\r\nttm->state = tt_unpopulated;\r\nreturn ttm;\r\n}\r\nvoid ttm_tt_unbind(struct ttm_tt *ttm)\r\n{\r\nint ret;\r\nstruct ttm_backend *be = ttm->be;\r\nif (ttm->state == tt_bound) {\r\nret = be->func->unbind(be);\r\nBUG_ON(ret);\r\nttm->state = tt_unbound;\r\n}\r\n}\r\nint ttm_tt_bind(struct ttm_tt *ttm, struct ttm_mem_reg *bo_mem)\r\n{\r\nint ret = 0;\r\nstruct ttm_backend *be;\r\nif (!ttm)\r\nreturn -EINVAL;\r\nif (ttm->state == tt_bound)\r\nreturn 0;\r\nbe = ttm->be;\r\nret = ttm_tt_populate(ttm);\r\nif (ret)\r\nreturn ret;\r\nret = be->func->bind(be, bo_mem);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nttm->state = tt_bound;\r\nif (ttm->page_flags & TTM_PAGE_FLAG_USER)\r\nttm->page_flags |= TTM_PAGE_FLAG_USER_DIRTY;\r\nreturn 0;\r\n}\r\nstatic int ttm_tt_swapin(struct ttm_tt *ttm)\r\n{\r\nstruct address_space *swap_space;\r\nstruct file *swap_storage;\r\nstruct page *from_page;\r\nstruct page *to_page;\r\nvoid *from_virtual;\r\nvoid *to_virtual;\r\nint i;\r\nint ret = -ENOMEM;\r\nif (ttm->page_flags & TTM_PAGE_FLAG_USER) {\r\nret = ttm_tt_set_user(ttm, ttm->tsk, ttm->start,\r\nttm->num_pages);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nttm->page_flags &= ~TTM_PAGE_FLAG_SWAPPED;\r\nreturn 0;\r\n}\r\nswap_storage = ttm->swap_storage;\r\nBUG_ON(swap_storage == NULL);\r\nswap_space = swap_storage->f_path.dentry->d_inode->i_mapping;\r\nfor (i = 0; i < ttm->num_pages; ++i) {\r\nfrom_page = shmem_read_mapping_page(swap_space, i);\r\nif (IS_ERR(from_page)) {\r\nret = PTR_ERR(from_page);\r\ngoto out_err;\r\n}\r\nto_page = __ttm_tt_get_page(ttm, i);\r\nif (unlikely(to_page == NULL))\r\ngoto out_err;\r\npreempt_disable();\r\nfrom_virtual = kmap_atomic(from_page, KM_USER0);\r\nto_virtual = kmap_atomic(to_page, KM_USER1);\r\nmemcpy(to_virtual, from_virtual, PAGE_SIZE);\r\nkunmap_atomic(to_virtual, KM_USER1);\r\nkunmap_atomic(from_virtual, KM_USER0);\r\npreempt_enable();\r\npage_cache_release(from_page);\r\n}\r\nif (!(ttm->page_flags & TTM_PAGE_FLAG_PERSISTENT_SWAP))\r\nfput(swap_storage);\r\nttm->swap_storage = NULL;\r\nttm->page_flags &= ~TTM_PAGE_FLAG_SWAPPED;\r\nreturn 0;\r\nout_err:\r\nttm_tt_free_alloced_pages(ttm);\r\nreturn ret;\r\n}\r\nint ttm_tt_swapout(struct ttm_tt *ttm, struct file *persistent_swap_storage)\r\n{\r\nstruct address_space *swap_space;\r\nstruct file *swap_storage;\r\nstruct page *from_page;\r\nstruct page *to_page;\r\nvoid *from_virtual;\r\nvoid *to_virtual;\r\nint i;\r\nint ret = -ENOMEM;\r\nBUG_ON(ttm->state != tt_unbound && ttm->state != tt_unpopulated);\r\nBUG_ON(ttm->caching_state != tt_cached);\r\nif (ttm->page_flags & TTM_PAGE_FLAG_USER) {\r\nttm_tt_free_user_pages(ttm);\r\nttm->page_flags |= TTM_PAGE_FLAG_SWAPPED;\r\nttm->swap_storage = NULL;\r\nreturn 0;\r\n}\r\nif (!persistent_swap_storage) {\r\nswap_storage = shmem_file_setup("ttm swap",\r\nttm->num_pages << PAGE_SHIFT,\r\n0);\r\nif (unlikely(IS_ERR(swap_storage))) {\r\nprintk(KERN_ERR "Failed allocating swap storage.\n");\r\nreturn PTR_ERR(swap_storage);\r\n}\r\n} else\r\nswap_storage = persistent_swap_storage;\r\nswap_space = swap_storage->f_path.dentry->d_inode->i_mapping;\r\nfor (i = 0; i < ttm->num_pages; ++i) {\r\nfrom_page = ttm->pages[i];\r\nif (unlikely(from_page == NULL))\r\ncontinue;\r\nto_page = shmem_read_mapping_page(swap_space, i);\r\nif (unlikely(IS_ERR(to_page))) {\r\nret = PTR_ERR(to_page);\r\ngoto out_err;\r\n}\r\npreempt_disable();\r\nfrom_virtual = kmap_atomic(from_page, KM_USER0);\r\nto_virtual = kmap_atomic(to_page, KM_USER1);\r\nmemcpy(to_virtual, from_virtual, PAGE_SIZE);\r\nkunmap_atomic(to_virtual, KM_USER1);\r\nkunmap_atomic(from_virtual, KM_USER0);\r\npreempt_enable();\r\nset_page_dirty(to_page);\r\nmark_page_accessed(to_page);\r\npage_cache_release(to_page);\r\n}\r\nttm_tt_free_alloced_pages(ttm);\r\nttm->swap_storage = swap_storage;\r\nttm->page_flags |= TTM_PAGE_FLAG_SWAPPED;\r\nif (persistent_swap_storage)\r\nttm->page_flags |= TTM_PAGE_FLAG_PERSISTENT_SWAP;\r\nreturn 0;\r\nout_err:\r\nif (!persistent_swap_storage)\r\nfput(swap_storage);\r\nreturn ret;\r\n}
