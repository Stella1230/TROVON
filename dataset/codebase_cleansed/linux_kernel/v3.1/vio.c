static int vio_cmo_num_OF_devs(void)\r\n{\r\nstruct device_node *node_vroot;\r\nint count = 0;\r\nnode_vroot = of_find_node_by_name(NULL, "vdevice");\r\nif (node_vroot) {\r\nstruct device_node *of_node;\r\nstruct property *prop;\r\nfor_each_child_of_node(node_vroot, of_node) {\r\nprop = of_find_property(of_node, "ibm,my-dma-window",\r\nNULL);\r\nif (prop)\r\ncount++;\r\n}\r\n}\r\nof_node_put(node_vroot);\r\nreturn count;\r\n}\r\nstatic inline int vio_cmo_alloc(struct vio_dev *viodev, size_t size)\r\n{\r\nunsigned long flags;\r\nsize_t reserve_free = 0;\r\nsize_t excess_free = 0;\r\nint ret = -ENOMEM;\r\nspin_lock_irqsave(&vio_cmo.lock, flags);\r\nif (viodev->cmo.entitled > viodev->cmo.allocated)\r\nreserve_free = viodev->cmo.entitled - viodev->cmo.allocated;\r\nif (vio_cmo.spare >= VIO_CMO_MIN_ENT)\r\nexcess_free = vio_cmo.excess.free;\r\nif ((reserve_free + excess_free) >= size) {\r\nvio_cmo.curr += size;\r\nif (vio_cmo.curr > vio_cmo.high)\r\nvio_cmo.high = vio_cmo.curr;\r\nviodev->cmo.allocated += size;\r\nsize -= min(reserve_free, size);\r\nvio_cmo.excess.free -= size;\r\nret = 0;\r\n}\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\nreturn ret;\r\n}\r\nstatic inline void vio_cmo_dealloc(struct vio_dev *viodev, size_t size)\r\n{\r\nunsigned long flags;\r\nsize_t spare_needed = 0;\r\nsize_t excess_freed = 0;\r\nsize_t reserve_freed = size;\r\nsize_t tmp;\r\nint balance = 0;\r\nspin_lock_irqsave(&vio_cmo.lock, flags);\r\nvio_cmo.curr -= size;\r\nif (viodev->cmo.allocated > viodev->cmo.entitled) {\r\nexcess_freed = min(reserve_freed, (viodev->cmo.allocated -\r\nviodev->cmo.entitled));\r\nreserve_freed -= excess_freed;\r\n}\r\nviodev->cmo.allocated -= (reserve_freed + excess_freed);\r\nspare_needed = VIO_CMO_MIN_ENT - vio_cmo.spare;\r\nif (spare_needed && excess_freed) {\r\ntmp = min(excess_freed, spare_needed);\r\nvio_cmo.excess.size -= tmp;\r\nvio_cmo.reserve.size += tmp;\r\nvio_cmo.spare += tmp;\r\nexcess_freed -= tmp;\r\nspare_needed -= tmp;\r\nbalance = 1;\r\n}\r\nif (spare_needed && reserve_freed) {\r\ntmp = min3(spare_needed, reserve_freed, (viodev->cmo.entitled - VIO_CMO_MIN_ENT));\r\nvio_cmo.spare += tmp;\r\nviodev->cmo.entitled -= tmp;\r\nreserve_freed -= tmp;\r\nspare_needed -= tmp;\r\nbalance = 1;\r\n}\r\nif (excess_freed && (vio_cmo.desired > vio_cmo.reserve.size)) {\r\ntmp = min(excess_freed, (vio_cmo.desired - vio_cmo.reserve.size));\r\nvio_cmo.excess.size -= tmp;\r\nvio_cmo.reserve.size += tmp;\r\nexcess_freed -= tmp;\r\nbalance = 1;\r\n}\r\nif (excess_freed)\r\nvio_cmo.excess.free += excess_freed;\r\nif (balance)\r\nschedule_delayed_work(&vio_cmo.balance_q, VIO_CMO_BALANCE_DELAY);\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\n}\r\nint vio_cmo_entitlement_update(size_t new_entitlement)\r\n{\r\nstruct vio_dev *viodev;\r\nstruct vio_cmo_dev_entry *dev_ent;\r\nunsigned long flags;\r\nsize_t avail, delta, tmp;\r\nspin_lock_irqsave(&vio_cmo.lock, flags);\r\nif (new_entitlement > vio_cmo.entitled) {\r\ndelta = new_entitlement - vio_cmo.entitled;\r\nif (vio_cmo.spare < VIO_CMO_MIN_ENT) {\r\ntmp = min(delta, (VIO_CMO_MIN_ENT - vio_cmo.spare));\r\nvio_cmo.spare += tmp;\r\nvio_cmo.reserve.size += tmp;\r\ndelta -= tmp;\r\n}\r\nvio_cmo.entitled += delta;\r\nvio_cmo.excess.size += delta;\r\nvio_cmo.excess.free += delta;\r\ngoto out;\r\n}\r\ndelta = vio_cmo.entitled - new_entitlement;\r\navail = vio_cmo.excess.free;\r\nlist_for_each_entry(dev_ent, &vio_cmo.device_list, list) {\r\nif (avail >= delta)\r\nbreak;\r\nviodev = dev_ent->viodev;\r\nif ((viodev->cmo.entitled > viodev->cmo.allocated) &&\r\n(viodev->cmo.entitled > VIO_CMO_MIN_ENT))\r\navail += viodev->cmo.entitled -\r\nmax_t(size_t, viodev->cmo.allocated,\r\nVIO_CMO_MIN_ENT);\r\n}\r\nif (delta <= avail) {\r\nvio_cmo.entitled -= delta;\r\ntmp = min(vio_cmo.excess.free, delta);\r\nvio_cmo.excess.size -= tmp;\r\nvio_cmo.excess.free -= tmp;\r\ndelta -= tmp;\r\nlist_for_each_entry(dev_ent, &vio_cmo.device_list, list) {\r\nif (!delta)\r\nbreak;\r\nviodev = dev_ent->viodev;\r\ntmp = 0;\r\nif ((viodev->cmo.entitled > viodev->cmo.allocated) &&\r\n(viodev->cmo.entitled > VIO_CMO_MIN_ENT))\r\ntmp = viodev->cmo.entitled -\r\nmax_t(size_t, viodev->cmo.allocated,\r\nVIO_CMO_MIN_ENT);\r\nviodev->cmo.entitled -= min(tmp, delta);\r\ndelta -= min(tmp, delta);\r\n}\r\n} else {\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\nreturn -ENOMEM;\r\n}\r\nout:\r\nschedule_delayed_work(&vio_cmo.balance_q, 0);\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\nreturn 0;\r\n}\r\nstatic void vio_cmo_balance(struct work_struct *work)\r\n{\r\nstruct vio_cmo *cmo;\r\nstruct vio_dev *viodev;\r\nstruct vio_cmo_dev_entry *dev_ent;\r\nunsigned long flags;\r\nsize_t avail = 0, level, chunk, need;\r\nint devcount = 0, fulfilled;\r\ncmo = container_of(work, struct vio_cmo, balance_q.work);\r\nspin_lock_irqsave(&vio_cmo.lock, flags);\r\ncmo->min = vio_cmo_num_OF_devs() * VIO_CMO_MIN_ENT;\r\nBUG_ON(cmo->min > cmo->entitled);\r\ncmo->spare = min_t(size_t, VIO_CMO_MIN_ENT, (cmo->entitled - cmo->min));\r\ncmo->min += cmo->spare;\r\ncmo->desired = cmo->min;\r\navail = cmo->entitled - cmo->spare;\r\nlist_for_each_entry(dev_ent, &vio_cmo.device_list, list) {\r\nviodev = dev_ent->viodev;\r\ndevcount++;\r\nviodev->cmo.entitled = VIO_CMO_MIN_ENT;\r\ncmo->desired += (viodev->cmo.desired - VIO_CMO_MIN_ENT);\r\navail -= max_t(size_t, viodev->cmo.allocated, VIO_CMO_MIN_ENT);\r\n}\r\nlevel = VIO_CMO_MIN_ENT;\r\nwhile (avail) {\r\nfulfilled = 0;\r\nlist_for_each_entry(dev_ent, &vio_cmo.device_list, list) {\r\nviodev = dev_ent->viodev;\r\nif (viodev->cmo.desired <= level) {\r\nfulfilled++;\r\ncontinue;\r\n}\r\nchunk = min_t(size_t, avail, VIO_CMO_BALANCE_CHUNK);\r\nchunk = min(chunk, (viodev->cmo.desired -\r\nviodev->cmo.entitled));\r\nviodev->cmo.entitled += chunk;\r\nneed = max(viodev->cmo.allocated, viodev->cmo.entitled)-\r\nmax(viodev->cmo.allocated, level);\r\navail -= need;\r\n}\r\nif (fulfilled == devcount)\r\nbreak;\r\nlevel += VIO_CMO_BALANCE_CHUNK;\r\n}\r\ncmo->reserve.size = cmo->min;\r\ncmo->excess.free = 0;\r\ncmo->excess.size = 0;\r\nneed = 0;\r\nlist_for_each_entry(dev_ent, &vio_cmo.device_list, list) {\r\nviodev = dev_ent->viodev;\r\nif (viodev->cmo.entitled)\r\ncmo->reserve.size += (viodev->cmo.entitled -\r\nVIO_CMO_MIN_ENT);\r\nif (viodev->cmo.allocated > viodev->cmo.entitled)\r\nneed += viodev->cmo.allocated - viodev->cmo.entitled;\r\n}\r\ncmo->excess.size = cmo->entitled - cmo->reserve.size;\r\ncmo->excess.free = cmo->excess.size - need;\r\ncancel_delayed_work(to_delayed_work(work));\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\n}\r\nstatic void *vio_dma_iommu_alloc_coherent(struct device *dev, size_t size,\r\ndma_addr_t *dma_handle, gfp_t flag)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\nvoid *ret;\r\nif (vio_cmo_alloc(viodev, roundup(size, PAGE_SIZE))) {\r\natomic_inc(&viodev->cmo.allocs_failed);\r\nreturn NULL;\r\n}\r\nret = dma_iommu_ops.alloc_coherent(dev, size, dma_handle, flag);\r\nif (unlikely(ret == NULL)) {\r\nvio_cmo_dealloc(viodev, roundup(size, PAGE_SIZE));\r\natomic_inc(&viodev->cmo.allocs_failed);\r\n}\r\nreturn ret;\r\n}\r\nstatic void vio_dma_iommu_free_coherent(struct device *dev, size_t size,\r\nvoid *vaddr, dma_addr_t dma_handle)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\ndma_iommu_ops.free_coherent(dev, size, vaddr, dma_handle);\r\nvio_cmo_dealloc(viodev, roundup(size, PAGE_SIZE));\r\n}\r\nstatic dma_addr_t vio_dma_iommu_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\ndma_addr_t ret = DMA_ERROR_CODE;\r\nif (vio_cmo_alloc(viodev, roundup(size, IOMMU_PAGE_SIZE))) {\r\natomic_inc(&viodev->cmo.allocs_failed);\r\nreturn ret;\r\n}\r\nret = dma_iommu_ops.map_page(dev, page, offset, size, direction, attrs);\r\nif (unlikely(dma_mapping_error(dev, ret))) {\r\nvio_cmo_dealloc(viodev, roundup(size, IOMMU_PAGE_SIZE));\r\natomic_inc(&viodev->cmo.allocs_failed);\r\n}\r\nreturn ret;\r\n}\r\nstatic void vio_dma_iommu_unmap_page(struct device *dev, dma_addr_t dma_handle,\r\nsize_t size,\r\nenum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\ndma_iommu_ops.unmap_page(dev, dma_handle, size, direction, attrs);\r\nvio_cmo_dealloc(viodev, roundup(size, IOMMU_PAGE_SIZE));\r\n}\r\nstatic int vio_dma_iommu_map_sg(struct device *dev, struct scatterlist *sglist,\r\nint nelems, enum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\nstruct scatterlist *sgl;\r\nint ret, count = 0;\r\nsize_t alloc_size = 0;\r\nfor (sgl = sglist; count < nelems; count++, sgl++)\r\nalloc_size += roundup(sgl->length, IOMMU_PAGE_SIZE);\r\nif (vio_cmo_alloc(viodev, alloc_size)) {\r\natomic_inc(&viodev->cmo.allocs_failed);\r\nreturn 0;\r\n}\r\nret = dma_iommu_ops.map_sg(dev, sglist, nelems, direction, attrs);\r\nif (unlikely(!ret)) {\r\nvio_cmo_dealloc(viodev, alloc_size);\r\natomic_inc(&viodev->cmo.allocs_failed);\r\nreturn ret;\r\n}\r\nfor (sgl = sglist, count = 0; count < ret; count++, sgl++)\r\nalloc_size -= roundup(sgl->dma_length, IOMMU_PAGE_SIZE);\r\nif (alloc_size)\r\nvio_cmo_dealloc(viodev, alloc_size);\r\nreturn ret;\r\n}\r\nstatic void vio_dma_iommu_unmap_sg(struct device *dev,\r\nstruct scatterlist *sglist, int nelems,\r\nenum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\nstruct scatterlist *sgl;\r\nsize_t alloc_size = 0;\r\nint count = 0;\r\nfor (sgl = sglist; count < nelems; count++, sgl++)\r\nalloc_size += roundup(sgl->dma_length, IOMMU_PAGE_SIZE);\r\ndma_iommu_ops.unmap_sg(dev, sglist, nelems, direction, attrs);\r\nvio_cmo_dealloc(viodev, alloc_size);\r\n}\r\nstatic int vio_dma_iommu_dma_supported(struct device *dev, u64 mask)\r\n{\r\nreturn dma_iommu_ops.dma_supported(dev, mask);\r\n}\r\nvoid vio_cmo_set_dev_desired(struct vio_dev *viodev, size_t desired)\r\n{\r\nunsigned long flags;\r\nstruct vio_cmo_dev_entry *dev_ent;\r\nint found = 0;\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\nreturn;\r\nspin_lock_irqsave(&vio_cmo.lock, flags);\r\nif (desired < VIO_CMO_MIN_ENT)\r\ndesired = VIO_CMO_MIN_ENT;\r\nlist_for_each_entry(dev_ent, &vio_cmo.device_list, list)\r\nif (viodev == dev_ent->viodev) {\r\nfound = 1;\r\nbreak;\r\n}\r\nif (!found) {\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\nreturn;\r\n}\r\nif (desired >= viodev->cmo.desired) {\r\nvio_cmo.desired += desired - viodev->cmo.desired;\r\nviodev->cmo.desired = desired;\r\n} else {\r\nvio_cmo.desired -= viodev->cmo.desired - desired;\r\nviodev->cmo.desired = desired;\r\nif (viodev->cmo.entitled > desired) {\r\nvio_cmo.reserve.size -= viodev->cmo.entitled - desired;\r\nvio_cmo.excess.size += viodev->cmo.entitled - desired;\r\nif (viodev->cmo.allocated < viodev->cmo.entitled)\r\nvio_cmo.excess.free += viodev->cmo.entitled -\r\nmax(viodev->cmo.allocated, desired);\r\nviodev->cmo.entitled = desired;\r\n}\r\n}\r\nschedule_delayed_work(&vio_cmo.balance_q, 0);\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\n}\r\nstatic int vio_cmo_bus_probe(struct vio_dev *viodev)\r\n{\r\nstruct vio_cmo_dev_entry *dev_ent;\r\nstruct device *dev = &viodev->dev;\r\nstruct vio_driver *viodrv = to_vio_driver(dev->driver);\r\nunsigned long flags;\r\nsize_t size;\r\nif (of_get_property(viodev->dev.of_node,\r\n"ibm,my-dma-window", NULL)) {\r\nif (!viodrv->get_desired_dma) {\r\ndev_err(dev, "%s: device driver does not support CMO\n",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nviodev->cmo.desired = IOMMU_PAGE_ALIGN(viodrv->get_desired_dma(viodev));\r\nif (viodev->cmo.desired < VIO_CMO_MIN_ENT)\r\nviodev->cmo.desired = VIO_CMO_MIN_ENT;\r\nsize = VIO_CMO_MIN_ENT;\r\ndev_ent = kmalloc(sizeof(struct vio_cmo_dev_entry),\r\nGFP_KERNEL);\r\nif (!dev_ent)\r\nreturn -ENOMEM;\r\ndev_ent->viodev = viodev;\r\nspin_lock_irqsave(&vio_cmo.lock, flags);\r\nlist_add(&dev_ent->list, &vio_cmo.device_list);\r\n} else {\r\nviodev->cmo.desired = 0;\r\nsize = 0;\r\nspin_lock_irqsave(&vio_cmo.lock, flags);\r\n}\r\nif (vio_cmo.min == ((vio_cmo_num_OF_devs() + 1) *\r\nVIO_CMO_MIN_ENT)) {\r\nif (size)\r\nvio_cmo.desired += (viodev->cmo.desired -\r\nVIO_CMO_MIN_ENT);\r\n} else {\r\nsize_t tmp;\r\ntmp = vio_cmo.spare + vio_cmo.excess.free;\r\nif (tmp < size) {\r\ndev_err(dev, "%s: insufficient free "\r\n"entitlement to add device. "\r\n"Need %lu, have %lu\n", __func__,\r\nsize, (vio_cmo.spare + tmp));\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\nreturn -ENOMEM;\r\n}\r\ntmp = min(size, vio_cmo.excess.free);\r\nvio_cmo.excess.free -= tmp;\r\nvio_cmo.excess.size -= tmp;\r\nvio_cmo.reserve.size += tmp;\r\nvio_cmo.spare -= size - tmp;\r\nvio_cmo.min += size;\r\nvio_cmo.desired += viodev->cmo.desired;\r\n}\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\nreturn 0;\r\n}\r\nstatic void vio_cmo_bus_remove(struct vio_dev *viodev)\r\n{\r\nstruct vio_cmo_dev_entry *dev_ent;\r\nunsigned long flags;\r\nsize_t tmp;\r\nspin_lock_irqsave(&vio_cmo.lock, flags);\r\nif (viodev->cmo.allocated) {\r\ndev_err(&viodev->dev, "%s: device had %lu bytes of IO "\r\n"allocated after remove operation.\n",\r\n__func__, viodev->cmo.allocated);\r\nBUG();\r\n}\r\nlist_for_each_entry(dev_ent, &vio_cmo.device_list, list)\r\nif (viodev == dev_ent->viodev) {\r\nlist_del(&dev_ent->list);\r\nkfree(dev_ent);\r\nbreak;\r\n}\r\nif (viodev->cmo.entitled) {\r\nvio_cmo.desired -= (viodev->cmo.desired - VIO_CMO_MIN_ENT);\r\nviodev->cmo.entitled -= VIO_CMO_MIN_ENT;\r\nif (viodev->cmo.entitled && (vio_cmo.spare < VIO_CMO_MIN_ENT)) {\r\ntmp = min(viodev->cmo.entitled, (VIO_CMO_MIN_ENT -\r\nvio_cmo.spare));\r\nvio_cmo.spare += tmp;\r\nviodev->cmo.entitled -= tmp;\r\n}\r\nvio_cmo.excess.size += viodev->cmo.entitled;\r\nvio_cmo.excess.free += viodev->cmo.entitled;\r\nvio_cmo.reserve.size -= viodev->cmo.entitled;\r\nviodev->cmo.entitled = VIO_CMO_MIN_ENT;\r\nviodev->cmo.desired = VIO_CMO_MIN_ENT;\r\natomic_set(&viodev->cmo.allocs_failed, 0);\r\n}\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\n}\r\nstatic void vio_cmo_set_dma_ops(struct vio_dev *viodev)\r\n{\r\nset_dma_ops(&viodev->dev, &vio_dma_mapping_ops);\r\n}\r\nstatic void vio_cmo_bus_init(void)\r\n{\r\nstruct hvcall_mpp_data mpp_data;\r\nint err;\r\nmemset(&vio_cmo, 0, sizeof(struct vio_cmo));\r\nspin_lock_init(&vio_cmo.lock);\r\nINIT_LIST_HEAD(&vio_cmo.device_list);\r\nINIT_DELAYED_WORK(&vio_cmo.balance_q, vio_cmo_balance);\r\nerr = h_get_mpp(&mpp_data);\r\nif (err != H_SUCCESS) {\r\nprintk(KERN_ERR "%s: unable to determine system IO "\\r\n"entitlement. (%d)\n", __func__, err);\r\nvio_cmo.entitled = 0;\r\n} else {\r\nvio_cmo.entitled = mpp_data.entitled_mem;\r\n}\r\nvio_cmo.spare = VIO_CMO_MIN_ENT;\r\nvio_cmo.reserve.size = vio_cmo.spare;\r\nvio_cmo.reserve.size += (vio_cmo_num_OF_devs() *\r\nVIO_CMO_MIN_ENT);\r\nif (vio_cmo.reserve.size > vio_cmo.entitled) {\r\nprintk(KERN_ERR "%s: insufficient system entitlement\n",\r\n__func__);\r\npanic("%s: Insufficient system entitlement", __func__);\r\n}\r\nvio_cmo.excess.size = vio_cmo.entitled - vio_cmo.reserve.size;\r\nvio_cmo.excess.free = vio_cmo.excess.size;\r\nvio_cmo.min = vio_cmo.reserve.size;\r\nvio_cmo.desired = vio_cmo.reserve.size;\r\n}\r\nstatic ssize_t viodev_cmo_allocs_failed_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\nreturn sprintf(buf, "%d\n", atomic_read(&viodev->cmo.allocs_failed));\r\n}\r\nstatic ssize_t viodev_cmo_allocs_failed_reset(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t count)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\natomic_set(&viodev->cmo.allocs_failed, 0);\r\nreturn count;\r\n}\r\nstatic ssize_t viodev_cmo_desired_set(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t count)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\nsize_t new_desired;\r\nint ret;\r\nret = strict_strtoul(buf, 10, &new_desired);\r\nif (ret)\r\nreturn ret;\r\nvio_cmo_set_dev_desired(viodev, new_desired);\r\nreturn count;\r\n}\r\nstatic ssize_t viobus_cmo_high_reset(struct bus_type *bt, const char *buf,\r\nsize_t count)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&vio_cmo.lock, flags);\r\nvio_cmo.high = vio_cmo.curr;\r\nspin_unlock_irqrestore(&vio_cmo.lock, flags);\r\nreturn count;\r\n}\r\nstatic void vio_cmo_sysfs_init(void)\r\n{\r\nvio_bus_type.dev_attrs = vio_cmo_dev_attrs;\r\nvio_bus_type.bus_attrs = vio_cmo_bus_attrs;\r\n}\r\nint vio_cmo_entitlement_update(size_t new_entitlement) { return 0; }\r\nvoid vio_cmo_set_dev_desired(struct vio_dev *viodev, size_t desired) {}\r\nstatic int vio_cmo_bus_probe(struct vio_dev *viodev) { return 0; }\r\nstatic void vio_cmo_bus_remove(struct vio_dev *viodev) {}\r\nstatic void vio_cmo_set_dma_ops(struct vio_dev *viodev) {}\r\nstatic void vio_cmo_bus_init(void) {}\r\nstatic void vio_cmo_sysfs_init(void) { }\r\nstatic struct iommu_table *vio_build_iommu_table(struct vio_dev *dev)\r\n{\r\nconst unsigned char *dma_window;\r\nstruct iommu_table *tbl;\r\nunsigned long offset, size;\r\nif (firmware_has_feature(FW_FEATURE_ISERIES))\r\nreturn vio_build_iommu_table_iseries(dev);\r\ndma_window = of_get_property(dev->dev.of_node,\r\n"ibm,my-dma-window", NULL);\r\nif (!dma_window)\r\nreturn NULL;\r\ntbl = kzalloc(sizeof(*tbl), GFP_KERNEL);\r\nif (tbl == NULL)\r\nreturn NULL;\r\nof_parse_dma_window(dev->dev.of_node, dma_window,\r\n&tbl->it_index, &offset, &size);\r\ntbl->it_size = size >> IOMMU_PAGE_SHIFT;\r\ntbl->it_offset = offset >> IOMMU_PAGE_SHIFT;\r\ntbl->it_busno = 0;\r\ntbl->it_type = TCE_VB;\r\ntbl->it_blocksize = 16;\r\nreturn iommu_init_table(tbl, -1);\r\n}\r\nstatic const struct vio_device_id *vio_match_device(\r\nconst struct vio_device_id *ids, const struct vio_dev *dev)\r\n{\r\nwhile (ids->type[0] != '\0') {\r\nif ((strncmp(dev->type, ids->type, strlen(ids->type)) == 0) &&\r\nof_device_is_compatible(dev->dev.of_node,\r\nids->compat))\r\nreturn ids;\r\nids++;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int vio_bus_probe(struct device *dev)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\nstruct vio_driver *viodrv = to_vio_driver(dev->driver);\r\nconst struct vio_device_id *id;\r\nint error = -ENODEV;\r\nif (!viodrv->probe)\r\nreturn error;\r\nid = vio_match_device(viodrv->id_table, viodev);\r\nif (id) {\r\nmemset(&viodev->cmo, 0, sizeof(viodev->cmo));\r\nif (firmware_has_feature(FW_FEATURE_CMO)) {\r\nerror = vio_cmo_bus_probe(viodev);\r\nif (error)\r\nreturn error;\r\n}\r\nerror = viodrv->probe(viodev, id);\r\nif (error && firmware_has_feature(FW_FEATURE_CMO))\r\nvio_cmo_bus_remove(viodev);\r\n}\r\nreturn error;\r\n}\r\nstatic int vio_bus_remove(struct device *dev)\r\n{\r\nstruct vio_dev *viodev = to_vio_dev(dev);\r\nstruct vio_driver *viodrv = to_vio_driver(dev->driver);\r\nstruct device *devptr;\r\nint ret = 1;\r\ndevptr = get_device(dev);\r\nif (viodrv->remove)\r\nret = viodrv->remove(viodev);\r\nif (!ret && firmware_has_feature(FW_FEATURE_CMO))\r\nvio_cmo_bus_remove(viodev);\r\nput_device(devptr);\r\nreturn ret;\r\n}\r\nint vio_register_driver(struct vio_driver *viodrv)\r\n{\r\nprintk(KERN_DEBUG "%s: driver %s registering\n", __func__,\r\nviodrv->driver.name);\r\nviodrv->driver.bus = &vio_bus_type;\r\nreturn driver_register(&viodrv->driver);\r\n}\r\nvoid vio_unregister_driver(struct vio_driver *viodrv)\r\n{\r\ndriver_unregister(&viodrv->driver);\r\n}\r\nstatic void __devinit vio_dev_release(struct device *dev)\r\n{\r\nstruct iommu_table *tbl = get_iommu_table_base(dev);\r\nif (!firmware_has_feature(FW_FEATURE_ISERIES) && tbl)\r\niommu_free_table(tbl, dev->of_node ?\r\ndev->of_node->full_name : dev_name(dev));\r\nof_node_put(dev->of_node);\r\nkfree(to_vio_dev(dev));\r\n}\r\nstruct vio_dev *vio_register_device_node(struct device_node *of_node)\r\n{\r\nstruct vio_dev *viodev;\r\nconst unsigned int *unit_address;\r\nif (of_node->type == NULL) {\r\nprintk(KERN_WARNING "%s: node %s missing 'device_type'\n",\r\n__func__,\r\nof_node->name ? of_node->name : "<unknown>");\r\nreturn NULL;\r\n}\r\nunit_address = of_get_property(of_node, "reg", NULL);\r\nif (unit_address == NULL) {\r\nprintk(KERN_WARNING "%s: node %s missing 'reg'\n",\r\n__func__,\r\nof_node->name ? of_node->name : "<unknown>");\r\nreturn NULL;\r\n}\r\nviodev = kzalloc(sizeof(struct vio_dev), GFP_KERNEL);\r\nif (viodev == NULL)\r\nreturn NULL;\r\nviodev->irq = irq_of_parse_and_map(of_node, 0);\r\ndev_set_name(&viodev->dev, "%x", *unit_address);\r\nviodev->name = of_node->name;\r\nviodev->type = of_node->type;\r\nviodev->unit_address = *unit_address;\r\nif (firmware_has_feature(FW_FEATURE_ISERIES)) {\r\nunit_address = of_get_property(of_node,\r\n"linux,unit_address", NULL);\r\nif (unit_address != NULL)\r\nviodev->unit_address = *unit_address;\r\n}\r\nviodev->dev.of_node = of_node_get(of_node);\r\nif (firmware_has_feature(FW_FEATURE_CMO))\r\nvio_cmo_set_dma_ops(viodev);\r\nelse\r\nset_dma_ops(&viodev->dev, &dma_iommu_ops);\r\nset_iommu_table_base(&viodev->dev, vio_build_iommu_table(viodev));\r\nset_dev_node(&viodev->dev, of_node_to_nid(of_node));\r\nviodev->dev.parent = &vio_bus_device.dev;\r\nviodev->dev.bus = &vio_bus_type;\r\nviodev->dev.release = vio_dev_release;\r\ndma_set_mask(&viodev->dev, DMA_BIT_MASK(64));\r\ndma_set_coherent_mask(&viodev->dev, DMA_BIT_MASK(64));\r\nif (device_register(&viodev->dev)) {\r\nprintk(KERN_ERR "%s: failed to register device %s\n",\r\n__func__, dev_name(&viodev->dev));\r\nput_device(&viodev->dev);\r\nreturn NULL;\r\n}\r\nreturn viodev;\r\n}\r\nstatic int __init vio_bus_init(void)\r\n{\r\nint err;\r\nstruct device_node *node_vroot;\r\nif (firmware_has_feature(FW_FEATURE_CMO))\r\nvio_cmo_sysfs_init();\r\nerr = bus_register(&vio_bus_type);\r\nif (err) {\r\nprintk(KERN_ERR "failed to register VIO bus\n");\r\nreturn err;\r\n}\r\nerr = device_register(&vio_bus_device.dev);\r\nif (err) {\r\nprintk(KERN_WARNING "%s: device_register returned %i\n",\r\n__func__, err);\r\nreturn err;\r\n}\r\nif (firmware_has_feature(FW_FEATURE_CMO))\r\nvio_cmo_bus_init();\r\nnode_vroot = of_find_node_by_name(NULL, "vdevice");\r\nif (node_vroot) {\r\nstruct device_node *of_node;\r\nfor (of_node = node_vroot->child; of_node != NULL;\r\nof_node = of_node->sibling)\r\nvio_register_device_node(of_node);\r\nof_node_put(node_vroot);\r\n}\r\nreturn 0;\r\n}\r\nstatic ssize_t name_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nreturn sprintf(buf, "%s\n", to_vio_dev(dev)->name);\r\n}\r\nstatic ssize_t devspec_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct device_node *of_node = dev->of_node;\r\nreturn sprintf(buf, "%s\n", of_node ? of_node->full_name : "none");\r\n}\r\nstatic ssize_t modalias_show(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nconst struct vio_dev *vio_dev = to_vio_dev(dev);\r\nstruct device_node *dn;\r\nconst char *cp;\r\ndn = dev->of_node;\r\nif (!dn)\r\nreturn -ENODEV;\r\ncp = of_get_property(dn, "compatible", NULL);\r\nif (!cp)\r\nreturn -ENODEV;\r\nreturn sprintf(buf, "vio:T%sS%s\n", vio_dev->type, cp);\r\n}\r\nvoid __devinit vio_unregister_device(struct vio_dev *viodev)\r\n{\r\ndevice_unregister(&viodev->dev);\r\n}\r\nstatic int vio_bus_match(struct device *dev, struct device_driver *drv)\r\n{\r\nconst struct vio_dev *vio_dev = to_vio_dev(dev);\r\nstruct vio_driver *vio_drv = to_vio_driver(drv);\r\nconst struct vio_device_id *ids = vio_drv->id_table;\r\nreturn (ids != NULL) && (vio_match_device(ids, vio_dev) != NULL);\r\n}\r\nstatic int vio_hotplug(struct device *dev, struct kobj_uevent_env *env)\r\n{\r\nconst struct vio_dev *vio_dev = to_vio_dev(dev);\r\nstruct device_node *dn;\r\nconst char *cp;\r\ndn = dev->of_node;\r\nif (!dn)\r\nreturn -ENODEV;\r\ncp = of_get_property(dn, "compatible", NULL);\r\nif (!cp)\r\nreturn -ENODEV;\r\nadd_uevent_var(env, "MODALIAS=vio:T%sS%s", vio_dev->type, cp);\r\nreturn 0;\r\n}\r\nconst void *vio_get_attribute(struct vio_dev *vdev, char *which, int *length)\r\n{\r\nreturn of_get_property(vdev->dev.of_node, which, length);\r\n}\r\nstatic struct vio_dev *vio_find_name(const char *name)\r\n{\r\nstruct device *found;\r\nfound = bus_find_device_by_name(&vio_bus_type, NULL, name);\r\nif (!found)\r\nreturn NULL;\r\nreturn to_vio_dev(found);\r\n}\r\nstruct vio_dev *vio_find_node(struct device_node *vnode)\r\n{\r\nconst uint32_t *unit_address;\r\nchar kobj_name[20];\r\nunit_address = of_get_property(vnode, "reg", NULL);\r\nif (!unit_address)\r\nreturn NULL;\r\nsnprintf(kobj_name, sizeof(kobj_name), "%x", *unit_address);\r\nreturn vio_find_name(kobj_name);\r\n}\r\nint vio_enable_interrupts(struct vio_dev *dev)\r\n{\r\nint rc = h_vio_signal(dev->unit_address, VIO_IRQ_ENABLE);\r\nif (rc != H_SUCCESS)\r\nprintk(KERN_ERR "vio: Error 0x%x enabling interrupts\n", rc);\r\nreturn rc;\r\n}\r\nint vio_disable_interrupts(struct vio_dev *dev)\r\n{\r\nint rc = h_vio_signal(dev->unit_address, VIO_IRQ_DISABLE);\r\nif (rc != H_SUCCESS)\r\nprintk(KERN_ERR "vio: Error 0x%x disabling interrupts\n", rc);\r\nreturn rc;\r\n}
