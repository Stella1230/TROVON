static int __init setup_unaligned_printk(char *str)\r\n{\r\nlong val;\r\nif (strict_strtol(str, 0, &val) != 0)\r\nreturn 0;\r\nunaligned_printk = val;\r\npr_info("Printk for each unaligned data accesses is %s\n",\r\nunaligned_printk ? "enabled" : "disabled");\r\nreturn 1;\r\n}\r\nstatic inline tile_bundle_bits set_BrOff_X1(tile_bundle_bits n, s32 offset)\r\n{\r\ntile_bundle_bits result;\r\ntile_bundle_bits mask = create_BrOff_X1(-1);\r\nresult = n & (~mask);\r\nresult |= create_BrOff_X1(offset);\r\nreturn result;\r\n}\r\nstatic inline tile_bundle_bits move_X1(tile_bundle_bits n, int dest, int src)\r\n{\r\ntile_bundle_bits result;\r\ntile_bundle_bits op;\r\nresult = n & (~TILE_X1_MASK);\r\nop = create_Opcode_X1(SPECIAL_0_OPCODE_X1) |\r\ncreate_RRROpcodeExtension_X1(OR_SPECIAL_0_OPCODE_X1) |\r\ncreate_Dest_X1(dest) |\r\ncreate_SrcB_X1(TREG_ZERO) |\r\ncreate_SrcA_X1(src) ;\r\nresult |= op;\r\nreturn result;\r\n}\r\nstatic inline tile_bundle_bits nop_X1(tile_bundle_bits n)\r\n{\r\nreturn move_X1(n, TREG_ZERO, TREG_ZERO);\r\n}\r\nstatic inline tile_bundle_bits addi_X1(\r\ntile_bundle_bits n, int dest, int src, int imm)\r\n{\r\nn &= ~TILE_X1_MASK;\r\nn |= (create_SrcA_X1(src) |\r\ncreate_Dest_X1(dest) |\r\ncreate_Imm8_X1(imm) |\r\ncreate_S_X1(0) |\r\ncreate_Opcode_X1(IMM_0_OPCODE_X1) |\r\ncreate_ImmOpcodeExtension_X1(ADDI_IMM_0_OPCODE_X1));\r\nreturn n;\r\n}\r\nstatic tile_bundle_bits rewrite_load_store_unaligned(\r\nstruct single_step_state *state,\r\ntile_bundle_bits bundle,\r\nstruct pt_regs *regs,\r\nenum mem_op mem_op,\r\nint size, int sign_ext)\r\n{\r\nunsigned char __user *addr;\r\nint val_reg, addr_reg, err, val;\r\nif (bundle & TILE_BUNDLE_Y_ENCODING_MASK) {\r\naddr_reg = get_SrcA_Y2(bundle);\r\nval_reg = get_SrcBDest_Y2(bundle);\r\n} else if (mem_op == MEMOP_LOAD || mem_op == MEMOP_LOAD_POSTINCR) {\r\naddr_reg = get_SrcA_X1(bundle);\r\nval_reg = get_Dest_X1(bundle);\r\n} else {\r\naddr_reg = get_SrcA_X1(bundle);\r\nval_reg = get_SrcB_X1(bundle);\r\n}\r\nif ((val_reg >= PTREGS_NR_GPRS &&\r\n(val_reg != TREG_ZERO ||\r\nmem_op == MEMOP_LOAD ||\r\nmem_op == MEMOP_LOAD_POSTINCR)) ||\r\naddr_reg >= PTREGS_NR_GPRS)\r\nreturn bundle;\r\naddr = (void __user *)regs->regs[addr_reg];\r\nif (((unsigned long)addr % size) == 0)\r\nreturn bundle;\r\n#ifndef __LITTLE_ENDIAN\r\n# error We assume little-endian representation with copy_xx_user size 2 here\r\n#endif\r\nif (mem_op == MEMOP_LOAD || mem_op == MEMOP_LOAD_POSTINCR) {\r\nunsigned short val_16;\r\nswitch (size) {\r\ncase 2:\r\nerr = copy_from_user(&val_16, addr, sizeof(val_16));\r\nval = sign_ext ? ((short)val_16) : val_16;\r\nbreak;\r\ncase 4:\r\nerr = copy_from_user(&val, addr, sizeof(val));\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nif (err == 0) {\r\nstate->update_reg = val_reg;\r\nstate->update_value = val;\r\nstate->update = 1;\r\n}\r\n} else {\r\nval = (val_reg == TREG_ZERO) ? 0 : regs->regs[val_reg];\r\nerr = copy_to_user(addr, &val, size);\r\n}\r\nif (err) {\r\nsiginfo_t info = {\r\n.si_signo = SIGSEGV,\r\n.si_code = SEGV_MAPERR,\r\n.si_addr = addr\r\n};\r\ntrace_unhandled_signal("segfault", regs,\r\n(unsigned long)addr, SIGSEGV);\r\nforce_sig_info(info.si_signo, &info, current);\r\nreturn (tile_bundle_bits) 0;\r\n}\r\nif (unaligned_fixup == 0) {\r\nsiginfo_t info = {\r\n.si_signo = SIGBUS,\r\n.si_code = BUS_ADRALN,\r\n.si_addr = addr\r\n};\r\ntrace_unhandled_signal("unaligned trap", regs,\r\n(unsigned long)addr, SIGBUS);\r\nforce_sig_info(info.si_signo, &info, current);\r\nreturn (tile_bundle_bits) 0;\r\n}\r\nif (unaligned_printk || unaligned_fixup_count == 0) {\r\npr_info("Process %d/%s: PC %#lx: Fixup of"\r\n" unaligned %s at %#lx.\n",\r\ncurrent->pid, current->comm, regs->pc,\r\n(mem_op == MEMOP_LOAD ||\r\nmem_op == MEMOP_LOAD_POSTINCR) ?\r\n"load" : "store",\r\n(unsigned long)addr);\r\nif (!unaligned_printk) {\r\n#define P pr_info\r\nP("\n");\r\nP("Unaligned fixups in the kernel will slow your application considerably.\n");\r\nP("To find them, write a \"1\" to /proc/sys/tile/unaligned_fixup/printk,\n");\r\nP("which requests the kernel show all unaligned fixups, or write a \"0\"\n");\r\nP("to /proc/sys/tile/unaligned_fixup/enabled, in which case each unaligned\n");\r\nP("access will become a SIGBUS you can debug. No further warnings will be\n");\r\nP("shown so as to avoid additional slowdown, but you can track the number\n");\r\nP("of fixups performed via /proc/sys/tile/unaligned_fixup/count.\n");\r\nP("Use the tile-addr2line command (see \"info addr2line\") to decode PCs.\n");\r\nP("\n");\r\n#undef P\r\n}\r\n}\r\n++unaligned_fixup_count;\r\nif (bundle & TILE_BUNDLE_Y_ENCODING_MASK) {\r\nbundle &= ~(create_SrcBDest_Y2(-1) |\r\ncreate_Opcode_Y2(-1));\r\nbundle |= (create_SrcBDest_Y2(TREG_ZERO) |\r\ncreate_Opcode_Y2(LW_OPCODE_Y2));\r\n} else if (mem_op == MEMOP_LOAD_POSTINCR) {\r\nbundle = addi_X1(bundle, addr_reg, addr_reg,\r\nget_Imm8_X1(bundle));\r\n} else if (mem_op == MEMOP_STORE_POSTINCR) {\r\nbundle = addi_X1(bundle, addr_reg, addr_reg,\r\nget_Dest_Imm8_X1(bundle));\r\n} else {\r\nbundle &= ~(create_Opcode_X1(-1) |\r\ncreate_UnShOpcodeExtension_X1(-1) |\r\ncreate_UnOpcodeExtension_X1(-1));\r\nbundle |= (create_Opcode_X1(SHUN_0_OPCODE_X1) |\r\ncreate_UnShOpcodeExtension_X1(\r\nUN_0_SHUN_0_OPCODE_X1) |\r\ncreate_UnOpcodeExtension_X1(\r\nNOP_UN_0_SHUN_0_OPCODE_X1));\r\n}\r\nreturn bundle;\r\n}\r\nvoid single_step_execve(void)\r\n{\r\nstruct thread_info *ti = current_thread_info();\r\nkfree(ti->step_state);\r\nti->step_state = NULL;\r\n}\r\nvoid single_step_once(struct pt_regs *regs)\r\n{\r\nextern tile_bundle_bits __single_step_ill_insn;\r\nextern tile_bundle_bits __single_step_j_insn;\r\nextern tile_bundle_bits __single_step_addli_insn;\r\nextern tile_bundle_bits __single_step_auli_insn;\r\nstruct thread_info *info = (void *)current_thread_info();\r\nstruct single_step_state *state = info->step_state;\r\nint is_single_step = test_ti_thread_flag(info, TIF_SINGLESTEP);\r\ntile_bundle_bits __user *buffer, *pc;\r\ntile_bundle_bits bundle;\r\nint temp_reg;\r\nint target_reg = TREG_LR;\r\nint err;\r\nenum mem_op mem_op = MEMOP_NONE;\r\nint size = 0, sign_ext = 0;\r\nasm(\r\n" .pushsection .rodata.single_step\n"\r\n" .align 8\n"\r\n" .globl __single_step_ill_insn\n"\r\n"__single_step_ill_insn:\n"\r\n" ill\n"\r\n" .globl __single_step_addli_insn\n"\r\n"__single_step_addli_insn:\n"\r\n" { nop; addli r0, zero, 0 }\n"\r\n" .globl __single_step_auli_insn\n"\r\n"__single_step_auli_insn:\n"\r\n" { nop; auli r0, r0, 0 }\n"\r\n" .globl __single_step_j_insn\n"\r\n"__single_step_j_insn:\n"\r\n" j .\n"\r\n" .popsection\n"\r\n);\r\nlocal_irq_enable();\r\nif (state == NULL) {\r\nstate = kmalloc(sizeof(struct single_step_state), GFP_KERNEL);\r\nif (state == NULL) {\r\npr_err("Out of kernel memory trying to single-step\n");\r\nreturn;\r\n}\r\ndown_write(&current->mm->mmap_sem);\r\nbuffer = (void __user *) do_mmap(NULL, 0, 64,\r\nPROT_EXEC | PROT_READ | PROT_WRITE,\r\nMAP_PRIVATE | MAP_ANONYMOUS,\r\n0);\r\nup_write(&current->mm->mmap_sem);\r\nif (IS_ERR((void __force *)buffer)) {\r\nkfree(state);\r\npr_err("Out of kernel pages trying to single-step\n");\r\nreturn;\r\n}\r\nstate->buffer = buffer;\r\nstate->is_enabled = 0;\r\ninfo->step_state = state;\r\nBUG_ON(get_Opcode_X1(__single_step_addli_insn) !=\r\nADDLI_OPCODE_X1);\r\nBUG_ON(get_Opcode_X1(__single_step_auli_insn) !=\r\nAULI_OPCODE_X1);\r\nBUG_ON(get_SrcA_X1(__single_step_addli_insn) != TREG_ZERO);\r\nBUG_ON(get_Dest_X1(__single_step_addli_insn) != 0);\r\nBUG_ON(get_JOffLong_X1(__single_step_j_insn) != 0);\r\n}\r\nif (regs->faultnum == INT_SWINT_1)\r\nregs->pc -= 8;\r\npc = (tile_bundle_bits __user *)(regs->pc);\r\nif (get_user(bundle, pc) != 0) {\r\npr_err("Couldn't read instruction at %p trying to step\n", pc);\r\nreturn;\r\n}\r\nstate->orig_pc = (unsigned long)pc;\r\nstate->next_pc = (unsigned long)(pc + 1);\r\nstate->branch_next_pc = 0;\r\nstate->update = 0;\r\nif (!(bundle & TILE_BUNDLE_Y_ENCODING_MASK)) {\r\nint opcode = get_Opcode_X1(bundle);\r\nswitch (opcode) {\r\ncase BRANCH_OPCODE_X1:\r\n{\r\ns32 offset = signExtend17(get_BrOff_X1(bundle));\r\nstate->branch_next_pc = (unsigned long)(pc + offset);\r\nbundle = set_BrOff_X1(bundle, 2);\r\n}\r\nbreak;\r\ncase JALB_OPCODE_X1:\r\ncase JALF_OPCODE_X1:\r\nstate->update = 1;\r\nstate->next_pc =\r\n(unsigned long) (pc + get_JOffLong_X1(bundle));\r\nbreak;\r\ncase JB_OPCODE_X1:\r\ncase JF_OPCODE_X1:\r\nstate->next_pc =\r\n(unsigned long) (pc + get_JOffLong_X1(bundle));\r\nbundle = nop_X1(bundle);\r\nbreak;\r\ncase SPECIAL_0_OPCODE_X1:\r\nswitch (get_RRROpcodeExtension_X1(bundle)) {\r\ncase JALRP_SPECIAL_0_OPCODE_X1:\r\ncase JALR_SPECIAL_0_OPCODE_X1:\r\nstate->update = 1;\r\nstate->next_pc =\r\nregs->regs[get_SrcA_X1(bundle)];\r\nbreak;\r\ncase JRP_SPECIAL_0_OPCODE_X1:\r\ncase JR_SPECIAL_0_OPCODE_X1:\r\nstate->next_pc =\r\nregs->regs[get_SrcA_X1(bundle)];\r\nbundle = nop_X1(bundle);\r\nbreak;\r\ncase LNK_SPECIAL_0_OPCODE_X1:\r\nstate->update = 1;\r\ntarget_reg = get_Dest_X1(bundle);\r\nbreak;\r\ncase SH_SPECIAL_0_OPCODE_X1:\r\nmem_op = MEMOP_STORE;\r\nsize = 2;\r\nbreak;\r\ncase SW_SPECIAL_0_OPCODE_X1:\r\nmem_op = MEMOP_STORE;\r\nsize = 4;\r\nbreak;\r\n}\r\nbreak;\r\ncase SHUN_0_OPCODE_X1:\r\nif (get_UnShOpcodeExtension_X1(bundle) ==\r\nUN_0_SHUN_0_OPCODE_X1) {\r\nswitch (get_UnOpcodeExtension_X1(bundle)) {\r\ncase LH_UN_0_SHUN_0_OPCODE_X1:\r\nmem_op = MEMOP_LOAD;\r\nsize = 2;\r\nsign_ext = 1;\r\nbreak;\r\ncase LH_U_UN_0_SHUN_0_OPCODE_X1:\r\nmem_op = MEMOP_LOAD;\r\nsize = 2;\r\nsign_ext = 0;\r\nbreak;\r\ncase LW_UN_0_SHUN_0_OPCODE_X1:\r\nmem_op = MEMOP_LOAD;\r\nsize = 4;\r\nbreak;\r\ncase IRET_UN_0_SHUN_0_OPCODE_X1:\r\n{\r\nunsigned long ex0_0 = __insn_mfspr(\r\nSPR_EX_CONTEXT_0_0);\r\nunsigned long ex0_1 = __insn_mfspr(\r\nSPR_EX_CONTEXT_0_1);\r\nif (EX1_PL(ex0_1) == USER_PL) {\r\nstate->next_pc = ex0_0;\r\nregs->ex1 = ex0_1;\r\nbundle = nop_X1(bundle);\r\n}\r\n}\r\n}\r\n}\r\nbreak;\r\n#if CHIP_HAS_WH64()\r\ncase IMM_0_OPCODE_X1:\r\nswitch (get_ImmOpcodeExtension_X1(bundle)) {\r\ncase LWADD_IMM_0_OPCODE_X1:\r\nmem_op = MEMOP_LOAD_POSTINCR;\r\nsize = 4;\r\nbreak;\r\ncase LHADD_IMM_0_OPCODE_X1:\r\nmem_op = MEMOP_LOAD_POSTINCR;\r\nsize = 2;\r\nsign_ext = 1;\r\nbreak;\r\ncase LHADD_U_IMM_0_OPCODE_X1:\r\nmem_op = MEMOP_LOAD_POSTINCR;\r\nsize = 2;\r\nsign_ext = 0;\r\nbreak;\r\ncase SWADD_IMM_0_OPCODE_X1:\r\nmem_op = MEMOP_STORE_POSTINCR;\r\nsize = 4;\r\nbreak;\r\ncase SHADD_IMM_0_OPCODE_X1:\r\nmem_op = MEMOP_STORE_POSTINCR;\r\nsize = 2;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\n#endif\r\n}\r\nif (state->update) {\r\nu32 mask = (u32) ~((1ULL << get_Dest_X0(bundle)) |\r\n(1ULL << get_SrcA_X0(bundle)) |\r\n(1ULL << get_SrcB_X0(bundle)) |\r\n(1ULL << target_reg));\r\ntemp_reg = __builtin_ctz(mask);\r\nstate->update_reg = temp_reg;\r\nstate->update_value = regs->regs[temp_reg];\r\nregs->regs[temp_reg] = (unsigned long) (pc+1);\r\nregs->flags |= PT_FLAGS_RESTORE_REGS;\r\nbundle = move_X1(bundle, target_reg, temp_reg);\r\n}\r\n} else {\r\nint opcode = get_Opcode_Y2(bundle);\r\nswitch (opcode) {\r\ncase LH_OPCODE_Y2:\r\nmem_op = MEMOP_LOAD;\r\nsize = 2;\r\nsign_ext = 1;\r\nbreak;\r\ncase LH_U_OPCODE_Y2:\r\nmem_op = MEMOP_LOAD;\r\nsize = 2;\r\nsign_ext = 0;\r\nbreak;\r\ncase LW_OPCODE_Y2:\r\nmem_op = MEMOP_LOAD;\r\nsize = 4;\r\nbreak;\r\ncase SH_OPCODE_Y2:\r\nmem_op = MEMOP_STORE;\r\nsize = 2;\r\nbreak;\r\ncase SW_OPCODE_Y2:\r\nmem_op = MEMOP_STORE;\r\nsize = 4;\r\nbreak;\r\n}\r\n}\r\nif (mem_op != MEMOP_NONE && unaligned_fixup >= 0) {\r\nbundle = rewrite_load_store_unaligned(state, bundle, regs,\r\nmem_op, size, sign_ext);\r\nif (bundle == 0)\r\nreturn;\r\n}\r\nbuffer = state->buffer;\r\nerr = __put_user(bundle, buffer++);\r\nif (is_single_step) {\r\nerr |= __put_user(__single_step_ill_insn, buffer++);\r\nerr |= __put_user(__single_step_ill_insn, buffer++);\r\n} else {\r\nlong delta;\r\nif (state->update) {\r\nint ha16;\r\nbundle = __single_step_addli_insn;\r\nbundle |= create_Dest_X1(state->update_reg);\r\nbundle |= create_Imm16_X1(state->update_value);\r\nerr |= __put_user(bundle, buffer++);\r\nbundle = __single_step_auli_insn;\r\nbundle |= create_Dest_X1(state->update_reg);\r\nbundle |= create_SrcA_X1(state->update_reg);\r\nha16 = (state->update_value + 0x8000) >> 16;\r\nbundle |= create_Imm16_X1(ha16);\r\nerr |= __put_user(bundle, buffer++);\r\nstate->update = 0;\r\n}\r\ndelta = ((regs->pc + TILE_BUNDLE_SIZE_IN_BYTES) -\r\n(unsigned long)buffer) >>\r\nTILE_LOG2_BUNDLE_ALIGNMENT_IN_BYTES;\r\nbundle = __single_step_j_insn;\r\nbundle |= create_JOffLong_X1(delta);\r\nerr |= __put_user(bundle, buffer++);\r\n}\r\nif (err) {\r\npr_err("Fault when writing to single-step buffer\n");\r\nreturn;\r\n}\r\n__flush_icache_range((unsigned long)state->buffer,\r\n(unsigned long)buffer);\r\nstate->is_enabled = is_single_step;\r\nregs->pc = (unsigned long)state->buffer;\r\nif (regs->faultnum == INT_SWINT_1)\r\nregs->pc += 8;\r\n}\r\nvoid gx_singlestep_handle(struct pt_regs *regs, int fault_num)\r\n{\r\nunsigned long *ss_pc = &__get_cpu_var(ss_saved_pc);\r\nstruct thread_info *info = (void *)current_thread_info();\r\nint is_single_step = test_ti_thread_flag(info, TIF_SINGLESTEP);\r\nunsigned long control = __insn_mfspr(SPR_SINGLE_STEP_CONTROL_K);\r\nif (is_single_step == 0) {\r\n__insn_mtspr(SPR_SINGLE_STEP_EN_K_K, 0);\r\n} else if ((*ss_pc != regs->pc) ||\r\n(!(control & SPR_SINGLE_STEP_CONTROL_1__CANCELED_MASK))) {\r\nptrace_notify(SIGTRAP);\r\ncontrol |= SPR_SINGLE_STEP_CONTROL_1__CANCELED_MASK;\r\ncontrol |= SPR_SINGLE_STEP_CONTROL_1__INHIBIT_MASK;\r\n__insn_mtspr(SPR_SINGLE_STEP_CONTROL_K, control);\r\n}\r\n}\r\nvoid single_step_once(struct pt_regs *regs)\r\n{\r\nunsigned long *ss_pc = &__get_cpu_var(ss_saved_pc);\r\nunsigned long control = __insn_mfspr(SPR_SINGLE_STEP_CONTROL_K);\r\n*ss_pc = regs->pc;\r\ncontrol |= SPR_SINGLE_STEP_CONTROL_1__CANCELED_MASK;\r\ncontrol |= SPR_SINGLE_STEP_CONTROL_1__INHIBIT_MASK;\r\n__insn_mtspr(SPR_SINGLE_STEP_CONTROL_K, control);\r\n__insn_mtspr(SPR_SINGLE_STEP_EN_K_K, 1 << USER_PL);\r\n}\r\nvoid single_step_execve(void)\r\n{\r\n}
