static void ll_cl_fini(struct ll_cl_context *lcc)\r\n{\r\nstruct lu_env *env = lcc->lcc_env;\r\nstruct cl_io *io = lcc->lcc_io;\r\nstruct cl_page *page = lcc->lcc_page;\r\nLASSERT(lcc->lcc_cookie == current);\r\nLASSERT(env != NULL);\r\nif (page != NULL) {\r\nlu_ref_del(&page->cp_reference, "cl_io", io);\r\ncl_page_put(env, page);\r\n}\r\ncl_env_put(env, &lcc->lcc_refcheck);\r\n}\r\nstatic struct ll_cl_context *ll_cl_init(struct file *file,\r\nstruct page *vmpage, int create)\r\n{\r\nstruct ll_cl_context *lcc;\r\nstruct lu_env *env;\r\nstruct cl_io *io;\r\nstruct cl_object *clob;\r\nstruct ccc_io *cio;\r\nint refcheck;\r\nint result = 0;\r\nclob = ll_i2info(vmpage->mapping->host)->lli_clob;\r\nLASSERT(clob != NULL);\r\nenv = cl_env_get(&refcheck);\r\nif (IS_ERR(env))\r\nreturn ERR_CAST(env);\r\nlcc = &vvp_env_info(env)->vti_io_ctx;\r\nmemset(lcc, 0, sizeof(*lcc));\r\nlcc->lcc_env = env;\r\nlcc->lcc_refcheck = refcheck;\r\nlcc->lcc_cookie = current;\r\ncio = ccc_env_io(env);\r\nio = cio->cui_cl.cis_io;\r\nif (io == NULL && create) {\r\nstruct inode *inode = vmpage->mapping->host;\r\nloff_t pos;\r\nif (mutex_trylock(&inode->i_mutex)) {\r\nmutex_unlock(&(inode)->i_mutex);\r\nCERROR("Proc %s is dirtying page w/o inode lock, this will break truncate\n",\r\ncurrent->comm);\r\ndump_stack();\r\nLBUG();\r\nreturn ERR_PTR(-EIO);\r\n}\r\nio = ccc_env_thread_io(env);\r\nll_io_init(io, file, 1);\r\nio->ci_lockreq = CILR_NEVER;\r\npos = vmpage->index << PAGE_CACHE_SHIFT;\r\nresult = cl_io_rw_init(env, io, CIT_WRITE, pos, PAGE_CACHE_SIZE);\r\nif (result == 0) {\r\ncio->cui_fd = LUSTRE_FPRIVATE(file);\r\ncio->cui_iter = NULL;\r\nresult = cl_io_iter_init(env, io);\r\nif (result == 0) {\r\nresult = cl_io_lock(env, io);\r\nif (result == 0)\r\nresult = cl_io_start(env, io);\r\n}\r\n} else\r\nresult = io->ci_result;\r\n}\r\nlcc->lcc_io = io;\r\nif (io == NULL)\r\nresult = -EIO;\r\nif (result == 0) {\r\nstruct cl_page *page;\r\nLASSERT(io != NULL);\r\nLASSERT(io->ci_state == CIS_IO_GOING);\r\nLASSERT(cio->cui_fd == LUSTRE_FPRIVATE(file));\r\npage = cl_page_find(env, clob, vmpage->index, vmpage,\r\nCPT_CACHEABLE);\r\nif (!IS_ERR(page)) {\r\nlcc->lcc_page = page;\r\nlu_ref_add(&page->cp_reference, "cl_io", io);\r\nresult = 0;\r\n} else\r\nresult = PTR_ERR(page);\r\n}\r\nif (result) {\r\nll_cl_fini(lcc);\r\nlcc = ERR_PTR(result);\r\n}\r\nCDEBUG(D_VFSTRACE, "%lu@"DFID" -> %d %p %p\n",\r\nvmpage->index, PFID(lu_object_fid(&clob->co_lu)), result,\r\nenv, io);\r\nreturn lcc;\r\n}\r\nstatic struct ll_cl_context *ll_cl_get(void)\r\n{\r\nstruct ll_cl_context *lcc;\r\nstruct lu_env *env;\r\nint refcheck;\r\nenv = cl_env_get(&refcheck);\r\nLASSERT(!IS_ERR(env));\r\nlcc = &vvp_env_info(env)->vti_io_ctx;\r\nLASSERT(env == lcc->lcc_env);\r\nLASSERT(current == lcc->lcc_cookie);\r\ncl_env_put(env, &refcheck);\r\nreturn lcc;\r\n}\r\nint ll_prepare_write(struct file *file, struct page *vmpage, unsigned from,\r\nunsigned to)\r\n{\r\nstruct ll_cl_context *lcc;\r\nint result;\r\nlcc = ll_cl_init(file, vmpage, 1);\r\nif (!IS_ERR(lcc)) {\r\nstruct lu_env *env = lcc->lcc_env;\r\nstruct cl_io *io = lcc->lcc_io;\r\nstruct cl_page *page = lcc->lcc_page;\r\ncl_page_assume(env, io, page);\r\nresult = cl_io_prepare_write(env, io, page, from, to);\r\nif (result == 0) {\r\ncl_page_get(page);\r\nlu_ref_add(&page->cp_reference, "prepare_write",\r\ncurrent);\r\n} else {\r\ncl_page_unassume(env, io, page);\r\nll_cl_fini(lcc);\r\n}\r\n} else {\r\nresult = PTR_ERR(lcc);\r\n}\r\nreturn result;\r\n}\r\nint ll_commit_write(struct file *file, struct page *vmpage, unsigned from,\r\nunsigned to)\r\n{\r\nstruct ll_cl_context *lcc;\r\nstruct lu_env *env;\r\nstruct cl_io *io;\r\nstruct cl_page *page;\r\nint result = 0;\r\nlcc = ll_cl_get();\r\nenv = lcc->lcc_env;\r\npage = lcc->lcc_page;\r\nio = lcc->lcc_io;\r\nLASSERT(cl_page_is_owned(page, io));\r\nLASSERT(from <= to);\r\nif (from != to)\r\nresult = cl_io_commit_write(env, io, page, from, to);\r\nif (cl_page_is_owned(page, io))\r\ncl_page_unassume(env, io, page);\r\nlu_ref_del(&page->cp_reference, "prepare_write", current);\r\ncl_page_put(env, page);\r\nll_cl_fini(lcc);\r\nreturn result;\r\n}\r\nstruct obd_capa *cl_capa_lookup(struct inode *inode, enum cl_req_type crt)\r\n{\r\n__u64 opc;\r\nopc = crt == CRT_WRITE ? CAPA_OPC_OSS_WRITE : CAPA_OPC_OSS_RW;\r\nreturn ll_osscapa_get(inode, opc);\r\n}\r\nstatic unsigned long ll_ra_count_get(struct ll_sb_info *sbi,\r\nstruct ra_io_arg *ria,\r\nunsigned long pages)\r\n{\r\nstruct ll_ra_info *ra = &sbi->ll_ra_info;\r\nlong ret;\r\nret = min(ra->ra_max_pages - atomic_read(&ra->ra_cur_pages), pages);\r\nif (ret < 0 || ret < min_t(long, PTLRPC_MAX_BRW_PAGES, pages)) {\r\nret = 0;\r\ngoto out;\r\n}\r\nif (ria->ria_pages == 0) {\r\nlong beyond_rpc = (ria->ria_start + ret) % PTLRPC_MAX_BRW_PAGES;\r\nif ( beyond_rpc < ret)\r\nret -= beyond_rpc;\r\n}\r\nif (atomic_add_return(ret, &ra->ra_cur_pages) > ra->ra_max_pages) {\r\natomic_sub(ret, &ra->ra_cur_pages);\r\nret = 0;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nvoid ll_ra_count_put(struct ll_sb_info *sbi, unsigned long len)\r\n{\r\nstruct ll_ra_info *ra = &sbi->ll_ra_info;\r\natomic_sub(len, &ra->ra_cur_pages);\r\n}\r\nstatic void ll_ra_stats_inc_sbi(struct ll_sb_info *sbi, enum ra_stat which)\r\n{\r\nLASSERTF(which >= 0 && which < _NR_RA_STAT, "which: %u\n", which);\r\nlprocfs_counter_incr(sbi->ll_ra_stats, which);\r\n}\r\nvoid ll_ra_stats_inc(struct address_space *mapping, enum ra_stat which)\r\n{\r\nstruct ll_sb_info *sbi = ll_i2sbi(mapping->host);\r\nll_ra_stats_inc_sbi(sbi, which);\r\n}\r\nstatic int index_in_window(unsigned long index, unsigned long point,\r\nunsigned long before, unsigned long after)\r\n{\r\nunsigned long start = point - before, end = point + after;\r\nif (start > point)\r\nstart = 0;\r\nif (end < point)\r\nend = ~0;\r\nreturn start <= index && index <= end;\r\n}\r\nstatic struct ll_readahead_state *ll_ras_get(struct file *f)\r\n{\r\nstruct ll_file_data *fd;\r\nfd = LUSTRE_FPRIVATE(f);\r\nreturn &fd->fd_ras;\r\n}\r\nvoid ll_ra_read_in(struct file *f, struct ll_ra_read *rar)\r\n{\r\nstruct ll_readahead_state *ras;\r\nras = ll_ras_get(f);\r\nspin_lock(&ras->ras_lock);\r\nras->ras_requests++;\r\nras->ras_request_index = 0;\r\nras->ras_consecutive_requests++;\r\nrar->lrr_reader = current;\r\nlist_add(&rar->lrr_linkage, &ras->ras_read_beads);\r\nspin_unlock(&ras->ras_lock);\r\n}\r\nvoid ll_ra_read_ex(struct file *f, struct ll_ra_read *rar)\r\n{\r\nstruct ll_readahead_state *ras;\r\nras = ll_ras_get(f);\r\nspin_lock(&ras->ras_lock);\r\nlist_del_init(&rar->lrr_linkage);\r\nspin_unlock(&ras->ras_lock);\r\n}\r\nstatic struct ll_ra_read *ll_ra_read_get_locked(struct ll_readahead_state *ras)\r\n{\r\nstruct ll_ra_read *scan;\r\nlist_for_each_entry(scan, &ras->ras_read_beads, lrr_linkage) {\r\nif (scan->lrr_reader == current)\r\nreturn scan;\r\n}\r\nreturn NULL;\r\n}\r\nstruct ll_ra_read *ll_ra_read_get(struct file *f)\r\n{\r\nstruct ll_readahead_state *ras;\r\nstruct ll_ra_read *bead;\r\nras = ll_ras_get(f);\r\nspin_lock(&ras->ras_lock);\r\nbead = ll_ra_read_get_locked(ras);\r\nspin_unlock(&ras->ras_lock);\r\nreturn bead;\r\n}\r\nstatic int cl_read_ahead_page(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page_list *queue, struct cl_page *page,\r\nstruct page *vmpage)\r\n{\r\nstruct ccc_page *cp;\r\nint rc;\r\nrc = 0;\r\ncl_page_assume(env, io, page);\r\nlu_ref_add(&page->cp_reference, "ra", current);\r\ncp = cl2ccc_page(cl_page_at(page, &vvp_device_type));\r\nif (!cp->cpg_defer_uptodate && !PageUptodate(vmpage)) {\r\nrc = cl_page_is_under_lock(env, io, page);\r\nif (rc == -EBUSY) {\r\ncp->cpg_defer_uptodate = 1;\r\ncp->cpg_ra_used = 0;\r\ncl_page_list_add(queue, page);\r\nrc = 1;\r\n} else {\r\ncl_page_delete(env, page);\r\nrc = -ENOLCK;\r\n}\r\n} else {\r\ncl_page_unassume(env, io, page);\r\n}\r\nlu_ref_del(&page->cp_reference, "ra", current);\r\ncl_page_put(env, page);\r\nreturn rc;\r\n}\r\nstatic int ll_read_ahead_page(const struct lu_env *env, struct cl_io *io,\r\nstruct cl_page_list *queue,\r\npgoff_t index, struct address_space *mapping)\r\n{\r\nstruct page *vmpage;\r\nstruct cl_object *clob = ll_i2info(mapping->host)->lli_clob;\r\nstruct cl_page *page;\r\nenum ra_stat which = _NR_RA_STAT;\r\nint rc = 0;\r\nconst char *msg = NULL;\r\nvmpage = grab_cache_page_nowait(mapping, index);\r\nif (vmpage != NULL) {\r\nif (vmpage->mapping == mapping) {\r\npage = cl_page_find(env, clob, vmpage->index,\r\nvmpage, CPT_CACHEABLE);\r\nif (!IS_ERR(page)) {\r\nrc = cl_read_ahead_page(env, io, queue,\r\npage, vmpage);\r\nif (rc == -ENOLCK) {\r\nwhich = RA_STAT_FAILED_MATCH;\r\nmsg = "lock match failed";\r\n}\r\n} else {\r\nwhich = RA_STAT_FAILED_GRAB_PAGE;\r\nmsg = "cl_page_find failed";\r\n}\r\n} else {\r\nwhich = RA_STAT_WRONG_GRAB_PAGE;\r\nmsg = "g_c_p_n returned invalid page";\r\n}\r\nif (rc != 1)\r\nunlock_page(vmpage);\r\npage_cache_release(vmpage);\r\n} else {\r\nwhich = RA_STAT_FAILED_GRAB_PAGE;\r\nmsg = "g_c_p_n failed";\r\n}\r\nif (msg != NULL) {\r\nll_ra_stats_inc(mapping, which);\r\nCDEBUG(D_READA, "%s\n", msg);\r\n}\r\nreturn rc;\r\n}\r\nstatic inline int stride_io_mode(struct ll_readahead_state *ras)\r\n{\r\nreturn ras->ras_consecutive_stride_requests > 1;\r\n}\r\nstatic unsigned long\r\nstride_pg_count(pgoff_t st_off, unsigned long st_len, unsigned long st_pgs,\r\nunsigned long off, unsigned long length)\r\n{\r\n__u64 start = off > st_off ? off - st_off : 0;\r\n__u64 end = off + length > st_off ? off + length - st_off : 0;\r\nunsigned long start_left = 0;\r\nunsigned long end_left = 0;\r\nunsigned long pg_count;\r\nif (st_len == 0 || length == 0 || end == 0)\r\nreturn length;\r\nstart_left = do_div(start, st_len);\r\nif (start_left < st_pgs)\r\nstart_left = st_pgs - start_left;\r\nelse\r\nstart_left = 0;\r\nend_left = do_div(end, st_len);\r\nif (end_left > st_pgs)\r\nend_left = st_pgs;\r\nCDEBUG(D_READA, "start %llu, end %llu start_left %lu end_left %lu \n",\r\nstart, end, start_left, end_left);\r\nif (start == end)\r\npg_count = end_left - (st_pgs - start_left);\r\nelse\r\npg_count = start_left + st_pgs * (end - start - 1) + end_left;\r\nCDEBUG(D_READA, "st_off %lu, st_len %lu st_pgs %lu off %lu length %lu pgcount %lu\n",\r\nst_off, st_len, st_pgs, off, length, pg_count);\r\nreturn pg_count;\r\n}\r\nstatic int ria_page_count(struct ra_io_arg *ria)\r\n{\r\n__u64 length = ria->ria_end >= ria->ria_start ?\r\nria->ria_end - ria->ria_start + 1 : 0;\r\nreturn stride_pg_count(ria->ria_stoff, ria->ria_length,\r\nria->ria_pages, ria->ria_start,\r\nlength);\r\n}\r\nstatic int ras_inside_ra_window(unsigned long idx, struct ra_io_arg *ria)\r\n{\r\nreturn ria->ria_length == 0 || ria->ria_length == ria->ria_pages ||\r\n(idx >= ria->ria_stoff && (idx - ria->ria_stoff) %\r\nria->ria_length < ria->ria_pages);\r\n}\r\nstatic int ll_read_ahead_pages(const struct lu_env *env,\r\nstruct cl_io *io, struct cl_page_list *queue,\r\nstruct ra_io_arg *ria,\r\nunsigned long *reserved_pages,\r\nstruct address_space *mapping,\r\nunsigned long *ra_end)\r\n{\r\nint rc, count = 0, stride_ria;\r\nunsigned long page_idx;\r\nLASSERT(ria != NULL);\r\nRIA_DEBUG(ria);\r\nstride_ria = ria->ria_length > ria->ria_pages && ria->ria_pages > 0;\r\nfor (page_idx = ria->ria_start; page_idx <= ria->ria_end &&\r\n*reserved_pages > 0; page_idx++) {\r\nif (ras_inside_ra_window(page_idx, ria)) {\r\nrc = ll_read_ahead_page(env, io, queue,\r\npage_idx, mapping);\r\nif (rc == 1) {\r\n(*reserved_pages)--;\r\ncount ++;\r\n} else if (rc == -ENOLCK)\r\nbreak;\r\n} else if (stride_ria) {\r\npgoff_t offset;\r\nLASSERTF(page_idx > ria->ria_stoff, "Invalid page_idx %lu rs %lu re %lu ro %lu rl %lu rp %lu\n",\r\npage_idx,\r\nria->ria_start, ria->ria_end, ria->ria_stoff,\r\nria->ria_length, ria->ria_pages);\r\noffset = page_idx - ria->ria_stoff;\r\noffset = offset % (ria->ria_length);\r\nif (offset > ria->ria_pages) {\r\npage_idx += ria->ria_length - offset;\r\nCDEBUG(D_READA, "i %lu skip %lu \n", page_idx,\r\nria->ria_length - offset);\r\ncontinue;\r\n}\r\n}\r\n}\r\n*ra_end = page_idx;\r\nreturn count;\r\n}\r\nint ll_readahead(const struct lu_env *env, struct cl_io *io,\r\nstruct ll_readahead_state *ras, struct address_space *mapping,\r\nstruct cl_page_list *queue, int flags)\r\n{\r\nstruct vvp_io *vio = vvp_env_io(env);\r\nstruct vvp_thread_info *vti = vvp_env_info(env);\r\nstruct cl_attr *attr = ccc_env_thread_attr(env);\r\nunsigned long start = 0, end = 0, reserved;\r\nunsigned long ra_end, len;\r\nstruct inode *inode;\r\nstruct ll_ra_read *bead;\r\nstruct ra_io_arg *ria = &vti->vti_ria;\r\nstruct ll_inode_info *lli;\r\nstruct cl_object *clob;\r\nint ret = 0;\r\n__u64 kms;\r\ninode = mapping->host;\r\nlli = ll_i2info(inode);\r\nclob = lli->lli_clob;\r\nmemset(ria, 0, sizeof(*ria));\r\ncl_object_attr_lock(clob);\r\nret = cl_object_attr_get(env, clob, attr);\r\ncl_object_attr_unlock(clob);\r\nif (ret != 0)\r\nreturn ret;\r\nkms = attr->cat_kms;\r\nif (kms == 0) {\r\nll_ra_stats_inc(mapping, RA_STAT_ZERO_LEN);\r\nreturn 0;\r\n}\r\nspin_lock(&ras->ras_lock);\r\nif (vio->cui_ra_window_set)\r\nbead = &vio->cui_bead;\r\nelse\r\nbead = NULL;\r\nif (bead != NULL && ras->ras_window_start + ras->ras_window_len <\r\nbead->lrr_start + bead->lrr_count) {\r\nras->ras_window_len = bead->lrr_start + bead->lrr_count -\r\nras->ras_window_start;\r\n}\r\nif (ras->ras_window_len) {\r\nstart = ras->ras_next_readahead;\r\nend = ras->ras_window_start + ras->ras_window_len - 1;\r\n}\r\nif (end != 0) {\r\nunsigned long rpc_boundary;\r\nrpc_boundary = (end + 1) & (~(PTLRPC_MAX_BRW_PAGES - 1));\r\nif (rpc_boundary > 0)\r\nrpc_boundary--;\r\nif (rpc_boundary > start)\r\nend = rpc_boundary;\r\nend = min(end, (unsigned long)((kms - 1) >> PAGE_CACHE_SHIFT));\r\nras->ras_next_readahead = max(end, end + 1);\r\nRAS_CDEBUG(ras);\r\n}\r\nria->ria_start = start;\r\nria->ria_end = end;\r\nif (stride_io_mode(ras)) {\r\nria->ria_stoff = ras->ras_stride_offset;\r\nria->ria_length = ras->ras_stride_length;\r\nria->ria_pages = ras->ras_stride_pages;\r\n}\r\nspin_unlock(&ras->ras_lock);\r\nif (end == 0) {\r\nll_ra_stats_inc(mapping, RA_STAT_ZERO_WINDOW);\r\nreturn 0;\r\n}\r\nlen = ria_page_count(ria);\r\nif (len == 0)\r\nreturn 0;\r\nreserved = ll_ra_count_get(ll_i2sbi(inode), ria, len);\r\nif (reserved < len)\r\nll_ra_stats_inc(mapping, RA_STAT_MAX_IN_FLIGHT);\r\nCDEBUG(D_READA, "reserved page %lu ra_cur %d ra_max %lu\n", reserved,\r\natomic_read(&ll_i2sbi(inode)->ll_ra_info.ra_cur_pages),\r\nll_i2sbi(inode)->ll_ra_info.ra_max_pages);\r\nret = ll_read_ahead_pages(env, io, queue,\r\nria, &reserved, mapping, &ra_end);\r\nLASSERTF(reserved >= 0, "reserved %lu\n", reserved);\r\nif (reserved != 0)\r\nll_ra_count_put(ll_i2sbi(inode), reserved);\r\nif (ra_end == end + 1 && ra_end == (kms >> PAGE_CACHE_SHIFT))\r\nll_ra_stats_inc(mapping, RA_STAT_EOF);\r\nCDEBUG(D_READA, "ra_end %lu end %lu stride end %lu \n",\r\nra_end, end, ria->ria_end);\r\nif (ra_end != end + 1) {\r\nspin_lock(&ras->ras_lock);\r\nif (ra_end < ras->ras_next_readahead &&\r\nindex_in_window(ra_end, ras->ras_window_start, 0,\r\nras->ras_window_len)) {\r\nras->ras_next_readahead = ra_end;\r\nRAS_CDEBUG(ras);\r\n}\r\nspin_unlock(&ras->ras_lock);\r\n}\r\nreturn ret;\r\n}\r\nstatic void ras_set_start(struct inode *inode, struct ll_readahead_state *ras,\r\nunsigned long index)\r\n{\r\nras->ras_window_start = index & (~(RAS_INCREASE_STEP(inode) - 1));\r\n}\r\nstatic void ras_reset(struct inode *inode, struct ll_readahead_state *ras,\r\nunsigned long index)\r\n{\r\nras->ras_last_readpage = index;\r\nras->ras_consecutive_requests = 0;\r\nras->ras_consecutive_pages = 0;\r\nras->ras_window_len = 0;\r\nras_set_start(inode, ras, index);\r\nras->ras_next_readahead = max(ras->ras_window_start, index);\r\nRAS_CDEBUG(ras);\r\n}\r\nstatic void ras_stride_reset(struct ll_readahead_state *ras)\r\n{\r\nras->ras_consecutive_stride_requests = 0;\r\nras->ras_stride_length = 0;\r\nras->ras_stride_pages = 0;\r\nRAS_CDEBUG(ras);\r\n}\r\nvoid ll_readahead_init(struct inode *inode, struct ll_readahead_state *ras)\r\n{\r\nspin_lock_init(&ras->ras_lock);\r\nras_reset(inode, ras, 0);\r\nras->ras_requests = 0;\r\nINIT_LIST_HEAD(&ras->ras_read_beads);\r\n}\r\nstatic int index_in_stride_window(struct ll_readahead_state *ras,\r\nunsigned long index)\r\n{\r\nunsigned long stride_gap;\r\nif (ras->ras_stride_length == 0 || ras->ras_stride_pages == 0 ||\r\nras->ras_stride_pages == ras->ras_stride_length)\r\nreturn 0;\r\nstride_gap = index - ras->ras_last_readpage - 1;\r\nif (stride_gap == 0)\r\nreturn ras->ras_consecutive_pages + 1 <= ras->ras_stride_pages;\r\nreturn (ras->ras_stride_length - ras->ras_stride_pages) == stride_gap &&\r\nras->ras_consecutive_pages == ras->ras_stride_pages;\r\n}\r\nstatic void ras_update_stride_detector(struct ll_readahead_state *ras,\r\nunsigned long index)\r\n{\r\nunsigned long stride_gap = index - ras->ras_last_readpage - 1;\r\nif (!stride_io_mode(ras) && (stride_gap != 0 ||\r\nras->ras_consecutive_stride_requests == 0)) {\r\nras->ras_stride_pages = ras->ras_consecutive_pages;\r\nras->ras_stride_length = stride_gap +ras->ras_consecutive_pages;\r\n}\r\nLASSERT(ras->ras_request_index == 0);\r\nLASSERT(ras->ras_consecutive_stride_requests == 0);\r\nif (index <= ras->ras_last_readpage) {\r\nras_stride_reset(ras);\r\nreturn;\r\n}\r\nras->ras_stride_pages = ras->ras_consecutive_pages;\r\nras->ras_stride_length = stride_gap +ras->ras_consecutive_pages;\r\nRAS_CDEBUG(ras);\r\nreturn;\r\n}\r\nstatic unsigned long\r\nstride_page_count(struct ll_readahead_state *ras, unsigned long len)\r\n{\r\nreturn stride_pg_count(ras->ras_stride_offset, ras->ras_stride_length,\r\nras->ras_stride_pages, ras->ras_stride_offset,\r\nlen);\r\n}\r\nstatic void ras_stride_increase_window(struct ll_readahead_state *ras,\r\nstruct ll_ra_info *ra,\r\nunsigned long inc_len)\r\n{\r\nunsigned long left, step, window_len;\r\nunsigned long stride_len;\r\nLASSERT(ras->ras_stride_length > 0);\r\nLASSERTF(ras->ras_window_start + ras->ras_window_len\r\n>= ras->ras_stride_offset, "window_start %lu, window_len %lu stride_offset %lu\n",\r\nras->ras_window_start,\r\nras->ras_window_len, ras->ras_stride_offset);\r\nstride_len = ras->ras_window_start + ras->ras_window_len -\r\nras->ras_stride_offset;\r\nleft = stride_len % ras->ras_stride_length;\r\nwindow_len = ras->ras_window_len - left;\r\nif (left < ras->ras_stride_pages)\r\nleft += inc_len;\r\nelse\r\nleft = ras->ras_stride_pages + inc_len;\r\nLASSERT(ras->ras_stride_pages != 0);\r\nstep = left / ras->ras_stride_pages;\r\nleft %= ras->ras_stride_pages;\r\nwindow_len += step * ras->ras_stride_length + left;\r\nif (stride_page_count(ras, window_len) <= ra->ra_max_pages_per_file)\r\nras->ras_window_len = window_len;\r\nRAS_CDEBUG(ras);\r\n}\r\nstatic void ras_increase_window(struct inode *inode,\r\nstruct ll_readahead_state *ras,\r\nstruct ll_ra_info *ra)\r\n{\r\nif (stride_io_mode(ras))\r\nras_stride_increase_window(ras, ra, RAS_INCREASE_STEP(inode));\r\nelse\r\nras->ras_window_len = min(ras->ras_window_len +\r\nRAS_INCREASE_STEP(inode),\r\nra->ra_max_pages_per_file);\r\n}\r\nvoid ras_update(struct ll_sb_info *sbi, struct inode *inode,\r\nstruct ll_readahead_state *ras, unsigned long index,\r\nunsigned hit)\r\n{\r\nstruct ll_ra_info *ra = &sbi->ll_ra_info;\r\nint zero = 0, stride_detect = 0, ra_miss = 0;\r\nspin_lock(&ras->ras_lock);\r\nll_ra_stats_inc_sbi(sbi, hit ? RA_STAT_HIT : RA_STAT_MISS);\r\nif (!index_in_window(index, ras->ras_last_readpage, 8, 8)) {\r\nzero = 1;\r\nll_ra_stats_inc_sbi(sbi, RA_STAT_DISTANT_READPAGE);\r\n} else if (!hit && ras->ras_window_len &&\r\nindex < ras->ras_next_readahead &&\r\nindex_in_window(index, ras->ras_window_start, 0,\r\nras->ras_window_len)) {\r\nra_miss = 1;\r\nll_ra_stats_inc_sbi(sbi, RA_STAT_MISS_IN_WINDOW);\r\n}\r\nif (ras->ras_requests == 2 && !ras->ras_request_index) {\r\n__u64 kms_pages;\r\nkms_pages = (i_size_read(inode) + PAGE_CACHE_SIZE - 1) >>\r\nPAGE_CACHE_SHIFT;\r\nCDEBUG(D_READA, "kmsp %llu mwp %lu mp %lu\n", kms_pages,\r\nra->ra_max_read_ahead_whole_pages, ra->ra_max_pages_per_file);\r\nif (kms_pages &&\r\nkms_pages <= ra->ra_max_read_ahead_whole_pages) {\r\nras->ras_window_start = 0;\r\nras->ras_last_readpage = 0;\r\nras->ras_next_readahead = 0;\r\nras->ras_window_len = min(ra->ra_max_pages_per_file,\r\nra->ra_max_read_ahead_whole_pages);\r\ngoto out_unlock;\r\n}\r\n}\r\nif (zero) {\r\nif (!index_in_stride_window(ras, index)) {\r\nif (ras->ras_consecutive_stride_requests == 0 &&\r\nras->ras_request_index == 0) {\r\nras_update_stride_detector(ras, index);\r\nras->ras_consecutive_stride_requests++;\r\n} else {\r\nras_stride_reset(ras);\r\n}\r\nras_reset(inode, ras, index);\r\nras->ras_consecutive_pages++;\r\ngoto out_unlock;\r\n} else {\r\nras->ras_consecutive_pages = 0;\r\nras->ras_consecutive_requests = 0;\r\nif (++ras->ras_consecutive_stride_requests > 1)\r\nstride_detect = 1;\r\nRAS_CDEBUG(ras);\r\n}\r\n} else {\r\nif (ra_miss) {\r\nif (index_in_stride_window(ras, index) &&\r\nstride_io_mode(ras)) {\r\nif (index != ras->ras_last_readpage + 1)\r\nras->ras_consecutive_pages = 0;\r\nras_reset(inode, ras, index);\r\nRAS_CDEBUG(ras);\r\n} else {\r\nras_reset(inode, ras, index);\r\nras->ras_consecutive_pages++;\r\nras_stride_reset(ras);\r\ngoto out_unlock;\r\n}\r\n} else if (stride_io_mode(ras)) {\r\nif (!index_in_stride_window(ras, index)) {\r\nras_stride_reset(ras);\r\nras->ras_window_len = 0;\r\nras->ras_next_readahead = index;\r\n}\r\n}\r\n}\r\nras->ras_consecutive_pages++;\r\nras->ras_last_readpage = index;\r\nras_set_start(inode, ras, index);\r\nif (stride_io_mode(ras))\r\nras->ras_next_readahead = max(index, ras->ras_next_readahead);\r\nelse\r\nras->ras_next_readahead = max(ras->ras_window_start,\r\nras->ras_next_readahead);\r\nRAS_CDEBUG(ras);\r\nif (!ras->ras_window_len && ras->ras_consecutive_pages == 4) {\r\nras->ras_window_len = RAS_INCREASE_STEP(inode);\r\ngoto out_unlock;\r\n}\r\nif (ras->ras_consecutive_stride_requests == 2 && stride_detect) {\r\nras->ras_next_readahead = max(index, ras->ras_next_readahead);\r\nras->ras_stride_offset = index;\r\nras->ras_window_len = RAS_INCREASE_STEP(inode);\r\n}\r\nif ((ras->ras_consecutive_requests > 1 || stride_detect) &&\r\n!ras->ras_request_index)\r\nras_increase_window(inode, ras, ra);\r\nout_unlock:\r\nRAS_CDEBUG(ras);\r\nras->ras_request_index++;\r\nspin_unlock(&ras->ras_lock);\r\nreturn;\r\n}\r\nint ll_writepage(struct page *vmpage, struct writeback_control *wbc)\r\n{\r\nstruct inode *inode = vmpage->mapping->host;\r\nstruct ll_inode_info *lli = ll_i2info(inode);\r\nstruct lu_env *env;\r\nstruct cl_io *io;\r\nstruct cl_page *page;\r\nstruct cl_object *clob;\r\nstruct cl_env_nest nest;\r\nbool redirtied = false;\r\nbool unlocked = false;\r\nint result;\r\nLASSERT(PageLocked(vmpage));\r\nLASSERT(!PageWriteback(vmpage));\r\nLASSERT(ll_i2dtexp(inode) != NULL);\r\nenv = cl_env_nested_get(&nest);\r\nif (IS_ERR(env)) {\r\nresult = PTR_ERR(env);\r\ngoto out;\r\n}\r\nclob = ll_i2info(inode)->lli_clob;\r\nLASSERT(clob != NULL);\r\nio = ccc_env_thread_io(env);\r\nio->ci_obj = clob;\r\nio->ci_ignore_layout = 1;\r\nresult = cl_io_init(env, io, CIT_MISC, clob);\r\nif (result == 0) {\r\npage = cl_page_find(env, clob, vmpage->index,\r\nvmpage, CPT_CACHEABLE);\r\nif (!IS_ERR(page)) {\r\nlu_ref_add(&page->cp_reference, "writepage",\r\ncurrent);\r\ncl_page_assume(env, io, page);\r\nresult = cl_page_flush(env, io, page);\r\nif (result != 0) {\r\nif (!PageError(vmpage)) {\r\nredirty_page_for_writepage(wbc, vmpage);\r\nresult = 0;\r\nredirtied = true;\r\n}\r\n}\r\ncl_page_disown(env, io, page);\r\nunlocked = true;\r\nlu_ref_del(&page->cp_reference,\r\n"writepage", current);\r\ncl_page_put(env, page);\r\n} else {\r\nresult = PTR_ERR(page);\r\n}\r\n}\r\ncl_io_fini(env, io);\r\nif (redirtied && wbc->sync_mode == WB_SYNC_ALL) {\r\nloff_t offset = cl_offset(clob, vmpage->index);\r\nresult = cl_sync_file_range(inode, offset,\r\noffset + PAGE_CACHE_SIZE - 1,\r\nCL_FSYNC_LOCAL, 1);\r\nif (result > 0) {\r\nwbc->nr_to_write -= result - 1;\r\nresult = 0;\r\n}\r\n}\r\ncl_env_nested_put(&nest, env);\r\ngoto out;\r\nout:\r\nif (result < 0) {\r\nif (!lli->lli_async_rc)\r\nlli->lli_async_rc = result;\r\nSetPageError(vmpage);\r\nif (!unlocked)\r\nunlock_page(vmpage);\r\n}\r\nreturn result;\r\n}\r\nint ll_writepages(struct address_space *mapping, struct writeback_control *wbc)\r\n{\r\nstruct inode *inode = mapping->host;\r\nstruct ll_sb_info *sbi = ll_i2sbi(inode);\r\nloff_t start;\r\nloff_t end;\r\nenum cl_fsync_mode mode;\r\nint range_whole = 0;\r\nint result;\r\nint ignore_layout = 0;\r\nif (wbc->range_cyclic) {\r\nstart = mapping->writeback_index << PAGE_CACHE_SHIFT;\r\nend = OBD_OBJECT_EOF;\r\n} else {\r\nstart = wbc->range_start;\r\nend = wbc->range_end;\r\nif (end == LLONG_MAX) {\r\nend = OBD_OBJECT_EOF;\r\nrange_whole = start == 0;\r\n}\r\n}\r\nmode = CL_FSYNC_NONE;\r\nif (wbc->sync_mode == WB_SYNC_ALL)\r\nmode = CL_FSYNC_LOCAL;\r\nif (sbi->ll_umounting)\r\nignore_layout = 1;\r\nresult = cl_sync_file_range(inode, start, end, mode, ignore_layout);\r\nif (result > 0) {\r\nwbc->nr_to_write -= result;\r\nresult = 0;\r\n}\r\nif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0)) {\r\nif (end == OBD_OBJECT_EOF)\r\nend = i_size_read(inode);\r\nmapping->writeback_index = (end >> PAGE_CACHE_SHIFT) + 1;\r\n}\r\nreturn result;\r\n}\r\nint ll_readpage(struct file *file, struct page *vmpage)\r\n{\r\nstruct ll_cl_context *lcc;\r\nint result;\r\nlcc = ll_cl_init(file, vmpage, 0);\r\nif (!IS_ERR(lcc)) {\r\nstruct lu_env *env = lcc->lcc_env;\r\nstruct cl_io *io = lcc->lcc_io;\r\nstruct cl_page *page = lcc->lcc_page;\r\nLASSERT(page->cp_type == CPT_CACHEABLE);\r\nif (likely(!PageUptodate(vmpage))) {\r\ncl_page_assume(env, io, page);\r\nresult = cl_io_read_page(env, io, page);\r\n} else {\r\nunlock_page(vmpage);\r\nresult = 0;\r\n}\r\nll_cl_fini(lcc);\r\n} else {\r\nunlock_page(vmpage);\r\nresult = PTR_ERR(lcc);\r\n}\r\nreturn result;\r\n}
