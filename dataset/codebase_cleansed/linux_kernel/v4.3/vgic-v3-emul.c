static bool handle_mmio_rao_wi(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 reg = 0xffffffff;\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_ctlr(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 reg = 0;\r\nif (vcpu->kvm->arch.vgic.enabled)\r\nreg = GICD_CTLR_ENABLE_SS_G1;\r\nreg |= GICD_CTLR_ARE_NS | GICD_CTLR_DS;\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nif (mmio->is_write) {\r\nvcpu->kvm->arch.vgic.enabled = !!(reg & GICD_CTLR_ENABLE_SS_G1);\r\nvgic_update_state(vcpu->kvm);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_typer(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 reg;\r\nreg = (min(vcpu->kvm->arch.vgic.nr_irqs, 1024) >> 5) - 1;\r\nreg |= (INTERRUPT_ID_BITS - 1) << 19;\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_iidr(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio, phys_addr_t offset)\r\n{\r\nu32 reg;\r\nreg = (PRODUCT_ID_KVM << 24) | (IMPLEMENTER_ARM << 0);\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_set_enable_reg_dist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nif (likely(offset >= VGIC_NR_PRIVATE_IRQS / 8))\r\nreturn vgic_handle_enable_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id,\r\nACCESS_WRITE_SETBIT);\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_clear_enable_reg_dist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nif (likely(offset >= VGIC_NR_PRIVATE_IRQS / 8))\r\nreturn vgic_handle_enable_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id,\r\nACCESS_WRITE_CLEARBIT);\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_set_pending_reg_dist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nif (likely(offset >= VGIC_NR_PRIVATE_IRQS / 8))\r\nreturn vgic_handle_set_pending_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id);\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_clear_pending_reg_dist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nif (likely(offset >= VGIC_NR_PRIVATE_IRQS / 8))\r\nreturn vgic_handle_clear_pending_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id);\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_set_active_reg_dist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nif (likely(offset >= VGIC_NR_PRIVATE_IRQS / 8))\r\nreturn vgic_handle_set_active_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id);\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_clear_active_reg_dist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nif (likely(offset >= VGIC_NR_PRIVATE_IRQS / 8))\r\nreturn vgic_handle_clear_active_reg(vcpu->kvm, mmio, offset,\r\nvcpu->vcpu_id);\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_priority_reg_dist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 *reg;\r\nif (unlikely(offset < VGIC_NR_PRIVATE_IRQS)) {\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nreg = vgic_bytemap_get_reg(&vcpu->kvm->arch.vgic.irq_priority,\r\nvcpu->vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_cfg_reg_dist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 *reg;\r\nif (unlikely(offset < VGIC_NR_PRIVATE_IRQS / 4)) {\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nreg = vgic_bitmap_get_reg(&vcpu->kvm->arch.vgic.irq_cfg,\r\nvcpu->vcpu_id, offset >> 1);\r\nreturn vgic_handle_cfg_reg(reg, mmio, offset);\r\n}\r\nstatic u32 compress_mpidr(unsigned long mpidr)\r\n{\r\nu32 ret;\r\nret = MPIDR_AFFINITY_LEVEL(mpidr, 0);\r\nret |= MPIDR_AFFINITY_LEVEL(mpidr, 1) << 8;\r\nret |= MPIDR_AFFINITY_LEVEL(mpidr, 2) << 16;\r\nret |= MPIDR_AFFINITY_LEVEL(mpidr, 3) << 24;\r\nreturn ret;\r\n}\r\nstatic unsigned long uncompress_mpidr(u32 value)\r\n{\r\nunsigned long mpidr;\r\nmpidr = ((value >> 0) & 0xFF) << MPIDR_LEVEL_SHIFT(0);\r\nmpidr |= ((value >> 8) & 0xFF) << MPIDR_LEVEL_SHIFT(1);\r\nmpidr |= ((value >> 16) & 0xFF) << MPIDR_LEVEL_SHIFT(2);\r\nmpidr |= (u64)((value >> 24) & 0xFF) << MPIDR_LEVEL_SHIFT(3);\r\nreturn mpidr;\r\n}\r\nstatic bool handle_mmio_route_reg(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nint spi;\r\nu32 reg;\r\nint vcpu_id;\r\nunsigned long *bmap, mpidr;\r\nif ((offset & 4)) {\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nspi = offset / 8;\r\nmpidr = uncompress_mpidr(dist->irq_spi_mpidr[spi]);\r\nreg = mpidr;\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nif (!mmio->is_write)\r\nreturn false;\r\nvcpu = kvm_mpidr_to_vcpu(kvm, mpidr);\r\nif (likely(vcpu)) {\r\nvcpu_id = vcpu->vcpu_id;\r\nbmap = vgic_bitmap_get_shared_map(&dist->irq_spi_target[vcpu_id]);\r\n__clear_bit(spi, bmap);\r\n}\r\ndist->irq_spi_mpidr[spi] = compress_mpidr(reg);\r\nvcpu = kvm_mpidr_to_vcpu(kvm, reg & MPIDR_HWID_BITMASK);\r\nif (likely(vcpu)) {\r\nvcpu_id = vcpu->vcpu_id;\r\ndist->irq_spi_cpu[spi] = vcpu_id;\r\nbmap = vgic_bitmap_get_shared_map(&dist->irq_spi_target[vcpu_id]);\r\n__set_bit(spi, bmap);\r\n} else {\r\ndist->irq_spi_cpu[spi] = VCPU_NOT_ALLOCATED;\r\n}\r\nvgic_update_state(kvm);\r\nreturn true;\r\n}\r\nstatic bool handle_mmio_idregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 reg = 0;\r\nswitch (offset + GICD_IDREGS) {\r\ncase GICD_PIDR2:\r\nreg = 0x3b;\r\nbreak;\r\n}\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_ctlr_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nvgic_reg_access(mmio, NULL, offset,\r\nACCESS_READ_RAZ | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_typer_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nu32 reg;\r\nu64 mpidr;\r\nstruct kvm_vcpu *redist_vcpu = mmio->private;\r\nint target_vcpu_id = redist_vcpu->vcpu_id;\r\nif ((offset & ~3) == 4) {\r\nmpidr = kvm_vcpu_get_mpidr_aff(redist_vcpu);\r\nreg = compress_mpidr(mpidr);\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nreg = redist_vcpu->vcpu_id << 8;\r\nif (target_vcpu_id == atomic_read(&vcpu->kvm->online_vcpus) - 1)\r\nreg |= GICR_TYPER_LAST;\r\nvgic_reg_access(mmio, &reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_IGNORED);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_set_enable_reg_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct kvm_vcpu *redist_vcpu = mmio->private;\r\nreturn vgic_handle_enable_reg(vcpu->kvm, mmio, offset,\r\nredist_vcpu->vcpu_id,\r\nACCESS_WRITE_SETBIT);\r\n}\r\nstatic bool handle_mmio_clear_enable_reg_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct kvm_vcpu *redist_vcpu = mmio->private;\r\nreturn vgic_handle_enable_reg(vcpu->kvm, mmio, offset,\r\nredist_vcpu->vcpu_id,\r\nACCESS_WRITE_CLEARBIT);\r\n}\r\nstatic bool handle_mmio_set_active_reg_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct kvm_vcpu *redist_vcpu = mmio->private;\r\nreturn vgic_handle_set_active_reg(vcpu->kvm, mmio, offset,\r\nredist_vcpu->vcpu_id);\r\n}\r\nstatic bool handle_mmio_clear_active_reg_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct kvm_vcpu *redist_vcpu = mmio->private;\r\nreturn vgic_handle_clear_active_reg(vcpu->kvm, mmio, offset,\r\nredist_vcpu->vcpu_id);\r\n}\r\nstatic bool handle_mmio_set_pending_reg_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct kvm_vcpu *redist_vcpu = mmio->private;\r\nreturn vgic_handle_set_pending_reg(vcpu->kvm, mmio, offset,\r\nredist_vcpu->vcpu_id);\r\n}\r\nstatic bool handle_mmio_clear_pending_reg_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct kvm_vcpu *redist_vcpu = mmio->private;\r\nreturn vgic_handle_clear_pending_reg(vcpu->kvm, mmio, offset,\r\nredist_vcpu->vcpu_id);\r\n}\r\nstatic bool handle_mmio_priority_reg_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct kvm_vcpu *redist_vcpu = mmio->private;\r\nu32 *reg;\r\nreg = vgic_bytemap_get_reg(&vcpu->kvm->arch.vgic.irq_priority,\r\nredist_vcpu->vcpu_id, offset);\r\nvgic_reg_access(mmio, reg, offset,\r\nACCESS_READ_VALUE | ACCESS_WRITE_VALUE);\r\nreturn false;\r\n}\r\nstatic bool handle_mmio_cfg_reg_redist(struct kvm_vcpu *vcpu,\r\nstruct kvm_exit_mmio *mmio,\r\nphys_addr_t offset)\r\n{\r\nstruct kvm_vcpu *redist_vcpu = mmio->private;\r\nu32 *reg = vgic_bitmap_get_reg(&vcpu->kvm->arch.vgic.irq_cfg,\r\nredist_vcpu->vcpu_id, offset >> 1);\r\nreturn vgic_handle_cfg_reg(reg, mmio, offset);\r\n}\r\nstatic bool vgic_v3_queue_sgi(struct kvm_vcpu *vcpu, int irq)\r\n{\r\nif (vgic_queue_irq(vcpu, 0, irq)) {\r\nvgic_dist_irq_clear_pending(vcpu, irq);\r\nvgic_cpu_irq_clear(vcpu, irq);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int vgic_v3_map_resources(struct kvm *kvm,\r\nconst struct vgic_params *params)\r\n{\r\nint ret = 0;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\ngpa_t rdbase = dist->vgic_redist_base;\r\nstruct vgic_io_device *iodevs = NULL;\r\nint i;\r\nif (!irqchip_in_kernel(kvm))\r\nreturn 0;\r\nmutex_lock(&kvm->lock);\r\nif (vgic_ready(kvm))\r\ngoto out;\r\nif (IS_VGIC_ADDR_UNDEF(dist->vgic_dist_base) ||\r\nIS_VGIC_ADDR_UNDEF(dist->vgic_redist_base)) {\r\nkvm_err("Need to set vgic distributor addresses first\n");\r\nret = -ENXIO;\r\ngoto out;\r\n}\r\nif (!vgic_initialized(kvm)) {\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\nret = vgic_register_kvm_io_dev(kvm, dist->vgic_dist_base,\r\nGIC_V3_DIST_SIZE, vgic_v3_dist_ranges,\r\n-1, &dist->dist_iodev);\r\nif (ret)\r\ngoto out;\r\niodevs = kcalloc(dist->nr_cpus, sizeof(iodevs[0]), GFP_KERNEL);\r\nif (!iodevs) {\r\nret = -ENOMEM;\r\ngoto out_unregister;\r\n}\r\nfor (i = 0; i < dist->nr_cpus; i++) {\r\nret = vgic_register_kvm_io_dev(kvm, rdbase,\r\nSZ_128K, vgic_redist_ranges,\r\ni, &iodevs[i]);\r\nif (ret)\r\ngoto out_unregister;\r\nrdbase += GIC_V3_REDIST_SIZE;\r\n}\r\ndist->redist_iodevs = iodevs;\r\ndist->ready = true;\r\ngoto out;\r\nout_unregister:\r\nkvm_io_bus_unregister_dev(kvm, KVM_MMIO_BUS, &dist->dist_iodev.dev);\r\nif (iodevs) {\r\nfor (i = 0; i < dist->nr_cpus; i++) {\r\nif (iodevs[i].dev.ops)\r\nkvm_io_bus_unregister_dev(kvm, KVM_MMIO_BUS,\r\n&iodevs[i].dev);\r\n}\r\n}\r\nout:\r\nif (ret)\r\nkvm_vgic_destroy(kvm);\r\nmutex_unlock(&kvm->lock);\r\nreturn ret;\r\n}\r\nstatic int vgic_v3_init_model(struct kvm *kvm)\r\n{\r\nint i;\r\nu32 mpidr;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nint nr_spis = dist->nr_irqs - VGIC_NR_PRIVATE_IRQS;\r\ndist->irq_spi_mpidr = kcalloc(nr_spis, sizeof(dist->irq_spi_mpidr[0]),\r\nGFP_KERNEL);\r\nif (!dist->irq_spi_mpidr)\r\nreturn -ENOMEM;\r\nmpidr = compress_mpidr(kvm_vcpu_get_mpidr_aff(kvm_get_vcpu(kvm, 0)));\r\nfor (i = VGIC_NR_PRIVATE_IRQS; i < dist->nr_irqs; i++) {\r\ndist->irq_spi_cpu[i - VGIC_NR_PRIVATE_IRQS] = 0;\r\ndist->irq_spi_mpidr[i - VGIC_NR_PRIVATE_IRQS] = mpidr;\r\nvgic_bitmap_set_irq_val(dist->irq_spi_target, 0, i, 1);\r\n}\r\nreturn 0;\r\n}\r\nstatic void vgic_v3_add_sgi_source(struct kvm_vcpu *vcpu, int irq, int source)\r\n{\r\n}\r\nvoid vgic_v3_init_emulation(struct kvm *kvm)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\ndist->vm_ops.queue_sgi = vgic_v3_queue_sgi;\r\ndist->vm_ops.add_sgi_source = vgic_v3_add_sgi_source;\r\ndist->vm_ops.init_model = vgic_v3_init_model;\r\ndist->vm_ops.map_resources = vgic_v3_map_resources;\r\nkvm->arch.max_vcpus = KVM_MAX_VCPUS;\r\n}\r\nstatic int match_mpidr(u64 sgi_aff, u16 sgi_cpu_mask, struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long affinity;\r\nint level0;\r\naffinity = kvm_vcpu_get_mpidr_aff(vcpu);\r\nlevel0 = MPIDR_AFFINITY_LEVEL(affinity, 0);\r\naffinity &= ~MPIDR_LEVEL_MASK;\r\nif (sgi_aff != affinity)\r\nreturn -1;\r\nif (!(sgi_cpu_mask & BIT(level0)))\r\nreturn -1;\r\nreturn level0;\r\n}\r\nvoid vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct kvm_vcpu *c_vcpu;\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nu16 target_cpus;\r\nu64 mpidr;\r\nint sgi, c;\r\nint vcpu_id = vcpu->vcpu_id;\r\nbool broadcast;\r\nint updated = 0;\r\nsgi = (reg & ICC_SGI1R_SGI_ID_MASK) >> ICC_SGI1R_SGI_ID_SHIFT;\r\nbroadcast = reg & BIT(ICC_SGI1R_IRQ_ROUTING_MODE_BIT);\r\ntarget_cpus = (reg & ICC_SGI1R_TARGET_LIST_MASK) >> ICC_SGI1R_TARGET_LIST_SHIFT;\r\nmpidr = SGI_AFFINITY_LEVEL(reg, 3);\r\nmpidr |= SGI_AFFINITY_LEVEL(reg, 2);\r\nmpidr |= SGI_AFFINITY_LEVEL(reg, 1);\r\nspin_lock(&dist->lock);\r\nkvm_for_each_vcpu(c, c_vcpu, kvm) {\r\nif (!broadcast && target_cpus == 0)\r\nbreak;\r\nif (broadcast && c == vcpu_id)\r\ncontinue;\r\nif (!broadcast) {\r\nint level0;\r\nlevel0 = match_mpidr(mpidr, target_cpus, c_vcpu);\r\nif (level0 == -1)\r\ncontinue;\r\ntarget_cpus &= ~BIT(level0);\r\n}\r\nvgic_dist_irq_set_pending(c_vcpu, sgi);\r\nupdated = 1;\r\nkvm_debug("SGI%d from CPU%d to CPU%d\n", sgi, vcpu_id, c);\r\n}\r\nif (updated)\r\nvgic_update_state(vcpu->kvm);\r\nspin_unlock(&dist->lock);\r\nif (updated)\r\nvgic_kick_vcpus(vcpu->kvm);\r\n}\r\nstatic int vgic_v3_create(struct kvm_device *dev, u32 type)\r\n{\r\nreturn kvm_vgic_create(dev->kvm, type);\r\n}\r\nstatic void vgic_v3_destroy(struct kvm_device *dev)\r\n{\r\nkfree(dev);\r\n}\r\nstatic int vgic_v3_set_attr(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr)\r\n{\r\nint ret;\r\nret = vgic_set_common_attr(dev, attr);\r\nif (ret != -ENXIO)\r\nreturn ret;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_DIST_REGS:\r\ncase KVM_DEV_ARM_VGIC_GRP_CPU_REGS:\r\nreturn -ENXIO;\r\n}\r\nreturn -ENXIO;\r\n}\r\nstatic int vgic_v3_get_attr(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr)\r\n{\r\nint ret;\r\nret = vgic_get_common_attr(dev, attr);\r\nif (ret != -ENXIO)\r\nreturn ret;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_DIST_REGS:\r\ncase KVM_DEV_ARM_VGIC_GRP_CPU_REGS:\r\nreturn -ENXIO;\r\n}\r\nreturn -ENXIO;\r\n}\r\nstatic int vgic_v3_has_attr(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr)\r\n{\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_ADDR:\r\nswitch (attr->attr) {\r\ncase KVM_VGIC_V2_ADDR_TYPE_DIST:\r\ncase KVM_VGIC_V2_ADDR_TYPE_CPU:\r\nreturn -ENXIO;\r\ncase KVM_VGIC_V3_ADDR_TYPE_DIST:\r\ncase KVM_VGIC_V3_ADDR_TYPE_REDIST:\r\nreturn 0;\r\n}\r\nbreak;\r\ncase KVM_DEV_ARM_VGIC_GRP_DIST_REGS:\r\ncase KVM_DEV_ARM_VGIC_GRP_CPU_REGS:\r\nreturn -ENXIO;\r\ncase KVM_DEV_ARM_VGIC_GRP_NR_IRQS:\r\nreturn 0;\r\ncase KVM_DEV_ARM_VGIC_GRP_CTRL:\r\nswitch (attr->attr) {\r\ncase KVM_DEV_ARM_VGIC_CTRL_INIT:\r\nreturn 0;\r\n}\r\n}\r\nreturn -ENXIO;\r\n}
