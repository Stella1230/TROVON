static u32 sysmmu_page_offset(sysmmu_iova_t iova, u32 size)\r\n{\r\nreturn iova & (size - 1);\r\n}\r\nstatic u32 lv1ent_offset(sysmmu_iova_t iova)\r\n{\r\nreturn iova >> SECT_ORDER;\r\n}\r\nstatic u32 lv2ent_offset(sysmmu_iova_t iova)\r\n{\r\nreturn (iova >> SPAGE_ORDER) & (NUM_LV2ENTRIES - 1);\r\n}\r\nstatic sysmmu_pte_t *section_entry(sysmmu_pte_t *pgtable, sysmmu_iova_t iova)\r\n{\r\nreturn pgtable + lv1ent_offset(iova);\r\n}\r\nstatic sysmmu_pte_t *page_entry(sysmmu_pte_t *sent, sysmmu_iova_t iova)\r\n{\r\nreturn (sysmmu_pte_t *)phys_to_virt(\r\nlv2table_base(sent)) + lv2ent_offset(iova);\r\n}\r\nstatic struct exynos_iommu_domain *to_exynos_domain(struct iommu_domain *dom)\r\n{\r\nreturn container_of(dom, struct exynos_iommu_domain, domain);\r\n}\r\nstatic bool set_sysmmu_active(struct sysmmu_drvdata *data)\r\n{\r\nreturn ++data->activations == 1;\r\n}\r\nstatic bool set_sysmmu_inactive(struct sysmmu_drvdata *data)\r\n{\r\nBUG_ON(data->activations < 1);\r\nreturn --data->activations == 0;\r\n}\r\nstatic bool is_sysmmu_active(struct sysmmu_drvdata *data)\r\n{\r\nreturn data->activations > 0;\r\n}\r\nstatic void sysmmu_unblock(void __iomem *sfrbase)\r\n{\r\n__raw_writel(CTRL_ENABLE, sfrbase + REG_MMU_CTRL);\r\n}\r\nstatic bool sysmmu_block(void __iomem *sfrbase)\r\n{\r\nint i = 120;\r\n__raw_writel(CTRL_BLOCK, sfrbase + REG_MMU_CTRL);\r\nwhile ((i > 0) && !(__raw_readl(sfrbase + REG_MMU_STATUS) & 1))\r\n--i;\r\nif (!(__raw_readl(sfrbase + REG_MMU_STATUS) & 1)) {\r\nsysmmu_unblock(sfrbase);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic void __sysmmu_tlb_invalidate(void __iomem *sfrbase)\r\n{\r\n__raw_writel(0x1, sfrbase + REG_MMU_FLUSH);\r\n}\r\nstatic void __sysmmu_tlb_invalidate_entry(void __iomem *sfrbase,\r\nsysmmu_iova_t iova, unsigned int num_inv)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < num_inv; i++) {\r\n__raw_writel((iova & SPAGE_MASK) | 1,\r\nsfrbase + REG_MMU_FLUSH_ENTRY);\r\niova += SPAGE_SIZE;\r\n}\r\n}\r\nstatic void __sysmmu_set_ptbase(void __iomem *sfrbase,\r\nphys_addr_t pgd)\r\n{\r\n__raw_writel(pgd, sfrbase + REG_PT_BASE_ADDR);\r\n__sysmmu_tlb_invalidate(sfrbase);\r\n}\r\nstatic void show_fault_information(const char *name,\r\nenum exynos_sysmmu_inttype itype,\r\nphys_addr_t pgtable_base, sysmmu_iova_t fault_addr)\r\n{\r\nsysmmu_pte_t *ent;\r\nif ((itype >= SYSMMU_FAULTS_NUM) || (itype < SYSMMU_PAGEFAULT))\r\nitype = SYSMMU_FAULT_UNKNOWN;\r\npr_err("%s occurred at %#x by %s(Page table base: %pa)\n",\r\nsysmmu_fault_name[itype], fault_addr, name, &pgtable_base);\r\nent = section_entry(phys_to_virt(pgtable_base), fault_addr);\r\npr_err("\tLv1 entry: %#x\n", *ent);\r\nif (lv1ent_page(ent)) {\r\nent = page_entry(ent, fault_addr);\r\npr_err("\t Lv2 entry: %#x\n", *ent);\r\n}\r\n}\r\nstatic irqreturn_t exynos_sysmmu_irq(int irq, void *dev_id)\r\n{\r\nstruct sysmmu_drvdata *data = dev_id;\r\nenum exynos_sysmmu_inttype itype;\r\nsysmmu_iova_t addr = -1;\r\nint ret = -ENOSYS;\r\nWARN_ON(!is_sysmmu_active(data));\r\nspin_lock(&data->lock);\r\nif (!IS_ERR(data->clk_master))\r\nclk_enable(data->clk_master);\r\nitype = (enum exynos_sysmmu_inttype)\r\n__ffs(__raw_readl(data->sfrbase + REG_INT_STATUS));\r\nif (WARN_ON(!((itype >= 0) && (itype < SYSMMU_FAULT_UNKNOWN))))\r\nitype = SYSMMU_FAULT_UNKNOWN;\r\nelse\r\naddr = __raw_readl(data->sfrbase + fault_reg_offset[itype]);\r\nif (itype == SYSMMU_FAULT_UNKNOWN) {\r\npr_err("%s: Fault is not occurred by System MMU '%s'!\n",\r\n__func__, dev_name(data->sysmmu));\r\npr_err("%s: Please check if IRQ is correctly configured.\n",\r\n__func__);\r\nBUG();\r\n} else {\r\nunsigned int base =\r\n__raw_readl(data->sfrbase + REG_PT_BASE_ADDR);\r\nshow_fault_information(dev_name(data->sysmmu),\r\nitype, base, addr);\r\nif (data->domain)\r\nret = report_iommu_fault(&data->domain->domain,\r\ndata->master, addr, itype);\r\n}\r\nBUG_ON(ret != 0);\r\n__raw_writel(1 << itype, data->sfrbase + REG_INT_CLEAR);\r\nsysmmu_unblock(data->sfrbase);\r\nif (!IS_ERR(data->clk_master))\r\nclk_disable(data->clk_master);\r\nspin_unlock(&data->lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void __sysmmu_disable_nocount(struct sysmmu_drvdata *data)\r\n{\r\nif (!IS_ERR(data->clk_master))\r\nclk_enable(data->clk_master);\r\n__raw_writel(CTRL_DISABLE, data->sfrbase + REG_MMU_CTRL);\r\n__raw_writel(0, data->sfrbase + REG_MMU_CFG);\r\nclk_disable(data->clk);\r\nif (!IS_ERR(data->clk_master))\r\nclk_disable(data->clk_master);\r\n}\r\nstatic bool __sysmmu_disable(struct sysmmu_drvdata *data)\r\n{\r\nbool disabled;\r\nunsigned long flags;\r\nspin_lock_irqsave(&data->lock, flags);\r\ndisabled = set_sysmmu_inactive(data);\r\nif (disabled) {\r\ndata->pgtable = 0;\r\ndata->domain = NULL;\r\n__sysmmu_disable_nocount(data);\r\ndev_dbg(data->sysmmu, "Disabled\n");\r\n} else {\r\ndev_dbg(data->sysmmu, "%d times left to disable\n",\r\ndata->activations);\r\n}\r\nspin_unlock_irqrestore(&data->lock, flags);\r\nreturn disabled;\r\n}\r\nstatic void __sysmmu_init_config(struct sysmmu_drvdata *data)\r\n{\r\nunsigned int cfg = CFG_LRU | CFG_QOS(15);\r\nunsigned int ver;\r\nver = MMU_RAW_VER(__raw_readl(data->sfrbase + REG_MMU_VERSION));\r\nif (MMU_MAJ_VER(ver) == 3) {\r\nif (MMU_MIN_VER(ver) >= 2) {\r\ncfg |= CFG_FLPDCACHE;\r\nif (MMU_MIN_VER(ver) == 3) {\r\ncfg |= CFG_ACGEN;\r\ncfg &= ~CFG_LRU;\r\n} else {\r\ncfg |= CFG_SYSSEL;\r\n}\r\n}\r\n}\r\n__raw_writel(cfg, data->sfrbase + REG_MMU_CFG);\r\ndata->version = ver;\r\n}\r\nstatic void __sysmmu_enable_nocount(struct sysmmu_drvdata *data)\r\n{\r\nif (!IS_ERR(data->clk_master))\r\nclk_enable(data->clk_master);\r\nclk_enable(data->clk);\r\n__raw_writel(CTRL_BLOCK, data->sfrbase + REG_MMU_CTRL);\r\n__sysmmu_init_config(data);\r\n__sysmmu_set_ptbase(data->sfrbase, data->pgtable);\r\n__raw_writel(CTRL_ENABLE, data->sfrbase + REG_MMU_CTRL);\r\nif (!IS_ERR(data->clk_master))\r\nclk_disable(data->clk_master);\r\n}\r\nstatic int __sysmmu_enable(struct sysmmu_drvdata *data, phys_addr_t pgtable,\r\nstruct exynos_iommu_domain *domain)\r\n{\r\nint ret = 0;\r\nunsigned long flags;\r\nspin_lock_irqsave(&data->lock, flags);\r\nif (set_sysmmu_active(data)) {\r\ndata->pgtable = pgtable;\r\ndata->domain = domain;\r\n__sysmmu_enable_nocount(data);\r\ndev_dbg(data->sysmmu, "Enabled\n");\r\n} else {\r\nret = (pgtable == data->pgtable) ? 1 : -EBUSY;\r\ndev_dbg(data->sysmmu, "already enabled\n");\r\n}\r\nif (WARN_ON(ret < 0))\r\nset_sysmmu_inactive(data);\r\nspin_unlock_irqrestore(&data->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void __sysmmu_tlb_invalidate_flpdcache(struct sysmmu_drvdata *data,\r\nsysmmu_iova_t iova)\r\n{\r\nif (data->version == MAKE_MMU_VER(3, 3))\r\n__raw_writel(iova | 0x1, data->sfrbase + REG_MMU_FLUSH_ENTRY);\r\n}\r\nstatic void sysmmu_tlb_invalidate_flpdcache(struct sysmmu_drvdata *data,\r\nsysmmu_iova_t iova)\r\n{\r\nunsigned long flags;\r\nif (!IS_ERR(data->clk_master))\r\nclk_enable(data->clk_master);\r\nspin_lock_irqsave(&data->lock, flags);\r\nif (is_sysmmu_active(data))\r\n__sysmmu_tlb_invalidate_flpdcache(data, iova);\r\nspin_unlock_irqrestore(&data->lock, flags);\r\nif (!IS_ERR(data->clk_master))\r\nclk_disable(data->clk_master);\r\n}\r\nstatic void sysmmu_tlb_invalidate_entry(struct sysmmu_drvdata *data,\r\nsysmmu_iova_t iova, size_t size)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&data->lock, flags);\r\nif (is_sysmmu_active(data)) {\r\nunsigned int num_inv = 1;\r\nif (!IS_ERR(data->clk_master))\r\nclk_enable(data->clk_master);\r\nif (MMU_MAJ_VER(data->version) == 2)\r\nnum_inv = min_t(unsigned int, size / PAGE_SIZE, 64);\r\nif (sysmmu_block(data->sfrbase)) {\r\n__sysmmu_tlb_invalidate_entry(\r\ndata->sfrbase, iova, num_inv);\r\nsysmmu_unblock(data->sfrbase);\r\n}\r\nif (!IS_ERR(data->clk_master))\r\nclk_disable(data->clk_master);\r\n} else {\r\ndev_dbg(data->master,\r\n"disabled. Skipping TLB invalidation @ %#x\n", iova);\r\n}\r\nspin_unlock_irqrestore(&data->lock, flags);\r\n}\r\nstatic int __init exynos_sysmmu_probe(struct platform_device *pdev)\r\n{\r\nint irq, ret;\r\nstruct device *dev = &pdev->dev;\r\nstruct sysmmu_drvdata *data;\r\nstruct resource *res;\r\ndata = devm_kzalloc(dev, sizeof(*data), GFP_KERNEL);\r\nif (!data)\r\nreturn -ENOMEM;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ndata->sfrbase = devm_ioremap_resource(dev, res);\r\nif (IS_ERR(data->sfrbase))\r\nreturn PTR_ERR(data->sfrbase);\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq <= 0) {\r\ndev_err(dev, "Unable to find IRQ resource\n");\r\nreturn irq;\r\n}\r\nret = devm_request_irq(dev, irq, exynos_sysmmu_irq, 0,\r\ndev_name(dev), data);\r\nif (ret) {\r\ndev_err(dev, "Unabled to register handler of irq %d\n", irq);\r\nreturn ret;\r\n}\r\ndata->clk = devm_clk_get(dev, "sysmmu");\r\nif (IS_ERR(data->clk)) {\r\ndev_err(dev, "Failed to get clock!\n");\r\nreturn PTR_ERR(data->clk);\r\n} else {\r\nret = clk_prepare(data->clk);\r\nif (ret) {\r\ndev_err(dev, "Failed to prepare clk\n");\r\nreturn ret;\r\n}\r\n}\r\ndata->clk_master = devm_clk_get(dev, "master");\r\nif (!IS_ERR(data->clk_master)) {\r\nret = clk_prepare(data->clk_master);\r\nif (ret) {\r\nclk_unprepare(data->clk);\r\ndev_err(dev, "Failed to prepare master's clk\n");\r\nreturn ret;\r\n}\r\n}\r\ndata->sysmmu = dev;\r\nspin_lock_init(&data->lock);\r\nplatform_set_drvdata(pdev, data);\r\npm_runtime_enable(dev);\r\nreturn 0;\r\n}\r\nstatic int exynos_sysmmu_suspend(struct device *dev)\r\n{\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev);\r\ndev_dbg(dev, "suspend\n");\r\nif (is_sysmmu_active(data)) {\r\n__sysmmu_disable_nocount(data);\r\npm_runtime_put(dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic int exynos_sysmmu_resume(struct device *dev)\r\n{\r\nstruct sysmmu_drvdata *data = dev_get_drvdata(dev);\r\ndev_dbg(dev, "resume\n");\r\nif (is_sysmmu_active(data)) {\r\npm_runtime_get_sync(dev);\r\n__sysmmu_enable_nocount(data);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void pgtable_flush(void *vastart, void *vaend)\r\n{\r\ndmac_flush_range(vastart, vaend);\r\nouter_flush_range(virt_to_phys(vastart),\r\nvirt_to_phys(vaend));\r\n}\r\nstatic struct iommu_domain *exynos_iommu_domain_alloc(unsigned type)\r\n{\r\nstruct exynos_iommu_domain *domain;\r\nint i;\r\nif (type != IOMMU_DOMAIN_UNMANAGED)\r\nreturn NULL;\r\ndomain = kzalloc(sizeof(*domain), GFP_KERNEL);\r\nif (!domain)\r\nreturn NULL;\r\ndomain->pgtable = (sysmmu_pte_t *)__get_free_pages(GFP_KERNEL, 2);\r\nif (!domain->pgtable)\r\ngoto err_pgtable;\r\ndomain->lv2entcnt = (short *)__get_free_pages(GFP_KERNEL | __GFP_ZERO, 1);\r\nif (!domain->lv2entcnt)\r\ngoto err_counter;\r\nfor (i = 0; i < NUM_LV1ENTRIES; i += 8) {\r\ndomain->pgtable[i + 0] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 1] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 2] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 3] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 4] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 5] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 6] = ZERO_LV2LINK;\r\ndomain->pgtable[i + 7] = ZERO_LV2LINK;\r\n}\r\npgtable_flush(domain->pgtable, domain->pgtable + NUM_LV1ENTRIES);\r\nspin_lock_init(&domain->lock);\r\nspin_lock_init(&domain->pgtablelock);\r\nINIT_LIST_HEAD(&domain->clients);\r\ndomain->domain.geometry.aperture_start = 0;\r\ndomain->domain.geometry.aperture_end = ~0UL;\r\ndomain->domain.geometry.force_aperture = true;\r\nreturn &domain->domain;\r\nerr_counter:\r\nfree_pages((unsigned long)domain->pgtable, 2);\r\nerr_pgtable:\r\nkfree(domain);\r\nreturn NULL;\r\n}\r\nstatic void exynos_iommu_domain_free(struct iommu_domain *iommu_domain)\r\n{\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nstruct sysmmu_drvdata *data, *next;\r\nunsigned long flags;\r\nint i;\r\nWARN_ON(!list_empty(&domain->clients));\r\nspin_lock_irqsave(&domain->lock, flags);\r\nlist_for_each_entry_safe(data, next, &domain->clients, domain_node) {\r\nif (__sysmmu_disable(data))\r\ndata->master = NULL;\r\nlist_del_init(&data->domain_node);\r\n}\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\nfor (i = 0; i < NUM_LV1ENTRIES; i++)\r\nif (lv1ent_page(domain->pgtable + i))\r\nkmem_cache_free(lv2table_kmem_cache,\r\nphys_to_virt(lv2table_base(domain->pgtable + i)));\r\nfree_pages((unsigned long)domain->pgtable, 2);\r\nfree_pages((unsigned long)domain->lv2entcnt, 1);\r\nkfree(domain);\r\n}\r\nstatic int exynos_iommu_attach_device(struct iommu_domain *iommu_domain,\r\nstruct device *dev)\r\n{\r\nstruct exynos_iommu_owner *owner = dev->archdata.iommu;\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nstruct sysmmu_drvdata *data;\r\nphys_addr_t pagetable = virt_to_phys(domain->pgtable);\r\nunsigned long flags;\r\nint ret = -ENODEV;\r\nif (!has_sysmmu(dev))\r\nreturn -ENODEV;\r\nlist_for_each_entry(data, &owner->controllers, owner_node) {\r\npm_runtime_get_sync(data->sysmmu);\r\nret = __sysmmu_enable(data, pagetable, domain);\r\nif (ret >= 0) {\r\ndata->master = dev;\r\nspin_lock_irqsave(&domain->lock, flags);\r\nlist_add_tail(&data->domain_node, &domain->clients);\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\n}\r\n}\r\nif (ret < 0) {\r\ndev_err(dev, "%s: Failed to attach IOMMU with pgtable %pa\n",\r\n__func__, &pagetable);\r\nreturn ret;\r\n}\r\ndev_dbg(dev, "%s: Attached IOMMU with pgtable %pa %s\n",\r\n__func__, &pagetable, (ret == 0) ? "" : ", again");\r\nreturn ret;\r\n}\r\nstatic void exynos_iommu_detach_device(struct iommu_domain *iommu_domain,\r\nstruct device *dev)\r\n{\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nphys_addr_t pagetable = virt_to_phys(domain->pgtable);\r\nstruct sysmmu_drvdata *data, *next;\r\nunsigned long flags;\r\nbool found = false;\r\nif (!has_sysmmu(dev))\r\nreturn;\r\nspin_lock_irqsave(&domain->lock, flags);\r\nlist_for_each_entry_safe(data, next, &domain->clients, domain_node) {\r\nif (data->master == dev) {\r\nif (__sysmmu_disable(data)) {\r\ndata->master = NULL;\r\nlist_del_init(&data->domain_node);\r\n}\r\npm_runtime_put(data->sysmmu);\r\nfound = true;\r\n}\r\n}\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\nif (found)\r\ndev_dbg(dev, "%s: Detached IOMMU with pgtable %pa\n",\r\n__func__, &pagetable);\r\nelse\r\ndev_err(dev, "%s: No IOMMU is attached\n", __func__);\r\n}\r\nstatic sysmmu_pte_t *alloc_lv2entry(struct exynos_iommu_domain *domain,\r\nsysmmu_pte_t *sent, sysmmu_iova_t iova, short *pgcounter)\r\n{\r\nif (lv1ent_section(sent)) {\r\nWARN(1, "Trying mapping on %#08x mapped with 1MiB page", iova);\r\nreturn ERR_PTR(-EADDRINUSE);\r\n}\r\nif (lv1ent_fault(sent)) {\r\nsysmmu_pte_t *pent;\r\nbool need_flush_flpd_cache = lv1ent_zero(sent);\r\npent = kmem_cache_zalloc(lv2table_kmem_cache, GFP_ATOMIC);\r\nBUG_ON((unsigned int)pent & (LV2TABLE_SIZE - 1));\r\nif (!pent)\r\nreturn ERR_PTR(-ENOMEM);\r\n*sent = mk_lv1ent_page(virt_to_phys(pent));\r\nkmemleak_ignore(pent);\r\n*pgcounter = NUM_LV2ENTRIES;\r\npgtable_flush(pent, pent + NUM_LV2ENTRIES);\r\npgtable_flush(sent, sent + 1);\r\nif (need_flush_flpd_cache) {\r\nstruct sysmmu_drvdata *data;\r\nspin_lock(&domain->lock);\r\nlist_for_each_entry(data, &domain->clients, domain_node)\r\nsysmmu_tlb_invalidate_flpdcache(data, iova);\r\nspin_unlock(&domain->lock);\r\n}\r\n}\r\nreturn page_entry(sent, iova);\r\n}\r\nstatic int lv1set_section(struct exynos_iommu_domain *domain,\r\nsysmmu_pte_t *sent, sysmmu_iova_t iova,\r\nphys_addr_t paddr, short *pgcnt)\r\n{\r\nif (lv1ent_section(sent)) {\r\nWARN(1, "Trying mapping on 1MiB@%#08x that is mapped",\r\niova);\r\nreturn -EADDRINUSE;\r\n}\r\nif (lv1ent_page(sent)) {\r\nif (*pgcnt != NUM_LV2ENTRIES) {\r\nWARN(1, "Trying mapping on 1MiB@%#08x that is mapped",\r\niova);\r\nreturn -EADDRINUSE;\r\n}\r\nkmem_cache_free(lv2table_kmem_cache, page_entry(sent, 0));\r\n*pgcnt = 0;\r\n}\r\n*sent = mk_lv1ent_sect(paddr);\r\npgtable_flush(sent, sent + 1);\r\nspin_lock(&domain->lock);\r\nif (lv1ent_page_zero(sent)) {\r\nstruct sysmmu_drvdata *data;\r\nlist_for_each_entry(data, &domain->clients, domain_node)\r\nsysmmu_tlb_invalidate_flpdcache(data, iova);\r\n}\r\nspin_unlock(&domain->lock);\r\nreturn 0;\r\n}\r\nstatic int lv2set_page(sysmmu_pte_t *pent, phys_addr_t paddr, size_t size,\r\nshort *pgcnt)\r\n{\r\nif (size == SPAGE_SIZE) {\r\nif (WARN_ON(!lv2ent_fault(pent)))\r\nreturn -EADDRINUSE;\r\n*pent = mk_lv2ent_spage(paddr);\r\npgtable_flush(pent, pent + 1);\r\n*pgcnt -= 1;\r\n} else {\r\nint i;\r\nfor (i = 0; i < SPAGES_PER_LPAGE; i++, pent++) {\r\nif (WARN_ON(!lv2ent_fault(pent))) {\r\nif (i > 0)\r\nmemset(pent - i, 0, sizeof(*pent) * i);\r\nreturn -EADDRINUSE;\r\n}\r\n*pent = mk_lv2ent_lpage(paddr);\r\n}\r\npgtable_flush(pent - SPAGES_PER_LPAGE, pent);\r\n*pgcnt -= SPAGES_PER_LPAGE;\r\n}\r\nreturn 0;\r\n}\r\nstatic int exynos_iommu_map(struct iommu_domain *iommu_domain,\r\nunsigned long l_iova, phys_addr_t paddr, size_t size,\r\nint prot)\r\n{\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nsysmmu_pte_t *entry;\r\nsysmmu_iova_t iova = (sysmmu_iova_t)l_iova;\r\nunsigned long flags;\r\nint ret = -ENOMEM;\r\nBUG_ON(domain->pgtable == NULL);\r\nspin_lock_irqsave(&domain->pgtablelock, flags);\r\nentry = section_entry(domain->pgtable, iova);\r\nif (size == SECT_SIZE) {\r\nret = lv1set_section(domain, entry, iova, paddr,\r\n&domain->lv2entcnt[lv1ent_offset(iova)]);\r\n} else {\r\nsysmmu_pte_t *pent;\r\npent = alloc_lv2entry(domain, entry, iova,\r\n&domain->lv2entcnt[lv1ent_offset(iova)]);\r\nif (IS_ERR(pent))\r\nret = PTR_ERR(pent);\r\nelse\r\nret = lv2set_page(pent, paddr, size,\r\n&domain->lv2entcnt[lv1ent_offset(iova)]);\r\n}\r\nif (ret)\r\npr_err("%s: Failed(%d) to map %#zx bytes @ %#x\n",\r\n__func__, ret, size, iova);\r\nspin_unlock_irqrestore(&domain->pgtablelock, flags);\r\nreturn ret;\r\n}\r\nstatic void exynos_iommu_tlb_invalidate_entry(struct exynos_iommu_domain *domain,\r\nsysmmu_iova_t iova, size_t size)\r\n{\r\nstruct sysmmu_drvdata *data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&domain->lock, flags);\r\nlist_for_each_entry(data, &domain->clients, domain_node)\r\nsysmmu_tlb_invalidate_entry(data, iova, size);\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\n}\r\nstatic size_t exynos_iommu_unmap(struct iommu_domain *iommu_domain,\r\nunsigned long l_iova, size_t size)\r\n{\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nsysmmu_iova_t iova = (sysmmu_iova_t)l_iova;\r\nsysmmu_pte_t *ent;\r\nsize_t err_pgsize;\r\nunsigned long flags;\r\nBUG_ON(domain->pgtable == NULL);\r\nspin_lock_irqsave(&domain->pgtablelock, flags);\r\nent = section_entry(domain->pgtable, iova);\r\nif (lv1ent_section(ent)) {\r\nif (WARN_ON(size < SECT_SIZE)) {\r\nerr_pgsize = SECT_SIZE;\r\ngoto err;\r\n}\r\n*ent = ZERO_LV2LINK;\r\npgtable_flush(ent, ent + 1);\r\nsize = SECT_SIZE;\r\ngoto done;\r\n}\r\nif (unlikely(lv1ent_fault(ent))) {\r\nif (size > SECT_SIZE)\r\nsize = SECT_SIZE;\r\ngoto done;\r\n}\r\nent = page_entry(ent, iova);\r\nif (unlikely(lv2ent_fault(ent))) {\r\nsize = SPAGE_SIZE;\r\ngoto done;\r\n}\r\nif (lv2ent_small(ent)) {\r\n*ent = 0;\r\nsize = SPAGE_SIZE;\r\npgtable_flush(ent, ent + 1);\r\ndomain->lv2entcnt[lv1ent_offset(iova)] += 1;\r\ngoto done;\r\n}\r\nif (WARN_ON(size < LPAGE_SIZE)) {\r\nerr_pgsize = LPAGE_SIZE;\r\ngoto err;\r\n}\r\nmemset(ent, 0, sizeof(*ent) * SPAGES_PER_LPAGE);\r\npgtable_flush(ent, ent + SPAGES_PER_LPAGE);\r\nsize = LPAGE_SIZE;\r\ndomain->lv2entcnt[lv1ent_offset(iova)] += SPAGES_PER_LPAGE;\r\ndone:\r\nspin_unlock_irqrestore(&domain->pgtablelock, flags);\r\nexynos_iommu_tlb_invalidate_entry(domain, iova, size);\r\nreturn size;\r\nerr:\r\nspin_unlock_irqrestore(&domain->pgtablelock, flags);\r\npr_err("%s: Failed: size(%#zx) @ %#x is smaller than page size %#zx\n",\r\n__func__, size, iova, err_pgsize);\r\nreturn 0;\r\n}\r\nstatic phys_addr_t exynos_iommu_iova_to_phys(struct iommu_domain *iommu_domain,\r\ndma_addr_t iova)\r\n{\r\nstruct exynos_iommu_domain *domain = to_exynos_domain(iommu_domain);\r\nsysmmu_pte_t *entry;\r\nunsigned long flags;\r\nphys_addr_t phys = 0;\r\nspin_lock_irqsave(&domain->pgtablelock, flags);\r\nentry = section_entry(domain->pgtable, iova);\r\nif (lv1ent_section(entry)) {\r\nphys = section_phys(entry) + section_offs(iova);\r\n} else if (lv1ent_page(entry)) {\r\nentry = page_entry(entry, iova);\r\nif (lv2ent_large(entry))\r\nphys = lpage_phys(entry) + lpage_offs(iova);\r\nelse if (lv2ent_small(entry))\r\nphys = spage_phys(entry) + spage_offs(iova);\r\n}\r\nspin_unlock_irqrestore(&domain->pgtablelock, flags);\r\nreturn phys;\r\n}\r\nstatic int exynos_iommu_add_device(struct device *dev)\r\n{\r\nstruct iommu_group *group;\r\nint ret;\r\nif (!has_sysmmu(dev))\r\nreturn -ENODEV;\r\ngroup = iommu_group_get(dev);\r\nif (!group) {\r\ngroup = iommu_group_alloc();\r\nif (IS_ERR(group)) {\r\ndev_err(dev, "Failed to allocate IOMMU group\n");\r\nreturn PTR_ERR(group);\r\n}\r\n}\r\nret = iommu_group_add_device(group, dev);\r\niommu_group_put(group);\r\nreturn ret;\r\n}\r\nstatic void exynos_iommu_remove_device(struct device *dev)\r\n{\r\nif (!has_sysmmu(dev))\r\nreturn;\r\niommu_group_remove_device(dev);\r\n}\r\nstatic int exynos_iommu_of_xlate(struct device *dev,\r\nstruct of_phandle_args *spec)\r\n{\r\nstruct exynos_iommu_owner *owner = dev->archdata.iommu;\r\nstruct platform_device *sysmmu = of_find_device_by_node(spec->np);\r\nstruct sysmmu_drvdata *data;\r\nif (!sysmmu)\r\nreturn -ENODEV;\r\ndata = platform_get_drvdata(sysmmu);\r\nif (!data)\r\nreturn -ENODEV;\r\nif (!owner) {\r\nowner = kzalloc(sizeof(*owner), GFP_KERNEL);\r\nif (!owner)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&owner->controllers);\r\ndev->archdata.iommu = owner;\r\n}\r\nlist_add_tail(&data->owner_node, &owner->controllers);\r\nreturn 0;\r\n}\r\nstatic int __init exynos_iommu_init(void)\r\n{\r\nint ret;\r\nlv2table_kmem_cache = kmem_cache_create("exynos-iommu-lv2table",\r\nLV2TABLE_SIZE, LV2TABLE_SIZE, 0, NULL);\r\nif (!lv2table_kmem_cache) {\r\npr_err("%s: Failed to create kmem cache\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nret = platform_driver_register(&exynos_sysmmu_driver);\r\nif (ret) {\r\npr_err("%s: Failed to register driver\n", __func__);\r\ngoto err_reg_driver;\r\n}\r\nzero_lv2_table = kmem_cache_zalloc(lv2table_kmem_cache, GFP_KERNEL);\r\nif (zero_lv2_table == NULL) {\r\npr_err("%s: Failed to allocate zero level2 page table\n",\r\n__func__);\r\nret = -ENOMEM;\r\ngoto err_zero_lv2;\r\n}\r\nret = bus_set_iommu(&platform_bus_type, &exynos_iommu_ops);\r\nif (ret) {\r\npr_err("%s: Failed to register exynos-iommu driver.\n",\r\n__func__);\r\ngoto err_set_iommu;\r\n}\r\ninit_done = true;\r\nreturn 0;\r\nerr_set_iommu:\r\nkmem_cache_free(lv2table_kmem_cache, zero_lv2_table);\r\nerr_zero_lv2:\r\nplatform_driver_unregister(&exynos_sysmmu_driver);\r\nerr_reg_driver:\r\nkmem_cache_destroy(lv2table_kmem_cache);\r\nreturn ret;\r\n}\r\nstatic int __init exynos_iommu_of_setup(struct device_node *np)\r\n{\r\nstruct platform_device *pdev;\r\nif (!init_done)\r\nexynos_iommu_init();\r\npdev = of_platform_device_create(np, NULL, platform_bus_type.dev_root);\r\nif (IS_ERR(pdev))\r\nreturn PTR_ERR(pdev);\r\nof_iommu_set_ops(np, &exynos_iommu_ops);\r\nreturn 0;\r\n}
