static void wakeup_softirqd(void)\r\n{\r\nstruct task_struct *tsk = __this_cpu_read(ksoftirqd);\r\nif (tsk && tsk->state != TASK_RUNNING)\r\nwake_up_process(tsk);\r\n}\r\nvoid __local_bh_disable_ip(unsigned long ip, unsigned int cnt)\r\n{\r\nunsigned long flags;\r\nWARN_ON_ONCE(in_irq());\r\nraw_local_irq_save(flags);\r\n__preempt_count_add(cnt);\r\nif (softirq_count() == (cnt & SOFTIRQ_MASK))\r\ntrace_softirqs_off(ip);\r\nraw_local_irq_restore(flags);\r\nif (preempt_count() == cnt) {\r\n#ifdef CONFIG_DEBUG_PREEMPT\r\ncurrent->preempt_disable_ip = get_parent_ip(CALLER_ADDR1);\r\n#endif\r\ntrace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));\r\n}\r\n}\r\nstatic void __local_bh_enable(unsigned int cnt)\r\n{\r\nWARN_ON_ONCE(!irqs_disabled());\r\nif (softirq_count() == (cnt & SOFTIRQ_MASK))\r\ntrace_softirqs_on(_RET_IP_);\r\npreempt_count_sub(cnt);\r\n}\r\nvoid _local_bh_enable(void)\r\n{\r\nWARN_ON_ONCE(in_irq());\r\n__local_bh_enable(SOFTIRQ_DISABLE_OFFSET);\r\n}\r\nvoid __local_bh_enable_ip(unsigned long ip, unsigned int cnt)\r\n{\r\nWARN_ON_ONCE(in_irq() || irqs_disabled());\r\n#ifdef CONFIG_TRACE_IRQFLAGS\r\nlocal_irq_disable();\r\n#endif\r\nif (softirq_count() == SOFTIRQ_DISABLE_OFFSET)\r\ntrace_softirqs_on(ip);\r\npreempt_count_sub(cnt - 1);\r\nif (unlikely(!in_interrupt() && local_softirq_pending())) {\r\ndo_softirq();\r\n}\r\npreempt_count_dec();\r\n#ifdef CONFIG_TRACE_IRQFLAGS\r\nlocal_irq_enable();\r\n#endif\r\npreempt_check_resched();\r\n}\r\nstatic inline bool lockdep_softirq_start(void)\r\n{\r\nbool in_hardirq = false;\r\nif (trace_hardirq_context(current)) {\r\nin_hardirq = true;\r\ntrace_hardirq_exit();\r\n}\r\nlockdep_softirq_enter();\r\nreturn in_hardirq;\r\n}\r\nstatic inline void lockdep_softirq_end(bool in_hardirq)\r\n{\r\nlockdep_softirq_exit();\r\nif (in_hardirq)\r\ntrace_hardirq_enter();\r\n}\r\nstatic inline bool lockdep_softirq_start(void) { return false; }\r\nstatic inline void lockdep_softirq_end(bool in_hardirq) { }\r\nasmlinkage __visible void __do_softirq(void)\r\n{\r\nunsigned long end = jiffies + MAX_SOFTIRQ_TIME;\r\nunsigned long old_flags = current->flags;\r\nint max_restart = MAX_SOFTIRQ_RESTART;\r\nstruct softirq_action *h;\r\nbool in_hardirq;\r\n__u32 pending;\r\nint softirq_bit;\r\ncurrent->flags &= ~PF_MEMALLOC;\r\npending = local_softirq_pending();\r\naccount_irq_enter_time(current);\r\n__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);\r\nin_hardirq = lockdep_softirq_start();\r\nrestart:\r\nset_softirq_pending(0);\r\nlocal_irq_enable();\r\nh = softirq_vec;\r\nwhile ((softirq_bit = ffs(pending))) {\r\nunsigned int vec_nr;\r\nint prev_count;\r\nh += softirq_bit - 1;\r\nvec_nr = h - softirq_vec;\r\nprev_count = preempt_count();\r\nkstat_incr_softirqs_this_cpu(vec_nr);\r\ntrace_softirq_entry(vec_nr);\r\nh->action(h);\r\ntrace_softirq_exit(vec_nr);\r\nif (unlikely(prev_count != preempt_count())) {\r\npr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",\r\nvec_nr, softirq_to_name[vec_nr], h->action,\r\nprev_count, preempt_count());\r\npreempt_count_set(prev_count);\r\n}\r\nh++;\r\npending >>= softirq_bit;\r\n}\r\nrcu_bh_qs();\r\nlocal_irq_disable();\r\npending = local_softirq_pending();\r\nif (pending) {\r\nif (time_before(jiffies, end) && !need_resched() &&\r\n--max_restart)\r\ngoto restart;\r\nwakeup_softirqd();\r\n}\r\nlockdep_softirq_end(in_hardirq);\r\naccount_irq_exit_time(current);\r\n__local_bh_enable(SOFTIRQ_OFFSET);\r\nWARN_ON_ONCE(in_interrupt());\r\ntsk_restore_flags(current, old_flags, PF_MEMALLOC);\r\n}\r\nasmlinkage __visible void do_softirq(void)\r\n{\r\n__u32 pending;\r\nunsigned long flags;\r\nif (in_interrupt())\r\nreturn;\r\nlocal_irq_save(flags);\r\npending = local_softirq_pending();\r\nif (pending)\r\ndo_softirq_own_stack();\r\nlocal_irq_restore(flags);\r\n}\r\nvoid irq_enter(void)\r\n{\r\nrcu_irq_enter();\r\nif (is_idle_task(current) && !in_interrupt()) {\r\nlocal_bh_disable();\r\ntick_irq_enter();\r\n_local_bh_enable();\r\n}\r\n__irq_enter();\r\n}\r\nstatic inline void invoke_softirq(void)\r\n{\r\nif (!force_irqthreads) {\r\n#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK\r\n__do_softirq();\r\n#else\r\ndo_softirq_own_stack();\r\n#endif\r\n} else {\r\nwakeup_softirqd();\r\n}\r\n}\r\nstatic inline void tick_irq_exit(void)\r\n{\r\n#ifdef CONFIG_NO_HZ_COMMON\r\nint cpu = smp_processor_id();\r\nif ((idle_cpu(cpu) && !need_resched()) || tick_nohz_full_cpu(cpu)) {\r\nif (!in_interrupt())\r\ntick_nohz_irq_exit();\r\n}\r\n#endif\r\n}\r\nvoid irq_exit(void)\r\n{\r\n#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED\r\nlocal_irq_disable();\r\n#else\r\nWARN_ON_ONCE(!irqs_disabled());\r\n#endif\r\naccount_irq_exit_time(current);\r\npreempt_count_sub(HARDIRQ_OFFSET);\r\nif (!in_interrupt() && local_softirq_pending())\r\ninvoke_softirq();\r\ntick_irq_exit();\r\nrcu_irq_exit();\r\ntrace_hardirq_exit();\r\n}\r\ninline void raise_softirq_irqoff(unsigned int nr)\r\n{\r\n__raise_softirq_irqoff(nr);\r\nif (!in_interrupt())\r\nwakeup_softirqd();\r\n}\r\nvoid raise_softirq(unsigned int nr)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nraise_softirq_irqoff(nr);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid __raise_softirq_irqoff(unsigned int nr)\r\n{\r\ntrace_softirq_raise(nr);\r\nor_softirq_pending(1UL << nr);\r\n}\r\nvoid open_softirq(int nr, void (*action)(struct softirq_action *))\r\n{\r\nsoftirq_vec[nr].action = action;\r\n}\r\nvoid __tasklet_schedule(struct tasklet_struct *t)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nt->next = NULL;\r\n*__this_cpu_read(tasklet_vec.tail) = t;\r\n__this_cpu_write(tasklet_vec.tail, &(t->next));\r\nraise_softirq_irqoff(TASKLET_SOFTIRQ);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid __tasklet_hi_schedule(struct tasklet_struct *t)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nt->next = NULL;\r\n*__this_cpu_read(tasklet_hi_vec.tail) = t;\r\n__this_cpu_write(tasklet_hi_vec.tail, &(t->next));\r\nraise_softirq_irqoff(HI_SOFTIRQ);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid __tasklet_hi_schedule_first(struct tasklet_struct *t)\r\n{\r\nBUG_ON(!irqs_disabled());\r\nt->next = __this_cpu_read(tasklet_hi_vec.head);\r\n__this_cpu_write(tasklet_hi_vec.head, t);\r\n__raise_softirq_irqoff(HI_SOFTIRQ);\r\n}\r\nstatic void tasklet_action(struct softirq_action *a)\r\n{\r\nstruct tasklet_struct *list;\r\nlocal_irq_disable();\r\nlist = __this_cpu_read(tasklet_vec.head);\r\n__this_cpu_write(tasklet_vec.head, NULL);\r\n__this_cpu_write(tasklet_vec.tail, this_cpu_ptr(&tasklet_vec.head));\r\nlocal_irq_enable();\r\nwhile (list) {\r\nstruct tasklet_struct *t = list;\r\nlist = list->next;\r\nif (tasklet_trylock(t)) {\r\nif (!atomic_read(&t->count)) {\r\nif (!test_and_clear_bit(TASKLET_STATE_SCHED,\r\n&t->state))\r\nBUG();\r\nt->func(t->data);\r\ntasklet_unlock(t);\r\ncontinue;\r\n}\r\ntasklet_unlock(t);\r\n}\r\nlocal_irq_disable();\r\nt->next = NULL;\r\n*__this_cpu_read(tasklet_vec.tail) = t;\r\n__this_cpu_write(tasklet_vec.tail, &(t->next));\r\n__raise_softirq_irqoff(TASKLET_SOFTIRQ);\r\nlocal_irq_enable();\r\n}\r\n}\r\nstatic void tasklet_hi_action(struct softirq_action *a)\r\n{\r\nstruct tasklet_struct *list;\r\nlocal_irq_disable();\r\nlist = __this_cpu_read(tasklet_hi_vec.head);\r\n__this_cpu_write(tasklet_hi_vec.head, NULL);\r\n__this_cpu_write(tasklet_hi_vec.tail, this_cpu_ptr(&tasklet_hi_vec.head));\r\nlocal_irq_enable();\r\nwhile (list) {\r\nstruct tasklet_struct *t = list;\r\nlist = list->next;\r\nif (tasklet_trylock(t)) {\r\nif (!atomic_read(&t->count)) {\r\nif (!test_and_clear_bit(TASKLET_STATE_SCHED,\r\n&t->state))\r\nBUG();\r\nt->func(t->data);\r\ntasklet_unlock(t);\r\ncontinue;\r\n}\r\ntasklet_unlock(t);\r\n}\r\nlocal_irq_disable();\r\nt->next = NULL;\r\n*__this_cpu_read(tasklet_hi_vec.tail) = t;\r\n__this_cpu_write(tasklet_hi_vec.tail, &(t->next));\r\n__raise_softirq_irqoff(HI_SOFTIRQ);\r\nlocal_irq_enable();\r\n}\r\n}\r\nvoid tasklet_init(struct tasklet_struct *t,\r\nvoid (*func)(unsigned long), unsigned long data)\r\n{\r\nt->next = NULL;\r\nt->state = 0;\r\natomic_set(&t->count, 0);\r\nt->func = func;\r\nt->data = data;\r\n}\r\nvoid tasklet_kill(struct tasklet_struct *t)\r\n{\r\nif (in_interrupt())\r\npr_notice("Attempt to kill tasklet from interrupt\n");\r\nwhile (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {\r\ndo {\r\nyield();\r\n} while (test_bit(TASKLET_STATE_SCHED, &t->state));\r\n}\r\ntasklet_unlock_wait(t);\r\nclear_bit(TASKLET_STATE_SCHED, &t->state);\r\n}\r\nstatic enum hrtimer_restart __hrtimer_tasklet_trampoline(struct hrtimer *timer)\r\n{\r\nstruct tasklet_hrtimer *ttimer =\r\ncontainer_of(timer, struct tasklet_hrtimer, timer);\r\ntasklet_hi_schedule(&ttimer->tasklet);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nstatic void __tasklet_hrtimer_trampoline(unsigned long data)\r\n{\r\nstruct tasklet_hrtimer *ttimer = (void *)data;\r\nenum hrtimer_restart restart;\r\nrestart = ttimer->function(&ttimer->timer);\r\nif (restart != HRTIMER_NORESTART)\r\nhrtimer_restart(&ttimer->timer);\r\n}\r\nvoid __init softirq_init(void)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\nper_cpu(tasklet_vec, cpu).tail =\r\n&per_cpu(tasklet_vec, cpu).head;\r\nper_cpu(tasklet_hi_vec, cpu).tail =\r\n&per_cpu(tasklet_hi_vec, cpu).head;\r\n}\r\nopen_softirq(TASKLET_SOFTIRQ, tasklet_action);\r\nopen_softirq(HI_SOFTIRQ, tasklet_hi_action);\r\n}\r\nstatic int ksoftirqd_should_run(unsigned int cpu)\r\n{\r\nreturn local_softirq_pending();\r\n}\r\nstatic void run_ksoftirqd(unsigned int cpu)\r\n{\r\nlocal_irq_disable();\r\nif (local_softirq_pending()) {\r\n__do_softirq();\r\nlocal_irq_enable();\r\ncond_resched_rcu_qs();\r\nreturn;\r\n}\r\nlocal_irq_enable();\r\n}\r\nvoid tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu)\r\n{\r\nstruct tasklet_struct **i;\r\nBUG_ON(cpu_online(cpu));\r\nBUG_ON(test_bit(TASKLET_STATE_RUN, &t->state));\r\nif (!test_bit(TASKLET_STATE_SCHED, &t->state))\r\nreturn;\r\nfor (i = &per_cpu(tasklet_vec, cpu).head; *i; i = &(*i)->next) {\r\nif (*i == t) {\r\n*i = t->next;\r\nif (*i == NULL)\r\nper_cpu(tasklet_vec, cpu).tail = i;\r\nreturn;\r\n}\r\n}\r\nBUG();\r\n}\r\nstatic void takeover_tasklets(unsigned int cpu)\r\n{\r\nlocal_irq_disable();\r\nif (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {\r\n*__this_cpu_read(tasklet_vec.tail) = per_cpu(tasklet_vec, cpu).head;\r\nthis_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);\r\nper_cpu(tasklet_vec, cpu).head = NULL;\r\nper_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;\r\n}\r\nraise_softirq_irqoff(TASKLET_SOFTIRQ);\r\nif (&per_cpu(tasklet_hi_vec, cpu).head != per_cpu(tasklet_hi_vec, cpu).tail) {\r\n*__this_cpu_read(tasklet_hi_vec.tail) = per_cpu(tasklet_hi_vec, cpu).head;\r\n__this_cpu_write(tasklet_hi_vec.tail, per_cpu(tasklet_hi_vec, cpu).tail);\r\nper_cpu(tasklet_hi_vec, cpu).head = NULL;\r\nper_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;\r\n}\r\nraise_softirq_irqoff(HI_SOFTIRQ);\r\nlocal_irq_enable();\r\n}\r\nstatic int cpu_callback(struct notifier_block *nfb, unsigned long action,\r\nvoid *hcpu)\r\n{\r\nswitch (action) {\r\n#ifdef CONFIG_HOTPLUG_CPU\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\ntakeover_tasklets((unsigned long)hcpu);\r\nbreak;\r\n#endif\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic __init int spawn_ksoftirqd(void)\r\n{\r\nregister_cpu_notifier(&cpu_nfb);\r\nBUG_ON(smpboot_register_percpu_thread(&softirq_threads));\r\nreturn 0;\r\n}\r\nint __init __weak early_irq_init(void)\r\n{\r\nreturn 0;\r\n}\r\nint __init __weak arch_probe_nr_irqs(void)\r\n{\r\nreturn NR_IRQS_LEGACY;\r\n}\r\nint __init __weak arch_early_irq_init(void)\r\n{\r\nreturn 0;\r\n}\r\nunsigned int __weak arch_dynirq_lower_bound(unsigned int from)\r\n{\r\nreturn from;\r\n}
