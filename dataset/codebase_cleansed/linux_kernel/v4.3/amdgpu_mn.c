static void amdgpu_mn_destroy(struct work_struct *work)\r\n{\r\nstruct amdgpu_mn *rmn = container_of(work, struct amdgpu_mn, work);\r\nstruct amdgpu_device *adev = rmn->adev;\r\nstruct amdgpu_mn_node *node, *next_node;\r\nstruct amdgpu_bo *bo, *next_bo;\r\nmutex_lock(&adev->mn_lock);\r\nmutex_lock(&rmn->lock);\r\nhash_del(&rmn->node);\r\nrbtree_postorder_for_each_entry_safe(node, next_node, &rmn->objects,\r\nit.rb) {\r\ninterval_tree_remove(&node->it, &rmn->objects);\r\nlist_for_each_entry_safe(bo, next_bo, &node->bos, mn_list) {\r\nbo->mn = NULL;\r\nlist_del_init(&bo->mn_list);\r\n}\r\nkfree(node);\r\n}\r\nmutex_unlock(&rmn->lock);\r\nmutex_unlock(&adev->mn_lock);\r\nmmu_notifier_unregister(&rmn->mn, rmn->mm);\r\nkfree(rmn);\r\n}\r\nstatic void amdgpu_mn_release(struct mmu_notifier *mn,\r\nstruct mm_struct *mm)\r\n{\r\nstruct amdgpu_mn *rmn = container_of(mn, struct amdgpu_mn, mn);\r\nINIT_WORK(&rmn->work, amdgpu_mn_destroy);\r\nschedule_work(&rmn->work);\r\n}\r\nstatic void amdgpu_mn_invalidate_range_start(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start,\r\nunsigned long end)\r\n{\r\nstruct amdgpu_mn *rmn = container_of(mn, struct amdgpu_mn, mn);\r\nstruct interval_tree_node *it;\r\nend -= 1;\r\nmutex_lock(&rmn->lock);\r\nit = interval_tree_iter_first(&rmn->objects, start, end);\r\nwhile (it) {\r\nstruct amdgpu_mn_node *node;\r\nstruct amdgpu_bo *bo;\r\nlong r;\r\nnode = container_of(it, struct amdgpu_mn_node, it);\r\nit = interval_tree_iter_next(it, start, end);\r\nlist_for_each_entry(bo, &node->bos, mn_list) {\r\nif (!bo->tbo.ttm || bo->tbo.ttm->state != tt_bound)\r\ncontinue;\r\nr = amdgpu_bo_reserve(bo, true);\r\nif (r) {\r\nDRM_ERROR("(%ld) failed to reserve user bo\n", r);\r\ncontinue;\r\n}\r\nr = reservation_object_wait_timeout_rcu(bo->tbo.resv,\r\ntrue, false, MAX_SCHEDULE_TIMEOUT);\r\nif (r <= 0)\r\nDRM_ERROR("(%ld) failed to wait for user bo\n", r);\r\namdgpu_ttm_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (r)\r\nDRM_ERROR("(%ld) failed to validate user bo\n", r);\r\namdgpu_bo_unreserve(bo);\r\n}\r\n}\r\nmutex_unlock(&rmn->lock);\r\n}\r\nstatic struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct amdgpu_mn *rmn;\r\nint r;\r\ndown_write(&mm->mmap_sem);\r\nmutex_lock(&adev->mn_lock);\r\nhash_for_each_possible(adev->mn_hash, rmn, node, (unsigned long)mm)\r\nif (rmn->mm == mm)\r\ngoto release_locks;\r\nrmn = kzalloc(sizeof(*rmn), GFP_KERNEL);\r\nif (!rmn) {\r\nrmn = ERR_PTR(-ENOMEM);\r\ngoto release_locks;\r\n}\r\nrmn->adev = adev;\r\nrmn->mm = mm;\r\nrmn->mn.ops = &amdgpu_mn_ops;\r\nmutex_init(&rmn->lock);\r\nrmn->objects = RB_ROOT;\r\nr = __mmu_notifier_register(&rmn->mn, mm);\r\nif (r)\r\ngoto free_rmn;\r\nhash_add(adev->mn_hash, &rmn->node, (unsigned long)mm);\r\nrelease_locks:\r\nmutex_unlock(&adev->mn_lock);\r\nup_write(&mm->mmap_sem);\r\nreturn rmn;\r\nfree_rmn:\r\nmutex_unlock(&adev->mn_lock);\r\nup_write(&mm->mmap_sem);\r\nkfree(rmn);\r\nreturn ERR_PTR(r);\r\n}\r\nint amdgpu_mn_register(struct amdgpu_bo *bo, unsigned long addr)\r\n{\r\nunsigned long end = addr + amdgpu_bo_size(bo) - 1;\r\nstruct amdgpu_device *adev = bo->adev;\r\nstruct amdgpu_mn *rmn;\r\nstruct amdgpu_mn_node *node = NULL;\r\nstruct list_head bos;\r\nstruct interval_tree_node *it;\r\nrmn = amdgpu_mn_get(adev);\r\nif (IS_ERR(rmn))\r\nreturn PTR_ERR(rmn);\r\nINIT_LIST_HEAD(&bos);\r\nmutex_lock(&rmn->lock);\r\nwhile ((it = interval_tree_iter_first(&rmn->objects, addr, end))) {\r\nkfree(node);\r\nnode = container_of(it, struct amdgpu_mn_node, it);\r\ninterval_tree_remove(&node->it, &rmn->objects);\r\naddr = min(it->start, addr);\r\nend = max(it->last, end);\r\nlist_splice(&node->bos, &bos);\r\n}\r\nif (!node) {\r\nnode = kmalloc(sizeof(struct amdgpu_mn_node), GFP_KERNEL);\r\nif (!node) {\r\nmutex_unlock(&rmn->lock);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nbo->mn = rmn;\r\nnode->it.start = addr;\r\nnode->it.last = end;\r\nINIT_LIST_HEAD(&node->bos);\r\nlist_splice(&bos, &node->bos);\r\nlist_add(&bo->mn_list, &node->bos);\r\ninterval_tree_insert(&node->it, &rmn->objects);\r\nmutex_unlock(&rmn->lock);\r\nreturn 0;\r\n}\r\nvoid amdgpu_mn_unregister(struct amdgpu_bo *bo)\r\n{\r\nstruct amdgpu_device *adev = bo->adev;\r\nstruct amdgpu_mn *rmn;\r\nstruct list_head *head;\r\nmutex_lock(&adev->mn_lock);\r\nrmn = bo->mn;\r\nif (rmn == NULL) {\r\nmutex_unlock(&adev->mn_lock);\r\nreturn;\r\n}\r\nmutex_lock(&rmn->lock);\r\nhead = bo->mn_list.next;\r\nbo->mn = NULL;\r\nlist_del(&bo->mn_list);\r\nif (list_empty(head)) {\r\nstruct amdgpu_mn_node *node;\r\nnode = container_of(head, struct amdgpu_mn_node, bos);\r\ninterval_tree_remove(&node->it, &rmn->objects);\r\nkfree(node);\r\n}\r\nmutex_unlock(&rmn->lock);\r\nmutex_unlock(&adev->mn_lock);\r\n}
