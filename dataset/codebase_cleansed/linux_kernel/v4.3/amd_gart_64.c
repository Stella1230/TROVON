static unsigned long alloc_iommu(struct device *dev, int size,\r\nunsigned long align_mask)\r\n{\r\nunsigned long offset, flags;\r\nunsigned long boundary_size;\r\nunsigned long base_index;\r\nbase_index = ALIGN(iommu_bus_base & dma_get_seg_boundary(dev),\r\nPAGE_SIZE) >> PAGE_SHIFT;\r\nboundary_size = ALIGN((u64)dma_get_seg_boundary(dev) + 1,\r\nPAGE_SIZE) >> PAGE_SHIFT;\r\nspin_lock_irqsave(&iommu_bitmap_lock, flags);\r\noffset = iommu_area_alloc(iommu_gart_bitmap, iommu_pages, next_bit,\r\nsize, base_index, boundary_size, align_mask);\r\nif (offset == -1) {\r\nneed_flush = true;\r\noffset = iommu_area_alloc(iommu_gart_bitmap, iommu_pages, 0,\r\nsize, base_index, boundary_size,\r\nalign_mask);\r\n}\r\nif (offset != -1) {\r\nnext_bit = offset+size;\r\nif (next_bit >= iommu_pages) {\r\nnext_bit = 0;\r\nneed_flush = true;\r\n}\r\n}\r\nif (iommu_fullflush)\r\nneed_flush = true;\r\nspin_unlock_irqrestore(&iommu_bitmap_lock, flags);\r\nreturn offset;\r\n}\r\nstatic void free_iommu(unsigned long offset, int size)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&iommu_bitmap_lock, flags);\r\nbitmap_clear(iommu_gart_bitmap, offset, size);\r\nif (offset >= next_bit)\r\nnext_bit = offset + size;\r\nspin_unlock_irqrestore(&iommu_bitmap_lock, flags);\r\n}\r\nstatic void flush_gart(void)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&iommu_bitmap_lock, flags);\r\nif (need_flush) {\r\namd_flush_garts();\r\nneed_flush = false;\r\n}\r\nspin_unlock_irqrestore(&iommu_bitmap_lock, flags);\r\n}\r\nstatic void dump_leak(void)\r\n{\r\nstatic int dump;\r\nif (dump)\r\nreturn;\r\ndump = 1;\r\nshow_stack(NULL, NULL);\r\ndebug_dma_dump_mappings(NULL);\r\n}\r\nstatic void iommu_full(struct device *dev, size_t size, int dir)\r\n{\r\ndev_err(dev, "PCI-DMA: Out of IOMMU space for %lu bytes\n", size);\r\nif (size > PAGE_SIZE*EMERGENCY_PAGES) {\r\nif (dir == PCI_DMA_FROMDEVICE || dir == PCI_DMA_BIDIRECTIONAL)\r\npanic("PCI-DMA: Memory would be corrupted\n");\r\nif (dir == PCI_DMA_TODEVICE || dir == PCI_DMA_BIDIRECTIONAL)\r\npanic(KERN_ERR\r\n"PCI-DMA: Random memory would be DMAed\n");\r\n}\r\n#ifdef CONFIG_IOMMU_LEAK\r\ndump_leak();\r\n#endif\r\n}\r\nstatic inline int\r\nneed_iommu(struct device *dev, unsigned long addr, size_t size)\r\n{\r\nreturn force_iommu || !dma_capable(dev, addr, size);\r\n}\r\nstatic inline int\r\nnonforced_iommu(struct device *dev, unsigned long addr, size_t size)\r\n{\r\nreturn !dma_capable(dev, addr, size);\r\n}\r\nstatic dma_addr_t dma_map_area(struct device *dev, dma_addr_t phys_mem,\r\nsize_t size, int dir, unsigned long align_mask)\r\n{\r\nunsigned long npages = iommu_num_pages(phys_mem, size, PAGE_SIZE);\r\nunsigned long iommu_page;\r\nint i;\r\nif (unlikely(phys_mem + size > GART_MAX_PHYS_ADDR))\r\nreturn bad_dma_addr;\r\niommu_page = alloc_iommu(dev, npages, align_mask);\r\nif (iommu_page == -1) {\r\nif (!nonforced_iommu(dev, phys_mem, size))\r\nreturn phys_mem;\r\nif (panic_on_overflow)\r\npanic("dma_map_area overflow %lu bytes\n", size);\r\niommu_full(dev, size, dir);\r\nreturn bad_dma_addr;\r\n}\r\nfor (i = 0; i < npages; i++) {\r\niommu_gatt_base[iommu_page + i] = GPTE_ENCODE(phys_mem);\r\nphys_mem += PAGE_SIZE;\r\n}\r\nreturn iommu_bus_base + iommu_page*PAGE_SIZE + (phys_mem & ~PAGE_MASK);\r\n}\r\nstatic dma_addr_t gart_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nunsigned long bus;\r\nphys_addr_t paddr = page_to_phys(page) + offset;\r\nif (!dev)\r\ndev = &x86_dma_fallback_dev;\r\nif (!need_iommu(dev, paddr, size))\r\nreturn paddr;\r\nbus = dma_map_area(dev, paddr, size, dir, 0);\r\nflush_gart();\r\nreturn bus;\r\n}\r\nstatic void gart_unmap_page(struct device *dev, dma_addr_t dma_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nunsigned long iommu_page;\r\nint npages;\r\nint i;\r\nif (dma_addr < iommu_bus_base + EMERGENCY_PAGES*PAGE_SIZE ||\r\ndma_addr >= iommu_bus_base + iommu_size)\r\nreturn;\r\niommu_page = (dma_addr - iommu_bus_base)>>PAGE_SHIFT;\r\nnpages = iommu_num_pages(dma_addr, size, PAGE_SIZE);\r\nfor (i = 0; i < npages; i++) {\r\niommu_gatt_base[iommu_page + i] = gart_unmapped_entry;\r\n}\r\nfree_iommu(iommu_page, npages);\r\n}\r\nstatic void gart_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\r\nenum dma_data_direction dir, struct dma_attrs *attrs)\r\n{\r\nstruct scatterlist *s;\r\nint i;\r\nfor_each_sg(sg, s, nents, i) {\r\nif (!s->dma_length || !s->length)\r\nbreak;\r\ngart_unmap_page(dev, s->dma_address, s->dma_length, dir, NULL);\r\n}\r\n}\r\nstatic int dma_map_sg_nonforce(struct device *dev, struct scatterlist *sg,\r\nint nents, int dir)\r\n{\r\nstruct scatterlist *s;\r\nint i;\r\n#ifdef CONFIG_IOMMU_DEBUG\r\npr_debug("dma_map_sg overflow\n");\r\n#endif\r\nfor_each_sg(sg, s, nents, i) {\r\nunsigned long addr = sg_phys(s);\r\nif (nonforced_iommu(dev, addr, s->length)) {\r\naddr = dma_map_area(dev, addr, s->length, dir, 0);\r\nif (addr == bad_dma_addr) {\r\nif (i > 0)\r\ngart_unmap_sg(dev, sg, i, dir, NULL);\r\nnents = 0;\r\nsg[0].dma_length = 0;\r\nbreak;\r\n}\r\n}\r\ns->dma_address = addr;\r\ns->dma_length = s->length;\r\n}\r\nflush_gart();\r\nreturn nents;\r\n}\r\nstatic int __dma_map_cont(struct device *dev, struct scatterlist *start,\r\nint nelems, struct scatterlist *sout,\r\nunsigned long pages)\r\n{\r\nunsigned long iommu_start = alloc_iommu(dev, pages, 0);\r\nunsigned long iommu_page = iommu_start;\r\nstruct scatterlist *s;\r\nint i;\r\nif (iommu_start == -1)\r\nreturn -1;\r\nfor_each_sg(start, s, nelems, i) {\r\nunsigned long pages, addr;\r\nunsigned long phys_addr = s->dma_address;\r\nBUG_ON(s != start && s->offset);\r\nif (s == start) {\r\nsout->dma_address = iommu_bus_base;\r\nsout->dma_address += iommu_page*PAGE_SIZE + s->offset;\r\nsout->dma_length = s->length;\r\n} else {\r\nsout->dma_length += s->length;\r\n}\r\naddr = phys_addr;\r\npages = iommu_num_pages(s->offset, s->length, PAGE_SIZE);\r\nwhile (pages--) {\r\niommu_gatt_base[iommu_page] = GPTE_ENCODE(addr);\r\naddr += PAGE_SIZE;\r\niommu_page++;\r\n}\r\n}\r\nBUG_ON(iommu_page - iommu_start != pages);\r\nreturn 0;\r\n}\r\nstatic inline int\r\ndma_map_cont(struct device *dev, struct scatterlist *start, int nelems,\r\nstruct scatterlist *sout, unsigned long pages, int need)\r\n{\r\nif (!need) {\r\nBUG_ON(nelems != 1);\r\nsout->dma_address = start->dma_address;\r\nsout->dma_length = start->length;\r\nreturn 0;\r\n}\r\nreturn __dma_map_cont(dev, start, nelems, sout, pages);\r\n}\r\nstatic int gart_map_sg(struct device *dev, struct scatterlist *sg, int nents,\r\nenum dma_data_direction dir, struct dma_attrs *attrs)\r\n{\r\nstruct scatterlist *s, *ps, *start_sg, *sgmap;\r\nint need = 0, nextneed, i, out, start;\r\nunsigned long pages = 0;\r\nunsigned int seg_size;\r\nunsigned int max_seg_size;\r\nif (nents == 0)\r\nreturn 0;\r\nif (!dev)\r\ndev = &x86_dma_fallback_dev;\r\nout = 0;\r\nstart = 0;\r\nstart_sg = sg;\r\nsgmap = sg;\r\nseg_size = 0;\r\nmax_seg_size = dma_get_max_seg_size(dev);\r\nps = NULL;\r\nfor_each_sg(sg, s, nents, i) {\r\ndma_addr_t addr = sg_phys(s);\r\ns->dma_address = addr;\r\nBUG_ON(s->length == 0);\r\nnextneed = need_iommu(dev, addr, s->length);\r\nif (i > start) {\r\nif (!iommu_merge || !nextneed || !need || s->offset ||\r\n(s->length + seg_size > max_seg_size) ||\r\n(ps->offset + ps->length) % PAGE_SIZE) {\r\nif (dma_map_cont(dev, start_sg, i - start,\r\nsgmap, pages, need) < 0)\r\ngoto error;\r\nout++;\r\nseg_size = 0;\r\nsgmap = sg_next(sgmap);\r\npages = 0;\r\nstart = i;\r\nstart_sg = s;\r\n}\r\n}\r\nseg_size += s->length;\r\nneed = nextneed;\r\npages += iommu_num_pages(s->offset, s->length, PAGE_SIZE);\r\nps = s;\r\n}\r\nif (dma_map_cont(dev, start_sg, i - start, sgmap, pages, need) < 0)\r\ngoto error;\r\nout++;\r\nflush_gart();\r\nif (out < nents) {\r\nsgmap = sg_next(sgmap);\r\nsgmap->dma_length = 0;\r\n}\r\nreturn out;\r\nerror:\r\nflush_gart();\r\ngart_unmap_sg(dev, sg, out, dir, NULL);\r\nif (force_iommu || iommu_merge) {\r\nout = dma_map_sg_nonforce(dev, sg, nents, dir);\r\nif (out > 0)\r\nreturn out;\r\n}\r\nif (panic_on_overflow)\r\npanic("dma_map_sg: overflow on %lu pages\n", pages);\r\niommu_full(dev, pages << PAGE_SHIFT, dir);\r\nfor_each_sg(sg, s, nents, i)\r\ns->dma_address = bad_dma_addr;\r\nreturn 0;\r\n}\r\nstatic void *\r\ngart_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_addr,\r\ngfp_t flag, struct dma_attrs *attrs)\r\n{\r\ndma_addr_t paddr;\r\nunsigned long align_mask;\r\nstruct page *page;\r\nif (force_iommu && !(flag & GFP_DMA)) {\r\nflag &= ~(__GFP_DMA | __GFP_HIGHMEM | __GFP_DMA32);\r\npage = alloc_pages(flag | __GFP_ZERO, get_order(size));\r\nif (!page)\r\nreturn NULL;\r\nalign_mask = (1UL << get_order(size)) - 1;\r\npaddr = dma_map_area(dev, page_to_phys(page), size,\r\nDMA_BIDIRECTIONAL, align_mask);\r\nflush_gart();\r\nif (paddr != bad_dma_addr) {\r\n*dma_addr = paddr;\r\nreturn page_address(page);\r\n}\r\n__free_pages(page, get_order(size));\r\n} else\r\nreturn dma_generic_alloc_coherent(dev, size, dma_addr, flag,\r\nattrs);\r\nreturn NULL;\r\n}\r\nstatic void\r\ngart_free_coherent(struct device *dev, size_t size, void *vaddr,\r\ndma_addr_t dma_addr, struct dma_attrs *attrs)\r\n{\r\ngart_unmap_page(dev, dma_addr, size, DMA_BIDIRECTIONAL, NULL);\r\ndma_generic_free_coherent(dev, size, vaddr, dma_addr, attrs);\r\n}\r\nstatic int gart_mapping_error(struct device *dev, dma_addr_t dma_addr)\r\n{\r\nreturn (dma_addr == bad_dma_addr);\r\n}\r\nstatic __init unsigned long check_iommu_size(unsigned long aper, u64 aper_size)\r\n{\r\nunsigned long a;\r\nif (!iommu_size) {\r\niommu_size = aper_size;\r\nif (!no_agp)\r\niommu_size /= 2;\r\n}\r\na = aper + iommu_size;\r\niommu_size -= round_up(a, PMD_PAGE_SIZE) - a;\r\nif (iommu_size < 64*1024*1024) {\r\npr_warning(\r\n"PCI-DMA: Warning: Small IOMMU %luMB."\r\n" Consider increasing the AGP aperture in BIOS\n",\r\niommu_size >> 20);\r\n}\r\nreturn iommu_size;\r\n}\r\nstatic __init unsigned read_aperture(struct pci_dev *dev, u32 *size)\r\n{\r\nunsigned aper_size = 0, aper_base_32, aper_order;\r\nu64 aper_base;\r\npci_read_config_dword(dev, AMD64_GARTAPERTUREBASE, &aper_base_32);\r\npci_read_config_dword(dev, AMD64_GARTAPERTURECTL, &aper_order);\r\naper_order = (aper_order >> 1) & 7;\r\naper_base = aper_base_32 & 0x7fff;\r\naper_base <<= 25;\r\naper_size = (32 * 1024 * 1024) << aper_order;\r\nif (aper_base + aper_size > 0x100000000UL || !aper_size)\r\naper_base = 0;\r\n*size = aper_size;\r\nreturn aper_base;\r\n}\r\nstatic void enable_gart_translations(void)\r\n{\r\nint i;\r\nif (!amd_nb_has_feature(AMD_NB_GART))\r\nreturn;\r\nfor (i = 0; i < amd_nb_num(); i++) {\r\nstruct pci_dev *dev = node_to_amd_nb(i)->misc;\r\nenable_gart_translation(dev, __pa(agp_gatt_table));\r\n}\r\namd_flush_garts();\r\n}\r\nvoid set_up_gart_resume(u32 aper_order, u32 aper_alloc)\r\n{\r\nfix_up_north_bridges = true;\r\naperture_order = aper_order;\r\naperture_alloc = aper_alloc;\r\n}\r\nstatic void gart_fixup_northbridges(void)\r\n{\r\nint i;\r\nif (!fix_up_north_bridges)\r\nreturn;\r\nif (!amd_nb_has_feature(AMD_NB_GART))\r\nreturn;\r\npr_info("PCI-DMA: Restoring GART aperture settings\n");\r\nfor (i = 0; i < amd_nb_num(); i++) {\r\nstruct pci_dev *dev = node_to_amd_nb(i)->misc;\r\ngart_set_size_and_enable(dev, aperture_order);\r\npci_write_config_dword(dev, AMD64_GARTAPERTUREBASE, aperture_alloc >> 25);\r\n}\r\n}\r\nstatic void gart_resume(void)\r\n{\r\npr_info("PCI-DMA: Resuming GART IOMMU\n");\r\ngart_fixup_northbridges();\r\nenable_gart_translations();\r\n}\r\nstatic __init int init_amd_gatt(struct agp_kern_info *info)\r\n{\r\nunsigned aper_size, gatt_size, new_aper_size;\r\nunsigned aper_base, new_aper_base;\r\nstruct pci_dev *dev;\r\nvoid *gatt;\r\nint i;\r\npr_info("PCI-DMA: Disabling AGP.\n");\r\naper_size = aper_base = info->aper_size = 0;\r\ndev = NULL;\r\nfor (i = 0; i < amd_nb_num(); i++) {\r\ndev = node_to_amd_nb(i)->misc;\r\nnew_aper_base = read_aperture(dev, &new_aper_size);\r\nif (!new_aper_base)\r\ngoto nommu;\r\nif (!aper_base) {\r\naper_size = new_aper_size;\r\naper_base = new_aper_base;\r\n}\r\nif (aper_size != new_aper_size || aper_base != new_aper_base)\r\ngoto nommu;\r\n}\r\nif (!aper_base)\r\ngoto nommu;\r\ninfo->aper_base = aper_base;\r\ninfo->aper_size = aper_size >> 20;\r\ngatt_size = (aper_size >> PAGE_SHIFT) * sizeof(u32);\r\ngatt = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(gatt_size));\r\nif (!gatt)\r\npanic("Cannot allocate GATT table");\r\nif (set_memory_uc((unsigned long)gatt, gatt_size >> PAGE_SHIFT))\r\npanic("Could not set GART PTEs to uncacheable pages");\r\nagp_gatt_table = gatt;\r\nregister_syscore_ops(&gart_syscore_ops);\r\nflush_gart();\r\npr_info("PCI-DMA: aperture base @ %x size %u KB\n",\r\naper_base, aper_size>>10);\r\nreturn 0;\r\nnommu:\r\npr_warning("PCI-DMA: More than 4GB of RAM and no IOMMU\n"\r\n"falling back to iommu=soft.\n");\r\nreturn -1;\r\n}\r\nstatic void gart_iommu_shutdown(void)\r\n{\r\nstruct pci_dev *dev;\r\nint i;\r\nif (!no_agp)\r\nreturn;\r\nif (!amd_nb_has_feature(AMD_NB_GART))\r\nreturn;\r\nfor (i = 0; i < amd_nb_num(); i++) {\r\nu32 ctl;\r\ndev = node_to_amd_nb(i)->misc;\r\npci_read_config_dword(dev, AMD64_GARTAPERTURECTL, &ctl);\r\nctl &= ~GARTEN;\r\npci_write_config_dword(dev, AMD64_GARTAPERTURECTL, ctl);\r\n}\r\n}\r\nint __init gart_iommu_init(void)\r\n{\r\nstruct agp_kern_info info;\r\nunsigned long iommu_start;\r\nunsigned long aper_base, aper_size;\r\nunsigned long start_pfn, end_pfn;\r\nunsigned long scratch;\r\nlong i;\r\nif (!amd_nb_has_feature(AMD_NB_GART))\r\nreturn 0;\r\n#ifndef CONFIG_AGP_AMD64\r\nno_agp = 1;\r\n#else\r\nno_agp = no_agp ||\r\n(agp_amd64_init() < 0) ||\r\n(agp_copy_info(agp_bridge, &info) < 0);\r\n#endif\r\nif (no_iommu ||\r\n(!force_iommu && max_pfn <= MAX_DMA32_PFN) ||\r\n!gart_iommu_aperture ||\r\n(no_agp && init_amd_gatt(&info) < 0)) {\r\nif (max_pfn > MAX_DMA32_PFN) {\r\npr_warning("More than 4GB of memory but GART IOMMU not available.\n");\r\npr_warning("falling back to iommu=soft.\n");\r\n}\r\nreturn 0;\r\n}\r\naper_size = info.aper_size << 20;\r\naper_base = info.aper_base;\r\nend_pfn = (aper_base>>PAGE_SHIFT) + (aper_size>>PAGE_SHIFT);\r\nstart_pfn = PFN_DOWN(aper_base);\r\nif (!pfn_range_is_mapped(start_pfn, end_pfn))\r\ninit_memory_mapping(start_pfn<<PAGE_SHIFT, end_pfn<<PAGE_SHIFT);\r\npr_info("PCI-DMA: using GART IOMMU.\n");\r\niommu_size = check_iommu_size(info.aper_base, aper_size);\r\niommu_pages = iommu_size >> PAGE_SHIFT;\r\niommu_gart_bitmap = (void *) __get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(iommu_pages/8));\r\nif (!iommu_gart_bitmap)\r\npanic("Cannot allocate iommu bitmap\n");\r\n#ifdef CONFIG_IOMMU_LEAK\r\nif (leak_trace) {\r\nint ret;\r\nret = dma_debug_resize_entries(iommu_pages);\r\nif (ret)\r\npr_debug("PCI-DMA: Cannot trace all the entries\n");\r\n}\r\n#endif\r\nbitmap_set(iommu_gart_bitmap, 0, EMERGENCY_PAGES);\r\npr_info("PCI-DMA: Reserving %luMB of IOMMU area in the AGP aperture\n",\r\niommu_size >> 20);\r\nagp_memory_reserved = iommu_size;\r\niommu_start = aper_size - iommu_size;\r\niommu_bus_base = info.aper_base + iommu_start;\r\nbad_dma_addr = iommu_bus_base;\r\niommu_gatt_base = agp_gatt_table + (iommu_start>>PAGE_SHIFT);\r\nset_memory_np((unsigned long)__va(iommu_bus_base),\r\niommu_size >> PAGE_SHIFT);\r\nwbinvd();\r\nenable_gart_translations();\r\nscratch = get_zeroed_page(GFP_KERNEL);\r\nif (!scratch)\r\npanic("Cannot allocate iommu scratch page");\r\ngart_unmapped_entry = GPTE_ENCODE(__pa(scratch));\r\nfor (i = EMERGENCY_PAGES; i < iommu_pages; i++)\r\niommu_gatt_base[i] = gart_unmapped_entry;\r\nflush_gart();\r\ndma_ops = &gart_dma_ops;\r\nx86_platform.iommu_shutdown = gart_iommu_shutdown;\r\nswiotlb = 0;\r\nreturn 0;\r\n}\r\nvoid __init gart_parse_options(char *p)\r\n{\r\nint arg;\r\n#ifdef CONFIG_IOMMU_LEAK\r\nif (!strncmp(p, "leak", 4)) {\r\nleak_trace = 1;\r\np += 4;\r\nif (*p == '=')\r\n++p;\r\nif (isdigit(*p) && get_option(&p, &arg))\r\niommu_leak_pages = arg;\r\n}\r\n#endif\r\nif (isdigit(*p) && get_option(&p, &arg))\r\niommu_size = arg;\r\nif (!strncmp(p, "fullflush", 9))\r\niommu_fullflush = 1;\r\nif (!strncmp(p, "nofullflush", 11))\r\niommu_fullflush = 0;\r\nif (!strncmp(p, "noagp", 5))\r\nno_agp = 1;\r\nif (!strncmp(p, "noaperture", 10))\r\nfix_aperture = 0;\r\nif (!strncmp(p, "force", 5))\r\ngart_iommu_aperture_allowed = 1;\r\nif (!strncmp(p, "allowed", 7))\r\ngart_iommu_aperture_allowed = 1;\r\nif (!strncmp(p, "memaper", 7)) {\r\nfallback_aper_force = 1;\r\np += 7;\r\nif (*p == '=') {\r\n++p;\r\nif (get_option(&p, &arg))\r\nfallback_aper_order = arg;\r\n}\r\n}\r\n}
