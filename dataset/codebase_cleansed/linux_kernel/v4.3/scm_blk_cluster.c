void __scm_free_rq_cluster(struct scm_request *scmrq)\r\n{\r\nint i;\r\nif (!scmrq->cluster.buf)\r\nreturn;\r\nfor (i = 0; i < 2 * write_cluster_size; i++)\r\nfree_page((unsigned long) scmrq->cluster.buf[i]);\r\nkfree(scmrq->cluster.buf);\r\n}\r\nint __scm_alloc_rq_cluster(struct scm_request *scmrq)\r\n{\r\nint i;\r\nscmrq->cluster.buf = kzalloc(sizeof(void *) * 2 * write_cluster_size,\r\nGFP_KERNEL);\r\nif (!scmrq->cluster.buf)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < 2 * write_cluster_size; i++) {\r\nscmrq->cluster.buf[i] = (void *) get_zeroed_page(GFP_DMA);\r\nif (!scmrq->cluster.buf[i])\r\nreturn -ENOMEM;\r\n}\r\nINIT_LIST_HEAD(&scmrq->cluster.list);\r\nreturn 0;\r\n}\r\nvoid scm_request_cluster_init(struct scm_request *scmrq)\r\n{\r\nscmrq->cluster.state = CLUSTER_NONE;\r\n}\r\nstatic bool clusters_intersect(struct request *A, struct request *B)\r\n{\r\nunsigned long firstA, lastA, firstB, lastB;\r\nfirstA = ((u64) blk_rq_pos(A) << 9) / CLUSTER_SIZE;\r\nlastA = (((u64) blk_rq_pos(A) << 9) +\r\nblk_rq_bytes(A) - 1) / CLUSTER_SIZE;\r\nfirstB = ((u64) blk_rq_pos(B) << 9) / CLUSTER_SIZE;\r\nlastB = (((u64) blk_rq_pos(B) << 9) +\r\nblk_rq_bytes(B) - 1) / CLUSTER_SIZE;\r\nreturn (firstB <= lastA && firstA <= lastB);\r\n}\r\nbool scm_reserve_cluster(struct scm_request *scmrq)\r\n{\r\nstruct request *req = scmrq->request[scmrq->aob->request.msb_count];\r\nstruct scm_blk_dev *bdev = scmrq->bdev;\r\nstruct scm_request *iter;\r\nint pos, add = 1;\r\nif (write_cluster_size == 0)\r\nreturn true;\r\nspin_lock(&bdev->lock);\r\nlist_for_each_entry(iter, &bdev->cluster_list, cluster.list) {\r\nif (iter == scmrq) {\r\nadd = 0;\r\ncontinue;\r\n}\r\nfor (pos = 0; pos < iter->aob->request.msb_count; pos++) {\r\nif (clusters_intersect(req, iter->request[pos]) &&\r\n(rq_data_dir(req) == WRITE ||\r\nrq_data_dir(iter->request[pos]) == WRITE)) {\r\nspin_unlock(&bdev->lock);\r\nreturn false;\r\n}\r\n}\r\n}\r\nif (add)\r\nlist_add(&scmrq->cluster.list, &bdev->cluster_list);\r\nspin_unlock(&bdev->lock);\r\nreturn true;\r\n}\r\nvoid scm_release_cluster(struct scm_request *scmrq)\r\n{\r\nstruct scm_blk_dev *bdev = scmrq->bdev;\r\nunsigned long flags;\r\nif (write_cluster_size == 0)\r\nreturn;\r\nspin_lock_irqsave(&bdev->lock, flags);\r\nlist_del(&scmrq->cluster.list);\r\nspin_unlock_irqrestore(&bdev->lock, flags);\r\n}\r\nvoid scm_blk_dev_cluster_setup(struct scm_blk_dev *bdev)\r\n{\r\nINIT_LIST_HEAD(&bdev->cluster_list);\r\nblk_queue_io_opt(bdev->rq, CLUSTER_SIZE);\r\n}\r\nstatic int scm_prepare_cluster_request(struct scm_request *scmrq)\r\n{\r\nstruct scm_blk_dev *bdev = scmrq->bdev;\r\nstruct scm_device *scmdev = bdev->gendisk->private_data;\r\nstruct request *req = scmrq->request[0];\r\nstruct msb *msb = &scmrq->aob->msb[0];\r\nstruct req_iterator iter;\r\nstruct aidaw *aidaw;\r\nstruct bio_vec bv;\r\nint i = 0;\r\nu64 addr;\r\nswitch (scmrq->cluster.state) {\r\ncase CLUSTER_NONE:\r\nscmrq->cluster.state = CLUSTER_READ;\r\ncase CLUSTER_READ:\r\nmsb->bs = MSB_BS_4K;\r\nmsb->oc = MSB_OC_READ;\r\nmsb->flags = MSB_FLAG_IDA;\r\nmsb->blk_count = write_cluster_size;\r\naddr = scmdev->address + ((u64) blk_rq_pos(req) << 9);\r\nmsb->scm_addr = round_down(addr, CLUSTER_SIZE);\r\nif (msb->scm_addr !=\r\nround_down(addr + (u64) blk_rq_bytes(req) - 1,\r\nCLUSTER_SIZE))\r\nmsb->blk_count = 2 * write_cluster_size;\r\naidaw = scm_aidaw_fetch(scmrq, msb->blk_count * PAGE_SIZE);\r\nif (!aidaw)\r\nreturn -ENOMEM;\r\nscmrq->aob->request.msb_count = 1;\r\nmsb->data_addr = (u64) aidaw;\r\nfor (i = 0; i < msb->blk_count; i++) {\r\naidaw->data_addr = (u64) scmrq->cluster.buf[i];\r\naidaw++;\r\n}\r\nbreak;\r\ncase CLUSTER_WRITE:\r\naidaw = (void *) msb->data_addr;\r\nmsb->oc = MSB_OC_WRITE;\r\nfor (addr = msb->scm_addr;\r\naddr < scmdev->address + ((u64) blk_rq_pos(req) << 9);\r\naddr += PAGE_SIZE) {\r\naidaw->data_addr = (u64) scmrq->cluster.buf[i];\r\naidaw++;\r\ni++;\r\n}\r\nrq_for_each_segment(bv, req, iter) {\r\naidaw->data_addr = (u64) page_address(bv.bv_page);\r\naidaw++;\r\ni++;\r\n}\r\nfor (; i < msb->blk_count; i++) {\r\naidaw->data_addr = (u64) scmrq->cluster.buf[i];\r\naidaw++;\r\n}\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nbool scm_need_cluster_request(struct scm_request *scmrq)\r\n{\r\nint pos = scmrq->aob->request.msb_count;\r\nif (rq_data_dir(scmrq->request[pos]) == READ)\r\nreturn false;\r\nreturn blk_rq_bytes(scmrq->request[pos]) < CLUSTER_SIZE;\r\n}\r\nvoid scm_initiate_cluster_request(struct scm_request *scmrq)\r\n{\r\nif (scm_prepare_cluster_request(scmrq))\r\ngoto requeue;\r\nif (eadm_start_aob(scmrq->aob))\r\ngoto requeue;\r\nreturn;\r\nrequeue:\r\nscm_request_requeue(scmrq);\r\n}\r\nbool scm_test_cluster_request(struct scm_request *scmrq)\r\n{\r\nreturn scmrq->cluster.state != CLUSTER_NONE;\r\n}\r\nvoid scm_cluster_request_irq(struct scm_request *scmrq)\r\n{\r\nstruct scm_blk_dev *bdev = scmrq->bdev;\r\nunsigned long flags;\r\nswitch (scmrq->cluster.state) {\r\ncase CLUSTER_NONE:\r\nBUG();\r\nbreak;\r\ncase CLUSTER_READ:\r\nif (scmrq->error) {\r\nscm_request_finish(scmrq);\r\nbreak;\r\n}\r\nscmrq->cluster.state = CLUSTER_WRITE;\r\nspin_lock_irqsave(&bdev->rq_lock, flags);\r\nscm_initiate_cluster_request(scmrq);\r\nspin_unlock_irqrestore(&bdev->rq_lock, flags);\r\nbreak;\r\ncase CLUSTER_WRITE:\r\nscm_request_finish(scmrq);\r\nbreak;\r\n}\r\n}\r\nbool scm_cluster_size_valid(void)\r\n{\r\nif (write_cluster_size == 1 || write_cluster_size > 128)\r\nreturn false;\r\nreturn !(write_cluster_size & (write_cluster_size - 1));\r\n}
