static u64 get_nsecs(void)\r\n{\r\nstruct timespec ts;\r\nclock_gettime(CLOCK_MONOTONIC, &ts);\r\nreturn ts.tv_sec * 1000000000ULL + ts.tv_nsec;\r\n}\r\nstatic void burn_nsecs(struct perf_sched *sched, u64 nsecs)\r\n{\r\nu64 T0 = get_nsecs(), T1;\r\ndo {\r\nT1 = get_nsecs();\r\n} while (T1 + sched->run_measurement_overhead < T0 + nsecs);\r\n}\r\nstatic void sleep_nsecs(u64 nsecs)\r\n{\r\nstruct timespec ts;\r\nts.tv_nsec = nsecs % 999999999;\r\nts.tv_sec = nsecs / 999999999;\r\nnanosleep(&ts, NULL);\r\n}\r\nstatic void calibrate_run_measurement_overhead(struct perf_sched *sched)\r\n{\r\nu64 T0, T1, delta, min_delta = 1000000000ULL;\r\nint i;\r\nfor (i = 0; i < 10; i++) {\r\nT0 = get_nsecs();\r\nburn_nsecs(sched, 0);\r\nT1 = get_nsecs();\r\ndelta = T1-T0;\r\nmin_delta = min(min_delta, delta);\r\n}\r\nsched->run_measurement_overhead = min_delta;\r\nprintf("run measurement overhead: %" PRIu64 " nsecs\n", min_delta);\r\n}\r\nstatic void calibrate_sleep_measurement_overhead(struct perf_sched *sched)\r\n{\r\nu64 T0, T1, delta, min_delta = 1000000000ULL;\r\nint i;\r\nfor (i = 0; i < 10; i++) {\r\nT0 = get_nsecs();\r\nsleep_nsecs(10000);\r\nT1 = get_nsecs();\r\ndelta = T1-T0;\r\nmin_delta = min(min_delta, delta);\r\n}\r\nmin_delta -= 10000;\r\nsched->sleep_measurement_overhead = min_delta;\r\nprintf("sleep measurement overhead: %" PRIu64 " nsecs\n", min_delta);\r\n}\r\nstatic struct sched_atom *\r\nget_new_event(struct task_desc *task, u64 timestamp)\r\n{\r\nstruct sched_atom *event = zalloc(sizeof(*event));\r\nunsigned long idx = task->nr_events;\r\nsize_t size;\r\nevent->timestamp = timestamp;\r\nevent->nr = idx;\r\ntask->nr_events++;\r\nsize = sizeof(struct sched_atom *) * task->nr_events;\r\ntask->atoms = realloc(task->atoms, size);\r\nBUG_ON(!task->atoms);\r\ntask->atoms[idx] = event;\r\nreturn event;\r\n}\r\nstatic struct sched_atom *last_event(struct task_desc *task)\r\n{\r\nif (!task->nr_events)\r\nreturn NULL;\r\nreturn task->atoms[task->nr_events - 1];\r\n}\r\nstatic void add_sched_event_run(struct perf_sched *sched, struct task_desc *task,\r\nu64 timestamp, u64 duration)\r\n{\r\nstruct sched_atom *event, *curr_event = last_event(task);\r\nif (curr_event && curr_event->type == SCHED_EVENT_RUN) {\r\nsched->nr_run_events_optimized++;\r\ncurr_event->duration += duration;\r\nreturn;\r\n}\r\nevent = get_new_event(task, timestamp);\r\nevent->type = SCHED_EVENT_RUN;\r\nevent->duration = duration;\r\nsched->nr_run_events++;\r\n}\r\nstatic void add_sched_event_wakeup(struct perf_sched *sched, struct task_desc *task,\r\nu64 timestamp, struct task_desc *wakee)\r\n{\r\nstruct sched_atom *event, *wakee_event;\r\nevent = get_new_event(task, timestamp);\r\nevent->type = SCHED_EVENT_WAKEUP;\r\nevent->wakee = wakee;\r\nwakee_event = last_event(wakee);\r\nif (!wakee_event || wakee_event->type != SCHED_EVENT_SLEEP) {\r\nsched->targetless_wakeups++;\r\nreturn;\r\n}\r\nif (wakee_event->wait_sem) {\r\nsched->multitarget_wakeups++;\r\nreturn;\r\n}\r\nwakee_event->wait_sem = zalloc(sizeof(*wakee_event->wait_sem));\r\nsem_init(wakee_event->wait_sem, 0, 0);\r\nwakee_event->specific_wait = 1;\r\nevent->wait_sem = wakee_event->wait_sem;\r\nsched->nr_wakeup_events++;\r\n}\r\nstatic void add_sched_event_sleep(struct perf_sched *sched, struct task_desc *task,\r\nu64 timestamp, u64 task_state __maybe_unused)\r\n{\r\nstruct sched_atom *event = get_new_event(task, timestamp);\r\nevent->type = SCHED_EVENT_SLEEP;\r\nsched->nr_sleep_events++;\r\n}\r\nstatic struct task_desc *register_pid(struct perf_sched *sched,\r\nunsigned long pid, const char *comm)\r\n{\r\nstruct task_desc *task;\r\nstatic int pid_max;\r\nif (sched->pid_to_task == NULL) {\r\nif (sysctl__read_int("kernel/pid_max", &pid_max) < 0)\r\npid_max = MAX_PID;\r\nBUG_ON((sched->pid_to_task = calloc(pid_max, sizeof(struct task_desc *))) == NULL);\r\n}\r\nif (pid >= (unsigned long)pid_max) {\r\nBUG_ON((sched->pid_to_task = realloc(sched->pid_to_task, (pid + 1) *\r\nsizeof(struct task_desc *))) == NULL);\r\nwhile (pid >= (unsigned long)pid_max)\r\nsched->pid_to_task[pid_max++] = NULL;\r\n}\r\ntask = sched->pid_to_task[pid];\r\nif (task)\r\nreturn task;\r\ntask = zalloc(sizeof(*task));\r\ntask->pid = pid;\r\ntask->nr = sched->nr_tasks;\r\nstrcpy(task->comm, comm);\r\nadd_sched_event_sleep(sched, task, 0, 0);\r\nsched->pid_to_task[pid] = task;\r\nsched->nr_tasks++;\r\nsched->tasks = realloc(sched->tasks, sched->nr_tasks * sizeof(struct task_desc *));\r\nBUG_ON(!sched->tasks);\r\nsched->tasks[task->nr] = task;\r\nif (verbose)\r\nprintf("registered task #%ld, PID %ld (%s)\n", sched->nr_tasks, pid, comm);\r\nreturn task;\r\n}\r\nstatic void print_task_traces(struct perf_sched *sched)\r\n{\r\nstruct task_desc *task;\r\nunsigned long i;\r\nfor (i = 0; i < sched->nr_tasks; i++) {\r\ntask = sched->tasks[i];\r\nprintf("task %6ld (%20s:%10ld), nr_events: %ld\n",\r\ntask->nr, task->comm, task->pid, task->nr_events);\r\n}\r\n}\r\nstatic void add_cross_task_wakeups(struct perf_sched *sched)\r\n{\r\nstruct task_desc *task1, *task2;\r\nunsigned long i, j;\r\nfor (i = 0; i < sched->nr_tasks; i++) {\r\ntask1 = sched->tasks[i];\r\nj = i + 1;\r\nif (j == sched->nr_tasks)\r\nj = 0;\r\ntask2 = sched->tasks[j];\r\nadd_sched_event_wakeup(sched, task1, 0, task2);\r\n}\r\n}\r\nstatic void perf_sched__process_event(struct perf_sched *sched,\r\nstruct sched_atom *atom)\r\n{\r\nint ret = 0;\r\nswitch (atom->type) {\r\ncase SCHED_EVENT_RUN:\r\nburn_nsecs(sched, atom->duration);\r\nbreak;\r\ncase SCHED_EVENT_SLEEP:\r\nif (atom->wait_sem)\r\nret = sem_wait(atom->wait_sem);\r\nBUG_ON(ret);\r\nbreak;\r\ncase SCHED_EVENT_WAKEUP:\r\nif (atom->wait_sem)\r\nret = sem_post(atom->wait_sem);\r\nBUG_ON(ret);\r\nbreak;\r\ncase SCHED_EVENT_MIGRATION:\r\nbreak;\r\ndefault:\r\nBUG_ON(1);\r\n}\r\n}\r\nstatic u64 get_cpu_usage_nsec_parent(void)\r\n{\r\nstruct rusage ru;\r\nu64 sum;\r\nint err;\r\nerr = getrusage(RUSAGE_SELF, &ru);\r\nBUG_ON(err);\r\nsum = ru.ru_utime.tv_sec*1e9 + ru.ru_utime.tv_usec*1e3;\r\nsum += ru.ru_stime.tv_sec*1e9 + ru.ru_stime.tv_usec*1e3;\r\nreturn sum;\r\n}\r\nstatic int self_open_counters(struct perf_sched *sched, unsigned long cur_task)\r\n{\r\nstruct perf_event_attr attr;\r\nchar sbuf[STRERR_BUFSIZE], info[STRERR_BUFSIZE];\r\nint fd;\r\nstruct rlimit limit;\r\nbool need_privilege = false;\r\nmemset(&attr, 0, sizeof(attr));\r\nattr.type = PERF_TYPE_SOFTWARE;\r\nattr.config = PERF_COUNT_SW_TASK_CLOCK;\r\nforce_again:\r\nfd = sys_perf_event_open(&attr, 0, -1, -1,\r\nperf_event_open_cloexec_flag());\r\nif (fd < 0) {\r\nif (errno == EMFILE) {\r\nif (sched->force) {\r\nBUG_ON(getrlimit(RLIMIT_NOFILE, &limit) == -1);\r\nlimit.rlim_cur += sched->nr_tasks - cur_task;\r\nif (limit.rlim_cur > limit.rlim_max) {\r\nlimit.rlim_max = limit.rlim_cur;\r\nneed_privilege = true;\r\n}\r\nif (setrlimit(RLIMIT_NOFILE, &limit) == -1) {\r\nif (need_privilege && errno == EPERM)\r\nstrcpy(info, "Need privilege\n");\r\n} else\r\ngoto force_again;\r\n} else\r\nstrcpy(info, "Have a try with -f option\n");\r\n}\r\npr_err("Error: sys_perf_event_open() syscall returned "\r\n"with %d (%s)\n%s", fd,\r\nstrerror_r(errno, sbuf, sizeof(sbuf)), info);\r\nexit(EXIT_FAILURE);\r\n}\r\nreturn fd;\r\n}\r\nstatic u64 get_cpu_usage_nsec_self(int fd)\r\n{\r\nu64 runtime;\r\nint ret;\r\nret = read(fd, &runtime, sizeof(runtime));\r\nBUG_ON(ret != sizeof(runtime));\r\nreturn runtime;\r\n}\r\nstatic void *thread_func(void *ctx)\r\n{\r\nstruct sched_thread_parms *parms = ctx;\r\nstruct task_desc *this_task = parms->task;\r\nstruct perf_sched *sched = parms->sched;\r\nu64 cpu_usage_0, cpu_usage_1;\r\nunsigned long i, ret;\r\nchar comm2[22];\r\nint fd = parms->fd;\r\nzfree(&parms);\r\nsprintf(comm2, ":%s", this_task->comm);\r\nprctl(PR_SET_NAME, comm2);\r\nif (fd < 0)\r\nreturn NULL;\r\nagain:\r\nret = sem_post(&this_task->ready_for_work);\r\nBUG_ON(ret);\r\nret = pthread_mutex_lock(&sched->start_work_mutex);\r\nBUG_ON(ret);\r\nret = pthread_mutex_unlock(&sched->start_work_mutex);\r\nBUG_ON(ret);\r\ncpu_usage_0 = get_cpu_usage_nsec_self(fd);\r\nfor (i = 0; i < this_task->nr_events; i++) {\r\nthis_task->curr_event = i;\r\nperf_sched__process_event(sched, this_task->atoms[i]);\r\n}\r\ncpu_usage_1 = get_cpu_usage_nsec_self(fd);\r\nthis_task->cpu_usage = cpu_usage_1 - cpu_usage_0;\r\nret = sem_post(&this_task->work_done_sem);\r\nBUG_ON(ret);\r\nret = pthread_mutex_lock(&sched->work_done_wait_mutex);\r\nBUG_ON(ret);\r\nret = pthread_mutex_unlock(&sched->work_done_wait_mutex);\r\nBUG_ON(ret);\r\ngoto again;\r\n}\r\nstatic void create_tasks(struct perf_sched *sched)\r\n{\r\nstruct task_desc *task;\r\npthread_attr_t attr;\r\nunsigned long i;\r\nint err;\r\nerr = pthread_attr_init(&attr);\r\nBUG_ON(err);\r\nerr = pthread_attr_setstacksize(&attr,\r\n(size_t) max(16 * 1024, PTHREAD_STACK_MIN));\r\nBUG_ON(err);\r\nerr = pthread_mutex_lock(&sched->start_work_mutex);\r\nBUG_ON(err);\r\nerr = pthread_mutex_lock(&sched->work_done_wait_mutex);\r\nBUG_ON(err);\r\nfor (i = 0; i < sched->nr_tasks; i++) {\r\nstruct sched_thread_parms *parms = malloc(sizeof(*parms));\r\nBUG_ON(parms == NULL);\r\nparms->task = task = sched->tasks[i];\r\nparms->sched = sched;\r\nparms->fd = self_open_counters(sched, i);\r\nsem_init(&task->sleep_sem, 0, 0);\r\nsem_init(&task->ready_for_work, 0, 0);\r\nsem_init(&task->work_done_sem, 0, 0);\r\ntask->curr_event = 0;\r\nerr = pthread_create(&task->thread, &attr, thread_func, parms);\r\nBUG_ON(err);\r\n}\r\n}\r\nstatic void wait_for_tasks(struct perf_sched *sched)\r\n{\r\nu64 cpu_usage_0, cpu_usage_1;\r\nstruct task_desc *task;\r\nunsigned long i, ret;\r\nsched->start_time = get_nsecs();\r\nsched->cpu_usage = 0;\r\npthread_mutex_unlock(&sched->work_done_wait_mutex);\r\nfor (i = 0; i < sched->nr_tasks; i++) {\r\ntask = sched->tasks[i];\r\nret = sem_wait(&task->ready_for_work);\r\nBUG_ON(ret);\r\nsem_init(&task->ready_for_work, 0, 0);\r\n}\r\nret = pthread_mutex_lock(&sched->work_done_wait_mutex);\r\nBUG_ON(ret);\r\ncpu_usage_0 = get_cpu_usage_nsec_parent();\r\npthread_mutex_unlock(&sched->start_work_mutex);\r\nfor (i = 0; i < sched->nr_tasks; i++) {\r\ntask = sched->tasks[i];\r\nret = sem_wait(&task->work_done_sem);\r\nBUG_ON(ret);\r\nsem_init(&task->work_done_sem, 0, 0);\r\nsched->cpu_usage += task->cpu_usage;\r\ntask->cpu_usage = 0;\r\n}\r\ncpu_usage_1 = get_cpu_usage_nsec_parent();\r\nif (!sched->runavg_cpu_usage)\r\nsched->runavg_cpu_usage = sched->cpu_usage;\r\nsched->runavg_cpu_usage = (sched->runavg_cpu_usage * (sched->replay_repeat - 1) + sched->cpu_usage) / sched->replay_repeat;\r\nsched->parent_cpu_usage = cpu_usage_1 - cpu_usage_0;\r\nif (!sched->runavg_parent_cpu_usage)\r\nsched->runavg_parent_cpu_usage = sched->parent_cpu_usage;\r\nsched->runavg_parent_cpu_usage = (sched->runavg_parent_cpu_usage * (sched->replay_repeat - 1) +\r\nsched->parent_cpu_usage)/sched->replay_repeat;\r\nret = pthread_mutex_lock(&sched->start_work_mutex);\r\nBUG_ON(ret);\r\nfor (i = 0; i < sched->nr_tasks; i++) {\r\ntask = sched->tasks[i];\r\nsem_init(&task->sleep_sem, 0, 0);\r\ntask->curr_event = 0;\r\n}\r\n}\r\nstatic void run_one_test(struct perf_sched *sched)\r\n{\r\nu64 T0, T1, delta, avg_delta, fluct;\r\nT0 = get_nsecs();\r\nwait_for_tasks(sched);\r\nT1 = get_nsecs();\r\ndelta = T1 - T0;\r\nsched->sum_runtime += delta;\r\nsched->nr_runs++;\r\navg_delta = sched->sum_runtime / sched->nr_runs;\r\nif (delta < avg_delta)\r\nfluct = avg_delta - delta;\r\nelse\r\nfluct = delta - avg_delta;\r\nsched->sum_fluct += fluct;\r\nif (!sched->run_avg)\r\nsched->run_avg = delta;\r\nsched->run_avg = (sched->run_avg * (sched->replay_repeat - 1) + delta) / sched->replay_repeat;\r\nprintf("#%-3ld: %0.3f, ", sched->nr_runs, (double)delta / 1000000.0);\r\nprintf("ravg: %0.2f, ", (double)sched->run_avg / 1e6);\r\nprintf("cpu: %0.2f / %0.2f",\r\n(double)sched->cpu_usage / 1e6, (double)sched->runavg_cpu_usage / 1e6);\r\n#if 0\r\nprintf(" [%0.2f / %0.2f]",\r\n(double)sched->parent_cpu_usage/1e6,\r\n(double)sched->runavg_parent_cpu_usage/1e6);\r\n#endif\r\nprintf("\n");\r\nif (sched->nr_sleep_corrections)\r\nprintf(" (%ld sleep corrections)\n", sched->nr_sleep_corrections);\r\nsched->nr_sleep_corrections = 0;\r\n}\r\nstatic void test_calibrations(struct perf_sched *sched)\r\n{\r\nu64 T0, T1;\r\nT0 = get_nsecs();\r\nburn_nsecs(sched, 1e6);\r\nT1 = get_nsecs();\r\nprintf("the run test took %" PRIu64 " nsecs\n", T1 - T0);\r\nT0 = get_nsecs();\r\nsleep_nsecs(1e6);\r\nT1 = get_nsecs();\r\nprintf("the sleep test took %" PRIu64 " nsecs\n", T1 - T0);\r\n}\r\nstatic int\r\nreplay_wakeup_event(struct perf_sched *sched,\r\nstruct perf_evsel *evsel, struct perf_sample *sample,\r\nstruct machine *machine __maybe_unused)\r\n{\r\nconst char *comm = perf_evsel__strval(evsel, sample, "comm");\r\nconst u32 pid = perf_evsel__intval(evsel, sample, "pid");\r\nstruct task_desc *waker, *wakee;\r\nif (verbose) {\r\nprintf("sched_wakeup event %p\n", evsel);\r\nprintf(" ... pid %d woke up %s/%d\n", sample->tid, comm, pid);\r\n}\r\nwaker = register_pid(sched, sample->tid, "<unknown>");\r\nwakee = register_pid(sched, pid, comm);\r\nadd_sched_event_wakeup(sched, waker, sample->time, wakee);\r\nreturn 0;\r\n}\r\nstatic int replay_switch_event(struct perf_sched *sched,\r\nstruct perf_evsel *evsel,\r\nstruct perf_sample *sample,\r\nstruct machine *machine __maybe_unused)\r\n{\r\nconst char *prev_comm = perf_evsel__strval(evsel, sample, "prev_comm"),\r\n*next_comm = perf_evsel__strval(evsel, sample, "next_comm");\r\nconst u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),\r\nnext_pid = perf_evsel__intval(evsel, sample, "next_pid");\r\nconst u64 prev_state = perf_evsel__intval(evsel, sample, "prev_state");\r\nstruct task_desc *prev, __maybe_unused *next;\r\nu64 timestamp0, timestamp = sample->time;\r\nint cpu = sample->cpu;\r\ns64 delta;\r\nif (verbose)\r\nprintf("sched_switch event %p\n", evsel);\r\nif (cpu >= MAX_CPUS || cpu < 0)\r\nreturn 0;\r\ntimestamp0 = sched->cpu_last_switched[cpu];\r\nif (timestamp0)\r\ndelta = timestamp - timestamp0;\r\nelse\r\ndelta = 0;\r\nif (delta < 0) {\r\npr_err("hm, delta: %" PRIu64 " < 0 ?\n", delta);\r\nreturn -1;\r\n}\r\npr_debug(" ... switch from %s/%d to %s/%d [ran %" PRIu64 " nsecs]\n",\r\nprev_comm, prev_pid, next_comm, next_pid, delta);\r\nprev = register_pid(sched, prev_pid, prev_comm);\r\nnext = register_pid(sched, next_pid, next_comm);\r\nsched->cpu_last_switched[cpu] = timestamp;\r\nadd_sched_event_run(sched, prev, timestamp, delta);\r\nadd_sched_event_sleep(sched, prev, timestamp, prev_state);\r\nreturn 0;\r\n}\r\nstatic int replay_fork_event(struct perf_sched *sched,\r\nunion perf_event *event,\r\nstruct machine *machine)\r\n{\r\nstruct thread *child, *parent;\r\nchild = machine__findnew_thread(machine, event->fork.pid,\r\nevent->fork.tid);\r\nparent = machine__findnew_thread(machine, event->fork.ppid,\r\nevent->fork.ptid);\r\nif (child == NULL || parent == NULL) {\r\npr_debug("thread does not exist on fork event: child %p, parent %p\n",\r\nchild, parent);\r\ngoto out_put;\r\n}\r\nif (verbose) {\r\nprintf("fork event\n");\r\nprintf("... parent: %s/%d\n", thread__comm_str(parent), parent->tid);\r\nprintf("... child: %s/%d\n", thread__comm_str(child), child->tid);\r\n}\r\nregister_pid(sched, parent->tid, thread__comm_str(parent));\r\nregister_pid(sched, child->tid, thread__comm_str(child));\r\nout_put:\r\nthread__put(child);\r\nthread__put(parent);\r\nreturn 0;\r\n}\r\nstatic int\r\nthread_lat_cmp(struct list_head *list, struct work_atoms *l, struct work_atoms *r)\r\n{\r\nstruct sort_dimension *sort;\r\nint ret = 0;\r\nBUG_ON(list_empty(list));\r\nlist_for_each_entry(sort, list, list) {\r\nret = sort->cmp(l, r);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn ret;\r\n}\r\nstatic struct work_atoms *\r\nthread_atoms_search(struct rb_root *root, struct thread *thread,\r\nstruct list_head *sort_list)\r\n{\r\nstruct rb_node *node = root->rb_node;\r\nstruct work_atoms key = { .thread = thread };\r\nwhile (node) {\r\nstruct work_atoms *atoms;\r\nint cmp;\r\natoms = container_of(node, struct work_atoms, node);\r\ncmp = thread_lat_cmp(sort_list, &key, atoms);\r\nif (cmp > 0)\r\nnode = node->rb_left;\r\nelse if (cmp < 0)\r\nnode = node->rb_right;\r\nelse {\r\nBUG_ON(thread != atoms->thread);\r\nreturn atoms;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic void\r\n__thread_latency_insert(struct rb_root *root, struct work_atoms *data,\r\nstruct list_head *sort_list)\r\n{\r\nstruct rb_node **new = &(root->rb_node), *parent = NULL;\r\nwhile (*new) {\r\nstruct work_atoms *this;\r\nint cmp;\r\nthis = container_of(*new, struct work_atoms, node);\r\nparent = *new;\r\ncmp = thread_lat_cmp(sort_list, data, this);\r\nif (cmp > 0)\r\nnew = &((*new)->rb_left);\r\nelse\r\nnew = &((*new)->rb_right);\r\n}\r\nrb_link_node(&data->node, parent, new);\r\nrb_insert_color(&data->node, root);\r\n}\r\nstatic int thread_atoms_insert(struct perf_sched *sched, struct thread *thread)\r\n{\r\nstruct work_atoms *atoms = zalloc(sizeof(*atoms));\r\nif (!atoms) {\r\npr_err("No memory at %s\n", __func__);\r\nreturn -1;\r\n}\r\natoms->thread = thread__get(thread);\r\nINIT_LIST_HEAD(&atoms->work_list);\r\n__thread_latency_insert(&sched->atom_root, atoms, &sched->cmp_pid);\r\nreturn 0;\r\n}\r\nstatic char sched_out_state(u64 prev_state)\r\n{\r\nconst char *str = TASK_STATE_TO_CHAR_STR;\r\nreturn str[prev_state];\r\n}\r\nstatic int\r\nadd_sched_out_event(struct work_atoms *atoms,\r\nchar run_state,\r\nu64 timestamp)\r\n{\r\nstruct work_atom *atom = zalloc(sizeof(*atom));\r\nif (!atom) {\r\npr_err("Non memory at %s", __func__);\r\nreturn -1;\r\n}\r\natom->sched_out_time = timestamp;\r\nif (run_state == 'R') {\r\natom->state = THREAD_WAIT_CPU;\r\natom->wake_up_time = atom->sched_out_time;\r\n}\r\nlist_add_tail(&atom->list, &atoms->work_list);\r\nreturn 0;\r\n}\r\nstatic void\r\nadd_runtime_event(struct work_atoms *atoms, u64 delta,\r\nu64 timestamp __maybe_unused)\r\n{\r\nstruct work_atom *atom;\r\nBUG_ON(list_empty(&atoms->work_list));\r\natom = list_entry(atoms->work_list.prev, struct work_atom, list);\r\natom->runtime += delta;\r\natoms->total_runtime += delta;\r\n}\r\nstatic void\r\nadd_sched_in_event(struct work_atoms *atoms, u64 timestamp)\r\n{\r\nstruct work_atom *atom;\r\nu64 delta;\r\nif (list_empty(&atoms->work_list))\r\nreturn;\r\natom = list_entry(atoms->work_list.prev, struct work_atom, list);\r\nif (atom->state != THREAD_WAIT_CPU)\r\nreturn;\r\nif (timestamp < atom->wake_up_time) {\r\natom->state = THREAD_IGNORE;\r\nreturn;\r\n}\r\natom->state = THREAD_SCHED_IN;\r\natom->sched_in_time = timestamp;\r\ndelta = atom->sched_in_time - atom->wake_up_time;\r\natoms->total_lat += delta;\r\nif (delta > atoms->max_lat) {\r\natoms->max_lat = delta;\r\natoms->max_lat_at = timestamp;\r\n}\r\natoms->nb_atoms++;\r\n}\r\nstatic int latency_switch_event(struct perf_sched *sched,\r\nstruct perf_evsel *evsel,\r\nstruct perf_sample *sample,\r\nstruct machine *machine)\r\n{\r\nconst u32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),\r\nnext_pid = perf_evsel__intval(evsel, sample, "next_pid");\r\nconst u64 prev_state = perf_evsel__intval(evsel, sample, "prev_state");\r\nstruct work_atoms *out_events, *in_events;\r\nstruct thread *sched_out, *sched_in;\r\nu64 timestamp0, timestamp = sample->time;\r\nint cpu = sample->cpu, err = -1;\r\ns64 delta;\r\nBUG_ON(cpu >= MAX_CPUS || cpu < 0);\r\ntimestamp0 = sched->cpu_last_switched[cpu];\r\nsched->cpu_last_switched[cpu] = timestamp;\r\nif (timestamp0)\r\ndelta = timestamp - timestamp0;\r\nelse\r\ndelta = 0;\r\nif (delta < 0) {\r\npr_err("hm, delta: %" PRIu64 " < 0 ?\n", delta);\r\nreturn -1;\r\n}\r\nsched_out = machine__findnew_thread(machine, -1, prev_pid);\r\nsched_in = machine__findnew_thread(machine, -1, next_pid);\r\nif (sched_out == NULL || sched_in == NULL)\r\ngoto out_put;\r\nout_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);\r\nif (!out_events) {\r\nif (thread_atoms_insert(sched, sched_out))\r\ngoto out_put;\r\nout_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);\r\nif (!out_events) {\r\npr_err("out-event: Internal tree error");\r\ngoto out_put;\r\n}\r\n}\r\nif (add_sched_out_event(out_events, sched_out_state(prev_state), timestamp))\r\nreturn -1;\r\nin_events = thread_atoms_search(&sched->atom_root, sched_in, &sched->cmp_pid);\r\nif (!in_events) {\r\nif (thread_atoms_insert(sched, sched_in))\r\ngoto out_put;\r\nin_events = thread_atoms_search(&sched->atom_root, sched_in, &sched->cmp_pid);\r\nif (!in_events) {\r\npr_err("in-event: Internal tree error");\r\ngoto out_put;\r\n}\r\nif (add_sched_out_event(in_events, 'R', timestamp))\r\ngoto out_put;\r\n}\r\nadd_sched_in_event(in_events, timestamp);\r\nerr = 0;\r\nout_put:\r\nthread__put(sched_out);\r\nthread__put(sched_in);\r\nreturn err;\r\n}\r\nstatic int latency_runtime_event(struct perf_sched *sched,\r\nstruct perf_evsel *evsel,\r\nstruct perf_sample *sample,\r\nstruct machine *machine)\r\n{\r\nconst u32 pid = perf_evsel__intval(evsel, sample, "pid");\r\nconst u64 runtime = perf_evsel__intval(evsel, sample, "runtime");\r\nstruct thread *thread = machine__findnew_thread(machine, -1, pid);\r\nstruct work_atoms *atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);\r\nu64 timestamp = sample->time;\r\nint cpu = sample->cpu, err = -1;\r\nif (thread == NULL)\r\nreturn -1;\r\nBUG_ON(cpu >= MAX_CPUS || cpu < 0);\r\nif (!atoms) {\r\nif (thread_atoms_insert(sched, thread))\r\ngoto out_put;\r\natoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);\r\nif (!atoms) {\r\npr_err("in-event: Internal tree error");\r\ngoto out_put;\r\n}\r\nif (add_sched_out_event(atoms, 'R', timestamp))\r\ngoto out_put;\r\n}\r\nadd_runtime_event(atoms, runtime, timestamp);\r\nerr = 0;\r\nout_put:\r\nthread__put(thread);\r\nreturn err;\r\n}\r\nstatic int latency_wakeup_event(struct perf_sched *sched,\r\nstruct perf_evsel *evsel,\r\nstruct perf_sample *sample,\r\nstruct machine *machine)\r\n{\r\nconst u32 pid = perf_evsel__intval(evsel, sample, "pid");\r\nstruct work_atoms *atoms;\r\nstruct work_atom *atom;\r\nstruct thread *wakee;\r\nu64 timestamp = sample->time;\r\nint err = -1;\r\nwakee = machine__findnew_thread(machine, -1, pid);\r\nif (wakee == NULL)\r\nreturn -1;\r\natoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);\r\nif (!atoms) {\r\nif (thread_atoms_insert(sched, wakee))\r\ngoto out_put;\r\natoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);\r\nif (!atoms) {\r\npr_err("wakeup-event: Internal tree error");\r\ngoto out_put;\r\n}\r\nif (add_sched_out_event(atoms, 'S', timestamp))\r\ngoto out_put;\r\n}\r\nBUG_ON(list_empty(&atoms->work_list));\r\natom = list_entry(atoms->work_list.prev, struct work_atom, list);\r\nif (sched->profile_cpu == -1 && atom->state != THREAD_SLEEPING)\r\ngoto out_ok;\r\nsched->nr_timestamps++;\r\nif (atom->sched_out_time > timestamp) {\r\nsched->nr_unordered_timestamps++;\r\ngoto out_ok;\r\n}\r\natom->state = THREAD_WAIT_CPU;\r\natom->wake_up_time = timestamp;\r\nout_ok:\r\nerr = 0;\r\nout_put:\r\nthread__put(wakee);\r\nreturn err;\r\n}\r\nstatic int latency_migrate_task_event(struct perf_sched *sched,\r\nstruct perf_evsel *evsel,\r\nstruct perf_sample *sample,\r\nstruct machine *machine)\r\n{\r\nconst u32 pid = perf_evsel__intval(evsel, sample, "pid");\r\nu64 timestamp = sample->time;\r\nstruct work_atoms *atoms;\r\nstruct work_atom *atom;\r\nstruct thread *migrant;\r\nint err = -1;\r\nif (sched->profile_cpu == -1)\r\nreturn 0;\r\nmigrant = machine__findnew_thread(machine, -1, pid);\r\nif (migrant == NULL)\r\nreturn -1;\r\natoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);\r\nif (!atoms) {\r\nif (thread_atoms_insert(sched, migrant))\r\ngoto out_put;\r\nregister_pid(sched, migrant->tid, thread__comm_str(migrant));\r\natoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);\r\nif (!atoms) {\r\npr_err("migration-event: Internal tree error");\r\ngoto out_put;\r\n}\r\nif (add_sched_out_event(atoms, 'R', timestamp))\r\ngoto out_put;\r\n}\r\nBUG_ON(list_empty(&atoms->work_list));\r\natom = list_entry(atoms->work_list.prev, struct work_atom, list);\r\natom->sched_in_time = atom->sched_out_time = atom->wake_up_time = timestamp;\r\nsched->nr_timestamps++;\r\nif (atom->sched_out_time > timestamp)\r\nsched->nr_unordered_timestamps++;\r\nerr = 0;\r\nout_put:\r\nthread__put(migrant);\r\nreturn err;\r\n}\r\nstatic void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_list)\r\n{\r\nint i;\r\nint ret;\r\nu64 avg;\r\nif (!work_list->nb_atoms)\r\nreturn;\r\nif (!strcmp(thread__comm_str(work_list->thread), "swapper"))\r\nreturn;\r\nsched->all_runtime += work_list->total_runtime;\r\nsched->all_count += work_list->nb_atoms;\r\nif (work_list->num_merged > 1)\r\nret = printf(" %s:(%d) ", thread__comm_str(work_list->thread), work_list->num_merged);\r\nelse\r\nret = printf(" %s:%d ", thread__comm_str(work_list->thread), work_list->thread->tid);\r\nfor (i = 0; i < 24 - ret; i++)\r\nprintf(" ");\r\navg = work_list->total_lat / work_list->nb_atoms;\r\nprintf("|%11.3f ms |%9" PRIu64 " | avg:%9.3f ms | max:%9.3f ms | max at: %13.6f s\n",\r\n(double)work_list->total_runtime / 1e6,\r\nwork_list->nb_atoms, (double)avg / 1e6,\r\n(double)work_list->max_lat / 1e6,\r\n(double)work_list->max_lat_at / 1e9);\r\n}\r\nstatic int pid_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nif (l->thread->tid < r->thread->tid)\r\nreturn -1;\r\nif (l->thread->tid > r->thread->tid)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int avg_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nu64 avgl, avgr;\r\nif (!l->nb_atoms)\r\nreturn -1;\r\nif (!r->nb_atoms)\r\nreturn 1;\r\navgl = l->total_lat / l->nb_atoms;\r\navgr = r->total_lat / r->nb_atoms;\r\nif (avgl < avgr)\r\nreturn -1;\r\nif (avgl > avgr)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int max_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nif (l->max_lat < r->max_lat)\r\nreturn -1;\r\nif (l->max_lat > r->max_lat)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int switch_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nif (l->nb_atoms < r->nb_atoms)\r\nreturn -1;\r\nif (l->nb_atoms > r->nb_atoms)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int runtime_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nif (l->total_runtime < r->total_runtime)\r\nreturn -1;\r\nif (l->total_runtime > r->total_runtime)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int sort_dimension__add(const char *tok, struct list_head *list)\r\n{\r\nsize_t i;\r\nstatic struct sort_dimension avg_sort_dimension = {\r\n.name = "avg",\r\n.cmp = avg_cmp,\r\n};\r\nstatic struct sort_dimension max_sort_dimension = {\r\n.name = "max",\r\n.cmp = max_cmp,\r\n};\r\nstatic struct sort_dimension pid_sort_dimension = {\r\n.name = "pid",\r\n.cmp = pid_cmp,\r\n};\r\nstatic struct sort_dimension runtime_sort_dimension = {\r\n.name = "runtime",\r\n.cmp = runtime_cmp,\r\n};\r\nstatic struct sort_dimension switch_sort_dimension = {\r\n.name = "switch",\r\n.cmp = switch_cmp,\r\n};\r\nstruct sort_dimension *available_sorts[] = {\r\n&pid_sort_dimension,\r\n&avg_sort_dimension,\r\n&max_sort_dimension,\r\n&switch_sort_dimension,\r\n&runtime_sort_dimension,\r\n};\r\nfor (i = 0; i < ARRAY_SIZE(available_sorts); i++) {\r\nif (!strcmp(available_sorts[i]->name, tok)) {\r\nlist_add_tail(&available_sorts[i]->list, list);\r\nreturn 0;\r\n}\r\n}\r\nreturn -1;\r\n}\r\nstatic void perf_sched__sort_lat(struct perf_sched *sched)\r\n{\r\nstruct rb_node *node;\r\nstruct rb_root *root = &sched->atom_root;\r\nagain:\r\nfor (;;) {\r\nstruct work_atoms *data;\r\nnode = rb_first(root);\r\nif (!node)\r\nbreak;\r\nrb_erase(node, root);\r\ndata = rb_entry(node, struct work_atoms, node);\r\n__thread_latency_insert(&sched->sorted_atom_root, data, &sched->sort_list);\r\n}\r\nif (root == &sched->atom_root) {\r\nroot = &sched->merged_atom_root;\r\ngoto again;\r\n}\r\n}\r\nstatic int process_sched_wakeup_event(struct perf_tool *tool,\r\nstruct perf_evsel *evsel,\r\nstruct perf_sample *sample,\r\nstruct machine *machine)\r\n{\r\nstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\r\nif (sched->tp_handler->wakeup_event)\r\nreturn sched->tp_handler->wakeup_event(sched, evsel, sample, machine);\r\nreturn 0;\r\n}\r\nstatic int map_switch_event(struct perf_sched *sched, struct perf_evsel *evsel,\r\nstruct perf_sample *sample, struct machine *machine)\r\n{\r\nconst u32 next_pid = perf_evsel__intval(evsel, sample, "next_pid");\r\nstruct thread *sched_in;\r\nint new_shortname;\r\nu64 timestamp0, timestamp = sample->time;\r\ns64 delta;\r\nint cpu, this_cpu = sample->cpu;\r\nBUG_ON(this_cpu >= MAX_CPUS || this_cpu < 0);\r\nif (this_cpu > sched->max_cpu)\r\nsched->max_cpu = this_cpu;\r\ntimestamp0 = sched->cpu_last_switched[this_cpu];\r\nsched->cpu_last_switched[this_cpu] = timestamp;\r\nif (timestamp0)\r\ndelta = timestamp - timestamp0;\r\nelse\r\ndelta = 0;\r\nif (delta < 0) {\r\npr_err("hm, delta: %" PRIu64 " < 0 ?\n", delta);\r\nreturn -1;\r\n}\r\nsched_in = machine__findnew_thread(machine, -1, next_pid);\r\nif (sched_in == NULL)\r\nreturn -1;\r\nsched->curr_thread[this_cpu] = thread__get(sched_in);\r\nprintf(" ");\r\nnew_shortname = 0;\r\nif (!sched_in->shortname[0]) {\r\nif (!strcmp(thread__comm_str(sched_in), "swapper")) {\r\nsched_in->shortname[0] = '.';\r\nsched_in->shortname[1] = ' ';\r\n} else {\r\nsched_in->shortname[0] = sched->next_shortname1;\r\nsched_in->shortname[1] = sched->next_shortname2;\r\nif (sched->next_shortname1 < 'Z') {\r\nsched->next_shortname1++;\r\n} else {\r\nsched->next_shortname1 = 'A';\r\nif (sched->next_shortname2 < '9')\r\nsched->next_shortname2++;\r\nelse\r\nsched->next_shortname2 = '0';\r\n}\r\n}\r\nnew_shortname = 1;\r\n}\r\nfor (cpu = 0; cpu <= sched->max_cpu; cpu++) {\r\nif (cpu != this_cpu)\r\nprintf(" ");\r\nelse\r\nprintf("*");\r\nif (sched->curr_thread[cpu])\r\nprintf("%2s ", sched->curr_thread[cpu]->shortname);\r\nelse\r\nprintf(" ");\r\n}\r\nprintf(" %12.6f secs ", (double)timestamp/1e9);\r\nif (new_shortname) {\r\nprintf("%s => %s:%d\n",\r\nsched_in->shortname, thread__comm_str(sched_in), sched_in->tid);\r\n} else {\r\nprintf("\n");\r\n}\r\nthread__put(sched_in);\r\nreturn 0;\r\n}\r\nstatic int process_sched_switch_event(struct perf_tool *tool,\r\nstruct perf_evsel *evsel,\r\nstruct perf_sample *sample,\r\nstruct machine *machine)\r\n{\r\nstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\r\nint this_cpu = sample->cpu, err = 0;\r\nu32 prev_pid = perf_evsel__intval(evsel, sample, "prev_pid"),\r\nnext_pid = perf_evsel__intval(evsel, sample, "next_pid");\r\nif (sched->curr_pid[this_cpu] != (u32)-1) {\r\nif (sched->curr_pid[this_cpu] != prev_pid)\r\nsched->nr_context_switch_bugs++;\r\n}\r\nif (sched->tp_handler->switch_event)\r\nerr = sched->tp_handler->switch_event(sched, evsel, sample, machine);\r\nsched->curr_pid[this_cpu] = next_pid;\r\nreturn err;\r\n}\r\nstatic int process_sched_runtime_event(struct perf_tool *tool,\r\nstruct perf_evsel *evsel,\r\nstruct perf_sample *sample,\r\nstruct machine *machine)\r\n{\r\nstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\r\nif (sched->tp_handler->runtime_event)\r\nreturn sched->tp_handler->runtime_event(sched, evsel, sample, machine);\r\nreturn 0;\r\n}\r\nstatic int perf_sched__process_fork_event(struct perf_tool *tool,\r\nunion perf_event *event,\r\nstruct perf_sample *sample,\r\nstruct machine *machine)\r\n{\r\nstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\r\nperf_event__process_fork(tool, event, sample, machine);\r\nif (sched->tp_handler->fork_event)\r\nreturn sched->tp_handler->fork_event(sched, event, machine);\r\nreturn 0;\r\n}\r\nstatic int process_sched_migrate_task_event(struct perf_tool *tool,\r\nstruct perf_evsel *evsel,\r\nstruct perf_sample *sample,\r\nstruct machine *machine)\r\n{\r\nstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\r\nif (sched->tp_handler->migrate_task_event)\r\nreturn sched->tp_handler->migrate_task_event(sched, evsel, sample, machine);\r\nreturn 0;\r\n}\r\nstatic int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_unused,\r\nunion perf_event *event __maybe_unused,\r\nstruct perf_sample *sample,\r\nstruct perf_evsel *evsel,\r\nstruct machine *machine)\r\n{\r\nint err = 0;\r\nif (evsel->handler != NULL) {\r\ntracepoint_handler f = evsel->handler;\r\nerr = f(tool, evsel, sample, machine);\r\n}\r\nreturn err;\r\n}\r\nstatic int perf_sched__read_events(struct perf_sched *sched)\r\n{\r\nconst struct perf_evsel_str_handler handlers[] = {\r\n{ "sched:sched_switch", process_sched_switch_event, },\r\n{ "sched:sched_stat_runtime", process_sched_runtime_event, },\r\n{ "sched:sched_wakeup", process_sched_wakeup_event, },\r\n{ "sched:sched_wakeup_new", process_sched_wakeup_event, },\r\n{ "sched:sched_migrate_task", process_sched_migrate_task_event, },\r\n};\r\nstruct perf_session *session;\r\nstruct perf_data_file file = {\r\n.path = input_name,\r\n.mode = PERF_DATA_MODE_READ,\r\n.force = sched->force,\r\n};\r\nint rc = -1;\r\nsession = perf_session__new(&file, false, &sched->tool);\r\nif (session == NULL) {\r\npr_debug("No Memory for session\n");\r\nreturn -1;\r\n}\r\nsymbol__init(&session->header.env);\r\nif (perf_session__set_tracepoints_handlers(session, handlers))\r\ngoto out_delete;\r\nif (perf_session__has_traces(session, "record -R")) {\r\nint err = perf_session__process_events(session);\r\nif (err) {\r\npr_err("Failed to process events, error %d", err);\r\ngoto out_delete;\r\n}\r\nsched->nr_events = session->evlist->stats.nr_events[0];\r\nsched->nr_lost_events = session->evlist->stats.total_lost;\r\nsched->nr_lost_chunks = session->evlist->stats.nr_events[PERF_RECORD_LOST];\r\n}\r\nrc = 0;\r\nout_delete:\r\nperf_session__delete(session);\r\nreturn rc;\r\n}\r\nstatic void print_bad_events(struct perf_sched *sched)\r\n{\r\nif (sched->nr_unordered_timestamps && sched->nr_timestamps) {\r\nprintf(" INFO: %.3f%% unordered timestamps (%ld out of %ld)\n",\r\n(double)sched->nr_unordered_timestamps/(double)sched->nr_timestamps*100.0,\r\nsched->nr_unordered_timestamps, sched->nr_timestamps);\r\n}\r\nif (sched->nr_lost_events && sched->nr_events) {\r\nprintf(" INFO: %.3f%% lost events (%ld out of %ld, in %ld chunks)\n",\r\n(double)sched->nr_lost_events/(double)sched->nr_events * 100.0,\r\nsched->nr_lost_events, sched->nr_events, sched->nr_lost_chunks);\r\n}\r\nif (sched->nr_context_switch_bugs && sched->nr_timestamps) {\r\nprintf(" INFO: %.3f%% context switch bugs (%ld out of %ld)",\r\n(double)sched->nr_context_switch_bugs/(double)sched->nr_timestamps*100.0,\r\nsched->nr_context_switch_bugs, sched->nr_timestamps);\r\nif (sched->nr_lost_events)\r\nprintf(" (due to lost events?)");\r\nprintf("\n");\r\n}\r\n}\r\nstatic void __merge_work_atoms(struct rb_root *root, struct work_atoms *data)\r\n{\r\nstruct rb_node **new = &(root->rb_node), *parent = NULL;\r\nstruct work_atoms *this;\r\nconst char *comm = thread__comm_str(data->thread), *this_comm;\r\nwhile (*new) {\r\nint cmp;\r\nthis = container_of(*new, struct work_atoms, node);\r\nparent = *new;\r\nthis_comm = thread__comm_str(this->thread);\r\ncmp = strcmp(comm, this_comm);\r\nif (cmp > 0) {\r\nnew = &((*new)->rb_left);\r\n} else if (cmp < 0) {\r\nnew = &((*new)->rb_right);\r\n} else {\r\nthis->num_merged++;\r\nthis->total_runtime += data->total_runtime;\r\nthis->nb_atoms += data->nb_atoms;\r\nthis->total_lat += data->total_lat;\r\nlist_splice(&data->work_list, &this->work_list);\r\nif (this->max_lat < data->max_lat) {\r\nthis->max_lat = data->max_lat;\r\nthis->max_lat_at = data->max_lat_at;\r\n}\r\nzfree(&data);\r\nreturn;\r\n}\r\n}\r\ndata->num_merged++;\r\nrb_link_node(&data->node, parent, new);\r\nrb_insert_color(&data->node, root);\r\n}\r\nstatic void perf_sched__merge_lat(struct perf_sched *sched)\r\n{\r\nstruct work_atoms *data;\r\nstruct rb_node *node;\r\nif (sched->skip_merge)\r\nreturn;\r\nwhile ((node = rb_first(&sched->atom_root))) {\r\nrb_erase(node, &sched->atom_root);\r\ndata = rb_entry(node, struct work_atoms, node);\r\n__merge_work_atoms(&sched->merged_atom_root, data);\r\n}\r\n}\r\nstatic int perf_sched__lat(struct perf_sched *sched)\r\n{\r\nstruct rb_node *next;\r\nsetup_pager();\r\nif (perf_sched__read_events(sched))\r\nreturn -1;\r\nperf_sched__merge_lat(sched);\r\nperf_sched__sort_lat(sched);\r\nprintf("\n -----------------------------------------------------------------------------------------------------------------\n");\r\nprintf(" Task | Runtime ms | Switches | Average delay ms | Maximum delay ms | Maximum delay at |\n");\r\nprintf(" -----------------------------------------------------------------------------------------------------------------\n");\r\nnext = rb_first(&sched->sorted_atom_root);\r\nwhile (next) {\r\nstruct work_atoms *work_list;\r\nwork_list = rb_entry(next, struct work_atoms, node);\r\noutput_lat_thread(sched, work_list);\r\nnext = rb_next(next);\r\nthread__zput(work_list->thread);\r\n}\r\nprintf(" -----------------------------------------------------------------------------------------------------------------\n");\r\nprintf(" TOTAL: |%11.3f ms |%9" PRIu64 " |\n",\r\n(double)sched->all_runtime / 1e6, sched->all_count);\r\nprintf(" ---------------------------------------------------\n");\r\nprint_bad_events(sched);\r\nprintf("\n");\r\nreturn 0;\r\n}\r\nstatic int perf_sched__map(struct perf_sched *sched)\r\n{\r\nsched->max_cpu = sysconf(_SC_NPROCESSORS_CONF);\r\nsetup_pager();\r\nif (perf_sched__read_events(sched))\r\nreturn -1;\r\nprint_bad_events(sched);\r\nreturn 0;\r\n}\r\nstatic int perf_sched__replay(struct perf_sched *sched)\r\n{\r\nunsigned long i;\r\ncalibrate_run_measurement_overhead(sched);\r\ncalibrate_sleep_measurement_overhead(sched);\r\ntest_calibrations(sched);\r\nif (perf_sched__read_events(sched))\r\nreturn -1;\r\nprintf("nr_run_events: %ld\n", sched->nr_run_events);\r\nprintf("nr_sleep_events: %ld\n", sched->nr_sleep_events);\r\nprintf("nr_wakeup_events: %ld\n", sched->nr_wakeup_events);\r\nif (sched->targetless_wakeups)\r\nprintf("target-less wakeups: %ld\n", sched->targetless_wakeups);\r\nif (sched->multitarget_wakeups)\r\nprintf("multi-target wakeups: %ld\n", sched->multitarget_wakeups);\r\nif (sched->nr_run_events_optimized)\r\nprintf("run atoms optimized: %ld\n",\r\nsched->nr_run_events_optimized);\r\nprint_task_traces(sched);\r\nadd_cross_task_wakeups(sched);\r\ncreate_tasks(sched);\r\nprintf("------------------------------------------------------------\n");\r\nfor (i = 0; i < sched->replay_repeat; i++)\r\nrun_one_test(sched);\r\nreturn 0;\r\n}\r\nstatic void setup_sorting(struct perf_sched *sched, const struct option *options,\r\nconst char * const usage_msg[])\r\n{\r\nchar *tmp, *tok, *str = strdup(sched->sort_order);\r\nfor (tok = strtok_r(str, ", ", &tmp);\r\ntok; tok = strtok_r(NULL, ", ", &tmp)) {\r\nif (sort_dimension__add(tok, &sched->sort_list) < 0) {\r\nerror("Unknown --sort key: `%s'", tok);\r\nusage_with_options(usage_msg, options);\r\n}\r\n}\r\nfree(str);\r\nsort_dimension__add("pid", &sched->cmp_pid);\r\n}\r\nstatic int __cmd_record(int argc, const char **argv)\r\n{\r\nunsigned int rec_argc, i, j;\r\nconst char **rec_argv;\r\nconst char * const record_args[] = {\r\n"record",\r\n"-a",\r\n"-R",\r\n"-m", "1024",\r\n"-c", "1",\r\n"-e", "sched:sched_switch",\r\n"-e", "sched:sched_stat_wait",\r\n"-e", "sched:sched_stat_sleep",\r\n"-e", "sched:sched_stat_iowait",\r\n"-e", "sched:sched_stat_runtime",\r\n"-e", "sched:sched_process_fork",\r\n"-e", "sched:sched_wakeup",\r\n"-e", "sched:sched_wakeup_new",\r\n"-e", "sched:sched_migrate_task",\r\n};\r\nrec_argc = ARRAY_SIZE(record_args) + argc - 1;\r\nrec_argv = calloc(rec_argc + 1, sizeof(char *));\r\nif (rec_argv == NULL)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < ARRAY_SIZE(record_args); i++)\r\nrec_argv[i] = strdup(record_args[i]);\r\nfor (j = 1; j < (unsigned int)argc; j++, i++)\r\nrec_argv[i] = argv[j];\r\nBUG_ON(i != rec_argc);\r\nreturn cmd_record(i, rec_argv, NULL);\r\n}\r\nint cmd_sched(int argc, const char **argv, const char *prefix __maybe_unused)\r\n{\r\nconst char default_sort_order[] = "avg, max, switch, runtime";\r\nstruct perf_sched sched = {\r\n.tool = {\r\n.sample = perf_sched__process_tracepoint_sample,\r\n.comm = perf_event__process_comm,\r\n.lost = perf_event__process_lost,\r\n.fork = perf_sched__process_fork_event,\r\n.ordered_events = true,\r\n},\r\n.cmp_pid = LIST_HEAD_INIT(sched.cmp_pid),\r\n.sort_list = LIST_HEAD_INIT(sched.sort_list),\r\n.start_work_mutex = PTHREAD_MUTEX_INITIALIZER,\r\n.work_done_wait_mutex = PTHREAD_MUTEX_INITIALIZER,\r\n.sort_order = default_sort_order,\r\n.replay_repeat = 10,\r\n.profile_cpu = -1,\r\n.next_shortname1 = 'A',\r\n.next_shortname2 = '0',\r\n.skip_merge = 0,\r\n};\r\nconst struct option latency_options[] = {\r\nOPT_STRING('s', "sort", &sched.sort_order, "key[,key2...]",\r\n"sort by key(s): runtime, switch, avg, max"),\r\nOPT_INCR('v', "verbose", &verbose,\r\n"be more verbose (show symbol address, etc)"),\r\nOPT_INTEGER('C', "CPU", &sched.profile_cpu,\r\n"CPU to profile on"),\r\nOPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,\r\n"dump raw trace in ASCII"),\r\nOPT_BOOLEAN('p', "pids", &sched.skip_merge,\r\n"latency stats per pid instead of per comm"),\r\nOPT_END()\r\n};\r\nconst struct option replay_options[] = {\r\nOPT_UINTEGER('r', "repeat", &sched.replay_repeat,\r\n"repeat the workload replay N times (-1: infinite)"),\r\nOPT_INCR('v', "verbose", &verbose,\r\n"be more verbose (show symbol address, etc)"),\r\nOPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,\r\n"dump raw trace in ASCII"),\r\nOPT_BOOLEAN('f', "force", &sched.force, "don't complain, do it"),\r\nOPT_END()\r\n};\r\nconst struct option sched_options[] = {\r\nOPT_STRING('i', "input", &input_name, "file",\r\n"input file name"),\r\nOPT_INCR('v', "verbose", &verbose,\r\n"be more verbose (show symbol address, etc)"),\r\nOPT_BOOLEAN('D', "dump-raw-trace", &dump_trace,\r\n"dump raw trace in ASCII"),\r\nOPT_END()\r\n};\r\nconst char * const latency_usage[] = {\r\n"perf sched latency [<options>]",\r\nNULL\r\n};\r\nconst char * const replay_usage[] = {\r\n"perf sched replay [<options>]",\r\nNULL\r\n};\r\nconst char *const sched_subcommands[] = { "record", "latency", "map",\r\n"replay", "script", NULL };\r\nconst char *sched_usage[] = {\r\nNULL,\r\nNULL\r\n};\r\nstruct trace_sched_handler lat_ops = {\r\n.wakeup_event = latency_wakeup_event,\r\n.switch_event = latency_switch_event,\r\n.runtime_event = latency_runtime_event,\r\n.migrate_task_event = latency_migrate_task_event,\r\n};\r\nstruct trace_sched_handler map_ops = {\r\n.switch_event = map_switch_event,\r\n};\r\nstruct trace_sched_handler replay_ops = {\r\n.wakeup_event = replay_wakeup_event,\r\n.switch_event = replay_switch_event,\r\n.fork_event = replay_fork_event,\r\n};\r\nunsigned int i;\r\nfor (i = 0; i < ARRAY_SIZE(sched.curr_pid); i++)\r\nsched.curr_pid[i] = -1;\r\nargc = parse_options_subcommand(argc, argv, sched_options, sched_subcommands,\r\nsched_usage, PARSE_OPT_STOP_AT_NON_OPTION);\r\nif (!argc)\r\nusage_with_options(sched_usage, sched_options);\r\nif (!strcmp(argv[0], "script"))\r\nreturn cmd_script(argc, argv, prefix);\r\nif (!strncmp(argv[0], "rec", 3)) {\r\nreturn __cmd_record(argc, argv);\r\n} else if (!strncmp(argv[0], "lat", 3)) {\r\nsched.tp_handler = &lat_ops;\r\nif (argc > 1) {\r\nargc = parse_options(argc, argv, latency_options, latency_usage, 0);\r\nif (argc)\r\nusage_with_options(latency_usage, latency_options);\r\n}\r\nsetup_sorting(&sched, latency_options, latency_usage);\r\nreturn perf_sched__lat(&sched);\r\n} else if (!strcmp(argv[0], "map")) {\r\nsched.tp_handler = &map_ops;\r\nsetup_sorting(&sched, latency_options, latency_usage);\r\nreturn perf_sched__map(&sched);\r\n} else if (!strncmp(argv[0], "rep", 3)) {\r\nsched.tp_handler = &replay_ops;\r\nif (argc) {\r\nargc = parse_options(argc, argv, replay_options, replay_usage, 0);\r\nif (argc)\r\nusage_with_options(replay_usage, replay_options);\r\n}\r\nreturn perf_sched__replay(&sched);\r\n} else {\r\nusage_with_options(sched_usage, sched_options);\r\n}\r\nreturn 0;\r\n}
