static inline int optimize_div(u32 *k)\r\n{\r\nif (!(*k & (*k-1))) {\r\n*k = ilog2(*k);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline bool is_range16(s32 imm)\r\n{\r\nreturn !(imm >= SBIT(15) || imm < -SBIT(15));\r\n}\r\nstatic inline void emit_addu(unsigned int dst, unsigned int src1,\r\nunsigned int src2, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, addu, dst, src1, src2);\r\n}\r\nstatic inline void emit_nop(struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, nop);\r\n}\r\nstatic inline void emit_load_imm(unsigned int dst, u32 imm, struct jit_ctx *ctx)\r\n{\r\nif (ctx->target != NULL) {\r\nif (!is_range16(imm)) {\r\nu32 *p = &ctx->target[ctx->idx];\r\nuasm_i_lui(&p, r_tmp_imm, (s32)imm >> 16);\r\np = &ctx->target[ctx->idx + 1];\r\nuasm_i_ori(&p, dst, r_tmp_imm, imm & 0xffff);\r\n} else {\r\nu32 *p = &ctx->target[ctx->idx];\r\nuasm_i_addiu(&p, dst, r_zero, imm);\r\n}\r\n}\r\nctx->idx++;\r\nif (!is_range16(imm))\r\nctx->idx++;\r\n}\r\nstatic inline void emit_or(unsigned int dst, unsigned int src1,\r\nunsigned int src2, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, or, dst, src1, src2);\r\n}\r\nstatic inline void emit_ori(unsigned int dst, unsigned src, u32 imm,\r\nstruct jit_ctx *ctx)\r\n{\r\nif (imm >= BIT(16)) {\r\nemit_load_imm(r_tmp, imm, ctx);\r\nemit_or(dst, src, r_tmp, ctx);\r\n} else {\r\nemit_instr(ctx, ori, dst, src, imm);\r\n}\r\n}\r\nstatic inline void emit_daddiu(unsigned int dst, unsigned int src,\r\nint imm, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, daddiu, dst, src, imm);\r\n}\r\nstatic inline void emit_addiu(unsigned int dst, unsigned int src,\r\nu32 imm, struct jit_ctx *ctx)\r\n{\r\nif (!is_range16(imm)) {\r\nemit_load_imm(r_tmp, imm, ctx);\r\nemit_addu(dst, r_tmp, src, ctx);\r\n} else {\r\nemit_instr(ctx, addiu, dst, src, imm);\r\n}\r\n}\r\nstatic inline void emit_and(unsigned int dst, unsigned int src1,\r\nunsigned int src2, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, and, dst, src1, src2);\r\n}\r\nstatic inline void emit_andi(unsigned int dst, unsigned int src,\r\nu32 imm, struct jit_ctx *ctx)\r\n{\r\nif (imm >= BIT(16)) {\r\nemit_load_imm(r_tmp, imm, ctx);\r\nemit_and(dst, src, r_tmp, ctx);\r\n} else {\r\nemit_instr(ctx, andi, dst, src, imm);\r\n}\r\n}\r\nstatic inline void emit_xor(unsigned int dst, unsigned int src1,\r\nunsigned int src2, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, xor, dst, src1, src2);\r\n}\r\nstatic inline void emit_xori(ptr dst, ptr src, u32 imm, struct jit_ctx *ctx)\r\n{\r\nif (imm >= BIT(16)) {\r\nemit_load_imm(r_tmp, imm, ctx);\r\nemit_xor(dst, src, r_tmp, ctx);\r\n} else {\r\nemit_instr(ctx, xori, dst, src, imm);\r\n}\r\n}\r\nstatic inline void emit_stack_offset(int offset, struct jit_ctx *ctx)\r\n{\r\nemit_long_instr(ctx, ADDIU, r_sp, r_sp, offset);\r\n}\r\nstatic inline void emit_subu(unsigned int dst, unsigned int src1,\r\nunsigned int src2, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, subu, dst, src1, src2);\r\n}\r\nstatic inline void emit_neg(unsigned int reg, struct jit_ctx *ctx)\r\n{\r\nemit_subu(reg, r_zero, reg, ctx);\r\n}\r\nstatic inline void emit_sllv(unsigned int dst, unsigned int src,\r\nunsigned int sa, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, sllv, dst, src, sa);\r\n}\r\nstatic inline void emit_sll(unsigned int dst, unsigned int src,\r\nunsigned int sa, struct jit_ctx *ctx)\r\n{\r\nif (sa >= BIT(5))\r\nemit_jit_reg_move(dst, r_zero, ctx);\r\nelse\r\nemit_instr(ctx, sll, dst, src, sa);\r\n}\r\nstatic inline void emit_srlv(unsigned int dst, unsigned int src,\r\nunsigned int sa, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, srlv, dst, src, sa);\r\n}\r\nstatic inline void emit_srl(unsigned int dst, unsigned int src,\r\nunsigned int sa, struct jit_ctx *ctx)\r\n{\r\nif (sa >= BIT(5))\r\nemit_jit_reg_move(dst, r_zero, ctx);\r\nelse\r\nemit_instr(ctx, srl, dst, src, sa);\r\n}\r\nstatic inline void emit_slt(unsigned int dst, unsigned int src1,\r\nunsigned int src2, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, slt, dst, src1, src2);\r\n}\r\nstatic inline void emit_sltu(unsigned int dst, unsigned int src1,\r\nunsigned int src2, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, sltu, dst, src1, src2);\r\n}\r\nstatic inline void emit_sltiu(unsigned dst, unsigned int src,\r\nunsigned int imm, struct jit_ctx *ctx)\r\n{\r\nif (!is_range16((s32)imm)) {\r\nemit_load_imm(r_tmp, imm, ctx);\r\nemit_sltu(dst, src, r_tmp, ctx);\r\n} else {\r\nemit_instr(ctx, sltiu, dst, src, imm);\r\n}\r\n}\r\nstatic inline void emit_store_stack_reg(ptr reg, ptr base,\r\nunsigned int offset,\r\nstruct jit_ctx *ctx)\r\n{\r\nemit_long_instr(ctx, SW, reg, offset, base);\r\n}\r\nstatic inline void emit_store(ptr reg, ptr base, unsigned int offset,\r\nstruct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, sw, reg, offset, base);\r\n}\r\nstatic inline void emit_load_stack_reg(ptr reg, ptr base,\r\nunsigned int offset,\r\nstruct jit_ctx *ctx)\r\n{\r\nemit_long_instr(ctx, LW, reg, offset, base);\r\n}\r\nstatic inline void emit_load(unsigned int reg, unsigned int base,\r\nunsigned int offset, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, lw, reg, offset, base);\r\n}\r\nstatic inline void emit_load_byte(unsigned int reg, unsigned int base,\r\nunsigned int offset, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, lb, reg, offset, base);\r\n}\r\nstatic inline void emit_half_load(unsigned int reg, unsigned int base,\r\nunsigned int offset, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, lh, reg, offset, base);\r\n}\r\nstatic inline void emit_mul(unsigned int dst, unsigned int src1,\r\nunsigned int src2, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, mul, dst, src1, src2);\r\n}\r\nstatic inline void emit_div(unsigned int dst, unsigned int src,\r\nstruct jit_ctx *ctx)\r\n{\r\nif (ctx->target != NULL) {\r\nu32 *p = &ctx->target[ctx->idx];\r\nuasm_i_divu(&p, dst, src);\r\np = &ctx->target[ctx->idx + 1];\r\nuasm_i_mflo(&p, dst);\r\n}\r\nctx->idx += 2;\r\n}\r\nstatic inline void emit_mod(unsigned int dst, unsigned int src,\r\nstruct jit_ctx *ctx)\r\n{\r\nif (ctx->target != NULL) {\r\nu32 *p = &ctx->target[ctx->idx];\r\nuasm_i_divu(&p, dst, src);\r\np = &ctx->target[ctx->idx + 1];\r\nuasm_i_mfhi(&p, dst);\r\n}\r\nctx->idx += 2;\r\n}\r\nstatic inline void emit_dsll(unsigned int dst, unsigned int src,\r\nunsigned int sa, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, dsll, dst, src, sa);\r\n}\r\nstatic inline void emit_dsrl32(unsigned int dst, unsigned int src,\r\nunsigned int sa, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, dsrl32, dst, src, sa);\r\n}\r\nstatic inline void emit_wsbh(unsigned int dst, unsigned int src,\r\nstruct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, wsbh, dst, src);\r\n}\r\nstatic inline void emit_load_ptr(unsigned int dst, unsigned int src,\r\nint imm, struct jit_ctx *ctx)\r\n{\r\nemit_long_instr(ctx, LW, dst, imm, src);\r\n}\r\nstatic inline void emit_load_func(unsigned int reg, ptr imm,\r\nstruct jit_ctx *ctx)\r\n{\r\nif (config_enabled(CONFIG_64BIT)) {\r\nemit_load_imm(r_tmp, (u64)imm >> 32, ctx);\r\nemit_dsll(r_tmp_imm, r_tmp, 16, ctx);\r\nemit_ori(r_tmp, r_tmp_imm, (imm >> 16) & 0xffff, ctx);\r\nemit_dsll(r_tmp_imm, r_tmp, 16, ctx);\r\nemit_ori(reg, r_tmp_imm, imm & 0xffff, ctx);\r\n} else {\r\nemit_load_imm(reg, imm, ctx);\r\n}\r\n}\r\nstatic inline void emit_reg_move(ptr dst, ptr src, struct jit_ctx *ctx)\r\n{\r\nemit_long_instr(ctx, ADDU, dst, src, r_zero);\r\n}\r\nstatic inline void emit_jit_reg_move(ptr dst, ptr src, struct jit_ctx *ctx)\r\n{\r\nemit_addu(dst, src, r_zero, ctx);\r\n}\r\nstatic inline u32 b_imm(unsigned int tgt, struct jit_ctx *ctx)\r\n{\r\nif (ctx->target == NULL)\r\nreturn 0;\r\nreturn ctx->offsets[tgt] -\r\n(ctx->idx * 4 - ctx->prologue_bytes) - 4;\r\n}\r\nstatic inline void emit_bcond(int cond, unsigned int reg1, unsigned int reg2,\r\nunsigned int imm, struct jit_ctx *ctx)\r\n{\r\nif (ctx->target != NULL) {\r\nu32 *p = &ctx->target[ctx->idx];\r\nswitch (cond) {\r\ncase MIPS_COND_EQ:\r\nuasm_i_beq(&p, reg1, reg2, imm);\r\nbreak;\r\ncase MIPS_COND_NE:\r\nuasm_i_bne(&p, reg1, reg2, imm);\r\nbreak;\r\ncase MIPS_COND_ALL:\r\nuasm_i_b(&p, imm);\r\nbreak;\r\ndefault:\r\npr_warn("%s: Unhandled branch conditional: %d\n",\r\n__func__, cond);\r\n}\r\n}\r\nctx->idx++;\r\n}\r\nstatic inline void emit_b(unsigned int imm, struct jit_ctx *ctx)\r\n{\r\nemit_bcond(MIPS_COND_ALL, r_zero, r_zero, imm, ctx);\r\n}\r\nstatic inline void emit_jalr(unsigned int link, unsigned int reg,\r\nstruct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, jalr, link, reg);\r\n}\r\nstatic inline void emit_jr(unsigned int reg, struct jit_ctx *ctx)\r\n{\r\nemit_instr(ctx, jr, reg);\r\n}\r\nstatic inline u16 align_sp(unsigned int num)\r\n{\r\nunsigned int align = config_enabled(CONFIG_64BIT) ? 16 : 8;\r\nnum = (num + (align - 1)) & -align;\r\nreturn num;\r\n}\r\nstatic bool is_load_to_a(u16 inst)\r\n{\r\nswitch (inst) {\r\ncase BPF_LD | BPF_W | BPF_LEN:\r\ncase BPF_LD | BPF_W | BPF_ABS:\r\ncase BPF_LD | BPF_H | BPF_ABS:\r\ncase BPF_LD | BPF_B | BPF_ABS:\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic void save_bpf_jit_regs(struct jit_ctx *ctx, unsigned offset)\r\n{\r\nint i = 0, real_off = 0;\r\nu32 sflags, tmp_flags;\r\nemit_stack_offset(-align_sp(offset), ctx);\r\ntmp_flags = sflags = ctx->flags >> SEEN_SREG_SFT;\r\nwhile (tmp_flags) {\r\nif ((sflags >> i) & 0x1) {\r\nemit_store_stack_reg(MIPS_R_S0 + i, r_sp, real_off,\r\nctx);\r\nreal_off += SZREG;\r\n}\r\ni++;\r\ntmp_flags >>= 1;\r\n}\r\nif (ctx->flags & SEEN_CALL) {\r\nemit_store_stack_reg(r_ra, r_sp, real_off, ctx);\r\nreal_off += SZREG;\r\n}\r\nif (ctx->flags & SEEN_MEM) {\r\nif (real_off % (SZREG * 2))\r\nreal_off += SZREG;\r\nemit_long_instr(ctx, ADDIU, r_M, r_sp, real_off);\r\n}\r\n}\r\nstatic void restore_bpf_jit_regs(struct jit_ctx *ctx,\r\nunsigned int offset)\r\n{\r\nint i, real_off = 0;\r\nu32 sflags, tmp_flags;\r\ntmp_flags = sflags = ctx->flags >> SEEN_SREG_SFT;\r\ni = 0;\r\nwhile (tmp_flags) {\r\nif ((sflags >> i) & 0x1) {\r\nemit_load_stack_reg(MIPS_R_S0 + i, r_sp, real_off,\r\nctx);\r\nreal_off += SZREG;\r\n}\r\ni++;\r\ntmp_flags >>= 1;\r\n}\r\nif (ctx->flags & SEEN_CALL)\r\nemit_load_stack_reg(r_ra, r_sp, real_off, ctx);\r\nemit_stack_offset(align_sp(offset), ctx);\r\n}\r\nstatic unsigned int get_stack_depth(struct jit_ctx *ctx)\r\n{\r\nint sp_off = 0;\r\nsp_off += hweight32(ctx->flags >> SEEN_SREG_SFT) * SZREG;\r\nif (ctx->flags & SEEN_MEM)\r\nsp_off += 4 * BPF_MEMWORDS;\r\nif (ctx->flags & SEEN_CALL)\r\nsp_off += SZREG;\r\nreturn sp_off;\r\n}\r\nstatic void build_prologue(struct jit_ctx *ctx)\r\n{\r\nu16 first_inst = ctx->skf->insns[0].code;\r\nint sp_off;\r\nsp_off = get_stack_depth(ctx);\r\nsave_bpf_jit_regs(ctx, sp_off);\r\nif (ctx->flags & SEEN_SKB)\r\nemit_reg_move(r_skb, MIPS_R_A0, ctx);\r\nif (ctx->flags & SEEN_SKB_DATA) {\r\nemit_load(r_skb_len, r_skb, offsetof(struct sk_buff, len),\r\nctx);\r\nemit_load(r_tmp, r_skb, offsetof(struct sk_buff, data_len),\r\nctx);\r\nemit_load_ptr(r_skb_data, r_skb,\r\noffsetof(struct sk_buff, data), ctx);\r\nemit_subu(r_skb_hl, r_skb_len, r_tmp, ctx);\r\n}\r\nif (ctx->flags & SEEN_X)\r\nemit_jit_reg_move(r_X, r_zero, ctx);\r\nif ((first_inst != (BPF_RET | BPF_K)) && !(is_load_to_a(first_inst)))\r\nemit_jit_reg_move(r_A, r_zero, ctx);\r\n}\r\nstatic void build_epilogue(struct jit_ctx *ctx)\r\n{\r\nunsigned int sp_off;\r\nsp_off = get_stack_depth(ctx);\r\nrestore_bpf_jit_regs(ctx, sp_off);\r\nemit_jr(r_ra, ctx);\r\nemit_nop(ctx);\r\n}\r\nstatic int build_body(struct jit_ctx *ctx)\r\n{\r\nconst struct bpf_prog *prog = ctx->skf;\r\nconst struct sock_filter *inst;\r\nunsigned int i, off, condt;\r\nu32 k, b_off __maybe_unused;\r\nu8 (*sk_load_func)(unsigned long *skb, int offset);\r\nfor (i = 0; i < prog->len; i++) {\r\nu16 code;\r\ninst = &(prog->insns[i]);\r\npr_debug("%s: code->0x%02x, jt->0x%x, jf->0x%x, k->0x%x\n",\r\n__func__, inst->code, inst->jt, inst->jf, inst->k);\r\nk = inst->k;\r\ncode = bpf_anc_helper(inst);\r\nif (ctx->target == NULL)\r\nctx->offsets[i] = ctx->idx * 4;\r\nswitch (code) {\r\ncase BPF_LD | BPF_IMM:\r\nctx->flags |= SEEN_A;\r\nemit_load_imm(r_A, k, ctx);\r\nbreak;\r\ncase BPF_LD | BPF_W | BPF_LEN:\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, len) != 4);\r\nctx->flags |= SEEN_SKB | SEEN_A;\r\noff = offsetof(struct sk_buff, len);\r\nemit_load(r_A, r_skb, off, ctx);\r\nbreak;\r\ncase BPF_LD | BPF_MEM:\r\nctx->flags |= SEEN_MEM | SEEN_A;\r\nemit_load(r_A, r_M, SCRATCH_OFF(k), ctx);\r\nbreak;\r\ncase BPF_LD | BPF_W | BPF_ABS:\r\nsk_load_func = CHOOSE_LOAD_FUNC(k, sk_load_word);\r\ngoto load;\r\ncase BPF_LD | BPF_H | BPF_ABS:\r\nsk_load_func = CHOOSE_LOAD_FUNC(k, sk_load_half);\r\ngoto load;\r\ncase BPF_LD | BPF_B | BPF_ABS:\r\nsk_load_func = CHOOSE_LOAD_FUNC(k, sk_load_byte);\r\nload:\r\nemit_load_imm(r_off, k, ctx);\r\nload_common:\r\nctx->flags |= SEEN_CALL | SEEN_OFF |\r\nSEEN_SKB | SEEN_A | SEEN_SKB_DATA;\r\nemit_load_func(r_s0, (ptr)sk_load_func, ctx);\r\nemit_reg_move(MIPS_R_A0, r_skb, ctx);\r\nemit_jalr(MIPS_R_RA, r_s0, ctx);\r\nemit_reg_move(MIPS_R_A1, r_off, ctx);\r\nemit_bcond(MIPS_COND_EQ, r_ret, 0, b_imm(i + 1, ctx),\r\nctx);\r\nemit_reg_move(r_ret, r_zero, ctx);\r\nemit_b(b_imm(prog->len, ctx), ctx);\r\nemit_nop(ctx);\r\nbreak;\r\ncase BPF_LD | BPF_W | BPF_IND:\r\nsk_load_func = sk_load_word;\r\ngoto load_ind;\r\ncase BPF_LD | BPF_H | BPF_IND:\r\nsk_load_func = sk_load_half;\r\ngoto load_ind;\r\ncase BPF_LD | BPF_B | BPF_IND:\r\nsk_load_func = sk_load_byte;\r\nload_ind:\r\nctx->flags |= SEEN_OFF | SEEN_X;\r\nemit_addiu(r_off, r_X, k, ctx);\r\ngoto load_common;\r\ncase BPF_LDX | BPF_IMM:\r\nctx->flags |= SEEN_X;\r\nemit_load_imm(r_X, k, ctx);\r\nbreak;\r\ncase BPF_LDX | BPF_MEM:\r\nctx->flags |= SEEN_X | SEEN_MEM;\r\nemit_load(r_X, r_M, SCRATCH_OFF(k), ctx);\r\nbreak;\r\ncase BPF_LDX | BPF_W | BPF_LEN:\r\nctx->flags |= SEEN_X | SEEN_SKB;\r\noff = offsetof(struct sk_buff, len);\r\nemit_load(r_X, r_skb, off, ctx);\r\nbreak;\r\ncase BPF_LDX | BPF_B | BPF_MSH:\r\nctx->flags |= SEEN_X | SEEN_CALL | SEEN_SKB;\r\nemit_load_func(r_s0, (ptr)sk_load_byte, ctx);\r\nemit_load_imm(MIPS_R_A1, k, ctx);\r\nemit_jalr(MIPS_R_RA, r_s0, ctx);\r\nemit_reg_move(MIPS_R_A0, r_skb, ctx);\r\nemit_bcond(MIPS_COND_NE, r_ret, 0,\r\nb_imm(prog->len, ctx), ctx);\r\nemit_reg_move(r_ret, r_zero, ctx);\r\nemit_andi(r_X, r_A, 0xf, ctx);\r\nemit_b(b_imm(i + 1, ctx), ctx);\r\nemit_sll(r_X, r_X, 2, ctx);\r\nbreak;\r\ncase BPF_ST:\r\nctx->flags |= SEEN_MEM | SEEN_A;\r\nemit_store(r_A, r_M, SCRATCH_OFF(k), ctx);\r\nbreak;\r\ncase BPF_STX:\r\nctx->flags |= SEEN_MEM | SEEN_X;\r\nemit_store(r_X, r_M, SCRATCH_OFF(k), ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_ADD | BPF_K:\r\nctx->flags |= SEEN_A;\r\nemit_addiu(r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_ADD | BPF_X:\r\nctx->flags |= SEEN_A | SEEN_X;\r\nemit_addu(r_A, r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_SUB | BPF_K:\r\nctx->flags |= SEEN_A;\r\nemit_addiu(r_A, r_A, -k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_SUB | BPF_X:\r\nctx->flags |= SEEN_A | SEEN_X;\r\nemit_subu(r_A, r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_MUL | BPF_K:\r\nctx->flags |= SEEN_A;\r\nemit_load_imm(r_s0, k, ctx);\r\nemit_mul(r_A, r_A, r_s0, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_MUL | BPF_X:\r\nctx->flags |= SEEN_A | SEEN_X;\r\nemit_mul(r_A, r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_DIV | BPF_K:\r\nif (k == 1)\r\nbreak;\r\nif (optimize_div(&k)) {\r\nctx->flags |= SEEN_A;\r\nemit_srl(r_A, r_A, k, ctx);\r\nbreak;\r\n}\r\nctx->flags |= SEEN_A;\r\nemit_load_imm(r_s0, k, ctx);\r\nemit_div(r_A, r_s0, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_MOD | BPF_K:\r\nif (k == 1) {\r\nctx->flags |= SEEN_A;\r\nemit_jit_reg_move(r_A, r_zero, ctx);\r\n} else {\r\nctx->flags |= SEEN_A;\r\nemit_load_imm(r_s0, k, ctx);\r\nemit_mod(r_A, r_s0, ctx);\r\n}\r\nbreak;\r\ncase BPF_ALU | BPF_DIV | BPF_X:\r\nctx->flags |= SEEN_X | SEEN_A;\r\nemit_bcond(MIPS_COND_EQ, r_X, r_zero,\r\nb_imm(prog->len, ctx), ctx);\r\nemit_load_imm(r_ret, 0, ctx);\r\nemit_div(r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_MOD | BPF_X:\r\nctx->flags |= SEEN_X | SEEN_A;\r\nemit_bcond(MIPS_COND_EQ, r_X, r_zero,\r\nb_imm(prog->len, ctx), ctx);\r\nemit_load_imm(r_ret, 0, ctx);\r\nemit_mod(r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_OR | BPF_K:\r\nctx->flags |= SEEN_A;\r\nemit_ori(r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_OR | BPF_X:\r\nctx->flags |= SEEN_A;\r\nemit_ori(r_A, r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_XOR | BPF_K:\r\nctx->flags |= SEEN_A;\r\nemit_xori(r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_ALU_XOR_X:\r\ncase BPF_ALU | BPF_XOR | BPF_X:\r\nctx->flags |= SEEN_A;\r\nemit_xor(r_A, r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_AND | BPF_K:\r\nctx->flags |= SEEN_A;\r\nemit_andi(r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_AND | BPF_X:\r\nctx->flags |= SEEN_A | SEEN_X;\r\nemit_and(r_A, r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_LSH | BPF_K:\r\nctx->flags |= SEEN_A;\r\nemit_sll(r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_LSH | BPF_X:\r\nctx->flags |= SEEN_A | SEEN_X;\r\nemit_sllv(r_A, r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_RSH | BPF_K:\r\nctx->flags |= SEEN_A;\r\nemit_srl(r_A, r_A, k, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_RSH | BPF_X:\r\nctx->flags |= SEEN_A | SEEN_X;\r\nemit_srlv(r_A, r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ALU | BPF_NEG:\r\nctx->flags |= SEEN_A;\r\nemit_neg(r_A, ctx);\r\nbreak;\r\ncase BPF_JMP | BPF_JA:\r\nemit_b(b_imm(i + k + 1, ctx), ctx);\r\nemit_nop(ctx);\r\nbreak;\r\ncase BPF_JMP | BPF_JEQ | BPF_K:\r\ncondt = MIPS_COND_EQ | MIPS_COND_K;\r\ngoto jmp_cmp;\r\ncase BPF_JMP | BPF_JEQ | BPF_X:\r\nctx->flags |= SEEN_X;\r\ncondt = MIPS_COND_EQ | MIPS_COND_X;\r\ngoto jmp_cmp;\r\ncase BPF_JMP | BPF_JGE | BPF_K:\r\ncondt = MIPS_COND_GE | MIPS_COND_K;\r\ngoto jmp_cmp;\r\ncase BPF_JMP | BPF_JGE | BPF_X:\r\nctx->flags |= SEEN_X;\r\ncondt = MIPS_COND_GE | MIPS_COND_X;\r\ngoto jmp_cmp;\r\ncase BPF_JMP | BPF_JGT | BPF_K:\r\ncondt = MIPS_COND_GT | MIPS_COND_K;\r\ngoto jmp_cmp;\r\ncase BPF_JMP | BPF_JGT | BPF_X:\r\nctx->flags |= SEEN_X;\r\ncondt = MIPS_COND_GT | MIPS_COND_X;\r\njmp_cmp:\r\nif ((condt & MIPS_COND_GE) ||\r\n(condt & MIPS_COND_GT)) {\r\nif (condt & MIPS_COND_K) {\r\nctx->flags |= SEEN_A;\r\nemit_sltiu(r_s0, r_A, k, ctx);\r\n} else {\r\nctx->flags |= SEEN_A |\r\nSEEN_X;\r\nemit_sltu(r_s0, r_A, r_X, ctx);\r\n}\r\nb_off = b_imm(i + inst->jf + 1, ctx);\r\nemit_bcond(MIPS_COND_NE, r_s0, r_zero, b_off,\r\nctx);\r\nemit_nop(ctx);\r\nif (condt & MIPS_COND_GT) {\r\nctx->flags |= SEEN_A | SEEN_X;\r\nif (condt & MIPS_COND_K)\r\nemit_load_imm(r_s0, k, ctx);\r\nelse\r\nemit_jit_reg_move(r_s0, r_X,\r\nctx);\r\nb_off = b_imm(i + inst->jf + 1, ctx);\r\nemit_bcond(MIPS_COND_EQ, r_A, r_s0,\r\nb_off, ctx);\r\nemit_nop(ctx);\r\nb_off = b_imm(i + inst->jt + 1, ctx);\r\nemit_b(b_off, ctx);\r\nemit_nop(ctx);\r\n} else {\r\nb_off = b_imm(i + inst->jt + 1, ctx);\r\nemit_b(b_off, ctx);\r\nemit_nop(ctx);\r\n}\r\n} else {\r\nif (condt & MIPS_COND_K) {\r\nctx->flags |= SEEN_A;\r\nemit_load_imm(r_s0, k, ctx);\r\nb_off = b_imm(i + inst->jt + 1, ctx);\r\nemit_bcond(MIPS_COND_EQ, r_A, r_s0,\r\nb_off, ctx);\r\nemit_nop(ctx);\r\nb_off = b_imm(i + inst->jf + 1,\r\nctx);\r\nemit_bcond(MIPS_COND_NE, r_A, r_s0,\r\nb_off, ctx);\r\nemit_nop(ctx);\r\n} else {\r\nctx->flags |= SEEN_A | SEEN_X;\r\nb_off = b_imm(i + inst->jt + 1,\r\nctx);\r\nemit_bcond(MIPS_COND_EQ, r_A, r_X,\r\nb_off, ctx);\r\nemit_nop(ctx);\r\nb_off = b_imm(i + inst->jf + 1, ctx);\r\nemit_bcond(MIPS_COND_NE, r_A, r_X,\r\nb_off, ctx);\r\nemit_nop(ctx);\r\n}\r\n}\r\nbreak;\r\ncase BPF_JMP | BPF_JSET | BPF_K:\r\nctx->flags |= SEEN_A;\r\nemit_load_imm(r_s1, k, ctx);\r\nemit_and(r_s0, r_A, r_s1, ctx);\r\nb_off = b_imm(i + inst->jt + 1, ctx);\r\nemit_bcond(MIPS_COND_NE, r_s0, r_zero, b_off, ctx);\r\nemit_nop(ctx);\r\nb_off = b_imm(i + inst->jf + 1, ctx);\r\nemit_b(b_off, ctx);\r\nemit_nop(ctx);\r\nbreak;\r\ncase BPF_JMP | BPF_JSET | BPF_X:\r\nctx->flags |= SEEN_X | SEEN_A;\r\nemit_and(r_s0, r_A, r_X, ctx);\r\nb_off = b_imm(i + inst->jt + 1, ctx);\r\nemit_bcond(MIPS_COND_NE, r_s0, r_zero, b_off, ctx);\r\nemit_nop(ctx);\r\nb_off = b_imm(i + inst->jf + 1, ctx);\r\nemit_b(b_off, ctx);\r\nemit_nop(ctx);\r\nbreak;\r\ncase BPF_RET | BPF_A:\r\nctx->flags |= SEEN_A;\r\nif (i != prog->len - 1)\r\nemit_b(b_imm(prog->len, ctx), ctx);\r\nemit_reg_move(r_ret, r_A, ctx);\r\nbreak;\r\ncase BPF_RET | BPF_K:\r\nemit_load_imm(r_ret, k, ctx);\r\nif (i != prog->len - 1) {\r\nemit_b(b_imm(prog->len, ctx), ctx);\r\nemit_nop(ctx);\r\n}\r\nbreak;\r\ncase BPF_MISC | BPF_TAX:\r\nctx->flags |= SEEN_X | SEEN_A;\r\nemit_jit_reg_move(r_X, r_A, ctx);\r\nbreak;\r\ncase BPF_MISC | BPF_TXA:\r\nctx->flags |= SEEN_A | SEEN_X;\r\nemit_jit_reg_move(r_A, r_X, ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_PROTOCOL:\r\nctx->flags |= SEEN_SKB | SEEN_OFF | SEEN_A;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff,\r\nprotocol) != 2);\r\noff = offsetof(struct sk_buff, protocol);\r\nemit_half_load(r_A, r_skb, off, ctx);\r\n#ifdef CONFIG_CPU_LITTLE_ENDIAN\r\nif (cpu_has_wsbh) {\r\nemit_wsbh(r_A, r_A, ctx);\r\n} else {\r\nemit_andi(r_tmp_imm, r_A, 0xff, ctx);\r\nemit_sll(r_tmp, r_tmp_imm, 8, ctx);\r\nemit_srl(r_tmp_imm, r_A, 8, ctx);\r\nemit_andi(r_tmp_imm, r_tmp_imm, 0xff, ctx);\r\nemit_or(r_A, r_tmp, r_tmp_imm, ctx);\r\n}\r\n#endif\r\nbreak;\r\ncase BPF_ANC | SKF_AD_CPU:\r\nctx->flags |= SEEN_A | SEEN_OFF;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct thread_info,\r\ncpu) != 4);\r\noff = offsetof(struct thread_info, cpu);\r\nemit_load(r_A, 28, off, ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_IFINDEX:\r\nctx->flags |= SEEN_SKB | SEEN_A;\r\noff = offsetof(struct sk_buff, dev);\r\nemit_load_ptr(r_s0, r_skb, off, ctx);\r\nemit_bcond(MIPS_COND_EQ, r_s0, r_zero,\r\nb_imm(prog->len, ctx), ctx);\r\nemit_reg_move(r_ret, r_zero, ctx);\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct net_device,\r\nifindex) != 4);\r\noff = offsetof(struct net_device, ifindex);\r\nemit_load(r_A, r_s0, off, ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_MARK:\r\nctx->flags |= SEEN_SKB | SEEN_A;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);\r\noff = offsetof(struct sk_buff, mark);\r\nemit_load(r_A, r_skb, off, ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_RXHASH:\r\nctx->flags |= SEEN_SKB | SEEN_A;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);\r\noff = offsetof(struct sk_buff, hash);\r\nemit_load(r_A, r_skb, off, ctx);\r\nbreak;\r\ncase BPF_ANC | SKF_AD_VLAN_TAG:\r\ncase BPF_ANC | SKF_AD_VLAN_TAG_PRESENT:\r\nctx->flags |= SEEN_SKB | SEEN_A;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff,\r\nvlan_tci) != 2);\r\noff = offsetof(struct sk_buff, vlan_tci);\r\nemit_half_load(r_s0, r_skb, off, ctx);\r\nif (code == (BPF_ANC | SKF_AD_VLAN_TAG)) {\r\nemit_andi(r_A, r_s0, (u16)~VLAN_TAG_PRESENT, ctx);\r\n} else {\r\nemit_andi(r_A, r_s0, VLAN_TAG_PRESENT, ctx);\r\nemit_sltu(r_A, r_zero, r_A, ctx);\r\n}\r\nbreak;\r\ncase BPF_ANC | SKF_AD_PKTTYPE:\r\nctx->flags |= SEEN_SKB;\r\nemit_load_byte(r_tmp, r_skb, PKT_TYPE_OFFSET(), ctx);\r\nemit_andi(r_A, r_tmp, PKT_TYPE_MAX, ctx);\r\n#ifdef __BIG_ENDIAN_BITFIELD\r\nemit_srl(r_A, r_A, 5, ctx);\r\n#endif\r\nbreak;\r\ncase BPF_ANC | SKF_AD_QUEUE:\r\nctx->flags |= SEEN_SKB | SEEN_A;\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff,\r\nqueue_mapping) != 2);\r\nBUILD_BUG_ON(offsetof(struct sk_buff,\r\nqueue_mapping) > 0xff);\r\noff = offsetof(struct sk_buff, queue_mapping);\r\nemit_half_load(r_A, r_skb, off, ctx);\r\nbreak;\r\ndefault:\r\npr_debug("%s: Unhandled opcode: 0x%02x\n", __FILE__,\r\ninst->code);\r\nreturn -1;\r\n}\r\n}\r\nif (ctx->target == NULL)\r\nctx->offsets[i] = ctx->idx * 4;\r\nreturn 0;\r\n}\r\nvoid bpf_jit_compile(struct bpf_prog *fp)\r\n{\r\nstruct jit_ctx ctx;\r\nunsigned int alloc_size, tmp_idx;\r\nif (!bpf_jit_enable)\r\nreturn;\r\nmemset(&ctx, 0, sizeof(ctx));\r\nctx.offsets = kcalloc(fp->len, sizeof(*ctx.offsets), GFP_KERNEL);\r\nif (ctx.offsets == NULL)\r\nreturn;\r\nctx.skf = fp;\r\nif (build_body(&ctx))\r\ngoto out;\r\ntmp_idx = ctx.idx;\r\nbuild_prologue(&ctx);\r\nctx.prologue_bytes = (ctx.idx - tmp_idx) * 4;\r\nbuild_epilogue(&ctx);\r\nalloc_size = 4 * ctx.idx;\r\nctx.target = module_alloc(alloc_size);\r\nif (ctx.target == NULL)\r\ngoto out;\r\nmemset(ctx.target, 0, alloc_size);\r\nctx.idx = 0;\r\nbuild_prologue(&ctx);\r\nbuild_body(&ctx);\r\nbuild_epilogue(&ctx);\r\nflush_icache_range((ptr)ctx.target, (ptr)(ctx.target + ctx.idx));\r\nif (bpf_jit_enable > 1)\r\nbpf_jit_dump(fp->len, alloc_size, 2, ctx.target);\r\nfp->bpf_func = (void *)ctx.target;\r\nfp->jited = true;\r\nout:\r\nkfree(ctx.offsets);\r\n}\r\nvoid bpf_jit_free(struct bpf_prog *fp)\r\n{\r\nif (fp->jited)\r\nmodule_memfree(fp->bpf_func);\r\nbpf_prog_unlock_free(fp);\r\n}
