static inline u32 sel_netif_hashfn(const struct net *ns, int ifindex)\r\n{\r\nreturn (((uintptr_t)ns + ifindex) & (SEL_NETIF_HASH_SIZE - 1));\r\n}\r\nstatic inline struct sel_netif *sel_netif_find(const struct net *ns,\r\nint ifindex)\r\n{\r\nint idx = sel_netif_hashfn(ns, ifindex);\r\nstruct sel_netif *netif;\r\nlist_for_each_entry_rcu(netif, &sel_netif_hash[idx], list)\r\nif (net_eq(netif->nsec.ns, ns) &&\r\nnetif->nsec.ifindex == ifindex)\r\nreturn netif;\r\nreturn NULL;\r\n}\r\nstatic int sel_netif_insert(struct sel_netif *netif)\r\n{\r\nint idx;\r\nif (sel_netif_total >= SEL_NETIF_HASH_MAX)\r\nreturn -ENOSPC;\r\nidx = sel_netif_hashfn(netif->nsec.ns, netif->nsec.ifindex);\r\nlist_add_rcu(&netif->list, &sel_netif_hash[idx]);\r\nsel_netif_total++;\r\nreturn 0;\r\n}\r\nstatic void sel_netif_destroy(struct sel_netif *netif)\r\n{\r\nlist_del_rcu(&netif->list);\r\nsel_netif_total--;\r\nkfree_rcu(netif, rcu_head);\r\n}\r\nstatic int sel_netif_sid_slow(struct net *ns, int ifindex, u32 *sid)\r\n{\r\nint ret;\r\nstruct sel_netif *netif;\r\nstruct sel_netif *new = NULL;\r\nstruct net_device *dev;\r\ndev = dev_get_by_index(ns, ifindex);\r\nif (unlikely(dev == NULL)) {\r\nprintk(KERN_WARNING\r\n"SELinux: failure in sel_netif_sid_slow(),"\r\n" invalid network interface (%d)\n", ifindex);\r\nreturn -ENOENT;\r\n}\r\nspin_lock_bh(&sel_netif_lock);\r\nnetif = sel_netif_find(ns, ifindex);\r\nif (netif != NULL) {\r\n*sid = netif->nsec.sid;\r\nret = 0;\r\ngoto out;\r\n}\r\nnew = kzalloc(sizeof(*new), GFP_ATOMIC);\r\nif (new == NULL) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = security_netif_sid(dev->name, &new->nsec.sid);\r\nif (ret != 0)\r\ngoto out;\r\nnew->nsec.ns = ns;\r\nnew->nsec.ifindex = ifindex;\r\nret = sel_netif_insert(new);\r\nif (ret != 0)\r\ngoto out;\r\n*sid = new->nsec.sid;\r\nout:\r\nspin_unlock_bh(&sel_netif_lock);\r\ndev_put(dev);\r\nif (unlikely(ret)) {\r\nprintk(KERN_WARNING\r\n"SELinux: failure in sel_netif_sid_slow(),"\r\n" unable to determine network interface label (%d)\n",\r\nifindex);\r\nkfree(new);\r\n}\r\nreturn ret;\r\n}\r\nint sel_netif_sid(struct net *ns, int ifindex, u32 *sid)\r\n{\r\nstruct sel_netif *netif;\r\nrcu_read_lock();\r\nnetif = sel_netif_find(ns, ifindex);\r\nif (likely(netif != NULL)) {\r\n*sid = netif->nsec.sid;\r\nrcu_read_unlock();\r\nreturn 0;\r\n}\r\nrcu_read_unlock();\r\nreturn sel_netif_sid_slow(ns, ifindex, sid);\r\n}\r\nstatic void sel_netif_kill(const struct net *ns, int ifindex)\r\n{\r\nstruct sel_netif *netif;\r\nrcu_read_lock();\r\nspin_lock_bh(&sel_netif_lock);\r\nnetif = sel_netif_find(ns, ifindex);\r\nif (netif)\r\nsel_netif_destroy(netif);\r\nspin_unlock_bh(&sel_netif_lock);\r\nrcu_read_unlock();\r\n}\r\nvoid sel_netif_flush(void)\r\n{\r\nint idx;\r\nstruct sel_netif *netif;\r\nspin_lock_bh(&sel_netif_lock);\r\nfor (idx = 0; idx < SEL_NETIF_HASH_SIZE; idx++)\r\nlist_for_each_entry(netif, &sel_netif_hash[idx], list)\r\nsel_netif_destroy(netif);\r\nspin_unlock_bh(&sel_netif_lock);\r\n}\r\nstatic int sel_netif_netdev_notifier_handler(struct notifier_block *this,\r\nunsigned long event, void *ptr)\r\n{\r\nstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\r\nif (event == NETDEV_DOWN)\r\nsel_netif_kill(dev_net(dev), dev->ifindex);\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic __init int sel_netif_init(void)\r\n{\r\nint i;\r\nif (!selinux_enabled)\r\nreturn 0;\r\nfor (i = 0; i < SEL_NETIF_HASH_SIZE; i++)\r\nINIT_LIST_HEAD(&sel_netif_hash[i]);\r\nregister_netdevice_notifier(&sel_netif_netdev_notifier);\r\nreturn 0;\r\n}
