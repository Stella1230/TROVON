static int mincore_hugetlb(pte_t *pte, unsigned long hmask, unsigned long addr,\r\nunsigned long end, struct mm_walk *walk)\r\n{\r\n#ifdef CONFIG_HUGETLB_PAGE\r\nunsigned char present;\r\nunsigned char *vec = walk->private;\r\npresent = pte && !huge_pte_none(huge_ptep_get(pte));\r\nfor (; addr != end; vec++, addr += PAGE_SIZE)\r\n*vec = present;\r\nwalk->private = vec;\r\n#else\r\nBUG();\r\n#endif\r\nreturn 0;\r\n}\r\nstatic unsigned char mincore_page(struct address_space *mapping, pgoff_t pgoff)\r\n{\r\nunsigned char present = 0;\r\nstruct page *page;\r\n#ifdef CONFIG_SWAP\r\nif (shmem_mapping(mapping)) {\r\npage = find_get_entry(mapping, pgoff);\r\nif (radix_tree_exceptional_entry(page)) {\r\nswp_entry_t swp = radix_to_swp_entry(page);\r\npage = find_get_page(swap_address_space(swp), swp.val);\r\n}\r\n} else\r\npage = find_get_page(mapping, pgoff);\r\n#else\r\npage = find_get_page(mapping, pgoff);\r\n#endif\r\nif (page) {\r\npresent = PageUptodate(page);\r\npage_cache_release(page);\r\n}\r\nreturn present;\r\n}\r\nstatic int __mincore_unmapped_range(unsigned long addr, unsigned long end,\r\nstruct vm_area_struct *vma, unsigned char *vec)\r\n{\r\nunsigned long nr = (end - addr) >> PAGE_SHIFT;\r\nint i;\r\nif (vma->vm_file) {\r\npgoff_t pgoff;\r\npgoff = linear_page_index(vma, addr);\r\nfor (i = 0; i < nr; i++, pgoff++)\r\nvec[i] = mincore_page(vma->vm_file->f_mapping, pgoff);\r\n} else {\r\nfor (i = 0; i < nr; i++)\r\nvec[i] = 0;\r\n}\r\nreturn nr;\r\n}\r\nstatic int mincore_unmapped_range(unsigned long addr, unsigned long end,\r\nstruct mm_walk *walk)\r\n{\r\nwalk->private += __mincore_unmapped_range(addr, end,\r\nwalk->vma, walk->private);\r\nreturn 0;\r\n}\r\nstatic int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\r\nstruct mm_walk *walk)\r\n{\r\nspinlock_t *ptl;\r\nstruct vm_area_struct *vma = walk->vma;\r\npte_t *ptep;\r\nunsigned char *vec = walk->private;\r\nint nr = (end - addr) >> PAGE_SHIFT;\r\nif (pmd_trans_huge_lock(pmd, vma, &ptl) == 1) {\r\nmemset(vec, 1, nr);\r\nspin_unlock(ptl);\r\ngoto out;\r\n}\r\nif (pmd_trans_unstable(pmd)) {\r\n__mincore_unmapped_range(addr, end, vma, vec);\r\ngoto out;\r\n}\r\nptep = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\r\nfor (; addr != end; ptep++, addr += PAGE_SIZE) {\r\npte_t pte = *ptep;\r\nif (pte_none(pte))\r\n__mincore_unmapped_range(addr, addr + PAGE_SIZE,\r\nvma, vec);\r\nelse if (pte_present(pte))\r\n*vec = 1;\r\nelse {\r\nswp_entry_t entry = pte_to_swp_entry(pte);\r\nif (non_swap_entry(entry)) {\r\n*vec = 1;\r\n} else {\r\n#ifdef CONFIG_SWAP\r\n*vec = mincore_page(swap_address_space(entry),\r\nentry.val);\r\n#else\r\nWARN_ON(1);\r\n*vec = 1;\r\n#endif\r\n}\r\n}\r\nvec++;\r\n}\r\npte_unmap_unlock(ptep - 1, ptl);\r\nout:\r\nwalk->private += nr;\r\ncond_resched();\r\nreturn 0;\r\n}\r\nstatic long do_mincore(unsigned long addr, unsigned long pages, unsigned char *vec)\r\n{\r\nstruct vm_area_struct *vma;\r\nunsigned long end;\r\nint err;\r\nstruct mm_walk mincore_walk = {\r\n.pmd_entry = mincore_pte_range,\r\n.pte_hole = mincore_unmapped_range,\r\n.hugetlb_entry = mincore_hugetlb,\r\n.private = vec,\r\n};\r\nvma = find_vma(current->mm, addr);\r\nif (!vma || addr < vma->vm_start)\r\nreturn -ENOMEM;\r\nmincore_walk.mm = vma->vm_mm;\r\nend = min(vma->vm_end, addr + (pages << PAGE_SHIFT));\r\nerr = walk_page_range(addr, end, &mincore_walk);\r\nif (err < 0)\r\nreturn err;\r\nreturn (end - addr) >> PAGE_SHIFT;\r\n}
