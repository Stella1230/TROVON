void mlx4_en_fill_qp_context(struct mlx4_en_priv *priv, int size, int stride,\r\nint is_tx, int rss, int qpn, int cqn,\r\nint user_prio, struct mlx4_qp_context *context)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct net_device *dev = priv->dev;\r\nmemset(context, 0, sizeof *context);\r\ncontext->flags = cpu_to_be32(7 << 16 | rss << MLX4_RSS_QPC_FLAG_OFFSET);\r\ncontext->pd = cpu_to_be32(mdev->priv_pdn);\r\ncontext->mtu_msgmax = 0xff;\r\nif (!is_tx && !rss)\r\ncontext->rq_size_stride = ilog2(size) << 3 | (ilog2(stride) - 4);\r\nif (is_tx) {\r\ncontext->sq_size_stride = ilog2(size) << 3 | (ilog2(stride) - 4);\r\nif (mdev->dev->caps.flags2 & MLX4_DEV_CAP_FLAG2_PORT_REMAP)\r\ncontext->params2 |= MLX4_QP_BIT_FPP;\r\n} else {\r\ncontext->sq_size_stride = ilog2(TXBB_SIZE) - 4;\r\n}\r\ncontext->usr_page = cpu_to_be32(mdev->priv_uar.index);\r\ncontext->local_qpn = cpu_to_be32(qpn);\r\ncontext->pri_path.ackto = 1 & 0x07;\r\ncontext->pri_path.sched_queue = 0x83 | (priv->port - 1) << 6;\r\nif (user_prio >= 0) {\r\ncontext->pri_path.sched_queue |= user_prio << 3;\r\ncontext->pri_path.feup = MLX4_FEUP_FORCE_ETH_UP;\r\n}\r\ncontext->pri_path.counter_index = priv->counter_index;\r\ncontext->cqn_send = cpu_to_be32(cqn);\r\ncontext->cqn_recv = cpu_to_be32(cqn);\r\ncontext->db_rec_addr = cpu_to_be64(priv->res.db.dma << 2);\r\nif (!(dev->features & NETIF_F_HW_VLAN_CTAG_RX))\r\ncontext->param3 |= cpu_to_be32(1 << 30);\r\nif (!is_tx && !rss &&\r\n(mdev->dev->caps.tunnel_offload_mode == MLX4_TUNNEL_OFFLOAD_MODE_VXLAN)) {\r\nen_dbg(HW, priv, "Setting RX qp %x tunnel mode to RX tunneled & non-tunneled\n", qpn);\r\ncontext->srqn = cpu_to_be32(7 << 28);\r\n}\r\n}\r\nint mlx4_en_map_buffer(struct mlx4_buf *buf)\r\n{\r\nstruct page **pages;\r\nint i;\r\nif (BITS_PER_LONG == 64 || buf->nbufs == 1)\r\nreturn 0;\r\npages = kmalloc(sizeof *pages * buf->nbufs, GFP_KERNEL);\r\nif (!pages)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < buf->nbufs; ++i)\r\npages[i] = virt_to_page(buf->page_list[i].buf);\r\nbuf->direct.buf = vmap(pages, buf->nbufs, VM_MAP, PAGE_KERNEL);\r\nkfree(pages);\r\nif (!buf->direct.buf)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid mlx4_en_unmap_buffer(struct mlx4_buf *buf)\r\n{\r\nif (BITS_PER_LONG == 64 || buf->nbufs == 1)\r\nreturn;\r\nvunmap(buf->direct.buf);\r\n}\r\nvoid mlx4_en_sqp_event(struct mlx4_qp *qp, enum mlx4_event event)\r\n{\r\nreturn;\r\n}
