static inline struct omap_dmadev *to_omap_dma_dev(struct dma_device *d)\r\n{\r\nreturn container_of(d, struct omap_dmadev, ddev);\r\n}\r\nstatic inline struct omap_chan *to_omap_dma_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct omap_chan, vc.chan);\r\n}\r\nstatic inline struct omap_desc *to_omap_dma_desc(struct dma_async_tx_descriptor *t)\r\n{\r\nreturn container_of(t, struct omap_desc, vd.tx);\r\n}\r\nstatic void omap_dma_desc_free(struct virt_dma_desc *vd)\r\n{\r\nkfree(container_of(vd, struct omap_desc, vd));\r\n}\r\nstatic void omap_dma_write(uint32_t val, unsigned type, void __iomem *addr)\r\n{\r\nswitch (type) {\r\ncase OMAP_DMA_REG_16BIT:\r\nwritew_relaxed(val, addr);\r\nbreak;\r\ncase OMAP_DMA_REG_2X16BIT:\r\nwritew_relaxed(val, addr);\r\nwritew_relaxed(val >> 16, addr + 2);\r\nbreak;\r\ncase OMAP_DMA_REG_32BIT:\r\nwritel_relaxed(val, addr);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\n}\r\n}\r\nstatic unsigned omap_dma_read(unsigned type, void __iomem *addr)\r\n{\r\nunsigned val;\r\nswitch (type) {\r\ncase OMAP_DMA_REG_16BIT:\r\nval = readw_relaxed(addr);\r\nbreak;\r\ncase OMAP_DMA_REG_2X16BIT:\r\nval = readw_relaxed(addr);\r\nval |= readw_relaxed(addr + 2) << 16;\r\nbreak;\r\ncase OMAP_DMA_REG_32BIT:\r\nval = readl_relaxed(addr);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\nval = 0;\r\n}\r\nreturn val;\r\n}\r\nstatic void omap_dma_glbl_write(struct omap_dmadev *od, unsigned reg, unsigned val)\r\n{\r\nconst struct omap_dma_reg *r = od->reg_map + reg;\r\nWARN_ON(r->stride);\r\nomap_dma_write(val, r->type, od->base + r->offset);\r\n}\r\nstatic unsigned omap_dma_glbl_read(struct omap_dmadev *od, unsigned reg)\r\n{\r\nconst struct omap_dma_reg *r = od->reg_map + reg;\r\nWARN_ON(r->stride);\r\nreturn omap_dma_read(r->type, od->base + r->offset);\r\n}\r\nstatic void omap_dma_chan_write(struct omap_chan *c, unsigned reg, unsigned val)\r\n{\r\nconst struct omap_dma_reg *r = c->reg_map + reg;\r\nomap_dma_write(val, r->type, c->channel_base + r->offset);\r\n}\r\nstatic unsigned omap_dma_chan_read(struct omap_chan *c, unsigned reg)\r\n{\r\nconst struct omap_dma_reg *r = c->reg_map + reg;\r\nreturn omap_dma_read(r->type, c->channel_base + r->offset);\r\n}\r\nstatic void omap_dma_clear_csr(struct omap_chan *c)\r\n{\r\nif (dma_omap1())\r\nomap_dma_chan_read(c, CSR);\r\nelse\r\nomap_dma_chan_write(c, CSR, ~0);\r\n}\r\nstatic unsigned omap_dma_get_csr(struct omap_chan *c)\r\n{\r\nunsigned val = omap_dma_chan_read(c, CSR);\r\nif (!dma_omap1())\r\nomap_dma_chan_write(c, CSR, val);\r\nreturn val;\r\n}\r\nstatic void omap_dma_assign(struct omap_dmadev *od, struct omap_chan *c,\r\nunsigned lch)\r\n{\r\nc->channel_base = od->base + od->plat->channel_stride * lch;\r\nod->lch_map[lch] = c;\r\n}\r\nstatic void omap_dma_start(struct omap_chan *c, struct omap_desc *d)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\nif (__dma_omap15xx(od->plat->dma_attr))\r\nomap_dma_chan_write(c, CPC, 0);\r\nelse\r\nomap_dma_chan_write(c, CDAC, 0);\r\nomap_dma_clear_csr(c);\r\nomap_dma_chan_write(c, CICR, d->cicr);\r\nomap_dma_chan_write(c, CCR, d->ccr | CCR_ENABLE);\r\n}\r\nstatic void omap_dma_stop(struct omap_chan *c)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\nuint32_t val;\r\nomap_dma_chan_write(c, CICR, 0);\r\nomap_dma_clear_csr(c);\r\nval = omap_dma_chan_read(c, CCR);\r\nif (od->plat->errata & DMA_ERRATA_i541 && val & CCR_TRIGGER_SRC) {\r\nuint32_t sysconfig;\r\nunsigned i;\r\nsysconfig = omap_dma_glbl_read(od, OCP_SYSCONFIG);\r\nval = sysconfig & ~DMA_SYSCONFIG_MIDLEMODE_MASK;\r\nval |= DMA_SYSCONFIG_MIDLEMODE(DMA_IDLEMODE_NO_IDLE);\r\nomap_dma_glbl_write(od, OCP_SYSCONFIG, val);\r\nval = omap_dma_chan_read(c, CCR);\r\nval &= ~CCR_ENABLE;\r\nomap_dma_chan_write(c, CCR, val);\r\nfor (i = 0; ; i++) {\r\nval = omap_dma_chan_read(c, CCR);\r\nif (!(val & (CCR_RD_ACTIVE | CCR_WR_ACTIVE)))\r\nbreak;\r\nif (i > 100)\r\nbreak;\r\nudelay(5);\r\n}\r\nif (val & (CCR_RD_ACTIVE | CCR_WR_ACTIVE))\r\ndev_err(c->vc.chan.device->dev,\r\n"DMA drain did not complete on lch %d\n",\r\nc->dma_ch);\r\nomap_dma_glbl_write(od, OCP_SYSCONFIG, sysconfig);\r\n} else {\r\nval &= ~CCR_ENABLE;\r\nomap_dma_chan_write(c, CCR, val);\r\n}\r\nmb();\r\nif (!__dma_omap15xx(od->plat->dma_attr) && c->cyclic) {\r\nval = omap_dma_chan_read(c, CLNK_CTRL);\r\nif (dma_omap1())\r\nval |= 1 << 14;\r\nelse\r\nval &= ~CLNK_CTRL_ENABLE_LNK;\r\nomap_dma_chan_write(c, CLNK_CTRL, val);\r\n}\r\n}\r\nstatic void omap_dma_start_sg(struct omap_chan *c, struct omap_desc *d,\r\nunsigned idx)\r\n{\r\nstruct omap_sg *sg = d->sg + idx;\r\nunsigned cxsa, cxei, cxfi;\r\nif (d->dir == DMA_DEV_TO_MEM || d->dir == DMA_MEM_TO_MEM) {\r\ncxsa = CDSA;\r\ncxei = CDEI;\r\ncxfi = CDFI;\r\n} else {\r\ncxsa = CSSA;\r\ncxei = CSEI;\r\ncxfi = CSFI;\r\n}\r\nomap_dma_chan_write(c, cxsa, sg->addr);\r\nomap_dma_chan_write(c, cxei, 0);\r\nomap_dma_chan_write(c, cxfi, 0);\r\nomap_dma_chan_write(c, CEN, sg->en);\r\nomap_dma_chan_write(c, CFN, sg->fn);\r\nomap_dma_start(c, d);\r\n}\r\nstatic void omap_dma_start_desc(struct omap_chan *c)\r\n{\r\nstruct virt_dma_desc *vd = vchan_next_desc(&c->vc);\r\nstruct omap_desc *d;\r\nunsigned cxsa, cxei, cxfi;\r\nif (!vd) {\r\nc->desc = NULL;\r\nreturn;\r\n}\r\nlist_del(&vd->node);\r\nc->desc = d = to_omap_dma_desc(&vd->tx);\r\nc->sgidx = 0;\r\nmb();\r\nomap_dma_chan_write(c, CCR, d->ccr);\r\nif (dma_omap1())\r\nomap_dma_chan_write(c, CCR2, d->ccr >> 16);\r\nif (d->dir == DMA_DEV_TO_MEM || d->dir == DMA_MEM_TO_MEM) {\r\ncxsa = CSSA;\r\ncxei = CSEI;\r\ncxfi = CSFI;\r\n} else {\r\ncxsa = CDSA;\r\ncxei = CDEI;\r\ncxfi = CDFI;\r\n}\r\nomap_dma_chan_write(c, cxsa, d->dev_addr);\r\nomap_dma_chan_write(c, cxei, 0);\r\nomap_dma_chan_write(c, cxfi, d->fi);\r\nomap_dma_chan_write(c, CSDP, d->csdp);\r\nomap_dma_chan_write(c, CLNK_CTRL, d->clnk_ctrl);\r\nomap_dma_start_sg(c, d, 0);\r\n}\r\nstatic void omap_dma_callback(int ch, u16 status, void *data)\r\n{\r\nstruct omap_chan *c = data;\r\nstruct omap_desc *d;\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nd = c->desc;\r\nif (d) {\r\nif (!c->cyclic) {\r\nif (++c->sgidx < d->sglen) {\r\nomap_dma_start_sg(c, d, c->sgidx);\r\n} else {\r\nomap_dma_start_desc(c);\r\nvchan_cookie_complete(&d->vd);\r\n}\r\n} else {\r\nvchan_cyclic_callback(&d->vd);\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\nstatic void omap_dma_sched(unsigned long data)\r\n{\r\nstruct omap_dmadev *d = (struct omap_dmadev *)data;\r\nLIST_HEAD(head);\r\nspin_lock_irq(&d->lock);\r\nlist_splice_tail_init(&d->pending, &head);\r\nspin_unlock_irq(&d->lock);\r\nwhile (!list_empty(&head)) {\r\nstruct omap_chan *c = list_first_entry(&head,\r\nstruct omap_chan, node);\r\nspin_lock_irq(&c->vc.lock);\r\nlist_del_init(&c->node);\r\nomap_dma_start_desc(c);\r\nspin_unlock_irq(&c->vc.lock);\r\n}\r\n}\r\nstatic irqreturn_t omap_dma_irq(int irq, void *devid)\r\n{\r\nstruct omap_dmadev *od = devid;\r\nunsigned status, channel;\r\nspin_lock(&od->irq_lock);\r\nstatus = omap_dma_glbl_read(od, IRQSTATUS_L1);\r\nstatus &= od->irq_enable_mask;\r\nif (status == 0) {\r\nspin_unlock(&od->irq_lock);\r\nreturn IRQ_NONE;\r\n}\r\nwhile ((channel = ffs(status)) != 0) {\r\nunsigned mask, csr;\r\nstruct omap_chan *c;\r\nchannel -= 1;\r\nmask = BIT(channel);\r\nstatus &= ~mask;\r\nc = od->lch_map[channel];\r\nif (c == NULL) {\r\ndev_err(od->ddev.dev, "invalid channel %u\n", channel);\r\ncontinue;\r\n}\r\ncsr = omap_dma_get_csr(c);\r\nomap_dma_glbl_write(od, IRQSTATUS_L1, mask);\r\nomap_dma_callback(channel, csr, c);\r\n}\r\nspin_unlock(&od->irq_lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int omap_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nint ret;\r\nif (od->legacy) {\r\nret = omap_request_dma(c->dma_sig, "DMA engine",\r\nomap_dma_callback, c, &c->dma_ch);\r\n} else {\r\nret = omap_request_dma(c->dma_sig, "DMA engine", NULL, NULL,\r\n&c->dma_ch);\r\n}\r\ndev_dbg(od->ddev.dev, "allocating channel %u for %u\n",\r\nc->dma_ch, c->dma_sig);\r\nif (ret >= 0) {\r\nomap_dma_assign(od, c, c->dma_ch);\r\nif (!od->legacy) {\r\nunsigned val;\r\nspin_lock_irq(&od->irq_lock);\r\nval = BIT(c->dma_ch);\r\nomap_dma_glbl_write(od, IRQSTATUS_L1, val);\r\nod->irq_enable_mask |= val;\r\nomap_dma_glbl_write(od, IRQENABLE_L1, od->irq_enable_mask);\r\nval = omap_dma_glbl_read(od, IRQENABLE_L0);\r\nval &= ~BIT(c->dma_ch);\r\nomap_dma_glbl_write(od, IRQENABLE_L0, val);\r\nspin_unlock_irq(&od->irq_lock);\r\n}\r\n}\r\nif (dma_omap1()) {\r\nif (__dma_omap16xx(od->plat->dma_attr)) {\r\nc->ccr = CCR_OMAP31_DISABLE;\r\nc->ccr |= c->dma_ch + 1;\r\n} else {\r\nc->ccr = c->dma_sig & 0x1f;\r\n}\r\n} else {\r\nc->ccr = c->dma_sig & 0x1f;\r\nc->ccr |= (c->dma_sig & ~0x1f) << 14;\r\n}\r\nif (od->plat->errata & DMA_ERRATA_IFRAME_BUFFERING)\r\nc->ccr |= CCR_BUFFERING_DISABLE;\r\nreturn ret;\r\n}\r\nstatic void omap_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nif (!od->legacy) {\r\nspin_lock_irq(&od->irq_lock);\r\nod->irq_enable_mask &= ~BIT(c->dma_ch);\r\nomap_dma_glbl_write(od, IRQENABLE_L1, od->irq_enable_mask);\r\nspin_unlock_irq(&od->irq_lock);\r\n}\r\nc->channel_base = NULL;\r\nod->lch_map[c->dma_ch] = NULL;\r\nvchan_free_chan_resources(&c->vc);\r\nomap_free_dma(c->dma_ch);\r\ndev_dbg(od->ddev.dev, "freeing channel for %u\n", c->dma_sig);\r\nc->dma_sig = 0;\r\n}\r\nstatic size_t omap_dma_sg_size(struct omap_sg *sg)\r\n{\r\nreturn sg->en * sg->fn;\r\n}\r\nstatic size_t omap_dma_desc_size(struct omap_desc *d)\r\n{\r\nunsigned i;\r\nsize_t size;\r\nfor (size = i = 0; i < d->sglen; i++)\r\nsize += omap_dma_sg_size(&d->sg[i]);\r\nreturn size * es_bytes[d->es];\r\n}\r\nstatic size_t omap_dma_desc_size_pos(struct omap_desc *d, dma_addr_t addr)\r\n{\r\nunsigned i;\r\nsize_t size, es_size = es_bytes[d->es];\r\nfor (size = i = 0; i < d->sglen; i++) {\r\nsize_t this_size = omap_dma_sg_size(&d->sg[i]) * es_size;\r\nif (size)\r\nsize += this_size;\r\nelse if (addr >= d->sg[i].addr &&\r\naddr < d->sg[i].addr + this_size)\r\nsize += d->sg[i].addr + this_size - addr;\r\n}\r\nreturn size;\r\n}\r\nstatic uint32_t omap_dma_chan_read_3_3(struct omap_chan *c, unsigned reg)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\nuint32_t val;\r\nval = omap_dma_chan_read(c, reg);\r\nif (val == 0 && od->plat->errata & DMA_ERRATA_3_3)\r\nval = omap_dma_chan_read(c, reg);\r\nreturn val;\r\n}\r\nstatic dma_addr_t omap_dma_get_src_pos(struct omap_chan *c)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\ndma_addr_t addr, cdac;\r\nif (__dma_omap15xx(od->plat->dma_attr)) {\r\naddr = omap_dma_chan_read(c, CPC);\r\n} else {\r\naddr = omap_dma_chan_read_3_3(c, CSAC);\r\ncdac = omap_dma_chan_read_3_3(c, CDAC);\r\nif (cdac == 0)\r\naddr = omap_dma_chan_read(c, CSSA);\r\n}\r\nif (dma_omap1())\r\naddr |= omap_dma_chan_read(c, CSSA) & 0xffff0000;\r\nreturn addr;\r\n}\r\nstatic dma_addr_t omap_dma_get_dst_pos(struct omap_chan *c)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(c->vc.chan.device);\r\ndma_addr_t addr;\r\nif (__dma_omap15xx(od->plat->dma_attr)) {\r\naddr = omap_dma_chan_read(c, CPC);\r\n} else {\r\naddr = omap_dma_chan_read_3_3(c, CDAC);\r\nif (addr == 0)\r\naddr = omap_dma_chan_read(c, CDSA);\r\n}\r\nif (dma_omap1())\r\naddr |= omap_dma_chan_read(c, CDSA) & 0xffff0000;\r\nreturn addr;\r\n}\r\nstatic enum dma_status omap_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct virt_dma_desc *vd;\r\nenum dma_status ret;\r\nunsigned long flags;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_COMPLETE || !txstate)\r\nreturn ret;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nvd = vchan_find_desc(&c->vc, cookie);\r\nif (vd) {\r\ntxstate->residue = omap_dma_desc_size(to_omap_dma_desc(&vd->tx));\r\n} else if (c->desc && c->desc->vd.tx.cookie == cookie) {\r\nstruct omap_desc *d = c->desc;\r\ndma_addr_t pos;\r\nif (d->dir == DMA_MEM_TO_DEV)\r\npos = omap_dma_get_src_pos(c);\r\nelse if (d->dir == DMA_DEV_TO_MEM)\r\npos = omap_dma_get_dst_pos(c);\r\nelse\r\npos = 0;\r\ntxstate->residue = omap_dma_desc_size_pos(d, pos);\r\n} else {\r\ntxstate->residue = 0;\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nreturn ret;\r\n}\r\nstatic void omap_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nif (vchan_issue_pending(&c->vc) && !c->desc) {\r\nif (!c->cyclic) {\r\nstruct omap_dmadev *d = to_omap_dma_dev(chan->device);\r\nspin_lock(&d->lock);\r\nif (list_empty(&c->node))\r\nlist_add_tail(&c->node, &d->pending);\r\nspin_unlock(&d->lock);\r\ntasklet_schedule(&d->task);\r\n} else {\r\nomap_dma_start_desc(c);\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *omap_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl, unsigned sglen,\r\nenum dma_transfer_direction dir, unsigned long tx_flags, void *context)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nenum dma_slave_buswidth dev_width;\r\nstruct scatterlist *sgent;\r\nstruct omap_desc *d;\r\ndma_addr_t dev_addr;\r\nunsigned i, j = 0, es, en, frame_bytes;\r\nu32 burst;\r\nif (dir == DMA_DEV_TO_MEM) {\r\ndev_addr = c->cfg.src_addr;\r\ndev_width = c->cfg.src_addr_width;\r\nburst = c->cfg.src_maxburst;\r\n} else if (dir == DMA_MEM_TO_DEV) {\r\ndev_addr = c->cfg.dst_addr;\r\ndev_width = c->cfg.dst_addr_width;\r\nburst = c->cfg.dst_maxburst;\r\n} else {\r\ndev_err(chan->device->dev, "%s: bad direction?\n", __func__);\r\nreturn NULL;\r\n}\r\nswitch (dev_width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nes = CSDP_DATA_TYPE_8;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nes = CSDP_DATA_TYPE_16;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nes = CSDP_DATA_TYPE_32;\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nd = kzalloc(sizeof(*d) + sglen * sizeof(d->sg[0]), GFP_ATOMIC);\r\nif (!d)\r\nreturn NULL;\r\nd->dir = dir;\r\nd->dev_addr = dev_addr;\r\nd->es = es;\r\nd->ccr = c->ccr | CCR_SYNC_FRAME;\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->ccr |= CCR_DST_AMODE_POSTINC | CCR_SRC_AMODE_CONSTANT;\r\nelse\r\nd->ccr |= CCR_DST_AMODE_CONSTANT | CCR_SRC_AMODE_POSTINC;\r\nd->cicr = CICR_DROP_IE | CICR_BLOCK_IE;\r\nd->csdp = es;\r\nif (dma_omap1()) {\r\nd->cicr |= CICR_TOUT_IE;\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_TIPB;\r\nelse\r\nd->csdp |= CSDP_DST_PORT_TIPB | CSDP_SRC_PORT_EMIFF;\r\n} else {\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->ccr |= CCR_TRIGGER_SRC;\r\nd->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\r\n}\r\nif (od->plat->errata & DMA_ERRATA_PARALLEL_CHANNELS)\r\nd->clnk_ctrl = c->dma_ch;\r\nen = burst;\r\nframe_bytes = es_bytes[es] * en;\r\nfor_each_sg(sgl, sgent, sglen, i) {\r\nd->sg[j].addr = sg_dma_address(sgent);\r\nd->sg[j].en = en;\r\nd->sg[j].fn = sg_dma_len(sgent) / frame_bytes;\r\nj++;\r\n}\r\nd->sglen = j;\r\nreturn vchan_tx_prep(&c->vc, &d->vd, tx_flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *omap_dma_prep_dma_cyclic(\r\nstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction dir, unsigned long flags)\r\n{\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nenum dma_slave_buswidth dev_width;\r\nstruct omap_desc *d;\r\ndma_addr_t dev_addr;\r\nunsigned es;\r\nu32 burst;\r\nif (dir == DMA_DEV_TO_MEM) {\r\ndev_addr = c->cfg.src_addr;\r\ndev_width = c->cfg.src_addr_width;\r\nburst = c->cfg.src_maxburst;\r\n} else if (dir == DMA_MEM_TO_DEV) {\r\ndev_addr = c->cfg.dst_addr;\r\ndev_width = c->cfg.dst_addr_width;\r\nburst = c->cfg.dst_maxburst;\r\n} else {\r\ndev_err(chan->device->dev, "%s: bad direction?\n", __func__);\r\nreturn NULL;\r\n}\r\nswitch (dev_width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nes = CSDP_DATA_TYPE_8;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nes = CSDP_DATA_TYPE_16;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nes = CSDP_DATA_TYPE_32;\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nd = kzalloc(sizeof(*d) + sizeof(d->sg[0]), GFP_ATOMIC);\r\nif (!d)\r\nreturn NULL;\r\nd->dir = dir;\r\nd->dev_addr = dev_addr;\r\nd->fi = burst;\r\nd->es = es;\r\nd->sg[0].addr = buf_addr;\r\nd->sg[0].en = period_len / es_bytes[es];\r\nd->sg[0].fn = buf_len / period_len;\r\nd->sglen = 1;\r\nd->ccr = c->ccr;\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->ccr |= CCR_DST_AMODE_POSTINC | CCR_SRC_AMODE_CONSTANT;\r\nelse\r\nd->ccr |= CCR_DST_AMODE_CONSTANT | CCR_SRC_AMODE_POSTINC;\r\nd->cicr = CICR_DROP_IE;\r\nif (flags & DMA_PREP_INTERRUPT)\r\nd->cicr |= CICR_FRAME_IE;\r\nd->csdp = es;\r\nif (dma_omap1()) {\r\nd->cicr |= CICR_TOUT_IE;\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_MPUI;\r\nelse\r\nd->csdp |= CSDP_DST_PORT_MPUI | CSDP_SRC_PORT_EMIFF;\r\n} else {\r\nif (burst)\r\nd->ccr |= CCR_SYNC_PACKET;\r\nelse\r\nd->ccr |= CCR_SYNC_ELEMENT;\r\nif (dir == DMA_DEV_TO_MEM)\r\nd->ccr |= CCR_TRIGGER_SRC;\r\nd->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\r\nd->csdp |= CSDP_DST_BURST_64 | CSDP_SRC_BURST_64;\r\n}\r\nif (__dma_omap15xx(od->plat->dma_attr))\r\nd->ccr |= CCR_AUTO_INIT | CCR_REPEAT;\r\nelse\r\nd->clnk_ctrl = c->dma_ch | CLNK_CTRL_ENABLE_LNK;\r\nc->cyclic = true;\r\nreturn vchan_tx_prep(&c->vc, &d->vd, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *omap_dma_prep_dma_memcpy(\r\nstruct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long tx_flags)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct omap_desc *d;\r\nuint8_t data_type;\r\nd = kzalloc(sizeof(*d) + sizeof(d->sg[0]), GFP_ATOMIC);\r\nif (!d)\r\nreturn NULL;\r\ndata_type = __ffs((src | dest | len));\r\nif (data_type > CSDP_DATA_TYPE_32)\r\ndata_type = CSDP_DATA_TYPE_32;\r\nd->dir = DMA_MEM_TO_MEM;\r\nd->dev_addr = src;\r\nd->fi = 0;\r\nd->es = data_type;\r\nd->sg[0].en = len / BIT(data_type);\r\nd->sg[0].fn = 1;\r\nd->sg[0].addr = dest;\r\nd->sglen = 1;\r\nd->ccr = c->ccr;\r\nd->ccr |= CCR_DST_AMODE_POSTINC | CCR_SRC_AMODE_POSTINC;\r\nd->cicr = CICR_DROP_IE;\r\nif (tx_flags & DMA_PREP_INTERRUPT)\r\nd->cicr |= CICR_FRAME_IE;\r\nd->csdp = data_type;\r\nif (dma_omap1()) {\r\nd->cicr |= CICR_TOUT_IE;\r\nd->csdp |= CSDP_DST_PORT_EMIFF | CSDP_SRC_PORT_EMIFF;\r\n} else {\r\nd->csdp |= CSDP_DST_PACKED | CSDP_SRC_PACKED;\r\nd->cicr |= CICR_MISALIGNED_ERR_IE | CICR_TRANS_ERR_IE;\r\nd->csdp |= CSDP_DST_BURST_64 | CSDP_SRC_BURST_64;\r\n}\r\nreturn vchan_tx_prep(&c->vc, &d->vd, tx_flags);\r\n}\r\nstatic int omap_dma_slave_config(struct dma_chan *chan, struct dma_slave_config *cfg)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nif (cfg->src_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES ||\r\ncfg->dst_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES)\r\nreturn -EINVAL;\r\nmemcpy(&c->cfg, cfg, sizeof(c->cfg));\r\nreturn 0;\r\n}\r\nstatic int omap_dma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nstruct omap_dmadev *d = to_omap_dma_dev(c->vc.chan.device);\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nspin_lock(&d->lock);\r\nlist_del_init(&c->node);\r\nspin_unlock(&d->lock);\r\nif (c->desc) {\r\nomap_dma_desc_free(&c->desc->vd);\r\nc->desc = NULL;\r\nif (!c->paused)\r\nomap_dma_stop(c);\r\n}\r\nif (c->cyclic) {\r\nc->cyclic = false;\r\nc->paused = false;\r\n}\r\nvchan_get_all_descriptors(&c->vc, &head);\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nvchan_dma_desc_free_list(&c->vc, &head);\r\nreturn 0;\r\n}\r\nstatic int omap_dma_pause(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nif (!c->cyclic)\r\nreturn -EINVAL;\r\nif (!c->paused) {\r\nomap_dma_stop(c);\r\nc->paused = true;\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_dma_resume(struct dma_chan *chan)\r\n{\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nif (!c->cyclic)\r\nreturn -EINVAL;\r\nif (c->paused) {\r\nmb();\r\nomap_dma_chan_write(c, CLNK_CTRL, c->desc->clnk_ctrl);\r\nomap_dma_start(c, c->desc);\r\nc->paused = false;\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_dma_chan_init(struct omap_dmadev *od)\r\n{\r\nstruct omap_chan *c;\r\nc = kzalloc(sizeof(*c), GFP_KERNEL);\r\nif (!c)\r\nreturn -ENOMEM;\r\nc->reg_map = od->reg_map;\r\nc->vc.desc_free = omap_dma_desc_free;\r\nvchan_init(&c->vc, &od->ddev);\r\nINIT_LIST_HEAD(&c->node);\r\nreturn 0;\r\n}\r\nstatic void omap_dma_free(struct omap_dmadev *od)\r\n{\r\ntasklet_kill(&od->task);\r\nwhile (!list_empty(&od->ddev.channels)) {\r\nstruct omap_chan *c = list_first_entry(&od->ddev.channels,\r\nstruct omap_chan, vc.chan.device_node);\r\nlist_del(&c->vc.chan.device_node);\r\ntasklet_kill(&c->vc.task);\r\nkfree(c);\r\n}\r\n}\r\nstatic int omap_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct omap_dmadev *od;\r\nstruct resource *res;\r\nint rc, i, irq;\r\nod = devm_kzalloc(&pdev->dev, sizeof(*od), GFP_KERNEL);\r\nif (!od)\r\nreturn -ENOMEM;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nod->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(od->base))\r\nreturn PTR_ERR(od->base);\r\nod->plat = omap_get_plat_info();\r\nif (!od->plat)\r\nreturn -EPROBE_DEFER;\r\nod->reg_map = od->plat->reg_map;\r\ndma_cap_set(DMA_SLAVE, od->ddev.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, od->ddev.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, od->ddev.cap_mask);\r\nod->ddev.device_alloc_chan_resources = omap_dma_alloc_chan_resources;\r\nod->ddev.device_free_chan_resources = omap_dma_free_chan_resources;\r\nod->ddev.device_tx_status = omap_dma_tx_status;\r\nod->ddev.device_issue_pending = omap_dma_issue_pending;\r\nod->ddev.device_prep_slave_sg = omap_dma_prep_slave_sg;\r\nod->ddev.device_prep_dma_cyclic = omap_dma_prep_dma_cyclic;\r\nod->ddev.device_prep_dma_memcpy = omap_dma_prep_dma_memcpy;\r\nod->ddev.device_config = omap_dma_slave_config;\r\nod->ddev.device_pause = omap_dma_pause;\r\nod->ddev.device_resume = omap_dma_resume;\r\nod->ddev.device_terminate_all = omap_dma_terminate_all;\r\nod->ddev.src_addr_widths = OMAP_DMA_BUSWIDTHS;\r\nod->ddev.dst_addr_widths = OMAP_DMA_BUSWIDTHS;\r\nod->ddev.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\nod->ddev.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\nod->ddev.dev = &pdev->dev;\r\nINIT_LIST_HEAD(&od->ddev.channels);\r\nINIT_LIST_HEAD(&od->pending);\r\nspin_lock_init(&od->lock);\r\nspin_lock_init(&od->irq_lock);\r\ntasklet_init(&od->task, omap_dma_sched, (unsigned long)od);\r\nod->dma_requests = OMAP_SDMA_REQUESTS;\r\nif (pdev->dev.of_node && of_property_read_u32(pdev->dev.of_node,\r\n"dma-requests",\r\n&od->dma_requests)) {\r\ndev_info(&pdev->dev,\r\n"Missing dma-requests property, using %u.\n",\r\nOMAP_SDMA_REQUESTS);\r\n}\r\nfor (i = 0; i < OMAP_SDMA_CHANNELS; i++) {\r\nrc = omap_dma_chan_init(od);\r\nif (rc) {\r\nomap_dma_free(od);\r\nreturn rc;\r\n}\r\n}\r\nirq = platform_get_irq(pdev, 1);\r\nif (irq <= 0) {\r\ndev_info(&pdev->dev, "failed to get L1 IRQ: %d\n", irq);\r\nod->legacy = true;\r\n} else {\r\nod->irq_enable_mask = 0;\r\nomap_dma_glbl_write(od, IRQENABLE_L1, 0);\r\nrc = devm_request_irq(&pdev->dev, irq, omap_dma_irq,\r\nIRQF_SHARED, "omap-dma-engine", od);\r\nif (rc)\r\nreturn rc;\r\n}\r\nrc = dma_async_device_register(&od->ddev);\r\nif (rc) {\r\npr_warn("OMAP-DMA: failed to register slave DMA engine device: %d\n",\r\nrc);\r\nomap_dma_free(od);\r\nreturn rc;\r\n}\r\nplatform_set_drvdata(pdev, od);\r\nif (pdev->dev.of_node) {\r\nomap_dma_info.dma_cap = od->ddev.cap_mask;\r\nrc = of_dma_controller_register(pdev->dev.of_node,\r\nof_dma_simple_xlate, &omap_dma_info);\r\nif (rc) {\r\npr_warn("OMAP-DMA: failed to register DMA controller\n");\r\ndma_async_device_unregister(&od->ddev);\r\nomap_dma_free(od);\r\n}\r\n}\r\ndev_info(&pdev->dev, "OMAP DMA engine driver\n");\r\nreturn rc;\r\n}\r\nstatic int omap_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct omap_dmadev *od = platform_get_drvdata(pdev);\r\nif (pdev->dev.of_node)\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&od->ddev);\r\nif (!od->legacy) {\r\nomap_dma_glbl_write(od, IRQENABLE_L0, 0);\r\n}\r\nomap_dma_free(od);\r\nreturn 0;\r\n}\r\nbool omap_dma_filter_fn(struct dma_chan *chan, void *param)\r\n{\r\nif (chan->device->dev->driver == &omap_dma_driver.driver) {\r\nstruct omap_dmadev *od = to_omap_dma_dev(chan->device);\r\nstruct omap_chan *c = to_omap_dma_chan(chan);\r\nunsigned req = *(unsigned *)param;\r\nif (req <= od->dma_requests) {\r\nc->dma_sig = req;\r\nreturn true;\r\n}\r\n}\r\nreturn false;\r\n}\r\nstatic int omap_dma_init(void)\r\n{\r\nreturn platform_driver_register(&omap_dma_driver);\r\n}\r\nstatic void __exit omap_dma_exit(void)\r\n{\r\nplatform_driver_unregister(&omap_dma_driver);\r\n}
