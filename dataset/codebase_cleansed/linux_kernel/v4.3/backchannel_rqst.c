static inline int xprt_need_to_requeue(struct rpc_xprt *xprt)\r\n{\r\nreturn xprt->bc_alloc_count < atomic_read(&xprt->bc_free_slots);\r\n}\r\nstatic inline void xprt_inc_alloc_count(struct rpc_xprt *xprt, unsigned int n)\r\n{\r\natomic_add(n, &xprt->bc_free_slots);\r\nxprt->bc_alloc_count += n;\r\n}\r\nstatic inline int xprt_dec_alloc_count(struct rpc_xprt *xprt, unsigned int n)\r\n{\r\natomic_sub(n, &xprt->bc_free_slots);\r\nreturn xprt->bc_alloc_count -= n;\r\n}\r\nstatic void xprt_free_allocation(struct rpc_rqst *req)\r\n{\r\nstruct xdr_buf *xbufp;\r\ndprintk("RPC: free allocations for req= %p\n", req);\r\nWARN_ON_ONCE(test_bit(RPC_BC_PA_IN_USE, &req->rq_bc_pa_state));\r\nxbufp = &req->rq_rcv_buf;\r\nfree_page((unsigned long)xbufp->head[0].iov_base);\r\nxbufp = &req->rq_snd_buf;\r\nfree_page((unsigned long)xbufp->head[0].iov_base);\r\nkfree(req);\r\n}\r\nstatic int xprt_alloc_xdr_buf(struct xdr_buf *buf, gfp_t gfp_flags)\r\n{\r\nstruct page *page;\r\npage = alloc_page(gfp_flags);\r\nif (page == NULL)\r\nreturn -ENOMEM;\r\nbuf->head[0].iov_base = page_address(page);\r\nbuf->head[0].iov_len = PAGE_SIZE;\r\nbuf->tail[0].iov_base = NULL;\r\nbuf->tail[0].iov_len = 0;\r\nbuf->page_len = 0;\r\nbuf->len = 0;\r\nbuf->buflen = PAGE_SIZE;\r\nreturn 0;\r\n}\r\nstatic\r\nstruct rpc_rqst *xprt_alloc_bc_req(struct rpc_xprt *xprt, gfp_t gfp_flags)\r\n{\r\nstruct rpc_rqst *req;\r\nreq = kzalloc(sizeof(*req), gfp_flags);\r\nif (req == NULL)\r\nreturn NULL;\r\nreq->rq_xprt = xprt;\r\nINIT_LIST_HEAD(&req->rq_list);\r\nINIT_LIST_HEAD(&req->rq_bc_list);\r\nif (xprt_alloc_xdr_buf(&req->rq_rcv_buf, gfp_flags) < 0) {\r\nprintk(KERN_ERR "Failed to create bc receive xbuf\n");\r\ngoto out_free;\r\n}\r\nreq->rq_rcv_buf.len = PAGE_SIZE;\r\nif (xprt_alloc_xdr_buf(&req->rq_snd_buf, gfp_flags) < 0) {\r\nprintk(KERN_ERR "Failed to create bc snd xbuf\n");\r\ngoto out_free;\r\n}\r\nreturn req;\r\nout_free:\r\nxprt_free_allocation(req);\r\nreturn NULL;\r\n}\r\nint xprt_setup_backchannel(struct rpc_xprt *xprt, unsigned int min_reqs)\r\n{\r\nstruct rpc_rqst *req;\r\nstruct list_head tmp_list;\r\nint i;\r\ndprintk("RPC: setup backchannel transport\n");\r\nINIT_LIST_HEAD(&tmp_list);\r\nfor (i = 0; i < min_reqs; i++) {\r\nreq = xprt_alloc_bc_req(xprt, GFP_KERNEL);\r\nif (req == NULL) {\r\nprintk(KERN_ERR "Failed to create bc rpc_rqst\n");\r\ngoto out_free;\r\n}\r\ndprintk("RPC: adding req= %p\n", req);\r\nlist_add(&req->rq_bc_pa_list, &tmp_list);\r\n}\r\nspin_lock_bh(&xprt->bc_pa_lock);\r\nlist_splice(&tmp_list, &xprt->bc_pa_list);\r\nxprt_inc_alloc_count(xprt, min_reqs);\r\nspin_unlock_bh(&xprt->bc_pa_lock);\r\ndprintk("RPC: setup backchannel transport done\n");\r\nreturn 0;\r\nout_free:\r\nwhile (!list_empty(&tmp_list)) {\r\nreq = list_first_entry(&tmp_list,\r\nstruct rpc_rqst,\r\nrq_bc_pa_list);\r\nlist_del(&req->rq_bc_pa_list);\r\nxprt_free_allocation(req);\r\n}\r\ndprintk("RPC: setup backchannel transport failed\n");\r\nreturn -ENOMEM;\r\n}\r\nvoid xprt_destroy_backchannel(struct rpc_xprt *xprt, unsigned int max_reqs)\r\n{\r\nstruct rpc_rqst *req = NULL, *tmp = NULL;\r\ndprintk("RPC: destroy backchannel transport\n");\r\nif (max_reqs == 0)\r\ngoto out;\r\nspin_lock_bh(&xprt->bc_pa_lock);\r\nxprt_dec_alloc_count(xprt, max_reqs);\r\nlist_for_each_entry_safe(req, tmp, &xprt->bc_pa_list, rq_bc_pa_list) {\r\ndprintk("RPC: req=%p\n", req);\r\nlist_del(&req->rq_bc_pa_list);\r\nxprt_free_allocation(req);\r\nif (--max_reqs == 0)\r\nbreak;\r\n}\r\nspin_unlock_bh(&xprt->bc_pa_lock);\r\nout:\r\ndprintk("RPC: backchannel list empty= %s\n",\r\nlist_empty(&xprt->bc_pa_list) ? "true" : "false");\r\n}\r\nstatic struct rpc_rqst *xprt_alloc_bc_request(struct rpc_xprt *xprt, __be32 xid)\r\n{\r\nstruct rpc_rqst *req = NULL;\r\ndprintk("RPC: allocate a backchannel request\n");\r\nif (atomic_read(&xprt->bc_free_slots) <= 0)\r\ngoto not_found;\r\nif (list_empty(&xprt->bc_pa_list)) {\r\nreq = xprt_alloc_bc_req(xprt, GFP_ATOMIC);\r\nif (!req)\r\ngoto not_found;\r\nlist_add_tail(&req->rq_bc_pa_list, &xprt->bc_pa_list);\r\nxprt->bc_alloc_count++;\r\n}\r\nreq = list_first_entry(&xprt->bc_pa_list, struct rpc_rqst,\r\nrq_bc_pa_list);\r\nreq->rq_reply_bytes_recvd = 0;\r\nreq->rq_bytes_sent = 0;\r\nmemcpy(&req->rq_private_buf, &req->rq_rcv_buf,\r\nsizeof(req->rq_private_buf));\r\nreq->rq_xid = xid;\r\nreq->rq_connect_cookie = xprt->connect_cookie;\r\nnot_found:\r\ndprintk("RPC: backchannel req=%p\n", req);\r\nreturn req;\r\n}\r\nvoid xprt_free_bc_request(struct rpc_rqst *req)\r\n{\r\nstruct rpc_xprt *xprt = req->rq_xprt;\r\ndprintk("RPC: free backchannel req=%p\n", req);\r\nreq->rq_connect_cookie = xprt->connect_cookie - 1;\r\nsmp_mb__before_atomic();\r\nclear_bit(RPC_BC_PA_IN_USE, &req->rq_bc_pa_state);\r\nsmp_mb__after_atomic();\r\nspin_lock_bh(&xprt->bc_pa_lock);\r\nif (xprt_need_to_requeue(xprt)) {\r\nlist_add_tail(&req->rq_bc_pa_list, &xprt->bc_pa_list);\r\nxprt->bc_alloc_count++;\r\nreq = NULL;\r\n}\r\nspin_unlock_bh(&xprt->bc_pa_lock);\r\nif (req != NULL) {\r\ndprintk("RPC: Last session removed req=%p\n", req);\r\nxprt_free_allocation(req);\r\nreturn;\r\n}\r\n}\r\nstruct rpc_rqst *xprt_lookup_bc_request(struct rpc_xprt *xprt, __be32 xid)\r\n{\r\nstruct rpc_rqst *req;\r\nspin_lock(&xprt->bc_pa_lock);\r\nlist_for_each_entry(req, &xprt->bc_pa_list, rq_bc_pa_list) {\r\nif (req->rq_connect_cookie != xprt->connect_cookie)\r\ncontinue;\r\nif (req->rq_xid == xid)\r\ngoto found;\r\n}\r\nreq = xprt_alloc_bc_request(xprt, xid);\r\nfound:\r\nspin_unlock(&xprt->bc_pa_lock);\r\nreturn req;\r\n}\r\nvoid xprt_complete_bc_request(struct rpc_rqst *req, uint32_t copied)\r\n{\r\nstruct rpc_xprt *xprt = req->rq_xprt;\r\nstruct svc_serv *bc_serv = xprt->bc_serv;\r\nspin_lock(&xprt->bc_pa_lock);\r\nlist_del(&req->rq_bc_pa_list);\r\nxprt_dec_alloc_count(xprt, 1);\r\nspin_unlock(&xprt->bc_pa_lock);\r\nreq->rq_private_buf.len = copied;\r\nset_bit(RPC_BC_PA_IN_USE, &req->rq_bc_pa_state);\r\ndprintk("RPC: add callback request to list\n");\r\nspin_lock(&bc_serv->sv_cb_lock);\r\nlist_add(&req->rq_bc_list, &bc_serv->sv_cb_list);\r\nwake_up(&bc_serv->sv_cb_waitq);\r\nspin_unlock(&bc_serv->sv_cb_lock);\r\n}
