static void mvpp2_write(struct mvpp2 *priv, u32 offset, u32 data)\r\n{\r\nwritel(data, priv->base + offset);\r\n}\r\nstatic u32 mvpp2_read(struct mvpp2 *priv, u32 offset)\r\n{\r\nreturn readl(priv->base + offset);\r\n}\r\nstatic void mvpp2_txq_inc_get(struct mvpp2_txq_pcpu *txq_pcpu)\r\n{\r\ntxq_pcpu->txq_get_index++;\r\nif (txq_pcpu->txq_get_index == txq_pcpu->size)\r\ntxq_pcpu->txq_get_index = 0;\r\n}\r\nstatic void mvpp2_txq_inc_put(struct mvpp2_txq_pcpu *txq_pcpu,\r\nstruct sk_buff *skb,\r\nstruct mvpp2_tx_desc *tx_desc)\r\n{\r\ntxq_pcpu->tx_skb[txq_pcpu->txq_put_index] = skb;\r\nif (skb)\r\ntxq_pcpu->tx_buffs[txq_pcpu->txq_put_index] =\r\ntx_desc->buf_phys_addr;\r\ntxq_pcpu->txq_put_index++;\r\nif (txq_pcpu->txq_put_index == txq_pcpu->size)\r\ntxq_pcpu->txq_put_index = 0;\r\n}\r\nstatic inline int mvpp2_egress_port(struct mvpp2_port *port)\r\n{\r\nreturn MVPP2_MAX_TCONT + port->id;\r\n}\r\nstatic inline int mvpp2_txq_phys(int port, int txq)\r\n{\r\nreturn (MVPP2_MAX_TCONT + port) * MVPP2_MAX_TXQ + txq;\r\n}\r\nstatic int mvpp2_prs_hw_write(struct mvpp2 *priv, struct mvpp2_prs_entry *pe)\r\n{\r\nint i;\r\nif (pe->index > MVPP2_PRS_TCAM_SRAM_SIZE - 1)\r\nreturn -EINVAL;\r\npe->tcam.word[MVPP2_PRS_TCAM_INV_WORD] &= ~MVPP2_PRS_TCAM_INV_MASK;\r\nmvpp2_write(priv, MVPP2_PRS_TCAM_IDX_REG, pe->index);\r\nfor (i = 0; i < MVPP2_PRS_TCAM_WORDS; i++)\r\nmvpp2_write(priv, MVPP2_PRS_TCAM_DATA_REG(i), pe->tcam.word[i]);\r\nmvpp2_write(priv, MVPP2_PRS_SRAM_IDX_REG, pe->index);\r\nfor (i = 0; i < MVPP2_PRS_SRAM_WORDS; i++)\r\nmvpp2_write(priv, MVPP2_PRS_SRAM_DATA_REG(i), pe->sram.word[i]);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_hw_read(struct mvpp2 *priv, struct mvpp2_prs_entry *pe)\r\n{\r\nint i;\r\nif (pe->index > MVPP2_PRS_TCAM_SRAM_SIZE - 1)\r\nreturn -EINVAL;\r\nmvpp2_write(priv, MVPP2_PRS_TCAM_IDX_REG, pe->index);\r\npe->tcam.word[MVPP2_PRS_TCAM_INV_WORD] = mvpp2_read(priv,\r\nMVPP2_PRS_TCAM_DATA_REG(MVPP2_PRS_TCAM_INV_WORD));\r\nif (pe->tcam.word[MVPP2_PRS_TCAM_INV_WORD] & MVPP2_PRS_TCAM_INV_MASK)\r\nreturn MVPP2_PRS_TCAM_ENTRY_INVALID;\r\nfor (i = 0; i < MVPP2_PRS_TCAM_WORDS; i++)\r\npe->tcam.word[i] = mvpp2_read(priv, MVPP2_PRS_TCAM_DATA_REG(i));\r\nmvpp2_write(priv, MVPP2_PRS_SRAM_IDX_REG, pe->index);\r\nfor (i = 0; i < MVPP2_PRS_SRAM_WORDS; i++)\r\npe->sram.word[i] = mvpp2_read(priv, MVPP2_PRS_SRAM_DATA_REG(i));\r\nreturn 0;\r\n}\r\nstatic void mvpp2_prs_hw_inv(struct mvpp2 *priv, int index)\r\n{\r\nmvpp2_write(priv, MVPP2_PRS_TCAM_IDX_REG, index);\r\nmvpp2_write(priv, MVPP2_PRS_TCAM_DATA_REG(MVPP2_PRS_TCAM_INV_WORD),\r\nMVPP2_PRS_TCAM_INV_MASK);\r\n}\r\nstatic void mvpp2_prs_shadow_set(struct mvpp2 *priv, int index, int lu)\r\n{\r\npriv->prs_shadow[index].valid = true;\r\npriv->prs_shadow[index].lu = lu;\r\n}\r\nstatic void mvpp2_prs_shadow_ri_set(struct mvpp2 *priv, int index,\r\nunsigned int ri, unsigned int ri_mask)\r\n{\r\npriv->prs_shadow[index].ri_mask = ri_mask;\r\npriv->prs_shadow[index].ri = ri;\r\n}\r\nstatic void mvpp2_prs_tcam_lu_set(struct mvpp2_prs_entry *pe, unsigned int lu)\r\n{\r\nint enable_off = MVPP2_PRS_TCAM_EN_OFFS(MVPP2_PRS_TCAM_LU_BYTE);\r\npe->tcam.byte[MVPP2_PRS_TCAM_LU_BYTE] = lu;\r\npe->tcam.byte[enable_off] = MVPP2_PRS_LU_MASK;\r\n}\r\nstatic void mvpp2_prs_tcam_port_set(struct mvpp2_prs_entry *pe,\r\nunsigned int port, bool add)\r\n{\r\nint enable_off = MVPP2_PRS_TCAM_EN_OFFS(MVPP2_PRS_TCAM_PORT_BYTE);\r\nif (add)\r\npe->tcam.byte[enable_off] &= ~(1 << port);\r\nelse\r\npe->tcam.byte[enable_off] |= 1 << port;\r\n}\r\nstatic void mvpp2_prs_tcam_port_map_set(struct mvpp2_prs_entry *pe,\r\nunsigned int ports)\r\n{\r\nunsigned char port_mask = MVPP2_PRS_PORT_MASK;\r\nint enable_off = MVPP2_PRS_TCAM_EN_OFFS(MVPP2_PRS_TCAM_PORT_BYTE);\r\npe->tcam.byte[MVPP2_PRS_TCAM_PORT_BYTE] = 0;\r\npe->tcam.byte[enable_off] &= ~port_mask;\r\npe->tcam.byte[enable_off] |= ~ports & MVPP2_PRS_PORT_MASK;\r\n}\r\nstatic unsigned int mvpp2_prs_tcam_port_map_get(struct mvpp2_prs_entry *pe)\r\n{\r\nint enable_off = MVPP2_PRS_TCAM_EN_OFFS(MVPP2_PRS_TCAM_PORT_BYTE);\r\nreturn ~(pe->tcam.byte[enable_off]) & MVPP2_PRS_PORT_MASK;\r\n}\r\nstatic void mvpp2_prs_tcam_data_byte_set(struct mvpp2_prs_entry *pe,\r\nunsigned int offs, unsigned char byte,\r\nunsigned char enable)\r\n{\r\npe->tcam.byte[MVPP2_PRS_TCAM_DATA_BYTE(offs)] = byte;\r\npe->tcam.byte[MVPP2_PRS_TCAM_DATA_BYTE_EN(offs)] = enable;\r\n}\r\nstatic void mvpp2_prs_tcam_data_byte_get(struct mvpp2_prs_entry *pe,\r\nunsigned int offs, unsigned char *byte,\r\nunsigned char *enable)\r\n{\r\n*byte = pe->tcam.byte[MVPP2_PRS_TCAM_DATA_BYTE(offs)];\r\n*enable = pe->tcam.byte[MVPP2_PRS_TCAM_DATA_BYTE_EN(offs)];\r\n}\r\nstatic bool mvpp2_prs_tcam_data_cmp(struct mvpp2_prs_entry *pe, int offs,\r\nu16 data)\r\n{\r\nint off = MVPP2_PRS_TCAM_DATA_BYTE(offs);\r\nu16 tcam_data;\r\ntcam_data = (8 << pe->tcam.byte[off + 1]) | pe->tcam.byte[off];\r\nif (tcam_data != data)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic void mvpp2_prs_tcam_ai_update(struct mvpp2_prs_entry *pe,\r\nunsigned int bits, unsigned int enable)\r\n{\r\nint i, ai_idx = MVPP2_PRS_TCAM_AI_BYTE;\r\nfor (i = 0; i < MVPP2_PRS_AI_BITS; i++) {\r\nif (!(enable & BIT(i)))\r\ncontinue;\r\nif (bits & BIT(i))\r\npe->tcam.byte[ai_idx] |= 1 << i;\r\nelse\r\npe->tcam.byte[ai_idx] &= ~(1 << i);\r\n}\r\npe->tcam.byte[MVPP2_PRS_TCAM_EN_OFFS(ai_idx)] |= enable;\r\n}\r\nstatic int mvpp2_prs_tcam_ai_get(struct mvpp2_prs_entry *pe)\r\n{\r\nreturn pe->tcam.byte[MVPP2_PRS_TCAM_AI_BYTE];\r\n}\r\nstatic void mvpp2_prs_match_etype(struct mvpp2_prs_entry *pe, int offset,\r\nunsigned short ethertype)\r\n{\r\nmvpp2_prs_tcam_data_byte_set(pe, offset + 0, ethertype >> 8, 0xff);\r\nmvpp2_prs_tcam_data_byte_set(pe, offset + 1, ethertype & 0xff, 0xff);\r\n}\r\nstatic void mvpp2_prs_sram_bits_set(struct mvpp2_prs_entry *pe, int bit_num,\r\nint val)\r\n{\r\npe->sram.byte[MVPP2_BIT_TO_BYTE(bit_num)] |= (val << (bit_num % 8));\r\n}\r\nstatic void mvpp2_prs_sram_bits_clear(struct mvpp2_prs_entry *pe, int bit_num,\r\nint val)\r\n{\r\npe->sram.byte[MVPP2_BIT_TO_BYTE(bit_num)] &= ~(val << (bit_num % 8));\r\n}\r\nstatic void mvpp2_prs_sram_ri_update(struct mvpp2_prs_entry *pe,\r\nunsigned int bits, unsigned int mask)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < MVPP2_PRS_SRAM_RI_CTRL_BITS; i++) {\r\nint ri_off = MVPP2_PRS_SRAM_RI_OFFS;\r\nif (!(mask & BIT(i)))\r\ncontinue;\r\nif (bits & BIT(i))\r\nmvpp2_prs_sram_bits_set(pe, ri_off + i, 1);\r\nelse\r\nmvpp2_prs_sram_bits_clear(pe, ri_off + i, 1);\r\nmvpp2_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_RI_CTRL_OFFS + i, 1);\r\n}\r\n}\r\nstatic int mvpp2_prs_sram_ri_get(struct mvpp2_prs_entry *pe)\r\n{\r\nreturn pe->sram.word[MVPP2_PRS_SRAM_RI_WORD];\r\n}\r\nstatic void mvpp2_prs_sram_ai_update(struct mvpp2_prs_entry *pe,\r\nunsigned int bits, unsigned int mask)\r\n{\r\nunsigned int i;\r\nint ai_off = MVPP2_PRS_SRAM_AI_OFFS;\r\nfor (i = 0; i < MVPP2_PRS_SRAM_AI_CTRL_BITS; i++) {\r\nif (!(mask & BIT(i)))\r\ncontinue;\r\nif (bits & BIT(i))\r\nmvpp2_prs_sram_bits_set(pe, ai_off + i, 1);\r\nelse\r\nmvpp2_prs_sram_bits_clear(pe, ai_off + i, 1);\r\nmvpp2_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_AI_CTRL_OFFS + i, 1);\r\n}\r\n}\r\nstatic int mvpp2_prs_sram_ai_get(struct mvpp2_prs_entry *pe)\r\n{\r\nu8 bits;\r\nint ai_off = MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_AI_OFFS);\r\nint ai_en_off = ai_off + 1;\r\nint ai_shift = MVPP2_PRS_SRAM_AI_OFFS % 8;\r\nbits = (pe->sram.byte[ai_off] >> ai_shift) |\r\n(pe->sram.byte[ai_en_off] << (8 - ai_shift));\r\nreturn bits;\r\n}\r\nstatic void mvpp2_prs_sram_next_lu_set(struct mvpp2_prs_entry *pe,\r\nunsigned int lu)\r\n{\r\nint sram_next_off = MVPP2_PRS_SRAM_NEXT_LU_OFFS;\r\nmvpp2_prs_sram_bits_clear(pe, sram_next_off,\r\nMVPP2_PRS_SRAM_NEXT_LU_MASK);\r\nmvpp2_prs_sram_bits_set(pe, sram_next_off, lu);\r\n}\r\nstatic void mvpp2_prs_sram_shift_set(struct mvpp2_prs_entry *pe, int shift,\r\nunsigned int op)\r\n{\r\nif (shift < 0) {\r\nmvpp2_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_SHIFT_SIGN_BIT, 1);\r\nshift = 0 - shift;\r\n} else {\r\nmvpp2_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_SHIFT_SIGN_BIT, 1);\r\n}\r\npe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_SHIFT_OFFS)] =\r\n(unsigned char)shift;\r\nmvpp2_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_MASK);\r\nmvpp2_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_OP_SEL_SHIFT_OFFS, op);\r\nmvpp2_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_OP_SEL_BASE_OFFS, 1);\r\n}\r\nstatic void mvpp2_prs_sram_offset_set(struct mvpp2_prs_entry *pe,\r\nunsigned int type, int offset,\r\nunsigned int op)\r\n{\r\nif (offset < 0) {\r\nmvpp2_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_UDF_SIGN_BIT, 1);\r\noffset = 0 - offset;\r\n} else {\r\nmvpp2_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_UDF_SIGN_BIT, 1);\r\n}\r\nmvpp2_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_UDF_OFFS,\r\nMVPP2_PRS_SRAM_UDF_MASK);\r\nmvpp2_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_UDF_OFFS, offset);\r\npe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_UDF_OFFS +\r\nMVPP2_PRS_SRAM_UDF_BITS)] &=\r\n~(MVPP2_PRS_SRAM_UDF_MASK >> (8 - (MVPP2_PRS_SRAM_UDF_OFFS % 8)));\r\npe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_UDF_OFFS +\r\nMVPP2_PRS_SRAM_UDF_BITS)] |=\r\n(offset >> (8 - (MVPP2_PRS_SRAM_UDF_OFFS % 8)));\r\nmvpp2_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_UDF_TYPE_OFFS,\r\nMVPP2_PRS_SRAM_UDF_TYPE_MASK);\r\nmvpp2_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_UDF_TYPE_OFFS, type);\r\nmvpp2_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_MASK);\r\nmvpp2_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS, op);\r\npe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS +\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_BITS)] &=\r\n~(MVPP2_PRS_SRAM_OP_SEL_UDF_MASK >>\r\n(8 - (MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS % 8)));\r\npe->sram.byte[MVPP2_BIT_TO_BYTE(MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS +\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_BITS)] |=\r\n(op >> (8 - (MVPP2_PRS_SRAM_OP_SEL_UDF_OFFS % 8)));\r\nmvpp2_prs_sram_bits_clear(pe, MVPP2_PRS_SRAM_OP_SEL_BASE_OFFS, 1);\r\n}\r\nstatic struct mvpp2_prs_entry *mvpp2_prs_flow_find(struct mvpp2 *priv, int flow)\r\n{\r\nstruct mvpp2_prs_entry *pe;\r\nint tid;\r\npe = kzalloc(sizeof(*pe), GFP_KERNEL);\r\nif (!pe)\r\nreturn NULL;\r\nmvpp2_prs_tcam_lu_set(pe, MVPP2_PRS_LU_FLOWS);\r\nfor (tid = MVPP2_PRS_TCAM_SRAM_SIZE - 1; tid >= 0; tid--) {\r\nu8 bits;\r\nif (!priv->prs_shadow[tid].valid ||\r\npriv->prs_shadow[tid].lu != MVPP2_PRS_LU_FLOWS)\r\ncontinue;\r\npe->index = tid;\r\nmvpp2_prs_hw_read(priv, pe);\r\nbits = mvpp2_prs_sram_ai_get(pe);\r\nif ((bits & MVPP2_PRS_FLOW_ID_MASK) == flow)\r\nreturn pe;\r\n}\r\nkfree(pe);\r\nreturn NULL;\r\n}\r\nstatic int mvpp2_prs_tcam_first_free(struct mvpp2 *priv, unsigned char start,\r\nunsigned char end)\r\n{\r\nint tid;\r\nif (start > end)\r\nswap(start, end);\r\nif (end >= MVPP2_PRS_TCAM_SRAM_SIZE)\r\nend = MVPP2_PRS_TCAM_SRAM_SIZE - 1;\r\nfor (tid = start; tid <= end; tid++) {\r\nif (!priv->prs_shadow[tid].valid)\r\nreturn tid;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic void mvpp2_prs_mac_drop_all_set(struct mvpp2 *priv, int port, bool add)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nif (priv->prs_shadow[MVPP2_PE_DROP_ALL].valid) {\r\npe.index = MVPP2_PE_DROP_ALL;\r\nmvpp2_prs_hw_read(priv, &pe);\r\n} else {\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);\r\npe.index = MVPP2_PE_DROP_ALL;\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_DROP_MASK,\r\nMVPP2_PRS_RI_DROP_MASK);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_MAC);\r\nmvpp2_prs_tcam_port_map_set(&pe, 0);\r\n}\r\nmvpp2_prs_tcam_port_set(&pe, port, add);\r\nmvpp2_prs_hw_write(priv, &pe);\r\n}\r\nstatic void mvpp2_prs_mac_promisc_set(struct mvpp2 *priv, int port, bool add)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nif (priv->prs_shadow[MVPP2_PE_MAC_PROMISCUOUS].valid) {\r\npe.index = MVPP2_PE_MAC_PROMISCUOUS;\r\nmvpp2_prs_hw_read(priv, &pe);\r\n} else {\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);\r\npe.index = MVPP2_PE_MAC_PROMISCUOUS;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_DSA);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L2_UCAST,\r\nMVPP2_PRS_RI_L2_CAST_MASK);\r\nmvpp2_prs_sram_shift_set(&pe, 2 * ETH_ALEN,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_tcam_port_map_set(&pe, 0);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_MAC);\r\n}\r\nmvpp2_prs_tcam_port_set(&pe, port, add);\r\nmvpp2_prs_hw_write(priv, &pe);\r\n}\r\nstatic void mvpp2_prs_mac_multi_set(struct mvpp2 *priv, int port, int index,\r\nbool add)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nunsigned char da_mc;\r\nda_mc = (index == MVPP2_PE_MAC_MC_ALL) ? 0x01 : 0x33;\r\nif (priv->prs_shadow[index].valid) {\r\npe.index = index;\r\nmvpp2_prs_hw_read(priv, &pe);\r\n} else {\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);\r\npe.index = index;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_DSA);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L2_MCAST,\r\nMVPP2_PRS_RI_L2_CAST_MASK);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 0, da_mc, 0xff);\r\nmvpp2_prs_sram_shift_set(&pe, 2 * ETH_ALEN,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_tcam_port_map_set(&pe, 0);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_MAC);\r\n}\r\nmvpp2_prs_tcam_port_set(&pe, port, add);\r\nmvpp2_prs_hw_write(priv, &pe);\r\n}\r\nstatic void mvpp2_prs_dsa_tag_set(struct mvpp2 *priv, int port, bool add,\r\nbool tagged, bool extend)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint tid, shift;\r\nif (extend) {\r\ntid = tagged ? MVPP2_PE_EDSA_TAGGED : MVPP2_PE_EDSA_UNTAGGED;\r\nshift = 8;\r\n} else {\r\ntid = tagged ? MVPP2_PE_DSA_TAGGED : MVPP2_PE_DSA_UNTAGGED;\r\nshift = 4;\r\n}\r\nif (priv->prs_shadow[tid].valid) {\r\npe.index = tid;\r\nmvpp2_prs_hw_read(priv, &pe);\r\n} else {\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_DSA);\r\npe.index = tid;\r\nmvpp2_prs_sram_shift_set(&pe, shift,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_DSA);\r\nif (tagged) {\r\nmvpp2_prs_tcam_data_byte_set(&pe, 0,\r\nMVPP2_PRS_TCAM_DSA_TAGGED_BIT,\r\nMVPP2_PRS_TCAM_DSA_TAGGED_BIT);\r\nmvpp2_prs_sram_ai_update(&pe, 0,\r\nMVPP2_PRS_SRAM_AI_MASK);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_VLAN);\r\n} else {\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_NONE,\r\nMVPP2_PRS_RI_VLAN_MASK);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);\r\n}\r\nmvpp2_prs_tcam_port_map_set(&pe, 0);\r\n}\r\nmvpp2_prs_tcam_port_set(&pe, port, add);\r\nmvpp2_prs_hw_write(priv, &pe);\r\n}\r\nstatic void mvpp2_prs_dsa_tag_ethertype_set(struct mvpp2 *priv, int port,\r\nbool add, bool tagged, bool extend)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint tid, shift, port_mask;\r\nif (extend) {\r\ntid = tagged ? MVPP2_PE_ETYPE_EDSA_TAGGED :\r\nMVPP2_PE_ETYPE_EDSA_UNTAGGED;\r\nport_mask = 0;\r\nshift = 8;\r\n} else {\r\ntid = tagged ? MVPP2_PE_ETYPE_DSA_TAGGED :\r\nMVPP2_PE_ETYPE_DSA_UNTAGGED;\r\nport_mask = MVPP2_PRS_PORT_MASK;\r\nshift = 4;\r\n}\r\nif (priv->prs_shadow[tid].valid) {\r\npe.index = tid;\r\nmvpp2_prs_hw_read(priv, &pe);\r\n} else {\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_DSA);\r\npe.index = tid;\r\nmvpp2_prs_match_etype(&pe, 0, ETH_P_EDSA);\r\nmvpp2_prs_match_etype(&pe, 2, 0);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_DSA_MASK,\r\nMVPP2_PRS_RI_DSA_MASK);\r\nmvpp2_prs_sram_shift_set(&pe, 2 + MVPP2_ETH_TYPE_LEN + shift,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_DSA);\r\nif (tagged) {\r\nmvpp2_prs_tcam_data_byte_set(&pe,\r\nMVPP2_ETH_TYPE_LEN + 2 + 3,\r\nMVPP2_PRS_TCAM_DSA_TAGGED_BIT,\r\nMVPP2_PRS_TCAM_DSA_TAGGED_BIT);\r\nmvpp2_prs_sram_ai_update(&pe, 0,\r\nMVPP2_PRS_SRAM_AI_MASK);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_VLAN);\r\n} else {\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_NONE,\r\nMVPP2_PRS_RI_VLAN_MASK);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);\r\n}\r\nmvpp2_prs_tcam_port_map_set(&pe, port_mask);\r\n}\r\nmvpp2_prs_tcam_port_set(&pe, port, add);\r\nmvpp2_prs_hw_write(priv, &pe);\r\n}\r\nstatic struct mvpp2_prs_entry *mvpp2_prs_vlan_find(struct mvpp2 *priv,\r\nunsigned short tpid, int ai)\r\n{\r\nstruct mvpp2_prs_entry *pe;\r\nint tid;\r\npe = kzalloc(sizeof(*pe), GFP_KERNEL);\r\nif (!pe)\r\nreturn NULL;\r\nmvpp2_prs_tcam_lu_set(pe, MVPP2_PRS_LU_VLAN);\r\nfor (tid = MVPP2_PE_FIRST_FREE_TID;\r\ntid <= MVPP2_PE_LAST_FREE_TID; tid++) {\r\nunsigned int ri_bits, ai_bits;\r\nbool match;\r\nif (!priv->prs_shadow[tid].valid ||\r\npriv->prs_shadow[tid].lu != MVPP2_PRS_LU_VLAN)\r\ncontinue;\r\npe->index = tid;\r\nmvpp2_prs_hw_read(priv, pe);\r\nmatch = mvpp2_prs_tcam_data_cmp(pe, 0, swab16(tpid));\r\nif (!match)\r\ncontinue;\r\nri_bits = mvpp2_prs_sram_ri_get(pe);\r\nri_bits &= MVPP2_PRS_RI_VLAN_MASK;\r\nai_bits = mvpp2_prs_tcam_ai_get(pe);\r\nai_bits &= ~MVPP2_PRS_DBL_VLAN_AI_BIT;\r\nif (ai != ai_bits)\r\ncontinue;\r\nif (ri_bits == MVPP2_PRS_RI_VLAN_SINGLE ||\r\nri_bits == MVPP2_PRS_RI_VLAN_TRIPLE)\r\nreturn pe;\r\n}\r\nkfree(pe);\r\nreturn NULL;\r\n}\r\nstatic int mvpp2_prs_vlan_add(struct mvpp2 *priv, unsigned short tpid, int ai,\r\nunsigned int port_map)\r\n{\r\nstruct mvpp2_prs_entry *pe;\r\nint tid_aux, tid;\r\nint ret = 0;\r\npe = mvpp2_prs_vlan_find(priv, tpid, ai);\r\nif (!pe) {\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_LAST_FREE_TID,\r\nMVPP2_PE_FIRST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\npe = kzalloc(sizeof(*pe), GFP_KERNEL);\r\nif (!pe)\r\nreturn -ENOMEM;\r\nfor (tid_aux = MVPP2_PE_LAST_FREE_TID;\r\ntid_aux >= MVPP2_PE_FIRST_FREE_TID; tid_aux--) {\r\nunsigned int ri_bits;\r\nif (!priv->prs_shadow[tid_aux].valid ||\r\npriv->prs_shadow[tid_aux].lu != MVPP2_PRS_LU_VLAN)\r\ncontinue;\r\npe->index = tid_aux;\r\nmvpp2_prs_hw_read(priv, pe);\r\nri_bits = mvpp2_prs_sram_ri_get(pe);\r\nif ((ri_bits & MVPP2_PRS_RI_VLAN_MASK) ==\r\nMVPP2_PRS_RI_VLAN_DOUBLE)\r\nbreak;\r\n}\r\nif (tid <= tid_aux) {\r\nret = -EINVAL;\r\ngoto error;\r\n}\r\nmemset(pe, 0 , sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(pe, MVPP2_PRS_LU_VLAN);\r\npe->index = tid;\r\nmvpp2_prs_match_etype(pe, 0, tpid);\r\nmvpp2_prs_sram_next_lu_set(pe, MVPP2_PRS_LU_L2);\r\nmvpp2_prs_sram_shift_set(pe, MVPP2_VLAN_TAG_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_ai_update(pe, 0, MVPP2_PRS_SRAM_AI_MASK);\r\nif (ai == MVPP2_PRS_SINGLE_VLAN_AI) {\r\nmvpp2_prs_sram_ri_update(pe, MVPP2_PRS_RI_VLAN_SINGLE,\r\nMVPP2_PRS_RI_VLAN_MASK);\r\n} else {\r\nai |= MVPP2_PRS_DBL_VLAN_AI_BIT;\r\nmvpp2_prs_sram_ri_update(pe, MVPP2_PRS_RI_VLAN_TRIPLE,\r\nMVPP2_PRS_RI_VLAN_MASK);\r\n}\r\nmvpp2_prs_tcam_ai_update(pe, ai, MVPP2_PRS_SRAM_AI_MASK);\r\nmvpp2_prs_shadow_set(priv, pe->index, MVPP2_PRS_LU_VLAN);\r\n}\r\nmvpp2_prs_tcam_port_map_set(pe, port_map);\r\nmvpp2_prs_hw_write(priv, pe);\r\nerror:\r\nkfree(pe);\r\nreturn ret;\r\n}\r\nstatic int mvpp2_prs_double_vlan_ai_free_get(struct mvpp2 *priv)\r\n{\r\nint i;\r\nfor (i = 1; i < MVPP2_PRS_DBL_VLANS_MAX; i++) {\r\nif (!priv->prs_double_vlans[i])\r\nreturn i;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic struct mvpp2_prs_entry *mvpp2_prs_double_vlan_find(struct mvpp2 *priv,\r\nunsigned short tpid1,\r\nunsigned short tpid2)\r\n{\r\nstruct mvpp2_prs_entry *pe;\r\nint tid;\r\npe = kzalloc(sizeof(*pe), GFP_KERNEL);\r\nif (!pe)\r\nreturn NULL;\r\nmvpp2_prs_tcam_lu_set(pe, MVPP2_PRS_LU_VLAN);\r\nfor (tid = MVPP2_PE_FIRST_FREE_TID;\r\ntid <= MVPP2_PE_LAST_FREE_TID; tid++) {\r\nunsigned int ri_mask;\r\nbool match;\r\nif (!priv->prs_shadow[tid].valid ||\r\npriv->prs_shadow[tid].lu != MVPP2_PRS_LU_VLAN)\r\ncontinue;\r\npe->index = tid;\r\nmvpp2_prs_hw_read(priv, pe);\r\nmatch = mvpp2_prs_tcam_data_cmp(pe, 0, swab16(tpid1))\r\n&& mvpp2_prs_tcam_data_cmp(pe, 4, swab16(tpid2));\r\nif (!match)\r\ncontinue;\r\nri_mask = mvpp2_prs_sram_ri_get(pe) & MVPP2_PRS_RI_VLAN_MASK;\r\nif (ri_mask == MVPP2_PRS_RI_VLAN_DOUBLE)\r\nreturn pe;\r\n}\r\nkfree(pe);\r\nreturn NULL;\r\n}\r\nstatic int mvpp2_prs_double_vlan_add(struct mvpp2 *priv, unsigned short tpid1,\r\nunsigned short tpid2,\r\nunsigned int port_map)\r\n{\r\nstruct mvpp2_prs_entry *pe;\r\nint tid_aux, tid, ai, ret = 0;\r\npe = mvpp2_prs_double_vlan_find(priv, tpid1, tpid2);\r\nif (!pe) {\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\npe = kzalloc(sizeof(*pe), GFP_KERNEL);\r\nif (!pe)\r\nreturn -ENOMEM;\r\nai = mvpp2_prs_double_vlan_ai_free_get(priv);\r\nif (ai < 0) {\r\nret = ai;\r\ngoto error;\r\n}\r\nfor (tid_aux = MVPP2_PE_FIRST_FREE_TID;\r\ntid_aux <= MVPP2_PE_LAST_FREE_TID; tid_aux++) {\r\nunsigned int ri_bits;\r\nif (!priv->prs_shadow[tid_aux].valid ||\r\npriv->prs_shadow[tid_aux].lu != MVPP2_PRS_LU_VLAN)\r\ncontinue;\r\npe->index = tid_aux;\r\nmvpp2_prs_hw_read(priv, pe);\r\nri_bits = mvpp2_prs_sram_ri_get(pe);\r\nri_bits &= MVPP2_PRS_RI_VLAN_MASK;\r\nif (ri_bits == MVPP2_PRS_RI_VLAN_SINGLE ||\r\nri_bits == MVPP2_PRS_RI_VLAN_TRIPLE)\r\nbreak;\r\n}\r\nif (tid >= tid_aux) {\r\nret = -ERANGE;\r\ngoto error;\r\n}\r\nmemset(pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(pe, MVPP2_PRS_LU_VLAN);\r\npe->index = tid;\r\npriv->prs_double_vlans[ai] = true;\r\nmvpp2_prs_match_etype(pe, 0, tpid1);\r\nmvpp2_prs_match_etype(pe, 4, tpid2);\r\nmvpp2_prs_sram_next_lu_set(pe, MVPP2_PRS_LU_VLAN);\r\nmvpp2_prs_sram_shift_set(pe, 2 * MVPP2_VLAN_TAG_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_ri_update(pe, MVPP2_PRS_RI_VLAN_DOUBLE,\r\nMVPP2_PRS_RI_VLAN_MASK);\r\nmvpp2_prs_sram_ai_update(pe, ai | MVPP2_PRS_DBL_VLAN_AI_BIT,\r\nMVPP2_PRS_SRAM_AI_MASK);\r\nmvpp2_prs_shadow_set(priv, pe->index, MVPP2_PRS_LU_VLAN);\r\n}\r\nmvpp2_prs_tcam_port_map_set(pe, port_map);\r\nmvpp2_prs_hw_write(priv, pe);\r\nerror:\r\nkfree(pe);\r\nreturn ret;\r\n}\r\nstatic int mvpp2_prs_ip4_proto(struct mvpp2 *priv, unsigned short proto,\r\nunsigned int ri, unsigned int ri_mask)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint tid;\r\nif ((proto != IPPROTO_TCP) && (proto != IPPROTO_UDP) &&\r\n(proto != IPPROTO_IGMP))\r\nreturn -EINVAL;\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);\r\npe.index = tid;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_sram_shift_set(&pe, 12, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,\r\nsizeof(struct iphdr) - 4,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_sram_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,\r\nMVPP2_PRS_IPV4_DIP_AI_BIT);\r\nmvpp2_prs_sram_ri_update(&pe, ri | MVPP2_PRS_RI_IP_FRAG_MASK,\r\nri_mask | MVPP2_PRS_RI_IP_FRAG_MASK);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 5, proto, MVPP2_PRS_TCAM_PROTO_MASK);\r\nmvpp2_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_hw_write(priv, &pe);\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\npe.index = tid;\r\npe.sram.word[MVPP2_PRS_SRAM_RI_WORD] = 0x0;\r\npe.sram.word[MVPP2_PRS_SRAM_RI_CTRL_WORD] = 0x0;\r\nmvpp2_prs_sram_ri_update(&pe, ri, ri_mask);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 2, 0x00, MVPP2_PRS_TCAM_PROTO_MASK_L);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 3, 0x00, MVPP2_PRS_TCAM_PROTO_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_ip4_cast(struct mvpp2 *priv, unsigned short l3_cast)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint mask, tid;\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);\r\npe.index = tid;\r\nswitch (l3_cast) {\r\ncase MVPP2_PRS_L3_MULTI_CAST:\r\nmvpp2_prs_tcam_data_byte_set(&pe, 0, MVPP2_PRS_IPV4_MC,\r\nMVPP2_PRS_IPV4_MC_MASK);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_MCAST,\r\nMVPP2_PRS_RI_L3_ADDR_MASK);\r\nbreak;\r\ncase MVPP2_PRS_L3_BROAD_CAST:\r\nmask = MVPP2_PRS_IPV4_BC_MASK;\r\nmvpp2_prs_tcam_data_byte_set(&pe, 0, mask, mask);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 1, mask, mask);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 2, mask, mask);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 3, mask, mask);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_BCAST,\r\nMVPP2_PRS_RI_L3_ADDR_MASK);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,\r\nMVPP2_PRS_IPV4_DIP_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_ip6_proto(struct mvpp2 *priv, unsigned short proto,\r\nunsigned int ri, unsigned int ri_mask)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint tid;\r\nif ((proto != IPPROTO_TCP) && (proto != IPPROTO_UDP) &&\r\n(proto != IPPROTO_ICMPV6) && (proto != IPPROTO_IPIP))\r\nreturn -EINVAL;\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\npe.index = tid;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_ri_update(&pe, ri, ri_mask);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,\r\nsizeof(struct ipv6hdr) - 6,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 0, proto, MVPP2_PRS_TCAM_PROTO_MASK);\r\nmvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,\r\nMVPP2_PRS_IPV6_NO_EXT_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP6);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_ip6_cast(struct mvpp2 *priv, unsigned short l3_cast)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint tid;\r\nif (l3_cast != MVPP2_PRS_L3_MULTI_CAST)\r\nreturn -EINVAL;\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\npe.index = tid;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_MCAST,\r\nMVPP2_PRS_RI_L3_ADDR_MASK);\r\nmvpp2_prs_sram_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,\r\nMVPP2_PRS_IPV6_NO_EXT_AI_BIT);\r\nmvpp2_prs_sram_shift_set(&pe, -18, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 0, MVPP2_PRS_IPV6_MC,\r\nMVPP2_PRS_IPV6_MC_MASK);\r\nmvpp2_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV6_NO_EXT_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP6);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nreturn 0;\r\n}\r\nstatic void mvpp2_prs_hw_port_init(struct mvpp2 *priv, int port, int lu_first,\r\nint lu_max, int offset)\r\n{\r\nu32 val;\r\nval = mvpp2_read(priv, MVPP2_PRS_INIT_LOOKUP_REG);\r\nval &= ~MVPP2_PRS_PORT_LU_MASK(port);\r\nval |= MVPP2_PRS_PORT_LU_VAL(port, lu_first);\r\nmvpp2_write(priv, MVPP2_PRS_INIT_LOOKUP_REG, val);\r\nval = mvpp2_read(priv, MVPP2_PRS_MAX_LOOP_REG(port));\r\nval &= ~MVPP2_PRS_MAX_LOOP_MASK(port);\r\nval |= MVPP2_PRS_MAX_LOOP_VAL(port, lu_max);\r\nmvpp2_write(priv, MVPP2_PRS_MAX_LOOP_REG(port), val);\r\nval = mvpp2_read(priv, MVPP2_PRS_INIT_OFFS_REG(port));\r\nval &= ~MVPP2_PRS_INIT_OFF_MASK(port);\r\nval |= MVPP2_PRS_INIT_OFF_VAL(port, offset);\r\nmvpp2_write(priv, MVPP2_PRS_INIT_OFFS_REG(port), val);\r\n}\r\nstatic void mvpp2_prs_def_flow_init(struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint port;\r\nfor (port = 0; port < MVPP2_MAX_PORTS; port++) {\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\npe.index = MVPP2_PE_FIRST_DEFAULT_FLOW - port;\r\nmvpp2_prs_tcam_port_map_set(&pe, 0);\r\nmvpp2_prs_sram_ai_update(&pe, port, MVPP2_PRS_FLOW_ID_MASK);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_DONE_BIT, 1);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_hw_write(priv, &pe);\r\n}\r\n}\r\nstatic void mvpp2_prs_mh_init(struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\npe.index = MVPP2_PE_MH_DEFAULT;\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MH);\r\nmvpp2_prs_sram_shift_set(&pe, MVPP2_MH_SIZE,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_MAC);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_MH);\r\nmvpp2_prs_hw_write(priv, &pe);\r\n}\r\nstatic void mvpp2_prs_mac_init(struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\npe.index = MVPP2_PE_MAC_NON_PROMISCUOUS;\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_MAC);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_DROP_MASK,\r\nMVPP2_PRS_RI_DROP_MASK);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_MAC);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nmvpp2_prs_mac_drop_all_set(priv, 0, false);\r\nmvpp2_prs_mac_promisc_set(priv, 0, false);\r\nmvpp2_prs_mac_multi_set(priv, MVPP2_PE_MAC_MC_ALL, 0, false);\r\nmvpp2_prs_mac_multi_set(priv, MVPP2_PE_MAC_MC_IP6, 0, false);\r\n}\r\nstatic void mvpp2_prs_dsa_init(struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nmvpp2_prs_dsa_tag_set(priv, 0, false, MVPP2_PRS_UNTAGGED,\r\nMVPP2_PRS_EDSA);\r\nmvpp2_prs_dsa_tag_set(priv, 0, false, MVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);\r\nmvpp2_prs_dsa_tag_set(priv, 0, false, MVPP2_PRS_UNTAGGED,\r\nMVPP2_PRS_DSA);\r\nmvpp2_prs_dsa_tag_set(priv, 0, false, MVPP2_PRS_TAGGED, MVPP2_PRS_DSA);\r\nmvpp2_prs_dsa_tag_ethertype_set(priv, 0, false,\r\nMVPP2_PRS_UNTAGGED, MVPP2_PRS_EDSA);\r\nmvpp2_prs_dsa_tag_ethertype_set(priv, 0, false,\r\nMVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);\r\nmvpp2_prs_dsa_tag_ethertype_set(priv, 0, true,\r\nMVPP2_PRS_UNTAGGED, MVPP2_PRS_DSA);\r\nmvpp2_prs_dsa_tag_ethertype_set(priv, 0, true,\r\nMVPP2_PRS_TAGGED, MVPP2_PRS_DSA);\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_DSA);\r\npe.index = MVPP2_PE_DSA_DEFAULT;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_VLAN);\r\nmvpp2_prs_sram_shift_set(&pe, 0, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_MAC);\r\nmvpp2_prs_sram_ai_update(&pe, 0, MVPP2_PRS_SRAM_AI_MASK);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_hw_write(priv, &pe);\r\n}\r\nstatic int mvpp2_prs_etype_init(struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint tid;\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);\r\npe.index = tid;\r\nmvpp2_prs_match_etype(&pe, 0, ETH_P_PPP_SES);\r\nmvpp2_prs_sram_shift_set(&pe, MVPP2_PPPOE_HDR_SIZE,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_PPPOE);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_PPPOE_MASK,\r\nMVPP2_PRS_RI_PPPOE_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_L2);\r\npriv->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;\r\npriv->prs_shadow[pe.index].finish = false;\r\nmvpp2_prs_shadow_ri_set(priv, pe.index, MVPP2_PRS_RI_PPPOE_MASK,\r\nMVPP2_PRS_RI_PPPOE_MASK);\r\nmvpp2_prs_hw_write(priv, &pe);\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);\r\npe.index = tid;\r\nmvpp2_prs_match_etype(&pe, 0, ETH_P_ARP);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_ARP,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,\r\nMVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_L2);\r\npriv->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;\r\npriv->prs_shadow[pe.index].finish = true;\r\nmvpp2_prs_shadow_ri_set(priv, pe.index, MVPP2_PRS_RI_L3_ARP,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_hw_write(priv, &pe);\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);\r\npe.index = tid;\r\nmvpp2_prs_match_etype(&pe, 0, MVPP2_IP_LBDT_TYPE);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_CPU_CODE_RX_SPEC |\r\nMVPP2_PRS_RI_UDF3_RX_SPECIAL,\r\nMVPP2_PRS_RI_CPU_CODE_MASK |\r\nMVPP2_PRS_RI_UDF3_MASK);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,\r\nMVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_L2);\r\npriv->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;\r\npriv->prs_shadow[pe.index].finish = true;\r\nmvpp2_prs_shadow_ri_set(priv, pe.index, MVPP2_PRS_RI_CPU_CODE_RX_SPEC |\r\nMVPP2_PRS_RI_UDF3_RX_SPECIAL,\r\nMVPP2_PRS_RI_CPU_CODE_MASK |\r\nMVPP2_PRS_RI_UDF3_MASK);\r\nmvpp2_prs_hw_write(priv, &pe);\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);\r\npe.index = tid;\r\nmvpp2_prs_match_etype(&pe, 0, ETH_P_IP);\r\nmvpp2_prs_tcam_data_byte_set(&pe, MVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_IPV4_HEAD | MVPP2_PRS_IPV4_IHL,\r\nMVPP2_PRS_IPV4_HEAD_MASK |\r\nMVPP2_PRS_IPV4_IHL_MASK);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 4,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,\r\nMVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_L2);\r\npriv->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;\r\npriv->prs_shadow[pe.index].finish = false;\r\nmvpp2_prs_shadow_ri_set(priv, pe.index, MVPP2_PRS_RI_L3_IP4,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_hw_write(priv, &pe);\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\npe.index = tid;\r\npe.tcam.byte[MVPP2_PRS_TCAM_DATA_BYTE(MVPP2_ETH_TYPE_LEN)] = 0x0;\r\npe.tcam.byte[MVPP2_PRS_TCAM_DATA_BYTE_EN(MVPP2_ETH_TYPE_LEN)] = 0x0;\r\nmvpp2_prs_tcam_data_byte_set(&pe, MVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_IPV4_HEAD,\r\nMVPP2_PRS_IPV4_HEAD_MASK);\r\npe.sram.word[MVPP2_PRS_SRAM_RI_WORD] = 0x0;\r\npe.sram.word[MVPP2_PRS_SRAM_RI_CTRL_WORD] = 0x0;\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4_OPT,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_L2);\r\npriv->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;\r\npriv->prs_shadow[pe.index].finish = false;\r\nmvpp2_prs_shadow_ri_set(priv, pe.index, MVPP2_PRS_RI_L3_IP4_OPT,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_hw_write(priv, &pe);\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);\r\npe.index = tid;\r\nmvpp2_prs_match_etype(&pe, 0, ETH_P_IPV6);\r\nmvpp2_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 8 +\r\nMVPP2_MAX_L3_ADDR_SIZE,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP6,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,\r\nMVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_L2);\r\npriv->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;\r\npriv->prs_shadow[pe.index].finish = false;\r\nmvpp2_prs_shadow_ri_set(priv, pe.index, MVPP2_PRS_RI_L3_IP6,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_L2);\r\npe.index = MVPP2_PE_ETH_TYPE_UN;\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UN,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,\r\nMVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_L2);\r\npriv->prs_shadow[pe.index].udf = MVPP2_PRS_UDF_L2_DEF;\r\npriv->prs_shadow[pe.index].finish = true;\r\nmvpp2_prs_shadow_ri_set(priv, pe.index, MVPP2_PRS_RI_L3_UN,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_vlan_init(struct platform_device *pdev, struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint err;\r\npriv->prs_double_vlans = devm_kcalloc(&pdev->dev, sizeof(bool),\r\nMVPP2_PRS_DBL_VLANS_MAX,\r\nGFP_KERNEL);\r\nif (!priv->prs_double_vlans)\r\nreturn -ENOMEM;\r\nerr = mvpp2_prs_double_vlan_add(priv, ETH_P_8021Q, ETH_P_8021AD,\r\nMVPP2_PRS_PORT_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_double_vlan_add(priv, ETH_P_8021Q, ETH_P_8021Q,\r\nMVPP2_PRS_PORT_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_vlan_add(priv, ETH_P_8021AD, MVPP2_PRS_SINGLE_VLAN_AI,\r\nMVPP2_PRS_PORT_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_vlan_add(priv, ETH_P_8021Q, MVPP2_PRS_SINGLE_VLAN_AI,\r\nMVPP2_PRS_PORT_MASK);\r\nif (err)\r\nreturn err;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_VLAN);\r\npe.index = MVPP2_PE_VLAN_DBL;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);\r\nmvpp2_prs_sram_ai_update(&pe, 0, MVPP2_PRS_SRAM_AI_MASK);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_DOUBLE,\r\nMVPP2_PRS_RI_VLAN_MASK);\r\nmvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_DBL_VLAN_AI_BIT,\r\nMVPP2_PRS_DBL_VLAN_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_VLAN);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_VLAN);\r\npe.index = MVPP2_PE_VLAN_NONE;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_L2);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_VLAN_NONE,\r\nMVPP2_PRS_RI_VLAN_MASK);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_VLAN);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_pppoe_init(struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint tid;\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_PPPOE);\r\npe.index = tid;\r\nmvpp2_prs_match_etype(&pe, 0, PPP_IP);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4_OPT,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 4,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,\r\nMVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_PPPOE);\r\nmvpp2_prs_hw_write(priv, &pe);\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\npe.index = tid;\r\nmvpp2_prs_tcam_data_byte_set(&pe, MVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_IPV4_HEAD | MVPP2_PRS_IPV4_IHL,\r\nMVPP2_PRS_IPV4_HEAD_MASK |\r\nMVPP2_PRS_IPV4_IHL_MASK);\r\npe.sram.word[MVPP2_PRS_SRAM_RI_WORD] = 0x0;\r\npe.sram.word[MVPP2_PRS_SRAM_RI_CTRL_WORD] = 0x0;\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP4,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_PPPOE);\r\nmvpp2_prs_hw_write(priv, &pe);\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_PPPOE);\r\npe.index = tid;\r\nmvpp2_prs_match_etype(&pe, 0, PPP_IPV6);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_IP6,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_sram_shift_set(&pe, MVPP2_ETH_TYPE_LEN + 4,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,\r\nMVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_PPPOE);\r\nmvpp2_prs_hw_write(priv, &pe);\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_PPPOE);\r\npe.index = tid;\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UN,\r\nMVPP2_PRS_RI_L3_PROTO_MASK);\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L3,\r\nMVPP2_ETH_TYPE_LEN,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_PPPOE);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_ip4_init(struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint err;\r\nerr = mvpp2_prs_ip4_proto(priv, IPPROTO_TCP, MVPP2_PRS_RI_L4_TCP,\r\nMVPP2_PRS_RI_L4_PROTO_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip4_proto(priv, IPPROTO_UDP, MVPP2_PRS_RI_L4_UDP,\r\nMVPP2_PRS_RI_L4_PROTO_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip4_proto(priv, IPPROTO_IGMP,\r\nMVPP2_PRS_RI_CPU_CODE_RX_SPEC |\r\nMVPP2_PRS_RI_UDF3_RX_SPECIAL,\r\nMVPP2_PRS_RI_CPU_CODE_MASK |\r\nMVPP2_PRS_RI_UDF3_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip4_cast(priv, MVPP2_PRS_L3_BROAD_CAST);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip4_cast(priv, MVPP2_PRS_L3_MULTI_CAST);\r\nif (err)\r\nreturn err;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);\r\npe.index = MVPP2_PE_IP4_PROTO_UN;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_sram_shift_set(&pe, 12, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,\r\nsizeof(struct iphdr) - 4,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_sram_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,\r\nMVPP2_PRS_IPV4_DIP_AI_BIT);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L4_OTHER,\r\nMVPP2_PRS_RI_L4_PROTO_MASK);\r\nmvpp2_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV4_DIP_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP4);\r\npe.index = MVPP2_PE_IP4_ADDR_UN;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UCAST,\r\nMVPP2_PRS_RI_L3_ADDR_MASK);\r\nmvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV4_DIP_AI_BIT,\r\nMVPP2_PRS_IPV4_DIP_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_ip6_init(struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint tid, err;\r\nerr = mvpp2_prs_ip6_proto(priv, IPPROTO_TCP,\r\nMVPP2_PRS_RI_L4_TCP,\r\nMVPP2_PRS_RI_L4_PROTO_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip6_proto(priv, IPPROTO_UDP,\r\nMVPP2_PRS_RI_L4_UDP,\r\nMVPP2_PRS_RI_L4_PROTO_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip6_proto(priv, IPPROTO_ICMPV6,\r\nMVPP2_PRS_RI_CPU_CODE_RX_SPEC |\r\nMVPP2_PRS_RI_UDF3_RX_SPECIAL,\r\nMVPP2_PRS_RI_CPU_CODE_MASK |\r\nMVPP2_PRS_RI_UDF3_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip6_proto(priv, IPPROTO_IPIP,\r\nMVPP2_PRS_RI_UDF7_IP6_LITE,\r\nMVPP2_PRS_RI_UDF7_MASK);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip6_cast(priv, MVPP2_PRS_L3_MULTI_CAST);\r\nif (err)\r\nreturn err;\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\nMVPP2_PE_LAST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\npe.index = tid;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UN |\r\nMVPP2_PRS_RI_DROP_MASK,\r\nMVPP2_PRS_RI_L3_PROTO_MASK |\r\nMVPP2_PRS_RI_DROP_MASK);\r\nmvpp2_prs_tcam_data_byte_set(&pe, 1, 0x00, MVPP2_PRS_IPV6_HOP_MASK);\r\nmvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,\r\nMVPP2_PRS_IPV6_NO_EXT_AI_BIT);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\npe.index = MVPP2_PE_IP6_PROTO_UN;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L4_OTHER,\r\nMVPP2_PRS_RI_L4_PROTO_MASK);\r\nmvpp2_prs_sram_offset_set(&pe, MVPP2_PRS_SRAM_UDF_TYPE_L4,\r\nsizeof(struct ipv6hdr) - 4,\r\nMVPP2_PRS_SRAM_OP_SEL_UDF_ADD);\r\nmvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,\r\nMVPP2_PRS_IPV6_NO_EXT_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\npe.index = MVPP2_PE_IP6_EXT_PROTO_UN;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_FLOWS);\r\nmvpp2_prs_sram_bits_set(&pe, MVPP2_PRS_SRAM_LU_GEN_BIT, 1);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L4_OTHER,\r\nMVPP2_PRS_RI_L4_PROTO_MASK);\r\nmvpp2_prs_tcam_ai_update(&pe, MVPP2_PRS_IPV6_EXT_AI_BIT,\r\nMVPP2_PRS_IPV6_EXT_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP4);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nmemset(&pe, 0, sizeof(struct mvpp2_prs_entry));\r\nmvpp2_prs_tcam_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\npe.index = MVPP2_PE_IP6_ADDR_UN;\r\nmvpp2_prs_sram_next_lu_set(&pe, MVPP2_PRS_LU_IP6);\r\nmvpp2_prs_sram_ri_update(&pe, MVPP2_PRS_RI_L3_UCAST,\r\nMVPP2_PRS_RI_L3_ADDR_MASK);\r\nmvpp2_prs_sram_ai_update(&pe, MVPP2_PRS_IPV6_NO_EXT_AI_BIT,\r\nMVPP2_PRS_IPV6_NO_EXT_AI_BIT);\r\nmvpp2_prs_sram_shift_set(&pe, -18, MVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\nmvpp2_prs_tcam_ai_update(&pe, 0, MVPP2_PRS_IPV6_NO_EXT_AI_BIT);\r\nmvpp2_prs_tcam_port_map_set(&pe, MVPP2_PRS_PORT_MASK);\r\nmvpp2_prs_shadow_set(priv, pe.index, MVPP2_PRS_LU_IP6);\r\nmvpp2_prs_hw_write(priv, &pe);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_default_init(struct platform_device *pdev,\r\nstruct mvpp2 *priv)\r\n{\r\nint err, index, i;\r\nmvpp2_write(priv, MVPP2_PRS_TCAM_CTRL_REG, MVPP2_PRS_TCAM_EN_MASK);\r\nfor (index = 0; index < MVPP2_PRS_TCAM_SRAM_SIZE; index++) {\r\nmvpp2_write(priv, MVPP2_PRS_TCAM_IDX_REG, index);\r\nfor (i = 0; i < MVPP2_PRS_TCAM_WORDS; i++)\r\nmvpp2_write(priv, MVPP2_PRS_TCAM_DATA_REG(i), 0);\r\nmvpp2_write(priv, MVPP2_PRS_SRAM_IDX_REG, index);\r\nfor (i = 0; i < MVPP2_PRS_SRAM_WORDS; i++)\r\nmvpp2_write(priv, MVPP2_PRS_SRAM_DATA_REG(i), 0);\r\n}\r\nfor (index = 0; index < MVPP2_PRS_TCAM_SRAM_SIZE; index++)\r\nmvpp2_prs_hw_inv(priv, index);\r\npriv->prs_shadow = devm_kcalloc(&pdev->dev, MVPP2_PRS_TCAM_SRAM_SIZE,\r\nsizeof(struct mvpp2_prs_shadow),\r\nGFP_KERNEL);\r\nif (!priv->prs_shadow)\r\nreturn -ENOMEM;\r\nfor (index = 0; index < MVPP2_MAX_PORTS; index++)\r\nmvpp2_prs_hw_port_init(priv, index, MVPP2_PRS_LU_MH,\r\nMVPP2_PRS_PORT_LU_MAX, 0);\r\nmvpp2_prs_def_flow_init(priv);\r\nmvpp2_prs_mh_init(priv);\r\nmvpp2_prs_mac_init(priv);\r\nmvpp2_prs_dsa_init(priv);\r\nerr = mvpp2_prs_etype_init(priv);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_vlan_init(pdev, priv);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_pppoe_init(priv);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip6_init(priv);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_ip4_init(priv);\r\nif (err)\r\nreturn err;\r\nreturn 0;\r\n}\r\nstatic bool mvpp2_prs_mac_range_equals(struct mvpp2_prs_entry *pe,\r\nconst u8 *da, unsigned char *mask)\r\n{\r\nunsigned char tcam_byte, tcam_mask;\r\nint index;\r\nfor (index = 0; index < ETH_ALEN; index++) {\r\nmvpp2_prs_tcam_data_byte_get(pe, index, &tcam_byte, &tcam_mask);\r\nif (tcam_mask != mask[index])\r\nreturn false;\r\nif ((tcam_mask & tcam_byte) != (da[index] & mask[index]))\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic struct mvpp2_prs_entry *\r\nmvpp2_prs_mac_da_range_find(struct mvpp2 *priv, int pmap, const u8 *da,\r\nunsigned char *mask, int udf_type)\r\n{\r\nstruct mvpp2_prs_entry *pe;\r\nint tid;\r\npe = kzalloc(sizeof(*pe), GFP_KERNEL);\r\nif (!pe)\r\nreturn NULL;\r\nmvpp2_prs_tcam_lu_set(pe, MVPP2_PRS_LU_MAC);\r\nfor (tid = MVPP2_PE_FIRST_FREE_TID;\r\ntid <= MVPP2_PE_LAST_FREE_TID; tid++) {\r\nunsigned int entry_pmap;\r\nif (!priv->prs_shadow[tid].valid ||\r\n(priv->prs_shadow[tid].lu != MVPP2_PRS_LU_MAC) ||\r\n(priv->prs_shadow[tid].udf != udf_type))\r\ncontinue;\r\npe->index = tid;\r\nmvpp2_prs_hw_read(priv, pe);\r\nentry_pmap = mvpp2_prs_tcam_port_map_get(pe);\r\nif (mvpp2_prs_mac_range_equals(pe, da, mask) &&\r\nentry_pmap == pmap)\r\nreturn pe;\r\n}\r\nkfree(pe);\r\nreturn NULL;\r\n}\r\nstatic int mvpp2_prs_mac_da_accept(struct mvpp2 *priv, int port,\r\nconst u8 *da, bool add)\r\n{\r\nstruct mvpp2_prs_entry *pe;\r\nunsigned int pmap, len, ri;\r\nunsigned char mask[ETH_ALEN] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };\r\nint tid;\r\npe = mvpp2_prs_mac_da_range_find(priv, (1 << port), da, mask,\r\nMVPP2_PRS_UDF_MAC_DEF);\r\nif (!pe) {\r\nif (!add)\r\nreturn 0;\r\nfor (tid = MVPP2_PE_FIRST_FREE_TID;\r\ntid <= MVPP2_PE_LAST_FREE_TID; tid++)\r\nif (priv->prs_shadow[tid].valid &&\r\n(priv->prs_shadow[tid].lu == MVPP2_PRS_LU_MAC) &&\r\n(priv->prs_shadow[tid].udf ==\r\nMVPP2_PRS_UDF_MAC_RANGE))\r\nbreak;\r\ntid = mvpp2_prs_tcam_first_free(priv, MVPP2_PE_FIRST_FREE_TID,\r\ntid - 1);\r\nif (tid < 0)\r\nreturn tid;\r\npe = kzalloc(sizeof(*pe), GFP_KERNEL);\r\nif (!pe)\r\nreturn -1;\r\nmvpp2_prs_tcam_lu_set(pe, MVPP2_PRS_LU_MAC);\r\npe->index = tid;\r\nmvpp2_prs_tcam_port_map_set(pe, 0);\r\n}\r\nmvpp2_prs_tcam_port_set(pe, port, add);\r\npmap = mvpp2_prs_tcam_port_map_get(pe);\r\nif (pmap == 0) {\r\nif (add) {\r\nkfree(pe);\r\nreturn -1;\r\n}\r\nmvpp2_prs_hw_inv(priv, pe->index);\r\npriv->prs_shadow[pe->index].valid = false;\r\nkfree(pe);\r\nreturn 0;\r\n}\r\nmvpp2_prs_sram_next_lu_set(pe, MVPP2_PRS_LU_DSA);\r\nlen = ETH_ALEN;\r\nwhile (len--)\r\nmvpp2_prs_tcam_data_byte_set(pe, len, da[len], 0xff);\r\nif (is_broadcast_ether_addr(da))\r\nri = MVPP2_PRS_RI_L2_BCAST;\r\nelse if (is_multicast_ether_addr(da))\r\nri = MVPP2_PRS_RI_L2_MCAST;\r\nelse\r\nri = MVPP2_PRS_RI_L2_UCAST | MVPP2_PRS_RI_MAC_ME_MASK;\r\nmvpp2_prs_sram_ri_update(pe, ri, MVPP2_PRS_RI_L2_CAST_MASK |\r\nMVPP2_PRS_RI_MAC_ME_MASK);\r\nmvpp2_prs_shadow_ri_set(priv, pe->index, ri, MVPP2_PRS_RI_L2_CAST_MASK |\r\nMVPP2_PRS_RI_MAC_ME_MASK);\r\nmvpp2_prs_sram_shift_set(pe, 2 * ETH_ALEN,\r\nMVPP2_PRS_SRAM_OP_SEL_SHIFT_ADD);\r\npriv->prs_shadow[pe->index].udf = MVPP2_PRS_UDF_MAC_DEF;\r\nmvpp2_prs_shadow_set(priv, pe->index, MVPP2_PRS_LU_MAC);\r\nmvpp2_prs_hw_write(priv, pe);\r\nkfree(pe);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_update_mac_da(struct net_device *dev, const u8 *da)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nint err;\r\nerr = mvpp2_prs_mac_da_accept(port->priv, port->id, dev->dev_addr,\r\nfalse);\r\nif (err)\r\nreturn err;\r\nerr = mvpp2_prs_mac_da_accept(port->priv, port->id, da, true);\r\nif (err)\r\nreturn err;\r\nether_addr_copy(dev->dev_addr, da);\r\nreturn 0;\r\n}\r\nstatic void mvpp2_prs_mcast_del_all(struct mvpp2 *priv, int port)\r\n{\r\nstruct mvpp2_prs_entry pe;\r\nint index, tid;\r\nfor (tid = MVPP2_PE_FIRST_FREE_TID;\r\ntid <= MVPP2_PE_LAST_FREE_TID; tid++) {\r\nunsigned char da[ETH_ALEN], da_mask[ETH_ALEN];\r\nif (!priv->prs_shadow[tid].valid ||\r\n(priv->prs_shadow[tid].lu != MVPP2_PRS_LU_MAC) ||\r\n(priv->prs_shadow[tid].udf != MVPP2_PRS_UDF_MAC_DEF))\r\ncontinue;\r\npe.index = tid;\r\nmvpp2_prs_hw_read(priv, &pe);\r\nfor (index = 0; index < ETH_ALEN; index++)\r\nmvpp2_prs_tcam_data_byte_get(&pe, index, &da[index],\r\n&da_mask[index]);\r\nif (is_multicast_ether_addr(da) && !is_broadcast_ether_addr(da))\r\nmvpp2_prs_mac_da_accept(priv, port, da, false);\r\n}\r\n}\r\nstatic int mvpp2_prs_tag_mode_set(struct mvpp2 *priv, int port, int type)\r\n{\r\nswitch (type) {\r\ncase MVPP2_TAG_TYPE_EDSA:\r\nmvpp2_prs_dsa_tag_set(priv, port, true,\r\nMVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);\r\nmvpp2_prs_dsa_tag_set(priv, port, true,\r\nMVPP2_PRS_UNTAGGED, MVPP2_PRS_EDSA);\r\nmvpp2_prs_dsa_tag_set(priv, port, false,\r\nMVPP2_PRS_TAGGED, MVPP2_PRS_DSA);\r\nmvpp2_prs_dsa_tag_set(priv, port, false,\r\nMVPP2_PRS_UNTAGGED, MVPP2_PRS_DSA);\r\nbreak;\r\ncase MVPP2_TAG_TYPE_DSA:\r\nmvpp2_prs_dsa_tag_set(priv, port, true,\r\nMVPP2_PRS_TAGGED, MVPP2_PRS_DSA);\r\nmvpp2_prs_dsa_tag_set(priv, port, true,\r\nMVPP2_PRS_UNTAGGED, MVPP2_PRS_DSA);\r\nmvpp2_prs_dsa_tag_set(priv, port, false,\r\nMVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);\r\nmvpp2_prs_dsa_tag_set(priv, port, false,\r\nMVPP2_PRS_UNTAGGED, MVPP2_PRS_EDSA);\r\nbreak;\r\ncase MVPP2_TAG_TYPE_MH:\r\ncase MVPP2_TAG_TYPE_NONE:\r\nmvpp2_prs_dsa_tag_set(priv, port, false,\r\nMVPP2_PRS_TAGGED, MVPP2_PRS_DSA);\r\nmvpp2_prs_dsa_tag_set(priv, port, false,\r\nMVPP2_PRS_UNTAGGED, MVPP2_PRS_DSA);\r\nmvpp2_prs_dsa_tag_set(priv, port, false,\r\nMVPP2_PRS_TAGGED, MVPP2_PRS_EDSA);\r\nmvpp2_prs_dsa_tag_set(priv, port, false,\r\nMVPP2_PRS_UNTAGGED, MVPP2_PRS_EDSA);\r\nbreak;\r\ndefault:\r\nif ((type < 0) || (type > MVPP2_TAG_TYPE_EDSA))\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mvpp2_prs_def_flow(struct mvpp2_port *port)\r\n{\r\nstruct mvpp2_prs_entry *pe;\r\nint tid;\r\npe = mvpp2_prs_flow_find(port->priv, port->id);\r\nif (!pe) {\r\ntid = mvpp2_prs_tcam_first_free(port->priv,\r\nMVPP2_PE_LAST_FREE_TID,\r\nMVPP2_PE_FIRST_FREE_TID);\r\nif (tid < 0)\r\nreturn tid;\r\npe = kzalloc(sizeof(*pe), GFP_KERNEL);\r\nif (!pe)\r\nreturn -ENOMEM;\r\nmvpp2_prs_tcam_lu_set(pe, MVPP2_PRS_LU_FLOWS);\r\npe->index = tid;\r\nmvpp2_prs_sram_ai_update(pe, port->id, MVPP2_PRS_FLOW_ID_MASK);\r\nmvpp2_prs_sram_bits_set(pe, MVPP2_PRS_SRAM_LU_DONE_BIT, 1);\r\nmvpp2_prs_shadow_set(port->priv, pe->index, MVPP2_PRS_LU_FLOWS);\r\n}\r\nmvpp2_prs_tcam_port_map_set(pe, (1 << port->id));\r\nmvpp2_prs_hw_write(port->priv, pe);\r\nkfree(pe);\r\nreturn 0;\r\n}\r\nstatic void mvpp2_cls_flow_write(struct mvpp2 *priv,\r\nstruct mvpp2_cls_flow_entry *fe)\r\n{\r\nmvpp2_write(priv, MVPP2_CLS_FLOW_INDEX_REG, fe->index);\r\nmvpp2_write(priv, MVPP2_CLS_FLOW_TBL0_REG, fe->data[0]);\r\nmvpp2_write(priv, MVPP2_CLS_FLOW_TBL1_REG, fe->data[1]);\r\nmvpp2_write(priv, MVPP2_CLS_FLOW_TBL2_REG, fe->data[2]);\r\n}\r\nstatic void mvpp2_cls_lookup_write(struct mvpp2 *priv,\r\nstruct mvpp2_cls_lookup_entry *le)\r\n{\r\nu32 val;\r\nval = (le->way << MVPP2_CLS_LKP_INDEX_WAY_OFFS) | le->lkpid;\r\nmvpp2_write(priv, MVPP2_CLS_LKP_INDEX_REG, val);\r\nmvpp2_write(priv, MVPP2_CLS_LKP_TBL_REG, le->data);\r\n}\r\nstatic void mvpp2_cls_init(struct mvpp2 *priv)\r\n{\r\nstruct mvpp2_cls_lookup_entry le;\r\nstruct mvpp2_cls_flow_entry fe;\r\nint index;\r\nmvpp2_write(priv, MVPP2_CLS_MODE_REG, MVPP2_CLS_MODE_ACTIVE_MASK);\r\nmemset(&fe.data, 0, MVPP2_CLS_FLOWS_TBL_DATA_WORDS);\r\nfor (index = 0; index < MVPP2_CLS_FLOWS_TBL_SIZE; index++) {\r\nfe.index = index;\r\nmvpp2_cls_flow_write(priv, &fe);\r\n}\r\nle.data = 0;\r\nfor (index = 0; index < MVPP2_CLS_LKP_TBL_SIZE; index++) {\r\nle.lkpid = index;\r\nle.way = 0;\r\nmvpp2_cls_lookup_write(priv, &le);\r\nle.way = 1;\r\nmvpp2_cls_lookup_write(priv, &le);\r\n}\r\n}\r\nstatic void mvpp2_cls_port_config(struct mvpp2_port *port)\r\n{\r\nstruct mvpp2_cls_lookup_entry le;\r\nu32 val;\r\nval = mvpp2_read(port->priv, MVPP2_CLS_PORT_WAY_REG);\r\nval &= ~MVPP2_CLS_PORT_WAY_MASK(port->id);\r\nmvpp2_write(port->priv, MVPP2_CLS_PORT_WAY_REG, val);\r\nle.lkpid = port->id;\r\nle.way = 0;\r\nle.data = 0;\r\nle.data &= ~MVPP2_CLS_LKP_TBL_RXQ_MASK;\r\nle.data |= port->first_rxq;\r\nle.data &= ~MVPP2_CLS_LKP_TBL_LOOKUP_EN_MASK;\r\nmvpp2_cls_lookup_write(port->priv, &le);\r\n}\r\nstatic void mvpp2_cls_oversize_rxq_set(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nmvpp2_write(port->priv, MVPP2_CLS_OVERSIZE_RXQ_LOW_REG(port->id),\r\nport->first_rxq & MVPP2_CLS_OVERSIZE_RXQ_LOW_MASK);\r\nmvpp2_write(port->priv, MVPP2_CLS_SWFWD_P2HQ_REG(port->id),\r\n(port->first_rxq >> MVPP2_CLS_OVERSIZE_RXQ_LOW_BITS));\r\nval = mvpp2_read(port->priv, MVPP2_CLS_SWFWD_PCTRL_REG);\r\nval |= MVPP2_CLS_SWFWD_PCTRL_MASK(port->id);\r\nmvpp2_write(port->priv, MVPP2_CLS_SWFWD_PCTRL_REG, val);\r\n}\r\nstatic int mvpp2_bm_pool_create(struct platform_device *pdev,\r\nstruct mvpp2 *priv,\r\nstruct mvpp2_bm_pool *bm_pool, int size)\r\n{\r\nint size_bytes;\r\nu32 val;\r\nsize_bytes = sizeof(u32) * size;\r\nbm_pool->virt_addr = dma_alloc_coherent(&pdev->dev, size_bytes,\r\n&bm_pool->phys_addr,\r\nGFP_KERNEL);\r\nif (!bm_pool->virt_addr)\r\nreturn -ENOMEM;\r\nif (!IS_ALIGNED((u32)bm_pool->virt_addr, MVPP2_BM_POOL_PTR_ALIGN)) {\r\ndma_free_coherent(&pdev->dev, size_bytes, bm_pool->virt_addr,\r\nbm_pool->phys_addr);\r\ndev_err(&pdev->dev, "BM pool %d is not %d bytes aligned\n",\r\nbm_pool->id, MVPP2_BM_POOL_PTR_ALIGN);\r\nreturn -ENOMEM;\r\n}\r\nmvpp2_write(priv, MVPP2_BM_POOL_BASE_REG(bm_pool->id),\r\nbm_pool->phys_addr);\r\nmvpp2_write(priv, MVPP2_BM_POOL_SIZE_REG(bm_pool->id), size);\r\nval = mvpp2_read(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id));\r\nval |= MVPP2_BM_START_MASK;\r\nmvpp2_write(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id), val);\r\nbm_pool->type = MVPP2_BM_FREE;\r\nbm_pool->size = size;\r\nbm_pool->pkt_size = 0;\r\nbm_pool->buf_num = 0;\r\natomic_set(&bm_pool->in_use, 0);\r\nreturn 0;\r\n}\r\nstatic void mvpp2_bm_pool_bufsize_set(struct mvpp2 *priv,\r\nstruct mvpp2_bm_pool *bm_pool,\r\nint buf_size)\r\n{\r\nu32 val;\r\nbm_pool->buf_size = buf_size;\r\nval = ALIGN(buf_size, 1 << MVPP2_POOL_BUF_SIZE_OFFSET);\r\nmvpp2_write(priv, MVPP2_POOL_BUF_SIZE_REG(bm_pool->id), val);\r\n}\r\nstatic void mvpp2_bm_bufs_free(struct mvpp2 *priv, struct mvpp2_bm_pool *bm_pool)\r\n{\r\nint i;\r\nfor (i = 0; i < bm_pool->buf_num; i++) {\r\nu32 vaddr;\r\nmvpp2_read(priv, MVPP2_BM_PHY_ALLOC_REG(bm_pool->id));\r\nvaddr = mvpp2_read(priv, MVPP2_BM_VIRT_ALLOC_REG);\r\nif (!vaddr)\r\nbreak;\r\ndev_kfree_skb_any((struct sk_buff *)vaddr);\r\n}\r\nbm_pool->buf_num -= i;\r\n}\r\nstatic int mvpp2_bm_pool_destroy(struct platform_device *pdev,\r\nstruct mvpp2 *priv,\r\nstruct mvpp2_bm_pool *bm_pool)\r\n{\r\nu32 val;\r\nmvpp2_bm_bufs_free(priv, bm_pool);\r\nif (bm_pool->buf_num) {\r\nWARN(1, "cannot free all buffers in pool %d\n", bm_pool->id);\r\nreturn 0;\r\n}\r\nval = mvpp2_read(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id));\r\nval |= MVPP2_BM_STOP_MASK;\r\nmvpp2_write(priv, MVPP2_BM_POOL_CTRL_REG(bm_pool->id), val);\r\ndma_free_coherent(&pdev->dev, sizeof(u32) * bm_pool->size,\r\nbm_pool->virt_addr,\r\nbm_pool->phys_addr);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_bm_pools_init(struct platform_device *pdev,\r\nstruct mvpp2 *priv)\r\n{\r\nint i, err, size;\r\nstruct mvpp2_bm_pool *bm_pool;\r\nsize = MVPP2_BM_POOL_SIZE_MAX;\r\nfor (i = 0; i < MVPP2_BM_POOLS_NUM; i++) {\r\nbm_pool = &priv->bm_pools[i];\r\nbm_pool->id = i;\r\nerr = mvpp2_bm_pool_create(pdev, priv, bm_pool, size);\r\nif (err)\r\ngoto err_unroll_pools;\r\nmvpp2_bm_pool_bufsize_set(priv, bm_pool, 0);\r\n}\r\nreturn 0;\r\nerr_unroll_pools:\r\ndev_err(&pdev->dev, "failed to create BM pool %d, size %d\n", i, size);\r\nfor (i = i - 1; i >= 0; i--)\r\nmvpp2_bm_pool_destroy(pdev, priv, &priv->bm_pools[i]);\r\nreturn err;\r\n}\r\nstatic int mvpp2_bm_init(struct platform_device *pdev, struct mvpp2 *priv)\r\n{\r\nint i, err;\r\nfor (i = 0; i < MVPP2_BM_POOLS_NUM; i++) {\r\nmvpp2_write(priv, MVPP2_BM_INTR_MASK_REG(i), 0);\r\nmvpp2_write(priv, MVPP2_BM_INTR_CAUSE_REG(i), 0);\r\n}\r\npriv->bm_pools = devm_kcalloc(&pdev->dev, MVPP2_BM_POOLS_NUM,\r\nsizeof(struct mvpp2_bm_pool), GFP_KERNEL);\r\nif (!priv->bm_pools)\r\nreturn -ENOMEM;\r\nerr = mvpp2_bm_pools_init(pdev, priv);\r\nif (err < 0)\r\nreturn err;\r\nreturn 0;\r\n}\r\nstatic void mvpp2_rxq_long_pool_set(struct mvpp2_port *port,\r\nint lrxq, int long_pool)\r\n{\r\nu32 val;\r\nint prxq;\r\nprxq = port->rxqs[lrxq]->id;\r\nval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(prxq));\r\nval &= ~MVPP2_RXQ_POOL_LONG_MASK;\r\nval |= ((long_pool << MVPP2_RXQ_POOL_LONG_OFFS) &\r\nMVPP2_RXQ_POOL_LONG_MASK);\r\nmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(prxq), val);\r\n}\r\nstatic void mvpp2_rxq_short_pool_set(struct mvpp2_port *port,\r\nint lrxq, int short_pool)\r\n{\r\nu32 val;\r\nint prxq;\r\nprxq = port->rxqs[lrxq]->id;\r\nval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(prxq));\r\nval &= ~MVPP2_RXQ_POOL_SHORT_MASK;\r\nval |= ((short_pool << MVPP2_RXQ_POOL_SHORT_OFFS) &\r\nMVPP2_RXQ_POOL_SHORT_MASK);\r\nmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(prxq), val);\r\n}\r\nstatic struct sk_buff *mvpp2_skb_alloc(struct mvpp2_port *port,\r\nstruct mvpp2_bm_pool *bm_pool,\r\ndma_addr_t *buf_phys_addr,\r\ngfp_t gfp_mask)\r\n{\r\nstruct sk_buff *skb;\r\ndma_addr_t phys_addr;\r\nskb = __dev_alloc_skb(bm_pool->pkt_size, gfp_mask);\r\nif (!skb)\r\nreturn NULL;\r\nphys_addr = dma_map_single(port->dev->dev.parent, skb->head,\r\nMVPP2_RX_BUF_SIZE(bm_pool->pkt_size),\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(port->dev->dev.parent, phys_addr))) {\r\ndev_kfree_skb_any(skb);\r\nreturn NULL;\r\n}\r\n*buf_phys_addr = phys_addr;\r\nreturn skb;\r\n}\r\nstatic inline u32 mvpp2_bm_cookie_pool_set(u32 cookie, int pool)\r\n{\r\nu32 bm;\r\nbm = cookie & ~(0xFF << MVPP2_BM_COOKIE_POOL_OFFS);\r\nbm |= ((pool & 0xFF) << MVPP2_BM_COOKIE_POOL_OFFS);\r\nreturn bm;\r\n}\r\nstatic inline int mvpp2_bm_cookie_pool_get(u32 cookie)\r\n{\r\nreturn (cookie >> MVPP2_BM_COOKIE_POOL_OFFS) & 0xFF;\r\n}\r\nstatic inline void mvpp2_bm_pool_put(struct mvpp2_port *port, int pool,\r\nu32 buf_phys_addr, u32 buf_virt_addr)\r\n{\r\nmvpp2_write(port->priv, MVPP2_BM_VIRT_RLS_REG, buf_virt_addr);\r\nmvpp2_write(port->priv, MVPP2_BM_PHY_RLS_REG(pool), buf_phys_addr);\r\n}\r\nstatic void mvpp2_bm_pool_mc_put(struct mvpp2_port *port, int pool,\r\nu32 buf_phys_addr, u32 buf_virt_addr,\r\nint mc_id)\r\n{\r\nu32 val = 0;\r\nval |= (mc_id & MVPP2_BM_MC_ID_MASK);\r\nmvpp2_write(port->priv, MVPP2_BM_MC_RLS_REG, val);\r\nmvpp2_bm_pool_put(port, pool,\r\nbuf_phys_addr | MVPP2_BM_PHY_RLS_MC_BUFF_MASK,\r\nbuf_virt_addr);\r\n}\r\nstatic void mvpp2_pool_refill(struct mvpp2_port *port, u32 bm,\r\nu32 phys_addr, u32 cookie)\r\n{\r\nint pool = mvpp2_bm_cookie_pool_get(bm);\r\nmvpp2_bm_pool_put(port, pool, phys_addr, cookie);\r\n}\r\nstatic int mvpp2_bm_bufs_add(struct mvpp2_port *port,\r\nstruct mvpp2_bm_pool *bm_pool, int buf_num)\r\n{\r\nstruct sk_buff *skb;\r\nint i, buf_size, total_size;\r\nu32 bm;\r\ndma_addr_t phys_addr;\r\nbuf_size = MVPP2_RX_BUF_SIZE(bm_pool->pkt_size);\r\ntotal_size = MVPP2_RX_TOTAL_SIZE(buf_size);\r\nif (buf_num < 0 ||\r\n(buf_num + bm_pool->buf_num > bm_pool->size)) {\r\nnetdev_err(port->dev,\r\n"cannot allocate %d buffers for pool %d\n",\r\nbuf_num, bm_pool->id);\r\nreturn 0;\r\n}\r\nbm = mvpp2_bm_cookie_pool_set(0, bm_pool->id);\r\nfor (i = 0; i < buf_num; i++) {\r\nskb = mvpp2_skb_alloc(port, bm_pool, &phys_addr, GFP_KERNEL);\r\nif (!skb)\r\nbreak;\r\nmvpp2_pool_refill(port, bm, (u32)phys_addr, (u32)skb);\r\n}\r\nbm_pool->buf_num += i;\r\nbm_pool->in_use_thresh = bm_pool->buf_num / 4;\r\nnetdev_dbg(port->dev,\r\n"%s pool %d: pkt_size=%4d, buf_size=%4d, total_size=%4d\n",\r\nbm_pool->type == MVPP2_BM_SWF_SHORT ? "short" : " long",\r\nbm_pool->id, bm_pool->pkt_size, buf_size, total_size);\r\nnetdev_dbg(port->dev,\r\n"%s pool %d: %d of %d buffers added\n",\r\nbm_pool->type == MVPP2_BM_SWF_SHORT ? "short" : " long",\r\nbm_pool->id, i, buf_num);\r\nreturn i;\r\n}\r\nstatic struct mvpp2_bm_pool *\r\nmvpp2_bm_pool_use(struct mvpp2_port *port, int pool, enum mvpp2_bm_type type,\r\nint pkt_size)\r\n{\r\nstruct mvpp2_bm_pool *new_pool = &port->priv->bm_pools[pool];\r\nint num;\r\nif (new_pool->type != MVPP2_BM_FREE && new_pool->type != type) {\r\nnetdev_err(port->dev, "mixing pool types is forbidden\n");\r\nreturn NULL;\r\n}\r\nif (new_pool->type == MVPP2_BM_FREE)\r\nnew_pool->type = type;\r\nif (((type == MVPP2_BM_SWF_LONG) && (pkt_size > new_pool->pkt_size)) ||\r\n(new_pool->pkt_size == 0)) {\r\nint pkts_num;\r\npkts_num = new_pool->buf_num;\r\nif (pkts_num == 0)\r\npkts_num = type == MVPP2_BM_SWF_LONG ?\r\nMVPP2_BM_LONG_BUF_NUM :\r\nMVPP2_BM_SHORT_BUF_NUM;\r\nelse\r\nmvpp2_bm_bufs_free(port->priv, new_pool);\r\nnew_pool->pkt_size = pkt_size;\r\nnum = mvpp2_bm_bufs_add(port, new_pool, pkts_num);\r\nif (num != pkts_num) {\r\nWARN(1, "pool %d: %d of %d allocated\n",\r\nnew_pool->id, num, pkts_num);\r\nreturn NULL;\r\n}\r\n}\r\nmvpp2_bm_pool_bufsize_set(port->priv, new_pool,\r\nMVPP2_RX_BUF_SIZE(new_pool->pkt_size));\r\nreturn new_pool;\r\n}\r\nstatic int mvpp2_swf_bm_pool_init(struct mvpp2_port *port)\r\n{\r\nint rxq;\r\nif (!port->pool_long) {\r\nport->pool_long =\r\nmvpp2_bm_pool_use(port, MVPP2_BM_SWF_LONG_POOL(port->id),\r\nMVPP2_BM_SWF_LONG,\r\nport->pkt_size);\r\nif (!port->pool_long)\r\nreturn -ENOMEM;\r\nport->pool_long->port_map |= (1 << port->id);\r\nfor (rxq = 0; rxq < rxq_number; rxq++)\r\nmvpp2_rxq_long_pool_set(port, rxq, port->pool_long->id);\r\n}\r\nif (!port->pool_short) {\r\nport->pool_short =\r\nmvpp2_bm_pool_use(port, MVPP2_BM_SWF_SHORT_POOL,\r\nMVPP2_BM_SWF_SHORT,\r\nMVPP2_BM_SHORT_PKT_SIZE);\r\nif (!port->pool_short)\r\nreturn -ENOMEM;\r\nport->pool_short->port_map |= (1 << port->id);\r\nfor (rxq = 0; rxq < rxq_number; rxq++)\r\nmvpp2_rxq_short_pool_set(port, rxq,\r\nport->pool_short->id);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mvpp2_bm_update_mtu(struct net_device *dev, int mtu)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nstruct mvpp2_bm_pool *port_pool = port->pool_long;\r\nint num, pkts_num = port_pool->buf_num;\r\nint pkt_size = MVPP2_RX_PKT_SIZE(mtu);\r\nmvpp2_bm_bufs_free(port->priv, port_pool);\r\nif (port_pool->buf_num) {\r\nWARN(1, "cannot free all buffers in pool %d\n", port_pool->id);\r\nreturn -EIO;\r\n}\r\nport_pool->pkt_size = pkt_size;\r\nnum = mvpp2_bm_bufs_add(port, port_pool, pkts_num);\r\nif (num != pkts_num) {\r\nWARN(1, "pool %d: %d of %d allocated\n",\r\nport_pool->id, num, pkts_num);\r\nreturn -EIO;\r\n}\r\nmvpp2_bm_pool_bufsize_set(port->priv, port_pool,\r\nMVPP2_RX_BUF_SIZE(port_pool->pkt_size));\r\ndev->mtu = mtu;\r\nnetdev_update_features(dev);\r\nreturn 0;\r\n}\r\nstatic inline void mvpp2_interrupts_enable(struct mvpp2_port *port)\r\n{\r\nint cpu, cpu_mask = 0;\r\nfor_each_present_cpu(cpu)\r\ncpu_mask |= 1 << cpu;\r\nmvpp2_write(port->priv, MVPP2_ISR_ENABLE_REG(port->id),\r\nMVPP2_ISR_ENABLE_INTERRUPT(cpu_mask));\r\n}\r\nstatic inline void mvpp2_interrupts_disable(struct mvpp2_port *port)\r\n{\r\nint cpu, cpu_mask = 0;\r\nfor_each_present_cpu(cpu)\r\ncpu_mask |= 1 << cpu;\r\nmvpp2_write(port->priv, MVPP2_ISR_ENABLE_REG(port->id),\r\nMVPP2_ISR_DISABLE_INTERRUPT(cpu_mask));\r\n}\r\nstatic void mvpp2_interrupts_mask(void *arg)\r\n{\r\nstruct mvpp2_port *port = arg;\r\nmvpp2_write(port->priv, MVPP2_ISR_RX_TX_MASK_REG(port->id), 0);\r\n}\r\nstatic void mvpp2_interrupts_unmask(void *arg)\r\n{\r\nstruct mvpp2_port *port = arg;\r\nmvpp2_write(port->priv, MVPP2_ISR_RX_TX_MASK_REG(port->id),\r\n(MVPP2_CAUSE_MISC_SUM_MASK |\r\nMVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK));\r\n}\r\nstatic void mvpp2_port_mii_set(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nval = readl(port->base + MVPP2_GMAC_CTRL_2_REG);\r\nswitch (port->phy_interface) {\r\ncase PHY_INTERFACE_MODE_SGMII:\r\nval |= MVPP2_GMAC_INBAND_AN_MASK;\r\nbreak;\r\ncase PHY_INTERFACE_MODE_RGMII:\r\nval |= MVPP2_GMAC_PORT_RGMII_MASK;\r\ndefault:\r\nval &= ~MVPP2_GMAC_PCS_ENABLE_MASK;\r\n}\r\nwritel(val, port->base + MVPP2_GMAC_CTRL_2_REG);\r\n}\r\nstatic void mvpp2_port_fc_adv_enable(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nval = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);\r\nval |= MVPP2_GMAC_FC_ADV_EN;\r\nwritel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);\r\n}\r\nstatic void mvpp2_port_enable(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nval = readl(port->base + MVPP2_GMAC_CTRL_0_REG);\r\nval |= MVPP2_GMAC_PORT_EN_MASK;\r\nval |= MVPP2_GMAC_MIB_CNTR_EN_MASK;\r\nwritel(val, port->base + MVPP2_GMAC_CTRL_0_REG);\r\n}\r\nstatic void mvpp2_port_disable(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nval = readl(port->base + MVPP2_GMAC_CTRL_0_REG);\r\nval &= ~(MVPP2_GMAC_PORT_EN_MASK);\r\nwritel(val, port->base + MVPP2_GMAC_CTRL_0_REG);\r\n}\r\nstatic void mvpp2_port_periodic_xon_disable(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nval = readl(port->base + MVPP2_GMAC_CTRL_1_REG) &\r\n~MVPP2_GMAC_PERIODIC_XON_EN_MASK;\r\nwritel(val, port->base + MVPP2_GMAC_CTRL_1_REG);\r\n}\r\nstatic void mvpp2_port_loopback_set(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nval = readl(port->base + MVPP2_GMAC_CTRL_1_REG);\r\nif (port->speed == 1000)\r\nval |= MVPP2_GMAC_GMII_LB_EN_MASK;\r\nelse\r\nval &= ~MVPP2_GMAC_GMII_LB_EN_MASK;\r\nif (port->phy_interface == PHY_INTERFACE_MODE_SGMII)\r\nval |= MVPP2_GMAC_PCS_LB_EN_MASK;\r\nelse\r\nval &= ~MVPP2_GMAC_PCS_LB_EN_MASK;\r\nwritel(val, port->base + MVPP2_GMAC_CTRL_1_REG);\r\n}\r\nstatic void mvpp2_port_reset(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nval = readl(port->base + MVPP2_GMAC_CTRL_2_REG) &\r\n~MVPP2_GMAC_PORT_RESET_MASK;\r\nwritel(val, port->base + MVPP2_GMAC_CTRL_2_REG);\r\nwhile (readl(port->base + MVPP2_GMAC_CTRL_2_REG) &\r\nMVPP2_GMAC_PORT_RESET_MASK)\r\ncontinue;\r\n}\r\nstatic inline void mvpp2_gmac_max_rx_size_set(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nval = readl(port->base + MVPP2_GMAC_CTRL_0_REG);\r\nval &= ~MVPP2_GMAC_MAX_RX_SIZE_MASK;\r\nval |= (((port->pkt_size - MVPP2_MH_SIZE) / 2) <<\r\nMVPP2_GMAC_MAX_RX_SIZE_OFFS);\r\nwritel(val, port->base + MVPP2_GMAC_CTRL_0_REG);\r\n}\r\nstatic void mvpp2_defaults_set(struct mvpp2_port *port)\r\n{\r\nint tx_port_num, val, queue, ptxq, lrxq;\r\nif (port->flags & MVPP2_F_LOOPBACK)\r\nmvpp2_port_loopback_set(port);\r\nval = readl(port->base + MVPP2_GMAC_PORT_FIFO_CFG_1_REG);\r\nval &= ~MVPP2_GMAC_TX_FIFO_MIN_TH_ALL_MASK;\r\nval |= MVPP2_GMAC_TX_FIFO_MIN_TH_MASK(64 - 4 - 2);\r\nwritel(val, port->base + MVPP2_GMAC_PORT_FIFO_CFG_1_REG);\r\ntx_port_num = mvpp2_egress_port(port);\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG,\r\ntx_port_num);\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_CMD_1_REG, 0);\r\nfor (queue = 0; queue < MVPP2_MAX_TXQ; queue++) {\r\nptxq = mvpp2_txq_phys(port->id, queue);\r\nmvpp2_write(port->priv,\r\nMVPP2_TXQ_SCHED_TOKEN_CNTR_REG(ptxq), 0);\r\n}\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_PERIOD_REG,\r\nport->priv->tclk / USEC_PER_SEC);\r\nval = mvpp2_read(port->priv, MVPP2_TXP_SCHED_REFILL_REG);\r\nval &= ~MVPP2_TXP_REFILL_PERIOD_ALL_MASK;\r\nval |= MVPP2_TXP_REFILL_PERIOD_MASK(1);\r\nval |= MVPP2_TXP_REFILL_TOKENS_ALL_MASK;\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_REFILL_REG, val);\r\nval = MVPP2_TXP_TOKEN_SIZE_MAX;\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_TOKEN_SIZE_REG, val);\r\nmvpp2_write(port->priv, MVPP2_RX_CTRL_REG(port->id),\r\nMVPP2_RX_USE_PSEUDO_FOR_CSUM_MASK |\r\nMVPP2_RX_LOW_LATENCY_PKT_SIZE(256));\r\nfor (lrxq = 0; lrxq < rxq_number; lrxq++) {\r\nqueue = port->rxqs[lrxq]->id;\r\nval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(queue));\r\nval |= MVPP2_SNOOP_PKT_SIZE_MASK |\r\nMVPP2_SNOOP_BUF_HDR_MASK;\r\nmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(queue), val);\r\n}\r\nmvpp2_interrupts_disable(port);\r\n}\r\nstatic void mvpp2_ingress_enable(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nint lrxq, queue;\r\nfor (lrxq = 0; lrxq < rxq_number; lrxq++) {\r\nqueue = port->rxqs[lrxq]->id;\r\nval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(queue));\r\nval &= ~MVPP2_RXQ_DISABLE_MASK;\r\nmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(queue), val);\r\n}\r\n}\r\nstatic void mvpp2_ingress_disable(struct mvpp2_port *port)\r\n{\r\nu32 val;\r\nint lrxq, queue;\r\nfor (lrxq = 0; lrxq < rxq_number; lrxq++) {\r\nqueue = port->rxqs[lrxq]->id;\r\nval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(queue));\r\nval |= MVPP2_RXQ_DISABLE_MASK;\r\nmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(queue), val);\r\n}\r\n}\r\nstatic void mvpp2_egress_enable(struct mvpp2_port *port)\r\n{\r\nu32 qmap;\r\nint queue;\r\nint tx_port_num = mvpp2_egress_port(port);\r\nqmap = 0;\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nstruct mvpp2_tx_queue *txq = port->txqs[queue];\r\nif (txq->descs != NULL)\r\nqmap |= (1 << queue);\r\n}\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_Q_CMD_REG, qmap);\r\n}\r\nstatic void mvpp2_egress_disable(struct mvpp2_port *port)\r\n{\r\nu32 reg_data;\r\nint delay;\r\nint tx_port_num = mvpp2_egress_port(port);\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);\r\nreg_data = (mvpp2_read(port->priv, MVPP2_TXP_SCHED_Q_CMD_REG)) &\r\nMVPP2_TXP_SCHED_ENQ_MASK;\r\nif (reg_data != 0)\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_Q_CMD_REG,\r\n(reg_data << MVPP2_TXP_SCHED_DISQ_OFFSET));\r\ndelay = 0;\r\ndo {\r\nif (delay >= MVPP2_TX_DISABLE_TIMEOUT_MSEC) {\r\nnetdev_warn(port->dev,\r\n"Tx stop timed out, status=0x%08x\n",\r\nreg_data);\r\nbreak;\r\n}\r\nmdelay(1);\r\ndelay++;\r\nreg_data = mvpp2_read(port->priv, MVPP2_TXP_SCHED_Q_CMD_REG);\r\n} while (reg_data & MVPP2_TXP_SCHED_ENQ_MASK);\r\n}\r\nstatic inline int\r\nmvpp2_rxq_received(struct mvpp2_port *port, int rxq_id)\r\n{\r\nu32 val = mvpp2_read(port->priv, MVPP2_RXQ_STATUS_REG(rxq_id));\r\nreturn val & MVPP2_RXQ_OCCUPIED_MASK;\r\n}\r\nstatic inline void\r\nmvpp2_rxq_status_update(struct mvpp2_port *port, int rxq_id,\r\nint used_count, int free_count)\r\n{\r\nu32 val = used_count | (free_count << MVPP2_RXQ_NUM_NEW_OFFSET);\r\nmvpp2_write(port->priv, MVPP2_RXQ_STATUS_UPDATE_REG(rxq_id), val);\r\n}\r\nstatic inline struct mvpp2_rx_desc *\r\nmvpp2_rxq_next_desc_get(struct mvpp2_rx_queue *rxq)\r\n{\r\nint rx_desc = rxq->next_desc_to_proc;\r\nrxq->next_desc_to_proc = MVPP2_QUEUE_NEXT_DESC(rxq, rx_desc);\r\nprefetch(rxq->descs + rxq->next_desc_to_proc);\r\nreturn rxq->descs + rx_desc;\r\n}\r\nstatic void mvpp2_rxq_offset_set(struct mvpp2_port *port,\r\nint prxq, int offset)\r\n{\r\nu32 val;\r\noffset = offset >> 5;\r\nval = mvpp2_read(port->priv, MVPP2_RXQ_CONFIG_REG(prxq));\r\nval &= ~MVPP2_RXQ_PACKET_OFFSET_MASK;\r\nval |= ((offset << MVPP2_RXQ_PACKET_OFFSET_OFFS) &\r\nMVPP2_RXQ_PACKET_OFFSET_MASK);\r\nmvpp2_write(port->priv, MVPP2_RXQ_CONFIG_REG(prxq), val);\r\n}\r\nstatic u32 mvpp2_bm_cookie_build(struct mvpp2_rx_desc *rx_desc)\r\n{\r\nint pool = (rx_desc->status & MVPP2_RXD_BM_POOL_ID_MASK) >>\r\nMVPP2_RXD_BM_POOL_ID_OFFS;\r\nint cpu = smp_processor_id();\r\nreturn ((pool & 0xFF) << MVPP2_BM_COOKIE_POOL_OFFS) |\r\n((cpu & 0xFF) << MVPP2_BM_COOKIE_CPU_OFFS);\r\n}\r\nstatic int mvpp2_txq_pend_desc_num_get(struct mvpp2_port *port,\r\nstruct mvpp2_tx_queue *txq)\r\n{\r\nu32 val;\r\nmvpp2_write(port->priv, MVPP2_TXQ_NUM_REG, txq->id);\r\nval = mvpp2_read(port->priv, MVPP2_TXQ_PENDING_REG);\r\nreturn val & MVPP2_TXQ_PENDING_MASK;\r\n}\r\nstatic struct mvpp2_tx_desc *\r\nmvpp2_txq_next_desc_get(struct mvpp2_tx_queue *txq)\r\n{\r\nint tx_desc = txq->next_desc_to_proc;\r\ntxq->next_desc_to_proc = MVPP2_QUEUE_NEXT_DESC(txq, tx_desc);\r\nreturn txq->descs + tx_desc;\r\n}\r\nstatic void mvpp2_aggr_txq_pend_desc_add(struct mvpp2_port *port, int pending)\r\n{\r\nmvpp2_write(port->priv, MVPP2_AGGR_TXQ_UPDATE_REG, pending);\r\n}\r\nstatic int mvpp2_aggr_desc_num_check(struct mvpp2 *priv,\r\nstruct mvpp2_tx_queue *aggr_txq, int num)\r\n{\r\nif ((aggr_txq->count + num) > aggr_txq->size) {\r\nint cpu = smp_processor_id();\r\nu32 val = mvpp2_read(priv, MVPP2_AGGR_TXQ_STATUS_REG(cpu));\r\naggr_txq->count = val & MVPP2_AGGR_TXQ_PENDING_MASK;\r\n}\r\nif ((aggr_txq->count + num) > aggr_txq->size)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic int mvpp2_txq_alloc_reserved_desc(struct mvpp2 *priv,\r\nstruct mvpp2_tx_queue *txq, int num)\r\n{\r\nu32 val;\r\nval = (txq->id << MVPP2_TXQ_RSVD_REQ_Q_OFFSET) | num;\r\nmvpp2_write(priv, MVPP2_TXQ_RSVD_REQ_REG, val);\r\nval = mvpp2_read(priv, MVPP2_TXQ_RSVD_RSLT_REG);\r\nreturn val & MVPP2_TXQ_RSVD_RSLT_MASK;\r\n}\r\nstatic int mvpp2_txq_reserved_desc_num_proc(struct mvpp2 *priv,\r\nstruct mvpp2_tx_queue *txq,\r\nstruct mvpp2_txq_pcpu *txq_pcpu,\r\nint num)\r\n{\r\nint req, cpu, desc_count;\r\nif (txq_pcpu->reserved_num >= num)\r\nreturn 0;\r\ndesc_count = 0;\r\nfor_each_present_cpu(cpu) {\r\nstruct mvpp2_txq_pcpu *txq_pcpu_aux;\r\ntxq_pcpu_aux = per_cpu_ptr(txq->pcpu, cpu);\r\ndesc_count += txq_pcpu_aux->count;\r\ndesc_count += txq_pcpu_aux->reserved_num;\r\n}\r\nreq = max(MVPP2_CPU_DESC_CHUNK, num - txq_pcpu->reserved_num);\r\ndesc_count += req;\r\nif (desc_count >\r\n(txq->size - (num_present_cpus() * MVPP2_CPU_DESC_CHUNK)))\r\nreturn -ENOMEM;\r\ntxq_pcpu->reserved_num += mvpp2_txq_alloc_reserved_desc(priv, txq, req);\r\nif (txq_pcpu->reserved_num < num)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void mvpp2_txq_desc_put(struct mvpp2_tx_queue *txq)\r\n{\r\nif (txq->next_desc_to_proc == 0)\r\ntxq->next_desc_to_proc = txq->last_desc - 1;\r\nelse\r\ntxq->next_desc_to_proc--;\r\n}\r\nstatic u32 mvpp2_txq_desc_csum(int l3_offs, int l3_proto,\r\nint ip_hdr_len, int l4_proto)\r\n{\r\nu32 command;\r\ncommand = (l3_offs << MVPP2_TXD_L3_OFF_SHIFT);\r\ncommand |= (ip_hdr_len << MVPP2_TXD_IP_HLEN_SHIFT);\r\ncommand |= MVPP2_TXD_IP_CSUM_DISABLE;\r\nif (l3_proto == swab16(ETH_P_IP)) {\r\ncommand &= ~MVPP2_TXD_IP_CSUM_DISABLE;\r\ncommand &= ~MVPP2_TXD_L3_IP6;\r\n} else {\r\ncommand |= MVPP2_TXD_L3_IP6;\r\n}\r\nif (l4_proto == IPPROTO_TCP) {\r\ncommand &= ~MVPP2_TXD_L4_UDP;\r\ncommand &= ~MVPP2_TXD_L4_CSUM_FRAG;\r\n} else if (l4_proto == IPPROTO_UDP) {\r\ncommand |= MVPP2_TXD_L4_UDP;\r\ncommand &= ~MVPP2_TXD_L4_CSUM_FRAG;\r\n} else {\r\ncommand |= MVPP2_TXD_L4_CSUM_NOT;\r\n}\r\nreturn command;\r\n}\r\nstatic inline int mvpp2_txq_sent_desc_proc(struct mvpp2_port *port,\r\nstruct mvpp2_tx_queue *txq)\r\n{\r\nu32 val;\r\nval = mvpp2_read(port->priv, MVPP2_TXQ_SENT_REG(txq->id));\r\nreturn (val & MVPP2_TRANSMITTED_COUNT_MASK) >>\r\nMVPP2_TRANSMITTED_COUNT_OFFSET;\r\n}\r\nstatic void mvpp2_txq_sent_counter_clear(void *arg)\r\n{\r\nstruct mvpp2_port *port = arg;\r\nint queue;\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nint id = port->txqs[queue]->id;\r\nmvpp2_read(port->priv, MVPP2_TXQ_SENT_REG(id));\r\n}\r\n}\r\nstatic void mvpp2_txp_max_tx_size_set(struct mvpp2_port *port)\r\n{\r\nu32 val, size, mtu;\r\nint txq, tx_port_num;\r\nmtu = port->pkt_size * 8;\r\nif (mtu > MVPP2_TXP_MTU_MAX)\r\nmtu = MVPP2_TXP_MTU_MAX;\r\nmtu = 3 * mtu;\r\ntx_port_num = mvpp2_egress_port(port);\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);\r\nval = mvpp2_read(port->priv, MVPP2_TXP_SCHED_MTU_REG);\r\nval &= ~MVPP2_TXP_MTU_MAX;\r\nval |= mtu;\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_MTU_REG, val);\r\nval = mvpp2_read(port->priv, MVPP2_TXP_SCHED_TOKEN_SIZE_REG);\r\nsize = val & MVPP2_TXP_TOKEN_SIZE_MAX;\r\nif (size < mtu) {\r\nsize = mtu;\r\nval &= ~MVPP2_TXP_TOKEN_SIZE_MAX;\r\nval |= size;\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_TOKEN_SIZE_REG, val);\r\n}\r\nfor (txq = 0; txq < txq_number; txq++) {\r\nval = mvpp2_read(port->priv,\r\nMVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq));\r\nsize = val & MVPP2_TXQ_TOKEN_SIZE_MAX;\r\nif (size < mtu) {\r\nsize = mtu;\r\nval &= ~MVPP2_TXQ_TOKEN_SIZE_MAX;\r\nval |= size;\r\nmvpp2_write(port->priv,\r\nMVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq),\r\nval);\r\n}\r\n}\r\n}\r\nstatic void mvpp2_rx_pkts_coal_set(struct mvpp2_port *port,\r\nstruct mvpp2_rx_queue *rxq, u32 pkts)\r\n{\r\nu32 val;\r\nval = (pkts & MVPP2_OCCUPIED_THRESH_MASK);\r\nmvpp2_write(port->priv, MVPP2_RXQ_NUM_REG, rxq->id);\r\nmvpp2_write(port->priv, MVPP2_RXQ_THRESH_REG, val);\r\nrxq->pkts_coal = pkts;\r\n}\r\nstatic void mvpp2_rx_time_coal_set(struct mvpp2_port *port,\r\nstruct mvpp2_rx_queue *rxq, u32 usec)\r\n{\r\nu32 val;\r\nval = (port->priv->tclk / USEC_PER_SEC) * usec;\r\nmvpp2_write(port->priv, MVPP2_ISR_RX_THRESHOLD_REG(rxq->id), val);\r\nrxq->time_coal = usec;\r\n}\r\nstatic void mvpp2_txq_bufs_free(struct mvpp2_port *port,\r\nstruct mvpp2_tx_queue *txq,\r\nstruct mvpp2_txq_pcpu *txq_pcpu, int num)\r\n{\r\nint i;\r\nfor (i = 0; i < num; i++) {\r\ndma_addr_t buf_phys_addr =\r\ntxq_pcpu->tx_buffs[txq_pcpu->txq_get_index];\r\nstruct sk_buff *skb = txq_pcpu->tx_skb[txq_pcpu->txq_get_index];\r\nmvpp2_txq_inc_get(txq_pcpu);\r\nif (!skb)\r\ncontinue;\r\ndma_unmap_single(port->dev->dev.parent, buf_phys_addr,\r\nskb_headlen(skb), DMA_TO_DEVICE);\r\ndev_kfree_skb_any(skb);\r\n}\r\n}\r\nstatic inline struct mvpp2_rx_queue *mvpp2_get_rx_queue(struct mvpp2_port *port,\r\nu32 cause)\r\n{\r\nint queue = fls(cause) - 1;\r\nreturn port->rxqs[queue];\r\n}\r\nstatic inline struct mvpp2_tx_queue *mvpp2_get_tx_queue(struct mvpp2_port *port,\r\nu32 cause)\r\n{\r\nint queue = fls(cause) - 1;\r\nreturn port->txqs[queue];\r\n}\r\nstatic void mvpp2_txq_done(struct mvpp2_port *port, struct mvpp2_tx_queue *txq,\r\nstruct mvpp2_txq_pcpu *txq_pcpu)\r\n{\r\nstruct netdev_queue *nq = netdev_get_tx_queue(port->dev, txq->log_id);\r\nint tx_done;\r\nif (txq_pcpu->cpu != smp_processor_id())\r\nnetdev_err(port->dev, "wrong cpu on the end of Tx processing\n");\r\ntx_done = mvpp2_txq_sent_desc_proc(port, txq);\r\nif (!tx_done)\r\nreturn;\r\nmvpp2_txq_bufs_free(port, txq, txq_pcpu, tx_done);\r\ntxq_pcpu->count -= tx_done;\r\nif (netif_tx_queue_stopped(nq))\r\nif (txq_pcpu->size - txq_pcpu->count >= MAX_SKB_FRAGS + 1)\r\nnetif_tx_wake_queue(nq);\r\n}\r\nstatic unsigned int mvpp2_tx_done(struct mvpp2_port *port, u32 cause)\r\n{\r\nstruct mvpp2_tx_queue *txq;\r\nstruct mvpp2_txq_pcpu *txq_pcpu;\r\nunsigned int tx_todo = 0;\r\nwhile (cause) {\r\ntxq = mvpp2_get_tx_queue(port, cause);\r\nif (!txq)\r\nbreak;\r\ntxq_pcpu = this_cpu_ptr(txq->pcpu);\r\nif (txq_pcpu->count) {\r\nmvpp2_txq_done(port, txq, txq_pcpu);\r\ntx_todo += txq_pcpu->count;\r\n}\r\ncause &= ~(1 << txq->log_id);\r\n}\r\nreturn tx_todo;\r\n}\r\nstatic int mvpp2_aggr_txq_init(struct platform_device *pdev,\r\nstruct mvpp2_tx_queue *aggr_txq,\r\nint desc_num, int cpu,\r\nstruct mvpp2 *priv)\r\n{\r\naggr_txq->descs = dma_alloc_coherent(&pdev->dev,\r\ndesc_num * MVPP2_DESC_ALIGNED_SIZE,\r\n&aggr_txq->descs_phys, GFP_KERNEL);\r\nif (!aggr_txq->descs)\r\nreturn -ENOMEM;\r\nBUG_ON(aggr_txq->descs !=\r\nPTR_ALIGN(aggr_txq->descs, MVPP2_CPU_D_CACHE_LINE_SIZE));\r\naggr_txq->last_desc = aggr_txq->size - 1;\r\naggr_txq->next_desc_to_proc = mvpp2_read(priv,\r\nMVPP2_AGGR_TXQ_INDEX_REG(cpu));\r\nmvpp2_write(priv, MVPP2_AGGR_TXQ_DESC_ADDR_REG(cpu),\r\naggr_txq->descs_phys);\r\nmvpp2_write(priv, MVPP2_AGGR_TXQ_DESC_SIZE_REG(cpu), desc_num);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_rxq_init(struct mvpp2_port *port,\r\nstruct mvpp2_rx_queue *rxq)\r\n{\r\nrxq->size = port->rx_ring_size;\r\nrxq->descs = dma_alloc_coherent(port->dev->dev.parent,\r\nrxq->size * MVPP2_DESC_ALIGNED_SIZE,\r\n&rxq->descs_phys, GFP_KERNEL);\r\nif (!rxq->descs)\r\nreturn -ENOMEM;\r\nBUG_ON(rxq->descs !=\r\nPTR_ALIGN(rxq->descs, MVPP2_CPU_D_CACHE_LINE_SIZE));\r\nrxq->last_desc = rxq->size - 1;\r\nmvpp2_write(port->priv, MVPP2_RXQ_STATUS_REG(rxq->id), 0);\r\nmvpp2_write(port->priv, MVPP2_RXQ_NUM_REG, rxq->id);\r\nmvpp2_write(port->priv, MVPP2_RXQ_DESC_ADDR_REG, rxq->descs_phys);\r\nmvpp2_write(port->priv, MVPP2_RXQ_DESC_SIZE_REG, rxq->size);\r\nmvpp2_write(port->priv, MVPP2_RXQ_INDEX_REG, 0);\r\nmvpp2_rxq_offset_set(port, rxq->id, NET_SKB_PAD);\r\nmvpp2_rx_pkts_coal_set(port, rxq, rxq->pkts_coal);\r\nmvpp2_rx_time_coal_set(port, rxq, rxq->time_coal);\r\nmvpp2_rxq_status_update(port, rxq->id, 0, rxq->size);\r\nreturn 0;\r\n}\r\nstatic void mvpp2_rxq_drop_pkts(struct mvpp2_port *port,\r\nstruct mvpp2_rx_queue *rxq)\r\n{\r\nint rx_received, i;\r\nrx_received = mvpp2_rxq_received(port, rxq->id);\r\nif (!rx_received)\r\nreturn;\r\nfor (i = 0; i < rx_received; i++) {\r\nstruct mvpp2_rx_desc *rx_desc = mvpp2_rxq_next_desc_get(rxq);\r\nu32 bm = mvpp2_bm_cookie_build(rx_desc);\r\nmvpp2_pool_refill(port, bm, rx_desc->buf_phys_addr,\r\nrx_desc->buf_cookie);\r\n}\r\nmvpp2_rxq_status_update(port, rxq->id, rx_received, rx_received);\r\n}\r\nstatic void mvpp2_rxq_deinit(struct mvpp2_port *port,\r\nstruct mvpp2_rx_queue *rxq)\r\n{\r\nmvpp2_rxq_drop_pkts(port, rxq);\r\nif (rxq->descs)\r\ndma_free_coherent(port->dev->dev.parent,\r\nrxq->size * MVPP2_DESC_ALIGNED_SIZE,\r\nrxq->descs,\r\nrxq->descs_phys);\r\nrxq->descs = NULL;\r\nrxq->last_desc = 0;\r\nrxq->next_desc_to_proc = 0;\r\nrxq->descs_phys = 0;\r\nmvpp2_write(port->priv, MVPP2_RXQ_STATUS_REG(rxq->id), 0);\r\nmvpp2_write(port->priv, MVPP2_RXQ_NUM_REG, rxq->id);\r\nmvpp2_write(port->priv, MVPP2_RXQ_DESC_ADDR_REG, 0);\r\nmvpp2_write(port->priv, MVPP2_RXQ_DESC_SIZE_REG, 0);\r\n}\r\nstatic int mvpp2_txq_init(struct mvpp2_port *port,\r\nstruct mvpp2_tx_queue *txq)\r\n{\r\nu32 val;\r\nint cpu, desc, desc_per_txq, tx_port_num;\r\nstruct mvpp2_txq_pcpu *txq_pcpu;\r\ntxq->size = port->tx_ring_size;\r\ntxq->descs = dma_alloc_coherent(port->dev->dev.parent,\r\ntxq->size * MVPP2_DESC_ALIGNED_SIZE,\r\n&txq->descs_phys, GFP_KERNEL);\r\nif (!txq->descs)\r\nreturn -ENOMEM;\r\nBUG_ON(txq->descs !=\r\nPTR_ALIGN(txq->descs, MVPP2_CPU_D_CACHE_LINE_SIZE));\r\ntxq->last_desc = txq->size - 1;\r\nmvpp2_write(port->priv, MVPP2_TXQ_NUM_REG, txq->id);\r\nmvpp2_write(port->priv, MVPP2_TXQ_DESC_ADDR_REG, txq->descs_phys);\r\nmvpp2_write(port->priv, MVPP2_TXQ_DESC_SIZE_REG, txq->size &\r\nMVPP2_TXQ_DESC_SIZE_MASK);\r\nmvpp2_write(port->priv, MVPP2_TXQ_INDEX_REG, 0);\r\nmvpp2_write(port->priv, MVPP2_TXQ_RSVD_CLR_REG,\r\ntxq->id << MVPP2_TXQ_RSVD_CLR_OFFSET);\r\nval = mvpp2_read(port->priv, MVPP2_TXQ_PENDING_REG);\r\nval &= ~MVPP2_TXQ_PENDING_MASK;\r\nmvpp2_write(port->priv, MVPP2_TXQ_PENDING_REG, val);\r\ndesc_per_txq = 16;\r\ndesc = (port->id * MVPP2_MAX_TXQ * desc_per_txq) +\r\n(txq->log_id * desc_per_txq);\r\nmvpp2_write(port->priv, MVPP2_TXQ_PREF_BUF_REG,\r\nMVPP2_PREF_BUF_PTR(desc) | MVPP2_PREF_BUF_SIZE_16 |\r\nMVPP2_PREF_BUF_THRESH(desc_per_txq/2));\r\ntx_port_num = mvpp2_egress_port(port);\r\nmvpp2_write(port->priv, MVPP2_TXP_SCHED_PORT_INDEX_REG, tx_port_num);\r\nval = mvpp2_read(port->priv, MVPP2_TXQ_SCHED_REFILL_REG(txq->log_id));\r\nval &= ~MVPP2_TXQ_REFILL_PERIOD_ALL_MASK;\r\nval |= MVPP2_TXQ_REFILL_PERIOD_MASK(1);\r\nval |= MVPP2_TXQ_REFILL_TOKENS_ALL_MASK;\r\nmvpp2_write(port->priv, MVPP2_TXQ_SCHED_REFILL_REG(txq->log_id), val);\r\nval = MVPP2_TXQ_TOKEN_SIZE_MAX;\r\nmvpp2_write(port->priv, MVPP2_TXQ_SCHED_TOKEN_SIZE_REG(txq->log_id),\r\nval);\r\nfor_each_present_cpu(cpu) {\r\ntxq_pcpu = per_cpu_ptr(txq->pcpu, cpu);\r\ntxq_pcpu->size = txq->size;\r\ntxq_pcpu->tx_skb = kmalloc(txq_pcpu->size *\r\nsizeof(*txq_pcpu->tx_skb),\r\nGFP_KERNEL);\r\nif (!txq_pcpu->tx_skb)\r\ngoto error;\r\ntxq_pcpu->tx_buffs = kmalloc(txq_pcpu->size *\r\nsizeof(dma_addr_t), GFP_KERNEL);\r\nif (!txq_pcpu->tx_buffs)\r\ngoto error;\r\ntxq_pcpu->count = 0;\r\ntxq_pcpu->reserved_num = 0;\r\ntxq_pcpu->txq_put_index = 0;\r\ntxq_pcpu->txq_get_index = 0;\r\n}\r\nreturn 0;\r\nerror:\r\nfor_each_present_cpu(cpu) {\r\ntxq_pcpu = per_cpu_ptr(txq->pcpu, cpu);\r\nkfree(txq_pcpu->tx_skb);\r\nkfree(txq_pcpu->tx_buffs);\r\n}\r\ndma_free_coherent(port->dev->dev.parent,\r\ntxq->size * MVPP2_DESC_ALIGNED_SIZE,\r\ntxq->descs, txq->descs_phys);\r\nreturn -ENOMEM;\r\n}\r\nstatic void mvpp2_txq_deinit(struct mvpp2_port *port,\r\nstruct mvpp2_tx_queue *txq)\r\n{\r\nstruct mvpp2_txq_pcpu *txq_pcpu;\r\nint cpu;\r\nfor_each_present_cpu(cpu) {\r\ntxq_pcpu = per_cpu_ptr(txq->pcpu, cpu);\r\nkfree(txq_pcpu->tx_skb);\r\nkfree(txq_pcpu->tx_buffs);\r\n}\r\nif (txq->descs)\r\ndma_free_coherent(port->dev->dev.parent,\r\ntxq->size * MVPP2_DESC_ALIGNED_SIZE,\r\ntxq->descs, txq->descs_phys);\r\ntxq->descs = NULL;\r\ntxq->last_desc = 0;\r\ntxq->next_desc_to_proc = 0;\r\ntxq->descs_phys = 0;\r\nmvpp2_write(port->priv, MVPP2_TXQ_SCHED_TOKEN_CNTR_REG(txq->id), 0);\r\nmvpp2_write(port->priv, MVPP2_TXQ_NUM_REG, txq->id);\r\nmvpp2_write(port->priv, MVPP2_TXQ_DESC_ADDR_REG, 0);\r\nmvpp2_write(port->priv, MVPP2_TXQ_DESC_SIZE_REG, 0);\r\n}\r\nstatic void mvpp2_txq_clean(struct mvpp2_port *port, struct mvpp2_tx_queue *txq)\r\n{\r\nstruct mvpp2_txq_pcpu *txq_pcpu;\r\nint delay, pending, cpu;\r\nu32 val;\r\nmvpp2_write(port->priv, MVPP2_TXQ_NUM_REG, txq->id);\r\nval = mvpp2_read(port->priv, MVPP2_TXQ_PREF_BUF_REG);\r\nval |= MVPP2_TXQ_DRAIN_EN_MASK;\r\nmvpp2_write(port->priv, MVPP2_TXQ_PREF_BUF_REG, val);\r\ndelay = 0;\r\ndo {\r\nif (delay >= MVPP2_TX_PENDING_TIMEOUT_MSEC) {\r\nnetdev_warn(port->dev,\r\n"port %d: cleaning queue %d timed out\n",\r\nport->id, txq->log_id);\r\nbreak;\r\n}\r\nmdelay(1);\r\ndelay++;\r\npending = mvpp2_txq_pend_desc_num_get(port, txq);\r\n} while (pending);\r\nval &= ~MVPP2_TXQ_DRAIN_EN_MASK;\r\nmvpp2_write(port->priv, MVPP2_TXQ_PREF_BUF_REG, val);\r\nfor_each_present_cpu(cpu) {\r\ntxq_pcpu = per_cpu_ptr(txq->pcpu, cpu);\r\nmvpp2_txq_bufs_free(port, txq, txq_pcpu, txq_pcpu->count);\r\ntxq_pcpu->count = 0;\r\ntxq_pcpu->txq_put_index = 0;\r\ntxq_pcpu->txq_get_index = 0;\r\n}\r\n}\r\nstatic void mvpp2_cleanup_txqs(struct mvpp2_port *port)\r\n{\r\nstruct mvpp2_tx_queue *txq;\r\nint queue;\r\nu32 val;\r\nval = mvpp2_read(port->priv, MVPP2_TX_PORT_FLUSH_REG);\r\nval |= MVPP2_TX_PORT_FLUSH_MASK(port->id);\r\nmvpp2_write(port->priv, MVPP2_TX_PORT_FLUSH_REG, val);\r\nfor (queue = 0; queue < txq_number; queue++) {\r\ntxq = port->txqs[queue];\r\nmvpp2_txq_clean(port, txq);\r\nmvpp2_txq_deinit(port, txq);\r\n}\r\non_each_cpu(mvpp2_txq_sent_counter_clear, port, 1);\r\nval &= ~MVPP2_TX_PORT_FLUSH_MASK(port->id);\r\nmvpp2_write(port->priv, MVPP2_TX_PORT_FLUSH_REG, val);\r\n}\r\nstatic void mvpp2_cleanup_rxqs(struct mvpp2_port *port)\r\n{\r\nint queue;\r\nfor (queue = 0; queue < rxq_number; queue++)\r\nmvpp2_rxq_deinit(port, port->rxqs[queue]);\r\n}\r\nstatic int mvpp2_setup_rxqs(struct mvpp2_port *port)\r\n{\r\nint queue, err;\r\nfor (queue = 0; queue < rxq_number; queue++) {\r\nerr = mvpp2_rxq_init(port, port->rxqs[queue]);\r\nif (err)\r\ngoto err_cleanup;\r\n}\r\nreturn 0;\r\nerr_cleanup:\r\nmvpp2_cleanup_rxqs(port);\r\nreturn err;\r\n}\r\nstatic int mvpp2_setup_txqs(struct mvpp2_port *port)\r\n{\r\nstruct mvpp2_tx_queue *txq;\r\nint queue, err;\r\nfor (queue = 0; queue < txq_number; queue++) {\r\ntxq = port->txqs[queue];\r\nerr = mvpp2_txq_init(port, txq);\r\nif (err)\r\ngoto err_cleanup;\r\n}\r\non_each_cpu(mvpp2_txq_sent_counter_clear, port, 1);\r\nreturn 0;\r\nerr_cleanup:\r\nmvpp2_cleanup_txqs(port);\r\nreturn err;\r\n}\r\nstatic irqreturn_t mvpp2_isr(int irq, void *dev_id)\r\n{\r\nstruct mvpp2_port *port = (struct mvpp2_port *)dev_id;\r\nmvpp2_interrupts_disable(port);\r\nnapi_schedule(&port->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void mvpp2_link_event(struct net_device *dev)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nstruct phy_device *phydev = port->phy_dev;\r\nint status_change = 0;\r\nu32 val;\r\nif (phydev->link) {\r\nif ((port->speed != phydev->speed) ||\r\n(port->duplex != phydev->duplex)) {\r\nu32 val;\r\nval = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);\r\nval &= ~(MVPP2_GMAC_CONFIG_MII_SPEED |\r\nMVPP2_GMAC_CONFIG_GMII_SPEED |\r\nMVPP2_GMAC_CONFIG_FULL_DUPLEX |\r\nMVPP2_GMAC_AN_SPEED_EN |\r\nMVPP2_GMAC_AN_DUPLEX_EN);\r\nif (phydev->duplex)\r\nval |= MVPP2_GMAC_CONFIG_FULL_DUPLEX;\r\nif (phydev->speed == SPEED_1000)\r\nval |= MVPP2_GMAC_CONFIG_GMII_SPEED;\r\nelse if (phydev->speed == SPEED_100)\r\nval |= MVPP2_GMAC_CONFIG_MII_SPEED;\r\nwritel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);\r\nport->duplex = phydev->duplex;\r\nport->speed = phydev->speed;\r\n}\r\n}\r\nif (phydev->link != port->link) {\r\nif (!phydev->link) {\r\nport->duplex = -1;\r\nport->speed = 0;\r\n}\r\nport->link = phydev->link;\r\nstatus_change = 1;\r\n}\r\nif (status_change) {\r\nif (phydev->link) {\r\nval = readl(port->base + MVPP2_GMAC_AUTONEG_CONFIG);\r\nval |= (MVPP2_GMAC_FORCE_LINK_PASS |\r\nMVPP2_GMAC_FORCE_LINK_DOWN);\r\nwritel(val, port->base + MVPP2_GMAC_AUTONEG_CONFIG);\r\nmvpp2_egress_enable(port);\r\nmvpp2_ingress_enable(port);\r\n} else {\r\nmvpp2_ingress_disable(port);\r\nmvpp2_egress_disable(port);\r\n}\r\nphy_print_status(phydev);\r\n}\r\n}\r\nstatic void mvpp2_timer_set(struct mvpp2_port_pcpu *port_pcpu)\r\n{\r\nktime_t interval;\r\nif (!port_pcpu->timer_scheduled) {\r\nport_pcpu->timer_scheduled = true;\r\ninterval = ktime_set(0, MVPP2_TXDONE_HRTIMER_PERIOD_NS);\r\nhrtimer_start(&port_pcpu->tx_done_timer, interval,\r\nHRTIMER_MODE_REL_PINNED);\r\n}\r\n}\r\nstatic void mvpp2_tx_proc_cb(unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *)data;\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nstruct mvpp2_port_pcpu *port_pcpu = this_cpu_ptr(port->pcpu);\r\nunsigned int tx_todo, cause;\r\nif (!netif_running(dev))\r\nreturn;\r\nport_pcpu->timer_scheduled = false;\r\ncause = (1 << txq_number) - 1;\r\ntx_todo = mvpp2_tx_done(port, cause);\r\nif (tx_todo)\r\nmvpp2_timer_set(port_pcpu);\r\n}\r\nstatic enum hrtimer_restart mvpp2_hr_timer_cb(struct hrtimer *timer)\r\n{\r\nstruct mvpp2_port_pcpu *port_pcpu = container_of(timer,\r\nstruct mvpp2_port_pcpu,\r\ntx_done_timer);\r\ntasklet_schedule(&port_pcpu->tx_done_tasklet);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nstatic void mvpp2_rx_error(struct mvpp2_port *port,\r\nstruct mvpp2_rx_desc *rx_desc)\r\n{\r\nu32 status = rx_desc->status;\r\nswitch (status & MVPP2_RXD_ERR_CODE_MASK) {\r\ncase MVPP2_RXD_ERR_CRC:\r\nnetdev_err(port->dev, "bad rx status %08x (crc error), size=%d\n",\r\nstatus, rx_desc->data_size);\r\nbreak;\r\ncase MVPP2_RXD_ERR_OVERRUN:\r\nnetdev_err(port->dev, "bad rx status %08x (overrun error), size=%d\n",\r\nstatus, rx_desc->data_size);\r\nbreak;\r\ncase MVPP2_RXD_ERR_RESOURCE:\r\nnetdev_err(port->dev, "bad rx status %08x (resource error), size=%d\n",\r\nstatus, rx_desc->data_size);\r\nbreak;\r\n}\r\n}\r\nstatic void mvpp2_rx_csum(struct mvpp2_port *port, u32 status,\r\nstruct sk_buff *skb)\r\n{\r\nif (((status & MVPP2_RXD_L3_IP4) &&\r\n!(status & MVPP2_RXD_IP4_HEADER_ERR)) ||\r\n(status & MVPP2_RXD_L3_IP6))\r\nif (((status & MVPP2_RXD_L4_UDP) ||\r\n(status & MVPP2_RXD_L4_TCP)) &&\r\n(status & MVPP2_RXD_L4_CSUM_OK)) {\r\nskb->csum = 0;\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nreturn;\r\n}\r\nskb->ip_summed = CHECKSUM_NONE;\r\n}\r\nstatic int mvpp2_rx_refill(struct mvpp2_port *port,\r\nstruct mvpp2_bm_pool *bm_pool,\r\nu32 bm, int is_recycle)\r\n{\r\nstruct sk_buff *skb;\r\ndma_addr_t phys_addr;\r\nif (is_recycle &&\r\n(atomic_read(&bm_pool->in_use) < bm_pool->in_use_thresh))\r\nreturn 0;\r\nskb = mvpp2_skb_alloc(port, bm_pool, &phys_addr, GFP_ATOMIC);\r\nif (!skb)\r\nreturn -ENOMEM;\r\nmvpp2_pool_refill(port, bm, (u32)phys_addr, (u32)skb);\r\natomic_dec(&bm_pool->in_use);\r\nreturn 0;\r\n}\r\nstatic u32 mvpp2_skb_tx_csum(struct mvpp2_port *port, struct sk_buff *skb)\r\n{\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nint ip_hdr_len = 0;\r\nu8 l4_proto;\r\nif (skb->protocol == htons(ETH_P_IP)) {\r\nstruct iphdr *ip4h = ip_hdr(skb);\r\nip_hdr_len = ip4h->ihl;\r\nl4_proto = ip4h->protocol;\r\n} else if (skb->protocol == htons(ETH_P_IPV6)) {\r\nstruct ipv6hdr *ip6h = ipv6_hdr(skb);\r\nif (skb_network_header_len(skb) > 0)\r\nip_hdr_len = (skb_network_header_len(skb) >> 2);\r\nl4_proto = ip6h->nexthdr;\r\n} else {\r\nreturn MVPP2_TXD_L4_CSUM_NOT;\r\n}\r\nreturn mvpp2_txq_desc_csum(skb_network_offset(skb),\r\nskb->protocol, ip_hdr_len, l4_proto);\r\n}\r\nreturn MVPP2_TXD_L4_CSUM_NOT | MVPP2_TXD_IP_CSUM_DISABLE;\r\n}\r\nstatic void mvpp2_buff_hdr_rx(struct mvpp2_port *port,\r\nstruct mvpp2_rx_desc *rx_desc)\r\n{\r\nstruct mvpp2_buff_hdr *buff_hdr;\r\nstruct sk_buff *skb;\r\nu32 rx_status = rx_desc->status;\r\nu32 buff_phys_addr;\r\nu32 buff_virt_addr;\r\nu32 buff_phys_addr_next;\r\nu32 buff_virt_addr_next;\r\nint mc_id;\r\nint pool_id;\r\npool_id = (rx_status & MVPP2_RXD_BM_POOL_ID_MASK) >>\r\nMVPP2_RXD_BM_POOL_ID_OFFS;\r\nbuff_phys_addr = rx_desc->buf_phys_addr;\r\nbuff_virt_addr = rx_desc->buf_cookie;\r\ndo {\r\nskb = (struct sk_buff *)buff_virt_addr;\r\nbuff_hdr = (struct mvpp2_buff_hdr *)skb->head;\r\nmc_id = MVPP2_B_HDR_INFO_MC_ID(buff_hdr->info);\r\nbuff_phys_addr_next = buff_hdr->next_buff_phys_addr;\r\nbuff_virt_addr_next = buff_hdr->next_buff_virt_addr;\r\nmvpp2_bm_pool_mc_put(port, pool_id, buff_phys_addr,\r\nbuff_virt_addr, mc_id);\r\nbuff_phys_addr = buff_phys_addr_next;\r\nbuff_virt_addr = buff_virt_addr_next;\r\n} while (!MVPP2_B_HDR_INFO_IS_LAST(buff_hdr->info));\r\n}\r\nstatic int mvpp2_rx(struct mvpp2_port *port, int rx_todo,\r\nstruct mvpp2_rx_queue *rxq)\r\n{\r\nstruct net_device *dev = port->dev;\r\nint rx_received, rx_filled, i;\r\nu32 rcvd_pkts = 0;\r\nu32 rcvd_bytes = 0;\r\nrx_received = mvpp2_rxq_received(port, rxq->id);\r\nif (rx_todo > rx_received)\r\nrx_todo = rx_received;\r\nrx_filled = 0;\r\nfor (i = 0; i < rx_todo; i++) {\r\nstruct mvpp2_rx_desc *rx_desc = mvpp2_rxq_next_desc_get(rxq);\r\nstruct mvpp2_bm_pool *bm_pool;\r\nstruct sk_buff *skb;\r\nu32 bm, rx_status;\r\nint pool, rx_bytes, err;\r\nrx_filled++;\r\nrx_status = rx_desc->status;\r\nrx_bytes = rx_desc->data_size - MVPP2_MH_SIZE;\r\nbm = mvpp2_bm_cookie_build(rx_desc);\r\npool = mvpp2_bm_cookie_pool_get(bm);\r\nbm_pool = &port->priv->bm_pools[pool];\r\nif (rx_status & MVPP2_RXD_BUF_HDR) {\r\nmvpp2_buff_hdr_rx(port, rx_desc);\r\ncontinue;\r\n}\r\nif (rx_status & MVPP2_RXD_ERR_SUMMARY) {\r\ndev->stats.rx_errors++;\r\nmvpp2_rx_error(port, rx_desc);\r\nmvpp2_pool_refill(port, bm, rx_desc->buf_phys_addr,\r\nrx_desc->buf_cookie);\r\ncontinue;\r\n}\r\nskb = (struct sk_buff *)rx_desc->buf_cookie;\r\nrcvd_pkts++;\r\nrcvd_bytes += rx_bytes;\r\natomic_inc(&bm_pool->in_use);\r\nskb_reserve(skb, MVPP2_MH_SIZE);\r\nskb_put(skb, rx_bytes);\r\nskb->protocol = eth_type_trans(skb, dev);\r\nmvpp2_rx_csum(port, rx_status, skb);\r\nnapi_gro_receive(&port->napi, skb);\r\nerr = mvpp2_rx_refill(port, bm_pool, bm, 0);\r\nif (err) {\r\nnetdev_err(port->dev, "failed to refill BM pools\n");\r\nrx_filled--;\r\n}\r\n}\r\nif (rcvd_pkts) {\r\nstruct mvpp2_pcpu_stats *stats = this_cpu_ptr(port->stats);\r\nu64_stats_update_begin(&stats->syncp);\r\nstats->rx_packets += rcvd_pkts;\r\nstats->rx_bytes += rcvd_bytes;\r\nu64_stats_update_end(&stats->syncp);\r\n}\r\nwmb();\r\nmvpp2_rxq_status_update(port, rxq->id, rx_todo, rx_filled);\r\nreturn rx_todo;\r\n}\r\nstatic inline void\r\ntx_desc_unmap_put(struct device *dev, struct mvpp2_tx_queue *txq,\r\nstruct mvpp2_tx_desc *desc)\r\n{\r\ndma_unmap_single(dev, desc->buf_phys_addr,\r\ndesc->data_size, DMA_TO_DEVICE);\r\nmvpp2_txq_desc_put(txq);\r\n}\r\nstatic int mvpp2_tx_frag_process(struct mvpp2_port *port, struct sk_buff *skb,\r\nstruct mvpp2_tx_queue *aggr_txq,\r\nstruct mvpp2_tx_queue *txq)\r\n{\r\nstruct mvpp2_txq_pcpu *txq_pcpu = this_cpu_ptr(txq->pcpu);\r\nstruct mvpp2_tx_desc *tx_desc;\r\nint i;\r\ndma_addr_t buf_phys_addr;\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nvoid *addr = page_address(frag->page.p) + frag->page_offset;\r\ntx_desc = mvpp2_txq_next_desc_get(aggr_txq);\r\ntx_desc->phys_txq = txq->id;\r\ntx_desc->data_size = frag->size;\r\nbuf_phys_addr = dma_map_single(port->dev->dev.parent, addr,\r\ntx_desc->data_size,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(port->dev->dev.parent, buf_phys_addr)) {\r\nmvpp2_txq_desc_put(txq);\r\ngoto error;\r\n}\r\ntx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_ALIGN;\r\ntx_desc->buf_phys_addr = buf_phys_addr & (~MVPP2_TX_DESC_ALIGN);\r\nif (i == (skb_shinfo(skb)->nr_frags - 1)) {\r\ntx_desc->command = MVPP2_TXD_L_DESC;\r\nmvpp2_txq_inc_put(txq_pcpu, skb, tx_desc);\r\n} else {\r\ntx_desc->command = 0;\r\nmvpp2_txq_inc_put(txq_pcpu, NULL, tx_desc);\r\n}\r\n}\r\nreturn 0;\r\nerror:\r\nfor (i = i - 1; i >= 0; i--) {\r\ntx_desc = txq->descs + i;\r\ntx_desc_unmap_put(port->dev->dev.parent, txq, tx_desc);\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic int mvpp2_tx(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nstruct mvpp2_tx_queue *txq, *aggr_txq;\r\nstruct mvpp2_txq_pcpu *txq_pcpu;\r\nstruct mvpp2_tx_desc *tx_desc;\r\ndma_addr_t buf_phys_addr;\r\nint frags = 0;\r\nu16 txq_id;\r\nu32 tx_cmd;\r\ntxq_id = skb_get_queue_mapping(skb);\r\ntxq = port->txqs[txq_id];\r\ntxq_pcpu = this_cpu_ptr(txq->pcpu);\r\naggr_txq = &port->priv->aggr_txqs[smp_processor_id()];\r\nfrags = skb_shinfo(skb)->nr_frags + 1;\r\nif (mvpp2_aggr_desc_num_check(port->priv, aggr_txq, frags) ||\r\nmvpp2_txq_reserved_desc_num_proc(port->priv, txq,\r\ntxq_pcpu, frags)) {\r\nfrags = 0;\r\ngoto out;\r\n}\r\ntx_desc = mvpp2_txq_next_desc_get(aggr_txq);\r\ntx_desc->phys_txq = txq->id;\r\ntx_desc->data_size = skb_headlen(skb);\r\nbuf_phys_addr = dma_map_single(dev->dev.parent, skb->data,\r\ntx_desc->data_size, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev->dev.parent, buf_phys_addr))) {\r\nmvpp2_txq_desc_put(txq);\r\nfrags = 0;\r\ngoto out;\r\n}\r\ntx_desc->packet_offset = buf_phys_addr & MVPP2_TX_DESC_ALIGN;\r\ntx_desc->buf_phys_addr = buf_phys_addr & ~MVPP2_TX_DESC_ALIGN;\r\ntx_cmd = mvpp2_skb_tx_csum(port, skb);\r\nif (frags == 1) {\r\ntx_cmd |= MVPP2_TXD_F_DESC | MVPP2_TXD_L_DESC;\r\ntx_desc->command = tx_cmd;\r\nmvpp2_txq_inc_put(txq_pcpu, skb, tx_desc);\r\n} else {\r\ntx_cmd |= MVPP2_TXD_F_DESC | MVPP2_TXD_PADDING_DISABLE;\r\ntx_desc->command = tx_cmd;\r\nmvpp2_txq_inc_put(txq_pcpu, NULL, tx_desc);\r\nif (mvpp2_tx_frag_process(port, skb, aggr_txq, txq)) {\r\ntx_desc_unmap_put(port->dev->dev.parent, txq, tx_desc);\r\nfrags = 0;\r\ngoto out;\r\n}\r\n}\r\ntxq_pcpu->reserved_num -= frags;\r\ntxq_pcpu->count += frags;\r\naggr_txq->count += frags;\r\nwmb();\r\nmvpp2_aggr_txq_pend_desc_add(port, frags);\r\nif (txq_pcpu->size - txq_pcpu->count < MAX_SKB_FRAGS + 1) {\r\nstruct netdev_queue *nq = netdev_get_tx_queue(dev, txq_id);\r\nnetif_tx_stop_queue(nq);\r\n}\r\nout:\r\nif (frags > 0) {\r\nstruct mvpp2_pcpu_stats *stats = this_cpu_ptr(port->stats);\r\nu64_stats_update_begin(&stats->syncp);\r\nstats->tx_packets++;\r\nstats->tx_bytes += skb->len;\r\nu64_stats_update_end(&stats->syncp);\r\n} else {\r\ndev->stats.tx_dropped++;\r\ndev_kfree_skb_any(skb);\r\n}\r\nif (txq_pcpu->count >= txq->done_pkts_coal)\r\nmvpp2_txq_done(port, txq, txq_pcpu);\r\nif (txq_pcpu->count <= frags && txq_pcpu->count > 0) {\r\nstruct mvpp2_port_pcpu *port_pcpu = this_cpu_ptr(port->pcpu);\r\nmvpp2_timer_set(port_pcpu);\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic inline void mvpp2_cause_error(struct net_device *dev, int cause)\r\n{\r\nif (cause & MVPP2_CAUSE_FCS_ERR_MASK)\r\nnetdev_err(dev, "FCS error\n");\r\nif (cause & MVPP2_CAUSE_RX_FIFO_OVERRUN_MASK)\r\nnetdev_err(dev, "rx fifo overrun error\n");\r\nif (cause & MVPP2_CAUSE_TX_FIFO_UNDERRUN_MASK)\r\nnetdev_err(dev, "tx fifo underrun error\n");\r\n}\r\nstatic int mvpp2_poll(struct napi_struct *napi, int budget)\r\n{\r\nu32 cause_rx_tx, cause_rx, cause_misc;\r\nint rx_done = 0;\r\nstruct mvpp2_port *port = netdev_priv(napi->dev);\r\ncause_rx_tx = mvpp2_read(port->priv,\r\nMVPP2_ISR_RX_TX_CAUSE_REG(port->id));\r\ncause_rx_tx &= ~MVPP2_CAUSE_TXQ_OCCUP_DESC_ALL_MASK;\r\ncause_misc = cause_rx_tx & MVPP2_CAUSE_MISC_SUM_MASK;\r\nif (cause_misc) {\r\nmvpp2_cause_error(port->dev, cause_misc);\r\nmvpp2_write(port->priv, MVPP2_ISR_MISC_CAUSE_REG, 0);\r\nmvpp2_write(port->priv, MVPP2_ISR_RX_TX_CAUSE_REG(port->id),\r\ncause_rx_tx & ~MVPP2_CAUSE_MISC_SUM_MASK);\r\n}\r\ncause_rx = cause_rx_tx & MVPP2_CAUSE_RXQ_OCCUP_DESC_ALL_MASK;\r\ncause_rx |= port->pending_cause_rx;\r\nwhile (cause_rx && budget > 0) {\r\nint count;\r\nstruct mvpp2_rx_queue *rxq;\r\nrxq = mvpp2_get_rx_queue(port, cause_rx);\r\nif (!rxq)\r\nbreak;\r\ncount = mvpp2_rx(port, budget, rxq);\r\nrx_done += count;\r\nbudget -= count;\r\nif (budget > 0) {\r\ncause_rx &= ~(1 << rxq->logic_rxq);\r\n}\r\n}\r\nif (budget > 0) {\r\ncause_rx = 0;\r\nnapi_complete(napi);\r\nmvpp2_interrupts_enable(port);\r\n}\r\nport->pending_cause_rx = cause_rx;\r\nreturn rx_done;\r\n}\r\nstatic void mvpp2_start_dev(struct mvpp2_port *port)\r\n{\r\nmvpp2_gmac_max_rx_size_set(port);\r\nmvpp2_txp_max_tx_size_set(port);\r\nnapi_enable(&port->napi);\r\nmvpp2_interrupts_enable(port);\r\nmvpp2_port_enable(port);\r\nphy_start(port->phy_dev);\r\nnetif_tx_start_all_queues(port->dev);\r\n}\r\nstatic void mvpp2_stop_dev(struct mvpp2_port *port)\r\n{\r\nmvpp2_ingress_disable(port);\r\nmdelay(10);\r\nmvpp2_interrupts_disable(port);\r\nnapi_disable(&port->napi);\r\nnetif_carrier_off(port->dev);\r\nnetif_tx_stop_all_queues(port->dev);\r\nmvpp2_egress_disable(port);\r\nmvpp2_port_disable(port);\r\nphy_stop(port->phy_dev);\r\n}\r\nstatic inline int mvpp2_check_mtu_valid(struct net_device *dev, int mtu)\r\n{\r\nif (mtu < 68) {\r\nnetdev_err(dev, "cannot change mtu to less than 68\n");\r\nreturn -EINVAL;\r\n}\r\nif (mtu > 9676) {\r\nnetdev_info(dev, "illegal MTU value %d, round to 9676\n", mtu);\r\nmtu = 9676;\r\n}\r\nif (!IS_ALIGNED(MVPP2_RX_PKT_SIZE(mtu), 8)) {\r\nnetdev_info(dev, "illegal MTU value %d, round to %d\n", mtu,\r\nALIGN(MVPP2_RX_PKT_SIZE(mtu), 8));\r\nmtu = ALIGN(MVPP2_RX_PKT_SIZE(mtu), 8);\r\n}\r\nreturn mtu;\r\n}\r\nstatic int mvpp2_check_ringparam_valid(struct net_device *dev,\r\nstruct ethtool_ringparam *ring)\r\n{\r\nu16 new_rx_pending = ring->rx_pending;\r\nu16 new_tx_pending = ring->tx_pending;\r\nif (ring->rx_pending == 0 || ring->tx_pending == 0)\r\nreturn -EINVAL;\r\nif (ring->rx_pending > MVPP2_MAX_RXD)\r\nnew_rx_pending = MVPP2_MAX_RXD;\r\nelse if (!IS_ALIGNED(ring->rx_pending, 16))\r\nnew_rx_pending = ALIGN(ring->rx_pending, 16);\r\nif (ring->tx_pending > MVPP2_MAX_TXD)\r\nnew_tx_pending = MVPP2_MAX_TXD;\r\nelse if (!IS_ALIGNED(ring->tx_pending, 32))\r\nnew_tx_pending = ALIGN(ring->tx_pending, 32);\r\nif (ring->rx_pending != new_rx_pending) {\r\nnetdev_info(dev, "illegal Rx ring size value %d, round to %d\n",\r\nring->rx_pending, new_rx_pending);\r\nring->rx_pending = new_rx_pending;\r\n}\r\nif (ring->tx_pending != new_tx_pending) {\r\nnetdev_info(dev, "illegal Tx ring size value %d, round to %d\n",\r\nring->tx_pending, new_tx_pending);\r\nring->tx_pending = new_tx_pending;\r\n}\r\nreturn 0;\r\n}\r\nstatic void mvpp2_get_mac_address(struct mvpp2_port *port, unsigned char *addr)\r\n{\r\nu32 mac_addr_l, mac_addr_m, mac_addr_h;\r\nmac_addr_l = readl(port->base + MVPP2_GMAC_CTRL_1_REG);\r\nmac_addr_m = readl(port->priv->lms_base + MVPP2_SRC_ADDR_MIDDLE);\r\nmac_addr_h = readl(port->priv->lms_base + MVPP2_SRC_ADDR_HIGH);\r\naddr[0] = (mac_addr_h >> 24) & 0xFF;\r\naddr[1] = (mac_addr_h >> 16) & 0xFF;\r\naddr[2] = (mac_addr_h >> 8) & 0xFF;\r\naddr[3] = mac_addr_h & 0xFF;\r\naddr[4] = mac_addr_m & 0xFF;\r\naddr[5] = (mac_addr_l >> MVPP2_GMAC_SA_LOW_OFFS) & 0xFF;\r\n}\r\nstatic int mvpp2_phy_connect(struct mvpp2_port *port)\r\n{\r\nstruct phy_device *phy_dev;\r\nphy_dev = of_phy_connect(port->dev, port->phy_node, mvpp2_link_event, 0,\r\nport->phy_interface);\r\nif (!phy_dev) {\r\nnetdev_err(port->dev, "cannot connect to phy\n");\r\nreturn -ENODEV;\r\n}\r\nphy_dev->supported &= PHY_GBIT_FEATURES;\r\nphy_dev->advertising = phy_dev->supported;\r\nport->phy_dev = phy_dev;\r\nport->link = 0;\r\nport->duplex = 0;\r\nport->speed = 0;\r\nreturn 0;\r\n}\r\nstatic void mvpp2_phy_disconnect(struct mvpp2_port *port)\r\n{\r\nphy_disconnect(port->phy_dev);\r\nport->phy_dev = NULL;\r\n}\r\nstatic int mvpp2_open(struct net_device *dev)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nunsigned char mac_bcast[ETH_ALEN] = {\r\n0xff, 0xff, 0xff, 0xff, 0xff, 0xff };\r\nint err;\r\nerr = mvpp2_prs_mac_da_accept(port->priv, port->id, mac_bcast, true);\r\nif (err) {\r\nnetdev_err(dev, "mvpp2_prs_mac_da_accept BC failed\n");\r\nreturn err;\r\n}\r\nerr = mvpp2_prs_mac_da_accept(port->priv, port->id,\r\ndev->dev_addr, true);\r\nif (err) {\r\nnetdev_err(dev, "mvpp2_prs_mac_da_accept MC failed\n");\r\nreturn err;\r\n}\r\nerr = mvpp2_prs_tag_mode_set(port->priv, port->id, MVPP2_TAG_TYPE_MH);\r\nif (err) {\r\nnetdev_err(dev, "mvpp2_prs_tag_mode_set failed\n");\r\nreturn err;\r\n}\r\nerr = mvpp2_prs_def_flow(port);\r\nif (err) {\r\nnetdev_err(dev, "mvpp2_prs_def_flow failed\n");\r\nreturn err;\r\n}\r\nerr = mvpp2_setup_rxqs(port);\r\nif (err) {\r\nnetdev_err(port->dev, "cannot allocate Rx queues\n");\r\nreturn err;\r\n}\r\nerr = mvpp2_setup_txqs(port);\r\nif (err) {\r\nnetdev_err(port->dev, "cannot allocate Tx queues\n");\r\ngoto err_cleanup_rxqs;\r\n}\r\nerr = request_irq(port->irq, mvpp2_isr, 0, dev->name, port);\r\nif (err) {\r\nnetdev_err(port->dev, "cannot request IRQ %d\n", port->irq);\r\ngoto err_cleanup_txqs;\r\n}\r\nnetif_carrier_off(port->dev);\r\nerr = mvpp2_phy_connect(port);\r\nif (err < 0)\r\ngoto err_free_irq;\r\non_each_cpu(mvpp2_interrupts_unmask, port, 1);\r\nmvpp2_start_dev(port);\r\nreturn 0;\r\nerr_free_irq:\r\nfree_irq(port->irq, port);\r\nerr_cleanup_txqs:\r\nmvpp2_cleanup_txqs(port);\r\nerr_cleanup_rxqs:\r\nmvpp2_cleanup_rxqs(port);\r\nreturn err;\r\n}\r\nstatic int mvpp2_stop(struct net_device *dev)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nstruct mvpp2_port_pcpu *port_pcpu;\r\nint cpu;\r\nmvpp2_stop_dev(port);\r\nmvpp2_phy_disconnect(port);\r\non_each_cpu(mvpp2_interrupts_mask, port, 1);\r\nfree_irq(port->irq, port);\r\nfor_each_present_cpu(cpu) {\r\nport_pcpu = per_cpu_ptr(port->pcpu, cpu);\r\nhrtimer_cancel(&port_pcpu->tx_done_timer);\r\nport_pcpu->timer_scheduled = false;\r\ntasklet_kill(&port_pcpu->tx_done_tasklet);\r\n}\r\nmvpp2_cleanup_rxqs(port);\r\nmvpp2_cleanup_txqs(port);\r\nreturn 0;\r\n}\r\nstatic void mvpp2_set_rx_mode(struct net_device *dev)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nstruct mvpp2 *priv = port->priv;\r\nstruct netdev_hw_addr *ha;\r\nint id = port->id;\r\nbool allmulti = dev->flags & IFF_ALLMULTI;\r\nmvpp2_prs_mac_promisc_set(priv, id, dev->flags & IFF_PROMISC);\r\nmvpp2_prs_mac_multi_set(priv, id, MVPP2_PE_MAC_MC_ALL, allmulti);\r\nmvpp2_prs_mac_multi_set(priv, id, MVPP2_PE_MAC_MC_IP6, allmulti);\r\nmvpp2_prs_mcast_del_all(priv, id);\r\nif (allmulti && !netdev_mc_empty(dev)) {\r\nnetdev_for_each_mc_addr(ha, dev)\r\nmvpp2_prs_mac_da_accept(priv, id, ha->addr, true);\r\n}\r\n}\r\nstatic int mvpp2_set_mac_address(struct net_device *dev, void *p)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nconst struct sockaddr *addr = p;\r\nint err;\r\nif (!is_valid_ether_addr(addr->sa_data)) {\r\nerr = -EADDRNOTAVAIL;\r\ngoto error;\r\n}\r\nif (!netif_running(dev)) {\r\nerr = mvpp2_prs_update_mac_da(dev, addr->sa_data);\r\nif (!err)\r\nreturn 0;\r\nerr = mvpp2_prs_update_mac_da(dev, dev->dev_addr);\r\nif (err)\r\ngoto error;\r\n}\r\nmvpp2_stop_dev(port);\r\nerr = mvpp2_prs_update_mac_da(dev, addr->sa_data);\r\nif (!err)\r\ngoto out_start;\r\nerr = mvpp2_prs_update_mac_da(dev, dev->dev_addr);\r\nif (err)\r\ngoto error;\r\nout_start:\r\nmvpp2_start_dev(port);\r\nmvpp2_egress_enable(port);\r\nmvpp2_ingress_enable(port);\r\nreturn 0;\r\nerror:\r\nnetdev_err(dev, "fail to change MAC address\n");\r\nreturn err;\r\n}\r\nstatic int mvpp2_change_mtu(struct net_device *dev, int mtu)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nint err;\r\nmtu = mvpp2_check_mtu_valid(dev, mtu);\r\nif (mtu < 0) {\r\nerr = mtu;\r\ngoto error;\r\n}\r\nif (!netif_running(dev)) {\r\nerr = mvpp2_bm_update_mtu(dev, mtu);\r\nif (!err) {\r\nport->pkt_size = MVPP2_RX_PKT_SIZE(mtu);\r\nreturn 0;\r\n}\r\nerr = mvpp2_bm_update_mtu(dev, dev->mtu);\r\nif (err)\r\ngoto error;\r\n}\r\nmvpp2_stop_dev(port);\r\nerr = mvpp2_bm_update_mtu(dev, mtu);\r\nif (!err) {\r\nport->pkt_size = MVPP2_RX_PKT_SIZE(mtu);\r\ngoto out_start;\r\n}\r\nerr = mvpp2_bm_update_mtu(dev, dev->mtu);\r\nif (err)\r\ngoto error;\r\nout_start:\r\nmvpp2_start_dev(port);\r\nmvpp2_egress_enable(port);\r\nmvpp2_ingress_enable(port);\r\nreturn 0;\r\nerror:\r\nnetdev_err(dev, "fail to change MTU\n");\r\nreturn err;\r\n}\r\nstatic struct rtnl_link_stats64 *\r\nmvpp2_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nunsigned int start;\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct mvpp2_pcpu_stats *cpu_stats;\r\nu64 rx_packets;\r\nu64 rx_bytes;\r\nu64 tx_packets;\r\nu64 tx_bytes;\r\ncpu_stats = per_cpu_ptr(port->stats, cpu);\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&cpu_stats->syncp);\r\nrx_packets = cpu_stats->rx_packets;\r\nrx_bytes = cpu_stats->rx_bytes;\r\ntx_packets = cpu_stats->tx_packets;\r\ntx_bytes = cpu_stats->tx_bytes;\r\n} while (u64_stats_fetch_retry_irq(&cpu_stats->syncp, start));\r\nstats->rx_packets += rx_packets;\r\nstats->rx_bytes += rx_bytes;\r\nstats->tx_packets += tx_packets;\r\nstats->tx_bytes += tx_bytes;\r\n}\r\nstats->rx_errors = dev->stats.rx_errors;\r\nstats->rx_dropped = dev->stats.rx_dropped;\r\nstats->tx_dropped = dev->stats.tx_dropped;\r\nreturn stats;\r\n}\r\nstatic int mvpp2_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nint ret;\r\nif (!port->phy_dev)\r\nreturn -ENOTSUPP;\r\nret = phy_mii_ioctl(port->phy_dev, ifr, cmd);\r\nif (!ret)\r\nmvpp2_link_event(dev);\r\nreturn ret;\r\n}\r\nstatic int mvpp2_ethtool_get_settings(struct net_device *dev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nif (!port->phy_dev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_gset(port->phy_dev, cmd);\r\n}\r\nstatic int mvpp2_ethtool_set_settings(struct net_device *dev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nif (!port->phy_dev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_sset(port->phy_dev, cmd);\r\n}\r\nstatic int mvpp2_ethtool_set_coalesce(struct net_device *dev,\r\nstruct ethtool_coalesce *c)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nint queue;\r\nfor (queue = 0; queue < rxq_number; queue++) {\r\nstruct mvpp2_rx_queue *rxq = port->rxqs[queue];\r\nrxq->time_coal = c->rx_coalesce_usecs;\r\nrxq->pkts_coal = c->rx_max_coalesced_frames;\r\nmvpp2_rx_pkts_coal_set(port, rxq, rxq->pkts_coal);\r\nmvpp2_rx_time_coal_set(port, rxq, rxq->time_coal);\r\n}\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nstruct mvpp2_tx_queue *txq = port->txqs[queue];\r\ntxq->done_pkts_coal = c->tx_max_coalesced_frames;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mvpp2_ethtool_get_coalesce(struct net_device *dev,\r\nstruct ethtool_coalesce *c)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nc->rx_coalesce_usecs = port->rxqs[0]->time_coal;\r\nc->rx_max_coalesced_frames = port->rxqs[0]->pkts_coal;\r\nc->tx_max_coalesced_frames = port->txqs[0]->done_pkts_coal;\r\nreturn 0;\r\n}\r\nstatic void mvpp2_ethtool_get_drvinfo(struct net_device *dev,\r\nstruct ethtool_drvinfo *drvinfo)\r\n{\r\nstrlcpy(drvinfo->driver, MVPP2_DRIVER_NAME,\r\nsizeof(drvinfo->driver));\r\nstrlcpy(drvinfo->version, MVPP2_DRIVER_VERSION,\r\nsizeof(drvinfo->version));\r\nstrlcpy(drvinfo->bus_info, dev_name(&dev->dev),\r\nsizeof(drvinfo->bus_info));\r\n}\r\nstatic void mvpp2_ethtool_get_ringparam(struct net_device *dev,\r\nstruct ethtool_ringparam *ring)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nring->rx_max_pending = MVPP2_MAX_RXD;\r\nring->tx_max_pending = MVPP2_MAX_TXD;\r\nring->rx_pending = port->rx_ring_size;\r\nring->tx_pending = port->tx_ring_size;\r\n}\r\nstatic int mvpp2_ethtool_set_ringparam(struct net_device *dev,\r\nstruct ethtool_ringparam *ring)\r\n{\r\nstruct mvpp2_port *port = netdev_priv(dev);\r\nu16 prev_rx_ring_size = port->rx_ring_size;\r\nu16 prev_tx_ring_size = port->tx_ring_size;\r\nint err;\r\nerr = mvpp2_check_ringparam_valid(dev, ring);\r\nif (err)\r\nreturn err;\r\nif (!netif_running(dev)) {\r\nport->rx_ring_size = ring->rx_pending;\r\nport->tx_ring_size = ring->tx_pending;\r\nreturn 0;\r\n}\r\nmvpp2_stop_dev(port);\r\nmvpp2_cleanup_rxqs(port);\r\nmvpp2_cleanup_txqs(port);\r\nport->rx_ring_size = ring->rx_pending;\r\nport->tx_ring_size = ring->tx_pending;\r\nerr = mvpp2_setup_rxqs(port);\r\nif (err) {\r\nport->rx_ring_size = prev_rx_ring_size;\r\nring->rx_pending = prev_rx_ring_size;\r\nerr = mvpp2_setup_rxqs(port);\r\nif (err)\r\ngoto err_out;\r\n}\r\nerr = mvpp2_setup_txqs(port);\r\nif (err) {\r\nport->tx_ring_size = prev_tx_ring_size;\r\nring->tx_pending = prev_tx_ring_size;\r\nerr = mvpp2_setup_txqs(port);\r\nif (err)\r\ngoto err_clean_rxqs;\r\n}\r\nmvpp2_start_dev(port);\r\nmvpp2_egress_enable(port);\r\nmvpp2_ingress_enable(port);\r\nreturn 0;\r\nerr_clean_rxqs:\r\nmvpp2_cleanup_rxqs(port);\r\nerr_out:\r\nnetdev_err(dev, "fail to change ring parameters");\r\nreturn err;\r\n}\r\nstatic void mvpp2_port_power_up(struct mvpp2_port *port)\r\n{\r\nmvpp2_port_mii_set(port);\r\nmvpp2_port_periodic_xon_disable(port);\r\nmvpp2_port_fc_adv_enable(port);\r\nmvpp2_port_reset(port);\r\n}\r\nstatic int mvpp2_port_init(struct mvpp2_port *port)\r\n{\r\nstruct device *dev = port->dev->dev.parent;\r\nstruct mvpp2 *priv = port->priv;\r\nstruct mvpp2_txq_pcpu *txq_pcpu;\r\nint queue, cpu, err;\r\nif (port->first_rxq + rxq_number > MVPP2_RXQ_TOTAL_NUM)\r\nreturn -EINVAL;\r\nmvpp2_egress_disable(port);\r\nmvpp2_port_disable(port);\r\nport->txqs = devm_kcalloc(dev, txq_number, sizeof(*port->txqs),\r\nGFP_KERNEL);\r\nif (!port->txqs)\r\nreturn -ENOMEM;\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nint queue_phy_id = mvpp2_txq_phys(port->id, queue);\r\nstruct mvpp2_tx_queue *txq;\r\ntxq = devm_kzalloc(dev, sizeof(*txq), GFP_KERNEL);\r\nif (!txq)\r\nreturn -ENOMEM;\r\ntxq->pcpu = alloc_percpu(struct mvpp2_txq_pcpu);\r\nif (!txq->pcpu) {\r\nerr = -ENOMEM;\r\ngoto err_free_percpu;\r\n}\r\ntxq->id = queue_phy_id;\r\ntxq->log_id = queue;\r\ntxq->done_pkts_coal = MVPP2_TXDONE_COAL_PKTS_THRESH;\r\nfor_each_present_cpu(cpu) {\r\ntxq_pcpu = per_cpu_ptr(txq->pcpu, cpu);\r\ntxq_pcpu->cpu = cpu;\r\n}\r\nport->txqs[queue] = txq;\r\n}\r\nport->rxqs = devm_kcalloc(dev, rxq_number, sizeof(*port->rxqs),\r\nGFP_KERNEL);\r\nif (!port->rxqs) {\r\nerr = -ENOMEM;\r\ngoto err_free_percpu;\r\n}\r\nfor (queue = 0; queue < rxq_number; queue++) {\r\nstruct mvpp2_rx_queue *rxq;\r\nrxq = devm_kzalloc(dev, sizeof(*rxq), GFP_KERNEL);\r\nif (!rxq)\r\ngoto err_free_percpu;\r\nrxq->id = port->first_rxq + queue;\r\nrxq->port = port->id;\r\nrxq->logic_rxq = queue;\r\nport->rxqs[queue] = rxq;\r\n}\r\nmvpp2_write(priv, MVPP2_ISR_RXQ_GROUP_REG(port->id), rxq_number);\r\nfor (queue = 0; queue < rxq_number; queue++) {\r\nstruct mvpp2_rx_queue *rxq = port->rxqs[queue];\r\nrxq->size = port->rx_ring_size;\r\nrxq->pkts_coal = MVPP2_RX_COAL_PKTS;\r\nrxq->time_coal = MVPP2_RX_COAL_USEC;\r\n}\r\nmvpp2_ingress_disable(port);\r\nmvpp2_defaults_set(port);\r\nmvpp2_cls_oversize_rxq_set(port);\r\nmvpp2_cls_port_config(port);\r\nport->pkt_size = MVPP2_RX_PKT_SIZE(port->dev->mtu);\r\nerr = mvpp2_swf_bm_pool_init(port);\r\nif (err)\r\ngoto err_free_percpu;\r\nreturn 0;\r\nerr_free_percpu:\r\nfor (queue = 0; queue < txq_number; queue++) {\r\nif (!port->txqs[queue])\r\ncontinue;\r\nfree_percpu(port->txqs[queue]->pcpu);\r\n}\r\nreturn err;\r\n}\r\nstatic int mvpp2_port_probe(struct platform_device *pdev,\r\nstruct device_node *port_node,\r\nstruct mvpp2 *priv,\r\nint *next_first_rxq)\r\n{\r\nstruct device_node *phy_node;\r\nstruct mvpp2_port *port;\r\nstruct mvpp2_port_pcpu *port_pcpu;\r\nstruct net_device *dev;\r\nstruct resource *res;\r\nconst char *dt_mac_addr;\r\nconst char *mac_from;\r\nchar hw_mac_addr[ETH_ALEN];\r\nu32 id;\r\nint features;\r\nint phy_mode;\r\nint priv_common_regs_num = 2;\r\nint err, i, cpu;\r\ndev = alloc_etherdev_mqs(sizeof(struct mvpp2_port), txq_number,\r\nrxq_number);\r\nif (!dev)\r\nreturn -ENOMEM;\r\nphy_node = of_parse_phandle(port_node, "phy", 0);\r\nif (!phy_node) {\r\ndev_err(&pdev->dev, "missing phy\n");\r\nerr = -ENODEV;\r\ngoto err_free_netdev;\r\n}\r\nphy_mode = of_get_phy_mode(port_node);\r\nif (phy_mode < 0) {\r\ndev_err(&pdev->dev, "incorrect phy mode\n");\r\nerr = phy_mode;\r\ngoto err_free_netdev;\r\n}\r\nif (of_property_read_u32(port_node, "port-id", &id)) {\r\nerr = -EINVAL;\r\ndev_err(&pdev->dev, "missing port-id value\n");\r\ngoto err_free_netdev;\r\n}\r\ndev->tx_queue_len = MVPP2_MAX_TXD;\r\ndev->watchdog_timeo = 5 * HZ;\r\ndev->netdev_ops = &mvpp2_netdev_ops;\r\ndev->ethtool_ops = &mvpp2_eth_tool_ops;\r\nport = netdev_priv(dev);\r\nport->irq = irq_of_parse_and_map(port_node, 0);\r\nif (port->irq <= 0) {\r\nerr = -EINVAL;\r\ngoto err_free_netdev;\r\n}\r\nif (of_property_read_bool(port_node, "marvell,loopback"))\r\nport->flags |= MVPP2_F_LOOPBACK;\r\nport->priv = priv;\r\nport->id = id;\r\nport->first_rxq = *next_first_rxq;\r\nport->phy_node = phy_node;\r\nport->phy_interface = phy_mode;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM,\r\npriv_common_regs_num + id);\r\nport->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(port->base)) {\r\nerr = PTR_ERR(port->base);\r\ngoto err_free_irq;\r\n}\r\nport->stats = netdev_alloc_pcpu_stats(struct mvpp2_pcpu_stats);\r\nif (!port->stats) {\r\nerr = -ENOMEM;\r\ngoto err_free_irq;\r\n}\r\ndt_mac_addr = of_get_mac_address(port_node);\r\nif (dt_mac_addr && is_valid_ether_addr(dt_mac_addr)) {\r\nmac_from = "device tree";\r\nether_addr_copy(dev->dev_addr, dt_mac_addr);\r\n} else {\r\nmvpp2_get_mac_address(port, hw_mac_addr);\r\nif (is_valid_ether_addr(hw_mac_addr)) {\r\nmac_from = "hardware";\r\nether_addr_copy(dev->dev_addr, hw_mac_addr);\r\n} else {\r\nmac_from = "random";\r\neth_hw_addr_random(dev);\r\n}\r\n}\r\nport->tx_ring_size = MVPP2_MAX_TXD;\r\nport->rx_ring_size = MVPP2_MAX_RXD;\r\nport->dev = dev;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nerr = mvpp2_port_init(port);\r\nif (err < 0) {\r\ndev_err(&pdev->dev, "failed to init port %d\n", id);\r\ngoto err_free_stats;\r\n}\r\nmvpp2_port_power_up(port);\r\nport->pcpu = alloc_percpu(struct mvpp2_port_pcpu);\r\nif (!port->pcpu) {\r\nerr = -ENOMEM;\r\ngoto err_free_txq_pcpu;\r\n}\r\nfor_each_present_cpu(cpu) {\r\nport_pcpu = per_cpu_ptr(port->pcpu, cpu);\r\nhrtimer_init(&port_pcpu->tx_done_timer, CLOCK_MONOTONIC,\r\nHRTIMER_MODE_REL_PINNED);\r\nport_pcpu->tx_done_timer.function = mvpp2_hr_timer_cb;\r\nport_pcpu->timer_scheduled = false;\r\ntasklet_init(&port_pcpu->tx_done_tasklet, mvpp2_tx_proc_cb,\r\n(unsigned long)dev);\r\n}\r\nnetif_napi_add(dev, &port->napi, mvpp2_poll, NAPI_POLL_WEIGHT);\r\nfeatures = NETIF_F_SG | NETIF_F_IP_CSUM;\r\ndev->features = features | NETIF_F_RXCSUM;\r\ndev->hw_features |= features | NETIF_F_RXCSUM | NETIF_F_GRO;\r\ndev->vlan_features |= features;\r\nerr = register_netdev(dev);\r\nif (err < 0) {\r\ndev_err(&pdev->dev, "failed to register netdev\n");\r\ngoto err_free_port_pcpu;\r\n}\r\nnetdev_info(dev, "Using %s mac address %pM\n", mac_from, dev->dev_addr);\r\n*next_first_rxq += rxq_number;\r\npriv->port_list[id] = port;\r\nreturn 0;\r\nerr_free_port_pcpu:\r\nfree_percpu(port->pcpu);\r\nerr_free_txq_pcpu:\r\nfor (i = 0; i < txq_number; i++)\r\nfree_percpu(port->txqs[i]->pcpu);\r\nerr_free_stats:\r\nfree_percpu(port->stats);\r\nerr_free_irq:\r\nirq_dispose_mapping(port->irq);\r\nerr_free_netdev:\r\nfree_netdev(dev);\r\nreturn err;\r\n}\r\nstatic void mvpp2_port_remove(struct mvpp2_port *port)\r\n{\r\nint i;\r\nunregister_netdev(port->dev);\r\nfree_percpu(port->pcpu);\r\nfree_percpu(port->stats);\r\nfor (i = 0; i < txq_number; i++)\r\nfree_percpu(port->txqs[i]->pcpu);\r\nirq_dispose_mapping(port->irq);\r\nfree_netdev(port->dev);\r\n}\r\nstatic void mvpp2_conf_mbus_windows(const struct mbus_dram_target_info *dram,\r\nstruct mvpp2 *priv)\r\n{\r\nu32 win_enable;\r\nint i;\r\nfor (i = 0; i < 6; i++) {\r\nmvpp2_write(priv, MVPP2_WIN_BASE(i), 0);\r\nmvpp2_write(priv, MVPP2_WIN_SIZE(i), 0);\r\nif (i < 4)\r\nmvpp2_write(priv, MVPP2_WIN_REMAP(i), 0);\r\n}\r\nwin_enable = 0;\r\nfor (i = 0; i < dram->num_cs; i++) {\r\nconst struct mbus_dram_window *cs = dram->cs + i;\r\nmvpp2_write(priv, MVPP2_WIN_BASE(i),\r\n(cs->base & 0xffff0000) | (cs->mbus_attr << 8) |\r\ndram->mbus_dram_target_id);\r\nmvpp2_write(priv, MVPP2_WIN_SIZE(i),\r\n(cs->size - 1) & 0xffff0000);\r\nwin_enable |= (1 << i);\r\n}\r\nmvpp2_write(priv, MVPP2_BASE_ADDR_ENABLE, win_enable);\r\n}\r\nstatic void mvpp2_rx_fifo_init(struct mvpp2 *priv)\r\n{\r\nint port;\r\nfor (port = 0; port < MVPP2_MAX_PORTS; port++) {\r\nmvpp2_write(priv, MVPP2_RX_DATA_FIFO_SIZE_REG(port),\r\nMVPP2_RX_FIFO_PORT_DATA_SIZE);\r\nmvpp2_write(priv, MVPP2_RX_ATTR_FIFO_SIZE_REG(port),\r\nMVPP2_RX_FIFO_PORT_ATTR_SIZE);\r\n}\r\nmvpp2_write(priv, MVPP2_RX_MIN_PKT_SIZE_REG,\r\nMVPP2_RX_FIFO_PORT_MIN_PKT);\r\nmvpp2_write(priv, MVPP2_RX_FIFO_INIT_REG, 0x1);\r\n}\r\nstatic int mvpp2_init(struct platform_device *pdev, struct mvpp2 *priv)\r\n{\r\nconst struct mbus_dram_target_info *dram_target_info;\r\nint err, i;\r\nu32 val;\r\nif (rxq_number % 4 || (rxq_number > MVPP2_MAX_RXQ) ||\r\n(txq_number > MVPP2_MAX_TXQ)) {\r\ndev_err(&pdev->dev, "invalid queue size parameter\n");\r\nreturn -EINVAL;\r\n}\r\ndram_target_info = mv_mbus_dram_info();\r\nif (dram_target_info)\r\nmvpp2_conf_mbus_windows(dram_target_info, priv);\r\nval = readl(priv->lms_base + MVPP2_PHY_AN_CFG0_REG);\r\nval |= MVPP2_PHY_AN_STOP_SMI0_MASK;\r\nwritel(val, priv->lms_base + MVPP2_PHY_AN_CFG0_REG);\r\npriv->aggr_txqs = devm_kcalloc(&pdev->dev, num_present_cpus(),\r\nsizeof(struct mvpp2_tx_queue),\r\nGFP_KERNEL);\r\nif (!priv->aggr_txqs)\r\nreturn -ENOMEM;\r\nfor_each_present_cpu(i) {\r\npriv->aggr_txqs[i].id = i;\r\npriv->aggr_txqs[i].size = MVPP2_AGGR_TXQ_SIZE;\r\nerr = mvpp2_aggr_txq_init(pdev, &priv->aggr_txqs[i],\r\nMVPP2_AGGR_TXQ_SIZE, i, priv);\r\nif (err < 0)\r\nreturn err;\r\n}\r\nmvpp2_rx_fifo_init(priv);\r\nfor (i = 0; i < MVPP2_MAX_PORTS; i++)\r\nmvpp2_write(priv, MVPP2_ISR_RXQ_GROUP_REG(i), rxq_number);\r\nwritel(MVPP2_EXT_GLOBAL_CTRL_DEFAULT,\r\npriv->lms_base + MVPP2_MNG_EXTENDED_GLOBAL_CTRL_REG);\r\nmvpp2_write(priv, MVPP2_TX_SNOOP_REG, 0x1);\r\nerr = mvpp2_bm_init(pdev, priv);\r\nif (err < 0)\r\nreturn err;\r\nerr = mvpp2_prs_default_init(pdev, priv);\r\nif (err < 0)\r\nreturn err;\r\nmvpp2_cls_init(priv);\r\nreturn 0;\r\n}\r\nstatic int mvpp2_probe(struct platform_device *pdev)\r\n{\r\nstruct device_node *dn = pdev->dev.of_node;\r\nstruct device_node *port_node;\r\nstruct mvpp2 *priv;\r\nstruct resource *res;\r\nint port_count, first_rxq;\r\nint err;\r\npriv = devm_kzalloc(&pdev->dev, sizeof(struct mvpp2), GFP_KERNEL);\r\nif (!priv)\r\nreturn -ENOMEM;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\npriv->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(priv->base))\r\nreturn PTR_ERR(priv->base);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\npriv->lms_base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(priv->lms_base))\r\nreturn PTR_ERR(priv->lms_base);\r\npriv->pp_clk = devm_clk_get(&pdev->dev, "pp_clk");\r\nif (IS_ERR(priv->pp_clk))\r\nreturn PTR_ERR(priv->pp_clk);\r\nerr = clk_prepare_enable(priv->pp_clk);\r\nif (err < 0)\r\nreturn err;\r\npriv->gop_clk = devm_clk_get(&pdev->dev, "gop_clk");\r\nif (IS_ERR(priv->gop_clk)) {\r\nerr = PTR_ERR(priv->gop_clk);\r\ngoto err_pp_clk;\r\n}\r\nerr = clk_prepare_enable(priv->gop_clk);\r\nif (err < 0)\r\ngoto err_pp_clk;\r\npriv->tclk = clk_get_rate(priv->pp_clk);\r\nerr = mvpp2_init(pdev, priv);\r\nif (err < 0) {\r\ndev_err(&pdev->dev, "failed to initialize controller\n");\r\ngoto err_gop_clk;\r\n}\r\nport_count = of_get_available_child_count(dn);\r\nif (port_count == 0) {\r\ndev_err(&pdev->dev, "no ports enabled\n");\r\nerr = -ENODEV;\r\ngoto err_gop_clk;\r\n}\r\npriv->port_list = devm_kcalloc(&pdev->dev, port_count,\r\nsizeof(struct mvpp2_port *),\r\nGFP_KERNEL);\r\nif (!priv->port_list) {\r\nerr = -ENOMEM;\r\ngoto err_gop_clk;\r\n}\r\nfirst_rxq = 0;\r\nfor_each_available_child_of_node(dn, port_node) {\r\nerr = mvpp2_port_probe(pdev, port_node, priv, &first_rxq);\r\nif (err < 0)\r\ngoto err_gop_clk;\r\n}\r\nplatform_set_drvdata(pdev, priv);\r\nreturn 0;\r\nerr_gop_clk:\r\nclk_disable_unprepare(priv->gop_clk);\r\nerr_pp_clk:\r\nclk_disable_unprepare(priv->pp_clk);\r\nreturn err;\r\n}\r\nstatic int mvpp2_remove(struct platform_device *pdev)\r\n{\r\nstruct mvpp2 *priv = platform_get_drvdata(pdev);\r\nstruct device_node *dn = pdev->dev.of_node;\r\nstruct device_node *port_node;\r\nint i = 0;\r\nfor_each_available_child_of_node(dn, port_node) {\r\nif (priv->port_list[i])\r\nmvpp2_port_remove(priv->port_list[i]);\r\ni++;\r\n}\r\nfor (i = 0; i < MVPP2_BM_POOLS_NUM; i++) {\r\nstruct mvpp2_bm_pool *bm_pool = &priv->bm_pools[i];\r\nmvpp2_bm_pool_destroy(pdev, priv, bm_pool);\r\n}\r\nfor_each_present_cpu(i) {\r\nstruct mvpp2_tx_queue *aggr_txq = &priv->aggr_txqs[i];\r\ndma_free_coherent(&pdev->dev,\r\nMVPP2_AGGR_TXQ_SIZE * MVPP2_DESC_ALIGNED_SIZE,\r\naggr_txq->descs,\r\naggr_txq->descs_phys);\r\n}\r\nclk_disable_unprepare(priv->pp_clk);\r\nclk_disable_unprepare(priv->gop_clk);\r\nreturn 0;\r\n}
