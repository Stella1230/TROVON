static void\r\namd_cpuid4(int leaf, union _cpuid4_leaf_eax *eax,\r\nunion _cpuid4_leaf_ebx *ebx,\r\nunion _cpuid4_leaf_ecx *ecx)\r\n{\r\nunsigned dummy;\r\nunsigned line_size, lines_per_tag, assoc, size_in_kb;\r\nunion l1_cache l1i, l1d;\r\nunion l2_cache l2;\r\nunion l3_cache l3;\r\nunion l1_cache *l1 = &l1d;\r\neax->full = 0;\r\nebx->full = 0;\r\necx->full = 0;\r\ncpuid(0x80000005, &dummy, &dummy, &l1d.val, &l1i.val);\r\ncpuid(0x80000006, &dummy, &dummy, &l2.val, &l3.val);\r\nswitch (leaf) {\r\ncase 1:\r\nl1 = &l1i;\r\ncase 0:\r\nif (!l1->val)\r\nreturn;\r\nassoc = assocs[l1->assoc];\r\nline_size = l1->line_size;\r\nlines_per_tag = l1->lines_per_tag;\r\nsize_in_kb = l1->size_in_kb;\r\nbreak;\r\ncase 2:\r\nif (!l2.val)\r\nreturn;\r\nassoc = assocs[l2.assoc];\r\nline_size = l2.line_size;\r\nlines_per_tag = l2.lines_per_tag;\r\nsize_in_kb = __this_cpu_read(cpu_info.x86_cache_size);\r\nbreak;\r\ncase 3:\r\nif (!l3.val)\r\nreturn;\r\nassoc = assocs[l3.assoc];\r\nline_size = l3.line_size;\r\nlines_per_tag = l3.lines_per_tag;\r\nsize_in_kb = l3.size_encoded * 512;\r\nif (boot_cpu_has(X86_FEATURE_AMD_DCM)) {\r\nsize_in_kb = size_in_kb >> 1;\r\nassoc = assoc >> 1;\r\n}\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\neax->split.is_self_initializing = 1;\r\neax->split.type = types[leaf];\r\neax->split.level = levels[leaf];\r\neax->split.num_threads_sharing = 0;\r\neax->split.num_cores_on_die = __this_cpu_read(cpu_info.x86_max_cores) - 1;\r\nif (assoc == 0xffff)\r\neax->split.is_fully_associative = 1;\r\nebx->split.coherency_line_size = line_size - 1;\r\nebx->split.ways_of_associativity = assoc - 1;\r\nebx->split.physical_line_partition = lines_per_tag - 1;\r\necx->split.number_of_sets = (size_in_kb * 1024) / line_size /\r\n(ebx->split.ways_of_associativity + 1) - 1;\r\n}\r\nstatic void amd_calc_l3_indices(struct amd_northbridge *nb)\r\n{\r\nstruct amd_l3_cache *l3 = &nb->l3_cache;\r\nunsigned int sc0, sc1, sc2, sc3;\r\nu32 val = 0;\r\npci_read_config_dword(nb->misc, 0x1C4, &val);\r\nl3->subcaches[0] = sc0 = !(val & BIT(0));\r\nl3->subcaches[1] = sc1 = !(val & BIT(4));\r\nif (boot_cpu_data.x86 == 0x15) {\r\nl3->subcaches[0] = sc0 += !(val & BIT(1));\r\nl3->subcaches[1] = sc1 += !(val & BIT(5));\r\n}\r\nl3->subcaches[2] = sc2 = !(val & BIT(8)) + !(val & BIT(9));\r\nl3->subcaches[3] = sc3 = !(val & BIT(12)) + !(val & BIT(13));\r\nl3->indices = (max(max3(sc0, sc1, sc2), sc3) << 10) - 1;\r\n}\r\nint amd_get_l3_disable_slot(struct amd_northbridge *nb, unsigned slot)\r\n{\r\nunsigned int reg = 0;\r\npci_read_config_dword(nb->misc, 0x1BC + slot * 4, &reg);\r\nif (reg & (3UL << 30))\r\nreturn reg & 0xfff;\r\nreturn -1;\r\n}\r\nstatic ssize_t show_cache_disable(struct cacheinfo *this_leaf, char *buf,\r\nunsigned int slot)\r\n{\r\nint index;\r\nstruct amd_northbridge *nb = this_leaf->priv;\r\nindex = amd_get_l3_disable_slot(nb, slot);\r\nif (index >= 0)\r\nreturn sprintf(buf, "%d\n", index);\r\nreturn sprintf(buf, "FREE\n");\r\n}\r\nstatic void amd_l3_disable_index(struct amd_northbridge *nb, int cpu,\r\nunsigned slot, unsigned long idx)\r\n{\r\nint i;\r\nidx |= BIT(30);\r\nfor (i = 0; i < 4; i++) {\r\nu32 reg = idx | (i << 20);\r\nif (!nb->l3_cache.subcaches[i])\r\ncontinue;\r\npci_write_config_dword(nb->misc, 0x1BC + slot * 4, reg);\r\nwbinvd_on_cpu(cpu);\r\nreg |= BIT(31);\r\npci_write_config_dword(nb->misc, 0x1BC + slot * 4, reg);\r\n}\r\n}\r\nint amd_set_l3_disable_slot(struct amd_northbridge *nb, int cpu, unsigned slot,\r\nunsigned long index)\r\n{\r\nint ret = 0;\r\nret = amd_get_l3_disable_slot(nb, slot);\r\nif (ret >= 0)\r\nreturn -EEXIST;\r\nif (index > nb->l3_cache.indices)\r\nreturn -EINVAL;\r\nif (index == amd_get_l3_disable_slot(nb, !slot))\r\nreturn -EEXIST;\r\namd_l3_disable_index(nb, cpu, slot, index);\r\nreturn 0;\r\n}\r\nstatic ssize_t store_cache_disable(struct cacheinfo *this_leaf,\r\nconst char *buf, size_t count,\r\nunsigned int slot)\r\n{\r\nunsigned long val = 0;\r\nint cpu, err = 0;\r\nstruct amd_northbridge *nb = this_leaf->priv;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\ncpu = cpumask_first(&this_leaf->shared_cpu_map);\r\nif (kstrtoul(buf, 10, &val) < 0)\r\nreturn -EINVAL;\r\nerr = amd_set_l3_disable_slot(nb, cpu, slot, val);\r\nif (err) {\r\nif (err == -EEXIST)\r\npr_warning("L3 slot %d in use/index already disabled!\n",\r\nslot);\r\nreturn err;\r\n}\r\nreturn count;\r\n}\r\nstatic ssize_t subcaches_show(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\r\nint cpu = cpumask_first(&this_leaf->shared_cpu_map);\r\nreturn sprintf(buf, "%x\n", amd_get_subcaches(cpu));\r\n}\r\nstatic ssize_t subcaches_store(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\r\nint cpu = cpumask_first(&this_leaf->shared_cpu_map);\r\nunsigned long val;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nif (kstrtoul(buf, 16, &val) < 0)\r\nreturn -EINVAL;\r\nif (amd_set_subcaches(cpu, val))\r\nreturn -EINVAL;\r\nreturn count;\r\n}\r\nstatic umode_t\r\ncache_private_attrs_is_visible(struct kobject *kobj,\r\nstruct attribute *attr, int unused)\r\n{\r\nstruct device *dev = kobj_to_dev(kobj);\r\nstruct cacheinfo *this_leaf = dev_get_drvdata(dev);\r\numode_t mode = attr->mode;\r\nif (!this_leaf->priv)\r\nreturn 0;\r\nif ((attr == &dev_attr_subcaches.attr) &&\r\namd_nb_has_feature(AMD_NB_L3_PARTITIONING))\r\nreturn mode;\r\nif ((attr == &dev_attr_cache_disable_0.attr ||\r\nattr == &dev_attr_cache_disable_1.attr) &&\r\namd_nb_has_feature(AMD_NB_L3_INDEX_DISABLE))\r\nreturn mode;\r\nreturn 0;\r\n}\r\nstatic void init_amd_l3_attrs(void)\r\n{\r\nint n = 1;\r\nstatic struct attribute **amd_l3_attrs;\r\nif (amd_l3_attrs)\r\nreturn;\r\nif (amd_nb_has_feature(AMD_NB_L3_INDEX_DISABLE))\r\nn += 2;\r\nif (amd_nb_has_feature(AMD_NB_L3_PARTITIONING))\r\nn += 1;\r\namd_l3_attrs = kcalloc(n, sizeof(*amd_l3_attrs), GFP_KERNEL);\r\nif (!amd_l3_attrs)\r\nreturn;\r\nn = 0;\r\nif (amd_nb_has_feature(AMD_NB_L3_INDEX_DISABLE)) {\r\namd_l3_attrs[n++] = &dev_attr_cache_disable_0.attr;\r\namd_l3_attrs[n++] = &dev_attr_cache_disable_1.attr;\r\n}\r\nif (amd_nb_has_feature(AMD_NB_L3_PARTITIONING))\r\namd_l3_attrs[n++] = &dev_attr_subcaches.attr;\r\ncache_private_group.attrs = amd_l3_attrs;\r\n}\r\nconst struct attribute_group *\r\ncache_get_priv_group(struct cacheinfo *this_leaf)\r\n{\r\nstruct amd_northbridge *nb = this_leaf->priv;\r\nif (this_leaf->level < 3 || !nb)\r\nreturn NULL;\r\nif (nb && nb->l3_cache.indices)\r\ninit_amd_l3_attrs();\r\nreturn &cache_private_group;\r\n}\r\nstatic void amd_init_l3_cache(struct _cpuid4_info_regs *this_leaf, int index)\r\n{\r\nint node;\r\nif (index < 3)\r\nreturn;\r\nnode = amd_get_nb_id(smp_processor_id());\r\nthis_leaf->nb = node_to_amd_nb(node);\r\nif (this_leaf->nb && !this_leaf->nb->l3_cache.indices)\r\namd_calc_l3_indices(this_leaf->nb);\r\n}\r\nstatic int\r\ncpuid4_cache_lookup_regs(int index, struct _cpuid4_info_regs *this_leaf)\r\n{\r\nunion _cpuid4_leaf_eax eax;\r\nunion _cpuid4_leaf_ebx ebx;\r\nunion _cpuid4_leaf_ecx ecx;\r\nunsigned edx;\r\nif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {\r\nif (cpu_has_topoext)\r\ncpuid_count(0x8000001d, index, &eax.full,\r\n&ebx.full, &ecx.full, &edx);\r\nelse\r\namd_cpuid4(index, &eax, &ebx, &ecx);\r\namd_init_l3_cache(this_leaf, index);\r\n} else {\r\ncpuid_count(4, index, &eax.full, &ebx.full, &ecx.full, &edx);\r\n}\r\nif (eax.split.type == CTYPE_NULL)\r\nreturn -EIO;\r\nthis_leaf->eax = eax;\r\nthis_leaf->ebx = ebx;\r\nthis_leaf->ecx = ecx;\r\nthis_leaf->size = (ecx.split.number_of_sets + 1) *\r\n(ebx.split.coherency_line_size + 1) *\r\n(ebx.split.physical_line_partition + 1) *\r\n(ebx.split.ways_of_associativity + 1);\r\nreturn 0;\r\n}\r\nstatic int find_num_cache_leaves(struct cpuinfo_x86 *c)\r\n{\r\nunsigned int eax, ebx, ecx, edx, op;\r\nunion _cpuid4_leaf_eax cache_eax;\r\nint i = -1;\r\nif (c->x86_vendor == X86_VENDOR_AMD)\r\nop = 0x8000001d;\r\nelse\r\nop = 4;\r\ndo {\r\n++i;\r\ncpuid_count(op, i, &eax, &ebx, &ecx, &edx);\r\ncache_eax.full = eax;\r\n} while (cache_eax.split.type != CTYPE_NULL);\r\nreturn i;\r\n}\r\nvoid init_amd_cacheinfo(struct cpuinfo_x86 *c)\r\n{\r\nif (cpu_has_topoext) {\r\nnum_cache_leaves = find_num_cache_leaves(c);\r\n} else if (c->extended_cpuid_level >= 0x80000006) {\r\nif (cpuid_edx(0x80000006) & 0xf000)\r\nnum_cache_leaves = 4;\r\nelse\r\nnum_cache_leaves = 3;\r\n}\r\n}\r\nunsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c)\r\n{\r\nunsigned int trace = 0, l1i = 0, l1d = 0, l2 = 0, l3 = 0;\r\nunsigned int new_l1d = 0, new_l1i = 0;\r\nunsigned int new_l2 = 0, new_l3 = 0, i;\r\nunsigned int l2_id = 0, l3_id = 0, num_threads_sharing, index_msb;\r\n#ifdef CONFIG_SMP\r\nunsigned int cpu = c->cpu_index;\r\n#endif\r\nif (c->cpuid_level > 3) {\r\nstatic int is_initialized;\r\nif (is_initialized == 0) {\r\nnum_cache_leaves = find_num_cache_leaves(c);\r\nis_initialized++;\r\n}\r\nfor (i = 0; i < num_cache_leaves; i++) {\r\nstruct _cpuid4_info_regs this_leaf = {};\r\nint retval;\r\nretval = cpuid4_cache_lookup_regs(i, &this_leaf);\r\nif (retval < 0)\r\ncontinue;\r\nswitch (this_leaf.eax.split.level) {\r\ncase 1:\r\nif (this_leaf.eax.split.type == CTYPE_DATA)\r\nnew_l1d = this_leaf.size/1024;\r\nelse if (this_leaf.eax.split.type == CTYPE_INST)\r\nnew_l1i = this_leaf.size/1024;\r\nbreak;\r\ncase 2:\r\nnew_l2 = this_leaf.size/1024;\r\nnum_threads_sharing = 1 + this_leaf.eax.split.num_threads_sharing;\r\nindex_msb = get_count_order(num_threads_sharing);\r\nl2_id = c->apicid & ~((1 << index_msb) - 1);\r\nbreak;\r\ncase 3:\r\nnew_l3 = this_leaf.size/1024;\r\nnum_threads_sharing = 1 + this_leaf.eax.split.num_threads_sharing;\r\nindex_msb = get_count_order(num_threads_sharing);\r\nl3_id = c->apicid & ~((1 << index_msb) - 1);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\n}\r\nif ((num_cache_leaves == 0 || c->x86 == 15) && c->cpuid_level > 1) {\r\nint j, n;\r\nunsigned int regs[4];\r\nunsigned char *dp = (unsigned char *)regs;\r\nint only_trace = 0;\r\nif (num_cache_leaves != 0 && c->x86 == 15)\r\nonly_trace = 1;\r\nn = cpuid_eax(2) & 0xFF;\r\nfor (i = 0 ; i < n ; i++) {\r\ncpuid(2, &regs[0], &regs[1], &regs[2], &regs[3]);\r\nfor (j = 0 ; j < 3 ; j++)\r\nif (regs[j] & (1 << 31))\r\nregs[j] = 0;\r\nfor (j = 1 ; j < 16 ; j++) {\r\nunsigned char des = dp[j];\r\nunsigned char k = 0;\r\nwhile (cache_table[k].descriptor != 0) {\r\nif (cache_table[k].descriptor == des) {\r\nif (only_trace && cache_table[k].cache_type != LVL_TRACE)\r\nbreak;\r\nswitch (cache_table[k].cache_type) {\r\ncase LVL_1_INST:\r\nl1i += cache_table[k].size;\r\nbreak;\r\ncase LVL_1_DATA:\r\nl1d += cache_table[k].size;\r\nbreak;\r\ncase LVL_2:\r\nl2 += cache_table[k].size;\r\nbreak;\r\ncase LVL_3:\r\nl3 += cache_table[k].size;\r\nbreak;\r\ncase LVL_TRACE:\r\ntrace += cache_table[k].size;\r\nbreak;\r\n}\r\nbreak;\r\n}\r\nk++;\r\n}\r\n}\r\n}\r\n}\r\nif (new_l1d)\r\nl1d = new_l1d;\r\nif (new_l1i)\r\nl1i = new_l1i;\r\nif (new_l2) {\r\nl2 = new_l2;\r\n#ifdef CONFIG_SMP\r\nper_cpu(cpu_llc_id, cpu) = l2_id;\r\n#endif\r\n}\r\nif (new_l3) {\r\nl3 = new_l3;\r\n#ifdef CONFIG_SMP\r\nper_cpu(cpu_llc_id, cpu) = l3_id;\r\n#endif\r\n}\r\n#ifdef CONFIG_SMP\r\nif (per_cpu(cpu_llc_id, cpu) == BAD_APICID)\r\nper_cpu(cpu_llc_id, cpu) = c->phys_proc_id;\r\n#endif\r\nc->x86_cache_size = l3 ? l3 : (l2 ? l2 : (l1i+l1d));\r\nreturn l2;\r\n}\r\nstatic int __cache_amd_cpumap_setup(unsigned int cpu, int index,\r\nstruct _cpuid4_info_regs *base)\r\n{\r\nstruct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);\r\nstruct cacheinfo *this_leaf;\r\nint i, sibling;\r\nif (cpu_has_topoext) {\r\nunsigned int apicid, nshared, first, last;\r\nthis_leaf = this_cpu_ci->info_list + index;\r\nnshared = base->eax.split.num_threads_sharing + 1;\r\napicid = cpu_data(cpu).apicid;\r\nfirst = apicid - (apicid % nshared);\r\nlast = first + nshared - 1;\r\nfor_each_online_cpu(i) {\r\nthis_cpu_ci = get_cpu_cacheinfo(i);\r\nif (!this_cpu_ci->info_list)\r\ncontinue;\r\napicid = cpu_data(i).apicid;\r\nif ((apicid < first) || (apicid > last))\r\ncontinue;\r\nthis_leaf = this_cpu_ci->info_list + index;\r\nfor_each_online_cpu(sibling) {\r\napicid = cpu_data(sibling).apicid;\r\nif ((apicid < first) || (apicid > last))\r\ncontinue;\r\ncpumask_set_cpu(sibling,\r\n&this_leaf->shared_cpu_map);\r\n}\r\n}\r\n} else if (index == 3) {\r\nfor_each_cpu(i, cpu_llc_shared_mask(cpu)) {\r\nthis_cpu_ci = get_cpu_cacheinfo(i);\r\nif (!this_cpu_ci->info_list)\r\ncontinue;\r\nthis_leaf = this_cpu_ci->info_list + index;\r\nfor_each_cpu(sibling, cpu_llc_shared_mask(cpu)) {\r\nif (!cpu_online(sibling))\r\ncontinue;\r\ncpumask_set_cpu(sibling,\r\n&this_leaf->shared_cpu_map);\r\n}\r\n}\r\n} else\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void __cache_cpumap_setup(unsigned int cpu, int index,\r\nstruct _cpuid4_info_regs *base)\r\n{\r\nstruct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);\r\nstruct cacheinfo *this_leaf, *sibling_leaf;\r\nunsigned long num_threads_sharing;\r\nint index_msb, i;\r\nstruct cpuinfo_x86 *c = &cpu_data(cpu);\r\nif (c->x86_vendor == X86_VENDOR_AMD) {\r\nif (__cache_amd_cpumap_setup(cpu, index, base))\r\nreturn;\r\n}\r\nthis_leaf = this_cpu_ci->info_list + index;\r\nnum_threads_sharing = 1 + base->eax.split.num_threads_sharing;\r\ncpumask_set_cpu(cpu, &this_leaf->shared_cpu_map);\r\nif (num_threads_sharing == 1)\r\nreturn;\r\nindex_msb = get_count_order(num_threads_sharing);\r\nfor_each_online_cpu(i)\r\nif (cpu_data(i).apicid >> index_msb == c->apicid >> index_msb) {\r\nstruct cpu_cacheinfo *sib_cpu_ci = get_cpu_cacheinfo(i);\r\nif (i == cpu || !sib_cpu_ci->info_list)\r\ncontinue;\r\nsibling_leaf = sib_cpu_ci->info_list + index;\r\ncpumask_set_cpu(i, &this_leaf->shared_cpu_map);\r\ncpumask_set_cpu(cpu, &sibling_leaf->shared_cpu_map);\r\n}\r\n}\r\nstatic void ci_leaf_init(struct cacheinfo *this_leaf,\r\nstruct _cpuid4_info_regs *base)\r\n{\r\nthis_leaf->level = base->eax.split.level;\r\nthis_leaf->type = cache_type_map[base->eax.split.type];\r\nthis_leaf->coherency_line_size =\r\nbase->ebx.split.coherency_line_size + 1;\r\nthis_leaf->ways_of_associativity =\r\nbase->ebx.split.ways_of_associativity + 1;\r\nthis_leaf->size = base->size;\r\nthis_leaf->number_of_sets = base->ecx.split.number_of_sets + 1;\r\nthis_leaf->physical_line_partition =\r\nbase->ebx.split.physical_line_partition + 1;\r\nthis_leaf->priv = base->nb;\r\n}\r\nstatic int __init_cache_level(unsigned int cpu)\r\n{\r\nstruct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);\r\nif (!num_cache_leaves)\r\nreturn -ENOENT;\r\nif (!this_cpu_ci)\r\nreturn -EINVAL;\r\nthis_cpu_ci->num_levels = 3;\r\nthis_cpu_ci->num_leaves = num_cache_leaves;\r\nreturn 0;\r\n}\r\nstatic int __populate_cache_leaves(unsigned int cpu)\r\n{\r\nunsigned int idx, ret;\r\nstruct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);\r\nstruct cacheinfo *this_leaf = this_cpu_ci->info_list;\r\nstruct _cpuid4_info_regs id4_regs = {};\r\nfor (idx = 0; idx < this_cpu_ci->num_leaves; idx++) {\r\nret = cpuid4_cache_lookup_regs(idx, &id4_regs);\r\nif (ret)\r\nreturn ret;\r\nci_leaf_init(this_leaf++, &id4_regs);\r\n__cache_cpumap_setup(cpu, idx, &id4_regs);\r\n}\r\nreturn 0;\r\n}
