static int to_c2_state(enum ib_qp_state ib_state)\r\n{\r\nswitch (ib_state) {\r\ncase IB_QPS_RESET:\r\nreturn C2_QP_STATE_IDLE;\r\ncase IB_QPS_RTS:\r\nreturn C2_QP_STATE_RTS;\r\ncase IB_QPS_SQD:\r\nreturn C2_QP_STATE_CLOSING;\r\ncase IB_QPS_SQE:\r\nreturn C2_QP_STATE_CLOSING;\r\ncase IB_QPS_ERR:\r\nreturn C2_QP_STATE_ERROR;\r\ndefault:\r\nreturn -1;\r\n}\r\n}\r\nstatic int to_ib_state(enum c2_qp_state c2_state)\r\n{\r\nswitch (c2_state) {\r\ncase C2_QP_STATE_IDLE:\r\nreturn IB_QPS_RESET;\r\ncase C2_QP_STATE_CONNECTING:\r\nreturn IB_QPS_RTR;\r\ncase C2_QP_STATE_RTS:\r\nreturn IB_QPS_RTS;\r\ncase C2_QP_STATE_CLOSING:\r\nreturn IB_QPS_SQD;\r\ncase C2_QP_STATE_ERROR:\r\nreturn IB_QPS_ERR;\r\ncase C2_QP_STATE_TERMINATE:\r\nreturn IB_QPS_SQE;\r\ndefault:\r\nreturn -1;\r\n}\r\n}\r\nstatic const char *to_ib_state_str(int ib_state)\r\n{\r\nstatic const char *state_str[] = {\r\n"IB_QPS_RESET",\r\n"IB_QPS_INIT",\r\n"IB_QPS_RTR",\r\n"IB_QPS_RTS",\r\n"IB_QPS_SQD",\r\n"IB_QPS_SQE",\r\n"IB_QPS_ERR"\r\n};\r\nif (ib_state < IB_QPS_RESET ||\r\nib_state > IB_QPS_ERR)\r\nreturn "<invalid IB QP state>";\r\nib_state -= IB_QPS_RESET;\r\nreturn state_str[ib_state];\r\n}\r\nvoid c2_set_qp_state(struct c2_qp *qp, int c2_state)\r\n{\r\nint new_state = to_ib_state(c2_state);\r\npr_debug("%s: qp[%p] state modify %s --> %s\n",\r\n__func__,\r\nqp,\r\nto_ib_state_str(qp->state),\r\nto_ib_state_str(new_state));\r\nqp->state = new_state;\r\n}\r\nint c2_qp_modify(struct c2_dev *c2dev, struct c2_qp *qp,\r\nstruct ib_qp_attr *attr, int attr_mask)\r\n{\r\nstruct c2wr_qp_modify_req wr;\r\nstruct c2wr_qp_modify_rep *reply;\r\nstruct c2_vq_req *vq_req;\r\nunsigned long flags;\r\nu8 next_state;\r\nint err;\r\npr_debug("%s:%d qp=%p, %s --> %s\n",\r\n__func__, __LINE__,\r\nqp,\r\nto_ib_state_str(qp->state),\r\nto_ib_state_str(attr->qp_state));\r\nvq_req = vq_req_alloc(c2dev);\r\nif (!vq_req)\r\nreturn -ENOMEM;\r\nc2_wr_set_id(&wr, CCWR_QP_MODIFY);\r\nwr.hdr.context = (unsigned long) vq_req;\r\nwr.rnic_handle = c2dev->adapter_handle;\r\nwr.qp_handle = qp->adapter_handle;\r\nwr.ord = cpu_to_be32(C2_QP_NO_ATTR_CHANGE);\r\nwr.ird = cpu_to_be32(C2_QP_NO_ATTR_CHANGE);\r\nwr.sq_depth = cpu_to_be32(C2_QP_NO_ATTR_CHANGE);\r\nwr.rq_depth = cpu_to_be32(C2_QP_NO_ATTR_CHANGE);\r\nif (attr_mask & IB_QP_STATE) {\r\nif (attr->qp_state < 0 || attr->qp_state > IB_QPS_ERR) {\r\nerr = -EINVAL;\r\ngoto bail0;\r\n}\r\nwr.next_qp_state = cpu_to_be32(to_c2_state(attr->qp_state));\r\nif (attr->qp_state == IB_QPS_ERR) {\r\nspin_lock_irqsave(&qp->lock, flags);\r\nif (qp->cm_id && qp->state == IB_QPS_RTS) {\r\npr_debug("Generating CLOSE event for QP-->ERR, "\r\n"qp=%p, cm_id=%p\n",qp,qp->cm_id);\r\nvq_req->cm_id = qp->cm_id;\r\nvq_req->event = IW_CM_EVENT_CLOSE;\r\n}\r\nspin_unlock_irqrestore(&qp->lock, flags);\r\n}\r\nnext_state = attr->qp_state;\r\n} else if (attr_mask & IB_QP_CUR_STATE) {\r\nif (attr->cur_qp_state != IB_QPS_RTR &&\r\nattr->cur_qp_state != IB_QPS_RTS &&\r\nattr->cur_qp_state != IB_QPS_SQD &&\r\nattr->cur_qp_state != IB_QPS_SQE) {\r\nerr = -EINVAL;\r\ngoto bail0;\r\n} else\r\nwr.next_qp_state =\r\ncpu_to_be32(to_c2_state(attr->cur_qp_state));\r\nnext_state = attr->cur_qp_state;\r\n} else {\r\nerr = 0;\r\ngoto bail0;\r\n}\r\nvq_req_get(c2dev, vq_req);\r\nerr = vq_send_wr(c2dev, (union c2wr *) & wr);\r\nif (err) {\r\nvq_req_put(c2dev, vq_req);\r\ngoto bail0;\r\n}\r\nerr = vq_wait_for_reply(c2dev, vq_req);\r\nif (err)\r\ngoto bail0;\r\nreply = (struct c2wr_qp_modify_rep *) (unsigned long) vq_req->reply_msg;\r\nif (!reply) {\r\nerr = -ENOMEM;\r\ngoto bail0;\r\n}\r\nerr = c2_errno(reply);\r\nif (!err)\r\nqp->state = next_state;\r\n#ifdef DEBUG\r\nelse\r\npr_debug("%s: c2_errno=%d\n", __func__, err);\r\n#endif\r\nspin_lock_irqsave(&qp->lock, flags);\r\nif (vq_req->event==IW_CM_EVENT_CLOSE && qp->cm_id) {\r\nqp->cm_id->rem_ref(qp->cm_id);\r\nqp->cm_id = NULL;\r\n}\r\nspin_unlock_irqrestore(&qp->lock, flags);\r\nvq_repbuf_free(c2dev, reply);\r\nbail0:\r\nvq_req_free(c2dev, vq_req);\r\npr_debug("%s:%d qp=%p, cur_state=%s\n",\r\n__func__, __LINE__,\r\nqp,\r\nto_ib_state_str(qp->state));\r\nreturn err;\r\n}\r\nint c2_qp_set_read_limits(struct c2_dev *c2dev, struct c2_qp *qp,\r\nint ord, int ird)\r\n{\r\nstruct c2wr_qp_modify_req wr;\r\nstruct c2wr_qp_modify_rep *reply;\r\nstruct c2_vq_req *vq_req;\r\nint err;\r\nvq_req = vq_req_alloc(c2dev);\r\nif (!vq_req)\r\nreturn -ENOMEM;\r\nc2_wr_set_id(&wr, CCWR_QP_MODIFY);\r\nwr.hdr.context = (unsigned long) vq_req;\r\nwr.rnic_handle = c2dev->adapter_handle;\r\nwr.qp_handle = qp->adapter_handle;\r\nwr.ord = cpu_to_be32(ord);\r\nwr.ird = cpu_to_be32(ird);\r\nwr.sq_depth = cpu_to_be32(C2_QP_NO_ATTR_CHANGE);\r\nwr.rq_depth = cpu_to_be32(C2_QP_NO_ATTR_CHANGE);\r\nwr.next_qp_state = cpu_to_be32(C2_QP_NO_ATTR_CHANGE);\r\nvq_req_get(c2dev, vq_req);\r\nerr = vq_send_wr(c2dev, (union c2wr *) & wr);\r\nif (err) {\r\nvq_req_put(c2dev, vq_req);\r\ngoto bail0;\r\n}\r\nerr = vq_wait_for_reply(c2dev, vq_req);\r\nif (err)\r\ngoto bail0;\r\nreply = (struct c2wr_qp_modify_rep *) (unsigned long)\r\nvq_req->reply_msg;\r\nif (!reply) {\r\nerr = -ENOMEM;\r\ngoto bail0;\r\n}\r\nerr = c2_errno(reply);\r\nvq_repbuf_free(c2dev, reply);\r\nbail0:\r\nvq_req_free(c2dev, vq_req);\r\nreturn err;\r\n}\r\nstatic int destroy_qp(struct c2_dev *c2dev, struct c2_qp *qp)\r\n{\r\nstruct c2_vq_req *vq_req;\r\nstruct c2wr_qp_destroy_req wr;\r\nstruct c2wr_qp_destroy_rep *reply;\r\nunsigned long flags;\r\nint err;\r\nvq_req = vq_req_alloc(c2dev);\r\nif (!vq_req) {\r\nreturn -ENOMEM;\r\n}\r\nc2_wr_set_id(&wr, CCWR_QP_DESTROY);\r\nwr.hdr.context = (unsigned long) vq_req;\r\nwr.rnic_handle = c2dev->adapter_handle;\r\nwr.qp_handle = qp->adapter_handle;\r\nvq_req_get(c2dev, vq_req);\r\nspin_lock_irqsave(&qp->lock, flags);\r\nif (qp->cm_id && qp->state == IB_QPS_RTS) {\r\npr_debug("destroy_qp: generating CLOSE event for QP-->ERR, "\r\n"qp=%p, cm_id=%p\n",qp,qp->cm_id);\r\nvq_req->qp = qp;\r\nvq_req->cm_id = qp->cm_id;\r\nvq_req->event = IW_CM_EVENT_CLOSE;\r\n}\r\nspin_unlock_irqrestore(&qp->lock, flags);\r\nerr = vq_send_wr(c2dev, (union c2wr *) & wr);\r\nif (err) {\r\nvq_req_put(c2dev, vq_req);\r\ngoto bail0;\r\n}\r\nerr = vq_wait_for_reply(c2dev, vq_req);\r\nif (err) {\r\ngoto bail0;\r\n}\r\nreply = (struct c2wr_qp_destroy_rep *) (unsigned long) (vq_req->reply_msg);\r\nif (!reply) {\r\nerr = -ENOMEM;\r\ngoto bail0;\r\n}\r\nspin_lock_irqsave(&qp->lock, flags);\r\nif (qp->cm_id) {\r\nqp->cm_id->rem_ref(qp->cm_id);\r\nqp->cm_id = NULL;\r\n}\r\nspin_unlock_irqrestore(&qp->lock, flags);\r\nvq_repbuf_free(c2dev, reply);\r\nbail0:\r\nvq_req_free(c2dev, vq_req);\r\nreturn err;\r\n}\r\nstatic int c2_alloc_qpn(struct c2_dev *c2dev, struct c2_qp *qp)\r\n{\r\nint ret;\r\nidr_preload(GFP_KERNEL);\r\nspin_lock_irq(&c2dev->qp_table.lock);\r\nret = idr_alloc_cyclic(&c2dev->qp_table.idr, qp, 0, 0, GFP_NOWAIT);\r\nif (ret >= 0)\r\nqp->qpn = ret;\r\nspin_unlock_irq(&c2dev->qp_table.lock);\r\nidr_preload_end();\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic void c2_free_qpn(struct c2_dev *c2dev, int qpn)\r\n{\r\nspin_lock_irq(&c2dev->qp_table.lock);\r\nidr_remove(&c2dev->qp_table.idr, qpn);\r\nspin_unlock_irq(&c2dev->qp_table.lock);\r\n}\r\nstruct c2_qp *c2_find_qpn(struct c2_dev *c2dev, int qpn)\r\n{\r\nunsigned long flags;\r\nstruct c2_qp *qp;\r\nspin_lock_irqsave(&c2dev->qp_table.lock, flags);\r\nqp = idr_find(&c2dev->qp_table.idr, qpn);\r\nspin_unlock_irqrestore(&c2dev->qp_table.lock, flags);\r\nreturn qp;\r\n}\r\nint c2_alloc_qp(struct c2_dev *c2dev,\r\nstruct c2_pd *pd,\r\nstruct ib_qp_init_attr *qp_attrs, struct c2_qp *qp)\r\n{\r\nstruct c2wr_qp_create_req wr;\r\nstruct c2wr_qp_create_rep *reply;\r\nstruct c2_vq_req *vq_req;\r\nstruct c2_cq *send_cq = to_c2cq(qp_attrs->send_cq);\r\nstruct c2_cq *recv_cq = to_c2cq(qp_attrs->recv_cq);\r\nunsigned long peer_pa;\r\nu32 q_size, msg_size, mmap_size;\r\nvoid __iomem *mmap;\r\nint err;\r\nerr = c2_alloc_qpn(c2dev, qp);\r\nif (err)\r\nreturn err;\r\nqp->ibqp.qp_num = qp->qpn;\r\nqp->ibqp.qp_type = IB_QPT_RC;\r\nqp->sq_mq.shared = c2_alloc_mqsp(c2dev, c2dev->kern_mqsp_pool,\r\n&qp->sq_mq.shared_dma, GFP_KERNEL);\r\nif (!qp->sq_mq.shared) {\r\nerr = -ENOMEM;\r\ngoto bail0;\r\n}\r\nqp->rq_mq.shared = c2_alloc_mqsp(c2dev, c2dev->kern_mqsp_pool,\r\n&qp->rq_mq.shared_dma, GFP_KERNEL);\r\nif (!qp->rq_mq.shared) {\r\nerr = -ENOMEM;\r\ngoto bail1;\r\n}\r\nvq_req = vq_req_alloc(c2dev);\r\nif (vq_req == NULL) {\r\nerr = -ENOMEM;\r\ngoto bail2;\r\n}\r\nmemset(&wr, 0, sizeof(wr));\r\nc2_wr_set_id(&wr, CCWR_QP_CREATE);\r\nwr.hdr.context = (unsigned long) vq_req;\r\nwr.rnic_handle = c2dev->adapter_handle;\r\nwr.sq_cq_handle = send_cq->adapter_handle;\r\nwr.rq_cq_handle = recv_cq->adapter_handle;\r\nwr.sq_depth = cpu_to_be32(qp_attrs->cap.max_send_wr + 1);\r\nwr.rq_depth = cpu_to_be32(qp_attrs->cap.max_recv_wr + 1);\r\nwr.srq_handle = 0;\r\nwr.flags = cpu_to_be32(QP_RDMA_READ | QP_RDMA_WRITE | QP_MW_BIND |\r\nQP_ZERO_STAG | QP_RDMA_READ_RESPONSE);\r\nwr.send_sgl_depth = cpu_to_be32(qp_attrs->cap.max_send_sge);\r\nwr.recv_sgl_depth = cpu_to_be32(qp_attrs->cap.max_recv_sge);\r\nwr.rdma_write_sgl_depth = cpu_to_be32(qp_attrs->cap.max_send_sge);\r\nwr.shared_sq_ht = cpu_to_be64(qp->sq_mq.shared_dma);\r\nwr.shared_rq_ht = cpu_to_be64(qp->rq_mq.shared_dma);\r\nwr.ord = cpu_to_be32(C2_MAX_ORD_PER_QP);\r\nwr.ird = cpu_to_be32(C2_MAX_IRD_PER_QP);\r\nwr.pd_id = pd->pd_id;\r\nwr.user_context = (unsigned long) qp;\r\nvq_req_get(c2dev, vq_req);\r\nerr = vq_send_wr(c2dev, (union c2wr *) & wr);\r\nif (err) {\r\nvq_req_put(c2dev, vq_req);\r\ngoto bail3;\r\n}\r\nerr = vq_wait_for_reply(c2dev, vq_req);\r\nif (err) {\r\ngoto bail3;\r\n}\r\nreply = (struct c2wr_qp_create_rep *) (unsigned long) (vq_req->reply_msg);\r\nif (!reply) {\r\nerr = -ENOMEM;\r\ngoto bail3;\r\n}\r\nif ((err = c2_wr_get_result(reply)) != 0) {\r\ngoto bail4;\r\n}\r\natomic_set(&qp->refcount, 1);\r\nqp->adapter_handle = reply->qp_handle;\r\nqp->state = IB_QPS_RESET;\r\nqp->send_sgl_depth = qp_attrs->cap.max_send_sge;\r\nqp->rdma_write_sgl_depth = qp_attrs->cap.max_send_sge;\r\nqp->recv_sgl_depth = qp_attrs->cap.max_recv_sge;\r\ninit_waitqueue_head(&qp->wait);\r\nq_size = be32_to_cpu(reply->sq_depth);\r\nmsg_size = be32_to_cpu(reply->sq_msg_size);\r\npeer_pa = c2dev->pa + be32_to_cpu(reply->sq_mq_start);\r\nmmap_size = PAGE_ALIGN(sizeof(struct c2_mq_shared) + msg_size * q_size);\r\nmmap = ioremap_nocache(peer_pa, mmap_size);\r\nif (!mmap) {\r\nerr = -ENOMEM;\r\ngoto bail5;\r\n}\r\nc2_mq_req_init(&qp->sq_mq,\r\nbe32_to_cpu(reply->sq_mq_index),\r\nq_size,\r\nmsg_size,\r\nmmap + sizeof(struct c2_mq_shared),\r\nmmap,\r\nC2_MQ_ADAPTER_TARGET);\r\nq_size = be32_to_cpu(reply->rq_depth);\r\nmsg_size = be32_to_cpu(reply->rq_msg_size);\r\npeer_pa = c2dev->pa + be32_to_cpu(reply->rq_mq_start);\r\nmmap_size = PAGE_ALIGN(sizeof(struct c2_mq_shared) + msg_size * q_size);\r\nmmap = ioremap_nocache(peer_pa, mmap_size);\r\nif (!mmap) {\r\nerr = -ENOMEM;\r\ngoto bail6;\r\n}\r\nc2_mq_req_init(&qp->rq_mq,\r\nbe32_to_cpu(reply->rq_mq_index),\r\nq_size,\r\nmsg_size,\r\nmmap + sizeof(struct c2_mq_shared),\r\nmmap,\r\nC2_MQ_ADAPTER_TARGET);\r\nvq_repbuf_free(c2dev, reply);\r\nvq_req_free(c2dev, vq_req);\r\nreturn 0;\r\nbail6:\r\niounmap(qp->sq_mq.peer);\r\nbail5:\r\ndestroy_qp(c2dev, qp);\r\nbail4:\r\nvq_repbuf_free(c2dev, reply);\r\nbail3:\r\nvq_req_free(c2dev, vq_req);\r\nbail2:\r\nc2_free_mqsp(qp->rq_mq.shared);\r\nbail1:\r\nc2_free_mqsp(qp->sq_mq.shared);\r\nbail0:\r\nc2_free_qpn(c2dev, qp->qpn);\r\nreturn err;\r\n}\r\nstatic inline void c2_lock_cqs(struct c2_cq *send_cq, struct c2_cq *recv_cq)\r\n{\r\nif (send_cq == recv_cq)\r\nspin_lock_irq(&send_cq->lock);\r\nelse if (send_cq > recv_cq) {\r\nspin_lock_irq(&send_cq->lock);\r\nspin_lock_nested(&recv_cq->lock, SINGLE_DEPTH_NESTING);\r\n} else {\r\nspin_lock_irq(&recv_cq->lock);\r\nspin_lock_nested(&send_cq->lock, SINGLE_DEPTH_NESTING);\r\n}\r\n}\r\nstatic inline void c2_unlock_cqs(struct c2_cq *send_cq, struct c2_cq *recv_cq)\r\n{\r\nif (send_cq == recv_cq)\r\nspin_unlock_irq(&send_cq->lock);\r\nelse if (send_cq > recv_cq) {\r\nspin_unlock(&recv_cq->lock);\r\nspin_unlock_irq(&send_cq->lock);\r\n} else {\r\nspin_unlock(&send_cq->lock);\r\nspin_unlock_irq(&recv_cq->lock);\r\n}\r\n}\r\nvoid c2_free_qp(struct c2_dev *c2dev, struct c2_qp *qp)\r\n{\r\nstruct c2_cq *send_cq;\r\nstruct c2_cq *recv_cq;\r\nsend_cq = to_c2cq(qp->ibqp.send_cq);\r\nrecv_cq = to_c2cq(qp->ibqp.recv_cq);\r\nc2_lock_cqs(send_cq, recv_cq);\r\nc2_free_qpn(c2dev, qp->qpn);\r\nc2_unlock_cqs(send_cq, recv_cq);\r\ndestroy_qp(c2dev, qp);\r\nc2_cq_clean(c2dev, qp, send_cq->cqn);\r\nif (send_cq != recv_cq)\r\nc2_cq_clean(c2dev, qp, recv_cq->cqn);\r\niounmap(qp->sq_mq.peer);\r\niounmap(qp->rq_mq.peer);\r\nc2_free_mqsp(qp->sq_mq.shared);\r\nc2_free_mqsp(qp->rq_mq.shared);\r\natomic_dec(&qp->refcount);\r\nwait_event(qp->wait, !atomic_read(&qp->refcount));\r\n}\r\nstatic int\r\nmove_sgl(struct c2_data_addr * dst, struct ib_sge *src, int count, u32 * p_len,\r\nu8 * actual_count)\r\n{\r\nu32 tot = 0;\r\nu8 acount = 0;\r\nwhile (count > 0) {\r\nif ((tot + src->length) < tot) {\r\nreturn -EINVAL;\r\n}\r\nif (src->length) {\r\ntot += src->length;\r\ndst->stag = cpu_to_be32(src->lkey);\r\ndst->to = cpu_to_be64(src->addr);\r\ndst->length = cpu_to_be32(src->length);\r\ndst++;\r\nacount++;\r\n}\r\nsrc++;\r\ncount--;\r\n}\r\nif (acount == 0) {\r\ndst->stag = 0;\r\ndst->to = 0;\r\ndst->length = 0;\r\n}\r\n*p_len = tot;\r\n*actual_count = acount;\r\nreturn 0;\r\n}\r\nstatic inline void c2_activity(struct c2_dev *c2dev, u32 mq_index, u16 shared)\r\n{\r\nwhile (readl(c2dev->regs + PCI_BAR0_ADAPTER_HINT) & 0x80000000)\r\nudelay(10);\r\n__raw_writel(C2_HINT_MAKE(mq_index, shared),\r\nc2dev->regs + PCI_BAR0_ADAPTER_HINT);\r\n}\r\nstatic int qp_wr_post(struct c2_mq *q, union c2wr * wr, struct c2_qp *qp, u32 size)\r\n{\r\nunion c2wr *msg;\r\nmsg = c2_mq_alloc(q);\r\nif (msg == NULL) {\r\nreturn -EINVAL;\r\n}\r\n#ifdef CCMSGMAGIC\r\n((c2wr_hdr_t *) wr)->magic = cpu_to_be32(CCWR_MAGIC);\r\n#endif\r\nc2_wr_set_result(wr, CCERR_PENDING);\r\nmemcpy((void *) msg, (void *) wr, size);\r\nc2_mq_produce(q);\r\nreturn 0;\r\n}\r\nint c2_post_send(struct ib_qp *ibqp, struct ib_send_wr *ib_wr,\r\nstruct ib_send_wr **bad_wr)\r\n{\r\nstruct c2_dev *c2dev = to_c2dev(ibqp->device);\r\nstruct c2_qp *qp = to_c2qp(ibqp);\r\nunion c2wr wr;\r\nunsigned long lock_flags;\r\nint err = 0;\r\nu32 flags;\r\nu32 tot_len;\r\nu8 actual_sge_count;\r\nu32 msg_size;\r\nif (qp->state > IB_QPS_RTS) {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nwhile (ib_wr) {\r\nflags = 0;\r\nwr.sqwr.sq_hdr.user_hdr.hdr.context = ib_wr->wr_id;\r\nif (ib_wr->send_flags & IB_SEND_SIGNALED) {\r\nflags |= SQ_SIGNALED;\r\n}\r\nswitch (ib_wr->opcode) {\r\ncase IB_WR_SEND:\r\ncase IB_WR_SEND_WITH_INV:\r\nif (ib_wr->opcode == IB_WR_SEND) {\r\nif (ib_wr->send_flags & IB_SEND_SOLICITED)\r\nc2_wr_set_id(&wr, C2_WR_TYPE_SEND_SE);\r\nelse\r\nc2_wr_set_id(&wr, C2_WR_TYPE_SEND);\r\nwr.sqwr.send.remote_stag = 0;\r\n} else {\r\nif (ib_wr->send_flags & IB_SEND_SOLICITED)\r\nc2_wr_set_id(&wr, C2_WR_TYPE_SEND_SE_INV);\r\nelse\r\nc2_wr_set_id(&wr, C2_WR_TYPE_SEND_INV);\r\nwr.sqwr.send.remote_stag =\r\ncpu_to_be32(ib_wr->ex.invalidate_rkey);\r\n}\r\nmsg_size = sizeof(struct c2wr_send_req) +\r\nsizeof(struct c2_data_addr) * ib_wr->num_sge;\r\nif (ib_wr->num_sge > qp->send_sgl_depth) {\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nif (ib_wr->send_flags & IB_SEND_FENCE) {\r\nflags |= SQ_READ_FENCE;\r\n}\r\nerr = move_sgl((struct c2_data_addr *) & (wr.sqwr.send.data),\r\nib_wr->sg_list,\r\nib_wr->num_sge,\r\n&tot_len, &actual_sge_count);\r\nwr.sqwr.send.sge_len = cpu_to_be32(tot_len);\r\nc2_wr_set_sge_count(&wr, actual_sge_count);\r\nbreak;\r\ncase IB_WR_RDMA_WRITE:\r\nc2_wr_set_id(&wr, C2_WR_TYPE_RDMA_WRITE);\r\nmsg_size = sizeof(struct c2wr_rdma_write_req) +\r\n(sizeof(struct c2_data_addr) * ib_wr->num_sge);\r\nif (ib_wr->num_sge > qp->rdma_write_sgl_depth) {\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nif (ib_wr->send_flags & IB_SEND_FENCE) {\r\nflags |= SQ_READ_FENCE;\r\n}\r\nwr.sqwr.rdma_write.remote_stag =\r\ncpu_to_be32(ib_wr->wr.rdma.rkey);\r\nwr.sqwr.rdma_write.remote_to =\r\ncpu_to_be64(ib_wr->wr.rdma.remote_addr);\r\nerr = move_sgl((struct c2_data_addr *)\r\n& (wr.sqwr.rdma_write.data),\r\nib_wr->sg_list,\r\nib_wr->num_sge,\r\n&tot_len, &actual_sge_count);\r\nwr.sqwr.rdma_write.sge_len = cpu_to_be32(tot_len);\r\nc2_wr_set_sge_count(&wr, actual_sge_count);\r\nbreak;\r\ncase IB_WR_RDMA_READ:\r\nc2_wr_set_id(&wr, C2_WR_TYPE_RDMA_READ);\r\nmsg_size = sizeof(struct c2wr_rdma_read_req);\r\nif (ib_wr->num_sge > 1) {\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nwr.sqwr.rdma_read.local_stag =\r\ncpu_to_be32(ib_wr->sg_list->lkey);\r\nwr.sqwr.rdma_read.local_to =\r\ncpu_to_be64(ib_wr->sg_list->addr);\r\nwr.sqwr.rdma_read.remote_stag =\r\ncpu_to_be32(ib_wr->wr.rdma.rkey);\r\nwr.sqwr.rdma_read.remote_to =\r\ncpu_to_be64(ib_wr->wr.rdma.remote_addr);\r\nwr.sqwr.rdma_read.length =\r\ncpu_to_be32(ib_wr->sg_list->length);\r\nbreak;\r\ndefault:\r\nmsg_size = 0;\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nif (err) {\r\nbreak;\r\n}\r\nc2_wr_set_flags(&wr, flags);\r\nspin_lock_irqsave(&qp->lock, lock_flags);\r\nerr = qp_wr_post(&qp->sq_mq, &wr, qp, msg_size);\r\nif (err) {\r\nspin_unlock_irqrestore(&qp->lock, lock_flags);\r\nbreak;\r\n}\r\nc2_activity(c2dev, qp->sq_mq.index, qp->sq_mq.hint_count);\r\nspin_unlock_irqrestore(&qp->lock, lock_flags);\r\nib_wr = ib_wr->next;\r\n}\r\nout:\r\nif (err)\r\n*bad_wr = ib_wr;\r\nreturn err;\r\n}\r\nint c2_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *ib_wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nstruct c2_dev *c2dev = to_c2dev(ibqp->device);\r\nstruct c2_qp *qp = to_c2qp(ibqp);\r\nunion c2wr wr;\r\nunsigned long lock_flags;\r\nint err = 0;\r\nif (qp->state > IB_QPS_RTS) {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nwhile (ib_wr) {\r\nu32 tot_len;\r\nu8 actual_sge_count;\r\nif (ib_wr->num_sge > qp->recv_sgl_depth) {\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nwr.rqwr.rq_hdr.user_hdr.hdr.context = ib_wr->wr_id;\r\nc2_wr_set_id(&wr, CCWR_RECV);\r\nc2_wr_set_flags(&wr, 0);\r\nBUG_ON(ib_wr->num_sge >= 256);\r\nerr = move_sgl((struct c2_data_addr *) & (wr.rqwr.data),\r\nib_wr->sg_list,\r\nib_wr->num_sge, &tot_len, &actual_sge_count);\r\nc2_wr_set_sge_count(&wr, actual_sge_count);\r\nif (err) {\r\nbreak;\r\n}\r\nspin_lock_irqsave(&qp->lock, lock_flags);\r\nerr = qp_wr_post(&qp->rq_mq, &wr, qp, qp->rq_mq.msg_size);\r\nif (err) {\r\nspin_unlock_irqrestore(&qp->lock, lock_flags);\r\nbreak;\r\n}\r\nc2_activity(c2dev, qp->rq_mq.index, qp->rq_mq.hint_count);\r\nspin_unlock_irqrestore(&qp->lock, lock_flags);\r\nib_wr = ib_wr->next;\r\n}\r\nout:\r\nif (err)\r\n*bad_wr = ib_wr;\r\nreturn err;\r\n}\r\nvoid c2_init_qp_table(struct c2_dev *c2dev)\r\n{\r\nspin_lock_init(&c2dev->qp_table.lock);\r\nidr_init(&c2dev->qp_table.idr);\r\n}\r\nvoid c2_cleanup_qp_table(struct c2_dev *c2dev)\r\n{\r\nidr_destroy(&c2dev->qp_table.idr);\r\n}
