static struct kvm_assigned_dev_kernel *kvm_find_assigned_dev(struct list_head *head,\r\nint assigned_dev_id)\r\n{\r\nstruct list_head *ptr;\r\nstruct kvm_assigned_dev_kernel *match;\r\nlist_for_each(ptr, head) {\r\nmatch = list_entry(ptr, struct kvm_assigned_dev_kernel, list);\r\nif (match->assigned_dev_id == assigned_dev_id)\r\nreturn match;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int find_index_from_host_irq(struct kvm_assigned_dev_kernel\r\n*assigned_dev, int irq)\r\n{\r\nint i, index;\r\nstruct msix_entry *host_msix_entries;\r\nhost_msix_entries = assigned_dev->host_msix_entries;\r\nindex = -1;\r\nfor (i = 0; i < assigned_dev->entries_nr; i++)\r\nif (irq == host_msix_entries[i].vector) {\r\nindex = i;\r\nbreak;\r\n}\r\nif (index < 0)\r\nprintk(KERN_WARNING "Fail to find correlated MSI-X entry!\n");\r\nreturn index;\r\n}\r\nstatic irqreturn_t kvm_assigned_dev_intx(int irq, void *dev_id)\r\n{\r\nstruct kvm_assigned_dev_kernel *assigned_dev = dev_id;\r\nint ret;\r\nspin_lock(&assigned_dev->intx_lock);\r\nif (pci_check_and_mask_intx(assigned_dev->dev)) {\r\nassigned_dev->host_irq_disabled = true;\r\nret = IRQ_WAKE_THREAD;\r\n} else\r\nret = IRQ_NONE;\r\nspin_unlock(&assigned_dev->intx_lock);\r\nreturn ret;\r\n}\r\nstatic void\r\nkvm_assigned_dev_raise_guest_irq(struct kvm_assigned_dev_kernel *assigned_dev,\r\nint vector)\r\n{\r\nif (unlikely(assigned_dev->irq_requested_type &\r\nKVM_DEV_IRQ_GUEST_INTX)) {\r\nspin_lock(&assigned_dev->intx_mask_lock);\r\nif (!(assigned_dev->flags & KVM_DEV_ASSIGN_MASK_INTX))\r\nkvm_set_irq(assigned_dev->kvm,\r\nassigned_dev->irq_source_id, vector, 1,\r\nfalse);\r\nspin_unlock(&assigned_dev->intx_mask_lock);\r\n} else\r\nkvm_set_irq(assigned_dev->kvm, assigned_dev->irq_source_id,\r\nvector, 1, false);\r\n}\r\nstatic irqreturn_t kvm_assigned_dev_thread_intx(int irq, void *dev_id)\r\n{\r\nstruct kvm_assigned_dev_kernel *assigned_dev = dev_id;\r\nif (!(assigned_dev->flags & KVM_DEV_ASSIGN_PCI_2_3)) {\r\nspin_lock_irq(&assigned_dev->intx_lock);\r\ndisable_irq_nosync(irq);\r\nassigned_dev->host_irq_disabled = true;\r\nspin_unlock_irq(&assigned_dev->intx_lock);\r\n}\r\nkvm_assigned_dev_raise_guest_irq(assigned_dev,\r\nassigned_dev->guest_irq);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t kvm_assigned_dev_msi(int irq, void *dev_id)\r\n{\r\nstruct kvm_assigned_dev_kernel *assigned_dev = dev_id;\r\nint ret = kvm_set_irq_inatomic(assigned_dev->kvm,\r\nassigned_dev->irq_source_id,\r\nassigned_dev->guest_irq, 1);\r\nreturn unlikely(ret == -EWOULDBLOCK) ? IRQ_WAKE_THREAD : IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t kvm_assigned_dev_thread_msi(int irq, void *dev_id)\r\n{\r\nstruct kvm_assigned_dev_kernel *assigned_dev = dev_id;\r\nkvm_assigned_dev_raise_guest_irq(assigned_dev,\r\nassigned_dev->guest_irq);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t kvm_assigned_dev_msix(int irq, void *dev_id)\r\n{\r\nstruct kvm_assigned_dev_kernel *assigned_dev = dev_id;\r\nint index = find_index_from_host_irq(assigned_dev, irq);\r\nu32 vector;\r\nint ret = 0;\r\nif (index >= 0) {\r\nvector = assigned_dev->guest_msix_entries[index].vector;\r\nret = kvm_set_irq_inatomic(assigned_dev->kvm,\r\nassigned_dev->irq_source_id,\r\nvector, 1);\r\n}\r\nreturn unlikely(ret == -EWOULDBLOCK) ? IRQ_WAKE_THREAD : IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t kvm_assigned_dev_thread_msix(int irq, void *dev_id)\r\n{\r\nstruct kvm_assigned_dev_kernel *assigned_dev = dev_id;\r\nint index = find_index_from_host_irq(assigned_dev, irq);\r\nu32 vector;\r\nif (index >= 0) {\r\nvector = assigned_dev->guest_msix_entries[index].vector;\r\nkvm_assigned_dev_raise_guest_irq(assigned_dev, vector);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void kvm_assigned_dev_ack_irq(struct kvm_irq_ack_notifier *kian)\r\n{\r\nstruct kvm_assigned_dev_kernel *dev =\r\ncontainer_of(kian, struct kvm_assigned_dev_kernel,\r\nack_notifier);\r\nkvm_set_irq(dev->kvm, dev->irq_source_id, dev->guest_irq, 0, false);\r\nspin_lock(&dev->intx_mask_lock);\r\nif (!(dev->flags & KVM_DEV_ASSIGN_MASK_INTX)) {\r\nbool reassert = false;\r\nspin_lock_irq(&dev->intx_lock);\r\nif (dev->host_irq_disabled) {\r\nif (!(dev->flags & KVM_DEV_ASSIGN_PCI_2_3))\r\nenable_irq(dev->host_irq);\r\nelse if (!pci_check_and_unmask_intx(dev->dev))\r\nreassert = true;\r\ndev->host_irq_disabled = reassert;\r\n}\r\nspin_unlock_irq(&dev->intx_lock);\r\nif (reassert)\r\nkvm_set_irq(dev->kvm, dev->irq_source_id,\r\ndev->guest_irq, 1, false);\r\n}\r\nspin_unlock(&dev->intx_mask_lock);\r\n}\r\nstatic void deassign_guest_irq(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *assigned_dev)\r\n{\r\nif (assigned_dev->ack_notifier.gsi != -1)\r\nkvm_unregister_irq_ack_notifier(kvm,\r\n&assigned_dev->ack_notifier);\r\nkvm_set_irq(assigned_dev->kvm, assigned_dev->irq_source_id,\r\nassigned_dev->guest_irq, 0, false);\r\nif (assigned_dev->irq_source_id != -1)\r\nkvm_free_irq_source_id(kvm, assigned_dev->irq_source_id);\r\nassigned_dev->irq_source_id = -1;\r\nassigned_dev->irq_requested_type &= ~(KVM_DEV_IRQ_GUEST_MASK);\r\n}\r\nstatic void deassign_host_irq(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *assigned_dev)\r\n{\r\nif (assigned_dev->irq_requested_type & KVM_DEV_IRQ_HOST_MSIX) {\r\nint i;\r\nfor (i = 0; i < assigned_dev->entries_nr; i++)\r\ndisable_irq(assigned_dev->host_msix_entries[i].vector);\r\nfor (i = 0; i < assigned_dev->entries_nr; i++)\r\nfree_irq(assigned_dev->host_msix_entries[i].vector,\r\nassigned_dev);\r\nassigned_dev->entries_nr = 0;\r\nkfree(assigned_dev->host_msix_entries);\r\nkfree(assigned_dev->guest_msix_entries);\r\npci_disable_msix(assigned_dev->dev);\r\n} else {\r\nif ((assigned_dev->irq_requested_type &\r\nKVM_DEV_IRQ_HOST_INTX) &&\r\n(assigned_dev->flags & KVM_DEV_ASSIGN_PCI_2_3)) {\r\nspin_lock_irq(&assigned_dev->intx_lock);\r\npci_intx(assigned_dev->dev, false);\r\nspin_unlock_irq(&assigned_dev->intx_lock);\r\nsynchronize_irq(assigned_dev->host_irq);\r\n} else\r\ndisable_irq(assigned_dev->host_irq);\r\nfree_irq(assigned_dev->host_irq, assigned_dev);\r\nif (assigned_dev->irq_requested_type & KVM_DEV_IRQ_HOST_MSI)\r\npci_disable_msi(assigned_dev->dev);\r\n}\r\nassigned_dev->irq_requested_type &= ~(KVM_DEV_IRQ_HOST_MASK);\r\n}\r\nstatic int kvm_deassign_irq(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *assigned_dev,\r\nunsigned long irq_requested_type)\r\n{\r\nunsigned long guest_irq_type, host_irq_type;\r\nif (!irqchip_in_kernel(kvm))\r\nreturn -EINVAL;\r\nif (!assigned_dev->irq_requested_type)\r\nreturn -ENXIO;\r\nhost_irq_type = irq_requested_type & KVM_DEV_IRQ_HOST_MASK;\r\nguest_irq_type = irq_requested_type & KVM_DEV_IRQ_GUEST_MASK;\r\nif (host_irq_type)\r\ndeassign_host_irq(kvm, assigned_dev);\r\nif (guest_irq_type)\r\ndeassign_guest_irq(kvm, assigned_dev);\r\nreturn 0;\r\n}\r\nstatic void kvm_free_assigned_irq(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *assigned_dev)\r\n{\r\nkvm_deassign_irq(kvm, assigned_dev, assigned_dev->irq_requested_type);\r\n}\r\nstatic void kvm_free_assigned_device(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel\r\n*assigned_dev)\r\n{\r\nkvm_free_assigned_irq(kvm, assigned_dev);\r\npci_reset_function(assigned_dev->dev);\r\nif (pci_load_and_free_saved_state(assigned_dev->dev,\r\n&assigned_dev->pci_saved_state))\r\nprintk(KERN_INFO "%s: Couldn't reload %s saved state\n",\r\n__func__, dev_name(&assigned_dev->dev->dev));\r\nelse\r\npci_restore_state(assigned_dev->dev);\r\npci_clear_dev_assigned(assigned_dev->dev);\r\npci_release_regions(assigned_dev->dev);\r\npci_disable_device(assigned_dev->dev);\r\npci_dev_put(assigned_dev->dev);\r\nlist_del(&assigned_dev->list);\r\nkfree(assigned_dev);\r\n}\r\nvoid kvm_free_all_assigned_devices(struct kvm *kvm)\r\n{\r\nstruct list_head *ptr, *ptr2;\r\nstruct kvm_assigned_dev_kernel *assigned_dev;\r\nlist_for_each_safe(ptr, ptr2, &kvm->arch.assigned_dev_head) {\r\nassigned_dev = list_entry(ptr,\r\nstruct kvm_assigned_dev_kernel,\r\nlist);\r\nkvm_free_assigned_device(kvm, assigned_dev);\r\n}\r\n}\r\nstatic int assigned_device_enable_host_intx(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *dev)\r\n{\r\nirq_handler_t irq_handler;\r\nunsigned long flags;\r\ndev->host_irq = dev->dev->irq;\r\nif (dev->flags & KVM_DEV_ASSIGN_PCI_2_3) {\r\nirq_handler = kvm_assigned_dev_intx;\r\nflags = IRQF_SHARED;\r\n} else {\r\nirq_handler = NULL;\r\nflags = IRQF_ONESHOT;\r\n}\r\nif (request_threaded_irq(dev->host_irq, irq_handler,\r\nkvm_assigned_dev_thread_intx, flags,\r\ndev->irq_name, dev))\r\nreturn -EIO;\r\nif (dev->flags & KVM_DEV_ASSIGN_PCI_2_3) {\r\nspin_lock_irq(&dev->intx_lock);\r\npci_intx(dev->dev, true);\r\nspin_unlock_irq(&dev->intx_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic int assigned_device_enable_host_msi(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *dev)\r\n{\r\nint r;\r\nif (!dev->dev->msi_enabled) {\r\nr = pci_enable_msi(dev->dev);\r\nif (r)\r\nreturn r;\r\n}\r\ndev->host_irq = dev->dev->irq;\r\nif (request_threaded_irq(dev->host_irq, kvm_assigned_dev_msi,\r\nkvm_assigned_dev_thread_msi, 0,\r\ndev->irq_name, dev)) {\r\npci_disable_msi(dev->dev);\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int assigned_device_enable_host_msix(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *dev)\r\n{\r\nint i, r = -EINVAL;\r\nif (dev->entries_nr == 0)\r\nreturn r;\r\nr = pci_enable_msix_exact(dev->dev,\r\ndev->host_msix_entries, dev->entries_nr);\r\nif (r)\r\nreturn r;\r\nfor (i = 0; i < dev->entries_nr; i++) {\r\nr = request_threaded_irq(dev->host_msix_entries[i].vector,\r\nkvm_assigned_dev_msix,\r\nkvm_assigned_dev_thread_msix,\r\n0, dev->irq_name, dev);\r\nif (r)\r\ngoto err;\r\n}\r\nreturn 0;\r\nerr:\r\nfor (i -= 1; i >= 0; i--)\r\nfree_irq(dev->host_msix_entries[i].vector, dev);\r\npci_disable_msix(dev->dev);\r\nreturn r;\r\n}\r\nstatic int assigned_device_enable_guest_intx(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *dev,\r\nstruct kvm_assigned_irq *irq)\r\n{\r\ndev->guest_irq = irq->guest_irq;\r\ndev->ack_notifier.gsi = irq->guest_irq;\r\nreturn 0;\r\n}\r\nstatic int assigned_device_enable_guest_msi(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *dev,\r\nstruct kvm_assigned_irq *irq)\r\n{\r\ndev->guest_irq = irq->guest_irq;\r\ndev->ack_notifier.gsi = -1;\r\nreturn 0;\r\n}\r\nstatic int assigned_device_enable_guest_msix(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *dev,\r\nstruct kvm_assigned_irq *irq)\r\n{\r\ndev->guest_irq = irq->guest_irq;\r\ndev->ack_notifier.gsi = -1;\r\nreturn 0;\r\n}\r\nstatic int assign_host_irq(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *dev,\r\n__u32 host_irq_type)\r\n{\r\nint r = -EEXIST;\r\nif (dev->irq_requested_type & KVM_DEV_IRQ_HOST_MASK)\r\nreturn r;\r\nsnprintf(dev->irq_name, sizeof(dev->irq_name), "kvm:%s",\r\npci_name(dev->dev));\r\nswitch (host_irq_type) {\r\ncase KVM_DEV_IRQ_HOST_INTX:\r\nr = assigned_device_enable_host_intx(kvm, dev);\r\nbreak;\r\n#ifdef __KVM_HAVE_MSI\r\ncase KVM_DEV_IRQ_HOST_MSI:\r\nr = assigned_device_enable_host_msi(kvm, dev);\r\nbreak;\r\n#endif\r\n#ifdef __KVM_HAVE_MSIX\r\ncase KVM_DEV_IRQ_HOST_MSIX:\r\nr = assigned_device_enable_host_msix(kvm, dev);\r\nbreak;\r\n#endif\r\ndefault:\r\nr = -EINVAL;\r\n}\r\ndev->host_irq_disabled = false;\r\nif (!r)\r\ndev->irq_requested_type |= host_irq_type;\r\nreturn r;\r\n}\r\nstatic int assign_guest_irq(struct kvm *kvm,\r\nstruct kvm_assigned_dev_kernel *dev,\r\nstruct kvm_assigned_irq *irq,\r\nunsigned long guest_irq_type)\r\n{\r\nint id;\r\nint r = -EEXIST;\r\nif (dev->irq_requested_type & KVM_DEV_IRQ_GUEST_MASK)\r\nreturn r;\r\nid = kvm_request_irq_source_id(kvm);\r\nif (id < 0)\r\nreturn id;\r\ndev->irq_source_id = id;\r\nswitch (guest_irq_type) {\r\ncase KVM_DEV_IRQ_GUEST_INTX:\r\nr = assigned_device_enable_guest_intx(kvm, dev, irq);\r\nbreak;\r\n#ifdef __KVM_HAVE_MSI\r\ncase KVM_DEV_IRQ_GUEST_MSI:\r\nr = assigned_device_enable_guest_msi(kvm, dev, irq);\r\nbreak;\r\n#endif\r\n#ifdef __KVM_HAVE_MSIX\r\ncase KVM_DEV_IRQ_GUEST_MSIX:\r\nr = assigned_device_enable_guest_msix(kvm, dev, irq);\r\nbreak;\r\n#endif\r\ndefault:\r\nr = -EINVAL;\r\n}\r\nif (!r) {\r\ndev->irq_requested_type |= guest_irq_type;\r\nif (dev->ack_notifier.gsi != -1)\r\nkvm_register_irq_ack_notifier(kvm, &dev->ack_notifier);\r\n} else {\r\nkvm_free_irq_source_id(kvm, dev->irq_source_id);\r\ndev->irq_source_id = -1;\r\n}\r\nreturn r;\r\n}\r\nstatic int kvm_vm_ioctl_assign_irq(struct kvm *kvm,\r\nstruct kvm_assigned_irq *assigned_irq)\r\n{\r\nint r = -EINVAL;\r\nstruct kvm_assigned_dev_kernel *match;\r\nunsigned long host_irq_type, guest_irq_type;\r\nif (!irqchip_in_kernel(kvm))\r\nreturn r;\r\nmutex_lock(&kvm->lock);\r\nr = -ENODEV;\r\nmatch = kvm_find_assigned_dev(&kvm->arch.assigned_dev_head,\r\nassigned_irq->assigned_dev_id);\r\nif (!match)\r\ngoto out;\r\nhost_irq_type = (assigned_irq->flags & KVM_DEV_IRQ_HOST_MASK);\r\nguest_irq_type = (assigned_irq->flags & KVM_DEV_IRQ_GUEST_MASK);\r\nr = -EINVAL;\r\nif (hweight_long(host_irq_type) > 1)\r\ngoto out;\r\nif (hweight_long(guest_irq_type) > 1)\r\ngoto out;\r\nif (host_irq_type == 0 && guest_irq_type == 0)\r\ngoto out;\r\nr = 0;\r\nif (host_irq_type)\r\nr = assign_host_irq(kvm, match, host_irq_type);\r\nif (r)\r\ngoto out;\r\nif (guest_irq_type)\r\nr = assign_guest_irq(kvm, match, assigned_irq, guest_irq_type);\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\n}\r\nstatic int kvm_vm_ioctl_deassign_dev_irq(struct kvm *kvm,\r\nstruct kvm_assigned_irq\r\n*assigned_irq)\r\n{\r\nint r = -ENODEV;\r\nstruct kvm_assigned_dev_kernel *match;\r\nunsigned long irq_type;\r\nmutex_lock(&kvm->lock);\r\nmatch = kvm_find_assigned_dev(&kvm->arch.assigned_dev_head,\r\nassigned_irq->assigned_dev_id);\r\nif (!match)\r\ngoto out;\r\nirq_type = assigned_irq->flags & (KVM_DEV_IRQ_HOST_MASK |\r\nKVM_DEV_IRQ_GUEST_MASK);\r\nr = kvm_deassign_irq(kvm, match, irq_type);\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\n}\r\nstatic int probe_sysfs_permissions(struct pci_dev *dev)\r\n{\r\n#ifdef CONFIG_SYSFS\r\nint i;\r\nbool bar_found = false;\r\nfor (i = PCI_STD_RESOURCES; i <= PCI_STD_RESOURCE_END; i++) {\r\nchar *kpath, *syspath;\r\nstruct path path;\r\nstruct inode *inode;\r\nint r;\r\nif (!pci_resource_len(dev, i))\r\ncontinue;\r\nkpath = kobject_get_path(&dev->dev.kobj, GFP_KERNEL);\r\nif (!kpath)\r\nreturn -ENOMEM;\r\nsyspath = kasprintf(GFP_KERNEL, "/sys%s/resource%d", kpath, i);\r\nkfree(kpath);\r\nif (!syspath)\r\nreturn -ENOMEM;\r\nr = kern_path(syspath, LOOKUP_FOLLOW, &path);\r\nkfree(syspath);\r\nif (r)\r\nreturn r;\r\ninode = d_backing_inode(path.dentry);\r\nr = inode_permission(inode, MAY_READ | MAY_WRITE | MAY_ACCESS);\r\npath_put(&path);\r\nif (r)\r\nreturn r;\r\nbar_found = true;\r\n}\r\nif (!bar_found)\r\nreturn -EPERM;\r\nreturn 0;\r\n#else\r\nreturn -EINVAL;\r\n#endif\r\n}\r\nstatic int kvm_vm_ioctl_assign_device(struct kvm *kvm,\r\nstruct kvm_assigned_pci_dev *assigned_dev)\r\n{\r\nint r = 0, idx;\r\nstruct kvm_assigned_dev_kernel *match;\r\nstruct pci_dev *dev;\r\nif (!(assigned_dev->flags & KVM_DEV_ASSIGN_ENABLE_IOMMU))\r\nreturn -EINVAL;\r\nmutex_lock(&kvm->lock);\r\nidx = srcu_read_lock(&kvm->srcu);\r\nmatch = kvm_find_assigned_dev(&kvm->arch.assigned_dev_head,\r\nassigned_dev->assigned_dev_id);\r\nif (match) {\r\nr = -EEXIST;\r\ngoto out;\r\n}\r\nmatch = kzalloc(sizeof(struct kvm_assigned_dev_kernel), GFP_KERNEL);\r\nif (match == NULL) {\r\nprintk(KERN_INFO "%s: Couldn't allocate memory\n",\r\n__func__);\r\nr = -ENOMEM;\r\ngoto out;\r\n}\r\ndev = pci_get_domain_bus_and_slot(assigned_dev->segnr,\r\nassigned_dev->busnr,\r\nassigned_dev->devfn);\r\nif (!dev) {\r\nprintk(KERN_INFO "%s: host device not found\n", __func__);\r\nr = -EINVAL;\r\ngoto out_free;\r\n}\r\nif (dev->hdr_type != PCI_HEADER_TYPE_NORMAL) {\r\nr = -EPERM;\r\ngoto out_put;\r\n}\r\nr = probe_sysfs_permissions(dev);\r\nif (r)\r\ngoto out_put;\r\nif (pci_enable_device(dev)) {\r\nprintk(KERN_INFO "%s: Could not enable PCI device\n", __func__);\r\nr = -EBUSY;\r\ngoto out_put;\r\n}\r\nr = pci_request_regions(dev, "kvm_assigned_device");\r\nif (r) {\r\nprintk(KERN_INFO "%s: Could not get access to device regions\n",\r\n__func__);\r\ngoto out_disable;\r\n}\r\npci_reset_function(dev);\r\npci_save_state(dev);\r\nmatch->pci_saved_state = pci_store_saved_state(dev);\r\nif (!match->pci_saved_state)\r\nprintk(KERN_DEBUG "%s: Couldn't store %s saved state\n",\r\n__func__, dev_name(&dev->dev));\r\nif (!pci_intx_mask_supported(dev))\r\nassigned_dev->flags &= ~KVM_DEV_ASSIGN_PCI_2_3;\r\nmatch->assigned_dev_id = assigned_dev->assigned_dev_id;\r\nmatch->host_segnr = assigned_dev->segnr;\r\nmatch->host_busnr = assigned_dev->busnr;\r\nmatch->host_devfn = assigned_dev->devfn;\r\nmatch->flags = assigned_dev->flags;\r\nmatch->dev = dev;\r\nspin_lock_init(&match->intx_lock);\r\nspin_lock_init(&match->intx_mask_lock);\r\nmatch->irq_source_id = -1;\r\nmatch->kvm = kvm;\r\nmatch->ack_notifier.irq_acked = kvm_assigned_dev_ack_irq;\r\nlist_add(&match->list, &kvm->arch.assigned_dev_head);\r\nif (!kvm->arch.iommu_domain) {\r\nr = kvm_iommu_map_guest(kvm);\r\nif (r)\r\ngoto out_list_del;\r\n}\r\nr = kvm_assign_device(kvm, match->dev);\r\nif (r)\r\ngoto out_list_del;\r\nout:\r\nsrcu_read_unlock(&kvm->srcu, idx);\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\nout_list_del:\r\nif (pci_load_and_free_saved_state(dev, &match->pci_saved_state))\r\nprintk(KERN_INFO "%s: Couldn't reload %s saved state\n",\r\n__func__, dev_name(&dev->dev));\r\nlist_del(&match->list);\r\npci_release_regions(dev);\r\nout_disable:\r\npci_disable_device(dev);\r\nout_put:\r\npci_dev_put(dev);\r\nout_free:\r\nkfree(match);\r\nsrcu_read_unlock(&kvm->srcu, idx);\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\n}\r\nstatic int kvm_vm_ioctl_deassign_device(struct kvm *kvm,\r\nstruct kvm_assigned_pci_dev *assigned_dev)\r\n{\r\nint r = 0;\r\nstruct kvm_assigned_dev_kernel *match;\r\nmutex_lock(&kvm->lock);\r\nmatch = kvm_find_assigned_dev(&kvm->arch.assigned_dev_head,\r\nassigned_dev->assigned_dev_id);\r\nif (!match) {\r\nprintk(KERN_INFO "%s: device hasn't been assigned before, "\r\n"so cannot be deassigned\n", __func__);\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nkvm_deassign_device(kvm, match->dev);\r\nkvm_free_assigned_device(kvm, match);\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\n}\r\nstatic int kvm_vm_ioctl_set_msix_nr(struct kvm *kvm,\r\nstruct kvm_assigned_msix_nr *entry_nr)\r\n{\r\nint r = 0;\r\nstruct kvm_assigned_dev_kernel *adev;\r\nmutex_lock(&kvm->lock);\r\nadev = kvm_find_assigned_dev(&kvm->arch.assigned_dev_head,\r\nentry_nr->assigned_dev_id);\r\nif (!adev) {\r\nr = -EINVAL;\r\ngoto msix_nr_out;\r\n}\r\nif (adev->entries_nr == 0) {\r\nadev->entries_nr = entry_nr->entry_nr;\r\nif (adev->entries_nr == 0 ||\r\nadev->entries_nr > KVM_MAX_MSIX_PER_DEV) {\r\nr = -EINVAL;\r\ngoto msix_nr_out;\r\n}\r\nadev->host_msix_entries = kzalloc(sizeof(struct msix_entry) *\r\nentry_nr->entry_nr,\r\nGFP_KERNEL);\r\nif (!adev->host_msix_entries) {\r\nr = -ENOMEM;\r\ngoto msix_nr_out;\r\n}\r\nadev->guest_msix_entries =\r\nkzalloc(sizeof(struct msix_entry) * entry_nr->entry_nr,\r\nGFP_KERNEL);\r\nif (!adev->guest_msix_entries) {\r\nkfree(adev->host_msix_entries);\r\nr = -ENOMEM;\r\ngoto msix_nr_out;\r\n}\r\n} else\r\nr = -EINVAL;\r\nmsix_nr_out:\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\n}\r\nstatic int kvm_vm_ioctl_set_msix_entry(struct kvm *kvm,\r\nstruct kvm_assigned_msix_entry *entry)\r\n{\r\nint r = 0, i;\r\nstruct kvm_assigned_dev_kernel *adev;\r\nmutex_lock(&kvm->lock);\r\nadev = kvm_find_assigned_dev(&kvm->arch.assigned_dev_head,\r\nentry->assigned_dev_id);\r\nif (!adev) {\r\nr = -EINVAL;\r\ngoto msix_entry_out;\r\n}\r\nfor (i = 0; i < adev->entries_nr; i++)\r\nif (adev->guest_msix_entries[i].vector == 0 ||\r\nadev->guest_msix_entries[i].entry == entry->entry) {\r\nadev->guest_msix_entries[i].entry = entry->entry;\r\nadev->guest_msix_entries[i].vector = entry->gsi;\r\nadev->host_msix_entries[i].entry = entry->entry;\r\nbreak;\r\n}\r\nif (i == adev->entries_nr) {\r\nr = -ENOSPC;\r\ngoto msix_entry_out;\r\n}\r\nmsix_entry_out:\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\n}\r\nstatic int kvm_vm_ioctl_set_pci_irq_mask(struct kvm *kvm,\r\nstruct kvm_assigned_pci_dev *assigned_dev)\r\n{\r\nint r = 0;\r\nstruct kvm_assigned_dev_kernel *match;\r\nmutex_lock(&kvm->lock);\r\nmatch = kvm_find_assigned_dev(&kvm->arch.assigned_dev_head,\r\nassigned_dev->assigned_dev_id);\r\nif (!match) {\r\nr = -ENODEV;\r\ngoto out;\r\n}\r\nspin_lock(&match->intx_mask_lock);\r\nmatch->flags &= ~KVM_DEV_ASSIGN_MASK_INTX;\r\nmatch->flags |= assigned_dev->flags & KVM_DEV_ASSIGN_MASK_INTX;\r\nif (match->irq_requested_type & KVM_DEV_IRQ_GUEST_INTX) {\r\nif (assigned_dev->flags & KVM_DEV_ASSIGN_MASK_INTX) {\r\nkvm_set_irq(match->kvm, match->irq_source_id,\r\nmatch->guest_irq, 0, false);\r\n} else if (!(assigned_dev->flags & KVM_DEV_ASSIGN_PCI_2_3)) {\r\nspin_lock_irq(&match->intx_lock);\r\nif (match->host_irq_disabled) {\r\nenable_irq(match->host_irq);\r\nmatch->host_irq_disabled = false;\r\n}\r\nspin_unlock_irq(&match->intx_lock);\r\n}\r\n}\r\nspin_unlock(&match->intx_mask_lock);\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn r;\r\n}\r\nlong kvm_vm_ioctl_assigned_device(struct kvm *kvm, unsigned ioctl,\r\nunsigned long arg)\r\n{\r\nvoid __user *argp = (void __user *)arg;\r\nint r;\r\nswitch (ioctl) {\r\ncase KVM_ASSIGN_PCI_DEVICE: {\r\nstruct kvm_assigned_pci_dev assigned_dev;\r\nr = -EFAULT;\r\nif (copy_from_user(&assigned_dev, argp, sizeof assigned_dev))\r\ngoto out;\r\nr = kvm_vm_ioctl_assign_device(kvm, &assigned_dev);\r\nif (r)\r\ngoto out;\r\nbreak;\r\n}\r\ncase KVM_ASSIGN_IRQ: {\r\nr = -EOPNOTSUPP;\r\nbreak;\r\n}\r\ncase KVM_ASSIGN_DEV_IRQ: {\r\nstruct kvm_assigned_irq assigned_irq;\r\nr = -EFAULT;\r\nif (copy_from_user(&assigned_irq, argp, sizeof assigned_irq))\r\ngoto out;\r\nr = kvm_vm_ioctl_assign_irq(kvm, &assigned_irq);\r\nif (r)\r\ngoto out;\r\nbreak;\r\n}\r\ncase KVM_DEASSIGN_DEV_IRQ: {\r\nstruct kvm_assigned_irq assigned_irq;\r\nr = -EFAULT;\r\nif (copy_from_user(&assigned_irq, argp, sizeof assigned_irq))\r\ngoto out;\r\nr = kvm_vm_ioctl_deassign_dev_irq(kvm, &assigned_irq);\r\nif (r)\r\ngoto out;\r\nbreak;\r\n}\r\ncase KVM_DEASSIGN_PCI_DEVICE: {\r\nstruct kvm_assigned_pci_dev assigned_dev;\r\nr = -EFAULT;\r\nif (copy_from_user(&assigned_dev, argp, sizeof assigned_dev))\r\ngoto out;\r\nr = kvm_vm_ioctl_deassign_device(kvm, &assigned_dev);\r\nif (r)\r\ngoto out;\r\nbreak;\r\n}\r\n#ifdef __KVM_HAVE_MSIX\r\ncase KVM_ASSIGN_SET_MSIX_NR: {\r\nstruct kvm_assigned_msix_nr entry_nr;\r\nr = -EFAULT;\r\nif (copy_from_user(&entry_nr, argp, sizeof entry_nr))\r\ngoto out;\r\nr = kvm_vm_ioctl_set_msix_nr(kvm, &entry_nr);\r\nif (r)\r\ngoto out;\r\nbreak;\r\n}\r\ncase KVM_ASSIGN_SET_MSIX_ENTRY: {\r\nstruct kvm_assigned_msix_entry entry;\r\nr = -EFAULT;\r\nif (copy_from_user(&entry, argp, sizeof entry))\r\ngoto out;\r\nr = kvm_vm_ioctl_set_msix_entry(kvm, &entry);\r\nif (r)\r\ngoto out;\r\nbreak;\r\n}\r\n#endif\r\ncase KVM_ASSIGN_SET_INTX_MASK: {\r\nstruct kvm_assigned_pci_dev assigned_dev;\r\nr = -EFAULT;\r\nif (copy_from_user(&assigned_dev, argp, sizeof assigned_dev))\r\ngoto out;\r\nr = kvm_vm_ioctl_set_pci_irq_mask(kvm, &assigned_dev);\r\nbreak;\r\n}\r\ndefault:\r\nr = -ENOTTY;\r\nbreak;\r\n}\r\nout:\r\nreturn r;\r\n}
