static inline bool cast5_fpu_begin(bool fpu_enabled, unsigned int nbytes)\r\n{\r\nreturn glue_fpu_begin(CAST5_BLOCK_SIZE, CAST5_PARALLEL_BLOCKS,\r\nNULL, fpu_enabled, nbytes);\r\n}\r\nstatic inline void cast5_fpu_end(bool fpu_enabled)\r\n{\r\nreturn glue_fpu_end(fpu_enabled);\r\n}\r\nstatic int ecb_crypt(struct blkcipher_desc *desc, struct blkcipher_walk *walk,\r\nbool enc)\r\n{\r\nbool fpu_enabled = false;\r\nstruct cast5_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = CAST5_BLOCK_SIZE;\r\nunsigned int nbytes;\r\nvoid (*fn)(struct cast5_ctx *ctx, u8 *dst, const u8 *src);\r\nint err;\r\nfn = (enc) ? cast5_ecb_enc_16way : cast5_ecb_dec_16way;\r\nerr = blkcipher_walk_virt(desc, walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nwhile ((nbytes = walk->nbytes)) {\r\nu8 *wsrc = walk->src.virt.addr;\r\nu8 *wdst = walk->dst.virt.addr;\r\nfpu_enabled = cast5_fpu_begin(fpu_enabled, nbytes);\r\nif (nbytes >= bsize * CAST5_PARALLEL_BLOCKS) {\r\ndo {\r\nfn(ctx, wdst, wsrc);\r\nwsrc += bsize * CAST5_PARALLEL_BLOCKS;\r\nwdst += bsize * CAST5_PARALLEL_BLOCKS;\r\nnbytes -= bsize * CAST5_PARALLEL_BLOCKS;\r\n} while (nbytes >= bsize * CAST5_PARALLEL_BLOCKS);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\nfn = (enc) ? __cast5_encrypt : __cast5_decrypt;\r\ndo {\r\nfn(ctx, wdst, wsrc);\r\nwsrc += bsize;\r\nwdst += bsize;\r\nnbytes -= bsize;\r\n} while (nbytes >= bsize);\r\ndone:\r\nerr = blkcipher_walk_done(desc, walk, nbytes);\r\n}\r\ncast5_fpu_end(fpu_enabled);\r\nreturn err;\r\n}\r\nstatic int ecb_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct blkcipher_walk walk;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ecb_crypt(desc, &walk, true);\r\n}\r\nstatic int ecb_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct blkcipher_walk walk;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ecb_crypt(desc, &walk, false);\r\n}\r\nstatic unsigned int __cbc_encrypt(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct cast5_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = CAST5_BLOCK_SIZE;\r\nunsigned int nbytes = walk->nbytes;\r\nu64 *src = (u64 *)walk->src.virt.addr;\r\nu64 *dst = (u64 *)walk->dst.virt.addr;\r\nu64 *iv = (u64 *)walk->iv;\r\ndo {\r\n*dst = *src ^ *iv;\r\n__cast5_encrypt(ctx, (u8 *)dst, (u8 *)dst);\r\niv = dst;\r\nsrc += 1;\r\ndst += 1;\r\nnbytes -= bsize;\r\n} while (nbytes >= bsize);\r\n*(u64 *)walk->iv = *iv;\r\nreturn nbytes;\r\n}\r\nstatic int cbc_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nwhile ((nbytes = walk.nbytes)) {\r\nnbytes = __cbc_encrypt(desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nreturn err;\r\n}\r\nstatic unsigned int __cbc_decrypt(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct cast5_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = CAST5_BLOCK_SIZE;\r\nunsigned int nbytes = walk->nbytes;\r\nu64 *src = (u64 *)walk->src.virt.addr;\r\nu64 *dst = (u64 *)walk->dst.virt.addr;\r\nu64 last_iv;\r\nsrc += nbytes / bsize - 1;\r\ndst += nbytes / bsize - 1;\r\nlast_iv = *src;\r\nif (nbytes >= bsize * CAST5_PARALLEL_BLOCKS) {\r\ndo {\r\nnbytes -= bsize * (CAST5_PARALLEL_BLOCKS - 1);\r\nsrc -= CAST5_PARALLEL_BLOCKS - 1;\r\ndst -= CAST5_PARALLEL_BLOCKS - 1;\r\ncast5_cbc_dec_16way(ctx, (u8 *)dst, (u8 *)src);\r\nnbytes -= bsize;\r\nif (nbytes < bsize)\r\ngoto done;\r\n*dst ^= *(src - 1);\r\nsrc -= 1;\r\ndst -= 1;\r\n} while (nbytes >= bsize * CAST5_PARALLEL_BLOCKS);\r\n}\r\nfor (;;) {\r\n__cast5_decrypt(ctx, (u8 *)dst, (u8 *)src);\r\nnbytes -= bsize;\r\nif (nbytes < bsize)\r\nbreak;\r\n*dst ^= *(src - 1);\r\nsrc -= 1;\r\ndst -= 1;\r\n}\r\ndone:\r\n*dst ^= *(u64 *)walk->iv;\r\n*(u64 *)walk->iv = last_iv;\r\nreturn nbytes;\r\n}\r\nstatic int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nbool fpu_enabled = false;\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nwhile ((nbytes = walk.nbytes)) {\r\nfpu_enabled = cast5_fpu_begin(fpu_enabled, nbytes);\r\nnbytes = __cbc_decrypt(desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\ncast5_fpu_end(fpu_enabled);\r\nreturn err;\r\n}\r\nstatic void ctr_crypt_final(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct cast5_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nu8 *ctrblk = walk->iv;\r\nu8 keystream[CAST5_BLOCK_SIZE];\r\nu8 *src = walk->src.virt.addr;\r\nu8 *dst = walk->dst.virt.addr;\r\nunsigned int nbytes = walk->nbytes;\r\n__cast5_encrypt(ctx, keystream, ctrblk);\r\ncrypto_xor(keystream, src, nbytes);\r\nmemcpy(dst, keystream, nbytes);\r\ncrypto_inc(ctrblk, CAST5_BLOCK_SIZE);\r\n}\r\nstatic unsigned int __ctr_crypt(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct cast5_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = CAST5_BLOCK_SIZE;\r\nunsigned int nbytes = walk->nbytes;\r\nu64 *src = (u64 *)walk->src.virt.addr;\r\nu64 *dst = (u64 *)walk->dst.virt.addr;\r\nif (nbytes >= bsize * CAST5_PARALLEL_BLOCKS) {\r\ndo {\r\ncast5_ctr_16way(ctx, (u8 *)dst, (u8 *)src,\r\n(__be64 *)walk->iv);\r\nsrc += CAST5_PARALLEL_BLOCKS;\r\ndst += CAST5_PARALLEL_BLOCKS;\r\nnbytes -= bsize * CAST5_PARALLEL_BLOCKS;\r\n} while (nbytes >= bsize * CAST5_PARALLEL_BLOCKS);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\ndo {\r\nu64 ctrblk;\r\nif (dst != src)\r\n*dst = *src;\r\nctrblk = *(u64 *)walk->iv;\r\nbe64_add_cpu((__be64 *)walk->iv, 1);\r\n__cast5_encrypt(ctx, (u8 *)&ctrblk, (u8 *)&ctrblk);\r\n*dst ^= ctrblk;\r\nsrc += 1;\r\ndst += 1;\r\nnbytes -= bsize;\r\n} while (nbytes >= bsize);\r\ndone:\r\nreturn nbytes;\r\n}\r\nstatic int ctr_crypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nbool fpu_enabled = false;\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt_block(desc, &walk, CAST5_BLOCK_SIZE);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nwhile ((nbytes = walk.nbytes) >= CAST5_BLOCK_SIZE) {\r\nfpu_enabled = cast5_fpu_begin(fpu_enabled, nbytes);\r\nnbytes = __ctr_crypt(desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\ncast5_fpu_end(fpu_enabled);\r\nif (walk.nbytes) {\r\nctr_crypt_final(desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, 0);\r\n}\r\nreturn err;\r\n}\r\nstatic int __init cast5_init(void)\r\n{\r\nconst char *feature_name;\r\nif (!cpu_has_xfeatures(XSTATE_SSE | XSTATE_YMM, &feature_name)) {\r\npr_info("CPU feature '%s' is not supported.\n", feature_name);\r\nreturn -ENODEV;\r\n}\r\nreturn crypto_register_algs(cast5_algs, ARRAY_SIZE(cast5_algs));\r\n}\r\nstatic void __exit cast5_exit(void)\r\n{\r\ncrypto_unregister_algs(cast5_algs, ARRAY_SIZE(cast5_algs));\r\n}
