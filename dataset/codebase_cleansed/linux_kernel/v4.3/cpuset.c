static inline struct cpuset *css_cs(struct cgroup_subsys_state *css)\r\n{\r\nreturn css ? container_of(css, struct cpuset, css) : NULL;\r\n}\r\nstatic inline struct cpuset *task_cs(struct task_struct *task)\r\n{\r\nreturn css_cs(task_css(task, cpuset_cgrp_id));\r\n}\r\nstatic inline struct cpuset *parent_cs(struct cpuset *cs)\r\n{\r\nreturn css_cs(cs->css.parent);\r\n}\r\nstatic inline bool task_has_mempolicy(struct task_struct *task)\r\n{\r\nreturn task->mempolicy;\r\n}\r\nstatic inline bool task_has_mempolicy(struct task_struct *task)\r\n{\r\nreturn false;\r\n}\r\nstatic inline bool is_cpuset_online(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_ONLINE, &cs->flags);\r\n}\r\nstatic inline int is_cpu_exclusive(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_CPU_EXCLUSIVE, &cs->flags);\r\n}\r\nstatic inline int is_mem_exclusive(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_MEM_EXCLUSIVE, &cs->flags);\r\n}\r\nstatic inline int is_mem_hardwall(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_MEM_HARDWALL, &cs->flags);\r\n}\r\nstatic inline int is_sched_load_balance(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\r\n}\r\nstatic inline int is_memory_migrate(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_MEMORY_MIGRATE, &cs->flags);\r\n}\r\nstatic inline int is_spread_page(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_SPREAD_PAGE, &cs->flags);\r\n}\r\nstatic inline int is_spread_slab(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_SPREAD_SLAB, &cs->flags);\r\n}\r\nstatic struct dentry *cpuset_mount(struct file_system_type *fs_type,\r\nint flags, const char *unused_dev_name, void *data)\r\n{\r\nstruct file_system_type *cgroup_fs = get_fs_type("cgroup");\r\nstruct dentry *ret = ERR_PTR(-ENODEV);\r\nif (cgroup_fs) {\r\nchar mountopts[] =\r\n"cpuset,noprefix,"\r\n"release_agent=/sbin/cpuset_release_agent";\r\nret = cgroup_fs->mount(cgroup_fs, flags,\r\nunused_dev_name, mountopts);\r\nput_filesystem(cgroup_fs);\r\n}\r\nreturn ret;\r\n}\r\nstatic void guarantee_online_cpus(struct cpuset *cs, struct cpumask *pmask)\r\n{\r\nwhile (!cpumask_intersects(cs->effective_cpus, cpu_online_mask))\r\ncs = parent_cs(cs);\r\ncpumask_and(pmask, cs->effective_cpus, cpu_online_mask);\r\n}\r\nstatic void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\r\n{\r\nwhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\r\ncs = parent_cs(cs);\r\nnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\r\n}\r\nstatic void cpuset_update_task_spread_flag(struct cpuset *cs,\r\nstruct task_struct *tsk)\r\n{\r\nif (is_spread_page(cs))\r\ntask_set_spread_page(tsk);\r\nelse\r\ntask_clear_spread_page(tsk);\r\nif (is_spread_slab(cs))\r\ntask_set_spread_slab(tsk);\r\nelse\r\ntask_clear_spread_slab(tsk);\r\n}\r\nstatic int is_cpuset_subset(const struct cpuset *p, const struct cpuset *q)\r\n{\r\nreturn cpumask_subset(p->cpus_allowed, q->cpus_allowed) &&\r\nnodes_subset(p->mems_allowed, q->mems_allowed) &&\r\nis_cpu_exclusive(p) <= is_cpu_exclusive(q) &&\r\nis_mem_exclusive(p) <= is_mem_exclusive(q);\r\n}\r\nstatic struct cpuset *alloc_trial_cpuset(struct cpuset *cs)\r\n{\r\nstruct cpuset *trial;\r\ntrial = kmemdup(cs, sizeof(*cs), GFP_KERNEL);\r\nif (!trial)\r\nreturn NULL;\r\nif (!alloc_cpumask_var(&trial->cpus_allowed, GFP_KERNEL))\r\ngoto free_cs;\r\nif (!alloc_cpumask_var(&trial->effective_cpus, GFP_KERNEL))\r\ngoto free_cpus;\r\ncpumask_copy(trial->cpus_allowed, cs->cpus_allowed);\r\ncpumask_copy(trial->effective_cpus, cs->effective_cpus);\r\nreturn trial;\r\nfree_cpus:\r\nfree_cpumask_var(trial->cpus_allowed);\r\nfree_cs:\r\nkfree(trial);\r\nreturn NULL;\r\n}\r\nstatic void free_trial_cpuset(struct cpuset *trial)\r\n{\r\nfree_cpumask_var(trial->effective_cpus);\r\nfree_cpumask_var(trial->cpus_allowed);\r\nkfree(trial);\r\n}\r\nstatic int validate_change(struct cpuset *cur, struct cpuset *trial)\r\n{\r\nstruct cgroup_subsys_state *css;\r\nstruct cpuset *c, *par;\r\nint ret;\r\nrcu_read_lock();\r\nret = -EBUSY;\r\ncpuset_for_each_child(c, css, cur)\r\nif (!is_cpuset_subset(c, trial))\r\ngoto out;\r\nret = 0;\r\nif (cur == &top_cpuset)\r\ngoto out;\r\npar = parent_cs(cur);\r\nret = -EACCES;\r\nif (!cgroup_on_dfl(cur->css.cgroup) && !is_cpuset_subset(trial, par))\r\ngoto out;\r\nret = -EINVAL;\r\ncpuset_for_each_child(c, css, par) {\r\nif ((is_cpu_exclusive(trial) || is_cpu_exclusive(c)) &&\r\nc != cur &&\r\ncpumask_intersects(trial->cpus_allowed, c->cpus_allowed))\r\ngoto out;\r\nif ((is_mem_exclusive(trial) || is_mem_exclusive(c)) &&\r\nc != cur &&\r\nnodes_intersects(trial->mems_allowed, c->mems_allowed))\r\ngoto out;\r\n}\r\nret = -ENOSPC;\r\nif ((cgroup_has_tasks(cur->css.cgroup) || cur->attach_in_progress)) {\r\nif (!cpumask_empty(cur->cpus_allowed) &&\r\ncpumask_empty(trial->cpus_allowed))\r\ngoto out;\r\nif (!nodes_empty(cur->mems_allowed) &&\r\nnodes_empty(trial->mems_allowed))\r\ngoto out;\r\n}\r\nret = -EBUSY;\r\nif (is_cpu_exclusive(cur) &&\r\n!cpuset_cpumask_can_shrink(cur->cpus_allowed,\r\ntrial->cpus_allowed))\r\ngoto out;\r\nret = 0;\r\nout:\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int cpusets_overlap(struct cpuset *a, struct cpuset *b)\r\n{\r\nreturn cpumask_intersects(a->effective_cpus, b->effective_cpus);\r\n}\r\nstatic void\r\nupdate_domain_attr(struct sched_domain_attr *dattr, struct cpuset *c)\r\n{\r\nif (dattr->relax_domain_level < c->relax_domain_level)\r\ndattr->relax_domain_level = c->relax_domain_level;\r\nreturn;\r\n}\r\nstatic void update_domain_attr_tree(struct sched_domain_attr *dattr,\r\nstruct cpuset *root_cs)\r\n{\r\nstruct cpuset *cp;\r\nstruct cgroup_subsys_state *pos_css;\r\nrcu_read_lock();\r\ncpuset_for_each_descendant_pre(cp, pos_css, root_cs) {\r\nif (cpumask_empty(cp->cpus_allowed)) {\r\npos_css = css_rightmost_descendant(pos_css);\r\ncontinue;\r\n}\r\nif (is_sched_load_balance(cp))\r\nupdate_domain_attr(dattr, cp);\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic int generate_sched_domains(cpumask_var_t **domains,\r\nstruct sched_domain_attr **attributes)\r\n{\r\nstruct cpuset *cp;\r\nstruct cpuset **csa;\r\nint csn;\r\nint i, j, k;\r\ncpumask_var_t *doms;\r\ncpumask_var_t non_isolated_cpus;\r\nstruct sched_domain_attr *dattr;\r\nint ndoms = 0;\r\nint nslot;\r\nstruct cgroup_subsys_state *pos_css;\r\ndoms = NULL;\r\ndattr = NULL;\r\ncsa = NULL;\r\nif (!alloc_cpumask_var(&non_isolated_cpus, GFP_KERNEL))\r\ngoto done;\r\ncpumask_andnot(non_isolated_cpus, cpu_possible_mask, cpu_isolated_map);\r\nif (is_sched_load_balance(&top_cpuset)) {\r\nndoms = 1;\r\ndoms = alloc_sched_domains(ndoms);\r\nif (!doms)\r\ngoto done;\r\ndattr = kmalloc(sizeof(struct sched_domain_attr), GFP_KERNEL);\r\nif (dattr) {\r\n*dattr = SD_ATTR_INIT;\r\nupdate_domain_attr_tree(dattr, &top_cpuset);\r\n}\r\ncpumask_and(doms[0], top_cpuset.effective_cpus,\r\nnon_isolated_cpus);\r\ngoto done;\r\n}\r\ncsa = kmalloc(nr_cpusets() * sizeof(cp), GFP_KERNEL);\r\nif (!csa)\r\ngoto done;\r\ncsn = 0;\r\nrcu_read_lock();\r\ncpuset_for_each_descendant_pre(cp, pos_css, &top_cpuset) {\r\nif (cp == &top_cpuset)\r\ncontinue;\r\nif (!cpumask_empty(cp->cpus_allowed) &&\r\n!(is_sched_load_balance(cp) &&\r\ncpumask_intersects(cp->cpus_allowed, non_isolated_cpus)))\r\ncontinue;\r\nif (is_sched_load_balance(cp))\r\ncsa[csn++] = cp;\r\npos_css = css_rightmost_descendant(pos_css);\r\n}\r\nrcu_read_unlock();\r\nfor (i = 0; i < csn; i++)\r\ncsa[i]->pn = i;\r\nndoms = csn;\r\nrestart:\r\nfor (i = 0; i < csn; i++) {\r\nstruct cpuset *a = csa[i];\r\nint apn = a->pn;\r\nfor (j = 0; j < csn; j++) {\r\nstruct cpuset *b = csa[j];\r\nint bpn = b->pn;\r\nif (apn != bpn && cpusets_overlap(a, b)) {\r\nfor (k = 0; k < csn; k++) {\r\nstruct cpuset *c = csa[k];\r\nif (c->pn == bpn)\r\nc->pn = apn;\r\n}\r\nndoms--;\r\ngoto restart;\r\n}\r\n}\r\n}\r\ndoms = alloc_sched_domains(ndoms);\r\nif (!doms)\r\ngoto done;\r\ndattr = kmalloc(ndoms * sizeof(struct sched_domain_attr), GFP_KERNEL);\r\nfor (nslot = 0, i = 0; i < csn; i++) {\r\nstruct cpuset *a = csa[i];\r\nstruct cpumask *dp;\r\nint apn = a->pn;\r\nif (apn < 0) {\r\ncontinue;\r\n}\r\ndp = doms[nslot];\r\nif (nslot == ndoms) {\r\nstatic int warnings = 10;\r\nif (warnings) {\r\npr_warn("rebuild_sched_domains confused: nslot %d, ndoms %d, csn %d, i %d, apn %d\n",\r\nnslot, ndoms, csn, i, apn);\r\nwarnings--;\r\n}\r\ncontinue;\r\n}\r\ncpumask_clear(dp);\r\nif (dattr)\r\n*(dattr + nslot) = SD_ATTR_INIT;\r\nfor (j = i; j < csn; j++) {\r\nstruct cpuset *b = csa[j];\r\nif (apn == b->pn) {\r\ncpumask_or(dp, dp, b->effective_cpus);\r\ncpumask_and(dp, dp, non_isolated_cpus);\r\nif (dattr)\r\nupdate_domain_attr_tree(dattr + nslot, b);\r\nb->pn = -1;\r\n}\r\n}\r\nnslot++;\r\n}\r\nBUG_ON(nslot != ndoms);\r\ndone:\r\nfree_cpumask_var(non_isolated_cpus);\r\nkfree(csa);\r\nif (doms == NULL)\r\nndoms = 1;\r\n*domains = doms;\r\n*attributes = dattr;\r\nreturn ndoms;\r\n}\r\nstatic void rebuild_sched_domains_locked(void)\r\n{\r\nstruct sched_domain_attr *attr;\r\ncpumask_var_t *doms;\r\nint ndoms;\r\nlockdep_assert_held(&cpuset_mutex);\r\nget_online_cpus();\r\nif (!cpumask_equal(top_cpuset.effective_cpus, cpu_active_mask))\r\ngoto out;\r\nndoms = generate_sched_domains(&doms, &attr);\r\npartition_sched_domains(ndoms, doms, attr);\r\nout:\r\nput_online_cpus();\r\n}\r\nstatic void rebuild_sched_domains_locked(void)\r\n{\r\n}\r\nvoid rebuild_sched_domains(void)\r\n{\r\nmutex_lock(&cpuset_mutex);\r\nrebuild_sched_domains_locked();\r\nmutex_unlock(&cpuset_mutex);\r\n}\r\nstatic void update_tasks_cpumask(struct cpuset *cs)\r\n{\r\nstruct css_task_iter it;\r\nstruct task_struct *task;\r\ncss_task_iter_start(&cs->css, &it);\r\nwhile ((task = css_task_iter_next(&it)))\r\nset_cpus_allowed_ptr(task, cs->effective_cpus);\r\ncss_task_iter_end(&it);\r\n}\r\nstatic void update_cpumasks_hier(struct cpuset *cs, struct cpumask *new_cpus)\r\n{\r\nstruct cpuset *cp;\r\nstruct cgroup_subsys_state *pos_css;\r\nbool need_rebuild_sched_domains = false;\r\nrcu_read_lock();\r\ncpuset_for_each_descendant_pre(cp, pos_css, cs) {\r\nstruct cpuset *parent = parent_cs(cp);\r\ncpumask_and(new_cpus, cp->cpus_allowed, parent->effective_cpus);\r\nif (cgroup_on_dfl(cp->css.cgroup) && cpumask_empty(new_cpus))\r\ncpumask_copy(new_cpus, parent->effective_cpus);\r\nif (cpumask_equal(new_cpus, cp->effective_cpus)) {\r\npos_css = css_rightmost_descendant(pos_css);\r\ncontinue;\r\n}\r\nif (!css_tryget_online(&cp->css))\r\ncontinue;\r\nrcu_read_unlock();\r\nspin_lock_irq(&callback_lock);\r\ncpumask_copy(cp->effective_cpus, new_cpus);\r\nspin_unlock_irq(&callback_lock);\r\nWARN_ON(!cgroup_on_dfl(cp->css.cgroup) &&\r\n!cpumask_equal(cp->cpus_allowed, cp->effective_cpus));\r\nupdate_tasks_cpumask(cp);\r\nif (!cpumask_empty(cp->cpus_allowed) &&\r\nis_sched_load_balance(cp))\r\nneed_rebuild_sched_domains = true;\r\nrcu_read_lock();\r\ncss_put(&cp->css);\r\n}\r\nrcu_read_unlock();\r\nif (need_rebuild_sched_domains)\r\nrebuild_sched_domains_locked();\r\n}\r\nstatic int update_cpumask(struct cpuset *cs, struct cpuset *trialcs,\r\nconst char *buf)\r\n{\r\nint retval;\r\nif (cs == &top_cpuset)\r\nreturn -EACCES;\r\nif (!*buf) {\r\ncpumask_clear(trialcs->cpus_allowed);\r\n} else {\r\nretval = cpulist_parse(buf, trialcs->cpus_allowed);\r\nif (retval < 0)\r\nreturn retval;\r\nif (!cpumask_subset(trialcs->cpus_allowed,\r\ntop_cpuset.cpus_allowed))\r\nreturn -EINVAL;\r\n}\r\nif (cpumask_equal(cs->cpus_allowed, trialcs->cpus_allowed))\r\nreturn 0;\r\nretval = validate_change(cs, trialcs);\r\nif (retval < 0)\r\nreturn retval;\r\nspin_lock_irq(&callback_lock);\r\ncpumask_copy(cs->cpus_allowed, trialcs->cpus_allowed);\r\nspin_unlock_irq(&callback_lock);\r\nupdate_cpumasks_hier(cs, trialcs->cpus_allowed);\r\nreturn 0;\r\n}\r\nstatic void cpuset_migrate_mm(struct mm_struct *mm, const nodemask_t *from,\r\nconst nodemask_t *to)\r\n{\r\nstruct task_struct *tsk = current;\r\ntsk->mems_allowed = *to;\r\ndo_migrate_pages(mm, from, to, MPOL_MF_MOVE_ALL);\r\nrcu_read_lock();\r\nguarantee_online_mems(task_cs(tsk), &tsk->mems_allowed);\r\nrcu_read_unlock();\r\n}\r\nstatic void cpuset_change_task_nodemask(struct task_struct *tsk,\r\nnodemask_t *newmems)\r\n{\r\nbool need_loop;\r\nif (unlikely(test_thread_flag(TIF_MEMDIE)))\r\nreturn;\r\nif (current->flags & PF_EXITING)\r\nreturn;\r\ntask_lock(tsk);\r\nneed_loop = task_has_mempolicy(tsk) ||\r\n!nodes_intersects(*newmems, tsk->mems_allowed);\r\nif (need_loop) {\r\nlocal_irq_disable();\r\nwrite_seqcount_begin(&tsk->mems_allowed_seq);\r\n}\r\nnodes_or(tsk->mems_allowed, tsk->mems_allowed, *newmems);\r\nmpol_rebind_task(tsk, newmems, MPOL_REBIND_STEP1);\r\nmpol_rebind_task(tsk, newmems, MPOL_REBIND_STEP2);\r\ntsk->mems_allowed = *newmems;\r\nif (need_loop) {\r\nwrite_seqcount_end(&tsk->mems_allowed_seq);\r\nlocal_irq_enable();\r\n}\r\ntask_unlock(tsk);\r\n}\r\nstatic void update_tasks_nodemask(struct cpuset *cs)\r\n{\r\nstatic nodemask_t newmems;\r\nstruct css_task_iter it;\r\nstruct task_struct *task;\r\ncpuset_being_rebound = cs;\r\nguarantee_online_mems(cs, &newmems);\r\ncss_task_iter_start(&cs->css, &it);\r\nwhile ((task = css_task_iter_next(&it))) {\r\nstruct mm_struct *mm;\r\nbool migrate;\r\ncpuset_change_task_nodemask(task, &newmems);\r\nmm = get_task_mm(task);\r\nif (!mm)\r\ncontinue;\r\nmigrate = is_memory_migrate(cs);\r\nmpol_rebind_mm(mm, &cs->mems_allowed);\r\nif (migrate)\r\ncpuset_migrate_mm(mm, &cs->old_mems_allowed, &newmems);\r\nmmput(mm);\r\n}\r\ncss_task_iter_end(&it);\r\ncs->old_mems_allowed = newmems;\r\ncpuset_being_rebound = NULL;\r\n}\r\nstatic void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)\r\n{\r\nstruct cpuset *cp;\r\nstruct cgroup_subsys_state *pos_css;\r\nrcu_read_lock();\r\ncpuset_for_each_descendant_pre(cp, pos_css, cs) {\r\nstruct cpuset *parent = parent_cs(cp);\r\nnodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);\r\nif (cgroup_on_dfl(cp->css.cgroup) && nodes_empty(*new_mems))\r\n*new_mems = parent->effective_mems;\r\nif (nodes_equal(*new_mems, cp->effective_mems)) {\r\npos_css = css_rightmost_descendant(pos_css);\r\ncontinue;\r\n}\r\nif (!css_tryget_online(&cp->css))\r\ncontinue;\r\nrcu_read_unlock();\r\nspin_lock_irq(&callback_lock);\r\ncp->effective_mems = *new_mems;\r\nspin_unlock_irq(&callback_lock);\r\nWARN_ON(!cgroup_on_dfl(cp->css.cgroup) &&\r\n!nodes_equal(cp->mems_allowed, cp->effective_mems));\r\nupdate_tasks_nodemask(cp);\r\nrcu_read_lock();\r\ncss_put(&cp->css);\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,\r\nconst char *buf)\r\n{\r\nint retval;\r\nif (cs == &top_cpuset) {\r\nretval = -EACCES;\r\ngoto done;\r\n}\r\nif (!*buf) {\r\nnodes_clear(trialcs->mems_allowed);\r\n} else {\r\nretval = nodelist_parse(buf, trialcs->mems_allowed);\r\nif (retval < 0)\r\ngoto done;\r\nif (!nodes_subset(trialcs->mems_allowed,\r\ntop_cpuset.mems_allowed)) {\r\nretval = -EINVAL;\r\ngoto done;\r\n}\r\n}\r\nif (nodes_equal(cs->mems_allowed, trialcs->mems_allowed)) {\r\nretval = 0;\r\ngoto done;\r\n}\r\nretval = validate_change(cs, trialcs);\r\nif (retval < 0)\r\ngoto done;\r\nspin_lock_irq(&callback_lock);\r\ncs->mems_allowed = trialcs->mems_allowed;\r\nspin_unlock_irq(&callback_lock);\r\nupdate_nodemasks_hier(cs, &trialcs->mems_allowed);\r\ndone:\r\nreturn retval;\r\n}\r\nint current_cpuset_is_being_rebound(void)\r\n{\r\nint ret;\r\nrcu_read_lock();\r\nret = task_cs(current) == cpuset_being_rebound;\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int update_relax_domain_level(struct cpuset *cs, s64 val)\r\n{\r\n#ifdef CONFIG_SMP\r\nif (val < -1 || val >= sched_domain_level_max)\r\nreturn -EINVAL;\r\n#endif\r\nif (val != cs->relax_domain_level) {\r\ncs->relax_domain_level = val;\r\nif (!cpumask_empty(cs->cpus_allowed) &&\r\nis_sched_load_balance(cs))\r\nrebuild_sched_domains_locked();\r\n}\r\nreturn 0;\r\n}\r\nstatic void update_tasks_flags(struct cpuset *cs)\r\n{\r\nstruct css_task_iter it;\r\nstruct task_struct *task;\r\ncss_task_iter_start(&cs->css, &it);\r\nwhile ((task = css_task_iter_next(&it)))\r\ncpuset_update_task_spread_flag(cs, task);\r\ncss_task_iter_end(&it);\r\n}\r\nstatic int update_flag(cpuset_flagbits_t bit, struct cpuset *cs,\r\nint turning_on)\r\n{\r\nstruct cpuset *trialcs;\r\nint balance_flag_changed;\r\nint spread_flag_changed;\r\nint err;\r\ntrialcs = alloc_trial_cpuset(cs);\r\nif (!trialcs)\r\nreturn -ENOMEM;\r\nif (turning_on)\r\nset_bit(bit, &trialcs->flags);\r\nelse\r\nclear_bit(bit, &trialcs->flags);\r\nerr = validate_change(cs, trialcs);\r\nif (err < 0)\r\ngoto out;\r\nbalance_flag_changed = (is_sched_load_balance(cs) !=\r\nis_sched_load_balance(trialcs));\r\nspread_flag_changed = ((is_spread_slab(cs) != is_spread_slab(trialcs))\r\n|| (is_spread_page(cs) != is_spread_page(trialcs)));\r\nspin_lock_irq(&callback_lock);\r\ncs->flags = trialcs->flags;\r\nspin_unlock_irq(&callback_lock);\r\nif (!cpumask_empty(trialcs->cpus_allowed) && balance_flag_changed)\r\nrebuild_sched_domains_locked();\r\nif (spread_flag_changed)\r\nupdate_tasks_flags(cs);\r\nout:\r\nfree_trial_cpuset(trialcs);\r\nreturn err;\r\n}\r\nstatic void fmeter_init(struct fmeter *fmp)\r\n{\r\nfmp->cnt = 0;\r\nfmp->val = 0;\r\nfmp->time = 0;\r\nspin_lock_init(&fmp->lock);\r\n}\r\nstatic void fmeter_update(struct fmeter *fmp)\r\n{\r\ntime_t now = get_seconds();\r\ntime_t ticks = now - fmp->time;\r\nif (ticks == 0)\r\nreturn;\r\nticks = min(FM_MAXTICKS, ticks);\r\nwhile (ticks-- > 0)\r\nfmp->val = (FM_COEF * fmp->val) / FM_SCALE;\r\nfmp->time = now;\r\nfmp->val += ((FM_SCALE - FM_COEF) * fmp->cnt) / FM_SCALE;\r\nfmp->cnt = 0;\r\n}\r\nstatic void fmeter_markevent(struct fmeter *fmp)\r\n{\r\nspin_lock(&fmp->lock);\r\nfmeter_update(fmp);\r\nfmp->cnt = min(FM_MAXCNT, fmp->cnt + FM_SCALE);\r\nspin_unlock(&fmp->lock);\r\n}\r\nstatic int fmeter_getrate(struct fmeter *fmp)\r\n{\r\nint val;\r\nspin_lock(&fmp->lock);\r\nfmeter_update(fmp);\r\nval = fmp->val;\r\nspin_unlock(&fmp->lock);\r\nreturn val;\r\n}\r\nstatic int cpuset_can_attach(struct cgroup_subsys_state *css,\r\nstruct cgroup_taskset *tset)\r\n{\r\nstruct cpuset *cs = css_cs(css);\r\nstruct task_struct *task;\r\nint ret;\r\ncpuset_attach_old_cs = task_cs(cgroup_taskset_first(tset));\r\nmutex_lock(&cpuset_mutex);\r\nret = -ENOSPC;\r\nif (!cgroup_on_dfl(css->cgroup) &&\r\n(cpumask_empty(cs->cpus_allowed) || nodes_empty(cs->mems_allowed)))\r\ngoto out_unlock;\r\ncgroup_taskset_for_each(task, tset) {\r\nret = task_can_attach(task, cs->cpus_allowed);\r\nif (ret)\r\ngoto out_unlock;\r\nret = security_task_setscheduler(task);\r\nif (ret)\r\ngoto out_unlock;\r\n}\r\ncs->attach_in_progress++;\r\nret = 0;\r\nout_unlock:\r\nmutex_unlock(&cpuset_mutex);\r\nreturn ret;\r\n}\r\nstatic void cpuset_cancel_attach(struct cgroup_subsys_state *css,\r\nstruct cgroup_taskset *tset)\r\n{\r\nmutex_lock(&cpuset_mutex);\r\ncss_cs(css)->attach_in_progress--;\r\nmutex_unlock(&cpuset_mutex);\r\n}\r\nstatic void cpuset_attach(struct cgroup_subsys_state *css,\r\nstruct cgroup_taskset *tset)\r\n{\r\nstatic nodemask_t cpuset_attach_nodemask_to;\r\nstruct mm_struct *mm;\r\nstruct task_struct *task;\r\nstruct task_struct *leader = cgroup_taskset_first(tset);\r\nstruct cpuset *cs = css_cs(css);\r\nstruct cpuset *oldcs = cpuset_attach_old_cs;\r\nmutex_lock(&cpuset_mutex);\r\nif (cs == &top_cpuset)\r\ncpumask_copy(cpus_attach, cpu_possible_mask);\r\nelse\r\nguarantee_online_cpus(cs, cpus_attach);\r\nguarantee_online_mems(cs, &cpuset_attach_nodemask_to);\r\ncgroup_taskset_for_each(task, tset) {\r\nWARN_ON_ONCE(set_cpus_allowed_ptr(task, cpus_attach));\r\ncpuset_change_task_nodemask(task, &cpuset_attach_nodemask_to);\r\ncpuset_update_task_spread_flag(cs, task);\r\n}\r\ncpuset_attach_nodemask_to = cs->effective_mems;\r\nmm = get_task_mm(leader);\r\nif (mm) {\r\nmpol_rebind_mm(mm, &cpuset_attach_nodemask_to);\r\nif (is_memory_migrate(cs)) {\r\ncpuset_migrate_mm(mm, &oldcs->old_mems_allowed,\r\n&cpuset_attach_nodemask_to);\r\n}\r\nmmput(mm);\r\n}\r\ncs->old_mems_allowed = cpuset_attach_nodemask_to;\r\ncs->attach_in_progress--;\r\nif (!cs->attach_in_progress)\r\nwake_up(&cpuset_attach_wq);\r\nmutex_unlock(&cpuset_mutex);\r\n}\r\nstatic int cpuset_write_u64(struct cgroup_subsys_state *css, struct cftype *cft,\r\nu64 val)\r\n{\r\nstruct cpuset *cs = css_cs(css);\r\ncpuset_filetype_t type = cft->private;\r\nint retval = 0;\r\nmutex_lock(&cpuset_mutex);\r\nif (!is_cpuset_online(cs)) {\r\nretval = -ENODEV;\r\ngoto out_unlock;\r\n}\r\nswitch (type) {\r\ncase FILE_CPU_EXCLUSIVE:\r\nretval = update_flag(CS_CPU_EXCLUSIVE, cs, val);\r\nbreak;\r\ncase FILE_MEM_EXCLUSIVE:\r\nretval = update_flag(CS_MEM_EXCLUSIVE, cs, val);\r\nbreak;\r\ncase FILE_MEM_HARDWALL:\r\nretval = update_flag(CS_MEM_HARDWALL, cs, val);\r\nbreak;\r\ncase FILE_SCHED_LOAD_BALANCE:\r\nretval = update_flag(CS_SCHED_LOAD_BALANCE, cs, val);\r\nbreak;\r\ncase FILE_MEMORY_MIGRATE:\r\nretval = update_flag(CS_MEMORY_MIGRATE, cs, val);\r\nbreak;\r\ncase FILE_MEMORY_PRESSURE_ENABLED:\r\ncpuset_memory_pressure_enabled = !!val;\r\nbreak;\r\ncase FILE_MEMORY_PRESSURE:\r\nretval = -EACCES;\r\nbreak;\r\ncase FILE_SPREAD_PAGE:\r\nretval = update_flag(CS_SPREAD_PAGE, cs, val);\r\nbreak;\r\ncase FILE_SPREAD_SLAB:\r\nretval = update_flag(CS_SPREAD_SLAB, cs, val);\r\nbreak;\r\ndefault:\r\nretval = -EINVAL;\r\nbreak;\r\n}\r\nout_unlock:\r\nmutex_unlock(&cpuset_mutex);\r\nreturn retval;\r\n}\r\nstatic int cpuset_write_s64(struct cgroup_subsys_state *css, struct cftype *cft,\r\ns64 val)\r\n{\r\nstruct cpuset *cs = css_cs(css);\r\ncpuset_filetype_t type = cft->private;\r\nint retval = -ENODEV;\r\nmutex_lock(&cpuset_mutex);\r\nif (!is_cpuset_online(cs))\r\ngoto out_unlock;\r\nswitch (type) {\r\ncase FILE_SCHED_RELAX_DOMAIN_LEVEL:\r\nretval = update_relax_domain_level(cs, val);\r\nbreak;\r\ndefault:\r\nretval = -EINVAL;\r\nbreak;\r\n}\r\nout_unlock:\r\nmutex_unlock(&cpuset_mutex);\r\nreturn retval;\r\n}\r\nstatic ssize_t cpuset_write_resmask(struct kernfs_open_file *of,\r\nchar *buf, size_t nbytes, loff_t off)\r\n{\r\nstruct cpuset *cs = css_cs(of_css(of));\r\nstruct cpuset *trialcs;\r\nint retval = -ENODEV;\r\nbuf = strstrip(buf);\r\ncss_get(&cs->css);\r\nkernfs_break_active_protection(of->kn);\r\nflush_work(&cpuset_hotplug_work);\r\nmutex_lock(&cpuset_mutex);\r\nif (!is_cpuset_online(cs))\r\ngoto out_unlock;\r\ntrialcs = alloc_trial_cpuset(cs);\r\nif (!trialcs) {\r\nretval = -ENOMEM;\r\ngoto out_unlock;\r\n}\r\nswitch (of_cft(of)->private) {\r\ncase FILE_CPULIST:\r\nretval = update_cpumask(cs, trialcs, buf);\r\nbreak;\r\ncase FILE_MEMLIST:\r\nretval = update_nodemask(cs, trialcs, buf);\r\nbreak;\r\ndefault:\r\nretval = -EINVAL;\r\nbreak;\r\n}\r\nfree_trial_cpuset(trialcs);\r\nout_unlock:\r\nmutex_unlock(&cpuset_mutex);\r\nkernfs_unbreak_active_protection(of->kn);\r\ncss_put(&cs->css);\r\nreturn retval ?: nbytes;\r\n}\r\nstatic int cpuset_common_seq_show(struct seq_file *sf, void *v)\r\n{\r\nstruct cpuset *cs = css_cs(seq_css(sf));\r\ncpuset_filetype_t type = seq_cft(sf)->private;\r\nint ret = 0;\r\nspin_lock_irq(&callback_lock);\r\nswitch (type) {\r\ncase FILE_CPULIST:\r\nseq_printf(sf, "%*pbl\n", cpumask_pr_args(cs->cpus_allowed));\r\nbreak;\r\ncase FILE_MEMLIST:\r\nseq_printf(sf, "%*pbl\n", nodemask_pr_args(&cs->mems_allowed));\r\nbreak;\r\ncase FILE_EFFECTIVE_CPULIST:\r\nseq_printf(sf, "%*pbl\n", cpumask_pr_args(cs->effective_cpus));\r\nbreak;\r\ncase FILE_EFFECTIVE_MEMLIST:\r\nseq_printf(sf, "%*pbl\n", nodemask_pr_args(&cs->effective_mems));\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\n}\r\nspin_unlock_irq(&callback_lock);\r\nreturn ret;\r\n}\r\nstatic u64 cpuset_read_u64(struct cgroup_subsys_state *css, struct cftype *cft)\r\n{\r\nstruct cpuset *cs = css_cs(css);\r\ncpuset_filetype_t type = cft->private;\r\nswitch (type) {\r\ncase FILE_CPU_EXCLUSIVE:\r\nreturn is_cpu_exclusive(cs);\r\ncase FILE_MEM_EXCLUSIVE:\r\nreturn is_mem_exclusive(cs);\r\ncase FILE_MEM_HARDWALL:\r\nreturn is_mem_hardwall(cs);\r\ncase FILE_SCHED_LOAD_BALANCE:\r\nreturn is_sched_load_balance(cs);\r\ncase FILE_MEMORY_MIGRATE:\r\nreturn is_memory_migrate(cs);\r\ncase FILE_MEMORY_PRESSURE_ENABLED:\r\nreturn cpuset_memory_pressure_enabled;\r\ncase FILE_MEMORY_PRESSURE:\r\nreturn fmeter_getrate(&cs->fmeter);\r\ncase FILE_SPREAD_PAGE:\r\nreturn is_spread_page(cs);\r\ncase FILE_SPREAD_SLAB:\r\nreturn is_spread_slab(cs);\r\ndefault:\r\nBUG();\r\n}\r\nreturn 0;\r\n}\r\nstatic s64 cpuset_read_s64(struct cgroup_subsys_state *css, struct cftype *cft)\r\n{\r\nstruct cpuset *cs = css_cs(css);\r\ncpuset_filetype_t type = cft->private;\r\nswitch (type) {\r\ncase FILE_SCHED_RELAX_DOMAIN_LEVEL:\r\nreturn cs->relax_domain_level;\r\ndefault:\r\nBUG();\r\n}\r\nreturn 0;\r\n}\r\nstatic struct cgroup_subsys_state *\r\ncpuset_css_alloc(struct cgroup_subsys_state *parent_css)\r\n{\r\nstruct cpuset *cs;\r\nif (!parent_css)\r\nreturn &top_cpuset.css;\r\ncs = kzalloc(sizeof(*cs), GFP_KERNEL);\r\nif (!cs)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (!alloc_cpumask_var(&cs->cpus_allowed, GFP_KERNEL))\r\ngoto free_cs;\r\nif (!alloc_cpumask_var(&cs->effective_cpus, GFP_KERNEL))\r\ngoto free_cpus;\r\nset_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\r\ncpumask_clear(cs->cpus_allowed);\r\nnodes_clear(cs->mems_allowed);\r\ncpumask_clear(cs->effective_cpus);\r\nnodes_clear(cs->effective_mems);\r\nfmeter_init(&cs->fmeter);\r\ncs->relax_domain_level = -1;\r\nreturn &cs->css;\r\nfree_cpus:\r\nfree_cpumask_var(cs->cpus_allowed);\r\nfree_cs:\r\nkfree(cs);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nstatic int cpuset_css_online(struct cgroup_subsys_state *css)\r\n{\r\nstruct cpuset *cs = css_cs(css);\r\nstruct cpuset *parent = parent_cs(cs);\r\nstruct cpuset *tmp_cs;\r\nstruct cgroup_subsys_state *pos_css;\r\nif (!parent)\r\nreturn 0;\r\nmutex_lock(&cpuset_mutex);\r\nset_bit(CS_ONLINE, &cs->flags);\r\nif (is_spread_page(parent))\r\nset_bit(CS_SPREAD_PAGE, &cs->flags);\r\nif (is_spread_slab(parent))\r\nset_bit(CS_SPREAD_SLAB, &cs->flags);\r\ncpuset_inc();\r\nspin_lock_irq(&callback_lock);\r\nif (cgroup_on_dfl(cs->css.cgroup)) {\r\ncpumask_copy(cs->effective_cpus, parent->effective_cpus);\r\ncs->effective_mems = parent->effective_mems;\r\n}\r\nspin_unlock_irq(&callback_lock);\r\nif (!test_bit(CGRP_CPUSET_CLONE_CHILDREN, &css->cgroup->flags))\r\ngoto out_unlock;\r\nrcu_read_lock();\r\ncpuset_for_each_child(tmp_cs, pos_css, parent) {\r\nif (is_mem_exclusive(tmp_cs) || is_cpu_exclusive(tmp_cs)) {\r\nrcu_read_unlock();\r\ngoto out_unlock;\r\n}\r\n}\r\nrcu_read_unlock();\r\nspin_lock_irq(&callback_lock);\r\ncs->mems_allowed = parent->mems_allowed;\r\ncs->effective_mems = parent->mems_allowed;\r\ncpumask_copy(cs->cpus_allowed, parent->cpus_allowed);\r\ncpumask_copy(cs->effective_cpus, parent->cpus_allowed);\r\nspin_unlock_irq(&callback_lock);\r\nout_unlock:\r\nmutex_unlock(&cpuset_mutex);\r\nreturn 0;\r\n}\r\nstatic void cpuset_css_offline(struct cgroup_subsys_state *css)\r\n{\r\nstruct cpuset *cs = css_cs(css);\r\nmutex_lock(&cpuset_mutex);\r\nif (is_sched_load_balance(cs))\r\nupdate_flag(CS_SCHED_LOAD_BALANCE, cs, 0);\r\ncpuset_dec();\r\nclear_bit(CS_ONLINE, &cs->flags);\r\nmutex_unlock(&cpuset_mutex);\r\n}\r\nstatic void cpuset_css_free(struct cgroup_subsys_state *css)\r\n{\r\nstruct cpuset *cs = css_cs(css);\r\nfree_cpumask_var(cs->effective_cpus);\r\nfree_cpumask_var(cs->cpus_allowed);\r\nkfree(cs);\r\n}\r\nstatic void cpuset_bind(struct cgroup_subsys_state *root_css)\r\n{\r\nmutex_lock(&cpuset_mutex);\r\nspin_lock_irq(&callback_lock);\r\nif (cgroup_on_dfl(root_css->cgroup)) {\r\ncpumask_copy(top_cpuset.cpus_allowed, cpu_possible_mask);\r\ntop_cpuset.mems_allowed = node_possible_map;\r\n} else {\r\ncpumask_copy(top_cpuset.cpus_allowed,\r\ntop_cpuset.effective_cpus);\r\ntop_cpuset.mems_allowed = top_cpuset.effective_mems;\r\n}\r\nspin_unlock_irq(&callback_lock);\r\nmutex_unlock(&cpuset_mutex);\r\n}\r\nint __init cpuset_init(void)\r\n{\r\nint err = 0;\r\nif (!alloc_cpumask_var(&top_cpuset.cpus_allowed, GFP_KERNEL))\r\nBUG();\r\nif (!alloc_cpumask_var(&top_cpuset.effective_cpus, GFP_KERNEL))\r\nBUG();\r\ncpumask_setall(top_cpuset.cpus_allowed);\r\nnodes_setall(top_cpuset.mems_allowed);\r\ncpumask_setall(top_cpuset.effective_cpus);\r\nnodes_setall(top_cpuset.effective_mems);\r\nfmeter_init(&top_cpuset.fmeter);\r\nset_bit(CS_SCHED_LOAD_BALANCE, &top_cpuset.flags);\r\ntop_cpuset.relax_domain_level = -1;\r\nerr = register_filesystem(&cpuset_fs_type);\r\nif (err < 0)\r\nreturn err;\r\nif (!alloc_cpumask_var(&cpus_attach, GFP_KERNEL))\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic void remove_tasks_in_empty_cpuset(struct cpuset *cs)\r\n{\r\nstruct cpuset *parent;\r\nparent = parent_cs(cs);\r\nwhile (cpumask_empty(parent->cpus_allowed) ||\r\nnodes_empty(parent->mems_allowed))\r\nparent = parent_cs(parent);\r\nif (cgroup_transfer_tasks(parent->css.cgroup, cs->css.cgroup)) {\r\npr_err("cpuset: failed to transfer tasks out of empty cpuset ");\r\npr_cont_cgroup_name(cs->css.cgroup);\r\npr_cont("\n");\r\n}\r\n}\r\nstatic void\r\nhotplug_update_tasks_legacy(struct cpuset *cs,\r\nstruct cpumask *new_cpus, nodemask_t *new_mems,\r\nbool cpus_updated, bool mems_updated)\r\n{\r\nbool is_empty;\r\nspin_lock_irq(&callback_lock);\r\ncpumask_copy(cs->cpus_allowed, new_cpus);\r\ncpumask_copy(cs->effective_cpus, new_cpus);\r\ncs->mems_allowed = *new_mems;\r\ncs->effective_mems = *new_mems;\r\nspin_unlock_irq(&callback_lock);\r\nif (cpus_updated && !cpumask_empty(cs->cpus_allowed))\r\nupdate_tasks_cpumask(cs);\r\nif (mems_updated && !nodes_empty(cs->mems_allowed))\r\nupdate_tasks_nodemask(cs);\r\nis_empty = cpumask_empty(cs->cpus_allowed) ||\r\nnodes_empty(cs->mems_allowed);\r\nmutex_unlock(&cpuset_mutex);\r\nif (is_empty)\r\nremove_tasks_in_empty_cpuset(cs);\r\nmutex_lock(&cpuset_mutex);\r\n}\r\nstatic void\r\nhotplug_update_tasks(struct cpuset *cs,\r\nstruct cpumask *new_cpus, nodemask_t *new_mems,\r\nbool cpus_updated, bool mems_updated)\r\n{\r\nif (cpumask_empty(new_cpus))\r\ncpumask_copy(new_cpus, parent_cs(cs)->effective_cpus);\r\nif (nodes_empty(*new_mems))\r\n*new_mems = parent_cs(cs)->effective_mems;\r\nspin_lock_irq(&callback_lock);\r\ncpumask_copy(cs->effective_cpus, new_cpus);\r\ncs->effective_mems = *new_mems;\r\nspin_unlock_irq(&callback_lock);\r\nif (cpus_updated)\r\nupdate_tasks_cpumask(cs);\r\nif (mems_updated)\r\nupdate_tasks_nodemask(cs);\r\n}\r\nstatic void cpuset_hotplug_update_tasks(struct cpuset *cs)\r\n{\r\nstatic cpumask_t new_cpus;\r\nstatic nodemask_t new_mems;\r\nbool cpus_updated;\r\nbool mems_updated;\r\nretry:\r\nwait_event(cpuset_attach_wq, cs->attach_in_progress == 0);\r\nmutex_lock(&cpuset_mutex);\r\nif (cs->attach_in_progress) {\r\nmutex_unlock(&cpuset_mutex);\r\ngoto retry;\r\n}\r\ncpumask_and(&new_cpus, cs->cpus_allowed, parent_cs(cs)->effective_cpus);\r\nnodes_and(new_mems, cs->mems_allowed, parent_cs(cs)->effective_mems);\r\ncpus_updated = !cpumask_equal(&new_cpus, cs->effective_cpus);\r\nmems_updated = !nodes_equal(new_mems, cs->effective_mems);\r\nif (cgroup_on_dfl(cs->css.cgroup))\r\nhotplug_update_tasks(cs, &new_cpus, &new_mems,\r\ncpus_updated, mems_updated);\r\nelse\r\nhotplug_update_tasks_legacy(cs, &new_cpus, &new_mems,\r\ncpus_updated, mems_updated);\r\nmutex_unlock(&cpuset_mutex);\r\n}\r\nstatic void cpuset_hotplug_workfn(struct work_struct *work)\r\n{\r\nstatic cpumask_t new_cpus;\r\nstatic nodemask_t new_mems;\r\nbool cpus_updated, mems_updated;\r\nbool on_dfl = cgroup_on_dfl(top_cpuset.css.cgroup);\r\nmutex_lock(&cpuset_mutex);\r\ncpumask_copy(&new_cpus, cpu_active_mask);\r\nnew_mems = node_states[N_MEMORY];\r\ncpus_updated = !cpumask_equal(top_cpuset.effective_cpus, &new_cpus);\r\nmems_updated = !nodes_equal(top_cpuset.effective_mems, new_mems);\r\nif (cpus_updated) {\r\nspin_lock_irq(&callback_lock);\r\nif (!on_dfl)\r\ncpumask_copy(top_cpuset.cpus_allowed, &new_cpus);\r\ncpumask_copy(top_cpuset.effective_cpus, &new_cpus);\r\nspin_unlock_irq(&callback_lock);\r\n}\r\nif (mems_updated) {\r\nspin_lock_irq(&callback_lock);\r\nif (!on_dfl)\r\ntop_cpuset.mems_allowed = new_mems;\r\ntop_cpuset.effective_mems = new_mems;\r\nspin_unlock_irq(&callback_lock);\r\nupdate_tasks_nodemask(&top_cpuset);\r\n}\r\nmutex_unlock(&cpuset_mutex);\r\nif (cpus_updated || mems_updated) {\r\nstruct cpuset *cs;\r\nstruct cgroup_subsys_state *pos_css;\r\nrcu_read_lock();\r\ncpuset_for_each_descendant_pre(cs, pos_css, &top_cpuset) {\r\nif (cs == &top_cpuset || !css_tryget_online(&cs->css))\r\ncontinue;\r\nrcu_read_unlock();\r\ncpuset_hotplug_update_tasks(cs);\r\nrcu_read_lock();\r\ncss_put(&cs->css);\r\n}\r\nrcu_read_unlock();\r\n}\r\nif (cpus_updated)\r\nrebuild_sched_domains();\r\n}\r\nvoid cpuset_update_active_cpus(bool cpu_online)\r\n{\r\npartition_sched_domains(1, NULL, NULL);\r\nschedule_work(&cpuset_hotplug_work);\r\n}\r\nstatic int cpuset_track_online_nodes(struct notifier_block *self,\r\nunsigned long action, void *arg)\r\n{\r\nschedule_work(&cpuset_hotplug_work);\r\nreturn NOTIFY_OK;\r\n}\r\nvoid __init cpuset_init_smp(void)\r\n{\r\ncpumask_copy(top_cpuset.cpus_allowed, cpu_active_mask);\r\ntop_cpuset.mems_allowed = node_states[N_MEMORY];\r\ntop_cpuset.old_mems_allowed = top_cpuset.mems_allowed;\r\ncpumask_copy(top_cpuset.effective_cpus, cpu_active_mask);\r\ntop_cpuset.effective_mems = node_states[N_MEMORY];\r\nregister_hotmemory_notifier(&cpuset_track_online_nodes_nb);\r\n}\r\nvoid cpuset_cpus_allowed(struct task_struct *tsk, struct cpumask *pmask)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&callback_lock, flags);\r\nrcu_read_lock();\r\nguarantee_online_cpus(task_cs(tsk), pmask);\r\nrcu_read_unlock();\r\nspin_unlock_irqrestore(&callback_lock, flags);\r\n}\r\nvoid cpuset_cpus_allowed_fallback(struct task_struct *tsk)\r\n{\r\nrcu_read_lock();\r\ndo_set_cpus_allowed(tsk, task_cs(tsk)->effective_cpus);\r\nrcu_read_unlock();\r\n}\r\nvoid __init cpuset_init_current_mems_allowed(void)\r\n{\r\nnodes_setall(current->mems_allowed);\r\n}\r\nnodemask_t cpuset_mems_allowed(struct task_struct *tsk)\r\n{\r\nnodemask_t mask;\r\nunsigned long flags;\r\nspin_lock_irqsave(&callback_lock, flags);\r\nrcu_read_lock();\r\nguarantee_online_mems(task_cs(tsk), &mask);\r\nrcu_read_unlock();\r\nspin_unlock_irqrestore(&callback_lock, flags);\r\nreturn mask;\r\n}\r\nint cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)\r\n{\r\nreturn nodes_intersects(*nodemask, current->mems_allowed);\r\n}\r\nstatic struct cpuset *nearest_hardwall_ancestor(struct cpuset *cs)\r\n{\r\nwhile (!(is_mem_exclusive(cs) || is_mem_hardwall(cs)) && parent_cs(cs))\r\ncs = parent_cs(cs);\r\nreturn cs;\r\n}\r\nint __cpuset_node_allowed(int node, gfp_t gfp_mask)\r\n{\r\nstruct cpuset *cs;\r\nint allowed;\r\nunsigned long flags;\r\nif (in_interrupt())\r\nreturn 1;\r\nif (node_isset(node, current->mems_allowed))\r\nreturn 1;\r\nif (unlikely(test_thread_flag(TIF_MEMDIE)))\r\nreturn 1;\r\nif (gfp_mask & __GFP_HARDWALL)\r\nreturn 0;\r\nif (current->flags & PF_EXITING)\r\nreturn 1;\r\nspin_lock_irqsave(&callback_lock, flags);\r\nrcu_read_lock();\r\ncs = nearest_hardwall_ancestor(task_cs(current));\r\nallowed = node_isset(node, cs->mems_allowed);\r\nrcu_read_unlock();\r\nspin_unlock_irqrestore(&callback_lock, flags);\r\nreturn allowed;\r\n}\r\nstatic int cpuset_spread_node(int *rotor)\r\n{\r\nint node;\r\nnode = next_node(*rotor, current->mems_allowed);\r\nif (node == MAX_NUMNODES)\r\nnode = first_node(current->mems_allowed);\r\n*rotor = node;\r\nreturn node;\r\n}\r\nint cpuset_mem_spread_node(void)\r\n{\r\nif (current->cpuset_mem_spread_rotor == NUMA_NO_NODE)\r\ncurrent->cpuset_mem_spread_rotor =\r\nnode_random(&current->mems_allowed);\r\nreturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\r\n}\r\nint cpuset_slab_spread_node(void)\r\n{\r\nif (current->cpuset_slab_spread_rotor == NUMA_NO_NODE)\r\ncurrent->cpuset_slab_spread_rotor =\r\nnode_random(&current->mems_allowed);\r\nreturn cpuset_spread_node(&current->cpuset_slab_spread_rotor);\r\n}\r\nint cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\r\nconst struct task_struct *tsk2)\r\n{\r\nreturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\r\n}\r\nvoid cpuset_print_task_mems_allowed(struct task_struct *tsk)\r\n{\r\nstruct cgroup *cgrp;\r\nrcu_read_lock();\r\ncgrp = task_cs(tsk)->css.cgroup;\r\npr_info("%s cpuset=", tsk->comm);\r\npr_cont_cgroup_name(cgrp);\r\npr_cont(" mems_allowed=%*pbl\n", nodemask_pr_args(&tsk->mems_allowed));\r\nrcu_read_unlock();\r\n}\r\nvoid __cpuset_memory_pressure_bump(void)\r\n{\r\nrcu_read_lock();\r\nfmeter_markevent(&task_cs(current)->fmeter);\r\nrcu_read_unlock();\r\n}\r\nint proc_cpuset_show(struct seq_file *m, struct pid_namespace *ns,\r\nstruct pid *pid, struct task_struct *tsk)\r\n{\r\nchar *buf, *p;\r\nstruct cgroup_subsys_state *css;\r\nint retval;\r\nretval = -ENOMEM;\r\nbuf = kmalloc(PATH_MAX, GFP_KERNEL);\r\nif (!buf)\r\ngoto out;\r\nretval = -ENAMETOOLONG;\r\nrcu_read_lock();\r\ncss = task_css(tsk, cpuset_cgrp_id);\r\np = cgroup_path(css->cgroup, buf, PATH_MAX);\r\nrcu_read_unlock();\r\nif (!p)\r\ngoto out_free;\r\nseq_puts(m, p);\r\nseq_putc(m, '\n');\r\nretval = 0;\r\nout_free:\r\nkfree(buf);\r\nout:\r\nreturn retval;\r\n}\r\nvoid cpuset_task_status_allowed(struct seq_file *m, struct task_struct *task)\r\n{\r\nseq_printf(m, "Mems_allowed:\t%*pb\n",\r\nnodemask_pr_args(&task->mems_allowed));\r\nseq_printf(m, "Mems_allowed_list:\t%*pbl\n",\r\nnodemask_pr_args(&task->mems_allowed));\r\n}
