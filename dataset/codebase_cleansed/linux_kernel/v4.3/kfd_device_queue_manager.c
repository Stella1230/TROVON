static inline\r\nenum KFD_MQD_TYPE get_mqd_type_from_queue_type(enum kfd_queue_type type)\r\n{\r\nif (type == KFD_QUEUE_TYPE_SDMA)\r\nreturn KFD_MQD_TYPE_SDMA;\r\nreturn KFD_MQD_TYPE_CP;\r\n}\r\nunsigned int get_first_pipe(struct device_queue_manager *dqm)\r\n{\r\nBUG_ON(!dqm || !dqm->dev);\r\nreturn dqm->dev->shared_resources.first_compute_pipe;\r\n}\r\nunsigned int get_pipes_num(struct device_queue_manager *dqm)\r\n{\r\nBUG_ON(!dqm || !dqm->dev);\r\nreturn dqm->dev->shared_resources.compute_pipe_count;\r\n}\r\nstatic inline unsigned int get_pipes_num_cpsch(void)\r\n{\r\nreturn PIPE_PER_ME_CP_SCHEDULING;\r\n}\r\nvoid program_sh_mem_settings(struct device_queue_manager *dqm,\r\nstruct qcm_process_device *qpd)\r\n{\r\nreturn dqm->dev->kfd2kgd->program_sh_mem_settings(\r\ndqm->dev->kgd, qpd->vmid,\r\nqpd->sh_mem_config,\r\nqpd->sh_mem_ape1_base,\r\nqpd->sh_mem_ape1_limit,\r\nqpd->sh_mem_bases);\r\n}\r\nstatic int allocate_vmid(struct device_queue_manager *dqm,\r\nstruct qcm_process_device *qpd,\r\nstruct queue *q)\r\n{\r\nint bit, allocated_vmid;\r\nif (dqm->vmid_bitmap == 0)\r\nreturn -ENOMEM;\r\nbit = find_first_bit((unsigned long *)&dqm->vmid_bitmap, CIK_VMID_NUM);\r\nclear_bit(bit, (unsigned long *)&dqm->vmid_bitmap);\r\nallocated_vmid = bit + KFD_VMID_START_OFFSET;\r\npr_debug("kfd: vmid allocation %d\n", allocated_vmid);\r\nqpd->vmid = allocated_vmid;\r\nq->properties.vmid = allocated_vmid;\r\nset_pasid_vmid_mapping(dqm, q->process->pasid, q->properties.vmid);\r\nprogram_sh_mem_settings(dqm, qpd);\r\nreturn 0;\r\n}\r\nstatic void deallocate_vmid(struct device_queue_manager *dqm,\r\nstruct qcm_process_device *qpd,\r\nstruct queue *q)\r\n{\r\nint bit = qpd->vmid - KFD_VMID_START_OFFSET;\r\nset_pasid_vmid_mapping(dqm, 0, qpd->vmid);\r\nset_bit(bit, (unsigned long *)&dqm->vmid_bitmap);\r\nqpd->vmid = 0;\r\nq->properties.vmid = 0;\r\n}\r\nstatic int create_queue_nocpsch(struct device_queue_manager *dqm,\r\nstruct queue *q,\r\nstruct qcm_process_device *qpd,\r\nint *allocated_vmid)\r\n{\r\nint retval;\r\nBUG_ON(!dqm || !q || !qpd || !allocated_vmid);\r\npr_debug("kfd: In func %s\n", __func__);\r\nprint_queue(q);\r\nmutex_lock(&dqm->lock);\r\nif (dqm->total_queue_count >= max_num_of_queues_per_device) {\r\npr_warn("amdkfd: Can't create new usermode queue because %d queues were already created\n",\r\ndqm->total_queue_count);\r\nmutex_unlock(&dqm->lock);\r\nreturn -EPERM;\r\n}\r\nif (list_empty(&qpd->queues_list)) {\r\nretval = allocate_vmid(dqm, qpd, q);\r\nif (retval != 0) {\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\n}\r\n*allocated_vmid = qpd->vmid;\r\nq->properties.vmid = qpd->vmid;\r\nif (q->properties.type == KFD_QUEUE_TYPE_COMPUTE)\r\nretval = create_compute_queue_nocpsch(dqm, q, qpd);\r\nif (q->properties.type == KFD_QUEUE_TYPE_SDMA)\r\nretval = create_sdma_queue_nocpsch(dqm, q, qpd);\r\nif (retval != 0) {\r\nif (list_empty(&qpd->queues_list)) {\r\ndeallocate_vmid(dqm, qpd, q);\r\n*allocated_vmid = 0;\r\n}\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\nlist_add(&q->list, &qpd->queues_list);\r\nif (q->properties.is_active)\r\ndqm->queue_count++;\r\nif (q->properties.type == KFD_QUEUE_TYPE_SDMA)\r\ndqm->sdma_queue_count++;\r\ndqm->total_queue_count++;\r\npr_debug("Total of %d queues are accountable so far\n",\r\ndqm->total_queue_count);\r\nmutex_unlock(&dqm->lock);\r\nreturn 0;\r\n}\r\nstatic int allocate_hqd(struct device_queue_manager *dqm, struct queue *q)\r\n{\r\nbool set;\r\nint pipe, bit, i;\r\nset = false;\r\nfor (pipe = dqm->next_pipe_to_allocate, i = 0; i < get_pipes_num(dqm);\r\npipe = ((pipe + 1) % get_pipes_num(dqm)), ++i) {\r\nif (dqm->allocated_queues[pipe] != 0) {\r\nbit = find_first_bit(\r\n(unsigned long *)&dqm->allocated_queues[pipe],\r\nQUEUES_PER_PIPE);\r\nclear_bit(bit,\r\n(unsigned long *)&dqm->allocated_queues[pipe]);\r\nq->pipe = pipe;\r\nq->queue = bit;\r\nset = true;\r\nbreak;\r\n}\r\n}\r\nif (set == false)\r\nreturn -EBUSY;\r\npr_debug("kfd: DQM %s hqd slot - pipe (%d) queue(%d)\n",\r\n__func__, q->pipe, q->queue);\r\ndqm->next_pipe_to_allocate = (pipe + 1) % get_pipes_num(dqm);\r\nreturn 0;\r\n}\r\nstatic inline void deallocate_hqd(struct device_queue_manager *dqm,\r\nstruct queue *q)\r\n{\r\nset_bit(q->queue, (unsigned long *)&dqm->allocated_queues[q->pipe]);\r\n}\r\nstatic int create_compute_queue_nocpsch(struct device_queue_manager *dqm,\r\nstruct queue *q,\r\nstruct qcm_process_device *qpd)\r\n{\r\nint retval;\r\nstruct mqd_manager *mqd;\r\nBUG_ON(!dqm || !q || !qpd);\r\nmqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_COMPUTE);\r\nif (mqd == NULL)\r\nreturn -ENOMEM;\r\nretval = allocate_hqd(dqm, q);\r\nif (retval != 0)\r\nreturn retval;\r\nretval = mqd->init_mqd(mqd, &q->mqd, &q->mqd_mem_obj,\r\n&q->gart_mqd_addr, &q->properties);\r\nif (retval != 0) {\r\ndeallocate_hqd(dqm, q);\r\nreturn retval;\r\n}\r\npr_debug("kfd: loading mqd to hqd on pipe (%d) queue (%d)\n",\r\nq->pipe,\r\nq->queue);\r\nretval = mqd->load_mqd(mqd, q->mqd, q->pipe,\r\nq->queue, (uint32_t __user *) q->properties.write_ptr);\r\nif (retval != 0) {\r\ndeallocate_hqd(dqm, q);\r\nmqd->uninit_mqd(mqd, q->mqd, q->mqd_mem_obj);\r\nreturn retval;\r\n}\r\nreturn 0;\r\n}\r\nstatic int destroy_queue_nocpsch(struct device_queue_manager *dqm,\r\nstruct qcm_process_device *qpd,\r\nstruct queue *q)\r\n{\r\nint retval;\r\nstruct mqd_manager *mqd;\r\nBUG_ON(!dqm || !q || !q->mqd || !qpd);\r\nretval = 0;\r\npr_debug("kfd: In Func %s\n", __func__);\r\nmutex_lock(&dqm->lock);\r\nif (q->properties.type == KFD_QUEUE_TYPE_COMPUTE) {\r\nmqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_COMPUTE);\r\nif (mqd == NULL) {\r\nretval = -ENOMEM;\r\ngoto out;\r\n}\r\ndeallocate_hqd(dqm, q);\r\n} else if (q->properties.type == KFD_QUEUE_TYPE_SDMA) {\r\nmqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_SDMA);\r\nif (mqd == NULL) {\r\nretval = -ENOMEM;\r\ngoto out;\r\n}\r\ndqm->sdma_queue_count--;\r\ndeallocate_sdma_queue(dqm, q->sdma_id);\r\n} else {\r\npr_debug("q->properties.type is invalid (%d)\n",\r\nq->properties.type);\r\nretval = -EINVAL;\r\ngoto out;\r\n}\r\nretval = mqd->destroy_mqd(mqd, q->mqd,\r\nKFD_PREEMPT_TYPE_WAVEFRONT_RESET,\r\nQUEUE_PREEMPT_DEFAULT_TIMEOUT_MS,\r\nq->pipe, q->queue);\r\nif (retval != 0)\r\ngoto out;\r\nmqd->uninit_mqd(mqd, q->mqd, q->mqd_mem_obj);\r\nlist_del(&q->list);\r\nif (list_empty(&qpd->queues_list))\r\ndeallocate_vmid(dqm, qpd, q);\r\nif (q->properties.is_active)\r\ndqm->queue_count--;\r\ndqm->total_queue_count--;\r\npr_debug("Total of %d queues are accountable so far\n",\r\ndqm->total_queue_count);\r\nout:\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\nstatic int update_queue(struct device_queue_manager *dqm, struct queue *q)\r\n{\r\nint retval;\r\nstruct mqd_manager *mqd;\r\nbool prev_active = false;\r\nBUG_ON(!dqm || !q || !q->mqd);\r\nmutex_lock(&dqm->lock);\r\nmqd = dqm->ops.get_mqd_manager(dqm,\r\nget_mqd_type_from_queue_type(q->properties.type));\r\nif (mqd == NULL) {\r\nmutex_unlock(&dqm->lock);\r\nreturn -ENOMEM;\r\n}\r\nif (q->properties.is_active == true)\r\nprev_active = true;\r\nretval = mqd->update_mqd(mqd, q->mqd, &q->properties);\r\nif ((q->properties.is_active == true) && (prev_active == false))\r\ndqm->queue_count++;\r\nelse if ((q->properties.is_active == false) && (prev_active == true))\r\ndqm->queue_count--;\r\nif (sched_policy != KFD_SCHED_POLICY_NO_HWS)\r\nretval = execute_queues_cpsch(dqm, false);\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\nstatic struct mqd_manager *get_mqd_manager_nocpsch(\r\nstruct device_queue_manager *dqm, enum KFD_MQD_TYPE type)\r\n{\r\nstruct mqd_manager *mqd;\r\nBUG_ON(!dqm || type >= KFD_MQD_TYPE_MAX);\r\npr_debug("kfd: In func %s mqd type %d\n", __func__, type);\r\nmqd = dqm->mqds[type];\r\nif (!mqd) {\r\nmqd = mqd_manager_init(type, dqm->dev);\r\nif (mqd == NULL)\r\npr_err("kfd: mqd manager is NULL");\r\ndqm->mqds[type] = mqd;\r\n}\r\nreturn mqd;\r\n}\r\nstatic int register_process_nocpsch(struct device_queue_manager *dqm,\r\nstruct qcm_process_device *qpd)\r\n{\r\nstruct device_process_node *n;\r\nint retval;\r\nBUG_ON(!dqm || !qpd);\r\npr_debug("kfd: In func %s\n", __func__);\r\nn = kzalloc(sizeof(struct device_process_node), GFP_KERNEL);\r\nif (!n)\r\nreturn -ENOMEM;\r\nn->qpd = qpd;\r\nmutex_lock(&dqm->lock);\r\nlist_add(&n->list, &dqm->queues);\r\nretval = dqm->ops_asic_specific.register_process(dqm, qpd);\r\ndqm->processes_count++;\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\nstatic int unregister_process_nocpsch(struct device_queue_manager *dqm,\r\nstruct qcm_process_device *qpd)\r\n{\r\nint retval;\r\nstruct device_process_node *cur, *next;\r\nBUG_ON(!dqm || !qpd);\r\npr_debug("In func %s\n", __func__);\r\npr_debug("qpd->queues_list is %s\n",\r\nlist_empty(&qpd->queues_list) ? "empty" : "not empty");\r\nretval = 0;\r\nmutex_lock(&dqm->lock);\r\nlist_for_each_entry_safe(cur, next, &dqm->queues, list) {\r\nif (qpd == cur->qpd) {\r\nlist_del(&cur->list);\r\nkfree(cur);\r\ndqm->processes_count--;\r\ngoto out;\r\n}\r\n}\r\nretval = 1;\r\nout:\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\nstatic int\r\nset_pasid_vmid_mapping(struct device_queue_manager *dqm, unsigned int pasid,\r\nunsigned int vmid)\r\n{\r\nuint32_t pasid_mapping;\r\npasid_mapping = (pasid == 0) ? 0 :\r\n(uint32_t)pasid |\r\nATC_VMID_PASID_MAPPING_VALID;\r\nreturn dqm->dev->kfd2kgd->set_pasid_vmid_mapping(\r\ndqm->dev->kgd, pasid_mapping,\r\nvmid);\r\n}\r\nint init_pipelines(struct device_queue_manager *dqm,\r\nunsigned int pipes_num, unsigned int first_pipe)\r\n{\r\nvoid *hpdptr;\r\nstruct mqd_manager *mqd;\r\nunsigned int i, err, inx;\r\nuint64_t pipe_hpd_addr;\r\nBUG_ON(!dqm || !dqm->dev);\r\npr_debug("kfd: In func %s\n", __func__);\r\nerr = kfd_gtt_sa_allocate(dqm->dev, CIK_HPD_EOP_BYTES * pipes_num,\r\n&dqm->pipeline_mem);\r\nif (err) {\r\npr_err("kfd: error allocate vidmem num pipes: %d\n",\r\npipes_num);\r\nreturn -ENOMEM;\r\n}\r\nhpdptr = dqm->pipeline_mem->cpu_ptr;\r\ndqm->pipelines_addr = dqm->pipeline_mem->gpu_addr;\r\nmemset(hpdptr, 0, CIK_HPD_EOP_BYTES * pipes_num);\r\nmqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_COMPUTE);\r\nif (mqd == NULL) {\r\nkfd_gtt_sa_free(dqm->dev, dqm->pipeline_mem);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < pipes_num; i++) {\r\ninx = i + first_pipe;\r\npipe_hpd_addr = dqm->pipelines_addr + i * CIK_HPD_EOP_BYTES;\r\npr_debug("kfd: pipeline address %llX\n", pipe_hpd_addr);\r\ndqm->dev->kfd2kgd->init_pipeline(dqm->dev->kgd, inx,\r\nCIK_HPD_EOP_BYTES_LOG2 - 3, pipe_hpd_addr);\r\n}\r\nreturn 0;\r\n}\r\nstatic void init_interrupts(struct device_queue_manager *dqm)\r\n{\r\nunsigned int i;\r\nBUG_ON(dqm == NULL);\r\nfor (i = 0 ; i < get_pipes_num(dqm) ; i++)\r\ndqm->dev->kfd2kgd->init_interrupts(dqm->dev->kgd,\r\ni + get_first_pipe(dqm));\r\n}\r\nstatic int init_scheduler(struct device_queue_manager *dqm)\r\n{\r\nint retval;\r\nBUG_ON(!dqm);\r\npr_debug("kfd: In %s\n", __func__);\r\nretval = init_pipelines(dqm, get_pipes_num(dqm), get_first_pipe(dqm));\r\nreturn retval;\r\n}\r\nstatic int initialize_nocpsch(struct device_queue_manager *dqm)\r\n{\r\nint i;\r\nBUG_ON(!dqm);\r\npr_debug("kfd: In func %s num of pipes: %d\n",\r\n__func__, get_pipes_num(dqm));\r\nmutex_init(&dqm->lock);\r\nINIT_LIST_HEAD(&dqm->queues);\r\ndqm->queue_count = dqm->next_pipe_to_allocate = 0;\r\ndqm->sdma_queue_count = 0;\r\ndqm->allocated_queues = kcalloc(get_pipes_num(dqm),\r\nsizeof(unsigned int), GFP_KERNEL);\r\nif (!dqm->allocated_queues) {\r\nmutex_destroy(&dqm->lock);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < get_pipes_num(dqm); i++)\r\ndqm->allocated_queues[i] = (1 << QUEUES_PER_PIPE) - 1;\r\ndqm->vmid_bitmap = (1 << VMID_PER_DEVICE) - 1;\r\ndqm->sdma_bitmap = (1 << CIK_SDMA_QUEUES) - 1;\r\ninit_scheduler(dqm);\r\nreturn 0;\r\n}\r\nstatic void uninitialize_nocpsch(struct device_queue_manager *dqm)\r\n{\r\nint i;\r\nBUG_ON(!dqm);\r\nBUG_ON(dqm->queue_count > 0 || dqm->processes_count > 0);\r\nkfree(dqm->allocated_queues);\r\nfor (i = 0 ; i < KFD_MQD_TYPE_MAX ; i++)\r\nkfree(dqm->mqds[i]);\r\nmutex_destroy(&dqm->lock);\r\nkfd_gtt_sa_free(dqm->dev, dqm->pipeline_mem);\r\n}\r\nstatic int start_nocpsch(struct device_queue_manager *dqm)\r\n{\r\ninit_interrupts(dqm);\r\nreturn 0;\r\n}\r\nstatic int stop_nocpsch(struct device_queue_manager *dqm)\r\n{\r\nreturn 0;\r\n}\r\nstatic int allocate_sdma_queue(struct device_queue_manager *dqm,\r\nunsigned int *sdma_queue_id)\r\n{\r\nint bit;\r\nif (dqm->sdma_bitmap == 0)\r\nreturn -ENOMEM;\r\nbit = find_first_bit((unsigned long *)&dqm->sdma_bitmap,\r\nCIK_SDMA_QUEUES);\r\nclear_bit(bit, (unsigned long *)&dqm->sdma_bitmap);\r\n*sdma_queue_id = bit;\r\nreturn 0;\r\n}\r\nstatic void deallocate_sdma_queue(struct device_queue_manager *dqm,\r\nunsigned int sdma_queue_id)\r\n{\r\nif (sdma_queue_id >= CIK_SDMA_QUEUES)\r\nreturn;\r\nset_bit(sdma_queue_id, (unsigned long *)&dqm->sdma_bitmap);\r\n}\r\nstatic int create_sdma_queue_nocpsch(struct device_queue_manager *dqm,\r\nstruct queue *q,\r\nstruct qcm_process_device *qpd)\r\n{\r\nstruct mqd_manager *mqd;\r\nint retval;\r\nmqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_SDMA);\r\nif (!mqd)\r\nreturn -ENOMEM;\r\nretval = allocate_sdma_queue(dqm, &q->sdma_id);\r\nif (retval != 0)\r\nreturn retval;\r\nq->properties.sdma_queue_id = q->sdma_id % CIK_SDMA_QUEUES_PER_ENGINE;\r\nq->properties.sdma_engine_id = q->sdma_id / CIK_SDMA_ENGINE_NUM;\r\npr_debug("kfd: sdma id is: %d\n", q->sdma_id);\r\npr_debug(" sdma queue id: %d\n", q->properties.sdma_queue_id);\r\npr_debug(" sdma engine id: %d\n", q->properties.sdma_engine_id);\r\ndqm->ops_asic_specific.init_sdma_vm(dqm, q, qpd);\r\nretval = mqd->init_mqd(mqd, &q->mqd, &q->mqd_mem_obj,\r\n&q->gart_mqd_addr, &q->properties);\r\nif (retval != 0) {\r\ndeallocate_sdma_queue(dqm, q->sdma_id);\r\nreturn retval;\r\n}\r\nretval = mqd->load_mqd(mqd, q->mqd, 0,\r\n0, NULL);\r\nif (retval != 0) {\r\ndeallocate_sdma_queue(dqm, q->sdma_id);\r\nmqd->uninit_mqd(mqd, q->mqd, q->mqd_mem_obj);\r\nreturn retval;\r\n}\r\nreturn 0;\r\n}\r\nstatic int set_sched_resources(struct device_queue_manager *dqm)\r\n{\r\nstruct scheduling_resources res;\r\nunsigned int queue_num, queue_mask;\r\nBUG_ON(!dqm);\r\npr_debug("kfd: In func %s\n", __func__);\r\nqueue_num = get_pipes_num_cpsch() * QUEUES_PER_PIPE;\r\nqueue_mask = (1 << queue_num) - 1;\r\nres.vmid_mask = (1 << VMID_PER_DEVICE) - 1;\r\nres.vmid_mask <<= KFD_VMID_START_OFFSET;\r\nres.queue_mask = queue_mask << (get_first_pipe(dqm) * QUEUES_PER_PIPE);\r\nres.gws_mask = res.oac_mask = res.gds_heap_base =\r\nres.gds_heap_size = 0;\r\npr_debug("kfd: scheduling resources:\n"\r\n" vmid mask: 0x%8X\n"\r\n" queue mask: 0x%8llX\n",\r\nres.vmid_mask, res.queue_mask);\r\nreturn pm_send_set_resources(&dqm->packets, &res);\r\n}\r\nstatic int initialize_cpsch(struct device_queue_manager *dqm)\r\n{\r\nint retval;\r\nBUG_ON(!dqm);\r\npr_debug("kfd: In func %s num of pipes: %d\n",\r\n__func__, get_pipes_num_cpsch());\r\nmutex_init(&dqm->lock);\r\nINIT_LIST_HEAD(&dqm->queues);\r\ndqm->queue_count = dqm->processes_count = 0;\r\ndqm->sdma_queue_count = 0;\r\ndqm->active_runlist = false;\r\nretval = dqm->ops_asic_specific.initialize(dqm);\r\nif (retval != 0)\r\ngoto fail_init_pipelines;\r\nreturn 0;\r\nfail_init_pipelines:\r\nmutex_destroy(&dqm->lock);\r\nreturn retval;\r\n}\r\nstatic int start_cpsch(struct device_queue_manager *dqm)\r\n{\r\nstruct device_process_node *node;\r\nint retval;\r\nBUG_ON(!dqm);\r\nretval = 0;\r\nretval = pm_init(&dqm->packets, dqm);\r\nif (retval != 0)\r\ngoto fail_packet_manager_init;\r\nretval = set_sched_resources(dqm);\r\nif (retval != 0)\r\ngoto fail_set_sched_resources;\r\npr_debug("kfd: allocating fence memory\n");\r\nretval = kfd_gtt_sa_allocate(dqm->dev, sizeof(*dqm->fence_addr),\r\n&dqm->fence_mem);\r\nif (retval != 0)\r\ngoto fail_allocate_vidmem;\r\ndqm->fence_addr = dqm->fence_mem->cpu_ptr;\r\ndqm->fence_gpu_addr = dqm->fence_mem->gpu_addr;\r\ninit_interrupts(dqm);\r\nlist_for_each_entry(node, &dqm->queues, list)\r\nif (node->qpd->pqm->process && dqm->dev)\r\nkfd_bind_process_to_device(dqm->dev,\r\nnode->qpd->pqm->process);\r\nexecute_queues_cpsch(dqm, true);\r\nreturn 0;\r\nfail_allocate_vidmem:\r\nfail_set_sched_resources:\r\npm_uninit(&dqm->packets);\r\nfail_packet_manager_init:\r\nreturn retval;\r\n}\r\nstatic int stop_cpsch(struct device_queue_manager *dqm)\r\n{\r\nstruct device_process_node *node;\r\nstruct kfd_process_device *pdd;\r\nBUG_ON(!dqm);\r\ndestroy_queues_cpsch(dqm, true, true);\r\nlist_for_each_entry(node, &dqm->queues, list) {\r\npdd = qpd_to_pdd(node->qpd);\r\npdd->bound = false;\r\n}\r\nkfd_gtt_sa_free(dqm->dev, dqm->fence_mem);\r\npm_uninit(&dqm->packets);\r\nreturn 0;\r\n}\r\nstatic int create_kernel_queue_cpsch(struct device_queue_manager *dqm,\r\nstruct kernel_queue *kq,\r\nstruct qcm_process_device *qpd)\r\n{\r\nBUG_ON(!dqm || !kq || !qpd);\r\npr_debug("kfd: In func %s\n", __func__);\r\nmutex_lock(&dqm->lock);\r\nif (dqm->total_queue_count >= max_num_of_queues_per_device) {\r\npr_warn("amdkfd: Can't create new kernel queue because %d queues were already created\n",\r\ndqm->total_queue_count);\r\nmutex_unlock(&dqm->lock);\r\nreturn -EPERM;\r\n}\r\ndqm->total_queue_count++;\r\npr_debug("Total of %d queues are accountable so far\n",\r\ndqm->total_queue_count);\r\nlist_add(&kq->list, &qpd->priv_queue_list);\r\ndqm->queue_count++;\r\nqpd->is_debug = true;\r\nexecute_queues_cpsch(dqm, false);\r\nmutex_unlock(&dqm->lock);\r\nreturn 0;\r\n}\r\nstatic void destroy_kernel_queue_cpsch(struct device_queue_manager *dqm,\r\nstruct kernel_queue *kq,\r\nstruct qcm_process_device *qpd)\r\n{\r\nBUG_ON(!dqm || !kq);\r\npr_debug("kfd: In %s\n", __func__);\r\nmutex_lock(&dqm->lock);\r\ndestroy_queues_cpsch(dqm, true, false);\r\nlist_del(&kq->list);\r\ndqm->queue_count--;\r\nqpd->is_debug = false;\r\nexecute_queues_cpsch(dqm, false);\r\ndqm->total_queue_count--;\r\npr_debug("Total of %d queues are accountable so far\n",\r\ndqm->total_queue_count);\r\nmutex_unlock(&dqm->lock);\r\n}\r\nstatic void select_sdma_engine_id(struct queue *q)\r\n{\r\nstatic int sdma_id;\r\nq->sdma_id = sdma_id;\r\nsdma_id = (sdma_id + 1) % 2;\r\n}\r\nstatic int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,\r\nstruct qcm_process_device *qpd, int *allocate_vmid)\r\n{\r\nint retval;\r\nstruct mqd_manager *mqd;\r\nBUG_ON(!dqm || !q || !qpd);\r\nretval = 0;\r\nif (allocate_vmid)\r\n*allocate_vmid = 0;\r\nmutex_lock(&dqm->lock);\r\nif (dqm->total_queue_count >= max_num_of_queues_per_device) {\r\npr_warn("amdkfd: Can't create new usermode queue because %d queues were already created\n",\r\ndqm->total_queue_count);\r\nretval = -EPERM;\r\ngoto out;\r\n}\r\nif (q->properties.type == KFD_QUEUE_TYPE_SDMA)\r\nselect_sdma_engine_id(q);\r\nmqd = dqm->ops.get_mqd_manager(dqm,\r\nget_mqd_type_from_queue_type(q->properties.type));\r\nif (mqd == NULL) {\r\nmutex_unlock(&dqm->lock);\r\nreturn -ENOMEM;\r\n}\r\ndqm->ops_asic_specific.init_sdma_vm(dqm, q, qpd);\r\nretval = mqd->init_mqd(mqd, &q->mqd, &q->mqd_mem_obj,\r\n&q->gart_mqd_addr, &q->properties);\r\nif (retval != 0)\r\ngoto out;\r\nlist_add(&q->list, &qpd->queues_list);\r\nif (q->properties.is_active) {\r\ndqm->queue_count++;\r\nretval = execute_queues_cpsch(dqm, false);\r\n}\r\nif (q->properties.type == KFD_QUEUE_TYPE_SDMA)\r\ndqm->sdma_queue_count++;\r\ndqm->total_queue_count++;\r\npr_debug("Total of %d queues are accountable so far\n",\r\ndqm->total_queue_count);\r\nout:\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\nint amdkfd_fence_wait_timeout(unsigned int *fence_addr,\r\nunsigned int fence_value,\r\nunsigned long timeout)\r\n{\r\nBUG_ON(!fence_addr);\r\ntimeout += jiffies;\r\nwhile (*fence_addr != fence_value) {\r\nif (time_after(jiffies, timeout)) {\r\npr_err("kfd: qcm fence wait loop timeout expired\n");\r\nreturn -ETIME;\r\n}\r\nschedule();\r\n}\r\nreturn 0;\r\n}\r\nstatic int destroy_sdma_queues(struct device_queue_manager *dqm,\r\nunsigned int sdma_engine)\r\n{\r\nreturn pm_send_unmap_queue(&dqm->packets, KFD_QUEUE_TYPE_SDMA,\r\nKFD_PREEMPT_TYPE_FILTER_DYNAMIC_QUEUES, 0, false,\r\nsdma_engine);\r\n}\r\nstatic int destroy_queues_cpsch(struct device_queue_manager *dqm,\r\nbool preempt_static_queues, bool lock)\r\n{\r\nint retval;\r\nenum kfd_preempt_type_filter preempt_type;\r\nstruct kfd_process_device *pdd;\r\nBUG_ON(!dqm);\r\nretval = 0;\r\nif (lock)\r\nmutex_lock(&dqm->lock);\r\nif (dqm->active_runlist == false)\r\ngoto out;\r\npr_debug("kfd: Before destroying queues, sdma queue count is : %u\n",\r\ndqm->sdma_queue_count);\r\nif (dqm->sdma_queue_count > 0) {\r\ndestroy_sdma_queues(dqm, 0);\r\ndestroy_sdma_queues(dqm, 1);\r\n}\r\npreempt_type = preempt_static_queues ?\r\nKFD_PREEMPT_TYPE_FILTER_ALL_QUEUES :\r\nKFD_PREEMPT_TYPE_FILTER_DYNAMIC_QUEUES;\r\nretval = pm_send_unmap_queue(&dqm->packets, KFD_QUEUE_TYPE_COMPUTE,\r\npreempt_type, 0, false, 0);\r\nif (retval != 0)\r\ngoto out;\r\n*dqm->fence_addr = KFD_FENCE_INIT;\r\npm_send_query_status(&dqm->packets, dqm->fence_gpu_addr,\r\nKFD_FENCE_COMPLETED);\r\nretval = amdkfd_fence_wait_timeout(dqm->fence_addr, KFD_FENCE_COMPLETED,\r\nQUEUE_PREEMPT_DEFAULT_TIMEOUT_MS);\r\nif (retval != 0) {\r\npdd = kfd_get_process_device_data(dqm->dev,\r\nkfd_get_process(current));\r\npdd->reset_wavefronts = true;\r\ngoto out;\r\n}\r\npm_release_ib(&dqm->packets);\r\ndqm->active_runlist = false;\r\nout:\r\nif (lock)\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\nstatic int execute_queues_cpsch(struct device_queue_manager *dqm, bool lock)\r\n{\r\nint retval;\r\nBUG_ON(!dqm);\r\nif (lock)\r\nmutex_lock(&dqm->lock);\r\nretval = destroy_queues_cpsch(dqm, false, false);\r\nif (retval != 0) {\r\npr_err("kfd: the cp might be in an unrecoverable state due to an unsuccessful queues preemption");\r\ngoto out;\r\n}\r\nif (dqm->queue_count <= 0 || dqm->processes_count <= 0) {\r\nretval = 0;\r\ngoto out;\r\n}\r\nif (dqm->active_runlist) {\r\nretval = 0;\r\ngoto out;\r\n}\r\nretval = pm_send_runlist(&dqm->packets, &dqm->queues);\r\nif (retval != 0) {\r\npr_err("kfd: failed to execute runlist");\r\ngoto out;\r\n}\r\ndqm->active_runlist = true;\r\nout:\r\nif (lock)\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\nstatic int destroy_queue_cpsch(struct device_queue_manager *dqm,\r\nstruct qcm_process_device *qpd,\r\nstruct queue *q)\r\n{\r\nint retval;\r\nstruct mqd_manager *mqd;\r\nbool preempt_all_queues;\r\nBUG_ON(!dqm || !qpd || !q);\r\npreempt_all_queues = false;\r\nretval = 0;\r\nmutex_lock(&dqm->lock);\r\nif (qpd->is_debug) {\r\nretval = -EBUSY;\r\ngoto failed_try_destroy_debugged_queue;\r\n}\r\nmqd = dqm->ops.get_mqd_manager(dqm,\r\nget_mqd_type_from_queue_type(q->properties.type));\r\nif (!mqd) {\r\nretval = -ENOMEM;\r\ngoto failed;\r\n}\r\nif (q->properties.type == KFD_QUEUE_TYPE_SDMA)\r\ndqm->sdma_queue_count--;\r\nlist_del(&q->list);\r\nif (q->properties.is_active)\r\ndqm->queue_count--;\r\nexecute_queues_cpsch(dqm, false);\r\nmqd->uninit_mqd(mqd, q->mqd, q->mqd_mem_obj);\r\ndqm->total_queue_count--;\r\npr_debug("Total of %d queues are accountable so far\n",\r\ndqm->total_queue_count);\r\nmutex_unlock(&dqm->lock);\r\nreturn 0;\r\nfailed:\r\nfailed_try_destroy_debugged_queue:\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\n}\r\nstatic bool set_cache_memory_policy(struct device_queue_manager *dqm,\r\nstruct qcm_process_device *qpd,\r\nenum cache_policy default_policy,\r\nenum cache_policy alternate_policy,\r\nvoid __user *alternate_aperture_base,\r\nuint64_t alternate_aperture_size)\r\n{\r\nbool retval;\r\npr_debug("kfd: In func %s\n", __func__);\r\nmutex_lock(&dqm->lock);\r\nif (alternate_aperture_size == 0) {\r\nqpd->sh_mem_ape1_base = 1;\r\nqpd->sh_mem_ape1_limit = 0;\r\n} else {\r\nuint64_t base = (uintptr_t)alternate_aperture_base;\r\nuint64_t limit = base + alternate_aperture_size - 1;\r\nif (limit <= base)\r\ngoto out;\r\nif ((base & APE1_FIXED_BITS_MASK) != 0)\r\ngoto out;\r\nif ((limit & APE1_FIXED_BITS_MASK) != APE1_LIMIT_ALIGNMENT)\r\ngoto out;\r\nqpd->sh_mem_ape1_base = base >> 16;\r\nqpd->sh_mem_ape1_limit = limit >> 16;\r\n}\r\nretval = dqm->ops_asic_specific.set_cache_memory_policy(\r\ndqm,\r\nqpd,\r\ndefault_policy,\r\nalternate_policy,\r\nalternate_aperture_base,\r\nalternate_aperture_size);\r\nif ((sched_policy == KFD_SCHED_POLICY_NO_HWS) && (qpd->vmid != 0))\r\nprogram_sh_mem_settings(dqm, qpd);\r\npr_debug("kfd: sh_mem_config: 0x%x, ape1_base: 0x%x, ape1_limit: 0x%x\n",\r\nqpd->sh_mem_config, qpd->sh_mem_ape1_base,\r\nqpd->sh_mem_ape1_limit);\r\nmutex_unlock(&dqm->lock);\r\nreturn retval;\r\nout:\r\nmutex_unlock(&dqm->lock);\r\nreturn false;\r\n}\r\nstruct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)\r\n{\r\nstruct device_queue_manager *dqm;\r\nBUG_ON(!dev);\r\npr_debug("kfd: loading device queue manager\n");\r\ndqm = kzalloc(sizeof(struct device_queue_manager), GFP_KERNEL);\r\nif (!dqm)\r\nreturn NULL;\r\ndqm->dev = dev;\r\nswitch (sched_policy) {\r\ncase KFD_SCHED_POLICY_HWS:\r\ncase KFD_SCHED_POLICY_HWS_NO_OVERSUBSCRIPTION:\r\ndqm->ops.create_queue = create_queue_cpsch;\r\ndqm->ops.initialize = initialize_cpsch;\r\ndqm->ops.start = start_cpsch;\r\ndqm->ops.stop = stop_cpsch;\r\ndqm->ops.destroy_queue = destroy_queue_cpsch;\r\ndqm->ops.update_queue = update_queue;\r\ndqm->ops.get_mqd_manager = get_mqd_manager_nocpsch;\r\ndqm->ops.register_process = register_process_nocpsch;\r\ndqm->ops.unregister_process = unregister_process_nocpsch;\r\ndqm->ops.uninitialize = uninitialize_nocpsch;\r\ndqm->ops.create_kernel_queue = create_kernel_queue_cpsch;\r\ndqm->ops.destroy_kernel_queue = destroy_kernel_queue_cpsch;\r\ndqm->ops.set_cache_memory_policy = set_cache_memory_policy;\r\nbreak;\r\ncase KFD_SCHED_POLICY_NO_HWS:\r\ndqm->ops.start = start_nocpsch;\r\ndqm->ops.stop = stop_nocpsch;\r\ndqm->ops.create_queue = create_queue_nocpsch;\r\ndqm->ops.destroy_queue = destroy_queue_nocpsch;\r\ndqm->ops.update_queue = update_queue;\r\ndqm->ops.get_mqd_manager = get_mqd_manager_nocpsch;\r\ndqm->ops.register_process = register_process_nocpsch;\r\ndqm->ops.unregister_process = unregister_process_nocpsch;\r\ndqm->ops.initialize = initialize_nocpsch;\r\ndqm->ops.uninitialize = uninitialize_nocpsch;\r\ndqm->ops.set_cache_memory_policy = set_cache_memory_policy;\r\nbreak;\r\ndefault:\r\nBUG();\r\nbreak;\r\n}\r\nswitch (dev->device_info->asic_family) {\r\ncase CHIP_CARRIZO:\r\ndevice_queue_manager_init_vi(&dqm->ops_asic_specific);\r\nbreak;\r\ncase CHIP_KAVERI:\r\ndevice_queue_manager_init_cik(&dqm->ops_asic_specific);\r\nbreak;\r\n}\r\nif (dqm->ops.initialize(dqm) != 0) {\r\nkfree(dqm);\r\nreturn NULL;\r\n}\r\nreturn dqm;\r\n}\r\nvoid device_queue_manager_uninit(struct device_queue_manager *dqm)\r\n{\r\nBUG_ON(!dqm);\r\ndqm->ops.uninitialize(dqm);\r\nkfree(dqm);\r\n}
