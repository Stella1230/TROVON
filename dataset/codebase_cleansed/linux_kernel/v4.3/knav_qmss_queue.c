void knav_queue_notify(struct knav_queue_inst *inst)\r\n{\r\nstruct knav_queue *qh;\r\nif (!inst)\r\nreturn;\r\nrcu_read_lock();\r\nfor_each_handle_rcu(qh, inst) {\r\nif (atomic_read(&qh->notifier_enabled) <= 0)\r\ncontinue;\r\nif (WARN_ON(!qh->notifier_fn))\r\ncontinue;\r\natomic_inc(&qh->stats.notifies);\r\nqh->notifier_fn(qh->notifier_fn_arg);\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic irqreturn_t knav_queue_int_handler(int irq, void *_instdata)\r\n{\r\nstruct knav_queue_inst *inst = _instdata;\r\nknav_queue_notify(inst);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int knav_queue_setup_irq(struct knav_range_info *range,\r\nstruct knav_queue_inst *inst)\r\n{\r\nunsigned queue = inst->id - range->queue_base;\r\nunsigned long cpu_map;\r\nint ret = 0, irq;\r\nif (range->flags & RANGE_HAS_IRQ) {\r\nirq = range->irqs[queue].irq;\r\ncpu_map = range->irqs[queue].cpu_map;\r\nret = request_irq(irq, knav_queue_int_handler, 0,\r\ninst->irq_name, inst);\r\nif (ret)\r\nreturn ret;\r\ndisable_irq(irq);\r\nif (cpu_map) {\r\nret = irq_set_affinity_hint(irq, to_cpumask(&cpu_map));\r\nif (ret) {\r\ndev_warn(range->kdev->dev,\r\n"Failed to set IRQ affinity\n");\r\nreturn ret;\r\n}\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void knav_queue_free_irq(struct knav_queue_inst *inst)\r\n{\r\nstruct knav_range_info *range = inst->range;\r\nunsigned queue = inst->id - inst->range->queue_base;\r\nint irq;\r\nif (range->flags & RANGE_HAS_IRQ) {\r\nirq = range->irqs[queue].irq;\r\nirq_set_affinity_hint(irq, NULL);\r\nfree_irq(irq, inst);\r\n}\r\n}\r\nstatic inline bool knav_queue_is_busy(struct knav_queue_inst *inst)\r\n{\r\nreturn !list_empty(&inst->handles);\r\n}\r\nstatic inline bool knav_queue_is_reserved(struct knav_queue_inst *inst)\r\n{\r\nreturn inst->range->flags & RANGE_RESERVED;\r\n}\r\nstatic inline bool knav_queue_is_shared(struct knav_queue_inst *inst)\r\n{\r\nstruct knav_queue *tmp;\r\nrcu_read_lock();\r\nfor_each_handle_rcu(tmp, inst) {\r\nif (tmp->flags & KNAV_QUEUE_SHARED) {\r\nrcu_read_unlock();\r\nreturn true;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn false;\r\n}\r\nstatic inline bool knav_queue_match_type(struct knav_queue_inst *inst,\r\nunsigned type)\r\n{\r\nif ((type == KNAV_QUEUE_QPEND) &&\r\n(inst->range->flags & RANGE_HAS_IRQ)) {\r\nreturn true;\r\n} else if ((type == KNAV_QUEUE_ACC) &&\r\n(inst->range->flags & RANGE_HAS_ACCUMULATOR)) {\r\nreturn true;\r\n} else if ((type == KNAV_QUEUE_GP) &&\r\n!(inst->range->flags &\r\n(RANGE_HAS_ACCUMULATOR | RANGE_HAS_IRQ))) {\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic inline struct knav_queue_inst *\r\nknav_queue_match_id_to_inst(struct knav_device *kdev, unsigned id)\r\n{\r\nstruct knav_queue_inst *inst;\r\nint idx;\r\nfor_each_instance(idx, inst, kdev) {\r\nif (inst->id == id)\r\nreturn inst;\r\n}\r\nreturn NULL;\r\n}\r\nstatic inline struct knav_queue_inst *knav_queue_find_by_id(int id)\r\n{\r\nif (kdev->base_id <= id &&\r\nkdev->base_id + kdev->num_queues > id) {\r\nid -= kdev->base_id;\r\nreturn knav_queue_match_id_to_inst(kdev, id);\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct knav_queue *__knav_queue_open(struct knav_queue_inst *inst,\r\nconst char *name, unsigned flags)\r\n{\r\nstruct knav_queue *qh;\r\nunsigned id;\r\nint ret = 0;\r\nqh = devm_kzalloc(inst->kdev->dev, sizeof(*qh), GFP_KERNEL);\r\nif (!qh)\r\nreturn ERR_PTR(-ENOMEM);\r\nqh->flags = flags;\r\nqh->inst = inst;\r\nid = inst->id - inst->qmgr->start_queue;\r\nqh->reg_push = &inst->qmgr->reg_push[id];\r\nqh->reg_pop = &inst->qmgr->reg_pop[id];\r\nqh->reg_peek = &inst->qmgr->reg_peek[id];\r\nif (!knav_queue_is_busy(inst)) {\r\nstruct knav_range_info *range = inst->range;\r\ninst->name = kstrndup(name, KNAV_NAME_SIZE, GFP_KERNEL);\r\nif (range->ops && range->ops->open_queue)\r\nret = range->ops->open_queue(range, inst, flags);\r\nif (ret) {\r\ndevm_kfree(inst->kdev->dev, qh);\r\nreturn ERR_PTR(ret);\r\n}\r\n}\r\nlist_add_tail_rcu(&qh->list, &inst->handles);\r\nreturn qh;\r\n}\r\nstatic struct knav_queue *\r\nknav_queue_open_by_id(const char *name, unsigned id, unsigned flags)\r\n{\r\nstruct knav_queue_inst *inst;\r\nstruct knav_queue *qh;\r\nmutex_lock(&knav_dev_lock);\r\nqh = ERR_PTR(-ENODEV);\r\ninst = knav_queue_find_by_id(id);\r\nif (!inst)\r\ngoto unlock_ret;\r\nqh = ERR_PTR(-EEXIST);\r\nif (!(flags & KNAV_QUEUE_SHARED) && knav_queue_is_busy(inst))\r\ngoto unlock_ret;\r\nqh = ERR_PTR(-EBUSY);\r\nif ((flags & KNAV_QUEUE_SHARED) &&\r\n(knav_queue_is_busy(inst) && !knav_queue_is_shared(inst)))\r\ngoto unlock_ret;\r\nqh = __knav_queue_open(inst, name, flags);\r\nunlock_ret:\r\nmutex_unlock(&knav_dev_lock);\r\nreturn qh;\r\n}\r\nstatic struct knav_queue *knav_queue_open_by_type(const char *name,\r\nunsigned type, unsigned flags)\r\n{\r\nstruct knav_queue_inst *inst;\r\nstruct knav_queue *qh = ERR_PTR(-EINVAL);\r\nint idx;\r\nmutex_lock(&knav_dev_lock);\r\nfor_each_instance(idx, inst, kdev) {\r\nif (knav_queue_is_reserved(inst))\r\ncontinue;\r\nif (!knav_queue_match_type(inst, type))\r\ncontinue;\r\nif (knav_queue_is_busy(inst))\r\ncontinue;\r\nqh = __knav_queue_open(inst, name, flags);\r\ngoto unlock_ret;\r\n}\r\nunlock_ret:\r\nmutex_unlock(&knav_dev_lock);\r\nreturn qh;\r\n}\r\nstatic void knav_queue_set_notify(struct knav_queue_inst *inst, bool enabled)\r\n{\r\nstruct knav_range_info *range = inst->range;\r\nif (range->ops && range->ops->set_notify)\r\nrange->ops->set_notify(range, inst, enabled);\r\n}\r\nstatic int knav_queue_enable_notifier(struct knav_queue *qh)\r\n{\r\nstruct knav_queue_inst *inst = qh->inst;\r\nbool first;\r\nif (WARN_ON(!qh->notifier_fn))\r\nreturn -EINVAL;\r\nfirst = (atomic_inc_return(&qh->notifier_enabled) == 1);\r\nif (!first)\r\nreturn 0;\r\nfirst = (atomic_inc_return(&inst->num_notifiers) == 1);\r\nif (first)\r\nknav_queue_set_notify(inst, true);\r\nreturn 0;\r\n}\r\nstatic int knav_queue_disable_notifier(struct knav_queue *qh)\r\n{\r\nstruct knav_queue_inst *inst = qh->inst;\r\nbool last;\r\nlast = (atomic_dec_return(&qh->notifier_enabled) == 0);\r\nif (!last)\r\nreturn 0;\r\nlast = (atomic_dec_return(&inst->num_notifiers) == 0);\r\nif (last)\r\nknav_queue_set_notify(inst, false);\r\nreturn 0;\r\n}\r\nstatic int knav_queue_set_notifier(struct knav_queue *qh,\r\nstruct knav_queue_notify_config *cfg)\r\n{\r\nknav_queue_notify_fn old_fn = qh->notifier_fn;\r\nif (!cfg)\r\nreturn -EINVAL;\r\nif (!(qh->inst->range->flags & (RANGE_HAS_ACCUMULATOR | RANGE_HAS_IRQ)))\r\nreturn -ENOTSUPP;\r\nif (!cfg->fn && old_fn)\r\nknav_queue_disable_notifier(qh);\r\nqh->notifier_fn = cfg->fn;\r\nqh->notifier_fn_arg = cfg->fn_arg;\r\nif (cfg->fn && !old_fn)\r\nknav_queue_enable_notifier(qh);\r\nreturn 0;\r\n}\r\nstatic int knav_gp_set_notify(struct knav_range_info *range,\r\nstruct knav_queue_inst *inst,\r\nbool enabled)\r\n{\r\nunsigned queue;\r\nif (range->flags & RANGE_HAS_IRQ) {\r\nqueue = inst->id - range->queue_base;\r\nif (enabled)\r\nenable_irq(range->irqs[queue].irq);\r\nelse\r\ndisable_irq_nosync(range->irqs[queue].irq);\r\n}\r\nreturn 0;\r\n}\r\nstatic int knav_gp_open_queue(struct knav_range_info *range,\r\nstruct knav_queue_inst *inst, unsigned flags)\r\n{\r\nreturn knav_queue_setup_irq(range, inst);\r\n}\r\nstatic int knav_gp_close_queue(struct knav_range_info *range,\r\nstruct knav_queue_inst *inst)\r\n{\r\nknav_queue_free_irq(inst);\r\nreturn 0;\r\n}\r\nstatic int knav_queue_get_count(void *qhandle)\r\n{\r\nstruct knav_queue *qh = qhandle;\r\nstruct knav_queue_inst *inst = qh->inst;\r\nreturn readl_relaxed(&qh->reg_peek[0].entry_count) +\r\natomic_read(&inst->desc_count);\r\n}\r\nstatic void knav_queue_debug_show_instance(struct seq_file *s,\r\nstruct knav_queue_inst *inst)\r\n{\r\nstruct knav_device *kdev = inst->kdev;\r\nstruct knav_queue *qh;\r\nif (!knav_queue_is_busy(inst))\r\nreturn;\r\nseq_printf(s, "\tqueue id %d (%s)\n",\r\nkdev->base_id + inst->id, inst->name);\r\nfor_each_handle_rcu(qh, inst) {\r\nseq_printf(s, "\t\thandle %p: ", qh);\r\nseq_printf(s, "pushes %8d, ",\r\natomic_read(&qh->stats.pushes));\r\nseq_printf(s, "pops %8d, ",\r\natomic_read(&qh->stats.pops));\r\nseq_printf(s, "count %8d, ",\r\nknav_queue_get_count(qh));\r\nseq_printf(s, "notifies %8d, ",\r\natomic_read(&qh->stats.notifies));\r\nseq_printf(s, "push errors %8d, ",\r\natomic_read(&qh->stats.push_errors));\r\nseq_printf(s, "pop errors %8d\n",\r\natomic_read(&qh->stats.pop_errors));\r\n}\r\n}\r\nstatic int knav_queue_debug_show(struct seq_file *s, void *v)\r\n{\r\nstruct knav_queue_inst *inst;\r\nint idx;\r\nmutex_lock(&knav_dev_lock);\r\nseq_printf(s, "%s: %u-%u\n",\r\ndev_name(kdev->dev), kdev->base_id,\r\nkdev->base_id + kdev->num_queues - 1);\r\nfor_each_instance(idx, inst, kdev)\r\nknav_queue_debug_show_instance(s, inst);\r\nmutex_unlock(&knav_dev_lock);\r\nreturn 0;\r\n}\r\nstatic int knav_queue_debug_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, knav_queue_debug_show, NULL);\r\n}\r\nstatic inline int knav_queue_pdsp_wait(u32 * __iomem addr, unsigned timeout,\r\nu32 flags)\r\n{\r\nunsigned long end;\r\nu32 val = 0;\r\nend = jiffies + msecs_to_jiffies(timeout);\r\nwhile (time_after(end, jiffies)) {\r\nval = readl_relaxed(addr);\r\nif (flags)\r\nval &= flags;\r\nif (!val)\r\nbreak;\r\ncpu_relax();\r\n}\r\nreturn val ? -ETIMEDOUT : 0;\r\n}\r\nstatic int knav_queue_flush(struct knav_queue *qh)\r\n{\r\nstruct knav_queue_inst *inst = qh->inst;\r\nunsigned id = inst->id - inst->qmgr->start_queue;\r\natomic_set(&inst->desc_count, 0);\r\nwritel_relaxed(0, &inst->qmgr->reg_push[id].ptr_size_thresh);\r\nreturn 0;\r\n}\r\nvoid *knav_queue_open(const char *name, unsigned id,\r\nunsigned flags)\r\n{\r\nstruct knav_queue *qh = ERR_PTR(-EINVAL);\r\nswitch (id) {\r\ncase KNAV_QUEUE_QPEND:\r\ncase KNAV_QUEUE_ACC:\r\ncase KNAV_QUEUE_GP:\r\nqh = knav_queue_open_by_type(name, id, flags);\r\nbreak;\r\ndefault:\r\nqh = knav_queue_open_by_id(name, id, flags);\r\nbreak;\r\n}\r\nreturn qh;\r\n}\r\nvoid knav_queue_close(void *qhandle)\r\n{\r\nstruct knav_queue *qh = qhandle;\r\nstruct knav_queue_inst *inst = qh->inst;\r\nwhile (atomic_read(&qh->notifier_enabled) > 0)\r\nknav_queue_disable_notifier(qh);\r\nmutex_lock(&knav_dev_lock);\r\nlist_del_rcu(&qh->list);\r\nmutex_unlock(&knav_dev_lock);\r\nsynchronize_rcu();\r\nif (!knav_queue_is_busy(inst)) {\r\nstruct knav_range_info *range = inst->range;\r\nif (range->ops && range->ops->close_queue)\r\nrange->ops->close_queue(range, inst);\r\n}\r\ndevm_kfree(inst->kdev->dev, qh);\r\n}\r\nint knav_queue_device_control(void *qhandle, enum knav_queue_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct knav_queue *qh = qhandle;\r\nstruct knav_queue_notify_config *cfg;\r\nint ret;\r\nswitch ((int)cmd) {\r\ncase KNAV_QUEUE_GET_ID:\r\nret = qh->inst->kdev->base_id + qh->inst->id;\r\nbreak;\r\ncase KNAV_QUEUE_FLUSH:\r\nret = knav_queue_flush(qh);\r\nbreak;\r\ncase KNAV_QUEUE_SET_NOTIFIER:\r\ncfg = (void *)arg;\r\nret = knav_queue_set_notifier(qh, cfg);\r\nbreak;\r\ncase KNAV_QUEUE_ENABLE_NOTIFY:\r\nret = knav_queue_enable_notifier(qh);\r\nbreak;\r\ncase KNAV_QUEUE_DISABLE_NOTIFY:\r\nret = knav_queue_disable_notifier(qh);\r\nbreak;\r\ncase KNAV_QUEUE_GET_COUNT:\r\nret = knav_queue_get_count(qh);\r\nbreak;\r\ndefault:\r\nret = -ENOTSUPP;\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nint knav_queue_push(void *qhandle, dma_addr_t dma,\r\nunsigned size, unsigned flags)\r\n{\r\nstruct knav_queue *qh = qhandle;\r\nu32 val;\r\nval = (u32)dma | ((size / 16) - 1);\r\nwritel_relaxed(val, &qh->reg_push[0].ptr_size_thresh);\r\natomic_inc(&qh->stats.pushes);\r\nreturn 0;\r\n}\r\ndma_addr_t knav_queue_pop(void *qhandle, unsigned *size)\r\n{\r\nstruct knav_queue *qh = qhandle;\r\nstruct knav_queue_inst *inst = qh->inst;\r\ndma_addr_t dma;\r\nu32 val, idx;\r\nif (inst->descs) {\r\nif (unlikely(atomic_dec_return(&inst->desc_count) < 0)) {\r\natomic_inc(&inst->desc_count);\r\nreturn 0;\r\n}\r\nidx = atomic_inc_return(&inst->desc_head);\r\nidx &= ACC_DESCS_MASK;\r\nval = inst->descs[idx];\r\n} else {\r\nval = readl_relaxed(&qh->reg_pop[0].ptr_size_thresh);\r\nif (unlikely(!val))\r\nreturn 0;\r\n}\r\ndma = val & DESC_PTR_MASK;\r\nif (size)\r\n*size = ((val & DESC_SIZE_MASK) + 1) * 16;\r\natomic_inc(&qh->stats.pops);\r\nreturn dma;\r\n}\r\nstatic void kdesc_fill_pool(struct knav_pool *pool)\r\n{\r\nstruct knav_region *region;\r\nint i;\r\nregion = pool->region;\r\npool->desc_size = region->desc_size;\r\nfor (i = 0; i < pool->num_desc; i++) {\r\nint index = pool->region_offset + i;\r\ndma_addr_t dma_addr;\r\nunsigned dma_size;\r\ndma_addr = region->dma_start + (region->desc_size * index);\r\ndma_size = ALIGN(pool->desc_size, SMP_CACHE_BYTES);\r\ndma_sync_single_for_device(pool->dev, dma_addr, dma_size,\r\nDMA_TO_DEVICE);\r\nknav_queue_push(pool->queue, dma_addr, dma_size, 0);\r\n}\r\n}\r\nstatic void kdesc_empty_pool(struct knav_pool *pool)\r\n{\r\ndma_addr_t dma;\r\nunsigned size;\r\nvoid *desc;\r\nint i;\r\nif (!pool->queue)\r\nreturn;\r\nfor (i = 0;; i++) {\r\ndma = knav_queue_pop(pool->queue, &size);\r\nif (!dma)\r\nbreak;\r\ndesc = knav_pool_desc_dma_to_virt(pool, dma);\r\nif (!desc) {\r\ndev_dbg(pool->kdev->dev,\r\n"couldn't unmap desc, continuing\n");\r\ncontinue;\r\n}\r\n}\r\nWARN_ON(i != pool->num_desc);\r\nknav_queue_close(pool->queue);\r\n}\r\ndma_addr_t knav_pool_desc_virt_to_dma(void *ph, void *virt)\r\n{\r\nstruct knav_pool *pool = ph;\r\nreturn pool->region->dma_start + (virt - pool->region->virt_start);\r\n}\r\nvoid *knav_pool_desc_dma_to_virt(void *ph, dma_addr_t dma)\r\n{\r\nstruct knav_pool *pool = ph;\r\nreturn pool->region->virt_start + (dma - pool->region->dma_start);\r\n}\r\nvoid *knav_pool_create(const char *name,\r\nint num_desc, int region_id)\r\n{\r\nstruct knav_region *reg_itr, *region = NULL;\r\nstruct knav_pool *pool, *pi;\r\nstruct list_head *node;\r\nunsigned last_offset;\r\nbool slot_found;\r\nint ret;\r\nif (!kdev->dev)\r\nreturn ERR_PTR(-ENODEV);\r\npool = devm_kzalloc(kdev->dev, sizeof(*pool), GFP_KERNEL);\r\nif (!pool) {\r\ndev_err(kdev->dev, "out of memory allocating pool\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nfor_each_region(kdev, reg_itr) {\r\nif (reg_itr->id != region_id)\r\ncontinue;\r\nregion = reg_itr;\r\nbreak;\r\n}\r\nif (!region) {\r\ndev_err(kdev->dev, "region-id(%d) not found\n", region_id);\r\nret = -EINVAL;\r\ngoto err;\r\n}\r\npool->queue = knav_queue_open(name, KNAV_QUEUE_GP, 0);\r\nif (IS_ERR_OR_NULL(pool->queue)) {\r\ndev_err(kdev->dev,\r\n"failed to open queue for pool(%s), error %ld\n",\r\nname, PTR_ERR(pool->queue));\r\nret = PTR_ERR(pool->queue);\r\ngoto err;\r\n}\r\npool->name = kstrndup(name, KNAV_NAME_SIZE, GFP_KERNEL);\r\npool->kdev = kdev;\r\npool->dev = kdev->dev;\r\nmutex_lock(&knav_dev_lock);\r\nif (num_desc > (region->num_desc - region->used_desc)) {\r\ndev_err(kdev->dev, "out of descs in region(%d) for pool(%s)\n",\r\nregion_id, name);\r\nret = -ENOMEM;\r\ngoto err_unlock;\r\n}\r\nlast_offset = 0;\r\nslot_found = false;\r\nnode = &region->pools;\r\nlist_for_each_entry(pi, &region->pools, region_inst) {\r\nif ((pi->region_offset - last_offset) >= num_desc) {\r\nslot_found = true;\r\nbreak;\r\n}\r\nlast_offset = pi->region_offset + pi->num_desc;\r\n}\r\nnode = &pi->region_inst;\r\nif (slot_found) {\r\npool->region = region;\r\npool->num_desc = num_desc;\r\npool->region_offset = last_offset;\r\nregion->used_desc += num_desc;\r\nlist_add_tail(&pool->list, &kdev->pools);\r\nlist_add_tail(&pool->region_inst, node);\r\n} else {\r\ndev_err(kdev->dev, "pool(%s) create failed: fragmented desc pool in region(%d)\n",\r\nname, region_id);\r\nret = -ENOMEM;\r\ngoto err_unlock;\r\n}\r\nmutex_unlock(&knav_dev_lock);\r\nkdesc_fill_pool(pool);\r\nreturn pool;\r\nerr_unlock:\r\nmutex_unlock(&knav_dev_lock);\r\nerr:\r\nkfree(pool->name);\r\ndevm_kfree(kdev->dev, pool);\r\nreturn ERR_PTR(ret);\r\n}\r\nvoid knav_pool_destroy(void *ph)\r\n{\r\nstruct knav_pool *pool = ph;\r\nif (!pool)\r\nreturn;\r\nif (!pool->region)\r\nreturn;\r\nkdesc_empty_pool(pool);\r\nmutex_lock(&knav_dev_lock);\r\npool->region->used_desc -= pool->num_desc;\r\nlist_del(&pool->region_inst);\r\nlist_del(&pool->list);\r\nmutex_unlock(&knav_dev_lock);\r\nkfree(pool->name);\r\ndevm_kfree(kdev->dev, pool);\r\n}\r\nvoid *knav_pool_desc_get(void *ph)\r\n{\r\nstruct knav_pool *pool = ph;\r\ndma_addr_t dma;\r\nunsigned size;\r\nvoid *data;\r\ndma = knav_queue_pop(pool->queue, &size);\r\nif (unlikely(!dma))\r\nreturn ERR_PTR(-ENOMEM);\r\ndata = knav_pool_desc_dma_to_virt(pool, dma);\r\nreturn data;\r\n}\r\nvoid knav_pool_desc_put(void *ph, void *desc)\r\n{\r\nstruct knav_pool *pool = ph;\r\ndma_addr_t dma;\r\ndma = knav_pool_desc_virt_to_dma(pool, desc);\r\nknav_queue_push(pool->queue, dma, pool->region->desc_size, 0);\r\n}\r\nint knav_pool_desc_map(void *ph, void *desc, unsigned size,\r\ndma_addr_t *dma, unsigned *dma_sz)\r\n{\r\nstruct knav_pool *pool = ph;\r\n*dma = knav_pool_desc_virt_to_dma(pool, desc);\r\nsize = min(size, pool->region->desc_size);\r\nsize = ALIGN(size, SMP_CACHE_BYTES);\r\n*dma_sz = size;\r\ndma_sync_single_for_device(pool->dev, *dma, size, DMA_TO_DEVICE);\r\n__iowmb();\r\nreturn 0;\r\n}\r\nvoid *knav_pool_desc_unmap(void *ph, dma_addr_t dma, unsigned dma_sz)\r\n{\r\nstruct knav_pool *pool = ph;\r\nunsigned desc_sz;\r\nvoid *desc;\r\ndesc_sz = min(dma_sz, pool->region->desc_size);\r\ndesc = knav_pool_desc_dma_to_virt(pool, dma);\r\ndma_sync_single_for_cpu(pool->dev, dma, desc_sz, DMA_FROM_DEVICE);\r\nprefetch(desc);\r\nreturn desc;\r\n}\r\nint knav_pool_count(void *ph)\r\n{\r\nstruct knav_pool *pool = ph;\r\nreturn knav_queue_get_count(pool->queue);\r\n}\r\nstatic void knav_queue_setup_region(struct knav_device *kdev,\r\nstruct knav_region *region)\r\n{\r\nunsigned hw_num_desc, hw_desc_size, size;\r\nstruct knav_reg_region __iomem *regs;\r\nstruct knav_qmgr_info *qmgr;\r\nstruct knav_pool *pool;\r\nint id = region->id;\r\nstruct page *page;\r\nif (!region->num_desc) {\r\ndev_warn(kdev->dev, "unused region %s\n", region->name);\r\nreturn;\r\n}\r\nhw_num_desc = ilog2(region->num_desc - 1) + 1;\r\nif (region->num_desc < 32) {\r\nregion->num_desc = 0;\r\ndev_warn(kdev->dev, "too few descriptors in region %s\n",\r\nregion->name);\r\nreturn;\r\n}\r\nsize = region->num_desc * region->desc_size;\r\nregion->virt_start = alloc_pages_exact(size, GFP_KERNEL | GFP_DMA |\r\nGFP_DMA32);\r\nif (!region->virt_start) {\r\nregion->num_desc = 0;\r\ndev_err(kdev->dev, "memory alloc failed for region %s\n",\r\nregion->name);\r\nreturn;\r\n}\r\nregion->virt_end = region->virt_start + size;\r\npage = virt_to_page(region->virt_start);\r\nregion->dma_start = dma_map_page(kdev->dev, page, 0, size,\r\nDMA_BIDIRECTIONAL);\r\nif (dma_mapping_error(kdev->dev, region->dma_start)) {\r\ndev_err(kdev->dev, "dma map failed for region %s\n",\r\nregion->name);\r\ngoto fail;\r\n}\r\nregion->dma_end = region->dma_start + size;\r\npool = devm_kzalloc(kdev->dev, sizeof(*pool), GFP_KERNEL);\r\nif (!pool) {\r\ndev_err(kdev->dev, "out of memory allocating dummy pool\n");\r\ngoto fail;\r\n}\r\npool->num_desc = 0;\r\npool->region_offset = region->num_desc;\r\nlist_add(&pool->region_inst, &region->pools);\r\ndev_dbg(kdev->dev,\r\n"region %s (%d): size:%d, link:%d@%d, phys:%08x-%08x, virt:%p-%p\n",\r\nregion->name, id, region->desc_size, region->num_desc,\r\nregion->link_index, region->dma_start, region->dma_end,\r\nregion->virt_start, region->virt_end);\r\nhw_desc_size = (region->desc_size / 16) - 1;\r\nhw_num_desc -= 5;\r\nfor_each_qmgr(kdev, qmgr) {\r\nregs = qmgr->reg_region + id;\r\nwritel_relaxed(region->dma_start, &regs->base);\r\nwritel_relaxed(region->link_index, &regs->start_index);\r\nwritel_relaxed(hw_desc_size << 16 | hw_num_desc,\r\n&regs->size_count);\r\n}\r\nreturn;\r\nfail:\r\nif (region->dma_start)\r\ndma_unmap_page(kdev->dev, region->dma_start, size,\r\nDMA_BIDIRECTIONAL);\r\nif (region->virt_start)\r\nfree_pages_exact(region->virt_start, size);\r\nregion->num_desc = 0;\r\nreturn;\r\n}\r\nstatic const char *knav_queue_find_name(struct device_node *node)\r\n{\r\nconst char *name;\r\nif (of_property_read_string(node, "label", &name) < 0)\r\nname = node->name;\r\nif (!name)\r\nname = "unknown";\r\nreturn name;\r\n}\r\nstatic int knav_queue_setup_regions(struct knav_device *kdev,\r\nstruct device_node *regions)\r\n{\r\nstruct device *dev = kdev->dev;\r\nstruct knav_region *region;\r\nstruct device_node *child;\r\nu32 temp[2];\r\nint ret;\r\nfor_each_child_of_node(regions, child) {\r\nregion = devm_kzalloc(dev, sizeof(*region), GFP_KERNEL);\r\nif (!region) {\r\ndev_err(dev, "out of memory allocating region\n");\r\nreturn -ENOMEM;\r\n}\r\nregion->name = knav_queue_find_name(child);\r\nof_property_read_u32(child, "id", &region->id);\r\nret = of_property_read_u32_array(child, "region-spec", temp, 2);\r\nif (!ret) {\r\nregion->num_desc = temp[0];\r\nregion->desc_size = temp[1];\r\n} else {\r\ndev_err(dev, "invalid region info %s\n", region->name);\r\ndevm_kfree(dev, region);\r\ncontinue;\r\n}\r\nif (!of_get_property(child, "link-index", NULL)) {\r\ndev_err(dev, "No link info for %s\n", region->name);\r\ndevm_kfree(dev, region);\r\ncontinue;\r\n}\r\nret = of_property_read_u32(child, "link-index",\r\n&region->link_index);\r\nif (ret) {\r\ndev_err(dev, "link index not found for %s\n",\r\nregion->name);\r\ndevm_kfree(dev, region);\r\ncontinue;\r\n}\r\nINIT_LIST_HEAD(&region->pools);\r\nlist_add_tail(&region->list, &kdev->regions);\r\n}\r\nif (list_empty(&kdev->regions)) {\r\ndev_err(dev, "no valid region information found\n");\r\nreturn -ENODEV;\r\n}\r\nfor_each_region(kdev, region)\r\nknav_queue_setup_region(kdev, region);\r\nreturn 0;\r\n}\r\nstatic int knav_get_link_ram(struct knav_device *kdev,\r\nconst char *name,\r\nstruct knav_link_ram_block *block)\r\n{\r\nstruct platform_device *pdev = to_platform_device(kdev->dev);\r\nstruct device_node *node = pdev->dev.of_node;\r\nu32 temp[2];\r\nif (!of_property_read_u32_array(node, name , temp, 2)) {\r\nif (temp[0]) {\r\nblock->phys = (dma_addr_t)temp[0];\r\nblock->virt = NULL;\r\nblock->size = temp[1];\r\n} else {\r\nblock->size = temp[1];\r\nblock->virt = dmam_alloc_coherent(kdev->dev,\r\n8 * block->size, &block->phys,\r\nGFP_KERNEL);\r\nif (!block->virt) {\r\ndev_err(kdev->dev, "failed to alloc linkram\n");\r\nreturn -ENOMEM;\r\n}\r\n}\r\n} else {\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nstatic int knav_queue_setup_link_ram(struct knav_device *kdev)\r\n{\r\nstruct knav_link_ram_block *block;\r\nstruct knav_qmgr_info *qmgr;\r\nfor_each_qmgr(kdev, qmgr) {\r\nblock = &kdev->link_rams[0];\r\ndev_dbg(kdev->dev, "linkram0: phys:%x, virt:%p, size:%x\n",\r\nblock->phys, block->virt, block->size);\r\nwritel_relaxed(block->phys, &qmgr->reg_config->link_ram_base0);\r\nwritel_relaxed(block->size, &qmgr->reg_config->link_ram_size0);\r\nblock++;\r\nif (!block->size)\r\nreturn 0;\r\ndev_dbg(kdev->dev, "linkram1: phys:%x, virt:%p, size:%x\n",\r\nblock->phys, block->virt, block->size);\r\nwritel_relaxed(block->phys, &qmgr->reg_config->link_ram_base1);\r\n}\r\nreturn 0;\r\n}\r\nstatic int knav_setup_queue_range(struct knav_device *kdev,\r\nstruct device_node *node)\r\n{\r\nstruct device *dev = kdev->dev;\r\nstruct knav_range_info *range;\r\nstruct knav_qmgr_info *qmgr;\r\nu32 temp[2], start, end, id, index;\r\nint ret, i;\r\nrange = devm_kzalloc(dev, sizeof(*range), GFP_KERNEL);\r\nif (!range) {\r\ndev_err(dev, "out of memory allocating range\n");\r\nreturn -ENOMEM;\r\n}\r\nrange->kdev = kdev;\r\nrange->name = knav_queue_find_name(node);\r\nret = of_property_read_u32_array(node, "qrange", temp, 2);\r\nif (!ret) {\r\nrange->queue_base = temp[0] - kdev->base_id;\r\nrange->num_queues = temp[1];\r\n} else {\r\ndev_err(dev, "invalid queue range %s\n", range->name);\r\ndevm_kfree(dev, range);\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < RANGE_MAX_IRQS; i++) {\r\nstruct of_phandle_args oirq;\r\nif (of_irq_parse_one(node, i, &oirq))\r\nbreak;\r\nrange->irqs[i].irq = irq_create_of_mapping(&oirq);\r\nif (range->irqs[i].irq == IRQ_NONE)\r\nbreak;\r\nrange->num_irqs++;\r\nif (oirq.args_count == 3)\r\nrange->irqs[i].cpu_map =\r\n(oirq.args[2] & 0x0000ff00) >> 8;\r\n}\r\nrange->num_irqs = min(range->num_irqs, range->num_queues);\r\nif (range->num_irqs)\r\nrange->flags |= RANGE_HAS_IRQ;\r\nif (of_get_property(node, "qalloc-by-id", NULL))\r\nrange->flags |= RANGE_RESERVED;\r\nif (of_get_property(node, "accumulator", NULL)) {\r\nret = knav_init_acc_range(kdev, node, range);\r\nif (ret < 0) {\r\ndevm_kfree(dev, range);\r\nreturn ret;\r\n}\r\n} else {\r\nrange->ops = &knav_gp_range_ops;\r\n}\r\nfor_each_qmgr(kdev, qmgr) {\r\nstart = max(qmgr->start_queue, range->queue_base);\r\nend = min(qmgr->start_queue + qmgr->num_queues,\r\nrange->queue_base + range->num_queues);\r\nfor (id = start; id < end; id++) {\r\nindex = id - qmgr->start_queue;\r\nwritel_relaxed(THRESH_GTE | 1,\r\n&qmgr->reg_peek[index].ptr_size_thresh);\r\nwritel_relaxed(0,\r\n&qmgr->reg_push[index].ptr_size_thresh);\r\n}\r\n}\r\nlist_add_tail(&range->list, &kdev->queue_ranges);\r\ndev_dbg(dev, "added range %s: %d-%d, %d irqs%s%s%s\n",\r\nrange->name, range->queue_base,\r\nrange->queue_base + range->num_queues - 1,\r\nrange->num_irqs,\r\n(range->flags & RANGE_HAS_IRQ) ? ", has irq" : "",\r\n(range->flags & RANGE_RESERVED) ? ", reserved" : "",\r\n(range->flags & RANGE_HAS_ACCUMULATOR) ? ", acc" : "");\r\nkdev->num_queues_in_use += range->num_queues;\r\nreturn 0;\r\n}\r\nstatic int knav_setup_queue_pools(struct knav_device *kdev,\r\nstruct device_node *queue_pools)\r\n{\r\nstruct device_node *type, *range;\r\nint ret;\r\nfor_each_child_of_node(queue_pools, type) {\r\nfor_each_child_of_node(type, range) {\r\nret = knav_setup_queue_range(kdev, range);\r\n}\r\n}\r\nif (list_empty(&kdev->queue_ranges)) {\r\ndev_err(kdev->dev, "no valid queue range found\n");\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nstatic void knav_free_queue_range(struct knav_device *kdev,\r\nstruct knav_range_info *range)\r\n{\r\nif (range->ops && range->ops->free_range)\r\nrange->ops->free_range(range);\r\nlist_del(&range->list);\r\ndevm_kfree(kdev->dev, range);\r\n}\r\nstatic void knav_free_queue_ranges(struct knav_device *kdev)\r\n{\r\nstruct knav_range_info *range;\r\nfor (;;) {\r\nrange = first_queue_range(kdev);\r\nif (!range)\r\nbreak;\r\nknav_free_queue_range(kdev, range);\r\n}\r\n}\r\nstatic void knav_queue_free_regions(struct knav_device *kdev)\r\n{\r\nstruct knav_region *region;\r\nstruct knav_pool *pool, *tmp;\r\nunsigned size;\r\nfor (;;) {\r\nregion = first_region(kdev);\r\nif (!region)\r\nbreak;\r\nlist_for_each_entry_safe(pool, tmp, &region->pools, region_inst)\r\nknav_pool_destroy(pool);\r\nsize = region->virt_end - region->virt_start;\r\nif (size)\r\nfree_pages_exact(region->virt_start, size);\r\nlist_del(&region->list);\r\ndevm_kfree(kdev->dev, region);\r\n}\r\n}\r\nstatic void __iomem *knav_queue_map_reg(struct knav_device *kdev,\r\nstruct device_node *node, int index)\r\n{\r\nstruct resource res;\r\nvoid __iomem *regs;\r\nint ret;\r\nret = of_address_to_resource(node, index, &res);\r\nif (ret) {\r\ndev_err(kdev->dev, "Can't translate of node(%s) address for index(%d)\n",\r\nnode->name, index);\r\nreturn ERR_PTR(ret);\r\n}\r\nregs = devm_ioremap_resource(kdev->dev, &res);\r\nif (IS_ERR(regs))\r\ndev_err(kdev->dev, "Failed to map register base for index(%d) node(%s)\n",\r\nindex, node->name);\r\nreturn regs;\r\n}\r\nstatic int knav_queue_init_qmgrs(struct knav_device *kdev,\r\nstruct device_node *qmgrs)\r\n{\r\nstruct device *dev = kdev->dev;\r\nstruct knav_qmgr_info *qmgr;\r\nstruct device_node *child;\r\nu32 temp[2];\r\nint ret;\r\nfor_each_child_of_node(qmgrs, child) {\r\nqmgr = devm_kzalloc(dev, sizeof(*qmgr), GFP_KERNEL);\r\nif (!qmgr) {\r\ndev_err(dev, "out of memory allocating qmgr\n");\r\nreturn -ENOMEM;\r\n}\r\nret = of_property_read_u32_array(child, "managed-queues",\r\ntemp, 2);\r\nif (!ret) {\r\nqmgr->start_queue = temp[0];\r\nqmgr->num_queues = temp[1];\r\n} else {\r\ndev_err(dev, "invalid qmgr queue range\n");\r\ndevm_kfree(dev, qmgr);\r\ncontinue;\r\n}\r\ndev_info(dev, "qmgr start queue %d, number of queues %d\n",\r\nqmgr->start_queue, qmgr->num_queues);\r\nqmgr->reg_peek =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_PEEK_REG_INDEX);\r\nqmgr->reg_status =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_STATUS_REG_INDEX);\r\nqmgr->reg_config =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_CONFIG_REG_INDEX);\r\nqmgr->reg_region =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_REGION_REG_INDEX);\r\nqmgr->reg_push =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_PUSH_REG_INDEX);\r\nqmgr->reg_pop =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_POP_REG_INDEX);\r\nif (IS_ERR(qmgr->reg_peek) || IS_ERR(qmgr->reg_status) ||\r\nIS_ERR(qmgr->reg_config) || IS_ERR(qmgr->reg_region) ||\r\nIS_ERR(qmgr->reg_push) || IS_ERR(qmgr->reg_pop)) {\r\ndev_err(dev, "failed to map qmgr regs\n");\r\nif (!IS_ERR(qmgr->reg_peek))\r\ndevm_iounmap(dev, qmgr->reg_peek);\r\nif (!IS_ERR(qmgr->reg_status))\r\ndevm_iounmap(dev, qmgr->reg_status);\r\nif (!IS_ERR(qmgr->reg_config))\r\ndevm_iounmap(dev, qmgr->reg_config);\r\nif (!IS_ERR(qmgr->reg_region))\r\ndevm_iounmap(dev, qmgr->reg_region);\r\nif (!IS_ERR(qmgr->reg_push))\r\ndevm_iounmap(dev, qmgr->reg_push);\r\nif (!IS_ERR(qmgr->reg_pop))\r\ndevm_iounmap(dev, qmgr->reg_pop);\r\ndevm_kfree(dev, qmgr);\r\ncontinue;\r\n}\r\nlist_add_tail(&qmgr->list, &kdev->qmgrs);\r\ndev_info(dev, "added qmgr start queue %d, num of queues %d, reg_peek %p, reg_status %p, reg_config %p, reg_region %p, reg_push %p, reg_pop %p\n",\r\nqmgr->start_queue, qmgr->num_queues,\r\nqmgr->reg_peek, qmgr->reg_status,\r\nqmgr->reg_config, qmgr->reg_region,\r\nqmgr->reg_push, qmgr->reg_pop);\r\n}\r\nreturn 0;\r\n}\r\nstatic int knav_queue_init_pdsps(struct knav_device *kdev,\r\nstruct device_node *pdsps)\r\n{\r\nstruct device *dev = kdev->dev;\r\nstruct knav_pdsp_info *pdsp;\r\nstruct device_node *child;\r\nint ret;\r\nfor_each_child_of_node(pdsps, child) {\r\npdsp = devm_kzalloc(dev, sizeof(*pdsp), GFP_KERNEL);\r\nif (!pdsp) {\r\ndev_err(dev, "out of memory allocating pdsp\n");\r\nreturn -ENOMEM;\r\n}\r\npdsp->name = knav_queue_find_name(child);\r\nret = of_property_read_string(child, "firmware",\r\n&pdsp->firmware);\r\nif (ret < 0 || !pdsp->firmware) {\r\ndev_err(dev, "unknown firmware for pdsp %s\n",\r\npdsp->name);\r\ndevm_kfree(dev, pdsp);\r\ncontinue;\r\n}\r\ndev_dbg(dev, "pdsp name %s fw name :%s\n", pdsp->name,\r\npdsp->firmware);\r\npdsp->iram =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_PDSP_IRAM_REG_INDEX);\r\npdsp->regs =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_PDSP_REGS_REG_INDEX);\r\npdsp->intd =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_PDSP_INTD_REG_INDEX);\r\npdsp->command =\r\nknav_queue_map_reg(kdev, child,\r\nKNAV_QUEUE_PDSP_CMD_REG_INDEX);\r\nif (IS_ERR(pdsp->command) || IS_ERR(pdsp->iram) ||\r\nIS_ERR(pdsp->regs) || IS_ERR(pdsp->intd)) {\r\ndev_err(dev, "failed to map pdsp %s regs\n",\r\npdsp->name);\r\nif (!IS_ERR(pdsp->command))\r\ndevm_iounmap(dev, pdsp->command);\r\nif (!IS_ERR(pdsp->iram))\r\ndevm_iounmap(dev, pdsp->iram);\r\nif (!IS_ERR(pdsp->regs))\r\ndevm_iounmap(dev, pdsp->regs);\r\nif (!IS_ERR(pdsp->intd))\r\ndevm_iounmap(dev, pdsp->intd);\r\ndevm_kfree(dev, pdsp);\r\ncontinue;\r\n}\r\nof_property_read_u32(child, "id", &pdsp->id);\r\nlist_add_tail(&pdsp->list, &kdev->pdsps);\r\ndev_dbg(dev, "added pdsp %s: command %p, iram %p, regs %p, intd %p, firmware %s\n",\r\npdsp->name, pdsp->command, pdsp->iram, pdsp->regs,\r\npdsp->intd, pdsp->firmware);\r\n}\r\nreturn 0;\r\n}\r\nstatic int knav_queue_stop_pdsp(struct knav_device *kdev,\r\nstruct knav_pdsp_info *pdsp)\r\n{\r\nu32 val, timeout = 1000;\r\nint ret;\r\nval = readl_relaxed(&pdsp->regs->control) & ~PDSP_CTRL_ENABLE;\r\nwritel_relaxed(val, &pdsp->regs->control);\r\nret = knav_queue_pdsp_wait(&pdsp->regs->control, timeout,\r\nPDSP_CTRL_RUNNING);\r\nif (ret < 0) {\r\ndev_err(kdev->dev, "timed out on pdsp %s stop\n", pdsp->name);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int knav_queue_load_pdsp(struct knav_device *kdev,\r\nstruct knav_pdsp_info *pdsp)\r\n{\r\nint i, ret, fwlen;\r\nconst struct firmware *fw;\r\nu32 *fwdata;\r\nret = request_firmware(&fw, pdsp->firmware, kdev->dev);\r\nif (ret) {\r\ndev_err(kdev->dev, "failed to get firmware %s for pdsp %s\n",\r\npdsp->firmware, pdsp->name);\r\nreturn ret;\r\n}\r\nwritel_relaxed(pdsp->id + 1, pdsp->command + 0x18);\r\nfwdata = (u32 *)fw->data;\r\nfwlen = (fw->size + sizeof(u32) - 1) / sizeof(u32);\r\nfor (i = 0; i < fwlen; i++)\r\nwritel_relaxed(be32_to_cpu(fwdata[i]), pdsp->iram + i);\r\nrelease_firmware(fw);\r\nreturn 0;\r\n}\r\nstatic int knav_queue_start_pdsp(struct knav_device *kdev,\r\nstruct knav_pdsp_info *pdsp)\r\n{\r\nu32 val, timeout = 1000;\r\nint ret;\r\nwritel_relaxed(0xffffffff, pdsp->command);\r\nwhile (readl_relaxed(pdsp->command) != 0xffffffff)\r\ncpu_relax();\r\nval = readl_relaxed(&pdsp->regs->control);\r\nval &= ~(PDSP_CTRL_PC_MASK | PDSP_CTRL_SOFT_RESET);\r\nwritel_relaxed(val, &pdsp->regs->control);\r\nval = readl_relaxed(&pdsp->regs->control) | PDSP_CTRL_ENABLE;\r\nwritel_relaxed(val, &pdsp->regs->control);\r\nret = knav_queue_pdsp_wait(pdsp->command, timeout, 0);\r\nif (ret < 0) {\r\ndev_err(kdev->dev,\r\n"timed out on pdsp %s command register wait\n",\r\npdsp->name);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void knav_queue_stop_pdsps(struct knav_device *kdev)\r\n{\r\nstruct knav_pdsp_info *pdsp;\r\nfor_each_pdsp(kdev, pdsp)\r\nknav_queue_stop_pdsp(kdev, pdsp);\r\n}\r\nstatic int knav_queue_start_pdsps(struct knav_device *kdev)\r\n{\r\nstruct knav_pdsp_info *pdsp;\r\nint ret;\r\nknav_queue_stop_pdsps(kdev);\r\nfor_each_pdsp(kdev, pdsp) {\r\nret = knav_queue_load_pdsp(kdev, pdsp);\r\nif (ret < 0)\r\nreturn ret;\r\n}\r\nfor_each_pdsp(kdev, pdsp) {\r\nret = knav_queue_start_pdsp(kdev, pdsp);\r\nWARN_ON(ret);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline struct knav_qmgr_info *knav_find_qmgr(unsigned id)\r\n{\r\nstruct knav_qmgr_info *qmgr;\r\nfor_each_qmgr(kdev, qmgr) {\r\nif ((id >= qmgr->start_queue) &&\r\n(id < qmgr->start_queue + qmgr->num_queues))\r\nreturn qmgr;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int knav_queue_init_queue(struct knav_device *kdev,\r\nstruct knav_range_info *range,\r\nstruct knav_queue_inst *inst,\r\nunsigned id)\r\n{\r\nchar irq_name[KNAV_NAME_SIZE];\r\ninst->qmgr = knav_find_qmgr(id);\r\nif (!inst->qmgr)\r\nreturn -1;\r\nINIT_LIST_HEAD(&inst->handles);\r\ninst->kdev = kdev;\r\ninst->range = range;\r\ninst->irq_num = -1;\r\ninst->id = id;\r\nscnprintf(irq_name, sizeof(irq_name), "hwqueue-%d", id);\r\ninst->irq_name = kstrndup(irq_name, sizeof(irq_name), GFP_KERNEL);\r\nif (range->ops && range->ops->init_queue)\r\nreturn range->ops->init_queue(range, inst);\r\nelse\r\nreturn 0;\r\n}\r\nstatic int knav_queue_init_queues(struct knav_device *kdev)\r\n{\r\nstruct knav_range_info *range;\r\nint size, id, base_idx;\r\nint idx = 0, ret = 0;\r\nsize = sizeof(struct knav_queue_inst);\r\nkdev->inst_shift = order_base_2(size);\r\nsize = (1 << kdev->inst_shift) * kdev->num_queues_in_use;\r\nkdev->instances = devm_kzalloc(kdev->dev, size, GFP_KERNEL);\r\nif (!kdev->instances)\r\nreturn -ENOMEM;\r\nfor_each_queue_range(kdev, range) {\r\nif (range->ops && range->ops->init_range)\r\nrange->ops->init_range(range);\r\nbase_idx = idx;\r\nfor (id = range->queue_base;\r\nid < range->queue_base + range->num_queues; id++, idx++) {\r\nret = knav_queue_init_queue(kdev, range,\r\nknav_queue_idx_to_inst(kdev, idx), id);\r\nif (ret < 0)\r\nreturn ret;\r\n}\r\nrange->queue_base_inst =\r\nknav_queue_idx_to_inst(kdev, base_idx);\r\n}\r\nreturn 0;\r\n}\r\nstatic int knav_queue_probe(struct platform_device *pdev)\r\n{\r\nstruct device_node *node = pdev->dev.of_node;\r\nstruct device_node *qmgrs, *queue_pools, *regions, *pdsps;\r\nstruct device *dev = &pdev->dev;\r\nu32 temp[2];\r\nint ret;\r\nif (!node) {\r\ndev_err(dev, "device tree info unavailable\n");\r\nreturn -ENODEV;\r\n}\r\nkdev = devm_kzalloc(dev, sizeof(struct knav_device), GFP_KERNEL);\r\nif (!kdev) {\r\ndev_err(dev, "memory allocation failed\n");\r\nreturn -ENOMEM;\r\n}\r\nplatform_set_drvdata(pdev, kdev);\r\nkdev->dev = dev;\r\nINIT_LIST_HEAD(&kdev->queue_ranges);\r\nINIT_LIST_HEAD(&kdev->qmgrs);\r\nINIT_LIST_HEAD(&kdev->pools);\r\nINIT_LIST_HEAD(&kdev->regions);\r\nINIT_LIST_HEAD(&kdev->pdsps);\r\npm_runtime_enable(&pdev->dev);\r\nret = pm_runtime_get_sync(&pdev->dev);\r\nif (ret < 0) {\r\ndev_err(dev, "Failed to enable QMSS\n");\r\nreturn ret;\r\n}\r\nif (of_property_read_u32_array(node, "queue-range", temp, 2)) {\r\ndev_err(dev, "queue-range not specified\n");\r\nret = -ENODEV;\r\ngoto err;\r\n}\r\nkdev->base_id = temp[0];\r\nkdev->num_queues = temp[1];\r\nqmgrs = of_get_child_by_name(node, "qmgrs");\r\nif (!qmgrs) {\r\ndev_err(dev, "queue manager info not specified\n");\r\nret = -ENODEV;\r\ngoto err;\r\n}\r\nret = knav_queue_init_qmgrs(kdev, qmgrs);\r\nof_node_put(qmgrs);\r\nif (ret)\r\ngoto err;\r\npdsps = of_get_child_by_name(node, "pdsps");\r\nif (pdsps) {\r\nret = knav_queue_init_pdsps(kdev, pdsps);\r\nif (ret)\r\ngoto err;\r\nret = knav_queue_start_pdsps(kdev);\r\nif (ret)\r\ngoto err;\r\n}\r\nof_node_put(pdsps);\r\nqueue_pools = of_get_child_by_name(node, "queue-pools");\r\nif (!queue_pools) {\r\ndev_err(dev, "queue-pools not specified\n");\r\nret = -ENODEV;\r\ngoto err;\r\n}\r\nret = knav_setup_queue_pools(kdev, queue_pools);\r\nof_node_put(queue_pools);\r\nif (ret)\r\ngoto err;\r\nret = knav_get_link_ram(kdev, "linkram0", &kdev->link_rams[0]);\r\nif (ret) {\r\ndev_err(kdev->dev, "could not setup linking ram\n");\r\ngoto err;\r\n}\r\nret = knav_get_link_ram(kdev, "linkram1", &kdev->link_rams[1]);\r\nif (ret) {\r\n}\r\nret = knav_queue_setup_link_ram(kdev);\r\nif (ret)\r\ngoto err;\r\nregions = of_get_child_by_name(node, "descriptor-regions");\r\nif (!regions) {\r\ndev_err(dev, "descriptor-regions not specified\n");\r\ngoto err;\r\n}\r\nret = knav_queue_setup_regions(kdev, regions);\r\nof_node_put(regions);\r\nif (ret)\r\ngoto err;\r\nret = knav_queue_init_queues(kdev);\r\nif (ret < 0) {\r\ndev_err(dev, "hwqueue initialization failed\n");\r\ngoto err;\r\n}\r\ndebugfs_create_file("qmss", S_IFREG | S_IRUGO, NULL, NULL,\r\n&knav_queue_debug_ops);\r\nreturn 0;\r\nerr:\r\nknav_queue_stop_pdsps(kdev);\r\nknav_queue_free_regions(kdev);\r\nknav_free_queue_ranges(kdev);\r\npm_runtime_put_sync(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nreturn ret;\r\n}\r\nstatic int knav_queue_remove(struct platform_device *pdev)\r\n{\r\npm_runtime_put_sync(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nreturn 0;\r\n}
