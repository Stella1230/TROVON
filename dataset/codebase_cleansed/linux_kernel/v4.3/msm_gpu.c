static void bs_init(struct msm_gpu *gpu)\r\n{\r\nif (gpu->bus_scale_table) {\r\ngpu->bsc = msm_bus_scale_register_client(gpu->bus_scale_table);\r\nDBG("bus scale client: %08x", gpu->bsc);\r\n}\r\n}\r\nstatic void bs_fini(struct msm_gpu *gpu)\r\n{\r\nif (gpu->bsc) {\r\nmsm_bus_scale_unregister_client(gpu->bsc);\r\ngpu->bsc = 0;\r\n}\r\n}\r\nstatic void bs_set(struct msm_gpu *gpu, int idx)\r\n{\r\nif (gpu->bsc) {\r\nDBG("set bus scaling: %d", idx);\r\nmsm_bus_scale_client_update_request(gpu->bsc, idx);\r\n}\r\n}\r\nstatic void bs_init(struct msm_gpu *gpu) {}\r\nstatic void bs_fini(struct msm_gpu *gpu) {}\r\nstatic void bs_set(struct msm_gpu *gpu, int idx) {}\r\nstatic int enable_pwrrail(struct msm_gpu *gpu)\r\n{\r\nstruct drm_device *dev = gpu->dev;\r\nint ret = 0;\r\nif (gpu->gpu_reg) {\r\nret = regulator_enable(gpu->gpu_reg);\r\nif (ret) {\r\ndev_err(dev->dev, "failed to enable 'gpu_reg': %d\n", ret);\r\nreturn ret;\r\n}\r\n}\r\nif (gpu->gpu_cx) {\r\nret = regulator_enable(gpu->gpu_cx);\r\nif (ret) {\r\ndev_err(dev->dev, "failed to enable 'gpu_cx': %d\n", ret);\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int disable_pwrrail(struct msm_gpu *gpu)\r\n{\r\nif (gpu->gpu_cx)\r\nregulator_disable(gpu->gpu_cx);\r\nif (gpu->gpu_reg)\r\nregulator_disable(gpu->gpu_reg);\r\nreturn 0;\r\n}\r\nstatic int enable_clk(struct msm_gpu *gpu)\r\n{\r\nstruct clk *rate_clk = NULL;\r\nint i;\r\nfor (i = ARRAY_SIZE(gpu->grp_clks) - 1; i > 0; i--) {\r\nif (gpu->grp_clks[i]) {\r\nclk_prepare(gpu->grp_clks[i]);\r\nrate_clk = gpu->grp_clks[i];\r\n}\r\n}\r\nif (rate_clk && gpu->fast_rate)\r\nclk_set_rate(rate_clk, gpu->fast_rate);\r\nfor (i = ARRAY_SIZE(gpu->grp_clks) - 1; i > 0; i--)\r\nif (gpu->grp_clks[i])\r\nclk_enable(gpu->grp_clks[i]);\r\nreturn 0;\r\n}\r\nstatic int disable_clk(struct msm_gpu *gpu)\r\n{\r\nstruct clk *rate_clk = NULL;\r\nint i;\r\nfor (i = ARRAY_SIZE(gpu->grp_clks) - 1; i > 0; i--) {\r\nif (gpu->grp_clks[i]) {\r\nclk_disable(gpu->grp_clks[i]);\r\nrate_clk = gpu->grp_clks[i];\r\n}\r\n}\r\nif (rate_clk && gpu->slow_rate)\r\nclk_set_rate(rate_clk, gpu->slow_rate);\r\nfor (i = ARRAY_SIZE(gpu->grp_clks) - 1; i > 0; i--)\r\nif (gpu->grp_clks[i])\r\nclk_unprepare(gpu->grp_clks[i]);\r\nreturn 0;\r\n}\r\nstatic int enable_axi(struct msm_gpu *gpu)\r\n{\r\nif (gpu->ebi1_clk)\r\nclk_prepare_enable(gpu->ebi1_clk);\r\nif (gpu->bus_freq)\r\nbs_set(gpu, gpu->bus_freq);\r\nreturn 0;\r\n}\r\nstatic int disable_axi(struct msm_gpu *gpu)\r\n{\r\nif (gpu->ebi1_clk)\r\nclk_disable_unprepare(gpu->ebi1_clk);\r\nif (gpu->bus_freq)\r\nbs_set(gpu, 0);\r\nreturn 0;\r\n}\r\nint msm_gpu_pm_resume(struct msm_gpu *gpu)\r\n{\r\nstruct drm_device *dev = gpu->dev;\r\nint ret;\r\nDBG("%s: active_cnt=%d", gpu->name, gpu->active_cnt);\r\nWARN_ON(!mutex_is_locked(&dev->struct_mutex));\r\nif (gpu->active_cnt++ > 0)\r\nreturn 0;\r\nif (WARN_ON(gpu->active_cnt <= 0))\r\nreturn -EINVAL;\r\nret = enable_pwrrail(gpu);\r\nif (ret)\r\nreturn ret;\r\nret = enable_clk(gpu);\r\nif (ret)\r\nreturn ret;\r\nret = enable_axi(gpu);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nint msm_gpu_pm_suspend(struct msm_gpu *gpu)\r\n{\r\nstruct drm_device *dev = gpu->dev;\r\nint ret;\r\nDBG("%s: active_cnt=%d", gpu->name, gpu->active_cnt);\r\nWARN_ON(!mutex_is_locked(&dev->struct_mutex));\r\nif (--gpu->active_cnt > 0)\r\nreturn 0;\r\nif (WARN_ON(gpu->active_cnt < 0))\r\nreturn -EINVAL;\r\nret = disable_axi(gpu);\r\nif (ret)\r\nreturn ret;\r\nret = disable_clk(gpu);\r\nif (ret)\r\nreturn ret;\r\nret = disable_pwrrail(gpu);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nstatic void inactive_worker(struct work_struct *work)\r\n{\r\nstruct msm_gpu *gpu = container_of(work, struct msm_gpu, inactive_work);\r\nstruct drm_device *dev = gpu->dev;\r\nif (gpu->inactive)\r\nreturn;\r\nDBG("%s: inactive!\n", gpu->name);\r\nmutex_lock(&dev->struct_mutex);\r\nif (!(msm_gpu_active(gpu) || gpu->inactive)) {\r\ndisable_axi(gpu);\r\ndisable_clk(gpu);\r\ngpu->inactive = true;\r\n}\r\nmutex_unlock(&dev->struct_mutex);\r\n}\r\nstatic void inactive_handler(unsigned long data)\r\n{\r\nstruct msm_gpu *gpu = (struct msm_gpu *)data;\r\nstruct msm_drm_private *priv = gpu->dev->dev_private;\r\nqueue_work(priv->wq, &gpu->inactive_work);\r\n}\r\nstatic void inactive_cancel(struct msm_gpu *gpu)\r\n{\r\nDBG("%s", gpu->name);\r\ndel_timer(&gpu->inactive_timer);\r\nif (gpu->inactive) {\r\nenable_clk(gpu);\r\nenable_axi(gpu);\r\ngpu->inactive = false;\r\n}\r\n}\r\nstatic void inactive_start(struct msm_gpu *gpu)\r\n{\r\nDBG("%s", gpu->name);\r\nmod_timer(&gpu->inactive_timer,\r\nround_jiffies_up(jiffies + DRM_MSM_INACTIVE_JIFFIES));\r\n}\r\nstatic void recover_worker(struct work_struct *work)\r\n{\r\nstruct msm_gpu *gpu = container_of(work, struct msm_gpu, recover_work);\r\nstruct drm_device *dev = gpu->dev;\r\ndev_err(dev->dev, "%s: hangcheck recover!\n", gpu->name);\r\nmutex_lock(&dev->struct_mutex);\r\nif (msm_gpu_active(gpu)) {\r\nstruct msm_gem_submit *submit;\r\nuint32_t fence = gpu->funcs->last_fence(gpu);\r\nretire_submits(gpu, fence + 1);\r\ninactive_cancel(gpu);\r\ngpu->funcs->recover(gpu);\r\nlist_for_each_entry(submit, &gpu->submit_list, node) {\r\ngpu->funcs->submit(gpu, submit, NULL);\r\n}\r\n}\r\nmutex_unlock(&dev->struct_mutex);\r\nmsm_gpu_retire(gpu);\r\n}\r\nstatic void hangcheck_timer_reset(struct msm_gpu *gpu)\r\n{\r\nDBG("%s", gpu->name);\r\nmod_timer(&gpu->hangcheck_timer,\r\nround_jiffies_up(jiffies + DRM_MSM_HANGCHECK_JIFFIES));\r\n}\r\nstatic void hangcheck_handler(unsigned long data)\r\n{\r\nstruct msm_gpu *gpu = (struct msm_gpu *)data;\r\nstruct drm_device *dev = gpu->dev;\r\nstruct msm_drm_private *priv = dev->dev_private;\r\nuint32_t fence = gpu->funcs->last_fence(gpu);\r\nif (fence != gpu->hangcheck_fence) {\r\ngpu->hangcheck_fence = fence;\r\n} else if (fence < gpu->submitted_fence) {\r\ngpu->hangcheck_fence = fence;\r\ndev_err(dev->dev, "%s: hangcheck detected gpu lockup!\n",\r\ngpu->name);\r\ndev_err(dev->dev, "%s: completed fence: %u\n",\r\ngpu->name, fence);\r\ndev_err(dev->dev, "%s: submitted fence: %u\n",\r\ngpu->name, gpu->submitted_fence);\r\nqueue_work(priv->wq, &gpu->recover_work);\r\n}\r\nif (gpu->submitted_fence > gpu->hangcheck_fence)\r\nhangcheck_timer_reset(gpu);\r\nqueue_work(priv->wq, &gpu->retire_work);\r\n}\r\nstatic int update_hw_cntrs(struct msm_gpu *gpu, uint32_t ncntrs, uint32_t *cntrs)\r\n{\r\nuint32_t current_cntrs[ARRAY_SIZE(gpu->last_cntrs)];\r\nint i, n = min(ncntrs, gpu->num_perfcntrs);\r\nfor (i = 0; i < gpu->num_perfcntrs; i++)\r\ncurrent_cntrs[i] = gpu_read(gpu, gpu->perfcntrs[i].sample_reg);\r\nfor (i = 0; i < n; i++)\r\ncntrs[i] = current_cntrs[i] - gpu->last_cntrs[i];\r\nfor (i = 0; i < gpu->num_perfcntrs; i++)\r\ngpu->last_cntrs[i] = current_cntrs[i];\r\nreturn n;\r\n}\r\nstatic void update_sw_cntrs(struct msm_gpu *gpu)\r\n{\r\nktime_t time;\r\nuint32_t elapsed;\r\nunsigned long flags;\r\nspin_lock_irqsave(&gpu->perf_lock, flags);\r\nif (!gpu->perfcntr_active)\r\ngoto out;\r\ntime = ktime_get();\r\nelapsed = ktime_to_us(ktime_sub(time, gpu->last_sample.time));\r\ngpu->totaltime += elapsed;\r\nif (gpu->last_sample.active)\r\ngpu->activetime += elapsed;\r\ngpu->last_sample.active = msm_gpu_active(gpu);\r\ngpu->last_sample.time = time;\r\nout:\r\nspin_unlock_irqrestore(&gpu->perf_lock, flags);\r\n}\r\nvoid msm_gpu_perfcntr_start(struct msm_gpu *gpu)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&gpu->perf_lock, flags);\r\ngpu->last_sample.active = msm_gpu_active(gpu);\r\ngpu->last_sample.time = ktime_get();\r\ngpu->activetime = gpu->totaltime = 0;\r\ngpu->perfcntr_active = true;\r\nupdate_hw_cntrs(gpu, 0, NULL);\r\nspin_unlock_irqrestore(&gpu->perf_lock, flags);\r\n}\r\nvoid msm_gpu_perfcntr_stop(struct msm_gpu *gpu)\r\n{\r\ngpu->perfcntr_active = false;\r\n}\r\nint msm_gpu_perfcntr_sample(struct msm_gpu *gpu, uint32_t *activetime,\r\nuint32_t *totaltime, uint32_t ncntrs, uint32_t *cntrs)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(&gpu->perf_lock, flags);\r\nif (!gpu->perfcntr_active) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\n*activetime = gpu->activetime;\r\n*totaltime = gpu->totaltime;\r\ngpu->activetime = gpu->totaltime = 0;\r\nret = update_hw_cntrs(gpu, ncntrs, cntrs);\r\nout:\r\nspin_unlock_irqrestore(&gpu->perf_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void retire_submits(struct msm_gpu *gpu, uint32_t fence)\r\n{\r\nstruct drm_device *dev = gpu->dev;\r\nWARN_ON(!mutex_is_locked(&dev->struct_mutex));\r\nwhile (!list_empty(&gpu->submit_list)) {\r\nstruct msm_gem_submit *submit;\r\nsubmit = list_first_entry(&gpu->submit_list,\r\nstruct msm_gem_submit, node);\r\nif (submit->fence <= fence) {\r\nlist_del(&submit->node);\r\nkfree(submit);\r\n} else {\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void retire_worker(struct work_struct *work)\r\n{\r\nstruct msm_gpu *gpu = container_of(work, struct msm_gpu, retire_work);\r\nstruct drm_device *dev = gpu->dev;\r\nuint32_t fence = gpu->funcs->last_fence(gpu);\r\nmsm_update_fence(gpu->dev, fence);\r\nmutex_lock(&dev->struct_mutex);\r\nretire_submits(gpu, fence);\r\nwhile (!list_empty(&gpu->active_list)) {\r\nstruct msm_gem_object *obj;\r\nobj = list_first_entry(&gpu->active_list,\r\nstruct msm_gem_object, mm_list);\r\nif ((obj->read_fence <= fence) &&\r\n(obj->write_fence <= fence)) {\r\nmsm_gem_move_to_inactive(&obj->base);\r\nmsm_gem_put_iova(&obj->base, gpu->id);\r\ndrm_gem_object_unreference(&obj->base);\r\n} else {\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&dev->struct_mutex);\r\nif (!msm_gpu_active(gpu))\r\ninactive_start(gpu);\r\n}\r\nvoid msm_gpu_retire(struct msm_gpu *gpu)\r\n{\r\nstruct msm_drm_private *priv = gpu->dev->dev_private;\r\nqueue_work(priv->wq, &gpu->retire_work);\r\nupdate_sw_cntrs(gpu);\r\n}\r\nint msm_gpu_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit,\r\nstruct msm_file_private *ctx)\r\n{\r\nstruct drm_device *dev = gpu->dev;\r\nstruct msm_drm_private *priv = dev->dev_private;\r\nint i, ret;\r\nWARN_ON(!mutex_is_locked(&dev->struct_mutex));\r\nsubmit->fence = ++priv->next_fence;\r\ngpu->submitted_fence = submit->fence;\r\ninactive_cancel(gpu);\r\nlist_add_tail(&submit->node, &gpu->submit_list);\r\nmsm_rd_dump_submit(submit);\r\ngpu->submitted_fence = submit->fence;\r\nupdate_sw_cntrs(gpu);\r\nfor (i = 0; i < submit->nr_bos; i++) {\r\nstruct msm_gem_object *msm_obj = submit->bos[i].obj;\r\nWARN_ON(is_active(msm_obj) && (msm_obj->gpu != gpu));\r\nif (!is_active(msm_obj)) {\r\nuint32_t iova;\r\ndrm_gem_object_reference(&msm_obj->base);\r\nmsm_gem_get_iova_locked(&msm_obj->base,\r\nsubmit->gpu->id, &iova);\r\n}\r\nif (submit->bos[i].flags & MSM_SUBMIT_BO_READ)\r\nmsm_gem_move_to_active(&msm_obj->base, gpu, false, submit->fence);\r\nif (submit->bos[i].flags & MSM_SUBMIT_BO_WRITE)\r\nmsm_gem_move_to_active(&msm_obj->base, gpu, true, submit->fence);\r\n}\r\nret = gpu->funcs->submit(gpu, submit, ctx);\r\npriv->lastctx = ctx;\r\nhangcheck_timer_reset(gpu);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t irq_handler(int irq, void *data)\r\n{\r\nstruct msm_gpu *gpu = data;\r\nreturn gpu->funcs->irq(gpu);\r\n}\r\nint msm_gpu_init(struct drm_device *drm, struct platform_device *pdev,\r\nstruct msm_gpu *gpu, const struct msm_gpu_funcs *funcs,\r\nconst char *name, const char *ioname, const char *irqname, int ringsz)\r\n{\r\nstruct iommu_domain *iommu;\r\nint i, ret;\r\nif (WARN_ON(gpu->num_perfcntrs > ARRAY_SIZE(gpu->last_cntrs)))\r\ngpu->num_perfcntrs = ARRAY_SIZE(gpu->last_cntrs);\r\ngpu->dev = drm;\r\ngpu->funcs = funcs;\r\ngpu->name = name;\r\ngpu->inactive = true;\r\nINIT_LIST_HEAD(&gpu->active_list);\r\nINIT_WORK(&gpu->retire_work, retire_worker);\r\nINIT_WORK(&gpu->inactive_work, inactive_worker);\r\nINIT_WORK(&gpu->recover_work, recover_worker);\r\nINIT_LIST_HEAD(&gpu->submit_list);\r\nsetup_timer(&gpu->inactive_timer, inactive_handler,\r\n(unsigned long)gpu);\r\nsetup_timer(&gpu->hangcheck_timer, hangcheck_handler,\r\n(unsigned long)gpu);\r\nspin_lock_init(&gpu->perf_lock);\r\nBUG_ON(ARRAY_SIZE(clk_names) != ARRAY_SIZE(gpu->grp_clks));\r\ngpu->mmio = msm_ioremap(pdev, ioname, name);\r\nif (IS_ERR(gpu->mmio)) {\r\nret = PTR_ERR(gpu->mmio);\r\ngoto fail;\r\n}\r\ngpu->irq = platform_get_irq_byname(pdev, irqname);\r\nif (gpu->irq < 0) {\r\nret = gpu->irq;\r\ndev_err(drm->dev, "failed to get irq: %d\n", ret);\r\ngoto fail;\r\n}\r\nret = devm_request_irq(&pdev->dev, gpu->irq, irq_handler,\r\nIRQF_TRIGGER_HIGH, gpu->name, gpu);\r\nif (ret) {\r\ndev_err(drm->dev, "failed to request IRQ%u: %d\n", gpu->irq, ret);\r\ngoto fail;\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(clk_names); i++) {\r\ngpu->grp_clks[i] = devm_clk_get(&pdev->dev, clk_names[i]);\r\nDBG("grp_clks[%s]: %p", clk_names[i], gpu->grp_clks[i]);\r\nif (IS_ERR(gpu->grp_clks[i]))\r\ngpu->grp_clks[i] = NULL;\r\n}\r\ngpu->ebi1_clk = devm_clk_get(&pdev->dev, "bus_clk");\r\nDBG("ebi1_clk: %p", gpu->ebi1_clk);\r\nif (IS_ERR(gpu->ebi1_clk))\r\ngpu->ebi1_clk = NULL;\r\ngpu->gpu_reg = devm_regulator_get(&pdev->dev, "vdd");\r\nDBG("gpu_reg: %p", gpu->gpu_reg);\r\nif (IS_ERR(gpu->gpu_reg))\r\ngpu->gpu_reg = NULL;\r\ngpu->gpu_cx = devm_regulator_get(&pdev->dev, "vddcx");\r\nDBG("gpu_cx: %p", gpu->gpu_cx);\r\nif (IS_ERR(gpu->gpu_cx))\r\ngpu->gpu_cx = NULL;\r\niommu = iommu_domain_alloc(&platform_bus_type);\r\nif (iommu) {\r\ndev_info(drm->dev, "%s: using IOMMU\n", name);\r\ngpu->mmu = msm_iommu_new(&pdev->dev, iommu);\r\n} else {\r\ndev_info(drm->dev, "%s: no IOMMU, fallback to VRAM carveout!\n", name);\r\n}\r\ngpu->id = msm_register_mmu(drm, gpu->mmu);\r\nmutex_lock(&drm->struct_mutex);\r\ngpu->rb = msm_ringbuffer_new(gpu, ringsz);\r\nmutex_unlock(&drm->struct_mutex);\r\nif (IS_ERR(gpu->rb)) {\r\nret = PTR_ERR(gpu->rb);\r\ngpu->rb = NULL;\r\ndev_err(drm->dev, "could not create ringbuffer: %d\n", ret);\r\ngoto fail;\r\n}\r\nbs_init(gpu);\r\nreturn 0;\r\nfail:\r\nreturn ret;\r\n}\r\nvoid msm_gpu_cleanup(struct msm_gpu *gpu)\r\n{\r\nDBG("%s", gpu->name);\r\nWARN_ON(!list_empty(&gpu->active_list));\r\nbs_fini(gpu);\r\nif (gpu->rb) {\r\nif (gpu->rb_iova)\r\nmsm_gem_put_iova(gpu->rb->bo, gpu->id);\r\nmsm_ringbuffer_destroy(gpu->rb);\r\n}\r\nif (gpu->mmu)\r\ngpu->mmu->funcs->destroy(gpu->mmu);\r\n}
