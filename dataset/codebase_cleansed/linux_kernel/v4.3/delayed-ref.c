static int comp_tree_refs(struct btrfs_delayed_tree_ref *ref2,\r\nstruct btrfs_delayed_tree_ref *ref1, int type)\r\n{\r\nif (type == BTRFS_TREE_BLOCK_REF_KEY) {\r\nif (ref1->root < ref2->root)\r\nreturn -1;\r\nif (ref1->root > ref2->root)\r\nreturn 1;\r\n} else {\r\nif (ref1->parent < ref2->parent)\r\nreturn -1;\r\nif (ref1->parent > ref2->parent)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int comp_data_refs(struct btrfs_delayed_data_ref *ref2,\r\nstruct btrfs_delayed_data_ref *ref1)\r\n{\r\nif (ref1->node.type == BTRFS_EXTENT_DATA_REF_KEY) {\r\nif (ref1->root < ref2->root)\r\nreturn -1;\r\nif (ref1->root > ref2->root)\r\nreturn 1;\r\nif (ref1->objectid < ref2->objectid)\r\nreturn -1;\r\nif (ref1->objectid > ref2->objectid)\r\nreturn 1;\r\nif (ref1->offset < ref2->offset)\r\nreturn -1;\r\nif (ref1->offset > ref2->offset)\r\nreturn 1;\r\n} else {\r\nif (ref1->parent < ref2->parent)\r\nreturn -1;\r\nif (ref1->parent > ref2->parent)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct btrfs_delayed_ref_head *htree_insert(struct rb_root *root,\r\nstruct rb_node *node)\r\n{\r\nstruct rb_node **p = &root->rb_node;\r\nstruct rb_node *parent_node = NULL;\r\nstruct btrfs_delayed_ref_head *entry;\r\nstruct btrfs_delayed_ref_head *ins;\r\nu64 bytenr;\r\nins = rb_entry(node, struct btrfs_delayed_ref_head, href_node);\r\nbytenr = ins->node.bytenr;\r\nwhile (*p) {\r\nparent_node = *p;\r\nentry = rb_entry(parent_node, struct btrfs_delayed_ref_head,\r\nhref_node);\r\nif (bytenr < entry->node.bytenr)\r\np = &(*p)->rb_left;\r\nelse if (bytenr > entry->node.bytenr)\r\np = &(*p)->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nrb_link_node(node, parent_node, p);\r\nrb_insert_color(node, root);\r\nreturn NULL;\r\n}\r\nstatic struct btrfs_delayed_ref_head *\r\nfind_ref_head(struct rb_root *root, u64 bytenr,\r\nint return_bigger)\r\n{\r\nstruct rb_node *n;\r\nstruct btrfs_delayed_ref_head *entry;\r\nn = root->rb_node;\r\nentry = NULL;\r\nwhile (n) {\r\nentry = rb_entry(n, struct btrfs_delayed_ref_head, href_node);\r\nif (bytenr < entry->node.bytenr)\r\nn = n->rb_left;\r\nelse if (bytenr > entry->node.bytenr)\r\nn = n->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nif (entry && return_bigger) {\r\nif (bytenr > entry->node.bytenr) {\r\nn = rb_next(&entry->href_node);\r\nif (!n)\r\nn = rb_first(root);\r\nentry = rb_entry(n, struct btrfs_delayed_ref_head,\r\nhref_node);\r\nreturn entry;\r\n}\r\nreturn entry;\r\n}\r\nreturn NULL;\r\n}\r\nint btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_head *head)\r\n{\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nassert_spin_locked(&delayed_refs->lock);\r\nif (mutex_trylock(&head->mutex))\r\nreturn 0;\r\natomic_inc(&head->node.refs);\r\nspin_unlock(&delayed_refs->lock);\r\nmutex_lock(&head->mutex);\r\nspin_lock(&delayed_refs->lock);\r\nif (!head->node.in_tree) {\r\nmutex_unlock(&head->mutex);\r\nbtrfs_put_delayed_ref(&head->node);\r\nreturn -EAGAIN;\r\n}\r\nbtrfs_put_delayed_ref(&head->node);\r\nreturn 0;\r\n}\r\nstatic inline void drop_delayed_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_head *head,\r\nstruct btrfs_delayed_ref_node *ref)\r\n{\r\nif (btrfs_delayed_ref_is_head(ref)) {\r\nhead = btrfs_delayed_node_to_head(ref);\r\nrb_erase(&head->href_node, &delayed_refs->href_root);\r\n} else {\r\nassert_spin_locked(&head->lock);\r\nlist_del(&ref->list);\r\n}\r\nref->in_tree = 0;\r\nbtrfs_put_delayed_ref(ref);\r\natomic_dec(&delayed_refs->num_entries);\r\nif (trans->delayed_ref_updates)\r\ntrans->delayed_ref_updates--;\r\n}\r\nint btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nu64 seq)\r\n{\r\nstruct seq_list *elem;\r\nint ret = 0;\r\nspin_lock(&fs_info->tree_mod_seq_lock);\r\nif (!list_empty(&fs_info->tree_mod_seq_list)) {\r\nelem = list_first_entry(&fs_info->tree_mod_seq_list,\r\nstruct seq_list, list);\r\nif (seq >= elem->seq) {\r\npr_debug("holding back delayed_ref %#x.%x, lowest is %#x.%x (%p)\n",\r\n(u32)(seq >> 32), (u32)seq,\r\n(u32)(elem->seq >> 32), (u32)elem->seq,\r\ndelayed_refs);\r\nret = 1;\r\n}\r\n}\r\nspin_unlock(&fs_info->tree_mod_seq_lock);\r\nreturn ret;\r\n}\r\nstruct btrfs_delayed_ref_head *\r\nbtrfs_select_ref_head(struct btrfs_trans_handle *trans)\r\n{\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nstruct btrfs_delayed_ref_head *head;\r\nu64 start;\r\nbool loop = false;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nagain:\r\nstart = delayed_refs->run_delayed_start;\r\nhead = find_ref_head(&delayed_refs->href_root, start, 1);\r\nif (!head && !loop) {\r\ndelayed_refs->run_delayed_start = 0;\r\nstart = 0;\r\nloop = true;\r\nhead = find_ref_head(&delayed_refs->href_root, start, 1);\r\nif (!head)\r\nreturn NULL;\r\n} else if (!head && loop) {\r\nreturn NULL;\r\n}\r\nwhile (head->processing) {\r\nstruct rb_node *node;\r\nnode = rb_next(&head->href_node);\r\nif (!node) {\r\nif (loop)\r\nreturn NULL;\r\ndelayed_refs->run_delayed_start = 0;\r\nstart = 0;\r\nloop = true;\r\ngoto again;\r\n}\r\nhead = rb_entry(node, struct btrfs_delayed_ref_head,\r\nhref_node);\r\n}\r\nhead->processing = 1;\r\nWARN_ON(delayed_refs->num_heads_ready == 0);\r\ndelayed_refs->num_heads_ready--;\r\ndelayed_refs->run_delayed_start = head->node.bytenr +\r\nhead->node.num_bytes;\r\nreturn head;\r\n}\r\nstatic int\r\nadd_delayed_ref_tail_merge(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_root *root,\r\nstruct btrfs_delayed_ref_head *href,\r\nstruct btrfs_delayed_ref_node *ref)\r\n{\r\nstruct btrfs_delayed_ref_node *exist;\r\nint mod;\r\nint ret = 0;\r\nspin_lock(&href->lock);\r\nif (list_empty(&href->ref_list))\r\ngoto add_tail;\r\nexist = list_entry(href->ref_list.prev, struct btrfs_delayed_ref_node,\r\nlist);\r\nif (exist->type != ref->type || exist->no_quota != ref->no_quota ||\r\nexist->seq != ref->seq)\r\ngoto add_tail;\r\nif ((exist->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nexist->type == BTRFS_SHARED_BLOCK_REF_KEY) &&\r\ncomp_tree_refs(btrfs_delayed_node_to_tree_ref(exist),\r\nbtrfs_delayed_node_to_tree_ref(ref),\r\nref->type))\r\ngoto add_tail;\r\nif ((exist->type == BTRFS_EXTENT_DATA_REF_KEY ||\r\nexist->type == BTRFS_SHARED_DATA_REF_KEY) &&\r\ncomp_data_refs(btrfs_delayed_node_to_data_ref(exist),\r\nbtrfs_delayed_node_to_data_ref(ref)))\r\ngoto add_tail;\r\nret = 1;\r\nif (exist->action == ref->action) {\r\nmod = ref->ref_mod;\r\n} else {\r\nif (exist->ref_mod < ref->ref_mod) {\r\nexist->action = ref->action;\r\nmod = -exist->ref_mod;\r\nexist->ref_mod = ref->ref_mod;\r\n} else\r\nmod = -ref->ref_mod;\r\n}\r\nexist->ref_mod += mod;\r\nif (exist->ref_mod == 0)\r\ndrop_delayed_ref(trans, root, href, exist);\r\nspin_unlock(&href->lock);\r\nreturn ret;\r\nadd_tail:\r\nlist_add_tail(&ref->list, &href->ref_list);\r\natomic_inc(&root->num_entries);\r\ntrans->delayed_ref_updates++;\r\nspin_unlock(&href->lock);\r\nreturn ret;\r\n}\r\nstatic noinline void\r\nupdate_existing_head_ref(struct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_node *existing,\r\nstruct btrfs_delayed_ref_node *update)\r\n{\r\nstruct btrfs_delayed_ref_head *existing_ref;\r\nstruct btrfs_delayed_ref_head *ref;\r\nint old_ref_mod;\r\nexisting_ref = btrfs_delayed_node_to_head(existing);\r\nref = btrfs_delayed_node_to_head(update);\r\nBUG_ON(existing_ref->is_data != ref->is_data);\r\nspin_lock(&existing_ref->lock);\r\nif (ref->must_insert_reserved) {\r\nexisting_ref->must_insert_reserved = ref->must_insert_reserved;\r\nexisting->num_bytes = update->num_bytes;\r\n}\r\nif (ref->extent_op) {\r\nif (!existing_ref->extent_op) {\r\nexisting_ref->extent_op = ref->extent_op;\r\n} else {\r\nif (ref->extent_op->update_key) {\r\nmemcpy(&existing_ref->extent_op->key,\r\n&ref->extent_op->key,\r\nsizeof(ref->extent_op->key));\r\nexisting_ref->extent_op->update_key = 1;\r\n}\r\nif (ref->extent_op->update_flags) {\r\nexisting_ref->extent_op->flags_to_set |=\r\nref->extent_op->flags_to_set;\r\nexisting_ref->extent_op->update_flags = 1;\r\n}\r\nbtrfs_free_delayed_extent_op(ref->extent_op);\r\n}\r\n}\r\nold_ref_mod = existing_ref->total_ref_mod;\r\nexisting->ref_mod += update->ref_mod;\r\nexisting_ref->total_ref_mod += update->ref_mod;\r\nif (existing_ref->is_data) {\r\nif (existing_ref->total_ref_mod >= 0 && old_ref_mod < 0)\r\ndelayed_refs->pending_csums -= existing->num_bytes;\r\nif (existing_ref->total_ref_mod < 0 && old_ref_mod >= 0)\r\ndelayed_refs->pending_csums += existing->num_bytes;\r\n}\r\nspin_unlock(&existing_ref->lock);\r\n}\r\nnoinline void\r\nadd_delayed_tree_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_head *head_ref,\r\nstruct btrfs_delayed_ref_node *ref, u64 bytenr,\r\nu64 num_bytes, u64 parent, u64 ref_root, int level,\r\nint action, int no_quota)\r\n{\r\nstruct btrfs_delayed_tree_ref *full_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nu64 seq = 0;\r\nint ret;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\naction = BTRFS_ADD_DELAYED_REF;\r\nif (is_fstree(ref_root))\r\nseq = atomic64_read(&fs_info->tree_mod_seq);\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = 1;\r\nref->action = action;\r\nref->is_head = 0;\r\nref->in_tree = 1;\r\nref->no_quota = no_quota;\r\nref->seq = seq;\r\nfull_ref = btrfs_delayed_node_to_tree_ref(ref);\r\nfull_ref->parent = parent;\r\nfull_ref->root = ref_root;\r\nif (parent)\r\nref->type = BTRFS_SHARED_BLOCK_REF_KEY;\r\nelse\r\nref->type = BTRFS_TREE_BLOCK_REF_KEY;\r\nfull_ref->level = level;\r\ntrace_add_delayed_tree_ref(ref, full_ref, action);\r\nret = add_delayed_ref_tail_merge(trans, delayed_refs, head_ref, ref);\r\nif (ret > 0)\r\nkmem_cache_free(btrfs_delayed_tree_ref_cachep, full_ref);\r\n}\r\nstatic noinline void\r\nadd_delayed_data_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_head *head_ref,\r\nstruct btrfs_delayed_ref_node *ref, u64 bytenr,\r\nu64 num_bytes, u64 parent, u64 ref_root, u64 owner,\r\nu64 offset, int action, int no_quota)\r\n{\r\nstruct btrfs_delayed_data_ref *full_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nu64 seq = 0;\r\nint ret;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\naction = BTRFS_ADD_DELAYED_REF;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nif (is_fstree(ref_root))\r\nseq = atomic64_read(&fs_info->tree_mod_seq);\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = 1;\r\nref->action = action;\r\nref->is_head = 0;\r\nref->in_tree = 1;\r\nref->no_quota = no_quota;\r\nref->seq = seq;\r\nfull_ref = btrfs_delayed_node_to_data_ref(ref);\r\nfull_ref->parent = parent;\r\nfull_ref->root = ref_root;\r\nif (parent)\r\nref->type = BTRFS_SHARED_DATA_REF_KEY;\r\nelse\r\nref->type = BTRFS_EXTENT_DATA_REF_KEY;\r\nfull_ref->objectid = owner;\r\nfull_ref->offset = offset;\r\ntrace_add_delayed_data_ref(ref, full_ref, action);\r\nret = add_delayed_ref_tail_merge(trans, delayed_refs, head_ref, ref);\r\nif (ret > 0)\r\nkmem_cache_free(btrfs_delayed_data_ref_cachep, full_ref);\r\n}\r\nint btrfs_add_delayed_tree_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes, u64 parent,\r\nu64 ref_root, int level, int action,\r\nstruct btrfs_delayed_extent_op *extent_op,\r\nint no_quota)\r\n{\r\nstruct btrfs_delayed_tree_ref *ref;\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nstruct btrfs_qgroup_extent_record *record = NULL;\r\nif (!is_fstree(ref_root) || !fs_info->quota_enabled)\r\nno_quota = 0;\r\nBUG_ON(extent_op && extent_op->is_data);\r\nref = kmem_cache_alloc(btrfs_delayed_tree_ref_cachep, GFP_NOFS);\r\nif (!ref)\r\nreturn -ENOMEM;\r\nhead_ref = kmem_cache_alloc(btrfs_delayed_ref_head_cachep, GFP_NOFS);\r\nif (!head_ref)\r\ngoto free_ref;\r\nif (fs_info->quota_enabled && is_fstree(ref_root)) {\r\nrecord = kmalloc(sizeof(*record), GFP_NOFS);\r\nif (!record)\r\ngoto free_head_ref;\r\n}\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nhead_ref = add_delayed_ref_head(fs_info, trans, &head_ref->node, record,\r\nbytenr, num_bytes, action, 0);\r\nadd_delayed_tree_ref(fs_info, trans, head_ref, &ref->node, bytenr,\r\nnum_bytes, parent, ref_root, level, action,\r\nno_quota);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\nfree_head_ref:\r\nkmem_cache_free(btrfs_delayed_ref_head_cachep, head_ref);\r\nfree_ref:\r\nkmem_cache_free(btrfs_delayed_tree_ref_cachep, ref);\r\nreturn -ENOMEM;\r\n}\r\nint btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes,\r\nu64 parent, u64 ref_root,\r\nu64 owner, u64 offset, int action,\r\nstruct btrfs_delayed_extent_op *extent_op,\r\nint no_quota)\r\n{\r\nstruct btrfs_delayed_data_ref *ref;\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nstruct btrfs_qgroup_extent_record *record = NULL;\r\nif (!is_fstree(ref_root) || !fs_info->quota_enabled)\r\nno_quota = 0;\r\nBUG_ON(extent_op && !extent_op->is_data);\r\nref = kmem_cache_alloc(btrfs_delayed_data_ref_cachep, GFP_NOFS);\r\nif (!ref)\r\nreturn -ENOMEM;\r\nhead_ref = kmem_cache_alloc(btrfs_delayed_ref_head_cachep, GFP_NOFS);\r\nif (!head_ref) {\r\nkmem_cache_free(btrfs_delayed_data_ref_cachep, ref);\r\nreturn -ENOMEM;\r\n}\r\nif (fs_info->quota_enabled && is_fstree(ref_root)) {\r\nrecord = kmalloc(sizeof(*record), GFP_NOFS);\r\nif (!record) {\r\nkmem_cache_free(btrfs_delayed_data_ref_cachep, ref);\r\nkmem_cache_free(btrfs_delayed_ref_head_cachep,\r\nhead_ref);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nhead_ref = add_delayed_ref_head(fs_info, trans, &head_ref->node, record,\r\nbytenr, num_bytes, action, 1);\r\nadd_delayed_data_ref(fs_info, trans, head_ref, &ref->node, bytenr,\r\nnum_bytes, parent, ref_root, owner, offset,\r\naction, no_quota);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\n}\r\nint btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes,\r\nstruct btrfs_delayed_extent_op *extent_op)\r\n{\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nhead_ref = kmem_cache_alloc(btrfs_delayed_ref_head_cachep, GFP_NOFS);\r\nif (!head_ref)\r\nreturn -ENOMEM;\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nadd_delayed_ref_head(fs_info, trans, &head_ref->node, NULL, bytenr,\r\nnum_bytes, BTRFS_UPDATE_DELAYED_HEAD,\r\nextent_op->is_data);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\n}\r\nstruct btrfs_delayed_ref_head *\r\nbtrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr)\r\n{\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nreturn find_ref_head(&delayed_refs->href_root, bytenr, 0);\r\n}\r\nvoid btrfs_delayed_ref_exit(void)\r\n{\r\nif (btrfs_delayed_ref_head_cachep)\r\nkmem_cache_destroy(btrfs_delayed_ref_head_cachep);\r\nif (btrfs_delayed_tree_ref_cachep)\r\nkmem_cache_destroy(btrfs_delayed_tree_ref_cachep);\r\nif (btrfs_delayed_data_ref_cachep)\r\nkmem_cache_destroy(btrfs_delayed_data_ref_cachep);\r\nif (btrfs_delayed_extent_op_cachep)\r\nkmem_cache_destroy(btrfs_delayed_extent_op_cachep);\r\n}\r\nint btrfs_delayed_ref_init(void)\r\n{\r\nbtrfs_delayed_ref_head_cachep = kmem_cache_create(\r\n"btrfs_delayed_ref_head",\r\nsizeof(struct btrfs_delayed_ref_head), 0,\r\nSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\r\nif (!btrfs_delayed_ref_head_cachep)\r\ngoto fail;\r\nbtrfs_delayed_tree_ref_cachep = kmem_cache_create(\r\n"btrfs_delayed_tree_ref",\r\nsizeof(struct btrfs_delayed_tree_ref), 0,\r\nSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\r\nif (!btrfs_delayed_tree_ref_cachep)\r\ngoto fail;\r\nbtrfs_delayed_data_ref_cachep = kmem_cache_create(\r\n"btrfs_delayed_data_ref",\r\nsizeof(struct btrfs_delayed_data_ref), 0,\r\nSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\r\nif (!btrfs_delayed_data_ref_cachep)\r\ngoto fail;\r\nbtrfs_delayed_extent_op_cachep = kmem_cache_create(\r\n"btrfs_delayed_extent_op",\r\nsizeof(struct btrfs_delayed_extent_op), 0,\r\nSLAB_RECLAIM_ACCOUNT | SLAB_MEM_SPREAD, NULL);\r\nif (!btrfs_delayed_extent_op_cachep)\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\nbtrfs_delayed_ref_exit();\r\nreturn -ENOMEM;\r\n}
