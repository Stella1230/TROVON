static inline void *mb_correct_addr_and_bit(int *bit, void *addr)\r\n{\r\n#if BITS_PER_LONG == 64\r\n*bit += ((unsigned long) addr & 7UL) << 3;\r\naddr = (void *) ((unsigned long) addr & ~7UL);\r\n#elif BITS_PER_LONG == 32\r\n*bit += ((unsigned long) addr & 3UL) << 3;\r\naddr = (void *) ((unsigned long) addr & ~3UL);\r\n#else\r\n#error "how many bits you are?!"\r\n#endif\r\nreturn addr;\r\n}\r\nstatic inline int mb_test_bit(int bit, void *addr)\r\n{\r\naddr = mb_correct_addr_and_bit(&bit, addr);\r\nreturn ext4_test_bit(bit, addr);\r\n}\r\nstatic inline void mb_set_bit(int bit, void *addr)\r\n{\r\naddr = mb_correct_addr_and_bit(&bit, addr);\r\next4_set_bit(bit, addr);\r\n}\r\nstatic inline void mb_clear_bit(int bit, void *addr)\r\n{\r\naddr = mb_correct_addr_and_bit(&bit, addr);\r\next4_clear_bit(bit, addr);\r\n}\r\nstatic inline int mb_test_and_clear_bit(int bit, void *addr)\r\n{\r\naddr = mb_correct_addr_and_bit(&bit, addr);\r\nreturn ext4_test_and_clear_bit(bit, addr);\r\n}\r\nstatic inline int mb_find_next_zero_bit(void *addr, int max, int start)\r\n{\r\nint fix = 0, ret, tmpmax;\r\naddr = mb_correct_addr_and_bit(&fix, addr);\r\ntmpmax = max + fix;\r\nstart += fix;\r\nret = ext4_find_next_zero_bit(addr, tmpmax, start) - fix;\r\nif (ret > max)\r\nreturn max;\r\nreturn ret;\r\n}\r\nstatic inline int mb_find_next_bit(void *addr, int max, int start)\r\n{\r\nint fix = 0, ret, tmpmax;\r\naddr = mb_correct_addr_and_bit(&fix, addr);\r\ntmpmax = max + fix;\r\nstart += fix;\r\nret = ext4_find_next_bit(addr, tmpmax, start) - fix;\r\nif (ret > max)\r\nreturn max;\r\nreturn ret;\r\n}\r\nstatic void *mb_find_buddy(struct ext4_buddy *e4b, int order, int *max)\r\n{\r\nchar *bb;\r\nBUG_ON(e4b->bd_bitmap == e4b->bd_buddy);\r\nBUG_ON(max == NULL);\r\nif (order > e4b->bd_blkbits + 1) {\r\n*max = 0;\r\nreturn NULL;\r\n}\r\nif (order == 0) {\r\n*max = 1 << (e4b->bd_blkbits + 3);\r\nreturn e4b->bd_bitmap;\r\n}\r\nbb = e4b->bd_buddy + EXT4_SB(e4b->bd_sb)->s_mb_offsets[order];\r\n*max = EXT4_SB(e4b->bd_sb)->s_mb_maxs[order];\r\nreturn bb;\r\n}\r\nstatic void mb_free_blocks_double(struct inode *inode, struct ext4_buddy *e4b,\r\nint first, int count)\r\n{\r\nint i;\r\nstruct super_block *sb = e4b->bd_sb;\r\nif (unlikely(e4b->bd_info->bb_bitmap == NULL))\r\nreturn;\r\nassert_spin_locked(ext4_group_lock_ptr(sb, e4b->bd_group));\r\nfor (i = 0; i < count; i++) {\r\nif (!mb_test_bit(first + i, e4b->bd_info->bb_bitmap)) {\r\next4_fsblk_t blocknr;\r\nblocknr = ext4_group_first_block_no(sb, e4b->bd_group);\r\nblocknr += EXT4_C2B(EXT4_SB(sb), first + i);\r\next4_grp_locked_error(sb, e4b->bd_group,\r\ninode ? inode->i_ino : 0,\r\nblocknr,\r\n"freeing block already freed "\r\n"(bit %u)",\r\nfirst + i);\r\n}\r\nmb_clear_bit(first + i, e4b->bd_info->bb_bitmap);\r\n}\r\n}\r\nstatic void mb_mark_used_double(struct ext4_buddy *e4b, int first, int count)\r\n{\r\nint i;\r\nif (unlikely(e4b->bd_info->bb_bitmap == NULL))\r\nreturn;\r\nassert_spin_locked(ext4_group_lock_ptr(e4b->bd_sb, e4b->bd_group));\r\nfor (i = 0; i < count; i++) {\r\nBUG_ON(mb_test_bit(first + i, e4b->bd_info->bb_bitmap));\r\nmb_set_bit(first + i, e4b->bd_info->bb_bitmap);\r\n}\r\n}\r\nstatic void mb_cmp_bitmaps(struct ext4_buddy *e4b, void *bitmap)\r\n{\r\nif (memcmp(e4b->bd_info->bb_bitmap, bitmap, e4b->bd_sb->s_blocksize)) {\r\nunsigned char *b1, *b2;\r\nint i;\r\nb1 = (unsigned char *) e4b->bd_info->bb_bitmap;\r\nb2 = (unsigned char *) bitmap;\r\nfor (i = 0; i < e4b->bd_sb->s_blocksize; i++) {\r\nif (b1[i] != b2[i]) {\r\next4_msg(e4b->bd_sb, KERN_ERR,\r\n"corruption in group %u "\r\n"at byte %u(%u): %x in copy != %x "\r\n"on disk/prealloc",\r\ne4b->bd_group, i, i * 8, b1[i], b2[i]);\r\nBUG();\r\n}\r\n}\r\n}\r\n}\r\nstatic inline void mb_free_blocks_double(struct inode *inode,\r\nstruct ext4_buddy *e4b, int first, int count)\r\n{\r\nreturn;\r\n}\r\nstatic inline void mb_mark_used_double(struct ext4_buddy *e4b,\r\nint first, int count)\r\n{\r\nreturn;\r\n}\r\nstatic inline void mb_cmp_bitmaps(struct ext4_buddy *e4b, void *bitmap)\r\n{\r\nreturn;\r\n}\r\nstatic int __mb_check_buddy(struct ext4_buddy *e4b, char *file,\r\nconst char *function, int line)\r\n{\r\nstruct super_block *sb = e4b->bd_sb;\r\nint order = e4b->bd_blkbits + 1;\r\nint max;\r\nint max2;\r\nint i;\r\nint j;\r\nint k;\r\nint count;\r\nstruct ext4_group_info *grp;\r\nint fragments = 0;\r\nint fstart;\r\nstruct list_head *cur;\r\nvoid *buddy;\r\nvoid *buddy2;\r\n{\r\nstatic int mb_check_counter;\r\nif (mb_check_counter++ % 100 != 0)\r\nreturn 0;\r\n}\r\nwhile (order > 1) {\r\nbuddy = mb_find_buddy(e4b, order, &max);\r\nMB_CHECK_ASSERT(buddy);\r\nbuddy2 = mb_find_buddy(e4b, order - 1, &max2);\r\nMB_CHECK_ASSERT(buddy2);\r\nMB_CHECK_ASSERT(buddy != buddy2);\r\nMB_CHECK_ASSERT(max * 2 == max2);\r\ncount = 0;\r\nfor (i = 0; i < max; i++) {\r\nif (mb_test_bit(i, buddy)) {\r\nif (!mb_test_bit(i << 1, buddy2)) {\r\nMB_CHECK_ASSERT(\r\nmb_test_bit((i<<1)+1, buddy2));\r\n} else if (!mb_test_bit((i << 1) + 1, buddy2)) {\r\nMB_CHECK_ASSERT(\r\nmb_test_bit(i << 1, buddy2));\r\n}\r\ncontinue;\r\n}\r\nMB_CHECK_ASSERT(mb_test_bit(i << 1, buddy2));\r\nMB_CHECK_ASSERT(mb_test_bit((i << 1) + 1, buddy2));\r\nfor (j = 0; j < (1 << order); j++) {\r\nk = (i * (1 << order)) + j;\r\nMB_CHECK_ASSERT(\r\n!mb_test_bit(k, e4b->bd_bitmap));\r\n}\r\ncount++;\r\n}\r\nMB_CHECK_ASSERT(e4b->bd_info->bb_counters[order] == count);\r\norder--;\r\n}\r\nfstart = -1;\r\nbuddy = mb_find_buddy(e4b, 0, &max);\r\nfor (i = 0; i < max; i++) {\r\nif (!mb_test_bit(i, buddy)) {\r\nMB_CHECK_ASSERT(i >= e4b->bd_info->bb_first_free);\r\nif (fstart == -1) {\r\nfragments++;\r\nfstart = i;\r\n}\r\ncontinue;\r\n}\r\nfstart = -1;\r\nfor (j = 0; j < e4b->bd_blkbits + 1; j++) {\r\nbuddy2 = mb_find_buddy(e4b, j, &max2);\r\nk = i >> j;\r\nMB_CHECK_ASSERT(k < max2);\r\nMB_CHECK_ASSERT(mb_test_bit(k, buddy2));\r\n}\r\n}\r\nMB_CHECK_ASSERT(!EXT4_MB_GRP_NEED_INIT(e4b->bd_info));\r\nMB_CHECK_ASSERT(e4b->bd_info->bb_fragments == fragments);\r\ngrp = ext4_get_group_info(sb, e4b->bd_group);\r\nlist_for_each(cur, &grp->bb_prealloc_list) {\r\next4_group_t groupnr;\r\nstruct ext4_prealloc_space *pa;\r\npa = list_entry(cur, struct ext4_prealloc_space, pa_group_list);\r\next4_get_group_no_and_offset(sb, pa->pa_pstart, &groupnr, &k);\r\nMB_CHECK_ASSERT(groupnr == e4b->bd_group);\r\nfor (i = 0; i < pa->pa_len; i++)\r\nMB_CHECK_ASSERT(mb_test_bit(k + i, buddy));\r\n}\r\nreturn 0;\r\n}\r\nstatic void ext4_mb_mark_free_simple(struct super_block *sb,\r\nvoid *buddy, ext4_grpblk_t first, ext4_grpblk_t len,\r\nstruct ext4_group_info *grp)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\next4_grpblk_t min;\r\next4_grpblk_t max;\r\next4_grpblk_t chunk;\r\nunsigned short border;\r\nBUG_ON(len > EXT4_CLUSTERS_PER_GROUP(sb));\r\nborder = 2 << sb->s_blocksize_bits;\r\nwhile (len > 0) {\r\nmax = ffs(first | border) - 1;\r\nmin = fls(len) - 1;\r\nif (max < min)\r\nmin = max;\r\nchunk = 1 << min;\r\ngrp->bb_counters[min]++;\r\nif (min > 0)\r\nmb_clear_bit(first >> min,\r\nbuddy + sbi->s_mb_offsets[min]);\r\nlen -= chunk;\r\nfirst += chunk;\r\n}\r\n}\r\nstatic void\r\nmb_set_largest_free_order(struct super_block *sb, struct ext4_group_info *grp)\r\n{\r\nint i;\r\nint bits;\r\ngrp->bb_largest_free_order = -1;\r\nbits = sb->s_blocksize_bits + 1;\r\nfor (i = bits; i >= 0; i--) {\r\nif (grp->bb_counters[i] > 0) {\r\ngrp->bb_largest_free_order = i;\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic noinline_for_stack\r\nvoid ext4_mb_generate_buddy(struct super_block *sb,\r\nvoid *buddy, void *bitmap, ext4_group_t group)\r\n{\r\nstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\next4_grpblk_t max = EXT4_CLUSTERS_PER_GROUP(sb);\r\next4_grpblk_t i = 0;\r\next4_grpblk_t first;\r\next4_grpblk_t len;\r\nunsigned free = 0;\r\nunsigned fragments = 0;\r\nunsigned long long period = get_cycles();\r\ni = mb_find_next_zero_bit(bitmap, max, 0);\r\ngrp->bb_first_free = i;\r\nwhile (i < max) {\r\nfragments++;\r\nfirst = i;\r\ni = mb_find_next_bit(bitmap, max, i);\r\nlen = i - first;\r\nfree += len;\r\nif (len > 1)\r\next4_mb_mark_free_simple(sb, buddy, first, len, grp);\r\nelse\r\ngrp->bb_counters[0]++;\r\nif (i < max)\r\ni = mb_find_next_zero_bit(bitmap, max, i);\r\n}\r\ngrp->bb_fragments = fragments;\r\nif (free != grp->bb_free) {\r\next4_grp_locked_error(sb, group, 0, 0,\r\n"block bitmap and bg descriptor "\r\n"inconsistent: %u vs %u free clusters",\r\nfree, grp->bb_free);\r\ngrp->bb_free = free;\r\nif (!EXT4_MB_GRP_BBITMAP_CORRUPT(grp))\r\npercpu_counter_sub(&sbi->s_freeclusters_counter,\r\ngrp->bb_free);\r\nset_bit(EXT4_GROUP_INFO_BBITMAP_CORRUPT_BIT, &grp->bb_state);\r\n}\r\nmb_set_largest_free_order(sb, grp);\r\nclear_bit(EXT4_GROUP_INFO_NEED_INIT_BIT, &(grp->bb_state));\r\nperiod = get_cycles() - period;\r\nspin_lock(&EXT4_SB(sb)->s_bal_lock);\r\nEXT4_SB(sb)->s_mb_buddies_generated++;\r\nEXT4_SB(sb)->s_mb_generation_time += period;\r\nspin_unlock(&EXT4_SB(sb)->s_bal_lock);\r\n}\r\nstatic void mb_regenerate_buddy(struct ext4_buddy *e4b)\r\n{\r\nint count;\r\nint order = 1;\r\nvoid *buddy;\r\nwhile ((buddy = mb_find_buddy(e4b, order++, &count))) {\r\next4_set_bits(buddy, 0, count);\r\n}\r\ne4b->bd_info->bb_fragments = 0;\r\nmemset(e4b->bd_info->bb_counters, 0,\r\nsizeof(*e4b->bd_info->bb_counters) *\r\n(e4b->bd_sb->s_blocksize_bits + 2));\r\next4_mb_generate_buddy(e4b->bd_sb, e4b->bd_buddy,\r\ne4b->bd_bitmap, e4b->bd_group);\r\n}\r\nstatic int ext4_mb_init_cache(struct page *page, char *incore)\r\n{\r\next4_group_t ngroups;\r\nint blocksize;\r\nint blocks_per_page;\r\nint groups_per_page;\r\nint err = 0;\r\nint i;\r\next4_group_t first_group, group;\r\nint first_block;\r\nstruct super_block *sb;\r\nstruct buffer_head *bhs;\r\nstruct buffer_head **bh = NULL;\r\nstruct inode *inode;\r\nchar *data;\r\nchar *bitmap;\r\nstruct ext4_group_info *grinfo;\r\nmb_debug(1, "init page %lu\n", page->index);\r\ninode = page->mapping->host;\r\nsb = inode->i_sb;\r\nngroups = ext4_get_groups_count(sb);\r\nblocksize = 1 << inode->i_blkbits;\r\nblocks_per_page = PAGE_CACHE_SIZE / blocksize;\r\ngroups_per_page = blocks_per_page >> 1;\r\nif (groups_per_page == 0)\r\ngroups_per_page = 1;\r\nif (groups_per_page > 1) {\r\ni = sizeof(struct buffer_head *) * groups_per_page;\r\nbh = kzalloc(i, GFP_NOFS);\r\nif (bh == NULL) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\n} else\r\nbh = &bhs;\r\nfirst_group = page->index * blocks_per_page / 2;\r\nfor (i = 0, group = first_group; i < groups_per_page; i++, group++) {\r\nif (group >= ngroups)\r\nbreak;\r\ngrinfo = ext4_get_group_info(sb, group);\r\nif (PageUptodate(page) && !EXT4_MB_GRP_NEED_INIT(grinfo)) {\r\nbh[i] = NULL;\r\ncontinue;\r\n}\r\nif (!(bh[i] = ext4_read_block_bitmap_nowait(sb, group))) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nmb_debug(1, "read bitmap for group %u\n", group);\r\n}\r\nfor (i = 0, group = first_group; i < groups_per_page; i++, group++) {\r\nif (bh[i] && ext4_wait_block_bitmap(sb, group, bh[i]))\r\nerr = -EIO;\r\n}\r\nfirst_block = page->index * blocks_per_page;\r\nfor (i = 0; i < blocks_per_page; i++) {\r\ngroup = (first_block + i) >> 1;\r\nif (group >= ngroups)\r\nbreak;\r\nif (!bh[group - first_group])\r\ncontinue;\r\nif (!buffer_verified(bh[group - first_group]))\r\ncontinue;\r\nerr = 0;\r\ndata = page_address(page) + (i * blocksize);\r\nbitmap = bh[group - first_group]->b_data;\r\nif ((first_block + i) & 1) {\r\nBUG_ON(incore == NULL);\r\nmb_debug(1, "put buddy for group %u in page %lu/%x\n",\r\ngroup, page->index, i * blocksize);\r\ntrace_ext4_mb_buddy_bitmap_load(sb, group);\r\ngrinfo = ext4_get_group_info(sb, group);\r\ngrinfo->bb_fragments = 0;\r\nmemset(grinfo->bb_counters, 0,\r\nsizeof(*grinfo->bb_counters) *\r\n(sb->s_blocksize_bits+2));\r\next4_lock_group(sb, group);\r\nmemset(data, 0xff, blocksize);\r\next4_mb_generate_buddy(sb, data, incore, group);\r\next4_unlock_group(sb, group);\r\nincore = NULL;\r\n} else {\r\nBUG_ON(incore != NULL);\r\nmb_debug(1, "put bitmap for group %u in page %lu/%x\n",\r\ngroup, page->index, i * blocksize);\r\ntrace_ext4_mb_bitmap_load(sb, group);\r\next4_lock_group(sb, group);\r\nmemcpy(data, bitmap, blocksize);\r\next4_mb_generate_from_pa(sb, data, group);\r\next4_mb_generate_from_freelist(sb, data, group);\r\next4_unlock_group(sb, group);\r\nincore = data;\r\n}\r\n}\r\nSetPageUptodate(page);\r\nout:\r\nif (bh) {\r\nfor (i = 0; i < groups_per_page; i++)\r\nbrelse(bh[i]);\r\nif (bh != &bhs)\r\nkfree(bh);\r\n}\r\nreturn err;\r\n}\r\nstatic int ext4_mb_get_buddy_page_lock(struct super_block *sb,\r\next4_group_t group, struct ext4_buddy *e4b)\r\n{\r\nstruct inode *inode = EXT4_SB(sb)->s_buddy_cache;\r\nint block, pnum, poff;\r\nint blocks_per_page;\r\nstruct page *page;\r\ne4b->bd_buddy_page = NULL;\r\ne4b->bd_bitmap_page = NULL;\r\nblocks_per_page = PAGE_CACHE_SIZE / sb->s_blocksize;\r\nblock = group * 2;\r\npnum = block / blocks_per_page;\r\npoff = block % blocks_per_page;\r\npage = find_or_create_page(inode->i_mapping, pnum, GFP_NOFS);\r\nif (!page)\r\nreturn -ENOMEM;\r\nBUG_ON(page->mapping != inode->i_mapping);\r\ne4b->bd_bitmap_page = page;\r\ne4b->bd_bitmap = page_address(page) + (poff * sb->s_blocksize);\r\nif (blocks_per_page >= 2) {\r\nreturn 0;\r\n}\r\nblock++;\r\npnum = block / blocks_per_page;\r\npage = find_or_create_page(inode->i_mapping, pnum, GFP_NOFS);\r\nif (!page)\r\nreturn -ENOMEM;\r\nBUG_ON(page->mapping != inode->i_mapping);\r\ne4b->bd_buddy_page = page;\r\nreturn 0;\r\n}\r\nstatic void ext4_mb_put_buddy_page_lock(struct ext4_buddy *e4b)\r\n{\r\nif (e4b->bd_bitmap_page) {\r\nunlock_page(e4b->bd_bitmap_page);\r\npage_cache_release(e4b->bd_bitmap_page);\r\n}\r\nif (e4b->bd_buddy_page) {\r\nunlock_page(e4b->bd_buddy_page);\r\npage_cache_release(e4b->bd_buddy_page);\r\n}\r\n}\r\nstatic noinline_for_stack\r\nint ext4_mb_init_group(struct super_block *sb, ext4_group_t group)\r\n{\r\nstruct ext4_group_info *this_grp;\r\nstruct ext4_buddy e4b;\r\nstruct page *page;\r\nint ret = 0;\r\nmight_sleep();\r\nmb_debug(1, "init group %u\n", group);\r\nthis_grp = ext4_get_group_info(sb, group);\r\nret = ext4_mb_get_buddy_page_lock(sb, group, &e4b);\r\nif (ret || !EXT4_MB_GRP_NEED_INIT(this_grp)) {\r\ngoto err;\r\n}\r\npage = e4b.bd_bitmap_page;\r\nret = ext4_mb_init_cache(page, NULL);\r\nif (ret)\r\ngoto err;\r\nif (!PageUptodate(page)) {\r\nret = -EIO;\r\ngoto err;\r\n}\r\nif (e4b.bd_buddy_page == NULL) {\r\nret = 0;\r\ngoto err;\r\n}\r\npage = e4b.bd_buddy_page;\r\nret = ext4_mb_init_cache(page, e4b.bd_bitmap);\r\nif (ret)\r\ngoto err;\r\nif (!PageUptodate(page)) {\r\nret = -EIO;\r\ngoto err;\r\n}\r\nerr:\r\next4_mb_put_buddy_page_lock(&e4b);\r\nreturn ret;\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_load_buddy(struct super_block *sb, ext4_group_t group,\r\nstruct ext4_buddy *e4b)\r\n{\r\nint blocks_per_page;\r\nint block;\r\nint pnum;\r\nint poff;\r\nstruct page *page;\r\nint ret;\r\nstruct ext4_group_info *grp;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nstruct inode *inode = sbi->s_buddy_cache;\r\nmight_sleep();\r\nmb_debug(1, "load group %u\n", group);\r\nblocks_per_page = PAGE_CACHE_SIZE / sb->s_blocksize;\r\ngrp = ext4_get_group_info(sb, group);\r\ne4b->bd_blkbits = sb->s_blocksize_bits;\r\ne4b->bd_info = grp;\r\ne4b->bd_sb = sb;\r\ne4b->bd_group = group;\r\ne4b->bd_buddy_page = NULL;\r\ne4b->bd_bitmap_page = NULL;\r\nif (unlikely(EXT4_MB_GRP_NEED_INIT(grp))) {\r\nret = ext4_mb_init_group(sb, group);\r\nif (ret)\r\nreturn ret;\r\n}\r\nblock = group * 2;\r\npnum = block / blocks_per_page;\r\npoff = block % blocks_per_page;\r\npage = find_get_page_flags(inode->i_mapping, pnum, FGP_ACCESSED);\r\nif (page == NULL || !PageUptodate(page)) {\r\nif (page)\r\npage_cache_release(page);\r\npage = find_or_create_page(inode->i_mapping, pnum, GFP_NOFS);\r\nif (page) {\r\nBUG_ON(page->mapping != inode->i_mapping);\r\nif (!PageUptodate(page)) {\r\nret = ext4_mb_init_cache(page, NULL);\r\nif (ret) {\r\nunlock_page(page);\r\ngoto err;\r\n}\r\nmb_cmp_bitmaps(e4b, page_address(page) +\r\n(poff * sb->s_blocksize));\r\n}\r\nunlock_page(page);\r\n}\r\n}\r\nif (page == NULL) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nif (!PageUptodate(page)) {\r\nret = -EIO;\r\ngoto err;\r\n}\r\ne4b->bd_bitmap_page = page;\r\ne4b->bd_bitmap = page_address(page) + (poff * sb->s_blocksize);\r\nblock++;\r\npnum = block / blocks_per_page;\r\npoff = block % blocks_per_page;\r\npage = find_get_page_flags(inode->i_mapping, pnum, FGP_ACCESSED);\r\nif (page == NULL || !PageUptodate(page)) {\r\nif (page)\r\npage_cache_release(page);\r\npage = find_or_create_page(inode->i_mapping, pnum, GFP_NOFS);\r\nif (page) {\r\nBUG_ON(page->mapping != inode->i_mapping);\r\nif (!PageUptodate(page)) {\r\nret = ext4_mb_init_cache(page, e4b->bd_bitmap);\r\nif (ret) {\r\nunlock_page(page);\r\ngoto err;\r\n}\r\n}\r\nunlock_page(page);\r\n}\r\n}\r\nif (page == NULL) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nif (!PageUptodate(page)) {\r\nret = -EIO;\r\ngoto err;\r\n}\r\ne4b->bd_buddy_page = page;\r\ne4b->bd_buddy = page_address(page) + (poff * sb->s_blocksize);\r\nBUG_ON(e4b->bd_bitmap_page == NULL);\r\nBUG_ON(e4b->bd_buddy_page == NULL);\r\nreturn 0;\r\nerr:\r\nif (page)\r\npage_cache_release(page);\r\nif (e4b->bd_bitmap_page)\r\npage_cache_release(e4b->bd_bitmap_page);\r\nif (e4b->bd_buddy_page)\r\npage_cache_release(e4b->bd_buddy_page);\r\ne4b->bd_buddy = NULL;\r\ne4b->bd_bitmap = NULL;\r\nreturn ret;\r\n}\r\nstatic void ext4_mb_unload_buddy(struct ext4_buddy *e4b)\r\n{\r\nif (e4b->bd_bitmap_page)\r\npage_cache_release(e4b->bd_bitmap_page);\r\nif (e4b->bd_buddy_page)\r\npage_cache_release(e4b->bd_buddy_page);\r\n}\r\nstatic int mb_find_order_for_block(struct ext4_buddy *e4b, int block)\r\n{\r\nint order = 1;\r\nvoid *bb;\r\nBUG_ON(e4b->bd_bitmap == e4b->bd_buddy);\r\nBUG_ON(block >= (1 << (e4b->bd_blkbits + 3)));\r\nbb = e4b->bd_buddy;\r\nwhile (order <= e4b->bd_blkbits + 1) {\r\nblock = block >> 1;\r\nif (!mb_test_bit(block, bb)) {\r\nreturn order;\r\n}\r\nbb += 1 << (e4b->bd_blkbits - order);\r\norder++;\r\n}\r\nreturn 0;\r\n}\r\nstatic void mb_clear_bits(void *bm, int cur, int len)\r\n{\r\n__u32 *addr;\r\nlen = cur + len;\r\nwhile (cur < len) {\r\nif ((cur & 31) == 0 && (len - cur) >= 32) {\r\naddr = bm + (cur >> 3);\r\n*addr = 0;\r\ncur += 32;\r\ncontinue;\r\n}\r\nmb_clear_bit(cur, bm);\r\ncur++;\r\n}\r\n}\r\nstatic int mb_test_and_clear_bits(void *bm, int cur, int len)\r\n{\r\n__u32 *addr;\r\nint zero_bit = -1;\r\nlen = cur + len;\r\nwhile (cur < len) {\r\nif ((cur & 31) == 0 && (len - cur) >= 32) {\r\naddr = bm + (cur >> 3);\r\nif (*addr != (__u32)(-1) && zero_bit == -1)\r\nzero_bit = cur + mb_find_next_zero_bit(addr, 32, 0);\r\n*addr = 0;\r\ncur += 32;\r\ncontinue;\r\n}\r\nif (!mb_test_and_clear_bit(cur, bm) && zero_bit == -1)\r\nzero_bit = cur;\r\ncur++;\r\n}\r\nreturn zero_bit;\r\n}\r\nvoid ext4_set_bits(void *bm, int cur, int len)\r\n{\r\n__u32 *addr;\r\nlen = cur + len;\r\nwhile (cur < len) {\r\nif ((cur & 31) == 0 && (len - cur) >= 32) {\r\naddr = bm + (cur >> 3);\r\n*addr = 0xffffffff;\r\ncur += 32;\r\ncontinue;\r\n}\r\nmb_set_bit(cur, bm);\r\ncur++;\r\n}\r\n}\r\nstatic inline int mb_buddy_adjust_border(int* bit, void* bitmap, int side)\r\n{\r\nif (mb_test_bit(*bit + side, bitmap)) {\r\nmb_clear_bit(*bit, bitmap);\r\n(*bit) -= side;\r\nreturn 1;\r\n}\r\nelse {\r\n(*bit) += side;\r\nmb_set_bit(*bit, bitmap);\r\nreturn -1;\r\n}\r\n}\r\nstatic void mb_buddy_mark_free(struct ext4_buddy *e4b, int first, int last)\r\n{\r\nint max;\r\nint order = 1;\r\nvoid *buddy = mb_find_buddy(e4b, order, &max);\r\nwhile (buddy) {\r\nvoid *buddy2;\r\nif (first & 1)\r\ne4b->bd_info->bb_counters[order] += mb_buddy_adjust_border(&first, buddy, -1);\r\nif (!(last & 1))\r\ne4b->bd_info->bb_counters[order] += mb_buddy_adjust_border(&last, buddy, 1);\r\nif (first > last)\r\nbreak;\r\norder++;\r\nif (first == last || !(buddy2 = mb_find_buddy(e4b, order, &max))) {\r\nmb_clear_bits(buddy, first, last - first + 1);\r\ne4b->bd_info->bb_counters[order - 1] += last - first + 1;\r\nbreak;\r\n}\r\nfirst >>= 1;\r\nlast >>= 1;\r\nbuddy = buddy2;\r\n}\r\n}\r\nstatic void mb_free_blocks(struct inode *inode, struct ext4_buddy *e4b,\r\nint first, int count)\r\n{\r\nint left_is_free = 0;\r\nint right_is_free = 0;\r\nint block;\r\nint last = first + count - 1;\r\nstruct super_block *sb = e4b->bd_sb;\r\nif (WARN_ON(count == 0))\r\nreturn;\r\nBUG_ON(last >= (sb->s_blocksize << 3));\r\nassert_spin_locked(ext4_group_lock_ptr(sb, e4b->bd_group));\r\nif (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(e4b->bd_info)))\r\nreturn;\r\nmb_check_buddy(e4b);\r\nmb_free_blocks_double(inode, e4b, first, count);\r\ne4b->bd_info->bb_free += count;\r\nif (first < e4b->bd_info->bb_first_free)\r\ne4b->bd_info->bb_first_free = first;\r\nif (first != 0)\r\nleft_is_free = !mb_test_bit(first - 1, e4b->bd_bitmap);\r\nblock = mb_test_and_clear_bits(e4b->bd_bitmap, first, count);\r\nif (last + 1 < EXT4_SB(sb)->s_mb_maxs[0])\r\nright_is_free = !mb_test_bit(last + 1, e4b->bd_bitmap);\r\nif (unlikely(block != -1)) {\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\next4_fsblk_t blocknr;\r\nblocknr = ext4_group_first_block_no(sb, e4b->bd_group);\r\nblocknr += EXT4_C2B(EXT4_SB(sb), block);\r\next4_grp_locked_error(sb, e4b->bd_group,\r\ninode ? inode->i_ino : 0,\r\nblocknr,\r\n"freeing already freed block "\r\n"(bit %u); block bitmap corrupt.",\r\nblock);\r\nif (!EXT4_MB_GRP_BBITMAP_CORRUPT(e4b->bd_info))\r\npercpu_counter_sub(&sbi->s_freeclusters_counter,\r\ne4b->bd_info->bb_free);\r\nset_bit(EXT4_GROUP_INFO_BBITMAP_CORRUPT_BIT,\r\n&e4b->bd_info->bb_state);\r\nmb_regenerate_buddy(e4b);\r\ngoto done;\r\n}\r\nif (left_is_free && right_is_free)\r\ne4b->bd_info->bb_fragments--;\r\nelse if (!left_is_free && !right_is_free)\r\ne4b->bd_info->bb_fragments++;\r\nif (first & 1) {\r\nfirst += !left_is_free;\r\ne4b->bd_info->bb_counters[0] += left_is_free ? -1 : 1;\r\n}\r\nif (!(last & 1)) {\r\nlast -= !right_is_free;\r\ne4b->bd_info->bb_counters[0] += right_is_free ? -1 : 1;\r\n}\r\nif (first <= last)\r\nmb_buddy_mark_free(e4b, first >> 1, last >> 1);\r\ndone:\r\nmb_set_largest_free_order(sb, e4b->bd_info);\r\nmb_check_buddy(e4b);\r\n}\r\nstatic int mb_find_extent(struct ext4_buddy *e4b, int block,\r\nint needed, struct ext4_free_extent *ex)\r\n{\r\nint next = block;\r\nint max, order;\r\nvoid *buddy;\r\nassert_spin_locked(ext4_group_lock_ptr(e4b->bd_sb, e4b->bd_group));\r\nBUG_ON(ex == NULL);\r\nbuddy = mb_find_buddy(e4b, 0, &max);\r\nBUG_ON(buddy == NULL);\r\nBUG_ON(block >= max);\r\nif (mb_test_bit(block, buddy)) {\r\nex->fe_len = 0;\r\nex->fe_start = 0;\r\nex->fe_group = 0;\r\nreturn 0;\r\n}\r\norder = mb_find_order_for_block(e4b, block);\r\nblock = block >> order;\r\nex->fe_len = 1 << order;\r\nex->fe_start = block << order;\r\nex->fe_group = e4b->bd_group;\r\nnext = next - ex->fe_start;\r\nex->fe_len -= next;\r\nex->fe_start += next;\r\nwhile (needed > ex->fe_len &&\r\nmb_find_buddy(e4b, order, &max)) {\r\nif (block + 1 >= max)\r\nbreak;\r\nnext = (block + 1) * (1 << order);\r\nif (mb_test_bit(next, e4b->bd_bitmap))\r\nbreak;\r\norder = mb_find_order_for_block(e4b, next);\r\nblock = next >> order;\r\nex->fe_len += 1 << order;\r\n}\r\nBUG_ON(ex->fe_start + ex->fe_len > (1 << (e4b->bd_blkbits + 3)));\r\nreturn ex->fe_len;\r\n}\r\nstatic int mb_mark_used(struct ext4_buddy *e4b, struct ext4_free_extent *ex)\r\n{\r\nint ord;\r\nint mlen = 0;\r\nint max = 0;\r\nint cur;\r\nint start = ex->fe_start;\r\nint len = ex->fe_len;\r\nunsigned ret = 0;\r\nint len0 = len;\r\nvoid *buddy;\r\nBUG_ON(start + len > (e4b->bd_sb->s_blocksize << 3));\r\nBUG_ON(e4b->bd_group != ex->fe_group);\r\nassert_spin_locked(ext4_group_lock_ptr(e4b->bd_sb, e4b->bd_group));\r\nmb_check_buddy(e4b);\r\nmb_mark_used_double(e4b, start, len);\r\ne4b->bd_info->bb_free -= len;\r\nif (e4b->bd_info->bb_first_free == start)\r\ne4b->bd_info->bb_first_free += len;\r\nif (start != 0)\r\nmlen = !mb_test_bit(start - 1, e4b->bd_bitmap);\r\nif (start + len < EXT4_SB(e4b->bd_sb)->s_mb_maxs[0])\r\nmax = !mb_test_bit(start + len, e4b->bd_bitmap);\r\nif (mlen && max)\r\ne4b->bd_info->bb_fragments++;\r\nelse if (!mlen && !max)\r\ne4b->bd_info->bb_fragments--;\r\nwhile (len) {\r\nord = mb_find_order_for_block(e4b, start);\r\nif (((start >> ord) << ord) == start && len >= (1 << ord)) {\r\nmlen = 1 << ord;\r\nbuddy = mb_find_buddy(e4b, ord, &max);\r\nBUG_ON((start >> ord) >= max);\r\nmb_set_bit(start >> ord, buddy);\r\ne4b->bd_info->bb_counters[ord]--;\r\nstart += mlen;\r\nlen -= mlen;\r\nBUG_ON(len < 0);\r\ncontinue;\r\n}\r\nif (ret == 0)\r\nret = len | (ord << 16);\r\nBUG_ON(ord <= 0);\r\nbuddy = mb_find_buddy(e4b, ord, &max);\r\nmb_set_bit(start >> ord, buddy);\r\ne4b->bd_info->bb_counters[ord]--;\r\nord--;\r\ncur = (start >> ord) & ~1U;\r\nbuddy = mb_find_buddy(e4b, ord, &max);\r\nmb_clear_bit(cur, buddy);\r\nmb_clear_bit(cur + 1, buddy);\r\ne4b->bd_info->bb_counters[ord]++;\r\ne4b->bd_info->bb_counters[ord]++;\r\n}\r\nmb_set_largest_free_order(e4b->bd_sb, e4b->bd_info);\r\next4_set_bits(e4b->bd_bitmap, ex->fe_start, len0);\r\nmb_check_buddy(e4b);\r\nreturn ret;\r\n}\r\nstatic void ext4_mb_use_best_found(struct ext4_allocation_context *ac,\r\nstruct ext4_buddy *e4b)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\r\nint ret;\r\nBUG_ON(ac->ac_b_ex.fe_group != e4b->bd_group);\r\nBUG_ON(ac->ac_status == AC_STATUS_FOUND);\r\nac->ac_b_ex.fe_len = min(ac->ac_b_ex.fe_len, ac->ac_g_ex.fe_len);\r\nac->ac_b_ex.fe_logical = ac->ac_g_ex.fe_logical;\r\nret = mb_mark_used(e4b, &ac->ac_b_ex);\r\nac->ac_f_ex = ac->ac_b_ex;\r\nac->ac_status = AC_STATUS_FOUND;\r\nac->ac_tail = ret & 0xffff;\r\nac->ac_buddy = ret >> 16;\r\nac->ac_bitmap_page = e4b->bd_bitmap_page;\r\nget_page(ac->ac_bitmap_page);\r\nac->ac_buddy_page = e4b->bd_buddy_page;\r\nget_page(ac->ac_buddy_page);\r\nif (ac->ac_flags & EXT4_MB_STREAM_ALLOC) {\r\nspin_lock(&sbi->s_md_lock);\r\nsbi->s_mb_last_group = ac->ac_f_ex.fe_group;\r\nsbi->s_mb_last_start = ac->ac_f_ex.fe_start;\r\nspin_unlock(&sbi->s_md_lock);\r\n}\r\n}\r\nstatic void ext4_mb_check_limits(struct ext4_allocation_context *ac,\r\nstruct ext4_buddy *e4b,\r\nint finish_group)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\r\nstruct ext4_free_extent *bex = &ac->ac_b_ex;\r\nstruct ext4_free_extent *gex = &ac->ac_g_ex;\r\nstruct ext4_free_extent ex;\r\nint max;\r\nif (ac->ac_status == AC_STATUS_FOUND)\r\nreturn;\r\nif (ac->ac_found > sbi->s_mb_max_to_scan &&\r\n!(ac->ac_flags & EXT4_MB_HINT_FIRST)) {\r\nac->ac_status = AC_STATUS_BREAK;\r\nreturn;\r\n}\r\nif (bex->fe_len < gex->fe_len)\r\nreturn;\r\nif ((finish_group || ac->ac_found > sbi->s_mb_min_to_scan)\r\n&& bex->fe_group == e4b->bd_group) {\r\nmax = mb_find_extent(e4b, bex->fe_start, gex->fe_len, &ex);\r\nif (max >= gex->fe_len) {\r\next4_mb_use_best_found(ac, e4b);\r\nreturn;\r\n}\r\n}\r\n}\r\nstatic void ext4_mb_measure_extent(struct ext4_allocation_context *ac,\r\nstruct ext4_free_extent *ex,\r\nstruct ext4_buddy *e4b)\r\n{\r\nstruct ext4_free_extent *bex = &ac->ac_b_ex;\r\nstruct ext4_free_extent *gex = &ac->ac_g_ex;\r\nBUG_ON(ex->fe_len <= 0);\r\nBUG_ON(ex->fe_len > EXT4_CLUSTERS_PER_GROUP(ac->ac_sb));\r\nBUG_ON(ex->fe_start >= EXT4_CLUSTERS_PER_GROUP(ac->ac_sb));\r\nBUG_ON(ac->ac_status != AC_STATUS_CONTINUE);\r\nac->ac_found++;\r\nif (unlikely(ac->ac_flags & EXT4_MB_HINT_FIRST)) {\r\n*bex = *ex;\r\next4_mb_use_best_found(ac, e4b);\r\nreturn;\r\n}\r\nif (ex->fe_len == gex->fe_len) {\r\n*bex = *ex;\r\next4_mb_use_best_found(ac, e4b);\r\nreturn;\r\n}\r\nif (bex->fe_len == 0) {\r\n*bex = *ex;\r\nreturn;\r\n}\r\nif (bex->fe_len < gex->fe_len) {\r\nif (ex->fe_len > bex->fe_len)\r\n*bex = *ex;\r\n} else if (ex->fe_len > gex->fe_len) {\r\nif (ex->fe_len < bex->fe_len)\r\n*bex = *ex;\r\n}\r\next4_mb_check_limits(ac, e4b, 0);\r\n}\r\nstatic noinline_for_stack\r\nint ext4_mb_try_best_found(struct ext4_allocation_context *ac,\r\nstruct ext4_buddy *e4b)\r\n{\r\nstruct ext4_free_extent ex = ac->ac_b_ex;\r\next4_group_t group = ex.fe_group;\r\nint max;\r\nint err;\r\nBUG_ON(ex.fe_len <= 0);\r\nerr = ext4_mb_load_buddy(ac->ac_sb, group, e4b);\r\nif (err)\r\nreturn err;\r\next4_lock_group(ac->ac_sb, group);\r\nmax = mb_find_extent(e4b, ex.fe_start, ex.fe_len, &ex);\r\nif (max > 0) {\r\nac->ac_b_ex = ex;\r\next4_mb_use_best_found(ac, e4b);\r\n}\r\next4_unlock_group(ac->ac_sb, group);\r\next4_mb_unload_buddy(e4b);\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack\r\nint ext4_mb_find_by_goal(struct ext4_allocation_context *ac,\r\nstruct ext4_buddy *e4b)\r\n{\r\next4_group_t group = ac->ac_g_ex.fe_group;\r\nint max;\r\nint err;\r\nstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\r\nstruct ext4_group_info *grp = ext4_get_group_info(ac->ac_sb, group);\r\nstruct ext4_free_extent ex;\r\nif (!(ac->ac_flags & EXT4_MB_HINT_TRY_GOAL))\r\nreturn 0;\r\nif (grp->bb_free == 0)\r\nreturn 0;\r\nerr = ext4_mb_load_buddy(ac->ac_sb, group, e4b);\r\nif (err)\r\nreturn err;\r\nif (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(e4b->bd_info))) {\r\next4_mb_unload_buddy(e4b);\r\nreturn 0;\r\n}\r\next4_lock_group(ac->ac_sb, group);\r\nmax = mb_find_extent(e4b, ac->ac_g_ex.fe_start,\r\nac->ac_g_ex.fe_len, &ex);\r\nex.fe_logical = 0xDEADFA11;\r\nif (max >= ac->ac_g_ex.fe_len && ac->ac_g_ex.fe_len == sbi->s_stripe) {\r\next4_fsblk_t start;\r\nstart = ext4_group_first_block_no(ac->ac_sb, e4b->bd_group) +\r\nex.fe_start;\r\nif (do_div(start, sbi->s_stripe) == 0) {\r\nac->ac_found++;\r\nac->ac_b_ex = ex;\r\next4_mb_use_best_found(ac, e4b);\r\n}\r\n} else if (max >= ac->ac_g_ex.fe_len) {\r\nBUG_ON(ex.fe_len <= 0);\r\nBUG_ON(ex.fe_group != ac->ac_g_ex.fe_group);\r\nBUG_ON(ex.fe_start != ac->ac_g_ex.fe_start);\r\nac->ac_found++;\r\nac->ac_b_ex = ex;\r\next4_mb_use_best_found(ac, e4b);\r\n} else if (max > 0 && (ac->ac_flags & EXT4_MB_HINT_MERGE)) {\r\nBUG_ON(ex.fe_len <= 0);\r\nBUG_ON(ex.fe_group != ac->ac_g_ex.fe_group);\r\nBUG_ON(ex.fe_start != ac->ac_g_ex.fe_start);\r\nac->ac_found++;\r\nac->ac_b_ex = ex;\r\next4_mb_use_best_found(ac, e4b);\r\n}\r\next4_unlock_group(ac->ac_sb, group);\r\next4_mb_unload_buddy(e4b);\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack\r\nvoid ext4_mb_simple_scan_group(struct ext4_allocation_context *ac,\r\nstruct ext4_buddy *e4b)\r\n{\r\nstruct super_block *sb = ac->ac_sb;\r\nstruct ext4_group_info *grp = e4b->bd_info;\r\nvoid *buddy;\r\nint i;\r\nint k;\r\nint max;\r\nBUG_ON(ac->ac_2order <= 0);\r\nfor (i = ac->ac_2order; i <= sb->s_blocksize_bits + 1; i++) {\r\nif (grp->bb_counters[i] == 0)\r\ncontinue;\r\nbuddy = mb_find_buddy(e4b, i, &max);\r\nBUG_ON(buddy == NULL);\r\nk = mb_find_next_zero_bit(buddy, max, 0);\r\nBUG_ON(k >= max);\r\nac->ac_found++;\r\nac->ac_b_ex.fe_len = 1 << i;\r\nac->ac_b_ex.fe_start = k << i;\r\nac->ac_b_ex.fe_group = e4b->bd_group;\r\next4_mb_use_best_found(ac, e4b);\r\nBUG_ON(ac->ac_b_ex.fe_len != ac->ac_g_ex.fe_len);\r\nif (EXT4_SB(sb)->s_mb_stats)\r\natomic_inc(&EXT4_SB(sb)->s_bal_2orders);\r\nbreak;\r\n}\r\n}\r\nstatic noinline_for_stack\r\nvoid ext4_mb_complex_scan_group(struct ext4_allocation_context *ac,\r\nstruct ext4_buddy *e4b)\r\n{\r\nstruct super_block *sb = ac->ac_sb;\r\nvoid *bitmap = e4b->bd_bitmap;\r\nstruct ext4_free_extent ex;\r\nint i;\r\nint free;\r\nfree = e4b->bd_info->bb_free;\r\nBUG_ON(free <= 0);\r\ni = e4b->bd_info->bb_first_free;\r\nwhile (free && ac->ac_status == AC_STATUS_CONTINUE) {\r\ni = mb_find_next_zero_bit(bitmap,\r\nEXT4_CLUSTERS_PER_GROUP(sb), i);\r\nif (i >= EXT4_CLUSTERS_PER_GROUP(sb)) {\r\next4_grp_locked_error(sb, e4b->bd_group, 0, 0,\r\n"%d free clusters as per "\r\n"group info. But bitmap says 0",\r\nfree);\r\nbreak;\r\n}\r\nmb_find_extent(e4b, i, ac->ac_g_ex.fe_len, &ex);\r\nBUG_ON(ex.fe_len <= 0);\r\nif (free < ex.fe_len) {\r\next4_grp_locked_error(sb, e4b->bd_group, 0, 0,\r\n"%d free clusters as per "\r\n"group info. But got %d blocks",\r\nfree, ex.fe_len);\r\nbreak;\r\n}\r\nex.fe_logical = 0xDEADC0DE;\r\next4_mb_measure_extent(ac, &ex, e4b);\r\ni += ex.fe_len;\r\nfree -= ex.fe_len;\r\n}\r\next4_mb_check_limits(ac, e4b, 1);\r\n}\r\nstatic noinline_for_stack\r\nvoid ext4_mb_scan_aligned(struct ext4_allocation_context *ac,\r\nstruct ext4_buddy *e4b)\r\n{\r\nstruct super_block *sb = ac->ac_sb;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nvoid *bitmap = e4b->bd_bitmap;\r\nstruct ext4_free_extent ex;\r\next4_fsblk_t first_group_block;\r\next4_fsblk_t a;\r\next4_grpblk_t i;\r\nint max;\r\nBUG_ON(sbi->s_stripe == 0);\r\nfirst_group_block = ext4_group_first_block_no(sb, e4b->bd_group);\r\na = first_group_block + sbi->s_stripe - 1;\r\ndo_div(a, sbi->s_stripe);\r\ni = (a * sbi->s_stripe) - first_group_block;\r\nwhile (i < EXT4_CLUSTERS_PER_GROUP(sb)) {\r\nif (!mb_test_bit(i, bitmap)) {\r\nmax = mb_find_extent(e4b, i, sbi->s_stripe, &ex);\r\nif (max >= sbi->s_stripe) {\r\nac->ac_found++;\r\nex.fe_logical = 0xDEADF00D;\r\nac->ac_b_ex = ex;\r\next4_mb_use_best_found(ac, e4b);\r\nbreak;\r\n}\r\n}\r\ni += sbi->s_stripe;\r\n}\r\n}\r\nstatic int ext4_mb_good_group(struct ext4_allocation_context *ac,\r\next4_group_t group, int cr)\r\n{\r\nunsigned free, fragments;\r\nint flex_size = ext4_flex_bg_size(EXT4_SB(ac->ac_sb));\r\nstruct ext4_group_info *grp = ext4_get_group_info(ac->ac_sb, group);\r\nBUG_ON(cr < 0 || cr >= 4);\r\nfree = grp->bb_free;\r\nif (free == 0)\r\nreturn 0;\r\nif (cr <= 2 && free < ac->ac_g_ex.fe_len)\r\nreturn 0;\r\nif (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(grp)))\r\nreturn 0;\r\nif (unlikely(EXT4_MB_GRP_NEED_INIT(grp))) {\r\nint ret = ext4_mb_init_group(ac->ac_sb, group);\r\nif (ret)\r\nreturn ret;\r\n}\r\nfragments = grp->bb_fragments;\r\nif (fragments == 0)\r\nreturn 0;\r\nswitch (cr) {\r\ncase 0:\r\nBUG_ON(ac->ac_2order == 0);\r\nif ((ac->ac_flags & EXT4_MB_HINT_DATA) &&\r\n(flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) &&\r\n((group % flex_size) == 0))\r\nreturn 0;\r\nif ((ac->ac_2order > ac->ac_sb->s_blocksize_bits+1) ||\r\n(free / fragments) >= ac->ac_g_ex.fe_len)\r\nreturn 1;\r\nif (grp->bb_largest_free_order < ac->ac_2order)\r\nreturn 0;\r\nreturn 1;\r\ncase 1:\r\nif ((free / fragments) >= ac->ac_g_ex.fe_len)\r\nreturn 1;\r\nbreak;\r\ncase 2:\r\nif (free >= ac->ac_g_ex.fe_len)\r\nreturn 1;\r\nbreak;\r\ncase 3:\r\nreturn 1;\r\ndefault:\r\nBUG();\r\n}\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_regular_allocator(struct ext4_allocation_context *ac)\r\n{\r\next4_group_t ngroups, group, i;\r\nint cr;\r\nint err = 0, first_err = 0;\r\nstruct ext4_sb_info *sbi;\r\nstruct super_block *sb;\r\nstruct ext4_buddy e4b;\r\nsb = ac->ac_sb;\r\nsbi = EXT4_SB(sb);\r\nngroups = ext4_get_groups_count(sb);\r\nif (!(ext4_test_inode_flag(ac->ac_inode, EXT4_INODE_EXTENTS)))\r\nngroups = sbi->s_blockfile_groups;\r\nBUG_ON(ac->ac_status == AC_STATUS_FOUND);\r\nerr = ext4_mb_find_by_goal(ac, &e4b);\r\nif (err || ac->ac_status == AC_STATUS_FOUND)\r\ngoto out;\r\nif (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY))\r\ngoto out;\r\ni = fls(ac->ac_g_ex.fe_len);\r\nac->ac_2order = 0;\r\nif (i >= sbi->s_mb_order2_reqs) {\r\nif ((ac->ac_g_ex.fe_len & (~(1 << (i - 1)))) == 0)\r\nac->ac_2order = i - 1;\r\n}\r\nif (ac->ac_flags & EXT4_MB_STREAM_ALLOC) {\r\nspin_lock(&sbi->s_md_lock);\r\nac->ac_g_ex.fe_group = sbi->s_mb_last_group;\r\nac->ac_g_ex.fe_start = sbi->s_mb_last_start;\r\nspin_unlock(&sbi->s_md_lock);\r\n}\r\ncr = ac->ac_2order ? 0 : 1;\r\nrepeat:\r\nfor (; cr < 4 && ac->ac_status == AC_STATUS_CONTINUE; cr++) {\r\nac->ac_criteria = cr;\r\ngroup = ac->ac_g_ex.fe_group;\r\nfor (i = 0; i < ngroups; group++, i++) {\r\nint ret = 0;\r\ncond_resched();\r\nif (group >= ngroups)\r\ngroup = 0;\r\nret = ext4_mb_good_group(ac, group, cr);\r\nif (ret <= 0) {\r\nif (!first_err)\r\nfirst_err = ret;\r\ncontinue;\r\n}\r\nerr = ext4_mb_load_buddy(sb, group, &e4b);\r\nif (err)\r\ngoto out;\r\next4_lock_group(sb, group);\r\nret = ext4_mb_good_group(ac, group, cr);\r\nif (ret <= 0) {\r\next4_unlock_group(sb, group);\r\next4_mb_unload_buddy(&e4b);\r\nif (!first_err)\r\nfirst_err = ret;\r\ncontinue;\r\n}\r\nac->ac_groups_scanned++;\r\nif (cr == 0 && ac->ac_2order < sb->s_blocksize_bits+2)\r\next4_mb_simple_scan_group(ac, &e4b);\r\nelse if (cr == 1 && sbi->s_stripe &&\r\n!(ac->ac_g_ex.fe_len % sbi->s_stripe))\r\next4_mb_scan_aligned(ac, &e4b);\r\nelse\r\next4_mb_complex_scan_group(ac, &e4b);\r\next4_unlock_group(sb, group);\r\next4_mb_unload_buddy(&e4b);\r\nif (ac->ac_status != AC_STATUS_CONTINUE)\r\nbreak;\r\n}\r\n}\r\nif (ac->ac_b_ex.fe_len > 0 && ac->ac_status != AC_STATUS_FOUND &&\r\n!(ac->ac_flags & EXT4_MB_HINT_FIRST)) {\r\next4_mb_try_best_found(ac, &e4b);\r\nif (ac->ac_status != AC_STATUS_FOUND) {\r\nac->ac_b_ex.fe_group = 0;\r\nac->ac_b_ex.fe_start = 0;\r\nac->ac_b_ex.fe_len = 0;\r\nac->ac_status = AC_STATUS_CONTINUE;\r\nac->ac_flags |= EXT4_MB_HINT_FIRST;\r\ncr = 3;\r\natomic_inc(&sbi->s_mb_lost_chunks);\r\ngoto repeat;\r\n}\r\n}\r\nout:\r\nif (!err && ac->ac_status != AC_STATUS_FOUND && first_err)\r\nerr = first_err;\r\nreturn err;\r\n}\r\nstatic void *ext4_mb_seq_groups_start(struct seq_file *seq, loff_t *pos)\r\n{\r\nstruct super_block *sb = seq->private;\r\next4_group_t group;\r\nif (*pos < 0 || *pos >= ext4_get_groups_count(sb))\r\nreturn NULL;\r\ngroup = *pos + 1;\r\nreturn (void *) ((unsigned long) group);\r\n}\r\nstatic void *ext4_mb_seq_groups_next(struct seq_file *seq, void *v, loff_t *pos)\r\n{\r\nstruct super_block *sb = seq->private;\r\next4_group_t group;\r\n++*pos;\r\nif (*pos < 0 || *pos >= ext4_get_groups_count(sb))\r\nreturn NULL;\r\ngroup = *pos + 1;\r\nreturn (void *) ((unsigned long) group);\r\n}\r\nstatic int ext4_mb_seq_groups_show(struct seq_file *seq, void *v)\r\n{\r\nstruct super_block *sb = seq->private;\r\next4_group_t group = (ext4_group_t) ((unsigned long) v);\r\nint i;\r\nint err, buddy_loaded = 0;\r\nstruct ext4_buddy e4b;\r\nstruct ext4_group_info *grinfo;\r\nstruct sg {\r\nstruct ext4_group_info info;\r\next4_grpblk_t counters[16];\r\n} sg;\r\ngroup--;\r\nif (group == 0)\r\nseq_puts(seq, "#group: free frags first ["\r\n" 2^0 2^1 2^2 2^3 2^4 2^5 2^6 "\r\n" 2^7 2^8 2^9 2^10 2^11 2^12 2^13 ]");\r\ni = (sb->s_blocksize_bits + 2) * sizeof(sg.info.bb_counters[0]) +\r\nsizeof(struct ext4_group_info);\r\ngrinfo = ext4_get_group_info(sb, group);\r\nif (unlikely(EXT4_MB_GRP_NEED_INIT(grinfo))) {\r\nerr = ext4_mb_load_buddy(sb, group, &e4b);\r\nif (err) {\r\nseq_printf(seq, "#%-5u: I/O error\n", group);\r\nreturn 0;\r\n}\r\nbuddy_loaded = 1;\r\n}\r\nmemcpy(&sg, ext4_get_group_info(sb, group), i);\r\nif (buddy_loaded)\r\next4_mb_unload_buddy(&e4b);\r\nseq_printf(seq, "#%-5u: %-5u %-5u %-5u [", group, sg.info.bb_free,\r\nsg.info.bb_fragments, sg.info.bb_first_free);\r\nfor (i = 0; i <= 13; i++)\r\nseq_printf(seq, " %-5u", i <= sb->s_blocksize_bits + 1 ?\r\nsg.info.bb_counters[i] : 0);\r\nseq_printf(seq, " ]\n");\r\nreturn 0;\r\n}\r\nstatic void ext4_mb_seq_groups_stop(struct seq_file *seq, void *v)\r\n{\r\n}\r\nstatic int ext4_mb_seq_groups_open(struct inode *inode, struct file *file)\r\n{\r\nstruct super_block *sb = PDE_DATA(inode);\r\nint rc;\r\nrc = seq_open(file, &ext4_mb_seq_groups_ops);\r\nif (rc == 0) {\r\nstruct seq_file *m = file->private_data;\r\nm->private = sb;\r\n}\r\nreturn rc;\r\n}\r\nstatic struct kmem_cache *get_groupinfo_cache(int blocksize_bits)\r\n{\r\nint cache_index = blocksize_bits - EXT4_MIN_BLOCK_LOG_SIZE;\r\nstruct kmem_cache *cachep = ext4_groupinfo_caches[cache_index];\r\nBUG_ON(!cachep);\r\nreturn cachep;\r\n}\r\nint ext4_mb_alloc_groupinfo(struct super_block *sb, ext4_group_t ngroups)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nunsigned size;\r\nstruct ext4_group_info ***new_groupinfo;\r\nsize = (ngroups + EXT4_DESC_PER_BLOCK(sb) - 1) >>\r\nEXT4_DESC_PER_BLOCK_BITS(sb);\r\nif (size <= sbi->s_group_info_size)\r\nreturn 0;\r\nsize = roundup_pow_of_two(sizeof(*sbi->s_group_info) * size);\r\nnew_groupinfo = ext4_kvzalloc(size, GFP_KERNEL);\r\nif (!new_groupinfo) {\r\next4_msg(sb, KERN_ERR, "can't allocate buddy meta group");\r\nreturn -ENOMEM;\r\n}\r\nif (sbi->s_group_info) {\r\nmemcpy(new_groupinfo, sbi->s_group_info,\r\nsbi->s_group_info_size * sizeof(*sbi->s_group_info));\r\nkvfree(sbi->s_group_info);\r\n}\r\nsbi->s_group_info = new_groupinfo;\r\nsbi->s_group_info_size = size / sizeof(*sbi->s_group_info);\r\next4_debug("allocated s_groupinfo array for %d meta_bg's\n",\r\nsbi->s_group_info_size);\r\nreturn 0;\r\n}\r\nint ext4_mb_add_groupinfo(struct super_block *sb, ext4_group_t group,\r\nstruct ext4_group_desc *desc)\r\n{\r\nint i;\r\nint metalen = 0;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nstruct ext4_group_info **meta_group_info;\r\nstruct kmem_cache *cachep = get_groupinfo_cache(sb->s_blocksize_bits);\r\nif (group % EXT4_DESC_PER_BLOCK(sb) == 0) {\r\nmetalen = sizeof(*meta_group_info) <<\r\nEXT4_DESC_PER_BLOCK_BITS(sb);\r\nmeta_group_info = kmalloc(metalen, GFP_NOFS);\r\nif (meta_group_info == NULL) {\r\next4_msg(sb, KERN_ERR, "can't allocate mem "\r\n"for a buddy group");\r\ngoto exit_meta_group_info;\r\n}\r\nsbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)] =\r\nmeta_group_info;\r\n}\r\nmeta_group_info =\r\nsbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)];\r\ni = group & (EXT4_DESC_PER_BLOCK(sb) - 1);\r\nmeta_group_info[i] = kmem_cache_zalloc(cachep, GFP_NOFS);\r\nif (meta_group_info[i] == NULL) {\r\next4_msg(sb, KERN_ERR, "can't allocate buddy mem");\r\ngoto exit_group_info;\r\n}\r\nset_bit(EXT4_GROUP_INFO_NEED_INIT_BIT,\r\n&(meta_group_info[i]->bb_state));\r\nif (desc->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\r\nmeta_group_info[i]->bb_free =\r\next4_free_clusters_after_init(sb, group, desc);\r\n} else {\r\nmeta_group_info[i]->bb_free =\r\next4_free_group_clusters(sb, desc);\r\n}\r\nINIT_LIST_HEAD(&meta_group_info[i]->bb_prealloc_list);\r\ninit_rwsem(&meta_group_info[i]->alloc_sem);\r\nmeta_group_info[i]->bb_free_root = RB_ROOT;\r\nmeta_group_info[i]->bb_largest_free_order = -1;\r\n#ifdef DOUBLE_CHECK\r\n{\r\nstruct buffer_head *bh;\r\nmeta_group_info[i]->bb_bitmap =\r\nkmalloc(sb->s_blocksize, GFP_NOFS);\r\nBUG_ON(meta_group_info[i]->bb_bitmap == NULL);\r\nbh = ext4_read_block_bitmap(sb, group);\r\nBUG_ON(bh == NULL);\r\nmemcpy(meta_group_info[i]->bb_bitmap, bh->b_data,\r\nsb->s_blocksize);\r\nput_bh(bh);\r\n}\r\n#endif\r\nreturn 0;\r\nexit_group_info:\r\nif (group % EXT4_DESC_PER_BLOCK(sb) == 0) {\r\nkfree(sbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)]);\r\nsbi->s_group_info[group >> EXT4_DESC_PER_BLOCK_BITS(sb)] = NULL;\r\n}\r\nexit_meta_group_info:\r\nreturn -ENOMEM;\r\n}\r\nstatic int ext4_mb_init_backend(struct super_block *sb)\r\n{\r\next4_group_t ngroups = ext4_get_groups_count(sb);\r\next4_group_t i;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nint err;\r\nstruct ext4_group_desc *desc;\r\nstruct kmem_cache *cachep;\r\nerr = ext4_mb_alloc_groupinfo(sb, ngroups);\r\nif (err)\r\nreturn err;\r\nsbi->s_buddy_cache = new_inode(sb);\r\nif (sbi->s_buddy_cache == NULL) {\r\next4_msg(sb, KERN_ERR, "can't get new inode");\r\ngoto err_freesgi;\r\n}\r\nsbi->s_buddy_cache->i_ino = EXT4_BAD_INO;\r\nEXT4_I(sbi->s_buddy_cache)->i_disksize = 0;\r\nfor (i = 0; i < ngroups; i++) {\r\ndesc = ext4_get_group_desc(sb, i, NULL);\r\nif (desc == NULL) {\r\next4_msg(sb, KERN_ERR, "can't read descriptor %u", i);\r\ngoto err_freebuddy;\r\n}\r\nif (ext4_mb_add_groupinfo(sb, i, desc) != 0)\r\ngoto err_freebuddy;\r\n}\r\nreturn 0;\r\nerr_freebuddy:\r\ncachep = get_groupinfo_cache(sb->s_blocksize_bits);\r\nwhile (i-- > 0)\r\nkmem_cache_free(cachep, ext4_get_group_info(sb, i));\r\ni = sbi->s_group_info_size;\r\nwhile (i-- > 0)\r\nkfree(sbi->s_group_info[i]);\r\niput(sbi->s_buddy_cache);\r\nerr_freesgi:\r\nkvfree(sbi->s_group_info);\r\nreturn -ENOMEM;\r\n}\r\nstatic void ext4_groupinfo_destroy_slabs(void)\r\n{\r\nint i;\r\nfor (i = 0; i < NR_GRPINFO_CACHES; i++) {\r\nif (ext4_groupinfo_caches[i])\r\nkmem_cache_destroy(ext4_groupinfo_caches[i]);\r\next4_groupinfo_caches[i] = NULL;\r\n}\r\n}\r\nstatic int ext4_groupinfo_create_slab(size_t size)\r\n{\r\nstatic DEFINE_MUTEX(ext4_grpinfo_slab_create_mutex);\r\nint slab_size;\r\nint blocksize_bits = order_base_2(size);\r\nint cache_index = blocksize_bits - EXT4_MIN_BLOCK_LOG_SIZE;\r\nstruct kmem_cache *cachep;\r\nif (cache_index >= NR_GRPINFO_CACHES)\r\nreturn -EINVAL;\r\nif (unlikely(cache_index < 0))\r\ncache_index = 0;\r\nmutex_lock(&ext4_grpinfo_slab_create_mutex);\r\nif (ext4_groupinfo_caches[cache_index]) {\r\nmutex_unlock(&ext4_grpinfo_slab_create_mutex);\r\nreturn 0;\r\n}\r\nslab_size = offsetof(struct ext4_group_info,\r\nbb_counters[blocksize_bits + 2]);\r\ncachep = kmem_cache_create(ext4_groupinfo_slab_names[cache_index],\r\nslab_size, 0, SLAB_RECLAIM_ACCOUNT,\r\nNULL);\r\next4_groupinfo_caches[cache_index] = cachep;\r\nmutex_unlock(&ext4_grpinfo_slab_create_mutex);\r\nif (!cachep) {\r\nprintk(KERN_EMERG\r\n"EXT4-fs: no memory for groupinfo slab cache\n");\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nint ext4_mb_init(struct super_block *sb)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nunsigned i, j;\r\nunsigned offset;\r\nunsigned max;\r\nint ret;\r\ni = (sb->s_blocksize_bits + 2) * sizeof(*sbi->s_mb_offsets);\r\nsbi->s_mb_offsets = kmalloc(i, GFP_KERNEL);\r\nif (sbi->s_mb_offsets == NULL) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ni = (sb->s_blocksize_bits + 2) * sizeof(*sbi->s_mb_maxs);\r\nsbi->s_mb_maxs = kmalloc(i, GFP_KERNEL);\r\nif (sbi->s_mb_maxs == NULL) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = ext4_groupinfo_create_slab(sb->s_blocksize);\r\nif (ret < 0)\r\ngoto out;\r\nsbi->s_mb_maxs[0] = sb->s_blocksize << 3;\r\nsbi->s_mb_offsets[0] = 0;\r\ni = 1;\r\noffset = 0;\r\nmax = sb->s_blocksize << 2;\r\ndo {\r\nsbi->s_mb_offsets[i] = offset;\r\nsbi->s_mb_maxs[i] = max;\r\noffset += 1 << (sb->s_blocksize_bits - i);\r\nmax = max >> 1;\r\ni++;\r\n} while (i <= sb->s_blocksize_bits + 1);\r\nspin_lock_init(&sbi->s_md_lock);\r\nspin_lock_init(&sbi->s_bal_lock);\r\nsbi->s_mb_max_to_scan = MB_DEFAULT_MAX_TO_SCAN;\r\nsbi->s_mb_min_to_scan = MB_DEFAULT_MIN_TO_SCAN;\r\nsbi->s_mb_stats = MB_DEFAULT_STATS;\r\nsbi->s_mb_stream_request = MB_DEFAULT_STREAM_THRESHOLD;\r\nsbi->s_mb_order2_reqs = MB_DEFAULT_ORDER2_REQS;\r\nsbi->s_mb_group_prealloc = max(MB_DEFAULT_GROUP_PREALLOC >>\r\nsbi->s_cluster_bits, 32);\r\nif (sbi->s_stripe > 1) {\r\nsbi->s_mb_group_prealloc = roundup(\r\nsbi->s_mb_group_prealloc, sbi->s_stripe);\r\n}\r\nsbi->s_locality_groups = alloc_percpu(struct ext4_locality_group);\r\nif (sbi->s_locality_groups == NULL) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nfor_each_possible_cpu(i) {\r\nstruct ext4_locality_group *lg;\r\nlg = per_cpu_ptr(sbi->s_locality_groups, i);\r\nmutex_init(&lg->lg_mutex);\r\nfor (j = 0; j < PREALLOC_TB_SIZE; j++)\r\nINIT_LIST_HEAD(&lg->lg_prealloc_list[j]);\r\nspin_lock_init(&lg->lg_prealloc_lock);\r\n}\r\nret = ext4_mb_init_backend(sb);\r\nif (ret != 0)\r\ngoto out_free_locality_groups;\r\nif (sbi->s_proc)\r\nproc_create_data("mb_groups", S_IRUGO, sbi->s_proc,\r\n&ext4_mb_seq_groups_fops, sb);\r\nreturn 0;\r\nout_free_locality_groups:\r\nfree_percpu(sbi->s_locality_groups);\r\nsbi->s_locality_groups = NULL;\r\nout:\r\nkfree(sbi->s_mb_offsets);\r\nsbi->s_mb_offsets = NULL;\r\nkfree(sbi->s_mb_maxs);\r\nsbi->s_mb_maxs = NULL;\r\nreturn ret;\r\n}\r\nstatic void ext4_mb_cleanup_pa(struct ext4_group_info *grp)\r\n{\r\nstruct ext4_prealloc_space *pa;\r\nstruct list_head *cur, *tmp;\r\nint count = 0;\r\nlist_for_each_safe(cur, tmp, &grp->bb_prealloc_list) {\r\npa = list_entry(cur, struct ext4_prealloc_space, pa_group_list);\r\nlist_del(&pa->pa_group_list);\r\ncount++;\r\nkmem_cache_free(ext4_pspace_cachep, pa);\r\n}\r\nif (count)\r\nmb_debug(1, "mballoc: %u PAs left\n", count);\r\n}\r\nint ext4_mb_release(struct super_block *sb)\r\n{\r\next4_group_t ngroups = ext4_get_groups_count(sb);\r\next4_group_t i;\r\nint num_meta_group_infos;\r\nstruct ext4_group_info *grinfo;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nstruct kmem_cache *cachep = get_groupinfo_cache(sb->s_blocksize_bits);\r\nif (sbi->s_proc)\r\nremove_proc_entry("mb_groups", sbi->s_proc);\r\nif (sbi->s_group_info) {\r\nfor (i = 0; i < ngroups; i++) {\r\ngrinfo = ext4_get_group_info(sb, i);\r\n#ifdef DOUBLE_CHECK\r\nkfree(grinfo->bb_bitmap);\r\n#endif\r\next4_lock_group(sb, i);\r\next4_mb_cleanup_pa(grinfo);\r\next4_unlock_group(sb, i);\r\nkmem_cache_free(cachep, grinfo);\r\n}\r\nnum_meta_group_infos = (ngroups +\r\nEXT4_DESC_PER_BLOCK(sb) - 1) >>\r\nEXT4_DESC_PER_BLOCK_BITS(sb);\r\nfor (i = 0; i < num_meta_group_infos; i++)\r\nkfree(sbi->s_group_info[i]);\r\nkvfree(sbi->s_group_info);\r\n}\r\nkfree(sbi->s_mb_offsets);\r\nkfree(sbi->s_mb_maxs);\r\niput(sbi->s_buddy_cache);\r\nif (sbi->s_mb_stats) {\r\next4_msg(sb, KERN_INFO,\r\n"mballoc: %u blocks %u reqs (%u success)",\r\natomic_read(&sbi->s_bal_allocated),\r\natomic_read(&sbi->s_bal_reqs),\r\natomic_read(&sbi->s_bal_success));\r\next4_msg(sb, KERN_INFO,\r\n"mballoc: %u extents scanned, %u goal hits, "\r\n"%u 2^N hits, %u breaks, %u lost",\r\natomic_read(&sbi->s_bal_ex_scanned),\r\natomic_read(&sbi->s_bal_goals),\r\natomic_read(&sbi->s_bal_2orders),\r\natomic_read(&sbi->s_bal_breaks),\r\natomic_read(&sbi->s_mb_lost_chunks));\r\next4_msg(sb, KERN_INFO,\r\n"mballoc: %lu generated and it took %Lu",\r\nsbi->s_mb_buddies_generated,\r\nsbi->s_mb_generation_time);\r\next4_msg(sb, KERN_INFO,\r\n"mballoc: %u preallocated, %u discarded",\r\natomic_read(&sbi->s_mb_preallocated),\r\natomic_read(&sbi->s_mb_discarded));\r\n}\r\nfree_percpu(sbi->s_locality_groups);\r\nreturn 0;\r\n}\r\nstatic inline int ext4_issue_discard(struct super_block *sb,\r\next4_group_t block_group, ext4_grpblk_t cluster, int count)\r\n{\r\next4_fsblk_t discard_block;\r\ndiscard_block = (EXT4_C2B(EXT4_SB(sb), cluster) +\r\next4_group_first_block_no(sb, block_group));\r\ncount = EXT4_C2B(EXT4_SB(sb), count);\r\ntrace_ext4_discard_blocks(sb,\r\n(unsigned long long) discard_block, count);\r\nreturn sb_issue_discard(sb, discard_block, count, GFP_NOFS, 0);\r\n}\r\nstatic void ext4_free_data_callback(struct super_block *sb,\r\nstruct ext4_journal_cb_entry *jce,\r\nint rc)\r\n{\r\nstruct ext4_free_data *entry = (struct ext4_free_data *)jce;\r\nstruct ext4_buddy e4b;\r\nstruct ext4_group_info *db;\r\nint err, count = 0, count2 = 0;\r\nmb_debug(1, "gonna free %u blocks in group %u (0x%p):",\r\nentry->efd_count, entry->efd_group, entry);\r\nif (test_opt(sb, DISCARD)) {\r\nerr = ext4_issue_discard(sb, entry->efd_group,\r\nentry->efd_start_cluster,\r\nentry->efd_count);\r\nif (err && err != -EOPNOTSUPP)\r\next4_msg(sb, KERN_WARNING, "discard request in"\r\n" group:%d block:%d count:%d failed"\r\n" with %d", entry->efd_group,\r\nentry->efd_start_cluster,\r\nentry->efd_count, err);\r\n}\r\nerr = ext4_mb_load_buddy(sb, entry->efd_group, &e4b);\r\nBUG_ON(err != 0);\r\ndb = e4b.bd_info;\r\ncount += entry->efd_count;\r\ncount2++;\r\next4_lock_group(sb, entry->efd_group);\r\nrb_erase(&entry->efd_node, &(db->bb_free_root));\r\nmb_free_blocks(NULL, &e4b, entry->efd_start_cluster, entry->efd_count);\r\nif (!test_opt(sb, DISCARD))\r\nEXT4_MB_GRP_CLEAR_TRIMMED(db);\r\nif (!db->bb_free_root.rb_node) {\r\npage_cache_release(e4b.bd_buddy_page);\r\npage_cache_release(e4b.bd_bitmap_page);\r\n}\r\next4_unlock_group(sb, entry->efd_group);\r\nkmem_cache_free(ext4_free_data_cachep, entry);\r\next4_mb_unload_buddy(&e4b);\r\nmb_debug(1, "freed %u blocks in %u structures\n", count, count2);\r\n}\r\nint __init ext4_init_mballoc(void)\r\n{\r\next4_pspace_cachep = KMEM_CACHE(ext4_prealloc_space,\r\nSLAB_RECLAIM_ACCOUNT);\r\nif (ext4_pspace_cachep == NULL)\r\nreturn -ENOMEM;\r\next4_ac_cachep = KMEM_CACHE(ext4_allocation_context,\r\nSLAB_RECLAIM_ACCOUNT);\r\nif (ext4_ac_cachep == NULL) {\r\nkmem_cache_destroy(ext4_pspace_cachep);\r\nreturn -ENOMEM;\r\n}\r\next4_free_data_cachep = KMEM_CACHE(ext4_free_data,\r\nSLAB_RECLAIM_ACCOUNT);\r\nif (ext4_free_data_cachep == NULL) {\r\nkmem_cache_destroy(ext4_pspace_cachep);\r\nkmem_cache_destroy(ext4_ac_cachep);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nvoid ext4_exit_mballoc(void)\r\n{\r\nrcu_barrier();\r\nkmem_cache_destroy(ext4_pspace_cachep);\r\nkmem_cache_destroy(ext4_ac_cachep);\r\nkmem_cache_destroy(ext4_free_data_cachep);\r\next4_groupinfo_destroy_slabs();\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,\r\nhandle_t *handle, unsigned int reserv_clstrs)\r\n{\r\nstruct buffer_head *bitmap_bh = NULL;\r\nstruct ext4_group_desc *gdp;\r\nstruct buffer_head *gdp_bh;\r\nstruct ext4_sb_info *sbi;\r\nstruct super_block *sb;\r\next4_fsblk_t block;\r\nint err, len;\r\nBUG_ON(ac->ac_status != AC_STATUS_FOUND);\r\nBUG_ON(ac->ac_b_ex.fe_len <= 0);\r\nsb = ac->ac_sb;\r\nsbi = EXT4_SB(sb);\r\nerr = -EIO;\r\nbitmap_bh = ext4_read_block_bitmap(sb, ac->ac_b_ex.fe_group);\r\nif (!bitmap_bh)\r\ngoto out_err;\r\nBUFFER_TRACE(bitmap_bh, "getting write access");\r\nerr = ext4_journal_get_write_access(handle, bitmap_bh);\r\nif (err)\r\ngoto out_err;\r\nerr = -EIO;\r\ngdp = ext4_get_group_desc(sb, ac->ac_b_ex.fe_group, &gdp_bh);\r\nif (!gdp)\r\ngoto out_err;\r\next4_debug("using block group %u(%d)\n", ac->ac_b_ex.fe_group,\r\next4_free_group_clusters(sb, gdp));\r\nBUFFER_TRACE(gdp_bh, "get_write_access");\r\nerr = ext4_journal_get_write_access(handle, gdp_bh);\r\nif (err)\r\ngoto out_err;\r\nblock = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\r\nlen = EXT4_C2B(sbi, ac->ac_b_ex.fe_len);\r\nif (!ext4_data_block_valid(sbi, block, len)) {\r\next4_error(sb, "Allocating blocks %llu-%llu which overlap "\r\n"fs metadata", block, block+len);\r\next4_lock_group(sb, ac->ac_b_ex.fe_group);\r\next4_set_bits(bitmap_bh->b_data, ac->ac_b_ex.fe_start,\r\nac->ac_b_ex.fe_len);\r\next4_unlock_group(sb, ac->ac_b_ex.fe_group);\r\nerr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\r\nif (!err)\r\nerr = -EAGAIN;\r\ngoto out_err;\r\n}\r\next4_lock_group(sb, ac->ac_b_ex.fe_group);\r\n#ifdef AGGRESSIVE_CHECK\r\n{\r\nint i;\r\nfor (i = 0; i < ac->ac_b_ex.fe_len; i++) {\r\nBUG_ON(mb_test_bit(ac->ac_b_ex.fe_start + i,\r\nbitmap_bh->b_data));\r\n}\r\n}\r\n#endif\r\next4_set_bits(bitmap_bh->b_data, ac->ac_b_ex.fe_start,\r\nac->ac_b_ex.fe_len);\r\nif (gdp->bg_flags & cpu_to_le16(EXT4_BG_BLOCK_UNINIT)) {\r\ngdp->bg_flags &= cpu_to_le16(~EXT4_BG_BLOCK_UNINIT);\r\next4_free_group_clusters_set(sb, gdp,\r\next4_free_clusters_after_init(sb,\r\nac->ac_b_ex.fe_group, gdp));\r\n}\r\nlen = ext4_free_group_clusters(sb, gdp) - ac->ac_b_ex.fe_len;\r\next4_free_group_clusters_set(sb, gdp, len);\r\next4_block_bitmap_csum_set(sb, ac->ac_b_ex.fe_group, gdp, bitmap_bh);\r\next4_group_desc_csum_set(sb, ac->ac_b_ex.fe_group, gdp);\r\next4_unlock_group(sb, ac->ac_b_ex.fe_group);\r\npercpu_counter_sub(&sbi->s_freeclusters_counter, ac->ac_b_ex.fe_len);\r\nif (!(ac->ac_flags & EXT4_MB_DELALLOC_RESERVED))\r\npercpu_counter_sub(&sbi->s_dirtyclusters_counter,\r\nreserv_clstrs);\r\nif (sbi->s_log_groups_per_flex) {\r\next4_group_t flex_group = ext4_flex_group(sbi,\r\nac->ac_b_ex.fe_group);\r\natomic64_sub(ac->ac_b_ex.fe_len,\r\n&sbi->s_flex_groups[flex_group].free_clusters);\r\n}\r\nerr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\r\nif (err)\r\ngoto out_err;\r\nerr = ext4_handle_dirty_metadata(handle, NULL, gdp_bh);\r\nout_err:\r\nbrelse(bitmap_bh);\r\nreturn err;\r\n}\r\nstatic void ext4_mb_normalize_group_request(struct ext4_allocation_context *ac)\r\n{\r\nstruct super_block *sb = ac->ac_sb;\r\nstruct ext4_locality_group *lg = ac->ac_lg;\r\nBUG_ON(lg == NULL);\r\nac->ac_g_ex.fe_len = EXT4_SB(sb)->s_mb_group_prealloc;\r\nmb_debug(1, "#%u: goal %u blocks for locality group\n",\r\ncurrent->pid, ac->ac_g_ex.fe_len);\r\n}\r\nstatic noinline_for_stack void\r\next4_mb_normalize_request(struct ext4_allocation_context *ac,\r\nstruct ext4_allocation_request *ar)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\r\nint bsbits, max;\r\next4_lblk_t end;\r\nloff_t size, start_off;\r\nloff_t orig_size __maybe_unused;\r\next4_lblk_t start;\r\nstruct ext4_inode_info *ei = EXT4_I(ac->ac_inode);\r\nstruct ext4_prealloc_space *pa;\r\nif (!(ac->ac_flags & EXT4_MB_HINT_DATA))\r\nreturn;\r\nif (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY))\r\nreturn;\r\nif (ac->ac_flags & EXT4_MB_HINT_NOPREALLOC)\r\nreturn;\r\nif (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC) {\r\next4_mb_normalize_group_request(ac);\r\nreturn ;\r\n}\r\nbsbits = ac->ac_sb->s_blocksize_bits;\r\nsize = ac->ac_o_ex.fe_logical + EXT4_C2B(sbi, ac->ac_o_ex.fe_len);\r\nsize = size << bsbits;\r\nif (size < i_size_read(ac->ac_inode))\r\nsize = i_size_read(ac->ac_inode);\r\norig_size = size;\r\nmax = 2 << bsbits;\r\n#define NRL_CHECK_SIZE(req, size, max, chunk_size) \\r\n(req <= (size) || max <= (chunk_size))\r\nstart_off = 0;\r\nif (size <= 16 * 1024) {\r\nsize = 16 * 1024;\r\n} else if (size <= 32 * 1024) {\r\nsize = 32 * 1024;\r\n} else if (size <= 64 * 1024) {\r\nsize = 64 * 1024;\r\n} else if (size <= 128 * 1024) {\r\nsize = 128 * 1024;\r\n} else if (size <= 256 * 1024) {\r\nsize = 256 * 1024;\r\n} else if (size <= 512 * 1024) {\r\nsize = 512 * 1024;\r\n} else if (size <= 1024 * 1024) {\r\nsize = 1024 * 1024;\r\n} else if (NRL_CHECK_SIZE(size, 4 * 1024 * 1024, max, 2 * 1024)) {\r\nstart_off = ((loff_t)ac->ac_o_ex.fe_logical >>\r\n(21 - bsbits)) << 21;\r\nsize = 2 * 1024 * 1024;\r\n} else if (NRL_CHECK_SIZE(size, 8 * 1024 * 1024, max, 4 * 1024)) {\r\nstart_off = ((loff_t)ac->ac_o_ex.fe_logical >>\r\n(22 - bsbits)) << 22;\r\nsize = 4 * 1024 * 1024;\r\n} else if (NRL_CHECK_SIZE(ac->ac_o_ex.fe_len,\r\n(8<<20)>>bsbits, max, 8 * 1024)) {\r\nstart_off = ((loff_t)ac->ac_o_ex.fe_logical >>\r\n(23 - bsbits)) << 23;\r\nsize = 8 * 1024 * 1024;\r\n} else {\r\nstart_off = (loff_t) ac->ac_o_ex.fe_logical << bsbits;\r\nsize = (loff_t) EXT4_C2B(EXT4_SB(ac->ac_sb),\r\nac->ac_o_ex.fe_len) << bsbits;\r\n}\r\nsize = size >> bsbits;\r\nstart = start_off >> bsbits;\r\nif (ar->pleft && start <= ar->lleft) {\r\nsize -= ar->lleft + 1 - start;\r\nstart = ar->lleft + 1;\r\n}\r\nif (ar->pright && start + size - 1 >= ar->lright)\r\nsize -= start + size - ar->lright;\r\nend = start + size;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) {\r\next4_lblk_t pa_end;\r\nif (pa->pa_deleted)\r\ncontinue;\r\nspin_lock(&pa->pa_lock);\r\nif (pa->pa_deleted) {\r\nspin_unlock(&pa->pa_lock);\r\ncontinue;\r\n}\r\npa_end = pa->pa_lstart + EXT4_C2B(EXT4_SB(ac->ac_sb),\r\npa->pa_len);\r\nBUG_ON(!(ac->ac_o_ex.fe_logical >= pa_end ||\r\nac->ac_o_ex.fe_logical < pa->pa_lstart));\r\nif (pa->pa_lstart >= end || pa_end <= start) {\r\nspin_unlock(&pa->pa_lock);\r\ncontinue;\r\n}\r\nBUG_ON(pa->pa_lstart <= start && pa_end >= end);\r\nif (pa_end <= ac->ac_o_ex.fe_logical) {\r\nBUG_ON(pa_end < start);\r\nstart = pa_end;\r\n} else if (pa->pa_lstart > ac->ac_o_ex.fe_logical) {\r\nBUG_ON(pa->pa_lstart > end);\r\nend = pa->pa_lstart;\r\n}\r\nspin_unlock(&pa->pa_lock);\r\n}\r\nrcu_read_unlock();\r\nsize = end - start;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) {\r\next4_lblk_t pa_end;\r\nspin_lock(&pa->pa_lock);\r\nif (pa->pa_deleted == 0) {\r\npa_end = pa->pa_lstart + EXT4_C2B(EXT4_SB(ac->ac_sb),\r\npa->pa_len);\r\nBUG_ON(!(start >= pa_end || end <= pa->pa_lstart));\r\n}\r\nspin_unlock(&pa->pa_lock);\r\n}\r\nrcu_read_unlock();\r\nif (start + size <= ac->ac_o_ex.fe_logical &&\r\nstart > ac->ac_o_ex.fe_logical) {\r\next4_msg(ac->ac_sb, KERN_ERR,\r\n"start %lu, size %lu, fe_logical %lu",\r\n(unsigned long) start, (unsigned long) size,\r\n(unsigned long) ac->ac_o_ex.fe_logical);\r\nBUG();\r\n}\r\nBUG_ON(size <= 0 || size > EXT4_BLOCKS_PER_GROUP(ac->ac_sb));\r\nac->ac_g_ex.fe_logical = start;\r\nac->ac_g_ex.fe_len = EXT4_NUM_B2C(sbi, size);\r\nif (ar->pright && (ar->lright == (start + size))) {\r\next4_get_group_no_and_offset(ac->ac_sb, ar->pright - size,\r\n&ac->ac_f_ex.fe_group,\r\n&ac->ac_f_ex.fe_start);\r\nac->ac_flags |= EXT4_MB_HINT_TRY_GOAL;\r\n}\r\nif (ar->pleft && (ar->lleft + 1 == start)) {\r\next4_get_group_no_and_offset(ac->ac_sb, ar->pleft + 1,\r\n&ac->ac_f_ex.fe_group,\r\n&ac->ac_f_ex.fe_start);\r\nac->ac_flags |= EXT4_MB_HINT_TRY_GOAL;\r\n}\r\nmb_debug(1, "goal: %u(was %u) blocks at %u\n", (unsigned) size,\r\n(unsigned) orig_size, (unsigned) start);\r\n}\r\nstatic void ext4_mb_collect_stats(struct ext4_allocation_context *ac)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\r\nif (sbi->s_mb_stats && ac->ac_g_ex.fe_len > 1) {\r\natomic_inc(&sbi->s_bal_reqs);\r\natomic_add(ac->ac_b_ex.fe_len, &sbi->s_bal_allocated);\r\nif (ac->ac_b_ex.fe_len >= ac->ac_o_ex.fe_len)\r\natomic_inc(&sbi->s_bal_success);\r\natomic_add(ac->ac_found, &sbi->s_bal_ex_scanned);\r\nif (ac->ac_g_ex.fe_start == ac->ac_b_ex.fe_start &&\r\nac->ac_g_ex.fe_group == ac->ac_b_ex.fe_group)\r\natomic_inc(&sbi->s_bal_goals);\r\nif (ac->ac_found > sbi->s_mb_max_to_scan)\r\natomic_inc(&sbi->s_bal_breaks);\r\n}\r\nif (ac->ac_op == EXT4_MB_HISTORY_ALLOC)\r\ntrace_ext4_mballoc_alloc(ac);\r\nelse\r\ntrace_ext4_mballoc_prealloc(ac);\r\n}\r\nstatic void ext4_discard_allocated_blocks(struct ext4_allocation_context *ac)\r\n{\r\nstruct ext4_prealloc_space *pa = ac->ac_pa;\r\nstruct ext4_buddy e4b;\r\nint err;\r\nif (pa == NULL) {\r\nif (ac->ac_f_ex.fe_len == 0)\r\nreturn;\r\nerr = ext4_mb_load_buddy(ac->ac_sb, ac->ac_f_ex.fe_group, &e4b);\r\nif (err) {\r\nWARN(1, "mb_load_buddy failed (%d)", err);\r\nreturn;\r\n}\r\next4_lock_group(ac->ac_sb, ac->ac_f_ex.fe_group);\r\nmb_free_blocks(ac->ac_inode, &e4b, ac->ac_f_ex.fe_start,\r\nac->ac_f_ex.fe_len);\r\next4_unlock_group(ac->ac_sb, ac->ac_f_ex.fe_group);\r\next4_mb_unload_buddy(&e4b);\r\nreturn;\r\n}\r\nif (pa->pa_type == MB_INODE_PA)\r\npa->pa_free += ac->ac_b_ex.fe_len;\r\n}\r\nstatic void ext4_mb_use_inode_pa(struct ext4_allocation_context *ac,\r\nstruct ext4_prealloc_space *pa)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\r\next4_fsblk_t start;\r\next4_fsblk_t end;\r\nint len;\r\nstart = pa->pa_pstart + (ac->ac_o_ex.fe_logical - pa->pa_lstart);\r\nend = min(pa->pa_pstart + EXT4_C2B(sbi, pa->pa_len),\r\nstart + EXT4_C2B(sbi, ac->ac_o_ex.fe_len));\r\nlen = EXT4_NUM_B2C(sbi, end - start);\r\next4_get_group_no_and_offset(ac->ac_sb, start, &ac->ac_b_ex.fe_group,\r\n&ac->ac_b_ex.fe_start);\r\nac->ac_b_ex.fe_len = len;\r\nac->ac_status = AC_STATUS_FOUND;\r\nac->ac_pa = pa;\r\nBUG_ON(start < pa->pa_pstart);\r\nBUG_ON(end > pa->pa_pstart + EXT4_C2B(sbi, pa->pa_len));\r\nBUG_ON(pa->pa_free < len);\r\npa->pa_free -= len;\r\nmb_debug(1, "use %llu/%u from inode pa %p\n", start, len, pa);\r\n}\r\nstatic void ext4_mb_use_group_pa(struct ext4_allocation_context *ac,\r\nstruct ext4_prealloc_space *pa)\r\n{\r\nunsigned int len = ac->ac_o_ex.fe_len;\r\next4_get_group_no_and_offset(ac->ac_sb, pa->pa_pstart,\r\n&ac->ac_b_ex.fe_group,\r\n&ac->ac_b_ex.fe_start);\r\nac->ac_b_ex.fe_len = len;\r\nac->ac_status = AC_STATUS_FOUND;\r\nac->ac_pa = pa;\r\nmb_debug(1, "use %u/%u from group pa %p\n", pa->pa_lstart-len, len, pa);\r\n}\r\nstatic struct ext4_prealloc_space *\r\next4_mb_check_group_pa(ext4_fsblk_t goal_block,\r\nstruct ext4_prealloc_space *pa,\r\nstruct ext4_prealloc_space *cpa)\r\n{\r\next4_fsblk_t cur_distance, new_distance;\r\nif (cpa == NULL) {\r\natomic_inc(&pa->pa_count);\r\nreturn pa;\r\n}\r\ncur_distance = abs(goal_block - cpa->pa_pstart);\r\nnew_distance = abs(goal_block - pa->pa_pstart);\r\nif (cur_distance <= new_distance)\r\nreturn cpa;\r\natomic_dec(&cpa->pa_count);\r\natomic_inc(&pa->pa_count);\r\nreturn pa;\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_use_preallocated(struct ext4_allocation_context *ac)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\r\nint order, i;\r\nstruct ext4_inode_info *ei = EXT4_I(ac->ac_inode);\r\nstruct ext4_locality_group *lg;\r\nstruct ext4_prealloc_space *pa, *cpa = NULL;\r\next4_fsblk_t goal_block;\r\nif (!(ac->ac_flags & EXT4_MB_HINT_DATA))\r\nreturn 0;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(pa, &ei->i_prealloc_list, pa_inode_list) {\r\nif (ac->ac_o_ex.fe_logical < pa->pa_lstart ||\r\nac->ac_o_ex.fe_logical >= (pa->pa_lstart +\r\nEXT4_C2B(sbi, pa->pa_len)))\r\ncontinue;\r\nif (!(ext4_test_inode_flag(ac->ac_inode, EXT4_INODE_EXTENTS)) &&\r\n(pa->pa_pstart + EXT4_C2B(sbi, pa->pa_len) >\r\nEXT4_MAX_BLOCK_FILE_PHYS))\r\ncontinue;\r\nspin_lock(&pa->pa_lock);\r\nif (pa->pa_deleted == 0 && pa->pa_free) {\r\natomic_inc(&pa->pa_count);\r\next4_mb_use_inode_pa(ac, pa);\r\nspin_unlock(&pa->pa_lock);\r\nac->ac_criteria = 10;\r\nrcu_read_unlock();\r\nreturn 1;\r\n}\r\nspin_unlock(&pa->pa_lock);\r\n}\r\nrcu_read_unlock();\r\nif (!(ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC))\r\nreturn 0;\r\nlg = ac->ac_lg;\r\nif (lg == NULL)\r\nreturn 0;\r\norder = fls(ac->ac_o_ex.fe_len) - 1;\r\nif (order > PREALLOC_TB_SIZE - 1)\r\norder = PREALLOC_TB_SIZE - 1;\r\ngoal_block = ext4_grp_offs_to_block(ac->ac_sb, &ac->ac_g_ex);\r\nfor (i = order; i < PREALLOC_TB_SIZE; i++) {\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(pa, &lg->lg_prealloc_list[i],\r\npa_inode_list) {\r\nspin_lock(&pa->pa_lock);\r\nif (pa->pa_deleted == 0 &&\r\npa->pa_free >= ac->ac_o_ex.fe_len) {\r\ncpa = ext4_mb_check_group_pa(goal_block,\r\npa, cpa);\r\n}\r\nspin_unlock(&pa->pa_lock);\r\n}\r\nrcu_read_unlock();\r\n}\r\nif (cpa) {\r\next4_mb_use_group_pa(ac, cpa);\r\nac->ac_criteria = 20;\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ext4_mb_generate_from_freelist(struct super_block *sb, void *bitmap,\r\next4_group_t group)\r\n{\r\nstruct rb_node *n;\r\nstruct ext4_group_info *grp;\r\nstruct ext4_free_data *entry;\r\ngrp = ext4_get_group_info(sb, group);\r\nn = rb_first(&(grp->bb_free_root));\r\nwhile (n) {\r\nentry = rb_entry(n, struct ext4_free_data, efd_node);\r\next4_set_bits(bitmap, entry->efd_start_cluster, entry->efd_count);\r\nn = rb_next(n);\r\n}\r\nreturn;\r\n}\r\nstatic noinline_for_stack\r\nvoid ext4_mb_generate_from_pa(struct super_block *sb, void *bitmap,\r\next4_group_t group)\r\n{\r\nstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\r\nstruct ext4_prealloc_space *pa;\r\nstruct list_head *cur;\r\next4_group_t groupnr;\r\next4_grpblk_t start;\r\nint preallocated = 0;\r\nint len;\r\nlist_for_each(cur, &grp->bb_prealloc_list) {\r\npa = list_entry(cur, struct ext4_prealloc_space, pa_group_list);\r\nspin_lock(&pa->pa_lock);\r\next4_get_group_no_and_offset(sb, pa->pa_pstart,\r\n&groupnr, &start);\r\nlen = pa->pa_len;\r\nspin_unlock(&pa->pa_lock);\r\nif (unlikely(len == 0))\r\ncontinue;\r\nBUG_ON(groupnr != group);\r\next4_set_bits(bitmap, start, len);\r\npreallocated += len;\r\n}\r\nmb_debug(1, "prellocated %u for group %u\n", preallocated, group);\r\n}\r\nstatic void ext4_mb_pa_callback(struct rcu_head *head)\r\n{\r\nstruct ext4_prealloc_space *pa;\r\npa = container_of(head, struct ext4_prealloc_space, u.pa_rcu);\r\nBUG_ON(atomic_read(&pa->pa_count));\r\nBUG_ON(pa->pa_deleted == 0);\r\nkmem_cache_free(ext4_pspace_cachep, pa);\r\n}\r\nstatic void ext4_mb_put_pa(struct ext4_allocation_context *ac,\r\nstruct super_block *sb, struct ext4_prealloc_space *pa)\r\n{\r\next4_group_t grp;\r\next4_fsblk_t grp_blk;\r\nspin_lock(&pa->pa_lock);\r\nif (!atomic_dec_and_test(&pa->pa_count) || pa->pa_free != 0) {\r\nspin_unlock(&pa->pa_lock);\r\nreturn;\r\n}\r\nif (pa->pa_deleted == 1) {\r\nspin_unlock(&pa->pa_lock);\r\nreturn;\r\n}\r\npa->pa_deleted = 1;\r\nspin_unlock(&pa->pa_lock);\r\ngrp_blk = pa->pa_pstart;\r\nif (pa->pa_type == MB_GROUP_PA)\r\ngrp_blk--;\r\ngrp = ext4_get_group_number(sb, grp_blk);\r\next4_lock_group(sb, grp);\r\nlist_del(&pa->pa_group_list);\r\next4_unlock_group(sb, grp);\r\nspin_lock(pa->pa_obj_lock);\r\nlist_del_rcu(&pa->pa_inode_list);\r\nspin_unlock(pa->pa_obj_lock);\r\ncall_rcu(&(pa)->u.pa_rcu, ext4_mb_pa_callback);\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_new_inode_pa(struct ext4_allocation_context *ac)\r\n{\r\nstruct super_block *sb = ac->ac_sb;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nstruct ext4_prealloc_space *pa;\r\nstruct ext4_group_info *grp;\r\nstruct ext4_inode_info *ei;\r\nBUG_ON(ac->ac_o_ex.fe_len >= ac->ac_b_ex.fe_len);\r\nBUG_ON(ac->ac_status != AC_STATUS_FOUND);\r\nBUG_ON(!S_ISREG(ac->ac_inode->i_mode));\r\npa = kmem_cache_alloc(ext4_pspace_cachep, GFP_NOFS);\r\nif (pa == NULL)\r\nreturn -ENOMEM;\r\nif (ac->ac_b_ex.fe_len < ac->ac_g_ex.fe_len) {\r\nint winl;\r\nint wins;\r\nint win;\r\nint offs;\r\nBUG_ON(ac->ac_g_ex.fe_logical > ac->ac_o_ex.fe_logical);\r\nBUG_ON(ac->ac_g_ex.fe_len < ac->ac_o_ex.fe_len);\r\nwinl = ac->ac_o_ex.fe_logical - ac->ac_g_ex.fe_logical;\r\nwins = EXT4_C2B(sbi, ac->ac_b_ex.fe_len - ac->ac_o_ex.fe_len);\r\nwin = min(winl, wins);\r\noffs = ac->ac_o_ex.fe_logical %\r\nEXT4_C2B(sbi, ac->ac_b_ex.fe_len);\r\nif (offs && offs < win)\r\nwin = offs;\r\nac->ac_b_ex.fe_logical = ac->ac_o_ex.fe_logical -\r\nEXT4_NUM_B2C(sbi, win);\r\nBUG_ON(ac->ac_o_ex.fe_logical < ac->ac_b_ex.fe_logical);\r\nBUG_ON(ac->ac_o_ex.fe_len > ac->ac_b_ex.fe_len);\r\n}\r\nac->ac_f_ex = ac->ac_b_ex;\r\npa->pa_lstart = ac->ac_b_ex.fe_logical;\r\npa->pa_pstart = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\r\npa->pa_len = ac->ac_b_ex.fe_len;\r\npa->pa_free = pa->pa_len;\r\natomic_set(&pa->pa_count, 1);\r\nspin_lock_init(&pa->pa_lock);\r\nINIT_LIST_HEAD(&pa->pa_inode_list);\r\nINIT_LIST_HEAD(&pa->pa_group_list);\r\npa->pa_deleted = 0;\r\npa->pa_type = MB_INODE_PA;\r\nmb_debug(1, "new inode pa %p: %llu/%u for %u\n", pa,\r\npa->pa_pstart, pa->pa_len, pa->pa_lstart);\r\ntrace_ext4_mb_new_inode_pa(ac, pa);\r\next4_mb_use_inode_pa(ac, pa);\r\natomic_add(pa->pa_free, &sbi->s_mb_preallocated);\r\nei = EXT4_I(ac->ac_inode);\r\ngrp = ext4_get_group_info(sb, ac->ac_b_ex.fe_group);\r\npa->pa_obj_lock = &ei->i_prealloc_lock;\r\npa->pa_inode = ac->ac_inode;\r\next4_lock_group(sb, ac->ac_b_ex.fe_group);\r\nlist_add(&pa->pa_group_list, &grp->bb_prealloc_list);\r\next4_unlock_group(sb, ac->ac_b_ex.fe_group);\r\nspin_lock(pa->pa_obj_lock);\r\nlist_add_rcu(&pa->pa_inode_list, &ei->i_prealloc_list);\r\nspin_unlock(pa->pa_obj_lock);\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_new_group_pa(struct ext4_allocation_context *ac)\r\n{\r\nstruct super_block *sb = ac->ac_sb;\r\nstruct ext4_locality_group *lg;\r\nstruct ext4_prealloc_space *pa;\r\nstruct ext4_group_info *grp;\r\nBUG_ON(ac->ac_o_ex.fe_len >= ac->ac_b_ex.fe_len);\r\nBUG_ON(ac->ac_status != AC_STATUS_FOUND);\r\nBUG_ON(!S_ISREG(ac->ac_inode->i_mode));\r\nBUG_ON(ext4_pspace_cachep == NULL);\r\npa = kmem_cache_alloc(ext4_pspace_cachep, GFP_NOFS);\r\nif (pa == NULL)\r\nreturn -ENOMEM;\r\nac->ac_f_ex = ac->ac_b_ex;\r\npa->pa_pstart = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\r\npa->pa_lstart = pa->pa_pstart;\r\npa->pa_len = ac->ac_b_ex.fe_len;\r\npa->pa_free = pa->pa_len;\r\natomic_set(&pa->pa_count, 1);\r\nspin_lock_init(&pa->pa_lock);\r\nINIT_LIST_HEAD(&pa->pa_inode_list);\r\nINIT_LIST_HEAD(&pa->pa_group_list);\r\npa->pa_deleted = 0;\r\npa->pa_type = MB_GROUP_PA;\r\nmb_debug(1, "new group pa %p: %llu/%u for %u\n", pa,\r\npa->pa_pstart, pa->pa_len, pa->pa_lstart);\r\ntrace_ext4_mb_new_group_pa(ac, pa);\r\next4_mb_use_group_pa(ac, pa);\r\natomic_add(pa->pa_free, &EXT4_SB(sb)->s_mb_preallocated);\r\ngrp = ext4_get_group_info(sb, ac->ac_b_ex.fe_group);\r\nlg = ac->ac_lg;\r\nBUG_ON(lg == NULL);\r\npa->pa_obj_lock = &lg->lg_prealloc_lock;\r\npa->pa_inode = NULL;\r\next4_lock_group(sb, ac->ac_b_ex.fe_group);\r\nlist_add(&pa->pa_group_list, &grp->bb_prealloc_list);\r\next4_unlock_group(sb, ac->ac_b_ex.fe_group);\r\nreturn 0;\r\n}\r\nstatic int ext4_mb_new_preallocation(struct ext4_allocation_context *ac)\r\n{\r\nint err;\r\nif (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC)\r\nerr = ext4_mb_new_group_pa(ac);\r\nelse\r\nerr = ext4_mb_new_inode_pa(ac);\r\nreturn err;\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_release_inode_pa(struct ext4_buddy *e4b, struct buffer_head *bitmap_bh,\r\nstruct ext4_prealloc_space *pa)\r\n{\r\nstruct super_block *sb = e4b->bd_sb;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nunsigned int end;\r\nunsigned int next;\r\next4_group_t group;\r\next4_grpblk_t bit;\r\nunsigned long long grp_blk_start;\r\nint err = 0;\r\nint free = 0;\r\nBUG_ON(pa->pa_deleted == 0);\r\next4_get_group_no_and_offset(sb, pa->pa_pstart, &group, &bit);\r\ngrp_blk_start = pa->pa_pstart - EXT4_C2B(sbi, bit);\r\nBUG_ON(group != e4b->bd_group && pa->pa_len != 0);\r\nend = bit + pa->pa_len;\r\nwhile (bit < end) {\r\nbit = mb_find_next_zero_bit(bitmap_bh->b_data, end, bit);\r\nif (bit >= end)\r\nbreak;\r\nnext = mb_find_next_bit(bitmap_bh->b_data, end, bit);\r\nmb_debug(1, " free preallocated %u/%u in group %u\n",\r\n(unsigned) ext4_group_first_block_no(sb, group) + bit,\r\n(unsigned) next - bit, (unsigned) group);\r\nfree += next - bit;\r\ntrace_ext4_mballoc_discard(sb, NULL, group, bit, next - bit);\r\ntrace_ext4_mb_release_inode_pa(pa, (grp_blk_start +\r\nEXT4_C2B(sbi, bit)),\r\nnext - bit);\r\nmb_free_blocks(pa->pa_inode, e4b, bit, next - bit);\r\nbit = next + 1;\r\n}\r\nif (free != pa->pa_free) {\r\next4_msg(e4b->bd_sb, KERN_CRIT,\r\n"pa %p: logic %lu, phys. %lu, len %lu",\r\npa, (unsigned long) pa->pa_lstart,\r\n(unsigned long) pa->pa_pstart,\r\n(unsigned long) pa->pa_len);\r\next4_grp_locked_error(sb, group, 0, 0, "free %u, pa_free %u",\r\nfree, pa->pa_free);\r\n}\r\natomic_add(free, &sbi->s_mb_discarded);\r\nreturn err;\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_release_group_pa(struct ext4_buddy *e4b,\r\nstruct ext4_prealloc_space *pa)\r\n{\r\nstruct super_block *sb = e4b->bd_sb;\r\next4_group_t group;\r\next4_grpblk_t bit;\r\ntrace_ext4_mb_release_group_pa(sb, pa);\r\nBUG_ON(pa->pa_deleted == 0);\r\next4_get_group_no_and_offset(sb, pa->pa_pstart, &group, &bit);\r\nBUG_ON(group != e4b->bd_group && pa->pa_len != 0);\r\nmb_free_blocks(pa->pa_inode, e4b, bit, pa->pa_len);\r\natomic_add(pa->pa_len, &EXT4_SB(sb)->s_mb_discarded);\r\ntrace_ext4_mballoc_discard(sb, NULL, group, bit, pa->pa_len);\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_discard_group_preallocations(struct super_block *sb,\r\next4_group_t group, int needed)\r\n{\r\nstruct ext4_group_info *grp = ext4_get_group_info(sb, group);\r\nstruct buffer_head *bitmap_bh = NULL;\r\nstruct ext4_prealloc_space *pa, *tmp;\r\nstruct list_head list;\r\nstruct ext4_buddy e4b;\r\nint err;\r\nint busy = 0;\r\nint free = 0;\r\nmb_debug(1, "discard preallocation for group %u\n", group);\r\nif (list_empty(&grp->bb_prealloc_list))\r\nreturn 0;\r\nbitmap_bh = ext4_read_block_bitmap(sb, group);\r\nif (bitmap_bh == NULL) {\r\next4_error(sb, "Error reading block bitmap for %u", group);\r\nreturn 0;\r\n}\r\nerr = ext4_mb_load_buddy(sb, group, &e4b);\r\nif (err) {\r\next4_error(sb, "Error loading buddy information for %u", group);\r\nput_bh(bitmap_bh);\r\nreturn 0;\r\n}\r\nif (needed == 0)\r\nneeded = EXT4_CLUSTERS_PER_GROUP(sb) + 1;\r\nINIT_LIST_HEAD(&list);\r\nrepeat:\r\next4_lock_group(sb, group);\r\nlist_for_each_entry_safe(pa, tmp,\r\n&grp->bb_prealloc_list, pa_group_list) {\r\nspin_lock(&pa->pa_lock);\r\nif (atomic_read(&pa->pa_count)) {\r\nspin_unlock(&pa->pa_lock);\r\nbusy = 1;\r\ncontinue;\r\n}\r\nif (pa->pa_deleted) {\r\nspin_unlock(&pa->pa_lock);\r\ncontinue;\r\n}\r\npa->pa_deleted = 1;\r\nfree += pa->pa_free;\r\nspin_unlock(&pa->pa_lock);\r\nlist_del(&pa->pa_group_list);\r\nlist_add(&pa->u.pa_tmp_list, &list);\r\n}\r\nif (free < needed && busy) {\r\nbusy = 0;\r\next4_unlock_group(sb, group);\r\ncond_resched();\r\ngoto repeat;\r\n}\r\nif (list_empty(&list)) {\r\nBUG_ON(free != 0);\r\ngoto out;\r\n}\r\nlist_for_each_entry_safe(pa, tmp, &list, u.pa_tmp_list) {\r\nspin_lock(pa->pa_obj_lock);\r\nlist_del_rcu(&pa->pa_inode_list);\r\nspin_unlock(pa->pa_obj_lock);\r\nif (pa->pa_type == MB_GROUP_PA)\r\next4_mb_release_group_pa(&e4b, pa);\r\nelse\r\next4_mb_release_inode_pa(&e4b, bitmap_bh, pa);\r\nlist_del(&pa->u.pa_tmp_list);\r\ncall_rcu(&(pa)->u.pa_rcu, ext4_mb_pa_callback);\r\n}\r\nout:\r\next4_unlock_group(sb, group);\r\next4_mb_unload_buddy(&e4b);\r\nput_bh(bitmap_bh);\r\nreturn free;\r\n}\r\nvoid ext4_discard_preallocations(struct inode *inode)\r\n{\r\nstruct ext4_inode_info *ei = EXT4_I(inode);\r\nstruct super_block *sb = inode->i_sb;\r\nstruct buffer_head *bitmap_bh = NULL;\r\nstruct ext4_prealloc_space *pa, *tmp;\r\next4_group_t group = 0;\r\nstruct list_head list;\r\nstruct ext4_buddy e4b;\r\nint err;\r\nif (!S_ISREG(inode->i_mode)) {\r\nreturn;\r\n}\r\nmb_debug(1, "discard preallocation for inode %lu\n", inode->i_ino);\r\ntrace_ext4_discard_preallocations(inode);\r\nINIT_LIST_HEAD(&list);\r\nrepeat:\r\nspin_lock(&ei->i_prealloc_lock);\r\nwhile (!list_empty(&ei->i_prealloc_list)) {\r\npa = list_entry(ei->i_prealloc_list.next,\r\nstruct ext4_prealloc_space, pa_inode_list);\r\nBUG_ON(pa->pa_obj_lock != &ei->i_prealloc_lock);\r\nspin_lock(&pa->pa_lock);\r\nif (atomic_read(&pa->pa_count)) {\r\nspin_unlock(&pa->pa_lock);\r\nspin_unlock(&ei->i_prealloc_lock);\r\next4_msg(sb, KERN_ERR,\r\n"uh-oh! used pa while discarding");\r\nWARN_ON(1);\r\nschedule_timeout_uninterruptible(HZ);\r\ngoto repeat;\r\n}\r\nif (pa->pa_deleted == 0) {\r\npa->pa_deleted = 1;\r\nspin_unlock(&pa->pa_lock);\r\nlist_del_rcu(&pa->pa_inode_list);\r\nlist_add(&pa->u.pa_tmp_list, &list);\r\ncontinue;\r\n}\r\nspin_unlock(&pa->pa_lock);\r\nspin_unlock(&ei->i_prealloc_lock);\r\nschedule_timeout_uninterruptible(HZ);\r\ngoto repeat;\r\n}\r\nspin_unlock(&ei->i_prealloc_lock);\r\nlist_for_each_entry_safe(pa, tmp, &list, u.pa_tmp_list) {\r\nBUG_ON(pa->pa_type != MB_INODE_PA);\r\ngroup = ext4_get_group_number(sb, pa->pa_pstart);\r\nerr = ext4_mb_load_buddy(sb, group, &e4b);\r\nif (err) {\r\next4_error(sb, "Error loading buddy information for %u",\r\ngroup);\r\ncontinue;\r\n}\r\nbitmap_bh = ext4_read_block_bitmap(sb, group);\r\nif (bitmap_bh == NULL) {\r\next4_error(sb, "Error reading block bitmap for %u",\r\ngroup);\r\next4_mb_unload_buddy(&e4b);\r\ncontinue;\r\n}\r\next4_lock_group(sb, group);\r\nlist_del(&pa->pa_group_list);\r\next4_mb_release_inode_pa(&e4b, bitmap_bh, pa);\r\next4_unlock_group(sb, group);\r\next4_mb_unload_buddy(&e4b);\r\nput_bh(bitmap_bh);\r\nlist_del(&pa->u.pa_tmp_list);\r\ncall_rcu(&(pa)->u.pa_rcu, ext4_mb_pa_callback);\r\n}\r\n}\r\nstatic void ext4_mb_show_ac(struct ext4_allocation_context *ac)\r\n{\r\nstruct super_block *sb = ac->ac_sb;\r\next4_group_t ngroups, i;\r\nif (!ext4_mballoc_debug ||\r\n(EXT4_SB(sb)->s_mount_flags & EXT4_MF_FS_ABORTED))\r\nreturn;\r\next4_msg(ac->ac_sb, KERN_ERR, "Can't allocate:"\r\n" Allocation context details:");\r\next4_msg(ac->ac_sb, KERN_ERR, "status %d flags %d",\r\nac->ac_status, ac->ac_flags);\r\next4_msg(ac->ac_sb, KERN_ERR, "orig %lu/%lu/%lu@%lu, "\r\n"goal %lu/%lu/%lu@%lu, "\r\n"best %lu/%lu/%lu@%lu cr %d",\r\n(unsigned long)ac->ac_o_ex.fe_group,\r\n(unsigned long)ac->ac_o_ex.fe_start,\r\n(unsigned long)ac->ac_o_ex.fe_len,\r\n(unsigned long)ac->ac_o_ex.fe_logical,\r\n(unsigned long)ac->ac_g_ex.fe_group,\r\n(unsigned long)ac->ac_g_ex.fe_start,\r\n(unsigned long)ac->ac_g_ex.fe_len,\r\n(unsigned long)ac->ac_g_ex.fe_logical,\r\n(unsigned long)ac->ac_b_ex.fe_group,\r\n(unsigned long)ac->ac_b_ex.fe_start,\r\n(unsigned long)ac->ac_b_ex.fe_len,\r\n(unsigned long)ac->ac_b_ex.fe_logical,\r\n(int)ac->ac_criteria);\r\next4_msg(ac->ac_sb, KERN_ERR, "%d found", ac->ac_found);\r\next4_msg(ac->ac_sb, KERN_ERR, "groups: ");\r\nngroups = ext4_get_groups_count(sb);\r\nfor (i = 0; i < ngroups; i++) {\r\nstruct ext4_group_info *grp = ext4_get_group_info(sb, i);\r\nstruct ext4_prealloc_space *pa;\r\next4_grpblk_t start;\r\nstruct list_head *cur;\r\next4_lock_group(sb, i);\r\nlist_for_each(cur, &grp->bb_prealloc_list) {\r\npa = list_entry(cur, struct ext4_prealloc_space,\r\npa_group_list);\r\nspin_lock(&pa->pa_lock);\r\next4_get_group_no_and_offset(sb, pa->pa_pstart,\r\nNULL, &start);\r\nspin_unlock(&pa->pa_lock);\r\nprintk(KERN_ERR "PA:%u:%d:%u \n", i,\r\nstart, pa->pa_len);\r\n}\r\next4_unlock_group(sb, i);\r\nif (grp->bb_free == 0)\r\ncontinue;\r\nprintk(KERN_ERR "%u: %d/%d \n",\r\ni, grp->bb_free, grp->bb_fragments);\r\n}\r\nprintk(KERN_ERR "\n");\r\n}\r\nstatic inline void ext4_mb_show_ac(struct ext4_allocation_context *ac)\r\n{\r\nreturn;\r\n}\r\nstatic void ext4_mb_group_or_file(struct ext4_allocation_context *ac)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\r\nint bsbits = ac->ac_sb->s_blocksize_bits;\r\nloff_t size, isize;\r\nif (!(ac->ac_flags & EXT4_MB_HINT_DATA))\r\nreturn;\r\nif (unlikely(ac->ac_flags & EXT4_MB_HINT_GOAL_ONLY))\r\nreturn;\r\nsize = ac->ac_o_ex.fe_logical + EXT4_C2B(sbi, ac->ac_o_ex.fe_len);\r\nisize = (i_size_read(ac->ac_inode) + ac->ac_sb->s_blocksize - 1)\r\n>> bsbits;\r\nif ((size == isize) &&\r\n!ext4_fs_is_busy(sbi) &&\r\n(atomic_read(&ac->ac_inode->i_writecount) == 0)) {\r\nac->ac_flags |= EXT4_MB_HINT_NOPREALLOC;\r\nreturn;\r\n}\r\nif (sbi->s_mb_group_prealloc <= 0) {\r\nac->ac_flags |= EXT4_MB_STREAM_ALLOC;\r\nreturn;\r\n}\r\nsize = max(size, isize);\r\nif (size > sbi->s_mb_stream_request) {\r\nac->ac_flags |= EXT4_MB_STREAM_ALLOC;\r\nreturn;\r\n}\r\nBUG_ON(ac->ac_lg != NULL);\r\nac->ac_lg = raw_cpu_ptr(sbi->s_locality_groups);\r\nac->ac_flags |= EXT4_MB_HINT_GROUP_ALLOC;\r\nmutex_lock(&ac->ac_lg->lg_mutex);\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_initialize_context(struct ext4_allocation_context *ac,\r\nstruct ext4_allocation_request *ar)\r\n{\r\nstruct super_block *sb = ar->inode->i_sb;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nstruct ext4_super_block *es = sbi->s_es;\r\next4_group_t group;\r\nunsigned int len;\r\next4_fsblk_t goal;\r\next4_grpblk_t block;\r\nlen = ar->len;\r\nif (len >= EXT4_CLUSTERS_PER_GROUP(sb))\r\nlen = EXT4_CLUSTERS_PER_GROUP(sb);\r\ngoal = ar->goal;\r\nif (goal < le32_to_cpu(es->s_first_data_block) ||\r\ngoal >= ext4_blocks_count(es))\r\ngoal = le32_to_cpu(es->s_first_data_block);\r\next4_get_group_no_and_offset(sb, goal, &group, &block);\r\nac->ac_b_ex.fe_logical = EXT4_LBLK_CMASK(sbi, ar->logical);\r\nac->ac_status = AC_STATUS_CONTINUE;\r\nac->ac_sb = sb;\r\nac->ac_inode = ar->inode;\r\nac->ac_o_ex.fe_logical = ac->ac_b_ex.fe_logical;\r\nac->ac_o_ex.fe_group = group;\r\nac->ac_o_ex.fe_start = block;\r\nac->ac_o_ex.fe_len = len;\r\nac->ac_g_ex = ac->ac_o_ex;\r\nac->ac_flags = ar->flags;\r\next4_mb_group_or_file(ac);\r\nmb_debug(1, "init ac: %u blocks @ %u, goal %u, flags %x, 2^%d, "\r\n"left: %u/%u, right %u/%u to %swritable\n",\r\n(unsigned) ar->len, (unsigned) ar->logical,\r\n(unsigned) ar->goal, ac->ac_flags, ac->ac_2order,\r\n(unsigned) ar->lleft, (unsigned) ar->pleft,\r\n(unsigned) ar->lright, (unsigned) ar->pright,\r\natomic_read(&ar->inode->i_writecount) ? "" : "non-");\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack void\r\next4_mb_discard_lg_preallocations(struct super_block *sb,\r\nstruct ext4_locality_group *lg,\r\nint order, int total_entries)\r\n{\r\next4_group_t group = 0;\r\nstruct ext4_buddy e4b;\r\nstruct list_head discard_list;\r\nstruct ext4_prealloc_space *pa, *tmp;\r\nmb_debug(1, "discard locality group preallocation\n");\r\nINIT_LIST_HEAD(&discard_list);\r\nspin_lock(&lg->lg_prealloc_lock);\r\nlist_for_each_entry_rcu(pa, &lg->lg_prealloc_list[order],\r\npa_inode_list) {\r\nspin_lock(&pa->pa_lock);\r\nif (atomic_read(&pa->pa_count)) {\r\nspin_unlock(&pa->pa_lock);\r\ncontinue;\r\n}\r\nif (pa->pa_deleted) {\r\nspin_unlock(&pa->pa_lock);\r\ncontinue;\r\n}\r\nBUG_ON(pa->pa_type != MB_GROUP_PA);\r\npa->pa_deleted = 1;\r\nspin_unlock(&pa->pa_lock);\r\nlist_del_rcu(&pa->pa_inode_list);\r\nlist_add(&pa->u.pa_tmp_list, &discard_list);\r\ntotal_entries--;\r\nif (total_entries <= 5) {\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&lg->lg_prealloc_lock);\r\nlist_for_each_entry_safe(pa, tmp, &discard_list, u.pa_tmp_list) {\r\ngroup = ext4_get_group_number(sb, pa->pa_pstart);\r\nif (ext4_mb_load_buddy(sb, group, &e4b)) {\r\next4_error(sb, "Error loading buddy information for %u",\r\ngroup);\r\ncontinue;\r\n}\r\next4_lock_group(sb, group);\r\nlist_del(&pa->pa_group_list);\r\next4_mb_release_group_pa(&e4b, pa);\r\next4_unlock_group(sb, group);\r\next4_mb_unload_buddy(&e4b);\r\nlist_del(&pa->u.pa_tmp_list);\r\ncall_rcu(&(pa)->u.pa_rcu, ext4_mb_pa_callback);\r\n}\r\n}\r\nstatic void ext4_mb_add_n_trim(struct ext4_allocation_context *ac)\r\n{\r\nint order, added = 0, lg_prealloc_count = 1;\r\nstruct super_block *sb = ac->ac_sb;\r\nstruct ext4_locality_group *lg = ac->ac_lg;\r\nstruct ext4_prealloc_space *tmp_pa, *pa = ac->ac_pa;\r\norder = fls(pa->pa_free) - 1;\r\nif (order > PREALLOC_TB_SIZE - 1)\r\norder = PREALLOC_TB_SIZE - 1;\r\nspin_lock(&lg->lg_prealloc_lock);\r\nlist_for_each_entry_rcu(tmp_pa, &lg->lg_prealloc_list[order],\r\npa_inode_list) {\r\nspin_lock(&tmp_pa->pa_lock);\r\nif (tmp_pa->pa_deleted) {\r\nspin_unlock(&tmp_pa->pa_lock);\r\ncontinue;\r\n}\r\nif (!added && pa->pa_free < tmp_pa->pa_free) {\r\nlist_add_tail_rcu(&pa->pa_inode_list,\r\n&tmp_pa->pa_inode_list);\r\nadded = 1;\r\n}\r\nspin_unlock(&tmp_pa->pa_lock);\r\nlg_prealloc_count++;\r\n}\r\nif (!added)\r\nlist_add_tail_rcu(&pa->pa_inode_list,\r\n&lg->lg_prealloc_list[order]);\r\nspin_unlock(&lg->lg_prealloc_lock);\r\nif (lg_prealloc_count > 8) {\r\next4_mb_discard_lg_preallocations(sb, lg,\r\norder, lg_prealloc_count);\r\nreturn;\r\n}\r\nreturn ;\r\n}\r\nstatic int ext4_mb_release_context(struct ext4_allocation_context *ac)\r\n{\r\nstruct ext4_sb_info *sbi = EXT4_SB(ac->ac_sb);\r\nstruct ext4_prealloc_space *pa = ac->ac_pa;\r\nif (pa) {\r\nif (pa->pa_type == MB_GROUP_PA) {\r\nspin_lock(&pa->pa_lock);\r\npa->pa_pstart += EXT4_C2B(sbi, ac->ac_b_ex.fe_len);\r\npa->pa_lstart += EXT4_C2B(sbi, ac->ac_b_ex.fe_len);\r\npa->pa_free -= ac->ac_b_ex.fe_len;\r\npa->pa_len -= ac->ac_b_ex.fe_len;\r\nspin_unlock(&pa->pa_lock);\r\n}\r\n}\r\nif (pa) {\r\nif ((pa->pa_type == MB_GROUP_PA) && likely(pa->pa_free)) {\r\nspin_lock(pa->pa_obj_lock);\r\nlist_del_rcu(&pa->pa_inode_list);\r\nspin_unlock(pa->pa_obj_lock);\r\next4_mb_add_n_trim(ac);\r\n}\r\next4_mb_put_pa(ac, ac->ac_sb, pa);\r\n}\r\nif (ac->ac_bitmap_page)\r\npage_cache_release(ac->ac_bitmap_page);\r\nif (ac->ac_buddy_page)\r\npage_cache_release(ac->ac_buddy_page);\r\nif (ac->ac_flags & EXT4_MB_HINT_GROUP_ALLOC)\r\nmutex_unlock(&ac->ac_lg->lg_mutex);\r\next4_mb_collect_stats(ac);\r\nreturn 0;\r\n}\r\nstatic int ext4_mb_discard_preallocations(struct super_block *sb, int needed)\r\n{\r\next4_group_t i, ngroups = ext4_get_groups_count(sb);\r\nint ret;\r\nint freed = 0;\r\ntrace_ext4_mb_discard_preallocations(sb, needed);\r\nfor (i = 0; i < ngroups && needed > 0; i++) {\r\nret = ext4_mb_discard_group_preallocations(sb, i, needed);\r\nfreed += ret;\r\nneeded -= ret;\r\n}\r\nreturn freed;\r\n}\r\next4_fsblk_t ext4_mb_new_blocks(handle_t *handle,\r\nstruct ext4_allocation_request *ar, int *errp)\r\n{\r\nint freed;\r\nstruct ext4_allocation_context *ac = NULL;\r\nstruct ext4_sb_info *sbi;\r\nstruct super_block *sb;\r\next4_fsblk_t block = 0;\r\nunsigned int inquota = 0;\r\nunsigned int reserv_clstrs = 0;\r\nmight_sleep();\r\nsb = ar->inode->i_sb;\r\nsbi = EXT4_SB(sb);\r\ntrace_ext4_request_blocks(ar);\r\nif (IS_NOQUOTA(ar->inode))\r\nar->flags |= EXT4_MB_USE_ROOT_BLOCKS;\r\nif ((ar->flags & EXT4_MB_DELALLOC_RESERVED) == 0) {\r\nwhile (ar->len &&\r\next4_claim_free_clusters(sbi, ar->len, ar->flags)) {\r\ncond_resched();\r\nar->len = ar->len >> 1;\r\n}\r\nif (!ar->len) {\r\n*errp = -ENOSPC;\r\nreturn 0;\r\n}\r\nreserv_clstrs = ar->len;\r\nif (ar->flags & EXT4_MB_USE_ROOT_BLOCKS) {\r\ndquot_alloc_block_nofail(ar->inode,\r\nEXT4_C2B(sbi, ar->len));\r\n} else {\r\nwhile (ar->len &&\r\ndquot_alloc_block(ar->inode,\r\nEXT4_C2B(sbi, ar->len))) {\r\nar->flags |= EXT4_MB_HINT_NOPREALLOC;\r\nar->len--;\r\n}\r\n}\r\ninquota = ar->len;\r\nif (ar->len == 0) {\r\n*errp = -EDQUOT;\r\ngoto out;\r\n}\r\n}\r\nac = kmem_cache_zalloc(ext4_ac_cachep, GFP_NOFS);\r\nif (!ac) {\r\nar->len = 0;\r\n*errp = -ENOMEM;\r\ngoto out;\r\n}\r\n*errp = ext4_mb_initialize_context(ac, ar);\r\nif (*errp) {\r\nar->len = 0;\r\ngoto out;\r\n}\r\nac->ac_op = EXT4_MB_HISTORY_PREALLOC;\r\nif (!ext4_mb_use_preallocated(ac)) {\r\nac->ac_op = EXT4_MB_HISTORY_ALLOC;\r\next4_mb_normalize_request(ac, ar);\r\nrepeat:\r\n*errp = ext4_mb_regular_allocator(ac);\r\nif (*errp)\r\ngoto discard_and_exit;\r\nif (ac->ac_status == AC_STATUS_FOUND &&\r\nac->ac_o_ex.fe_len < ac->ac_b_ex.fe_len)\r\n*errp = ext4_mb_new_preallocation(ac);\r\nif (*errp) {\r\ndiscard_and_exit:\r\next4_discard_allocated_blocks(ac);\r\ngoto errout;\r\n}\r\n}\r\nif (likely(ac->ac_status == AC_STATUS_FOUND)) {\r\n*errp = ext4_mb_mark_diskspace_used(ac, handle, reserv_clstrs);\r\nif (*errp == -EAGAIN) {\r\next4_mb_release_context(ac);\r\nac->ac_b_ex.fe_group = 0;\r\nac->ac_b_ex.fe_start = 0;\r\nac->ac_b_ex.fe_len = 0;\r\nac->ac_status = AC_STATUS_CONTINUE;\r\ngoto repeat;\r\n} else if (*errp) {\r\next4_discard_allocated_blocks(ac);\r\ngoto errout;\r\n} else {\r\nblock = ext4_grp_offs_to_block(sb, &ac->ac_b_ex);\r\nar->len = ac->ac_b_ex.fe_len;\r\n}\r\n} else {\r\nfreed = ext4_mb_discard_preallocations(sb, ac->ac_o_ex.fe_len);\r\nif (freed)\r\ngoto repeat;\r\n*errp = -ENOSPC;\r\n}\r\nerrout:\r\nif (*errp) {\r\nac->ac_b_ex.fe_len = 0;\r\nar->len = 0;\r\next4_mb_show_ac(ac);\r\n}\r\next4_mb_release_context(ac);\r\nout:\r\nif (ac)\r\nkmem_cache_free(ext4_ac_cachep, ac);\r\nif (inquota && ar->len < inquota)\r\ndquot_free_block(ar->inode, EXT4_C2B(sbi, inquota - ar->len));\r\nif (!ar->len) {\r\nif ((ar->flags & EXT4_MB_DELALLOC_RESERVED) == 0)\r\npercpu_counter_sub(&sbi->s_dirtyclusters_counter,\r\nreserv_clstrs);\r\n}\r\ntrace_ext4_allocate_blocks(ar, (unsigned long long)block);\r\nreturn block;\r\n}\r\nstatic int can_merge(struct ext4_free_data *entry1,\r\nstruct ext4_free_data *entry2)\r\n{\r\nif ((entry1->efd_tid == entry2->efd_tid) &&\r\n(entry1->efd_group == entry2->efd_group) &&\r\n((entry1->efd_start_cluster + entry1->efd_count) == entry2->efd_start_cluster))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic noinline_for_stack int\r\next4_mb_free_metadata(handle_t *handle, struct ext4_buddy *e4b,\r\nstruct ext4_free_data *new_entry)\r\n{\r\next4_group_t group = e4b->bd_group;\r\next4_grpblk_t cluster;\r\nstruct ext4_free_data *entry;\r\nstruct ext4_group_info *db = e4b->bd_info;\r\nstruct super_block *sb = e4b->bd_sb;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nstruct rb_node **n = &db->bb_free_root.rb_node, *node;\r\nstruct rb_node *parent = NULL, *new_node;\r\nBUG_ON(!ext4_handle_valid(handle));\r\nBUG_ON(e4b->bd_bitmap_page == NULL);\r\nBUG_ON(e4b->bd_buddy_page == NULL);\r\nnew_node = &new_entry->efd_node;\r\ncluster = new_entry->efd_start_cluster;\r\nif (!*n) {\r\npage_cache_get(e4b->bd_buddy_page);\r\npage_cache_get(e4b->bd_bitmap_page);\r\n}\r\nwhile (*n) {\r\nparent = *n;\r\nentry = rb_entry(parent, struct ext4_free_data, efd_node);\r\nif (cluster < entry->efd_start_cluster)\r\nn = &(*n)->rb_left;\r\nelse if (cluster >= (entry->efd_start_cluster + entry->efd_count))\r\nn = &(*n)->rb_right;\r\nelse {\r\next4_grp_locked_error(sb, group, 0,\r\next4_group_first_block_no(sb, group) +\r\nEXT4_C2B(sbi, cluster),\r\n"Block already on to-be-freed list");\r\nreturn 0;\r\n}\r\n}\r\nrb_link_node(new_node, parent, n);\r\nrb_insert_color(new_node, &db->bb_free_root);\r\nnode = rb_prev(new_node);\r\nif (node) {\r\nentry = rb_entry(node, struct ext4_free_data, efd_node);\r\nif (can_merge(entry, new_entry) &&\r\next4_journal_callback_try_del(handle, &entry->efd_jce)) {\r\nnew_entry->efd_start_cluster = entry->efd_start_cluster;\r\nnew_entry->efd_count += entry->efd_count;\r\nrb_erase(node, &(db->bb_free_root));\r\nkmem_cache_free(ext4_free_data_cachep, entry);\r\n}\r\n}\r\nnode = rb_next(new_node);\r\nif (node) {\r\nentry = rb_entry(node, struct ext4_free_data, efd_node);\r\nif (can_merge(new_entry, entry) &&\r\next4_journal_callback_try_del(handle, &entry->efd_jce)) {\r\nnew_entry->efd_count += entry->efd_count;\r\nrb_erase(node, &(db->bb_free_root));\r\nkmem_cache_free(ext4_free_data_cachep, entry);\r\n}\r\n}\r\next4_journal_callback_add(handle, ext4_free_data_callback,\r\n&new_entry->efd_jce);\r\nreturn 0;\r\n}\r\nvoid ext4_free_blocks(handle_t *handle, struct inode *inode,\r\nstruct buffer_head *bh, ext4_fsblk_t block,\r\nunsigned long count, int flags)\r\n{\r\nstruct buffer_head *bitmap_bh = NULL;\r\nstruct super_block *sb = inode->i_sb;\r\nstruct ext4_group_desc *gdp;\r\nunsigned int overflow;\r\next4_grpblk_t bit;\r\nstruct buffer_head *gd_bh;\r\next4_group_t block_group;\r\nstruct ext4_sb_info *sbi;\r\nstruct ext4_buddy e4b;\r\nunsigned int count_clusters;\r\nint err = 0;\r\nint ret;\r\nmight_sleep();\r\nif (bh) {\r\nif (block)\r\nBUG_ON(block != bh->b_blocknr);\r\nelse\r\nblock = bh->b_blocknr;\r\n}\r\nsbi = EXT4_SB(sb);\r\nif (!(flags & EXT4_FREE_BLOCKS_VALIDATED) &&\r\n!ext4_data_block_valid(sbi, block, count)) {\r\next4_error(sb, "Freeing blocks not in datazone - "\r\n"block = %llu, count = %lu", block, count);\r\ngoto error_return;\r\n}\r\next4_debug("freeing block %llu\n", block);\r\ntrace_ext4_free_blocks(inode, block, count, flags);\r\nif (flags & EXT4_FREE_BLOCKS_FORGET) {\r\nstruct buffer_head *tbh = bh;\r\nint i;\r\nBUG_ON(bh && (count > 1));\r\nfor (i = 0; i < count; i++) {\r\ncond_resched();\r\nif (!bh)\r\ntbh = sb_find_get_block(inode->i_sb,\r\nblock + i);\r\nif (!tbh)\r\ncontinue;\r\next4_forget(handle, flags & EXT4_FREE_BLOCKS_METADATA,\r\ninode, tbh, block + i);\r\n}\r\n}\r\nif (!ext4_should_writeback_data(inode))\r\nflags |= EXT4_FREE_BLOCKS_METADATA;\r\noverflow = EXT4_PBLK_COFF(sbi, block);\r\nif (overflow) {\r\nif (flags & EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER) {\r\noverflow = sbi->s_cluster_ratio - overflow;\r\nblock += overflow;\r\nif (count > overflow)\r\ncount -= overflow;\r\nelse\r\nreturn;\r\n} else {\r\nblock -= overflow;\r\ncount += overflow;\r\n}\r\n}\r\noverflow = EXT4_LBLK_COFF(sbi, count);\r\nif (overflow) {\r\nif (flags & EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER) {\r\nif (count > overflow)\r\ncount -= overflow;\r\nelse\r\nreturn;\r\n} else\r\ncount += sbi->s_cluster_ratio - overflow;\r\n}\r\ndo_more:\r\noverflow = 0;\r\next4_get_group_no_and_offset(sb, block, &block_group, &bit);\r\nif (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(\r\next4_get_group_info(sb, block_group))))\r\nreturn;\r\nif (EXT4_C2B(sbi, bit) + count > EXT4_BLOCKS_PER_GROUP(sb)) {\r\noverflow = EXT4_C2B(sbi, bit) + count -\r\nEXT4_BLOCKS_PER_GROUP(sb);\r\ncount -= overflow;\r\n}\r\ncount_clusters = EXT4_NUM_B2C(sbi, count);\r\nbitmap_bh = ext4_read_block_bitmap(sb, block_group);\r\nif (!bitmap_bh) {\r\nerr = -EIO;\r\ngoto error_return;\r\n}\r\ngdp = ext4_get_group_desc(sb, block_group, &gd_bh);\r\nif (!gdp) {\r\nerr = -EIO;\r\ngoto error_return;\r\n}\r\nif (in_range(ext4_block_bitmap(sb, gdp), block, count) ||\r\nin_range(ext4_inode_bitmap(sb, gdp), block, count) ||\r\nin_range(block, ext4_inode_table(sb, gdp),\r\nEXT4_SB(sb)->s_itb_per_group) ||\r\nin_range(block + count - 1, ext4_inode_table(sb, gdp),\r\nEXT4_SB(sb)->s_itb_per_group)) {\r\next4_error(sb, "Freeing blocks in system zone - "\r\n"Block = %llu, count = %lu", block, count);\r\ngoto error_return;\r\n}\r\nBUFFER_TRACE(bitmap_bh, "getting write access");\r\nerr = ext4_journal_get_write_access(handle, bitmap_bh);\r\nif (err)\r\ngoto error_return;\r\nBUFFER_TRACE(gd_bh, "get_write_access");\r\nerr = ext4_journal_get_write_access(handle, gd_bh);\r\nif (err)\r\ngoto error_return;\r\n#ifdef AGGRESSIVE_CHECK\r\n{\r\nint i;\r\nfor (i = 0; i < count_clusters; i++)\r\nBUG_ON(!mb_test_bit(bit + i, bitmap_bh->b_data));\r\n}\r\n#endif\r\ntrace_ext4_mballoc_free(sb, inode, block_group, bit, count_clusters);\r\nerr = ext4_mb_load_buddy(sb, block_group, &e4b);\r\nif (err)\r\ngoto error_return;\r\nif ((flags & EXT4_FREE_BLOCKS_METADATA) && ext4_handle_valid(handle)) {\r\nstruct ext4_free_data *new_entry;\r\nnew_entry = kmem_cache_alloc(ext4_free_data_cachep,\r\nGFP_NOFS|__GFP_NOFAIL);\r\nnew_entry->efd_start_cluster = bit;\r\nnew_entry->efd_group = block_group;\r\nnew_entry->efd_count = count_clusters;\r\nnew_entry->efd_tid = handle->h_transaction->t_tid;\r\next4_lock_group(sb, block_group);\r\nmb_clear_bits(bitmap_bh->b_data, bit, count_clusters);\r\next4_mb_free_metadata(handle, &e4b, new_entry);\r\n} else {\r\nif (test_opt(sb, DISCARD)) {\r\nerr = ext4_issue_discard(sb, block_group, bit, count);\r\nif (err && err != -EOPNOTSUPP)\r\next4_msg(sb, KERN_WARNING, "discard request in"\r\n" group:%d block:%d count:%lu failed"\r\n" with %d", block_group, bit, count,\r\nerr);\r\n} else\r\nEXT4_MB_GRP_CLEAR_TRIMMED(e4b.bd_info);\r\next4_lock_group(sb, block_group);\r\nmb_clear_bits(bitmap_bh->b_data, bit, count_clusters);\r\nmb_free_blocks(inode, &e4b, bit, count_clusters);\r\n}\r\nret = ext4_free_group_clusters(sb, gdp) + count_clusters;\r\next4_free_group_clusters_set(sb, gdp, ret);\r\next4_block_bitmap_csum_set(sb, block_group, gdp, bitmap_bh);\r\next4_group_desc_csum_set(sb, block_group, gdp);\r\next4_unlock_group(sb, block_group);\r\nif (sbi->s_log_groups_per_flex) {\r\next4_group_t flex_group = ext4_flex_group(sbi, block_group);\r\natomic64_add(count_clusters,\r\n&sbi->s_flex_groups[flex_group].free_clusters);\r\n}\r\nif (!(flags & EXT4_FREE_BLOCKS_NO_QUOT_UPDATE))\r\ndquot_free_block(inode, EXT4_C2B(sbi, count_clusters));\r\npercpu_counter_add(&sbi->s_freeclusters_counter, count_clusters);\r\next4_mb_unload_buddy(&e4b);\r\nBUFFER_TRACE(bitmap_bh, "dirtied bitmap block");\r\nerr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\r\nBUFFER_TRACE(gd_bh, "dirtied group descriptor block");\r\nret = ext4_handle_dirty_metadata(handle, NULL, gd_bh);\r\nif (!err)\r\nerr = ret;\r\nif (overflow && !err) {\r\nblock += count;\r\ncount = overflow;\r\nput_bh(bitmap_bh);\r\ngoto do_more;\r\n}\r\nerror_return:\r\nbrelse(bitmap_bh);\r\next4_std_error(sb, err);\r\nreturn;\r\n}\r\nint ext4_group_add_blocks(handle_t *handle, struct super_block *sb,\r\next4_fsblk_t block, unsigned long count)\r\n{\r\nstruct buffer_head *bitmap_bh = NULL;\r\nstruct buffer_head *gd_bh;\r\next4_group_t block_group;\r\next4_grpblk_t bit;\r\nunsigned int i;\r\nstruct ext4_group_desc *desc;\r\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\r\nstruct ext4_buddy e4b;\r\nint err = 0, ret, blk_free_count;\r\next4_grpblk_t blocks_freed;\r\next4_debug("Adding block(s) %llu-%llu\n", block, block + count - 1);\r\nif (count == 0)\r\nreturn 0;\r\next4_get_group_no_and_offset(sb, block, &block_group, &bit);\r\nif (bit + count > EXT4_BLOCKS_PER_GROUP(sb)) {\r\next4_warning(sb, "too much blocks added to group %u\n",\r\nblock_group);\r\nerr = -EINVAL;\r\ngoto error_return;\r\n}\r\nbitmap_bh = ext4_read_block_bitmap(sb, block_group);\r\nif (!bitmap_bh) {\r\nerr = -EIO;\r\ngoto error_return;\r\n}\r\ndesc = ext4_get_group_desc(sb, block_group, &gd_bh);\r\nif (!desc) {\r\nerr = -EIO;\r\ngoto error_return;\r\n}\r\nif (in_range(ext4_block_bitmap(sb, desc), block, count) ||\r\nin_range(ext4_inode_bitmap(sb, desc), block, count) ||\r\nin_range(block, ext4_inode_table(sb, desc), sbi->s_itb_per_group) ||\r\nin_range(block + count - 1, ext4_inode_table(sb, desc),\r\nsbi->s_itb_per_group)) {\r\next4_error(sb, "Adding blocks in system zones - "\r\n"Block = %llu, count = %lu",\r\nblock, count);\r\nerr = -EINVAL;\r\ngoto error_return;\r\n}\r\nBUFFER_TRACE(bitmap_bh, "getting write access");\r\nerr = ext4_journal_get_write_access(handle, bitmap_bh);\r\nif (err)\r\ngoto error_return;\r\nBUFFER_TRACE(gd_bh, "get_write_access");\r\nerr = ext4_journal_get_write_access(handle, gd_bh);\r\nif (err)\r\ngoto error_return;\r\nfor (i = 0, blocks_freed = 0; i < count; i++) {\r\nBUFFER_TRACE(bitmap_bh, "clear bit");\r\nif (!mb_test_bit(bit + i, bitmap_bh->b_data)) {\r\next4_error(sb, "bit already cleared for block %llu",\r\n(ext4_fsblk_t)(block + i));\r\nBUFFER_TRACE(bitmap_bh, "bit already cleared");\r\n} else {\r\nblocks_freed++;\r\n}\r\n}\r\nerr = ext4_mb_load_buddy(sb, block_group, &e4b);\r\nif (err)\r\ngoto error_return;\r\next4_lock_group(sb, block_group);\r\nmb_clear_bits(bitmap_bh->b_data, bit, count);\r\nmb_free_blocks(NULL, &e4b, bit, count);\r\nblk_free_count = blocks_freed + ext4_free_group_clusters(sb, desc);\r\next4_free_group_clusters_set(sb, desc, blk_free_count);\r\next4_block_bitmap_csum_set(sb, block_group, desc, bitmap_bh);\r\next4_group_desc_csum_set(sb, block_group, desc);\r\next4_unlock_group(sb, block_group);\r\npercpu_counter_add(&sbi->s_freeclusters_counter,\r\nEXT4_NUM_B2C(sbi, blocks_freed));\r\nif (sbi->s_log_groups_per_flex) {\r\next4_group_t flex_group = ext4_flex_group(sbi, block_group);\r\natomic64_add(EXT4_NUM_B2C(sbi, blocks_freed),\r\n&sbi->s_flex_groups[flex_group].free_clusters);\r\n}\r\next4_mb_unload_buddy(&e4b);\r\nBUFFER_TRACE(bitmap_bh, "dirtied bitmap block");\r\nerr = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);\r\nBUFFER_TRACE(gd_bh, "dirtied group descriptor block");\r\nret = ext4_handle_dirty_metadata(handle, NULL, gd_bh);\r\nif (!err)\r\nerr = ret;\r\nerror_return:\r\nbrelse(bitmap_bh);\r\next4_std_error(sb, err);\r\nreturn err;\r\n}\r\nstatic int ext4_trim_extent(struct super_block *sb, int start, int count,\r\next4_group_t group, struct ext4_buddy *e4b)\r\n__releases(bitlock)\r\n__acquires(bitlock)\r\n{\r\nstruct ext4_free_extent ex;\r\nint ret = 0;\r\ntrace_ext4_trim_extent(sb, group, start, count);\r\nassert_spin_locked(ext4_group_lock_ptr(sb, group));\r\nex.fe_start = start;\r\nex.fe_group = group;\r\nex.fe_len = count;\r\nmb_mark_used(e4b, &ex);\r\next4_unlock_group(sb, group);\r\nret = ext4_issue_discard(sb, group, start, count);\r\next4_lock_group(sb, group);\r\nmb_free_blocks(NULL, e4b, start, ex.fe_len);\r\nreturn ret;\r\n}\r\nstatic ext4_grpblk_t\r\next4_trim_all_free(struct super_block *sb, ext4_group_t group,\r\next4_grpblk_t start, ext4_grpblk_t max,\r\next4_grpblk_t minblocks)\r\n{\r\nvoid *bitmap;\r\next4_grpblk_t next, count = 0, free_count = 0;\r\nstruct ext4_buddy e4b;\r\nint ret = 0;\r\ntrace_ext4_trim_all_free(sb, group, start, max);\r\nret = ext4_mb_load_buddy(sb, group, &e4b);\r\nif (ret) {\r\next4_error(sb, "Error in loading buddy "\r\n"information for %u", group);\r\nreturn ret;\r\n}\r\nbitmap = e4b.bd_bitmap;\r\next4_lock_group(sb, group);\r\nif (EXT4_MB_GRP_WAS_TRIMMED(e4b.bd_info) &&\r\nminblocks >= atomic_read(&EXT4_SB(sb)->s_last_trim_minblks))\r\ngoto out;\r\nstart = (e4b.bd_info->bb_first_free > start) ?\r\ne4b.bd_info->bb_first_free : start;\r\nwhile (start <= max) {\r\nstart = mb_find_next_zero_bit(bitmap, max + 1, start);\r\nif (start > max)\r\nbreak;\r\nnext = mb_find_next_bit(bitmap, max + 1, start);\r\nif ((next - start) >= minblocks) {\r\nret = ext4_trim_extent(sb, start,\r\nnext - start, group, &e4b);\r\nif (ret && ret != -EOPNOTSUPP)\r\nbreak;\r\nret = 0;\r\ncount += next - start;\r\n}\r\nfree_count += next - start;\r\nstart = next + 1;\r\nif (fatal_signal_pending(current)) {\r\ncount = -ERESTARTSYS;\r\nbreak;\r\n}\r\nif (need_resched()) {\r\next4_unlock_group(sb, group);\r\ncond_resched();\r\next4_lock_group(sb, group);\r\n}\r\nif ((e4b.bd_info->bb_free - free_count) < minblocks)\r\nbreak;\r\n}\r\nif (!ret) {\r\nret = count;\r\nEXT4_MB_GRP_SET_TRIMMED(e4b.bd_info);\r\n}\r\nout:\r\next4_unlock_group(sb, group);\r\next4_mb_unload_buddy(&e4b);\r\next4_debug("trimmed %d blocks in the group %d\n",\r\ncount, group);\r\nreturn ret;\r\n}\r\nint ext4_trim_fs(struct super_block *sb, struct fstrim_range *range)\r\n{\r\nstruct ext4_group_info *grp;\r\next4_group_t group, first_group, last_group;\r\next4_grpblk_t cnt = 0, first_cluster, last_cluster;\r\nuint64_t start, end, minlen, trimmed = 0;\r\next4_fsblk_t first_data_blk =\r\nle32_to_cpu(EXT4_SB(sb)->s_es->s_first_data_block);\r\next4_fsblk_t max_blks = ext4_blocks_count(EXT4_SB(sb)->s_es);\r\nint ret = 0;\r\nstart = range->start >> sb->s_blocksize_bits;\r\nend = start + (range->len >> sb->s_blocksize_bits) - 1;\r\nminlen = EXT4_NUM_B2C(EXT4_SB(sb),\r\nrange->minlen >> sb->s_blocksize_bits);\r\nif (minlen > EXT4_CLUSTERS_PER_GROUP(sb) ||\r\nstart >= max_blks ||\r\nrange->len < sb->s_blocksize)\r\nreturn -EINVAL;\r\nif (end >= max_blks)\r\nend = max_blks - 1;\r\nif (end <= first_data_blk)\r\ngoto out;\r\nif (start < first_data_blk)\r\nstart = first_data_blk;\r\next4_get_group_no_and_offset(sb, (ext4_fsblk_t) start,\r\n&first_group, &first_cluster);\r\next4_get_group_no_and_offset(sb, (ext4_fsblk_t) end,\r\n&last_group, &last_cluster);\r\nend = EXT4_CLUSTERS_PER_GROUP(sb) - 1;\r\nfor (group = first_group; group <= last_group; group++) {\r\ngrp = ext4_get_group_info(sb, group);\r\nif (unlikely(EXT4_MB_GRP_NEED_INIT(grp))) {\r\nret = ext4_mb_init_group(sb, group);\r\nif (ret)\r\nbreak;\r\n}\r\nif (group == last_group)\r\nend = last_cluster;\r\nif (grp->bb_free >= minlen) {\r\ncnt = ext4_trim_all_free(sb, group, first_cluster,\r\nend, minlen);\r\nif (cnt < 0) {\r\nret = cnt;\r\nbreak;\r\n}\r\ntrimmed += cnt;\r\n}\r\nfirst_cluster = 0;\r\n}\r\nif (!ret)\r\natomic_set(&EXT4_SB(sb)->s_last_trim_minblks, minlen);\r\nout:\r\nrange->len = EXT4_C2B(EXT4_SB(sb), trimmed) << sb->s_blocksize_bits;\r\nreturn ret;\r\n}
