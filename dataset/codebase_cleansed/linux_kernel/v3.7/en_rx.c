static int mlx4_en_alloc_frags(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_desc *rx_desc,\r\nstruct mlx4_en_rx_alloc *frags,\r\nstruct mlx4_en_rx_alloc *ring_alloc)\r\n{\r\nstruct mlx4_en_rx_alloc page_alloc[MLX4_EN_MAX_RX_FRAGS];\r\nstruct mlx4_en_frag_info *frag_info;\r\nstruct page *page;\r\ndma_addr_t dma;\r\nint i;\r\nfor (i = 0; i < priv->num_frags; i++) {\r\nfrag_info = &priv->frag_info[i];\r\nif (ring_alloc[i].offset == frag_info->last_offset) {\r\npage = alloc_pages(GFP_ATOMIC | __GFP_COMP,\r\nMLX4_EN_ALLOC_ORDER);\r\nif (!page)\r\ngoto out;\r\ndma = dma_map_page(priv->ddev, page, 0,\r\nMLX4_EN_ALLOC_SIZE, PCI_DMA_FROMDEVICE);\r\nif (dma_mapping_error(priv->ddev, dma)) {\r\nput_page(page);\r\ngoto out;\r\n}\r\npage_alloc[i].page = page;\r\npage_alloc[i].dma = dma;\r\npage_alloc[i].offset = frag_info->frag_align;\r\n} else {\r\npage_alloc[i].page = ring_alloc[i].page;\r\nget_page(ring_alloc[i].page);\r\npage_alloc[i].dma = ring_alloc[i].dma;\r\npage_alloc[i].offset = ring_alloc[i].offset +\r\nfrag_info->frag_stride;\r\n}\r\n}\r\nfor (i = 0; i < priv->num_frags; i++) {\r\nfrags[i] = ring_alloc[i];\r\ndma = ring_alloc[i].dma + ring_alloc[i].offset;\r\nring_alloc[i] = page_alloc[i];\r\nrx_desc->data[i].addr = cpu_to_be64(dma);\r\n}\r\nreturn 0;\r\nout:\r\nwhile (i--) {\r\nfrag_info = &priv->frag_info[i];\r\nif (ring_alloc[i].offset == frag_info->last_offset)\r\ndma_unmap_page(priv->ddev, page_alloc[i].dma,\r\nMLX4_EN_ALLOC_SIZE, PCI_DMA_FROMDEVICE);\r\nput_page(page_alloc[i].page);\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void mlx4_en_free_frag(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_alloc *frags,\r\nint i)\r\n{\r\nstruct mlx4_en_frag_info *frag_info = &priv->frag_info[i];\r\nif (frags[i].offset == frag_info->last_offset) {\r\ndma_unmap_page(priv->ddev, frags[i].dma, MLX4_EN_ALLOC_SIZE,\r\nPCI_DMA_FROMDEVICE);\r\n}\r\nif (frags[i].page)\r\nput_page(frags[i].page);\r\n}\r\nstatic int mlx4_en_init_allocator(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring)\r\n{\r\nstruct mlx4_en_rx_alloc *page_alloc;\r\nint i;\r\nfor (i = 0; i < priv->num_frags; i++) {\r\npage_alloc = &ring->page_alloc[i];\r\npage_alloc->page = alloc_pages(GFP_ATOMIC | __GFP_COMP,\r\nMLX4_EN_ALLOC_ORDER);\r\nif (!page_alloc->page)\r\ngoto out;\r\npage_alloc->dma = dma_map_page(priv->ddev, page_alloc->page, 0,\r\nMLX4_EN_ALLOC_SIZE, PCI_DMA_FROMDEVICE);\r\nif (dma_mapping_error(priv->ddev, page_alloc->dma)) {\r\nput_page(page_alloc->page);\r\npage_alloc->page = NULL;\r\ngoto out;\r\n}\r\npage_alloc->offset = priv->frag_info[i].frag_align;\r\nen_dbg(DRV, priv, "Initialized allocator:%d with page:%p\n",\r\ni, page_alloc->page);\r\n}\r\nreturn 0;\r\nout:\r\nwhile (i--) {\r\npage_alloc = &ring->page_alloc[i];\r\ndma_unmap_page(priv->ddev, page_alloc->dma,\r\nMLX4_EN_ALLOC_SIZE, PCI_DMA_FROMDEVICE);\r\nput_page(page_alloc->page);\r\npage_alloc->page = NULL;\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void mlx4_en_destroy_allocator(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring)\r\n{\r\nstruct mlx4_en_rx_alloc *page_alloc;\r\nint i;\r\nfor (i = 0; i < priv->num_frags; i++) {\r\npage_alloc = &ring->page_alloc[i];\r\nen_dbg(DRV, priv, "Freeing allocator:%d count:%d\n",\r\ni, page_count(page_alloc->page));\r\ndma_unmap_page(priv->ddev, page_alloc->dma,\r\nMLX4_EN_ALLOC_SIZE, PCI_DMA_FROMDEVICE);\r\nput_page(page_alloc->page);\r\npage_alloc->page = NULL;\r\n}\r\n}\r\nstatic void mlx4_en_init_rx_desc(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring, int index)\r\n{\r\nstruct mlx4_en_rx_desc *rx_desc = ring->buf + ring->stride * index;\r\nint possible_frags;\r\nint i;\r\nfor (i = 0; i < priv->num_frags; i++) {\r\nrx_desc->data[i].byte_count =\r\ncpu_to_be32(priv->frag_info[i].frag_size);\r\nrx_desc->data[i].lkey = cpu_to_be32(priv->mdev->mr.key);\r\n}\r\npossible_frags = (ring->stride - sizeof(struct mlx4_en_rx_desc)) / DS_SIZE;\r\nfor (i = priv->num_frags; i < possible_frags; i++) {\r\nrx_desc->data[i].byte_count = 0;\r\nrx_desc->data[i].lkey = cpu_to_be32(MLX4_EN_MEMTYPE_PAD);\r\nrx_desc->data[i].addr = 0;\r\n}\r\n}\r\nstatic int mlx4_en_prepare_rx_desc(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring, int index)\r\n{\r\nstruct mlx4_en_rx_desc *rx_desc = ring->buf + (index * ring->stride);\r\nstruct mlx4_en_rx_alloc *frags = ring->rx_info +\r\n(index << priv->log_rx_info);\r\nreturn mlx4_en_alloc_frags(priv, rx_desc, frags, ring->page_alloc);\r\n}\r\nstatic inline void mlx4_en_update_rx_prod_db(struct mlx4_en_rx_ring *ring)\r\n{\r\n*ring->wqres.db.db = cpu_to_be32(ring->prod & 0xffff);\r\n}\r\nstatic void mlx4_en_free_rx_desc(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring,\r\nint index)\r\n{\r\nstruct mlx4_en_rx_alloc *frags;\r\nint nr;\r\nfrags = ring->rx_info + (index << priv->log_rx_info);\r\nfor (nr = 0; nr < priv->num_frags; nr++) {\r\nen_dbg(DRV, priv, "Freeing fragment:%d\n", nr);\r\nmlx4_en_free_frag(priv, frags, nr);\r\n}\r\n}\r\nstatic int mlx4_en_fill_rx_buffers(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_rx_ring *ring;\r\nint ring_ind;\r\nint buf_ind;\r\nint new_size;\r\nfor (buf_ind = 0; buf_ind < priv->prof->rx_ring_size; buf_ind++) {\r\nfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {\r\nring = &priv->rx_ring[ring_ind];\r\nif (mlx4_en_prepare_rx_desc(priv, ring,\r\nring->actual_size)) {\r\nif (ring->actual_size < MLX4_EN_MIN_RX_SIZE) {\r\nen_err(priv, "Failed to allocate "\r\n"enough rx buffers\n");\r\nreturn -ENOMEM;\r\n} else {\r\nnew_size = rounddown_pow_of_two(ring->actual_size);\r\nen_warn(priv, "Only %d buffers allocated "\r\n"reducing ring size to %d",\r\nring->actual_size, new_size);\r\ngoto reduce_rings;\r\n}\r\n}\r\nring->actual_size++;\r\nring->prod++;\r\n}\r\n}\r\nreturn 0;\r\nreduce_rings:\r\nfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {\r\nring = &priv->rx_ring[ring_ind];\r\nwhile (ring->actual_size > new_size) {\r\nring->actual_size--;\r\nring->prod--;\r\nmlx4_en_free_rx_desc(priv, ring, ring->actual_size);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void mlx4_en_free_rx_buf(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring)\r\n{\r\nint index;\r\nen_dbg(DRV, priv, "Freeing Rx buf - cons:%d prod:%d\n",\r\nring->cons, ring->prod);\r\nBUG_ON((u32) (ring->prod - ring->cons) > ring->actual_size);\r\nwhile (ring->cons != ring->prod) {\r\nindex = ring->cons & ring->size_mask;\r\nen_dbg(DRV, priv, "Processing descriptor:%d\n", index);\r\nmlx4_en_free_rx_desc(priv, ring, index);\r\n++ring->cons;\r\n}\r\n}\r\nint mlx4_en_create_rx_ring(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring, u32 size, u16 stride)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err = -ENOMEM;\r\nint tmp;\r\nring->prod = 0;\r\nring->cons = 0;\r\nring->size = size;\r\nring->size_mask = size - 1;\r\nring->stride = stride;\r\nring->log_stride = ffs(ring->stride) - 1;\r\nring->buf_size = ring->size * ring->stride + TXBB_SIZE;\r\ntmp = size * roundup_pow_of_two(MLX4_EN_MAX_RX_FRAGS *\r\nsizeof(struct mlx4_en_rx_alloc));\r\nring->rx_info = vmalloc(tmp);\r\nif (!ring->rx_info)\r\nreturn -ENOMEM;\r\nen_dbg(DRV, priv, "Allocated rx_info ring at addr:%p size:%d\n",\r\nring->rx_info, tmp);\r\nerr = mlx4_alloc_hwq_res(mdev->dev, &ring->wqres,\r\nring->buf_size, 2 * PAGE_SIZE);\r\nif (err)\r\ngoto err_ring;\r\nerr = mlx4_en_map_buffer(&ring->wqres.buf);\r\nif (err) {\r\nen_err(priv, "Failed to map RX buffer\n");\r\ngoto err_hwq;\r\n}\r\nring->buf = ring->wqres.buf.direct.buf;\r\nreturn 0;\r\nerr_hwq:\r\nmlx4_free_hwq_res(mdev->dev, &ring->wqres, ring->buf_size);\r\nerr_ring:\r\nvfree(ring->rx_info);\r\nring->rx_info = NULL;\r\nreturn err;\r\n}\r\nint mlx4_en_activate_rx_rings(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_rx_ring *ring;\r\nint i;\r\nint ring_ind;\r\nint err;\r\nint stride = roundup_pow_of_two(sizeof(struct mlx4_en_rx_desc) +\r\nDS_SIZE * priv->num_frags);\r\nfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {\r\nring = &priv->rx_ring[ring_ind];\r\nring->prod = 0;\r\nring->cons = 0;\r\nring->actual_size = 0;\r\nring->cqn = priv->rx_cq[ring_ind].mcq.cqn;\r\nring->stride = stride;\r\nif (ring->stride <= TXBB_SIZE)\r\nring->buf += TXBB_SIZE;\r\nring->log_stride = ffs(ring->stride) - 1;\r\nring->buf_size = ring->size * ring->stride;\r\nmemset(ring->buf, 0, ring->buf_size);\r\nmlx4_en_update_rx_prod_db(ring);\r\nfor (i = 0; i < ring->size; i++)\r\nmlx4_en_init_rx_desc(priv, ring, i);\r\nerr = mlx4_en_init_allocator(priv, ring);\r\nif (err) {\r\nen_err(priv, "Failed initializing ring allocator\n");\r\nif (ring->stride <= TXBB_SIZE)\r\nring->buf -= TXBB_SIZE;\r\nring_ind--;\r\ngoto err_allocator;\r\n}\r\n}\r\nerr = mlx4_en_fill_rx_buffers(priv);\r\nif (err)\r\ngoto err_buffers;\r\nfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {\r\nring = &priv->rx_ring[ring_ind];\r\nring->size_mask = ring->actual_size - 1;\r\nmlx4_en_update_rx_prod_db(ring);\r\n}\r\nreturn 0;\r\nerr_buffers:\r\nfor (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++)\r\nmlx4_en_free_rx_buf(priv, &priv->rx_ring[ring_ind]);\r\nring_ind = priv->rx_ring_num - 1;\r\nerr_allocator:\r\nwhile (ring_ind >= 0) {\r\nif (priv->rx_ring[ring_ind].stride <= TXBB_SIZE)\r\npriv->rx_ring[ring_ind].buf -= TXBB_SIZE;\r\nmlx4_en_destroy_allocator(priv, &priv->rx_ring[ring_ind]);\r\nring_ind--;\r\n}\r\nreturn err;\r\n}\r\nvoid mlx4_en_destroy_rx_ring(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring, u32 size, u16 stride)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nmlx4_en_unmap_buffer(&ring->wqres.buf);\r\nmlx4_free_hwq_res(mdev->dev, &ring->wqres, size * stride + TXBB_SIZE);\r\nvfree(ring->rx_info);\r\nring->rx_info = NULL;\r\n#ifdef CONFIG_RFS_ACCEL\r\nmlx4_en_cleanup_filters(priv, ring);\r\n#endif\r\n}\r\nvoid mlx4_en_deactivate_rx_ring(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring)\r\n{\r\nmlx4_en_free_rx_buf(priv, ring);\r\nif (ring->stride <= TXBB_SIZE)\r\nring->buf -= TXBB_SIZE;\r\nmlx4_en_destroy_allocator(priv, ring);\r\n}\r\nstatic int mlx4_en_complete_rx_desc(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_desc *rx_desc,\r\nstruct mlx4_en_rx_alloc *frags,\r\nstruct sk_buff *skb,\r\nint length)\r\n{\r\nstruct skb_frag_struct *skb_frags_rx = skb_shinfo(skb)->frags;\r\nstruct mlx4_en_frag_info *frag_info;\r\nint nr;\r\ndma_addr_t dma;\r\nfor (nr = 0; nr < priv->num_frags; nr++) {\r\nfrag_info = &priv->frag_info[nr];\r\nif (length <= frag_info->frag_prefix_size)\r\nbreak;\r\nif (!frags[nr].page)\r\ngoto fail;\r\ndma = be64_to_cpu(rx_desc->data[nr].addr);\r\ndma_sync_single_for_cpu(priv->ddev, dma, frag_info->frag_size,\r\nDMA_FROM_DEVICE);\r\nget_page(frags[nr].page);\r\n__skb_frag_set_page(&skb_frags_rx[nr], frags[nr].page);\r\nskb_frag_size_set(&skb_frags_rx[nr], frag_info->frag_size);\r\nskb_frags_rx[nr].page_offset = frags[nr].offset;\r\nskb->truesize += frag_info->frag_stride;\r\n}\r\nif (nr > 0)\r\nskb_frag_size_set(&skb_frags_rx[nr - 1],\r\nlength - priv->frag_info[nr - 1].frag_prefix_size);\r\nreturn nr;\r\nfail:\r\nwhile (nr > 0) {\r\nnr--;\r\n__skb_frag_unref(&skb_frags_rx[nr]);\r\n}\r\nreturn 0;\r\n}\r\nstatic struct sk_buff *mlx4_en_rx_skb(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_desc *rx_desc,\r\nstruct mlx4_en_rx_alloc *frags,\r\nunsigned int length)\r\n{\r\nstruct sk_buff *skb;\r\nvoid *va;\r\nint used_frags;\r\ndma_addr_t dma;\r\nskb = netdev_alloc_skb(priv->dev, SMALL_PACKET_SIZE + NET_IP_ALIGN);\r\nif (!skb) {\r\nen_dbg(RX_ERR, priv, "Failed allocating skb\n");\r\nreturn NULL;\r\n}\r\nskb_reserve(skb, NET_IP_ALIGN);\r\nskb->len = length;\r\nva = page_address(frags[0].page) + frags[0].offset;\r\nif (length <= SMALL_PACKET_SIZE) {\r\ndma = be64_to_cpu(rx_desc->data[0].addr);\r\ndma_sync_single_for_cpu(priv->ddev, dma, length,\r\nDMA_FROM_DEVICE);\r\nskb_copy_to_linear_data(skb, va, length);\r\nskb->tail += length;\r\n} else {\r\nused_frags = mlx4_en_complete_rx_desc(priv, rx_desc, frags,\r\nskb, length);\r\nif (unlikely(!used_frags)) {\r\nkfree_skb(skb);\r\nreturn NULL;\r\n}\r\nskb_shinfo(skb)->nr_frags = used_frags;\r\nmemcpy(skb->data, va, HEADER_COPY_SIZE);\r\nskb->tail += HEADER_COPY_SIZE;\r\nskb_shinfo(skb)->frags[0].page_offset += HEADER_COPY_SIZE;\r\nskb_frag_size_sub(&skb_shinfo(skb)->frags[0], HEADER_COPY_SIZE);\r\nskb->data_len = length - HEADER_COPY_SIZE;\r\n}\r\nreturn skb;\r\n}\r\nstatic void validate_loopback(struct mlx4_en_priv *priv, struct sk_buff *skb)\r\n{\r\nint i;\r\nint offset = ETH_HLEN;\r\nfor (i = 0; i < MLX4_LOOPBACK_TEST_PAYLOAD; i++, offset++) {\r\nif (*(skb->data + offset) != (unsigned char) (i & 0xff))\r\ngoto out_loopback;\r\n}\r\npriv->loopback_ok = 1;\r\nout_loopback:\r\ndev_kfree_skb_any(skb);\r\n}\r\nstatic void mlx4_en_refill_rx_buffers(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *ring)\r\n{\r\nint index = ring->prod & ring->size_mask;\r\nwhile ((u32) (ring->prod - ring->cons) < ring->actual_size) {\r\nif (mlx4_en_prepare_rx_desc(priv, ring, index))\r\nbreak;\r\nring->prod++;\r\nindex = ring->prod & ring->size_mask;\r\n}\r\n}\r\nint mlx4_en_process_rx_cq(struct net_device *dev, struct mlx4_en_cq *cq, int budget)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_cqe *cqe;\r\nstruct mlx4_en_rx_ring *ring = &priv->rx_ring[cq->ring];\r\nstruct mlx4_en_rx_alloc *frags;\r\nstruct mlx4_en_rx_desc *rx_desc;\r\nstruct sk_buff *skb;\r\nint index;\r\nint nr;\r\nunsigned int length;\r\nint polled = 0;\r\nint ip_summed;\r\nstruct ethhdr *ethh;\r\ndma_addr_t dma;\r\nu64 s_mac;\r\nif (!priv->port_up)\r\nreturn 0;\r\nindex = cq->mcq.cons_index & ring->size_mask;\r\ncqe = &cq->buf[index];\r\nwhile (XNOR(cqe->owner_sr_opcode & MLX4_CQE_OWNER_MASK,\r\ncq->mcq.cons_index & cq->size)) {\r\nfrags = ring->rx_info + (index << priv->log_rx_info);\r\nrx_desc = ring->buf + (index << ring->log_stride);\r\nrmb();\r\nif (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==\r\nMLX4_CQE_OPCODE_ERROR)) {\r\nen_err(priv, "CQE completed in error - vendor "\r\n"syndrom:%d syndrom:%d\n",\r\n((struct mlx4_err_cqe *) cqe)->vendor_err_syndrome,\r\n((struct mlx4_err_cqe *) cqe)->syndrome);\r\ngoto next;\r\n}\r\nif (unlikely(cqe->badfcs_enc & MLX4_CQE_BAD_FCS)) {\r\nen_dbg(RX_ERR, priv, "Accepted frame with bad FCS\n");\r\ngoto next;\r\n}\r\ndma = be64_to_cpu(rx_desc->data[0].addr);\r\ndma_sync_single_for_cpu(priv->ddev, dma, sizeof(*ethh),\r\nDMA_FROM_DEVICE);\r\nethh = (struct ethhdr *)(page_address(frags[0].page) +\r\nfrags[0].offset);\r\ns_mac = mlx4_en_mac_to_u64(ethh->h_source);\r\nif (s_mac == priv->mac &&\r\n!((dev->features & NETIF_F_LOOPBACK) ||\r\npriv->validate_loopback))\r\ngoto next;\r\nlength = be32_to_cpu(cqe->byte_cnt);\r\nlength -= ring->fcs_del;\r\nring->bytes += length;\r\nring->packets++;\r\nif (likely(dev->features & NETIF_F_RXCSUM)) {\r\nif ((cqe->status & cpu_to_be16(MLX4_CQE_STATUS_IPOK)) &&\r\n(cqe->checksum == cpu_to_be16(0xffff))) {\r\nring->csum_ok++;\r\nif (dev->features & NETIF_F_GRO) {\r\nstruct sk_buff *gro_skb = napi_get_frags(&cq->napi);\r\nif (!gro_skb)\r\ngoto next;\r\nnr = mlx4_en_complete_rx_desc(priv,\r\nrx_desc, frags, gro_skb,\r\nlength);\r\nif (!nr)\r\ngoto next;\r\nskb_shinfo(gro_skb)->nr_frags = nr;\r\ngro_skb->len = length;\r\ngro_skb->data_len = length;\r\ngro_skb->ip_summed = CHECKSUM_UNNECESSARY;\r\nif (cqe->vlan_my_qpn &\r\ncpu_to_be32(MLX4_CQE_VLAN_PRESENT_MASK)) {\r\nu16 vid = be16_to_cpu(cqe->sl_vid);\r\n__vlan_hwaccel_put_tag(gro_skb, vid);\r\n}\r\nif (dev->features & NETIF_F_RXHASH)\r\ngro_skb->rxhash = be32_to_cpu(cqe->immed_rss_invalid);\r\nskb_record_rx_queue(gro_skb, cq->ring);\r\nnapi_gro_frags(&cq->napi);\r\ngoto next;\r\n}\r\nip_summed = CHECKSUM_UNNECESSARY;\r\n} else {\r\nip_summed = CHECKSUM_NONE;\r\nring->csum_none++;\r\n}\r\n} else {\r\nip_summed = CHECKSUM_NONE;\r\nring->csum_none++;\r\n}\r\nskb = mlx4_en_rx_skb(priv, rx_desc, frags, length);\r\nif (!skb) {\r\npriv->stats.rx_dropped++;\r\ngoto next;\r\n}\r\nif (unlikely(priv->validate_loopback)) {\r\nvalidate_loopback(priv, skb);\r\ngoto next;\r\n}\r\nskb->ip_summed = ip_summed;\r\nskb->protocol = eth_type_trans(skb, dev);\r\nskb_record_rx_queue(skb, cq->ring);\r\nif (dev->features & NETIF_F_RXHASH)\r\nskb->rxhash = be32_to_cpu(cqe->immed_rss_invalid);\r\nif (be32_to_cpu(cqe->vlan_my_qpn) &\r\nMLX4_CQE_VLAN_PRESENT_MASK)\r\n__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->sl_vid));\r\nnetif_receive_skb(skb);\r\nnext:\r\nfor (nr = 0; nr < priv->num_frags; nr++)\r\nmlx4_en_free_frag(priv, frags, nr);\r\n++cq->mcq.cons_index;\r\nindex = (cq->mcq.cons_index) & ring->size_mask;\r\ncqe = &cq->buf[index];\r\nif (++polled == budget) {\r\ngoto out;\r\n}\r\n}\r\nout:\r\nAVG_PERF_COUNTER(priv->pstats.rx_coal_avg, polled);\r\nmlx4_cq_set_ci(&cq->mcq);\r\nwmb();\r\nring->cons = cq->mcq.cons_index;\r\nmlx4_en_refill_rx_buffers(priv, ring);\r\nmlx4_en_update_rx_prod_db(ring);\r\nreturn polled;\r\n}\r\nvoid mlx4_en_rx_irq(struct mlx4_cq *mcq)\r\n{\r\nstruct mlx4_en_cq *cq = container_of(mcq, struct mlx4_en_cq, mcq);\r\nstruct mlx4_en_priv *priv = netdev_priv(cq->dev);\r\nif (priv->port_up)\r\nnapi_schedule(&cq->napi);\r\nelse\r\nmlx4_en_arm_cq(priv, cq);\r\n}\r\nint mlx4_en_poll_rx_cq(struct napi_struct *napi, int budget)\r\n{\r\nstruct mlx4_en_cq *cq = container_of(napi, struct mlx4_en_cq, napi);\r\nstruct net_device *dev = cq->dev;\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nint done;\r\ndone = mlx4_en_process_rx_cq(dev, cq, budget);\r\nif (done == budget)\r\nINC_PERF_COUNTER(priv->pstats.napi_quota);\r\nelse {\r\nnapi_complete(napi);\r\nmlx4_en_arm_cq(priv, cq);\r\n}\r\nreturn done;\r\n}\r\nstatic int mlx4_en_last_alloc_offset(struct mlx4_en_priv *priv, u16 stride, u16 align)\r\n{\r\nu16 res = MLX4_EN_ALLOC_SIZE % stride;\r\nu16 offset = MLX4_EN_ALLOC_SIZE - stride - res + align;\r\nen_dbg(DRV, priv, "Calculated last offset for stride:%d align:%d "\r\n"res:%d offset:%d\n", stride, align, res, offset);\r\nreturn offset;\r\n}\r\nvoid mlx4_en_calc_rx_buf(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nint eff_mtu = dev->mtu + ETH_HLEN + VLAN_HLEN + ETH_LLC_SNAP_SIZE;\r\nint buf_size = 0;\r\nint i = 0;\r\nwhile (buf_size < eff_mtu) {\r\npriv->frag_info[i].frag_size =\r\n(eff_mtu > buf_size + frag_sizes[i]) ?\r\nfrag_sizes[i] : eff_mtu - buf_size;\r\npriv->frag_info[i].frag_prefix_size = buf_size;\r\nif (!i) {\r\npriv->frag_info[i].frag_align = NET_IP_ALIGN;\r\npriv->frag_info[i].frag_stride =\r\nALIGN(frag_sizes[i] + NET_IP_ALIGN, SMP_CACHE_BYTES);\r\n} else {\r\npriv->frag_info[i].frag_align = 0;\r\npriv->frag_info[i].frag_stride =\r\nALIGN(frag_sizes[i], SMP_CACHE_BYTES);\r\n}\r\npriv->frag_info[i].last_offset = mlx4_en_last_alloc_offset(\r\npriv, priv->frag_info[i].frag_stride,\r\npriv->frag_info[i].frag_align);\r\nbuf_size += priv->frag_info[i].frag_size;\r\ni++;\r\n}\r\npriv->num_frags = i;\r\npriv->rx_skb_size = eff_mtu;\r\npriv->log_rx_info = ROUNDUP_LOG2(i * sizeof(struct mlx4_en_rx_alloc));\r\nen_dbg(DRV, priv, "Rx buffer scatter-list (effective-mtu:%d "\r\n"num_frags:%d):\n", eff_mtu, priv->num_frags);\r\nfor (i = 0; i < priv->num_frags; i++) {\r\nen_dbg(DRV, priv, " frag:%d - size:%d prefix:%d align:%d "\r\n"stride:%d last_offset:%d\n", i,\r\npriv->frag_info[i].frag_size,\r\npriv->frag_info[i].frag_prefix_size,\r\npriv->frag_info[i].frag_align,\r\npriv->frag_info[i].frag_stride,\r\npriv->frag_info[i].last_offset);\r\n}\r\n}\r\nstatic int mlx4_en_config_rss_qp(struct mlx4_en_priv *priv, int qpn,\r\nstruct mlx4_en_rx_ring *ring,\r\nenum mlx4_qp_state *state,\r\nstruct mlx4_qp *qp)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_qp_context *context;\r\nint err = 0;\r\ncontext = kmalloc(sizeof *context , GFP_KERNEL);\r\nif (!context) {\r\nen_err(priv, "Failed to allocate qp context\n");\r\nreturn -ENOMEM;\r\n}\r\nerr = mlx4_qp_alloc(mdev->dev, qpn, qp);\r\nif (err) {\r\nen_err(priv, "Failed to allocate qp #%x\n", qpn);\r\ngoto out;\r\n}\r\nqp->event = mlx4_en_sqp_event;\r\nmemset(context, 0, sizeof *context);\r\nmlx4_en_fill_qp_context(priv, ring->actual_size, ring->stride, 0, 0,\r\nqpn, ring->cqn, -1, context);\r\ncontext->db_rec_addr = cpu_to_be64(ring->wqres.db.dma);\r\nif (mdev->dev->caps.flags & MLX4_DEV_CAP_FLAG_FCS_KEEP) {\r\ncontext->param3 |= cpu_to_be32(1 << 29);\r\nring->fcs_del = ETH_FCS_LEN;\r\n} else\r\nring->fcs_del = 0;\r\nerr = mlx4_qp_to_ready(mdev->dev, &ring->wqres.mtt, context, qp, state);\r\nif (err) {\r\nmlx4_qp_remove(mdev->dev, qp);\r\nmlx4_qp_free(mdev->dev, qp);\r\n}\r\nmlx4_en_update_rx_prod_db(ring);\r\nout:\r\nkfree(context);\r\nreturn err;\r\n}\r\nint mlx4_en_create_drop_qp(struct mlx4_en_priv *priv)\r\n{\r\nint err;\r\nu32 qpn;\r\nerr = mlx4_qp_reserve_range(priv->mdev->dev, 1, 1, &qpn);\r\nif (err) {\r\nen_err(priv, "Failed reserving drop qpn\n");\r\nreturn err;\r\n}\r\nerr = mlx4_qp_alloc(priv->mdev->dev, qpn, &priv->drop_qp);\r\nif (err) {\r\nen_err(priv, "Failed allocating drop qp\n");\r\nmlx4_qp_release_range(priv->mdev->dev, qpn, 1);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nvoid mlx4_en_destroy_drop_qp(struct mlx4_en_priv *priv)\r\n{\r\nu32 qpn;\r\nqpn = priv->drop_qp.qpn;\r\nmlx4_qp_remove(priv->mdev->dev, &priv->drop_qp);\r\nmlx4_qp_free(priv->mdev->dev, &priv->drop_qp);\r\nmlx4_qp_release_range(priv->mdev->dev, qpn, 1);\r\n}\r\nint mlx4_en_config_rss_steer(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_en_rss_map *rss_map = &priv->rss_map;\r\nstruct mlx4_qp_context context;\r\nstruct mlx4_rss_context *rss_context;\r\nint rss_rings;\r\nvoid *ptr;\r\nu8 rss_mask = (MLX4_RSS_IPV4 | MLX4_RSS_TCP_IPV4 | MLX4_RSS_IPV6 |\r\nMLX4_RSS_TCP_IPV6);\r\nint i, qpn;\r\nint err = 0;\r\nint good_qps = 0;\r\nstatic const u32 rsskey[10] = { 0xD181C62C, 0xF7F4DB5B, 0x1983A2FC,\r\n0x943E1ADB, 0xD9389E6B, 0xD1039C2C, 0xA74499AD,\r\n0x593D56D9, 0xF3253C06, 0x2ADC1FFC};\r\nen_dbg(DRV, priv, "Configuring rss steering\n");\r\nerr = mlx4_qp_reserve_range(mdev->dev, priv->rx_ring_num,\r\npriv->rx_ring_num,\r\n&rss_map->base_qpn);\r\nif (err) {\r\nen_err(priv, "Failed reserving %d qps\n", priv->rx_ring_num);\r\nreturn err;\r\n}\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\nqpn = rss_map->base_qpn + i;\r\nerr = mlx4_en_config_rss_qp(priv, qpn, &priv->rx_ring[i],\r\n&rss_map->state[i],\r\n&rss_map->qps[i]);\r\nif (err)\r\ngoto rss_err;\r\n++good_qps;\r\n}\r\nerr = mlx4_qp_alloc(mdev->dev, priv->base_qpn, &rss_map->indir_qp);\r\nif (err) {\r\nen_err(priv, "Failed to allocate RSS indirection QP\n");\r\ngoto rss_err;\r\n}\r\nrss_map->indir_qp.event = mlx4_en_sqp_event;\r\nmlx4_en_fill_qp_context(priv, 0, 0, 0, 1, priv->base_qpn,\r\npriv->rx_ring[0].cqn, -1, &context);\r\nif (!priv->prof->rss_rings || priv->prof->rss_rings > priv->rx_ring_num)\r\nrss_rings = priv->rx_ring_num;\r\nelse\r\nrss_rings = priv->prof->rss_rings;\r\nptr = ((void *) &context) + offsetof(struct mlx4_qp_context, pri_path)\r\n+ MLX4_RSS_OFFSET_IN_QPC_PRI_PATH;\r\nrss_context = ptr;\r\nrss_context->base_qpn = cpu_to_be32(ilog2(rss_rings) << 24 |\r\n(rss_map->base_qpn));\r\nrss_context->default_qpn = cpu_to_be32(rss_map->base_qpn);\r\nif (priv->mdev->profile.udp_rss) {\r\nrss_mask |= MLX4_RSS_UDP_IPV4 | MLX4_RSS_UDP_IPV6;\r\nrss_context->base_qpn_udp = rss_context->default_qpn;\r\n}\r\nrss_context->flags = rss_mask;\r\nrss_context->hash_fn = MLX4_RSS_HASH_TOP;\r\nfor (i = 0; i < 10; i++)\r\nrss_context->rss_key[i] = cpu_to_be32(rsskey[i]);\r\nerr = mlx4_qp_to_ready(mdev->dev, &priv->res.mtt, &context,\r\n&rss_map->indir_qp, &rss_map->indir_state);\r\nif (err)\r\ngoto indir_err;\r\nreturn 0;\r\nindir_err:\r\nmlx4_qp_modify(mdev->dev, NULL, rss_map->indir_state,\r\nMLX4_QP_STATE_RST, NULL, 0, 0, &rss_map->indir_qp);\r\nmlx4_qp_remove(mdev->dev, &rss_map->indir_qp);\r\nmlx4_qp_free(mdev->dev, &rss_map->indir_qp);\r\nrss_err:\r\nfor (i = 0; i < good_qps; i++) {\r\nmlx4_qp_modify(mdev->dev, NULL, rss_map->state[i],\r\nMLX4_QP_STATE_RST, NULL, 0, 0, &rss_map->qps[i]);\r\nmlx4_qp_remove(mdev->dev, &rss_map->qps[i]);\r\nmlx4_qp_free(mdev->dev, &rss_map->qps[i]);\r\n}\r\nmlx4_qp_release_range(mdev->dev, rss_map->base_qpn, priv->rx_ring_num);\r\nreturn err;\r\n}\r\nvoid mlx4_en_release_rss_steer(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_en_rss_map *rss_map = &priv->rss_map;\r\nint i;\r\nmlx4_qp_modify(mdev->dev, NULL, rss_map->indir_state,\r\nMLX4_QP_STATE_RST, NULL, 0, 0, &rss_map->indir_qp);\r\nmlx4_qp_remove(mdev->dev, &rss_map->indir_qp);\r\nmlx4_qp_free(mdev->dev, &rss_map->indir_qp);\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\nmlx4_qp_modify(mdev->dev, NULL, rss_map->state[i],\r\nMLX4_QP_STATE_RST, NULL, 0, 0, &rss_map->qps[i]);\r\nmlx4_qp_remove(mdev->dev, &rss_map->qps[i]);\r\nmlx4_qp_free(mdev->dev, &rss_map->qps[i]);\r\n}\r\nmlx4_qp_release_range(mdev->dev, rss_map->base_qpn, priv->rx_ring_num);\r\n}
