static int mmc_prep_request(struct request_queue *q, struct request *req)\r\n{\r\nstruct mmc_queue *mq = q->queuedata;\r\nif (req->cmd_type != REQ_TYPE_FS && !(req->cmd_flags & REQ_DISCARD)) {\r\nblk_dump_rq_flags(req, "MMC bad request");\r\nreturn BLKPREP_KILL;\r\n}\r\nif (mq && mmc_card_removed(mq->card))\r\nreturn BLKPREP_KILL;\r\nreq->cmd_flags |= REQ_DONTPREP;\r\nreturn BLKPREP_OK;\r\n}\r\nstatic int mmc_queue_thread(void *d)\r\n{\r\nstruct mmc_queue *mq = d;\r\nstruct request_queue *q = mq->queue;\r\ncurrent->flags |= PF_MEMALLOC;\r\ndown(&mq->thread_sem);\r\ndo {\r\nstruct request *req = NULL;\r\nstruct mmc_queue_req *tmp;\r\nspin_lock_irq(q->queue_lock);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nreq = blk_fetch_request(q);\r\nmq->mqrq_cur->req = req;\r\nspin_unlock_irq(q->queue_lock);\r\nif (req || mq->mqrq_prev->req) {\r\nset_current_state(TASK_RUNNING);\r\nmq->issue_fn(mq, req);\r\n} else {\r\nif (kthread_should_stop()) {\r\nset_current_state(TASK_RUNNING);\r\nbreak;\r\n}\r\nup(&mq->thread_sem);\r\nschedule();\r\ndown(&mq->thread_sem);\r\n}\r\nmq->mqrq_prev->brq.mrq.data = NULL;\r\nmq->mqrq_prev->req = NULL;\r\ntmp = mq->mqrq_prev;\r\nmq->mqrq_prev = mq->mqrq_cur;\r\nmq->mqrq_cur = tmp;\r\n} while (1);\r\nup(&mq->thread_sem);\r\nreturn 0;\r\n}\r\nstatic void mmc_request_fn(struct request_queue *q)\r\n{\r\nstruct mmc_queue *mq = q->queuedata;\r\nstruct request *req;\r\nif (!mq) {\r\nwhile ((req = blk_fetch_request(q)) != NULL) {\r\nreq->cmd_flags |= REQ_QUIET;\r\n__blk_end_request_all(req, -EIO);\r\n}\r\nreturn;\r\n}\r\nif (!mq->mqrq_cur->req && !mq->mqrq_prev->req)\r\nwake_up_process(mq->thread);\r\n}\r\nstatic struct scatterlist *mmc_alloc_sg(int sg_len, int *err)\r\n{\r\nstruct scatterlist *sg;\r\nsg = kmalloc(sizeof(struct scatterlist)*sg_len, GFP_KERNEL);\r\nif (!sg)\r\n*err = -ENOMEM;\r\nelse {\r\n*err = 0;\r\nsg_init_table(sg, sg_len);\r\n}\r\nreturn sg;\r\n}\r\nstatic void mmc_queue_setup_discard(struct request_queue *q,\r\nstruct mmc_card *card)\r\n{\r\nunsigned max_discard;\r\nmax_discard = mmc_calc_max_discard(card);\r\nif (!max_discard)\r\nreturn;\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);\r\nq->limits.max_discard_sectors = max_discard;\r\nif (card->erased_byte == 0 && !mmc_can_discard(card))\r\nq->limits.discard_zeroes_data = 1;\r\nq->limits.discard_granularity = card->pref_erase << 9;\r\nif (card->pref_erase > max_discard)\r\nq->limits.discard_granularity = 0;\r\nif (mmc_can_secure_erase_trim(card) || mmc_can_sanitize(card))\r\nqueue_flag_set_unlocked(QUEUE_FLAG_SECDISCARD, q);\r\n}\r\nint mmc_init_queue(struct mmc_queue *mq, struct mmc_card *card,\r\nspinlock_t *lock, const char *subname)\r\n{\r\nstruct mmc_host *host = card->host;\r\nu64 limit = BLK_BOUNCE_HIGH;\r\nint ret;\r\nstruct mmc_queue_req *mqrq_cur = &mq->mqrq[0];\r\nstruct mmc_queue_req *mqrq_prev = &mq->mqrq[1];\r\nif (mmc_dev(host)->dma_mask && *mmc_dev(host)->dma_mask)\r\nlimit = *mmc_dev(host)->dma_mask;\r\nmq->card = card;\r\nmq->queue = blk_init_queue(mmc_request_fn, lock);\r\nif (!mq->queue)\r\nreturn -ENOMEM;\r\nmq->mqrq_cur = mqrq_cur;\r\nmq->mqrq_prev = mqrq_prev;\r\nmq->queue->queuedata = mq;\r\nblk_queue_prep_rq(mq->queue, mmc_prep_request);\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, mq->queue);\r\nif (mmc_can_erase(card))\r\nmmc_queue_setup_discard(mq->queue, card);\r\n#ifdef CONFIG_MMC_BLOCK_BOUNCE\r\nif (host->max_segs == 1) {\r\nunsigned int bouncesz;\r\nbouncesz = MMC_QUEUE_BOUNCESZ;\r\nif (bouncesz > host->max_req_size)\r\nbouncesz = host->max_req_size;\r\nif (bouncesz > host->max_seg_size)\r\nbouncesz = host->max_seg_size;\r\nif (bouncesz > (host->max_blk_count * 512))\r\nbouncesz = host->max_blk_count * 512;\r\nif (bouncesz > 512) {\r\nmqrq_cur->bounce_buf = kmalloc(bouncesz, GFP_KERNEL);\r\nif (!mqrq_cur->bounce_buf) {\r\npr_warning("%s: unable to "\r\n"allocate bounce cur buffer\n",\r\nmmc_card_name(card));\r\n}\r\nmqrq_prev->bounce_buf = kmalloc(bouncesz, GFP_KERNEL);\r\nif (!mqrq_prev->bounce_buf) {\r\npr_warning("%s: unable to "\r\n"allocate bounce prev buffer\n",\r\nmmc_card_name(card));\r\nkfree(mqrq_cur->bounce_buf);\r\nmqrq_cur->bounce_buf = NULL;\r\n}\r\n}\r\nif (mqrq_cur->bounce_buf && mqrq_prev->bounce_buf) {\r\nblk_queue_bounce_limit(mq->queue, BLK_BOUNCE_ANY);\r\nblk_queue_max_hw_sectors(mq->queue, bouncesz / 512);\r\nblk_queue_max_segments(mq->queue, bouncesz / 512);\r\nblk_queue_max_segment_size(mq->queue, bouncesz);\r\nmqrq_cur->sg = mmc_alloc_sg(1, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\nmqrq_cur->bounce_sg =\r\nmmc_alloc_sg(bouncesz / 512, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\nmqrq_prev->sg = mmc_alloc_sg(1, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\nmqrq_prev->bounce_sg =\r\nmmc_alloc_sg(bouncesz / 512, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\n}\r\n}\r\n#endif\r\nif (!mqrq_cur->bounce_buf && !mqrq_prev->bounce_buf) {\r\nblk_queue_bounce_limit(mq->queue, limit);\r\nblk_queue_max_hw_sectors(mq->queue,\r\nmin(host->max_blk_count, host->max_req_size / 512));\r\nblk_queue_max_segments(mq->queue, host->max_segs);\r\nblk_queue_max_segment_size(mq->queue, host->max_seg_size);\r\nmqrq_cur->sg = mmc_alloc_sg(host->max_segs, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\nmqrq_prev->sg = mmc_alloc_sg(host->max_segs, &ret);\r\nif (ret)\r\ngoto cleanup_queue;\r\n}\r\nsema_init(&mq->thread_sem, 1);\r\nmq->thread = kthread_run(mmc_queue_thread, mq, "mmcqd/%d%s",\r\nhost->index, subname ? subname : "");\r\nif (IS_ERR(mq->thread)) {\r\nret = PTR_ERR(mq->thread);\r\ngoto free_bounce_sg;\r\n}\r\nreturn 0;\r\nfree_bounce_sg:\r\nkfree(mqrq_cur->bounce_sg);\r\nmqrq_cur->bounce_sg = NULL;\r\nkfree(mqrq_prev->bounce_sg);\r\nmqrq_prev->bounce_sg = NULL;\r\ncleanup_queue:\r\nkfree(mqrq_cur->sg);\r\nmqrq_cur->sg = NULL;\r\nkfree(mqrq_cur->bounce_buf);\r\nmqrq_cur->bounce_buf = NULL;\r\nkfree(mqrq_prev->sg);\r\nmqrq_prev->sg = NULL;\r\nkfree(mqrq_prev->bounce_buf);\r\nmqrq_prev->bounce_buf = NULL;\r\nblk_cleanup_queue(mq->queue);\r\nreturn ret;\r\n}\r\nvoid mmc_cleanup_queue(struct mmc_queue *mq)\r\n{\r\nstruct request_queue *q = mq->queue;\r\nunsigned long flags;\r\nstruct mmc_queue_req *mqrq_cur = mq->mqrq_cur;\r\nstruct mmc_queue_req *mqrq_prev = mq->mqrq_prev;\r\nmmc_queue_resume(mq);\r\nkthread_stop(mq->thread);\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nq->queuedata = NULL;\r\nblk_start_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\nkfree(mqrq_cur->bounce_sg);\r\nmqrq_cur->bounce_sg = NULL;\r\nkfree(mqrq_cur->sg);\r\nmqrq_cur->sg = NULL;\r\nkfree(mqrq_cur->bounce_buf);\r\nmqrq_cur->bounce_buf = NULL;\r\nkfree(mqrq_prev->bounce_sg);\r\nmqrq_prev->bounce_sg = NULL;\r\nkfree(mqrq_prev->sg);\r\nmqrq_prev->sg = NULL;\r\nkfree(mqrq_prev->bounce_buf);\r\nmqrq_prev->bounce_buf = NULL;\r\nmq->card = NULL;\r\n}\r\nvoid mmc_queue_suspend(struct mmc_queue *mq)\r\n{\r\nstruct request_queue *q = mq->queue;\r\nunsigned long flags;\r\nif (!(mq->flags & MMC_QUEUE_SUSPENDED)) {\r\nmq->flags |= MMC_QUEUE_SUSPENDED;\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_stop_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\ndown(&mq->thread_sem);\r\n}\r\n}\r\nvoid mmc_queue_resume(struct mmc_queue *mq)\r\n{\r\nstruct request_queue *q = mq->queue;\r\nunsigned long flags;\r\nif (mq->flags & MMC_QUEUE_SUSPENDED) {\r\nmq->flags &= ~MMC_QUEUE_SUSPENDED;\r\nup(&mq->thread_sem);\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_start_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\n}\r\n}\r\nunsigned int mmc_queue_map_sg(struct mmc_queue *mq, struct mmc_queue_req *mqrq)\r\n{\r\nunsigned int sg_len;\r\nsize_t buflen;\r\nstruct scatterlist *sg;\r\nint i;\r\nif (!mqrq->bounce_buf)\r\nreturn blk_rq_map_sg(mq->queue, mqrq->req, mqrq->sg);\r\nBUG_ON(!mqrq->bounce_sg);\r\nsg_len = blk_rq_map_sg(mq->queue, mqrq->req, mqrq->bounce_sg);\r\nmqrq->bounce_sg_len = sg_len;\r\nbuflen = 0;\r\nfor_each_sg(mqrq->bounce_sg, sg, sg_len, i)\r\nbuflen += sg->length;\r\nsg_init_one(mqrq->sg, mqrq->bounce_buf, buflen);\r\nreturn 1;\r\n}\r\nvoid mmc_queue_bounce_pre(struct mmc_queue_req *mqrq)\r\n{\r\nif (!mqrq->bounce_buf)\r\nreturn;\r\nif (rq_data_dir(mqrq->req) != WRITE)\r\nreturn;\r\nsg_copy_to_buffer(mqrq->bounce_sg, mqrq->bounce_sg_len,\r\nmqrq->bounce_buf, mqrq->sg[0].length);\r\n}\r\nvoid mmc_queue_bounce_post(struct mmc_queue_req *mqrq)\r\n{\r\nif (!mqrq->bounce_buf)\r\nreturn;\r\nif (rq_data_dir(mqrq->req) != READ)\r\nreturn;\r\nsg_copy_from_buffer(mqrq->bounce_sg, mqrq->bounce_sg_len,\r\nmqrq->bounce_buf, mqrq->sg[0].length);\r\n}
