static int iwch_finish_mem_reg(struct iwch_mr *mhp, u32 stag)\r\n{\r\nu32 mmid;\r\nmhp->attr.state = 1;\r\nmhp->attr.stag = stag;\r\nmmid = stag >> 8;\r\nmhp->ibmr.rkey = mhp->ibmr.lkey = stag;\r\nPDBG("%s mmid 0x%x mhp %p\n", __func__, mmid, mhp);\r\nreturn insert_handle(mhp->rhp, &mhp->rhp->mmidr, mhp, mmid);\r\n}\r\nint iwch_register_mem(struct iwch_dev *rhp, struct iwch_pd *php,\r\nstruct iwch_mr *mhp, int shift)\r\n{\r\nu32 stag;\r\nint ret;\r\nif (cxio_register_phys_mem(&rhp->rdev,\r\n&stag, mhp->attr.pdid,\r\nmhp->attr.perms,\r\nmhp->attr.zbva,\r\nmhp->attr.va_fbo,\r\nmhp->attr.len,\r\nshift - 12,\r\nmhp->attr.pbl_size, mhp->attr.pbl_addr))\r\nreturn -ENOMEM;\r\nret = iwch_finish_mem_reg(mhp, stag);\r\nif (ret)\r\ncxio_dereg_mem(&rhp->rdev, mhp->attr.stag, mhp->attr.pbl_size,\r\nmhp->attr.pbl_addr);\r\nreturn ret;\r\n}\r\nint iwch_reregister_mem(struct iwch_dev *rhp, struct iwch_pd *php,\r\nstruct iwch_mr *mhp,\r\nint shift,\r\nint npages)\r\n{\r\nu32 stag;\r\nint ret;\r\nif (npages > mhp->attr.pbl_size)\r\nreturn -ENOMEM;\r\nstag = mhp->attr.stag;\r\nif (cxio_reregister_phys_mem(&rhp->rdev,\r\n&stag, mhp->attr.pdid,\r\nmhp->attr.perms,\r\nmhp->attr.zbva,\r\nmhp->attr.va_fbo,\r\nmhp->attr.len,\r\nshift - 12,\r\nmhp->attr.pbl_size, mhp->attr.pbl_addr))\r\nreturn -ENOMEM;\r\nret = iwch_finish_mem_reg(mhp, stag);\r\nif (ret)\r\ncxio_dereg_mem(&rhp->rdev, mhp->attr.stag, mhp->attr.pbl_size,\r\nmhp->attr.pbl_addr);\r\nreturn ret;\r\n}\r\nint iwch_alloc_pbl(struct iwch_mr *mhp, int npages)\r\n{\r\nmhp->attr.pbl_addr = cxio_hal_pblpool_alloc(&mhp->rhp->rdev,\r\nnpages << 3);\r\nif (!mhp->attr.pbl_addr)\r\nreturn -ENOMEM;\r\nmhp->attr.pbl_size = npages;\r\nreturn 0;\r\n}\r\nvoid iwch_free_pbl(struct iwch_mr *mhp)\r\n{\r\ncxio_hal_pblpool_free(&mhp->rhp->rdev, mhp->attr.pbl_addr,\r\nmhp->attr.pbl_size << 3);\r\n}\r\nint iwch_write_pbl(struct iwch_mr *mhp, __be64 *pages, int npages, int offset)\r\n{\r\nreturn cxio_write_pbl(&mhp->rhp->rdev, pages,\r\nmhp->attr.pbl_addr + (offset << 3), npages);\r\n}\r\nint build_phys_page_list(struct ib_phys_buf *buffer_list,\r\nint num_phys_buf,\r\nu64 *iova_start,\r\nu64 *total_size,\r\nint *npages,\r\nint *shift,\r\n__be64 **page_list)\r\n{\r\nu64 mask;\r\nint i, j, n;\r\nmask = 0;\r\n*total_size = 0;\r\nfor (i = 0; i < num_phys_buf; ++i) {\r\nif (i != 0 && buffer_list[i].addr & ~PAGE_MASK)\r\nreturn -EINVAL;\r\nif (i != 0 && i != num_phys_buf - 1 &&\r\n(buffer_list[i].size & ~PAGE_MASK))\r\nreturn -EINVAL;\r\n*total_size += buffer_list[i].size;\r\nif (i > 0)\r\nmask |= buffer_list[i].addr;\r\nelse\r\nmask |= buffer_list[i].addr & PAGE_MASK;\r\nif (i != num_phys_buf - 1)\r\nmask |= buffer_list[i].addr + buffer_list[i].size;\r\nelse\r\nmask |= (buffer_list[i].addr + buffer_list[i].size +\r\nPAGE_SIZE - 1) & PAGE_MASK;\r\n}\r\nif (*total_size > 0xFFFFFFFFULL)\r\nreturn -ENOMEM;\r\nfor (*shift = PAGE_SHIFT; *shift < 27; ++(*shift))\r\nif ((1ULL << *shift) & mask)\r\nbreak;\r\nbuffer_list[0].size += buffer_list[0].addr & ((1ULL << *shift) - 1);\r\nbuffer_list[0].addr &= ~0ull << *shift;\r\n*npages = 0;\r\nfor (i = 0; i < num_phys_buf; ++i)\r\n*npages += (buffer_list[i].size +\r\n(1ULL << *shift) - 1) >> *shift;\r\nif (!*npages)\r\nreturn -EINVAL;\r\n*page_list = kmalloc(sizeof(u64) * *npages, GFP_KERNEL);\r\nif (!*page_list)\r\nreturn -ENOMEM;\r\nn = 0;\r\nfor (i = 0; i < num_phys_buf; ++i)\r\nfor (j = 0;\r\nj < (buffer_list[i].size + (1ULL << *shift) - 1) >> *shift;\r\n++j)\r\n(*page_list)[n++] = cpu_to_be64(buffer_list[i].addr +\r\n((u64) j << *shift));\r\nPDBG("%s va 0x%llx mask 0x%llx shift %d len %lld pbl_size %d\n",\r\n__func__, (unsigned long long) *iova_start,\r\n(unsigned long long) mask, *shift, (unsigned long long) *total_size,\r\n*npages);\r\nreturn 0;\r\n}
