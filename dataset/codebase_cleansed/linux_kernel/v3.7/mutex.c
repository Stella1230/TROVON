void\r\n__mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)\r\n{\r\natomic_set(&lock->count, 1);\r\nspin_lock_init(&lock->wait_lock);\r\nINIT_LIST_HEAD(&lock->wait_list);\r\nmutex_clear_owner(lock);\r\ndebug_mutex_init(lock, name, key);\r\n}\r\nvoid __sched mutex_lock(struct mutex *lock)\r\n{\r\nmight_sleep();\r\n__mutex_fastpath_lock(&lock->count, __mutex_lock_slowpath);\r\nmutex_set_owner(lock);\r\n}\r\nvoid __sched mutex_unlock(struct mutex *lock)\r\n{\r\n#ifndef CONFIG_DEBUG_MUTEXES\r\nmutex_clear_owner(lock);\r\n#endif\r\n__mutex_fastpath_unlock(&lock->count, __mutex_unlock_slowpath);\r\n}\r\nstatic inline int __sched\r\n__mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,\r\nstruct lockdep_map *nest_lock, unsigned long ip)\r\n{\r\nstruct task_struct *task = current;\r\nstruct mutex_waiter waiter;\r\nunsigned long flags;\r\npreempt_disable();\r\nmutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);\r\n#ifdef CONFIG_MUTEX_SPIN_ON_OWNER\r\nfor (;;) {\r\nstruct task_struct *owner;\r\nowner = ACCESS_ONCE(lock->owner);\r\nif (owner && !mutex_spin_on_owner(lock, owner))\r\nbreak;\r\nif (atomic_cmpxchg(&lock->count, 1, 0) == 1) {\r\nlock_acquired(&lock->dep_map, ip);\r\nmutex_set_owner(lock);\r\npreempt_enable();\r\nreturn 0;\r\n}\r\nif (!owner && (need_resched() || rt_task(task)))\r\nbreak;\r\narch_mutex_cpu_relax();\r\n}\r\n#endif\r\nspin_lock_mutex(&lock->wait_lock, flags);\r\ndebug_mutex_lock_common(lock, &waiter);\r\ndebug_mutex_add_waiter(lock, &waiter, task_thread_info(task));\r\nlist_add_tail(&waiter.list, &lock->wait_list);\r\nwaiter.task = task;\r\nif (atomic_xchg(&lock->count, -1) == 1)\r\ngoto done;\r\nlock_contended(&lock->dep_map, ip);\r\nfor (;;) {\r\nif (atomic_xchg(&lock->count, -1) == 1)\r\nbreak;\r\nif (unlikely(signal_pending_state(state, task))) {\r\nmutex_remove_waiter(lock, &waiter,\r\ntask_thread_info(task));\r\nmutex_release(&lock->dep_map, 1, ip);\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\ndebug_mutex_free_waiter(&waiter);\r\npreempt_enable();\r\nreturn -EINTR;\r\n}\r\n__set_task_state(task, state);\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\nschedule_preempt_disabled();\r\nspin_lock_mutex(&lock->wait_lock, flags);\r\n}\r\ndone:\r\nlock_acquired(&lock->dep_map, ip);\r\nmutex_remove_waiter(lock, &waiter, current_thread_info());\r\nmutex_set_owner(lock);\r\nif (likely(list_empty(&lock->wait_list)))\r\natomic_set(&lock->count, 0);\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\ndebug_mutex_free_waiter(&waiter);\r\npreempt_enable();\r\nreturn 0;\r\n}\r\nvoid __sched\r\nmutex_lock_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\nmight_sleep();\r\n__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, subclass, NULL, _RET_IP_);\r\n}\r\nvoid __sched\r\n_mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest)\r\n{\r\nmight_sleep();\r\n__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, nest, _RET_IP_);\r\n}\r\nint __sched\r\nmutex_lock_killable_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\nmight_sleep();\r\nreturn __mutex_lock_common(lock, TASK_KILLABLE, subclass, NULL, _RET_IP_);\r\n}\r\nint __sched\r\nmutex_lock_interruptible_nested(struct mutex *lock, unsigned int subclass)\r\n{\r\nmight_sleep();\r\nreturn __mutex_lock_common(lock, TASK_INTERRUPTIBLE,\r\nsubclass, NULL, _RET_IP_);\r\n}\r\nstatic inline void\r\n__mutex_unlock_common_slowpath(atomic_t *lock_count, int nested)\r\n{\r\nstruct mutex *lock = container_of(lock_count, struct mutex, count);\r\nunsigned long flags;\r\nspin_lock_mutex(&lock->wait_lock, flags);\r\nmutex_release(&lock->dep_map, nested, _RET_IP_);\r\ndebug_mutex_unlock(lock);\r\nif (__mutex_slowpath_needs_to_unlock())\r\natomic_set(&lock->count, 1);\r\nif (!list_empty(&lock->wait_list)) {\r\nstruct mutex_waiter *waiter =\r\nlist_entry(lock->wait_list.next,\r\nstruct mutex_waiter, list);\r\ndebug_mutex_wake_waiter(lock, waiter);\r\nwake_up_process(waiter->task);\r\n}\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\n}\r\nstatic __used noinline void\r\n__mutex_unlock_slowpath(atomic_t *lock_count)\r\n{\r\n__mutex_unlock_common_slowpath(lock_count, 1);\r\n}\r\nint __sched mutex_lock_interruptible(struct mutex *lock)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __mutex_fastpath_lock_retval\r\n(&lock->count, __mutex_lock_interruptible_slowpath);\r\nif (!ret)\r\nmutex_set_owner(lock);\r\nreturn ret;\r\n}\r\nint __sched mutex_lock_killable(struct mutex *lock)\r\n{\r\nint ret;\r\nmight_sleep();\r\nret = __mutex_fastpath_lock_retval\r\n(&lock->count, __mutex_lock_killable_slowpath);\r\nif (!ret)\r\nmutex_set_owner(lock);\r\nreturn ret;\r\n}\r\nstatic __used noinline void __sched\r\n__mutex_lock_slowpath(atomic_t *lock_count)\r\n{\r\nstruct mutex *lock = container_of(lock_count, struct mutex, count);\r\n__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);\r\n}\r\nstatic noinline int __sched\r\n__mutex_lock_killable_slowpath(atomic_t *lock_count)\r\n{\r\nstruct mutex *lock = container_of(lock_count, struct mutex, count);\r\nreturn __mutex_lock_common(lock, TASK_KILLABLE, 0, NULL, _RET_IP_);\r\n}\r\nstatic noinline int __sched\r\n__mutex_lock_interruptible_slowpath(atomic_t *lock_count)\r\n{\r\nstruct mutex *lock = container_of(lock_count, struct mutex, count);\r\nreturn __mutex_lock_common(lock, TASK_INTERRUPTIBLE, 0, NULL, _RET_IP_);\r\n}\r\nstatic inline int __mutex_trylock_slowpath(atomic_t *lock_count)\r\n{\r\nstruct mutex *lock = container_of(lock_count, struct mutex, count);\r\nunsigned long flags;\r\nint prev;\r\nspin_lock_mutex(&lock->wait_lock, flags);\r\nprev = atomic_xchg(&lock->count, -1);\r\nif (likely(prev == 1)) {\r\nmutex_set_owner(lock);\r\nmutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);\r\n}\r\nif (likely(list_empty(&lock->wait_list)))\r\natomic_set(&lock->count, 0);\r\nspin_unlock_mutex(&lock->wait_lock, flags);\r\nreturn prev == 1;\r\n}\r\nint __sched mutex_trylock(struct mutex *lock)\r\n{\r\nint ret;\r\nret = __mutex_fastpath_trylock(&lock->count, __mutex_trylock_slowpath);\r\nif (ret)\r\nmutex_set_owner(lock);\r\nreturn ret;\r\n}\r\nint atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock)\r\n{\r\nif (atomic_add_unless(cnt, -1, 1))\r\nreturn 0;\r\nmutex_lock(lock);\r\nif (!atomic_dec_and_test(cnt)) {\r\nmutex_unlock(lock);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}
