static unsigned long __init uv_early_read_mmr(unsigned long addr)\r\n{\r\nunsigned long val, *mmr;\r\nmmr = early_ioremap(UV_LOCAL_MMR_BASE | addr, sizeof(*mmr));\r\nval = *mmr;\r\nearly_iounmap(mmr, sizeof(*mmr));\r\nreturn val;\r\n}\r\nstatic inline bool is_GRU_range(u64 start, u64 end)\r\n{\r\nreturn start >= gru_start_paddr && end <= gru_end_paddr;\r\n}\r\nstatic bool uv_is_untracked_pat_range(u64 start, u64 end)\r\n{\r\nreturn is_ISA_range(start, end) || is_GRU_range(start, end);\r\n}\r\nstatic int __init early_get_pnodeid(void)\r\n{\r\nunion uvh_node_id_u node_id;\r\nunion uvh_rh_gam_config_mmr_u m_n_config;\r\nint pnode;\r\nnode_id.v = uv_early_read_mmr(UVH_NODE_ID);\r\nm_n_config.v = uv_early_read_mmr(UVH_RH_GAM_CONFIG_MMR);\r\nuv_min_hub_revision_id = node_id.s.revision;\r\nif (node_id.s.part_number == UV2_HUB_PART_NUMBER)\r\nuv_min_hub_revision_id += UV2_HUB_REVISION_BASE - 1;\r\nif (node_id.s.part_number == UV2_HUB_PART_NUMBER_X)\r\nuv_min_hub_revision_id += UV2_HUB_REVISION_BASE - 1;\r\nuv_hub_info->hub_revision = uv_min_hub_revision_id;\r\npnode = (node_id.s.node_id >> 1) & ((1 << m_n_config.s.n_skt) - 1);\r\nreturn pnode;\r\n}\r\nstatic void __init early_get_apic_pnode_shift(void)\r\n{\r\nuvh_apicid.v = uv_early_read_mmr(UVH_APICID);\r\nif (!uvh_apicid.v)\r\nuvh_apicid.s.pnode_shift = UV_APIC_PNODE_SHIFT;\r\n}\r\nstatic void __init uv_set_apicid_hibit(void)\r\n{\r\nunion uv1h_lb_target_physical_apic_id_mask_u apicid_mask;\r\nif (is_uv1_hub()) {\r\napicid_mask.v =\r\nuv_early_read_mmr(UV1H_LB_TARGET_PHYSICAL_APIC_ID_MASK);\r\nuv_apicid_hibits =\r\napicid_mask.s1.bit_enables & UV_APICID_HIBIT_MASK;\r\n}\r\n}\r\nstatic int __init uv_acpi_madt_oem_check(char *oem_id, char *oem_table_id)\r\n{\r\nint pnodeid, is_uv1, is_uv2;\r\nis_uv1 = !strcmp(oem_id, "SGI");\r\nis_uv2 = !strcmp(oem_id, "SGI2");\r\nif (is_uv1 || is_uv2) {\r\nuv_hub_info->hub_revision =\r\nis_uv1 ? UV1_HUB_REVISION_BASE : UV2_HUB_REVISION_BASE;\r\npnodeid = early_get_pnodeid();\r\nearly_get_apic_pnode_shift();\r\nx86_platform.is_untracked_pat_range = uv_is_untracked_pat_range;\r\nx86_platform.nmi_init = uv_nmi_init;\r\nif (!strcmp(oem_table_id, "UVL"))\r\nuv_system_type = UV_LEGACY_APIC;\r\nelse if (!strcmp(oem_table_id, "UVX"))\r\nuv_system_type = UV_X2APIC;\r\nelse if (!strcmp(oem_table_id, "UVH")) {\r\n__this_cpu_write(x2apic_extra_bits,\r\npnodeid << uvh_apicid.s.pnode_shift);\r\nuv_system_type = UV_NON_UNIQUE_APIC;\r\nuv_set_apicid_hibit();\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nenum uv_system_type get_uv_system_type(void)\r\n{\r\nreturn uv_system_type;\r\n}\r\nint is_uv_system(void)\r\n{\r\nreturn uv_system_type != UV_NONE;\r\n}\r\nstatic int __cpuinit uv_wakeup_secondary(int phys_apicid, unsigned long start_rip)\r\n{\r\n#ifdef CONFIG_SMP\r\nunsigned long val;\r\nint pnode;\r\npnode = uv_apicid_to_pnode(phys_apicid);\r\nphys_apicid |= uv_apicid_hibits;\r\nval = (1UL << UVH_IPI_INT_SEND_SHFT) |\r\n(phys_apicid << UVH_IPI_INT_APIC_ID_SHFT) |\r\n((start_rip << UVH_IPI_INT_VECTOR_SHFT) >> 12) |\r\nAPIC_DM_INIT;\r\nuv_write_global_mmr64(pnode, UVH_IPI_INT, val);\r\nval = (1UL << UVH_IPI_INT_SEND_SHFT) |\r\n(phys_apicid << UVH_IPI_INT_APIC_ID_SHFT) |\r\n((start_rip << UVH_IPI_INT_VECTOR_SHFT) >> 12) |\r\nAPIC_DM_STARTUP;\r\nuv_write_global_mmr64(pnode, UVH_IPI_INT, val);\r\natomic_set(&init_deasserted, 1);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void uv_send_IPI_one(int cpu, int vector)\r\n{\r\nunsigned long apicid;\r\nint pnode;\r\napicid = per_cpu(x86_cpu_to_apicid, cpu);\r\npnode = uv_apicid_to_pnode(apicid);\r\nuv_hub_send_ipi(pnode, apicid, vector);\r\n}\r\nstatic void uv_send_IPI_mask(const struct cpumask *mask, int vector)\r\n{\r\nunsigned int cpu;\r\nfor_each_cpu(cpu, mask)\r\nuv_send_IPI_one(cpu, vector);\r\n}\r\nstatic void uv_send_IPI_mask_allbutself(const struct cpumask *mask, int vector)\r\n{\r\nunsigned int this_cpu = smp_processor_id();\r\nunsigned int cpu;\r\nfor_each_cpu(cpu, mask) {\r\nif (cpu != this_cpu)\r\nuv_send_IPI_one(cpu, vector);\r\n}\r\n}\r\nstatic void uv_send_IPI_allbutself(int vector)\r\n{\r\nunsigned int this_cpu = smp_processor_id();\r\nunsigned int cpu;\r\nfor_each_online_cpu(cpu) {\r\nif (cpu != this_cpu)\r\nuv_send_IPI_one(cpu, vector);\r\n}\r\n}\r\nstatic void uv_send_IPI_all(int vector)\r\n{\r\nuv_send_IPI_mask(cpu_online_mask, vector);\r\n}\r\nstatic int uv_apic_id_valid(int apicid)\r\n{\r\nreturn 1;\r\n}\r\nstatic int uv_apic_id_registered(void)\r\n{\r\nreturn 1;\r\n}\r\nstatic void uv_init_apic_ldr(void)\r\n{\r\n}\r\nstatic int\r\nuv_cpu_mask_to_apicid_and(const struct cpumask *cpumask,\r\nconst struct cpumask *andmask,\r\nunsigned int *apicid)\r\n{\r\nint unsigned cpu;\r\nfor_each_cpu_and(cpu, cpumask, andmask) {\r\nif (cpumask_test_cpu(cpu, cpu_online_mask))\r\nbreak;\r\n}\r\nif (likely(cpu < nr_cpu_ids)) {\r\n*apicid = per_cpu(x86_cpu_to_apicid, cpu) | uv_apicid_hibits;\r\nreturn 0;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic unsigned int x2apic_get_apic_id(unsigned long x)\r\n{\r\nunsigned int id;\r\nWARN_ON(preemptible() && num_online_cpus() > 1);\r\nid = x | __this_cpu_read(x2apic_extra_bits);\r\nreturn id;\r\n}\r\nstatic unsigned long set_apic_id(unsigned int id)\r\n{\r\nunsigned long x;\r\nx = id;\r\nreturn x;\r\n}\r\nstatic unsigned int uv_read_apic_id(void)\r\n{\r\nreturn x2apic_get_apic_id(apic_read(APIC_ID));\r\n}\r\nstatic int uv_phys_pkg_id(int initial_apicid, int index_msb)\r\n{\r\nreturn uv_read_apic_id() >> index_msb;\r\n}\r\nstatic void uv_send_IPI_self(int vector)\r\n{\r\napic_write(APIC_SELF_IPI, vector);\r\n}\r\nstatic int uv_probe(void)\r\n{\r\nreturn apic == &apic_x2apic_uv_x;\r\n}\r\nstatic __cpuinit void set_x2apic_extra_bits(int pnode)\r\n{\r\n__this_cpu_write(x2apic_extra_bits, pnode << uvh_apicid.s.pnode_shift);\r\n}\r\nstatic __init int boot_pnode_to_blade(int pnode)\r\n{\r\nint blade;\r\nfor (blade = 0; blade < uv_num_possible_blades(); blade++)\r\nif (pnode == uv_blade_info[blade].pnode)\r\nreturn blade;\r\nBUG();\r\n}\r\nstatic __init void get_lowmem_redirect(unsigned long *base, unsigned long *size)\r\n{\r\nunion uvh_rh_gam_alias210_overlay_config_2_mmr_u alias;\r\nunion uvh_rh_gam_alias210_redirect_config_2_mmr_u redirect;\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(redir_addrs); i++) {\r\nalias.v = uv_read_local_mmr(redir_addrs[i].alias);\r\nif (alias.s.enable && alias.s.base == 0) {\r\n*size = (1UL << alias.s.m_alias);\r\nredirect.v = uv_read_local_mmr(redir_addrs[i].redirect);\r\n*base = (unsigned long)redirect.s.dest_base << DEST_SHIFT;\r\nreturn;\r\n}\r\n}\r\n*base = *size = 0;\r\n}\r\nstatic __init void map_high(char *id, unsigned long base, int pshift,\r\nint bshift, int max_pnode, enum map_type map_type)\r\n{\r\nunsigned long bytes, paddr;\r\npaddr = base << pshift;\r\nbytes = (1UL << bshift) * (max_pnode + 1);\r\nprintk(KERN_INFO "UV: Map %s_HI 0x%lx - 0x%lx\n", id, paddr,\r\npaddr + bytes);\r\nif (map_type == map_uc)\r\ninit_extra_mapping_uc(paddr, bytes);\r\nelse\r\ninit_extra_mapping_wb(paddr, bytes);\r\n}\r\nstatic __init void map_gru_high(int max_pnode)\r\n{\r\nunion uvh_rh_gam_gru_overlay_config_mmr_u gru;\r\nint shift = UVH_RH_GAM_GRU_OVERLAY_CONFIG_MMR_BASE_SHFT;\r\ngru.v = uv_read_local_mmr(UVH_RH_GAM_GRU_OVERLAY_CONFIG_MMR);\r\nif (gru.s.enable) {\r\nmap_high("GRU", gru.s.base, shift, shift, max_pnode, map_wb);\r\ngru_start_paddr = ((u64)gru.s.base << shift);\r\ngru_end_paddr = gru_start_paddr + (1UL << shift) * (max_pnode + 1);\r\n}\r\n}\r\nstatic __init void map_mmr_high(int max_pnode)\r\n{\r\nunion uvh_rh_gam_mmr_overlay_config_mmr_u mmr;\r\nint shift = UVH_RH_GAM_MMR_OVERLAY_CONFIG_MMR_BASE_SHFT;\r\nmmr.v = uv_read_local_mmr(UVH_RH_GAM_MMR_OVERLAY_CONFIG_MMR);\r\nif (mmr.s.enable)\r\nmap_high("MMR", mmr.s.base, shift, shift, max_pnode, map_uc);\r\n}\r\nstatic __init void map_mmioh_high(int max_pnode)\r\n{\r\nunion uvh_rh_gam_mmioh_overlay_config_mmr_u mmioh;\r\nint shift;\r\nmmioh.v = uv_read_local_mmr(UVH_RH_GAM_MMIOH_OVERLAY_CONFIG_MMR);\r\nif (is_uv1_hub() && mmioh.s1.enable) {\r\nshift = UV1H_RH_GAM_MMIOH_OVERLAY_CONFIG_MMR_BASE_SHFT;\r\nmap_high("MMIOH", mmioh.s1.base, shift, mmioh.s1.m_io,\r\nmax_pnode, map_uc);\r\n}\r\nif (is_uv2_hub() && mmioh.s2.enable) {\r\nshift = UV2H_RH_GAM_MMIOH_OVERLAY_CONFIG_MMR_BASE_SHFT;\r\nmap_high("MMIOH", mmioh.s2.base, shift, mmioh.s2.m_io,\r\nmax_pnode, map_uc);\r\n}\r\n}\r\nstatic __init void map_low_mmrs(void)\r\n{\r\ninit_extra_mapping_uc(UV_GLOBAL_MMR32_BASE, UV_GLOBAL_MMR32_SIZE);\r\ninit_extra_mapping_uc(UV_LOCAL_MMR_BASE, UV_LOCAL_MMR_SIZE);\r\n}\r\nstatic __init void uv_rtc_init(void)\r\n{\r\nlong status;\r\nu64 ticks_per_sec;\r\nstatus = uv_bios_freq_base(BIOS_FREQ_BASE_REALTIME_CLOCK,\r\n&ticks_per_sec);\r\nif (status != BIOS_STATUS_SUCCESS || ticks_per_sec < 100000) {\r\nprintk(KERN_WARNING\r\n"unable to determine platform RTC clock frequency, "\r\n"guessing.\n");\r\nsn_rtc_cycles_per_second = 1000000000000UL / 30000UL;\r\n} else\r\nsn_rtc_cycles_per_second = ticks_per_sec;\r\n}\r\nstatic void uv_heartbeat(unsigned long ignored)\r\n{\r\nstruct timer_list *timer = &uv_hub_info->scir.timer;\r\nunsigned char bits = uv_hub_info->scir.state;\r\nbits ^= SCIR_CPU_HEARTBEAT;\r\nif (idle_cpu(raw_smp_processor_id()))\r\nbits &= ~SCIR_CPU_ACTIVITY;\r\nelse\r\nbits |= SCIR_CPU_ACTIVITY;\r\nuv_set_scir_bits(bits);\r\nmod_timer_pinned(timer, jiffies + SCIR_CPU_HB_INTERVAL);\r\n}\r\nstatic void __cpuinit uv_heartbeat_enable(int cpu)\r\n{\r\nwhile (!uv_cpu_hub_info(cpu)->scir.enabled) {\r\nstruct timer_list *timer = &uv_cpu_hub_info(cpu)->scir.timer;\r\nuv_set_cpu_scir_bits(cpu, SCIR_CPU_HEARTBEAT|SCIR_CPU_ACTIVITY);\r\nsetup_timer(timer, uv_heartbeat, cpu);\r\ntimer->expires = jiffies + SCIR_CPU_HB_INTERVAL;\r\nadd_timer_on(timer, cpu);\r\nuv_cpu_hub_info(cpu)->scir.enabled = 1;\r\ncpu = 0;\r\n}\r\n}\r\nstatic void __cpuinit uv_heartbeat_disable(int cpu)\r\n{\r\nif (uv_cpu_hub_info(cpu)->scir.enabled) {\r\nuv_cpu_hub_info(cpu)->scir.enabled = 0;\r\ndel_timer(&uv_cpu_hub_info(cpu)->scir.timer);\r\n}\r\nuv_set_cpu_scir_bits(cpu, 0xff);\r\n}\r\nstatic __cpuinit int uv_scir_cpu_notify(struct notifier_block *self,\r\nunsigned long action, void *hcpu)\r\n{\r\nlong cpu = (long)hcpu;\r\nswitch (action) {\r\ncase CPU_ONLINE:\r\nuv_heartbeat_enable(cpu);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\nuv_heartbeat_disable(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic __init void uv_scir_register_cpu_notifier(void)\r\n{\r\nhotcpu_notifier(uv_scir_cpu_notify, 0);\r\n}\r\nstatic __init void uv_scir_register_cpu_notifier(void)\r\n{\r\n}\r\nstatic __init int uv_init_heartbeat(void)\r\n{\r\nint cpu;\r\nif (is_uv_system())\r\nfor_each_online_cpu(cpu)\r\nuv_heartbeat_enable(cpu);\r\nreturn 0;\r\n}\r\nint uv_set_vga_state(struct pci_dev *pdev, bool decode,\r\nunsigned int command_bits, u32 flags)\r\n{\r\nint domain, bus, rc;\r\nPR_DEVEL("devfn %x decode %d cmd %x flags %d\n",\r\npdev->devfn, decode, command_bits, flags);\r\nif (!(flags & PCI_VGA_STATE_CHANGE_BRIDGE))\r\nreturn 0;\r\nif ((command_bits & PCI_COMMAND_IO) == 0)\r\nreturn 0;\r\ndomain = pci_domain_nr(pdev->bus);\r\nbus = pdev->bus->number;\r\nrc = uv_bios_set_legacy_vga_target(decode, domain, bus);\r\nPR_DEVEL("vga decode %d %x:%x, rc: %d\n", decode, domain, bus, rc);\r\nreturn rc;\r\n}\r\nvoid __cpuinit uv_cpu_init(void)\r\n{\r\nif (!uv_blade_info)\r\nreturn;\r\nuv_blade_info[uv_numa_blade_id()].nr_online_cpus++;\r\nif (get_uv_system_type() == UV_NON_UNIQUE_APIC)\r\nset_x2apic_extra_bits(uv_hub_info->pnode);\r\n}\r\nint uv_handle_nmi(unsigned int reason, struct pt_regs *regs)\r\n{\r\nunsigned long real_uv_nmi;\r\nint bid;\r\nbid = uv_numa_blade_id();\r\nreal_uv_nmi = (uv_read_local_mmr(UVH_NMI_MMR) & UV_NMI_PENDING_MASK);\r\nif (unlikely(real_uv_nmi)) {\r\nspin_lock(&uv_blade_info[bid].nmi_lock);\r\nreal_uv_nmi = (uv_read_local_mmr(UVH_NMI_MMR) & UV_NMI_PENDING_MASK);\r\nif (real_uv_nmi) {\r\nuv_blade_info[bid].nmi_count++;\r\nuv_write_local_mmr(UVH_NMI_MMR_CLEAR, UV_NMI_PENDING_MASK);\r\n}\r\nspin_unlock(&uv_blade_info[bid].nmi_lock);\r\n}\r\nif (likely(__get_cpu_var(cpu_last_nmi_count) == uv_blade_info[bid].nmi_count))\r\nreturn NMI_DONE;\r\n__get_cpu_var(cpu_last_nmi_count) = uv_blade_info[bid].nmi_count;\r\nspin_lock(&uv_nmi_lock);\r\npr_info("UV NMI stack dump cpu %u:\n", smp_processor_id());\r\ndump_stack();\r\nspin_unlock(&uv_nmi_lock);\r\nreturn NMI_HANDLED;\r\n}\r\nvoid uv_register_nmi_notifier(void)\r\n{\r\nif (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))\r\nprintk(KERN_WARNING "UV NMI handler failed to register\n");\r\n}\r\nvoid uv_nmi_init(void)\r\n{\r\nunsigned int value;\r\nvalue = apic_read(APIC_LVT1) | APIC_DM_NMI;\r\nvalue &= ~APIC_LVT_MASKED;\r\napic_write(APIC_LVT1, value);\r\n}\r\nvoid __init uv_system_init(void)\r\n{\r\nunion uvh_rh_gam_config_mmr_u m_n_config;\r\nunion uvh_rh_gam_mmioh_overlay_config_mmr_u mmioh;\r\nunion uvh_node_id_u node_id;\r\nunsigned long gnode_upper, lowmem_redir_base, lowmem_redir_size;\r\nint bytes, nid, cpu, lcpu, pnode, blade, i, j, m_val, n_val, n_io;\r\nint gnode_extra, max_pnode = 0;\r\nunsigned long mmr_base, present, paddr;\r\nunsigned short pnode_mask, pnode_io_mask;\r\nprintk(KERN_INFO "UV: Found %s hub\n", is_uv1_hub() ? "UV1" : "UV2");\r\nmap_low_mmrs();\r\nm_n_config.v = uv_read_local_mmr(UVH_RH_GAM_CONFIG_MMR );\r\nm_val = m_n_config.s.m_skt;\r\nn_val = m_n_config.s.n_skt;\r\nmmioh.v = uv_read_local_mmr(UVH_RH_GAM_MMIOH_OVERLAY_CONFIG_MMR);\r\nn_io = is_uv1_hub() ? mmioh.s1.n_io : mmioh.s2.n_io;\r\nmmr_base =\r\nuv_read_local_mmr(UVH_RH_GAM_MMR_OVERLAY_CONFIG_MMR) &\r\n~UV_MMR_ENABLE;\r\npnode_mask = (1 << n_val) - 1;\r\npnode_io_mask = (1 << n_io) - 1;\r\nnode_id.v = uv_read_local_mmr(UVH_NODE_ID);\r\ngnode_extra = (node_id.s.node_id & ~((1 << n_val) - 1)) >> 1;\r\ngnode_upper = ((unsigned long)gnode_extra << m_val);\r\nprintk(KERN_INFO "UV: N %d, M %d, N_IO: %d, gnode_upper 0x%lx, gnode_extra 0x%x, pnode_mask 0x%x, pnode_io_mask 0x%x\n",\r\nn_val, m_val, n_io, gnode_upper, gnode_extra, pnode_mask, pnode_io_mask);\r\nprintk(KERN_DEBUG "UV: global MMR base 0x%lx\n", mmr_base);\r\nfor(i = 0; i < UVH_NODE_PRESENT_TABLE_DEPTH; i++)\r\nuv_possible_blades +=\r\nhweight64(uv_read_local_mmr( UVH_NODE_PRESENT_TABLE + i * 8));\r\nprintk(KERN_INFO "UV: Found %d blades, %d hubs\n",\r\nis_uv1_hub() ? uv_num_possible_blades() :\r\n(uv_num_possible_blades() + 1) / 2,\r\nuv_num_possible_blades());\r\nbytes = sizeof(struct uv_blade_info) * uv_num_possible_blades();\r\nuv_blade_info = kzalloc(bytes, GFP_KERNEL);\r\nBUG_ON(!uv_blade_info);\r\nfor (blade = 0; blade < uv_num_possible_blades(); blade++)\r\nuv_blade_info[blade].memory_nid = -1;\r\nget_lowmem_redirect(&lowmem_redir_base, &lowmem_redir_size);\r\nbytes = sizeof(uv_node_to_blade[0]) * num_possible_nodes();\r\nuv_node_to_blade = kmalloc(bytes, GFP_KERNEL);\r\nBUG_ON(!uv_node_to_blade);\r\nmemset(uv_node_to_blade, 255, bytes);\r\nbytes = sizeof(uv_cpu_to_blade[0]) * num_possible_cpus();\r\nuv_cpu_to_blade = kmalloc(bytes, GFP_KERNEL);\r\nBUG_ON(!uv_cpu_to_blade);\r\nmemset(uv_cpu_to_blade, 255, bytes);\r\nblade = 0;\r\nfor (i = 0; i < UVH_NODE_PRESENT_TABLE_DEPTH; i++) {\r\npresent = uv_read_local_mmr(UVH_NODE_PRESENT_TABLE + i * 8);\r\nfor (j = 0; j < 64; j++) {\r\nif (!test_bit(j, &present))\r\ncontinue;\r\npnode = (i * 64 + j) & pnode_mask;\r\nuv_blade_info[blade].pnode = pnode;\r\nuv_blade_info[blade].nr_possible_cpus = 0;\r\nuv_blade_info[blade].nr_online_cpus = 0;\r\nspin_lock_init(&uv_blade_info[blade].nmi_lock);\r\nmax_pnode = max(pnode, max_pnode);\r\nblade++;\r\n}\r\n}\r\nuv_bios_init();\r\nuv_bios_get_sn_info(0, &uv_type, &sn_partition_id, &sn_coherency_id,\r\n&sn_region_size, &system_serial_number);\r\nuv_rtc_init();\r\nfor_each_present_cpu(cpu) {\r\nint apicid = per_cpu(x86_cpu_to_apicid, cpu);\r\nnid = cpu_to_node(cpu);\r\nuv_cpu_hub_info(cpu)->pnode_mask = pnode_mask;\r\nuv_cpu_hub_info(cpu)->apic_pnode_shift = uvh_apicid.s.pnode_shift;\r\nuv_cpu_hub_info(cpu)->hub_revision = uv_hub_info->hub_revision;\r\nuv_cpu_hub_info(cpu)->m_shift = 64 - m_val;\r\nuv_cpu_hub_info(cpu)->n_lshift = is_uv2_1_hub() ?\r\n(m_val == 40 ? 40 : 39) : m_val;\r\npnode = uv_apicid_to_pnode(apicid);\r\nblade = boot_pnode_to_blade(pnode);\r\nlcpu = uv_blade_info[blade].nr_possible_cpus;\r\nuv_blade_info[blade].nr_possible_cpus++;\r\nuv_blade_info[blade].memory_nid = nid;\r\nuv_cpu_hub_info(cpu)->lowmem_remap_base = lowmem_redir_base;\r\nuv_cpu_hub_info(cpu)->lowmem_remap_top = lowmem_redir_size;\r\nuv_cpu_hub_info(cpu)->m_val = m_val;\r\nuv_cpu_hub_info(cpu)->n_val = n_val;\r\nuv_cpu_hub_info(cpu)->numa_blade_id = blade;\r\nuv_cpu_hub_info(cpu)->blade_processor_id = lcpu;\r\nuv_cpu_hub_info(cpu)->pnode = pnode;\r\nuv_cpu_hub_info(cpu)->gpa_mask = (1UL << (m_val + n_val)) - 1;\r\nuv_cpu_hub_info(cpu)->gnode_upper = gnode_upper;\r\nuv_cpu_hub_info(cpu)->gnode_extra = gnode_extra;\r\nuv_cpu_hub_info(cpu)->global_mmr_base = mmr_base;\r\nuv_cpu_hub_info(cpu)->coherency_domain_number = sn_coherency_id;\r\nuv_cpu_hub_info(cpu)->scir.offset = uv_scir_offset(apicid);\r\nuv_node_to_blade[nid] = blade;\r\nuv_cpu_to_blade[cpu] = blade;\r\n}\r\nfor_each_online_node(nid) {\r\nif (uv_node_to_blade[nid] >= 0)\r\ncontinue;\r\npaddr = node_start_pfn(nid) << PAGE_SHIFT;\r\npnode = uv_gpa_to_pnode(uv_soc_phys_ram_to_gpa(paddr));\r\nblade = boot_pnode_to_blade(pnode);\r\nuv_node_to_blade[nid] = blade;\r\n}\r\nmap_gru_high(max_pnode);\r\nmap_mmr_high(max_pnode);\r\nmap_mmioh_high(max_pnode & pnode_io_mask);\r\nuv_cpu_init();\r\nuv_scir_register_cpu_notifier();\r\nuv_register_nmi_notifier();\r\nproc_mkdir("sgi_uv", NULL);\r\npci_register_set_vga_state(uv_set_vga_state);\r\nif (is_kdump_kernel())\r\nreboot_type = BOOT_ACPI;\r\n}
