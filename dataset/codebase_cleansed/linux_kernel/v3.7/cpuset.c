static inline struct cpuset *cgroup_cs(struct cgroup *cont)\r\n{\r\nreturn container_of(cgroup_subsys_state(cont, cpuset_subsys_id),\r\nstruct cpuset, css);\r\n}\r\nstatic inline struct cpuset *task_cs(struct task_struct *task)\r\n{\r\nreturn container_of(task_subsys_state(task, cpuset_subsys_id),\r\nstruct cpuset, css);\r\n}\r\nstatic inline bool task_has_mempolicy(struct task_struct *task)\r\n{\r\nreturn task->mempolicy;\r\n}\r\nstatic inline bool task_has_mempolicy(struct task_struct *task)\r\n{\r\nreturn false;\r\n}\r\nstatic inline int is_cpu_exclusive(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_CPU_EXCLUSIVE, &cs->flags);\r\n}\r\nstatic inline int is_mem_exclusive(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_MEM_EXCLUSIVE, &cs->flags);\r\n}\r\nstatic inline int is_mem_hardwall(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_MEM_HARDWALL, &cs->flags);\r\n}\r\nstatic inline int is_sched_load_balance(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\r\n}\r\nstatic inline int is_memory_migrate(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_MEMORY_MIGRATE, &cs->flags);\r\n}\r\nstatic inline int is_spread_page(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_SPREAD_PAGE, &cs->flags);\r\n}\r\nstatic inline int is_spread_slab(const struct cpuset *cs)\r\n{\r\nreturn test_bit(CS_SPREAD_SLAB, &cs->flags);\r\n}\r\nstatic struct dentry *cpuset_mount(struct file_system_type *fs_type,\r\nint flags, const char *unused_dev_name, void *data)\r\n{\r\nstruct file_system_type *cgroup_fs = get_fs_type("cgroup");\r\nstruct dentry *ret = ERR_PTR(-ENODEV);\r\nif (cgroup_fs) {\r\nchar mountopts[] =\r\n"cpuset,noprefix,"\r\n"release_agent=/sbin/cpuset_release_agent";\r\nret = cgroup_fs->mount(cgroup_fs, flags,\r\nunused_dev_name, mountopts);\r\nput_filesystem(cgroup_fs);\r\n}\r\nreturn ret;\r\n}\r\nstatic void guarantee_online_cpus(const struct cpuset *cs,\r\nstruct cpumask *pmask)\r\n{\r\nwhile (cs && !cpumask_intersects(cs->cpus_allowed, cpu_online_mask))\r\ncs = cs->parent;\r\nif (cs)\r\ncpumask_and(pmask, cs->cpus_allowed, cpu_online_mask);\r\nelse\r\ncpumask_copy(pmask, cpu_online_mask);\r\nBUG_ON(!cpumask_intersects(pmask, cpu_online_mask));\r\n}\r\nstatic void guarantee_online_mems(const struct cpuset *cs, nodemask_t *pmask)\r\n{\r\nwhile (cs && !nodes_intersects(cs->mems_allowed,\r\nnode_states[N_HIGH_MEMORY]))\r\ncs = cs->parent;\r\nif (cs)\r\nnodes_and(*pmask, cs->mems_allowed,\r\nnode_states[N_HIGH_MEMORY]);\r\nelse\r\n*pmask = node_states[N_HIGH_MEMORY];\r\nBUG_ON(!nodes_intersects(*pmask, node_states[N_HIGH_MEMORY]));\r\n}\r\nstatic void cpuset_update_task_spread_flag(struct cpuset *cs,\r\nstruct task_struct *tsk)\r\n{\r\nif (is_spread_page(cs))\r\ntsk->flags |= PF_SPREAD_PAGE;\r\nelse\r\ntsk->flags &= ~PF_SPREAD_PAGE;\r\nif (is_spread_slab(cs))\r\ntsk->flags |= PF_SPREAD_SLAB;\r\nelse\r\ntsk->flags &= ~PF_SPREAD_SLAB;\r\n}\r\nstatic int is_cpuset_subset(const struct cpuset *p, const struct cpuset *q)\r\n{\r\nreturn cpumask_subset(p->cpus_allowed, q->cpus_allowed) &&\r\nnodes_subset(p->mems_allowed, q->mems_allowed) &&\r\nis_cpu_exclusive(p) <= is_cpu_exclusive(q) &&\r\nis_mem_exclusive(p) <= is_mem_exclusive(q);\r\n}\r\nstatic struct cpuset *alloc_trial_cpuset(const struct cpuset *cs)\r\n{\r\nstruct cpuset *trial;\r\ntrial = kmemdup(cs, sizeof(*cs), GFP_KERNEL);\r\nif (!trial)\r\nreturn NULL;\r\nif (!alloc_cpumask_var(&trial->cpus_allowed, GFP_KERNEL)) {\r\nkfree(trial);\r\nreturn NULL;\r\n}\r\ncpumask_copy(trial->cpus_allowed, cs->cpus_allowed);\r\nreturn trial;\r\n}\r\nstatic void free_trial_cpuset(struct cpuset *trial)\r\n{\r\nfree_cpumask_var(trial->cpus_allowed);\r\nkfree(trial);\r\n}\r\nstatic int validate_change(const struct cpuset *cur, const struct cpuset *trial)\r\n{\r\nstruct cgroup *cont;\r\nstruct cpuset *c, *par;\r\nlist_for_each_entry(cont, &cur->css.cgroup->children, sibling) {\r\nif (!is_cpuset_subset(cgroup_cs(cont), trial))\r\nreturn -EBUSY;\r\n}\r\nif (cur == &top_cpuset)\r\nreturn 0;\r\npar = cur->parent;\r\nif (!is_cpuset_subset(trial, par))\r\nreturn -EACCES;\r\nlist_for_each_entry(cont, &par->css.cgroup->children, sibling) {\r\nc = cgroup_cs(cont);\r\nif ((is_cpu_exclusive(trial) || is_cpu_exclusive(c)) &&\r\nc != cur &&\r\ncpumask_intersects(trial->cpus_allowed, c->cpus_allowed))\r\nreturn -EINVAL;\r\nif ((is_mem_exclusive(trial) || is_mem_exclusive(c)) &&\r\nc != cur &&\r\nnodes_intersects(trial->mems_allowed, c->mems_allowed))\r\nreturn -EINVAL;\r\n}\r\nif (cgroup_task_count(cur->css.cgroup)) {\r\nif (cpumask_empty(trial->cpus_allowed) ||\r\nnodes_empty(trial->mems_allowed)) {\r\nreturn -ENOSPC;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int cpusets_overlap(struct cpuset *a, struct cpuset *b)\r\n{\r\nreturn cpumask_intersects(a->cpus_allowed, b->cpus_allowed);\r\n}\r\nstatic void\r\nupdate_domain_attr(struct sched_domain_attr *dattr, struct cpuset *c)\r\n{\r\nif (dattr->relax_domain_level < c->relax_domain_level)\r\ndattr->relax_domain_level = c->relax_domain_level;\r\nreturn;\r\n}\r\nstatic void\r\nupdate_domain_attr_tree(struct sched_domain_attr *dattr, struct cpuset *c)\r\n{\r\nLIST_HEAD(q);\r\nlist_add(&c->stack_list, &q);\r\nwhile (!list_empty(&q)) {\r\nstruct cpuset *cp;\r\nstruct cgroup *cont;\r\nstruct cpuset *child;\r\ncp = list_first_entry(&q, struct cpuset, stack_list);\r\nlist_del(q.next);\r\nif (cpumask_empty(cp->cpus_allowed))\r\ncontinue;\r\nif (is_sched_load_balance(cp))\r\nupdate_domain_attr(dattr, cp);\r\nlist_for_each_entry(cont, &cp->css.cgroup->children, sibling) {\r\nchild = cgroup_cs(cont);\r\nlist_add_tail(&child->stack_list, &q);\r\n}\r\n}\r\n}\r\nstatic int generate_sched_domains(cpumask_var_t **domains,\r\nstruct sched_domain_attr **attributes)\r\n{\r\nLIST_HEAD(q);\r\nstruct cpuset *cp;\r\nstruct cpuset **csa;\r\nint csn;\r\nint i, j, k;\r\ncpumask_var_t *doms;\r\nstruct sched_domain_attr *dattr;\r\nint ndoms = 0;\r\nint nslot;\r\ndoms = NULL;\r\ndattr = NULL;\r\ncsa = NULL;\r\nif (is_sched_load_balance(&top_cpuset)) {\r\nndoms = 1;\r\ndoms = alloc_sched_domains(ndoms);\r\nif (!doms)\r\ngoto done;\r\ndattr = kmalloc(sizeof(struct sched_domain_attr), GFP_KERNEL);\r\nif (dattr) {\r\n*dattr = SD_ATTR_INIT;\r\nupdate_domain_attr_tree(dattr, &top_cpuset);\r\n}\r\ncpumask_copy(doms[0], top_cpuset.cpus_allowed);\r\ngoto done;\r\n}\r\ncsa = kmalloc(number_of_cpusets * sizeof(cp), GFP_KERNEL);\r\nif (!csa)\r\ngoto done;\r\ncsn = 0;\r\nlist_add(&top_cpuset.stack_list, &q);\r\nwhile (!list_empty(&q)) {\r\nstruct cgroup *cont;\r\nstruct cpuset *child;\r\ncp = list_first_entry(&q, struct cpuset, stack_list);\r\nlist_del(q.next);\r\nif (cpumask_empty(cp->cpus_allowed))\r\ncontinue;\r\nif (is_sched_load_balance(cp)) {\r\ncsa[csn++] = cp;\r\ncontinue;\r\n}\r\nlist_for_each_entry(cont, &cp->css.cgroup->children, sibling) {\r\nchild = cgroup_cs(cont);\r\nlist_add_tail(&child->stack_list, &q);\r\n}\r\n}\r\nfor (i = 0; i < csn; i++)\r\ncsa[i]->pn = i;\r\nndoms = csn;\r\nrestart:\r\nfor (i = 0; i < csn; i++) {\r\nstruct cpuset *a = csa[i];\r\nint apn = a->pn;\r\nfor (j = 0; j < csn; j++) {\r\nstruct cpuset *b = csa[j];\r\nint bpn = b->pn;\r\nif (apn != bpn && cpusets_overlap(a, b)) {\r\nfor (k = 0; k < csn; k++) {\r\nstruct cpuset *c = csa[k];\r\nif (c->pn == bpn)\r\nc->pn = apn;\r\n}\r\nndoms--;\r\ngoto restart;\r\n}\r\n}\r\n}\r\ndoms = alloc_sched_domains(ndoms);\r\nif (!doms)\r\ngoto done;\r\ndattr = kmalloc(ndoms * sizeof(struct sched_domain_attr), GFP_KERNEL);\r\nfor (nslot = 0, i = 0; i < csn; i++) {\r\nstruct cpuset *a = csa[i];\r\nstruct cpumask *dp;\r\nint apn = a->pn;\r\nif (apn < 0) {\r\ncontinue;\r\n}\r\ndp = doms[nslot];\r\nif (nslot == ndoms) {\r\nstatic int warnings = 10;\r\nif (warnings) {\r\nprintk(KERN_WARNING\r\n"rebuild_sched_domains confused:"\r\n" nslot %d, ndoms %d, csn %d, i %d,"\r\n" apn %d\n",\r\nnslot, ndoms, csn, i, apn);\r\nwarnings--;\r\n}\r\ncontinue;\r\n}\r\ncpumask_clear(dp);\r\nif (dattr)\r\n*(dattr + nslot) = SD_ATTR_INIT;\r\nfor (j = i; j < csn; j++) {\r\nstruct cpuset *b = csa[j];\r\nif (apn == b->pn) {\r\ncpumask_or(dp, dp, b->cpus_allowed);\r\nif (dattr)\r\nupdate_domain_attr_tree(dattr + nslot, b);\r\nb->pn = -1;\r\n}\r\n}\r\nnslot++;\r\n}\r\nBUG_ON(nslot != ndoms);\r\ndone:\r\nkfree(csa);\r\nif (doms == NULL)\r\nndoms = 1;\r\n*domains = doms;\r\n*attributes = dattr;\r\nreturn ndoms;\r\n}\r\nstatic void do_rebuild_sched_domains(struct work_struct *unused)\r\n{\r\nstruct sched_domain_attr *attr;\r\ncpumask_var_t *doms;\r\nint ndoms;\r\nget_online_cpus();\r\ncgroup_lock();\r\nndoms = generate_sched_domains(&doms, &attr);\r\ncgroup_unlock();\r\npartition_sched_domains(ndoms, doms, attr);\r\nput_online_cpus();\r\n}\r\nstatic void do_rebuild_sched_domains(struct work_struct *unused)\r\n{\r\n}\r\nstatic int generate_sched_domains(cpumask_var_t **domains,\r\nstruct sched_domain_attr **attributes)\r\n{\r\n*domains = NULL;\r\nreturn 1;\r\n}\r\nstatic void async_rebuild_sched_domains(void)\r\n{\r\nqueue_work(cpuset_wq, &rebuild_sched_domains_work);\r\n}\r\nvoid rebuild_sched_domains(void)\r\n{\r\ndo_rebuild_sched_domains(NULL);\r\n}\r\nstatic int cpuset_test_cpumask(struct task_struct *tsk,\r\nstruct cgroup_scanner *scan)\r\n{\r\nreturn !cpumask_equal(&tsk->cpus_allowed,\r\n(cgroup_cs(scan->cg))->cpus_allowed);\r\n}\r\nstatic void cpuset_change_cpumask(struct task_struct *tsk,\r\nstruct cgroup_scanner *scan)\r\n{\r\nset_cpus_allowed_ptr(tsk, ((cgroup_cs(scan->cg))->cpus_allowed));\r\n}\r\nstatic void update_tasks_cpumask(struct cpuset *cs, struct ptr_heap *heap)\r\n{\r\nstruct cgroup_scanner scan;\r\nscan.cg = cs->css.cgroup;\r\nscan.test_task = cpuset_test_cpumask;\r\nscan.process_task = cpuset_change_cpumask;\r\nscan.heap = heap;\r\ncgroup_scan_tasks(&scan);\r\n}\r\nstatic int update_cpumask(struct cpuset *cs, struct cpuset *trialcs,\r\nconst char *buf)\r\n{\r\nstruct ptr_heap heap;\r\nint retval;\r\nint is_load_balanced;\r\nif (cs == &top_cpuset)\r\nreturn -EACCES;\r\nif (!*buf) {\r\ncpumask_clear(trialcs->cpus_allowed);\r\n} else {\r\nretval = cpulist_parse(buf, trialcs->cpus_allowed);\r\nif (retval < 0)\r\nreturn retval;\r\nif (!cpumask_subset(trialcs->cpus_allowed, cpu_active_mask))\r\nreturn -EINVAL;\r\n}\r\nretval = validate_change(cs, trialcs);\r\nif (retval < 0)\r\nreturn retval;\r\nif (cpumask_equal(cs->cpus_allowed, trialcs->cpus_allowed))\r\nreturn 0;\r\nretval = heap_init(&heap, PAGE_SIZE, GFP_KERNEL, NULL);\r\nif (retval)\r\nreturn retval;\r\nis_load_balanced = is_sched_load_balance(trialcs);\r\nmutex_lock(&callback_mutex);\r\ncpumask_copy(cs->cpus_allowed, trialcs->cpus_allowed);\r\nmutex_unlock(&callback_mutex);\r\nupdate_tasks_cpumask(cs, &heap);\r\nheap_free(&heap);\r\nif (is_load_balanced)\r\nasync_rebuild_sched_domains();\r\nreturn 0;\r\n}\r\nstatic void cpuset_migrate_mm(struct mm_struct *mm, const nodemask_t *from,\r\nconst nodemask_t *to)\r\n{\r\nstruct task_struct *tsk = current;\r\ntsk->mems_allowed = *to;\r\ndo_migrate_pages(mm, from, to, MPOL_MF_MOVE_ALL);\r\nguarantee_online_mems(task_cs(tsk),&tsk->mems_allowed);\r\n}\r\nstatic void cpuset_change_task_nodemask(struct task_struct *tsk,\r\nnodemask_t *newmems)\r\n{\r\nbool need_loop;\r\nif (unlikely(test_thread_flag(TIF_MEMDIE)))\r\nreturn;\r\nif (current->flags & PF_EXITING)\r\nreturn;\r\ntask_lock(tsk);\r\nneed_loop = task_has_mempolicy(tsk) ||\r\n!nodes_intersects(*newmems, tsk->mems_allowed);\r\nif (need_loop)\r\nwrite_seqcount_begin(&tsk->mems_allowed_seq);\r\nnodes_or(tsk->mems_allowed, tsk->mems_allowed, *newmems);\r\nmpol_rebind_task(tsk, newmems, MPOL_REBIND_STEP1);\r\nmpol_rebind_task(tsk, newmems, MPOL_REBIND_STEP2);\r\ntsk->mems_allowed = *newmems;\r\nif (need_loop)\r\nwrite_seqcount_end(&tsk->mems_allowed_seq);\r\ntask_unlock(tsk);\r\n}\r\nstatic void cpuset_change_nodemask(struct task_struct *p,\r\nstruct cgroup_scanner *scan)\r\n{\r\nstruct mm_struct *mm;\r\nstruct cpuset *cs;\r\nint migrate;\r\nconst nodemask_t *oldmem = scan->data;\r\nstatic nodemask_t newmems;\r\ncs = cgroup_cs(scan->cg);\r\nguarantee_online_mems(cs, &newmems);\r\ncpuset_change_task_nodemask(p, &newmems);\r\nmm = get_task_mm(p);\r\nif (!mm)\r\nreturn;\r\nmigrate = is_memory_migrate(cs);\r\nmpol_rebind_mm(mm, &cs->mems_allowed);\r\nif (migrate)\r\ncpuset_migrate_mm(mm, oldmem, &cs->mems_allowed);\r\nmmput(mm);\r\n}\r\nstatic void update_tasks_nodemask(struct cpuset *cs, const nodemask_t *oldmem,\r\nstruct ptr_heap *heap)\r\n{\r\nstruct cgroup_scanner scan;\r\ncpuset_being_rebound = cs;\r\nscan.cg = cs->css.cgroup;\r\nscan.test_task = NULL;\r\nscan.process_task = cpuset_change_nodemask;\r\nscan.heap = heap;\r\nscan.data = (nodemask_t *)oldmem;\r\ncgroup_scan_tasks(&scan);\r\ncpuset_being_rebound = NULL;\r\n}\r\nstatic int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,\r\nconst char *buf)\r\n{\r\nNODEMASK_ALLOC(nodemask_t, oldmem, GFP_KERNEL);\r\nint retval;\r\nstruct ptr_heap heap;\r\nif (!oldmem)\r\nreturn -ENOMEM;\r\nif (cs == &top_cpuset) {\r\nretval = -EACCES;\r\ngoto done;\r\n}\r\nif (!*buf) {\r\nnodes_clear(trialcs->mems_allowed);\r\n} else {\r\nretval = nodelist_parse(buf, trialcs->mems_allowed);\r\nif (retval < 0)\r\ngoto done;\r\nif (!nodes_subset(trialcs->mems_allowed,\r\nnode_states[N_HIGH_MEMORY])) {\r\nretval = -EINVAL;\r\ngoto done;\r\n}\r\n}\r\n*oldmem = cs->mems_allowed;\r\nif (nodes_equal(*oldmem, trialcs->mems_allowed)) {\r\nretval = 0;\r\ngoto done;\r\n}\r\nretval = validate_change(cs, trialcs);\r\nif (retval < 0)\r\ngoto done;\r\nretval = heap_init(&heap, PAGE_SIZE, GFP_KERNEL, NULL);\r\nif (retval < 0)\r\ngoto done;\r\nmutex_lock(&callback_mutex);\r\ncs->mems_allowed = trialcs->mems_allowed;\r\nmutex_unlock(&callback_mutex);\r\nupdate_tasks_nodemask(cs, oldmem, &heap);\r\nheap_free(&heap);\r\ndone:\r\nNODEMASK_FREE(oldmem);\r\nreturn retval;\r\n}\r\nint current_cpuset_is_being_rebound(void)\r\n{\r\nreturn task_cs(current) == cpuset_being_rebound;\r\n}\r\nstatic int update_relax_domain_level(struct cpuset *cs, s64 val)\r\n{\r\n#ifdef CONFIG_SMP\r\nif (val < -1 || val >= sched_domain_level_max)\r\nreturn -EINVAL;\r\n#endif\r\nif (val != cs->relax_domain_level) {\r\ncs->relax_domain_level = val;\r\nif (!cpumask_empty(cs->cpus_allowed) &&\r\nis_sched_load_balance(cs))\r\nasync_rebuild_sched_domains();\r\n}\r\nreturn 0;\r\n}\r\nstatic void cpuset_change_flag(struct task_struct *tsk,\r\nstruct cgroup_scanner *scan)\r\n{\r\ncpuset_update_task_spread_flag(cgroup_cs(scan->cg), tsk);\r\n}\r\nstatic void update_tasks_flags(struct cpuset *cs, struct ptr_heap *heap)\r\n{\r\nstruct cgroup_scanner scan;\r\nscan.cg = cs->css.cgroup;\r\nscan.test_task = NULL;\r\nscan.process_task = cpuset_change_flag;\r\nscan.heap = heap;\r\ncgroup_scan_tasks(&scan);\r\n}\r\nstatic int update_flag(cpuset_flagbits_t bit, struct cpuset *cs,\r\nint turning_on)\r\n{\r\nstruct cpuset *trialcs;\r\nint balance_flag_changed;\r\nint spread_flag_changed;\r\nstruct ptr_heap heap;\r\nint err;\r\ntrialcs = alloc_trial_cpuset(cs);\r\nif (!trialcs)\r\nreturn -ENOMEM;\r\nif (turning_on)\r\nset_bit(bit, &trialcs->flags);\r\nelse\r\nclear_bit(bit, &trialcs->flags);\r\nerr = validate_change(cs, trialcs);\r\nif (err < 0)\r\ngoto out;\r\nerr = heap_init(&heap, PAGE_SIZE, GFP_KERNEL, NULL);\r\nif (err < 0)\r\ngoto out;\r\nbalance_flag_changed = (is_sched_load_balance(cs) !=\r\nis_sched_load_balance(trialcs));\r\nspread_flag_changed = ((is_spread_slab(cs) != is_spread_slab(trialcs))\r\n|| (is_spread_page(cs) != is_spread_page(trialcs)));\r\nmutex_lock(&callback_mutex);\r\ncs->flags = trialcs->flags;\r\nmutex_unlock(&callback_mutex);\r\nif (!cpumask_empty(trialcs->cpus_allowed) && balance_flag_changed)\r\nasync_rebuild_sched_domains();\r\nif (spread_flag_changed)\r\nupdate_tasks_flags(cs, &heap);\r\nheap_free(&heap);\r\nout:\r\nfree_trial_cpuset(trialcs);\r\nreturn err;\r\n}\r\nstatic void fmeter_init(struct fmeter *fmp)\r\n{\r\nfmp->cnt = 0;\r\nfmp->val = 0;\r\nfmp->time = 0;\r\nspin_lock_init(&fmp->lock);\r\n}\r\nstatic void fmeter_update(struct fmeter *fmp)\r\n{\r\ntime_t now = get_seconds();\r\ntime_t ticks = now - fmp->time;\r\nif (ticks == 0)\r\nreturn;\r\nticks = min(FM_MAXTICKS, ticks);\r\nwhile (ticks-- > 0)\r\nfmp->val = (FM_COEF * fmp->val) / FM_SCALE;\r\nfmp->time = now;\r\nfmp->val += ((FM_SCALE - FM_COEF) * fmp->cnt) / FM_SCALE;\r\nfmp->cnt = 0;\r\n}\r\nstatic void fmeter_markevent(struct fmeter *fmp)\r\n{\r\nspin_lock(&fmp->lock);\r\nfmeter_update(fmp);\r\nfmp->cnt = min(FM_MAXCNT, fmp->cnt + FM_SCALE);\r\nspin_unlock(&fmp->lock);\r\n}\r\nstatic int fmeter_getrate(struct fmeter *fmp)\r\n{\r\nint val;\r\nspin_lock(&fmp->lock);\r\nfmeter_update(fmp);\r\nval = fmp->val;\r\nspin_unlock(&fmp->lock);\r\nreturn val;\r\n}\r\nstatic int cpuset_can_attach(struct cgroup *cgrp, struct cgroup_taskset *tset)\r\n{\r\nstruct cpuset *cs = cgroup_cs(cgrp);\r\nstruct task_struct *task;\r\nint ret;\r\nif (cpumask_empty(cs->cpus_allowed) || nodes_empty(cs->mems_allowed))\r\nreturn -ENOSPC;\r\ncgroup_taskset_for_each(task, cgrp, tset) {\r\nif (task->flags & PF_THREAD_BOUND)\r\nreturn -EINVAL;\r\nif ((ret = security_task_setscheduler(task)))\r\nreturn ret;\r\n}\r\nif (cs == &top_cpuset)\r\ncpumask_copy(cpus_attach, cpu_possible_mask);\r\nelse\r\nguarantee_online_cpus(cs, cpus_attach);\r\nguarantee_online_mems(cs, &cpuset_attach_nodemask_to);\r\nreturn 0;\r\n}\r\nstatic void cpuset_attach(struct cgroup *cgrp, struct cgroup_taskset *tset)\r\n{\r\nstruct mm_struct *mm;\r\nstruct task_struct *task;\r\nstruct task_struct *leader = cgroup_taskset_first(tset);\r\nstruct cgroup *oldcgrp = cgroup_taskset_cur_cgroup(tset);\r\nstruct cpuset *cs = cgroup_cs(cgrp);\r\nstruct cpuset *oldcs = cgroup_cs(oldcgrp);\r\ncgroup_taskset_for_each(task, cgrp, tset) {\r\nWARN_ON_ONCE(set_cpus_allowed_ptr(task, cpus_attach));\r\ncpuset_change_task_nodemask(task, &cpuset_attach_nodemask_to);\r\ncpuset_update_task_spread_flag(cs, task);\r\n}\r\ncpuset_attach_nodemask_from = oldcs->mems_allowed;\r\ncpuset_attach_nodemask_to = cs->mems_allowed;\r\nmm = get_task_mm(leader);\r\nif (mm) {\r\nmpol_rebind_mm(mm, &cpuset_attach_nodemask_to);\r\nif (is_memory_migrate(cs))\r\ncpuset_migrate_mm(mm, &cpuset_attach_nodemask_from,\r\n&cpuset_attach_nodemask_to);\r\nmmput(mm);\r\n}\r\n}\r\nstatic int cpuset_write_u64(struct cgroup *cgrp, struct cftype *cft, u64 val)\r\n{\r\nint retval = 0;\r\nstruct cpuset *cs = cgroup_cs(cgrp);\r\ncpuset_filetype_t type = cft->private;\r\nif (!cgroup_lock_live_group(cgrp))\r\nreturn -ENODEV;\r\nswitch (type) {\r\ncase FILE_CPU_EXCLUSIVE:\r\nretval = update_flag(CS_CPU_EXCLUSIVE, cs, val);\r\nbreak;\r\ncase FILE_MEM_EXCLUSIVE:\r\nretval = update_flag(CS_MEM_EXCLUSIVE, cs, val);\r\nbreak;\r\ncase FILE_MEM_HARDWALL:\r\nretval = update_flag(CS_MEM_HARDWALL, cs, val);\r\nbreak;\r\ncase FILE_SCHED_LOAD_BALANCE:\r\nretval = update_flag(CS_SCHED_LOAD_BALANCE, cs, val);\r\nbreak;\r\ncase FILE_MEMORY_MIGRATE:\r\nretval = update_flag(CS_MEMORY_MIGRATE, cs, val);\r\nbreak;\r\ncase FILE_MEMORY_PRESSURE_ENABLED:\r\ncpuset_memory_pressure_enabled = !!val;\r\nbreak;\r\ncase FILE_MEMORY_PRESSURE:\r\nretval = -EACCES;\r\nbreak;\r\ncase FILE_SPREAD_PAGE:\r\nretval = update_flag(CS_SPREAD_PAGE, cs, val);\r\nbreak;\r\ncase FILE_SPREAD_SLAB:\r\nretval = update_flag(CS_SPREAD_SLAB, cs, val);\r\nbreak;\r\ndefault:\r\nretval = -EINVAL;\r\nbreak;\r\n}\r\ncgroup_unlock();\r\nreturn retval;\r\n}\r\nstatic int cpuset_write_s64(struct cgroup *cgrp, struct cftype *cft, s64 val)\r\n{\r\nint retval = 0;\r\nstruct cpuset *cs = cgroup_cs(cgrp);\r\ncpuset_filetype_t type = cft->private;\r\nif (!cgroup_lock_live_group(cgrp))\r\nreturn -ENODEV;\r\nswitch (type) {\r\ncase FILE_SCHED_RELAX_DOMAIN_LEVEL:\r\nretval = update_relax_domain_level(cs, val);\r\nbreak;\r\ndefault:\r\nretval = -EINVAL;\r\nbreak;\r\n}\r\ncgroup_unlock();\r\nreturn retval;\r\n}\r\nstatic int cpuset_write_resmask(struct cgroup *cgrp, struct cftype *cft,\r\nconst char *buf)\r\n{\r\nint retval = 0;\r\nstruct cpuset *cs = cgroup_cs(cgrp);\r\nstruct cpuset *trialcs;\r\nif (!cgroup_lock_live_group(cgrp))\r\nreturn -ENODEV;\r\ntrialcs = alloc_trial_cpuset(cs);\r\nif (!trialcs) {\r\nretval = -ENOMEM;\r\ngoto out;\r\n}\r\nswitch (cft->private) {\r\ncase FILE_CPULIST:\r\nretval = update_cpumask(cs, trialcs, buf);\r\nbreak;\r\ncase FILE_MEMLIST:\r\nretval = update_nodemask(cs, trialcs, buf);\r\nbreak;\r\ndefault:\r\nretval = -EINVAL;\r\nbreak;\r\n}\r\nfree_trial_cpuset(trialcs);\r\nout:\r\ncgroup_unlock();\r\nreturn retval;\r\n}\r\nstatic size_t cpuset_sprintf_cpulist(char *page, struct cpuset *cs)\r\n{\r\nsize_t count;\r\nmutex_lock(&callback_mutex);\r\ncount = cpulist_scnprintf(page, PAGE_SIZE, cs->cpus_allowed);\r\nmutex_unlock(&callback_mutex);\r\nreturn count;\r\n}\r\nstatic size_t cpuset_sprintf_memlist(char *page, struct cpuset *cs)\r\n{\r\nsize_t count;\r\nmutex_lock(&callback_mutex);\r\ncount = nodelist_scnprintf(page, PAGE_SIZE, cs->mems_allowed);\r\nmutex_unlock(&callback_mutex);\r\nreturn count;\r\n}\r\nstatic ssize_t cpuset_common_file_read(struct cgroup *cont,\r\nstruct cftype *cft,\r\nstruct file *file,\r\nchar __user *buf,\r\nsize_t nbytes, loff_t *ppos)\r\n{\r\nstruct cpuset *cs = cgroup_cs(cont);\r\ncpuset_filetype_t type = cft->private;\r\nchar *page;\r\nssize_t retval = 0;\r\nchar *s;\r\nif (!(page = (char *)__get_free_page(GFP_TEMPORARY)))\r\nreturn -ENOMEM;\r\ns = page;\r\nswitch (type) {\r\ncase FILE_CPULIST:\r\ns += cpuset_sprintf_cpulist(s, cs);\r\nbreak;\r\ncase FILE_MEMLIST:\r\ns += cpuset_sprintf_memlist(s, cs);\r\nbreak;\r\ndefault:\r\nretval = -EINVAL;\r\ngoto out;\r\n}\r\n*s++ = '\n';\r\nretval = simple_read_from_buffer(buf, nbytes, ppos, page, s - page);\r\nout:\r\nfree_page((unsigned long)page);\r\nreturn retval;\r\n}\r\nstatic u64 cpuset_read_u64(struct cgroup *cont, struct cftype *cft)\r\n{\r\nstruct cpuset *cs = cgroup_cs(cont);\r\ncpuset_filetype_t type = cft->private;\r\nswitch (type) {\r\ncase FILE_CPU_EXCLUSIVE:\r\nreturn is_cpu_exclusive(cs);\r\ncase FILE_MEM_EXCLUSIVE:\r\nreturn is_mem_exclusive(cs);\r\ncase FILE_MEM_HARDWALL:\r\nreturn is_mem_hardwall(cs);\r\ncase FILE_SCHED_LOAD_BALANCE:\r\nreturn is_sched_load_balance(cs);\r\ncase FILE_MEMORY_MIGRATE:\r\nreturn is_memory_migrate(cs);\r\ncase FILE_MEMORY_PRESSURE_ENABLED:\r\nreturn cpuset_memory_pressure_enabled;\r\ncase FILE_MEMORY_PRESSURE:\r\nreturn fmeter_getrate(&cs->fmeter);\r\ncase FILE_SPREAD_PAGE:\r\nreturn is_spread_page(cs);\r\ncase FILE_SPREAD_SLAB:\r\nreturn is_spread_slab(cs);\r\ndefault:\r\nBUG();\r\n}\r\nreturn 0;\r\n}\r\nstatic s64 cpuset_read_s64(struct cgroup *cont, struct cftype *cft)\r\n{\r\nstruct cpuset *cs = cgroup_cs(cont);\r\ncpuset_filetype_t type = cft->private;\r\nswitch (type) {\r\ncase FILE_SCHED_RELAX_DOMAIN_LEVEL:\r\nreturn cs->relax_domain_level;\r\ndefault:\r\nBUG();\r\n}\r\nreturn 0;\r\n}\r\nstatic void cpuset_post_clone(struct cgroup *cgroup)\r\n{\r\nstruct cgroup *parent, *child;\r\nstruct cpuset *cs, *parent_cs;\r\nparent = cgroup->parent;\r\nlist_for_each_entry(child, &parent->children, sibling) {\r\ncs = cgroup_cs(child);\r\nif (is_mem_exclusive(cs) || is_cpu_exclusive(cs))\r\nreturn;\r\n}\r\ncs = cgroup_cs(cgroup);\r\nparent_cs = cgroup_cs(parent);\r\nmutex_lock(&callback_mutex);\r\ncs->mems_allowed = parent_cs->mems_allowed;\r\ncpumask_copy(cs->cpus_allowed, parent_cs->cpus_allowed);\r\nmutex_unlock(&callback_mutex);\r\nreturn;\r\n}\r\nstatic struct cgroup_subsys_state *cpuset_create(struct cgroup *cont)\r\n{\r\nstruct cpuset *cs;\r\nstruct cpuset *parent;\r\nif (!cont->parent) {\r\nreturn &top_cpuset.css;\r\n}\r\nparent = cgroup_cs(cont->parent);\r\ncs = kmalloc(sizeof(*cs), GFP_KERNEL);\r\nif (!cs)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (!alloc_cpumask_var(&cs->cpus_allowed, GFP_KERNEL)) {\r\nkfree(cs);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\ncs->flags = 0;\r\nif (is_spread_page(parent))\r\nset_bit(CS_SPREAD_PAGE, &cs->flags);\r\nif (is_spread_slab(parent))\r\nset_bit(CS_SPREAD_SLAB, &cs->flags);\r\nset_bit(CS_SCHED_LOAD_BALANCE, &cs->flags);\r\ncpumask_clear(cs->cpus_allowed);\r\nnodes_clear(cs->mems_allowed);\r\nfmeter_init(&cs->fmeter);\r\ncs->relax_domain_level = -1;\r\ncs->parent = parent;\r\nnumber_of_cpusets++;\r\nreturn &cs->css ;\r\n}\r\nstatic void cpuset_destroy(struct cgroup *cont)\r\n{\r\nstruct cpuset *cs = cgroup_cs(cont);\r\nif (is_sched_load_balance(cs))\r\nupdate_flag(CS_SCHED_LOAD_BALANCE, cs, 0);\r\nnumber_of_cpusets--;\r\nfree_cpumask_var(cs->cpus_allowed);\r\nkfree(cs);\r\n}\r\nint __init cpuset_init(void)\r\n{\r\nint err = 0;\r\nif (!alloc_cpumask_var(&top_cpuset.cpus_allowed, GFP_KERNEL))\r\nBUG();\r\ncpumask_setall(top_cpuset.cpus_allowed);\r\nnodes_setall(top_cpuset.mems_allowed);\r\nfmeter_init(&top_cpuset.fmeter);\r\nset_bit(CS_SCHED_LOAD_BALANCE, &top_cpuset.flags);\r\ntop_cpuset.relax_domain_level = -1;\r\nerr = register_filesystem(&cpuset_fs_type);\r\nif (err < 0)\r\nreturn err;\r\nif (!alloc_cpumask_var(&cpus_attach, GFP_KERNEL))\r\nBUG();\r\nnumber_of_cpusets = 1;\r\nreturn 0;\r\n}\r\nstatic void cpuset_do_move_task(struct task_struct *tsk,\r\nstruct cgroup_scanner *scan)\r\n{\r\nstruct cgroup *new_cgroup = scan->data;\r\ncgroup_attach_task(new_cgroup, tsk);\r\n}\r\nstatic void move_member_tasks_to_cpuset(struct cpuset *from, struct cpuset *to)\r\n{\r\nstruct cgroup_scanner scan;\r\nscan.cg = from->css.cgroup;\r\nscan.test_task = NULL;\r\nscan.process_task = cpuset_do_move_task;\r\nscan.heap = NULL;\r\nscan.data = to->css.cgroup;\r\nif (cgroup_scan_tasks(&scan))\r\nprintk(KERN_ERR "move_member_tasks_to_cpuset: "\r\n"cgroup_scan_tasks failed\n");\r\n}\r\nstatic void remove_tasks_in_empty_cpuset(struct cpuset *cs)\r\n{\r\nstruct cpuset *parent;\r\nif (list_empty(&cs->css.cgroup->css_sets))\r\nreturn;\r\nparent = cs->parent;\r\nwhile (cpumask_empty(parent->cpus_allowed) ||\r\nnodes_empty(parent->mems_allowed))\r\nparent = parent->parent;\r\nmove_member_tasks_to_cpuset(cs, parent);\r\n}\r\nstatic struct cpuset *cpuset_next(struct list_head *queue)\r\n{\r\nstruct cpuset *cp;\r\nstruct cpuset *child;\r\nstruct cgroup *cont;\r\nif (list_empty(queue))\r\nreturn NULL;\r\ncp = list_first_entry(queue, struct cpuset, stack_list);\r\nlist_del(queue->next);\r\nlist_for_each_entry(cont, &cp->css.cgroup->children, sibling) {\r\nchild = cgroup_cs(cont);\r\nlist_add_tail(&child->stack_list, queue);\r\n}\r\nreturn cp;\r\n}\r\nstatic void\r\nscan_cpusets_upon_hotplug(struct cpuset *root, enum hotplug_event event)\r\n{\r\nLIST_HEAD(queue);\r\nstruct cpuset *cp;\r\nstatic nodemask_t oldmems;\r\nlist_add_tail((struct list_head *)&root->stack_list, &queue);\r\nswitch (event) {\r\ncase CPUSET_CPU_OFFLINE:\r\nwhile ((cp = cpuset_next(&queue)) != NULL) {\r\nif (cpumask_subset(cp->cpus_allowed, cpu_active_mask))\r\ncontinue;\r\nmutex_lock(&callback_mutex);\r\ncpumask_and(cp->cpus_allowed, cp->cpus_allowed,\r\ncpu_active_mask);\r\nmutex_unlock(&callback_mutex);\r\nif (cpumask_empty(cp->cpus_allowed))\r\nremove_tasks_in_empty_cpuset(cp);\r\nelse\r\nupdate_tasks_cpumask(cp, NULL);\r\n}\r\nbreak;\r\ncase CPUSET_MEM_OFFLINE:\r\nwhile ((cp = cpuset_next(&queue)) != NULL) {\r\nif (nodes_subset(cp->mems_allowed,\r\nnode_states[N_HIGH_MEMORY]))\r\ncontinue;\r\noldmems = cp->mems_allowed;\r\nmutex_lock(&callback_mutex);\r\nnodes_and(cp->mems_allowed, cp->mems_allowed,\r\nnode_states[N_HIGH_MEMORY]);\r\nmutex_unlock(&callback_mutex);\r\nif (nodes_empty(cp->mems_allowed))\r\nremove_tasks_in_empty_cpuset(cp);\r\nelse\r\nupdate_tasks_nodemask(cp, &oldmems, NULL);\r\n}\r\n}\r\n}\r\nvoid cpuset_update_active_cpus(bool cpu_online)\r\n{\r\nstruct sched_domain_attr *attr;\r\ncpumask_var_t *doms;\r\nint ndoms;\r\ncgroup_lock();\r\nmutex_lock(&callback_mutex);\r\ncpumask_copy(top_cpuset.cpus_allowed, cpu_active_mask);\r\nmutex_unlock(&callback_mutex);\r\nif (!cpu_online)\r\nscan_cpusets_upon_hotplug(&top_cpuset, CPUSET_CPU_OFFLINE);\r\nndoms = generate_sched_domains(&doms, &attr);\r\ncgroup_unlock();\r\npartition_sched_domains(ndoms, doms, attr);\r\n}\r\nstatic int cpuset_track_online_nodes(struct notifier_block *self,\r\nunsigned long action, void *arg)\r\n{\r\nstatic nodemask_t oldmems;\r\ncgroup_lock();\r\nswitch (action) {\r\ncase MEM_ONLINE:\r\noldmems = top_cpuset.mems_allowed;\r\nmutex_lock(&callback_mutex);\r\ntop_cpuset.mems_allowed = node_states[N_HIGH_MEMORY];\r\nmutex_unlock(&callback_mutex);\r\nupdate_tasks_nodemask(&top_cpuset, &oldmems, NULL);\r\nbreak;\r\ncase MEM_OFFLINE:\r\nscan_cpusets_upon_hotplug(&top_cpuset, CPUSET_MEM_OFFLINE);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\ncgroup_unlock();\r\nreturn NOTIFY_OK;\r\n}\r\nvoid __init cpuset_init_smp(void)\r\n{\r\ncpumask_copy(top_cpuset.cpus_allowed, cpu_active_mask);\r\ntop_cpuset.mems_allowed = node_states[N_HIGH_MEMORY];\r\nhotplug_memory_notifier(cpuset_track_online_nodes, 10);\r\ncpuset_wq = create_singlethread_workqueue("cpuset");\r\nBUG_ON(!cpuset_wq);\r\n}\r\nvoid cpuset_cpus_allowed(struct task_struct *tsk, struct cpumask *pmask)\r\n{\r\nmutex_lock(&callback_mutex);\r\ntask_lock(tsk);\r\nguarantee_online_cpus(task_cs(tsk), pmask);\r\ntask_unlock(tsk);\r\nmutex_unlock(&callback_mutex);\r\n}\r\nvoid cpuset_cpus_allowed_fallback(struct task_struct *tsk)\r\n{\r\nconst struct cpuset *cs;\r\nrcu_read_lock();\r\ncs = task_cs(tsk);\r\nif (cs)\r\ndo_set_cpus_allowed(tsk, cs->cpus_allowed);\r\nrcu_read_unlock();\r\n}\r\nvoid cpuset_init_current_mems_allowed(void)\r\n{\r\nnodes_setall(current->mems_allowed);\r\n}\r\nnodemask_t cpuset_mems_allowed(struct task_struct *tsk)\r\n{\r\nnodemask_t mask;\r\nmutex_lock(&callback_mutex);\r\ntask_lock(tsk);\r\nguarantee_online_mems(task_cs(tsk), &mask);\r\ntask_unlock(tsk);\r\nmutex_unlock(&callback_mutex);\r\nreturn mask;\r\n}\r\nint cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)\r\n{\r\nreturn nodes_intersects(*nodemask, current->mems_allowed);\r\n}\r\nstatic const struct cpuset *nearest_hardwall_ancestor(const struct cpuset *cs)\r\n{\r\nwhile (!(is_mem_exclusive(cs) || is_mem_hardwall(cs)) && cs->parent)\r\ncs = cs->parent;\r\nreturn cs;\r\n}\r\nint __cpuset_node_allowed_softwall(int node, gfp_t gfp_mask)\r\n{\r\nconst struct cpuset *cs;\r\nint allowed;\r\nif (in_interrupt() || (gfp_mask & __GFP_THISNODE))\r\nreturn 1;\r\nmight_sleep_if(!(gfp_mask & __GFP_HARDWALL));\r\nif (node_isset(node, current->mems_allowed))\r\nreturn 1;\r\nif (unlikely(test_thread_flag(TIF_MEMDIE)))\r\nreturn 1;\r\nif (gfp_mask & __GFP_HARDWALL)\r\nreturn 0;\r\nif (current->flags & PF_EXITING)\r\nreturn 1;\r\nmutex_lock(&callback_mutex);\r\ntask_lock(current);\r\ncs = nearest_hardwall_ancestor(task_cs(current));\r\ntask_unlock(current);\r\nallowed = node_isset(node, cs->mems_allowed);\r\nmutex_unlock(&callback_mutex);\r\nreturn allowed;\r\n}\r\nint __cpuset_node_allowed_hardwall(int node, gfp_t gfp_mask)\r\n{\r\nif (in_interrupt() || (gfp_mask & __GFP_THISNODE))\r\nreturn 1;\r\nif (node_isset(node, current->mems_allowed))\r\nreturn 1;\r\nif (unlikely(test_thread_flag(TIF_MEMDIE)))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nvoid cpuset_unlock(void)\r\n{\r\nmutex_unlock(&callback_mutex);\r\n}\r\nstatic int cpuset_spread_node(int *rotor)\r\n{\r\nint node;\r\nnode = next_node(*rotor, current->mems_allowed);\r\nif (node == MAX_NUMNODES)\r\nnode = first_node(current->mems_allowed);\r\n*rotor = node;\r\nreturn node;\r\n}\r\nint cpuset_mem_spread_node(void)\r\n{\r\nif (current->cpuset_mem_spread_rotor == NUMA_NO_NODE)\r\ncurrent->cpuset_mem_spread_rotor =\r\nnode_random(&current->mems_allowed);\r\nreturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\r\n}\r\nint cpuset_slab_spread_node(void)\r\n{\r\nif (current->cpuset_slab_spread_rotor == NUMA_NO_NODE)\r\ncurrent->cpuset_slab_spread_rotor =\r\nnode_random(&current->mems_allowed);\r\nreturn cpuset_spread_node(&current->cpuset_slab_spread_rotor);\r\n}\r\nint cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\r\nconst struct task_struct *tsk2)\r\n{\r\nreturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\r\n}\r\nvoid cpuset_print_task_mems_allowed(struct task_struct *tsk)\r\n{\r\nstruct dentry *dentry;\r\ndentry = task_cs(tsk)->css.cgroup->dentry;\r\nspin_lock(&cpuset_buffer_lock);\r\nsnprintf(cpuset_name, CPUSET_NAME_LEN,\r\ndentry ? (const char *)dentry->d_name.name : "/");\r\nnodelist_scnprintf(cpuset_nodelist, CPUSET_NODELIST_LEN,\r\ntsk->mems_allowed);\r\nprintk(KERN_INFO "%s cpuset=%s mems_allowed=%s\n",\r\ntsk->comm, cpuset_name, cpuset_nodelist);\r\nspin_unlock(&cpuset_buffer_lock);\r\n}\r\nvoid __cpuset_memory_pressure_bump(void)\r\n{\r\ntask_lock(current);\r\nfmeter_markevent(&task_cs(current)->fmeter);\r\ntask_unlock(current);\r\n}\r\nstatic int proc_cpuset_show(struct seq_file *m, void *unused_v)\r\n{\r\nstruct pid *pid;\r\nstruct task_struct *tsk;\r\nchar *buf;\r\nstruct cgroup_subsys_state *css;\r\nint retval;\r\nretval = -ENOMEM;\r\nbuf = kmalloc(PAGE_SIZE, GFP_KERNEL);\r\nif (!buf)\r\ngoto out;\r\nretval = -ESRCH;\r\npid = m->private;\r\ntsk = get_pid_task(pid, PIDTYPE_PID);\r\nif (!tsk)\r\ngoto out_free;\r\nretval = -EINVAL;\r\ncgroup_lock();\r\ncss = task_subsys_state(tsk, cpuset_subsys_id);\r\nretval = cgroup_path(css->cgroup, buf, PAGE_SIZE);\r\nif (retval < 0)\r\ngoto out_unlock;\r\nseq_puts(m, buf);\r\nseq_putc(m, '\n');\r\nout_unlock:\r\ncgroup_unlock();\r\nput_task_struct(tsk);\r\nout_free:\r\nkfree(buf);\r\nout:\r\nreturn retval;\r\n}\r\nstatic int cpuset_open(struct inode *inode, struct file *file)\r\n{\r\nstruct pid *pid = PROC_I(inode)->pid;\r\nreturn single_open(file, proc_cpuset_show, pid);\r\n}\r\nvoid cpuset_task_status_allowed(struct seq_file *m, struct task_struct *task)\r\n{\r\nseq_printf(m, "Mems_allowed:\t");\r\nseq_nodemask(m, &task->mems_allowed);\r\nseq_printf(m, "\n");\r\nseq_printf(m, "Mems_allowed_list:\t");\r\nseq_nodemask_list(m, &task->mems_allowed);\r\nseq_printf(m, "\n");\r\n}
