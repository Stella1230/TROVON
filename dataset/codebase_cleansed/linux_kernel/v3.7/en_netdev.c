static int mlx4_en_setup_tc(struct net_device *dev, u8 up)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nint i;\r\nunsigned int q, offset = 0;\r\nif (up && up != MLX4_EN_NUM_UP)\r\nreturn -EINVAL;\r\nnetdev_set_num_tc(dev, up);\r\nq = priv->tx_ring_num / up;\r\nfor (i = 0; i < up; i++) {\r\nnetdev_set_tc_queue(dev, i, q, offset);\r\noffset += q;\r\n}\r\nreturn 0;\r\n}\r\nstatic void mlx4_en_filter_work(struct work_struct *work)\r\n{\r\nstruct mlx4_en_filter *filter = container_of(work,\r\nstruct mlx4_en_filter,\r\nwork);\r\nstruct mlx4_en_priv *priv = filter->priv;\r\nstruct mlx4_spec_list spec_tcp = {\r\n.id = MLX4_NET_TRANS_RULE_ID_TCP,\r\n{\r\n.tcp_udp = {\r\n.dst_port = filter->dst_port,\r\n.dst_port_msk = (__force __be16)-1,\r\n.src_port = filter->src_port,\r\n.src_port_msk = (__force __be16)-1,\r\n},\r\n},\r\n};\r\nstruct mlx4_spec_list spec_ip = {\r\n.id = MLX4_NET_TRANS_RULE_ID_IPV4,\r\n{\r\n.ipv4 = {\r\n.dst_ip = filter->dst_ip,\r\n.dst_ip_msk = (__force __be32)-1,\r\n.src_ip = filter->src_ip,\r\n.src_ip_msk = (__force __be32)-1,\r\n},\r\n},\r\n};\r\nstruct mlx4_spec_list spec_eth = {\r\n.id = MLX4_NET_TRANS_RULE_ID_ETH,\r\n};\r\nstruct mlx4_net_trans_rule rule = {\r\n.list = LIST_HEAD_INIT(rule.list),\r\n.queue_mode = MLX4_NET_TRANS_Q_LIFO,\r\n.exclusive = 1,\r\n.allow_loopback = 1,\r\n.promisc_mode = MLX4_FS_PROMISC_NONE,\r\n.port = priv->port,\r\n.priority = MLX4_DOMAIN_RFS,\r\n};\r\nint rc;\r\n__be64 mac;\r\n__be64 mac_mask = cpu_to_be64(MLX4_MAC_MASK << 16);\r\nlist_add_tail(&spec_eth.list, &rule.list);\r\nlist_add_tail(&spec_ip.list, &rule.list);\r\nlist_add_tail(&spec_tcp.list, &rule.list);\r\nmac = cpu_to_be64((priv->mac & MLX4_MAC_MASK) << 16);\r\nrule.qpn = priv->rss_map.qps[filter->rxq_index].qpn;\r\nmemcpy(spec_eth.eth.dst_mac, &mac, ETH_ALEN);\r\nmemcpy(spec_eth.eth.dst_mac_msk, &mac_mask, ETH_ALEN);\r\nfilter->activated = 0;\r\nif (filter->reg_id) {\r\nrc = mlx4_flow_detach(priv->mdev->dev, filter->reg_id);\r\nif (rc && rc != -ENOENT)\r\nen_err(priv, "Error detaching flow. rc = %d\n", rc);\r\n}\r\nrc = mlx4_flow_attach(priv->mdev->dev, &rule, &filter->reg_id);\r\nif (rc)\r\nen_err(priv, "Error attaching flow. err = %d\n", rc);\r\nmlx4_en_filter_rfs_expire(priv);\r\nfilter->activated = 1;\r\n}\r\nstatic inline struct hlist_head *\r\nfilter_hash_bucket(struct mlx4_en_priv *priv, __be32 src_ip, __be32 dst_ip,\r\n__be16 src_port, __be16 dst_port)\r\n{\r\nunsigned long l;\r\nint bucket_idx;\r\nl = (__force unsigned long)src_port |\r\n((__force unsigned long)dst_port << 2);\r\nl ^= (__force unsigned long)(src_ip ^ dst_ip);\r\nbucket_idx = hash_long(l, MLX4_EN_FILTER_HASH_SHIFT);\r\nreturn &priv->filter_hash[bucket_idx];\r\n}\r\nstatic struct mlx4_en_filter *\r\nmlx4_en_filter_alloc(struct mlx4_en_priv *priv, int rxq_index, __be32 src_ip,\r\n__be32 dst_ip, __be16 src_port, __be16 dst_port,\r\nu32 flow_id)\r\n{\r\nstruct mlx4_en_filter *filter = NULL;\r\nfilter = kzalloc(sizeof(struct mlx4_en_filter), GFP_ATOMIC);\r\nif (!filter)\r\nreturn NULL;\r\nfilter->priv = priv;\r\nfilter->rxq_index = rxq_index;\r\nINIT_WORK(&filter->work, mlx4_en_filter_work);\r\nfilter->src_ip = src_ip;\r\nfilter->dst_ip = dst_ip;\r\nfilter->src_port = src_port;\r\nfilter->dst_port = dst_port;\r\nfilter->flow_id = flow_id;\r\nfilter->id = priv->last_filter_id++ % RPS_NO_FILTER;\r\nlist_add_tail(&filter->next, &priv->filters);\r\nhlist_add_head(&filter->filter_chain,\r\nfilter_hash_bucket(priv, src_ip, dst_ip, src_port,\r\ndst_port));\r\nreturn filter;\r\n}\r\nstatic void mlx4_en_filter_free(struct mlx4_en_filter *filter)\r\n{\r\nstruct mlx4_en_priv *priv = filter->priv;\r\nint rc;\r\nlist_del(&filter->next);\r\nrc = mlx4_flow_detach(priv->mdev->dev, filter->reg_id);\r\nif (rc && rc != -ENOENT)\r\nen_err(priv, "Error detaching flow. rc = %d\n", rc);\r\nkfree(filter);\r\n}\r\nstatic inline struct mlx4_en_filter *\r\nmlx4_en_filter_find(struct mlx4_en_priv *priv, __be32 src_ip, __be32 dst_ip,\r\n__be16 src_port, __be16 dst_port)\r\n{\r\nstruct hlist_node *elem;\r\nstruct mlx4_en_filter *filter;\r\nstruct mlx4_en_filter *ret = NULL;\r\nhlist_for_each_entry(filter, elem,\r\nfilter_hash_bucket(priv, src_ip, dst_ip,\r\nsrc_port, dst_port),\r\nfilter_chain) {\r\nif (filter->src_ip == src_ip &&\r\nfilter->dst_ip == dst_ip &&\r\nfilter->src_port == src_port &&\r\nfilter->dst_port == dst_port) {\r\nret = filter;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nmlx4_en_filter_rfs(struct net_device *net_dev, const struct sk_buff *skb,\r\nu16 rxq_index, u32 flow_id)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(net_dev);\r\nstruct mlx4_en_filter *filter;\r\nconst struct iphdr *ip;\r\nconst __be16 *ports;\r\n__be32 src_ip;\r\n__be32 dst_ip;\r\n__be16 src_port;\r\n__be16 dst_port;\r\nint nhoff = skb_network_offset(skb);\r\nint ret = 0;\r\nif (skb->protocol != htons(ETH_P_IP))\r\nreturn -EPROTONOSUPPORT;\r\nip = (const struct iphdr *)(skb->data + nhoff);\r\nif (ip_is_fragment(ip))\r\nreturn -EPROTONOSUPPORT;\r\nports = (const __be16 *)(skb->data + nhoff + 4 * ip->ihl);\r\nsrc_ip = ip->saddr;\r\ndst_ip = ip->daddr;\r\nsrc_port = ports[0];\r\ndst_port = ports[1];\r\nif (ip->protocol != IPPROTO_TCP)\r\nreturn -EPROTONOSUPPORT;\r\nspin_lock_bh(&priv->filters_lock);\r\nfilter = mlx4_en_filter_find(priv, src_ip, dst_ip, src_port, dst_port);\r\nif (filter) {\r\nif (filter->rxq_index == rxq_index)\r\ngoto out;\r\nfilter->rxq_index = rxq_index;\r\n} else {\r\nfilter = mlx4_en_filter_alloc(priv, rxq_index,\r\nsrc_ip, dst_ip,\r\nsrc_port, dst_port, flow_id);\r\nif (!filter) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\n}\r\nqueue_work(priv->mdev->workqueue, &filter->work);\r\nout:\r\nret = filter->id;\r\nerr:\r\nspin_unlock_bh(&priv->filters_lock);\r\nreturn ret;\r\n}\r\nvoid mlx4_en_cleanup_filters(struct mlx4_en_priv *priv,\r\nstruct mlx4_en_rx_ring *rx_ring)\r\n{\r\nstruct mlx4_en_filter *filter, *tmp;\r\nLIST_HEAD(del_list);\r\nspin_lock_bh(&priv->filters_lock);\r\nlist_for_each_entry_safe(filter, tmp, &priv->filters, next) {\r\nlist_move(&filter->next, &del_list);\r\nhlist_del(&filter->filter_chain);\r\n}\r\nspin_unlock_bh(&priv->filters_lock);\r\nlist_for_each_entry_safe(filter, tmp, &del_list, next) {\r\ncancel_work_sync(&filter->work);\r\nmlx4_en_filter_free(filter);\r\n}\r\n}\r\nstatic void mlx4_en_filter_rfs_expire(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_filter *filter = NULL, *tmp, *last_filter = NULL;\r\nLIST_HEAD(del_list);\r\nint i = 0;\r\nspin_lock_bh(&priv->filters_lock);\r\nlist_for_each_entry_safe(filter, tmp, &priv->filters, next) {\r\nif (i > MLX4_EN_FILTER_EXPIRY_QUOTA)\r\nbreak;\r\nif (filter->activated &&\r\n!work_pending(&filter->work) &&\r\nrps_may_expire_flow(priv->dev,\r\nfilter->rxq_index, filter->flow_id,\r\nfilter->id)) {\r\nlist_move(&filter->next, &del_list);\r\nhlist_del(&filter->filter_chain);\r\n} else\r\nlast_filter = filter;\r\ni++;\r\n}\r\nif (last_filter && (&last_filter->next != priv->filters.next))\r\nlist_move(&priv->filters, &last_filter->next);\r\nspin_unlock_bh(&priv->filters_lock);\r\nlist_for_each_entry_safe(filter, tmp, &del_list, next)\r\nmlx4_en_filter_free(filter);\r\n}\r\nstatic int mlx4_en_vlan_rx_add_vid(struct net_device *dev, unsigned short vid)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err;\r\nint idx;\r\nen_dbg(HW, priv, "adding VLAN:%d\n", vid);\r\nset_bit(vid, priv->active_vlans);\r\nmutex_lock(&mdev->state_lock);\r\nif (mdev->device_up && priv->port_up) {\r\nerr = mlx4_SET_VLAN_FLTR(mdev->dev, priv);\r\nif (err)\r\nen_err(priv, "Failed configuring VLAN filter\n");\r\n}\r\nif (mlx4_register_vlan(mdev->dev, priv->port, vid, &idx))\r\nen_err(priv, "failed adding vlan %d\n", vid);\r\nmutex_unlock(&mdev->state_lock);\r\nreturn 0;\r\n}\r\nstatic int mlx4_en_vlan_rx_kill_vid(struct net_device *dev, unsigned short vid)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err;\r\nint idx;\r\nen_dbg(HW, priv, "Killing VID:%d\n", vid);\r\nclear_bit(vid, priv->active_vlans);\r\nmutex_lock(&mdev->state_lock);\r\nif (!mlx4_find_cached_vlan(mdev->dev, priv->port, vid, &idx))\r\nmlx4_unregister_vlan(mdev->dev, priv->port, idx);\r\nelse\r\nen_err(priv, "could not find vid %d in cache\n", vid);\r\nif (mdev->device_up && priv->port_up) {\r\nerr = mlx4_SET_VLAN_FLTR(mdev->dev, priv);\r\nif (err)\r\nen_err(priv, "Failed configuring VLAN filter\n");\r\n}\r\nmutex_unlock(&mdev->state_lock);\r\nreturn 0;\r\n}\r\nu64 mlx4_en_mac_to_u64(u8 *addr)\r\n{\r\nu64 mac = 0;\r\nint i;\r\nfor (i = 0; i < ETH_ALEN; i++) {\r\nmac <<= 8;\r\nmac |= addr[i];\r\n}\r\nreturn mac;\r\n}\r\nstatic int mlx4_en_set_mac(struct net_device *dev, void *addr)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct sockaddr *saddr = addr;\r\nif (!is_valid_ether_addr(saddr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(dev->dev_addr, saddr->sa_data, ETH_ALEN);\r\npriv->mac = mlx4_en_mac_to_u64(dev->dev_addr);\r\nqueue_work(mdev->workqueue, &priv->mac_task);\r\nreturn 0;\r\n}\r\nstatic void mlx4_en_do_set_mac(struct work_struct *work)\r\n{\r\nstruct mlx4_en_priv *priv = container_of(work, struct mlx4_en_priv,\r\nmac_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err = 0;\r\nmutex_lock(&mdev->state_lock);\r\nif (priv->port_up) {\r\nerr = mlx4_replace_mac(mdev->dev, priv->port,\r\npriv->base_qpn, priv->mac);\r\nif (err)\r\nen_err(priv, "Failed changing HW MAC address\n");\r\n} else\r\nen_dbg(HW, priv, "Port is down while "\r\n"registering mac, exiting...\n");\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nstatic void mlx4_en_clear_list(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_mc_list *tmp, *mc_to_del;\r\nlist_for_each_entry_safe(mc_to_del, tmp, &priv->mc_list, list) {\r\nlist_del(&mc_to_del->list);\r\nkfree(mc_to_del);\r\n}\r\n}\r\nstatic void mlx4_en_cache_mclist(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct netdev_hw_addr *ha;\r\nstruct mlx4_en_mc_list *tmp;\r\nmlx4_en_clear_list(dev);\r\nnetdev_for_each_mc_addr(ha, dev) {\r\ntmp = kzalloc(sizeof(struct mlx4_en_mc_list), GFP_ATOMIC);\r\nif (!tmp) {\r\nen_err(priv, "failed to allocate multicast list\n");\r\nmlx4_en_clear_list(dev);\r\nreturn;\r\n}\r\nmemcpy(tmp->addr, ha->addr, ETH_ALEN);\r\nlist_add_tail(&tmp->list, &priv->mc_list);\r\n}\r\n}\r\nstatic void update_mclist_flags(struct mlx4_en_priv *priv,\r\nstruct list_head *dst,\r\nstruct list_head *src)\r\n{\r\nstruct mlx4_en_mc_list *dst_tmp, *src_tmp, *new_mc;\r\nbool found;\r\nlist_for_each_entry(dst_tmp, dst, list) {\r\nfound = false;\r\nlist_for_each_entry(src_tmp, src, list) {\r\nif (!memcmp(dst_tmp->addr, src_tmp->addr, ETH_ALEN)) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found)\r\ndst_tmp->action = MCLIST_REM;\r\n}\r\nlist_for_each_entry(src_tmp, src, list) {\r\nfound = false;\r\nlist_for_each_entry(dst_tmp, dst, list) {\r\nif (!memcmp(dst_tmp->addr, src_tmp->addr, ETH_ALEN)) {\r\ndst_tmp->action = MCLIST_NONE;\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nnew_mc = kmalloc(sizeof(struct mlx4_en_mc_list),\r\nGFP_KERNEL);\r\nif (!new_mc) {\r\nen_err(priv, "Failed to allocate current multicast list\n");\r\nreturn;\r\n}\r\nmemcpy(new_mc, src_tmp,\r\nsizeof(struct mlx4_en_mc_list));\r\nnew_mc->action = MCLIST_ADD;\r\nlist_add_tail(&new_mc->list, dst);\r\n}\r\n}\r\n}\r\nstatic void mlx4_en_set_multicast(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nif (!priv->port_up)\r\nreturn;\r\nqueue_work(priv->mdev->workqueue, &priv->mcast_task);\r\n}\r\nstatic void mlx4_en_do_set_multicast(struct work_struct *work)\r\n{\r\nstruct mlx4_en_priv *priv = container_of(work, struct mlx4_en_priv,\r\nmcast_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct net_device *dev = priv->dev;\r\nstruct mlx4_en_mc_list *mclist, *tmp;\r\nu64 mcast_addr = 0;\r\nu8 mc_list[16] = {0};\r\nint err = 0;\r\nmutex_lock(&mdev->state_lock);\r\nif (!mdev->device_up) {\r\nen_dbg(HW, priv, "Card is not up, "\r\n"ignoring multicast change.\n");\r\ngoto out;\r\n}\r\nif (!priv->port_up) {\r\nen_dbg(HW, priv, "Port is down, "\r\n"ignoring multicast change.\n");\r\ngoto out;\r\n}\r\nif (!netif_carrier_ok(dev)) {\r\nif (!mlx4_en_QUERY_PORT(mdev, priv->port)) {\r\nif (priv->port_state.link_state) {\r\npriv->last_link_state = MLX4_DEV_EVENT_PORT_UP;\r\nnetif_carrier_on(dev);\r\nen_dbg(LINK, priv, "Link Up\n");\r\n}\r\n}\r\n}\r\nif (dev->flags & IFF_PROMISC) {\r\nif (!(priv->flags & MLX4_EN_FLAG_PROMISC)) {\r\nif (netif_msg_rx_status(priv))\r\nen_warn(priv, "Entering promiscuous mode\n");\r\npriv->flags |= MLX4_EN_FLAG_PROMISC;\r\nswitch (mdev->dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED:\r\nerr = mlx4_flow_steer_promisc_add(mdev->dev,\r\npriv->port,\r\npriv->base_qpn,\r\nMLX4_FS_PROMISC_UPLINK);\r\nif (err)\r\nen_err(priv, "Failed enabling promiscuous mode\n");\r\npriv->flags |= MLX4_EN_FLAG_MC_PROMISC;\r\nbreak;\r\ncase MLX4_STEERING_MODE_B0:\r\nerr = mlx4_unicast_promisc_add(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nif (err)\r\nen_err(priv, "Failed enabling unicast promiscuous mode\n");\r\nif (!(priv->flags & MLX4_EN_FLAG_MC_PROMISC)) {\r\nerr = mlx4_multicast_promisc_add(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nif (err)\r\nen_err(priv, "Failed enabling multicast promiscuous mode\n");\r\npriv->flags |= MLX4_EN_FLAG_MC_PROMISC;\r\n}\r\nbreak;\r\ncase MLX4_STEERING_MODE_A0:\r\nerr = mlx4_SET_PORT_qpn_calc(mdev->dev,\r\npriv->port,\r\npriv->base_qpn,\r\n1);\r\nif (err)\r\nen_err(priv, "Failed enabling promiscuous mode\n");\r\nbreak;\r\n}\r\nerr = mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0,\r\n0, MLX4_MCAST_DISABLE);\r\nif (err)\r\nen_err(priv, "Failed disabling "\r\n"multicast filter\n");\r\nerr = mlx4_SET_VLAN_FLTR(mdev->dev, priv);\r\nif (err)\r\nen_err(priv, "Failed disabling VLAN filter\n");\r\n}\r\ngoto out;\r\n}\r\nif (priv->flags & MLX4_EN_FLAG_PROMISC) {\r\nif (netif_msg_rx_status(priv))\r\nen_warn(priv, "Leaving promiscuous mode\n");\r\npriv->flags &= ~MLX4_EN_FLAG_PROMISC;\r\nswitch (mdev->dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED:\r\nerr = mlx4_flow_steer_promisc_remove(mdev->dev,\r\npriv->port,\r\nMLX4_FS_PROMISC_UPLINK);\r\nif (err)\r\nen_err(priv, "Failed disabling promiscuous mode\n");\r\npriv->flags &= ~MLX4_EN_FLAG_MC_PROMISC;\r\nbreak;\r\ncase MLX4_STEERING_MODE_B0:\r\nerr = mlx4_unicast_promisc_remove(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nif (err)\r\nen_err(priv, "Failed disabling unicast promiscuous mode\n");\r\nif (priv->flags & MLX4_EN_FLAG_MC_PROMISC) {\r\nerr = mlx4_multicast_promisc_remove(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nif (err)\r\nen_err(priv, "Failed disabling multicast promiscuous mode\n");\r\npriv->flags &= ~MLX4_EN_FLAG_MC_PROMISC;\r\n}\r\nbreak;\r\ncase MLX4_STEERING_MODE_A0:\r\nerr = mlx4_SET_PORT_qpn_calc(mdev->dev,\r\npriv->port,\r\npriv->base_qpn, 0);\r\nif (err)\r\nen_err(priv, "Failed disabling promiscuous mode\n");\r\nbreak;\r\n}\r\nerr = mlx4_SET_VLAN_FLTR(mdev->dev, priv);\r\nif (err)\r\nen_err(priv, "Failed enabling VLAN filter\n");\r\n}\r\nif (dev->flags & IFF_ALLMULTI) {\r\nerr = mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0,\r\n0, MLX4_MCAST_DISABLE);\r\nif (err)\r\nen_err(priv, "Failed disabling multicast filter\n");\r\nif (!(priv->flags & MLX4_EN_FLAG_MC_PROMISC)) {\r\nswitch (mdev->dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED:\r\nerr = mlx4_flow_steer_promisc_add(mdev->dev,\r\npriv->port,\r\npriv->base_qpn,\r\nMLX4_FS_PROMISC_ALL_MULTI);\r\nbreak;\r\ncase MLX4_STEERING_MODE_B0:\r\nerr = mlx4_multicast_promisc_add(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nbreak;\r\ncase MLX4_STEERING_MODE_A0:\r\nbreak;\r\n}\r\nif (err)\r\nen_err(priv, "Failed entering multicast promisc mode\n");\r\npriv->flags |= MLX4_EN_FLAG_MC_PROMISC;\r\n}\r\n} else {\r\nif (priv->flags & MLX4_EN_FLAG_MC_PROMISC) {\r\nswitch (mdev->dev->caps.steering_mode) {\r\ncase MLX4_STEERING_MODE_DEVICE_MANAGED:\r\nerr = mlx4_flow_steer_promisc_remove(mdev->dev,\r\npriv->port,\r\nMLX4_FS_PROMISC_ALL_MULTI);\r\nbreak;\r\ncase MLX4_STEERING_MODE_B0:\r\nerr = mlx4_multicast_promisc_remove(mdev->dev,\r\npriv->base_qpn,\r\npriv->port);\r\nbreak;\r\ncase MLX4_STEERING_MODE_A0:\r\nbreak;\r\n}\r\nif (err)\r\nen_err(priv, "Failed disabling multicast promiscuous mode\n");\r\npriv->flags &= ~MLX4_EN_FLAG_MC_PROMISC;\r\n}\r\nerr = mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0,\r\n0, MLX4_MCAST_DISABLE);\r\nif (err)\r\nen_err(priv, "Failed disabling multicast filter\n");\r\nmlx4_SET_MCAST_FLTR(mdev->dev, priv->port, ETH_BCAST,\r\n1, MLX4_MCAST_CONFIG);\r\nnetif_tx_lock_bh(dev);\r\nmlx4_en_cache_mclist(dev);\r\nnetif_tx_unlock_bh(dev);\r\nlist_for_each_entry(mclist, &priv->mc_list, list) {\r\nmcast_addr = mlx4_en_mac_to_u64(mclist->addr);\r\nmlx4_SET_MCAST_FLTR(mdev->dev, priv->port,\r\nmcast_addr, 0, MLX4_MCAST_CONFIG);\r\n}\r\nerr = mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0,\r\n0, MLX4_MCAST_ENABLE);\r\nif (err)\r\nen_err(priv, "Failed enabling multicast filter\n");\r\nupdate_mclist_flags(priv, &priv->curr_list, &priv->mc_list);\r\nlist_for_each_entry_safe(mclist, tmp, &priv->curr_list, list) {\r\nif (mclist->action == MCLIST_REM) {\r\nmemcpy(&mc_list[10], mclist->addr, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nerr = mlx4_multicast_detach(mdev->dev,\r\n&priv->rss_map.indir_qp,\r\nmc_list,\r\nMLX4_PROT_ETH,\r\nmclist->reg_id);\r\nif (err)\r\nen_err(priv, "Fail to detach multicast address\n");\r\nlist_del(&mclist->list);\r\nkfree(mclist);\r\n} else if (mclist->action == MCLIST_ADD) {\r\nmemcpy(&mc_list[10], mclist->addr, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nerr = mlx4_multicast_attach(mdev->dev,\r\n&priv->rss_map.indir_qp,\r\nmc_list,\r\npriv->port, 0,\r\nMLX4_PROT_ETH,\r\n&mclist->reg_id);\r\nif (err)\r\nen_err(priv, "Fail to attach multicast address\n");\r\n}\r\n}\r\n}\r\nout:\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nstatic void mlx4_en_netpoll(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_cq *cq;\r\nunsigned long flags;\r\nint i;\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\ncq = &priv->rx_cq[i];\r\nspin_lock_irqsave(&cq->lock, flags);\r\nnapi_synchronize(&cq->napi);\r\nmlx4_en_process_rx_cq(dev, cq, 0);\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\n}\r\n}\r\nstatic void mlx4_en_tx_timeout(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nif (netif_msg_timer(priv))\r\nen_warn(priv, "Tx timeout called on port:%d\n", priv->port);\r\npriv->port_stats.tx_timeout++;\r\nen_dbg(DRV, priv, "Scheduling watchdog\n");\r\nqueue_work(mdev->workqueue, &priv->watchdog_task);\r\n}\r\nstatic struct net_device_stats *mlx4_en_get_stats(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nspin_lock_bh(&priv->stats_lock);\r\nmemcpy(&priv->ret_stats, &priv->stats, sizeof(priv->stats));\r\nspin_unlock_bh(&priv->stats_lock);\r\nreturn &priv->ret_stats;\r\n}\r\nstatic void mlx4_en_set_default_moderation(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_cq *cq;\r\nint i;\r\npriv->rx_frames = MLX4_EN_RX_COAL_TARGET;\r\npriv->rx_usecs = MLX4_EN_RX_COAL_TIME;\r\npriv->tx_frames = MLX4_EN_TX_COAL_PKTS;\r\npriv->tx_usecs = MLX4_EN_TX_COAL_TIME;\r\nen_dbg(INTR, priv, "Default coalesing params for mtu:%d - "\r\n"rx_frames:%d rx_usecs:%d\n",\r\npriv->dev->mtu, priv->rx_frames, priv->rx_usecs);\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\ncq = &priv->rx_cq[i];\r\ncq->moder_cnt = priv->rx_frames;\r\ncq->moder_time = priv->rx_usecs;\r\npriv->last_moder_time[i] = MLX4_EN_AUTO_CONF;\r\npriv->last_moder_packets[i] = 0;\r\npriv->last_moder_bytes[i] = 0;\r\n}\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\ncq = &priv->tx_cq[i];\r\ncq->moder_cnt = priv->tx_frames;\r\ncq->moder_time = priv->tx_usecs;\r\n}\r\npriv->pkt_rate_low = MLX4_EN_RX_RATE_LOW;\r\npriv->rx_usecs_low = MLX4_EN_RX_COAL_TIME_LOW;\r\npriv->pkt_rate_high = MLX4_EN_RX_RATE_HIGH;\r\npriv->rx_usecs_high = MLX4_EN_RX_COAL_TIME_HIGH;\r\npriv->sample_interval = MLX4_EN_SAMPLE_INTERVAL;\r\npriv->adaptive_rx_coal = 1;\r\npriv->last_moder_jiffies = 0;\r\npriv->last_moder_tx_packets = 0;\r\n}\r\nstatic void mlx4_en_auto_moderation(struct mlx4_en_priv *priv)\r\n{\r\nunsigned long period = (unsigned long) (jiffies - priv->last_moder_jiffies);\r\nstruct mlx4_en_cq *cq;\r\nunsigned long packets;\r\nunsigned long rate;\r\nunsigned long avg_pkt_size;\r\nunsigned long rx_packets;\r\nunsigned long rx_bytes;\r\nunsigned long rx_pkt_diff;\r\nint moder_time;\r\nint ring, err;\r\nif (!priv->adaptive_rx_coal || period < priv->sample_interval * HZ)\r\nreturn;\r\nfor (ring = 0; ring < priv->rx_ring_num; ring++) {\r\nspin_lock_bh(&priv->stats_lock);\r\nrx_packets = priv->rx_ring[ring].packets;\r\nrx_bytes = priv->rx_ring[ring].bytes;\r\nspin_unlock_bh(&priv->stats_lock);\r\nrx_pkt_diff = ((unsigned long) (rx_packets -\r\npriv->last_moder_packets[ring]));\r\npackets = rx_pkt_diff;\r\nrate = packets * HZ / period;\r\navg_pkt_size = packets ? ((unsigned long) (rx_bytes -\r\npriv->last_moder_bytes[ring])) / packets : 0;\r\nif (rate > (MLX4_EN_RX_RATE_THRESH / priv->rx_ring_num) &&\r\navg_pkt_size > MLX4_EN_AVG_PKT_SMALL) {\r\nif (rate < priv->pkt_rate_low)\r\nmoder_time = priv->rx_usecs_low;\r\nelse if (rate > priv->pkt_rate_high)\r\nmoder_time = priv->rx_usecs_high;\r\nelse\r\nmoder_time = (rate - priv->pkt_rate_low) *\r\n(priv->rx_usecs_high - priv->rx_usecs_low) /\r\n(priv->pkt_rate_high - priv->pkt_rate_low) +\r\npriv->rx_usecs_low;\r\n} else {\r\nmoder_time = priv->rx_usecs_low;\r\n}\r\nif (moder_time != priv->last_moder_time[ring]) {\r\npriv->last_moder_time[ring] = moder_time;\r\ncq = &priv->rx_cq[ring];\r\ncq->moder_time = moder_time;\r\nerr = mlx4_en_set_cq_moder(priv, cq);\r\nif (err)\r\nen_err(priv, "Failed modifying moderation "\r\n"for cq:%d\n", ring);\r\n}\r\npriv->last_moder_packets[ring] = rx_packets;\r\npriv->last_moder_bytes[ring] = rx_bytes;\r\n}\r\npriv->last_moder_jiffies = jiffies;\r\n}\r\nstatic void mlx4_en_do_get_stats(struct work_struct *work)\r\n{\r\nstruct delayed_work *delay = to_delayed_work(work);\r\nstruct mlx4_en_priv *priv = container_of(delay, struct mlx4_en_priv,\r\nstats_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err;\r\nerr = mlx4_en_DUMP_ETH_STATS(mdev, priv->port, 0);\r\nif (err)\r\nen_dbg(HW, priv, "Could not update stats\n");\r\nmutex_lock(&mdev->state_lock);\r\nif (mdev->device_up) {\r\nif (priv->port_up)\r\nmlx4_en_auto_moderation(priv);\r\nqueue_delayed_work(mdev->workqueue, &priv->stats_task, STATS_DELAY);\r\n}\r\nif (mdev->mac_removed[MLX4_MAX_PORTS + 1 - priv->port]) {\r\nqueue_work(mdev->workqueue, &priv->mac_task);\r\nmdev->mac_removed[MLX4_MAX_PORTS + 1 - priv->port] = 0;\r\n}\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nstatic void mlx4_en_linkstate(struct work_struct *work)\r\n{\r\nstruct mlx4_en_priv *priv = container_of(work, struct mlx4_en_priv,\r\nlinkstate_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint linkstate = priv->link_state;\r\nmutex_lock(&mdev->state_lock);\r\nif (priv->last_link_state != linkstate) {\r\nif (linkstate == MLX4_DEV_EVENT_PORT_DOWN) {\r\nen_info(priv, "Link Down\n");\r\nnetif_carrier_off(priv->dev);\r\n} else {\r\nen_info(priv, "Link Up\n");\r\nnetif_carrier_on(priv->dev);\r\n}\r\n}\r\npriv->last_link_state = linkstate;\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nint mlx4_en_start_port(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_en_cq *cq;\r\nstruct mlx4_en_tx_ring *tx_ring;\r\nint rx_index = 0;\r\nint tx_index = 0;\r\nint err = 0;\r\nint i;\r\nint j;\r\nu8 mc_list[16] = {0};\r\nif (priv->port_up) {\r\nen_dbg(DRV, priv, "start port called while port already up\n");\r\nreturn 0;\r\n}\r\nINIT_LIST_HEAD(&priv->mc_list);\r\nINIT_LIST_HEAD(&priv->curr_list);\r\ndev->mtu = min(dev->mtu, priv->max_mtu);\r\nmlx4_en_calc_rx_buf(dev);\r\nen_dbg(DRV, priv, "Rx buf size:%d\n", priv->rx_skb_size);\r\nerr = mlx4_en_activate_rx_rings(priv);\r\nif (err) {\r\nen_err(priv, "Failed to activate RX rings\n");\r\nreturn err;\r\n}\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\ncq = &priv->rx_cq[i];\r\nerr = mlx4_en_activate_cq(priv, cq, i);\r\nif (err) {\r\nen_err(priv, "Failed activating Rx CQ\n");\r\ngoto cq_err;\r\n}\r\nfor (j = 0; j < cq->size; j++)\r\ncq->buf[j].owner_sr_opcode = MLX4_CQE_OWNER_MASK;\r\nerr = mlx4_en_set_cq_moder(priv, cq);\r\nif (err) {\r\nen_err(priv, "Failed setting cq moderation parameters");\r\nmlx4_en_deactivate_cq(priv, cq);\r\ngoto cq_err;\r\n}\r\nmlx4_en_arm_cq(priv, cq);\r\npriv->rx_ring[i].cqn = cq->mcq.cqn;\r\n++rx_index;\r\n}\r\nen_dbg(DRV, priv, "Getting qp number for port %d\n", priv->port);\r\nerr = mlx4_get_eth_qp(mdev->dev, priv->port,\r\npriv->mac, &priv->base_qpn);\r\nif (err) {\r\nen_err(priv, "Failed getting eth qp\n");\r\ngoto cq_err;\r\n}\r\nmdev->mac_removed[priv->port] = 0;\r\nerr = mlx4_en_config_rss_steer(priv);\r\nif (err) {\r\nen_err(priv, "Failed configuring rss steering\n");\r\ngoto mac_err;\r\n}\r\nerr = mlx4_en_create_drop_qp(priv);\r\nif (err)\r\ngoto rss_err;\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\ncq = &priv->tx_cq[i];\r\nerr = mlx4_en_activate_cq(priv, cq, i);\r\nif (err) {\r\nen_err(priv, "Failed allocating Tx CQ\n");\r\ngoto tx_err;\r\n}\r\nerr = mlx4_en_set_cq_moder(priv, cq);\r\nif (err) {\r\nen_err(priv, "Failed setting cq moderation parameters");\r\nmlx4_en_deactivate_cq(priv, cq);\r\ngoto tx_err;\r\n}\r\nen_dbg(DRV, priv, "Resetting index of collapsed CQ:%d to -1\n", i);\r\ncq->buf->wqe_index = cpu_to_be16(0xffff);\r\ntx_ring = &priv->tx_ring[i];\r\nerr = mlx4_en_activate_tx_ring(priv, tx_ring, cq->mcq.cqn,\r\ni / priv->mdev->profile.num_tx_rings_p_up);\r\nif (err) {\r\nen_err(priv, "Failed allocating Tx ring\n");\r\nmlx4_en_deactivate_cq(priv, cq);\r\ngoto tx_err;\r\n}\r\ntx_ring->tx_queue = netdev_get_tx_queue(dev, i);\r\nmlx4_en_arm_cq(priv, cq);\r\nfor (j = 0; j < tx_ring->buf_size; j += STAMP_STRIDE)\r\n*((u32 *) (tx_ring->buf + j)) = 0xffffffff;\r\n++tx_index;\r\n}\r\nerr = mlx4_SET_PORT_general(mdev->dev, priv->port,\r\npriv->rx_skb_size + ETH_FCS_LEN,\r\npriv->prof->tx_pause,\r\npriv->prof->tx_ppp,\r\npriv->prof->rx_pause,\r\npriv->prof->rx_ppp);\r\nif (err) {\r\nen_err(priv, "Failed setting port general configurations "\r\n"for port %d, with error %d\n", priv->port, err);\r\ngoto tx_err;\r\n}\r\nerr = mlx4_SET_PORT_qpn_calc(mdev->dev, priv->port, priv->base_qpn, 0);\r\nif (err) {\r\nen_err(priv, "Failed setting default qp numbers\n");\r\ngoto tx_err;\r\n}\r\nen_dbg(HW, priv, "Initializing port\n");\r\nerr = mlx4_INIT_PORT(mdev->dev, priv->port);\r\nif (err) {\r\nen_err(priv, "Failed Initializing port\n");\r\ngoto tx_err;\r\n}\r\nmemset(&mc_list[10], 0xff, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nif (mlx4_multicast_attach(mdev->dev, &priv->rss_map.indir_qp, mc_list,\r\npriv->port, 0, MLX4_PROT_ETH,\r\n&priv->broadcast_id))\r\nmlx4_warn(mdev, "Failed Attaching Broadcast\n");\r\npriv->flags &= ~(MLX4_EN_FLAG_PROMISC | MLX4_EN_FLAG_MC_PROMISC);\r\nif (mdev->dev->caps.steering_mode ==\r\nMLX4_STEERING_MODE_DEVICE_MANAGED) {\r\nmlx4_flow_steer_promisc_remove(mdev->dev,\r\npriv->port,\r\nMLX4_FS_PROMISC_UPLINK);\r\nmlx4_flow_steer_promisc_remove(mdev->dev,\r\npriv->port,\r\nMLX4_FS_PROMISC_ALL_MULTI);\r\n}\r\nqueue_work(mdev->workqueue, &priv->mcast_task);\r\nmlx4_set_stats_bitmap(mdev->dev, &priv->stats_bitmap);\r\npriv->port_up = true;\r\nnetif_tx_start_all_queues(dev);\r\nreturn 0;\r\ntx_err:\r\nwhile (tx_index--) {\r\nmlx4_en_deactivate_tx_ring(priv, &priv->tx_ring[tx_index]);\r\nmlx4_en_deactivate_cq(priv, &priv->tx_cq[tx_index]);\r\n}\r\nmlx4_en_destroy_drop_qp(priv);\r\nrss_err:\r\nmlx4_en_release_rss_steer(priv);\r\nmac_err:\r\nmlx4_put_eth_qp(mdev->dev, priv->port, priv->mac, priv->base_qpn);\r\ncq_err:\r\nwhile (rx_index--)\r\nmlx4_en_deactivate_cq(priv, &priv->rx_cq[rx_index]);\r\nfor (i = 0; i < priv->rx_ring_num; i++)\r\nmlx4_en_deactivate_rx_ring(priv, &priv->rx_ring[i]);\r\nreturn err;\r\n}\r\nvoid mlx4_en_stop_port(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct mlx4_en_mc_list *mclist, *tmp;\r\nint i;\r\nu8 mc_list[16] = {0};\r\nif (!priv->port_up) {\r\nen_dbg(DRV, priv, "stop port called while port already down\n");\r\nreturn;\r\n}\r\nnetif_tx_lock_bh(dev);\r\nnetif_tx_stop_all_queues(dev);\r\nnetif_tx_unlock_bh(dev);\r\npriv->port_up = false;\r\nmemset(&mc_list[10], 0xff, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nmlx4_multicast_detach(mdev->dev, &priv->rss_map.indir_qp, mc_list,\r\nMLX4_PROT_ETH, priv->broadcast_id);\r\nlist_for_each_entry(mclist, &priv->curr_list, list) {\r\nmemcpy(&mc_list[10], mclist->addr, ETH_ALEN);\r\nmc_list[5] = priv->port;\r\nmlx4_multicast_detach(mdev->dev, &priv->rss_map.indir_qp,\r\nmc_list, MLX4_PROT_ETH, mclist->reg_id);\r\n}\r\nmlx4_en_clear_list(dev);\r\nlist_for_each_entry_safe(mclist, tmp, &priv->curr_list, list) {\r\nlist_del(&mclist->list);\r\nkfree(mclist);\r\n}\r\nmlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0, 1, MLX4_MCAST_CONFIG);\r\nmlx4_en_destroy_drop_qp(priv);\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\nmlx4_en_deactivate_tx_ring(priv, &priv->tx_ring[i]);\r\nmlx4_en_deactivate_cq(priv, &priv->tx_cq[i]);\r\n}\r\nmsleep(10);\r\nfor (i = 0; i < priv->tx_ring_num; i++)\r\nmlx4_en_free_tx_buf(dev, &priv->tx_ring[i]);\r\nmlx4_en_release_rss_steer(priv);\r\nmlx4_put_eth_qp(mdev->dev, priv->port, priv->mac, priv->base_qpn);\r\nmdev->mac_removed[priv->port] = 1;\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\nmlx4_en_deactivate_rx_ring(priv, &priv->rx_ring[i]);\r\nwhile (test_bit(NAPI_STATE_SCHED, &priv->rx_cq[i].napi.state))\r\nmsleep(1);\r\nmlx4_en_deactivate_cq(priv, &priv->rx_cq[i]);\r\n}\r\nmlx4_CLOSE_PORT(mdev->dev, priv->port);\r\n}\r\nstatic void mlx4_en_restart(struct work_struct *work)\r\n{\r\nstruct mlx4_en_priv *priv = container_of(work, struct mlx4_en_priv,\r\nwatchdog_task);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nstruct net_device *dev = priv->dev;\r\nint i;\r\nen_dbg(DRV, priv, "Watchdog task called for port %d\n", priv->port);\r\nmutex_lock(&mdev->state_lock);\r\nif (priv->port_up) {\r\nmlx4_en_stop_port(dev);\r\nfor (i = 0; i < priv->tx_ring_num; i++)\r\nnetdev_tx_reset_queue(priv->tx_ring[i].tx_queue);\r\nif (mlx4_en_start_port(dev))\r\nen_err(priv, "Failed restarting port %d\n", priv->port);\r\n}\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nstatic void mlx4_en_clear_stats(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint i;\r\nif (mlx4_en_DUMP_ETH_STATS(mdev, priv->port, 1))\r\nen_dbg(HW, priv, "Failed dumping statistics\n");\r\nmemset(&priv->stats, 0, sizeof(priv->stats));\r\nmemset(&priv->pstats, 0, sizeof(priv->pstats));\r\nmemset(&priv->pkstats, 0, sizeof(priv->pkstats));\r\nmemset(&priv->port_stats, 0, sizeof(priv->port_stats));\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\npriv->tx_ring[i].bytes = 0;\r\npriv->tx_ring[i].packets = 0;\r\npriv->tx_ring[i].tx_csum = 0;\r\n}\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\npriv->rx_ring[i].bytes = 0;\r\npriv->rx_ring[i].packets = 0;\r\npriv->rx_ring[i].csum_ok = 0;\r\npriv->rx_ring[i].csum_none = 0;\r\n}\r\n}\r\nstatic int mlx4_en_open(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err = 0;\r\nmutex_lock(&mdev->state_lock);\r\nif (!mdev->device_up) {\r\nen_err(priv, "Cannot open - device down/disabled\n");\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\nmlx4_en_clear_stats(dev);\r\nerr = mlx4_en_start_port(dev);\r\nif (err)\r\nen_err(priv, "Failed starting port:%d\n", priv->port);\r\nout:\r\nmutex_unlock(&mdev->state_lock);\r\nreturn err;\r\n}\r\nstatic int mlx4_en_close(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nen_dbg(IFDOWN, priv, "Close port called\n");\r\nmutex_lock(&mdev->state_lock);\r\nmlx4_en_stop_port(dev);\r\nnetif_carrier_off(dev);\r\nmutex_unlock(&mdev->state_lock);\r\nreturn 0;\r\n}\r\nvoid mlx4_en_free_resources(struct mlx4_en_priv *priv)\r\n{\r\nint i;\r\n#ifdef CONFIG_RFS_ACCEL\r\nfree_irq_cpu_rmap(priv->dev->rx_cpu_rmap);\r\npriv->dev->rx_cpu_rmap = NULL;\r\n#endif\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\nif (priv->tx_ring[i].tx_info)\r\nmlx4_en_destroy_tx_ring(priv, &priv->tx_ring[i]);\r\nif (priv->tx_cq[i].buf)\r\nmlx4_en_destroy_cq(priv, &priv->tx_cq[i]);\r\n}\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\nif (priv->rx_ring[i].rx_info)\r\nmlx4_en_destroy_rx_ring(priv, &priv->rx_ring[i],\r\npriv->prof->rx_ring_size, priv->stride);\r\nif (priv->rx_cq[i].buf)\r\nmlx4_en_destroy_cq(priv, &priv->rx_cq[i]);\r\n}\r\nif (priv->base_tx_qpn) {\r\nmlx4_qp_release_range(priv->mdev->dev, priv->base_tx_qpn, priv->tx_ring_num);\r\npriv->base_tx_qpn = 0;\r\n}\r\n}\r\nint mlx4_en_alloc_resources(struct mlx4_en_priv *priv)\r\n{\r\nstruct mlx4_en_port_profile *prof = priv->prof;\r\nint i;\r\nint err;\r\nerr = mlx4_qp_reserve_range(priv->mdev->dev, priv->tx_ring_num, 256, &priv->base_tx_qpn);\r\nif (err) {\r\nen_err(priv, "failed reserving range for TX rings\n");\r\nreturn err;\r\n}\r\nfor (i = 0; i < priv->tx_ring_num; i++) {\r\nif (mlx4_en_create_cq(priv, &priv->tx_cq[i],\r\nprof->tx_ring_size, i, TX))\r\ngoto err;\r\nif (mlx4_en_create_tx_ring(priv, &priv->tx_ring[i], priv->base_tx_qpn + i,\r\nprof->tx_ring_size, TXBB_SIZE))\r\ngoto err;\r\n}\r\nfor (i = 0; i < priv->rx_ring_num; i++) {\r\nif (mlx4_en_create_cq(priv, &priv->rx_cq[i],\r\nprof->rx_ring_size, i, RX))\r\ngoto err;\r\nif (mlx4_en_create_rx_ring(priv, &priv->rx_ring[i],\r\nprof->rx_ring_size, priv->stride))\r\ngoto err;\r\n}\r\n#ifdef CONFIG_RFS_ACCEL\r\npriv->dev->rx_cpu_rmap = alloc_irq_cpu_rmap(priv->rx_ring_num);\r\nif (!priv->dev->rx_cpu_rmap)\r\ngoto err;\r\nINIT_LIST_HEAD(&priv->filters);\r\nspin_lock_init(&priv->filters_lock);\r\n#endif\r\nreturn 0;\r\nerr:\r\nen_err(priv, "Failed to allocate NIC resources\n");\r\nreturn -ENOMEM;\r\n}\r\nvoid mlx4_en_destroy_netdev(struct net_device *dev)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nen_dbg(DRV, priv, "Destroying netdev on port:%d\n", priv->port);\r\nif (priv->registered)\r\nunregister_netdev(dev);\r\nif (priv->allocated)\r\nmlx4_free_hwq_res(mdev->dev, &priv->res, MLX4_EN_PAGE_SIZE);\r\ncancel_delayed_work(&priv->stats_task);\r\nflush_workqueue(mdev->workqueue);\r\nmutex_lock(&mdev->state_lock);\r\nmdev->pndev[priv->port] = NULL;\r\nmutex_unlock(&mdev->state_lock);\r\nmlx4_en_free_resources(priv);\r\nkfree(priv->tx_ring);\r\nkfree(priv->tx_cq);\r\nfree_netdev(dev);\r\n}\r\nstatic int mlx4_en_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(dev);\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nint err = 0;\r\nen_dbg(DRV, priv, "Change MTU called - current:%d new:%d\n",\r\ndev->mtu, new_mtu);\r\nif ((new_mtu < MLX4_EN_MIN_MTU) || (new_mtu > priv->max_mtu)) {\r\nen_err(priv, "Bad MTU size:%d.\n", new_mtu);\r\nreturn -EPERM;\r\n}\r\ndev->mtu = new_mtu;\r\nif (netif_running(dev)) {\r\nmutex_lock(&mdev->state_lock);\r\nif (!mdev->device_up) {\r\nen_dbg(DRV, priv, "Change MTU called with card down!?\n");\r\n} else {\r\nmlx4_en_stop_port(dev);\r\nerr = mlx4_en_start_port(dev);\r\nif (err) {\r\nen_err(priv, "Failed restarting port:%d\n",\r\npriv->port);\r\nqueue_work(mdev->workqueue, &priv->watchdog_task);\r\n}\r\n}\r\nmutex_unlock(&mdev->state_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mlx4_en_set_features(struct net_device *netdev,\r\nnetdev_features_t features)\r\n{\r\nstruct mlx4_en_priv *priv = netdev_priv(netdev);\r\nif (features & NETIF_F_LOOPBACK)\r\npriv->ctrl_flags |= cpu_to_be32(MLX4_WQE_CTRL_FORCE_LOOPBACK);\r\nelse\r\npriv->ctrl_flags &=\r\ncpu_to_be32(~MLX4_WQE_CTRL_FORCE_LOOPBACK);\r\nreturn 0;\r\n}\r\nint mlx4_en_init_netdev(struct mlx4_en_dev *mdev, int port,\r\nstruct mlx4_en_port_profile *prof)\r\n{\r\nstruct net_device *dev;\r\nstruct mlx4_en_priv *priv;\r\nint i;\r\nint err;\r\ndev = alloc_etherdev_mqs(sizeof(struct mlx4_en_priv),\r\nprof->tx_ring_num, prof->rx_ring_num);\r\nif (dev == NULL)\r\nreturn -ENOMEM;\r\nSET_NETDEV_DEV(dev, &mdev->dev->pdev->dev);\r\ndev->dev_id = port - 1;\r\npriv = netdev_priv(dev);\r\nmemset(priv, 0, sizeof(struct mlx4_en_priv));\r\npriv->dev = dev;\r\npriv->mdev = mdev;\r\npriv->ddev = &mdev->pdev->dev;\r\npriv->prof = prof;\r\npriv->port = port;\r\npriv->port_up = false;\r\npriv->flags = prof->flags;\r\npriv->ctrl_flags = cpu_to_be32(MLX4_WQE_CTRL_CQ_UPDATE |\r\nMLX4_WQE_CTRL_SOLICITED);\r\npriv->tx_ring_num = prof->tx_ring_num;\r\npriv->tx_ring = kzalloc(sizeof(struct mlx4_en_tx_ring) *\r\npriv->tx_ring_num, GFP_KERNEL);\r\nif (!priv->tx_ring) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\npriv->tx_cq = kzalloc(sizeof(struct mlx4_en_cq) * priv->tx_ring_num,\r\nGFP_KERNEL);\r\nif (!priv->tx_cq) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\npriv->rx_ring_num = prof->rx_ring_num;\r\npriv->mac_index = -1;\r\npriv->msg_enable = MLX4_EN_MSG_LEVEL;\r\nspin_lock_init(&priv->stats_lock);\r\nINIT_WORK(&priv->mcast_task, mlx4_en_do_set_multicast);\r\nINIT_WORK(&priv->mac_task, mlx4_en_do_set_mac);\r\nINIT_WORK(&priv->watchdog_task, mlx4_en_restart);\r\nINIT_WORK(&priv->linkstate_task, mlx4_en_linkstate);\r\nINIT_DELAYED_WORK(&priv->stats_task, mlx4_en_do_get_stats);\r\n#ifdef CONFIG_MLX4_EN_DCB\r\nif (!mlx4_is_slave(priv->mdev->dev))\r\ndev->dcbnl_ops = &mlx4_en_dcbnl_ops;\r\n#endif\r\npriv->max_mtu = mdev->dev->caps.eth_mtu_cap[priv->port];\r\npriv->mac = mdev->dev->caps.def_mac[priv->port];\r\nif (ILLEGAL_MAC(priv->mac)) {\r\nen_err(priv, "Port: %d, invalid mac burned: 0x%llx, quiting\n",\r\npriv->port, priv->mac);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\npriv->stride = roundup_pow_of_two(sizeof(struct mlx4_en_rx_desc) +\r\nDS_SIZE * MLX4_EN_MAX_RX_FRAGS);\r\nerr = mlx4_en_alloc_resources(priv);\r\nif (err)\r\ngoto out;\r\nerr = mlx4_alloc_hwq_res(mdev->dev, &priv->res,\r\nMLX4_EN_PAGE_SIZE, MLX4_EN_PAGE_SIZE);\r\nif (err) {\r\nen_err(priv, "Failed to allocate page for rx qps\n");\r\ngoto out;\r\n}\r\npriv->allocated = 1;\r\ndev->netdev_ops = &mlx4_netdev_ops;\r\ndev->watchdog_timeo = MLX4_EN_WATCHDOG_TIMEOUT;\r\nnetif_set_real_num_tx_queues(dev, priv->tx_ring_num);\r\nnetif_set_real_num_rx_queues(dev, priv->rx_ring_num);\r\nSET_ETHTOOL_OPS(dev, &mlx4_en_ethtool_ops);\r\ndev->addr_len = ETH_ALEN;\r\nfor (i = 0; i < ETH_ALEN; i++) {\r\ndev->dev_addr[ETH_ALEN - 1 - i] = (u8) (priv->mac >> (8 * i));\r\ndev->perm_addr[ETH_ALEN - 1 - i] = (u8) (priv->mac >> (8 * i));\r\n}\r\ndev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\r\nif (mdev->LSO_support)\r\ndev->hw_features |= NETIF_F_TSO | NETIF_F_TSO6;\r\ndev->vlan_features = dev->hw_features;\r\ndev->hw_features |= NETIF_F_RXCSUM | NETIF_F_RXHASH;\r\ndev->features = dev->hw_features | NETIF_F_HIGHDMA |\r\nNETIF_F_HW_VLAN_TX | NETIF_F_HW_VLAN_RX |\r\nNETIF_F_HW_VLAN_FILTER;\r\ndev->hw_features |= NETIF_F_LOOPBACK;\r\nif (mdev->dev->caps.steering_mode ==\r\nMLX4_STEERING_MODE_DEVICE_MANAGED)\r\ndev->hw_features |= NETIF_F_NTUPLE;\r\nmdev->pndev[port] = dev;\r\nnetif_carrier_off(dev);\r\nerr = register_netdev(dev);\r\nif (err) {\r\nen_err(priv, "Netdev registration failed for port %d\n", port);\r\ngoto out;\r\n}\r\npriv->registered = 1;\r\nen_warn(priv, "Using %d TX rings\n", prof->tx_ring_num);\r\nen_warn(priv, "Using %d RX rings\n", prof->rx_ring_num);\r\nmlx4_en_calc_rx_buf(dev);\r\nerr = mlx4_SET_PORT_general(mdev->dev, priv->port,\r\npriv->rx_skb_size + ETH_FCS_LEN,\r\nprof->tx_pause, prof->tx_ppp,\r\nprof->rx_pause, prof->rx_ppp);\r\nif (err) {\r\nen_err(priv, "Failed setting port general configurations "\r\n"for port %d, with error %d\n", priv->port, err);\r\ngoto out;\r\n}\r\nen_warn(priv, "Initializing port\n");\r\nerr = mlx4_INIT_PORT(mdev->dev, priv->port);\r\nif (err) {\r\nen_err(priv, "Failed Initializing port\n");\r\ngoto out;\r\n}\r\nmlx4_en_set_default_moderation(priv);\r\nqueue_delayed_work(mdev->workqueue, &priv->stats_task, STATS_DELAY);\r\nreturn 0;\r\nout:\r\nmlx4_en_destroy_netdev(dev);\r\nreturn err;\r\n}
