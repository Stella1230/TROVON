static void chclr_write(struct sh_dmae_chan *sh_dc, u32 data)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_dc);\r\n__raw_writel(data, shdev->chan_reg +\r\nshdev->pdata->channel[sh_dc->shdma_chan.id].chclr_offset);\r\n}\r\nstatic void sh_dmae_writel(struct sh_dmae_chan *sh_dc, u32 data, u32 reg)\r\n{\r\n__raw_writel(data, sh_dc->base + reg / sizeof(u32));\r\n}\r\nstatic u32 sh_dmae_readl(struct sh_dmae_chan *sh_dc, u32 reg)\r\n{\r\nreturn __raw_readl(sh_dc->base + reg / sizeof(u32));\r\n}\r\nstatic u16 dmaor_read(struct sh_dmae_device *shdev)\r\n{\r\nu32 __iomem *addr = shdev->chan_reg + DMAOR / sizeof(u32);\r\nif (shdev->pdata->dmaor_is_32bit)\r\nreturn __raw_readl(addr);\r\nelse\r\nreturn __raw_readw(addr);\r\n}\r\nstatic void dmaor_write(struct sh_dmae_device *shdev, u16 data)\r\n{\r\nu32 __iomem *addr = shdev->chan_reg + DMAOR / sizeof(u32);\r\nif (shdev->pdata->dmaor_is_32bit)\r\n__raw_writel(data, addr);\r\nelse\r\n__raw_writew(data, addr);\r\n}\r\nstatic void chcr_write(struct sh_dmae_chan *sh_dc, u32 data)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_dc);\r\n__raw_writel(data, sh_dc->base + shdev->chcr_offset / sizeof(u32));\r\n}\r\nstatic u32 chcr_read(struct sh_dmae_chan *sh_dc)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_dc);\r\nreturn __raw_readl(sh_dc->base + shdev->chcr_offset / sizeof(u32));\r\n}\r\nstatic void sh_dmae_ctl_stop(struct sh_dmae_device *shdev)\r\n{\r\nunsigned short dmaor;\r\nunsigned long flags;\r\nspin_lock_irqsave(&sh_dmae_lock, flags);\r\ndmaor = dmaor_read(shdev);\r\ndmaor_write(shdev, dmaor & ~(DMAOR_NMIF | DMAOR_AE | DMAOR_DME));\r\nspin_unlock_irqrestore(&sh_dmae_lock, flags);\r\n}\r\nstatic int sh_dmae_rst(struct sh_dmae_device *shdev)\r\n{\r\nunsigned short dmaor;\r\nunsigned long flags;\r\nspin_lock_irqsave(&sh_dmae_lock, flags);\r\ndmaor = dmaor_read(shdev) & ~(DMAOR_NMIF | DMAOR_AE | DMAOR_DME);\r\nif (shdev->pdata->chclr_present) {\r\nint i;\r\nfor (i = 0; i < shdev->pdata->channel_num; i++) {\r\nstruct sh_dmae_chan *sh_chan = shdev->chan[i];\r\nif (sh_chan)\r\nchclr_write(sh_chan, 0);\r\n}\r\n}\r\ndmaor_write(shdev, dmaor | shdev->pdata->dmaor_init);\r\ndmaor = dmaor_read(shdev);\r\nspin_unlock_irqrestore(&sh_dmae_lock, flags);\r\nif (dmaor & (DMAOR_AE | DMAOR_NMIF)) {\r\ndev_warn(shdev->shdma_dev.dma_dev.dev, "Can't initialize DMAOR.\n");\r\nreturn -EIO;\r\n}\r\nif (shdev->pdata->dmaor_init & ~dmaor)\r\ndev_warn(shdev->shdma_dev.dma_dev.dev,\r\n"DMAOR=0x%x hasn't latched the initial value 0x%x.\n",\r\ndmaor, shdev->pdata->dmaor_init);\r\nreturn 0;\r\n}\r\nstatic bool dmae_is_busy(struct sh_dmae_chan *sh_chan)\r\n{\r\nu32 chcr = chcr_read(sh_chan);\r\nif ((chcr & (CHCR_DE | CHCR_TE)) == CHCR_DE)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic unsigned int calc_xmit_shift(struct sh_dmae_chan *sh_chan, u32 chcr)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nstruct sh_dmae_pdata *pdata = shdev->pdata;\r\nint cnt = ((chcr & pdata->ts_low_mask) >> pdata->ts_low_shift) |\r\n((chcr & pdata->ts_high_mask) >> pdata->ts_high_shift);\r\nif (cnt >= pdata->ts_shift_num)\r\ncnt = 0;\r\nreturn pdata->ts_shift[cnt];\r\n}\r\nstatic u32 log2size_to_chcr(struct sh_dmae_chan *sh_chan, int l2size)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nstruct sh_dmae_pdata *pdata = shdev->pdata;\r\nint i;\r\nfor (i = 0; i < pdata->ts_shift_num; i++)\r\nif (pdata->ts_shift[i] == l2size)\r\nbreak;\r\nif (i == pdata->ts_shift_num)\r\ni = 0;\r\nreturn ((i << pdata->ts_low_shift) & pdata->ts_low_mask) |\r\n((i << pdata->ts_high_shift) & pdata->ts_high_mask);\r\n}\r\nstatic void dmae_set_reg(struct sh_dmae_chan *sh_chan, struct sh_dmae_regs *hw)\r\n{\r\nsh_dmae_writel(sh_chan, hw->sar, SAR);\r\nsh_dmae_writel(sh_chan, hw->dar, DAR);\r\nsh_dmae_writel(sh_chan, hw->tcr >> sh_chan->xmit_shift, TCR);\r\n}\r\nstatic void dmae_start(struct sh_dmae_chan *sh_chan)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nu32 chcr = chcr_read(sh_chan);\r\nif (shdev->pdata->needs_tend_set)\r\nsh_dmae_writel(sh_chan, 0xFFFFFFFF, TEND);\r\nchcr |= CHCR_DE | shdev->chcr_ie_bit;\r\nchcr_write(sh_chan, chcr & ~CHCR_TE);\r\n}\r\nstatic void dmae_init(struct sh_dmae_chan *sh_chan)\r\n{\r\nu32 chcr = DM_INC | SM_INC | 0x400 | log2size_to_chcr(sh_chan,\r\nLOG2_DEFAULT_XFER_SIZE);\r\nsh_chan->xmit_shift = calc_xmit_shift(sh_chan, chcr);\r\nchcr_write(sh_chan, chcr);\r\n}\r\nstatic int dmae_set_chcr(struct sh_dmae_chan *sh_chan, u32 val)\r\n{\r\nif (dmae_is_busy(sh_chan))\r\nreturn -EBUSY;\r\nsh_chan->xmit_shift = calc_xmit_shift(sh_chan, val);\r\nchcr_write(sh_chan, val);\r\nreturn 0;\r\n}\r\nstatic int dmae_set_dmars(struct sh_dmae_chan *sh_chan, u16 val)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nstruct sh_dmae_pdata *pdata = shdev->pdata;\r\nconst struct sh_dmae_channel *chan_pdata = &pdata->channel[sh_chan->shdma_chan.id];\r\nu16 __iomem *addr = shdev->dmars;\r\nunsigned int shift = chan_pdata->dmars_bit;\r\nif (dmae_is_busy(sh_chan))\r\nreturn -EBUSY;\r\nif (pdata->no_dmars)\r\nreturn 0;\r\nif (!addr)\r\naddr = (u16 __iomem *)shdev->chan_reg;\r\naddr += chan_pdata->dmars / sizeof(u16);\r\n__raw_writew((__raw_readw(addr) & (0xff00 >> shift)) | (val << shift),\r\naddr);\r\nreturn 0;\r\n}\r\nstatic void sh_dmae_start_xfer(struct shdma_chan *schan,\r\nstruct shdma_desc *sdesc)\r\n{\r\nstruct sh_dmae_chan *sh_chan = container_of(schan, struct sh_dmae_chan,\r\nshdma_chan);\r\nstruct sh_dmae_desc *sh_desc = container_of(sdesc,\r\nstruct sh_dmae_desc, shdma_desc);\r\ndev_dbg(sh_chan->shdma_chan.dev, "Queue #%d to %d: %u@%x -> %x\n",\r\nsdesc->async_tx.cookie, sh_chan->shdma_chan.id,\r\nsh_desc->hw.tcr, sh_desc->hw.sar, sh_desc->hw.dar);\r\ndmae_set_reg(sh_chan, &sh_desc->hw);\r\ndmae_start(sh_chan);\r\n}\r\nstatic bool sh_dmae_channel_busy(struct shdma_chan *schan)\r\n{\r\nstruct sh_dmae_chan *sh_chan = container_of(schan, struct sh_dmae_chan,\r\nshdma_chan);\r\nreturn dmae_is_busy(sh_chan);\r\n}\r\nstatic void sh_dmae_setup_xfer(struct shdma_chan *schan,\r\nint slave_id)\r\n{\r\nstruct sh_dmae_chan *sh_chan = container_of(schan, struct sh_dmae_chan,\r\nshdma_chan);\r\nif (slave_id >= 0) {\r\nconst struct sh_dmae_slave_config *cfg =\r\nsh_chan->config;\r\ndmae_set_dmars(sh_chan, cfg->mid_rid);\r\ndmae_set_chcr(sh_chan, cfg->chcr);\r\n} else {\r\ndmae_init(sh_chan);\r\n}\r\n}\r\nstatic const struct sh_dmae_slave_config *dmae_find_slave(\r\nstruct sh_dmae_chan *sh_chan, int slave_id)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nstruct sh_dmae_pdata *pdata = shdev->pdata;\r\nconst struct sh_dmae_slave_config *cfg;\r\nint i;\r\nif (slave_id >= SH_DMA_SLAVE_NUMBER)\r\nreturn NULL;\r\nfor (i = 0, cfg = pdata->slave; i < pdata->slave_num; i++, cfg++)\r\nif (cfg->slave_id == slave_id)\r\nreturn cfg;\r\nreturn NULL;\r\n}\r\nstatic int sh_dmae_set_slave(struct shdma_chan *schan,\r\nint slave_id, bool try)\r\n{\r\nstruct sh_dmae_chan *sh_chan = container_of(schan, struct sh_dmae_chan,\r\nshdma_chan);\r\nconst struct sh_dmae_slave_config *cfg = dmae_find_slave(sh_chan, slave_id);\r\nif (!cfg)\r\nreturn -ENODEV;\r\nif (!try)\r\nsh_chan->config = cfg;\r\nreturn 0;\r\n}\r\nstatic void dmae_halt(struct sh_dmae_chan *sh_chan)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nu32 chcr = chcr_read(sh_chan);\r\nchcr &= ~(CHCR_DE | CHCR_TE | shdev->chcr_ie_bit);\r\nchcr_write(sh_chan, chcr);\r\n}\r\nstatic int sh_dmae_desc_setup(struct shdma_chan *schan,\r\nstruct shdma_desc *sdesc,\r\ndma_addr_t src, dma_addr_t dst, size_t *len)\r\n{\r\nstruct sh_dmae_desc *sh_desc = container_of(sdesc,\r\nstruct sh_dmae_desc, shdma_desc);\r\nif (*len > schan->max_xfer_len)\r\n*len = schan->max_xfer_len;\r\nsh_desc->hw.sar = src;\r\nsh_desc->hw.dar = dst;\r\nsh_desc->hw.tcr = *len;\r\nreturn 0;\r\n}\r\nstatic void sh_dmae_halt(struct shdma_chan *schan)\r\n{\r\nstruct sh_dmae_chan *sh_chan = container_of(schan, struct sh_dmae_chan,\r\nshdma_chan);\r\ndmae_halt(sh_chan);\r\n}\r\nstatic bool sh_dmae_chan_irq(struct shdma_chan *schan, int irq)\r\n{\r\nstruct sh_dmae_chan *sh_chan = container_of(schan, struct sh_dmae_chan,\r\nshdma_chan);\r\nif (!(chcr_read(sh_chan) & CHCR_TE))\r\nreturn false;\r\ndmae_halt(sh_chan);\r\nreturn true;\r\n}\r\nstatic size_t sh_dmae_get_partial(struct shdma_chan *schan,\r\nstruct shdma_desc *sdesc)\r\n{\r\nstruct sh_dmae_chan *sh_chan = container_of(schan, struct sh_dmae_chan,\r\nshdma_chan);\r\nstruct sh_dmae_desc *sh_desc = container_of(sdesc,\r\nstruct sh_dmae_desc, shdma_desc);\r\nreturn (sh_desc->hw.tcr - sh_dmae_readl(sh_chan, TCR)) <<\r\nsh_chan->xmit_shift;\r\n}\r\nstatic bool sh_dmae_reset(struct sh_dmae_device *shdev)\r\n{\r\nbool ret;\r\nsh_dmae_ctl_stop(shdev);\r\nret = shdma_reset(&shdev->shdma_dev);\r\nsh_dmae_rst(shdev);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t sh_dmae_err(int irq, void *data)\r\n{\r\nstruct sh_dmae_device *shdev = data;\r\nif (!(dmaor_read(shdev) & DMAOR_AE))\r\nreturn IRQ_NONE;\r\nsh_dmae_reset(shdev);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic bool sh_dmae_desc_completed(struct shdma_chan *schan,\r\nstruct shdma_desc *sdesc)\r\n{\r\nstruct sh_dmae_chan *sh_chan = container_of(schan,\r\nstruct sh_dmae_chan, shdma_chan);\r\nstruct sh_dmae_desc *sh_desc = container_of(sdesc,\r\nstruct sh_dmae_desc, shdma_desc);\r\nu32 sar_buf = sh_dmae_readl(sh_chan, SAR);\r\nu32 dar_buf = sh_dmae_readl(sh_chan, DAR);\r\nreturn (sdesc->direction == DMA_DEV_TO_MEM &&\r\n(sh_desc->hw.dar + sh_desc->hw.tcr) == dar_buf) ||\r\n(sdesc->direction != DMA_DEV_TO_MEM &&\r\n(sh_desc->hw.sar + sh_desc->hw.tcr) == sar_buf);\r\n}\r\nstatic bool sh_dmae_nmi_notify(struct sh_dmae_device *shdev)\r\n{\r\nif ((dmaor_read(shdev) & DMAOR_NMIF) == 0)\r\nreturn false;\r\nreturn sh_dmae_reset(shdev);\r\n}\r\nstatic int sh_dmae_nmi_handler(struct notifier_block *self,\r\nunsigned long cmd, void *data)\r\n{\r\nstruct sh_dmae_device *shdev;\r\nint ret = NOTIFY_DONE;\r\nbool triggered;\r\nif (!in_nmi())\r\nreturn NOTIFY_DONE;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(shdev, &sh_dmae_devices, node) {\r\ntriggered = sh_dmae_nmi_notify(shdev);\r\nif (triggered == true)\r\nret = NOTIFY_OK;\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int __devinit sh_dmae_chan_probe(struct sh_dmae_device *shdev, int id,\r\nint irq, unsigned long flags)\r\n{\r\nconst struct sh_dmae_channel *chan_pdata = &shdev->pdata->channel[id];\r\nstruct shdma_dev *sdev = &shdev->shdma_dev;\r\nstruct platform_device *pdev = to_platform_device(sdev->dma_dev.dev);\r\nstruct sh_dmae_chan *sh_chan;\r\nstruct shdma_chan *schan;\r\nint err;\r\nsh_chan = kzalloc(sizeof(struct sh_dmae_chan), GFP_KERNEL);\r\nif (!sh_chan) {\r\ndev_err(sdev->dma_dev.dev,\r\n"No free memory for allocating dma channels!\n");\r\nreturn -ENOMEM;\r\n}\r\nschan = &sh_chan->shdma_chan;\r\nschan->max_xfer_len = SH_DMA_TCR_MAX + 1;\r\nshdma_chan_probe(sdev, schan, id);\r\nsh_chan->base = shdev->chan_reg + chan_pdata->offset / sizeof(u32);\r\nif (pdev->id >= 0)\r\nsnprintf(sh_chan->dev_id, sizeof(sh_chan->dev_id),\r\n"sh-dmae%d.%d", pdev->id, id);\r\nelse\r\nsnprintf(sh_chan->dev_id, sizeof(sh_chan->dev_id),\r\n"sh-dma%d", id);\r\nerr = shdma_request_irq(schan, irq, flags, sh_chan->dev_id);\r\nif (err) {\r\ndev_err(sdev->dma_dev.dev,\r\n"DMA channel %d request_irq error %d\n",\r\nid, err);\r\ngoto err_no_irq;\r\n}\r\nshdev->chan[id] = sh_chan;\r\nreturn 0;\r\nerr_no_irq:\r\nshdma_chan_remove(schan);\r\nkfree(sh_chan);\r\nreturn err;\r\n}\r\nstatic void sh_dmae_chan_remove(struct sh_dmae_device *shdev)\r\n{\r\nstruct dma_device *dma_dev = &shdev->shdma_dev.dma_dev;\r\nstruct shdma_chan *schan;\r\nint i;\r\nshdma_for_each_chan(schan, &shdev->shdma_dev, i) {\r\nstruct sh_dmae_chan *sh_chan = container_of(schan,\r\nstruct sh_dmae_chan, shdma_chan);\r\nBUG_ON(!schan);\r\nshdma_free_irq(&sh_chan->shdma_chan);\r\nshdma_chan_remove(schan);\r\nkfree(sh_chan);\r\n}\r\ndma_dev->chancnt = 0;\r\n}\r\nstatic void sh_dmae_shutdown(struct platform_device *pdev)\r\n{\r\nstruct sh_dmae_device *shdev = platform_get_drvdata(pdev);\r\nsh_dmae_ctl_stop(shdev);\r\n}\r\nstatic int sh_dmae_runtime_suspend(struct device *dev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int sh_dmae_runtime_resume(struct device *dev)\r\n{\r\nstruct sh_dmae_device *shdev = dev_get_drvdata(dev);\r\nreturn sh_dmae_rst(shdev);\r\n}\r\nstatic int sh_dmae_suspend(struct device *dev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int sh_dmae_resume(struct device *dev)\r\n{\r\nstruct sh_dmae_device *shdev = dev_get_drvdata(dev);\r\nint i, ret;\r\nret = sh_dmae_rst(shdev);\r\nif (ret < 0)\r\ndev_err(dev, "Failed to reset!\n");\r\nfor (i = 0; i < shdev->pdata->channel_num; i++) {\r\nstruct sh_dmae_chan *sh_chan = shdev->chan[i];\r\nif (!sh_chan->shdma_chan.desc_num)\r\ncontinue;\r\nif (sh_chan->shdma_chan.slave_id >= 0) {\r\nconst struct sh_dmae_slave_config *cfg = sh_chan->config;\r\ndmae_set_dmars(sh_chan, cfg->mid_rid);\r\ndmae_set_chcr(sh_chan, cfg->chcr);\r\n} else {\r\ndmae_init(sh_chan);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic dma_addr_t sh_dmae_slave_addr(struct shdma_chan *schan)\r\n{\r\nstruct sh_dmae_chan *sh_chan = container_of(schan,\r\nstruct sh_dmae_chan, shdma_chan);\r\nreturn sh_chan->config->addr;\r\n}\r\nstatic struct shdma_desc *sh_dmae_embedded_desc(void *buf, int i)\r\n{\r\nreturn &((struct sh_dmae_desc *)buf)[i].shdma_desc;\r\n}\r\nstatic int __devinit sh_dmae_probe(struct platform_device *pdev)\r\n{\r\nstruct sh_dmae_pdata *pdata = pdev->dev.platform_data;\r\nunsigned long irqflags = IRQF_DISABLED,\r\nchan_flag[SH_DMAE_MAX_CHANNELS] = {};\r\nint errirq, chan_irq[SH_DMAE_MAX_CHANNELS];\r\nint err, i, irq_cnt = 0, irqres = 0, irq_cap = 0;\r\nstruct sh_dmae_device *shdev;\r\nstruct dma_device *dma_dev;\r\nstruct resource *chan, *dmars, *errirq_res, *chanirq_res;\r\nif (!pdata || !pdata->channel_num)\r\nreturn -ENODEV;\r\nchan = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ndmars = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nerrirq_res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (!chan || !errirq_res)\r\nreturn -ENODEV;\r\nif (!request_mem_region(chan->start, resource_size(chan), pdev->name)) {\r\ndev_err(&pdev->dev, "DMAC register region already claimed\n");\r\nreturn -EBUSY;\r\n}\r\nif (dmars && !request_mem_region(dmars->start, resource_size(dmars), pdev->name)) {\r\ndev_err(&pdev->dev, "DMAC DMARS region already claimed\n");\r\nerr = -EBUSY;\r\ngoto ermrdmars;\r\n}\r\nerr = -ENOMEM;\r\nshdev = kzalloc(sizeof(struct sh_dmae_device), GFP_KERNEL);\r\nif (!shdev) {\r\ndev_err(&pdev->dev, "Not enough memory\n");\r\ngoto ealloc;\r\n}\r\ndma_dev = &shdev->shdma_dev.dma_dev;\r\nshdev->chan_reg = ioremap(chan->start, resource_size(chan));\r\nif (!shdev->chan_reg)\r\ngoto emapchan;\r\nif (dmars) {\r\nshdev->dmars = ioremap(dmars->start, resource_size(dmars));\r\nif (!shdev->dmars)\r\ngoto emapdmars;\r\n}\r\nif (!pdata->slave_only)\r\ndma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);\r\nif (pdata->slave && pdata->slave_num)\r\ndma_cap_set(DMA_SLAVE, dma_dev->cap_mask);\r\ndma_dev->copy_align = LOG2_DEFAULT_XFER_SIZE;\r\nshdev->shdma_dev.ops = &sh_dmae_shdma_ops;\r\nshdev->shdma_dev.desc_size = sizeof(struct sh_dmae_desc);\r\nerr = shdma_init(&pdev->dev, &shdev->shdma_dev,\r\npdata->channel_num);\r\nif (err < 0)\r\ngoto eshdma;\r\nshdev->pdata = pdev->dev.platform_data;\r\nif (pdata->chcr_offset)\r\nshdev->chcr_offset = pdata->chcr_offset;\r\nelse\r\nshdev->chcr_offset = CHCR;\r\nif (pdata->chcr_ie_bit)\r\nshdev->chcr_ie_bit = pdata->chcr_ie_bit;\r\nelse\r\nshdev->chcr_ie_bit = CHCR_IE;\r\nplatform_set_drvdata(pdev, shdev);\r\npm_runtime_enable(&pdev->dev);\r\nerr = pm_runtime_get_sync(&pdev->dev);\r\nif (err < 0)\r\ndev_err(&pdev->dev, "%s(): GET = %d\n", __func__, err);\r\nspin_lock_irq(&sh_dmae_lock);\r\nlist_add_tail_rcu(&shdev->node, &sh_dmae_devices);\r\nspin_unlock_irq(&sh_dmae_lock);\r\nerr = sh_dmae_rst(shdev);\r\nif (err)\r\ngoto rst_err;\r\n#if defined(CONFIG_CPU_SH4) || defined(CONFIG_ARCH_SHMOBILE)\r\nchanirq_res = platform_get_resource(pdev, IORESOURCE_IRQ, 1);\r\nif (!chanirq_res)\r\nchanirq_res = errirq_res;\r\nelse\r\nirqres++;\r\nif (chanirq_res == errirq_res ||\r\n(errirq_res->flags & IORESOURCE_BITS) == IORESOURCE_IRQ_SHAREABLE)\r\nirqflags = IRQF_SHARED;\r\nerrirq = errirq_res->start;\r\nerr = request_irq(errirq, sh_dmae_err, irqflags,\r\n"DMAC Address Error", shdev);\r\nif (err) {\r\ndev_err(&pdev->dev,\r\n"DMA failed requesting irq #%d, error %d\n",\r\nerrirq, err);\r\ngoto eirq_err;\r\n}\r\n#else\r\nchanirq_res = errirq_res;\r\n#endif\r\nif (chanirq_res->start == chanirq_res->end &&\r\n!platform_get_resource(pdev, IORESOURCE_IRQ, 1)) {\r\nfor (; irq_cnt < pdata->channel_num; irq_cnt++) {\r\nif (irq_cnt < SH_DMAE_MAX_CHANNELS) {\r\nchan_irq[irq_cnt] = chanirq_res->start;\r\nchan_flag[irq_cnt] = IRQF_SHARED;\r\n} else {\r\nirq_cap = 1;\r\nbreak;\r\n}\r\n}\r\n} else {\r\ndo {\r\nfor (i = chanirq_res->start; i <= chanirq_res->end; i++) {\r\nif (irq_cnt >= SH_DMAE_MAX_CHANNELS) {\r\nirq_cap = 1;\r\nbreak;\r\n}\r\nif ((errirq_res->flags & IORESOURCE_BITS) ==\r\nIORESOURCE_IRQ_SHAREABLE)\r\nchan_flag[irq_cnt] = IRQF_SHARED;\r\nelse\r\nchan_flag[irq_cnt] = IRQF_DISABLED;\r\ndev_dbg(&pdev->dev,\r\n"Found IRQ %d for channel %d\n",\r\ni, irq_cnt);\r\nchan_irq[irq_cnt++] = i;\r\n}\r\nif (irq_cnt >= SH_DMAE_MAX_CHANNELS)\r\nbreak;\r\nchanirq_res = platform_get_resource(pdev,\r\nIORESOURCE_IRQ, ++irqres);\r\n} while (irq_cnt < pdata->channel_num && chanirq_res);\r\n}\r\nfor (i = 0; i < irq_cnt; i++) {\r\nerr = sh_dmae_chan_probe(shdev, i, chan_irq[i], chan_flag[i]);\r\nif (err)\r\ngoto chan_probe_err;\r\n}\r\nif (irq_cap)\r\ndev_notice(&pdev->dev, "Attempting to register %d DMA "\r\n"channels when a maximum of %d are supported.\n",\r\npdata->channel_num, SH_DMAE_MAX_CHANNELS);\r\npm_runtime_put(&pdev->dev);\r\nerr = dma_async_device_register(&shdev->shdma_dev.dma_dev);\r\nif (err < 0)\r\ngoto edmadevreg;\r\nreturn err;\r\nedmadevreg:\r\npm_runtime_get(&pdev->dev);\r\nchan_probe_err:\r\nsh_dmae_chan_remove(shdev);\r\n#if defined(CONFIG_CPU_SH4) || defined(CONFIG_ARCH_SHMOBILE)\r\nfree_irq(errirq, shdev);\r\neirq_err:\r\n#endif\r\nrst_err:\r\nspin_lock_irq(&sh_dmae_lock);\r\nlist_del_rcu(&shdev->node);\r\nspin_unlock_irq(&sh_dmae_lock);\r\npm_runtime_put(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nplatform_set_drvdata(pdev, NULL);\r\nshdma_cleanup(&shdev->shdma_dev);\r\neshdma:\r\nif (dmars)\r\niounmap(shdev->dmars);\r\nemapdmars:\r\niounmap(shdev->chan_reg);\r\nsynchronize_rcu();\r\nemapchan:\r\nkfree(shdev);\r\nealloc:\r\nif (dmars)\r\nrelease_mem_region(dmars->start, resource_size(dmars));\r\nermrdmars:\r\nrelease_mem_region(chan->start, resource_size(chan));\r\nreturn err;\r\n}\r\nstatic int __devexit sh_dmae_remove(struct platform_device *pdev)\r\n{\r\nstruct sh_dmae_device *shdev = platform_get_drvdata(pdev);\r\nstruct dma_device *dma_dev = &shdev->shdma_dev.dma_dev;\r\nstruct resource *res;\r\nint errirq = platform_get_irq(pdev, 0);\r\ndma_async_device_unregister(dma_dev);\r\nif (errirq > 0)\r\nfree_irq(errirq, shdev);\r\nspin_lock_irq(&sh_dmae_lock);\r\nlist_del_rcu(&shdev->node);\r\nspin_unlock_irq(&sh_dmae_lock);\r\npm_runtime_disable(&pdev->dev);\r\nsh_dmae_chan_remove(shdev);\r\nshdma_cleanup(&shdev->shdma_dev);\r\nif (shdev->dmars)\r\niounmap(shdev->dmars);\r\niounmap(shdev->chan_reg);\r\nplatform_set_drvdata(pdev, NULL);\r\nsynchronize_rcu();\r\nkfree(shdev);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (res)\r\nrelease_mem_region(res->start, resource_size(res));\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nif (res)\r\nrelease_mem_region(res->start, resource_size(res));\r\nreturn 0;\r\n}\r\nstatic int __init sh_dmae_init(void)\r\n{\r\nint err = register_die_notifier(&sh_dmae_nmi_notifier);\r\nif (err)\r\nreturn err;\r\nreturn platform_driver_probe(&sh_dmae_driver, sh_dmae_probe);\r\n}\r\nstatic void __exit sh_dmae_exit(void)\r\n{\r\nplatform_driver_unregister(&sh_dmae_driver);\r\nunregister_die_notifier(&sh_dmae_nmi_notifier);\r\n}
