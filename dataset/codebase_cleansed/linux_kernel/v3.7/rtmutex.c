static void\r\nrt_mutex_set_owner(struct rt_mutex *lock, struct task_struct *owner)\r\n{\r\nunsigned long val = (unsigned long)owner;\r\nif (rt_mutex_has_waiters(lock))\r\nval |= RT_MUTEX_HAS_WAITERS;\r\nlock->owner = (struct task_struct *)val;\r\n}\r\nstatic inline void clear_rt_mutex_waiters(struct rt_mutex *lock)\r\n{\r\nlock->owner = (struct task_struct *)\r\n((unsigned long)lock->owner & ~RT_MUTEX_HAS_WAITERS);\r\n}\r\nstatic void fixup_rt_mutex_waiters(struct rt_mutex *lock)\r\n{\r\nif (!rt_mutex_has_waiters(lock))\r\nclear_rt_mutex_waiters(lock);\r\n}\r\nstatic inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\r\n{\r\nunsigned long owner, *p = (unsigned long *) &lock->owner;\r\ndo {\r\nowner = *p;\r\n} while (cmpxchg(p, owner, owner | RT_MUTEX_HAS_WAITERS) != owner);\r\n}\r\nstatic inline void mark_rt_mutex_waiters(struct rt_mutex *lock)\r\n{\r\nlock->owner = (struct task_struct *)\r\n((unsigned long)lock->owner | RT_MUTEX_HAS_WAITERS);\r\n}\r\nint rt_mutex_getprio(struct task_struct *task)\r\n{\r\nif (likely(!task_has_pi_waiters(task)))\r\nreturn task->normal_prio;\r\nreturn min(task_top_pi_waiter(task)->pi_list_entry.prio,\r\ntask->normal_prio);\r\n}\r\nstatic void __rt_mutex_adjust_prio(struct task_struct *task)\r\n{\r\nint prio = rt_mutex_getprio(task);\r\nif (task->prio != prio)\r\nrt_mutex_setprio(task, prio);\r\n}\r\nstatic void rt_mutex_adjust_prio(struct task_struct *task)\r\n{\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\n__rt_mutex_adjust_prio(task);\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\n}\r\nstatic int rt_mutex_adjust_prio_chain(struct task_struct *task,\r\nint deadlock_detect,\r\nstruct rt_mutex *orig_lock,\r\nstruct rt_mutex_waiter *orig_waiter,\r\nstruct task_struct *top_task)\r\n{\r\nstruct rt_mutex *lock;\r\nstruct rt_mutex_waiter *waiter, *top_waiter = orig_waiter;\r\nint detect_deadlock, ret = 0, depth = 0;\r\nunsigned long flags;\r\ndetect_deadlock = debug_rt_mutex_detect_deadlock(orig_waiter,\r\ndeadlock_detect);\r\nagain:\r\nif (++depth > max_lock_depth) {\r\nstatic int prev_max;\r\nif (prev_max != max_lock_depth) {\r\nprev_max = max_lock_depth;\r\nprintk(KERN_WARNING "Maximum lock depth %d reached "\r\n"task: %s (%d)\n", max_lock_depth,\r\ntop_task->comm, task_pid_nr(top_task));\r\n}\r\nput_task_struct(task);\r\nreturn deadlock_detect ? -EDEADLK : 0;\r\n}\r\nretry:\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\nwaiter = task->pi_blocked_on;\r\nif (!waiter)\r\ngoto out_unlock_pi;\r\nif (orig_waiter && !rt_mutex_owner(orig_lock))\r\ngoto out_unlock_pi;\r\nif (top_waiter && (!task_has_pi_waiters(task) ||\r\ntop_waiter != task_top_pi_waiter(task)))\r\ngoto out_unlock_pi;\r\nif (!detect_deadlock && waiter->list_entry.prio == task->prio)\r\ngoto out_unlock_pi;\r\nlock = waiter->lock;\r\nif (!raw_spin_trylock(&lock->wait_lock)) {\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\ncpu_relax();\r\ngoto retry;\r\n}\r\nif (lock == orig_lock || rt_mutex_owner(lock) == top_task) {\r\ndebug_rt_mutex_deadlock(deadlock_detect, orig_waiter, lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nret = deadlock_detect ? -EDEADLK : 0;\r\ngoto out_unlock_pi;\r\n}\r\ntop_waiter = rt_mutex_top_waiter(lock);\r\nplist_del(&waiter->list_entry, &lock->wait_list);\r\nwaiter->list_entry.prio = task->prio;\r\nplist_add(&waiter->list_entry, &lock->wait_list);\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nif (!rt_mutex_owner(lock)) {\r\nif (top_waiter != rt_mutex_top_waiter(lock))\r\nwake_up_process(rt_mutex_top_waiter(lock)->task);\r\nraw_spin_unlock(&lock->wait_lock);\r\ngoto out_put_task;\r\n}\r\nput_task_struct(task);\r\ntask = rt_mutex_owner(lock);\r\nget_task_struct(task);\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\nif (waiter == rt_mutex_top_waiter(lock)) {\r\nplist_del(&top_waiter->pi_list_entry, &task->pi_waiters);\r\nwaiter->pi_list_entry.prio = waiter->list_entry.prio;\r\nplist_add(&waiter->pi_list_entry, &task->pi_waiters);\r\n__rt_mutex_adjust_prio(task);\r\n} else if (top_waiter == waiter) {\r\nplist_del(&waiter->pi_list_entry, &task->pi_waiters);\r\nwaiter = rt_mutex_top_waiter(lock);\r\nwaiter->pi_list_entry.prio = waiter->list_entry.prio;\r\nplist_add(&waiter->pi_list_entry, &task->pi_waiters);\r\n__rt_mutex_adjust_prio(task);\r\n}\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\ntop_waiter = rt_mutex_top_waiter(lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nif (!detect_deadlock && waiter != top_waiter)\r\ngoto out_put_task;\r\ngoto again;\r\nout_unlock_pi:\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nout_put_task:\r\nput_task_struct(task);\r\nreturn ret;\r\n}\r\nstatic int try_to_take_rt_mutex(struct rt_mutex *lock, struct task_struct *task,\r\nstruct rt_mutex_waiter *waiter)\r\n{\r\nmark_rt_mutex_waiters(lock);\r\nif (rt_mutex_owner(lock))\r\nreturn 0;\r\nif (rt_mutex_has_waiters(lock)) {\r\nif (task->prio >= rt_mutex_top_waiter(lock)->list_entry.prio) {\r\nif (!waiter || waiter != rt_mutex_top_waiter(lock))\r\nreturn 0;\r\n}\r\n}\r\nif (waiter || rt_mutex_has_waiters(lock)) {\r\nunsigned long flags;\r\nstruct rt_mutex_waiter *top;\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\nif (waiter) {\r\nplist_del(&waiter->list_entry, &lock->wait_list);\r\ntask->pi_blocked_on = NULL;\r\n}\r\nif (rt_mutex_has_waiters(lock)) {\r\ntop = rt_mutex_top_waiter(lock);\r\ntop->pi_list_entry.prio = top->list_entry.prio;\r\nplist_add(&top->pi_list_entry, &task->pi_waiters);\r\n}\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\n}\r\ndebug_rt_mutex_lock(lock);\r\nrt_mutex_set_owner(lock, task);\r\nrt_mutex_deadlock_account_lock(lock, task);\r\nreturn 1;\r\n}\r\nstatic int task_blocks_on_rt_mutex(struct rt_mutex *lock,\r\nstruct rt_mutex_waiter *waiter,\r\nstruct task_struct *task,\r\nint detect_deadlock)\r\n{\r\nstruct task_struct *owner = rt_mutex_owner(lock);\r\nstruct rt_mutex_waiter *top_waiter = waiter;\r\nunsigned long flags;\r\nint chain_walk = 0, res;\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\n__rt_mutex_adjust_prio(task);\r\nwaiter->task = task;\r\nwaiter->lock = lock;\r\nplist_node_init(&waiter->list_entry, task->prio);\r\nplist_node_init(&waiter->pi_list_entry, task->prio);\r\nif (rt_mutex_has_waiters(lock))\r\ntop_waiter = rt_mutex_top_waiter(lock);\r\nplist_add(&waiter->list_entry, &lock->wait_list);\r\ntask->pi_blocked_on = waiter;\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nif (!owner)\r\nreturn 0;\r\nif (waiter == rt_mutex_top_waiter(lock)) {\r\nraw_spin_lock_irqsave(&owner->pi_lock, flags);\r\nplist_del(&top_waiter->pi_list_entry, &owner->pi_waiters);\r\nplist_add(&waiter->pi_list_entry, &owner->pi_waiters);\r\n__rt_mutex_adjust_prio(owner);\r\nif (owner->pi_blocked_on)\r\nchain_walk = 1;\r\nraw_spin_unlock_irqrestore(&owner->pi_lock, flags);\r\n}\r\nelse if (debug_rt_mutex_detect_deadlock(waiter, detect_deadlock))\r\nchain_walk = 1;\r\nif (!chain_walk)\r\nreturn 0;\r\nget_task_struct(owner);\r\nraw_spin_unlock(&lock->wait_lock);\r\nres = rt_mutex_adjust_prio_chain(owner, detect_deadlock, lock, waiter,\r\ntask);\r\nraw_spin_lock(&lock->wait_lock);\r\nreturn res;\r\n}\r\nstatic void wakeup_next_waiter(struct rt_mutex *lock)\r\n{\r\nstruct rt_mutex_waiter *waiter;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&current->pi_lock, flags);\r\nwaiter = rt_mutex_top_waiter(lock);\r\nplist_del(&waiter->pi_list_entry, &current->pi_waiters);\r\nrt_mutex_set_owner(lock, NULL);\r\nraw_spin_unlock_irqrestore(&current->pi_lock, flags);\r\nwake_up_process(waiter->task);\r\n}\r\nstatic void remove_waiter(struct rt_mutex *lock,\r\nstruct rt_mutex_waiter *waiter)\r\n{\r\nint first = (waiter == rt_mutex_top_waiter(lock));\r\nstruct task_struct *owner = rt_mutex_owner(lock);\r\nunsigned long flags;\r\nint chain_walk = 0;\r\nraw_spin_lock_irqsave(&current->pi_lock, flags);\r\nplist_del(&waiter->list_entry, &lock->wait_list);\r\ncurrent->pi_blocked_on = NULL;\r\nraw_spin_unlock_irqrestore(&current->pi_lock, flags);\r\nif (!owner)\r\nreturn;\r\nif (first) {\r\nraw_spin_lock_irqsave(&owner->pi_lock, flags);\r\nplist_del(&waiter->pi_list_entry, &owner->pi_waiters);\r\nif (rt_mutex_has_waiters(lock)) {\r\nstruct rt_mutex_waiter *next;\r\nnext = rt_mutex_top_waiter(lock);\r\nplist_add(&next->pi_list_entry, &owner->pi_waiters);\r\n}\r\n__rt_mutex_adjust_prio(owner);\r\nif (owner->pi_blocked_on)\r\nchain_walk = 1;\r\nraw_spin_unlock_irqrestore(&owner->pi_lock, flags);\r\n}\r\nWARN_ON(!plist_node_empty(&waiter->pi_list_entry));\r\nif (!chain_walk)\r\nreturn;\r\nget_task_struct(owner);\r\nraw_spin_unlock(&lock->wait_lock);\r\nrt_mutex_adjust_prio_chain(owner, 0, lock, NULL, current);\r\nraw_spin_lock(&lock->wait_lock);\r\n}\r\nvoid rt_mutex_adjust_pi(struct task_struct *task)\r\n{\r\nstruct rt_mutex_waiter *waiter;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&task->pi_lock, flags);\r\nwaiter = task->pi_blocked_on;\r\nif (!waiter || waiter->list_entry.prio == task->prio) {\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nreturn;\r\n}\r\nraw_spin_unlock_irqrestore(&task->pi_lock, flags);\r\nget_task_struct(task);\r\nrt_mutex_adjust_prio_chain(task, 0, NULL, NULL, task);\r\n}\r\nstatic int __sched\r\n__rt_mutex_slowlock(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout,\r\nstruct rt_mutex_waiter *waiter)\r\n{\r\nint ret = 0;\r\nfor (;;) {\r\nif (try_to_take_rt_mutex(lock, current, waiter))\r\nbreak;\r\nif (unlikely(state == TASK_INTERRUPTIBLE)) {\r\nif (signal_pending(current))\r\nret = -EINTR;\r\nif (timeout && !timeout->task)\r\nret = -ETIMEDOUT;\r\nif (ret)\r\nbreak;\r\n}\r\nraw_spin_unlock(&lock->wait_lock);\r\ndebug_rt_mutex_print_deadlock(waiter);\r\nschedule_rt_mutex(lock);\r\nraw_spin_lock(&lock->wait_lock);\r\nset_current_state(state);\r\n}\r\nreturn ret;\r\n}\r\nstatic int __sched\r\nrt_mutex_slowlock(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout,\r\nint detect_deadlock)\r\n{\r\nstruct rt_mutex_waiter waiter;\r\nint ret = 0;\r\ndebug_rt_mutex_init_waiter(&waiter);\r\nraw_spin_lock(&lock->wait_lock);\r\nif (try_to_take_rt_mutex(lock, current, NULL)) {\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn 0;\r\n}\r\nset_current_state(state);\r\nif (unlikely(timeout)) {\r\nhrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);\r\nif (!hrtimer_active(&timeout->timer))\r\ntimeout->task = NULL;\r\n}\r\nret = task_blocks_on_rt_mutex(lock, &waiter, current, detect_deadlock);\r\nif (likely(!ret))\r\nret = __rt_mutex_slowlock(lock, state, timeout, &waiter);\r\nset_current_state(TASK_RUNNING);\r\nif (unlikely(ret))\r\nremove_waiter(lock, &waiter);\r\nfixup_rt_mutex_waiters(lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nif (unlikely(timeout))\r\nhrtimer_cancel(&timeout->timer);\r\ndebug_rt_mutex_free_waiter(&waiter);\r\nreturn ret;\r\n}\r\nstatic inline int\r\nrt_mutex_slowtrylock(struct rt_mutex *lock)\r\n{\r\nint ret = 0;\r\nraw_spin_lock(&lock->wait_lock);\r\nif (likely(rt_mutex_owner(lock) != current)) {\r\nret = try_to_take_rt_mutex(lock, current, NULL);\r\nfixup_rt_mutex_waiters(lock);\r\n}\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn ret;\r\n}\r\nstatic void __sched\r\nrt_mutex_slowunlock(struct rt_mutex *lock)\r\n{\r\nraw_spin_lock(&lock->wait_lock);\r\ndebug_rt_mutex_unlock(lock);\r\nrt_mutex_deadlock_account_unlock(current);\r\nif (!rt_mutex_has_waiters(lock)) {\r\nlock->owner = NULL;\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn;\r\n}\r\nwakeup_next_waiter(lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nrt_mutex_adjust_prio(current);\r\n}\r\nstatic inline int\r\nrt_mutex_fastlock(struct rt_mutex *lock, int state,\r\nint detect_deadlock,\r\nint (*slowfn)(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout,\r\nint detect_deadlock))\r\n{\r\nif (!detect_deadlock && likely(rt_mutex_cmpxchg(lock, NULL, current))) {\r\nrt_mutex_deadlock_account_lock(lock, current);\r\nreturn 0;\r\n} else\r\nreturn slowfn(lock, state, NULL, detect_deadlock);\r\n}\r\nstatic inline int\r\nrt_mutex_timed_fastlock(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout, int detect_deadlock,\r\nint (*slowfn)(struct rt_mutex *lock, int state,\r\nstruct hrtimer_sleeper *timeout,\r\nint detect_deadlock))\r\n{\r\nif (!detect_deadlock && likely(rt_mutex_cmpxchg(lock, NULL, current))) {\r\nrt_mutex_deadlock_account_lock(lock, current);\r\nreturn 0;\r\n} else\r\nreturn slowfn(lock, state, timeout, detect_deadlock);\r\n}\r\nstatic inline int\r\nrt_mutex_fasttrylock(struct rt_mutex *lock,\r\nint (*slowfn)(struct rt_mutex *lock))\r\n{\r\nif (likely(rt_mutex_cmpxchg(lock, NULL, current))) {\r\nrt_mutex_deadlock_account_lock(lock, current);\r\nreturn 1;\r\n}\r\nreturn slowfn(lock);\r\n}\r\nstatic inline void\r\nrt_mutex_fastunlock(struct rt_mutex *lock,\r\nvoid (*slowfn)(struct rt_mutex *lock))\r\n{\r\nif (likely(rt_mutex_cmpxchg(lock, current, NULL)))\r\nrt_mutex_deadlock_account_unlock(current);\r\nelse\r\nslowfn(lock);\r\n}\r\nvoid __sched rt_mutex_lock(struct rt_mutex *lock)\r\n{\r\nmight_sleep();\r\nrt_mutex_fastlock(lock, TASK_UNINTERRUPTIBLE, 0, rt_mutex_slowlock);\r\n}\r\nint __sched rt_mutex_lock_interruptible(struct rt_mutex *lock,\r\nint detect_deadlock)\r\n{\r\nmight_sleep();\r\nreturn rt_mutex_fastlock(lock, TASK_INTERRUPTIBLE,\r\ndetect_deadlock, rt_mutex_slowlock);\r\n}\r\nint\r\nrt_mutex_timed_lock(struct rt_mutex *lock, struct hrtimer_sleeper *timeout,\r\nint detect_deadlock)\r\n{\r\nmight_sleep();\r\nreturn rt_mutex_timed_fastlock(lock, TASK_INTERRUPTIBLE, timeout,\r\ndetect_deadlock, rt_mutex_slowlock);\r\n}\r\nint __sched rt_mutex_trylock(struct rt_mutex *lock)\r\n{\r\nreturn rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);\r\n}\r\nvoid __sched rt_mutex_unlock(struct rt_mutex *lock)\r\n{\r\nrt_mutex_fastunlock(lock, rt_mutex_slowunlock);\r\n}\r\nvoid rt_mutex_destroy(struct rt_mutex *lock)\r\n{\r\nWARN_ON(rt_mutex_is_locked(lock));\r\n#ifdef CONFIG_DEBUG_RT_MUTEXES\r\nlock->magic = NULL;\r\n#endif\r\n}\r\nvoid __rt_mutex_init(struct rt_mutex *lock, const char *name)\r\n{\r\nlock->owner = NULL;\r\nraw_spin_lock_init(&lock->wait_lock);\r\nplist_head_init(&lock->wait_list);\r\ndebug_rt_mutex_init(lock, name);\r\n}\r\nvoid rt_mutex_init_proxy_locked(struct rt_mutex *lock,\r\nstruct task_struct *proxy_owner)\r\n{\r\n__rt_mutex_init(lock, NULL);\r\ndebug_rt_mutex_proxy_lock(lock, proxy_owner);\r\nrt_mutex_set_owner(lock, proxy_owner);\r\nrt_mutex_deadlock_account_lock(lock, proxy_owner);\r\n}\r\nvoid rt_mutex_proxy_unlock(struct rt_mutex *lock,\r\nstruct task_struct *proxy_owner)\r\n{\r\ndebug_rt_mutex_proxy_unlock(lock);\r\nrt_mutex_set_owner(lock, NULL);\r\nrt_mutex_deadlock_account_unlock(proxy_owner);\r\n}\r\nint rt_mutex_start_proxy_lock(struct rt_mutex *lock,\r\nstruct rt_mutex_waiter *waiter,\r\nstruct task_struct *task, int detect_deadlock)\r\n{\r\nint ret;\r\nraw_spin_lock(&lock->wait_lock);\r\nif (try_to_take_rt_mutex(lock, task, NULL)) {\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn 1;\r\n}\r\nret = task_blocks_on_rt_mutex(lock, waiter, task, detect_deadlock);\r\nif (ret && !rt_mutex_owner(lock)) {\r\nret = 0;\r\n}\r\nif (unlikely(ret))\r\nremove_waiter(lock, waiter);\r\nraw_spin_unlock(&lock->wait_lock);\r\ndebug_rt_mutex_print_deadlock(waiter);\r\nreturn ret;\r\n}\r\nstruct task_struct *rt_mutex_next_owner(struct rt_mutex *lock)\r\n{\r\nif (!rt_mutex_has_waiters(lock))\r\nreturn NULL;\r\nreturn rt_mutex_top_waiter(lock)->task;\r\n}\r\nint rt_mutex_finish_proxy_lock(struct rt_mutex *lock,\r\nstruct hrtimer_sleeper *to,\r\nstruct rt_mutex_waiter *waiter,\r\nint detect_deadlock)\r\n{\r\nint ret;\r\nraw_spin_lock(&lock->wait_lock);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nret = __rt_mutex_slowlock(lock, TASK_INTERRUPTIBLE, to, waiter);\r\nset_current_state(TASK_RUNNING);\r\nif (unlikely(ret))\r\nremove_waiter(lock, waiter);\r\nfixup_rt_mutex_waiters(lock);\r\nraw_spin_unlock(&lock->wait_lock);\r\nreturn ret;\r\n}
