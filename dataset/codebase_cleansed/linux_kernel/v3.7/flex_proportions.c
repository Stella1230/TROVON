int fprop_global_init(struct fprop_global *p)\r\n{\r\nint err;\r\np->period = 0;\r\nerr = percpu_counter_init(&p->events, 1);\r\nif (err)\r\nreturn err;\r\nseqcount_init(&p->sequence);\r\nreturn 0;\r\n}\r\nvoid fprop_global_destroy(struct fprop_global *p)\r\n{\r\npercpu_counter_destroy(&p->events);\r\n}\r\nbool fprop_new_period(struct fprop_global *p, int periods)\r\n{\r\ns64 events;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nevents = percpu_counter_sum(&p->events);\r\nif (events <= 1) {\r\nlocal_irq_restore(flags);\r\nreturn false;\r\n}\r\nwrite_seqcount_begin(&p->sequence);\r\nif (periods < 64)\r\nevents -= events >> periods;\r\npercpu_counter_add(&p->events, -events);\r\np->period += periods;\r\nwrite_seqcount_end(&p->sequence);\r\nlocal_irq_restore(flags);\r\nreturn true;\r\n}\r\nint fprop_local_init_single(struct fprop_local_single *pl)\r\n{\r\npl->events = 0;\r\npl->period = 0;\r\nraw_spin_lock_init(&pl->lock);\r\nreturn 0;\r\n}\r\nvoid fprop_local_destroy_single(struct fprop_local_single *pl)\r\n{\r\n}\r\nstatic void fprop_reflect_period_single(struct fprop_global *p,\r\nstruct fprop_local_single *pl)\r\n{\r\nunsigned int period = p->period;\r\nunsigned long flags;\r\nif (pl->period == period)\r\nreturn;\r\nraw_spin_lock_irqsave(&pl->lock, flags);\r\nif (pl->period >= period) {\r\nraw_spin_unlock_irqrestore(&pl->lock, flags);\r\nreturn;\r\n}\r\nif (period - pl->period < BITS_PER_LONG)\r\npl->events >>= period - pl->period;\r\nelse\r\npl->events = 0;\r\npl->period = period;\r\nraw_spin_unlock_irqrestore(&pl->lock, flags);\r\n}\r\nvoid __fprop_inc_single(struct fprop_global *p, struct fprop_local_single *pl)\r\n{\r\nfprop_reflect_period_single(p, pl);\r\npl->events++;\r\npercpu_counter_add(&p->events, 1);\r\n}\r\nvoid fprop_fraction_single(struct fprop_global *p,\r\nstruct fprop_local_single *pl,\r\nunsigned long *numerator, unsigned long *denominator)\r\n{\r\nunsigned int seq;\r\ns64 num, den;\r\ndo {\r\nseq = read_seqcount_begin(&p->sequence);\r\nfprop_reflect_period_single(p, pl);\r\nnum = pl->events;\r\nden = percpu_counter_read_positive(&p->events);\r\n} while (read_seqcount_retry(&p->sequence, seq));\r\nif (den <= num) {\r\nif (num)\r\nden = num;\r\nelse\r\nden = 1;\r\n}\r\n*denominator = den;\r\n*numerator = num;\r\n}\r\nint fprop_local_init_percpu(struct fprop_local_percpu *pl)\r\n{\r\nint err;\r\nerr = percpu_counter_init(&pl->events, 0);\r\nif (err)\r\nreturn err;\r\npl->period = 0;\r\nraw_spin_lock_init(&pl->lock);\r\nreturn 0;\r\n}\r\nvoid fprop_local_destroy_percpu(struct fprop_local_percpu *pl)\r\n{\r\npercpu_counter_destroy(&pl->events);\r\n}\r\nstatic void fprop_reflect_period_percpu(struct fprop_global *p,\r\nstruct fprop_local_percpu *pl)\r\n{\r\nunsigned int period = p->period;\r\nunsigned long flags;\r\nif (pl->period == period)\r\nreturn;\r\nraw_spin_lock_irqsave(&pl->lock, flags);\r\nif (pl->period >= period) {\r\nraw_spin_unlock_irqrestore(&pl->lock, flags);\r\nreturn;\r\n}\r\nif (period - pl->period < BITS_PER_LONG) {\r\ns64 val = percpu_counter_read(&pl->events);\r\nif (val < (nr_cpu_ids * PROP_BATCH))\r\nval = percpu_counter_sum(&pl->events);\r\n__percpu_counter_add(&pl->events,\r\n-val + (val >> (period-pl->period)), PROP_BATCH);\r\n} else\r\npercpu_counter_set(&pl->events, 0);\r\npl->period = period;\r\nraw_spin_unlock_irqrestore(&pl->lock, flags);\r\n}\r\nvoid __fprop_inc_percpu(struct fprop_global *p, struct fprop_local_percpu *pl)\r\n{\r\nfprop_reflect_period_percpu(p, pl);\r\n__percpu_counter_add(&pl->events, 1, PROP_BATCH);\r\npercpu_counter_add(&p->events, 1);\r\n}\r\nvoid fprop_fraction_percpu(struct fprop_global *p,\r\nstruct fprop_local_percpu *pl,\r\nunsigned long *numerator, unsigned long *denominator)\r\n{\r\nunsigned int seq;\r\ns64 num, den;\r\ndo {\r\nseq = read_seqcount_begin(&p->sequence);\r\nfprop_reflect_period_percpu(p, pl);\r\nnum = percpu_counter_read_positive(&pl->events);\r\nden = percpu_counter_read_positive(&p->events);\r\n} while (read_seqcount_retry(&p->sequence, seq));\r\nif (den <= num) {\r\nif (num)\r\nden = num;\r\nelse\r\nden = 1;\r\n}\r\n*denominator = den;\r\n*numerator = num;\r\n}\r\nvoid __fprop_inc_percpu_max(struct fprop_global *p,\r\nstruct fprop_local_percpu *pl, int max_frac)\r\n{\r\nif (unlikely(max_frac < FPROP_FRAC_BASE)) {\r\nunsigned long numerator, denominator;\r\nfprop_fraction_percpu(p, pl, &numerator, &denominator);\r\nif (numerator >\r\n(((u64)denominator) * max_frac) >> FPROP_FRAC_SHIFT)\r\nreturn;\r\n} else\r\nfprop_reflect_period_percpu(p, pl);\r\n__percpu_counter_add(&pl->events, 1, PROP_BATCH);\r\npercpu_counter_add(&p->events, 1);\r\n}
