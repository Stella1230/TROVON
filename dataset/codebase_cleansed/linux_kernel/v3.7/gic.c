static void __iomem *gic_get_percpu_base(union gic_base *base)\r\n{\r\nreturn *__this_cpu_ptr(base->percpu_base);\r\n}\r\nstatic void __iomem *gic_get_common_base(union gic_base *base)\r\n{\r\nreturn base->common_base;\r\n}\r\nstatic inline void __iomem *gic_data_dist_base(struct gic_chip_data *data)\r\n{\r\nreturn data->get_base(&data->dist_base);\r\n}\r\nstatic inline void __iomem *gic_data_cpu_base(struct gic_chip_data *data)\r\n{\r\nreturn data->get_base(&data->cpu_base);\r\n}\r\nstatic inline void __iomem *gic_dist_base(struct irq_data *d)\r\n{\r\nstruct gic_chip_data *gic_data = irq_data_get_irq_chip_data(d);\r\nreturn gic_data_dist_base(gic_data);\r\n}\r\nstatic inline void __iomem *gic_cpu_base(struct irq_data *d)\r\n{\r\nstruct gic_chip_data *gic_data = irq_data_get_irq_chip_data(d);\r\nreturn gic_data_cpu_base(gic_data);\r\n}\r\nstatic inline unsigned int gic_irq(struct irq_data *d)\r\n{\r\nreturn d->hwirq;\r\n}\r\nstatic void gic_mask_irq(struct irq_data *d)\r\n{\r\nu32 mask = 1 << (gic_irq(d) % 32);\r\nraw_spin_lock(&irq_controller_lock);\r\nwritel_relaxed(mask, gic_dist_base(d) + GIC_DIST_ENABLE_CLEAR + (gic_irq(d) / 32) * 4);\r\nif (gic_arch_extn.irq_mask)\r\ngic_arch_extn.irq_mask(d);\r\nraw_spin_unlock(&irq_controller_lock);\r\n}\r\nstatic void gic_unmask_irq(struct irq_data *d)\r\n{\r\nu32 mask = 1 << (gic_irq(d) % 32);\r\nraw_spin_lock(&irq_controller_lock);\r\nif (gic_arch_extn.irq_unmask)\r\ngic_arch_extn.irq_unmask(d);\r\nwritel_relaxed(mask, gic_dist_base(d) + GIC_DIST_ENABLE_SET + (gic_irq(d) / 32) * 4);\r\nraw_spin_unlock(&irq_controller_lock);\r\n}\r\nstatic void gic_eoi_irq(struct irq_data *d)\r\n{\r\nif (gic_arch_extn.irq_eoi) {\r\nraw_spin_lock(&irq_controller_lock);\r\ngic_arch_extn.irq_eoi(d);\r\nraw_spin_unlock(&irq_controller_lock);\r\n}\r\nwritel_relaxed(gic_irq(d), gic_cpu_base(d) + GIC_CPU_EOI);\r\n}\r\nstatic int gic_set_type(struct irq_data *d, unsigned int type)\r\n{\r\nvoid __iomem *base = gic_dist_base(d);\r\nunsigned int gicirq = gic_irq(d);\r\nu32 enablemask = 1 << (gicirq % 32);\r\nu32 enableoff = (gicirq / 32) * 4;\r\nu32 confmask = 0x2 << ((gicirq % 16) * 2);\r\nu32 confoff = (gicirq / 16) * 4;\r\nbool enabled = false;\r\nu32 val;\r\nif (gicirq < 16)\r\nreturn -EINVAL;\r\nif (type != IRQ_TYPE_LEVEL_HIGH && type != IRQ_TYPE_EDGE_RISING)\r\nreturn -EINVAL;\r\nraw_spin_lock(&irq_controller_lock);\r\nif (gic_arch_extn.irq_set_type)\r\ngic_arch_extn.irq_set_type(d, type);\r\nval = readl_relaxed(base + GIC_DIST_CONFIG + confoff);\r\nif (type == IRQ_TYPE_LEVEL_HIGH)\r\nval &= ~confmask;\r\nelse if (type == IRQ_TYPE_EDGE_RISING)\r\nval |= confmask;\r\nif (readl_relaxed(base + GIC_DIST_ENABLE_SET + enableoff) & enablemask) {\r\nwritel_relaxed(enablemask, base + GIC_DIST_ENABLE_CLEAR + enableoff);\r\nenabled = true;\r\n}\r\nwritel_relaxed(val, base + GIC_DIST_CONFIG + confoff);\r\nif (enabled)\r\nwritel_relaxed(enablemask, base + GIC_DIST_ENABLE_SET + enableoff);\r\nraw_spin_unlock(&irq_controller_lock);\r\nreturn 0;\r\n}\r\nstatic int gic_retrigger(struct irq_data *d)\r\n{\r\nif (gic_arch_extn.irq_retrigger)\r\nreturn gic_arch_extn.irq_retrigger(d);\r\nreturn -ENXIO;\r\n}\r\nstatic int gic_set_affinity(struct irq_data *d, const struct cpumask *mask_val,\r\nbool force)\r\n{\r\nvoid __iomem *reg = gic_dist_base(d) + GIC_DIST_TARGET + (gic_irq(d) & ~3);\r\nunsigned int shift = (gic_irq(d) % 4) * 8;\r\nunsigned int cpu = cpumask_any_and(mask_val, cpu_online_mask);\r\nu32 val, mask, bit;\r\nif (cpu >= 8 || cpu >= nr_cpu_ids)\r\nreturn -EINVAL;\r\nmask = 0xff << shift;\r\nbit = 1 << (cpu_logical_map(cpu) + shift);\r\nraw_spin_lock(&irq_controller_lock);\r\nval = readl_relaxed(reg) & ~mask;\r\nwritel_relaxed(val | bit, reg);\r\nraw_spin_unlock(&irq_controller_lock);\r\nreturn IRQ_SET_MASK_OK;\r\n}\r\nstatic int gic_set_wake(struct irq_data *d, unsigned int on)\r\n{\r\nint ret = -ENXIO;\r\nif (gic_arch_extn.irq_set_wake)\r\nret = gic_arch_extn.irq_set_wake(d, on);\r\nreturn ret;\r\n}\r\nasmlinkage void __exception_irq_entry gic_handle_irq(struct pt_regs *regs)\r\n{\r\nu32 irqstat, irqnr;\r\nstruct gic_chip_data *gic = &gic_data[0];\r\nvoid __iomem *cpu_base = gic_data_cpu_base(gic);\r\ndo {\r\nirqstat = readl_relaxed(cpu_base + GIC_CPU_INTACK);\r\nirqnr = irqstat & ~0x1c00;\r\nif (likely(irqnr > 15 && irqnr < 1021)) {\r\nirqnr = irq_find_mapping(gic->domain, irqnr);\r\nhandle_IRQ(irqnr, regs);\r\ncontinue;\r\n}\r\nif (irqnr < 16) {\r\nwritel_relaxed(irqstat, cpu_base + GIC_CPU_EOI);\r\n#ifdef CONFIG_SMP\r\nhandle_IPI(irqnr, regs);\r\n#endif\r\ncontinue;\r\n}\r\nbreak;\r\n} while (1);\r\n}\r\nstatic void gic_handle_cascade_irq(unsigned int irq, struct irq_desc *desc)\r\n{\r\nstruct gic_chip_data *chip_data = irq_get_handler_data(irq);\r\nstruct irq_chip *chip = irq_get_chip(irq);\r\nunsigned int cascade_irq, gic_irq;\r\nunsigned long status;\r\nchained_irq_enter(chip, desc);\r\nraw_spin_lock(&irq_controller_lock);\r\nstatus = readl_relaxed(gic_data_cpu_base(chip_data) + GIC_CPU_INTACK);\r\nraw_spin_unlock(&irq_controller_lock);\r\ngic_irq = (status & 0x3ff);\r\nif (gic_irq == 1023)\r\ngoto out;\r\ncascade_irq = irq_find_mapping(chip_data->domain, gic_irq);\r\nif (unlikely(gic_irq < 32 || gic_irq > 1020))\r\ndo_bad_IRQ(cascade_irq, desc);\r\nelse\r\ngeneric_handle_irq(cascade_irq);\r\nout:\r\nchained_irq_exit(chip, desc);\r\n}\r\nvoid __init gic_cascade_irq(unsigned int gic_nr, unsigned int irq)\r\n{\r\nif (gic_nr >= MAX_GIC_NR)\r\nBUG();\r\nif (irq_set_handler_data(irq, &gic_data[gic_nr]) != 0)\r\nBUG();\r\nirq_set_chained_handler(irq, gic_handle_cascade_irq);\r\n}\r\nstatic void __init gic_dist_init(struct gic_chip_data *gic)\r\n{\r\nunsigned int i;\r\nu32 cpumask;\r\nunsigned int gic_irqs = gic->gic_irqs;\r\nvoid __iomem *base = gic_data_dist_base(gic);\r\nu32 cpu = cpu_logical_map(smp_processor_id());\r\ncpumask = 1 << cpu;\r\ncpumask |= cpumask << 8;\r\ncpumask |= cpumask << 16;\r\nwritel_relaxed(0, base + GIC_DIST_CTRL);\r\nfor (i = 32; i < gic_irqs; i += 16)\r\nwritel_relaxed(0, base + GIC_DIST_CONFIG + i * 4 / 16);\r\nfor (i = 32; i < gic_irqs; i += 4)\r\nwritel_relaxed(cpumask, base + GIC_DIST_TARGET + i * 4 / 4);\r\nfor (i = 32; i < gic_irqs; i += 4)\r\nwritel_relaxed(0xa0a0a0a0, base + GIC_DIST_PRI + i * 4 / 4);\r\nfor (i = 32; i < gic_irqs; i += 32)\r\nwritel_relaxed(0xffffffff, base + GIC_DIST_ENABLE_CLEAR + i * 4 / 32);\r\nwritel_relaxed(1, base + GIC_DIST_CTRL);\r\n}\r\nstatic void __cpuinit gic_cpu_init(struct gic_chip_data *gic)\r\n{\r\nvoid __iomem *dist_base = gic_data_dist_base(gic);\r\nvoid __iomem *base = gic_data_cpu_base(gic);\r\nint i;\r\nwritel_relaxed(0xffff0000, dist_base + GIC_DIST_ENABLE_CLEAR);\r\nwritel_relaxed(0x0000ffff, dist_base + GIC_DIST_ENABLE_SET);\r\nfor (i = 0; i < 32; i += 4)\r\nwritel_relaxed(0xa0a0a0a0, dist_base + GIC_DIST_PRI + i * 4 / 4);\r\nwritel_relaxed(0xf0, base + GIC_CPU_PRIMASK);\r\nwritel_relaxed(1, base + GIC_CPU_CTRL);\r\n}\r\nstatic void gic_dist_save(unsigned int gic_nr)\r\n{\r\nunsigned int gic_irqs;\r\nvoid __iomem *dist_base;\r\nint i;\r\nif (gic_nr >= MAX_GIC_NR)\r\nBUG();\r\ngic_irqs = gic_data[gic_nr].gic_irqs;\r\ndist_base = gic_data_dist_base(&gic_data[gic_nr]);\r\nif (!dist_base)\r\nreturn;\r\nfor (i = 0; i < DIV_ROUND_UP(gic_irqs, 16); i++)\r\ngic_data[gic_nr].saved_spi_conf[i] =\r\nreadl_relaxed(dist_base + GIC_DIST_CONFIG + i * 4);\r\nfor (i = 0; i < DIV_ROUND_UP(gic_irqs, 4); i++)\r\ngic_data[gic_nr].saved_spi_target[i] =\r\nreadl_relaxed(dist_base + GIC_DIST_TARGET + i * 4);\r\nfor (i = 0; i < DIV_ROUND_UP(gic_irqs, 32); i++)\r\ngic_data[gic_nr].saved_spi_enable[i] =\r\nreadl_relaxed(dist_base + GIC_DIST_ENABLE_SET + i * 4);\r\n}\r\nstatic void gic_dist_restore(unsigned int gic_nr)\r\n{\r\nunsigned int gic_irqs;\r\nunsigned int i;\r\nvoid __iomem *dist_base;\r\nif (gic_nr >= MAX_GIC_NR)\r\nBUG();\r\ngic_irqs = gic_data[gic_nr].gic_irqs;\r\ndist_base = gic_data_dist_base(&gic_data[gic_nr]);\r\nif (!dist_base)\r\nreturn;\r\nwritel_relaxed(0, dist_base + GIC_DIST_CTRL);\r\nfor (i = 0; i < DIV_ROUND_UP(gic_irqs, 16); i++)\r\nwritel_relaxed(gic_data[gic_nr].saved_spi_conf[i],\r\ndist_base + GIC_DIST_CONFIG + i * 4);\r\nfor (i = 0; i < DIV_ROUND_UP(gic_irqs, 4); i++)\r\nwritel_relaxed(0xa0a0a0a0,\r\ndist_base + GIC_DIST_PRI + i * 4);\r\nfor (i = 0; i < DIV_ROUND_UP(gic_irqs, 4); i++)\r\nwritel_relaxed(gic_data[gic_nr].saved_spi_target[i],\r\ndist_base + GIC_DIST_TARGET + i * 4);\r\nfor (i = 0; i < DIV_ROUND_UP(gic_irqs, 32); i++)\r\nwritel_relaxed(gic_data[gic_nr].saved_spi_enable[i],\r\ndist_base + GIC_DIST_ENABLE_SET + i * 4);\r\nwritel_relaxed(1, dist_base + GIC_DIST_CTRL);\r\n}\r\nstatic void gic_cpu_save(unsigned int gic_nr)\r\n{\r\nint i;\r\nu32 *ptr;\r\nvoid __iomem *dist_base;\r\nvoid __iomem *cpu_base;\r\nif (gic_nr >= MAX_GIC_NR)\r\nBUG();\r\ndist_base = gic_data_dist_base(&gic_data[gic_nr]);\r\ncpu_base = gic_data_cpu_base(&gic_data[gic_nr]);\r\nif (!dist_base || !cpu_base)\r\nreturn;\r\nptr = __this_cpu_ptr(gic_data[gic_nr].saved_ppi_enable);\r\nfor (i = 0; i < DIV_ROUND_UP(32, 32); i++)\r\nptr[i] = readl_relaxed(dist_base + GIC_DIST_ENABLE_SET + i * 4);\r\nptr = __this_cpu_ptr(gic_data[gic_nr].saved_ppi_conf);\r\nfor (i = 0; i < DIV_ROUND_UP(32, 16); i++)\r\nptr[i] = readl_relaxed(dist_base + GIC_DIST_CONFIG + i * 4);\r\n}\r\nstatic void gic_cpu_restore(unsigned int gic_nr)\r\n{\r\nint i;\r\nu32 *ptr;\r\nvoid __iomem *dist_base;\r\nvoid __iomem *cpu_base;\r\nif (gic_nr >= MAX_GIC_NR)\r\nBUG();\r\ndist_base = gic_data_dist_base(&gic_data[gic_nr]);\r\ncpu_base = gic_data_cpu_base(&gic_data[gic_nr]);\r\nif (!dist_base || !cpu_base)\r\nreturn;\r\nptr = __this_cpu_ptr(gic_data[gic_nr].saved_ppi_enable);\r\nfor (i = 0; i < DIV_ROUND_UP(32, 32); i++)\r\nwritel_relaxed(ptr[i], dist_base + GIC_DIST_ENABLE_SET + i * 4);\r\nptr = __this_cpu_ptr(gic_data[gic_nr].saved_ppi_conf);\r\nfor (i = 0; i < DIV_ROUND_UP(32, 16); i++)\r\nwritel_relaxed(ptr[i], dist_base + GIC_DIST_CONFIG + i * 4);\r\nfor (i = 0; i < DIV_ROUND_UP(32, 4); i++)\r\nwritel_relaxed(0xa0a0a0a0, dist_base + GIC_DIST_PRI + i * 4);\r\nwritel_relaxed(0xf0, cpu_base + GIC_CPU_PRIMASK);\r\nwritel_relaxed(1, cpu_base + GIC_CPU_CTRL);\r\n}\r\nstatic int gic_notifier(struct notifier_block *self, unsigned long cmd, void *v)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_GIC_NR; i++) {\r\n#ifdef CONFIG_GIC_NON_BANKED\r\nif (!gic_data[i].get_base)\r\ncontinue;\r\n#endif\r\nswitch (cmd) {\r\ncase CPU_PM_ENTER:\r\ngic_cpu_save(i);\r\nbreak;\r\ncase CPU_PM_ENTER_FAILED:\r\ncase CPU_PM_EXIT:\r\ngic_cpu_restore(i);\r\nbreak;\r\ncase CPU_CLUSTER_PM_ENTER:\r\ngic_dist_save(i);\r\nbreak;\r\ncase CPU_CLUSTER_PM_ENTER_FAILED:\r\ncase CPU_CLUSTER_PM_EXIT:\r\ngic_dist_restore(i);\r\nbreak;\r\n}\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void __init gic_pm_init(struct gic_chip_data *gic)\r\n{\r\ngic->saved_ppi_enable = __alloc_percpu(DIV_ROUND_UP(32, 32) * 4,\r\nsizeof(u32));\r\nBUG_ON(!gic->saved_ppi_enable);\r\ngic->saved_ppi_conf = __alloc_percpu(DIV_ROUND_UP(32, 16) * 4,\r\nsizeof(u32));\r\nBUG_ON(!gic->saved_ppi_conf);\r\nif (gic == &gic_data[0])\r\ncpu_pm_register_notifier(&gic_notifier_block);\r\n}\r\nstatic void __init gic_pm_init(struct gic_chip_data *gic)\r\n{\r\n}\r\nstatic int gic_irq_domain_map(struct irq_domain *d, unsigned int irq,\r\nirq_hw_number_t hw)\r\n{\r\nif (hw < 32) {\r\nirq_set_percpu_devid(irq);\r\nirq_set_chip_and_handler(irq, &gic_chip,\r\nhandle_percpu_devid_irq);\r\nset_irq_flags(irq, IRQF_VALID | IRQF_NOAUTOEN);\r\n} else {\r\nirq_set_chip_and_handler(irq, &gic_chip,\r\nhandle_fasteoi_irq);\r\nset_irq_flags(irq, IRQF_VALID | IRQF_PROBE);\r\n}\r\nirq_set_chip_data(irq, d->host_data);\r\nreturn 0;\r\n}\r\nstatic int gic_irq_domain_xlate(struct irq_domain *d,\r\nstruct device_node *controller,\r\nconst u32 *intspec, unsigned int intsize,\r\nunsigned long *out_hwirq, unsigned int *out_type)\r\n{\r\nif (d->of_node != controller)\r\nreturn -EINVAL;\r\nif (intsize < 3)\r\nreturn -EINVAL;\r\n*out_hwirq = intspec[1] + 16;\r\nif (!intspec[0])\r\n*out_hwirq += 16;\r\n*out_type = intspec[2] & IRQ_TYPE_SENSE_MASK;\r\nreturn 0;\r\n}\r\nvoid __init gic_init_bases(unsigned int gic_nr, int irq_start,\r\nvoid __iomem *dist_base, void __iomem *cpu_base,\r\nu32 percpu_offset, struct device_node *node)\r\n{\r\nirq_hw_number_t hwirq_base;\r\nstruct gic_chip_data *gic;\r\nint gic_irqs, irq_base;\r\nBUG_ON(gic_nr >= MAX_GIC_NR);\r\ngic = &gic_data[gic_nr];\r\n#ifdef CONFIG_GIC_NON_BANKED\r\nif (percpu_offset) {\r\nunsigned int cpu;\r\ngic->dist_base.percpu_base = alloc_percpu(void __iomem *);\r\ngic->cpu_base.percpu_base = alloc_percpu(void __iomem *);\r\nif (WARN_ON(!gic->dist_base.percpu_base ||\r\n!gic->cpu_base.percpu_base)) {\r\nfree_percpu(gic->dist_base.percpu_base);\r\nfree_percpu(gic->cpu_base.percpu_base);\r\nreturn;\r\n}\r\nfor_each_possible_cpu(cpu) {\r\nunsigned long offset = percpu_offset * cpu_logical_map(cpu);\r\n*per_cpu_ptr(gic->dist_base.percpu_base, cpu) = dist_base + offset;\r\n*per_cpu_ptr(gic->cpu_base.percpu_base, cpu) = cpu_base + offset;\r\n}\r\ngic_set_base_accessor(gic, gic_get_percpu_base);\r\n} else\r\n#endif\r\n{\r\nWARN(percpu_offset,\r\n"GIC_NON_BANKED not enabled, ignoring %08x offset!",\r\npercpu_offset);\r\ngic->dist_base.common_base = dist_base;\r\ngic->cpu_base.common_base = cpu_base;\r\ngic_set_base_accessor(gic, gic_get_common_base);\r\n}\r\nif (gic_nr == 0 && (irq_start & 31) > 0) {\r\nhwirq_base = 16;\r\nif (irq_start != -1)\r\nirq_start = (irq_start & ~31) + 16;\r\n} else {\r\nhwirq_base = 32;\r\n}\r\ngic_irqs = readl_relaxed(gic_data_dist_base(gic) + GIC_DIST_CTR) & 0x1f;\r\ngic_irqs = (gic_irqs + 1) * 32;\r\nif (gic_irqs > 1020)\r\ngic_irqs = 1020;\r\ngic->gic_irqs = gic_irqs;\r\ngic_irqs -= hwirq_base;\r\nirq_base = irq_alloc_descs(irq_start, 16, gic_irqs, numa_node_id());\r\nif (IS_ERR_VALUE(irq_base)) {\r\nWARN(1, "Cannot allocate irq_descs @ IRQ%d, assuming pre-allocated\n",\r\nirq_start);\r\nirq_base = irq_start;\r\n}\r\ngic->domain = irq_domain_add_legacy(node, gic_irqs, irq_base,\r\nhwirq_base, &gic_irq_domain_ops, gic);\r\nif (WARN_ON(!gic->domain))\r\nreturn;\r\ngic_chip.flags |= gic_arch_extn.flags;\r\ngic_dist_init(gic);\r\ngic_cpu_init(gic);\r\ngic_pm_init(gic);\r\n}\r\nvoid __cpuinit gic_secondary_init(unsigned int gic_nr)\r\n{\r\nBUG_ON(gic_nr >= MAX_GIC_NR);\r\ngic_cpu_init(&gic_data[gic_nr]);\r\n}\r\nvoid gic_raise_softirq(const struct cpumask *mask, unsigned int irq)\r\n{\r\nint cpu;\r\nunsigned long map = 0;\r\nfor_each_cpu(cpu, mask)\r\nmap |= 1 << cpu_logical_map(cpu);\r\ndsb();\r\nwritel_relaxed(map << 16 | irq, gic_data_dist_base(&gic_data[0]) + GIC_DIST_SOFTINT);\r\n}\r\nint __init gic_of_init(struct device_node *node, struct device_node *parent)\r\n{\r\nvoid __iomem *cpu_base;\r\nvoid __iomem *dist_base;\r\nu32 percpu_offset;\r\nint irq;\r\nif (WARN_ON(!node))\r\nreturn -ENODEV;\r\ndist_base = of_iomap(node, 0);\r\nWARN(!dist_base, "unable to map gic dist registers\n");\r\ncpu_base = of_iomap(node, 1);\r\nWARN(!cpu_base, "unable to map gic cpu registers\n");\r\nif (of_property_read_u32(node, "cpu-offset", &percpu_offset))\r\npercpu_offset = 0;\r\ngic_init_bases(gic_cnt, -1, dist_base, cpu_base, percpu_offset, node);\r\nif (parent) {\r\nirq = irq_of_parse_and_map(node, 0);\r\ngic_cascade_irq(gic_cnt, irq);\r\n}\r\ngic_cnt++;\r\nreturn 0;\r\n}
