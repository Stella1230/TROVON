static inline void high_nmcpy(unsigned char *dst, char *src)\r\n{\r\nmemcpy(dst, src, 8);\r\n}\r\nstatic inline void low_nmcpy(unsigned char *dst, char *src)\r\n{\r\nmemcpy(&dst[8], src, 8);\r\n}\r\nstatic int afiucv_pm_prepare(struct device *dev)\r\n{\r\n#ifdef CONFIG_PM_DEBUG\r\nprintk(KERN_WARNING "afiucv_pm_prepare\n");\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void afiucv_pm_complete(struct device *dev)\r\n{\r\n#ifdef CONFIG_PM_DEBUG\r\nprintk(KERN_WARNING "afiucv_pm_complete\n");\r\n#endif\r\n}\r\nstatic int afiucv_pm_freeze(struct device *dev)\r\n{\r\nstruct iucv_sock *iucv;\r\nstruct sock *sk;\r\nstruct hlist_node *node;\r\nint err = 0;\r\n#ifdef CONFIG_PM_DEBUG\r\nprintk(KERN_WARNING "afiucv_pm_freeze\n");\r\n#endif\r\nread_lock(&iucv_sk_list.lock);\r\nsk_for_each(sk, node, &iucv_sk_list.head) {\r\niucv = iucv_sk(sk);\r\nswitch (sk->sk_state) {\r\ncase IUCV_DISCONN:\r\ncase IUCV_CLOSING:\r\ncase IUCV_CONNECTED:\r\niucv_sever_path(sk, 0);\r\nbreak;\r\ncase IUCV_OPEN:\r\ncase IUCV_BOUND:\r\ncase IUCV_LISTEN:\r\ncase IUCV_CLOSED:\r\ndefault:\r\nbreak;\r\n}\r\nskb_queue_purge(&iucv->send_skb_q);\r\nskb_queue_purge(&iucv->backlog_skb_q);\r\n}\r\nread_unlock(&iucv_sk_list.lock);\r\nreturn err;\r\n}\r\nstatic int afiucv_pm_restore_thaw(struct device *dev)\r\n{\r\nstruct sock *sk;\r\nstruct hlist_node *node;\r\n#ifdef CONFIG_PM_DEBUG\r\nprintk(KERN_WARNING "afiucv_pm_restore_thaw\n");\r\n#endif\r\nread_lock(&iucv_sk_list.lock);\r\nsk_for_each(sk, node, &iucv_sk_list.head) {\r\nswitch (sk->sk_state) {\r\ncase IUCV_CONNECTED:\r\nsk->sk_err = EPIPE;\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\nbreak;\r\ncase IUCV_DISCONN:\r\ncase IUCV_CLOSING:\r\ncase IUCV_LISTEN:\r\ncase IUCV_BOUND:\r\ncase IUCV_OPEN:\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nread_unlock(&iucv_sk_list.lock);\r\nreturn 0;\r\n}\r\nstatic inline size_t iucv_msg_length(struct iucv_message *msg)\r\n{\r\nsize_t datalen;\r\nif (msg->flags & IUCV_IPRMDATA) {\r\ndatalen = 0xff - msg->rmmsg[7];\r\nreturn (datalen < 8) ? datalen : 8;\r\n}\r\nreturn msg->length;\r\n}\r\nstatic int iucv_sock_in_state(struct sock *sk, int state, int state2)\r\n{\r\nreturn (sk->sk_state == state || sk->sk_state == state2);\r\n}\r\nstatic inline int iucv_below_msglim(struct sock *sk)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nif (sk->sk_state != IUCV_CONNECTED)\r\nreturn 1;\r\nif (iucv->transport == AF_IUCV_TRANS_IUCV)\r\nreturn (skb_queue_len(&iucv->send_skb_q) < iucv->path->msglim);\r\nelse\r\nreturn ((atomic_read(&iucv->msg_sent) < iucv->msglimit_peer) &&\r\n(atomic_read(&iucv->pendings) <= 0));\r\n}\r\nstatic void iucv_sock_wake_msglim(struct sock *sk)\r\n{\r\nstruct socket_wq *wq;\r\nrcu_read_lock();\r\nwq = rcu_dereference(sk->sk_wq);\r\nif (wq_has_sleeper(wq))\r\nwake_up_interruptible_all(&wq->wait);\r\nsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\r\nrcu_read_unlock();\r\n}\r\nstatic int afiucv_hs_send(struct iucv_message *imsg, struct sock *sock,\r\nstruct sk_buff *skb, u8 flags)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sock);\r\nstruct af_iucv_trans_hdr *phs_hdr;\r\nstruct sk_buff *nskb;\r\nint err, confirm_recv = 0;\r\nmemset(skb->head, 0, ETH_HLEN);\r\nphs_hdr = (struct af_iucv_trans_hdr *)skb_push(skb,\r\nsizeof(struct af_iucv_trans_hdr));\r\nskb_reset_mac_header(skb);\r\nskb_reset_network_header(skb);\r\nskb_push(skb, ETH_HLEN);\r\nskb_reset_mac_header(skb);\r\nmemset(phs_hdr, 0, sizeof(struct af_iucv_trans_hdr));\r\nphs_hdr->magic = ETH_P_AF_IUCV;\r\nphs_hdr->version = 1;\r\nphs_hdr->flags = flags;\r\nif (flags == AF_IUCV_FLAG_SYN)\r\nphs_hdr->window = iucv->msglimit;\r\nelse if ((flags == AF_IUCV_FLAG_WIN) || !flags) {\r\nconfirm_recv = atomic_read(&iucv->msg_recv);\r\nphs_hdr->window = confirm_recv;\r\nif (confirm_recv)\r\nphs_hdr->flags = phs_hdr->flags | AF_IUCV_FLAG_WIN;\r\n}\r\nmemcpy(phs_hdr->destUserID, iucv->dst_user_id, 8);\r\nmemcpy(phs_hdr->destAppName, iucv->dst_name, 8);\r\nmemcpy(phs_hdr->srcUserID, iucv->src_user_id, 8);\r\nmemcpy(phs_hdr->srcAppName, iucv->src_name, 8);\r\nASCEBC(phs_hdr->destUserID, sizeof(phs_hdr->destUserID));\r\nASCEBC(phs_hdr->destAppName, sizeof(phs_hdr->destAppName));\r\nASCEBC(phs_hdr->srcUserID, sizeof(phs_hdr->srcUserID));\r\nASCEBC(phs_hdr->srcAppName, sizeof(phs_hdr->srcAppName));\r\nif (imsg)\r\nmemcpy(&phs_hdr->iucv_hdr, imsg, sizeof(struct iucv_message));\r\nskb->dev = iucv->hs_dev;\r\nif (!skb->dev)\r\nreturn -ENODEV;\r\nif (!(skb->dev->flags & IFF_UP) || !netif_carrier_ok(skb->dev))\r\nreturn -ENETDOWN;\r\nif (skb->len > skb->dev->mtu) {\r\nif (sock->sk_type == SOCK_SEQPACKET)\r\nreturn -EMSGSIZE;\r\nelse\r\nskb_trim(skb, skb->dev->mtu);\r\n}\r\nskb->protocol = ETH_P_AF_IUCV;\r\nnskb = skb_clone(skb, GFP_ATOMIC);\r\nif (!nskb)\r\nreturn -ENOMEM;\r\nskb_queue_tail(&iucv->send_skb_q, nskb);\r\nerr = dev_queue_xmit(skb);\r\nif (net_xmit_eval(err)) {\r\nskb_unlink(nskb, &iucv->send_skb_q);\r\nkfree_skb(nskb);\r\n} else {\r\natomic_sub(confirm_recv, &iucv->msg_recv);\r\nWARN_ON(atomic_read(&iucv->msg_recv) < 0);\r\n}\r\nreturn net_xmit_eval(err);\r\n}\r\nstatic struct sock *__iucv_get_sock_by_name(char *nm)\r\n{\r\nstruct sock *sk;\r\nstruct hlist_node *node;\r\nsk_for_each(sk, node, &iucv_sk_list.head)\r\nif (!memcmp(&iucv_sk(sk)->src_name, nm, 8))\r\nreturn sk;\r\nreturn NULL;\r\n}\r\nstatic void iucv_sock_destruct(struct sock *sk)\r\n{\r\nskb_queue_purge(&sk->sk_receive_queue);\r\nskb_queue_purge(&sk->sk_error_queue);\r\nsk_mem_reclaim(sk);\r\nif (!sock_flag(sk, SOCK_DEAD)) {\r\npr_err("Attempt to release alive iucv socket %p\n", sk);\r\nreturn;\r\n}\r\nWARN_ON(atomic_read(&sk->sk_rmem_alloc));\r\nWARN_ON(atomic_read(&sk->sk_wmem_alloc));\r\nWARN_ON(sk->sk_wmem_queued);\r\nWARN_ON(sk->sk_forward_alloc);\r\n}\r\nstatic void iucv_sock_cleanup_listen(struct sock *parent)\r\n{\r\nstruct sock *sk;\r\nwhile ((sk = iucv_accept_dequeue(parent, NULL))) {\r\niucv_sock_close(sk);\r\niucv_sock_kill(sk);\r\n}\r\nparent->sk_state = IUCV_CLOSED;\r\n}\r\nstatic void iucv_sock_kill(struct sock *sk)\r\n{\r\nif (!sock_flag(sk, SOCK_ZAPPED) || sk->sk_socket)\r\nreturn;\r\niucv_sock_unlink(&iucv_sk_list, sk);\r\nsock_set_flag(sk, SOCK_DEAD);\r\nsock_put(sk);\r\n}\r\nstatic void iucv_sever_path(struct sock *sk, int with_user_data)\r\n{\r\nunsigned char user_data[16];\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct iucv_path *path = iucv->path;\r\nif (iucv->path) {\r\niucv->path = NULL;\r\nif (with_user_data) {\r\nlow_nmcpy(user_data, iucv->src_name);\r\nhigh_nmcpy(user_data, iucv->dst_name);\r\nASCEBC(user_data, sizeof(user_data));\r\npr_iucv->path_sever(path, user_data);\r\n} else\r\npr_iucv->path_sever(path, NULL);\r\niucv_path_free(path);\r\n}\r\n}\r\nstatic int iucv_send_ctrl(struct sock *sk, u8 flags)\r\n{\r\nint err = 0;\r\nint blen;\r\nstruct sk_buff *skb;\r\nblen = sizeof(struct af_iucv_trans_hdr) + ETH_HLEN;\r\nskb = sock_alloc_send_skb(sk, blen, 1, &err);\r\nif (skb) {\r\nskb_reserve(skb, blen);\r\nerr = afiucv_hs_send(NULL, sk, skb, flags);\r\n}\r\nreturn err;\r\n}\r\nstatic void iucv_sock_close(struct sock *sk)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nunsigned long timeo;\r\nint err = 0;\r\nlock_sock(sk);\r\nswitch (sk->sk_state) {\r\ncase IUCV_LISTEN:\r\niucv_sock_cleanup_listen(sk);\r\nbreak;\r\ncase IUCV_CONNECTED:\r\nif (iucv->transport == AF_IUCV_TRANS_HIPER) {\r\nerr = iucv_send_ctrl(sk, AF_IUCV_FLAG_FIN);\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\n}\r\ncase IUCV_DISCONN:\r\nsk->sk_state = IUCV_CLOSING;\r\nsk->sk_state_change(sk);\r\nif (!err && !skb_queue_empty(&iucv->send_skb_q)) {\r\nif (sock_flag(sk, SOCK_LINGER) && sk->sk_lingertime)\r\ntimeo = sk->sk_lingertime;\r\nelse\r\ntimeo = IUCV_DISCONN_TIMEOUT;\r\niucv_sock_wait(sk,\r\niucv_sock_in_state(sk, IUCV_CLOSED, 0),\r\ntimeo);\r\n}\r\ncase IUCV_CLOSING:\r\nsk->sk_state = IUCV_CLOSED;\r\nsk->sk_state_change(sk);\r\nsk->sk_err = ECONNRESET;\r\nsk->sk_state_change(sk);\r\nskb_queue_purge(&iucv->send_skb_q);\r\nskb_queue_purge(&iucv->backlog_skb_q);\r\ndefault:\r\niucv_sever_path(sk, 1);\r\n}\r\nif (iucv->hs_dev) {\r\ndev_put(iucv->hs_dev);\r\niucv->hs_dev = NULL;\r\nsk->sk_bound_dev_if = 0;\r\n}\r\nsock_set_flag(sk, SOCK_ZAPPED);\r\nrelease_sock(sk);\r\n}\r\nstatic void iucv_sock_init(struct sock *sk, struct sock *parent)\r\n{\r\nif (parent)\r\nsk->sk_type = parent->sk_type;\r\n}\r\nstatic struct sock *iucv_sock_alloc(struct socket *sock, int proto, gfp_t prio)\r\n{\r\nstruct sock *sk;\r\nstruct iucv_sock *iucv;\r\nsk = sk_alloc(&init_net, PF_IUCV, prio, &iucv_proto);\r\nif (!sk)\r\nreturn NULL;\r\niucv = iucv_sk(sk);\r\nsock_init_data(sock, sk);\r\nINIT_LIST_HEAD(&iucv->accept_q);\r\nspin_lock_init(&iucv->accept_q_lock);\r\nskb_queue_head_init(&iucv->send_skb_q);\r\nINIT_LIST_HEAD(&iucv->message_q.list);\r\nspin_lock_init(&iucv->message_q.lock);\r\nskb_queue_head_init(&iucv->backlog_skb_q);\r\niucv->send_tag = 0;\r\natomic_set(&iucv->pendings, 0);\r\niucv->flags = 0;\r\niucv->msglimit = 0;\r\natomic_set(&iucv->msg_sent, 0);\r\natomic_set(&iucv->msg_recv, 0);\r\niucv->path = NULL;\r\niucv->sk_txnotify = afiucv_hs_callback_txnotify;\r\nmemset(&iucv->src_user_id , 0, 32);\r\nif (pr_iucv)\r\niucv->transport = AF_IUCV_TRANS_IUCV;\r\nelse\r\niucv->transport = AF_IUCV_TRANS_HIPER;\r\nsk->sk_destruct = iucv_sock_destruct;\r\nsk->sk_sndtimeo = IUCV_CONN_TIMEOUT;\r\nsk->sk_allocation = GFP_DMA;\r\nsock_reset_flag(sk, SOCK_ZAPPED);\r\nsk->sk_protocol = proto;\r\nsk->sk_state = IUCV_OPEN;\r\niucv_sock_link(&iucv_sk_list, sk);\r\nreturn sk;\r\n}\r\nstatic int iucv_sock_create(struct net *net, struct socket *sock, int protocol,\r\nint kern)\r\n{\r\nstruct sock *sk;\r\nif (protocol && protocol != PF_IUCV)\r\nreturn -EPROTONOSUPPORT;\r\nsock->state = SS_UNCONNECTED;\r\nswitch (sock->type) {\r\ncase SOCK_STREAM:\r\nsock->ops = &iucv_sock_ops;\r\nbreak;\r\ncase SOCK_SEQPACKET:\r\nsock->ops = &iucv_sock_ops;\r\nbreak;\r\ndefault:\r\nreturn -ESOCKTNOSUPPORT;\r\n}\r\nsk = iucv_sock_alloc(sock, protocol, GFP_KERNEL);\r\nif (!sk)\r\nreturn -ENOMEM;\r\niucv_sock_init(sk, NULL);\r\nreturn 0;\r\n}\r\nvoid iucv_sock_link(struct iucv_sock_list *l, struct sock *sk)\r\n{\r\nwrite_lock_bh(&l->lock);\r\nsk_add_node(sk, &l->head);\r\nwrite_unlock_bh(&l->lock);\r\n}\r\nvoid iucv_sock_unlink(struct iucv_sock_list *l, struct sock *sk)\r\n{\r\nwrite_lock_bh(&l->lock);\r\nsk_del_node_init(sk);\r\nwrite_unlock_bh(&l->lock);\r\n}\r\nvoid iucv_accept_enqueue(struct sock *parent, struct sock *sk)\r\n{\r\nunsigned long flags;\r\nstruct iucv_sock *par = iucv_sk(parent);\r\nsock_hold(sk);\r\nspin_lock_irqsave(&par->accept_q_lock, flags);\r\nlist_add_tail(&iucv_sk(sk)->accept_q, &par->accept_q);\r\nspin_unlock_irqrestore(&par->accept_q_lock, flags);\r\niucv_sk(sk)->parent = parent;\r\nsk_acceptq_added(parent);\r\n}\r\nvoid iucv_accept_unlink(struct sock *sk)\r\n{\r\nunsigned long flags;\r\nstruct iucv_sock *par = iucv_sk(iucv_sk(sk)->parent);\r\nspin_lock_irqsave(&par->accept_q_lock, flags);\r\nlist_del_init(&iucv_sk(sk)->accept_q);\r\nspin_unlock_irqrestore(&par->accept_q_lock, flags);\r\nsk_acceptq_removed(iucv_sk(sk)->parent);\r\niucv_sk(sk)->parent = NULL;\r\nsock_put(sk);\r\n}\r\nstruct sock *iucv_accept_dequeue(struct sock *parent, struct socket *newsock)\r\n{\r\nstruct iucv_sock *isk, *n;\r\nstruct sock *sk;\r\nlist_for_each_entry_safe(isk, n, &iucv_sk(parent)->accept_q, accept_q) {\r\nsk = (struct sock *) isk;\r\nlock_sock(sk);\r\nif (sk->sk_state == IUCV_CLOSED) {\r\niucv_accept_unlink(sk);\r\nrelease_sock(sk);\r\ncontinue;\r\n}\r\nif (sk->sk_state == IUCV_CONNECTED ||\r\nsk->sk_state == IUCV_DISCONN ||\r\n!newsock) {\r\niucv_accept_unlink(sk);\r\nif (newsock)\r\nsock_graft(sk, newsock);\r\nrelease_sock(sk);\r\nreturn sk;\r\n}\r\nrelease_sock(sk);\r\n}\r\nreturn NULL;\r\n}\r\nstatic int iucv_sock_bind(struct socket *sock, struct sockaddr *addr,\r\nint addr_len)\r\n{\r\nstruct sockaddr_iucv *sa = (struct sockaddr_iucv *) addr;\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv;\r\nint err = 0;\r\nstruct net_device *dev;\r\nchar uid[9];\r\nif (!addr || addr->sa_family != AF_IUCV)\r\nreturn -EINVAL;\r\nlock_sock(sk);\r\nif (sk->sk_state != IUCV_OPEN) {\r\nerr = -EBADFD;\r\ngoto done;\r\n}\r\nwrite_lock_bh(&iucv_sk_list.lock);\r\niucv = iucv_sk(sk);\r\nif (__iucv_get_sock_by_name(sa->siucv_name)) {\r\nerr = -EADDRINUSE;\r\ngoto done_unlock;\r\n}\r\nif (iucv->path)\r\ngoto done_unlock;\r\nif (pr_iucv)\r\nif (!memcmp(sa->siucv_user_id, iucv_userid, 8))\r\ngoto vm_bind;\r\nmemcpy(uid, sa->siucv_user_id, sizeof(uid));\r\nASCEBC(uid, 8);\r\nrcu_read_lock();\r\nfor_each_netdev_rcu(&init_net, dev) {\r\nif (!memcmp(dev->perm_addr, uid, 8)) {\r\nmemcpy(iucv->src_name, sa->siucv_name, 8);\r\nmemcpy(iucv->src_user_id, sa->siucv_user_id, 8);\r\nsk->sk_bound_dev_if = dev->ifindex;\r\niucv->hs_dev = dev;\r\ndev_hold(dev);\r\nsk->sk_state = IUCV_BOUND;\r\niucv->transport = AF_IUCV_TRANS_HIPER;\r\nif (!iucv->msglimit)\r\niucv->msglimit = IUCV_HIPER_MSGLIM_DEFAULT;\r\nrcu_read_unlock();\r\ngoto done_unlock;\r\n}\r\n}\r\nrcu_read_unlock();\r\nvm_bind:\r\nif (pr_iucv) {\r\nmemcpy(iucv->src_name, sa->siucv_name, 8);\r\nmemcpy(iucv->src_user_id, iucv_userid, 8);\r\nsk->sk_state = IUCV_BOUND;\r\niucv->transport = AF_IUCV_TRANS_IUCV;\r\nif (!iucv->msglimit)\r\niucv->msglimit = IUCV_QUEUELEN_DEFAULT;\r\ngoto done_unlock;\r\n}\r\nerr = -ENODEV;\r\ndone_unlock:\r\nwrite_unlock_bh(&iucv_sk_list.lock);\r\ndone:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_autobind(struct sock *sk)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nchar name[12];\r\nint err = 0;\r\nif (unlikely(!pr_iucv))\r\nreturn -EPROTO;\r\nmemcpy(iucv->src_user_id, iucv_userid, 8);\r\nwrite_lock_bh(&iucv_sk_list.lock);\r\nsprintf(name, "%08x", atomic_inc_return(&iucv_sk_list.autobind_name));\r\nwhile (__iucv_get_sock_by_name(name)) {\r\nsprintf(name, "%08x",\r\natomic_inc_return(&iucv_sk_list.autobind_name));\r\n}\r\nwrite_unlock_bh(&iucv_sk_list.lock);\r\nmemcpy(&iucv->src_name, name, 8);\r\nif (!iucv->msglimit)\r\niucv->msglimit = IUCV_QUEUELEN_DEFAULT;\r\nreturn err;\r\n}\r\nstatic int afiucv_path_connect(struct socket *sock, struct sockaddr *addr)\r\n{\r\nstruct sockaddr_iucv *sa = (struct sockaddr_iucv *) addr;\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nunsigned char user_data[16];\r\nint err;\r\nhigh_nmcpy(user_data, sa->siucv_name);\r\nlow_nmcpy(user_data, iucv->src_name);\r\nASCEBC(user_data, sizeof(user_data));\r\niucv->path = iucv_path_alloc(iucv->msglimit,\r\nIUCV_IPRMDATA, GFP_KERNEL);\r\nif (!iucv->path) {\r\nerr = -ENOMEM;\r\ngoto done;\r\n}\r\nerr = pr_iucv->path_connect(iucv->path, &af_iucv_handler,\r\nsa->siucv_user_id, NULL, user_data,\r\nsk);\r\nif (err) {\r\niucv_path_free(iucv->path);\r\niucv->path = NULL;\r\nswitch (err) {\r\ncase 0x0b:\r\nerr = -ENETUNREACH;\r\nbreak;\r\ncase 0x0d:\r\ncase 0x0e:\r\nerr = -EAGAIN;\r\nbreak;\r\ncase 0x0f:\r\nerr = -EACCES;\r\nbreak;\r\ndefault:\r\nerr = -ECONNREFUSED;\r\nbreak;\r\n}\r\n}\r\ndone:\r\nreturn err;\r\n}\r\nstatic int iucv_sock_connect(struct socket *sock, struct sockaddr *addr,\r\nint alen, int flags)\r\n{\r\nstruct sockaddr_iucv *sa = (struct sockaddr_iucv *) addr;\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nint err;\r\nif (addr->sa_family != AF_IUCV || alen < sizeof(struct sockaddr_iucv))\r\nreturn -EINVAL;\r\nif (sk->sk_state != IUCV_OPEN && sk->sk_state != IUCV_BOUND)\r\nreturn -EBADFD;\r\nif (sk->sk_state == IUCV_OPEN &&\r\niucv->transport == AF_IUCV_TRANS_HIPER)\r\nreturn -EBADFD;\r\nif (sk->sk_type != SOCK_STREAM && sk->sk_type != SOCK_SEQPACKET)\r\nreturn -EINVAL;\r\nif (sk->sk_state == IUCV_OPEN) {\r\nerr = iucv_sock_autobind(sk);\r\nif (unlikely(err))\r\nreturn err;\r\n}\r\nlock_sock(sk);\r\nmemcpy(iucv->dst_user_id, sa->siucv_user_id, 8);\r\nmemcpy(iucv->dst_name, sa->siucv_name, 8);\r\nif (iucv->transport == AF_IUCV_TRANS_HIPER)\r\nerr = iucv_send_ctrl(sock->sk, AF_IUCV_FLAG_SYN);\r\nelse\r\nerr = afiucv_path_connect(sock, addr);\r\nif (err)\r\ngoto done;\r\nif (sk->sk_state != IUCV_CONNECTED)\r\nerr = iucv_sock_wait(sk, iucv_sock_in_state(sk, IUCV_CONNECTED,\r\nIUCV_DISCONN),\r\nsock_sndtimeo(sk, flags & O_NONBLOCK));\r\nif (sk->sk_state == IUCV_DISCONN || sk->sk_state == IUCV_CLOSED)\r\nerr = -ECONNREFUSED;\r\nif (err && iucv->transport == AF_IUCV_TRANS_IUCV)\r\niucv_sever_path(sk, 0);\r\ndone:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_listen(struct socket *sock, int backlog)\r\n{\r\nstruct sock *sk = sock->sk;\r\nint err;\r\nlock_sock(sk);\r\nerr = -EINVAL;\r\nif (sk->sk_state != IUCV_BOUND)\r\ngoto done;\r\nif (sock->type != SOCK_STREAM && sock->type != SOCK_SEQPACKET)\r\ngoto done;\r\nsk->sk_max_ack_backlog = backlog;\r\nsk->sk_ack_backlog = 0;\r\nsk->sk_state = IUCV_LISTEN;\r\nerr = 0;\r\ndone:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_accept(struct socket *sock, struct socket *newsock,\r\nint flags)\r\n{\r\nDECLARE_WAITQUEUE(wait, current);\r\nstruct sock *sk = sock->sk, *nsk;\r\nlong timeo;\r\nint err = 0;\r\nlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\r\nif (sk->sk_state != IUCV_LISTEN) {\r\nerr = -EBADFD;\r\ngoto done;\r\n}\r\ntimeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\r\nadd_wait_queue_exclusive(sk_sleep(sk), &wait);\r\nwhile (!(nsk = iucv_accept_dequeue(sk, newsock))) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (!timeo) {\r\nerr = -EAGAIN;\r\nbreak;\r\n}\r\nrelease_sock(sk);\r\ntimeo = schedule_timeout(timeo);\r\nlock_sock_nested(sk, SINGLE_DEPTH_NESTING);\r\nif (sk->sk_state != IUCV_LISTEN) {\r\nerr = -EBADFD;\r\nbreak;\r\n}\r\nif (signal_pending(current)) {\r\nerr = sock_intr_errno(timeo);\r\nbreak;\r\n}\r\n}\r\nset_current_state(TASK_RUNNING);\r\nremove_wait_queue(sk_sleep(sk), &wait);\r\nif (err)\r\ngoto done;\r\nnewsock->state = SS_CONNECTED;\r\ndone:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_getname(struct socket *sock, struct sockaddr *addr,\r\nint *len, int peer)\r\n{\r\nstruct sockaddr_iucv *siucv = (struct sockaddr_iucv *) addr;\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\naddr->sa_family = AF_IUCV;\r\n*len = sizeof(struct sockaddr_iucv);\r\nif (peer) {\r\nmemcpy(siucv->siucv_user_id, iucv->dst_user_id, 8);\r\nmemcpy(siucv->siucv_name, iucv->dst_name, 8);\r\n} else {\r\nmemcpy(siucv->siucv_user_id, iucv->src_user_id, 8);\r\nmemcpy(siucv->siucv_name, iucv->src_name, 8);\r\n}\r\nmemset(&siucv->siucv_port, 0, sizeof(siucv->siucv_port));\r\nmemset(&siucv->siucv_addr, 0, sizeof(siucv->siucv_addr));\r\nmemset(&siucv->siucv_nodeid, 0, sizeof(siucv->siucv_nodeid));\r\nreturn 0;\r\n}\r\nstatic int iucv_send_iprm(struct iucv_path *path, struct iucv_message *msg,\r\nstruct sk_buff *skb)\r\n{\r\nu8 prmdata[8];\r\nmemcpy(prmdata, (void *) skb->data, skb->len);\r\nprmdata[7] = 0xff - (u8) skb->len;\r\nreturn pr_iucv->message_send(path, msg, IUCV_IPRMDATA, 0,\r\n(void *) prmdata, 8);\r\n}\r\nstatic int iucv_sock_sendmsg(struct kiocb *iocb, struct socket *sock,\r\nstruct msghdr *msg, size_t len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct sk_buff *skb;\r\nstruct iucv_message txmsg;\r\nstruct cmsghdr *cmsg;\r\nint cmsg_done;\r\nlong timeo;\r\nchar user_id[9];\r\nchar appl_id[9];\r\nint err;\r\nint noblock = msg->msg_flags & MSG_DONTWAIT;\r\nerr = sock_error(sk);\r\nif (err)\r\nreturn err;\r\nif (msg->msg_flags & MSG_OOB)\r\nreturn -EOPNOTSUPP;\r\nif (sk->sk_type == SOCK_SEQPACKET && !(msg->msg_flags & MSG_EOR))\r\nreturn -EOPNOTSUPP;\r\nlock_sock(sk);\r\nif (sk->sk_shutdown & SEND_SHUTDOWN) {\r\nerr = -EPIPE;\r\ngoto out;\r\n}\r\nif (sk->sk_state != IUCV_CONNECTED) {\r\nerr = -ENOTCONN;\r\ngoto out;\r\n}\r\ncmsg_done = 0;\r\ntxmsg.class = 0;\r\nfor (cmsg = CMSG_FIRSTHDR(msg); cmsg;\r\ncmsg = CMSG_NXTHDR(msg, cmsg)) {\r\nif (!CMSG_OK(msg, cmsg)) {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nif (cmsg->cmsg_level != SOL_IUCV)\r\ncontinue;\r\nif (cmsg->cmsg_type & cmsg_done) {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\ncmsg_done |= cmsg->cmsg_type;\r\nswitch (cmsg->cmsg_type) {\r\ncase SCM_IUCV_TRGCLS:\r\nif (cmsg->cmsg_len != CMSG_LEN(TRGCLS_SIZE)) {\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\nmemcpy(&txmsg.class,\r\n(void *) CMSG_DATA(cmsg), TRGCLS_SIZE);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\ngoto out;\r\nbreak;\r\n}\r\n}\r\nif (iucv->transport == AF_IUCV_TRANS_HIPER)\r\nskb = sock_alloc_send_skb(sk,\r\nlen + sizeof(struct af_iucv_trans_hdr) + ETH_HLEN,\r\nnoblock, &err);\r\nelse\r\nskb = sock_alloc_send_skb(sk, len, noblock, &err);\r\nif (!skb) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nif (iucv->transport == AF_IUCV_TRANS_HIPER)\r\nskb_reserve(skb, sizeof(struct af_iucv_trans_hdr) + ETH_HLEN);\r\nif (memcpy_fromiovec(skb_put(skb, len), msg->msg_iov, len)) {\r\nerr = -EFAULT;\r\ngoto fail;\r\n}\r\ntimeo = sock_sndtimeo(sk, noblock);\r\nerr = iucv_sock_wait(sk, iucv_below_msglim(sk), timeo);\r\nif (err)\r\ngoto fail;\r\nif (sk->sk_state != IUCV_CONNECTED) {\r\nerr = -ECONNRESET;\r\ngoto fail;\r\n}\r\ntxmsg.tag = iucv->send_tag++;\r\nmemcpy(CB_TAG(skb), &txmsg.tag, CB_TAG_LEN);\r\nif (iucv->transport == AF_IUCV_TRANS_HIPER) {\r\natomic_inc(&iucv->msg_sent);\r\nerr = afiucv_hs_send(&txmsg, sk, skb, 0);\r\nif (err) {\r\natomic_dec(&iucv->msg_sent);\r\ngoto fail;\r\n}\r\ngoto release;\r\n}\r\nskb_queue_tail(&iucv->send_skb_q, skb);\r\nif (((iucv->path->flags & IUCV_IPRMDATA) & iucv->flags)\r\n&& skb->len <= 7) {\r\nerr = iucv_send_iprm(iucv->path, &txmsg, skb);\r\nif (err == 0) {\r\nskb_unlink(skb, &iucv->send_skb_q);\r\nkfree_skb(skb);\r\n}\r\nif (err == 0x15) {\r\npr_iucv->path_sever(iucv->path, NULL);\r\nskb_unlink(skb, &iucv->send_skb_q);\r\nerr = -EPIPE;\r\ngoto fail;\r\n}\r\n} else\r\nerr = pr_iucv->message_send(iucv->path, &txmsg, 0, 0,\r\n(void *) skb->data, skb->len);\r\nif (err) {\r\nif (err == 3) {\r\nuser_id[8] = 0;\r\nmemcpy(user_id, iucv->dst_user_id, 8);\r\nappl_id[8] = 0;\r\nmemcpy(appl_id, iucv->dst_name, 8);\r\npr_err("Application %s on z/VM guest %s"\r\n" exceeds message limit\n",\r\nappl_id, user_id);\r\nerr = -EAGAIN;\r\n} else\r\nerr = -EPIPE;\r\nskb_unlink(skb, &iucv->send_skb_q);\r\ngoto fail;\r\n}\r\nrelease:\r\nrelease_sock(sk);\r\nreturn len;\r\nfail:\r\nkfree_skb(skb);\r\nout:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_fragment_skb(struct sock *sk, struct sk_buff *skb, int len)\r\n{\r\nint dataleft, size, copied = 0;\r\nstruct sk_buff *nskb;\r\ndataleft = len;\r\nwhile (dataleft) {\r\nif (dataleft >= sk->sk_rcvbuf / 4)\r\nsize = sk->sk_rcvbuf / 4;\r\nelse\r\nsize = dataleft;\r\nnskb = alloc_skb(size, GFP_ATOMIC | GFP_DMA);\r\nif (!nskb)\r\nreturn -ENOMEM;\r\nmemcpy(CB_TRGCLS(nskb), CB_TRGCLS(skb), CB_TRGCLS_LEN);\r\nmemcpy(nskb->data, skb->data + copied, size);\r\ncopied += size;\r\ndataleft -= size;\r\nskb_reset_transport_header(nskb);\r\nskb_reset_network_header(nskb);\r\nnskb->len = size;\r\nskb_queue_tail(&iucv_sk(sk)->backlog_skb_q, nskb);\r\n}\r\nreturn 0;\r\n}\r\nstatic void iucv_process_message(struct sock *sk, struct sk_buff *skb,\r\nstruct iucv_path *path,\r\nstruct iucv_message *msg)\r\n{\r\nint rc;\r\nunsigned int len;\r\nlen = iucv_msg_length(msg);\r\nmemcpy(CB_TRGCLS(skb), &msg->class, CB_TRGCLS_LEN);\r\nif ((msg->flags & IUCV_IPRMDATA) && len > 7) {\r\nif (memcmp(msg->rmmsg, iprm_shutdown, 8) == 0) {\r\nskb->data = NULL;\r\nskb->len = 0;\r\n}\r\n} else {\r\nrc = pr_iucv->message_receive(path, msg,\r\nmsg->flags & IUCV_IPRMDATA,\r\nskb->data, len, NULL);\r\nif (rc) {\r\nkfree_skb(skb);\r\nreturn;\r\n}\r\nif (sk->sk_type == SOCK_STREAM &&\r\nskb->truesize >= sk->sk_rcvbuf / 4) {\r\nrc = iucv_fragment_skb(sk, skb, len);\r\nkfree_skb(skb);\r\nskb = NULL;\r\nif (rc) {\r\npr_iucv->path_sever(path, NULL);\r\nreturn;\r\n}\r\nskb = skb_dequeue(&iucv_sk(sk)->backlog_skb_q);\r\n} else {\r\nskb_reset_transport_header(skb);\r\nskb_reset_network_header(skb);\r\nskb->len = len;\r\n}\r\n}\r\nif (sock_queue_rcv_skb(sk, skb))\r\nskb_queue_head(&iucv_sk(sk)->backlog_skb_q, skb);\r\n}\r\nstatic void iucv_process_message_q(struct sock *sk)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct sk_buff *skb;\r\nstruct sock_msg_q *p, *n;\r\nlist_for_each_entry_safe(p, n, &iucv->message_q.list, list) {\r\nskb = alloc_skb(iucv_msg_length(&p->msg), GFP_ATOMIC | GFP_DMA);\r\nif (!skb)\r\nbreak;\r\niucv_process_message(sk, skb, p->path, &p->msg);\r\nlist_del(&p->list);\r\nkfree(p);\r\nif (!skb_queue_empty(&iucv->backlog_skb_q))\r\nbreak;\r\n}\r\n}\r\nstatic int iucv_sock_recvmsg(struct kiocb *iocb, struct socket *sock,\r\nstruct msghdr *msg, size_t len, int flags)\r\n{\r\nint noblock = flags & MSG_DONTWAIT;\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nunsigned int copied, rlen;\r\nstruct sk_buff *skb, *rskb, *cskb;\r\nint err = 0;\r\nif ((sk->sk_state == IUCV_DISCONN) &&\r\nskb_queue_empty(&iucv->backlog_skb_q) &&\r\nskb_queue_empty(&sk->sk_receive_queue) &&\r\nlist_empty(&iucv->message_q.list))\r\nreturn 0;\r\nif (flags & (MSG_OOB))\r\nreturn -EOPNOTSUPP;\r\nskb = skb_recv_datagram(sk, flags, noblock, &err);\r\nif (!skb) {\r\nif (sk->sk_shutdown & RCV_SHUTDOWN)\r\nreturn 0;\r\nreturn err;\r\n}\r\nrlen = skb->len;\r\ncopied = min_t(unsigned int, rlen, len);\r\nif (!rlen)\r\nsk->sk_shutdown = sk->sk_shutdown | RCV_SHUTDOWN;\r\ncskb = skb;\r\nif (skb_copy_datagram_iovec(cskb, 0, msg->msg_iov, copied)) {\r\nif (!(flags & MSG_PEEK))\r\nskb_queue_head(&sk->sk_receive_queue, skb);\r\nreturn -EFAULT;\r\n}\r\nif (sk->sk_type == SOCK_SEQPACKET) {\r\nif (copied < rlen)\r\nmsg->msg_flags |= MSG_TRUNC;\r\nmsg->msg_flags |= MSG_EOR;\r\n}\r\nerr = put_cmsg(msg, SOL_IUCV, SCM_IUCV_TRGCLS,\r\nCB_TRGCLS_LEN, CB_TRGCLS(skb));\r\nif (err) {\r\nif (!(flags & MSG_PEEK))\r\nskb_queue_head(&sk->sk_receive_queue, skb);\r\nreturn err;\r\n}\r\nif (!(flags & MSG_PEEK)) {\r\nif (sk->sk_type == SOCK_STREAM) {\r\nskb_pull(skb, copied);\r\nif (skb->len) {\r\nskb_queue_head(&sk->sk_receive_queue, skb);\r\ngoto done;\r\n}\r\n}\r\nkfree_skb(skb);\r\nif (iucv->transport == AF_IUCV_TRANS_HIPER) {\r\natomic_inc(&iucv->msg_recv);\r\nif (atomic_read(&iucv->msg_recv) > iucv->msglimit) {\r\nWARN_ON(1);\r\niucv_sock_close(sk);\r\nreturn -EFAULT;\r\n}\r\n}\r\nspin_lock_bh(&iucv->message_q.lock);\r\nrskb = skb_dequeue(&iucv->backlog_skb_q);\r\nwhile (rskb) {\r\nif (sock_queue_rcv_skb(sk, rskb)) {\r\nskb_queue_head(&iucv->backlog_skb_q,\r\nrskb);\r\nbreak;\r\n} else {\r\nrskb = skb_dequeue(&iucv->backlog_skb_q);\r\n}\r\n}\r\nif (skb_queue_empty(&iucv->backlog_skb_q)) {\r\nif (!list_empty(&iucv->message_q.list))\r\niucv_process_message_q(sk);\r\nif (atomic_read(&iucv->msg_recv) >=\r\niucv->msglimit / 2) {\r\nerr = iucv_send_ctrl(sk, AF_IUCV_FLAG_WIN);\r\nif (err) {\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\n}\r\n}\r\n}\r\nspin_unlock_bh(&iucv->message_q.lock);\r\n}\r\ndone:\r\nif (sk->sk_type == SOCK_SEQPACKET && (flags & MSG_TRUNC))\r\ncopied = rlen;\r\nreturn copied;\r\n}\r\nstatic inline unsigned int iucv_accept_poll(struct sock *parent)\r\n{\r\nstruct iucv_sock *isk, *n;\r\nstruct sock *sk;\r\nlist_for_each_entry_safe(isk, n, &iucv_sk(parent)->accept_q, accept_q) {\r\nsk = (struct sock *) isk;\r\nif (sk->sk_state == IUCV_CONNECTED)\r\nreturn POLLIN | POLLRDNORM;\r\n}\r\nreturn 0;\r\n}\r\nunsigned int iucv_sock_poll(struct file *file, struct socket *sock,\r\npoll_table *wait)\r\n{\r\nstruct sock *sk = sock->sk;\r\nunsigned int mask = 0;\r\nsock_poll_wait(file, sk_sleep(sk), wait);\r\nif (sk->sk_state == IUCV_LISTEN)\r\nreturn iucv_accept_poll(sk);\r\nif (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue))\r\nmask |= POLLERR;\r\nif (sk->sk_shutdown & RCV_SHUTDOWN)\r\nmask |= POLLRDHUP;\r\nif (sk->sk_shutdown == SHUTDOWN_MASK)\r\nmask |= POLLHUP;\r\nif (!skb_queue_empty(&sk->sk_receive_queue) ||\r\n(sk->sk_shutdown & RCV_SHUTDOWN))\r\nmask |= POLLIN | POLLRDNORM;\r\nif (sk->sk_state == IUCV_CLOSED)\r\nmask |= POLLHUP;\r\nif (sk->sk_state == IUCV_DISCONN)\r\nmask |= POLLIN;\r\nif (sock_writeable(sk) && iucv_below_msglim(sk))\r\nmask |= POLLOUT | POLLWRNORM | POLLWRBAND;\r\nelse\r\nset_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\r\nreturn mask;\r\n}\r\nstatic int iucv_sock_shutdown(struct socket *sock, int how)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct iucv_message txmsg;\r\nint err = 0;\r\nhow++;\r\nif ((how & ~SHUTDOWN_MASK) || !how)\r\nreturn -EINVAL;\r\nlock_sock(sk);\r\nswitch (sk->sk_state) {\r\ncase IUCV_LISTEN:\r\ncase IUCV_DISCONN:\r\ncase IUCV_CLOSING:\r\ncase IUCV_CLOSED:\r\nerr = -ENOTCONN;\r\ngoto fail;\r\ndefault:\r\nbreak;\r\n}\r\nif (how == SEND_SHUTDOWN || how == SHUTDOWN_MASK) {\r\nif (iucv->transport == AF_IUCV_TRANS_IUCV) {\r\ntxmsg.class = 0;\r\ntxmsg.tag = 0;\r\nerr = pr_iucv->message_send(iucv->path, &txmsg,\r\nIUCV_IPRMDATA, 0, (void *) iprm_shutdown, 8);\r\nif (err) {\r\nswitch (err) {\r\ncase 1:\r\nerr = -ENOTCONN;\r\nbreak;\r\ncase 2:\r\nerr = -ECONNRESET;\r\nbreak;\r\ndefault:\r\nerr = -ENOTCONN;\r\nbreak;\r\n}\r\n}\r\n} else\r\niucv_send_ctrl(sk, AF_IUCV_FLAG_SHT);\r\n}\r\nsk->sk_shutdown |= how;\r\nif (how == RCV_SHUTDOWN || how == SHUTDOWN_MASK) {\r\nif (iucv->transport == AF_IUCV_TRANS_IUCV) {\r\nerr = pr_iucv->path_quiesce(iucv->path, NULL);\r\nif (err)\r\nerr = -ENOTCONN;\r\n}\r\nskb_queue_purge(&sk->sk_receive_queue);\r\n}\r\nsk->sk_state_change(sk);\r\nfail:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_release(struct socket *sock)\r\n{\r\nstruct sock *sk = sock->sk;\r\nint err = 0;\r\nif (!sk)\r\nreturn 0;\r\niucv_sock_close(sk);\r\nsock_orphan(sk);\r\niucv_sock_kill(sk);\r\nreturn err;\r\n}\r\nstatic int iucv_sock_setsockopt(struct socket *sock, int level, int optname,\r\nchar __user *optval, unsigned int optlen)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nint val;\r\nint rc;\r\nif (level != SOL_IUCV)\r\nreturn -ENOPROTOOPT;\r\nif (optlen < sizeof(int))\r\nreturn -EINVAL;\r\nif (get_user(val, (int __user *) optval))\r\nreturn -EFAULT;\r\nrc = 0;\r\nlock_sock(sk);\r\nswitch (optname) {\r\ncase SO_IPRMDATA_MSG:\r\nif (val)\r\niucv->flags |= IUCV_IPRMDATA;\r\nelse\r\niucv->flags &= ~IUCV_IPRMDATA;\r\nbreak;\r\ncase SO_MSGLIMIT:\r\nswitch (sk->sk_state) {\r\ncase IUCV_OPEN:\r\ncase IUCV_BOUND:\r\nif (val < 1 || val > (u16)(~0))\r\nrc = -EINVAL;\r\nelse\r\niucv->msglimit = val;\r\nbreak;\r\ndefault:\r\nrc = -EINVAL;\r\nbreak;\r\n}\r\nbreak;\r\ndefault:\r\nrc = -ENOPROTOOPT;\r\nbreak;\r\n}\r\nrelease_sock(sk);\r\nreturn rc;\r\n}\r\nstatic int iucv_sock_getsockopt(struct socket *sock, int level, int optname,\r\nchar __user *optval, int __user *optlen)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nunsigned int val;\r\nint len;\r\nif (level != SOL_IUCV)\r\nreturn -ENOPROTOOPT;\r\nif (get_user(len, optlen))\r\nreturn -EFAULT;\r\nif (len < 0)\r\nreturn -EINVAL;\r\nlen = min_t(unsigned int, len, sizeof(int));\r\nswitch (optname) {\r\ncase SO_IPRMDATA_MSG:\r\nval = (iucv->flags & IUCV_IPRMDATA) ? 1 : 0;\r\nbreak;\r\ncase SO_MSGLIMIT:\r\nlock_sock(sk);\r\nval = (iucv->path != NULL) ? iucv->path->msglim\r\n: iucv->msglimit;\r\nrelease_sock(sk);\r\nbreak;\r\ncase SO_MSGSIZE:\r\nif (sk->sk_state == IUCV_OPEN)\r\nreturn -EBADFD;\r\nval = (iucv->hs_dev) ? iucv->hs_dev->mtu -\r\nsizeof(struct af_iucv_trans_hdr) - ETH_HLEN :\r\n0x7fffffff;\r\nbreak;\r\ndefault:\r\nreturn -ENOPROTOOPT;\r\n}\r\nif (put_user(len, optlen))\r\nreturn -EFAULT;\r\nif (copy_to_user(optval, &val, len))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int iucv_callback_connreq(struct iucv_path *path,\r\nu8 ipvmid[8], u8 ipuser[16])\r\n{\r\nunsigned char user_data[16];\r\nunsigned char nuser_data[16];\r\nunsigned char src_name[8];\r\nstruct hlist_node *node;\r\nstruct sock *sk, *nsk;\r\nstruct iucv_sock *iucv, *niucv;\r\nint err;\r\nmemcpy(src_name, ipuser, 8);\r\nEBCASC(src_name, 8);\r\nread_lock(&iucv_sk_list.lock);\r\niucv = NULL;\r\nsk = NULL;\r\nsk_for_each(sk, node, &iucv_sk_list.head)\r\nif (sk->sk_state == IUCV_LISTEN &&\r\n!memcmp(&iucv_sk(sk)->src_name, src_name, 8)) {\r\niucv = iucv_sk(sk);\r\nbreak;\r\n}\r\nread_unlock(&iucv_sk_list.lock);\r\nif (!iucv)\r\nreturn -EINVAL;\r\nbh_lock_sock(sk);\r\nlow_nmcpy(user_data, iucv->src_name);\r\nhigh_nmcpy(user_data, iucv->dst_name);\r\nASCEBC(user_data, sizeof(user_data));\r\nif (sk->sk_state != IUCV_LISTEN) {\r\nerr = pr_iucv->path_sever(path, user_data);\r\niucv_path_free(path);\r\ngoto fail;\r\n}\r\nif (sk_acceptq_is_full(sk)) {\r\nerr = pr_iucv->path_sever(path, user_data);\r\niucv_path_free(path);\r\ngoto fail;\r\n}\r\nnsk = iucv_sock_alloc(NULL, sk->sk_type, GFP_ATOMIC);\r\nif (!nsk) {\r\nerr = pr_iucv->path_sever(path, user_data);\r\niucv_path_free(path);\r\ngoto fail;\r\n}\r\nniucv = iucv_sk(nsk);\r\niucv_sock_init(nsk, sk);\r\nmemcpy(niucv->dst_name, ipuser + 8, 8);\r\nEBCASC(niucv->dst_name, 8);\r\nmemcpy(niucv->dst_user_id, ipvmid, 8);\r\nmemcpy(niucv->src_name, iucv->src_name, 8);\r\nmemcpy(niucv->src_user_id, iucv->src_user_id, 8);\r\nniucv->path = path;\r\nhigh_nmcpy(nuser_data, ipuser + 8);\r\nmemcpy(nuser_data + 8, niucv->src_name, 8);\r\nASCEBC(nuser_data + 8, 8);\r\nniucv->msglimit = iucv->msglimit;\r\npath->msglim = iucv->msglimit;\r\nerr = pr_iucv->path_accept(path, &af_iucv_handler, nuser_data, nsk);\r\nif (err) {\r\niucv_sever_path(nsk, 1);\r\niucv_sock_kill(nsk);\r\ngoto fail;\r\n}\r\niucv_accept_enqueue(sk, nsk);\r\nnsk->sk_state = IUCV_CONNECTED;\r\nsk->sk_data_ready(sk, 1);\r\nerr = 0;\r\nfail:\r\nbh_unlock_sock(sk);\r\nreturn 0;\r\n}\r\nstatic void iucv_callback_connack(struct iucv_path *path, u8 ipuser[16])\r\n{\r\nstruct sock *sk = path->private;\r\nsk->sk_state = IUCV_CONNECTED;\r\nsk->sk_state_change(sk);\r\n}\r\nstatic void iucv_callback_rx(struct iucv_path *path, struct iucv_message *msg)\r\n{\r\nstruct sock *sk = path->private;\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct sk_buff *skb;\r\nstruct sock_msg_q *save_msg;\r\nint len;\r\nif (sk->sk_shutdown & RCV_SHUTDOWN) {\r\npr_iucv->message_reject(path, msg);\r\nreturn;\r\n}\r\nspin_lock(&iucv->message_q.lock);\r\nif (!list_empty(&iucv->message_q.list) ||\r\n!skb_queue_empty(&iucv->backlog_skb_q))\r\ngoto save_message;\r\nlen = atomic_read(&sk->sk_rmem_alloc);\r\nlen += SKB_TRUESIZE(iucv_msg_length(msg));\r\nif (len > sk->sk_rcvbuf)\r\ngoto save_message;\r\nskb = alloc_skb(iucv_msg_length(msg), GFP_ATOMIC | GFP_DMA);\r\nif (!skb)\r\ngoto save_message;\r\niucv_process_message(sk, skb, path, msg);\r\ngoto out_unlock;\r\nsave_message:\r\nsave_msg = kzalloc(sizeof(struct sock_msg_q), GFP_ATOMIC | GFP_DMA);\r\nif (!save_msg)\r\ngoto out_unlock;\r\nsave_msg->path = path;\r\nsave_msg->msg = *msg;\r\nlist_add_tail(&save_msg->list, &iucv->message_q.list);\r\nout_unlock:\r\nspin_unlock(&iucv->message_q.lock);\r\n}\r\nstatic void iucv_callback_txdone(struct iucv_path *path,\r\nstruct iucv_message *msg)\r\n{\r\nstruct sock *sk = path->private;\r\nstruct sk_buff *this = NULL;\r\nstruct sk_buff_head *list = &iucv_sk(sk)->send_skb_q;\r\nstruct sk_buff *list_skb = list->next;\r\nunsigned long flags;\r\nbh_lock_sock(sk);\r\nif (!skb_queue_empty(list)) {\r\nspin_lock_irqsave(&list->lock, flags);\r\nwhile (list_skb != (struct sk_buff *)list) {\r\nif (!memcmp(&msg->tag, CB_TAG(list_skb), CB_TAG_LEN)) {\r\nthis = list_skb;\r\nbreak;\r\n}\r\nlist_skb = list_skb->next;\r\n}\r\nif (this)\r\n__skb_unlink(this, list);\r\nspin_unlock_irqrestore(&list->lock, flags);\r\nif (this) {\r\nkfree_skb(this);\r\niucv_sock_wake_msglim(sk);\r\n}\r\n}\r\nif (sk->sk_state == IUCV_CLOSING) {\r\nif (skb_queue_empty(&iucv_sk(sk)->send_skb_q)) {\r\nsk->sk_state = IUCV_CLOSED;\r\nsk->sk_state_change(sk);\r\n}\r\n}\r\nbh_unlock_sock(sk);\r\n}\r\nstatic void iucv_callback_connrej(struct iucv_path *path, u8 ipuser[16])\r\n{\r\nstruct sock *sk = path->private;\r\nif (sk->sk_state == IUCV_CLOSED)\r\nreturn;\r\nbh_lock_sock(sk);\r\niucv_sever_path(sk, 1);\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\nbh_unlock_sock(sk);\r\n}\r\nstatic void iucv_callback_shutdown(struct iucv_path *path, u8 ipuser[16])\r\n{\r\nstruct sock *sk = path->private;\r\nbh_lock_sock(sk);\r\nif (sk->sk_state != IUCV_CLOSED) {\r\nsk->sk_shutdown |= SEND_SHUTDOWN;\r\nsk->sk_state_change(sk);\r\n}\r\nbh_unlock_sock(sk);\r\n}\r\nstatic void afiucv_swap_src_dest(struct sk_buff *skb)\r\n{\r\nstruct af_iucv_trans_hdr *trans_hdr =\r\n(struct af_iucv_trans_hdr *)skb->data;\r\nchar tmpID[8];\r\nchar tmpName[8];\r\nASCEBC(trans_hdr->destUserID, sizeof(trans_hdr->destUserID));\r\nASCEBC(trans_hdr->destAppName, sizeof(trans_hdr->destAppName));\r\nASCEBC(trans_hdr->srcUserID, sizeof(trans_hdr->srcUserID));\r\nASCEBC(trans_hdr->srcAppName, sizeof(trans_hdr->srcAppName));\r\nmemcpy(tmpID, trans_hdr->srcUserID, 8);\r\nmemcpy(tmpName, trans_hdr->srcAppName, 8);\r\nmemcpy(trans_hdr->srcUserID, trans_hdr->destUserID, 8);\r\nmemcpy(trans_hdr->srcAppName, trans_hdr->destAppName, 8);\r\nmemcpy(trans_hdr->destUserID, tmpID, 8);\r\nmemcpy(trans_hdr->destAppName, tmpName, 8);\r\nskb_push(skb, ETH_HLEN);\r\nmemset(skb->data, 0, ETH_HLEN);\r\n}\r\nstatic int afiucv_hs_callback_syn(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct sock *nsk;\r\nstruct iucv_sock *iucv, *niucv;\r\nstruct af_iucv_trans_hdr *trans_hdr;\r\nint err;\r\niucv = iucv_sk(sk);\r\ntrans_hdr = (struct af_iucv_trans_hdr *)skb->data;\r\nif (!iucv) {\r\nafiucv_swap_src_dest(skb);\r\ntrans_hdr->flags = AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_FIN;\r\nerr = dev_queue_xmit(skb);\r\ngoto out;\r\n}\r\nnsk = iucv_sock_alloc(NULL, sk->sk_type, GFP_ATOMIC);\r\nbh_lock_sock(sk);\r\nif ((sk->sk_state != IUCV_LISTEN) ||\r\nsk_acceptq_is_full(sk) ||\r\n!nsk) {\r\nif (nsk)\r\nsk_free(nsk);\r\nafiucv_swap_src_dest(skb);\r\ntrans_hdr->flags = AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_FIN;\r\nerr = dev_queue_xmit(skb);\r\nbh_unlock_sock(sk);\r\ngoto out;\r\n}\r\nniucv = iucv_sk(nsk);\r\niucv_sock_init(nsk, sk);\r\nniucv->transport = AF_IUCV_TRANS_HIPER;\r\nniucv->msglimit = iucv->msglimit;\r\nif (!trans_hdr->window)\r\nniucv->msglimit_peer = IUCV_HIPER_MSGLIM_DEFAULT;\r\nelse\r\nniucv->msglimit_peer = trans_hdr->window;\r\nmemcpy(niucv->dst_name, trans_hdr->srcAppName, 8);\r\nmemcpy(niucv->dst_user_id, trans_hdr->srcUserID, 8);\r\nmemcpy(niucv->src_name, iucv->src_name, 8);\r\nmemcpy(niucv->src_user_id, iucv->src_user_id, 8);\r\nnsk->sk_bound_dev_if = sk->sk_bound_dev_if;\r\nniucv->hs_dev = iucv->hs_dev;\r\ndev_hold(niucv->hs_dev);\r\nafiucv_swap_src_dest(skb);\r\ntrans_hdr->flags = AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_ACK;\r\ntrans_hdr->window = niucv->msglimit;\r\nerr = dev_queue_xmit(skb);\r\nif (!err) {\r\niucv_accept_enqueue(sk, nsk);\r\nnsk->sk_state = IUCV_CONNECTED;\r\nsk->sk_data_ready(sk, 1);\r\n} else\r\niucv_sock_kill(nsk);\r\nbh_unlock_sock(sk);\r\nout:\r\nreturn NET_RX_SUCCESS;\r\n}\r\nstatic int afiucv_hs_callback_synack(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct af_iucv_trans_hdr *trans_hdr =\r\n(struct af_iucv_trans_hdr *)skb->data;\r\nif (!iucv)\r\ngoto out;\r\nif (sk->sk_state != IUCV_BOUND)\r\ngoto out;\r\nbh_lock_sock(sk);\r\niucv->msglimit_peer = trans_hdr->window;\r\nsk->sk_state = IUCV_CONNECTED;\r\nsk->sk_state_change(sk);\r\nbh_unlock_sock(sk);\r\nout:\r\nkfree_skb(skb);\r\nreturn NET_RX_SUCCESS;\r\n}\r\nstatic int afiucv_hs_callback_synfin(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nif (!iucv)\r\ngoto out;\r\nif (sk->sk_state != IUCV_BOUND)\r\ngoto out;\r\nbh_lock_sock(sk);\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\nbh_unlock_sock(sk);\r\nout:\r\nkfree_skb(skb);\r\nreturn NET_RX_SUCCESS;\r\n}\r\nstatic int afiucv_hs_callback_fin(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nif (!iucv)\r\ngoto out;\r\nbh_lock_sock(sk);\r\nif (sk->sk_state == IUCV_CONNECTED) {\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\n}\r\nbh_unlock_sock(sk);\r\nout:\r\nkfree_skb(skb);\r\nreturn NET_RX_SUCCESS;\r\n}\r\nstatic int afiucv_hs_callback_win(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nstruct af_iucv_trans_hdr *trans_hdr =\r\n(struct af_iucv_trans_hdr *)skb->data;\r\nif (!iucv)\r\nreturn NET_RX_SUCCESS;\r\nif (sk->sk_state != IUCV_CONNECTED)\r\nreturn NET_RX_SUCCESS;\r\natomic_sub(trans_hdr->window, &iucv->msg_sent);\r\niucv_sock_wake_msglim(sk);\r\nreturn NET_RX_SUCCESS;\r\n}\r\nstatic int afiucv_hs_callback_rx(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct iucv_sock *iucv = iucv_sk(sk);\r\nif (!iucv) {\r\nkfree_skb(skb);\r\nreturn NET_RX_SUCCESS;\r\n}\r\nif (sk->sk_state != IUCV_CONNECTED) {\r\nkfree_skb(skb);\r\nreturn NET_RX_SUCCESS;\r\n}\r\nif (sk->sk_shutdown & RCV_SHUTDOWN) {\r\nkfree_skb(skb);\r\nreturn NET_RX_SUCCESS;\r\n}\r\nif (skb->len < sizeof(struct af_iucv_trans_hdr)) {\r\nkfree_skb(skb);\r\nreturn NET_RX_SUCCESS;\r\n}\r\nskb_pull(skb, sizeof(struct af_iucv_trans_hdr));\r\nskb_reset_transport_header(skb);\r\nskb_reset_network_header(skb);\r\nspin_lock(&iucv->message_q.lock);\r\nif (skb_queue_empty(&iucv->backlog_skb_q)) {\r\nif (sock_queue_rcv_skb(sk, skb)) {\r\nskb_queue_tail(&iucv->backlog_skb_q, skb);\r\n}\r\n} else\r\nskb_queue_tail(&iucv_sk(sk)->backlog_skb_q, skb);\r\nspin_unlock(&iucv->message_q.lock);\r\nreturn NET_RX_SUCCESS;\r\n}\r\nstatic int afiucv_hs_rcv(struct sk_buff *skb, struct net_device *dev,\r\nstruct packet_type *pt, struct net_device *orig_dev)\r\n{\r\nstruct hlist_node *node;\r\nstruct sock *sk;\r\nstruct iucv_sock *iucv;\r\nstruct af_iucv_trans_hdr *trans_hdr;\r\nchar nullstring[8];\r\nint err = 0;\r\nskb_pull(skb, ETH_HLEN);\r\ntrans_hdr = (struct af_iucv_trans_hdr *)skb->data;\r\nEBCASC(trans_hdr->destAppName, sizeof(trans_hdr->destAppName));\r\nEBCASC(trans_hdr->destUserID, sizeof(trans_hdr->destUserID));\r\nEBCASC(trans_hdr->srcAppName, sizeof(trans_hdr->srcAppName));\r\nEBCASC(trans_hdr->srcUserID, sizeof(trans_hdr->srcUserID));\r\nmemset(nullstring, 0, sizeof(nullstring));\r\niucv = NULL;\r\nsk = NULL;\r\nread_lock(&iucv_sk_list.lock);\r\nsk_for_each(sk, node, &iucv_sk_list.head) {\r\nif (trans_hdr->flags == AF_IUCV_FLAG_SYN) {\r\nif ((!memcmp(&iucv_sk(sk)->src_name,\r\ntrans_hdr->destAppName, 8)) &&\r\n(!memcmp(&iucv_sk(sk)->src_user_id,\r\ntrans_hdr->destUserID, 8)) &&\r\n(!memcmp(&iucv_sk(sk)->dst_name, nullstring, 8)) &&\r\n(!memcmp(&iucv_sk(sk)->dst_user_id,\r\nnullstring, 8))) {\r\niucv = iucv_sk(sk);\r\nbreak;\r\n}\r\n} else {\r\nif ((!memcmp(&iucv_sk(sk)->src_name,\r\ntrans_hdr->destAppName, 8)) &&\r\n(!memcmp(&iucv_sk(sk)->src_user_id,\r\ntrans_hdr->destUserID, 8)) &&\r\n(!memcmp(&iucv_sk(sk)->dst_name,\r\ntrans_hdr->srcAppName, 8)) &&\r\n(!memcmp(&iucv_sk(sk)->dst_user_id,\r\ntrans_hdr->srcUserID, 8))) {\r\niucv = iucv_sk(sk);\r\nbreak;\r\n}\r\n}\r\n}\r\nread_unlock(&iucv_sk_list.lock);\r\nif (!iucv)\r\nsk = NULL;\r\nswitch (trans_hdr->flags) {\r\ncase AF_IUCV_FLAG_SYN:\r\nerr = afiucv_hs_callback_syn(sk, skb);\r\nbreak;\r\ncase (AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_ACK):\r\nerr = afiucv_hs_callback_synack(sk, skb);\r\nbreak;\r\ncase (AF_IUCV_FLAG_SYN | AF_IUCV_FLAG_FIN):\r\nerr = afiucv_hs_callback_synfin(sk, skb);\r\nbreak;\r\ncase (AF_IUCV_FLAG_FIN):\r\nerr = afiucv_hs_callback_fin(sk, skb);\r\nbreak;\r\ncase (AF_IUCV_FLAG_WIN):\r\nerr = afiucv_hs_callback_win(sk, skb);\r\nif (skb->len == sizeof(struct af_iucv_trans_hdr)) {\r\nkfree_skb(skb);\r\nbreak;\r\n}\r\ncase (AF_IUCV_FLAG_SHT):\r\ncase 0:\r\nmemcpy(CB_TRGCLS(skb), &trans_hdr->iucv_hdr.class,\r\nCB_TRGCLS_LEN);\r\nerr = afiucv_hs_callback_rx(sk, skb);\r\nbreak;\r\ndefault:\r\n;\r\n}\r\nreturn err;\r\n}\r\nstatic void afiucv_hs_callback_txnotify(struct sk_buff *skb,\r\nenum iucv_tx_notify n)\r\n{\r\nstruct sock *isk = skb->sk;\r\nstruct sock *sk = NULL;\r\nstruct iucv_sock *iucv = NULL;\r\nstruct sk_buff_head *list;\r\nstruct sk_buff *list_skb;\r\nstruct sk_buff *nskb;\r\nunsigned long flags;\r\nstruct hlist_node *node;\r\nread_lock_irqsave(&iucv_sk_list.lock, flags);\r\nsk_for_each(sk, node, &iucv_sk_list.head)\r\nif (sk == isk) {\r\niucv = iucv_sk(sk);\r\nbreak;\r\n}\r\nread_unlock_irqrestore(&iucv_sk_list.lock, flags);\r\nif (!iucv || sock_flag(sk, SOCK_ZAPPED))\r\nreturn;\r\nlist = &iucv->send_skb_q;\r\nspin_lock_irqsave(&list->lock, flags);\r\nif (skb_queue_empty(list))\r\ngoto out_unlock;\r\nlist_skb = list->next;\r\nnskb = list_skb->next;\r\nwhile (list_skb != (struct sk_buff *)list) {\r\nif (skb_shinfo(list_skb) == skb_shinfo(skb)) {\r\nswitch (n) {\r\ncase TX_NOTIFY_OK:\r\n__skb_unlink(list_skb, list);\r\nkfree_skb(list_skb);\r\niucv_sock_wake_msglim(sk);\r\nbreak;\r\ncase TX_NOTIFY_PENDING:\r\natomic_inc(&iucv->pendings);\r\nbreak;\r\ncase TX_NOTIFY_DELAYED_OK:\r\n__skb_unlink(list_skb, list);\r\natomic_dec(&iucv->pendings);\r\nif (atomic_read(&iucv->pendings) <= 0)\r\niucv_sock_wake_msglim(sk);\r\nkfree_skb(list_skb);\r\nbreak;\r\ncase TX_NOTIFY_UNREACHABLE:\r\ncase TX_NOTIFY_DELAYED_UNREACHABLE:\r\ncase TX_NOTIFY_TPQFULL:\r\ncase TX_NOTIFY_GENERALERROR:\r\ncase TX_NOTIFY_DELAYED_GENERALERROR:\r\n__skb_unlink(list_skb, list);\r\nkfree_skb(list_skb);\r\nif (sk->sk_state == IUCV_CONNECTED) {\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\n}\r\nbreak;\r\n}\r\nbreak;\r\n}\r\nlist_skb = nskb;\r\nnskb = nskb->next;\r\n}\r\nout_unlock:\r\nspin_unlock_irqrestore(&list->lock, flags);\r\nif (sk->sk_state == IUCV_CLOSING) {\r\nif (skb_queue_empty(&iucv_sk(sk)->send_skb_q)) {\r\nsk->sk_state = IUCV_CLOSED;\r\nsk->sk_state_change(sk);\r\n}\r\n}\r\n}\r\nstatic int afiucv_netdev_event(struct notifier_block *this,\r\nunsigned long event, void *ptr)\r\n{\r\nstruct net_device *event_dev = (struct net_device *)ptr;\r\nstruct hlist_node *node;\r\nstruct sock *sk;\r\nstruct iucv_sock *iucv;\r\nswitch (event) {\r\ncase NETDEV_REBOOT:\r\ncase NETDEV_GOING_DOWN:\r\nsk_for_each(sk, node, &iucv_sk_list.head) {\r\niucv = iucv_sk(sk);\r\nif ((iucv->hs_dev == event_dev) &&\r\n(sk->sk_state == IUCV_CONNECTED)) {\r\nif (event == NETDEV_GOING_DOWN)\r\niucv_send_ctrl(sk, AF_IUCV_FLAG_FIN);\r\nsk->sk_state = IUCV_DISCONN;\r\nsk->sk_state_change(sk);\r\n}\r\n}\r\nbreak;\r\ncase NETDEV_DOWN:\r\ncase NETDEV_UNREGISTER:\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic int afiucv_iucv_init(void)\r\n{\r\nint err;\r\nerr = pr_iucv->iucv_register(&af_iucv_handler, 0);\r\nif (err)\r\ngoto out;\r\naf_iucv_driver.bus = pr_iucv->bus;\r\nerr = driver_register(&af_iucv_driver);\r\nif (err)\r\ngoto out_iucv;\r\naf_iucv_dev = kzalloc(sizeof(struct device), GFP_KERNEL);\r\nif (!af_iucv_dev) {\r\nerr = -ENOMEM;\r\ngoto out_driver;\r\n}\r\ndev_set_name(af_iucv_dev, "af_iucv");\r\naf_iucv_dev->bus = pr_iucv->bus;\r\naf_iucv_dev->parent = pr_iucv->root;\r\naf_iucv_dev->release = (void (*)(struct device *))kfree;\r\naf_iucv_dev->driver = &af_iucv_driver;\r\nerr = device_register(af_iucv_dev);\r\nif (err)\r\ngoto out_driver;\r\nreturn 0;\r\nout_driver:\r\ndriver_unregister(&af_iucv_driver);\r\nout_iucv:\r\npr_iucv->iucv_unregister(&af_iucv_handler, 0);\r\nout:\r\nreturn err;\r\n}\r\nstatic int __init afiucv_init(void)\r\n{\r\nint err;\r\nif (MACHINE_IS_VM) {\r\ncpcmd("QUERY USERID", iucv_userid, sizeof(iucv_userid), &err);\r\nif (unlikely(err)) {\r\nWARN_ON(err);\r\nerr = -EPROTONOSUPPORT;\r\ngoto out;\r\n}\r\npr_iucv = try_then_request_module(symbol_get(iucv_if), "iucv");\r\nif (!pr_iucv) {\r\nprintk(KERN_WARNING "iucv_if lookup failed\n");\r\nmemset(&iucv_userid, 0, sizeof(iucv_userid));\r\n}\r\n} else {\r\nmemset(&iucv_userid, 0, sizeof(iucv_userid));\r\npr_iucv = NULL;\r\n}\r\nerr = proto_register(&iucv_proto, 0);\r\nif (err)\r\ngoto out;\r\nerr = sock_register(&iucv_sock_family_ops);\r\nif (err)\r\ngoto out_proto;\r\nif (pr_iucv) {\r\nerr = afiucv_iucv_init();\r\nif (err)\r\ngoto out_sock;\r\n} else\r\nregister_netdevice_notifier(&afiucv_netdev_notifier);\r\ndev_add_pack(&iucv_packet_type);\r\nreturn 0;\r\nout_sock:\r\nsock_unregister(PF_IUCV);\r\nout_proto:\r\nproto_unregister(&iucv_proto);\r\nout:\r\nif (pr_iucv)\r\nsymbol_put(iucv_if);\r\nreturn err;\r\n}\r\nstatic void __exit afiucv_exit(void)\r\n{\r\nif (pr_iucv) {\r\ndevice_unregister(af_iucv_dev);\r\ndriver_unregister(&af_iucv_driver);\r\npr_iucv->iucv_unregister(&af_iucv_handler, 0);\r\nsymbol_put(iucv_if);\r\n} else\r\nunregister_netdevice_notifier(&afiucv_netdev_notifier);\r\ndev_remove_pack(&iucv_packet_type);\r\nsock_unregister(PF_IUCV);\r\nproto_unregister(&iucv_proto);\r\n}
