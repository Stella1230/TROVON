static inline struct netem_skb_cb *netem_skb_cb(struct sk_buff *skb)\r\n{\r\nqdisc_cb_private_validate(skb, sizeof(struct netem_skb_cb));\r\nreturn (struct netem_skb_cb *)qdisc_skb_cb(skb)->data;\r\n}\r\nstatic void init_crandom(struct crndstate *state, unsigned long rho)\r\n{\r\nstate->rho = rho;\r\nstate->last = net_random();\r\n}\r\nstatic u32 get_crandom(struct crndstate *state)\r\n{\r\nu64 value, rho;\r\nunsigned long answer;\r\nif (state->rho == 0)\r\nreturn net_random();\r\nvalue = net_random();\r\nrho = (u64)state->rho + 1;\r\nanswer = (value * ((1ull<<32) - rho) + state->last * rho) >> 32;\r\nstate->last = answer;\r\nreturn answer;\r\n}\r\nstatic bool loss_4state(struct netem_sched_data *q)\r\n{\r\nstruct clgstate *clg = &q->clg;\r\nu32 rnd = net_random();\r\nswitch (clg->state) {\r\ncase 1:\r\nif (rnd < clg->a4) {\r\nclg->state = 4;\r\nreturn true;\r\n} else if (clg->a4 < rnd && rnd < clg->a1) {\r\nclg->state = 3;\r\nreturn true;\r\n} else if (clg->a1 < rnd)\r\nclg->state = 1;\r\nbreak;\r\ncase 2:\r\nif (rnd < clg->a5) {\r\nclg->state = 3;\r\nreturn true;\r\n} else\r\nclg->state = 2;\r\nbreak;\r\ncase 3:\r\nif (rnd < clg->a3)\r\nclg->state = 2;\r\nelse if (clg->a3 < rnd && rnd < clg->a2 + clg->a3) {\r\nclg->state = 1;\r\nreturn true;\r\n} else if (clg->a2 + clg->a3 < rnd) {\r\nclg->state = 3;\r\nreturn true;\r\n}\r\nbreak;\r\ncase 4:\r\nclg->state = 1;\r\nbreak;\r\n}\r\nreturn false;\r\n}\r\nstatic bool loss_gilb_ell(struct netem_sched_data *q)\r\n{\r\nstruct clgstate *clg = &q->clg;\r\nswitch (clg->state) {\r\ncase 1:\r\nif (net_random() < clg->a1)\r\nclg->state = 2;\r\nif (net_random() < clg->a4)\r\nreturn true;\r\ncase 2:\r\nif (net_random() < clg->a2)\r\nclg->state = 1;\r\nif (clg->a3 > net_random())\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool loss_event(struct netem_sched_data *q)\r\n{\r\nswitch (q->loss_model) {\r\ncase CLG_RANDOM:\r\nreturn q->loss && q->loss >= get_crandom(&q->loss_cor);\r\ncase CLG_4_STATES:\r\nreturn loss_4state(q);\r\ncase CLG_GILB_ELL:\r\nreturn loss_gilb_ell(q);\r\n}\r\nreturn false;\r\n}\r\nstatic psched_tdiff_t tabledist(psched_tdiff_t mu, psched_tdiff_t sigma,\r\nstruct crndstate *state,\r\nconst struct disttable *dist)\r\n{\r\npsched_tdiff_t x;\r\nlong t;\r\nu32 rnd;\r\nif (sigma == 0)\r\nreturn mu;\r\nrnd = get_crandom(state);\r\nif (dist == NULL)\r\nreturn (rnd % (2*sigma)) - sigma + mu;\r\nt = dist->table[rnd % dist->size];\r\nx = (sigma % NETEM_DIST_SCALE) * t;\r\nif (x >= 0)\r\nx += NETEM_DIST_SCALE/2;\r\nelse\r\nx -= NETEM_DIST_SCALE/2;\r\nreturn x / NETEM_DIST_SCALE + (sigma / NETEM_DIST_SCALE) * t + mu;\r\n}\r\nstatic psched_time_t packet_len_2_sched_time(unsigned int len, struct netem_sched_data *q)\r\n{\r\nu64 ticks;\r\nlen += q->packet_overhead;\r\nif (q->cell_size) {\r\nu32 cells = reciprocal_divide(len, q->cell_size_reciprocal);\r\nif (len > cells * q->cell_size)\r\ncells++;\r\nlen = cells * (q->cell_size + q->cell_overhead);\r\n}\r\nticks = (u64)len * NSEC_PER_SEC;\r\ndo_div(ticks, q->rate);\r\nreturn PSCHED_NS2TICKS(ticks);\r\n}\r\nstatic void tfifo_enqueue(struct sk_buff *nskb, struct Qdisc *sch)\r\n{\r\nstruct sk_buff_head *list = &sch->q;\r\npsched_time_t tnext = netem_skb_cb(nskb)->time_to_send;\r\nstruct sk_buff *skb = skb_peek_tail(list);\r\nif (likely(!skb || tnext >= netem_skb_cb(skb)->time_to_send))\r\nreturn __skb_queue_tail(list, nskb);\r\nskb_queue_reverse_walk(list, skb) {\r\nif (tnext >= netem_skb_cb(skb)->time_to_send)\r\nbreak;\r\n}\r\n__skb_queue_after(list, skb, nskb);\r\n}\r\nstatic int netem_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nstruct netem_skb_cb *cb;\r\nstruct sk_buff *skb2;\r\nint count = 1;\r\nif (q->duplicate && q->duplicate >= get_crandom(&q->dup_cor))\r\n++count;\r\nif (loss_event(q)) {\r\nif (q->ecn && INET_ECN_set_ce(skb))\r\nsch->qstats.drops++;\r\nelse\r\n--count;\r\n}\r\nif (count == 0) {\r\nsch->qstats.drops++;\r\nkfree_skb(skb);\r\nreturn NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\r\n}\r\nif (q->latency || q->jitter)\r\nskb_orphan(skb);\r\nif (count > 1 && (skb2 = skb_clone(skb, GFP_ATOMIC)) != NULL) {\r\nstruct Qdisc *rootq = qdisc_root(sch);\r\nu32 dupsave = q->duplicate;\r\nq->duplicate = 0;\r\nqdisc_enqueue_root(skb2, rootq);\r\nq->duplicate = dupsave;\r\n}\r\nif (q->corrupt && q->corrupt >= get_crandom(&q->corrupt_cor)) {\r\nif (!(skb = skb_unshare(skb, GFP_ATOMIC)) ||\r\n(skb->ip_summed == CHECKSUM_PARTIAL &&\r\nskb_checksum_help(skb)))\r\nreturn qdisc_drop(skb, sch);\r\nskb->data[net_random() % skb_headlen(skb)] ^= 1<<(net_random() % 8);\r\n}\r\nif (unlikely(skb_queue_len(&sch->q) >= sch->limit))\r\nreturn qdisc_reshape_fail(skb, sch);\r\nsch->qstats.backlog += qdisc_pkt_len(skb);\r\ncb = netem_skb_cb(skb);\r\nif (q->gap == 0 ||\r\nq->counter < q->gap - 1 ||\r\nq->reorder < get_crandom(&q->reorder_cor)) {\r\npsched_time_t now;\r\npsched_tdiff_t delay;\r\ndelay = tabledist(q->latency, q->jitter,\r\n&q->delay_cor, q->delay_dist);\r\nnow = psched_get_time();\r\nif (q->rate) {\r\nstruct sk_buff_head *list = &sch->q;\r\ndelay += packet_len_2_sched_time(skb->len, q);\r\nif (!skb_queue_empty(list)) {\r\ndelay -= now - netem_skb_cb(skb_peek(list))->time_to_send;\r\nnow = netem_skb_cb(skb_peek_tail(list))->time_to_send;\r\n}\r\n}\r\ncb->time_to_send = now + delay;\r\n++q->counter;\r\ntfifo_enqueue(skb, sch);\r\n} else {\r\ncb->time_to_send = psched_get_time();\r\nq->counter = 0;\r\n__skb_queue_head(&sch->q, skb);\r\nsch->qstats.requeues++;\r\n}\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic unsigned int netem_drop(struct Qdisc *sch)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nunsigned int len;\r\nlen = qdisc_queue_drop(sch);\r\nif (!len && q->qdisc && q->qdisc->ops->drop)\r\nlen = q->qdisc->ops->drop(q->qdisc);\r\nif (len)\r\nsch->qstats.drops++;\r\nreturn len;\r\n}\r\nstatic struct sk_buff *netem_dequeue(struct Qdisc *sch)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nif (qdisc_is_throttled(sch))\r\nreturn NULL;\r\ntfifo_dequeue:\r\nskb = qdisc_peek_head(sch);\r\nif (skb) {\r\nconst struct netem_skb_cb *cb = netem_skb_cb(skb);\r\nif (cb->time_to_send <= psched_get_time()) {\r\n__skb_unlink(skb, &sch->q);\r\nsch->qstats.backlog -= qdisc_pkt_len(skb);\r\n#ifdef CONFIG_NET_CLS_ACT\r\nif (G_TC_FROM(skb->tc_verd) & AT_INGRESS)\r\nskb->tstamp.tv64 = 0;\r\n#endif\r\nif (q->qdisc) {\r\nint err = qdisc_enqueue(skb, q->qdisc);\r\nif (unlikely(err != NET_XMIT_SUCCESS)) {\r\nif (net_xmit_drop_count(err)) {\r\nsch->qstats.drops++;\r\nqdisc_tree_decrease_qlen(sch, 1);\r\n}\r\n}\r\ngoto tfifo_dequeue;\r\n}\r\ndeliver:\r\nqdisc_unthrottled(sch);\r\nqdisc_bstats_update(sch, skb);\r\nreturn skb;\r\n}\r\nif (q->qdisc) {\r\nskb = q->qdisc->ops->dequeue(q->qdisc);\r\nif (skb)\r\ngoto deliver;\r\n}\r\nqdisc_watchdog_schedule(&q->watchdog, cb->time_to_send);\r\n}\r\nif (q->qdisc) {\r\nskb = q->qdisc->ops->dequeue(q->qdisc);\r\nif (skb)\r\ngoto deliver;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void netem_reset(struct Qdisc *sch)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nqdisc_reset_queue(sch);\r\nif (q->qdisc)\r\nqdisc_reset(q->qdisc);\r\nqdisc_watchdog_cancel(&q->watchdog);\r\n}\r\nstatic void dist_free(struct disttable *d)\r\n{\r\nif (d) {\r\nif (is_vmalloc_addr(d))\r\nvfree(d);\r\nelse\r\nkfree(d);\r\n}\r\n}\r\nstatic int get_dist_table(struct Qdisc *sch, const struct nlattr *attr)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nsize_t n = nla_len(attr)/sizeof(__s16);\r\nconst __s16 *data = nla_data(attr);\r\nspinlock_t *root_lock;\r\nstruct disttable *d;\r\nint i;\r\nsize_t s;\r\nif (n > NETEM_DIST_MAX)\r\nreturn -EINVAL;\r\ns = sizeof(struct disttable) + n * sizeof(s16);\r\nd = kmalloc(s, GFP_KERNEL | __GFP_NOWARN);\r\nif (!d)\r\nd = vmalloc(s);\r\nif (!d)\r\nreturn -ENOMEM;\r\nd->size = n;\r\nfor (i = 0; i < n; i++)\r\nd->table[i] = data[i];\r\nroot_lock = qdisc_root_sleeping_lock(sch);\r\nspin_lock_bh(root_lock);\r\nswap(q->delay_dist, d);\r\nspin_unlock_bh(root_lock);\r\ndist_free(d);\r\nreturn 0;\r\n}\r\nstatic void get_correlation(struct Qdisc *sch, const struct nlattr *attr)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nconst struct tc_netem_corr *c = nla_data(attr);\r\ninit_crandom(&q->delay_cor, c->delay_corr);\r\ninit_crandom(&q->loss_cor, c->loss_corr);\r\ninit_crandom(&q->dup_cor, c->dup_corr);\r\n}\r\nstatic void get_reorder(struct Qdisc *sch, const struct nlattr *attr)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nconst struct tc_netem_reorder *r = nla_data(attr);\r\nq->reorder = r->probability;\r\ninit_crandom(&q->reorder_cor, r->correlation);\r\n}\r\nstatic void get_corrupt(struct Qdisc *sch, const struct nlattr *attr)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nconst struct tc_netem_corrupt *r = nla_data(attr);\r\nq->corrupt = r->probability;\r\ninit_crandom(&q->corrupt_cor, r->correlation);\r\n}\r\nstatic void get_rate(struct Qdisc *sch, const struct nlattr *attr)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nconst struct tc_netem_rate *r = nla_data(attr);\r\nq->rate = r->rate;\r\nq->packet_overhead = r->packet_overhead;\r\nq->cell_size = r->cell_size;\r\nif (q->cell_size)\r\nq->cell_size_reciprocal = reciprocal_value(q->cell_size);\r\nq->cell_overhead = r->cell_overhead;\r\n}\r\nstatic int get_loss_clg(struct Qdisc *sch, const struct nlattr *attr)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nconst struct nlattr *la;\r\nint rem;\r\nnla_for_each_nested(la, attr, rem) {\r\nu16 type = nla_type(la);\r\nswitch(type) {\r\ncase NETEM_LOSS_GI: {\r\nconst struct tc_netem_gimodel *gi = nla_data(la);\r\nif (nla_len(la) < sizeof(struct tc_netem_gimodel)) {\r\npr_info("netem: incorrect gi model size\n");\r\nreturn -EINVAL;\r\n}\r\nq->loss_model = CLG_4_STATES;\r\nq->clg.state = 1;\r\nq->clg.a1 = gi->p13;\r\nq->clg.a2 = gi->p31;\r\nq->clg.a3 = gi->p32;\r\nq->clg.a4 = gi->p14;\r\nq->clg.a5 = gi->p23;\r\nbreak;\r\n}\r\ncase NETEM_LOSS_GE: {\r\nconst struct tc_netem_gemodel *ge = nla_data(la);\r\nif (nla_len(la) < sizeof(struct tc_netem_gemodel)) {\r\npr_info("netem: incorrect ge model size\n");\r\nreturn -EINVAL;\r\n}\r\nq->loss_model = CLG_GILB_ELL;\r\nq->clg.state = 1;\r\nq->clg.a1 = ge->p;\r\nq->clg.a2 = ge->r;\r\nq->clg.a3 = ge->h;\r\nq->clg.a4 = ge->k1;\r\nbreak;\r\n}\r\ndefault:\r\npr_info("netem: unknown loss type %u\n", type);\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int parse_attr(struct nlattr *tb[], int maxtype, struct nlattr *nla,\r\nconst struct nla_policy *policy, int len)\r\n{\r\nint nested_len = nla_len(nla) - NLA_ALIGN(len);\r\nif (nested_len < 0) {\r\npr_info("netem: invalid attributes len %d\n", nested_len);\r\nreturn -EINVAL;\r\n}\r\nif (nested_len >= nla_attr_size(0))\r\nreturn nla_parse(tb, maxtype, nla_data(nla) + NLA_ALIGN(len),\r\nnested_len, policy);\r\nmemset(tb, 0, sizeof(struct nlattr *) * (maxtype + 1));\r\nreturn 0;\r\n}\r\nstatic int netem_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_NETEM_MAX + 1];\r\nstruct tc_netem_qopt *qopt;\r\nint ret;\r\nif (opt == NULL)\r\nreturn -EINVAL;\r\nqopt = nla_data(opt);\r\nret = parse_attr(tb, TCA_NETEM_MAX, opt, netem_policy, sizeof(*qopt));\r\nif (ret < 0)\r\nreturn ret;\r\nsch->limit = qopt->limit;\r\nq->latency = qopt->latency;\r\nq->jitter = qopt->jitter;\r\nq->limit = qopt->limit;\r\nq->gap = qopt->gap;\r\nq->counter = 0;\r\nq->loss = qopt->loss;\r\nq->duplicate = qopt->duplicate;\r\nif (q->gap)\r\nq->reorder = ~0;\r\nif (tb[TCA_NETEM_CORR])\r\nget_correlation(sch, tb[TCA_NETEM_CORR]);\r\nif (tb[TCA_NETEM_DELAY_DIST]) {\r\nret = get_dist_table(sch, tb[TCA_NETEM_DELAY_DIST]);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (tb[TCA_NETEM_REORDER])\r\nget_reorder(sch, tb[TCA_NETEM_REORDER]);\r\nif (tb[TCA_NETEM_CORRUPT])\r\nget_corrupt(sch, tb[TCA_NETEM_CORRUPT]);\r\nif (tb[TCA_NETEM_RATE])\r\nget_rate(sch, tb[TCA_NETEM_RATE]);\r\nif (tb[TCA_NETEM_ECN])\r\nq->ecn = nla_get_u32(tb[TCA_NETEM_ECN]);\r\nq->loss_model = CLG_RANDOM;\r\nif (tb[TCA_NETEM_LOSS])\r\nret = get_loss_clg(sch, tb[TCA_NETEM_LOSS]);\r\nreturn ret;\r\n}\r\nstatic int netem_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nint ret;\r\nif (!opt)\r\nreturn -EINVAL;\r\nqdisc_watchdog_init(&q->watchdog, sch);\r\nq->loss_model = CLG_RANDOM;\r\nret = netem_change(sch, opt);\r\nif (ret)\r\npr_info("netem: change failed\n");\r\nreturn ret;\r\n}\r\nstatic void netem_destroy(struct Qdisc *sch)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nqdisc_watchdog_cancel(&q->watchdog);\r\nif (q->qdisc)\r\nqdisc_destroy(q->qdisc);\r\ndist_free(q->delay_dist);\r\n}\r\nstatic int dump_loss_model(const struct netem_sched_data *q,\r\nstruct sk_buff *skb)\r\n{\r\nstruct nlattr *nest;\r\nnest = nla_nest_start(skb, TCA_NETEM_LOSS);\r\nif (nest == NULL)\r\ngoto nla_put_failure;\r\nswitch (q->loss_model) {\r\ncase CLG_RANDOM:\r\nnla_nest_cancel(skb, nest);\r\nreturn 0;\r\ncase CLG_4_STATES: {\r\nstruct tc_netem_gimodel gi = {\r\n.p13 = q->clg.a1,\r\n.p31 = q->clg.a2,\r\n.p32 = q->clg.a3,\r\n.p14 = q->clg.a4,\r\n.p23 = q->clg.a5,\r\n};\r\nif (nla_put(skb, NETEM_LOSS_GI, sizeof(gi), &gi))\r\ngoto nla_put_failure;\r\nbreak;\r\n}\r\ncase CLG_GILB_ELL: {\r\nstruct tc_netem_gemodel ge = {\r\n.p = q->clg.a1,\r\n.r = q->clg.a2,\r\n.h = q->clg.a3,\r\n.k1 = q->clg.a4,\r\n};\r\nif (nla_put(skb, NETEM_LOSS_GE, sizeof(ge), &ge))\r\ngoto nla_put_failure;\r\nbreak;\r\n}\r\n}\r\nnla_nest_end(skb, nest);\r\nreturn 0;\r\nnla_put_failure:\r\nnla_nest_cancel(skb, nest);\r\nreturn -1;\r\n}\r\nstatic int netem_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nconst struct netem_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *nla = (struct nlattr *) skb_tail_pointer(skb);\r\nstruct tc_netem_qopt qopt;\r\nstruct tc_netem_corr cor;\r\nstruct tc_netem_reorder reorder;\r\nstruct tc_netem_corrupt corrupt;\r\nstruct tc_netem_rate rate;\r\nqopt.latency = q->latency;\r\nqopt.jitter = q->jitter;\r\nqopt.limit = q->limit;\r\nqopt.loss = q->loss;\r\nqopt.gap = q->gap;\r\nqopt.duplicate = q->duplicate;\r\nif (nla_put(skb, TCA_OPTIONS, sizeof(qopt), &qopt))\r\ngoto nla_put_failure;\r\ncor.delay_corr = q->delay_cor.rho;\r\ncor.loss_corr = q->loss_cor.rho;\r\ncor.dup_corr = q->dup_cor.rho;\r\nif (nla_put(skb, TCA_NETEM_CORR, sizeof(cor), &cor))\r\ngoto nla_put_failure;\r\nreorder.probability = q->reorder;\r\nreorder.correlation = q->reorder_cor.rho;\r\nif (nla_put(skb, TCA_NETEM_REORDER, sizeof(reorder), &reorder))\r\ngoto nla_put_failure;\r\ncorrupt.probability = q->corrupt;\r\ncorrupt.correlation = q->corrupt_cor.rho;\r\nif (nla_put(skb, TCA_NETEM_CORRUPT, sizeof(corrupt), &corrupt))\r\ngoto nla_put_failure;\r\nrate.rate = q->rate;\r\nrate.packet_overhead = q->packet_overhead;\r\nrate.cell_size = q->cell_size;\r\nrate.cell_overhead = q->cell_overhead;\r\nif (nla_put(skb, TCA_NETEM_RATE, sizeof(rate), &rate))\r\ngoto nla_put_failure;\r\nif (q->ecn && nla_put_u32(skb, TCA_NETEM_ECN, q->ecn))\r\ngoto nla_put_failure;\r\nif (dump_loss_model(q, skb) != 0)\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, nla);\r\nnla_put_failure:\r\nnlmsg_trim(skb, nla);\r\nreturn -1;\r\n}\r\nstatic int netem_dump_class(struct Qdisc *sch, unsigned long cl,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nif (cl != 1 || !q->qdisc)\r\nreturn -ENOENT;\r\ntcm->tcm_handle |= TC_H_MIN(1);\r\ntcm->tcm_info = q->qdisc->handle;\r\nreturn 0;\r\n}\r\nstatic int netem_graft(struct Qdisc *sch, unsigned long arg, struct Qdisc *new,\r\nstruct Qdisc **old)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nsch_tree_lock(sch);\r\n*old = q->qdisc;\r\nq->qdisc = new;\r\nif (*old) {\r\nqdisc_tree_decrease_qlen(*old, (*old)->q.qlen);\r\nqdisc_reset(*old);\r\n}\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic struct Qdisc *netem_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nstruct netem_sched_data *q = qdisc_priv(sch);\r\nreturn q->qdisc;\r\n}\r\nstatic unsigned long netem_get(struct Qdisc *sch, u32 classid)\r\n{\r\nreturn 1;\r\n}\r\nstatic void netem_put(struct Qdisc *sch, unsigned long arg)\r\n{\r\n}\r\nstatic void netem_walk(struct Qdisc *sch, struct qdisc_walker *walker)\r\n{\r\nif (!walker->stop) {\r\nif (walker->count >= walker->skip)\r\nif (walker->fn(sch, 1, walker) < 0) {\r\nwalker->stop = 1;\r\nreturn;\r\n}\r\nwalker->count++;\r\n}\r\n}\r\nstatic int __init netem_module_init(void)\r\n{\r\npr_info("netem: version " VERSION "\n");\r\nreturn register_qdisc(&netem_qdisc_ops);\r\n}\r\nstatic void __exit netem_module_exit(void)\r\n{\r\nunregister_qdisc(&netem_qdisc_ops);\r\n}
