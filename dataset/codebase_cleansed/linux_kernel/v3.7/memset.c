void *memset(void *pdst, int c, unsigned int plen)\r\n{\r\nregister char *return_dst __asm__ ("r10") = pdst;\r\nregister int n __asm__ ("r12") = plen;\r\nregister int lc __asm__ ("r11") = c;\r\n__asm__("movu.b %0,r13 \n\\r\nlslq 8,r13 \n\\r\nmove.b %0,r13 \n\\r\nmove.d r13,%0 \n\\r\nlslq 16,r13 \n\\r\nor.d r13,%0"\r\n: "=r" (lc)\r\n: "0" (lc)\r\n: "r13");\r\n{\r\nregister char *dst __asm__ ("r13") = pdst;\r\nif (((unsigned long) pdst & 3) != 0\r\n&& n >= 3)\r\n{\r\nif ((unsigned long) dst & 1)\r\n{\r\n*dst = (char) lc;\r\nn--;\r\ndst++;\r\n}\r\nif ((unsigned long) dst & 2)\r\n{\r\n*(short *) dst = lc;\r\nn -= 2;\r\ndst += 2;\r\n}\r\n}\r\nif (n >= MEMSET_BY_BLOCK_THRESHOLD)\r\n{\r\n__asm__ volatile\r\n("\\r\n;; GCC does promise correct register allocations, but let's \n\\r\n;; make sure it keeps its promises. \n\\r\n.ifnc %0-%1-%4,$r13-$r12-$r11 \n\\r\n.error \"GCC reg alloc bug: %0-%1-%4 != $r13-$r12-$r11\" \n\\r\n.endif \n\\r\n\n\\r\n;; Save the registers we'll clobber in the movem process \n\\r\n;; on the stack. Don't mention them to gcc, it will only be \n\\r\n;; upset. \n\\r\nsubq 11*4,sp \n\\r\nmovem r10,[sp] \n\\r\n\n\\r\nmove.d r11,r0 \n\\r\nmove.d r11,r1 \n\\r\nmove.d r11,r2 \n\\r\nmove.d r11,r3 \n\\r\nmove.d r11,r4 \n\\r\nmove.d r11,r5 \n\\r\nmove.d r11,r6 \n\\r\nmove.d r11,r7 \n\\r\nmove.d r11,r8 \n\\r\nmove.d r11,r9 \n\\r\nmove.d r11,r10 \n\\r\n\n\\r\n;; Now we've got this: \n\\r\n;; r13 - dst \n\\r\n;; r12 - n \n\\r\n\n\\r\n;; Update n for the first loop \n\\r\nsubq 12*4,r12 \n\\r\n0: \n\\r\n"\r\n#ifdef __arch_common_v10_v32\r\n" setf\n"\r\n#endif\r\n" subq 12*4,r12 \n\\r\nbge 0b \n\\r\nmovem r11,[r13+] \n\\r\n\n\\r\n;; Compensate for last loop underflowing n. \n\\r\naddq 12*4,r12 \n\\r\n\n\\r\n;; Restore registers from stack. \n\\r\nmovem [sp+],r10"\r\n: "=r" (dst), "=r" (n)\r\n: "0" (dst), "1" (n), "r" (lc));\r\n}\r\nwhile (n >= 16)\r\n{\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\nn -= 16;\r\n}\r\nswitch (n)\r\n{\r\ncase 0:\r\nbreak;\r\ncase 1:\r\n*dst = (char) lc;\r\nbreak;\r\ncase 2:\r\n*(short *) dst = (short) lc;\r\nbreak;\r\ncase 3:\r\n*(short *) dst = (short) lc; dst += 2;\r\n*dst = (char) lc;\r\nbreak;\r\ncase 4:\r\n*(long *) dst = lc;\r\nbreak;\r\ncase 5:\r\n*(long *) dst = lc; dst += 4;\r\n*dst = (char) lc;\r\nbreak;\r\ncase 6:\r\n*(long *) dst = lc; dst += 4;\r\n*(short *) dst = (short) lc;\r\nbreak;\r\ncase 7:\r\n*(long *) dst = lc; dst += 4;\r\n*(short *) dst = (short) lc; dst += 2;\r\n*dst = (char) lc;\r\nbreak;\r\ncase 8:\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc;\r\nbreak;\r\ncase 9:\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*dst = (char) lc;\r\nbreak;\r\ncase 10:\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(short *) dst = (short) lc;\r\nbreak;\r\ncase 11:\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(short *) dst = (short) lc; dst += 2;\r\n*dst = (char) lc;\r\nbreak;\r\ncase 12:\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc;\r\nbreak;\r\ncase 13:\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*dst = (char) lc;\r\nbreak;\r\ncase 14:\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(short *) dst = (short) lc;\r\nbreak;\r\ncase 15:\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(long *) dst = lc; dst += 4;\r\n*(short *) dst = (short) lc; dst += 2;\r\n*dst = (char) lc;\r\nbreak;\r\n}\r\n}\r\nreturn return_dst;\r\n}
