unsigned long __recover_optprobed_insn(kprobe_opcode_t *buf, unsigned long addr)\r\n{\r\nstruct optimized_kprobe *op;\r\nstruct kprobe *kp;\r\nlong offs;\r\nint i;\r\nfor (i = 0; i < RELATIVEJUMP_SIZE; i++) {\r\nkp = get_kprobe((void *)addr - i);\r\nif (kp && kprobe_optimized(kp)) {\r\nop = container_of(kp, struct optimized_kprobe, kp);\r\nif (list_empty(&op->list))\r\ngoto found;\r\n}\r\n}\r\nreturn addr;\r\nfound:\r\nmemcpy(buf, (void *)addr, MAX_INSN_SIZE * sizeof(kprobe_opcode_t));\r\nif (addr == (unsigned long)kp->addr) {\r\nbuf[0] = kp->opcode;\r\nmemcpy(buf + 1, op->optinsn.copied_insn, RELATIVE_ADDR_SIZE);\r\n} else {\r\noffs = addr - (unsigned long)kp->addr - 1;\r\nmemcpy(buf, op->optinsn.copied_insn + offs, RELATIVE_ADDR_SIZE - offs);\r\n}\r\nreturn (unsigned long)buf;\r\n}\r\nstatic void __kprobes synthesize_set_arg1(kprobe_opcode_t *addr, unsigned long val)\r\n{\r\n#ifdef CONFIG_X86_64\r\n*addr++ = 0x48;\r\n*addr++ = 0xbf;\r\n#else\r\n*addr++ = 0xb8;\r\n#endif\r\n*(unsigned long *)addr = val;\r\n}\r\nstatic void __used __kprobes kprobes_optinsn_template_holder(void)\r\n{\r\nasm volatile (\r\n".global optprobe_template_entry\n"\r\n"optprobe_template_entry:\n"\r\n#ifdef CONFIG_X86_64\r\n" pushq %rsp\n"\r\n" pushfq\n"\r\nSAVE_REGS_STRING\r\n" movq %rsp, %rsi\n"\r\n".global optprobe_template_val\n"\r\n"optprobe_template_val:\n"\r\nASM_NOP5\r\nASM_NOP5\r\n".global optprobe_template_call\n"\r\n"optprobe_template_call:\n"\r\nASM_NOP5\r\n" movq 144(%rsp), %rdx\n"\r\n" movq %rdx, 152(%rsp)\n"\r\nRESTORE_REGS_STRING\r\n" addq $8, %rsp\n"\r\n" popfq\n"\r\n#else\r\n" pushf\n"\r\nSAVE_REGS_STRING\r\n" movl %esp, %edx\n"\r\n".global optprobe_template_val\n"\r\n"optprobe_template_val:\n"\r\nASM_NOP5\r\n".global optprobe_template_call\n"\r\n"optprobe_template_call:\n"\r\nASM_NOP5\r\nRESTORE_REGS_STRING\r\n" addl $4, %esp\n"\r\n" popf\n"\r\n#endif\r\n".global optprobe_template_end\n"\r\n"optprobe_template_end:\n");\r\n}\r\nstatic void __kprobes optimized_callback(struct optimized_kprobe *op, struct pt_regs *regs)\r\n{\r\nstruct kprobe_ctlblk *kcb = get_kprobe_ctlblk();\r\nunsigned long flags;\r\nif (kprobe_disabled(&op->kp))\r\nreturn;\r\nlocal_irq_save(flags);\r\nif (kprobe_running()) {\r\nkprobes_inc_nmissed_count(&op->kp);\r\n} else {\r\n#ifdef CONFIG_X86_64\r\nregs->cs = __KERNEL_CS;\r\n#else\r\nregs->cs = __KERNEL_CS | get_kernel_rpl();\r\nregs->gs = 0;\r\n#endif\r\nregs->ip = (unsigned long)op->kp.addr + INT3_SIZE;\r\nregs->orig_ax = ~0UL;\r\n__this_cpu_write(current_kprobe, &op->kp);\r\nkcb->kprobe_status = KPROBE_HIT_ACTIVE;\r\nopt_pre_handler(&op->kp, regs);\r\n__this_cpu_write(current_kprobe, NULL);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int __kprobes copy_optimized_instructions(u8 *dest, u8 *src)\r\n{\r\nint len = 0, ret;\r\nwhile (len < RELATIVEJUMP_SIZE) {\r\nret = __copy_instruction(dest + len, src + len);\r\nif (!ret || !can_boost(dest + len))\r\nreturn -EINVAL;\r\nlen += ret;\r\n}\r\nif (ftrace_text_reserved(src, src + len - 1) ||\r\nalternatives_text_reserved(src, src + len - 1) ||\r\njump_label_text_reserved(src, src + len - 1))\r\nreturn -EBUSY;\r\nreturn len;\r\n}\r\nstatic int __kprobes insn_is_indirect_jump(struct insn *insn)\r\n{\r\nreturn ((insn->opcode.bytes[0] == 0xff &&\r\n(X86_MODRM_REG(insn->modrm.value) & 6) == 4) ||\r\ninsn->opcode.bytes[0] == 0xea);\r\n}\r\nstatic int insn_jump_into_range(struct insn *insn, unsigned long start, int len)\r\n{\r\nunsigned long target = 0;\r\nswitch (insn->opcode.bytes[0]) {\r\ncase 0xe0:\r\ncase 0xe1:\r\ncase 0xe2:\r\ncase 0xe3:\r\ncase 0xe9:\r\ncase 0xeb:\r\nbreak;\r\ncase 0x0f:\r\nif ((insn->opcode.bytes[1] & 0xf0) == 0x80)\r\nbreak;\r\nreturn 0;\r\ndefault:\r\nif ((insn->opcode.bytes[0] & 0xf0) == 0x70)\r\nbreak;\r\nreturn 0;\r\n}\r\ntarget = (unsigned long)insn->next_byte + insn->immediate.value;\r\nreturn (start <= target && target <= start + len);\r\n}\r\nstatic int __kprobes can_optimize(unsigned long paddr)\r\n{\r\nunsigned long addr, size = 0, offset = 0;\r\nstruct insn insn;\r\nkprobe_opcode_t buf[MAX_INSN_SIZE];\r\nif (!kallsyms_lookup_size_offset(paddr, &size, &offset))\r\nreturn 0;\r\nif ((paddr >= (unsigned long)__entry_text_start) &&\r\n(paddr < (unsigned long)__entry_text_end))\r\nreturn 0;\r\nif (size - offset < RELATIVEJUMP_SIZE)\r\nreturn 0;\r\naddr = paddr - offset;\r\nwhile (addr < paddr - offset + size) {\r\nif (search_exception_tables(addr))\r\nreturn 0;\r\nkernel_insn_init(&insn, (void *)recover_probed_instruction(buf, addr));\r\ninsn_get_length(&insn);\r\nif (insn.opcode.bytes[0] == BREAKPOINT_INSTRUCTION)\r\nreturn 0;\r\ninsn.kaddr = (void *)addr;\r\ninsn.next_byte = (void *)(addr + insn.length);\r\nif (insn_is_indirect_jump(&insn) ||\r\ninsn_jump_into_range(&insn, paddr + INT3_SIZE,\r\nRELATIVE_ADDR_SIZE))\r\nreturn 0;\r\naddr += insn.length;\r\n}\r\nreturn 1;\r\n}\r\nint __kprobes arch_check_optimized_kprobe(struct optimized_kprobe *op)\r\n{\r\nint i;\r\nstruct kprobe *p;\r\nfor (i = 1; i < op->optinsn.size; i++) {\r\np = get_kprobe(op->kp.addr + i);\r\nif (p && !kprobe_disabled(p))\r\nreturn -EEXIST;\r\n}\r\nreturn 0;\r\n}\r\nint __kprobes\r\narch_within_optimized_kprobe(struct optimized_kprobe *op, unsigned long addr)\r\n{\r\nreturn ((unsigned long)op->kp.addr <= addr &&\r\n(unsigned long)op->kp.addr + op->optinsn.size > addr);\r\n}\r\nstatic __kprobes\r\nvoid __arch_remove_optimized_kprobe(struct optimized_kprobe *op, int dirty)\r\n{\r\nif (op->optinsn.insn) {\r\nfree_optinsn_slot(op->optinsn.insn, dirty);\r\nop->optinsn.insn = NULL;\r\nop->optinsn.size = 0;\r\n}\r\n}\r\nvoid __kprobes arch_remove_optimized_kprobe(struct optimized_kprobe *op)\r\n{\r\n__arch_remove_optimized_kprobe(op, 1);\r\n}\r\nint __kprobes arch_prepare_optimized_kprobe(struct optimized_kprobe *op)\r\n{\r\nu8 *buf;\r\nint ret;\r\nlong rel;\r\nif (!can_optimize((unsigned long)op->kp.addr))\r\nreturn -EILSEQ;\r\nop->optinsn.insn = get_optinsn_slot();\r\nif (!op->optinsn.insn)\r\nreturn -ENOMEM;\r\nrel = (long)op->optinsn.insn - (long)op->kp.addr + RELATIVEJUMP_SIZE;\r\nif (abs(rel) > 0x7fffffff)\r\nreturn -ERANGE;\r\nbuf = (u8 *)op->optinsn.insn;\r\nret = copy_optimized_instructions(buf + TMPL_END_IDX, op->kp.addr);\r\nif (ret < 0) {\r\n__arch_remove_optimized_kprobe(op, 0);\r\nreturn ret;\r\n}\r\nop->optinsn.size = ret;\r\nmemcpy(buf, &optprobe_template_entry, TMPL_END_IDX);\r\nsynthesize_set_arg1(buf + TMPL_MOVE_IDX, (unsigned long)op);\r\nsynthesize_relcall(buf + TMPL_CALL_IDX, optimized_callback);\r\nsynthesize_reljump(buf + TMPL_END_IDX + op->optinsn.size,\r\n(u8 *)op->kp.addr + op->optinsn.size);\r\nflush_icache_range((unsigned long) buf,\r\n(unsigned long) buf + TMPL_END_IDX +\r\nop->optinsn.size + RELATIVEJUMP_SIZE);\r\nreturn 0;\r\n}\r\nstatic void __kprobes setup_optimize_kprobe(struct text_poke_param *tprm,\r\nu8 *insn_buf,\r\nstruct optimized_kprobe *op)\r\n{\r\ns32 rel = (s32)((long)op->optinsn.insn -\r\n((long)op->kp.addr + RELATIVEJUMP_SIZE));\r\nmemcpy(op->optinsn.copied_insn, op->kp.addr + INT3_SIZE,\r\nRELATIVE_ADDR_SIZE);\r\ninsn_buf[0] = RELATIVEJUMP_OPCODE;\r\n*(s32 *)(&insn_buf[1]) = rel;\r\ntprm->addr = op->kp.addr;\r\ntprm->opcode = insn_buf;\r\ntprm->len = RELATIVEJUMP_SIZE;\r\n}\r\nvoid __kprobes arch_optimize_kprobes(struct list_head *oplist)\r\n{\r\nstruct optimized_kprobe *op, *tmp;\r\nint c = 0;\r\nlist_for_each_entry_safe(op, tmp, oplist, list) {\r\nWARN_ON(kprobe_disabled(&op->kp));\r\nsetup_optimize_kprobe(&jump_poke_params[c],\r\njump_poke_bufs[c].buf, op);\r\nlist_del_init(&op->list);\r\nif (++c >= MAX_OPTIMIZE_PROBES)\r\nbreak;\r\n}\r\ntext_poke_smp_batch(jump_poke_params, c);\r\n}\r\nstatic void __kprobes setup_unoptimize_kprobe(struct text_poke_param *tprm,\r\nu8 *insn_buf,\r\nstruct optimized_kprobe *op)\r\n{\r\ninsn_buf[0] = BREAKPOINT_INSTRUCTION;\r\nmemcpy(insn_buf + 1, op->optinsn.copied_insn, RELATIVE_ADDR_SIZE);\r\ntprm->addr = op->kp.addr;\r\ntprm->opcode = insn_buf;\r\ntprm->len = RELATIVEJUMP_SIZE;\r\n}\r\nextern void arch_unoptimize_kprobes(struct list_head *oplist,\r\nstruct list_head *done_list)\r\n{\r\nstruct optimized_kprobe *op, *tmp;\r\nint c = 0;\r\nlist_for_each_entry_safe(op, tmp, oplist, list) {\r\nsetup_unoptimize_kprobe(&jump_poke_params[c],\r\njump_poke_bufs[c].buf, op);\r\nlist_move(&op->list, done_list);\r\nif (++c >= MAX_OPTIMIZE_PROBES)\r\nbreak;\r\n}\r\ntext_poke_smp_batch(jump_poke_params, c);\r\n}\r\nvoid __kprobes arch_unoptimize_kprobe(struct optimized_kprobe *op)\r\n{\r\nu8 buf[RELATIVEJUMP_SIZE];\r\nbuf[0] = BREAKPOINT_INSTRUCTION;\r\nmemcpy(buf + 1, op->optinsn.copied_insn, RELATIVE_ADDR_SIZE);\r\ntext_poke_smp(op->kp.addr, buf, RELATIVEJUMP_SIZE);\r\n}\r\nint __kprobes\r\nsetup_detour_execution(struct kprobe *p, struct pt_regs *regs, int reenter)\r\n{\r\nstruct optimized_kprobe *op;\r\nif (p->flags & KPROBE_FLAG_OPTIMIZED) {\r\nop = container_of(p, struct optimized_kprobe, kp);\r\nregs->ip = (unsigned long)op->optinsn.insn + TMPL_END_IDX;\r\nif (!reenter)\r\nreset_current_kprobe();\r\npreempt_enable_no_resched();\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nint __kprobes arch_init_optprobes(void)\r\n{\r\njump_poke_bufs = kmalloc(sizeof(struct jump_poke_buffer) *\r\nMAX_OPTIMIZE_PROBES, GFP_KERNEL);\r\nif (!jump_poke_bufs)\r\nreturn -ENOMEM;\r\njump_poke_params = kmalloc(sizeof(struct text_poke_param) *\r\nMAX_OPTIMIZE_PROBES, GFP_KERNEL);\r\nif (!jump_poke_params) {\r\nkfree(jump_poke_bufs);\r\njump_poke_bufs = NULL;\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}
