static inline int qec_global_reset(void __iomem *gregs)\r\n{\r\nint tries = QEC_RESET_TRIES;\r\nsbus_writel(GLOB_CTRL_RESET, gregs + GLOB_CTRL);\r\nwhile (--tries) {\r\nu32 tmp = sbus_readl(gregs + GLOB_CTRL);\r\nif (tmp & GLOB_CTRL_RESET) {\r\nudelay(20);\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\nif (tries)\r\nreturn 0;\r\nprintk(KERN_ERR "QuadEther: AIEEE cannot reset the QEC!\n");\r\nreturn -1;\r\n}\r\nstatic inline int qe_stop(struct sunqe *qep)\r\n{\r\nvoid __iomem *cregs = qep->qcregs;\r\nvoid __iomem *mregs = qep->mregs;\r\nint tries;\r\nsbus_writeb(MREGS_BCONFIG_RESET, mregs + MREGS_BCONFIG);\r\ntries = MACE_RESET_RETRIES;\r\nwhile (--tries) {\r\nu8 tmp = sbus_readb(mregs + MREGS_BCONFIG);\r\nif (tmp & MREGS_BCONFIG_RESET) {\r\nudelay(20);\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\nif (!tries) {\r\nprintk(KERN_ERR "QuadEther: AIEEE cannot reset the MACE!\n");\r\nreturn -1;\r\n}\r\nsbus_writel(CREG_CTRL_RESET, cregs + CREG_CTRL);\r\ntries = QE_RESET_RETRIES;\r\nwhile (--tries) {\r\nu32 tmp = sbus_readl(cregs + CREG_CTRL);\r\nif (tmp & CREG_CTRL_RESET) {\r\nudelay(20);\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\nif (!tries) {\r\nprintk(KERN_ERR "QuadEther: Cannot reset QE channel!\n");\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void qe_init_rings(struct sunqe *qep)\r\n{\r\nstruct qe_init_block *qb = qep->qe_block;\r\nstruct sunqe_buffers *qbufs = qep->buffers;\r\n__u32 qbufs_dvma = qep->buffers_dvma;\r\nint i;\r\nqep->rx_new = qep->rx_old = qep->tx_new = qep->tx_old = 0;\r\nmemset(qb, 0, sizeof(struct qe_init_block));\r\nmemset(qbufs, 0, sizeof(struct sunqe_buffers));\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nqb->qe_rxd[i].rx_addr = qbufs_dvma + qebuf_offset(rx_buf, i);\r\nqb->qe_rxd[i].rx_flags =\r\n(RXD_OWN | ((RXD_PKT_SZ) & RXD_LENGTH));\r\n}\r\n}\r\nstatic int qe_init(struct sunqe *qep, int from_irq)\r\n{\r\nstruct sunqec *qecp = qep->parent;\r\nvoid __iomem *cregs = qep->qcregs;\r\nvoid __iomem *mregs = qep->mregs;\r\nvoid __iomem *gregs = qecp->gregs;\r\nunsigned char *e = &qep->dev->dev_addr[0];\r\nu32 tmp;\r\nint i;\r\nif (qe_stop(qep))\r\nreturn -EAGAIN;\r\nsbus_writel(qep->qblock_dvma + qib_offset(qe_rxd, 0), cregs + CREG_RXDS);\r\nsbus_writel(qep->qblock_dvma + qib_offset(qe_txd, 0), cregs + CREG_TXDS);\r\nsbus_writel(0, cregs + CREG_RIMASK);\r\nsbus_writel(1, cregs + CREG_TIMASK);\r\nsbus_writel(0, cregs + CREG_QMASK);\r\nsbus_writel(CREG_MMASK_RXCOLL, cregs + CREG_MMASK);\r\ntmp = qep->channel * sbus_readl(gregs + GLOB_MSIZE);\r\nsbus_writel(tmp, cregs + CREG_RXRBUFPTR);\r\nsbus_writel(tmp, cregs + CREG_RXWBUFPTR);\r\ntmp = sbus_readl(cregs + CREG_RXRBUFPTR) +\r\nsbus_readl(gregs + GLOB_RSIZE);\r\nsbus_writel(tmp, cregs + CREG_TXRBUFPTR);\r\nsbus_writel(tmp, cregs + CREG_TXWBUFPTR);\r\nsbus_writel(0, cregs + CREG_CCNT);\r\nsbus_writel(0, cregs + CREG_PIPG);\r\nsbus_writeb(MREGS_PHYCONFIG_AUTO, mregs + MREGS_PHYCONFIG);\r\nsbus_writeb(MREGS_TXFCNTL_AUTOPAD, mregs + MREGS_TXFCNTL);\r\nsbus_writeb(0, mregs + MREGS_RXFCNTL);\r\nsbus_writeb(MREGS_IMASK_COLL | MREGS_IMASK_RXIRQ, mregs + MREGS_IMASK);\r\nsbus_writeb(MREGS_BCONFIG_BSWAP | MREGS_BCONFIG_64TS, mregs + MREGS_BCONFIG);\r\nsbus_writeb((MREGS_FCONFIG_TXF16 | MREGS_FCONFIG_RXF32 |\r\nMREGS_FCONFIG_RFWU | MREGS_FCONFIG_TFWU),\r\nmregs + MREGS_FCONFIG);\r\nsbus_writeb(MREGS_PLSCONFIG_TP, mregs + MREGS_PLSCONFIG);\r\nsbus_writeb(MREGS_IACONFIG_ACHNGE | MREGS_IACONFIG_PARESET,\r\nmregs + MREGS_IACONFIG);\r\nwhile ((sbus_readb(mregs + MREGS_IACONFIG) & MREGS_IACONFIG_ACHNGE) != 0)\r\nbarrier();\r\nsbus_writeb(e[0], mregs + MREGS_ETHADDR);\r\nsbus_writeb(e[1], mregs + MREGS_ETHADDR);\r\nsbus_writeb(e[2], mregs + MREGS_ETHADDR);\r\nsbus_writeb(e[3], mregs + MREGS_ETHADDR);\r\nsbus_writeb(e[4], mregs + MREGS_ETHADDR);\r\nsbus_writeb(e[5], mregs + MREGS_ETHADDR);\r\nsbus_writeb(MREGS_IACONFIG_ACHNGE | MREGS_IACONFIG_LARESET,\r\nmregs + MREGS_IACONFIG);\r\nwhile ((sbus_readb(mregs + MREGS_IACONFIG) & MREGS_IACONFIG_ACHNGE) != 0)\r\nbarrier();\r\nfor (i = 0; i < 8; i++)\r\nsbus_writeb(0, mregs + MREGS_FILTER);\r\nsbus_writeb(0, mregs + MREGS_IACONFIG);\r\nqe_init_rings(qep);\r\nmdelay(5);\r\nif (!(sbus_readb(mregs + MREGS_PHYCONFIG) & MREGS_PHYCONFIG_LTESTDIS)) {\r\nint tries = 50;\r\nwhile (--tries) {\r\nu8 tmp;\r\nmdelay(5);\r\nbarrier();\r\ntmp = sbus_readb(mregs + MREGS_PHYCONFIG);\r\nif ((tmp & MREGS_PHYCONFIG_LSTAT) != 0)\r\nbreak;\r\n}\r\nif (tries == 0)\r\nprintk(KERN_NOTICE "%s: Warning, link state is down.\n", qep->dev->name);\r\n}\r\nsbus_readb(mregs + MREGS_MPCNT);\r\nqe_set_multicast(qep->dev);\r\nreturn 0;\r\n}\r\nstatic int qe_is_bolixed(struct sunqe *qep, u32 qe_status)\r\n{\r\nstruct net_device *dev = qep->dev;\r\nint mace_hwbug_workaround = 0;\r\nif (qe_status & CREG_STAT_EDEFER) {\r\nprintk(KERN_ERR "%s: Excessive transmit defers.\n", dev->name);\r\ndev->stats.tx_errors++;\r\n}\r\nif (qe_status & CREG_STAT_CLOSS) {\r\nprintk(KERN_ERR "%s: Carrier lost, link down?\n", dev->name);\r\ndev->stats.tx_errors++;\r\ndev->stats.tx_carrier_errors++;\r\n}\r\nif (qe_status & CREG_STAT_ERETRIES) {\r\nprintk(KERN_ERR "%s: Excessive transmit retries (more than 16).\n", dev->name);\r\ndev->stats.tx_errors++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (qe_status & CREG_STAT_LCOLL) {\r\nprintk(KERN_ERR "%s: Late transmit collision.\n", dev->name);\r\ndev->stats.tx_errors++;\r\ndev->stats.collisions++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (qe_status & CREG_STAT_FUFLOW) {\r\nprintk(KERN_ERR "%s: Transmit fifo underflow, driver bug.\n", dev->name);\r\ndev->stats.tx_errors++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (qe_status & CREG_STAT_JERROR) {\r\nprintk(KERN_ERR "%s: Jabber error.\n", dev->name);\r\n}\r\nif (qe_status & CREG_STAT_BERROR) {\r\nprintk(KERN_ERR "%s: Babble error.\n", dev->name);\r\n}\r\nif (qe_status & CREG_STAT_CCOFLOW) {\r\ndev->stats.tx_errors += 256;\r\ndev->stats.collisions += 256;\r\n}\r\nif (qe_status & CREG_STAT_TXDERROR) {\r\nprintk(KERN_ERR "%s: Transmit descriptor is bogus, driver bug.\n", dev->name);\r\ndev->stats.tx_errors++;\r\ndev->stats.tx_aborted_errors++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (qe_status & CREG_STAT_TXLERR) {\r\nprintk(KERN_ERR "%s: Transmit late error.\n", dev->name);\r\ndev->stats.tx_errors++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (qe_status & CREG_STAT_TXPERR) {\r\nprintk(KERN_ERR "%s: Transmit DMA parity error.\n", dev->name);\r\ndev->stats.tx_errors++;\r\ndev->stats.tx_aborted_errors++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (qe_status & CREG_STAT_TXSERR) {\r\nprintk(KERN_ERR "%s: Transmit DMA sbus error ack.\n", dev->name);\r\ndev->stats.tx_errors++;\r\ndev->stats.tx_aborted_errors++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (qe_status & CREG_STAT_RCCOFLOW) {\r\ndev->stats.rx_errors += 256;\r\ndev->stats.collisions += 256;\r\n}\r\nif (qe_status & CREG_STAT_RUOFLOW) {\r\ndev->stats.rx_errors += 256;\r\ndev->stats.rx_over_errors += 256;\r\n}\r\nif (qe_status & CREG_STAT_MCOFLOW) {\r\ndev->stats.rx_errors += 256;\r\ndev->stats.rx_missed_errors += 256;\r\n}\r\nif (qe_status & CREG_STAT_RXFOFLOW) {\r\nprintk(KERN_ERR "%s: Receive fifo overflow.\n", dev->name);\r\ndev->stats.rx_errors++;\r\ndev->stats.rx_over_errors++;\r\n}\r\nif (qe_status & CREG_STAT_RLCOLL) {\r\nprintk(KERN_ERR "%s: Late receive collision.\n", dev->name);\r\ndev->stats.rx_errors++;\r\ndev->stats.collisions++;\r\n}\r\nif (qe_status & CREG_STAT_FCOFLOW) {\r\ndev->stats.rx_errors += 256;\r\ndev->stats.rx_frame_errors += 256;\r\n}\r\nif (qe_status & CREG_STAT_CECOFLOW) {\r\ndev->stats.rx_errors += 256;\r\ndev->stats.rx_crc_errors += 256;\r\n}\r\nif (qe_status & CREG_STAT_RXDROP) {\r\nprintk(KERN_ERR "%s: Receive packet dropped.\n", dev->name);\r\ndev->stats.rx_errors++;\r\ndev->stats.rx_dropped++;\r\ndev->stats.rx_missed_errors++;\r\n}\r\nif (qe_status & CREG_STAT_RXSMALL) {\r\nprintk(KERN_ERR "%s: Receive buffer too small, driver bug.\n", dev->name);\r\ndev->stats.rx_errors++;\r\ndev->stats.rx_length_errors++;\r\n}\r\nif (qe_status & CREG_STAT_RXLERR) {\r\nprintk(KERN_ERR "%s: Receive late error.\n", dev->name);\r\ndev->stats.rx_errors++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (qe_status & CREG_STAT_RXPERR) {\r\nprintk(KERN_ERR "%s: Receive DMA parity error.\n", dev->name);\r\ndev->stats.rx_errors++;\r\ndev->stats.rx_missed_errors++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (qe_status & CREG_STAT_RXSERR) {\r\nprintk(KERN_ERR "%s: Receive DMA sbus error ack.\n", dev->name);\r\ndev->stats.rx_errors++;\r\ndev->stats.rx_missed_errors++;\r\nmace_hwbug_workaround = 1;\r\n}\r\nif (mace_hwbug_workaround)\r\nqe_init(qep, 1);\r\nreturn mace_hwbug_workaround;\r\n}\r\nstatic void qe_rx(struct sunqe *qep)\r\n{\r\nstruct qe_rxd *rxbase = &qep->qe_block->qe_rxd[0];\r\nstruct net_device *dev = qep->dev;\r\nstruct qe_rxd *this;\r\nstruct sunqe_buffers *qbufs = qep->buffers;\r\n__u32 qbufs_dvma = qep->buffers_dvma;\r\nint elem = qep->rx_new, drops = 0;\r\nu32 flags;\r\nthis = &rxbase[elem];\r\nwhile (!((flags = this->rx_flags) & RXD_OWN)) {\r\nstruct sk_buff *skb;\r\nunsigned char *this_qbuf =\r\n&qbufs->rx_buf[elem & (RX_RING_SIZE - 1)][0];\r\n__u32 this_qbuf_dvma = qbufs_dvma +\r\nqebuf_offset(rx_buf, (elem & (RX_RING_SIZE - 1)));\r\nstruct qe_rxd *end_rxd =\r\n&rxbase[(elem+RX_RING_SIZE)&(RX_RING_MAXSIZE-1)];\r\nint len = (flags & RXD_LENGTH) - 4;\r\nif (len < ETH_ZLEN) {\r\ndev->stats.rx_errors++;\r\ndev->stats.rx_length_errors++;\r\ndev->stats.rx_dropped++;\r\n} else {\r\nskb = netdev_alloc_skb(dev, len + 2);\r\nif (skb == NULL) {\r\ndrops++;\r\ndev->stats.rx_dropped++;\r\n} else {\r\nskb_reserve(skb, 2);\r\nskb_put(skb, len);\r\nskb_copy_to_linear_data(skb, this_qbuf,\r\nlen);\r\nskb->protocol = eth_type_trans(skb, qep->dev);\r\nnetif_rx(skb);\r\ndev->stats.rx_packets++;\r\ndev->stats.rx_bytes += len;\r\n}\r\n}\r\nend_rxd->rx_addr = this_qbuf_dvma;\r\nend_rxd->rx_flags = (RXD_OWN | ((RXD_PKT_SZ) & RXD_LENGTH));\r\nelem = NEXT_RX(elem);\r\nthis = &rxbase[elem];\r\n}\r\nqep->rx_new = elem;\r\nif (drops)\r\nprintk(KERN_NOTICE "%s: Memory squeeze, deferring packet.\n", qep->dev->name);\r\n}\r\nstatic irqreturn_t qec_interrupt(int irq, void *dev_id)\r\n{\r\nstruct sunqec *qecp = dev_id;\r\nu32 qec_status;\r\nint channel = 0;\r\nqec_status = sbus_readl(qecp->gregs + GLOB_STAT);\r\nwhile (channel < 4) {\r\nif (qec_status & 0xf) {\r\nstruct sunqe *qep = qecp->qes[channel];\r\nu32 qe_status;\r\nqe_status = sbus_readl(qep->qcregs + CREG_STAT);\r\nif (qe_status & CREG_STAT_ERRORS) {\r\nif (qe_is_bolixed(qep, qe_status))\r\ngoto next;\r\n}\r\nif (qe_status & CREG_STAT_RXIRQ)\r\nqe_rx(qep);\r\nif (netif_queue_stopped(qep->dev) &&\r\n(qe_status & CREG_STAT_TXIRQ)) {\r\nspin_lock(&qep->lock);\r\nqe_tx_reclaim(qep);\r\nif (TX_BUFFS_AVAIL(qep) > 0) {\r\nnetif_wake_queue(qep->dev);\r\nsbus_writel(1, qep->qcregs + CREG_TIMASK);\r\n}\r\nspin_unlock(&qep->lock);\r\n}\r\nnext:\r\n;\r\n}\r\nqec_status >>= 4;\r\nchannel++;\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int qe_open(struct net_device *dev)\r\n{\r\nstruct sunqe *qep = netdev_priv(dev);\r\nqep->mconfig = (MREGS_MCONFIG_TXENAB |\r\nMREGS_MCONFIG_RXENAB |\r\nMREGS_MCONFIG_MBAENAB);\r\nreturn qe_init(qep, 0);\r\n}\r\nstatic int qe_close(struct net_device *dev)\r\n{\r\nstruct sunqe *qep = netdev_priv(dev);\r\nqe_stop(qep);\r\nreturn 0;\r\n}\r\nstatic void qe_tx_reclaim(struct sunqe *qep)\r\n{\r\nstruct qe_txd *txbase = &qep->qe_block->qe_txd[0];\r\nint elem = qep->tx_old;\r\nwhile (elem != qep->tx_new) {\r\nu32 flags = txbase[elem].tx_flags;\r\nif (flags & TXD_OWN)\r\nbreak;\r\nelem = NEXT_TX(elem);\r\n}\r\nqep->tx_old = elem;\r\n}\r\nstatic void qe_tx_timeout(struct net_device *dev)\r\n{\r\nstruct sunqe *qep = netdev_priv(dev);\r\nint tx_full;\r\nspin_lock_irq(&qep->lock);\r\nqe_tx_reclaim(qep);\r\ntx_full = TX_BUFFS_AVAIL(qep) <= 0;\r\nspin_unlock_irq(&qep->lock);\r\nif (! tx_full)\r\ngoto out;\r\nprintk(KERN_ERR "%s: transmit timed out, resetting\n", dev->name);\r\nqe_init(qep, 1);\r\nout:\r\nnetif_wake_queue(dev);\r\n}\r\nstatic int qe_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct sunqe *qep = netdev_priv(dev);\r\nstruct sunqe_buffers *qbufs = qep->buffers;\r\n__u32 txbuf_dvma, qbufs_dvma = qep->buffers_dvma;\r\nunsigned char *txbuf;\r\nint len, entry;\r\nspin_lock_irq(&qep->lock);\r\nqe_tx_reclaim(qep);\r\nlen = skb->len;\r\nentry = qep->tx_new;\r\ntxbuf = &qbufs->tx_buf[entry & (TX_RING_SIZE - 1)][0];\r\ntxbuf_dvma = qbufs_dvma +\r\nqebuf_offset(tx_buf, (entry & (TX_RING_SIZE - 1)));\r\nqep->qe_block->qe_txd[entry].tx_flags = TXD_UPDATE;\r\nskb_copy_from_linear_data(skb, txbuf, len);\r\nqep->qe_block->qe_txd[entry].tx_addr = txbuf_dvma;\r\nqep->qe_block->qe_txd[entry].tx_flags =\r\n(TXD_OWN | TXD_SOP | TXD_EOP | (len & TXD_LENGTH));\r\nqep->tx_new = NEXT_TX(entry);\r\nsbus_writel(CREG_CTRL_TWAKEUP, qep->qcregs + CREG_CTRL);\r\ndev->stats.tx_packets++;\r\ndev->stats.tx_bytes += len;\r\nif (TX_BUFFS_AVAIL(qep) <= 0) {\r\nnetif_stop_queue(dev);\r\nsbus_writel(0, qep->qcregs + CREG_TIMASK);\r\n}\r\nspin_unlock_irq(&qep->lock);\r\ndev_kfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void qe_set_multicast(struct net_device *dev)\r\n{\r\nstruct sunqe *qep = netdev_priv(dev);\r\nstruct netdev_hw_addr *ha;\r\nu8 new_mconfig = qep->mconfig;\r\nint i;\r\nu32 crc;\r\nnetif_stop_queue(dev);\r\nif ((dev->flags & IFF_ALLMULTI) || (netdev_mc_count(dev) > 64)) {\r\nsbus_writeb(MREGS_IACONFIG_ACHNGE | MREGS_IACONFIG_LARESET,\r\nqep->mregs + MREGS_IACONFIG);\r\nwhile ((sbus_readb(qep->mregs + MREGS_IACONFIG) & MREGS_IACONFIG_ACHNGE) != 0)\r\nbarrier();\r\nfor (i = 0; i < 8; i++)\r\nsbus_writeb(0xff, qep->mregs + MREGS_FILTER);\r\nsbus_writeb(0, qep->mregs + MREGS_IACONFIG);\r\n} else if (dev->flags & IFF_PROMISC) {\r\nnew_mconfig |= MREGS_MCONFIG_PROMISC;\r\n} else {\r\nu16 hash_table[4];\r\nu8 *hbytes = (unsigned char *) &hash_table[0];\r\nmemset(hash_table, 0, sizeof(hash_table));\r\nnetdev_for_each_mc_addr(ha, dev) {\r\ncrc = ether_crc_le(6, ha->addr);\r\ncrc >>= 26;\r\nhash_table[crc >> 4] |= 1 << (crc & 0xf);\r\n}\r\nsbus_writeb(MREGS_IACONFIG_ACHNGE | MREGS_IACONFIG_LARESET,\r\nqep->mregs + MREGS_IACONFIG);\r\nwhile ((sbus_readb(qep->mregs + MREGS_IACONFIG) & MREGS_IACONFIG_ACHNGE) != 0)\r\nbarrier();\r\nfor (i = 0; i < 8; i++) {\r\nu8 tmp = *hbytes++;\r\nsbus_writeb(tmp, qep->mregs + MREGS_FILTER);\r\n}\r\nsbus_writeb(0, qep->mregs + MREGS_IACONFIG);\r\n}\r\nqep->mconfig = new_mconfig;\r\nsbus_writeb(qep->mconfig, qep->mregs + MREGS_MCONFIG);\r\nnetif_wake_queue(dev);\r\n}\r\nstatic void qe_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\r\n{\r\nconst struct linux_prom_registers *regs;\r\nstruct sunqe *qep = netdev_priv(dev);\r\nstruct platform_device *op;\r\nstrcpy(info->driver, "sunqe");\r\nstrcpy(info->version, "3.0");\r\nop = qep->op;\r\nregs = of_get_property(op->dev.of_node, "reg", NULL);\r\nif (regs)\r\nsprintf(info->bus_info, "SBUS:%d", regs->which_io);\r\n}\r\nstatic u32 qe_get_link(struct net_device *dev)\r\n{\r\nstruct sunqe *qep = netdev_priv(dev);\r\nvoid __iomem *mregs = qep->mregs;\r\nu8 phyconfig;\r\nspin_lock_irq(&qep->lock);\r\nphyconfig = sbus_readb(mregs + MREGS_PHYCONFIG);\r\nspin_unlock_irq(&qep->lock);\r\nreturn phyconfig & MREGS_PHYCONFIG_LSTAT;\r\n}\r\nstatic void qec_init_once(struct sunqec *qecp, struct platform_device *op)\r\n{\r\nu8 bsizes = qecp->qec_bursts;\r\nif (sbus_can_burst64() && (bsizes & DMA_BURST64)) {\r\nsbus_writel(GLOB_CTRL_B64, qecp->gregs + GLOB_CTRL);\r\n} else if (bsizes & DMA_BURST32) {\r\nsbus_writel(GLOB_CTRL_B32, qecp->gregs + GLOB_CTRL);\r\n} else {\r\nsbus_writel(GLOB_CTRL_B16, qecp->gregs + GLOB_CTRL);\r\n}\r\nsbus_writel(GLOB_PSIZE_2048, qecp->gregs + GLOB_PSIZE);\r\nsbus_writel((resource_size(&op->resource[1]) >> 2),\r\nqecp->gregs + GLOB_MSIZE);\r\nsbus_writel((resource_size(&op->resource[1]) >> 2) >> 1,\r\nqecp->gregs + GLOB_TSIZE);\r\nsbus_writel((resource_size(&op->resource[1]) >> 2) >> 1,\r\nqecp->gregs + GLOB_RSIZE);\r\n}\r\nstatic u8 __devinit qec_get_burst(struct device_node *dp)\r\n{\r\nu8 bsizes, bsizes_more;\r\nbsizes = of_getintprop_default(dp, "burst-sizes", 0xff);\r\nbsizes &= 0xff;\r\nbsizes_more = of_getintprop_default(dp->parent, "burst-sizes", 0xff);\r\nif (bsizes_more != 0xff)\r\nbsizes &= bsizes_more;\r\nif (bsizes == 0xff || (bsizes & DMA_BURST16) == 0 ||\r\n(bsizes & DMA_BURST32)==0)\r\nbsizes = (DMA_BURST32 - 1);\r\nreturn bsizes;\r\n}\r\nstatic struct sunqec * __devinit get_qec(struct platform_device *child)\r\n{\r\nstruct platform_device *op = to_platform_device(child->dev.parent);\r\nstruct sunqec *qecp;\r\nqecp = dev_get_drvdata(&op->dev);\r\nif (!qecp) {\r\nqecp = kzalloc(sizeof(struct sunqec), GFP_KERNEL);\r\nif (qecp) {\r\nu32 ctrl;\r\nqecp->op = op;\r\nqecp->gregs = of_ioremap(&op->resource[0], 0,\r\nGLOB_REG_SIZE,\r\n"QEC Global Registers");\r\nif (!qecp->gregs)\r\ngoto fail;\r\nctrl = sbus_readl(qecp->gregs + GLOB_CTRL);\r\nctrl &= 0xf0000000;\r\nif (ctrl != GLOB_CTRL_MMODE) {\r\nprintk(KERN_ERR "qec: Not in MACE mode!\n");\r\ngoto fail;\r\n}\r\nif (qec_global_reset(qecp->gregs))\r\ngoto fail;\r\nqecp->qec_bursts = qec_get_burst(op->dev.of_node);\r\nqec_init_once(qecp, op);\r\nif (request_irq(op->archdata.irqs[0], qec_interrupt,\r\nIRQF_SHARED, "qec", (void *) qecp)) {\r\nprintk(KERN_ERR "qec: Can't register irq.\n");\r\ngoto fail;\r\n}\r\ndev_set_drvdata(&op->dev, qecp);\r\nqecp->next_module = root_qec_dev;\r\nroot_qec_dev = qecp;\r\n}\r\n}\r\nreturn qecp;\r\nfail:\r\nif (qecp->gregs)\r\nof_iounmap(&op->resource[0], qecp->gregs, GLOB_REG_SIZE);\r\nkfree(qecp);\r\nreturn NULL;\r\n}\r\nstatic int __devinit qec_ether_init(struct platform_device *op)\r\n{\r\nstatic unsigned version_printed;\r\nstruct net_device *dev;\r\nstruct sunqec *qecp;\r\nstruct sunqe *qe;\r\nint i, res;\r\nif (version_printed++ == 0)\r\nprintk(KERN_INFO "%s", version);\r\ndev = alloc_etherdev(sizeof(struct sunqe));\r\nif (!dev)\r\nreturn -ENOMEM;\r\nmemcpy(dev->dev_addr, idprom->id_ethaddr, 6);\r\nqe = netdev_priv(dev);\r\nres = -ENODEV;\r\ni = of_getintprop_default(op->dev.of_node, "channel#", -1);\r\nif (i == -1)\r\ngoto fail;\r\nqe->channel = i;\r\nspin_lock_init(&qe->lock);\r\nqecp = get_qec(op);\r\nif (!qecp)\r\ngoto fail;\r\nqecp->qes[qe->channel] = qe;\r\nqe->dev = dev;\r\nqe->parent = qecp;\r\nqe->op = op;\r\nres = -ENOMEM;\r\nqe->qcregs = of_ioremap(&op->resource[0], 0,\r\nCREG_REG_SIZE, "QEC Channel Registers");\r\nif (!qe->qcregs) {\r\nprintk(KERN_ERR "qe: Cannot map channel registers.\n");\r\ngoto fail;\r\n}\r\nqe->mregs = of_ioremap(&op->resource[1], 0,\r\nMREGS_REG_SIZE, "QE MACE Registers");\r\nif (!qe->mregs) {\r\nprintk(KERN_ERR "qe: Cannot map MACE registers.\n");\r\ngoto fail;\r\n}\r\nqe->qe_block = dma_alloc_coherent(&op->dev, PAGE_SIZE,\r\n&qe->qblock_dvma, GFP_ATOMIC);\r\nqe->buffers = dma_alloc_coherent(&op->dev, sizeof(struct sunqe_buffers),\r\n&qe->buffers_dvma, GFP_ATOMIC);\r\nif (qe->qe_block == NULL || qe->qblock_dvma == 0 ||\r\nqe->buffers == NULL || qe->buffers_dvma == 0)\r\ngoto fail;\r\nqe_stop(qe);\r\nSET_NETDEV_DEV(dev, &op->dev);\r\ndev->watchdog_timeo = 5*HZ;\r\ndev->irq = op->archdata.irqs[0];\r\ndev->dma = 0;\r\ndev->ethtool_ops = &qe_ethtool_ops;\r\ndev->netdev_ops = &qec_ops;\r\nres = register_netdev(dev);\r\nif (res)\r\ngoto fail;\r\ndev_set_drvdata(&op->dev, qe);\r\nprintk(KERN_INFO "%s: qe channel[%d] %pM\n", dev->name, qe->channel,\r\ndev->dev_addr);\r\nreturn 0;\r\nfail:\r\nif (qe->qcregs)\r\nof_iounmap(&op->resource[0], qe->qcregs, CREG_REG_SIZE);\r\nif (qe->mregs)\r\nof_iounmap(&op->resource[1], qe->mregs, MREGS_REG_SIZE);\r\nif (qe->qe_block)\r\ndma_free_coherent(&op->dev, PAGE_SIZE,\r\nqe->qe_block, qe->qblock_dvma);\r\nif (qe->buffers)\r\ndma_free_coherent(&op->dev,\r\nsizeof(struct sunqe_buffers),\r\nqe->buffers,\r\nqe->buffers_dvma);\r\nfree_netdev(dev);\r\nreturn res;\r\n}\r\nstatic int __devinit qec_sbus_probe(struct platform_device *op)\r\n{\r\nreturn qec_ether_init(op);\r\n}\r\nstatic int __devexit qec_sbus_remove(struct platform_device *op)\r\n{\r\nstruct sunqe *qp = dev_get_drvdata(&op->dev);\r\nstruct net_device *net_dev = qp->dev;\r\nunregister_netdev(net_dev);\r\nof_iounmap(&op->resource[0], qp->qcregs, CREG_REG_SIZE);\r\nof_iounmap(&op->resource[1], qp->mregs, MREGS_REG_SIZE);\r\ndma_free_coherent(&op->dev, PAGE_SIZE,\r\nqp->qe_block, qp->qblock_dvma);\r\ndma_free_coherent(&op->dev, sizeof(struct sunqe_buffers),\r\nqp->buffers, qp->buffers_dvma);\r\nfree_netdev(net_dev);\r\ndev_set_drvdata(&op->dev, NULL);\r\nreturn 0;\r\n}\r\nstatic int __init qec_init(void)\r\n{\r\nreturn platform_driver_register(&qec_sbus_driver);\r\n}\r\nstatic void __exit qec_exit(void)\r\n{\r\nplatform_driver_unregister(&qec_sbus_driver);\r\nwhile (root_qec_dev) {\r\nstruct sunqec *next = root_qec_dev->next_module;\r\nstruct platform_device *op = root_qec_dev->op;\r\nfree_irq(op->archdata.irqs[0], (void *) root_qec_dev);\r\nof_iounmap(&op->resource[0], root_qec_dev->gregs,\r\nGLOB_REG_SIZE);\r\nkfree(root_qec_dev);\r\nroot_qec_dev = next;\r\n}\r\n}
