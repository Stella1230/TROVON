static u64 amd_pmu_event_map(int hw_event)\r\n{\r\nreturn amd_perfmon_event_map[hw_event];\r\n}\r\nstatic int amd_pmu_hw_config(struct perf_event *event)\r\n{\r\nint ret;\r\nif (event->attr.precise_ip && get_ibs_caps())\r\nreturn -ENOENT;\r\nret = x86_pmu_hw_config(event);\r\nif (ret)\r\nreturn ret;\r\nif (has_branch_stack(event))\r\nreturn -EOPNOTSUPP;\r\nif (event->attr.exclude_host && event->attr.exclude_guest)\r\nevent->hw.config &= ~(ARCH_PERFMON_EVENTSEL_USR |\r\nARCH_PERFMON_EVENTSEL_OS);\r\nelse if (event->attr.exclude_host)\r\nevent->hw.config |= AMD_PERFMON_EVENTSEL_GUESTONLY;\r\nelse if (event->attr.exclude_guest)\r\nevent->hw.config |= AMD_PERFMON_EVENTSEL_HOSTONLY;\r\nif (event->attr.type != PERF_TYPE_RAW)\r\nreturn 0;\r\nevent->hw.config |= event->attr.config & AMD64_RAW_EVENT_MASK;\r\nreturn 0;\r\n}\r\nstatic inline unsigned int amd_get_event_code(struct hw_perf_event *hwc)\r\n{\r\nreturn ((hwc->config >> 24) & 0x0f00) | (hwc->config & 0x00ff);\r\n}\r\nstatic inline int amd_is_nb_event(struct hw_perf_event *hwc)\r\n{\r\nreturn (hwc->config & 0xe0) == 0xe0;\r\n}\r\nstatic inline int amd_has_nb(struct cpu_hw_events *cpuc)\r\n{\r\nstruct amd_nb *nb = cpuc->amd_nb;\r\nreturn nb && nb->nb_id != -1;\r\n}\r\nstatic void amd_put_event_constraints(struct cpu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct amd_nb *nb = cpuc->amd_nb;\r\nint i;\r\nif (!(amd_has_nb(cpuc) && amd_is_nb_event(hwc)))\r\nreturn;\r\nfor (i = 0; i < x86_pmu.num_counters; i++) {\r\nif (cmpxchg(nb->owners + i, event, NULL) == event)\r\nbreak;\r\n}\r\n}\r\nstatic struct event_constraint *\r\namd_get_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct amd_nb *nb = cpuc->amd_nb;\r\nstruct perf_event *old = NULL;\r\nint max = x86_pmu.num_counters;\r\nint i, j, k = -1;\r\nif (!(amd_has_nb(cpuc) && amd_is_nb_event(hwc)))\r\nreturn &unconstrained;\r\nfor (i = 0; i < max; i++) {\r\nif (k == -1 && !nb->owners[i])\r\nk = i;\r\nif (nb->owners[i] == event)\r\ngoto done;\r\n}\r\nif (hwc->idx != -1) {\r\ni = hwc->idx;\r\n} else if (k != -1) {\r\ni = k;\r\n} else {\r\ni = 0;\r\n}\r\nj = i;\r\ndo {\r\nold = cmpxchg(nb->owners+i, NULL, event);\r\nif (!old)\r\nbreak;\r\nif (++i == max)\r\ni = 0;\r\n} while (i != j);\r\ndone:\r\nif (!old)\r\nreturn &nb->event_constraints[i];\r\nreturn &emptyconstraint;\r\n}\r\nstatic struct amd_nb *amd_alloc_nb(int cpu)\r\n{\r\nstruct amd_nb *nb;\r\nint i;\r\nnb = kmalloc_node(sizeof(struct amd_nb), GFP_KERNEL | __GFP_ZERO,\r\ncpu_to_node(cpu));\r\nif (!nb)\r\nreturn NULL;\r\nnb->nb_id = -1;\r\nfor (i = 0; i < x86_pmu.num_counters; i++) {\r\n__set_bit(i, nb->event_constraints[i].idxmsk);\r\nnb->event_constraints[i].weight = 1;\r\n}\r\nreturn nb;\r\n}\r\nstatic int amd_pmu_cpu_prepare(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\r\nWARN_ON_ONCE(cpuc->amd_nb);\r\nif (boot_cpu_data.x86_max_cores < 2)\r\nreturn NOTIFY_OK;\r\ncpuc->amd_nb = amd_alloc_nb(cpu);\r\nif (!cpuc->amd_nb)\r\nreturn NOTIFY_BAD;\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void amd_pmu_cpu_starting(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\r\nstruct amd_nb *nb;\r\nint i, nb_id;\r\ncpuc->perf_ctr_virt_mask = AMD_PERFMON_EVENTSEL_HOSTONLY;\r\nif (boot_cpu_data.x86_max_cores < 2)\r\nreturn;\r\nnb_id = amd_get_nb_id(cpu);\r\nWARN_ON_ONCE(nb_id == BAD_APICID);\r\nfor_each_online_cpu(i) {\r\nnb = per_cpu(cpu_hw_events, i).amd_nb;\r\nif (WARN_ON_ONCE(!nb))\r\ncontinue;\r\nif (nb->nb_id == nb_id) {\r\ncpuc->kfree_on_online = cpuc->amd_nb;\r\ncpuc->amd_nb = nb;\r\nbreak;\r\n}\r\n}\r\ncpuc->amd_nb->nb_id = nb_id;\r\ncpuc->amd_nb->refcnt++;\r\n}\r\nstatic void amd_pmu_cpu_dead(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nif (boot_cpu_data.x86_max_cores < 2)\r\nreturn;\r\ncpuhw = &per_cpu(cpu_hw_events, cpu);\r\nif (cpuhw->amd_nb) {\r\nstruct amd_nb *nb = cpuhw->amd_nb;\r\nif (nb->nb_id == -1 || --nb->refcnt == 0)\r\nkfree(nb);\r\ncpuhw->amd_nb = NULL;\r\n}\r\n}\r\nstatic struct event_constraint *\r\namd_get_event_constraints_f15h(struct cpu_hw_events *cpuc, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nunsigned int event_code = amd_get_event_code(hwc);\r\nswitch (event_code & AMD_EVENT_TYPE_MASK) {\r\ncase AMD_EVENT_FP:\r\nswitch (event_code) {\r\ncase 0x000:\r\nif (!(hwc->config & 0x0000F000ULL))\r\nbreak;\r\nif (!(hwc->config & 0x00000F00ULL))\r\nbreak;\r\nreturn &amd_f15_PMC3;\r\ncase 0x004:\r\nif (hweight_long(hwc->config & ARCH_PERFMON_EVENTSEL_UMASK) <= 1)\r\nbreak;\r\nreturn &amd_f15_PMC3;\r\ncase 0x003:\r\ncase 0x00B:\r\ncase 0x00D:\r\nreturn &amd_f15_PMC3;\r\n}\r\nreturn &amd_f15_PMC53;\r\ncase AMD_EVENT_LS:\r\ncase AMD_EVENT_DC:\r\ncase AMD_EVENT_EX_LS:\r\nswitch (event_code) {\r\ncase 0x023:\r\ncase 0x043:\r\ncase 0x045:\r\ncase 0x046:\r\ncase 0x054:\r\ncase 0x055:\r\nreturn &amd_f15_PMC20;\r\ncase 0x02D:\r\nreturn &amd_f15_PMC3;\r\ncase 0x02E:\r\nreturn &amd_f15_PMC30;\r\ncase 0x031:\r\nif (hweight_long(hwc->config & ARCH_PERFMON_EVENTSEL_UMASK) <= 1)\r\nreturn &amd_f15_PMC20;\r\nreturn &emptyconstraint;\r\ncase 0x1C0:\r\nreturn &amd_f15_PMC53;\r\ndefault:\r\nreturn &amd_f15_PMC50;\r\n}\r\ncase AMD_EVENT_CU:\r\ncase AMD_EVENT_IC_DE:\r\ncase AMD_EVENT_DE:\r\nswitch (event_code) {\r\ncase 0x08F:\r\ncase 0x187:\r\ncase 0x188:\r\nreturn &amd_f15_PMC0;\r\ncase 0x0DB ... 0x0DF:\r\ncase 0x1D6:\r\ncase 0x1D8:\r\nreturn &amd_f15_PMC50;\r\ndefault:\r\nreturn &amd_f15_PMC20;\r\n}\r\ncase AMD_EVENT_NB:\r\nreturn &emptyconstraint;\r\ndefault:\r\nreturn &emptyconstraint;\r\n}\r\n}\r\nstatic int setup_event_constraints(void)\r\n{\r\nif (boot_cpu_data.x86 >= 0x15)\r\nx86_pmu.get_event_constraints = amd_get_event_constraints_f15h;\r\nreturn 0;\r\n}\r\nstatic int setup_perfctr_core(void)\r\n{\r\nif (!cpu_has_perfctr_core) {\r\nWARN(x86_pmu.get_event_constraints == amd_get_event_constraints_f15h,\r\nKERN_ERR "Odd, counter constraints enabled but no core perfctrs detected!");\r\nreturn -ENODEV;\r\n}\r\nWARN(x86_pmu.get_event_constraints == amd_get_event_constraints,\r\nKERN_ERR "hw perf events core counters need constraints handler!");\r\nx86_pmu.eventsel = MSR_F15H_PERF_CTL;\r\nx86_pmu.perfctr = MSR_F15H_PERF_CTR;\r\nx86_pmu.num_counters = AMD64_NUM_COUNTERS_CORE;\r\nprintk(KERN_INFO "perf: AMD core performance counters detected\n");\r\nreturn 0;\r\n}\r\n__init int amd_pmu_init(void)\r\n{\r\nif (boot_cpu_data.x86 < 6)\r\nreturn -ENODEV;\r\nx86_pmu = amd_pmu;\r\nsetup_event_constraints();\r\nsetup_perfctr_core();\r\nmemcpy(hw_cache_event_ids, amd_hw_cache_event_ids,\r\nsizeof(hw_cache_event_ids));\r\nreturn 0;\r\n}\r\nvoid amd_pmu_enable_virt(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\ncpuc->perf_ctr_virt_mask = 0;\r\nx86_pmu_disable_all();\r\nx86_pmu_enable_all(0);\r\n}\r\nvoid amd_pmu_disable_virt(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\ncpuc->perf_ctr_virt_mask = AMD_PERFMON_EVENTSEL_HOSTONLY;\r\nx86_pmu_disable_all();\r\nx86_pmu_enable_all(0);\r\n}
