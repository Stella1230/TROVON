void update_rlimit_cpu(struct task_struct *task, unsigned long rlim_new)\r\n{\r\ncputime_t cputime = secs_to_cputime(rlim_new);\r\nspin_lock_irq(&task->sighand->siglock);\r\nset_process_cpu_timer(task, CPUCLOCK_PROF, &cputime, NULL);\r\nspin_unlock_irq(&task->sighand->siglock);\r\n}\r\nstatic int check_clock(const clockid_t which_clock)\r\n{\r\nint error = 0;\r\nstruct task_struct *p;\r\nconst pid_t pid = CPUCLOCK_PID(which_clock);\r\nif (CPUCLOCK_WHICH(which_clock) >= CPUCLOCK_MAX)\r\nreturn -EINVAL;\r\nif (pid == 0)\r\nreturn 0;\r\nrcu_read_lock();\r\np = find_task_by_vpid(pid);\r\nif (!p || !(CPUCLOCK_PERTHREAD(which_clock) ?\r\nsame_thread_group(p, current) : has_group_leader_pid(p))) {\r\nerror = -EINVAL;\r\n}\r\nrcu_read_unlock();\r\nreturn error;\r\n}\r\nstatic inline union cpu_time_count\r\ntimespec_to_sample(const clockid_t which_clock, const struct timespec *tp)\r\n{\r\nunion cpu_time_count ret;\r\nret.sched = 0;\r\nif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\r\nret.sched = (unsigned long long)tp->tv_sec * NSEC_PER_SEC + tp->tv_nsec;\r\n} else {\r\nret.cpu = timespec_to_cputime(tp);\r\n}\r\nreturn ret;\r\n}\r\nstatic void sample_to_timespec(const clockid_t which_clock,\r\nunion cpu_time_count cpu,\r\nstruct timespec *tp)\r\n{\r\nif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED)\r\n*tp = ns_to_timespec(cpu.sched);\r\nelse\r\ncputime_to_timespec(cpu.cpu, tp);\r\n}\r\nstatic inline int cpu_time_before(const clockid_t which_clock,\r\nunion cpu_time_count now,\r\nunion cpu_time_count then)\r\n{\r\nif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\r\nreturn now.sched < then.sched;\r\n} else {\r\nreturn now.cpu < then.cpu;\r\n}\r\n}\r\nstatic inline void cpu_time_add(const clockid_t which_clock,\r\nunion cpu_time_count *acc,\r\nunion cpu_time_count val)\r\n{\r\nif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\r\nacc->sched += val.sched;\r\n} else {\r\nacc->cpu += val.cpu;\r\n}\r\n}\r\nstatic inline union cpu_time_count cpu_time_sub(const clockid_t which_clock,\r\nunion cpu_time_count a,\r\nunion cpu_time_count b)\r\n{\r\nif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\r\na.sched -= b.sched;\r\n} else {\r\na.cpu -= b.cpu;\r\n}\r\nreturn a;\r\n}\r\nstatic void bump_cpu_timer(struct k_itimer *timer,\r\nunion cpu_time_count now)\r\n{\r\nint i;\r\nif (timer->it.cpu.incr.sched == 0)\r\nreturn;\r\nif (CPUCLOCK_WHICH(timer->it_clock) == CPUCLOCK_SCHED) {\r\nunsigned long long delta, incr;\r\nif (now.sched < timer->it.cpu.expires.sched)\r\nreturn;\r\nincr = timer->it.cpu.incr.sched;\r\ndelta = now.sched + incr - timer->it.cpu.expires.sched;\r\nfor (i = 0; incr < delta - incr; i++)\r\nincr = incr << 1;\r\nfor (; i >= 0; incr >>= 1, i--) {\r\nif (delta < incr)\r\ncontinue;\r\ntimer->it.cpu.expires.sched += incr;\r\ntimer->it_overrun += 1 << i;\r\ndelta -= incr;\r\n}\r\n} else {\r\ncputime_t delta, incr;\r\nif (now.cpu < timer->it.cpu.expires.cpu)\r\nreturn;\r\nincr = timer->it.cpu.incr.cpu;\r\ndelta = now.cpu + incr - timer->it.cpu.expires.cpu;\r\nfor (i = 0; incr < delta - incr; i++)\r\nincr += incr;\r\nfor (; i >= 0; incr = incr >> 1, i--) {\r\nif (delta < incr)\r\ncontinue;\r\ntimer->it.cpu.expires.cpu += incr;\r\ntimer->it_overrun += 1 << i;\r\ndelta -= incr;\r\n}\r\n}\r\n}\r\nstatic inline cputime_t prof_ticks(struct task_struct *p)\r\n{\r\nreturn p->utime + p->stime;\r\n}\r\nstatic inline cputime_t virt_ticks(struct task_struct *p)\r\n{\r\nreturn p->utime;\r\n}\r\nstatic int\r\nposix_cpu_clock_getres(const clockid_t which_clock, struct timespec *tp)\r\n{\r\nint error = check_clock(which_clock);\r\nif (!error) {\r\ntp->tv_sec = 0;\r\ntp->tv_nsec = ((NSEC_PER_SEC + HZ - 1) / HZ);\r\nif (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {\r\ntp->tv_nsec = 1;\r\n}\r\n}\r\nreturn error;\r\n}\r\nstatic int\r\nposix_cpu_clock_set(const clockid_t which_clock, const struct timespec *tp)\r\n{\r\nint error = check_clock(which_clock);\r\nif (error == 0) {\r\nerror = -EPERM;\r\n}\r\nreturn error;\r\n}\r\nstatic int cpu_clock_sample(const clockid_t which_clock, struct task_struct *p,\r\nunion cpu_time_count *cpu)\r\n{\r\nswitch (CPUCLOCK_WHICH(which_clock)) {\r\ndefault:\r\nreturn -EINVAL;\r\ncase CPUCLOCK_PROF:\r\ncpu->cpu = prof_ticks(p);\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\ncpu->cpu = virt_ticks(p);\r\nbreak;\r\ncase CPUCLOCK_SCHED:\r\ncpu->sched = task_sched_runtime(p);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nvoid thread_group_cputime(struct task_struct *tsk, struct task_cputime *times)\r\n{\r\nstruct signal_struct *sig = tsk->signal;\r\nstruct task_struct *t;\r\ntimes->utime = sig->utime;\r\ntimes->stime = sig->stime;\r\ntimes->sum_exec_runtime = sig->sum_sched_runtime;\r\nrcu_read_lock();\r\nif (!likely(pid_alive(tsk)))\r\ngoto out;\r\nt = tsk;\r\ndo {\r\ntimes->utime += t->utime;\r\ntimes->stime += t->stime;\r\ntimes->sum_exec_runtime += task_sched_runtime(t);\r\n} while_each_thread(tsk, t);\r\nvoid update_gt_cputime(struct task_cputime *a, struct task_cputime *b)\r\n{\r\nif (b->utime > a->utime)\r\na->utime = b->utime;\r\nif (b->stime > a->stime)\r\na->stime = b->stime;\r\nif (b->sum_exec_runtime > a->sum_exec_runtime)\r\na->sum_exec_runtime = b->sum_exec_runtime;\r\n}\r\nvoid thread_group_cputimer(struct task_struct *tsk, struct task_cputime *times)\r\n{\r\nstruct thread_group_cputimer *cputimer = &tsk->signal->cputimer;\r\nstruct task_cputime sum;\r\nunsigned long flags;\r\nif (!cputimer->running) {\r\nthread_group_cputime(tsk, &sum);\r\nraw_spin_lock_irqsave(&cputimer->lock, flags);\r\ncputimer->running = 1;\r\nupdate_gt_cputime(&cputimer->cputime, &sum);\r\n} else\r\nraw_spin_lock_irqsave(&cputimer->lock, flags);\r\n*times = cputimer->cputime;\r\nraw_spin_unlock_irqrestore(&cputimer->lock, flags);\r\n}\r\nstatic int cpu_clock_sample_group(const clockid_t which_clock,\r\nstruct task_struct *p,\r\nunion cpu_time_count *cpu)\r\n{\r\nstruct task_cputime cputime;\r\nswitch (CPUCLOCK_WHICH(which_clock)) {\r\ndefault:\r\nreturn -EINVAL;\r\ncase CPUCLOCK_PROF:\r\nthread_group_cputime(p, &cputime);\r\ncpu->cpu = cputime.utime + cputime.stime;\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\nthread_group_cputime(p, &cputime);\r\ncpu->cpu = cputime.utime;\r\nbreak;\r\ncase CPUCLOCK_SCHED:\r\nthread_group_cputime(p, &cputime);\r\ncpu->sched = cputime.sum_exec_runtime;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int posix_cpu_clock_get(const clockid_t which_clock, struct timespec *tp)\r\n{\r\nconst pid_t pid = CPUCLOCK_PID(which_clock);\r\nint error = -EINVAL;\r\nunion cpu_time_count rtn;\r\nif (pid == 0) {\r\nif (CPUCLOCK_PERTHREAD(which_clock)) {\r\nerror = cpu_clock_sample(which_clock,\r\ncurrent, &rtn);\r\n} else {\r\nread_lock(&tasklist_lock);\r\nerror = cpu_clock_sample_group(which_clock,\r\ncurrent, &rtn);\r\nread_unlock(&tasklist_lock);\r\n}\r\n} else {\r\nstruct task_struct *p;\r\nrcu_read_lock();\r\np = find_task_by_vpid(pid);\r\nif (p) {\r\nif (CPUCLOCK_PERTHREAD(which_clock)) {\r\nif (same_thread_group(p, current)) {\r\nerror = cpu_clock_sample(which_clock,\r\np, &rtn);\r\n}\r\n} else {\r\nread_lock(&tasklist_lock);\r\nif (thread_group_leader(p) && p->sighand) {\r\nerror =\r\ncpu_clock_sample_group(which_clock,\r\np, &rtn);\r\n}\r\nread_unlock(&tasklist_lock);\r\n}\r\n}\r\nrcu_read_unlock();\r\n}\r\nif (error)\r\nreturn error;\r\nsample_to_timespec(which_clock, rtn, tp);\r\nreturn 0;\r\n}\r\nstatic int posix_cpu_timer_create(struct k_itimer *new_timer)\r\n{\r\nint ret = 0;\r\nconst pid_t pid = CPUCLOCK_PID(new_timer->it_clock);\r\nstruct task_struct *p;\r\nif (CPUCLOCK_WHICH(new_timer->it_clock) >= CPUCLOCK_MAX)\r\nreturn -EINVAL;\r\nINIT_LIST_HEAD(&new_timer->it.cpu.entry);\r\nrcu_read_lock();\r\nif (CPUCLOCK_PERTHREAD(new_timer->it_clock)) {\r\nif (pid == 0) {\r\np = current;\r\n} else {\r\np = find_task_by_vpid(pid);\r\nif (p && !same_thread_group(p, current))\r\np = NULL;\r\n}\r\n} else {\r\nif (pid == 0) {\r\np = current->group_leader;\r\n} else {\r\np = find_task_by_vpid(pid);\r\nif (p && !has_group_leader_pid(p))\r\np = NULL;\r\n}\r\n}\r\nnew_timer->it.cpu.task = p;\r\nif (p) {\r\nget_task_struct(p);\r\n} else {\r\nret = -EINVAL;\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int posix_cpu_timer_del(struct k_itimer *timer)\r\n{\r\nstruct task_struct *p = timer->it.cpu.task;\r\nint ret = 0;\r\nif (likely(p != NULL)) {\r\nread_lock(&tasklist_lock);\r\nif (unlikely(p->sighand == NULL)) {\r\nBUG_ON(!list_empty(&timer->it.cpu.entry));\r\n} else {\r\nspin_lock(&p->sighand->siglock);\r\nif (timer->it.cpu.firing)\r\nret = TIMER_RETRY;\r\nelse\r\nlist_del(&timer->it.cpu.entry);\r\nspin_unlock(&p->sighand->siglock);\r\n}\r\nread_unlock(&tasklist_lock);\r\nif (!ret)\r\nput_task_struct(p);\r\n}\r\nreturn ret;\r\n}\r\nstatic void cleanup_timers(struct list_head *head,\r\ncputime_t utime, cputime_t stime,\r\nunsigned long long sum_exec_runtime)\r\n{\r\nstruct cpu_timer_list *timer, *next;\r\ncputime_t ptime = utime + stime;\r\nlist_for_each_entry_safe(timer, next, head, entry) {\r\nlist_del_init(&timer->entry);\r\nif (timer->expires.cpu < ptime) {\r\ntimer->expires.cpu = 0;\r\n} else {\r\ntimer->expires.cpu -= ptime;\r\n}\r\n}\r\n++head;\r\nlist_for_each_entry_safe(timer, next, head, entry) {\r\nlist_del_init(&timer->entry);\r\nif (timer->expires.cpu < utime) {\r\ntimer->expires.cpu = 0;\r\n} else {\r\ntimer->expires.cpu -= utime;\r\n}\r\n}\r\n++head;\r\nlist_for_each_entry_safe(timer, next, head, entry) {\r\nlist_del_init(&timer->entry);\r\nif (timer->expires.sched < sum_exec_runtime) {\r\ntimer->expires.sched = 0;\r\n} else {\r\ntimer->expires.sched -= sum_exec_runtime;\r\n}\r\n}\r\n}\r\nvoid posix_cpu_timers_exit(struct task_struct *tsk)\r\n{\r\ncleanup_timers(tsk->cpu_timers,\r\ntsk->utime, tsk->stime, tsk->se.sum_exec_runtime);\r\n}\r\nvoid posix_cpu_timers_exit_group(struct task_struct *tsk)\r\n{\r\nstruct signal_struct *const sig = tsk->signal;\r\ncleanup_timers(tsk->signal->cpu_timers,\r\ntsk->utime + sig->utime, tsk->stime + sig->stime,\r\ntsk->se.sum_exec_runtime + sig->sum_sched_runtime);\r\n}\r\nstatic void clear_dead_task(struct k_itimer *timer, union cpu_time_count now)\r\n{\r\nput_task_struct(timer->it.cpu.task);\r\ntimer->it.cpu.task = NULL;\r\ntimer->it.cpu.expires = cpu_time_sub(timer->it_clock,\r\ntimer->it.cpu.expires,\r\nnow);\r\n}\r\nstatic inline int expires_gt(cputime_t expires, cputime_t new_exp)\r\n{\r\nreturn expires == 0 || expires > new_exp;\r\n}\r\nstatic void arm_timer(struct k_itimer *timer)\r\n{\r\nstruct task_struct *p = timer->it.cpu.task;\r\nstruct list_head *head, *listpos;\r\nstruct task_cputime *cputime_expires;\r\nstruct cpu_timer_list *const nt = &timer->it.cpu;\r\nstruct cpu_timer_list *next;\r\nif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\r\nhead = p->cpu_timers;\r\ncputime_expires = &p->cputime_expires;\r\n} else {\r\nhead = p->signal->cpu_timers;\r\ncputime_expires = &p->signal->cputime_expires;\r\n}\r\nhead += CPUCLOCK_WHICH(timer->it_clock);\r\nlistpos = head;\r\nlist_for_each_entry(next, head, entry) {\r\nif (cpu_time_before(timer->it_clock, nt->expires, next->expires))\r\nbreak;\r\nlistpos = &next->entry;\r\n}\r\nlist_add(&nt->entry, listpos);\r\nif (listpos == head) {\r\nunion cpu_time_count *exp = &nt->expires;\r\nswitch (CPUCLOCK_WHICH(timer->it_clock)) {\r\ncase CPUCLOCK_PROF:\r\nif (expires_gt(cputime_expires->prof_exp, exp->cpu))\r\ncputime_expires->prof_exp = exp->cpu;\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\nif (expires_gt(cputime_expires->virt_exp, exp->cpu))\r\ncputime_expires->virt_exp = exp->cpu;\r\nbreak;\r\ncase CPUCLOCK_SCHED:\r\nif (cputime_expires->sched_exp == 0 ||\r\ncputime_expires->sched_exp > exp->sched)\r\ncputime_expires->sched_exp = exp->sched;\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void cpu_timer_fire(struct k_itimer *timer)\r\n{\r\nif ((timer->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE) {\r\ntimer->it.cpu.expires.sched = 0;\r\n} else if (unlikely(timer->sigq == NULL)) {\r\nwake_up_process(timer->it_process);\r\ntimer->it.cpu.expires.sched = 0;\r\n} else if (timer->it.cpu.incr.sched == 0) {\r\nposix_timer_event(timer, 0);\r\ntimer->it.cpu.expires.sched = 0;\r\n} else if (posix_timer_event(timer, ++timer->it_requeue_pending)) {\r\nposix_cpu_timer_schedule(timer);\r\n}\r\n}\r\nstatic int cpu_timer_sample_group(const clockid_t which_clock,\r\nstruct task_struct *p,\r\nunion cpu_time_count *cpu)\r\n{\r\nstruct task_cputime cputime;\r\nthread_group_cputimer(p, &cputime);\r\nswitch (CPUCLOCK_WHICH(which_clock)) {\r\ndefault:\r\nreturn -EINVAL;\r\ncase CPUCLOCK_PROF:\r\ncpu->cpu = cputime.utime + cputime.stime;\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\ncpu->cpu = cputime.utime;\r\nbreak;\r\ncase CPUCLOCK_SCHED:\r\ncpu->sched = cputime.sum_exec_runtime + task_delta_exec(p);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int posix_cpu_timer_set(struct k_itimer *timer, int flags,\r\nstruct itimerspec *new, struct itimerspec *old)\r\n{\r\nstruct task_struct *p = timer->it.cpu.task;\r\nunion cpu_time_count old_expires, new_expires, old_incr, val;\r\nint ret;\r\nif (unlikely(p == NULL)) {\r\nreturn -ESRCH;\r\n}\r\nnew_expires = timespec_to_sample(timer->it_clock, &new->it_value);\r\nread_lock(&tasklist_lock);\r\nif (unlikely(p->sighand == NULL)) {\r\nread_unlock(&tasklist_lock);\r\nput_task_struct(p);\r\ntimer->it.cpu.task = NULL;\r\nreturn -ESRCH;\r\n}\r\nBUG_ON(!irqs_disabled());\r\nret = 0;\r\nold_incr = timer->it.cpu.incr;\r\nspin_lock(&p->sighand->siglock);\r\nold_expires = timer->it.cpu.expires;\r\nif (unlikely(timer->it.cpu.firing)) {\r\ntimer->it.cpu.firing = -1;\r\nret = TIMER_RETRY;\r\n} else\r\nlist_del_init(&timer->it.cpu.entry);\r\nif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\r\ncpu_clock_sample(timer->it_clock, p, &val);\r\n} else {\r\ncpu_timer_sample_group(timer->it_clock, p, &val);\r\n}\r\nif (old) {\r\nif (old_expires.sched == 0) {\r\nold->it_value.tv_sec = 0;\r\nold->it_value.tv_nsec = 0;\r\n} else {\r\nbump_cpu_timer(timer, val);\r\nif (cpu_time_before(timer->it_clock, val,\r\ntimer->it.cpu.expires)) {\r\nold_expires = cpu_time_sub(\r\ntimer->it_clock,\r\ntimer->it.cpu.expires, val);\r\nsample_to_timespec(timer->it_clock,\r\nold_expires,\r\n&old->it_value);\r\n} else {\r\nold->it_value.tv_nsec = 1;\r\nold->it_value.tv_sec = 0;\r\n}\r\n}\r\n}\r\nif (unlikely(ret)) {\r\nspin_unlock(&p->sighand->siglock);\r\nread_unlock(&tasklist_lock);\r\ngoto out;\r\n}\r\nif (new_expires.sched != 0 && !(flags & TIMER_ABSTIME)) {\r\ncpu_time_add(timer->it_clock, &new_expires, val);\r\n}\r\ntimer->it.cpu.expires = new_expires;\r\nif (new_expires.sched != 0 &&\r\ncpu_time_before(timer->it_clock, val, new_expires)) {\r\narm_timer(timer);\r\n}\r\nspin_unlock(&p->sighand->siglock);\r\nread_unlock(&tasklist_lock);\r\ntimer->it.cpu.incr = timespec_to_sample(timer->it_clock,\r\n&new->it_interval);\r\ntimer->it_requeue_pending = (timer->it_requeue_pending + 2) &\r\n~REQUEUE_PENDING;\r\ntimer->it_overrun_last = 0;\r\ntimer->it_overrun = -1;\r\nif (new_expires.sched != 0 &&\r\n!cpu_time_before(timer->it_clock, val, new_expires)) {\r\ncpu_timer_fire(timer);\r\n}\r\nret = 0;\r\nout:\r\nif (old) {\r\nsample_to_timespec(timer->it_clock,\r\nold_incr, &old->it_interval);\r\n}\r\nreturn ret;\r\n}\r\nstatic void posix_cpu_timer_get(struct k_itimer *timer, struct itimerspec *itp)\r\n{\r\nunion cpu_time_count now;\r\nstruct task_struct *p = timer->it.cpu.task;\r\nint clear_dead;\r\nsample_to_timespec(timer->it_clock,\r\ntimer->it.cpu.incr, &itp->it_interval);\r\nif (timer->it.cpu.expires.sched == 0) {\r\nitp->it_value.tv_sec = itp->it_value.tv_nsec = 0;\r\nreturn;\r\n}\r\nif (unlikely(p == NULL)) {\r\ndead:\r\nsample_to_timespec(timer->it_clock, timer->it.cpu.expires,\r\n&itp->it_value);\r\nreturn;\r\n}\r\nif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\r\ncpu_clock_sample(timer->it_clock, p, &now);\r\nclear_dead = p->exit_state;\r\n} else {\r\nread_lock(&tasklist_lock);\r\nif (unlikely(p->sighand == NULL)) {\r\nput_task_struct(p);\r\ntimer->it.cpu.task = NULL;\r\ntimer->it.cpu.expires.sched = 0;\r\nread_unlock(&tasklist_lock);\r\ngoto dead;\r\n} else {\r\ncpu_timer_sample_group(timer->it_clock, p, &now);\r\nclear_dead = (unlikely(p->exit_state) &&\r\nthread_group_empty(p));\r\n}\r\nread_unlock(&tasklist_lock);\r\n}\r\nif (unlikely(clear_dead)) {\r\nclear_dead_task(timer, now);\r\ngoto dead;\r\n}\r\nif (cpu_time_before(timer->it_clock, now, timer->it.cpu.expires)) {\r\nsample_to_timespec(timer->it_clock,\r\ncpu_time_sub(timer->it_clock,\r\ntimer->it.cpu.expires, now),\r\n&itp->it_value);\r\n} else {\r\nitp->it_value.tv_nsec = 1;\r\nitp->it_value.tv_sec = 0;\r\n}\r\n}\r\nstatic void check_thread_timers(struct task_struct *tsk,\r\nstruct list_head *firing)\r\n{\r\nint maxfire;\r\nstruct list_head *timers = tsk->cpu_timers;\r\nstruct signal_struct *const sig = tsk->signal;\r\nunsigned long soft;\r\nmaxfire = 20;\r\ntsk->cputime_expires.prof_exp = 0;\r\nwhile (!list_empty(timers)) {\r\nstruct cpu_timer_list *t = list_first_entry(timers,\r\nstruct cpu_timer_list,\r\nentry);\r\nif (!--maxfire || prof_ticks(tsk) < t->expires.cpu) {\r\ntsk->cputime_expires.prof_exp = t->expires.cpu;\r\nbreak;\r\n}\r\nt->firing = 1;\r\nlist_move_tail(&t->entry, firing);\r\n}\r\n++timers;\r\nmaxfire = 20;\r\ntsk->cputime_expires.virt_exp = 0;\r\nwhile (!list_empty(timers)) {\r\nstruct cpu_timer_list *t = list_first_entry(timers,\r\nstruct cpu_timer_list,\r\nentry);\r\nif (!--maxfire || virt_ticks(tsk) < t->expires.cpu) {\r\ntsk->cputime_expires.virt_exp = t->expires.cpu;\r\nbreak;\r\n}\r\nt->firing = 1;\r\nlist_move_tail(&t->entry, firing);\r\n}\r\n++timers;\r\nmaxfire = 20;\r\ntsk->cputime_expires.sched_exp = 0;\r\nwhile (!list_empty(timers)) {\r\nstruct cpu_timer_list *t = list_first_entry(timers,\r\nstruct cpu_timer_list,\r\nentry);\r\nif (!--maxfire || tsk->se.sum_exec_runtime < t->expires.sched) {\r\ntsk->cputime_expires.sched_exp = t->expires.sched;\r\nbreak;\r\n}\r\nt->firing = 1;\r\nlist_move_tail(&t->entry, firing);\r\n}\r\nsoft = ACCESS_ONCE(sig->rlim[RLIMIT_RTTIME].rlim_cur);\r\nif (soft != RLIM_INFINITY) {\r\nunsigned long hard =\r\nACCESS_ONCE(sig->rlim[RLIMIT_RTTIME].rlim_max);\r\nif (hard != RLIM_INFINITY &&\r\ntsk->rt.timeout > DIV_ROUND_UP(hard, USEC_PER_SEC/HZ)) {\r\n__group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);\r\nreturn;\r\n}\r\nif (tsk->rt.timeout > DIV_ROUND_UP(soft, USEC_PER_SEC/HZ)) {\r\nif (soft < hard) {\r\nsoft += USEC_PER_SEC;\r\nsig->rlim[RLIMIT_RTTIME].rlim_cur = soft;\r\n}\r\nprintk(KERN_INFO\r\n"RT Watchdog Timeout: %s[%d]\n",\r\ntsk->comm, task_pid_nr(tsk));\r\n__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\r\n}\r\n}\r\n}\r\nstatic void stop_process_timers(struct signal_struct *sig)\r\n{\r\nstruct thread_group_cputimer *cputimer = &sig->cputimer;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&cputimer->lock, flags);\r\ncputimer->running = 0;\r\nraw_spin_unlock_irqrestore(&cputimer->lock, flags);\r\n}\r\nstatic void check_cpu_itimer(struct task_struct *tsk, struct cpu_itimer *it,\r\ncputime_t *expires, cputime_t cur_time, int signo)\r\n{\r\nif (!it->expires)\r\nreturn;\r\nif (cur_time >= it->expires) {\r\nif (it->incr) {\r\nit->expires += it->incr;\r\nit->error += it->incr_error;\r\nif (it->error >= onecputick) {\r\nit->expires -= cputime_one_jiffy;\r\nit->error -= onecputick;\r\n}\r\n} else {\r\nit->expires = 0;\r\n}\r\ntrace_itimer_expire(signo == SIGPROF ?\r\nITIMER_PROF : ITIMER_VIRTUAL,\r\ntsk->signal->leader_pid, cur_time);\r\n__group_send_sig_info(signo, SEND_SIG_PRIV, tsk);\r\n}\r\nif (it->expires && (!*expires || it->expires < *expires)) {\r\n*expires = it->expires;\r\n}\r\n}\r\nstatic inline int task_cputime_zero(const struct task_cputime *cputime)\r\n{\r\nif (!cputime->utime && !cputime->stime && !cputime->sum_exec_runtime)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void check_process_timers(struct task_struct *tsk,\r\nstruct list_head *firing)\r\n{\r\nint maxfire;\r\nstruct signal_struct *const sig = tsk->signal;\r\ncputime_t utime, ptime, virt_expires, prof_expires;\r\nunsigned long long sum_sched_runtime, sched_expires;\r\nstruct list_head *timers = sig->cpu_timers;\r\nstruct task_cputime cputime;\r\nunsigned long soft;\r\nthread_group_cputimer(tsk, &cputime);\r\nutime = cputime.utime;\r\nptime = utime + cputime.stime;\r\nsum_sched_runtime = cputime.sum_exec_runtime;\r\nmaxfire = 20;\r\nprof_expires = 0;\r\nwhile (!list_empty(timers)) {\r\nstruct cpu_timer_list *tl = list_first_entry(timers,\r\nstruct cpu_timer_list,\r\nentry);\r\nif (!--maxfire || ptime < tl->expires.cpu) {\r\nprof_expires = tl->expires.cpu;\r\nbreak;\r\n}\r\ntl->firing = 1;\r\nlist_move_tail(&tl->entry, firing);\r\n}\r\n++timers;\r\nmaxfire = 20;\r\nvirt_expires = 0;\r\nwhile (!list_empty(timers)) {\r\nstruct cpu_timer_list *tl = list_first_entry(timers,\r\nstruct cpu_timer_list,\r\nentry);\r\nif (!--maxfire || utime < tl->expires.cpu) {\r\nvirt_expires = tl->expires.cpu;\r\nbreak;\r\n}\r\ntl->firing = 1;\r\nlist_move_tail(&tl->entry, firing);\r\n}\r\n++timers;\r\nmaxfire = 20;\r\nsched_expires = 0;\r\nwhile (!list_empty(timers)) {\r\nstruct cpu_timer_list *tl = list_first_entry(timers,\r\nstruct cpu_timer_list,\r\nentry);\r\nif (!--maxfire || sum_sched_runtime < tl->expires.sched) {\r\nsched_expires = tl->expires.sched;\r\nbreak;\r\n}\r\ntl->firing = 1;\r\nlist_move_tail(&tl->entry, firing);\r\n}\r\ncheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_PROF], &prof_expires, ptime,\r\nSIGPROF);\r\ncheck_cpu_itimer(tsk, &sig->it[CPUCLOCK_VIRT], &virt_expires, utime,\r\nSIGVTALRM);\r\nsoft = ACCESS_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);\r\nif (soft != RLIM_INFINITY) {\r\nunsigned long psecs = cputime_to_secs(ptime);\r\nunsigned long hard =\r\nACCESS_ONCE(sig->rlim[RLIMIT_CPU].rlim_max);\r\ncputime_t x;\r\nif (psecs >= hard) {\r\n__group_send_sig_info(SIGKILL, SEND_SIG_PRIV, tsk);\r\nreturn;\r\n}\r\nif (psecs >= soft) {\r\n__group_send_sig_info(SIGXCPU, SEND_SIG_PRIV, tsk);\r\nif (soft < hard) {\r\nsoft++;\r\nsig->rlim[RLIMIT_CPU].rlim_cur = soft;\r\n}\r\n}\r\nx = secs_to_cputime(soft);\r\nif (!prof_expires || x < prof_expires) {\r\nprof_expires = x;\r\n}\r\n}\r\nsig->cputime_expires.prof_exp = prof_expires;\r\nsig->cputime_expires.virt_exp = virt_expires;\r\nsig->cputime_expires.sched_exp = sched_expires;\r\nif (task_cputime_zero(&sig->cputime_expires))\r\nstop_process_timers(sig);\r\n}\r\nvoid posix_cpu_timer_schedule(struct k_itimer *timer)\r\n{\r\nstruct task_struct *p = timer->it.cpu.task;\r\nunion cpu_time_count now;\r\nif (unlikely(p == NULL))\r\ngoto out;\r\nif (CPUCLOCK_PERTHREAD(timer->it_clock)) {\r\ncpu_clock_sample(timer->it_clock, p, &now);\r\nbump_cpu_timer(timer, now);\r\nif (unlikely(p->exit_state)) {\r\nclear_dead_task(timer, now);\r\ngoto out;\r\n}\r\nread_lock(&tasklist_lock);\r\nspin_lock(&p->sighand->siglock);\r\n} else {\r\nread_lock(&tasklist_lock);\r\nif (unlikely(p->sighand == NULL)) {\r\nput_task_struct(p);\r\ntimer->it.cpu.task = p = NULL;\r\ntimer->it.cpu.expires.sched = 0;\r\ngoto out_unlock;\r\n} else if (unlikely(p->exit_state) && thread_group_empty(p)) {\r\nclear_dead_task(timer, now);\r\ngoto out_unlock;\r\n}\r\nspin_lock(&p->sighand->siglock);\r\ncpu_timer_sample_group(timer->it_clock, p, &now);\r\nbump_cpu_timer(timer, now);\r\n}\r\nBUG_ON(!irqs_disabled());\r\narm_timer(timer);\r\nspin_unlock(&p->sighand->siglock);\r\nout_unlock:\r\nread_unlock(&tasklist_lock);\r\nout:\r\ntimer->it_overrun_last = timer->it_overrun;\r\ntimer->it_overrun = -1;\r\n++timer->it_requeue_pending;\r\n}\r\nstatic inline int task_cputime_expired(const struct task_cputime *sample,\r\nconst struct task_cputime *expires)\r\n{\r\nif (expires->utime && sample->utime >= expires->utime)\r\nreturn 1;\r\nif (expires->stime && sample->utime + sample->stime >= expires->stime)\r\nreturn 1;\r\nif (expires->sum_exec_runtime != 0 &&\r\nsample->sum_exec_runtime >= expires->sum_exec_runtime)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline int fastpath_timer_check(struct task_struct *tsk)\r\n{\r\nstruct signal_struct *sig;\r\nif (!task_cputime_zero(&tsk->cputime_expires)) {\r\nstruct task_cputime task_sample = {\r\n.utime = tsk->utime,\r\n.stime = tsk->stime,\r\n.sum_exec_runtime = tsk->se.sum_exec_runtime\r\n};\r\nif (task_cputime_expired(&task_sample, &tsk->cputime_expires))\r\nreturn 1;\r\n}\r\nsig = tsk->signal;\r\nif (sig->cputimer.running) {\r\nstruct task_cputime group_sample;\r\nraw_spin_lock(&sig->cputimer.lock);\r\ngroup_sample = sig->cputimer.cputime;\r\nraw_spin_unlock(&sig->cputimer.lock);\r\nif (task_cputime_expired(&group_sample, &sig->cputime_expires))\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid run_posix_cpu_timers(struct task_struct *tsk)\r\n{\r\nLIST_HEAD(firing);\r\nstruct k_itimer *timer, *next;\r\nunsigned long flags;\r\nBUG_ON(!irqs_disabled());\r\nif (!fastpath_timer_check(tsk))\r\nreturn;\r\nif (!lock_task_sighand(tsk, &flags))\r\nreturn;\r\ncheck_thread_timers(tsk, &firing);\r\nif (tsk->signal->cputimer.running)\r\ncheck_process_timers(tsk, &firing);\r\nunlock_task_sighand(tsk, &flags);\r\nlist_for_each_entry_safe(timer, next, &firing, it.cpu.entry) {\r\nint cpu_firing;\r\nspin_lock(&timer->it_lock);\r\nlist_del_init(&timer->it.cpu.entry);\r\ncpu_firing = timer->it.cpu.firing;\r\ntimer->it.cpu.firing = 0;\r\nif (likely(cpu_firing >= 0))\r\ncpu_timer_fire(timer);\r\nspin_unlock(&timer->it_lock);\r\n}\r\n}\r\nvoid set_process_cpu_timer(struct task_struct *tsk, unsigned int clock_idx,\r\ncputime_t *newval, cputime_t *oldval)\r\n{\r\nunion cpu_time_count now;\r\nBUG_ON(clock_idx == CPUCLOCK_SCHED);\r\ncpu_timer_sample_group(clock_idx, tsk, &now);\r\nif (oldval) {\r\nif (*oldval) {\r\nif (*oldval <= now.cpu) {\r\n*oldval = cputime_one_jiffy;\r\n} else {\r\n*oldval -= now.cpu;\r\n}\r\n}\r\nif (!*newval)\r\nreturn;\r\n*newval += now.cpu;\r\n}\r\nswitch (clock_idx) {\r\ncase CPUCLOCK_PROF:\r\nif (expires_gt(tsk->signal->cputime_expires.prof_exp, *newval))\r\ntsk->signal->cputime_expires.prof_exp = *newval;\r\nbreak;\r\ncase CPUCLOCK_VIRT:\r\nif (expires_gt(tsk->signal->cputime_expires.virt_exp, *newval))\r\ntsk->signal->cputime_expires.virt_exp = *newval;\r\nbreak;\r\n}\r\n}\r\nstatic int do_cpu_nanosleep(const clockid_t which_clock, int flags,\r\nstruct timespec *rqtp, struct itimerspec *it)\r\n{\r\nstruct k_itimer timer;\r\nint error;\r\nmemset(&timer, 0, sizeof timer);\r\nspin_lock_init(&timer.it_lock);\r\ntimer.it_clock = which_clock;\r\ntimer.it_overrun = -1;\r\nerror = posix_cpu_timer_create(&timer);\r\ntimer.it_process = current;\r\nif (!error) {\r\nstatic struct itimerspec zero_it;\r\nmemset(it, 0, sizeof *it);\r\nit->it_value = *rqtp;\r\nspin_lock_irq(&timer.it_lock);\r\nerror = posix_cpu_timer_set(&timer, flags, it, NULL);\r\nif (error) {\r\nspin_unlock_irq(&timer.it_lock);\r\nreturn error;\r\n}\r\nwhile (!signal_pending(current)) {\r\nif (timer.it.cpu.expires.sched == 0) {\r\nspin_unlock_irq(&timer.it_lock);\r\nreturn 0;\r\n}\r\n__set_current_state(TASK_INTERRUPTIBLE);\r\nspin_unlock_irq(&timer.it_lock);\r\nschedule();\r\nspin_lock_irq(&timer.it_lock);\r\n}\r\nsample_to_timespec(which_clock, timer.it.cpu.expires, rqtp);\r\nposix_cpu_timer_set(&timer, 0, &zero_it, it);\r\nspin_unlock_irq(&timer.it_lock);\r\nif ((it->it_value.tv_sec | it->it_value.tv_nsec) == 0) {\r\nreturn 0;\r\n}\r\nerror = -ERESTART_RESTARTBLOCK;\r\n}\r\nreturn error;\r\n}\r\nstatic int posix_cpu_nsleep(const clockid_t which_clock, int flags,\r\nstruct timespec *rqtp, struct timespec __user *rmtp)\r\n{\r\nstruct restart_block *restart_block =\r\n&current_thread_info()->restart_block;\r\nstruct itimerspec it;\r\nint error;\r\nif (CPUCLOCK_PERTHREAD(which_clock) &&\r\n(CPUCLOCK_PID(which_clock) == 0 ||\r\nCPUCLOCK_PID(which_clock) == current->pid))\r\nreturn -EINVAL;\r\nerror = do_cpu_nanosleep(which_clock, flags, rqtp, &it);\r\nif (error == -ERESTART_RESTARTBLOCK) {\r\nif (flags & TIMER_ABSTIME)\r\nreturn -ERESTARTNOHAND;\r\nif (rmtp && copy_to_user(rmtp, &it.it_value, sizeof *rmtp))\r\nreturn -EFAULT;\r\nrestart_block->fn = posix_cpu_nsleep_restart;\r\nrestart_block->nanosleep.clockid = which_clock;\r\nrestart_block->nanosleep.rmtp = rmtp;\r\nrestart_block->nanosleep.expires = timespec_to_ns(rqtp);\r\n}\r\nreturn error;\r\n}\r\nstatic long posix_cpu_nsleep_restart(struct restart_block *restart_block)\r\n{\r\nclockid_t which_clock = restart_block->nanosleep.clockid;\r\nstruct timespec t;\r\nstruct itimerspec it;\r\nint error;\r\nt = ns_to_timespec(restart_block->nanosleep.expires);\r\nerror = do_cpu_nanosleep(which_clock, TIMER_ABSTIME, &t, &it);\r\nif (error == -ERESTART_RESTARTBLOCK) {\r\nstruct timespec __user *rmtp = restart_block->nanosleep.rmtp;\r\nif (rmtp && copy_to_user(rmtp, &it.it_value, sizeof *rmtp))\r\nreturn -EFAULT;\r\nrestart_block->nanosleep.expires = timespec_to_ns(&t);\r\n}\r\nreturn error;\r\n}\r\nstatic int process_cpu_clock_getres(const clockid_t which_clock,\r\nstruct timespec *tp)\r\n{\r\nreturn posix_cpu_clock_getres(PROCESS_CLOCK, tp);\r\n}\r\nstatic int process_cpu_clock_get(const clockid_t which_clock,\r\nstruct timespec *tp)\r\n{\r\nreturn posix_cpu_clock_get(PROCESS_CLOCK, tp);\r\n}\r\nstatic int process_cpu_timer_create(struct k_itimer *timer)\r\n{\r\ntimer->it_clock = PROCESS_CLOCK;\r\nreturn posix_cpu_timer_create(timer);\r\n}\r\nstatic int process_cpu_nsleep(const clockid_t which_clock, int flags,\r\nstruct timespec *rqtp,\r\nstruct timespec __user *rmtp)\r\n{\r\nreturn posix_cpu_nsleep(PROCESS_CLOCK, flags, rqtp, rmtp);\r\n}\r\nstatic long process_cpu_nsleep_restart(struct restart_block *restart_block)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic int thread_cpu_clock_getres(const clockid_t which_clock,\r\nstruct timespec *tp)\r\n{\r\nreturn posix_cpu_clock_getres(THREAD_CLOCK, tp);\r\n}\r\nstatic int thread_cpu_clock_get(const clockid_t which_clock,\r\nstruct timespec *tp)\r\n{\r\nreturn posix_cpu_clock_get(THREAD_CLOCK, tp);\r\n}\r\nstatic int thread_cpu_timer_create(struct k_itimer *timer)\r\n{\r\ntimer->it_clock = THREAD_CLOCK;\r\nreturn posix_cpu_timer_create(timer);\r\n}\r\nstatic __init int init_posix_cpu_timers(void)\r\n{\r\nstruct k_clock process = {\r\n.clock_getres = process_cpu_clock_getres,\r\n.clock_get = process_cpu_clock_get,\r\n.timer_create = process_cpu_timer_create,\r\n.nsleep = process_cpu_nsleep,\r\n.nsleep_restart = process_cpu_nsleep_restart,\r\n};\r\nstruct k_clock thread = {\r\n.clock_getres = thread_cpu_clock_getres,\r\n.clock_get = thread_cpu_clock_get,\r\n.timer_create = thread_cpu_timer_create,\r\n};\r\nstruct timespec ts;\r\nposix_timers_register_clock(CLOCK_PROCESS_CPUTIME_ID, &process);\r\nposix_timers_register_clock(CLOCK_THREAD_CPUTIME_ID, &thread);\r\ncputime_to_timespec(cputime_one_jiffy, &ts);\r\nonecputick = ts.tv_nsec;\r\nWARN_ON(ts.tv_sec != 0);\r\nreturn 0;\r\n}
