static void __iomem *inic_port_base(struct ata_port *ap)\r\n{\r\nstruct inic_host_priv *hpriv = ap->host->private_data;\r\nreturn hpriv->mmio_base + ap->port_no * PORT_SIZE;\r\n}\r\nstatic void inic_reset_port(void __iomem *port_base)\r\n{\r\nvoid __iomem *idma_ctl = port_base + PORT_IDMA_CTL;\r\nreadw(idma_ctl);\r\nmsleep(1);\r\nwritew(IDMA_CTL_RST_IDMA, idma_ctl);\r\nreadw(idma_ctl);\r\nmsleep(1);\r\nwritew(0, idma_ctl);\r\nwriteb(0xff, port_base + PORT_IRQ_STAT);\r\n}\r\nstatic int inic_scr_read(struct ata_link *link, unsigned sc_reg, u32 *val)\r\n{\r\nvoid __iomem *scr_addr = inic_port_base(link->ap) + PORT_SCR;\r\nvoid __iomem *addr;\r\nif (unlikely(sc_reg >= ARRAY_SIZE(scr_map)))\r\nreturn -EINVAL;\r\naddr = scr_addr + scr_map[sc_reg] * 4;\r\n*val = readl(scr_addr + scr_map[sc_reg] * 4);\r\nif (sc_reg == SCR_ERROR)\r\n*val &= ~SERR_PHYRDY_CHG;\r\nreturn 0;\r\n}\r\nstatic int inic_scr_write(struct ata_link *link, unsigned sc_reg, u32 val)\r\n{\r\nvoid __iomem *scr_addr = inic_port_base(link->ap) + PORT_SCR;\r\nif (unlikely(sc_reg >= ARRAY_SIZE(scr_map)))\r\nreturn -EINVAL;\r\nwritel(val, scr_addr + scr_map[sc_reg] * 4);\r\nreturn 0;\r\n}\r\nstatic void inic_stop_idma(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_base = inic_port_base(ap);\r\nreadb(port_base + PORT_RPQ_FIFO);\r\nreadb(port_base + PORT_RPQ_CNT);\r\nwritew(0, port_base + PORT_IDMA_CTL);\r\n}\r\nstatic void inic_host_err_intr(struct ata_port *ap, u8 irq_stat, u16 idma_stat)\r\n{\r\nstruct ata_eh_info *ehi = &ap->link.eh_info;\r\nstruct inic_port_priv *pp = ap->private_data;\r\nstruct inic_cpb *cpb = &pp->pkt->cpb;\r\nbool freeze = false;\r\nata_ehi_clear_desc(ehi);\r\nata_ehi_push_desc(ehi, "irq_stat=0x%x idma_stat=0x%x",\r\nirq_stat, idma_stat);\r\ninic_stop_idma(ap);\r\nif (irq_stat & (PIRQ_OFFLINE | PIRQ_ONLINE)) {\r\nata_ehi_push_desc(ehi, "hotplug");\r\nata_ehi_hotplugged(ehi);\r\nfreeze = true;\r\n}\r\nif (idma_stat & IDMA_STAT_PERR) {\r\nata_ehi_push_desc(ehi, "PCI error");\r\nfreeze = true;\r\n}\r\nif (idma_stat & IDMA_STAT_CPBERR) {\r\nata_ehi_push_desc(ehi, "CPB error");\r\nif (cpb->resp_flags & CPB_RESP_IGNORED) {\r\n__ata_ehi_push_desc(ehi, " ignored");\r\nehi->err_mask |= AC_ERR_INVALID;\r\nfreeze = true;\r\n}\r\nif (cpb->resp_flags & CPB_RESP_ATA_ERR)\r\nehi->err_mask |= AC_ERR_DEV;\r\nif (cpb->resp_flags & CPB_RESP_SPURIOUS) {\r\n__ata_ehi_push_desc(ehi, " spurious-intr");\r\nehi->err_mask |= AC_ERR_HSM;\r\nfreeze = true;\r\n}\r\nif (cpb->resp_flags &\r\n(CPB_RESP_UNDERFLOW | CPB_RESP_OVERFLOW)) {\r\n__ata_ehi_push_desc(ehi, " data-over/underflow");\r\nehi->err_mask |= AC_ERR_HSM;\r\nfreeze = true;\r\n}\r\n}\r\nif (freeze)\r\nata_port_freeze(ap);\r\nelse\r\nata_port_abort(ap);\r\n}\r\nstatic void inic_host_intr(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_base = inic_port_base(ap);\r\nstruct ata_queued_cmd *qc = ata_qc_from_tag(ap, ap->link.active_tag);\r\nu8 irq_stat;\r\nu16 idma_stat;\r\nirq_stat = readb(port_base + PORT_IRQ_STAT);\r\nwriteb(irq_stat, port_base + PORT_IRQ_STAT);\r\nidma_stat = readw(port_base + PORT_IDMA_STAT);\r\nif (unlikely((irq_stat & PIRQ_ERR) || (idma_stat & IDMA_STAT_ERR)))\r\ninic_host_err_intr(ap, irq_stat, idma_stat);\r\nif (unlikely(!qc))\r\ngoto spurious;\r\nif (likely(idma_stat & IDMA_STAT_DONE)) {\r\ninic_stop_idma(ap);\r\nif (unlikely(readb(port_base + PORT_TF_COMMAND) &\r\n(ATA_DF | ATA_ERR)))\r\nqc->err_mask |= AC_ERR_DEV;\r\nata_qc_complete(qc);\r\nreturn;\r\n}\r\nspurious:\r\nata_port_warn(ap, "unhandled interrupt: cmd=0x%x irq_stat=0x%x idma_stat=0x%x\n",\r\nqc ? qc->tf.command : 0xff, irq_stat, idma_stat);\r\n}\r\nstatic irqreturn_t inic_interrupt(int irq, void *dev_instance)\r\n{\r\nstruct ata_host *host = dev_instance;\r\nstruct inic_host_priv *hpriv = host->private_data;\r\nu16 host_irq_stat;\r\nint i, handled = 0;\r\nhost_irq_stat = readw(hpriv->mmio_base + HOST_IRQ_STAT);\r\nif (unlikely(!(host_irq_stat & HIRQ_GLOBAL)))\r\ngoto out;\r\nspin_lock(&host->lock);\r\nfor (i = 0; i < NR_PORTS; i++)\r\nif (host_irq_stat & (HIRQ_PORT0 << i)) {\r\ninic_host_intr(host->ports[i]);\r\nhandled++;\r\n}\r\nspin_unlock(&host->lock);\r\nout:\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic int inic_check_atapi_dma(struct ata_queued_cmd *qc)\r\n{\r\nif (atapi_cmd_type(qc->cdb[0]) == READ)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void inic_fill_sg(struct inic_prd *prd, struct ata_queued_cmd *qc)\r\n{\r\nstruct scatterlist *sg;\r\nunsigned int si;\r\nu8 flags = 0;\r\nif (qc->tf.flags & ATA_TFLAG_WRITE)\r\nflags |= PRD_WRITE;\r\nif (ata_is_dma(qc->tf.protocol))\r\nflags |= PRD_DMA;\r\nfor_each_sg(qc->sg, sg, qc->n_elem, si) {\r\nprd->mad = cpu_to_le32(sg_dma_address(sg));\r\nprd->len = cpu_to_le16(sg_dma_len(sg));\r\nprd->flags = flags;\r\nprd++;\r\n}\r\nWARN_ON(!si);\r\nprd[-1].flags |= PRD_END;\r\n}\r\nstatic void inic_qc_prep(struct ata_queued_cmd *qc)\r\n{\r\nstruct inic_port_priv *pp = qc->ap->private_data;\r\nstruct inic_pkt *pkt = pp->pkt;\r\nstruct inic_cpb *cpb = &pkt->cpb;\r\nstruct inic_prd *prd = pkt->prd;\r\nbool is_atapi = ata_is_atapi(qc->tf.protocol);\r\nbool is_data = ata_is_data(qc->tf.protocol);\r\nunsigned int cdb_len = 0;\r\nVPRINTK("ENTER\n");\r\nif (is_atapi)\r\ncdb_len = qc->dev->cdb_len;\r\nmemset(pkt, 0, sizeof(struct inic_pkt));\r\ncpb->ctl_flags = CPB_CTL_VALID | CPB_CTL_IEN;\r\nif (is_atapi || is_data)\r\ncpb->ctl_flags |= CPB_CTL_DATA;\r\ncpb->len = cpu_to_le32(qc->nbytes + cdb_len);\r\ncpb->prd = cpu_to_le32(pp->pkt_dma + offsetof(struct inic_pkt, prd));\r\ncpb->device = qc->tf.device;\r\ncpb->feature = qc->tf.feature;\r\ncpb->nsect = qc->tf.nsect;\r\ncpb->lbal = qc->tf.lbal;\r\ncpb->lbam = qc->tf.lbam;\r\ncpb->lbah = qc->tf.lbah;\r\nif (qc->tf.flags & ATA_TFLAG_LBA48) {\r\ncpb->hob_feature = qc->tf.hob_feature;\r\ncpb->hob_nsect = qc->tf.hob_nsect;\r\ncpb->hob_lbal = qc->tf.hob_lbal;\r\ncpb->hob_lbam = qc->tf.hob_lbam;\r\ncpb->hob_lbah = qc->tf.hob_lbah;\r\n}\r\ncpb->command = qc->tf.command;\r\nif (is_atapi) {\r\nmemcpy(pkt->cdb, qc->cdb, ATAPI_CDB_LEN);\r\nprd->mad = cpu_to_le32(pp->pkt_dma +\r\noffsetof(struct inic_pkt, cdb));\r\nprd->len = cpu_to_le16(cdb_len);\r\nprd->flags = PRD_CDB | PRD_WRITE;\r\nif (!is_data)\r\nprd->flags |= PRD_END;\r\nprd++;\r\n}\r\nif (is_data)\r\ninic_fill_sg(prd, qc);\r\npp->cpb_tbl[0] = pp->pkt_dma;\r\n}\r\nstatic unsigned int inic_qc_issue(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nvoid __iomem *port_base = inic_port_base(ap);\r\nwritew(HCTL_FTHD0 | HCTL_LEDEN, port_base + HOST_CTL);\r\nwritew(IDMA_CTL_GO, port_base + PORT_IDMA_CTL);\r\nwriteb(0, port_base + PORT_CPB_PTQFIFO);\r\nreturn 0;\r\n}\r\nstatic void inic_tf_read(struct ata_port *ap, struct ata_taskfile *tf)\r\n{\r\nvoid __iomem *port_base = inic_port_base(ap);\r\ntf->feature = readb(port_base + PORT_TF_FEATURE);\r\ntf->nsect = readb(port_base + PORT_TF_NSECT);\r\ntf->lbal = readb(port_base + PORT_TF_LBAL);\r\ntf->lbam = readb(port_base + PORT_TF_LBAM);\r\ntf->lbah = readb(port_base + PORT_TF_LBAH);\r\ntf->device = readb(port_base + PORT_TF_DEVICE);\r\ntf->command = readb(port_base + PORT_TF_COMMAND);\r\n}\r\nstatic bool inic_qc_fill_rtf(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_taskfile *rtf = &qc->result_tf;\r\nstruct ata_taskfile tf;\r\ninic_tf_read(qc->ap, &tf);\r\nif (!(tf.command & ATA_ERR))\r\nreturn false;\r\nrtf->command = tf.command;\r\nrtf->feature = tf.feature;\r\nreturn true;\r\n}\r\nstatic void inic_freeze(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_base = inic_port_base(ap);\r\nwriteb(PIRQ_MASK_FREEZE, port_base + PORT_IRQ_MASK);\r\nwriteb(0xff, port_base + PORT_IRQ_STAT);\r\n}\r\nstatic void inic_thaw(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_base = inic_port_base(ap);\r\nwriteb(0xff, port_base + PORT_IRQ_STAT);\r\nwriteb(PIRQ_MASK_DEFAULT, port_base + PORT_IRQ_MASK);\r\n}\r\nstatic int inic_check_ready(struct ata_link *link)\r\n{\r\nvoid __iomem *port_base = inic_port_base(link->ap);\r\nreturn ata_check_ready(readb(port_base + PORT_TF_COMMAND));\r\n}\r\nstatic int inic_hardreset(struct ata_link *link, unsigned int *class,\r\nunsigned long deadline)\r\n{\r\nstruct ata_port *ap = link->ap;\r\nvoid __iomem *port_base = inic_port_base(ap);\r\nvoid __iomem *idma_ctl = port_base + PORT_IDMA_CTL;\r\nconst unsigned long *timing = sata_ehc_deb_timing(&link->eh_context);\r\nint rc;\r\ninic_reset_port(port_base);\r\nwritew(IDMA_CTL_RST_ATA, idma_ctl);\r\nreadw(idma_ctl);\r\nata_msleep(ap, 1);\r\nwritew(0, idma_ctl);\r\nrc = sata_link_resume(link, timing, deadline);\r\nif (rc) {\r\nata_link_warn(link,\r\n"failed to resume link after reset (errno=%d)\n",\r\nrc);\r\nreturn rc;\r\n}\r\n*class = ATA_DEV_NONE;\r\nif (ata_link_online(link)) {\r\nstruct ata_taskfile tf;\r\nrc = ata_wait_after_reset(link, deadline, inic_check_ready);\r\nif (rc) {\r\nata_link_warn(link,\r\n"device not ready after hardreset (errno=%d)\n",\r\nrc);\r\nreturn rc;\r\n}\r\ninic_tf_read(ap, &tf);\r\n*class = ata_dev_classify(&tf);\r\n}\r\nreturn 0;\r\n}\r\nstatic void inic_error_handler(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_base = inic_port_base(ap);\r\ninic_reset_port(port_base);\r\nata_std_error_handler(ap);\r\n}\r\nstatic void inic_post_internal_cmd(struct ata_queued_cmd *qc)\r\n{\r\nif (qc->flags & ATA_QCFLAG_FAILED)\r\ninic_reset_port(inic_port_base(qc->ap));\r\n}\r\nstatic void init_port(struct ata_port *ap)\r\n{\r\nvoid __iomem *port_base = inic_port_base(ap);\r\nstruct inic_port_priv *pp = ap->private_data;\r\nmemset(pp->pkt, 0, sizeof(struct inic_pkt));\r\nmemset(pp->cpb_tbl, 0, IDMA_CPB_TBL_SIZE);\r\nwritel(pp->cpb_tbl_dma, port_base + PORT_CPB_CPBLAR);\r\n}\r\nstatic int inic_port_resume(struct ata_port *ap)\r\n{\r\ninit_port(ap);\r\nreturn 0;\r\n}\r\nstatic int inic_port_start(struct ata_port *ap)\r\n{\r\nstruct device *dev = ap->host->dev;\r\nstruct inic_port_priv *pp;\r\npp = devm_kzalloc(dev, sizeof(*pp), GFP_KERNEL);\r\nif (!pp)\r\nreturn -ENOMEM;\r\nap->private_data = pp;\r\npp->pkt = dmam_alloc_coherent(dev, sizeof(struct inic_pkt),\r\n&pp->pkt_dma, GFP_KERNEL);\r\nif (!pp->pkt)\r\nreturn -ENOMEM;\r\npp->cpb_tbl = dmam_alloc_coherent(dev, IDMA_CPB_TBL_SIZE,\r\n&pp->cpb_tbl_dma, GFP_KERNEL);\r\nif (!pp->cpb_tbl)\r\nreturn -ENOMEM;\r\ninit_port(ap);\r\nreturn 0;\r\n}\r\nstatic int init_controller(void __iomem *mmio_base, u16 hctl)\r\n{\r\nint i;\r\nu16 val;\r\nhctl &= ~HCTL_KNOWN_BITS;\r\nwritew(hctl | HCTL_SOFTRST, mmio_base + HOST_CTL);\r\nreadw(mmio_base + HOST_CTL);\r\nfor (i = 0; i < 10; i++) {\r\nmsleep(1);\r\nval = readw(mmio_base + HOST_CTL);\r\nif (!(val & HCTL_SOFTRST))\r\nbreak;\r\n}\r\nif (val & HCTL_SOFTRST)\r\nreturn -EIO;\r\nfor (i = 0; i < NR_PORTS; i++) {\r\nvoid __iomem *port_base = mmio_base + i * PORT_SIZE;\r\nwriteb(0xff, port_base + PORT_IRQ_MASK);\r\ninic_reset_port(port_base);\r\n}\r\nwritew(hctl & ~HCTL_IRQOFF, mmio_base + HOST_CTL);\r\nval = readw(mmio_base + HOST_IRQ_MASK);\r\nval &= ~(HIRQ_PORT0 | HIRQ_PORT1);\r\nwritew(val, mmio_base + HOST_IRQ_MASK);\r\nreturn 0;\r\n}\r\nstatic int inic_pci_device_resume(struct pci_dev *pdev)\r\n{\r\nstruct ata_host *host = dev_get_drvdata(&pdev->dev);\r\nstruct inic_host_priv *hpriv = host->private_data;\r\nint rc;\r\nrc = ata_pci_device_do_resume(pdev);\r\nif (rc)\r\nreturn rc;\r\nif (pdev->dev.power.power_state.event == PM_EVENT_SUSPEND) {\r\nrc = init_controller(hpriv->mmio_base, hpriv->cached_hctl);\r\nif (rc)\r\nreturn rc;\r\n}\r\nata_host_resume(host);\r\nreturn 0;\r\n}\r\nstatic int inic_init_one(struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nconst struct ata_port_info *ppi[] = { &inic_port_info, NULL };\r\nstruct ata_host *host;\r\nstruct inic_host_priv *hpriv;\r\nvoid __iomem * const *iomap;\r\nint mmio_bar;\r\nint i, rc;\r\nata_print_version_once(&pdev->dev, DRV_VERSION);\r\nhost = ata_host_alloc_pinfo(&pdev->dev, ppi, NR_PORTS);\r\nhpriv = devm_kzalloc(&pdev->dev, sizeof(*hpriv), GFP_KERNEL);\r\nif (!host || !hpriv)\r\nreturn -ENOMEM;\r\nhost->private_data = hpriv;\r\nrc = pcim_enable_device(pdev);\r\nif (rc)\r\nreturn rc;\r\nif (pci_resource_flags(pdev, MMIO_BAR_PCI) & IORESOURCE_MEM)\r\nmmio_bar = MMIO_BAR_PCI;\r\nelse\r\nmmio_bar = MMIO_BAR_CARDBUS;\r\nrc = pcim_iomap_regions(pdev, 1 << mmio_bar, DRV_NAME);\r\nif (rc)\r\nreturn rc;\r\nhost->iomap = iomap = pcim_iomap_table(pdev);\r\nhpriv->mmio_base = iomap[mmio_bar];\r\nhpriv->cached_hctl = readw(hpriv->mmio_base + HOST_CTL);\r\nfor (i = 0; i < NR_PORTS; i++) {\r\nstruct ata_port *ap = host->ports[i];\r\nata_port_pbar_desc(ap, mmio_bar, -1, "mmio");\r\nata_port_pbar_desc(ap, mmio_bar, i * PORT_SIZE, "port");\r\n}\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc) {\r\ndev_err(&pdev->dev, "32-bit DMA enable failed\n");\r\nreturn rc;\r\n}\r\nrc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc) {\r\ndev_err(&pdev->dev, "32-bit consistent DMA enable failed\n");\r\nreturn rc;\r\n}\r\nrc = pci_set_dma_max_seg_size(pdev, 65536 - 512);\r\nif (rc) {\r\ndev_err(&pdev->dev, "failed to set the maximum segment size\n");\r\nreturn rc;\r\n}\r\nrc = init_controller(hpriv->mmio_base, hpriv->cached_hctl);\r\nif (rc) {\r\ndev_err(&pdev->dev, "failed to initialize controller\n");\r\nreturn rc;\r\n}\r\npci_set_master(pdev);\r\nreturn ata_host_activate(host, pdev->irq, inic_interrupt, IRQF_SHARED,\r\n&inic_sht);\r\n}
