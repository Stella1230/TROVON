static bool ixgbe_cache_ring_dcb_sriov(struct ixgbe_adapter *adapter)\r\n{\r\n#ifdef IXGBE_FCOE\r\nstruct ixgbe_ring_feature *fcoe = &adapter->ring_feature[RING_F_FCOE];\r\n#endif\r\nstruct ixgbe_ring_feature *vmdq = &adapter->ring_feature[RING_F_VMDQ];\r\nint i;\r\nu16 reg_idx;\r\nu8 tcs = netdev_get_num_tc(adapter->netdev);\r\nif (tcs <= 1)\r\nreturn false;\r\nif (!(adapter->flags & IXGBE_FLAG_SRIOV_ENABLED))\r\nreturn false;\r\nreg_idx = vmdq->offset * __ALIGN_MASK(1, ~vmdq->mask);\r\nfor (i = 0; i < adapter->num_rx_queues; i++, reg_idx++) {\r\nif ((reg_idx & ~vmdq->mask) >= tcs)\r\nreg_idx = __ALIGN_MASK(reg_idx, ~vmdq->mask);\r\nadapter->rx_ring[i]->reg_idx = reg_idx;\r\n}\r\nreg_idx = vmdq->offset * __ALIGN_MASK(1, ~vmdq->mask);\r\nfor (i = 0; i < adapter->num_tx_queues; i++, reg_idx++) {\r\nif ((reg_idx & ~vmdq->mask) >= tcs)\r\nreg_idx = __ALIGN_MASK(reg_idx, ~vmdq->mask);\r\nadapter->tx_ring[i]->reg_idx = reg_idx;\r\n}\r\n#ifdef IXGBE_FCOE\r\nif (!(adapter->flags & IXGBE_FLAG_FCOE_ENABLED))\r\nreturn true;\r\nif (fcoe->offset < tcs)\r\nreturn true;\r\nif (fcoe->indices) {\r\nu16 queues_per_pool = __ALIGN_MASK(1, ~vmdq->mask);\r\nu8 fcoe_tc = ixgbe_fcoe_get_tc(adapter);\r\nreg_idx = (vmdq->offset + vmdq->indices) * queues_per_pool;\r\nfor (i = fcoe->offset; i < adapter->num_rx_queues; i++) {\r\nreg_idx = __ALIGN_MASK(reg_idx, ~vmdq->mask) + fcoe_tc;\r\nadapter->rx_ring[i]->reg_idx = reg_idx;\r\nreg_idx++;\r\n}\r\nreg_idx = (vmdq->offset + vmdq->indices) * queues_per_pool;\r\nfor (i = fcoe->offset; i < adapter->num_tx_queues; i++) {\r\nreg_idx = __ALIGN_MASK(reg_idx, ~vmdq->mask) + fcoe_tc;\r\nadapter->tx_ring[i]->reg_idx = reg_idx;\r\nreg_idx++;\r\n}\r\n}\r\n#endif\r\nreturn true;\r\n}\r\nstatic void ixgbe_get_first_reg_idx(struct ixgbe_adapter *adapter, u8 tc,\r\nunsigned int *tx, unsigned int *rx)\r\n{\r\nstruct net_device *dev = adapter->netdev;\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nu8 num_tcs = netdev_get_num_tc(dev);\r\n*tx = 0;\r\n*rx = 0;\r\nswitch (hw->mac.type) {\r\ncase ixgbe_mac_82598EB:\r\n*tx = tc << 2;\r\n*rx = tc << 3;\r\nbreak;\r\ncase ixgbe_mac_82599EB:\r\ncase ixgbe_mac_X540:\r\nif (num_tcs > 4) {\r\n*rx = tc << 4;\r\nif (tc < 3)\r\n*tx = tc << 5;\r\nelse if (tc < 5)\r\n*tx = (tc + 2) << 4;\r\nelse\r\n*tx = (tc + 8) << 3;\r\n} else {\r\n*rx = tc << 5;\r\nif (tc < 2)\r\n*tx = tc << 6;\r\nelse\r\n*tx = (tc + 4) << 4;\r\n}\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic bool ixgbe_cache_ring_dcb(struct ixgbe_adapter *adapter)\r\n{\r\nstruct net_device *dev = adapter->netdev;\r\nunsigned int tx_idx, rx_idx;\r\nint tc, offset, rss_i, i;\r\nu8 num_tcs = netdev_get_num_tc(dev);\r\nif (num_tcs <= 1)\r\nreturn false;\r\nrss_i = adapter->ring_feature[RING_F_RSS].indices;\r\nfor (tc = 0, offset = 0; tc < num_tcs; tc++, offset += rss_i) {\r\nixgbe_get_first_reg_idx(adapter, tc, &tx_idx, &rx_idx);\r\nfor (i = 0; i < rss_i; i++, tx_idx++, rx_idx++) {\r\nadapter->tx_ring[offset + i]->reg_idx = tx_idx;\r\nadapter->rx_ring[offset + i]->reg_idx = rx_idx;\r\nadapter->tx_ring[offset + i]->dcb_tc = tc;\r\nadapter->rx_ring[offset + i]->dcb_tc = tc;\r\n}\r\n}\r\nreturn true;\r\n}\r\nstatic bool ixgbe_cache_ring_sriov(struct ixgbe_adapter *adapter)\r\n{\r\n#ifdef IXGBE_FCOE\r\nstruct ixgbe_ring_feature *fcoe = &adapter->ring_feature[RING_F_FCOE];\r\n#endif\r\nstruct ixgbe_ring_feature *vmdq = &adapter->ring_feature[RING_F_VMDQ];\r\nstruct ixgbe_ring_feature *rss = &adapter->ring_feature[RING_F_RSS];\r\nint i;\r\nu16 reg_idx;\r\nif (!(adapter->flags & IXGBE_FLAG_VMDQ_ENABLED))\r\nreturn false;\r\nreg_idx = vmdq->offset * __ALIGN_MASK(1, ~vmdq->mask);\r\nfor (i = 0; i < adapter->num_rx_queues; i++, reg_idx++) {\r\n#ifdef IXGBE_FCOE\r\nif (fcoe->offset && (i > fcoe->offset))\r\nbreak;\r\n#endif\r\nif ((reg_idx & ~vmdq->mask) >= rss->indices)\r\nreg_idx = __ALIGN_MASK(reg_idx, ~vmdq->mask);\r\nadapter->rx_ring[i]->reg_idx = reg_idx;\r\n}\r\n#ifdef IXGBE_FCOE\r\nfor (; i < adapter->num_rx_queues; i++, reg_idx++)\r\nadapter->rx_ring[i]->reg_idx = reg_idx;\r\n#endif\r\nreg_idx = vmdq->offset * __ALIGN_MASK(1, ~vmdq->mask);\r\nfor (i = 0; i < adapter->num_tx_queues; i++, reg_idx++) {\r\n#ifdef IXGBE_FCOE\r\nif (fcoe->offset && (i > fcoe->offset))\r\nbreak;\r\n#endif\r\nif ((reg_idx & rss->mask) >= rss->indices)\r\nreg_idx = __ALIGN_MASK(reg_idx, ~vmdq->mask);\r\nadapter->tx_ring[i]->reg_idx = reg_idx;\r\n}\r\n#ifdef IXGBE_FCOE\r\nfor (; i < adapter->num_tx_queues; i++, reg_idx++)\r\nadapter->tx_ring[i]->reg_idx = reg_idx;\r\n#endif\r\nreturn true;\r\n}\r\nstatic bool ixgbe_cache_ring_rss(struct ixgbe_adapter *adapter)\r\n{\r\nint i;\r\nfor (i = 0; i < adapter->num_rx_queues; i++)\r\nadapter->rx_ring[i]->reg_idx = i;\r\nfor (i = 0; i < adapter->num_tx_queues; i++)\r\nadapter->tx_ring[i]->reg_idx = i;\r\nreturn true;\r\n}\r\nstatic void ixgbe_cache_ring_register(struct ixgbe_adapter *adapter)\r\n{\r\nadapter->rx_ring[0]->reg_idx = 0;\r\nadapter->tx_ring[0]->reg_idx = 0;\r\n#ifdef CONFIG_IXGBE_DCB\r\nif (ixgbe_cache_ring_dcb_sriov(adapter))\r\nreturn;\r\nif (ixgbe_cache_ring_dcb(adapter))\r\nreturn;\r\n#endif\r\nif (ixgbe_cache_ring_sriov(adapter))\r\nreturn;\r\nixgbe_cache_ring_rss(adapter);\r\n}\r\nstatic bool ixgbe_set_dcb_sriov_queues(struct ixgbe_adapter *adapter)\r\n{\r\nint i;\r\nu16 vmdq_i = adapter->ring_feature[RING_F_VMDQ].limit;\r\nu16 vmdq_m = 0;\r\n#ifdef IXGBE_FCOE\r\nu16 fcoe_i = 0;\r\n#endif\r\nu8 tcs = netdev_get_num_tc(adapter->netdev);\r\nif (tcs <= 1)\r\nreturn false;\r\nif (!(adapter->flags & IXGBE_FLAG_SRIOV_ENABLED))\r\nreturn false;\r\nvmdq_i += adapter->ring_feature[RING_F_VMDQ].offset;\r\nif (tcs > 4) {\r\nvmdq_i = min_t(u16, vmdq_i, 16);\r\nvmdq_m = IXGBE_82599_VMDQ_8Q_MASK;\r\n} else {\r\nvmdq_i = min_t(u16, vmdq_i, 32);\r\nvmdq_m = IXGBE_82599_VMDQ_4Q_MASK;\r\n}\r\n#ifdef IXGBE_FCOE\r\nfcoe_i = (128 / __ALIGN_MASK(1, ~vmdq_m)) - vmdq_i;\r\n#endif\r\nvmdq_i -= adapter->ring_feature[RING_F_VMDQ].offset;\r\nadapter->ring_feature[RING_F_VMDQ].indices = vmdq_i;\r\nadapter->ring_feature[RING_F_VMDQ].mask = vmdq_m;\r\nadapter->ring_feature[RING_F_RSS].indices = 1;\r\nadapter->ring_feature[RING_F_RSS].mask = IXGBE_RSS_DISABLED_MASK;\r\nadapter->flags &= ~IXGBE_FLAG_FDIR_HASH_CAPABLE;\r\nadapter->num_rx_pools = vmdq_i;\r\nadapter->num_rx_queues_per_pool = tcs;\r\nadapter->num_tx_queues = vmdq_i * tcs;\r\nadapter->num_rx_queues = vmdq_i * tcs;\r\n#ifdef IXGBE_FCOE\r\nif (adapter->flags & IXGBE_FLAG_FCOE_ENABLED) {\r\nstruct ixgbe_ring_feature *fcoe;\r\nfcoe = &adapter->ring_feature[RING_F_FCOE];\r\nfcoe_i = min_t(u16, fcoe_i, num_online_cpus());\r\nfcoe_i = min_t(u16, fcoe_i, fcoe->limit);\r\nif (fcoe_i) {\r\nfcoe->indices = fcoe_i;\r\nfcoe->offset = vmdq_i * tcs;\r\nadapter->num_tx_queues += fcoe_i;\r\nadapter->num_rx_queues += fcoe_i;\r\n} else if (tcs > 1) {\r\nfcoe->indices = 1;\r\nfcoe->offset = ixgbe_fcoe_get_tc(adapter);\r\n} else {\r\nadapter->flags &= ~IXGBE_FLAG_FCOE_ENABLED;\r\nfcoe->indices = 0;\r\nfcoe->offset = 0;\r\n}\r\n}\r\n#endif\r\nfor (i = 0; i < tcs; i++)\r\nnetdev_set_tc_queue(adapter->netdev, i, 1, i);\r\nreturn true;\r\n}\r\nstatic bool ixgbe_set_dcb_queues(struct ixgbe_adapter *adapter)\r\n{\r\nstruct net_device *dev = adapter->netdev;\r\nstruct ixgbe_ring_feature *f;\r\nint rss_i, rss_m, i;\r\nint tcs;\r\ntcs = netdev_get_num_tc(dev);\r\nif (tcs <= 1)\r\nreturn false;\r\nrss_i = dev->num_tx_queues / tcs;\r\nif (adapter->hw.mac.type == ixgbe_mac_82598EB) {\r\nrss_i = min_t(u16, rss_i, 4);\r\nrss_m = IXGBE_RSS_4Q_MASK;\r\n} else if (tcs > 4) {\r\nrss_i = min_t(u16, rss_i, 8);\r\nrss_m = IXGBE_RSS_8Q_MASK;\r\n} else {\r\nrss_i = min_t(u16, rss_i, 16);\r\nrss_m = IXGBE_RSS_16Q_MASK;\r\n}\r\nf = &adapter->ring_feature[RING_F_RSS];\r\nrss_i = min_t(int, rss_i, f->limit);\r\nf->indices = rss_i;\r\nf->mask = rss_m;\r\nadapter->flags &= ~IXGBE_FLAG_FDIR_HASH_CAPABLE;\r\n#ifdef IXGBE_FCOE\r\nif (adapter->flags & IXGBE_FLAG_FCOE_ENABLED) {\r\nu8 tc = ixgbe_fcoe_get_tc(adapter);\r\nf = &adapter->ring_feature[RING_F_FCOE];\r\nf->indices = min_t(u16, rss_i, f->limit);\r\nf->offset = rss_i * tc;\r\n}\r\n#endif\r\nfor (i = 0; i < tcs; i++)\r\nnetdev_set_tc_queue(dev, i, rss_i, rss_i * i);\r\nadapter->num_tx_queues = rss_i * tcs;\r\nadapter->num_rx_queues = rss_i * tcs;\r\nreturn true;\r\n}\r\nstatic bool ixgbe_set_sriov_queues(struct ixgbe_adapter *adapter)\r\n{\r\nu16 vmdq_i = adapter->ring_feature[RING_F_VMDQ].limit;\r\nu16 vmdq_m = 0;\r\nu16 rss_i = adapter->ring_feature[RING_F_RSS].limit;\r\nu16 rss_m = IXGBE_RSS_DISABLED_MASK;\r\n#ifdef IXGBE_FCOE\r\nu16 fcoe_i = 0;\r\n#endif\r\nif (!(adapter->flags & IXGBE_FLAG_SRIOV_ENABLED))\r\nreturn false;\r\nvmdq_i += adapter->ring_feature[RING_F_VMDQ].offset;\r\nvmdq_i = min_t(u16, IXGBE_MAX_VMDQ_INDICES, vmdq_i);\r\nif ((vmdq_i > 32) || (rss_i < 4)) {\r\nvmdq_m = IXGBE_82599_VMDQ_2Q_MASK;\r\nrss_m = IXGBE_RSS_2Q_MASK;\r\nrss_i = min_t(u16, rss_i, 2);\r\n} else {\r\nvmdq_m = IXGBE_82599_VMDQ_4Q_MASK;\r\nrss_m = IXGBE_RSS_4Q_MASK;\r\nrss_i = 4;\r\n}\r\n#ifdef IXGBE_FCOE\r\nfcoe_i = 128 - (vmdq_i * __ALIGN_MASK(1, ~vmdq_m));\r\n#endif\r\nvmdq_i -= adapter->ring_feature[RING_F_VMDQ].offset;\r\nadapter->ring_feature[RING_F_VMDQ].indices = vmdq_i;\r\nadapter->ring_feature[RING_F_VMDQ].mask = vmdq_m;\r\nadapter->ring_feature[RING_F_RSS].indices = rss_i;\r\nadapter->ring_feature[RING_F_RSS].mask = rss_m;\r\nadapter->num_rx_pools = vmdq_i;\r\nadapter->num_rx_queues_per_pool = rss_i;\r\nadapter->num_rx_queues = vmdq_i * rss_i;\r\nadapter->num_tx_queues = vmdq_i * rss_i;\r\nadapter->flags &= ~IXGBE_FLAG_FDIR_HASH_CAPABLE;\r\n#ifdef IXGBE_FCOE\r\nif (adapter->flags & IXGBE_FLAG_FCOE_ENABLED) {\r\nstruct ixgbe_ring_feature *fcoe;\r\nfcoe = &adapter->ring_feature[RING_F_FCOE];\r\nfcoe_i = min_t(u16, fcoe_i, fcoe->limit);\r\nif (vmdq_i > 1 && fcoe_i) {\r\nfcoe_i = min_t(u16, fcoe_i, num_online_cpus());\r\nfcoe->indices = fcoe_i;\r\nfcoe->offset = vmdq_i * rss_i;\r\n} else {\r\nfcoe_i = min_t(u16, fcoe_i + rss_i, num_online_cpus());\r\nif (!(adapter->flags & IXGBE_FLAG_MSIX_ENABLED))\r\nfcoe_i = rss_i;\r\nfcoe->indices = min_t(u16, fcoe_i, fcoe->limit);\r\nfcoe->offset = fcoe_i - fcoe->indices;\r\nfcoe_i -= rss_i;\r\n}\r\nadapter->num_tx_queues += fcoe_i;\r\nadapter->num_rx_queues += fcoe_i;\r\n}\r\n#endif\r\nreturn true;\r\n}\r\nstatic bool ixgbe_set_rss_queues(struct ixgbe_adapter *adapter)\r\n{\r\nstruct ixgbe_ring_feature *f;\r\nu16 rss_i;\r\nf = &adapter->ring_feature[RING_F_RSS];\r\nrss_i = f->limit;\r\nf->indices = rss_i;\r\nf->mask = IXGBE_RSS_16Q_MASK;\r\nadapter->flags &= ~IXGBE_FLAG_FDIR_HASH_CAPABLE;\r\nif (rss_i > 1 && adapter->atr_sample_rate) {\r\nf = &adapter->ring_feature[RING_F_FDIR];\r\nf->indices = min_t(u16, num_online_cpus(), f->limit);\r\nrss_i = max_t(u16, rss_i, f->indices);\r\nif (!(adapter->flags & IXGBE_FLAG_FDIR_PERFECT_CAPABLE))\r\nadapter->flags |= IXGBE_FLAG_FDIR_HASH_CAPABLE;\r\n}\r\n#ifdef IXGBE_FCOE\r\nif (adapter->flags & IXGBE_FLAG_FCOE_ENABLED) {\r\nstruct net_device *dev = adapter->netdev;\r\nu16 fcoe_i;\r\nf = &adapter->ring_feature[RING_F_FCOE];\r\nfcoe_i = min_t(u16, f->limit + rss_i, num_online_cpus());\r\nfcoe_i = min_t(u16, fcoe_i, dev->num_tx_queues);\r\nif (!(adapter->flags & IXGBE_FLAG_MSIX_ENABLED))\r\nfcoe_i = rss_i;\r\nf->indices = min_t(u16, fcoe_i, f->limit);\r\nf->offset = fcoe_i - f->indices;\r\nrss_i = max_t(u16, fcoe_i, rss_i);\r\n}\r\n#endif\r\nadapter->num_rx_queues = rss_i;\r\nadapter->num_tx_queues = rss_i;\r\nreturn true;\r\n}\r\nstatic void ixgbe_set_num_queues(struct ixgbe_adapter *adapter)\r\n{\r\nadapter->num_rx_queues = 1;\r\nadapter->num_tx_queues = 1;\r\nadapter->num_rx_pools = adapter->num_rx_queues;\r\nadapter->num_rx_queues_per_pool = 1;\r\n#ifdef CONFIG_IXGBE_DCB\r\nif (ixgbe_set_dcb_sriov_queues(adapter))\r\nreturn;\r\nif (ixgbe_set_dcb_queues(adapter))\r\nreturn;\r\n#endif\r\nif (ixgbe_set_sriov_queues(adapter))\r\nreturn;\r\nixgbe_set_rss_queues(adapter);\r\n}\r\nstatic void ixgbe_acquire_msix_vectors(struct ixgbe_adapter *adapter,\r\nint vectors)\r\n{\r\nint err, vector_threshold;\r\nvector_threshold = MIN_MSIX_COUNT;\r\nwhile (vectors >= vector_threshold) {\r\nerr = pci_enable_msix(adapter->pdev, adapter->msix_entries,\r\nvectors);\r\nif (!err)\r\nbreak;\r\nelse if (err < 0)\r\nvectors = 0;\r\nelse\r\nvectors = err;\r\n}\r\nif (vectors < vector_threshold) {\r\nnetif_printk(adapter, hw, KERN_DEBUG, adapter->netdev,\r\n"Unable to allocate MSI-X interrupts\n");\r\nadapter->flags &= ~IXGBE_FLAG_MSIX_ENABLED;\r\nkfree(adapter->msix_entries);\r\nadapter->msix_entries = NULL;\r\n} else {\r\nadapter->flags |= IXGBE_FLAG_MSIX_ENABLED;\r\nvectors -= NON_Q_VECTORS;\r\nadapter->num_q_vectors = min(vectors, adapter->max_q_vectors);\r\n}\r\n}\r\nstatic void ixgbe_add_ring(struct ixgbe_ring *ring,\r\nstruct ixgbe_ring_container *head)\r\n{\r\nring->next = head->ring;\r\nhead->ring = ring;\r\nhead->count++;\r\n}\r\nstatic int ixgbe_alloc_q_vector(struct ixgbe_adapter *adapter,\r\nint v_count, int v_idx,\r\nint txr_count, int txr_idx,\r\nint rxr_count, int rxr_idx)\r\n{\r\nstruct ixgbe_q_vector *q_vector;\r\nstruct ixgbe_ring *ring;\r\nint node = -1;\r\nint cpu = -1;\r\nint ring_count, size;\r\nring_count = txr_count + rxr_count;\r\nsize = sizeof(struct ixgbe_q_vector) +\r\n(sizeof(struct ixgbe_ring) * ring_count);\r\nif (adapter->flags & IXGBE_FLAG_FDIR_HASH_CAPABLE) {\r\nif (cpu_online(v_idx)) {\r\ncpu = v_idx;\r\nnode = cpu_to_node(cpu);\r\n}\r\n}\r\nq_vector = kzalloc_node(size, GFP_KERNEL, node);\r\nif (!q_vector)\r\nq_vector = kzalloc(size, GFP_KERNEL);\r\nif (!q_vector)\r\nreturn -ENOMEM;\r\nif (cpu != -1)\r\ncpumask_set_cpu(cpu, &q_vector->affinity_mask);\r\nelse\r\ncpumask_copy(&q_vector->affinity_mask, cpu_online_mask);\r\nq_vector->numa_node = node;\r\nnetif_napi_add(adapter->netdev, &q_vector->napi,\r\nixgbe_poll, 64);\r\nadapter->q_vector[v_idx] = q_vector;\r\nq_vector->adapter = adapter;\r\nq_vector->v_idx = v_idx;\r\nq_vector->tx.work_limit = adapter->tx_work_limit;\r\nring = q_vector->ring;\r\nwhile (txr_count) {\r\nring->dev = &adapter->pdev->dev;\r\nring->netdev = adapter->netdev;\r\nring->q_vector = q_vector;\r\nixgbe_add_ring(ring, &q_vector->tx);\r\nring->count = adapter->tx_ring_count;\r\nring->queue_index = txr_idx;\r\nadapter->tx_ring[txr_idx] = ring;\r\ntxr_count--;\r\ntxr_idx += v_count;\r\nring++;\r\n}\r\nwhile (rxr_count) {\r\nring->dev = &adapter->pdev->dev;\r\nring->netdev = adapter->netdev;\r\nring->q_vector = q_vector;\r\nixgbe_add_ring(ring, &q_vector->rx);\r\nif (adapter->hw.mac.type == ixgbe_mac_82599EB)\r\nset_bit(__IXGBE_RX_CSUM_UDP_ZERO_ERR, &ring->state);\r\n#ifdef IXGBE_FCOE\r\nif (adapter->netdev->features & NETIF_F_FCOE_MTU) {\r\nstruct ixgbe_ring_feature *f;\r\nf = &adapter->ring_feature[RING_F_FCOE];\r\nif ((rxr_idx >= f->offset) &&\r\n(rxr_idx < f->offset + f->indices))\r\nset_bit(__IXGBE_RX_FCOE, &ring->state);\r\n}\r\n#endif\r\nring->count = adapter->rx_ring_count;\r\nring->queue_index = rxr_idx;\r\nadapter->rx_ring[rxr_idx] = ring;\r\nrxr_count--;\r\nrxr_idx += v_count;\r\nring++;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ixgbe_free_q_vector(struct ixgbe_adapter *adapter, int v_idx)\r\n{\r\nstruct ixgbe_q_vector *q_vector = adapter->q_vector[v_idx];\r\nstruct ixgbe_ring *ring;\r\nixgbe_for_each_ring(ring, q_vector->tx)\r\nadapter->tx_ring[ring->queue_index] = NULL;\r\nixgbe_for_each_ring(ring, q_vector->rx)\r\nadapter->rx_ring[ring->queue_index] = NULL;\r\nadapter->q_vector[v_idx] = NULL;\r\nnetif_napi_del(&q_vector->napi);\r\nkfree_rcu(q_vector, rcu);\r\n}\r\nstatic int ixgbe_alloc_q_vectors(struct ixgbe_adapter *adapter)\r\n{\r\nint q_vectors = adapter->num_q_vectors;\r\nint rxr_remaining = adapter->num_rx_queues;\r\nint txr_remaining = adapter->num_tx_queues;\r\nint rxr_idx = 0, txr_idx = 0, v_idx = 0;\r\nint err;\r\nif (!(adapter->flags & IXGBE_FLAG_MSIX_ENABLED))\r\nq_vectors = 1;\r\nif (q_vectors >= (rxr_remaining + txr_remaining)) {\r\nfor (; rxr_remaining; v_idx++) {\r\nerr = ixgbe_alloc_q_vector(adapter, q_vectors, v_idx,\r\n0, 0, 1, rxr_idx);\r\nif (err)\r\ngoto err_out;\r\nrxr_remaining--;\r\nrxr_idx++;\r\n}\r\n}\r\nfor (; v_idx < q_vectors; v_idx++) {\r\nint rqpv = DIV_ROUND_UP(rxr_remaining, q_vectors - v_idx);\r\nint tqpv = DIV_ROUND_UP(txr_remaining, q_vectors - v_idx);\r\nerr = ixgbe_alloc_q_vector(adapter, q_vectors, v_idx,\r\ntqpv, txr_idx,\r\nrqpv, rxr_idx);\r\nif (err)\r\ngoto err_out;\r\nrxr_remaining -= rqpv;\r\ntxr_remaining -= tqpv;\r\nrxr_idx++;\r\ntxr_idx++;\r\n}\r\nreturn 0;\r\nerr_out:\r\nadapter->num_tx_queues = 0;\r\nadapter->num_rx_queues = 0;\r\nadapter->num_q_vectors = 0;\r\nwhile (v_idx--)\r\nixgbe_free_q_vector(adapter, v_idx);\r\nreturn -ENOMEM;\r\n}\r\nstatic void ixgbe_free_q_vectors(struct ixgbe_adapter *adapter)\r\n{\r\nint v_idx = adapter->num_q_vectors;\r\nadapter->num_tx_queues = 0;\r\nadapter->num_rx_queues = 0;\r\nadapter->num_q_vectors = 0;\r\nwhile (v_idx--)\r\nixgbe_free_q_vector(adapter, v_idx);\r\n}\r\nstatic void ixgbe_reset_interrupt_capability(struct ixgbe_adapter *adapter)\r\n{\r\nif (adapter->flags & IXGBE_FLAG_MSIX_ENABLED) {\r\nadapter->flags &= ~IXGBE_FLAG_MSIX_ENABLED;\r\npci_disable_msix(adapter->pdev);\r\nkfree(adapter->msix_entries);\r\nadapter->msix_entries = NULL;\r\n} else if (adapter->flags & IXGBE_FLAG_MSI_ENABLED) {\r\nadapter->flags &= ~IXGBE_FLAG_MSI_ENABLED;\r\npci_disable_msi(adapter->pdev);\r\n}\r\n}\r\nstatic void ixgbe_set_interrupt_capability(struct ixgbe_adapter *adapter)\r\n{\r\nstruct ixgbe_hw *hw = &adapter->hw;\r\nint vector, v_budget, err;\r\nv_budget = max(adapter->num_rx_queues, adapter->num_tx_queues);\r\nv_budget = min_t(int, v_budget, num_online_cpus());\r\nv_budget += NON_Q_VECTORS;\r\nv_budget = min_t(int, v_budget, hw->mac.max_msix_vectors);\r\nadapter->msix_entries = kcalloc(v_budget,\r\nsizeof(struct msix_entry), GFP_KERNEL);\r\nif (adapter->msix_entries) {\r\nfor (vector = 0; vector < v_budget; vector++)\r\nadapter->msix_entries[vector].entry = vector;\r\nixgbe_acquire_msix_vectors(adapter, v_budget);\r\nif (adapter->flags & IXGBE_FLAG_MSIX_ENABLED)\r\nreturn;\r\n}\r\nif (netdev_get_num_tc(adapter->netdev) > 1) {\r\ne_err(probe, "num TCs exceeds number of queues - disabling DCB\n");\r\nnetdev_reset_tc(adapter->netdev);\r\nif (adapter->hw.mac.type == ixgbe_mac_82598EB)\r\nadapter->hw.fc.requested_mode = adapter->last_lfc_mode;\r\nadapter->flags &= ~IXGBE_FLAG_DCB_ENABLED;\r\nadapter->temp_dcb_cfg.pfc_mode_enable = false;\r\nadapter->dcb_cfg.pfc_mode_enable = false;\r\n}\r\nadapter->dcb_cfg.num_tcs.pg_tcs = 1;\r\nadapter->dcb_cfg.num_tcs.pfc_tcs = 1;\r\nixgbe_disable_sriov(adapter);\r\nadapter->ring_feature[RING_F_RSS].limit = 1;\r\nixgbe_set_num_queues(adapter);\r\nadapter->num_q_vectors = 1;\r\nerr = pci_enable_msi(adapter->pdev);\r\nif (err) {\r\nnetif_printk(adapter, hw, KERN_DEBUG, adapter->netdev,\r\n"Unable to allocate MSI interrupt, "\r\n"falling back to legacy. Error: %d\n", err);\r\nreturn;\r\n}\r\nadapter->flags |= IXGBE_FLAG_MSI_ENABLED;\r\n}\r\nint ixgbe_init_interrupt_scheme(struct ixgbe_adapter *adapter)\r\n{\r\nint err;\r\nixgbe_set_num_queues(adapter);\r\nixgbe_set_interrupt_capability(adapter);\r\nerr = ixgbe_alloc_q_vectors(adapter);\r\nif (err) {\r\ne_dev_err("Unable to allocate memory for queue vectors\n");\r\ngoto err_alloc_q_vectors;\r\n}\r\nixgbe_cache_ring_register(adapter);\r\ne_dev_info("Multiqueue %s: Rx Queue count = %u, Tx Queue count = %u\n",\r\n(adapter->num_rx_queues > 1) ? "Enabled" : "Disabled",\r\nadapter->num_rx_queues, adapter->num_tx_queues);\r\nset_bit(__IXGBE_DOWN, &adapter->state);\r\nreturn 0;\r\nerr_alloc_q_vectors:\r\nixgbe_reset_interrupt_capability(adapter);\r\nreturn err;\r\n}\r\nvoid ixgbe_clear_interrupt_scheme(struct ixgbe_adapter *adapter)\r\n{\r\nadapter->num_tx_queues = 0;\r\nadapter->num_rx_queues = 0;\r\nixgbe_free_q_vectors(adapter);\r\nixgbe_reset_interrupt_capability(adapter);\r\n}\r\nvoid ixgbe_tx_ctxtdesc(struct ixgbe_ring *tx_ring, u32 vlan_macip_lens,\r\nu32 fcoe_sof_eof, u32 type_tucmd, u32 mss_l4len_idx)\r\n{\r\nstruct ixgbe_adv_tx_context_desc *context_desc;\r\nu16 i = tx_ring->next_to_use;\r\ncontext_desc = IXGBE_TX_CTXTDESC(tx_ring, i);\r\ni++;\r\ntx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\r\ntype_tucmd |= IXGBE_TXD_CMD_DEXT | IXGBE_ADVTXD_DTYP_CTXT;\r\ncontext_desc->vlan_macip_lens = cpu_to_le32(vlan_macip_lens);\r\ncontext_desc->seqnum_seed = cpu_to_le32(fcoe_sof_eof);\r\ncontext_desc->type_tucmd_mlhl = cpu_to_le32(type_tucmd);\r\ncontext_desc->mss_l4len_idx = cpu_to_le32(mss_l4len_idx);\r\n}
