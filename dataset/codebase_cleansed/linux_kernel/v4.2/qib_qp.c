static inline unsigned mk_qpn(struct qib_qpn_table *qpt,\r\nstruct qpn_map *map, unsigned off)\r\n{\r\nreturn (map - qpt->map) * BITS_PER_PAGE + off;\r\n}\r\nstatic inline unsigned find_next_offset(struct qib_qpn_table *qpt,\r\nstruct qpn_map *map, unsigned off,\r\nunsigned n)\r\n{\r\nif (qpt->mask) {\r\noff++;\r\nif (((off & qpt->mask) >> 1) >= n)\r\noff = (off | qpt->mask) + 2;\r\n} else\r\noff = find_next_zero_bit(map->page, BITS_PER_PAGE, off);\r\nreturn off;\r\n}\r\nstatic void get_map_page(struct qib_qpn_table *qpt, struct qpn_map *map)\r\n{\r\nunsigned long page = get_zeroed_page(GFP_KERNEL);\r\nspin_lock(&qpt->lock);\r\nif (map->page)\r\nfree_page(page);\r\nelse\r\nmap->page = (void *)page;\r\nspin_unlock(&qpt->lock);\r\n}\r\nstatic int alloc_qpn(struct qib_devdata *dd, struct qib_qpn_table *qpt,\r\nenum ib_qp_type type, u8 port)\r\n{\r\nu32 i, offset, max_scan, qpn;\r\nstruct qpn_map *map;\r\nu32 ret;\r\nif (type == IB_QPT_SMI || type == IB_QPT_GSI) {\r\nunsigned n;\r\nret = type == IB_QPT_GSI;\r\nn = 1 << (ret + 2 * (port - 1));\r\nspin_lock(&qpt->lock);\r\nif (qpt->flags & n)\r\nret = -EINVAL;\r\nelse\r\nqpt->flags |= n;\r\nspin_unlock(&qpt->lock);\r\ngoto bail;\r\n}\r\nqpn = qpt->last + 2;\r\nif (qpn >= QPN_MAX)\r\nqpn = 2;\r\nif (qpt->mask && ((qpn & qpt->mask) >> 1) >= dd->n_krcv_queues)\r\nqpn = (qpn | qpt->mask) + 2;\r\noffset = qpn & BITS_PER_PAGE_MASK;\r\nmap = &qpt->map[qpn / BITS_PER_PAGE];\r\nmax_scan = qpt->nmaps - !offset;\r\nfor (i = 0;;) {\r\nif (unlikely(!map->page)) {\r\nget_map_page(qpt, map);\r\nif (unlikely(!map->page))\r\nbreak;\r\n}\r\ndo {\r\nif (!test_and_set_bit(offset, map->page)) {\r\nqpt->last = qpn;\r\nret = qpn;\r\ngoto bail;\r\n}\r\noffset = find_next_offset(qpt, map, offset,\r\ndd->n_krcv_queues);\r\nqpn = mk_qpn(qpt, map, offset);\r\n} while (offset < BITS_PER_PAGE && qpn < QPN_MAX);\r\nif (++i > max_scan) {\r\nif (qpt->nmaps == QPNMAP_ENTRIES)\r\nbreak;\r\nmap = &qpt->map[qpt->nmaps++];\r\noffset = 0;\r\n} else if (map < &qpt->map[qpt->nmaps]) {\r\n++map;\r\noffset = 0;\r\n} else {\r\nmap = &qpt->map[0];\r\noffset = 2;\r\n}\r\nqpn = mk_qpn(qpt, map, offset);\r\n}\r\nret = -ENOMEM;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic void free_qpn(struct qib_qpn_table *qpt, u32 qpn)\r\n{\r\nstruct qpn_map *map;\r\nmap = qpt->map + qpn / BITS_PER_PAGE;\r\nif (map->page)\r\nclear_bit(qpn & BITS_PER_PAGE_MASK, map->page);\r\n}\r\nstatic inline unsigned qpn_hash(struct qib_ibdev *dev, u32 qpn)\r\n{\r\nreturn jhash_1word(qpn, dev->qp_rnd) &\r\n(dev->qp_table_size - 1);\r\n}\r\nstatic void insert_qp(struct qib_ibdev *dev, struct qib_qp *qp)\r\n{\r\nstruct qib_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);\r\nunsigned long flags;\r\nunsigned n = qpn_hash(dev, qp->ibqp.qp_num);\r\natomic_inc(&qp->refcount);\r\nspin_lock_irqsave(&dev->qpt_lock, flags);\r\nif (qp->ibqp.qp_num == 0)\r\nrcu_assign_pointer(ibp->qp0, qp);\r\nelse if (qp->ibqp.qp_num == 1)\r\nrcu_assign_pointer(ibp->qp1, qp);\r\nelse {\r\nqp->next = dev->qp_table[n];\r\nrcu_assign_pointer(dev->qp_table[n], qp);\r\n}\r\nspin_unlock_irqrestore(&dev->qpt_lock, flags);\r\n}\r\nstatic void remove_qp(struct qib_ibdev *dev, struct qib_qp *qp)\r\n{\r\nstruct qib_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);\r\nunsigned n = qpn_hash(dev, qp->ibqp.qp_num);\r\nunsigned long flags;\r\nint removed = 1;\r\nspin_lock_irqsave(&dev->qpt_lock, flags);\r\nif (rcu_dereference_protected(ibp->qp0,\r\nlockdep_is_held(&dev->qpt_lock)) == qp) {\r\nRCU_INIT_POINTER(ibp->qp0, NULL);\r\n} else if (rcu_dereference_protected(ibp->qp1,\r\nlockdep_is_held(&dev->qpt_lock)) == qp) {\r\nRCU_INIT_POINTER(ibp->qp1, NULL);\r\n} else {\r\nstruct qib_qp *q;\r\nstruct qib_qp __rcu **qpp;\r\nremoved = 0;\r\nqpp = &dev->qp_table[n];\r\nfor (; (q = rcu_dereference_protected(*qpp,\r\nlockdep_is_held(&dev->qpt_lock))) != NULL;\r\nqpp = &q->next)\r\nif (q == qp) {\r\nRCU_INIT_POINTER(*qpp,\r\nrcu_dereference_protected(qp->next,\r\nlockdep_is_held(&dev->qpt_lock)));\r\nremoved = 1;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&dev->qpt_lock, flags);\r\nif (removed) {\r\nsynchronize_rcu();\r\natomic_dec(&qp->refcount);\r\n}\r\n}\r\nunsigned qib_free_all_qps(struct qib_devdata *dd)\r\n{\r\nstruct qib_ibdev *dev = &dd->verbs_dev;\r\nunsigned long flags;\r\nstruct qib_qp *qp;\r\nunsigned n, qp_inuse = 0;\r\nfor (n = 0; n < dd->num_pports; n++) {\r\nstruct qib_ibport *ibp = &dd->pport[n].ibport_data;\r\nif (!qib_mcast_tree_empty(ibp))\r\nqp_inuse++;\r\nrcu_read_lock();\r\nif (rcu_dereference(ibp->qp0))\r\nqp_inuse++;\r\nif (rcu_dereference(ibp->qp1))\r\nqp_inuse++;\r\nrcu_read_unlock();\r\n}\r\nspin_lock_irqsave(&dev->qpt_lock, flags);\r\nfor (n = 0; n < dev->qp_table_size; n++) {\r\nqp = rcu_dereference_protected(dev->qp_table[n],\r\nlockdep_is_held(&dev->qpt_lock));\r\nRCU_INIT_POINTER(dev->qp_table[n], NULL);\r\nfor (; qp; qp = rcu_dereference_protected(qp->next,\r\nlockdep_is_held(&dev->qpt_lock)))\r\nqp_inuse++;\r\n}\r\nspin_unlock_irqrestore(&dev->qpt_lock, flags);\r\nsynchronize_rcu();\r\nreturn qp_inuse;\r\n}\r\nstruct qib_qp *qib_lookup_qpn(struct qib_ibport *ibp, u32 qpn)\r\n{\r\nstruct qib_qp *qp = NULL;\r\nrcu_read_lock();\r\nif (unlikely(qpn <= 1)) {\r\nif (qpn == 0)\r\nqp = rcu_dereference(ibp->qp0);\r\nelse\r\nqp = rcu_dereference(ibp->qp1);\r\nif (qp)\r\natomic_inc(&qp->refcount);\r\n} else {\r\nstruct qib_ibdev *dev = &ppd_from_ibp(ibp)->dd->verbs_dev;\r\nunsigned n = qpn_hash(dev, qpn);\r\nfor (qp = rcu_dereference(dev->qp_table[n]); qp;\r\nqp = rcu_dereference(qp->next))\r\nif (qp->ibqp.qp_num == qpn) {\r\natomic_inc(&qp->refcount);\r\nbreak;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn qp;\r\n}\r\nstatic void qib_reset_qp(struct qib_qp *qp, enum ib_qp_type type)\r\n{\r\nqp->remote_qpn = 0;\r\nqp->qkey = 0;\r\nqp->qp_access_flags = 0;\r\natomic_set(&qp->s_dma_busy, 0);\r\nqp->s_flags &= QIB_S_SIGNAL_REQ_WR;\r\nqp->s_hdrwords = 0;\r\nqp->s_wqe = NULL;\r\nqp->s_draining = 0;\r\nqp->s_next_psn = 0;\r\nqp->s_last_psn = 0;\r\nqp->s_sending_psn = 0;\r\nqp->s_sending_hpsn = 0;\r\nqp->s_psn = 0;\r\nqp->r_psn = 0;\r\nqp->r_msn = 0;\r\nif (type == IB_QPT_RC) {\r\nqp->s_state = IB_OPCODE_RC_SEND_LAST;\r\nqp->r_state = IB_OPCODE_RC_SEND_LAST;\r\n} else {\r\nqp->s_state = IB_OPCODE_UC_SEND_LAST;\r\nqp->r_state = IB_OPCODE_UC_SEND_LAST;\r\n}\r\nqp->s_ack_state = IB_OPCODE_RC_ACKNOWLEDGE;\r\nqp->r_nak_state = 0;\r\nqp->r_aflags = 0;\r\nqp->r_flags = 0;\r\nqp->s_head = 0;\r\nqp->s_tail = 0;\r\nqp->s_cur = 0;\r\nqp->s_acked = 0;\r\nqp->s_last = 0;\r\nqp->s_ssn = 1;\r\nqp->s_lsn = 0;\r\nqp->s_mig_state = IB_MIG_MIGRATED;\r\nmemset(qp->s_ack_queue, 0, sizeof(qp->s_ack_queue));\r\nqp->r_head_ack_queue = 0;\r\nqp->s_tail_ack_queue = 0;\r\nqp->s_num_rd_atomic = 0;\r\nif (qp->r_rq.wq) {\r\nqp->r_rq.wq->head = 0;\r\nqp->r_rq.wq->tail = 0;\r\n}\r\nqp->r_sge.num_sge = 0;\r\n}\r\nstatic void clear_mr_refs(struct qib_qp *qp, int clr_sends)\r\n{\r\nunsigned n;\r\nif (test_and_clear_bit(QIB_R_REWIND_SGE, &qp->r_aflags))\r\nqib_put_ss(&qp->s_rdma_read_sge);\r\nqib_put_ss(&qp->r_sge);\r\nif (clr_sends) {\r\nwhile (qp->s_last != qp->s_head) {\r\nstruct qib_swqe *wqe = get_swqe_ptr(qp, qp->s_last);\r\nunsigned i;\r\nfor (i = 0; i < wqe->wr.num_sge; i++) {\r\nstruct qib_sge *sge = &wqe->sg_list[i];\r\nqib_put_mr(sge->mr);\r\n}\r\nif (qp->ibqp.qp_type == IB_QPT_UD ||\r\nqp->ibqp.qp_type == IB_QPT_SMI ||\r\nqp->ibqp.qp_type == IB_QPT_GSI)\r\natomic_dec(&to_iah(wqe->wr.wr.ud.ah)->refcount);\r\nif (++qp->s_last >= qp->s_size)\r\nqp->s_last = 0;\r\n}\r\nif (qp->s_rdma_mr) {\r\nqib_put_mr(qp->s_rdma_mr);\r\nqp->s_rdma_mr = NULL;\r\n}\r\n}\r\nif (qp->ibqp.qp_type != IB_QPT_RC)\r\nreturn;\r\nfor (n = 0; n < ARRAY_SIZE(qp->s_ack_queue); n++) {\r\nstruct qib_ack_entry *e = &qp->s_ack_queue[n];\r\nif (e->opcode == IB_OPCODE_RC_RDMA_READ_REQUEST &&\r\ne->rdma_sge.mr) {\r\nqib_put_mr(e->rdma_sge.mr);\r\ne->rdma_sge.mr = NULL;\r\n}\r\n}\r\n}\r\nint qib_error_qp(struct qib_qp *qp, enum ib_wc_status err)\r\n{\r\nstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct ib_wc wc;\r\nint ret = 0;\r\nif (qp->state == IB_QPS_ERR || qp->state == IB_QPS_RESET)\r\ngoto bail;\r\nqp->state = IB_QPS_ERR;\r\nif (qp->s_flags & (QIB_S_TIMER | QIB_S_WAIT_RNR)) {\r\nqp->s_flags &= ~(QIB_S_TIMER | QIB_S_WAIT_RNR);\r\ndel_timer(&qp->s_timer);\r\n}\r\nif (qp->s_flags & QIB_S_ANY_WAIT_SEND)\r\nqp->s_flags &= ~QIB_S_ANY_WAIT_SEND;\r\nspin_lock(&dev->pending_lock);\r\nif (!list_empty(&qp->iowait) && !(qp->s_flags & QIB_S_BUSY)) {\r\nqp->s_flags &= ~QIB_S_ANY_WAIT_IO;\r\nlist_del_init(&qp->iowait);\r\n}\r\nspin_unlock(&dev->pending_lock);\r\nif (!(qp->s_flags & QIB_S_BUSY)) {\r\nqp->s_hdrwords = 0;\r\nif (qp->s_rdma_mr) {\r\nqib_put_mr(qp->s_rdma_mr);\r\nqp->s_rdma_mr = NULL;\r\n}\r\nif (qp->s_tx) {\r\nqib_put_txreq(qp->s_tx);\r\nqp->s_tx = NULL;\r\n}\r\n}\r\nif (qp->s_last != qp->s_head)\r\nqib_schedule_send(qp);\r\nclear_mr_refs(qp, 0);\r\nmemset(&wc, 0, sizeof(wc));\r\nwc.qp = &qp->ibqp;\r\nwc.opcode = IB_WC_RECV;\r\nif (test_and_clear_bit(QIB_R_WRID_VALID, &qp->r_aflags)) {\r\nwc.wr_id = qp->r_wr_id;\r\nwc.status = err;\r\nqib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);\r\n}\r\nwc.status = IB_WC_WR_FLUSH_ERR;\r\nif (qp->r_rq.wq) {\r\nstruct qib_rwq *wq;\r\nu32 head;\r\nu32 tail;\r\nspin_lock(&qp->r_rq.lock);\r\nwq = qp->r_rq.wq;\r\nhead = wq->head;\r\nif (head >= qp->r_rq.size)\r\nhead = 0;\r\ntail = wq->tail;\r\nif (tail >= qp->r_rq.size)\r\ntail = 0;\r\nwhile (tail != head) {\r\nwc.wr_id = get_rwqe_ptr(&qp->r_rq, tail)->wr_id;\r\nif (++tail >= qp->r_rq.size)\r\ntail = 0;\r\nqib_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);\r\n}\r\nwq->tail = tail;\r\nspin_unlock(&qp->r_rq.lock);\r\n} else if (qp->ibqp.event_handler)\r\nret = 1;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\r\nint attr_mask, struct ib_udata *udata)\r\n{\r\nstruct qib_ibdev *dev = to_idev(ibqp->device);\r\nstruct qib_qp *qp = to_iqp(ibqp);\r\nenum ib_qp_state cur_state, new_state;\r\nstruct ib_event ev;\r\nint lastwqe = 0;\r\nint mig = 0;\r\nint ret;\r\nu32 pmtu = 0;\r\nspin_lock_irq(&qp->r_lock);\r\nspin_lock(&qp->s_lock);\r\ncur_state = attr_mask & IB_QP_CUR_STATE ?\r\nattr->cur_qp_state : qp->state;\r\nnew_state = attr_mask & IB_QP_STATE ? attr->qp_state : cur_state;\r\nif (!ib_modify_qp_is_ok(cur_state, new_state, ibqp->qp_type,\r\nattr_mask, IB_LINK_LAYER_UNSPECIFIED))\r\ngoto inval;\r\nif (attr_mask & IB_QP_AV) {\r\nif (attr->ah_attr.dlid >= QIB_MULTICAST_LID_BASE)\r\ngoto inval;\r\nif (qib_check_ah(qp->ibqp.device, &attr->ah_attr))\r\ngoto inval;\r\n}\r\nif (attr_mask & IB_QP_ALT_PATH) {\r\nif (attr->alt_ah_attr.dlid >= QIB_MULTICAST_LID_BASE)\r\ngoto inval;\r\nif (qib_check_ah(qp->ibqp.device, &attr->alt_ah_attr))\r\ngoto inval;\r\nif (attr->alt_pkey_index >= qib_get_npkeys(dd_from_dev(dev)))\r\ngoto inval;\r\n}\r\nif (attr_mask & IB_QP_PKEY_INDEX)\r\nif (attr->pkey_index >= qib_get_npkeys(dd_from_dev(dev)))\r\ngoto inval;\r\nif (attr_mask & IB_QP_MIN_RNR_TIMER)\r\nif (attr->min_rnr_timer > 31)\r\ngoto inval;\r\nif (attr_mask & IB_QP_PORT)\r\nif (qp->ibqp.qp_type == IB_QPT_SMI ||\r\nqp->ibqp.qp_type == IB_QPT_GSI ||\r\nattr->port_num == 0 ||\r\nattr->port_num > ibqp->device->phys_port_cnt)\r\ngoto inval;\r\nif (attr_mask & IB_QP_DEST_QPN)\r\nif (attr->dest_qp_num > QIB_QPN_MASK)\r\ngoto inval;\r\nif (attr_mask & IB_QP_RETRY_CNT)\r\nif (attr->retry_cnt > 7)\r\ngoto inval;\r\nif (attr_mask & IB_QP_RNR_RETRY)\r\nif (attr->rnr_retry > 7)\r\ngoto inval;\r\nif (attr_mask & IB_QP_PATH_MTU) {\r\nstruct qib_devdata *dd = dd_from_dev(dev);\r\nint mtu, pidx = qp->port_num - 1;\r\nmtu = ib_mtu_enum_to_int(attr->path_mtu);\r\nif (mtu == -1)\r\ngoto inval;\r\nif (mtu > dd->pport[pidx].ibmtu) {\r\nswitch (dd->pport[pidx].ibmtu) {\r\ncase 4096:\r\npmtu = IB_MTU_4096;\r\nbreak;\r\ncase 2048:\r\npmtu = IB_MTU_2048;\r\nbreak;\r\ncase 1024:\r\npmtu = IB_MTU_1024;\r\nbreak;\r\ncase 512:\r\npmtu = IB_MTU_512;\r\nbreak;\r\ncase 256:\r\npmtu = IB_MTU_256;\r\nbreak;\r\ndefault:\r\npmtu = IB_MTU_2048;\r\n}\r\n} else\r\npmtu = attr->path_mtu;\r\n}\r\nif (attr_mask & IB_QP_PATH_MIG_STATE) {\r\nif (attr->path_mig_state == IB_MIG_REARM) {\r\nif (qp->s_mig_state == IB_MIG_ARMED)\r\ngoto inval;\r\nif (new_state != IB_QPS_RTS)\r\ngoto inval;\r\n} else if (attr->path_mig_state == IB_MIG_MIGRATED) {\r\nif (qp->s_mig_state == IB_MIG_REARM)\r\ngoto inval;\r\nif (new_state != IB_QPS_RTS && new_state != IB_QPS_SQD)\r\ngoto inval;\r\nif (qp->s_mig_state == IB_MIG_ARMED)\r\nmig = 1;\r\n} else\r\ngoto inval;\r\n}\r\nif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)\r\nif (attr->max_dest_rd_atomic > QIB_MAX_RDMA_ATOMIC)\r\ngoto inval;\r\nswitch (new_state) {\r\ncase IB_QPS_RESET:\r\nif (qp->state != IB_QPS_RESET) {\r\nqp->state = IB_QPS_RESET;\r\nspin_lock(&dev->pending_lock);\r\nif (!list_empty(&qp->iowait))\r\nlist_del_init(&qp->iowait);\r\nspin_unlock(&dev->pending_lock);\r\nqp->s_flags &= ~(QIB_S_TIMER | QIB_S_ANY_WAIT);\r\nspin_unlock(&qp->s_lock);\r\nspin_unlock_irq(&qp->r_lock);\r\ncancel_work_sync(&qp->s_work);\r\ndel_timer_sync(&qp->s_timer);\r\nwait_event(qp->wait_dma, !atomic_read(&qp->s_dma_busy));\r\nif (qp->s_tx) {\r\nqib_put_txreq(qp->s_tx);\r\nqp->s_tx = NULL;\r\n}\r\nremove_qp(dev, qp);\r\nwait_event(qp->wait, !atomic_read(&qp->refcount));\r\nspin_lock_irq(&qp->r_lock);\r\nspin_lock(&qp->s_lock);\r\nclear_mr_refs(qp, 1);\r\nqib_reset_qp(qp, ibqp->qp_type);\r\n}\r\nbreak;\r\ncase IB_QPS_RTR:\r\nqp->r_flags &= ~QIB_R_COMM_EST;\r\nqp->state = new_state;\r\nbreak;\r\ncase IB_QPS_SQD:\r\nqp->s_draining = qp->s_last != qp->s_cur;\r\nqp->state = new_state;\r\nbreak;\r\ncase IB_QPS_SQE:\r\nif (qp->ibqp.qp_type == IB_QPT_RC)\r\ngoto inval;\r\nqp->state = new_state;\r\nbreak;\r\ncase IB_QPS_ERR:\r\nlastwqe = qib_error_qp(qp, IB_WC_WR_FLUSH_ERR);\r\nbreak;\r\ndefault:\r\nqp->state = new_state;\r\nbreak;\r\n}\r\nif (attr_mask & IB_QP_PKEY_INDEX)\r\nqp->s_pkey_index = attr->pkey_index;\r\nif (attr_mask & IB_QP_PORT)\r\nqp->port_num = attr->port_num;\r\nif (attr_mask & IB_QP_DEST_QPN)\r\nqp->remote_qpn = attr->dest_qp_num;\r\nif (attr_mask & IB_QP_SQ_PSN) {\r\nqp->s_next_psn = attr->sq_psn & QIB_PSN_MASK;\r\nqp->s_psn = qp->s_next_psn;\r\nqp->s_sending_psn = qp->s_next_psn;\r\nqp->s_last_psn = qp->s_next_psn - 1;\r\nqp->s_sending_hpsn = qp->s_last_psn;\r\n}\r\nif (attr_mask & IB_QP_RQ_PSN)\r\nqp->r_psn = attr->rq_psn & QIB_PSN_MASK;\r\nif (attr_mask & IB_QP_ACCESS_FLAGS)\r\nqp->qp_access_flags = attr->qp_access_flags;\r\nif (attr_mask & IB_QP_AV) {\r\nqp->remote_ah_attr = attr->ah_attr;\r\nqp->s_srate = attr->ah_attr.static_rate;\r\n}\r\nif (attr_mask & IB_QP_ALT_PATH) {\r\nqp->alt_ah_attr = attr->alt_ah_attr;\r\nqp->s_alt_pkey_index = attr->alt_pkey_index;\r\n}\r\nif (attr_mask & IB_QP_PATH_MIG_STATE) {\r\nqp->s_mig_state = attr->path_mig_state;\r\nif (mig) {\r\nqp->remote_ah_attr = qp->alt_ah_attr;\r\nqp->port_num = qp->alt_ah_attr.port_num;\r\nqp->s_pkey_index = qp->s_alt_pkey_index;\r\n}\r\n}\r\nif (attr_mask & IB_QP_PATH_MTU) {\r\nqp->path_mtu = pmtu;\r\nqp->pmtu = ib_mtu_enum_to_int(pmtu);\r\n}\r\nif (attr_mask & IB_QP_RETRY_CNT) {\r\nqp->s_retry_cnt = attr->retry_cnt;\r\nqp->s_retry = attr->retry_cnt;\r\n}\r\nif (attr_mask & IB_QP_RNR_RETRY) {\r\nqp->s_rnr_retry_cnt = attr->rnr_retry;\r\nqp->s_rnr_retry = attr->rnr_retry;\r\n}\r\nif (attr_mask & IB_QP_MIN_RNR_TIMER)\r\nqp->r_min_rnr_timer = attr->min_rnr_timer;\r\nif (attr_mask & IB_QP_TIMEOUT) {\r\nqp->timeout = attr->timeout;\r\nqp->timeout_jiffies =\r\nusecs_to_jiffies((4096UL * (1UL << qp->timeout)) /\r\n1000UL);\r\n}\r\nif (attr_mask & IB_QP_QKEY)\r\nqp->qkey = attr->qkey;\r\nif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)\r\nqp->r_max_rd_atomic = attr->max_dest_rd_atomic;\r\nif (attr_mask & IB_QP_MAX_QP_RD_ATOMIC)\r\nqp->s_max_rd_atomic = attr->max_rd_atomic;\r\nspin_unlock(&qp->s_lock);\r\nspin_unlock_irq(&qp->r_lock);\r\nif (cur_state == IB_QPS_RESET && new_state == IB_QPS_INIT)\r\ninsert_qp(dev, qp);\r\nif (lastwqe) {\r\nev.device = qp->ibqp.device;\r\nev.element.qp = &qp->ibqp;\r\nev.event = IB_EVENT_QP_LAST_WQE_REACHED;\r\nqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\r\n}\r\nif (mig) {\r\nev.device = qp->ibqp.device;\r\nev.element.qp = &qp->ibqp;\r\nev.event = IB_EVENT_PATH_MIG;\r\nqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\r\n}\r\nret = 0;\r\ngoto bail;\r\ninval:\r\nspin_unlock(&qp->s_lock);\r\nspin_unlock_irq(&qp->r_lock);\r\nret = -EINVAL;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\r\nint attr_mask, struct ib_qp_init_attr *init_attr)\r\n{\r\nstruct qib_qp *qp = to_iqp(ibqp);\r\nattr->qp_state = qp->state;\r\nattr->cur_qp_state = attr->qp_state;\r\nattr->path_mtu = qp->path_mtu;\r\nattr->path_mig_state = qp->s_mig_state;\r\nattr->qkey = qp->qkey;\r\nattr->rq_psn = qp->r_psn & QIB_PSN_MASK;\r\nattr->sq_psn = qp->s_next_psn & QIB_PSN_MASK;\r\nattr->dest_qp_num = qp->remote_qpn;\r\nattr->qp_access_flags = qp->qp_access_flags;\r\nattr->cap.max_send_wr = qp->s_size - 1;\r\nattr->cap.max_recv_wr = qp->ibqp.srq ? 0 : qp->r_rq.size - 1;\r\nattr->cap.max_send_sge = qp->s_max_sge;\r\nattr->cap.max_recv_sge = qp->r_rq.max_sge;\r\nattr->cap.max_inline_data = 0;\r\nattr->ah_attr = qp->remote_ah_attr;\r\nattr->alt_ah_attr = qp->alt_ah_attr;\r\nattr->pkey_index = qp->s_pkey_index;\r\nattr->alt_pkey_index = qp->s_alt_pkey_index;\r\nattr->en_sqd_async_notify = 0;\r\nattr->sq_draining = qp->s_draining;\r\nattr->max_rd_atomic = qp->s_max_rd_atomic;\r\nattr->max_dest_rd_atomic = qp->r_max_rd_atomic;\r\nattr->min_rnr_timer = qp->r_min_rnr_timer;\r\nattr->port_num = qp->port_num;\r\nattr->timeout = qp->timeout;\r\nattr->retry_cnt = qp->s_retry_cnt;\r\nattr->rnr_retry = qp->s_rnr_retry_cnt;\r\nattr->alt_port_num = qp->alt_ah_attr.port_num;\r\nattr->alt_timeout = qp->alt_timeout;\r\ninit_attr->event_handler = qp->ibqp.event_handler;\r\ninit_attr->qp_context = qp->ibqp.qp_context;\r\ninit_attr->send_cq = qp->ibqp.send_cq;\r\ninit_attr->recv_cq = qp->ibqp.recv_cq;\r\ninit_attr->srq = qp->ibqp.srq;\r\ninit_attr->cap = attr->cap;\r\nif (qp->s_flags & QIB_S_SIGNAL_REQ_WR)\r\ninit_attr->sq_sig_type = IB_SIGNAL_REQ_WR;\r\nelse\r\ninit_attr->sq_sig_type = IB_SIGNAL_ALL_WR;\r\ninit_attr->qp_type = qp->ibqp.qp_type;\r\ninit_attr->port_num = qp->port_num;\r\nreturn 0;\r\n}\r\n__be32 qib_compute_aeth(struct qib_qp *qp)\r\n{\r\nu32 aeth = qp->r_msn & QIB_MSN_MASK;\r\nif (qp->ibqp.srq) {\r\naeth |= QIB_AETH_CREDIT_INVAL << QIB_AETH_CREDIT_SHIFT;\r\n} else {\r\nu32 min, max, x;\r\nu32 credits;\r\nstruct qib_rwq *wq = qp->r_rq.wq;\r\nu32 head;\r\nu32 tail;\r\nhead = wq->head;\r\nif (head >= qp->r_rq.size)\r\nhead = 0;\r\ntail = wq->tail;\r\nif (tail >= qp->r_rq.size)\r\ntail = 0;\r\ncredits = head - tail;\r\nif ((int)credits < 0)\r\ncredits += qp->r_rq.size;\r\nmin = 0;\r\nmax = 31;\r\nfor (;;) {\r\nx = (min + max) / 2;\r\nif (credit_table[x] == credits)\r\nbreak;\r\nif (credit_table[x] > credits)\r\nmax = x;\r\nelse if (min == x)\r\nbreak;\r\nelse\r\nmin = x;\r\n}\r\naeth |= x << QIB_AETH_CREDIT_SHIFT;\r\n}\r\nreturn cpu_to_be32(aeth);\r\n}\r\nstruct ib_qp *qib_create_qp(struct ib_pd *ibpd,\r\nstruct ib_qp_init_attr *init_attr,\r\nstruct ib_udata *udata)\r\n{\r\nstruct qib_qp *qp;\r\nint err;\r\nstruct qib_swqe *swq = NULL;\r\nstruct qib_ibdev *dev;\r\nstruct qib_devdata *dd;\r\nsize_t sz;\r\nsize_t sg_list_sz;\r\nstruct ib_qp *ret;\r\nif (init_attr->cap.max_send_sge > ib_qib_max_sges ||\r\ninit_attr->cap.max_send_wr > ib_qib_max_qp_wrs ||\r\ninit_attr->create_flags) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nif (!init_attr->srq) {\r\nif (init_attr->cap.max_recv_sge > ib_qib_max_sges ||\r\ninit_attr->cap.max_recv_wr > ib_qib_max_qp_wrs) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nif (init_attr->cap.max_send_sge +\r\ninit_attr->cap.max_send_wr +\r\ninit_attr->cap.max_recv_sge +\r\ninit_attr->cap.max_recv_wr == 0) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\n}\r\nswitch (init_attr->qp_type) {\r\ncase IB_QPT_SMI:\r\ncase IB_QPT_GSI:\r\nif (init_attr->port_num == 0 ||\r\ninit_attr->port_num > ibpd->device->phys_port_cnt) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\ncase IB_QPT_UC:\r\ncase IB_QPT_RC:\r\ncase IB_QPT_UD:\r\nsz = sizeof(struct qib_sge) *\r\ninit_attr->cap.max_send_sge +\r\nsizeof(struct qib_swqe);\r\nswq = vmalloc((init_attr->cap.max_send_wr + 1) * sz);\r\nif (swq == NULL) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nsz = sizeof(*qp);\r\nsg_list_sz = 0;\r\nif (init_attr->srq) {\r\nstruct qib_srq *srq = to_isrq(init_attr->srq);\r\nif (srq->rq.max_sge > 1)\r\nsg_list_sz = sizeof(*qp->r_sg_list) *\r\n(srq->rq.max_sge - 1);\r\n} else if (init_attr->cap.max_recv_sge > 1)\r\nsg_list_sz = sizeof(*qp->r_sg_list) *\r\n(init_attr->cap.max_recv_sge - 1);\r\nqp = kzalloc(sz + sg_list_sz, GFP_KERNEL);\r\nif (!qp) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_swq;\r\n}\r\nRCU_INIT_POINTER(qp->next, NULL);\r\nqp->s_hdr = kzalloc(sizeof(*qp->s_hdr), GFP_KERNEL);\r\nif (!qp->s_hdr) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_qp;\r\n}\r\nqp->timeout_jiffies =\r\nusecs_to_jiffies((4096UL * (1UL << qp->timeout)) /\r\n1000UL);\r\nif (init_attr->srq)\r\nsz = 0;\r\nelse {\r\nqp->r_rq.size = init_attr->cap.max_recv_wr + 1;\r\nqp->r_rq.max_sge = init_attr->cap.max_recv_sge;\r\nsz = (sizeof(struct ib_sge) * qp->r_rq.max_sge) +\r\nsizeof(struct qib_rwqe);\r\nqp->r_rq.wq = vmalloc_user(sizeof(struct qib_rwq) +\r\nqp->r_rq.size * sz);\r\nif (!qp->r_rq.wq) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_qp;\r\n}\r\n}\r\nspin_lock_init(&qp->r_lock);\r\nspin_lock_init(&qp->s_lock);\r\nspin_lock_init(&qp->r_rq.lock);\r\natomic_set(&qp->refcount, 0);\r\ninit_waitqueue_head(&qp->wait);\r\ninit_waitqueue_head(&qp->wait_dma);\r\ninit_timer(&qp->s_timer);\r\nqp->s_timer.data = (unsigned long)qp;\r\nINIT_WORK(&qp->s_work, qib_do_send);\r\nINIT_LIST_HEAD(&qp->iowait);\r\nINIT_LIST_HEAD(&qp->rspwait);\r\nqp->state = IB_QPS_RESET;\r\nqp->s_wq = swq;\r\nqp->s_size = init_attr->cap.max_send_wr + 1;\r\nqp->s_max_sge = init_attr->cap.max_send_sge;\r\nif (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)\r\nqp->s_flags = QIB_S_SIGNAL_REQ_WR;\r\ndev = to_idev(ibpd->device);\r\ndd = dd_from_dev(dev);\r\nerr = alloc_qpn(dd, &dev->qpn_table, init_attr->qp_type,\r\ninit_attr->port_num);\r\nif (err < 0) {\r\nret = ERR_PTR(err);\r\nvfree(qp->r_rq.wq);\r\ngoto bail_qp;\r\n}\r\nqp->ibqp.qp_num = err;\r\nqp->port_num = init_attr->port_num;\r\nqib_reset_qp(qp, init_attr->qp_type);\r\nbreak;\r\ndefault:\r\nret = ERR_PTR(-ENOSYS);\r\ngoto bail;\r\n}\r\ninit_attr->cap.max_inline_data = 0;\r\nif (udata && udata->outlen >= sizeof(__u64)) {\r\nif (!qp->r_rq.wq) {\r\n__u64 offset = 0;\r\nerr = ib_copy_to_udata(udata, &offset,\r\nsizeof(offset));\r\nif (err) {\r\nret = ERR_PTR(err);\r\ngoto bail_ip;\r\n}\r\n} else {\r\nu32 s = sizeof(struct qib_rwq) + qp->r_rq.size * sz;\r\nqp->ip = qib_create_mmap_info(dev, s,\r\nibpd->uobject->context,\r\nqp->r_rq.wq);\r\nif (!qp->ip) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_ip;\r\n}\r\nerr = ib_copy_to_udata(udata, &(qp->ip->offset),\r\nsizeof(qp->ip->offset));\r\nif (err) {\r\nret = ERR_PTR(err);\r\ngoto bail_ip;\r\n}\r\n}\r\n}\r\nspin_lock(&dev->n_qps_lock);\r\nif (dev->n_qps_allocated == ib_qib_max_qps) {\r\nspin_unlock(&dev->n_qps_lock);\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_ip;\r\n}\r\ndev->n_qps_allocated++;\r\nspin_unlock(&dev->n_qps_lock);\r\nif (qp->ip) {\r\nspin_lock_irq(&dev->pending_lock);\r\nlist_add(&qp->ip->pending_mmaps, &dev->pending_mmaps);\r\nspin_unlock_irq(&dev->pending_lock);\r\n}\r\nret = &qp->ibqp;\r\ngoto bail;\r\nbail_ip:\r\nif (qp->ip)\r\nkref_put(&qp->ip->ref, qib_release_mmap_info);\r\nelse\r\nvfree(qp->r_rq.wq);\r\nfree_qpn(&dev->qpn_table, qp->ibqp.qp_num);\r\nbail_qp:\r\nkfree(qp->s_hdr);\r\nkfree(qp);\r\nbail_swq:\r\nvfree(swq);\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_destroy_qp(struct ib_qp *ibqp)\r\n{\r\nstruct qib_qp *qp = to_iqp(ibqp);\r\nstruct qib_ibdev *dev = to_idev(ibqp->device);\r\nspin_lock_irq(&qp->s_lock);\r\nif (qp->state != IB_QPS_RESET) {\r\nqp->state = IB_QPS_RESET;\r\nspin_lock(&dev->pending_lock);\r\nif (!list_empty(&qp->iowait))\r\nlist_del_init(&qp->iowait);\r\nspin_unlock(&dev->pending_lock);\r\nqp->s_flags &= ~(QIB_S_TIMER | QIB_S_ANY_WAIT);\r\nspin_unlock_irq(&qp->s_lock);\r\ncancel_work_sync(&qp->s_work);\r\ndel_timer_sync(&qp->s_timer);\r\nwait_event(qp->wait_dma, !atomic_read(&qp->s_dma_busy));\r\nif (qp->s_tx) {\r\nqib_put_txreq(qp->s_tx);\r\nqp->s_tx = NULL;\r\n}\r\nremove_qp(dev, qp);\r\nwait_event(qp->wait, !atomic_read(&qp->refcount));\r\nclear_mr_refs(qp, 1);\r\n} else\r\nspin_unlock_irq(&qp->s_lock);\r\nfree_qpn(&dev->qpn_table, qp->ibqp.qp_num);\r\nspin_lock(&dev->n_qps_lock);\r\ndev->n_qps_allocated--;\r\nspin_unlock(&dev->n_qps_lock);\r\nif (qp->ip)\r\nkref_put(&qp->ip->ref, qib_release_mmap_info);\r\nelse\r\nvfree(qp->r_rq.wq);\r\nvfree(qp->s_wq);\r\nkfree(qp->s_hdr);\r\nkfree(qp);\r\nreturn 0;\r\n}\r\nvoid qib_init_qpn_table(struct qib_devdata *dd, struct qib_qpn_table *qpt)\r\n{\r\nspin_lock_init(&qpt->lock);\r\nqpt->last = 1;\r\nqpt->nmaps = 1;\r\nqpt->mask = dd->qpn_mask;\r\n}\r\nvoid qib_free_qpn_table(struct qib_qpn_table *qpt)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(qpt->map); i++)\r\nif (qpt->map[i].page)\r\nfree_page((unsigned long) qpt->map[i].page);\r\n}\r\nvoid qib_get_credit(struct qib_qp *qp, u32 aeth)\r\n{\r\nu32 credit = (aeth >> QIB_AETH_CREDIT_SHIFT) & QIB_AETH_CREDIT_MASK;\r\nif (credit == QIB_AETH_CREDIT_INVAL) {\r\nif (!(qp->s_flags & QIB_S_UNLIMITED_CREDIT)) {\r\nqp->s_flags |= QIB_S_UNLIMITED_CREDIT;\r\nif (qp->s_flags & QIB_S_WAIT_SSN_CREDIT) {\r\nqp->s_flags &= ~QIB_S_WAIT_SSN_CREDIT;\r\nqib_schedule_send(qp);\r\n}\r\n}\r\n} else if (!(qp->s_flags & QIB_S_UNLIMITED_CREDIT)) {\r\ncredit = (aeth + credit_table[credit]) & QIB_MSN_MASK;\r\nif (qib_cmp24(credit, qp->s_lsn) > 0) {\r\nqp->s_lsn = credit;\r\nif (qp->s_flags & QIB_S_WAIT_SSN_CREDIT) {\r\nqp->s_flags &= ~QIB_S_WAIT_SSN_CREDIT;\r\nqib_schedule_send(qp);\r\n}\r\n}\r\n}\r\n}\r\nstruct qib_qp_iter *qib_qp_iter_init(struct qib_ibdev *dev)\r\n{\r\nstruct qib_qp_iter *iter;\r\niter = kzalloc(sizeof(*iter), GFP_KERNEL);\r\nif (!iter)\r\nreturn NULL;\r\niter->dev = dev;\r\nif (qib_qp_iter_next(iter)) {\r\nkfree(iter);\r\nreturn NULL;\r\n}\r\nreturn iter;\r\n}\r\nint qib_qp_iter_next(struct qib_qp_iter *iter)\r\n{\r\nstruct qib_ibdev *dev = iter->dev;\r\nint n = iter->n;\r\nint ret = 1;\r\nstruct qib_qp *pqp = iter->qp;\r\nstruct qib_qp *qp;\r\nfor (; n < dev->qp_table_size; n++) {\r\nif (pqp)\r\nqp = rcu_dereference(pqp->next);\r\nelse\r\nqp = rcu_dereference(dev->qp_table[n]);\r\npqp = qp;\r\nif (qp) {\r\niter->qp = qp;\r\niter->n = n;\r\nreturn 0;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nvoid qib_qp_iter_print(struct seq_file *s, struct qib_qp_iter *iter)\r\n{\r\nstruct qib_swqe *wqe;\r\nstruct qib_qp *qp = iter->qp;\r\nwqe = get_swqe_ptr(qp, qp->s_last);\r\nseq_printf(s,\r\n"N %d QP%u %s %u %u %u f=%x %u %u %u %u %u PSN %x %x %x %x %x (%u %u %u %u %u %u) QP%u LID %x\n",\r\niter->n,\r\nqp->ibqp.qp_num,\r\nqp_type_str[qp->ibqp.qp_type],\r\nqp->state,\r\nwqe->wr.opcode,\r\nqp->s_hdrwords,\r\nqp->s_flags,\r\natomic_read(&qp->s_dma_busy),\r\n!list_empty(&qp->iowait),\r\nqp->timeout,\r\nwqe->ssn,\r\nqp->s_lsn,\r\nqp->s_last_psn,\r\nqp->s_psn, qp->s_next_psn,\r\nqp->s_sending_psn, qp->s_sending_hpsn,\r\nqp->s_last, qp->s_acked, qp->s_cur,\r\nqp->s_tail, qp->s_head, qp->s_size,\r\nqp->remote_qpn,\r\nqp->remote_ah_attr.dlid);\r\n}
