static void *real_vmalloc_addr(void *x)\r\n{\r\nunsigned long addr = (unsigned long) x;\r\npte_t *p;\r\np = __find_linux_pte_or_hugepte(swapper_pg_dir, addr, NULL);\r\nif (!p || !pte_present(*p))\r\nreturn NULL;\r\naddr = (pte_pfn(*p) << PAGE_SHIFT) | (addr & ~PAGE_MASK);\r\nreturn __va(addr);\r\n}\r\nstatic int global_invalidates(struct kvm *kvm, unsigned long flags)\r\n{\r\nint global;\r\nif (kvm->arch.online_vcores == 1 && local_paca->kvm_hstate.kvm_vcpu)\r\nglobal = 0;\r\nelse\r\nglobal = 1;\r\nif (!global) {\r\nsmp_wmb();\r\ncpumask_setall(&kvm->arch.need_tlb_flush);\r\ncpumask_clear_cpu(local_paca->kvm_hstate.kvm_vcore->pcpu,\r\n&kvm->arch.need_tlb_flush);\r\n}\r\nreturn global;\r\n}\r\nvoid kvmppc_add_revmap_chain(struct kvm *kvm, struct revmap_entry *rev,\r\nunsigned long *rmap, long pte_index, int realmode)\r\n{\r\nstruct revmap_entry *head, *tail;\r\nunsigned long i;\r\nif (*rmap & KVMPPC_RMAP_PRESENT) {\r\ni = *rmap & KVMPPC_RMAP_INDEX;\r\nhead = &kvm->arch.revmap[i];\r\nif (realmode)\r\nhead = real_vmalloc_addr(head);\r\ntail = &kvm->arch.revmap[head->back];\r\nif (realmode)\r\ntail = real_vmalloc_addr(tail);\r\nrev->forw = i;\r\nrev->back = head->back;\r\ntail->forw = pte_index;\r\nhead->back = pte_index;\r\n} else {\r\nrev->forw = rev->back = pte_index;\r\n*rmap = (*rmap & ~KVMPPC_RMAP_INDEX) |\r\npte_index | KVMPPC_RMAP_PRESENT;\r\n}\r\nunlock_rmap(rmap);\r\n}\r\nstatic void remove_revmap_chain(struct kvm *kvm, long pte_index,\r\nstruct revmap_entry *rev,\r\nunsigned long hpte_v, unsigned long hpte_r)\r\n{\r\nstruct revmap_entry *next, *prev;\r\nunsigned long gfn, ptel, head;\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long *rmap;\r\nunsigned long rcbits;\r\nrcbits = hpte_r & (HPTE_R_R | HPTE_R_C);\r\nptel = rev->guest_rpte |= rcbits;\r\ngfn = hpte_rpn(ptel, hpte_page_size(hpte_v, ptel));\r\nmemslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);\r\nif (!memslot)\r\nreturn;\r\nrmap = real_vmalloc_addr(&memslot->arch.rmap[gfn - memslot->base_gfn]);\r\nlock_rmap(rmap);\r\nhead = *rmap & KVMPPC_RMAP_INDEX;\r\nnext = real_vmalloc_addr(&kvm->arch.revmap[rev->forw]);\r\nprev = real_vmalloc_addr(&kvm->arch.revmap[rev->back]);\r\nnext->back = rev->back;\r\nprev->forw = rev->forw;\r\nif (head == pte_index) {\r\nhead = rev->forw;\r\nif (head == pte_index)\r\n*rmap &= ~(KVMPPC_RMAP_PRESENT | KVMPPC_RMAP_INDEX);\r\nelse\r\n*rmap = (*rmap & ~KVMPPC_RMAP_INDEX) | head;\r\n}\r\n*rmap |= rcbits << KVMPPC_RMAP_RC_SHIFT;\r\nunlock_rmap(rmap);\r\n}\r\nlong kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,\r\nlong pte_index, unsigned long pteh, unsigned long ptel,\r\npgd_t *pgdir, bool realmode, unsigned long *pte_idx_ret)\r\n{\r\nunsigned long i, pa, gpa, gfn, psize;\r\nunsigned long slot_fn, hva;\r\n__be64 *hpte;\r\nstruct revmap_entry *rev;\r\nunsigned long g_ptel;\r\nstruct kvm_memory_slot *memslot;\r\nunsigned hpage_shift;\r\nunsigned long is_io;\r\nunsigned long *rmap;\r\npte_t *ptep;\r\nunsigned int writing;\r\nunsigned long mmu_seq;\r\nunsigned long rcbits, irq_flags = 0;\r\npsize = hpte_page_size(pteh, ptel);\r\nif (!psize)\r\nreturn H_PARAMETER;\r\nwriting = hpte_is_writable(ptel);\r\npteh &= ~(HPTE_V_HVLOCK | HPTE_V_ABSENT | HPTE_V_VALID);\r\nptel &= ~HPTE_GR_RESERVED;\r\ng_ptel = ptel;\r\nmmu_seq = kvm->mmu_notifier_seq;\r\nsmp_rmb();\r\ngpa = (ptel & HPTE_R_RPN) & ~(psize - 1);\r\ngfn = gpa >> PAGE_SHIFT;\r\nmemslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);\r\npa = 0;\r\nis_io = ~0ul;\r\nrmap = NULL;\r\nif (!(memslot && !(memslot->flags & KVM_MEMSLOT_INVALID))) {\r\npteh |= HPTE_V_ABSENT;\r\nptel |= HPTE_R_KEY_HI | HPTE_R_KEY_LO;\r\ngoto do_insert;\r\n}\r\nif (!slot_is_aligned(memslot, psize))\r\nreturn H_PARAMETER;\r\nslot_fn = gfn - memslot->base_gfn;\r\nrmap = &memslot->arch.rmap[slot_fn];\r\nhva = __gfn_to_hva_memslot(memslot, gfn);\r\nif (realmode)\r\nptep = __find_linux_pte_or_hugepte(pgdir, hva, &hpage_shift);\r\nelse {\r\nlocal_irq_save(irq_flags);\r\nptep = find_linux_pte_or_hugepte(pgdir, hva, &hpage_shift);\r\n}\r\nif (ptep) {\r\npte_t pte;\r\nunsigned int host_pte_size;\r\nif (hpage_shift)\r\nhost_pte_size = 1ul << hpage_shift;\r\nelse\r\nhost_pte_size = PAGE_SIZE;\r\nif (host_pte_size < psize) {\r\nif (!realmode)\r\nlocal_irq_restore(flags);\r\nreturn H_PARAMETER;\r\n}\r\npte = kvmppc_read_update_linux_pte(ptep, writing);\r\nif (pte_present(pte) && !pte_protnone(pte)) {\r\nif (writing && !pte_write(pte))\r\nptel = hpte_make_readonly(ptel);\r\nis_io = hpte_cache_bits(pte_val(pte));\r\npa = pte_pfn(pte) << PAGE_SHIFT;\r\npa |= hva & (host_pte_size - 1);\r\npa |= gpa & ~PAGE_MASK;\r\n}\r\n}\r\nif (!realmode)\r\nlocal_irq_restore(irq_flags);\r\nptel &= ~(HPTE_R_PP0 - psize);\r\nptel |= pa;\r\nif (pa)\r\npteh |= HPTE_V_VALID;\r\nelse\r\npteh |= HPTE_V_ABSENT;\r\nif (is_io != ~0ul && !hpte_cache_flags_ok(ptel, is_io)) {\r\nif (is_io)\r\nreturn H_PARAMETER;\r\nptel &= ~(HPTE_R_W|HPTE_R_I|HPTE_R_G);\r\nptel |= HPTE_R_M;\r\n}\r\ndo_insert:\r\nif (pte_index >= kvm->arch.hpt_npte)\r\nreturn H_PARAMETER;\r\nif (likely((flags & H_EXACT) == 0)) {\r\npte_index &= ~7UL;\r\nhpte = (__be64 *)(kvm->arch.hpt_virt + (pte_index << 4));\r\nfor (i = 0; i < 8; ++i) {\r\nif ((be64_to_cpu(*hpte) & HPTE_V_VALID) == 0 &&\r\ntry_lock_hpte(hpte, HPTE_V_HVLOCK | HPTE_V_VALID |\r\nHPTE_V_ABSENT))\r\nbreak;\r\nhpte += 2;\r\n}\r\nif (i == 8) {\r\nhpte -= 16;\r\nfor (i = 0; i < 8; ++i) {\r\nu64 pte;\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\npte = be64_to_cpu(hpte[0]);\r\nif (!(pte & (HPTE_V_VALID | HPTE_V_ABSENT)))\r\nbreak;\r\n__unlock_hpte(hpte, pte);\r\nhpte += 2;\r\n}\r\nif (i == 8)\r\nreturn H_PTEG_FULL;\r\n}\r\npte_index += i;\r\n} else {\r\nhpte = (__be64 *)(kvm->arch.hpt_virt + (pte_index << 4));\r\nif (!try_lock_hpte(hpte, HPTE_V_HVLOCK | HPTE_V_VALID |\r\nHPTE_V_ABSENT)) {\r\nu64 pte;\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\npte = be64_to_cpu(hpte[0]);\r\nif (pte & (HPTE_V_VALID | HPTE_V_ABSENT)) {\r\n__unlock_hpte(hpte, pte);\r\nreturn H_PTEG_FULL;\r\n}\r\n}\r\n}\r\nrev = &kvm->arch.revmap[pte_index];\r\nif (realmode)\r\nrev = real_vmalloc_addr(rev);\r\nif (rev) {\r\nrev->guest_rpte = g_ptel;\r\nnote_hpte_modification(kvm, rev);\r\n}\r\nif (pteh & HPTE_V_VALID) {\r\nif (realmode)\r\nrmap = real_vmalloc_addr(rmap);\r\nlock_rmap(rmap);\r\nif (mmu_notifier_retry(kvm, mmu_seq)) {\r\npteh |= HPTE_V_ABSENT;\r\npteh &= ~HPTE_V_VALID;\r\nunlock_rmap(rmap);\r\n} else {\r\nkvmppc_add_revmap_chain(kvm, rev, rmap, pte_index,\r\nrealmode);\r\nrcbits = *rmap >> KVMPPC_RMAP_RC_SHIFT;\r\nptel &= rcbits | ~(HPTE_R_R | HPTE_R_C);\r\n}\r\n}\r\nhpte[1] = cpu_to_be64(ptel);\r\neieio();\r\n__unlock_hpte(hpte, pteh);\r\nasm volatile("ptesync" : : : "memory");\r\n*pte_idx_ret = pte_index;\r\nreturn H_SUCCESS;\r\n}\r\nlong kvmppc_h_enter(struct kvm_vcpu *vcpu, unsigned long flags,\r\nlong pte_index, unsigned long pteh, unsigned long ptel)\r\n{\r\nreturn kvmppc_do_h_enter(vcpu->kvm, flags, pte_index, pteh, ptel,\r\nvcpu->arch.pgdir, true, &vcpu->arch.gpr[4]);\r\n}\r\nstatic inline int try_lock_tlbie(unsigned int *lock)\r\n{\r\nunsigned int tmp, old;\r\nunsigned int token = LOCK_TOKEN;\r\nasm volatile("1:lwarx %1,0,%2\n"\r\n" cmpwi cr0,%1,0\n"\r\n" bne 2f\n"\r\n" stwcx. %3,0,%2\n"\r\n" bne- 1b\n"\r\n" isync\n"\r\n"2:"\r\n: "=&r" (tmp), "=&r" (old)\r\n: "r" (lock), "r" (token)\r\n: "cc", "memory");\r\nreturn old == 0;\r\n}\r\nstatic void do_tlbies(struct kvm *kvm, unsigned long *rbvalues,\r\nlong npages, int global, bool need_sync)\r\n{\r\nlong i;\r\nif (global) {\r\nwhile (!try_lock_tlbie(&kvm->arch.tlbie_lock))\r\ncpu_relax();\r\nif (need_sync)\r\nasm volatile("ptesync" : : : "memory");\r\nfor (i = 0; i < npages; ++i)\r\nasm volatile(PPC_TLBIE(%1,%0) : :\r\n"r" (rbvalues[i]), "r" (kvm->arch.lpid));\r\nasm volatile("eieio; tlbsync; ptesync" : : : "memory");\r\nkvm->arch.tlbie_lock = 0;\r\n} else {\r\nif (need_sync)\r\nasm volatile("ptesync" : : : "memory");\r\nfor (i = 0; i < npages; ++i)\r\nasm volatile("tlbiel %0" : : "r" (rbvalues[i]));\r\nasm volatile("ptesync" : : : "memory");\r\n}\r\n}\r\nlong kvmppc_do_h_remove(struct kvm *kvm, unsigned long flags,\r\nunsigned long pte_index, unsigned long avpn,\r\nunsigned long *hpret)\r\n{\r\n__be64 *hpte;\r\nunsigned long v, r, rb;\r\nstruct revmap_entry *rev;\r\nu64 pte;\r\nif (pte_index >= kvm->arch.hpt_npte)\r\nreturn H_PARAMETER;\r\nhpte = (__be64 *)(kvm->arch.hpt_virt + (pte_index << 4));\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\npte = be64_to_cpu(hpte[0]);\r\nif ((pte & (HPTE_V_ABSENT | HPTE_V_VALID)) == 0 ||\r\n((flags & H_AVPN) && (pte & ~0x7fUL) != avpn) ||\r\n((flags & H_ANDCOND) && (pte & avpn) != 0)) {\r\n__unlock_hpte(hpte, pte);\r\nreturn H_NOT_FOUND;\r\n}\r\nrev = real_vmalloc_addr(&kvm->arch.revmap[pte_index]);\r\nv = pte & ~HPTE_V_HVLOCK;\r\nif (v & HPTE_V_VALID) {\r\nu64 pte1;\r\npte1 = be64_to_cpu(hpte[1]);\r\nhpte[0] &= ~cpu_to_be64(HPTE_V_VALID);\r\nrb = compute_tlbie_rb(v, pte1, pte_index);\r\ndo_tlbies(kvm, &rb, 1, global_invalidates(kvm, flags), true);\r\nremove_revmap_chain(kvm, pte_index, rev, v, pte1);\r\n}\r\nr = rev->guest_rpte & ~HPTE_GR_RESERVED;\r\nnote_hpte_modification(kvm, rev);\r\nunlock_hpte(hpte, 0);\r\nhpret[0] = v;\r\nhpret[1] = r;\r\nreturn H_SUCCESS;\r\n}\r\nlong kvmppc_h_remove(struct kvm_vcpu *vcpu, unsigned long flags,\r\nunsigned long pte_index, unsigned long avpn)\r\n{\r\nreturn kvmppc_do_h_remove(vcpu->kvm, flags, pte_index, avpn,\r\n&vcpu->arch.gpr[4]);\r\n}\r\nlong kvmppc_h_bulk_remove(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nunsigned long *args = &vcpu->arch.gpr[4];\r\n__be64 *hp, *hptes[4];\r\nunsigned long tlbrb[4];\r\nlong int i, j, k, n, found, indexes[4];\r\nunsigned long flags, req, pte_index, rcbits;\r\nint global;\r\nlong int ret = H_SUCCESS;\r\nstruct revmap_entry *rev, *revs[4];\r\nu64 hp0;\r\nglobal = global_invalidates(kvm, 0);\r\nfor (i = 0; i < 4 && ret == H_SUCCESS; ) {\r\nn = 0;\r\nfor (; i < 4; ++i) {\r\nj = i * 2;\r\npte_index = args[j];\r\nflags = pte_index >> 56;\r\npte_index &= ((1ul << 56) - 1);\r\nreq = flags >> 6;\r\nflags &= 3;\r\nif (req == 3) {\r\ni = 4;\r\nbreak;\r\n}\r\nif (req != 1 || flags == 3 ||\r\npte_index >= kvm->arch.hpt_npte) {\r\nargs[j] = ((0xa0 | flags) << 56) + pte_index;\r\nret = H_PARAMETER;\r\nbreak;\r\n}\r\nhp = (__be64 *) (kvm->arch.hpt_virt + (pte_index << 4));\r\nif (!try_lock_hpte(hp, HPTE_V_HVLOCK)) {\r\nif (n)\r\nbreak;\r\nwhile (!try_lock_hpte(hp, HPTE_V_HVLOCK))\r\ncpu_relax();\r\n}\r\nfound = 0;\r\nhp0 = be64_to_cpu(hp[0]);\r\nif (hp0 & (HPTE_V_ABSENT | HPTE_V_VALID)) {\r\nswitch (flags & 3) {\r\ncase 0:\r\nfound = 1;\r\nbreak;\r\ncase 1:\r\nif (!(hp0 & args[j + 1]))\r\nfound = 1;\r\nbreak;\r\ncase 2:\r\nif ((hp0 & ~0x7fUL) == args[j + 1])\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nhp[0] &= ~cpu_to_be64(HPTE_V_HVLOCK);\r\nargs[j] = ((0x90 | flags) << 56) + pte_index;\r\ncontinue;\r\n}\r\nargs[j] = ((0x80 | flags) << 56) + pte_index;\r\nrev = real_vmalloc_addr(&kvm->arch.revmap[pte_index]);\r\nnote_hpte_modification(kvm, rev);\r\nif (!(hp0 & HPTE_V_VALID)) {\r\nrcbits = rev->guest_rpte & (HPTE_R_R|HPTE_R_C);\r\nargs[j] |= rcbits << (56 - 5);\r\nhp[0] = 0;\r\ncontinue;\r\n}\r\nhp[0] &= ~cpu_to_be64(HPTE_V_VALID);\r\ntlbrb[n] = compute_tlbie_rb(be64_to_cpu(hp[0]),\r\nbe64_to_cpu(hp[1]), pte_index);\r\nindexes[n] = j;\r\nhptes[n] = hp;\r\nrevs[n] = rev;\r\n++n;\r\n}\r\nif (!n)\r\nbreak;\r\ndo_tlbies(kvm, tlbrb, n, global, true);\r\nfor (k = 0; k < n; ++k) {\r\nj = indexes[k];\r\npte_index = args[j] & ((1ul << 56) - 1);\r\nhp = hptes[k];\r\nrev = revs[k];\r\nremove_revmap_chain(kvm, pte_index, rev,\r\nbe64_to_cpu(hp[0]), be64_to_cpu(hp[1]));\r\nrcbits = rev->guest_rpte & (HPTE_R_R|HPTE_R_C);\r\nargs[j] |= rcbits << (56 - 5);\r\n__unlock_hpte(hp, 0);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nlong kvmppc_h_protect(struct kvm_vcpu *vcpu, unsigned long flags,\r\nunsigned long pte_index, unsigned long avpn,\r\nunsigned long va)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\n__be64 *hpte;\r\nstruct revmap_entry *rev;\r\nunsigned long v, r, rb, mask, bits;\r\nu64 pte;\r\nif (pte_index >= kvm->arch.hpt_npte)\r\nreturn H_PARAMETER;\r\nhpte = (__be64 *)(kvm->arch.hpt_virt + (pte_index << 4));\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\npte = be64_to_cpu(hpte[0]);\r\nif ((pte & (HPTE_V_ABSENT | HPTE_V_VALID)) == 0 ||\r\n((flags & H_AVPN) && (pte & ~0x7fUL) != avpn)) {\r\n__unlock_hpte(hpte, pte);\r\nreturn H_NOT_FOUND;\r\n}\r\nv = pte;\r\nbits = (flags << 55) & HPTE_R_PP0;\r\nbits |= (flags << 48) & HPTE_R_KEY_HI;\r\nbits |= flags & (HPTE_R_PP | HPTE_R_N | HPTE_R_KEY_LO);\r\nmask = HPTE_R_PP0 | HPTE_R_PP | HPTE_R_N |\r\nHPTE_R_KEY_HI | HPTE_R_KEY_LO;\r\nrev = real_vmalloc_addr(&kvm->arch.revmap[pte_index]);\r\nif (rev) {\r\nr = (rev->guest_rpte & ~mask) | bits;\r\nrev->guest_rpte = r;\r\nnote_hpte_modification(kvm, rev);\r\n}\r\nif (v & HPTE_V_VALID) {\r\npte = be64_to_cpu(hpte[1]);\r\nr = (pte & ~mask) | bits;\r\nif (hpte_is_writable(r) && !hpte_is_writable(pte))\r\nr = hpte_make_readonly(r);\r\nif (r != pte) {\r\nrb = compute_tlbie_rb(v, r, pte_index);\r\nhpte[0] = cpu_to_be64((v & ~HPTE_V_VALID) |\r\nHPTE_V_ABSENT);\r\ndo_tlbies(kvm, &rb, 1, global_invalidates(kvm, flags),\r\ntrue);\r\nhpte[1] = cpu_to_be64(r);\r\n}\r\n}\r\nunlock_hpte(hpte, v & ~HPTE_V_HVLOCK);\r\nasm volatile("ptesync" : : : "memory");\r\nreturn H_SUCCESS;\r\n}\r\nlong kvmppc_h_read(struct kvm_vcpu *vcpu, unsigned long flags,\r\nunsigned long pte_index)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\n__be64 *hpte;\r\nunsigned long v, r;\r\nint i, n = 1;\r\nstruct revmap_entry *rev = NULL;\r\nif (pte_index >= kvm->arch.hpt_npte)\r\nreturn H_PARAMETER;\r\nif (flags & H_READ_4) {\r\npte_index &= ~3;\r\nn = 4;\r\n}\r\nrev = real_vmalloc_addr(&kvm->arch.revmap[pte_index]);\r\nfor (i = 0; i < n; ++i, ++pte_index) {\r\nhpte = (__be64 *)(kvm->arch.hpt_virt + (pte_index << 4));\r\nv = be64_to_cpu(hpte[0]) & ~HPTE_V_HVLOCK;\r\nr = be64_to_cpu(hpte[1]);\r\nif (v & HPTE_V_ABSENT) {\r\nv &= ~HPTE_V_ABSENT;\r\nv |= HPTE_V_VALID;\r\n}\r\nif (v & HPTE_V_VALID) {\r\nr = rev[i].guest_rpte | (r & (HPTE_R_R | HPTE_R_C));\r\nr &= ~HPTE_GR_RESERVED;\r\n}\r\nvcpu->arch.gpr[4 + i * 2] = v;\r\nvcpu->arch.gpr[5 + i * 2] = r;\r\n}\r\nreturn H_SUCCESS;\r\n}\r\nvoid kvmppc_invalidate_hpte(struct kvm *kvm, __be64 *hptep,\r\nunsigned long pte_index)\r\n{\r\nunsigned long rb;\r\nhptep[0] &= ~cpu_to_be64(HPTE_V_VALID);\r\nrb = compute_tlbie_rb(be64_to_cpu(hptep[0]), be64_to_cpu(hptep[1]),\r\npte_index);\r\ndo_tlbies(kvm, &rb, 1, 1, true);\r\n}\r\nvoid kvmppc_clear_ref_hpte(struct kvm *kvm, __be64 *hptep,\r\nunsigned long pte_index)\r\n{\r\nunsigned long rb;\r\nunsigned char rbyte;\r\nrb = compute_tlbie_rb(be64_to_cpu(hptep[0]), be64_to_cpu(hptep[1]),\r\npte_index);\r\nrbyte = (be64_to_cpu(hptep[1]) & ~HPTE_R_R) >> 8;\r\n*((char *)hptep + 14) = rbyte;\r\ndo_tlbies(kvm, &rb, 1, 1, false);\r\n}\r\nlong kvmppc_hv_find_lock_hpte(struct kvm *kvm, gva_t eaddr, unsigned long slb_v,\r\nunsigned long valid)\r\n{\r\nunsigned int i;\r\nunsigned int pshift;\r\nunsigned long somask;\r\nunsigned long vsid, hash;\r\nunsigned long avpn;\r\n__be64 *hpte;\r\nunsigned long mask, val;\r\nunsigned long v, r;\r\nmask = SLB_VSID_B | HPTE_V_AVPN | HPTE_V_SECONDARY;\r\nval = 0;\r\npshift = 12;\r\nif (slb_v & SLB_VSID_L) {\r\nmask |= HPTE_V_LARGE;\r\nval |= HPTE_V_LARGE;\r\npshift = slb_base_page_shift[(slb_v & SLB_VSID_LP) >> 4];\r\n}\r\nif (slb_v & SLB_VSID_B_1T) {\r\nsomask = (1UL << 40) - 1;\r\nvsid = (slb_v & ~SLB_VSID_B) >> SLB_VSID_SHIFT_1T;\r\nvsid ^= vsid << 25;\r\n} else {\r\nsomask = (1UL << 28) - 1;\r\nvsid = (slb_v & ~SLB_VSID_B) >> SLB_VSID_SHIFT;\r\n}\r\nhash = (vsid ^ ((eaddr & somask) >> pshift)) & kvm->arch.hpt_mask;\r\navpn = slb_v & ~(somask >> 16);\r\navpn |= (eaddr & somask) >> 16;\r\nif (pshift >= 24)\r\navpn &= ~((1UL << (pshift - 16)) - 1);\r\nelse\r\navpn &= ~0x7fUL;\r\nval |= avpn;\r\nfor (;;) {\r\nhpte = (__be64 *)(kvm->arch.hpt_virt + (hash << 7));\r\nfor (i = 0; i < 16; i += 2) {\r\nv = be64_to_cpu(hpte[i]) & ~HPTE_V_HVLOCK;\r\nif (!(v & valid) || (v & mask) != val)\r\ncontinue;\r\nwhile (!try_lock_hpte(&hpte[i], HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = be64_to_cpu(hpte[i]) & ~HPTE_V_HVLOCK;\r\nr = be64_to_cpu(hpte[i+1]);\r\nif ((v & valid) && (v & mask) == val &&\r\nhpte_base_page_size(v, r) == (1ul << pshift))\r\nreturn (hash << 3) + (i >> 1);\r\n__unlock_hpte(&hpte[i], v);\r\n}\r\nif (val & HPTE_V_SECONDARY)\r\nbreak;\r\nval |= HPTE_V_SECONDARY;\r\nhash = hash ^ kvm->arch.hpt_mask;\r\n}\r\nreturn -1;\r\n}\r\nlong kvmppc_hpte_hv_fault(struct kvm_vcpu *vcpu, unsigned long addr,\r\nunsigned long slb_v, unsigned int status, bool data)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nlong int index;\r\nunsigned long v, r, gr;\r\n__be64 *hpte;\r\nunsigned long valid;\r\nstruct revmap_entry *rev;\r\nunsigned long pp, key;\r\nvalid = HPTE_V_VALID;\r\nif (status & DSISR_NOHPTE)\r\nvalid |= HPTE_V_ABSENT;\r\nindex = kvmppc_hv_find_lock_hpte(kvm, addr, slb_v, valid);\r\nif (index < 0) {\r\nif (status & DSISR_NOHPTE)\r\nreturn status;\r\nreturn 0;\r\n}\r\nhpte = (__be64 *)(kvm->arch.hpt_virt + (index << 4));\r\nv = be64_to_cpu(hpte[0]) & ~HPTE_V_HVLOCK;\r\nr = be64_to_cpu(hpte[1]);\r\nrev = real_vmalloc_addr(&kvm->arch.revmap[index]);\r\ngr = rev->guest_rpte;\r\nunlock_hpte(hpte, v);\r\nif ((status & DSISR_NOHPTE) && (v & HPTE_V_VALID))\r\nreturn 0;\r\npp = gr & (HPTE_R_PP0 | HPTE_R_PP);\r\nkey = (vcpu->arch.shregs.msr & MSR_PR) ? SLB_VSID_KP : SLB_VSID_KS;\r\nstatus &= ~DSISR_NOHPTE;\r\nif (!data) {\r\nif (gr & (HPTE_R_N | HPTE_R_G))\r\nreturn status | SRR1_ISI_N_OR_G;\r\nif (!hpte_read_permission(pp, slb_v & key))\r\nreturn status | SRR1_ISI_PROT;\r\n} else if (status & DSISR_ISSTORE) {\r\nif (!hpte_write_permission(pp, slb_v & key))\r\nreturn status | DSISR_PROTFAULT;\r\n} else {\r\nif (!hpte_read_permission(pp, slb_v & key))\r\nreturn status | DSISR_PROTFAULT;\r\n}\r\nif (data && (vcpu->arch.shregs.msr & MSR_DR)) {\r\nunsigned int perm = hpte_get_skey_perm(gr, vcpu->arch.amr);\r\nif (status & DSISR_ISSTORE)\r\nperm >>= 1;\r\nif (perm & 1)\r\nreturn status | DSISR_KEYFAULT;\r\n}\r\nvcpu->arch.pgfault_addr = addr;\r\nvcpu->arch.pgfault_index = index;\r\nvcpu->arch.pgfault_hpte[0] = v;\r\nvcpu->arch.pgfault_hpte[1] = r;\r\nif (data && (vcpu->arch.shregs.msr & MSR_IR) &&\r\n(r & (HPTE_R_KEY_HI | HPTE_R_KEY_LO)) ==\r\n(HPTE_R_KEY_HI | HPTE_R_KEY_LO))\r\nreturn -2;\r\nreturn -1;\r\n}
