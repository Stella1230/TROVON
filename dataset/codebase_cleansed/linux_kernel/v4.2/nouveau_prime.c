struct sg_table *nouveau_gem_prime_get_sg_table(struct drm_gem_object *obj)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(obj);\r\nint npages = nvbo->bo.num_pages;\r\nreturn drm_prime_pages_to_sg(nvbo->bo.ttm->pages, npages);\r\n}\r\nvoid *nouveau_gem_prime_vmap(struct drm_gem_object *obj)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(obj);\r\nint ret;\r\nret = ttm_bo_kmap(&nvbo->bo, 0, nvbo->bo.num_pages,\r\n&nvbo->dma_buf_vmap);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nreturn nvbo->dma_buf_vmap.virtual;\r\n}\r\nvoid nouveau_gem_prime_vunmap(struct drm_gem_object *obj, void *vaddr)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(obj);\r\nttm_bo_kunmap(&nvbo->dma_buf_vmap);\r\n}\r\nstruct drm_gem_object *nouveau_gem_prime_import_sg_table(struct drm_device *dev,\r\nstruct dma_buf_attachment *attach,\r\nstruct sg_table *sg)\r\n{\r\nstruct nouveau_bo *nvbo;\r\nstruct reservation_object *robj = attach->dmabuf->resv;\r\nu32 flags = 0;\r\nint ret;\r\nflags = TTM_PL_FLAG_TT;\r\nww_mutex_lock(&robj->lock, NULL);\r\nret = nouveau_bo_new(dev, attach->dmabuf->size, 0, flags, 0, 0,\r\nsg, robj, &nvbo);\r\nww_mutex_unlock(&robj->lock);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nnvbo->valid_domains = NOUVEAU_GEM_DOMAIN_GART;\r\nret = drm_gem_object_init(dev, &nvbo->gem, nvbo->bo.mem.size);\r\nif (ret) {\r\nnouveau_bo_ref(NULL, &nvbo);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nreturn &nvbo->gem;\r\n}\r\nint nouveau_gem_prime_pin(struct drm_gem_object *obj)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(obj);\r\nint ret;\r\nret = nouveau_bo_pin(nvbo, TTM_PL_FLAG_TT, false);\r\nif (ret)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nvoid nouveau_gem_prime_unpin(struct drm_gem_object *obj)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(obj);\r\nnouveau_bo_unpin(nvbo);\r\n}\r\nstruct reservation_object *nouveau_gem_prime_res_obj(struct drm_gem_object *obj)\r\n{\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(obj);\r\nreturn nvbo->bo.resv;\r\n}
