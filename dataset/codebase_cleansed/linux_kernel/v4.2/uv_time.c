static void uv_rtc_send_IPI(int cpu)\r\n{\r\nunsigned long apicid, val;\r\nint pnode;\r\napicid = cpu_physical_id(cpu);\r\npnode = uv_apicid_to_pnode(apicid);\r\napicid |= uv_apicid_hibits;\r\nval = (1UL << UVH_IPI_INT_SEND_SHFT) |\r\n(apicid << UVH_IPI_INT_APIC_ID_SHFT) |\r\n(X86_PLATFORM_IPI_VECTOR << UVH_IPI_INT_VECTOR_SHFT);\r\nuv_write_global_mmr64(pnode, UVH_IPI_INT, val);\r\n}\r\nstatic int uv_intr_pending(int pnode)\r\n{\r\nif (is_uv1_hub())\r\nreturn uv_read_global_mmr64(pnode, UVH_EVENT_OCCURRED0) &\r\nUV1H_EVENT_OCCURRED0_RTC1_MASK;\r\nelse if (is_uvx_hub())\r\nreturn uv_read_global_mmr64(pnode, UVXH_EVENT_OCCURRED2) &\r\nUVXH_EVENT_OCCURRED2_RTC_1_MASK;\r\nreturn 0;\r\n}\r\nstatic int uv_setup_intr(int cpu, u64 expires)\r\n{\r\nu64 val;\r\nunsigned long apicid = cpu_physical_id(cpu) | uv_apicid_hibits;\r\nint pnode = uv_cpu_to_pnode(cpu);\r\nuv_write_global_mmr64(pnode, UVH_RTC1_INT_CONFIG,\r\nUVH_RTC1_INT_CONFIG_M_MASK);\r\nuv_write_global_mmr64(pnode, UVH_INT_CMPB, -1L);\r\nif (is_uv1_hub())\r\nuv_write_global_mmr64(pnode, UVH_EVENT_OCCURRED0_ALIAS,\r\nUV1H_EVENT_OCCURRED0_RTC1_MASK);\r\nelse\r\nuv_write_global_mmr64(pnode, UVXH_EVENT_OCCURRED2_ALIAS,\r\nUVXH_EVENT_OCCURRED2_RTC_1_MASK);\r\nval = (X86_PLATFORM_IPI_VECTOR << UVH_RTC1_INT_CONFIG_VECTOR_SHFT) |\r\n((u64)apicid << UVH_RTC1_INT_CONFIG_APIC_ID_SHFT);\r\nuv_write_global_mmr64(pnode, UVH_RTC1_INT_CONFIG, val);\r\nuv_write_global_mmr64(pnode, UVH_INT_CMPB, expires);\r\nif (uv_read_rtc(NULL) <= expires)\r\nreturn 0;\r\nreturn !uv_intr_pending(pnode);\r\n}\r\nstatic __init void uv_rtc_deallocate_timers(void)\r\n{\r\nint bid;\r\nfor_each_possible_blade(bid) {\r\nkfree(blade_info[bid]);\r\n}\r\nkfree(blade_info);\r\n}\r\nstatic __init int uv_rtc_allocate_timers(void)\r\n{\r\nint cpu;\r\nblade_info = kzalloc(uv_possible_blades * sizeof(void *), GFP_KERNEL);\r\nif (!blade_info)\r\nreturn -ENOMEM;\r\nfor_each_present_cpu(cpu) {\r\nint nid = cpu_to_node(cpu);\r\nint bid = uv_cpu_to_blade_id(cpu);\r\nint bcpu = uv_cpu_hub_info(cpu)->blade_processor_id;\r\nstruct uv_rtc_timer_head *head = blade_info[bid];\r\nif (!head) {\r\nhead = kmalloc_node(sizeof(struct uv_rtc_timer_head) +\r\n(uv_blade_nr_possible_cpus(bid) *\r\n2 * sizeof(u64)),\r\nGFP_KERNEL, nid);\r\nif (!head) {\r\nuv_rtc_deallocate_timers();\r\nreturn -ENOMEM;\r\n}\r\nspin_lock_init(&head->lock);\r\nhead->ncpus = uv_blade_nr_possible_cpus(bid);\r\nhead->next_cpu = -1;\r\nblade_info[bid] = head;\r\n}\r\nhead->cpu[bcpu].lcpu = cpu;\r\nhead->cpu[bcpu].expires = ULLONG_MAX;\r\n}\r\nreturn 0;\r\n}\r\nstatic void uv_rtc_find_next_timer(struct uv_rtc_timer_head *head, int pnode)\r\n{\r\nu64 lowest = ULLONG_MAX;\r\nint c, bcpu = -1;\r\nhead->next_cpu = -1;\r\nfor (c = 0; c < head->ncpus; c++) {\r\nu64 exp = head->cpu[c].expires;\r\nif (exp < lowest) {\r\nbcpu = c;\r\nlowest = exp;\r\n}\r\n}\r\nif (bcpu >= 0) {\r\nhead->next_cpu = bcpu;\r\nc = head->cpu[bcpu].lcpu;\r\nif (uv_setup_intr(c, lowest))\r\nuv_rtc_send_IPI(c);\r\n} else {\r\nuv_write_global_mmr64(pnode, UVH_RTC1_INT_CONFIG,\r\nUVH_RTC1_INT_CONFIG_M_MASK);\r\n}\r\n}\r\nstatic int uv_rtc_set_timer(int cpu, u64 expires)\r\n{\r\nint pnode = uv_cpu_to_pnode(cpu);\r\nint bid = uv_cpu_to_blade_id(cpu);\r\nstruct uv_rtc_timer_head *head = blade_info[bid];\r\nint bcpu = uv_cpu_hub_info(cpu)->blade_processor_id;\r\nu64 *t = &head->cpu[bcpu].expires;\r\nunsigned long flags;\r\nint next_cpu;\r\nspin_lock_irqsave(&head->lock, flags);\r\nnext_cpu = head->next_cpu;\r\n*t = expires;\r\nif (next_cpu < 0 || bcpu == next_cpu ||\r\nexpires < head->cpu[next_cpu].expires) {\r\nhead->next_cpu = bcpu;\r\nif (uv_setup_intr(cpu, expires)) {\r\n*t = ULLONG_MAX;\r\nuv_rtc_find_next_timer(head, pnode);\r\nspin_unlock_irqrestore(&head->lock, flags);\r\nreturn -ETIME;\r\n}\r\n}\r\nspin_unlock_irqrestore(&head->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int uv_rtc_unset_timer(int cpu, int force)\r\n{\r\nint pnode = uv_cpu_to_pnode(cpu);\r\nint bid = uv_cpu_to_blade_id(cpu);\r\nstruct uv_rtc_timer_head *head = blade_info[bid];\r\nint bcpu = uv_cpu_hub_info(cpu)->blade_processor_id;\r\nu64 *t = &head->cpu[bcpu].expires;\r\nunsigned long flags;\r\nint rc = 0;\r\nspin_lock_irqsave(&head->lock, flags);\r\nif ((head->next_cpu == bcpu && uv_read_rtc(NULL) >= *t) || force)\r\nrc = 1;\r\nif (rc) {\r\n*t = ULLONG_MAX;\r\nif (head->next_cpu == bcpu)\r\nuv_rtc_find_next_timer(head, pnode);\r\n}\r\nspin_unlock_irqrestore(&head->lock, flags);\r\nreturn rc;\r\n}\r\nstatic cycle_t uv_read_rtc(struct clocksource *cs)\r\n{\r\nunsigned long offset;\r\nif (uv_get_min_hub_revision_id() == 1)\r\noffset = 0;\r\nelse\r\noffset = (uv_blade_processor_id() * L1_CACHE_BYTES) % PAGE_SIZE;\r\nreturn (cycle_t)uv_read_local_mmr(UVH_RTC | offset);\r\n}\r\nstatic int uv_rtc_next_event(unsigned long delta,\r\nstruct clock_event_device *ced)\r\n{\r\nint ced_cpu = cpumask_first(ced->cpumask);\r\nreturn uv_rtc_set_timer(ced_cpu, delta + uv_read_rtc(NULL));\r\n}\r\nstatic void uv_rtc_timer_setup(enum clock_event_mode mode,\r\nstruct clock_event_device *evt)\r\n{\r\nint ced_cpu = cpumask_first(evt->cpumask);\r\nswitch (mode) {\r\ncase CLOCK_EVT_MODE_PERIODIC:\r\ncase CLOCK_EVT_MODE_ONESHOT:\r\ncase CLOCK_EVT_MODE_RESUME:\r\nbreak;\r\ncase CLOCK_EVT_MODE_UNUSED:\r\ncase CLOCK_EVT_MODE_SHUTDOWN:\r\nuv_rtc_unset_timer(ced_cpu, 1);\r\nbreak;\r\n}\r\n}\r\nstatic void uv_rtc_interrupt(void)\r\n{\r\nint cpu = smp_processor_id();\r\nstruct clock_event_device *ced = &per_cpu(cpu_ced, cpu);\r\nif (!ced || !ced->event_handler)\r\nreturn;\r\nif (uv_rtc_unset_timer(cpu, 0) != 1)\r\nreturn;\r\nced->event_handler(ced);\r\n}\r\nstatic int __init uv_enable_evt_rtc(char *str)\r\n{\r\nuv_rtc_evt_enable = 1;\r\nreturn 1;\r\n}\r\nstatic __init void uv_rtc_register_clockevents(struct work_struct *dummy)\r\n{\r\nstruct clock_event_device *ced = this_cpu_ptr(&cpu_ced);\r\n*ced = clock_event_device_uv;\r\nced->cpumask = cpumask_of(smp_processor_id());\r\nclockevents_register_device(ced);\r\n}\r\nstatic __init int uv_rtc_setup_clock(void)\r\n{\r\nint rc;\r\nif (!is_uv_system())\r\nreturn -ENODEV;\r\nrc = clocksource_register_hz(&clocksource_uv, sn_rtc_cycles_per_second);\r\nif (rc)\r\nprintk(KERN_INFO "UV RTC clocksource failed rc %d\n", rc);\r\nelse\r\nprintk(KERN_INFO "UV RTC clocksource registered freq %lu MHz\n",\r\nsn_rtc_cycles_per_second/(unsigned long)1E6);\r\nif (rc || !uv_rtc_evt_enable || x86_platform_ipi_callback)\r\nreturn rc;\r\nrc = uv_rtc_allocate_timers();\r\nif (rc)\r\ngoto error;\r\nx86_platform_ipi_callback = uv_rtc_interrupt;\r\nclock_event_device_uv.mult = div_sc(sn_rtc_cycles_per_second,\r\nNSEC_PER_SEC, clock_event_device_uv.shift);\r\nclock_event_device_uv.min_delta_ns = NSEC_PER_SEC /\r\nsn_rtc_cycles_per_second;\r\nclock_event_device_uv.max_delta_ns = clocksource_uv.mask *\r\n(NSEC_PER_SEC / sn_rtc_cycles_per_second);\r\nrc = schedule_on_each_cpu(uv_rtc_register_clockevents);\r\nif (rc) {\r\nx86_platform_ipi_callback = NULL;\r\nuv_rtc_deallocate_timers();\r\ngoto error;\r\n}\r\nprintk(KERN_INFO "UV RTC clockevents registered\n");\r\nreturn 0;\r\nerror:\r\nclocksource_unregister(&clocksource_uv);\r\nprintk(KERN_INFO "UV RTC clockevents failed rc %d\n", rc);\r\nreturn rc;\r\n}
