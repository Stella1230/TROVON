static unsigned int int_log(unsigned int n, unsigned int base)\r\n{\r\nint result = 0;\r\nwhile (n > 1) {\r\nn = dm_div_up(n, base);\r\nresult++;\r\n}\r\nreturn result;\r\n}\r\nstatic inline unsigned int get_child(unsigned int n, unsigned int k)\r\n{\r\nreturn (n * CHILDREN_PER_NODE) + k;\r\n}\r\nstatic inline sector_t *get_node(struct dm_table *t,\r\nunsigned int l, unsigned int n)\r\n{\r\nreturn t->index[l] + (n * KEYS_PER_NODE);\r\n}\r\nstatic sector_t high(struct dm_table *t, unsigned int l, unsigned int n)\r\n{\r\nfor (; l < t->depth - 1; l++)\r\nn = get_child(n, CHILDREN_PER_NODE - 1);\r\nif (n >= t->counts[l])\r\nreturn (sector_t) - 1;\r\nreturn get_node(t, l, n)[KEYS_PER_NODE - 1];\r\n}\r\nstatic int setup_btree_index(unsigned int l, struct dm_table *t)\r\n{\r\nunsigned int n, k;\r\nsector_t *node;\r\nfor (n = 0U; n < t->counts[l]; n++) {\r\nnode = get_node(t, l, n);\r\nfor (k = 0U; k < KEYS_PER_NODE; k++)\r\nnode[k] = high(t, l + 1, get_child(n, k));\r\n}\r\nreturn 0;\r\n}\r\nvoid *dm_vcalloc(unsigned long nmemb, unsigned long elem_size)\r\n{\r\nunsigned long size;\r\nvoid *addr;\r\nif (nmemb > (ULONG_MAX / elem_size))\r\nreturn NULL;\r\nsize = nmemb * elem_size;\r\naddr = vzalloc(size);\r\nreturn addr;\r\n}\r\nstatic int alloc_targets(struct dm_table *t, unsigned int num)\r\n{\r\nsector_t *n_highs;\r\nstruct dm_target *n_targets;\r\nn_highs = (sector_t *) dm_vcalloc(num + 1, sizeof(struct dm_target) +\r\nsizeof(sector_t));\r\nif (!n_highs)\r\nreturn -ENOMEM;\r\nn_targets = (struct dm_target *) (n_highs + num);\r\nmemset(n_highs, -1, sizeof(*n_highs) * num);\r\nvfree(t->highs);\r\nt->num_allocated = num;\r\nt->highs = n_highs;\r\nt->targets = n_targets;\r\nreturn 0;\r\n}\r\nint dm_table_create(struct dm_table **result, fmode_t mode,\r\nunsigned num_targets, struct mapped_device *md)\r\n{\r\nstruct dm_table *t = kzalloc(sizeof(*t), GFP_KERNEL);\r\nif (!t)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&t->devices);\r\nINIT_LIST_HEAD(&t->target_callbacks);\r\nif (!num_targets)\r\nnum_targets = KEYS_PER_NODE;\r\nnum_targets = dm_round_up(num_targets, KEYS_PER_NODE);\r\nif (!num_targets) {\r\nkfree(t);\r\nreturn -ENOMEM;\r\n}\r\nif (alloc_targets(t, num_targets)) {\r\nkfree(t);\r\nreturn -ENOMEM;\r\n}\r\nt->mode = mode;\r\nt->md = md;\r\n*result = t;\r\nreturn 0;\r\n}\r\nstatic void free_devices(struct list_head *devices, struct mapped_device *md)\r\n{\r\nstruct list_head *tmp, *next;\r\nlist_for_each_safe(tmp, next, devices) {\r\nstruct dm_dev_internal *dd =\r\nlist_entry(tmp, struct dm_dev_internal, list);\r\nDMWARN("%s: dm_table_destroy: dm_put_device call missing for %s",\r\ndm_device_name(md), dd->dm_dev->name);\r\ndm_put_table_device(md, dd->dm_dev);\r\nkfree(dd);\r\n}\r\n}\r\nvoid dm_table_destroy(struct dm_table *t)\r\n{\r\nunsigned int i;\r\nif (!t)\r\nreturn;\r\nif (t->depth >= 2)\r\nvfree(t->index[t->depth - 2]);\r\nfor (i = 0; i < t->num_targets; i++) {\r\nstruct dm_target *tgt = t->targets + i;\r\nif (tgt->type->dtr)\r\ntgt->type->dtr(tgt);\r\ndm_put_target_type(tgt->type);\r\n}\r\nvfree(t->highs);\r\nfree_devices(&t->devices, t->md);\r\ndm_free_md_mempools(t->mempools);\r\nkfree(t);\r\n}\r\nstatic struct dm_dev_internal *find_device(struct list_head *l, dev_t dev)\r\n{\r\nstruct dm_dev_internal *dd;\r\nlist_for_each_entry (dd, l, list)\r\nif (dd->dm_dev->bdev->bd_dev == dev)\r\nreturn dd;\r\nreturn NULL;\r\n}\r\nstatic int device_area_is_invalid(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nstruct request_queue *q;\r\nstruct queue_limits *limits = data;\r\nstruct block_device *bdev = dev->bdev;\r\nsector_t dev_size =\r\ni_size_read(bdev->bd_inode) >> SECTOR_SHIFT;\r\nunsigned short logical_block_size_sectors =\r\nlimits->logical_block_size >> SECTOR_SHIFT;\r\nchar b[BDEVNAME_SIZE];\r\nq = bdev_get_queue(bdev);\r\nif (!q || !q->make_request_fn) {\r\nDMWARN("%s: %s is not yet initialised: "\r\n"start=%llu, len=%llu, dev_size=%llu",\r\ndm_device_name(ti->table->md), bdevname(bdev, b),\r\n(unsigned long long)start,\r\n(unsigned long long)len,\r\n(unsigned long long)dev_size);\r\nreturn 1;\r\n}\r\nif (!dev_size)\r\nreturn 0;\r\nif ((start >= dev_size) || (start + len > dev_size)) {\r\nDMWARN("%s: %s too small for target: "\r\n"start=%llu, len=%llu, dev_size=%llu",\r\ndm_device_name(ti->table->md), bdevname(bdev, b),\r\n(unsigned long long)start,\r\n(unsigned long long)len,\r\n(unsigned long long)dev_size);\r\nreturn 1;\r\n}\r\nif (logical_block_size_sectors <= 1)\r\nreturn 0;\r\nif (start & (logical_block_size_sectors - 1)) {\r\nDMWARN("%s: start=%llu not aligned to h/w "\r\n"logical block size %u of %s",\r\ndm_device_name(ti->table->md),\r\n(unsigned long long)start,\r\nlimits->logical_block_size, bdevname(bdev, b));\r\nreturn 1;\r\n}\r\nif (len & (logical_block_size_sectors - 1)) {\r\nDMWARN("%s: len=%llu not aligned to h/w "\r\n"logical block size %u of %s",\r\ndm_device_name(ti->table->md),\r\n(unsigned long long)len,\r\nlimits->logical_block_size, bdevname(bdev, b));\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int upgrade_mode(struct dm_dev_internal *dd, fmode_t new_mode,\r\nstruct mapped_device *md)\r\n{\r\nint r;\r\nstruct dm_dev *old_dev, *new_dev;\r\nold_dev = dd->dm_dev;\r\nr = dm_get_table_device(md, dd->dm_dev->bdev->bd_dev,\r\ndd->dm_dev->mode | new_mode, &new_dev);\r\nif (r)\r\nreturn r;\r\ndd->dm_dev = new_dev;\r\ndm_put_table_device(md, old_dev);\r\nreturn 0;\r\n}\r\nint dm_get_device(struct dm_target *ti, const char *path, fmode_t mode,\r\nstruct dm_dev **result)\r\n{\r\nint r;\r\ndev_t uninitialized_var(dev);\r\nstruct dm_dev_internal *dd;\r\nstruct dm_table *t = ti->table;\r\nstruct block_device *bdev;\r\nBUG_ON(!t);\r\nbdev = lookup_bdev(path);\r\nif (IS_ERR(bdev)) {\r\ndev = name_to_dev_t(path);\r\nif (!dev)\r\nreturn -ENODEV;\r\n} else {\r\ndev = bdev->bd_dev;\r\nbdput(bdev);\r\n}\r\ndd = find_device(&t->devices, dev);\r\nif (!dd) {\r\ndd = kmalloc(sizeof(*dd), GFP_KERNEL);\r\nif (!dd)\r\nreturn -ENOMEM;\r\nif ((r = dm_get_table_device(t->md, dev, mode, &dd->dm_dev))) {\r\nkfree(dd);\r\nreturn r;\r\n}\r\natomic_set(&dd->count, 0);\r\nlist_add(&dd->list, &t->devices);\r\n} else if (dd->dm_dev->mode != (mode | dd->dm_dev->mode)) {\r\nr = upgrade_mode(dd, mode, t->md);\r\nif (r)\r\nreturn r;\r\n}\r\natomic_inc(&dd->count);\r\n*result = dd->dm_dev;\r\nreturn 0;\r\n}\r\nstatic int dm_set_device_limits(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nstruct queue_limits *limits = data;\r\nstruct block_device *bdev = dev->bdev;\r\nstruct request_queue *q = bdev_get_queue(bdev);\r\nchar b[BDEVNAME_SIZE];\r\nif (unlikely(!q)) {\r\nDMWARN("%s: Cannot set limits for nonexistent device %s",\r\ndm_device_name(ti->table->md), bdevname(bdev, b));\r\nreturn 0;\r\n}\r\nif (bdev_stack_limits(limits, bdev, start) < 0)\r\nDMWARN("%s: adding target device %s caused an alignment inconsistency: "\r\n"physical_block_size=%u, logical_block_size=%u, "\r\n"alignment_offset=%u, start=%llu",\r\ndm_device_name(ti->table->md), bdevname(bdev, b),\r\nq->limits.physical_block_size,\r\nq->limits.logical_block_size,\r\nq->limits.alignment_offset,\r\n(unsigned long long) start << SECTOR_SHIFT);\r\nif (dm_queue_merge_is_compulsory(q) && !ti->type->merge)\r\nblk_limits_max_hw_sectors(limits,\r\n(unsigned int) (PAGE_SIZE >> 9));\r\nreturn 0;\r\n}\r\nvoid dm_put_device(struct dm_target *ti, struct dm_dev *d)\r\n{\r\nint found = 0;\r\nstruct list_head *devices = &ti->table->devices;\r\nstruct dm_dev_internal *dd;\r\nlist_for_each_entry(dd, devices, list) {\r\nif (dd->dm_dev == d) {\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nDMWARN("%s: device %s not in table devices list",\r\ndm_device_name(ti->table->md), d->name);\r\nreturn;\r\n}\r\nif (atomic_dec_and_test(&dd->count)) {\r\ndm_put_table_device(ti->table->md, d);\r\nlist_del(&dd->list);\r\nkfree(dd);\r\n}\r\n}\r\nstatic int adjoin(struct dm_table *table, struct dm_target *ti)\r\n{\r\nstruct dm_target *prev;\r\nif (!table->num_targets)\r\nreturn !ti->begin;\r\nprev = &table->targets[table->num_targets - 1];\r\nreturn (ti->begin == (prev->begin + prev->len));\r\n}\r\nstatic char **realloc_argv(unsigned *array_size, char **old_argv)\r\n{\r\nchar **argv;\r\nunsigned new_size;\r\ngfp_t gfp;\r\nif (*array_size) {\r\nnew_size = *array_size * 2;\r\ngfp = GFP_KERNEL;\r\n} else {\r\nnew_size = 8;\r\ngfp = GFP_NOIO;\r\n}\r\nargv = kmalloc(new_size * sizeof(*argv), gfp);\r\nif (argv) {\r\nmemcpy(argv, old_argv, *array_size * sizeof(*argv));\r\n*array_size = new_size;\r\n}\r\nkfree(old_argv);\r\nreturn argv;\r\n}\r\nint dm_split_args(int *argc, char ***argvp, char *input)\r\n{\r\nchar *start, *end = input, *out, **argv = NULL;\r\nunsigned array_size = 0;\r\n*argc = 0;\r\nif (!input) {\r\n*argvp = NULL;\r\nreturn 0;\r\n}\r\nargv = realloc_argv(&array_size, argv);\r\nif (!argv)\r\nreturn -ENOMEM;\r\nwhile (1) {\r\nstart = skip_spaces(end);\r\nif (!*start)\r\nbreak;\r\nend = out = start;\r\nwhile (*end) {\r\nif (*end == '\\' && *(end + 1)) {\r\n*out++ = *(end + 1);\r\nend += 2;\r\ncontinue;\r\n}\r\nif (isspace(*end))\r\nbreak;\r\n*out++ = *end++;\r\n}\r\nif ((*argc + 1) > array_size) {\r\nargv = realloc_argv(&array_size, argv);\r\nif (!argv)\r\nreturn -ENOMEM;\r\n}\r\nif (*end)\r\nend++;\r\n*out = '\0';\r\nargv[*argc] = start;\r\n(*argc)++;\r\n}\r\n*argvp = argv;\r\nreturn 0;\r\n}\r\nstatic int validate_hardware_logical_block_alignment(struct dm_table *table,\r\nstruct queue_limits *limits)\r\n{\r\nunsigned short device_logical_block_size_sects =\r\nlimits->logical_block_size >> SECTOR_SHIFT;\r\nunsigned short next_target_start = 0;\r\nunsigned short remaining = 0;\r\nstruct dm_target *uninitialized_var(ti);\r\nstruct queue_limits ti_limits;\r\nunsigned i = 0;\r\nwhile (i < dm_table_get_num_targets(table)) {\r\nti = dm_table_get_target(table, i++);\r\nblk_set_stacking_limits(&ti_limits);\r\nif (ti->type->iterate_devices)\r\nti->type->iterate_devices(ti, dm_set_device_limits,\r\n&ti_limits);\r\nif (remaining < ti->len &&\r\nremaining & ((ti_limits.logical_block_size >>\r\nSECTOR_SHIFT) - 1))\r\nbreak;\r\nnext_target_start =\r\n(unsigned short) ((next_target_start + ti->len) &\r\n(device_logical_block_size_sects - 1));\r\nremaining = next_target_start ?\r\ndevice_logical_block_size_sects - next_target_start : 0;\r\n}\r\nif (remaining) {\r\nDMWARN("%s: table line %u (start sect %llu len %llu) "\r\n"not aligned to h/w logical block size %u",\r\ndm_device_name(table->md), i,\r\n(unsigned long long) ti->begin,\r\n(unsigned long long) ti->len,\r\nlimits->logical_block_size);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint dm_table_add_target(struct dm_table *t, const char *type,\r\nsector_t start, sector_t len, char *params)\r\n{\r\nint r = -EINVAL, argc;\r\nchar **argv;\r\nstruct dm_target *tgt;\r\nif (t->singleton) {\r\nDMERR("%s: target type %s must appear alone in table",\r\ndm_device_name(t->md), t->targets->type->name);\r\nreturn -EINVAL;\r\n}\r\nBUG_ON(t->num_targets >= t->num_allocated);\r\ntgt = t->targets + t->num_targets;\r\nmemset(tgt, 0, sizeof(*tgt));\r\nif (!len) {\r\nDMERR("%s: zero-length target", dm_device_name(t->md));\r\nreturn -EINVAL;\r\n}\r\ntgt->type = dm_get_target_type(type);\r\nif (!tgt->type) {\r\nDMERR("%s: %s: unknown target type", dm_device_name(t->md),\r\ntype);\r\nreturn -EINVAL;\r\n}\r\nif (dm_target_needs_singleton(tgt->type)) {\r\nif (t->num_targets) {\r\nDMERR("%s: target type %s must appear alone in table",\r\ndm_device_name(t->md), type);\r\nreturn -EINVAL;\r\n}\r\nt->singleton = 1;\r\n}\r\nif (dm_target_always_writeable(tgt->type) && !(t->mode & FMODE_WRITE)) {\r\nDMERR("%s: target type %s may not be included in read-only tables",\r\ndm_device_name(t->md), type);\r\nreturn -EINVAL;\r\n}\r\nif (t->immutable_target_type) {\r\nif (t->immutable_target_type != tgt->type) {\r\nDMERR("%s: immutable target type %s cannot be mixed with other target types",\r\ndm_device_name(t->md), t->immutable_target_type->name);\r\nreturn -EINVAL;\r\n}\r\n} else if (dm_target_is_immutable(tgt->type)) {\r\nif (t->num_targets) {\r\nDMERR("%s: immutable target type %s cannot be mixed with other target types",\r\ndm_device_name(t->md), tgt->type->name);\r\nreturn -EINVAL;\r\n}\r\nt->immutable_target_type = tgt->type;\r\n}\r\ntgt->table = t;\r\ntgt->begin = start;\r\ntgt->len = len;\r\ntgt->error = "Unknown error";\r\nif (!adjoin(t, tgt)) {\r\ntgt->error = "Gap in table";\r\nr = -EINVAL;\r\ngoto bad;\r\n}\r\nr = dm_split_args(&argc, &argv, params);\r\nif (r) {\r\ntgt->error = "couldn't split parameters (insufficient memory)";\r\ngoto bad;\r\n}\r\nr = tgt->type->ctr(tgt, argc, argv);\r\nkfree(argv);\r\nif (r)\r\ngoto bad;\r\nt->highs[t->num_targets++] = tgt->begin + tgt->len - 1;\r\nif (!tgt->num_discard_bios && tgt->discards_supported)\r\nDMWARN("%s: %s: ignoring discards_supported because num_discard_bios is zero.",\r\ndm_device_name(t->md), type);\r\nreturn 0;\r\nbad:\r\nDMERR("%s: %s: %s", dm_device_name(t->md), type, tgt->error);\r\ndm_put_target_type(tgt->type);\r\nreturn r;\r\n}\r\nstatic int validate_next_arg(struct dm_arg *arg, struct dm_arg_set *arg_set,\r\nunsigned *value, char **error, unsigned grouped)\r\n{\r\nconst char *arg_str = dm_shift_arg(arg_set);\r\nchar dummy;\r\nif (!arg_str ||\r\n(sscanf(arg_str, "%u%c", value, &dummy) != 1) ||\r\n(*value < arg->min) ||\r\n(*value > arg->max) ||\r\n(grouped && arg_set->argc < *value)) {\r\n*error = arg->error;\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint dm_read_arg(struct dm_arg *arg, struct dm_arg_set *arg_set,\r\nunsigned *value, char **error)\r\n{\r\nreturn validate_next_arg(arg, arg_set, value, error, 0);\r\n}\r\nint dm_read_arg_group(struct dm_arg *arg, struct dm_arg_set *arg_set,\r\nunsigned *value, char **error)\r\n{\r\nreturn validate_next_arg(arg, arg_set, value, error, 1);\r\n}\r\nconst char *dm_shift_arg(struct dm_arg_set *as)\r\n{\r\nchar *r;\r\nif (as->argc) {\r\nas->argc--;\r\nr = *as->argv;\r\nas->argv++;\r\nreturn r;\r\n}\r\nreturn NULL;\r\n}\r\nvoid dm_consume_args(struct dm_arg_set *as, unsigned num_args)\r\n{\r\nBUG_ON(as->argc < num_args);\r\nas->argc -= num_args;\r\nas->argv += num_args;\r\n}\r\nstatic bool __table_type_request_based(unsigned table_type)\r\n{\r\nreturn (table_type == DM_TYPE_REQUEST_BASED ||\r\ntable_type == DM_TYPE_MQ_REQUEST_BASED);\r\n}\r\nstatic int dm_table_set_type(struct dm_table *t)\r\n{\r\nunsigned i;\r\nunsigned bio_based = 0, request_based = 0, hybrid = 0;\r\nbool use_blk_mq = false;\r\nstruct dm_target *tgt;\r\nstruct dm_dev_internal *dd;\r\nstruct list_head *devices;\r\nunsigned live_md_type = dm_get_md_type(t->md);\r\nfor (i = 0; i < t->num_targets; i++) {\r\ntgt = t->targets + i;\r\nif (dm_target_hybrid(tgt))\r\nhybrid = 1;\r\nelse if (dm_target_request_based(tgt))\r\nrequest_based = 1;\r\nelse\r\nbio_based = 1;\r\nif (bio_based && request_based) {\r\nDMWARN("Inconsistent table: different target types"\r\n" can't be mixed up");\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (hybrid && !bio_based && !request_based) {\r\nif (__table_type_request_based(live_md_type))\r\nrequest_based = 1;\r\nelse\r\nbio_based = 1;\r\n}\r\nif (bio_based) {\r\nt->type = DM_TYPE_BIO_BASED;\r\nreturn 0;\r\n}\r\nBUG_ON(!request_based);\r\nif (t->num_targets > 1) {\r\nDMWARN("Request-based dm doesn't support multiple targets yet");\r\nreturn -EINVAL;\r\n}\r\ndevices = dm_table_get_devices(t);\r\nlist_for_each_entry(dd, devices, list) {\r\nstruct request_queue *q = bdev_get_queue(dd->dm_dev->bdev);\r\nif (!blk_queue_stackable(q)) {\r\nDMERR("table load rejected: including"\r\n" non-request-stackable devices");\r\nreturn -EINVAL;\r\n}\r\nif (q->mq_ops)\r\nuse_blk_mq = true;\r\n}\r\nif (use_blk_mq) {\r\nlist_for_each_entry(dd, devices, list)\r\nif (!bdev_get_queue(dd->dm_dev->bdev)->mq_ops) {\r\nDMERR("table load rejected: not all devices"\r\n" are blk-mq request-stackable");\r\nreturn -EINVAL;\r\n}\r\nt->type = DM_TYPE_MQ_REQUEST_BASED;\r\n} else if (list_empty(devices) && __table_type_request_based(live_md_type)) {\r\nt->type = live_md_type;\r\n} else\r\nt->type = DM_TYPE_REQUEST_BASED;\r\nreturn 0;\r\n}\r\nunsigned dm_table_get_type(struct dm_table *t)\r\n{\r\nreturn t->type;\r\n}\r\nstruct target_type *dm_table_get_immutable_target_type(struct dm_table *t)\r\n{\r\nreturn t->immutable_target_type;\r\n}\r\nbool dm_table_request_based(struct dm_table *t)\r\n{\r\nreturn __table_type_request_based(dm_table_get_type(t));\r\n}\r\nbool dm_table_mq_request_based(struct dm_table *t)\r\n{\r\nreturn dm_table_get_type(t) == DM_TYPE_MQ_REQUEST_BASED;\r\n}\r\nstatic int dm_table_alloc_md_mempools(struct dm_table *t, struct mapped_device *md)\r\n{\r\nunsigned type = dm_table_get_type(t);\r\nunsigned per_bio_data_size = 0;\r\nstruct dm_target *tgt;\r\nunsigned i;\r\nif (unlikely(type == DM_TYPE_NONE)) {\r\nDMWARN("no table type is set, can't allocate mempools");\r\nreturn -EINVAL;\r\n}\r\nif (type == DM_TYPE_BIO_BASED)\r\nfor (i = 0; i < t->num_targets; i++) {\r\ntgt = t->targets + i;\r\nper_bio_data_size = max(per_bio_data_size, tgt->per_bio_data_size);\r\n}\r\nt->mempools = dm_alloc_md_mempools(md, type, t->integrity_supported, per_bio_data_size);\r\nif (!t->mempools)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid dm_table_free_md_mempools(struct dm_table *t)\r\n{\r\ndm_free_md_mempools(t->mempools);\r\nt->mempools = NULL;\r\n}\r\nstruct dm_md_mempools *dm_table_get_md_mempools(struct dm_table *t)\r\n{\r\nreturn t->mempools;\r\n}\r\nstatic int setup_indexes(struct dm_table *t)\r\n{\r\nint i;\r\nunsigned int total = 0;\r\nsector_t *indexes;\r\nfor (i = t->depth - 2; i >= 0; i--) {\r\nt->counts[i] = dm_div_up(t->counts[i + 1], CHILDREN_PER_NODE);\r\ntotal += t->counts[i];\r\n}\r\nindexes = (sector_t *) dm_vcalloc(total, (unsigned long) NODE_SIZE);\r\nif (!indexes)\r\nreturn -ENOMEM;\r\nfor (i = t->depth - 2; i >= 0; i--) {\r\nt->index[i] = indexes;\r\nindexes += (KEYS_PER_NODE * t->counts[i]);\r\nsetup_btree_index(i, t);\r\n}\r\nreturn 0;\r\n}\r\nstatic int dm_table_build_index(struct dm_table *t)\r\n{\r\nint r = 0;\r\nunsigned int leaf_nodes;\r\nleaf_nodes = dm_div_up(t->num_targets, KEYS_PER_NODE);\r\nt->depth = 1 + int_log(leaf_nodes, CHILDREN_PER_NODE);\r\nt->counts[t->depth - 1] = leaf_nodes;\r\nt->index[t->depth - 1] = t->highs;\r\nif (t->depth >= 2)\r\nr = setup_indexes(t);\r\nreturn r;\r\n}\r\nstatic struct gendisk * dm_table_get_integrity_disk(struct dm_table *t,\r\nbool match_all)\r\n{\r\nstruct list_head *devices = dm_table_get_devices(t);\r\nstruct dm_dev_internal *dd = NULL;\r\nstruct gendisk *prev_disk = NULL, *template_disk = NULL;\r\nlist_for_each_entry(dd, devices, list) {\r\ntemplate_disk = dd->dm_dev->bdev->bd_disk;\r\nif (!blk_get_integrity(template_disk))\r\ngoto no_integrity;\r\nif (!match_all && !blk_integrity_is_initialized(template_disk))\r\ncontinue;\r\nelse if (prev_disk &&\r\nblk_integrity_compare(prev_disk, template_disk) < 0)\r\ngoto no_integrity;\r\nprev_disk = template_disk;\r\n}\r\nreturn template_disk;\r\nno_integrity:\r\nif (prev_disk)\r\nDMWARN("%s: integrity not set: %s and %s profile mismatch",\r\ndm_device_name(t->md),\r\nprev_disk->disk_name,\r\ntemplate_disk->disk_name);\r\nreturn NULL;\r\n}\r\nstatic int dm_table_prealloc_integrity(struct dm_table *t, struct mapped_device *md)\r\n{\r\nstruct gendisk *template_disk = NULL;\r\ntemplate_disk = dm_table_get_integrity_disk(t, false);\r\nif (!template_disk)\r\nreturn 0;\r\nif (!blk_integrity_is_initialized(dm_disk(md))) {\r\nt->integrity_supported = 1;\r\nreturn blk_integrity_register(dm_disk(md), NULL);\r\n}\r\nif (blk_integrity_is_initialized(template_disk) &&\r\nblk_integrity_compare(dm_disk(md), template_disk) < 0) {\r\nDMWARN("%s: conflict with existing integrity profile: "\r\n"%s profile mismatch",\r\ndm_device_name(t->md),\r\ntemplate_disk->disk_name);\r\nreturn 1;\r\n}\r\nt->integrity_supported = 1;\r\nreturn 0;\r\n}\r\nint dm_table_complete(struct dm_table *t)\r\n{\r\nint r;\r\nr = dm_table_set_type(t);\r\nif (r) {\r\nDMERR("unable to set table type");\r\nreturn r;\r\n}\r\nr = dm_table_build_index(t);\r\nif (r) {\r\nDMERR("unable to build btrees");\r\nreturn r;\r\n}\r\nr = dm_table_prealloc_integrity(t, t->md);\r\nif (r) {\r\nDMERR("could not register integrity profile.");\r\nreturn r;\r\n}\r\nr = dm_table_alloc_md_mempools(t, t->md);\r\nif (r)\r\nDMERR("unable to allocate mempools");\r\nreturn r;\r\n}\r\nvoid dm_table_event_callback(struct dm_table *t,\r\nvoid (*fn)(void *), void *context)\r\n{\r\nmutex_lock(&_event_lock);\r\nt->event_fn = fn;\r\nt->event_context = context;\r\nmutex_unlock(&_event_lock);\r\n}\r\nvoid dm_table_event(struct dm_table *t)\r\n{\r\nBUG_ON(in_interrupt());\r\nmutex_lock(&_event_lock);\r\nif (t->event_fn)\r\nt->event_fn(t->event_context);\r\nmutex_unlock(&_event_lock);\r\n}\r\nsector_t dm_table_get_size(struct dm_table *t)\r\n{\r\nreturn t->num_targets ? (t->highs[t->num_targets - 1] + 1) : 0;\r\n}\r\nstruct dm_target *dm_table_get_target(struct dm_table *t, unsigned int index)\r\n{\r\nif (index >= t->num_targets)\r\nreturn NULL;\r\nreturn t->targets + index;\r\n}\r\nstruct dm_target *dm_table_find_target(struct dm_table *t, sector_t sector)\r\n{\r\nunsigned int l, n = 0, k = 0;\r\nsector_t *node;\r\nfor (l = 0; l < t->depth; l++) {\r\nn = get_child(n, k);\r\nnode = get_node(t, l, n);\r\nfor (k = 0; k < KEYS_PER_NODE; k++)\r\nif (node[k] >= sector)\r\nbreak;\r\n}\r\nreturn &t->targets[(KEYS_PER_NODE * n) + k];\r\n}\r\nstatic int count_device(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nunsigned *num_devices = data;\r\n(*num_devices)++;\r\nreturn 0;\r\n}\r\nbool dm_table_has_no_data_devices(struct dm_table *table)\r\n{\r\nstruct dm_target *uninitialized_var(ti);\r\nunsigned i = 0, num_devices = 0;\r\nwhile (i < dm_table_get_num_targets(table)) {\r\nti = dm_table_get_target(table, i++);\r\nif (!ti->type->iterate_devices)\r\nreturn false;\r\nti->type->iterate_devices(ti, count_device, &num_devices);\r\nif (num_devices)\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nint dm_calculate_queue_limits(struct dm_table *table,\r\nstruct queue_limits *limits)\r\n{\r\nstruct dm_target *uninitialized_var(ti);\r\nstruct queue_limits ti_limits;\r\nunsigned i = 0;\r\nblk_set_stacking_limits(limits);\r\nwhile (i < dm_table_get_num_targets(table)) {\r\nblk_set_stacking_limits(&ti_limits);\r\nti = dm_table_get_target(table, i++);\r\nif (!ti->type->iterate_devices)\r\ngoto combine_limits;\r\nti->type->iterate_devices(ti, dm_set_device_limits,\r\n&ti_limits);\r\nif (ti->type->io_hints)\r\nti->type->io_hints(ti, &ti_limits);\r\nif (ti->type->iterate_devices(ti, device_area_is_invalid,\r\n&ti_limits))\r\nreturn -EINVAL;\r\ncombine_limits:\r\nif (blk_stack_limits(limits, &ti_limits, 0) < 0)\r\nDMWARN("%s: adding target device "\r\n"(start sect %llu len %llu) "\r\n"caused an alignment inconsistency",\r\ndm_device_name(table->md),\r\n(unsigned long long) ti->begin,\r\n(unsigned long long) ti->len);\r\n}\r\nreturn validate_hardware_logical_block_alignment(table, limits);\r\n}\r\nstatic void dm_table_set_integrity(struct dm_table *t)\r\n{\r\nstruct gendisk *template_disk = NULL;\r\nif (!blk_get_integrity(dm_disk(t->md)))\r\nreturn;\r\ntemplate_disk = dm_table_get_integrity_disk(t, true);\r\nif (template_disk)\r\nblk_integrity_register(dm_disk(t->md),\r\nblk_get_integrity(template_disk));\r\nelse if (blk_integrity_is_initialized(dm_disk(t->md)))\r\nDMWARN("%s: device no longer has a valid integrity profile",\r\ndm_device_name(t->md));\r\nelse\r\nDMWARN("%s: unable to establish an integrity profile",\r\ndm_device_name(t->md));\r\n}\r\nstatic int device_flush_capable(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nunsigned flush = (*(unsigned *)data);\r\nstruct request_queue *q = bdev_get_queue(dev->bdev);\r\nreturn q && (q->flush_flags & flush);\r\n}\r\nstatic bool dm_table_supports_flush(struct dm_table *t, unsigned flush)\r\n{\r\nstruct dm_target *ti;\r\nunsigned i = 0;\r\nwhile (i < dm_table_get_num_targets(t)) {\r\nti = dm_table_get_target(t, i++);\r\nif (!ti->num_flush_bios)\r\ncontinue;\r\nif (ti->flush_supported)\r\nreturn true;\r\nif (ti->type->iterate_devices &&\r\nti->type->iterate_devices(ti, device_flush_capable, &flush))\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool dm_table_discard_zeroes_data(struct dm_table *t)\r\n{\r\nstruct dm_target *ti;\r\nunsigned i = 0;\r\nwhile (i < dm_table_get_num_targets(t)) {\r\nti = dm_table_get_target(t, i++);\r\nif (ti->discard_zeroes_data_unsupported)\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int device_is_nonrot(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nstruct request_queue *q = bdev_get_queue(dev->bdev);\r\nreturn q && blk_queue_nonrot(q);\r\n}\r\nstatic int device_is_not_random(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nstruct request_queue *q = bdev_get_queue(dev->bdev);\r\nreturn q && !blk_queue_add_random(q);\r\n}\r\nstatic int queue_supports_sg_merge(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nstruct request_queue *q = bdev_get_queue(dev->bdev);\r\nreturn q && !test_bit(QUEUE_FLAG_NO_SG_MERGE, &q->queue_flags);\r\n}\r\nstatic int queue_supports_sg_gaps(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nstruct request_queue *q = bdev_get_queue(dev->bdev);\r\nreturn q && !test_bit(QUEUE_FLAG_SG_GAPS, &q->queue_flags);\r\n}\r\nstatic bool dm_table_all_devices_attribute(struct dm_table *t,\r\niterate_devices_callout_fn func)\r\n{\r\nstruct dm_target *ti;\r\nunsigned i = 0;\r\nwhile (i < dm_table_get_num_targets(t)) {\r\nti = dm_table_get_target(t, i++);\r\nif (!ti->type->iterate_devices ||\r\n!ti->type->iterate_devices(ti, func, NULL))\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int device_not_write_same_capable(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nstruct request_queue *q = bdev_get_queue(dev->bdev);\r\nreturn q && !q->limits.max_write_same_sectors;\r\n}\r\nstatic bool dm_table_supports_write_same(struct dm_table *t)\r\n{\r\nstruct dm_target *ti;\r\nunsigned i = 0;\r\nwhile (i < dm_table_get_num_targets(t)) {\r\nti = dm_table_get_target(t, i++);\r\nif (!ti->num_write_same_bios)\r\nreturn false;\r\nif (!ti->type->iterate_devices ||\r\nti->type->iterate_devices(ti, device_not_write_same_capable, NULL))\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int device_discard_capable(struct dm_target *ti, struct dm_dev *dev,\r\nsector_t start, sector_t len, void *data)\r\n{\r\nstruct request_queue *q = bdev_get_queue(dev->bdev);\r\nreturn q && blk_queue_discard(q);\r\n}\r\nstatic bool dm_table_supports_discards(struct dm_table *t)\r\n{\r\nstruct dm_target *ti;\r\nunsigned i = 0;\r\nwhile (i < dm_table_get_num_targets(t)) {\r\nti = dm_table_get_target(t, i++);\r\nif (!ti->num_discard_bios)\r\ncontinue;\r\nif (ti->discards_supported)\r\nreturn true;\r\nif (ti->type->iterate_devices &&\r\nti->type->iterate_devices(ti, device_discard_capable, NULL))\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nvoid dm_table_set_restrictions(struct dm_table *t, struct request_queue *q,\r\nstruct queue_limits *limits)\r\n{\r\nunsigned flush = 0;\r\nq->limits = *limits;\r\nif (!dm_table_supports_discards(t))\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_DISCARD, q);\r\nelse\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);\r\nif (dm_table_supports_flush(t, REQ_FLUSH)) {\r\nflush |= REQ_FLUSH;\r\nif (dm_table_supports_flush(t, REQ_FUA))\r\nflush |= REQ_FUA;\r\n}\r\nblk_queue_flush(q, flush);\r\nif (!dm_table_discard_zeroes_data(t))\r\nq->limits.discard_zeroes_data = 0;\r\nif (dm_table_all_devices_attribute(t, device_is_nonrot))\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, q);\r\nelse\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_NONROT, q);\r\nif (!dm_table_supports_write_same(t))\r\nq->limits.max_write_same_sectors = 0;\r\nif (dm_table_all_devices_attribute(t, queue_supports_sg_merge))\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_NO_SG_MERGE, q);\r\nelse\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NO_SG_MERGE, q);\r\nif (dm_table_all_devices_attribute(t, queue_supports_sg_gaps))\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_SG_GAPS, q);\r\nelse\r\nqueue_flag_set_unlocked(QUEUE_FLAG_SG_GAPS, q);\r\ndm_table_set_integrity(t);\r\nif (blk_queue_add_random(q) && dm_table_all_devices_attribute(t, device_is_not_random))\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_ADD_RANDOM, q);\r\nsmp_mb();\r\nif (dm_table_request_based(t))\r\nqueue_flag_set_unlocked(QUEUE_FLAG_STACKABLE, q);\r\n}\r\nunsigned int dm_table_get_num_targets(struct dm_table *t)\r\n{\r\nreturn t->num_targets;\r\n}\r\nstruct list_head *dm_table_get_devices(struct dm_table *t)\r\n{\r\nreturn &t->devices;\r\n}\r\nfmode_t dm_table_get_mode(struct dm_table *t)\r\n{\r\nreturn t->mode;\r\n}\r\nstatic void suspend_targets(struct dm_table *t, enum suspend_mode mode)\r\n{\r\nint i = t->num_targets;\r\nstruct dm_target *ti = t->targets;\r\nwhile (i--) {\r\nswitch (mode) {\r\ncase PRESUSPEND:\r\nif (ti->type->presuspend)\r\nti->type->presuspend(ti);\r\nbreak;\r\ncase PRESUSPEND_UNDO:\r\nif (ti->type->presuspend_undo)\r\nti->type->presuspend_undo(ti);\r\nbreak;\r\ncase POSTSUSPEND:\r\nif (ti->type->postsuspend)\r\nti->type->postsuspend(ti);\r\nbreak;\r\n}\r\nti++;\r\n}\r\n}\r\nvoid dm_table_presuspend_targets(struct dm_table *t)\r\n{\r\nif (!t)\r\nreturn;\r\nsuspend_targets(t, PRESUSPEND);\r\n}\r\nvoid dm_table_presuspend_undo_targets(struct dm_table *t)\r\n{\r\nif (!t)\r\nreturn;\r\nsuspend_targets(t, PRESUSPEND_UNDO);\r\n}\r\nvoid dm_table_postsuspend_targets(struct dm_table *t)\r\n{\r\nif (!t)\r\nreturn;\r\nsuspend_targets(t, POSTSUSPEND);\r\n}\r\nint dm_table_resume_targets(struct dm_table *t)\r\n{\r\nint i, r = 0;\r\nfor (i = 0; i < t->num_targets; i++) {\r\nstruct dm_target *ti = t->targets + i;\r\nif (!ti->type->preresume)\r\ncontinue;\r\nr = ti->type->preresume(ti);\r\nif (r) {\r\nDMERR("%s: %s: preresume failed, error = %d",\r\ndm_device_name(t->md), ti->type->name, r);\r\nreturn r;\r\n}\r\n}\r\nfor (i = 0; i < t->num_targets; i++) {\r\nstruct dm_target *ti = t->targets + i;\r\nif (ti->type->resume)\r\nti->type->resume(ti);\r\n}\r\nreturn 0;\r\n}\r\nvoid dm_table_add_target_callbacks(struct dm_table *t, struct dm_target_callbacks *cb)\r\n{\r\nlist_add(&cb->list, &t->target_callbacks);\r\n}\r\nint dm_table_any_congested(struct dm_table *t, int bdi_bits)\r\n{\r\nstruct dm_dev_internal *dd;\r\nstruct list_head *devices = dm_table_get_devices(t);\r\nstruct dm_target_callbacks *cb;\r\nint r = 0;\r\nlist_for_each_entry(dd, devices, list) {\r\nstruct request_queue *q = bdev_get_queue(dd->dm_dev->bdev);\r\nchar b[BDEVNAME_SIZE];\r\nif (likely(q))\r\nr |= bdi_congested(&q->backing_dev_info, bdi_bits);\r\nelse\r\nDMWARN_LIMIT("%s: any_congested: nonexistent device %s",\r\ndm_device_name(t->md),\r\nbdevname(dd->dm_dev->bdev, b));\r\n}\r\nlist_for_each_entry(cb, &t->target_callbacks, list)\r\nif (cb->congested_fn)\r\nr |= cb->congested_fn(cb, bdi_bits);\r\nreturn r;\r\n}\r\nstruct mapped_device *dm_table_get_md(struct dm_table *t)\r\n{\r\nreturn t->md;\r\n}\r\nvoid dm_table_run_md_queue_async(struct dm_table *t)\r\n{\r\nstruct mapped_device *md;\r\nstruct request_queue *queue;\r\nunsigned long flags;\r\nif (!dm_table_request_based(t))\r\nreturn;\r\nmd = dm_table_get_md(t);\r\nqueue = dm_get_md_queue(md);\r\nif (queue) {\r\nif (queue->mq_ops)\r\nblk_mq_run_hw_queues(queue, true);\r\nelse {\r\nspin_lock_irqsave(queue->queue_lock, flags);\r\nblk_run_queue_async(queue);\r\nspin_unlock_irqrestore(queue->queue_lock, flags);\r\n}\r\n}\r\n}
