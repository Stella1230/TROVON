static enum retry_state ecache_work_evict_list(struct ct_pcpu *pcpu)\r\n{\r\nstruct nf_conn *refs[16];\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct hlist_nulls_node *n;\r\nunsigned int evicted = 0;\r\nenum retry_state ret = STATE_DONE;\r\nspin_lock(&pcpu->lock);\r\nhlist_nulls_for_each_entry(h, n, &pcpu->dying, hnnode) {\r\nstruct nf_conn *ct = nf_ct_tuplehash_to_ctrack(h);\r\nif (nf_ct_is_dying(ct))\r\ncontinue;\r\nif (nf_conntrack_event(IPCT_DESTROY, ct)) {\r\nret = STATE_CONGESTED;\r\nbreak;\r\n}\r\nset_bit(IPS_DYING_BIT, &ct->status);\r\nrefs[evicted] = ct;\r\nif (++evicted >= ARRAY_SIZE(refs)) {\r\nret = STATE_RESTART;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&pcpu->lock);\r\nwhile (evicted)\r\nnf_ct_put(refs[--evicted]);\r\nreturn ret;\r\n}\r\nstatic void ecache_work(struct work_struct *work)\r\n{\r\nstruct netns_ct *ctnet =\r\ncontainer_of(work, struct netns_ct, ecache_dwork.work);\r\nint cpu, delay = -1;\r\nstruct ct_pcpu *pcpu;\r\nlocal_bh_disable();\r\nfor_each_possible_cpu(cpu) {\r\nenum retry_state ret;\r\npcpu = per_cpu_ptr(ctnet->pcpu_lists, cpu);\r\nret = ecache_work_evict_list(pcpu);\r\nswitch (ret) {\r\ncase STATE_CONGESTED:\r\ndelay = ECACHE_RETRY_WAIT;\r\ngoto out;\r\ncase STATE_RESTART:\r\ndelay = 0;\r\nbreak;\r\ncase STATE_DONE:\r\nbreak;\r\n}\r\n}\r\nout:\r\nlocal_bh_enable();\r\nctnet->ecache_dwork_pending = delay > 0;\r\nif (delay >= 0)\r\nschedule_delayed_work(&ctnet->ecache_dwork, delay);\r\n}\r\nvoid nf_ct_deliver_cached_events(struct nf_conn *ct)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nunsigned long events, missed;\r\nstruct nf_ct_event_notifier *notify;\r\nstruct nf_conntrack_ecache *e;\r\nstruct nf_ct_event item;\r\nint ret;\r\nrcu_read_lock();\r\nnotify = rcu_dereference(net->ct.nf_conntrack_event_cb);\r\nif (notify == NULL)\r\ngoto out_unlock;\r\ne = nf_ct_ecache_find(ct);\r\nif (e == NULL)\r\ngoto out_unlock;\r\nevents = xchg(&e->cache, 0);\r\nif (!nf_ct_is_confirmed(ct) || nf_ct_is_dying(ct) || !events)\r\ngoto out_unlock;\r\nmissed = e->missed;\r\nif (!((events | missed) & e->ctmask))\r\ngoto out_unlock;\r\nitem.ct = ct;\r\nitem.portid = 0;\r\nitem.report = 0;\r\nret = notify->fcn(events | missed, &item);\r\nif (likely(ret >= 0 && !missed))\r\ngoto out_unlock;\r\nspin_lock_bh(&ct->lock);\r\nif (ret < 0)\r\ne->missed |= events;\r\nelse\r\ne->missed &= ~missed;\r\nspin_unlock_bh(&ct->lock);\r\nout_unlock:\r\nrcu_read_unlock();\r\n}\r\nint nf_conntrack_register_notifier(struct net *net,\r\nstruct nf_ct_event_notifier *new)\r\n{\r\nint ret;\r\nstruct nf_ct_event_notifier *notify;\r\nmutex_lock(&nf_ct_ecache_mutex);\r\nnotify = rcu_dereference_protected(net->ct.nf_conntrack_event_cb,\r\nlockdep_is_held(&nf_ct_ecache_mutex));\r\nif (notify != NULL) {\r\nret = -EBUSY;\r\ngoto out_unlock;\r\n}\r\nrcu_assign_pointer(net->ct.nf_conntrack_event_cb, new);\r\nret = 0;\r\nout_unlock:\r\nmutex_unlock(&nf_ct_ecache_mutex);\r\nreturn ret;\r\n}\r\nvoid nf_conntrack_unregister_notifier(struct net *net,\r\nstruct nf_ct_event_notifier *new)\r\n{\r\nstruct nf_ct_event_notifier *notify;\r\nmutex_lock(&nf_ct_ecache_mutex);\r\nnotify = rcu_dereference_protected(net->ct.nf_conntrack_event_cb,\r\nlockdep_is_held(&nf_ct_ecache_mutex));\r\nBUG_ON(notify != new);\r\nRCU_INIT_POINTER(net->ct.nf_conntrack_event_cb, NULL);\r\nmutex_unlock(&nf_ct_ecache_mutex);\r\n}\r\nint nf_ct_expect_register_notifier(struct net *net,\r\nstruct nf_exp_event_notifier *new)\r\n{\r\nint ret;\r\nstruct nf_exp_event_notifier *notify;\r\nmutex_lock(&nf_ct_ecache_mutex);\r\nnotify = rcu_dereference_protected(net->ct.nf_expect_event_cb,\r\nlockdep_is_held(&nf_ct_ecache_mutex));\r\nif (notify != NULL) {\r\nret = -EBUSY;\r\ngoto out_unlock;\r\n}\r\nrcu_assign_pointer(net->ct.nf_expect_event_cb, new);\r\nret = 0;\r\nout_unlock:\r\nmutex_unlock(&nf_ct_ecache_mutex);\r\nreturn ret;\r\n}\r\nvoid nf_ct_expect_unregister_notifier(struct net *net,\r\nstruct nf_exp_event_notifier *new)\r\n{\r\nstruct nf_exp_event_notifier *notify;\r\nmutex_lock(&nf_ct_ecache_mutex);\r\nnotify = rcu_dereference_protected(net->ct.nf_expect_event_cb,\r\nlockdep_is_held(&nf_ct_ecache_mutex));\r\nBUG_ON(notify != new);\r\nRCU_INIT_POINTER(net->ct.nf_expect_event_cb, NULL);\r\nmutex_unlock(&nf_ct_ecache_mutex);\r\n}\r\nstatic int nf_conntrack_event_init_sysctl(struct net *net)\r\n{\r\nstruct ctl_table *table;\r\ntable = kmemdup(event_sysctl_table, sizeof(event_sysctl_table),\r\nGFP_KERNEL);\r\nif (!table)\r\ngoto out;\r\ntable[0].data = &net->ct.sysctl_events;\r\nif (net->user_ns != &init_user_ns)\r\ntable[0].procname = NULL;\r\nnet->ct.event_sysctl_header =\r\nregister_net_sysctl(net, "net/netfilter", table);\r\nif (!net->ct.event_sysctl_header) {\r\nprintk(KERN_ERR "nf_ct_event: can't register to sysctl.\n");\r\ngoto out_register;\r\n}\r\nreturn 0;\r\nout_register:\r\nkfree(table);\r\nout:\r\nreturn -ENOMEM;\r\n}\r\nstatic void nf_conntrack_event_fini_sysctl(struct net *net)\r\n{\r\nstruct ctl_table *table;\r\ntable = net->ct.event_sysctl_header->ctl_table_arg;\r\nunregister_net_sysctl_table(net->ct.event_sysctl_header);\r\nkfree(table);\r\n}\r\nstatic int nf_conntrack_event_init_sysctl(struct net *net)\r\n{\r\nreturn 0;\r\n}\r\nstatic void nf_conntrack_event_fini_sysctl(struct net *net)\r\n{\r\n}\r\nint nf_conntrack_ecache_pernet_init(struct net *net)\r\n{\r\nnet->ct.sysctl_events = nf_ct_events;\r\nINIT_DELAYED_WORK(&net->ct.ecache_dwork, ecache_work);\r\nreturn nf_conntrack_event_init_sysctl(net);\r\n}\r\nvoid nf_conntrack_ecache_pernet_fini(struct net *net)\r\n{\r\ncancel_delayed_work_sync(&net->ct.ecache_dwork);\r\nnf_conntrack_event_fini_sysctl(net);\r\n}\r\nint nf_conntrack_ecache_init(void)\r\n{\r\nint ret = nf_ct_extend_register(&event_extend);\r\nif (ret < 0)\r\npr_err("nf_ct_event: Unable to register event extension.\n");\r\nreturn ret;\r\n}\r\nvoid nf_conntrack_ecache_fini(void)\r\n{\r\nnf_ct_extend_unregister(&event_extend);\r\n}
