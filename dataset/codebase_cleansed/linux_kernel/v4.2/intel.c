static void early_init_intel(struct cpuinfo_x86 *c)\r\n{\r\nu64 misc_enable;\r\nif (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {\r\nif (msr_clear_bit(MSR_IA32_MISC_ENABLE,\r\nMSR_IA32_MISC_ENABLE_LIMIT_CPUID_BIT) > 0) {\r\nc->cpuid_level = cpuid_eax(0);\r\nget_cpu_cap(c);\r\n}\r\n}\r\nif ((c->x86 == 0xf && c->x86_model >= 0x03) ||\r\n(c->x86 == 0x6 && c->x86_model >= 0x0e))\r\nset_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);\r\nif (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64)) {\r\nunsigned lower_word;\r\nwrmsr(MSR_IA32_UCODE_REV, 0, 0);\r\nsync_core();\r\nrdmsr(MSR_IA32_UCODE_REV, lower_word, c->microcode);\r\n}\r\nif (c->x86 == 6 && c->x86_model == 0x1c && c->x86_mask <= 2 &&\r\nc->microcode < 0x20e) {\r\nprintk(KERN_WARNING "Atom PSE erratum detected, BIOS microcode update recommended\n");\r\nclear_cpu_cap(c, X86_FEATURE_PSE);\r\n}\r\n#ifdef CONFIG_X86_64\r\nset_cpu_cap(c, X86_FEATURE_SYSENTER32);\r\n#else\r\nif (c->x86 == 15 && c->x86_cache_alignment == 64)\r\nc->x86_cache_alignment = 128;\r\n#endif\r\nif (c->x86 == 0xF && c->x86_model == 0x3\r\n&& (c->x86_mask == 0x3 || c->x86_mask == 0x4))\r\nc->x86_phys_bits = 36;\r\nif (c->x86_power & (1 << 8)) {\r\nset_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);\r\nset_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);\r\nif (!check_tsc_unstable())\r\nset_sched_clock_stable();\r\n}\r\nif (c->x86 == 6) {\r\nswitch (c->x86_model) {\r\ncase 0x27:\r\ncase 0x35:\r\nset_cpu_cap(c, X86_FEATURE_NONSTOP_TSC_S3);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nif (c->x86 == 6 && c->x86_model < 15)\r\nclear_cpu_cap(c, X86_FEATURE_PAT);\r\n#ifdef CONFIG_KMEMCHECK\r\nif (c->x86 == 15)\r\nif (msr_clear_bit(MSR_IA32_MISC_ENABLE,\r\nMSR_IA32_MISC_ENABLE_FAST_STRING_BIT) > 0)\r\npr_info("kmemcheck: Disabling fast string operations\n");\r\n#endif\r\nif (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {\r\nrdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);\r\nif (!(misc_enable & MSR_IA32_MISC_ENABLE_FAST_STRING)) {\r\nprintk(KERN_INFO "Disabled fast string operations\n");\r\nsetup_clear_cpu_cap(X86_FEATURE_REP_GOOD);\r\nsetup_clear_cpu_cap(X86_FEATURE_ERMS);\r\n}\r\n}\r\nif (c->x86 == 5 && c->x86_model == 9) {\r\npr_info("Disabling PGE capability bit\n");\r\nsetup_clear_cpu_cap(X86_FEATURE_PGE);\r\n}\r\n}\r\nint ppro_with_ram_bug(void)\r\n{\r\nif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&\r\nboot_cpu_data.x86 == 6 &&\r\nboot_cpu_data.x86_model == 1 &&\r\nboot_cpu_data.x86_mask < 8) {\r\nprintk(KERN_INFO "Pentium Pro with Errata#50 detected. Taking evasive action.\n");\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void intel_smp_check(struct cpuinfo_x86 *c)\r\n{\r\nif (!c->cpu_index)\r\nreturn;\r\nif (c->x86 == 5 &&\r\nc->x86_mask >= 1 && c->x86_mask <= 4 &&\r\nc->x86_model <= 3) {\r\nWARN_ONCE(1, "WARNING: SMP operation may be unreliable"\r\n"with B stepping processors.\n");\r\n}\r\n}\r\nstatic int __init forcepae_setup(char *__unused)\r\n{\r\nforcepae = 1;\r\nreturn 1;\r\n}\r\nstatic void intel_workarounds(struct cpuinfo_x86 *c)\r\n{\r\n#ifdef CONFIG_X86_F00F_BUG\r\nclear_cpu_bug(c, X86_BUG_F00F);\r\nif (!paravirt_enabled() && c->x86 == 5 && c->x86_model < 9) {\r\nstatic int f00f_workaround_enabled;\r\nset_cpu_bug(c, X86_BUG_F00F);\r\nif (!f00f_workaround_enabled) {\r\nprintk(KERN_NOTICE "Intel Pentium with F0 0F bug - workaround enabled.\n");\r\nf00f_workaround_enabled = 1;\r\n}\r\n}\r\n#endif\r\nif ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)\r\nclear_cpu_cap(c, X86_FEATURE_SEP);\r\nif (forcepae) {\r\nprintk(KERN_WARNING "PAE forced!\n");\r\nset_cpu_cap(c, X86_FEATURE_PAE);\r\nadd_taint(TAINT_CPU_OUT_OF_SPEC, LOCKDEP_NOW_UNRELIABLE);\r\n}\r\nif ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {\r\nif (msr_set_bit(MSR_IA32_MISC_ENABLE,\r\nMSR_IA32_MISC_ENABLE_PREFETCH_DISABLE_BIT)\r\n> 0) {\r\npr_info("CPU: C0 stepping P4 Xeon detected.\n");\r\npr_info("CPU: Disabling hardware prefetching (Errata 037)\n");\r\n}\r\n}\r\nif (cpu_has_apic && (c->x86<<8 | c->x86_model<<4) == 0x520 &&\r\n(c->x86_mask < 0x6 || c->x86_mask == 0xb))\r\nset_cpu_bug(c, X86_BUG_11AP);\r\n#ifdef CONFIG_X86_INTEL_USERCOPY\r\nswitch (c->x86) {\r\ncase 4:\r\nbreak;\r\ncase 5:\r\nbreak;\r\ncase 6:\r\nmovsl_mask.mask = 7;\r\nbreak;\r\ncase 15:\r\nmovsl_mask.mask = 7;\r\nbreak;\r\n}\r\n#endif\r\nintel_smp_check(c);\r\n}\r\nstatic void intel_workarounds(struct cpuinfo_x86 *c)\r\n{\r\n}\r\nstatic void srat_detect_node(struct cpuinfo_x86 *c)\r\n{\r\n#ifdef CONFIG_NUMA\r\nunsigned node;\r\nint cpu = smp_processor_id();\r\nnode = numa_cpu_node(cpu);\r\nif (node == NUMA_NO_NODE || !node_online(node)) {\r\nnode = cpu_to_node(cpu);\r\n}\r\nnuma_set_node(cpu, node);\r\n#endif\r\n}\r\nstatic int intel_num_cpu_cores(struct cpuinfo_x86 *c)\r\n{\r\nunsigned int eax, ebx, ecx, edx;\r\nif (c->cpuid_level < 4)\r\nreturn 1;\r\ncpuid_count(4, 0, &eax, &ebx, &ecx, &edx);\r\nif (eax & 0x1f)\r\nreturn (eax >> 26) + 1;\r\nelse\r\nreturn 1;\r\n}\r\nstatic void detect_vmx_virtcap(struct cpuinfo_x86 *c)\r\n{\r\n#define X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW 0x00200000\r\n#define X86_VMX_FEATURE_PROC_CTLS_VNMI 0x00400000\r\n#define X86_VMX_FEATURE_PROC_CTLS_2ND_CTLS 0x80000000\r\n#define X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC 0x00000001\r\n#define X86_VMX_FEATURE_PROC_CTLS2_EPT 0x00000002\r\n#define X86_VMX_FEATURE_PROC_CTLS2_VPID 0x00000020\r\nu32 vmx_msr_low, vmx_msr_high, msr_ctl, msr_ctl2;\r\nclear_cpu_cap(c, X86_FEATURE_TPR_SHADOW);\r\nclear_cpu_cap(c, X86_FEATURE_VNMI);\r\nclear_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);\r\nclear_cpu_cap(c, X86_FEATURE_EPT);\r\nclear_cpu_cap(c, X86_FEATURE_VPID);\r\nrdmsr(MSR_IA32_VMX_PROCBASED_CTLS, vmx_msr_low, vmx_msr_high);\r\nmsr_ctl = vmx_msr_high | vmx_msr_low;\r\nif (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW)\r\nset_cpu_cap(c, X86_FEATURE_TPR_SHADOW);\r\nif (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_VNMI)\r\nset_cpu_cap(c, X86_FEATURE_VNMI);\r\nif (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_2ND_CTLS) {\r\nrdmsr(MSR_IA32_VMX_PROCBASED_CTLS2,\r\nvmx_msr_low, vmx_msr_high);\r\nmsr_ctl2 = vmx_msr_high | vmx_msr_low;\r\nif ((msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC) &&\r\n(msr_ctl & X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW))\r\nset_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);\r\nif (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_EPT)\r\nset_cpu_cap(c, X86_FEATURE_EPT);\r\nif (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VPID)\r\nset_cpu_cap(c, X86_FEATURE_VPID);\r\n}\r\n}\r\nstatic void init_intel(struct cpuinfo_x86 *c)\r\n{\r\nunsigned int l2 = 0;\r\nearly_init_intel(c);\r\nintel_workarounds(c);\r\ndetect_extended_topology(c);\r\nif (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {\r\nc->x86_max_cores = intel_num_cpu_cores(c);\r\n#ifdef CONFIG_X86_32\r\ndetect_ht(c);\r\n#endif\r\n}\r\nl2 = init_intel_cacheinfo(c);\r\nif (l2 == 0) {\r\ncpu_detect_cache_sizes(c);\r\nl2 = c->x86_cache_size;\r\n}\r\nif (c->cpuid_level > 9) {\r\nunsigned eax = cpuid_eax(10);\r\nif ((eax & 0xff) && (((eax>>8) & 0xff) > 1))\r\nset_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);\r\n}\r\nif (cpu_has_xmm2)\r\nset_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);\r\nif (cpu_has_ds) {\r\nunsigned int l1;\r\nrdmsr(MSR_IA32_MISC_ENABLE, l1, l2);\r\nif (!(l1 & (1<<11)))\r\nset_cpu_cap(c, X86_FEATURE_BTS);\r\nif (!(l1 & (1<<12)))\r\nset_cpu_cap(c, X86_FEATURE_PEBS);\r\n}\r\nif (c->x86 == 6 && cpu_has_clflush &&\r\n(c->x86_model == 29 || c->x86_model == 46 || c->x86_model == 47))\r\nset_cpu_bug(c, X86_BUG_CLFLUSH_MONITOR);\r\n#ifdef CONFIG_X86_64\r\nif (c->x86 == 15)\r\nc->x86_cache_alignment = c->x86_clflush_size * 2;\r\nif (c->x86 == 6)\r\nset_cpu_cap(c, X86_FEATURE_REP_GOOD);\r\n#else\r\nif (c->x86 == 6) {\r\nchar *p = NULL;\r\nswitch (c->x86_model) {\r\ncase 5:\r\nif (l2 == 0)\r\np = "Celeron (Covington)";\r\nelse if (l2 == 256)\r\np = "Mobile Pentium II (Dixon)";\r\nbreak;\r\ncase 6:\r\nif (l2 == 128)\r\np = "Celeron (Mendocino)";\r\nelse if (c->x86_mask == 0 || c->x86_mask == 5)\r\np = "Celeron-A";\r\nbreak;\r\ncase 8:\r\nif (l2 == 128)\r\np = "Celeron (Coppermine)";\r\nbreak;\r\n}\r\nif (p)\r\nstrcpy(c->x86_model_id, p);\r\n}\r\nif (c->x86 == 15)\r\nset_cpu_cap(c, X86_FEATURE_P4);\r\nif (c->x86 == 6)\r\nset_cpu_cap(c, X86_FEATURE_P3);\r\n#endif\r\nsrat_detect_node(c);\r\nif (cpu_has(c, X86_FEATURE_VMX))\r\ndetect_vmx_virtcap(c);\r\nif (cpu_has(c, X86_FEATURE_EPB)) {\r\nu64 epb;\r\nrdmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);\r\nif ((epb & 0xF) == ENERGY_PERF_BIAS_PERFORMANCE) {\r\npr_warn_once("ENERGY_PERF_BIAS: Set to 'normal', was 'performance'\n");\r\npr_warn_once("ENERGY_PERF_BIAS: View and update with x86_energy_perf_policy(8)\n");\r\nepb = (epb & ~0xF) | ENERGY_PERF_BIAS_NORMAL;\r\nwrmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);\r\n}\r\n}\r\n}\r\nstatic unsigned int intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)\r\n{\r\nif ((c->x86 == 6) && (c->x86_model == 11) && (size == 0))\r\nsize = 256;\r\nif ((c->x86 == 5) && (c->x86_model == 9))\r\nsize = 16;\r\nreturn size;\r\n}\r\nstatic void intel_tlb_lookup(const unsigned char desc)\r\n{\r\nunsigned char k;\r\nif (desc == 0)\r\nreturn;\r\nfor (k = 0; intel_tlb_table[k].descriptor != desc && \\r\nintel_tlb_table[k].descriptor != 0; k++)\r\n;\r\nif (intel_tlb_table[k].tlb_type == 0)\r\nreturn;\r\nswitch (intel_tlb_table[k].tlb_type) {\r\ncase STLB_4K:\r\nif (tlb_lli_4k[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_4k[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lld_4k[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_4k[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase STLB_4K_2M:\r\nif (tlb_lli_4k[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_4k[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lld_4k[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_4k[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lli_2m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_2m[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lld_2m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_2m[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lli_4m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_4m[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lld_4m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_4m[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase TLB_INST_ALL:\r\nif (tlb_lli_4k[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_4k[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lli_2m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_2m[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lli_4m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_4m[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase TLB_INST_4K:\r\nif (tlb_lli_4k[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_4k[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase TLB_INST_4M:\r\nif (tlb_lli_4m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_4m[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase TLB_INST_2M_4M:\r\nif (tlb_lli_2m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_2m[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lli_4m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lli_4m[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase TLB_DATA_4K:\r\ncase TLB_DATA0_4K:\r\nif (tlb_lld_4k[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_4k[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase TLB_DATA_4M:\r\ncase TLB_DATA0_4M:\r\nif (tlb_lld_4m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_4m[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase TLB_DATA_2M_4M:\r\ncase TLB_DATA0_2M_4M:\r\nif (tlb_lld_2m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_2m[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lld_4m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_4m[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase TLB_DATA_4K_4M:\r\nif (tlb_lld_4k[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_4k[ENTRIES] = intel_tlb_table[k].entries;\r\nif (tlb_lld_4m[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_4m[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\ncase TLB_DATA_1G:\r\nif (tlb_lld_1g[ENTRIES] < intel_tlb_table[k].entries)\r\ntlb_lld_1g[ENTRIES] = intel_tlb_table[k].entries;\r\nbreak;\r\n}\r\n}\r\nstatic void intel_detect_tlb(struct cpuinfo_x86 *c)\r\n{\r\nint i, j, n;\r\nunsigned int regs[4];\r\nunsigned char *desc = (unsigned char *)regs;\r\nif (c->cpuid_level < 2)\r\nreturn;\r\nn = cpuid_eax(2) & 0xFF;\r\nfor (i = 0 ; i < n ; i++) {\r\ncpuid(2, &regs[0], &regs[1], &regs[2], &regs[3]);\r\nfor (j = 0 ; j < 3 ; j++)\r\nif (regs[j] & (1 << 31))\r\nregs[j] = 0;\r\nfor (j = 1 ; j < 16 ; j++)\r\nintel_tlb_lookup(desc[j]);\r\n}\r\n}
