static void octeon_mgmt_set_rx_irq(struct octeon_mgmt *p, int enable)\r\n{\r\nunion cvmx_mixx_intena mix_intena;\r\nunsigned long flags;\r\nspin_lock_irqsave(&p->lock, flags);\r\nmix_intena.u64 = cvmx_read_csr(p->mix + MIX_INTENA);\r\nmix_intena.s.ithena = enable ? 1 : 0;\r\ncvmx_write_csr(p->mix + MIX_INTENA, mix_intena.u64);\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\nstatic void octeon_mgmt_set_tx_irq(struct octeon_mgmt *p, int enable)\r\n{\r\nunion cvmx_mixx_intena mix_intena;\r\nunsigned long flags;\r\nspin_lock_irqsave(&p->lock, flags);\r\nmix_intena.u64 = cvmx_read_csr(p->mix + MIX_INTENA);\r\nmix_intena.s.othena = enable ? 1 : 0;\r\ncvmx_write_csr(p->mix + MIX_INTENA, mix_intena.u64);\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\nstatic void octeon_mgmt_enable_rx_irq(struct octeon_mgmt *p)\r\n{\r\nocteon_mgmt_set_rx_irq(p, 1);\r\n}\r\nstatic void octeon_mgmt_disable_rx_irq(struct octeon_mgmt *p)\r\n{\r\nocteon_mgmt_set_rx_irq(p, 0);\r\n}\r\nstatic void octeon_mgmt_enable_tx_irq(struct octeon_mgmt *p)\r\n{\r\nocteon_mgmt_set_tx_irq(p, 1);\r\n}\r\nstatic void octeon_mgmt_disable_tx_irq(struct octeon_mgmt *p)\r\n{\r\nocteon_mgmt_set_tx_irq(p, 0);\r\n}\r\nstatic unsigned int ring_max_fill(unsigned int ring_size)\r\n{\r\nreturn ring_size - 8;\r\n}\r\nstatic unsigned int ring_size_to_bytes(unsigned int ring_size)\r\n{\r\nreturn ring_size * sizeof(union mgmt_port_ring_entry);\r\n}\r\nstatic void octeon_mgmt_rx_fill_ring(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nwhile (p->rx_current_fill < ring_max_fill(OCTEON_MGMT_RX_RING_SIZE)) {\r\nunsigned int size;\r\nunion mgmt_port_ring_entry re;\r\nstruct sk_buff *skb;\r\nsize = netdev->mtu + OCTEON_MGMT_RX_HEADROOM + 8 + NET_IP_ALIGN;\r\nskb = netdev_alloc_skb(netdev, size);\r\nif (!skb)\r\nbreak;\r\nskb_reserve(skb, NET_IP_ALIGN);\r\n__skb_queue_tail(&p->rx_list, skb);\r\nre.d64 = 0;\r\nre.s.len = size;\r\nre.s.addr = dma_map_single(p->dev, skb->data,\r\nsize,\r\nDMA_FROM_DEVICE);\r\np->rx_ring[p->rx_next_fill] = re.d64;\r\ndma_sync_single_for_device(p->dev, p->rx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\np->rx_next_fill =\r\n(p->rx_next_fill + 1) % OCTEON_MGMT_RX_RING_SIZE;\r\np->rx_current_fill++;\r\ncvmx_write_csr(p->mix + MIX_IRING2, 1);\r\n}\r\n}\r\nstatic void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)\r\n{\r\nunion cvmx_mixx_orcnt mix_orcnt;\r\nunion mgmt_port_ring_entry re;\r\nstruct sk_buff *skb;\r\nint cleaned = 0;\r\nunsigned long flags;\r\nmix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);\r\nwhile (mix_orcnt.s.orcnt) {\r\nspin_lock_irqsave(&p->tx_list.lock, flags);\r\nmix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);\r\nif (mix_orcnt.s.orcnt == 0) {\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\nbreak;\r\n}\r\ndma_sync_single_for_cpu(p->dev, p->tx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nre.d64 = p->tx_ring[p->tx_next_clean];\r\np->tx_next_clean =\r\n(p->tx_next_clean + 1) % OCTEON_MGMT_TX_RING_SIZE;\r\nskb = __skb_dequeue(&p->tx_list);\r\nmix_orcnt.u64 = 0;\r\nmix_orcnt.s.orcnt = 1;\r\ncvmx_write_csr(p->mix + MIX_ORCNT, mix_orcnt.u64);\r\np->tx_current_fill--;\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\ndma_unmap_single(p->dev, re.s.addr, re.s.len,\r\nDMA_TO_DEVICE);\r\nif (unlikely(re.s.tstamp)) {\r\nstruct skb_shared_hwtstamps ts;\r\nu64 ns;\r\nmemset(&ts, 0, sizeof(ts));\r\nns = cvmx_read_csr(CVMX_MIXX_TSTAMP(p->port));\r\ncvmx_write_csr(CVMX_MIXX_TSCTL(p->port), 0);\r\nts.hwtstamp = ns_to_ktime(ns);\r\nskb_tstamp_tx(skb, &ts);\r\n}\r\ndev_kfree_skb_any(skb);\r\ncleaned++;\r\nmix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);\r\n}\r\nif (cleaned && netif_queue_stopped(p->netdev))\r\nnetif_wake_queue(p->netdev);\r\n}\r\nstatic void octeon_mgmt_clean_tx_tasklet(unsigned long arg)\r\n{\r\nstruct octeon_mgmt *p = (struct octeon_mgmt *)arg;\r\nocteon_mgmt_clean_tx_buffers(p);\r\nocteon_mgmt_enable_tx_irq(p);\r\n}\r\nstatic void octeon_mgmt_update_rx_stats(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nunsigned long flags;\r\nu64 drop, bad;\r\ndrop = cvmx_read_csr(p->agl + AGL_GMX_RX_STATS_PKTS_DRP);\r\nbad = cvmx_read_csr(p->agl + AGL_GMX_RX_STATS_PKTS_BAD);\r\nif (drop || bad) {\r\nspin_lock_irqsave(&p->lock, flags);\r\nnetdev->stats.rx_errors += bad;\r\nnetdev->stats.rx_dropped += drop;\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\n}\r\nstatic void octeon_mgmt_update_tx_stats(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nunsigned long flags;\r\nunion cvmx_agl_gmx_txx_stat0 s0;\r\nunion cvmx_agl_gmx_txx_stat1 s1;\r\ns0.u64 = cvmx_read_csr(p->agl + AGL_GMX_TX_STAT0);\r\ns1.u64 = cvmx_read_csr(p->agl + AGL_GMX_TX_STAT1);\r\nif (s0.s.xsdef || s0.s.xscol || s1.s.scol || s1.s.mcol) {\r\nspin_lock_irqsave(&p->lock, flags);\r\nnetdev->stats.tx_errors += s0.s.xsdef + s0.s.xscol;\r\nnetdev->stats.collisions += s1.s.scol + s1.s.mcol;\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\n}\r\nstatic u64 octeon_mgmt_dequeue_rx_buffer(struct octeon_mgmt *p,\r\nstruct sk_buff **pskb)\r\n{\r\nunion mgmt_port_ring_entry re;\r\ndma_sync_single_for_cpu(p->dev, p->rx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nre.d64 = p->rx_ring[p->rx_next];\r\np->rx_next = (p->rx_next + 1) % OCTEON_MGMT_RX_RING_SIZE;\r\np->rx_current_fill--;\r\n*pskb = __skb_dequeue(&p->rx_list);\r\ndma_unmap_single(p->dev, re.s.addr,\r\nETH_FRAME_LEN + OCTEON_MGMT_RX_HEADROOM,\r\nDMA_FROM_DEVICE);\r\nreturn re.d64;\r\n}\r\nstatic int octeon_mgmt_receive_one(struct octeon_mgmt *p)\r\n{\r\nstruct net_device *netdev = p->netdev;\r\nunion cvmx_mixx_ircnt mix_ircnt;\r\nunion mgmt_port_ring_entry re;\r\nstruct sk_buff *skb;\r\nstruct sk_buff *skb2;\r\nstruct sk_buff *skb_new;\r\nunion mgmt_port_ring_entry re2;\r\nint rc = 1;\r\nre.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb);\r\nif (likely(re.s.code == RING_ENTRY_CODE_DONE)) {\r\nskb_put(skb, re.s.len);\r\ngood:\r\nif (p->has_rx_tstamp) {\r\nu64 ns = *(u64 *)skb->data;\r\nstruct skb_shared_hwtstamps *ts;\r\nts = skb_hwtstamps(skb);\r\nts->hwtstamp = ns_to_ktime(ns);\r\n__skb_pull(skb, 8);\r\n}\r\nskb->protocol = eth_type_trans(skb, netdev);\r\nnetdev->stats.rx_packets++;\r\nnetdev->stats.rx_bytes += skb->len;\r\nnetif_receive_skb(skb);\r\nrc = 0;\r\n} else if (re.s.code == RING_ENTRY_CODE_MORE) {\r\nskb_put(skb, re.s.len);\r\ndo {\r\nre2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);\r\nif (re2.s.code != RING_ENTRY_CODE_MORE\r\n&& re2.s.code != RING_ENTRY_CODE_DONE)\r\ngoto split_error;\r\nskb_put(skb2, re2.s.len);\r\nskb_new = skb_copy_expand(skb, 0, skb2->len,\r\nGFP_ATOMIC);\r\nif (!skb_new)\r\ngoto split_error;\r\nif (skb_copy_bits(skb2, 0, skb_tail_pointer(skb_new),\r\nskb2->len))\r\ngoto split_error;\r\nskb_put(skb_new, skb2->len);\r\ndev_kfree_skb_any(skb);\r\ndev_kfree_skb_any(skb2);\r\nskb = skb_new;\r\n} while (re2.s.code == RING_ENTRY_CODE_MORE);\r\ngoto good;\r\n} else {\r\ndev_kfree_skb_any(skb);\r\n}\r\ngoto done;\r\nsplit_error:\r\ndev_kfree_skb_any(skb);\r\ndev_kfree_skb_any(skb2);\r\nwhile (re2.s.code == RING_ENTRY_CODE_MORE) {\r\nre2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);\r\ndev_kfree_skb_any(skb2);\r\n}\r\nnetdev->stats.rx_errors++;\r\ndone:\r\nmix_ircnt.u64 = 0;\r\nmix_ircnt.s.ircnt = 1;\r\ncvmx_write_csr(p->mix + MIX_IRCNT, mix_ircnt.u64);\r\nreturn rc;\r\n}\r\nstatic int octeon_mgmt_receive_packets(struct octeon_mgmt *p, int budget)\r\n{\r\nunsigned int work_done = 0;\r\nunion cvmx_mixx_ircnt mix_ircnt;\r\nint rc;\r\nmix_ircnt.u64 = cvmx_read_csr(p->mix + MIX_IRCNT);\r\nwhile (work_done < budget && mix_ircnt.s.ircnt) {\r\nrc = octeon_mgmt_receive_one(p);\r\nif (!rc)\r\nwork_done++;\r\nmix_ircnt.u64 = cvmx_read_csr(p->mix + MIX_IRCNT);\r\n}\r\nocteon_mgmt_rx_fill_ring(p->netdev);\r\nreturn work_done;\r\n}\r\nstatic int octeon_mgmt_napi_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct octeon_mgmt *p = container_of(napi, struct octeon_mgmt, napi);\r\nstruct net_device *netdev = p->netdev;\r\nunsigned int work_done = 0;\r\nwork_done = octeon_mgmt_receive_packets(p, budget);\r\nif (work_done < budget) {\r\nnapi_complete(napi);\r\nocteon_mgmt_enable_rx_irq(p);\r\n}\r\nocteon_mgmt_update_rx_stats(netdev);\r\nreturn work_done;\r\n}\r\nstatic void octeon_mgmt_reset_hw(struct octeon_mgmt *p)\r\n{\r\nunion cvmx_mixx_ctl mix_ctl;\r\nunion cvmx_mixx_bist mix_bist;\r\nunion cvmx_agl_gmx_bist agl_gmx_bist;\r\nmix_ctl.u64 = 0;\r\ncvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);\r\ndo {\r\nmix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);\r\n} while (mix_ctl.s.busy);\r\nmix_ctl.s.reset = 1;\r\ncvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);\r\ncvmx_read_csr(p->mix + MIX_CTL);\r\nocteon_io_clk_delay(64);\r\nmix_bist.u64 = cvmx_read_csr(p->mix + MIX_BIST);\r\nif (mix_bist.u64)\r\ndev_warn(p->dev, "MIX failed BIST (0x%016llx)\n",\r\n(unsigned long long)mix_bist.u64);\r\nagl_gmx_bist.u64 = cvmx_read_csr(CVMX_AGL_GMX_BIST);\r\nif (agl_gmx_bist.u64)\r\ndev_warn(p->dev, "AGL failed BIST (0x%016llx)\n",\r\n(unsigned long long)agl_gmx_bist.u64);\r\n}\r\nstatic void octeon_mgmt_cam_state_add(struct octeon_mgmt_cam_state *cs,\r\nunsigned char *addr)\r\n{\r\nint i;\r\nfor (i = 0; i < 6; i++)\r\ncs->cam[i] |= (u64)addr[i] << (8 * (cs->cam_index));\r\ncs->cam_mask |= (1ULL << cs->cam_index);\r\ncs->cam_index++;\r\n}\r\nstatic void octeon_mgmt_set_rx_filtering(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nunion cvmx_agl_gmx_rxx_adr_ctl adr_ctl;\r\nunion cvmx_agl_gmx_prtx_cfg agl_gmx_prtx;\r\nunsigned long flags;\r\nunsigned int prev_packet_enable;\r\nunsigned int cam_mode = 1;\r\nunsigned int multicast_mode = 1;\r\nstruct octeon_mgmt_cam_state cam_state;\r\nstruct netdev_hw_addr *ha;\r\nint available_cam_entries;\r\nmemset(&cam_state, 0, sizeof(cam_state));\r\nif ((netdev->flags & IFF_PROMISC) || netdev->uc.count > 7) {\r\ncam_mode = 0;\r\navailable_cam_entries = 8;\r\n} else {\r\navailable_cam_entries = 7 - netdev->uc.count;\r\n}\r\nif (netdev->flags & IFF_MULTICAST) {\r\nif (cam_mode == 0 || (netdev->flags & IFF_ALLMULTI) ||\r\nnetdev_mc_count(netdev) > available_cam_entries)\r\nmulticast_mode = 2;\r\nelse\r\nmulticast_mode = 0;\r\n}\r\nif (cam_mode == 1) {\r\nocteon_mgmt_cam_state_add(&cam_state, netdev->dev_addr);\r\nnetdev_for_each_uc_addr(ha, netdev)\r\nocteon_mgmt_cam_state_add(&cam_state, ha->addr);\r\n}\r\nif (multicast_mode == 0) {\r\nnetdev_for_each_mc_addr(ha, netdev)\r\nocteon_mgmt_cam_state_add(&cam_state, ha->addr);\r\n}\r\nspin_lock_irqsave(&p->lock, flags);\r\nagl_gmx_prtx.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\r\nprev_packet_enable = agl_gmx_prtx.s.en;\r\nagl_gmx_prtx.s.en = 0;\r\ncvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, agl_gmx_prtx.u64);\r\nadr_ctl.u64 = 0;\r\nadr_ctl.s.cam_mode = cam_mode;\r\nadr_ctl.s.mcst = multicast_mode;\r\nadr_ctl.s.bcst = 1;\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CTL, adr_ctl.u64);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM0, cam_state.cam[0]);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM1, cam_state.cam[1]);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM2, cam_state.cam[2]);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM3, cam_state.cam[3]);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM4, cam_state.cam[4]);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM5, cam_state.cam[5]);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM_EN, cam_state.cam_mask);\r\nagl_gmx_prtx.s.en = prev_packet_enable;\r\ncvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, agl_gmx_prtx.u64);\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\nstatic int octeon_mgmt_set_mac_address(struct net_device *netdev, void *addr)\r\n{\r\nint r = eth_mac_addr(netdev, addr);\r\nif (r)\r\nreturn r;\r\nocteon_mgmt_set_rx_filtering(netdev);\r\nreturn 0;\r\n}\r\nstatic int octeon_mgmt_change_mtu(struct net_device *netdev, int new_mtu)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint size_without_fcs = new_mtu + OCTEON_MGMT_RX_HEADROOM;\r\nif (size_without_fcs < 64 || size_without_fcs > 16383) {\r\ndev_warn(p->dev, "MTU must be between %d and %d.\n",\r\n64 - OCTEON_MGMT_RX_HEADROOM,\r\n16383 - OCTEON_MGMT_RX_HEADROOM);\r\nreturn -EINVAL;\r\n}\r\nnetdev->mtu = new_mtu;\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_FRM_MAX, size_without_fcs);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_JABBER,\r\n(size_without_fcs + 7) & 0xfff8);\r\nreturn 0;\r\n}\r\nstatic irqreturn_t octeon_mgmt_interrupt(int cpl, void *dev_id)\r\n{\r\nstruct net_device *netdev = dev_id;\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nunion cvmx_mixx_isr mixx_isr;\r\nmixx_isr.u64 = cvmx_read_csr(p->mix + MIX_ISR);\r\ncvmx_write_csr(p->mix + MIX_ISR, mixx_isr.u64);\r\ncvmx_read_csr(p->mix + MIX_ISR);\r\nif (mixx_isr.s.irthresh) {\r\nocteon_mgmt_disable_rx_irq(p);\r\nnapi_schedule(&p->napi);\r\n}\r\nif (mixx_isr.s.orthresh) {\r\nocteon_mgmt_disable_tx_irq(p);\r\ntasklet_schedule(&p->tx_clean_tasklet);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int octeon_mgmt_ioctl_hwtstamp(struct net_device *netdev,\r\nstruct ifreq *rq, int cmd)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nstruct hwtstamp_config config;\r\nunion cvmx_mio_ptp_clock_cfg ptp;\r\nunion cvmx_agl_gmx_rxx_frm_ctl rxx_frm_ctl;\r\nbool have_hw_timestamps = false;\r\nif (copy_from_user(&config, rq->ifr_data, sizeof(config)))\r\nreturn -EFAULT;\r\nif (config.flags)\r\nreturn -EINVAL;\r\nif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\r\nptp.u64 = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_CFG);\r\nif (!ptp.s.ext_clk_en) {\r\nu64 clock_comp = (NSEC_PER_SEC << 32) / octeon_get_io_clock_rate();\r\nif (!ptp.s.ptp_en)\r\ncvmx_write_csr(CVMX_MIO_PTP_CLOCK_COMP, clock_comp);\r\npr_info("PTP Clock: Using sclk reference at %lld Hz\n",\r\n(NSEC_PER_SEC << 32) / clock_comp);\r\n} else {\r\nu64 clock_comp = cvmx_read_csr(CVMX_MIO_PTP_CLOCK_COMP);\r\npr_info("PTP Clock: Using GPIO %d at %lld Hz\n",\r\nptp.s.ext_clk_in,\r\n(NSEC_PER_SEC << 32) / clock_comp);\r\n}\r\nif (!ptp.s.ptp_en) {\r\nptp.s.ptp_en = 1;\r\ncvmx_write_csr(CVMX_MIO_PTP_CLOCK_CFG, ptp.u64);\r\n}\r\nhave_hw_timestamps = true;\r\n}\r\nif (!have_hw_timestamps)\r\nreturn -EINVAL;\r\nswitch (config.tx_type) {\r\ncase HWTSTAMP_TX_OFF:\r\ncase HWTSTAMP_TX_ON:\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nswitch (config.rx_filter) {\r\ncase HWTSTAMP_FILTER_NONE:\r\np->has_rx_tstamp = false;\r\nrxx_frm_ctl.u64 = cvmx_read_csr(p->agl + AGL_GMX_RX_FRM_CTL);\r\nrxx_frm_ctl.s.ptp_mode = 0;\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_FRM_CTL, rxx_frm_ctl.u64);\r\nbreak;\r\ncase HWTSTAMP_FILTER_ALL:\r\ncase HWTSTAMP_FILTER_SOME:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\r\np->has_rx_tstamp = have_hw_timestamps;\r\nconfig.rx_filter = HWTSTAMP_FILTER_ALL;\r\nif (p->has_rx_tstamp) {\r\nrxx_frm_ctl.u64 = cvmx_read_csr(p->agl + AGL_GMX_RX_FRM_CTL);\r\nrxx_frm_ctl.s.ptp_mode = 1;\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_FRM_CTL, rxx_frm_ctl.u64);\r\n}\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nif (copy_to_user(rq->ifr_data, &config, sizeof(config)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int octeon_mgmt_ioctl(struct net_device *netdev,\r\nstruct ifreq *rq, int cmd)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nswitch (cmd) {\r\ncase SIOCSHWTSTAMP:\r\nreturn octeon_mgmt_ioctl_hwtstamp(netdev, rq, cmd);\r\ndefault:\r\nif (p->phydev)\r\nreturn phy_mii_ioctl(p->phydev, rq, cmd);\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic void octeon_mgmt_disable_link(struct octeon_mgmt *p)\r\n{\r\nunion cvmx_agl_gmx_prtx_cfg prtx_cfg;\r\nprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\r\nprtx_cfg.s.en = 0;\r\nprtx_cfg.s.tx_en = 0;\r\nprtx_cfg.s.rx_en = 0;\r\ncvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, prtx_cfg.u64);\r\nif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\r\nint i;\r\nfor (i = 0; i < 10; i++) {\r\nprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\r\nif (prtx_cfg.s.tx_idle == 1 || prtx_cfg.s.rx_idle == 1)\r\nbreak;\r\nmdelay(1);\r\ni++;\r\n}\r\n}\r\n}\r\nstatic void octeon_mgmt_enable_link(struct octeon_mgmt *p)\r\n{\r\nunion cvmx_agl_gmx_prtx_cfg prtx_cfg;\r\nprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\r\nprtx_cfg.s.tx_en = 1;\r\nprtx_cfg.s.rx_en = 1;\r\nprtx_cfg.s.en = 1;\r\ncvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, prtx_cfg.u64);\r\n}\r\nstatic void octeon_mgmt_update_link(struct octeon_mgmt *p)\r\n{\r\nunion cvmx_agl_gmx_prtx_cfg prtx_cfg;\r\nprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\r\nif (!p->phydev->link)\r\nprtx_cfg.s.duplex = 1;\r\nelse\r\nprtx_cfg.s.duplex = p->phydev->duplex;\r\nswitch (p->phydev->speed) {\r\ncase 10:\r\nprtx_cfg.s.speed = 0;\r\nprtx_cfg.s.slottime = 0;\r\nif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\r\nprtx_cfg.s.burst = 1;\r\nprtx_cfg.s.speed_msb = 1;\r\n}\r\nbreak;\r\ncase 100:\r\nprtx_cfg.s.speed = 0;\r\nprtx_cfg.s.slottime = 0;\r\nif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\r\nprtx_cfg.s.burst = 1;\r\nprtx_cfg.s.speed_msb = 0;\r\n}\r\nbreak;\r\ncase 1000:\r\nif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\r\nprtx_cfg.s.speed = 1;\r\nprtx_cfg.s.speed_msb = 0;\r\nprtx_cfg.s.slottime = 1;\r\nprtx_cfg.s.burst = p->phydev->duplex;\r\n}\r\nbreak;\r\ncase 0:\r\ndefault:\r\nbreak;\r\n}\r\ncvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, prtx_cfg.u64);\r\nprtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);\r\nif (OCTEON_IS_MODEL(OCTEON_CN6XXX)) {\r\nunion cvmx_agl_gmx_txx_clk agl_clk;\r\nunion cvmx_agl_prtx_ctl prtx_ctl;\r\nprtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\r\nagl_clk.u64 = cvmx_read_csr(p->agl + AGL_GMX_TX_CLK);\r\nagl_clk.s.clk_cnt = 1;\r\nif (prtx_ctl.s.mode == 0) {\r\nif (p->phydev->speed == 10)\r\nagl_clk.s.clk_cnt = 50;\r\nelse if (p->phydev->speed == 100)\r\nagl_clk.s.clk_cnt = 5;\r\n}\r\ncvmx_write_csr(p->agl + AGL_GMX_TX_CLK, agl_clk.u64);\r\n}\r\n}\r\nstatic void octeon_mgmt_adjust_link(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nunsigned long flags;\r\nint link_changed = 0;\r\nif (!p->phydev)\r\nreturn;\r\nspin_lock_irqsave(&p->lock, flags);\r\nif (!p->phydev->link && p->last_link)\r\nlink_changed = -1;\r\nif (p->phydev->link\r\n&& (p->last_duplex != p->phydev->duplex\r\n|| p->last_link != p->phydev->link\r\n|| p->last_speed != p->phydev->speed)) {\r\nocteon_mgmt_disable_link(p);\r\nlink_changed = 1;\r\nocteon_mgmt_update_link(p);\r\nocteon_mgmt_enable_link(p);\r\n}\r\np->last_link = p->phydev->link;\r\np->last_speed = p->phydev->speed;\r\np->last_duplex = p->phydev->duplex;\r\nspin_unlock_irqrestore(&p->lock, flags);\r\nif (link_changed != 0) {\r\nif (link_changed > 0) {\r\npr_info("%s: Link is up - %d/%s\n", netdev->name,\r\np->phydev->speed,\r\nDUPLEX_FULL == p->phydev->duplex ?\r\n"Full" : "Half");\r\n} else {\r\npr_info("%s: Link is down\n", netdev->name);\r\n}\r\n}\r\n}\r\nstatic int octeon_mgmt_init_phy(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nif (octeon_is_simulation() || p->phy_np == NULL) {\r\nnetif_carrier_on(netdev);\r\nreturn 0;\r\n}\r\np->phydev = of_phy_connect(netdev, p->phy_np,\r\nocteon_mgmt_adjust_link, 0,\r\nPHY_INTERFACE_MODE_MII);\r\nif (!p->phydev)\r\nreturn -ENODEV;\r\nreturn 0;\r\n}\r\nstatic int octeon_mgmt_open(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nunion cvmx_mixx_ctl mix_ctl;\r\nunion cvmx_agl_gmx_inf_mode agl_gmx_inf_mode;\r\nunion cvmx_mixx_oring1 oring1;\r\nunion cvmx_mixx_iring1 iring1;\r\nunion cvmx_agl_gmx_rxx_frm_ctl rxx_frm_ctl;\r\nunion cvmx_mixx_irhwm mix_irhwm;\r\nunion cvmx_mixx_orhwm mix_orhwm;\r\nunion cvmx_mixx_intena mix_intena;\r\nstruct sockaddr sa;\r\np->tx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nGFP_KERNEL);\r\nif (!p->tx_ring)\r\nreturn -ENOMEM;\r\np->tx_ring_handle =\r\ndma_map_single(p->dev, p->tx_ring,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\np->tx_next = 0;\r\np->tx_next_clean = 0;\r\np->tx_current_fill = 0;\r\np->rx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nGFP_KERNEL);\r\nif (!p->rx_ring)\r\ngoto err_nomem;\r\np->rx_ring_handle =\r\ndma_map_single(p->dev, p->rx_ring,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\np->rx_next = 0;\r\np->rx_next_fill = 0;\r\np->rx_current_fill = 0;\r\nocteon_mgmt_reset_hw(p);\r\nmix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);\r\nif (mix_ctl.s.reset) {\r\nmix_ctl.s.reset = 0;\r\ncvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);\r\ndo {\r\nmix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);\r\n} while (mix_ctl.s.reset);\r\n}\r\nif (OCTEON_IS_MODEL(OCTEON_CN5XXX)) {\r\nagl_gmx_inf_mode.u64 = 0;\r\nagl_gmx_inf_mode.s.en = 1;\r\ncvmx_write_csr(CVMX_AGL_GMX_INF_MODE, agl_gmx_inf_mode.u64);\r\n}\r\nif (OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X)\r\n|| OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X)) {\r\nunion cvmx_agl_gmx_drv_ctl drv_ctl;\r\ndrv_ctl.u64 = cvmx_read_csr(CVMX_AGL_GMX_DRV_CTL);\r\nif (p->port) {\r\ndrv_ctl.s.byp_en1 = 1;\r\ndrv_ctl.s.nctl1 = 6;\r\ndrv_ctl.s.pctl1 = 6;\r\n} else {\r\ndrv_ctl.s.byp_en = 1;\r\ndrv_ctl.s.nctl = 6;\r\ndrv_ctl.s.pctl = 6;\r\n}\r\ncvmx_write_csr(CVMX_AGL_GMX_DRV_CTL, drv_ctl.u64);\r\n}\r\noring1.u64 = 0;\r\noring1.s.obase = p->tx_ring_handle >> 3;\r\noring1.s.osize = OCTEON_MGMT_TX_RING_SIZE;\r\ncvmx_write_csr(p->mix + MIX_ORING1, oring1.u64);\r\niring1.u64 = 0;\r\niring1.s.ibase = p->rx_ring_handle >> 3;\r\niring1.s.isize = OCTEON_MGMT_RX_RING_SIZE;\r\ncvmx_write_csr(p->mix + MIX_IRING1, iring1.u64);\r\nmemcpy(sa.sa_data, netdev->dev_addr, ETH_ALEN);\r\nocteon_mgmt_set_mac_address(netdev, &sa);\r\nocteon_mgmt_change_mtu(netdev, netdev->mtu);\r\nmix_ctl.u64 = 0;\r\nmix_ctl.s.crc_strip = 1;\r\nmix_ctl.s.en = 1;\r\nmix_ctl.s.nbtarb = 0;\r\nmix_ctl.s.mrq_hwm = 1;\r\n#ifdef __LITTLE_ENDIAN\r\nmix_ctl.s.lendian = 1;\r\n#endif\r\ncvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);\r\nif (octeon_mgmt_init_phy(netdev)) {\r\ndev_err(p->dev, "Cannot initialize PHY on MIX%d.\n", p->port);\r\ngoto err_noirq;\r\n}\r\nif (OCTEON_IS_MODEL(OCTEON_CN6XXX) && p->phydev) {\r\nunion cvmx_agl_prtx_ctl agl_prtx_ctl;\r\nint rgmii_mode = (p->phydev->supported &\r\n(SUPPORTED_1000baseT_Half | SUPPORTED_1000baseT_Full)) != 0;\r\nagl_prtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\r\nagl_prtx_ctl.s.mode = rgmii_mode ? 0 : 1;\r\ncvmx_write_csr(p->agl_prt_ctl, agl_prtx_ctl.u64);\r\n#define NS_PER_PHY_CLK 8\r\nagl_prtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\r\nagl_prtx_ctl.s.clkrst = 0;\r\nif (rgmii_mode) {\r\nagl_prtx_ctl.s.dllrst = 0;\r\nagl_prtx_ctl.s.clktx_byp = 0;\r\n}\r\ncvmx_write_csr(p->agl_prt_ctl, agl_prtx_ctl.u64);\r\ncvmx_read_csr(p->agl_prt_ctl);\r\nndelay(256 * NS_PER_PHY_CLK);\r\nagl_prtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\r\nagl_prtx_ctl.s.enable = 1;\r\ncvmx_write_csr(p->agl_prt_ctl, agl_prtx_ctl.u64);\r\nagl_prtx_ctl.u64 = cvmx_read_csr(p->agl_prt_ctl);\r\nagl_prtx_ctl.s.comp = 1;\r\nagl_prtx_ctl.s.drv_byp = 0;\r\ncvmx_write_csr(p->agl_prt_ctl, agl_prtx_ctl.u64);\r\ncvmx_read_csr(p->agl_prt_ctl);\r\nndelay(1040 * NS_PER_PHY_CLK);\r\ncvmx_write_csr(CVMX_AGL_GMX_TX_IFG, 0xae);\r\n}\r\nocteon_mgmt_rx_fill_ring(netdev);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_STATS_CTL, 1);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_STATS_PKTS_DRP, 0);\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_STATS_PKTS_BAD, 0);\r\ncvmx_write_csr(p->agl + AGL_GMX_TX_STATS_CTL, 1);\r\ncvmx_write_csr(p->agl + AGL_GMX_TX_STAT0, 0);\r\ncvmx_write_csr(p->agl + AGL_GMX_TX_STAT1, 0);\r\ncvmx_write_csr(p->mix + MIX_ISR, cvmx_read_csr(p->mix + MIX_ISR));\r\nif (request_irq(p->irq, octeon_mgmt_interrupt, 0, netdev->name,\r\nnetdev)) {\r\ndev_err(p->dev, "request_irq(%d) failed.\n", p->irq);\r\ngoto err_noirq;\r\n}\r\nmix_irhwm.u64 = 0;\r\nmix_irhwm.s.irhwm = 0;\r\ncvmx_write_csr(p->mix + MIX_IRHWM, mix_irhwm.u64);\r\nmix_orhwm.u64 = 0;\r\nmix_orhwm.s.orhwm = 0;\r\ncvmx_write_csr(p->mix + MIX_ORHWM, mix_orhwm.u64);\r\nmix_intena.u64 = 0;\r\nmix_intena.s.ithena = 1;\r\nmix_intena.s.othena = 1;\r\ncvmx_write_csr(p->mix + MIX_INTENA, mix_intena.u64);\r\nrxx_frm_ctl.u64 = 0;\r\nrxx_frm_ctl.s.ptp_mode = p->has_rx_tstamp ? 1 : 0;\r\nrxx_frm_ctl.s.pre_align = 1;\r\nrxx_frm_ctl.s.pad_len = 1;\r\nrxx_frm_ctl.s.vlan_len = 1;\r\nrxx_frm_ctl.s.pre_free = 1;\r\nrxx_frm_ctl.s.ctl_smac = 0;\r\nrxx_frm_ctl.s.ctl_mcst = 1;\r\nrxx_frm_ctl.s.ctl_bck = 1;\r\nrxx_frm_ctl.s.ctl_drp = 1;\r\nrxx_frm_ctl.s.pre_strp = 1;\r\nrxx_frm_ctl.s.pre_chk = 1;\r\ncvmx_write_csr(p->agl + AGL_GMX_RX_FRM_CTL, rxx_frm_ctl.u64);\r\nocteon_mgmt_disable_link(p);\r\nif (p->phydev)\r\nocteon_mgmt_update_link(p);\r\nocteon_mgmt_enable_link(p);\r\np->last_link = 0;\r\np->last_speed = 0;\r\nif (p->phydev) {\r\nnetif_carrier_off(netdev);\r\nphy_start_aneg(p->phydev);\r\n}\r\nnetif_wake_queue(netdev);\r\nnapi_enable(&p->napi);\r\nreturn 0;\r\nerr_noirq:\r\nocteon_mgmt_reset_hw(p);\r\ndma_unmap_single(p->dev, p->rx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nkfree(p->rx_ring);\r\nerr_nomem:\r\ndma_unmap_single(p->dev, p->tx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nkfree(p->tx_ring);\r\nreturn -ENOMEM;\r\n}\r\nstatic int octeon_mgmt_stop(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nnapi_disable(&p->napi);\r\nnetif_stop_queue(netdev);\r\nif (p->phydev)\r\nphy_disconnect(p->phydev);\r\np->phydev = NULL;\r\nnetif_carrier_off(netdev);\r\nocteon_mgmt_reset_hw(p);\r\nfree_irq(p->irq, netdev);\r\nskb_queue_purge(&p->tx_list);\r\nskb_queue_purge(&p->rx_list);\r\ndma_unmap_single(p->dev, p->rx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nkfree(p->rx_ring);\r\ndma_unmap_single(p->dev, p->tx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nkfree(p->tx_ring);\r\nreturn 0;\r\n}\r\nstatic int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nunion mgmt_port_ring_entry re;\r\nunsigned long flags;\r\nint rv = NETDEV_TX_BUSY;\r\nre.d64 = 0;\r\nre.s.tstamp = ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) != 0);\r\nre.s.len = skb->len;\r\nre.s.addr = dma_map_single(p->dev, skb->data,\r\nskb->len,\r\nDMA_TO_DEVICE);\r\nspin_lock_irqsave(&p->tx_list.lock, flags);\r\nif (unlikely(p->tx_current_fill >= ring_max_fill(OCTEON_MGMT_TX_RING_SIZE) - 1)) {\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\nnetif_stop_queue(netdev);\r\nspin_lock_irqsave(&p->tx_list.lock, flags);\r\n}\r\nif (unlikely(p->tx_current_fill >=\r\nring_max_fill(OCTEON_MGMT_TX_RING_SIZE))) {\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\ndma_unmap_single(p->dev, re.s.addr, re.s.len,\r\nDMA_TO_DEVICE);\r\ngoto out;\r\n}\r\n__skb_queue_tail(&p->tx_list, skb);\r\np->tx_ring[p->tx_next] = re.d64;\r\np->tx_next = (p->tx_next + 1) % OCTEON_MGMT_TX_RING_SIZE;\r\np->tx_current_fill++;\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\ndma_sync_single_for_device(p->dev, p->tx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nnetdev->stats.tx_packets++;\r\nnetdev->stats.tx_bytes += skb->len;\r\ncvmx_write_csr(p->mix + MIX_ORING2, 1);\r\nnetdev->trans_start = jiffies;\r\nrv = NETDEV_TX_OK;\r\nout:\r\nocteon_mgmt_update_tx_stats(netdev);\r\nreturn rv;\r\n}\r\nstatic void octeon_mgmt_poll_controller(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nocteon_mgmt_receive_packets(p, 16);\r\nocteon_mgmt_update_rx_stats(netdev);\r\n}\r\nstatic void octeon_mgmt_get_drvinfo(struct net_device *netdev,\r\nstruct ethtool_drvinfo *info)\r\n{\r\nstrlcpy(info->driver, DRV_NAME, sizeof(info->driver));\r\nstrlcpy(info->version, DRV_VERSION, sizeof(info->version));\r\nstrlcpy(info->fw_version, "N/A", sizeof(info->fw_version));\r\nstrlcpy(info->bus_info, "N/A", sizeof(info->bus_info));\r\ninfo->n_stats = 0;\r\ninfo->testinfo_len = 0;\r\ninfo->regdump_len = 0;\r\ninfo->eedump_len = 0;\r\n}\r\nstatic int octeon_mgmt_get_settings(struct net_device *netdev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nif (p->phydev)\r\nreturn phy_ethtool_gset(p->phydev, cmd);\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic int octeon_mgmt_set_settings(struct net_device *netdev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nif (!capable(CAP_NET_ADMIN))\r\nreturn -EPERM;\r\nif (p->phydev)\r\nreturn phy_ethtool_sset(p->phydev, cmd);\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic int octeon_mgmt_nway_reset(struct net_device *dev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(dev);\r\nif (!capable(CAP_NET_ADMIN))\r\nreturn -EPERM;\r\nif (p->phydev)\r\nreturn phy_start_aneg(p->phydev);\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic int octeon_mgmt_probe(struct platform_device *pdev)\r\n{\r\nstruct net_device *netdev;\r\nstruct octeon_mgmt *p;\r\nconst __be32 *data;\r\nconst u8 *mac;\r\nstruct resource *res_mix;\r\nstruct resource *res_agl;\r\nstruct resource *res_agl_prt_ctl;\r\nint len;\r\nint result;\r\nnetdev = alloc_etherdev(sizeof(struct octeon_mgmt));\r\nif (netdev == NULL)\r\nreturn -ENOMEM;\r\nSET_NETDEV_DEV(netdev, &pdev->dev);\r\nplatform_set_drvdata(pdev, netdev);\r\np = netdev_priv(netdev);\r\nnetif_napi_add(netdev, &p->napi, octeon_mgmt_napi_poll,\r\nOCTEON_MGMT_NAPI_WEIGHT);\r\np->netdev = netdev;\r\np->dev = &pdev->dev;\r\np->has_rx_tstamp = false;\r\ndata = of_get_property(pdev->dev.of_node, "cell-index", &len);\r\nif (data && len == sizeof(*data)) {\r\np->port = be32_to_cpup(data);\r\n} else {\r\ndev_err(&pdev->dev, "no 'cell-index' property\n");\r\nresult = -ENXIO;\r\ngoto err;\r\n}\r\nsnprintf(netdev->name, IFNAMSIZ, "mgmt%d", p->port);\r\nresult = platform_get_irq(pdev, 0);\r\nif (result < 0)\r\ngoto err;\r\np->irq = result;\r\nres_mix = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (res_mix == NULL) {\r\ndev_err(&pdev->dev, "no 'reg' resource\n");\r\nresult = -ENXIO;\r\ngoto err;\r\n}\r\nres_agl = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nif (res_agl == NULL) {\r\ndev_err(&pdev->dev, "no 'reg' resource\n");\r\nresult = -ENXIO;\r\ngoto err;\r\n}\r\nres_agl_prt_ctl = platform_get_resource(pdev, IORESOURCE_MEM, 3);\r\nif (res_agl_prt_ctl == NULL) {\r\ndev_err(&pdev->dev, "no 'reg' resource\n");\r\nresult = -ENXIO;\r\ngoto err;\r\n}\r\np->mix_phys = res_mix->start;\r\np->mix_size = resource_size(res_mix);\r\np->agl_phys = res_agl->start;\r\np->agl_size = resource_size(res_agl);\r\np->agl_prt_ctl_phys = res_agl_prt_ctl->start;\r\np->agl_prt_ctl_size = resource_size(res_agl_prt_ctl);\r\nif (!devm_request_mem_region(&pdev->dev, p->mix_phys, p->mix_size,\r\nres_mix->name)) {\r\ndev_err(&pdev->dev, "request_mem_region (%s) failed\n",\r\nres_mix->name);\r\nresult = -ENXIO;\r\ngoto err;\r\n}\r\nif (!devm_request_mem_region(&pdev->dev, p->agl_phys, p->agl_size,\r\nres_agl->name)) {\r\nresult = -ENXIO;\r\ndev_err(&pdev->dev, "request_mem_region (%s) failed\n",\r\nres_agl->name);\r\ngoto err;\r\n}\r\nif (!devm_request_mem_region(&pdev->dev, p->agl_prt_ctl_phys,\r\np->agl_prt_ctl_size, res_agl_prt_ctl->name)) {\r\nresult = -ENXIO;\r\ndev_err(&pdev->dev, "request_mem_region (%s) failed\n",\r\nres_agl_prt_ctl->name);\r\ngoto err;\r\n}\r\np->mix = (u64)devm_ioremap(&pdev->dev, p->mix_phys, p->mix_size);\r\np->agl = (u64)devm_ioremap(&pdev->dev, p->agl_phys, p->agl_size);\r\np->agl_prt_ctl = (u64)devm_ioremap(&pdev->dev, p->agl_prt_ctl_phys,\r\np->agl_prt_ctl_size);\r\nspin_lock_init(&p->lock);\r\nskb_queue_head_init(&p->tx_list);\r\nskb_queue_head_init(&p->rx_list);\r\ntasklet_init(&p->tx_clean_tasklet,\r\nocteon_mgmt_clean_tx_tasklet, (unsigned long)p);\r\nnetdev->priv_flags |= IFF_UNICAST_FLT;\r\nnetdev->netdev_ops = &octeon_mgmt_ops;\r\nnetdev->ethtool_ops = &octeon_mgmt_ethtool_ops;\r\nmac = of_get_mac_address(pdev->dev.of_node);\r\nif (mac)\r\nmemcpy(netdev->dev_addr, mac, ETH_ALEN);\r\nelse\r\neth_hw_addr_random(netdev);\r\np->phy_np = of_parse_phandle(pdev->dev.of_node, "phy-handle", 0);\r\nresult = dma_coerce_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\r\nif (result)\r\ngoto err;\r\nnetif_carrier_off(netdev);\r\nresult = register_netdev(netdev);\r\nif (result)\r\ngoto err;\r\ndev_info(&pdev->dev, "Version " DRV_VERSION "\n");\r\nreturn 0;\r\nerr:\r\nfree_netdev(netdev);\r\nreturn result;\r\n}\r\nstatic int octeon_mgmt_remove(struct platform_device *pdev)\r\n{\r\nstruct net_device *netdev = platform_get_drvdata(pdev);\r\nunregister_netdev(netdev);\r\nfree_netdev(netdev);\r\nreturn 0;\r\n}\r\nstatic int __init octeon_mgmt_mod_init(void)\r\n{\r\nocteon_mdiobus_force_mod_depencency();\r\nreturn platform_driver_register(&octeon_mgmt_driver);\r\n}\r\nstatic void __exit octeon_mgmt_mod_exit(void)\r\n{\r\nplatform_driver_unregister(&octeon_mgmt_driver);\r\n}
