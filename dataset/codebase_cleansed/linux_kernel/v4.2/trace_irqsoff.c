static inline int\r\npreempt_trace(void)\r\n{\r\nreturn ((trace_type & TRACER_PREEMPT_OFF) && preempt_count());\r\n}\r\nstatic inline int\r\nirq_trace(void)\r\n{\r\nreturn ((trace_type & TRACER_IRQS_OFF) &&\r\nirqs_disabled());\r\n}\r\nstatic int func_prolog_dec(struct trace_array *tr,\r\nstruct trace_array_cpu **data,\r\nunsigned long *flags)\r\n{\r\nlong disabled;\r\nint cpu;\r\ncpu = raw_smp_processor_id();\r\nif (likely(!per_cpu(tracing_cpu, cpu)))\r\nreturn 0;\r\nlocal_save_flags(*flags);\r\nif (!irqs_disabled_flags(*flags))\r\nreturn 0;\r\n*data = per_cpu_ptr(tr->trace_buffer.data, cpu);\r\ndisabled = atomic_inc_return(&(*data)->disabled);\r\nif (likely(disabled == 1))\r\nreturn 1;\r\natomic_dec(&(*data)->disabled);\r\nreturn 0;\r\n}\r\nstatic void\r\nirqsoff_tracer_call(unsigned long ip, unsigned long parent_ip,\r\nstruct ftrace_ops *op, struct pt_regs *pt_regs)\r\n{\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nif (!func_prolog_dec(tr, &data, &flags))\r\nreturn;\r\ntrace_function(tr, ip, parent_ip, flags, preempt_count());\r\natomic_dec(&data->disabled);\r\n}\r\nstatic int\r\nirqsoff_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\r\n{\r\nint cpu;\r\nif (!(bit & TRACE_DISPLAY_GRAPH))\r\nreturn -EINVAL;\r\nif (!(is_graph() ^ set))\r\nreturn 0;\r\nstop_irqsoff_tracer(irqsoff_trace, !set);\r\nfor_each_possible_cpu(cpu)\r\nper_cpu(tracing_cpu, cpu) = 0;\r\ntr->max_latency = 0;\r\ntracing_reset_online_cpus(&irqsoff_trace->trace_buffer);\r\nreturn start_irqsoff_tracer(irqsoff_trace, set);\r\n}\r\nstatic int irqsoff_graph_entry(struct ftrace_graph_ent *trace)\r\n{\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint ret;\r\nint pc;\r\nif (!func_prolog_dec(tr, &data, &flags))\r\nreturn 0;\r\npc = preempt_count();\r\nret = __trace_graph_entry(tr, trace, flags, pc);\r\natomic_dec(&data->disabled);\r\nreturn ret;\r\n}\r\nstatic void irqsoff_graph_return(struct ftrace_graph_ret *trace)\r\n{\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint pc;\r\nif (!func_prolog_dec(tr, &data, &flags))\r\nreturn;\r\npc = preempt_count();\r\n__trace_graph_return(tr, trace, flags, pc);\r\natomic_dec(&data->disabled);\r\n}\r\nstatic void irqsoff_trace_open(struct trace_iterator *iter)\r\n{\r\nif (is_graph())\r\ngraph_trace_open(iter);\r\n}\r\nstatic void irqsoff_trace_close(struct trace_iterator *iter)\r\n{\r\nif (iter->private)\r\ngraph_trace_close(iter);\r\n}\r\nstatic enum print_line_t irqsoff_print_line(struct trace_iterator *iter)\r\n{\r\nif (is_graph())\r\nreturn print_graph_function_flags(iter, GRAPH_TRACER_FLAGS);\r\nreturn TRACE_TYPE_UNHANDLED;\r\n}\r\nstatic void irqsoff_print_header(struct seq_file *s)\r\n{\r\nif (is_graph())\r\nprint_graph_headers_flags(s, GRAPH_TRACER_FLAGS);\r\nelse\r\ntrace_default_header(s);\r\n}\r\nstatic void\r\n__trace_function(struct trace_array *tr,\r\nunsigned long ip, unsigned long parent_ip,\r\nunsigned long flags, int pc)\r\n{\r\nif (is_graph())\r\ntrace_graph_function(tr, ip, parent_ip, flags, pc);\r\nelse\r\ntrace_function(tr, ip, parent_ip, flags, pc);\r\n}\r\nstatic int\r\nirqsoff_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic int irqsoff_graph_entry(struct ftrace_graph_ent *trace)\r\n{\r\nreturn -1;\r\n}\r\nstatic enum print_line_t irqsoff_print_line(struct trace_iterator *iter)\r\n{\r\nreturn TRACE_TYPE_UNHANDLED;\r\n}\r\nstatic void irqsoff_graph_return(struct ftrace_graph_ret *trace) { }\r\nstatic void irqsoff_trace_open(struct trace_iterator *iter) { }\r\nstatic void irqsoff_trace_close(struct trace_iterator *iter) { }\r\nstatic void irqsoff_print_header(struct seq_file *s)\r\n{\r\ntrace_default_header(s);\r\n}\r\nstatic void irqsoff_print_header(struct seq_file *s)\r\n{\r\ntrace_latency_header(s);\r\n}\r\nstatic int report_latency(struct trace_array *tr, cycle_t delta)\r\n{\r\nif (tracing_thresh) {\r\nif (delta < tracing_thresh)\r\nreturn 0;\r\n} else {\r\nif (delta <= tr->max_latency)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void\r\ncheck_critical_timing(struct trace_array *tr,\r\nstruct trace_array_cpu *data,\r\nunsigned long parent_ip,\r\nint cpu)\r\n{\r\ncycle_t T0, T1, delta;\r\nunsigned long flags;\r\nint pc;\r\nT0 = data->preempt_timestamp;\r\nT1 = ftrace_now(cpu);\r\ndelta = T1-T0;\r\nlocal_save_flags(flags);\r\npc = preempt_count();\r\nif (!report_latency(tr, delta))\r\ngoto out;\r\nraw_spin_lock_irqsave(&max_trace_lock, flags);\r\nif (!report_latency(tr, delta))\r\ngoto out_unlock;\r\n__trace_function(tr, CALLER_ADDR0, parent_ip, flags, pc);\r\n__trace_stack(tr, flags, 5, pc);\r\nif (data->critical_sequence != max_sequence)\r\ngoto out_unlock;\r\ndata->critical_end = parent_ip;\r\nif (likely(!is_tracing_stopped())) {\r\ntr->max_latency = delta;\r\nupdate_max_tr_single(tr, current, cpu);\r\n}\r\nmax_sequence++;\r\nout_unlock:\r\nraw_spin_unlock_irqrestore(&max_trace_lock, flags);\r\nout:\r\ndata->critical_sequence = max_sequence;\r\ndata->preempt_timestamp = ftrace_now(cpu);\r\n__trace_function(tr, CALLER_ADDR0, parent_ip, flags, pc);\r\n}\r\nstatic inline void\r\nstart_critical_timing(unsigned long ip, unsigned long parent_ip)\r\n{\r\nint cpu;\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nif (!tracer_enabled || !tracing_is_enabled())\r\nreturn;\r\ncpu = raw_smp_processor_id();\r\nif (per_cpu(tracing_cpu, cpu))\r\nreturn;\r\ndata = per_cpu_ptr(tr->trace_buffer.data, cpu);\r\nif (unlikely(!data) || atomic_read(&data->disabled))\r\nreturn;\r\natomic_inc(&data->disabled);\r\ndata->critical_sequence = max_sequence;\r\ndata->preempt_timestamp = ftrace_now(cpu);\r\ndata->critical_start = parent_ip ? : ip;\r\nlocal_save_flags(flags);\r\n__trace_function(tr, ip, parent_ip, flags, preempt_count());\r\nper_cpu(tracing_cpu, cpu) = 1;\r\natomic_dec(&data->disabled);\r\n}\r\nstatic inline void\r\nstop_critical_timing(unsigned long ip, unsigned long parent_ip)\r\n{\r\nint cpu;\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\ncpu = raw_smp_processor_id();\r\nif (unlikely(per_cpu(tracing_cpu, cpu)))\r\nper_cpu(tracing_cpu, cpu) = 0;\r\nelse\r\nreturn;\r\nif (!tracer_enabled || !tracing_is_enabled())\r\nreturn;\r\ndata = per_cpu_ptr(tr->trace_buffer.data, cpu);\r\nif (unlikely(!data) ||\r\n!data->critical_start || atomic_read(&data->disabled))\r\nreturn;\r\natomic_inc(&data->disabled);\r\nlocal_save_flags(flags);\r\n__trace_function(tr, ip, parent_ip, flags, preempt_count());\r\ncheck_critical_timing(tr, data, parent_ip ? : ip, cpu);\r\ndata->critical_start = 0;\r\natomic_dec(&data->disabled);\r\n}\r\nvoid start_critical_timings(void)\r\n{\r\nif (preempt_trace() || irq_trace())\r\nstart_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\r\n}\r\nvoid stop_critical_timings(void)\r\n{\r\nif (preempt_trace() || irq_trace())\r\nstop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\r\n}\r\nvoid time_hardirqs_on(unsigned long a0, unsigned long a1)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstop_critical_timing(a0, a1);\r\n}\r\nvoid time_hardirqs_off(unsigned long a0, unsigned long a1)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstart_critical_timing(a0, a1);\r\n}\r\nvoid trace_softirqs_on(unsigned long ip)\r\n{\r\n}\r\nvoid trace_softirqs_off(unsigned long ip)\r\n{\r\n}\r\ninline void print_irqtrace_events(struct task_struct *curr)\r\n{\r\n}\r\nvoid trace_hardirqs_on(void)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\r\n}\r\nvoid trace_hardirqs_off(void)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstart_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\r\n}\r\n__visible void trace_hardirqs_on_caller(unsigned long caller_addr)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstop_critical_timing(CALLER_ADDR0, caller_addr);\r\n}\r\n__visible void trace_hardirqs_off_caller(unsigned long caller_addr)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstart_critical_timing(CALLER_ADDR0, caller_addr);\r\n}\r\nvoid trace_preempt_on(unsigned long a0, unsigned long a1)\r\n{\r\nif (preempt_trace() && !irq_trace())\r\nstop_critical_timing(a0, a1);\r\n}\r\nvoid trace_preempt_off(unsigned long a0, unsigned long a1)\r\n{\r\nif (preempt_trace() && !irq_trace())\r\nstart_critical_timing(a0, a1);\r\n}\r\nstatic int register_irqsoff_function(struct trace_array *tr, int graph, int set)\r\n{\r\nint ret;\r\nif (function_enabled || (!set && !(trace_flags & TRACE_ITER_FUNCTION)))\r\nreturn 0;\r\nif (graph)\r\nret = register_ftrace_graph(&irqsoff_graph_return,\r\n&irqsoff_graph_entry);\r\nelse\r\nret = register_ftrace_function(tr->ops);\r\nif (!ret)\r\nfunction_enabled = true;\r\nreturn ret;\r\n}\r\nstatic void unregister_irqsoff_function(struct trace_array *tr, int graph)\r\n{\r\nif (!function_enabled)\r\nreturn;\r\nif (graph)\r\nunregister_ftrace_graph();\r\nelse\r\nunregister_ftrace_function(tr->ops);\r\nfunction_enabled = false;\r\n}\r\nstatic void irqsoff_function_set(struct trace_array *tr, int set)\r\n{\r\nif (set)\r\nregister_irqsoff_function(tr, is_graph(), 1);\r\nelse\r\nunregister_irqsoff_function(tr, is_graph());\r\n}\r\nstatic int irqsoff_flag_changed(struct trace_array *tr, u32 mask, int set)\r\n{\r\nstruct tracer *tracer = tr->current_trace;\r\nif (mask & TRACE_ITER_FUNCTION)\r\nirqsoff_function_set(tr, set);\r\nreturn trace_keep_overwrite(tracer, mask, set);\r\n}\r\nstatic int start_irqsoff_tracer(struct trace_array *tr, int graph)\r\n{\r\nint ret;\r\nret = register_irqsoff_function(tr, graph, 0);\r\nif (!ret && tracing_is_enabled())\r\ntracer_enabled = 1;\r\nelse\r\ntracer_enabled = 0;\r\nreturn ret;\r\n}\r\nstatic void stop_irqsoff_tracer(struct trace_array *tr, int graph)\r\n{\r\ntracer_enabled = 0;\r\nunregister_irqsoff_function(tr, graph);\r\n}\r\nstatic int __irqsoff_tracer_init(struct trace_array *tr)\r\n{\r\nif (irqsoff_busy)\r\nreturn -EBUSY;\r\nsave_flags = trace_flags;\r\nset_tracer_flag(tr, TRACE_ITER_OVERWRITE, 1);\r\nset_tracer_flag(tr, TRACE_ITER_LATENCY_FMT, 1);\r\ntr->max_latency = 0;\r\nirqsoff_trace = tr;\r\nsmp_wmb();\r\ntracing_reset_online_cpus(&tr->trace_buffer);\r\nftrace_init_array_ops(tr, irqsoff_tracer_call);\r\nif (start_irqsoff_tracer(tr, (tr->flags & TRACE_ARRAY_FL_GLOBAL &&\r\nis_graph())))\r\nprintk(KERN_ERR "failed to start irqsoff tracer\n");\r\nirqsoff_busy = true;\r\nreturn 0;\r\n}\r\nstatic void irqsoff_tracer_reset(struct trace_array *tr)\r\n{\r\nint lat_flag = save_flags & TRACE_ITER_LATENCY_FMT;\r\nint overwrite_flag = save_flags & TRACE_ITER_OVERWRITE;\r\nstop_irqsoff_tracer(tr, is_graph());\r\nset_tracer_flag(tr, TRACE_ITER_LATENCY_FMT, lat_flag);\r\nset_tracer_flag(tr, TRACE_ITER_OVERWRITE, overwrite_flag);\r\nftrace_reset_array_ops(tr);\r\nirqsoff_busy = false;\r\n}\r\nstatic void irqsoff_tracer_start(struct trace_array *tr)\r\n{\r\ntracer_enabled = 1;\r\n}\r\nstatic void irqsoff_tracer_stop(struct trace_array *tr)\r\n{\r\ntracer_enabled = 0;\r\n}\r\nstatic int irqsoff_tracer_init(struct trace_array *tr)\r\n{\r\ntrace_type = TRACER_IRQS_OFF;\r\nreturn __irqsoff_tracer_init(tr);\r\n}\r\nstatic int preemptoff_tracer_init(struct trace_array *tr)\r\n{\r\ntrace_type = TRACER_PREEMPT_OFF;\r\nreturn __irqsoff_tracer_init(tr);\r\n}\r\nstatic int preemptirqsoff_tracer_init(struct trace_array *tr)\r\n{\r\ntrace_type = TRACER_IRQS_OFF | TRACER_PREEMPT_OFF;\r\nreturn __irqsoff_tracer_init(tr);\r\n}\r\n__init static int init_irqsoff_tracer(void)\r\n{\r\nregister_irqsoff(irqsoff_tracer);\r\nregister_preemptoff(preemptoff_tracer);\r\nregister_preemptirqsoff(preemptirqsoff_tracer);\r\nreturn 0;\r\n}
