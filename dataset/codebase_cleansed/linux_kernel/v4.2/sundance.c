static void sundance_reset(struct net_device *dev, unsigned long reset_cmd)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base + ASICCtrl;\r\nint countdown;\r\niowrite32 (reset_cmd | ioread32 (ioaddr), ioaddr);\r\ncountdown = 10 + 1;\r\nwhile (ioread32 (ioaddr) & (ResetBusy << 16)) {\r\nif (--countdown == 0) {\r\nprintk(KERN_WARNING "%s : reset not completed !!\n", dev->name);\r\nbreak;\r\n}\r\nudelay(100);\r\n}\r\n}\r\nstatic void sundance_poll_controller(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\ndisable_irq(np->pci_dev->irq);\r\nintr_handler(np->pci_dev->irq, dev);\r\nenable_irq(np->pci_dev->irq);\r\n}\r\nstatic int sundance_probe1(struct pci_dev *pdev,\r\nconst struct pci_device_id *ent)\r\n{\r\nstruct net_device *dev;\r\nstruct netdev_private *np;\r\nstatic int card_idx;\r\nint chip_idx = ent->driver_data;\r\nint irq;\r\nint i;\r\nvoid __iomem *ioaddr;\r\nu16 mii_ctl;\r\nvoid *ring_space;\r\ndma_addr_t ring_dma;\r\n#ifdef USE_IO_OPS\r\nint bar = 0;\r\n#else\r\nint bar = 1;\r\n#endif\r\nint phy, phy_end, phy_idx = 0;\r\n#ifndef MODULE\r\nstatic int printed_version;\r\nif (!printed_version++)\r\nprintk(version);\r\n#endif\r\nif (pci_enable_device(pdev))\r\nreturn -EIO;\r\npci_set_master(pdev);\r\nirq = pdev->irq;\r\ndev = alloc_etherdev(sizeof(*np));\r\nif (!dev)\r\nreturn -ENOMEM;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nif (pci_request_regions(pdev, DRV_NAME))\r\ngoto err_out_netdev;\r\nioaddr = pci_iomap(pdev, bar, netdev_io_size);\r\nif (!ioaddr)\r\ngoto err_out_res;\r\nfor (i = 0; i < 3; i++)\r\n((__le16 *)dev->dev_addr)[i] =\r\ncpu_to_le16(eeprom_read(ioaddr, i + EEPROM_SA_OFFSET));\r\nnp = netdev_priv(dev);\r\nnp->base = ioaddr;\r\nnp->pci_dev = pdev;\r\nnp->chip_id = chip_idx;\r\nnp->msg_enable = (1 << debug) - 1;\r\nspin_lock_init(&np->lock);\r\nspin_lock_init(&np->statlock);\r\ntasklet_init(&np->rx_tasklet, rx_poll, (unsigned long)dev);\r\ntasklet_init(&np->tx_tasklet, tx_poll, (unsigned long)dev);\r\nring_space = dma_alloc_coherent(&pdev->dev, TX_TOTAL_SIZE,\r\n&ring_dma, GFP_KERNEL);\r\nif (!ring_space)\r\ngoto err_out_cleardev;\r\nnp->tx_ring = (struct netdev_desc *)ring_space;\r\nnp->tx_ring_dma = ring_dma;\r\nring_space = dma_alloc_coherent(&pdev->dev, RX_TOTAL_SIZE,\r\n&ring_dma, GFP_KERNEL);\r\nif (!ring_space)\r\ngoto err_out_unmap_tx;\r\nnp->rx_ring = (struct netdev_desc *)ring_space;\r\nnp->rx_ring_dma = ring_dma;\r\nnp->mii_if.dev = dev;\r\nnp->mii_if.mdio_read = mdio_read;\r\nnp->mii_if.mdio_write = mdio_write;\r\nnp->mii_if.phy_id_mask = 0x1f;\r\nnp->mii_if.reg_num_mask = 0x1f;\r\ndev->netdev_ops = &netdev_ops;\r\ndev->ethtool_ops = &ethtool_ops;\r\ndev->watchdog_timeo = TX_TIMEOUT;\r\npci_set_drvdata(pdev, dev);\r\ni = register_netdev(dev);\r\nif (i)\r\ngoto err_out_unmap_rx;\r\nprintk(KERN_INFO "%s: %s at %p, %pM, IRQ %d.\n",\r\ndev->name, pci_id_tbl[chip_idx].name, ioaddr,\r\ndev->dev_addr, irq);\r\nnp->phys[0] = 1;\r\nnp->mii_preamble_required++;\r\nif (sundance_pci_tbl[np->chip_id].device == 0x0200) {\r\nphy = 0;\r\nphy_end = 31;\r\n} else {\r\nphy = 1;\r\nphy_end = 32;\r\n}\r\nfor (; phy <= phy_end && phy_idx < MII_CNT; phy++) {\r\nint phyx = phy & 0x1f;\r\nint mii_status = mdio_read(dev, phyx, MII_BMSR);\r\nif (mii_status != 0xffff && mii_status != 0x0000) {\r\nnp->phys[phy_idx++] = phyx;\r\nnp->mii_if.advertising = mdio_read(dev, phyx, MII_ADVERTISE);\r\nif ((mii_status & 0x0040) == 0)\r\nnp->mii_preamble_required++;\r\nprintk(KERN_INFO "%s: MII PHY found at address %d, status "\r\n"0x%4.4x advertising %4.4x.\n",\r\ndev->name, phyx, mii_status, np->mii_if.advertising);\r\n}\r\n}\r\nnp->mii_preamble_required--;\r\nif (phy_idx == 0) {\r\nprintk(KERN_INFO "%s: No MII transceiver found, aborting. ASIC status %x\n",\r\ndev->name, ioread32(ioaddr + ASICCtrl));\r\ngoto err_out_unregister;\r\n}\r\nnp->mii_if.phy_id = np->phys[0];\r\nnp->an_enable = 1;\r\nif (card_idx < MAX_UNITS) {\r\nif (media[card_idx] != NULL) {\r\nnp->an_enable = 0;\r\nif (strcmp (media[card_idx], "100mbps_fd") == 0 ||\r\nstrcmp (media[card_idx], "4") == 0) {\r\nnp->speed = 100;\r\nnp->mii_if.full_duplex = 1;\r\n} else if (strcmp (media[card_idx], "100mbps_hd") == 0 ||\r\nstrcmp (media[card_idx], "3") == 0) {\r\nnp->speed = 100;\r\nnp->mii_if.full_duplex = 0;\r\n} else if (strcmp (media[card_idx], "10mbps_fd") == 0 ||\r\nstrcmp (media[card_idx], "2") == 0) {\r\nnp->speed = 10;\r\nnp->mii_if.full_duplex = 1;\r\n} else if (strcmp (media[card_idx], "10mbps_hd") == 0 ||\r\nstrcmp (media[card_idx], "1") == 0) {\r\nnp->speed = 10;\r\nnp->mii_if.full_duplex = 0;\r\n} else {\r\nnp->an_enable = 1;\r\n}\r\n}\r\nif (flowctrl == 1)\r\nnp->flowctrl = 1;\r\n}\r\nif (ioread32 (ioaddr + ASICCtrl) & 0x80) {\r\nif (np->an_enable) {\r\nnp->speed = 100;\r\nnp->mii_if.full_duplex = 1;\r\nnp->an_enable = 0;\r\n}\r\n}\r\nmdio_write (dev, np->phys[0], MII_BMCR, BMCR_RESET);\r\nmdelay (300);\r\nif (np->flowctrl)\r\nmdio_write (dev, np->phys[0], MII_ADVERTISE, np->mii_if.advertising | 0x0400);\r\nmdio_write (dev, np->phys[0], MII_BMCR, BMCR_ANENABLE|BMCR_ANRESTART);\r\nif (!np->an_enable) {\r\nmii_ctl = 0;\r\nmii_ctl |= (np->speed == 100) ? BMCR_SPEED100 : 0;\r\nmii_ctl |= (np->mii_if.full_duplex) ? BMCR_FULLDPLX : 0;\r\nmdio_write (dev, np->phys[0], MII_BMCR, mii_ctl);\r\nprintk (KERN_INFO "Override speed=%d, %s duplex\n",\r\nnp->speed, np->mii_if.full_duplex ? "Full" : "Half");\r\n}\r\nif (netif_msg_hw(np))\r\nprintk("ASIC Control is %x.\n", ioread32(ioaddr + ASICCtrl));\r\nsundance_reset(dev, 0x00ff << 16);\r\nif (netif_msg_hw(np))\r\nprintk("ASIC Control is now %x.\n", ioread32(ioaddr + ASICCtrl));\r\ncard_idx++;\r\nreturn 0;\r\nerr_out_unregister:\r\nunregister_netdev(dev);\r\nerr_out_unmap_rx:\r\ndma_free_coherent(&pdev->dev, RX_TOTAL_SIZE,\r\nnp->rx_ring, np->rx_ring_dma);\r\nerr_out_unmap_tx:\r\ndma_free_coherent(&pdev->dev, TX_TOTAL_SIZE,\r\nnp->tx_ring, np->tx_ring_dma);\r\nerr_out_cleardev:\r\npci_iounmap(pdev, ioaddr);\r\nerr_out_res:\r\npci_release_regions(pdev);\r\nerr_out_netdev:\r\nfree_netdev (dev);\r\nreturn -ENODEV;\r\n}\r\nstatic int change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nif ((new_mtu < 68) || (new_mtu > 8191))\r\nreturn -EINVAL;\r\nif (netif_running(dev))\r\nreturn -EBUSY;\r\ndev->mtu = new_mtu;\r\nreturn 0;\r\n}\r\nstatic int eeprom_read(void __iomem *ioaddr, int location)\r\n{\r\nint boguscnt = 10000;\r\niowrite16(0x0200 | (location & 0xff), ioaddr + EECtrl);\r\ndo {\r\neeprom_delay(ioaddr + EECtrl);\r\nif (! (ioread16(ioaddr + EECtrl) & 0x8000)) {\r\nreturn ioread16(ioaddr + EEData);\r\n}\r\n} while (--boguscnt > 0);\r\nreturn 0;\r\n}\r\nstatic void mdio_sync(void __iomem *mdio_addr)\r\n{\r\nint bits = 32;\r\nwhile (--bits >= 0) {\r\niowrite8(MDIO_WRITE1, mdio_addr);\r\nmdio_delay();\r\niowrite8(MDIO_WRITE1 | MDIO_ShiftClk, mdio_addr);\r\nmdio_delay();\r\n}\r\n}\r\nstatic int mdio_read(struct net_device *dev, int phy_id, int location)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *mdio_addr = np->base + MIICtrl;\r\nint mii_cmd = (0xf6 << 10) | (phy_id << 5) | location;\r\nint i, retval = 0;\r\nif (np->mii_preamble_required)\r\nmdio_sync(mdio_addr);\r\nfor (i = 15; i >= 0; i--) {\r\nint dataval = (mii_cmd & (1 << i)) ? MDIO_WRITE1 : MDIO_WRITE0;\r\niowrite8(dataval, mdio_addr);\r\nmdio_delay();\r\niowrite8(dataval | MDIO_ShiftClk, mdio_addr);\r\nmdio_delay();\r\n}\r\nfor (i = 19; i > 0; i--) {\r\niowrite8(MDIO_EnbIn, mdio_addr);\r\nmdio_delay();\r\nretval = (retval << 1) | ((ioread8(mdio_addr) & MDIO_Data) ? 1 : 0);\r\niowrite8(MDIO_EnbIn | MDIO_ShiftClk, mdio_addr);\r\nmdio_delay();\r\n}\r\nreturn (retval>>1) & 0xffff;\r\n}\r\nstatic void mdio_write(struct net_device *dev, int phy_id, int location, int value)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *mdio_addr = np->base + MIICtrl;\r\nint mii_cmd = (0x5002 << 16) | (phy_id << 23) | (location<<18) | value;\r\nint i;\r\nif (np->mii_preamble_required)\r\nmdio_sync(mdio_addr);\r\nfor (i = 31; i >= 0; i--) {\r\nint dataval = (mii_cmd & (1 << i)) ? MDIO_WRITE1 : MDIO_WRITE0;\r\niowrite8(dataval, mdio_addr);\r\nmdio_delay();\r\niowrite8(dataval | MDIO_ShiftClk, mdio_addr);\r\nmdio_delay();\r\n}\r\nfor (i = 2; i > 0; i--) {\r\niowrite8(MDIO_EnbIn, mdio_addr);\r\nmdio_delay();\r\niowrite8(MDIO_EnbIn | MDIO_ShiftClk, mdio_addr);\r\nmdio_delay();\r\n}\r\n}\r\nstatic int mdio_wait_link(struct net_device *dev, int wait)\r\n{\r\nint bmsr;\r\nint phy_id;\r\nstruct netdev_private *np;\r\nnp = netdev_priv(dev);\r\nphy_id = np->phys[0];\r\ndo {\r\nbmsr = mdio_read(dev, phy_id, MII_BMSR);\r\nif (bmsr & 0x0004)\r\nreturn 0;\r\nmdelay(1);\r\n} while (--wait > 0);\r\nreturn -1;\r\n}\r\nstatic int netdev_open(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nconst int irq = np->pci_dev->irq;\r\nunsigned long flags;\r\nint i;\r\nsundance_reset(dev, 0x00ff << 16);\r\ni = request_irq(irq, intr_handler, IRQF_SHARED, dev->name, dev);\r\nif (i)\r\nreturn i;\r\nif (netif_msg_ifup(np))\r\nprintk(KERN_DEBUG "%s: netdev_open() irq %d\n", dev->name, irq);\r\ninit_ring(dev);\r\niowrite32(np->rx_ring_dma, ioaddr + RxListPtr);\r\n__set_mac_addr(dev);\r\n#if defined(CONFIG_VLAN_8021Q) || defined(CONFIG_VLAN_8021Q_MODULE)\r\niowrite16(dev->mtu + 18, ioaddr + MaxFrameSize);\r\n#else\r\niowrite16(dev->mtu + 14, ioaddr + MaxFrameSize);\r\n#endif\r\nif (dev->mtu > 2047)\r\niowrite32(ioread32(ioaddr + ASICCtrl) | 0x0C, ioaddr + ASICCtrl);\r\nif (dev->if_port == 0)\r\ndev->if_port = np->default_port;\r\nspin_lock_init(&np->mcastlock);\r\nset_rx_mode(dev);\r\niowrite16(0, ioaddr + IntrEnable);\r\niowrite16(0, ioaddr + DownCounter);\r\niowrite8(100, ioaddr + RxDMAPollPeriod);\r\niowrite8(127, ioaddr + TxDMAPollPeriod);\r\nif (np->pci_dev->revision >= 0x14)\r\niowrite8(0x01, ioaddr + DebugCtrl1);\r\nnetif_start_queue(dev);\r\nspin_lock_irqsave(&np->lock, flags);\r\nreset_tx(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\niowrite16 (StatsEnable | RxEnable | TxEnable, ioaddr + MACCtrl1);\r\niowrite8(ioread8(ioaddr + WakeEvent) | 0x00, ioaddr + WakeEvent);\r\nnp->wol_enabled = 0;\r\nif (netif_msg_ifup(np))\r\nprintk(KERN_DEBUG "%s: Done netdev_open(), status: Rx %x Tx %x "\r\n"MAC Control %x, %4.4x %4.4x.\n",\r\ndev->name, ioread32(ioaddr + RxStatus), ioread8(ioaddr + TxStatus),\r\nioread32(ioaddr + MACCtrl0),\r\nioread16(ioaddr + MACCtrl1), ioread16(ioaddr + MACCtrl0));\r\ninit_timer(&np->timer);\r\nnp->timer.expires = jiffies + 3*HZ;\r\nnp->timer.data = (unsigned long)dev;\r\nnp->timer.function = netdev_timer;\r\nadd_timer(&np->timer);\r\niowrite16(DEFAULT_INTR, ioaddr + IntrEnable);\r\nreturn 0;\r\n}\r\nstatic void check_duplex(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nint mii_lpa = mdio_read(dev, np->phys[0], MII_LPA);\r\nint negotiated = mii_lpa & np->mii_if.advertising;\r\nint duplex;\r\nif (!np->an_enable || mii_lpa == 0xffff) {\r\nif (np->mii_if.full_duplex)\r\niowrite16 (ioread16 (ioaddr + MACCtrl0) | EnbFullDuplex,\r\nioaddr + MACCtrl0);\r\nreturn;\r\n}\r\nduplex = (negotiated & 0x0100) || (negotiated & 0x01C0) == 0x0040;\r\nif (np->mii_if.full_duplex != duplex) {\r\nnp->mii_if.full_duplex = duplex;\r\nif (netif_msg_link(np))\r\nprintk(KERN_INFO "%s: Setting %s-duplex based on MII #%d "\r\n"negotiated capability %4.4x.\n", dev->name,\r\nduplex ? "full" : "half", np->phys[0], negotiated);\r\niowrite16(ioread16(ioaddr + MACCtrl0) | (duplex ? 0x20 : 0), ioaddr + MACCtrl0);\r\n}\r\n}\r\nstatic void netdev_timer(unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *)data;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nint next_tick = 10*HZ;\r\nif (netif_msg_timer(np)) {\r\nprintk(KERN_DEBUG "%s: Media selection timer tick, intr status %4.4x, "\r\n"Tx %x Rx %x.\n",\r\ndev->name, ioread16(ioaddr + IntrEnable),\r\nioread8(ioaddr + TxStatus), ioread32(ioaddr + RxStatus));\r\n}\r\ncheck_duplex(dev);\r\nnp->timer.expires = jiffies + next_tick;\r\nadd_timer(&np->timer);\r\n}\r\nstatic void tx_timeout(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nunsigned long flag;\r\nnetif_stop_queue(dev);\r\ntasklet_disable(&np->tx_tasklet);\r\niowrite16(0, ioaddr + IntrEnable);\r\nprintk(KERN_WARNING "%s: Transmit timed out, TxStatus %2.2x "\r\n"TxFrameId %2.2x,"\r\n" resetting...\n", dev->name, ioread8(ioaddr + TxStatus),\r\nioread8(ioaddr + TxFrameId));\r\n{\r\nint i;\r\nfor (i=0; i<TX_RING_SIZE; i++) {\r\nprintk(KERN_DEBUG "%02x %08llx %08x %08x(%02x) %08x %08x\n", i,\r\n(unsigned long long)(np->tx_ring_dma + i*sizeof(*np->tx_ring)),\r\nle32_to_cpu(np->tx_ring[i].next_desc),\r\nle32_to_cpu(np->tx_ring[i].status),\r\n(le32_to_cpu(np->tx_ring[i].status) >> 2) & 0xff,\r\nle32_to_cpu(np->tx_ring[i].frag[0].addr),\r\nle32_to_cpu(np->tx_ring[i].frag[0].length));\r\n}\r\nprintk(KERN_DEBUG "TxListPtr=%08x netif_queue_stopped=%d\n",\r\nioread32(np->base + TxListPtr),\r\nnetif_queue_stopped(dev));\r\nprintk(KERN_DEBUG "cur_tx=%d(%02x) dirty_tx=%d(%02x)\n",\r\nnp->cur_tx, np->cur_tx % TX_RING_SIZE,\r\nnp->dirty_tx, np->dirty_tx % TX_RING_SIZE);\r\nprintk(KERN_DEBUG "cur_rx=%d dirty_rx=%d\n", np->cur_rx, np->dirty_rx);\r\nprintk(KERN_DEBUG "cur_task=%d\n", np->cur_task);\r\n}\r\nspin_lock_irqsave(&np->lock, flag);\r\nreset_tx(dev);\r\nspin_unlock_irqrestore(&np->lock, flag);\r\ndev->if_port = 0;\r\ndev->trans_start = jiffies;\r\ndev->stats.tx_errors++;\r\nif (np->cur_tx - np->dirty_tx < TX_QUEUE_LEN - 4) {\r\nnetif_wake_queue(dev);\r\n}\r\niowrite16(DEFAULT_INTR, ioaddr + IntrEnable);\r\ntasklet_enable(&np->tx_tasklet);\r\n}\r\nstatic void init_ring(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint i;\r\nnp->cur_rx = np->cur_tx = 0;\r\nnp->dirty_rx = np->dirty_tx = 0;\r\nnp->cur_task = 0;\r\nnp->rx_buf_sz = (dev->mtu <= 1520 ? PKT_BUF_SZ : dev->mtu + 16);\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nnp->rx_ring[i].next_desc = cpu_to_le32(np->rx_ring_dma +\r\n((i+1)%RX_RING_SIZE)*sizeof(*np->rx_ring));\r\nnp->rx_ring[i].status = 0;\r\nnp->rx_ring[i].frag[0].length = 0;\r\nnp->rx_skbuff[i] = NULL;\r\n}\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nstruct sk_buff *skb =\r\nnetdev_alloc_skb(dev, np->rx_buf_sz + 2);\r\nnp->rx_skbuff[i] = skb;\r\nif (skb == NULL)\r\nbreak;\r\nskb_reserve(skb, 2);\r\nnp->rx_ring[i].frag[0].addr = cpu_to_le32(\r\ndma_map_single(&np->pci_dev->dev, skb->data,\r\nnp->rx_buf_sz, DMA_FROM_DEVICE));\r\nif (dma_mapping_error(&np->pci_dev->dev,\r\nnp->rx_ring[i].frag[0].addr)) {\r\ndev_kfree_skb(skb);\r\nnp->rx_skbuff[i] = NULL;\r\nbreak;\r\n}\r\nnp->rx_ring[i].frag[0].length = cpu_to_le32(np->rx_buf_sz | LastFrag);\r\n}\r\nnp->dirty_rx = (unsigned int)(i - RX_RING_SIZE);\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nnp->tx_skbuff[i] = NULL;\r\nnp->tx_ring[i].status = 0;\r\n}\r\n}\r\nstatic void tx_poll (unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *)data;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nunsigned head = np->cur_task % TX_RING_SIZE;\r\nstruct netdev_desc *txdesc =\r\n&np->tx_ring[(np->cur_tx - 1) % TX_RING_SIZE];\r\nfor (; np->cur_tx - np->cur_task > 0; np->cur_task++) {\r\nint entry = np->cur_task % TX_RING_SIZE;\r\ntxdesc = &np->tx_ring[entry];\r\nif (np->last_tx) {\r\nnp->last_tx->next_desc = cpu_to_le32(np->tx_ring_dma +\r\nentry*sizeof(struct netdev_desc));\r\n}\r\nnp->last_tx = txdesc;\r\n}\r\ntxdesc->status |= cpu_to_le32(DescIntrOnTx);\r\nif (ioread32 (np->base + TxListPtr) == 0)\r\niowrite32 (np->tx_ring_dma + head * sizeof(struct netdev_desc),\r\nnp->base + TxListPtr);\r\n}\r\nstatic netdev_tx_t\r\nstart_tx (struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nstruct netdev_desc *txdesc;\r\nunsigned entry;\r\nentry = np->cur_tx % TX_RING_SIZE;\r\nnp->tx_skbuff[entry] = skb;\r\ntxdesc = &np->tx_ring[entry];\r\ntxdesc->next_desc = 0;\r\ntxdesc->status = cpu_to_le32 ((entry << 2) | DisableAlign);\r\ntxdesc->frag[0].addr = cpu_to_le32(dma_map_single(&np->pci_dev->dev,\r\nskb->data, skb->len, DMA_TO_DEVICE));\r\nif (dma_mapping_error(&np->pci_dev->dev,\r\ntxdesc->frag[0].addr))\r\ngoto drop_frame;\r\ntxdesc->frag[0].length = cpu_to_le32 (skb->len | LastFrag);\r\nnp->cur_tx++;\r\nmb();\r\ntasklet_schedule(&np->tx_tasklet);\r\nif (np->cur_tx - np->dirty_tx < TX_QUEUE_LEN - 1 &&\r\n!netif_queue_stopped(dev)) {\r\n} else {\r\nnetif_stop_queue (dev);\r\n}\r\nif (netif_msg_tx_queued(np)) {\r\nprintk (KERN_DEBUG\r\n"%s: Transmit frame #%d queued in slot %d.\n",\r\ndev->name, np->cur_tx, entry);\r\n}\r\nreturn NETDEV_TX_OK;\r\ndrop_frame:\r\ndev_kfree_skb_any(skb);\r\nnp->tx_skbuff[entry] = NULL;\r\ndev->stats.tx_dropped++;\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int\r\nreset_tx (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nstruct sk_buff *skb;\r\nint i;\r\niowrite16 (TxDisable, ioaddr + MACCtrl1);\r\nsundance_reset(dev, (NetworkReset|FIFOReset|DMAReset|TxReset) << 16);\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nnp->tx_ring[i].next_desc = 0;\r\nskb = np->tx_skbuff[i];\r\nif (skb) {\r\ndma_unmap_single(&np->pci_dev->dev,\r\nle32_to_cpu(np->tx_ring[i].frag[0].addr),\r\nskb->len, DMA_TO_DEVICE);\r\ndev_kfree_skb_any(skb);\r\nnp->tx_skbuff[i] = NULL;\r\ndev->stats.tx_dropped++;\r\n}\r\n}\r\nnp->cur_tx = np->dirty_tx = 0;\r\nnp->cur_task = 0;\r\nnp->last_tx = NULL;\r\niowrite8(127, ioaddr + TxDMAPollPeriod);\r\niowrite16 (StatsEnable | RxEnable | TxEnable, ioaddr + MACCtrl1);\r\nreturn 0;\r\n}\r\nstatic irqreturn_t intr_handler(int irq, void *dev_instance)\r\n{\r\nstruct net_device *dev = (struct net_device *)dev_instance;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nint hw_frame_id;\r\nint tx_cnt;\r\nint tx_status;\r\nint handled = 0;\r\nint i;\r\ndo {\r\nint intr_status = ioread16(ioaddr + IntrStatus);\r\niowrite16(intr_status, ioaddr + IntrStatus);\r\nif (netif_msg_intr(np))\r\nprintk(KERN_DEBUG "%s: Interrupt, status %4.4x.\n",\r\ndev->name, intr_status);\r\nif (!(intr_status & DEFAULT_INTR))\r\nbreak;\r\nhandled = 1;\r\nif (intr_status & (IntrRxDMADone)) {\r\niowrite16(DEFAULT_INTR & ~(IntrRxDone|IntrRxDMADone),\r\nioaddr + IntrEnable);\r\nif (np->budget < 0)\r\nnp->budget = RX_BUDGET;\r\ntasklet_schedule(&np->rx_tasklet);\r\n}\r\nif (intr_status & (IntrTxDone | IntrDrvRqst)) {\r\ntx_status = ioread16 (ioaddr + TxStatus);\r\nfor (tx_cnt=32; tx_status & 0x80; --tx_cnt) {\r\nif (netif_msg_tx_done(np))\r\nprintk\r\n("%s: Transmit status is %2.2x.\n",\r\ndev->name, tx_status);\r\nif (tx_status & 0x1e) {\r\nif (netif_msg_tx_err(np))\r\nprintk("%s: Transmit error status %4.4x.\n",\r\ndev->name, tx_status);\r\ndev->stats.tx_errors++;\r\nif (tx_status & 0x10)\r\ndev->stats.tx_fifo_errors++;\r\nif (tx_status & 0x08)\r\ndev->stats.collisions++;\r\nif (tx_status & 0x04)\r\ndev->stats.tx_fifo_errors++;\r\nif (tx_status & 0x02)\r\ndev->stats.tx_window_errors++;\r\nif (tx_status & 0x10) {\r\nsundance_reset(dev, (NetworkReset|FIFOReset|TxReset) << 16);\r\n}\r\ni = 10;\r\ndo {\r\niowrite16(ioread16(ioaddr + MACCtrl1) | TxEnable, ioaddr + MACCtrl1);\r\nif (ioread16(ioaddr + MACCtrl1) & TxEnabled)\r\nbreak;\r\nmdelay(1);\r\n} while (--i);\r\n}\r\niowrite16 (0, ioaddr + TxStatus);\r\nif (tx_cnt < 0) {\r\niowrite32(5000, ioaddr + DownCounter);\r\nbreak;\r\n}\r\ntx_status = ioread16 (ioaddr + TxStatus);\r\n}\r\nhw_frame_id = (tx_status >> 8) & 0xff;\r\n} else {\r\nhw_frame_id = ioread8(ioaddr + TxFrameId);\r\n}\r\nif (np->pci_dev->revision >= 0x14) {\r\nspin_lock(&np->lock);\r\nfor (; np->cur_tx - np->dirty_tx > 0; np->dirty_tx++) {\r\nint entry = np->dirty_tx % TX_RING_SIZE;\r\nstruct sk_buff *skb;\r\nint sw_frame_id;\r\nsw_frame_id = (le32_to_cpu(\r\nnp->tx_ring[entry].status) >> 2) & 0xff;\r\nif (sw_frame_id == hw_frame_id &&\r\n!(le32_to_cpu(np->tx_ring[entry].status)\r\n& 0x00010000))\r\nbreak;\r\nif (sw_frame_id == (hw_frame_id + 1) %\r\nTX_RING_SIZE)\r\nbreak;\r\nskb = np->tx_skbuff[entry];\r\ndma_unmap_single(&np->pci_dev->dev,\r\nle32_to_cpu(np->tx_ring[entry].frag[0].addr),\r\nskb->len, DMA_TO_DEVICE);\r\ndev_kfree_skb_irq (np->tx_skbuff[entry]);\r\nnp->tx_skbuff[entry] = NULL;\r\nnp->tx_ring[entry].frag[0].addr = 0;\r\nnp->tx_ring[entry].frag[0].length = 0;\r\n}\r\nspin_unlock(&np->lock);\r\n} else {\r\nspin_lock(&np->lock);\r\nfor (; np->cur_tx - np->dirty_tx > 0; np->dirty_tx++) {\r\nint entry = np->dirty_tx % TX_RING_SIZE;\r\nstruct sk_buff *skb;\r\nif (!(le32_to_cpu(np->tx_ring[entry].status)\r\n& 0x00010000))\r\nbreak;\r\nskb = np->tx_skbuff[entry];\r\ndma_unmap_single(&np->pci_dev->dev,\r\nle32_to_cpu(np->tx_ring[entry].frag[0].addr),\r\nskb->len, DMA_TO_DEVICE);\r\ndev_kfree_skb_irq (np->tx_skbuff[entry]);\r\nnp->tx_skbuff[entry] = NULL;\r\nnp->tx_ring[entry].frag[0].addr = 0;\r\nnp->tx_ring[entry].frag[0].length = 0;\r\n}\r\nspin_unlock(&np->lock);\r\n}\r\nif (netif_queue_stopped(dev) &&\r\nnp->cur_tx - np->dirty_tx < TX_QUEUE_LEN - 4) {\r\nnetif_wake_queue (dev);\r\n}\r\nif (intr_status & (IntrPCIErr | LinkChange | StatsMax))\r\nnetdev_error(dev, intr_status);\r\n} while (0);\r\nif (netif_msg_intr(np))\r\nprintk(KERN_DEBUG "%s: exiting interrupt, status=%#4.4x.\n",\r\ndev->name, ioread16(ioaddr + IntrStatus));\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic void rx_poll(unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *)data;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint entry = np->cur_rx % RX_RING_SIZE;\r\nint boguscnt = np->budget;\r\nvoid __iomem *ioaddr = np->base;\r\nint received = 0;\r\nwhile (1) {\r\nstruct netdev_desc *desc = &(np->rx_ring[entry]);\r\nu32 frame_status = le32_to_cpu(desc->status);\r\nint pkt_len;\r\nif (--boguscnt < 0) {\r\ngoto not_done;\r\n}\r\nif (!(frame_status & DescOwn))\r\nbreak;\r\npkt_len = frame_status & 0x1fff;\r\nif (netif_msg_rx_status(np))\r\nprintk(KERN_DEBUG " netdev_rx() status was %8.8x.\n",\r\nframe_status);\r\nif (frame_status & 0x001f4000) {\r\nif (netif_msg_rx_err(np))\r\nprintk(KERN_DEBUG " netdev_rx() Rx error was %8.8x.\n",\r\nframe_status);\r\ndev->stats.rx_errors++;\r\nif (frame_status & 0x00100000)\r\ndev->stats.rx_length_errors++;\r\nif (frame_status & 0x00010000)\r\ndev->stats.rx_fifo_errors++;\r\nif (frame_status & 0x00060000)\r\ndev->stats.rx_frame_errors++;\r\nif (frame_status & 0x00080000)\r\ndev->stats.rx_crc_errors++;\r\nif (frame_status & 0x00100000) {\r\nprintk(KERN_WARNING "%s: Oversized Ethernet frame,"\r\n" status %8.8x.\n",\r\ndev->name, frame_status);\r\n}\r\n} else {\r\nstruct sk_buff *skb;\r\n#ifndef final_version\r\nif (netif_msg_rx_status(np))\r\nprintk(KERN_DEBUG " netdev_rx() normal Rx pkt length %d"\r\n", bogus_cnt %d.\n",\r\npkt_len, boguscnt);\r\n#endif\r\nif (pkt_len < rx_copybreak &&\r\n(skb = netdev_alloc_skb(dev, pkt_len + 2)) != NULL) {\r\nskb_reserve(skb, 2);\r\ndma_sync_single_for_cpu(&np->pci_dev->dev,\r\nle32_to_cpu(desc->frag[0].addr),\r\nnp->rx_buf_sz, DMA_FROM_DEVICE);\r\nskb_copy_to_linear_data(skb, np->rx_skbuff[entry]->data, pkt_len);\r\ndma_sync_single_for_device(&np->pci_dev->dev,\r\nle32_to_cpu(desc->frag[0].addr),\r\nnp->rx_buf_sz, DMA_FROM_DEVICE);\r\nskb_put(skb, pkt_len);\r\n} else {\r\ndma_unmap_single(&np->pci_dev->dev,\r\nle32_to_cpu(desc->frag[0].addr),\r\nnp->rx_buf_sz, DMA_FROM_DEVICE);\r\nskb_put(skb = np->rx_skbuff[entry], pkt_len);\r\nnp->rx_skbuff[entry] = NULL;\r\n}\r\nskb->protocol = eth_type_trans(skb, dev);\r\nnetif_rx(skb);\r\n}\r\nentry = (entry + 1) % RX_RING_SIZE;\r\nreceived++;\r\n}\r\nnp->cur_rx = entry;\r\nrefill_rx (dev);\r\nnp->budget -= received;\r\niowrite16(DEFAULT_INTR, ioaddr + IntrEnable);\r\nreturn;\r\nnot_done:\r\nnp->cur_rx = entry;\r\nrefill_rx (dev);\r\nif (!received)\r\nreceived = 1;\r\nnp->budget -= received;\r\nif (np->budget <= 0)\r\nnp->budget = RX_BUDGET;\r\ntasklet_schedule(&np->rx_tasklet);\r\n}\r\nstatic void refill_rx (struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint entry;\r\nint cnt = 0;\r\nfor (;(np->cur_rx - np->dirty_rx + RX_RING_SIZE) % RX_RING_SIZE > 0;\r\nnp->dirty_rx = (np->dirty_rx + 1) % RX_RING_SIZE) {\r\nstruct sk_buff *skb;\r\nentry = np->dirty_rx % RX_RING_SIZE;\r\nif (np->rx_skbuff[entry] == NULL) {\r\nskb = netdev_alloc_skb(dev, np->rx_buf_sz + 2);\r\nnp->rx_skbuff[entry] = skb;\r\nif (skb == NULL)\r\nbreak;\r\nskb_reserve(skb, 2);\r\nnp->rx_ring[entry].frag[0].addr = cpu_to_le32(\r\ndma_map_single(&np->pci_dev->dev, skb->data,\r\nnp->rx_buf_sz, DMA_FROM_DEVICE));\r\nif (dma_mapping_error(&np->pci_dev->dev,\r\nnp->rx_ring[entry].frag[0].addr)) {\r\ndev_kfree_skb_irq(skb);\r\nnp->rx_skbuff[entry] = NULL;\r\nbreak;\r\n}\r\n}\r\nnp->rx_ring[entry].frag[0].length =\r\ncpu_to_le32(np->rx_buf_sz | LastFrag);\r\nnp->rx_ring[entry].status = 0;\r\ncnt++;\r\n}\r\n}\r\nstatic void netdev_error(struct net_device *dev, int intr_status)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nu16 mii_ctl, mii_advertise, mii_lpa;\r\nint speed;\r\nif (intr_status & LinkChange) {\r\nif (mdio_wait_link(dev, 10) == 0) {\r\nprintk(KERN_INFO "%s: Link up\n", dev->name);\r\nif (np->an_enable) {\r\nmii_advertise = mdio_read(dev, np->phys[0],\r\nMII_ADVERTISE);\r\nmii_lpa = mdio_read(dev, np->phys[0], MII_LPA);\r\nmii_advertise &= mii_lpa;\r\nprintk(KERN_INFO "%s: Link changed: ",\r\ndev->name);\r\nif (mii_advertise & ADVERTISE_100FULL) {\r\nnp->speed = 100;\r\nprintk("100Mbps, full duplex\n");\r\n} else if (mii_advertise & ADVERTISE_100HALF) {\r\nnp->speed = 100;\r\nprintk("100Mbps, half duplex\n");\r\n} else if (mii_advertise & ADVERTISE_10FULL) {\r\nnp->speed = 10;\r\nprintk("10Mbps, full duplex\n");\r\n} else if (mii_advertise & ADVERTISE_10HALF) {\r\nnp->speed = 10;\r\nprintk("10Mbps, half duplex\n");\r\n} else\r\nprintk("\n");\r\n} else {\r\nmii_ctl = mdio_read(dev, np->phys[0], MII_BMCR);\r\nspeed = (mii_ctl & BMCR_SPEED100) ? 100 : 10;\r\nnp->speed = speed;\r\nprintk(KERN_INFO "%s: Link changed: %dMbps ,",\r\ndev->name, speed);\r\nprintk("%s duplex.\n",\r\n(mii_ctl & BMCR_FULLDPLX) ?\r\n"full" : "half");\r\n}\r\ncheck_duplex(dev);\r\nif (np->flowctrl && np->mii_if.full_duplex) {\r\niowrite16(ioread16(ioaddr + MulticastFilter1+2) | 0x0200,\r\nioaddr + MulticastFilter1+2);\r\niowrite16(ioread16(ioaddr + MACCtrl0) | EnbFlowCtrl,\r\nioaddr + MACCtrl0);\r\n}\r\nnetif_carrier_on(dev);\r\n} else {\r\nprintk(KERN_INFO "%s: Link down\n", dev->name);\r\nnetif_carrier_off(dev);\r\n}\r\n}\r\nif (intr_status & StatsMax) {\r\nget_stats(dev);\r\n}\r\nif (intr_status & IntrPCIErr) {\r\nprintk(KERN_ERR "%s: Something Wicked happened! %4.4x.\n",\r\ndev->name, intr_status);\r\n}\r\n}\r\nstatic struct net_device_stats *get_stats(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nunsigned long flags;\r\nu8 late_coll, single_coll, mult_coll;\r\nspin_lock_irqsave(&np->statlock, flags);\r\ndev->stats.rx_missed_errors += ioread8(ioaddr + RxMissed);\r\ndev->stats.tx_packets += ioread16(ioaddr + TxFramesOK);\r\ndev->stats.rx_packets += ioread16(ioaddr + RxFramesOK);\r\ndev->stats.tx_carrier_errors += ioread8(ioaddr + StatsCarrierError);\r\nmult_coll = ioread8(ioaddr + StatsMultiColl);\r\nnp->xstats.tx_multiple_collisions += mult_coll;\r\nsingle_coll = ioread8(ioaddr + StatsOneColl);\r\nnp->xstats.tx_single_collisions += single_coll;\r\nlate_coll = ioread8(ioaddr + StatsLateColl);\r\nnp->xstats.tx_late_collisions += late_coll;\r\ndev->stats.collisions += mult_coll\r\n+ single_coll\r\n+ late_coll;\r\nnp->xstats.tx_deferred += ioread8(ioaddr + StatsTxDefer);\r\nnp->xstats.tx_deferred_excessive += ioread8(ioaddr + StatsTxXSDefer);\r\nnp->xstats.tx_aborted += ioread8(ioaddr + StatsTxAbort);\r\nnp->xstats.tx_bcasts += ioread8(ioaddr + StatsBcastTx);\r\nnp->xstats.rx_bcasts += ioread8(ioaddr + StatsBcastRx);\r\nnp->xstats.tx_mcasts += ioread8(ioaddr + StatsMcastTx);\r\nnp->xstats.rx_mcasts += ioread8(ioaddr + StatsMcastRx);\r\ndev->stats.tx_bytes += ioread16(ioaddr + TxOctetsLow);\r\ndev->stats.tx_bytes += ioread16(ioaddr + TxOctetsHigh) << 16;\r\ndev->stats.rx_bytes += ioread16(ioaddr + RxOctetsLow);\r\ndev->stats.rx_bytes += ioread16(ioaddr + RxOctetsHigh) << 16;\r\nspin_unlock_irqrestore(&np->statlock, flags);\r\nreturn &dev->stats;\r\n}\r\nstatic void set_rx_mode(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nu16 mc_filter[4];\r\nu32 rx_mode;\r\nint i;\r\nif (dev->flags & IFF_PROMISC) {\r\nmemset(mc_filter, 0xff, sizeof(mc_filter));\r\nrx_mode = AcceptBroadcast | AcceptMulticast | AcceptAll | AcceptMyPhys;\r\n} else if ((netdev_mc_count(dev) > multicast_filter_limit) ||\r\n(dev->flags & IFF_ALLMULTI)) {\r\nmemset(mc_filter, 0xff, sizeof(mc_filter));\r\nrx_mode = AcceptBroadcast | AcceptMulticast | AcceptMyPhys;\r\n} else if (!netdev_mc_empty(dev)) {\r\nstruct netdev_hw_addr *ha;\r\nint bit;\r\nint index;\r\nint crc;\r\nmemset (mc_filter, 0, sizeof (mc_filter));\r\nnetdev_for_each_mc_addr(ha, dev) {\r\ncrc = ether_crc_le(ETH_ALEN, ha->addr);\r\nfor (index=0, bit=0; bit < 6; bit++, crc <<= 1)\r\nif (crc & 0x80000000) index |= 1 << bit;\r\nmc_filter[index/16] |= (1 << (index % 16));\r\n}\r\nrx_mode = AcceptBroadcast | AcceptMultiHash | AcceptMyPhys;\r\n} else {\r\niowrite8(AcceptBroadcast | AcceptMyPhys, ioaddr + RxMode);\r\nreturn;\r\n}\r\nif (np->mii_if.full_duplex && np->flowctrl)\r\nmc_filter[3] |= 0x0200;\r\nfor (i = 0; i < 4; i++)\r\niowrite16(mc_filter[i], ioaddr + MulticastFilter0 + i*2);\r\niowrite8(rx_mode, ioaddr + RxMode);\r\n}\r\nstatic int __set_mac_addr(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nu16 addr16;\r\naddr16 = (dev->dev_addr[0] | (dev->dev_addr[1] << 8));\r\niowrite16(addr16, np->base + StationAddr);\r\naddr16 = (dev->dev_addr[2] | (dev->dev_addr[3] << 8));\r\niowrite16(addr16, np->base + StationAddr+2);\r\naddr16 = (dev->dev_addr[4] | (dev->dev_addr[5] << 8));\r\niowrite16(addr16, np->base + StationAddr+4);\r\nreturn 0;\r\n}\r\nstatic int sundance_set_mac_addr(struct net_device *dev, void *data)\r\n{\r\nconst struct sockaddr *addr = data;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(dev->dev_addr, addr->sa_data, ETH_ALEN);\r\n__set_mac_addr(dev);\r\nreturn 0;\r\n}\r\nstatic int check_if_running(struct net_device *dev)\r\n{\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic void get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nstrlcpy(info->driver, DRV_NAME, sizeof(info->driver));\r\nstrlcpy(info->version, DRV_VERSION, sizeof(info->version));\r\nstrlcpy(info->bus_info, pci_name(np->pci_dev), sizeof(info->bus_info));\r\n}\r\nstatic int get_settings(struct net_device *dev, struct ethtool_cmd *ecmd)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nspin_lock_irq(&np->lock);\r\nmii_ethtool_gset(&np->mii_if, ecmd);\r\nspin_unlock_irq(&np->lock);\r\nreturn 0;\r\n}\r\nstatic int set_settings(struct net_device *dev, struct ethtool_cmd *ecmd)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint res;\r\nspin_lock_irq(&np->lock);\r\nres = mii_ethtool_sset(&np->mii_if, ecmd);\r\nspin_unlock_irq(&np->lock);\r\nreturn res;\r\n}\r\nstatic int nway_reset(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nreturn mii_nway_restart(&np->mii_if);\r\n}\r\nstatic u32 get_link(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nreturn mii_link_ok(&np->mii_if);\r\n}\r\nstatic u32 get_msglevel(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nreturn np->msg_enable;\r\n}\r\nstatic void set_msglevel(struct net_device *dev, u32 val)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nnp->msg_enable = val;\r\n}\r\nstatic void get_strings(struct net_device *dev, u32 stringset,\r\nu8 *data)\r\n{\r\nif (stringset == ETH_SS_STATS)\r\nmemcpy(data, sundance_stats, sizeof(sundance_stats));\r\n}\r\nstatic int get_sset_count(struct net_device *dev, int sset)\r\n{\r\nswitch (sset) {\r\ncase ETH_SS_STATS:\r\nreturn ARRAY_SIZE(sundance_stats);\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void get_ethtool_stats(struct net_device *dev,\r\nstruct ethtool_stats *stats, u64 *data)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint i = 0;\r\nget_stats(dev);\r\ndata[i++] = np->xstats.tx_multiple_collisions;\r\ndata[i++] = np->xstats.tx_single_collisions;\r\ndata[i++] = np->xstats.tx_late_collisions;\r\ndata[i++] = np->xstats.tx_deferred;\r\ndata[i++] = np->xstats.tx_deferred_excessive;\r\ndata[i++] = np->xstats.tx_aborted;\r\ndata[i++] = np->xstats.tx_bcasts;\r\ndata[i++] = np->xstats.rx_bcasts;\r\ndata[i++] = np->xstats.tx_mcasts;\r\ndata[i++] = np->xstats.rx_mcasts;\r\n}\r\nstatic void sundance_get_wol(struct net_device *dev,\r\nstruct ethtool_wolinfo *wol)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nu8 wol_bits;\r\nwol->wolopts = 0;\r\nwol->supported = (WAKE_PHY | WAKE_MAGIC);\r\nif (!np->wol_enabled)\r\nreturn;\r\nwol_bits = ioread8(ioaddr + WakeEvent);\r\nif (wol_bits & MagicPktEnable)\r\nwol->wolopts |= WAKE_MAGIC;\r\nif (wol_bits & LinkEventEnable)\r\nwol->wolopts |= WAKE_PHY;\r\n}\r\nstatic int sundance_set_wol(struct net_device *dev,\r\nstruct ethtool_wolinfo *wol)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nu8 wol_bits;\r\nif (!device_can_wakeup(&np->pci_dev->dev))\r\nreturn -EOPNOTSUPP;\r\nnp->wol_enabled = !!(wol->wolopts);\r\nwol_bits = ioread8(ioaddr + WakeEvent);\r\nwol_bits &= ~(WakePktEnable | MagicPktEnable |\r\nLinkEventEnable | WolEnable);\r\nif (np->wol_enabled) {\r\nif (wol->wolopts & WAKE_MAGIC)\r\nwol_bits |= (MagicPktEnable | WolEnable);\r\nif (wol->wolopts & WAKE_PHY)\r\nwol_bits |= (LinkEventEnable | WolEnable);\r\n}\r\niowrite8(wol_bits, ioaddr + WakeEvent);\r\ndevice_set_wakeup_enable(&np->pci_dev->dev, np->wol_enabled);\r\nreturn 0;\r\n}\r\nstatic int netdev_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint rc;\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nspin_lock_irq(&np->lock);\r\nrc = generic_mii_ioctl(&np->mii_if, if_mii(rq), cmd, NULL);\r\nspin_unlock_irq(&np->lock);\r\nreturn rc;\r\n}\r\nstatic int netdev_close(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nstruct sk_buff *skb;\r\nint i;\r\ntasklet_kill(&np->rx_tasklet);\r\ntasklet_kill(&np->tx_tasklet);\r\nnp->cur_tx = 0;\r\nnp->dirty_tx = 0;\r\nnp->cur_task = 0;\r\nnp->last_tx = NULL;\r\nnetif_stop_queue(dev);\r\nif (netif_msg_ifdown(np)) {\r\nprintk(KERN_DEBUG "%s: Shutting down ethercard, status was Tx %2.2x "\r\n"Rx %4.4x Int %2.2x.\n",\r\ndev->name, ioread8(ioaddr + TxStatus),\r\nioread32(ioaddr + RxStatus), ioread16(ioaddr + IntrStatus));\r\nprintk(KERN_DEBUG "%s: Queue pointers were Tx %d / %d, Rx %d / %d.\n",\r\ndev->name, np->cur_tx, np->dirty_tx, np->cur_rx, np->dirty_rx);\r\n}\r\niowrite16(0x0000, ioaddr + IntrEnable);\r\niowrite32(0x500, ioaddr + DMACtrl);\r\niowrite16(TxDisable | RxDisable | StatsDisable, ioaddr + MACCtrl1);\r\nfor (i = 2000; i > 0; i--) {\r\nif ((ioread32(ioaddr + DMACtrl) & 0xc000) == 0)\r\nbreak;\r\nmdelay(1);\r\n}\r\niowrite16(GlobalReset | DMAReset | FIFOReset | NetworkReset,\r\nioaddr + ASIC_HI_WORD(ASICCtrl));\r\nfor (i = 2000; i > 0; i--) {\r\nif ((ioread16(ioaddr + ASIC_HI_WORD(ASICCtrl)) & ResetBusy) == 0)\r\nbreak;\r\nmdelay(1);\r\n}\r\n#ifdef __i386__\r\nif (netif_msg_hw(np)) {\r\nprintk(KERN_DEBUG " Tx ring at %8.8x:\n",\r\n(int)(np->tx_ring_dma));\r\nfor (i = 0; i < TX_RING_SIZE; i++)\r\nprintk(KERN_DEBUG " #%d desc. %4.4x %8.8x %8.8x.\n",\r\ni, np->tx_ring[i].status, np->tx_ring[i].frag[0].addr,\r\nnp->tx_ring[i].frag[0].length);\r\nprintk(KERN_DEBUG " Rx ring %8.8x:\n",\r\n(int)(np->rx_ring_dma));\r\nfor (i = 0; i < 4 ; i++) {\r\nprintk(KERN_DEBUG " #%d desc. %4.4x %4.4x %8.8x\n",\r\ni, np->rx_ring[i].status, np->rx_ring[i].frag[0].addr,\r\nnp->rx_ring[i].frag[0].length);\r\n}\r\n}\r\n#endif\r\nfree_irq(np->pci_dev->irq, dev);\r\ndel_timer_sync(&np->timer);\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nnp->rx_ring[i].status = 0;\r\nskb = np->rx_skbuff[i];\r\nif (skb) {\r\ndma_unmap_single(&np->pci_dev->dev,\r\nle32_to_cpu(np->rx_ring[i].frag[0].addr),\r\nnp->rx_buf_sz, DMA_FROM_DEVICE);\r\ndev_kfree_skb(skb);\r\nnp->rx_skbuff[i] = NULL;\r\n}\r\nnp->rx_ring[i].frag[0].addr = cpu_to_le32(0xBADF00D0);\r\n}\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nnp->tx_ring[i].next_desc = 0;\r\nskb = np->tx_skbuff[i];\r\nif (skb) {\r\ndma_unmap_single(&np->pci_dev->dev,\r\nle32_to_cpu(np->tx_ring[i].frag[0].addr),\r\nskb->len, DMA_TO_DEVICE);\r\ndev_kfree_skb(skb);\r\nnp->tx_skbuff[i] = NULL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void sundance_remove1(struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nif (dev) {\r\nstruct netdev_private *np = netdev_priv(dev);\r\nunregister_netdev(dev);\r\ndma_free_coherent(&pdev->dev, RX_TOTAL_SIZE,\r\nnp->rx_ring, np->rx_ring_dma);\r\ndma_free_coherent(&pdev->dev, TX_TOTAL_SIZE,\r\nnp->tx_ring, np->tx_ring_dma);\r\npci_iounmap(pdev, np->base);\r\npci_release_regions(pdev);\r\nfree_netdev(dev);\r\n}\r\n}\r\nstatic int sundance_suspend(struct pci_dev *pci_dev, pm_message_t state)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pci_dev);\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nif (!netif_running(dev))\r\nreturn 0;\r\nnetdev_close(dev);\r\nnetif_device_detach(dev);\r\npci_save_state(pci_dev);\r\nif (np->wol_enabled) {\r\niowrite8(AcceptBroadcast | AcceptMyPhys, ioaddr + RxMode);\r\niowrite16(RxEnable, ioaddr + MACCtrl1);\r\n}\r\npci_enable_wake(pci_dev, pci_choose_state(pci_dev, state),\r\nnp->wol_enabled);\r\npci_set_power_state(pci_dev, pci_choose_state(pci_dev, state));\r\nreturn 0;\r\n}\r\nstatic int sundance_resume(struct pci_dev *pci_dev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pci_dev);\r\nint err = 0;\r\nif (!netif_running(dev))\r\nreturn 0;\r\npci_set_power_state(pci_dev, PCI_D0);\r\npci_restore_state(pci_dev);\r\npci_enable_wake(pci_dev, PCI_D0, 0);\r\nerr = netdev_open(dev);\r\nif (err) {\r\nprintk(KERN_ERR "%s: Can't resume interface!\n",\r\ndev->name);\r\ngoto out;\r\n}\r\nnetif_device_attach(dev);\r\nout:\r\nreturn err;\r\n}\r\nstatic int __init sundance_init(void)\r\n{\r\n#ifdef MODULE\r\nprintk(version);\r\n#endif\r\nreturn pci_register_driver(&sundance_driver);\r\n}\r\nstatic void __exit sundance_exit(void)\r\n{\r\npci_unregister_driver(&sundance_driver);\r\n}
