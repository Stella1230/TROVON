inline int __vringh_get_head(const struct vringh *vrh,\r\nint (*getu16)(const struct vringh *vrh,\r\nu16 *val, const __virtio16 *p),\r\nu16 *last_avail_idx)\r\n{\r\nu16 avail_idx, i, head;\r\nint err;\r\nerr = getu16(vrh, &avail_idx, &vrh->vring.avail->idx);\r\nif (err) {\r\nvringh_bad("Failed to access avail idx at %p",\r\n&vrh->vring.avail->idx);\r\nreturn err;\r\n}\r\nif (*last_avail_idx == avail_idx)\r\nreturn vrh->vring.num;\r\nvirtio_rmb(vrh->weak_barriers);\r\ni = *last_avail_idx & (vrh->vring.num - 1);\r\nerr = getu16(vrh, &head, &vrh->vring.avail->ring[i]);\r\nif (err) {\r\nvringh_bad("Failed to read head: idx %d address %p",\r\n*last_avail_idx, &vrh->vring.avail->ring[i]);\r\nreturn err;\r\n}\r\nif (head >= vrh->vring.num) {\r\nvringh_bad("Guest says index %u > %u is available",\r\nhead, vrh->vring.num);\r\nreturn -EINVAL;\r\n}\r\n(*last_avail_idx)++;\r\nreturn head;\r\n}\r\nstatic inline ssize_t vringh_iov_xfer(struct vringh_kiov *iov,\r\nvoid *ptr, size_t len,\r\nint (*xfer)(void *addr, void *ptr,\r\nsize_t len))\r\n{\r\nint err, done = 0;\r\nwhile (len && iov->i < iov->used) {\r\nsize_t partlen;\r\npartlen = min(iov->iov[iov->i].iov_len, len);\r\nerr = xfer(iov->iov[iov->i].iov_base, ptr, partlen);\r\nif (err)\r\nreturn err;\r\ndone += partlen;\r\nlen -= partlen;\r\nptr += partlen;\r\niov->consumed += partlen;\r\niov->iov[iov->i].iov_len -= partlen;\r\niov->iov[iov->i].iov_base += partlen;\r\nif (!iov->iov[iov->i].iov_len) {\r\niov->iov[iov->i].iov_len = iov->consumed;\r\niov->iov[iov->i].iov_base -= iov->consumed;\r\niov->consumed = 0;\r\niov->i++;\r\n}\r\n}\r\nreturn done;\r\n}\r\nstatic inline bool range_check(struct vringh *vrh, u64 addr, size_t *len,\r\nstruct vringh_range *range,\r\nbool (*getrange)(struct vringh *,\r\nu64, struct vringh_range *))\r\n{\r\nif (addr < range->start || addr > range->end_incl) {\r\nif (!getrange(vrh, addr, range))\r\nreturn false;\r\n}\r\nBUG_ON(addr < range->start || addr > range->end_incl);\r\nif (unlikely(addr + *len == 0)) {\r\nif (range->end_incl == -1ULL)\r\nreturn true;\r\ngoto truncate;\r\n}\r\nif (addr + *len < addr) {\r\nvringh_bad("Wrapping descriptor %zu@0x%llx",\r\n*len, (unsigned long long)addr);\r\nreturn false;\r\n}\r\nif (unlikely(addr + *len - 1 > range->end_incl))\r\ngoto truncate;\r\nreturn true;\r\ntruncate:\r\n*len = range->end_incl + 1 - addr;\r\nreturn true;\r\n}\r\nstatic inline bool no_range_check(struct vringh *vrh, u64 addr, size_t *len,\r\nstruct vringh_range *range,\r\nbool (*getrange)(struct vringh *,\r\nu64, struct vringh_range *))\r\n{\r\nreturn true;\r\n}\r\nstatic int move_to_indirect(const struct vringh *vrh,\r\nint *up_next, u16 *i, void *addr,\r\nconst struct vring_desc *desc,\r\nstruct vring_desc **descs, int *desc_max)\r\n{\r\nu32 len;\r\nif (*up_next != -1) {\r\nvringh_bad("Multilevel indirect %u->%u", *up_next, *i);\r\nreturn -EINVAL;\r\n}\r\nlen = vringh32_to_cpu(vrh, desc->len);\r\nif (unlikely(len % sizeof(struct vring_desc))) {\r\nvringh_bad("Strange indirect len %u", desc->len);\r\nreturn -EINVAL;\r\n}\r\nif (desc->flags & cpu_to_vringh16(vrh, VRING_DESC_F_NEXT))\r\n*up_next = vringh16_to_cpu(vrh, desc->next);\r\nelse\r\n*up_next = -2;\r\n*descs = addr;\r\n*desc_max = len / sizeof(struct vring_desc);\r\n*i = 0;\r\nreturn 0;\r\n}\r\nstatic int resize_iovec(struct vringh_kiov *iov, gfp_t gfp)\r\n{\r\nstruct kvec *new;\r\nunsigned int flag, new_num = (iov->max_num & ~VRINGH_IOV_ALLOCATED) * 2;\r\nif (new_num < 8)\r\nnew_num = 8;\r\nflag = (iov->max_num & VRINGH_IOV_ALLOCATED);\r\nif (flag)\r\nnew = krealloc(iov->iov, new_num * sizeof(struct iovec), gfp);\r\nelse {\r\nnew = kmalloc(new_num * sizeof(struct iovec), gfp);\r\nif (new) {\r\nmemcpy(new, iov->iov,\r\niov->max_num * sizeof(struct iovec));\r\nflag = VRINGH_IOV_ALLOCATED;\r\n}\r\n}\r\nif (!new)\r\nreturn -ENOMEM;\r\niov->iov = new;\r\niov->max_num = (new_num | flag);\r\nreturn 0;\r\n}\r\nstatic u16 __cold return_from_indirect(const struct vringh *vrh, int *up_next,\r\nstruct vring_desc **descs, int *desc_max)\r\n{\r\nu16 i = *up_next;\r\n*up_next = -1;\r\n*descs = vrh->vring.desc;\r\n*desc_max = vrh->vring.num;\r\nreturn i;\r\n}\r\nstatic int slow_copy(struct vringh *vrh, void *dst, const void *src,\r\nbool (*rcheck)(struct vringh *vrh, u64 addr, size_t *len,\r\nstruct vringh_range *range,\r\nbool (*getrange)(struct vringh *vrh,\r\nu64,\r\nstruct vringh_range *)),\r\nbool (*getrange)(struct vringh *vrh,\r\nu64 addr,\r\nstruct vringh_range *r),\r\nstruct vringh_range *range,\r\nint (*copy)(void *dst, const void *src, size_t len))\r\n{\r\nsize_t part, len = sizeof(struct vring_desc);\r\ndo {\r\nu64 addr;\r\nint err;\r\npart = len;\r\naddr = (u64)(unsigned long)src - range->offset;\r\nif (!rcheck(vrh, addr, &part, range, getrange))\r\nreturn -EINVAL;\r\nerr = copy(dst, src, part);\r\nif (err)\r\nreturn err;\r\ndst += part;\r\nsrc += part;\r\nlen -= part;\r\n} while (len);\r\nreturn 0;\r\n}\r\nstatic inline int\r\n__vringh_iov(struct vringh *vrh, u16 i,\r\nstruct vringh_kiov *riov,\r\nstruct vringh_kiov *wiov,\r\nbool (*rcheck)(struct vringh *vrh, u64 addr, size_t *len,\r\nstruct vringh_range *range,\r\nbool (*getrange)(struct vringh *, u64,\r\nstruct vringh_range *)),\r\nbool (*getrange)(struct vringh *, u64, struct vringh_range *),\r\ngfp_t gfp,\r\nint (*copy)(void *dst, const void *src, size_t len))\r\n{\r\nint err, count = 0, up_next, desc_max;\r\nstruct vring_desc desc, *descs;\r\nstruct vringh_range range = { -1ULL, 0 }, slowrange;\r\nbool slow = false;\r\ndescs = vrh->vring.desc;\r\ndesc_max = vrh->vring.num;\r\nup_next = -1;\r\nif (riov)\r\nriov->i = riov->used = 0;\r\nelse if (wiov)\r\nwiov->i = wiov->used = 0;\r\nelse\r\nBUG();\r\nfor (;;) {\r\nvoid *addr;\r\nstruct vringh_kiov *iov;\r\nsize_t len;\r\nif (unlikely(slow))\r\nerr = slow_copy(vrh, &desc, &descs[i], rcheck, getrange,\r\n&slowrange, copy);\r\nelse\r\nerr = copy(&desc, &descs[i], sizeof(desc));\r\nif (unlikely(err))\r\ngoto fail;\r\nif (unlikely(desc.flags &\r\ncpu_to_vringh16(vrh, VRING_DESC_F_INDIRECT))) {\r\nu64 a = vringh64_to_cpu(vrh, desc.addr);\r\nlen = vringh32_to_cpu(vrh, desc.len);\r\nif (!rcheck(vrh, a, &len, &range, getrange)) {\r\nerr = -EINVAL;\r\ngoto fail;\r\n}\r\nif (unlikely(len != vringh32_to_cpu(vrh, desc.len))) {\r\nslow = true;\r\nslowrange = range;\r\n}\r\naddr = (void *)(long)(a + range.offset);\r\nerr = move_to_indirect(vrh, &up_next, &i, addr, &desc,\r\n&descs, &desc_max);\r\nif (err)\r\ngoto fail;\r\ncontinue;\r\n}\r\nif (count++ == vrh->vring.num) {\r\nvringh_bad("Descriptor loop in %p", descs);\r\nerr = -ELOOP;\r\ngoto fail;\r\n}\r\nif (desc.flags & cpu_to_vringh16(vrh, VRING_DESC_F_WRITE))\r\niov = wiov;\r\nelse {\r\niov = riov;\r\nif (unlikely(wiov && wiov->i)) {\r\nvringh_bad("Readable desc %p after writable",\r\n&descs[i]);\r\nerr = -EINVAL;\r\ngoto fail;\r\n}\r\n}\r\nif (!iov) {\r\nvringh_bad("Unexpected %s desc",\r\n!wiov ? "writable" : "readable");\r\nerr = -EPROTO;\r\ngoto fail;\r\n}\r\nagain:\r\nlen = vringh32_to_cpu(vrh, desc.len);\r\nif (!rcheck(vrh, vringh64_to_cpu(vrh, desc.addr), &len, &range,\r\ngetrange)) {\r\nerr = -EINVAL;\r\ngoto fail;\r\n}\r\naddr = (void *)(unsigned long)(vringh64_to_cpu(vrh, desc.addr) +\r\nrange.offset);\r\nif (unlikely(iov->used == (iov->max_num & ~VRINGH_IOV_ALLOCATED))) {\r\nerr = resize_iovec(iov, gfp);\r\nif (err)\r\ngoto fail;\r\n}\r\niov->iov[iov->used].iov_base = addr;\r\niov->iov[iov->used].iov_len = len;\r\niov->used++;\r\nif (unlikely(len != vringh32_to_cpu(vrh, desc.len))) {\r\ndesc.len = cpu_to_vringh32(vrh,\r\nvringh32_to_cpu(vrh, desc.len) - len);\r\ndesc.addr = cpu_to_vringh64(vrh,\r\nvringh64_to_cpu(vrh, desc.addr) + len);\r\ngoto again;\r\n}\r\nif (desc.flags & cpu_to_vringh16(vrh, VRING_DESC_F_NEXT)) {\r\ni = vringh16_to_cpu(vrh, desc.next);\r\n} else {\r\nif (unlikely(up_next > 0)) {\r\ni = return_from_indirect(vrh, &up_next,\r\n&descs, &desc_max);\r\nslow = false;\r\n} else\r\nbreak;\r\n}\r\nif (i >= desc_max) {\r\nvringh_bad("Chained index %u > %u", i, desc_max);\r\nerr = -EINVAL;\r\ngoto fail;\r\n}\r\n}\r\nreturn 0;\r\nfail:\r\nreturn err;\r\n}\r\nstatic inline int __vringh_complete(struct vringh *vrh,\r\nconst struct vring_used_elem *used,\r\nunsigned int num_used,\r\nint (*putu16)(const struct vringh *vrh,\r\n__virtio16 *p, u16 val),\r\nint (*putused)(struct vring_used_elem *dst,\r\nconst struct vring_used_elem\r\n*src, unsigned num))\r\n{\r\nstruct vring_used *used_ring;\r\nint err;\r\nu16 used_idx, off;\r\nused_ring = vrh->vring.used;\r\nused_idx = vrh->last_used_idx + vrh->completed;\r\noff = used_idx % vrh->vring.num;\r\nif (num_used > 1 && unlikely(off + num_used >= vrh->vring.num)) {\r\nu16 part = vrh->vring.num - off;\r\nerr = putused(&used_ring->ring[off], used, part);\r\nif (!err)\r\nerr = putused(&used_ring->ring[0], used + part,\r\nnum_used - part);\r\n} else\r\nerr = putused(&used_ring->ring[off], used, num_used);\r\nif (err) {\r\nvringh_bad("Failed to write %u used entries %u at %p",\r\nnum_used, off, &used_ring->ring[off]);\r\nreturn err;\r\n}\r\nvirtio_wmb(vrh->weak_barriers);\r\nerr = putu16(vrh, &vrh->vring.used->idx, used_idx + num_used);\r\nif (err) {\r\nvringh_bad("Failed to update used index at %p",\r\n&vrh->vring.used->idx);\r\nreturn err;\r\n}\r\nvrh->completed += num_used;\r\nreturn 0;\r\n}\r\nstatic inline int __vringh_need_notify(struct vringh *vrh,\r\nint (*getu16)(const struct vringh *vrh,\r\nu16 *val,\r\nconst __virtio16 *p))\r\n{\r\nbool notify;\r\nu16 used_event;\r\nint err;\r\nvirtio_mb(vrh->weak_barriers);\r\nif (!vrh->event_indices) {\r\nu16 flags;\r\nerr = getu16(vrh, &flags, &vrh->vring.avail->flags);\r\nif (err) {\r\nvringh_bad("Failed to get flags at %p",\r\n&vrh->vring.avail->flags);\r\nreturn err;\r\n}\r\nreturn (!(flags & VRING_AVAIL_F_NO_INTERRUPT));\r\n}\r\nerr = getu16(vrh, &used_event, &vring_used_event(&vrh->vring));\r\nif (err) {\r\nvringh_bad("Failed to get used event idx at %p",\r\n&vring_used_event(&vrh->vring));\r\nreturn err;\r\n}\r\nif (unlikely(vrh->completed > 0xffff))\r\nnotify = true;\r\nelse\r\nnotify = vring_need_event(used_event,\r\nvrh->last_used_idx + vrh->completed,\r\nvrh->last_used_idx);\r\nvrh->last_used_idx += vrh->completed;\r\nvrh->completed = 0;\r\nreturn notify;\r\n}\r\nstatic inline bool __vringh_notify_enable(struct vringh *vrh,\r\nint (*getu16)(const struct vringh *vrh,\r\nu16 *val, const __virtio16 *p),\r\nint (*putu16)(const struct vringh *vrh,\r\n__virtio16 *p, u16 val))\r\n{\r\nu16 avail;\r\nif (!vrh->event_indices) {\r\nif (putu16(vrh, &vrh->vring.used->flags, 0) != 0) {\r\nvringh_bad("Clearing used flags %p",\r\n&vrh->vring.used->flags);\r\nreturn true;\r\n}\r\n} else {\r\nif (putu16(vrh, &vring_avail_event(&vrh->vring),\r\nvrh->last_avail_idx) != 0) {\r\nvringh_bad("Updating avail event index %p",\r\n&vring_avail_event(&vrh->vring));\r\nreturn true;\r\n}\r\n}\r\nvirtio_mb(vrh->weak_barriers);\r\nif (getu16(vrh, &avail, &vrh->vring.avail->idx) != 0) {\r\nvringh_bad("Failed to check avail idx at %p",\r\n&vrh->vring.avail->idx);\r\nreturn true;\r\n}\r\nreturn avail == vrh->last_avail_idx;\r\n}\r\nstatic inline void __vringh_notify_disable(struct vringh *vrh,\r\nint (*putu16)(const struct vringh *vrh,\r\n__virtio16 *p, u16 val))\r\n{\r\nif (!vrh->event_indices) {\r\nif (putu16(vrh, &vrh->vring.used->flags,\r\nVRING_USED_F_NO_NOTIFY)) {\r\nvringh_bad("Setting used flags %p",\r\n&vrh->vring.used->flags);\r\n}\r\n}\r\n}\r\nstatic inline int getu16_user(const struct vringh *vrh, u16 *val, const __virtio16 *p)\r\n{\r\n__virtio16 v = 0;\r\nint rc = get_user(v, (__force __virtio16 __user *)p);\r\n*val = vringh16_to_cpu(vrh, v);\r\nreturn rc;\r\n}\r\nstatic inline int putu16_user(const struct vringh *vrh, __virtio16 *p, u16 val)\r\n{\r\n__virtio16 v = cpu_to_vringh16(vrh, val);\r\nreturn put_user(v, (__force __virtio16 __user *)p);\r\n}\r\nstatic inline int copydesc_user(void *dst, const void *src, size_t len)\r\n{\r\nreturn copy_from_user(dst, (__force void __user *)src, len) ?\r\n-EFAULT : 0;\r\n}\r\nstatic inline int putused_user(struct vring_used_elem *dst,\r\nconst struct vring_used_elem *src,\r\nunsigned int num)\r\n{\r\nreturn copy_to_user((__force void __user *)dst, src,\r\nsizeof(*dst) * num) ? -EFAULT : 0;\r\n}\r\nstatic inline int xfer_from_user(void *src, void *dst, size_t len)\r\n{\r\nreturn copy_from_user(dst, (__force void __user *)src, len) ?\r\n-EFAULT : 0;\r\n}\r\nstatic inline int xfer_to_user(void *dst, void *src, size_t len)\r\n{\r\nreturn copy_to_user((__force void __user *)dst, src, len) ?\r\n-EFAULT : 0;\r\n}\r\nint vringh_init_user(struct vringh *vrh, u64 features,\r\nunsigned int num, bool weak_barriers,\r\nstruct vring_desc __user *desc,\r\nstruct vring_avail __user *avail,\r\nstruct vring_used __user *used)\r\n{\r\nif (!num || num > 0xffff || (num & (num - 1))) {\r\nvringh_bad("Bad ring size %u", num);\r\nreturn -EINVAL;\r\n}\r\nvrh->little_endian = (features & (1ULL << VIRTIO_F_VERSION_1));\r\nvrh->event_indices = (features & (1 << VIRTIO_RING_F_EVENT_IDX));\r\nvrh->weak_barriers = weak_barriers;\r\nvrh->completed = 0;\r\nvrh->last_avail_idx = 0;\r\nvrh->last_used_idx = 0;\r\nvrh->vring.num = num;\r\nvrh->vring.desc = (__force struct vring_desc *)desc;\r\nvrh->vring.avail = (__force struct vring_avail *)avail;\r\nvrh->vring.used = (__force struct vring_used *)used;\r\nreturn 0;\r\n}\r\nint vringh_getdesc_user(struct vringh *vrh,\r\nstruct vringh_iov *riov,\r\nstruct vringh_iov *wiov,\r\nbool (*getrange)(struct vringh *vrh,\r\nu64 addr, struct vringh_range *r),\r\nu16 *head)\r\n{\r\nint err;\r\n*head = vrh->vring.num;\r\nerr = __vringh_get_head(vrh, getu16_user, &vrh->last_avail_idx);\r\nif (err < 0)\r\nreturn err;\r\nif (err == vrh->vring.num)\r\nreturn 0;\r\nBUILD_BUG_ON(sizeof(struct vringh_kiov) != sizeof(struct vringh_iov));\r\nBUILD_BUG_ON(offsetof(struct vringh_kiov, iov) !=\r\noffsetof(struct vringh_iov, iov));\r\nBUILD_BUG_ON(offsetof(struct vringh_kiov, i) !=\r\noffsetof(struct vringh_iov, i));\r\nBUILD_BUG_ON(offsetof(struct vringh_kiov, used) !=\r\noffsetof(struct vringh_iov, used));\r\nBUILD_BUG_ON(offsetof(struct vringh_kiov, max_num) !=\r\noffsetof(struct vringh_iov, max_num));\r\nBUILD_BUG_ON(sizeof(struct iovec) != sizeof(struct kvec));\r\nBUILD_BUG_ON(offsetof(struct iovec, iov_base) !=\r\noffsetof(struct kvec, iov_base));\r\nBUILD_BUG_ON(offsetof(struct iovec, iov_len) !=\r\noffsetof(struct kvec, iov_len));\r\nBUILD_BUG_ON(sizeof(((struct iovec *)NULL)->iov_base)\r\n!= sizeof(((struct kvec *)NULL)->iov_base));\r\nBUILD_BUG_ON(sizeof(((struct iovec *)NULL)->iov_len)\r\n!= sizeof(((struct kvec *)NULL)->iov_len));\r\n*head = err;\r\nerr = __vringh_iov(vrh, *head, (struct vringh_kiov *)riov,\r\n(struct vringh_kiov *)wiov,\r\nrange_check, getrange, GFP_KERNEL, copydesc_user);\r\nif (err)\r\nreturn err;\r\nreturn 1;\r\n}\r\nssize_t vringh_iov_pull_user(struct vringh_iov *riov, void *dst, size_t len)\r\n{\r\nreturn vringh_iov_xfer((struct vringh_kiov *)riov,\r\ndst, len, xfer_from_user);\r\n}\r\nssize_t vringh_iov_push_user(struct vringh_iov *wiov,\r\nconst void *src, size_t len)\r\n{\r\nreturn vringh_iov_xfer((struct vringh_kiov *)wiov,\r\n(void *)src, len, xfer_to_user);\r\n}\r\nvoid vringh_abandon_user(struct vringh *vrh, unsigned int num)\r\n{\r\nvrh->last_avail_idx -= num;\r\n}\r\nint vringh_complete_user(struct vringh *vrh, u16 head, u32 len)\r\n{\r\nstruct vring_used_elem used;\r\nused.id = cpu_to_vringh32(vrh, head);\r\nused.len = cpu_to_vringh32(vrh, len);\r\nreturn __vringh_complete(vrh, &used, 1, putu16_user, putused_user);\r\n}\r\nint vringh_complete_multi_user(struct vringh *vrh,\r\nconst struct vring_used_elem used[],\r\nunsigned num_used)\r\n{\r\nreturn __vringh_complete(vrh, used, num_used,\r\nputu16_user, putused_user);\r\n}\r\nbool vringh_notify_enable_user(struct vringh *vrh)\r\n{\r\nreturn __vringh_notify_enable(vrh, getu16_user, putu16_user);\r\n}\r\nvoid vringh_notify_disable_user(struct vringh *vrh)\r\n{\r\n__vringh_notify_disable(vrh, putu16_user);\r\n}\r\nint vringh_need_notify_user(struct vringh *vrh)\r\n{\r\nreturn __vringh_need_notify(vrh, getu16_user);\r\n}\r\nstatic inline int getu16_kern(const struct vringh *vrh,\r\nu16 *val, const __virtio16 *p)\r\n{\r\n*val = vringh16_to_cpu(vrh, ACCESS_ONCE(*p));\r\nreturn 0;\r\n}\r\nstatic inline int putu16_kern(const struct vringh *vrh, __virtio16 *p, u16 val)\r\n{\r\nACCESS_ONCE(*p) = cpu_to_vringh16(vrh, val);\r\nreturn 0;\r\n}\r\nstatic inline int copydesc_kern(void *dst, const void *src, size_t len)\r\n{\r\nmemcpy(dst, src, len);\r\nreturn 0;\r\n}\r\nstatic inline int putused_kern(struct vring_used_elem *dst,\r\nconst struct vring_used_elem *src,\r\nunsigned int num)\r\n{\r\nmemcpy(dst, src, num * sizeof(*dst));\r\nreturn 0;\r\n}\r\nstatic inline int xfer_kern(void *src, void *dst, size_t len)\r\n{\r\nmemcpy(dst, src, len);\r\nreturn 0;\r\n}\r\nint vringh_init_kern(struct vringh *vrh, u64 features,\r\nunsigned int num, bool weak_barriers,\r\nstruct vring_desc *desc,\r\nstruct vring_avail *avail,\r\nstruct vring_used *used)\r\n{\r\nif (!num || num > 0xffff || (num & (num - 1))) {\r\nvringh_bad("Bad ring size %u", num);\r\nreturn -EINVAL;\r\n}\r\nvrh->little_endian = (features & (1ULL << VIRTIO_F_VERSION_1));\r\nvrh->event_indices = (features & (1 << VIRTIO_RING_F_EVENT_IDX));\r\nvrh->weak_barriers = weak_barriers;\r\nvrh->completed = 0;\r\nvrh->last_avail_idx = 0;\r\nvrh->last_used_idx = 0;\r\nvrh->vring.num = num;\r\nvrh->vring.desc = desc;\r\nvrh->vring.avail = avail;\r\nvrh->vring.used = used;\r\nreturn 0;\r\n}\r\nint vringh_getdesc_kern(struct vringh *vrh,\r\nstruct vringh_kiov *riov,\r\nstruct vringh_kiov *wiov,\r\nu16 *head,\r\ngfp_t gfp)\r\n{\r\nint err;\r\nerr = __vringh_get_head(vrh, getu16_kern, &vrh->last_avail_idx);\r\nif (err < 0)\r\nreturn err;\r\nif (err == vrh->vring.num)\r\nreturn 0;\r\n*head = err;\r\nerr = __vringh_iov(vrh, *head, riov, wiov, no_range_check, NULL,\r\ngfp, copydesc_kern);\r\nif (err)\r\nreturn err;\r\nreturn 1;\r\n}\r\nssize_t vringh_iov_pull_kern(struct vringh_kiov *riov, void *dst, size_t len)\r\n{\r\nreturn vringh_iov_xfer(riov, dst, len, xfer_kern);\r\n}\r\nssize_t vringh_iov_push_kern(struct vringh_kiov *wiov,\r\nconst void *src, size_t len)\r\n{\r\nreturn vringh_iov_xfer(wiov, (void *)src, len, xfer_kern);\r\n}\r\nvoid vringh_abandon_kern(struct vringh *vrh, unsigned int num)\r\n{\r\nvrh->last_avail_idx -= num;\r\n}\r\nint vringh_complete_kern(struct vringh *vrh, u16 head, u32 len)\r\n{\r\nstruct vring_used_elem used;\r\nused.id = cpu_to_vringh32(vrh, head);\r\nused.len = cpu_to_vringh32(vrh, len);\r\nreturn __vringh_complete(vrh, &used, 1, putu16_kern, putused_kern);\r\n}\r\nbool vringh_notify_enable_kern(struct vringh *vrh)\r\n{\r\nreturn __vringh_notify_enable(vrh, getu16_kern, putu16_kern);\r\n}\r\nvoid vringh_notify_disable_kern(struct vringh *vrh)\r\n{\r\n__vringh_notify_disable(vrh, putu16_kern);\r\n}\r\nint vringh_need_notify_kern(struct vringh *vrh)\r\n{\r\nreturn __vringh_need_notify(vrh, getu16_kern);\r\n}
