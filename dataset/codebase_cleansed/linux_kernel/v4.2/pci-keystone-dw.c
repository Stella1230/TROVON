static inline struct pcie_port *sys_to_pcie(struct pci_sys_data *sys)\r\n{\r\nreturn sys->private_data;\r\n}\r\nstatic inline void update_reg_offset_bit_pos(u32 offset, u32 *reg_offset,\r\nu32 *bit_pos)\r\n{\r\n*reg_offset = offset % 8;\r\n*bit_pos = offset >> 3;\r\n}\r\nu32 ks_dw_pcie_get_msi_addr(struct pcie_port *pp)\r\n{\r\nstruct keystone_pcie *ks_pcie = to_keystone_pcie(pp);\r\nreturn ks_pcie->app.start + MSI_IRQ;\r\n}\r\nvoid ks_dw_pcie_handle_msi_irq(struct keystone_pcie *ks_pcie, int offset)\r\n{\r\nstruct pcie_port *pp = &ks_pcie->pp;\r\nu32 pending, vector;\r\nint src, virq;\r\npending = readl(ks_pcie->va_app_base + MSI0_IRQ_STATUS + (offset << 4));\r\nfor (src = 0; src < 4; src++) {\r\nif (BIT(src) & pending) {\r\nvector = offset + (src << 3);\r\nvirq = irq_linear_revmap(pp->irq_domain, vector);\r\ndev_dbg(pp->dev, "irq: bit %d, vector %d, virq %d\n",\r\nsrc, vector, virq);\r\ngeneric_handle_irq(virq);\r\n}\r\n}\r\n}\r\nstatic void ks_dw_pcie_msi_irq_ack(struct irq_data *d)\r\n{\r\nu32 offset, reg_offset, bit_pos;\r\nstruct keystone_pcie *ks_pcie;\r\nunsigned int irq = d->irq;\r\nstruct msi_desc *msi;\r\nstruct pcie_port *pp;\r\nmsi = irq_get_msi_desc(irq);\r\npp = sys_to_pcie(msi->dev->bus->sysdata);\r\nks_pcie = to_keystone_pcie(pp);\r\noffset = irq - irq_linear_revmap(pp->irq_domain, 0);\r\nupdate_reg_offset_bit_pos(offset, &reg_offset, &bit_pos);\r\nwritel(BIT(bit_pos),\r\nks_pcie->va_app_base + MSI0_IRQ_STATUS + (reg_offset << 4));\r\nwritel(reg_offset + MSI_IRQ_OFFSET, ks_pcie->va_app_base + IRQ_EOI);\r\n}\r\nvoid ks_dw_pcie_msi_set_irq(struct pcie_port *pp, int irq)\r\n{\r\nu32 reg_offset, bit_pos;\r\nstruct keystone_pcie *ks_pcie = to_keystone_pcie(pp);\r\nupdate_reg_offset_bit_pos(irq, &reg_offset, &bit_pos);\r\nwritel(BIT(bit_pos),\r\nks_pcie->va_app_base + MSI0_IRQ_ENABLE_SET + (reg_offset << 4));\r\n}\r\nvoid ks_dw_pcie_msi_clear_irq(struct pcie_port *pp, int irq)\r\n{\r\nu32 reg_offset, bit_pos;\r\nstruct keystone_pcie *ks_pcie = to_keystone_pcie(pp);\r\nupdate_reg_offset_bit_pos(irq, &reg_offset, &bit_pos);\r\nwritel(BIT(bit_pos),\r\nks_pcie->va_app_base + MSI0_IRQ_ENABLE_CLR + (reg_offset << 4));\r\n}\r\nstatic void ks_dw_pcie_msi_irq_mask(struct irq_data *d)\r\n{\r\nstruct keystone_pcie *ks_pcie;\r\nunsigned int irq = d->irq;\r\nstruct msi_desc *msi;\r\nstruct pcie_port *pp;\r\nu32 offset;\r\nmsi = irq_get_msi_desc(irq);\r\npp = sys_to_pcie(msi->dev->bus->sysdata);\r\nks_pcie = to_keystone_pcie(pp);\r\noffset = irq - irq_linear_revmap(pp->irq_domain, 0);\r\nif (IS_ENABLED(CONFIG_PCI_MSI)) {\r\nif (msi->msi_attrib.maskbit)\r\npci_msi_mask_irq(d);\r\n}\r\nks_dw_pcie_msi_clear_irq(pp, offset);\r\n}\r\nstatic void ks_dw_pcie_msi_irq_unmask(struct irq_data *d)\r\n{\r\nstruct keystone_pcie *ks_pcie;\r\nunsigned int irq = d->irq;\r\nstruct msi_desc *msi;\r\nstruct pcie_port *pp;\r\nu32 offset;\r\nmsi = irq_get_msi_desc(irq);\r\npp = sys_to_pcie(msi->dev->bus->sysdata);\r\nks_pcie = to_keystone_pcie(pp);\r\noffset = irq - irq_linear_revmap(pp->irq_domain, 0);\r\nif (IS_ENABLED(CONFIG_PCI_MSI)) {\r\nif (msi->msi_attrib.maskbit)\r\npci_msi_unmask_irq(d);\r\n}\r\nks_dw_pcie_msi_set_irq(pp, offset);\r\n}\r\nstatic int ks_dw_pcie_msi_map(struct irq_domain *domain, unsigned int irq,\r\nirq_hw_number_t hwirq)\r\n{\r\nirq_set_chip_and_handler(irq, &ks_dw_pcie_msi_irq_chip,\r\nhandle_level_irq);\r\nirq_set_chip_data(irq, domain->host_data);\r\nset_irq_flags(irq, IRQF_VALID);\r\nreturn 0;\r\n}\r\nint ks_dw_pcie_msi_host_init(struct pcie_port *pp, struct msi_controller *chip)\r\n{\r\nstruct keystone_pcie *ks_pcie = to_keystone_pcie(pp);\r\nint i;\r\npp->irq_domain = irq_domain_add_linear(ks_pcie->msi_intc_np,\r\nMAX_MSI_IRQS,\r\n&ks_dw_pcie_msi_domain_ops,\r\nchip);\r\nif (!pp->irq_domain) {\r\ndev_err(pp->dev, "irq domain init failed\n");\r\nreturn -ENXIO;\r\n}\r\nfor (i = 0; i < MAX_MSI_IRQS; i++)\r\nirq_create_mapping(pp->irq_domain, i);\r\nreturn 0;\r\n}\r\nvoid ks_dw_pcie_enable_legacy_irqs(struct keystone_pcie *ks_pcie)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_LEGACY_IRQS; i++)\r\nwritel(0x1, ks_pcie->va_app_base + IRQ_ENABLE_SET + (i << 4));\r\n}\r\nvoid ks_dw_pcie_handle_legacy_irq(struct keystone_pcie *ks_pcie, int offset)\r\n{\r\nstruct pcie_port *pp = &ks_pcie->pp;\r\nu32 pending;\r\nint virq;\r\npending = readl(ks_pcie->va_app_base + IRQ_STATUS + (offset << 4));\r\nif (BIT(0) & pending) {\r\nvirq = irq_linear_revmap(ks_pcie->legacy_irq_domain, offset);\r\ndev_dbg(pp->dev, ": irq: irq_offset %d, virq %d\n", offset,\r\nvirq);\r\ngeneric_handle_irq(virq);\r\n}\r\nwritel(offset, ks_pcie->va_app_base + IRQ_EOI);\r\n}\r\nstatic void ks_dw_pcie_ack_legacy_irq(struct irq_data *d)\r\n{\r\n}\r\nstatic void ks_dw_pcie_mask_legacy_irq(struct irq_data *d)\r\n{\r\n}\r\nstatic void ks_dw_pcie_unmask_legacy_irq(struct irq_data *d)\r\n{\r\n}\r\nstatic int ks_dw_pcie_init_legacy_irq_map(struct irq_domain *d,\r\nunsigned int irq, irq_hw_number_t hw_irq)\r\n{\r\nirq_set_chip_and_handler(irq, &ks_dw_pcie_legacy_irq_chip,\r\nhandle_level_irq);\r\nirq_set_chip_data(irq, d->host_data);\r\nset_irq_flags(irq, IRQF_VALID);\r\nreturn 0;\r\n}\r\nstatic void ks_dw_pcie_set_dbi_mode(void __iomem *reg_virt)\r\n{\r\nu32 val;\r\nwritel(DBI_CS2_EN_VAL | readl(reg_virt + CMD_STATUS),\r\nreg_virt + CMD_STATUS);\r\ndo {\r\nval = readl(reg_virt + CMD_STATUS);\r\n} while (!(val & DBI_CS2_EN_VAL));\r\n}\r\nstatic void ks_dw_pcie_clear_dbi_mode(void __iomem *reg_virt)\r\n{\r\nu32 val;\r\nwritel(~DBI_CS2_EN_VAL & readl(reg_virt + CMD_STATUS),\r\nreg_virt + CMD_STATUS);\r\ndo {\r\nval = readl(reg_virt + CMD_STATUS);\r\n} while (val & DBI_CS2_EN_VAL);\r\n}\r\nvoid ks_dw_pcie_setup_rc_app_regs(struct keystone_pcie *ks_pcie)\r\n{\r\nstruct pcie_port *pp = &ks_pcie->pp;\r\nu32 start = pp->mem.start, end = pp->mem.end;\r\nint i, tr_size;\r\nks_dw_pcie_set_dbi_mode(ks_pcie->va_app_base);\r\nwritel(0, pp->dbi_base + PCI_BASE_ADDRESS_0);\r\nwritel(0, pp->dbi_base + PCI_BASE_ADDRESS_1);\r\nks_dw_pcie_clear_dbi_mode(ks_pcie->va_app_base);\r\nwritel(CFG_PCIM_WIN_SZ_IDX & 0x7, ks_pcie->va_app_base + OB_SIZE);\r\ntr_size = (1 << (CFG_PCIM_WIN_SZ_IDX & 0x7)) * SZ_1M;\r\nfor (i = 0; (i < CFG_PCIM_WIN_CNT) && (start < end); i++) {\r\nwritel(start | 1, ks_pcie->va_app_base + OB_OFFSET_INDEX(i));\r\nwritel(0, ks_pcie->va_app_base + OB_OFFSET_HI(i));\r\nstart += tr_size;\r\n}\r\nwritel(OB_XLAT_EN_VAL | readl(ks_pcie->va_app_base + CMD_STATUS),\r\nks_pcie->va_app_base + CMD_STATUS);\r\n}\r\nstatic void __iomem *ks_pcie_cfg_setup(struct keystone_pcie *ks_pcie, u8 bus,\r\nunsigned int devfn)\r\n{\r\nu8 device = PCI_SLOT(devfn), function = PCI_FUNC(devfn);\r\nstruct pcie_port *pp = &ks_pcie->pp;\r\nu32 regval;\r\nif (bus == 0)\r\nreturn pp->dbi_base;\r\nregval = (bus << 16) | (device << 8) | function;\r\nif (bus != 1)\r\nregval |= BIT(24);\r\nwritel(regval, ks_pcie->va_app_base + CFG_SETUP);\r\nreturn pp->va_cfg0_base;\r\n}\r\nint ks_dw_pcie_rd_other_conf(struct pcie_port *pp, struct pci_bus *bus,\r\nunsigned int devfn, int where, int size, u32 *val)\r\n{\r\nstruct keystone_pcie *ks_pcie = to_keystone_pcie(pp);\r\nu8 bus_num = bus->number;\r\nvoid __iomem *addr;\r\naddr = ks_pcie_cfg_setup(ks_pcie, bus_num, devfn);\r\nreturn dw_pcie_cfg_read(addr + (where & ~0x3), where, size, val);\r\n}\r\nint ks_dw_pcie_wr_other_conf(struct pcie_port *pp, struct pci_bus *bus,\r\nunsigned int devfn, int where, int size, u32 val)\r\n{\r\nstruct keystone_pcie *ks_pcie = to_keystone_pcie(pp);\r\nu8 bus_num = bus->number;\r\nvoid __iomem *addr;\r\naddr = ks_pcie_cfg_setup(ks_pcie, bus_num, devfn);\r\nreturn dw_pcie_cfg_write(addr + (where & ~0x3), where, size, val);\r\n}\r\nvoid ks_dw_pcie_v3_65_scan_bus(struct pcie_port *pp)\r\n{\r\nstruct keystone_pcie *ks_pcie = to_keystone_pcie(pp);\r\nks_dw_pcie_set_dbi_mode(ks_pcie->va_app_base);\r\nwritel(1, pp->dbi_base + PCI_BASE_ADDRESS_0);\r\nwritel(SZ_4K - 1, pp->dbi_base + PCI_BASE_ADDRESS_0);\r\nks_dw_pcie_clear_dbi_mode(ks_pcie->va_app_base);\r\nwritel(ks_pcie->app.start, pp->dbi_base + PCI_BASE_ADDRESS_0);\r\n}\r\nint ks_dw_pcie_link_up(struct pcie_port *pp)\r\n{\r\nu32 val = readl(pp->dbi_base + DEBUG0);\r\nreturn (val & LTSSM_STATE_MASK) == LTSSM_STATE_L0;\r\n}\r\nvoid ks_dw_pcie_initiate_link_train(struct keystone_pcie *ks_pcie)\r\n{\r\nu32 val;\r\nval = readl(ks_pcie->va_app_base + CMD_STATUS);\r\nval &= ~LTSSM_EN_VAL;\r\nwritel(LTSSM_EN_VAL | val, ks_pcie->va_app_base + CMD_STATUS);\r\nval = readl(ks_pcie->va_app_base + CMD_STATUS);\r\nwritel(LTSSM_EN_VAL | val, ks_pcie->va_app_base + CMD_STATUS);\r\n}\r\nint __init ks_dw_pcie_host_init(struct keystone_pcie *ks_pcie,\r\nstruct device_node *msi_intc_np)\r\n{\r\nstruct pcie_port *pp = &ks_pcie->pp;\r\nstruct platform_device *pdev = to_platform_device(pp->dev);\r\nstruct resource *res;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\npp->dbi_base = devm_ioremap_resource(pp->dev, res);\r\nif (IS_ERR(pp->dbi_base))\r\nreturn PTR_ERR(pp->dbi_base);\r\npp->va_cfg0_base = pp->dbi_base + SPACE0_REMOTE_CFG_OFFSET;\r\npp->va_cfg1_base = pp->va_cfg0_base;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nks_pcie->va_app_base = devm_ioremap_resource(pp->dev, res);\r\nif (IS_ERR(ks_pcie->va_app_base))\r\nreturn PTR_ERR(ks_pcie->va_app_base);\r\nks_pcie->app = *res;\r\nks_pcie->legacy_irq_domain =\r\nirq_domain_add_linear(ks_pcie->legacy_intc_np,\r\nMAX_LEGACY_IRQS,\r\n&ks_dw_pcie_legacy_irq_domain_ops,\r\nNULL);\r\nif (!ks_pcie->legacy_irq_domain) {\r\ndev_err(pp->dev, "Failed to add irq domain for legacy irqs\n");\r\nreturn -EINVAL;\r\n}\r\nreturn dw_pcie_host_init(pp);\r\n}
