static u64 amd_pmu_event_map(int hw_event)\r\n{\r\nreturn amd_perfmon_event_map[hw_event];\r\n}\r\nstatic inline int amd_pmu_addr_offset(int index, bool eventsel)\r\n{\r\nint offset;\r\nif (!index)\r\nreturn index;\r\nif (eventsel)\r\noffset = event_offsets[index];\r\nelse\r\noffset = count_offsets[index];\r\nif (offset)\r\nreturn offset;\r\nif (!cpu_has_perfctr_core)\r\noffset = index;\r\nelse\r\noffset = index << 1;\r\nif (eventsel)\r\nevent_offsets[index] = offset;\r\nelse\r\ncount_offsets[index] = offset;\r\nreturn offset;\r\n}\r\nstatic int amd_core_hw_config(struct perf_event *event)\r\n{\r\nif (event->attr.exclude_host && event->attr.exclude_guest)\r\nevent->hw.config &= ~(ARCH_PERFMON_EVENTSEL_USR |\r\nARCH_PERFMON_EVENTSEL_OS);\r\nelse if (event->attr.exclude_host)\r\nevent->hw.config |= AMD64_EVENTSEL_GUESTONLY;\r\nelse if (event->attr.exclude_guest)\r\nevent->hw.config |= AMD64_EVENTSEL_HOSTONLY;\r\nreturn 0;\r\n}\r\nstatic inline unsigned int amd_get_event_code(struct hw_perf_event *hwc)\r\n{\r\nreturn ((hwc->config >> 24) & 0x0f00) | (hwc->config & 0x00ff);\r\n}\r\nstatic inline int amd_is_nb_event(struct hw_perf_event *hwc)\r\n{\r\nreturn (hwc->config & 0xe0) == 0xe0;\r\n}\r\nstatic inline int amd_has_nb(struct cpu_hw_events *cpuc)\r\n{\r\nstruct amd_nb *nb = cpuc->amd_nb;\r\nreturn nb && nb->nb_id != -1;\r\n}\r\nstatic int amd_pmu_hw_config(struct perf_event *event)\r\n{\r\nint ret;\r\nif (event->attr.precise_ip && get_ibs_caps())\r\nreturn -ENOENT;\r\nif (has_branch_stack(event))\r\nreturn -EOPNOTSUPP;\r\nret = x86_pmu_hw_config(event);\r\nif (ret)\r\nreturn ret;\r\nif (event->attr.type == PERF_TYPE_RAW)\r\nevent->hw.config |= event->attr.config & AMD64_RAW_EVENT_MASK;\r\nreturn amd_core_hw_config(event);\r\n}\r\nstatic void __amd_put_nb_event_constraints(struct cpu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nstruct amd_nb *nb = cpuc->amd_nb;\r\nint i;\r\nfor (i = 0; i < x86_pmu.num_counters; i++) {\r\nif (cmpxchg(nb->owners + i, event, NULL) == event)\r\nbreak;\r\n}\r\n}\r\nstatic struct event_constraint *\r\n__amd_get_nb_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event,\r\nstruct event_constraint *c)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct amd_nb *nb = cpuc->amd_nb;\r\nstruct perf_event *old;\r\nint idx, new = -1;\r\nif (!c)\r\nc = &unconstrained;\r\nif (cpuc->is_fake)\r\nreturn c;\r\nfor_each_set_bit(idx, c->idxmsk, x86_pmu.num_counters) {\r\nif (new == -1 || hwc->idx == idx)\r\nold = cmpxchg(nb->owners + idx, NULL, event);\r\nelse if (nb->owners[idx] == event)\r\nold = event;\r\nelse\r\ncontinue;\r\nif (old && old != event)\r\ncontinue;\r\nif (new != -1)\r\ncmpxchg(nb->owners + new, event, NULL);\r\nnew = idx;\r\nif (old == event)\r\nbreak;\r\n}\r\nif (new == -1)\r\nreturn &emptyconstraint;\r\nreturn &nb->event_constraints[new];\r\n}\r\nstatic struct amd_nb *amd_alloc_nb(int cpu)\r\n{\r\nstruct amd_nb *nb;\r\nint i;\r\nnb = kzalloc_node(sizeof(struct amd_nb), GFP_KERNEL, cpu_to_node(cpu));\r\nif (!nb)\r\nreturn NULL;\r\nnb->nb_id = -1;\r\nfor (i = 0; i < x86_pmu.num_counters; i++) {\r\n__set_bit(i, nb->event_constraints[i].idxmsk);\r\nnb->event_constraints[i].weight = 1;\r\n}\r\nreturn nb;\r\n}\r\nstatic int amd_pmu_cpu_prepare(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\r\nWARN_ON_ONCE(cpuc->amd_nb);\r\nif (boot_cpu_data.x86_max_cores < 2)\r\nreturn NOTIFY_OK;\r\ncpuc->amd_nb = amd_alloc_nb(cpu);\r\nif (!cpuc->amd_nb)\r\nreturn NOTIFY_BAD;\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void amd_pmu_cpu_starting(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\r\nvoid **onln = &cpuc->kfree_on_online[X86_PERF_KFREE_SHARED];\r\nstruct amd_nb *nb;\r\nint i, nb_id;\r\ncpuc->perf_ctr_virt_mask = AMD64_EVENTSEL_HOSTONLY;\r\nif (boot_cpu_data.x86_max_cores < 2)\r\nreturn;\r\nnb_id = amd_get_nb_id(cpu);\r\nWARN_ON_ONCE(nb_id == BAD_APICID);\r\nfor_each_online_cpu(i) {\r\nnb = per_cpu(cpu_hw_events, i).amd_nb;\r\nif (WARN_ON_ONCE(!nb))\r\ncontinue;\r\nif (nb->nb_id == nb_id) {\r\n*onln = cpuc->amd_nb;\r\ncpuc->amd_nb = nb;\r\nbreak;\r\n}\r\n}\r\ncpuc->amd_nb->nb_id = nb_id;\r\ncpuc->amd_nb->refcnt++;\r\n}\r\nstatic void amd_pmu_cpu_dead(int cpu)\r\n{\r\nstruct cpu_hw_events *cpuhw;\r\nif (boot_cpu_data.x86_max_cores < 2)\r\nreturn;\r\ncpuhw = &per_cpu(cpu_hw_events, cpu);\r\nif (cpuhw->amd_nb) {\r\nstruct amd_nb *nb = cpuhw->amd_nb;\r\nif (nb->nb_id == -1 || --nb->refcnt == 0)\r\nkfree(nb);\r\ncpuhw->amd_nb = NULL;\r\n}\r\n}\r\nstatic struct event_constraint *\r\namd_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\r\nstruct perf_event *event)\r\n{\r\nif (!(amd_has_nb(cpuc) && amd_is_nb_event(&event->hw)))\r\nreturn &unconstrained;\r\nreturn __amd_get_nb_event_constraints(cpuc, event, NULL);\r\n}\r\nstatic void amd_put_event_constraints(struct cpu_hw_events *cpuc,\r\nstruct perf_event *event)\r\n{\r\nif (amd_has_nb(cpuc) && amd_is_nb_event(&event->hw))\r\n__amd_put_nb_event_constraints(cpuc, event);\r\n}\r\nstatic struct event_constraint *\r\namd_get_event_constraints_f15h(struct cpu_hw_events *cpuc, int idx,\r\nstruct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nunsigned int event_code = amd_get_event_code(hwc);\r\nswitch (event_code & AMD_EVENT_TYPE_MASK) {\r\ncase AMD_EVENT_FP:\r\nswitch (event_code) {\r\ncase 0x000:\r\nif (!(hwc->config & 0x0000F000ULL))\r\nbreak;\r\nif (!(hwc->config & 0x00000F00ULL))\r\nbreak;\r\nreturn &amd_f15_PMC3;\r\ncase 0x004:\r\nif (hweight_long(hwc->config & ARCH_PERFMON_EVENTSEL_UMASK) <= 1)\r\nbreak;\r\nreturn &amd_f15_PMC3;\r\ncase 0x003:\r\ncase 0x00B:\r\ncase 0x00D:\r\nreturn &amd_f15_PMC3;\r\n}\r\nreturn &amd_f15_PMC53;\r\ncase AMD_EVENT_LS:\r\ncase AMD_EVENT_DC:\r\ncase AMD_EVENT_EX_LS:\r\nswitch (event_code) {\r\ncase 0x023:\r\ncase 0x043:\r\ncase 0x045:\r\ncase 0x046:\r\ncase 0x054:\r\ncase 0x055:\r\nreturn &amd_f15_PMC20;\r\ncase 0x02D:\r\nreturn &amd_f15_PMC3;\r\ncase 0x02E:\r\nreturn &amd_f15_PMC30;\r\ncase 0x031:\r\nif (hweight_long(hwc->config & ARCH_PERFMON_EVENTSEL_UMASK) <= 1)\r\nreturn &amd_f15_PMC20;\r\nreturn &emptyconstraint;\r\ncase 0x1C0:\r\nreturn &amd_f15_PMC53;\r\ndefault:\r\nreturn &amd_f15_PMC50;\r\n}\r\ncase AMD_EVENT_CU:\r\ncase AMD_EVENT_IC_DE:\r\ncase AMD_EVENT_DE:\r\nswitch (event_code) {\r\ncase 0x08F:\r\ncase 0x187:\r\ncase 0x188:\r\nreturn &amd_f15_PMC0;\r\ncase 0x0DB ... 0x0DF:\r\ncase 0x1D6:\r\ncase 0x1D8:\r\nreturn &amd_f15_PMC50;\r\ndefault:\r\nreturn &amd_f15_PMC20;\r\n}\r\ncase AMD_EVENT_NB:\r\nreturn &emptyconstraint;\r\ndefault:\r\nreturn &emptyconstraint;\r\n}\r\n}\r\nstatic ssize_t amd_event_sysfs_show(char *page, u64 config)\r\n{\r\nu64 event = (config & ARCH_PERFMON_EVENTSEL_EVENT) |\r\n(config & AMD64_EVENTSEL_EVENT) >> 24;\r\nreturn x86_event_sysfs_show(page, config, event);\r\n}\r\nstatic int __init amd_core_pmu_init(void)\r\n{\r\nif (!cpu_has_perfctr_core)\r\nreturn 0;\r\nswitch (boot_cpu_data.x86) {\r\ncase 0x15:\r\npr_cont("Fam15h ");\r\nx86_pmu.get_event_constraints = amd_get_event_constraints_f15h;\r\nbreak;\r\ndefault:\r\npr_err("core perfctr but no constraints; unknown hardware!\n");\r\nreturn -ENODEV;\r\n}\r\nx86_pmu.eventsel = MSR_F15H_PERF_CTL;\r\nx86_pmu.perfctr = MSR_F15H_PERF_CTR;\r\nx86_pmu.num_counters = AMD64_NUM_COUNTERS_CORE;\r\npr_cont("core perfctr, ");\r\nreturn 0;\r\n}\r\n__init int amd_pmu_init(void)\r\n{\r\nint ret;\r\nif (boot_cpu_data.x86 < 6)\r\nreturn -ENODEV;\r\nx86_pmu = amd_pmu;\r\nret = amd_core_pmu_init();\r\nif (ret)\r\nreturn ret;\r\nmemcpy(hw_cache_event_ids, amd_hw_cache_event_ids,\r\nsizeof(hw_cache_event_ids));\r\nreturn 0;\r\n}\r\nvoid amd_pmu_enable_virt(void)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\ncpuc->perf_ctr_virt_mask = 0;\r\nx86_pmu_disable_all();\r\nx86_pmu_enable_all(0);\r\n}\r\nvoid amd_pmu_disable_virt(void)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\ncpuc->perf_ctr_virt_mask = AMD64_EVENTSEL_HOSTONLY;\r\nx86_pmu_disable_all();\r\nx86_pmu_enable_all(0);\r\n}
