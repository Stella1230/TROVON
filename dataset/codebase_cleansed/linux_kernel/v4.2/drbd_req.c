static void _drbd_start_io_acct(struct drbd_device *device, struct drbd_request *req)\r\n{\r\ngeneric_start_io_acct(bio_data_dir(req->master_bio), req->i.size >> 9,\r\n&device->vdisk->part0);\r\n}\r\nstatic void _drbd_end_io_acct(struct drbd_device *device, struct drbd_request *req)\r\n{\r\ngeneric_end_io_acct(bio_data_dir(req->master_bio),\r\n&device->vdisk->part0, req->start_jif);\r\n}\r\nstatic struct drbd_request *drbd_req_new(struct drbd_device *device,\r\nstruct bio *bio_src)\r\n{\r\nstruct drbd_request *req;\r\nreq = mempool_alloc(drbd_request_mempool, GFP_NOIO);\r\nif (!req)\r\nreturn NULL;\r\nmemset(req, 0, sizeof(*req));\r\ndrbd_req_make_private_bio(req, bio_src);\r\nreq->rq_state = bio_data_dir(bio_src) == WRITE ? RQ_WRITE : 0;\r\nreq->device = device;\r\nreq->master_bio = bio_src;\r\nreq->epoch = 0;\r\ndrbd_clear_interval(&req->i);\r\nreq->i.sector = bio_src->bi_iter.bi_sector;\r\nreq->i.size = bio_src->bi_iter.bi_size;\r\nreq->i.local = true;\r\nreq->i.waiting = false;\r\nINIT_LIST_HEAD(&req->tl_requests);\r\nINIT_LIST_HEAD(&req->w.list);\r\nINIT_LIST_HEAD(&req->req_pending_master_completion);\r\nINIT_LIST_HEAD(&req->req_pending_local);\r\natomic_set(&req->completion_ref, 1);\r\nkref_init(&req->kref);\r\nreturn req;\r\n}\r\nstatic void drbd_remove_request_interval(struct rb_root *root,\r\nstruct drbd_request *req)\r\n{\r\nstruct drbd_device *device = req->device;\r\nstruct drbd_interval *i = &req->i;\r\ndrbd_remove_interval(root, i);\r\nif (i->waiting)\r\nwake_up(&device->misc_wait);\r\n}\r\nvoid drbd_req_destroy(struct kref *kref)\r\n{\r\nstruct drbd_request *req = container_of(kref, struct drbd_request, kref);\r\nstruct drbd_device *device = req->device;\r\nconst unsigned s = req->rq_state;\r\nif ((req->master_bio && !(s & RQ_POSTPONED)) ||\r\natomic_read(&req->completion_ref) ||\r\n(s & RQ_LOCAL_PENDING) ||\r\n((s & RQ_NET_MASK) && !(s & RQ_NET_DONE))) {\r\ndrbd_err(device, "drbd_req_destroy: Logic BUG rq_state = 0x%x, completion_ref = %d\n",\r\ns, atomic_read(&req->completion_ref));\r\nreturn;\r\n}\r\nlist_del_init(&req->tl_requests);\r\nif (!drbd_interval_empty(&req->i)) {\r\nstruct rb_root *root;\r\nif (s & RQ_WRITE)\r\nroot = &device->write_requests;\r\nelse\r\nroot = &device->read_requests;\r\ndrbd_remove_request_interval(root, req);\r\n} else if (s & (RQ_NET_MASK & ~RQ_NET_DONE) && req->i.size != 0)\r\ndrbd_err(device, "drbd_req_destroy: Logic BUG: interval empty, but: rq_state=0x%x, sect=%llu, size=%u\n",\r\ns, (unsigned long long)req->i.sector, req->i.size);\r\nif (s & RQ_WRITE) {\r\nif ((s & (RQ_POSTPONED|RQ_LOCAL_MASK|RQ_NET_MASK)) != RQ_POSTPONED) {\r\nif (!(s & RQ_NET_OK) || !(s & RQ_LOCAL_OK))\r\ndrbd_set_out_of_sync(device, req->i.sector, req->i.size);\r\nif ((s & RQ_NET_OK) && (s & RQ_LOCAL_OK) && (s & RQ_NET_SIS))\r\ndrbd_set_in_sync(device, req->i.sector, req->i.size);\r\n}\r\nif (s & RQ_IN_ACT_LOG) {\r\nif (get_ldev_if_state(device, D_FAILED)) {\r\ndrbd_al_complete_io(device, &req->i);\r\nput_ldev(device);\r\n} else if (__ratelimit(&drbd_ratelimit_state)) {\r\ndrbd_warn(device, "Should have called drbd_al_complete_io(, %llu, %u), "\r\n"but my Disk seems to have failed :(\n",\r\n(unsigned long long) req->i.sector, req->i.size);\r\n}\r\n}\r\n}\r\nmempool_free(req, drbd_request_mempool);\r\n}\r\nstatic void wake_all_senders(struct drbd_connection *connection)\r\n{\r\nwake_up(&connection->sender_work.q_wait);\r\n}\r\nvoid start_new_tl_epoch(struct drbd_connection *connection)\r\n{\r\nif (connection->current_tle_writes == 0)\r\nreturn;\r\nconnection->current_tle_writes = 0;\r\natomic_inc(&connection->current_tle_nr);\r\nwake_all_senders(connection);\r\n}\r\nvoid complete_master_bio(struct drbd_device *device,\r\nstruct bio_and_error *m)\r\n{\r\nbio_endio(m->bio, m->error);\r\ndec_ap_bio(device);\r\n}\r\nstatic\r\nvoid drbd_req_complete(struct drbd_request *req, struct bio_and_error *m)\r\n{\r\nconst unsigned s = req->rq_state;\r\nstruct drbd_device *device = req->device;\r\nint rw;\r\nint error, ok;\r\nif ((s & RQ_LOCAL_PENDING && !(s & RQ_LOCAL_ABORTED)) ||\r\n(s & RQ_NET_QUEUED) || (s & RQ_NET_PENDING) ||\r\n(s & RQ_COMPLETION_SUSP)) {\r\ndrbd_err(device, "drbd_req_complete: Logic BUG rq_state = 0x%x\n", s);\r\nreturn;\r\n}\r\nif (!req->master_bio) {\r\ndrbd_err(device, "drbd_req_complete: Logic BUG, master_bio == NULL!\n");\r\nreturn;\r\n}\r\nrw = bio_rw(req->master_bio);\r\nok = (s & RQ_LOCAL_OK) || (s & RQ_NET_OK);\r\nerror = PTR_ERR(req->private_bio);\r\nif (rw == WRITE &&\r\nreq->epoch == atomic_read(&first_peer_device(device)->connection->current_tle_nr))\r\nstart_new_tl_epoch(first_peer_device(device)->connection);\r\n_drbd_end_io_acct(device, req);\r\nif (!ok && rw == READ && !list_empty(&req->tl_requests))\r\nreq->rq_state |= RQ_POSTPONED;\r\nif (!(req->rq_state & RQ_POSTPONED)) {\r\nm->error = ok ? 0 : (error ?: -EIO);\r\nm->bio = req->master_bio;\r\nreq->master_bio = NULL;\r\nreq->i.completed = true;\r\n}\r\nif (req->i.waiting)\r\nwake_up(&device->misc_wait);\r\nlist_del_init(&req->req_pending_master_completion);\r\n}\r\nstatic int drbd_req_put_completion_ref(struct drbd_request *req, struct bio_and_error *m, int put)\r\n{\r\nstruct drbd_device *device = req->device;\r\nD_ASSERT(device, m || (req->rq_state & RQ_POSTPONED));\r\nif (!atomic_sub_and_test(put, &req->completion_ref))\r\nreturn 0;\r\ndrbd_req_complete(req, m);\r\nif (req->rq_state & RQ_POSTPONED) {\r\ndrbd_restart_request(req);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void set_if_null_req_next(struct drbd_peer_device *peer_device, struct drbd_request *req)\r\n{\r\nstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\r\nif (!connection)\r\nreturn;\r\nif (connection->req_next == NULL)\r\nconnection->req_next = req;\r\n}\r\nstatic void advance_conn_req_next(struct drbd_peer_device *peer_device, struct drbd_request *req)\r\n{\r\nstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\r\nif (!connection)\r\nreturn;\r\nif (connection->req_next != req)\r\nreturn;\r\nlist_for_each_entry_continue(req, &connection->transfer_log, tl_requests) {\r\nconst unsigned s = req->rq_state;\r\nif (s & RQ_NET_QUEUED)\r\nbreak;\r\n}\r\nif (&req->tl_requests == &connection->transfer_log)\r\nreq = NULL;\r\nconnection->req_next = req;\r\n}\r\nstatic void set_if_null_req_ack_pending(struct drbd_peer_device *peer_device, struct drbd_request *req)\r\n{\r\nstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\r\nif (!connection)\r\nreturn;\r\nif (connection->req_ack_pending == NULL)\r\nconnection->req_ack_pending = req;\r\n}\r\nstatic void advance_conn_req_ack_pending(struct drbd_peer_device *peer_device, struct drbd_request *req)\r\n{\r\nstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\r\nif (!connection)\r\nreturn;\r\nif (connection->req_ack_pending != req)\r\nreturn;\r\nlist_for_each_entry_continue(req, &connection->transfer_log, tl_requests) {\r\nconst unsigned s = req->rq_state;\r\nif ((s & RQ_NET_SENT) && (s & RQ_NET_PENDING))\r\nbreak;\r\n}\r\nif (&req->tl_requests == &connection->transfer_log)\r\nreq = NULL;\r\nconnection->req_ack_pending = req;\r\n}\r\nstatic void set_if_null_req_not_net_done(struct drbd_peer_device *peer_device, struct drbd_request *req)\r\n{\r\nstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\r\nif (!connection)\r\nreturn;\r\nif (connection->req_not_net_done == NULL)\r\nconnection->req_not_net_done = req;\r\n}\r\nstatic void advance_conn_req_not_net_done(struct drbd_peer_device *peer_device, struct drbd_request *req)\r\n{\r\nstruct drbd_connection *connection = peer_device ? peer_device->connection : NULL;\r\nif (!connection)\r\nreturn;\r\nif (connection->req_not_net_done != req)\r\nreturn;\r\nlist_for_each_entry_continue(req, &connection->transfer_log, tl_requests) {\r\nconst unsigned s = req->rq_state;\r\nif ((s & RQ_NET_SENT) && !(s & RQ_NET_DONE))\r\nbreak;\r\n}\r\nif (&req->tl_requests == &connection->transfer_log)\r\nreq = NULL;\r\nconnection->req_not_net_done = req;\r\n}\r\nstatic void mod_rq_state(struct drbd_request *req, struct bio_and_error *m,\r\nint clear, int set)\r\n{\r\nstruct drbd_device *device = req->device;\r\nstruct drbd_peer_device *peer_device = first_peer_device(device);\r\nunsigned s = req->rq_state;\r\nint c_put = 0;\r\nint k_put = 0;\r\nif (drbd_suspended(device) && !((s | clear) & RQ_COMPLETION_SUSP))\r\nset |= RQ_COMPLETION_SUSP;\r\nreq->rq_state &= ~clear;\r\nreq->rq_state |= set;\r\nif (req->rq_state == s)\r\nreturn;\r\nif (!(s & RQ_LOCAL_PENDING) && (set & RQ_LOCAL_PENDING))\r\natomic_inc(&req->completion_ref);\r\nif (!(s & RQ_NET_PENDING) && (set & RQ_NET_PENDING)) {\r\ninc_ap_pending(device);\r\natomic_inc(&req->completion_ref);\r\n}\r\nif (!(s & RQ_NET_QUEUED) && (set & RQ_NET_QUEUED)) {\r\natomic_inc(&req->completion_ref);\r\nset_if_null_req_next(peer_device, req);\r\n}\r\nif (!(s & RQ_EXP_BARR_ACK) && (set & RQ_EXP_BARR_ACK))\r\nkref_get(&req->kref);\r\nif (!(s & RQ_NET_SENT) && (set & RQ_NET_SENT)) {\r\nif (!(s & RQ_NET_DONE)) {\r\natomic_add(req->i.size >> 9, &device->ap_in_flight);\r\nset_if_null_req_not_net_done(peer_device, req);\r\n}\r\nif (s & RQ_NET_PENDING)\r\nset_if_null_req_ack_pending(peer_device, req);\r\n}\r\nif (!(s & RQ_COMPLETION_SUSP) && (set & RQ_COMPLETION_SUSP))\r\natomic_inc(&req->completion_ref);\r\nif ((s & RQ_COMPLETION_SUSP) && (clear & RQ_COMPLETION_SUSP))\r\n++c_put;\r\nif (!(s & RQ_LOCAL_ABORTED) && (set & RQ_LOCAL_ABORTED)) {\r\nD_ASSERT(device, req->rq_state & RQ_LOCAL_PENDING);\r\nkref_get(&req->kref);\r\n++c_put;\r\n}\r\nif ((s & RQ_LOCAL_PENDING) && (clear & RQ_LOCAL_PENDING)) {\r\nif (req->rq_state & RQ_LOCAL_ABORTED)\r\n++k_put;\r\nelse\r\n++c_put;\r\nlist_del_init(&req->req_pending_local);\r\n}\r\nif ((s & RQ_NET_PENDING) && (clear & RQ_NET_PENDING)) {\r\ndec_ap_pending(device);\r\n++c_put;\r\nreq->acked_jif = jiffies;\r\nadvance_conn_req_ack_pending(peer_device, req);\r\n}\r\nif ((s & RQ_NET_QUEUED) && (clear & RQ_NET_QUEUED)) {\r\n++c_put;\r\nadvance_conn_req_next(peer_device, req);\r\n}\r\nif (!(s & RQ_NET_DONE) && (set & RQ_NET_DONE)) {\r\nif (s & RQ_NET_SENT)\r\natomic_sub(req->i.size >> 9, &device->ap_in_flight);\r\nif (s & RQ_EXP_BARR_ACK)\r\n++k_put;\r\nreq->net_done_jif = jiffies;\r\nadvance_conn_req_next(peer_device, req);\r\nadvance_conn_req_ack_pending(peer_device, req);\r\nadvance_conn_req_not_net_done(peer_device, req);\r\n}\r\nif (k_put || c_put) {\r\nint at_least = k_put + !!c_put;\r\nint refcount = atomic_read(&req->kref.refcount);\r\nif (refcount < at_least)\r\ndrbd_err(device,\r\n"mod_rq_state: Logic BUG: %x -> %x: refcount = %d, should be >= %d\n",\r\ns, req->rq_state, refcount, at_least);\r\n}\r\nif (req->i.waiting)\r\nwake_up(&device->misc_wait);\r\nif (c_put)\r\nk_put += drbd_req_put_completion_ref(req, m, c_put);\r\nif (k_put)\r\nkref_sub(&req->kref, k_put, drbd_req_destroy);\r\n}\r\nstatic void drbd_report_io_error(struct drbd_device *device, struct drbd_request *req)\r\n{\r\nchar b[BDEVNAME_SIZE];\r\nif (!__ratelimit(&drbd_ratelimit_state))\r\nreturn;\r\ndrbd_warn(device, "local %s IO error sector %llu+%u on %s\n",\r\n(req->rq_state & RQ_WRITE) ? "WRITE" : "READ",\r\n(unsigned long long)req->i.sector,\r\nreq->i.size >> 9,\r\nbdevname(device->ldev->backing_bdev, b));\r\n}\r\nstatic inline bool is_pending_write_protocol_A(struct drbd_request *req)\r\n{\r\nreturn (req->rq_state &\r\n(RQ_WRITE|RQ_NET_PENDING|RQ_EXP_WRITE_ACK|RQ_EXP_RECEIVE_ACK))\r\n== (RQ_WRITE|RQ_NET_PENDING);\r\n}\r\nint __req_mod(struct drbd_request *req, enum drbd_req_event what,\r\nstruct bio_and_error *m)\r\n{\r\nstruct drbd_device *const device = req->device;\r\nstruct drbd_peer_device *const peer_device = first_peer_device(device);\r\nstruct drbd_connection *const connection = peer_device ? peer_device->connection : NULL;\r\nstruct net_conf *nc;\r\nint p, rv = 0;\r\nif (m)\r\nm->bio = NULL;\r\nswitch (what) {\r\ndefault:\r\ndrbd_err(device, "LOGIC BUG in %s:%u\n", __FILE__ , __LINE__);\r\nbreak;\r\ncase TO_BE_SENT:\r\nD_ASSERT(device, !(req->rq_state & RQ_NET_MASK));\r\nrcu_read_lock();\r\nnc = rcu_dereference(connection->net_conf);\r\np = nc->wire_protocol;\r\nrcu_read_unlock();\r\nreq->rq_state |=\r\np == DRBD_PROT_C ? RQ_EXP_WRITE_ACK :\r\np == DRBD_PROT_B ? RQ_EXP_RECEIVE_ACK : 0;\r\nmod_rq_state(req, m, 0, RQ_NET_PENDING);\r\nbreak;\r\ncase TO_BE_SUBMITTED:\r\nD_ASSERT(device, !(req->rq_state & RQ_LOCAL_MASK));\r\nmod_rq_state(req, m, 0, RQ_LOCAL_PENDING);\r\nbreak;\r\ncase COMPLETED_OK:\r\nif (req->rq_state & RQ_WRITE)\r\ndevice->writ_cnt += req->i.size >> 9;\r\nelse\r\ndevice->read_cnt += req->i.size >> 9;\r\nmod_rq_state(req, m, RQ_LOCAL_PENDING,\r\nRQ_LOCAL_COMPLETED|RQ_LOCAL_OK);\r\nbreak;\r\ncase ABORT_DISK_IO:\r\nmod_rq_state(req, m, 0, RQ_LOCAL_ABORTED);\r\nbreak;\r\ncase WRITE_COMPLETED_WITH_ERROR:\r\ndrbd_report_io_error(device, req);\r\n__drbd_chk_io_error(device, DRBD_WRITE_ERROR);\r\nmod_rq_state(req, m, RQ_LOCAL_PENDING, RQ_LOCAL_COMPLETED);\r\nbreak;\r\ncase READ_COMPLETED_WITH_ERROR:\r\ndrbd_set_out_of_sync(device, req->i.sector, req->i.size);\r\ndrbd_report_io_error(device, req);\r\n__drbd_chk_io_error(device, DRBD_READ_ERROR);\r\ncase READ_AHEAD_COMPLETED_WITH_ERROR:\r\nmod_rq_state(req, m, RQ_LOCAL_PENDING, RQ_LOCAL_COMPLETED);\r\nbreak;\r\ncase DISCARD_COMPLETED_NOTSUPP:\r\ncase DISCARD_COMPLETED_WITH_ERROR:\r\nmod_rq_state(req, m, RQ_LOCAL_PENDING, RQ_LOCAL_COMPLETED);\r\nbreak;\r\ncase QUEUE_FOR_NET_READ:\r\nD_ASSERT(device, drbd_interval_empty(&req->i));\r\ndrbd_insert_interval(&device->read_requests, &req->i);\r\nset_bit(UNPLUG_REMOTE, &device->flags);\r\nD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\r\nD_ASSERT(device, (req->rq_state & RQ_LOCAL_MASK) == 0);\r\nmod_rq_state(req, m, 0, RQ_NET_QUEUED);\r\nreq->w.cb = w_send_read_req;\r\ndrbd_queue_work(&connection->sender_work,\r\n&req->w);\r\nbreak;\r\ncase QUEUE_FOR_NET_WRITE:\r\nD_ASSERT(device, drbd_interval_empty(&req->i));\r\ndrbd_insert_interval(&device->write_requests, &req->i);\r\nset_bit(UNPLUG_REMOTE, &device->flags);\r\nD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\r\nmod_rq_state(req, m, 0, RQ_NET_QUEUED|RQ_EXP_BARR_ACK);\r\nreq->w.cb = w_send_dblock;\r\ndrbd_queue_work(&connection->sender_work,\r\n&req->w);\r\nrcu_read_lock();\r\nnc = rcu_dereference(connection->net_conf);\r\np = nc->max_epoch_size;\r\nrcu_read_unlock();\r\nif (connection->current_tle_writes >= p)\r\nstart_new_tl_epoch(connection);\r\nbreak;\r\ncase QUEUE_FOR_SEND_OOS:\r\nmod_rq_state(req, m, 0, RQ_NET_QUEUED);\r\nreq->w.cb = w_send_out_of_sync;\r\ndrbd_queue_work(&connection->sender_work,\r\n&req->w);\r\nbreak;\r\ncase READ_RETRY_REMOTE_CANCELED:\r\ncase SEND_CANCELED:\r\ncase SEND_FAILED:\r\nmod_rq_state(req, m, RQ_NET_QUEUED, 0);\r\nbreak;\r\ncase HANDED_OVER_TO_NETWORK:\r\nif (is_pending_write_protocol_A(req))\r\nmod_rq_state(req, m, RQ_NET_QUEUED|RQ_NET_PENDING,\r\nRQ_NET_SENT|RQ_NET_OK);\r\nelse\r\nmod_rq_state(req, m, RQ_NET_QUEUED, RQ_NET_SENT);\r\nbreak;\r\ncase OOS_HANDED_TO_NETWORK:\r\nmod_rq_state(req, m, RQ_NET_QUEUED, RQ_NET_DONE);\r\nbreak;\r\ncase CONNECTION_LOST_WHILE_PENDING:\r\nmod_rq_state(req, m,\r\nRQ_NET_OK|RQ_NET_PENDING|RQ_COMPLETION_SUSP,\r\nRQ_NET_DONE);\r\nbreak;\r\ncase CONFLICT_RESOLVED:\r\nD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\r\nD_ASSERT(device, req->rq_state & RQ_EXP_WRITE_ACK);\r\nmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_DONE|RQ_NET_OK);\r\nbreak;\r\ncase WRITE_ACKED_BY_PEER_AND_SIS:\r\nreq->rq_state |= RQ_NET_SIS;\r\ncase WRITE_ACKED_BY_PEER:\r\ngoto ack_common;\r\ncase RECV_ACKED_BY_PEER:\r\nD_ASSERT(device, req->rq_state & RQ_EXP_RECEIVE_ACK);\r\nack_common:\r\nmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_OK);\r\nbreak;\r\ncase POSTPONE_WRITE:\r\nD_ASSERT(device, req->rq_state & RQ_EXP_WRITE_ACK);\r\nD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\r\nreq->rq_state |= RQ_POSTPONED;\r\nif (req->i.waiting)\r\nwake_up(&device->misc_wait);\r\nbreak;\r\ncase NEG_ACKED:\r\nmod_rq_state(req, m, RQ_NET_OK|RQ_NET_PENDING, 0);\r\nbreak;\r\ncase FAIL_FROZEN_DISK_IO:\r\nif (!(req->rq_state & RQ_LOCAL_COMPLETED))\r\nbreak;\r\nmod_rq_state(req, m, RQ_COMPLETION_SUSP, 0);\r\nbreak;\r\ncase RESTART_FROZEN_DISK_IO:\r\nif (!(req->rq_state & RQ_LOCAL_COMPLETED))\r\nbreak;\r\nmod_rq_state(req, m,\r\nRQ_COMPLETION_SUSP|RQ_LOCAL_COMPLETED,\r\nRQ_LOCAL_PENDING);\r\nrv = MR_READ;\r\nif (bio_data_dir(req->master_bio) == WRITE)\r\nrv = MR_WRITE;\r\nget_ldev(device);\r\nreq->w.cb = w_restart_disk_io;\r\ndrbd_queue_work(&connection->sender_work,\r\n&req->w);\r\nbreak;\r\ncase RESEND:\r\nif (!(req->rq_state & RQ_WRITE) && !req->w.cb) {\r\nmod_rq_state(req, m, RQ_COMPLETION_SUSP, 0);\r\nbreak;\r\n}\r\nif (!(req->rq_state & RQ_NET_OK)) {\r\nmod_rq_state(req, m, RQ_COMPLETION_SUSP, RQ_NET_QUEUED|RQ_NET_PENDING);\r\nif (req->w.cb) {\r\ndrbd_queue_work(&connection->sender_work,\r\n&req->w);\r\nrv = req->rq_state & RQ_WRITE ? MR_WRITE : MR_READ;\r\n}\r\nbreak;\r\n}\r\ncase BARRIER_ACKED:\r\nif (!(req->rq_state & RQ_WRITE))\r\nbreak;\r\nif (req->rq_state & RQ_NET_PENDING) {\r\ndrbd_err(device, "FIXME (BARRIER_ACKED but pending)\n");\r\n}\r\nmod_rq_state(req, m, RQ_COMPLETION_SUSP,\r\n(req->rq_state & RQ_NET_MASK) ? RQ_NET_DONE : 0);\r\nbreak;\r\ncase DATA_RECEIVED:\r\nD_ASSERT(device, req->rq_state & RQ_NET_PENDING);\r\nmod_rq_state(req, m, RQ_NET_PENDING, RQ_NET_OK|RQ_NET_DONE);\r\nbreak;\r\ncase QUEUE_AS_DRBD_BARRIER:\r\nstart_new_tl_epoch(connection);\r\nmod_rq_state(req, m, 0, RQ_NET_OK|RQ_NET_DONE);\r\nbreak;\r\n};\r\nreturn rv;\r\n}\r\nstatic bool drbd_may_do_local_read(struct drbd_device *device, sector_t sector, int size)\r\n{\r\nunsigned long sbnr, ebnr;\r\nsector_t esector, nr_sectors;\r\nif (device->state.disk == D_UP_TO_DATE)\r\nreturn true;\r\nif (device->state.disk != D_INCONSISTENT)\r\nreturn false;\r\nesector = sector + (size >> 9) - 1;\r\nnr_sectors = drbd_get_capacity(device->this_bdev);\r\nD_ASSERT(device, sector < nr_sectors);\r\nD_ASSERT(device, esector < nr_sectors);\r\nsbnr = BM_SECT_TO_BIT(sector);\r\nebnr = BM_SECT_TO_BIT(esector);\r\nreturn drbd_bm_count_bits(device, sbnr, ebnr) == 0;\r\n}\r\nstatic bool remote_due_to_read_balancing(struct drbd_device *device, sector_t sector,\r\nenum drbd_read_balancing rbm)\r\n{\r\nstruct backing_dev_info *bdi;\r\nint stripe_shift;\r\nswitch (rbm) {\r\ncase RB_CONGESTED_REMOTE:\r\nbdi = &device->ldev->backing_bdev->bd_disk->queue->backing_dev_info;\r\nreturn bdi_read_congested(bdi);\r\ncase RB_LEAST_PENDING:\r\nreturn atomic_read(&device->local_cnt) >\r\natomic_read(&device->ap_pending_cnt) + atomic_read(&device->rs_pending_cnt);\r\ncase RB_32K_STRIPING:\r\ncase RB_64K_STRIPING:\r\ncase RB_128K_STRIPING:\r\ncase RB_256K_STRIPING:\r\ncase RB_512K_STRIPING:\r\ncase RB_1M_STRIPING:\r\nstripe_shift = (rbm - RB_32K_STRIPING + 15);\r\nreturn (sector >> (stripe_shift - 9)) & 1;\r\ncase RB_ROUND_ROBIN:\r\nreturn test_and_change_bit(READ_BALANCE_RR, &device->flags);\r\ncase RB_PREFER_REMOTE:\r\nreturn true;\r\ncase RB_PREFER_LOCAL:\r\ndefault:\r\nreturn false;\r\n}\r\n}\r\nstatic void complete_conflicting_writes(struct drbd_request *req)\r\n{\r\nDEFINE_WAIT(wait);\r\nstruct drbd_device *device = req->device;\r\nstruct drbd_interval *i;\r\nsector_t sector = req->i.sector;\r\nint size = req->i.size;\r\ni = drbd_find_overlap(&device->write_requests, sector, size);\r\nif (!i)\r\nreturn;\r\nfor (;;) {\r\nprepare_to_wait(&device->misc_wait, &wait, TASK_UNINTERRUPTIBLE);\r\ni = drbd_find_overlap(&device->write_requests, sector, size);\r\nif (!i)\r\nbreak;\r\ni->waiting = true;\r\nspin_unlock_irq(&device->resource->req_lock);\r\nschedule();\r\nspin_lock_irq(&device->resource->req_lock);\r\n}\r\nfinish_wait(&device->misc_wait, &wait);\r\n}\r\nstatic void maybe_pull_ahead(struct drbd_device *device)\r\n{\r\nstruct drbd_connection *connection = first_peer_device(device)->connection;\r\nstruct net_conf *nc;\r\nbool congested = false;\r\nenum drbd_on_congestion on_congestion;\r\nrcu_read_lock();\r\nnc = rcu_dereference(connection->net_conf);\r\non_congestion = nc ? nc->on_congestion : OC_BLOCK;\r\nrcu_read_unlock();\r\nif (on_congestion == OC_BLOCK ||\r\nconnection->agreed_pro_version < 96)\r\nreturn;\r\nif (on_congestion == OC_PULL_AHEAD && device->state.conn == C_AHEAD)\r\nreturn;\r\nif (!get_ldev_if_state(device, D_UP_TO_DATE))\r\nreturn;\r\nif (nc->cong_fill &&\r\natomic_read(&device->ap_in_flight) >= nc->cong_fill) {\r\ndrbd_info(device, "Congestion-fill threshold reached\n");\r\ncongested = true;\r\n}\r\nif (device->act_log->used >= nc->cong_extents) {\r\ndrbd_info(device, "Congestion-extents threshold reached\n");\r\ncongested = true;\r\n}\r\nif (congested) {\r\nstart_new_tl_epoch(first_peer_device(device)->connection);\r\nif (on_congestion == OC_PULL_AHEAD)\r\n_drbd_set_state(_NS(device, conn, C_AHEAD), 0, NULL);\r\nelse\r\n_drbd_set_state(_NS(device, conn, C_DISCONNECTING), 0, NULL);\r\n}\r\nput_ldev(device);\r\n}\r\nstatic bool do_remote_read(struct drbd_request *req)\r\n{\r\nstruct drbd_device *device = req->device;\r\nenum drbd_read_balancing rbm;\r\nif (req->private_bio) {\r\nif (!drbd_may_do_local_read(device,\r\nreq->i.sector, req->i.size)) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\nput_ldev(device);\r\n}\r\n}\r\nif (device->state.pdsk != D_UP_TO_DATE)\r\nreturn false;\r\nif (req->private_bio == NULL)\r\nreturn true;\r\nrcu_read_lock();\r\nrbm = rcu_dereference(device->ldev->disk_conf)->read_balancing;\r\nrcu_read_unlock();\r\nif (rbm == RB_PREFER_LOCAL && req->private_bio)\r\nreturn false;\r\nif (remote_due_to_read_balancing(device, req->i.sector, rbm)) {\r\nif (req->private_bio) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\nput_ldev(device);\r\n}\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int drbd_process_write_request(struct drbd_request *req)\r\n{\r\nstruct drbd_device *device = req->device;\r\nint remote, send_oos;\r\nremote = drbd_should_do_remote(device->state);\r\nsend_oos = drbd_should_send_out_of_sync(device->state);\r\nif (unlikely(req->i.size == 0)) {\r\nD_ASSERT(device, req->master_bio->bi_rw & REQ_FLUSH);\r\nif (remote)\r\n_req_mod(req, QUEUE_AS_DRBD_BARRIER);\r\nreturn remote;\r\n}\r\nif (!remote && !send_oos)\r\nreturn 0;\r\nD_ASSERT(device, !(remote && send_oos));\r\nif (remote) {\r\n_req_mod(req, TO_BE_SENT);\r\n_req_mod(req, QUEUE_FOR_NET_WRITE);\r\n} else if (drbd_set_out_of_sync(device, req->i.sector, req->i.size))\r\n_req_mod(req, QUEUE_FOR_SEND_OOS);\r\nreturn remote;\r\n}\r\nstatic void\r\ndrbd_submit_req_private_bio(struct drbd_request *req)\r\n{\r\nstruct drbd_device *device = req->device;\r\nstruct bio *bio = req->private_bio;\r\nconst int rw = bio_rw(bio);\r\nbio->bi_bdev = device->ldev->backing_bdev;\r\nif (get_ldev(device)) {\r\nreq->pre_submit_jif = jiffies;\r\nif (drbd_insert_fault(device,\r\nrw == WRITE ? DRBD_FAULT_DT_WR\r\n: rw == READ ? DRBD_FAULT_DT_RD\r\n: DRBD_FAULT_DT_RA))\r\nbio_endio(bio, -EIO);\r\nelse\r\ngeneric_make_request(bio);\r\nput_ldev(device);\r\n} else\r\nbio_endio(bio, -EIO);\r\n}\r\nstatic void drbd_queue_write(struct drbd_device *device, struct drbd_request *req)\r\n{\r\nspin_lock_irq(&device->resource->req_lock);\r\nlist_add_tail(&req->tl_requests, &device->submit.writes);\r\nlist_add_tail(&req->req_pending_master_completion,\r\n&device->pending_master_completion[1 ]);\r\nspin_unlock_irq(&device->resource->req_lock);\r\nqueue_work(device->submit.wq, &device->submit.worker);\r\nwake_up(&device->al_wait);\r\n}\r\nstatic struct drbd_request *\r\ndrbd_request_prepare(struct drbd_device *device, struct bio *bio, unsigned long start_jif)\r\n{\r\nconst int rw = bio_data_dir(bio);\r\nstruct drbd_request *req;\r\nreq = drbd_req_new(device, bio);\r\nif (!req) {\r\ndec_ap_bio(device);\r\ndrbd_err(device, "could not kmalloc() req\n");\r\nbio_endio(bio, -ENOMEM);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nreq->start_jif = start_jif;\r\nif (!get_ldev(device)) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\n}\r\n_drbd_start_io_acct(device, req);\r\nif (rw == WRITE && req->private_bio && req->i.size\r\n&& !test_bit(AL_SUSPENDED, &device->flags)) {\r\nif (!drbd_al_begin_io_fastpath(device, &req->i)) {\r\natomic_inc(&device->ap_actlog_cnt);\r\ndrbd_queue_write(device, req);\r\nreturn NULL;\r\n}\r\nreq->rq_state |= RQ_IN_ACT_LOG;\r\nreq->in_actlog_jif = jiffies;\r\n}\r\nreturn req;\r\n}\r\nstatic void drbd_send_and_submit(struct drbd_device *device, struct drbd_request *req)\r\n{\r\nstruct drbd_resource *resource = device->resource;\r\nconst int rw = bio_rw(req->master_bio);\r\nstruct bio_and_error m = { NULL, };\r\nbool no_remote = false;\r\nbool submit_private_bio = false;\r\nspin_lock_irq(&resource->req_lock);\r\nif (rw == WRITE) {\r\ncomplete_conflicting_writes(req);\r\nmaybe_pull_ahead(device);\r\n}\r\nif (drbd_suspended(device)) {\r\nreq->rq_state |= RQ_POSTPONED;\r\nif (req->private_bio) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\nput_ldev(device);\r\n}\r\ngoto out;\r\n}\r\nif (rw != WRITE) {\r\nif (!do_remote_read(req) && !req->private_bio)\r\ngoto nodata;\r\n}\r\nreq->epoch = atomic_read(&first_peer_device(device)->connection->current_tle_nr);\r\nif (likely(req->i.size!=0)) {\r\nif (rw == WRITE)\r\nfirst_peer_device(device)->connection->current_tle_writes++;\r\nlist_add_tail(&req->tl_requests, &first_peer_device(device)->connection->transfer_log);\r\n}\r\nif (rw == WRITE) {\r\nif (!drbd_process_write_request(req))\r\nno_remote = true;\r\n} else {\r\nif (req->private_bio == NULL) {\r\n_req_mod(req, TO_BE_SENT);\r\n_req_mod(req, QUEUE_FOR_NET_READ);\r\n} else\r\nno_remote = true;\r\n}\r\nif (list_empty(&req->req_pending_master_completion))\r\nlist_add_tail(&req->req_pending_master_completion,\r\n&device->pending_master_completion[rw == WRITE]);\r\nif (req->private_bio) {\r\nlist_add_tail(&req->req_pending_local,\r\n&device->pending_completion[rw == WRITE]);\r\n_req_mod(req, TO_BE_SUBMITTED);\r\nsubmit_private_bio = true;\r\n} else if (no_remote) {\r\nnodata:\r\nif (__ratelimit(&drbd_ratelimit_state))\r\ndrbd_err(device, "IO ERROR: neither local nor remote data, sector %llu+%u\n",\r\n(unsigned long long)req->i.sector, req->i.size >> 9);\r\n}\r\nout:\r\nif (drbd_req_put_completion_ref(req, &m, 1))\r\nkref_put(&req->kref, drbd_req_destroy);\r\nspin_unlock_irq(&resource->req_lock);\r\nif (submit_private_bio)\r\ndrbd_submit_req_private_bio(req);\r\nif (m.bio)\r\ncomplete_master_bio(device, &m);\r\n}\r\nvoid __drbd_make_request(struct drbd_device *device, struct bio *bio, unsigned long start_jif)\r\n{\r\nstruct drbd_request *req = drbd_request_prepare(device, bio, start_jif);\r\nif (IS_ERR_OR_NULL(req))\r\nreturn;\r\ndrbd_send_and_submit(device, req);\r\n}\r\nstatic void submit_fast_path(struct drbd_device *device, struct list_head *incoming)\r\n{\r\nstruct drbd_request *req, *tmp;\r\nlist_for_each_entry_safe(req, tmp, incoming, tl_requests) {\r\nconst int rw = bio_data_dir(req->master_bio);\r\nif (rw == WRITE\r\n&& req->private_bio && req->i.size\r\n&& !test_bit(AL_SUSPENDED, &device->flags)) {\r\nif (!drbd_al_begin_io_fastpath(device, &req->i))\r\ncontinue;\r\nreq->rq_state |= RQ_IN_ACT_LOG;\r\nreq->in_actlog_jif = jiffies;\r\natomic_dec(&device->ap_actlog_cnt);\r\n}\r\nlist_del_init(&req->tl_requests);\r\ndrbd_send_and_submit(device, req);\r\n}\r\n}\r\nstatic bool prepare_al_transaction_nonblock(struct drbd_device *device,\r\nstruct list_head *incoming,\r\nstruct list_head *pending,\r\nstruct list_head *later)\r\n{\r\nstruct drbd_request *req, *tmp;\r\nint wake = 0;\r\nint err;\r\nspin_lock_irq(&device->al_lock);\r\nlist_for_each_entry_safe(req, tmp, incoming, tl_requests) {\r\nerr = drbd_al_begin_io_nonblock(device, &req->i);\r\nif (err == -ENOBUFS)\r\nbreak;\r\nif (err == -EBUSY)\r\nwake = 1;\r\nif (err)\r\nlist_move_tail(&req->tl_requests, later);\r\nelse\r\nlist_move_tail(&req->tl_requests, pending);\r\n}\r\nspin_unlock_irq(&device->al_lock);\r\nif (wake)\r\nwake_up(&device->al_wait);\r\nreturn !list_empty(pending);\r\n}\r\nvoid send_and_submit_pending(struct drbd_device *device, struct list_head *pending)\r\n{\r\nstruct drbd_request *req, *tmp;\r\nlist_for_each_entry_safe(req, tmp, pending, tl_requests) {\r\nreq->rq_state |= RQ_IN_ACT_LOG;\r\nreq->in_actlog_jif = jiffies;\r\natomic_dec(&device->ap_actlog_cnt);\r\nlist_del_init(&req->tl_requests);\r\ndrbd_send_and_submit(device, req);\r\n}\r\n}\r\nvoid do_submit(struct work_struct *ws)\r\n{\r\nstruct drbd_device *device = container_of(ws, struct drbd_device, submit.worker);\r\nLIST_HEAD(incoming);\r\nLIST_HEAD(pending);\r\nLIST_HEAD(busy);\r\nspin_lock_irq(&device->resource->req_lock);\r\nlist_splice_tail_init(&device->submit.writes, &incoming);\r\nspin_unlock_irq(&device->resource->req_lock);\r\nfor (;;) {\r\nDEFINE_WAIT(wait);\r\nlist_splice_init(&busy, &incoming);\r\nsubmit_fast_path(device, &incoming);\r\nif (list_empty(&incoming))\r\nbreak;\r\nfor (;;) {\r\nprepare_to_wait(&device->al_wait, &wait, TASK_UNINTERRUPTIBLE);\r\nlist_splice_init(&busy, &incoming);\r\nprepare_al_transaction_nonblock(device, &incoming, &pending, &busy);\r\nif (!list_empty(&pending))\r\nbreak;\r\nschedule();\r\nif (!list_empty(&incoming))\r\ncontinue;\r\nspin_lock_irq(&device->resource->req_lock);\r\nlist_splice_tail_init(&device->submit.writes, &incoming);\r\nspin_unlock_irq(&device->resource->req_lock);\r\n}\r\nfinish_wait(&device->al_wait, &wait);\r\nwhile (list_empty(&incoming)) {\r\nLIST_HEAD(more_pending);\r\nLIST_HEAD(more_incoming);\r\nbool made_progress;\r\nif (list_empty(&device->submit.writes))\r\nbreak;\r\nspin_lock_irq(&device->resource->req_lock);\r\nlist_splice_tail_init(&device->submit.writes, &more_incoming);\r\nspin_unlock_irq(&device->resource->req_lock);\r\nif (list_empty(&more_incoming))\r\nbreak;\r\nmade_progress = prepare_al_transaction_nonblock(device, &more_incoming, &more_pending, &busy);\r\nlist_splice_tail_init(&more_pending, &pending);\r\nlist_splice_tail_init(&more_incoming, &incoming);\r\nif (!made_progress)\r\nbreak;\r\n}\r\ndrbd_al_begin_io_commit(device);\r\nsend_and_submit_pending(device, &pending);\r\n}\r\n}\r\nvoid drbd_make_request(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct drbd_device *device = (struct drbd_device *) q->queuedata;\r\nunsigned long start_jif;\r\nstart_jif = jiffies;\r\nD_ASSERT(device, IS_ALIGNED(bio->bi_iter.bi_size, 512));\r\ninc_ap_bio(device);\r\n__drbd_make_request(device, bio, start_jif);\r\n}\r\nint drbd_merge_bvec(struct request_queue *q, struct bvec_merge_data *bvm, struct bio_vec *bvec)\r\n{\r\nstruct drbd_device *device = (struct drbd_device *) q->queuedata;\r\nunsigned int bio_size = bvm->bi_size;\r\nint limit = DRBD_MAX_BIO_SIZE;\r\nint backing_limit;\r\nif (bio_size && get_ldev(device)) {\r\nunsigned int max_hw_sectors = queue_max_hw_sectors(q);\r\nstruct request_queue * const b =\r\ndevice->ldev->backing_bdev->bd_disk->queue;\r\nif (b->merge_bvec_fn) {\r\nbvm->bi_bdev = device->ldev->backing_bdev;\r\nbacking_limit = b->merge_bvec_fn(b, bvm, bvec);\r\nlimit = min(limit, backing_limit);\r\n}\r\nput_ldev(device);\r\nif ((limit >> 9) > max_hw_sectors)\r\nlimit = max_hw_sectors << 9;\r\n}\r\nreturn limit;\r\n}\r\nvoid request_timer_fn(unsigned long data)\r\n{\r\nstruct drbd_device *device = (struct drbd_device *) data;\r\nstruct drbd_connection *connection = first_peer_device(device)->connection;\r\nstruct drbd_request *req_read, *req_write, *req_peer;\r\nstruct net_conf *nc;\r\nunsigned long oldest_submit_jif;\r\nunsigned long ent = 0, dt = 0, et, nt;\r\nunsigned long now;\r\nrcu_read_lock();\r\nnc = rcu_dereference(connection->net_conf);\r\nif (nc && device->state.conn >= C_WF_REPORT_PARAMS)\r\nent = nc->timeout * HZ/10 * nc->ko_count;\r\nif (get_ldev(device)) {\r\ndt = rcu_dereference(device->ldev->disk_conf)->disk_timeout * HZ / 10;\r\nput_ldev(device);\r\n}\r\nrcu_read_unlock();\r\net = min_not_zero(dt, ent);\r\nif (!et)\r\nreturn;\r\nnow = jiffies;\r\nnt = now + et;\r\nspin_lock_irq(&device->resource->req_lock);\r\nreq_read = list_first_entry_or_null(&device->pending_completion[0], struct drbd_request, req_pending_local);\r\nreq_write = list_first_entry_or_null(&device->pending_completion[1], struct drbd_request, req_pending_local);\r\nreq_peer = connection->req_not_net_done;\r\nif (!req_peer && connection->req_next && connection->req_next->pre_send_jif)\r\nreq_peer = connection->req_next;\r\nif (req_peer && req_peer->device != device)\r\nreq_peer = NULL;\r\nif (req_peer == NULL && req_write == NULL && req_read == NULL)\r\ngoto out;\r\noldest_submit_jif =\r\n(req_write && req_read)\r\n? ( time_before(req_write->pre_submit_jif, req_read->pre_submit_jif)\r\n? req_write->pre_submit_jif : req_read->pre_submit_jif )\r\n: req_write ? req_write->pre_submit_jif\r\n: req_read ? req_read->pre_submit_jif : now;\r\nif (ent && req_peer &&\r\ntime_after(now, req_peer->pre_send_jif + ent) &&\r\n!time_in_range(now, connection->last_reconnect_jif, connection->last_reconnect_jif + ent)) {\r\ndrbd_warn(device, "Remote failed to finish a request within ko-count * timeout\n");\r\n_conn_request_state(connection, NS(conn, C_TIMEOUT), CS_VERBOSE | CS_HARD);\r\n}\r\nif (dt && oldest_submit_jif != now &&\r\ntime_after(now, oldest_submit_jif + dt) &&\r\n!time_in_range(now, device->last_reattach_jif, device->last_reattach_jif + dt)) {\r\ndrbd_warn(device, "Local backing device failed to meet the disk-timeout\n");\r\n__drbd_chk_io_error(device, DRBD_FORCE_DETACH);\r\n}\r\nent = (ent && req_peer && time_before(now, req_peer->pre_send_jif + ent))\r\n? req_peer->pre_send_jif + ent : now + et;\r\ndt = (dt && oldest_submit_jif != now && time_before(now, oldest_submit_jif + dt))\r\n? oldest_submit_jif + dt : now + et;\r\nnt = time_before(ent, dt) ? ent : dt;\r\nout:\r\nspin_unlock_irq(&device->resource->req_lock);\r\nmod_timer(&device->request_timer, nt);\r\n}
