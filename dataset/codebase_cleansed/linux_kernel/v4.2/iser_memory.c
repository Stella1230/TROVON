static void\r\niser_free_bounce_sg(struct iser_data_buf *data)\r\n{\r\nstruct scatterlist *sg;\r\nint count;\r\nfor_each_sg(data->sg, sg, data->size, count)\r\n__free_page(sg_page(sg));\r\nkfree(data->sg);\r\ndata->sg = data->orig_sg;\r\ndata->size = data->orig_size;\r\ndata->orig_sg = NULL;\r\ndata->orig_size = 0;\r\n}\r\nstatic int\r\niser_alloc_bounce_sg(struct iser_data_buf *data)\r\n{\r\nstruct scatterlist *sg;\r\nstruct page *page;\r\nunsigned long length = data->data_len;\r\nint i = 0, nents = DIV_ROUND_UP(length, PAGE_SIZE);\r\nsg = kcalloc(nents, sizeof(*sg), GFP_ATOMIC);\r\nif (!sg)\r\ngoto err;\r\nsg_init_table(sg, nents);\r\nwhile (length) {\r\nu32 page_len = min_t(u32, length, PAGE_SIZE);\r\npage = alloc_page(GFP_ATOMIC);\r\nif (!page)\r\ngoto err;\r\nsg_set_page(&sg[i], page, page_len, 0);\r\nlength -= page_len;\r\ni++;\r\n}\r\ndata->orig_sg = data->sg;\r\ndata->orig_size = data->size;\r\ndata->sg = sg;\r\ndata->size = nents;\r\nreturn 0;\r\nerr:\r\nfor (; i > 0; i--)\r\n__free_page(sg_page(&sg[i - 1]));\r\nkfree(sg);\r\nreturn -ENOMEM;\r\n}\r\nstatic void\r\niser_copy_bounce(struct iser_data_buf *data, bool to_buffer)\r\n{\r\nstruct scatterlist *osg, *bsg = data->sg;\r\nvoid *oaddr, *baddr;\r\nunsigned int left = data->data_len;\r\nunsigned int bsg_off = 0;\r\nint i;\r\nfor_each_sg(data->orig_sg, osg, data->orig_size, i) {\r\nunsigned int copy_len, osg_off = 0;\r\noaddr = kmap_atomic(sg_page(osg)) + osg->offset;\r\ncopy_len = min(left, osg->length);\r\nwhile (copy_len) {\r\nunsigned int len = min(copy_len, bsg->length - bsg_off);\r\nbaddr = kmap_atomic(sg_page(bsg)) + bsg->offset;\r\nif (to_buffer)\r\nmemcpy(baddr + bsg_off, oaddr + osg_off, len);\r\nelse\r\nmemcpy(oaddr + osg_off, baddr + bsg_off, len);\r\nkunmap_atomic(baddr - bsg->offset);\r\nosg_off += len;\r\nbsg_off += len;\r\ncopy_len -= len;\r\nif (bsg_off >= bsg->length) {\r\nbsg = sg_next(bsg);\r\nbsg_off = 0;\r\n}\r\n}\r\nkunmap_atomic(oaddr - osg->offset);\r\nleft -= osg_off;\r\n}\r\n}\r\nstatic inline void\r\niser_copy_from_bounce(struct iser_data_buf *data)\r\n{\r\niser_copy_bounce(data, false);\r\n}\r\nstatic inline void\r\niser_copy_to_bounce(struct iser_data_buf *data)\r\n{\r\niser_copy_bounce(data, true);\r\n}\r\nstruct fast_reg_descriptor *\r\niser_reg_desc_get(struct ib_conn *ib_conn)\r\n{\r\nstruct fast_reg_descriptor *desc;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ib_conn->lock, flags);\r\ndesc = list_first_entry(&ib_conn->fastreg.pool,\r\nstruct fast_reg_descriptor, list);\r\nlist_del(&desc->list);\r\nspin_unlock_irqrestore(&ib_conn->lock, flags);\r\nreturn desc;\r\n}\r\nvoid\r\niser_reg_desc_put(struct ib_conn *ib_conn,\r\nstruct fast_reg_descriptor *desc)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&ib_conn->lock, flags);\r\nlist_add(&desc->list, &ib_conn->fastreg.pool);\r\nspin_unlock_irqrestore(&ib_conn->lock, flags);\r\n}\r\nstatic int iser_start_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct ib_device *dev = iser_task->iser_conn->ib_conn.device->ib_device;\r\nint rc;\r\nrc = iser_alloc_bounce_sg(data);\r\nif (rc) {\r\niser_err("Failed to allocate bounce for data len %lu\n",\r\ndata->data_len);\r\nreturn rc;\r\n}\r\nif (cmd_dir == ISER_DIR_OUT)\r\niser_copy_to_bounce(data);\r\ndata->dma_nents = ib_dma_map_sg(dev, data->sg, data->size,\r\n(cmd_dir == ISER_DIR_OUT) ?\r\nDMA_TO_DEVICE : DMA_FROM_DEVICE);\r\nif (!data->dma_nents) {\r\niser_err("Got dma_nents %d, something went wrong...\n",\r\ndata->dma_nents);\r\nrc = -ENOMEM;\r\ngoto err;\r\n}\r\nreturn 0;\r\nerr:\r\niser_free_bounce_sg(data);\r\nreturn rc;\r\n}\r\nvoid iser_finalize_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct ib_device *dev = iser_task->iser_conn->ib_conn.device->ib_device;\r\nib_dma_unmap_sg(dev, data->sg, data->size,\r\n(cmd_dir == ISER_DIR_OUT) ?\r\nDMA_TO_DEVICE : DMA_FROM_DEVICE);\r\nif (cmd_dir == ISER_DIR_IN)\r\niser_copy_from_bounce(data);\r\niser_free_bounce_sg(data);\r\n}\r\nstatic int iser_sg_to_page_vec(struct iser_data_buf *data,\r\nstruct ib_device *ibdev, u64 *pages,\r\nint *offset, int *data_size)\r\n{\r\nstruct scatterlist *sg, *sgl = data->sg;\r\nu64 start_addr, end_addr, page, chunk_start = 0;\r\nunsigned long total_sz = 0;\r\nunsigned int dma_len;\r\nint i, new_chunk, cur_page, last_ent = data->dma_nents - 1;\r\n*offset = (u64) sgl[0].offset & ~MASK_4K;\r\nnew_chunk = 1;\r\ncur_page = 0;\r\nfor_each_sg(sgl, sg, data->dma_nents, i) {\r\nstart_addr = ib_sg_dma_address(ibdev, sg);\r\nif (new_chunk)\r\nchunk_start = start_addr;\r\ndma_len = ib_sg_dma_len(ibdev, sg);\r\nend_addr = start_addr + dma_len;\r\ntotal_sz += dma_len;\r\nif (!IS_4K_ALIGNED(end_addr) && i < last_ent) {\r\nnew_chunk = 0;\r\ncontinue;\r\n}\r\nnew_chunk = 1;\r\npage = chunk_start & MASK_4K;\r\ndo {\r\npages[cur_page++] = page;\r\npage += SIZE_4K;\r\n} while (page < end_addr);\r\n}\r\n*data_size = total_sz;\r\niser_dbg("page_vec->data_size:%d cur_page %d\n",\r\n*data_size, cur_page);\r\nreturn cur_page;\r\n}\r\nstatic int iser_data_buf_aligned_len(struct iser_data_buf *data,\r\nstruct ib_device *ibdev)\r\n{\r\nstruct scatterlist *sg, *sgl, *next_sg = NULL;\r\nu64 start_addr, end_addr;\r\nint i, ret_len, start_check = 0;\r\nif (data->dma_nents == 1)\r\nreturn 1;\r\nsgl = data->sg;\r\nstart_addr = ib_sg_dma_address(ibdev, sgl);\r\nfor_each_sg(sgl, sg, data->dma_nents, i) {\r\nif (start_check && !IS_4K_ALIGNED(start_addr))\r\nbreak;\r\nnext_sg = sg_next(sg);\r\nif (!next_sg)\r\nbreak;\r\nend_addr = start_addr + ib_sg_dma_len(ibdev, sg);\r\nstart_addr = ib_sg_dma_address(ibdev, next_sg);\r\nif (end_addr == start_addr) {\r\nstart_check = 0;\r\ncontinue;\r\n} else\r\nstart_check = 1;\r\nif (!IS_4K_ALIGNED(end_addr))\r\nbreak;\r\n}\r\nret_len = (next_sg) ? i : i+1;\r\niser_dbg("Found %d aligned entries out of %d in sg:0x%p\n",\r\nret_len, data->dma_nents, data);\r\nreturn ret_len;\r\n}\r\nstatic void iser_data_buf_dump(struct iser_data_buf *data,\r\nstruct ib_device *ibdev)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(data->sg, sg, data->dma_nents, i)\r\niser_dbg("sg[%d] dma_addr:0x%lX page:0x%p "\r\n"off:0x%x sz:0x%x dma_len:0x%x\n",\r\ni, (unsigned long)ib_sg_dma_address(ibdev, sg),\r\nsg_page(sg), sg->offset,\r\nsg->length, ib_sg_dma_len(ibdev, sg));\r\n}\r\nstatic void iser_dump_page_vec(struct iser_page_vec *page_vec)\r\n{\r\nint i;\r\niser_err("page vec length %d data size %d\n",\r\npage_vec->length, page_vec->data_size);\r\nfor (i = 0; i < page_vec->length; i++)\r\niser_err("%d %lx\n",i,(unsigned long)page_vec->pages[i]);\r\n}\r\nint iser_dma_map_task_data(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data,\r\nenum iser_data_dir iser_dir,\r\nenum dma_data_direction dma_dir)\r\n{\r\nstruct ib_device *dev;\r\niser_task->dir[iser_dir] = 1;\r\ndev = iser_task->iser_conn->ib_conn.device->ib_device;\r\ndata->dma_nents = ib_dma_map_sg(dev, data->sg, data->size, dma_dir);\r\nif (data->dma_nents == 0) {\r\niser_err("dma_map_sg failed!!!\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nvoid iser_dma_unmap_task_data(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data,\r\nenum dma_data_direction dir)\r\n{\r\nstruct ib_device *dev;\r\ndev = iser_task->iser_conn->ib_conn.device->ib_device;\r\nib_dma_unmap_sg(dev, data->sg, data->size, dir);\r\n}\r\nstatic int\r\niser_reg_dma(struct iser_device *device, struct iser_data_buf *mem,\r\nstruct iser_mem_reg *reg)\r\n{\r\nstruct scatterlist *sg = mem->sg;\r\nreg->sge.lkey = device->mr->lkey;\r\nreg->rkey = device->mr->rkey;\r\nreg->sge.addr = ib_sg_dma_address(device->ib_device, &sg[0]);\r\nreg->sge.length = ib_sg_dma_len(device->ib_device, &sg[0]);\r\niser_dbg("Single DMA entry: lkey=0x%x, rkey=0x%x, addr=0x%llx,"\r\n" length=0x%x\n", reg->sge.lkey, reg->rkey,\r\nreg->sge.addr, reg->sge.length);\r\nreturn 0;\r\n}\r\nstatic int fall_to_bounce_buf(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *mem,\r\nenum iser_data_dir cmd_dir,\r\nint aligned_len)\r\n{\r\nstruct iscsi_conn *iscsi_conn = iser_task->iser_conn->iscsi_conn;\r\nstruct iser_device *device = iser_task->iser_conn->ib_conn.device;\r\niscsi_conn->fmr_unalign_cnt++;\r\niser_warn("rdma alignment violation (%d/%d aligned) or FMR not supported\n",\r\naligned_len, mem->size);\r\nif (iser_debug_level > 0)\r\niser_data_buf_dump(mem, device->ib_device);\r\niser_dma_unmap_task_data(iser_task, mem,\r\n(cmd_dir == ISER_DIR_OUT) ?\r\nDMA_TO_DEVICE : DMA_FROM_DEVICE);\r\nif (iser_start_rdma_unaligned_sg(iser_task, mem, cmd_dir) != 0)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic\r\nint iser_reg_page_vec(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *mem,\r\nstruct iser_page_vec *page_vec,\r\nstruct iser_mem_reg *mem_reg)\r\n{\r\nstruct ib_conn *ib_conn = &iser_task->iser_conn->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_pool_fmr *fmr;\r\nint ret, plen;\r\nplen = iser_sg_to_page_vec(mem, device->ib_device,\r\npage_vec->pages,\r\n&page_vec->offset,\r\n&page_vec->data_size);\r\npage_vec->length = plen;\r\nif (plen * SIZE_4K < page_vec->data_size) {\r\niser_err("page vec too short to hold this SG\n");\r\niser_data_buf_dump(mem, device->ib_device);\r\niser_dump_page_vec(page_vec);\r\nreturn -EINVAL;\r\n}\r\nfmr = ib_fmr_pool_map_phys(ib_conn->fmr.pool,\r\npage_vec->pages,\r\npage_vec->length,\r\npage_vec->pages[0]);\r\nif (IS_ERR(fmr)) {\r\nret = PTR_ERR(fmr);\r\niser_err("ib_fmr_pool_map_phys failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nmem_reg->sge.lkey = fmr->fmr->lkey;\r\nmem_reg->rkey = fmr->fmr->rkey;\r\nmem_reg->sge.addr = page_vec->pages[0] + page_vec->offset;\r\nmem_reg->sge.length = page_vec->data_size;\r\nmem_reg->mem_h = fmr;\r\nreturn 0;\r\n}\r\nvoid iser_unreg_mem_fmr(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct iser_mem_reg *reg = &iser_task->rdma_reg[cmd_dir];\r\nint ret;\r\nif (!reg->mem_h)\r\nreturn;\r\niser_dbg("PHYSICAL Mem.Unregister mem_h %p\n", reg->mem_h);\r\nret = ib_fmr_pool_unmap((struct ib_pool_fmr *)reg->mem_h);\r\nif (ret)\r\niser_err("ib_fmr_pool_unmap failed %d\n", ret);\r\nreg->mem_h = NULL;\r\n}\r\nvoid iser_unreg_mem_fastreg(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct iser_mem_reg *reg = &iser_task->rdma_reg[cmd_dir];\r\nif (!reg->mem_h)\r\nreturn;\r\niser_reg_desc_put(&iser_task->iser_conn->ib_conn,\r\nreg->mem_h);\r\nreg->mem_h = NULL;\r\n}\r\nint iser_reg_rdma_mem_fmr(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct ib_conn *ib_conn = &iser_task->iser_conn->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_device *ibdev = device->ib_device;\r\nstruct iser_data_buf *mem = &iser_task->data[cmd_dir];\r\nstruct iser_mem_reg *mem_reg;\r\nint aligned_len;\r\nint err;\r\nint i;\r\nmem_reg = &iser_task->rdma_reg[cmd_dir];\r\naligned_len = iser_data_buf_aligned_len(mem, ibdev);\r\nif (aligned_len != mem->dma_nents) {\r\nerr = fall_to_bounce_buf(iser_task, mem,\r\ncmd_dir, aligned_len);\r\nif (err) {\r\niser_err("failed to allocate bounce buffer\n");\r\nreturn err;\r\n}\r\n}\r\nif (mem->dma_nents == 1) {\r\nreturn iser_reg_dma(device, mem, mem_reg);\r\n} else {\r\nerr = iser_reg_page_vec(iser_task, mem, ib_conn->fmr.page_vec,\r\nmem_reg);\r\nif (err && err != -EAGAIN) {\r\niser_data_buf_dump(mem, ibdev);\r\niser_err("mem->dma_nents = %d (dlength = 0x%x)\n",\r\nmem->dma_nents,\r\nntoh24(iser_task->desc.iscsi_header.dlength));\r\niser_err("page_vec: data_size = 0x%x, length = %d, offset = 0x%x\n",\r\nib_conn->fmr.page_vec->data_size,\r\nib_conn->fmr.page_vec->length,\r\nib_conn->fmr.page_vec->offset);\r\nfor (i = 0; i < ib_conn->fmr.page_vec->length; i++)\r\niser_err("page_vec[%d] = 0x%llx\n", i,\r\n(unsigned long long)ib_conn->fmr.page_vec->pages[i]);\r\n}\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\niser_set_dif_domain(struct scsi_cmnd *sc, struct ib_sig_attrs *sig_attrs,\r\nstruct ib_sig_domain *domain)\r\n{\r\ndomain->sig_type = IB_SIG_TYPE_T10_DIF;\r\ndomain->sig.dif.pi_interval = scsi_prot_interval(sc);\r\ndomain->sig.dif.ref_tag = scsi_prot_ref_tag(sc);\r\ndomain->sig.dif.apptag_check_mask = 0xffff;\r\ndomain->sig.dif.app_escape = true;\r\ndomain->sig.dif.ref_escape = true;\r\nif (sc->prot_flags & SCSI_PROT_REF_INCREMENT)\r\ndomain->sig.dif.ref_remap = true;\r\n}\r\nstatic int\r\niser_set_sig_attrs(struct scsi_cmnd *sc, struct ib_sig_attrs *sig_attrs)\r\n{\r\nswitch (scsi_get_prot_op(sc)) {\r\ncase SCSI_PROT_WRITE_INSERT:\r\ncase SCSI_PROT_READ_STRIP:\r\nsig_attrs->mem.sig_type = IB_SIG_TYPE_NONE;\r\niser_set_dif_domain(sc, sig_attrs, &sig_attrs->wire);\r\nsig_attrs->wire.sig.dif.bg_type = IB_T10DIF_CRC;\r\nbreak;\r\ncase SCSI_PROT_READ_INSERT:\r\ncase SCSI_PROT_WRITE_STRIP:\r\nsig_attrs->wire.sig_type = IB_SIG_TYPE_NONE;\r\niser_set_dif_domain(sc, sig_attrs, &sig_attrs->mem);\r\nsig_attrs->mem.sig.dif.bg_type = sc->prot_flags & SCSI_PROT_IP_CHECKSUM ?\r\nIB_T10DIF_CSUM : IB_T10DIF_CRC;\r\nbreak;\r\ncase SCSI_PROT_READ_PASS:\r\ncase SCSI_PROT_WRITE_PASS:\r\niser_set_dif_domain(sc, sig_attrs, &sig_attrs->wire);\r\nsig_attrs->wire.sig.dif.bg_type = IB_T10DIF_CRC;\r\niser_set_dif_domain(sc, sig_attrs, &sig_attrs->mem);\r\nsig_attrs->mem.sig.dif.bg_type = sc->prot_flags & SCSI_PROT_IP_CHECKSUM ?\r\nIB_T10DIF_CSUM : IB_T10DIF_CRC;\r\nbreak;\r\ndefault:\r\niser_err("Unsupported PI operation %d\n",\r\nscsi_get_prot_op(sc));\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void\r\niser_set_prot_checks(struct scsi_cmnd *sc, u8 *mask)\r\n{\r\n*mask = 0;\r\nif (sc->prot_flags & SCSI_PROT_REF_CHECK)\r\n*mask |= ISER_CHECK_REFTAG;\r\nif (sc->prot_flags & SCSI_PROT_GUARD_CHECK)\r\n*mask |= ISER_CHECK_GUARD;\r\n}\r\nstatic void\r\niser_inv_rkey(struct ib_send_wr *inv_wr, struct ib_mr *mr)\r\n{\r\nu32 rkey;\r\nmemset(inv_wr, 0, sizeof(*inv_wr));\r\ninv_wr->opcode = IB_WR_LOCAL_INV;\r\ninv_wr->wr_id = ISER_FASTREG_LI_WRID;\r\ninv_wr->ex.invalidate_rkey = mr->rkey;\r\nrkey = ib_inc_rkey(mr->rkey);\r\nib_update_fast_reg_key(mr, rkey);\r\n}\r\nstatic int\r\niser_reg_sig_mr(struct iscsi_iser_task *iser_task,\r\nstruct fast_reg_descriptor *desc,\r\nstruct iser_mem_reg *data_reg,\r\nstruct iser_mem_reg *prot_reg,\r\nstruct iser_mem_reg *sig_reg)\r\n{\r\nstruct ib_conn *ib_conn = &iser_task->iser_conn->ib_conn;\r\nstruct iser_pi_context *pi_ctx = desc->pi_ctx;\r\nstruct ib_send_wr sig_wr, inv_wr;\r\nstruct ib_send_wr *bad_wr, *wr = NULL;\r\nstruct ib_sig_attrs sig_attrs;\r\nint ret;\r\nmemset(&sig_attrs, 0, sizeof(sig_attrs));\r\nret = iser_set_sig_attrs(iser_task->sc, &sig_attrs);\r\nif (ret)\r\ngoto err;\r\niser_set_prot_checks(iser_task->sc, &sig_attrs.check_mask);\r\nif (!(desc->reg_indicators & ISER_SIG_KEY_VALID)) {\r\niser_inv_rkey(&inv_wr, pi_ctx->sig_mr);\r\nwr = &inv_wr;\r\n}\r\nmemset(&sig_wr, 0, sizeof(sig_wr));\r\nsig_wr.opcode = IB_WR_REG_SIG_MR;\r\nsig_wr.wr_id = ISER_FASTREG_LI_WRID;\r\nsig_wr.sg_list = &data_reg->sge;\r\nsig_wr.num_sge = 1;\r\nsig_wr.wr.sig_handover.sig_attrs = &sig_attrs;\r\nsig_wr.wr.sig_handover.sig_mr = pi_ctx->sig_mr;\r\nif (scsi_prot_sg_count(iser_task->sc))\r\nsig_wr.wr.sig_handover.prot = &prot_reg->sge;\r\nsig_wr.wr.sig_handover.access_flags = IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE;\r\nif (!wr)\r\nwr = &sig_wr;\r\nelse\r\nwr->next = &sig_wr;\r\nret = ib_post_send(ib_conn->qp, wr, &bad_wr);\r\nif (ret) {\r\niser_err("reg_sig_mr failed, ret:%d\n", ret);\r\ngoto err;\r\n}\r\ndesc->reg_indicators &= ~ISER_SIG_KEY_VALID;\r\nsig_reg->sge.lkey = pi_ctx->sig_mr->lkey;\r\nsig_reg->rkey = pi_ctx->sig_mr->rkey;\r\nsig_reg->sge.addr = 0;\r\nsig_reg->sge.length = scsi_transfer_length(iser_task->sc);\r\niser_dbg("sig_sge: lkey: 0x%x, rkey: 0x%x, addr: 0x%llx, length: %u\n",\r\nsig_reg->sge.lkey, sig_reg->rkey, sig_reg->sge.addr,\r\nsig_reg->sge.length);\r\nerr:\r\nreturn ret;\r\n}\r\nstatic int iser_fast_reg_mr(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *mem,\r\nstruct fast_reg_descriptor *desc,\r\nenum iser_reg_indicator ind,\r\nstruct iser_mem_reg *reg)\r\n{\r\nstruct ib_conn *ib_conn = &iser_task->iser_conn->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_mr *mr;\r\nstruct ib_fast_reg_page_list *frpl;\r\nstruct ib_send_wr fastreg_wr, inv_wr;\r\nstruct ib_send_wr *bad_wr, *wr = NULL;\r\nint ret, offset, size, plen;\r\nif (mem->dma_nents == 1)\r\nreturn iser_reg_dma(device, mem, reg);\r\nif (ind == ISER_DATA_KEY_VALID) {\r\nmr = desc->data_mr;\r\nfrpl = desc->data_frpl;\r\n} else {\r\nmr = desc->pi_ctx->prot_mr;\r\nfrpl = desc->pi_ctx->prot_frpl;\r\n}\r\nplen = iser_sg_to_page_vec(mem, device->ib_device, frpl->page_list,\r\n&offset, &size);\r\nif (plen * SIZE_4K < size) {\r\niser_err("fast reg page_list too short to hold this SG\n");\r\nreturn -EINVAL;\r\n}\r\nif (!(desc->reg_indicators & ind)) {\r\niser_inv_rkey(&inv_wr, mr);\r\nwr = &inv_wr;\r\n}\r\nmemset(&fastreg_wr, 0, sizeof(fastreg_wr));\r\nfastreg_wr.wr_id = ISER_FASTREG_LI_WRID;\r\nfastreg_wr.opcode = IB_WR_FAST_REG_MR;\r\nfastreg_wr.wr.fast_reg.iova_start = frpl->page_list[0] + offset;\r\nfastreg_wr.wr.fast_reg.page_list = frpl;\r\nfastreg_wr.wr.fast_reg.page_list_len = plen;\r\nfastreg_wr.wr.fast_reg.page_shift = SHIFT_4K;\r\nfastreg_wr.wr.fast_reg.length = size;\r\nfastreg_wr.wr.fast_reg.rkey = mr->rkey;\r\nfastreg_wr.wr.fast_reg.access_flags = (IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE |\r\nIB_ACCESS_REMOTE_READ);\r\nif (!wr)\r\nwr = &fastreg_wr;\r\nelse\r\nwr->next = &fastreg_wr;\r\nret = ib_post_send(ib_conn->qp, wr, &bad_wr);\r\nif (ret) {\r\niser_err("fast registration failed, ret:%d\n", ret);\r\nreturn ret;\r\n}\r\ndesc->reg_indicators &= ~ind;\r\nreg->sge.lkey = mr->lkey;\r\nreg->rkey = mr->rkey;\r\nreg->sge.addr = frpl->page_list[0] + offset;\r\nreg->sge.length = size;\r\nreturn ret;\r\n}\r\nint iser_reg_rdma_mem_fastreg(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct ib_conn *ib_conn = &iser_task->iser_conn->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_device *ibdev = device->ib_device;\r\nstruct iser_data_buf *mem = &iser_task->data[cmd_dir];\r\nstruct iser_mem_reg *mem_reg = &iser_task->rdma_reg[cmd_dir];\r\nstruct fast_reg_descriptor *desc = NULL;\r\nint err, aligned_len;\r\naligned_len = iser_data_buf_aligned_len(mem, ibdev);\r\nif (aligned_len != mem->dma_nents) {\r\nerr = fall_to_bounce_buf(iser_task, mem,\r\ncmd_dir, aligned_len);\r\nif (err) {\r\niser_err("failed to allocate bounce buffer\n");\r\nreturn err;\r\n}\r\n}\r\nif (mem->dma_nents != 1 ||\r\nscsi_get_prot_op(iser_task->sc) != SCSI_PROT_NORMAL) {\r\ndesc = iser_reg_desc_get(ib_conn);\r\nmem_reg->mem_h = desc;\r\n}\r\nerr = iser_fast_reg_mr(iser_task, mem, desc,\r\nISER_DATA_KEY_VALID, mem_reg);\r\nif (err)\r\ngoto err_reg;\r\nif (scsi_get_prot_op(iser_task->sc) != SCSI_PROT_NORMAL) {\r\nstruct iser_mem_reg prot_reg;\r\nmemset(&prot_reg, 0, sizeof(prot_reg));\r\nif (scsi_prot_sg_count(iser_task->sc)) {\r\nmem = &iser_task->prot[cmd_dir];\r\naligned_len = iser_data_buf_aligned_len(mem, ibdev);\r\nif (aligned_len != mem->dma_nents) {\r\nerr = fall_to_bounce_buf(iser_task, mem,\r\ncmd_dir, aligned_len);\r\nif (err) {\r\niser_err("failed to allocate bounce buffer\n");\r\nreturn err;\r\n}\r\n}\r\nerr = iser_fast_reg_mr(iser_task, mem, desc,\r\nISER_PROT_KEY_VALID, &prot_reg);\r\nif (err)\r\ngoto err_reg;\r\n}\r\nerr = iser_reg_sig_mr(iser_task, desc, mem_reg,\r\n&prot_reg, mem_reg);\r\nif (err) {\r\niser_err("Failed to register signature mr\n");\r\nreturn err;\r\n}\r\ndesc->reg_indicators |= ISER_FASTREG_PROTECTED;\r\n}\r\nreturn 0;\r\nerr_reg:\r\nif (desc)\r\niser_reg_desc_put(ib_conn, desc);\r\nreturn err;\r\n}
