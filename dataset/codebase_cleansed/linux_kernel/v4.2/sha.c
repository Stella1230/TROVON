static void qce_ahash_done(void *data)\r\n{\r\nstruct crypto_async_request *async_req = data;\r\nstruct ahash_request *req = ahash_request_cast(async_req);\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct qce_sha_reqctx *rctx = ahash_request_ctx(req);\r\nstruct qce_alg_template *tmpl = to_ahash_tmpl(async_req->tfm);\r\nstruct qce_device *qce = tmpl->qce;\r\nstruct qce_result_dump *result = qce->dma.result_buf;\r\nunsigned int digestsize = crypto_ahash_digestsize(ahash);\r\nint error;\r\nu32 status;\r\nerror = qce_dma_terminate_all(&qce->dma);\r\nif (error)\r\ndev_dbg(qce->dev, "ahash dma termination error (%d)\n", error);\r\nqce_unmapsg(qce->dev, req->src, rctx->src_nents, DMA_TO_DEVICE,\r\nrctx->src_chained);\r\nqce_unmapsg(qce->dev, &rctx->result_sg, 1, DMA_FROM_DEVICE, 0);\r\nmemcpy(rctx->digest, result->auth_iv, digestsize);\r\nif (req->result)\r\nmemcpy(req->result, result->auth_iv, digestsize);\r\nrctx->byte_count[0] = cpu_to_be32(result->auth_byte_count[0]);\r\nrctx->byte_count[1] = cpu_to_be32(result->auth_byte_count[1]);\r\nerror = qce_check_status(qce, &status);\r\nif (error < 0)\r\ndev_dbg(qce->dev, "ahash operation error (%x)\n", status);\r\nreq->src = rctx->src_orig;\r\nreq->nbytes = rctx->nbytes_orig;\r\nrctx->last_blk = false;\r\nrctx->first_blk = false;\r\nqce->async_req_done(tmpl->qce, error);\r\n}\r\nstatic int qce_ahash_async_req_handle(struct crypto_async_request *async_req)\r\n{\r\nstruct ahash_request *req = ahash_request_cast(async_req);\r\nstruct qce_sha_reqctx *rctx = ahash_request_ctx(req);\r\nstruct qce_sha_ctx *ctx = crypto_tfm_ctx(async_req->tfm);\r\nstruct qce_alg_template *tmpl = to_ahash_tmpl(async_req->tfm);\r\nstruct qce_device *qce = tmpl->qce;\r\nunsigned long flags = rctx->flags;\r\nint ret;\r\nif (IS_SHA_HMAC(flags)) {\r\nrctx->authkey = ctx->authkey;\r\nrctx->authklen = QCE_SHA_HMAC_KEY_SIZE;\r\n} else if (IS_CMAC(flags)) {\r\nrctx->authkey = ctx->authkey;\r\nrctx->authklen = AES_KEYSIZE_128;\r\n}\r\nrctx->src_nents = qce_countsg(req->src, req->nbytes,\r\n&rctx->src_chained);\r\nret = qce_mapsg(qce->dev, req->src, rctx->src_nents, DMA_TO_DEVICE,\r\nrctx->src_chained);\r\nif (ret < 0)\r\nreturn ret;\r\nsg_init_one(&rctx->result_sg, qce->dma.result_buf, QCE_RESULT_BUF_SZ);\r\nret = qce_mapsg(qce->dev, &rctx->result_sg, 1, DMA_FROM_DEVICE, 0);\r\nif (ret < 0)\r\ngoto error_unmap_src;\r\nret = qce_dma_prep_sgs(&qce->dma, req->src, rctx->src_nents,\r\n&rctx->result_sg, 1, qce_ahash_done, async_req);\r\nif (ret)\r\ngoto error_unmap_dst;\r\nqce_dma_issue_pending(&qce->dma);\r\nret = qce_start(async_req, tmpl->crypto_alg_type, 0, 0);\r\nif (ret)\r\ngoto error_terminate;\r\nreturn 0;\r\nerror_terminate:\r\nqce_dma_terminate_all(&qce->dma);\r\nerror_unmap_dst:\r\nqce_unmapsg(qce->dev, &rctx->result_sg, 1, DMA_FROM_DEVICE, 0);\r\nerror_unmap_src:\r\nqce_unmapsg(qce->dev, req->src, rctx->src_nents, DMA_TO_DEVICE,\r\nrctx->src_chained);\r\nreturn ret;\r\n}\r\nstatic int qce_ahash_init(struct ahash_request *req)\r\n{\r\nstruct qce_sha_reqctx *rctx = ahash_request_ctx(req);\r\nstruct qce_alg_template *tmpl = to_ahash_tmpl(req->base.tfm);\r\nconst u32 *std_iv = tmpl->std_iv;\r\nmemset(rctx, 0, sizeof(*rctx));\r\nrctx->first_blk = true;\r\nrctx->last_blk = false;\r\nrctx->flags = tmpl->alg_flags;\r\nmemcpy(rctx->digest, std_iv, sizeof(rctx->digest));\r\nreturn 0;\r\n}\r\nstatic int qce_ahash_export(struct ahash_request *req, void *out)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct qce_sha_reqctx *rctx = ahash_request_ctx(req);\r\nunsigned long flags = rctx->flags;\r\nunsigned int digestsize = crypto_ahash_digestsize(ahash);\r\nunsigned int blocksize =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(ahash));\r\nif (IS_SHA1(flags) || IS_SHA1_HMAC(flags)) {\r\nstruct sha1_state *out_state = out;\r\nout_state->count = rctx->count;\r\nqce_cpu_to_be32p_array((__be32 *)out_state->state,\r\nrctx->digest, digestsize);\r\nmemcpy(out_state->buffer, rctx->buf, blocksize);\r\n} else if (IS_SHA256(flags) || IS_SHA256_HMAC(flags)) {\r\nstruct sha256_state *out_state = out;\r\nout_state->count = rctx->count;\r\nqce_cpu_to_be32p_array((__be32 *)out_state->state,\r\nrctx->digest, digestsize);\r\nmemcpy(out_state->buf, rctx->buf, blocksize);\r\n} else {\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int qce_import_common(struct ahash_request *req, u64 in_count,\r\nconst u32 *state, const u8 *buffer, bool hmac)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct qce_sha_reqctx *rctx = ahash_request_ctx(req);\r\nunsigned int digestsize = crypto_ahash_digestsize(ahash);\r\nunsigned int blocksize;\r\nu64 count = in_count;\r\nblocksize = crypto_tfm_alg_blocksize(crypto_ahash_tfm(ahash));\r\nrctx->count = in_count;\r\nmemcpy(rctx->buf, buffer, blocksize);\r\nif (in_count <= blocksize) {\r\nrctx->first_blk = 1;\r\n} else {\r\nrctx->first_blk = 0;\r\nif (hmac)\r\ncount += SHA_PADDING;\r\n}\r\nrctx->byte_count[0] = (__force __be32)(count & ~SHA_PADDING_MASK);\r\nrctx->byte_count[1] = (__force __be32)(count >> 32);\r\nqce_cpu_to_be32p_array((__be32 *)rctx->digest, (const u8 *)state,\r\ndigestsize);\r\nrctx->buflen = (unsigned int)(in_count & (blocksize - 1));\r\nreturn 0;\r\n}\r\nstatic int qce_ahash_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct qce_sha_reqctx *rctx = ahash_request_ctx(req);\r\nunsigned long flags = rctx->flags;\r\nbool hmac = IS_SHA_HMAC(flags);\r\nint ret = -EINVAL;\r\nif (IS_SHA1(flags) || IS_SHA1_HMAC(flags)) {\r\nconst struct sha1_state *state = in;\r\nret = qce_import_common(req, state->count, state->state,\r\nstate->buffer, hmac);\r\n} else if (IS_SHA256(flags) || IS_SHA256_HMAC(flags)) {\r\nconst struct sha256_state *state = in;\r\nret = qce_import_common(req, state->count, state->state,\r\nstate->buf, hmac);\r\n}\r\nreturn ret;\r\n}\r\nstatic int qce_ahash_update(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct qce_sha_reqctx *rctx = ahash_request_ctx(req);\r\nstruct qce_alg_template *tmpl = to_ahash_tmpl(req->base.tfm);\r\nstruct qce_device *qce = tmpl->qce;\r\nstruct scatterlist *sg_last, *sg;\r\nunsigned int total, len;\r\nunsigned int hash_later;\r\nunsigned int nbytes;\r\nunsigned int blocksize;\r\nblocksize = crypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nrctx->count += req->nbytes;\r\ntotal = req->nbytes + rctx->buflen;\r\nif (total <= blocksize) {\r\nscatterwalk_map_and_copy(rctx->buf + rctx->buflen, req->src,\r\n0, req->nbytes, 0);\r\nrctx->buflen += req->nbytes;\r\nreturn 0;\r\n}\r\nrctx->src_orig = req->src;\r\nrctx->nbytes_orig = req->nbytes;\r\nif (rctx->buflen)\r\nmemcpy(rctx->tmpbuf, rctx->buf, rctx->buflen);\r\nhash_later = total % blocksize;\r\nif (hash_later) {\r\nunsigned int src_offset = req->nbytes - hash_later;\r\nscatterwalk_map_and_copy(rctx->buf, req->src, src_offset,\r\nhash_later, 0);\r\n}\r\nnbytes = total - hash_later;\r\nlen = rctx->buflen;\r\nsg = sg_last = req->src;\r\nwhile (len < nbytes && sg) {\r\nif (len + sg_dma_len(sg) > nbytes)\r\nbreak;\r\nlen += sg_dma_len(sg);\r\nsg_last = sg;\r\nsg = sg_next(sg);\r\n}\r\nif (!sg_last)\r\nreturn -EINVAL;\r\nsg_mark_end(sg_last);\r\nif (rctx->buflen) {\r\nsg_init_table(rctx->sg, 2);\r\nsg_set_buf(rctx->sg, rctx->tmpbuf, rctx->buflen);\r\nscatterwalk_sg_chain(rctx->sg, 2, req->src);\r\nreq->src = rctx->sg;\r\n}\r\nreq->nbytes = nbytes;\r\nrctx->buflen = hash_later;\r\nreturn qce->async_req_enqueue(tmpl->qce, &req->base);\r\n}\r\nstatic int qce_ahash_final(struct ahash_request *req)\r\n{\r\nstruct qce_sha_reqctx *rctx = ahash_request_ctx(req);\r\nstruct qce_alg_template *tmpl = to_ahash_tmpl(req->base.tfm);\r\nstruct qce_device *qce = tmpl->qce;\r\nif (!rctx->buflen)\r\nreturn 0;\r\nrctx->last_blk = true;\r\nrctx->src_orig = req->src;\r\nrctx->nbytes_orig = req->nbytes;\r\nmemcpy(rctx->tmpbuf, rctx->buf, rctx->buflen);\r\nsg_init_one(rctx->sg, rctx->tmpbuf, rctx->buflen);\r\nreq->src = rctx->sg;\r\nreq->nbytes = rctx->buflen;\r\nreturn qce->async_req_enqueue(tmpl->qce, &req->base);\r\n}\r\nstatic int qce_ahash_digest(struct ahash_request *req)\r\n{\r\nstruct qce_sha_reqctx *rctx = ahash_request_ctx(req);\r\nstruct qce_alg_template *tmpl = to_ahash_tmpl(req->base.tfm);\r\nstruct qce_device *qce = tmpl->qce;\r\nint ret;\r\nret = qce_ahash_init(req);\r\nif (ret)\r\nreturn ret;\r\nrctx->src_orig = req->src;\r\nrctx->nbytes_orig = req->nbytes;\r\nrctx->first_blk = true;\r\nrctx->last_blk = true;\r\nreturn qce->async_req_enqueue(tmpl->qce, &req->base);\r\n}\r\nstatic void qce_digest_complete(struct crypto_async_request *req, int error)\r\n{\r\nstruct qce_ahash_result *result = req->data;\r\nif (error == -EINPROGRESS)\r\nreturn;\r\nresult->error = error;\r\ncomplete(&result->completion);\r\n}\r\nstatic int qce_ahash_hmac_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nunsigned int digestsize = crypto_ahash_digestsize(tfm);\r\nstruct qce_sha_ctx *ctx = crypto_tfm_ctx(&tfm->base);\r\nstruct qce_ahash_result result;\r\nstruct ahash_request *req;\r\nstruct scatterlist sg;\r\nunsigned int blocksize;\r\nstruct crypto_ahash *ahash_tfm;\r\nu8 *buf;\r\nint ret;\r\nconst char *alg_name;\r\nblocksize = crypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nmemset(ctx->authkey, 0, sizeof(ctx->authkey));\r\nif (keylen <= blocksize) {\r\nmemcpy(ctx->authkey, key, keylen);\r\nreturn 0;\r\n}\r\nif (digestsize == SHA1_DIGEST_SIZE)\r\nalg_name = "sha1-qce";\r\nelse if (digestsize == SHA256_DIGEST_SIZE)\r\nalg_name = "sha256-qce";\r\nelse\r\nreturn -EINVAL;\r\nahash_tfm = crypto_alloc_ahash(alg_name, CRYPTO_ALG_TYPE_AHASH,\r\nCRYPTO_ALG_TYPE_AHASH_MASK);\r\nif (IS_ERR(ahash_tfm))\r\nreturn PTR_ERR(ahash_tfm);\r\nreq = ahash_request_alloc(ahash_tfm, GFP_KERNEL);\r\nif (!req) {\r\nret = -ENOMEM;\r\ngoto err_free_ahash;\r\n}\r\ninit_completion(&result.completion);\r\nahash_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG,\r\nqce_digest_complete, &result);\r\ncrypto_ahash_clear_flags(ahash_tfm, ~0);\r\nbuf = kzalloc(keylen + QCE_MAX_ALIGN_SIZE, GFP_KERNEL);\r\nif (!buf) {\r\nret = -ENOMEM;\r\ngoto err_free_req;\r\n}\r\nmemcpy(buf, key, keylen);\r\nsg_init_one(&sg, buf, keylen);\r\nahash_request_set_crypt(req, &sg, ctx->authkey, keylen);\r\nret = crypto_ahash_digest(req);\r\nif (ret == -EINPROGRESS || ret == -EBUSY) {\r\nret = wait_for_completion_interruptible(&result.completion);\r\nif (!ret)\r\nret = result.error;\r\n}\r\nif (ret)\r\ncrypto_ahash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nkfree(buf);\r\nerr_free_req:\r\nahash_request_free(req);\r\nerr_free_ahash:\r\ncrypto_free_ahash(ahash_tfm);\r\nreturn ret;\r\n}\r\nstatic int qce_ahash_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\r\nstruct qce_sha_ctx *ctx = crypto_tfm_ctx(tfm);\r\ncrypto_ahash_set_reqsize(ahash, sizeof(struct qce_sha_reqctx));\r\nmemset(ctx, 0, sizeof(*ctx));\r\nreturn 0;\r\n}\r\nstatic int qce_ahash_register_one(const struct qce_ahash_def *def,\r\nstruct qce_device *qce)\r\n{\r\nstruct qce_alg_template *tmpl;\r\nstruct ahash_alg *alg;\r\nstruct crypto_alg *base;\r\nint ret;\r\ntmpl = kzalloc(sizeof(*tmpl), GFP_KERNEL);\r\nif (!tmpl)\r\nreturn -ENOMEM;\r\ntmpl->std_iv = def->std_iv;\r\nalg = &tmpl->alg.ahash;\r\nalg->init = qce_ahash_init;\r\nalg->update = qce_ahash_update;\r\nalg->final = qce_ahash_final;\r\nalg->digest = qce_ahash_digest;\r\nalg->export = qce_ahash_export;\r\nalg->import = qce_ahash_import;\r\nif (IS_SHA_HMAC(def->flags))\r\nalg->setkey = qce_ahash_hmac_setkey;\r\nalg->halg.digestsize = def->digestsize;\r\nalg->halg.statesize = def->statesize;\r\nbase = &alg->halg.base;\r\nbase->cra_blocksize = def->blocksize;\r\nbase->cra_priority = 300;\r\nbase->cra_flags = CRYPTO_ALG_ASYNC;\r\nbase->cra_ctxsize = sizeof(struct qce_sha_ctx);\r\nbase->cra_alignmask = 0;\r\nbase->cra_module = THIS_MODULE;\r\nbase->cra_init = qce_ahash_cra_init;\r\nINIT_LIST_HEAD(&base->cra_list);\r\nsnprintf(base->cra_name, CRYPTO_MAX_ALG_NAME, "%s", def->name);\r\nsnprintf(base->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ndef->drv_name);\r\nINIT_LIST_HEAD(&tmpl->entry);\r\ntmpl->crypto_alg_type = CRYPTO_ALG_TYPE_AHASH;\r\ntmpl->alg_flags = def->flags;\r\ntmpl->qce = qce;\r\nret = crypto_register_ahash(alg);\r\nif (ret) {\r\nkfree(tmpl);\r\ndev_err(qce->dev, "%s registration failed\n", base->cra_name);\r\nreturn ret;\r\n}\r\nlist_add_tail(&tmpl->entry, &ahash_algs);\r\ndev_dbg(qce->dev, "%s is registered\n", base->cra_name);\r\nreturn 0;\r\n}\r\nstatic void qce_ahash_unregister(struct qce_device *qce)\r\n{\r\nstruct qce_alg_template *tmpl, *n;\r\nlist_for_each_entry_safe(tmpl, n, &ahash_algs, entry) {\r\ncrypto_unregister_ahash(&tmpl->alg.ahash);\r\nlist_del(&tmpl->entry);\r\nkfree(tmpl);\r\n}\r\n}\r\nstatic int qce_ahash_register(struct qce_device *qce)\r\n{\r\nint ret, i;\r\nfor (i = 0; i < ARRAY_SIZE(ahash_def); i++) {\r\nret = qce_ahash_register_one(&ahash_def[i], qce);\r\nif (ret)\r\ngoto err;\r\n}\r\nreturn 0;\r\nerr:\r\nqce_ahash_unregister(qce);\r\nreturn ret;\r\n}
