void inet_peer_base_init(struct inet_peer_base *bp)\r\n{\r\nbp->root = peer_avl_empty_rcu;\r\nseqlock_init(&bp->lock);\r\nbp->total = 0;\r\n}\r\nstatic void inetpeer_gc_worker(struct work_struct *work)\r\n{\r\nstruct inet_peer *p, *n, *c;\r\nstruct list_head list;\r\nspin_lock_bh(&gc_lock);\r\nlist_replace_init(&gc_list, &list);\r\nspin_unlock_bh(&gc_lock);\r\nif (list_empty(&list))\r\nreturn;\r\nlist_for_each_entry_safe(p, n, &list, gc_list) {\r\nif (need_resched())\r\ncond_resched();\r\nc = rcu_dereference_protected(p->avl_left, 1);\r\nif (c != peer_avl_empty) {\r\nlist_add_tail(&c->gc_list, &list);\r\np->avl_left = peer_avl_empty_rcu;\r\n}\r\nc = rcu_dereference_protected(p->avl_right, 1);\r\nif (c != peer_avl_empty) {\r\nlist_add_tail(&c->gc_list, &list);\r\np->avl_right = peer_avl_empty_rcu;\r\n}\r\nn = list_entry(p->gc_list.next, struct inet_peer, gc_list);\r\nif (!atomic_read(&p->refcnt)) {\r\nlist_del(&p->gc_list);\r\nkmem_cache_free(peer_cachep, p);\r\n}\r\n}\r\nif (list_empty(&list))\r\nreturn;\r\nspin_lock_bh(&gc_lock);\r\nlist_splice(&list, &gc_list);\r\nspin_unlock_bh(&gc_lock);\r\nschedule_delayed_work(&gc_work, gc_delay);\r\n}\r\nvoid __init inet_initpeers(void)\r\n{\r\nstruct sysinfo si;\r\nsi_meminfo(&si);\r\nif (si.totalram <= (32768*1024)/PAGE_SIZE)\r\ninet_peer_threshold >>= 1;\r\nif (si.totalram <= (16384*1024)/PAGE_SIZE)\r\ninet_peer_threshold >>= 1;\r\nif (si.totalram <= (8192*1024)/PAGE_SIZE)\r\ninet_peer_threshold >>= 2;\r\npeer_cachep = kmem_cache_create("inet_peer_cache",\r\nsizeof(struct inet_peer),\r\n0, SLAB_HWCACHE_ALIGN | SLAB_PANIC,\r\nNULL);\r\nINIT_DEFERRABLE_WORK(&gc_work, inetpeer_gc_worker);\r\n}\r\nstatic int addr_compare(const struct inetpeer_addr *a,\r\nconst struct inetpeer_addr *b)\r\n{\r\nint i, n = (a->family == AF_INET ? 1 : 4);\r\nfor (i = 0; i < n; i++) {\r\nif (a->addr.a6[i] == b->addr.a6[i])\r\ncontinue;\r\nif ((__force u32)a->addr.a6[i] < (__force u32)b->addr.a6[i])\r\nreturn -1;\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct inet_peer *lookup_rcu(const struct inetpeer_addr *daddr,\r\nstruct inet_peer_base *base)\r\n{\r\nstruct inet_peer *u = rcu_dereference(base->root);\r\nint count = 0;\r\nwhile (u != peer_avl_empty) {\r\nint cmp = addr_compare(daddr, &u->daddr);\r\nif (cmp == 0) {\r\nif (!atomic_add_unless(&u->refcnt, 1, -1))\r\nu = NULL;\r\nreturn u;\r\n}\r\nif (cmp == -1)\r\nu = rcu_dereference(u->avl_left);\r\nelse\r\nu = rcu_dereference(u->avl_right);\r\nif (unlikely(++count == PEER_MAXDEPTH))\r\nbreak;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void peer_avl_rebalance(struct inet_peer __rcu **stack[],\r\nstruct inet_peer __rcu ***stackend,\r\nstruct inet_peer_base *base)\r\n{\r\nstruct inet_peer __rcu **nodep;\r\nstruct inet_peer *node, *l, *r;\r\nint lh, rh;\r\nwhile (stackend > stack) {\r\nnodep = *--stackend;\r\nnode = rcu_deref_locked(*nodep, base);\r\nl = rcu_deref_locked(node->avl_left, base);\r\nr = rcu_deref_locked(node->avl_right, base);\r\nlh = node_height(l);\r\nrh = node_height(r);\r\nif (lh > rh + 1) {\r\nstruct inet_peer *ll, *lr, *lrl, *lrr;\r\nint lrh;\r\nll = rcu_deref_locked(l->avl_left, base);\r\nlr = rcu_deref_locked(l->avl_right, base);\r\nlrh = node_height(lr);\r\nif (lrh <= node_height(ll)) {\r\nRCU_INIT_POINTER(node->avl_left, lr);\r\nRCU_INIT_POINTER(node->avl_right, r);\r\nnode->avl_height = lrh + 1;\r\nRCU_INIT_POINTER(l->avl_left, ll);\r\nRCU_INIT_POINTER(l->avl_right, node);\r\nl->avl_height = node->avl_height + 1;\r\nRCU_INIT_POINTER(*nodep, l);\r\n} else {\r\nlrl = rcu_deref_locked(lr->avl_left, base);\r\nlrr = rcu_deref_locked(lr->avl_right, base);\r\nRCU_INIT_POINTER(node->avl_left, lrr);\r\nRCU_INIT_POINTER(node->avl_right, r);\r\nnode->avl_height = rh + 1;\r\nRCU_INIT_POINTER(l->avl_left, ll);\r\nRCU_INIT_POINTER(l->avl_right, lrl);\r\nl->avl_height = rh + 1;\r\nRCU_INIT_POINTER(lr->avl_left, l);\r\nRCU_INIT_POINTER(lr->avl_right, node);\r\nlr->avl_height = rh + 2;\r\nRCU_INIT_POINTER(*nodep, lr);\r\n}\r\n} else if (rh > lh + 1) {\r\nstruct inet_peer *rr, *rl, *rlr, *rll;\r\nint rlh;\r\nrr = rcu_deref_locked(r->avl_right, base);\r\nrl = rcu_deref_locked(r->avl_left, base);\r\nrlh = node_height(rl);\r\nif (rlh <= node_height(rr)) {\r\nRCU_INIT_POINTER(node->avl_right, rl);\r\nRCU_INIT_POINTER(node->avl_left, l);\r\nnode->avl_height = rlh + 1;\r\nRCU_INIT_POINTER(r->avl_right, rr);\r\nRCU_INIT_POINTER(r->avl_left, node);\r\nr->avl_height = node->avl_height + 1;\r\nRCU_INIT_POINTER(*nodep, r);\r\n} else {\r\nrlr = rcu_deref_locked(rl->avl_right, base);\r\nrll = rcu_deref_locked(rl->avl_left, base);\r\nRCU_INIT_POINTER(node->avl_right, rll);\r\nRCU_INIT_POINTER(node->avl_left, l);\r\nnode->avl_height = lh + 1;\r\nRCU_INIT_POINTER(r->avl_right, rr);\r\nRCU_INIT_POINTER(r->avl_left, rlr);\r\nr->avl_height = lh + 1;\r\nRCU_INIT_POINTER(rl->avl_right, r);\r\nRCU_INIT_POINTER(rl->avl_left, node);\r\nrl->avl_height = lh + 2;\r\nRCU_INIT_POINTER(*nodep, rl);\r\n}\r\n} else {\r\nnode->avl_height = (lh > rh ? lh : rh) + 1;\r\n}\r\n}\r\n}\r\nstatic void inetpeer_free_rcu(struct rcu_head *head)\r\n{\r\nkmem_cache_free(peer_cachep, container_of(head, struct inet_peer, rcu));\r\n}\r\nstatic void unlink_from_pool(struct inet_peer *p, struct inet_peer_base *base,\r\nstruct inet_peer __rcu **stack[PEER_MAXDEPTH])\r\n{\r\nstruct inet_peer __rcu ***stackptr, ***delp;\r\nif (lookup(&p->daddr, stack, base) != p)\r\nBUG();\r\ndelp = stackptr - 1;\r\nif (p->avl_left == peer_avl_empty_rcu) {\r\n*delp[0] = p->avl_right;\r\n--stackptr;\r\n} else {\r\nstruct inet_peer *t;\r\nt = lookup_rightempty(p, base);\r\nBUG_ON(rcu_deref_locked(*stackptr[-1], base) != t);\r\n**--stackptr = t->avl_left;\r\nRCU_INIT_POINTER(*delp[0], t);\r\nt->avl_left = p->avl_left;\r\nt->avl_right = p->avl_right;\r\nt->avl_height = p->avl_height;\r\nBUG_ON(delp[1] != &p->avl_left);\r\ndelp[1] = &t->avl_left;\r\n}\r\npeer_avl_rebalance(stack, stackptr, base);\r\nbase->total--;\r\ncall_rcu(&p->rcu, inetpeer_free_rcu);\r\n}\r\nstatic int inet_peer_gc(struct inet_peer_base *base,\r\nstruct inet_peer __rcu **stack[PEER_MAXDEPTH],\r\nstruct inet_peer __rcu ***stackptr)\r\n{\r\nstruct inet_peer *p, *gchead = NULL;\r\n__u32 delta, ttl;\r\nint cnt = 0;\r\nif (base->total >= inet_peer_threshold)\r\nttl = 0;\r\nelse\r\nttl = inet_peer_maxttl\r\n- (inet_peer_maxttl - inet_peer_minttl) / HZ *\r\nbase->total / inet_peer_threshold * HZ;\r\nstackptr--;\r\nwhile (stackptr > stack) {\r\nstackptr--;\r\np = rcu_deref_locked(**stackptr, base);\r\nif (atomic_read(&p->refcnt) == 0) {\r\nsmp_rmb();\r\ndelta = (__u32)jiffies - p->dtime;\r\nif (delta >= ttl &&\r\natomic_cmpxchg(&p->refcnt, 0, -1) == 0) {\r\np->gc_next = gchead;\r\ngchead = p;\r\n}\r\n}\r\n}\r\nwhile ((p = gchead) != NULL) {\r\ngchead = p->gc_next;\r\ncnt++;\r\nunlink_from_pool(p, base, stack);\r\n}\r\nreturn cnt;\r\n}\r\nstruct inet_peer *inet_getpeer(struct inet_peer_base *base,\r\nconst struct inetpeer_addr *daddr,\r\nint create)\r\n{\r\nstruct inet_peer __rcu **stack[PEER_MAXDEPTH], ***stackptr;\r\nstruct inet_peer *p;\r\nunsigned int sequence;\r\nint invalidated, gccnt = 0;\r\nrcu_read_lock();\r\nsequence = read_seqbegin(&base->lock);\r\np = lookup_rcu(daddr, base);\r\ninvalidated = read_seqretry(&base->lock, sequence);\r\nrcu_read_unlock();\r\nif (p)\r\nreturn p;\r\nif (!create && !invalidated)\r\nreturn NULL;\r\nwrite_seqlock_bh(&base->lock);\r\nrelookup:\r\np = lookup(daddr, stack, base);\r\nif (p != peer_avl_empty) {\r\natomic_inc(&p->refcnt);\r\nwrite_sequnlock_bh(&base->lock);\r\nreturn p;\r\n}\r\nif (!gccnt) {\r\ngccnt = inet_peer_gc(base, stack, stackptr);\r\nif (gccnt && create)\r\ngoto relookup;\r\n}\r\np = create ? kmem_cache_alloc(peer_cachep, GFP_ATOMIC) : NULL;\r\nif (p) {\r\np->daddr = *daddr;\r\natomic_set(&p->refcnt, 1);\r\natomic_set(&p->rid, 0);\r\np->metrics[RTAX_LOCK-1] = INETPEER_METRICS_NEW;\r\np->rate_tokens = 0;\r\np->rate_last = jiffies - 60*HZ;\r\nINIT_LIST_HEAD(&p->gc_list);\r\nlink_to_pool(p, base);\r\nbase->total++;\r\n}\r\nwrite_sequnlock_bh(&base->lock);\r\nreturn p;\r\n}\r\nvoid inet_putpeer(struct inet_peer *p)\r\n{\r\np->dtime = (__u32)jiffies;\r\nsmp_mb__before_atomic();\r\natomic_dec(&p->refcnt);\r\n}\r\nbool inet_peer_xrlim_allow(struct inet_peer *peer, int timeout)\r\n{\r\nunsigned long now, token;\r\nbool rc = false;\r\nif (!peer)\r\nreturn true;\r\ntoken = peer->rate_tokens;\r\nnow = jiffies;\r\ntoken += now - peer->rate_last;\r\npeer->rate_last = now;\r\nif (token > XRLIM_BURST_FACTOR * timeout)\r\ntoken = XRLIM_BURST_FACTOR * timeout;\r\nif (token >= timeout) {\r\ntoken -= timeout;\r\nrc = true;\r\n}\r\npeer->rate_tokens = token;\r\nreturn rc;\r\n}\r\nstatic void inetpeer_inval_rcu(struct rcu_head *head)\r\n{\r\nstruct inet_peer *p = container_of(head, struct inet_peer, gc_rcu);\r\nspin_lock_bh(&gc_lock);\r\nlist_add_tail(&p->gc_list, &gc_list);\r\nspin_unlock_bh(&gc_lock);\r\nschedule_delayed_work(&gc_work, gc_delay);\r\n}\r\nvoid inetpeer_invalidate_tree(struct inet_peer_base *base)\r\n{\r\nstruct inet_peer *root;\r\nwrite_seqlock_bh(&base->lock);\r\nroot = rcu_deref_locked(base->root, base);\r\nif (root != peer_avl_empty) {\r\nbase->root = peer_avl_empty_rcu;\r\nbase->total = 0;\r\ncall_rcu(&root->gc_rcu, inetpeer_inval_rcu);\r\n}\r\nwrite_sequnlock_bh(&base->lock);\r\n}
