void *ion_heap_map_kernel(struct ion_heap *heap,\r\nstruct ion_buffer *buffer)\r\n{\r\nstruct scatterlist *sg;\r\nint i, j;\r\nvoid *vaddr;\r\npgprot_t pgprot;\r\nstruct sg_table *table = buffer->sg_table;\r\nint npages = PAGE_ALIGN(buffer->size) / PAGE_SIZE;\r\nstruct page **pages = vmalloc(sizeof(struct page *) * npages);\r\nstruct page **tmp = pages;\r\nif (!pages)\r\nreturn NULL;\r\nif (buffer->flags & ION_FLAG_CACHED)\r\npgprot = PAGE_KERNEL;\r\nelse\r\npgprot = pgprot_writecombine(PAGE_KERNEL);\r\nfor_each_sg(table->sgl, sg, table->nents, i) {\r\nint npages_this_entry = PAGE_ALIGN(sg->length) / PAGE_SIZE;\r\nstruct page *page = sg_page(sg);\r\nBUG_ON(i >= npages);\r\nfor (j = 0; j < npages_this_entry; j++)\r\n*(tmp++) = page++;\r\n}\r\nvaddr = vmap(pages, npages, VM_MAP, pgprot);\r\nvfree(pages);\r\nif (vaddr == NULL)\r\nreturn ERR_PTR(-ENOMEM);\r\nreturn vaddr;\r\n}\r\nvoid ion_heap_unmap_kernel(struct ion_heap *heap,\r\nstruct ion_buffer *buffer)\r\n{\r\nvunmap(buffer->vaddr);\r\n}\r\nint ion_heap_map_user(struct ion_heap *heap, struct ion_buffer *buffer,\r\nstruct vm_area_struct *vma)\r\n{\r\nstruct sg_table *table = buffer->sg_table;\r\nunsigned long addr = vma->vm_start;\r\nunsigned long offset = vma->vm_pgoff * PAGE_SIZE;\r\nstruct scatterlist *sg;\r\nint i;\r\nint ret;\r\nfor_each_sg(table->sgl, sg, table->nents, i) {\r\nstruct page *page = sg_page(sg);\r\nunsigned long remainder = vma->vm_end - addr;\r\nunsigned long len = sg->length;\r\nif (offset >= sg->length) {\r\noffset -= sg->length;\r\ncontinue;\r\n} else if (offset) {\r\npage += offset / PAGE_SIZE;\r\nlen = sg->length - offset;\r\noffset = 0;\r\n}\r\nlen = min(len, remainder);\r\nret = remap_pfn_range(vma, addr, page_to_pfn(page), len,\r\nvma->vm_page_prot);\r\nif (ret)\r\nreturn ret;\r\naddr += len;\r\nif (addr >= vma->vm_end)\r\nreturn 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ion_heap_clear_pages(struct page **pages, int num, pgprot_t pgprot)\r\n{\r\nvoid *addr = vm_map_ram(pages, num, -1, pgprot);\r\nif (!addr)\r\nreturn -ENOMEM;\r\nmemset(addr, 0, PAGE_SIZE * num);\r\nvm_unmap_ram(addr, num);\r\nreturn 0;\r\n}\r\nstatic int ion_heap_sglist_zero(struct scatterlist *sgl, unsigned int nents,\r\npgprot_t pgprot)\r\n{\r\nint p = 0;\r\nint ret = 0;\r\nstruct sg_page_iter piter;\r\nstruct page *pages[32];\r\nfor_each_sg_page(sgl, &piter, nents, 0) {\r\npages[p++] = sg_page_iter_page(&piter);\r\nif (p == ARRAY_SIZE(pages)) {\r\nret = ion_heap_clear_pages(pages, p, pgprot);\r\nif (ret)\r\nreturn ret;\r\np = 0;\r\n}\r\n}\r\nif (p)\r\nret = ion_heap_clear_pages(pages, p, pgprot);\r\nreturn ret;\r\n}\r\nint ion_heap_buffer_zero(struct ion_buffer *buffer)\r\n{\r\nstruct sg_table *table = buffer->sg_table;\r\npgprot_t pgprot;\r\nif (buffer->flags & ION_FLAG_CACHED)\r\npgprot = PAGE_KERNEL;\r\nelse\r\npgprot = pgprot_writecombine(PAGE_KERNEL);\r\nreturn ion_heap_sglist_zero(table->sgl, table->nents, pgprot);\r\n}\r\nint ion_heap_pages_zero(struct page *page, size_t size, pgprot_t pgprot)\r\n{\r\nstruct scatterlist sg;\r\nsg_init_table(&sg, 1);\r\nsg_set_page(&sg, page, size, 0);\r\nreturn ion_heap_sglist_zero(&sg, 1, pgprot);\r\n}\r\nvoid ion_heap_freelist_add(struct ion_heap *heap, struct ion_buffer *buffer)\r\n{\r\nspin_lock(&heap->free_lock);\r\nlist_add(&buffer->list, &heap->free_list);\r\nheap->free_list_size += buffer->size;\r\nspin_unlock(&heap->free_lock);\r\nwake_up(&heap->waitqueue);\r\n}\r\nsize_t ion_heap_freelist_size(struct ion_heap *heap)\r\n{\r\nsize_t size;\r\nspin_lock(&heap->free_lock);\r\nsize = heap->free_list_size;\r\nspin_unlock(&heap->free_lock);\r\nreturn size;\r\n}\r\nstatic size_t _ion_heap_freelist_drain(struct ion_heap *heap, size_t size,\r\nbool skip_pools)\r\n{\r\nstruct ion_buffer *buffer;\r\nsize_t total_drained = 0;\r\nif (ion_heap_freelist_size(heap) == 0)\r\nreturn 0;\r\nspin_lock(&heap->free_lock);\r\nif (size == 0)\r\nsize = heap->free_list_size;\r\nwhile (!list_empty(&heap->free_list)) {\r\nif (total_drained >= size)\r\nbreak;\r\nbuffer = list_first_entry(&heap->free_list, struct ion_buffer,\r\nlist);\r\nlist_del(&buffer->list);\r\nheap->free_list_size -= buffer->size;\r\nif (skip_pools)\r\nbuffer->private_flags |= ION_PRIV_FLAG_SHRINKER_FREE;\r\ntotal_drained += buffer->size;\r\nspin_unlock(&heap->free_lock);\r\nion_buffer_destroy(buffer);\r\nspin_lock(&heap->free_lock);\r\n}\r\nspin_unlock(&heap->free_lock);\r\nreturn total_drained;\r\n}\r\nsize_t ion_heap_freelist_drain(struct ion_heap *heap, size_t size)\r\n{\r\nreturn _ion_heap_freelist_drain(heap, size, false);\r\n}\r\nsize_t ion_heap_freelist_shrink(struct ion_heap *heap, size_t size)\r\n{\r\nreturn _ion_heap_freelist_drain(heap, size, true);\r\n}\r\nstatic int ion_heap_deferred_free(void *data)\r\n{\r\nstruct ion_heap *heap = data;\r\nwhile (true) {\r\nstruct ion_buffer *buffer;\r\nwait_event_freezable(heap->waitqueue,\r\nion_heap_freelist_size(heap) > 0);\r\nspin_lock(&heap->free_lock);\r\nif (list_empty(&heap->free_list)) {\r\nspin_unlock(&heap->free_lock);\r\ncontinue;\r\n}\r\nbuffer = list_first_entry(&heap->free_list, struct ion_buffer,\r\nlist);\r\nlist_del(&buffer->list);\r\nheap->free_list_size -= buffer->size;\r\nspin_unlock(&heap->free_lock);\r\nion_buffer_destroy(buffer);\r\n}\r\nreturn 0;\r\n}\r\nint ion_heap_init_deferred_free(struct ion_heap *heap)\r\n{\r\nstruct sched_param param = { .sched_priority = 0 };\r\nINIT_LIST_HEAD(&heap->free_list);\r\ninit_waitqueue_head(&heap->waitqueue);\r\nheap->task = kthread_run(ion_heap_deferred_free, heap,\r\n"%s", heap->name);\r\nif (IS_ERR(heap->task)) {\r\npr_err("%s: creating thread for deferred free failed\n",\r\n__func__);\r\nreturn PTR_ERR_OR_ZERO(heap->task);\r\n}\r\nsched_setscheduler(heap->task, SCHED_IDLE, &param);\r\nreturn 0;\r\n}\r\nstatic unsigned long ion_heap_shrink_count(struct shrinker *shrinker,\r\nstruct shrink_control *sc)\r\n{\r\nstruct ion_heap *heap = container_of(shrinker, struct ion_heap,\r\nshrinker);\r\nint total = 0;\r\ntotal = ion_heap_freelist_size(heap) / PAGE_SIZE;\r\nif (heap->ops->shrink)\r\ntotal += heap->ops->shrink(heap, sc->gfp_mask, 0);\r\nreturn total;\r\n}\r\nstatic unsigned long ion_heap_shrink_scan(struct shrinker *shrinker,\r\nstruct shrink_control *sc)\r\n{\r\nstruct ion_heap *heap = container_of(shrinker, struct ion_heap,\r\nshrinker);\r\nint freed = 0;\r\nint to_scan = sc->nr_to_scan;\r\nif (to_scan == 0)\r\nreturn 0;\r\nif (heap->flags & ION_HEAP_FLAG_DEFER_FREE)\r\nfreed = ion_heap_freelist_shrink(heap, to_scan * PAGE_SIZE) /\r\nPAGE_SIZE;\r\nto_scan -= freed;\r\nif (to_scan <= 0)\r\nreturn freed;\r\nif (heap->ops->shrink)\r\nfreed += heap->ops->shrink(heap, sc->gfp_mask, to_scan);\r\nreturn freed;\r\n}\r\nvoid ion_heap_init_shrinker(struct ion_heap *heap)\r\n{\r\nheap->shrinker.count_objects = ion_heap_shrink_count;\r\nheap->shrinker.scan_objects = ion_heap_shrink_scan;\r\nheap->shrinker.seeks = DEFAULT_SEEKS;\r\nheap->shrinker.batch = 0;\r\nregister_shrinker(&heap->shrinker);\r\n}\r\nstruct ion_heap *ion_heap_create(struct ion_platform_heap *heap_data)\r\n{\r\nstruct ion_heap *heap = NULL;\r\nswitch (heap_data->type) {\r\ncase ION_HEAP_TYPE_SYSTEM_CONTIG:\r\nheap = ion_system_contig_heap_create(heap_data);\r\nbreak;\r\ncase ION_HEAP_TYPE_SYSTEM:\r\nheap = ion_system_heap_create(heap_data);\r\nbreak;\r\ncase ION_HEAP_TYPE_CARVEOUT:\r\nheap = ion_carveout_heap_create(heap_data);\r\nbreak;\r\ncase ION_HEAP_TYPE_CHUNK:\r\nheap = ion_chunk_heap_create(heap_data);\r\nbreak;\r\ncase ION_HEAP_TYPE_DMA:\r\nheap = ion_cma_heap_create(heap_data);\r\nbreak;\r\ndefault:\r\npr_err("%s: Invalid heap type %d\n", __func__,\r\nheap_data->type);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nif (IS_ERR_OR_NULL(heap)) {\r\npr_err("%s: error creating heap %s type %d base %lu size %zu\n",\r\n__func__, heap_data->name, heap_data->type,\r\nheap_data->base, heap_data->size);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nheap->name = heap_data->name;\r\nheap->id = heap_data->id;\r\nreturn heap;\r\n}\r\nvoid ion_heap_destroy(struct ion_heap *heap)\r\n{\r\nif (!heap)\r\nreturn;\r\nswitch (heap->type) {\r\ncase ION_HEAP_TYPE_SYSTEM_CONTIG:\r\nion_system_contig_heap_destroy(heap);\r\nbreak;\r\ncase ION_HEAP_TYPE_SYSTEM:\r\nion_system_heap_destroy(heap);\r\nbreak;\r\ncase ION_HEAP_TYPE_CARVEOUT:\r\nion_carveout_heap_destroy(heap);\r\nbreak;\r\ncase ION_HEAP_TYPE_CHUNK:\r\nion_chunk_heap_destroy(heap);\r\nbreak;\r\ncase ION_HEAP_TYPE_DMA:\r\nion_cma_heap_destroy(heap);\r\nbreak;\r\ndefault:\r\npr_err("%s: Invalid heap type %d\n", __func__,\r\nheap->type);\r\n}\r\n}
