static inline void count_compact_event(enum vm_event_item item)\r\n{\r\ncount_vm_event(item);\r\n}\r\nstatic inline void count_compact_events(enum vm_event_item item, long delta)\r\n{\r\ncount_vm_events(item, delta);\r\n}\r\nstatic unsigned long release_freepages(struct list_head *freelist)\r\n{\r\nstruct page *page, *next;\r\nunsigned long high_pfn = 0;\r\nlist_for_each_entry_safe(page, next, freelist, lru) {\r\nunsigned long pfn = page_to_pfn(page);\r\nlist_del(&page->lru);\r\n__free_page(page);\r\nif (pfn > high_pfn)\r\nhigh_pfn = pfn;\r\n}\r\nreturn high_pfn;\r\n}\r\nstatic void map_pages(struct list_head *list)\r\n{\r\nstruct page *page;\r\nlist_for_each_entry(page, list, lru) {\r\narch_alloc_page(page, 0);\r\nkernel_map_pages(page, 1, 1);\r\nkasan_alloc_pages(page, 0);\r\n}\r\n}\r\nstatic inline bool migrate_async_suitable(int migratetype)\r\n{\r\nreturn is_migrate_cma(migratetype) || migratetype == MIGRATE_MOVABLE;\r\n}\r\nstatic struct page *pageblock_pfn_to_page(unsigned long start_pfn,\r\nunsigned long end_pfn, struct zone *zone)\r\n{\r\nstruct page *start_page;\r\nstruct page *end_page;\r\nend_pfn--;\r\nif (!pfn_valid(start_pfn) || !pfn_valid(end_pfn))\r\nreturn NULL;\r\nstart_page = pfn_to_page(start_pfn);\r\nif (page_zone(start_page) != zone)\r\nreturn NULL;\r\nend_page = pfn_to_page(end_pfn);\r\nif (page_zone_id(start_page) != page_zone_id(end_page))\r\nreturn NULL;\r\nreturn start_page;\r\n}\r\nvoid defer_compaction(struct zone *zone, int order)\r\n{\r\nzone->compact_considered = 0;\r\nzone->compact_defer_shift++;\r\nif (order < zone->compact_order_failed)\r\nzone->compact_order_failed = order;\r\nif (zone->compact_defer_shift > COMPACT_MAX_DEFER_SHIFT)\r\nzone->compact_defer_shift = COMPACT_MAX_DEFER_SHIFT;\r\ntrace_mm_compaction_defer_compaction(zone, order);\r\n}\r\nbool compaction_deferred(struct zone *zone, int order)\r\n{\r\nunsigned long defer_limit = 1UL << zone->compact_defer_shift;\r\nif (order < zone->compact_order_failed)\r\nreturn false;\r\nif (++zone->compact_considered > defer_limit)\r\nzone->compact_considered = defer_limit;\r\nif (zone->compact_considered >= defer_limit)\r\nreturn false;\r\ntrace_mm_compaction_deferred(zone, order);\r\nreturn true;\r\n}\r\nvoid compaction_defer_reset(struct zone *zone, int order,\r\nbool alloc_success)\r\n{\r\nif (alloc_success) {\r\nzone->compact_considered = 0;\r\nzone->compact_defer_shift = 0;\r\n}\r\nif (order >= zone->compact_order_failed)\r\nzone->compact_order_failed = order + 1;\r\ntrace_mm_compaction_defer_reset(zone, order);\r\n}\r\nbool compaction_restarting(struct zone *zone, int order)\r\n{\r\nif (order < zone->compact_order_failed)\r\nreturn false;\r\nreturn zone->compact_defer_shift == COMPACT_MAX_DEFER_SHIFT &&\r\nzone->compact_considered >= 1UL << zone->compact_defer_shift;\r\n}\r\nstatic inline bool isolation_suitable(struct compact_control *cc,\r\nstruct page *page)\r\n{\r\nif (cc->ignore_skip_hint)\r\nreturn true;\r\nreturn !get_pageblock_skip(page);\r\n}\r\nstatic void __reset_isolation_suitable(struct zone *zone)\r\n{\r\nunsigned long start_pfn = zone->zone_start_pfn;\r\nunsigned long end_pfn = zone_end_pfn(zone);\r\nunsigned long pfn;\r\nzone->compact_cached_migrate_pfn[0] = start_pfn;\r\nzone->compact_cached_migrate_pfn[1] = start_pfn;\r\nzone->compact_cached_free_pfn = end_pfn;\r\nzone->compact_blockskip_flush = false;\r\nfor (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {\r\nstruct page *page;\r\ncond_resched();\r\nif (!pfn_valid(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (zone != page_zone(page))\r\ncontinue;\r\nclear_pageblock_skip(page);\r\n}\r\n}\r\nvoid reset_isolation_suitable(pg_data_t *pgdat)\r\n{\r\nint zoneid;\r\nfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\r\nstruct zone *zone = &pgdat->node_zones[zoneid];\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (zone->compact_blockskip_flush)\r\n__reset_isolation_suitable(zone);\r\n}\r\n}\r\nstatic void update_pageblock_skip(struct compact_control *cc,\r\nstruct page *page, unsigned long nr_isolated,\r\nbool migrate_scanner)\r\n{\r\nstruct zone *zone = cc->zone;\r\nunsigned long pfn;\r\nif (cc->ignore_skip_hint)\r\nreturn;\r\nif (!page)\r\nreturn;\r\nif (nr_isolated)\r\nreturn;\r\nset_pageblock_skip(page);\r\npfn = page_to_pfn(page);\r\nif (migrate_scanner) {\r\nif (pfn > zone->compact_cached_migrate_pfn[0])\r\nzone->compact_cached_migrate_pfn[0] = pfn;\r\nif (cc->mode != MIGRATE_ASYNC &&\r\npfn > zone->compact_cached_migrate_pfn[1])\r\nzone->compact_cached_migrate_pfn[1] = pfn;\r\n} else {\r\nif (pfn < zone->compact_cached_free_pfn)\r\nzone->compact_cached_free_pfn = pfn;\r\n}\r\n}\r\nstatic inline bool isolation_suitable(struct compact_control *cc,\r\nstruct page *page)\r\n{\r\nreturn true;\r\n}\r\nstatic void update_pageblock_skip(struct compact_control *cc,\r\nstruct page *page, unsigned long nr_isolated,\r\nbool migrate_scanner)\r\n{\r\n}\r\nstatic bool compact_trylock_irqsave(spinlock_t *lock, unsigned long *flags,\r\nstruct compact_control *cc)\r\n{\r\nif (cc->mode == MIGRATE_ASYNC) {\r\nif (!spin_trylock_irqsave(lock, *flags)) {\r\ncc->contended = COMPACT_CONTENDED_LOCK;\r\nreturn false;\r\n}\r\n} else {\r\nspin_lock_irqsave(lock, *flags);\r\n}\r\nreturn true;\r\n}\r\nstatic bool compact_unlock_should_abort(spinlock_t *lock,\r\nunsigned long flags, bool *locked, struct compact_control *cc)\r\n{\r\nif (*locked) {\r\nspin_unlock_irqrestore(lock, flags);\r\n*locked = false;\r\n}\r\nif (fatal_signal_pending(current)) {\r\ncc->contended = COMPACT_CONTENDED_SCHED;\r\nreturn true;\r\n}\r\nif (need_resched()) {\r\nif (cc->mode == MIGRATE_ASYNC) {\r\ncc->contended = COMPACT_CONTENDED_SCHED;\r\nreturn true;\r\n}\r\ncond_resched();\r\n}\r\nreturn false;\r\n}\r\nstatic inline bool compact_should_abort(struct compact_control *cc)\r\n{\r\nif (need_resched()) {\r\nif (cc->mode == MIGRATE_ASYNC) {\r\ncc->contended = COMPACT_CONTENDED_SCHED;\r\nreturn true;\r\n}\r\ncond_resched();\r\n}\r\nreturn false;\r\n}\r\nstatic unsigned long isolate_freepages_block(struct compact_control *cc,\r\nunsigned long *start_pfn,\r\nunsigned long end_pfn,\r\nstruct list_head *freelist,\r\nbool strict)\r\n{\r\nint nr_scanned = 0, total_isolated = 0;\r\nstruct page *cursor, *valid_page = NULL;\r\nunsigned long flags = 0;\r\nbool locked = false;\r\nunsigned long blockpfn = *start_pfn;\r\ncursor = pfn_to_page(blockpfn);\r\nfor (; blockpfn < end_pfn; blockpfn++, cursor++) {\r\nint isolated, i;\r\nstruct page *page = cursor;\r\nif (!(blockpfn % SWAP_CLUSTER_MAX)\r\n&& compact_unlock_should_abort(&cc->zone->lock, flags,\r\n&locked, cc))\r\nbreak;\r\nnr_scanned++;\r\nif (!pfn_valid_within(blockpfn))\r\ngoto isolate_fail;\r\nif (!valid_page)\r\nvalid_page = page;\r\nif (!PageBuddy(page))\r\ngoto isolate_fail;\r\nif (!locked) {\r\nlocked = compact_trylock_irqsave(&cc->zone->lock,\r\n&flags, cc);\r\nif (!locked)\r\nbreak;\r\nif (!PageBuddy(page))\r\ngoto isolate_fail;\r\n}\r\nisolated = split_free_page(page);\r\ntotal_isolated += isolated;\r\nfor (i = 0; i < isolated; i++) {\r\nlist_add(&page->lru, freelist);\r\npage++;\r\n}\r\nif (isolated) {\r\ncc->nr_freepages += isolated;\r\nif (!strict &&\r\ncc->nr_migratepages <= cc->nr_freepages) {\r\nblockpfn += isolated;\r\nbreak;\r\n}\r\nblockpfn += isolated - 1;\r\ncursor += isolated - 1;\r\ncontinue;\r\n}\r\nisolate_fail:\r\nif (strict)\r\nbreak;\r\nelse\r\ncontinue;\r\n}\r\ntrace_mm_compaction_isolate_freepages(*start_pfn, blockpfn,\r\nnr_scanned, total_isolated);\r\n*start_pfn = blockpfn;\r\nif (strict && blockpfn < end_pfn)\r\ntotal_isolated = 0;\r\nif (locked)\r\nspin_unlock_irqrestore(&cc->zone->lock, flags);\r\nif (blockpfn == end_pfn)\r\nupdate_pageblock_skip(cc, valid_page, total_isolated, false);\r\ncount_compact_events(COMPACTFREE_SCANNED, nr_scanned);\r\nif (total_isolated)\r\ncount_compact_events(COMPACTISOLATED, total_isolated);\r\nreturn total_isolated;\r\n}\r\nunsigned long\r\nisolate_freepages_range(struct compact_control *cc,\r\nunsigned long start_pfn, unsigned long end_pfn)\r\n{\r\nunsigned long isolated, pfn, block_end_pfn;\r\nLIST_HEAD(freelist);\r\npfn = start_pfn;\r\nblock_end_pfn = ALIGN(pfn + 1, pageblock_nr_pages);\r\nfor (; pfn < end_pfn; pfn += isolated,\r\nblock_end_pfn += pageblock_nr_pages) {\r\nunsigned long isolate_start_pfn = pfn;\r\nblock_end_pfn = min(block_end_pfn, end_pfn);\r\nif (pfn >= block_end_pfn) {\r\nblock_end_pfn = ALIGN(pfn + 1, pageblock_nr_pages);\r\nblock_end_pfn = min(block_end_pfn, end_pfn);\r\n}\r\nif (!pageblock_pfn_to_page(pfn, block_end_pfn, cc->zone))\r\nbreak;\r\nisolated = isolate_freepages_block(cc, &isolate_start_pfn,\r\nblock_end_pfn, &freelist, true);\r\nif (!isolated)\r\nbreak;\r\n}\r\nmap_pages(&freelist);\r\nif (pfn < end_pfn) {\r\nrelease_freepages(&freelist);\r\nreturn 0;\r\n}\r\nreturn pfn;\r\n}\r\nstatic void acct_isolated(struct zone *zone, struct compact_control *cc)\r\n{\r\nstruct page *page;\r\nunsigned int count[2] = { 0, };\r\nif (list_empty(&cc->migratepages))\r\nreturn;\r\nlist_for_each_entry(page, &cc->migratepages, lru)\r\ncount[!!page_is_file_cache(page)]++;\r\nmod_zone_page_state(zone, NR_ISOLATED_ANON, count[0]);\r\nmod_zone_page_state(zone, NR_ISOLATED_FILE, count[1]);\r\n}\r\nstatic bool too_many_isolated(struct zone *zone)\r\n{\r\nunsigned long active, inactive, isolated;\r\ninactive = zone_page_state(zone, NR_INACTIVE_FILE) +\r\nzone_page_state(zone, NR_INACTIVE_ANON);\r\nactive = zone_page_state(zone, NR_ACTIVE_FILE) +\r\nzone_page_state(zone, NR_ACTIVE_ANON);\r\nisolated = zone_page_state(zone, NR_ISOLATED_FILE) +\r\nzone_page_state(zone, NR_ISOLATED_ANON);\r\nreturn isolated > (inactive + active) / 2;\r\n}\r\nstatic unsigned long\r\nisolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\r\nunsigned long end_pfn, isolate_mode_t isolate_mode)\r\n{\r\nstruct zone *zone = cc->zone;\r\nunsigned long nr_scanned = 0, nr_isolated = 0;\r\nstruct list_head *migratelist = &cc->migratepages;\r\nstruct lruvec *lruvec;\r\nunsigned long flags = 0;\r\nbool locked = false;\r\nstruct page *page = NULL, *valid_page = NULL;\r\nunsigned long start_pfn = low_pfn;\r\nwhile (unlikely(too_many_isolated(zone))) {\r\nif (cc->mode == MIGRATE_ASYNC)\r\nreturn 0;\r\ncongestion_wait(BLK_RW_ASYNC, HZ/10);\r\nif (fatal_signal_pending(current))\r\nreturn 0;\r\n}\r\nif (compact_should_abort(cc))\r\nreturn 0;\r\nfor (; low_pfn < end_pfn; low_pfn++) {\r\nif (!(low_pfn % SWAP_CLUSTER_MAX)\r\n&& compact_unlock_should_abort(&zone->lru_lock, flags,\r\n&locked, cc))\r\nbreak;\r\nif (!pfn_valid_within(low_pfn))\r\ncontinue;\r\nnr_scanned++;\r\npage = pfn_to_page(low_pfn);\r\nif (!valid_page)\r\nvalid_page = page;\r\nif (PageBuddy(page)) {\r\nunsigned long freepage_order = page_order_unsafe(page);\r\nif (freepage_order > 0 && freepage_order < MAX_ORDER)\r\nlow_pfn += (1UL << freepage_order) - 1;\r\ncontinue;\r\n}\r\nif (!PageLRU(page)) {\r\nif (unlikely(balloon_page_movable(page))) {\r\nif (balloon_page_isolate(page)) {\r\ngoto isolate_success;\r\n}\r\n}\r\ncontinue;\r\n}\r\nif (PageTransHuge(page)) {\r\nif (!locked)\r\nlow_pfn = ALIGN(low_pfn + 1,\r\npageblock_nr_pages) - 1;\r\nelse\r\nlow_pfn += (1 << compound_order(page)) - 1;\r\ncontinue;\r\n}\r\nif (!page_mapping(page) &&\r\npage_count(page) > page_mapcount(page))\r\ncontinue;\r\nif (!locked) {\r\nlocked = compact_trylock_irqsave(&zone->lru_lock,\r\n&flags, cc);\r\nif (!locked)\r\nbreak;\r\nif (!PageLRU(page))\r\ncontinue;\r\nif (PageTransHuge(page)) {\r\nlow_pfn += (1 << compound_order(page)) - 1;\r\ncontinue;\r\n}\r\n}\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nif (__isolate_lru_page(page, isolate_mode) != 0)\r\ncontinue;\r\nVM_BUG_ON_PAGE(PageTransCompound(page), page);\r\ndel_page_from_lru_list(page, lruvec, page_lru(page));\r\nisolate_success:\r\nlist_add(&page->lru, migratelist);\r\ncc->nr_migratepages++;\r\nnr_isolated++;\r\nif (cc->nr_migratepages == COMPACT_CLUSTER_MAX) {\r\n++low_pfn;\r\nbreak;\r\n}\r\n}\r\nif (unlikely(low_pfn > end_pfn))\r\nlow_pfn = end_pfn;\r\nif (locked)\r\nspin_unlock_irqrestore(&zone->lru_lock, flags);\r\nif (low_pfn == end_pfn)\r\nupdate_pageblock_skip(cc, valid_page, nr_isolated, true);\r\ntrace_mm_compaction_isolate_migratepages(start_pfn, low_pfn,\r\nnr_scanned, nr_isolated);\r\ncount_compact_events(COMPACTMIGRATE_SCANNED, nr_scanned);\r\nif (nr_isolated)\r\ncount_compact_events(COMPACTISOLATED, nr_isolated);\r\nreturn low_pfn;\r\n}\r\nunsigned long\r\nisolate_migratepages_range(struct compact_control *cc, unsigned long start_pfn,\r\nunsigned long end_pfn)\r\n{\r\nunsigned long pfn, block_end_pfn;\r\npfn = start_pfn;\r\nblock_end_pfn = ALIGN(pfn + 1, pageblock_nr_pages);\r\nfor (; pfn < end_pfn; pfn = block_end_pfn,\r\nblock_end_pfn += pageblock_nr_pages) {\r\nblock_end_pfn = min(block_end_pfn, end_pfn);\r\nif (!pageblock_pfn_to_page(pfn, block_end_pfn, cc->zone))\r\ncontinue;\r\npfn = isolate_migratepages_block(cc, pfn, block_end_pfn,\r\nISOLATE_UNEVICTABLE);\r\nif (!pfn) {\r\nputback_movable_pages(&cc->migratepages);\r\ncc->nr_migratepages = 0;\r\nbreak;\r\n}\r\nif (cc->nr_migratepages == COMPACT_CLUSTER_MAX)\r\nbreak;\r\n}\r\nacct_isolated(cc->zone, cc);\r\nreturn pfn;\r\n}\r\nstatic bool suitable_migration_target(struct page *page)\r\n{\r\nif (PageBuddy(page)) {\r\nif (page_order_unsafe(page) >= pageblock_order)\r\nreturn false;\r\n}\r\nif (migrate_async_suitable(get_pageblock_migratetype(page)))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void isolate_freepages(struct compact_control *cc)\r\n{\r\nstruct zone *zone = cc->zone;\r\nstruct page *page;\r\nunsigned long block_start_pfn;\r\nunsigned long isolate_start_pfn;\r\nunsigned long block_end_pfn;\r\nunsigned long low_pfn;\r\nstruct list_head *freelist = &cc->freepages;\r\nisolate_start_pfn = cc->free_pfn;\r\nblock_start_pfn = cc->free_pfn & ~(pageblock_nr_pages-1);\r\nblock_end_pfn = min(block_start_pfn + pageblock_nr_pages,\r\nzone_end_pfn(zone));\r\nlow_pfn = ALIGN(cc->migrate_pfn + 1, pageblock_nr_pages);\r\nfor (; block_start_pfn >= low_pfn &&\r\ncc->nr_migratepages > cc->nr_freepages;\r\nblock_end_pfn = block_start_pfn,\r\nblock_start_pfn -= pageblock_nr_pages,\r\nisolate_start_pfn = block_start_pfn) {\r\nif (!(block_start_pfn % (SWAP_CLUSTER_MAX * pageblock_nr_pages))\r\n&& compact_should_abort(cc))\r\nbreak;\r\npage = pageblock_pfn_to_page(block_start_pfn, block_end_pfn,\r\nzone);\r\nif (!page)\r\ncontinue;\r\nif (!suitable_migration_target(page))\r\ncontinue;\r\nif (!isolation_suitable(cc, page))\r\ncontinue;\r\nisolate_freepages_block(cc, &isolate_start_pfn,\r\nblock_end_pfn, freelist, false);\r\ncc->free_pfn = (isolate_start_pfn < block_end_pfn) ?\r\nisolate_start_pfn :\r\nblock_start_pfn - pageblock_nr_pages;\r\nif (cc->contended)\r\nbreak;\r\n}\r\nmap_pages(freelist);\r\nif (block_start_pfn < low_pfn)\r\ncc->free_pfn = cc->migrate_pfn;\r\n}\r\nstatic struct page *compaction_alloc(struct page *migratepage,\r\nunsigned long data,\r\nint **result)\r\n{\r\nstruct compact_control *cc = (struct compact_control *)data;\r\nstruct page *freepage;\r\nif (list_empty(&cc->freepages)) {\r\nif (!cc->contended)\r\nisolate_freepages(cc);\r\nif (list_empty(&cc->freepages))\r\nreturn NULL;\r\n}\r\nfreepage = list_entry(cc->freepages.next, struct page, lru);\r\nlist_del(&freepage->lru);\r\ncc->nr_freepages--;\r\nreturn freepage;\r\n}\r\nstatic void compaction_free(struct page *page, unsigned long data)\r\n{\r\nstruct compact_control *cc = (struct compact_control *)data;\r\nlist_add(&page->lru, &cc->freepages);\r\ncc->nr_freepages++;\r\n}\r\nstatic isolate_migrate_t isolate_migratepages(struct zone *zone,\r\nstruct compact_control *cc)\r\n{\r\nunsigned long low_pfn, end_pfn;\r\nstruct page *page;\r\nconst isolate_mode_t isolate_mode =\r\n(sysctl_compact_unevictable_allowed ? ISOLATE_UNEVICTABLE : 0) |\r\n(cc->mode == MIGRATE_ASYNC ? ISOLATE_ASYNC_MIGRATE : 0);\r\nlow_pfn = cc->migrate_pfn;\r\nend_pfn = ALIGN(low_pfn + 1, pageblock_nr_pages);\r\nfor (; end_pfn <= cc->free_pfn;\r\nlow_pfn = end_pfn, end_pfn += pageblock_nr_pages) {\r\nif (!(low_pfn % (SWAP_CLUSTER_MAX * pageblock_nr_pages))\r\n&& compact_should_abort(cc))\r\nbreak;\r\npage = pageblock_pfn_to_page(low_pfn, end_pfn, zone);\r\nif (!page)\r\ncontinue;\r\nif (!isolation_suitable(cc, page))\r\ncontinue;\r\nif (cc->mode == MIGRATE_ASYNC &&\r\n!migrate_async_suitable(get_pageblock_migratetype(page)))\r\ncontinue;\r\nlow_pfn = isolate_migratepages_block(cc, low_pfn, end_pfn,\r\nisolate_mode);\r\nif (!low_pfn || cc->contended) {\r\nacct_isolated(zone, cc);\r\nreturn ISOLATE_ABORT;\r\n}\r\nbreak;\r\n}\r\nacct_isolated(zone, cc);\r\ncc->migrate_pfn = (end_pfn <= cc->free_pfn) ? low_pfn : cc->free_pfn;\r\nreturn cc->nr_migratepages ? ISOLATE_SUCCESS : ISOLATE_NONE;\r\n}\r\nstatic int __compact_finished(struct zone *zone, struct compact_control *cc,\r\nconst int migratetype)\r\n{\r\nunsigned int order;\r\nunsigned long watermark;\r\nif (cc->contended || fatal_signal_pending(current))\r\nreturn COMPACT_PARTIAL;\r\nif (cc->free_pfn <= cc->migrate_pfn) {\r\nzone->compact_cached_migrate_pfn[0] = zone->zone_start_pfn;\r\nzone->compact_cached_migrate_pfn[1] = zone->zone_start_pfn;\r\nzone->compact_cached_free_pfn = zone_end_pfn(zone);\r\nif (!current_is_kswapd())\r\nzone->compact_blockskip_flush = true;\r\nreturn COMPACT_COMPLETE;\r\n}\r\nif (cc->order == -1)\r\nreturn COMPACT_CONTINUE;\r\nwatermark = low_wmark_pages(zone);\r\nif (!zone_watermark_ok(zone, cc->order, watermark, cc->classzone_idx,\r\ncc->alloc_flags))\r\nreturn COMPACT_CONTINUE;\r\nfor (order = cc->order; order < MAX_ORDER; order++) {\r\nstruct free_area *area = &zone->free_area[order];\r\nbool can_steal;\r\nif (!list_empty(&area->free_list[migratetype]))\r\nreturn COMPACT_PARTIAL;\r\n#ifdef CONFIG_CMA\r\nif (migratetype == MIGRATE_MOVABLE &&\r\n!list_empty(&area->free_list[MIGRATE_CMA]))\r\nreturn COMPACT_PARTIAL;\r\n#endif\r\nif (find_suitable_fallback(area, order, migratetype,\r\ntrue, &can_steal) != -1)\r\nreturn COMPACT_PARTIAL;\r\n}\r\nreturn COMPACT_NO_SUITABLE_PAGE;\r\n}\r\nstatic int compact_finished(struct zone *zone, struct compact_control *cc,\r\nconst int migratetype)\r\n{\r\nint ret;\r\nret = __compact_finished(zone, cc, migratetype);\r\ntrace_mm_compaction_finished(zone, cc->order, ret);\r\nif (ret == COMPACT_NO_SUITABLE_PAGE)\r\nret = COMPACT_CONTINUE;\r\nreturn ret;\r\n}\r\nstatic unsigned long __compaction_suitable(struct zone *zone, int order,\r\nint alloc_flags, int classzone_idx)\r\n{\r\nint fragindex;\r\nunsigned long watermark;\r\nif (order == -1)\r\nreturn COMPACT_CONTINUE;\r\nwatermark = low_wmark_pages(zone);\r\nif (zone_watermark_ok(zone, order, watermark, classzone_idx,\r\nalloc_flags))\r\nreturn COMPACT_PARTIAL;\r\nwatermark += (2UL << order);\r\nif (!zone_watermark_ok(zone, 0, watermark, classzone_idx, alloc_flags))\r\nreturn COMPACT_SKIPPED;\r\nfragindex = fragmentation_index(zone, order);\r\nif (fragindex >= 0 && fragindex <= sysctl_extfrag_threshold)\r\nreturn COMPACT_NOT_SUITABLE_ZONE;\r\nreturn COMPACT_CONTINUE;\r\n}\r\nunsigned long compaction_suitable(struct zone *zone, int order,\r\nint alloc_flags, int classzone_idx)\r\n{\r\nunsigned long ret;\r\nret = __compaction_suitable(zone, order, alloc_flags, classzone_idx);\r\ntrace_mm_compaction_suitable(zone, order, ret);\r\nif (ret == COMPACT_NOT_SUITABLE_ZONE)\r\nret = COMPACT_SKIPPED;\r\nreturn ret;\r\n}\r\nstatic int compact_zone(struct zone *zone, struct compact_control *cc)\r\n{\r\nint ret;\r\nunsigned long start_pfn = zone->zone_start_pfn;\r\nunsigned long end_pfn = zone_end_pfn(zone);\r\nconst int migratetype = gfpflags_to_migratetype(cc->gfp_mask);\r\nconst bool sync = cc->mode != MIGRATE_ASYNC;\r\nunsigned long last_migrated_pfn = 0;\r\nret = compaction_suitable(zone, cc->order, cc->alloc_flags,\r\ncc->classzone_idx);\r\nswitch (ret) {\r\ncase COMPACT_PARTIAL:\r\ncase COMPACT_SKIPPED:\r\nreturn ret;\r\ncase COMPACT_CONTINUE:\r\n;\r\n}\r\nif (compaction_restarting(zone, cc->order) && !current_is_kswapd())\r\n__reset_isolation_suitable(zone);\r\ncc->migrate_pfn = zone->compact_cached_migrate_pfn[sync];\r\ncc->free_pfn = zone->compact_cached_free_pfn;\r\nif (cc->free_pfn < start_pfn || cc->free_pfn > end_pfn) {\r\ncc->free_pfn = end_pfn & ~(pageblock_nr_pages-1);\r\nzone->compact_cached_free_pfn = cc->free_pfn;\r\n}\r\nif (cc->migrate_pfn < start_pfn || cc->migrate_pfn > end_pfn) {\r\ncc->migrate_pfn = start_pfn;\r\nzone->compact_cached_migrate_pfn[0] = cc->migrate_pfn;\r\nzone->compact_cached_migrate_pfn[1] = cc->migrate_pfn;\r\n}\r\ntrace_mm_compaction_begin(start_pfn, cc->migrate_pfn,\r\ncc->free_pfn, end_pfn, sync);\r\nmigrate_prep_local();\r\nwhile ((ret = compact_finished(zone, cc, migratetype)) ==\r\nCOMPACT_CONTINUE) {\r\nint err;\r\nunsigned long isolate_start_pfn = cc->migrate_pfn;\r\nswitch (isolate_migratepages(zone, cc)) {\r\ncase ISOLATE_ABORT:\r\nret = COMPACT_PARTIAL;\r\nputback_movable_pages(&cc->migratepages);\r\ncc->nr_migratepages = 0;\r\ngoto out;\r\ncase ISOLATE_NONE:\r\ngoto check_drain;\r\ncase ISOLATE_SUCCESS:\r\n;\r\n}\r\nerr = migrate_pages(&cc->migratepages, compaction_alloc,\r\ncompaction_free, (unsigned long)cc, cc->mode,\r\nMR_COMPACTION);\r\ntrace_mm_compaction_migratepages(cc->nr_migratepages, err,\r\n&cc->migratepages);\r\ncc->nr_migratepages = 0;\r\nif (err) {\r\nputback_movable_pages(&cc->migratepages);\r\nif (err == -ENOMEM && cc->free_pfn > cc->migrate_pfn) {\r\nret = COMPACT_PARTIAL;\r\ngoto out;\r\n}\r\n}\r\nif (!last_migrated_pfn)\r\nlast_migrated_pfn = isolate_start_pfn;\r\ncheck_drain:\r\nif (cc->order > 0 && last_migrated_pfn) {\r\nint cpu;\r\nunsigned long current_block_start =\r\ncc->migrate_pfn & ~((1UL << cc->order) - 1);\r\nif (last_migrated_pfn < current_block_start) {\r\ncpu = get_cpu();\r\nlru_add_drain_cpu(cpu);\r\ndrain_local_pages(zone);\r\nput_cpu();\r\nlast_migrated_pfn = 0;\r\n}\r\n}\r\n}\r\nout:\r\nif (cc->nr_freepages > 0) {\r\nunsigned long free_pfn = release_freepages(&cc->freepages);\r\ncc->nr_freepages = 0;\r\nVM_BUG_ON(free_pfn == 0);\r\nfree_pfn &= ~(pageblock_nr_pages-1);\r\nif (free_pfn > zone->compact_cached_free_pfn)\r\nzone->compact_cached_free_pfn = free_pfn;\r\n}\r\ntrace_mm_compaction_end(start_pfn, cc->migrate_pfn,\r\ncc->free_pfn, end_pfn, sync, ret);\r\nreturn ret;\r\n}\r\nstatic unsigned long compact_zone_order(struct zone *zone, int order,\r\ngfp_t gfp_mask, enum migrate_mode mode, int *contended,\r\nint alloc_flags, int classzone_idx)\r\n{\r\nunsigned long ret;\r\nstruct compact_control cc = {\r\n.nr_freepages = 0,\r\n.nr_migratepages = 0,\r\n.order = order,\r\n.gfp_mask = gfp_mask,\r\n.zone = zone,\r\n.mode = mode,\r\n.alloc_flags = alloc_flags,\r\n.classzone_idx = classzone_idx,\r\n};\r\nINIT_LIST_HEAD(&cc.freepages);\r\nINIT_LIST_HEAD(&cc.migratepages);\r\nret = compact_zone(zone, &cc);\r\nVM_BUG_ON(!list_empty(&cc.freepages));\r\nVM_BUG_ON(!list_empty(&cc.migratepages));\r\n*contended = cc.contended;\r\nreturn ret;\r\n}\r\nunsigned long try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\r\nint alloc_flags, const struct alloc_context *ac,\r\nenum migrate_mode mode, int *contended)\r\n{\r\nint may_enter_fs = gfp_mask & __GFP_FS;\r\nint may_perform_io = gfp_mask & __GFP_IO;\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\nint rc = COMPACT_DEFERRED;\r\nint all_zones_contended = COMPACT_CONTENDED_LOCK;\r\n*contended = COMPACT_CONTENDED_NONE;\r\nif (!order || !may_enter_fs || !may_perform_io)\r\nreturn COMPACT_SKIPPED;\r\ntrace_mm_compaction_try_to_compact_pages(order, gfp_mask, mode);\r\nfor_each_zone_zonelist_nodemask(zone, z, ac->zonelist, ac->high_zoneidx,\r\nac->nodemask) {\r\nint status;\r\nint zone_contended;\r\nif (compaction_deferred(zone, order))\r\ncontinue;\r\nstatus = compact_zone_order(zone, order, gfp_mask, mode,\r\n&zone_contended, alloc_flags,\r\nac->classzone_idx);\r\nrc = max(status, rc);\r\nall_zones_contended &= zone_contended;\r\nif (zone_watermark_ok(zone, order, low_wmark_pages(zone),\r\nac->classzone_idx, alloc_flags)) {\r\ncompaction_defer_reset(zone, order, false);\r\nif (zone_contended == COMPACT_CONTENDED_SCHED)\r\n*contended = COMPACT_CONTENDED_SCHED;\r\ngoto break_loop;\r\n}\r\nif (mode != MIGRATE_ASYNC && status == COMPACT_COMPLETE) {\r\ndefer_compaction(zone, order);\r\n}\r\nif ((zone_contended == COMPACT_CONTENDED_SCHED)\r\n|| fatal_signal_pending(current)) {\r\n*contended = COMPACT_CONTENDED_SCHED;\r\ngoto break_loop;\r\n}\r\ncontinue;\r\nbreak_loop:\r\nall_zones_contended = 0;\r\nbreak;\r\n}\r\nif (rc > COMPACT_SKIPPED && all_zones_contended)\r\n*contended = COMPACT_CONTENDED_LOCK;\r\nreturn rc;\r\n}\r\nstatic void __compact_pgdat(pg_data_t *pgdat, struct compact_control *cc)\r\n{\r\nint zoneid;\r\nstruct zone *zone;\r\nfor (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {\r\nzone = &pgdat->node_zones[zoneid];\r\nif (!populated_zone(zone))\r\ncontinue;\r\ncc->nr_freepages = 0;\r\ncc->nr_migratepages = 0;\r\ncc->zone = zone;\r\nINIT_LIST_HEAD(&cc->freepages);\r\nINIT_LIST_HEAD(&cc->migratepages);\r\nif (cc->order == -1)\r\n__reset_isolation_suitable(zone);\r\nif (cc->order == -1 || !compaction_deferred(zone, cc->order))\r\ncompact_zone(zone, cc);\r\nif (cc->order > 0) {\r\nif (zone_watermark_ok(zone, cc->order,\r\nlow_wmark_pages(zone), 0, 0))\r\ncompaction_defer_reset(zone, cc->order, false);\r\n}\r\nVM_BUG_ON(!list_empty(&cc->freepages));\r\nVM_BUG_ON(!list_empty(&cc->migratepages));\r\n}\r\n}\r\nvoid compact_pgdat(pg_data_t *pgdat, int order)\r\n{\r\nstruct compact_control cc = {\r\n.order = order,\r\n.mode = MIGRATE_ASYNC,\r\n};\r\nif (!order)\r\nreturn;\r\n__compact_pgdat(pgdat, &cc);\r\n}\r\nstatic void compact_node(int nid)\r\n{\r\nstruct compact_control cc = {\r\n.order = -1,\r\n.mode = MIGRATE_SYNC,\r\n.ignore_skip_hint = true,\r\n};\r\n__compact_pgdat(NODE_DATA(nid), &cc);\r\n}\r\nstatic void compact_nodes(void)\r\n{\r\nint nid;\r\nlru_add_drain_all();\r\nfor_each_online_node(nid)\r\ncompact_node(nid);\r\n}\r\nint sysctl_compaction_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *length, loff_t *ppos)\r\n{\r\nif (write)\r\ncompact_nodes();\r\nreturn 0;\r\n}\r\nint sysctl_extfrag_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *length, loff_t *ppos)\r\n{\r\nproc_dointvec_minmax(table, write, buffer, length, ppos);\r\nreturn 0;\r\n}\r\nstatic ssize_t sysfs_compact_node(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nint nid = dev->id;\r\nif (nid >= 0 && nid < nr_node_ids && node_online(nid)) {\r\nlru_add_drain_all();\r\ncompact_node(nid);\r\n}\r\nreturn count;\r\n}\r\nint compaction_register_node(struct node *node)\r\n{\r\nreturn device_create_file(&node->dev, &dev_attr_compact);\r\n}\r\nvoid compaction_unregister_node(struct node *node)\r\n{\r\nreturn device_remove_file(&node->dev, &dev_attr_compact);\r\n}
