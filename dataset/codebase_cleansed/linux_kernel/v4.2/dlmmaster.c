static inline int dlm_mle_equal(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry *mle,\r\nconst char *name,\r\nunsigned int namelen)\r\n{\r\nif (dlm != mle->dlm)\r\nreturn 0;\r\nif (namelen != mle->mnamelen ||\r\nmemcmp(name, mle->mname, namelen) != 0)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nint dlm_is_host_down(int errno)\r\n{\r\nswitch (errno) {\r\ncase -EBADF:\r\ncase -ECONNREFUSED:\r\ncase -ENOTCONN:\r\ncase -ECONNRESET:\r\ncase -EPIPE:\r\ncase -EHOSTDOWN:\r\ncase -EHOSTUNREACH:\r\ncase -ETIMEDOUT:\r\ncase -ECONNABORTED:\r\ncase -ENETDOWN:\r\ncase -ENETUNREACH:\r\ncase -ENETRESET:\r\ncase -ESHUTDOWN:\r\ncase -ENOPROTOOPT:\r\ncase -EINVAL:\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void __dlm_mle_attach_hb_events(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry *mle)\r\n{\r\nassert_spin_locked(&dlm->spinlock);\r\nlist_add_tail(&mle->hb_events, &dlm->mle_hb_events);\r\n}\r\nstatic inline void __dlm_mle_detach_hb_events(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry *mle)\r\n{\r\nif (!list_empty(&mle->hb_events))\r\nlist_del_init(&mle->hb_events);\r\n}\r\nstatic inline void dlm_mle_detach_hb_events(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry *mle)\r\n{\r\nspin_lock(&dlm->spinlock);\r\n__dlm_mle_detach_hb_events(dlm, mle);\r\nspin_unlock(&dlm->spinlock);\r\n}\r\nstatic void dlm_get_mle_inuse(struct dlm_master_list_entry *mle)\r\n{\r\nstruct dlm_ctxt *dlm;\r\ndlm = mle->dlm;\r\nassert_spin_locked(&dlm->spinlock);\r\nassert_spin_locked(&dlm->master_lock);\r\nmle->inuse++;\r\nkref_get(&mle->mle_refs);\r\n}\r\nstatic void dlm_put_mle_inuse(struct dlm_master_list_entry *mle)\r\n{\r\nstruct dlm_ctxt *dlm;\r\ndlm = mle->dlm;\r\nspin_lock(&dlm->spinlock);\r\nspin_lock(&dlm->master_lock);\r\nmle->inuse--;\r\n__dlm_put_mle(mle);\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\n}\r\nstatic void __dlm_put_mle(struct dlm_master_list_entry *mle)\r\n{\r\nstruct dlm_ctxt *dlm;\r\ndlm = mle->dlm;\r\nassert_spin_locked(&dlm->spinlock);\r\nassert_spin_locked(&dlm->master_lock);\r\nif (!atomic_read(&mle->mle_refs.refcount)) {\r\nmlog(ML_ERROR, "bad mle: %p\n", mle);\r\ndlm_print_one_mle(mle);\r\nBUG();\r\n} else\r\nkref_put(&mle->mle_refs, dlm_mle_release);\r\n}\r\nstatic void dlm_put_mle(struct dlm_master_list_entry *mle)\r\n{\r\nstruct dlm_ctxt *dlm;\r\ndlm = mle->dlm;\r\nspin_lock(&dlm->spinlock);\r\nspin_lock(&dlm->master_lock);\r\n__dlm_put_mle(mle);\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\n}\r\nstatic inline void dlm_get_mle(struct dlm_master_list_entry *mle)\r\n{\r\nkref_get(&mle->mle_refs);\r\n}\r\nstatic void dlm_init_mle(struct dlm_master_list_entry *mle,\r\nenum dlm_mle_type type,\r\nstruct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nconst char *name,\r\nunsigned int namelen)\r\n{\r\nassert_spin_locked(&dlm->spinlock);\r\nmle->dlm = dlm;\r\nmle->type = type;\r\nINIT_HLIST_NODE(&mle->master_hash_node);\r\nINIT_LIST_HEAD(&mle->hb_events);\r\nmemset(mle->maybe_map, 0, sizeof(mle->maybe_map));\r\nspin_lock_init(&mle->spinlock);\r\ninit_waitqueue_head(&mle->wq);\r\natomic_set(&mle->woken, 0);\r\nkref_init(&mle->mle_refs);\r\nmemset(mle->response_map, 0, sizeof(mle->response_map));\r\nmle->master = O2NM_MAX_NODES;\r\nmle->new_master = O2NM_MAX_NODES;\r\nmle->inuse = 0;\r\nBUG_ON(mle->type != DLM_MLE_BLOCK &&\r\nmle->type != DLM_MLE_MASTER &&\r\nmle->type != DLM_MLE_MIGRATION);\r\nif (mle->type == DLM_MLE_MASTER) {\r\nBUG_ON(!res);\r\nmle->mleres = res;\r\nmemcpy(mle->mname, res->lockname.name, res->lockname.len);\r\nmle->mnamelen = res->lockname.len;\r\nmle->mnamehash = res->lockname.hash;\r\n} else {\r\nBUG_ON(!name);\r\nmle->mleres = NULL;\r\nmemcpy(mle->mname, name, namelen);\r\nmle->mnamelen = namelen;\r\nmle->mnamehash = dlm_lockid_hash(name, namelen);\r\n}\r\natomic_inc(&dlm->mle_tot_count[mle->type]);\r\natomic_inc(&dlm->mle_cur_count[mle->type]);\r\nmemcpy(mle->node_map, dlm->domain_map, sizeof(mle->node_map));\r\nmemcpy(mle->vote_map, dlm->domain_map, sizeof(mle->vote_map));\r\nclear_bit(dlm->node_num, mle->vote_map);\r\nclear_bit(dlm->node_num, mle->node_map);\r\n__dlm_mle_attach_hb_events(dlm, mle);\r\n}\r\nvoid __dlm_unlink_mle(struct dlm_ctxt *dlm, struct dlm_master_list_entry *mle)\r\n{\r\nassert_spin_locked(&dlm->spinlock);\r\nassert_spin_locked(&dlm->master_lock);\r\nif (!hlist_unhashed(&mle->master_hash_node))\r\nhlist_del_init(&mle->master_hash_node);\r\n}\r\nvoid __dlm_insert_mle(struct dlm_ctxt *dlm, struct dlm_master_list_entry *mle)\r\n{\r\nstruct hlist_head *bucket;\r\nassert_spin_locked(&dlm->master_lock);\r\nbucket = dlm_master_hash(dlm, mle->mnamehash);\r\nhlist_add_head(&mle->master_hash_node, bucket);\r\n}\r\nstatic int dlm_find_mle(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry **mle,\r\nchar *name, unsigned int namelen)\r\n{\r\nstruct dlm_master_list_entry *tmpmle;\r\nstruct hlist_head *bucket;\r\nunsigned int hash;\r\nassert_spin_locked(&dlm->master_lock);\r\nhash = dlm_lockid_hash(name, namelen);\r\nbucket = dlm_master_hash(dlm, hash);\r\nhlist_for_each_entry(tmpmle, bucket, master_hash_node) {\r\nif (!dlm_mle_equal(dlm, tmpmle, name, namelen))\r\ncontinue;\r\ndlm_get_mle(tmpmle);\r\n*mle = tmpmle;\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid dlm_hb_event_notify_attached(struct dlm_ctxt *dlm, int idx, int node_up)\r\n{\r\nstruct dlm_master_list_entry *mle;\r\nassert_spin_locked(&dlm->spinlock);\r\nlist_for_each_entry(mle, &dlm->mle_hb_events, hb_events) {\r\nif (node_up)\r\ndlm_mle_node_up(dlm, mle, NULL, idx);\r\nelse\r\ndlm_mle_node_down(dlm, mle, NULL, idx);\r\n}\r\n}\r\nstatic void dlm_mle_node_down(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry *mle,\r\nstruct o2nm_node *node, int idx)\r\n{\r\nspin_lock(&mle->spinlock);\r\nif (!test_bit(idx, mle->node_map))\r\nmlog(0, "node %u already removed from nodemap!\n", idx);\r\nelse\r\nclear_bit(idx, mle->node_map);\r\nspin_unlock(&mle->spinlock);\r\n}\r\nstatic void dlm_mle_node_up(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry *mle,\r\nstruct o2nm_node *node, int idx)\r\n{\r\nspin_lock(&mle->spinlock);\r\nif (test_bit(idx, mle->node_map))\r\nmlog(0, "node %u already in node map!\n", idx);\r\nelse\r\nset_bit(idx, mle->node_map);\r\nspin_unlock(&mle->spinlock);\r\n}\r\nint dlm_init_mle_cache(void)\r\n{\r\ndlm_mle_cache = kmem_cache_create("o2dlm_mle",\r\nsizeof(struct dlm_master_list_entry),\r\n0, SLAB_HWCACHE_ALIGN,\r\nNULL);\r\nif (dlm_mle_cache == NULL)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid dlm_destroy_mle_cache(void)\r\n{\r\nif (dlm_mle_cache)\r\nkmem_cache_destroy(dlm_mle_cache);\r\n}\r\nstatic void dlm_mle_release(struct kref *kref)\r\n{\r\nstruct dlm_master_list_entry *mle;\r\nstruct dlm_ctxt *dlm;\r\nmle = container_of(kref, struct dlm_master_list_entry, mle_refs);\r\ndlm = mle->dlm;\r\nassert_spin_locked(&dlm->spinlock);\r\nassert_spin_locked(&dlm->master_lock);\r\nmlog(0, "Releasing mle for %.*s, type %d\n", mle->mnamelen, mle->mname,\r\nmle->type);\r\n__dlm_unlink_mle(dlm, mle);\r\n__dlm_mle_detach_hb_events(dlm, mle);\r\natomic_dec(&dlm->mle_cur_count[mle->type]);\r\nkmem_cache_free(dlm_mle_cache, mle);\r\n}\r\nint dlm_init_master_caches(void)\r\n{\r\ndlm_lockres_cache = kmem_cache_create("o2dlm_lockres",\r\nsizeof(struct dlm_lock_resource),\r\n0, SLAB_HWCACHE_ALIGN, NULL);\r\nif (!dlm_lockres_cache)\r\ngoto bail;\r\ndlm_lockname_cache = kmem_cache_create("o2dlm_lockname",\r\nDLM_LOCKID_NAME_MAX, 0,\r\nSLAB_HWCACHE_ALIGN, NULL);\r\nif (!dlm_lockname_cache)\r\ngoto bail;\r\nreturn 0;\r\nbail:\r\ndlm_destroy_master_caches();\r\nreturn -ENOMEM;\r\n}\r\nvoid dlm_destroy_master_caches(void)\r\n{\r\nif (dlm_lockname_cache) {\r\nkmem_cache_destroy(dlm_lockname_cache);\r\ndlm_lockname_cache = NULL;\r\n}\r\nif (dlm_lockres_cache) {\r\nkmem_cache_destroy(dlm_lockres_cache);\r\ndlm_lockres_cache = NULL;\r\n}\r\n}\r\nstatic void dlm_lockres_release(struct kref *kref)\r\n{\r\nstruct dlm_lock_resource *res;\r\nstruct dlm_ctxt *dlm;\r\nres = container_of(kref, struct dlm_lock_resource, refs);\r\ndlm = res->dlm;\r\nBUG_ON(!res->lockname.name);\r\nmlog(0, "destroying lockres %.*s\n", res->lockname.len,\r\nres->lockname.name);\r\nspin_lock(&dlm->track_lock);\r\nif (!list_empty(&res->tracking))\r\nlist_del_init(&res->tracking);\r\nelse {\r\nmlog(ML_ERROR, "Resource %.*s not on the Tracking list\n",\r\nres->lockname.len, res->lockname.name);\r\ndlm_print_one_lock_resource(res);\r\n}\r\nspin_unlock(&dlm->track_lock);\r\natomic_dec(&dlm->res_cur_count);\r\nif (!hlist_unhashed(&res->hash_node) ||\r\n!list_empty(&res->granted) ||\r\n!list_empty(&res->converting) ||\r\n!list_empty(&res->blocked) ||\r\n!list_empty(&res->dirty) ||\r\n!list_empty(&res->recovering) ||\r\n!list_empty(&res->purge)) {\r\nmlog(ML_ERROR,\r\n"Going to BUG for resource %.*s."\r\n" We're on a list! [%c%c%c%c%c%c%c]\n",\r\nres->lockname.len, res->lockname.name,\r\n!hlist_unhashed(&res->hash_node) ? 'H' : ' ',\r\n!list_empty(&res->granted) ? 'G' : ' ',\r\n!list_empty(&res->converting) ? 'C' : ' ',\r\n!list_empty(&res->blocked) ? 'B' : ' ',\r\n!list_empty(&res->dirty) ? 'D' : ' ',\r\n!list_empty(&res->recovering) ? 'R' : ' ',\r\n!list_empty(&res->purge) ? 'P' : ' ');\r\ndlm_print_one_lock_resource(res);\r\n}\r\nBUG_ON(!hlist_unhashed(&res->hash_node));\r\nBUG_ON(!list_empty(&res->granted));\r\nBUG_ON(!list_empty(&res->converting));\r\nBUG_ON(!list_empty(&res->blocked));\r\nBUG_ON(!list_empty(&res->dirty));\r\nBUG_ON(!list_empty(&res->recovering));\r\nBUG_ON(!list_empty(&res->purge));\r\nkmem_cache_free(dlm_lockname_cache, (void *)res->lockname.name);\r\nkmem_cache_free(dlm_lockres_cache, res);\r\n}\r\nvoid dlm_lockres_put(struct dlm_lock_resource *res)\r\n{\r\nkref_put(&res->refs, dlm_lockres_release);\r\n}\r\nstatic void dlm_init_lockres(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nconst char *name, unsigned int namelen)\r\n{\r\nchar *qname;\r\nqname = (char *) res->lockname.name;\r\nmemcpy(qname, name, namelen);\r\nres->lockname.len = namelen;\r\nres->lockname.hash = dlm_lockid_hash(name, namelen);\r\ninit_waitqueue_head(&res->wq);\r\nspin_lock_init(&res->spinlock);\r\nINIT_HLIST_NODE(&res->hash_node);\r\nINIT_LIST_HEAD(&res->granted);\r\nINIT_LIST_HEAD(&res->converting);\r\nINIT_LIST_HEAD(&res->blocked);\r\nINIT_LIST_HEAD(&res->dirty);\r\nINIT_LIST_HEAD(&res->recovering);\r\nINIT_LIST_HEAD(&res->purge);\r\nINIT_LIST_HEAD(&res->tracking);\r\natomic_set(&res->asts_reserved, 0);\r\nres->migration_pending = 0;\r\nres->inflight_locks = 0;\r\nres->inflight_assert_workers = 0;\r\nres->dlm = dlm;\r\nkref_init(&res->refs);\r\natomic_inc(&dlm->res_tot_count);\r\natomic_inc(&dlm->res_cur_count);\r\nspin_lock(&res->spinlock);\r\ndlm_set_lockres_owner(dlm, res, DLM_LOCK_RES_OWNER_UNKNOWN);\r\nspin_unlock(&res->spinlock);\r\nres->state = DLM_LOCK_RES_IN_PROGRESS;\r\nres->last_used = 0;\r\nspin_lock(&dlm->spinlock);\r\nlist_add_tail(&res->tracking, &dlm->tracking_list);\r\nspin_unlock(&dlm->spinlock);\r\nmemset(res->lvb, 0, DLM_LVB_LEN);\r\nmemset(res->refmap, 0, sizeof(res->refmap));\r\n}\r\nstruct dlm_lock_resource *dlm_new_lockres(struct dlm_ctxt *dlm,\r\nconst char *name,\r\nunsigned int namelen)\r\n{\r\nstruct dlm_lock_resource *res = NULL;\r\nres = kmem_cache_zalloc(dlm_lockres_cache, GFP_NOFS);\r\nif (!res)\r\ngoto error;\r\nres->lockname.name = kmem_cache_zalloc(dlm_lockname_cache, GFP_NOFS);\r\nif (!res->lockname.name)\r\ngoto error;\r\ndlm_init_lockres(dlm, res, name, namelen);\r\nreturn res;\r\nerror:\r\nif (res)\r\nkmem_cache_free(dlm_lockres_cache, res);\r\nreturn NULL;\r\n}\r\nvoid dlm_lockres_set_refmap_bit(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res, int bit)\r\n{\r\nassert_spin_locked(&res->spinlock);\r\nmlog(0, "res %.*s, set node %u, %ps()\n", res->lockname.len,\r\nres->lockname.name, bit, __builtin_return_address(0));\r\nset_bit(bit, res->refmap);\r\n}\r\nvoid dlm_lockres_clear_refmap_bit(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res, int bit)\r\n{\r\nassert_spin_locked(&res->spinlock);\r\nmlog(0, "res %.*s, clr node %u, %ps()\n", res->lockname.len,\r\nres->lockname.name, bit, __builtin_return_address(0));\r\nclear_bit(bit, res->refmap);\r\n}\r\nstatic void __dlm_lockres_grab_inflight_ref(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nres->inflight_locks++;\r\nmlog(0, "%s: res %.*s, inflight++: now %u, %ps()\n", dlm->name,\r\nres->lockname.len, res->lockname.name, res->inflight_locks,\r\n__builtin_return_address(0));\r\n}\r\nvoid dlm_lockres_grab_inflight_ref(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nassert_spin_locked(&res->spinlock);\r\n__dlm_lockres_grab_inflight_ref(dlm, res);\r\n}\r\nvoid dlm_lockres_drop_inflight_ref(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nassert_spin_locked(&res->spinlock);\r\nBUG_ON(res->inflight_locks == 0);\r\nres->inflight_locks--;\r\nmlog(0, "%s: res %.*s, inflight--: now %u, %ps()\n", dlm->name,\r\nres->lockname.len, res->lockname.name, res->inflight_locks,\r\n__builtin_return_address(0));\r\nwake_up(&res->wq);\r\n}\r\nvoid __dlm_lockres_grab_inflight_worker(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nassert_spin_locked(&res->spinlock);\r\nres->inflight_assert_workers++;\r\nmlog(0, "%s:%.*s: inflight assert worker++: now %u\n",\r\ndlm->name, res->lockname.len, res->lockname.name,\r\nres->inflight_assert_workers);\r\n}\r\nstatic void __dlm_lockres_drop_inflight_worker(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nassert_spin_locked(&res->spinlock);\r\nBUG_ON(res->inflight_assert_workers == 0);\r\nres->inflight_assert_workers--;\r\nmlog(0, "%s:%.*s: inflight assert worker--: now %u\n",\r\ndlm->name, res->lockname.len, res->lockname.name,\r\nres->inflight_assert_workers);\r\n}\r\nstatic void dlm_lockres_drop_inflight_worker(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nspin_lock(&res->spinlock);\r\n__dlm_lockres_drop_inflight_worker(dlm, res);\r\nspin_unlock(&res->spinlock);\r\n}\r\nstruct dlm_lock_resource * dlm_get_lock_resource(struct dlm_ctxt *dlm,\r\nconst char *lockid,\r\nint namelen,\r\nint flags)\r\n{\r\nstruct dlm_lock_resource *tmpres=NULL, *res=NULL;\r\nstruct dlm_master_list_entry *mle = NULL;\r\nstruct dlm_master_list_entry *alloc_mle = NULL;\r\nint blocked = 0;\r\nint ret, nodenum;\r\nstruct dlm_node_iter iter;\r\nunsigned int hash;\r\nint tries = 0;\r\nint bit, wait_on_recovery = 0;\r\nBUG_ON(!lockid);\r\nhash = dlm_lockid_hash(lockid, namelen);\r\nmlog(0, "get lockres %s (len %d)\n", lockid, namelen);\r\nlookup:\r\nspin_lock(&dlm->spinlock);\r\ntmpres = __dlm_lookup_lockres_full(dlm, lockid, namelen, hash);\r\nif (tmpres) {\r\nspin_unlock(&dlm->spinlock);\r\nspin_lock(&tmpres->spinlock);\r\nif (hlist_unhashed(&tmpres->hash_node)) {\r\nspin_unlock(&tmpres->spinlock);\r\ndlm_lockres_put(tmpres);\r\ntmpres = NULL;\r\ngoto lookup;\r\n}\r\nif (tmpres->owner == DLM_LOCK_RES_OWNER_UNKNOWN) {\r\n__dlm_wait_on_lockres(tmpres);\r\nBUG_ON(tmpres->owner == DLM_LOCK_RES_OWNER_UNKNOWN);\r\nspin_unlock(&tmpres->spinlock);\r\ndlm_lockres_put(tmpres);\r\ntmpres = NULL;\r\ngoto lookup;\r\n}\r\nif (tmpres->state & DLM_LOCK_RES_DROPPING_REF) {\r\nBUG_ON(tmpres->owner == dlm->node_num);\r\n__dlm_wait_on_lockres_flags(tmpres,\r\nDLM_LOCK_RES_DROPPING_REF);\r\nspin_unlock(&tmpres->spinlock);\r\ndlm_lockres_put(tmpres);\r\ntmpres = NULL;\r\ngoto lookup;\r\n}\r\ndlm_lockres_grab_inflight_ref(dlm, tmpres);\r\nspin_unlock(&tmpres->spinlock);\r\nif (res)\r\ndlm_lockres_put(res);\r\nres = tmpres;\r\ngoto leave;\r\n}\r\nif (!res) {\r\nspin_unlock(&dlm->spinlock);\r\nmlog(0, "allocating a new resource\n");\r\nalloc_mle = kmem_cache_alloc(dlm_mle_cache, GFP_NOFS);\r\nif (!alloc_mle)\r\ngoto leave;\r\nres = dlm_new_lockres(dlm, lockid, namelen);\r\nif (!res)\r\ngoto leave;\r\ngoto lookup;\r\n}\r\nmlog(0, "no lockres found, allocated our own: %p\n", res);\r\nif (flags & LKM_LOCAL) {\r\nspin_lock(&res->spinlock);\r\ndlm_change_lockres_owner(dlm, res, dlm->node_num);\r\n__dlm_insert_lockres(dlm, res);\r\ndlm_lockres_grab_inflight_ref(dlm, res);\r\nspin_unlock(&res->spinlock);\r\nspin_unlock(&dlm->spinlock);\r\ngoto wake_waiters;\r\n}\r\nspin_lock(&dlm->master_lock);\r\nblocked = dlm_find_mle(dlm, &mle, (char *)lockid, namelen);\r\nif (blocked) {\r\nint mig;\r\nif (mle->type == DLM_MLE_MASTER) {\r\nmlog(ML_ERROR, "master entry for nonexistent lock!\n");\r\nBUG();\r\n}\r\nmig = (mle->type == DLM_MLE_MIGRATION);\r\nif (mig || mle->master != O2NM_MAX_NODES) {\r\nBUG_ON(mig && mle->master == dlm->node_num);\r\nmlog(0, "%s:%.*s: late on %s\n",\r\ndlm->name, namelen, lockid,\r\nmig ? "MIGRATION" : "BLOCK");\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\nif (!mig)\r\ndlm_mle_detach_hb_events(dlm, mle);\r\ndlm_put_mle(mle);\r\nmle = NULL;\r\nif (mig)\r\nmsleep(100);\r\ngoto lookup;\r\n}\r\n} else {\r\nmle = alloc_mle;\r\nalloc_mle = NULL;\r\ndlm_init_mle(mle, DLM_MLE_MASTER, dlm, res, NULL, 0);\r\nset_bit(dlm->node_num, mle->maybe_map);\r\n__dlm_insert_mle(dlm, mle);\r\nbit = find_next_bit(dlm->recovery_map, O2NM_MAX_NODES, 0);\r\nif (bit < O2NM_MAX_NODES) {\r\nmlog(0, "%s: res %.*s, At least one node (%d) "\r\n"to recover before lock mastery can begin\n",\r\ndlm->name, namelen, (char *)lockid, bit);\r\nwait_on_recovery = 1;\r\n}\r\n}\r\n__dlm_insert_lockres(dlm, res);\r\n__dlm_lockres_grab_inflight_ref(dlm, res);\r\ndlm_get_mle_inuse(mle);\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\nredo_request:\r\nwhile (wait_on_recovery) {\r\nif (dlm_is_recovery_lock(lockid, namelen)) {\r\nmlog(0, "%s: Recovery map is not empty, but must "\r\n"master $RECOVERY lock now\n", dlm->name);\r\nif (!dlm_pre_master_reco_lockres(dlm, res))\r\nwait_on_recovery = 0;\r\nelse {\r\nmlog(0, "%s: waiting 500ms for heartbeat state "\r\n"change\n", dlm->name);\r\nmsleep(500);\r\n}\r\ncontinue;\r\n}\r\ndlm_kick_recovery_thread(dlm);\r\nmsleep(1000);\r\ndlm_wait_for_recovery(dlm);\r\nspin_lock(&dlm->spinlock);\r\nbit = find_next_bit(dlm->recovery_map, O2NM_MAX_NODES, 0);\r\nif (bit < O2NM_MAX_NODES) {\r\nmlog(0, "%s: res %.*s, At least one node (%d) "\r\n"to recover before lock mastery can begin\n",\r\ndlm->name, namelen, (char *)lockid, bit);\r\nwait_on_recovery = 1;\r\n} else\r\nwait_on_recovery = 0;\r\nspin_unlock(&dlm->spinlock);\r\nif (wait_on_recovery)\r\ndlm_wait_for_node_recovery(dlm, bit, 10000);\r\n}\r\nif (blocked)\r\ngoto wait;\r\nret = -EINVAL;\r\ndlm_node_iter_init(mle->vote_map, &iter);\r\nwhile ((nodenum = dlm_node_iter_next(&iter)) >= 0) {\r\nret = dlm_do_master_request(res, mle, nodenum);\r\nif (ret < 0)\r\nmlog_errno(ret);\r\nif (mle->master != O2NM_MAX_NODES) {\r\nif (mle->master <= nodenum)\r\nbreak;\r\nmlog(0, "%s: res %.*s, Requests only up to %u but "\r\n"master is %u, keep going\n", dlm->name, namelen,\r\nlockid, nodenum, mle->master);\r\n}\r\n}\r\nwait:\r\nret = dlm_wait_for_lock_mastery(dlm, res, mle, &blocked);\r\nif (ret < 0) {\r\nwait_on_recovery = 1;\r\nmlog(0, "%s: res %.*s, Node map changed, redo the master "\r\n"request now, blocked=%d\n", dlm->name, res->lockname.len,\r\nres->lockname.name, blocked);\r\nif (++tries > 20) {\r\nmlog(ML_ERROR, "%s: res %.*s, Spinning on "\r\n"dlm_wait_for_lock_mastery, blocked = %d\n",\r\ndlm->name, res->lockname.len,\r\nres->lockname.name, blocked);\r\ndlm_print_one_lock_resource(res);\r\ndlm_print_one_mle(mle);\r\ntries = 0;\r\n}\r\ngoto redo_request;\r\n}\r\nmlog(0, "%s: res %.*s, Mastered by %u\n", dlm->name, res->lockname.len,\r\nres->lockname.name, res->owner);\r\nBUG_ON(res->owner == O2NM_MAX_NODES);\r\ndlm_mle_detach_hb_events(dlm, mle);\r\ndlm_put_mle(mle);\r\ndlm_put_mle_inuse(mle);\r\nwake_waiters:\r\nspin_lock(&res->spinlock);\r\nres->state &= ~DLM_LOCK_RES_IN_PROGRESS;\r\nspin_unlock(&res->spinlock);\r\nwake_up(&res->wq);\r\nleave:\r\nif (alloc_mle)\r\nkmem_cache_free(dlm_mle_cache, alloc_mle);\r\nreturn res;\r\n}\r\nstatic int dlm_wait_for_lock_mastery(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nstruct dlm_master_list_entry *mle,\r\nint *blocked)\r\n{\r\nu8 m;\r\nint ret, bit;\r\nint map_changed, voting_done;\r\nint assert, sleep;\r\nrecheck:\r\nret = 0;\r\nassert = 0;\r\nspin_lock(&res->spinlock);\r\nif (res->owner != DLM_LOCK_RES_OWNER_UNKNOWN) {\r\nmlog(0, "%s:%.*s: owner is suddenly %u\n", dlm->name,\r\nres->lockname.len, res->lockname.name, res->owner);\r\nspin_unlock(&res->spinlock);\r\nif (res->owner != dlm->node_num) {\r\nret = dlm_do_master_request(res, mle, res->owner);\r\nif (ret < 0) {\r\nmlog(ML_ERROR, "link to %u went down?: %d\n", res->owner, ret);\r\nmsleep(500);\r\ngoto recheck;\r\n}\r\n}\r\nret = 0;\r\ngoto leave;\r\n}\r\nspin_unlock(&res->spinlock);\r\nspin_lock(&mle->spinlock);\r\nm = mle->master;\r\nmap_changed = (memcmp(mle->vote_map, mle->node_map,\r\nsizeof(mle->vote_map)) != 0);\r\nvoting_done = (memcmp(mle->vote_map, mle->response_map,\r\nsizeof(mle->vote_map)) == 0);\r\nif (map_changed) {\r\nint b;\r\nmlog(0, "%s: %.*s: node map changed, restarting\n",\r\ndlm->name, res->lockname.len, res->lockname.name);\r\nret = dlm_restart_lock_mastery(dlm, res, mle, *blocked);\r\nb = (mle->type == DLM_MLE_BLOCK);\r\nif ((*blocked && !b) || (!*blocked && b)) {\r\nmlog(0, "%s:%.*s: status change: old=%d new=%d\n",\r\ndlm->name, res->lockname.len, res->lockname.name,\r\n*blocked, b);\r\n*blocked = b;\r\n}\r\nspin_unlock(&mle->spinlock);\r\nif (ret < 0) {\r\nmlog_errno(ret);\r\ngoto leave;\r\n}\r\nmlog(0, "%s:%.*s: restart lock mastery succeeded, "\r\n"rechecking now\n", dlm->name, res->lockname.len,\r\nres->lockname.name);\r\ngoto recheck;\r\n} else {\r\nif (!voting_done) {\r\nmlog(0, "map not changed and voting not done "\r\n"for %s:%.*s\n", dlm->name, res->lockname.len,\r\nres->lockname.name);\r\n}\r\n}\r\nif (m != O2NM_MAX_NODES) {\r\nsleep = 0;\r\n} else {\r\nsleep = 1;\r\nif (voting_done && !*blocked) {\r\nbit = find_next_bit(mle->maybe_map, O2NM_MAX_NODES, 0);\r\nif (dlm->node_num <= bit) {\r\nmle->master = dlm->node_num;\r\nassert = 1;\r\nsleep = 0;\r\n}\r\n}\r\n}\r\nspin_unlock(&mle->spinlock);\r\nif (sleep) {\r\nunsigned long timeo = msecs_to_jiffies(DLM_MASTERY_TIMEOUT_MS);\r\natomic_set(&mle->woken, 0);\r\n(void)wait_event_timeout(mle->wq,\r\n(atomic_read(&mle->woken) == 1),\r\ntimeo);\r\nif (res->owner == O2NM_MAX_NODES) {\r\nmlog(0, "%s:%.*s: waiting again\n", dlm->name,\r\nres->lockname.len, res->lockname.name);\r\ngoto recheck;\r\n}\r\nmlog(0, "done waiting, master is %u\n", res->owner);\r\nret = 0;\r\ngoto leave;\r\n}\r\nret = 0;\r\nif (assert) {\r\nm = dlm->node_num;\r\nmlog(0, "about to master %.*s here, this=%u\n",\r\nres->lockname.len, res->lockname.name, m);\r\nret = dlm_do_assert_master(dlm, res, mle->vote_map, 0);\r\nif (ret) {\r\nmlog_errno(ret);\r\n}\r\nret = 0;\r\n}\r\nspin_lock(&res->spinlock);\r\ndlm_change_lockres_owner(dlm, res, m);\r\nspin_unlock(&res->spinlock);\r\nleave:\r\nreturn ret;\r\n}\r\nstatic void dlm_bitmap_diff_iter_init(struct dlm_bitmap_diff_iter *iter,\r\nunsigned long *orig_bm,\r\nunsigned long *cur_bm)\r\n{\r\nunsigned long p1, p2;\r\nint i;\r\niter->curnode = -1;\r\niter->orig_bm = orig_bm;\r\niter->cur_bm = cur_bm;\r\nfor (i = 0; i < BITS_TO_LONGS(O2NM_MAX_NODES); i++) {\r\np1 = *(iter->orig_bm + i);\r\np2 = *(iter->cur_bm + i);\r\niter->diff_bm[i] = (p1 & ~p2) | (p2 & ~p1);\r\n}\r\n}\r\nstatic int dlm_bitmap_diff_iter_next(struct dlm_bitmap_diff_iter *iter,\r\nenum dlm_node_state_change *state)\r\n{\r\nint bit;\r\nif (iter->curnode >= O2NM_MAX_NODES)\r\nreturn -ENOENT;\r\nbit = find_next_bit(iter->diff_bm, O2NM_MAX_NODES,\r\niter->curnode+1);\r\nif (bit >= O2NM_MAX_NODES) {\r\niter->curnode = O2NM_MAX_NODES;\r\nreturn -ENOENT;\r\n}\r\nif (test_bit(bit, iter->orig_bm))\r\n*state = NODE_DOWN;\r\nelse\r\n*state = NODE_UP;\r\niter->curnode = bit;\r\nreturn bit;\r\n}\r\nstatic int dlm_restart_lock_mastery(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nstruct dlm_master_list_entry *mle,\r\nint blocked)\r\n{\r\nstruct dlm_bitmap_diff_iter bdi;\r\nenum dlm_node_state_change sc;\r\nint node;\r\nint ret = 0;\r\nmlog(0, "something happened such that the "\r\n"master process may need to be restarted!\n");\r\nassert_spin_locked(&mle->spinlock);\r\ndlm_bitmap_diff_iter_init(&bdi, mle->vote_map, mle->node_map);\r\nnode = dlm_bitmap_diff_iter_next(&bdi, &sc);\r\nwhile (node >= 0) {\r\nif (sc == NODE_UP) {\r\nmlog(ML_NOTICE, "node %d up while restarting\n", node);\r\nmlog(0, "sending request to new node\n");\r\nclear_bit(node, mle->response_map);\r\nset_bit(node, mle->vote_map);\r\n} else {\r\nmlog(ML_ERROR, "node down! %d\n", node);\r\nif (blocked) {\r\nint lowest = find_next_bit(mle->maybe_map,\r\nO2NM_MAX_NODES, 0);\r\nclear_bit(node, mle->maybe_map);\r\nif (node == lowest) {\r\nmlog(0, "expected master %u died"\r\n" while this node was blocked "\r\n"waiting on it!\n", node);\r\nlowest = find_next_bit(mle->maybe_map,\r\nO2NM_MAX_NODES,\r\nlowest+1);\r\nif (lowest < O2NM_MAX_NODES) {\r\nmlog(0, "%s:%.*s:still "\r\n"blocked. waiting on %u "\r\n"now\n", dlm->name,\r\nres->lockname.len,\r\nres->lockname.name,\r\nlowest);\r\n} else {\r\nmlog(0, "%s:%.*s: no "\r\n"longer blocking. try to "\r\n"master this here\n",\r\ndlm->name,\r\nres->lockname.len,\r\nres->lockname.name);\r\nmle->type = DLM_MLE_MASTER;\r\nmle->mleres = res;\r\n}\r\n}\r\n}\r\nmemset(mle->maybe_map, 0, sizeof(mle->maybe_map));\r\nmemset(mle->response_map, 0, sizeof(mle->response_map));\r\nmemcpy(mle->vote_map, mle->node_map,\r\nsizeof(mle->node_map));\r\nif (mle->type != DLM_MLE_BLOCK)\r\nset_bit(dlm->node_num, mle->maybe_map);\r\n}\r\nret = -EAGAIN;\r\nnode = dlm_bitmap_diff_iter_next(&bdi, &sc);\r\n}\r\nreturn ret;\r\n}\r\nstatic int dlm_do_master_request(struct dlm_lock_resource *res,\r\nstruct dlm_master_list_entry *mle, int to)\r\n{\r\nstruct dlm_ctxt *dlm = mle->dlm;\r\nstruct dlm_master_request request;\r\nint ret, response=0, resend;\r\nmemset(&request, 0, sizeof(request));\r\nrequest.node_idx = dlm->node_num;\r\nBUG_ON(mle->type == DLM_MLE_MIGRATION);\r\nrequest.namelen = (u8)mle->mnamelen;\r\nmemcpy(request.name, mle->mname, request.namelen);\r\nagain:\r\nret = o2net_send_message(DLM_MASTER_REQUEST_MSG, dlm->key, &request,\r\nsizeof(request), to, &response);\r\nif (ret < 0) {\r\nif (ret == -ESRCH) {\r\nmlog(ML_ERROR, "TCP stack not ready!\n");\r\nBUG();\r\n} else if (ret == -EINVAL) {\r\nmlog(ML_ERROR, "bad args passed to o2net!\n");\r\nBUG();\r\n} else if (ret == -ENOMEM) {\r\nmlog(ML_ERROR, "out of memory while trying to send "\r\n"network message! retrying\n");\r\nmsleep(50);\r\ngoto again;\r\n} else if (!dlm_is_host_down(ret)) {\r\nmlog_errno(ret);\r\nmlog(ML_ERROR, "unhandled error!");\r\nBUG();\r\n}\r\nmlog(ML_ERROR, "link to %d went down!\n", to);\r\ngoto out;\r\n}\r\nret = 0;\r\nresend = 0;\r\nspin_lock(&mle->spinlock);\r\nswitch (response) {\r\ncase DLM_MASTER_RESP_YES:\r\nset_bit(to, mle->response_map);\r\nmlog(0, "node %u is the master, response=YES\n", to);\r\nmlog(0, "%s:%.*s: master node %u now knows I have a "\r\n"reference\n", dlm->name, res->lockname.len,\r\nres->lockname.name, to);\r\nmle->master = to;\r\nbreak;\r\ncase DLM_MASTER_RESP_NO:\r\nmlog(0, "node %u not master, response=NO\n", to);\r\nset_bit(to, mle->response_map);\r\nbreak;\r\ncase DLM_MASTER_RESP_MAYBE:\r\nmlog(0, "node %u not master, response=MAYBE\n", to);\r\nset_bit(to, mle->response_map);\r\nset_bit(to, mle->maybe_map);\r\nbreak;\r\ncase DLM_MASTER_RESP_ERROR:\r\nmlog(0, "node %u hit an error, resending\n", to);\r\nresend = 1;\r\nresponse = 0;\r\nbreak;\r\ndefault:\r\nmlog(ML_ERROR, "bad response! %u\n", response);\r\nBUG();\r\n}\r\nspin_unlock(&mle->spinlock);\r\nif (resend) {\r\nmsleep(50);\r\ngoto again;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nint dlm_master_request_handler(struct o2net_msg *msg, u32 len, void *data,\r\nvoid **ret_data)\r\n{\r\nu8 response = DLM_MASTER_RESP_MAYBE;\r\nstruct dlm_ctxt *dlm = data;\r\nstruct dlm_lock_resource *res = NULL;\r\nstruct dlm_master_request *request = (struct dlm_master_request *) msg->buf;\r\nstruct dlm_master_list_entry *mle = NULL, *tmpmle = NULL;\r\nchar *name;\r\nunsigned int namelen, hash;\r\nint found, ret;\r\nint set_maybe;\r\nint dispatch_assert = 0;\r\nif (!dlm_grab(dlm))\r\nreturn DLM_MASTER_RESP_NO;\r\nif (!dlm_domain_fully_joined(dlm)) {\r\nresponse = DLM_MASTER_RESP_NO;\r\ngoto send_response;\r\n}\r\nname = request->name;\r\nnamelen = request->namelen;\r\nhash = dlm_lockid_hash(name, namelen);\r\nif (namelen > DLM_LOCKID_NAME_MAX) {\r\nresponse = DLM_IVBUFLEN;\r\ngoto send_response;\r\n}\r\nway_up_top:\r\nspin_lock(&dlm->spinlock);\r\nres = __dlm_lookup_lockres(dlm, name, namelen, hash);\r\nif (res) {\r\nspin_unlock(&dlm->spinlock);\r\nspin_lock(&res->spinlock);\r\nif (hlist_unhashed(&res->hash_node)) {\r\nspin_unlock(&res->spinlock);\r\ndlm_lockres_put(res);\r\ngoto way_up_top;\r\n}\r\nif (res->state & (DLM_LOCK_RES_RECOVERING|\r\nDLM_LOCK_RES_MIGRATING)) {\r\nspin_unlock(&res->spinlock);\r\nmlog(0, "returning DLM_MASTER_RESP_ERROR since res is "\r\n"being recovered/migrated\n");\r\nresponse = DLM_MASTER_RESP_ERROR;\r\nif (mle)\r\nkmem_cache_free(dlm_mle_cache, mle);\r\ngoto send_response;\r\n}\r\nif (res->owner == dlm->node_num) {\r\ndlm_lockres_set_refmap_bit(dlm, res, request->node_idx);\r\nspin_unlock(&res->spinlock);\r\nresponse = DLM_MASTER_RESP_YES;\r\nif (mle)\r\nkmem_cache_free(dlm_mle_cache, mle);\r\ndispatch_assert = 1;\r\ngoto send_response;\r\n} else if (res->owner != DLM_LOCK_RES_OWNER_UNKNOWN) {\r\nspin_unlock(&res->spinlock);\r\nresponse = DLM_MASTER_RESP_NO;\r\nif (mle)\r\nkmem_cache_free(dlm_mle_cache, mle);\r\ngoto send_response;\r\n}\r\nif (!(res->state & DLM_LOCK_RES_IN_PROGRESS)) {\r\nmlog(ML_ERROR, "lock with no owner should be "\r\n"in-progress!\n");\r\nBUG();\r\n}\r\nspin_lock(&dlm->master_lock);\r\nfound = dlm_find_mle(dlm, &tmpmle, name, namelen);\r\nif (!found) {\r\nmlog(ML_ERROR, "no mle found for this lock!\n");\r\nBUG();\r\n}\r\nset_maybe = 1;\r\nspin_lock(&tmpmle->spinlock);\r\nif (tmpmle->type == DLM_MLE_BLOCK) {\r\nresponse = DLM_MASTER_RESP_NO;\r\n} else if (tmpmle->type == DLM_MLE_MIGRATION) {\r\nmlog(0, "node %u is master, but trying to migrate to "\r\n"node %u.\n", tmpmle->master, tmpmle->new_master);\r\nif (tmpmle->master == dlm->node_num) {\r\nmlog(ML_ERROR, "no owner on lockres, but this "\r\n"node is trying to migrate it to %u?!\n",\r\ntmpmle->new_master);\r\nBUG();\r\n} else {\r\nresponse = DLM_MASTER_RESP_NO;\r\n}\r\n} else if (tmpmle->master != DLM_LOCK_RES_OWNER_UNKNOWN) {\r\nset_maybe = 0;\r\nif (tmpmle->master == dlm->node_num) {\r\nresponse = DLM_MASTER_RESP_YES;\r\ndispatch_assert = 1;\r\ndlm_lockres_set_refmap_bit(dlm, res,\r\nrequest->node_idx);\r\n} else\r\nresponse = DLM_MASTER_RESP_NO;\r\n} else {\r\nresponse = DLM_MASTER_RESP_MAYBE;\r\n}\r\nif (set_maybe)\r\nset_bit(request->node_idx, tmpmle->maybe_map);\r\nspin_unlock(&tmpmle->spinlock);\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&res->spinlock);\r\ndlm_put_mle(tmpmle);\r\nif (mle)\r\nkmem_cache_free(dlm_mle_cache, mle);\r\ngoto send_response;\r\n}\r\nspin_lock(&dlm->master_lock);\r\nfound = dlm_find_mle(dlm, &tmpmle, name, namelen);\r\nif (!found) {\r\nif (!mle) {\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\nmle = kmem_cache_alloc(dlm_mle_cache, GFP_NOFS);\r\nif (!mle) {\r\nresponse = DLM_MASTER_RESP_ERROR;\r\nmlog_errno(-ENOMEM);\r\ngoto send_response;\r\n}\r\ngoto way_up_top;\r\n}\r\ndlm_init_mle(mle, DLM_MLE_BLOCK, dlm, NULL, name, namelen);\r\nset_bit(request->node_idx, mle->maybe_map);\r\n__dlm_insert_mle(dlm, mle);\r\nresponse = DLM_MASTER_RESP_NO;\r\n} else {\r\nset_maybe = 1;\r\nspin_lock(&tmpmle->spinlock);\r\nif (tmpmle->master == dlm->node_num) {\r\nmlog(ML_ERROR, "no lockres, but an mle with this node as master!\n");\r\nBUG();\r\n}\r\nif (tmpmle->type == DLM_MLE_BLOCK)\r\nresponse = DLM_MASTER_RESP_NO;\r\nelse if (tmpmle->type == DLM_MLE_MIGRATION) {\r\nmlog(0, "migration mle was found (%u->%u)\n",\r\ntmpmle->master, tmpmle->new_master);\r\nresponse = DLM_MASTER_RESP_NO;\r\n} else\r\nresponse = DLM_MASTER_RESP_MAYBE;\r\nif (set_maybe)\r\nset_bit(request->node_idx, tmpmle->maybe_map);\r\nspin_unlock(&tmpmle->spinlock);\r\n}\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\nif (found) {\r\ndlm_put_mle(tmpmle);\r\n}\r\nsend_response:\r\nif (dispatch_assert) {\r\nif (response != DLM_MASTER_RESP_YES)\r\nmlog(ML_ERROR, "invalid response %d\n", response);\r\nif (!res) {\r\nmlog(ML_ERROR, "bad lockres while trying to assert!\n");\r\nBUG();\r\n}\r\nmlog(0, "%u is the owner of %.*s, cleaning everyone else\n",\r\ndlm->node_num, res->lockname.len, res->lockname.name);\r\nspin_lock(&res->spinlock);\r\nret = dlm_dispatch_assert_master(dlm, res, 0, request->node_idx,\r\nDLM_ASSERT_MASTER_MLE_CLEANUP);\r\nif (ret < 0) {\r\nmlog(ML_ERROR, "failed to dispatch assert master work\n");\r\nresponse = DLM_MASTER_RESP_ERROR;\r\ndlm_lockres_put(res);\r\n} else\r\n__dlm_lockres_grab_inflight_worker(dlm, res);\r\nspin_unlock(&res->spinlock);\r\n} else {\r\nif (res)\r\ndlm_lockres_put(res);\r\n}\r\ndlm_put(dlm);\r\nreturn response;\r\n}\r\nstatic int dlm_do_assert_master(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nvoid *nodemap, u32 flags)\r\n{\r\nstruct dlm_assert_master assert;\r\nint to, tmpret;\r\nstruct dlm_node_iter iter;\r\nint ret = 0;\r\nint reassert;\r\nconst char *lockname = res->lockname.name;\r\nunsigned int namelen = res->lockname.len;\r\nBUG_ON(namelen > O2NM_MAX_NAME_LEN);\r\nspin_lock(&res->spinlock);\r\nres->state |= DLM_LOCK_RES_SETREF_INPROG;\r\nspin_unlock(&res->spinlock);\r\nagain:\r\nreassert = 0;\r\ndlm_node_iter_init(nodemap, &iter);\r\nwhile ((to = dlm_node_iter_next(&iter)) >= 0) {\r\nint r = 0;\r\nstruct dlm_master_list_entry *mle = NULL;\r\nmlog(0, "sending assert master to %d (%.*s)\n", to,\r\nnamelen, lockname);\r\nmemset(&assert, 0, sizeof(assert));\r\nassert.node_idx = dlm->node_num;\r\nassert.namelen = namelen;\r\nmemcpy(assert.name, lockname, namelen);\r\nassert.flags = cpu_to_be32(flags);\r\ntmpret = o2net_send_message(DLM_ASSERT_MASTER_MSG, dlm->key,\r\n&assert, sizeof(assert), to, &r);\r\nif (tmpret < 0) {\r\nmlog(ML_ERROR, "Error %d when sending message %u (key "\r\n"0x%x) to node %u\n", tmpret,\r\nDLM_ASSERT_MASTER_MSG, dlm->key, to);\r\nif (!dlm_is_host_down(tmpret)) {\r\nmlog(ML_ERROR, "unhandled error=%d!\n", tmpret);\r\nBUG();\r\n}\r\nmlog(0, "link to %d went down!\n", to);\r\nret = tmpret;\r\nr = 0;\r\n} else if (r < 0) {\r\nmlog(ML_ERROR,"during assert master of %.*s to %u, "\r\n"got %d.\n", namelen, lockname, to, r);\r\nspin_lock(&dlm->spinlock);\r\nspin_lock(&dlm->master_lock);\r\nif (dlm_find_mle(dlm, &mle, (char *)lockname,\r\nnamelen)) {\r\ndlm_print_one_mle(mle);\r\n__dlm_put_mle(mle);\r\n}\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\nBUG();\r\n}\r\nif (r & DLM_ASSERT_RESPONSE_REASSERT &&\r\n!(r & DLM_ASSERT_RESPONSE_MASTERY_REF)) {\r\nmlog(ML_ERROR, "%.*s: very strange, "\r\n"master MLE but no lockres on %u\n",\r\nnamelen, lockname, to);\r\n}\r\nif (r & DLM_ASSERT_RESPONSE_REASSERT) {\r\nmlog(0, "%.*s: node %u create mles on other "\r\n"nodes and requests a re-assert\n",\r\nnamelen, lockname, to);\r\nreassert = 1;\r\n}\r\nif (r & DLM_ASSERT_RESPONSE_MASTERY_REF) {\r\nmlog(0, "%.*s: node %u has a reference to this "\r\n"lockres, set the bit in the refmap\n",\r\nnamelen, lockname, to);\r\nspin_lock(&res->spinlock);\r\ndlm_lockres_set_refmap_bit(dlm, res, to);\r\nspin_unlock(&res->spinlock);\r\n}\r\n}\r\nif (reassert)\r\ngoto again;\r\nspin_lock(&res->spinlock);\r\nres->state &= ~DLM_LOCK_RES_SETREF_INPROG;\r\nspin_unlock(&res->spinlock);\r\nwake_up(&res->wq);\r\nreturn ret;\r\n}\r\nint dlm_assert_master_handler(struct o2net_msg *msg, u32 len, void *data,\r\nvoid **ret_data)\r\n{\r\nstruct dlm_ctxt *dlm = data;\r\nstruct dlm_master_list_entry *mle = NULL;\r\nstruct dlm_assert_master *assert = (struct dlm_assert_master *)msg->buf;\r\nstruct dlm_lock_resource *res = NULL;\r\nchar *name;\r\nunsigned int namelen, hash;\r\nu32 flags;\r\nint master_request = 0, have_lockres_ref = 0;\r\nint ret = 0;\r\nif (!dlm_grab(dlm))\r\nreturn 0;\r\nname = assert->name;\r\nnamelen = assert->namelen;\r\nhash = dlm_lockid_hash(name, namelen);\r\nflags = be32_to_cpu(assert->flags);\r\nif (namelen > DLM_LOCKID_NAME_MAX) {\r\nmlog(ML_ERROR, "Invalid name length!");\r\ngoto done;\r\n}\r\nspin_lock(&dlm->spinlock);\r\nif (flags)\r\nmlog(0, "assert_master with flags: %u\n", flags);\r\nspin_lock(&dlm->master_lock);\r\nif (!dlm_find_mle(dlm, &mle, name, namelen)) {\r\nmlog(0, "just got an assert_master from %u, but no "\r\n"MLE for it! (%.*s)\n", assert->node_idx,\r\nnamelen, name);\r\n} else {\r\nint bit = find_next_bit (mle->maybe_map, O2NM_MAX_NODES, 0);\r\nif (bit >= O2NM_MAX_NODES) {\r\nmlog(0, "no bits set in the maybe_map, but %u "\r\n"is asserting! (%.*s)\n", assert->node_idx,\r\nnamelen, name);\r\n} else if (bit != assert->node_idx) {\r\nif (flags & DLM_ASSERT_MASTER_MLE_CLEANUP) {\r\nmlog(0, "master %u was found, %u should "\r\n"back off\n", assert->node_idx, bit);\r\n} else {\r\nmlog(0, "%u is the lowest node, "\r\n"%u is asserting. (%.*s) %u must "\r\n"have begun after %u won.\n", bit,\r\nassert->node_idx, namelen, name, bit,\r\nassert->node_idx);\r\n}\r\n}\r\nif (mle->type == DLM_MLE_MIGRATION) {\r\nif (flags & DLM_ASSERT_MASTER_MLE_CLEANUP) {\r\nmlog(0, "%s:%.*s: got cleanup assert"\r\n" from %u for migration\n",\r\ndlm->name, namelen, name,\r\nassert->node_idx);\r\n} else if (!(flags & DLM_ASSERT_MASTER_FINISH_MIGRATION)) {\r\nmlog(0, "%s:%.*s: got unrelated assert"\r\n" from %u for migration, ignoring\n",\r\ndlm->name, namelen, name,\r\nassert->node_idx);\r\n__dlm_put_mle(mle);\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\ngoto done;\r\n}\r\n}\r\n}\r\nspin_unlock(&dlm->master_lock);\r\nres = __dlm_lookup_lockres(dlm, name, namelen, hash);\r\nif (res) {\r\nspin_lock(&res->spinlock);\r\nif (res->state & DLM_LOCK_RES_RECOVERING) {\r\nmlog(ML_ERROR, "%u asserting but %.*s is "\r\n"RECOVERING!\n", assert->node_idx, namelen, name);\r\ngoto kill;\r\n}\r\nif (!mle) {\r\nif (res->owner != DLM_LOCK_RES_OWNER_UNKNOWN &&\r\nres->owner != assert->node_idx) {\r\nmlog(ML_ERROR, "DIE! Mastery assert from %u, "\r\n"but current owner is %u! (%.*s)\n",\r\nassert->node_idx, res->owner, namelen,\r\nname);\r\n__dlm_print_one_lock_resource(res);\r\nBUG();\r\n}\r\n} else if (mle->type != DLM_MLE_MIGRATION) {\r\nif (res->owner != DLM_LOCK_RES_OWNER_UNKNOWN) {\r\nif (res->owner == assert->node_idx) {\r\nmlog(0, "owner %u re-asserting on "\r\n"lock %.*s\n", assert->node_idx,\r\nnamelen, name);\r\ngoto ok;\r\n}\r\nmlog(ML_ERROR, "got assert_master from "\r\n"node %u, but %u is the owner! "\r\n"(%.*s)\n", assert->node_idx,\r\nres->owner, namelen, name);\r\ngoto kill;\r\n}\r\nif (!(res->state & DLM_LOCK_RES_IN_PROGRESS)) {\r\nmlog(ML_ERROR, "got assert from %u, but lock "\r\n"with no owner should be "\r\n"in-progress! (%.*s)\n",\r\nassert->node_idx,\r\nnamelen, name);\r\ngoto kill;\r\n}\r\n} else {\r\nif (assert->node_idx != mle->new_master) {\r\nmlog(ML_ERROR, "got assert from %u, but "\r\n"new master is %u, and old master "\r\n"was %u (%.*s)\n",\r\nassert->node_idx, mle->new_master,\r\nmle->master, namelen, name);\r\ngoto kill;\r\n}\r\n}\r\nok:\r\nspin_unlock(&res->spinlock);\r\n}\r\nif (mle) {\r\nint extra_ref = 0;\r\nint nn = -1;\r\nint rr, err = 0;\r\nspin_lock(&mle->spinlock);\r\nif (mle->type == DLM_MLE_BLOCK || mle->type == DLM_MLE_MIGRATION)\r\nextra_ref = 1;\r\nelse {\r\nwhile ((nn = find_next_bit (mle->response_map, O2NM_MAX_NODES,\r\nnn+1)) < O2NM_MAX_NODES) {\r\nif (nn != dlm->node_num && nn != assert->node_idx) {\r\nmaster_request = 1;\r\nbreak;\r\n}\r\n}\r\n}\r\nmle->master = assert->node_idx;\r\natomic_set(&mle->woken, 1);\r\nwake_up(&mle->wq);\r\nspin_unlock(&mle->spinlock);\r\nif (res) {\r\nint wake = 0;\r\nspin_lock(&res->spinlock);\r\nif (mle->type == DLM_MLE_MIGRATION) {\r\nmlog(0, "finishing off migration of lockres %.*s, "\r\n"from %u to %u\n",\r\nres->lockname.len, res->lockname.name,\r\ndlm->node_num, mle->new_master);\r\nres->state &= ~DLM_LOCK_RES_MIGRATING;\r\nwake = 1;\r\ndlm_change_lockres_owner(dlm, res, mle->new_master);\r\nBUG_ON(res->state & DLM_LOCK_RES_DIRTY);\r\n} else {\r\ndlm_change_lockres_owner(dlm, res, mle->master);\r\n}\r\nspin_unlock(&res->spinlock);\r\nhave_lockres_ref = 1;\r\nif (wake)\r\nwake_up(&res->wq);\r\n}\r\nspin_lock(&dlm->master_lock);\r\nrr = atomic_read(&mle->mle_refs.refcount);\r\nif (mle->inuse > 0) {\r\nif (extra_ref && rr < 3)\r\nerr = 1;\r\nelse if (!extra_ref && rr < 2)\r\nerr = 1;\r\n} else {\r\nif (extra_ref && rr < 2)\r\nerr = 1;\r\nelse if (!extra_ref && rr < 1)\r\nerr = 1;\r\n}\r\nif (err) {\r\nmlog(ML_ERROR, "%s:%.*s: got assert master from %u "\r\n"that will mess up this node, refs=%d, extra=%d, "\r\n"inuse=%d\n", dlm->name, namelen, name,\r\nassert->node_idx, rr, extra_ref, mle->inuse);\r\ndlm_print_one_mle(mle);\r\n}\r\n__dlm_unlink_mle(dlm, mle);\r\n__dlm_mle_detach_hb_events(dlm, mle);\r\n__dlm_put_mle(mle);\r\nif (extra_ref) {\r\n__dlm_put_mle(mle);\r\n}\r\nspin_unlock(&dlm->master_lock);\r\n} else if (res) {\r\nif (res->owner != assert->node_idx) {\r\nmlog(0, "assert_master from %u, but current "\r\n"owner is %u (%.*s), no mle\n", assert->node_idx,\r\nres->owner, namelen, name);\r\n}\r\n}\r\nspin_unlock(&dlm->spinlock);\r\ndone:\r\nret = 0;\r\nif (res) {\r\nspin_lock(&res->spinlock);\r\nres->state |= DLM_LOCK_RES_SETREF_INPROG;\r\nspin_unlock(&res->spinlock);\r\n*ret_data = (void *)res;\r\n}\r\ndlm_put(dlm);\r\nif (master_request) {\r\nmlog(0, "need to tell master to reassert\n");\r\nret |= DLM_ASSERT_RESPONSE_REASSERT;\r\nif (!have_lockres_ref) {\r\nmlog(ML_ERROR, "strange, got assert from %u, MASTER "\r\n"mle present here for %s:%.*s, but no lockres!\n",\r\nassert->node_idx, dlm->name, namelen, name);\r\n}\r\n}\r\nif (have_lockres_ref) {\r\nret |= DLM_ASSERT_RESPONSE_MASTERY_REF;\r\nmlog(0, "%s:%.*s: got assert from %u, need a ref\n",\r\ndlm->name, namelen, name, assert->node_idx);\r\n}\r\nreturn ret;\r\nkill:\r\nmlog(ML_ERROR, "Bad message received from another node. Dumping state "\r\n"and killing the other node now! This node is OK and can continue.\n");\r\n__dlm_print_one_lock_resource(res);\r\nspin_unlock(&res->spinlock);\r\nspin_lock(&dlm->master_lock);\r\nif (mle)\r\n__dlm_put_mle(mle);\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\n*ret_data = (void *)res;\r\ndlm_put(dlm);\r\nreturn -EINVAL;\r\n}\r\nvoid dlm_assert_master_post_handler(int status, void *data, void *ret_data)\r\n{\r\nstruct dlm_lock_resource *res = (struct dlm_lock_resource *)ret_data;\r\nif (ret_data) {\r\nspin_lock(&res->spinlock);\r\nres->state &= ~DLM_LOCK_RES_SETREF_INPROG;\r\nspin_unlock(&res->spinlock);\r\nwake_up(&res->wq);\r\ndlm_lockres_put(res);\r\n}\r\nreturn;\r\n}\r\nint dlm_dispatch_assert_master(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nint ignore_higher, u8 request_from, u32 flags)\r\n{\r\nstruct dlm_work_item *item;\r\nitem = kzalloc(sizeof(*item), GFP_ATOMIC);\r\nif (!item)\r\nreturn -ENOMEM;\r\ndlm_grab(dlm);\r\ndlm_init_work_item(dlm, item, dlm_assert_master_worker, NULL);\r\nitem->u.am.lockres = res;\r\nitem->u.am.ignore_higher = ignore_higher;\r\nitem->u.am.request_from = request_from;\r\nitem->u.am.flags = flags;\r\nif (ignore_higher)\r\nmlog(0, "IGNORE HIGHER: %.*s\n", res->lockname.len,\r\nres->lockname.name);\r\nspin_lock(&dlm->work_lock);\r\nlist_add_tail(&item->list, &dlm->work_list);\r\nspin_unlock(&dlm->work_lock);\r\nqueue_work(dlm->dlm_worker, &dlm->dispatched_work);\r\nreturn 0;\r\n}\r\nstatic void dlm_assert_master_worker(struct dlm_work_item *item, void *data)\r\n{\r\nstruct dlm_ctxt *dlm = data;\r\nint ret = 0;\r\nstruct dlm_lock_resource *res;\r\nunsigned long nodemap[BITS_TO_LONGS(O2NM_MAX_NODES)];\r\nint ignore_higher;\r\nint bit;\r\nu8 request_from;\r\nu32 flags;\r\ndlm = item->dlm;\r\nres = item->u.am.lockres;\r\nignore_higher = item->u.am.ignore_higher;\r\nrequest_from = item->u.am.request_from;\r\nflags = item->u.am.flags;\r\nspin_lock(&dlm->spinlock);\r\nmemcpy(nodemap, dlm->domain_map, sizeof(nodemap));\r\nspin_unlock(&dlm->spinlock);\r\nclear_bit(dlm->node_num, nodemap);\r\nif (ignore_higher) {\r\nclear_bit(request_from, nodemap);\r\nbit = dlm->node_num;\r\nwhile (1) {\r\nbit = find_next_bit(nodemap, O2NM_MAX_NODES,\r\nbit+1);\r\nif (bit >= O2NM_MAX_NODES)\r\nbreak;\r\nclear_bit(bit, nodemap);\r\n}\r\n}\r\nspin_lock(&res->spinlock);\r\nif (res->state & DLM_LOCK_RES_MIGRATING) {\r\nmlog(0, "Someone asked us to assert mastery, but we're "\r\n"in the middle of migration. Skipping assert, "\r\n"the new master will handle that.\n");\r\nspin_unlock(&res->spinlock);\r\ngoto put;\r\n} else\r\n__dlm_lockres_reserve_ast(res);\r\nspin_unlock(&res->spinlock);\r\nmlog(0, "worker about to master %.*s here, this=%u\n",\r\nres->lockname.len, res->lockname.name, dlm->node_num);\r\nret = dlm_do_assert_master(dlm, res, nodemap, flags);\r\nif (ret < 0) {\r\nif (!dlm_is_host_down(ret))\r\nmlog_errno(ret);\r\n}\r\ndlm_lockres_release_ast(dlm, res);\r\nput:\r\ndlm_lockres_drop_inflight_worker(dlm, res);\r\ndlm_lockres_put(res);\r\nmlog(0, "finished with dlm_assert_master_worker\n");\r\n}\r\nstatic int dlm_pre_master_reco_lockres(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nstruct dlm_node_iter iter;\r\nint nodenum;\r\nint ret = 0;\r\nu8 master = DLM_LOCK_RES_OWNER_UNKNOWN;\r\nspin_lock(&dlm->spinlock);\r\ndlm_node_iter_init(dlm->domain_map, &iter);\r\nspin_unlock(&dlm->spinlock);\r\nwhile ((nodenum = dlm_node_iter_next(&iter)) >= 0) {\r\nif (nodenum == dlm->node_num)\r\ncontinue;\r\nret = dlm_do_master_requery(dlm, res, nodenum, &master);\r\nif (ret < 0) {\r\nmlog_errno(ret);\r\nif (!dlm_is_host_down(ret))\r\nBUG();\r\nret = 0;\r\n}\r\nif (master != DLM_LOCK_RES_OWNER_UNKNOWN) {\r\nspin_lock(&dlm->spinlock);\r\nif (test_bit(master, dlm->recovery_map)) {\r\nmlog(ML_NOTICE, "%s: node %u has not seen "\r\n"node %u go down yet, and thinks the "\r\n"dead node is mastering the recovery "\r\n"lock. must wait.\n", dlm->name,\r\nnodenum, master);\r\nret = -EAGAIN;\r\n}\r\nspin_unlock(&dlm->spinlock);\r\nmlog(0, "%s: reco lock master is %u\n", dlm->name,\r\nmaster);\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nint dlm_drop_lockres_ref(struct dlm_ctxt *dlm, struct dlm_lock_resource *res)\r\n{\r\nstruct dlm_deref_lockres deref;\r\nint ret = 0, r;\r\nconst char *lockname;\r\nunsigned int namelen;\r\nlockname = res->lockname.name;\r\nnamelen = res->lockname.len;\r\nBUG_ON(namelen > O2NM_MAX_NAME_LEN);\r\nmemset(&deref, 0, sizeof(deref));\r\nderef.node_idx = dlm->node_num;\r\nderef.namelen = namelen;\r\nmemcpy(deref.name, lockname, namelen);\r\nret = o2net_send_message(DLM_DEREF_LOCKRES_MSG, dlm->key,\r\n&deref, sizeof(deref), res->owner, &r);\r\nif (ret < 0)\r\nmlog(ML_ERROR, "%s: res %.*s, error %d send DEREF to node %u\n",\r\ndlm->name, namelen, lockname, ret, res->owner);\r\nelse if (r < 0) {\r\nmlog(ML_ERROR, "%s: res %.*s, DEREF to node %u got %d\n",\r\ndlm->name, namelen, lockname, res->owner, r);\r\ndlm_print_one_lock_resource(res);\r\nBUG();\r\n}\r\nreturn ret;\r\n}\r\nint dlm_deref_lockres_handler(struct o2net_msg *msg, u32 len, void *data,\r\nvoid **ret_data)\r\n{\r\nstruct dlm_ctxt *dlm = data;\r\nstruct dlm_deref_lockres *deref = (struct dlm_deref_lockres *)msg->buf;\r\nstruct dlm_lock_resource *res = NULL;\r\nchar *name;\r\nunsigned int namelen;\r\nint ret = -EINVAL;\r\nu8 node;\r\nunsigned int hash;\r\nstruct dlm_work_item *item;\r\nint cleared = 0;\r\nint dispatch = 0;\r\nif (!dlm_grab(dlm))\r\nreturn 0;\r\nname = deref->name;\r\nnamelen = deref->namelen;\r\nnode = deref->node_idx;\r\nif (namelen > DLM_LOCKID_NAME_MAX) {\r\nmlog(ML_ERROR, "Invalid name length!");\r\ngoto done;\r\n}\r\nif (deref->node_idx >= O2NM_MAX_NODES) {\r\nmlog(ML_ERROR, "Invalid node number: %u\n", node);\r\ngoto done;\r\n}\r\nhash = dlm_lockid_hash(name, namelen);\r\nspin_lock(&dlm->spinlock);\r\nres = __dlm_lookup_lockres_full(dlm, name, namelen, hash);\r\nif (!res) {\r\nspin_unlock(&dlm->spinlock);\r\nmlog(ML_ERROR, "%s:%.*s: bad lockres name\n",\r\ndlm->name, namelen, name);\r\ngoto done;\r\n}\r\nspin_unlock(&dlm->spinlock);\r\nspin_lock(&res->spinlock);\r\nif (res->state & DLM_LOCK_RES_SETREF_INPROG)\r\ndispatch = 1;\r\nelse {\r\nBUG_ON(res->state & DLM_LOCK_RES_DROPPING_REF);\r\nif (test_bit(node, res->refmap)) {\r\ndlm_lockres_clear_refmap_bit(dlm, res, node);\r\ncleared = 1;\r\n}\r\n}\r\nspin_unlock(&res->spinlock);\r\nif (!dispatch) {\r\nif (cleared)\r\ndlm_lockres_calc_usage(dlm, res);\r\nelse {\r\nmlog(ML_ERROR, "%s:%.*s: node %u trying to drop ref "\r\n"but it is already dropped!\n", dlm->name,\r\nres->lockname.len, res->lockname.name, node);\r\ndlm_print_one_lock_resource(res);\r\n}\r\nret = 0;\r\ngoto done;\r\n}\r\nitem = kzalloc(sizeof(*item), GFP_NOFS);\r\nif (!item) {\r\nret = -ENOMEM;\r\nmlog_errno(ret);\r\ngoto done;\r\n}\r\ndlm_init_work_item(dlm, item, dlm_deref_lockres_worker, NULL);\r\nitem->u.dl.deref_res = res;\r\nitem->u.dl.deref_node = node;\r\nspin_lock(&dlm->work_lock);\r\nlist_add_tail(&item->list, &dlm->work_list);\r\nspin_unlock(&dlm->work_lock);\r\nqueue_work(dlm->dlm_worker, &dlm->dispatched_work);\r\nreturn 0;\r\ndone:\r\nif (res)\r\ndlm_lockres_put(res);\r\ndlm_put(dlm);\r\nreturn ret;\r\n}\r\nstatic void dlm_deref_lockres_worker(struct dlm_work_item *item, void *data)\r\n{\r\nstruct dlm_ctxt *dlm;\r\nstruct dlm_lock_resource *res;\r\nu8 node;\r\nu8 cleared = 0;\r\ndlm = item->dlm;\r\nres = item->u.dl.deref_res;\r\nnode = item->u.dl.deref_node;\r\nspin_lock(&res->spinlock);\r\nBUG_ON(res->state & DLM_LOCK_RES_DROPPING_REF);\r\nif (test_bit(node, res->refmap)) {\r\n__dlm_wait_on_lockres_flags(res, DLM_LOCK_RES_SETREF_INPROG);\r\ndlm_lockres_clear_refmap_bit(dlm, res, node);\r\ncleared = 1;\r\n}\r\nspin_unlock(&res->spinlock);\r\nif (cleared) {\r\nmlog(0, "%s:%.*s node %u ref dropped in dispatch\n",\r\ndlm->name, res->lockname.len, res->lockname.name, node);\r\ndlm_lockres_calc_usage(dlm, res);\r\n} else {\r\nmlog(ML_ERROR, "%s:%.*s: node %u trying to drop ref "\r\n"but it is already dropped!\n", dlm->name,\r\nres->lockname.len, res->lockname.name, node);\r\ndlm_print_one_lock_resource(res);\r\n}\r\ndlm_lockres_put(res);\r\n}\r\nstatic int dlm_is_lockres_migrateable(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nenum dlm_lockres_list idx;\r\nint nonlocal = 0, node_ref;\r\nstruct list_head *queue;\r\nstruct dlm_lock *lock;\r\nu64 cookie;\r\nassert_spin_locked(&res->spinlock);\r\nif (res->state & DLM_LOCK_RES_MIGRATING)\r\nreturn 0;\r\nif (res->state & DLM_LOCK_RES_RECOVERING)\r\nreturn 0;\r\nif (res->owner != dlm->node_num)\r\nreturn 0;\r\nfor (idx = DLM_GRANTED_LIST; idx <= DLM_BLOCKED_LIST; idx++) {\r\nqueue = dlm_list_idx_to_ptr(res, idx);\r\nlist_for_each_entry(lock, queue, list) {\r\nif (lock->ml.node != dlm->node_num) {\r\nnonlocal++;\r\ncontinue;\r\n}\r\ncookie = be64_to_cpu(lock->ml.cookie);\r\nmlog(0, "%s: Not migrateable res %.*s, lock %u:%llu on "\r\n"%s list\n", dlm->name, res->lockname.len,\r\nres->lockname.name,\r\ndlm_get_lock_cookie_node(cookie),\r\ndlm_get_lock_cookie_seq(cookie),\r\ndlm_list_in_text(idx));\r\nreturn 0;\r\n}\r\n}\r\nif (!nonlocal) {\r\nnode_ref = find_next_bit(res->refmap, O2NM_MAX_NODES, 0);\r\nif (node_ref >= O2NM_MAX_NODES)\r\nreturn 0;\r\n}\r\nmlog(0, "%s: res %.*s, Migrateable\n", dlm->name, res->lockname.len,\r\nres->lockname.name);\r\nreturn 1;\r\n}\r\nstatic int dlm_migrate_lockres(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res, u8 target)\r\n{\r\nstruct dlm_master_list_entry *mle = NULL;\r\nstruct dlm_master_list_entry *oldmle = NULL;\r\nstruct dlm_migratable_lockres *mres = NULL;\r\nint ret = 0;\r\nconst char *name;\r\nunsigned int namelen;\r\nint mle_added = 0;\r\nint wake = 0;\r\nif (!dlm_grab(dlm))\r\nreturn -EINVAL;\r\nBUG_ON(target == O2NM_MAX_NODES);\r\nname = res->lockname.name;\r\nnamelen = res->lockname.len;\r\nmlog(0, "%s: Migrating %.*s to node %u\n", dlm->name, namelen, name,\r\ntarget);\r\nret = -ENOMEM;\r\nmres = (struct dlm_migratable_lockres *) __get_free_page(GFP_NOFS);\r\nif (!mres) {\r\nmlog_errno(ret);\r\ngoto leave;\r\n}\r\nmle = kmem_cache_alloc(dlm_mle_cache, GFP_NOFS);\r\nif (!mle) {\r\nmlog_errno(ret);\r\ngoto leave;\r\n}\r\nret = 0;\r\nspin_lock(&dlm->spinlock);\r\nspin_lock(&dlm->master_lock);\r\nret = dlm_add_migration_mle(dlm, res, mle, &oldmle, name,\r\nnamelen, target, dlm->node_num);\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\nif (ret == -EEXIST) {\r\nmlog(0, "another process is already migrating it\n");\r\ngoto fail;\r\n}\r\nmle_added = 1;\r\nif (dlm_mark_lockres_migrating(dlm, res, target) < 0) {\r\nmlog(ML_ERROR, "tried to migrate %.*s to %u, but "\r\n"the target went down.\n", res->lockname.len,\r\nres->lockname.name, target);\r\nspin_lock(&res->spinlock);\r\nres->state &= ~DLM_LOCK_RES_MIGRATING;\r\nwake = 1;\r\nspin_unlock(&res->spinlock);\r\nret = -EINVAL;\r\n}\r\nfail:\r\nif (oldmle) {\r\ndlm_mle_detach_hb_events(dlm, oldmle);\r\ndlm_put_mle(oldmle);\r\n}\r\nif (ret < 0) {\r\nif (mle_added) {\r\ndlm_mle_detach_hb_events(dlm, mle);\r\ndlm_put_mle(mle);\r\n} else if (mle) {\r\nkmem_cache_free(dlm_mle_cache, mle);\r\nmle = NULL;\r\n}\r\ngoto leave;\r\n}\r\nflush_workqueue(dlm->dlm_worker);\r\nspin_lock(&dlm->spinlock);\r\nspin_lock(&dlm->master_lock);\r\ndlm_get_mle_inuse(mle);\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\nret = dlm_send_one_lockres(dlm, res, mres, target,\r\nDLM_MRES_MIGRATION);\r\nif (ret < 0) {\r\nmlog(0, "migration to node %u failed with %d\n",\r\ntarget, ret);\r\ndlm_mle_detach_hb_events(dlm, mle);\r\ndlm_put_mle(mle);\r\ndlm_put_mle_inuse(mle);\r\nspin_lock(&res->spinlock);\r\nres->state &= ~DLM_LOCK_RES_MIGRATING;\r\nwake = 1;\r\nspin_unlock(&res->spinlock);\r\nif (dlm_is_host_down(ret))\r\ndlm_wait_for_node_death(dlm, target,\r\nDLM_NODE_DEATH_WAIT_MAX);\r\ngoto leave;\r\n}\r\nwhile (1) {\r\nret = wait_event_interruptible_timeout(mle->wq,\r\n(atomic_read(&mle->woken) == 1),\r\nmsecs_to_jiffies(5000));\r\nif (ret >= 0) {\r\nif (atomic_read(&mle->woken) == 1 ||\r\nres->owner == target)\r\nbreak;\r\nmlog(0, "%s:%.*s: timed out during migration\n",\r\ndlm->name, res->lockname.len, res->lockname.name);\r\nif (dlm_is_node_dead(dlm, target)) {\r\nmlog(0, "%s:%.*s: expected migration "\r\n"target %u is no longer up, restarting\n",\r\ndlm->name, res->lockname.len,\r\nres->lockname.name, target);\r\nret = -EINVAL;\r\ndlm_mle_detach_hb_events(dlm, mle);\r\ndlm_put_mle(mle);\r\ndlm_put_mle_inuse(mle);\r\nspin_lock(&res->spinlock);\r\nres->state &= ~DLM_LOCK_RES_MIGRATING;\r\nwake = 1;\r\nspin_unlock(&res->spinlock);\r\ngoto leave;\r\n}\r\n} else\r\nmlog(0, "%s:%.*s: caught signal during migration\n",\r\ndlm->name, res->lockname.len, res->lockname.name);\r\n}\r\nspin_lock(&res->spinlock);\r\ndlm_set_lockres_owner(dlm, res, target);\r\nres->state &= ~DLM_LOCK_RES_MIGRATING;\r\ndlm_remove_nonlocal_locks(dlm, res);\r\nspin_unlock(&res->spinlock);\r\nwake_up(&res->wq);\r\ndlm_mle_detach_hb_events(dlm, mle);\r\ndlm_put_mle_inuse(mle);\r\nret = 0;\r\ndlm_lockres_calc_usage(dlm, res);\r\nleave:\r\nif (ret < 0)\r\ndlm_kick_thread(dlm, res);\r\nif (wake)\r\nwake_up(&res->wq);\r\nif (mres)\r\nfree_page((unsigned long)mres);\r\ndlm_put(dlm);\r\nmlog(0, "%s: Migrating %.*s to %u, returns %d\n", dlm->name, namelen,\r\nname, target, ret);\r\nreturn ret;\r\n}\r\nint dlm_empty_lockres(struct dlm_ctxt *dlm, struct dlm_lock_resource *res)\r\n{\r\nint ret;\r\nint lock_dropped = 0;\r\nu8 target = O2NM_MAX_NODES;\r\nassert_spin_locked(&dlm->spinlock);\r\nspin_lock(&res->spinlock);\r\nif (dlm_is_lockres_migrateable(dlm, res))\r\ntarget = dlm_pick_migration_target(dlm, res);\r\nspin_unlock(&res->spinlock);\r\nif (target == O2NM_MAX_NODES)\r\ngoto leave;\r\nspin_unlock(&dlm->spinlock);\r\nlock_dropped = 1;\r\nret = dlm_migrate_lockres(dlm, res, target);\r\nif (ret)\r\nmlog(0, "%s: res %.*s, Migrate to node %u failed with %d\n",\r\ndlm->name, res->lockname.len, res->lockname.name,\r\ntarget, ret);\r\nspin_lock(&dlm->spinlock);\r\nleave:\r\nreturn lock_dropped;\r\n}\r\nint dlm_lock_basts_flushed(struct dlm_ctxt *dlm, struct dlm_lock *lock)\r\n{\r\nint ret;\r\nspin_lock(&dlm->ast_lock);\r\nspin_lock(&lock->spinlock);\r\nret = (list_empty(&lock->bast_list) && !lock->bast_pending);\r\nspin_unlock(&lock->spinlock);\r\nspin_unlock(&dlm->ast_lock);\r\nreturn ret;\r\n}\r\nstatic int dlm_migration_can_proceed(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nu8 mig_target)\r\n{\r\nint can_proceed;\r\nspin_lock(&res->spinlock);\r\ncan_proceed = !!(res->state & DLM_LOCK_RES_MIGRATING);\r\nspin_unlock(&res->spinlock);\r\nspin_lock(&dlm->spinlock);\r\nif (!test_bit(mig_target, dlm->domain_map))\r\ncan_proceed = 1;\r\nspin_unlock(&dlm->spinlock);\r\nreturn can_proceed;\r\n}\r\nstatic int dlm_lockres_is_dirty(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nint ret;\r\nspin_lock(&res->spinlock);\r\nret = !!(res->state & DLM_LOCK_RES_DIRTY);\r\nspin_unlock(&res->spinlock);\r\nreturn ret;\r\n}\r\nstatic int dlm_mark_lockres_migrating(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nu8 target)\r\n{\r\nint ret = 0;\r\nmlog(0, "dlm_mark_lockres_migrating: %.*s, from %u to %u\n",\r\nres->lockname.len, res->lockname.name, dlm->node_num,\r\ntarget);\r\nspin_lock(&res->spinlock);\r\nBUG_ON(res->migration_pending);\r\nres->migration_pending = 1;\r\n__dlm_lockres_reserve_ast(res);\r\nspin_unlock(&res->spinlock);\r\ndlm_kick_thread(dlm, res);\r\nspin_lock(&res->spinlock);\r\nBUG_ON(res->state & DLM_LOCK_RES_BLOCK_DIRTY);\r\nres->state |= DLM_LOCK_RES_BLOCK_DIRTY;\r\nspin_unlock(&res->spinlock);\r\nwait_event(dlm->ast_wq, !dlm_lockres_is_dirty(dlm, res));\r\ndlm_lockres_release_ast(dlm, res);\r\nmlog(0, "about to wait on migration_wq, dirty=%s\n",\r\nres->state & DLM_LOCK_RES_DIRTY ? "yes" : "no");\r\nagain:\r\nret = wait_event_interruptible_timeout(dlm->migration_wq,\r\ndlm_migration_can_proceed(dlm, res, target),\r\nmsecs_to_jiffies(1000));\r\nif (ret < 0) {\r\nmlog(0, "woken again: migrating? %s, dead? %s\n",\r\nres->state & DLM_LOCK_RES_MIGRATING ? "yes":"no",\r\ntest_bit(target, dlm->domain_map) ? "no":"yes");\r\n} else {\r\nmlog(0, "all is well: migrating? %s, dead? %s\n",\r\nres->state & DLM_LOCK_RES_MIGRATING ? "yes":"no",\r\ntest_bit(target, dlm->domain_map) ? "no":"yes");\r\n}\r\nif (!dlm_migration_can_proceed(dlm, res, target)) {\r\nmlog(0, "trying again...\n");\r\ngoto again;\r\n}\r\nret = 0;\r\nspin_lock(&dlm->spinlock);\r\nif (!test_bit(target, dlm->domain_map)) {\r\nmlog(ML_ERROR, "aha. migration target %u just went down\n",\r\ntarget);\r\nret = -EHOSTDOWN;\r\n}\r\nspin_unlock(&dlm->spinlock);\r\nspin_lock(&res->spinlock);\r\nBUG_ON(!(res->state & DLM_LOCK_RES_BLOCK_DIRTY));\r\nres->state &= ~DLM_LOCK_RES_BLOCK_DIRTY;\r\nif (!ret)\r\nBUG_ON(!(res->state & DLM_LOCK_RES_MIGRATING));\r\nspin_unlock(&res->spinlock);\r\nreturn ret;\r\n}\r\nstatic void dlm_remove_nonlocal_locks(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nstruct list_head *queue = &res->granted;\r\nint i, bit;\r\nstruct dlm_lock *lock, *next;\r\nassert_spin_locked(&res->spinlock);\r\nBUG_ON(res->owner == dlm->node_num);\r\nfor (i=0; i<3; i++) {\r\nlist_for_each_entry_safe(lock, next, queue, list) {\r\nif (lock->ml.node != dlm->node_num) {\r\nmlog(0, "putting lock for node %u\n",\r\nlock->ml.node);\r\nBUG_ON(!list_empty(&lock->ast_list));\r\nBUG_ON(!list_empty(&lock->bast_list));\r\nBUG_ON(lock->ast_pending);\r\nBUG_ON(lock->bast_pending);\r\ndlm_lockres_clear_refmap_bit(dlm, res,\r\nlock->ml.node);\r\nlist_del_init(&lock->list);\r\ndlm_lock_put(lock);\r\ndlm_lock_put(lock);\r\n}\r\n}\r\nqueue++;\r\n}\r\nbit = 0;\r\nwhile (1) {\r\nbit = find_next_bit(res->refmap, O2NM_MAX_NODES, bit);\r\nif (bit >= O2NM_MAX_NODES)\r\nbreak;\r\nif (bit != dlm->node_num) {\r\nmlog(0, "%s:%.*s: node %u had a ref to this "\r\n"migrating lockres, clearing\n", dlm->name,\r\nres->lockname.len, res->lockname.name, bit);\r\ndlm_lockres_clear_refmap_bit(dlm, res, bit);\r\n}\r\nbit++;\r\n}\r\n}\r\nstatic u8 dlm_pick_migration_target(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nenum dlm_lockres_list idx;\r\nstruct list_head *queue = &res->granted;\r\nstruct dlm_lock *lock;\r\nint noderef;\r\nu8 nodenum = O2NM_MAX_NODES;\r\nassert_spin_locked(&dlm->spinlock);\r\nassert_spin_locked(&res->spinlock);\r\nfor (idx = DLM_GRANTED_LIST; idx <= DLM_BLOCKED_LIST; idx++) {\r\nqueue = dlm_list_idx_to_ptr(res, idx);\r\nlist_for_each_entry(lock, queue, list) {\r\nif (lock->ml.node == dlm->node_num)\r\ncontinue;\r\nif (test_bit(lock->ml.node, dlm->exit_domain_map))\r\ncontinue;\r\nnodenum = lock->ml.node;\r\ngoto bail;\r\n}\r\n}\r\nnoderef = -1;\r\nwhile (1) {\r\nnoderef = find_next_bit(res->refmap, O2NM_MAX_NODES,\r\nnoderef + 1);\r\nif (noderef >= O2NM_MAX_NODES)\r\nbreak;\r\nif (noderef == dlm->node_num)\r\ncontinue;\r\nif (test_bit(noderef, dlm->exit_domain_map))\r\ncontinue;\r\nnodenum = noderef;\r\ngoto bail;\r\n}\r\nbail:\r\nreturn nodenum;\r\n}\r\nstatic int dlm_do_migrate_request(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nu8 master, u8 new_master,\r\nstruct dlm_node_iter *iter)\r\n{\r\nstruct dlm_migrate_request migrate;\r\nint ret, skip, status = 0;\r\nint nodenum;\r\nmemset(&migrate, 0, sizeof(migrate));\r\nmigrate.namelen = res->lockname.len;\r\nmemcpy(migrate.name, res->lockname.name, migrate.namelen);\r\nmigrate.new_master = new_master;\r\nmigrate.master = master;\r\nret = 0;\r\nwhile ((nodenum = dlm_node_iter_next(iter)) >= 0) {\r\nif (nodenum == master ||\r\nnodenum == new_master)\r\ncontinue;\r\nspin_lock(&dlm->spinlock);\r\nskip = (!test_bit(nodenum, dlm->domain_map));\r\nspin_unlock(&dlm->spinlock);\r\nif (skip) {\r\nclear_bit(nodenum, iter->node_map);\r\ncontinue;\r\n}\r\nret = o2net_send_message(DLM_MIGRATE_REQUEST_MSG, dlm->key,\r\n&migrate, sizeof(migrate), nodenum,\r\n&status);\r\nif (ret < 0) {\r\nmlog(ML_ERROR, "%s: res %.*s, Error %d send "\r\n"MIGRATE_REQUEST to node %u\n", dlm->name,\r\nmigrate.namelen, migrate.name, ret, nodenum);\r\nif (!dlm_is_host_down(ret)) {\r\nmlog(ML_ERROR, "unhandled error=%d!\n", ret);\r\nBUG();\r\n}\r\nclear_bit(nodenum, iter->node_map);\r\nret = 0;\r\n} else if (status < 0) {\r\nmlog(0, "migrate request (node %u) returned %d!\n",\r\nnodenum, status);\r\nret = status;\r\n} else if (status == DLM_MIGRATE_RESPONSE_MASTERY_REF) {\r\nmlog(0, "%s:%.*s: need ref for node %u\n",\r\ndlm->name, res->lockname.len, res->lockname.name,\r\nnodenum);\r\nspin_lock(&res->spinlock);\r\ndlm_lockres_set_refmap_bit(dlm, res, nodenum);\r\nspin_unlock(&res->spinlock);\r\n}\r\n}\r\nif (ret < 0)\r\nmlog_errno(ret);\r\nmlog(0, "returning ret=%d\n", ret);\r\nreturn ret;\r\n}\r\nint dlm_migrate_request_handler(struct o2net_msg *msg, u32 len, void *data,\r\nvoid **ret_data)\r\n{\r\nstruct dlm_ctxt *dlm = data;\r\nstruct dlm_lock_resource *res = NULL;\r\nstruct dlm_migrate_request *migrate = (struct dlm_migrate_request *) msg->buf;\r\nstruct dlm_master_list_entry *mle = NULL, *oldmle = NULL;\r\nconst char *name;\r\nunsigned int namelen, hash;\r\nint ret = 0;\r\nif (!dlm_grab(dlm))\r\nreturn -EINVAL;\r\nname = migrate->name;\r\nnamelen = migrate->namelen;\r\nhash = dlm_lockid_hash(name, namelen);\r\nmle = kmem_cache_alloc(dlm_mle_cache, GFP_NOFS);\r\nif (!mle) {\r\nret = -ENOMEM;\r\ngoto leave;\r\n}\r\nspin_lock(&dlm->spinlock);\r\nres = __dlm_lookup_lockres(dlm, name, namelen, hash);\r\nif (res) {\r\nspin_lock(&res->spinlock);\r\nif (res->state & DLM_LOCK_RES_RECOVERING) {\r\nspin_unlock(&res->spinlock);\r\nmlog(ML_ERROR, "Got a migrate request, but the "\r\n"lockres is marked as recovering!");\r\nkmem_cache_free(dlm_mle_cache, mle);\r\nret = -EINVAL;\r\ngoto unlock;\r\n}\r\nres->state |= DLM_LOCK_RES_MIGRATING;\r\nspin_unlock(&res->spinlock);\r\n}\r\nspin_lock(&dlm->master_lock);\r\nret = dlm_add_migration_mle(dlm, res, mle, &oldmle,\r\nname, namelen,\r\nmigrate->new_master,\r\nmigrate->master);\r\nspin_unlock(&dlm->master_lock);\r\nunlock:\r\nspin_unlock(&dlm->spinlock);\r\nif (oldmle) {\r\ndlm_mle_detach_hb_events(dlm, oldmle);\r\ndlm_put_mle(oldmle);\r\n}\r\nif (res)\r\ndlm_lockres_put(res);\r\nleave:\r\ndlm_put(dlm);\r\nreturn ret;\r\n}\r\nstatic int dlm_add_migration_mle(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res,\r\nstruct dlm_master_list_entry *mle,\r\nstruct dlm_master_list_entry **oldmle,\r\nconst char *name, unsigned int namelen,\r\nu8 new_master, u8 master)\r\n{\r\nint found;\r\nint ret = 0;\r\n*oldmle = NULL;\r\nassert_spin_locked(&dlm->spinlock);\r\nassert_spin_locked(&dlm->master_lock);\r\nfound = dlm_find_mle(dlm, oldmle, (char *)name, namelen);\r\nif (found) {\r\nstruct dlm_master_list_entry *tmp = *oldmle;\r\nspin_lock(&tmp->spinlock);\r\nif (tmp->type == DLM_MLE_MIGRATION) {\r\nif (master == dlm->node_num) {\r\nmlog(0, "tried to migrate %.*s, but some "\r\n"process beat me to it\n",\r\nnamelen, name);\r\nret = -EEXIST;\r\n} else {\r\nmlog(ML_ERROR, "migration error mle: "\r\n"master=%u new_master=%u // request: "\r\n"master=%u new_master=%u // "\r\n"lockres=%.*s\n",\r\ntmp->master, tmp->new_master,\r\nmaster, new_master,\r\nnamelen, name);\r\nBUG();\r\n}\r\n} else {\r\ntmp->master = master;\r\natomic_set(&tmp->woken, 1);\r\nwake_up(&tmp->wq);\r\n__dlm_unlink_mle(dlm, tmp);\r\n__dlm_mle_detach_hb_events(dlm, tmp);\r\nif (tmp->type == DLM_MLE_MASTER) {\r\nret = DLM_MIGRATE_RESPONSE_MASTERY_REF;\r\nmlog(0, "%s:%.*s: master=%u, newmaster=%u, "\r\n"telling master to get ref "\r\n"for cleared out mle during "\r\n"migration\n", dlm->name,\r\nnamelen, name, master,\r\nnew_master);\r\n}\r\n}\r\nspin_unlock(&tmp->spinlock);\r\n}\r\ndlm_init_mle(mle, DLM_MLE_MIGRATION, dlm, res, name, namelen);\r\nmle->new_master = new_master;\r\nmle->master = master;\r\nset_bit(new_master, mle->maybe_map);\r\n__dlm_insert_mle(dlm, mle);\r\nreturn ret;\r\n}\r\nstatic struct dlm_lock_resource *dlm_reset_mleres_owner(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry *mle)\r\n{\r\nstruct dlm_lock_resource *res;\r\nres = __dlm_lookup_lockres(dlm, mle->mname, mle->mnamelen,\r\nmle->mnamehash);\r\nif (res) {\r\nspin_unlock(&dlm->master_lock);\r\nspin_lock(&res->spinlock);\r\ndlm_set_lockres_owner(dlm, res, DLM_LOCK_RES_OWNER_UNKNOWN);\r\ndlm_move_lockres_to_recovery_list(dlm, res);\r\nspin_unlock(&res->spinlock);\r\ndlm_lockres_put(res);\r\n__dlm_mle_detach_hb_events(dlm, mle);\r\nspin_lock(&dlm->master_lock);\r\n__dlm_put_mle(mle);\r\nspin_unlock(&dlm->master_lock);\r\n}\r\nreturn res;\r\n}\r\nstatic void dlm_clean_migration_mle(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry *mle)\r\n{\r\n__dlm_mle_detach_hb_events(dlm, mle);\r\nspin_lock(&mle->spinlock);\r\n__dlm_unlink_mle(dlm, mle);\r\natomic_set(&mle->woken, 1);\r\nspin_unlock(&mle->spinlock);\r\nwake_up(&mle->wq);\r\n}\r\nstatic void dlm_clean_block_mle(struct dlm_ctxt *dlm,\r\nstruct dlm_master_list_entry *mle, u8 dead_node)\r\n{\r\nint bit;\r\nBUG_ON(mle->type != DLM_MLE_BLOCK);\r\nspin_lock(&mle->spinlock);\r\nbit = find_next_bit(mle->maybe_map, O2NM_MAX_NODES, 0);\r\nif (bit != dead_node) {\r\nmlog(0, "mle found, but dead node %u would not have been "\r\n"master\n", dead_node);\r\nspin_unlock(&mle->spinlock);\r\n} else {\r\nmlog(0, "node %u was expected master\n", dead_node);\r\natomic_set(&mle->woken, 1);\r\nspin_unlock(&mle->spinlock);\r\nwake_up(&mle->wq);\r\n__dlm_mle_detach_hb_events(dlm, mle);\r\n__dlm_put_mle(mle);\r\n}\r\n}\r\nvoid dlm_clean_master_list(struct dlm_ctxt *dlm, u8 dead_node)\r\n{\r\nstruct dlm_master_list_entry *mle;\r\nstruct dlm_lock_resource *res;\r\nstruct hlist_head *bucket;\r\nstruct hlist_node *tmp;\r\nunsigned int i;\r\nmlog(0, "dlm=%s, dead node=%u\n", dlm->name, dead_node);\r\ntop:\r\nassert_spin_locked(&dlm->spinlock);\r\nspin_lock(&dlm->master_lock);\r\nfor (i = 0; i < DLM_HASH_BUCKETS; i++) {\r\nbucket = dlm_master_hash(dlm, i);\r\nhlist_for_each_entry_safe(mle, tmp, bucket, master_hash_node) {\r\nBUG_ON(mle->type != DLM_MLE_BLOCK &&\r\nmle->type != DLM_MLE_MASTER &&\r\nmle->type != DLM_MLE_MIGRATION);\r\nif (mle->type == DLM_MLE_MASTER)\r\ncontinue;\r\nif (mle->type == DLM_MLE_BLOCK) {\r\ndlm_clean_block_mle(dlm, mle, dead_node);\r\ncontinue;\r\n}\r\nif (mle->master != dead_node &&\r\nmle->new_master != dead_node)\r\ncontinue;\r\ndlm_clean_migration_mle(dlm, mle);\r\nmlog(0, "%s: node %u died during migration from "\r\n"%u to %u!\n", dlm->name, dead_node, mle->master,\r\nmle->new_master);\r\nres = dlm_reset_mleres_owner(dlm, mle);\r\nif (res)\r\ngoto top;\r\n__dlm_put_mle(mle);\r\n}\r\n}\r\nspin_unlock(&dlm->master_lock);\r\n}\r\nint dlm_finish_migration(struct dlm_ctxt *dlm, struct dlm_lock_resource *res,\r\nu8 old_master)\r\n{\r\nstruct dlm_node_iter iter;\r\nint ret = 0;\r\nspin_lock(&dlm->spinlock);\r\ndlm_node_iter_init(dlm->domain_map, &iter);\r\nclear_bit(old_master, iter.node_map);\r\nclear_bit(dlm->node_num, iter.node_map);\r\nspin_unlock(&dlm->spinlock);\r\nspin_lock(&res->spinlock);\r\ndlm_lockres_set_refmap_bit(dlm, res, old_master);\r\nspin_unlock(&res->spinlock);\r\nmlog(0, "now time to do a migrate request to other nodes\n");\r\nret = dlm_do_migrate_request(dlm, res, old_master,\r\ndlm->node_num, &iter);\r\nif (ret < 0) {\r\nmlog_errno(ret);\r\ngoto leave;\r\n}\r\nmlog(0, "doing assert master of %.*s to all except the original node\n",\r\nres->lockname.len, res->lockname.name);\r\nret = dlm_do_assert_master(dlm, res, iter.node_map,\r\nDLM_ASSERT_MASTER_FINISH_MIGRATION);\r\nif (ret < 0) {\r\nmlog_errno(ret);\r\nret = 0;\r\n}\r\nmemset(iter.node_map, 0, sizeof(iter.node_map));\r\nset_bit(old_master, iter.node_map);\r\nmlog(0, "doing assert master of %.*s back to %u\n",\r\nres->lockname.len, res->lockname.name, old_master);\r\nret = dlm_do_assert_master(dlm, res, iter.node_map,\r\nDLM_ASSERT_MASTER_FINISH_MIGRATION);\r\nif (ret < 0) {\r\nmlog(0, "assert master to original master failed "\r\n"with %d.\n", ret);\r\nret = 0;\r\n}\r\nspin_lock(&res->spinlock);\r\ndlm_set_lockres_owner(dlm, res, dlm->node_num);\r\nres->state &= ~DLM_LOCK_RES_MIGRATING;\r\nspin_unlock(&res->spinlock);\r\ndlm_kick_thread(dlm, res);\r\nwake_up(&res->wq);\r\nleave:\r\nreturn ret;\r\n}\r\nvoid __dlm_lockres_reserve_ast(struct dlm_lock_resource *res)\r\n{\r\nassert_spin_locked(&res->spinlock);\r\nif (res->state & DLM_LOCK_RES_MIGRATING) {\r\n__dlm_print_one_lock_resource(res);\r\n}\r\nBUG_ON(res->state & DLM_LOCK_RES_MIGRATING);\r\natomic_inc(&res->asts_reserved);\r\n}\r\nvoid dlm_lockres_release_ast(struct dlm_ctxt *dlm,\r\nstruct dlm_lock_resource *res)\r\n{\r\nif (!atomic_dec_and_lock(&res->asts_reserved, &res->spinlock))\r\nreturn;\r\nif (!res->migration_pending) {\r\nspin_unlock(&res->spinlock);\r\nreturn;\r\n}\r\nBUG_ON(res->state & DLM_LOCK_RES_MIGRATING);\r\nres->migration_pending = 0;\r\nres->state |= DLM_LOCK_RES_MIGRATING;\r\nspin_unlock(&res->spinlock);\r\nwake_up(&res->wq);\r\nwake_up(&dlm->migration_wq);\r\n}\r\nvoid dlm_force_free_mles(struct dlm_ctxt *dlm)\r\n{\r\nint i;\r\nstruct hlist_head *bucket;\r\nstruct dlm_master_list_entry *mle;\r\nstruct hlist_node *tmp;\r\nspin_lock(&dlm->spinlock);\r\nspin_lock(&dlm->master_lock);\r\nBUG_ON(dlm->dlm_state != DLM_CTXT_LEAVING);\r\nBUG_ON((find_next_bit(dlm->domain_map, O2NM_MAX_NODES, 0) < O2NM_MAX_NODES));\r\nfor (i = 0; i < DLM_HASH_BUCKETS; i++) {\r\nbucket = dlm_master_hash(dlm, i);\r\nhlist_for_each_entry_safe(mle, tmp, bucket, master_hash_node) {\r\nif (mle->type != DLM_MLE_BLOCK) {\r\nmlog(ML_ERROR, "bad mle: %p\n", mle);\r\ndlm_print_one_mle(mle);\r\n}\r\natomic_set(&mle->woken, 1);\r\nwake_up(&mle->wq);\r\n__dlm_unlink_mle(dlm, mle);\r\n__dlm_mle_detach_hb_events(dlm, mle);\r\n__dlm_put_mle(mle);\r\n}\r\n}\r\nspin_unlock(&dlm->master_lock);\r\nspin_unlock(&dlm->spinlock);\r\n}
