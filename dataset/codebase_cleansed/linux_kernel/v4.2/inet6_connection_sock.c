int inet6_csk_bind_conflict(const struct sock *sk,\r\nconst struct inet_bind_bucket *tb, bool relax)\r\n{\r\nconst struct sock *sk2;\r\nint reuse = sk->sk_reuse;\r\nint reuseport = sk->sk_reuseport;\r\nkuid_t uid = sock_i_uid((struct sock *)sk);\r\nsk_for_each_bound(sk2, &tb->owners) {\r\nif (sk != sk2 &&\r\n(!sk->sk_bound_dev_if ||\r\n!sk2->sk_bound_dev_if ||\r\nsk->sk_bound_dev_if == sk2->sk_bound_dev_if)) {\r\nif ((!reuse || !sk2->sk_reuse ||\r\nsk2->sk_state == TCP_LISTEN) &&\r\n(!reuseport || !sk2->sk_reuseport ||\r\n(sk2->sk_state != TCP_TIME_WAIT &&\r\n!uid_eq(uid,\r\nsock_i_uid((struct sock *)sk2))))) {\r\nif (ipv6_rcv_saddr_equal(sk, sk2))\r\nbreak;\r\n}\r\nif (!relax && reuse && sk2->sk_reuse &&\r\nsk2->sk_state != TCP_LISTEN &&\r\nipv6_rcv_saddr_equal(sk, sk2))\r\nbreak;\r\n}\r\n}\r\nreturn sk2 != NULL;\r\n}\r\nstruct dst_entry *inet6_csk_route_req(struct sock *sk,\r\nstruct flowi6 *fl6,\r\nconst struct request_sock *req)\r\n{\r\nstruct inet_request_sock *ireq = inet_rsk(req);\r\nstruct ipv6_pinfo *np = inet6_sk(sk);\r\nstruct in6_addr *final_p, final;\r\nstruct dst_entry *dst;\r\nmemset(fl6, 0, sizeof(*fl6));\r\nfl6->flowi6_proto = IPPROTO_TCP;\r\nfl6->daddr = ireq->ir_v6_rmt_addr;\r\nfinal_p = fl6_update_dst(fl6, np->opt, &final);\r\nfl6->saddr = ireq->ir_v6_loc_addr;\r\nfl6->flowi6_oif = ireq->ir_iif;\r\nfl6->flowi6_mark = ireq->ir_mark;\r\nfl6->fl6_dport = ireq->ir_rmt_port;\r\nfl6->fl6_sport = htons(ireq->ir_num);\r\nsecurity_req_classify_flow(req, flowi6_to_flowi(fl6));\r\ndst = ip6_dst_lookup_flow(sk, fl6, final_p);\r\nif (IS_ERR(dst))\r\nreturn NULL;\r\nreturn dst;\r\n}\r\nstatic u32 inet6_synq_hash(const struct in6_addr *raddr, const __be16 rport,\r\nconst u32 rnd, const u32 synq_hsize)\r\n{\r\nu32 c;\r\nc = jhash_3words((__force u32)raddr->s6_addr32[0],\r\n(__force u32)raddr->s6_addr32[1],\r\n(__force u32)raddr->s6_addr32[2],\r\nrnd);\r\nc = jhash_2words((__force u32)raddr->s6_addr32[3],\r\n(__force u32)rport,\r\nc);\r\nreturn c & (synq_hsize - 1);\r\n}\r\nstruct request_sock *inet6_csk_search_req(struct sock *sk,\r\nconst __be16 rport,\r\nconst struct in6_addr *raddr,\r\nconst struct in6_addr *laddr,\r\nconst int iif)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct listen_sock *lopt = icsk->icsk_accept_queue.listen_opt;\r\nstruct request_sock *req;\r\nu32 hash = inet6_synq_hash(raddr, rport, lopt->hash_rnd,\r\nlopt->nr_table_entries);\r\nspin_lock(&icsk->icsk_accept_queue.syn_wait_lock);\r\nfor (req = lopt->syn_table[hash]; req != NULL; req = req->dl_next) {\r\nconst struct inet_request_sock *ireq = inet_rsk(req);\r\nif (ireq->ir_rmt_port == rport &&\r\nreq->rsk_ops->family == AF_INET6 &&\r\nipv6_addr_equal(&ireq->ir_v6_rmt_addr, raddr) &&\r\nipv6_addr_equal(&ireq->ir_v6_loc_addr, laddr) &&\r\n(!ireq->ir_iif || ireq->ir_iif == iif)) {\r\natomic_inc(&req->rsk_refcnt);\r\nWARN_ON(req->sk != NULL);\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&icsk->icsk_accept_queue.syn_wait_lock);\r\nreturn req;\r\n}\r\nvoid inet6_csk_reqsk_queue_hash_add(struct sock *sk,\r\nstruct request_sock *req,\r\nconst unsigned long timeout)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct listen_sock *lopt = icsk->icsk_accept_queue.listen_opt;\r\nconst u32 h = inet6_synq_hash(&inet_rsk(req)->ir_v6_rmt_addr,\r\ninet_rsk(req)->ir_rmt_port,\r\nlopt->hash_rnd, lopt->nr_table_entries);\r\nreqsk_queue_hash_req(&icsk->icsk_accept_queue, h, req, timeout);\r\ninet_csk_reqsk_queue_added(sk, timeout);\r\n}\r\nvoid inet6_csk_addr2sockaddr(struct sock *sk, struct sockaddr *uaddr)\r\n{\r\nstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *) uaddr;\r\nsin6->sin6_family = AF_INET6;\r\nsin6->sin6_addr = sk->sk_v6_daddr;\r\nsin6->sin6_port = inet_sk(sk)->inet_dport;\r\nsin6->sin6_flowinfo = 0;\r\nsin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,\r\nsk->sk_bound_dev_if);\r\n}\r\nstatic inline\r\nvoid __inet6_csk_dst_store(struct sock *sk, struct dst_entry *dst,\r\nconst struct in6_addr *daddr,\r\nconst struct in6_addr *saddr)\r\n{\r\n__ip6_dst_store(sk, dst, daddr, saddr);\r\n}\r\nstatic inline\r\nstruct dst_entry *__inet6_csk_dst_check(struct sock *sk, u32 cookie)\r\n{\r\nreturn __sk_dst_check(sk, cookie);\r\n}\r\nstatic struct dst_entry *inet6_csk_route_socket(struct sock *sk,\r\nstruct flowi6 *fl6)\r\n{\r\nstruct inet_sock *inet = inet_sk(sk);\r\nstruct ipv6_pinfo *np = inet6_sk(sk);\r\nstruct in6_addr *final_p, final;\r\nstruct dst_entry *dst;\r\nmemset(fl6, 0, sizeof(*fl6));\r\nfl6->flowi6_proto = sk->sk_protocol;\r\nfl6->daddr = sk->sk_v6_daddr;\r\nfl6->saddr = np->saddr;\r\nfl6->flowlabel = np->flow_label;\r\nIP6_ECN_flow_xmit(sk, fl6->flowlabel);\r\nfl6->flowi6_oif = sk->sk_bound_dev_if;\r\nfl6->flowi6_mark = sk->sk_mark;\r\nfl6->fl6_sport = inet->inet_sport;\r\nfl6->fl6_dport = inet->inet_dport;\r\nsecurity_sk_classify_flow(sk, flowi6_to_flowi(fl6));\r\nfinal_p = fl6_update_dst(fl6, np->opt, &final);\r\ndst = __inet6_csk_dst_check(sk, np->dst_cookie);\r\nif (!dst) {\r\ndst = ip6_dst_lookup_flow(sk, fl6, final_p);\r\nif (!IS_ERR(dst))\r\n__inet6_csk_dst_store(sk, dst, NULL, NULL);\r\n}\r\nreturn dst;\r\n}\r\nint inet6_csk_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl_unused)\r\n{\r\nstruct ipv6_pinfo *np = inet6_sk(sk);\r\nstruct flowi6 fl6;\r\nstruct dst_entry *dst;\r\nint res;\r\ndst = inet6_csk_route_socket(sk, &fl6);\r\nif (IS_ERR(dst)) {\r\nsk->sk_err_soft = -PTR_ERR(dst);\r\nsk->sk_route_caps = 0;\r\nkfree_skb(skb);\r\nreturn PTR_ERR(dst);\r\n}\r\nrcu_read_lock();\r\nskb_dst_set_noref(skb, dst);\r\nfl6.daddr = sk->sk_v6_daddr;\r\nres = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);\r\nrcu_read_unlock();\r\nreturn res;\r\n}\r\nstruct dst_entry *inet6_csk_update_pmtu(struct sock *sk, u32 mtu)\r\n{\r\nstruct flowi6 fl6;\r\nstruct dst_entry *dst = inet6_csk_route_socket(sk, &fl6);\r\nif (IS_ERR(dst))\r\nreturn NULL;\r\ndst->ops->update_pmtu(dst, sk, NULL, mtu);\r\ndst = inet6_csk_route_socket(sk, &fl6);\r\nreturn IS_ERR(dst) ? NULL : dst;\r\n}
