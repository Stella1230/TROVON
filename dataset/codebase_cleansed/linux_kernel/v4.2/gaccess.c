int ipte_lock_held(struct kvm_vcpu *vcpu)\r\n{\r\nunion ipte_control *ic = &vcpu->kvm->arch.sca->ipte_control;\r\nif (vcpu->arch.sie_block->eca & 1)\r\nreturn ic->kh != 0;\r\nreturn vcpu->kvm->arch.ipte_lock_count != 0;\r\n}\r\nstatic void ipte_lock_simple(struct kvm_vcpu *vcpu)\r\n{\r\nunion ipte_control old, new, *ic;\r\nmutex_lock(&vcpu->kvm->arch.ipte_mutex);\r\nvcpu->kvm->arch.ipte_lock_count++;\r\nif (vcpu->kvm->arch.ipte_lock_count > 1)\r\ngoto out;\r\nic = &vcpu->kvm->arch.sca->ipte_control;\r\ndo {\r\nold = READ_ONCE(*ic);\r\nwhile (old.k) {\r\ncond_resched();\r\nold = READ_ONCE(*ic);\r\n}\r\nnew = old;\r\nnew.k = 1;\r\n} while (cmpxchg(&ic->val, old.val, new.val) != old.val);\r\nout:\r\nmutex_unlock(&vcpu->kvm->arch.ipte_mutex);\r\n}\r\nstatic void ipte_unlock_simple(struct kvm_vcpu *vcpu)\r\n{\r\nunion ipte_control old, new, *ic;\r\nmutex_lock(&vcpu->kvm->arch.ipte_mutex);\r\nvcpu->kvm->arch.ipte_lock_count--;\r\nif (vcpu->kvm->arch.ipte_lock_count)\r\ngoto out;\r\nic = &vcpu->kvm->arch.sca->ipte_control;\r\ndo {\r\nold = READ_ONCE(*ic);\r\nnew = old;\r\nnew.k = 0;\r\n} while (cmpxchg(&ic->val, old.val, new.val) != old.val);\r\nwake_up(&vcpu->kvm->arch.ipte_wq);\r\nout:\r\nmutex_unlock(&vcpu->kvm->arch.ipte_mutex);\r\n}\r\nstatic void ipte_lock_siif(struct kvm_vcpu *vcpu)\r\n{\r\nunion ipte_control old, new, *ic;\r\nic = &vcpu->kvm->arch.sca->ipte_control;\r\ndo {\r\nold = READ_ONCE(*ic);\r\nwhile (old.kg) {\r\ncond_resched();\r\nold = READ_ONCE(*ic);\r\n}\r\nnew = old;\r\nnew.k = 1;\r\nnew.kh++;\r\n} while (cmpxchg(&ic->val, old.val, new.val) != old.val);\r\n}\r\nstatic void ipte_unlock_siif(struct kvm_vcpu *vcpu)\r\n{\r\nunion ipte_control old, new, *ic;\r\nic = &vcpu->kvm->arch.sca->ipte_control;\r\ndo {\r\nold = READ_ONCE(*ic);\r\nnew = old;\r\nnew.kh--;\r\nif (!new.kh)\r\nnew.k = 0;\r\n} while (cmpxchg(&ic->val, old.val, new.val) != old.val);\r\nif (!new.kh)\r\nwake_up(&vcpu->kvm->arch.ipte_wq);\r\n}\r\nvoid ipte_lock(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->arch.sie_block->eca & 1)\r\nipte_lock_siif(vcpu);\r\nelse\r\nipte_lock_simple(vcpu);\r\n}\r\nvoid ipte_unlock(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->arch.sie_block->eca & 1)\r\nipte_unlock_siif(vcpu);\r\nelse\r\nipte_unlock_simple(vcpu);\r\n}\r\nstatic int ar_translation(struct kvm_vcpu *vcpu, union asce *asce, ar_t ar,\r\nint write)\r\n{\r\nunion alet alet;\r\nstruct ale ale;\r\nstruct aste aste;\r\nunsigned long ald_addr, authority_table_addr;\r\nunion ald ald;\r\nint eax, rc;\r\nu8 authority_table;\r\nif (ar >= NUM_ACRS)\r\nreturn -EINVAL;\r\nsave_access_regs(vcpu->run->s.regs.acrs);\r\nalet.val = vcpu->run->s.regs.acrs[ar];\r\nif (ar == 0 || alet.val == 0) {\r\nasce->val = vcpu->arch.sie_block->gcr[1];\r\nreturn 0;\r\n} else if (alet.val == 1) {\r\nasce->val = vcpu->arch.sie_block->gcr[7];\r\nreturn 0;\r\n}\r\nif (alet.reserved)\r\nreturn PGM_ALET_SPECIFICATION;\r\nif (alet.p)\r\nald_addr = vcpu->arch.sie_block->gcr[5];\r\nelse\r\nald_addr = vcpu->arch.sie_block->gcr[2];\r\nald_addr &= 0x7fffffc0;\r\nrc = read_guest_real(vcpu, ald_addr + 16, &ald.val, sizeof(union ald));\r\nif (rc)\r\nreturn rc;\r\nif (alet.alen / 8 > ald.all)\r\nreturn PGM_ALEN_TRANSLATION;\r\nif (0x7fffffff - ald.alo * 128 < alet.alen * 16)\r\nreturn PGM_ADDRESSING;\r\nrc = read_guest_real(vcpu, ald.alo * 128 + alet.alen * 16, &ale,\r\nsizeof(struct ale));\r\nif (rc)\r\nreturn rc;\r\nif (ale.i == 1)\r\nreturn PGM_ALEN_TRANSLATION;\r\nif (ale.alesn != alet.alesn)\r\nreturn PGM_ALE_SEQUENCE;\r\nrc = read_guest_real(vcpu, ale.asteo * 64, &aste, sizeof(struct aste));\r\nif (rc)\r\nreturn rc;\r\nif (aste.i)\r\nreturn PGM_ASTE_VALIDITY;\r\nif (aste.astesn != ale.astesn)\r\nreturn PGM_ASTE_SEQUENCE;\r\nif (ale.p == 1) {\r\neax = (vcpu->arch.sie_block->gcr[8] >> 16) & 0xffff;\r\nif (ale.aleax != eax) {\r\nif (eax / 16 > aste.atl)\r\nreturn PGM_EXTENDED_AUTHORITY;\r\nauthority_table_addr = aste.ato * 4 + eax / 4;\r\nrc = read_guest_real(vcpu, authority_table_addr,\r\n&authority_table,\r\nsizeof(u8));\r\nif (rc)\r\nreturn rc;\r\nif ((authority_table & (0x40 >> ((eax & 3) * 2))) == 0)\r\nreturn PGM_EXTENDED_AUTHORITY;\r\n}\r\n}\r\nif (ale.fo == 1 && write)\r\nreturn PGM_PROTECTION;\r\nasce->val = aste.asce;\r\nreturn 0;\r\n}\r\nstatic int get_vcpu_asce(struct kvm_vcpu *vcpu, union asce *asce,\r\nar_t ar, int write)\r\n{\r\nint rc;\r\npsw_t *psw = &vcpu->arch.sie_block->gpsw;\r\nstruct kvm_s390_pgm_info *pgm = &vcpu->arch.pgm;\r\nstruct trans_exc_code_bits *tec_bits;\r\nmemset(pgm, 0, sizeof(*pgm));\r\ntec_bits = (struct trans_exc_code_bits *)&pgm->trans_exc_code;\r\ntec_bits->fsi = write ? FSI_STORE : FSI_FETCH;\r\ntec_bits->as = psw_bits(*psw).as;\r\nif (!psw_bits(*psw).t) {\r\nasce->val = 0;\r\nasce->r = 1;\r\nreturn 0;\r\n}\r\nswitch (psw_bits(vcpu->arch.sie_block->gpsw).as) {\r\ncase PSW_AS_PRIMARY:\r\nasce->val = vcpu->arch.sie_block->gcr[1];\r\nreturn 0;\r\ncase PSW_AS_SECONDARY:\r\nasce->val = vcpu->arch.sie_block->gcr[7];\r\nreturn 0;\r\ncase PSW_AS_HOME:\r\nasce->val = vcpu->arch.sie_block->gcr[13];\r\nreturn 0;\r\ncase PSW_AS_ACCREG:\r\nrc = ar_translation(vcpu, asce, ar, write);\r\nswitch (rc) {\r\ncase PGM_ALEN_TRANSLATION:\r\ncase PGM_ALE_SEQUENCE:\r\ncase PGM_ASTE_VALIDITY:\r\ncase PGM_ASTE_SEQUENCE:\r\ncase PGM_EXTENDED_AUTHORITY:\r\nvcpu->arch.pgm.exc_access_id = ar;\r\nbreak;\r\ncase PGM_PROTECTION:\r\ntec_bits->b60 = 1;\r\ntec_bits->b61 = 1;\r\nbreak;\r\n}\r\nif (rc > 0)\r\npgm->code = rc;\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nstatic int deref_table(struct kvm *kvm, unsigned long gpa, unsigned long *val)\r\n{\r\nreturn kvm_read_guest(kvm, gpa, val, sizeof(*val));\r\n}\r\nstatic unsigned long guest_translate(struct kvm_vcpu *vcpu, unsigned long gva,\r\nunsigned long *gpa, const union asce asce,\r\nint write)\r\n{\r\nunion vaddress vaddr = {.addr = gva};\r\nunion raddress raddr = {.addr = gva};\r\nunion page_table_entry pte;\r\nint dat_protection = 0;\r\nunion ctlreg0 ctlreg0;\r\nunsigned long ptr;\r\nint edat1, edat2;\r\nctlreg0.val = vcpu->arch.sie_block->gcr[0];\r\nedat1 = ctlreg0.edat && test_kvm_facility(vcpu->kvm, 8);\r\nedat2 = edat1 && test_kvm_facility(vcpu->kvm, 78);\r\nif (asce.r)\r\ngoto real_address;\r\nptr = asce.origin * 4096;\r\nswitch (asce.dt) {\r\ncase ASCE_TYPE_REGION1:\r\nif (vaddr.rfx01 > asce.tl)\r\nreturn PGM_REGION_FIRST_TRANS;\r\nptr += vaddr.rfx * 8;\r\nbreak;\r\ncase ASCE_TYPE_REGION2:\r\nif (vaddr.rfx)\r\nreturn PGM_ASCE_TYPE;\r\nif (vaddr.rsx01 > asce.tl)\r\nreturn PGM_REGION_SECOND_TRANS;\r\nptr += vaddr.rsx * 8;\r\nbreak;\r\ncase ASCE_TYPE_REGION3:\r\nif (vaddr.rfx || vaddr.rsx)\r\nreturn PGM_ASCE_TYPE;\r\nif (vaddr.rtx01 > asce.tl)\r\nreturn PGM_REGION_THIRD_TRANS;\r\nptr += vaddr.rtx * 8;\r\nbreak;\r\ncase ASCE_TYPE_SEGMENT:\r\nif (vaddr.rfx || vaddr.rsx || vaddr.rtx)\r\nreturn PGM_ASCE_TYPE;\r\nif (vaddr.sx01 > asce.tl)\r\nreturn PGM_SEGMENT_TRANSLATION;\r\nptr += vaddr.sx * 8;\r\nbreak;\r\n}\r\nswitch (asce.dt) {\r\ncase ASCE_TYPE_REGION1: {\r\nunion region1_table_entry rfte;\r\nif (kvm_is_error_gpa(vcpu->kvm, ptr))\r\nreturn PGM_ADDRESSING;\r\nif (deref_table(vcpu->kvm, ptr, &rfte.val))\r\nreturn -EFAULT;\r\nif (rfte.i)\r\nreturn PGM_REGION_FIRST_TRANS;\r\nif (rfte.tt != TABLE_TYPE_REGION1)\r\nreturn PGM_TRANSLATION_SPEC;\r\nif (vaddr.rsx01 < rfte.tf || vaddr.rsx01 > rfte.tl)\r\nreturn PGM_REGION_SECOND_TRANS;\r\nif (edat1)\r\ndat_protection |= rfte.p;\r\nptr = rfte.rto * 4096 + vaddr.rsx * 8;\r\n}\r\ncase ASCE_TYPE_REGION2: {\r\nunion region2_table_entry rste;\r\nif (kvm_is_error_gpa(vcpu->kvm, ptr))\r\nreturn PGM_ADDRESSING;\r\nif (deref_table(vcpu->kvm, ptr, &rste.val))\r\nreturn -EFAULT;\r\nif (rste.i)\r\nreturn PGM_REGION_SECOND_TRANS;\r\nif (rste.tt != TABLE_TYPE_REGION2)\r\nreturn PGM_TRANSLATION_SPEC;\r\nif (vaddr.rtx01 < rste.tf || vaddr.rtx01 > rste.tl)\r\nreturn PGM_REGION_THIRD_TRANS;\r\nif (edat1)\r\ndat_protection |= rste.p;\r\nptr = rste.rto * 4096 + vaddr.rtx * 8;\r\n}\r\ncase ASCE_TYPE_REGION3: {\r\nunion region3_table_entry rtte;\r\nif (kvm_is_error_gpa(vcpu->kvm, ptr))\r\nreturn PGM_ADDRESSING;\r\nif (deref_table(vcpu->kvm, ptr, &rtte.val))\r\nreturn -EFAULT;\r\nif (rtte.i)\r\nreturn PGM_REGION_THIRD_TRANS;\r\nif (rtte.tt != TABLE_TYPE_REGION3)\r\nreturn PGM_TRANSLATION_SPEC;\r\nif (rtte.cr && asce.p && edat2)\r\nreturn PGM_TRANSLATION_SPEC;\r\nif (rtte.fc && edat2) {\r\ndat_protection |= rtte.fc1.p;\r\nraddr.rfaa = rtte.fc1.rfaa;\r\ngoto absolute_address;\r\n}\r\nif (vaddr.sx01 < rtte.fc0.tf)\r\nreturn PGM_SEGMENT_TRANSLATION;\r\nif (vaddr.sx01 > rtte.fc0.tl)\r\nreturn PGM_SEGMENT_TRANSLATION;\r\nif (edat1)\r\ndat_protection |= rtte.fc0.p;\r\nptr = rtte.fc0.sto * 4096 + vaddr.sx * 8;\r\n}\r\ncase ASCE_TYPE_SEGMENT: {\r\nunion segment_table_entry ste;\r\nif (kvm_is_error_gpa(vcpu->kvm, ptr))\r\nreturn PGM_ADDRESSING;\r\nif (deref_table(vcpu->kvm, ptr, &ste.val))\r\nreturn -EFAULT;\r\nif (ste.i)\r\nreturn PGM_SEGMENT_TRANSLATION;\r\nif (ste.tt != TABLE_TYPE_SEGMENT)\r\nreturn PGM_TRANSLATION_SPEC;\r\nif (ste.cs && asce.p)\r\nreturn PGM_TRANSLATION_SPEC;\r\nif (ste.fc && edat1) {\r\ndat_protection |= ste.fc1.p;\r\nraddr.sfaa = ste.fc1.sfaa;\r\ngoto absolute_address;\r\n}\r\ndat_protection |= ste.fc0.p;\r\nptr = ste.fc0.pto * 2048 + vaddr.px * 8;\r\n}\r\n}\r\nif (kvm_is_error_gpa(vcpu->kvm, ptr))\r\nreturn PGM_ADDRESSING;\r\nif (deref_table(vcpu->kvm, ptr, &pte.val))\r\nreturn -EFAULT;\r\nif (pte.i)\r\nreturn PGM_PAGE_TRANSLATION;\r\nif (pte.z)\r\nreturn PGM_TRANSLATION_SPEC;\r\nif (pte.co && !edat1)\r\nreturn PGM_TRANSLATION_SPEC;\r\ndat_protection |= pte.p;\r\nraddr.pfra = pte.pfra;\r\nreal_address:\r\nraddr.addr = kvm_s390_real_to_abs(vcpu, raddr.addr);\r\nabsolute_address:\r\nif (write && dat_protection)\r\nreturn PGM_PROTECTION;\r\nif (kvm_is_error_gpa(vcpu->kvm, raddr.addr))\r\nreturn PGM_ADDRESSING;\r\n*gpa = raddr.addr;\r\nreturn 0;\r\n}\r\nstatic inline int is_low_address(unsigned long ga)\r\n{\r\nreturn (ga & ~0x11fful) == 0;\r\n}\r\nstatic int low_address_protection_enabled(struct kvm_vcpu *vcpu,\r\nconst union asce asce)\r\n{\r\nunion ctlreg0 ctlreg0 = {.val = vcpu->arch.sie_block->gcr[0]};\r\npsw_t *psw = &vcpu->arch.sie_block->gpsw;\r\nif (!ctlreg0.lap)\r\nreturn 0;\r\nif (psw_bits(*psw).t && asce.p)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int guest_page_range(struct kvm_vcpu *vcpu, unsigned long ga,\r\nunsigned long *pages, unsigned long nr_pages,\r\nconst union asce asce, int write)\r\n{\r\nstruct kvm_s390_pgm_info *pgm = &vcpu->arch.pgm;\r\npsw_t *psw = &vcpu->arch.sie_block->gpsw;\r\nstruct trans_exc_code_bits *tec_bits;\r\nint lap_enabled, rc;\r\ntec_bits = (struct trans_exc_code_bits *)&pgm->trans_exc_code;\r\nlap_enabled = low_address_protection_enabled(vcpu, asce);\r\nwhile (nr_pages) {\r\nga = kvm_s390_logical_to_effective(vcpu, ga);\r\ntec_bits->addr = ga >> PAGE_SHIFT;\r\nif (write && lap_enabled && is_low_address(ga)) {\r\npgm->code = PGM_PROTECTION;\r\nreturn pgm->code;\r\n}\r\nga &= PAGE_MASK;\r\nif (psw_bits(*psw).t) {\r\nrc = guest_translate(vcpu, ga, pages, asce, write);\r\nif (rc < 0)\r\nreturn rc;\r\nif (rc == PGM_PROTECTION)\r\ntec_bits->b61 = 1;\r\nif (rc)\r\npgm->code = rc;\r\n} else {\r\n*pages = kvm_s390_real_to_abs(vcpu, ga);\r\nif (kvm_is_error_gpa(vcpu->kvm, *pages))\r\npgm->code = PGM_ADDRESSING;\r\n}\r\nif (pgm->code)\r\nreturn pgm->code;\r\nga += PAGE_SIZE;\r\npages++;\r\nnr_pages--;\r\n}\r\nreturn 0;\r\n}\r\nint access_guest(struct kvm_vcpu *vcpu, unsigned long ga, ar_t ar, void *data,\r\nunsigned long len, int write)\r\n{\r\npsw_t *psw = &vcpu->arch.sie_block->gpsw;\r\nunsigned long _len, nr_pages, gpa, idx;\r\nunsigned long pages_array[2];\r\nunsigned long *pages;\r\nint need_ipte_lock;\r\nunion asce asce;\r\nint rc;\r\nif (!len)\r\nreturn 0;\r\nrc = get_vcpu_asce(vcpu, &asce, ar, write);\r\nif (rc)\r\nreturn rc;\r\nnr_pages = (((ga & ~PAGE_MASK) + len - 1) >> PAGE_SHIFT) + 1;\r\npages = pages_array;\r\nif (nr_pages > ARRAY_SIZE(pages_array))\r\npages = vmalloc(nr_pages * sizeof(unsigned long));\r\nif (!pages)\r\nreturn -ENOMEM;\r\nneed_ipte_lock = psw_bits(*psw).t && !asce.r;\r\nif (need_ipte_lock)\r\nipte_lock(vcpu);\r\nrc = guest_page_range(vcpu, ga, pages, nr_pages, asce, write);\r\nfor (idx = 0; idx < nr_pages && !rc; idx++) {\r\ngpa = *(pages + idx) + (ga & ~PAGE_MASK);\r\n_len = min(PAGE_SIZE - (gpa & ~PAGE_MASK), len);\r\nif (write)\r\nrc = kvm_write_guest(vcpu->kvm, gpa, data, _len);\r\nelse\r\nrc = kvm_read_guest(vcpu->kvm, gpa, data, _len);\r\nlen -= _len;\r\nga += _len;\r\ndata += _len;\r\n}\r\nif (need_ipte_lock)\r\nipte_unlock(vcpu);\r\nif (nr_pages > ARRAY_SIZE(pages_array))\r\nvfree(pages);\r\nreturn rc;\r\n}\r\nint access_guest_real(struct kvm_vcpu *vcpu, unsigned long gra,\r\nvoid *data, unsigned long len, int write)\r\n{\r\nunsigned long _len, gpa;\r\nint rc = 0;\r\nwhile (len && !rc) {\r\ngpa = kvm_s390_real_to_abs(vcpu, gra);\r\n_len = min(PAGE_SIZE - (gpa & ~PAGE_MASK), len);\r\nif (write)\r\nrc = write_guest_abs(vcpu, gpa, data, _len);\r\nelse\r\nrc = read_guest_abs(vcpu, gpa, data, _len);\r\nlen -= _len;\r\ngra += _len;\r\ndata += _len;\r\n}\r\nreturn rc;\r\n}\r\nint guest_translate_address(struct kvm_vcpu *vcpu, unsigned long gva, ar_t ar,\r\nunsigned long *gpa, int write)\r\n{\r\nstruct kvm_s390_pgm_info *pgm = &vcpu->arch.pgm;\r\npsw_t *psw = &vcpu->arch.sie_block->gpsw;\r\nstruct trans_exc_code_bits *tec;\r\nunion asce asce;\r\nint rc;\r\ngva = kvm_s390_logical_to_effective(vcpu, gva);\r\ntec = (struct trans_exc_code_bits *)&pgm->trans_exc_code;\r\nrc = get_vcpu_asce(vcpu, &asce, ar, write);\r\ntec->addr = gva >> PAGE_SHIFT;\r\nif (rc)\r\nreturn rc;\r\nif (is_low_address(gva) && low_address_protection_enabled(vcpu, asce)) {\r\nif (write) {\r\nrc = pgm->code = PGM_PROTECTION;\r\nreturn rc;\r\n}\r\n}\r\nif (psw_bits(*psw).t && !asce.r) {\r\nrc = guest_translate(vcpu, gva, gpa, asce, write);\r\nif (rc > 0) {\r\nif (rc == PGM_PROTECTION)\r\ntec->b61 = 1;\r\npgm->code = rc;\r\n}\r\n} else {\r\nrc = 0;\r\n*gpa = kvm_s390_real_to_abs(vcpu, gva);\r\nif (kvm_is_error_gpa(vcpu->kvm, *gpa))\r\nrc = pgm->code = PGM_ADDRESSING;\r\n}\r\nreturn rc;\r\n}\r\nint check_gva_range(struct kvm_vcpu *vcpu, unsigned long gva, ar_t ar,\r\nunsigned long length, int is_write)\r\n{\r\nunsigned long gpa;\r\nunsigned long currlen;\r\nint rc = 0;\r\nipte_lock(vcpu);\r\nwhile (length > 0 && !rc) {\r\ncurrlen = min(length, PAGE_SIZE - (gva % PAGE_SIZE));\r\nrc = guest_translate_address(vcpu, gva, ar, &gpa, is_write);\r\ngva += currlen;\r\nlength -= currlen;\r\n}\r\nipte_unlock(vcpu);\r\nreturn rc;\r\n}\r\nint kvm_s390_check_low_addr_prot_real(struct kvm_vcpu *vcpu, unsigned long gra)\r\n{\r\nstruct kvm_s390_pgm_info *pgm = &vcpu->arch.pgm;\r\npsw_t *psw = &vcpu->arch.sie_block->gpsw;\r\nstruct trans_exc_code_bits *tec_bits;\r\nunion ctlreg0 ctlreg0 = {.val = vcpu->arch.sie_block->gcr[0]};\r\nif (!ctlreg0.lap || !is_low_address(gra))\r\nreturn 0;\r\nmemset(pgm, 0, sizeof(*pgm));\r\ntec_bits = (struct trans_exc_code_bits *)&pgm->trans_exc_code;\r\ntec_bits->fsi = FSI_STORE;\r\ntec_bits->as = psw_bits(*psw).as;\r\ntec_bits->addr = gra >> PAGE_SHIFT;\r\npgm->code = PGM_PROTECTION;\r\nreturn pgm->code;\r\n}
