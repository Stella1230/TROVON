static int idr_max(int layers)\r\n{\r\nint bits = min_t(int, layers * IDR_BITS, MAX_IDR_SHIFT);\r\nreturn (1 << bits) - 1;\r\n}\r\nstatic int idr_layer_prefix_mask(int layer)\r\n{\r\nreturn ~idr_max(layer + 1);\r\n}\r\nstatic struct idr_layer *get_from_free_list(struct idr *idp)\r\n{\r\nstruct idr_layer *p;\r\nunsigned long flags;\r\nspin_lock_irqsave(&idp->lock, flags);\r\nif ((p = idp->id_free)) {\r\nidp->id_free = p->ary[0];\r\nidp->id_free_cnt--;\r\np->ary[0] = NULL;\r\n}\r\nspin_unlock_irqrestore(&idp->lock, flags);\r\nreturn(p);\r\n}\r\nstatic struct idr_layer *idr_layer_alloc(gfp_t gfp_mask, struct idr *layer_idr)\r\n{\r\nstruct idr_layer *new;\r\nif (layer_idr)\r\nreturn get_from_free_list(layer_idr);\r\nnew = kmem_cache_zalloc(idr_layer_cache, gfp_mask | __GFP_NOWARN);\r\nif (new)\r\nreturn new;\r\nif (!in_interrupt()) {\r\npreempt_disable();\r\nnew = __this_cpu_read(idr_preload_head);\r\nif (new) {\r\n__this_cpu_write(idr_preload_head, new->ary[0]);\r\n__this_cpu_dec(idr_preload_cnt);\r\nnew->ary[0] = NULL;\r\n}\r\npreempt_enable();\r\nif (new)\r\nreturn new;\r\n}\r\nreturn kmem_cache_zalloc(idr_layer_cache, gfp_mask);\r\n}\r\nstatic void idr_layer_rcu_free(struct rcu_head *head)\r\n{\r\nstruct idr_layer *layer;\r\nlayer = container_of(head, struct idr_layer, rcu_head);\r\nkmem_cache_free(idr_layer_cache, layer);\r\n}\r\nstatic inline void free_layer(struct idr *idr, struct idr_layer *p)\r\n{\r\nif (idr->hint == p)\r\nRCU_INIT_POINTER(idr->hint, NULL);\r\ncall_rcu(&p->rcu_head, idr_layer_rcu_free);\r\n}\r\nstatic void __move_to_free_list(struct idr *idp, struct idr_layer *p)\r\n{\r\np->ary[0] = idp->id_free;\r\nidp->id_free = p;\r\nidp->id_free_cnt++;\r\n}\r\nstatic void move_to_free_list(struct idr *idp, struct idr_layer *p)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&idp->lock, flags);\r\n__move_to_free_list(idp, p);\r\nspin_unlock_irqrestore(&idp->lock, flags);\r\n}\r\nstatic void idr_mark_full(struct idr_layer **pa, int id)\r\n{\r\nstruct idr_layer *p = pa[0];\r\nint l = 0;\r\n__set_bit(id & IDR_MASK, p->bitmap);\r\nwhile (bitmap_full(p->bitmap, IDR_SIZE)) {\r\nif (!(p = pa[++l]))\r\nbreak;\r\nid = id >> IDR_BITS;\r\n__set_bit((id & IDR_MASK), p->bitmap);\r\n}\r\n}\r\nstatic int __idr_pre_get(struct idr *idp, gfp_t gfp_mask)\r\n{\r\nwhile (idp->id_free_cnt < MAX_IDR_FREE) {\r\nstruct idr_layer *new;\r\nnew = kmem_cache_zalloc(idr_layer_cache, gfp_mask);\r\nif (new == NULL)\r\nreturn (0);\r\nmove_to_free_list(idp, new);\r\n}\r\nreturn 1;\r\n}\r\nstatic int sub_alloc(struct idr *idp, int *starting_id, struct idr_layer **pa,\r\ngfp_t gfp_mask, struct idr *layer_idr)\r\n{\r\nint n, m, sh;\r\nstruct idr_layer *p, *new;\r\nint l, id, oid;\r\nid = *starting_id;\r\nrestart:\r\np = idp->top;\r\nl = idp->layers;\r\npa[l--] = NULL;\r\nwhile (1) {\r\nn = (id >> (IDR_BITS*l)) & IDR_MASK;\r\nm = find_next_zero_bit(p->bitmap, IDR_SIZE, n);\r\nif (m == IDR_SIZE) {\r\nl++;\r\noid = id;\r\nid = (id | ((1 << (IDR_BITS * l)) - 1)) + 1;\r\nif (id > idr_max(idp->layers)) {\r\n*starting_id = id;\r\nreturn -EAGAIN;\r\n}\r\np = pa[l];\r\nBUG_ON(!p);\r\nsh = IDR_BITS * (l + 1);\r\nif (oid >> sh == id >> sh)\r\ncontinue;\r\nelse\r\ngoto restart;\r\n}\r\nif (m != n) {\r\nsh = IDR_BITS*l;\r\nid = ((id >> sh) ^ n ^ m) << sh;\r\n}\r\nif ((id >= MAX_IDR_BIT) || (id < 0))\r\nreturn -ENOSPC;\r\nif (l == 0)\r\nbreak;\r\nif (!p->ary[m]) {\r\nnew = idr_layer_alloc(gfp_mask, layer_idr);\r\nif (!new)\r\nreturn -ENOMEM;\r\nnew->layer = l-1;\r\nnew->prefix = id & idr_layer_prefix_mask(new->layer);\r\nrcu_assign_pointer(p->ary[m], new);\r\np->count++;\r\n}\r\npa[l--] = p;\r\np = p->ary[m];\r\n}\r\npa[l] = p;\r\nreturn id;\r\n}\r\nstatic int idr_get_empty_slot(struct idr *idp, int starting_id,\r\nstruct idr_layer **pa, gfp_t gfp_mask,\r\nstruct idr *layer_idr)\r\n{\r\nstruct idr_layer *p, *new;\r\nint layers, v, id;\r\nunsigned long flags;\r\nid = starting_id;\r\nbuild_up:\r\np = idp->top;\r\nlayers = idp->layers;\r\nif (unlikely(!p)) {\r\nif (!(p = idr_layer_alloc(gfp_mask, layer_idr)))\r\nreturn -ENOMEM;\r\np->layer = 0;\r\nlayers = 1;\r\n}\r\nwhile (id > idr_max(layers)) {\r\nlayers++;\r\nif (!p->count) {\r\np->layer++;\r\nWARN_ON_ONCE(p->prefix);\r\ncontinue;\r\n}\r\nif (!(new = idr_layer_alloc(gfp_mask, layer_idr))) {\r\nspin_lock_irqsave(&idp->lock, flags);\r\nfor (new = p; p && p != idp->top; new = p) {\r\np = p->ary[0];\r\nnew->ary[0] = NULL;\r\nnew->count = 0;\r\nbitmap_clear(new->bitmap, 0, IDR_SIZE);\r\n__move_to_free_list(idp, new);\r\n}\r\nspin_unlock_irqrestore(&idp->lock, flags);\r\nreturn -ENOMEM;\r\n}\r\nnew->ary[0] = p;\r\nnew->count = 1;\r\nnew->layer = layers-1;\r\nnew->prefix = id & idr_layer_prefix_mask(new->layer);\r\nif (bitmap_full(p->bitmap, IDR_SIZE))\r\n__set_bit(0, new->bitmap);\r\np = new;\r\n}\r\nrcu_assign_pointer(idp->top, p);\r\nidp->layers = layers;\r\nv = sub_alloc(idp, &id, pa, gfp_mask, layer_idr);\r\nif (v == -EAGAIN)\r\ngoto build_up;\r\nreturn(v);\r\n}\r\nstatic void idr_fill_slot(struct idr *idr, void *ptr, int id,\r\nstruct idr_layer **pa)\r\n{\r\nrcu_assign_pointer(idr->hint, pa[0]);\r\nrcu_assign_pointer(pa[0]->ary[id & IDR_MASK], (struct idr_layer *)ptr);\r\npa[0]->count++;\r\nidr_mark_full(pa, id);\r\n}\r\nvoid idr_preload(gfp_t gfp_mask)\r\n{\r\nWARN_ON_ONCE(in_interrupt());\r\nmight_sleep_if(gfp_mask & __GFP_WAIT);\r\npreempt_disable();\r\nwhile (__this_cpu_read(idr_preload_cnt) < MAX_IDR_FREE) {\r\nstruct idr_layer *new;\r\npreempt_enable();\r\nnew = kmem_cache_zalloc(idr_layer_cache, gfp_mask);\r\npreempt_disable();\r\nif (!new)\r\nbreak;\r\nnew->ary[0] = __this_cpu_read(idr_preload_head);\r\n__this_cpu_write(idr_preload_head, new);\r\n__this_cpu_inc(idr_preload_cnt);\r\n}\r\n}\r\nint idr_alloc(struct idr *idr, void *ptr, int start, int end, gfp_t gfp_mask)\r\n{\r\nint max = end > 0 ? end - 1 : INT_MAX;\r\nstruct idr_layer *pa[MAX_IDR_LEVEL + 1];\r\nint id;\r\nmight_sleep_if(gfp_mask & __GFP_WAIT);\r\nif (WARN_ON_ONCE(start < 0))\r\nreturn -EINVAL;\r\nif (unlikely(max < start))\r\nreturn -ENOSPC;\r\nid = idr_get_empty_slot(idr, start, pa, gfp_mask, NULL);\r\nif (unlikely(id < 0))\r\nreturn id;\r\nif (unlikely(id > max))\r\nreturn -ENOSPC;\r\nidr_fill_slot(idr, ptr, id, pa);\r\nreturn id;\r\n}\r\nint idr_alloc_cyclic(struct idr *idr, void *ptr, int start, int end,\r\ngfp_t gfp_mask)\r\n{\r\nint id;\r\nid = idr_alloc(idr, ptr, max(start, idr->cur), end, gfp_mask);\r\nif (id == -ENOSPC)\r\nid = idr_alloc(idr, ptr, start, end, gfp_mask);\r\nif (likely(id >= 0))\r\nidr->cur = id + 1;\r\nreturn id;\r\n}\r\nstatic void idr_remove_warning(int id)\r\n{\r\nWARN(1, "idr_remove called for id=%d which is not allocated.\n", id);\r\n}\r\nstatic void sub_remove(struct idr *idp, int shift, int id)\r\n{\r\nstruct idr_layer *p = idp->top;\r\nstruct idr_layer **pa[MAX_IDR_LEVEL + 1];\r\nstruct idr_layer ***paa = &pa[0];\r\nstruct idr_layer *to_free;\r\nint n;\r\n*paa = NULL;\r\n*++paa = &idp->top;\r\nwhile ((shift > 0) && p) {\r\nn = (id >> shift) & IDR_MASK;\r\n__clear_bit(n, p->bitmap);\r\n*++paa = &p->ary[n];\r\np = p->ary[n];\r\nshift -= IDR_BITS;\r\n}\r\nn = id & IDR_MASK;\r\nif (likely(p != NULL && test_bit(n, p->bitmap))) {\r\n__clear_bit(n, p->bitmap);\r\nRCU_INIT_POINTER(p->ary[n], NULL);\r\nto_free = NULL;\r\nwhile(*paa && ! --((**paa)->count)){\r\nif (to_free)\r\nfree_layer(idp, to_free);\r\nto_free = **paa;\r\n**paa-- = NULL;\r\n}\r\nif (!*paa)\r\nidp->layers = 0;\r\nif (to_free)\r\nfree_layer(idp, to_free);\r\n} else\r\nidr_remove_warning(id);\r\n}\r\nvoid idr_remove(struct idr *idp, int id)\r\n{\r\nstruct idr_layer *p;\r\nstruct idr_layer *to_free;\r\nif (id < 0)\r\nreturn;\r\nif (id > idr_max(idp->layers)) {\r\nidr_remove_warning(id);\r\nreturn;\r\n}\r\nsub_remove(idp, (idp->layers - 1) * IDR_BITS, id);\r\nif (idp->top && idp->top->count == 1 && (idp->layers > 1) &&\r\nidp->top->ary[0]) {\r\nto_free = idp->top;\r\np = idp->top->ary[0];\r\nrcu_assign_pointer(idp->top, p);\r\n--idp->layers;\r\nto_free->count = 0;\r\nbitmap_clear(to_free->bitmap, 0, IDR_SIZE);\r\nfree_layer(idp, to_free);\r\n}\r\n}\r\nstatic void __idr_remove_all(struct idr *idp)\r\n{\r\nint n, id, max;\r\nint bt_mask;\r\nstruct idr_layer *p;\r\nstruct idr_layer *pa[MAX_IDR_LEVEL + 1];\r\nstruct idr_layer **paa = &pa[0];\r\nn = idp->layers * IDR_BITS;\r\n*paa = idp->top;\r\nRCU_INIT_POINTER(idp->top, NULL);\r\nmax = idr_max(idp->layers);\r\nid = 0;\r\nwhile (id >= 0 && id <= max) {\r\np = *paa;\r\nwhile (n > IDR_BITS && p) {\r\nn -= IDR_BITS;\r\np = p->ary[(id >> n) & IDR_MASK];\r\n*++paa = p;\r\n}\r\nbt_mask = id;\r\nid += 1 << n;\r\nwhile (n < fls(id ^ bt_mask)) {\r\nif (*paa)\r\nfree_layer(idp, *paa);\r\nn += IDR_BITS;\r\n--paa;\r\n}\r\n}\r\nidp->layers = 0;\r\n}\r\nvoid idr_destroy(struct idr *idp)\r\n{\r\n__idr_remove_all(idp);\r\nwhile (idp->id_free_cnt) {\r\nstruct idr_layer *p = get_from_free_list(idp);\r\nkmem_cache_free(idr_layer_cache, p);\r\n}\r\n}\r\nvoid *idr_find_slowpath(struct idr *idp, int id)\r\n{\r\nint n;\r\nstruct idr_layer *p;\r\nif (id < 0)\r\nreturn NULL;\r\np = rcu_dereference_raw(idp->top);\r\nif (!p)\r\nreturn NULL;\r\nn = (p->layer+1) * IDR_BITS;\r\nif (id > idr_max(p->layer + 1))\r\nreturn NULL;\r\nBUG_ON(n == 0);\r\nwhile (n > 0 && p) {\r\nn -= IDR_BITS;\r\nBUG_ON(n != p->layer*IDR_BITS);\r\np = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);\r\n}\r\nreturn((void *)p);\r\n}\r\nint idr_for_each(struct idr *idp,\r\nint (*fn)(int id, void *p, void *data), void *data)\r\n{\r\nint n, id, max, error = 0;\r\nstruct idr_layer *p;\r\nstruct idr_layer *pa[MAX_IDR_LEVEL + 1];\r\nstruct idr_layer **paa = &pa[0];\r\nn = idp->layers * IDR_BITS;\r\n*paa = rcu_dereference_raw(idp->top);\r\nmax = idr_max(idp->layers);\r\nid = 0;\r\nwhile (id >= 0 && id <= max) {\r\np = *paa;\r\nwhile (n > 0 && p) {\r\nn -= IDR_BITS;\r\np = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);\r\n*++paa = p;\r\n}\r\nif (p) {\r\nerror = fn(id, (void *)p, data);\r\nif (error)\r\nbreak;\r\n}\r\nid += 1 << n;\r\nwhile (n < fls(id)) {\r\nn += IDR_BITS;\r\n--paa;\r\n}\r\n}\r\nreturn error;\r\n}\r\nvoid *idr_get_next(struct idr *idp, int *nextidp)\r\n{\r\nstruct idr_layer *p, *pa[MAX_IDR_LEVEL + 1];\r\nstruct idr_layer **paa = &pa[0];\r\nint id = *nextidp;\r\nint n, max;\r\np = *paa = rcu_dereference_raw(idp->top);\r\nif (!p)\r\nreturn NULL;\r\nn = (p->layer + 1) * IDR_BITS;\r\nmax = idr_max(p->layer + 1);\r\nwhile (id >= 0 && id <= max) {\r\np = *paa;\r\nwhile (n > 0 && p) {\r\nn -= IDR_BITS;\r\np = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);\r\n*++paa = p;\r\n}\r\nif (p) {\r\n*nextidp = id;\r\nreturn p;\r\n}\r\nid = round_up(id + 1, 1 << n);\r\nwhile (n < fls(id)) {\r\nn += IDR_BITS;\r\n--paa;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nvoid *idr_replace(struct idr *idp, void *ptr, int id)\r\n{\r\nint n;\r\nstruct idr_layer *p, *old_p;\r\nif (id < 0)\r\nreturn ERR_PTR(-EINVAL);\r\np = idp->top;\r\nif (!p)\r\nreturn ERR_PTR(-ENOENT);\r\nif (id > idr_max(p->layer + 1))\r\nreturn ERR_PTR(-ENOENT);\r\nn = p->layer * IDR_BITS;\r\nwhile ((n > 0) && p) {\r\np = p->ary[(id >> n) & IDR_MASK];\r\nn -= IDR_BITS;\r\n}\r\nn = id & IDR_MASK;\r\nif (unlikely(p == NULL || !test_bit(n, p->bitmap)))\r\nreturn ERR_PTR(-ENOENT);\r\nold_p = p->ary[n];\r\nrcu_assign_pointer(p->ary[n], ptr);\r\nreturn old_p;\r\n}\r\nvoid __init idr_init_cache(void)\r\n{\r\nidr_layer_cache = kmem_cache_create("idr_layer_cache",\r\nsizeof(struct idr_layer), 0, SLAB_PANIC, NULL);\r\n}\r\nvoid idr_init(struct idr *idp)\r\n{\r\nmemset(idp, 0, sizeof(struct idr));\r\nspin_lock_init(&idp->lock);\r\n}\r\nstatic int idr_has_entry(int id, void *p, void *data)\r\n{\r\nreturn 1;\r\n}\r\nbool idr_is_empty(struct idr *idp)\r\n{\r\nreturn !idr_for_each(idp, idr_has_entry, NULL);\r\n}\r\nstatic void free_bitmap(struct ida *ida, struct ida_bitmap *bitmap)\r\n{\r\nunsigned long flags;\r\nif (!ida->free_bitmap) {\r\nspin_lock_irqsave(&ida->idr.lock, flags);\r\nif (!ida->free_bitmap) {\r\nida->free_bitmap = bitmap;\r\nbitmap = NULL;\r\n}\r\nspin_unlock_irqrestore(&ida->idr.lock, flags);\r\n}\r\nkfree(bitmap);\r\n}\r\nint ida_pre_get(struct ida *ida, gfp_t gfp_mask)\r\n{\r\nif (!__idr_pre_get(&ida->idr, gfp_mask))\r\nreturn 0;\r\nif (!ida->free_bitmap) {\r\nstruct ida_bitmap *bitmap;\r\nbitmap = kmalloc(sizeof(struct ida_bitmap), gfp_mask);\r\nif (!bitmap)\r\nreturn 0;\r\nfree_bitmap(ida, bitmap);\r\n}\r\nreturn 1;\r\n}\r\nint ida_get_new_above(struct ida *ida, int starting_id, int *p_id)\r\n{\r\nstruct idr_layer *pa[MAX_IDR_LEVEL + 1];\r\nstruct ida_bitmap *bitmap;\r\nunsigned long flags;\r\nint idr_id = starting_id / IDA_BITMAP_BITS;\r\nint offset = starting_id % IDA_BITMAP_BITS;\r\nint t, id;\r\nrestart:\r\nt = idr_get_empty_slot(&ida->idr, idr_id, pa, 0, &ida->idr);\r\nif (t < 0)\r\nreturn t == -ENOMEM ? -EAGAIN : t;\r\nif (t * IDA_BITMAP_BITS >= MAX_IDR_BIT)\r\nreturn -ENOSPC;\r\nif (t != idr_id)\r\noffset = 0;\r\nidr_id = t;\r\nbitmap = (void *)pa[0]->ary[idr_id & IDR_MASK];\r\nif (!bitmap) {\r\nspin_lock_irqsave(&ida->idr.lock, flags);\r\nbitmap = ida->free_bitmap;\r\nida->free_bitmap = NULL;\r\nspin_unlock_irqrestore(&ida->idr.lock, flags);\r\nif (!bitmap)\r\nreturn -EAGAIN;\r\nmemset(bitmap, 0, sizeof(struct ida_bitmap));\r\nrcu_assign_pointer(pa[0]->ary[idr_id & IDR_MASK],\r\n(void *)bitmap);\r\npa[0]->count++;\r\n}\r\nt = find_next_zero_bit(bitmap->bitmap, IDA_BITMAP_BITS, offset);\r\nif (t == IDA_BITMAP_BITS) {\r\nidr_id++;\r\noffset = 0;\r\ngoto restart;\r\n}\r\nid = idr_id * IDA_BITMAP_BITS + t;\r\nif (id >= MAX_IDR_BIT)\r\nreturn -ENOSPC;\r\n__set_bit(t, bitmap->bitmap);\r\nif (++bitmap->nr_busy == IDA_BITMAP_BITS)\r\nidr_mark_full(pa, idr_id);\r\n*p_id = id;\r\nif (ida->idr.id_free_cnt || ida->free_bitmap) {\r\nstruct idr_layer *p = get_from_free_list(&ida->idr);\r\nif (p)\r\nkmem_cache_free(idr_layer_cache, p);\r\n}\r\nreturn 0;\r\n}\r\nvoid ida_remove(struct ida *ida, int id)\r\n{\r\nstruct idr_layer *p = ida->idr.top;\r\nint shift = (ida->idr.layers - 1) * IDR_BITS;\r\nint idr_id = id / IDA_BITMAP_BITS;\r\nint offset = id % IDA_BITMAP_BITS;\r\nint n;\r\nstruct ida_bitmap *bitmap;\r\nif (idr_id > idr_max(ida->idr.layers))\r\ngoto err;\r\nwhile ((shift > 0) && p) {\r\nn = (idr_id >> shift) & IDR_MASK;\r\n__clear_bit(n, p->bitmap);\r\np = p->ary[n];\r\nshift -= IDR_BITS;\r\n}\r\nif (p == NULL)\r\ngoto err;\r\nn = idr_id & IDR_MASK;\r\n__clear_bit(n, p->bitmap);\r\nbitmap = (void *)p->ary[n];\r\nif (!bitmap || !test_bit(offset, bitmap->bitmap))\r\ngoto err;\r\n__clear_bit(offset, bitmap->bitmap);\r\nif (--bitmap->nr_busy == 0) {\r\n__set_bit(n, p->bitmap);\r\nidr_remove(&ida->idr, idr_id);\r\nfree_bitmap(ida, bitmap);\r\n}\r\nreturn;\r\nerr:\r\nWARN(1, "ida_remove called for id=%d which is not allocated.\n", id);\r\n}\r\nvoid ida_destroy(struct ida *ida)\r\n{\r\nidr_destroy(&ida->idr);\r\nkfree(ida->free_bitmap);\r\n}\r\nint ida_simple_get(struct ida *ida, unsigned int start, unsigned int end,\r\ngfp_t gfp_mask)\r\n{\r\nint ret, id;\r\nunsigned int max;\r\nunsigned long flags;\r\nBUG_ON((int)start < 0);\r\nBUG_ON((int)end < 0);\r\nif (end == 0)\r\nmax = 0x80000000;\r\nelse {\r\nBUG_ON(end < start);\r\nmax = end - 1;\r\n}\r\nagain:\r\nif (!ida_pre_get(ida, gfp_mask))\r\nreturn -ENOMEM;\r\nspin_lock_irqsave(&simple_ida_lock, flags);\r\nret = ida_get_new_above(ida, start, &id);\r\nif (!ret) {\r\nif (id > max) {\r\nida_remove(ida, id);\r\nret = -ENOSPC;\r\n} else {\r\nret = id;\r\n}\r\n}\r\nspin_unlock_irqrestore(&simple_ida_lock, flags);\r\nif (unlikely(ret == -EAGAIN))\r\ngoto again;\r\nreturn ret;\r\n}\r\nvoid ida_simple_remove(struct ida *ida, unsigned int id)\r\n{\r\nunsigned long flags;\r\nBUG_ON((int)id < 0);\r\nspin_lock_irqsave(&simple_ida_lock, flags);\r\nida_remove(ida, id);\r\nspin_unlock_irqrestore(&simple_ida_lock, flags);\r\n}\r\nvoid ida_init(struct ida *ida)\r\n{\r\nmemset(ida, 0, sizeof(struct ida));\r\nidr_init(&ida->idr);\r\n}
