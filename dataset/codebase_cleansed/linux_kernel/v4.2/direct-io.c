static inline unsigned dio_pages_present(struct dio_submit *sdio)\r\n{\r\nreturn sdio->tail - sdio->head;\r\n}\r\nstatic inline int dio_refill_pages(struct dio *dio, struct dio_submit *sdio)\r\n{\r\nssize_t ret;\r\nret = iov_iter_get_pages(sdio->iter, dio->pages, LONG_MAX, DIO_PAGES,\r\n&sdio->from);\r\nif (ret < 0 && sdio->blocks_available && (dio->rw & WRITE)) {\r\nstruct page *page = ZERO_PAGE(0);\r\nif (dio->page_errors == 0)\r\ndio->page_errors = ret;\r\npage_cache_get(page);\r\ndio->pages[0] = page;\r\nsdio->head = 0;\r\nsdio->tail = 1;\r\nsdio->from = 0;\r\nsdio->to = PAGE_SIZE;\r\nreturn 0;\r\n}\r\nif (ret >= 0) {\r\niov_iter_advance(sdio->iter, ret);\r\nret += sdio->from;\r\nsdio->head = 0;\r\nsdio->tail = (ret + PAGE_SIZE - 1) / PAGE_SIZE;\r\nsdio->to = ((ret - 1) & (PAGE_SIZE - 1)) + 1;\r\nreturn 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic inline struct page *dio_get_page(struct dio *dio,\r\nstruct dio_submit *sdio)\r\n{\r\nif (dio_pages_present(sdio) == 0) {\r\nint ret;\r\nret = dio_refill_pages(dio, sdio);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nBUG_ON(dio_pages_present(sdio) == 0);\r\n}\r\nreturn dio->pages[sdio->head];\r\n}\r\nstatic ssize_t dio_complete(struct dio *dio, loff_t offset, ssize_t ret,\r\nbool is_async)\r\n{\r\nssize_t transferred = 0;\r\nif (ret == -EIOCBQUEUED)\r\nret = 0;\r\nif (dio->result) {\r\ntransferred = dio->result;\r\nif ((dio->rw == READ) && ((offset + transferred) > dio->i_size))\r\ntransferred = dio->i_size - offset;\r\n}\r\nif (ret == 0)\r\nret = dio->page_errors;\r\nif (ret == 0)\r\nret = dio->io_error;\r\nif (ret == 0)\r\nret = transferred;\r\nif (dio->end_io && dio->result)\r\ndio->end_io(dio->iocb, offset, transferred, dio->private);\r\nif (!(dio->flags & DIO_SKIP_DIO_COUNT))\r\ninode_dio_end(dio->inode);\r\nif (is_async) {\r\nif (dio->rw & WRITE) {\r\nint err;\r\nerr = generic_write_sync(dio->iocb->ki_filp, offset,\r\ntransferred);\r\nif (err < 0 && ret > 0)\r\nret = err;\r\n}\r\ndio->iocb->ki_complete(dio->iocb, ret, 0);\r\n}\r\nkmem_cache_free(dio_cache, dio);\r\nreturn ret;\r\n}\r\nstatic void dio_aio_complete_work(struct work_struct *work)\r\n{\r\nstruct dio *dio = container_of(work, struct dio, complete_work);\r\ndio_complete(dio, dio->iocb->ki_pos, 0, true);\r\n}\r\nstatic void dio_bio_end_aio(struct bio *bio, int error)\r\n{\r\nstruct dio *dio = bio->bi_private;\r\nunsigned long remaining;\r\nunsigned long flags;\r\ndio_bio_complete(dio, bio);\r\nspin_lock_irqsave(&dio->bio_lock, flags);\r\nremaining = --dio->refcount;\r\nif (remaining == 1 && dio->waiter)\r\nwake_up_process(dio->waiter);\r\nspin_unlock_irqrestore(&dio->bio_lock, flags);\r\nif (remaining == 0) {\r\nif (dio->result && dio->defer_completion) {\r\nINIT_WORK(&dio->complete_work, dio_aio_complete_work);\r\nqueue_work(dio->inode->i_sb->s_dio_done_wq,\r\n&dio->complete_work);\r\n} else {\r\ndio_complete(dio, dio->iocb->ki_pos, 0, true);\r\n}\r\n}\r\n}\r\nstatic void dio_bio_end_io(struct bio *bio, int error)\r\n{\r\nstruct dio *dio = bio->bi_private;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dio->bio_lock, flags);\r\nbio->bi_private = dio->bio_list;\r\ndio->bio_list = bio;\r\nif (--dio->refcount == 1 && dio->waiter)\r\nwake_up_process(dio->waiter);\r\nspin_unlock_irqrestore(&dio->bio_lock, flags);\r\n}\r\nvoid dio_end_io(struct bio *bio, int error)\r\n{\r\nstruct dio *dio = bio->bi_private;\r\nif (dio->is_async)\r\ndio_bio_end_aio(bio, error);\r\nelse\r\ndio_bio_end_io(bio, error);\r\n}\r\nstatic inline void\r\ndio_bio_alloc(struct dio *dio, struct dio_submit *sdio,\r\nstruct block_device *bdev,\r\nsector_t first_sector, int nr_vecs)\r\n{\r\nstruct bio *bio;\r\nbio = bio_alloc(GFP_KERNEL, nr_vecs);\r\nbio->bi_bdev = bdev;\r\nbio->bi_iter.bi_sector = first_sector;\r\nif (dio->is_async)\r\nbio->bi_end_io = dio_bio_end_aio;\r\nelse\r\nbio->bi_end_io = dio_bio_end_io;\r\nsdio->bio = bio;\r\nsdio->logical_offset_in_bio = sdio->cur_page_fs_offset;\r\n}\r\nstatic inline void dio_bio_submit(struct dio *dio, struct dio_submit *sdio)\r\n{\r\nstruct bio *bio = sdio->bio;\r\nunsigned long flags;\r\nbio->bi_private = dio;\r\nspin_lock_irqsave(&dio->bio_lock, flags);\r\ndio->refcount++;\r\nspin_unlock_irqrestore(&dio->bio_lock, flags);\r\nif (dio->is_async && dio->rw == READ)\r\nbio_set_pages_dirty(bio);\r\nif (sdio->submit_io)\r\nsdio->submit_io(dio->rw, bio, dio->inode,\r\nsdio->logical_offset_in_bio);\r\nelse\r\nsubmit_bio(dio->rw, bio);\r\nsdio->bio = NULL;\r\nsdio->boundary = 0;\r\nsdio->logical_offset_in_bio = 0;\r\n}\r\nstatic inline void dio_cleanup(struct dio *dio, struct dio_submit *sdio)\r\n{\r\nwhile (sdio->head < sdio->tail)\r\npage_cache_release(dio->pages[sdio->head++]);\r\n}\r\nstatic struct bio *dio_await_one(struct dio *dio)\r\n{\r\nunsigned long flags;\r\nstruct bio *bio = NULL;\r\nspin_lock_irqsave(&dio->bio_lock, flags);\r\nwhile (dio->refcount > 1 && dio->bio_list == NULL) {\r\n__set_current_state(TASK_UNINTERRUPTIBLE);\r\ndio->waiter = current;\r\nspin_unlock_irqrestore(&dio->bio_lock, flags);\r\nio_schedule();\r\nspin_lock_irqsave(&dio->bio_lock, flags);\r\ndio->waiter = NULL;\r\n}\r\nif (dio->bio_list) {\r\nbio = dio->bio_list;\r\ndio->bio_list = bio->bi_private;\r\n}\r\nspin_unlock_irqrestore(&dio->bio_lock, flags);\r\nreturn bio;\r\n}\r\nstatic int dio_bio_complete(struct dio *dio, struct bio *bio)\r\n{\r\nconst int uptodate = test_bit(BIO_UPTODATE, &bio->bi_flags);\r\nstruct bio_vec *bvec;\r\nunsigned i;\r\nif (!uptodate)\r\ndio->io_error = -EIO;\r\nif (dio->is_async && dio->rw == READ) {\r\nbio_check_pages_dirty(bio);\r\n} else {\r\nbio_for_each_segment_all(bvec, bio, i) {\r\nstruct page *page = bvec->bv_page;\r\nif (dio->rw == READ && !PageCompound(page))\r\nset_page_dirty_lock(page);\r\npage_cache_release(page);\r\n}\r\nbio_put(bio);\r\n}\r\nreturn uptodate ? 0 : -EIO;\r\n}\r\nstatic void dio_await_completion(struct dio *dio)\r\n{\r\nstruct bio *bio;\r\ndo {\r\nbio = dio_await_one(dio);\r\nif (bio)\r\ndio_bio_complete(dio, bio);\r\n} while (bio);\r\n}\r\nstatic inline int dio_bio_reap(struct dio *dio, struct dio_submit *sdio)\r\n{\r\nint ret = 0;\r\nif (sdio->reap_counter++ >= 64) {\r\nwhile (dio->bio_list) {\r\nunsigned long flags;\r\nstruct bio *bio;\r\nint ret2;\r\nspin_lock_irqsave(&dio->bio_lock, flags);\r\nbio = dio->bio_list;\r\ndio->bio_list = bio->bi_private;\r\nspin_unlock_irqrestore(&dio->bio_lock, flags);\r\nret2 = dio_bio_complete(dio, bio);\r\nif (ret == 0)\r\nret = ret2;\r\n}\r\nsdio->reap_counter = 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic int sb_init_dio_done_wq(struct super_block *sb)\r\n{\r\nstruct workqueue_struct *old;\r\nstruct workqueue_struct *wq = alloc_workqueue("dio/%s",\r\nWQ_MEM_RECLAIM, 0,\r\nsb->s_id);\r\nif (!wq)\r\nreturn -ENOMEM;\r\nold = cmpxchg(&sb->s_dio_done_wq, NULL, wq);\r\nif (old)\r\ndestroy_workqueue(wq);\r\nreturn 0;\r\n}\r\nstatic int dio_set_defer_completion(struct dio *dio)\r\n{\r\nstruct super_block *sb = dio->inode->i_sb;\r\nif (dio->defer_completion)\r\nreturn 0;\r\ndio->defer_completion = true;\r\nif (!sb->s_dio_done_wq)\r\nreturn sb_init_dio_done_wq(sb);\r\nreturn 0;\r\n}\r\nstatic int get_more_blocks(struct dio *dio, struct dio_submit *sdio,\r\nstruct buffer_head *map_bh)\r\n{\r\nint ret;\r\nsector_t fs_startblk;\r\nsector_t fs_endblk;\r\nunsigned long fs_count;\r\nint create;\r\nunsigned int i_blkbits = sdio->blkbits + sdio->blkfactor;\r\nret = dio->page_errors;\r\nif (ret == 0) {\r\nBUG_ON(sdio->block_in_file >= sdio->final_block_in_request);\r\nfs_startblk = sdio->block_in_file >> sdio->blkfactor;\r\nfs_endblk = (sdio->final_block_in_request - 1) >>\r\nsdio->blkfactor;\r\nfs_count = fs_endblk - fs_startblk + 1;\r\nmap_bh->b_state = 0;\r\nmap_bh->b_size = fs_count << i_blkbits;\r\ncreate = dio->rw & WRITE;\r\nif (dio->flags & DIO_SKIP_HOLES) {\r\nif (sdio->block_in_file < (i_size_read(dio->inode) >>\r\nsdio->blkbits))\r\ncreate = 0;\r\n}\r\nret = (*sdio->get_block)(dio->inode, fs_startblk,\r\nmap_bh, create);\r\ndio->private = map_bh->b_private;\r\nif (ret == 0 && buffer_defer_completion(map_bh))\r\nret = dio_set_defer_completion(dio);\r\n}\r\nreturn ret;\r\n}\r\nstatic inline int dio_new_bio(struct dio *dio, struct dio_submit *sdio,\r\nsector_t start_sector, struct buffer_head *map_bh)\r\n{\r\nsector_t sector;\r\nint ret, nr_pages;\r\nret = dio_bio_reap(dio, sdio);\r\nif (ret)\r\ngoto out;\r\nsector = start_sector << (sdio->blkbits - 9);\r\nnr_pages = min(sdio->pages_in_io, bio_get_nr_vecs(map_bh->b_bdev));\r\nBUG_ON(nr_pages <= 0);\r\ndio_bio_alloc(dio, sdio, map_bh->b_bdev, sector, nr_pages);\r\nsdio->boundary = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic inline int dio_bio_add_page(struct dio_submit *sdio)\r\n{\r\nint ret;\r\nret = bio_add_page(sdio->bio, sdio->cur_page,\r\nsdio->cur_page_len, sdio->cur_page_offset);\r\nif (ret == sdio->cur_page_len) {\r\nif ((sdio->cur_page_len + sdio->cur_page_offset) == PAGE_SIZE)\r\nsdio->pages_in_io--;\r\npage_cache_get(sdio->cur_page);\r\nsdio->final_block_in_bio = sdio->cur_page_block +\r\n(sdio->cur_page_len >> sdio->blkbits);\r\nret = 0;\r\n} else {\r\nret = 1;\r\n}\r\nreturn ret;\r\n}\r\nstatic inline int dio_send_cur_page(struct dio *dio, struct dio_submit *sdio,\r\nstruct buffer_head *map_bh)\r\n{\r\nint ret = 0;\r\nif (sdio->bio) {\r\nloff_t cur_offset = sdio->cur_page_fs_offset;\r\nloff_t bio_next_offset = sdio->logical_offset_in_bio +\r\nsdio->bio->bi_iter.bi_size;\r\nif (sdio->final_block_in_bio != sdio->cur_page_block ||\r\ncur_offset != bio_next_offset)\r\ndio_bio_submit(dio, sdio);\r\n}\r\nif (sdio->bio == NULL) {\r\nret = dio_new_bio(dio, sdio, sdio->cur_page_block, map_bh);\r\nif (ret)\r\ngoto out;\r\n}\r\nif (dio_bio_add_page(sdio) != 0) {\r\ndio_bio_submit(dio, sdio);\r\nret = dio_new_bio(dio, sdio, sdio->cur_page_block, map_bh);\r\nif (ret == 0) {\r\nret = dio_bio_add_page(sdio);\r\nBUG_ON(ret != 0);\r\n}\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic inline int\r\nsubmit_page_section(struct dio *dio, struct dio_submit *sdio, struct page *page,\r\nunsigned offset, unsigned len, sector_t blocknr,\r\nstruct buffer_head *map_bh)\r\n{\r\nint ret = 0;\r\nif (dio->rw & WRITE) {\r\ntask_io_account_write(len);\r\n}\r\nif (sdio->cur_page == page &&\r\nsdio->cur_page_offset + sdio->cur_page_len == offset &&\r\nsdio->cur_page_block +\r\n(sdio->cur_page_len >> sdio->blkbits) == blocknr) {\r\nsdio->cur_page_len += len;\r\ngoto out;\r\n}\r\nif (sdio->cur_page) {\r\nret = dio_send_cur_page(dio, sdio, map_bh);\r\npage_cache_release(sdio->cur_page);\r\nsdio->cur_page = NULL;\r\nif (ret)\r\nreturn ret;\r\n}\r\npage_cache_get(page);\r\nsdio->cur_page = page;\r\nsdio->cur_page_offset = offset;\r\nsdio->cur_page_len = len;\r\nsdio->cur_page_block = blocknr;\r\nsdio->cur_page_fs_offset = sdio->block_in_file << sdio->blkbits;\r\nout:\r\nif (sdio->boundary) {\r\nret = dio_send_cur_page(dio, sdio, map_bh);\r\ndio_bio_submit(dio, sdio);\r\npage_cache_release(sdio->cur_page);\r\nsdio->cur_page = NULL;\r\n}\r\nreturn ret;\r\n}\r\nstatic void clean_blockdev_aliases(struct dio *dio, struct buffer_head *map_bh)\r\n{\r\nunsigned i;\r\nunsigned nblocks;\r\nnblocks = map_bh->b_size >> dio->inode->i_blkbits;\r\nfor (i = 0; i < nblocks; i++) {\r\nunmap_underlying_metadata(map_bh->b_bdev,\r\nmap_bh->b_blocknr + i);\r\n}\r\n}\r\nstatic inline void dio_zero_block(struct dio *dio, struct dio_submit *sdio,\r\nint end, struct buffer_head *map_bh)\r\n{\r\nunsigned dio_blocks_per_fs_block;\r\nunsigned this_chunk_blocks;\r\nunsigned this_chunk_bytes;\r\nstruct page *page;\r\nsdio->start_zero_done = 1;\r\nif (!sdio->blkfactor || !buffer_new(map_bh))\r\nreturn;\r\ndio_blocks_per_fs_block = 1 << sdio->blkfactor;\r\nthis_chunk_blocks = sdio->block_in_file & (dio_blocks_per_fs_block - 1);\r\nif (!this_chunk_blocks)\r\nreturn;\r\nif (end)\r\nthis_chunk_blocks = dio_blocks_per_fs_block - this_chunk_blocks;\r\nthis_chunk_bytes = this_chunk_blocks << sdio->blkbits;\r\npage = ZERO_PAGE(0);\r\nif (submit_page_section(dio, sdio, page, 0, this_chunk_bytes,\r\nsdio->next_block_for_io, map_bh))\r\nreturn;\r\nsdio->next_block_for_io += this_chunk_blocks;\r\n}\r\nstatic int do_direct_IO(struct dio *dio, struct dio_submit *sdio,\r\nstruct buffer_head *map_bh)\r\n{\r\nconst unsigned blkbits = sdio->blkbits;\r\nint ret = 0;\r\nwhile (sdio->block_in_file < sdio->final_block_in_request) {\r\nstruct page *page;\r\nsize_t from, to;\r\npage = dio_get_page(dio, sdio);\r\nif (IS_ERR(page)) {\r\nret = PTR_ERR(page);\r\ngoto out;\r\n}\r\nfrom = sdio->head ? 0 : sdio->from;\r\nto = (sdio->head == sdio->tail - 1) ? sdio->to : PAGE_SIZE;\r\nsdio->head++;\r\nwhile (from < to) {\r\nunsigned this_chunk_bytes;\r\nunsigned this_chunk_blocks;\r\nunsigned u;\r\nif (sdio->blocks_available == 0) {\r\nunsigned long blkmask;\r\nunsigned long dio_remainder;\r\nret = get_more_blocks(dio, sdio, map_bh);\r\nif (ret) {\r\npage_cache_release(page);\r\ngoto out;\r\n}\r\nif (!buffer_mapped(map_bh))\r\ngoto do_holes;\r\nsdio->blocks_available =\r\nmap_bh->b_size >> sdio->blkbits;\r\nsdio->next_block_for_io =\r\nmap_bh->b_blocknr << sdio->blkfactor;\r\nif (buffer_new(map_bh))\r\nclean_blockdev_aliases(dio, map_bh);\r\nif (!sdio->blkfactor)\r\ngoto do_holes;\r\nblkmask = (1 << sdio->blkfactor) - 1;\r\ndio_remainder = (sdio->block_in_file & blkmask);\r\nif (!buffer_new(map_bh))\r\nsdio->next_block_for_io += dio_remainder;\r\nsdio->blocks_available -= dio_remainder;\r\n}\r\ndo_holes:\r\nif (!buffer_mapped(map_bh)) {\r\nloff_t i_size_aligned;\r\nif (dio->rw & WRITE) {\r\npage_cache_release(page);\r\nreturn -ENOTBLK;\r\n}\r\ni_size_aligned = ALIGN(i_size_read(dio->inode),\r\n1 << blkbits);\r\nif (sdio->block_in_file >=\r\ni_size_aligned >> blkbits) {\r\npage_cache_release(page);\r\ngoto out;\r\n}\r\nzero_user(page, from, 1 << blkbits);\r\nsdio->block_in_file++;\r\nfrom += 1 << blkbits;\r\ndio->result += 1 << blkbits;\r\ngoto next_block;\r\n}\r\nif (unlikely(sdio->blkfactor && !sdio->start_zero_done))\r\ndio_zero_block(dio, sdio, 0, map_bh);\r\nthis_chunk_blocks = sdio->blocks_available;\r\nu = (to - from) >> blkbits;\r\nif (this_chunk_blocks > u)\r\nthis_chunk_blocks = u;\r\nu = sdio->final_block_in_request - sdio->block_in_file;\r\nif (this_chunk_blocks > u)\r\nthis_chunk_blocks = u;\r\nthis_chunk_bytes = this_chunk_blocks << blkbits;\r\nBUG_ON(this_chunk_bytes == 0);\r\nif (this_chunk_blocks == sdio->blocks_available)\r\nsdio->boundary = buffer_boundary(map_bh);\r\nret = submit_page_section(dio, sdio, page,\r\nfrom,\r\nthis_chunk_bytes,\r\nsdio->next_block_for_io,\r\nmap_bh);\r\nif (ret) {\r\npage_cache_release(page);\r\ngoto out;\r\n}\r\nsdio->next_block_for_io += this_chunk_blocks;\r\nsdio->block_in_file += this_chunk_blocks;\r\nfrom += this_chunk_bytes;\r\ndio->result += this_chunk_bytes;\r\nsdio->blocks_available -= this_chunk_blocks;\r\nnext_block:\r\nBUG_ON(sdio->block_in_file > sdio->final_block_in_request);\r\nif (sdio->block_in_file == sdio->final_block_in_request)\r\nbreak;\r\n}\r\npage_cache_release(page);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic inline int drop_refcount(struct dio *dio)\r\n{\r\nint ret2;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dio->bio_lock, flags);\r\nret2 = --dio->refcount;\r\nspin_unlock_irqrestore(&dio->bio_lock, flags);\r\nreturn ret2;\r\n}\r\nstatic inline ssize_t\r\ndo_blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,\r\nstruct block_device *bdev, struct iov_iter *iter,\r\nloff_t offset, get_block_t get_block, dio_iodone_t end_io,\r\ndio_submit_t submit_io, int flags)\r\n{\r\nunsigned i_blkbits = ACCESS_ONCE(inode->i_blkbits);\r\nunsigned blkbits = i_blkbits;\r\nunsigned blocksize_mask = (1 << blkbits) - 1;\r\nssize_t retval = -EINVAL;\r\nsize_t count = iov_iter_count(iter);\r\nloff_t end = offset + count;\r\nstruct dio *dio;\r\nstruct dio_submit sdio = { 0, };\r\nstruct buffer_head map_bh = { 0, };\r\nstruct blk_plug plug;\r\nunsigned long align = offset | iov_iter_alignment(iter);\r\nif (align & blocksize_mask) {\r\nif (bdev)\r\nblkbits = blksize_bits(bdev_logical_block_size(bdev));\r\nblocksize_mask = (1 << blkbits) - 1;\r\nif (align & blocksize_mask)\r\ngoto out;\r\n}\r\nif (iov_iter_rw(iter) == READ && !iov_iter_count(iter))\r\nreturn 0;\r\ndio = kmem_cache_alloc(dio_cache, GFP_KERNEL);\r\nretval = -ENOMEM;\r\nif (!dio)\r\ngoto out;\r\nmemset(dio, 0, offsetof(struct dio, pages));\r\ndio->flags = flags;\r\nif (dio->flags & DIO_LOCKING) {\r\nif (iov_iter_rw(iter) == READ) {\r\nstruct address_space *mapping =\r\niocb->ki_filp->f_mapping;\r\nmutex_lock(&inode->i_mutex);\r\nretval = filemap_write_and_wait_range(mapping, offset,\r\nend - 1);\r\nif (retval) {\r\nmutex_unlock(&inode->i_mutex);\r\nkmem_cache_free(dio_cache, dio);\r\ngoto out;\r\n}\r\n}\r\n}\r\nif (is_sync_kiocb(iocb))\r\ndio->is_async = false;\r\nelse if (!(dio->flags & DIO_ASYNC_EXTEND) &&\r\niov_iter_rw(iter) == WRITE && end > i_size_read(inode))\r\ndio->is_async = false;\r\nelse\r\ndio->is_async = true;\r\ndio->inode = inode;\r\ndio->rw = iov_iter_rw(iter) == WRITE ? WRITE_ODIRECT : READ;\r\nif (dio->is_async && iov_iter_rw(iter) == WRITE &&\r\n((iocb->ki_filp->f_flags & O_DSYNC) ||\r\nIS_SYNC(iocb->ki_filp->f_mapping->host))) {\r\nretval = dio_set_defer_completion(dio);\r\nif (retval) {\r\nkmem_cache_free(dio_cache, dio);\r\ngoto out;\r\n}\r\n}\r\nif (!(dio->flags & DIO_SKIP_DIO_COUNT))\r\ninode_dio_begin(inode);\r\nretval = 0;\r\nsdio.blkbits = blkbits;\r\nsdio.blkfactor = i_blkbits - blkbits;\r\nsdio.block_in_file = offset >> blkbits;\r\nsdio.get_block = get_block;\r\ndio->end_io = end_io;\r\nsdio.submit_io = submit_io;\r\nsdio.final_block_in_bio = -1;\r\nsdio.next_block_for_io = -1;\r\ndio->iocb = iocb;\r\ndio->i_size = i_size_read(inode);\r\nspin_lock_init(&dio->bio_lock);\r\ndio->refcount = 1;\r\nsdio.iter = iter;\r\nsdio.final_block_in_request =\r\n(offset + iov_iter_count(iter)) >> blkbits;\r\nif (unlikely(sdio.blkfactor))\r\nsdio.pages_in_io = 2;\r\nsdio.pages_in_io += iov_iter_npages(iter, INT_MAX);\r\nblk_start_plug(&plug);\r\nretval = do_direct_IO(dio, &sdio, &map_bh);\r\nif (retval)\r\ndio_cleanup(dio, &sdio);\r\nif (retval == -ENOTBLK) {\r\nretval = 0;\r\n}\r\ndio_zero_block(dio, &sdio, 1, &map_bh);\r\nif (sdio.cur_page) {\r\nssize_t ret2;\r\nret2 = dio_send_cur_page(dio, &sdio, &map_bh);\r\nif (retval == 0)\r\nretval = ret2;\r\npage_cache_release(sdio.cur_page);\r\nsdio.cur_page = NULL;\r\n}\r\nif (sdio.bio)\r\ndio_bio_submit(dio, &sdio);\r\nblk_finish_plug(&plug);\r\ndio_cleanup(dio, &sdio);\r\nif (iov_iter_rw(iter) == READ && (dio->flags & DIO_LOCKING))\r\nmutex_unlock(&dio->inode->i_mutex);\r\nBUG_ON(retval == -EIOCBQUEUED);\r\nif (dio->is_async && retval == 0 && dio->result &&\r\n(iov_iter_rw(iter) == READ || dio->result == count))\r\nretval = -EIOCBQUEUED;\r\nelse\r\ndio_await_completion(dio);\r\nif (drop_refcount(dio) == 0) {\r\nretval = dio_complete(dio, offset, retval, false);\r\n} else\r\nBUG_ON(retval != -EIOCBQUEUED);\r\nout:\r\nreturn retval;\r\n}\r\nssize_t __blockdev_direct_IO(struct kiocb *iocb, struct inode *inode,\r\nstruct block_device *bdev, struct iov_iter *iter,\r\nloff_t offset, get_block_t get_block,\r\ndio_iodone_t end_io, dio_submit_t submit_io,\r\nint flags)\r\n{\r\nprefetch(&bdev->bd_disk->part_tbl);\r\nprefetch(bdev->bd_queue);\r\nprefetch((char *)bdev->bd_queue + SMP_CACHE_BYTES);\r\nreturn do_blockdev_direct_IO(iocb, inode, bdev, iter, offset, get_block,\r\nend_io, submit_io, flags);\r\n}\r\nstatic __init int dio_init(void)\r\n{\r\ndio_cache = KMEM_CACHE(dio, SLAB_PANIC);\r\nreturn 0;\r\n}
