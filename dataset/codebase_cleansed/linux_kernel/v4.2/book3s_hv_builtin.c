static int __init early_parse_kvm_cma_resv(char *p)\r\n{\r\npr_debug("%s(%s)\n", __func__, p);\r\nif (!p)\r\nreturn -EINVAL;\r\nreturn kstrtoul(p, 0, &kvm_cma_resv_ratio);\r\n}\r\nstruct page *kvm_alloc_hpt(unsigned long nr_pages)\r\n{\r\nVM_BUG_ON(order_base_2(nr_pages) < KVM_CMA_CHUNK_ORDER - PAGE_SHIFT);\r\nreturn cma_alloc(kvm_cma, nr_pages, order_base_2(HPT_ALIGN_PAGES));\r\n}\r\nvoid kvm_release_hpt(struct page *page, unsigned long nr_pages)\r\n{\r\ncma_release(kvm_cma, page, nr_pages);\r\n}\r\nvoid __init kvm_cma_reserve(void)\r\n{\r\nunsigned long align_size;\r\nstruct memblock_region *reg;\r\nphys_addr_t selected_size = 0;\r\nif (!cpu_has_feature(CPU_FTR_HVMODE))\r\nreturn;\r\nfor_each_memblock(memory, reg)\r\nselected_size += memblock_region_memory_end_pfn(reg) -\r\nmemblock_region_memory_base_pfn(reg);\r\nselected_size = (selected_size * kvm_cma_resv_ratio / 100) << PAGE_SHIFT;\r\nif (selected_size) {\r\npr_debug("%s: reserving %ld MiB for global area\n", __func__,\r\n(unsigned long)selected_size / SZ_1M);\r\nalign_size = HPT_ALIGN_PAGES << PAGE_SHIFT;\r\ncma_declare_contiguous(0, selected_size, 0, align_size,\r\nKVM_CMA_CHUNK_ORDER - PAGE_SHIFT, false, &kvm_cma);\r\n}\r\n}\r\nlong int kvmppc_rm_h_confer(struct kvm_vcpu *vcpu, int target,\r\nunsigned int yield_count)\r\n{\r\nstruct kvmppc_vcore *vc = vcpu->arch.vcore;\r\nint threads_running;\r\nint threads_ceded;\r\nint threads_conferring;\r\nu64 stop = get_tb() + 10 * tb_ticks_per_usec;\r\nint rv = H_SUCCESS;\r\nset_bit(vcpu->arch.ptid, &vc->conferring_threads);\r\nwhile ((get_tb() < stop) && !VCORE_IS_EXITING(vc)) {\r\nthreads_running = VCORE_ENTRY_MAP(vc);\r\nthreads_ceded = vc->napping_threads;\r\nthreads_conferring = vc->conferring_threads;\r\nif ((threads_ceded | threads_conferring) == threads_running) {\r\nrv = H_TOO_HARD;\r\nbreak;\r\n}\r\n}\r\nclear_bit(vcpu->arch.ptid, &vc->conferring_threads);\r\nreturn rv;\r\n}\r\nvoid kvm_hv_vm_activated(void)\r\n{\r\nget_online_cpus();\r\natomic_inc(&hv_vm_count);\r\nput_online_cpus();\r\n}\r\nvoid kvm_hv_vm_deactivated(void)\r\n{\r\nget_online_cpus();\r\natomic_dec(&hv_vm_count);\r\nput_online_cpus();\r\n}\r\nbool kvm_hv_mode_active(void)\r\n{\r\nreturn atomic_read(&hv_vm_count) != 0;\r\n}\r\nint kvmppc_hcall_impl_hv_realmode(unsigned long cmd)\r\n{\r\ncmd /= 4;\r\nif (cmd < hcall_real_table_end - hcall_real_table &&\r\nhcall_real_table[cmd])\r\nreturn 1;\r\nreturn 0;\r\n}\r\nint kvmppc_hwrng_present(void)\r\n{\r\nreturn powernv_hwrng_present();\r\n}\r\nlong kvmppc_h_random(struct kvm_vcpu *vcpu)\r\n{\r\nif (powernv_get_random_real_mode(&vcpu->arch.gpr[4]))\r\nreturn H_SUCCESS;\r\nreturn H_HARDWARE;\r\n}\r\nstatic inline void rm_writeb(unsigned long paddr, u8 val)\r\n{\r\n__asm__ __volatile__("stbcix %0,0,%1"\r\n: : "r" (val), "r" (paddr) : "memory");\r\n}\r\nvoid kvmhv_rm_send_ipi(int cpu)\r\n{\r\nunsigned long xics_phys;\r\nif (cpu_has_feature(CPU_FTR_ARCH_207S) &&\r\ncpu_first_thread_sibling(cpu) ==\r\ncpu_first_thread_sibling(raw_smp_processor_id())) {\r\nunsigned long msg = PPC_DBELL_TYPE(PPC_DBELL_SERVER);\r\nmsg |= cpu_thread_in_core(cpu);\r\n__asm__ __volatile__ (PPC_MSGSND(%0) : : "r" (msg));\r\nreturn;\r\n}\r\nxics_phys = paca[cpu].kvm_hstate.xics_phys;\r\nrm_writeb(xics_phys + XICS_MFRR, IPI_PRIORITY);\r\n}\r\nstatic void kvmhv_interrupt_vcore(struct kvmppc_vcore *vc, int active)\r\n{\r\nint cpu = vc->pcpu;\r\nsmp_mb();\r\nfor (; active; active >>= 1, ++cpu)\r\nif (active & 1)\r\nkvmhv_rm_send_ipi(cpu);\r\n}\r\nvoid kvmhv_commence_exit(int trap)\r\n{\r\nstruct kvmppc_vcore *vc = local_paca->kvm_hstate.kvm_vcore;\r\nint ptid = local_paca->kvm_hstate.ptid;\r\nint me, ee;\r\nme = 0x100 << ptid;\r\ndo {\r\nee = vc->entry_exit_map;\r\n} while (cmpxchg(&vc->entry_exit_map, ee, ee | me) != ee);\r\nif ((ee >> 8) != 0)\r\nreturn;\r\nif (trap != BOOK3S_INTERRUPT_HV_DECREMENTER)\r\nkvmhv_interrupt_vcore(vc, ee & ~(1 << ptid));\r\n}
