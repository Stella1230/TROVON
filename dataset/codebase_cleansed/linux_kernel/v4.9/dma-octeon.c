static dma_addr_t octeon_hole_phys_to_dma(phys_addr_t paddr)\r\n{\r\nif (paddr >= CVMX_PCIE_BAR1_PHYS_BASE && paddr < (CVMX_PCIE_BAR1_PHYS_BASE + CVMX_PCIE_BAR1_PHYS_SIZE))\r\nreturn paddr - CVMX_PCIE_BAR1_PHYS_BASE + CVMX_PCIE_BAR1_RC_BASE;\r\nelse\r\nreturn paddr;\r\n}\r\nstatic phys_addr_t octeon_hole_dma_to_phys(dma_addr_t daddr)\r\n{\r\nif (daddr >= CVMX_PCIE_BAR1_RC_BASE)\r\nreturn daddr + CVMX_PCIE_BAR1_PHYS_BASE - CVMX_PCIE_BAR1_RC_BASE;\r\nelse\r\nreturn daddr;\r\n}\r\nstatic dma_addr_t octeon_gen1_phys_to_dma(struct device *dev, phys_addr_t paddr)\r\n{\r\nif (paddr >= 0x410000000ull && paddr < 0x420000000ull)\r\npaddr -= 0x400000000ull;\r\nreturn octeon_hole_phys_to_dma(paddr);\r\n}\r\nstatic phys_addr_t octeon_gen1_dma_to_phys(struct device *dev, dma_addr_t daddr)\r\n{\r\ndaddr = octeon_hole_dma_to_phys(daddr);\r\nif (daddr >= 0x10000000ull && daddr < 0x20000000ull)\r\ndaddr += 0x400000000ull;\r\nreturn daddr;\r\n}\r\nstatic dma_addr_t octeon_gen2_phys_to_dma(struct device *dev, phys_addr_t paddr)\r\n{\r\nreturn octeon_hole_phys_to_dma(paddr);\r\n}\r\nstatic phys_addr_t octeon_gen2_dma_to_phys(struct device *dev, dma_addr_t daddr)\r\n{\r\nreturn octeon_hole_dma_to_phys(daddr);\r\n}\r\nstatic dma_addr_t octeon_big_phys_to_dma(struct device *dev, phys_addr_t paddr)\r\n{\r\nif (paddr >= 0x410000000ull && paddr < 0x420000000ull)\r\npaddr -= 0x400000000ull;\r\nif (paddr >= 0xf0000000ull)\r\npaddr = OCTEON_BAR2_PCI_ADDRESS + paddr;\r\nreturn paddr;\r\n}\r\nstatic phys_addr_t octeon_big_dma_to_phys(struct device *dev, dma_addr_t daddr)\r\n{\r\nif (daddr >= OCTEON_BAR2_PCI_ADDRESS)\r\ndaddr -= OCTEON_BAR2_PCI_ADDRESS;\r\nif (daddr >= 0x10000000ull && daddr < 0x20000000ull)\r\ndaddr += 0x400000000ull;\r\nreturn daddr;\r\n}\r\nstatic dma_addr_t octeon_small_phys_to_dma(struct device *dev,\r\nphys_addr_t paddr)\r\n{\r\nif (paddr >= 0x410000000ull && paddr < 0x420000000ull)\r\npaddr -= 0x400000000ull;\r\nif (paddr >= octeon_bar1_pci_phys && paddr < octeon_bar1_pci_phys + 0x8000000ull)\r\npaddr = paddr - octeon_bar1_pci_phys;\r\nelse\r\npaddr = OCTEON_BAR2_PCI_ADDRESS + paddr;\r\nreturn paddr;\r\n}\r\nstatic phys_addr_t octeon_small_dma_to_phys(struct device *dev,\r\ndma_addr_t daddr)\r\n{\r\nif (daddr >= OCTEON_BAR2_PCI_ADDRESS)\r\ndaddr -= OCTEON_BAR2_PCI_ADDRESS;\r\nelse\r\ndaddr += octeon_bar1_pci_phys;\r\nif (daddr >= 0x10000000ull && daddr < 0x20000000ull)\r\ndaddr += 0x400000000ull;\r\nreturn daddr;\r\n}\r\nstatic dma_addr_t octeon_dma_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size, enum dma_data_direction direction,\r\nunsigned long attrs)\r\n{\r\ndma_addr_t daddr = swiotlb_map_page(dev, page, offset, size,\r\ndirection, attrs);\r\nmb();\r\nreturn daddr;\r\n}\r\nstatic int octeon_dma_map_sg(struct device *dev, struct scatterlist *sg,\r\nint nents, enum dma_data_direction direction, unsigned long attrs)\r\n{\r\nint r = swiotlb_map_sg_attrs(dev, sg, nents, direction, attrs);\r\nmb();\r\nreturn r;\r\n}\r\nstatic void octeon_dma_sync_single_for_device(struct device *dev,\r\ndma_addr_t dma_handle, size_t size, enum dma_data_direction direction)\r\n{\r\nswiotlb_sync_single_for_device(dev, dma_handle, size, direction);\r\nmb();\r\n}\r\nstatic void octeon_dma_sync_sg_for_device(struct device *dev,\r\nstruct scatterlist *sg, int nelems, enum dma_data_direction direction)\r\n{\r\nswiotlb_sync_sg_for_device(dev, sg, nelems, direction);\r\nmb();\r\n}\r\nstatic void *octeon_dma_alloc_coherent(struct device *dev, size_t size,\r\ndma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs)\r\n{\r\nvoid *ret;\r\ngfp &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);\r\n#ifdef CONFIG_ZONE_DMA\r\nif (dev == NULL)\r\ngfp |= __GFP_DMA;\r\nelse if (dev->coherent_dma_mask <= DMA_BIT_MASK(24))\r\ngfp |= __GFP_DMA;\r\nelse\r\n#endif\r\n#ifdef CONFIG_ZONE_DMA32\r\nif (dev->coherent_dma_mask <= DMA_BIT_MASK(32))\r\ngfp |= __GFP_DMA32;\r\nelse\r\n#endif\r\n;\r\ngfp |= __GFP_NORETRY;\r\nret = swiotlb_alloc_coherent(dev, size, dma_handle, gfp);\r\nmb();\r\nreturn ret;\r\n}\r\nstatic void octeon_dma_free_coherent(struct device *dev, size_t size,\r\nvoid *vaddr, dma_addr_t dma_handle, unsigned long attrs)\r\n{\r\nswiotlb_free_coherent(dev, size, vaddr, dma_handle);\r\n}\r\nstatic dma_addr_t octeon_unity_phys_to_dma(struct device *dev, phys_addr_t paddr)\r\n{\r\nreturn paddr;\r\n}\r\nstatic phys_addr_t octeon_unity_dma_to_phys(struct device *dev, dma_addr_t daddr)\r\n{\r\nreturn daddr;\r\n}\r\ndma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)\r\n{\r\nstruct octeon_dma_map_ops *ops = container_of(get_dma_ops(dev),\r\nstruct octeon_dma_map_ops,\r\ndma_map_ops);\r\nreturn ops->phys_to_dma(dev, paddr);\r\n}\r\nphys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr)\r\n{\r\nstruct octeon_dma_map_ops *ops = container_of(get_dma_ops(dev),\r\nstruct octeon_dma_map_ops,\r\ndma_map_ops);\r\nreturn ops->dma_to_phys(dev, daddr);\r\n}\r\nvoid __init plat_swiotlb_setup(void)\r\n{\r\nint i;\r\nphys_addr_t max_addr;\r\nphys_addr_t addr_size;\r\nsize_t swiotlbsize;\r\nunsigned long swiotlb_nslabs;\r\nmax_addr = 0;\r\naddr_size = 0;\r\nfor (i = 0 ; i < boot_mem_map.nr_map; i++) {\r\nstruct boot_mem_map_entry *e = &boot_mem_map.map[i];\r\nif (e->type != BOOT_MEM_RAM && e->type != BOOT_MEM_INIT_RAM)\r\ncontinue;\r\nif (e->addr > 0x410000000ull && !OCTEON_IS_OCTEON2())\r\ncontinue;\r\naddr_size += e->size;\r\nif (max_addr < e->addr + e->size)\r\nmax_addr = e->addr + e->size;\r\n}\r\nswiotlbsize = PAGE_SIZE;\r\n#ifdef CONFIG_PCI\r\nif (OCTEON_IS_MODEL(OCTEON_CN31XX)\r\n|| OCTEON_IS_MODEL(OCTEON_CN38XX_PASS2)) {\r\nswiotlbsize = addr_size / 4;\r\nif (swiotlbsize > 64 * (1<<20))\r\nswiotlbsize = 64 * (1<<20);\r\n} else if (max_addr > 0xf0000000ul) {\r\nswiotlbsize = 64 * (1<<20);\r\n}\r\n#endif\r\n#ifdef CONFIG_USB_OHCI_HCD_PLATFORM\r\nif (OCTEON_IS_OCTEON2() && max_addr >= 0x100000000ul)\r\nswiotlbsize = 64 * (1<<20);\r\n#endif\r\nswiotlb_nslabs = swiotlbsize >> IO_TLB_SHIFT;\r\nswiotlb_nslabs = ALIGN(swiotlb_nslabs, IO_TLB_SEGSIZE);\r\nswiotlbsize = swiotlb_nslabs << IO_TLB_SHIFT;\r\nocteon_swiotlb = alloc_bootmem_low_pages(swiotlbsize);\r\nif (swiotlb_init_with_tbl(octeon_swiotlb, swiotlb_nslabs, 1) == -ENOMEM)\r\npanic("Cannot allocate SWIOTLB buffer");\r\nmips_dma_map_ops = &octeon_linear_dma_map_ops.dma_map_ops;\r\n}\r\nvoid __init octeon_pci_dma_init(void)\r\n{\r\nswitch (octeon_dma_bar_type) {\r\ncase OCTEON_DMA_BAR_TYPE_PCIE2:\r\n_octeon_pci_dma_map_ops.phys_to_dma = octeon_gen2_phys_to_dma;\r\n_octeon_pci_dma_map_ops.dma_to_phys = octeon_gen2_dma_to_phys;\r\nbreak;\r\ncase OCTEON_DMA_BAR_TYPE_PCIE:\r\n_octeon_pci_dma_map_ops.phys_to_dma = octeon_gen1_phys_to_dma;\r\n_octeon_pci_dma_map_ops.dma_to_phys = octeon_gen1_dma_to_phys;\r\nbreak;\r\ncase OCTEON_DMA_BAR_TYPE_BIG:\r\n_octeon_pci_dma_map_ops.phys_to_dma = octeon_big_phys_to_dma;\r\n_octeon_pci_dma_map_ops.dma_to_phys = octeon_big_dma_to_phys;\r\nbreak;\r\ncase OCTEON_DMA_BAR_TYPE_SMALL:\r\n_octeon_pci_dma_map_ops.phys_to_dma = octeon_small_phys_to_dma;\r\n_octeon_pci_dma_map_ops.dma_to_phys = octeon_small_dma_to_phys;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nocteon_pci_dma_map_ops = &_octeon_pci_dma_map_ops.dma_map_ops;\r\n}
