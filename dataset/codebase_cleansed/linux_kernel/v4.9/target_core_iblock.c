static inline struct iblock_dev *IBLOCK_DEV(struct se_device *dev)\r\n{\r\nreturn container_of(dev, struct iblock_dev, dev);\r\n}\r\nstatic int iblock_attach_hba(struct se_hba *hba, u32 host_id)\r\n{\r\npr_debug("CORE_HBA[%d] - TCM iBlock HBA Driver %s on"\r\n" Generic Target Core Stack %s\n", hba->hba_id,\r\nIBLOCK_VERSION, TARGET_CORE_VERSION);\r\nreturn 0;\r\n}\r\nstatic void iblock_detach_hba(struct se_hba *hba)\r\n{\r\n}\r\nstatic struct se_device *iblock_alloc_device(struct se_hba *hba, const char *name)\r\n{\r\nstruct iblock_dev *ib_dev = NULL;\r\nib_dev = kzalloc(sizeof(struct iblock_dev), GFP_KERNEL);\r\nif (!ib_dev) {\r\npr_err("Unable to allocate struct iblock_dev\n");\r\nreturn NULL;\r\n}\r\npr_debug( "IBLOCK: Allocated ib_dev for %s\n", name);\r\nreturn &ib_dev->dev;\r\n}\r\nstatic int iblock_configure_device(struct se_device *dev)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct request_queue *q;\r\nstruct block_device *bd = NULL;\r\nstruct blk_integrity *bi;\r\nfmode_t mode;\r\nint ret = -ENOMEM;\r\nif (!(ib_dev->ibd_flags & IBDF_HAS_UDEV_PATH)) {\r\npr_err("Missing udev_path= parameters for IBLOCK\n");\r\nreturn -EINVAL;\r\n}\r\nib_dev->ibd_bio_set = bioset_create(IBLOCK_BIO_POOL_SIZE, 0);\r\nif (!ib_dev->ibd_bio_set) {\r\npr_err("IBLOCK: Unable to create bioset\n");\r\ngoto out;\r\n}\r\npr_debug( "IBLOCK: Claiming struct block_device: %s\n",\r\nib_dev->ibd_udev_path);\r\nmode = FMODE_READ|FMODE_EXCL;\r\nif (!ib_dev->ibd_readonly)\r\nmode |= FMODE_WRITE;\r\nelse\r\ndev->dev_flags |= DF_READ_ONLY;\r\nbd = blkdev_get_by_path(ib_dev->ibd_udev_path, mode, ib_dev);\r\nif (IS_ERR(bd)) {\r\nret = PTR_ERR(bd);\r\ngoto out_free_bioset;\r\n}\r\nib_dev->ibd_bd = bd;\r\nq = bdev_get_queue(bd);\r\ndev->dev_attrib.hw_block_size = bdev_logical_block_size(bd);\r\ndev->dev_attrib.hw_max_sectors = queue_max_hw_sectors(q);\r\ndev->dev_attrib.hw_queue_depth = q->nr_requests;\r\nif (target_configure_unmap_from_queue(&dev->dev_attrib, q))\r\npr_debug("IBLOCK: BLOCK Discard support available,"\r\n" disabled by default\n");\r\ndev->dev_attrib.max_write_same_len = 0xFFFF;\r\nif (blk_queue_nonrot(q))\r\ndev->dev_attrib.is_nonrot = 1;\r\nbi = bdev_get_integrity(bd);\r\nif (bi) {\r\nstruct bio_set *bs = ib_dev->ibd_bio_set;\r\nif (!strcmp(bi->profile->name, "T10-DIF-TYPE3-IP") ||\r\n!strcmp(bi->profile->name, "T10-DIF-TYPE1-IP")) {\r\npr_err("IBLOCK export of blk_integrity: %s not"\r\n" supported\n", bi->profile->name);\r\nret = -ENOSYS;\r\ngoto out_blkdev_put;\r\n}\r\nif (!strcmp(bi->profile->name, "T10-DIF-TYPE3-CRC")) {\r\ndev->dev_attrib.pi_prot_type = TARGET_DIF_TYPE3_PROT;\r\n} else if (!strcmp(bi->profile->name, "T10-DIF-TYPE1-CRC")) {\r\ndev->dev_attrib.pi_prot_type = TARGET_DIF_TYPE1_PROT;\r\n}\r\nif (dev->dev_attrib.pi_prot_type) {\r\nif (bioset_integrity_create(bs, IBLOCK_BIO_POOL_SIZE) < 0) {\r\npr_err("Unable to allocate bioset for PI\n");\r\nret = -ENOMEM;\r\ngoto out_blkdev_put;\r\n}\r\npr_debug("IBLOCK setup BIP bs->bio_integrity_pool: %p\n",\r\nbs->bio_integrity_pool);\r\n}\r\ndev->dev_attrib.hw_pi_prot_type = dev->dev_attrib.pi_prot_type;\r\n}\r\nreturn 0;\r\nout_blkdev_put:\r\nblkdev_put(ib_dev->ibd_bd, FMODE_WRITE|FMODE_READ|FMODE_EXCL);\r\nout_free_bioset:\r\nbioset_free(ib_dev->ibd_bio_set);\r\nib_dev->ibd_bio_set = NULL;\r\nout:\r\nreturn ret;\r\n}\r\nstatic void iblock_dev_call_rcu(struct rcu_head *p)\r\n{\r\nstruct se_device *dev = container_of(p, struct se_device, rcu_head);\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nkfree(ib_dev);\r\n}\r\nstatic void iblock_free_device(struct se_device *dev)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nif (ib_dev->ibd_bd != NULL)\r\nblkdev_put(ib_dev->ibd_bd, FMODE_WRITE|FMODE_READ|FMODE_EXCL);\r\nif (ib_dev->ibd_bio_set != NULL)\r\nbioset_free(ib_dev->ibd_bio_set);\r\ncall_rcu(&dev->rcu_head, iblock_dev_call_rcu);\r\n}\r\nstatic unsigned long long iblock_emulate_read_cap_with_block_size(\r\nstruct se_device *dev,\r\nstruct block_device *bd,\r\nstruct request_queue *q)\r\n{\r\nunsigned long long blocks_long = (div_u64(i_size_read(bd->bd_inode),\r\nbdev_logical_block_size(bd)) - 1);\r\nu32 block_size = bdev_logical_block_size(bd);\r\nif (block_size == dev->dev_attrib.block_size)\r\nreturn blocks_long;\r\nswitch (block_size) {\r\ncase 4096:\r\nswitch (dev->dev_attrib.block_size) {\r\ncase 2048:\r\nblocks_long <<= 1;\r\nbreak;\r\ncase 1024:\r\nblocks_long <<= 2;\r\nbreak;\r\ncase 512:\r\nblocks_long <<= 3;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ncase 2048:\r\nswitch (dev->dev_attrib.block_size) {\r\ncase 4096:\r\nblocks_long >>= 1;\r\nbreak;\r\ncase 1024:\r\nblocks_long <<= 1;\r\nbreak;\r\ncase 512:\r\nblocks_long <<= 2;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ncase 1024:\r\nswitch (dev->dev_attrib.block_size) {\r\ncase 4096:\r\nblocks_long >>= 2;\r\nbreak;\r\ncase 2048:\r\nblocks_long >>= 1;\r\nbreak;\r\ncase 512:\r\nblocks_long <<= 1;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ncase 512:\r\nswitch (dev->dev_attrib.block_size) {\r\ncase 4096:\r\nblocks_long >>= 3;\r\nbreak;\r\ncase 2048:\r\nblocks_long >>= 2;\r\nbreak;\r\ncase 1024:\r\nblocks_long >>= 1;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn blocks_long;\r\n}\r\nstatic void iblock_complete_cmd(struct se_cmd *cmd)\r\n{\r\nstruct iblock_req *ibr = cmd->priv;\r\nu8 status;\r\nif (!atomic_dec_and_test(&ibr->pending))\r\nreturn;\r\nif (atomic_read(&ibr->ib_bio_err_cnt))\r\nstatus = SAM_STAT_CHECK_CONDITION;\r\nelse\r\nstatus = SAM_STAT_GOOD;\r\ntarget_complete_cmd(cmd, status);\r\nkfree(ibr);\r\n}\r\nstatic void iblock_bio_done(struct bio *bio)\r\n{\r\nstruct se_cmd *cmd = bio->bi_private;\r\nstruct iblock_req *ibr = cmd->priv;\r\nif (bio->bi_error) {\r\npr_err("bio error: %p, err: %d\n", bio, bio->bi_error);\r\natomic_inc(&ibr->ib_bio_err_cnt);\r\nsmp_mb__after_atomic();\r\n}\r\nbio_put(bio);\r\niblock_complete_cmd(cmd);\r\n}\r\nstatic struct bio *\r\niblock_get_bio(struct se_cmd *cmd, sector_t lba, u32 sg_num, int op,\r\nint op_flags)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(cmd->se_dev);\r\nstruct bio *bio;\r\nif (sg_num > BIO_MAX_PAGES)\r\nsg_num = BIO_MAX_PAGES;\r\nbio = bio_alloc_bioset(GFP_NOIO, sg_num, ib_dev->ibd_bio_set);\r\nif (!bio) {\r\npr_err("Unable to allocate memory for bio\n");\r\nreturn NULL;\r\n}\r\nbio->bi_bdev = ib_dev->ibd_bd;\r\nbio->bi_private = cmd;\r\nbio->bi_end_io = &iblock_bio_done;\r\nbio->bi_iter.bi_sector = lba;\r\nbio_set_op_attrs(bio, op, op_flags);\r\nreturn bio;\r\n}\r\nstatic void iblock_submit_bios(struct bio_list *list)\r\n{\r\nstruct blk_plug plug;\r\nstruct bio *bio;\r\nblk_start_plug(&plug);\r\nwhile ((bio = bio_list_pop(list)))\r\nsubmit_bio(bio);\r\nblk_finish_plug(&plug);\r\n}\r\nstatic void iblock_end_io_flush(struct bio *bio)\r\n{\r\nstruct se_cmd *cmd = bio->bi_private;\r\nif (bio->bi_error)\r\npr_err("IBLOCK: cache flush failed: %d\n", bio->bi_error);\r\nif (cmd) {\r\nif (bio->bi_error)\r\ntarget_complete_cmd(cmd, SAM_STAT_CHECK_CONDITION);\r\nelse\r\ntarget_complete_cmd(cmd, SAM_STAT_GOOD);\r\n}\r\nbio_put(bio);\r\n}\r\nstatic sense_reason_t\r\niblock_execute_sync_cache(struct se_cmd *cmd)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(cmd->se_dev);\r\nint immed = (cmd->t_task_cdb[1] & 0x2);\r\nstruct bio *bio;\r\nif (immed)\r\ntarget_complete_cmd(cmd, SAM_STAT_GOOD);\r\nbio = bio_alloc(GFP_KERNEL, 0);\r\nbio->bi_end_io = iblock_end_io_flush;\r\nbio->bi_bdev = ib_dev->ibd_bd;\r\nbio_set_op_attrs(bio, REQ_OP_WRITE, WRITE_FLUSH);\r\nif (!immed)\r\nbio->bi_private = cmd;\r\nsubmit_bio(bio);\r\nreturn 0;\r\n}\r\nstatic sense_reason_t\r\niblock_execute_unmap(struct se_cmd *cmd, sector_t lba, sector_t nolb)\r\n{\r\nstruct block_device *bdev = IBLOCK_DEV(cmd->se_dev)->ibd_bd;\r\nstruct se_device *dev = cmd->se_dev;\r\nint ret;\r\nret = blkdev_issue_discard(bdev,\r\ntarget_to_linux_sector(dev, lba),\r\ntarget_to_linux_sector(dev, nolb),\r\nGFP_KERNEL, 0);\r\nif (ret < 0) {\r\npr_err("blkdev_issue_discard() failed: %d\n", ret);\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\n}\r\nreturn 0;\r\n}\r\nstatic sense_reason_t\r\niblock_execute_write_same_direct(struct block_device *bdev, struct se_cmd *cmd)\r\n{\r\nstruct se_device *dev = cmd->se_dev;\r\nstruct scatterlist *sg = &cmd->t_data_sg[0];\r\nstruct page *page = NULL;\r\nint ret;\r\nif (sg->offset) {\r\npage = alloc_page(GFP_KERNEL);\r\nif (!page)\r\nreturn TCM_OUT_OF_RESOURCES;\r\nsg_copy_to_buffer(sg, cmd->t_data_nents, page_address(page),\r\ndev->dev_attrib.block_size);\r\n}\r\nret = blkdev_issue_write_same(bdev,\r\ntarget_to_linux_sector(dev, cmd->t_task_lba),\r\ntarget_to_linux_sector(dev,\r\nsbc_get_write_same_sectors(cmd)),\r\nGFP_KERNEL, page ? page : sg_page(sg));\r\nif (page)\r\n__free_page(page);\r\nif (ret)\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\ntarget_complete_cmd(cmd, GOOD);\r\nreturn 0;\r\n}\r\nstatic sense_reason_t\r\niblock_execute_write_same(struct se_cmd *cmd)\r\n{\r\nstruct block_device *bdev = IBLOCK_DEV(cmd->se_dev)->ibd_bd;\r\nstruct iblock_req *ibr;\r\nstruct scatterlist *sg;\r\nstruct bio *bio;\r\nstruct bio_list list;\r\nstruct se_device *dev = cmd->se_dev;\r\nsector_t block_lba = target_to_linux_sector(dev, cmd->t_task_lba);\r\nsector_t sectors = target_to_linux_sector(dev,\r\nsbc_get_write_same_sectors(cmd));\r\nif (cmd->prot_op) {\r\npr_err("WRITE_SAME: Protection information with IBLOCK"\r\n" backends not supported\n");\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\n}\r\nsg = &cmd->t_data_sg[0];\r\nif (cmd->t_data_nents > 1 ||\r\nsg->length != cmd->se_dev->dev_attrib.block_size) {\r\npr_err("WRITE_SAME: Illegal SGL t_data_nents: %u length: %u"\r\n" block_size: %u\n", cmd->t_data_nents, sg->length,\r\ncmd->se_dev->dev_attrib.block_size);\r\nreturn TCM_INVALID_CDB_FIELD;\r\n}\r\nif (bdev_write_same(bdev))\r\nreturn iblock_execute_write_same_direct(bdev, cmd);\r\nibr = kzalloc(sizeof(struct iblock_req), GFP_KERNEL);\r\nif (!ibr)\r\ngoto fail;\r\ncmd->priv = ibr;\r\nbio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE, 0);\r\nif (!bio)\r\ngoto fail_free_ibr;\r\nbio_list_init(&list);\r\nbio_list_add(&list, bio);\r\natomic_set(&ibr->pending, 1);\r\nwhile (sectors) {\r\nwhile (bio_add_page(bio, sg_page(sg), sg->length, sg->offset)\r\n!= sg->length) {\r\nbio = iblock_get_bio(cmd, block_lba, 1, REQ_OP_WRITE,\r\n0);\r\nif (!bio)\r\ngoto fail_put_bios;\r\natomic_inc(&ibr->pending);\r\nbio_list_add(&list, bio);\r\n}\r\nblock_lba += sg->length >> IBLOCK_LBA_SHIFT;\r\nsectors -= 1;\r\n}\r\niblock_submit_bios(&list);\r\nreturn 0;\r\nfail_put_bios:\r\nwhile ((bio = bio_list_pop(&list)))\r\nbio_put(bio);\r\nfail_free_ibr:\r\nkfree(ibr);\r\nfail:\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\n}\r\nstatic ssize_t iblock_set_configfs_dev_params(struct se_device *dev,\r\nconst char *page, ssize_t count)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nchar *orig, *ptr, *arg_p, *opts;\r\nsubstring_t args[MAX_OPT_ARGS];\r\nint ret = 0, token;\r\nunsigned long tmp_readonly;\r\nopts = kstrdup(page, GFP_KERNEL);\r\nif (!opts)\r\nreturn -ENOMEM;\r\norig = opts;\r\nwhile ((ptr = strsep(&opts, ",\n")) != NULL) {\r\nif (!*ptr)\r\ncontinue;\r\ntoken = match_token(ptr, tokens, args);\r\nswitch (token) {\r\ncase Opt_udev_path:\r\nif (ib_dev->ibd_bd) {\r\npr_err("Unable to set udev_path= while"\r\n" ib_dev->ibd_bd exists\n");\r\nret = -EEXIST;\r\ngoto out;\r\n}\r\nif (match_strlcpy(ib_dev->ibd_udev_path, &args[0],\r\nSE_UDEV_PATH_LEN) == 0) {\r\nret = -EINVAL;\r\nbreak;\r\n}\r\npr_debug("IBLOCK: Referencing UDEV path: %s\n",\r\nib_dev->ibd_udev_path);\r\nib_dev->ibd_flags |= IBDF_HAS_UDEV_PATH;\r\nbreak;\r\ncase Opt_readonly:\r\narg_p = match_strdup(&args[0]);\r\nif (!arg_p) {\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\nret = kstrtoul(arg_p, 0, &tmp_readonly);\r\nkfree(arg_p);\r\nif (ret < 0) {\r\npr_err("kstrtoul() failed for"\r\n" readonly=\n");\r\ngoto out;\r\n}\r\nib_dev->ibd_readonly = tmp_readonly;\r\npr_debug("IBLOCK: readonly: %d\n", ib_dev->ibd_readonly);\r\nbreak;\r\ncase Opt_force:\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nout:\r\nkfree(orig);\r\nreturn (!ret) ? count : ret;\r\n}\r\nstatic ssize_t iblock_show_configfs_dev_params(struct se_device *dev, char *b)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct block_device *bd = ib_dev->ibd_bd;\r\nchar buf[BDEVNAME_SIZE];\r\nssize_t bl = 0;\r\nif (bd)\r\nbl += sprintf(b + bl, "iBlock device: %s",\r\nbdevname(bd, buf));\r\nif (ib_dev->ibd_flags & IBDF_HAS_UDEV_PATH)\r\nbl += sprintf(b + bl, " UDEV PATH: %s",\r\nib_dev->ibd_udev_path);\r\nbl += sprintf(b + bl, " readonly: %d\n", ib_dev->ibd_readonly);\r\nbl += sprintf(b + bl, " ");\r\nif (bd) {\r\nbl += sprintf(b + bl, "Major: %d Minor: %d %s\n",\r\nMAJOR(bd->bd_dev), MINOR(bd->bd_dev), (!bd->bd_contains) ?\r\n"" : (bd->bd_holder == ib_dev) ?\r\n"CLAIMED: IBLOCK" : "CLAIMED: OS");\r\n} else {\r\nbl += sprintf(b + bl, "Major: 0 Minor: 0\n");\r\n}\r\nreturn bl;\r\n}\r\nstatic int\r\niblock_alloc_bip(struct se_cmd *cmd, struct bio *bio)\r\n{\r\nstruct se_device *dev = cmd->se_dev;\r\nstruct blk_integrity *bi;\r\nstruct bio_integrity_payload *bip;\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct scatterlist *sg;\r\nint i, rc;\r\nbi = bdev_get_integrity(ib_dev->ibd_bd);\r\nif (!bi) {\r\npr_err("Unable to locate bio_integrity\n");\r\nreturn -ENODEV;\r\n}\r\nbip = bio_integrity_alloc(bio, GFP_NOIO, cmd->t_prot_nents);\r\nif (IS_ERR(bip)) {\r\npr_err("Unable to allocate bio_integrity_payload\n");\r\nreturn PTR_ERR(bip);\r\n}\r\nbip->bip_iter.bi_size = (cmd->data_length / dev->dev_attrib.block_size) *\r\ndev->prot_length;\r\nbip->bip_iter.bi_sector = bio->bi_iter.bi_sector;\r\npr_debug("IBLOCK BIP Size: %u Sector: %llu\n", bip->bip_iter.bi_size,\r\n(unsigned long long)bip->bip_iter.bi_sector);\r\nfor_each_sg(cmd->t_prot_sg, sg, cmd->t_prot_nents, i) {\r\nrc = bio_integrity_add_page(bio, sg_page(sg), sg->length,\r\nsg->offset);\r\nif (rc != sg->length) {\r\npr_err("bio_integrity_add_page() failed; %d\n", rc);\r\nreturn -ENOMEM;\r\n}\r\npr_debug("Added bio integrity page: %p length: %d offset; %d\n",\r\nsg_page(sg), sg->length, sg->offset);\r\n}\r\nreturn 0;\r\n}\r\nstatic sense_reason_t\r\niblock_execute_rw(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,\r\nenum dma_data_direction data_direction)\r\n{\r\nstruct se_device *dev = cmd->se_dev;\r\nsector_t block_lba = target_to_linux_sector(dev, cmd->t_task_lba);\r\nstruct iblock_req *ibr;\r\nstruct bio *bio, *bio_start;\r\nstruct bio_list list;\r\nstruct scatterlist *sg;\r\nu32 sg_num = sgl_nents;\r\nunsigned bio_cnt;\r\nint i, op, op_flags = 0;\r\nif (data_direction == DMA_TO_DEVICE) {\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct request_queue *q = bdev_get_queue(ib_dev->ibd_bd);\r\nop = REQ_OP_WRITE;\r\nif (test_bit(QUEUE_FLAG_FUA, &q->queue_flags)) {\r\nif (cmd->se_cmd_flags & SCF_FUA)\r\nop_flags = WRITE_FUA;\r\nelse if (!test_bit(QUEUE_FLAG_WC, &q->queue_flags))\r\nop_flags = WRITE_FUA;\r\n}\r\n} else {\r\nop = REQ_OP_READ;\r\n}\r\nibr = kzalloc(sizeof(struct iblock_req), GFP_KERNEL);\r\nif (!ibr)\r\ngoto fail;\r\ncmd->priv = ibr;\r\nif (!sgl_nents) {\r\natomic_set(&ibr->pending, 1);\r\niblock_complete_cmd(cmd);\r\nreturn 0;\r\n}\r\nbio = iblock_get_bio(cmd, block_lba, sgl_nents, op, op_flags);\r\nif (!bio)\r\ngoto fail_free_ibr;\r\nbio_start = bio;\r\nbio_list_init(&list);\r\nbio_list_add(&list, bio);\r\natomic_set(&ibr->pending, 2);\r\nbio_cnt = 1;\r\nfor_each_sg(sgl, sg, sgl_nents, i) {\r\nwhile (bio_add_page(bio, sg_page(sg), sg->length, sg->offset)\r\n!= sg->length) {\r\nif (bio_cnt >= IBLOCK_MAX_BIO_PER_TASK) {\r\niblock_submit_bios(&list);\r\nbio_cnt = 0;\r\n}\r\nbio = iblock_get_bio(cmd, block_lba, sg_num, op,\r\nop_flags);\r\nif (!bio)\r\ngoto fail_put_bios;\r\natomic_inc(&ibr->pending);\r\nbio_list_add(&list, bio);\r\nbio_cnt++;\r\n}\r\nblock_lba += sg->length >> IBLOCK_LBA_SHIFT;\r\nsg_num--;\r\n}\r\nif (cmd->prot_type && dev->dev_attrib.pi_prot_type) {\r\nint rc = iblock_alloc_bip(cmd, bio_start);\r\nif (rc)\r\ngoto fail_put_bios;\r\n}\r\niblock_submit_bios(&list);\r\niblock_complete_cmd(cmd);\r\nreturn 0;\r\nfail_put_bios:\r\nwhile ((bio = bio_list_pop(&list)))\r\nbio_put(bio);\r\nfail_free_ibr:\r\nkfree(ibr);\r\nfail:\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\n}\r\nstatic sector_t iblock_get_blocks(struct se_device *dev)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct block_device *bd = ib_dev->ibd_bd;\r\nstruct request_queue *q = bdev_get_queue(bd);\r\nreturn iblock_emulate_read_cap_with_block_size(dev, bd, q);\r\n}\r\nstatic sector_t iblock_get_alignment_offset_lbas(struct se_device *dev)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct block_device *bd = ib_dev->ibd_bd;\r\nint ret;\r\nret = bdev_alignment_offset(bd);\r\nif (ret == -1)\r\nreturn 0;\r\nreturn ret / bdev_logical_block_size(bd);\r\n}\r\nstatic unsigned int iblock_get_lbppbe(struct se_device *dev)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct block_device *bd = ib_dev->ibd_bd;\r\nint logs_per_phys = bdev_physical_block_size(bd) / bdev_logical_block_size(bd);\r\nreturn ilog2(logs_per_phys);\r\n}\r\nstatic unsigned int iblock_get_io_min(struct se_device *dev)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct block_device *bd = ib_dev->ibd_bd;\r\nreturn bdev_io_min(bd);\r\n}\r\nstatic unsigned int iblock_get_io_opt(struct se_device *dev)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct block_device *bd = ib_dev->ibd_bd;\r\nreturn bdev_io_opt(bd);\r\n}\r\nstatic sense_reason_t\r\niblock_parse_cdb(struct se_cmd *cmd)\r\n{\r\nreturn sbc_parse_cdb(cmd, &iblock_sbc_ops);\r\n}\r\nstatic bool iblock_get_write_cache(struct se_device *dev)\r\n{\r\nstruct iblock_dev *ib_dev = IBLOCK_DEV(dev);\r\nstruct block_device *bd = ib_dev->ibd_bd;\r\nstruct request_queue *q = bdev_get_queue(bd);\r\nreturn test_bit(QUEUE_FLAG_WC, &q->queue_flags);\r\n}\r\nstatic int __init iblock_module_init(void)\r\n{\r\nreturn transport_backend_register(&iblock_ops);\r\n}\r\nstatic void __exit iblock_module_exit(void)\r\n{\r\ntarget_backend_unregister(&iblock_ops);\r\n}
