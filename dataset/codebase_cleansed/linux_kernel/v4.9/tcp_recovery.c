int tcp_rack_mark_lost(struct sock *sk)\r\n{\r\nstruct tcp_sock *tp = tcp_sk(sk);\r\nstruct sk_buff *skb;\r\nu32 reo_wnd, prior_retrans = tp->retrans_out;\r\nif (inet_csk(sk)->icsk_ca_state < TCP_CA_Recovery || !tp->rack.advanced)\r\nreturn 0;\r\ntp->rack.advanced = 0;\r\nreo_wnd = 1000;\r\nif (tp->rack.reord && tcp_min_rtt(tp) != ~0U)\r\nreo_wnd = max(tcp_min_rtt(tp) >> 2, reo_wnd);\r\ntcp_for_write_queue(skb, sk) {\r\nstruct tcp_skb_cb *scb = TCP_SKB_CB(skb);\r\nif (skb == tcp_send_head(sk))\r\nbreak;\r\nif (!after(scb->end_seq, tp->snd_una) ||\r\nscb->sacked & TCPCB_SACKED_ACKED)\r\ncontinue;\r\nif (skb_mstamp_after(&tp->rack.mstamp, &skb->skb_mstamp)) {\r\nif (skb_mstamp_us_delta(&tp->rack.mstamp,\r\n&skb->skb_mstamp) <= reo_wnd)\r\ncontinue;\r\ntcp_skb_mark_lost_uncond_verify(tp, skb);\r\nif (scb->sacked & TCPCB_SACKED_RETRANS) {\r\nscb->sacked &= ~TCPCB_SACKED_RETRANS;\r\ntp->retrans_out -= tcp_skb_pcount(skb);\r\nNET_INC_STATS(sock_net(sk),\r\nLINUX_MIB_TCPLOSTRETRANSMIT);\r\n}\r\n} else if (!(scb->sacked & TCPCB_RETRANS)) {\r\nbreak;\r\n}\r\n}\r\nreturn prior_retrans - tp->retrans_out;\r\n}\r\nvoid tcp_rack_advance(struct tcp_sock *tp,\r\nconst struct skb_mstamp *xmit_time, u8 sacked)\r\n{\r\nif (tp->rack.mstamp.v64 &&\r\n!skb_mstamp_after(xmit_time, &tp->rack.mstamp))\r\nreturn;\r\nif (sacked & TCPCB_RETRANS) {\r\nstruct skb_mstamp now;\r\nskb_mstamp_get(&now);\r\nif (skb_mstamp_us_delta(&now, xmit_time) < tcp_min_rtt(tp))\r\nreturn;\r\n}\r\ntp->rack.mstamp = *xmit_time;\r\ntp->rack.advanced = 1;\r\n}
