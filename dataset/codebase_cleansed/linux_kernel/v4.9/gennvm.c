static struct nvm_target *gen_find_target(struct gen_dev *gn, const char *name)\r\n{\r\nstruct nvm_target *tgt;\r\nlist_for_each_entry(tgt, &gn->targets, list)\r\nif (!strcmp(name, tgt->disk->disk_name))\r\nreturn tgt;\r\nreturn NULL;\r\n}\r\nstatic int gen_create_tgt(struct nvm_dev *dev, struct nvm_ioctl_create *create)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nstruct nvm_ioctl_create_simple *s = &create->conf.s;\r\nstruct request_queue *tqueue;\r\nstruct gendisk *tdisk;\r\nstruct nvm_tgt_type *tt;\r\nstruct nvm_target *t;\r\nvoid *targetdata;\r\ntt = nvm_find_target_type(create->tgttype, 1);\r\nif (!tt) {\r\npr_err("nvm: target type %s not found\n", create->tgttype);\r\nreturn -EINVAL;\r\n}\r\nmutex_lock(&gn->lock);\r\nt = gen_find_target(gn, create->tgtname);\r\nif (t) {\r\npr_err("nvm: target name already exists.\n");\r\nmutex_unlock(&gn->lock);\r\nreturn -EINVAL;\r\n}\r\nmutex_unlock(&gn->lock);\r\nt = kmalloc(sizeof(struct nvm_target), GFP_KERNEL);\r\nif (!t)\r\nreturn -ENOMEM;\r\ntqueue = blk_alloc_queue_node(GFP_KERNEL, dev->q->node);\r\nif (!tqueue)\r\ngoto err_t;\r\nblk_queue_make_request(tqueue, tt->make_rq);\r\ntdisk = alloc_disk(0);\r\nif (!tdisk)\r\ngoto err_queue;\r\nsprintf(tdisk->disk_name, "%s", create->tgtname);\r\ntdisk->flags = GENHD_FL_EXT_DEVT;\r\ntdisk->major = 0;\r\ntdisk->first_minor = 0;\r\ntdisk->fops = &gen_fops;\r\ntdisk->queue = tqueue;\r\ntargetdata = tt->init(dev, tdisk, s->lun_begin, s->lun_end);\r\nif (IS_ERR(targetdata))\r\ngoto err_init;\r\ntdisk->private_data = targetdata;\r\ntqueue->queuedata = targetdata;\r\nblk_queue_max_hw_sectors(tqueue, 8 * dev->ops->max_phys_sect);\r\nset_capacity(tdisk, tt->capacity(targetdata));\r\nadd_disk(tdisk);\r\nt->type = tt;\r\nt->disk = tdisk;\r\nt->dev = dev;\r\nmutex_lock(&gn->lock);\r\nlist_add_tail(&t->list, &gn->targets);\r\nmutex_unlock(&gn->lock);\r\nreturn 0;\r\nerr_init:\r\nput_disk(tdisk);\r\nerr_queue:\r\nblk_cleanup_queue(tqueue);\r\nerr_t:\r\nkfree(t);\r\nreturn -ENOMEM;\r\n}\r\nstatic void __gen_remove_target(struct nvm_target *t)\r\n{\r\nstruct nvm_tgt_type *tt = t->type;\r\nstruct gendisk *tdisk = t->disk;\r\nstruct request_queue *q = tdisk->queue;\r\ndel_gendisk(tdisk);\r\nblk_cleanup_queue(q);\r\nif (tt->exit)\r\ntt->exit(tdisk->private_data);\r\nput_disk(tdisk);\r\nlist_del(&t->list);\r\nkfree(t);\r\n}\r\nstatic int gen_remove_tgt(struct nvm_dev *dev, struct nvm_ioctl_remove *remove)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nstruct nvm_target *t;\r\nif (!gn)\r\nreturn 1;\r\nmutex_lock(&gn->lock);\r\nt = gen_find_target(gn, remove->tgtname);\r\nif (!t) {\r\nmutex_unlock(&gn->lock);\r\nreturn 1;\r\n}\r\n__gen_remove_target(t);\r\nmutex_unlock(&gn->lock);\r\nreturn 0;\r\n}\r\nstatic int gen_get_area(struct nvm_dev *dev, sector_t *lba, sector_t len)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nstruct gen_area *area, *prev, *next;\r\nsector_t begin = 0;\r\nsector_t max_sectors = (dev->sec_size * dev->total_secs) >> 9;\r\nif (len > max_sectors)\r\nreturn -EINVAL;\r\narea = kmalloc(sizeof(struct gen_area), GFP_KERNEL);\r\nif (!area)\r\nreturn -ENOMEM;\r\nprev = NULL;\r\nspin_lock(&dev->lock);\r\nlist_for_each_entry(next, &gn->area_list, list) {\r\nif (begin + len > next->begin) {\r\nbegin = next->end;\r\nprev = next;\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\nif ((begin + len) > max_sectors) {\r\nspin_unlock(&dev->lock);\r\nkfree(area);\r\nreturn -EINVAL;\r\n}\r\narea->begin = *lba = begin;\r\narea->end = begin + len;\r\nif (prev)\r\nlist_add(&area->list, &prev->list);\r\nelse\r\nlist_add(&area->list, &gn->area_list);\r\nspin_unlock(&dev->lock);\r\nreturn 0;\r\n}\r\nstatic void gen_put_area(struct nvm_dev *dev, sector_t begin)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nstruct gen_area *area;\r\nspin_lock(&dev->lock);\r\nlist_for_each_entry(area, &gn->area_list, list) {\r\nif (area->begin != begin)\r\ncontinue;\r\nlist_del(&area->list);\r\nspin_unlock(&dev->lock);\r\nkfree(area);\r\nreturn;\r\n}\r\nspin_unlock(&dev->lock);\r\n}\r\nstatic void gen_blocks_free(struct nvm_dev *dev)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nstruct gen_lun *lun;\r\nint i;\r\ngen_for_each_lun(gn, lun, i) {\r\nif (!lun->vlun.blocks)\r\nbreak;\r\nvfree(lun->vlun.blocks);\r\n}\r\n}\r\nstatic void gen_luns_free(struct nvm_dev *dev)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nkfree(gn->luns);\r\n}\r\nstatic int gen_luns_init(struct nvm_dev *dev, struct gen_dev *gn)\r\n{\r\nstruct gen_lun *lun;\r\nint i;\r\ngn->luns = kcalloc(dev->nr_luns, sizeof(struct gen_lun), GFP_KERNEL);\r\nif (!gn->luns)\r\nreturn -ENOMEM;\r\ngen_for_each_lun(gn, lun, i) {\r\nspin_lock_init(&lun->vlun.lock);\r\nINIT_LIST_HEAD(&lun->free_list);\r\nINIT_LIST_HEAD(&lun->used_list);\r\nINIT_LIST_HEAD(&lun->bb_list);\r\nlun->reserved_blocks = 2;\r\nlun->vlun.id = i;\r\nlun->vlun.lun_id = i % dev->luns_per_chnl;\r\nlun->vlun.chnl_id = i / dev->luns_per_chnl;\r\nlun->vlun.nr_free_blocks = dev->blks_per_lun;\r\n}\r\nreturn 0;\r\n}\r\nstatic int gen_block_bb(struct gen_dev *gn, struct ppa_addr ppa,\r\nu8 *blks, int nr_blks)\r\n{\r\nstruct nvm_dev *dev = gn->dev;\r\nstruct gen_lun *lun;\r\nstruct nvm_block *blk;\r\nint i;\r\nnr_blks = nvm_bb_tbl_fold(dev, blks, nr_blks);\r\nif (nr_blks < 0)\r\nreturn nr_blks;\r\nlun = &gn->luns[(dev->luns_per_chnl * ppa.g.ch) + ppa.g.lun];\r\nfor (i = 0; i < nr_blks; i++) {\r\nif (blks[i] == 0)\r\ncontinue;\r\nblk = &lun->vlun.blocks[i];\r\nlist_move_tail(&blk->list, &lun->bb_list);\r\nlun->vlun.nr_free_blocks--;\r\n}\r\nreturn 0;\r\n}\r\nstatic int gen_block_map(u64 slba, u32 nlb, __le64 *entries, void *private)\r\n{\r\nstruct nvm_dev *dev = private;\r\nstruct gen_dev *gn = dev->mp;\r\nu64 elba = slba + nlb;\r\nstruct gen_lun *lun;\r\nstruct nvm_block *blk;\r\nu64 i;\r\nint lun_id;\r\nif (unlikely(elba > dev->total_secs)) {\r\npr_err("gen: L2P data from device is out of bounds!\n");\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < nlb; i++) {\r\nu64 pba = le64_to_cpu(entries[i]);\r\nif (unlikely(pba >= dev->total_secs && pba != U64_MAX)) {\r\npr_err("gen: L2P data entry is out of bounds!\n");\r\nreturn -EINVAL;\r\n}\r\nif (!pba)\r\ncontinue;\r\nlun_id = div_u64(pba, dev->sec_per_lun);\r\nlun = &gn->luns[lun_id];\r\npba = pba - (dev->sec_per_lun * lun_id);\r\nblk = &lun->vlun.blocks[div_u64(pba, dev->sec_per_blk)];\r\nif (!blk->state) {\r\nlist_move_tail(&blk->list, &lun->used_list);\r\nblk->state = NVM_BLK_ST_TGT;\r\nlun->vlun.nr_free_blocks--;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int gen_blocks_init(struct nvm_dev *dev, struct gen_dev *gn)\r\n{\r\nstruct gen_lun *lun;\r\nstruct nvm_block *block;\r\nsector_t lun_iter, blk_iter, cur_block_id = 0;\r\nint ret, nr_blks;\r\nu8 *blks;\r\nnr_blks = dev->blks_per_lun * dev->plane_mode;\r\nblks = kmalloc(nr_blks, GFP_KERNEL);\r\nif (!blks)\r\nreturn -ENOMEM;\r\ngen_for_each_lun(gn, lun, lun_iter) {\r\nlun->vlun.blocks = vzalloc(sizeof(struct nvm_block) *\r\ndev->blks_per_lun);\r\nif (!lun->vlun.blocks) {\r\nkfree(blks);\r\nreturn -ENOMEM;\r\n}\r\nfor (blk_iter = 0; blk_iter < dev->blks_per_lun; blk_iter++) {\r\nblock = &lun->vlun.blocks[blk_iter];\r\nINIT_LIST_HEAD(&block->list);\r\nblock->lun = &lun->vlun;\r\nblock->id = cur_block_id++;\r\nif (unlikely(lun_iter == 0 && blk_iter == 0)) {\r\nlun->vlun.nr_free_blocks--;\r\ncontinue;\r\n}\r\nlist_add_tail(&block->list, &lun->free_list);\r\n}\r\nif (dev->ops->get_bb_tbl) {\r\nstruct ppa_addr ppa;\r\nppa.ppa = 0;\r\nppa.g.ch = lun->vlun.chnl_id;\r\nppa.g.lun = lun->vlun.lun_id;\r\nret = nvm_get_bb_tbl(dev, ppa, blks);\r\nif (ret)\r\npr_err("gen: could not get BB table\n");\r\nret = gen_block_bb(gn, ppa, blks, nr_blks);\r\nif (ret)\r\npr_err("gen: BB table map failed\n");\r\n}\r\n}\r\nif ((dev->identity.dom & NVM_RSP_L2P) && dev->ops->get_l2p_tbl) {\r\nret = dev->ops->get_l2p_tbl(dev, 0, dev->total_secs,\r\ngen_block_map, dev);\r\nif (ret) {\r\npr_err("gen: could not read L2P table.\n");\r\npr_warn("gen: default block initialization");\r\n}\r\n}\r\nkfree(blks);\r\nreturn 0;\r\n}\r\nstatic void gen_free(struct nvm_dev *dev)\r\n{\r\ngen_blocks_free(dev);\r\ngen_luns_free(dev);\r\nkfree(dev->mp);\r\ndev->mp = NULL;\r\n}\r\nstatic int gen_register(struct nvm_dev *dev)\r\n{\r\nstruct gen_dev *gn;\r\nint ret;\r\nif (!try_module_get(THIS_MODULE))\r\nreturn -ENODEV;\r\ngn = kzalloc(sizeof(struct gen_dev), GFP_KERNEL);\r\nif (!gn)\r\nreturn -ENOMEM;\r\ngn->dev = dev;\r\ngn->nr_luns = dev->nr_luns;\r\nINIT_LIST_HEAD(&gn->area_list);\r\nmutex_init(&gn->lock);\r\nINIT_LIST_HEAD(&gn->targets);\r\ndev->mp = gn;\r\nret = gen_luns_init(dev, gn);\r\nif (ret) {\r\npr_err("gen: could not initialize luns\n");\r\ngoto err;\r\n}\r\nret = gen_blocks_init(dev, gn);\r\nif (ret) {\r\npr_err("gen: could not initialize blocks\n");\r\ngoto err;\r\n}\r\nreturn 1;\r\nerr:\r\ngen_free(dev);\r\nmodule_put(THIS_MODULE);\r\nreturn ret;\r\n}\r\nstatic void gen_unregister(struct nvm_dev *dev)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nstruct nvm_target *t, *tmp;\r\nmutex_lock(&gn->lock);\r\nlist_for_each_entry_safe(t, tmp, &gn->targets, list) {\r\nif (t->dev != dev)\r\ncontinue;\r\n__gen_remove_target(t);\r\n}\r\nmutex_unlock(&gn->lock);\r\ngen_free(dev);\r\nmodule_put(THIS_MODULE);\r\n}\r\nstatic struct nvm_block *gen_get_blk(struct nvm_dev *dev,\r\nstruct nvm_lun *vlun, unsigned long flags)\r\n{\r\nstruct gen_lun *lun = container_of(vlun, struct gen_lun, vlun);\r\nstruct nvm_block *blk = NULL;\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nspin_lock(&vlun->lock);\r\nif (list_empty(&lun->free_list)) {\r\npr_err_ratelimited("gen: lun %u have no free pages available",\r\nlun->vlun.id);\r\ngoto out;\r\n}\r\nif (!is_gc && lun->vlun.nr_free_blocks < lun->reserved_blocks)\r\ngoto out;\r\nblk = list_first_entry(&lun->free_list, struct nvm_block, list);\r\nlist_move_tail(&blk->list, &lun->used_list);\r\nblk->state = NVM_BLK_ST_TGT;\r\nlun->vlun.nr_free_blocks--;\r\nout:\r\nspin_unlock(&vlun->lock);\r\nreturn blk;\r\n}\r\nstatic void gen_put_blk(struct nvm_dev *dev, struct nvm_block *blk)\r\n{\r\nstruct nvm_lun *vlun = blk->lun;\r\nstruct gen_lun *lun = container_of(vlun, struct gen_lun, vlun);\r\nspin_lock(&vlun->lock);\r\nif (blk->state & NVM_BLK_ST_TGT) {\r\nlist_move_tail(&blk->list, &lun->free_list);\r\nlun->vlun.nr_free_blocks++;\r\nblk->state = NVM_BLK_ST_FREE;\r\n} else if (blk->state & NVM_BLK_ST_BAD) {\r\nlist_move_tail(&blk->list, &lun->bb_list);\r\nblk->state = NVM_BLK_ST_BAD;\r\n} else {\r\nWARN_ON_ONCE(1);\r\npr_err("gen: erroneous block type (%lu -> %u)\n",\r\nblk->id, blk->state);\r\nlist_move_tail(&blk->list, &lun->bb_list);\r\n}\r\nspin_unlock(&vlun->lock);\r\n}\r\nstatic void gen_mark_blk(struct nvm_dev *dev, struct ppa_addr ppa, int type)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nstruct gen_lun *lun;\r\nstruct nvm_block *blk;\r\npr_debug("gen: ppa (ch: %u lun: %u blk: %u pg: %u) -> %u\n",\r\nppa.g.ch, ppa.g.lun, ppa.g.blk, ppa.g.pg, type);\r\nif (unlikely(ppa.g.ch > dev->nr_chnls ||\r\nppa.g.lun > dev->luns_per_chnl ||\r\nppa.g.blk > dev->blks_per_lun)) {\r\nWARN_ON_ONCE(1);\r\npr_err("gen: ppa broken (ch: %u > %u lun: %u > %u blk: %u > %u",\r\nppa.g.ch, dev->nr_chnls,\r\nppa.g.lun, dev->luns_per_chnl,\r\nppa.g.blk, dev->blks_per_lun);\r\nreturn;\r\n}\r\nlun = &gn->luns[(dev->luns_per_chnl * ppa.g.ch) + ppa.g.lun];\r\nblk = &lun->vlun.blocks[ppa.g.blk];\r\nblk->state = type;\r\n}\r\nstatic void gen_mark_blk_bad(struct nvm_dev *dev, struct nvm_rq *rqd)\r\n{\r\nint bit = -1;\r\nint max_secs = dev->ops->max_phys_sect;\r\nvoid *comp_bits = &rqd->ppa_status;\r\nnvm_addr_to_generic_mode(dev, rqd);\r\nif (rqd->nr_ppas == 1) {\r\ngen_mark_blk(dev, rqd->ppa_addr, NVM_BLK_ST_BAD);\r\nreturn;\r\n}\r\nwhile ((bit = find_next_bit(comp_bits, max_secs, bit + 1)) < max_secs)\r\ngen_mark_blk(dev, rqd->ppa_list[bit], NVM_BLK_ST_BAD);\r\n}\r\nstatic void gen_end_io(struct nvm_rq *rqd)\r\n{\r\nstruct nvm_tgt_instance *ins = rqd->ins;\r\nif (rqd->error == NVM_RSP_ERR_FAILWRITE)\r\ngen_mark_blk_bad(rqd->dev, rqd);\r\nins->tt->end_io(rqd);\r\n}\r\nstatic int gen_submit_io(struct nvm_dev *dev, struct nvm_rq *rqd)\r\n{\r\nif (!dev->ops->submit_io)\r\nreturn -ENODEV;\r\nnvm_generic_to_addr_mode(dev, rqd);\r\nrqd->dev = dev;\r\nrqd->end_io = gen_end_io;\r\nreturn dev->ops->submit_io(dev, rqd);\r\n}\r\nstatic int gen_erase_blk(struct nvm_dev *dev, struct nvm_block *blk,\r\nunsigned long flags)\r\n{\r\nstruct ppa_addr addr = block_to_ppa(dev, blk);\r\nreturn nvm_erase_ppa(dev, &addr, 1);\r\n}\r\nstatic int gen_reserve_lun(struct nvm_dev *dev, int lunid)\r\n{\r\nreturn test_and_set_bit(lunid, dev->lun_map);\r\n}\r\nstatic void gen_release_lun(struct nvm_dev *dev, int lunid)\r\n{\r\nWARN_ON(!test_and_clear_bit(lunid, dev->lun_map));\r\n}\r\nstatic struct nvm_lun *gen_get_lun(struct nvm_dev *dev, int lunid)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nif (unlikely(lunid >= dev->nr_luns))\r\nreturn NULL;\r\nreturn &gn->luns[lunid].vlun;\r\n}\r\nstatic void gen_lun_info_print(struct nvm_dev *dev)\r\n{\r\nstruct gen_dev *gn = dev->mp;\r\nstruct gen_lun *lun;\r\nunsigned int i;\r\ngen_for_each_lun(gn, lun, i) {\r\nspin_lock(&lun->vlun.lock);\r\npr_info("%s: lun%8u\t%u\n", dev->name, i,\r\nlun->vlun.nr_free_blocks);\r\nspin_unlock(&lun->vlun.lock);\r\n}\r\n}\r\nstatic int __init gen_module_init(void)\r\n{\r\nreturn nvm_register_mgr(&gen);\r\n}\r\nstatic void gen_module_exit(void)\r\n{\r\nnvm_unregister_mgr(&gen);\r\n}
