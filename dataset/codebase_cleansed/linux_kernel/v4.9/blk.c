static u32 nsblk_meta_size(struct nd_namespace_blk *nsblk)\r\n{\r\nreturn nsblk->lbasize - ((nsblk->lbasize >= 4096) ? 4096 : 512);\r\n}\r\nstatic u32 nsblk_internal_lbasize(struct nd_namespace_blk *nsblk)\r\n{\r\nreturn roundup(nsblk->lbasize, INT_LBASIZE_ALIGNMENT);\r\n}\r\nstatic u32 nsblk_sector_size(struct nd_namespace_blk *nsblk)\r\n{\r\nreturn nsblk->lbasize - nsblk_meta_size(nsblk);\r\n}\r\nstatic resource_size_t to_dev_offset(struct nd_namespace_blk *nsblk,\r\nresource_size_t ns_offset, unsigned int len)\r\n{\r\nint i;\r\nfor (i = 0; i < nsblk->num_resources; i++) {\r\nif (ns_offset < resource_size(nsblk->res[i])) {\r\nif (ns_offset + len > resource_size(nsblk->res[i])) {\r\ndev_WARN_ONCE(&nsblk->common.dev, 1,\r\n"illegal request\n");\r\nreturn SIZE_MAX;\r\n}\r\nreturn nsblk->res[i]->start + ns_offset;\r\n}\r\nns_offset -= resource_size(nsblk->res[i]);\r\n}\r\ndev_WARN_ONCE(&nsblk->common.dev, 1, "request out of range\n");\r\nreturn SIZE_MAX;\r\n}\r\nstatic struct nd_blk_region *to_ndbr(struct nd_namespace_blk *nsblk)\r\n{\r\nstruct nd_region *nd_region;\r\nstruct device *parent;\r\nparent = nsblk->common.dev.parent;\r\nnd_region = container_of(parent, struct nd_region, dev);\r\nreturn container_of(nd_region, struct nd_blk_region, nd_region);\r\n}\r\nstatic int nd_blk_rw_integrity(struct nd_namespace_blk *nsblk,\r\nstruct bio_integrity_payload *bip, u64 lba, int rw)\r\n{\r\nstruct nd_blk_region *ndbr = to_ndbr(nsblk);\r\nunsigned int len = nsblk_meta_size(nsblk);\r\nresource_size_t dev_offset, ns_offset;\r\nu32 internal_lbasize, sector_size;\r\nint err = 0;\r\ninternal_lbasize = nsblk_internal_lbasize(nsblk);\r\nsector_size = nsblk_sector_size(nsblk);\r\nns_offset = lba * internal_lbasize + sector_size;\r\ndev_offset = to_dev_offset(nsblk, ns_offset, len);\r\nif (dev_offset == SIZE_MAX)\r\nreturn -EIO;\r\nwhile (len) {\r\nunsigned int cur_len;\r\nstruct bio_vec bv;\r\nvoid *iobuf;\r\nbv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);\r\ncur_len = min(len, bv.bv_len);\r\niobuf = kmap_atomic(bv.bv_page);\r\nerr = ndbr->do_io(ndbr, dev_offset, iobuf + bv.bv_offset,\r\ncur_len, rw);\r\nkunmap_atomic(iobuf);\r\nif (err)\r\nreturn err;\r\nlen -= cur_len;\r\ndev_offset += cur_len;\r\nbvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len);\r\n}\r\nreturn err;\r\n}\r\nstatic int nd_blk_rw_integrity(struct nd_namespace_blk *nsblk,\r\nstruct bio_integrity_payload *bip, u64 lba, int rw)\r\n{\r\nreturn 0;\r\n}\r\nstatic int nsblk_do_bvec(struct nd_namespace_blk *nsblk,\r\nstruct bio_integrity_payload *bip, struct page *page,\r\nunsigned int len, unsigned int off, int rw, sector_t sector)\r\n{\r\nstruct nd_blk_region *ndbr = to_ndbr(nsblk);\r\nresource_size_t dev_offset, ns_offset;\r\nu32 internal_lbasize, sector_size;\r\nint err = 0;\r\nvoid *iobuf;\r\nu64 lba;\r\ninternal_lbasize = nsblk_internal_lbasize(nsblk);\r\nsector_size = nsblk_sector_size(nsblk);\r\nwhile (len) {\r\nunsigned int cur_len;\r\ncur_len = bip ? min(len, sector_size) : len;\r\nlba = div_u64(sector << SECTOR_SHIFT, sector_size);\r\nns_offset = lba * internal_lbasize;\r\ndev_offset = to_dev_offset(nsblk, ns_offset, cur_len);\r\nif (dev_offset == SIZE_MAX)\r\nreturn -EIO;\r\niobuf = kmap_atomic(page);\r\nerr = ndbr->do_io(ndbr, dev_offset, iobuf + off, cur_len, rw);\r\nkunmap_atomic(iobuf);\r\nif (err)\r\nreturn err;\r\nif (bip) {\r\nerr = nd_blk_rw_integrity(nsblk, bip, lba, rw);\r\nif (err)\r\nreturn err;\r\n}\r\nlen -= cur_len;\r\noff += cur_len;\r\nsector += sector_size >> SECTOR_SHIFT;\r\n}\r\nreturn err;\r\n}\r\nstatic blk_qc_t nd_blk_make_request(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct bio_integrity_payload *bip;\r\nstruct nd_namespace_blk *nsblk;\r\nstruct bvec_iter iter;\r\nunsigned long start;\r\nstruct bio_vec bvec;\r\nint err = 0, rw;\r\nbool do_acct;\r\nif (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {\r\nbio->bi_error = -EIO;\r\ngoto out;\r\n}\r\nbip = bio_integrity(bio);\r\nnsblk = q->queuedata;\r\nrw = bio_data_dir(bio);\r\ndo_acct = nd_iostat_start(bio, &start);\r\nbio_for_each_segment(bvec, bio, iter) {\r\nunsigned int len = bvec.bv_len;\r\nBUG_ON(len > PAGE_SIZE);\r\nerr = nsblk_do_bvec(nsblk, bip, bvec.bv_page, len,\r\nbvec.bv_offset, rw, iter.bi_sector);\r\nif (err) {\r\ndev_dbg(&nsblk->common.dev,\r\n"io error in %s sector %lld, len %d,\n",\r\n(rw == READ) ? "READ" : "WRITE",\r\n(unsigned long long) iter.bi_sector, len);\r\nbio->bi_error = err;\r\nbreak;\r\n}\r\n}\r\nif (do_acct)\r\nnd_iostat_end(bio, start);\r\nout:\r\nbio_endio(bio);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nstatic int nsblk_rw_bytes(struct nd_namespace_common *ndns,\r\nresource_size_t offset, void *iobuf, size_t n, int rw)\r\n{\r\nstruct nd_namespace_blk *nsblk = to_nd_namespace_blk(&ndns->dev);\r\nstruct nd_blk_region *ndbr = to_ndbr(nsblk);\r\nresource_size_t dev_offset;\r\ndev_offset = to_dev_offset(nsblk, offset, n);\r\nif (unlikely(offset + n > nsblk->size)) {\r\ndev_WARN_ONCE(&ndns->dev, 1, "request out of range\n");\r\nreturn -EFAULT;\r\n}\r\nif (dev_offset == SIZE_MAX)\r\nreturn -EIO;\r\nreturn ndbr->do_io(ndbr, dev_offset, iobuf, n, rw);\r\n}\r\nstatic void nd_blk_release_queue(void *q)\r\n{\r\nblk_cleanup_queue(q);\r\n}\r\nstatic void nd_blk_release_disk(void *disk)\r\n{\r\ndel_gendisk(disk);\r\nput_disk(disk);\r\n}\r\nstatic int nsblk_attach_disk(struct nd_namespace_blk *nsblk)\r\n{\r\nstruct device *dev = &nsblk->common.dev;\r\nresource_size_t available_disk_size;\r\nstruct request_queue *q;\r\nstruct gendisk *disk;\r\nu64 internal_nlba;\r\ninternal_nlba = div_u64(nsblk->size, nsblk_internal_lbasize(nsblk));\r\navailable_disk_size = internal_nlba * nsblk_sector_size(nsblk);\r\nq = blk_alloc_queue(GFP_KERNEL);\r\nif (!q)\r\nreturn -ENOMEM;\r\nif (devm_add_action_or_reset(dev, nd_blk_release_queue, q))\r\nreturn -ENOMEM;\r\nblk_queue_make_request(q, nd_blk_make_request);\r\nblk_queue_max_hw_sectors(q, UINT_MAX);\r\nblk_queue_bounce_limit(q, BLK_BOUNCE_ANY);\r\nblk_queue_logical_block_size(q, nsblk_sector_size(nsblk));\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, q);\r\nq->queuedata = nsblk;\r\ndisk = alloc_disk(0);\r\nif (!disk)\r\nreturn -ENOMEM;\r\ndisk->first_minor = 0;\r\ndisk->fops = &nd_blk_fops;\r\ndisk->queue = q;\r\ndisk->flags = GENHD_FL_EXT_DEVT;\r\nnvdimm_namespace_disk_name(&nsblk->common, disk->disk_name);\r\nset_capacity(disk, 0);\r\ndevice_add_disk(dev, disk);\r\nif (devm_add_action_or_reset(dev, nd_blk_release_disk, disk))\r\nreturn -ENOMEM;\r\nif (nsblk_meta_size(nsblk)) {\r\nint rc = nd_integrity_init(disk, nsblk_meta_size(nsblk));\r\nif (rc)\r\nreturn rc;\r\n}\r\nset_capacity(disk, available_disk_size >> SECTOR_SHIFT);\r\nrevalidate_disk(disk);\r\nreturn 0;\r\n}\r\nstatic int nd_blk_probe(struct device *dev)\r\n{\r\nstruct nd_namespace_common *ndns;\r\nstruct nd_namespace_blk *nsblk;\r\nndns = nvdimm_namespace_common_probe(dev);\r\nif (IS_ERR(ndns))\r\nreturn PTR_ERR(ndns);\r\nnsblk = to_nd_namespace_blk(&ndns->dev);\r\nnsblk->size = nvdimm_namespace_capacity(ndns);\r\ndev_set_drvdata(dev, nsblk);\r\nndns->rw_bytes = nsblk_rw_bytes;\r\nif (is_nd_btt(dev))\r\nreturn nvdimm_namespace_attach_btt(ndns);\r\nelse if (nd_btt_probe(dev, ndns) == 0) {\r\nreturn -ENXIO;\r\n} else\r\nreturn nsblk_attach_disk(nsblk);\r\n}\r\nstatic int nd_blk_remove(struct device *dev)\r\n{\r\nif (is_nd_btt(dev))\r\nnvdimm_namespace_detach_btt(to_nd_btt(dev));\r\nreturn 0;\r\n}\r\nstatic int __init nd_blk_init(void)\r\n{\r\nreturn nd_driver_register(&nd_blk_driver);\r\n}\r\nstatic void __exit nd_blk_exit(void)\r\n{\r\ndriver_unregister(&nd_blk_driver.drv);\r\n}
