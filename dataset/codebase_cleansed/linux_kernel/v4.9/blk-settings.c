void blk_queue_prep_rq(struct request_queue *q, prep_rq_fn *pfn)\r\n{\r\nq->prep_rq_fn = pfn;\r\n}\r\nvoid blk_queue_unprep_rq(struct request_queue *q, unprep_rq_fn *ufn)\r\n{\r\nq->unprep_rq_fn = ufn;\r\n}\r\nvoid blk_queue_softirq_done(struct request_queue *q, softirq_done_fn *fn)\r\n{\r\nq->softirq_done_fn = fn;\r\n}\r\nvoid blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout)\r\n{\r\nq->rq_timeout = timeout;\r\n}\r\nvoid blk_queue_rq_timed_out(struct request_queue *q, rq_timed_out_fn *fn)\r\n{\r\nq->rq_timed_out_fn = fn;\r\n}\r\nvoid blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn)\r\n{\r\nq->lld_busy_fn = fn;\r\n}\r\nvoid blk_set_default_limits(struct queue_limits *lim)\r\n{\r\nlim->max_segments = BLK_MAX_SEGMENTS;\r\nlim->max_integrity_segments = 0;\r\nlim->seg_boundary_mask = BLK_SEG_BOUNDARY_MASK;\r\nlim->virt_boundary_mask = 0;\r\nlim->max_segment_size = BLK_MAX_SEGMENT_SIZE;\r\nlim->max_sectors = lim->max_hw_sectors = BLK_SAFE_MAX_SECTORS;\r\nlim->max_dev_sectors = 0;\r\nlim->chunk_sectors = 0;\r\nlim->max_write_same_sectors = 0;\r\nlim->max_discard_sectors = 0;\r\nlim->max_hw_discard_sectors = 0;\r\nlim->discard_granularity = 0;\r\nlim->discard_alignment = 0;\r\nlim->discard_misaligned = 0;\r\nlim->discard_zeroes_data = 0;\r\nlim->logical_block_size = lim->physical_block_size = lim->io_min = 512;\r\nlim->bounce_pfn = (unsigned long)(BLK_BOUNCE_ANY >> PAGE_SHIFT);\r\nlim->alignment_offset = 0;\r\nlim->io_opt = 0;\r\nlim->misaligned = 0;\r\nlim->cluster = 1;\r\n}\r\nvoid blk_set_stacking_limits(struct queue_limits *lim)\r\n{\r\nblk_set_default_limits(lim);\r\nlim->discard_zeroes_data = 1;\r\nlim->max_segments = USHRT_MAX;\r\nlim->max_hw_sectors = UINT_MAX;\r\nlim->max_segment_size = UINT_MAX;\r\nlim->max_sectors = UINT_MAX;\r\nlim->max_dev_sectors = UINT_MAX;\r\nlim->max_write_same_sectors = UINT_MAX;\r\n}\r\nvoid blk_queue_make_request(struct request_queue *q, make_request_fn *mfn)\r\n{\r\nq->nr_requests = BLKDEV_MAX_RQ;\r\nq->make_request_fn = mfn;\r\nblk_queue_dma_alignment(q, 511);\r\nblk_queue_congestion_threshold(q);\r\nq->nr_batching = BLK_BATCH_REQ;\r\nblk_set_default_limits(&q->limits);\r\nblk_queue_bounce_limit(q, BLK_BOUNCE_HIGH);\r\n}\r\nvoid blk_queue_bounce_limit(struct request_queue *q, u64 max_addr)\r\n{\r\nunsigned long b_pfn = max_addr >> PAGE_SHIFT;\r\nint dma = 0;\r\nq->bounce_gfp = GFP_NOIO;\r\n#if BITS_PER_LONG == 64\r\nif (b_pfn < (min_t(u64, 0xffffffffUL, BLK_BOUNCE_HIGH) >> PAGE_SHIFT))\r\ndma = 1;\r\nq->limits.bounce_pfn = max(max_low_pfn, b_pfn);\r\n#else\r\nif (b_pfn < blk_max_low_pfn)\r\ndma = 1;\r\nq->limits.bounce_pfn = b_pfn;\r\n#endif\r\nif (dma) {\r\ninit_emergency_isa_pool();\r\nq->bounce_gfp = GFP_NOIO | GFP_DMA;\r\nq->limits.bounce_pfn = b_pfn;\r\n}\r\n}\r\nvoid blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)\r\n{\r\nstruct queue_limits *limits = &q->limits;\r\nunsigned int max_sectors;\r\nif ((max_hw_sectors << 9) < PAGE_SIZE) {\r\nmax_hw_sectors = 1 << (PAGE_SHIFT - 9);\r\nprintk(KERN_INFO "%s: set to minimum %d\n",\r\n__func__, max_hw_sectors);\r\n}\r\nlimits->max_hw_sectors = max_hw_sectors;\r\nmax_sectors = min_not_zero(max_hw_sectors, limits->max_dev_sectors);\r\nmax_sectors = min_t(unsigned int, max_sectors, BLK_DEF_MAX_SECTORS);\r\nlimits->max_sectors = max_sectors;\r\n}\r\nvoid blk_queue_chunk_sectors(struct request_queue *q, unsigned int chunk_sectors)\r\n{\r\nBUG_ON(!is_power_of_2(chunk_sectors));\r\nq->limits.chunk_sectors = chunk_sectors;\r\n}\r\nvoid blk_queue_max_discard_sectors(struct request_queue *q,\r\nunsigned int max_discard_sectors)\r\n{\r\nq->limits.max_hw_discard_sectors = max_discard_sectors;\r\nq->limits.max_discard_sectors = max_discard_sectors;\r\n}\r\nvoid blk_queue_max_write_same_sectors(struct request_queue *q,\r\nunsigned int max_write_same_sectors)\r\n{\r\nq->limits.max_write_same_sectors = max_write_same_sectors;\r\n}\r\nvoid blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)\r\n{\r\nif (!max_segments) {\r\nmax_segments = 1;\r\nprintk(KERN_INFO "%s: set to minimum %d\n",\r\n__func__, max_segments);\r\n}\r\nq->limits.max_segments = max_segments;\r\n}\r\nvoid blk_queue_max_segment_size(struct request_queue *q, unsigned int max_size)\r\n{\r\nif (max_size < PAGE_SIZE) {\r\nmax_size = PAGE_SIZE;\r\nprintk(KERN_INFO "%s: set to minimum %d\n",\r\n__func__, max_size);\r\n}\r\nq->limits.max_segment_size = max_size;\r\n}\r\nvoid blk_queue_logical_block_size(struct request_queue *q, unsigned short size)\r\n{\r\nq->limits.logical_block_size = size;\r\nif (q->limits.physical_block_size < size)\r\nq->limits.physical_block_size = size;\r\nif (q->limits.io_min < q->limits.physical_block_size)\r\nq->limits.io_min = q->limits.physical_block_size;\r\n}\r\nvoid blk_queue_physical_block_size(struct request_queue *q, unsigned int size)\r\n{\r\nq->limits.physical_block_size = size;\r\nif (q->limits.physical_block_size < q->limits.logical_block_size)\r\nq->limits.physical_block_size = q->limits.logical_block_size;\r\nif (q->limits.io_min < q->limits.physical_block_size)\r\nq->limits.io_min = q->limits.physical_block_size;\r\n}\r\nvoid blk_queue_alignment_offset(struct request_queue *q, unsigned int offset)\r\n{\r\nq->limits.alignment_offset =\r\noffset & (q->limits.physical_block_size - 1);\r\nq->limits.misaligned = 0;\r\n}\r\nvoid blk_limits_io_min(struct queue_limits *limits, unsigned int min)\r\n{\r\nlimits->io_min = min;\r\nif (limits->io_min < limits->logical_block_size)\r\nlimits->io_min = limits->logical_block_size;\r\nif (limits->io_min < limits->physical_block_size)\r\nlimits->io_min = limits->physical_block_size;\r\n}\r\nvoid blk_queue_io_min(struct request_queue *q, unsigned int min)\r\n{\r\nblk_limits_io_min(&q->limits, min);\r\n}\r\nvoid blk_limits_io_opt(struct queue_limits *limits, unsigned int opt)\r\n{\r\nlimits->io_opt = opt;\r\n}\r\nvoid blk_queue_io_opt(struct request_queue *q, unsigned int opt)\r\n{\r\nblk_limits_io_opt(&q->limits, opt);\r\n}\r\nvoid blk_queue_stack_limits(struct request_queue *t, struct request_queue *b)\r\n{\r\nblk_stack_limits(&t->limits, &b->limits, 0);\r\n}\r\nint blk_stack_limits(struct queue_limits *t, struct queue_limits *b,\r\nsector_t start)\r\n{\r\nunsigned int top, bottom, alignment, ret = 0;\r\nt->max_sectors = min_not_zero(t->max_sectors, b->max_sectors);\r\nt->max_hw_sectors = min_not_zero(t->max_hw_sectors, b->max_hw_sectors);\r\nt->max_dev_sectors = min_not_zero(t->max_dev_sectors, b->max_dev_sectors);\r\nt->max_write_same_sectors = min(t->max_write_same_sectors,\r\nb->max_write_same_sectors);\r\nt->bounce_pfn = min_not_zero(t->bounce_pfn, b->bounce_pfn);\r\nt->seg_boundary_mask = min_not_zero(t->seg_boundary_mask,\r\nb->seg_boundary_mask);\r\nt->virt_boundary_mask = min_not_zero(t->virt_boundary_mask,\r\nb->virt_boundary_mask);\r\nt->max_segments = min_not_zero(t->max_segments, b->max_segments);\r\nt->max_integrity_segments = min_not_zero(t->max_integrity_segments,\r\nb->max_integrity_segments);\r\nt->max_segment_size = min_not_zero(t->max_segment_size,\r\nb->max_segment_size);\r\nt->misaligned |= b->misaligned;\r\nalignment = queue_limit_alignment_offset(b, start);\r\nif (t->alignment_offset != alignment) {\r\ntop = max(t->physical_block_size, t->io_min)\r\n+ t->alignment_offset;\r\nbottom = max(b->physical_block_size, b->io_min) + alignment;\r\nif (max(top, bottom) % min(top, bottom)) {\r\nt->misaligned = 1;\r\nret = -1;\r\n}\r\n}\r\nt->logical_block_size = max(t->logical_block_size,\r\nb->logical_block_size);\r\nt->physical_block_size = max(t->physical_block_size,\r\nb->physical_block_size);\r\nt->io_min = max(t->io_min, b->io_min);\r\nt->io_opt = lcm_not_zero(t->io_opt, b->io_opt);\r\nt->cluster &= b->cluster;\r\nt->discard_zeroes_data &= b->discard_zeroes_data;\r\nif (t->physical_block_size & (t->logical_block_size - 1)) {\r\nt->physical_block_size = t->logical_block_size;\r\nt->misaligned = 1;\r\nret = -1;\r\n}\r\nif (t->io_min & (t->physical_block_size - 1)) {\r\nt->io_min = t->physical_block_size;\r\nt->misaligned = 1;\r\nret = -1;\r\n}\r\nif (t->io_opt & (t->physical_block_size - 1)) {\r\nt->io_opt = 0;\r\nt->misaligned = 1;\r\nret = -1;\r\n}\r\nt->raid_partial_stripes_expensive =\r\nmax(t->raid_partial_stripes_expensive,\r\nb->raid_partial_stripes_expensive);\r\nt->alignment_offset = lcm_not_zero(t->alignment_offset, alignment)\r\n% max(t->physical_block_size, t->io_min);\r\nif (t->alignment_offset & (t->logical_block_size - 1)) {\r\nt->misaligned = 1;\r\nret = -1;\r\n}\r\nif (b->discard_granularity) {\r\nalignment = queue_limit_discard_alignment(b, start);\r\nif (t->discard_granularity != 0 &&\r\nt->discard_alignment != alignment) {\r\ntop = t->discard_granularity + t->discard_alignment;\r\nbottom = b->discard_granularity + alignment;\r\nif ((max(top, bottom) % min(top, bottom)) != 0)\r\nt->discard_misaligned = 1;\r\n}\r\nt->max_discard_sectors = min_not_zero(t->max_discard_sectors,\r\nb->max_discard_sectors);\r\nt->max_hw_discard_sectors = min_not_zero(t->max_hw_discard_sectors,\r\nb->max_hw_discard_sectors);\r\nt->discard_granularity = max(t->discard_granularity,\r\nb->discard_granularity);\r\nt->discard_alignment = lcm_not_zero(t->discard_alignment, alignment) %\r\nt->discard_granularity;\r\n}\r\nreturn ret;\r\n}\r\nint bdev_stack_limits(struct queue_limits *t, struct block_device *bdev,\r\nsector_t start)\r\n{\r\nstruct request_queue *bq = bdev_get_queue(bdev);\r\nstart += get_start_sect(bdev);\r\nreturn blk_stack_limits(t, &bq->limits, start);\r\n}\r\nvoid disk_stack_limits(struct gendisk *disk, struct block_device *bdev,\r\nsector_t offset)\r\n{\r\nstruct request_queue *t = disk->queue;\r\nif (bdev_stack_limits(&t->limits, bdev, offset >> 9) < 0) {\r\nchar top[BDEVNAME_SIZE], bottom[BDEVNAME_SIZE];\r\ndisk_name(disk, 0, top);\r\nbdevname(bdev, bottom);\r\nprintk(KERN_NOTICE "%s: Warning: Device %s is misaligned\n",\r\ntop, bottom);\r\n}\r\n}\r\nvoid blk_queue_dma_pad(struct request_queue *q, unsigned int mask)\r\n{\r\nq->dma_pad_mask = mask;\r\n}\r\nvoid blk_queue_update_dma_pad(struct request_queue *q, unsigned int mask)\r\n{\r\nif (mask > q->dma_pad_mask)\r\nq->dma_pad_mask = mask;\r\n}\r\nint blk_queue_dma_drain(struct request_queue *q,\r\ndma_drain_needed_fn *dma_drain_needed,\r\nvoid *buf, unsigned int size)\r\n{\r\nif (queue_max_segments(q) < 2)\r\nreturn -EINVAL;\r\nblk_queue_max_segments(q, queue_max_segments(q) - 1);\r\nq->dma_drain_needed = dma_drain_needed;\r\nq->dma_drain_buffer = buf;\r\nq->dma_drain_size = size;\r\nreturn 0;\r\n}\r\nvoid blk_queue_segment_boundary(struct request_queue *q, unsigned long mask)\r\n{\r\nif (mask < PAGE_SIZE - 1) {\r\nmask = PAGE_SIZE - 1;\r\nprintk(KERN_INFO "%s: set to minimum %lx\n",\r\n__func__, mask);\r\n}\r\nq->limits.seg_boundary_mask = mask;\r\n}\r\nvoid blk_queue_virt_boundary(struct request_queue *q, unsigned long mask)\r\n{\r\nq->limits.virt_boundary_mask = mask;\r\n}\r\nvoid blk_queue_dma_alignment(struct request_queue *q, int mask)\r\n{\r\nq->dma_alignment = mask;\r\n}\r\nvoid blk_queue_update_dma_alignment(struct request_queue *q, int mask)\r\n{\r\nBUG_ON(mask > PAGE_SIZE);\r\nif (mask > q->dma_alignment)\r\nq->dma_alignment = mask;\r\n}\r\nvoid blk_queue_flush_queueable(struct request_queue *q, bool queueable)\r\n{\r\nspin_lock_irq(q->queue_lock);\r\nif (queueable)\r\nclear_bit(QUEUE_FLAG_FLUSH_NQ, &q->queue_flags);\r\nelse\r\nset_bit(QUEUE_FLAG_FLUSH_NQ, &q->queue_flags);\r\nspin_unlock_irq(q->queue_lock);\r\n}\r\nvoid blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)\r\n{\r\nspin_lock_irq(q->queue_lock);\r\nif (wc)\r\nqueue_flag_set(QUEUE_FLAG_WC, q);\r\nelse\r\nqueue_flag_clear(QUEUE_FLAG_WC, q);\r\nif (fua)\r\nqueue_flag_set(QUEUE_FLAG_FUA, q);\r\nelse\r\nqueue_flag_clear(QUEUE_FLAG_FUA, q);\r\nspin_unlock_irq(q->queue_lock);\r\n}\r\nstatic int __init blk_settings_init(void)\r\n{\r\nblk_max_low_pfn = max_low_pfn - 1;\r\nblk_max_pfn = max_pfn - 1;\r\nreturn 0;\r\n}
