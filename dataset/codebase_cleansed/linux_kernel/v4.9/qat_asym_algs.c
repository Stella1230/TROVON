static void qat_dh_cb(struct icp_qat_fw_pke_resp *resp)\r\n{\r\nstruct qat_asym_request *req = (void *)(__force long)resp->opaque;\r\nstruct kpp_request *areq = req->areq.dh;\r\nstruct device *dev = &GET_DEV(req->ctx.dh->inst->accel_dev);\r\nint err = ICP_QAT_FW_PKE_RESP_PKE_STAT_GET(\r\nresp->pke_resp_hdr.comn_resp_flags);\r\nerr = (err == ICP_QAT_FW_COMN_STATUS_FLAG_OK) ? 0 : -EINVAL;\r\nif (areq->src) {\r\nif (req->src_align)\r\ndma_free_coherent(dev, req->ctx.dh->p_size,\r\nreq->src_align, req->in.dh.in.b);\r\nelse\r\ndma_unmap_single(dev, req->in.dh.in.b,\r\nreq->ctx.dh->p_size, DMA_TO_DEVICE);\r\n}\r\nareq->dst_len = req->ctx.dh->p_size;\r\nif (req->dst_align) {\r\nscatterwalk_map_and_copy(req->dst_align, areq->dst, 0,\r\nareq->dst_len, 1);\r\ndma_free_coherent(dev, req->ctx.dh->p_size, req->dst_align,\r\nreq->out.dh.r);\r\n} else {\r\ndma_unmap_single(dev, req->out.dh.r, req->ctx.dh->p_size,\r\nDMA_FROM_DEVICE);\r\n}\r\ndma_unmap_single(dev, req->phy_in, sizeof(struct qat_dh_input_params),\r\nDMA_TO_DEVICE);\r\ndma_unmap_single(dev, req->phy_out,\r\nsizeof(struct qat_dh_output_params),\r\nDMA_TO_DEVICE);\r\nkpp_request_complete(areq, err);\r\n}\r\nstatic unsigned long qat_dh_fn_id(unsigned int len, bool g2)\r\n{\r\nunsigned int bitslen = len << 3;\r\nswitch (bitslen) {\r\ncase 1536:\r\nreturn g2 ? PKE_DH_G2_1536 : PKE_DH_1536;\r\ncase 2048:\r\nreturn g2 ? PKE_DH_G2_2048 : PKE_DH_2048;\r\ncase 3072:\r\nreturn g2 ? PKE_DH_G2_3072 : PKE_DH_3072;\r\ncase 4096:\r\nreturn g2 ? PKE_DH_G2_4096 : PKE_DH_4096;\r\ndefault:\r\nreturn 0;\r\n};\r\n}\r\nstatic inline struct qat_dh_ctx *qat_dh_get_params(struct crypto_kpp *tfm)\r\n{\r\nreturn kpp_tfm_ctx(tfm);\r\n}\r\nstatic int qat_dh_compute_value(struct kpp_request *req)\r\n{\r\nstruct crypto_kpp *tfm = crypto_kpp_reqtfm(req);\r\nstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nstruct qat_asym_request *qat_req =\r\nPTR_ALIGN(kpp_request_ctx(req), 64);\r\nstruct icp_qat_fw_pke_request *msg = &qat_req->req;\r\nint ret, ctr = 0;\r\nint n_input_params = 0;\r\nif (unlikely(!ctx->xa))\r\nreturn -EINVAL;\r\nif (req->dst_len < ctx->p_size) {\r\nreq->dst_len = ctx->p_size;\r\nreturn -EOVERFLOW;\r\n}\r\nmemset(msg, '\0', sizeof(*msg));\r\nICP_QAT_FW_PKE_HDR_VALID_FLAG_SET(msg->pke_hdr,\r\nICP_QAT_FW_COMN_REQ_FLAG_SET);\r\nmsg->pke_hdr.cd_pars.func_id = qat_dh_fn_id(ctx->p_size,\r\n!req->src && ctx->g2);\r\nif (unlikely(!msg->pke_hdr.cd_pars.func_id))\r\nreturn -EINVAL;\r\nqat_req->cb = qat_dh_cb;\r\nqat_req->ctx.dh = ctx;\r\nqat_req->areq.dh = req;\r\nmsg->pke_hdr.service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_PKE;\r\nmsg->pke_hdr.comn_req_flags =\r\nICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_PTR_TYPE_FLAT,\r\nQAT_COMN_CD_FLD_TYPE_64BIT_ADR);\r\nif (req->src) {\r\nqat_req->in.dh.in.xa = ctx->dma_xa;\r\nqat_req->in.dh.in.p = ctx->dma_p;\r\nn_input_params = 3;\r\n} else {\r\nif (ctx->g2) {\r\nqat_req->in.dh.in_g2.xa = ctx->dma_xa;\r\nqat_req->in.dh.in_g2.p = ctx->dma_p;\r\nn_input_params = 2;\r\n} else {\r\nqat_req->in.dh.in.b = ctx->dma_g;\r\nqat_req->in.dh.in.xa = ctx->dma_xa;\r\nqat_req->in.dh.in.p = ctx->dma_p;\r\nn_input_params = 3;\r\n}\r\n}\r\nret = -ENOMEM;\r\nif (req->src) {\r\nif (sg_is_last(req->src) && req->src_len == ctx->p_size) {\r\nqat_req->src_align = NULL;\r\nqat_req->in.dh.in.b = dma_map_single(dev,\r\nsg_virt(req->src),\r\nreq->src_len,\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev,\r\nqat_req->in.dh.in.b)))\r\nreturn ret;\r\n} else {\r\nint shift = ctx->p_size - req->src_len;\r\nqat_req->src_align = dma_zalloc_coherent(dev,\r\nctx->p_size,\r\n&qat_req->in.dh.in.b,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->src_align))\r\nreturn ret;\r\nscatterwalk_map_and_copy(qat_req->src_align + shift,\r\nreq->src, 0, req->src_len, 0);\r\n}\r\n}\r\nif (sg_is_last(req->dst) && req->dst_len == ctx->p_size) {\r\nqat_req->dst_align = NULL;\r\nqat_req->out.dh.r = dma_map_single(dev, sg_virt(req->dst),\r\nreq->dst_len,\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->out.dh.r)))\r\ngoto unmap_src;\r\n} else {\r\nqat_req->dst_align = dma_zalloc_coherent(dev, ctx->p_size,\r\n&qat_req->out.dh.r,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->dst_align))\r\ngoto unmap_src;\r\n}\r\nqat_req->in.dh.in_tab[n_input_params] = 0;\r\nqat_req->out.dh.out_tab[1] = 0;\r\nqat_req->phy_in = dma_map_single(dev, &qat_req->in.dh.in.b,\r\nsizeof(struct qat_dh_input_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_in)))\r\ngoto unmap_dst;\r\nqat_req->phy_out = dma_map_single(dev, &qat_req->out.dh.r,\r\nsizeof(struct qat_dh_output_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_out)))\r\ngoto unmap_in_params;\r\nmsg->pke_mid.src_data_addr = qat_req->phy_in;\r\nmsg->pke_mid.dest_data_addr = qat_req->phy_out;\r\nmsg->pke_mid.opaque = (uint64_t)(__force long)qat_req;\r\nmsg->input_param_count = n_input_params;\r\nmsg->output_param_count = 1;\r\ndo {\r\nret = adf_send_message(ctx->inst->pke_tx, (uint32_t *)msg);\r\n} while (ret == -EBUSY && ctr++ < 100);\r\nif (!ret)\r\nreturn -EINPROGRESS;\r\nif (!dma_mapping_error(dev, qat_req->phy_out))\r\ndma_unmap_single(dev, qat_req->phy_out,\r\nsizeof(struct qat_dh_output_params),\r\nDMA_TO_DEVICE);\r\nunmap_in_params:\r\nif (!dma_mapping_error(dev, qat_req->phy_in))\r\ndma_unmap_single(dev, qat_req->phy_in,\r\nsizeof(struct qat_dh_input_params),\r\nDMA_TO_DEVICE);\r\nunmap_dst:\r\nif (qat_req->dst_align)\r\ndma_free_coherent(dev, ctx->p_size, qat_req->dst_align,\r\nqat_req->out.dh.r);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->out.dh.r))\r\ndma_unmap_single(dev, qat_req->out.dh.r, ctx->p_size,\r\nDMA_FROM_DEVICE);\r\nunmap_src:\r\nif (req->src) {\r\nif (qat_req->src_align)\r\ndma_free_coherent(dev, ctx->p_size, qat_req->src_align,\r\nqat_req->in.dh.in.b);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->in.dh.in.b))\r\ndma_unmap_single(dev, qat_req->in.dh.in.b,\r\nctx->p_size,\r\nDMA_TO_DEVICE);\r\n}\r\nreturn ret;\r\n}\r\nstatic int qat_dh_check_params_length(unsigned int p_len)\r\n{\r\nswitch (p_len) {\r\ncase 1536:\r\ncase 2048:\r\ncase 3072:\r\ncase 4096:\r\nreturn 0;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic int qat_dh_set_params(struct qat_dh_ctx *ctx, struct dh *params)\r\n{\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nif (unlikely(!params->p || !params->g))\r\nreturn -EINVAL;\r\nif (qat_dh_check_params_length(params->p_size << 3))\r\nreturn -EINVAL;\r\nctx->p_size = params->p_size;\r\nctx->p = dma_zalloc_coherent(dev, ctx->p_size, &ctx->dma_p, GFP_KERNEL);\r\nif (!ctx->p)\r\nreturn -ENOMEM;\r\nmemcpy(ctx->p, params->p, ctx->p_size);\r\nif (params->g_size == 1 && *(char *)params->g == 0x02) {\r\nctx->g2 = true;\r\nreturn 0;\r\n}\r\nctx->g = dma_zalloc_coherent(dev, ctx->p_size, &ctx->dma_g, GFP_KERNEL);\r\nif (!ctx->g) {\r\ndma_free_coherent(dev, ctx->p_size, ctx->p, ctx->dma_p);\r\nctx->p = NULL;\r\nreturn -ENOMEM;\r\n}\r\nmemcpy(ctx->g + (ctx->p_size - params->g_size), params->g,\r\nparams->g_size);\r\nreturn 0;\r\n}\r\nstatic void qat_dh_clear_ctx(struct device *dev, struct qat_dh_ctx *ctx)\r\n{\r\nif (ctx->g) {\r\ndma_free_coherent(dev, ctx->p_size, ctx->g, ctx->dma_g);\r\nctx->g = NULL;\r\n}\r\nif (ctx->xa) {\r\ndma_free_coherent(dev, ctx->p_size, ctx->xa, ctx->dma_xa);\r\nctx->xa = NULL;\r\n}\r\nif (ctx->p) {\r\ndma_free_coherent(dev, ctx->p_size, ctx->p, ctx->dma_p);\r\nctx->p = NULL;\r\n}\r\nctx->p_size = 0;\r\nctx->g2 = false;\r\n}\r\nstatic int qat_dh_set_secret(struct crypto_kpp *tfm, void *buf,\r\nunsigned int len)\r\n{\r\nstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\r\nstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\r\nstruct dh params;\r\nint ret;\r\nif (crypto_dh_decode_key(buf, len, &params) < 0)\r\nreturn -EINVAL;\r\nqat_dh_clear_ctx(dev, ctx);\r\nret = qat_dh_set_params(ctx, &params);\r\nif (ret < 0)\r\nreturn ret;\r\nctx->xa = dma_zalloc_coherent(dev, ctx->p_size, &ctx->dma_xa,\r\nGFP_KERNEL);\r\nif (!ctx->xa) {\r\nqat_dh_clear_ctx(dev, ctx);\r\nreturn -ENOMEM;\r\n}\r\nmemcpy(ctx->xa + (ctx->p_size - params.key_size), params.key,\r\nparams.key_size);\r\nreturn 0;\r\n}\r\nstatic int qat_dh_max_size(struct crypto_kpp *tfm)\r\n{\r\nstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\r\nreturn ctx->p ? ctx->p_size : -EINVAL;\r\n}\r\nstatic int qat_dh_init_tfm(struct crypto_kpp *tfm)\r\n{\r\nstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\r\nstruct qat_crypto_instance *inst =\r\nqat_crypto_get_instance_node(get_current_node());\r\nif (!inst)\r\nreturn -EINVAL;\r\nctx->p_size = 0;\r\nctx->g2 = false;\r\nctx->inst = inst;\r\nreturn 0;\r\n}\r\nstatic void qat_dh_exit_tfm(struct crypto_kpp *tfm)\r\n{\r\nstruct qat_dh_ctx *ctx = kpp_tfm_ctx(tfm);\r\nstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\r\nqat_dh_clear_ctx(dev, ctx);\r\nqat_crypto_put_instance(ctx->inst);\r\n}\r\nstatic void qat_rsa_cb(struct icp_qat_fw_pke_resp *resp)\r\n{\r\nstruct qat_asym_request *req = (void *)(__force long)resp->opaque;\r\nstruct akcipher_request *areq = req->areq.rsa;\r\nstruct device *dev = &GET_DEV(req->ctx.rsa->inst->accel_dev);\r\nint err = ICP_QAT_FW_PKE_RESP_PKE_STAT_GET(\r\nresp->pke_resp_hdr.comn_resp_flags);\r\nerr = (err == ICP_QAT_FW_COMN_STATUS_FLAG_OK) ? 0 : -EINVAL;\r\nif (req->src_align)\r\ndma_free_coherent(dev, req->ctx.rsa->key_sz, req->src_align,\r\nreq->in.rsa.enc.m);\r\nelse\r\ndma_unmap_single(dev, req->in.rsa.enc.m, req->ctx.rsa->key_sz,\r\nDMA_TO_DEVICE);\r\nareq->dst_len = req->ctx.rsa->key_sz;\r\nif (req->dst_align) {\r\nscatterwalk_map_and_copy(req->dst_align, areq->dst, 0,\r\nareq->dst_len, 1);\r\ndma_free_coherent(dev, req->ctx.rsa->key_sz, req->dst_align,\r\nreq->out.rsa.enc.c);\r\n} else {\r\ndma_unmap_single(dev, req->out.rsa.enc.c, req->ctx.rsa->key_sz,\r\nDMA_FROM_DEVICE);\r\n}\r\ndma_unmap_single(dev, req->phy_in, sizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\ndma_unmap_single(dev, req->phy_out,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nakcipher_request_complete(areq, err);\r\n}\r\nvoid qat_alg_asym_callback(void *_resp)\r\n{\r\nstruct icp_qat_fw_pke_resp *resp = _resp;\r\nstruct qat_asym_request *areq = (void *)(__force long)resp->opaque;\r\nareq->cb(resp);\r\n}\r\nstatic unsigned long qat_rsa_enc_fn_id(unsigned int len)\r\n{\r\nunsigned int bitslen = len << 3;\r\nswitch (bitslen) {\r\ncase 512:\r\nreturn PKE_RSA_EP_512;\r\ncase 1024:\r\nreturn PKE_RSA_EP_1024;\r\ncase 1536:\r\nreturn PKE_RSA_EP_1536;\r\ncase 2048:\r\nreturn PKE_RSA_EP_2048;\r\ncase 3072:\r\nreturn PKE_RSA_EP_3072;\r\ncase 4096:\r\nreturn PKE_RSA_EP_4096;\r\ndefault:\r\nreturn 0;\r\n};\r\n}\r\nstatic unsigned long qat_rsa_dec_fn_id(unsigned int len)\r\n{\r\nunsigned int bitslen = len << 3;\r\nswitch (bitslen) {\r\ncase 512:\r\nreturn PKE_RSA_DP1_512;\r\ncase 1024:\r\nreturn PKE_RSA_DP1_1024;\r\ncase 1536:\r\nreturn PKE_RSA_DP1_1536;\r\ncase 2048:\r\nreturn PKE_RSA_DP1_2048;\r\ncase 3072:\r\nreturn PKE_RSA_DP1_3072;\r\ncase 4096:\r\nreturn PKE_RSA_DP1_4096;\r\ndefault:\r\nreturn 0;\r\n};\r\n}\r\nstatic unsigned long qat_rsa_dec_fn_id_crt(unsigned int len)\r\n{\r\nunsigned int bitslen = len << 3;\r\nswitch (bitslen) {\r\ncase 512:\r\nreturn PKE_RSA_DP2_512;\r\ncase 1024:\r\nreturn PKE_RSA_DP2_1024;\r\ncase 1536:\r\nreturn PKE_RSA_DP2_1536;\r\ncase 2048:\r\nreturn PKE_RSA_DP2_2048;\r\ncase 3072:\r\nreturn PKE_RSA_DP2_3072;\r\ncase 4096:\r\nreturn PKE_RSA_DP2_4096;\r\ndefault:\r\nreturn 0;\r\n};\r\n}\r\nstatic int qat_rsa_enc(struct akcipher_request *req)\r\n{\r\nstruct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nstruct qat_asym_request *qat_req =\r\nPTR_ALIGN(akcipher_request_ctx(req), 64);\r\nstruct icp_qat_fw_pke_request *msg = &qat_req->req;\r\nint ret, ctr = 0;\r\nif (unlikely(!ctx->n || !ctx->e))\r\nreturn -EINVAL;\r\nif (req->dst_len < ctx->key_sz) {\r\nreq->dst_len = ctx->key_sz;\r\nreturn -EOVERFLOW;\r\n}\r\nmemset(msg, '\0', sizeof(*msg));\r\nICP_QAT_FW_PKE_HDR_VALID_FLAG_SET(msg->pke_hdr,\r\nICP_QAT_FW_COMN_REQ_FLAG_SET);\r\nmsg->pke_hdr.cd_pars.func_id = qat_rsa_enc_fn_id(ctx->key_sz);\r\nif (unlikely(!msg->pke_hdr.cd_pars.func_id))\r\nreturn -EINVAL;\r\nqat_req->cb = qat_rsa_cb;\r\nqat_req->ctx.rsa = ctx;\r\nqat_req->areq.rsa = req;\r\nmsg->pke_hdr.service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_PKE;\r\nmsg->pke_hdr.comn_req_flags =\r\nICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_PTR_TYPE_FLAT,\r\nQAT_COMN_CD_FLD_TYPE_64BIT_ADR);\r\nqat_req->in.rsa.enc.e = ctx->dma_e;\r\nqat_req->in.rsa.enc.n = ctx->dma_n;\r\nret = -ENOMEM;\r\nif (sg_is_last(req->src) && req->src_len == ctx->key_sz) {\r\nqat_req->src_align = NULL;\r\nqat_req->in.rsa.enc.m = dma_map_single(dev, sg_virt(req->src),\r\nreq->src_len, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->in.rsa.enc.m)))\r\nreturn ret;\r\n} else {\r\nint shift = ctx->key_sz - req->src_len;\r\nqat_req->src_align = dma_zalloc_coherent(dev, ctx->key_sz,\r\n&qat_req->in.rsa.enc.m,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->src_align))\r\nreturn ret;\r\nscatterwalk_map_and_copy(qat_req->src_align + shift, req->src,\r\n0, req->src_len, 0);\r\n}\r\nif (sg_is_last(req->dst) && req->dst_len == ctx->key_sz) {\r\nqat_req->dst_align = NULL;\r\nqat_req->out.rsa.enc.c = dma_map_single(dev, sg_virt(req->dst),\r\nreq->dst_len,\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->out.rsa.enc.c)))\r\ngoto unmap_src;\r\n} else {\r\nqat_req->dst_align = dma_zalloc_coherent(dev, ctx->key_sz,\r\n&qat_req->out.rsa.enc.c,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->dst_align))\r\ngoto unmap_src;\r\n}\r\nqat_req->in.rsa.in_tab[3] = 0;\r\nqat_req->out.rsa.out_tab[1] = 0;\r\nqat_req->phy_in = dma_map_single(dev, &qat_req->in.rsa.enc.m,\r\nsizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_in)))\r\ngoto unmap_dst;\r\nqat_req->phy_out = dma_map_single(dev, &qat_req->out.rsa.enc.c,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_out)))\r\ngoto unmap_in_params;\r\nmsg->pke_mid.src_data_addr = qat_req->phy_in;\r\nmsg->pke_mid.dest_data_addr = qat_req->phy_out;\r\nmsg->pke_mid.opaque = (uint64_t)(__force long)qat_req;\r\nmsg->input_param_count = 3;\r\nmsg->output_param_count = 1;\r\ndo {\r\nret = adf_send_message(ctx->inst->pke_tx, (uint32_t *)msg);\r\n} while (ret == -EBUSY && ctr++ < 100);\r\nif (!ret)\r\nreturn -EINPROGRESS;\r\nif (!dma_mapping_error(dev, qat_req->phy_out))\r\ndma_unmap_single(dev, qat_req->phy_out,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nunmap_in_params:\r\nif (!dma_mapping_error(dev, qat_req->phy_in))\r\ndma_unmap_single(dev, qat_req->phy_in,\r\nsizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\nunmap_dst:\r\nif (qat_req->dst_align)\r\ndma_free_coherent(dev, ctx->key_sz, qat_req->dst_align,\r\nqat_req->out.rsa.enc.c);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->out.rsa.enc.c))\r\ndma_unmap_single(dev, qat_req->out.rsa.enc.c,\r\nctx->key_sz, DMA_FROM_DEVICE);\r\nunmap_src:\r\nif (qat_req->src_align)\r\ndma_free_coherent(dev, ctx->key_sz, qat_req->src_align,\r\nqat_req->in.rsa.enc.m);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->in.rsa.enc.m))\r\ndma_unmap_single(dev, qat_req->in.rsa.enc.m,\r\nctx->key_sz, DMA_TO_DEVICE);\r\nreturn ret;\r\n}\r\nstatic int qat_rsa_dec(struct akcipher_request *req)\r\n{\r\nstruct crypto_akcipher *tfm = crypto_akcipher_reqtfm(req);\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nstruct qat_asym_request *qat_req =\r\nPTR_ALIGN(akcipher_request_ctx(req), 64);\r\nstruct icp_qat_fw_pke_request *msg = &qat_req->req;\r\nint ret, ctr = 0;\r\nif (unlikely(!ctx->n || !ctx->d))\r\nreturn -EINVAL;\r\nif (req->dst_len < ctx->key_sz) {\r\nreq->dst_len = ctx->key_sz;\r\nreturn -EOVERFLOW;\r\n}\r\nmemset(msg, '\0', sizeof(*msg));\r\nICP_QAT_FW_PKE_HDR_VALID_FLAG_SET(msg->pke_hdr,\r\nICP_QAT_FW_COMN_REQ_FLAG_SET);\r\nmsg->pke_hdr.cd_pars.func_id = ctx->crt_mode ?\r\nqat_rsa_dec_fn_id_crt(ctx->key_sz) :\r\nqat_rsa_dec_fn_id(ctx->key_sz);\r\nif (unlikely(!msg->pke_hdr.cd_pars.func_id))\r\nreturn -EINVAL;\r\nqat_req->cb = qat_rsa_cb;\r\nqat_req->ctx.rsa = ctx;\r\nqat_req->areq.rsa = req;\r\nmsg->pke_hdr.service_type = ICP_QAT_FW_COMN_REQ_CPM_FW_PKE;\r\nmsg->pke_hdr.comn_req_flags =\r\nICP_QAT_FW_COMN_FLAGS_BUILD(QAT_COMN_PTR_TYPE_FLAT,\r\nQAT_COMN_CD_FLD_TYPE_64BIT_ADR);\r\nif (ctx->crt_mode) {\r\nqat_req->in.rsa.dec_crt.p = ctx->dma_p;\r\nqat_req->in.rsa.dec_crt.q = ctx->dma_q;\r\nqat_req->in.rsa.dec_crt.dp = ctx->dma_dp;\r\nqat_req->in.rsa.dec_crt.dq = ctx->dma_dq;\r\nqat_req->in.rsa.dec_crt.qinv = ctx->dma_qinv;\r\n} else {\r\nqat_req->in.rsa.dec.d = ctx->dma_d;\r\nqat_req->in.rsa.dec.n = ctx->dma_n;\r\n}\r\nret = -ENOMEM;\r\nif (sg_is_last(req->src) && req->src_len == ctx->key_sz) {\r\nqat_req->src_align = NULL;\r\nqat_req->in.rsa.dec.c = dma_map_single(dev, sg_virt(req->src),\r\nreq->dst_len, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->in.rsa.dec.c)))\r\nreturn ret;\r\n} else {\r\nint shift = ctx->key_sz - req->src_len;\r\nqat_req->src_align = dma_zalloc_coherent(dev, ctx->key_sz,\r\n&qat_req->in.rsa.dec.c,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->src_align))\r\nreturn ret;\r\nscatterwalk_map_and_copy(qat_req->src_align + shift, req->src,\r\n0, req->src_len, 0);\r\n}\r\nif (sg_is_last(req->dst) && req->dst_len == ctx->key_sz) {\r\nqat_req->dst_align = NULL;\r\nqat_req->out.rsa.dec.m = dma_map_single(dev, sg_virt(req->dst),\r\nreq->dst_len,\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->out.rsa.dec.m)))\r\ngoto unmap_src;\r\n} else {\r\nqat_req->dst_align = dma_zalloc_coherent(dev, ctx->key_sz,\r\n&qat_req->out.rsa.dec.m,\r\nGFP_KERNEL);\r\nif (unlikely(!qat_req->dst_align))\r\ngoto unmap_src;\r\n}\r\nif (ctx->crt_mode)\r\nqat_req->in.rsa.in_tab[6] = 0;\r\nelse\r\nqat_req->in.rsa.in_tab[3] = 0;\r\nqat_req->out.rsa.out_tab[1] = 0;\r\nqat_req->phy_in = dma_map_single(dev, &qat_req->in.rsa.dec.c,\r\nsizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_in)))\r\ngoto unmap_dst;\r\nqat_req->phy_out = dma_map_single(dev, &qat_req->out.rsa.dec.m,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(dev, qat_req->phy_out)))\r\ngoto unmap_in_params;\r\nmsg->pke_mid.src_data_addr = qat_req->phy_in;\r\nmsg->pke_mid.dest_data_addr = qat_req->phy_out;\r\nmsg->pke_mid.opaque = (uint64_t)(__force long)qat_req;\r\nif (ctx->crt_mode)\r\nmsg->input_param_count = 6;\r\nelse\r\nmsg->input_param_count = 3;\r\nmsg->output_param_count = 1;\r\ndo {\r\nret = adf_send_message(ctx->inst->pke_tx, (uint32_t *)msg);\r\n} while (ret == -EBUSY && ctr++ < 100);\r\nif (!ret)\r\nreturn -EINPROGRESS;\r\nif (!dma_mapping_error(dev, qat_req->phy_out))\r\ndma_unmap_single(dev, qat_req->phy_out,\r\nsizeof(struct qat_rsa_output_params),\r\nDMA_TO_DEVICE);\r\nunmap_in_params:\r\nif (!dma_mapping_error(dev, qat_req->phy_in))\r\ndma_unmap_single(dev, qat_req->phy_in,\r\nsizeof(struct qat_rsa_input_params),\r\nDMA_TO_DEVICE);\r\nunmap_dst:\r\nif (qat_req->dst_align)\r\ndma_free_coherent(dev, ctx->key_sz, qat_req->dst_align,\r\nqat_req->out.rsa.dec.m);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->out.rsa.dec.m))\r\ndma_unmap_single(dev, qat_req->out.rsa.dec.m,\r\nctx->key_sz, DMA_FROM_DEVICE);\r\nunmap_src:\r\nif (qat_req->src_align)\r\ndma_free_coherent(dev, ctx->key_sz, qat_req->src_align,\r\nqat_req->in.rsa.dec.c);\r\nelse\r\nif (!dma_mapping_error(dev, qat_req->in.rsa.dec.c))\r\ndma_unmap_single(dev, qat_req->in.rsa.dec.c,\r\nctx->key_sz, DMA_TO_DEVICE);\r\nreturn ret;\r\n}\r\nint qat_rsa_set_n(struct qat_rsa_ctx *ctx, const char *value, size_t vlen)\r\n{\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nconst char *ptr = value;\r\nint ret;\r\nwhile (!*ptr && vlen) {\r\nptr++;\r\nvlen--;\r\n}\r\nctx->key_sz = vlen;\r\nret = -EINVAL;\r\nif (!qat_rsa_enc_fn_id(ctx->key_sz))\r\ngoto err;\r\nret = -ENOMEM;\r\nctx->n = dma_zalloc_coherent(dev, ctx->key_sz, &ctx->dma_n, GFP_KERNEL);\r\nif (!ctx->n)\r\ngoto err;\r\nmemcpy(ctx->n, ptr, ctx->key_sz);\r\nreturn 0;\r\nerr:\r\nctx->key_sz = 0;\r\nctx->n = NULL;\r\nreturn ret;\r\n}\r\nint qat_rsa_set_e(struct qat_rsa_ctx *ctx, const char *value, size_t vlen)\r\n{\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nconst char *ptr = value;\r\nwhile (!*ptr && vlen) {\r\nptr++;\r\nvlen--;\r\n}\r\nif (!ctx->key_sz || !vlen || vlen > ctx->key_sz) {\r\nctx->e = NULL;\r\nreturn -EINVAL;\r\n}\r\nctx->e = dma_zalloc_coherent(dev, ctx->key_sz, &ctx->dma_e, GFP_KERNEL);\r\nif (!ctx->e)\r\nreturn -ENOMEM;\r\nmemcpy(ctx->e + (ctx->key_sz - vlen), ptr, vlen);\r\nreturn 0;\r\n}\r\nint qat_rsa_set_d(struct qat_rsa_ctx *ctx, const char *value, size_t vlen)\r\n{\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nconst char *ptr = value;\r\nint ret;\r\nwhile (!*ptr && vlen) {\r\nptr++;\r\nvlen--;\r\n}\r\nret = -EINVAL;\r\nif (!ctx->key_sz || !vlen || vlen > ctx->key_sz)\r\ngoto err;\r\nret = -ENOMEM;\r\nctx->d = dma_zalloc_coherent(dev, ctx->key_sz, &ctx->dma_d, GFP_KERNEL);\r\nif (!ctx->d)\r\ngoto err;\r\nmemcpy(ctx->d + (ctx->key_sz - vlen), ptr, vlen);\r\nreturn 0;\r\nerr:\r\nctx->d = NULL;\r\nreturn ret;\r\n}\r\nstatic void qat_rsa_drop_leading_zeros(const char **ptr, unsigned int *len)\r\n{\r\nwhile (!**ptr && *len) {\r\n(*ptr)++;\r\n(*len)--;\r\n}\r\n}\r\nstatic void qat_rsa_setkey_crt(struct qat_rsa_ctx *ctx, struct rsa_key *rsa_key)\r\n{\r\nstruct qat_crypto_instance *inst = ctx->inst;\r\nstruct device *dev = &GET_DEV(inst->accel_dev);\r\nconst char *ptr;\r\nunsigned int len;\r\nunsigned int half_key_sz = ctx->key_sz / 2;\r\nptr = rsa_key->p;\r\nlen = rsa_key->p_sz;\r\nqat_rsa_drop_leading_zeros(&ptr, &len);\r\nif (!len)\r\ngoto err;\r\nctx->p = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_p, GFP_KERNEL);\r\nif (!ctx->p)\r\ngoto err;\r\nmemcpy(ctx->p + (half_key_sz - len), ptr, len);\r\nptr = rsa_key->q;\r\nlen = rsa_key->q_sz;\r\nqat_rsa_drop_leading_zeros(&ptr, &len);\r\nif (!len)\r\ngoto free_p;\r\nctx->q = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_q, GFP_KERNEL);\r\nif (!ctx->q)\r\ngoto free_p;\r\nmemcpy(ctx->q + (half_key_sz - len), ptr, len);\r\nptr = rsa_key->dp;\r\nlen = rsa_key->dp_sz;\r\nqat_rsa_drop_leading_zeros(&ptr, &len);\r\nif (!len)\r\ngoto free_q;\r\nctx->dp = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_dp,\r\nGFP_KERNEL);\r\nif (!ctx->dp)\r\ngoto free_q;\r\nmemcpy(ctx->dp + (half_key_sz - len), ptr, len);\r\nptr = rsa_key->dq;\r\nlen = rsa_key->dq_sz;\r\nqat_rsa_drop_leading_zeros(&ptr, &len);\r\nif (!len)\r\ngoto free_dp;\r\nctx->dq = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_dq,\r\nGFP_KERNEL);\r\nif (!ctx->dq)\r\ngoto free_dp;\r\nmemcpy(ctx->dq + (half_key_sz - len), ptr, len);\r\nptr = rsa_key->qinv;\r\nlen = rsa_key->qinv_sz;\r\nqat_rsa_drop_leading_zeros(&ptr, &len);\r\nif (!len)\r\ngoto free_dq;\r\nctx->qinv = dma_zalloc_coherent(dev, half_key_sz, &ctx->dma_qinv,\r\nGFP_KERNEL);\r\nif (!ctx->qinv)\r\ngoto free_dq;\r\nmemcpy(ctx->qinv + (half_key_sz - len), ptr, len);\r\nctx->crt_mode = true;\r\nreturn;\r\nfree_dq:\r\nmemset(ctx->dq, '\0', half_key_sz);\r\ndma_free_coherent(dev, half_key_sz, ctx->dq, ctx->dma_dq);\r\nctx->dq = NULL;\r\nfree_dp:\r\nmemset(ctx->dp, '\0', half_key_sz);\r\ndma_free_coherent(dev, half_key_sz, ctx->dp, ctx->dma_dp);\r\nctx->dp = NULL;\r\nfree_q:\r\nmemset(ctx->q, '\0', half_key_sz);\r\ndma_free_coherent(dev, half_key_sz, ctx->q, ctx->dma_q);\r\nctx->q = NULL;\r\nfree_p:\r\nmemset(ctx->p, '\0', half_key_sz);\r\ndma_free_coherent(dev, half_key_sz, ctx->p, ctx->dma_p);\r\nctx->p = NULL;\r\nerr:\r\nctx->crt_mode = false;\r\n}\r\nstatic void qat_rsa_clear_ctx(struct device *dev, struct qat_rsa_ctx *ctx)\r\n{\r\nunsigned int half_key_sz = ctx->key_sz / 2;\r\nif (ctx->n)\r\ndma_free_coherent(dev, ctx->key_sz, ctx->n, ctx->dma_n);\r\nif (ctx->e)\r\ndma_free_coherent(dev, ctx->key_sz, ctx->e, ctx->dma_e);\r\nif (ctx->d) {\r\nmemset(ctx->d, '\0', ctx->key_sz);\r\ndma_free_coherent(dev, ctx->key_sz, ctx->d, ctx->dma_d);\r\n}\r\nif (ctx->p) {\r\nmemset(ctx->p, '\0', half_key_sz);\r\ndma_free_coherent(dev, half_key_sz, ctx->p, ctx->dma_p);\r\n}\r\nif (ctx->q) {\r\nmemset(ctx->q, '\0', half_key_sz);\r\ndma_free_coherent(dev, half_key_sz, ctx->q, ctx->dma_q);\r\n}\r\nif (ctx->dp) {\r\nmemset(ctx->dp, '\0', half_key_sz);\r\ndma_free_coherent(dev, half_key_sz, ctx->dp, ctx->dma_dp);\r\n}\r\nif (ctx->dq) {\r\nmemset(ctx->dq, '\0', half_key_sz);\r\ndma_free_coherent(dev, half_key_sz, ctx->dq, ctx->dma_dq);\r\n}\r\nif (ctx->qinv) {\r\nmemset(ctx->qinv, '\0', half_key_sz);\r\ndma_free_coherent(dev, half_key_sz, ctx->qinv, ctx->dma_qinv);\r\n}\r\nctx->n = NULL;\r\nctx->e = NULL;\r\nctx->d = NULL;\r\nctx->p = NULL;\r\nctx->q = NULL;\r\nctx->dp = NULL;\r\nctx->dq = NULL;\r\nctx->qinv = NULL;\r\nctx->crt_mode = false;\r\nctx->key_sz = 0;\r\n}\r\nstatic int qat_rsa_setkey(struct crypto_akcipher *tfm, const void *key,\r\nunsigned int keylen, bool private)\r\n{\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\r\nstruct rsa_key rsa_key;\r\nint ret;\r\nqat_rsa_clear_ctx(dev, ctx);\r\nif (private)\r\nret = rsa_parse_priv_key(&rsa_key, key, keylen);\r\nelse\r\nret = rsa_parse_pub_key(&rsa_key, key, keylen);\r\nif (ret < 0)\r\ngoto free;\r\nret = qat_rsa_set_n(ctx, rsa_key.n, rsa_key.n_sz);\r\nif (ret < 0)\r\ngoto free;\r\nret = qat_rsa_set_e(ctx, rsa_key.e, rsa_key.e_sz);\r\nif (ret < 0)\r\ngoto free;\r\nif (private) {\r\nret = qat_rsa_set_d(ctx, rsa_key.d, rsa_key.d_sz);\r\nif (ret < 0)\r\ngoto free;\r\nqat_rsa_setkey_crt(ctx, &rsa_key);\r\n}\r\nif (!ctx->n || !ctx->e) {\r\nret = -EINVAL;\r\ngoto free;\r\n}\r\nif (private && !ctx->d) {\r\nret = -EINVAL;\r\ngoto free;\r\n}\r\nreturn 0;\r\nfree:\r\nqat_rsa_clear_ctx(dev, ctx);\r\nreturn ret;\r\n}\r\nstatic int qat_rsa_setpubkey(struct crypto_akcipher *tfm, const void *key,\r\nunsigned int keylen)\r\n{\r\nreturn qat_rsa_setkey(tfm, key, keylen, false);\r\n}\r\nstatic int qat_rsa_setprivkey(struct crypto_akcipher *tfm, const void *key,\r\nunsigned int keylen)\r\n{\r\nreturn qat_rsa_setkey(tfm, key, keylen, true);\r\n}\r\nstatic int qat_rsa_max_size(struct crypto_akcipher *tfm)\r\n{\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nreturn (ctx->n) ? ctx->key_sz : -EINVAL;\r\n}\r\nstatic int qat_rsa_init_tfm(struct crypto_akcipher *tfm)\r\n{\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct qat_crypto_instance *inst =\r\nqat_crypto_get_instance_node(get_current_node());\r\nif (!inst)\r\nreturn -EINVAL;\r\nctx->key_sz = 0;\r\nctx->inst = inst;\r\nreturn 0;\r\n}\r\nstatic void qat_rsa_exit_tfm(struct crypto_akcipher *tfm)\r\n{\r\nstruct qat_rsa_ctx *ctx = akcipher_tfm_ctx(tfm);\r\nstruct device *dev = &GET_DEV(ctx->inst->accel_dev);\r\nif (ctx->n)\r\ndma_free_coherent(dev, ctx->key_sz, ctx->n, ctx->dma_n);\r\nif (ctx->e)\r\ndma_free_coherent(dev, ctx->key_sz, ctx->e, ctx->dma_e);\r\nif (ctx->d) {\r\nmemset(ctx->d, '\0', ctx->key_sz);\r\ndma_free_coherent(dev, ctx->key_sz, ctx->d, ctx->dma_d);\r\n}\r\nqat_crypto_put_instance(ctx->inst);\r\nctx->n = NULL;\r\nctx->e = NULL;\r\nctx->d = NULL;\r\n}\r\nint qat_asym_algs_register(void)\r\n{\r\nint ret = 0;\r\nmutex_lock(&algs_lock);\r\nif (++active_devs == 1) {\r\nrsa.base.cra_flags = 0;\r\nret = crypto_register_akcipher(&rsa);\r\nif (ret)\r\ngoto unlock;\r\nret = crypto_register_kpp(&dh);\r\n}\r\nunlock:\r\nmutex_unlock(&algs_lock);\r\nreturn ret;\r\n}\r\nvoid qat_asym_algs_unregister(void)\r\n{\r\nmutex_lock(&algs_lock);\r\nif (--active_devs == 0) {\r\ncrypto_unregister_akcipher(&rsa);\r\ncrypto_unregister_kpp(&dh);\r\n}\r\nmutex_unlock(&algs_lock);\r\n}
