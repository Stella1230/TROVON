static struct vgic_irq *vgic_add_lpi(struct kvm *kvm, u32 intid)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct vgic_irq *irq = vgic_get_irq(kvm, NULL, intid), *oldirq;\r\nif (irq)\r\nreturn irq;\r\nirq = kzalloc(sizeof(struct vgic_irq), GFP_KERNEL);\r\nif (!irq)\r\nreturn ERR_PTR(-ENOMEM);\r\nINIT_LIST_HEAD(&irq->lpi_list);\r\nINIT_LIST_HEAD(&irq->ap_list);\r\nspin_lock_init(&irq->irq_lock);\r\nirq->config = VGIC_CONFIG_EDGE;\r\nkref_init(&irq->refcount);\r\nirq->intid = intid;\r\nspin_lock(&dist->lpi_list_lock);\r\nlist_for_each_entry(oldirq, &dist->lpi_list_head, lpi_list) {\r\nif (oldirq->intid != intid)\r\ncontinue;\r\nkfree(irq);\r\nirq = oldirq;\r\nvgic_get_irq_kref(irq);\r\ngoto out_unlock;\r\n}\r\nlist_add_tail(&irq->lpi_list, &dist->lpi_list_head);\r\ndist->lpi_list_count++;\r\nout_unlock:\r\nspin_unlock(&dist->lpi_list_lock);\r\nreturn irq;\r\n}\r\nstatic struct its_device *find_its_device(struct vgic_its *its, u32 device_id)\r\n{\r\nstruct its_device *device;\r\nlist_for_each_entry(device, &its->device_list, dev_list)\r\nif (device_id == device->device_id)\r\nreturn device;\r\nreturn NULL;\r\n}\r\nstatic struct its_itte *find_itte(struct vgic_its *its, u32 device_id,\r\nu32 event_id)\r\n{\r\nstruct its_device *device;\r\nstruct its_itte *itte;\r\ndevice = find_its_device(its, device_id);\r\nif (device == NULL)\r\nreturn NULL;\r\nlist_for_each_entry(itte, &device->itt_head, itte_list)\r\nif (itte->event_id == event_id)\r\nreturn itte;\r\nreturn NULL;\r\n}\r\nstatic struct its_collection *find_collection(struct vgic_its *its, int coll_id)\r\n{\r\nstruct its_collection *collection;\r\nlist_for_each_entry(collection, &its->collection_list, coll_list) {\r\nif (coll_id == collection->collection_id)\r\nreturn collection;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int update_lpi_config(struct kvm *kvm, struct vgic_irq *irq,\r\nstruct kvm_vcpu *filter_vcpu)\r\n{\r\nu64 propbase = PROPBASER_ADDRESS(kvm->arch.vgic.propbaser);\r\nu8 prop;\r\nint ret;\r\nret = kvm_read_guest(kvm, propbase + irq->intid - GIC_LPI_OFFSET,\r\n&prop, 1);\r\nif (ret)\r\nreturn ret;\r\nspin_lock(&irq->irq_lock);\r\nif (!filter_vcpu || filter_vcpu == irq->target_vcpu) {\r\nirq->priority = LPI_PROP_PRIORITY(prop);\r\nirq->enabled = LPI_PROP_ENABLE_BIT(prop);\r\nvgic_queue_irq_unlock(kvm, irq);\r\n} else {\r\nspin_unlock(&irq->irq_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic int vgic_copy_lpi_list(struct kvm *kvm, u32 **intid_ptr)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nstruct vgic_irq *irq;\r\nu32 *intids;\r\nint irq_count = dist->lpi_list_count, i = 0;\r\nintids = kmalloc_array(irq_count, sizeof(intids[0]), GFP_KERNEL);\r\nif (!intids)\r\nreturn -ENOMEM;\r\nspin_lock(&dist->lpi_list_lock);\r\nlist_for_each_entry(irq, &dist->lpi_list_head, lpi_list) {\r\nintids[i] = irq->intid;\r\nif (++i == irq_count)\r\nbreak;\r\n}\r\nspin_unlock(&dist->lpi_list_lock);\r\n*intid_ptr = intids;\r\nreturn irq_count;\r\n}\r\nstatic void update_affinity_itte(struct kvm *kvm, struct its_itte *itte)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nif (!its_is_collection_mapped(itte->collection))\r\nreturn;\r\nvcpu = kvm_get_vcpu(kvm, itte->collection->target_addr);\r\nspin_lock(&itte->irq->irq_lock);\r\nitte->irq->target_vcpu = vcpu;\r\nspin_unlock(&itte->irq->irq_lock);\r\n}\r\nstatic void update_affinity_collection(struct kvm *kvm, struct vgic_its *its,\r\nstruct its_collection *coll)\r\n{\r\nstruct its_device *device;\r\nstruct its_itte *itte;\r\nfor_each_lpi_its(device, itte, its) {\r\nif (!itte->collection || coll != itte->collection)\r\ncontinue;\r\nupdate_affinity_itte(kvm, itte);\r\n}\r\n}\r\nstatic u32 max_lpis_propbaser(u64 propbaser)\r\n{\r\nint nr_idbits = (propbaser & 0x1f) + 1;\r\nreturn 1U << min(nr_idbits, INTERRUPT_ID_BITS_ITS);\r\n}\r\nstatic int its_sync_lpi_pending_table(struct kvm_vcpu *vcpu)\r\n{\r\ngpa_t pendbase = PENDBASER_ADDRESS(vcpu->arch.vgic_cpu.pendbaser);\r\nstruct vgic_irq *irq;\r\nint last_byte_offset = -1;\r\nint ret = 0;\r\nu32 *intids;\r\nint nr_irqs, i;\r\nnr_irqs = vgic_copy_lpi_list(vcpu->kvm, &intids);\r\nif (nr_irqs < 0)\r\nreturn nr_irqs;\r\nfor (i = 0; i < nr_irqs; i++) {\r\nint byte_offset, bit_nr;\r\nu8 pendmask;\r\nbyte_offset = intids[i] / BITS_PER_BYTE;\r\nbit_nr = intids[i] % BITS_PER_BYTE;\r\nif (byte_offset != last_byte_offset) {\r\nret = kvm_read_guest(vcpu->kvm, pendbase + byte_offset,\r\n&pendmask, 1);\r\nif (ret) {\r\nkfree(intids);\r\nreturn ret;\r\n}\r\nlast_byte_offset = byte_offset;\r\n}\r\nirq = vgic_get_irq(vcpu->kvm, NULL, intids[i]);\r\nspin_lock(&irq->irq_lock);\r\nirq->pending = pendmask & (1U << bit_nr);\r\nvgic_queue_irq_unlock(vcpu->kvm, irq);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nkfree(intids);\r\nreturn ret;\r\n}\r\nstatic unsigned long vgic_mmio_read_its_ctlr(struct kvm *vcpu,\r\nstruct vgic_its *its,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 reg = 0;\r\nmutex_lock(&its->cmd_lock);\r\nif (its->creadr == its->cwriter)\r\nreg |= GITS_CTLR_QUIESCENT;\r\nif (its->enabled)\r\nreg |= GITS_CTLR_ENABLE;\r\nmutex_unlock(&its->cmd_lock);\r\nreturn reg;\r\n}\r\nstatic void vgic_mmio_write_its_ctlr(struct kvm *kvm, struct vgic_its *its,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nits->enabled = !!(val & GITS_CTLR_ENABLE);\r\n}\r\nstatic unsigned long vgic_mmio_read_its_typer(struct kvm *kvm,\r\nstruct vgic_its *its,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu64 reg = GITS_TYPER_PLPIS;\r\nreg |= 0x0f << GITS_TYPER_DEVBITS_SHIFT;\r\nreg |= 0x0f << GITS_TYPER_IDBITS_SHIFT;\r\nreturn extract_bytes(reg, addr & 7, len);\r\n}\r\nstatic unsigned long vgic_mmio_read_its_iidr(struct kvm *kvm,\r\nstruct vgic_its *its,\r\ngpa_t addr, unsigned int len)\r\n{\r\nreturn (PRODUCT_ID_KVM << 24) | (IMPLEMENTER_ARM << 0);\r\n}\r\nstatic unsigned long vgic_mmio_read_its_idregs(struct kvm *kvm,\r\nstruct vgic_its *its,\r\ngpa_t addr, unsigned int len)\r\n{\r\nswitch (addr & 0xffff) {\r\ncase GITS_PIDR0:\r\nreturn 0x92;\r\ncase GITS_PIDR1:\r\nreturn 0xb4;\r\ncase GITS_PIDR2:\r\nreturn GIC_PIDR2_ARCH_GICv3 | 0x0b;\r\ncase GITS_PIDR4:\r\nreturn 0x40;\r\ncase GITS_CIDR0:\r\nreturn 0x0d;\r\ncase GITS_CIDR1:\r\nreturn 0xf0;\r\ncase GITS_CIDR2:\r\nreturn 0x05;\r\ncase GITS_CIDR3:\r\nreturn 0xb1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,\r\nu32 devid, u32 eventid)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nstruct its_itte *itte;\r\nif (!its->enabled)\r\nreturn -EBUSY;\r\nitte = find_itte(its, devid, eventid);\r\nif (!itte || !its_is_collection_mapped(itte->collection))\r\nreturn E_ITS_INT_UNMAPPED_INTERRUPT;\r\nvcpu = kvm_get_vcpu(kvm, itte->collection->target_addr);\r\nif (!vcpu)\r\nreturn E_ITS_INT_UNMAPPED_INTERRUPT;\r\nif (!vcpu->arch.vgic_cpu.lpis_enabled)\r\nreturn -EBUSY;\r\nspin_lock(&itte->irq->irq_lock);\r\nitte->irq->pending = true;\r\nvgic_queue_irq_unlock(kvm, itte->irq);\r\nreturn 0;\r\n}\r\nstatic struct vgic_io_device *vgic_get_its_iodev(struct kvm_io_device *dev)\r\n{\r\nstruct vgic_io_device *iodev;\r\nif (dev->ops != &kvm_io_gic_ops)\r\nreturn NULL;\r\niodev = container_of(dev, struct vgic_io_device, dev);\r\nif (iodev->iodev_type != IODEV_ITS)\r\nreturn NULL;\r\nreturn iodev;\r\n}\r\nint vgic_its_inject_msi(struct kvm *kvm, struct kvm_msi *msi)\r\n{\r\nu64 address;\r\nstruct kvm_io_device *kvm_io_dev;\r\nstruct vgic_io_device *iodev;\r\nint ret;\r\nif (!vgic_has_its(kvm))\r\nreturn -ENODEV;\r\nif (!(msi->flags & KVM_MSI_VALID_DEVID))\r\nreturn -EINVAL;\r\naddress = (u64)msi->address_hi << 32 | msi->address_lo;\r\nkvm_io_dev = kvm_io_bus_get_dev(kvm, KVM_MMIO_BUS, address);\r\nif (!kvm_io_dev)\r\nreturn -EINVAL;\r\niodev = vgic_get_its_iodev(kvm_io_dev);\r\nif (!iodev)\r\nreturn -EINVAL;\r\nmutex_lock(&iodev->its->its_lock);\r\nret = vgic_its_trigger_msi(kvm, iodev->its, msi->devid, msi->data);\r\nmutex_unlock(&iodev->its->its_lock);\r\nif (ret < 0)\r\nreturn ret;\r\nif (ret)\r\nreturn 0;\r\nelse\r\nreturn 1;\r\n}\r\nstatic void its_free_itte(struct kvm *kvm, struct its_itte *itte)\r\n{\r\nlist_del(&itte->itte_list);\r\nif (itte->irq)\r\nvgic_put_irq(kvm, itte->irq);\r\nkfree(itte);\r\n}\r\nstatic u64 its_cmd_mask_field(u64 *its_cmd, int word, int shift, int size)\r\n{\r\nreturn (le64_to_cpu(its_cmd[word]) >> shift) & (BIT_ULL(size) - 1);\r\n}\r\nstatic int vgic_its_cmd_handle_discard(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nu32 device_id = its_cmd_get_deviceid(its_cmd);\r\nu32 event_id = its_cmd_get_id(its_cmd);\r\nstruct its_itte *itte;\r\nitte = find_itte(its, device_id, event_id);\r\nif (itte && itte->collection) {\r\nits_free_itte(kvm, itte);\r\nreturn 0;\r\n}\r\nreturn E_ITS_DISCARD_UNMAPPED_INTERRUPT;\r\n}\r\nstatic int vgic_its_cmd_handle_movi(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nu32 device_id = its_cmd_get_deviceid(its_cmd);\r\nu32 event_id = its_cmd_get_id(its_cmd);\r\nu32 coll_id = its_cmd_get_collection(its_cmd);\r\nstruct kvm_vcpu *vcpu;\r\nstruct its_itte *itte;\r\nstruct its_collection *collection;\r\nitte = find_itte(its, device_id, event_id);\r\nif (!itte)\r\nreturn E_ITS_MOVI_UNMAPPED_INTERRUPT;\r\nif (!its_is_collection_mapped(itte->collection))\r\nreturn E_ITS_MOVI_UNMAPPED_COLLECTION;\r\ncollection = find_collection(its, coll_id);\r\nif (!its_is_collection_mapped(collection))\r\nreturn E_ITS_MOVI_UNMAPPED_COLLECTION;\r\nitte->collection = collection;\r\nvcpu = kvm_get_vcpu(kvm, collection->target_addr);\r\nspin_lock(&itte->irq->irq_lock);\r\nitte->irq->target_vcpu = vcpu;\r\nspin_unlock(&itte->irq->irq_lock);\r\nreturn 0;\r\n}\r\nstatic bool vgic_its_check_id(struct vgic_its *its, u64 baser, int id)\r\n{\r\nint l1_tbl_size = GITS_BASER_NR_PAGES(baser) * SZ_64K;\r\nint index;\r\nu64 indirect_ptr;\r\ngfn_t gfn;\r\nif (!(baser & GITS_BASER_INDIRECT)) {\r\nphys_addr_t addr;\r\nif (id >= (l1_tbl_size / GITS_BASER_ENTRY_SIZE(baser)))\r\nreturn false;\r\naddr = BASER_ADDRESS(baser) + id * GITS_BASER_ENTRY_SIZE(baser);\r\ngfn = addr >> PAGE_SHIFT;\r\nreturn kvm_is_visible_gfn(its->dev->kvm, gfn);\r\n}\r\nindex = id / (SZ_64K / GITS_BASER_ENTRY_SIZE(baser));\r\nif (index >= (l1_tbl_size / sizeof(u64)))\r\nreturn false;\r\nif (kvm_read_guest(its->dev->kvm,\r\nBASER_ADDRESS(baser) + index * sizeof(indirect_ptr),\r\n&indirect_ptr, sizeof(indirect_ptr)))\r\nreturn false;\r\nindirect_ptr = le64_to_cpu(indirect_ptr);\r\nif (!(indirect_ptr & BIT_ULL(63)))\r\nreturn false;\r\nindirect_ptr &= GENMASK_ULL(51, 16);\r\nindex = id % (SZ_64K / GITS_BASER_ENTRY_SIZE(baser));\r\nindirect_ptr += index * GITS_BASER_ENTRY_SIZE(baser);\r\ngfn = indirect_ptr >> PAGE_SHIFT;\r\nreturn kvm_is_visible_gfn(its->dev->kvm, gfn);\r\n}\r\nstatic int vgic_its_alloc_collection(struct vgic_its *its,\r\nstruct its_collection **colp,\r\nu32 coll_id)\r\n{\r\nstruct its_collection *collection;\r\nif (!vgic_its_check_id(its, its->baser_coll_table, coll_id))\r\nreturn E_ITS_MAPC_COLLECTION_OOR;\r\ncollection = kzalloc(sizeof(*collection), GFP_KERNEL);\r\ncollection->collection_id = coll_id;\r\ncollection->target_addr = COLLECTION_NOT_MAPPED;\r\nlist_add_tail(&collection->coll_list, &its->collection_list);\r\n*colp = collection;\r\nreturn 0;\r\n}\r\nstatic void vgic_its_free_collection(struct vgic_its *its, u32 coll_id)\r\n{\r\nstruct its_collection *collection;\r\nstruct its_device *device;\r\nstruct its_itte *itte;\r\ncollection = find_collection(its, coll_id);\r\nif (!collection)\r\nreturn;\r\nfor_each_lpi_its(device, itte, its)\r\nif (itte->collection &&\r\nitte->collection->collection_id == coll_id)\r\nitte->collection = NULL;\r\nlist_del(&collection->coll_list);\r\nkfree(collection);\r\n}\r\nstatic int vgic_its_cmd_handle_mapi(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nu32 device_id = its_cmd_get_deviceid(its_cmd);\r\nu32 event_id = its_cmd_get_id(its_cmd);\r\nu32 coll_id = its_cmd_get_collection(its_cmd);\r\nstruct its_itte *itte;\r\nstruct its_device *device;\r\nstruct its_collection *collection, *new_coll = NULL;\r\nint lpi_nr;\r\nstruct vgic_irq *irq;\r\ndevice = find_its_device(its, device_id);\r\nif (!device)\r\nreturn E_ITS_MAPTI_UNMAPPED_DEVICE;\r\nif (its_cmd_get_command(its_cmd) == GITS_CMD_MAPTI)\r\nlpi_nr = its_cmd_get_physical_id(its_cmd);\r\nelse\r\nlpi_nr = event_id;\r\nif (lpi_nr < GIC_LPI_OFFSET ||\r\nlpi_nr >= max_lpis_propbaser(kvm->arch.vgic.propbaser))\r\nreturn E_ITS_MAPTI_PHYSICALID_OOR;\r\nif (find_itte(its, device_id, event_id))\r\nreturn 0;\r\ncollection = find_collection(its, coll_id);\r\nif (!collection) {\r\nint ret = vgic_its_alloc_collection(its, &collection, coll_id);\r\nif (ret)\r\nreturn ret;\r\nnew_coll = collection;\r\n}\r\nitte = kzalloc(sizeof(struct its_itte), GFP_KERNEL);\r\nif (!itte) {\r\nif (new_coll)\r\nvgic_its_free_collection(its, coll_id);\r\nreturn -ENOMEM;\r\n}\r\nitte->event_id = event_id;\r\nlist_add_tail(&itte->itte_list, &device->itt_head);\r\nitte->collection = collection;\r\nitte->lpi = lpi_nr;\r\nirq = vgic_add_lpi(kvm, lpi_nr);\r\nif (IS_ERR(irq)) {\r\nif (new_coll)\r\nvgic_its_free_collection(its, coll_id);\r\nits_free_itte(kvm, itte);\r\nreturn PTR_ERR(irq);\r\n}\r\nitte->irq = irq;\r\nupdate_affinity_itte(kvm, itte);\r\nupdate_lpi_config(kvm, itte->irq, NULL);\r\nreturn 0;\r\n}\r\nstatic void vgic_its_unmap_device(struct kvm *kvm, struct its_device *device)\r\n{\r\nstruct its_itte *itte, *temp;\r\nlist_for_each_entry_safe(itte, temp, &device->itt_head, itte_list)\r\nits_free_itte(kvm, itte);\r\nlist_del(&device->dev_list);\r\nkfree(device);\r\n}\r\nstatic int vgic_its_cmd_handle_mapd(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nu32 device_id = its_cmd_get_deviceid(its_cmd);\r\nbool valid = its_cmd_get_validbit(its_cmd);\r\nstruct its_device *device;\r\nif (!vgic_its_check_id(its, its->baser_device_table, device_id))\r\nreturn E_ITS_MAPD_DEVICE_OOR;\r\ndevice = find_its_device(its, device_id);\r\nif (device)\r\nvgic_its_unmap_device(kvm, device);\r\nif (!valid)\r\nreturn 0;\r\ndevice = kzalloc(sizeof(struct its_device), GFP_KERNEL);\r\nif (!device)\r\nreturn -ENOMEM;\r\ndevice->device_id = device_id;\r\nINIT_LIST_HEAD(&device->itt_head);\r\nlist_add_tail(&device->dev_list, &its->device_list);\r\nreturn 0;\r\n}\r\nstatic int vgic_its_cmd_handle_mapc(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nu16 coll_id;\r\nu32 target_addr;\r\nstruct its_collection *collection;\r\nbool valid;\r\nvalid = its_cmd_get_validbit(its_cmd);\r\ncoll_id = its_cmd_get_collection(its_cmd);\r\ntarget_addr = its_cmd_get_target_addr(its_cmd);\r\nif (target_addr >= atomic_read(&kvm->online_vcpus))\r\nreturn E_ITS_MAPC_PROCNUM_OOR;\r\nif (!valid) {\r\nvgic_its_free_collection(its, coll_id);\r\n} else {\r\ncollection = find_collection(its, coll_id);\r\nif (!collection) {\r\nint ret;\r\nret = vgic_its_alloc_collection(its, &collection,\r\ncoll_id);\r\nif (ret)\r\nreturn ret;\r\ncollection->target_addr = target_addr;\r\n} else {\r\ncollection->target_addr = target_addr;\r\nupdate_affinity_collection(kvm, its, collection);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int vgic_its_cmd_handle_clear(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nu32 device_id = its_cmd_get_deviceid(its_cmd);\r\nu32 event_id = its_cmd_get_id(its_cmd);\r\nstruct its_itte *itte;\r\nitte = find_itte(its, device_id, event_id);\r\nif (!itte)\r\nreturn E_ITS_CLEAR_UNMAPPED_INTERRUPT;\r\nitte->irq->pending = false;\r\nreturn 0;\r\n}\r\nstatic int vgic_its_cmd_handle_inv(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nu32 device_id = its_cmd_get_deviceid(its_cmd);\r\nu32 event_id = its_cmd_get_id(its_cmd);\r\nstruct its_itte *itte;\r\nitte = find_itte(its, device_id, event_id);\r\nif (!itte)\r\nreturn E_ITS_INV_UNMAPPED_INTERRUPT;\r\nreturn update_lpi_config(kvm, itte->irq, NULL);\r\n}\r\nstatic int vgic_its_cmd_handle_invall(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nu32 coll_id = its_cmd_get_collection(its_cmd);\r\nstruct its_collection *collection;\r\nstruct kvm_vcpu *vcpu;\r\nstruct vgic_irq *irq;\r\nu32 *intids;\r\nint irq_count, i;\r\ncollection = find_collection(its, coll_id);\r\nif (!its_is_collection_mapped(collection))\r\nreturn E_ITS_INVALL_UNMAPPED_COLLECTION;\r\nvcpu = kvm_get_vcpu(kvm, collection->target_addr);\r\nirq_count = vgic_copy_lpi_list(kvm, &intids);\r\nif (irq_count < 0)\r\nreturn irq_count;\r\nfor (i = 0; i < irq_count; i++) {\r\nirq = vgic_get_irq(kvm, NULL, intids[i]);\r\nif (!irq)\r\ncontinue;\r\nupdate_lpi_config(kvm, irq, vcpu);\r\nvgic_put_irq(kvm, irq);\r\n}\r\nkfree(intids);\r\nreturn 0;\r\n}\r\nstatic int vgic_its_cmd_handle_movall(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nstruct vgic_dist *dist = &kvm->arch.vgic;\r\nu32 target1_addr = its_cmd_get_target_addr(its_cmd);\r\nu32 target2_addr = its_cmd_mask_field(its_cmd, 3, 16, 32);\r\nstruct kvm_vcpu *vcpu1, *vcpu2;\r\nstruct vgic_irq *irq;\r\nif (target1_addr >= atomic_read(&kvm->online_vcpus) ||\r\ntarget2_addr >= atomic_read(&kvm->online_vcpus))\r\nreturn E_ITS_MOVALL_PROCNUM_OOR;\r\nif (target1_addr == target2_addr)\r\nreturn 0;\r\nvcpu1 = kvm_get_vcpu(kvm, target1_addr);\r\nvcpu2 = kvm_get_vcpu(kvm, target2_addr);\r\nspin_lock(&dist->lpi_list_lock);\r\nlist_for_each_entry(irq, &dist->lpi_list_head, lpi_list) {\r\nspin_lock(&irq->irq_lock);\r\nif (irq->target_vcpu == vcpu1)\r\nirq->target_vcpu = vcpu2;\r\nspin_unlock(&irq->irq_lock);\r\n}\r\nspin_unlock(&dist->lpi_list_lock);\r\nreturn 0;\r\n}\r\nstatic int vgic_its_cmd_handle_int(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nu32 msi_data = its_cmd_get_id(its_cmd);\r\nu64 msi_devid = its_cmd_get_deviceid(its_cmd);\r\nreturn vgic_its_trigger_msi(kvm, its, msi_devid, msi_data);\r\n}\r\nstatic int vgic_its_handle_command(struct kvm *kvm, struct vgic_its *its,\r\nu64 *its_cmd)\r\n{\r\nint ret = -ENODEV;\r\nmutex_lock(&its->its_lock);\r\nswitch (its_cmd_get_command(its_cmd)) {\r\ncase GITS_CMD_MAPD:\r\nret = vgic_its_cmd_handle_mapd(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_MAPC:\r\nret = vgic_its_cmd_handle_mapc(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_MAPI:\r\nret = vgic_its_cmd_handle_mapi(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_MAPTI:\r\nret = vgic_its_cmd_handle_mapi(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_MOVI:\r\nret = vgic_its_cmd_handle_movi(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_DISCARD:\r\nret = vgic_its_cmd_handle_discard(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_CLEAR:\r\nret = vgic_its_cmd_handle_clear(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_MOVALL:\r\nret = vgic_its_cmd_handle_movall(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_INT:\r\nret = vgic_its_cmd_handle_int(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_INV:\r\nret = vgic_its_cmd_handle_inv(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_INVALL:\r\nret = vgic_its_cmd_handle_invall(kvm, its, its_cmd);\r\nbreak;\r\ncase GITS_CMD_SYNC:\r\nret = 0;\r\nbreak;\r\n}\r\nmutex_unlock(&its->its_lock);\r\nreturn ret;\r\n}\r\nstatic u64 vgic_sanitise_its_baser(u64 reg)\r\n{\r\nreg = vgic_sanitise_field(reg, GITS_BASER_SHAREABILITY_MASK,\r\nGITS_BASER_SHAREABILITY_SHIFT,\r\nvgic_sanitise_shareability);\r\nreg = vgic_sanitise_field(reg, GITS_BASER_INNER_CACHEABILITY_MASK,\r\nGITS_BASER_INNER_CACHEABILITY_SHIFT,\r\nvgic_sanitise_inner_cacheability);\r\nreg = vgic_sanitise_field(reg, GITS_BASER_OUTER_CACHEABILITY_MASK,\r\nGITS_BASER_OUTER_CACHEABILITY_SHIFT,\r\nvgic_sanitise_outer_cacheability);\r\nreg &= ~GENMASK_ULL(15, 12);\r\nreg = (reg & ~GITS_BASER_PAGE_SIZE_MASK) | GITS_BASER_PAGE_SIZE_64K;\r\nreturn reg;\r\n}\r\nstatic u64 vgic_sanitise_its_cbaser(u64 reg)\r\n{\r\nreg = vgic_sanitise_field(reg, GITS_CBASER_SHAREABILITY_MASK,\r\nGITS_CBASER_SHAREABILITY_SHIFT,\r\nvgic_sanitise_shareability);\r\nreg = vgic_sanitise_field(reg, GITS_CBASER_INNER_CACHEABILITY_MASK,\r\nGITS_CBASER_INNER_CACHEABILITY_SHIFT,\r\nvgic_sanitise_inner_cacheability);\r\nreg = vgic_sanitise_field(reg, GITS_CBASER_OUTER_CACHEABILITY_MASK,\r\nGITS_CBASER_OUTER_CACHEABILITY_SHIFT,\r\nvgic_sanitise_outer_cacheability);\r\nreg &= ~(GENMASK_ULL(51, 48) | GENMASK_ULL(15, 12));\r\nreturn reg;\r\n}\r\nstatic unsigned long vgic_mmio_read_its_cbaser(struct kvm *kvm,\r\nstruct vgic_its *its,\r\ngpa_t addr, unsigned int len)\r\n{\r\nreturn extract_bytes(its->cbaser, addr & 7, len);\r\n}\r\nstatic void vgic_mmio_write_its_cbaser(struct kvm *kvm, struct vgic_its *its,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nif (its->enabled)\r\nreturn;\r\nmutex_lock(&its->cmd_lock);\r\nits->cbaser = update_64bit_reg(its->cbaser, addr & 7, len, val);\r\nits->cbaser = vgic_sanitise_its_cbaser(its->cbaser);\r\nits->creadr = 0;\r\nits->cwriter = its->creadr;\r\nmutex_unlock(&its->cmd_lock);\r\n}\r\nstatic void vgic_mmio_write_its_cwriter(struct kvm *kvm, struct vgic_its *its,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\ngpa_t cbaser;\r\nu64 cmd_buf[4];\r\nu32 reg;\r\nif (!its)\r\nreturn;\r\nmutex_lock(&its->cmd_lock);\r\nreg = update_64bit_reg(its->cwriter, addr & 7, len, val);\r\nreg = ITS_CMD_OFFSET(reg);\r\nif (reg >= ITS_CMD_BUFFER_SIZE(its->cbaser)) {\r\nmutex_unlock(&its->cmd_lock);\r\nreturn;\r\n}\r\nits->cwriter = reg;\r\ncbaser = CBASER_ADDRESS(its->cbaser);\r\nwhile (its->cwriter != its->creadr) {\r\nint ret = kvm_read_guest(kvm, cbaser + its->creadr,\r\ncmd_buf, ITS_CMD_SIZE);\r\nif (!ret)\r\nvgic_its_handle_command(kvm, its, cmd_buf);\r\nits->creadr += ITS_CMD_SIZE;\r\nif (its->creadr == ITS_CMD_BUFFER_SIZE(its->cbaser))\r\nits->creadr = 0;\r\n}\r\nmutex_unlock(&its->cmd_lock);\r\n}\r\nstatic unsigned long vgic_mmio_read_its_cwriter(struct kvm *kvm,\r\nstruct vgic_its *its,\r\ngpa_t addr, unsigned int len)\r\n{\r\nreturn extract_bytes(its->cwriter, addr & 0x7, len);\r\n}\r\nstatic unsigned long vgic_mmio_read_its_creadr(struct kvm *kvm,\r\nstruct vgic_its *its,\r\ngpa_t addr, unsigned int len)\r\n{\r\nreturn extract_bytes(its->creadr, addr & 0x7, len);\r\n}\r\nstatic unsigned long vgic_mmio_read_its_baser(struct kvm *kvm,\r\nstruct vgic_its *its,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu64 reg;\r\nswitch (BASER_INDEX(addr)) {\r\ncase 0:\r\nreg = its->baser_device_table;\r\nbreak;\r\ncase 1:\r\nreg = its->baser_coll_table;\r\nbreak;\r\ndefault:\r\nreg = 0;\r\nbreak;\r\n}\r\nreturn extract_bytes(reg, addr & 7, len);\r\n}\r\nstatic void vgic_mmio_write_its_baser(struct kvm *kvm,\r\nstruct vgic_its *its,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu64 entry_size, device_type;\r\nu64 reg, *regptr, clearbits = 0;\r\nif (its->enabled)\r\nreturn;\r\nswitch (BASER_INDEX(addr)) {\r\ncase 0:\r\nregptr = &its->baser_device_table;\r\nentry_size = 8;\r\ndevice_type = GITS_BASER_TYPE_DEVICE;\r\nbreak;\r\ncase 1:\r\nregptr = &its->baser_coll_table;\r\nentry_size = 8;\r\ndevice_type = GITS_BASER_TYPE_COLLECTION;\r\nclearbits = GITS_BASER_INDIRECT;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\nreg = update_64bit_reg(*regptr, addr & 7, len, val);\r\nreg &= ~GITS_BASER_RO_MASK;\r\nreg &= ~clearbits;\r\nreg |= (entry_size - 1) << GITS_BASER_ENTRY_SIZE_SHIFT;\r\nreg |= device_type << GITS_BASER_TYPE_SHIFT;\r\nreg = vgic_sanitise_its_baser(reg);\r\n*regptr = reg;\r\n}\r\nstatic void its_mmio_write_wi(struct kvm *kvm, struct vgic_its *its,\r\ngpa_t addr, unsigned int len, unsigned long val)\r\n{\r\n}\r\nvoid vgic_enable_lpis(struct kvm_vcpu *vcpu)\r\n{\r\nif (!(vcpu->arch.vgic_cpu.pendbaser & GICR_PENDBASER_PTZ))\r\nits_sync_lpi_pending_table(vcpu);\r\n}\r\nstatic int vgic_register_its_iodev(struct kvm *kvm, struct vgic_its *its)\r\n{\r\nstruct vgic_io_device *iodev = &its->iodev;\r\nint ret;\r\nif (!its->initialized)\r\nreturn -EBUSY;\r\nif (IS_VGIC_ADDR_UNDEF(its->vgic_its_base))\r\nreturn -ENXIO;\r\niodev->regions = its_registers;\r\niodev->nr_regions = ARRAY_SIZE(its_registers);\r\nkvm_iodevice_init(&iodev->dev, &kvm_io_gic_ops);\r\niodev->base_addr = its->vgic_its_base;\r\niodev->iodev_type = IODEV_ITS;\r\niodev->its = its;\r\nmutex_lock(&kvm->slots_lock);\r\nret = kvm_io_bus_register_dev(kvm, KVM_MMIO_BUS, iodev->base_addr,\r\nKVM_VGIC_V3_ITS_SIZE, &iodev->dev);\r\nmutex_unlock(&kvm->slots_lock);\r\nreturn ret;\r\n}\r\nstatic int vgic_its_create(struct kvm_device *dev, u32 type)\r\n{\r\nstruct vgic_its *its;\r\nif (type != KVM_DEV_TYPE_ARM_VGIC_ITS)\r\nreturn -ENODEV;\r\nits = kzalloc(sizeof(struct vgic_its), GFP_KERNEL);\r\nif (!its)\r\nreturn -ENOMEM;\r\nmutex_init(&its->its_lock);\r\nmutex_init(&its->cmd_lock);\r\nits->vgic_its_base = VGIC_ADDR_UNDEF;\r\nINIT_LIST_HEAD(&its->device_list);\r\nINIT_LIST_HEAD(&its->collection_list);\r\ndev->kvm->arch.vgic.has_its = true;\r\nits->initialized = false;\r\nits->enabled = false;\r\nits->dev = dev;\r\nits->baser_device_table = INITIAL_BASER_VALUE |\r\n((u64)GITS_BASER_TYPE_DEVICE << GITS_BASER_TYPE_SHIFT);\r\nits->baser_coll_table = INITIAL_BASER_VALUE |\r\n((u64)GITS_BASER_TYPE_COLLECTION << GITS_BASER_TYPE_SHIFT);\r\ndev->kvm->arch.vgic.propbaser = INITIAL_PROPBASER_VALUE;\r\ndev->private = its;\r\nreturn 0;\r\n}\r\nstatic void vgic_its_destroy(struct kvm_device *kvm_dev)\r\n{\r\nstruct kvm *kvm = kvm_dev->kvm;\r\nstruct vgic_its *its = kvm_dev->private;\r\nstruct its_device *dev;\r\nstruct its_itte *itte;\r\nstruct list_head *dev_cur, *dev_temp;\r\nstruct list_head *cur, *temp;\r\nif (!its->device_list.next)\r\nreturn;\r\nmutex_lock(&its->its_lock);\r\nlist_for_each_safe(dev_cur, dev_temp, &its->device_list) {\r\ndev = container_of(dev_cur, struct its_device, dev_list);\r\nlist_for_each_safe(cur, temp, &dev->itt_head) {\r\nitte = (container_of(cur, struct its_itte, itte_list));\r\nits_free_itte(kvm, itte);\r\n}\r\nlist_del(dev_cur);\r\nkfree(dev);\r\n}\r\nlist_for_each_safe(cur, temp, &its->collection_list) {\r\nlist_del(cur);\r\nkfree(container_of(cur, struct its_collection, coll_list));\r\n}\r\nmutex_unlock(&its->its_lock);\r\nkfree(its);\r\n}\r\nstatic int vgic_its_has_attr(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr)\r\n{\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_ADDR:\r\nswitch (attr->attr) {\r\ncase KVM_VGIC_ITS_ADDR_TYPE:\r\nreturn 0;\r\n}\r\nbreak;\r\ncase KVM_DEV_ARM_VGIC_GRP_CTRL:\r\nswitch (attr->attr) {\r\ncase KVM_DEV_ARM_VGIC_CTRL_INIT:\r\nreturn 0;\r\n}\r\nbreak;\r\n}\r\nreturn -ENXIO;\r\n}\r\nstatic int vgic_its_set_attr(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr)\r\n{\r\nstruct vgic_its *its = dev->private;\r\nint ret;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_ADDR: {\r\nu64 __user *uaddr = (u64 __user *)(long)attr->addr;\r\nunsigned long type = (unsigned long)attr->attr;\r\nu64 addr;\r\nif (type != KVM_VGIC_ITS_ADDR_TYPE)\r\nreturn -ENODEV;\r\nif (copy_from_user(&addr, uaddr, sizeof(addr)))\r\nreturn -EFAULT;\r\nret = vgic_check_ioaddr(dev->kvm, &its->vgic_its_base,\r\naddr, SZ_64K);\r\nif (ret)\r\nreturn ret;\r\nits->vgic_its_base = addr;\r\nreturn 0;\r\n}\r\ncase KVM_DEV_ARM_VGIC_GRP_CTRL:\r\nswitch (attr->attr) {\r\ncase KVM_DEV_ARM_VGIC_CTRL_INIT:\r\nits->initialized = true;\r\nreturn 0;\r\n}\r\nbreak;\r\n}\r\nreturn -ENXIO;\r\n}\r\nstatic int vgic_its_get_attr(struct kvm_device *dev,\r\nstruct kvm_device_attr *attr)\r\n{\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_ADDR: {\r\nstruct vgic_its *its = dev->private;\r\nu64 addr = its->vgic_its_base;\r\nu64 __user *uaddr = (u64 __user *)(long)attr->addr;\r\nunsigned long type = (unsigned long)attr->attr;\r\nif (type != KVM_VGIC_ITS_ADDR_TYPE)\r\nreturn -ENODEV;\r\nif (copy_to_user(uaddr, &addr, sizeof(addr)))\r\nreturn -EFAULT;\r\nbreak;\r\ndefault:\r\nreturn -ENXIO;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint kvm_vgic_register_its_device(void)\r\n{\r\nreturn kvm_register_device_ops(&kvm_arm_vgic_its_ops,\r\nKVM_DEV_TYPE_ARM_VGIC_ITS);\r\n}\r\nint vgic_register_its_iodevs(struct kvm *kvm)\r\n{\r\nstruct kvm_device *dev;\r\nint ret = 0;\r\nlist_for_each_entry(dev, &kvm->devices, vm_node) {\r\nif (dev->ops != &kvm_arm_vgic_its_ops)\r\ncontinue;\r\nret = vgic_register_its_iodev(kvm, dev->private);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn ret;\r\n}
