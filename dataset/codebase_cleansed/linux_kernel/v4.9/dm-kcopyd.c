static void io_job_start(struct dm_kcopyd_throttle *t)\r\n{\r\nunsigned throttle, now, difference;\r\nint slept = 0, skew;\r\nif (unlikely(!t))\r\nreturn;\r\ntry_again:\r\nspin_lock_irq(&throttle_spinlock);\r\nthrottle = ACCESS_ONCE(t->throttle);\r\nif (likely(throttle >= 100))\r\ngoto skip_limit;\r\nnow = jiffies;\r\ndifference = now - t->last_jiffies;\r\nt->last_jiffies = now;\r\nif (t->num_io_jobs)\r\nt->io_period += difference;\r\nt->total_period += difference;\r\nif (unlikely(t->io_period > t->total_period))\r\nt->io_period = t->total_period;\r\nif (unlikely(t->total_period >= (1 << ACCOUNT_INTERVAL_SHIFT))) {\r\nint shift = fls(t->total_period >> ACCOUNT_INTERVAL_SHIFT);\r\nt->total_period >>= shift;\r\nt->io_period >>= shift;\r\n}\r\nskew = t->io_period - throttle * t->total_period / 100;\r\nif (unlikely(skew > 0) && slept < MAX_SLEEPS) {\r\nslept++;\r\nspin_unlock_irq(&throttle_spinlock);\r\nmsleep(SLEEP_MSEC);\r\ngoto try_again;\r\n}\r\nskip_limit:\r\nt->num_io_jobs++;\r\nspin_unlock_irq(&throttle_spinlock);\r\n}\r\nstatic void io_job_finish(struct dm_kcopyd_throttle *t)\r\n{\r\nunsigned long flags;\r\nif (unlikely(!t))\r\nreturn;\r\nspin_lock_irqsave(&throttle_spinlock, flags);\r\nt->num_io_jobs--;\r\nif (likely(ACCESS_ONCE(t->throttle) >= 100))\r\ngoto skip_limit;\r\nif (!t->num_io_jobs) {\r\nunsigned now, difference;\r\nnow = jiffies;\r\ndifference = now - t->last_jiffies;\r\nt->last_jiffies = now;\r\nt->io_period += difference;\r\nt->total_period += difference;\r\nif (unlikely(t->io_period > t->total_period))\r\nt->io_period = t->total_period;\r\n}\r\nskip_limit:\r\nspin_unlock_irqrestore(&throttle_spinlock, flags);\r\n}\r\nstatic void wake(struct dm_kcopyd_client *kc)\r\n{\r\nqueue_work(kc->kcopyd_wq, &kc->kcopyd_work);\r\n}\r\nstatic struct page_list *alloc_pl(gfp_t gfp)\r\n{\r\nstruct page_list *pl;\r\npl = kmalloc(sizeof(*pl), gfp);\r\nif (!pl)\r\nreturn NULL;\r\npl->page = alloc_page(gfp);\r\nif (!pl->page) {\r\nkfree(pl);\r\nreturn NULL;\r\n}\r\nreturn pl;\r\n}\r\nstatic void free_pl(struct page_list *pl)\r\n{\r\n__free_page(pl->page);\r\nkfree(pl);\r\n}\r\nstatic void kcopyd_put_pages(struct dm_kcopyd_client *kc, struct page_list *pl)\r\n{\r\nstruct page_list *next;\r\ndo {\r\nnext = pl->next;\r\nif (kc->nr_free_pages >= kc->nr_reserved_pages)\r\nfree_pl(pl);\r\nelse {\r\npl->next = kc->pages;\r\nkc->pages = pl;\r\nkc->nr_free_pages++;\r\n}\r\npl = next;\r\n} while (pl);\r\n}\r\nstatic int kcopyd_get_pages(struct dm_kcopyd_client *kc,\r\nunsigned int nr, struct page_list **pages)\r\n{\r\nstruct page_list *pl;\r\n*pages = NULL;\r\ndo {\r\npl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY | __GFP_KSWAPD_RECLAIM);\r\nif (unlikely(!pl)) {\r\npl = kc->pages;\r\nif (unlikely(!pl))\r\ngoto out_of_memory;\r\nkc->pages = pl->next;\r\nkc->nr_free_pages--;\r\n}\r\npl->next = *pages;\r\n*pages = pl;\r\n} while (--nr);\r\nreturn 0;\r\nout_of_memory:\r\nif (*pages)\r\nkcopyd_put_pages(kc, *pages);\r\nreturn -ENOMEM;\r\n}\r\nstatic void drop_pages(struct page_list *pl)\r\n{\r\nstruct page_list *next;\r\nwhile (pl) {\r\nnext = pl->next;\r\nfree_pl(pl);\r\npl = next;\r\n}\r\n}\r\nstatic int client_reserve_pages(struct dm_kcopyd_client *kc, unsigned nr_pages)\r\n{\r\nunsigned i;\r\nstruct page_list *pl = NULL, *next;\r\nfor (i = 0; i < nr_pages; i++) {\r\nnext = alloc_pl(GFP_KERNEL);\r\nif (!next) {\r\nif (pl)\r\ndrop_pages(pl);\r\nreturn -ENOMEM;\r\n}\r\nnext->next = pl;\r\npl = next;\r\n}\r\nkc->nr_reserved_pages += nr_pages;\r\nkcopyd_put_pages(kc, pl);\r\nreturn 0;\r\n}\r\nstatic void client_free_pages(struct dm_kcopyd_client *kc)\r\n{\r\nBUG_ON(kc->nr_free_pages != kc->nr_reserved_pages);\r\ndrop_pages(kc->pages);\r\nkc->pages = NULL;\r\nkc->nr_free_pages = kc->nr_reserved_pages = 0;\r\n}\r\nint __init dm_kcopyd_init(void)\r\n{\r\n_job_cache = kmem_cache_create("kcopyd_job",\r\nsizeof(struct kcopyd_job) * (SPLIT_COUNT + 1),\r\n__alignof__(struct kcopyd_job), 0, NULL);\r\nif (!_job_cache)\r\nreturn -ENOMEM;\r\nzero_page_list.next = &zero_page_list;\r\nzero_page_list.page = ZERO_PAGE(0);\r\nreturn 0;\r\n}\r\nvoid dm_kcopyd_exit(void)\r\n{\r\nkmem_cache_destroy(_job_cache);\r\n_job_cache = NULL;\r\n}\r\nstatic struct kcopyd_job *pop(struct list_head *jobs,\r\nstruct dm_kcopyd_client *kc)\r\n{\r\nstruct kcopyd_job *job = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&kc->job_lock, flags);\r\nif (!list_empty(jobs)) {\r\njob = list_entry(jobs->next, struct kcopyd_job, list);\r\nlist_del(&job->list);\r\n}\r\nspin_unlock_irqrestore(&kc->job_lock, flags);\r\nreturn job;\r\n}\r\nstatic void push(struct list_head *jobs, struct kcopyd_job *job)\r\n{\r\nunsigned long flags;\r\nstruct dm_kcopyd_client *kc = job->kc;\r\nspin_lock_irqsave(&kc->job_lock, flags);\r\nlist_add_tail(&job->list, jobs);\r\nspin_unlock_irqrestore(&kc->job_lock, flags);\r\n}\r\nstatic void push_head(struct list_head *jobs, struct kcopyd_job *job)\r\n{\r\nunsigned long flags;\r\nstruct dm_kcopyd_client *kc = job->kc;\r\nspin_lock_irqsave(&kc->job_lock, flags);\r\nlist_add(&job->list, jobs);\r\nspin_unlock_irqrestore(&kc->job_lock, flags);\r\n}\r\nstatic int run_complete_job(struct kcopyd_job *job)\r\n{\r\nvoid *context = job->context;\r\nint read_err = job->read_err;\r\nunsigned long write_err = job->write_err;\r\ndm_kcopyd_notify_fn fn = job->fn;\r\nstruct dm_kcopyd_client *kc = job->kc;\r\nif (job->pages && job->pages != &zero_page_list)\r\nkcopyd_put_pages(kc, job->pages);\r\nif (job->master_job == job)\r\nmempool_free(job, kc->job_pool);\r\nfn(read_err, write_err, context);\r\nif (atomic_dec_and_test(&kc->nr_jobs))\r\nwake_up(&kc->destroyq);\r\nreturn 0;\r\n}\r\nstatic void complete_io(unsigned long error, void *context)\r\n{\r\nstruct kcopyd_job *job = (struct kcopyd_job *) context;\r\nstruct dm_kcopyd_client *kc = job->kc;\r\nio_job_finish(kc->throttle);\r\nif (error) {\r\nif (op_is_write(job->rw))\r\njob->write_err |= error;\r\nelse\r\njob->read_err = 1;\r\nif (!test_bit(DM_KCOPYD_IGNORE_ERROR, &job->flags)) {\r\npush(&kc->complete_jobs, job);\r\nwake(kc);\r\nreturn;\r\n}\r\n}\r\nif (op_is_write(job->rw))\r\npush(&kc->complete_jobs, job);\r\nelse {\r\njob->rw = WRITE;\r\npush(&kc->io_jobs, job);\r\n}\r\nwake(kc);\r\n}\r\nstatic int run_io_job(struct kcopyd_job *job)\r\n{\r\nint r;\r\nstruct dm_io_request io_req = {\r\n.bi_op = job->rw,\r\n.bi_op_flags = 0,\r\n.mem.type = DM_IO_PAGE_LIST,\r\n.mem.ptr.pl = job->pages,\r\n.mem.offset = 0,\r\n.notify.fn = complete_io,\r\n.notify.context = job,\r\n.client = job->kc->io_client,\r\n};\r\nio_job_start(job->kc->throttle);\r\nif (job->rw == READ)\r\nr = dm_io(&io_req, 1, &job->source, NULL);\r\nelse\r\nr = dm_io(&io_req, job->num_dests, job->dests, NULL);\r\nreturn r;\r\n}\r\nstatic int run_pages_job(struct kcopyd_job *job)\r\n{\r\nint r;\r\nunsigned nr_pages = dm_div_up(job->dests[0].count, PAGE_SIZE >> 9);\r\nr = kcopyd_get_pages(job->kc, nr_pages, &job->pages);\r\nif (!r) {\r\npush(&job->kc->io_jobs, job);\r\nreturn 0;\r\n}\r\nif (r == -ENOMEM)\r\nreturn 1;\r\nreturn r;\r\n}\r\nstatic int process_jobs(struct list_head *jobs, struct dm_kcopyd_client *kc,\r\nint (*fn) (struct kcopyd_job *))\r\n{\r\nstruct kcopyd_job *job;\r\nint r, count = 0;\r\nwhile ((job = pop(jobs, kc))) {\r\nr = fn(job);\r\nif (r < 0) {\r\nif (op_is_write(job->rw))\r\njob->write_err = (unsigned long) -1L;\r\nelse\r\njob->read_err = 1;\r\npush(&kc->complete_jobs, job);\r\nbreak;\r\n}\r\nif (r > 0) {\r\npush_head(jobs, job);\r\nbreak;\r\n}\r\ncount++;\r\n}\r\nreturn count;\r\n}\r\nstatic void do_work(struct work_struct *work)\r\n{\r\nstruct dm_kcopyd_client *kc = container_of(work,\r\nstruct dm_kcopyd_client, kcopyd_work);\r\nstruct blk_plug plug;\r\nblk_start_plug(&plug);\r\nprocess_jobs(&kc->complete_jobs, kc, run_complete_job);\r\nprocess_jobs(&kc->pages_jobs, kc, run_pages_job);\r\nprocess_jobs(&kc->io_jobs, kc, run_io_job);\r\nblk_finish_plug(&plug);\r\n}\r\nstatic void dispatch_job(struct kcopyd_job *job)\r\n{\r\nstruct dm_kcopyd_client *kc = job->kc;\r\natomic_inc(&kc->nr_jobs);\r\nif (unlikely(!job->source.count))\r\npush(&kc->complete_jobs, job);\r\nelse if (job->pages == &zero_page_list)\r\npush(&kc->io_jobs, job);\r\nelse\r\npush(&kc->pages_jobs, job);\r\nwake(kc);\r\n}\r\nstatic void segment_complete(int read_err, unsigned long write_err,\r\nvoid *context)\r\n{\r\nsector_t progress = 0;\r\nsector_t count = 0;\r\nstruct kcopyd_job *sub_job = (struct kcopyd_job *) context;\r\nstruct kcopyd_job *job = sub_job->master_job;\r\nstruct dm_kcopyd_client *kc = job->kc;\r\nmutex_lock(&job->lock);\r\nif (read_err)\r\njob->read_err = 1;\r\nif (write_err)\r\njob->write_err |= write_err;\r\nif ((!job->read_err && !job->write_err) ||\r\ntest_bit(DM_KCOPYD_IGNORE_ERROR, &job->flags)) {\r\nprogress = job->progress;\r\ncount = job->source.count - progress;\r\nif (count) {\r\nif (count > SUB_JOB_SIZE)\r\ncount = SUB_JOB_SIZE;\r\njob->progress += count;\r\n}\r\n}\r\nmutex_unlock(&job->lock);\r\nif (count) {\r\nint i;\r\n*sub_job = *job;\r\nsub_job->source.sector += progress;\r\nsub_job->source.count = count;\r\nfor (i = 0; i < job->num_dests; i++) {\r\nsub_job->dests[i].sector += progress;\r\nsub_job->dests[i].count = count;\r\n}\r\nsub_job->fn = segment_complete;\r\nsub_job->context = sub_job;\r\ndispatch_job(sub_job);\r\n} else if (atomic_dec_and_test(&job->sub_jobs)) {\r\npush(&kc->complete_jobs, job);\r\nwake(kc);\r\n}\r\n}\r\nstatic void split_job(struct kcopyd_job *master_job)\r\n{\r\nint i;\r\natomic_inc(&master_job->kc->nr_jobs);\r\natomic_set(&master_job->sub_jobs, SPLIT_COUNT);\r\nfor (i = 0; i < SPLIT_COUNT; i++) {\r\nmaster_job[i + 1].master_job = master_job;\r\nsegment_complete(0, 0u, &master_job[i + 1]);\r\n}\r\n}\r\nint dm_kcopyd_copy(struct dm_kcopyd_client *kc, struct dm_io_region *from,\r\nunsigned int num_dests, struct dm_io_region *dests,\r\nunsigned int flags, dm_kcopyd_notify_fn fn, void *context)\r\n{\r\nstruct kcopyd_job *job;\r\nint i;\r\njob = mempool_alloc(kc->job_pool, GFP_NOIO);\r\njob->kc = kc;\r\njob->flags = flags;\r\njob->read_err = 0;\r\njob->write_err = 0;\r\njob->num_dests = num_dests;\r\nmemcpy(&job->dests, dests, sizeof(*dests) * num_dests);\r\nif (from) {\r\njob->source = *from;\r\njob->pages = NULL;\r\njob->rw = READ;\r\n} else {\r\nmemset(&job->source, 0, sizeof job->source);\r\njob->source.count = job->dests[0].count;\r\njob->pages = &zero_page_list;\r\njob->rw = REQ_OP_WRITE_SAME;\r\nfor (i = 0; i < job->num_dests; i++)\r\nif (!bdev_write_same(job->dests[i].bdev)) {\r\njob->rw = WRITE;\r\nbreak;\r\n}\r\n}\r\njob->fn = fn;\r\njob->context = context;\r\njob->master_job = job;\r\nif (job->source.count <= SUB_JOB_SIZE)\r\ndispatch_job(job);\r\nelse {\r\nmutex_init(&job->lock);\r\njob->progress = 0;\r\nsplit_job(job);\r\n}\r\nreturn 0;\r\n}\r\nint dm_kcopyd_zero(struct dm_kcopyd_client *kc,\r\nunsigned num_dests, struct dm_io_region *dests,\r\nunsigned flags, dm_kcopyd_notify_fn fn, void *context)\r\n{\r\nreturn dm_kcopyd_copy(kc, NULL, num_dests, dests, flags, fn, context);\r\n}\r\nvoid *dm_kcopyd_prepare_callback(struct dm_kcopyd_client *kc,\r\ndm_kcopyd_notify_fn fn, void *context)\r\n{\r\nstruct kcopyd_job *job;\r\njob = mempool_alloc(kc->job_pool, GFP_NOIO);\r\nmemset(job, 0, sizeof(struct kcopyd_job));\r\njob->kc = kc;\r\njob->fn = fn;\r\njob->context = context;\r\njob->master_job = job;\r\natomic_inc(&kc->nr_jobs);\r\nreturn job;\r\n}\r\nvoid dm_kcopyd_do_callback(void *j, int read_err, unsigned long write_err)\r\n{\r\nstruct kcopyd_job *job = j;\r\nstruct dm_kcopyd_client *kc = job->kc;\r\njob->read_err = read_err;\r\njob->write_err = write_err;\r\npush(&kc->complete_jobs, job);\r\nwake(kc);\r\n}\r\nstruct dm_kcopyd_client *dm_kcopyd_client_create(struct dm_kcopyd_throttle *throttle)\r\n{\r\nint r = -ENOMEM;\r\nstruct dm_kcopyd_client *kc;\r\nkc = kmalloc(sizeof(*kc), GFP_KERNEL);\r\nif (!kc)\r\nreturn ERR_PTR(-ENOMEM);\r\nspin_lock_init(&kc->job_lock);\r\nINIT_LIST_HEAD(&kc->complete_jobs);\r\nINIT_LIST_HEAD(&kc->io_jobs);\r\nINIT_LIST_HEAD(&kc->pages_jobs);\r\nkc->throttle = throttle;\r\nkc->job_pool = mempool_create_slab_pool(MIN_JOBS, _job_cache);\r\nif (!kc->job_pool)\r\ngoto bad_slab;\r\nINIT_WORK(&kc->kcopyd_work, do_work);\r\nkc->kcopyd_wq = alloc_workqueue("kcopyd", WQ_MEM_RECLAIM, 0);\r\nif (!kc->kcopyd_wq)\r\ngoto bad_workqueue;\r\nkc->pages = NULL;\r\nkc->nr_reserved_pages = kc->nr_free_pages = 0;\r\nr = client_reserve_pages(kc, RESERVE_PAGES);\r\nif (r)\r\ngoto bad_client_pages;\r\nkc->io_client = dm_io_client_create();\r\nif (IS_ERR(kc->io_client)) {\r\nr = PTR_ERR(kc->io_client);\r\ngoto bad_io_client;\r\n}\r\ninit_waitqueue_head(&kc->destroyq);\r\natomic_set(&kc->nr_jobs, 0);\r\nreturn kc;\r\nbad_io_client:\r\nclient_free_pages(kc);\r\nbad_client_pages:\r\ndestroy_workqueue(kc->kcopyd_wq);\r\nbad_workqueue:\r\nmempool_destroy(kc->job_pool);\r\nbad_slab:\r\nkfree(kc);\r\nreturn ERR_PTR(r);\r\n}\r\nvoid dm_kcopyd_client_destroy(struct dm_kcopyd_client *kc)\r\n{\r\nwait_event(kc->destroyq, !atomic_read(&kc->nr_jobs));\r\nBUG_ON(!list_empty(&kc->complete_jobs));\r\nBUG_ON(!list_empty(&kc->io_jobs));\r\nBUG_ON(!list_empty(&kc->pages_jobs));\r\ndestroy_workqueue(kc->kcopyd_wq);\r\ndm_io_client_destroy(kc->io_client);\r\nclient_free_pages(kc);\r\nmempool_destroy(kc->job_pool);\r\nkfree(kc);\r\n}
