static inline void\r\npfm_put_task(struct task_struct *task)\r\n{\r\nif (task != current) put_task_struct(task);\r\n}\r\nstatic inline void\r\npfm_reserve_page(unsigned long a)\r\n{\r\nSetPageReserved(vmalloc_to_page((void *)a));\r\n}\r\nstatic inline void\r\npfm_unreserve_page(unsigned long a)\r\n{\r\nClearPageReserved(vmalloc_to_page((void*)a));\r\n}\r\nstatic inline unsigned long\r\npfm_protect_ctx_ctxsw(pfm_context_t *x)\r\n{\r\nspin_lock(&(x)->ctx_lock);\r\nreturn 0UL;\r\n}\r\nstatic inline void\r\npfm_unprotect_ctx_ctxsw(pfm_context_t *x, unsigned long f)\r\n{\r\nspin_unlock(&(x)->ctx_lock);\r\n}\r\nstatic struct dentry *\r\npfmfs_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data)\r\n{\r\nreturn mount_pseudo(fs_type, "pfm:", NULL, &pfmfs_dentry_operations,\r\nPFMFS_MAGIC);\r\n}\r\nstatic inline void\r\npfm_clear_psr_pp(void)\r\n{\r\nia64_rsm(IA64_PSR_PP);\r\nia64_srlz_i();\r\n}\r\nstatic inline void\r\npfm_set_psr_pp(void)\r\n{\r\nia64_ssm(IA64_PSR_PP);\r\nia64_srlz_i();\r\n}\r\nstatic inline void\r\npfm_clear_psr_up(void)\r\n{\r\nia64_rsm(IA64_PSR_UP);\r\nia64_srlz_i();\r\n}\r\nstatic inline void\r\npfm_set_psr_up(void)\r\n{\r\nia64_ssm(IA64_PSR_UP);\r\nia64_srlz_i();\r\n}\r\nstatic inline unsigned long\r\npfm_get_psr(void)\r\n{\r\nunsigned long tmp;\r\ntmp = ia64_getreg(_IA64_REG_PSR);\r\nia64_srlz_i();\r\nreturn tmp;\r\n}\r\nstatic inline void\r\npfm_set_psr_l(unsigned long val)\r\n{\r\nia64_setreg(_IA64_REG_PSR_L, val);\r\nia64_srlz_i();\r\n}\r\nstatic inline void\r\npfm_freeze_pmu(void)\r\n{\r\nia64_set_pmc(0,1UL);\r\nia64_srlz_d();\r\n}\r\nstatic inline void\r\npfm_unfreeze_pmu(void)\r\n{\r\nia64_set_pmc(0,0UL);\r\nia64_srlz_d();\r\n}\r\nstatic inline void\r\npfm_restore_ibrs(unsigned long *ibrs, unsigned int nibrs)\r\n{\r\nint i;\r\nfor (i=0; i < nibrs; i++) {\r\nia64_set_ibr(i, ibrs[i]);\r\nia64_dv_serialize_instruction();\r\n}\r\nia64_srlz_i();\r\n}\r\nstatic inline void\r\npfm_restore_dbrs(unsigned long *dbrs, unsigned int ndbrs)\r\n{\r\nint i;\r\nfor (i=0; i < ndbrs; i++) {\r\nia64_set_dbr(i, dbrs[i]);\r\nia64_dv_serialize_data();\r\n}\r\nia64_srlz_d();\r\n}\r\nstatic inline unsigned long\r\npfm_read_soft_counter(pfm_context_t *ctx, int i)\r\n{\r\nreturn ctx->ctx_pmds[i].val + (ia64_get_pmd(i) & pmu_conf->ovfl_val);\r\n}\r\nstatic inline void\r\npfm_write_soft_counter(pfm_context_t *ctx, int i, unsigned long val)\r\n{\r\nunsigned long ovfl_val = pmu_conf->ovfl_val;\r\nctx->ctx_pmds[i].val = val & ~ovfl_val;\r\nia64_set_pmd(i, val & ovfl_val);\r\n}\r\nstatic pfm_msg_t *\r\npfm_get_new_msg(pfm_context_t *ctx)\r\n{\r\nint idx, next;\r\nnext = (ctx->ctx_msgq_tail+1) % PFM_MAX_MSGS;\r\nDPRINT(("ctx_fd=%p head=%d tail=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail));\r\nif (next == ctx->ctx_msgq_head) return NULL;\r\nidx = ctx->ctx_msgq_tail;\r\nctx->ctx_msgq_tail = next;\r\nDPRINT(("ctx=%p head=%d tail=%d msg=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail, idx));\r\nreturn ctx->ctx_msgq+idx;\r\n}\r\nstatic pfm_msg_t *\r\npfm_get_next_msg(pfm_context_t *ctx)\r\n{\r\npfm_msg_t *msg;\r\nDPRINT(("ctx=%p head=%d tail=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail));\r\nif (PFM_CTXQ_EMPTY(ctx)) return NULL;\r\nmsg = ctx->ctx_msgq+ctx->ctx_msgq_head;\r\nctx->ctx_msgq_head = (ctx->ctx_msgq_head+1) % PFM_MAX_MSGS;\r\nDPRINT(("ctx=%p head=%d tail=%d type=%d\n", ctx, ctx->ctx_msgq_head, ctx->ctx_msgq_tail, msg->pfm_gen_msg.msg_type));\r\nreturn msg;\r\n}\r\nstatic void\r\npfm_reset_msgq(pfm_context_t *ctx)\r\n{\r\nctx->ctx_msgq_head = ctx->ctx_msgq_tail = 0;\r\nDPRINT(("ctx=%p msgq reset\n", ctx));\r\n}\r\nstatic void *\r\npfm_rvmalloc(unsigned long size)\r\n{\r\nvoid *mem;\r\nunsigned long addr;\r\nsize = PAGE_ALIGN(size);\r\nmem = vzalloc(size);\r\nif (mem) {\r\naddr = (unsigned long)mem;\r\nwhile (size > 0) {\r\npfm_reserve_page(addr);\r\naddr+=PAGE_SIZE;\r\nsize-=PAGE_SIZE;\r\n}\r\n}\r\nreturn mem;\r\n}\r\nstatic void\r\npfm_rvfree(void *mem, unsigned long size)\r\n{\r\nunsigned long addr;\r\nif (mem) {\r\nDPRINT(("freeing physical buffer @%p size=%lu\n", mem, size));\r\naddr = (unsigned long) mem;\r\nwhile ((long) size > 0) {\r\npfm_unreserve_page(addr);\r\naddr+=PAGE_SIZE;\r\nsize-=PAGE_SIZE;\r\n}\r\nvfree(mem);\r\n}\r\nreturn;\r\n}\r\nstatic pfm_context_t *\r\npfm_context_alloc(int ctx_flags)\r\n{\r\npfm_context_t *ctx;\r\nctx = kzalloc(sizeof(pfm_context_t), GFP_KERNEL);\r\nif (ctx) {\r\nDPRINT(("alloc ctx @%p\n", ctx));\r\nspin_lock_init(&ctx->ctx_lock);\r\nctx->ctx_state = PFM_CTX_UNLOADED;\r\nctx->ctx_fl_block = (ctx_flags & PFM_FL_NOTIFY_BLOCK) ? 1 : 0;\r\nctx->ctx_fl_system = (ctx_flags & PFM_FL_SYSTEM_WIDE) ? 1: 0;\r\nctx->ctx_fl_no_msg = (ctx_flags & PFM_FL_OVFL_NO_MSG) ? 1: 0;\r\ninit_completion(&ctx->ctx_restart_done);\r\nctx->ctx_last_activation = PFM_INVALID_ACTIVATION;\r\nSET_LAST_CPU(ctx, -1);\r\nctx->ctx_msgq_head = ctx->ctx_msgq_tail = 0;\r\ninit_waitqueue_head(&ctx->ctx_msgq_wait);\r\ninit_waitqueue_head(&ctx->ctx_zombieq);\r\n}\r\nreturn ctx;\r\n}\r\nstatic void\r\npfm_context_free(pfm_context_t *ctx)\r\n{\r\nif (ctx) {\r\nDPRINT(("free ctx @%p\n", ctx));\r\nkfree(ctx);\r\n}\r\n}\r\nstatic void\r\npfm_mask_monitoring(struct task_struct *task)\r\n{\r\npfm_context_t *ctx = PFM_GET_CTX(task);\r\nunsigned long mask, val, ovfl_mask;\r\nint i;\r\nDPRINT_ovfl(("masking monitoring for [%d]\n", task_pid_nr(task)));\r\novfl_mask = pmu_conf->ovfl_val;\r\nmask = ctx->ctx_used_pmds[0];\r\nfor (i = 0; mask; i++, mask>>=1) {\r\nif ((mask & 0x1) == 0) continue;\r\nval = ia64_get_pmd(i);\r\nif (PMD_IS_COUNTING(i)) {\r\nctx->ctx_pmds[i].val += (val & ovfl_mask);\r\n} else {\r\nctx->ctx_pmds[i].val = val;\r\n}\r\nDPRINT_ovfl(("pmd[%d]=0x%lx hw_pmd=0x%lx\n",\r\ni,\r\nctx->ctx_pmds[i].val,\r\nval & ovfl_mask));\r\n}\r\nmask = ctx->ctx_used_monitors[0] >> PMU_FIRST_COUNTER;\r\nfor(i= PMU_FIRST_COUNTER; mask; i++, mask>>=1) {\r\nif ((mask & 0x1) == 0UL) continue;\r\nia64_set_pmc(i, ctx->th_pmcs[i] & ~0xfUL);\r\nctx->th_pmcs[i] &= ~0xfUL;\r\nDPRINT_ovfl(("pmc[%d]=0x%lx\n", i, ctx->th_pmcs[i]));\r\n}\r\nia64_srlz_d();\r\n}\r\nstatic void\r\npfm_restore_monitoring(struct task_struct *task)\r\n{\r\npfm_context_t *ctx = PFM_GET_CTX(task);\r\nunsigned long mask, ovfl_mask;\r\nunsigned long psr, val;\r\nint i, is_system;\r\nis_system = ctx->ctx_fl_system;\r\novfl_mask = pmu_conf->ovfl_val;\r\nif (task != current) {\r\nprintk(KERN_ERR "perfmon.%d: invalid task[%d] current[%d]\n", __LINE__, task_pid_nr(task), task_pid_nr(current));\r\nreturn;\r\n}\r\nif (ctx->ctx_state != PFM_CTX_MASKED) {\r\nprintk(KERN_ERR "perfmon.%d: task[%d] current[%d] invalid state=%d\n", __LINE__,\r\ntask_pid_nr(task), task_pid_nr(current), ctx->ctx_state);\r\nreturn;\r\n}\r\npsr = pfm_get_psr();\r\nif (is_system && (PFM_CPUINFO_GET() & PFM_CPUINFO_DCR_PP)) {\r\nia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) & ~IA64_DCR_PP);\r\npfm_clear_psr_pp();\r\n} else {\r\npfm_clear_psr_up();\r\n}\r\nmask = ctx->ctx_used_pmds[0];\r\nfor (i = 0; mask; i++, mask>>=1) {\r\nif ((mask & 0x1) == 0) continue;\r\nif (PMD_IS_COUNTING(i)) {\r\nval = ctx->ctx_pmds[i].val & ovfl_mask;\r\nctx->ctx_pmds[i].val &= ~ovfl_mask;\r\n} else {\r\nval = ctx->ctx_pmds[i].val;\r\n}\r\nia64_set_pmd(i, val);\r\nDPRINT(("pmd[%d]=0x%lx hw_pmd=0x%lx\n",\r\ni,\r\nctx->ctx_pmds[i].val,\r\nval));\r\n}\r\nmask = ctx->ctx_used_monitors[0] >> PMU_FIRST_COUNTER;\r\nfor(i= PMU_FIRST_COUNTER; mask; i++, mask>>=1) {\r\nif ((mask & 0x1) == 0UL) continue;\r\nctx->th_pmcs[i] = ctx->ctx_pmcs[i];\r\nia64_set_pmc(i, ctx->th_pmcs[i]);\r\nDPRINT(("[%d] pmc[%d]=0x%lx\n",\r\ntask_pid_nr(task), i, ctx->th_pmcs[i]));\r\n}\r\nia64_srlz_d();\r\nif (ctx->ctx_fl_using_dbreg) {\r\npfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);\r\npfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);\r\n}\r\nif (is_system && (PFM_CPUINFO_GET() & PFM_CPUINFO_DCR_PP)) {\r\nia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) | IA64_DCR_PP);\r\nia64_srlz_i();\r\n}\r\npfm_set_psr_l(psr);\r\n}\r\nstatic inline void\r\npfm_save_pmds(unsigned long *pmds, unsigned long mask)\r\n{\r\nint i;\r\nia64_srlz_d();\r\nfor (i=0; mask; i++, mask>>=1) {\r\nif (mask & 0x1) pmds[i] = ia64_get_pmd(i);\r\n}\r\n}\r\nstatic inline void\r\npfm_restore_pmds(unsigned long *pmds, unsigned long mask)\r\n{\r\nint i;\r\nunsigned long val, ovfl_val = pmu_conf->ovfl_val;\r\nfor (i=0; mask; i++, mask>>=1) {\r\nif ((mask & 0x1) == 0) continue;\r\nval = PMD_IS_COUNTING(i) ? pmds[i] & ovfl_val : pmds[i];\r\nia64_set_pmd(i, val);\r\n}\r\nia64_srlz_d();\r\n}\r\nstatic inline void\r\npfm_copy_pmds(struct task_struct *task, pfm_context_t *ctx)\r\n{\r\nunsigned long ovfl_val = pmu_conf->ovfl_val;\r\nunsigned long mask = ctx->ctx_all_pmds[0];\r\nunsigned long val;\r\nint i;\r\nDPRINT(("mask=0x%lx\n", mask));\r\nfor (i=0; mask; i++, mask>>=1) {\r\nval = ctx->ctx_pmds[i].val;\r\nif (PMD_IS_COUNTING(i)) {\r\nctx->ctx_pmds[i].val = val & ~ovfl_val;\r\nval &= ovfl_val;\r\n}\r\nctx->th_pmds[i] = val;\r\nDPRINT(("pmd[%d]=0x%lx soft_val=0x%lx\n",\r\ni,\r\nctx->th_pmds[i],\r\nctx->ctx_pmds[i].val));\r\n}\r\n}\r\nstatic inline void\r\npfm_copy_pmcs(struct task_struct *task, pfm_context_t *ctx)\r\n{\r\nunsigned long mask = ctx->ctx_all_pmcs[0];\r\nint i;\r\nDPRINT(("mask=0x%lx\n", mask));\r\nfor (i=0; mask; i++, mask>>=1) {\r\nctx->th_pmcs[i] = ctx->ctx_pmcs[i];\r\nDPRINT(("pmc[%d]=0x%lx\n", i, ctx->th_pmcs[i]));\r\n}\r\n}\r\nstatic inline void\r\npfm_restore_pmcs(unsigned long *pmcs, unsigned long mask)\r\n{\r\nint i;\r\nfor (i=0; mask; i++, mask>>=1) {\r\nif ((mask & 0x1) == 0) continue;\r\nia64_set_pmc(i, pmcs[i]);\r\n}\r\nia64_srlz_d();\r\n}\r\nstatic inline int\r\npfm_uuid_cmp(pfm_uuid_t a, pfm_uuid_t b)\r\n{\r\nreturn memcmp(a, b, sizeof(pfm_uuid_t));\r\n}\r\nstatic inline int\r\npfm_buf_fmt_exit(pfm_buffer_fmt_t *fmt, struct task_struct *task, void *buf, struct pt_regs *regs)\r\n{\r\nint ret = 0;\r\nif (fmt->fmt_exit) ret = (*fmt->fmt_exit)(task, buf, regs);\r\nreturn ret;\r\n}\r\nstatic inline int\r\npfm_buf_fmt_getsize(pfm_buffer_fmt_t *fmt, struct task_struct *task, unsigned int flags, int cpu, void *arg, unsigned long *size)\r\n{\r\nint ret = 0;\r\nif (fmt->fmt_getsize) ret = (*fmt->fmt_getsize)(task, flags, cpu, arg, size);\r\nreturn ret;\r\n}\r\nstatic inline int\r\npfm_buf_fmt_validate(pfm_buffer_fmt_t *fmt, struct task_struct *task, unsigned int flags,\r\nint cpu, void *arg)\r\n{\r\nint ret = 0;\r\nif (fmt->fmt_validate) ret = (*fmt->fmt_validate)(task, flags, cpu, arg);\r\nreturn ret;\r\n}\r\nstatic inline int\r\npfm_buf_fmt_init(pfm_buffer_fmt_t *fmt, struct task_struct *task, void *buf, unsigned int flags,\r\nint cpu, void *arg)\r\n{\r\nint ret = 0;\r\nif (fmt->fmt_init) ret = (*fmt->fmt_init)(task, buf, flags, cpu, arg);\r\nreturn ret;\r\n}\r\nstatic inline int\r\npfm_buf_fmt_restart(pfm_buffer_fmt_t *fmt, struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs)\r\n{\r\nint ret = 0;\r\nif (fmt->fmt_restart) ret = (*fmt->fmt_restart)(task, ctrl, buf, regs);\r\nreturn ret;\r\n}\r\nstatic inline int\r\npfm_buf_fmt_restart_active(pfm_buffer_fmt_t *fmt, struct task_struct *task, pfm_ovfl_ctrl_t *ctrl, void *buf, struct pt_regs *regs)\r\n{\r\nint ret = 0;\r\nif (fmt->fmt_restart_active) ret = (*fmt->fmt_restart_active)(task, ctrl, buf, regs);\r\nreturn ret;\r\n}\r\nstatic pfm_buffer_fmt_t *\r\n__pfm_find_buffer_fmt(pfm_uuid_t uuid)\r\n{\r\nstruct list_head * pos;\r\npfm_buffer_fmt_t * entry;\r\nlist_for_each(pos, &pfm_buffer_fmt_list) {\r\nentry = list_entry(pos, pfm_buffer_fmt_t, fmt_list);\r\nif (pfm_uuid_cmp(uuid, entry->fmt_uuid) == 0)\r\nreturn entry;\r\n}\r\nreturn NULL;\r\n}\r\nstatic pfm_buffer_fmt_t *\r\npfm_find_buffer_fmt(pfm_uuid_t uuid)\r\n{\r\npfm_buffer_fmt_t * fmt;\r\nspin_lock(&pfm_buffer_fmt_lock);\r\nfmt = __pfm_find_buffer_fmt(uuid);\r\nspin_unlock(&pfm_buffer_fmt_lock);\r\nreturn fmt;\r\n}\r\nint\r\npfm_register_buffer_fmt(pfm_buffer_fmt_t *fmt)\r\n{\r\nint ret = 0;\r\nif (fmt == NULL || fmt->fmt_name == NULL) return -EINVAL;\r\nif (fmt->fmt_handler == NULL) return -EINVAL;\r\nspin_lock(&pfm_buffer_fmt_lock);\r\nif (__pfm_find_buffer_fmt(fmt->fmt_uuid)) {\r\nprintk(KERN_ERR "perfmon: duplicate sampling format: %s\n", fmt->fmt_name);\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\nlist_add(&fmt->fmt_list, &pfm_buffer_fmt_list);\r\nprintk(KERN_INFO "perfmon: added sampling format %s\n", fmt->fmt_name);\r\nout:\r\nspin_unlock(&pfm_buffer_fmt_lock);\r\nreturn ret;\r\n}\r\nint\r\npfm_unregister_buffer_fmt(pfm_uuid_t uuid)\r\n{\r\npfm_buffer_fmt_t *fmt;\r\nint ret = 0;\r\nspin_lock(&pfm_buffer_fmt_lock);\r\nfmt = __pfm_find_buffer_fmt(uuid);\r\nif (!fmt) {\r\nprintk(KERN_ERR "perfmon: cannot unregister format, not found\n");\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nlist_del_init(&fmt->fmt_list);\r\nprintk(KERN_INFO "perfmon: removed sampling format: %s\n", fmt->fmt_name);\r\nout:\r\nspin_unlock(&pfm_buffer_fmt_lock);\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_reserve_session(struct task_struct *task, int is_syswide, unsigned int cpu)\r\n{\r\nunsigned long flags;\r\nLOCK_PFS(flags);\r\nDPRINT(("in sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",\r\npfm_sessions.pfs_sys_sessions,\r\npfm_sessions.pfs_task_sessions,\r\npfm_sessions.pfs_sys_use_dbregs,\r\nis_syswide,\r\ncpu));\r\nif (is_syswide) {\r\nif (pfm_sessions.pfs_task_sessions > 0UL) {\r\nDPRINT(("system wide not possible, %u conflicting task_sessions\n",\r\npfm_sessions.pfs_task_sessions));\r\ngoto abort;\r\n}\r\nif (pfm_sessions.pfs_sys_session[cpu]) goto error_conflict;\r\nDPRINT(("reserving system wide session on CPU%u currently on CPU%u\n", cpu, smp_processor_id()));\r\npfm_sessions.pfs_sys_session[cpu] = task;\r\npfm_sessions.pfs_sys_sessions++ ;\r\n} else {\r\nif (pfm_sessions.pfs_sys_sessions) goto abort;\r\npfm_sessions.pfs_task_sessions++;\r\n}\r\nDPRINT(("out sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",\r\npfm_sessions.pfs_sys_sessions,\r\npfm_sessions.pfs_task_sessions,\r\npfm_sessions.pfs_sys_use_dbregs,\r\nis_syswide,\r\ncpu));\r\ncpu_idle_poll_ctrl(true);\r\nUNLOCK_PFS(flags);\r\nreturn 0;\r\nerror_conflict:\r\nDPRINT(("system wide not possible, conflicting session [%d] on CPU%d\n",\r\ntask_pid_nr(pfm_sessions.pfs_sys_session[cpu]),\r\ncpu));\r\nabort:\r\nUNLOCK_PFS(flags);\r\nreturn -EBUSY;\r\n}\r\nstatic int\r\npfm_unreserve_session(pfm_context_t *ctx, int is_syswide, unsigned int cpu)\r\n{\r\nunsigned long flags;\r\nLOCK_PFS(flags);\r\nDPRINT(("in sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",\r\npfm_sessions.pfs_sys_sessions,\r\npfm_sessions.pfs_task_sessions,\r\npfm_sessions.pfs_sys_use_dbregs,\r\nis_syswide,\r\ncpu));\r\nif (is_syswide) {\r\npfm_sessions.pfs_sys_session[cpu] = NULL;\r\nif (ctx && ctx->ctx_fl_using_dbreg) {\r\nif (pfm_sessions.pfs_sys_use_dbregs == 0) {\r\nprintk(KERN_ERR "perfmon: invalid release for ctx %p sys_use_dbregs=0\n", ctx);\r\n} else {\r\npfm_sessions.pfs_sys_use_dbregs--;\r\n}\r\n}\r\npfm_sessions.pfs_sys_sessions--;\r\n} else {\r\npfm_sessions.pfs_task_sessions--;\r\n}\r\nDPRINT(("out sys_sessions=%u task_sessions=%u dbregs=%u syswide=%d cpu=%u\n",\r\npfm_sessions.pfs_sys_sessions,\r\npfm_sessions.pfs_task_sessions,\r\npfm_sessions.pfs_sys_use_dbregs,\r\nis_syswide,\r\ncpu));\r\ncpu_idle_poll_ctrl(false);\r\nUNLOCK_PFS(flags);\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_remove_smpl_mapping(void *vaddr, unsigned long size)\r\n{\r\nstruct task_struct *task = current;\r\nint r;\r\nif (task->mm == NULL || size == 0UL || vaddr == NULL) {\r\nprintk(KERN_ERR "perfmon: pfm_remove_smpl_mapping [%d] invalid context mm=%p\n", task_pid_nr(task), task->mm);\r\nreturn -EINVAL;\r\n}\r\nDPRINT(("smpl_vaddr=%p size=%lu\n", vaddr, size));\r\nr = vm_munmap((unsigned long)vaddr, size);\r\nif (r !=0) {\r\nprintk(KERN_ERR "perfmon: [%d] unable to unmap sampling buffer @%p size=%lu\n", task_pid_nr(task), vaddr, size);\r\n}\r\nDPRINT(("do_unmap(%p, %lu)=%d\n", vaddr, size, r));\r\nreturn 0;\r\n}\r\nstatic inline void\r\npfm_exit_smpl_buffer(pfm_buffer_fmt_t *fmt)\r\n{\r\nif (fmt == NULL) return;\r\npfm_buf_fmt_exit(fmt, current, NULL, NULL);\r\n}\r\nstatic int __init\r\ninit_pfm_fs(void)\r\n{\r\nint err = register_filesystem(&pfm_fs_type);\r\nif (!err) {\r\npfmfs_mnt = kern_mount(&pfm_fs_type);\r\nerr = PTR_ERR(pfmfs_mnt);\r\nif (IS_ERR(pfmfs_mnt))\r\nunregister_filesystem(&pfm_fs_type);\r\nelse\r\nerr = 0;\r\n}\r\nreturn err;\r\n}\r\nstatic ssize_t\r\npfm_read(struct file *filp, char __user *buf, size_t size, loff_t *ppos)\r\n{\r\npfm_context_t *ctx;\r\npfm_msg_t *msg;\r\nssize_t ret;\r\nunsigned long flags;\r\nDECLARE_WAITQUEUE(wait, current);\r\nif (PFM_IS_FILE(filp) == 0) {\r\nprintk(KERN_ERR "perfmon: pfm_poll: bad magic [%d]\n", task_pid_nr(current));\r\nreturn -EINVAL;\r\n}\r\nctx = filp->private_data;\r\nif (ctx == NULL) {\r\nprintk(KERN_ERR "perfmon: pfm_read: NULL ctx [%d]\n", task_pid_nr(current));\r\nreturn -EINVAL;\r\n}\r\nif (size < sizeof(pfm_msg_t)) {\r\nDPRINT(("message is too small ctx=%p (>=%ld)\n", ctx, sizeof(pfm_msg_t)));\r\nreturn -EINVAL;\r\n}\r\nPROTECT_CTX(ctx, flags);\r\nadd_wait_queue(&ctx->ctx_msgq_wait, &wait);\r\nfor(;;) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nDPRINT(("head=%d tail=%d\n", ctx->ctx_msgq_head, ctx->ctx_msgq_tail));\r\nret = 0;\r\nif(PFM_CTXQ_EMPTY(ctx) == 0) break;\r\nUNPROTECT_CTX(ctx, flags);\r\nret = -EAGAIN;\r\nif(filp->f_flags & O_NONBLOCK) break;\r\nif(signal_pending(current)) {\r\nret = -EINTR;\r\nbreak;\r\n}\r\nschedule();\r\nPROTECT_CTX(ctx, flags);\r\n}\r\nDPRINT(("[%d] back to running ret=%ld\n", task_pid_nr(current), ret));\r\nset_current_state(TASK_RUNNING);\r\nremove_wait_queue(&ctx->ctx_msgq_wait, &wait);\r\nif (ret < 0) goto abort;\r\nret = -EINVAL;\r\nmsg = pfm_get_next_msg(ctx);\r\nif (msg == NULL) {\r\nprintk(KERN_ERR "perfmon: pfm_read no msg for ctx=%p [%d]\n", ctx, task_pid_nr(current));\r\ngoto abort_locked;\r\n}\r\nDPRINT(("fd=%d type=%d\n", msg->pfm_gen_msg.msg_ctx_fd, msg->pfm_gen_msg.msg_type));\r\nret = -EFAULT;\r\nif(copy_to_user(buf, msg, sizeof(pfm_msg_t)) == 0) ret = sizeof(pfm_msg_t);\r\nabort_locked:\r\nUNPROTECT_CTX(ctx, flags);\r\nabort:\r\nreturn ret;\r\n}\r\nstatic ssize_t\r\npfm_write(struct file *file, const char __user *ubuf,\r\nsize_t size, loff_t *ppos)\r\n{\r\nDPRINT(("pfm_write called\n"));\r\nreturn -EINVAL;\r\n}\r\nstatic unsigned int\r\npfm_poll(struct file *filp, poll_table * wait)\r\n{\r\npfm_context_t *ctx;\r\nunsigned long flags;\r\nunsigned int mask = 0;\r\nif (PFM_IS_FILE(filp) == 0) {\r\nprintk(KERN_ERR "perfmon: pfm_poll: bad magic [%d]\n", task_pid_nr(current));\r\nreturn 0;\r\n}\r\nctx = filp->private_data;\r\nif (ctx == NULL) {\r\nprintk(KERN_ERR "perfmon: pfm_poll: NULL ctx [%d]\n", task_pid_nr(current));\r\nreturn 0;\r\n}\r\nDPRINT(("pfm_poll ctx_fd=%d before poll_wait\n", ctx->ctx_fd));\r\npoll_wait(filp, &ctx->ctx_msgq_wait, wait);\r\nPROTECT_CTX(ctx, flags);\r\nif (PFM_CTXQ_EMPTY(ctx) == 0)\r\nmask = POLLIN | POLLRDNORM;\r\nUNPROTECT_CTX(ctx, flags);\r\nDPRINT(("pfm_poll ctx_fd=%d mask=0x%x\n", ctx->ctx_fd, mask));\r\nreturn mask;\r\n}\r\nstatic long\r\npfm_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\r\n{\r\nDPRINT(("pfm_ioctl called\n"));\r\nreturn -EINVAL;\r\n}\r\nstatic inline int\r\npfm_do_fasync(int fd, struct file *filp, pfm_context_t *ctx, int on)\r\n{\r\nint ret;\r\nret = fasync_helper (fd, filp, on, &ctx->ctx_async_queue);\r\nDPRINT(("pfm_fasync called by [%d] on ctx_fd=%d on=%d async_queue=%p ret=%d\n",\r\ntask_pid_nr(current),\r\nfd,\r\non,\r\nctx->ctx_async_queue, ret));\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_fasync(int fd, struct file *filp, int on)\r\n{\r\npfm_context_t *ctx;\r\nint ret;\r\nif (PFM_IS_FILE(filp) == 0) {\r\nprintk(KERN_ERR "perfmon: pfm_fasync bad magic [%d]\n", task_pid_nr(current));\r\nreturn -EBADF;\r\n}\r\nctx = filp->private_data;\r\nif (ctx == NULL) {\r\nprintk(KERN_ERR "perfmon: pfm_fasync NULL ctx [%d]\n", task_pid_nr(current));\r\nreturn -EBADF;\r\n}\r\nret = pfm_do_fasync(fd, filp, ctx, on);\r\nDPRINT(("pfm_fasync called on ctx_fd=%d on=%d async_queue=%p ret=%d\n",\r\nfd,\r\non,\r\nctx->ctx_async_queue, ret));\r\nreturn ret;\r\n}\r\nstatic void\r\npfm_syswide_force_stop(void *info)\r\n{\r\npfm_context_t *ctx = (pfm_context_t *)info;\r\nstruct pt_regs *regs = task_pt_regs(current);\r\nstruct task_struct *owner;\r\nunsigned long flags;\r\nint ret;\r\nif (ctx->ctx_cpu != smp_processor_id()) {\r\nprintk(KERN_ERR "perfmon: pfm_syswide_force_stop for CPU%d but on CPU%d\n",\r\nctx->ctx_cpu,\r\nsmp_processor_id());\r\nreturn;\r\n}\r\nowner = GET_PMU_OWNER();\r\nif (owner != ctx->ctx_task) {\r\nprintk(KERN_ERR "perfmon: pfm_syswide_force_stop CPU%d unexpected owner [%d] instead of [%d]\n",\r\nsmp_processor_id(),\r\ntask_pid_nr(owner), task_pid_nr(ctx->ctx_task));\r\nreturn;\r\n}\r\nif (GET_PMU_CTX() != ctx) {\r\nprintk(KERN_ERR "perfmon: pfm_syswide_force_stop CPU%d unexpected ctx %p instead of %p\n",\r\nsmp_processor_id(),\r\nGET_PMU_CTX(), ctx);\r\nreturn;\r\n}\r\nDPRINT(("on CPU%d forcing system wide stop for [%d]\n", smp_processor_id(), task_pid_nr(ctx->ctx_task)));\r\nlocal_irq_save(flags);\r\nret = pfm_context_unload(ctx, NULL, 0, regs);\r\nif (ret) {\r\nDPRINT(("context_unload returned %d\n", ret));\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void\r\npfm_syswide_cleanup_other_cpu(pfm_context_t *ctx)\r\n{\r\nint ret;\r\nDPRINT(("calling CPU%d for cleanup\n", ctx->ctx_cpu));\r\nret = smp_call_function_single(ctx->ctx_cpu, pfm_syswide_force_stop, ctx, 1);\r\nDPRINT(("called CPU%d for cleanup ret=%d\n", ctx->ctx_cpu, ret));\r\n}\r\nstatic int\r\npfm_flush(struct file *filp, fl_owner_t id)\r\n{\r\npfm_context_t *ctx;\r\nstruct task_struct *task;\r\nstruct pt_regs *regs;\r\nunsigned long flags;\r\nunsigned long smpl_buf_size = 0UL;\r\nvoid *smpl_buf_vaddr = NULL;\r\nint state, is_system;\r\nif (PFM_IS_FILE(filp) == 0) {\r\nDPRINT(("bad magic for\n"));\r\nreturn -EBADF;\r\n}\r\nctx = filp->private_data;\r\nif (ctx == NULL) {\r\nprintk(KERN_ERR "perfmon: pfm_flush: NULL ctx [%d]\n", task_pid_nr(current));\r\nreturn -EBADF;\r\n}\r\nPROTECT_CTX(ctx, flags);\r\nstate = ctx->ctx_state;\r\nis_system = ctx->ctx_fl_system;\r\ntask = PFM_CTX_TASK(ctx);\r\nregs = task_pt_regs(task);\r\nDPRINT(("ctx_state=%d is_current=%d\n",\r\nstate,\r\ntask == current ? 1 : 0));\r\nif (task == current) {\r\n#ifdef CONFIG_SMP\r\nif (is_system && ctx->ctx_cpu != smp_processor_id()) {\r\nDPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));\r\nlocal_irq_restore(flags);\r\npfm_syswide_cleanup_other_cpu(ctx);\r\nlocal_irq_save(flags);\r\n} else\r\n#endif\r\n{\r\nDPRINT(("forcing unload\n"));\r\npfm_context_unload(ctx, NULL, 0, regs);\r\nDPRINT(("ctx_state=%d\n", ctx->ctx_state));\r\n}\r\n}\r\nif (ctx->ctx_smpl_vaddr && current->mm) {\r\nsmpl_buf_vaddr = ctx->ctx_smpl_vaddr;\r\nsmpl_buf_size = ctx->ctx_smpl_size;\r\n}\r\nUNPROTECT_CTX(ctx, flags);\r\nif (smpl_buf_vaddr) pfm_remove_smpl_mapping(smpl_buf_vaddr, smpl_buf_size);\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_close(struct inode *inode, struct file *filp)\r\n{\r\npfm_context_t *ctx;\r\nstruct task_struct *task;\r\nstruct pt_regs *regs;\r\nDECLARE_WAITQUEUE(wait, current);\r\nunsigned long flags;\r\nunsigned long smpl_buf_size = 0UL;\r\nvoid *smpl_buf_addr = NULL;\r\nint free_possible = 1;\r\nint state, is_system;\r\nDPRINT(("pfm_close called private=%p\n", filp->private_data));\r\nif (PFM_IS_FILE(filp) == 0) {\r\nDPRINT(("bad magic\n"));\r\nreturn -EBADF;\r\n}\r\nctx = filp->private_data;\r\nif (ctx == NULL) {\r\nprintk(KERN_ERR "perfmon: pfm_close: NULL ctx [%d]\n", task_pid_nr(current));\r\nreturn -EBADF;\r\n}\r\nPROTECT_CTX(ctx, flags);\r\nstate = ctx->ctx_state;\r\nis_system = ctx->ctx_fl_system;\r\ntask = PFM_CTX_TASK(ctx);\r\nregs = task_pt_regs(task);\r\nDPRINT(("ctx_state=%d is_current=%d\n",\r\nstate,\r\ntask == current ? 1 : 0));\r\nif (state == PFM_CTX_UNLOADED) goto doit;\r\nif (state == PFM_CTX_MASKED && CTX_OVFL_NOBLOCK(ctx) == 0) {\r\nctx->ctx_fl_going_zombie = 1;\r\ncomplete(&ctx->ctx_restart_done);\r\nDPRINT(("waking up ctx_state=%d\n", state));\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nadd_wait_queue(&ctx->ctx_zombieq, &wait);\r\nUNPROTECT_CTX(ctx, flags);\r\nschedule();\r\nPROTECT_CTX(ctx, flags);\r\nremove_wait_queue(&ctx->ctx_zombieq, &wait);\r\nset_current_state(TASK_RUNNING);\r\nDPRINT(("after zombie wakeup ctx_state=%d for\n", state));\r\n}\r\nelse if (task != current) {\r\n#ifdef CONFIG_SMP\r\nctx->ctx_state = PFM_CTX_ZOMBIE;\r\nDPRINT(("zombie ctx for [%d]\n", task_pid_nr(task)));\r\nfree_possible = 0;\r\n#else\r\npfm_context_unload(ctx, NULL, 0, regs);\r\n#endif\r\n}\r\ndoit:\r\nstate = ctx->ctx_state;\r\nif (ctx->ctx_smpl_hdr) {\r\nsmpl_buf_addr = ctx->ctx_smpl_hdr;\r\nsmpl_buf_size = ctx->ctx_smpl_size;\r\nctx->ctx_smpl_hdr = NULL;\r\nctx->ctx_fl_is_sampling = 0;\r\n}\r\nDPRINT(("ctx_state=%d free_possible=%d addr=%p size=%lu\n",\r\nstate,\r\nfree_possible,\r\nsmpl_buf_addr,\r\nsmpl_buf_size));\r\nif (smpl_buf_addr) pfm_exit_smpl_buffer(ctx->ctx_buf_fmt);\r\nif (state == PFM_CTX_ZOMBIE) {\r\npfm_unreserve_session(ctx, ctx->ctx_fl_system , ctx->ctx_cpu);\r\n}\r\nfilp->private_data = NULL;\r\nUNPROTECT_CTX(ctx, flags);\r\nif (smpl_buf_addr) pfm_rvfree(smpl_buf_addr, smpl_buf_size);\r\nif (free_possible) pfm_context_free(ctx);\r\nreturn 0;\r\n}\r\nstatic char *pfmfs_dname(struct dentry *dentry, char *buffer, int buflen)\r\n{\r\nreturn dynamic_dname(dentry, buffer, buflen, "pfm:[%lu]",\r\nd_inode(dentry)->i_ino);\r\n}\r\nstatic struct file *\r\npfm_alloc_file(pfm_context_t *ctx)\r\n{\r\nstruct file *file;\r\nstruct inode *inode;\r\nstruct path path;\r\nstruct qstr this = { .name = "" };\r\ninode = new_inode(pfmfs_mnt->mnt_sb);\r\nif (!inode)\r\nreturn ERR_PTR(-ENOMEM);\r\nDPRINT(("new inode ino=%ld @%p\n", inode->i_ino, inode));\r\ninode->i_mode = S_IFCHR|S_IRUGO;\r\ninode->i_uid = current_fsuid();\r\ninode->i_gid = current_fsgid();\r\npath.dentry = d_alloc(pfmfs_mnt->mnt_root, &this);\r\nif (!path.dentry) {\r\niput(inode);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\npath.mnt = mntget(pfmfs_mnt);\r\nd_add(path.dentry, inode);\r\nfile = alloc_file(&path, FMODE_READ, &pfm_file_ops);\r\nif (IS_ERR(file)) {\r\npath_put(&path);\r\nreturn file;\r\n}\r\nfile->f_flags = O_RDONLY;\r\nfile->private_data = ctx;\r\nreturn file;\r\n}\r\nstatic int\r\npfm_remap_buffer(struct vm_area_struct *vma, unsigned long buf, unsigned long addr, unsigned long size)\r\n{\r\nDPRINT(("CPU%d buf=0x%lx addr=0x%lx size=%ld\n", smp_processor_id(), buf, addr, size));\r\nwhile (size > 0) {\r\nunsigned long pfn = ia64_tpa(buf) >> PAGE_SHIFT;\r\nif (remap_pfn_range(vma, addr, pfn, PAGE_SIZE, PAGE_READONLY))\r\nreturn -ENOMEM;\r\naddr += PAGE_SIZE;\r\nbuf += PAGE_SIZE;\r\nsize -= PAGE_SIZE;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_smpl_buffer_alloc(struct task_struct *task, struct file *filp, pfm_context_t *ctx, unsigned long rsize, void **user_vaddr)\r\n{\r\nstruct mm_struct *mm = task->mm;\r\nstruct vm_area_struct *vma = NULL;\r\nunsigned long size;\r\nvoid *smpl_buf;\r\nsize = PAGE_ALIGN(rsize);\r\nDPRINT(("sampling buffer rsize=%lu size=%lu bytes\n", rsize, size));\r\nif (size > task_rlimit(task, RLIMIT_MEMLOCK))\r\nreturn -ENOMEM;\r\nsmpl_buf = pfm_rvmalloc(size);\r\nif (smpl_buf == NULL) {\r\nDPRINT(("Can't allocate sampling buffer\n"));\r\nreturn -ENOMEM;\r\n}\r\nDPRINT(("smpl_buf @%p\n", smpl_buf));\r\nvma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);\r\nif (!vma) {\r\nDPRINT(("Cannot allocate vma\n"));\r\ngoto error_kmem;\r\n}\r\nINIT_LIST_HEAD(&vma->anon_vma_chain);\r\nvma->vm_mm = mm;\r\nvma->vm_file = get_file(filp);\r\nvma->vm_flags = VM_READ|VM_MAYREAD|VM_DONTEXPAND|VM_DONTDUMP;\r\nvma->vm_page_prot = PAGE_READONLY;\r\nctx->ctx_smpl_hdr = smpl_buf;\r\nctx->ctx_smpl_size = size;\r\ndown_write(&task->mm->mmap_sem);\r\nvma->vm_start = get_unmapped_area(NULL, 0, size, 0, MAP_PRIVATE|MAP_ANONYMOUS);\r\nif (IS_ERR_VALUE(vma->vm_start)) {\r\nDPRINT(("Cannot find unmapped area for size %ld\n", size));\r\nup_write(&task->mm->mmap_sem);\r\ngoto error;\r\n}\r\nvma->vm_end = vma->vm_start + size;\r\nvma->vm_pgoff = vma->vm_start >> PAGE_SHIFT;\r\nDPRINT(("aligned size=%ld, hdr=%p mapped @0x%lx\n", size, ctx->ctx_smpl_hdr, vma->vm_start));\r\nif (pfm_remap_buffer(vma, (unsigned long)smpl_buf, vma->vm_start, size)) {\r\nDPRINT(("Can't remap buffer\n"));\r\nup_write(&task->mm->mmap_sem);\r\ngoto error;\r\n}\r\ninsert_vm_struct(mm, vma);\r\nvm_stat_account(vma->vm_mm, vma->vm_flags, vma_pages(vma));\r\nup_write(&task->mm->mmap_sem);\r\nctx->ctx_smpl_vaddr = (void *)vma->vm_start;\r\n*(unsigned long *)user_vaddr = vma->vm_start;\r\nreturn 0;\r\nerror:\r\nkmem_cache_free(vm_area_cachep, vma);\r\nerror_kmem:\r\npfm_rvfree(smpl_buf, size);\r\nreturn -ENOMEM;\r\n}\r\nstatic int\r\npfm_bad_permissions(struct task_struct *task)\r\n{\r\nconst struct cred *tcred;\r\nkuid_t uid = current_uid();\r\nkgid_t gid = current_gid();\r\nint ret;\r\nrcu_read_lock();\r\ntcred = __task_cred(task);\r\nDPRINT(("cur: uid=%d gid=%d task: euid=%d suid=%d uid=%d egid=%d sgid=%d\n",\r\nfrom_kuid(&init_user_ns, uid),\r\nfrom_kgid(&init_user_ns, gid),\r\nfrom_kuid(&init_user_ns, tcred->euid),\r\nfrom_kuid(&init_user_ns, tcred->suid),\r\nfrom_kuid(&init_user_ns, tcred->uid),\r\nfrom_kgid(&init_user_ns, tcred->egid),\r\nfrom_kgid(&init_user_ns, tcred->sgid)));\r\nret = ((!uid_eq(uid, tcred->euid))\r\n|| (!uid_eq(uid, tcred->suid))\r\n|| (!uid_eq(uid, tcred->uid))\r\n|| (!gid_eq(gid, tcred->egid))\r\n|| (!gid_eq(gid, tcred->sgid))\r\n|| (!gid_eq(gid, tcred->gid))) && !capable(CAP_SYS_PTRACE);\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int\r\npfarg_is_sane(struct task_struct *task, pfarg_context_t *pfx)\r\n{\r\nint ctx_flags;\r\nctx_flags = pfx->ctx_flags;\r\nif (ctx_flags & PFM_FL_SYSTEM_WIDE) {\r\nif (ctx_flags & PFM_FL_NOTIFY_BLOCK) {\r\nDPRINT(("cannot use blocking mode when in system wide monitoring\n"));\r\nreturn -EINVAL;\r\n}\r\n} else {\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_setup_buffer_fmt(struct task_struct *task, struct file *filp, pfm_context_t *ctx, unsigned int ctx_flags,\r\nunsigned int cpu, pfarg_context_t *arg)\r\n{\r\npfm_buffer_fmt_t *fmt = NULL;\r\nunsigned long size = 0UL;\r\nvoid *uaddr = NULL;\r\nvoid *fmt_arg = NULL;\r\nint ret = 0;\r\n#define PFM_CTXARG_BUF_ARG(a) (pfm_buffer_fmt_t *)(a+1)\r\nfmt = pfm_find_buffer_fmt(arg->ctx_smpl_buf_id);\r\nif (fmt == NULL) {\r\nDPRINT(("[%d] cannot find buffer format\n", task_pid_nr(task)));\r\nreturn -EINVAL;\r\n}\r\nif (fmt->fmt_arg_size) fmt_arg = PFM_CTXARG_BUF_ARG(arg);\r\nret = pfm_buf_fmt_validate(fmt, task, ctx_flags, cpu, fmt_arg);\r\nDPRINT(("[%d] after validate(0x%x,%d,%p)=%d\n", task_pid_nr(task), ctx_flags, cpu, fmt_arg, ret));\r\nif (ret) goto error;\r\nctx->ctx_buf_fmt = fmt;\r\nctx->ctx_fl_is_sampling = 1;\r\nret = pfm_buf_fmt_getsize(fmt, task, ctx_flags, cpu, fmt_arg, &size);\r\nif (ret) goto error;\r\nif (size) {\r\nret = pfm_smpl_buffer_alloc(current, filp, ctx, size, &uaddr);\r\nif (ret) goto error;\r\narg->ctx_smpl_vaddr = uaddr;\r\n}\r\nret = pfm_buf_fmt_init(fmt, task, ctx->ctx_smpl_hdr, ctx_flags, cpu, fmt_arg);\r\nerror:\r\nreturn ret;\r\n}\r\nstatic void\r\npfm_reset_pmu_state(pfm_context_t *ctx)\r\n{\r\nint i;\r\nfor (i=1; PMC_IS_LAST(i) == 0; i++) {\r\nif (PMC_IS_IMPL(i) == 0) continue;\r\nctx->ctx_pmcs[i] = PMC_DFL_VAL(i);\r\nDPRINT(("pmc[%d]=0x%lx\n", i, ctx->ctx_pmcs[i]));\r\n}\r\nctx->ctx_all_pmcs[0] = pmu_conf->impl_pmcs[0] & ~0x1;\r\nctx->ctx_all_pmds[0] = pmu_conf->impl_pmds[0];\r\nDPRINT(("<%d> all_pmcs=0x%lx all_pmds=0x%lx\n", ctx->ctx_fd, ctx->ctx_all_pmcs[0],ctx->ctx_all_pmds[0]));\r\nctx->ctx_used_ibrs[0] = 0UL;\r\nctx->ctx_used_dbrs[0] = 0UL;\r\n}\r\nstatic int\r\npfm_ctx_getsize(void *arg, size_t *sz)\r\n{\r\npfarg_context_t *req = (pfarg_context_t *)arg;\r\npfm_buffer_fmt_t *fmt;\r\n*sz = 0;\r\nif (!pfm_uuid_cmp(req->ctx_smpl_buf_id, pfm_null_uuid)) return 0;\r\nfmt = pfm_find_buffer_fmt(req->ctx_smpl_buf_id);\r\nif (fmt == NULL) {\r\nDPRINT(("cannot find buffer format\n"));\r\nreturn -EINVAL;\r\n}\r\n*sz = fmt->fmt_arg_size;\r\nDPRINT(("arg_size=%lu\n", *sz));\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_task_incompatible(pfm_context_t *ctx, struct task_struct *task)\r\n{\r\nif (task->mm == NULL) {\r\nDPRINT(("task [%d] has not memory context (kernel thread)\n", task_pid_nr(task)));\r\nreturn -EPERM;\r\n}\r\nif (pfm_bad_permissions(task)) {\r\nDPRINT(("no permission to attach to [%d]\n", task_pid_nr(task)));\r\nreturn -EPERM;\r\n}\r\nif (CTX_OVFL_NOBLOCK(ctx) == 0 && task == current) {\r\nDPRINT(("cannot load a blocking context on self for [%d]\n", task_pid_nr(task)));\r\nreturn -EINVAL;\r\n}\r\nif (task->exit_state == EXIT_ZOMBIE) {\r\nDPRINT(("cannot attach to zombie task [%d]\n", task_pid_nr(task)));\r\nreturn -EBUSY;\r\n}\r\nif (task == current) return 0;\r\nif (!task_is_stopped_or_traced(task)) {\r\nDPRINT(("cannot attach to non-stopped task [%d] state=%ld\n", task_pid_nr(task), task->state));\r\nreturn -EBUSY;\r\n}\r\nwait_task_inactive(task, 0);\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_get_task(pfm_context_t *ctx, pid_t pid, struct task_struct **task)\r\n{\r\nstruct task_struct *p = current;\r\nint ret;\r\nif (pid < 2) return -EPERM;\r\nif (pid != task_pid_vnr(current)) {\r\nread_lock(&tasklist_lock);\r\np = find_task_by_vpid(pid);\r\nif (p) get_task_struct(p);\r\nread_unlock(&tasklist_lock);\r\nif (p == NULL) return -ESRCH;\r\n}\r\nret = pfm_task_incompatible(ctx, p);\r\nif (ret == 0) {\r\n*task = p;\r\n} else if (p != current) {\r\npfm_put_task(p);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_context_create(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\npfarg_context_t *req = (pfarg_context_t *)arg;\r\nstruct file *filp;\r\nstruct path path;\r\nint ctx_flags;\r\nint fd;\r\nint ret;\r\nret = pfarg_is_sane(current, req);\r\nif (ret < 0)\r\nreturn ret;\r\nctx_flags = req->ctx_flags;\r\nret = -ENOMEM;\r\nfd = get_unused_fd_flags(0);\r\nif (fd < 0)\r\nreturn fd;\r\nctx = pfm_context_alloc(ctx_flags);\r\nif (!ctx)\r\ngoto error;\r\nfilp = pfm_alloc_file(ctx);\r\nif (IS_ERR(filp)) {\r\nret = PTR_ERR(filp);\r\ngoto error_file;\r\n}\r\nreq->ctx_fd = ctx->ctx_fd = fd;\r\nif (pfm_uuid_cmp(req->ctx_smpl_buf_id, pfm_null_uuid)) {\r\nret = pfm_setup_buffer_fmt(current, filp, ctx, ctx_flags, 0, req);\r\nif (ret)\r\ngoto buffer_error;\r\n}\r\nDPRINT(("ctx=%p flags=0x%x system=%d notify_block=%d excl_idle=%d no_msg=%d ctx_fd=%d\n",\r\nctx,\r\nctx_flags,\r\nctx->ctx_fl_system,\r\nctx->ctx_fl_block,\r\nctx->ctx_fl_excl_idle,\r\nctx->ctx_fl_no_msg,\r\nctx->ctx_fd));\r\npfm_reset_pmu_state(ctx);\r\nfd_install(fd, filp);\r\nreturn 0;\r\nbuffer_error:\r\npath = filp->f_path;\r\nput_filp(filp);\r\npath_put(&path);\r\nif (ctx->ctx_buf_fmt) {\r\npfm_buf_fmt_exit(ctx->ctx_buf_fmt, current, NULL, regs);\r\n}\r\nerror_file:\r\npfm_context_free(ctx);\r\nerror:\r\nput_unused_fd(fd);\r\nreturn ret;\r\n}\r\nstatic inline unsigned long\r\npfm_new_counter_value (pfm_counter_t *reg, int is_long_reset)\r\n{\r\nunsigned long val = is_long_reset ? reg->long_reset : reg->short_reset;\r\nunsigned long new_seed, old_seed = reg->seed, mask = reg->mask;\r\nextern unsigned long carta_random32 (unsigned long seed);\r\nif (reg->flags & PFM_REGFL_RANDOM) {\r\nnew_seed = carta_random32(old_seed);\r\nval -= (old_seed & mask);\r\nif ((mask >> 32) != 0)\r\nnew_seed |= carta_random32(old_seed >> 32) << 32;\r\nreg->seed = new_seed;\r\n}\r\nreg->lval = val;\r\nreturn val;\r\n}\r\nstatic void\r\npfm_reset_regs_masked(pfm_context_t *ctx, unsigned long *ovfl_regs, int is_long_reset)\r\n{\r\nunsigned long mask = ovfl_regs[0];\r\nunsigned long reset_others = 0UL;\r\nunsigned long val;\r\nint i;\r\nmask >>= PMU_FIRST_COUNTER;\r\nfor(i = PMU_FIRST_COUNTER; mask; i++, mask >>= 1) {\r\nif ((mask & 0x1UL) == 0UL) continue;\r\nctx->ctx_pmds[i].val = val = pfm_new_counter_value(ctx->ctx_pmds+ i, is_long_reset);\r\nreset_others |= ctx->ctx_pmds[i].reset_pmds[0];\r\nDPRINT_ovfl((" %s reset ctx_pmds[%d]=%lx\n", is_long_reset ? "long" : "short", i, val));\r\n}\r\nfor(i = 0; reset_others; i++, reset_others >>= 1) {\r\nif ((reset_others & 0x1) == 0) continue;\r\nctx->ctx_pmds[i].val = val = pfm_new_counter_value(ctx->ctx_pmds + i, is_long_reset);\r\nDPRINT_ovfl(("%s reset_others pmd[%d]=%lx\n",\r\nis_long_reset ? "long" : "short", i, val));\r\n}\r\n}\r\nstatic void\r\npfm_reset_regs(pfm_context_t *ctx, unsigned long *ovfl_regs, int is_long_reset)\r\n{\r\nunsigned long mask = ovfl_regs[0];\r\nunsigned long reset_others = 0UL;\r\nunsigned long val;\r\nint i;\r\nDPRINT_ovfl(("ovfl_regs=0x%lx is_long_reset=%d\n", ovfl_regs[0], is_long_reset));\r\nif (ctx->ctx_state == PFM_CTX_MASKED) {\r\npfm_reset_regs_masked(ctx, ovfl_regs, is_long_reset);\r\nreturn;\r\n}\r\nmask >>= PMU_FIRST_COUNTER;\r\nfor(i = PMU_FIRST_COUNTER; mask; i++, mask >>= 1) {\r\nif ((mask & 0x1UL) == 0UL) continue;\r\nval = pfm_new_counter_value(ctx->ctx_pmds+ i, is_long_reset);\r\nreset_others |= ctx->ctx_pmds[i].reset_pmds[0];\r\nDPRINT_ovfl((" %s reset ctx_pmds[%d]=%lx\n", is_long_reset ? "long" : "short", i, val));\r\npfm_write_soft_counter(ctx, i, val);\r\n}\r\nfor(i = 0; reset_others; i++, reset_others >>= 1) {\r\nif ((reset_others & 0x1) == 0) continue;\r\nval = pfm_new_counter_value(ctx->ctx_pmds + i, is_long_reset);\r\nif (PMD_IS_COUNTING(i)) {\r\npfm_write_soft_counter(ctx, i, val);\r\n} else {\r\nia64_set_pmd(i, val);\r\n}\r\nDPRINT_ovfl(("%s reset_others pmd[%d]=%lx\n",\r\nis_long_reset ? "long" : "short", i, val));\r\n}\r\nia64_srlz_d();\r\n}\r\nstatic int\r\npfm_write_pmcs(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nstruct task_struct *task;\r\npfarg_reg_t *req = (pfarg_reg_t *)arg;\r\nunsigned long value, pmc_pm;\r\nunsigned long smpl_pmds, reset_pmds, impl_pmds;\r\nunsigned int cnum, reg_flags, flags, pmc_type;\r\nint i, can_access_pmu = 0, is_loaded, is_system, expert_mode;\r\nint is_monitor, is_counting, state;\r\nint ret = -EINVAL;\r\npfm_reg_check_t wr_func;\r\n#define PFM_CHECK_PMC_PM(x, y, z) ((x)->ctx_fl_system ^ PMC_PM(y, z))\r\nstate = ctx->ctx_state;\r\nis_loaded = state == PFM_CTX_LOADED ? 1 : 0;\r\nis_system = ctx->ctx_fl_system;\r\ntask = ctx->ctx_task;\r\nimpl_pmds = pmu_conf->impl_pmds[0];\r\nif (state == PFM_CTX_ZOMBIE) return -EINVAL;\r\nif (is_loaded) {\r\nif (is_system && ctx->ctx_cpu != smp_processor_id()) {\r\nDPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));\r\nreturn -EBUSY;\r\n}\r\ncan_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;\r\n}\r\nexpert_mode = pfm_sysctl.expert_mode;\r\nfor (i = 0; i < count; i++, req++) {\r\ncnum = req->reg_num;\r\nreg_flags = req->reg_flags;\r\nvalue = req->reg_value;\r\nsmpl_pmds = req->reg_smpl_pmds[0];\r\nreset_pmds = req->reg_reset_pmds[0];\r\nflags = 0;\r\nif (cnum >= PMU_MAX_PMCS) {\r\nDPRINT(("pmc%u is invalid\n", cnum));\r\ngoto error;\r\n}\r\npmc_type = pmu_conf->pmc_desc[cnum].type;\r\npmc_pm = (value >> pmu_conf->pmc_desc[cnum].pm_pos) & 0x1;\r\nis_counting = (pmc_type & PFM_REG_COUNTING) == PFM_REG_COUNTING ? 1 : 0;\r\nis_monitor = (pmc_type & PFM_REG_MONITOR) == PFM_REG_MONITOR ? 1 : 0;\r\nif ((pmc_type & PFM_REG_IMPL) == 0 || (pmc_type & PFM_REG_CONTROL) == PFM_REG_CONTROL) {\r\nDPRINT(("pmc%u is unimplemented or no-access pmc_type=%x\n", cnum, pmc_type));\r\ngoto error;\r\n}\r\nwr_func = pmu_conf->pmc_desc[cnum].write_check;\r\nif (is_monitor && value != PMC_DFL_VAL(cnum) && is_system ^ pmc_pm) {\r\nDPRINT(("pmc%u pmc_pm=%lu is_system=%d\n",\r\ncnum,\r\npmc_pm,\r\nis_system));\r\ngoto error;\r\n}\r\nif (is_counting) {\r\nvalue |= 1 << PMU_PMC_OI;\r\nif (reg_flags & PFM_REGFL_OVFL_NOTIFY) {\r\nflags |= PFM_REGFL_OVFL_NOTIFY;\r\n}\r\nif (reg_flags & PFM_REGFL_RANDOM) flags |= PFM_REGFL_RANDOM;\r\nif ((smpl_pmds & impl_pmds) != smpl_pmds) {\r\nDPRINT(("invalid smpl_pmds 0x%lx for pmc%u\n", smpl_pmds, cnum));\r\ngoto error;\r\n}\r\nif ((reset_pmds & impl_pmds) != reset_pmds) {\r\nDPRINT(("invalid reset_pmds 0x%lx for pmc%u\n", reset_pmds, cnum));\r\ngoto error;\r\n}\r\n} else {\r\nif (reg_flags & (PFM_REGFL_OVFL_NOTIFY|PFM_REGFL_RANDOM)) {\r\nDPRINT(("cannot set ovfl_notify or random on pmc%u\n", cnum));\r\ngoto error;\r\n}\r\n}\r\nif (likely(expert_mode == 0 && wr_func)) {\r\nret = (*wr_func)(task, ctx, cnum, &value, regs);\r\nif (ret) goto error;\r\nret = -EINVAL;\r\n}\r\nPFM_REG_RETFLAG_SET(req->reg_flags, 0);\r\nif (is_counting) {\r\nctx->ctx_pmds[cnum].flags = flags;\r\nctx->ctx_pmds[cnum].reset_pmds[0] = reset_pmds;\r\nctx->ctx_pmds[cnum].smpl_pmds[0] = smpl_pmds;\r\nctx->ctx_pmds[cnum].eventid = req->reg_smpl_eventid;\r\nCTX_USED_PMD(ctx, reset_pmds);\r\nCTX_USED_PMD(ctx, smpl_pmds);\r\nif (state == PFM_CTX_MASKED) ctx->ctx_ovfl_regs[0] &= ~1UL << cnum;\r\n}\r\nCTX_USED_PMD(ctx, pmu_conf->pmc_desc[cnum].dep_pmd[0]);\r\nif (is_monitor) CTX_USED_MONITOR(ctx, 1UL << cnum);\r\nctx->ctx_pmcs[cnum] = value;\r\nif (is_loaded) {\r\nif (is_system == 0) ctx->th_pmcs[cnum] = value;\r\nif (can_access_pmu) {\r\nia64_set_pmc(cnum, value);\r\n}\r\n#ifdef CONFIG_SMP\r\nelse {\r\nctx->ctx_reload_pmcs[0] |= 1UL << cnum;\r\n}\r\n#endif\r\n}\r\nDPRINT(("pmc[%u]=0x%lx ld=%d apmu=%d flags=0x%x all_pmcs=0x%lx used_pmds=0x%lx eventid=%ld smpl_pmds=0x%lx reset_pmds=0x%lx reloads_pmcs=0x%lx used_monitors=0x%lx ovfl_regs=0x%lx\n",\r\ncnum,\r\nvalue,\r\nis_loaded,\r\ncan_access_pmu,\r\nflags,\r\nctx->ctx_all_pmcs[0],\r\nctx->ctx_used_pmds[0],\r\nctx->ctx_pmds[cnum].eventid,\r\nsmpl_pmds,\r\nreset_pmds,\r\nctx->ctx_reload_pmcs[0],\r\nctx->ctx_used_monitors[0],\r\nctx->ctx_ovfl_regs[0]));\r\n}\r\nif (can_access_pmu) ia64_srlz_d();\r\nreturn 0;\r\nerror:\r\nPFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_write_pmds(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nstruct task_struct *task;\r\npfarg_reg_t *req = (pfarg_reg_t *)arg;\r\nunsigned long value, hw_value, ovfl_mask;\r\nunsigned int cnum;\r\nint i, can_access_pmu = 0, state;\r\nint is_counting, is_loaded, is_system, expert_mode;\r\nint ret = -EINVAL;\r\npfm_reg_check_t wr_func;\r\nstate = ctx->ctx_state;\r\nis_loaded = state == PFM_CTX_LOADED ? 1 : 0;\r\nis_system = ctx->ctx_fl_system;\r\novfl_mask = pmu_conf->ovfl_val;\r\ntask = ctx->ctx_task;\r\nif (unlikely(state == PFM_CTX_ZOMBIE)) return -EINVAL;\r\nif (likely(is_loaded)) {\r\nif (unlikely(is_system && ctx->ctx_cpu != smp_processor_id())) {\r\nDPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));\r\nreturn -EBUSY;\r\n}\r\ncan_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;\r\n}\r\nexpert_mode = pfm_sysctl.expert_mode;\r\nfor (i = 0; i < count; i++, req++) {\r\ncnum = req->reg_num;\r\nvalue = req->reg_value;\r\nif (!PMD_IS_IMPL(cnum)) {\r\nDPRINT(("pmd[%u] is unimplemented or invalid\n", cnum));\r\ngoto abort_mission;\r\n}\r\nis_counting = PMD_IS_COUNTING(cnum);\r\nwr_func = pmu_conf->pmd_desc[cnum].write_check;\r\nif (unlikely(expert_mode == 0 && wr_func)) {\r\nunsigned long v = value;\r\nret = (*wr_func)(task, ctx, cnum, &v, regs);\r\nif (ret) goto abort_mission;\r\nvalue = v;\r\nret = -EINVAL;\r\n}\r\nPFM_REG_RETFLAG_SET(req->reg_flags, 0);\r\nhw_value = value;\r\nif (is_counting) {\r\nctx->ctx_pmds[cnum].lval = value;\r\nif (is_loaded) {\r\nhw_value = value & ovfl_mask;\r\nvalue = value & ~ovfl_mask;\r\n}\r\n}\r\nctx->ctx_pmds[cnum].long_reset = req->reg_long_reset;\r\nctx->ctx_pmds[cnum].short_reset = req->reg_short_reset;\r\nctx->ctx_pmds[cnum].seed = req->reg_random_seed;\r\nctx->ctx_pmds[cnum].mask = req->reg_random_mask;\r\nctx->ctx_pmds[cnum].val = value;\r\nCTX_USED_PMD(ctx, PMD_PMD_DEP(cnum));\r\nCTX_USED_PMD(ctx, RDEP(cnum));\r\nif (is_counting && state == PFM_CTX_MASKED) {\r\nctx->ctx_ovfl_regs[0] &= ~1UL << cnum;\r\n}\r\nif (is_loaded) {\r\nif (is_system == 0) ctx->th_pmds[cnum] = hw_value;\r\nif (can_access_pmu) {\r\nia64_set_pmd(cnum, hw_value);\r\n} else {\r\n#ifdef CONFIG_SMP\r\nctx->ctx_reload_pmds[0] |= 1UL << cnum;\r\n#endif\r\n}\r\n}\r\nDPRINT(("pmd[%u]=0x%lx ld=%d apmu=%d, hw_value=0x%lx ctx_pmd=0x%lx short_reset=0x%lx "\r\n"long_reset=0x%lx notify=%c seed=0x%lx mask=0x%lx used_pmds=0x%lx reset_pmds=0x%lx reload_pmds=0x%lx all_pmds=0x%lx ovfl_regs=0x%lx\n",\r\ncnum,\r\nvalue,\r\nis_loaded,\r\ncan_access_pmu,\r\nhw_value,\r\nctx->ctx_pmds[cnum].val,\r\nctx->ctx_pmds[cnum].short_reset,\r\nctx->ctx_pmds[cnum].long_reset,\r\nPMC_OVFL_NOTIFY(ctx, cnum) ? 'Y':'N',\r\nctx->ctx_pmds[cnum].seed,\r\nctx->ctx_pmds[cnum].mask,\r\nctx->ctx_used_pmds[0],\r\nctx->ctx_pmds[cnum].reset_pmds[0],\r\nctx->ctx_reload_pmds[0],\r\nctx->ctx_all_pmds[0],\r\nctx->ctx_ovfl_regs[0]));\r\n}\r\nif (can_access_pmu) ia64_srlz_d();\r\nreturn 0;\r\nabort_mission:\r\nPFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_read_pmds(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nstruct task_struct *task;\r\nunsigned long val = 0UL, lval, ovfl_mask, sval;\r\npfarg_reg_t *req = (pfarg_reg_t *)arg;\r\nunsigned int cnum, reg_flags = 0;\r\nint i, can_access_pmu = 0, state;\r\nint is_loaded, is_system, is_counting, expert_mode;\r\nint ret = -EINVAL;\r\npfm_reg_check_t rd_func;\r\nstate = ctx->ctx_state;\r\nis_loaded = state == PFM_CTX_LOADED ? 1 : 0;\r\nis_system = ctx->ctx_fl_system;\r\novfl_mask = pmu_conf->ovfl_val;\r\ntask = ctx->ctx_task;\r\nif (state == PFM_CTX_ZOMBIE) return -EINVAL;\r\nif (likely(is_loaded)) {\r\nif (unlikely(is_system && ctx->ctx_cpu != smp_processor_id())) {\r\nDPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));\r\nreturn -EBUSY;\r\n}\r\ncan_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;\r\nif (can_access_pmu) ia64_srlz_d();\r\n}\r\nexpert_mode = pfm_sysctl.expert_mode;\r\nDPRINT(("ld=%d apmu=%d ctx_state=%d\n",\r\nis_loaded,\r\ncan_access_pmu,\r\nstate));\r\nfor (i = 0; i < count; i++, req++) {\r\ncnum = req->reg_num;\r\nreg_flags = req->reg_flags;\r\nif (unlikely(!PMD_IS_IMPL(cnum))) goto error;\r\nif (unlikely(!CTX_IS_USED_PMD(ctx, cnum))) goto error;\r\nsval = ctx->ctx_pmds[cnum].val;\r\nlval = ctx->ctx_pmds[cnum].lval;\r\nis_counting = PMD_IS_COUNTING(cnum);\r\nif (can_access_pmu){\r\nval = ia64_get_pmd(cnum);\r\n} else {\r\nval = is_loaded ? ctx->th_pmds[cnum] : 0UL;\r\n}\r\nrd_func = pmu_conf->pmd_desc[cnum].read_check;\r\nif (is_counting) {\r\nval &= ovfl_mask;\r\nval += sval;\r\n}\r\nif (unlikely(expert_mode == 0 && rd_func)) {\r\nunsigned long v = val;\r\nret = (*rd_func)(ctx->ctx_task, ctx, cnum, &v, regs);\r\nif (ret) goto error;\r\nval = v;\r\nret = -EINVAL;\r\n}\r\nPFM_REG_RETFLAG_SET(reg_flags, 0);\r\nDPRINT(("pmd[%u]=0x%lx\n", cnum, val));\r\nreq->reg_value = val;\r\nreq->reg_flags = reg_flags;\r\nreq->reg_last_reset_val = lval;\r\n}\r\nreturn 0;\r\nerror:\r\nPFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);\r\nreturn ret;\r\n}\r\nint\r\npfm_mod_write_pmcs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)\r\n{\r\npfm_context_t *ctx;\r\nif (req == NULL) return -EINVAL;\r\nctx = GET_PMU_CTX();\r\nif (ctx == NULL) return -EINVAL;\r\nif (task != current && ctx->ctx_fl_system == 0) return -EBUSY;\r\nreturn pfm_write_pmcs(ctx, req, nreq, regs);\r\n}\r\nint\r\npfm_mod_read_pmds(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)\r\n{\r\npfm_context_t *ctx;\r\nif (req == NULL) return -EINVAL;\r\nctx = GET_PMU_CTX();\r\nif (ctx == NULL) return -EINVAL;\r\nif (task != current && ctx->ctx_fl_system == 0) return -EBUSY;\r\nreturn pfm_read_pmds(ctx, req, nreq, regs);\r\n}\r\nint\r\npfm_use_debug_registers(struct task_struct *task)\r\n{\r\npfm_context_t *ctx = task->thread.pfm_context;\r\nunsigned long flags;\r\nint ret = 0;\r\nif (pmu_conf->use_rr_dbregs == 0) return 0;\r\nDPRINT(("called for [%d]\n", task_pid_nr(task)));\r\nif (task->thread.flags & IA64_THREAD_DBG_VALID) return 0;\r\nif (ctx && ctx->ctx_fl_using_dbreg == 1) return -1;\r\nLOCK_PFS(flags);\r\nif (pfm_sessions.pfs_sys_use_dbregs> 0)\r\nret = -1;\r\nelse\r\npfm_sessions.pfs_ptrace_use_dbregs++;\r\nDPRINT(("ptrace_use_dbregs=%u sys_use_dbregs=%u by [%d] ret = %d\n",\r\npfm_sessions.pfs_ptrace_use_dbregs,\r\npfm_sessions.pfs_sys_use_dbregs,\r\ntask_pid_nr(task), ret));\r\nUNLOCK_PFS(flags);\r\nreturn ret;\r\n}\r\nint\r\npfm_release_debug_registers(struct task_struct *task)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nif (pmu_conf->use_rr_dbregs == 0) return 0;\r\nLOCK_PFS(flags);\r\nif (pfm_sessions.pfs_ptrace_use_dbregs == 0) {\r\nprintk(KERN_ERR "perfmon: invalid release for [%d] ptrace_use_dbregs=0\n", task_pid_nr(task));\r\nret = -1;\r\n} else {\r\npfm_sessions.pfs_ptrace_use_dbregs--;\r\nret = 0;\r\n}\r\nUNLOCK_PFS(flags);\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_restart(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nstruct task_struct *task;\r\npfm_buffer_fmt_t *fmt;\r\npfm_ovfl_ctrl_t rst_ctrl;\r\nint state, is_system;\r\nint ret = 0;\r\nstate = ctx->ctx_state;\r\nfmt = ctx->ctx_buf_fmt;\r\nis_system = ctx->ctx_fl_system;\r\ntask = PFM_CTX_TASK(ctx);\r\nswitch(state) {\r\ncase PFM_CTX_MASKED:\r\nbreak;\r\ncase PFM_CTX_LOADED:\r\nif (CTX_HAS_SMPL(ctx) && fmt->fmt_restart_active) break;\r\ncase PFM_CTX_UNLOADED:\r\ncase PFM_CTX_ZOMBIE:\r\nDPRINT(("invalid state=%d\n", state));\r\nreturn -EBUSY;\r\ndefault:\r\nDPRINT(("state=%d, cannot operate (no active_restart handler)\n", state));\r\nreturn -EINVAL;\r\n}\r\nif (is_system && ctx->ctx_cpu != smp_processor_id()) {\r\nDPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));\r\nreturn -EBUSY;\r\n}\r\nif (unlikely(task == NULL)) {\r\nprintk(KERN_ERR "perfmon: [%d] pfm_restart no task\n", task_pid_nr(current));\r\nreturn -EINVAL;\r\n}\r\nif (task == current || is_system) {\r\nfmt = ctx->ctx_buf_fmt;\r\nDPRINT(("restarting self %d ovfl=0x%lx\n",\r\ntask_pid_nr(task),\r\nctx->ctx_ovfl_regs[0]));\r\nif (CTX_HAS_SMPL(ctx)) {\r\nprefetch(ctx->ctx_smpl_hdr);\r\nrst_ctrl.bits.mask_monitoring = 0;\r\nrst_ctrl.bits.reset_ovfl_pmds = 0;\r\nif (state == PFM_CTX_LOADED)\r\nret = pfm_buf_fmt_restart_active(fmt, task, &rst_ctrl, ctx->ctx_smpl_hdr, regs);\r\nelse\r\nret = pfm_buf_fmt_restart(fmt, task, &rst_ctrl, ctx->ctx_smpl_hdr, regs);\r\n} else {\r\nrst_ctrl.bits.mask_monitoring = 0;\r\nrst_ctrl.bits.reset_ovfl_pmds = 1;\r\n}\r\nif (ret == 0) {\r\nif (rst_ctrl.bits.reset_ovfl_pmds)\r\npfm_reset_regs(ctx, ctx->ctx_ovfl_regs, PFM_PMD_LONG_RESET);\r\nif (rst_ctrl.bits.mask_monitoring == 0) {\r\nDPRINT(("resuming monitoring for [%d]\n", task_pid_nr(task)));\r\nif (state == PFM_CTX_MASKED) pfm_restore_monitoring(task);\r\n} else {\r\nDPRINT(("keeping monitoring stopped for [%d]\n", task_pid_nr(task)));\r\n}\r\n}\r\nctx->ctx_ovfl_regs[0] = 0UL;\r\nctx->ctx_state = PFM_CTX_LOADED;\r\nctx->ctx_fl_can_restart = 0;\r\nreturn 0;\r\n}\r\nif (state == PFM_CTX_MASKED) {\r\nif (ctx->ctx_fl_can_restart == 0) return -EINVAL;\r\nctx->ctx_fl_can_restart = 0;\r\n}\r\nif (CTX_OVFL_NOBLOCK(ctx) == 0 && state == PFM_CTX_MASKED) {\r\nDPRINT(("unblocking [%d]\n", task_pid_nr(task)));\r\ncomplete(&ctx->ctx_restart_done);\r\n} else {\r\nDPRINT(("[%d] armed exit trap\n", task_pid_nr(task)));\r\nctx->ctx_fl_trap_reason = PFM_TRAP_REASON_RESET;\r\nPFM_SET_WORK_PENDING(task, 1);\r\nset_notify_resume(task);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_debug(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nunsigned int m = *(unsigned int *)arg;\r\npfm_sysctl.debug = m == 0 ? 0 : 1;\r\nprintk(KERN_INFO "perfmon debugging %s (timing reset)\n", pfm_sysctl.debug ? "on" : "off");\r\nif (m == 0) {\r\nmemset(pfm_stats, 0, sizeof(pfm_stats));\r\nfor(m=0; m < NR_CPUS; m++) pfm_stats[m].pfm_ovfl_intr_cycles_min = ~0UL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_write_ibr_dbr(int mode, pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nstruct thread_struct *thread = NULL;\r\nstruct task_struct *task;\r\npfarg_dbreg_t *req = (pfarg_dbreg_t *)arg;\r\nunsigned long flags;\r\ndbreg_t dbreg;\r\nunsigned int rnum;\r\nint first_time;\r\nint ret = 0, state;\r\nint i, can_access_pmu = 0;\r\nint is_system, is_loaded;\r\nif (pmu_conf->use_rr_dbregs == 0) return -EINVAL;\r\nstate = ctx->ctx_state;\r\nis_loaded = state == PFM_CTX_LOADED ? 1 : 0;\r\nis_system = ctx->ctx_fl_system;\r\ntask = ctx->ctx_task;\r\nif (state == PFM_CTX_ZOMBIE) return -EINVAL;\r\nif (is_loaded) {\r\nthread = &task->thread;\r\nif (unlikely(is_system && ctx->ctx_cpu != smp_processor_id())) {\r\nDPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));\r\nreturn -EBUSY;\r\n}\r\ncan_access_pmu = GET_PMU_OWNER() == task || is_system ? 1 : 0;\r\n}\r\nfirst_time = ctx->ctx_fl_using_dbreg == 0;\r\nif (is_loaded && (thread->flags & IA64_THREAD_DBG_VALID) != 0) {\r\nDPRINT(("debug registers already in use for [%d]\n", task_pid_nr(task)));\r\nreturn -EBUSY;\r\n}\r\nif (is_loaded) {\r\nLOCK_PFS(flags);\r\nif (first_time && is_system) {\r\nif (pfm_sessions.pfs_ptrace_use_dbregs)\r\nret = -EBUSY;\r\nelse\r\npfm_sessions.pfs_sys_use_dbregs++;\r\n}\r\nUNLOCK_PFS(flags);\r\n}\r\nif (ret != 0) return ret;\r\nctx->ctx_fl_using_dbreg = 1;\r\nif (first_time && can_access_pmu) {\r\nDPRINT(("[%d] clearing ibrs, dbrs\n", task_pid_nr(task)));\r\nfor (i=0; i < pmu_conf->num_ibrs; i++) {\r\nia64_set_ibr(i, 0UL);\r\nia64_dv_serialize_instruction();\r\n}\r\nia64_srlz_i();\r\nfor (i=0; i < pmu_conf->num_dbrs; i++) {\r\nia64_set_dbr(i, 0UL);\r\nia64_dv_serialize_data();\r\n}\r\nia64_srlz_d();\r\n}\r\nfor (i = 0; i < count; i++, req++) {\r\nrnum = req->dbreg_num;\r\ndbreg.val = req->dbreg_value;\r\nret = -EINVAL;\r\nif ((mode == PFM_CODE_RR && rnum >= PFM_NUM_IBRS) || ((mode == PFM_DATA_RR) && rnum >= PFM_NUM_DBRS)) {\r\nDPRINT(("invalid register %u val=0x%lx mode=%d i=%d count=%d\n",\r\nrnum, dbreg.val, mode, i, count));\r\ngoto abort_mission;\r\n}\r\nif (rnum & 0x1) {\r\nif (mode == PFM_CODE_RR)\r\ndbreg.ibr.ibr_x = 0;\r\nelse\r\ndbreg.dbr.dbr_r = dbreg.dbr.dbr_w = 0;\r\n}\r\nPFM_REG_RETFLAG_SET(req->dbreg_flags, 0);\r\nif (mode == PFM_CODE_RR) {\r\nCTX_USED_IBR(ctx, rnum);\r\nif (can_access_pmu) {\r\nia64_set_ibr(rnum, dbreg.val);\r\nia64_dv_serialize_instruction();\r\n}\r\nctx->ctx_ibrs[rnum] = dbreg.val;\r\nDPRINT(("write ibr%u=0x%lx used_ibrs=0x%x ld=%d apmu=%d\n",\r\nrnum, dbreg.val, ctx->ctx_used_ibrs[0], is_loaded, can_access_pmu));\r\n} else {\r\nCTX_USED_DBR(ctx, rnum);\r\nif (can_access_pmu) {\r\nia64_set_dbr(rnum, dbreg.val);\r\nia64_dv_serialize_data();\r\n}\r\nctx->ctx_dbrs[rnum] = dbreg.val;\r\nDPRINT(("write dbr%u=0x%lx used_dbrs=0x%x ld=%d apmu=%d\n",\r\nrnum, dbreg.val, ctx->ctx_used_dbrs[0], is_loaded, can_access_pmu));\r\n}\r\n}\r\nreturn 0;\r\nabort_mission:\r\nif (first_time) {\r\nLOCK_PFS(flags);\r\nif (ctx->ctx_fl_system) {\r\npfm_sessions.pfs_sys_use_dbregs--;\r\n}\r\nUNLOCK_PFS(flags);\r\nctx->ctx_fl_using_dbreg = 0;\r\n}\r\nPFM_REG_RETFLAG_SET(req->dbreg_flags, PFM_REG_RETFL_EINVAL);\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_write_ibrs(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nreturn pfm_write_ibr_dbr(PFM_CODE_RR, ctx, arg, count, regs);\r\n}\r\nstatic int\r\npfm_write_dbrs(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nreturn pfm_write_ibr_dbr(PFM_DATA_RR, ctx, arg, count, regs);\r\n}\r\nint\r\npfm_mod_write_ibrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)\r\n{\r\npfm_context_t *ctx;\r\nif (req == NULL) return -EINVAL;\r\nctx = GET_PMU_CTX();\r\nif (ctx == NULL) return -EINVAL;\r\nif (task != current && ctx->ctx_fl_system == 0) return -EBUSY;\r\nreturn pfm_write_ibrs(ctx, req, nreq, regs);\r\n}\r\nint\r\npfm_mod_write_dbrs(struct task_struct *task, void *req, unsigned int nreq, struct pt_regs *regs)\r\n{\r\npfm_context_t *ctx;\r\nif (req == NULL) return -EINVAL;\r\nctx = GET_PMU_CTX();\r\nif (ctx == NULL) return -EINVAL;\r\nif (task != current && ctx->ctx_fl_system == 0) return -EBUSY;\r\nreturn pfm_write_dbrs(ctx, req, nreq, regs);\r\n}\r\nstatic int\r\npfm_get_features(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\npfarg_features_t *req = (pfarg_features_t *)arg;\r\nreq->ft_version = PFM_VERSION;\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_stop(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nstruct pt_regs *tregs;\r\nstruct task_struct *task = PFM_CTX_TASK(ctx);\r\nint state, is_system;\r\nstate = ctx->ctx_state;\r\nis_system = ctx->ctx_fl_system;\r\nif (state == PFM_CTX_UNLOADED) return -EINVAL;\r\nif (is_system && ctx->ctx_cpu != smp_processor_id()) {\r\nDPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));\r\nreturn -EBUSY;\r\n}\r\nDPRINT(("task [%d] ctx_state=%d is_system=%d\n",\r\ntask_pid_nr(PFM_CTX_TASK(ctx)),\r\nstate,\r\nis_system));\r\nif (is_system) {\r\nia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) & ~IA64_DCR_PP);\r\nia64_srlz_i();\r\nPFM_CPUINFO_CLEAR(PFM_CPUINFO_DCR_PP);\r\npfm_clear_psr_pp();\r\nia64_psr(regs)->pp = 0;\r\nreturn 0;\r\n}\r\nif (task == current) {\r\npfm_clear_psr_up();\r\nia64_psr(regs)->up = 0;\r\n} else {\r\ntregs = task_pt_regs(task);\r\nia64_psr(tregs)->up = 0;\r\nctx->ctx_saved_psr_up = 0;\r\nDPRINT(("task=[%d]\n", task_pid_nr(task)));\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_start(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nstruct pt_regs *tregs;\r\nint state, is_system;\r\nstate = ctx->ctx_state;\r\nis_system = ctx->ctx_fl_system;\r\nif (state != PFM_CTX_LOADED) return -EINVAL;\r\nif (is_system && ctx->ctx_cpu != smp_processor_id()) {\r\nDPRINT(("should be running on CPU%d\n", ctx->ctx_cpu));\r\nreturn -EBUSY;\r\n}\r\nif (is_system) {\r\nia64_psr(regs)->pp = 1;\r\nPFM_CPUINFO_SET(PFM_CPUINFO_DCR_PP);\r\npfm_set_psr_pp();\r\nia64_setreg(_IA64_REG_CR_DCR, ia64_getreg(_IA64_REG_CR_DCR) | IA64_DCR_PP);\r\nia64_srlz_i();\r\nreturn 0;\r\n}\r\nif (ctx->ctx_task == current) {\r\npfm_set_psr_up();\r\nia64_psr(regs)->up = 1;\r\n} else {\r\ntregs = task_pt_regs(ctx->ctx_task);\r\nctx->ctx_saved_psr_up = IA64_PSR_UP;\r\nia64_psr(tregs)->up = 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_get_pmc_reset(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\npfarg_reg_t *req = (pfarg_reg_t *)arg;\r\nunsigned int cnum;\r\nint i;\r\nint ret = -EINVAL;\r\nfor (i = 0; i < count; i++, req++) {\r\ncnum = req->reg_num;\r\nif (!PMC_IS_IMPL(cnum)) goto abort_mission;\r\nreq->reg_value = PMC_DFL_VAL(cnum);\r\nPFM_REG_RETFLAG_SET(req->reg_flags, 0);\r\nDPRINT(("pmc_reset_val pmc[%u]=0x%lx\n", cnum, req->reg_value));\r\n}\r\nreturn 0;\r\nabort_mission:\r\nPFM_REG_RETFLAG_SET(req->reg_flags, PFM_REG_RETFL_EINVAL);\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_check_task_exist(pfm_context_t *ctx)\r\n{\r\nstruct task_struct *g, *t;\r\nint ret = -ESRCH;\r\nread_lock(&tasklist_lock);\r\ndo_each_thread (g, t) {\r\nif (t->thread.pfm_context == ctx) {\r\nret = 0;\r\ngoto out;\r\n}\r\n} while_each_thread (g, t);\r\nout:\r\nread_unlock(&tasklist_lock);\r\nDPRINT(("pfm_check_task_exist: ret=%d ctx=%p\n", ret, ctx));\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_context_load(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nstruct task_struct *task;\r\nstruct thread_struct *thread;\r\nstruct pfm_context_t *old;\r\nunsigned long flags;\r\n#ifndef CONFIG_SMP\r\nstruct task_struct *owner_task = NULL;\r\n#endif\r\npfarg_load_t *req = (pfarg_load_t *)arg;\r\nunsigned long *pmcs_source, *pmds_source;\r\nint the_cpu;\r\nint ret = 0;\r\nint state, is_system, set_dbregs = 0;\r\nstate = ctx->ctx_state;\r\nis_system = ctx->ctx_fl_system;\r\nif (state != PFM_CTX_UNLOADED) {\r\nDPRINT(("cannot load to [%d], invalid ctx_state=%d\n",\r\nreq->load_pid,\r\nctx->ctx_state));\r\nreturn -EBUSY;\r\n}\r\nDPRINT(("load_pid [%d] using_dbreg=%d\n", req->load_pid, ctx->ctx_fl_using_dbreg));\r\nif (CTX_OVFL_NOBLOCK(ctx) == 0 && req->load_pid == current->pid) {\r\nDPRINT(("cannot use blocking mode on self\n"));\r\nreturn -EINVAL;\r\n}\r\nret = pfm_get_task(ctx, req->load_pid, &task);\r\nif (ret) {\r\nDPRINT(("load_pid [%d] get_task=%d\n", req->load_pid, ret));\r\nreturn ret;\r\n}\r\nret = -EINVAL;\r\nif (is_system && task != current) {\r\nDPRINT(("system wide is self monitoring only load_pid=%d\n",\r\nreq->load_pid));\r\ngoto error;\r\n}\r\nthread = &task->thread;\r\nret = 0;\r\nif (ctx->ctx_fl_using_dbreg) {\r\nif (thread->flags & IA64_THREAD_DBG_VALID) {\r\nret = -EBUSY;\r\nDPRINT(("load_pid [%d] task is debugged, cannot load range restrictions\n", req->load_pid));\r\ngoto error;\r\n}\r\nLOCK_PFS(flags);\r\nif (is_system) {\r\nif (pfm_sessions.pfs_ptrace_use_dbregs) {\r\nDPRINT(("cannot load [%d] dbregs in use\n",\r\ntask_pid_nr(task)));\r\nret = -EBUSY;\r\n} else {\r\npfm_sessions.pfs_sys_use_dbregs++;\r\nDPRINT(("load [%d] increased sys_use_dbreg=%u\n", task_pid_nr(task), pfm_sessions.pfs_sys_use_dbregs));\r\nset_dbregs = 1;\r\n}\r\n}\r\nUNLOCK_PFS(flags);\r\nif (ret) goto error;\r\n}\r\nthe_cpu = ctx->ctx_cpu = smp_processor_id();\r\nret = -EBUSY;\r\nret = pfm_reserve_session(current, is_system, the_cpu);\r\nif (ret) goto error;\r\nDPRINT(("before cmpxchg() old_ctx=%p new_ctx=%p\n",\r\nthread->pfm_context, ctx));\r\nret = -EBUSY;\r\nold = ia64_cmpxchg(acq, &thread->pfm_context, NULL, ctx, sizeof(pfm_context_t *));\r\nif (old != NULL) {\r\nDPRINT(("load_pid [%d] already has a context\n", req->load_pid));\r\ngoto error_unres;\r\n}\r\npfm_reset_msgq(ctx);\r\nctx->ctx_state = PFM_CTX_LOADED;\r\nctx->ctx_task = task;\r\nif (is_system) {\r\nPFM_CPUINFO_SET(PFM_CPUINFO_SYST_WIDE);\r\nPFM_CPUINFO_CLEAR(PFM_CPUINFO_DCR_PP);\r\nif (ctx->ctx_fl_excl_idle) PFM_CPUINFO_SET(PFM_CPUINFO_EXCL_IDLE);\r\n} else {\r\nthread->flags |= IA64_THREAD_PM_VALID;\r\n}\r\npfm_copy_pmds(task, ctx);\r\npfm_copy_pmcs(task, ctx);\r\npmcs_source = ctx->th_pmcs;\r\npmds_source = ctx->th_pmds;\r\nif (task == current) {\r\nif (is_system == 0) {\r\nia64_psr(regs)->sp = 0;\r\nDPRINT(("clearing psr.sp for [%d]\n", task_pid_nr(task)));\r\nSET_LAST_CPU(ctx, smp_processor_id());\r\nINC_ACTIVATION();\r\nSET_ACTIVATION(ctx);\r\n#ifndef CONFIG_SMP\r\nowner_task = GET_PMU_OWNER();\r\nif (owner_task) pfm_lazy_save_regs(owner_task);\r\n#endif\r\n}\r\npfm_restore_pmds(pmds_source, ctx->ctx_all_pmds[0]);\r\npfm_restore_pmcs(pmcs_source, ctx->ctx_all_pmcs[0]);\r\nctx->ctx_reload_pmcs[0] = 0UL;\r\nctx->ctx_reload_pmds[0] = 0UL;\r\nif (ctx->ctx_fl_using_dbreg) {\r\npfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);\r\npfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);\r\n}\r\nSET_PMU_OWNER(task, ctx);\r\nDPRINT(("context loaded on PMU for [%d]\n", task_pid_nr(task)));\r\n} else {\r\nregs = task_pt_regs(task);\r\nctx->ctx_last_activation = PFM_INVALID_ACTIVATION;\r\nSET_LAST_CPU(ctx, -1);\r\nctx->ctx_saved_psr_up = 0UL;\r\nia64_psr(regs)->up = ia64_psr(regs)->pp = 0;\r\n}\r\nret = 0;\r\nerror_unres:\r\nif (ret) pfm_unreserve_session(ctx, ctx->ctx_fl_system, the_cpu);\r\nerror:\r\nif (ret && set_dbregs) {\r\nLOCK_PFS(flags);\r\npfm_sessions.pfs_sys_use_dbregs--;\r\nUNLOCK_PFS(flags);\r\n}\r\nif (is_system == 0 && task != current) {\r\npfm_put_task(task);\r\nif (ret == 0) {\r\nret = pfm_check_task_exist(ctx);\r\nif (ret) {\r\nctx->ctx_state = PFM_CTX_UNLOADED;\r\nctx->ctx_task = NULL;\r\n}\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\npfm_context_unload(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs)\r\n{\r\nstruct task_struct *task = PFM_CTX_TASK(ctx);\r\nstruct pt_regs *tregs;\r\nint prev_state, is_system;\r\nint ret;\r\nDPRINT(("ctx_state=%d task [%d]\n", ctx->ctx_state, task ? task_pid_nr(task) : -1));\r\nprev_state = ctx->ctx_state;\r\nis_system = ctx->ctx_fl_system;\r\nif (prev_state == PFM_CTX_UNLOADED) {\r\nDPRINT(("ctx_state=%d, nothing to do\n", prev_state));\r\nreturn 0;\r\n}\r\nret = pfm_stop(ctx, NULL, 0, regs);\r\nif (ret) return ret;\r\nctx->ctx_state = PFM_CTX_UNLOADED;\r\nif (is_system) {\r\nPFM_CPUINFO_CLEAR(PFM_CPUINFO_SYST_WIDE);\r\nPFM_CPUINFO_CLEAR(PFM_CPUINFO_EXCL_IDLE);\r\npfm_flush_pmds(current, ctx);\r\nif (prev_state != PFM_CTX_ZOMBIE)\r\npfm_unreserve_session(ctx, 1 , ctx->ctx_cpu);\r\ntask->thread.pfm_context = NULL;\r\nctx->ctx_task = NULL;\r\nreturn 0;\r\n}\r\ntregs = task == current ? regs : task_pt_regs(task);\r\nif (task == current) {\r\nia64_psr(regs)->sp = 1;\r\nDPRINT(("setting psr.sp for [%d]\n", task_pid_nr(task)));\r\n}\r\npfm_flush_pmds(task, ctx);\r\nif (prev_state != PFM_CTX_ZOMBIE)\r\npfm_unreserve_session(ctx, 0 , ctx->ctx_cpu);\r\nctx->ctx_last_activation = PFM_INVALID_ACTIVATION;\r\nSET_LAST_CPU(ctx, -1);\r\ntask->thread.flags &= ~IA64_THREAD_PM_VALID;\r\ntask->thread.pfm_context = NULL;\r\nctx->ctx_task = NULL;\r\nPFM_SET_WORK_PENDING(task, 0);\r\nctx->ctx_fl_trap_reason = PFM_TRAP_REASON_NONE;\r\nctx->ctx_fl_can_restart = 0;\r\nctx->ctx_fl_going_zombie = 0;\r\nDPRINT(("disconnected [%d] from context\n", task_pid_nr(task)));\r\nreturn 0;\r\n}\r\nvoid\r\npfm_exit_thread(struct task_struct *task)\r\n{\r\npfm_context_t *ctx;\r\nunsigned long flags;\r\nstruct pt_regs *regs = task_pt_regs(task);\r\nint ret, state;\r\nint free_ok = 0;\r\nctx = PFM_GET_CTX(task);\r\nPROTECT_CTX(ctx, flags);\r\nDPRINT(("state=%d task [%d]\n", ctx->ctx_state, task_pid_nr(task)));\r\nstate = ctx->ctx_state;\r\nswitch(state) {\r\ncase PFM_CTX_UNLOADED:\r\nprintk(KERN_ERR "perfmon: pfm_exit_thread [%d] ctx unloaded\n", task_pid_nr(task));\r\nbreak;\r\ncase PFM_CTX_LOADED:\r\ncase PFM_CTX_MASKED:\r\nret = pfm_context_unload(ctx, NULL, 0, regs);\r\nif (ret) {\r\nprintk(KERN_ERR "perfmon: pfm_exit_thread [%d] state=%d unload failed %d\n", task_pid_nr(task), state, ret);\r\n}\r\nDPRINT(("ctx unloaded for current state was %d\n", state));\r\npfm_end_notify_user(ctx);\r\nbreak;\r\ncase PFM_CTX_ZOMBIE:\r\nret = pfm_context_unload(ctx, NULL, 0, regs);\r\nif (ret) {\r\nprintk(KERN_ERR "perfmon: pfm_exit_thread [%d] state=%d unload failed %d\n", task_pid_nr(task), state, ret);\r\n}\r\nfree_ok = 1;\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "perfmon: pfm_exit_thread [%d] unexpected state=%d\n", task_pid_nr(task), state);\r\nbreak;\r\n}\r\nUNPROTECT_CTX(ctx, flags);\r\n{ u64 psr = pfm_get_psr();\r\nBUG_ON(psr & (IA64_PSR_UP|IA64_PSR_PP));\r\nBUG_ON(GET_PMU_OWNER());\r\nBUG_ON(ia64_psr(regs)->up);\r\nBUG_ON(ia64_psr(regs)->pp);\r\n}\r\nif (free_ok) pfm_context_free(ctx);\r\n}\r\nstatic int\r\npfm_check_task_state(pfm_context_t *ctx, int cmd, unsigned long flags)\r\n{\r\nstruct task_struct *task;\r\nint state, old_state;\r\nrecheck:\r\nstate = ctx->ctx_state;\r\ntask = ctx->ctx_task;\r\nif (task == NULL) {\r\nDPRINT(("context %d no task, state=%d\n", ctx->ctx_fd, state));\r\nreturn 0;\r\n}\r\nDPRINT(("context %d state=%d [%d] task_state=%ld must_stop=%d\n",\r\nctx->ctx_fd,\r\nstate,\r\ntask_pid_nr(task),\r\ntask->state, PFM_CMD_STOPPED(cmd)));\r\nif (task == current || ctx->ctx_fl_system) return 0;\r\nswitch(state) {\r\ncase PFM_CTX_UNLOADED:\r\nreturn 0;\r\ncase PFM_CTX_ZOMBIE:\r\nDPRINT(("cmd %d state zombie cannot operate on context\n", cmd));\r\nreturn -EINVAL;\r\ncase PFM_CTX_MASKED:\r\nif (cmd != PFM_UNLOAD_CONTEXT) return 0;\r\n}\r\nif (PFM_CMD_STOPPED(cmd)) {\r\nif (!task_is_stopped_or_traced(task)) {\r\nDPRINT(("[%d] task not in stopped state\n", task_pid_nr(task)));\r\nreturn -EBUSY;\r\n}\r\nold_state = state;\r\nUNPROTECT_CTX(ctx, flags);\r\nwait_task_inactive(task, 0);\r\nPROTECT_CTX(ctx, flags);\r\nif (ctx->ctx_state != old_state) {\r\nDPRINT(("old_state=%d new_state=%d\n", old_state, ctx->ctx_state));\r\ngoto recheck;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nasmlinkage long\r\nsys_perfmonctl (int fd, int cmd, void __user *arg, int count)\r\n{\r\nstruct fd f = {NULL, 0};\r\npfm_context_t *ctx = NULL;\r\nunsigned long flags = 0UL;\r\nvoid *args_k = NULL;\r\nlong ret;\r\nsize_t base_sz, sz, xtra_sz = 0;\r\nint narg, completed_args = 0, call_made = 0, cmd_flags;\r\nint (*func)(pfm_context_t *ctx, void *arg, int count, struct pt_regs *regs);\r\nint (*getsize)(void *arg, size_t *sz);\r\n#define PFM_MAX_ARGSIZE 4096\r\nif (unlikely(pmu_conf == NULL)) return -ENOSYS;\r\nif (unlikely(cmd < 0 || cmd >= PFM_CMD_COUNT)) {\r\nDPRINT(("invalid cmd=%d\n", cmd));\r\nreturn -EINVAL;\r\n}\r\nfunc = pfm_cmd_tab[cmd].cmd_func;\r\nnarg = pfm_cmd_tab[cmd].cmd_narg;\r\nbase_sz = pfm_cmd_tab[cmd].cmd_argsize;\r\ngetsize = pfm_cmd_tab[cmd].cmd_getsize;\r\ncmd_flags = pfm_cmd_tab[cmd].cmd_flags;\r\nif (unlikely(func == NULL)) {\r\nDPRINT(("invalid cmd=%d\n", cmd));\r\nreturn -EINVAL;\r\n}\r\nDPRINT(("cmd=%s idx=%d narg=0x%x argsz=%lu count=%d\n",\r\nPFM_CMD_NAME(cmd),\r\ncmd,\r\nnarg,\r\nbase_sz,\r\ncount));\r\nif (unlikely((narg == PFM_CMD_ARG_MANY && count <= 0) || (narg > 0 && narg != count)))\r\nreturn -EINVAL;\r\nrestart_args:\r\nsz = xtra_sz + base_sz*count;\r\nif (unlikely(sz > PFM_MAX_ARGSIZE)) {\r\nprintk(KERN_ERR "perfmon: [%d] argument too big %lu\n", task_pid_nr(current), sz);\r\nreturn -E2BIG;\r\n}\r\nif (likely(count && args_k == NULL)) {\r\nargs_k = kmalloc(PFM_MAX_ARGSIZE, GFP_KERNEL);\r\nif (args_k == NULL) return -ENOMEM;\r\n}\r\nret = -EFAULT;\r\nif (sz && copy_from_user(args_k, arg, sz)) {\r\nDPRINT(("cannot copy_from_user %lu bytes @%p\n", sz, arg));\r\ngoto error_args;\r\n}\r\nif (completed_args == 0 && getsize) {\r\nret = (*getsize)(args_k, &xtra_sz);\r\nif (ret) goto error_args;\r\ncompleted_args = 1;\r\nDPRINT(("restart_args sz=%lu xtra_sz=%lu\n", sz, xtra_sz));\r\nif (likely(xtra_sz)) goto restart_args;\r\n}\r\nif (unlikely((cmd_flags & PFM_CMD_FD) == 0)) goto skip_fd;\r\nret = -EBADF;\r\nf = fdget(fd);\r\nif (unlikely(f.file == NULL)) {\r\nDPRINT(("invalid fd %d\n", fd));\r\ngoto error_args;\r\n}\r\nif (unlikely(PFM_IS_FILE(f.file) == 0)) {\r\nDPRINT(("fd %d not related to perfmon\n", fd));\r\ngoto error_args;\r\n}\r\nctx = f.file->private_data;\r\nif (unlikely(ctx == NULL)) {\r\nDPRINT(("no context for fd %d\n", fd));\r\ngoto error_args;\r\n}\r\nprefetch(&ctx->ctx_state);\r\nPROTECT_CTX(ctx, flags);\r\nret = pfm_check_task_state(ctx, cmd, flags);\r\nif (unlikely(ret)) goto abort_locked;\r\nskip_fd:\r\nret = (*func)(ctx, args_k, count, task_pt_regs(current));\r\ncall_made = 1;\r\nabort_locked:\r\nif (likely(ctx)) {\r\nDPRINT(("context unlocked\n"));\r\nUNPROTECT_CTX(ctx, flags);\r\n}\r\nif (call_made && PFM_CMD_RW_ARG(cmd) && copy_to_user(arg, args_k, base_sz*count)) ret = -EFAULT;\r\nerror_args:\r\nif (f.file)\r\nfdput(f);\r\nkfree(args_k);\r\nDPRINT(("cmd=%s ret=%ld\n", PFM_CMD_NAME(cmd), ret));\r\nreturn ret;\r\n}\r\nstatic void\r\npfm_resume_after_ovfl(pfm_context_t *ctx, unsigned long ovfl_regs, struct pt_regs *regs)\r\n{\r\npfm_buffer_fmt_t *fmt = ctx->ctx_buf_fmt;\r\npfm_ovfl_ctrl_t rst_ctrl;\r\nint state;\r\nint ret = 0;\r\nstate = ctx->ctx_state;\r\nif (CTX_HAS_SMPL(ctx)) {\r\nrst_ctrl.bits.mask_monitoring = 0;\r\nrst_ctrl.bits.reset_ovfl_pmds = 0;\r\nif (state == PFM_CTX_LOADED)\r\nret = pfm_buf_fmt_restart_active(fmt, current, &rst_ctrl, ctx->ctx_smpl_hdr, regs);\r\nelse\r\nret = pfm_buf_fmt_restart(fmt, current, &rst_ctrl, ctx->ctx_smpl_hdr, regs);\r\n} else {\r\nrst_ctrl.bits.mask_monitoring = 0;\r\nrst_ctrl.bits.reset_ovfl_pmds = 1;\r\n}\r\nif (ret == 0) {\r\nif (rst_ctrl.bits.reset_ovfl_pmds) {\r\npfm_reset_regs(ctx, &ovfl_regs, PFM_PMD_LONG_RESET);\r\n}\r\nif (rst_ctrl.bits.mask_monitoring == 0) {\r\nDPRINT(("resuming monitoring\n"));\r\nif (ctx->ctx_state == PFM_CTX_MASKED) pfm_restore_monitoring(current);\r\n} else {\r\nDPRINT(("stopping monitoring\n"));\r\n}\r\nctx->ctx_state = PFM_CTX_LOADED;\r\n}\r\n}\r\nstatic void\r\npfm_context_force_terminate(pfm_context_t *ctx, struct pt_regs *regs)\r\n{\r\nint ret;\r\nDPRINT(("entering for [%d]\n", task_pid_nr(current)));\r\nret = pfm_context_unload(ctx, NULL, 0, regs);\r\nif (ret) {\r\nprintk(KERN_ERR "pfm_context_force_terminate: [%d] unloaded failed with %d\n", task_pid_nr(current), ret);\r\n}\r\nwake_up_interruptible(&ctx->ctx_zombieq);\r\n}\r\nvoid\r\npfm_handle_work(void)\r\n{\r\npfm_context_t *ctx;\r\nstruct pt_regs *regs;\r\nunsigned long flags, dummy_flags;\r\nunsigned long ovfl_regs;\r\nunsigned int reason;\r\nint ret;\r\nctx = PFM_GET_CTX(current);\r\nif (ctx == NULL) {\r\nprintk(KERN_ERR "perfmon: [%d] has no PFM context\n",\r\ntask_pid_nr(current));\r\nreturn;\r\n}\r\nPROTECT_CTX(ctx, flags);\r\nPFM_SET_WORK_PENDING(current, 0);\r\nregs = task_pt_regs(current);\r\nreason = ctx->ctx_fl_trap_reason;\r\nctx->ctx_fl_trap_reason = PFM_TRAP_REASON_NONE;\r\novfl_regs = ctx->ctx_ovfl_regs[0];\r\nDPRINT(("reason=%d state=%d\n", reason, ctx->ctx_state));\r\nif (ctx->ctx_fl_going_zombie || ctx->ctx_state == PFM_CTX_ZOMBIE)\r\ngoto do_zombie;\r\nif (reason == PFM_TRAP_REASON_RESET)\r\ngoto skip_blocking;\r\nUNPROTECT_CTX(ctx, flags);\r\nlocal_irq_enable();\r\nDPRINT(("before block sleeping\n"));\r\nret = wait_for_completion_interruptible(&ctx->ctx_restart_done);\r\nDPRINT(("after block sleeping ret=%d\n", ret));\r\nPROTECT_CTX(ctx, dummy_flags);\r\novfl_regs = ctx->ctx_ovfl_regs[0];\r\nif (ctx->ctx_fl_going_zombie) {\r\ndo_zombie:\r\nDPRINT(("context is zombie, bailing out\n"));\r\npfm_context_force_terminate(ctx, regs);\r\ngoto nothing_to_do;\r\n}\r\nif (ret < 0)\r\ngoto nothing_to_do;\r\nskip_blocking:\r\npfm_resume_after_ovfl(ctx, ovfl_regs, regs);\r\nctx->ctx_ovfl_regs[0] = 0UL;\r\nnothing_to_do:\r\nUNPROTECT_CTX(ctx, flags);\r\n}\r\nstatic int\r\npfm_notify_user(pfm_context_t *ctx, pfm_msg_t *msg)\r\n{\r\nif (ctx->ctx_state == PFM_CTX_ZOMBIE) {\r\nDPRINT(("ignoring overflow notification, owner is zombie\n"));\r\nreturn 0;\r\n}\r\nDPRINT(("waking up somebody\n"));\r\nif (msg) wake_up_interruptible(&ctx->ctx_msgq_wait);\r\nkill_fasync (&ctx->ctx_async_queue, SIGIO, POLL_IN);\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_ovfl_notify_user(pfm_context_t *ctx, unsigned long ovfl_pmds)\r\n{\r\npfm_msg_t *msg = NULL;\r\nif (ctx->ctx_fl_no_msg == 0) {\r\nmsg = pfm_get_new_msg(ctx);\r\nif (msg == NULL) {\r\nprintk(KERN_ERR "perfmon: pfm_ovfl_notify_user no more notification msgs\n");\r\nreturn -1;\r\n}\r\nmsg->pfm_ovfl_msg.msg_type = PFM_MSG_OVFL;\r\nmsg->pfm_ovfl_msg.msg_ctx_fd = ctx->ctx_fd;\r\nmsg->pfm_ovfl_msg.msg_active_set = 0;\r\nmsg->pfm_ovfl_msg.msg_ovfl_pmds[0] = ovfl_pmds;\r\nmsg->pfm_ovfl_msg.msg_ovfl_pmds[1] = 0UL;\r\nmsg->pfm_ovfl_msg.msg_ovfl_pmds[2] = 0UL;\r\nmsg->pfm_ovfl_msg.msg_ovfl_pmds[3] = 0UL;\r\nmsg->pfm_ovfl_msg.msg_tstamp = 0UL;\r\n}\r\nDPRINT(("ovfl msg: msg=%p no_msg=%d fd=%d ovfl_pmds=0x%lx\n",\r\nmsg,\r\nctx->ctx_fl_no_msg,\r\nctx->ctx_fd,\r\novfl_pmds));\r\nreturn pfm_notify_user(ctx, msg);\r\n}\r\nstatic int\r\npfm_end_notify_user(pfm_context_t *ctx)\r\n{\r\npfm_msg_t *msg;\r\nmsg = pfm_get_new_msg(ctx);\r\nif (msg == NULL) {\r\nprintk(KERN_ERR "perfmon: pfm_end_notify_user no more notification msgs\n");\r\nreturn -1;\r\n}\r\nmemset(msg, 0, sizeof(*msg));\r\nmsg->pfm_end_msg.msg_type = PFM_MSG_END;\r\nmsg->pfm_end_msg.msg_ctx_fd = ctx->ctx_fd;\r\nmsg->pfm_ovfl_msg.msg_tstamp = 0UL;\r\nDPRINT(("end msg: msg=%p no_msg=%d ctx_fd=%d\n",\r\nmsg,\r\nctx->ctx_fl_no_msg,\r\nctx->ctx_fd));\r\nreturn pfm_notify_user(ctx, msg);\r\n}\r\nstatic void pfm_overflow_handler(struct task_struct *task, pfm_context_t *ctx,\r\nunsigned long pmc0, struct pt_regs *regs)\r\n{\r\npfm_ovfl_arg_t *ovfl_arg;\r\nunsigned long mask;\r\nunsigned long old_val, ovfl_val, new_val;\r\nunsigned long ovfl_notify = 0UL, ovfl_pmds = 0UL, smpl_pmds = 0UL, reset_pmds;\r\nunsigned long tstamp;\r\npfm_ovfl_ctrl_t ovfl_ctrl;\r\nunsigned int i, has_smpl;\r\nint must_notify = 0;\r\nif (unlikely(ctx->ctx_state == PFM_CTX_ZOMBIE)) goto stop_monitoring;\r\nif (unlikely((pmc0 & 0x1) == 0)) goto sanity_check;\r\ntstamp = ia64_get_itc();\r\nmask = pmc0 >> PMU_FIRST_COUNTER;\r\novfl_val = pmu_conf->ovfl_val;\r\nhas_smpl = CTX_HAS_SMPL(ctx);\r\nDPRINT_ovfl(("pmc0=0x%lx pid=%d iip=0x%lx, %s "\r\n"used_pmds=0x%lx\n",\r\npmc0,\r\ntask ? task_pid_nr(task): -1,\r\n(regs ? regs->cr_iip : 0),\r\nCTX_OVFL_NOBLOCK(ctx) ? "nonblocking" : "blocking",\r\nctx->ctx_used_pmds[0]));\r\nfor (i = PMU_FIRST_COUNTER; mask ; i++, mask >>= 1) {\r\nif ((mask & 0x1) == 0) continue;\r\nold_val = new_val = ctx->ctx_pmds[i].val;\r\nnew_val += 1 + ovfl_val;\r\nctx->ctx_pmds[i].val = new_val;\r\nif (likely(old_val > new_val)) {\r\novfl_pmds |= 1UL << i;\r\nif (PMC_OVFL_NOTIFY(ctx, i)) ovfl_notify |= 1UL << i;\r\n}\r\nDPRINT_ovfl(("ctx_pmd[%d].val=0x%lx old_val=0x%lx pmd=0x%lx ovfl_pmds=0x%lx ovfl_notify=0x%lx\n",\r\ni,\r\nnew_val,\r\nold_val,\r\nia64_get_pmd(i) & ovfl_val,\r\novfl_pmds,\r\novfl_notify));\r\n}\r\nif (ovfl_pmds == 0UL) return;\r\novfl_ctrl.val = 0;\r\nreset_pmds = 0UL;\r\nif (has_smpl) {\r\nunsigned long start_cycles, end_cycles;\r\nunsigned long pmd_mask;\r\nint j, k, ret = 0;\r\nint this_cpu = smp_processor_id();\r\npmd_mask = ovfl_pmds >> PMU_FIRST_COUNTER;\r\novfl_arg = &ctx->ctx_ovfl_arg;\r\nprefetch(ctx->ctx_smpl_hdr);\r\nfor(i=PMU_FIRST_COUNTER; pmd_mask && ret == 0; i++, pmd_mask >>=1) {\r\nmask = 1UL << i;\r\nif ((pmd_mask & 0x1) == 0) continue;\r\novfl_arg->ovfl_pmd = (unsigned char )i;\r\novfl_arg->ovfl_notify = ovfl_notify & mask ? 1 : 0;\r\novfl_arg->active_set = 0;\r\novfl_arg->ovfl_ctrl.val = 0;\r\novfl_arg->smpl_pmds[0] = smpl_pmds = ctx->ctx_pmds[i].smpl_pmds[0];\r\novfl_arg->pmd_value = ctx->ctx_pmds[i].val;\r\novfl_arg->pmd_last_reset = ctx->ctx_pmds[i].lval;\r\novfl_arg->pmd_eventid = ctx->ctx_pmds[i].eventid;\r\nif (smpl_pmds) {\r\nfor(j=0, k=0; smpl_pmds; j++, smpl_pmds >>=1) {\r\nif ((smpl_pmds & 0x1) == 0) continue;\r\novfl_arg->smpl_pmds_values[k++] = PMD_IS_COUNTING(j) ? pfm_read_soft_counter(ctx, j) : ia64_get_pmd(j);\r\nDPRINT_ovfl(("smpl_pmd[%d]=pmd%u=0x%lx\n", k-1, j, ovfl_arg->smpl_pmds_values[k-1]));\r\n}\r\n}\r\npfm_stats[this_cpu].pfm_smpl_handler_calls++;\r\nstart_cycles = ia64_get_itc();\r\nret = (*ctx->ctx_buf_fmt->fmt_handler)(task, ctx->ctx_smpl_hdr, ovfl_arg, regs, tstamp);\r\nend_cycles = ia64_get_itc();\r\novfl_ctrl.bits.notify_user |= ovfl_arg->ovfl_ctrl.bits.notify_user;\r\novfl_ctrl.bits.block_task |= ovfl_arg->ovfl_ctrl.bits.block_task;\r\novfl_ctrl.bits.mask_monitoring |= ovfl_arg->ovfl_ctrl.bits.mask_monitoring;\r\nif (ovfl_arg->ovfl_ctrl.bits.reset_ovfl_pmds) reset_pmds |= mask;\r\npfm_stats[this_cpu].pfm_smpl_handler_cycles += end_cycles - start_cycles;\r\n}\r\nif (ret && pmd_mask) {\r\nDPRINT(("handler aborts leftover ovfl_pmds=0x%lx\n",\r\npmd_mask<<PMU_FIRST_COUNTER));\r\n}\r\novfl_pmds &= ~reset_pmds;\r\n} else {\r\novfl_ctrl.bits.notify_user = ovfl_notify ? 1 : 0;\r\novfl_ctrl.bits.block_task = ovfl_notify ? 1 : 0;\r\novfl_ctrl.bits.mask_monitoring = ovfl_notify ? 1 : 0;\r\novfl_ctrl.bits.reset_ovfl_pmds = ovfl_notify ? 0 : 1;\r\nif (ovfl_notify == 0) reset_pmds = ovfl_pmds;\r\n}\r\nDPRINT_ovfl(("ovfl_pmds=0x%lx reset_pmds=0x%lx\n", ovfl_pmds, reset_pmds));\r\nif (reset_pmds) {\r\nunsigned long bm = reset_pmds;\r\npfm_reset_regs(ctx, &bm, PFM_PMD_SHORT_RESET);\r\n}\r\nif (ovfl_notify && ovfl_ctrl.bits.notify_user) {\r\nctx->ctx_ovfl_regs[0] = ovfl_pmds;\r\nif (CTX_OVFL_NOBLOCK(ctx) == 0 && ovfl_ctrl.bits.block_task) {\r\nctx->ctx_fl_trap_reason = PFM_TRAP_REASON_BLOCK;\r\nPFM_SET_WORK_PENDING(task, 1);\r\nset_notify_resume(task);\r\n}\r\nmust_notify = 1;\r\n}\r\nDPRINT_ovfl(("owner [%d] pending=%ld reason=%u ovfl_pmds=0x%lx ovfl_notify=0x%lx masked=%d\n",\r\nGET_PMU_OWNER() ? task_pid_nr(GET_PMU_OWNER()) : -1,\r\nPFM_GET_WORK_PENDING(task),\r\nctx->ctx_fl_trap_reason,\r\novfl_pmds,\r\novfl_notify,\r\novfl_ctrl.bits.mask_monitoring ? 1 : 0));\r\nif (ovfl_ctrl.bits.mask_monitoring) {\r\npfm_mask_monitoring(task);\r\nctx->ctx_state = PFM_CTX_MASKED;\r\nctx->ctx_fl_can_restart = 1;\r\n}\r\nif (must_notify) pfm_ovfl_notify_user(ctx, ovfl_notify);\r\nreturn;\r\nsanity_check:\r\nprintk(KERN_ERR "perfmon: CPU%d overflow handler [%d] pmc0=0x%lx\n",\r\nsmp_processor_id(),\r\ntask ? task_pid_nr(task) : -1,\r\npmc0);\r\nreturn;\r\nstop_monitoring:\r\nDPRINT(("ctx is zombie for [%d], converted to spurious\n", task ? task_pid_nr(task): -1));\r\npfm_clear_psr_up();\r\nia64_psr(regs)->up = 0;\r\nia64_psr(regs)->sp = 1;\r\nreturn;\r\n}\r\nstatic int\r\npfm_do_interrupt_handler(void *arg, struct pt_regs *regs)\r\n{\r\nstruct task_struct *task;\r\npfm_context_t *ctx;\r\nunsigned long flags;\r\nu64 pmc0;\r\nint this_cpu = smp_processor_id();\r\nint retval = 0;\r\npfm_stats[this_cpu].pfm_ovfl_intr_count++;\r\npmc0 = ia64_get_pmc(0);\r\ntask = GET_PMU_OWNER();\r\nctx = GET_PMU_CTX();\r\nif (PMC0_HAS_OVFL(pmc0) && task) {\r\nif (!ctx) goto report_spurious1;\r\nif (ctx->ctx_fl_system == 0 && (task->thread.flags & IA64_THREAD_PM_VALID) == 0)\r\ngoto report_spurious2;\r\nPROTECT_CTX_NOPRINT(ctx, flags);\r\npfm_overflow_handler(task, ctx, pmc0, regs);\r\nUNPROTECT_CTX_NOPRINT(ctx, flags);\r\n} else {\r\npfm_stats[this_cpu].pfm_spurious_ovfl_intr_count++;\r\nretval = -1;\r\n}\r\npfm_unfreeze_pmu();\r\nreturn retval;\r\nreport_spurious1:\r\nprintk(KERN_INFO "perfmon: spurious overflow interrupt on CPU%d: process %d has no PFM context\n",\r\nthis_cpu, task_pid_nr(task));\r\npfm_unfreeze_pmu();\r\nreturn -1;\r\nreport_spurious2:\r\nprintk(KERN_INFO "perfmon: spurious overflow interrupt on CPU%d: process %d, invalid flag\n",\r\nthis_cpu,\r\ntask_pid_nr(task));\r\npfm_unfreeze_pmu();\r\nreturn -1;\r\n}\r\nstatic irqreturn_t\r\npfm_interrupt_handler(int irq, void *arg)\r\n{\r\nunsigned long start_cycles, total_cycles;\r\nunsigned long min, max;\r\nint this_cpu;\r\nint ret;\r\nstruct pt_regs *regs = get_irq_regs();\r\nthis_cpu = get_cpu();\r\nif (likely(!pfm_alt_intr_handler)) {\r\nmin = pfm_stats[this_cpu].pfm_ovfl_intr_cycles_min;\r\nmax = pfm_stats[this_cpu].pfm_ovfl_intr_cycles_max;\r\nstart_cycles = ia64_get_itc();\r\nret = pfm_do_interrupt_handler(arg, regs);\r\ntotal_cycles = ia64_get_itc();\r\nif (likely(ret == 0)) {\r\ntotal_cycles -= start_cycles;\r\nif (total_cycles < min) pfm_stats[this_cpu].pfm_ovfl_intr_cycles_min = total_cycles;\r\nif (total_cycles > max) pfm_stats[this_cpu].pfm_ovfl_intr_cycles_max = total_cycles;\r\npfm_stats[this_cpu].pfm_ovfl_intr_cycles += total_cycles;\r\n}\r\n}\r\nelse {\r\n(*pfm_alt_intr_handler->handler)(irq, arg, regs);\r\n}\r\nput_cpu();\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void *\r\npfm_proc_start(struct seq_file *m, loff_t *pos)\r\n{\r\nif (*pos == 0) {\r\nreturn PFM_PROC_SHOW_HEADER;\r\n}\r\nwhile (*pos <= nr_cpu_ids) {\r\nif (cpu_online(*pos - 1)) {\r\nreturn (void *)*pos;\r\n}\r\n++*pos;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void *\r\npfm_proc_next(struct seq_file *m, void *v, loff_t *pos)\r\n{\r\n++*pos;\r\nreturn pfm_proc_start(m, pos);\r\n}\r\nstatic void\r\npfm_proc_stop(struct seq_file *m, void *v)\r\n{\r\n}\r\nstatic void\r\npfm_proc_show_header(struct seq_file *m)\r\n{\r\nstruct list_head * pos;\r\npfm_buffer_fmt_t * entry;\r\nunsigned long flags;\r\nseq_printf(m,\r\n"perfmon version : %u.%u\n"\r\n"model : %s\n"\r\n"fastctxsw : %s\n"\r\n"expert mode : %s\n"\r\n"ovfl_mask : 0x%lx\n"\r\n"PMU flags : 0x%x\n",\r\nPFM_VERSION_MAJ, PFM_VERSION_MIN,\r\npmu_conf->pmu_name,\r\npfm_sysctl.fastctxsw > 0 ? "Yes": "No",\r\npfm_sysctl.expert_mode > 0 ? "Yes": "No",\r\npmu_conf->ovfl_val,\r\npmu_conf->flags);\r\nLOCK_PFS(flags);\r\nseq_printf(m,\r\n"proc_sessions : %u\n"\r\n"sys_sessions : %u\n"\r\n"sys_use_dbregs : %u\n"\r\n"ptrace_use_dbregs : %u\n",\r\npfm_sessions.pfs_task_sessions,\r\npfm_sessions.pfs_sys_sessions,\r\npfm_sessions.pfs_sys_use_dbregs,\r\npfm_sessions.pfs_ptrace_use_dbregs);\r\nUNLOCK_PFS(flags);\r\nspin_lock(&pfm_buffer_fmt_lock);\r\nlist_for_each(pos, &pfm_buffer_fmt_list) {\r\nentry = list_entry(pos, pfm_buffer_fmt_t, fmt_list);\r\nseq_printf(m, "format : %16phD %s\n",\r\nentry->fmt_uuid, entry->fmt_name);\r\n}\r\nspin_unlock(&pfm_buffer_fmt_lock);\r\n}\r\nstatic int\r\npfm_proc_show(struct seq_file *m, void *v)\r\n{\r\nunsigned long psr;\r\nunsigned int i;\r\nint cpu;\r\nif (v == PFM_PROC_SHOW_HEADER) {\r\npfm_proc_show_header(m);\r\nreturn 0;\r\n}\r\ncpu = (long)v - 1;\r\nseq_printf(m,\r\n"CPU%-2d overflow intrs : %lu\n"\r\n"CPU%-2d overflow cycles : %lu\n"\r\n"CPU%-2d overflow min : %lu\n"\r\n"CPU%-2d overflow max : %lu\n"\r\n"CPU%-2d smpl handler calls : %lu\n"\r\n"CPU%-2d smpl handler cycles : %lu\n"\r\n"CPU%-2d spurious intrs : %lu\n"\r\n"CPU%-2d replay intrs : %lu\n"\r\n"CPU%-2d syst_wide : %d\n"\r\n"CPU%-2d dcr_pp : %d\n"\r\n"CPU%-2d exclude idle : %d\n"\r\n"CPU%-2d owner : %d\n"\r\n"CPU%-2d context : %p\n"\r\n"CPU%-2d activations : %lu\n",\r\ncpu, pfm_stats[cpu].pfm_ovfl_intr_count,\r\ncpu, pfm_stats[cpu].pfm_ovfl_intr_cycles,\r\ncpu, pfm_stats[cpu].pfm_ovfl_intr_cycles_min,\r\ncpu, pfm_stats[cpu].pfm_ovfl_intr_cycles_max,\r\ncpu, pfm_stats[cpu].pfm_smpl_handler_calls,\r\ncpu, pfm_stats[cpu].pfm_smpl_handler_cycles,\r\ncpu, pfm_stats[cpu].pfm_spurious_ovfl_intr_count,\r\ncpu, pfm_stats[cpu].pfm_replay_ovfl_intr_count,\r\ncpu, pfm_get_cpu_data(pfm_syst_info, cpu) & PFM_CPUINFO_SYST_WIDE ? 1 : 0,\r\ncpu, pfm_get_cpu_data(pfm_syst_info, cpu) & PFM_CPUINFO_DCR_PP ? 1 : 0,\r\ncpu, pfm_get_cpu_data(pfm_syst_info, cpu) & PFM_CPUINFO_EXCL_IDLE ? 1 : 0,\r\ncpu, pfm_get_cpu_data(pmu_owner, cpu) ? pfm_get_cpu_data(pmu_owner, cpu)->pid: -1,\r\ncpu, pfm_get_cpu_data(pmu_ctx, cpu),\r\ncpu, pfm_get_cpu_data(pmu_activation_number, cpu));\r\nif (num_online_cpus() == 1 && pfm_sysctl.debug > 0) {\r\npsr = pfm_get_psr();\r\nia64_srlz_d();\r\nseq_printf(m,\r\n"CPU%-2d psr : 0x%lx\n"\r\n"CPU%-2d pmc0 : 0x%lx\n",\r\ncpu, psr,\r\ncpu, ia64_get_pmc(0));\r\nfor (i=0; PMC_IS_LAST(i) == 0; i++) {\r\nif (PMC_IS_COUNTING(i) == 0) continue;\r\nseq_printf(m,\r\n"CPU%-2d pmc%u : 0x%lx\n"\r\n"CPU%-2d pmd%u : 0x%lx\n",\r\ncpu, i, ia64_get_pmc(i),\r\ncpu, i, ia64_get_pmd(i));\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\npfm_proc_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &pfm_seq_ops);\r\n}\r\nvoid\r\npfm_syst_wide_update_task(struct task_struct *task, unsigned long info, int is_ctxswin)\r\n{\r\nstruct pt_regs *regs;\r\nunsigned long dcr;\r\nunsigned long dcr_pp;\r\ndcr_pp = info & PFM_CPUINFO_DCR_PP ? 1 : 0;\r\nif ((info & PFM_CPUINFO_EXCL_IDLE) == 0 || task->pid) {\r\nregs = task_pt_regs(task);\r\nia64_psr(regs)->pp = is_ctxswin ? dcr_pp : 0;\r\nreturn;\r\n}\r\nif (dcr_pp) {\r\ndcr = ia64_getreg(_IA64_REG_CR_DCR);\r\nif (is_ctxswin) {\r\nia64_setreg(_IA64_REG_CR_DCR, dcr & ~IA64_DCR_PP);\r\npfm_clear_psr_pp();\r\nia64_srlz_i();\r\nreturn;\r\n}\r\nia64_setreg(_IA64_REG_CR_DCR, dcr |IA64_DCR_PP);\r\npfm_set_psr_pp();\r\nia64_srlz_i();\r\n}\r\n}\r\nstatic void\r\npfm_force_cleanup(pfm_context_t *ctx, struct pt_regs *regs)\r\n{\r\nstruct task_struct *task = ctx->ctx_task;\r\nia64_psr(regs)->up = 0;\r\nia64_psr(regs)->sp = 1;\r\nif (GET_PMU_OWNER() == task) {\r\nDPRINT(("cleared ownership for [%d]\n",\r\ntask_pid_nr(ctx->ctx_task)));\r\nSET_PMU_OWNER(NULL, NULL);\r\n}\r\nPFM_SET_WORK_PENDING(task, 0);\r\ntask->thread.pfm_context = NULL;\r\ntask->thread.flags &= ~IA64_THREAD_PM_VALID;\r\nDPRINT(("force cleanup for [%d]\n", task_pid_nr(task)));\r\n}\r\nvoid\r\npfm_save_regs(struct task_struct *task)\r\n{\r\npfm_context_t *ctx;\r\nunsigned long flags;\r\nu64 psr;\r\nctx = PFM_GET_CTX(task);\r\nif (ctx == NULL) return;\r\nflags = pfm_protect_ctx_ctxsw(ctx);\r\nif (ctx->ctx_state == PFM_CTX_ZOMBIE) {\r\nstruct pt_regs *regs = task_pt_regs(task);\r\npfm_clear_psr_up();\r\npfm_force_cleanup(ctx, regs);\r\nBUG_ON(ctx->ctx_smpl_hdr);\r\npfm_unprotect_ctx_ctxsw(ctx, flags);\r\npfm_context_free(ctx);\r\nreturn;\r\n}\r\nia64_srlz_d();\r\npsr = pfm_get_psr();\r\nBUG_ON(psr & (IA64_PSR_I));\r\npfm_clear_psr_up();\r\nctx->ctx_saved_psr_up = psr & IA64_PSR_UP;\r\nSET_PMU_OWNER(NULL, NULL);\r\npfm_save_pmds(ctx->th_pmds, ctx->ctx_used_pmds[0]);\r\nctx->th_pmcs[0] = ia64_get_pmc(0);\r\nif (ctx->th_pmcs[0] & ~0x1UL) pfm_unfreeze_pmu();\r\npfm_unprotect_ctx_ctxsw(ctx, flags);\r\n}\r\nvoid\r\npfm_save_regs(struct task_struct *task)\r\n{\r\npfm_context_t *ctx;\r\nu64 psr;\r\nctx = PFM_GET_CTX(task);\r\nif (ctx == NULL) return;\r\npsr = pfm_get_psr();\r\nBUG_ON(psr & (IA64_PSR_I));\r\npfm_clear_psr_up();\r\nctx->ctx_saved_psr_up = psr & IA64_PSR_UP;\r\n}\r\nstatic void\r\npfm_lazy_save_regs (struct task_struct *task)\r\n{\r\npfm_context_t *ctx;\r\nunsigned long flags;\r\n{ u64 psr = pfm_get_psr();\r\nBUG_ON(psr & IA64_PSR_UP);\r\n}\r\nctx = PFM_GET_CTX(task);\r\nPROTECT_CTX(ctx,flags);\r\nSET_PMU_OWNER(NULL, NULL);\r\npfm_save_pmds(ctx->th_pmds, ctx->ctx_used_pmds[0]);\r\nctx->th_pmcs[0] = ia64_get_pmc(0);\r\nif (ctx->th_pmcs[0] & ~0x1UL) pfm_unfreeze_pmu();\r\nUNPROTECT_CTX(ctx,flags);\r\n}\r\nvoid\r\npfm_load_regs (struct task_struct *task)\r\n{\r\npfm_context_t *ctx;\r\nunsigned long pmc_mask = 0UL, pmd_mask = 0UL;\r\nunsigned long flags;\r\nu64 psr, psr_up;\r\nint need_irq_resend;\r\nctx = PFM_GET_CTX(task);\r\nif (unlikely(ctx == NULL)) return;\r\nBUG_ON(GET_PMU_OWNER());\r\nif (unlikely((task->thread.flags & IA64_THREAD_PM_VALID) == 0)) return;\r\nflags = pfm_protect_ctx_ctxsw(ctx);\r\npsr = pfm_get_psr();\r\nneed_irq_resend = pmu_conf->flags & PFM_PMU_IRQ_RESEND;\r\nBUG_ON(psr & (IA64_PSR_UP|IA64_PSR_PP));\r\nBUG_ON(psr & IA64_PSR_I);\r\nif (unlikely(ctx->ctx_state == PFM_CTX_ZOMBIE)) {\r\nstruct pt_regs *regs = task_pt_regs(task);\r\nBUG_ON(ctx->ctx_smpl_hdr);\r\npfm_force_cleanup(ctx, regs);\r\npfm_unprotect_ctx_ctxsw(ctx, flags);\r\npfm_context_free(ctx);\r\nreturn;\r\n}\r\nif (ctx->ctx_fl_using_dbreg) {\r\npfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);\r\npfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);\r\n}\r\npsr_up = ctx->ctx_saved_psr_up;\r\nif (GET_LAST_CPU(ctx) == smp_processor_id() && ctx->ctx_last_activation == GET_ACTIVATION()) {\r\npmc_mask = ctx->ctx_reload_pmcs[0];\r\npmd_mask = ctx->ctx_reload_pmds[0];\r\n} else {\r\npmd_mask = pfm_sysctl.fastctxsw ? ctx->ctx_used_pmds[0] : ctx->ctx_all_pmds[0];\r\npmc_mask = ctx->ctx_all_pmcs[0];\r\n}\r\nif (pmd_mask) pfm_restore_pmds(ctx->th_pmds, pmd_mask);\r\nif (pmc_mask) pfm_restore_pmcs(ctx->th_pmcs, pmc_mask);\r\nif (unlikely(PMC0_HAS_OVFL(ctx->th_pmcs[0]))) {\r\nia64_set_pmc(0, ctx->th_pmcs[0]);\r\nia64_srlz_d();\r\nctx->th_pmcs[0] = 0UL;\r\nif (need_irq_resend) ia64_resend_irq(IA64_PERFMON_VECTOR);\r\npfm_stats[smp_processor_id()].pfm_replay_ovfl_intr_count++;\r\n}\r\nctx->ctx_reload_pmcs[0] = 0UL;\r\nctx->ctx_reload_pmds[0] = 0UL;\r\nSET_LAST_CPU(ctx, smp_processor_id());\r\nINC_ACTIVATION();\r\nSET_ACTIVATION(ctx);\r\nSET_PMU_OWNER(task, ctx);\r\nif (likely(psr_up)) pfm_set_psr_up();\r\npfm_unprotect_ctx_ctxsw(ctx, flags);\r\n}\r\nvoid\r\npfm_load_regs (struct task_struct *task)\r\n{\r\npfm_context_t *ctx;\r\nstruct task_struct *owner;\r\nunsigned long pmd_mask, pmc_mask;\r\nu64 psr, psr_up;\r\nint need_irq_resend;\r\nowner = GET_PMU_OWNER();\r\nctx = PFM_GET_CTX(task);\r\npsr = pfm_get_psr();\r\nBUG_ON(psr & (IA64_PSR_UP|IA64_PSR_PP));\r\nBUG_ON(psr & IA64_PSR_I);\r\nif (ctx->ctx_fl_using_dbreg) {\r\npfm_restore_ibrs(ctx->ctx_ibrs, pmu_conf->num_ibrs);\r\npfm_restore_dbrs(ctx->ctx_dbrs, pmu_conf->num_dbrs);\r\n}\r\npsr_up = ctx->ctx_saved_psr_up;\r\nneed_irq_resend = pmu_conf->flags & PFM_PMU_IRQ_RESEND;\r\nif (likely(owner == task)) {\r\nif (likely(psr_up)) pfm_set_psr_up();\r\nreturn;\r\n}\r\nif (owner) pfm_lazy_save_regs(owner);\r\npmd_mask = pfm_sysctl.fastctxsw ? ctx->ctx_used_pmds[0] : ctx->ctx_all_pmds[0];\r\npmc_mask = ctx->ctx_all_pmcs[0];\r\npfm_restore_pmds(ctx->th_pmds, pmd_mask);\r\npfm_restore_pmcs(ctx->th_pmcs, pmc_mask);\r\nif (unlikely(PMC0_HAS_OVFL(ctx->th_pmcs[0]))) {\r\nia64_set_pmc(0, ctx->th_pmcs[0]);\r\nia64_srlz_d();\r\nctx->th_pmcs[0] = 0UL;\r\nif (need_irq_resend) ia64_resend_irq(IA64_PERFMON_VECTOR);\r\npfm_stats[smp_processor_id()].pfm_replay_ovfl_intr_count++;\r\n}\r\nSET_PMU_OWNER(task, ctx);\r\nif (likely(psr_up)) pfm_set_psr_up();\r\n}\r\nstatic void\r\npfm_flush_pmds(struct task_struct *task, pfm_context_t *ctx)\r\n{\r\nu64 pmc0;\r\nunsigned long mask2, val, pmd_val, ovfl_val;\r\nint i, can_access_pmu = 0;\r\nint is_self;\r\nis_self = ctx->ctx_task == task ? 1 : 0;\r\ncan_access_pmu = (GET_PMU_OWNER() == task) || (ctx->ctx_fl_system && ctx->ctx_cpu == smp_processor_id());\r\nif (can_access_pmu) {\r\nSET_PMU_OWNER(NULL, NULL);\r\nDPRINT(("releasing ownership\n"));\r\nia64_srlz_d();\r\npmc0 = ia64_get_pmc(0);\r\npfm_unfreeze_pmu();\r\n} else {\r\npmc0 = ctx->th_pmcs[0];\r\nctx->th_pmcs[0] = 0;\r\n}\r\novfl_val = pmu_conf->ovfl_val;\r\nmask2 = ctx->ctx_used_pmds[0];\r\nDPRINT(("is_self=%d ovfl_val=0x%lx mask2=0x%lx\n", is_self, ovfl_val, mask2));\r\nfor (i = 0; mask2; i++, mask2>>=1) {\r\nif ((mask2 & 0x1) == 0) continue;\r\nval = pmd_val = can_access_pmu ? ia64_get_pmd(i) : ctx->th_pmds[i];\r\nif (PMD_IS_COUNTING(i)) {\r\nDPRINT(("[%d] pmd[%d] ctx_pmd=0x%lx hw_pmd=0x%lx\n",\r\ntask_pid_nr(task),\r\ni,\r\nctx->ctx_pmds[i].val,\r\nval & ovfl_val));\r\nval = ctx->ctx_pmds[i].val + (val & ovfl_val);\r\npmd_val = 0UL;\r\nif (pmc0 & (1UL << i)) {\r\nval += 1 + ovfl_val;\r\nDPRINT(("[%d] pmd[%d] overflowed\n", task_pid_nr(task), i));\r\n}\r\n}\r\nDPRINT(("[%d] ctx_pmd[%d]=0x%lx pmd_val=0x%lx\n", task_pid_nr(task), i, val, pmd_val));\r\nif (is_self) ctx->th_pmds[i] = pmd_val;\r\nctx->ctx_pmds[i].val = val;\r\n}\r\n}\r\nstatic void\r\npfm_alt_save_pmu_state(void *data)\r\n{\r\nstruct pt_regs *regs;\r\nregs = task_pt_regs(current);\r\nDPRINT(("called\n"));\r\npfm_clear_psr_up();\r\npfm_clear_psr_pp();\r\nia64_psr(regs)->pp = 0;\r\npfm_freeze_pmu();\r\nia64_srlz_d();\r\n}\r\nvoid\r\npfm_alt_restore_pmu_state(void *data)\r\n{\r\nstruct pt_regs *regs;\r\nregs = task_pt_regs(current);\r\nDPRINT(("called\n"));\r\npfm_clear_psr_up();\r\npfm_clear_psr_pp();\r\nia64_psr(regs)->pp = 0;\r\npfm_unfreeze_pmu();\r\nia64_srlz_d();\r\n}\r\nint\r\npfm_install_alt_pmu_interrupt(pfm_intr_handler_desc_t *hdl)\r\n{\r\nint ret, i;\r\nint reserve_cpu;\r\nif (hdl == NULL || hdl->handler == NULL) return -EINVAL;\r\nif (pfm_alt_intr_handler) return -EBUSY;\r\nif (!spin_trylock(&pfm_alt_install_check)) {\r\nreturn -EBUSY;\r\n}\r\nfor_each_online_cpu(reserve_cpu) {\r\nret = pfm_reserve_session(NULL, 1, reserve_cpu);\r\nif (ret) goto cleanup_reserve;\r\n}\r\nret = on_each_cpu(pfm_alt_save_pmu_state, NULL, 1);\r\nif (ret) {\r\nDPRINT(("on_each_cpu() failed: %d\n", ret));\r\ngoto cleanup_reserve;\r\n}\r\npfm_alt_intr_handler = hdl;\r\nspin_unlock(&pfm_alt_install_check);\r\nreturn 0;\r\ncleanup_reserve:\r\nfor_each_online_cpu(i) {\r\nif (i >= reserve_cpu) break;\r\npfm_unreserve_session(NULL, 1, i);\r\n}\r\nspin_unlock(&pfm_alt_install_check);\r\nreturn ret;\r\n}\r\nint\r\npfm_remove_alt_pmu_interrupt(pfm_intr_handler_desc_t *hdl)\r\n{\r\nint i;\r\nint ret;\r\nif (hdl == NULL) return -EINVAL;\r\nif (pfm_alt_intr_handler != hdl) return -EINVAL;\r\nif (!spin_trylock(&pfm_alt_install_check)) {\r\nreturn -EBUSY;\r\n}\r\npfm_alt_intr_handler = NULL;\r\nret = on_each_cpu(pfm_alt_restore_pmu_state, NULL, 1);\r\nif (ret) {\r\nDPRINT(("on_each_cpu() failed: %d\n", ret));\r\n}\r\nfor_each_online_cpu(i) {\r\npfm_unreserve_session(NULL, 1, i);\r\n}\r\nspin_unlock(&pfm_alt_install_check);\r\nreturn 0;\r\n}\r\nstatic int __init\r\npfm_probe_pmu(void)\r\n{\r\npmu_config_t **p;\r\nint family;\r\nfamily = local_cpu_data->family;\r\np = pmu_confs;\r\nwhile(*p) {\r\nif ((*p)->probe) {\r\nif ((*p)->probe() == 0) goto found;\r\n} else if ((*p)->pmu_family == family || (*p)->pmu_family == 0xff) {\r\ngoto found;\r\n}\r\np++;\r\n}\r\nreturn -1;\r\nfound:\r\npmu_conf = *p;\r\nreturn 0;\r\n}\r\nint __init\r\npfm_init(void)\r\n{\r\nunsigned int n, n_counters, i;\r\nprintk("perfmon: version %u.%u IRQ %u\n",\r\nPFM_VERSION_MAJ,\r\nPFM_VERSION_MIN,\r\nIA64_PERFMON_VECTOR);\r\nif (pfm_probe_pmu()) {\r\nprintk(KERN_INFO "perfmon: disabled, there is no support for processor family %d\n",\r\nlocal_cpu_data->family);\r\nreturn -ENODEV;\r\n}\r\nn = 0;\r\nfor (i=0; PMC_IS_LAST(i) == 0; i++) {\r\nif (PMC_IS_IMPL(i) == 0) continue;\r\npmu_conf->impl_pmcs[i>>6] |= 1UL << (i&63);\r\nn++;\r\n}\r\npmu_conf->num_pmcs = n;\r\nn = 0; n_counters = 0;\r\nfor (i=0; PMD_IS_LAST(i) == 0; i++) {\r\nif (PMD_IS_IMPL(i) == 0) continue;\r\npmu_conf->impl_pmds[i>>6] |= 1UL << (i&63);\r\nn++;\r\nif (PMD_IS_COUNTING(i)) n_counters++;\r\n}\r\npmu_conf->num_pmds = n;\r\npmu_conf->num_counters = n_counters;\r\nif (pmu_conf->use_rr_dbregs) {\r\nif (pmu_conf->num_ibrs > IA64_NUM_DBG_REGS) {\r\nprintk(KERN_INFO "perfmon: unsupported number of code debug registers (%u)\n", pmu_conf->num_ibrs);\r\npmu_conf = NULL;\r\nreturn -1;\r\n}\r\nif (pmu_conf->num_dbrs > IA64_NUM_DBG_REGS) {\r\nprintk(KERN_INFO "perfmon: unsupported number of data debug registers (%u)\n", pmu_conf->num_ibrs);\r\npmu_conf = NULL;\r\nreturn -1;\r\n}\r\n}\r\nprintk("perfmon: %s PMU detected, %u PMCs, %u PMDs, %u counters (%lu bits)\n",\r\npmu_conf->pmu_name,\r\npmu_conf->num_pmcs,\r\npmu_conf->num_pmds,\r\npmu_conf->num_counters,\r\nffz(pmu_conf->ovfl_val));\r\nif (pmu_conf->num_pmds >= PFM_NUM_PMD_REGS || pmu_conf->num_pmcs >= PFM_NUM_PMC_REGS) {\r\nprintk(KERN_ERR "perfmon: not enough pmc/pmd, perfmon disabled\n");\r\npmu_conf = NULL;\r\nreturn -1;\r\n}\r\nperfmon_dir = proc_create("perfmon", S_IRUGO, NULL, &pfm_proc_fops);\r\nif (perfmon_dir == NULL) {\r\nprintk(KERN_ERR "perfmon: cannot create /proc entry, perfmon disabled\n");\r\npmu_conf = NULL;\r\nreturn -1;\r\n}\r\npfm_sysctl_header = register_sysctl_table(pfm_sysctl_root);\r\nspin_lock_init(&pfm_sessions.pfs_lock);\r\nspin_lock_init(&pfm_buffer_fmt_lock);\r\ninit_pfm_fs();\r\nfor(i=0; i < NR_CPUS; i++) pfm_stats[i].pfm_ovfl_intr_cycles_min = ~0UL;\r\nreturn 0;\r\n}\r\nvoid\r\npfm_init_percpu (void)\r\n{\r\nstatic int first_time=1;\r\npfm_clear_psr_pp();\r\npfm_clear_psr_up();\r\npfm_unfreeze_pmu();\r\nif (first_time) {\r\nregister_percpu_irq(IA64_PERFMON_VECTOR, &perfmon_irqaction);\r\nfirst_time=0;\r\n}\r\nia64_setreg(_IA64_REG_CR_PMV, IA64_PERFMON_VECTOR);\r\nia64_srlz_d();\r\n}\r\nvoid\r\ndump_pmu_state(const char *from)\r\n{\r\nstruct task_struct *task;\r\nstruct pt_regs *regs;\r\npfm_context_t *ctx;\r\nunsigned long psr, dcr, info, flags;\r\nint i, this_cpu;\r\nlocal_irq_save(flags);\r\nthis_cpu = smp_processor_id();\r\nregs = task_pt_regs(current);\r\ninfo = PFM_CPUINFO_GET();\r\ndcr = ia64_getreg(_IA64_REG_CR_DCR);\r\nif (info == 0 && ia64_psr(regs)->pp == 0 && (dcr & IA64_DCR_PP) == 0) {\r\nlocal_irq_restore(flags);\r\nreturn;\r\n}\r\nprintk("CPU%d from %s() current [%d] iip=0x%lx %s\n",\r\nthis_cpu,\r\nfrom,\r\ntask_pid_nr(current),\r\nregs->cr_iip,\r\ncurrent->comm);\r\ntask = GET_PMU_OWNER();\r\nctx = GET_PMU_CTX();\r\nprintk("->CPU%d owner [%d] ctx=%p\n", this_cpu, task ? task_pid_nr(task) : -1, ctx);\r\npsr = pfm_get_psr();\r\nprintk("->CPU%d pmc0=0x%lx psr.pp=%d psr.up=%d dcr.pp=%d syst_info=0x%lx user_psr.up=%d user_psr.pp=%d\n",\r\nthis_cpu,\r\nia64_get_pmc(0),\r\npsr & IA64_PSR_PP ? 1 : 0,\r\npsr & IA64_PSR_UP ? 1 : 0,\r\ndcr & IA64_DCR_PP ? 1 : 0,\r\ninfo,\r\nia64_psr(regs)->up,\r\nia64_psr(regs)->pp);\r\nia64_psr(regs)->up = 0;\r\nia64_psr(regs)->pp = 0;\r\nfor (i=1; PMC_IS_LAST(i) == 0; i++) {\r\nif (PMC_IS_IMPL(i) == 0) continue;\r\nprintk("->CPU%d pmc[%d]=0x%lx thread_pmc[%d]=0x%lx\n", this_cpu, i, ia64_get_pmc(i), i, ctx->th_pmcs[i]);\r\n}\r\nfor (i=1; PMD_IS_LAST(i) == 0; i++) {\r\nif (PMD_IS_IMPL(i) == 0) continue;\r\nprintk("->CPU%d pmd[%d]=0x%lx thread_pmd[%d]=0x%lx\n", this_cpu, i, ia64_get_pmd(i), i, ctx->th_pmds[i]);\r\n}\r\nif (ctx) {\r\nprintk("->CPU%d ctx_state=%d vaddr=%p addr=%p fd=%d ctx_task=[%d] saved_psr_up=0x%lx\n",\r\nthis_cpu,\r\nctx->ctx_state,\r\nctx->ctx_smpl_vaddr,\r\nctx->ctx_smpl_hdr,\r\nctx->ctx_msgq_head,\r\nctx->ctx_msgq_tail,\r\nctx->ctx_saved_psr_up);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nvoid\r\npfm_inherit(struct task_struct *task, struct pt_regs *regs)\r\n{\r\nstruct thread_struct *thread;\r\nDPRINT(("perfmon: pfm_inherit clearing state for [%d]\n", task_pid_nr(task)));\r\nthread = &task->thread;\r\nthread->pfm_context = NULL;\r\nPFM_SET_WORK_PENDING(task, 0);\r\n}\r\nasmlinkage long\r\nsys_perfmonctl (int fd, int cmd, void *arg, int count)\r\n{\r\nreturn -ENOSYS;\r\n}
