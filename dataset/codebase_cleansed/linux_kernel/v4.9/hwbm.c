void hwbm_buf_free(struct hwbm_pool *bm_pool, void *buf)\r\n{\r\nif (likely(bm_pool->frag_size <= PAGE_SIZE))\r\nskb_free_frag(buf);\r\nelse\r\nkfree(buf);\r\n}\r\nint hwbm_pool_refill(struct hwbm_pool *bm_pool, gfp_t gfp)\r\n{\r\nint frag_size = bm_pool->frag_size;\r\nvoid *buf;\r\nif (likely(frag_size <= PAGE_SIZE))\r\nbuf = netdev_alloc_frag(frag_size);\r\nelse\r\nbuf = kmalloc(frag_size, gfp);\r\nif (!buf)\r\nreturn -ENOMEM;\r\nif (bm_pool->construct)\r\nif (bm_pool->construct(bm_pool, buf)) {\r\nhwbm_buf_free(bm_pool, buf);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nint hwbm_pool_add(struct hwbm_pool *bm_pool, unsigned int buf_num, gfp_t gfp)\r\n{\r\nint err, i;\r\nunsigned long flags;\r\nspin_lock_irqsave(&bm_pool->lock, flags);\r\nif (bm_pool->buf_num == bm_pool->size) {\r\npr_warn("pool already filled\n");\r\nspin_unlock_irqrestore(&bm_pool->lock, flags);\r\nreturn bm_pool->buf_num;\r\n}\r\nif (buf_num + bm_pool->buf_num > bm_pool->size) {\r\npr_warn("cannot allocate %d buffers for pool\n",\r\nbuf_num);\r\nspin_unlock_irqrestore(&bm_pool->lock, flags);\r\nreturn 0;\r\n}\r\nif ((buf_num + bm_pool->buf_num) < bm_pool->buf_num) {\r\npr_warn("Adding %d buffers to the %d current buffers will overflow\n",\r\nbuf_num, bm_pool->buf_num);\r\nspin_unlock_irqrestore(&bm_pool->lock, flags);\r\nreturn 0;\r\n}\r\nfor (i = 0; i < buf_num; i++) {\r\nerr = hwbm_pool_refill(bm_pool, gfp);\r\nif (err < 0)\r\nbreak;\r\n}\r\nbm_pool->buf_num += i;\r\npr_debug("hwpm pool: %d of %d buffers added\n", i, buf_num);\r\nspin_unlock_irqrestore(&bm_pool->lock, flags);\r\nreturn i;\r\n}
