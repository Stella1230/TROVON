static void bdi_debug_init(void)\r\n{\r\nbdi_debug_root = debugfs_create_dir("bdi", NULL);\r\n}\r\nstatic int bdi_debug_stats_show(struct seq_file *m, void *v)\r\n{\r\nstruct backing_dev_info *bdi = m->private;\r\nstruct bdi_writeback *wb = &bdi->wb;\r\nunsigned long background_thresh;\r\nunsigned long dirty_thresh;\r\nunsigned long wb_thresh;\r\nunsigned long nr_dirty, nr_io, nr_more_io, nr_dirty_time;\r\nstruct inode *inode;\r\nnr_dirty = nr_io = nr_more_io = nr_dirty_time = 0;\r\nspin_lock(&wb->list_lock);\r\nlist_for_each_entry(inode, &wb->b_dirty, i_io_list)\r\nnr_dirty++;\r\nlist_for_each_entry(inode, &wb->b_io, i_io_list)\r\nnr_io++;\r\nlist_for_each_entry(inode, &wb->b_more_io, i_io_list)\r\nnr_more_io++;\r\nlist_for_each_entry(inode, &wb->b_dirty_time, i_io_list)\r\nif (inode->i_state & I_DIRTY_TIME)\r\nnr_dirty_time++;\r\nspin_unlock(&wb->list_lock);\r\nglobal_dirty_limits(&background_thresh, &dirty_thresh);\r\nwb_thresh = wb_calc_thresh(wb, dirty_thresh);\r\n#define K(x) ((x) << (PAGE_SHIFT - 10))\r\nseq_printf(m,\r\n"BdiWriteback: %10lu kB\n"\r\n"BdiReclaimable: %10lu kB\n"\r\n"BdiDirtyThresh: %10lu kB\n"\r\n"DirtyThresh: %10lu kB\n"\r\n"BackgroundThresh: %10lu kB\n"\r\n"BdiDirtied: %10lu kB\n"\r\n"BdiWritten: %10lu kB\n"\r\n"BdiWriteBandwidth: %10lu kBps\n"\r\n"b_dirty: %10lu\n"\r\n"b_io: %10lu\n"\r\n"b_more_io: %10lu\n"\r\n"b_dirty_time: %10lu\n"\r\n"bdi_list: %10u\n"\r\n"state: %10lx\n",\r\n(unsigned long) K(wb_stat(wb, WB_WRITEBACK)),\r\n(unsigned long) K(wb_stat(wb, WB_RECLAIMABLE)),\r\nK(wb_thresh),\r\nK(dirty_thresh),\r\nK(background_thresh),\r\n(unsigned long) K(wb_stat(wb, WB_DIRTIED)),\r\n(unsigned long) K(wb_stat(wb, WB_WRITTEN)),\r\n(unsigned long) K(wb->write_bandwidth),\r\nnr_dirty,\r\nnr_io,\r\nnr_more_io,\r\nnr_dirty_time,\r\n!list_empty(&bdi->bdi_list), bdi->wb.state);\r\n#undef K\r\nreturn 0;\r\n}\r\nstatic int bdi_debug_stats_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, bdi_debug_stats_show, inode->i_private);\r\n}\r\nstatic void bdi_debug_register(struct backing_dev_info *bdi, const char *name)\r\n{\r\nbdi->debug_dir = debugfs_create_dir(name, bdi_debug_root);\r\nbdi->debug_stats = debugfs_create_file("stats", 0444, bdi->debug_dir,\r\nbdi, &bdi_debug_stats_fops);\r\n}\r\nstatic void bdi_debug_unregister(struct backing_dev_info *bdi)\r\n{\r\ndebugfs_remove(bdi->debug_stats);\r\ndebugfs_remove(bdi->debug_dir);\r\n}\r\nstatic inline void bdi_debug_init(void)\r\n{\r\n}\r\nstatic inline void bdi_debug_register(struct backing_dev_info *bdi,\r\nconst char *name)\r\n{\r\n}\r\nstatic inline void bdi_debug_unregister(struct backing_dev_info *bdi)\r\n{\r\n}\r\nstatic ssize_t read_ahead_kb_store(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct backing_dev_info *bdi = dev_get_drvdata(dev);\r\nunsigned long read_ahead_kb;\r\nssize_t ret;\r\nret = kstrtoul(buf, 10, &read_ahead_kb);\r\nif (ret < 0)\r\nreturn ret;\r\nbdi->ra_pages = read_ahead_kb >> (PAGE_SHIFT - 10);\r\nreturn count;\r\n}\r\nstatic ssize_t min_ratio_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t count)\r\n{\r\nstruct backing_dev_info *bdi = dev_get_drvdata(dev);\r\nunsigned int ratio;\r\nssize_t ret;\r\nret = kstrtouint(buf, 10, &ratio);\r\nif (ret < 0)\r\nreturn ret;\r\nret = bdi_set_min_ratio(bdi, ratio);\r\nif (!ret)\r\nret = count;\r\nreturn ret;\r\n}\r\nstatic ssize_t max_ratio_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t count)\r\n{\r\nstruct backing_dev_info *bdi = dev_get_drvdata(dev);\r\nunsigned int ratio;\r\nssize_t ret;\r\nret = kstrtouint(buf, 10, &ratio);\r\nif (ret < 0)\r\nreturn ret;\r\nret = bdi_set_max_ratio(bdi, ratio);\r\nif (!ret)\r\nret = count;\r\nreturn ret;\r\n}\r\nstatic ssize_t stable_pages_required_show(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *page)\r\n{\r\nstruct backing_dev_info *bdi = dev_get_drvdata(dev);\r\nreturn snprintf(page, PAGE_SIZE-1, "%d\n",\r\nbdi_cap_stable_pages_required(bdi) ? 1 : 0);\r\n}\r\nstatic __init int bdi_class_init(void)\r\n{\r\nbdi_class = class_create(THIS_MODULE, "bdi");\r\nif (IS_ERR(bdi_class))\r\nreturn PTR_ERR(bdi_class);\r\nbdi_class->dev_groups = bdi_dev_groups;\r\nbdi_debug_init();\r\nreturn 0;\r\n}\r\nstatic int __init default_bdi_init(void)\r\n{\r\nint err;\r\nbdi_wq = alloc_workqueue("writeback", WQ_MEM_RECLAIM | WQ_FREEZABLE |\r\nWQ_UNBOUND | WQ_SYSFS, 0);\r\nif (!bdi_wq)\r\nreturn -ENOMEM;\r\nerr = bdi_init(&noop_backing_dev_info);\r\nreturn err;\r\n}\r\nvoid wb_wakeup_delayed(struct bdi_writeback *wb)\r\n{\r\nunsigned long timeout;\r\ntimeout = msecs_to_jiffies(dirty_writeback_interval * 10);\r\nspin_lock_bh(&wb->work_lock);\r\nif (test_bit(WB_registered, &wb->state))\r\nqueue_delayed_work(bdi_wq, &wb->dwork, timeout);\r\nspin_unlock_bh(&wb->work_lock);\r\n}\r\nstatic int wb_init(struct bdi_writeback *wb, struct backing_dev_info *bdi,\r\nint blkcg_id, gfp_t gfp)\r\n{\r\nint i, err;\r\nmemset(wb, 0, sizeof(*wb));\r\nwb->bdi = bdi;\r\nwb->last_old_flush = jiffies;\r\nINIT_LIST_HEAD(&wb->b_dirty);\r\nINIT_LIST_HEAD(&wb->b_io);\r\nINIT_LIST_HEAD(&wb->b_more_io);\r\nINIT_LIST_HEAD(&wb->b_dirty_time);\r\nspin_lock_init(&wb->list_lock);\r\nwb->bw_time_stamp = jiffies;\r\nwb->balanced_dirty_ratelimit = INIT_BW;\r\nwb->dirty_ratelimit = INIT_BW;\r\nwb->write_bandwidth = INIT_BW;\r\nwb->avg_write_bandwidth = INIT_BW;\r\nspin_lock_init(&wb->work_lock);\r\nINIT_LIST_HEAD(&wb->work_list);\r\nINIT_DELAYED_WORK(&wb->dwork, wb_workfn);\r\nwb->congested = wb_congested_get_create(bdi, blkcg_id, gfp);\r\nif (!wb->congested)\r\nreturn -ENOMEM;\r\nerr = fprop_local_init_percpu(&wb->completions, gfp);\r\nif (err)\r\ngoto out_put_cong;\r\nfor (i = 0; i < NR_WB_STAT_ITEMS; i++) {\r\nerr = percpu_counter_init(&wb->stat[i], 0, gfp);\r\nif (err)\r\ngoto out_destroy_stat;\r\n}\r\nreturn 0;\r\nout_destroy_stat:\r\nwhile (i--)\r\npercpu_counter_destroy(&wb->stat[i]);\r\nfprop_local_destroy_percpu(&wb->completions);\r\nout_put_cong:\r\nwb_congested_put(wb->congested);\r\nreturn err;\r\n}\r\nstatic void wb_shutdown(struct bdi_writeback *wb)\r\n{\r\nspin_lock_bh(&wb->work_lock);\r\nif (!test_and_clear_bit(WB_registered, &wb->state)) {\r\nspin_unlock_bh(&wb->work_lock);\r\nreturn;\r\n}\r\nspin_unlock_bh(&wb->work_lock);\r\nmod_delayed_work(bdi_wq, &wb->dwork, 0);\r\nflush_delayed_work(&wb->dwork);\r\nWARN_ON(!list_empty(&wb->work_list));\r\n}\r\nstatic void wb_exit(struct bdi_writeback *wb)\r\n{\r\nint i;\r\nWARN_ON(delayed_work_pending(&wb->dwork));\r\nfor (i = 0; i < NR_WB_STAT_ITEMS; i++)\r\npercpu_counter_destroy(&wb->stat[i]);\r\nfprop_local_destroy_percpu(&wb->completions);\r\nwb_congested_put(wb->congested);\r\n}\r\nstruct bdi_writeback_congested *\r\nwb_congested_get_create(struct backing_dev_info *bdi, int blkcg_id, gfp_t gfp)\r\n{\r\nstruct bdi_writeback_congested *new_congested = NULL, *congested;\r\nstruct rb_node **node, *parent;\r\nunsigned long flags;\r\nretry:\r\nspin_lock_irqsave(&cgwb_lock, flags);\r\nnode = &bdi->cgwb_congested_tree.rb_node;\r\nparent = NULL;\r\nwhile (*node != NULL) {\r\nparent = *node;\r\ncongested = container_of(parent, struct bdi_writeback_congested,\r\nrb_node);\r\nif (congested->blkcg_id < blkcg_id)\r\nnode = &parent->rb_left;\r\nelse if (congested->blkcg_id > blkcg_id)\r\nnode = &parent->rb_right;\r\nelse\r\ngoto found;\r\n}\r\nif (new_congested) {\r\ncongested = new_congested;\r\nnew_congested = NULL;\r\nrb_link_node(&congested->rb_node, parent, node);\r\nrb_insert_color(&congested->rb_node, &bdi->cgwb_congested_tree);\r\ngoto found;\r\n}\r\nspin_unlock_irqrestore(&cgwb_lock, flags);\r\nnew_congested = kzalloc(sizeof(*new_congested), gfp);\r\nif (!new_congested)\r\nreturn NULL;\r\natomic_set(&new_congested->refcnt, 0);\r\nnew_congested->bdi = bdi;\r\nnew_congested->blkcg_id = blkcg_id;\r\ngoto retry;\r\nfound:\r\natomic_inc(&congested->refcnt);\r\nspin_unlock_irqrestore(&cgwb_lock, flags);\r\nkfree(new_congested);\r\nreturn congested;\r\n}\r\nvoid wb_congested_put(struct bdi_writeback_congested *congested)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nif (!atomic_dec_and_lock(&congested->refcnt, &cgwb_lock)) {\r\nlocal_irq_restore(flags);\r\nreturn;\r\n}\r\nif (congested->bdi) {\r\nrb_erase(&congested->rb_node,\r\n&congested->bdi->cgwb_congested_tree);\r\ncongested->bdi = NULL;\r\n}\r\nspin_unlock_irqrestore(&cgwb_lock, flags);\r\nkfree(congested);\r\n}\r\nstatic void cgwb_release_workfn(struct work_struct *work)\r\n{\r\nstruct bdi_writeback *wb = container_of(work, struct bdi_writeback,\r\nrelease_work);\r\nstruct backing_dev_info *bdi = wb->bdi;\r\nspin_lock_irq(&cgwb_lock);\r\nlist_del_rcu(&wb->bdi_node);\r\nspin_unlock_irq(&cgwb_lock);\r\nwb_shutdown(wb);\r\ncss_put(wb->memcg_css);\r\ncss_put(wb->blkcg_css);\r\nfprop_local_destroy_percpu(&wb->memcg_completions);\r\npercpu_ref_exit(&wb->refcnt);\r\nwb_exit(wb);\r\nkfree_rcu(wb, rcu);\r\nif (atomic_dec_and_test(&bdi->usage_cnt))\r\nwake_up_all(&cgwb_release_wait);\r\n}\r\nstatic void cgwb_release(struct percpu_ref *refcnt)\r\n{\r\nstruct bdi_writeback *wb = container_of(refcnt, struct bdi_writeback,\r\nrefcnt);\r\nschedule_work(&wb->release_work);\r\n}\r\nstatic void cgwb_kill(struct bdi_writeback *wb)\r\n{\r\nlockdep_assert_held(&cgwb_lock);\r\nWARN_ON(!radix_tree_delete(&wb->bdi->cgwb_tree, wb->memcg_css->id));\r\nlist_del(&wb->memcg_node);\r\nlist_del(&wb->blkcg_node);\r\npercpu_ref_kill(&wb->refcnt);\r\n}\r\nstatic int cgwb_create(struct backing_dev_info *bdi,\r\nstruct cgroup_subsys_state *memcg_css, gfp_t gfp)\r\n{\r\nstruct mem_cgroup *memcg;\r\nstruct cgroup_subsys_state *blkcg_css;\r\nstruct blkcg *blkcg;\r\nstruct list_head *memcg_cgwb_list, *blkcg_cgwb_list;\r\nstruct bdi_writeback *wb;\r\nunsigned long flags;\r\nint ret = 0;\r\nmemcg = mem_cgroup_from_css(memcg_css);\r\nblkcg_css = cgroup_get_e_css(memcg_css->cgroup, &io_cgrp_subsys);\r\nblkcg = css_to_blkcg(blkcg_css);\r\nmemcg_cgwb_list = mem_cgroup_cgwb_list(memcg);\r\nblkcg_cgwb_list = &blkcg->cgwb_list;\r\nspin_lock_irqsave(&cgwb_lock, flags);\r\nwb = radix_tree_lookup(&bdi->cgwb_tree, memcg_css->id);\r\nif (wb && wb->blkcg_css != blkcg_css) {\r\ncgwb_kill(wb);\r\nwb = NULL;\r\n}\r\nspin_unlock_irqrestore(&cgwb_lock, flags);\r\nif (wb)\r\ngoto out_put;\r\nwb = kmalloc(sizeof(*wb), gfp);\r\nif (!wb)\r\nreturn -ENOMEM;\r\nret = wb_init(wb, bdi, blkcg_css->id, gfp);\r\nif (ret)\r\ngoto err_free;\r\nret = percpu_ref_init(&wb->refcnt, cgwb_release, 0, gfp);\r\nif (ret)\r\ngoto err_wb_exit;\r\nret = fprop_local_init_percpu(&wb->memcg_completions, gfp);\r\nif (ret)\r\ngoto err_ref_exit;\r\nwb->memcg_css = memcg_css;\r\nwb->blkcg_css = blkcg_css;\r\nINIT_WORK(&wb->release_work, cgwb_release_workfn);\r\nset_bit(WB_registered, &wb->state);\r\nret = -ENODEV;\r\nspin_lock_irqsave(&cgwb_lock, flags);\r\nif (test_bit(WB_registered, &bdi->wb.state) &&\r\nblkcg_cgwb_list->next && memcg_cgwb_list->next) {\r\nret = radix_tree_insert(&bdi->cgwb_tree, memcg_css->id, wb);\r\nif (!ret) {\r\natomic_inc(&bdi->usage_cnt);\r\nlist_add_tail_rcu(&wb->bdi_node, &bdi->wb_list);\r\nlist_add(&wb->memcg_node, memcg_cgwb_list);\r\nlist_add(&wb->blkcg_node, blkcg_cgwb_list);\r\ncss_get(memcg_css);\r\ncss_get(blkcg_css);\r\n}\r\n}\r\nspin_unlock_irqrestore(&cgwb_lock, flags);\r\nif (ret) {\r\nif (ret == -EEXIST)\r\nret = 0;\r\ngoto err_fprop_exit;\r\n}\r\ngoto out_put;\r\nerr_fprop_exit:\r\nfprop_local_destroy_percpu(&wb->memcg_completions);\r\nerr_ref_exit:\r\npercpu_ref_exit(&wb->refcnt);\r\nerr_wb_exit:\r\nwb_exit(wb);\r\nerr_free:\r\nkfree(wb);\r\nout_put:\r\ncss_put(blkcg_css);\r\nreturn ret;\r\n}\r\nstruct bdi_writeback *wb_get_create(struct backing_dev_info *bdi,\r\nstruct cgroup_subsys_state *memcg_css,\r\ngfp_t gfp)\r\n{\r\nstruct bdi_writeback *wb;\r\nmight_sleep_if(gfpflags_allow_blocking(gfp));\r\nif (!memcg_css->parent)\r\nreturn &bdi->wb;\r\ndo {\r\nrcu_read_lock();\r\nwb = radix_tree_lookup(&bdi->cgwb_tree, memcg_css->id);\r\nif (wb) {\r\nstruct cgroup_subsys_state *blkcg_css;\r\nblkcg_css = cgroup_get_e_css(memcg_css->cgroup,\r\n&io_cgrp_subsys);\r\nif (unlikely(wb->blkcg_css != blkcg_css ||\r\n!wb_tryget(wb)))\r\nwb = NULL;\r\ncss_put(blkcg_css);\r\n}\r\nrcu_read_unlock();\r\n} while (!wb && !cgwb_create(bdi, memcg_css, gfp));\r\nreturn wb;\r\n}\r\nstatic int cgwb_bdi_init(struct backing_dev_info *bdi)\r\n{\r\nint ret;\r\nINIT_RADIX_TREE(&bdi->cgwb_tree, GFP_ATOMIC);\r\nbdi->cgwb_congested_tree = RB_ROOT;\r\natomic_set(&bdi->usage_cnt, 1);\r\nret = wb_init(&bdi->wb, bdi, 1, GFP_KERNEL);\r\nif (!ret) {\r\nbdi->wb.memcg_css = &root_mem_cgroup->css;\r\nbdi->wb.blkcg_css = blkcg_root_css;\r\n}\r\nreturn ret;\r\n}\r\nstatic void cgwb_bdi_destroy(struct backing_dev_info *bdi)\r\n{\r\nstruct radix_tree_iter iter;\r\nstruct rb_node *rbn;\r\nvoid **slot;\r\nWARN_ON(test_bit(WB_registered, &bdi->wb.state));\r\nspin_lock_irq(&cgwb_lock);\r\nradix_tree_for_each_slot(slot, &bdi->cgwb_tree, &iter, 0)\r\ncgwb_kill(*slot);\r\nwhile ((rbn = rb_first(&bdi->cgwb_congested_tree))) {\r\nstruct bdi_writeback_congested *congested =\r\nrb_entry(rbn, struct bdi_writeback_congested, rb_node);\r\nrb_erase(rbn, &bdi->cgwb_congested_tree);\r\ncongested->bdi = NULL;\r\n}\r\nspin_unlock_irq(&cgwb_lock);\r\natomic_dec(&bdi->usage_cnt);\r\nwait_event(cgwb_release_wait, !atomic_read(&bdi->usage_cnt));\r\n}\r\nvoid wb_memcg_offline(struct mem_cgroup *memcg)\r\n{\r\nLIST_HEAD(to_destroy);\r\nstruct list_head *memcg_cgwb_list = mem_cgroup_cgwb_list(memcg);\r\nstruct bdi_writeback *wb, *next;\r\nspin_lock_irq(&cgwb_lock);\r\nlist_for_each_entry_safe(wb, next, memcg_cgwb_list, memcg_node)\r\ncgwb_kill(wb);\r\nmemcg_cgwb_list->next = NULL;\r\nspin_unlock_irq(&cgwb_lock);\r\n}\r\nvoid wb_blkcg_offline(struct blkcg *blkcg)\r\n{\r\nLIST_HEAD(to_destroy);\r\nstruct bdi_writeback *wb, *next;\r\nspin_lock_irq(&cgwb_lock);\r\nlist_for_each_entry_safe(wb, next, &blkcg->cgwb_list, blkcg_node)\r\ncgwb_kill(wb);\r\nblkcg->cgwb_list.next = NULL;\r\nspin_unlock_irq(&cgwb_lock);\r\n}\r\nstatic int cgwb_bdi_init(struct backing_dev_info *bdi)\r\n{\r\nint err;\r\nbdi->wb_congested = kzalloc(sizeof(*bdi->wb_congested), GFP_KERNEL);\r\nif (!bdi->wb_congested)\r\nreturn -ENOMEM;\r\nerr = wb_init(&bdi->wb, bdi, 1, GFP_KERNEL);\r\nif (err) {\r\nkfree(bdi->wb_congested);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic void cgwb_bdi_destroy(struct backing_dev_info *bdi) { }\r\nint bdi_init(struct backing_dev_info *bdi)\r\n{\r\nint ret;\r\nbdi->dev = NULL;\r\nbdi->min_ratio = 0;\r\nbdi->max_ratio = 100;\r\nbdi->max_prop_frac = FPROP_FRAC_BASE;\r\nINIT_LIST_HEAD(&bdi->bdi_list);\r\nINIT_LIST_HEAD(&bdi->wb_list);\r\ninit_waitqueue_head(&bdi->wb_waitq);\r\nret = cgwb_bdi_init(bdi);\r\nlist_add_tail_rcu(&bdi->wb.bdi_node, &bdi->wb_list);\r\nreturn ret;\r\n}\r\nint bdi_register(struct backing_dev_info *bdi, struct device *parent,\r\nconst char *fmt, ...)\r\n{\r\nva_list args;\r\nstruct device *dev;\r\nif (bdi->dev)\r\nreturn 0;\r\nva_start(args, fmt);\r\ndev = device_create_vargs(bdi_class, parent, MKDEV(0, 0), bdi, fmt, args);\r\nva_end(args);\r\nif (IS_ERR(dev))\r\nreturn PTR_ERR(dev);\r\nbdi->dev = dev;\r\nbdi_debug_register(bdi, dev_name(dev));\r\nset_bit(WB_registered, &bdi->wb.state);\r\nspin_lock_bh(&bdi_lock);\r\nlist_add_tail_rcu(&bdi->bdi_list, &bdi_list);\r\nspin_unlock_bh(&bdi_lock);\r\ntrace_writeback_bdi_register(bdi);\r\nreturn 0;\r\n}\r\nint bdi_register_dev(struct backing_dev_info *bdi, dev_t dev)\r\n{\r\nreturn bdi_register(bdi, NULL, "%u:%u", MAJOR(dev), MINOR(dev));\r\n}\r\nint bdi_register_owner(struct backing_dev_info *bdi, struct device *owner)\r\n{\r\nint rc;\r\nrc = bdi_register(bdi, NULL, "%u:%u", MAJOR(owner->devt),\r\nMINOR(owner->devt));\r\nif (rc)\r\nreturn rc;\r\nbdi->owner = owner;\r\nget_device(owner);\r\nreturn 0;\r\n}\r\nstatic void bdi_remove_from_list(struct backing_dev_info *bdi)\r\n{\r\nspin_lock_bh(&bdi_lock);\r\nlist_del_rcu(&bdi->bdi_list);\r\nspin_unlock_bh(&bdi_lock);\r\nsynchronize_rcu_expedited();\r\n}\r\nvoid bdi_unregister(struct backing_dev_info *bdi)\r\n{\r\nbdi_remove_from_list(bdi);\r\nwb_shutdown(&bdi->wb);\r\ncgwb_bdi_destroy(bdi);\r\nif (bdi->dev) {\r\nbdi_debug_unregister(bdi);\r\ndevice_unregister(bdi->dev);\r\nbdi->dev = NULL;\r\n}\r\nif (bdi->owner) {\r\nput_device(bdi->owner);\r\nbdi->owner = NULL;\r\n}\r\n}\r\nvoid bdi_exit(struct backing_dev_info *bdi)\r\n{\r\nWARN_ON_ONCE(bdi->dev);\r\nwb_exit(&bdi->wb);\r\n}\r\nvoid bdi_destroy(struct backing_dev_info *bdi)\r\n{\r\nbdi_unregister(bdi);\r\nbdi_exit(bdi);\r\n}\r\nint bdi_setup_and_register(struct backing_dev_info *bdi, char *name)\r\n{\r\nint err;\r\nbdi->name = name;\r\nbdi->capabilities = 0;\r\nerr = bdi_init(bdi);\r\nif (err)\r\nreturn err;\r\nerr = bdi_register(bdi, NULL, "%.28s-%ld", name,\r\natomic_long_inc_return(&bdi_seq));\r\nif (err) {\r\nbdi_destroy(bdi);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nvoid clear_wb_congested(struct bdi_writeback_congested *congested, int sync)\r\n{\r\nwait_queue_head_t *wqh = &congestion_wqh[sync];\r\nenum wb_congested_state bit;\r\nbit = sync ? WB_sync_congested : WB_async_congested;\r\nif (test_and_clear_bit(bit, &congested->state))\r\natomic_dec(&nr_wb_congested[sync]);\r\nsmp_mb__after_atomic();\r\nif (waitqueue_active(wqh))\r\nwake_up(wqh);\r\n}\r\nvoid set_wb_congested(struct bdi_writeback_congested *congested, int sync)\r\n{\r\nenum wb_congested_state bit;\r\nbit = sync ? WB_sync_congested : WB_async_congested;\r\nif (!test_and_set_bit(bit, &congested->state))\r\natomic_inc(&nr_wb_congested[sync]);\r\n}\r\nlong congestion_wait(int sync, long timeout)\r\n{\r\nlong ret;\r\nunsigned long start = jiffies;\r\nDEFINE_WAIT(wait);\r\nwait_queue_head_t *wqh = &congestion_wqh[sync];\r\nprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);\r\nret = io_schedule_timeout(timeout);\r\nfinish_wait(wqh, &wait);\r\ntrace_writeback_congestion_wait(jiffies_to_usecs(timeout),\r\njiffies_to_usecs(jiffies - start));\r\nreturn ret;\r\n}\r\nlong wait_iff_congested(struct pglist_data *pgdat, int sync, long timeout)\r\n{\r\nlong ret;\r\nunsigned long start = jiffies;\r\nDEFINE_WAIT(wait);\r\nwait_queue_head_t *wqh = &congestion_wqh[sync];\r\nif (atomic_read(&nr_wb_congested[sync]) == 0 ||\r\n!test_bit(PGDAT_CONGESTED, &pgdat->flags)) {\r\ncond_resched();\r\nret = timeout - (jiffies - start);\r\nif (ret < 0)\r\nret = 0;\r\ngoto out;\r\n}\r\nprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);\r\nret = io_schedule_timeout(timeout);\r\nfinish_wait(wqh, &wait);\r\nout:\r\ntrace_writeback_wait_iff_congested(jiffies_to_usecs(timeout),\r\njiffies_to_usecs(jiffies - start));\r\nreturn ret;\r\n}\r\nint pdflush_proc_obsolete(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp, loff_t *ppos)\r\n{\r\nchar kbuf[] = "0\n";\r\nif (*ppos || *lenp < sizeof(kbuf)) {\r\n*lenp = 0;\r\nreturn 0;\r\n}\r\nif (copy_to_user(buffer, kbuf, sizeof(kbuf)))\r\nreturn -EFAULT;\r\npr_warn_once("%s exported in /proc is scheduled for removal\n",\r\ntable->procname);\r\n*lenp = 2;\r\n*ppos += *lenp;\r\nreturn 2;\r\n}
