pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)\r\n{\r\nreturn (pte_t *)__get_free_page(PGALLOC_GFP & ~__GFP_ACCOUNT);\r\n}\r\npgtable_t pte_alloc_one(struct mm_struct *mm, unsigned long address)\r\n{\r\nstruct page *pte;\r\npte = alloc_pages(__userpte_alloc_gfp, 0);\r\nif (!pte)\r\nreturn NULL;\r\nif (!pgtable_page_ctor(pte)) {\r\n__free_page(pte);\r\nreturn NULL;\r\n}\r\nreturn pte;\r\n}\r\nstatic int __init setup_userpte(char *arg)\r\n{\r\nif (!arg)\r\nreturn -EINVAL;\r\nif (strcmp(arg, "nohigh") == 0)\r\n__userpte_alloc_gfp &= ~__GFP_HIGHMEM;\r\nelse\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nvoid ___pte_free_tlb(struct mmu_gather *tlb, struct page *pte)\r\n{\r\npgtable_page_dtor(pte);\r\nparavirt_release_pte(page_to_pfn(pte));\r\ntlb_remove_page(tlb, pte);\r\n}\r\nvoid ___pmd_free_tlb(struct mmu_gather *tlb, pmd_t *pmd)\r\n{\r\nstruct page *page = virt_to_page(pmd);\r\nparavirt_release_pmd(__pa(pmd) >> PAGE_SHIFT);\r\n#ifdef CONFIG_X86_PAE\r\ntlb->need_flush_all = 1;\r\n#endif\r\npgtable_pmd_page_dtor(page);\r\ntlb_remove_page(tlb, page);\r\n}\r\nvoid ___pud_free_tlb(struct mmu_gather *tlb, pud_t *pud)\r\n{\r\nparavirt_release_pud(__pa(pud) >> PAGE_SHIFT);\r\ntlb_remove_page(tlb, virt_to_page(pud));\r\n}\r\nstatic inline void pgd_list_add(pgd_t *pgd)\r\n{\r\nstruct page *page = virt_to_page(pgd);\r\nlist_add(&page->lru, &pgd_list);\r\n}\r\nstatic inline void pgd_list_del(pgd_t *pgd)\r\n{\r\nstruct page *page = virt_to_page(pgd);\r\nlist_del(&page->lru);\r\n}\r\nstatic void pgd_set_mm(pgd_t *pgd, struct mm_struct *mm)\r\n{\r\nBUILD_BUG_ON(sizeof(virt_to_page(pgd)->index) < sizeof(mm));\r\nvirt_to_page(pgd)->index = (pgoff_t)mm;\r\n}\r\nstruct mm_struct *pgd_page_get_mm(struct page *page)\r\n{\r\nreturn (struct mm_struct *)page->index;\r\n}\r\nstatic void pgd_ctor(struct mm_struct *mm, pgd_t *pgd)\r\n{\r\nif (CONFIG_PGTABLE_LEVELS == 2 ||\r\n(CONFIG_PGTABLE_LEVELS == 3 && SHARED_KERNEL_PMD) ||\r\nCONFIG_PGTABLE_LEVELS == 4) {\r\nclone_pgd_range(pgd + KERNEL_PGD_BOUNDARY,\r\nswapper_pg_dir + KERNEL_PGD_BOUNDARY,\r\nKERNEL_PGD_PTRS);\r\n}\r\nif (!SHARED_KERNEL_PMD) {\r\npgd_set_mm(pgd, mm);\r\npgd_list_add(pgd);\r\n}\r\n}\r\nstatic void pgd_dtor(pgd_t *pgd)\r\n{\r\nif (SHARED_KERNEL_PMD)\r\nreturn;\r\nspin_lock(&pgd_lock);\r\npgd_list_del(pgd);\r\nspin_unlock(&pgd_lock);\r\n}\r\nvoid pud_populate(struct mm_struct *mm, pud_t *pudp, pmd_t *pmd)\r\n{\r\nparavirt_alloc_pmd(mm, __pa(pmd) >> PAGE_SHIFT);\r\nset_pud(pudp, __pud(__pa(pmd) | _PAGE_PRESENT));\r\nflush_tlb_mm(mm);\r\n}\r\nstatic void free_pmds(struct mm_struct *mm, pmd_t *pmds[])\r\n{\r\nint i;\r\nfor(i = 0; i < PREALLOCATED_PMDS; i++)\r\nif (pmds[i]) {\r\npgtable_pmd_page_dtor(virt_to_page(pmds[i]));\r\nfree_page((unsigned long)pmds[i]);\r\nmm_dec_nr_pmds(mm);\r\n}\r\n}\r\nstatic int preallocate_pmds(struct mm_struct *mm, pmd_t *pmds[])\r\n{\r\nint i;\r\nbool failed = false;\r\ngfp_t gfp = PGALLOC_GFP;\r\nif (mm == &init_mm)\r\ngfp &= ~__GFP_ACCOUNT;\r\nfor(i = 0; i < PREALLOCATED_PMDS; i++) {\r\npmd_t *pmd = (pmd_t *)__get_free_page(gfp);\r\nif (!pmd)\r\nfailed = true;\r\nif (pmd && !pgtable_pmd_page_ctor(virt_to_page(pmd))) {\r\nfree_page((unsigned long)pmd);\r\npmd = NULL;\r\nfailed = true;\r\n}\r\nif (pmd)\r\nmm_inc_nr_pmds(mm);\r\npmds[i] = pmd;\r\n}\r\nif (failed) {\r\nfree_pmds(mm, pmds);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void pgd_mop_up_pmds(struct mm_struct *mm, pgd_t *pgdp)\r\n{\r\nint i;\r\nfor(i = 0; i < PREALLOCATED_PMDS; i++) {\r\npgd_t pgd = pgdp[i];\r\nif (pgd_val(pgd) != 0) {\r\npmd_t *pmd = (pmd_t *)pgd_page_vaddr(pgd);\r\npgdp[i] = native_make_pgd(0);\r\nparavirt_release_pmd(pgd_val(pgd) >> PAGE_SHIFT);\r\npmd_free(mm, pmd);\r\nmm_dec_nr_pmds(mm);\r\n}\r\n}\r\n}\r\nstatic void pgd_prepopulate_pmd(struct mm_struct *mm, pgd_t *pgd, pmd_t *pmds[])\r\n{\r\npud_t *pud;\r\nint i;\r\nif (PREALLOCATED_PMDS == 0)\r\nreturn;\r\npud = pud_offset(pgd, 0);\r\nfor (i = 0; i < PREALLOCATED_PMDS; i++, pud++) {\r\npmd_t *pmd = pmds[i];\r\nif (i >= KERNEL_PGD_BOUNDARY)\r\nmemcpy(pmd, (pmd_t *)pgd_page_vaddr(swapper_pg_dir[i]),\r\nsizeof(pmd_t) * PTRS_PER_PMD);\r\npud_populate(mm, pud, pmd);\r\n}\r\n}\r\nstatic int __init pgd_cache_init(void)\r\n{\r\nif (!SHARED_KERNEL_PMD)\r\nreturn 0;\r\npgd_cache = kmem_cache_create("pgd_cache", PGD_SIZE, PGD_ALIGN,\r\nSLAB_PANIC, NULL);\r\nif (!pgd_cache)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic inline pgd_t *_pgd_alloc(void)\r\n{\r\nif (!SHARED_KERNEL_PMD)\r\nreturn (pgd_t *)__get_free_page(PGALLOC_GFP);\r\nreturn kmem_cache_alloc(pgd_cache, PGALLOC_GFP);\r\n}\r\nstatic inline void _pgd_free(pgd_t *pgd)\r\n{\r\nif (!SHARED_KERNEL_PMD)\r\nfree_page((unsigned long)pgd);\r\nelse\r\nkmem_cache_free(pgd_cache, pgd);\r\n}\r\nstatic inline pgd_t *_pgd_alloc(void)\r\n{\r\nreturn (pgd_t *)__get_free_page(PGALLOC_GFP);\r\n}\r\nstatic inline void _pgd_free(pgd_t *pgd)\r\n{\r\nfree_page((unsigned long)pgd);\r\n}\r\npgd_t *pgd_alloc(struct mm_struct *mm)\r\n{\r\npgd_t *pgd;\r\npmd_t *pmds[PREALLOCATED_PMDS];\r\npgd = _pgd_alloc();\r\nif (pgd == NULL)\r\ngoto out;\r\nmm->pgd = pgd;\r\nif (preallocate_pmds(mm, pmds) != 0)\r\ngoto out_free_pgd;\r\nif (paravirt_pgd_alloc(mm) != 0)\r\ngoto out_free_pmds;\r\nspin_lock(&pgd_lock);\r\npgd_ctor(mm, pgd);\r\npgd_prepopulate_pmd(mm, pgd, pmds);\r\nspin_unlock(&pgd_lock);\r\nreturn pgd;\r\nout_free_pmds:\r\nfree_pmds(mm, pmds);\r\nout_free_pgd:\r\n_pgd_free(pgd);\r\nout:\r\nreturn NULL;\r\n}\r\nvoid pgd_free(struct mm_struct *mm, pgd_t *pgd)\r\n{\r\npgd_mop_up_pmds(mm, pgd);\r\npgd_dtor(pgd);\r\nparavirt_pgd_free(mm, pgd);\r\n_pgd_free(pgd);\r\n}\r\nint ptep_set_access_flags(struct vm_area_struct *vma,\r\nunsigned long address, pte_t *ptep,\r\npte_t entry, int dirty)\r\n{\r\nint changed = !pte_same(*ptep, entry);\r\nif (changed && dirty) {\r\n*ptep = entry;\r\npte_update(vma->vm_mm, address, ptep);\r\n}\r\nreturn changed;\r\n}\r\nint pmdp_set_access_flags(struct vm_area_struct *vma,\r\nunsigned long address, pmd_t *pmdp,\r\npmd_t entry, int dirty)\r\n{\r\nint changed = !pmd_same(*pmdp, entry);\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\nif (changed && dirty) {\r\n*pmdp = entry;\r\n}\r\nreturn changed;\r\n}\r\nint ptep_test_and_clear_young(struct vm_area_struct *vma,\r\nunsigned long addr, pte_t *ptep)\r\n{\r\nint ret = 0;\r\nif (pte_young(*ptep))\r\nret = test_and_clear_bit(_PAGE_BIT_ACCESSED,\r\n(unsigned long *) &ptep->pte);\r\nif (ret)\r\npte_update(vma->vm_mm, addr, ptep);\r\nreturn ret;\r\n}\r\nint pmdp_test_and_clear_young(struct vm_area_struct *vma,\r\nunsigned long addr, pmd_t *pmdp)\r\n{\r\nint ret = 0;\r\nif (pmd_young(*pmdp))\r\nret = test_and_clear_bit(_PAGE_BIT_ACCESSED,\r\n(unsigned long *)pmdp);\r\nreturn ret;\r\n}\r\nint ptep_clear_flush_young(struct vm_area_struct *vma,\r\nunsigned long address, pte_t *ptep)\r\n{\r\nreturn ptep_test_and_clear_young(vma, address, ptep);\r\n}\r\nint pmdp_clear_flush_young(struct vm_area_struct *vma,\r\nunsigned long address, pmd_t *pmdp)\r\n{\r\nint young;\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\nyoung = pmdp_test_and_clear_young(vma, address, pmdp);\r\nif (young)\r\nflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\r\nreturn young;\r\n}\r\nvoid __init reserve_top_address(unsigned long reserve)\r\n{\r\n#ifdef CONFIG_X86_32\r\nBUG_ON(fixmaps_set > 0);\r\n__FIXADDR_TOP = round_down(-reserve, 1 << PMD_SHIFT) - PAGE_SIZE;\r\nprintk(KERN_INFO "Reserving virtual address space above 0x%08lx (rounded to 0x%08lx)\n",\r\n-reserve, __FIXADDR_TOP + PAGE_SIZE);\r\n#endif\r\n}\r\nvoid __native_set_fixmap(enum fixed_addresses idx, pte_t pte)\r\n{\r\nunsigned long address = __fix_to_virt(idx);\r\nif (idx >= __end_of_fixed_addresses) {\r\nBUG();\r\nreturn;\r\n}\r\nset_pte_vaddr(address, pte);\r\nfixmaps_set++;\r\n}\r\nvoid native_set_fixmap(enum fixed_addresses idx, phys_addr_t phys,\r\npgprot_t flags)\r\n{\r\n__native_set_fixmap(idx, pfn_pte(phys >> PAGE_SHIFT, flags));\r\n}\r\nint pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot)\r\n{\r\nu8 mtrr, uniform;\r\nmtrr = mtrr_type_lookup(addr, addr + PUD_SIZE, &uniform);\r\nif ((mtrr != MTRR_TYPE_INVALID) && (!uniform) &&\r\n(mtrr != MTRR_TYPE_WRBACK))\r\nreturn 0;\r\nprot = pgprot_4k_2_large(prot);\r\nset_pte((pte_t *)pud, pfn_pte(\r\n(u64)addr >> PAGE_SHIFT,\r\n__pgprot(pgprot_val(prot) | _PAGE_PSE)));\r\nreturn 1;\r\n}\r\nint pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot)\r\n{\r\nu8 mtrr, uniform;\r\nmtrr = mtrr_type_lookup(addr, addr + PMD_SIZE, &uniform);\r\nif ((mtrr != MTRR_TYPE_INVALID) && (!uniform) &&\r\n(mtrr != MTRR_TYPE_WRBACK)) {\r\npr_warn_once("%s: Cannot satisfy [mem %#010llx-%#010llx] with a huge-page mapping due to MTRR override.\n",\r\n__func__, addr, addr + PMD_SIZE);\r\nreturn 0;\r\n}\r\nprot = pgprot_4k_2_large(prot);\r\nset_pte((pte_t *)pmd, pfn_pte(\r\n(u64)addr >> PAGE_SHIFT,\r\n__pgprot(pgprot_val(prot) | _PAGE_PSE)));\r\nreturn 1;\r\n}\r\nint pud_clear_huge(pud_t *pud)\r\n{\r\nif (pud_large(*pud)) {\r\npud_clear(pud);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nint pmd_clear_huge(pmd_t *pmd)\r\n{\r\nif (pmd_large(*pmd)) {\r\npmd_clear(pmd);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}
