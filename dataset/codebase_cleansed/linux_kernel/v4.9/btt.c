static int arena_read_bytes(struct arena_info *arena, resource_size_t offset,\r\nvoid *buf, size_t n)\r\n{\r\nstruct nd_btt *nd_btt = arena->nd_btt;\r\nstruct nd_namespace_common *ndns = nd_btt->ndns;\r\noffset += SZ_4K;\r\nreturn nvdimm_read_bytes(ndns, offset, buf, n);\r\n}\r\nstatic int arena_write_bytes(struct arena_info *arena, resource_size_t offset,\r\nvoid *buf, size_t n)\r\n{\r\nstruct nd_btt *nd_btt = arena->nd_btt;\r\nstruct nd_namespace_common *ndns = nd_btt->ndns;\r\noffset += SZ_4K;\r\nreturn nvdimm_write_bytes(ndns, offset, buf, n);\r\n}\r\nstatic int btt_info_write(struct arena_info *arena, struct btt_sb *super)\r\n{\r\nint ret;\r\nret = arena_write_bytes(arena, arena->info2off, super,\r\nsizeof(struct btt_sb));\r\nif (ret)\r\nreturn ret;\r\nreturn arena_write_bytes(arena, arena->infooff, super,\r\nsizeof(struct btt_sb));\r\n}\r\nstatic int btt_info_read(struct arena_info *arena, struct btt_sb *super)\r\n{\r\nWARN_ON(!super);\r\nreturn arena_read_bytes(arena, arena->infooff, super,\r\nsizeof(struct btt_sb));\r\n}\r\nstatic int __btt_map_write(struct arena_info *arena, u32 lba, __le32 mapping)\r\n{\r\nu64 ns_off = arena->mapoff + (lba * MAP_ENT_SIZE);\r\nWARN_ON(lba >= arena->external_nlba);\r\nreturn arena_write_bytes(arena, ns_off, &mapping, MAP_ENT_SIZE);\r\n}\r\nstatic int btt_map_write(struct arena_info *arena, u32 lba, u32 mapping,\r\nu32 z_flag, u32 e_flag)\r\n{\r\nu32 ze;\r\n__le32 mapping_le;\r\nmapping &= MAP_LBA_MASK;\r\nze = (z_flag << 1) + e_flag;\r\nswitch (ze) {\r\ncase 0:\r\nmapping |= MAP_ENT_NORMAL;\r\nbreak;\r\ncase 1:\r\nmapping |= (1 << MAP_ERR_SHIFT);\r\nbreak;\r\ncase 2:\r\nmapping |= (1 << MAP_TRIM_SHIFT);\r\nbreak;\r\ndefault:\r\nWARN_ONCE(1, "Invalid use of Z and E flags\n");\r\nreturn -EIO;\r\n}\r\nmapping_le = cpu_to_le32(mapping);\r\nreturn __btt_map_write(arena, lba, mapping_le);\r\n}\r\nstatic int btt_map_read(struct arena_info *arena, u32 lba, u32 *mapping,\r\nint *trim, int *error)\r\n{\r\nint ret;\r\n__le32 in;\r\nu32 raw_mapping, postmap, ze, z_flag, e_flag;\r\nu64 ns_off = arena->mapoff + (lba * MAP_ENT_SIZE);\r\nWARN_ON(lba >= arena->external_nlba);\r\nret = arena_read_bytes(arena, ns_off, &in, MAP_ENT_SIZE);\r\nif (ret)\r\nreturn ret;\r\nraw_mapping = le32_to_cpu(in);\r\nz_flag = (raw_mapping & MAP_TRIM_MASK) >> MAP_TRIM_SHIFT;\r\ne_flag = (raw_mapping & MAP_ERR_MASK) >> MAP_ERR_SHIFT;\r\nze = (z_flag << 1) + e_flag;\r\npostmap = raw_mapping & MAP_LBA_MASK;\r\nz_flag = 0;\r\ne_flag = 0;\r\nswitch (ze) {\r\ncase 0:\r\n*mapping = lba;\r\nbreak;\r\ncase 1:\r\n*mapping = postmap;\r\ne_flag = 1;\r\nbreak;\r\ncase 2:\r\n*mapping = postmap;\r\nz_flag = 1;\r\nbreak;\r\ncase 3:\r\n*mapping = postmap;\r\nbreak;\r\ndefault:\r\nreturn -EIO;\r\n}\r\nif (trim)\r\n*trim = z_flag;\r\nif (error)\r\n*error = e_flag;\r\nreturn ret;\r\n}\r\nstatic int btt_log_read_pair(struct arena_info *arena, u32 lane,\r\nstruct log_entry *ent)\r\n{\r\nWARN_ON(!ent);\r\nreturn arena_read_bytes(arena,\r\narena->logoff + (2 * lane * LOG_ENT_SIZE), ent,\r\n2 * LOG_ENT_SIZE);\r\n}\r\nstatic void arena_debugfs_init(struct arena_info *a, struct dentry *parent,\r\nint idx)\r\n{\r\nchar dirname[32];\r\nstruct dentry *d;\r\nif (!parent)\r\nreturn;\r\nsnprintf(dirname, 32, "arena%d", idx);\r\nd = debugfs_create_dir(dirname, parent);\r\nif (IS_ERR_OR_NULL(d))\r\nreturn;\r\na->debugfs_dir = d;\r\ndebugfs_create_x64("size", S_IRUGO, d, &a->size);\r\ndebugfs_create_x64("external_lba_start", S_IRUGO, d,\r\n&a->external_lba_start);\r\ndebugfs_create_x32("internal_nlba", S_IRUGO, d, &a->internal_nlba);\r\ndebugfs_create_u32("internal_lbasize", S_IRUGO, d,\r\n&a->internal_lbasize);\r\ndebugfs_create_x32("external_nlba", S_IRUGO, d, &a->external_nlba);\r\ndebugfs_create_u32("external_lbasize", S_IRUGO, d,\r\n&a->external_lbasize);\r\ndebugfs_create_u32("nfree", S_IRUGO, d, &a->nfree);\r\ndebugfs_create_u16("version_major", S_IRUGO, d, &a->version_major);\r\ndebugfs_create_u16("version_minor", S_IRUGO, d, &a->version_minor);\r\ndebugfs_create_x64("nextoff", S_IRUGO, d, &a->nextoff);\r\ndebugfs_create_x64("infooff", S_IRUGO, d, &a->infooff);\r\ndebugfs_create_x64("dataoff", S_IRUGO, d, &a->dataoff);\r\ndebugfs_create_x64("mapoff", S_IRUGO, d, &a->mapoff);\r\ndebugfs_create_x64("logoff", S_IRUGO, d, &a->logoff);\r\ndebugfs_create_x64("info2off", S_IRUGO, d, &a->info2off);\r\ndebugfs_create_x32("flags", S_IRUGO, d, &a->flags);\r\n}\r\nstatic void btt_debugfs_init(struct btt *btt)\r\n{\r\nint i = 0;\r\nstruct arena_info *arena;\r\nbtt->debugfs_dir = debugfs_create_dir(dev_name(&btt->nd_btt->dev),\r\ndebugfs_root);\r\nif (IS_ERR_OR_NULL(btt->debugfs_dir))\r\nreturn;\r\nlist_for_each_entry(arena, &btt->arena_list, list) {\r\narena_debugfs_init(arena, btt->debugfs_dir, i);\r\ni++;\r\n}\r\n}\r\nstatic int btt_log_get_old(struct log_entry *ent)\r\n{\r\nint old;\r\nif (ent[0].seq == 0) {\r\nent[0].seq = cpu_to_le32(1);\r\nreturn 0;\r\n}\r\nif (ent[0].seq == ent[1].seq)\r\nreturn -EINVAL;\r\nif (le32_to_cpu(ent[0].seq) + le32_to_cpu(ent[1].seq) > 5)\r\nreturn -EINVAL;\r\nif (le32_to_cpu(ent[0].seq) < le32_to_cpu(ent[1].seq)) {\r\nif (le32_to_cpu(ent[1].seq) - le32_to_cpu(ent[0].seq) == 1)\r\nold = 0;\r\nelse\r\nold = 1;\r\n} else {\r\nif (le32_to_cpu(ent[0].seq) - le32_to_cpu(ent[1].seq) == 1)\r\nold = 1;\r\nelse\r\nold = 0;\r\n}\r\nreturn old;\r\n}\r\nstatic struct device *to_dev(struct arena_info *arena)\r\n{\r\nreturn &arena->nd_btt->dev;\r\n}\r\nstatic int btt_log_read(struct arena_info *arena, u32 lane,\r\nstruct log_entry *ent, int old_flag)\r\n{\r\nint ret;\r\nint old_ent, ret_ent;\r\nstruct log_entry log[2];\r\nret = btt_log_read_pair(arena, lane, log);\r\nif (ret)\r\nreturn -EIO;\r\nold_ent = btt_log_get_old(log);\r\nif (old_ent < 0 || old_ent > 1) {\r\ndev_info(to_dev(arena),\r\n"log corruption (%d): lane %d seq [%d, %d]\n",\r\nold_ent, lane, log[0].seq, log[1].seq);\r\nreturn -EIO;\r\n}\r\nret_ent = (old_flag ? old_ent : (1 - old_ent));\r\nif (ent != NULL)\r\nmemcpy(ent, &log[ret_ent], LOG_ENT_SIZE);\r\nreturn ret_ent;\r\n}\r\nstatic int __btt_log_write(struct arena_info *arena, u32 lane,\r\nu32 sub, struct log_entry *ent)\r\n{\r\nint ret;\r\nunsigned int log_half = (LOG_ENT_SIZE - 2 * sizeof(u64)) / 2;\r\nu64 ns_off = arena->logoff + (((2 * lane) + sub) * LOG_ENT_SIZE);\r\nvoid *src = ent;\r\nret = arena_write_bytes(arena, ns_off, src, log_half);\r\nif (ret)\r\nreturn ret;\r\nns_off += log_half;\r\nsrc += log_half;\r\nreturn arena_write_bytes(arena, ns_off, src, log_half);\r\n}\r\nstatic int btt_flog_write(struct arena_info *arena, u32 lane, u32 sub,\r\nstruct log_entry *ent)\r\n{\r\nint ret;\r\nret = __btt_log_write(arena, lane, sub, ent);\r\nif (ret)\r\nreturn ret;\r\narena->freelist[lane].sub = 1 - arena->freelist[lane].sub;\r\nif (++(arena->freelist[lane].seq) == 4)\r\narena->freelist[lane].seq = 1;\r\narena->freelist[lane].block = le32_to_cpu(ent->old_map);\r\nreturn ret;\r\n}\r\nstatic int btt_map_init(struct arena_info *arena)\r\n{\r\nint ret = -EINVAL;\r\nvoid *zerobuf;\r\nsize_t offset = 0;\r\nsize_t chunk_size = SZ_2M;\r\nsize_t mapsize = arena->logoff - arena->mapoff;\r\nzerobuf = kzalloc(chunk_size, GFP_KERNEL);\r\nif (!zerobuf)\r\nreturn -ENOMEM;\r\nwhile (mapsize) {\r\nsize_t size = min(mapsize, chunk_size);\r\nret = arena_write_bytes(arena, arena->mapoff + offset, zerobuf,\r\nsize);\r\nif (ret)\r\ngoto free;\r\noffset += size;\r\nmapsize -= size;\r\ncond_resched();\r\n}\r\nfree:\r\nkfree(zerobuf);\r\nreturn ret;\r\n}\r\nstatic int btt_log_init(struct arena_info *arena)\r\n{\r\nint ret;\r\nu32 i;\r\nstruct log_entry log, zerolog;\r\nmemset(&zerolog, 0, sizeof(zerolog));\r\nfor (i = 0; i < arena->nfree; i++) {\r\nlog.lba = cpu_to_le32(i);\r\nlog.old_map = cpu_to_le32(arena->external_nlba + i);\r\nlog.new_map = cpu_to_le32(arena->external_nlba + i);\r\nlog.seq = cpu_to_le32(LOG_SEQ_INIT);\r\nret = __btt_log_write(arena, i, 0, &log);\r\nif (ret)\r\nreturn ret;\r\nret = __btt_log_write(arena, i, 1, &zerolog);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int btt_freelist_init(struct arena_info *arena)\r\n{\r\nint old, new, ret;\r\nu32 i, map_entry;\r\nstruct log_entry log_new, log_old;\r\narena->freelist = kcalloc(arena->nfree, sizeof(struct free_entry),\r\nGFP_KERNEL);\r\nif (!arena->freelist)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < arena->nfree; i++) {\r\nold = btt_log_read(arena, i, &log_old, LOG_OLD_ENT);\r\nif (old < 0)\r\nreturn old;\r\nnew = btt_log_read(arena, i, &log_new, LOG_NEW_ENT);\r\nif (new < 0)\r\nreturn new;\r\narena->freelist[i].sub = 1 - new;\r\narena->freelist[i].seq = nd_inc_seq(le32_to_cpu(log_new.seq));\r\narena->freelist[i].block = le32_to_cpu(log_new.old_map);\r\nif (log_new.old_map == log_new.new_map)\r\ncontinue;\r\nret = btt_map_read(arena, le32_to_cpu(log_new.lba), &map_entry,\r\nNULL, NULL);\r\nif (ret)\r\nreturn ret;\r\nif ((le32_to_cpu(log_new.new_map) != map_entry) &&\r\n(le32_to_cpu(log_new.old_map) == map_entry)) {\r\nret = btt_map_write(arena, le32_to_cpu(log_new.lba),\r\nle32_to_cpu(log_new.new_map), 0, 0);\r\nif (ret)\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int btt_rtt_init(struct arena_info *arena)\r\n{\r\narena->rtt = kcalloc(arena->nfree, sizeof(u32), GFP_KERNEL);\r\nif (arena->rtt == NULL)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic int btt_maplocks_init(struct arena_info *arena)\r\n{\r\nu32 i;\r\narena->map_locks = kcalloc(arena->nfree, sizeof(struct aligned_lock),\r\nGFP_KERNEL);\r\nif (!arena->map_locks)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < arena->nfree; i++)\r\nspin_lock_init(&arena->map_locks[i].lock);\r\nreturn 0;\r\n}\r\nstatic struct arena_info *alloc_arena(struct btt *btt, size_t size,\r\nsize_t start, size_t arena_off)\r\n{\r\nstruct arena_info *arena;\r\nu64 logsize, mapsize, datasize;\r\nu64 available = size;\r\narena = kzalloc(sizeof(struct arena_info), GFP_KERNEL);\r\nif (!arena)\r\nreturn NULL;\r\narena->nd_btt = btt->nd_btt;\r\nif (!size)\r\nreturn arena;\r\narena->size = size;\r\narena->external_lba_start = start;\r\narena->external_lbasize = btt->lbasize;\r\narena->internal_lbasize = roundup(arena->external_lbasize,\r\nINT_LBASIZE_ALIGNMENT);\r\narena->nfree = BTT_DEFAULT_NFREE;\r\narena->version_major = 1;\r\narena->version_minor = 1;\r\nif (available % BTT_PG_SIZE)\r\navailable -= (available % BTT_PG_SIZE);\r\navailable -= 2 * BTT_PG_SIZE;\r\nlogsize = roundup(2 * arena->nfree * sizeof(struct log_entry),\r\nBTT_PG_SIZE);\r\navailable -= logsize;\r\narena->internal_nlba = div_u64(available - BTT_PG_SIZE,\r\narena->internal_lbasize + MAP_ENT_SIZE);\r\narena->external_nlba = arena->internal_nlba - arena->nfree;\r\nmapsize = roundup((arena->external_nlba * MAP_ENT_SIZE), BTT_PG_SIZE);\r\ndatasize = available - mapsize;\r\narena->infooff = arena_off;\r\narena->dataoff = arena->infooff + BTT_PG_SIZE;\r\narena->mapoff = arena->dataoff + datasize;\r\narena->logoff = arena->mapoff + mapsize;\r\narena->info2off = arena->logoff + logsize;\r\nreturn arena;\r\n}\r\nstatic void free_arenas(struct btt *btt)\r\n{\r\nstruct arena_info *arena, *next;\r\nlist_for_each_entry_safe(arena, next, &btt->arena_list, list) {\r\nlist_del(&arena->list);\r\nkfree(arena->rtt);\r\nkfree(arena->map_locks);\r\nkfree(arena->freelist);\r\ndebugfs_remove_recursive(arena->debugfs_dir);\r\nkfree(arena);\r\n}\r\n}\r\nstatic void parse_arena_meta(struct arena_info *arena, struct btt_sb *super,\r\nu64 arena_off)\r\n{\r\narena->internal_nlba = le32_to_cpu(super->internal_nlba);\r\narena->internal_lbasize = le32_to_cpu(super->internal_lbasize);\r\narena->external_nlba = le32_to_cpu(super->external_nlba);\r\narena->external_lbasize = le32_to_cpu(super->external_lbasize);\r\narena->nfree = le32_to_cpu(super->nfree);\r\narena->version_major = le16_to_cpu(super->version_major);\r\narena->version_minor = le16_to_cpu(super->version_minor);\r\narena->nextoff = (super->nextoff == 0) ? 0 : (arena_off +\r\nle64_to_cpu(super->nextoff));\r\narena->infooff = arena_off;\r\narena->dataoff = arena_off + le64_to_cpu(super->dataoff);\r\narena->mapoff = arena_off + le64_to_cpu(super->mapoff);\r\narena->logoff = arena_off + le64_to_cpu(super->logoff);\r\narena->info2off = arena_off + le64_to_cpu(super->info2off);\r\narena->size = (le64_to_cpu(super->nextoff) > 0)\r\n? (le64_to_cpu(super->nextoff))\r\n: (arena->info2off - arena->infooff + BTT_PG_SIZE);\r\narena->flags = le32_to_cpu(super->flags);\r\n}\r\nstatic int discover_arenas(struct btt *btt)\r\n{\r\nint ret = 0;\r\nstruct arena_info *arena;\r\nstruct btt_sb *super;\r\nsize_t remaining = btt->rawsize;\r\nu64 cur_nlba = 0;\r\nsize_t cur_off = 0;\r\nint num_arenas = 0;\r\nsuper = kzalloc(sizeof(*super), GFP_KERNEL);\r\nif (!super)\r\nreturn -ENOMEM;\r\nwhile (remaining) {\r\narena = alloc_arena(btt, 0, 0, 0);\r\nif (!arena) {\r\nret = -ENOMEM;\r\ngoto out_super;\r\n}\r\narena->infooff = cur_off;\r\nret = btt_info_read(arena, super);\r\nif (ret)\r\ngoto out;\r\nif (!nd_btt_arena_is_valid(btt->nd_btt, super)) {\r\nif (remaining == btt->rawsize) {\r\nbtt->init_state = INIT_NOTFOUND;\r\ndev_info(to_dev(arena), "No existing arenas\n");\r\ngoto out;\r\n} else {\r\ndev_info(to_dev(arena),\r\n"Found corrupted metadata!\n");\r\nret = -ENODEV;\r\ngoto out;\r\n}\r\n}\r\narena->external_lba_start = cur_nlba;\r\nparse_arena_meta(arena, super, cur_off);\r\nret = btt_freelist_init(arena);\r\nif (ret)\r\ngoto out;\r\nret = btt_rtt_init(arena);\r\nif (ret)\r\ngoto out;\r\nret = btt_maplocks_init(arena);\r\nif (ret)\r\ngoto out;\r\nlist_add_tail(&arena->list, &btt->arena_list);\r\nremaining -= arena->size;\r\ncur_off += arena->size;\r\ncur_nlba += arena->external_nlba;\r\nnum_arenas++;\r\nif (arena->nextoff == 0)\r\nbreak;\r\n}\r\nbtt->num_arenas = num_arenas;\r\nbtt->nlba = cur_nlba;\r\nbtt->init_state = INIT_READY;\r\nkfree(super);\r\nreturn ret;\r\nout:\r\nkfree(arena);\r\nfree_arenas(btt);\r\nout_super:\r\nkfree(super);\r\nreturn ret;\r\n}\r\nstatic int create_arenas(struct btt *btt)\r\n{\r\nsize_t remaining = btt->rawsize;\r\nsize_t cur_off = 0;\r\nwhile (remaining) {\r\nstruct arena_info *arena;\r\nsize_t arena_size = min_t(u64, ARENA_MAX_SIZE, remaining);\r\nremaining -= arena_size;\r\nif (arena_size < ARENA_MIN_SIZE)\r\nbreak;\r\narena = alloc_arena(btt, arena_size, btt->nlba, cur_off);\r\nif (!arena) {\r\nfree_arenas(btt);\r\nreturn -ENOMEM;\r\n}\r\nbtt->nlba += arena->external_nlba;\r\nif (remaining >= ARENA_MIN_SIZE)\r\narena->nextoff = arena->size;\r\nelse\r\narena->nextoff = 0;\r\ncur_off += arena_size;\r\nlist_add_tail(&arena->list, &btt->arena_list);\r\n}\r\nreturn 0;\r\n}\r\nstatic int btt_arena_write_layout(struct arena_info *arena)\r\n{\r\nint ret;\r\nu64 sum;\r\nstruct btt_sb *super;\r\nstruct nd_btt *nd_btt = arena->nd_btt;\r\nconst u8 *parent_uuid = nd_dev_to_uuid(&nd_btt->ndns->dev);\r\nret = btt_map_init(arena);\r\nif (ret)\r\nreturn ret;\r\nret = btt_log_init(arena);\r\nif (ret)\r\nreturn ret;\r\nsuper = kzalloc(sizeof(struct btt_sb), GFP_NOIO);\r\nif (!super)\r\nreturn -ENOMEM;\r\nstrncpy(super->signature, BTT_SIG, BTT_SIG_LEN);\r\nmemcpy(super->uuid, nd_btt->uuid, 16);\r\nmemcpy(super->parent_uuid, parent_uuid, 16);\r\nsuper->flags = cpu_to_le32(arena->flags);\r\nsuper->version_major = cpu_to_le16(arena->version_major);\r\nsuper->version_minor = cpu_to_le16(arena->version_minor);\r\nsuper->external_lbasize = cpu_to_le32(arena->external_lbasize);\r\nsuper->external_nlba = cpu_to_le32(arena->external_nlba);\r\nsuper->internal_lbasize = cpu_to_le32(arena->internal_lbasize);\r\nsuper->internal_nlba = cpu_to_le32(arena->internal_nlba);\r\nsuper->nfree = cpu_to_le32(arena->nfree);\r\nsuper->infosize = cpu_to_le32(sizeof(struct btt_sb));\r\nsuper->nextoff = cpu_to_le64(arena->nextoff);\r\nsuper->dataoff = cpu_to_le64(arena->dataoff - arena->infooff);\r\nsuper->mapoff = cpu_to_le64(arena->mapoff - arena->infooff);\r\nsuper->logoff = cpu_to_le64(arena->logoff - arena->infooff);\r\nsuper->info2off = cpu_to_le64(arena->info2off - arena->infooff);\r\nsuper->flags = 0;\r\nsum = nd_sb_checksum((struct nd_gen_sb *) super);\r\nsuper->checksum = cpu_to_le64(sum);\r\nret = btt_info_write(arena, super);\r\nkfree(super);\r\nreturn ret;\r\n}\r\nstatic int btt_meta_init(struct btt *btt)\r\n{\r\nint ret = 0;\r\nstruct arena_info *arena;\r\nmutex_lock(&btt->init_lock);\r\nlist_for_each_entry(arena, &btt->arena_list, list) {\r\nret = btt_arena_write_layout(arena);\r\nif (ret)\r\ngoto unlock;\r\nret = btt_freelist_init(arena);\r\nif (ret)\r\ngoto unlock;\r\nret = btt_rtt_init(arena);\r\nif (ret)\r\ngoto unlock;\r\nret = btt_maplocks_init(arena);\r\nif (ret)\r\ngoto unlock;\r\n}\r\nbtt->init_state = INIT_READY;\r\nunlock:\r\nmutex_unlock(&btt->init_lock);\r\nreturn ret;\r\n}\r\nstatic u32 btt_meta_size(struct btt *btt)\r\n{\r\nreturn btt->lbasize - btt->sector_size;\r\n}\r\nstatic int lba_to_arena(struct btt *btt, sector_t sector, __u32 *premap,\r\nstruct arena_info **arena)\r\n{\r\nstruct arena_info *arena_list;\r\n__u64 lba = div_u64(sector << SECTOR_SHIFT, btt->sector_size);\r\nlist_for_each_entry(arena_list, &btt->arena_list, list) {\r\nif (lba < arena_list->external_nlba) {\r\n*arena = arena_list;\r\n*premap = lba;\r\nreturn 0;\r\n}\r\nlba -= arena_list->external_nlba;\r\n}\r\nreturn -EIO;\r\n}\r\nstatic void lock_map(struct arena_info *arena, u32 premap)\r\n__acquires(&arena->map_locks[idx].lock\r\nstatic void unlock_map(struct arena_info *arena, u32 premap)\r\n__releases(&arena->map_locks[idx].lock\r\nstatic u64 to_namespace_offset(struct arena_info *arena, u64 lba)\r\n{\r\nreturn arena->dataoff + ((u64)lba * arena->internal_lbasize);\r\n}\r\nstatic int btt_data_read(struct arena_info *arena, struct page *page,\r\nunsigned int off, u32 lba, u32 len)\r\n{\r\nint ret;\r\nu64 nsoff = to_namespace_offset(arena, lba);\r\nvoid *mem = kmap_atomic(page);\r\nret = arena_read_bytes(arena, nsoff, mem + off, len);\r\nkunmap_atomic(mem);\r\nreturn ret;\r\n}\r\nstatic int btt_data_write(struct arena_info *arena, u32 lba,\r\nstruct page *page, unsigned int off, u32 len)\r\n{\r\nint ret;\r\nu64 nsoff = to_namespace_offset(arena, lba);\r\nvoid *mem = kmap_atomic(page);\r\nret = arena_write_bytes(arena, nsoff, mem + off, len);\r\nkunmap_atomic(mem);\r\nreturn ret;\r\n}\r\nstatic void zero_fill_data(struct page *page, unsigned int off, u32 len)\r\n{\r\nvoid *mem = kmap_atomic(page);\r\nmemset(mem + off, 0, len);\r\nkunmap_atomic(mem);\r\n}\r\nstatic int btt_rw_integrity(struct btt *btt, struct bio_integrity_payload *bip,\r\nstruct arena_info *arena, u32 postmap, int rw)\r\n{\r\nunsigned int len = btt_meta_size(btt);\r\nu64 meta_nsoff;\r\nint ret = 0;\r\nif (bip == NULL)\r\nreturn 0;\r\nmeta_nsoff = to_namespace_offset(arena, postmap) + btt->sector_size;\r\nwhile (len) {\r\nunsigned int cur_len;\r\nstruct bio_vec bv;\r\nvoid *mem;\r\nbv = bvec_iter_bvec(bip->bip_vec, bip->bip_iter);\r\ncur_len = min(len, bv.bv_len);\r\nmem = kmap_atomic(bv.bv_page);\r\nif (rw)\r\nret = arena_write_bytes(arena, meta_nsoff,\r\nmem + bv.bv_offset, cur_len);\r\nelse\r\nret = arena_read_bytes(arena, meta_nsoff,\r\nmem + bv.bv_offset, cur_len);\r\nkunmap_atomic(mem);\r\nif (ret)\r\nreturn ret;\r\nlen -= cur_len;\r\nmeta_nsoff += cur_len;\r\nbvec_iter_advance(bip->bip_vec, &bip->bip_iter, cur_len);\r\n}\r\nreturn ret;\r\n}\r\nstatic int btt_rw_integrity(struct btt *btt, struct bio_integrity_payload *bip,\r\nstruct arena_info *arena, u32 postmap, int rw)\r\n{\r\nreturn 0;\r\n}\r\nstatic int btt_read_pg(struct btt *btt, struct bio_integrity_payload *bip,\r\nstruct page *page, unsigned int off, sector_t sector,\r\nunsigned int len)\r\n{\r\nint ret = 0;\r\nint t_flag, e_flag;\r\nstruct arena_info *arena = NULL;\r\nu32 lane = 0, premap, postmap;\r\nwhile (len) {\r\nu32 cur_len;\r\nlane = nd_region_acquire_lane(btt->nd_region);\r\nret = lba_to_arena(btt, sector, &premap, &arena);\r\nif (ret)\r\ngoto out_lane;\r\ncur_len = min(btt->sector_size, len);\r\nret = btt_map_read(arena, premap, &postmap, &t_flag, &e_flag);\r\nif (ret)\r\ngoto out_lane;\r\nwhile (1) {\r\nu32 new_map;\r\nif (t_flag) {\r\nzero_fill_data(page, off, cur_len);\r\ngoto out_lane;\r\n}\r\nif (e_flag) {\r\nret = -EIO;\r\ngoto out_lane;\r\n}\r\narena->rtt[lane] = RTT_VALID | postmap;\r\nbarrier();\r\nret = btt_map_read(arena, premap, &new_map, &t_flag,\r\n&e_flag);\r\nif (ret)\r\ngoto out_rtt;\r\nif (postmap == new_map)\r\nbreak;\r\npostmap = new_map;\r\n}\r\nret = btt_data_read(arena, page, off, postmap, cur_len);\r\nif (ret)\r\ngoto out_rtt;\r\nif (bip) {\r\nret = btt_rw_integrity(btt, bip, arena, postmap, READ);\r\nif (ret)\r\ngoto out_rtt;\r\n}\r\narena->rtt[lane] = RTT_INVALID;\r\nnd_region_release_lane(btt->nd_region, lane);\r\nlen -= cur_len;\r\noff += cur_len;\r\nsector += btt->sector_size >> SECTOR_SHIFT;\r\n}\r\nreturn 0;\r\nout_rtt:\r\narena->rtt[lane] = RTT_INVALID;\r\nout_lane:\r\nnd_region_release_lane(btt->nd_region, lane);\r\nreturn ret;\r\n}\r\nstatic int btt_write_pg(struct btt *btt, struct bio_integrity_payload *bip,\r\nsector_t sector, struct page *page, unsigned int off,\r\nunsigned int len)\r\n{\r\nint ret = 0;\r\nstruct arena_info *arena = NULL;\r\nu32 premap = 0, old_postmap, new_postmap, lane = 0, i;\r\nstruct log_entry log;\r\nint sub;\r\nwhile (len) {\r\nu32 cur_len;\r\nlane = nd_region_acquire_lane(btt->nd_region);\r\nret = lba_to_arena(btt, sector, &premap, &arena);\r\nif (ret)\r\ngoto out_lane;\r\ncur_len = min(btt->sector_size, len);\r\nif ((arena->flags & IB_FLAG_ERROR_MASK) != 0) {\r\nret = -EIO;\r\ngoto out_lane;\r\n}\r\nnew_postmap = arena->freelist[lane].block;\r\nfor (i = 0; i < arena->nfree; i++)\r\nwhile (arena->rtt[i] == (RTT_VALID | new_postmap))\r\ncpu_relax();\r\nif (new_postmap >= arena->internal_nlba) {\r\nret = -EIO;\r\ngoto out_lane;\r\n}\r\nret = btt_data_write(arena, new_postmap, page, off, cur_len);\r\nif (ret)\r\ngoto out_lane;\r\nif (bip) {\r\nret = btt_rw_integrity(btt, bip, arena, new_postmap,\r\nWRITE);\r\nif (ret)\r\ngoto out_lane;\r\n}\r\nlock_map(arena, premap);\r\nret = btt_map_read(arena, premap, &old_postmap, NULL, NULL);\r\nif (ret)\r\ngoto out_map;\r\nif (old_postmap >= arena->internal_nlba) {\r\nret = -EIO;\r\ngoto out_map;\r\n}\r\nlog.lba = cpu_to_le32(premap);\r\nlog.old_map = cpu_to_le32(old_postmap);\r\nlog.new_map = cpu_to_le32(new_postmap);\r\nlog.seq = cpu_to_le32(arena->freelist[lane].seq);\r\nsub = arena->freelist[lane].sub;\r\nret = btt_flog_write(arena, lane, sub, &log);\r\nif (ret)\r\ngoto out_map;\r\nret = btt_map_write(arena, premap, new_postmap, 0, 0);\r\nif (ret)\r\ngoto out_map;\r\nunlock_map(arena, premap);\r\nnd_region_release_lane(btt->nd_region, lane);\r\nlen -= cur_len;\r\noff += cur_len;\r\nsector += btt->sector_size >> SECTOR_SHIFT;\r\n}\r\nreturn 0;\r\nout_map:\r\nunlock_map(arena, premap);\r\nout_lane:\r\nnd_region_release_lane(btt->nd_region, lane);\r\nreturn ret;\r\n}\r\nstatic int btt_do_bvec(struct btt *btt, struct bio_integrity_payload *bip,\r\nstruct page *page, unsigned int len, unsigned int off,\r\nbool is_write, sector_t sector)\r\n{\r\nint ret;\r\nif (!is_write) {\r\nret = btt_read_pg(btt, bip, page, off, sector, len);\r\nflush_dcache_page(page);\r\n} else {\r\nflush_dcache_page(page);\r\nret = btt_write_pg(btt, bip, sector, page, off, len);\r\n}\r\nreturn ret;\r\n}\r\nstatic blk_qc_t btt_make_request(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct bio_integrity_payload *bip = bio_integrity(bio);\r\nstruct btt *btt = q->queuedata;\r\nstruct bvec_iter iter;\r\nunsigned long start;\r\nstruct bio_vec bvec;\r\nint err = 0;\r\nbool do_acct;\r\nif (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {\r\nbio->bi_error = -EIO;\r\ngoto out;\r\n}\r\ndo_acct = nd_iostat_start(bio, &start);\r\nbio_for_each_segment(bvec, bio, iter) {\r\nunsigned int len = bvec.bv_len;\r\nBUG_ON(len > PAGE_SIZE);\r\nBUG_ON(len < btt->sector_size);\r\nBUG_ON(len % btt->sector_size);\r\nerr = btt_do_bvec(btt, bip, bvec.bv_page, len, bvec.bv_offset,\r\nop_is_write(bio_op(bio)), iter.bi_sector);\r\nif (err) {\r\ndev_info(&btt->nd_btt->dev,\r\n"io error in %s sector %lld, len %d,\n",\r\n(op_is_write(bio_op(bio))) ? "WRITE" :\r\n"READ",\r\n(unsigned long long) iter.bi_sector, len);\r\nbio->bi_error = err;\r\nbreak;\r\n}\r\n}\r\nif (do_acct)\r\nnd_iostat_end(bio, start);\r\nout:\r\nbio_endio(bio);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nstatic int btt_rw_page(struct block_device *bdev, sector_t sector,\r\nstruct page *page, bool is_write)\r\n{\r\nstruct btt *btt = bdev->bd_disk->private_data;\r\nbtt_do_bvec(btt, NULL, page, PAGE_SIZE, 0, is_write, sector);\r\npage_endio(page, is_write, 0);\r\nreturn 0;\r\n}\r\nstatic int btt_getgeo(struct block_device *bd, struct hd_geometry *geo)\r\n{\r\ngeo->heads = 1 << 6;\r\ngeo->sectors = 1 << 5;\r\ngeo->cylinders = get_capacity(bd->bd_disk) >> 11;\r\nreturn 0;\r\n}\r\nstatic int btt_blk_init(struct btt *btt)\r\n{\r\nstruct nd_btt *nd_btt = btt->nd_btt;\r\nstruct nd_namespace_common *ndns = nd_btt->ndns;\r\nbtt->btt_queue = blk_alloc_queue(GFP_KERNEL);\r\nif (!btt->btt_queue)\r\nreturn -ENOMEM;\r\nbtt->btt_disk = alloc_disk(0);\r\nif (!btt->btt_disk) {\r\nblk_cleanup_queue(btt->btt_queue);\r\nreturn -ENOMEM;\r\n}\r\nnvdimm_namespace_disk_name(ndns, btt->btt_disk->disk_name);\r\nbtt->btt_disk->first_minor = 0;\r\nbtt->btt_disk->fops = &btt_fops;\r\nbtt->btt_disk->private_data = btt;\r\nbtt->btt_disk->queue = btt->btt_queue;\r\nbtt->btt_disk->flags = GENHD_FL_EXT_DEVT;\r\nblk_queue_make_request(btt->btt_queue, btt_make_request);\r\nblk_queue_logical_block_size(btt->btt_queue, btt->sector_size);\r\nblk_queue_max_hw_sectors(btt->btt_queue, UINT_MAX);\r\nblk_queue_bounce_limit(btt->btt_queue, BLK_BOUNCE_ANY);\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, btt->btt_queue);\r\nbtt->btt_queue->queuedata = btt;\r\nset_capacity(btt->btt_disk, 0);\r\ndevice_add_disk(&btt->nd_btt->dev, btt->btt_disk);\r\nif (btt_meta_size(btt)) {\r\nint rc = nd_integrity_init(btt->btt_disk, btt_meta_size(btt));\r\nif (rc) {\r\ndel_gendisk(btt->btt_disk);\r\nput_disk(btt->btt_disk);\r\nblk_cleanup_queue(btt->btt_queue);\r\nreturn rc;\r\n}\r\n}\r\nset_capacity(btt->btt_disk, btt->nlba * btt->sector_size >> 9);\r\nbtt->nd_btt->size = btt->nlba * (u64)btt->sector_size;\r\nrevalidate_disk(btt->btt_disk);\r\nreturn 0;\r\n}\r\nstatic void btt_blk_cleanup(struct btt *btt)\r\n{\r\ndel_gendisk(btt->btt_disk);\r\nput_disk(btt->btt_disk);\r\nblk_cleanup_queue(btt->btt_queue);\r\n}\r\nstatic struct btt *btt_init(struct nd_btt *nd_btt, unsigned long long rawsize,\r\nu32 lbasize, u8 *uuid, struct nd_region *nd_region)\r\n{\r\nint ret;\r\nstruct btt *btt;\r\nstruct device *dev = &nd_btt->dev;\r\nbtt = devm_kzalloc(dev, sizeof(struct btt), GFP_KERNEL);\r\nif (!btt)\r\nreturn NULL;\r\nbtt->nd_btt = nd_btt;\r\nbtt->rawsize = rawsize;\r\nbtt->lbasize = lbasize;\r\nbtt->sector_size = ((lbasize >= 4096) ? 4096 : 512);\r\nINIT_LIST_HEAD(&btt->arena_list);\r\nmutex_init(&btt->init_lock);\r\nbtt->nd_region = nd_region;\r\nret = discover_arenas(btt);\r\nif (ret) {\r\ndev_err(dev, "init: error in arena_discover: %d\n", ret);\r\nreturn NULL;\r\n}\r\nif (btt->init_state != INIT_READY && nd_region->ro) {\r\ndev_info(dev, "%s is read-only, unable to init btt metadata\n",\r\ndev_name(&nd_region->dev));\r\nreturn NULL;\r\n} else if (btt->init_state != INIT_READY) {\r\nbtt->num_arenas = (rawsize / ARENA_MAX_SIZE) +\r\n((rawsize % ARENA_MAX_SIZE) ? 1 : 0);\r\ndev_dbg(dev, "init: %d arenas for %llu rawsize\n",\r\nbtt->num_arenas, rawsize);\r\nret = create_arenas(btt);\r\nif (ret) {\r\ndev_info(dev, "init: create_arenas: %d\n", ret);\r\nreturn NULL;\r\n}\r\nret = btt_meta_init(btt);\r\nif (ret) {\r\ndev_err(dev, "init: error in meta_init: %d\n", ret);\r\nreturn NULL;\r\n}\r\n}\r\nret = btt_blk_init(btt);\r\nif (ret) {\r\ndev_err(dev, "init: error in blk_init: %d\n", ret);\r\nreturn NULL;\r\n}\r\nbtt_debugfs_init(btt);\r\nreturn btt;\r\n}\r\nstatic void btt_fini(struct btt *btt)\r\n{\r\nif (btt) {\r\nbtt_blk_cleanup(btt);\r\nfree_arenas(btt);\r\ndebugfs_remove_recursive(btt->debugfs_dir);\r\n}\r\n}\r\nint nvdimm_namespace_attach_btt(struct nd_namespace_common *ndns)\r\n{\r\nstruct nd_btt *nd_btt = to_nd_btt(ndns->claim);\r\nstruct nd_region *nd_region;\r\nstruct btt *btt;\r\nsize_t rawsize;\r\nif (!nd_btt->uuid || !nd_btt->ndns || !nd_btt->lbasize) {\r\ndev_dbg(&nd_btt->dev, "incomplete btt configuration\n");\r\nreturn -ENODEV;\r\n}\r\nrawsize = nvdimm_namespace_capacity(ndns) - SZ_4K;\r\nif (rawsize < ARENA_MIN_SIZE) {\r\ndev_dbg(&nd_btt->dev, "%s must be at least %ld bytes\n",\r\ndev_name(&ndns->dev), ARENA_MIN_SIZE + SZ_4K);\r\nreturn -ENXIO;\r\n}\r\nnd_region = to_nd_region(nd_btt->dev.parent);\r\nbtt = btt_init(nd_btt, rawsize, nd_btt->lbasize, nd_btt->uuid,\r\nnd_region);\r\nif (!btt)\r\nreturn -ENOMEM;\r\nnd_btt->btt = btt;\r\nreturn 0;\r\n}\r\nint nvdimm_namespace_detach_btt(struct nd_btt *nd_btt)\r\n{\r\nstruct btt *btt = nd_btt->btt;\r\nbtt_fini(btt);\r\nnd_btt->btt = NULL;\r\nreturn 0;\r\n}\r\nstatic int __init nd_btt_init(void)\r\n{\r\nint rc = 0;\r\ndebugfs_root = debugfs_create_dir("btt", NULL);\r\nif (IS_ERR_OR_NULL(debugfs_root))\r\nrc = -ENXIO;\r\nreturn rc;\r\n}\r\nstatic void __exit nd_btt_exit(void)\r\n{\r\ndebugfs_remove_recursive(debugfs_root);\r\n}
