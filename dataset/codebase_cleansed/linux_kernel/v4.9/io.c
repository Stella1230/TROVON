void bch_bbio_free(struct bio *bio, struct cache_set *c)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nmempool_free(b, c->bio_meta);\r\n}\r\nstruct bio *bch_bbio_alloc(struct cache_set *c)\r\n{\r\nstruct bbio *b = mempool_alloc(c->bio_meta, GFP_NOIO);\r\nstruct bio *bio = &b->bio;\r\nbio_init(bio);\r\nbio->bi_max_vecs = bucket_pages(c);\r\nbio->bi_io_vec = bio->bi_inline_vecs;\r\nreturn bio;\r\n}\r\nvoid __bch_submit_bbio(struct bio *bio, struct cache_set *c)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nbio->bi_iter.bi_sector = PTR_OFFSET(&b->key, 0);\r\nbio->bi_bdev = PTR_CACHE(c, &b->key, 0)->bdev;\r\nb->submit_time_us = local_clock_us();\r\nclosure_bio_submit(bio, bio->bi_private);\r\n}\r\nvoid bch_submit_bbio(struct bio *bio, struct cache_set *c,\r\nstruct bkey *k, unsigned ptr)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nbch_bkey_copy_single_ptr(&b->key, k, ptr);\r\n__bch_submit_bbio(bio, c);\r\n}\r\nvoid bch_count_io_errors(struct cache *ca, int error, const char *m)\r\n{\r\nif (ca->set->error_decay) {\r\nunsigned count = atomic_inc_return(&ca->io_count);\r\nwhile (count > ca->set->error_decay) {\r\nunsigned errors;\r\nunsigned old = count;\r\nunsigned new = count - ca->set->error_decay;\r\ncount = atomic_cmpxchg(&ca->io_count, old, new);\r\nif (count == old) {\r\ncount = new;\r\nerrors = atomic_read(&ca->io_errors);\r\ndo {\r\nold = errors;\r\nnew = ((uint64_t) errors * 127) / 128;\r\nerrors = atomic_cmpxchg(&ca->io_errors,\r\nold, new);\r\n} while (old != errors);\r\n}\r\n}\r\n}\r\nif (error) {\r\nchar buf[BDEVNAME_SIZE];\r\nunsigned errors = atomic_add_return(1 << IO_ERROR_SHIFT,\r\n&ca->io_errors);\r\nerrors >>= IO_ERROR_SHIFT;\r\nif (errors < ca->set->error_limit)\r\npr_err("%s: IO error on %s, recovering",\r\nbdevname(ca->bdev, buf), m);\r\nelse\r\nbch_cache_set_error(ca->set,\r\n"%s: too many IO errors %s",\r\nbdevname(ca->bdev, buf), m);\r\n}\r\n}\r\nvoid bch_bbio_count_io_errors(struct cache_set *c, struct bio *bio,\r\nint error, const char *m)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nstruct cache *ca = PTR_CACHE(c, &b->key, 0);\r\nunsigned threshold = op_is_write(bio_op(bio))\r\n? c->congested_write_threshold_us\r\n: c->congested_read_threshold_us;\r\nif (threshold) {\r\nunsigned t = local_clock_us();\r\nint us = t - b->submit_time_us;\r\nint congested = atomic_read(&c->congested);\r\nif (us > (int) threshold) {\r\nint ms = us / 1024;\r\nc->congested_last_us = t;\r\nms = min(ms, CONGESTED_MAX + congested);\r\natomic_sub(ms, &c->congested);\r\n} else if (congested < 0)\r\natomic_inc(&c->congested);\r\n}\r\nbch_count_io_errors(ca, error, m);\r\n}\r\nvoid bch_bbio_endio(struct cache_set *c, struct bio *bio,\r\nint error, const char *m)\r\n{\r\nstruct closure *cl = bio->bi_private;\r\nbch_bbio_count_io_errors(c, bio, error, m);\r\nbio_put(bio);\r\nclosure_put(cl);\r\n}
