static struct v4l2_clk *v4l2_clk_find(const char *dev_id)\r\n{\r\nstruct v4l2_clk *clk;\r\nlist_for_each_entry(clk, &clk_list, list)\r\nif (!strcmp(dev_id, clk->dev_id))\r\nreturn clk;\r\nreturn ERR_PTR(-ENODEV);\r\n}\r\nstruct v4l2_clk *v4l2_clk_get(struct device *dev, const char *id)\r\n{\r\nstruct v4l2_clk *clk;\r\nstruct clk *ccf_clk = clk_get(dev, id);\r\nchar clk_name[V4L2_CLK_NAME_SIZE];\r\nif (PTR_ERR(ccf_clk) == -EPROBE_DEFER)\r\nreturn ERR_PTR(-EPROBE_DEFER);\r\nif (!IS_ERR_OR_NULL(ccf_clk)) {\r\nclk = kzalloc(sizeof(*clk), GFP_KERNEL);\r\nif (!clk) {\r\nclk_put(ccf_clk);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nclk->clk = ccf_clk;\r\nreturn clk;\r\n}\r\nmutex_lock(&clk_lock);\r\nclk = v4l2_clk_find(dev_name(dev));\r\nif (PTR_ERR(clk) == -ENODEV && dev->of_node) {\r\nv4l2_clk_name_of(clk_name, sizeof(clk_name),\r\nof_node_full_name(dev->of_node));\r\nclk = v4l2_clk_find(clk_name);\r\n}\r\nif (!IS_ERR(clk))\r\natomic_inc(&clk->use_count);\r\nmutex_unlock(&clk_lock);\r\nreturn clk;\r\n}\r\nvoid v4l2_clk_put(struct v4l2_clk *clk)\r\n{\r\nstruct v4l2_clk *tmp;\r\nif (IS_ERR(clk))\r\nreturn;\r\nif (clk->clk) {\r\nclk_put(clk->clk);\r\nkfree(clk);\r\nreturn;\r\n}\r\nmutex_lock(&clk_lock);\r\nlist_for_each_entry(tmp, &clk_list, list)\r\nif (tmp == clk)\r\natomic_dec(&clk->use_count);\r\nmutex_unlock(&clk_lock);\r\n}\r\nstatic int v4l2_clk_lock_driver(struct v4l2_clk *clk)\r\n{\r\nstruct v4l2_clk *tmp;\r\nint ret = -ENODEV;\r\nmutex_lock(&clk_lock);\r\nlist_for_each_entry(tmp, &clk_list, list)\r\nif (tmp == clk) {\r\nret = !try_module_get(clk->ops->owner);\r\nif (ret)\r\nret = -EFAULT;\r\nbreak;\r\n}\r\nmutex_unlock(&clk_lock);\r\nreturn ret;\r\n}\r\nstatic void v4l2_clk_unlock_driver(struct v4l2_clk *clk)\r\n{\r\nmodule_put(clk->ops->owner);\r\n}\r\nint v4l2_clk_enable(struct v4l2_clk *clk)\r\n{\r\nint ret;\r\nif (clk->clk)\r\nreturn clk_prepare_enable(clk->clk);\r\nret = v4l2_clk_lock_driver(clk);\r\nif (ret < 0)\r\nreturn ret;\r\nmutex_lock(&clk->lock);\r\nif (++clk->enable == 1 && clk->ops->enable) {\r\nret = clk->ops->enable(clk);\r\nif (ret < 0)\r\nclk->enable--;\r\n}\r\nmutex_unlock(&clk->lock);\r\nreturn ret;\r\n}\r\nvoid v4l2_clk_disable(struct v4l2_clk *clk)\r\n{\r\nint enable;\r\nif (clk->clk)\r\nreturn clk_disable_unprepare(clk->clk);\r\nmutex_lock(&clk->lock);\r\nenable = --clk->enable;\r\nif (WARN(enable < 0, "Unbalanced %s() on %s!\n", __func__,\r\nclk->dev_id))\r\nclk->enable++;\r\nelse if (!enable && clk->ops->disable)\r\nclk->ops->disable(clk);\r\nmutex_unlock(&clk->lock);\r\nv4l2_clk_unlock_driver(clk);\r\n}\r\nunsigned long v4l2_clk_get_rate(struct v4l2_clk *clk)\r\n{\r\nint ret;\r\nif (clk->clk)\r\nreturn clk_get_rate(clk->clk);\r\nret = v4l2_clk_lock_driver(clk);\r\nif (ret < 0)\r\nreturn ret;\r\nmutex_lock(&clk->lock);\r\nif (!clk->ops->get_rate)\r\nret = -ENOSYS;\r\nelse\r\nret = clk->ops->get_rate(clk);\r\nmutex_unlock(&clk->lock);\r\nv4l2_clk_unlock_driver(clk);\r\nreturn ret;\r\n}\r\nint v4l2_clk_set_rate(struct v4l2_clk *clk, unsigned long rate)\r\n{\r\nint ret;\r\nif (clk->clk) {\r\nlong r = clk_round_rate(clk->clk, rate);\r\nif (r < 0)\r\nreturn r;\r\nreturn clk_set_rate(clk->clk, r);\r\n}\r\nret = v4l2_clk_lock_driver(clk);\r\nif (ret < 0)\r\nreturn ret;\r\nmutex_lock(&clk->lock);\r\nif (!clk->ops->set_rate)\r\nret = -ENOSYS;\r\nelse\r\nret = clk->ops->set_rate(clk, rate);\r\nmutex_unlock(&clk->lock);\r\nv4l2_clk_unlock_driver(clk);\r\nreturn ret;\r\n}\r\nstruct v4l2_clk *v4l2_clk_register(const struct v4l2_clk_ops *ops,\r\nconst char *dev_id,\r\nvoid *priv)\r\n{\r\nstruct v4l2_clk *clk;\r\nint ret;\r\nif (!ops || !dev_id)\r\nreturn ERR_PTR(-EINVAL);\r\nclk = kzalloc(sizeof(struct v4l2_clk), GFP_KERNEL);\r\nif (!clk)\r\nreturn ERR_PTR(-ENOMEM);\r\nclk->dev_id = kstrdup(dev_id, GFP_KERNEL);\r\nif (!clk->dev_id) {\r\nret = -ENOMEM;\r\ngoto ealloc;\r\n}\r\nclk->ops = ops;\r\nclk->priv = priv;\r\natomic_set(&clk->use_count, 0);\r\nmutex_init(&clk->lock);\r\nmutex_lock(&clk_lock);\r\nif (!IS_ERR(v4l2_clk_find(dev_id))) {\r\nmutex_unlock(&clk_lock);\r\nret = -EEXIST;\r\ngoto eexist;\r\n}\r\nlist_add_tail(&clk->list, &clk_list);\r\nmutex_unlock(&clk_lock);\r\nreturn clk;\r\neexist:\r\nealloc:\r\nkfree(clk->dev_id);\r\nkfree(clk);\r\nreturn ERR_PTR(ret);\r\n}\r\nvoid v4l2_clk_unregister(struct v4l2_clk *clk)\r\n{\r\nif (WARN(atomic_read(&clk->use_count),\r\n"%s(): Refusing to unregister ref-counted %s clock!\n",\r\n__func__, clk->dev_id))\r\nreturn;\r\nmutex_lock(&clk_lock);\r\nlist_del(&clk->list);\r\nmutex_unlock(&clk_lock);\r\nkfree(clk->dev_id);\r\nkfree(clk);\r\n}\r\nstatic unsigned long fixed_get_rate(struct v4l2_clk *clk)\r\n{\r\nstruct v4l2_clk_fixed *priv = clk->priv;\r\nreturn priv->rate;\r\n}\r\nstruct v4l2_clk *__v4l2_clk_register_fixed(const char *dev_id,\r\nunsigned long rate, struct module *owner)\r\n{\r\nstruct v4l2_clk *clk;\r\nstruct v4l2_clk_fixed *priv = kzalloc(sizeof(*priv), GFP_KERNEL);\r\nif (!priv)\r\nreturn ERR_PTR(-ENOMEM);\r\npriv->rate = rate;\r\npriv->ops.get_rate = fixed_get_rate;\r\npriv->ops.owner = owner;\r\nclk = v4l2_clk_register(&priv->ops, dev_id, priv);\r\nif (IS_ERR(clk))\r\nkfree(priv);\r\nreturn clk;\r\n}\r\nvoid v4l2_clk_unregister_fixed(struct v4l2_clk *clk)\r\n{\r\nkfree(clk->priv);\r\nv4l2_clk_unregister(clk);\r\n}
