static inline void sh_mmcif_bitset(struct sh_mmcif_host *host,\r\nunsigned int reg, u32 val)\r\n{\r\nwritel(val | readl(host->addr + reg), host->addr + reg);\r\n}\r\nstatic inline void sh_mmcif_bitclr(struct sh_mmcif_host *host,\r\nunsigned int reg, u32 val)\r\n{\r\nwritel(~val & readl(host->addr + reg), host->addr + reg);\r\n}\r\nstatic void sh_mmcif_dma_complete(void *arg)\r\n{\r\nstruct sh_mmcif_host *host = arg;\r\nstruct mmc_request *mrq = host->mrq;\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\ndev_dbg(dev, "Command completed\n");\r\nif (WARN(!mrq || !mrq->data, "%s: NULL data in DMA completion!\n",\r\ndev_name(dev)))\r\nreturn;\r\ncomplete(&host->dma_complete);\r\n}\r\nstatic void sh_mmcif_start_dma_rx(struct sh_mmcif_host *host)\r\n{\r\nstruct mmc_data *data = host->mrq->data;\r\nstruct scatterlist *sg = data->sg;\r\nstruct dma_async_tx_descriptor *desc = NULL;\r\nstruct dma_chan *chan = host->chan_rx;\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\ndma_cookie_t cookie = -EINVAL;\r\nint ret;\r\nret = dma_map_sg(chan->device->dev, sg, data->sg_len,\r\nDMA_FROM_DEVICE);\r\nif (ret > 0) {\r\nhost->dma_active = true;\r\ndesc = dmaengine_prep_slave_sg(chan, sg, ret,\r\nDMA_DEV_TO_MEM, DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\n}\r\nif (desc) {\r\ndesc->callback = sh_mmcif_dma_complete;\r\ndesc->callback_param = host;\r\ncookie = dmaengine_submit(desc);\r\nsh_mmcif_bitset(host, MMCIF_CE_BUF_ACC, BUF_ACC_DMAREN);\r\ndma_async_issue_pending(chan);\r\n}\r\ndev_dbg(dev, "%s(): mapped %d -> %d, cookie %d\n",\r\n__func__, data->sg_len, ret, cookie);\r\nif (!desc) {\r\nif (ret >= 0)\r\nret = -EIO;\r\nhost->chan_rx = NULL;\r\nhost->dma_active = false;\r\ndma_release_channel(chan);\r\nchan = host->chan_tx;\r\nif (chan) {\r\nhost->chan_tx = NULL;\r\ndma_release_channel(chan);\r\n}\r\ndev_warn(dev,\r\n"DMA failed: %d, falling back to PIO\n", ret);\r\nsh_mmcif_bitclr(host, MMCIF_CE_BUF_ACC, BUF_ACC_DMAREN | BUF_ACC_DMAWEN);\r\n}\r\ndev_dbg(dev, "%s(): desc %p, cookie %d, sg[%d]\n", __func__,\r\ndesc, cookie, data->sg_len);\r\n}\r\nstatic void sh_mmcif_start_dma_tx(struct sh_mmcif_host *host)\r\n{\r\nstruct mmc_data *data = host->mrq->data;\r\nstruct scatterlist *sg = data->sg;\r\nstruct dma_async_tx_descriptor *desc = NULL;\r\nstruct dma_chan *chan = host->chan_tx;\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\ndma_cookie_t cookie = -EINVAL;\r\nint ret;\r\nret = dma_map_sg(chan->device->dev, sg, data->sg_len,\r\nDMA_TO_DEVICE);\r\nif (ret > 0) {\r\nhost->dma_active = true;\r\ndesc = dmaengine_prep_slave_sg(chan, sg, ret,\r\nDMA_MEM_TO_DEV, DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\n}\r\nif (desc) {\r\ndesc->callback = sh_mmcif_dma_complete;\r\ndesc->callback_param = host;\r\ncookie = dmaengine_submit(desc);\r\nsh_mmcif_bitset(host, MMCIF_CE_BUF_ACC, BUF_ACC_DMAWEN);\r\ndma_async_issue_pending(chan);\r\n}\r\ndev_dbg(dev, "%s(): mapped %d -> %d, cookie %d\n",\r\n__func__, data->sg_len, ret, cookie);\r\nif (!desc) {\r\nif (ret >= 0)\r\nret = -EIO;\r\nhost->chan_tx = NULL;\r\nhost->dma_active = false;\r\ndma_release_channel(chan);\r\nchan = host->chan_rx;\r\nif (chan) {\r\nhost->chan_rx = NULL;\r\ndma_release_channel(chan);\r\n}\r\ndev_warn(dev,\r\n"DMA failed: %d, falling back to PIO\n", ret);\r\nsh_mmcif_bitclr(host, MMCIF_CE_BUF_ACC, BUF_ACC_DMAREN | BUF_ACC_DMAWEN);\r\n}\r\ndev_dbg(dev, "%s(): desc %p, cookie %d\n", __func__,\r\ndesc, cookie);\r\n}\r\nstatic struct dma_chan *\r\nsh_mmcif_request_dma_pdata(struct sh_mmcif_host *host, uintptr_t slave_id)\r\n{\r\ndma_cap_mask_t mask;\r\ndma_cap_zero(mask);\r\ndma_cap_set(DMA_SLAVE, mask);\r\nif (slave_id <= 0)\r\nreturn NULL;\r\nreturn dma_request_channel(mask, shdma_chan_filter, (void *)slave_id);\r\n}\r\nstatic int sh_mmcif_dma_slave_config(struct sh_mmcif_host *host,\r\nstruct dma_chan *chan,\r\nenum dma_transfer_direction direction)\r\n{\r\nstruct resource *res;\r\nstruct dma_slave_config cfg = { 0, };\r\nres = platform_get_resource(host->pd, IORESOURCE_MEM, 0);\r\ncfg.direction = direction;\r\nif (direction == DMA_DEV_TO_MEM) {\r\ncfg.src_addr = res->start + MMCIF_CE_DATA;\r\ncfg.src_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\n} else {\r\ncfg.dst_addr = res->start + MMCIF_CE_DATA;\r\ncfg.dst_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\n}\r\nreturn dmaengine_slave_config(chan, &cfg);\r\n}\r\nstatic void sh_mmcif_request_dma(struct sh_mmcif_host *host)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nhost->dma_active = false;\r\nif (IS_ENABLED(CONFIG_SUPERH) && dev->platform_data) {\r\nstruct sh_mmcif_plat_data *pdata = dev->platform_data;\r\nhost->chan_tx = sh_mmcif_request_dma_pdata(host,\r\npdata->slave_id_tx);\r\nhost->chan_rx = sh_mmcif_request_dma_pdata(host,\r\npdata->slave_id_rx);\r\n} else {\r\nhost->chan_tx = dma_request_slave_channel(dev, "tx");\r\nhost->chan_rx = dma_request_slave_channel(dev, "rx");\r\n}\r\ndev_dbg(dev, "%s: got channel TX %p RX %p\n", __func__, host->chan_tx,\r\nhost->chan_rx);\r\nif (!host->chan_tx || !host->chan_rx ||\r\nsh_mmcif_dma_slave_config(host, host->chan_tx, DMA_MEM_TO_DEV) ||\r\nsh_mmcif_dma_slave_config(host, host->chan_rx, DMA_DEV_TO_MEM))\r\ngoto error;\r\nreturn;\r\nerror:\r\nif (host->chan_tx)\r\ndma_release_channel(host->chan_tx);\r\nif (host->chan_rx)\r\ndma_release_channel(host->chan_rx);\r\nhost->chan_tx = host->chan_rx = NULL;\r\n}\r\nstatic void sh_mmcif_release_dma(struct sh_mmcif_host *host)\r\n{\r\nsh_mmcif_bitclr(host, MMCIF_CE_BUF_ACC, BUF_ACC_DMAREN | BUF_ACC_DMAWEN);\r\nif (host->chan_tx) {\r\nstruct dma_chan *chan = host->chan_tx;\r\nhost->chan_tx = NULL;\r\ndma_release_channel(chan);\r\n}\r\nif (host->chan_rx) {\r\nstruct dma_chan *chan = host->chan_rx;\r\nhost->chan_rx = NULL;\r\ndma_release_channel(chan);\r\n}\r\nhost->dma_active = false;\r\n}\r\nstatic void sh_mmcif_clock_control(struct sh_mmcif_host *host, unsigned int clk)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nstruct sh_mmcif_plat_data *p = dev->platform_data;\r\nbool sup_pclk = p ? p->sup_pclk : false;\r\nunsigned int current_clk = clk_get_rate(host->clk);\r\nunsigned int clkdiv;\r\nsh_mmcif_bitclr(host, MMCIF_CE_CLK_CTRL, CLK_ENABLE);\r\nsh_mmcif_bitclr(host, MMCIF_CE_CLK_CTRL, CLK_CLEAR);\r\nif (!clk)\r\nreturn;\r\nif (host->clkdiv_map) {\r\nunsigned int freq, best_freq, myclk, div, diff_min, diff;\r\nint i;\r\nclkdiv = 0;\r\ndiff_min = ~0;\r\nbest_freq = 0;\r\nfor (i = 31; i >= 0; i--) {\r\nif (!((1 << i) & host->clkdiv_map))\r\ncontinue;\r\ndiv = 1 << (i + 1);\r\nfreq = clk_round_rate(host->clk, clk * div);\r\nmyclk = freq / div;\r\ndiff = (myclk > clk) ? myclk - clk : clk - myclk;\r\nif (diff <= diff_min) {\r\nbest_freq = freq;\r\nclkdiv = i;\r\ndiff_min = diff;\r\n}\r\n}\r\ndev_dbg(dev, "clk %u/%u (%u, 0x%x)\n",\r\n(best_freq / (1 << (clkdiv + 1))), clk,\r\nbest_freq, clkdiv);\r\nclk_set_rate(host->clk, best_freq);\r\nclkdiv = clkdiv << 16;\r\n} else if (sup_pclk && clk == current_clk) {\r\nclkdiv = CLK_SUP_PCLK;\r\n} else {\r\nclkdiv = (fls(DIV_ROUND_UP(current_clk, clk) - 1) - 1) << 16;\r\n}\r\nsh_mmcif_bitset(host, MMCIF_CE_CLK_CTRL, CLK_CLEAR & clkdiv);\r\nsh_mmcif_bitset(host, MMCIF_CE_CLK_CTRL, CLK_ENABLE);\r\n}\r\nstatic void sh_mmcif_sync_reset(struct sh_mmcif_host *host)\r\n{\r\nu32 tmp;\r\ntmp = 0x010f0000 & sh_mmcif_readl(host->addr, MMCIF_CE_CLK_CTRL);\r\nsh_mmcif_writel(host->addr, MMCIF_CE_VERSION, SOFT_RST_ON);\r\nsh_mmcif_writel(host->addr, MMCIF_CE_VERSION, SOFT_RST_OFF);\r\nif (host->ccs_enable)\r\ntmp |= SCCSTO_29;\r\nif (host->clk_ctrl2_enable)\r\nsh_mmcif_writel(host->addr, MMCIF_CE_CLK_CTRL2, 0x0F0F0000);\r\nsh_mmcif_bitset(host, MMCIF_CE_CLK_CTRL, tmp |\r\nSRSPTO_256 | SRBSYTO_29 | SRWDTO_29);\r\nsh_mmcif_bitset(host, MMCIF_CE_BUF_ACC, BUF_ACC_ATYP);\r\n}\r\nstatic int sh_mmcif_error_manage(struct sh_mmcif_host *host)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nu32 state1, state2;\r\nint ret, timeout;\r\nhost->sd_error = false;\r\nstate1 = sh_mmcif_readl(host->addr, MMCIF_CE_HOST_STS1);\r\nstate2 = sh_mmcif_readl(host->addr, MMCIF_CE_HOST_STS2);\r\ndev_dbg(dev, "ERR HOST_STS1 = %08x\n", state1);\r\ndev_dbg(dev, "ERR HOST_STS2 = %08x\n", state2);\r\nif (state1 & STS1_CMDSEQ) {\r\nsh_mmcif_bitset(host, MMCIF_CE_CMD_CTRL, CMD_CTRL_BREAK);\r\nsh_mmcif_bitset(host, MMCIF_CE_CMD_CTRL, ~CMD_CTRL_BREAK);\r\nfor (timeout = 10000; timeout; timeout--) {\r\nif (!(sh_mmcif_readl(host->addr, MMCIF_CE_HOST_STS1)\r\n& STS1_CMDSEQ))\r\nbreak;\r\nmdelay(1);\r\n}\r\nif (!timeout) {\r\ndev_err(dev,\r\n"Forced end of command sequence timeout err\n");\r\nreturn -EIO;\r\n}\r\nsh_mmcif_sync_reset(host);\r\ndev_dbg(dev, "Forced end of command sequence\n");\r\nreturn -EIO;\r\n}\r\nif (state2 & STS2_CRC_ERR) {\r\ndev_err(dev, " CRC error: state %u, wait %u\n",\r\nhost->state, host->wait_for);\r\nret = -EIO;\r\n} else if (state2 & STS2_TIMEOUT_ERR) {\r\ndev_err(dev, " Timeout: state %u, wait %u\n",\r\nhost->state, host->wait_for);\r\nret = -ETIMEDOUT;\r\n} else {\r\ndev_dbg(dev, " End/Index error: state %u, wait %u\n",\r\nhost->state, host->wait_for);\r\nret = -EIO;\r\n}\r\nreturn ret;\r\n}\r\nstatic bool sh_mmcif_next_block(struct sh_mmcif_host *host, u32 *p)\r\n{\r\nstruct mmc_data *data = host->mrq->data;\r\nhost->sg_blkidx += host->blocksize;\r\nBUG_ON(host->sg_blkidx > data->sg->length);\r\nif (host->sg_blkidx == data->sg->length) {\r\nhost->sg_blkidx = 0;\r\nif (++host->sg_idx < data->sg_len)\r\nhost->pio_ptr = sg_virt(++data->sg);\r\n} else {\r\nhost->pio_ptr = p;\r\n}\r\nreturn host->sg_idx != data->sg_len;\r\n}\r\nstatic void sh_mmcif_single_read(struct sh_mmcif_host *host,\r\nstruct mmc_request *mrq)\r\n{\r\nhost->blocksize = (sh_mmcif_readl(host->addr, MMCIF_CE_BLOCK_SET) &\r\nBLOCK_SIZE_MASK) + 3;\r\nhost->wait_for = MMCIF_WAIT_FOR_READ;\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MBUFREN);\r\n}\r\nstatic bool sh_mmcif_read_block(struct sh_mmcif_host *host)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nstruct mmc_data *data = host->mrq->data;\r\nu32 *p = sg_virt(data->sg);\r\nint i;\r\nif (host->sd_error) {\r\ndata->error = sh_mmcif_error_manage(host);\r\ndev_dbg(dev, "%s(): %d\n", __func__, data->error);\r\nreturn false;\r\n}\r\nfor (i = 0; i < host->blocksize / 4; i++)\r\n*p++ = sh_mmcif_readl(host->addr, MMCIF_CE_DATA);\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MBUFRE);\r\nhost->wait_for = MMCIF_WAIT_FOR_READ_END;\r\nreturn true;\r\n}\r\nstatic void sh_mmcif_multi_read(struct sh_mmcif_host *host,\r\nstruct mmc_request *mrq)\r\n{\r\nstruct mmc_data *data = mrq->data;\r\nif (!data->sg_len || !data->sg->length)\r\nreturn;\r\nhost->blocksize = sh_mmcif_readl(host->addr, MMCIF_CE_BLOCK_SET) &\r\nBLOCK_SIZE_MASK;\r\nhost->wait_for = MMCIF_WAIT_FOR_MREAD;\r\nhost->sg_idx = 0;\r\nhost->sg_blkidx = 0;\r\nhost->pio_ptr = sg_virt(data->sg);\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MBUFREN);\r\n}\r\nstatic bool sh_mmcif_mread_block(struct sh_mmcif_host *host)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nstruct mmc_data *data = host->mrq->data;\r\nu32 *p = host->pio_ptr;\r\nint i;\r\nif (host->sd_error) {\r\ndata->error = sh_mmcif_error_manage(host);\r\ndev_dbg(dev, "%s(): %d\n", __func__, data->error);\r\nreturn false;\r\n}\r\nBUG_ON(!data->sg->length);\r\nfor (i = 0; i < host->blocksize / 4; i++)\r\n*p++ = sh_mmcif_readl(host->addr, MMCIF_CE_DATA);\r\nif (!sh_mmcif_next_block(host, p))\r\nreturn false;\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MBUFREN);\r\nreturn true;\r\n}\r\nstatic void sh_mmcif_single_write(struct sh_mmcif_host *host,\r\nstruct mmc_request *mrq)\r\n{\r\nhost->blocksize = (sh_mmcif_readl(host->addr, MMCIF_CE_BLOCK_SET) &\r\nBLOCK_SIZE_MASK) + 3;\r\nhost->wait_for = MMCIF_WAIT_FOR_WRITE;\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MBUFWEN);\r\n}\r\nstatic bool sh_mmcif_write_block(struct sh_mmcif_host *host)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nstruct mmc_data *data = host->mrq->data;\r\nu32 *p = sg_virt(data->sg);\r\nint i;\r\nif (host->sd_error) {\r\ndata->error = sh_mmcif_error_manage(host);\r\ndev_dbg(dev, "%s(): %d\n", __func__, data->error);\r\nreturn false;\r\n}\r\nfor (i = 0; i < host->blocksize / 4; i++)\r\nsh_mmcif_writel(host->addr, MMCIF_CE_DATA, *p++);\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MDTRANE);\r\nhost->wait_for = MMCIF_WAIT_FOR_WRITE_END;\r\nreturn true;\r\n}\r\nstatic void sh_mmcif_multi_write(struct sh_mmcif_host *host,\r\nstruct mmc_request *mrq)\r\n{\r\nstruct mmc_data *data = mrq->data;\r\nif (!data->sg_len || !data->sg->length)\r\nreturn;\r\nhost->blocksize = sh_mmcif_readl(host->addr, MMCIF_CE_BLOCK_SET) &\r\nBLOCK_SIZE_MASK;\r\nhost->wait_for = MMCIF_WAIT_FOR_MWRITE;\r\nhost->sg_idx = 0;\r\nhost->sg_blkidx = 0;\r\nhost->pio_ptr = sg_virt(data->sg);\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MBUFWEN);\r\n}\r\nstatic bool sh_mmcif_mwrite_block(struct sh_mmcif_host *host)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nstruct mmc_data *data = host->mrq->data;\r\nu32 *p = host->pio_ptr;\r\nint i;\r\nif (host->sd_error) {\r\ndata->error = sh_mmcif_error_manage(host);\r\ndev_dbg(dev, "%s(): %d\n", __func__, data->error);\r\nreturn false;\r\n}\r\nBUG_ON(!data->sg->length);\r\nfor (i = 0; i < host->blocksize / 4; i++)\r\nsh_mmcif_writel(host->addr, MMCIF_CE_DATA, *p++);\r\nif (!sh_mmcif_next_block(host, p))\r\nreturn false;\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MBUFWEN);\r\nreturn true;\r\n}\r\nstatic void sh_mmcif_get_response(struct sh_mmcif_host *host,\r\nstruct mmc_command *cmd)\r\n{\r\nif (cmd->flags & MMC_RSP_136) {\r\ncmd->resp[0] = sh_mmcif_readl(host->addr, MMCIF_CE_RESP3);\r\ncmd->resp[1] = sh_mmcif_readl(host->addr, MMCIF_CE_RESP2);\r\ncmd->resp[2] = sh_mmcif_readl(host->addr, MMCIF_CE_RESP1);\r\ncmd->resp[3] = sh_mmcif_readl(host->addr, MMCIF_CE_RESP0);\r\n} else\r\ncmd->resp[0] = sh_mmcif_readl(host->addr, MMCIF_CE_RESP0);\r\n}\r\nstatic void sh_mmcif_get_cmd12response(struct sh_mmcif_host *host,\r\nstruct mmc_command *cmd)\r\n{\r\ncmd->resp[0] = sh_mmcif_readl(host->addr, MMCIF_CE_RESP_CMD12);\r\n}\r\nstatic u32 sh_mmcif_set_cmd(struct sh_mmcif_host *host,\r\nstruct mmc_request *mrq)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nstruct mmc_data *data = mrq->data;\r\nstruct mmc_command *cmd = mrq->cmd;\r\nu32 opc = cmd->opcode;\r\nu32 tmp = 0;\r\nswitch (mmc_resp_type(cmd)) {\r\ncase MMC_RSP_NONE:\r\ntmp |= CMD_SET_RTYP_NO;\r\nbreak;\r\ncase MMC_RSP_R1:\r\ncase MMC_RSP_R3:\r\ntmp |= CMD_SET_RTYP_6B;\r\nbreak;\r\ncase MMC_RSP_R1B:\r\ntmp |= CMD_SET_RBSY | CMD_SET_RTYP_6B;\r\nbreak;\r\ncase MMC_RSP_R2:\r\ntmp |= CMD_SET_RTYP_17B;\r\nbreak;\r\ndefault:\r\ndev_err(dev, "Unsupported response type.\n");\r\nbreak;\r\n}\r\nif (data) {\r\ntmp |= CMD_SET_WDAT;\r\nswitch (host->bus_width) {\r\ncase MMC_BUS_WIDTH_1:\r\ntmp |= CMD_SET_DATW_1;\r\nbreak;\r\ncase MMC_BUS_WIDTH_4:\r\ntmp |= CMD_SET_DATW_4;\r\nbreak;\r\ncase MMC_BUS_WIDTH_8:\r\ntmp |= CMD_SET_DATW_8;\r\nbreak;\r\ndefault:\r\ndev_err(dev, "Unsupported bus width.\n");\r\nbreak;\r\n}\r\nswitch (host->timing) {\r\ncase MMC_TIMING_MMC_DDR52:\r\ntmp |= CMD_SET_DARS;\r\nbreak;\r\n}\r\n}\r\nif (opc == MMC_WRITE_BLOCK || opc == MMC_WRITE_MULTIPLE_BLOCK)\r\ntmp |= CMD_SET_DWEN;\r\nif (opc == MMC_READ_MULTIPLE_BLOCK || opc == MMC_WRITE_MULTIPLE_BLOCK) {\r\ntmp |= CMD_SET_CMLTE | CMD_SET_CMD12EN;\r\nsh_mmcif_bitset(host, MMCIF_CE_BLOCK_SET,\r\ndata->blocks << 16);\r\n}\r\nif (opc == MMC_SEND_OP_COND || opc == MMC_ALL_SEND_CID ||\r\nopc == MMC_SEND_CSD || opc == MMC_SEND_CID)\r\ntmp |= CMD_SET_RIDXC_BITS;\r\nif (opc == MMC_SEND_OP_COND)\r\ntmp |= CMD_SET_CRC7C_BITS;\r\nif (opc == MMC_ALL_SEND_CID ||\r\nopc == MMC_SEND_CSD || opc == MMC_SEND_CID)\r\ntmp |= CMD_SET_CRC7C_INTERNAL;\r\nreturn (opc << 24) | tmp;\r\n}\r\nstatic int sh_mmcif_data_trans(struct sh_mmcif_host *host,\r\nstruct mmc_request *mrq, u32 opc)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nswitch (opc) {\r\ncase MMC_READ_MULTIPLE_BLOCK:\r\nsh_mmcif_multi_read(host, mrq);\r\nreturn 0;\r\ncase MMC_WRITE_MULTIPLE_BLOCK:\r\nsh_mmcif_multi_write(host, mrq);\r\nreturn 0;\r\ncase MMC_WRITE_BLOCK:\r\nsh_mmcif_single_write(host, mrq);\r\nreturn 0;\r\ncase MMC_READ_SINGLE_BLOCK:\r\ncase MMC_SEND_EXT_CSD:\r\nsh_mmcif_single_read(host, mrq);\r\nreturn 0;\r\ndefault:\r\ndev_err(dev, "Unsupported CMD%d\n", opc);\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic void sh_mmcif_start_cmd(struct sh_mmcif_host *host,\r\nstruct mmc_request *mrq)\r\n{\r\nstruct mmc_command *cmd = mrq->cmd;\r\nu32 opc = cmd->opcode;\r\nu32 mask = 0;\r\nunsigned long flags;\r\nif (cmd->flags & MMC_RSP_BUSY)\r\nmask = MASK_START_CMD | MASK_MRBSYE;\r\nelse\r\nmask = MASK_START_CMD | MASK_MCRSPE;\r\nif (host->ccs_enable)\r\nmask |= MASK_MCCSTO;\r\nif (mrq->data) {\r\nsh_mmcif_writel(host->addr, MMCIF_CE_BLOCK_SET, 0);\r\nsh_mmcif_writel(host->addr, MMCIF_CE_BLOCK_SET,\r\nmrq->data->blksz);\r\n}\r\nopc = sh_mmcif_set_cmd(host, mrq);\r\nif (host->ccs_enable)\r\nsh_mmcif_writel(host->addr, MMCIF_CE_INT, 0xD80430C0);\r\nelse\r\nsh_mmcif_writel(host->addr, MMCIF_CE_INT, 0xD80430C0 | INT_CCS);\r\nsh_mmcif_writel(host->addr, MMCIF_CE_INT_MASK, mask);\r\nsh_mmcif_writel(host->addr, MMCIF_CE_ARG, cmd->arg);\r\nspin_lock_irqsave(&host->lock, flags);\r\nsh_mmcif_writel(host->addr, MMCIF_CE_CMD_SET, opc);\r\nhost->wait_for = MMCIF_WAIT_FOR_CMD;\r\nschedule_delayed_work(&host->timeout_work, host->timeout);\r\nspin_unlock_irqrestore(&host->lock, flags);\r\n}\r\nstatic void sh_mmcif_stop_cmd(struct sh_mmcif_host *host,\r\nstruct mmc_request *mrq)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nswitch (mrq->cmd->opcode) {\r\ncase MMC_READ_MULTIPLE_BLOCK:\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MCMD12DRE);\r\nbreak;\r\ncase MMC_WRITE_MULTIPLE_BLOCK:\r\nsh_mmcif_bitset(host, MMCIF_CE_INT_MASK, MASK_MCMD12RBE);\r\nbreak;\r\ndefault:\r\ndev_err(dev, "unsupported stop cmd\n");\r\nmrq->stop->error = sh_mmcif_error_manage(host);\r\nreturn;\r\n}\r\nhost->wait_for = MMCIF_WAIT_FOR_STOP;\r\n}\r\nstatic void sh_mmcif_request(struct mmc_host *mmc, struct mmc_request *mrq)\r\n{\r\nstruct sh_mmcif_host *host = mmc_priv(mmc);\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nunsigned long flags;\r\nspin_lock_irqsave(&host->lock, flags);\r\nif (host->state != STATE_IDLE) {\r\ndev_dbg(dev, "%s() rejected, state %u\n",\r\n__func__, host->state);\r\nspin_unlock_irqrestore(&host->lock, flags);\r\nmrq->cmd->error = -EAGAIN;\r\nmmc_request_done(mmc, mrq);\r\nreturn;\r\n}\r\nhost->state = STATE_REQUEST;\r\nspin_unlock_irqrestore(&host->lock, flags);\r\nhost->mrq = mrq;\r\nsh_mmcif_start_cmd(host, mrq);\r\n}\r\nstatic void sh_mmcif_clk_setup(struct sh_mmcif_host *host)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nif (host->mmc->f_max) {\r\nunsigned int f_max, f_min = 0, f_min_old;\r\nf_max = host->mmc->f_max;\r\nfor (f_min_old = f_max; f_min_old > 2;) {\r\nf_min = clk_round_rate(host->clk, f_min_old / 2);\r\nif (f_min == f_min_old)\r\nbreak;\r\nf_min_old = f_min;\r\n}\r\nhost->clkdiv_map = 0x3ff;\r\nhost->mmc->f_max = f_max / (1 << ffs(host->clkdiv_map));\r\nhost->mmc->f_min = f_min / (1 << fls(host->clkdiv_map));\r\n} else {\r\nunsigned int clk = clk_get_rate(host->clk);\r\nhost->mmc->f_max = clk / 2;\r\nhost->mmc->f_min = clk / 512;\r\n}\r\ndev_dbg(dev, "clk max/min = %d/%d\n",\r\nhost->mmc->f_max, host->mmc->f_min);\r\n}\r\nstatic void sh_mmcif_set_ios(struct mmc_host *mmc, struct mmc_ios *ios)\r\n{\r\nstruct sh_mmcif_host *host = mmc_priv(mmc);\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nunsigned long flags;\r\nspin_lock_irqsave(&host->lock, flags);\r\nif (host->state != STATE_IDLE) {\r\ndev_dbg(dev, "%s() rejected, state %u\n",\r\n__func__, host->state);\r\nspin_unlock_irqrestore(&host->lock, flags);\r\nreturn;\r\n}\r\nhost->state = STATE_IOS;\r\nspin_unlock_irqrestore(&host->lock, flags);\r\nswitch (ios->power_mode) {\r\ncase MMC_POWER_UP:\r\nif (!IS_ERR(mmc->supply.vmmc))\r\nmmc_regulator_set_ocr(mmc, mmc->supply.vmmc, ios->vdd);\r\nif (!host->power) {\r\nclk_prepare_enable(host->clk);\r\npm_runtime_get_sync(dev);\r\nsh_mmcif_sync_reset(host);\r\nsh_mmcif_request_dma(host);\r\nhost->power = true;\r\n}\r\nbreak;\r\ncase MMC_POWER_OFF:\r\nif (!IS_ERR(mmc->supply.vmmc))\r\nmmc_regulator_set_ocr(mmc, mmc->supply.vmmc, 0);\r\nif (host->power) {\r\nsh_mmcif_clock_control(host, 0);\r\nsh_mmcif_release_dma(host);\r\npm_runtime_put(dev);\r\nclk_disable_unprepare(host->clk);\r\nhost->power = false;\r\n}\r\nbreak;\r\ncase MMC_POWER_ON:\r\nsh_mmcif_clock_control(host, ios->clock);\r\nbreak;\r\n}\r\nhost->timing = ios->timing;\r\nhost->bus_width = ios->bus_width;\r\nhost->state = STATE_IDLE;\r\n}\r\nstatic int sh_mmcif_get_cd(struct mmc_host *mmc)\r\n{\r\nstruct sh_mmcif_host *host = mmc_priv(mmc);\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nstruct sh_mmcif_plat_data *p = dev->platform_data;\r\nint ret = mmc_gpio_get_cd(mmc);\r\nif (ret >= 0)\r\nreturn ret;\r\nif (!p || !p->get_cd)\r\nreturn -ENOSYS;\r\nelse\r\nreturn p->get_cd(host->pd);\r\n}\r\nstatic bool sh_mmcif_end_cmd(struct sh_mmcif_host *host)\r\n{\r\nstruct mmc_command *cmd = host->mrq->cmd;\r\nstruct mmc_data *data = host->mrq->data;\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nlong time;\r\nif (host->sd_error) {\r\nswitch (cmd->opcode) {\r\ncase MMC_ALL_SEND_CID:\r\ncase MMC_SELECT_CARD:\r\ncase MMC_APP_CMD:\r\ncmd->error = -ETIMEDOUT;\r\nbreak;\r\ndefault:\r\ncmd->error = sh_mmcif_error_manage(host);\r\nbreak;\r\n}\r\ndev_dbg(dev, "CMD%d error %d\n",\r\ncmd->opcode, cmd->error);\r\nhost->sd_error = false;\r\nreturn false;\r\n}\r\nif (!(cmd->flags & MMC_RSP_PRESENT)) {\r\ncmd->error = 0;\r\nreturn false;\r\n}\r\nsh_mmcif_get_response(host, cmd);\r\nif (!data)\r\nreturn false;\r\ninit_completion(&host->dma_complete);\r\nif (data->flags & MMC_DATA_READ) {\r\nif (host->chan_rx)\r\nsh_mmcif_start_dma_rx(host);\r\n} else {\r\nif (host->chan_tx)\r\nsh_mmcif_start_dma_tx(host);\r\n}\r\nif (!host->dma_active) {\r\ndata->error = sh_mmcif_data_trans(host, host->mrq, cmd->opcode);\r\nreturn !data->error;\r\n}\r\ntime = wait_for_completion_interruptible_timeout(&host->dma_complete,\r\nhost->timeout);\r\nif (data->flags & MMC_DATA_READ)\r\ndma_unmap_sg(host->chan_rx->device->dev,\r\ndata->sg, data->sg_len,\r\nDMA_FROM_DEVICE);\r\nelse\r\ndma_unmap_sg(host->chan_tx->device->dev,\r\ndata->sg, data->sg_len,\r\nDMA_TO_DEVICE);\r\nif (host->sd_error) {\r\ndev_err(host->mmc->parent,\r\n"Error IRQ while waiting for DMA completion!\n");\r\ndata->error = sh_mmcif_error_manage(host);\r\n} else if (!time) {\r\ndev_err(host->mmc->parent, "DMA timeout!\n");\r\ndata->error = -ETIMEDOUT;\r\n} else if (time < 0) {\r\ndev_err(host->mmc->parent,\r\n"wait_for_completion_...() error %ld!\n", time);\r\ndata->error = time;\r\n}\r\nsh_mmcif_bitclr(host, MMCIF_CE_BUF_ACC,\r\nBUF_ACC_DMAREN | BUF_ACC_DMAWEN);\r\nhost->dma_active = false;\r\nif (data->error) {\r\ndata->bytes_xfered = 0;\r\nif (data->flags & MMC_DATA_READ)\r\ndmaengine_terminate_all(host->chan_rx);\r\nelse\r\ndmaengine_terminate_all(host->chan_tx);\r\n}\r\nreturn false;\r\n}\r\nstatic irqreturn_t sh_mmcif_irqt(int irq, void *dev_id)\r\n{\r\nstruct sh_mmcif_host *host = dev_id;\r\nstruct mmc_request *mrq;\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nbool wait = false;\r\nunsigned long flags;\r\nint wait_work;\r\nspin_lock_irqsave(&host->lock, flags);\r\nwait_work = host->wait_for;\r\nspin_unlock_irqrestore(&host->lock, flags);\r\ncancel_delayed_work_sync(&host->timeout_work);\r\nmutex_lock(&host->thread_lock);\r\nmrq = host->mrq;\r\nif (!mrq) {\r\ndev_dbg(dev, "IRQ thread state %u, wait %u: NULL mrq!\n",\r\nhost->state, host->wait_for);\r\nmutex_unlock(&host->thread_lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nswitch (wait_work) {\r\ncase MMCIF_WAIT_FOR_REQUEST:\r\nmutex_unlock(&host->thread_lock);\r\nreturn IRQ_HANDLED;\r\ncase MMCIF_WAIT_FOR_CMD:\r\nwait = sh_mmcif_end_cmd(host);\r\nbreak;\r\ncase MMCIF_WAIT_FOR_MREAD:\r\nwait = sh_mmcif_mread_block(host);\r\nbreak;\r\ncase MMCIF_WAIT_FOR_READ:\r\nwait = sh_mmcif_read_block(host);\r\nbreak;\r\ncase MMCIF_WAIT_FOR_MWRITE:\r\nwait = sh_mmcif_mwrite_block(host);\r\nbreak;\r\ncase MMCIF_WAIT_FOR_WRITE:\r\nwait = sh_mmcif_write_block(host);\r\nbreak;\r\ncase MMCIF_WAIT_FOR_STOP:\r\nif (host->sd_error) {\r\nmrq->stop->error = sh_mmcif_error_manage(host);\r\ndev_dbg(dev, "%s(): %d\n", __func__, mrq->stop->error);\r\nbreak;\r\n}\r\nsh_mmcif_get_cmd12response(host, mrq->stop);\r\nmrq->stop->error = 0;\r\nbreak;\r\ncase MMCIF_WAIT_FOR_READ_END:\r\ncase MMCIF_WAIT_FOR_WRITE_END:\r\nif (host->sd_error) {\r\nmrq->data->error = sh_mmcif_error_manage(host);\r\ndev_dbg(dev, "%s(): %d\n", __func__, mrq->data->error);\r\n}\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nif (wait) {\r\nschedule_delayed_work(&host->timeout_work, host->timeout);\r\nmutex_unlock(&host->thread_lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nif (host->wait_for != MMCIF_WAIT_FOR_STOP) {\r\nstruct mmc_data *data = mrq->data;\r\nif (!mrq->cmd->error && data && !data->error)\r\ndata->bytes_xfered =\r\ndata->blocks * data->blksz;\r\nif (mrq->stop && !mrq->cmd->error && (!data || !data->error)) {\r\nsh_mmcif_stop_cmd(host, mrq);\r\nif (!mrq->stop->error) {\r\nschedule_delayed_work(&host->timeout_work, host->timeout);\r\nmutex_unlock(&host->thread_lock);\r\nreturn IRQ_HANDLED;\r\n}\r\n}\r\n}\r\nhost->wait_for = MMCIF_WAIT_FOR_REQUEST;\r\nhost->state = STATE_IDLE;\r\nhost->mrq = NULL;\r\nmmc_request_done(host->mmc, mrq);\r\nmutex_unlock(&host->thread_lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t sh_mmcif_intr(int irq, void *dev_id)\r\n{\r\nstruct sh_mmcif_host *host = dev_id;\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nu32 state, mask;\r\nstate = sh_mmcif_readl(host->addr, MMCIF_CE_INT);\r\nmask = sh_mmcif_readl(host->addr, MMCIF_CE_INT_MASK);\r\nif (host->ccs_enable)\r\nsh_mmcif_writel(host->addr, MMCIF_CE_INT, ~(state & mask));\r\nelse\r\nsh_mmcif_writel(host->addr, MMCIF_CE_INT, INT_CCS | ~(state & mask));\r\nsh_mmcif_bitclr(host, MMCIF_CE_INT_MASK, state & MASK_CLEAN);\r\nif (state & ~MASK_CLEAN)\r\ndev_dbg(dev, "IRQ state = 0x%08x incompletely cleared\n",\r\nstate);\r\nif (state & INT_ERR_STS || state & ~INT_ALL) {\r\nhost->sd_error = true;\r\ndev_dbg(dev, "int err state = 0x%08x\n", state);\r\n}\r\nif (state & ~(INT_CMD12RBE | INT_CMD12CRE)) {\r\nif (!host->mrq)\r\ndev_dbg(dev, "NULL IRQ state = 0x%08x\n", state);\r\nif (!host->dma_active)\r\nreturn IRQ_WAKE_THREAD;\r\nelse if (host->sd_error)\r\nsh_mmcif_dma_complete(host);\r\n} else {\r\ndev_dbg(dev, "Unexpected IRQ 0x%x\n", state);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void sh_mmcif_timeout_work(struct work_struct *work)\r\n{\r\nstruct delayed_work *d = to_delayed_work(work);\r\nstruct sh_mmcif_host *host = container_of(d, struct sh_mmcif_host, timeout_work);\r\nstruct mmc_request *mrq = host->mrq;\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nunsigned long flags;\r\nif (host->dying)\r\nreturn;\r\nspin_lock_irqsave(&host->lock, flags);\r\nif (host->state == STATE_IDLE) {\r\nspin_unlock_irqrestore(&host->lock, flags);\r\nreturn;\r\n}\r\ndev_err(dev, "Timeout waiting for %u on CMD%u\n",\r\nhost->wait_for, mrq->cmd->opcode);\r\nhost->state = STATE_TIMEOUT;\r\nspin_unlock_irqrestore(&host->lock, flags);\r\nswitch (host->wait_for) {\r\ncase MMCIF_WAIT_FOR_CMD:\r\nmrq->cmd->error = sh_mmcif_error_manage(host);\r\nbreak;\r\ncase MMCIF_WAIT_FOR_STOP:\r\nmrq->stop->error = sh_mmcif_error_manage(host);\r\nbreak;\r\ncase MMCIF_WAIT_FOR_MREAD:\r\ncase MMCIF_WAIT_FOR_MWRITE:\r\ncase MMCIF_WAIT_FOR_READ:\r\ncase MMCIF_WAIT_FOR_WRITE:\r\ncase MMCIF_WAIT_FOR_READ_END:\r\ncase MMCIF_WAIT_FOR_WRITE_END:\r\nmrq->data->error = sh_mmcif_error_manage(host);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nhost->state = STATE_IDLE;\r\nhost->wait_for = MMCIF_WAIT_FOR_REQUEST;\r\nhost->mrq = NULL;\r\nmmc_request_done(host->mmc, mrq);\r\n}\r\nstatic void sh_mmcif_init_ocr(struct sh_mmcif_host *host)\r\n{\r\nstruct device *dev = sh_mmcif_host_to_dev(host);\r\nstruct sh_mmcif_plat_data *pd = dev->platform_data;\r\nstruct mmc_host *mmc = host->mmc;\r\nmmc_regulator_get_supply(mmc);\r\nif (!pd)\r\nreturn;\r\nif (!mmc->ocr_avail)\r\nmmc->ocr_avail = pd->ocr;\r\nelse if (pd->ocr)\r\ndev_warn(mmc_dev(mmc), "Platform OCR mask is ignored\n");\r\n}\r\nstatic int sh_mmcif_probe(struct platform_device *pdev)\r\n{\r\nint ret = 0, irq[2];\r\nstruct mmc_host *mmc;\r\nstruct sh_mmcif_host *host;\r\nstruct device *dev = &pdev->dev;\r\nstruct sh_mmcif_plat_data *pd = dev->platform_data;\r\nstruct resource *res;\r\nvoid __iomem *reg;\r\nconst char *name;\r\nirq[0] = platform_get_irq(pdev, 0);\r\nirq[1] = platform_get_irq(pdev, 1);\r\nif (irq[0] < 0) {\r\ndev_err(dev, "Get irq error\n");\r\nreturn -ENXIO;\r\n}\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nreg = devm_ioremap_resource(dev, res);\r\nif (IS_ERR(reg))\r\nreturn PTR_ERR(reg);\r\nmmc = mmc_alloc_host(sizeof(struct sh_mmcif_host), dev);\r\nif (!mmc)\r\nreturn -ENOMEM;\r\nret = mmc_of_parse(mmc);\r\nif (ret < 0)\r\ngoto err_host;\r\nhost = mmc_priv(mmc);\r\nhost->mmc = mmc;\r\nhost->addr = reg;\r\nhost->timeout = msecs_to_jiffies(10000);\r\nhost->ccs_enable = !pd || !pd->ccs_unsupported;\r\nhost->clk_ctrl2_enable = pd && pd->clk_ctrl2_present;\r\nhost->pd = pdev;\r\nspin_lock_init(&host->lock);\r\nmmc->ops = &sh_mmcif_ops;\r\nsh_mmcif_init_ocr(host);\r\nmmc->caps |= MMC_CAP_MMC_HIGHSPEED | MMC_CAP_WAIT_WHILE_BUSY;\r\nmmc->caps2 |= MMC_CAP2_NO_SD | MMC_CAP2_NO_SDIO;\r\nmmc->max_busy_timeout = 10000;\r\nif (pd && pd->caps)\r\nmmc->caps |= pd->caps;\r\nmmc->max_segs = 32;\r\nmmc->max_blk_size = 512;\r\nmmc->max_req_size = PAGE_SIZE * mmc->max_segs;\r\nmmc->max_blk_count = mmc->max_req_size / mmc->max_blk_size;\r\nmmc->max_seg_size = mmc->max_req_size;\r\nplatform_set_drvdata(pdev, host);\r\nhost->clk = devm_clk_get(dev, NULL);\r\nif (IS_ERR(host->clk)) {\r\nret = PTR_ERR(host->clk);\r\ndev_err(dev, "cannot get clock: %d\n", ret);\r\ngoto err_host;\r\n}\r\nret = clk_prepare_enable(host->clk);\r\nif (ret < 0)\r\ngoto err_host;\r\nsh_mmcif_clk_setup(host);\r\npm_runtime_enable(dev);\r\nhost->power = false;\r\nret = pm_runtime_get_sync(dev);\r\nif (ret < 0)\r\ngoto err_clk;\r\nINIT_DELAYED_WORK(&host->timeout_work, sh_mmcif_timeout_work);\r\nsh_mmcif_sync_reset(host);\r\nsh_mmcif_writel(host->addr, MMCIF_CE_INT_MASK, MASK_ALL);\r\nname = irq[1] < 0 ? dev_name(dev) : "sh_mmc:error";\r\nret = devm_request_threaded_irq(dev, irq[0], sh_mmcif_intr,\r\nsh_mmcif_irqt, 0, name, host);\r\nif (ret) {\r\ndev_err(dev, "request_irq error (%s)\n", name);\r\ngoto err_clk;\r\n}\r\nif (irq[1] >= 0) {\r\nret = devm_request_threaded_irq(dev, irq[1],\r\nsh_mmcif_intr, sh_mmcif_irqt,\r\n0, "sh_mmc:int", host);\r\nif (ret) {\r\ndev_err(dev, "request_irq error (sh_mmc:int)\n");\r\ngoto err_clk;\r\n}\r\n}\r\nif (pd && pd->use_cd_gpio) {\r\nret = mmc_gpio_request_cd(mmc, pd->cd_gpio, 0);\r\nif (ret < 0)\r\ngoto err_clk;\r\n}\r\nmutex_init(&host->thread_lock);\r\nret = mmc_add_host(mmc);\r\nif (ret < 0)\r\ngoto err_clk;\r\ndev_pm_qos_expose_latency_limit(dev, 100);\r\ndev_info(dev, "Chip version 0x%04x, clock rate %luMHz\n",\r\nsh_mmcif_readl(host->addr, MMCIF_CE_VERSION) & 0xffff,\r\nclk_get_rate(host->clk) / 1000000UL);\r\npm_runtime_put(dev);\r\nclk_disable_unprepare(host->clk);\r\nreturn ret;\r\nerr_clk:\r\nclk_disable_unprepare(host->clk);\r\npm_runtime_put_sync(dev);\r\npm_runtime_disable(dev);\r\nerr_host:\r\nmmc_free_host(mmc);\r\nreturn ret;\r\n}\r\nstatic int sh_mmcif_remove(struct platform_device *pdev)\r\n{\r\nstruct sh_mmcif_host *host = platform_get_drvdata(pdev);\r\nhost->dying = true;\r\nclk_prepare_enable(host->clk);\r\npm_runtime_get_sync(&pdev->dev);\r\ndev_pm_qos_hide_latency_limit(&pdev->dev);\r\nmmc_remove_host(host->mmc);\r\nsh_mmcif_writel(host->addr, MMCIF_CE_INT_MASK, MASK_ALL);\r\ncancel_delayed_work_sync(&host->timeout_work);\r\nclk_disable_unprepare(host->clk);\r\nmmc_free_host(host->mmc);\r\npm_runtime_put_sync(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nreturn 0;\r\n}\r\nstatic int sh_mmcif_suspend(struct device *dev)\r\n{\r\nstruct sh_mmcif_host *host = dev_get_drvdata(dev);\r\npm_runtime_get_sync(dev);\r\nsh_mmcif_writel(host->addr, MMCIF_CE_INT_MASK, MASK_ALL);\r\npm_runtime_put(dev);\r\nreturn 0;\r\n}\r\nstatic int sh_mmcif_resume(struct device *dev)\r\n{\r\nreturn 0;\r\n}
