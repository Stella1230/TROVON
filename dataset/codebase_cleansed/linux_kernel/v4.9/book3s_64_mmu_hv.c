long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)\r\n{\r\nunsigned long hpt = 0;\r\nstruct revmap_entry *rev;\r\nstruct page *page = NULL;\r\nlong order = KVM_DEFAULT_HPT_ORDER;\r\nif (htab_orderp) {\r\norder = *htab_orderp;\r\nif (order < PPC_MIN_HPT_ORDER)\r\norder = PPC_MIN_HPT_ORDER;\r\n}\r\nkvm->arch.hpt_cma_alloc = 0;\r\npage = kvm_alloc_hpt(1ul << (order - PAGE_SHIFT));\r\nif (page) {\r\nhpt = (unsigned long)pfn_to_kaddr(page_to_pfn(page));\r\nmemset((void *)hpt, 0, (1ul << order));\r\nkvm->arch.hpt_cma_alloc = 1;\r\n}\r\nwhile (!hpt && order > PPC_MIN_HPT_ORDER && !htab_orderp) {\r\nhpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT|\r\n__GFP_NOWARN, order - PAGE_SHIFT);\r\nif (!hpt)\r\n--order;\r\n}\r\nif (!hpt)\r\nreturn -ENOMEM;\r\nkvm->arch.hpt_virt = hpt;\r\nkvm->arch.hpt_order = order;\r\nkvm->arch.hpt_npte = 1ul << (order - 4);\r\nkvm->arch.hpt_mask = (1ul << (order - 7)) - 1;\r\nrev = vmalloc(sizeof(struct revmap_entry) * kvm->arch.hpt_npte);\r\nif (!rev) {\r\npr_err("kvmppc_alloc_hpt: Couldn't alloc reverse map array\n");\r\ngoto out_freehpt;\r\n}\r\nkvm->arch.revmap = rev;\r\nkvm->arch.sdr1 = __pa(hpt) | (order - 18);\r\npr_info("KVM guest htab at %lx (order %ld), LPID %x\n",\r\nhpt, order, kvm->arch.lpid);\r\nif (htab_orderp)\r\n*htab_orderp = order;\r\nreturn 0;\r\nout_freehpt:\r\nif (kvm->arch.hpt_cma_alloc)\r\nkvm_release_hpt(page, 1 << (order - PAGE_SHIFT));\r\nelse\r\nfree_pages(hpt, order - PAGE_SHIFT);\r\nreturn -ENOMEM;\r\n}\r\nlong kvmppc_alloc_reset_hpt(struct kvm *kvm, u32 *htab_orderp)\r\n{\r\nlong err = -EBUSY;\r\nlong order;\r\nmutex_lock(&kvm->lock);\r\nif (kvm->arch.hpte_setup_done) {\r\nkvm->arch.hpte_setup_done = 0;\r\nsmp_mb();\r\nif (atomic_read(&kvm->arch.vcpus_running)) {\r\nkvm->arch.hpte_setup_done = 1;\r\ngoto out;\r\n}\r\n}\r\nif (kvm->arch.hpt_virt) {\r\norder = kvm->arch.hpt_order;\r\nmemset((void *)kvm->arch.hpt_virt, 0, 1ul << order);\r\nkvmppc_rmap_reset(kvm);\r\ncpumask_setall(&kvm->arch.need_tlb_flush);\r\n*htab_orderp = order;\r\nerr = 0;\r\n} else {\r\nerr = kvmppc_alloc_hpt(kvm, htab_orderp);\r\norder = *htab_orderp;\r\n}\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn err;\r\n}\r\nvoid kvmppc_free_hpt(struct kvm *kvm)\r\n{\r\nkvmppc_free_lpid(kvm->arch.lpid);\r\nvfree(kvm->arch.revmap);\r\nif (kvm->arch.hpt_cma_alloc)\r\nkvm_release_hpt(virt_to_page(kvm->arch.hpt_virt),\r\n1 << (kvm->arch.hpt_order - PAGE_SHIFT));\r\nelse\r\nfree_pages(kvm->arch.hpt_virt,\r\nkvm->arch.hpt_order - PAGE_SHIFT);\r\n}\r\nstatic inline unsigned long hpte0_pgsize_encoding(unsigned long pgsize)\r\n{\r\nreturn (pgsize > 0x1000) ? HPTE_V_LARGE : 0;\r\n}\r\nstatic inline unsigned long hpte1_pgsize_encoding(unsigned long pgsize)\r\n{\r\nreturn (pgsize == 0x10000) ? 0x1000 : 0;\r\n}\r\nvoid kvmppc_map_vrma(struct kvm_vcpu *vcpu, struct kvm_memory_slot *memslot,\r\nunsigned long porder)\r\n{\r\nunsigned long i;\r\nunsigned long npages;\r\nunsigned long hp_v, hp_r;\r\nunsigned long addr, hash;\r\nunsigned long psize;\r\nunsigned long hp0, hp1;\r\nunsigned long idx_ret;\r\nlong ret;\r\nstruct kvm *kvm = vcpu->kvm;\r\npsize = 1ul << porder;\r\nnpages = memslot->npages >> (porder - PAGE_SHIFT);\r\nif (npages > 1ul << (40 - porder))\r\nnpages = 1ul << (40 - porder);\r\nif (npages > kvm->arch.hpt_mask + 1)\r\nnpages = kvm->arch.hpt_mask + 1;\r\nhp0 = HPTE_V_1TB_SEG | (VRMA_VSID << (40 - 16)) |\r\nHPTE_V_BOLTED | hpte0_pgsize_encoding(psize);\r\nhp1 = hpte1_pgsize_encoding(psize) |\r\nHPTE_R_R | HPTE_R_C | HPTE_R_M | PP_RWXX;\r\nfor (i = 0; i < npages; ++i) {\r\naddr = i << porder;\r\nhash = (i ^ (VRMA_VSID ^ (VRMA_VSID << 25))) & kvm->arch.hpt_mask;\r\nhash = (hash << 3) + 7;\r\nhp_v = hp0 | ((addr >> 16) & ~0x7fUL);\r\nhp_r = hp1 | addr;\r\nret = kvmppc_virtmode_do_h_enter(kvm, H_EXACT, hash, hp_v, hp_r,\r\n&idx_ret);\r\nif (ret != H_SUCCESS) {\r\npr_err("KVM: map_vrma at %lx failed, ret=%ld\n",\r\naddr, ret);\r\nbreak;\r\n}\r\n}\r\n}\r\nint kvmppc_mmu_hv_init(void)\r\n{\r\nunsigned long host_lpid, rsvd_lpid;\r\nif (!cpu_has_feature(CPU_FTR_HVMODE))\r\nreturn -EINVAL;\r\nhost_lpid = mfspr(SPRN_LPID);\r\nrsvd_lpid = LPID_RSVD;\r\nkvmppc_init_lpid(rsvd_lpid + 1);\r\nkvmppc_claim_lpid(host_lpid);\r\nkvmppc_claim_lpid(rsvd_lpid);\r\nreturn 0;\r\n}\r\nstatic void kvmppc_mmu_book3s_64_hv_reset_msr(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long msr = vcpu->arch.intr_msr;\r\nif (MSR_TM_TRANSACTIONAL(vcpu->arch.shregs.msr))\r\nmsr |= MSR_TS_S;\r\nelse\r\nmsr |= vcpu->arch.shregs.msr & MSR_TS_MASK;\r\nkvmppc_set_msr(vcpu, msr);\r\n}\r\nlong kvmppc_virtmode_do_h_enter(struct kvm *kvm, unsigned long flags,\r\nlong pte_index, unsigned long pteh,\r\nunsigned long ptel, unsigned long *pte_idx_ret)\r\n{\r\nlong ret;\r\nrcu_read_lock_sched();\r\nret = kvmppc_do_h_enter(kvm, flags, pte_index, pteh, ptel,\r\ncurrent->mm->pgd, false, pte_idx_ret);\r\nrcu_read_unlock_sched();\r\nif (ret == H_TOO_HARD) {\r\npr_err("KVM: Oops, kvmppc_h_enter returned too hard!\n");\r\nret = H_RESOURCE;\r\n}\r\nreturn ret;\r\n}\r\nstatic struct kvmppc_slb *kvmppc_mmu_book3s_hv_find_slbe(struct kvm_vcpu *vcpu,\r\ngva_t eaddr)\r\n{\r\nu64 mask;\r\nint i;\r\nfor (i = 0; i < vcpu->arch.slb_nr; i++) {\r\nif (!(vcpu->arch.slb[i].orige & SLB_ESID_V))\r\ncontinue;\r\nif (vcpu->arch.slb[i].origv & SLB_VSID_B_1T)\r\nmask = ESID_MASK_1T;\r\nelse\r\nmask = ESID_MASK;\r\nif (((vcpu->arch.slb[i].orige ^ eaddr) & mask) == 0)\r\nreturn &vcpu->arch.slb[i];\r\n}\r\nreturn NULL;\r\n}\r\nstatic unsigned long kvmppc_mmu_get_real_addr(unsigned long v, unsigned long r,\r\nunsigned long ea)\r\n{\r\nunsigned long ra_mask;\r\nra_mask = hpte_page_size(v, r) - 1;\r\nreturn (r & HPTE_R_RPN & ~ra_mask) | (ea & ra_mask);\r\n}\r\nstatic int kvmppc_mmu_book3s_64_hv_xlate(struct kvm_vcpu *vcpu, gva_t eaddr,\r\nstruct kvmppc_pte *gpte, bool data, bool iswrite)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct kvmppc_slb *slbe;\r\nunsigned long slb_v;\r\nunsigned long pp, key;\r\nunsigned long v, gr;\r\n__be64 *hptep;\r\nint index;\r\nint virtmode = vcpu->arch.shregs.msr & (data ? MSR_DR : MSR_IR);\r\nif (virtmode) {\r\nslbe = kvmppc_mmu_book3s_hv_find_slbe(vcpu, eaddr);\r\nif (!slbe)\r\nreturn -EINVAL;\r\nslb_v = slbe->origv;\r\n} else {\r\nslb_v = vcpu->kvm->arch.vrma_slb_v;\r\n}\r\npreempt_disable();\r\nindex = kvmppc_hv_find_lock_hpte(kvm, eaddr, slb_v,\r\nHPTE_V_VALID | HPTE_V_ABSENT);\r\nif (index < 0) {\r\npreempt_enable();\r\nreturn -ENOENT;\r\n}\r\nhptep = (__be64 *)(kvm->arch.hpt_virt + (index << 4));\r\nv = be64_to_cpu(hptep[0]) & ~HPTE_V_HVLOCK;\r\ngr = kvm->arch.revmap[index].guest_rpte;\r\nunlock_hpte(hptep, v);\r\npreempt_enable();\r\ngpte->eaddr = eaddr;\r\ngpte->vpage = ((v & HPTE_V_AVPN) << 4) | ((eaddr >> 12) & 0xfff);\r\npp = gr & (HPTE_R_PP0 | HPTE_R_PP);\r\nkey = (vcpu->arch.shregs.msr & MSR_PR) ? SLB_VSID_KP : SLB_VSID_KS;\r\nkey &= slb_v;\r\ngpte->may_read = hpte_read_permission(pp, key);\r\ngpte->may_write = hpte_write_permission(pp, key);\r\ngpte->may_execute = gpte->may_read && !(gr & (HPTE_R_N | HPTE_R_G));\r\nif (data && virtmode) {\r\nint amrfield = hpte_get_skey_perm(gr, vcpu->arch.amr);\r\nif (amrfield & 1)\r\ngpte->may_read = 0;\r\nif (amrfield & 2)\r\ngpte->may_write = 0;\r\n}\r\ngpte->raddr = kvmppc_mmu_get_real_addr(v, gr, eaddr);\r\nreturn 0;\r\n}\r\nstatic int instruction_is_store(unsigned int instr)\r\n{\r\nunsigned int mask;\r\nmask = 0x10000000;\r\nif ((instr & 0xfc000000) == 0x7c000000)\r\nmask = 0x100;\r\nreturn (instr & mask) != 0;\r\n}\r\nstatic int kvmppc_hv_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned long gpa, gva_t ea, int is_store)\r\n{\r\nu32 last_inst;\r\nif (kvmppc_get_last_inst(vcpu, INST_GENERIC, &last_inst) !=\r\nEMULATE_DONE)\r\nreturn RESUME_GUEST;\r\nif (instruction_is_store(last_inst) != !!is_store)\r\nreturn RESUME_GUEST;\r\nvcpu->arch.paddr_accessed = gpa;\r\nvcpu->arch.vaddr_accessed = ea;\r\nreturn kvmppc_emulate_mmio(run, vcpu);\r\n}\r\nint kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned long ea, unsigned long dsisr)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nunsigned long hpte[3], r;\r\n__be64 *hptep;\r\nunsigned long mmu_seq, psize, pte_size;\r\nunsigned long gpa_base, gfn_base;\r\nunsigned long gpa, gfn, hva, pfn;\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long *rmap;\r\nstruct revmap_entry *rev;\r\nstruct page *page, *pages[1];\r\nlong index, ret, npages;\r\nbool is_ci;\r\nunsigned int writing, write_ok;\r\nstruct vm_area_struct *vma;\r\nunsigned long rcbits;\r\nif (ea != vcpu->arch.pgfault_addr)\r\nreturn RESUME_GUEST;\r\nindex = vcpu->arch.pgfault_index;\r\nhptep = (__be64 *)(kvm->arch.hpt_virt + (index << 4));\r\nrev = &kvm->arch.revmap[index];\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptep, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nhpte[0] = be64_to_cpu(hptep[0]) & ~HPTE_V_HVLOCK;\r\nhpte[1] = be64_to_cpu(hptep[1]);\r\nhpte[2] = r = rev->guest_rpte;\r\nunlock_hpte(hptep, hpte[0]);\r\npreempt_enable();\r\nif (hpte[0] != vcpu->arch.pgfault_hpte[0] ||\r\nhpte[1] != vcpu->arch.pgfault_hpte[1])\r\nreturn RESUME_GUEST;\r\npsize = hpte_page_size(hpte[0], r);\r\ngpa_base = r & HPTE_R_RPN & ~(psize - 1);\r\ngfn_base = gpa_base >> PAGE_SHIFT;\r\ngpa = gpa_base | (ea & (psize - 1));\r\ngfn = gpa >> PAGE_SHIFT;\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\ntrace_kvm_page_fault_enter(vcpu, hpte, memslot, ea, dsisr);\r\nif (!memslot || (memslot->flags & KVM_MEMSLOT_INVALID))\r\nreturn kvmppc_hv_emulate_mmio(run, vcpu, gpa, ea,\r\ndsisr & DSISR_ISSTORE);\r\nif (gfn_base < memslot->base_gfn)\r\nreturn -EFAULT;\r\nmmu_seq = kvm->mmu_notifier_seq;\r\nsmp_rmb();\r\nret = -EFAULT;\r\nis_ci = false;\r\npfn = 0;\r\npage = NULL;\r\npte_size = PAGE_SIZE;\r\nwriting = (dsisr & DSISR_ISSTORE) != 0;\r\nwrite_ok = writing;\r\nhva = gfn_to_hva_memslot(memslot, gfn);\r\nnpages = get_user_pages_fast(hva, 1, writing, pages);\r\nif (npages < 1) {\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma(current->mm, hva);\r\nif (vma && vma->vm_start <= hva && hva + psize <= vma->vm_end &&\r\n(vma->vm_flags & VM_PFNMAP)) {\r\npfn = vma->vm_pgoff +\r\n((hva - vma->vm_start) >> PAGE_SHIFT);\r\npte_size = psize;\r\nis_ci = pte_ci(__pte((pgprot_val(vma->vm_page_prot))));\r\nwrite_ok = vma->vm_flags & VM_WRITE;\r\n}\r\nup_read(&current->mm->mmap_sem);\r\nif (!pfn)\r\ngoto out_put;\r\n} else {\r\npage = pages[0];\r\npfn = page_to_pfn(page);\r\nif (PageHuge(page)) {\r\npage = compound_head(page);\r\npte_size <<= compound_order(page);\r\n}\r\nif (!writing && hpte_is_writable(r)) {\r\npte_t *ptep, pte;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nptep = find_linux_pte_or_hugepte(current->mm->pgd,\r\nhva, NULL, NULL);\r\nif (ptep) {\r\npte = kvmppc_read_update_linux_pte(ptep, 1);\r\nif (pte_write(pte))\r\nwrite_ok = 1;\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\n}\r\nif (psize > pte_size)\r\ngoto out_put;\r\nif (!hpte_cache_flags_ok(r, is_ci)) {\r\nif (is_ci)\r\ngoto out_put;\r\nr = (r & ~(HPTE_R_W|HPTE_R_I|HPTE_R_G)) | HPTE_R_M;\r\n}\r\nif (psize < PAGE_SIZE)\r\npsize = PAGE_SIZE;\r\nr = (r & ~(HPTE_R_PP0 - psize)) | ((pfn << PAGE_SHIFT) & ~(psize - 1));\r\nif (hpte_is_writable(r) && !write_ok)\r\nr = hpte_make_readonly(r);\r\nret = RESUME_GUEST;\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptep, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nif ((be64_to_cpu(hptep[0]) & ~HPTE_V_HVLOCK) != hpte[0] ||\r\nbe64_to_cpu(hptep[1]) != hpte[1] ||\r\nrev->guest_rpte != hpte[2])\r\ngoto out_unlock;\r\nhpte[0] = (hpte[0] & ~HPTE_V_ABSENT) | HPTE_V_VALID;\r\nrmap = &memslot->arch.rmap[gfn_base - memslot->base_gfn];\r\nlock_rmap(rmap);\r\nret = RESUME_GUEST;\r\nif (mmu_notifier_retry(vcpu->kvm, mmu_seq)) {\r\nunlock_rmap(rmap);\r\ngoto out_unlock;\r\n}\r\nrcbits = *rmap >> KVMPPC_RMAP_RC_SHIFT;\r\nr &= rcbits | ~(HPTE_R_R | HPTE_R_C);\r\nif (be64_to_cpu(hptep[0]) & HPTE_V_VALID) {\r\nunlock_rmap(rmap);\r\nhptep[0] |= cpu_to_be64(HPTE_V_ABSENT);\r\nkvmppc_invalidate_hpte(kvm, hptep, index);\r\nr |= be64_to_cpu(hptep[1]) & (HPTE_R_R | HPTE_R_C);\r\n} else {\r\nkvmppc_add_revmap_chain(kvm, rev, rmap, index, 0);\r\n}\r\nhptep[1] = cpu_to_be64(r);\r\neieio();\r\n__unlock_hpte(hptep, hpte[0]);\r\nasm volatile("ptesync" : : : "memory");\r\npreempt_enable();\r\nif (page && hpte_is_writable(r))\r\nSetPageDirty(page);\r\nout_put:\r\ntrace_kvm_page_fault_exit(vcpu, hpte, ret);\r\nif (page) {\r\nput_page(pages[0]);\r\n}\r\nreturn ret;\r\nout_unlock:\r\n__unlock_hpte(hptep, be64_to_cpu(hptep[0]));\r\npreempt_enable();\r\ngoto out_put;\r\n}\r\nstatic void kvmppc_rmap_reset(struct kvm *kvm)\r\n{\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nint srcu_idx;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\nslots = kvm_memslots(kvm);\r\nkvm_for_each_memslot(memslot, slots) {\r\nmemset(memslot->arch.rmap, 0,\r\nmemslot->npages * sizeof(*memslot->arch.rmap));\r\n}\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\n}\r\nstatic int kvm_handle_hva_range(struct kvm *kvm,\r\nunsigned long start,\r\nunsigned long end,\r\nint (*handler)(struct kvm *kvm,\r\nunsigned long *rmapp,\r\nunsigned long gfn))\r\n{\r\nint ret;\r\nint retval = 0;\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nslots = kvm_memslots(kvm);\r\nkvm_for_each_memslot(memslot, slots) {\r\nunsigned long hva_start, hva_end;\r\ngfn_t gfn, gfn_end;\r\nhva_start = max(start, memslot->userspace_addr);\r\nhva_end = min(end, memslot->userspace_addr +\r\n(memslot->npages << PAGE_SHIFT));\r\nif (hva_start >= hva_end)\r\ncontinue;\r\ngfn = hva_to_gfn_memslot(hva_start, memslot);\r\ngfn_end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, memslot);\r\nfor (; gfn < gfn_end; ++gfn) {\r\ngfn_t gfn_offset = gfn - memslot->base_gfn;\r\nret = handler(kvm, &memslot->arch.rmap[gfn_offset], gfn);\r\nretval |= ret;\r\n}\r\n}\r\nreturn retval;\r\n}\r\nstatic int kvm_handle_hva(struct kvm *kvm, unsigned long hva,\r\nint (*handler)(struct kvm *kvm, unsigned long *rmapp,\r\nunsigned long gfn))\r\n{\r\nreturn kvm_handle_hva_range(kvm, hva, hva + 1, handler);\r\n}\r\nstatic int kvm_unmap_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nunsigned long gfn)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.revmap;\r\nunsigned long h, i, j;\r\n__be64 *hptep;\r\nunsigned long ptel, psize, rcbits;\r\nfor (;;) {\r\nlock_rmap(rmapp);\r\nif (!(*rmapp & KVMPPC_RMAP_PRESENT)) {\r\nunlock_rmap(rmapp);\r\nbreak;\r\n}\r\ni = *rmapp & KVMPPC_RMAP_INDEX;\r\nhptep = (__be64 *) (kvm->arch.hpt_virt + (i << 4));\r\nif (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {\r\nunlock_rmap(rmapp);\r\nwhile (be64_to_cpu(hptep[0]) & HPTE_V_HVLOCK)\r\ncpu_relax();\r\ncontinue;\r\n}\r\nj = rev[i].forw;\r\nif (j == i) {\r\n*rmapp &= ~(KVMPPC_RMAP_PRESENT | KVMPPC_RMAP_INDEX);\r\n} else {\r\nh = rev[i].back;\r\nrev[h].forw = j;\r\nrev[j].back = h;\r\nrev[i].forw = rev[i].back = i;\r\n*rmapp = (*rmapp & ~KVMPPC_RMAP_INDEX) | j;\r\n}\r\nptel = rev[i].guest_rpte;\r\npsize = hpte_page_size(be64_to_cpu(hptep[0]), ptel);\r\nif ((be64_to_cpu(hptep[0]) & HPTE_V_VALID) &&\r\nhpte_rpn(ptel, psize) == gfn) {\r\nhptep[0] |= cpu_to_be64(HPTE_V_ABSENT);\r\nkvmppc_invalidate_hpte(kvm, hptep, i);\r\nrcbits = be64_to_cpu(hptep[1]) & (HPTE_R_R | HPTE_R_C);\r\n*rmapp |= rcbits << KVMPPC_RMAP_RC_SHIFT;\r\nif (rcbits & HPTE_R_C)\r\nkvmppc_update_rmap_change(rmapp, psize);\r\nif (rcbits & ~rev[i].guest_rpte) {\r\nrev[i].guest_rpte = ptel | rcbits;\r\nnote_hpte_modification(kvm, &rev[i]);\r\n}\r\n}\r\nunlock_rmap(rmapp);\r\n__unlock_hpte(hptep, be64_to_cpu(hptep[0]));\r\n}\r\nreturn 0;\r\n}\r\nint kvm_unmap_hva_hv(struct kvm *kvm, unsigned long hva)\r\n{\r\nkvm_handle_hva(kvm, hva, kvm_unmap_rmapp);\r\nreturn 0;\r\n}\r\nint kvm_unmap_hva_range_hv(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nkvm_handle_hva_range(kvm, start, end, kvm_unmap_rmapp);\r\nreturn 0;\r\n}\r\nvoid kvmppc_core_flush_memslot_hv(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot)\r\n{\r\nunsigned long *rmapp;\r\nunsigned long gfn;\r\nunsigned long n;\r\nrmapp = memslot->arch.rmap;\r\ngfn = memslot->base_gfn;\r\nfor (n = memslot->npages; n; --n) {\r\nif (*rmapp & KVMPPC_RMAP_PRESENT)\r\nkvm_unmap_rmapp(kvm, rmapp, gfn);\r\n++rmapp;\r\n++gfn;\r\n}\r\n}\r\nstatic int kvm_age_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nunsigned long gfn)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.revmap;\r\nunsigned long head, i, j;\r\n__be64 *hptep;\r\nint ret = 0;\r\nretry:\r\nlock_rmap(rmapp);\r\nif (*rmapp & KVMPPC_RMAP_REFERENCED) {\r\n*rmapp &= ~KVMPPC_RMAP_REFERENCED;\r\nret = 1;\r\n}\r\nif (!(*rmapp & KVMPPC_RMAP_PRESENT)) {\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\ni = head = *rmapp & KVMPPC_RMAP_INDEX;\r\ndo {\r\nhptep = (__be64 *) (kvm->arch.hpt_virt + (i << 4));\r\nj = rev[i].forw;\r\nif (!(be64_to_cpu(hptep[1]) & HPTE_R_R))\r\ncontinue;\r\nif (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {\r\nunlock_rmap(rmapp);\r\nwhile (be64_to_cpu(hptep[0]) & HPTE_V_HVLOCK)\r\ncpu_relax();\r\ngoto retry;\r\n}\r\nif ((be64_to_cpu(hptep[0]) & HPTE_V_VALID) &&\r\n(be64_to_cpu(hptep[1]) & HPTE_R_R)) {\r\nkvmppc_clear_ref_hpte(kvm, hptep, i);\r\nif (!(rev[i].guest_rpte & HPTE_R_R)) {\r\nrev[i].guest_rpte |= HPTE_R_R;\r\nnote_hpte_modification(kvm, &rev[i]);\r\n}\r\nret = 1;\r\n}\r\n__unlock_hpte(hptep, be64_to_cpu(hptep[0]));\r\n} while ((i = j) != head);\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\nint kvm_age_hva_hv(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nreturn kvm_handle_hva_range(kvm, start, end, kvm_age_rmapp);\r\n}\r\nstatic int kvm_test_age_rmapp(struct kvm *kvm, unsigned long *rmapp,\r\nunsigned long gfn)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.revmap;\r\nunsigned long head, i, j;\r\nunsigned long *hp;\r\nint ret = 1;\r\nif (*rmapp & KVMPPC_RMAP_REFERENCED)\r\nreturn 1;\r\nlock_rmap(rmapp);\r\nif (*rmapp & KVMPPC_RMAP_REFERENCED)\r\ngoto out;\r\nif (*rmapp & KVMPPC_RMAP_PRESENT) {\r\ni = head = *rmapp & KVMPPC_RMAP_INDEX;\r\ndo {\r\nhp = (unsigned long *)(kvm->arch.hpt_virt + (i << 4));\r\nj = rev[i].forw;\r\nif (be64_to_cpu(hp[1]) & HPTE_R_R)\r\ngoto out;\r\n} while ((i = j) != head);\r\n}\r\nret = 0;\r\nout:\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\nint kvm_test_age_hva_hv(struct kvm *kvm, unsigned long hva)\r\n{\r\nreturn kvm_handle_hva(kvm, hva, kvm_test_age_rmapp);\r\n}\r\nvoid kvm_set_spte_hva_hv(struct kvm *kvm, unsigned long hva, pte_t pte)\r\n{\r\nkvm_handle_hva(kvm, hva, kvm_unmap_rmapp);\r\n}\r\nstatic int vcpus_running(struct kvm *kvm)\r\n{\r\nreturn atomic_read(&kvm->arch.vcpus_running) != 0;\r\n}\r\nstatic int kvm_test_clear_dirty_npages(struct kvm *kvm, unsigned long *rmapp)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.revmap;\r\nunsigned long head, i, j;\r\nunsigned long n;\r\nunsigned long v, r;\r\n__be64 *hptep;\r\nint npages_dirty = 0;\r\nretry:\r\nlock_rmap(rmapp);\r\nif (*rmapp & KVMPPC_RMAP_CHANGED) {\r\nlong change_order = (*rmapp & KVMPPC_RMAP_CHG_ORDER)\r\n>> KVMPPC_RMAP_CHG_SHIFT;\r\n*rmapp &= ~(KVMPPC_RMAP_CHANGED | KVMPPC_RMAP_CHG_ORDER);\r\nnpages_dirty = 1;\r\nif (change_order > PAGE_SHIFT)\r\nnpages_dirty = 1ul << (change_order - PAGE_SHIFT);\r\n}\r\nif (!(*rmapp & KVMPPC_RMAP_PRESENT)) {\r\nunlock_rmap(rmapp);\r\nreturn npages_dirty;\r\n}\r\ni = head = *rmapp & KVMPPC_RMAP_INDEX;\r\ndo {\r\nunsigned long hptep1;\r\nhptep = (__be64 *) (kvm->arch.hpt_virt + (i << 4));\r\nj = rev[i].forw;\r\nhptep1 = be64_to_cpu(hptep[1]);\r\nif (!(hptep1 & HPTE_R_C) &&\r\n(!hpte_is_writable(hptep1) || vcpus_running(kvm)))\r\ncontinue;\r\nif (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {\r\nunlock_rmap(rmapp);\r\nwhile (hptep[0] & cpu_to_be64(HPTE_V_HVLOCK))\r\ncpu_relax();\r\ngoto retry;\r\n}\r\nif (!(hptep[0] & cpu_to_be64(HPTE_V_VALID))) {\r\n__unlock_hpte(hptep, be64_to_cpu(hptep[0]));\r\ncontinue;\r\n}\r\nhptep[0] |= cpu_to_be64(HPTE_V_ABSENT);\r\nkvmppc_invalidate_hpte(kvm, hptep, i);\r\nv = be64_to_cpu(hptep[0]);\r\nr = be64_to_cpu(hptep[1]);\r\nif (r & HPTE_R_C) {\r\nhptep[1] = cpu_to_be64(r & ~HPTE_R_C);\r\nif (!(rev[i].guest_rpte & HPTE_R_C)) {\r\nrev[i].guest_rpte |= HPTE_R_C;\r\nnote_hpte_modification(kvm, &rev[i]);\r\n}\r\nn = hpte_page_size(v, r);\r\nn = (n + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nif (n > npages_dirty)\r\nnpages_dirty = n;\r\neieio();\r\n}\r\nv &= ~HPTE_V_ABSENT;\r\nv |= HPTE_V_VALID;\r\n__unlock_hpte(hptep, v);\r\n} while ((i = j) != head);\r\nunlock_rmap(rmapp);\r\nreturn npages_dirty;\r\n}\r\nstatic void harvest_vpa_dirty(struct kvmppc_vpa *vpa,\r\nstruct kvm_memory_slot *memslot,\r\nunsigned long *map)\r\n{\r\nunsigned long gfn;\r\nif (!vpa->dirty || !vpa->pinned_addr)\r\nreturn;\r\ngfn = vpa->gpa >> PAGE_SHIFT;\r\nif (gfn < memslot->base_gfn ||\r\ngfn >= memslot->base_gfn + memslot->npages)\r\nreturn;\r\nvpa->dirty = false;\r\nif (map)\r\n__set_bit_le(gfn - memslot->base_gfn, map);\r\n}\r\nlong kvmppc_hv_get_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot,\r\nunsigned long *map)\r\n{\r\nunsigned long i, j;\r\nunsigned long *rmapp;\r\nstruct kvm_vcpu *vcpu;\r\npreempt_disable();\r\nrmapp = memslot->arch.rmap;\r\nfor (i = 0; i < memslot->npages; ++i) {\r\nint npages = kvm_test_clear_dirty_npages(kvm, rmapp);\r\nif (npages && map)\r\nfor (j = i; npages; ++j, --npages)\r\n__set_bit_le(j, map);\r\n++rmapp;\r\n}\r\nkvm_for_each_vcpu(i, vcpu, kvm) {\r\nspin_lock(&vcpu->arch.vpa_update_lock);\r\nharvest_vpa_dirty(&vcpu->arch.vpa, memslot, map);\r\nharvest_vpa_dirty(&vcpu->arch.dtl, memslot, map);\r\nspin_unlock(&vcpu->arch.vpa_update_lock);\r\n}\r\npreempt_enable();\r\nreturn 0;\r\n}\r\nvoid *kvmppc_pin_guest_page(struct kvm *kvm, unsigned long gpa,\r\nunsigned long *nb_ret)\r\n{\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long gfn = gpa >> PAGE_SHIFT;\r\nstruct page *page, *pages[1];\r\nint npages;\r\nunsigned long hva, offset;\r\nint srcu_idx;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\nif (!memslot || (memslot->flags & KVM_MEMSLOT_INVALID))\r\ngoto err;\r\nhva = gfn_to_hva_memslot(memslot, gfn);\r\nnpages = get_user_pages_fast(hva, 1, 1, pages);\r\nif (npages < 1)\r\ngoto err;\r\npage = pages[0];\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\noffset = gpa & (PAGE_SIZE - 1);\r\nif (nb_ret)\r\n*nb_ret = PAGE_SIZE - offset;\r\nreturn page_address(page) + offset;\r\nerr:\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\nreturn NULL;\r\n}\r\nvoid kvmppc_unpin_guest_page(struct kvm *kvm, void *va, unsigned long gpa,\r\nbool dirty)\r\n{\r\nstruct page *page = virt_to_page(va);\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long gfn;\r\nunsigned long *rmap;\r\nint srcu_idx;\r\nput_page(page);\r\nif (!dirty)\r\nreturn;\r\ngfn = gpa >> PAGE_SHIFT;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\nif (memslot) {\r\nrmap = &memslot->arch.rmap[gfn - memslot->base_gfn];\r\nlock_rmap(rmap);\r\n*rmap |= KVMPPC_RMAP_CHANGED;\r\nunlock_rmap(rmap);\r\n}\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\n}\r\nstatic int hpte_dirty(struct revmap_entry *revp, __be64 *hptp)\r\n{\r\nunsigned long rcbits_unset;\r\nif (revp->guest_rpte & HPTE_GR_MODIFIED)\r\nreturn 1;\r\nrcbits_unset = ~revp->guest_rpte & (HPTE_R_R | HPTE_R_C);\r\nif ((be64_to_cpu(hptp[0]) & HPTE_V_VALID) &&\r\n(be64_to_cpu(hptp[1]) & rcbits_unset))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic long record_hpte(unsigned long flags, __be64 *hptp,\r\nunsigned long *hpte, struct revmap_entry *revp,\r\nint want_valid, int first_pass)\r\n{\r\nunsigned long v, r;\r\nunsigned long rcbits_unset;\r\nint ok = 1;\r\nint valid, dirty;\r\ndirty = hpte_dirty(revp, hptp);\r\nif (!first_pass && !dirty)\r\nreturn 0;\r\nvalid = 0;\r\nif (be64_to_cpu(hptp[0]) & (HPTE_V_VALID | HPTE_V_ABSENT)) {\r\nvalid = 1;\r\nif ((flags & KVM_GET_HTAB_BOLTED_ONLY) &&\r\n!(be64_to_cpu(hptp[0]) & HPTE_V_BOLTED))\r\nvalid = 0;\r\n}\r\nif (valid != want_valid)\r\nreturn 0;\r\nv = r = 0;\r\nif (valid || dirty) {\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptp, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = be64_to_cpu(hptp[0]);\r\nvalid = !!(v & HPTE_V_VALID);\r\ndirty = !!(revp->guest_rpte & HPTE_GR_MODIFIED);\r\nrcbits_unset = ~revp->guest_rpte & (HPTE_R_R | HPTE_R_C);\r\nif (valid && (rcbits_unset & be64_to_cpu(hptp[1]))) {\r\nrevp->guest_rpte |= (be64_to_cpu(hptp[1]) &\r\n(HPTE_R_R | HPTE_R_C)) | HPTE_GR_MODIFIED;\r\ndirty = 1;\r\n}\r\nif (v & HPTE_V_ABSENT) {\r\nv &= ~HPTE_V_ABSENT;\r\nv |= HPTE_V_VALID;\r\nvalid = 1;\r\n}\r\nif ((flags & KVM_GET_HTAB_BOLTED_ONLY) && !(v & HPTE_V_BOLTED))\r\nvalid = 0;\r\nr = revp->guest_rpte;\r\nif (valid == want_valid && dirty) {\r\nr &= ~HPTE_GR_MODIFIED;\r\nrevp->guest_rpte = r;\r\n}\r\nunlock_hpte(hptp, be64_to_cpu(hptp[0]));\r\npreempt_enable();\r\nif (!(valid == want_valid && (first_pass || dirty)))\r\nok = 0;\r\n}\r\nhpte[0] = cpu_to_be64(v);\r\nhpte[1] = cpu_to_be64(r);\r\nreturn ok;\r\n}\r\nstatic ssize_t kvm_htab_read(struct file *file, char __user *buf,\r\nsize_t count, loff_t *ppos)\r\n{\r\nstruct kvm_htab_ctx *ctx = file->private_data;\r\nstruct kvm *kvm = ctx->kvm;\r\nstruct kvm_get_htab_header hdr;\r\n__be64 *hptp;\r\nstruct revmap_entry *revp;\r\nunsigned long i, nb, nw;\r\nunsigned long __user *lbuf;\r\nstruct kvm_get_htab_header __user *hptr;\r\nunsigned long flags;\r\nint first_pass;\r\nunsigned long hpte[2];\r\nif (!access_ok(VERIFY_WRITE, buf, count))\r\nreturn -EFAULT;\r\nfirst_pass = ctx->first_pass;\r\nflags = ctx->flags;\r\ni = ctx->index;\r\nhptp = (__be64 *)(kvm->arch.hpt_virt + (i * HPTE_SIZE));\r\nrevp = kvm->arch.revmap + i;\r\nlbuf = (unsigned long __user *)buf;\r\nnb = 0;\r\nwhile (nb + sizeof(hdr) + HPTE_SIZE < count) {\r\nhptr = (struct kvm_get_htab_header __user *)buf;\r\nhdr.n_valid = 0;\r\nhdr.n_invalid = 0;\r\nnw = nb;\r\nnb += sizeof(hdr);\r\nlbuf = (unsigned long __user *)(buf + sizeof(hdr));\r\nif (!first_pass) {\r\nwhile (i < kvm->arch.hpt_npte &&\r\n!hpte_dirty(revp, hptp)) {\r\n++i;\r\nhptp += 2;\r\n++revp;\r\n}\r\n}\r\nhdr.index = i;\r\nwhile (i < kvm->arch.hpt_npte &&\r\nhdr.n_valid < 0xffff &&\r\nnb + HPTE_SIZE < count &&\r\nrecord_hpte(flags, hptp, hpte, revp, 1, first_pass)) {\r\n++hdr.n_valid;\r\nif (__put_user(hpte[0], lbuf) ||\r\n__put_user(hpte[1], lbuf + 1))\r\nreturn -EFAULT;\r\nnb += HPTE_SIZE;\r\nlbuf += 2;\r\n++i;\r\nhptp += 2;\r\n++revp;\r\n}\r\nwhile (i < kvm->arch.hpt_npte &&\r\nhdr.n_invalid < 0xffff &&\r\nrecord_hpte(flags, hptp, hpte, revp, 0, first_pass)) {\r\n++hdr.n_invalid;\r\n++i;\r\nhptp += 2;\r\n++revp;\r\n}\r\nif (hdr.n_valid || hdr.n_invalid) {\r\nif (__copy_to_user(hptr, &hdr, sizeof(hdr)))\r\nreturn -EFAULT;\r\nnw = nb;\r\nbuf = (char __user *)lbuf;\r\n} else {\r\nnb = nw;\r\n}\r\nif (i >= kvm->arch.hpt_npte) {\r\ni = 0;\r\nctx->first_pass = 0;\r\nbreak;\r\n}\r\n}\r\nctx->index = i;\r\nreturn nb;\r\n}\r\nstatic ssize_t kvm_htab_write(struct file *file, const char __user *buf,\r\nsize_t count, loff_t *ppos)\r\n{\r\nstruct kvm_htab_ctx *ctx = file->private_data;\r\nstruct kvm *kvm = ctx->kvm;\r\nstruct kvm_get_htab_header hdr;\r\nunsigned long i, j;\r\nunsigned long v, r;\r\nunsigned long __user *lbuf;\r\n__be64 *hptp;\r\nunsigned long tmp[2];\r\nssize_t nb;\r\nlong int err, ret;\r\nint hpte_setup;\r\nif (!access_ok(VERIFY_READ, buf, count))\r\nreturn -EFAULT;\r\nmutex_lock(&kvm->lock);\r\nhpte_setup = kvm->arch.hpte_setup_done;\r\nif (hpte_setup) {\r\nkvm->arch.hpte_setup_done = 0;\r\nsmp_mb();\r\nif (atomic_read(&kvm->arch.vcpus_running)) {\r\nkvm->arch.hpte_setup_done = 1;\r\nmutex_unlock(&kvm->lock);\r\nreturn -EBUSY;\r\n}\r\n}\r\nerr = 0;\r\nfor (nb = 0; nb + sizeof(hdr) <= count; ) {\r\nerr = -EFAULT;\r\nif (__copy_from_user(&hdr, buf, sizeof(hdr)))\r\nbreak;\r\nerr = 0;\r\nif (nb + hdr.n_valid * HPTE_SIZE > count)\r\nbreak;\r\nnb += sizeof(hdr);\r\nbuf += sizeof(hdr);\r\nerr = -EINVAL;\r\ni = hdr.index;\r\nif (i >= kvm->arch.hpt_npte ||\r\ni + hdr.n_valid + hdr.n_invalid > kvm->arch.hpt_npte)\r\nbreak;\r\nhptp = (__be64 *)(kvm->arch.hpt_virt + (i * HPTE_SIZE));\r\nlbuf = (unsigned long __user *)buf;\r\nfor (j = 0; j < hdr.n_valid; ++j) {\r\n__be64 hpte_v;\r\n__be64 hpte_r;\r\nerr = -EFAULT;\r\nif (__get_user(hpte_v, lbuf) ||\r\n__get_user(hpte_r, lbuf + 1))\r\ngoto out;\r\nv = be64_to_cpu(hpte_v);\r\nr = be64_to_cpu(hpte_r);\r\nerr = -EINVAL;\r\nif (!(v & HPTE_V_VALID))\r\ngoto out;\r\nlbuf += 2;\r\nnb += HPTE_SIZE;\r\nif (be64_to_cpu(hptp[0]) & (HPTE_V_VALID | HPTE_V_ABSENT))\r\nkvmppc_do_h_remove(kvm, 0, i, 0, tmp);\r\nerr = -EIO;\r\nret = kvmppc_virtmode_do_h_enter(kvm, H_EXACT, i, v, r,\r\ntmp);\r\nif (ret != H_SUCCESS) {\r\npr_err("kvm_htab_write ret %ld i=%ld v=%lx "\r\n"r=%lx\n", ret, i, v, r);\r\ngoto out;\r\n}\r\nif (!hpte_setup && is_vrma_hpte(v)) {\r\nunsigned long psize = hpte_base_page_size(v, r);\r\nunsigned long senc = slb_pgsize_encoding(psize);\r\nunsigned long lpcr;\r\nkvm->arch.vrma_slb_v = senc | SLB_VSID_B_1T |\r\n(VRMA_VSID << SLB_VSID_SHIFT_1T);\r\nlpcr = senc << (LPCR_VRMASD_SH - 4);\r\nkvmppc_update_lpcr(kvm, lpcr, LPCR_VRMASD);\r\nhpte_setup = 1;\r\n}\r\n++i;\r\nhptp += 2;\r\n}\r\nfor (j = 0; j < hdr.n_invalid; ++j) {\r\nif (be64_to_cpu(hptp[0]) & (HPTE_V_VALID | HPTE_V_ABSENT))\r\nkvmppc_do_h_remove(kvm, 0, i, 0, tmp);\r\n++i;\r\nhptp += 2;\r\n}\r\nerr = 0;\r\n}\r\nout:\r\nsmp_wmb();\r\nkvm->arch.hpte_setup_done = hpte_setup;\r\nmutex_unlock(&kvm->lock);\r\nif (err)\r\nreturn err;\r\nreturn nb;\r\n}\r\nstatic int kvm_htab_release(struct inode *inode, struct file *filp)\r\n{\r\nstruct kvm_htab_ctx *ctx = filp->private_data;\r\nfilp->private_data = NULL;\r\nif (!(ctx->flags & KVM_GET_HTAB_WRITE))\r\natomic_dec(&ctx->kvm->arch.hpte_mod_interest);\r\nkvm_put_kvm(ctx->kvm);\r\nkfree(ctx);\r\nreturn 0;\r\n}\r\nint kvm_vm_ioctl_get_htab_fd(struct kvm *kvm, struct kvm_get_htab_fd *ghf)\r\n{\r\nint ret;\r\nstruct kvm_htab_ctx *ctx;\r\nint rwflag;\r\nif (ghf->flags & ~(KVM_GET_HTAB_BOLTED_ONLY | KVM_GET_HTAB_WRITE))\r\nreturn -EINVAL;\r\nctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\r\nif (!ctx)\r\nreturn -ENOMEM;\r\nkvm_get_kvm(kvm);\r\nctx->kvm = kvm;\r\nctx->index = ghf->start_index;\r\nctx->flags = ghf->flags;\r\nctx->first_pass = 1;\r\nrwflag = (ghf->flags & KVM_GET_HTAB_WRITE) ? O_WRONLY : O_RDONLY;\r\nret = anon_inode_getfd("kvm-htab", &kvm_htab_fops, ctx, rwflag | O_CLOEXEC);\r\nif (ret < 0) {\r\nkvm_put_kvm(kvm);\r\nreturn ret;\r\n}\r\nif (rwflag == O_RDONLY) {\r\nmutex_lock(&kvm->slots_lock);\r\natomic_inc(&kvm->arch.hpte_mod_interest);\r\nsynchronize_srcu_expedited(&kvm->srcu);\r\nmutex_unlock(&kvm->slots_lock);\r\n}\r\nreturn ret;\r\n}\r\nstatic int debugfs_htab_open(struct inode *inode, struct file *file)\r\n{\r\nstruct kvm *kvm = inode->i_private;\r\nstruct debugfs_htab_state *p;\r\np = kzalloc(sizeof(*p), GFP_KERNEL);\r\nif (!p)\r\nreturn -ENOMEM;\r\nkvm_get_kvm(kvm);\r\np->kvm = kvm;\r\nmutex_init(&p->mutex);\r\nfile->private_data = p;\r\nreturn nonseekable_open(inode, file);\r\n}\r\nstatic int debugfs_htab_release(struct inode *inode, struct file *file)\r\n{\r\nstruct debugfs_htab_state *p = file->private_data;\r\nkvm_put_kvm(p->kvm);\r\nkfree(p);\r\nreturn 0;\r\n}\r\nstatic ssize_t debugfs_htab_read(struct file *file, char __user *buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct debugfs_htab_state *p = file->private_data;\r\nssize_t ret, r;\r\nunsigned long i, n;\r\nunsigned long v, hr, gr;\r\nstruct kvm *kvm;\r\n__be64 *hptp;\r\nret = mutex_lock_interruptible(&p->mutex);\r\nif (ret)\r\nreturn ret;\r\nif (p->chars_left) {\r\nn = p->chars_left;\r\nif (n > len)\r\nn = len;\r\nr = copy_to_user(buf, p->buf + p->buf_index, n);\r\nn -= r;\r\np->chars_left -= n;\r\np->buf_index += n;\r\nbuf += n;\r\nlen -= n;\r\nret = n;\r\nif (r) {\r\nif (!n)\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\n}\r\nkvm = p->kvm;\r\ni = p->hpt_index;\r\nhptp = (__be64 *)(kvm->arch.hpt_virt + (i * HPTE_SIZE));\r\nfor (; len != 0 && i < kvm->arch.hpt_npte; ++i, hptp += 2) {\r\nif (!(be64_to_cpu(hptp[0]) & (HPTE_V_VALID | HPTE_V_ABSENT)))\r\ncontinue;\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptp, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = be64_to_cpu(hptp[0]) & ~HPTE_V_HVLOCK;\r\nhr = be64_to_cpu(hptp[1]);\r\ngr = kvm->arch.revmap[i].guest_rpte;\r\nunlock_hpte(hptp, v);\r\npreempt_enable();\r\nif (!(v & (HPTE_V_VALID | HPTE_V_ABSENT)))\r\ncontinue;\r\nn = scnprintf(p->buf, sizeof(p->buf),\r\n"%6lx %.16lx %.16lx %.16lx\n",\r\ni, v, hr, gr);\r\np->chars_left = n;\r\nif (n > len)\r\nn = len;\r\nr = copy_to_user(buf, p->buf, n);\r\nn -= r;\r\np->chars_left -= n;\r\np->buf_index = n;\r\nbuf += n;\r\nlen -= n;\r\nret += n;\r\nif (r) {\r\nif (!ret)\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\n}\r\np->hpt_index = i;\r\nout:\r\nmutex_unlock(&p->mutex);\r\nreturn ret;\r\n}\r\nssize_t debugfs_htab_write(struct file *file, const char __user *buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nreturn -EACCES;\r\n}\r\nvoid kvmppc_mmu_debugfs_init(struct kvm *kvm)\r\n{\r\nkvm->arch.htab_dentry = debugfs_create_file("htab", 0400,\r\nkvm->arch.debugfs_dir, kvm,\r\n&debugfs_htab_fops);\r\n}\r\nvoid kvmppc_mmu_book3s_hv_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_mmu *mmu = &vcpu->arch.mmu;\r\nvcpu->arch.slb_nr = 32;\r\nmmu->xlate = kvmppc_mmu_book3s_64_hv_xlate;\r\nmmu->reset_msr = kvmppc_mmu_book3s_64_hv_reset_msr;\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_SLB;\r\n}
