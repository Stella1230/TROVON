u8 ata_sff_check_status(struct ata_port *ap)\r\n{\r\nreturn ioread8(ap->ioaddr.status_addr);\r\n}\r\nstatic u8 ata_sff_altstatus(struct ata_port *ap)\r\n{\r\nif (ap->ops->sff_check_altstatus)\r\nreturn ap->ops->sff_check_altstatus(ap);\r\nreturn ioread8(ap->ioaddr.altstatus_addr);\r\n}\r\nstatic u8 ata_sff_irq_status(struct ata_port *ap)\r\n{\r\nu8 status;\r\nif (ap->ops->sff_check_altstatus || ap->ioaddr.altstatus_addr) {\r\nstatus = ata_sff_altstatus(ap);\r\nif (status & ATA_BUSY)\r\nreturn status;\r\n}\r\nstatus = ap->ops->sff_check_status(ap);\r\nreturn status;\r\n}\r\nstatic void ata_sff_sync(struct ata_port *ap)\r\n{\r\nif (ap->ops->sff_check_altstatus)\r\nap->ops->sff_check_altstatus(ap);\r\nelse if (ap->ioaddr.altstatus_addr)\r\nioread8(ap->ioaddr.altstatus_addr);\r\n}\r\nvoid ata_sff_pause(struct ata_port *ap)\r\n{\r\nata_sff_sync(ap);\r\nndelay(400);\r\n}\r\nvoid ata_sff_dma_pause(struct ata_port *ap)\r\n{\r\nif (ap->ops->sff_check_altstatus || ap->ioaddr.altstatus_addr) {\r\nata_sff_altstatus(ap);\r\nreturn;\r\n}\r\nBUG();\r\n}\r\nint ata_sff_busy_sleep(struct ata_port *ap,\r\nunsigned long tmout_pat, unsigned long tmout)\r\n{\r\nunsigned long timer_start, timeout;\r\nu8 status;\r\nstatus = ata_sff_busy_wait(ap, ATA_BUSY, 300);\r\ntimer_start = jiffies;\r\ntimeout = ata_deadline(timer_start, tmout_pat);\r\nwhile (status != 0xff && (status & ATA_BUSY) &&\r\ntime_before(jiffies, timeout)) {\r\nata_msleep(ap, 50);\r\nstatus = ata_sff_busy_wait(ap, ATA_BUSY, 3);\r\n}\r\nif (status != 0xff && (status & ATA_BUSY))\r\nata_port_warn(ap,\r\n"port is slow to respond, please be patient (Status 0x%x)\n",\r\nstatus);\r\ntimeout = ata_deadline(timer_start, tmout);\r\nwhile (status != 0xff && (status & ATA_BUSY) &&\r\ntime_before(jiffies, timeout)) {\r\nata_msleep(ap, 50);\r\nstatus = ap->ops->sff_check_status(ap);\r\n}\r\nif (status == 0xff)\r\nreturn -ENODEV;\r\nif (status & ATA_BUSY) {\r\nata_port_err(ap,\r\n"port failed to respond (%lu secs, Status 0x%x)\n",\r\nDIV_ROUND_UP(tmout, 1000), status);\r\nreturn -EBUSY;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ata_sff_check_ready(struct ata_link *link)\r\n{\r\nu8 status = link->ap->ops->sff_check_status(link->ap);\r\nreturn ata_check_ready(status);\r\n}\r\nint ata_sff_wait_ready(struct ata_link *link, unsigned long deadline)\r\n{\r\nreturn ata_wait_ready(link, deadline, ata_sff_check_ready);\r\n}\r\nstatic void ata_sff_set_devctl(struct ata_port *ap, u8 ctl)\r\n{\r\nif (ap->ops->sff_set_devctl)\r\nap->ops->sff_set_devctl(ap, ctl);\r\nelse\r\niowrite8(ctl, ap->ioaddr.ctl_addr);\r\n}\r\nvoid ata_sff_dev_select(struct ata_port *ap, unsigned int device)\r\n{\r\nu8 tmp;\r\nif (device == 0)\r\ntmp = ATA_DEVICE_OBS;\r\nelse\r\ntmp = ATA_DEVICE_OBS | ATA_DEV1;\r\niowrite8(tmp, ap->ioaddr.device_addr);\r\nata_sff_pause(ap);\r\n}\r\nstatic void ata_dev_select(struct ata_port *ap, unsigned int device,\r\nunsigned int wait, unsigned int can_sleep)\r\n{\r\nif (ata_msg_probe(ap))\r\nata_port_info(ap, "ata_dev_select: ENTER, device %u, wait %u\n",\r\ndevice, wait);\r\nif (wait)\r\nata_wait_idle(ap);\r\nap->ops->sff_dev_select(ap, device);\r\nif (wait) {\r\nif (can_sleep && ap->link.device[device].class == ATA_DEV_ATAPI)\r\nata_msleep(ap, 150);\r\nata_wait_idle(ap);\r\n}\r\n}\r\nvoid ata_sff_irq_on(struct ata_port *ap)\r\n{\r\nstruct ata_ioports *ioaddr = &ap->ioaddr;\r\nif (ap->ops->sff_irq_on) {\r\nap->ops->sff_irq_on(ap);\r\nreturn;\r\n}\r\nap->ctl &= ~ATA_NIEN;\r\nap->last_ctl = ap->ctl;\r\nif (ap->ops->sff_set_devctl || ioaddr->ctl_addr)\r\nata_sff_set_devctl(ap, ap->ctl);\r\nata_wait_idle(ap);\r\nif (ap->ops->sff_irq_clear)\r\nap->ops->sff_irq_clear(ap);\r\n}\r\nvoid ata_sff_tf_load(struct ata_port *ap, const struct ata_taskfile *tf)\r\n{\r\nstruct ata_ioports *ioaddr = &ap->ioaddr;\r\nunsigned int is_addr = tf->flags & ATA_TFLAG_ISADDR;\r\nif (tf->ctl != ap->last_ctl) {\r\nif (ioaddr->ctl_addr)\r\niowrite8(tf->ctl, ioaddr->ctl_addr);\r\nap->last_ctl = tf->ctl;\r\nata_wait_idle(ap);\r\n}\r\nif (is_addr && (tf->flags & ATA_TFLAG_LBA48)) {\r\nWARN_ON_ONCE(!ioaddr->ctl_addr);\r\niowrite8(tf->hob_feature, ioaddr->feature_addr);\r\niowrite8(tf->hob_nsect, ioaddr->nsect_addr);\r\niowrite8(tf->hob_lbal, ioaddr->lbal_addr);\r\niowrite8(tf->hob_lbam, ioaddr->lbam_addr);\r\niowrite8(tf->hob_lbah, ioaddr->lbah_addr);\r\nVPRINTK("hob: feat 0x%X nsect 0x%X, lba 0x%X 0x%X 0x%X\n",\r\ntf->hob_feature,\r\ntf->hob_nsect,\r\ntf->hob_lbal,\r\ntf->hob_lbam,\r\ntf->hob_lbah);\r\n}\r\nif (is_addr) {\r\niowrite8(tf->feature, ioaddr->feature_addr);\r\niowrite8(tf->nsect, ioaddr->nsect_addr);\r\niowrite8(tf->lbal, ioaddr->lbal_addr);\r\niowrite8(tf->lbam, ioaddr->lbam_addr);\r\niowrite8(tf->lbah, ioaddr->lbah_addr);\r\nVPRINTK("feat 0x%X nsect 0x%X lba 0x%X 0x%X 0x%X\n",\r\ntf->feature,\r\ntf->nsect,\r\ntf->lbal,\r\ntf->lbam,\r\ntf->lbah);\r\n}\r\nif (tf->flags & ATA_TFLAG_DEVICE) {\r\niowrite8(tf->device, ioaddr->device_addr);\r\nVPRINTK("device 0x%X\n", tf->device);\r\n}\r\nata_wait_idle(ap);\r\n}\r\nvoid ata_sff_tf_read(struct ata_port *ap, struct ata_taskfile *tf)\r\n{\r\nstruct ata_ioports *ioaddr = &ap->ioaddr;\r\ntf->command = ata_sff_check_status(ap);\r\ntf->feature = ioread8(ioaddr->error_addr);\r\ntf->nsect = ioread8(ioaddr->nsect_addr);\r\ntf->lbal = ioread8(ioaddr->lbal_addr);\r\ntf->lbam = ioread8(ioaddr->lbam_addr);\r\ntf->lbah = ioread8(ioaddr->lbah_addr);\r\ntf->device = ioread8(ioaddr->device_addr);\r\nif (tf->flags & ATA_TFLAG_LBA48) {\r\nif (likely(ioaddr->ctl_addr)) {\r\niowrite8(tf->ctl | ATA_HOB, ioaddr->ctl_addr);\r\ntf->hob_feature = ioread8(ioaddr->error_addr);\r\ntf->hob_nsect = ioread8(ioaddr->nsect_addr);\r\ntf->hob_lbal = ioread8(ioaddr->lbal_addr);\r\ntf->hob_lbam = ioread8(ioaddr->lbam_addr);\r\ntf->hob_lbah = ioread8(ioaddr->lbah_addr);\r\niowrite8(tf->ctl, ioaddr->ctl_addr);\r\nap->last_ctl = tf->ctl;\r\n} else\r\nWARN_ON_ONCE(1);\r\n}\r\n}\r\nvoid ata_sff_exec_command(struct ata_port *ap, const struct ata_taskfile *tf)\r\n{\r\nDPRINTK("ata%u: cmd 0x%X\n", ap->print_id, tf->command);\r\niowrite8(tf->command, ap->ioaddr.command_addr);\r\nata_sff_pause(ap);\r\n}\r\nstatic inline void ata_tf_to_host(struct ata_port *ap,\r\nconst struct ata_taskfile *tf)\r\n{\r\nap->ops->sff_tf_load(ap, tf);\r\nap->ops->sff_exec_command(ap, tf);\r\n}\r\nunsigned int ata_sff_data_xfer(struct ata_device *dev, unsigned char *buf,\r\nunsigned int buflen, int rw)\r\n{\r\nstruct ata_port *ap = dev->link->ap;\r\nvoid __iomem *data_addr = ap->ioaddr.data_addr;\r\nunsigned int words = buflen >> 1;\r\nif (rw == READ)\r\nioread16_rep(data_addr, buf, words);\r\nelse\r\niowrite16_rep(data_addr, buf, words);\r\nif (unlikely(buflen & 0x01)) {\r\nunsigned char pad[2] = { };\r\nbuf += buflen - 1;\r\nif (rw == READ) {\r\nioread16_rep(data_addr, pad, 1);\r\n*buf = pad[0];\r\n} else {\r\npad[0] = *buf;\r\niowrite16_rep(data_addr, pad, 1);\r\n}\r\nwords++;\r\n}\r\nreturn words << 1;\r\n}\r\nunsigned int ata_sff_data_xfer32(struct ata_device *dev, unsigned char *buf,\r\nunsigned int buflen, int rw)\r\n{\r\nstruct ata_port *ap = dev->link->ap;\r\nvoid __iomem *data_addr = ap->ioaddr.data_addr;\r\nunsigned int words = buflen >> 2;\r\nint slop = buflen & 3;\r\nif (!(ap->pflags & ATA_PFLAG_PIO32))\r\nreturn ata_sff_data_xfer(dev, buf, buflen, rw);\r\nif (rw == READ)\r\nioread32_rep(data_addr, buf, words);\r\nelse\r\niowrite32_rep(data_addr, buf, words);\r\nif (unlikely(slop)) {\r\nunsigned char pad[4] = { };\r\nbuf += buflen - slop;\r\nif (rw == READ) {\r\nif (slop < 3)\r\nioread16_rep(data_addr, pad, 1);\r\nelse\r\nioread32_rep(data_addr, pad, 1);\r\nmemcpy(buf, pad, slop);\r\n} else {\r\nmemcpy(pad, buf, slop);\r\nif (slop < 3)\r\niowrite16_rep(data_addr, pad, 1);\r\nelse\r\niowrite32_rep(data_addr, pad, 1);\r\n}\r\n}\r\nreturn (buflen + 1) & ~1;\r\n}\r\nunsigned int ata_sff_data_xfer_noirq(struct ata_device *dev, unsigned char *buf,\r\nunsigned int buflen, int rw)\r\n{\r\nunsigned long flags;\r\nunsigned int consumed;\r\nlocal_irq_save(flags);\r\nconsumed = ata_sff_data_xfer32(dev, buf, buflen, rw);\r\nlocal_irq_restore(flags);\r\nreturn consumed;\r\n}\r\nstatic void ata_pio_sector(struct ata_queued_cmd *qc)\r\n{\r\nint do_write = (qc->tf.flags & ATA_TFLAG_WRITE);\r\nstruct ata_port *ap = qc->ap;\r\nstruct page *page;\r\nunsigned int offset;\r\nunsigned char *buf;\r\nif (qc->curbytes == qc->nbytes - qc->sect_size)\r\nap->hsm_task_state = HSM_ST_LAST;\r\npage = sg_page(qc->cursg);\r\noffset = qc->cursg->offset + qc->cursg_ofs;\r\npage = nth_page(page, (offset >> PAGE_SHIFT));\r\noffset %= PAGE_SIZE;\r\nDPRINTK("data %s\n", qc->tf.flags & ATA_TFLAG_WRITE ? "write" : "read");\r\nif (PageHighMem(page)) {\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nbuf = kmap_atomic(page);\r\nap->ops->sff_data_xfer(qc->dev, buf + offset, qc->sect_size,\r\ndo_write);\r\nkunmap_atomic(buf);\r\nlocal_irq_restore(flags);\r\n} else {\r\nbuf = page_address(page);\r\nap->ops->sff_data_xfer(qc->dev, buf + offset, qc->sect_size,\r\ndo_write);\r\n}\r\nif (!do_write && !PageSlab(page))\r\nflush_dcache_page(page);\r\nqc->curbytes += qc->sect_size;\r\nqc->cursg_ofs += qc->sect_size;\r\nif (qc->cursg_ofs == qc->cursg->length) {\r\nqc->cursg = sg_next(qc->cursg);\r\nqc->cursg_ofs = 0;\r\n}\r\n}\r\nstatic void ata_pio_sectors(struct ata_queued_cmd *qc)\r\n{\r\nif (is_multi_taskfile(&qc->tf)) {\r\nunsigned int nsect;\r\nWARN_ON_ONCE(qc->dev->multi_count == 0);\r\nnsect = min((qc->nbytes - qc->curbytes) / qc->sect_size,\r\nqc->dev->multi_count);\r\nwhile (nsect--)\r\nata_pio_sector(qc);\r\n} else\r\nata_pio_sector(qc);\r\nata_sff_sync(qc->ap);\r\n}\r\nstatic void atapi_send_cdb(struct ata_port *ap, struct ata_queued_cmd *qc)\r\n{\r\nDPRINTK("send cdb\n");\r\nWARN_ON_ONCE(qc->dev->cdb_len < 12);\r\nap->ops->sff_data_xfer(qc->dev, qc->cdb, qc->dev->cdb_len, 1);\r\nata_sff_sync(ap);\r\nswitch (qc->tf.protocol) {\r\ncase ATAPI_PROT_PIO:\r\nap->hsm_task_state = HSM_ST;\r\nbreak;\r\ncase ATAPI_PROT_NODATA:\r\nap->hsm_task_state = HSM_ST_LAST;\r\nbreak;\r\n#ifdef CONFIG_ATA_BMDMA\r\ncase ATAPI_PROT_DMA:\r\nap->hsm_task_state = HSM_ST_LAST;\r\nap->ops->bmdma_start(qc);\r\nbreak;\r\n#endif\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic int __atapi_pio_bytes(struct ata_queued_cmd *qc, unsigned int bytes)\r\n{\r\nint rw = (qc->tf.flags & ATA_TFLAG_WRITE) ? WRITE : READ;\r\nstruct ata_port *ap = qc->ap;\r\nstruct ata_device *dev = qc->dev;\r\nstruct ata_eh_info *ehi = &dev->link->eh_info;\r\nstruct scatterlist *sg;\r\nstruct page *page;\r\nunsigned char *buf;\r\nunsigned int offset, count, consumed;\r\nnext_sg:\r\nsg = qc->cursg;\r\nif (unlikely(!sg)) {\r\nata_ehi_push_desc(ehi, "unexpected or too much trailing data "\r\n"buf=%u cur=%u bytes=%u",\r\nqc->nbytes, qc->curbytes, bytes);\r\nreturn -1;\r\n}\r\npage = sg_page(sg);\r\noffset = sg->offset + qc->cursg_ofs;\r\npage = nth_page(page, (offset >> PAGE_SHIFT));\r\noffset %= PAGE_SIZE;\r\ncount = min(sg->length - qc->cursg_ofs, bytes);\r\ncount = min(count, (unsigned int)PAGE_SIZE - offset);\r\nDPRINTK("data %s\n", qc->tf.flags & ATA_TFLAG_WRITE ? "write" : "read");\r\nif (PageHighMem(page)) {\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nbuf = kmap_atomic(page);\r\nconsumed = ap->ops->sff_data_xfer(dev, buf + offset,\r\ncount, rw);\r\nkunmap_atomic(buf);\r\nlocal_irq_restore(flags);\r\n} else {\r\nbuf = page_address(page);\r\nconsumed = ap->ops->sff_data_xfer(dev, buf + offset,\r\ncount, rw);\r\n}\r\nbytes -= min(bytes, consumed);\r\nqc->curbytes += count;\r\nqc->cursg_ofs += count;\r\nif (qc->cursg_ofs == sg->length) {\r\nqc->cursg = sg_next(qc->cursg);\r\nqc->cursg_ofs = 0;\r\n}\r\nif (bytes)\r\ngoto next_sg;\r\nreturn 0;\r\n}\r\nstatic void atapi_pio_bytes(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct ata_device *dev = qc->dev;\r\nstruct ata_eh_info *ehi = &dev->link->eh_info;\r\nunsigned int ireason, bc_lo, bc_hi, bytes;\r\nint i_write, do_write = (qc->tf.flags & ATA_TFLAG_WRITE) ? 1 : 0;\r\nap->ops->sff_tf_read(ap, &qc->result_tf);\r\nireason = qc->result_tf.nsect;\r\nbc_lo = qc->result_tf.lbam;\r\nbc_hi = qc->result_tf.lbah;\r\nbytes = (bc_hi << 8) | bc_lo;\r\nif (unlikely(ireason & ATAPI_COD))\r\ngoto atapi_check;\r\ni_write = ((ireason & ATAPI_IO) == 0) ? 1 : 0;\r\nif (unlikely(do_write != i_write))\r\ngoto atapi_check;\r\nif (unlikely(!bytes))\r\ngoto atapi_check;\r\nVPRINTK("ata%u: xfering %d bytes\n", ap->print_id, bytes);\r\nif (unlikely(__atapi_pio_bytes(qc, bytes)))\r\ngoto err_out;\r\nata_sff_sync(ap);\r\nreturn;\r\natapi_check:\r\nata_ehi_push_desc(ehi, "ATAPI check failed (ireason=0x%x bytes=%u)",\r\nireason, bytes);\r\nerr_out:\r\nqc->err_mask |= AC_ERR_HSM;\r\nap->hsm_task_state = HSM_ST_ERR;\r\n}\r\nstatic inline int ata_hsm_ok_in_wq(struct ata_port *ap,\r\nstruct ata_queued_cmd *qc)\r\n{\r\nif (qc->tf.flags & ATA_TFLAG_POLLING)\r\nreturn 1;\r\nif (ap->hsm_task_state == HSM_ST_FIRST) {\r\nif (qc->tf.protocol == ATA_PROT_PIO &&\r\n(qc->tf.flags & ATA_TFLAG_WRITE))\r\nreturn 1;\r\nif (ata_is_atapi(qc->tf.protocol) &&\r\n!(qc->dev->flags & ATA_DFLAG_CDB_INTR))\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ata_hsm_qc_complete(struct ata_queued_cmd *qc, int in_wq)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nif (ap->ops->error_handler) {\r\nif (in_wq) {\r\nqc = ata_qc_from_tag(ap, qc->tag);\r\nif (qc) {\r\nif (likely(!(qc->err_mask & AC_ERR_HSM))) {\r\nata_sff_irq_on(ap);\r\nata_qc_complete(qc);\r\n} else\r\nata_port_freeze(ap);\r\n}\r\n} else {\r\nif (likely(!(qc->err_mask & AC_ERR_HSM)))\r\nata_qc_complete(qc);\r\nelse\r\nata_port_freeze(ap);\r\n}\r\n} else {\r\nif (in_wq) {\r\nata_sff_irq_on(ap);\r\nata_qc_complete(qc);\r\n} else\r\nata_qc_complete(qc);\r\n}\r\n}\r\nint ata_sff_hsm_move(struct ata_port *ap, struct ata_queued_cmd *qc,\r\nu8 status, int in_wq)\r\n{\r\nstruct ata_link *link = qc->dev->link;\r\nstruct ata_eh_info *ehi = &link->eh_info;\r\nint poll_next;\r\nlockdep_assert_held(ap->lock);\r\nWARN_ON_ONCE((qc->flags & ATA_QCFLAG_ACTIVE) == 0);\r\nWARN_ON_ONCE(in_wq != ata_hsm_ok_in_wq(ap, qc));\r\nfsm_start:\r\nDPRINTK("ata%u: protocol %d task_state %d (dev_stat 0x%X)\n",\r\nap->print_id, qc->tf.protocol, ap->hsm_task_state, status);\r\nswitch (ap->hsm_task_state) {\r\ncase HSM_ST_FIRST:\r\npoll_next = (qc->tf.flags & ATA_TFLAG_POLLING);\r\nif (unlikely((status & ATA_DRQ) == 0)) {\r\nif (likely(status & (ATA_ERR | ATA_DF)))\r\nqc->err_mask |= AC_ERR_DEV;\r\nelse {\r\nata_ehi_push_desc(ehi,\r\n"ST_FIRST: !(DRQ|ERR|DF)");\r\nqc->err_mask |= AC_ERR_HSM;\r\n}\r\nap->hsm_task_state = HSM_ST_ERR;\r\ngoto fsm_start;\r\n}\r\nif (unlikely(status & (ATA_ERR | ATA_DF))) {\r\nif (!(qc->dev->horkage & ATA_HORKAGE_STUCK_ERR)) {\r\nata_ehi_push_desc(ehi, "ST_FIRST: "\r\n"DRQ=1 with device error, "\r\n"dev_stat 0x%X", status);\r\nqc->err_mask |= AC_ERR_HSM;\r\nap->hsm_task_state = HSM_ST_ERR;\r\ngoto fsm_start;\r\n}\r\n}\r\nif (qc->tf.protocol == ATA_PROT_PIO) {\r\nap->hsm_task_state = HSM_ST;\r\nata_pio_sectors(qc);\r\n} else\r\natapi_send_cdb(ap, qc);\r\nbreak;\r\ncase HSM_ST:\r\nif (qc->tf.protocol == ATAPI_PROT_PIO) {\r\nif ((status & ATA_DRQ) == 0) {\r\nap->hsm_task_state = HSM_ST_LAST;\r\ngoto fsm_start;\r\n}\r\nif (unlikely(status & (ATA_ERR | ATA_DF))) {\r\nata_ehi_push_desc(ehi, "ST-ATAPI: "\r\n"DRQ=1 with device error, "\r\n"dev_stat 0x%X", status);\r\nqc->err_mask |= AC_ERR_HSM;\r\nap->hsm_task_state = HSM_ST_ERR;\r\ngoto fsm_start;\r\n}\r\natapi_pio_bytes(qc);\r\nif (unlikely(ap->hsm_task_state == HSM_ST_ERR))\r\ngoto fsm_start;\r\n} else {\r\nif (unlikely((status & ATA_DRQ) == 0)) {\r\nif (likely(status & (ATA_ERR | ATA_DF))) {\r\nqc->err_mask |= AC_ERR_DEV;\r\nif (qc->dev->horkage &\r\nATA_HORKAGE_DIAGNOSTIC)\r\nqc->err_mask |=\r\nAC_ERR_NODEV_HINT;\r\n} else {\r\nata_ehi_push_desc(ehi, "ST-ATA: "\r\n"DRQ=0 without device error, "\r\n"dev_stat 0x%X", status);\r\nqc->err_mask |= AC_ERR_HSM |\r\nAC_ERR_NODEV_HINT;\r\n}\r\nap->hsm_task_state = HSM_ST_ERR;\r\ngoto fsm_start;\r\n}\r\nif (unlikely(status & (ATA_ERR | ATA_DF))) {\r\nqc->err_mask |= AC_ERR_DEV;\r\nif (!(qc->tf.flags & ATA_TFLAG_WRITE)) {\r\nata_pio_sectors(qc);\r\nstatus = ata_wait_idle(ap);\r\n}\r\nif (status & (ATA_BUSY | ATA_DRQ)) {\r\nata_ehi_push_desc(ehi, "ST-ATA: "\r\n"BUSY|DRQ persists on ERR|DF, "\r\n"dev_stat 0x%X", status);\r\nqc->err_mask |= AC_ERR_HSM;\r\n}\r\nif (status == 0x7f)\r\nqc->err_mask |= AC_ERR_NODEV_HINT;\r\nap->hsm_task_state = HSM_ST_ERR;\r\ngoto fsm_start;\r\n}\r\nata_pio_sectors(qc);\r\nif (ap->hsm_task_state == HSM_ST_LAST &&\r\n(!(qc->tf.flags & ATA_TFLAG_WRITE))) {\r\nstatus = ata_wait_idle(ap);\r\ngoto fsm_start;\r\n}\r\n}\r\npoll_next = 1;\r\nbreak;\r\ncase HSM_ST_LAST:\r\nif (unlikely(!ata_ok(status))) {\r\nqc->err_mask |= __ac_err_mask(status);\r\nap->hsm_task_state = HSM_ST_ERR;\r\ngoto fsm_start;\r\n}\r\nDPRINTK("ata%u: dev %u command complete, drv_stat 0x%x\n",\r\nap->print_id, qc->dev->devno, status);\r\nWARN_ON_ONCE(qc->err_mask & (AC_ERR_DEV | AC_ERR_HSM));\r\nap->hsm_task_state = HSM_ST_IDLE;\r\nata_hsm_qc_complete(qc, in_wq);\r\npoll_next = 0;\r\nbreak;\r\ncase HSM_ST_ERR:\r\nap->hsm_task_state = HSM_ST_IDLE;\r\nata_hsm_qc_complete(qc, in_wq);\r\npoll_next = 0;\r\nbreak;\r\ndefault:\r\npoll_next = 0;\r\nWARN(true, "ata%d: SFF host state machine in invalid state %d",\r\nap->print_id, ap->hsm_task_state);\r\n}\r\nreturn poll_next;\r\n}\r\nvoid ata_sff_queue_work(struct work_struct *work)\r\n{\r\nqueue_work(ata_sff_wq, work);\r\n}\r\nvoid ata_sff_queue_delayed_work(struct delayed_work *dwork, unsigned long delay)\r\n{\r\nqueue_delayed_work(ata_sff_wq, dwork, delay);\r\n}\r\nvoid ata_sff_queue_pio_task(struct ata_link *link, unsigned long delay)\r\n{\r\nstruct ata_port *ap = link->ap;\r\nWARN_ON((ap->sff_pio_task_link != NULL) &&\r\n(ap->sff_pio_task_link != link));\r\nap->sff_pio_task_link = link;\r\nata_sff_queue_delayed_work(&ap->sff_pio_task, msecs_to_jiffies(delay));\r\n}\r\nvoid ata_sff_flush_pio_task(struct ata_port *ap)\r\n{\r\nDPRINTK("ENTER\n");\r\ncancel_delayed_work_sync(&ap->sff_pio_task);\r\nspin_lock_irq(ap->lock);\r\nap->hsm_task_state = HSM_ST_IDLE;\r\nspin_unlock_irq(ap->lock);\r\nap->sff_pio_task_link = NULL;\r\nif (ata_msg_ctl(ap))\r\nata_port_dbg(ap, "%s: EXIT\n", __func__);\r\n}\r\nstatic void ata_sff_pio_task(struct work_struct *work)\r\n{\r\nstruct ata_port *ap =\r\ncontainer_of(work, struct ata_port, sff_pio_task.work);\r\nstruct ata_link *link = ap->sff_pio_task_link;\r\nstruct ata_queued_cmd *qc;\r\nu8 status;\r\nint poll_next;\r\nspin_lock_irq(ap->lock);\r\nBUG_ON(ap->sff_pio_task_link == NULL);\r\nqc = ata_qc_from_tag(ap, link->active_tag);\r\nif (!qc) {\r\nap->sff_pio_task_link = NULL;\r\ngoto out_unlock;\r\n}\r\nfsm_start:\r\nWARN_ON_ONCE(ap->hsm_task_state == HSM_ST_IDLE);\r\nstatus = ata_sff_busy_wait(ap, ATA_BUSY, 5);\r\nif (status & ATA_BUSY) {\r\nspin_unlock_irq(ap->lock);\r\nata_msleep(ap, 2);\r\nspin_lock_irq(ap->lock);\r\nstatus = ata_sff_busy_wait(ap, ATA_BUSY, 10);\r\nif (status & ATA_BUSY) {\r\nata_sff_queue_pio_task(link, ATA_SHORT_PAUSE);\r\ngoto out_unlock;\r\n}\r\n}\r\nap->sff_pio_task_link = NULL;\r\npoll_next = ata_sff_hsm_move(ap, qc, status, 1);\r\nif (poll_next)\r\ngoto fsm_start;\r\nout_unlock:\r\nspin_unlock_irq(ap->lock);\r\n}\r\nunsigned int ata_sff_qc_issue(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct ata_link *link = qc->dev->link;\r\nif (ap->flags & ATA_FLAG_PIO_POLLING)\r\nqc->tf.flags |= ATA_TFLAG_POLLING;\r\nata_dev_select(ap, qc->dev->devno, 1, 0);\r\nswitch (qc->tf.protocol) {\r\ncase ATA_PROT_NODATA:\r\nif (qc->tf.flags & ATA_TFLAG_POLLING)\r\nata_qc_set_polling(qc);\r\nata_tf_to_host(ap, &qc->tf);\r\nap->hsm_task_state = HSM_ST_LAST;\r\nif (qc->tf.flags & ATA_TFLAG_POLLING)\r\nata_sff_queue_pio_task(link, 0);\r\nbreak;\r\ncase ATA_PROT_PIO:\r\nif (qc->tf.flags & ATA_TFLAG_POLLING)\r\nata_qc_set_polling(qc);\r\nata_tf_to_host(ap, &qc->tf);\r\nif (qc->tf.flags & ATA_TFLAG_WRITE) {\r\nap->hsm_task_state = HSM_ST_FIRST;\r\nata_sff_queue_pio_task(link, 0);\r\n} else {\r\nap->hsm_task_state = HSM_ST;\r\nif (qc->tf.flags & ATA_TFLAG_POLLING)\r\nata_sff_queue_pio_task(link, 0);\r\n}\r\nbreak;\r\ncase ATAPI_PROT_PIO:\r\ncase ATAPI_PROT_NODATA:\r\nif (qc->tf.flags & ATA_TFLAG_POLLING)\r\nata_qc_set_polling(qc);\r\nata_tf_to_host(ap, &qc->tf);\r\nap->hsm_task_state = HSM_ST_FIRST;\r\nif ((!(qc->dev->flags & ATA_DFLAG_CDB_INTR)) ||\r\n(qc->tf.flags & ATA_TFLAG_POLLING))\r\nata_sff_queue_pio_task(link, 0);\r\nbreak;\r\ndefault:\r\nWARN_ON_ONCE(1);\r\nreturn AC_ERR_SYSTEM;\r\n}\r\nreturn 0;\r\n}\r\nbool ata_sff_qc_fill_rtf(struct ata_queued_cmd *qc)\r\n{\r\nqc->ap->ops->sff_tf_read(qc->ap, &qc->result_tf);\r\nreturn true;\r\n}\r\nstatic unsigned int ata_sff_idle_irq(struct ata_port *ap)\r\n{\r\nap->stats.idle_irq++;\r\n#ifdef ATA_IRQ_TRAP\r\nif ((ap->stats.idle_irq % 1000) == 0) {\r\nap->ops->sff_check_status(ap);\r\nif (ap->ops->sff_irq_clear)\r\nap->ops->sff_irq_clear(ap);\r\nata_port_warn(ap, "irq trap\n");\r\nreturn 1;\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic unsigned int __ata_sff_port_intr(struct ata_port *ap,\r\nstruct ata_queued_cmd *qc,\r\nbool hsmv_on_idle)\r\n{\r\nu8 status;\r\nVPRINTK("ata%u: protocol %d task_state %d\n",\r\nap->print_id, qc->tf.protocol, ap->hsm_task_state);\r\nswitch (ap->hsm_task_state) {\r\ncase HSM_ST_FIRST:\r\nif (!(qc->dev->flags & ATA_DFLAG_CDB_INTR))\r\nreturn ata_sff_idle_irq(ap);\r\nbreak;\r\ncase HSM_ST_IDLE:\r\nreturn ata_sff_idle_irq(ap);\r\ndefault:\r\nbreak;\r\n}\r\nstatus = ata_sff_irq_status(ap);\r\nif (status & ATA_BUSY) {\r\nif (hsmv_on_idle) {\r\nqc->err_mask |= AC_ERR_HSM;\r\nap->hsm_task_state = HSM_ST_ERR;\r\n} else\r\nreturn ata_sff_idle_irq(ap);\r\n}\r\nif (ap->ops->sff_irq_clear)\r\nap->ops->sff_irq_clear(ap);\r\nata_sff_hsm_move(ap, qc, status, 0);\r\nreturn 1;\r\n}\r\nunsigned int ata_sff_port_intr(struct ata_port *ap, struct ata_queued_cmd *qc)\r\n{\r\nreturn __ata_sff_port_intr(ap, qc, false);\r\n}\r\nirqreturn_t ata_sff_interrupt(int irq, void *dev_instance)\r\n{\r\nreturn __ata_sff_interrupt(irq, dev_instance, ata_sff_port_intr);\r\n}\r\nvoid ata_sff_lost_interrupt(struct ata_port *ap)\r\n{\r\nu8 status;\r\nstruct ata_queued_cmd *qc;\r\nqc = ata_qc_from_tag(ap, ap->link.active_tag);\r\nif (!qc || qc->tf.flags & ATA_TFLAG_POLLING)\r\nreturn;\r\nstatus = ata_sff_altstatus(ap);\r\nif (status & ATA_BUSY)\r\nreturn;\r\nata_port_warn(ap, "lost interrupt (Status 0x%x)\n",\r\nstatus);\r\nata_sff_port_intr(ap, qc);\r\n}\r\nvoid ata_sff_freeze(struct ata_port *ap)\r\n{\r\nap->ctl |= ATA_NIEN;\r\nap->last_ctl = ap->ctl;\r\nif (ap->ops->sff_set_devctl || ap->ioaddr.ctl_addr)\r\nata_sff_set_devctl(ap, ap->ctl);\r\nap->ops->sff_check_status(ap);\r\nif (ap->ops->sff_irq_clear)\r\nap->ops->sff_irq_clear(ap);\r\n}\r\nvoid ata_sff_thaw(struct ata_port *ap)\r\n{\r\nap->ops->sff_check_status(ap);\r\nif (ap->ops->sff_irq_clear)\r\nap->ops->sff_irq_clear(ap);\r\nata_sff_irq_on(ap);\r\n}\r\nint ata_sff_prereset(struct ata_link *link, unsigned long deadline)\r\n{\r\nstruct ata_eh_context *ehc = &link->eh_context;\r\nint rc;\r\nrc = ata_std_prereset(link, deadline);\r\nif (rc)\r\nreturn rc;\r\nif (ehc->i.action & ATA_EH_HARDRESET)\r\nreturn 0;\r\nif (!ata_link_offline(link)) {\r\nrc = ata_sff_wait_ready(link, deadline);\r\nif (rc && rc != -ENODEV) {\r\nata_link_warn(link,\r\n"device not ready (errno=%d), forcing hardreset\n",\r\nrc);\r\nehc->i.action |= ATA_EH_HARDRESET;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned int ata_devchk(struct ata_port *ap, unsigned int device)\r\n{\r\nstruct ata_ioports *ioaddr = &ap->ioaddr;\r\nu8 nsect, lbal;\r\nap->ops->sff_dev_select(ap, device);\r\niowrite8(0x55, ioaddr->nsect_addr);\r\niowrite8(0xaa, ioaddr->lbal_addr);\r\niowrite8(0xaa, ioaddr->nsect_addr);\r\niowrite8(0x55, ioaddr->lbal_addr);\r\niowrite8(0x55, ioaddr->nsect_addr);\r\niowrite8(0xaa, ioaddr->lbal_addr);\r\nnsect = ioread8(ioaddr->nsect_addr);\r\nlbal = ioread8(ioaddr->lbal_addr);\r\nif ((nsect == 0x55) && (lbal == 0xaa))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nunsigned int ata_sff_dev_classify(struct ata_device *dev, int present,\r\nu8 *r_err)\r\n{\r\nstruct ata_port *ap = dev->link->ap;\r\nstruct ata_taskfile tf;\r\nunsigned int class;\r\nu8 err;\r\nap->ops->sff_dev_select(ap, dev->devno);\r\nmemset(&tf, 0, sizeof(tf));\r\nap->ops->sff_tf_read(ap, &tf);\r\nerr = tf.feature;\r\nif (r_err)\r\n*r_err = err;\r\nif (err == 0)\r\ndev->horkage |= ATA_HORKAGE_DIAGNOSTIC;\r\nelse if (err == 1)\r\n;\r\nelse if ((dev->devno == 0) && (err == 0x81))\r\n;\r\nelse\r\nreturn ATA_DEV_NONE;\r\nclass = ata_dev_classify(&tf);\r\nif (class == ATA_DEV_UNKNOWN) {\r\nif (present && (dev->horkage & ATA_HORKAGE_DIAGNOSTIC))\r\nclass = ATA_DEV_ATA;\r\nelse\r\nclass = ATA_DEV_NONE;\r\n} else if ((class == ATA_DEV_ATA) &&\r\n(ap->ops->sff_check_status(ap) == 0))\r\nclass = ATA_DEV_NONE;\r\nreturn class;\r\n}\r\nint ata_sff_wait_after_reset(struct ata_link *link, unsigned int devmask,\r\nunsigned long deadline)\r\n{\r\nstruct ata_port *ap = link->ap;\r\nstruct ata_ioports *ioaddr = &ap->ioaddr;\r\nunsigned int dev0 = devmask & (1 << 0);\r\nunsigned int dev1 = devmask & (1 << 1);\r\nint rc, ret = 0;\r\nata_msleep(ap, ATA_WAIT_AFTER_RESET);\r\nrc = ata_sff_wait_ready(link, deadline);\r\nif (rc)\r\nreturn rc;\r\nif (dev1) {\r\nint i;\r\nap->ops->sff_dev_select(ap, 1);\r\nfor (i = 0; i < 2; i++) {\r\nu8 nsect, lbal;\r\nnsect = ioread8(ioaddr->nsect_addr);\r\nlbal = ioread8(ioaddr->lbal_addr);\r\nif ((nsect == 1) && (lbal == 1))\r\nbreak;\r\nata_msleep(ap, 50);\r\n}\r\nrc = ata_sff_wait_ready(link, deadline);\r\nif (rc) {\r\nif (rc != -ENODEV)\r\nreturn rc;\r\nret = rc;\r\n}\r\n}\r\nap->ops->sff_dev_select(ap, 0);\r\nif (dev1)\r\nap->ops->sff_dev_select(ap, 1);\r\nif (dev0)\r\nap->ops->sff_dev_select(ap, 0);\r\nreturn ret;\r\n}\r\nstatic int ata_bus_softreset(struct ata_port *ap, unsigned int devmask,\r\nunsigned long deadline)\r\n{\r\nstruct ata_ioports *ioaddr = &ap->ioaddr;\r\nDPRINTK("ata%u: bus reset via SRST\n", ap->print_id);\r\nif (ap->ioaddr.ctl_addr) {\r\niowrite8(ap->ctl, ioaddr->ctl_addr);\r\nudelay(20);\r\niowrite8(ap->ctl | ATA_SRST, ioaddr->ctl_addr);\r\nudelay(20);\r\niowrite8(ap->ctl, ioaddr->ctl_addr);\r\nap->last_ctl = ap->ctl;\r\n}\r\nreturn ata_sff_wait_after_reset(&ap->link, devmask, deadline);\r\n}\r\nint ata_sff_softreset(struct ata_link *link, unsigned int *classes,\r\nunsigned long deadline)\r\n{\r\nstruct ata_port *ap = link->ap;\r\nunsigned int slave_possible = ap->flags & ATA_FLAG_SLAVE_POSS;\r\nunsigned int devmask = 0;\r\nint rc;\r\nu8 err;\r\nDPRINTK("ENTER\n");\r\nif (ata_devchk(ap, 0))\r\ndevmask |= (1 << 0);\r\nif (slave_possible && ata_devchk(ap, 1))\r\ndevmask |= (1 << 1);\r\nap->ops->sff_dev_select(ap, 0);\r\nDPRINTK("about to softreset, devmask=%x\n", devmask);\r\nrc = ata_bus_softreset(ap, devmask, deadline);\r\nif (rc && (rc != -ENODEV || sata_scr_valid(link))) {\r\nata_link_err(link, "SRST failed (errno=%d)\n", rc);\r\nreturn rc;\r\n}\r\nclasses[0] = ata_sff_dev_classify(&link->device[0],\r\ndevmask & (1 << 0), &err);\r\nif (slave_possible && err != 0x81)\r\nclasses[1] = ata_sff_dev_classify(&link->device[1],\r\ndevmask & (1 << 1), &err);\r\nDPRINTK("EXIT, classes[0]=%u [1]=%u\n", classes[0], classes[1]);\r\nreturn 0;\r\n}\r\nint sata_sff_hardreset(struct ata_link *link, unsigned int *class,\r\nunsigned long deadline)\r\n{\r\nstruct ata_eh_context *ehc = &link->eh_context;\r\nconst unsigned long *timing = sata_ehc_deb_timing(ehc);\r\nbool online;\r\nint rc;\r\nrc = sata_link_hardreset(link, timing, deadline, &online,\r\nata_sff_check_ready);\r\nif (online)\r\n*class = ata_sff_dev_classify(link->device, 1, NULL);\r\nDPRINTK("EXIT, class=%u\n", *class);\r\nreturn rc;\r\n}\r\nvoid ata_sff_postreset(struct ata_link *link, unsigned int *classes)\r\n{\r\nstruct ata_port *ap = link->ap;\r\nata_std_postreset(link, classes);\r\nif (classes[0] != ATA_DEV_NONE)\r\nap->ops->sff_dev_select(ap, 1);\r\nif (classes[1] != ATA_DEV_NONE)\r\nap->ops->sff_dev_select(ap, 0);\r\nif (classes[0] == ATA_DEV_NONE && classes[1] == ATA_DEV_NONE) {\r\nDPRINTK("EXIT, no device\n");\r\nreturn;\r\n}\r\nif (ap->ops->sff_set_devctl || ap->ioaddr.ctl_addr) {\r\nata_sff_set_devctl(ap, ap->ctl);\r\nap->last_ctl = ap->ctl;\r\n}\r\n}\r\nvoid ata_sff_drain_fifo(struct ata_queued_cmd *qc)\r\n{\r\nint count;\r\nstruct ata_port *ap;\r\nif (qc == NULL || qc->dma_dir == DMA_TO_DEVICE)\r\nreturn;\r\nap = qc->ap;\r\nfor (count = 0; (ap->ops->sff_check_status(ap) & ATA_DRQ)\r\n&& count < 65536; count += 2)\r\nioread16(ap->ioaddr.data_addr);\r\nif (count)\r\nata_port_dbg(ap, "drained %d bytes to clear DRQ\n", count);\r\n}\r\nvoid ata_sff_error_handler(struct ata_port *ap)\r\n{\r\nata_reset_fn_t softreset = ap->ops->softreset;\r\nata_reset_fn_t hardreset = ap->ops->hardreset;\r\nstruct ata_queued_cmd *qc;\r\nunsigned long flags;\r\nqc = __ata_qc_from_tag(ap, ap->link.active_tag);\r\nif (qc && !(qc->flags & ATA_QCFLAG_FAILED))\r\nqc = NULL;\r\nspin_lock_irqsave(ap->lock, flags);\r\nif (ap->ops->sff_drain_fifo)\r\nap->ops->sff_drain_fifo(qc);\r\nspin_unlock_irqrestore(ap->lock, flags);\r\nif ((hardreset == sata_std_hardreset ||\r\nhardreset == sata_sff_hardreset) && !sata_scr_valid(&ap->link))\r\nhardreset = NULL;\r\nata_do_eh(ap, ap->ops->prereset, softreset, hardreset,\r\nap->ops->postreset);\r\n}\r\nvoid ata_sff_std_ports(struct ata_ioports *ioaddr)\r\n{\r\nioaddr->data_addr = ioaddr->cmd_addr + ATA_REG_DATA;\r\nioaddr->error_addr = ioaddr->cmd_addr + ATA_REG_ERR;\r\nioaddr->feature_addr = ioaddr->cmd_addr + ATA_REG_FEATURE;\r\nioaddr->nsect_addr = ioaddr->cmd_addr + ATA_REG_NSECT;\r\nioaddr->lbal_addr = ioaddr->cmd_addr + ATA_REG_LBAL;\r\nioaddr->lbam_addr = ioaddr->cmd_addr + ATA_REG_LBAM;\r\nioaddr->lbah_addr = ioaddr->cmd_addr + ATA_REG_LBAH;\r\nioaddr->device_addr = ioaddr->cmd_addr + ATA_REG_DEVICE;\r\nioaddr->status_addr = ioaddr->cmd_addr + ATA_REG_STATUS;\r\nioaddr->command_addr = ioaddr->cmd_addr + ATA_REG_CMD;\r\n}\r\nstatic int ata_resources_present(struct pci_dev *pdev, int port)\r\n{\r\nint i;\r\nport = port * 2;\r\nfor (i = 0; i < 2; i++) {\r\nif (pci_resource_start(pdev, port + i) == 0 ||\r\npci_resource_len(pdev, port + i) == 0)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nint ata_pci_sff_init_host(struct ata_host *host)\r\n{\r\nstruct device *gdev = host->dev;\r\nstruct pci_dev *pdev = to_pci_dev(gdev);\r\nunsigned int mask = 0;\r\nint i, rc;\r\nfor (i = 0; i < 2; i++) {\r\nstruct ata_port *ap = host->ports[i];\r\nint base = i * 2;\r\nvoid __iomem * const *iomap;\r\nif (ata_port_is_dummy(ap))\r\ncontinue;\r\nif (!ata_resources_present(pdev, i)) {\r\nap->ops = &ata_dummy_port_ops;\r\ncontinue;\r\n}\r\nrc = pcim_iomap_regions(pdev, 0x3 << base,\r\ndev_driver_string(gdev));\r\nif (rc) {\r\ndev_warn(gdev,\r\n"failed to request/iomap BARs for port %d (errno=%d)\n",\r\ni, rc);\r\nif (rc == -EBUSY)\r\npcim_pin_device(pdev);\r\nap->ops = &ata_dummy_port_ops;\r\ncontinue;\r\n}\r\nhost->iomap = iomap = pcim_iomap_table(pdev);\r\nap->ioaddr.cmd_addr = iomap[base];\r\nap->ioaddr.altstatus_addr =\r\nap->ioaddr.ctl_addr = (void __iomem *)\r\n((unsigned long)iomap[base + 1] | ATA_PCI_CTL_OFS);\r\nata_sff_std_ports(&ap->ioaddr);\r\nata_port_desc(ap, "cmd 0x%llx ctl 0x%llx",\r\n(unsigned long long)pci_resource_start(pdev, base),\r\n(unsigned long long)pci_resource_start(pdev, base + 1));\r\nmask |= 1 << i;\r\n}\r\nif (!mask) {\r\ndev_err(gdev, "no available native port\n");\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nint ata_pci_sff_prepare_host(struct pci_dev *pdev,\r\nconst struct ata_port_info * const *ppi,\r\nstruct ata_host **r_host)\r\n{\r\nstruct ata_host *host;\r\nint rc;\r\nif (!devres_open_group(&pdev->dev, NULL, GFP_KERNEL))\r\nreturn -ENOMEM;\r\nhost = ata_host_alloc_pinfo(&pdev->dev, ppi, 2);\r\nif (!host) {\r\ndev_err(&pdev->dev, "failed to allocate ATA host\n");\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nrc = ata_pci_sff_init_host(host);\r\nif (rc)\r\ngoto err_out;\r\ndevres_remove_group(&pdev->dev, NULL);\r\n*r_host = host;\r\nreturn 0;\r\nerr_out:\r\ndevres_release_group(&pdev->dev, NULL);\r\nreturn rc;\r\n}\r\nint ata_pci_sff_activate_host(struct ata_host *host,\r\nirq_handler_t irq_handler,\r\nstruct scsi_host_template *sht)\r\n{\r\nstruct device *dev = host->dev;\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nconst char *drv_name = dev_driver_string(host->dev);\r\nint legacy_mode = 0, rc;\r\nrc = ata_host_start(host);\r\nif (rc)\r\nreturn rc;\r\nif ((pdev->class >> 8) == PCI_CLASS_STORAGE_IDE) {\r\nu8 tmp8, mask;\r\npci_read_config_byte(pdev, PCI_CLASS_PROG, &tmp8);\r\nmask = (1 << 2) | (1 << 0);\r\nif ((tmp8 & mask) != mask)\r\nlegacy_mode = 1;\r\n}\r\nif (!devres_open_group(dev, NULL, GFP_KERNEL))\r\nreturn -ENOMEM;\r\nif (!legacy_mode && pdev->irq) {\r\nint i;\r\nrc = devm_request_irq(dev, pdev->irq, irq_handler,\r\nIRQF_SHARED, drv_name, host);\r\nif (rc)\r\ngoto out;\r\nfor (i = 0; i < 2; i++) {\r\nif (ata_port_is_dummy(host->ports[i]))\r\ncontinue;\r\nata_port_desc(host->ports[i], "irq %d", pdev->irq);\r\n}\r\n} else if (legacy_mode) {\r\nif (!ata_port_is_dummy(host->ports[0])) {\r\nrc = devm_request_irq(dev, ATA_PRIMARY_IRQ(pdev),\r\nirq_handler, IRQF_SHARED,\r\ndrv_name, host);\r\nif (rc)\r\ngoto out;\r\nata_port_desc(host->ports[0], "irq %d",\r\nATA_PRIMARY_IRQ(pdev));\r\n}\r\nif (!ata_port_is_dummy(host->ports[1])) {\r\nrc = devm_request_irq(dev, ATA_SECONDARY_IRQ(pdev),\r\nirq_handler, IRQF_SHARED,\r\ndrv_name, host);\r\nif (rc)\r\ngoto out;\r\nata_port_desc(host->ports[1], "irq %d",\r\nATA_SECONDARY_IRQ(pdev));\r\n}\r\n}\r\nrc = ata_host_register(host, sht);\r\nout:\r\nif (rc == 0)\r\ndevres_remove_group(dev, NULL);\r\nelse\r\ndevres_release_group(dev, NULL);\r\nreturn rc;\r\n}\r\nstatic const struct ata_port_info *ata_sff_find_valid_pi(\r\nconst struct ata_port_info * const *ppi)\r\n{\r\nint i;\r\nfor (i = 0; i < 2 && ppi[i]; i++)\r\nif (ppi[i]->port_ops != &ata_dummy_port_ops)\r\nreturn ppi[i];\r\nreturn NULL;\r\n}\r\nstatic int ata_pci_init_one(struct pci_dev *pdev,\r\nconst struct ata_port_info * const *ppi,\r\nstruct scsi_host_template *sht, void *host_priv,\r\nint hflags, bool bmdma)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nconst struct ata_port_info *pi;\r\nstruct ata_host *host = NULL;\r\nint rc;\r\nDPRINTK("ENTER\n");\r\npi = ata_sff_find_valid_pi(ppi);\r\nif (!pi) {\r\ndev_err(&pdev->dev, "no valid port_info specified\n");\r\nreturn -EINVAL;\r\n}\r\nif (!devres_open_group(dev, NULL, GFP_KERNEL))\r\nreturn -ENOMEM;\r\nrc = pcim_enable_device(pdev);\r\nif (rc)\r\ngoto out;\r\n#ifdef CONFIG_ATA_BMDMA\r\nif (bmdma)\r\nrc = ata_pci_bmdma_prepare_host(pdev, ppi, &host);\r\nelse\r\n#endif\r\nrc = ata_pci_sff_prepare_host(pdev, ppi, &host);\r\nif (rc)\r\ngoto out;\r\nhost->private_data = host_priv;\r\nhost->flags |= hflags;\r\n#ifdef CONFIG_ATA_BMDMA\r\nif (bmdma) {\r\npci_set_master(pdev);\r\nrc = ata_pci_sff_activate_host(host, ata_bmdma_interrupt, sht);\r\n} else\r\n#endif\r\nrc = ata_pci_sff_activate_host(host, ata_sff_interrupt, sht);\r\nout:\r\nif (rc == 0)\r\ndevres_remove_group(&pdev->dev, NULL);\r\nelse\r\ndevres_release_group(&pdev->dev, NULL);\r\nreturn rc;\r\n}\r\nint ata_pci_sff_init_one(struct pci_dev *pdev,\r\nconst struct ata_port_info * const *ppi,\r\nstruct scsi_host_template *sht, void *host_priv, int hflag)\r\n{\r\nreturn ata_pci_init_one(pdev, ppi, sht, host_priv, hflag, 0);\r\n}\r\nstatic void ata_bmdma_fill_sg(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct ata_bmdma_prd *prd = ap->bmdma_prd;\r\nstruct scatterlist *sg;\r\nunsigned int si, pi;\r\npi = 0;\r\nfor_each_sg(qc->sg, sg, qc->n_elem, si) {\r\nu32 addr, offset;\r\nu32 sg_len, len;\r\naddr = (u32) sg_dma_address(sg);\r\nsg_len = sg_dma_len(sg);\r\nwhile (sg_len) {\r\noffset = addr & 0xffff;\r\nlen = sg_len;\r\nif ((offset + sg_len) > 0x10000)\r\nlen = 0x10000 - offset;\r\nprd[pi].addr = cpu_to_le32(addr);\r\nprd[pi].flags_len = cpu_to_le32(len & 0xffff);\r\nVPRINTK("PRD[%u] = (0x%X, 0x%X)\n", pi, addr, len);\r\npi++;\r\nsg_len -= len;\r\naddr += len;\r\n}\r\n}\r\nprd[pi - 1].flags_len |= cpu_to_le32(ATA_PRD_EOT);\r\n}\r\nstatic void ata_bmdma_fill_sg_dumb(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct ata_bmdma_prd *prd = ap->bmdma_prd;\r\nstruct scatterlist *sg;\r\nunsigned int si, pi;\r\npi = 0;\r\nfor_each_sg(qc->sg, sg, qc->n_elem, si) {\r\nu32 addr, offset;\r\nu32 sg_len, len, blen;\r\naddr = (u32) sg_dma_address(sg);\r\nsg_len = sg_dma_len(sg);\r\nwhile (sg_len) {\r\noffset = addr & 0xffff;\r\nlen = sg_len;\r\nif ((offset + sg_len) > 0x10000)\r\nlen = 0x10000 - offset;\r\nblen = len & 0xffff;\r\nprd[pi].addr = cpu_to_le32(addr);\r\nif (blen == 0) {\r\nprd[pi].flags_len = cpu_to_le32(0x8000);\r\nblen = 0x8000;\r\nprd[++pi].addr = cpu_to_le32(addr + 0x8000);\r\n}\r\nprd[pi].flags_len = cpu_to_le32(blen);\r\nVPRINTK("PRD[%u] = (0x%X, 0x%X)\n", pi, addr, len);\r\npi++;\r\nsg_len -= len;\r\naddr += len;\r\n}\r\n}\r\nprd[pi - 1].flags_len |= cpu_to_le32(ATA_PRD_EOT);\r\n}\r\nvoid ata_bmdma_qc_prep(struct ata_queued_cmd *qc)\r\n{\r\nif (!(qc->flags & ATA_QCFLAG_DMAMAP))\r\nreturn;\r\nata_bmdma_fill_sg(qc);\r\n}\r\nvoid ata_bmdma_dumb_qc_prep(struct ata_queued_cmd *qc)\r\n{\r\nif (!(qc->flags & ATA_QCFLAG_DMAMAP))\r\nreturn;\r\nata_bmdma_fill_sg_dumb(qc);\r\n}\r\nunsigned int ata_bmdma_qc_issue(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nstruct ata_link *link = qc->dev->link;\r\nif (!ata_is_dma(qc->tf.protocol))\r\nreturn ata_sff_qc_issue(qc);\r\nata_dev_select(ap, qc->dev->devno, 1, 0);\r\nswitch (qc->tf.protocol) {\r\ncase ATA_PROT_DMA:\r\nWARN_ON_ONCE(qc->tf.flags & ATA_TFLAG_POLLING);\r\nap->ops->sff_tf_load(ap, &qc->tf);\r\nap->ops->bmdma_setup(qc);\r\nap->ops->bmdma_start(qc);\r\nap->hsm_task_state = HSM_ST_LAST;\r\nbreak;\r\ncase ATAPI_PROT_DMA:\r\nWARN_ON_ONCE(qc->tf.flags & ATA_TFLAG_POLLING);\r\nap->ops->sff_tf_load(ap, &qc->tf);\r\nap->ops->bmdma_setup(qc);\r\nap->hsm_task_state = HSM_ST_FIRST;\r\nif (!(qc->dev->flags & ATA_DFLAG_CDB_INTR))\r\nata_sff_queue_pio_task(link, 0);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\nreturn AC_ERR_SYSTEM;\r\n}\r\nreturn 0;\r\n}\r\nunsigned int ata_bmdma_port_intr(struct ata_port *ap, struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_eh_info *ehi = &ap->link.eh_info;\r\nu8 host_stat = 0;\r\nbool bmdma_stopped = false;\r\nunsigned int handled;\r\nif (ap->hsm_task_state == HSM_ST_LAST && ata_is_dma(qc->tf.protocol)) {\r\nhost_stat = ap->ops->bmdma_status(ap);\r\nVPRINTK("ata%u: host_stat 0x%X\n", ap->print_id, host_stat);\r\nif (!(host_stat & ATA_DMA_INTR))\r\nreturn ata_sff_idle_irq(ap);\r\nap->ops->bmdma_stop(qc);\r\nbmdma_stopped = true;\r\nif (unlikely(host_stat & ATA_DMA_ERR)) {\r\nqc->err_mask |= AC_ERR_HOST_BUS;\r\nap->hsm_task_state = HSM_ST_ERR;\r\n}\r\n}\r\nhandled = __ata_sff_port_intr(ap, qc, bmdma_stopped);\r\nif (unlikely(qc->err_mask) && ata_is_dma(qc->tf.protocol))\r\nata_ehi_push_desc(ehi, "BMDMA stat 0x%x", host_stat);\r\nreturn handled;\r\n}\r\nirqreturn_t ata_bmdma_interrupt(int irq, void *dev_instance)\r\n{\r\nreturn __ata_sff_interrupt(irq, dev_instance, ata_bmdma_port_intr);\r\n}\r\nvoid ata_bmdma_error_handler(struct ata_port *ap)\r\n{\r\nstruct ata_queued_cmd *qc;\r\nunsigned long flags;\r\nbool thaw = false;\r\nqc = __ata_qc_from_tag(ap, ap->link.active_tag);\r\nif (qc && !(qc->flags & ATA_QCFLAG_FAILED))\r\nqc = NULL;\r\nspin_lock_irqsave(ap->lock, flags);\r\nif (qc && ata_is_dma(qc->tf.protocol)) {\r\nu8 host_stat;\r\nhost_stat = ap->ops->bmdma_status(ap);\r\nif (qc->err_mask == AC_ERR_TIMEOUT && (host_stat & ATA_DMA_ERR)) {\r\nqc->err_mask = AC_ERR_HOST_BUS;\r\nthaw = true;\r\n}\r\nap->ops->bmdma_stop(qc);\r\nif (thaw) {\r\nap->ops->sff_check_status(ap);\r\nif (ap->ops->sff_irq_clear)\r\nap->ops->sff_irq_clear(ap);\r\n}\r\n}\r\nspin_unlock_irqrestore(ap->lock, flags);\r\nif (thaw)\r\nata_eh_thaw_port(ap);\r\nata_sff_error_handler(ap);\r\n}\r\nvoid ata_bmdma_post_internal_cmd(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nunsigned long flags;\r\nif (ata_is_dma(qc->tf.protocol)) {\r\nspin_lock_irqsave(ap->lock, flags);\r\nap->ops->bmdma_stop(qc);\r\nspin_unlock_irqrestore(ap->lock, flags);\r\n}\r\n}\r\nvoid ata_bmdma_irq_clear(struct ata_port *ap)\r\n{\r\nvoid __iomem *mmio = ap->ioaddr.bmdma_addr;\r\nif (!mmio)\r\nreturn;\r\niowrite8(ioread8(mmio + ATA_DMA_STATUS), mmio + ATA_DMA_STATUS);\r\n}\r\nvoid ata_bmdma_setup(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nunsigned int rw = (qc->tf.flags & ATA_TFLAG_WRITE);\r\nu8 dmactl;\r\nmb();\r\niowrite32(ap->bmdma_prd_dma, ap->ioaddr.bmdma_addr + ATA_DMA_TABLE_OFS);\r\ndmactl = ioread8(ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\r\ndmactl &= ~(ATA_DMA_WR | ATA_DMA_START);\r\nif (!rw)\r\ndmactl |= ATA_DMA_WR;\r\niowrite8(dmactl, ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\r\nap->ops->sff_exec_command(ap, &qc->tf);\r\n}\r\nvoid ata_bmdma_start(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nu8 dmactl;\r\ndmactl = ioread8(ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\r\niowrite8(dmactl | ATA_DMA_START, ap->ioaddr.bmdma_addr + ATA_DMA_CMD);\r\n}\r\nvoid ata_bmdma_stop(struct ata_queued_cmd *qc)\r\n{\r\nstruct ata_port *ap = qc->ap;\r\nvoid __iomem *mmio = ap->ioaddr.bmdma_addr;\r\niowrite8(ioread8(mmio + ATA_DMA_CMD) & ~ATA_DMA_START,\r\nmmio + ATA_DMA_CMD);\r\nata_sff_dma_pause(ap);\r\n}\r\nu8 ata_bmdma_status(struct ata_port *ap)\r\n{\r\nreturn ioread8(ap->ioaddr.bmdma_addr + ATA_DMA_STATUS);\r\n}\r\nint ata_bmdma_port_start(struct ata_port *ap)\r\n{\r\nif (ap->mwdma_mask || ap->udma_mask) {\r\nap->bmdma_prd =\r\ndmam_alloc_coherent(ap->host->dev, ATA_PRD_TBL_SZ,\r\n&ap->bmdma_prd_dma, GFP_KERNEL);\r\nif (!ap->bmdma_prd)\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nint ata_bmdma_port_start32(struct ata_port *ap)\r\n{\r\nap->pflags |= ATA_PFLAG_PIO32 | ATA_PFLAG_PIO32CHANGE;\r\nreturn ata_bmdma_port_start(ap);\r\n}\r\nint ata_pci_bmdma_clear_simplex(struct pci_dev *pdev)\r\n{\r\nunsigned long bmdma = pci_resource_start(pdev, 4);\r\nu8 simplex;\r\nif (bmdma == 0)\r\nreturn -ENOENT;\r\nsimplex = inb(bmdma + 0x02);\r\noutb(simplex & 0x60, bmdma + 0x02);\r\nsimplex = inb(bmdma + 0x02);\r\nif (simplex & 0x80)\r\nreturn -EOPNOTSUPP;\r\nreturn 0;\r\n}\r\nstatic void ata_bmdma_nodma(struct ata_host *host, const char *reason)\r\n{\r\nint i;\r\ndev_err(host->dev, "BMDMA: %s, falling back to PIO\n", reason);\r\nfor (i = 0; i < 2; i++) {\r\nhost->ports[i]->mwdma_mask = 0;\r\nhost->ports[i]->udma_mask = 0;\r\n}\r\n}\r\nvoid ata_pci_bmdma_init(struct ata_host *host)\r\n{\r\nstruct device *gdev = host->dev;\r\nstruct pci_dev *pdev = to_pci_dev(gdev);\r\nint i, rc;\r\nif (pci_resource_start(pdev, 4) == 0) {\r\nata_bmdma_nodma(host, "BAR4 is zero");\r\nreturn;\r\n}\r\nrc = dma_set_mask(&pdev->dev, ATA_DMA_MASK);\r\nif (rc)\r\nata_bmdma_nodma(host, "failed to set dma mask");\r\nif (!rc) {\r\nrc = dma_set_coherent_mask(&pdev->dev, ATA_DMA_MASK);\r\nif (rc)\r\nata_bmdma_nodma(host,\r\n"failed to set consistent dma mask");\r\n}\r\nrc = pcim_iomap_regions(pdev, 1 << 4, dev_driver_string(gdev));\r\nif (rc) {\r\nata_bmdma_nodma(host, "failed to request/iomap BAR4");\r\nreturn;\r\n}\r\nhost->iomap = pcim_iomap_table(pdev);\r\nfor (i = 0; i < 2; i++) {\r\nstruct ata_port *ap = host->ports[i];\r\nvoid __iomem *bmdma = host->iomap[4] + 8 * i;\r\nif (ata_port_is_dummy(ap))\r\ncontinue;\r\nap->ioaddr.bmdma_addr = bmdma;\r\nif ((!(ap->flags & ATA_FLAG_IGN_SIMPLEX)) &&\r\n(ioread8(bmdma + 2) & 0x80))\r\nhost->flags |= ATA_HOST_SIMPLEX;\r\nata_port_desc(ap, "bmdma 0x%llx",\r\n(unsigned long long)pci_resource_start(pdev, 4) + 8 * i);\r\n}\r\n}\r\nint ata_pci_bmdma_prepare_host(struct pci_dev *pdev,\r\nconst struct ata_port_info * const * ppi,\r\nstruct ata_host **r_host)\r\n{\r\nint rc;\r\nrc = ata_pci_sff_prepare_host(pdev, ppi, r_host);\r\nif (rc)\r\nreturn rc;\r\nata_pci_bmdma_init(*r_host);\r\nreturn 0;\r\n}\r\nint ata_pci_bmdma_init_one(struct pci_dev *pdev,\r\nconst struct ata_port_info * const * ppi,\r\nstruct scsi_host_template *sht, void *host_priv,\r\nint hflags)\r\n{\r\nreturn ata_pci_init_one(pdev, ppi, sht, host_priv, hflags, 1);\r\n}\r\nvoid ata_sff_port_init(struct ata_port *ap)\r\n{\r\nINIT_DELAYED_WORK(&ap->sff_pio_task, ata_sff_pio_task);\r\nap->ctl = ATA_DEVCTL_OBS;\r\nap->last_ctl = 0xFF;\r\n}\r\nint __init ata_sff_init(void)\r\n{\r\nata_sff_wq = alloc_workqueue("ata_sff", WQ_MEM_RECLAIM, WQ_MAX_ACTIVE);\r\nif (!ata_sff_wq)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid ata_sff_exit(void)\r\n{\r\ndestroy_workqueue(ata_sff_wq);\r\n}
