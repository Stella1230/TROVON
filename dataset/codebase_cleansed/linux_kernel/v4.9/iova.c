void\r\ninit_iova_domain(struct iova_domain *iovad, unsigned long granule,\r\nunsigned long start_pfn, unsigned long pfn_32bit)\r\n{\r\nBUG_ON((granule > PAGE_SIZE) || !is_power_of_2(granule));\r\nspin_lock_init(&iovad->iova_rbtree_lock);\r\niovad->rbroot = RB_ROOT;\r\niovad->cached32_node = NULL;\r\niovad->granule = granule;\r\niovad->start_pfn = start_pfn;\r\niovad->dma_32bit_pfn = pfn_32bit;\r\ninit_iova_rcaches(iovad);\r\n}\r\nstatic struct rb_node *\r\n__get_cached_rbnode(struct iova_domain *iovad, unsigned long *limit_pfn)\r\n{\r\nif ((*limit_pfn != iovad->dma_32bit_pfn) ||\r\n(iovad->cached32_node == NULL))\r\nreturn rb_last(&iovad->rbroot);\r\nelse {\r\nstruct rb_node *prev_node = rb_prev(iovad->cached32_node);\r\nstruct iova *curr_iova =\r\ncontainer_of(iovad->cached32_node, struct iova, node);\r\n*limit_pfn = curr_iova->pfn_lo - 1;\r\nreturn prev_node;\r\n}\r\n}\r\nstatic void\r\n__cached_rbnode_insert_update(struct iova_domain *iovad,\r\nunsigned long limit_pfn, struct iova *new)\r\n{\r\nif (limit_pfn != iovad->dma_32bit_pfn)\r\nreturn;\r\niovad->cached32_node = &new->node;\r\n}\r\nstatic void\r\n__cached_rbnode_delete_update(struct iova_domain *iovad, struct iova *free)\r\n{\r\nstruct iova *cached_iova;\r\nstruct rb_node *curr;\r\nif (!iovad->cached32_node)\r\nreturn;\r\ncurr = iovad->cached32_node;\r\ncached_iova = container_of(curr, struct iova, node);\r\nif (free->pfn_lo >= cached_iova->pfn_lo) {\r\nstruct rb_node *node = rb_next(&free->node);\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nif (node && iova->pfn_lo < iovad->dma_32bit_pfn)\r\niovad->cached32_node = node;\r\nelse\r\niovad->cached32_node = NULL;\r\n}\r\n}\r\nstatic unsigned int\r\niova_get_pad_size(unsigned int size, unsigned int limit_pfn)\r\n{\r\nreturn (limit_pfn + 1 - size) & (__roundup_pow_of_two(size) - 1);\r\n}\r\nstatic int __alloc_and_insert_iova_range(struct iova_domain *iovad,\r\nunsigned long size, unsigned long limit_pfn,\r\nstruct iova *new, bool size_aligned)\r\n{\r\nstruct rb_node *prev, *curr = NULL;\r\nunsigned long flags;\r\nunsigned long saved_pfn;\r\nunsigned int pad_size = 0;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nsaved_pfn = limit_pfn;\r\ncurr = __get_cached_rbnode(iovad, &limit_pfn);\r\nprev = curr;\r\nwhile (curr) {\r\nstruct iova *curr_iova = container_of(curr, struct iova, node);\r\nif (limit_pfn < curr_iova->pfn_lo)\r\ngoto move_left;\r\nelse if (limit_pfn < curr_iova->pfn_hi)\r\ngoto adjust_limit_pfn;\r\nelse {\r\nif (size_aligned)\r\npad_size = iova_get_pad_size(size, limit_pfn);\r\nif ((curr_iova->pfn_hi + size + pad_size) <= limit_pfn)\r\nbreak;\r\n}\r\nadjust_limit_pfn:\r\nlimit_pfn = curr_iova->pfn_lo - 1;\r\nmove_left:\r\nprev = curr;\r\ncurr = rb_prev(curr);\r\n}\r\nif (!curr) {\r\nif (size_aligned)\r\npad_size = iova_get_pad_size(size, limit_pfn);\r\nif ((iovad->start_pfn + size + pad_size) > limit_pfn) {\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nnew->pfn_lo = limit_pfn - (size + pad_size) + 1;\r\nnew->pfn_hi = new->pfn_lo + size - 1;\r\n{\r\nstruct rb_node **entry, *parent = NULL;\r\nif (prev)\r\nentry = &prev;\r\nelse\r\nentry = &iovad->rbroot.rb_node;\r\nwhile (*entry) {\r\nstruct iova *this = container_of(*entry,\r\nstruct iova, node);\r\nparent = *entry;\r\nif (new->pfn_lo < this->pfn_lo)\r\nentry = &((*entry)->rb_left);\r\nelse if (new->pfn_lo > this->pfn_lo)\r\nentry = &((*entry)->rb_right);\r\nelse\r\nBUG();\r\n}\r\nrb_link_node(&new->node, parent, entry);\r\nrb_insert_color(&new->node, &iovad->rbroot);\r\n}\r\n__cached_rbnode_insert_update(iovad, saved_pfn, new);\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn 0;\r\n}\r\nstatic void\r\niova_insert_rbtree(struct rb_root *root, struct iova *iova)\r\n{\r\nstruct rb_node **new = &(root->rb_node), *parent = NULL;\r\nwhile (*new) {\r\nstruct iova *this = container_of(*new, struct iova, node);\r\nparent = *new;\r\nif (iova->pfn_lo < this->pfn_lo)\r\nnew = &((*new)->rb_left);\r\nelse if (iova->pfn_lo > this->pfn_lo)\r\nnew = &((*new)->rb_right);\r\nelse\r\nBUG();\r\n}\r\nrb_link_node(&iova->node, parent, new);\r\nrb_insert_color(&iova->node, root);\r\n}\r\nstruct iova *alloc_iova_mem(void)\r\n{\r\nreturn kmem_cache_alloc(iova_cache, GFP_ATOMIC);\r\n}\r\nvoid free_iova_mem(struct iova *iova)\r\n{\r\nkmem_cache_free(iova_cache, iova);\r\n}\r\nint iova_cache_get(void)\r\n{\r\nmutex_lock(&iova_cache_mutex);\r\nif (!iova_cache_users) {\r\niova_cache = kmem_cache_create(\r\n"iommu_iova", sizeof(struct iova), 0,\r\nSLAB_HWCACHE_ALIGN, NULL);\r\nif (!iova_cache) {\r\nmutex_unlock(&iova_cache_mutex);\r\nprintk(KERN_ERR "Couldn't create iova cache\n");\r\nreturn -ENOMEM;\r\n}\r\n}\r\niova_cache_users++;\r\nmutex_unlock(&iova_cache_mutex);\r\nreturn 0;\r\n}\r\nvoid iova_cache_put(void)\r\n{\r\nmutex_lock(&iova_cache_mutex);\r\nif (WARN_ON(!iova_cache_users)) {\r\nmutex_unlock(&iova_cache_mutex);\r\nreturn;\r\n}\r\niova_cache_users--;\r\nif (!iova_cache_users)\r\nkmem_cache_destroy(iova_cache);\r\nmutex_unlock(&iova_cache_mutex);\r\n}\r\nstruct iova *\r\nalloc_iova(struct iova_domain *iovad, unsigned long size,\r\nunsigned long limit_pfn,\r\nbool size_aligned)\r\n{\r\nstruct iova *new_iova;\r\nint ret;\r\nnew_iova = alloc_iova_mem();\r\nif (!new_iova)\r\nreturn NULL;\r\nret = __alloc_and_insert_iova_range(iovad, size, limit_pfn,\r\nnew_iova, size_aligned);\r\nif (ret) {\r\nfree_iova_mem(new_iova);\r\nreturn NULL;\r\n}\r\nreturn new_iova;\r\n}\r\nstatic struct iova *\r\nprivate_find_iova(struct iova_domain *iovad, unsigned long pfn)\r\n{\r\nstruct rb_node *node = iovad->rbroot.rb_node;\r\nassert_spin_locked(&iovad->iova_rbtree_lock);\r\nwhile (node) {\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nif ((pfn >= iova->pfn_lo) && (pfn <= iova->pfn_hi)) {\r\nreturn iova;\r\n}\r\nif (pfn < iova->pfn_lo)\r\nnode = node->rb_left;\r\nelse if (pfn > iova->pfn_lo)\r\nnode = node->rb_right;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void private_free_iova(struct iova_domain *iovad, struct iova *iova)\r\n{\r\nassert_spin_locked(&iovad->iova_rbtree_lock);\r\n__cached_rbnode_delete_update(iovad, iova);\r\nrb_erase(&iova->node, &iovad->rbroot);\r\nfree_iova_mem(iova);\r\n}\r\nstruct iova *find_iova(struct iova_domain *iovad, unsigned long pfn)\r\n{\r\nunsigned long flags;\r\nstruct iova *iova;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\niova = private_find_iova(iovad, pfn);\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn iova;\r\n}\r\nvoid\r\n__free_iova(struct iova_domain *iovad, struct iova *iova)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nprivate_free_iova(iovad, iova);\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\n}\r\nvoid\r\nfree_iova(struct iova_domain *iovad, unsigned long pfn)\r\n{\r\nstruct iova *iova = find_iova(iovad, pfn);\r\nif (iova)\r\n__free_iova(iovad, iova);\r\n}\r\nunsigned long\r\nalloc_iova_fast(struct iova_domain *iovad, unsigned long size,\r\nunsigned long limit_pfn)\r\n{\r\nbool flushed_rcache = false;\r\nunsigned long iova_pfn;\r\nstruct iova *new_iova;\r\niova_pfn = iova_rcache_get(iovad, size, limit_pfn);\r\nif (iova_pfn)\r\nreturn iova_pfn;\r\nretry:\r\nnew_iova = alloc_iova(iovad, size, limit_pfn, true);\r\nif (!new_iova) {\r\nunsigned int cpu;\r\nif (flushed_rcache)\r\nreturn 0;\r\nflushed_rcache = true;\r\npreempt_disable();\r\nfor_each_online_cpu(cpu)\r\nfree_cpu_cached_iovas(cpu, iovad);\r\npreempt_enable();\r\ngoto retry;\r\n}\r\nreturn new_iova->pfn_lo;\r\n}\r\nvoid\r\nfree_iova_fast(struct iova_domain *iovad, unsigned long pfn, unsigned long size)\r\n{\r\nif (iova_rcache_insert(iovad, pfn, size))\r\nreturn;\r\nfree_iova(iovad, pfn);\r\n}\r\nvoid put_iova_domain(struct iova_domain *iovad)\r\n{\r\nstruct rb_node *node;\r\nunsigned long flags;\r\nfree_iova_rcaches(iovad);\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nnode = rb_first(&iovad->rbroot);\r\nwhile (node) {\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nrb_erase(node, &iovad->rbroot);\r\nfree_iova_mem(iova);\r\nnode = rb_first(&iovad->rbroot);\r\n}\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\n}\r\nstatic int\r\n__is_range_overlap(struct rb_node *node,\r\nunsigned long pfn_lo, unsigned long pfn_hi)\r\n{\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nif ((pfn_lo <= iova->pfn_hi) && (pfn_hi >= iova->pfn_lo))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic inline struct iova *\r\nalloc_and_init_iova(unsigned long pfn_lo, unsigned long pfn_hi)\r\n{\r\nstruct iova *iova;\r\niova = alloc_iova_mem();\r\nif (iova) {\r\niova->pfn_lo = pfn_lo;\r\niova->pfn_hi = pfn_hi;\r\n}\r\nreturn iova;\r\n}\r\nstatic struct iova *\r\n__insert_new_range(struct iova_domain *iovad,\r\nunsigned long pfn_lo, unsigned long pfn_hi)\r\n{\r\nstruct iova *iova;\r\niova = alloc_and_init_iova(pfn_lo, pfn_hi);\r\nif (iova)\r\niova_insert_rbtree(&iovad->rbroot, iova);\r\nreturn iova;\r\n}\r\nstatic void\r\n__adjust_overlap_range(struct iova *iova,\r\nunsigned long *pfn_lo, unsigned long *pfn_hi)\r\n{\r\nif (*pfn_lo < iova->pfn_lo)\r\niova->pfn_lo = *pfn_lo;\r\nif (*pfn_hi > iova->pfn_hi)\r\n*pfn_lo = iova->pfn_hi + 1;\r\n}\r\nstruct iova *\r\nreserve_iova(struct iova_domain *iovad,\r\nunsigned long pfn_lo, unsigned long pfn_hi)\r\n{\r\nstruct rb_node *node;\r\nunsigned long flags;\r\nstruct iova *iova;\r\nunsigned int overlap = 0;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nfor (node = rb_first(&iovad->rbroot); node; node = rb_next(node)) {\r\nif (__is_range_overlap(node, pfn_lo, pfn_hi)) {\r\niova = container_of(node, struct iova, node);\r\n__adjust_overlap_range(iova, &pfn_lo, &pfn_hi);\r\nif ((pfn_lo >= iova->pfn_lo) &&\r\n(pfn_hi <= iova->pfn_hi))\r\ngoto finish;\r\noverlap = 1;\r\n} else if (overlap)\r\nbreak;\r\n}\r\niova = __insert_new_range(iovad, pfn_lo, pfn_hi);\r\nfinish:\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn iova;\r\n}\r\nvoid\r\ncopy_reserved_iova(struct iova_domain *from, struct iova_domain *to)\r\n{\r\nunsigned long flags;\r\nstruct rb_node *node;\r\nspin_lock_irqsave(&from->iova_rbtree_lock, flags);\r\nfor (node = rb_first(&from->rbroot); node; node = rb_next(node)) {\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nstruct iova *new_iova;\r\nnew_iova = reserve_iova(to, iova->pfn_lo, iova->pfn_hi);\r\nif (!new_iova)\r\nprintk(KERN_ERR "Reserve iova range %lx@%lx failed\n",\r\niova->pfn_lo, iova->pfn_lo);\r\n}\r\nspin_unlock_irqrestore(&from->iova_rbtree_lock, flags);\r\n}\r\nstruct iova *\r\nsplit_and_remove_iova(struct iova_domain *iovad, struct iova *iova,\r\nunsigned long pfn_lo, unsigned long pfn_hi)\r\n{\r\nunsigned long flags;\r\nstruct iova *prev = NULL, *next = NULL;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nif (iova->pfn_lo < pfn_lo) {\r\nprev = alloc_and_init_iova(iova->pfn_lo, pfn_lo - 1);\r\nif (prev == NULL)\r\ngoto error;\r\n}\r\nif (iova->pfn_hi > pfn_hi) {\r\nnext = alloc_and_init_iova(pfn_hi + 1, iova->pfn_hi);\r\nif (next == NULL)\r\ngoto error;\r\n}\r\n__cached_rbnode_delete_update(iovad, iova);\r\nrb_erase(&iova->node, &iovad->rbroot);\r\nif (prev) {\r\niova_insert_rbtree(&iovad->rbroot, prev);\r\niova->pfn_lo = pfn_lo;\r\n}\r\nif (next) {\r\niova_insert_rbtree(&iovad->rbroot, next);\r\niova->pfn_hi = pfn_hi;\r\n}\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn iova;\r\nerror:\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nif (prev)\r\nfree_iova_mem(prev);\r\nreturn NULL;\r\n}\r\nstatic struct iova_magazine *iova_magazine_alloc(gfp_t flags)\r\n{\r\nreturn kzalloc(sizeof(struct iova_magazine), flags);\r\n}\r\nstatic void iova_magazine_free(struct iova_magazine *mag)\r\n{\r\nkfree(mag);\r\n}\r\nstatic void\r\niova_magazine_free_pfns(struct iova_magazine *mag, struct iova_domain *iovad)\r\n{\r\nunsigned long flags;\r\nint i;\r\nif (!mag)\r\nreturn;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nfor (i = 0 ; i < mag->size; ++i) {\r\nstruct iova *iova = private_find_iova(iovad, mag->pfns[i]);\r\nBUG_ON(!iova);\r\nprivate_free_iova(iovad, iova);\r\n}\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nmag->size = 0;\r\n}\r\nstatic bool iova_magazine_full(struct iova_magazine *mag)\r\n{\r\nreturn (mag && mag->size == IOVA_MAG_SIZE);\r\n}\r\nstatic bool iova_magazine_empty(struct iova_magazine *mag)\r\n{\r\nreturn (!mag || mag->size == 0);\r\n}\r\nstatic unsigned long iova_magazine_pop(struct iova_magazine *mag,\r\nunsigned long limit_pfn)\r\n{\r\nBUG_ON(iova_magazine_empty(mag));\r\nif (mag->pfns[mag->size - 1] >= limit_pfn)\r\nreturn 0;\r\nreturn mag->pfns[--mag->size];\r\n}\r\nstatic void iova_magazine_push(struct iova_magazine *mag, unsigned long pfn)\r\n{\r\nBUG_ON(iova_magazine_full(mag));\r\nmag->pfns[mag->size++] = pfn;\r\n}\r\nstatic void init_iova_rcaches(struct iova_domain *iovad)\r\n{\r\nstruct iova_cpu_rcache *cpu_rcache;\r\nstruct iova_rcache *rcache;\r\nunsigned int cpu;\r\nint i;\r\nfor (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {\r\nrcache = &iovad->rcaches[i];\r\nspin_lock_init(&rcache->lock);\r\nrcache->depot_size = 0;\r\nrcache->cpu_rcaches = __alloc_percpu(sizeof(*cpu_rcache), cache_line_size());\r\nif (WARN_ON(!rcache->cpu_rcaches))\r\ncontinue;\r\nfor_each_possible_cpu(cpu) {\r\ncpu_rcache = per_cpu_ptr(rcache->cpu_rcaches, cpu);\r\nspin_lock_init(&cpu_rcache->lock);\r\ncpu_rcache->loaded = iova_magazine_alloc(GFP_KERNEL);\r\ncpu_rcache->prev = iova_magazine_alloc(GFP_KERNEL);\r\n}\r\n}\r\n}\r\nstatic bool __iova_rcache_insert(struct iova_domain *iovad,\r\nstruct iova_rcache *rcache,\r\nunsigned long iova_pfn)\r\n{\r\nstruct iova_magazine *mag_to_free = NULL;\r\nstruct iova_cpu_rcache *cpu_rcache;\r\nbool can_insert = false;\r\nunsigned long flags;\r\ncpu_rcache = get_cpu_ptr(rcache->cpu_rcaches);\r\nspin_lock_irqsave(&cpu_rcache->lock, flags);\r\nif (!iova_magazine_full(cpu_rcache->loaded)) {\r\ncan_insert = true;\r\n} else if (!iova_magazine_full(cpu_rcache->prev)) {\r\nswap(cpu_rcache->prev, cpu_rcache->loaded);\r\ncan_insert = true;\r\n} else {\r\nstruct iova_magazine *new_mag = iova_magazine_alloc(GFP_ATOMIC);\r\nif (new_mag) {\r\nspin_lock(&rcache->lock);\r\nif (rcache->depot_size < MAX_GLOBAL_MAGS) {\r\nrcache->depot[rcache->depot_size++] =\r\ncpu_rcache->loaded;\r\n} else {\r\nmag_to_free = cpu_rcache->loaded;\r\n}\r\nspin_unlock(&rcache->lock);\r\ncpu_rcache->loaded = new_mag;\r\ncan_insert = true;\r\n}\r\n}\r\nif (can_insert)\r\niova_magazine_push(cpu_rcache->loaded, iova_pfn);\r\nspin_unlock_irqrestore(&cpu_rcache->lock, flags);\r\nput_cpu_ptr(rcache->cpu_rcaches);\r\nif (mag_to_free) {\r\niova_magazine_free_pfns(mag_to_free, iovad);\r\niova_magazine_free(mag_to_free);\r\n}\r\nreturn can_insert;\r\n}\r\nstatic bool iova_rcache_insert(struct iova_domain *iovad, unsigned long pfn,\r\nunsigned long size)\r\n{\r\nunsigned int log_size = order_base_2(size);\r\nif (log_size >= IOVA_RANGE_CACHE_MAX_SIZE)\r\nreturn false;\r\nreturn __iova_rcache_insert(iovad, &iovad->rcaches[log_size], pfn);\r\n}\r\nstatic unsigned long __iova_rcache_get(struct iova_rcache *rcache,\r\nunsigned long limit_pfn)\r\n{\r\nstruct iova_cpu_rcache *cpu_rcache;\r\nunsigned long iova_pfn = 0;\r\nbool has_pfn = false;\r\nunsigned long flags;\r\ncpu_rcache = get_cpu_ptr(rcache->cpu_rcaches);\r\nspin_lock_irqsave(&cpu_rcache->lock, flags);\r\nif (!iova_magazine_empty(cpu_rcache->loaded)) {\r\nhas_pfn = true;\r\n} else if (!iova_magazine_empty(cpu_rcache->prev)) {\r\nswap(cpu_rcache->prev, cpu_rcache->loaded);\r\nhas_pfn = true;\r\n} else {\r\nspin_lock(&rcache->lock);\r\nif (rcache->depot_size > 0) {\r\niova_magazine_free(cpu_rcache->loaded);\r\ncpu_rcache->loaded = rcache->depot[--rcache->depot_size];\r\nhas_pfn = true;\r\n}\r\nspin_unlock(&rcache->lock);\r\n}\r\nif (has_pfn)\r\niova_pfn = iova_magazine_pop(cpu_rcache->loaded, limit_pfn);\r\nspin_unlock_irqrestore(&cpu_rcache->lock, flags);\r\nput_cpu_ptr(rcache->cpu_rcaches);\r\nreturn iova_pfn;\r\n}\r\nstatic unsigned long iova_rcache_get(struct iova_domain *iovad,\r\nunsigned long size,\r\nunsigned long limit_pfn)\r\n{\r\nunsigned int log_size = order_base_2(size);\r\nif (log_size >= IOVA_RANGE_CACHE_MAX_SIZE)\r\nreturn 0;\r\nreturn __iova_rcache_get(&iovad->rcaches[log_size], limit_pfn);\r\n}\r\nstatic void free_cpu_iova_rcache(unsigned int cpu, struct iova_domain *iovad,\r\nstruct iova_rcache *rcache)\r\n{\r\nstruct iova_cpu_rcache *cpu_rcache = per_cpu_ptr(rcache->cpu_rcaches, cpu);\r\nunsigned long flags;\r\nspin_lock_irqsave(&cpu_rcache->lock, flags);\r\niova_magazine_free_pfns(cpu_rcache->loaded, iovad);\r\niova_magazine_free(cpu_rcache->loaded);\r\niova_magazine_free_pfns(cpu_rcache->prev, iovad);\r\niova_magazine_free(cpu_rcache->prev);\r\nspin_unlock_irqrestore(&cpu_rcache->lock, flags);\r\n}\r\nstatic void free_iova_rcaches(struct iova_domain *iovad)\r\n{\r\nstruct iova_rcache *rcache;\r\nunsigned long flags;\r\nunsigned int cpu;\r\nint i, j;\r\nfor (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {\r\nrcache = &iovad->rcaches[i];\r\nfor_each_possible_cpu(cpu)\r\nfree_cpu_iova_rcache(cpu, iovad, rcache);\r\nspin_lock_irqsave(&rcache->lock, flags);\r\nfree_percpu(rcache->cpu_rcaches);\r\nfor (j = 0; j < rcache->depot_size; ++j) {\r\niova_magazine_free_pfns(rcache->depot[j], iovad);\r\niova_magazine_free(rcache->depot[j]);\r\n}\r\nspin_unlock_irqrestore(&rcache->lock, flags);\r\n}\r\n}\r\nvoid free_cpu_cached_iovas(unsigned int cpu, struct iova_domain *iovad)\r\n{\r\nstruct iova_cpu_rcache *cpu_rcache;\r\nstruct iova_rcache *rcache;\r\nunsigned long flags;\r\nint i;\r\nfor (i = 0; i < IOVA_RANGE_CACHE_MAX_SIZE; ++i) {\r\nrcache = &iovad->rcaches[i];\r\ncpu_rcache = per_cpu_ptr(rcache->cpu_rcaches, cpu);\r\nspin_lock_irqsave(&cpu_rcache->lock, flags);\r\niova_magazine_free_pfns(cpu_rcache->loaded, iovad);\r\niova_magazine_free_pfns(cpu_rcache->prev, iovad);\r\nspin_unlock_irqrestore(&cpu_rcache->lock, flags);\r\n}\r\n}
