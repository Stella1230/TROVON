void save_v86_state(struct kernel_vm86_regs *regs, int retval)\r\n{\r\nstruct tss_struct *tss;\r\nstruct task_struct *tsk = current;\r\nstruct vm86plus_struct __user *user;\r\nstruct vm86 *vm86 = current->thread.vm86;\r\nlong err = 0;\r\nlocal_irq_enable();\r\nif (!vm86 || !vm86->user_vm86) {\r\npr_alert("no user_vm86: BAD\n");\r\ndo_exit(SIGSEGV);\r\n}\r\nset_flags(regs->pt.flags, VEFLAGS, X86_EFLAGS_VIF | vm86->veflags_mask);\r\nuser = vm86->user_vm86;\r\nif (!access_ok(VERIFY_WRITE, user, vm86->vm86plus.is_vm86pus ?\r\nsizeof(struct vm86plus_struct) :\r\nsizeof(struct vm86_struct))) {\r\npr_alert("could not access userspace vm86 info\n");\r\ndo_exit(SIGSEGV);\r\n}\r\nput_user_try {\r\nput_user_ex(regs->pt.bx, &user->regs.ebx);\r\nput_user_ex(regs->pt.cx, &user->regs.ecx);\r\nput_user_ex(regs->pt.dx, &user->regs.edx);\r\nput_user_ex(regs->pt.si, &user->regs.esi);\r\nput_user_ex(regs->pt.di, &user->regs.edi);\r\nput_user_ex(regs->pt.bp, &user->regs.ebp);\r\nput_user_ex(regs->pt.ax, &user->regs.eax);\r\nput_user_ex(regs->pt.ip, &user->regs.eip);\r\nput_user_ex(regs->pt.cs, &user->regs.cs);\r\nput_user_ex(regs->pt.flags, &user->regs.eflags);\r\nput_user_ex(regs->pt.sp, &user->regs.esp);\r\nput_user_ex(regs->pt.ss, &user->regs.ss);\r\nput_user_ex(regs->es, &user->regs.es);\r\nput_user_ex(regs->ds, &user->regs.ds);\r\nput_user_ex(regs->fs, &user->regs.fs);\r\nput_user_ex(regs->gs, &user->regs.gs);\r\nput_user_ex(vm86->screen_bitmap, &user->screen_bitmap);\r\n} put_user_catch(err);\r\nif (err) {\r\npr_alert("could not access userspace vm86 info\n");\r\ndo_exit(SIGSEGV);\r\n}\r\ntss = &per_cpu(cpu_tss, get_cpu());\r\ntsk->thread.sp0 = vm86->saved_sp0;\r\ntsk->thread.sysenter_cs = __KERNEL_CS;\r\nload_sp0(tss, &tsk->thread);\r\nvm86->saved_sp0 = 0;\r\nput_cpu();\r\nmemcpy(&regs->pt, &vm86->regs32, sizeof(struct pt_regs));\r\nlazy_load_gs(vm86->regs32.gs);\r\nregs->pt.ax = retval;\r\n}\r\nstatic void mark_screen_rdonly(struct mm_struct *mm)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\nint i;\r\ndown_write(&mm->mmap_sem);\r\npgd = pgd_offset(mm, 0xA0000);\r\nif (pgd_none_or_clear_bad(pgd))\r\ngoto out;\r\npud = pud_offset(pgd, 0xA0000);\r\nif (pud_none_or_clear_bad(pud))\r\ngoto out;\r\npmd = pmd_offset(pud, 0xA0000);\r\nif (pmd_trans_huge(*pmd)) {\r\nstruct vm_area_struct *vma = find_vma(mm, 0xA0000);\r\nsplit_huge_pmd(vma, pmd, 0xA0000);\r\n}\r\nif (pmd_none_or_clear_bad(pmd))\r\ngoto out;\r\npte = pte_offset_map_lock(mm, pmd, 0xA0000, &ptl);\r\nfor (i = 0; i < 32; i++) {\r\nif (pte_present(*pte))\r\nset_pte(pte, pte_wrprotect(*pte));\r\npte++;\r\n}\r\npte_unmap_unlock(pte, ptl);\r\nout:\r\nup_write(&mm->mmap_sem);\r\nflush_tlb();\r\n}\r\nstatic long do_sys_vm86(struct vm86plus_struct __user *user_vm86, bool plus)\r\n{\r\nstruct tss_struct *tss;\r\nstruct task_struct *tsk = current;\r\nstruct vm86 *vm86 = tsk->thread.vm86;\r\nstruct kernel_vm86_regs vm86regs;\r\nstruct pt_regs *regs = current_pt_regs();\r\nunsigned long err = 0;\r\nerr = security_mmap_addr(0);\r\nif (err) {\r\npr_info_once("Denied a call to vm86(old) from %s[%d] (uid: %d). Set the vm.mmap_min_addr sysctl to 0 and/or adjust LSM mmap_min_addr policy to enable vm86 if you are using a vm86-based DOS emulator.\n",\r\ncurrent->comm, task_pid_nr(current),\r\nfrom_kuid_munged(&init_user_ns, current_uid()));\r\nreturn -EPERM;\r\n}\r\nif (!vm86) {\r\nif (!(vm86 = kzalloc(sizeof(*vm86), GFP_KERNEL)))\r\nreturn -ENOMEM;\r\ntsk->thread.vm86 = vm86;\r\n}\r\nif (vm86->saved_sp0)\r\nreturn -EPERM;\r\nif (!access_ok(VERIFY_READ, user_vm86, plus ?\r\nsizeof(struct vm86_struct) :\r\nsizeof(struct vm86plus_struct)))\r\nreturn -EFAULT;\r\nmemset(&vm86regs, 0, sizeof(vm86regs));\r\nget_user_try {\r\nunsigned short seg;\r\nget_user_ex(vm86regs.pt.bx, &user_vm86->regs.ebx);\r\nget_user_ex(vm86regs.pt.cx, &user_vm86->regs.ecx);\r\nget_user_ex(vm86regs.pt.dx, &user_vm86->regs.edx);\r\nget_user_ex(vm86regs.pt.si, &user_vm86->regs.esi);\r\nget_user_ex(vm86regs.pt.di, &user_vm86->regs.edi);\r\nget_user_ex(vm86regs.pt.bp, &user_vm86->regs.ebp);\r\nget_user_ex(vm86regs.pt.ax, &user_vm86->regs.eax);\r\nget_user_ex(vm86regs.pt.ip, &user_vm86->regs.eip);\r\nget_user_ex(seg, &user_vm86->regs.cs);\r\nvm86regs.pt.cs = seg;\r\nget_user_ex(vm86regs.pt.flags, &user_vm86->regs.eflags);\r\nget_user_ex(vm86regs.pt.sp, &user_vm86->regs.esp);\r\nget_user_ex(seg, &user_vm86->regs.ss);\r\nvm86regs.pt.ss = seg;\r\nget_user_ex(vm86regs.es, &user_vm86->regs.es);\r\nget_user_ex(vm86regs.ds, &user_vm86->regs.ds);\r\nget_user_ex(vm86regs.fs, &user_vm86->regs.fs);\r\nget_user_ex(vm86regs.gs, &user_vm86->regs.gs);\r\nget_user_ex(vm86->flags, &user_vm86->flags);\r\nget_user_ex(vm86->screen_bitmap, &user_vm86->screen_bitmap);\r\nget_user_ex(vm86->cpu_type, &user_vm86->cpu_type);\r\n} get_user_catch(err);\r\nif (err)\r\nreturn err;\r\nif (copy_from_user(&vm86->int_revectored,\r\n&user_vm86->int_revectored,\r\nsizeof(struct revectored_struct)))\r\nreturn -EFAULT;\r\nif (copy_from_user(&vm86->int21_revectored,\r\n&user_vm86->int21_revectored,\r\nsizeof(struct revectored_struct)))\r\nreturn -EFAULT;\r\nif (plus) {\r\nif (copy_from_user(&vm86->vm86plus, &user_vm86->vm86plus,\r\nsizeof(struct vm86plus_info_struct)))\r\nreturn -EFAULT;\r\nvm86->vm86plus.is_vm86pus = 1;\r\n} else\r\nmemset(&vm86->vm86plus, 0,\r\nsizeof(struct vm86plus_info_struct));\r\nmemcpy(&vm86->regs32, regs, sizeof(struct pt_regs));\r\nvm86->user_vm86 = user_vm86;\r\nVEFLAGS = vm86regs.pt.flags;\r\nvm86regs.pt.flags &= SAFE_MASK;\r\nvm86regs.pt.flags |= regs->flags & ~SAFE_MASK;\r\nvm86regs.pt.flags |= X86_VM_MASK;\r\nvm86regs.pt.orig_ax = regs->orig_ax;\r\nswitch (vm86->cpu_type) {\r\ncase CPU_286:\r\nvm86->veflags_mask = 0;\r\nbreak;\r\ncase CPU_386:\r\nvm86->veflags_mask = X86_EFLAGS_NT | X86_EFLAGS_IOPL;\r\nbreak;\r\ncase CPU_486:\r\nvm86->veflags_mask = X86_EFLAGS_AC | X86_EFLAGS_NT | X86_EFLAGS_IOPL;\r\nbreak;\r\ndefault:\r\nvm86->veflags_mask = X86_EFLAGS_ID | X86_EFLAGS_AC | X86_EFLAGS_NT | X86_EFLAGS_IOPL;\r\nbreak;\r\n}\r\nvm86->saved_sp0 = tsk->thread.sp0;\r\nlazy_save_gs(vm86->regs32.gs);\r\ntss = &per_cpu(cpu_tss, get_cpu());\r\ntsk->thread.sp0 += 16;\r\nif (static_cpu_has(X86_FEATURE_SEP))\r\ntsk->thread.sysenter_cs = 0;\r\nload_sp0(tss, &tsk->thread);\r\nput_cpu();\r\nif (vm86->flags & VM86_SCREEN_BITMAP)\r\nmark_screen_rdonly(tsk->mm);\r\nmemcpy((struct kernel_vm86_regs *)regs, &vm86regs, sizeof(vm86regs));\r\nforce_iret();\r\nreturn regs->ax;\r\n}\r\nstatic inline void set_IF(struct kernel_vm86_regs *regs)\r\n{\r\nVEFLAGS |= X86_EFLAGS_VIF;\r\n}\r\nstatic inline void clear_IF(struct kernel_vm86_regs *regs)\r\n{\r\nVEFLAGS &= ~X86_EFLAGS_VIF;\r\n}\r\nstatic inline void clear_TF(struct kernel_vm86_regs *regs)\r\n{\r\nregs->pt.flags &= ~X86_EFLAGS_TF;\r\n}\r\nstatic inline void clear_AC(struct kernel_vm86_regs *regs)\r\n{\r\nregs->pt.flags &= ~X86_EFLAGS_AC;\r\n}\r\nstatic inline void set_vflags_long(unsigned long flags, struct kernel_vm86_regs *regs)\r\n{\r\nset_flags(VEFLAGS, flags, current->thread.vm86->veflags_mask);\r\nset_flags(regs->pt.flags, flags, SAFE_MASK);\r\nif (flags & X86_EFLAGS_IF)\r\nset_IF(regs);\r\nelse\r\nclear_IF(regs);\r\n}\r\nstatic inline void set_vflags_short(unsigned short flags, struct kernel_vm86_regs *regs)\r\n{\r\nset_flags(VFLAGS, flags, current->thread.vm86->veflags_mask);\r\nset_flags(regs->pt.flags, flags, SAFE_MASK);\r\nif (flags & X86_EFLAGS_IF)\r\nset_IF(regs);\r\nelse\r\nclear_IF(regs);\r\n}\r\nstatic inline unsigned long get_vflags(struct kernel_vm86_regs *regs)\r\n{\r\nunsigned long flags = regs->pt.flags & RETURN_MASK;\r\nif (VEFLAGS & X86_EFLAGS_VIF)\r\nflags |= X86_EFLAGS_IF;\r\nflags |= X86_EFLAGS_IOPL;\r\nreturn flags | (VEFLAGS & current->thread.vm86->veflags_mask);\r\n}\r\nstatic inline int is_revectored(int nr, struct revectored_struct *bitmap)\r\n{\r\nreturn test_bit(nr, bitmap->__map);\r\n}\r\nstatic void do_int(struct kernel_vm86_regs *regs, int i,\r\nunsigned char __user *ssp, unsigned short sp)\r\n{\r\nunsigned long __user *intr_ptr;\r\nunsigned long segoffs;\r\nstruct vm86 *vm86 = current->thread.vm86;\r\nif (regs->pt.cs == BIOSSEG)\r\ngoto cannot_handle;\r\nif (is_revectored(i, &vm86->int_revectored))\r\ngoto cannot_handle;\r\nif (i == 0x21 && is_revectored(AH(regs), &vm86->int21_revectored))\r\ngoto cannot_handle;\r\nintr_ptr = (unsigned long __user *) (i << 2);\r\nif (get_user(segoffs, intr_ptr))\r\ngoto cannot_handle;\r\nif ((segoffs >> 16) == BIOSSEG)\r\ngoto cannot_handle;\r\npushw(ssp, sp, get_vflags(regs), cannot_handle);\r\npushw(ssp, sp, regs->pt.cs, cannot_handle);\r\npushw(ssp, sp, IP(regs), cannot_handle);\r\nregs->pt.cs = segoffs >> 16;\r\nSP(regs) -= 6;\r\nIP(regs) = segoffs & 0xffff;\r\nclear_TF(regs);\r\nclear_IF(regs);\r\nclear_AC(regs);\r\nreturn;\r\ncannot_handle:\r\nsave_v86_state(regs, VM86_INTx + (i << 8));\r\n}\r\nint handle_vm86_trap(struct kernel_vm86_regs *regs, long error_code, int trapno)\r\n{\r\nstruct vm86 *vm86 = current->thread.vm86;\r\nif (vm86->vm86plus.is_vm86pus) {\r\nif ((trapno == 3) || (trapno == 1)) {\r\nsave_v86_state(regs, VM86_TRAP + (trapno << 8));\r\nreturn 0;\r\n}\r\ndo_int(regs, trapno, (unsigned char __user *) (regs->pt.ss << 4), SP(regs));\r\nreturn 0;\r\n}\r\nif (trapno != 1)\r\nreturn 1;\r\ncurrent->thread.trap_nr = trapno;\r\ncurrent->thread.error_code = error_code;\r\nforce_sig(SIGTRAP, current);\r\nreturn 0;\r\n}\r\nvoid handle_vm86_fault(struct kernel_vm86_regs *regs, long error_code)\r\n{\r\nunsigned char opcode;\r\nunsigned char __user *csp;\r\nunsigned char __user *ssp;\r\nunsigned short ip, sp, orig_flags;\r\nint data32, pref_done;\r\nstruct vm86plus_info_struct *vmpi = &current->thread.vm86->vm86plus;\r\n#define CHECK_IF_IN_TRAP \\r\nif (vmpi->vm86dbg_active && vmpi->vm86dbg_TFpendig) \\r\nnewflags |= X86_EFLAGS_TF\r\norig_flags = *(unsigned short *)&regs->pt.flags;\r\ncsp = (unsigned char __user *) (regs->pt.cs << 4);\r\nssp = (unsigned char __user *) (regs->pt.ss << 4);\r\nsp = SP(regs);\r\nip = IP(regs);\r\ndata32 = 0;\r\npref_done = 0;\r\ndo {\r\nswitch (opcode = popb(csp, ip, simulate_sigsegv)) {\r\ncase 0x66: data32 = 1; break;\r\ncase 0x67: break;\r\ncase 0x2e: break;\r\ncase 0x3e: break;\r\ncase 0x26: break;\r\ncase 0x36: break;\r\ncase 0x65: break;\r\ncase 0x64: break;\r\ncase 0xf2: break;\r\ncase 0xf3: break;\r\ndefault: pref_done = 1;\r\n}\r\n} while (!pref_done);\r\nswitch (opcode) {\r\ncase 0x9c:\r\nif (data32) {\r\npushl(ssp, sp, get_vflags(regs), simulate_sigsegv);\r\nSP(regs) -= 4;\r\n} else {\r\npushw(ssp, sp, get_vflags(regs), simulate_sigsegv);\r\nSP(regs) -= 2;\r\n}\r\nIP(regs) = ip;\r\ngoto vm86_fault_return;\r\ncase 0x9d:\r\n{\r\nunsigned long newflags;\r\nif (data32) {\r\nnewflags = popl(ssp, sp, simulate_sigsegv);\r\nSP(regs) += 4;\r\n} else {\r\nnewflags = popw(ssp, sp, simulate_sigsegv);\r\nSP(regs) += 2;\r\n}\r\nIP(regs) = ip;\r\nCHECK_IF_IN_TRAP;\r\nif (data32)\r\nset_vflags_long(newflags, regs);\r\nelse\r\nset_vflags_short(newflags, regs);\r\ngoto check_vip;\r\n}\r\ncase 0xcd: {\r\nint intno = popb(csp, ip, simulate_sigsegv);\r\nIP(regs) = ip;\r\nif (vmpi->vm86dbg_active) {\r\nif ((1 << (intno & 7)) & vmpi->vm86dbg_intxxtab[intno >> 3]) {\r\nsave_v86_state(regs, VM86_INTx + (intno << 8));\r\nreturn;\r\n}\r\n}\r\ndo_int(regs, intno, ssp, sp);\r\nreturn;\r\n}\r\ncase 0xcf:\r\n{\r\nunsigned long newip;\r\nunsigned long newcs;\r\nunsigned long newflags;\r\nif (data32) {\r\nnewip = popl(ssp, sp, simulate_sigsegv);\r\nnewcs = popl(ssp, sp, simulate_sigsegv);\r\nnewflags = popl(ssp, sp, simulate_sigsegv);\r\nSP(regs) += 12;\r\n} else {\r\nnewip = popw(ssp, sp, simulate_sigsegv);\r\nnewcs = popw(ssp, sp, simulate_sigsegv);\r\nnewflags = popw(ssp, sp, simulate_sigsegv);\r\nSP(regs) += 6;\r\n}\r\nIP(regs) = newip;\r\nregs->pt.cs = newcs;\r\nCHECK_IF_IN_TRAP;\r\nif (data32) {\r\nset_vflags_long(newflags, regs);\r\n} else {\r\nset_vflags_short(newflags, regs);\r\n}\r\ngoto check_vip;\r\n}\r\ncase 0xfa:\r\nIP(regs) = ip;\r\nclear_IF(regs);\r\ngoto vm86_fault_return;\r\ncase 0xfb:\r\nIP(regs) = ip;\r\nset_IF(regs);\r\ngoto check_vip;\r\ndefault:\r\nsave_v86_state(regs, VM86_UNKNOWN);\r\n}\r\nreturn;\r\ncheck_vip:\r\nif (VEFLAGS & X86_EFLAGS_VIP) {\r\nsave_v86_state(regs, VM86_STI);\r\nreturn;\r\n}\r\nvm86_fault_return:\r\nif (vmpi->force_return_for_pic && (VEFLAGS & (X86_EFLAGS_IF | X86_EFLAGS_VIF))) {\r\nsave_v86_state(regs, VM86_PICRETURN);\r\nreturn;\r\n}\r\nif (orig_flags & X86_EFLAGS_TF)\r\nhandle_vm86_trap(regs, 0, X86_TRAP_DB);\r\nreturn;\r\nsimulate_sigsegv:\r\nsave_v86_state(regs, VM86_UNKNOWN);\r\n}\r\nstatic irqreturn_t irq_handler(int intno, void *dev_id)\r\n{\r\nint irq_bit;\r\nunsigned long flags;\r\nspin_lock_irqsave(&irqbits_lock, flags);\r\nirq_bit = 1 << intno;\r\nif ((irqbits & irq_bit) || !vm86_irqs[intno].tsk)\r\ngoto out;\r\nirqbits |= irq_bit;\r\nif (vm86_irqs[intno].sig)\r\nsend_sig(vm86_irqs[intno].sig, vm86_irqs[intno].tsk, 1);\r\ndisable_irq_nosync(intno);\r\nspin_unlock_irqrestore(&irqbits_lock, flags);\r\nreturn IRQ_HANDLED;\r\nout:\r\nspin_unlock_irqrestore(&irqbits_lock, flags);\r\nreturn IRQ_NONE;\r\n}\r\nstatic inline void free_vm86_irq(int irqnumber)\r\n{\r\nunsigned long flags;\r\nfree_irq(irqnumber, NULL);\r\nvm86_irqs[irqnumber].tsk = NULL;\r\nspin_lock_irqsave(&irqbits_lock, flags);\r\nirqbits &= ~(1 << irqnumber);\r\nspin_unlock_irqrestore(&irqbits_lock, flags);\r\n}\r\nvoid release_vm86_irqs(struct task_struct *task)\r\n{\r\nint i;\r\nfor (i = FIRST_VM86_IRQ ; i <= LAST_VM86_IRQ; i++)\r\nif (vm86_irqs[i].tsk == task)\r\nfree_vm86_irq(i);\r\n}\r\nstatic inline int get_and_reset_irq(int irqnumber)\r\n{\r\nint bit;\r\nunsigned long flags;\r\nint ret = 0;\r\nif (invalid_vm86_irq(irqnumber)) return 0;\r\nif (vm86_irqs[irqnumber].tsk != current) return 0;\r\nspin_lock_irqsave(&irqbits_lock, flags);\r\nbit = irqbits & (1 << irqnumber);\r\nirqbits &= ~bit;\r\nif (bit) {\r\nenable_irq(irqnumber);\r\nret = 1;\r\n}\r\nspin_unlock_irqrestore(&irqbits_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int do_vm86_irq_handling(int subfunction, int irqnumber)\r\n{\r\nint ret;\r\nswitch (subfunction) {\r\ncase VM86_GET_AND_RESET_IRQ: {\r\nreturn get_and_reset_irq(irqnumber);\r\n}\r\ncase VM86_GET_IRQ_BITS: {\r\nreturn irqbits;\r\n}\r\ncase VM86_REQUEST_IRQ: {\r\nint sig = irqnumber >> 8;\r\nint irq = irqnumber & 255;\r\nif (!capable(CAP_SYS_ADMIN)) return -EPERM;\r\nif (!((1 << sig) & ALLOWED_SIGS)) return -EPERM;\r\nif (invalid_vm86_irq(irq)) return -EPERM;\r\nif (vm86_irqs[irq].tsk) return -EPERM;\r\nret = request_irq(irq, &irq_handler, 0, VM86_IRQNAME, NULL);\r\nif (ret) return ret;\r\nvm86_irqs[irq].sig = sig;\r\nvm86_irqs[irq].tsk = current;\r\nreturn irq;\r\n}\r\ncase VM86_FREE_IRQ: {\r\nif (invalid_vm86_irq(irqnumber)) return -EPERM;\r\nif (!vm86_irqs[irqnumber].tsk) return 0;\r\nif (vm86_irqs[irqnumber].tsk != current) return -EPERM;\r\nfree_vm86_irq(irqnumber);\r\nreturn 0;\r\n}\r\n}\r\nreturn -EINVAL;\r\n}
