static unsigned long vgic_mmio_read_v2_misc(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 value;\r\nswitch (addr & 0x0c) {\r\ncase GIC_DIST_CTRL:\r\nvalue = vcpu->kvm->arch.vgic.enabled ? GICD_ENABLE : 0;\r\nbreak;\r\ncase GIC_DIST_CTR:\r\nvalue = vcpu->kvm->arch.vgic.nr_spis + VGIC_NR_PRIVATE_IRQS;\r\nvalue = (value >> 5) - 1;\r\nvalue |= (atomic_read(&vcpu->kvm->online_vcpus) - 1) << 5;\r\nbreak;\r\ncase GIC_DIST_IIDR:\r\nvalue = (PRODUCT_ID_KVM << 24) | (IMPLEMENTER_ARM << 0);\r\nbreak;\r\ndefault:\r\nreturn 0;\r\n}\r\nreturn value;\r\n}\r\nstatic void vgic_mmio_write_v2_misc(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nstruct vgic_dist *dist = &vcpu->kvm->arch.vgic;\r\nbool was_enabled = dist->enabled;\r\nswitch (addr & 0x0c) {\r\ncase GIC_DIST_CTRL:\r\ndist->enabled = val & GICD_ENABLE;\r\nif (!was_enabled && dist->enabled)\r\nvgic_kick_vcpus(vcpu->kvm);\r\nbreak;\r\ncase GIC_DIST_CTR:\r\ncase GIC_DIST_IIDR:\r\nreturn;\r\n}\r\n}\r\nstatic void vgic_mmio_write_sgir(struct kvm_vcpu *source_vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nint nr_vcpus = atomic_read(&source_vcpu->kvm->online_vcpus);\r\nint intid = val & 0xf;\r\nint targets = (val >> 16) & 0xff;\r\nint mode = (val >> 24) & 0x03;\r\nint c;\r\nstruct kvm_vcpu *vcpu;\r\nswitch (mode) {\r\ncase 0x0:\r\nbreak;\r\ncase 0x1:\r\ntargets = (1U << nr_vcpus) - 1;\r\ntargets &= ~(1U << source_vcpu->vcpu_id);\r\nbreak;\r\ncase 0x2:\r\ntargets = (1U << source_vcpu->vcpu_id);\r\nbreak;\r\ncase 0x3:\r\nreturn;\r\n}\r\nkvm_for_each_vcpu(c, vcpu, source_vcpu->kvm) {\r\nstruct vgic_irq *irq;\r\nif (!(targets & (1U << c)))\r\ncontinue;\r\nirq = vgic_get_irq(source_vcpu->kvm, vcpu, intid);\r\nspin_lock(&irq->irq_lock);\r\nirq->pending = true;\r\nirq->source |= 1U << source_vcpu->vcpu_id;\r\nvgic_queue_irq_unlock(source_vcpu->kvm, irq);\r\nvgic_put_irq(source_vcpu->kvm, irq);\r\n}\r\n}\r\nstatic unsigned long vgic_mmio_read_target(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 8);\r\nint i;\r\nu64 val = 0;\r\nfor (i = 0; i < len; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nval |= (u64)irq->targets << (i * 8);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nreturn val;\r\n}\r\nstatic void vgic_mmio_write_target(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = VGIC_ADDR_TO_INTID(addr, 8);\r\nint i;\r\nif (intid < VGIC_NR_PRIVATE_IRQS)\r\nreturn;\r\nfor (i = 0; i < len; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, NULL, intid + i);\r\nint target;\r\nspin_lock(&irq->irq_lock);\r\nirq->targets = (val >> (i * 8)) & 0xff;\r\ntarget = irq->targets ? __ffs(irq->targets) : 0;\r\nirq->target_vcpu = kvm_get_vcpu(vcpu->kvm, target);\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nstatic unsigned long vgic_mmio_read_sgipend(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nu32 intid = addr & 0x0f;\r\nint i;\r\nu64 val = 0;\r\nfor (i = 0; i < len; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nval |= (u64)irq->source << (i * 8);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\nreturn val;\r\n}\r\nstatic void vgic_mmio_write_sgipendc(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = addr & 0x0f;\r\nint i;\r\nfor (i = 0; i < len; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nspin_lock(&irq->irq_lock);\r\nirq->source &= ~((val >> (i * 8)) & 0xff);\r\nif (!irq->source)\r\nirq->pending = false;\r\nspin_unlock(&irq->irq_lock);\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nstatic void vgic_mmio_write_sgipends(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nu32 intid = addr & 0x0f;\r\nint i;\r\nfor (i = 0; i < len; i++) {\r\nstruct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, intid + i);\r\nspin_lock(&irq->irq_lock);\r\nirq->source |= (val >> (i * 8)) & 0xff;\r\nif (irq->source) {\r\nirq->pending = true;\r\nvgic_queue_irq_unlock(vcpu->kvm, irq);\r\n} else {\r\nspin_unlock(&irq->irq_lock);\r\n}\r\nvgic_put_irq(vcpu->kvm, irq);\r\n}\r\n}\r\nstatic void vgic_set_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcr)\r\n{\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_set_vmcr(vcpu, vmcr);\r\nelse\r\nvgic_v3_set_vmcr(vcpu, vmcr);\r\n}\r\nstatic void vgic_get_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcr)\r\n{\r\nif (kvm_vgic_global_state.type == VGIC_V2)\r\nvgic_v2_get_vmcr(vcpu, vmcr);\r\nelse\r\nvgic_v3_get_vmcr(vcpu, vmcr);\r\n}\r\nstatic unsigned long vgic_mmio_read_vcpuif(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len)\r\n{\r\nstruct vgic_vmcr vmcr;\r\nu32 val;\r\nvgic_get_vmcr(vcpu, &vmcr);\r\nswitch (addr & 0xff) {\r\ncase GIC_CPU_CTRL:\r\nval = vmcr.ctlr;\r\nbreak;\r\ncase GIC_CPU_PRIMASK:\r\nval = vmcr.pmr;\r\nbreak;\r\ncase GIC_CPU_BINPOINT:\r\nval = vmcr.bpr;\r\nbreak;\r\ncase GIC_CPU_ALIAS_BINPOINT:\r\nval = vmcr.abpr;\r\nbreak;\r\ncase GIC_CPU_IDENT:\r\nval = ((PRODUCT_ID_KVM << 20) |\r\n(GICC_ARCH_VERSION_V2 << 16) |\r\nIMPLEMENTER_ARM);\r\nbreak;\r\ndefault:\r\nreturn 0;\r\n}\r\nreturn val;\r\n}\r\nstatic void vgic_mmio_write_vcpuif(struct kvm_vcpu *vcpu,\r\ngpa_t addr, unsigned int len,\r\nunsigned long val)\r\n{\r\nstruct vgic_vmcr vmcr;\r\nvgic_get_vmcr(vcpu, &vmcr);\r\nswitch (addr & 0xff) {\r\ncase GIC_CPU_CTRL:\r\nvmcr.ctlr = val;\r\nbreak;\r\ncase GIC_CPU_PRIMASK:\r\nvmcr.pmr = val;\r\nbreak;\r\ncase GIC_CPU_BINPOINT:\r\nvmcr.bpr = val;\r\nbreak;\r\ncase GIC_CPU_ALIAS_BINPOINT:\r\nvmcr.abpr = val;\r\nbreak;\r\n}\r\nvgic_set_vmcr(vcpu, &vmcr);\r\n}\r\nunsigned int vgic_v2_init_dist_iodev(struct vgic_io_device *dev)\r\n{\r\ndev->regions = vgic_v2_dist_registers;\r\ndev->nr_regions = ARRAY_SIZE(vgic_v2_dist_registers);\r\nkvm_iodevice_init(&dev->dev, &kvm_io_gic_ops);\r\nreturn SZ_4K;\r\n}\r\nint vgic_v2_has_attr_regs(struct kvm_device *dev, struct kvm_device_attr *attr)\r\n{\r\nint nr_irqs = dev->kvm->arch.vgic.nr_spis + VGIC_NR_PRIVATE_IRQS;\r\nconst struct vgic_register_region *regions;\r\ngpa_t addr;\r\nint nr_regions, i, len;\r\naddr = attr->attr & KVM_DEV_ARM_VGIC_OFFSET_MASK;\r\nswitch (attr->group) {\r\ncase KVM_DEV_ARM_VGIC_GRP_DIST_REGS:\r\nregions = vgic_v2_dist_registers;\r\nnr_regions = ARRAY_SIZE(vgic_v2_dist_registers);\r\nbreak;\r\ncase KVM_DEV_ARM_VGIC_GRP_CPU_REGS:\r\nregions = vgic_v2_cpu_registers;\r\nnr_regions = ARRAY_SIZE(vgic_v2_cpu_registers);\r\nbreak;\r\ndefault:\r\nreturn -ENXIO;\r\n}\r\nif (addr & 3)\r\nreturn -ENXIO;\r\nfor (i = 0; i < nr_regions; i++) {\r\nif (regions[i].bits_per_irq)\r\nlen = (regions[i].bits_per_irq * nr_irqs) / 8;\r\nelse\r\nlen = regions[i].len;\r\nif (regions[i].reg_offset <= addr &&\r\nregions[i].reg_offset + len > addr)\r\nreturn 0;\r\n}\r\nreturn -ENXIO;\r\n}\r\nstatic int vgic_uaccess(struct kvm_vcpu *vcpu, struct vgic_io_device *dev,\r\nbool is_write, int offset, u32 *val)\r\n{\r\nunsigned int len = 4;\r\nu8 buf[4];\r\nint ret;\r\nif (is_write) {\r\nvgic_data_host_to_mmio_bus(buf, len, *val);\r\nret = kvm_io_gic_ops.write(vcpu, &dev->dev, offset, len, buf);\r\n} else {\r\nret = kvm_io_gic_ops.read(vcpu, &dev->dev, offset, len, buf);\r\nif (!ret)\r\n*val = vgic_data_mmio_bus_to_host(buf, len);\r\n}\r\nreturn ret;\r\n}\r\nint vgic_v2_cpuif_uaccess(struct kvm_vcpu *vcpu, bool is_write,\r\nint offset, u32 *val)\r\n{\r\nstruct vgic_io_device dev = {\r\n.regions = vgic_v2_cpu_registers,\r\n.nr_regions = ARRAY_SIZE(vgic_v2_cpu_registers),\r\n.iodev_type = IODEV_CPUIF,\r\n};\r\nreturn vgic_uaccess(vcpu, &dev, is_write, offset, val);\r\n}\r\nint vgic_v2_dist_uaccess(struct kvm_vcpu *vcpu, bool is_write,\r\nint offset, u32 *val)\r\n{\r\nstruct vgic_io_device dev = {\r\n.regions = vgic_v2_dist_registers,\r\n.nr_regions = ARRAY_SIZE(vgic_v2_dist_registers),\r\n.iodev_type = IODEV_DIST,\r\n};\r\nreturn vgic_uaccess(vcpu, &dev, is_write, offset, val);\r\n}
