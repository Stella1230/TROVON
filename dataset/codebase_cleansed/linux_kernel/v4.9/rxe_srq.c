int rxe_srq_chk_attr(struct rxe_dev *rxe, struct rxe_srq *srq,\r\nstruct ib_srq_attr *attr, enum ib_srq_attr_mask mask)\r\n{\r\nif (srq && srq->error) {\r\npr_warn("srq in error state\n");\r\ngoto err1;\r\n}\r\nif (mask & IB_SRQ_MAX_WR) {\r\nif (attr->max_wr > rxe->attr.max_srq_wr) {\r\npr_warn("max_wr(%d) > max_srq_wr(%d)\n",\r\nattr->max_wr, rxe->attr.max_srq_wr);\r\ngoto err1;\r\n}\r\nif (attr->max_wr <= 0) {\r\npr_warn("max_wr(%d) <= 0\n", attr->max_wr);\r\ngoto err1;\r\n}\r\nif (srq && srq->limit && (attr->max_wr < srq->limit)) {\r\npr_warn("max_wr (%d) < srq->limit (%d)\n",\r\nattr->max_wr, srq->limit);\r\ngoto err1;\r\n}\r\nif (attr->max_wr < RXE_MIN_SRQ_WR)\r\nattr->max_wr = RXE_MIN_SRQ_WR;\r\n}\r\nif (mask & IB_SRQ_LIMIT) {\r\nif (attr->srq_limit > rxe->attr.max_srq_wr) {\r\npr_warn("srq_limit(%d) > max_srq_wr(%d)\n",\r\nattr->srq_limit, rxe->attr.max_srq_wr);\r\ngoto err1;\r\n}\r\nif (srq && (attr->srq_limit > srq->rq.queue->buf->index_mask)) {\r\npr_warn("srq_limit (%d) > cur limit(%d)\n",\r\nattr->srq_limit,\r\nsrq->rq.queue->buf->index_mask);\r\ngoto err1;\r\n}\r\n}\r\nif (mask == IB_SRQ_INIT_MASK) {\r\nif (attr->max_sge > rxe->attr.max_srq_sge) {\r\npr_warn("max_sge(%d) > max_srq_sge(%d)\n",\r\nattr->max_sge, rxe->attr.max_srq_sge);\r\ngoto err1;\r\n}\r\nif (attr->max_sge < RXE_MIN_SRQ_SGE)\r\nattr->max_sge = RXE_MIN_SRQ_SGE;\r\n}\r\nreturn 0;\r\nerr1:\r\nreturn -EINVAL;\r\n}\r\nint rxe_srq_from_init(struct rxe_dev *rxe, struct rxe_srq *srq,\r\nstruct ib_srq_init_attr *init,\r\nstruct ib_ucontext *context, struct ib_udata *udata)\r\n{\r\nint err;\r\nint srq_wqe_size;\r\nstruct rxe_queue *q;\r\nsrq->ibsrq.event_handler = init->event_handler;\r\nsrq->ibsrq.srq_context = init->srq_context;\r\nsrq->limit = init->attr.srq_limit;\r\nsrq->srq_num = srq->pelem.index;\r\nsrq->rq.max_wr = init->attr.max_wr;\r\nsrq->rq.max_sge = init->attr.max_sge;\r\nsrq_wqe_size = rcv_wqe_size(srq->rq.max_sge);\r\nspin_lock_init(&srq->rq.producer_lock);\r\nspin_lock_init(&srq->rq.consumer_lock);\r\nq = rxe_queue_init(rxe, &srq->rq.max_wr,\r\nsrq_wqe_size);\r\nif (!q) {\r\npr_warn("unable to allocate queue for srq\n");\r\nreturn -ENOMEM;\r\n}\r\nsrq->rq.queue = q;\r\nerr = do_mmap_info(rxe, udata, false, context, q->buf,\r\nq->buf_size, &q->ip);\r\nif (err)\r\nreturn err;\r\nif (udata && udata->outlen >= sizeof(struct mminfo) + sizeof(u32)) {\r\nif (copy_to_user(udata->outbuf + sizeof(struct mminfo),\r\n&srq->srq_num, sizeof(u32)))\r\nreturn -EFAULT;\r\n}\r\nreturn 0;\r\n}\r\nint rxe_srq_from_attr(struct rxe_dev *rxe, struct rxe_srq *srq,\r\nstruct ib_srq_attr *attr, enum ib_srq_attr_mask mask,\r\nstruct ib_udata *udata)\r\n{\r\nint err;\r\nstruct rxe_queue *q = srq->rq.queue;\r\nstruct mminfo mi = { .offset = 1, .size = 0};\r\nif (mask & IB_SRQ_MAX_WR) {\r\nif (udata && udata->inlen >= sizeof(__u64)) {\r\n__u64 mi_addr;\r\nerr = ib_copy_from_udata(&mi_addr, udata,\r\nsizeof(mi_addr));\r\nif (err)\r\ngoto err1;\r\nudata->outbuf = (void __user *)(unsigned long)mi_addr;\r\nudata->outlen = sizeof(mi);\r\nif (!access_ok(VERIFY_WRITE,\r\n(void __user *)udata->outbuf,\r\nudata->outlen)) {\r\nerr = -EFAULT;\r\ngoto err1;\r\n}\r\n}\r\nerr = rxe_queue_resize(q, (unsigned int *)&attr->max_wr,\r\nrcv_wqe_size(srq->rq.max_sge),\r\nsrq->rq.queue->ip ?\r\nsrq->rq.queue->ip->context :\r\nNULL,\r\nudata, &srq->rq.producer_lock,\r\n&srq->rq.consumer_lock);\r\nif (err)\r\ngoto err2;\r\n}\r\nif (mask & IB_SRQ_LIMIT)\r\nsrq->limit = attr->srq_limit;\r\nreturn 0;\r\nerr2:\r\nrxe_queue_cleanup(q);\r\nsrq->rq.queue = NULL;\r\nerr1:\r\nreturn err;\r\n}
