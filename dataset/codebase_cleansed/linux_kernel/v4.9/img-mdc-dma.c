static inline u32 mdc_readl(struct mdc_dma *mdma, u32 reg)\r\n{\r\nreturn readl(mdma->regs + reg);\r\n}\r\nstatic inline void mdc_writel(struct mdc_dma *mdma, u32 val, u32 reg)\r\n{\r\nwritel(val, mdma->regs + reg);\r\n}\r\nstatic inline u32 mdc_chan_readl(struct mdc_chan *mchan, u32 reg)\r\n{\r\nreturn mdc_readl(mchan->mdma, mchan->chan_nr * 0x040 + reg);\r\n}\r\nstatic inline void mdc_chan_writel(struct mdc_chan *mchan, u32 val, u32 reg)\r\n{\r\nmdc_writel(mchan->mdma, val, mchan->chan_nr * 0x040 + reg);\r\n}\r\nstatic inline struct mdc_chan *to_mdc_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(to_virt_chan(c), struct mdc_chan, vc);\r\n}\r\nstatic inline struct mdc_tx_desc *to_mdc_desc(struct dma_async_tx_descriptor *t)\r\n{\r\nstruct virt_dma_desc *vdesc = container_of(t, struct virt_dma_desc, tx);\r\nreturn container_of(vdesc, struct mdc_tx_desc, vd);\r\n}\r\nstatic inline struct device *mdma2dev(struct mdc_dma *mdma)\r\n{\r\nreturn mdma->dma_dev.dev;\r\n}\r\nstatic inline unsigned int to_mdc_width(unsigned int bytes)\r\n{\r\nreturn ffs(bytes) - 1;\r\n}\r\nstatic inline void mdc_set_read_width(struct mdc_hw_list_desc *ldesc,\r\nunsigned int bytes)\r\n{\r\nldesc->gen_conf |= to_mdc_width(bytes) <<\r\nMDC_GENERAL_CONFIG_WIDTH_R_SHIFT;\r\n}\r\nstatic inline void mdc_set_write_width(struct mdc_hw_list_desc *ldesc,\r\nunsigned int bytes)\r\n{\r\nldesc->gen_conf |= to_mdc_width(bytes) <<\r\nMDC_GENERAL_CONFIG_WIDTH_W_SHIFT;\r\n}\r\nstatic void mdc_list_desc_config(struct mdc_chan *mchan,\r\nstruct mdc_hw_list_desc *ldesc,\r\nenum dma_transfer_direction dir,\r\ndma_addr_t src, dma_addr_t dst, size_t len)\r\n{\r\nstruct mdc_dma *mdma = mchan->mdma;\r\nunsigned int max_burst, burst_size;\r\nldesc->gen_conf = MDC_GENERAL_CONFIG_IEN | MDC_GENERAL_CONFIG_LIST_IEN |\r\nMDC_GENERAL_CONFIG_LEVEL_INT | MDC_GENERAL_CONFIG_PHYSICAL_W |\r\nMDC_GENERAL_CONFIG_PHYSICAL_R;\r\nldesc->readport_conf =\r\n(mchan->thread << MDC_READ_PORT_CONFIG_STHREAD_SHIFT) |\r\n(mchan->thread << MDC_READ_PORT_CONFIG_RTHREAD_SHIFT) |\r\n(mchan->thread << MDC_READ_PORT_CONFIG_WTHREAD_SHIFT);\r\nldesc->read_addr = src;\r\nldesc->write_addr = dst;\r\nldesc->xfer_size = len - 1;\r\nldesc->node_addr = 0;\r\nldesc->cmds_done = 0;\r\nldesc->ctrl_status = MDC_CONTROL_AND_STATUS_LIST_EN |\r\nMDC_CONTROL_AND_STATUS_EN;\r\nldesc->next_desc = NULL;\r\nif (IS_ALIGNED(dst, mdma->bus_width) &&\r\nIS_ALIGNED(src, mdma->bus_width))\r\nmax_burst = mdma->bus_width * mdma->max_burst_mult;\r\nelse\r\nmax_burst = mdma->bus_width * (mdma->max_burst_mult - 1);\r\nif (dir == DMA_MEM_TO_DEV) {\r\nldesc->gen_conf |= MDC_GENERAL_CONFIG_INC_R;\r\nldesc->readport_conf |= MDC_READ_PORT_CONFIG_DREQ_ENABLE;\r\nmdc_set_read_width(ldesc, mdma->bus_width);\r\nmdc_set_write_width(ldesc, mchan->config.dst_addr_width);\r\nburst_size = min(max_burst, mchan->config.dst_maxburst *\r\nmchan->config.dst_addr_width);\r\n} else if (dir == DMA_DEV_TO_MEM) {\r\nldesc->gen_conf |= MDC_GENERAL_CONFIG_INC_W;\r\nldesc->readport_conf |= MDC_READ_PORT_CONFIG_DREQ_ENABLE;\r\nmdc_set_read_width(ldesc, mchan->config.src_addr_width);\r\nmdc_set_write_width(ldesc, mdma->bus_width);\r\nburst_size = min(max_burst, mchan->config.src_maxburst *\r\nmchan->config.src_addr_width);\r\n} else {\r\nldesc->gen_conf |= MDC_GENERAL_CONFIG_INC_R |\r\nMDC_GENERAL_CONFIG_INC_W;\r\nmdc_set_read_width(ldesc, mdma->bus_width);\r\nmdc_set_write_width(ldesc, mdma->bus_width);\r\nburst_size = max_burst;\r\n}\r\nldesc->readport_conf |= (burst_size - 1) <<\r\nMDC_READ_PORT_CONFIG_BURST_SIZE_SHIFT;\r\n}\r\nstatic void mdc_list_desc_free(struct mdc_tx_desc *mdesc)\r\n{\r\nstruct mdc_dma *mdma = mdesc->chan->mdma;\r\nstruct mdc_hw_list_desc *curr, *next;\r\ndma_addr_t curr_phys, next_phys;\r\ncurr = mdesc->list;\r\ncurr_phys = mdesc->list_phys;\r\nwhile (curr) {\r\nnext = curr->next_desc;\r\nnext_phys = curr->node_addr;\r\ndma_pool_free(mdma->desc_pool, curr, curr_phys);\r\ncurr = next;\r\ncurr_phys = next_phys;\r\n}\r\n}\r\nstatic void mdc_desc_free(struct virt_dma_desc *vd)\r\n{\r\nstruct mdc_tx_desc *mdesc = to_mdc_desc(&vd->tx);\r\nmdc_list_desc_free(mdesc);\r\nkfree(mdesc);\r\n}\r\nstatic struct dma_async_tx_descriptor *mdc_prep_dma_memcpy(\r\nstruct dma_chan *chan, dma_addr_t dest, dma_addr_t src, size_t len,\r\nunsigned long flags)\r\n{\r\nstruct mdc_chan *mchan = to_mdc_chan(chan);\r\nstruct mdc_dma *mdma = mchan->mdma;\r\nstruct mdc_tx_desc *mdesc;\r\nstruct mdc_hw_list_desc *curr, *prev = NULL;\r\ndma_addr_t curr_phys, prev_phys;\r\nif (!len)\r\nreturn NULL;\r\nmdesc = kzalloc(sizeof(*mdesc), GFP_NOWAIT);\r\nif (!mdesc)\r\nreturn NULL;\r\nmdesc->chan = mchan;\r\nmdesc->list_xfer_size = len;\r\nwhile (len > 0) {\r\nsize_t xfer_size;\r\ncurr = dma_pool_alloc(mdma->desc_pool, GFP_NOWAIT, &curr_phys);\r\nif (!curr)\r\ngoto free_desc;\r\nif (prev) {\r\nprev->node_addr = curr_phys;\r\nprev->next_desc = curr;\r\n} else {\r\nmdesc->list_phys = curr_phys;\r\nmdesc->list = curr;\r\n}\r\nxfer_size = min_t(size_t, mdma->max_xfer_size, len);\r\nmdc_list_desc_config(mchan, curr, DMA_MEM_TO_MEM, src, dest,\r\nxfer_size);\r\nprev = curr;\r\nprev_phys = curr_phys;\r\nmdesc->list_len++;\r\nsrc += xfer_size;\r\ndest += xfer_size;\r\nlen -= xfer_size;\r\n}\r\nreturn vchan_tx_prep(&mchan->vc, &mdesc->vd, flags);\r\nfree_desc:\r\nmdc_desc_free(&mdesc->vd);\r\nreturn NULL;\r\n}\r\nstatic int mdc_check_slave_width(struct mdc_chan *mchan,\r\nenum dma_transfer_direction dir)\r\n{\r\nenum dma_slave_buswidth width;\r\nif (dir == DMA_MEM_TO_DEV)\r\nwidth = mchan->config.dst_addr_width;\r\nelse\r\nwidth = mchan->config.src_addr_width;\r\nswitch (width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\ncase DMA_SLAVE_BUSWIDTH_8_BYTES:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (width > mchan->mdma->bus_width)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic struct dma_async_tx_descriptor *mdc_prep_dma_cyclic(\r\nstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction dir,\r\nunsigned long flags)\r\n{\r\nstruct mdc_chan *mchan = to_mdc_chan(chan);\r\nstruct mdc_dma *mdma = mchan->mdma;\r\nstruct mdc_tx_desc *mdesc;\r\nstruct mdc_hw_list_desc *curr, *prev = NULL;\r\ndma_addr_t curr_phys, prev_phys;\r\nif (!buf_len && !period_len)\r\nreturn NULL;\r\nif (!is_slave_direction(dir))\r\nreturn NULL;\r\nif (mdc_check_slave_width(mchan, dir) < 0)\r\nreturn NULL;\r\nmdesc = kzalloc(sizeof(*mdesc), GFP_NOWAIT);\r\nif (!mdesc)\r\nreturn NULL;\r\nmdesc->chan = mchan;\r\nmdesc->cyclic = true;\r\nmdesc->list_xfer_size = buf_len;\r\nmdesc->list_period_len = DIV_ROUND_UP(period_len,\r\nmdma->max_xfer_size);\r\nwhile (buf_len > 0) {\r\nsize_t remainder = min(period_len, buf_len);\r\nwhile (remainder > 0) {\r\nsize_t xfer_size;\r\ncurr = dma_pool_alloc(mdma->desc_pool, GFP_NOWAIT,\r\n&curr_phys);\r\nif (!curr)\r\ngoto free_desc;\r\nif (!prev) {\r\nmdesc->list_phys = curr_phys;\r\nmdesc->list = curr;\r\n} else {\r\nprev->node_addr = curr_phys;\r\nprev->next_desc = curr;\r\n}\r\nxfer_size = min_t(size_t, mdma->max_xfer_size,\r\nremainder);\r\nif (dir == DMA_MEM_TO_DEV) {\r\nmdc_list_desc_config(mchan, curr, dir,\r\nbuf_addr,\r\nmchan->config.dst_addr,\r\nxfer_size);\r\n} else {\r\nmdc_list_desc_config(mchan, curr, dir,\r\nmchan->config.src_addr,\r\nbuf_addr,\r\nxfer_size);\r\n}\r\nprev = curr;\r\nprev_phys = curr_phys;\r\nmdesc->list_len++;\r\nbuf_addr += xfer_size;\r\nbuf_len -= xfer_size;\r\nremainder -= xfer_size;\r\n}\r\n}\r\nprev->node_addr = mdesc->list_phys;\r\nreturn vchan_tx_prep(&mchan->vc, &mdesc->vd, flags);\r\nfree_desc:\r\nmdc_desc_free(&mdesc->vd);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *mdc_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction dir,\r\nunsigned long flags, void *context)\r\n{\r\nstruct mdc_chan *mchan = to_mdc_chan(chan);\r\nstruct mdc_dma *mdma = mchan->mdma;\r\nstruct mdc_tx_desc *mdesc;\r\nstruct scatterlist *sg;\r\nstruct mdc_hw_list_desc *curr, *prev = NULL;\r\ndma_addr_t curr_phys, prev_phys;\r\nunsigned int i;\r\nif (!sgl)\r\nreturn NULL;\r\nif (!is_slave_direction(dir))\r\nreturn NULL;\r\nif (mdc_check_slave_width(mchan, dir) < 0)\r\nreturn NULL;\r\nmdesc = kzalloc(sizeof(*mdesc), GFP_NOWAIT);\r\nif (!mdesc)\r\nreturn NULL;\r\nmdesc->chan = mchan;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\ndma_addr_t buf = sg_dma_address(sg);\r\nsize_t buf_len = sg_dma_len(sg);\r\nwhile (buf_len > 0) {\r\nsize_t xfer_size;\r\ncurr = dma_pool_alloc(mdma->desc_pool, GFP_NOWAIT,\r\n&curr_phys);\r\nif (!curr)\r\ngoto free_desc;\r\nif (!prev) {\r\nmdesc->list_phys = curr_phys;\r\nmdesc->list = curr;\r\n} else {\r\nprev->node_addr = curr_phys;\r\nprev->next_desc = curr;\r\n}\r\nxfer_size = min_t(size_t, mdma->max_xfer_size,\r\nbuf_len);\r\nif (dir == DMA_MEM_TO_DEV) {\r\nmdc_list_desc_config(mchan, curr, dir, buf,\r\nmchan->config.dst_addr,\r\nxfer_size);\r\n} else {\r\nmdc_list_desc_config(mchan, curr, dir,\r\nmchan->config.src_addr,\r\nbuf, xfer_size);\r\n}\r\nprev = curr;\r\nprev_phys = curr_phys;\r\nmdesc->list_len++;\r\nmdesc->list_xfer_size += xfer_size;\r\nbuf += xfer_size;\r\nbuf_len -= xfer_size;\r\n}\r\n}\r\nreturn vchan_tx_prep(&mchan->vc, &mdesc->vd, flags);\r\nfree_desc:\r\nmdc_desc_free(&mdesc->vd);\r\nreturn NULL;\r\n}\r\nstatic void mdc_issue_desc(struct mdc_chan *mchan)\r\n{\r\nstruct mdc_dma *mdma = mchan->mdma;\r\nstruct virt_dma_desc *vd;\r\nstruct mdc_tx_desc *mdesc;\r\nu32 val;\r\nvd = vchan_next_desc(&mchan->vc);\r\nif (!vd)\r\nreturn;\r\nlist_del(&vd->node);\r\nmdesc = to_mdc_desc(&vd->tx);\r\nmchan->desc = mdesc;\r\ndev_dbg(mdma2dev(mdma), "Issuing descriptor on channel %d\n",\r\nmchan->chan_nr);\r\nmdma->soc->enable_chan(mchan);\r\nval = mdc_chan_readl(mchan, MDC_GENERAL_CONFIG);\r\nval |= MDC_GENERAL_CONFIG_LIST_IEN | MDC_GENERAL_CONFIG_IEN |\r\nMDC_GENERAL_CONFIG_LEVEL_INT | MDC_GENERAL_CONFIG_PHYSICAL_W |\r\nMDC_GENERAL_CONFIG_PHYSICAL_R;\r\nmdc_chan_writel(mchan, val, MDC_GENERAL_CONFIG);\r\nval = (mchan->thread << MDC_READ_PORT_CONFIG_STHREAD_SHIFT) |\r\n(mchan->thread << MDC_READ_PORT_CONFIG_RTHREAD_SHIFT) |\r\n(mchan->thread << MDC_READ_PORT_CONFIG_WTHREAD_SHIFT);\r\nmdc_chan_writel(mchan, val, MDC_READ_PORT_CONFIG);\r\nmdc_chan_writel(mchan, mdesc->list_phys, MDC_LIST_NODE_ADDRESS);\r\nval = mdc_chan_readl(mchan, MDC_CONTROL_AND_STATUS);\r\nval |= MDC_CONTROL_AND_STATUS_LIST_EN;\r\nmdc_chan_writel(mchan, val, MDC_CONTROL_AND_STATUS);\r\n}\r\nstatic void mdc_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct mdc_chan *mchan = to_mdc_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&mchan->vc.lock, flags);\r\nif (vchan_issue_pending(&mchan->vc) && !mchan->desc)\r\nmdc_issue_desc(mchan);\r\nspin_unlock_irqrestore(&mchan->vc.lock, flags);\r\n}\r\nstatic enum dma_status mdc_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct mdc_chan *mchan = to_mdc_chan(chan);\r\nstruct mdc_tx_desc *mdesc;\r\nstruct virt_dma_desc *vd;\r\nunsigned long flags;\r\nsize_t bytes = 0;\r\nint ret;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_COMPLETE)\r\nreturn ret;\r\nif (!txstate)\r\nreturn ret;\r\nspin_lock_irqsave(&mchan->vc.lock, flags);\r\nvd = vchan_find_desc(&mchan->vc, cookie);\r\nif (vd) {\r\nmdesc = to_mdc_desc(&vd->tx);\r\nbytes = mdesc->list_xfer_size;\r\n} else if (mchan->desc && mchan->desc->vd.tx.cookie == cookie) {\r\nstruct mdc_hw_list_desc *ldesc;\r\nu32 val1, val2, done, processed, residue;\r\nint i, cmds;\r\nmdesc = mchan->desc;\r\ndo {\r\nval1 = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED) &\r\n~MDC_CMDS_PROCESSED_INT_ACTIVE;\r\nresidue = mdc_chan_readl(mchan,\r\nMDC_ACTIVE_TRANSFER_SIZE);\r\nval2 = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED) &\r\n~MDC_CMDS_PROCESSED_INT_ACTIVE;\r\n} while (val1 != val2);\r\ndone = (val1 >> MDC_CMDS_PROCESSED_CMDS_DONE_SHIFT) &\r\nMDC_CMDS_PROCESSED_CMDS_DONE_MASK;\r\nprocessed = (val1 >> MDC_CMDS_PROCESSED_CMDS_PROCESSED_SHIFT) &\r\nMDC_CMDS_PROCESSED_CMDS_PROCESSED_MASK;\r\ncmds = (done - processed) %\r\n(MDC_CMDS_PROCESSED_CMDS_DONE_MASK + 1);\r\nif (!mdesc->cmd_loaded)\r\ncmds--;\r\nelse\r\ncmds += mdesc->list_cmds_done;\r\nbytes = mdesc->list_xfer_size;\r\nldesc = mdesc->list;\r\nfor (i = 0; i < cmds; i++) {\r\nbytes -= ldesc->xfer_size + 1;\r\nldesc = ldesc->next_desc;\r\n}\r\nif (ldesc) {\r\nif (residue != MDC_TRANSFER_SIZE_MASK)\r\nbytes -= ldesc->xfer_size - residue;\r\nelse\r\nbytes -= ldesc->xfer_size + 1;\r\n}\r\n}\r\nspin_unlock_irqrestore(&mchan->vc.lock, flags);\r\ndma_set_residue(txstate, bytes);\r\nreturn ret;\r\n}\r\nstatic unsigned int mdc_get_new_events(struct mdc_chan *mchan)\r\n{\r\nu32 val, processed, done1, done2;\r\nunsigned int ret;\r\nval = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED);\r\nprocessed = (val >> MDC_CMDS_PROCESSED_CMDS_PROCESSED_SHIFT) &\r\nMDC_CMDS_PROCESSED_CMDS_PROCESSED_MASK;\r\ndo {\r\nval = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED);\r\ndone1 = (val >> MDC_CMDS_PROCESSED_CMDS_DONE_SHIFT) &\r\nMDC_CMDS_PROCESSED_CMDS_DONE_MASK;\r\nval &= ~((MDC_CMDS_PROCESSED_CMDS_PROCESSED_MASK <<\r\nMDC_CMDS_PROCESSED_CMDS_PROCESSED_SHIFT) |\r\nMDC_CMDS_PROCESSED_INT_ACTIVE);\r\nval |= done1 << MDC_CMDS_PROCESSED_CMDS_PROCESSED_SHIFT;\r\nmdc_chan_writel(mchan, val, MDC_CMDS_PROCESSED);\r\nval = mdc_chan_readl(mchan, MDC_CMDS_PROCESSED);\r\ndone2 = (val >> MDC_CMDS_PROCESSED_CMDS_DONE_SHIFT) &\r\nMDC_CMDS_PROCESSED_CMDS_DONE_MASK;\r\n} while (done1 != done2);\r\nif (done1 >= processed)\r\nret = done1 - processed;\r\nelse\r\nret = ((MDC_CMDS_PROCESSED_CMDS_PROCESSED_MASK + 1) -\r\nprocessed) + done1;\r\nreturn ret;\r\n}\r\nstatic int mdc_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct mdc_chan *mchan = to_mdc_chan(chan);\r\nstruct mdc_tx_desc *mdesc;\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&mchan->vc.lock, flags);\r\nmdc_chan_writel(mchan, MDC_CONTROL_AND_STATUS_CANCEL,\r\nMDC_CONTROL_AND_STATUS);\r\nmdesc = mchan->desc;\r\nmchan->desc = NULL;\r\nvchan_get_all_descriptors(&mchan->vc, &head);\r\nmdc_get_new_events(mchan);\r\nspin_unlock_irqrestore(&mchan->vc.lock, flags);\r\nif (mdesc)\r\nmdc_desc_free(&mdesc->vd);\r\nvchan_dma_desc_free_list(&mchan->vc, &head);\r\nreturn 0;\r\n}\r\nstatic int mdc_slave_config(struct dma_chan *chan,\r\nstruct dma_slave_config *config)\r\n{\r\nstruct mdc_chan *mchan = to_mdc_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&mchan->vc.lock, flags);\r\nmchan->config = *config;\r\nspin_unlock_irqrestore(&mchan->vc.lock, flags);\r\nreturn 0;\r\n}\r\nstatic void mdc_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct mdc_chan *mchan = to_mdc_chan(chan);\r\nstruct mdc_dma *mdma = mchan->mdma;\r\nmdc_terminate_all(chan);\r\nmdma->soc->disable_chan(mchan);\r\n}\r\nstatic irqreturn_t mdc_chan_irq(int irq, void *dev_id)\r\n{\r\nstruct mdc_chan *mchan = (struct mdc_chan *)dev_id;\r\nstruct mdc_tx_desc *mdesc;\r\nunsigned int i, new_events;\r\nspin_lock(&mchan->vc.lock);\r\ndev_dbg(mdma2dev(mchan->mdma), "IRQ on channel %d\n", mchan->chan_nr);\r\nnew_events = mdc_get_new_events(mchan);\r\nif (!new_events)\r\ngoto out;\r\nmdesc = mchan->desc;\r\nif (!mdesc) {\r\ndev_warn(mdma2dev(mchan->mdma),\r\n"IRQ with no active descriptor on channel %d\n",\r\nmchan->chan_nr);\r\ngoto out;\r\n}\r\nfor (i = 0; i < new_events; i++) {\r\nif (!mdesc->cmd_loaded) {\r\nmdesc->cmd_loaded = true;\r\ncontinue;\r\n}\r\nmdesc->list_cmds_done++;\r\nif (mdesc->cyclic) {\r\nmdesc->list_cmds_done %= mdesc->list_len;\r\nif (mdesc->list_cmds_done % mdesc->list_period_len == 0)\r\nvchan_cyclic_callback(&mdesc->vd);\r\n} else if (mdesc->list_cmds_done == mdesc->list_len) {\r\nmchan->desc = NULL;\r\nvchan_cookie_complete(&mdesc->vd);\r\nmdc_issue_desc(mchan);\r\nbreak;\r\n}\r\n}\r\nout:\r\nspin_unlock(&mchan->vc.lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic struct dma_chan *mdc_of_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct mdc_dma *mdma = ofdma->of_dma_data;\r\nstruct dma_chan *chan;\r\nif (dma_spec->args_count != 3)\r\nreturn NULL;\r\nlist_for_each_entry(chan, &mdma->dma_dev.channels, device_node) {\r\nstruct mdc_chan *mchan = to_mdc_chan(chan);\r\nif (!(dma_spec->args[1] & BIT(mchan->chan_nr)))\r\ncontinue;\r\nif (dma_get_slave_channel(chan)) {\r\nmchan->periph = dma_spec->args[0];\r\nmchan->thread = dma_spec->args[2];\r\nreturn chan;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic void pistachio_mdc_enable_chan(struct mdc_chan *mchan)\r\n{\r\nstruct mdc_dma *mdma = mchan->mdma;\r\nregmap_update_bits(mdma->periph_regs,\r\nPISTACHIO_CR_PERIPH_DMA_ROUTE(mchan->chan_nr),\r\nPISTACHIO_CR_PERIPH_DMA_ROUTE_MASK <<\r\nPISTACHIO_CR_PERIPH_DMA_ROUTE_SHIFT(mchan->chan_nr),\r\nmchan->periph <<\r\nPISTACHIO_CR_PERIPH_DMA_ROUTE_SHIFT(mchan->chan_nr));\r\n}\r\nstatic void pistachio_mdc_disable_chan(struct mdc_chan *mchan)\r\n{\r\nstruct mdc_dma *mdma = mchan->mdma;\r\nregmap_update_bits(mdma->periph_regs,\r\nPISTACHIO_CR_PERIPH_DMA_ROUTE(mchan->chan_nr),\r\nPISTACHIO_CR_PERIPH_DMA_ROUTE_MASK <<\r\nPISTACHIO_CR_PERIPH_DMA_ROUTE_SHIFT(mchan->chan_nr),\r\n0);\r\n}\r\nstatic int mdc_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct mdc_dma *mdma;\r\nstruct resource *res;\r\nunsigned int i;\r\nu32 val;\r\nint ret;\r\nmdma = devm_kzalloc(&pdev->dev, sizeof(*mdma), GFP_KERNEL);\r\nif (!mdma)\r\nreturn -ENOMEM;\r\nplatform_set_drvdata(pdev, mdma);\r\nmdma->soc = of_device_get_match_data(&pdev->dev);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nmdma->regs = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(mdma->regs))\r\nreturn PTR_ERR(mdma->regs);\r\nmdma->periph_regs = syscon_regmap_lookup_by_phandle(pdev->dev.of_node,\r\n"img,cr-periph");\r\nif (IS_ERR(mdma->periph_regs))\r\nreturn PTR_ERR(mdma->periph_regs);\r\nmdma->clk = devm_clk_get(&pdev->dev, "sys");\r\nif (IS_ERR(mdma->clk))\r\nreturn PTR_ERR(mdma->clk);\r\nret = clk_prepare_enable(mdma->clk);\r\nif (ret)\r\nreturn ret;\r\ndma_cap_zero(mdma->dma_dev.cap_mask);\r\ndma_cap_set(DMA_SLAVE, mdma->dma_dev.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, mdma->dma_dev.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, mdma->dma_dev.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, mdma->dma_dev.cap_mask);\r\nval = mdc_readl(mdma, MDC_GLOBAL_CONFIG_A);\r\nmdma->nr_channels = (val >> MDC_GLOBAL_CONFIG_A_DMA_CONTEXTS_SHIFT) &\r\nMDC_GLOBAL_CONFIG_A_DMA_CONTEXTS_MASK;\r\nmdma->nr_threads =\r\n1 << ((val >> MDC_GLOBAL_CONFIG_A_THREAD_ID_WIDTH_SHIFT) &\r\nMDC_GLOBAL_CONFIG_A_THREAD_ID_WIDTH_MASK);\r\nmdma->bus_width =\r\n(1 << ((val >> MDC_GLOBAL_CONFIG_A_SYS_DAT_WIDTH_SHIFT) &\r\nMDC_GLOBAL_CONFIG_A_SYS_DAT_WIDTH_MASK)) / 8;\r\nmdma->max_xfer_size = MDC_TRANSFER_SIZE_MASK + 1 - mdma->bus_width;\r\nof_property_read_u32(pdev->dev.of_node, "dma-channels",\r\n&mdma->nr_channels);\r\nret = of_property_read_u32(pdev->dev.of_node,\r\n"img,max-burst-multiplier",\r\n&mdma->max_burst_mult);\r\nif (ret)\r\ngoto disable_clk;\r\nmdma->dma_dev.dev = &pdev->dev;\r\nmdma->dma_dev.device_prep_slave_sg = mdc_prep_slave_sg;\r\nmdma->dma_dev.device_prep_dma_cyclic = mdc_prep_dma_cyclic;\r\nmdma->dma_dev.device_prep_dma_memcpy = mdc_prep_dma_memcpy;\r\nmdma->dma_dev.device_free_chan_resources = mdc_free_chan_resources;\r\nmdma->dma_dev.device_tx_status = mdc_tx_status;\r\nmdma->dma_dev.device_issue_pending = mdc_issue_pending;\r\nmdma->dma_dev.device_terminate_all = mdc_terminate_all;\r\nmdma->dma_dev.device_config = mdc_slave_config;\r\nmdma->dma_dev.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\nmdma->dma_dev.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\nfor (i = 1; i <= mdma->bus_width; i <<= 1) {\r\nmdma->dma_dev.src_addr_widths |= BIT(i);\r\nmdma->dma_dev.dst_addr_widths |= BIT(i);\r\n}\r\nINIT_LIST_HEAD(&mdma->dma_dev.channels);\r\nfor (i = 0; i < mdma->nr_channels; i++) {\r\nstruct mdc_chan *mchan = &mdma->channels[i];\r\nmchan->mdma = mdma;\r\nmchan->chan_nr = i;\r\nmchan->irq = platform_get_irq(pdev, i);\r\nif (mchan->irq < 0) {\r\nret = mchan->irq;\r\ngoto disable_clk;\r\n}\r\nret = devm_request_irq(&pdev->dev, mchan->irq, mdc_chan_irq,\r\nIRQ_TYPE_LEVEL_HIGH,\r\ndev_name(&pdev->dev), mchan);\r\nif (ret < 0)\r\ngoto disable_clk;\r\nmchan->vc.desc_free = mdc_desc_free;\r\nvchan_init(&mchan->vc, &mdma->dma_dev);\r\n}\r\nmdma->desc_pool = dmam_pool_create(dev_name(&pdev->dev), &pdev->dev,\r\nsizeof(struct mdc_hw_list_desc),\r\n4, 0);\r\nif (!mdma->desc_pool) {\r\nret = -ENOMEM;\r\ngoto disable_clk;\r\n}\r\nret = dma_async_device_register(&mdma->dma_dev);\r\nif (ret)\r\ngoto disable_clk;\r\nret = of_dma_controller_register(pdev->dev.of_node, mdc_of_xlate, mdma);\r\nif (ret)\r\ngoto unregister;\r\ndev_info(&pdev->dev, "MDC with %u channels and %u threads\n",\r\nmdma->nr_channels, mdma->nr_threads);\r\nreturn 0;\r\nunregister:\r\ndma_async_device_unregister(&mdma->dma_dev);\r\ndisable_clk:\r\nclk_disable_unprepare(mdma->clk);\r\nreturn ret;\r\n}\r\nstatic int mdc_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct mdc_dma *mdma = platform_get_drvdata(pdev);\r\nstruct mdc_chan *mchan, *next;\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&mdma->dma_dev);\r\nlist_for_each_entry_safe(mchan, next, &mdma->dma_dev.channels,\r\nvc.chan.device_node) {\r\nlist_del(&mchan->vc.chan.device_node);\r\ndevm_free_irq(&pdev->dev, mchan->irq, mchan);\r\ntasklet_kill(&mchan->vc.task);\r\n}\r\nclk_disable_unprepare(mdma->clk);\r\nreturn 0;\r\n}
