static void scrub_page(struct page *page)\r\n{\r\n#ifdef CONFIG_XEN_SCRUB_PAGES\r\nclear_highpage(page);\r\n#endif\r\n}\r\nstatic void __balloon_append(struct page *page)\r\n{\r\nif (PageHighMem(page)) {\r\nlist_add_tail(&page->lru, &ballooned_pages);\r\nballoon_stats.balloon_high++;\r\n} else {\r\nlist_add(&page->lru, &ballooned_pages);\r\nballoon_stats.balloon_low++;\r\n}\r\nwake_up(&balloon_wq);\r\n}\r\nstatic void balloon_append(struct page *page)\r\n{\r\n__balloon_append(page);\r\nadjust_managed_page_count(page, -1);\r\n}\r\nstatic struct page *balloon_retrieve(bool require_lowmem)\r\n{\r\nstruct page *page;\r\nif (list_empty(&ballooned_pages))\r\nreturn NULL;\r\npage = list_entry(ballooned_pages.next, struct page, lru);\r\nif (require_lowmem && PageHighMem(page))\r\nreturn NULL;\r\nlist_del(&page->lru);\r\nif (PageHighMem(page))\r\nballoon_stats.balloon_high--;\r\nelse\r\nballoon_stats.balloon_low--;\r\nadjust_managed_page_count(page, 1);\r\nreturn page;\r\n}\r\nstatic struct page *balloon_next_page(struct page *page)\r\n{\r\nstruct list_head *next = page->lru.next;\r\nif (next == &ballooned_pages)\r\nreturn NULL;\r\nreturn list_entry(next, struct page, lru);\r\n}\r\nstatic enum bp_state update_schedule(enum bp_state state)\r\n{\r\nif (state == BP_WAIT)\r\nreturn BP_WAIT;\r\nif (state == BP_ECANCELED)\r\nreturn BP_ECANCELED;\r\nif (state == BP_DONE) {\r\nballoon_stats.schedule_delay = 1;\r\nballoon_stats.retry_count = 1;\r\nreturn BP_DONE;\r\n}\r\n++balloon_stats.retry_count;\r\nif (balloon_stats.max_retry_count != RETRY_UNLIMITED &&\r\nballoon_stats.retry_count > balloon_stats.max_retry_count) {\r\nballoon_stats.schedule_delay = 1;\r\nballoon_stats.retry_count = 1;\r\nreturn BP_ECANCELED;\r\n}\r\nballoon_stats.schedule_delay <<= 1;\r\nif (balloon_stats.schedule_delay > balloon_stats.max_schedule_delay)\r\nballoon_stats.schedule_delay = balloon_stats.max_schedule_delay;\r\nreturn BP_EAGAIN;\r\n}\r\nstatic void release_memory_resource(struct resource *resource)\r\n{\r\nif (!resource)\r\nreturn;\r\nrelease_resource(resource);\r\nkfree(resource);\r\n}\r\nstatic struct resource *additional_memory_resource(phys_addr_t size)\r\n{\r\nstruct resource *res;\r\nint ret;\r\nres = kzalloc(sizeof(*res), GFP_KERNEL);\r\nif (!res)\r\nreturn NULL;\r\nres->name = "System RAM";\r\nres->flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;\r\nret = allocate_resource(&iomem_resource, res,\r\nsize, 0, -1,\r\nPAGES_PER_SECTION * PAGE_SIZE, NULL, NULL);\r\nif (ret < 0) {\r\npr_err("Cannot allocate new System RAM resource\n");\r\nkfree(res);\r\nreturn NULL;\r\n}\r\n#ifdef CONFIG_SPARSEMEM\r\n{\r\nunsigned long limit = 1UL << (MAX_PHYSMEM_BITS - PAGE_SHIFT);\r\nunsigned long pfn = res->start >> PAGE_SHIFT;\r\nif (pfn > limit) {\r\npr_err("New System RAM resource outside addressable RAM (%lu > %lu)\n",\r\npfn, limit);\r\nrelease_memory_resource(res);\r\nreturn NULL;\r\n}\r\n}\r\n#endif\r\nreturn res;\r\n}\r\nstatic enum bp_state reserve_additional_memory(void)\r\n{\r\nlong credit;\r\nstruct resource *resource;\r\nint nid, rc;\r\nunsigned long balloon_hotplug;\r\ncredit = balloon_stats.target_pages + balloon_stats.target_unpopulated\r\n- balloon_stats.total_pages;\r\nif (credit <= 0)\r\nreturn BP_WAIT;\r\nballoon_hotplug = round_up(credit, PAGES_PER_SECTION);\r\nresource = additional_memory_resource(balloon_hotplug * PAGE_SIZE);\r\nif (!resource)\r\ngoto err;\r\nnid = memory_add_physaddr_to_nid(resource->start);\r\n#ifdef CONFIG_XEN_HAVE_PVMMU\r\nBUILD_BUG_ON(XEN_PAGE_SIZE != PAGE_SIZE);\r\nif (!xen_feature(XENFEAT_auto_translated_physmap)) {\r\nunsigned long pfn, i;\r\npfn = PFN_DOWN(resource->start);\r\nfor (i = 0; i < balloon_hotplug; i++) {\r\nif (!set_phys_to_machine(pfn + i, INVALID_P2M_ENTRY)) {\r\npr_warn("set_phys_to_machine() failed, no memory added\n");\r\ngoto err;\r\n}\r\n}\r\n}\r\n#endif\r\nmutex_unlock(&balloon_mutex);\r\nrc = add_memory_resource(nid, resource, memhp_auto_online);\r\nmutex_lock(&balloon_mutex);\r\nif (rc) {\r\npr_warn("Cannot add additional memory (%i)\n", rc);\r\ngoto err;\r\n}\r\nballoon_stats.total_pages += balloon_hotplug;\r\nreturn BP_WAIT;\r\nerr:\r\nrelease_memory_resource(resource);\r\nreturn BP_ECANCELED;\r\n}\r\nstatic void xen_online_page(struct page *page)\r\n{\r\n__online_page_set_limits(page);\r\nmutex_lock(&balloon_mutex);\r\n__balloon_append(page);\r\nmutex_unlock(&balloon_mutex);\r\n}\r\nstatic int xen_memory_notifier(struct notifier_block *nb, unsigned long val, void *v)\r\n{\r\nif (val == MEM_ONLINE)\r\nschedule_delayed_work(&balloon_worker, 0);\r\nreturn NOTIFY_OK;\r\n}\r\nstatic enum bp_state reserve_additional_memory(void)\r\n{\r\nballoon_stats.target_pages = balloon_stats.current_pages;\r\nreturn BP_ECANCELED;\r\n}\r\nstatic long current_credit(void)\r\n{\r\nreturn balloon_stats.target_pages - balloon_stats.current_pages;\r\n}\r\nstatic bool balloon_is_inflated(void)\r\n{\r\nreturn balloon_stats.balloon_low || balloon_stats.balloon_high;\r\n}\r\nstatic enum bp_state increase_reservation(unsigned long nr_pages)\r\n{\r\nint rc;\r\nunsigned long i;\r\nstruct page *page;\r\nstruct xen_memory_reservation reservation = {\r\n.address_bits = 0,\r\n.extent_order = EXTENT_ORDER,\r\n.domid = DOMID_SELF\r\n};\r\nif (nr_pages > ARRAY_SIZE(frame_list))\r\nnr_pages = ARRAY_SIZE(frame_list);\r\npage = list_first_entry_or_null(&ballooned_pages, struct page, lru);\r\nfor (i = 0; i < nr_pages; i++) {\r\nif (!page) {\r\nnr_pages = i;\r\nbreak;\r\n}\r\nframe_list[i] = page_to_xen_pfn(page);\r\npage = balloon_next_page(page);\r\n}\r\nset_xen_guest_handle(reservation.extent_start, frame_list);\r\nreservation.nr_extents = nr_pages;\r\nrc = HYPERVISOR_memory_op(XENMEM_populate_physmap, &reservation);\r\nif (rc <= 0)\r\nreturn BP_EAGAIN;\r\nfor (i = 0; i < rc; i++) {\r\npage = balloon_retrieve(false);\r\nBUG_ON(page == NULL);\r\n#ifdef CONFIG_XEN_HAVE_PVMMU\r\nBUILD_BUG_ON(XEN_PAGE_SIZE != PAGE_SIZE);\r\nif (!xen_feature(XENFEAT_auto_translated_physmap)) {\r\nunsigned long pfn = page_to_pfn(page);\r\nset_phys_to_machine(pfn, frame_list[i]);\r\nif (!PageHighMem(page)) {\r\nint ret;\r\nret = HYPERVISOR_update_va_mapping(\r\n(unsigned long)__va(pfn << PAGE_SHIFT),\r\nmfn_pte(frame_list[i], PAGE_KERNEL),\r\n0);\r\nBUG_ON(ret);\r\n}\r\n}\r\n#endif\r\n__free_reserved_page(page);\r\n}\r\nballoon_stats.current_pages += rc;\r\nreturn BP_DONE;\r\n}\r\nstatic enum bp_state decrease_reservation(unsigned long nr_pages, gfp_t gfp)\r\n{\r\nenum bp_state state = BP_DONE;\r\nunsigned long i;\r\nstruct page *page, *tmp;\r\nint ret;\r\nstruct xen_memory_reservation reservation = {\r\n.address_bits = 0,\r\n.extent_order = EXTENT_ORDER,\r\n.domid = DOMID_SELF\r\n};\r\nLIST_HEAD(pages);\r\nif (nr_pages > ARRAY_SIZE(frame_list))\r\nnr_pages = ARRAY_SIZE(frame_list);\r\nfor (i = 0; i < nr_pages; i++) {\r\npage = alloc_page(gfp);\r\nif (page == NULL) {\r\nnr_pages = i;\r\nstate = BP_EAGAIN;\r\nbreak;\r\n}\r\nscrub_page(page);\r\nlist_add(&page->lru, &pages);\r\n}\r\nkmap_flush_unused();\r\ni = 0;\r\nlist_for_each_entry_safe(page, tmp, &pages, lru) {\r\nframe_list[i++] = xen_page_to_gfn(page);\r\n#ifdef CONFIG_XEN_HAVE_PVMMU\r\nBUILD_BUG_ON(XEN_PAGE_SIZE != PAGE_SIZE);\r\nif (!xen_feature(XENFEAT_auto_translated_physmap)) {\r\nunsigned long pfn = page_to_pfn(page);\r\nif (!PageHighMem(page)) {\r\nret = HYPERVISOR_update_va_mapping(\r\n(unsigned long)__va(pfn << PAGE_SHIFT),\r\n__pte_ma(0), 0);\r\nBUG_ON(ret);\r\n}\r\n__set_phys_to_machine(pfn, INVALID_P2M_ENTRY);\r\n}\r\n#endif\r\nlist_del(&page->lru);\r\nballoon_append(page);\r\n}\r\nflush_tlb_all();\r\nset_xen_guest_handle(reservation.extent_start, frame_list);\r\nreservation.nr_extents = nr_pages;\r\nret = HYPERVISOR_memory_op(XENMEM_decrease_reservation, &reservation);\r\nBUG_ON(ret != nr_pages);\r\nballoon_stats.current_pages -= nr_pages;\r\nreturn state;\r\n}\r\nstatic void balloon_process(struct work_struct *work)\r\n{\r\nenum bp_state state = BP_DONE;\r\nlong credit;\r\ndo {\r\nmutex_lock(&balloon_mutex);\r\ncredit = current_credit();\r\nif (credit > 0) {\r\nif (balloon_is_inflated())\r\nstate = increase_reservation(credit);\r\nelse\r\nstate = reserve_additional_memory();\r\n}\r\nif (credit < 0)\r\nstate = decrease_reservation(-credit, GFP_BALLOON);\r\nstate = update_schedule(state);\r\nmutex_unlock(&balloon_mutex);\r\ncond_resched();\r\n} while (credit && state == BP_DONE);\r\nif (state == BP_EAGAIN)\r\nschedule_delayed_work(&balloon_worker, balloon_stats.schedule_delay * HZ);\r\n}\r\nvoid balloon_set_new_target(unsigned long target)\r\n{\r\nballoon_stats.target_pages = target;\r\nschedule_delayed_work(&balloon_worker, 0);\r\n}\r\nstatic int add_ballooned_pages(int nr_pages)\r\n{\r\nenum bp_state st;\r\nif (xen_hotplug_unpopulated) {\r\nst = reserve_additional_memory();\r\nif (st != BP_ECANCELED) {\r\nmutex_unlock(&balloon_mutex);\r\nwait_event(balloon_wq,\r\n!list_empty(&ballooned_pages));\r\nmutex_lock(&balloon_mutex);\r\nreturn 0;\r\n}\r\n}\r\nst = decrease_reservation(nr_pages, GFP_USER);\r\nif (st != BP_DONE)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nint alloc_xenballooned_pages(int nr_pages, struct page **pages)\r\n{\r\nint pgno = 0;\r\nstruct page *page;\r\nint ret;\r\nmutex_lock(&balloon_mutex);\r\nballoon_stats.target_unpopulated += nr_pages;\r\nwhile (pgno < nr_pages) {\r\npage = balloon_retrieve(true);\r\nif (page) {\r\npages[pgno++] = page;\r\n#ifdef CONFIG_XEN_HAVE_PVMMU\r\nBUILD_BUG_ON(XEN_PAGE_SIZE != PAGE_SIZE);\r\nret = xen_alloc_p2m_entry(page_to_pfn(page));\r\nif (ret < 0)\r\ngoto out_undo;\r\n#endif\r\n} else {\r\nret = add_ballooned_pages(nr_pages - pgno);\r\nif (ret < 0)\r\ngoto out_undo;\r\n}\r\n}\r\nmutex_unlock(&balloon_mutex);\r\nreturn 0;\r\nout_undo:\r\nmutex_unlock(&balloon_mutex);\r\nfree_xenballooned_pages(pgno, pages);\r\nreturn ret;\r\n}\r\nvoid free_xenballooned_pages(int nr_pages, struct page **pages)\r\n{\r\nint i;\r\nmutex_lock(&balloon_mutex);\r\nfor (i = 0; i < nr_pages; i++) {\r\nif (pages[i])\r\nballoon_append(pages[i]);\r\n}\r\nballoon_stats.target_unpopulated -= nr_pages;\r\nif (current_credit())\r\nschedule_delayed_work(&balloon_worker, 0);\r\nmutex_unlock(&balloon_mutex);\r\n}\r\nstatic void __init balloon_add_region(unsigned long start_pfn,\r\nunsigned long pages)\r\n{\r\nunsigned long pfn, extra_pfn_end;\r\nstruct page *page;\r\nextra_pfn_end = min(max_pfn, start_pfn + pages);\r\nfor (pfn = start_pfn; pfn < extra_pfn_end; pfn++) {\r\npage = pfn_to_page(pfn);\r\n__balloon_append(page);\r\n}\r\nballoon_stats.total_pages += extra_pfn_end - start_pfn;\r\n}\r\nstatic int __init balloon_init(void)\r\n{\r\nint i;\r\nif (!xen_domain())\r\nreturn -ENODEV;\r\npr_info("Initialising balloon driver\n");\r\nballoon_stats.current_pages = xen_pv_domain()\r\n? min(xen_start_info->nr_pages - xen_released_pages, max_pfn)\r\n: get_num_physpages();\r\nballoon_stats.target_pages = balloon_stats.current_pages;\r\nballoon_stats.balloon_low = 0;\r\nballoon_stats.balloon_high = 0;\r\nballoon_stats.total_pages = balloon_stats.current_pages;\r\nballoon_stats.schedule_delay = 1;\r\nballoon_stats.max_schedule_delay = 32;\r\nballoon_stats.retry_count = 1;\r\nballoon_stats.max_retry_count = RETRY_UNLIMITED;\r\n#ifdef CONFIG_XEN_BALLOON_MEMORY_HOTPLUG\r\nset_online_page_callback(&xen_online_page);\r\nregister_memory_notifier(&xen_memory_nb);\r\nregister_sysctl_table(xen_root);\r\n#endif\r\nfor (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++)\r\nif (xen_extra_mem[i].n_pfns)\r\nballoon_add_region(xen_extra_mem[i].start_pfn,\r\nxen_extra_mem[i].n_pfns);\r\nreturn 0;\r\n}
