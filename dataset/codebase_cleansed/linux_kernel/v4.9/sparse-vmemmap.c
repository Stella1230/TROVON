static void * __ref __earlyonly_bootmem_alloc(int node,\r\nunsigned long size,\r\nunsigned long align,\r\nunsigned long goal)\r\n{\r\nreturn memblock_virt_alloc_try_nid(size, align, goal,\r\nBOOTMEM_ALLOC_ACCESSIBLE, node);\r\n}\r\nvoid * __meminit vmemmap_alloc_block(unsigned long size, int node)\r\n{\r\nif (slab_is_available()) {\r\nstruct page *page;\r\nif (node_state(node, N_HIGH_MEMORY))\r\npage = alloc_pages_node(\r\nnode, GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,\r\nget_order(size));\r\nelse\r\npage = alloc_pages(\r\nGFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,\r\nget_order(size));\r\nif (page)\r\nreturn page_address(page);\r\nreturn NULL;\r\n} else\r\nreturn __earlyonly_bootmem_alloc(node, size, size,\r\n__pa(MAX_DMA_ADDRESS));\r\n}\r\nstatic void * __meminit alloc_block_buf(unsigned long size, int node)\r\n{\r\nvoid *ptr;\r\nif (!vmemmap_buf)\r\nreturn vmemmap_alloc_block(size, node);\r\nptr = (void *)ALIGN((unsigned long)vmemmap_buf, size);\r\nif (ptr + size > vmemmap_buf_end)\r\nreturn vmemmap_alloc_block(size, node);\r\nvmemmap_buf = ptr + size;\r\nreturn ptr;\r\n}\r\nstatic unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)\r\n{\r\nreturn altmap->base_pfn + altmap->reserve + altmap->alloc\r\n+ altmap->align;\r\n}\r\nstatic unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)\r\n{\r\nunsigned long allocated = altmap->alloc + altmap->align;\r\nif (altmap->free > allocated)\r\nreturn altmap->free - allocated;\r\nreturn 0;\r\n}\r\nstatic unsigned long __meminit vmem_altmap_alloc(struct vmem_altmap *altmap,\r\nunsigned long nr_pfns)\r\n{\r\nunsigned long pfn = vmem_altmap_next_pfn(altmap);\r\nunsigned long nr_align;\r\nnr_align = 1UL << find_first_bit(&nr_pfns, BITS_PER_LONG);\r\nnr_align = ALIGN(pfn, nr_align) - pfn;\r\nif (nr_pfns + nr_align > vmem_altmap_nr_free(altmap))\r\nreturn ULONG_MAX;\r\naltmap->alloc += nr_pfns;\r\naltmap->align += nr_align;\r\nreturn pfn + nr_align;\r\n}\r\nstatic void * __meminit altmap_alloc_block_buf(unsigned long size,\r\nstruct vmem_altmap *altmap)\r\n{\r\nunsigned long pfn, nr_pfns;\r\nvoid *ptr;\r\nif (size & ~PAGE_MASK) {\r\npr_warn_once("%s: allocations must be multiple of PAGE_SIZE (%ld)\n",\r\n__func__, size);\r\nreturn NULL;\r\n}\r\nnr_pfns = size >> PAGE_SHIFT;\r\npfn = vmem_altmap_alloc(altmap, nr_pfns);\r\nif (pfn < ULONG_MAX)\r\nptr = __va(__pfn_to_phys(pfn));\r\nelse\r\nptr = NULL;\r\npr_debug("%s: pfn: %#lx alloc: %ld align: %ld nr: %#lx\n",\r\n__func__, pfn, altmap->alloc, altmap->align, nr_pfns);\r\nreturn ptr;\r\n}\r\nvoid * __meminit __vmemmap_alloc_block_buf(unsigned long size, int node,\r\nstruct vmem_altmap *altmap)\r\n{\r\nif (altmap)\r\nreturn altmap_alloc_block_buf(size, altmap);\r\nreturn alloc_block_buf(size, node);\r\n}\r\nvoid __meminit vmemmap_verify(pte_t *pte, int node,\r\nunsigned long start, unsigned long end)\r\n{\r\nunsigned long pfn = pte_pfn(*pte);\r\nint actual_node = early_pfn_to_nid(pfn);\r\nif (node_distance(actual_node, node) > LOCAL_DISTANCE)\r\npr_warn("[%lx-%lx] potential offnode page_structs\n",\r\nstart, end - 1);\r\n}\r\npte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node)\r\n{\r\npte_t *pte = pte_offset_kernel(pmd, addr);\r\nif (pte_none(*pte)) {\r\npte_t entry;\r\nvoid *p = alloc_block_buf(PAGE_SIZE, node);\r\nif (!p)\r\nreturn NULL;\r\nentry = pfn_pte(__pa(p) >> PAGE_SHIFT, PAGE_KERNEL);\r\nset_pte_at(&init_mm, addr, pte, entry);\r\n}\r\nreturn pte;\r\n}\r\npmd_t * __meminit vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node)\r\n{\r\npmd_t *pmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd)) {\r\nvoid *p = vmemmap_alloc_block(PAGE_SIZE, node);\r\nif (!p)\r\nreturn NULL;\r\npmd_populate_kernel(&init_mm, pmd, p);\r\n}\r\nreturn pmd;\r\n}\r\npud_t * __meminit vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node)\r\n{\r\npud_t *pud = pud_offset(pgd, addr);\r\nif (pud_none(*pud)) {\r\nvoid *p = vmemmap_alloc_block(PAGE_SIZE, node);\r\nif (!p)\r\nreturn NULL;\r\npud_populate(&init_mm, pud, p);\r\n}\r\nreturn pud;\r\n}\r\npgd_t * __meminit vmemmap_pgd_populate(unsigned long addr, int node)\r\n{\r\npgd_t *pgd = pgd_offset_k(addr);\r\nif (pgd_none(*pgd)) {\r\nvoid *p = vmemmap_alloc_block(PAGE_SIZE, node);\r\nif (!p)\r\nreturn NULL;\r\npgd_populate(&init_mm, pgd, p);\r\n}\r\nreturn pgd;\r\n}\r\nint __meminit vmemmap_populate_basepages(unsigned long start,\r\nunsigned long end, int node)\r\n{\r\nunsigned long addr = start;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nfor (; addr < end; addr += PAGE_SIZE) {\r\npgd = vmemmap_pgd_populate(addr, node);\r\nif (!pgd)\r\nreturn -ENOMEM;\r\npud = vmemmap_pud_populate(pgd, addr, node);\r\nif (!pud)\r\nreturn -ENOMEM;\r\npmd = vmemmap_pmd_populate(pud, addr, node);\r\nif (!pmd)\r\nreturn -ENOMEM;\r\npte = vmemmap_pte_populate(pmd, addr, node);\r\nif (!pte)\r\nreturn -ENOMEM;\r\nvmemmap_verify(pte, node, addr, addr + PAGE_SIZE);\r\n}\r\nreturn 0;\r\n}\r\nstruct page * __meminit sparse_mem_map_populate(unsigned long pnum, int nid)\r\n{\r\nunsigned long start;\r\nunsigned long end;\r\nstruct page *map;\r\nmap = pfn_to_page(pnum * PAGES_PER_SECTION);\r\nstart = (unsigned long)map;\r\nend = (unsigned long)(map + PAGES_PER_SECTION);\r\nif (vmemmap_populate(start, end, nid))\r\nreturn NULL;\r\nreturn map;\r\n}\r\nvoid __init sparse_mem_maps_populate_node(struct page **map_map,\r\nunsigned long pnum_begin,\r\nunsigned long pnum_end,\r\nunsigned long map_count, int nodeid)\r\n{\r\nunsigned long pnum;\r\nunsigned long size = sizeof(struct page) * PAGES_PER_SECTION;\r\nvoid *vmemmap_buf_start;\r\nsize = ALIGN(size, PMD_SIZE);\r\nvmemmap_buf_start = __earlyonly_bootmem_alloc(nodeid, size * map_count,\r\nPMD_SIZE, __pa(MAX_DMA_ADDRESS));\r\nif (vmemmap_buf_start) {\r\nvmemmap_buf = vmemmap_buf_start;\r\nvmemmap_buf_end = vmemmap_buf_start + size * map_count;\r\n}\r\nfor (pnum = pnum_begin; pnum < pnum_end; pnum++) {\r\nstruct mem_section *ms;\r\nif (!present_section_nr(pnum))\r\ncontinue;\r\nmap_map[pnum] = sparse_mem_map_populate(pnum, nodeid);\r\nif (map_map[pnum])\r\ncontinue;\r\nms = __nr_to_section(pnum);\r\npr_err("%s: sparsemem memory map backing failed some memory will not be available\n",\r\n__func__);\r\nms->section_mem_map = 0;\r\n}\r\nif (vmemmap_buf_start) {\r\nmemblock_free_early(__pa(vmemmap_buf),\r\nvmemmap_buf_end - vmemmap_buf);\r\nvmemmap_buf = NULL;\r\nvmemmap_buf_end = NULL;\r\n}\r\n}
