static void iot_init(struct io_tracker *iot)\r\n{\r\nspin_lock_init(&iot->lock);\r\niot->in_flight = 0ul;\r\niot->idle_time = 0ul;\r\niot->last_update_time = jiffies;\r\n}\r\nstatic bool __iot_idle_for(struct io_tracker *iot, unsigned long jifs)\r\n{\r\nif (iot->in_flight)\r\nreturn false;\r\nreturn time_after(jiffies, iot->idle_time + jifs);\r\n}\r\nstatic bool iot_idle_for(struct io_tracker *iot, unsigned long jifs)\r\n{\r\nbool r;\r\nunsigned long flags;\r\nspin_lock_irqsave(&iot->lock, flags);\r\nr = __iot_idle_for(iot, jifs);\r\nspin_unlock_irqrestore(&iot->lock, flags);\r\nreturn r;\r\n}\r\nstatic void iot_io_begin(struct io_tracker *iot, sector_t len)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&iot->lock, flags);\r\niot->in_flight += len;\r\nspin_unlock_irqrestore(&iot->lock, flags);\r\n}\r\nstatic void __iot_io_end(struct io_tracker *iot, sector_t len)\r\n{\r\niot->in_flight -= len;\r\nif (!iot->in_flight)\r\niot->idle_time = jiffies;\r\n}\r\nstatic void iot_io_end(struct io_tracker *iot, sector_t len)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&iot->lock, flags);\r\n__iot_io_end(iot, len);\r\nspin_unlock_irqrestore(&iot->lock, flags);\r\n}\r\nstatic void dm_hook_bio(struct dm_hook_info *h, struct bio *bio,\r\nbio_end_io_t *bi_end_io, void *bi_private)\r\n{\r\nh->bi_end_io = bio->bi_end_io;\r\nbio->bi_end_io = bi_end_io;\r\nbio->bi_private = bi_private;\r\n}\r\nstatic void dm_unhook_bio(struct dm_hook_info *h, struct bio *bio)\r\n{\r\nbio->bi_end_io = h->bi_end_io;\r\n}\r\nstatic void wake_worker(struct cache *cache)\r\n{\r\nqueue_work(cache->wq, &cache->worker);\r\n}\r\nstatic struct dm_bio_prison_cell *alloc_prison_cell(struct cache *cache)\r\n{\r\nreturn dm_bio_prison_alloc_cell(cache->prison, GFP_NOWAIT);\r\n}\r\nstatic void free_prison_cell(struct cache *cache, struct dm_bio_prison_cell *cell)\r\n{\r\ndm_bio_prison_free_cell(cache->prison, cell);\r\n}\r\nstatic struct dm_cache_migration *alloc_migration(struct cache *cache)\r\n{\r\nstruct dm_cache_migration *mg;\r\nmg = mempool_alloc(cache->migration_pool, GFP_NOWAIT);\r\nif (mg) {\r\nmg->cache = cache;\r\natomic_inc(&mg->cache->nr_allocated_migrations);\r\n}\r\nreturn mg;\r\n}\r\nstatic void free_migration(struct dm_cache_migration *mg)\r\n{\r\nstruct cache *cache = mg->cache;\r\nif (atomic_dec_and_test(&cache->nr_allocated_migrations))\r\nwake_up(&cache->migration_wait);\r\nmempool_free(mg, cache->migration_pool);\r\n}\r\nstatic int prealloc_data_structs(struct cache *cache, struct prealloc *p)\r\n{\r\nif (!p->mg) {\r\np->mg = alloc_migration(cache);\r\nif (!p->mg)\r\nreturn -ENOMEM;\r\n}\r\nif (!p->cell1) {\r\np->cell1 = alloc_prison_cell(cache);\r\nif (!p->cell1)\r\nreturn -ENOMEM;\r\n}\r\nif (!p->cell2) {\r\np->cell2 = alloc_prison_cell(cache);\r\nif (!p->cell2)\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void prealloc_free_structs(struct cache *cache, struct prealloc *p)\r\n{\r\nif (p->cell2)\r\nfree_prison_cell(cache, p->cell2);\r\nif (p->cell1)\r\nfree_prison_cell(cache, p->cell1);\r\nif (p->mg)\r\nfree_migration(p->mg);\r\n}\r\nstatic struct dm_cache_migration *prealloc_get_migration(struct prealloc *p)\r\n{\r\nstruct dm_cache_migration *mg = p->mg;\r\nBUG_ON(!mg);\r\np->mg = NULL;\r\nreturn mg;\r\n}\r\nstatic struct dm_bio_prison_cell *prealloc_get_cell(struct prealloc *p)\r\n{\r\nstruct dm_bio_prison_cell *r = NULL;\r\nif (p->cell1) {\r\nr = p->cell1;\r\np->cell1 = NULL;\r\n} else if (p->cell2) {\r\nr = p->cell2;\r\np->cell2 = NULL;\r\n} else\r\nBUG();\r\nreturn r;\r\n}\r\nstatic void prealloc_put_cell(struct prealloc *p, struct dm_bio_prison_cell *cell)\r\n{\r\nif (!p->cell2)\r\np->cell2 = cell;\r\nelse if (!p->cell1)\r\np->cell1 = cell;\r\nelse\r\nBUG();\r\n}\r\nstatic void build_key(dm_oblock_t begin, dm_oblock_t end, struct dm_cell_key *key)\r\n{\r\nkey->virtual = 0;\r\nkey->dev = 0;\r\nkey->block_begin = from_oblock(begin);\r\nkey->block_end = from_oblock(end);\r\n}\r\nstatic int bio_detain_range(struct cache *cache, dm_oblock_t oblock_begin, dm_oblock_t oblock_end,\r\nstruct bio *bio, struct dm_bio_prison_cell *cell_prealloc,\r\ncell_free_fn free_fn, void *free_context,\r\nstruct dm_bio_prison_cell **cell_result)\r\n{\r\nint r;\r\nstruct dm_cell_key key;\r\nbuild_key(oblock_begin, oblock_end, &key);\r\nr = dm_bio_detain(cache->prison, &key, bio, cell_prealloc, cell_result);\r\nif (r)\r\nfree_fn(free_context, cell_prealloc);\r\nreturn r;\r\n}\r\nstatic int bio_detain(struct cache *cache, dm_oblock_t oblock,\r\nstruct bio *bio, struct dm_bio_prison_cell *cell_prealloc,\r\ncell_free_fn free_fn, void *free_context,\r\nstruct dm_bio_prison_cell **cell_result)\r\n{\r\ndm_oblock_t end = to_oblock(from_oblock(oblock) + 1ULL);\r\nreturn bio_detain_range(cache, oblock, end, bio,\r\ncell_prealloc, free_fn, free_context, cell_result);\r\n}\r\nstatic int get_cell(struct cache *cache,\r\ndm_oblock_t oblock,\r\nstruct prealloc *structs,\r\nstruct dm_bio_prison_cell **cell_result)\r\n{\r\nint r;\r\nstruct dm_cell_key key;\r\nstruct dm_bio_prison_cell *cell_prealloc;\r\ncell_prealloc = prealloc_get_cell(structs);\r\nbuild_key(oblock, to_oblock(from_oblock(oblock) + 1ULL), &key);\r\nr = dm_get_cell(cache->prison, &key, cell_prealloc, cell_result);\r\nif (r)\r\nprealloc_put_cell(structs, cell_prealloc);\r\nreturn r;\r\n}\r\nstatic bool is_dirty(struct cache *cache, dm_cblock_t b)\r\n{\r\nreturn test_bit(from_cblock(b), cache->dirty_bitset);\r\n}\r\nstatic void set_dirty(struct cache *cache, dm_oblock_t oblock, dm_cblock_t cblock)\r\n{\r\nif (!test_and_set_bit(from_cblock(cblock), cache->dirty_bitset)) {\r\natomic_inc(&cache->nr_dirty);\r\npolicy_set_dirty(cache->policy, oblock);\r\n}\r\n}\r\nstatic void clear_dirty(struct cache *cache, dm_oblock_t oblock, dm_cblock_t cblock)\r\n{\r\nif (test_and_clear_bit(from_cblock(cblock), cache->dirty_bitset)) {\r\npolicy_clear_dirty(cache->policy, oblock);\r\nif (atomic_dec_return(&cache->nr_dirty) == 0)\r\ndm_table_event(cache->ti->table);\r\n}\r\n}\r\nstatic bool block_size_is_power_of_two(struct cache *cache)\r\n{\r\nreturn cache->sectors_per_block_shift >= 0;\r\n}\r\n__always_inline\r\n#endif\r\nstatic dm_block_t block_div(dm_block_t b, uint32_t n)\r\n{\r\ndo_div(b, n);\r\nreturn b;\r\n}\r\nstatic dm_block_t oblocks_per_dblock(struct cache *cache)\r\n{\r\ndm_block_t oblocks = cache->discard_block_size;\r\nif (block_size_is_power_of_two(cache))\r\noblocks >>= cache->sectors_per_block_shift;\r\nelse\r\noblocks = block_div(oblocks, cache->sectors_per_block);\r\nreturn oblocks;\r\n}\r\nstatic dm_dblock_t oblock_to_dblock(struct cache *cache, dm_oblock_t oblock)\r\n{\r\nreturn to_dblock(block_div(from_oblock(oblock),\r\noblocks_per_dblock(cache)));\r\n}\r\nstatic dm_oblock_t dblock_to_oblock(struct cache *cache, dm_dblock_t dblock)\r\n{\r\nreturn to_oblock(from_dblock(dblock) * oblocks_per_dblock(cache));\r\n}\r\nstatic void set_discard(struct cache *cache, dm_dblock_t b)\r\n{\r\nunsigned long flags;\r\nBUG_ON(from_dblock(b) >= from_dblock(cache->discard_nr_blocks));\r\natomic_inc(&cache->stats.discard_count);\r\nspin_lock_irqsave(&cache->lock, flags);\r\nset_bit(from_dblock(b), cache->discard_bitset);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\n}\r\nstatic void clear_discard(struct cache *cache, dm_dblock_t b)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&cache->lock, flags);\r\nclear_bit(from_dblock(b), cache->discard_bitset);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\n}\r\nstatic bool is_discarded(struct cache *cache, dm_dblock_t b)\r\n{\r\nint r;\r\nunsigned long flags;\r\nspin_lock_irqsave(&cache->lock, flags);\r\nr = test_bit(from_dblock(b), cache->discard_bitset);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nreturn r;\r\n}\r\nstatic bool is_discarded_oblock(struct cache *cache, dm_oblock_t b)\r\n{\r\nint r;\r\nunsigned long flags;\r\nspin_lock_irqsave(&cache->lock, flags);\r\nr = test_bit(from_dblock(oblock_to_dblock(cache, b)),\r\ncache->discard_bitset);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nreturn r;\r\n}\r\nstatic void load_stats(struct cache *cache)\r\n{\r\nstruct dm_cache_statistics stats;\r\ndm_cache_metadata_get_stats(cache->cmd, &stats);\r\natomic_set(&cache->stats.read_hit, stats.read_hits);\r\natomic_set(&cache->stats.read_miss, stats.read_misses);\r\natomic_set(&cache->stats.write_hit, stats.write_hits);\r\natomic_set(&cache->stats.write_miss, stats.write_misses);\r\n}\r\nstatic void save_stats(struct cache *cache)\r\n{\r\nstruct dm_cache_statistics stats;\r\nif (get_cache_mode(cache) >= CM_READ_ONLY)\r\nreturn;\r\nstats.read_hits = atomic_read(&cache->stats.read_hit);\r\nstats.read_misses = atomic_read(&cache->stats.read_miss);\r\nstats.write_hits = atomic_read(&cache->stats.write_hit);\r\nstats.write_misses = atomic_read(&cache->stats.write_miss);\r\ndm_cache_metadata_set_stats(cache->cmd, &stats);\r\n}\r\nstatic bool writethrough_mode(struct cache_features *f)\r\n{\r\nreturn f->io_mode == CM_IO_WRITETHROUGH;\r\n}\r\nstatic bool writeback_mode(struct cache_features *f)\r\n{\r\nreturn f->io_mode == CM_IO_WRITEBACK;\r\n}\r\nstatic bool passthrough_mode(struct cache_features *f)\r\n{\r\nreturn f->io_mode == CM_IO_PASSTHROUGH;\r\n}\r\nstatic size_t get_per_bio_data_size(struct cache *cache)\r\n{\r\nreturn writethrough_mode(&cache->features) ? PB_DATA_SIZE_WT : PB_DATA_SIZE_WB;\r\n}\r\nstatic struct per_bio_data *get_per_bio_data(struct bio *bio, size_t data_size)\r\n{\r\nstruct per_bio_data *pb = dm_per_bio_data(bio, data_size);\r\nBUG_ON(!pb);\r\nreturn pb;\r\n}\r\nstatic struct per_bio_data *init_per_bio_data(struct bio *bio, size_t data_size)\r\n{\r\nstruct per_bio_data *pb = get_per_bio_data(bio, data_size);\r\npb->tick = false;\r\npb->req_nr = dm_bio_get_target_bio_nr(bio);\r\npb->all_io_entry = NULL;\r\npb->len = 0;\r\nreturn pb;\r\n}\r\nstatic void remap_to_origin(struct cache *cache, struct bio *bio)\r\n{\r\nbio->bi_bdev = cache->origin_dev->bdev;\r\n}\r\nstatic void remap_to_cache(struct cache *cache, struct bio *bio,\r\ndm_cblock_t cblock)\r\n{\r\nsector_t bi_sector = bio->bi_iter.bi_sector;\r\nsector_t block = from_cblock(cblock);\r\nbio->bi_bdev = cache->cache_dev->bdev;\r\nif (!block_size_is_power_of_two(cache))\r\nbio->bi_iter.bi_sector =\r\n(block * cache->sectors_per_block) +\r\nsector_div(bi_sector, cache->sectors_per_block);\r\nelse\r\nbio->bi_iter.bi_sector =\r\n(block << cache->sectors_per_block_shift) |\r\n(bi_sector & (cache->sectors_per_block - 1));\r\n}\r\nstatic void check_if_tick_bio_needed(struct cache *cache, struct bio *bio)\r\n{\r\nunsigned long flags;\r\nsize_t pb_data_size = get_per_bio_data_size(cache);\r\nstruct per_bio_data *pb = get_per_bio_data(bio, pb_data_size);\r\nspin_lock_irqsave(&cache->lock, flags);\r\nif (cache->need_tick_bio &&\r\n!(bio->bi_opf & (REQ_FUA | REQ_PREFLUSH)) &&\r\nbio_op(bio) != REQ_OP_DISCARD) {\r\npb->tick = true;\r\ncache->need_tick_bio = false;\r\n}\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\n}\r\nstatic void remap_to_origin_clear_discard(struct cache *cache, struct bio *bio,\r\ndm_oblock_t oblock)\r\n{\r\ncheck_if_tick_bio_needed(cache, bio);\r\nremap_to_origin(cache, bio);\r\nif (bio_data_dir(bio) == WRITE)\r\nclear_discard(cache, oblock_to_dblock(cache, oblock));\r\n}\r\nstatic void remap_to_cache_dirty(struct cache *cache, struct bio *bio,\r\ndm_oblock_t oblock, dm_cblock_t cblock)\r\n{\r\ncheck_if_tick_bio_needed(cache, bio);\r\nremap_to_cache(cache, bio, cblock);\r\nif (bio_data_dir(bio) == WRITE) {\r\nset_dirty(cache, oblock, cblock);\r\nclear_discard(cache, oblock_to_dblock(cache, oblock));\r\n}\r\n}\r\nstatic dm_oblock_t get_bio_block(struct cache *cache, struct bio *bio)\r\n{\r\nsector_t block_nr = bio->bi_iter.bi_sector;\r\nif (!block_size_is_power_of_two(cache))\r\n(void) sector_div(block_nr, cache->sectors_per_block);\r\nelse\r\nblock_nr >>= cache->sectors_per_block_shift;\r\nreturn to_oblock(block_nr);\r\n}\r\nstatic int bio_triggers_commit(struct cache *cache, struct bio *bio)\r\n{\r\nreturn bio->bi_opf & (REQ_PREFLUSH | REQ_FUA);\r\n}\r\nstatic void inc_ds(struct cache *cache, struct bio *bio,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nsize_t pb_data_size = get_per_bio_data_size(cache);\r\nstruct per_bio_data *pb = get_per_bio_data(bio, pb_data_size);\r\nBUG_ON(!cell);\r\nBUG_ON(pb->all_io_entry);\r\npb->all_io_entry = dm_deferred_entry_inc(cache->all_io_ds);\r\n}\r\nstatic bool accountable_bio(struct cache *cache, struct bio *bio)\r\n{\r\nreturn ((bio->bi_bdev == cache->origin_dev->bdev) &&\r\nbio_op(bio) != REQ_OP_DISCARD);\r\n}\r\nstatic void accounted_begin(struct cache *cache, struct bio *bio)\r\n{\r\nsize_t pb_data_size = get_per_bio_data_size(cache);\r\nstruct per_bio_data *pb = get_per_bio_data(bio, pb_data_size);\r\nif (accountable_bio(cache, bio)) {\r\npb->len = bio_sectors(bio);\r\niot_io_begin(&cache->origin_tracker, pb->len);\r\n}\r\n}\r\nstatic void accounted_complete(struct cache *cache, struct bio *bio)\r\n{\r\nsize_t pb_data_size = get_per_bio_data_size(cache);\r\nstruct per_bio_data *pb = get_per_bio_data(bio, pb_data_size);\r\niot_io_end(&cache->origin_tracker, pb->len);\r\n}\r\nstatic void accounted_request(struct cache *cache, struct bio *bio)\r\n{\r\naccounted_begin(cache, bio);\r\ngeneric_make_request(bio);\r\n}\r\nstatic void issue(struct cache *cache, struct bio *bio)\r\n{\r\nunsigned long flags;\r\nif (!bio_triggers_commit(cache, bio)) {\r\naccounted_request(cache, bio);\r\nreturn;\r\n}\r\nspin_lock_irqsave(&cache->lock, flags);\r\ncache->commit_requested = true;\r\nbio_list_add(&cache->deferred_flush_bios, bio);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\n}\r\nstatic void inc_and_issue(struct cache *cache, struct bio *bio, struct dm_bio_prison_cell *cell)\r\n{\r\ninc_ds(cache, bio, cell);\r\nissue(cache, bio);\r\n}\r\nstatic void defer_writethrough_bio(struct cache *cache, struct bio *bio)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&cache->lock, flags);\r\nbio_list_add(&cache->deferred_writethrough_bios, bio);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwake_worker(cache);\r\n}\r\nstatic void writethrough_endio(struct bio *bio)\r\n{\r\nstruct per_bio_data *pb = get_per_bio_data(bio, PB_DATA_SIZE_WT);\r\ndm_unhook_bio(&pb->hook_info, bio);\r\nif (bio->bi_error) {\r\nbio_endio(bio);\r\nreturn;\r\n}\r\ndm_bio_restore(&pb->bio_details, bio);\r\nremap_to_cache(pb->cache, bio, pb->cblock);\r\ndefer_writethrough_bio(pb->cache, bio);\r\n}\r\nstatic void remap_to_origin_then_cache(struct cache *cache, struct bio *bio,\r\ndm_oblock_t oblock, dm_cblock_t cblock)\r\n{\r\nstruct per_bio_data *pb = get_per_bio_data(bio, PB_DATA_SIZE_WT);\r\npb->cache = cache;\r\npb->cblock = cblock;\r\ndm_hook_bio(&pb->hook_info, bio, writethrough_endio, NULL);\r\ndm_bio_record(&pb->bio_details, bio);\r\nremap_to_origin_clear_discard(pb->cache, bio, oblock);\r\n}\r\nstatic enum cache_metadata_mode get_cache_mode(struct cache *cache)\r\n{\r\nreturn cache->features.mode;\r\n}\r\nstatic const char *cache_device_name(struct cache *cache)\r\n{\r\nreturn dm_device_name(dm_table_get_md(cache->ti->table));\r\n}\r\nstatic void notify_mode_switch(struct cache *cache, enum cache_metadata_mode mode)\r\n{\r\nconst char *descs[] = {\r\n"write",\r\n"read-only",\r\n"fail"\r\n};\r\ndm_table_event(cache->ti->table);\r\nDMINFO("%s: switching cache to %s mode",\r\ncache_device_name(cache), descs[(int)mode]);\r\n}\r\nstatic void set_cache_mode(struct cache *cache, enum cache_metadata_mode new_mode)\r\n{\r\nbool needs_check;\r\nenum cache_metadata_mode old_mode = get_cache_mode(cache);\r\nif (dm_cache_metadata_needs_check(cache->cmd, &needs_check)) {\r\nDMERR("unable to read needs_check flag, setting failure mode");\r\nnew_mode = CM_FAIL;\r\n}\r\nif (new_mode == CM_WRITE && needs_check) {\r\nDMERR("%s: unable to switch cache to write mode until repaired.",\r\ncache_device_name(cache));\r\nif (old_mode != new_mode)\r\nnew_mode = old_mode;\r\nelse\r\nnew_mode = CM_READ_ONLY;\r\n}\r\nif (old_mode == CM_FAIL)\r\nnew_mode = CM_FAIL;\r\nswitch (new_mode) {\r\ncase CM_FAIL:\r\ncase CM_READ_ONLY:\r\ndm_cache_metadata_set_read_only(cache->cmd);\r\nbreak;\r\ncase CM_WRITE:\r\ndm_cache_metadata_set_read_write(cache->cmd);\r\nbreak;\r\n}\r\ncache->features.mode = new_mode;\r\nif (new_mode != old_mode)\r\nnotify_mode_switch(cache, new_mode);\r\n}\r\nstatic void abort_transaction(struct cache *cache)\r\n{\r\nconst char *dev_name = cache_device_name(cache);\r\nif (get_cache_mode(cache) >= CM_READ_ONLY)\r\nreturn;\r\nif (dm_cache_metadata_set_needs_check(cache->cmd)) {\r\nDMERR("%s: failed to set 'needs_check' flag in metadata", dev_name);\r\nset_cache_mode(cache, CM_FAIL);\r\n}\r\nDMERR_LIMIT("%s: aborting current metadata transaction", dev_name);\r\nif (dm_cache_metadata_abort(cache->cmd)) {\r\nDMERR("%s: failed to abort metadata transaction", dev_name);\r\nset_cache_mode(cache, CM_FAIL);\r\n}\r\n}\r\nstatic void metadata_operation_failed(struct cache *cache, const char *op, int r)\r\n{\r\nDMERR_LIMIT("%s: metadata operation '%s' failed: error = %d",\r\ncache_device_name(cache), op, r);\r\nabort_transaction(cache);\r\nset_cache_mode(cache, CM_READ_ONLY);\r\n}\r\nstatic void inc_io_migrations(struct cache *cache)\r\n{\r\natomic_inc(&cache->nr_io_migrations);\r\n}\r\nstatic void dec_io_migrations(struct cache *cache)\r\n{\r\natomic_dec(&cache->nr_io_migrations);\r\n}\r\nstatic bool discard_or_flush(struct bio *bio)\r\n{\r\nreturn bio_op(bio) == REQ_OP_DISCARD ||\r\nbio->bi_opf & (REQ_PREFLUSH | REQ_FUA);\r\n}\r\nstatic void __cell_defer(struct cache *cache, struct dm_bio_prison_cell *cell)\r\n{\r\nif (discard_or_flush(cell->holder)) {\r\ndm_cell_release(cache->prison, cell, &cache->deferred_bios);\r\nfree_prison_cell(cache, cell);\r\n} else\r\nlist_add_tail(&cell->user_list, &cache->deferred_cells);\r\n}\r\nstatic void cell_defer(struct cache *cache, struct dm_bio_prison_cell *cell, bool holder)\r\n{\r\nunsigned long flags;\r\nif (!holder && dm_cell_promote_or_release(cache->prison, cell)) {\r\nfree_prison_cell(cache, cell);\r\nreturn;\r\n}\r\nspin_lock_irqsave(&cache->lock, flags);\r\n__cell_defer(cache, cell);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwake_worker(cache);\r\n}\r\nstatic void cell_error_with_code(struct cache *cache, struct dm_bio_prison_cell *cell, int err)\r\n{\r\ndm_cell_error(cache->prison, cell, err);\r\nfree_prison_cell(cache, cell);\r\n}\r\nstatic void cell_requeue(struct cache *cache, struct dm_bio_prison_cell *cell)\r\n{\r\ncell_error_with_code(cache, cell, DM_ENDIO_REQUEUE);\r\n}\r\nstatic void free_io_migration(struct dm_cache_migration *mg)\r\n{\r\nstruct cache *cache = mg->cache;\r\ndec_io_migrations(cache);\r\nfree_migration(mg);\r\nwake_worker(cache);\r\n}\r\nstatic void migration_failure(struct dm_cache_migration *mg)\r\n{\r\nstruct cache *cache = mg->cache;\r\nconst char *dev_name = cache_device_name(cache);\r\nif (mg->writeback) {\r\nDMERR_LIMIT("%s: writeback failed; couldn't copy block", dev_name);\r\nset_dirty(cache, mg->old_oblock, mg->cblock);\r\ncell_defer(cache, mg->old_ocell, false);\r\n} else if (mg->demote) {\r\nDMERR_LIMIT("%s: demotion failed; couldn't copy block", dev_name);\r\npolicy_force_mapping(cache->policy, mg->new_oblock, mg->old_oblock);\r\ncell_defer(cache, mg->old_ocell, mg->promote ? false : true);\r\nif (mg->promote)\r\ncell_defer(cache, mg->new_ocell, true);\r\n} else {\r\nDMERR_LIMIT("%s: promotion failed; couldn't copy block", dev_name);\r\npolicy_remove_mapping(cache->policy, mg->new_oblock);\r\ncell_defer(cache, mg->new_ocell, true);\r\n}\r\nfree_io_migration(mg);\r\n}\r\nstatic void migration_success_pre_commit(struct dm_cache_migration *mg)\r\n{\r\nint r;\r\nunsigned long flags;\r\nstruct cache *cache = mg->cache;\r\nif (mg->writeback) {\r\nclear_dirty(cache, mg->old_oblock, mg->cblock);\r\ncell_defer(cache, mg->old_ocell, false);\r\nfree_io_migration(mg);\r\nreturn;\r\n} else if (mg->demote) {\r\nr = dm_cache_remove_mapping(cache->cmd, mg->cblock);\r\nif (r) {\r\nDMERR_LIMIT("%s: demotion failed; couldn't update on disk metadata",\r\ncache_device_name(cache));\r\nmetadata_operation_failed(cache, "dm_cache_remove_mapping", r);\r\npolicy_force_mapping(cache->policy, mg->new_oblock,\r\nmg->old_oblock);\r\nif (mg->promote)\r\ncell_defer(cache, mg->new_ocell, true);\r\nfree_io_migration(mg);\r\nreturn;\r\n}\r\n} else {\r\nr = dm_cache_insert_mapping(cache->cmd, mg->cblock, mg->new_oblock);\r\nif (r) {\r\nDMERR_LIMIT("%s: promotion failed; couldn't update on disk metadata",\r\ncache_device_name(cache));\r\nmetadata_operation_failed(cache, "dm_cache_insert_mapping", r);\r\npolicy_remove_mapping(cache->policy, mg->new_oblock);\r\nfree_io_migration(mg);\r\nreturn;\r\n}\r\n}\r\nspin_lock_irqsave(&cache->lock, flags);\r\nlist_add_tail(&mg->list, &cache->need_commit_migrations);\r\ncache->commit_requested = true;\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\n}\r\nstatic void migration_success_post_commit(struct dm_cache_migration *mg)\r\n{\r\nunsigned long flags;\r\nstruct cache *cache = mg->cache;\r\nif (mg->writeback) {\r\nDMWARN_LIMIT("%s: writeback unexpectedly triggered commit",\r\ncache_device_name(cache));\r\nreturn;\r\n} else if (mg->demote) {\r\ncell_defer(cache, mg->old_ocell, mg->promote ? false : true);\r\nif (mg->promote) {\r\nmg->demote = false;\r\nspin_lock_irqsave(&cache->lock, flags);\r\nlist_add_tail(&mg->list, &cache->quiesced_migrations);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\n} else {\r\nif (mg->invalidate)\r\npolicy_remove_mapping(cache->policy, mg->old_oblock);\r\nfree_io_migration(mg);\r\n}\r\n} else {\r\nif (mg->requeue_holder) {\r\nclear_dirty(cache, mg->new_oblock, mg->cblock);\r\ncell_defer(cache, mg->new_ocell, true);\r\n} else {\r\nset_dirty(cache, mg->new_oblock, mg->cblock);\r\nbio_endio(mg->new_ocell->holder);\r\ncell_defer(cache, mg->new_ocell, false);\r\n}\r\nfree_io_migration(mg);\r\n}\r\n}\r\nstatic void copy_complete(int read_err, unsigned long write_err, void *context)\r\n{\r\nunsigned long flags;\r\nstruct dm_cache_migration *mg = (struct dm_cache_migration *) context;\r\nstruct cache *cache = mg->cache;\r\nif (read_err || write_err)\r\nmg->err = true;\r\nspin_lock_irqsave(&cache->lock, flags);\r\nlist_add_tail(&mg->list, &cache->completed_migrations);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwake_worker(cache);\r\n}\r\nstatic void issue_copy(struct dm_cache_migration *mg)\r\n{\r\nint r;\r\nstruct dm_io_region o_region, c_region;\r\nstruct cache *cache = mg->cache;\r\nsector_t cblock = from_cblock(mg->cblock);\r\no_region.bdev = cache->origin_dev->bdev;\r\no_region.count = cache->sectors_per_block;\r\nc_region.bdev = cache->cache_dev->bdev;\r\nc_region.sector = cblock * cache->sectors_per_block;\r\nc_region.count = cache->sectors_per_block;\r\nif (mg->writeback || mg->demote) {\r\no_region.sector = from_oblock(mg->old_oblock) * cache->sectors_per_block;\r\nr = dm_kcopyd_copy(cache->copier, &c_region, 1, &o_region, 0, copy_complete, mg);\r\n} else {\r\no_region.sector = from_oblock(mg->new_oblock) * cache->sectors_per_block;\r\nr = dm_kcopyd_copy(cache->copier, &o_region, 1, &c_region, 0, copy_complete, mg);\r\n}\r\nif (r < 0) {\r\nDMERR_LIMIT("%s: issuing migration failed", cache_device_name(cache));\r\nmigration_failure(mg);\r\n}\r\n}\r\nstatic void overwrite_endio(struct bio *bio)\r\n{\r\nstruct dm_cache_migration *mg = bio->bi_private;\r\nstruct cache *cache = mg->cache;\r\nsize_t pb_data_size = get_per_bio_data_size(cache);\r\nstruct per_bio_data *pb = get_per_bio_data(bio, pb_data_size);\r\nunsigned long flags;\r\ndm_unhook_bio(&pb->hook_info, bio);\r\nif (bio->bi_error)\r\nmg->err = true;\r\nmg->requeue_holder = false;\r\nspin_lock_irqsave(&cache->lock, flags);\r\nlist_add_tail(&mg->list, &cache->completed_migrations);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwake_worker(cache);\r\n}\r\nstatic void issue_overwrite(struct dm_cache_migration *mg, struct bio *bio)\r\n{\r\nsize_t pb_data_size = get_per_bio_data_size(mg->cache);\r\nstruct per_bio_data *pb = get_per_bio_data(bio, pb_data_size);\r\ndm_hook_bio(&pb->hook_info, bio, overwrite_endio, mg);\r\nremap_to_cache_dirty(mg->cache, bio, mg->new_oblock, mg->cblock);\r\naccounted_request(mg->cache, bio);\r\n}\r\nstatic bool bio_writes_complete_block(struct cache *cache, struct bio *bio)\r\n{\r\nreturn (bio_data_dir(bio) == WRITE) &&\r\n(bio->bi_iter.bi_size == (cache->sectors_per_block << SECTOR_SHIFT));\r\n}\r\nstatic void avoid_copy(struct dm_cache_migration *mg)\r\n{\r\natomic_inc(&mg->cache->stats.copies_avoided);\r\nmigration_success_pre_commit(mg);\r\n}\r\nstatic void calc_discard_block_range(struct cache *cache, struct bio *bio,\r\ndm_dblock_t *b, dm_dblock_t *e)\r\n{\r\nsector_t sb = bio->bi_iter.bi_sector;\r\nsector_t se = bio_end_sector(bio);\r\n*b = to_dblock(dm_sector_div_up(sb, cache->discard_block_size));\r\nif (se - sb < cache->discard_block_size)\r\n*e = *b;\r\nelse\r\n*e = to_dblock(block_div(se, cache->discard_block_size));\r\n}\r\nstatic void issue_discard(struct dm_cache_migration *mg)\r\n{\r\ndm_dblock_t b, e;\r\nstruct bio *bio = mg->new_ocell->holder;\r\nstruct cache *cache = mg->cache;\r\ncalc_discard_block_range(cache, bio, &b, &e);\r\nwhile (b != e) {\r\nset_discard(cache, b);\r\nb = to_dblock(from_dblock(b) + 1);\r\n}\r\nbio_endio(bio);\r\ncell_defer(cache, mg->new_ocell, false);\r\nfree_migration(mg);\r\nwake_worker(cache);\r\n}\r\nstatic void issue_copy_or_discard(struct dm_cache_migration *mg)\r\n{\r\nbool avoid;\r\nstruct cache *cache = mg->cache;\r\nif (mg->discard) {\r\nissue_discard(mg);\r\nreturn;\r\n}\r\nif (mg->writeback || mg->demote)\r\navoid = !is_dirty(cache, mg->cblock) ||\r\nis_discarded_oblock(cache, mg->old_oblock);\r\nelse {\r\nstruct bio *bio = mg->new_ocell->holder;\r\navoid = is_discarded_oblock(cache, mg->new_oblock);\r\nif (writeback_mode(&cache->features) &&\r\n!avoid && bio_writes_complete_block(cache, bio)) {\r\nissue_overwrite(mg, bio);\r\nreturn;\r\n}\r\n}\r\navoid ? avoid_copy(mg) : issue_copy(mg);\r\n}\r\nstatic void complete_migration(struct dm_cache_migration *mg)\r\n{\r\nif (mg->err)\r\nmigration_failure(mg);\r\nelse\r\nmigration_success_pre_commit(mg);\r\n}\r\nstatic void process_migrations(struct cache *cache, struct list_head *head,\r\nvoid (*fn)(struct dm_cache_migration *))\r\n{\r\nunsigned long flags;\r\nstruct list_head list;\r\nstruct dm_cache_migration *mg, *tmp;\r\nINIT_LIST_HEAD(&list);\r\nspin_lock_irqsave(&cache->lock, flags);\r\nlist_splice_init(head, &list);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nlist_for_each_entry_safe(mg, tmp, &list, list)\r\nfn(mg);\r\n}\r\nstatic void __queue_quiesced_migration(struct dm_cache_migration *mg)\r\n{\r\nlist_add_tail(&mg->list, &mg->cache->quiesced_migrations);\r\n}\r\nstatic void queue_quiesced_migration(struct dm_cache_migration *mg)\r\n{\r\nunsigned long flags;\r\nstruct cache *cache = mg->cache;\r\nspin_lock_irqsave(&cache->lock, flags);\r\n__queue_quiesced_migration(mg);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwake_worker(cache);\r\n}\r\nstatic void queue_quiesced_migrations(struct cache *cache, struct list_head *work)\r\n{\r\nunsigned long flags;\r\nstruct dm_cache_migration *mg, *tmp;\r\nspin_lock_irqsave(&cache->lock, flags);\r\nlist_for_each_entry_safe(mg, tmp, work, list)\r\n__queue_quiesced_migration(mg);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwake_worker(cache);\r\n}\r\nstatic void check_for_quiesced_migrations(struct cache *cache,\r\nstruct per_bio_data *pb)\r\n{\r\nstruct list_head work;\r\nif (!pb->all_io_entry)\r\nreturn;\r\nINIT_LIST_HEAD(&work);\r\ndm_deferred_entry_dec(pb->all_io_entry, &work);\r\nif (!list_empty(&work))\r\nqueue_quiesced_migrations(cache, &work);\r\n}\r\nstatic void quiesce_migration(struct dm_cache_migration *mg)\r\n{\r\nif (!dm_deferred_set_add_work(mg->cache->all_io_ds, &mg->list))\r\nqueue_quiesced_migration(mg);\r\n}\r\nstatic void promote(struct cache *cache, struct prealloc *structs,\r\ndm_oblock_t oblock, dm_cblock_t cblock,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nstruct dm_cache_migration *mg = prealloc_get_migration(structs);\r\nmg->err = false;\r\nmg->discard = false;\r\nmg->writeback = false;\r\nmg->demote = false;\r\nmg->promote = true;\r\nmg->requeue_holder = true;\r\nmg->invalidate = false;\r\nmg->cache = cache;\r\nmg->new_oblock = oblock;\r\nmg->cblock = cblock;\r\nmg->old_ocell = NULL;\r\nmg->new_ocell = cell;\r\nmg->start_jiffies = jiffies;\r\ninc_io_migrations(cache);\r\nquiesce_migration(mg);\r\n}\r\nstatic void writeback(struct cache *cache, struct prealloc *structs,\r\ndm_oblock_t oblock, dm_cblock_t cblock,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nstruct dm_cache_migration *mg = prealloc_get_migration(structs);\r\nmg->err = false;\r\nmg->discard = false;\r\nmg->writeback = true;\r\nmg->demote = false;\r\nmg->promote = false;\r\nmg->requeue_holder = true;\r\nmg->invalidate = false;\r\nmg->cache = cache;\r\nmg->old_oblock = oblock;\r\nmg->cblock = cblock;\r\nmg->old_ocell = cell;\r\nmg->new_ocell = NULL;\r\nmg->start_jiffies = jiffies;\r\ninc_io_migrations(cache);\r\nquiesce_migration(mg);\r\n}\r\nstatic void demote_then_promote(struct cache *cache, struct prealloc *structs,\r\ndm_oblock_t old_oblock, dm_oblock_t new_oblock,\r\ndm_cblock_t cblock,\r\nstruct dm_bio_prison_cell *old_ocell,\r\nstruct dm_bio_prison_cell *new_ocell)\r\n{\r\nstruct dm_cache_migration *mg = prealloc_get_migration(structs);\r\nmg->err = false;\r\nmg->discard = false;\r\nmg->writeback = false;\r\nmg->demote = true;\r\nmg->promote = true;\r\nmg->requeue_holder = true;\r\nmg->invalidate = false;\r\nmg->cache = cache;\r\nmg->old_oblock = old_oblock;\r\nmg->new_oblock = new_oblock;\r\nmg->cblock = cblock;\r\nmg->old_ocell = old_ocell;\r\nmg->new_ocell = new_ocell;\r\nmg->start_jiffies = jiffies;\r\ninc_io_migrations(cache);\r\nquiesce_migration(mg);\r\n}\r\nstatic void invalidate(struct cache *cache, struct prealloc *structs,\r\ndm_oblock_t oblock, dm_cblock_t cblock,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nstruct dm_cache_migration *mg = prealloc_get_migration(structs);\r\nmg->err = false;\r\nmg->discard = false;\r\nmg->writeback = false;\r\nmg->demote = true;\r\nmg->promote = false;\r\nmg->requeue_holder = true;\r\nmg->invalidate = true;\r\nmg->cache = cache;\r\nmg->old_oblock = oblock;\r\nmg->cblock = cblock;\r\nmg->old_ocell = cell;\r\nmg->new_ocell = NULL;\r\nmg->start_jiffies = jiffies;\r\ninc_io_migrations(cache);\r\nquiesce_migration(mg);\r\n}\r\nstatic void discard(struct cache *cache, struct prealloc *structs,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nstruct dm_cache_migration *mg = prealloc_get_migration(structs);\r\nmg->err = false;\r\nmg->discard = true;\r\nmg->writeback = false;\r\nmg->demote = false;\r\nmg->promote = false;\r\nmg->requeue_holder = false;\r\nmg->invalidate = false;\r\nmg->cache = cache;\r\nmg->old_ocell = NULL;\r\nmg->new_ocell = cell;\r\nmg->start_jiffies = jiffies;\r\nquiesce_migration(mg);\r\n}\r\nstatic void defer_bio(struct cache *cache, struct bio *bio)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&cache->lock, flags);\r\nbio_list_add(&cache->deferred_bios, bio);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwake_worker(cache);\r\n}\r\nstatic void process_flush_bio(struct cache *cache, struct bio *bio)\r\n{\r\nsize_t pb_data_size = get_per_bio_data_size(cache);\r\nstruct per_bio_data *pb = get_per_bio_data(bio, pb_data_size);\r\nBUG_ON(bio->bi_iter.bi_size);\r\nif (!pb->req_nr)\r\nremap_to_origin(cache, bio);\r\nelse\r\nremap_to_cache(cache, bio, 0);\r\nissue(cache, bio);\r\n}\r\nstatic void process_discard_bio(struct cache *cache, struct prealloc *structs,\r\nstruct bio *bio)\r\n{\r\nint r;\r\ndm_dblock_t b, e;\r\nstruct dm_bio_prison_cell *cell_prealloc, *new_ocell;\r\ncalc_discard_block_range(cache, bio, &b, &e);\r\nif (b == e) {\r\nbio_endio(bio);\r\nreturn;\r\n}\r\ncell_prealloc = prealloc_get_cell(structs);\r\nr = bio_detain_range(cache, dblock_to_oblock(cache, b), dblock_to_oblock(cache, e), bio, cell_prealloc,\r\n(cell_free_fn) prealloc_put_cell,\r\nstructs, &new_ocell);\r\nif (r > 0)\r\nreturn;\r\ndiscard(cache, structs, new_ocell);\r\n}\r\nstatic bool spare_migration_bandwidth(struct cache *cache)\r\n{\r\nsector_t current_volume = (atomic_read(&cache->nr_io_migrations) + 1) *\r\ncache->sectors_per_block;\r\nreturn current_volume < cache->migration_threshold;\r\n}\r\nstatic void inc_hit_counter(struct cache *cache, struct bio *bio)\r\n{\r\natomic_inc(bio_data_dir(bio) == READ ?\r\n&cache->stats.read_hit : &cache->stats.write_hit);\r\n}\r\nstatic void inc_miss_counter(struct cache *cache, struct bio *bio)\r\n{\r\natomic_inc(bio_data_dir(bio) == READ ?\r\n&cache->stats.read_miss : &cache->stats.write_miss);\r\n}\r\nstatic void inc_fn(void *context, struct dm_bio_prison_cell *cell)\r\n{\r\nstruct bio *bio;\r\nstruct inc_detail *detail = context;\r\nstruct cache *cache = detail->cache;\r\ninc_ds(cache, cell->holder, cell);\r\nif (bio_data_dir(cell->holder) == WRITE)\r\ndetail->any_writes = true;\r\nwhile ((bio = bio_list_pop(&cell->bios))) {\r\nif (discard_or_flush(bio)) {\r\nbio_list_add(&detail->unhandled_bios, bio);\r\ncontinue;\r\n}\r\nif (bio_data_dir(bio) == WRITE)\r\ndetail->any_writes = true;\r\nbio_list_add(&detail->bios_for_issue, bio);\r\ninc_ds(cache, bio, cell);\r\n}\r\n}\r\nstatic void remap_cell_to_origin_clear_discard(struct cache *cache,\r\nstruct dm_bio_prison_cell *cell,\r\ndm_oblock_t oblock, bool issue_holder)\r\n{\r\nstruct bio *bio;\r\nunsigned long flags;\r\nstruct inc_detail detail;\r\ndetail.cache = cache;\r\nbio_list_init(&detail.bios_for_issue);\r\nbio_list_init(&detail.unhandled_bios);\r\ndetail.any_writes = false;\r\nspin_lock_irqsave(&cache->lock, flags);\r\ndm_cell_visit_release(cache->prison, inc_fn, &detail, cell);\r\nbio_list_merge(&cache->deferred_bios, &detail.unhandled_bios);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nremap_to_origin(cache, cell->holder);\r\nif (issue_holder)\r\nissue(cache, cell->holder);\r\nelse\r\naccounted_begin(cache, cell->holder);\r\nif (detail.any_writes)\r\nclear_discard(cache, oblock_to_dblock(cache, oblock));\r\nwhile ((bio = bio_list_pop(&detail.bios_for_issue))) {\r\nremap_to_origin(cache, bio);\r\nissue(cache, bio);\r\n}\r\nfree_prison_cell(cache, cell);\r\n}\r\nstatic void remap_cell_to_cache_dirty(struct cache *cache, struct dm_bio_prison_cell *cell,\r\ndm_oblock_t oblock, dm_cblock_t cblock, bool issue_holder)\r\n{\r\nstruct bio *bio;\r\nunsigned long flags;\r\nstruct inc_detail detail;\r\ndetail.cache = cache;\r\nbio_list_init(&detail.bios_for_issue);\r\nbio_list_init(&detail.unhandled_bios);\r\ndetail.any_writes = false;\r\nspin_lock_irqsave(&cache->lock, flags);\r\ndm_cell_visit_release(cache->prison, inc_fn, &detail, cell);\r\nbio_list_merge(&cache->deferred_bios, &detail.unhandled_bios);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nremap_to_cache(cache, cell->holder, cblock);\r\nif (issue_holder)\r\nissue(cache, cell->holder);\r\nelse\r\naccounted_begin(cache, cell->holder);\r\nif (detail.any_writes) {\r\nset_dirty(cache, oblock, cblock);\r\nclear_discard(cache, oblock_to_dblock(cache, oblock));\r\n}\r\nwhile ((bio = bio_list_pop(&detail.bios_for_issue))) {\r\nremap_to_cache(cache, bio, cblock);\r\nissue(cache, bio);\r\n}\r\nfree_prison_cell(cache, cell);\r\n}\r\nstatic int null_locker(struct policy_locker *locker, dm_oblock_t b)\r\n{\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic int cell_locker(struct policy_locker *locker, dm_oblock_t b)\r\n{\r\nstruct old_oblock_lock *l = container_of(locker, struct old_oblock_lock, locker);\r\nstruct dm_bio_prison_cell *cell_prealloc = prealloc_get_cell(l->structs);\r\nreturn bio_detain(l->cache, b, NULL, cell_prealloc,\r\n(cell_free_fn) prealloc_put_cell,\r\nl->structs, &l->cell);\r\n}\r\nstatic void process_cell(struct cache *cache, struct prealloc *structs,\r\nstruct dm_bio_prison_cell *new_ocell)\r\n{\r\nint r;\r\nbool release_cell = true;\r\nstruct bio *bio = new_ocell->holder;\r\ndm_oblock_t block = get_bio_block(cache, bio);\r\nstruct policy_result lookup_result;\r\nbool passthrough = passthrough_mode(&cache->features);\r\nbool fast_promotion, can_migrate;\r\nstruct old_oblock_lock ool;\r\nfast_promotion = is_discarded_oblock(cache, block) || bio_writes_complete_block(cache, bio);\r\ncan_migrate = !passthrough && (fast_promotion || spare_migration_bandwidth(cache));\r\nool.locker.fn = cell_locker;\r\nool.cache = cache;\r\nool.structs = structs;\r\nool.cell = NULL;\r\nr = policy_map(cache->policy, block, true, can_migrate, fast_promotion,\r\nbio, &ool.locker, &lookup_result);\r\nif (r == -EWOULDBLOCK)\r\nlookup_result.op = POLICY_MISS;\r\nswitch (lookup_result.op) {\r\ncase POLICY_HIT:\r\nif (passthrough) {\r\ninc_miss_counter(cache, bio);\r\nif (bio_data_dir(bio) == WRITE) {\r\natomic_inc(&cache->stats.demotion);\r\ninvalidate(cache, structs, block, lookup_result.cblock, new_ocell);\r\nrelease_cell = false;\r\n} else {\r\nremap_to_origin_clear_discard(cache, bio, block);\r\ninc_and_issue(cache, bio, new_ocell);\r\n}\r\n} else {\r\ninc_hit_counter(cache, bio);\r\nif (bio_data_dir(bio) == WRITE &&\r\nwritethrough_mode(&cache->features) &&\r\n!is_dirty(cache, lookup_result.cblock)) {\r\nremap_to_origin_then_cache(cache, bio, block, lookup_result.cblock);\r\ninc_and_issue(cache, bio, new_ocell);\r\n} else {\r\nremap_cell_to_cache_dirty(cache, new_ocell, block, lookup_result.cblock, true);\r\nrelease_cell = false;\r\n}\r\n}\r\nbreak;\r\ncase POLICY_MISS:\r\ninc_miss_counter(cache, bio);\r\nremap_cell_to_origin_clear_discard(cache, new_ocell, block, true);\r\nrelease_cell = false;\r\nbreak;\r\ncase POLICY_NEW:\r\natomic_inc(&cache->stats.promotion);\r\npromote(cache, structs, block, lookup_result.cblock, new_ocell);\r\nrelease_cell = false;\r\nbreak;\r\ncase POLICY_REPLACE:\r\natomic_inc(&cache->stats.demotion);\r\natomic_inc(&cache->stats.promotion);\r\ndemote_then_promote(cache, structs, lookup_result.old_oblock,\r\nblock, lookup_result.cblock,\r\nool.cell, new_ocell);\r\nrelease_cell = false;\r\nbreak;\r\ndefault:\r\nDMERR_LIMIT("%s: %s: erroring bio, unknown policy op: %u",\r\ncache_device_name(cache), __func__,\r\n(unsigned) lookup_result.op);\r\nbio_io_error(bio);\r\n}\r\nif (release_cell)\r\ncell_defer(cache, new_ocell, false);\r\n}\r\nstatic void process_bio(struct cache *cache, struct prealloc *structs,\r\nstruct bio *bio)\r\n{\r\nint r;\r\ndm_oblock_t block = get_bio_block(cache, bio);\r\nstruct dm_bio_prison_cell *cell_prealloc, *new_ocell;\r\ncell_prealloc = prealloc_get_cell(structs);\r\nr = bio_detain(cache, block, bio, cell_prealloc,\r\n(cell_free_fn) prealloc_put_cell,\r\nstructs, &new_ocell);\r\nif (r > 0)\r\nreturn;\r\nprocess_cell(cache, structs, new_ocell);\r\n}\r\nstatic int need_commit_due_to_time(struct cache *cache)\r\n{\r\nreturn jiffies < cache->last_commit_jiffies ||\r\njiffies > cache->last_commit_jiffies + COMMIT_PERIOD;\r\n}\r\nstatic int commit(struct cache *cache, bool clean_shutdown)\r\n{\r\nint r;\r\nif (get_cache_mode(cache) >= CM_READ_ONLY)\r\nreturn -EINVAL;\r\natomic_inc(&cache->stats.commit_count);\r\nr = dm_cache_commit(cache->cmd, clean_shutdown);\r\nif (r)\r\nmetadata_operation_failed(cache, "dm_cache_commit", r);\r\nreturn r;\r\n}\r\nstatic int commit_if_needed(struct cache *cache)\r\n{\r\nint r = 0;\r\nif ((cache->commit_requested || need_commit_due_to_time(cache)) &&\r\ndm_cache_changed_this_transaction(cache->cmd)) {\r\nr = commit(cache, false);\r\ncache->commit_requested = false;\r\ncache->last_commit_jiffies = jiffies;\r\n}\r\nreturn r;\r\n}\r\nstatic void process_deferred_bios(struct cache *cache)\r\n{\r\nbool prealloc_used = false;\r\nunsigned long flags;\r\nstruct bio_list bios;\r\nstruct bio *bio;\r\nstruct prealloc structs;\r\nmemset(&structs, 0, sizeof(structs));\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&cache->lock, flags);\r\nbio_list_merge(&bios, &cache->deferred_bios);\r\nbio_list_init(&cache->deferred_bios);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwhile (!bio_list_empty(&bios)) {\r\nprealloc_used = true;\r\nif (prealloc_data_structs(cache, &structs)) {\r\nspin_lock_irqsave(&cache->lock, flags);\r\nbio_list_merge(&cache->deferred_bios, &bios);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nbreak;\r\n}\r\nbio = bio_list_pop(&bios);\r\nif (bio->bi_opf & REQ_PREFLUSH)\r\nprocess_flush_bio(cache, bio);\r\nelse if (bio_op(bio) == REQ_OP_DISCARD)\r\nprocess_discard_bio(cache, &structs, bio);\r\nelse\r\nprocess_bio(cache, &structs, bio);\r\n}\r\nif (prealloc_used)\r\nprealloc_free_structs(cache, &structs);\r\n}\r\nstatic void process_deferred_cells(struct cache *cache)\r\n{\r\nbool prealloc_used = false;\r\nunsigned long flags;\r\nstruct dm_bio_prison_cell *cell, *tmp;\r\nstruct list_head cells;\r\nstruct prealloc structs;\r\nmemset(&structs, 0, sizeof(structs));\r\nINIT_LIST_HEAD(&cells);\r\nspin_lock_irqsave(&cache->lock, flags);\r\nlist_splice_init(&cache->deferred_cells, &cells);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nlist_for_each_entry_safe(cell, tmp, &cells, user_list) {\r\nprealloc_used = true;\r\nif (prealloc_data_structs(cache, &structs)) {\r\nspin_lock_irqsave(&cache->lock, flags);\r\nlist_splice(&cells, &cache->deferred_cells);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nbreak;\r\n}\r\nprocess_cell(cache, &structs, cell);\r\n}\r\nif (prealloc_used)\r\nprealloc_free_structs(cache, &structs);\r\n}\r\nstatic void process_deferred_flush_bios(struct cache *cache, bool submit_bios)\r\n{\r\nunsigned long flags;\r\nstruct bio_list bios;\r\nstruct bio *bio;\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&cache->lock, flags);\r\nbio_list_merge(&bios, &cache->deferred_flush_bios);\r\nbio_list_init(&cache->deferred_flush_bios);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwhile ((bio = bio_list_pop(&bios)))\r\nsubmit_bios ? accounted_request(cache, bio) : bio_io_error(bio);\r\n}\r\nstatic void process_deferred_writethrough_bios(struct cache *cache)\r\n{\r\nunsigned long flags;\r\nstruct bio_list bios;\r\nstruct bio *bio;\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&cache->lock, flags);\r\nbio_list_merge(&bios, &cache->deferred_writethrough_bios);\r\nbio_list_init(&cache->deferred_writethrough_bios);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nwhile ((bio = bio_list_pop(&bios)))\r\naccounted_request(cache, bio);\r\n}\r\nstatic void writeback_some_dirty_blocks(struct cache *cache)\r\n{\r\nbool prealloc_used = false;\r\ndm_oblock_t oblock;\r\ndm_cblock_t cblock;\r\nstruct prealloc structs;\r\nstruct dm_bio_prison_cell *old_ocell;\r\nbool busy = !iot_idle_for(&cache->origin_tracker, HZ);\r\nmemset(&structs, 0, sizeof(structs));\r\nwhile (spare_migration_bandwidth(cache)) {\r\nif (policy_writeback_work(cache->policy, &oblock, &cblock, busy))\r\nbreak;\r\nprealloc_used = true;\r\nif (prealloc_data_structs(cache, &structs) ||\r\nget_cell(cache, oblock, &structs, &old_ocell)) {\r\npolicy_set_dirty(cache->policy, oblock);\r\nbreak;\r\n}\r\nwriteback(cache, &structs, oblock, cblock, old_ocell);\r\n}\r\nif (prealloc_used)\r\nprealloc_free_structs(cache, &structs);\r\n}\r\nstatic void process_invalidation_request(struct cache *cache, struct invalidation_request *req)\r\n{\r\nint r = 0;\r\nuint64_t begin = from_cblock(req->cblocks->begin);\r\nuint64_t end = from_cblock(req->cblocks->end);\r\nwhile (begin != end) {\r\nr = policy_remove_cblock(cache->policy, to_cblock(begin));\r\nif (!r) {\r\nr = dm_cache_remove_mapping(cache->cmd, to_cblock(begin));\r\nif (r) {\r\nmetadata_operation_failed(cache, "dm_cache_remove_mapping", r);\r\nbreak;\r\n}\r\n} else if (r == -ENODATA) {\r\nr = 0;\r\n} else {\r\nDMERR("%s: policy_remove_cblock failed", cache_device_name(cache));\r\nbreak;\r\n}\r\nbegin++;\r\n}\r\ncache->commit_requested = true;\r\nreq->err = r;\r\natomic_set(&req->complete, 1);\r\nwake_up(&req->result_wait);\r\n}\r\nstatic void process_invalidation_requests(struct cache *cache)\r\n{\r\nstruct list_head list;\r\nstruct invalidation_request *req, *tmp;\r\nINIT_LIST_HEAD(&list);\r\nspin_lock(&cache->invalidation_lock);\r\nlist_splice_init(&cache->invalidation_requests, &list);\r\nspin_unlock(&cache->invalidation_lock);\r\nlist_for_each_entry_safe (req, tmp, &list, list)\r\nprocess_invalidation_request(cache, req);\r\n}\r\nstatic bool is_quiescing(struct cache *cache)\r\n{\r\nreturn atomic_read(&cache->quiescing);\r\n}\r\nstatic void ack_quiescing(struct cache *cache)\r\n{\r\nif (is_quiescing(cache)) {\r\natomic_inc(&cache->quiescing_ack);\r\nwake_up(&cache->quiescing_wait);\r\n}\r\n}\r\nstatic void wait_for_quiescing_ack(struct cache *cache)\r\n{\r\nwait_event(cache->quiescing_wait, atomic_read(&cache->quiescing_ack));\r\n}\r\nstatic void start_quiescing(struct cache *cache)\r\n{\r\natomic_inc(&cache->quiescing);\r\nwait_for_quiescing_ack(cache);\r\n}\r\nstatic void stop_quiescing(struct cache *cache)\r\n{\r\natomic_set(&cache->quiescing, 0);\r\natomic_set(&cache->quiescing_ack, 0);\r\n}\r\nstatic void wait_for_migrations(struct cache *cache)\r\n{\r\nwait_event(cache->migration_wait, !atomic_read(&cache->nr_allocated_migrations));\r\n}\r\nstatic void stop_worker(struct cache *cache)\r\n{\r\ncancel_delayed_work(&cache->waker);\r\nflush_workqueue(cache->wq);\r\n}\r\nstatic void requeue_deferred_cells(struct cache *cache)\r\n{\r\nunsigned long flags;\r\nstruct list_head cells;\r\nstruct dm_bio_prison_cell *cell, *tmp;\r\nINIT_LIST_HEAD(&cells);\r\nspin_lock_irqsave(&cache->lock, flags);\r\nlist_splice_init(&cache->deferred_cells, &cells);\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\nlist_for_each_entry_safe(cell, tmp, &cells, user_list)\r\ncell_requeue(cache, cell);\r\n}\r\nstatic void requeue_deferred_bios(struct cache *cache)\r\n{\r\nstruct bio *bio;\r\nstruct bio_list bios;\r\nbio_list_init(&bios);\r\nbio_list_merge(&bios, &cache->deferred_bios);\r\nbio_list_init(&cache->deferred_bios);\r\nwhile ((bio = bio_list_pop(&bios))) {\r\nbio->bi_error = DM_ENDIO_REQUEUE;\r\nbio_endio(bio);\r\n}\r\n}\r\nstatic int more_work(struct cache *cache)\r\n{\r\nif (is_quiescing(cache))\r\nreturn !list_empty(&cache->quiesced_migrations) ||\r\n!list_empty(&cache->completed_migrations) ||\r\n!list_empty(&cache->need_commit_migrations);\r\nelse\r\nreturn !bio_list_empty(&cache->deferred_bios) ||\r\n!list_empty(&cache->deferred_cells) ||\r\n!bio_list_empty(&cache->deferred_flush_bios) ||\r\n!bio_list_empty(&cache->deferred_writethrough_bios) ||\r\n!list_empty(&cache->quiesced_migrations) ||\r\n!list_empty(&cache->completed_migrations) ||\r\n!list_empty(&cache->need_commit_migrations) ||\r\ncache->invalidate;\r\n}\r\nstatic void do_worker(struct work_struct *ws)\r\n{\r\nstruct cache *cache = container_of(ws, struct cache, worker);\r\ndo {\r\nif (!is_quiescing(cache)) {\r\nwriteback_some_dirty_blocks(cache);\r\nprocess_deferred_writethrough_bios(cache);\r\nprocess_deferred_bios(cache);\r\nprocess_deferred_cells(cache);\r\nprocess_invalidation_requests(cache);\r\n}\r\nprocess_migrations(cache, &cache->quiesced_migrations, issue_copy_or_discard);\r\nprocess_migrations(cache, &cache->completed_migrations, complete_migration);\r\nif (commit_if_needed(cache)) {\r\nprocess_deferred_flush_bios(cache, false);\r\nprocess_migrations(cache, &cache->need_commit_migrations, migration_failure);\r\n} else {\r\nprocess_deferred_flush_bios(cache, true);\r\nprocess_migrations(cache, &cache->need_commit_migrations,\r\nmigration_success_post_commit);\r\n}\r\nack_quiescing(cache);\r\n} while (more_work(cache));\r\n}\r\nstatic void do_waker(struct work_struct *ws)\r\n{\r\nstruct cache *cache = container_of(to_delayed_work(ws), struct cache, waker);\r\npolicy_tick(cache->policy, true);\r\nwake_worker(cache);\r\nqueue_delayed_work(cache->wq, &cache->waker, COMMIT_PERIOD);\r\n}\r\nstatic int is_congested(struct dm_dev *dev, int bdi_bits)\r\n{\r\nstruct request_queue *q = bdev_get_queue(dev->bdev);\r\nreturn bdi_congested(&q->backing_dev_info, bdi_bits);\r\n}\r\nstatic int cache_is_congested(struct dm_target_callbacks *cb, int bdi_bits)\r\n{\r\nstruct cache *cache = container_of(cb, struct cache, callbacks);\r\nreturn is_congested(cache->origin_dev, bdi_bits) ||\r\nis_congested(cache->cache_dev, bdi_bits);\r\n}\r\nstatic void destroy(struct cache *cache)\r\n{\r\nunsigned i;\r\nmempool_destroy(cache->migration_pool);\r\nif (cache->all_io_ds)\r\ndm_deferred_set_destroy(cache->all_io_ds);\r\nif (cache->prison)\r\ndm_bio_prison_destroy(cache->prison);\r\nif (cache->wq)\r\ndestroy_workqueue(cache->wq);\r\nif (cache->dirty_bitset)\r\nfree_bitset(cache->dirty_bitset);\r\nif (cache->discard_bitset)\r\nfree_bitset(cache->discard_bitset);\r\nif (cache->copier)\r\ndm_kcopyd_client_destroy(cache->copier);\r\nif (cache->cmd)\r\ndm_cache_metadata_close(cache->cmd);\r\nif (cache->metadata_dev)\r\ndm_put_device(cache->ti, cache->metadata_dev);\r\nif (cache->origin_dev)\r\ndm_put_device(cache->ti, cache->origin_dev);\r\nif (cache->cache_dev)\r\ndm_put_device(cache->ti, cache->cache_dev);\r\nif (cache->policy)\r\ndm_cache_policy_destroy(cache->policy);\r\nfor (i = 0; i < cache->nr_ctr_args ; i++)\r\nkfree(cache->ctr_args[i]);\r\nkfree(cache->ctr_args);\r\nkfree(cache);\r\n}\r\nstatic void cache_dtr(struct dm_target *ti)\r\n{\r\nstruct cache *cache = ti->private;\r\ndestroy(cache);\r\n}\r\nstatic sector_t get_dev_size(struct dm_dev *dev)\r\n{\r\nreturn i_size_read(dev->bdev->bd_inode) >> SECTOR_SHIFT;\r\n}\r\nstatic void destroy_cache_args(struct cache_args *ca)\r\n{\r\nif (ca->metadata_dev)\r\ndm_put_device(ca->ti, ca->metadata_dev);\r\nif (ca->cache_dev)\r\ndm_put_device(ca->ti, ca->cache_dev);\r\nif (ca->origin_dev)\r\ndm_put_device(ca->ti, ca->origin_dev);\r\nkfree(ca);\r\n}\r\nstatic bool at_least_one_arg(struct dm_arg_set *as, char **error)\r\n{\r\nif (!as->argc) {\r\n*error = "Insufficient args";\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int parse_metadata_dev(struct cache_args *ca, struct dm_arg_set *as,\r\nchar **error)\r\n{\r\nint r;\r\nsector_t metadata_dev_size;\r\nchar b[BDEVNAME_SIZE];\r\nif (!at_least_one_arg(as, error))\r\nreturn -EINVAL;\r\nr = dm_get_device(ca->ti, dm_shift_arg(as), FMODE_READ | FMODE_WRITE,\r\n&ca->metadata_dev);\r\nif (r) {\r\n*error = "Error opening metadata device";\r\nreturn r;\r\n}\r\nmetadata_dev_size = get_dev_size(ca->metadata_dev);\r\nif (metadata_dev_size > DM_CACHE_METADATA_MAX_SECTORS_WARNING)\r\nDMWARN("Metadata device %s is larger than %u sectors: excess space will not be used.",\r\nbdevname(ca->metadata_dev->bdev, b), THIN_METADATA_MAX_SECTORS);\r\nreturn 0;\r\n}\r\nstatic int parse_cache_dev(struct cache_args *ca, struct dm_arg_set *as,\r\nchar **error)\r\n{\r\nint r;\r\nif (!at_least_one_arg(as, error))\r\nreturn -EINVAL;\r\nr = dm_get_device(ca->ti, dm_shift_arg(as), FMODE_READ | FMODE_WRITE,\r\n&ca->cache_dev);\r\nif (r) {\r\n*error = "Error opening cache device";\r\nreturn r;\r\n}\r\nca->cache_sectors = get_dev_size(ca->cache_dev);\r\nreturn 0;\r\n}\r\nstatic int parse_origin_dev(struct cache_args *ca, struct dm_arg_set *as,\r\nchar **error)\r\n{\r\nint r;\r\nif (!at_least_one_arg(as, error))\r\nreturn -EINVAL;\r\nr = dm_get_device(ca->ti, dm_shift_arg(as), FMODE_READ | FMODE_WRITE,\r\n&ca->origin_dev);\r\nif (r) {\r\n*error = "Error opening origin device";\r\nreturn r;\r\n}\r\nca->origin_sectors = get_dev_size(ca->origin_dev);\r\nif (ca->ti->len > ca->origin_sectors) {\r\n*error = "Device size larger than cached device";\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int parse_block_size(struct cache_args *ca, struct dm_arg_set *as,\r\nchar **error)\r\n{\r\nunsigned long block_size;\r\nif (!at_least_one_arg(as, error))\r\nreturn -EINVAL;\r\nif (kstrtoul(dm_shift_arg(as), 10, &block_size) || !block_size ||\r\nblock_size < DATA_DEV_BLOCK_SIZE_MIN_SECTORS ||\r\nblock_size > DATA_DEV_BLOCK_SIZE_MAX_SECTORS ||\r\nblock_size & (DATA_DEV_BLOCK_SIZE_MIN_SECTORS - 1)) {\r\n*error = "Invalid data block size";\r\nreturn -EINVAL;\r\n}\r\nif (block_size > ca->cache_sectors) {\r\n*error = "Data block size is larger than the cache device";\r\nreturn -EINVAL;\r\n}\r\nca->block_size = block_size;\r\nreturn 0;\r\n}\r\nstatic void init_features(struct cache_features *cf)\r\n{\r\ncf->mode = CM_WRITE;\r\ncf->io_mode = CM_IO_WRITEBACK;\r\n}\r\nstatic int parse_features(struct cache_args *ca, struct dm_arg_set *as,\r\nchar **error)\r\n{\r\nstatic struct dm_arg _args[] = {\r\n{0, 1, "Invalid number of cache feature arguments"},\r\n};\r\nint r;\r\nunsigned argc;\r\nconst char *arg;\r\nstruct cache_features *cf = &ca->features;\r\ninit_features(cf);\r\nr = dm_read_arg_group(_args, as, &argc, error);\r\nif (r)\r\nreturn -EINVAL;\r\nwhile (argc--) {\r\narg = dm_shift_arg(as);\r\nif (!strcasecmp(arg, "writeback"))\r\ncf->io_mode = CM_IO_WRITEBACK;\r\nelse if (!strcasecmp(arg, "writethrough"))\r\ncf->io_mode = CM_IO_WRITETHROUGH;\r\nelse if (!strcasecmp(arg, "passthrough"))\r\ncf->io_mode = CM_IO_PASSTHROUGH;\r\nelse {\r\n*error = "Unrecognised cache feature requested";\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int parse_policy(struct cache_args *ca, struct dm_arg_set *as,\r\nchar **error)\r\n{\r\nstatic struct dm_arg _args[] = {\r\n{0, 1024, "Invalid number of policy arguments"},\r\n};\r\nint r;\r\nif (!at_least_one_arg(as, error))\r\nreturn -EINVAL;\r\nca->policy_name = dm_shift_arg(as);\r\nr = dm_read_arg_group(_args, as, &ca->policy_argc, error);\r\nif (r)\r\nreturn -EINVAL;\r\nca->policy_argv = (const char **)as->argv;\r\ndm_consume_args(as, ca->policy_argc);\r\nreturn 0;\r\n}\r\nstatic int parse_cache_args(struct cache_args *ca, int argc, char **argv,\r\nchar **error)\r\n{\r\nint r;\r\nstruct dm_arg_set as;\r\nas.argc = argc;\r\nas.argv = argv;\r\nr = parse_metadata_dev(ca, &as, error);\r\nif (r)\r\nreturn r;\r\nr = parse_cache_dev(ca, &as, error);\r\nif (r)\r\nreturn r;\r\nr = parse_origin_dev(ca, &as, error);\r\nif (r)\r\nreturn r;\r\nr = parse_block_size(ca, &as, error);\r\nif (r)\r\nreturn r;\r\nr = parse_features(ca, &as, error);\r\nif (r)\r\nreturn r;\r\nr = parse_policy(ca, &as, error);\r\nif (r)\r\nreturn r;\r\nreturn 0;\r\n}\r\nstatic int process_config_option(struct cache *cache, const char *key, const char *value)\r\n{\r\nunsigned long tmp;\r\nif (!strcasecmp(key, "migration_threshold")) {\r\nif (kstrtoul(value, 10, &tmp))\r\nreturn -EINVAL;\r\ncache->migration_threshold = tmp;\r\nreturn 0;\r\n}\r\nreturn NOT_CORE_OPTION;\r\n}\r\nstatic int set_config_value(struct cache *cache, const char *key, const char *value)\r\n{\r\nint r = process_config_option(cache, key, value);\r\nif (r == NOT_CORE_OPTION)\r\nr = policy_set_config_value(cache->policy, key, value);\r\nif (r)\r\nDMWARN("bad config value for %s: %s", key, value);\r\nreturn r;\r\n}\r\nstatic int set_config_values(struct cache *cache, int argc, const char **argv)\r\n{\r\nint r = 0;\r\nif (argc & 1) {\r\nDMWARN("Odd number of policy arguments given but they should be <key> <value> pairs.");\r\nreturn -EINVAL;\r\n}\r\nwhile (argc) {\r\nr = set_config_value(cache, argv[0], argv[1]);\r\nif (r)\r\nbreak;\r\nargc -= 2;\r\nargv += 2;\r\n}\r\nreturn r;\r\n}\r\nstatic int create_cache_policy(struct cache *cache, struct cache_args *ca,\r\nchar **error)\r\n{\r\nstruct dm_cache_policy *p = dm_cache_policy_create(ca->policy_name,\r\ncache->cache_size,\r\ncache->origin_sectors,\r\ncache->sectors_per_block);\r\nif (IS_ERR(p)) {\r\n*error = "Error creating cache's policy";\r\nreturn PTR_ERR(p);\r\n}\r\ncache->policy = p;\r\nreturn 0;\r\n}\r\nstatic bool too_many_discard_blocks(sector_t discard_block_size,\r\nsector_t origin_size)\r\n{\r\n(void) sector_div(origin_size, discard_block_size);\r\nreturn origin_size > MAX_DISCARD_BLOCKS;\r\n}\r\nstatic sector_t calculate_discard_block_size(sector_t cache_block_size,\r\nsector_t origin_size)\r\n{\r\nsector_t discard_block_size = cache_block_size;\r\nif (origin_size)\r\nwhile (too_many_discard_blocks(discard_block_size, origin_size))\r\ndiscard_block_size *= 2;\r\nreturn discard_block_size;\r\n}\r\nstatic void set_cache_size(struct cache *cache, dm_cblock_t size)\r\n{\r\ndm_block_t nr_blocks = from_cblock(size);\r\nif (nr_blocks > (1 << 20) && cache->cache_size != size)\r\nDMWARN_LIMIT("You have created a cache device with a lot of individual cache blocks (%llu)\n"\r\n"All these mappings can consume a lot of kernel memory, and take some time to read/write.\n"\r\n"Please consider increasing the cache block size to reduce the overall cache block count.",\r\n(unsigned long long) nr_blocks);\r\ncache->cache_size = size;\r\n}\r\nstatic int cache_create(struct cache_args *ca, struct cache **result)\r\n{\r\nint r = 0;\r\nchar **error = &ca->ti->error;\r\nstruct cache *cache;\r\nstruct dm_target *ti = ca->ti;\r\ndm_block_t origin_blocks;\r\nstruct dm_cache_metadata *cmd;\r\nbool may_format = ca->features.mode == CM_WRITE;\r\ncache = kzalloc(sizeof(*cache), GFP_KERNEL);\r\nif (!cache)\r\nreturn -ENOMEM;\r\ncache->ti = ca->ti;\r\nti->private = cache;\r\nti->num_flush_bios = 2;\r\nti->flush_supported = true;\r\nti->num_discard_bios = 1;\r\nti->discards_supported = true;\r\nti->discard_zeroes_data_unsupported = true;\r\nti->split_discard_bios = false;\r\ncache->features = ca->features;\r\nti->per_io_data_size = get_per_bio_data_size(cache);\r\ncache->callbacks.congested_fn = cache_is_congested;\r\ndm_table_add_target_callbacks(ti->table, &cache->callbacks);\r\ncache->metadata_dev = ca->metadata_dev;\r\ncache->origin_dev = ca->origin_dev;\r\ncache->cache_dev = ca->cache_dev;\r\nca->metadata_dev = ca->origin_dev = ca->cache_dev = NULL;\r\norigin_blocks = cache->origin_sectors = ca->origin_sectors;\r\norigin_blocks = block_div(origin_blocks, ca->block_size);\r\ncache->origin_blocks = to_oblock(origin_blocks);\r\ncache->sectors_per_block = ca->block_size;\r\nif (dm_set_target_max_io_len(ti, cache->sectors_per_block)) {\r\nr = -EINVAL;\r\ngoto bad;\r\n}\r\nif (ca->block_size & (ca->block_size - 1)) {\r\ndm_block_t cache_size = ca->cache_sectors;\r\ncache->sectors_per_block_shift = -1;\r\ncache_size = block_div(cache_size, ca->block_size);\r\nset_cache_size(cache, to_cblock(cache_size));\r\n} else {\r\ncache->sectors_per_block_shift = __ffs(ca->block_size);\r\nset_cache_size(cache, to_cblock(ca->cache_sectors >> cache->sectors_per_block_shift));\r\n}\r\nr = create_cache_policy(cache, ca, error);\r\nif (r)\r\ngoto bad;\r\ncache->policy_nr_args = ca->policy_argc;\r\ncache->migration_threshold = DEFAULT_MIGRATION_THRESHOLD;\r\nr = set_config_values(cache, ca->policy_argc, ca->policy_argv);\r\nif (r) {\r\n*error = "Error setting cache policy's config values";\r\ngoto bad;\r\n}\r\ncmd = dm_cache_metadata_open(cache->metadata_dev->bdev,\r\nca->block_size, may_format,\r\ndm_cache_policy_get_hint_size(cache->policy));\r\nif (IS_ERR(cmd)) {\r\n*error = "Error creating metadata object";\r\nr = PTR_ERR(cmd);\r\ngoto bad;\r\n}\r\ncache->cmd = cmd;\r\nset_cache_mode(cache, CM_WRITE);\r\nif (get_cache_mode(cache) != CM_WRITE) {\r\n*error = "Unable to get write access to metadata, please check/repair metadata.";\r\nr = -EINVAL;\r\ngoto bad;\r\n}\r\nif (passthrough_mode(&cache->features)) {\r\nbool all_clean;\r\nr = dm_cache_metadata_all_clean(cache->cmd, &all_clean);\r\nif (r) {\r\n*error = "dm_cache_metadata_all_clean() failed";\r\ngoto bad;\r\n}\r\nif (!all_clean) {\r\n*error = "Cannot enter passthrough mode unless all blocks are clean";\r\nr = -EINVAL;\r\ngoto bad;\r\n}\r\n}\r\nspin_lock_init(&cache->lock);\r\nINIT_LIST_HEAD(&cache->deferred_cells);\r\nbio_list_init(&cache->deferred_bios);\r\nbio_list_init(&cache->deferred_flush_bios);\r\nbio_list_init(&cache->deferred_writethrough_bios);\r\nINIT_LIST_HEAD(&cache->quiesced_migrations);\r\nINIT_LIST_HEAD(&cache->completed_migrations);\r\nINIT_LIST_HEAD(&cache->need_commit_migrations);\r\natomic_set(&cache->nr_allocated_migrations, 0);\r\natomic_set(&cache->nr_io_migrations, 0);\r\ninit_waitqueue_head(&cache->migration_wait);\r\ninit_waitqueue_head(&cache->quiescing_wait);\r\natomic_set(&cache->quiescing, 0);\r\natomic_set(&cache->quiescing_ack, 0);\r\nr = -ENOMEM;\r\natomic_set(&cache->nr_dirty, 0);\r\ncache->dirty_bitset = alloc_bitset(from_cblock(cache->cache_size));\r\nif (!cache->dirty_bitset) {\r\n*error = "could not allocate dirty bitset";\r\ngoto bad;\r\n}\r\nclear_bitset(cache->dirty_bitset, from_cblock(cache->cache_size));\r\ncache->discard_block_size =\r\ncalculate_discard_block_size(cache->sectors_per_block,\r\ncache->origin_sectors);\r\ncache->discard_nr_blocks = to_dblock(dm_sector_div_up(cache->origin_sectors,\r\ncache->discard_block_size));\r\ncache->discard_bitset = alloc_bitset(from_dblock(cache->discard_nr_blocks));\r\nif (!cache->discard_bitset) {\r\n*error = "could not allocate discard bitset";\r\ngoto bad;\r\n}\r\nclear_bitset(cache->discard_bitset, from_dblock(cache->discard_nr_blocks));\r\ncache->copier = dm_kcopyd_client_create(&dm_kcopyd_throttle);\r\nif (IS_ERR(cache->copier)) {\r\n*error = "could not create kcopyd client";\r\nr = PTR_ERR(cache->copier);\r\ngoto bad;\r\n}\r\ncache->wq = alloc_ordered_workqueue("dm-" DM_MSG_PREFIX, WQ_MEM_RECLAIM);\r\nif (!cache->wq) {\r\n*error = "could not create workqueue for metadata object";\r\ngoto bad;\r\n}\r\nINIT_WORK(&cache->worker, do_worker);\r\nINIT_DELAYED_WORK(&cache->waker, do_waker);\r\ncache->last_commit_jiffies = jiffies;\r\ncache->prison = dm_bio_prison_create();\r\nif (!cache->prison) {\r\n*error = "could not create bio prison";\r\ngoto bad;\r\n}\r\ncache->all_io_ds = dm_deferred_set_create();\r\nif (!cache->all_io_ds) {\r\n*error = "could not create all_io deferred set";\r\ngoto bad;\r\n}\r\ncache->migration_pool = mempool_create_slab_pool(MIGRATION_POOL_SIZE,\r\nmigration_cache);\r\nif (!cache->migration_pool) {\r\n*error = "Error creating cache's migration mempool";\r\ngoto bad;\r\n}\r\ncache->need_tick_bio = true;\r\ncache->sized = false;\r\ncache->invalidate = false;\r\ncache->commit_requested = false;\r\ncache->loaded_mappings = false;\r\ncache->loaded_discards = false;\r\nload_stats(cache);\r\natomic_set(&cache->stats.demotion, 0);\r\natomic_set(&cache->stats.promotion, 0);\r\natomic_set(&cache->stats.copies_avoided, 0);\r\natomic_set(&cache->stats.cache_cell_clash, 0);\r\natomic_set(&cache->stats.commit_count, 0);\r\natomic_set(&cache->stats.discard_count, 0);\r\nspin_lock_init(&cache->invalidation_lock);\r\nINIT_LIST_HEAD(&cache->invalidation_requests);\r\niot_init(&cache->origin_tracker);\r\n*result = cache;\r\nreturn 0;\r\nbad:\r\ndestroy(cache);\r\nreturn r;\r\n}\r\nstatic int copy_ctr_args(struct cache *cache, int argc, const char **argv)\r\n{\r\nunsigned i;\r\nconst char **copy;\r\ncopy = kcalloc(argc, sizeof(*copy), GFP_KERNEL);\r\nif (!copy)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < argc; i++) {\r\ncopy[i] = kstrdup(argv[i], GFP_KERNEL);\r\nif (!copy[i]) {\r\nwhile (i--)\r\nkfree(copy[i]);\r\nkfree(copy);\r\nreturn -ENOMEM;\r\n}\r\n}\r\ncache->nr_ctr_args = argc;\r\ncache->ctr_args = copy;\r\nreturn 0;\r\n}\r\nstatic int cache_ctr(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nint r = -EINVAL;\r\nstruct cache_args *ca;\r\nstruct cache *cache = NULL;\r\nca = kzalloc(sizeof(*ca), GFP_KERNEL);\r\nif (!ca) {\r\nti->error = "Error allocating memory for cache";\r\nreturn -ENOMEM;\r\n}\r\nca->ti = ti;\r\nr = parse_cache_args(ca, argc, argv, &ti->error);\r\nif (r)\r\ngoto out;\r\nr = cache_create(ca, &cache);\r\nif (r)\r\ngoto out;\r\nr = copy_ctr_args(cache, argc - 3, (const char **)argv + 3);\r\nif (r) {\r\ndestroy(cache);\r\ngoto out;\r\n}\r\nti->private = cache;\r\nout:\r\ndestroy_cache_args(ca);\r\nreturn r;\r\n}\r\nstatic int cache_map(struct dm_target *ti, struct bio *bio)\r\n{\r\nstruct cache *cache = ti->private;\r\nint r;\r\nstruct dm_bio_prison_cell *cell = NULL;\r\ndm_oblock_t block = get_bio_block(cache, bio);\r\nsize_t pb_data_size = get_per_bio_data_size(cache);\r\nbool can_migrate = false;\r\nbool fast_promotion;\r\nstruct policy_result lookup_result;\r\nstruct per_bio_data *pb = init_per_bio_data(bio, pb_data_size);\r\nstruct old_oblock_lock ool;\r\nool.locker.fn = null_locker;\r\nif (unlikely(from_oblock(block) >= from_oblock(cache->origin_blocks))) {\r\nremap_to_origin(cache, bio);\r\naccounted_begin(cache, bio);\r\nreturn DM_MAPIO_REMAPPED;\r\n}\r\nif (discard_or_flush(bio)) {\r\ndefer_bio(cache, bio);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\ncell = alloc_prison_cell(cache);\r\nif (!cell) {\r\ndefer_bio(cache, bio);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\nr = bio_detain(cache, block, bio, cell,\r\n(cell_free_fn) free_prison_cell,\r\ncache, &cell);\r\nif (r) {\r\nif (r < 0)\r\ndefer_bio(cache, bio);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\nfast_promotion = is_discarded_oblock(cache, block) || bio_writes_complete_block(cache, bio);\r\nr = policy_map(cache->policy, block, false, can_migrate, fast_promotion,\r\nbio, &ool.locker, &lookup_result);\r\nif (r == -EWOULDBLOCK) {\r\ncell_defer(cache, cell, true);\r\nreturn DM_MAPIO_SUBMITTED;\r\n} else if (r) {\r\nDMERR_LIMIT("%s: Unexpected return from cache replacement policy: %d",\r\ncache_device_name(cache), r);\r\ncell_defer(cache, cell, false);\r\nbio_io_error(bio);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\nr = DM_MAPIO_REMAPPED;\r\nswitch (lookup_result.op) {\r\ncase POLICY_HIT:\r\nif (passthrough_mode(&cache->features)) {\r\nif (bio_data_dir(bio) == WRITE) {\r\ncell_defer(cache, cell, true);\r\nr = DM_MAPIO_SUBMITTED;\r\n} else {\r\ninc_miss_counter(cache, bio);\r\nremap_to_origin_clear_discard(cache, bio, block);\r\naccounted_begin(cache, bio);\r\ninc_ds(cache, bio, cell);\r\ncell_defer(cache, cell, false);\r\n}\r\n} else {\r\ninc_hit_counter(cache, bio);\r\nif (bio_data_dir(bio) == WRITE && writethrough_mode(&cache->features) &&\r\n!is_dirty(cache, lookup_result.cblock)) {\r\nremap_to_origin_then_cache(cache, bio, block, lookup_result.cblock);\r\naccounted_begin(cache, bio);\r\ninc_ds(cache, bio, cell);\r\ncell_defer(cache, cell, false);\r\n} else\r\nremap_cell_to_cache_dirty(cache, cell, block, lookup_result.cblock, false);\r\n}\r\nbreak;\r\ncase POLICY_MISS:\r\ninc_miss_counter(cache, bio);\r\nif (pb->req_nr != 0) {\r\nbio_endio(bio);\r\ncell_defer(cache, cell, false);\r\nr = DM_MAPIO_SUBMITTED;\r\n} else\r\nremap_cell_to_origin_clear_discard(cache, cell, block, false);\r\nbreak;\r\ndefault:\r\nDMERR_LIMIT("%s: %s: erroring bio: unknown policy op: %u",\r\ncache_device_name(cache), __func__,\r\n(unsigned) lookup_result.op);\r\ncell_defer(cache, cell, false);\r\nbio_io_error(bio);\r\nr = DM_MAPIO_SUBMITTED;\r\n}\r\nreturn r;\r\n}\r\nstatic int cache_end_io(struct dm_target *ti, struct bio *bio, int error)\r\n{\r\nstruct cache *cache = ti->private;\r\nunsigned long flags;\r\nsize_t pb_data_size = get_per_bio_data_size(cache);\r\nstruct per_bio_data *pb = get_per_bio_data(bio, pb_data_size);\r\nif (pb->tick) {\r\npolicy_tick(cache->policy, false);\r\nspin_lock_irqsave(&cache->lock, flags);\r\ncache->need_tick_bio = true;\r\nspin_unlock_irqrestore(&cache->lock, flags);\r\n}\r\ncheck_for_quiesced_migrations(cache, pb);\r\naccounted_complete(cache, bio);\r\nreturn 0;\r\n}\r\nstatic int write_dirty_bitset(struct cache *cache)\r\n{\r\nunsigned i, r;\r\nif (get_cache_mode(cache) >= CM_READ_ONLY)\r\nreturn -EINVAL;\r\nfor (i = 0; i < from_cblock(cache->cache_size); i++) {\r\nr = dm_cache_set_dirty(cache->cmd, to_cblock(i),\r\nis_dirty(cache, to_cblock(i)));\r\nif (r) {\r\nmetadata_operation_failed(cache, "dm_cache_set_dirty", r);\r\nreturn r;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int write_discard_bitset(struct cache *cache)\r\n{\r\nunsigned i, r;\r\nif (get_cache_mode(cache) >= CM_READ_ONLY)\r\nreturn -EINVAL;\r\nr = dm_cache_discard_bitset_resize(cache->cmd, cache->discard_block_size,\r\ncache->discard_nr_blocks);\r\nif (r) {\r\nDMERR("%s: could not resize on-disk discard bitset", cache_device_name(cache));\r\nmetadata_operation_failed(cache, "dm_cache_discard_bitset_resize", r);\r\nreturn r;\r\n}\r\nfor (i = 0; i < from_dblock(cache->discard_nr_blocks); i++) {\r\nr = dm_cache_set_discard(cache->cmd, to_dblock(i),\r\nis_discarded(cache, to_dblock(i)));\r\nif (r) {\r\nmetadata_operation_failed(cache, "dm_cache_set_discard", r);\r\nreturn r;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int write_hints(struct cache *cache)\r\n{\r\nint r;\r\nif (get_cache_mode(cache) >= CM_READ_ONLY)\r\nreturn -EINVAL;\r\nr = dm_cache_write_hints(cache->cmd, cache->policy);\r\nif (r) {\r\nmetadata_operation_failed(cache, "dm_cache_write_hints", r);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic bool sync_metadata(struct cache *cache)\r\n{\r\nint r1, r2, r3, r4;\r\nr1 = write_dirty_bitset(cache);\r\nif (r1)\r\nDMERR("%s: could not write dirty bitset", cache_device_name(cache));\r\nr2 = write_discard_bitset(cache);\r\nif (r2)\r\nDMERR("%s: could not write discard bitset", cache_device_name(cache));\r\nsave_stats(cache);\r\nr3 = write_hints(cache);\r\nif (r3)\r\nDMERR("%s: could not write hints", cache_device_name(cache));\r\nr4 = commit(cache, !r1 && !r2 && !r3);\r\nif (r4)\r\nDMERR("%s: could not write cache metadata", cache_device_name(cache));\r\nreturn !r1 && !r2 && !r3 && !r4;\r\n}\r\nstatic void cache_postsuspend(struct dm_target *ti)\r\n{\r\nstruct cache *cache = ti->private;\r\nstart_quiescing(cache);\r\nwait_for_migrations(cache);\r\nstop_worker(cache);\r\nrequeue_deferred_bios(cache);\r\nrequeue_deferred_cells(cache);\r\nstop_quiescing(cache);\r\nif (get_cache_mode(cache) == CM_WRITE)\r\n(void) sync_metadata(cache);\r\n}\r\nstatic int load_mapping(void *context, dm_oblock_t oblock, dm_cblock_t cblock,\r\nbool dirty, uint32_t hint, bool hint_valid)\r\n{\r\nint r;\r\nstruct cache *cache = context;\r\nr = policy_load_mapping(cache->policy, oblock, cblock, hint, hint_valid);\r\nif (r)\r\nreturn r;\r\nif (dirty)\r\nset_dirty(cache, oblock, cblock);\r\nelse\r\nclear_dirty(cache, oblock, cblock);\r\nreturn 0;\r\n}\r\nstatic void discard_load_info_init(struct cache *cache,\r\nstruct discard_load_info *li)\r\n{\r\nli->cache = cache;\r\nli->discard_begin = li->discard_end = 0;\r\n}\r\nstatic void set_discard_range(struct discard_load_info *li)\r\n{\r\nsector_t b, e;\r\nif (li->discard_begin == li->discard_end)\r\nreturn;\r\nb = li->discard_begin * li->block_size;\r\ne = li->discard_end * li->block_size;\r\nb = dm_sector_div_up(b, li->cache->discard_block_size);\r\nsector_div(e, li->cache->discard_block_size);\r\nif (e > from_dblock(li->cache->discard_nr_blocks))\r\ne = from_dblock(li->cache->discard_nr_blocks);\r\nfor (; b < e; b++)\r\nset_discard(li->cache, to_dblock(b));\r\n}\r\nstatic int load_discard(void *context, sector_t discard_block_size,\r\ndm_dblock_t dblock, bool discard)\r\n{\r\nstruct discard_load_info *li = context;\r\nli->block_size = discard_block_size;\r\nif (discard) {\r\nif (from_dblock(dblock) == li->discard_end)\r\nli->discard_end = li->discard_end + 1ULL;\r\nelse {\r\nset_discard_range(li);\r\nli->discard_begin = from_dblock(dblock);\r\nli->discard_end = li->discard_begin + 1ULL;\r\n}\r\n} else {\r\nset_discard_range(li);\r\nli->discard_begin = li->discard_end = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic dm_cblock_t get_cache_dev_size(struct cache *cache)\r\n{\r\nsector_t size = get_dev_size(cache->cache_dev);\r\n(void) sector_div(size, cache->sectors_per_block);\r\nreturn to_cblock(size);\r\n}\r\nstatic bool can_resize(struct cache *cache, dm_cblock_t new_size)\r\n{\r\nif (from_cblock(new_size) > from_cblock(cache->cache_size))\r\nreturn true;\r\nwhile (from_cblock(new_size) < from_cblock(cache->cache_size)) {\r\nnew_size = to_cblock(from_cblock(new_size) + 1);\r\nif (is_dirty(cache, new_size)) {\r\nDMERR("%s: unable to shrink cache; cache block %llu is dirty",\r\ncache_device_name(cache),\r\n(unsigned long long) from_cblock(new_size));\r\nreturn false;\r\n}\r\n}\r\nreturn true;\r\n}\r\nstatic int resize_cache_dev(struct cache *cache, dm_cblock_t new_size)\r\n{\r\nint r;\r\nr = dm_cache_resize(cache->cmd, new_size);\r\nif (r) {\r\nDMERR("%s: could not resize cache metadata", cache_device_name(cache));\r\nmetadata_operation_failed(cache, "dm_cache_resize", r);\r\nreturn r;\r\n}\r\nset_cache_size(cache, new_size);\r\nreturn 0;\r\n}\r\nstatic int cache_preresume(struct dm_target *ti)\r\n{\r\nint r = 0;\r\nstruct cache *cache = ti->private;\r\ndm_cblock_t csize = get_cache_dev_size(cache);\r\nif (!cache->sized) {\r\nr = resize_cache_dev(cache, csize);\r\nif (r)\r\nreturn r;\r\ncache->sized = true;\r\n} else if (csize != cache->cache_size) {\r\nif (!can_resize(cache, csize))\r\nreturn -EINVAL;\r\nr = resize_cache_dev(cache, csize);\r\nif (r)\r\nreturn r;\r\n}\r\nif (!cache->loaded_mappings) {\r\nr = dm_cache_load_mappings(cache->cmd, cache->policy,\r\nload_mapping, cache);\r\nif (r) {\r\nDMERR("%s: could not load cache mappings", cache_device_name(cache));\r\nmetadata_operation_failed(cache, "dm_cache_load_mappings", r);\r\nreturn r;\r\n}\r\ncache->loaded_mappings = true;\r\n}\r\nif (!cache->loaded_discards) {\r\nstruct discard_load_info li;\r\nclear_bitset(cache->discard_bitset, from_dblock(cache->discard_nr_blocks));\r\ndiscard_load_info_init(cache, &li);\r\nr = dm_cache_load_discards(cache->cmd, load_discard, &li);\r\nif (r) {\r\nDMERR("%s: could not load origin discards", cache_device_name(cache));\r\nmetadata_operation_failed(cache, "dm_cache_load_discards", r);\r\nreturn r;\r\n}\r\nset_discard_range(&li);\r\ncache->loaded_discards = true;\r\n}\r\nreturn r;\r\n}\r\nstatic void cache_resume(struct dm_target *ti)\r\n{\r\nstruct cache *cache = ti->private;\r\ncache->need_tick_bio = true;\r\ndo_waker(&cache->waker.work);\r\n}\r\nstatic void cache_status(struct dm_target *ti, status_type_t type,\r\nunsigned status_flags, char *result, unsigned maxlen)\r\n{\r\nint r = 0;\r\nunsigned i;\r\nssize_t sz = 0;\r\ndm_block_t nr_free_blocks_metadata = 0;\r\ndm_block_t nr_blocks_metadata = 0;\r\nchar buf[BDEVNAME_SIZE];\r\nstruct cache *cache = ti->private;\r\ndm_cblock_t residency;\r\nbool needs_check;\r\nswitch (type) {\r\ncase STATUSTYPE_INFO:\r\nif (get_cache_mode(cache) == CM_FAIL) {\r\nDMEMIT("Fail");\r\nbreak;\r\n}\r\nif (!(status_flags & DM_STATUS_NOFLUSH_FLAG) && !dm_suspended(ti))\r\n(void) commit(cache, false);\r\nr = dm_cache_get_free_metadata_block_count(cache->cmd, &nr_free_blocks_metadata);\r\nif (r) {\r\nDMERR("%s: dm_cache_get_free_metadata_block_count returned %d",\r\ncache_device_name(cache), r);\r\ngoto err;\r\n}\r\nr = dm_cache_get_metadata_dev_size(cache->cmd, &nr_blocks_metadata);\r\nif (r) {\r\nDMERR("%s: dm_cache_get_metadata_dev_size returned %d",\r\ncache_device_name(cache), r);\r\ngoto err;\r\n}\r\nresidency = policy_residency(cache->policy);\r\nDMEMIT("%u %llu/%llu %u %llu/%llu %u %u %u %u %u %u %lu ",\r\n(unsigned)DM_CACHE_METADATA_BLOCK_SIZE,\r\n(unsigned long long)(nr_blocks_metadata - nr_free_blocks_metadata),\r\n(unsigned long long)nr_blocks_metadata,\r\ncache->sectors_per_block,\r\n(unsigned long long) from_cblock(residency),\r\n(unsigned long long) from_cblock(cache->cache_size),\r\n(unsigned) atomic_read(&cache->stats.read_hit),\r\n(unsigned) atomic_read(&cache->stats.read_miss),\r\n(unsigned) atomic_read(&cache->stats.write_hit),\r\n(unsigned) atomic_read(&cache->stats.write_miss),\r\n(unsigned) atomic_read(&cache->stats.demotion),\r\n(unsigned) atomic_read(&cache->stats.promotion),\r\n(unsigned long) atomic_read(&cache->nr_dirty));\r\nif (writethrough_mode(&cache->features))\r\nDMEMIT("1 writethrough ");\r\nelse if (passthrough_mode(&cache->features))\r\nDMEMIT("1 passthrough ");\r\nelse if (writeback_mode(&cache->features))\r\nDMEMIT("1 writeback ");\r\nelse {\r\nDMERR("%s: internal error: unknown io mode: %d",\r\ncache_device_name(cache), (int) cache->features.io_mode);\r\ngoto err;\r\n}\r\nDMEMIT("2 migration_threshold %llu ", (unsigned long long) cache->migration_threshold);\r\nDMEMIT("%s ", dm_cache_policy_get_name(cache->policy));\r\nif (sz < maxlen) {\r\nr = policy_emit_config_values(cache->policy, result, maxlen, &sz);\r\nif (r)\r\nDMERR("%s: policy_emit_config_values returned %d",\r\ncache_device_name(cache), r);\r\n}\r\nif (get_cache_mode(cache) == CM_READ_ONLY)\r\nDMEMIT("ro ");\r\nelse\r\nDMEMIT("rw ");\r\nr = dm_cache_metadata_needs_check(cache->cmd, &needs_check);\r\nif (r || needs_check)\r\nDMEMIT("needs_check ");\r\nelse\r\nDMEMIT("- ");\r\nbreak;\r\ncase STATUSTYPE_TABLE:\r\nformat_dev_t(buf, cache->metadata_dev->bdev->bd_dev);\r\nDMEMIT("%s ", buf);\r\nformat_dev_t(buf, cache->cache_dev->bdev->bd_dev);\r\nDMEMIT("%s ", buf);\r\nformat_dev_t(buf, cache->origin_dev->bdev->bd_dev);\r\nDMEMIT("%s", buf);\r\nfor (i = 0; i < cache->nr_ctr_args - 1; i++)\r\nDMEMIT(" %s", cache->ctr_args[i]);\r\nif (cache->nr_ctr_args)\r\nDMEMIT(" %s", cache->ctr_args[cache->nr_ctr_args - 1]);\r\n}\r\nreturn;\r\nerr:\r\nDMEMIT("Error");\r\n}\r\nstatic int parse_cblock_range(struct cache *cache, const char *str,\r\nstruct cblock_range *result)\r\n{\r\nchar dummy;\r\nuint64_t b, e;\r\nint r;\r\nr = sscanf(str, "%llu-%llu%c", &b, &e, &dummy);\r\nif (r < 0)\r\nreturn r;\r\nif (r == 2) {\r\nresult->begin = to_cblock(b);\r\nresult->end = to_cblock(e);\r\nreturn 0;\r\n}\r\nr = sscanf(str, "%llu%c", &b, &dummy);\r\nif (r < 0)\r\nreturn r;\r\nif (r == 1) {\r\nresult->begin = to_cblock(b);\r\nresult->end = to_cblock(from_cblock(result->begin) + 1u);\r\nreturn 0;\r\n}\r\nDMERR("%s: invalid cblock range '%s'", cache_device_name(cache), str);\r\nreturn -EINVAL;\r\n}\r\nstatic int validate_cblock_range(struct cache *cache, struct cblock_range *range)\r\n{\r\nuint64_t b = from_cblock(range->begin);\r\nuint64_t e = from_cblock(range->end);\r\nuint64_t n = from_cblock(cache->cache_size);\r\nif (b >= n) {\r\nDMERR("%s: begin cblock out of range: %llu >= %llu",\r\ncache_device_name(cache), b, n);\r\nreturn -EINVAL;\r\n}\r\nif (e > n) {\r\nDMERR("%s: end cblock out of range: %llu > %llu",\r\ncache_device_name(cache), e, n);\r\nreturn -EINVAL;\r\n}\r\nif (b >= e) {\r\nDMERR("%s: invalid cblock range: %llu >= %llu",\r\ncache_device_name(cache), b, e);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int request_invalidation(struct cache *cache, struct cblock_range *range)\r\n{\r\nstruct invalidation_request req;\r\nINIT_LIST_HEAD(&req.list);\r\nreq.cblocks = range;\r\natomic_set(&req.complete, 0);\r\nreq.err = 0;\r\ninit_waitqueue_head(&req.result_wait);\r\nspin_lock(&cache->invalidation_lock);\r\nlist_add(&req.list, &cache->invalidation_requests);\r\nspin_unlock(&cache->invalidation_lock);\r\nwake_worker(cache);\r\nwait_event(req.result_wait, atomic_read(&req.complete));\r\nreturn req.err;\r\n}\r\nstatic int process_invalidate_cblocks_message(struct cache *cache, unsigned count,\r\nconst char **cblock_ranges)\r\n{\r\nint r = 0;\r\nunsigned i;\r\nstruct cblock_range range;\r\nif (!passthrough_mode(&cache->features)) {\r\nDMERR("%s: cache has to be in passthrough mode for invalidation",\r\ncache_device_name(cache));\r\nreturn -EPERM;\r\n}\r\nfor (i = 0; i < count; i++) {\r\nr = parse_cblock_range(cache, cblock_ranges[i], &range);\r\nif (r)\r\nbreak;\r\nr = validate_cblock_range(cache, &range);\r\nif (r)\r\nbreak;\r\nr = request_invalidation(cache, &range);\r\nif (r)\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstatic int cache_message(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nstruct cache *cache = ti->private;\r\nif (!argc)\r\nreturn -EINVAL;\r\nif (get_cache_mode(cache) >= CM_READ_ONLY) {\r\nDMERR("%s: unable to service cache target messages in READ_ONLY or FAIL mode",\r\ncache_device_name(cache));\r\nreturn -EOPNOTSUPP;\r\n}\r\nif (!strcasecmp(argv[0], "invalidate_cblocks"))\r\nreturn process_invalidate_cblocks_message(cache, argc - 1, (const char **) argv + 1);\r\nif (argc != 2)\r\nreturn -EINVAL;\r\nreturn set_config_value(cache, argv[0], argv[1]);\r\n}\r\nstatic int cache_iterate_devices(struct dm_target *ti,\r\niterate_devices_callout_fn fn, void *data)\r\n{\r\nint r = 0;\r\nstruct cache *cache = ti->private;\r\nr = fn(ti, cache->cache_dev, 0, get_dev_size(cache->cache_dev), data);\r\nif (!r)\r\nr = fn(ti, cache->origin_dev, 0, ti->len, data);\r\nreturn r;\r\n}\r\nstatic void set_discard_limits(struct cache *cache, struct queue_limits *limits)\r\n{\r\nlimits->max_discard_sectors = min_t(sector_t, cache->discard_block_size * 1024,\r\ncache->origin_sectors);\r\nlimits->discard_granularity = cache->discard_block_size << SECTOR_SHIFT;\r\n}\r\nstatic void cache_io_hints(struct dm_target *ti, struct queue_limits *limits)\r\n{\r\nstruct cache *cache = ti->private;\r\nuint64_t io_opt_sectors = limits->io_opt >> SECTOR_SHIFT;\r\nif (io_opt_sectors < cache->sectors_per_block ||\r\ndo_div(io_opt_sectors, cache->sectors_per_block)) {\r\nblk_limits_io_min(limits, cache->sectors_per_block << SECTOR_SHIFT);\r\nblk_limits_io_opt(limits, cache->sectors_per_block << SECTOR_SHIFT);\r\n}\r\nset_discard_limits(cache, limits);\r\n}\r\nstatic int __init dm_cache_init(void)\r\n{\r\nint r;\r\nr = dm_register_target(&cache_target);\r\nif (r) {\r\nDMERR("cache target registration failed: %d", r);\r\nreturn r;\r\n}\r\nmigration_cache = KMEM_CACHE(dm_cache_migration, 0);\r\nif (!migration_cache) {\r\ndm_unregister_target(&cache_target);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __exit dm_cache_exit(void)\r\n{\r\ndm_unregister_target(&cache_target);\r\nkmem_cache_destroy(migration_cache);\r\n}
