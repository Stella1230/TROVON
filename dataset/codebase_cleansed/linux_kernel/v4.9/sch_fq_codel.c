static unsigned int fq_codel_hash(const struct fq_codel_sched_data *q,\r\nstruct sk_buff *skb)\r\n{\r\nu32 hash = skb_get_hash_perturb(skb, q->perturbation);\r\nreturn reciprocal_scale(hash, q->flows_cnt);\r\n}\r\nstatic unsigned int fq_codel_classify(struct sk_buff *skb, struct Qdisc *sch,\r\nint *qerr)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nstruct tcf_proto *filter;\r\nstruct tcf_result res;\r\nint result;\r\nif (TC_H_MAJ(skb->priority) == sch->handle &&\r\nTC_H_MIN(skb->priority) > 0 &&\r\nTC_H_MIN(skb->priority) <= q->flows_cnt)\r\nreturn TC_H_MIN(skb->priority);\r\nfilter = rcu_dereference_bh(q->filter_list);\r\nif (!filter)\r\nreturn fq_codel_hash(q, skb) + 1;\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\r\nresult = tc_classify(skb, filter, &res, false);\r\nif (result >= 0) {\r\n#ifdef CONFIG_NET_CLS_ACT\r\nswitch (result) {\r\ncase TC_ACT_STOLEN:\r\ncase TC_ACT_QUEUED:\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\r\ncase TC_ACT_SHOT:\r\nreturn 0;\r\n}\r\n#endif\r\nif (TC_H_MIN(res.classid) <= q->flows_cnt)\r\nreturn TC_H_MIN(res.classid);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline struct sk_buff *dequeue_head(struct fq_codel_flow *flow)\r\n{\r\nstruct sk_buff *skb = flow->head;\r\nflow->head = skb->next;\r\nskb->next = NULL;\r\nreturn skb;\r\n}\r\nstatic inline void flow_queue_add(struct fq_codel_flow *flow,\r\nstruct sk_buff *skb)\r\n{\r\nif (flow->head == NULL)\r\nflow->head = skb;\r\nelse\r\nflow->tail->next = skb;\r\nflow->tail = skb;\r\nskb->next = NULL;\r\n}\r\nstatic unsigned int fq_codel_drop(struct Qdisc *sch, unsigned int max_packets,\r\nstruct sk_buff **to_free)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nunsigned int maxbacklog = 0, idx = 0, i, len;\r\nstruct fq_codel_flow *flow;\r\nunsigned int threshold;\r\nunsigned int mem = 0;\r\nfor (i = 0; i < q->flows_cnt; i++) {\r\nif (q->backlogs[i] > maxbacklog) {\r\nmaxbacklog = q->backlogs[i];\r\nidx = i;\r\n}\r\n}\r\nthreshold = maxbacklog >> 1;\r\nflow = &q->flows[idx];\r\nlen = 0;\r\ni = 0;\r\ndo {\r\nskb = dequeue_head(flow);\r\nlen += qdisc_pkt_len(skb);\r\nmem += get_codel_cb(skb)->mem_usage;\r\n__qdisc_drop(skb, to_free);\r\n} while (++i < max_packets && len < threshold);\r\nflow->dropped += i;\r\nq->backlogs[idx] -= len;\r\nq->memory_usage -= mem;\r\nsch->qstats.drops += i;\r\nsch->qstats.backlog -= len;\r\nsch->q.qlen -= i;\r\nreturn idx;\r\n}\r\nstatic int fq_codel_enqueue(struct sk_buff *skb, struct Qdisc *sch,\r\nstruct sk_buff **to_free)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nunsigned int idx, prev_backlog, prev_qlen;\r\nstruct fq_codel_flow *flow;\r\nint uninitialized_var(ret);\r\nunsigned int pkt_len;\r\nbool memory_limited;\r\nidx = fq_codel_classify(skb, sch, &ret);\r\nif (idx == 0) {\r\nif (ret & __NET_XMIT_BYPASS)\r\nqdisc_qstats_drop(sch);\r\n__qdisc_drop(skb, to_free);\r\nreturn ret;\r\n}\r\nidx--;\r\ncodel_set_enqueue_time(skb);\r\nflow = &q->flows[idx];\r\nflow_queue_add(flow, skb);\r\nq->backlogs[idx] += qdisc_pkt_len(skb);\r\nqdisc_qstats_backlog_inc(sch, skb);\r\nif (list_empty(&flow->flowchain)) {\r\nlist_add_tail(&flow->flowchain, &q->new_flows);\r\nq->new_flow_count++;\r\nflow->deficit = q->quantum;\r\nflow->dropped = 0;\r\n}\r\nget_codel_cb(skb)->mem_usage = skb->truesize;\r\nq->memory_usage += get_codel_cb(skb)->mem_usage;\r\nmemory_limited = q->memory_usage > q->memory_limit;\r\nif (++sch->q.qlen <= sch->limit && !memory_limited)\r\nreturn NET_XMIT_SUCCESS;\r\nprev_backlog = sch->qstats.backlog;\r\nprev_qlen = sch->q.qlen;\r\npkt_len = qdisc_pkt_len(skb);\r\nret = fq_codel_drop(sch, q->drop_batch_size, to_free);\r\nprev_qlen -= sch->q.qlen;\r\nprev_backlog -= sch->qstats.backlog;\r\nq->drop_overlimit += prev_qlen;\r\nif (memory_limited)\r\nq->drop_overmemory += prev_qlen;\r\nif (ret == idx) {\r\nqdisc_tree_reduce_backlog(sch, prev_qlen - 1,\r\nprev_backlog - pkt_len);\r\nreturn NET_XMIT_CN;\r\n}\r\nqdisc_tree_reduce_backlog(sch, prev_qlen, prev_backlog);\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic struct sk_buff *dequeue_func(struct codel_vars *vars, void *ctx)\r\n{\r\nstruct Qdisc *sch = ctx;\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nstruct fq_codel_flow *flow;\r\nstruct sk_buff *skb = NULL;\r\nflow = container_of(vars, struct fq_codel_flow, cvars);\r\nif (flow->head) {\r\nskb = dequeue_head(flow);\r\nq->backlogs[flow - q->flows] -= qdisc_pkt_len(skb);\r\nq->memory_usage -= get_codel_cb(skb)->mem_usage;\r\nsch->q.qlen--;\r\nsch->qstats.backlog -= qdisc_pkt_len(skb);\r\n}\r\nreturn skb;\r\n}\r\nstatic void drop_func(struct sk_buff *skb, void *ctx)\r\n{\r\nstruct Qdisc *sch = ctx;\r\nkfree_skb(skb);\r\nqdisc_qstats_drop(sch);\r\n}\r\nstatic struct sk_buff *fq_codel_dequeue(struct Qdisc *sch)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nstruct fq_codel_flow *flow;\r\nstruct list_head *head;\r\nu32 prev_drop_count, prev_ecn_mark;\r\nunsigned int prev_backlog;\r\nbegin:\r\nhead = &q->new_flows;\r\nif (list_empty(head)) {\r\nhead = &q->old_flows;\r\nif (list_empty(head))\r\nreturn NULL;\r\n}\r\nflow = list_first_entry(head, struct fq_codel_flow, flowchain);\r\nif (flow->deficit <= 0) {\r\nflow->deficit += q->quantum;\r\nlist_move_tail(&flow->flowchain, &q->old_flows);\r\ngoto begin;\r\n}\r\nprev_drop_count = q->cstats.drop_count;\r\nprev_ecn_mark = q->cstats.ecn_mark;\r\nprev_backlog = sch->qstats.backlog;\r\nskb = codel_dequeue(sch, &sch->qstats.backlog, &q->cparams,\r\n&flow->cvars, &q->cstats, qdisc_pkt_len,\r\ncodel_get_enqueue_time, drop_func, dequeue_func);\r\nflow->dropped += q->cstats.drop_count - prev_drop_count;\r\nflow->dropped += q->cstats.ecn_mark - prev_ecn_mark;\r\nif (!skb) {\r\nif ((head == &q->new_flows) && !list_empty(&q->old_flows))\r\nlist_move_tail(&flow->flowchain, &q->old_flows);\r\nelse\r\nlist_del_init(&flow->flowchain);\r\ngoto begin;\r\n}\r\nqdisc_bstats_update(sch, skb);\r\nflow->deficit -= qdisc_pkt_len(skb);\r\nif (q->cstats.drop_count && sch->q.qlen) {\r\nqdisc_tree_reduce_backlog(sch, q->cstats.drop_count,\r\nq->cstats.drop_len);\r\nq->cstats.drop_count = 0;\r\nq->cstats.drop_len = 0;\r\n}\r\nreturn skb;\r\n}\r\nstatic void fq_codel_flow_purge(struct fq_codel_flow *flow)\r\n{\r\nrtnl_kfree_skbs(flow->head, flow->tail);\r\nflow->head = NULL;\r\n}\r\nstatic void fq_codel_reset(struct Qdisc *sch)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nint i;\r\nINIT_LIST_HEAD(&q->new_flows);\r\nINIT_LIST_HEAD(&q->old_flows);\r\nfor (i = 0; i < q->flows_cnt; i++) {\r\nstruct fq_codel_flow *flow = q->flows + i;\r\nfq_codel_flow_purge(flow);\r\nINIT_LIST_HEAD(&flow->flowchain);\r\ncodel_vars_init(&flow->cvars);\r\n}\r\nmemset(q->backlogs, 0, q->flows_cnt * sizeof(u32));\r\nsch->q.qlen = 0;\r\nsch->qstats.backlog = 0;\r\nq->memory_usage = 0;\r\n}\r\nstatic int fq_codel_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *tb[TCA_FQ_CODEL_MAX + 1];\r\nint err;\r\nif (!opt)\r\nreturn -EINVAL;\r\nerr = nla_parse_nested(tb, TCA_FQ_CODEL_MAX, opt, fq_codel_policy);\r\nif (err < 0)\r\nreturn err;\r\nif (tb[TCA_FQ_CODEL_FLOWS]) {\r\nif (q->flows)\r\nreturn -EINVAL;\r\nq->flows_cnt = nla_get_u32(tb[TCA_FQ_CODEL_FLOWS]);\r\nif (!q->flows_cnt ||\r\nq->flows_cnt > 65536)\r\nreturn -EINVAL;\r\n}\r\nsch_tree_lock(sch);\r\nif (tb[TCA_FQ_CODEL_TARGET]) {\r\nu64 target = nla_get_u32(tb[TCA_FQ_CODEL_TARGET]);\r\nq->cparams.target = (target * NSEC_PER_USEC) >> CODEL_SHIFT;\r\n}\r\nif (tb[TCA_FQ_CODEL_CE_THRESHOLD]) {\r\nu64 val = nla_get_u32(tb[TCA_FQ_CODEL_CE_THRESHOLD]);\r\nq->cparams.ce_threshold = (val * NSEC_PER_USEC) >> CODEL_SHIFT;\r\n}\r\nif (tb[TCA_FQ_CODEL_INTERVAL]) {\r\nu64 interval = nla_get_u32(tb[TCA_FQ_CODEL_INTERVAL]);\r\nq->cparams.interval = (interval * NSEC_PER_USEC) >> CODEL_SHIFT;\r\n}\r\nif (tb[TCA_FQ_CODEL_LIMIT])\r\nsch->limit = nla_get_u32(tb[TCA_FQ_CODEL_LIMIT]);\r\nif (tb[TCA_FQ_CODEL_ECN])\r\nq->cparams.ecn = !!nla_get_u32(tb[TCA_FQ_CODEL_ECN]);\r\nif (tb[TCA_FQ_CODEL_QUANTUM])\r\nq->quantum = max(256U, nla_get_u32(tb[TCA_FQ_CODEL_QUANTUM]));\r\nif (tb[TCA_FQ_CODEL_DROP_BATCH_SIZE])\r\nq->drop_batch_size = min(1U, nla_get_u32(tb[TCA_FQ_CODEL_DROP_BATCH_SIZE]));\r\nif (tb[TCA_FQ_CODEL_MEMORY_LIMIT])\r\nq->memory_limit = min(1U << 31, nla_get_u32(tb[TCA_FQ_CODEL_MEMORY_LIMIT]));\r\nwhile (sch->q.qlen > sch->limit ||\r\nq->memory_usage > q->memory_limit) {\r\nstruct sk_buff *skb = fq_codel_dequeue(sch);\r\nq->cstats.drop_len += qdisc_pkt_len(skb);\r\nrtnl_kfree_skbs(skb, skb);\r\nq->cstats.drop_count++;\r\n}\r\nqdisc_tree_reduce_backlog(sch, q->cstats.drop_count, q->cstats.drop_len);\r\nq->cstats.drop_count = 0;\r\nq->cstats.drop_len = 0;\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic void *fq_codel_zalloc(size_t sz)\r\n{\r\nvoid *ptr = kzalloc(sz, GFP_KERNEL | __GFP_NOWARN);\r\nif (!ptr)\r\nptr = vzalloc(sz);\r\nreturn ptr;\r\n}\r\nstatic void fq_codel_free(void *addr)\r\n{\r\nkvfree(addr);\r\n}\r\nstatic void fq_codel_destroy(struct Qdisc *sch)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\ntcf_destroy_chain(&q->filter_list);\r\nfq_codel_free(q->backlogs);\r\nfq_codel_free(q->flows);\r\n}\r\nstatic int fq_codel_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nint i;\r\nsch->limit = 10*1024;\r\nq->flows_cnt = 1024;\r\nq->memory_limit = 32 << 20;\r\nq->drop_batch_size = 64;\r\nq->quantum = psched_mtu(qdisc_dev(sch));\r\nq->perturbation = prandom_u32();\r\nINIT_LIST_HEAD(&q->new_flows);\r\nINIT_LIST_HEAD(&q->old_flows);\r\ncodel_params_init(&q->cparams);\r\ncodel_stats_init(&q->cstats);\r\nq->cparams.ecn = true;\r\nq->cparams.mtu = psched_mtu(qdisc_dev(sch));\r\nif (opt) {\r\nint err = fq_codel_change(sch, opt);\r\nif (err)\r\nreturn err;\r\n}\r\nif (!q->flows) {\r\nq->flows = fq_codel_zalloc(q->flows_cnt *\r\nsizeof(struct fq_codel_flow));\r\nif (!q->flows)\r\nreturn -ENOMEM;\r\nq->backlogs = fq_codel_zalloc(q->flows_cnt * sizeof(u32));\r\nif (!q->backlogs) {\r\nfq_codel_free(q->flows);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < q->flows_cnt; i++) {\r\nstruct fq_codel_flow *flow = q->flows + i;\r\nINIT_LIST_HEAD(&flow->flowchain);\r\ncodel_vars_init(&flow->cvars);\r\n}\r\n}\r\nif (sch->limit >= 1)\r\nsch->flags |= TCQ_F_CAN_BYPASS;\r\nelse\r\nsch->flags &= ~TCQ_F_CAN_BYPASS;\r\nreturn 0;\r\n}\r\nstatic int fq_codel_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nstruct nlattr *opts;\r\nopts = nla_nest_start(skb, TCA_OPTIONS);\r\nif (opts == NULL)\r\ngoto nla_put_failure;\r\nif (nla_put_u32(skb, TCA_FQ_CODEL_TARGET,\r\ncodel_time_to_us(q->cparams.target)) ||\r\nnla_put_u32(skb, TCA_FQ_CODEL_LIMIT,\r\nsch->limit) ||\r\nnla_put_u32(skb, TCA_FQ_CODEL_INTERVAL,\r\ncodel_time_to_us(q->cparams.interval)) ||\r\nnla_put_u32(skb, TCA_FQ_CODEL_ECN,\r\nq->cparams.ecn) ||\r\nnla_put_u32(skb, TCA_FQ_CODEL_QUANTUM,\r\nq->quantum) ||\r\nnla_put_u32(skb, TCA_FQ_CODEL_DROP_BATCH_SIZE,\r\nq->drop_batch_size) ||\r\nnla_put_u32(skb, TCA_FQ_CODEL_MEMORY_LIMIT,\r\nq->memory_limit) ||\r\nnla_put_u32(skb, TCA_FQ_CODEL_FLOWS,\r\nq->flows_cnt))\r\ngoto nla_put_failure;\r\nif (q->cparams.ce_threshold != CODEL_DISABLED_THRESHOLD &&\r\nnla_put_u32(skb, TCA_FQ_CODEL_CE_THRESHOLD,\r\ncodel_time_to_us(q->cparams.ce_threshold)))\r\ngoto nla_put_failure;\r\nreturn nla_nest_end(skb, opts);\r\nnla_put_failure:\r\nreturn -1;\r\n}\r\nstatic int fq_codel_dump_stats(struct Qdisc *sch, struct gnet_dump *d)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nstruct tc_fq_codel_xstats st = {\r\n.type = TCA_FQ_CODEL_XSTATS_QDISC,\r\n};\r\nstruct list_head *pos;\r\nst.qdisc_stats.maxpacket = q->cstats.maxpacket;\r\nst.qdisc_stats.drop_overlimit = q->drop_overlimit;\r\nst.qdisc_stats.ecn_mark = q->cstats.ecn_mark;\r\nst.qdisc_stats.new_flow_count = q->new_flow_count;\r\nst.qdisc_stats.ce_mark = q->cstats.ce_mark;\r\nst.qdisc_stats.memory_usage = q->memory_usage;\r\nst.qdisc_stats.drop_overmemory = q->drop_overmemory;\r\nsch_tree_lock(sch);\r\nlist_for_each(pos, &q->new_flows)\r\nst.qdisc_stats.new_flows_len++;\r\nlist_for_each(pos, &q->old_flows)\r\nst.qdisc_stats.old_flows_len++;\r\nsch_tree_unlock(sch);\r\nreturn gnet_stats_copy_app(d, &st, sizeof(st));\r\n}\r\nstatic struct Qdisc *fq_codel_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nreturn NULL;\r\n}\r\nstatic unsigned long fq_codel_get(struct Qdisc *sch, u32 classid)\r\n{\r\nreturn 0;\r\n}\r\nstatic unsigned long fq_codel_bind(struct Qdisc *sch, unsigned long parent,\r\nu32 classid)\r\n{\r\nsch->flags &= ~TCQ_F_CAN_BYPASS;\r\nreturn 0;\r\n}\r\nstatic void fq_codel_put(struct Qdisc *q, unsigned long cl)\r\n{\r\n}\r\nstatic struct tcf_proto __rcu **fq_codel_find_tcf(struct Qdisc *sch,\r\nunsigned long cl)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nif (cl)\r\nreturn NULL;\r\nreturn &q->filter_list;\r\n}\r\nstatic int fq_codel_dump_class(struct Qdisc *sch, unsigned long cl,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\ntcm->tcm_handle |= TC_H_MIN(cl);\r\nreturn 0;\r\n}\r\nstatic int fq_codel_dump_class_stats(struct Qdisc *sch, unsigned long cl,\r\nstruct gnet_dump *d)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nu32 idx = cl - 1;\r\nstruct gnet_stats_queue qs = { 0 };\r\nstruct tc_fq_codel_xstats xstats;\r\nif (idx < q->flows_cnt) {\r\nconst struct fq_codel_flow *flow = &q->flows[idx];\r\nconst struct sk_buff *skb;\r\nmemset(&xstats, 0, sizeof(xstats));\r\nxstats.type = TCA_FQ_CODEL_XSTATS_CLASS;\r\nxstats.class_stats.deficit = flow->deficit;\r\nxstats.class_stats.ldelay =\r\ncodel_time_to_us(flow->cvars.ldelay);\r\nxstats.class_stats.count = flow->cvars.count;\r\nxstats.class_stats.lastcount = flow->cvars.lastcount;\r\nxstats.class_stats.dropping = flow->cvars.dropping;\r\nif (flow->cvars.dropping) {\r\ncodel_tdiff_t delta = flow->cvars.drop_next -\r\ncodel_get_time();\r\nxstats.class_stats.drop_next = (delta >= 0) ?\r\ncodel_time_to_us(delta) :\r\n-codel_time_to_us(-delta);\r\n}\r\nif (flow->head) {\r\nsch_tree_lock(sch);\r\nskb = flow->head;\r\nwhile (skb) {\r\nqs.qlen++;\r\nskb = skb->next;\r\n}\r\nsch_tree_unlock(sch);\r\n}\r\nqs.backlog = q->backlogs[idx];\r\nqs.drops = flow->dropped;\r\n}\r\nif (gnet_stats_copy_queue(d, NULL, &qs, qs.qlen) < 0)\r\nreturn -1;\r\nif (idx < q->flows_cnt)\r\nreturn gnet_stats_copy_app(d, &xstats, sizeof(xstats));\r\nreturn 0;\r\n}\r\nstatic void fq_codel_walk(struct Qdisc *sch, struct qdisc_walker *arg)\r\n{\r\nstruct fq_codel_sched_data *q = qdisc_priv(sch);\r\nunsigned int i;\r\nif (arg->stop)\r\nreturn;\r\nfor (i = 0; i < q->flows_cnt; i++) {\r\nif (list_empty(&q->flows[i].flowchain) ||\r\narg->count < arg->skip) {\r\narg->count++;\r\ncontinue;\r\n}\r\nif (arg->fn(sch, i + 1, arg) < 0) {\r\narg->stop = 1;\r\nbreak;\r\n}\r\narg->count++;\r\n}\r\n}\r\nstatic int __init fq_codel_module_init(void)\r\n{\r\nreturn register_qdisc(&fq_codel_qdisc_ops);\r\n}\r\nstatic void __exit fq_codel_module_exit(void)\r\n{\r\nunregister_qdisc(&fq_codel_qdisc_ops);\r\n}
