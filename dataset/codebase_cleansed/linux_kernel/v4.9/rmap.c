static inline struct anon_vma *anon_vma_alloc(void)\r\n{\r\nstruct anon_vma *anon_vma;\r\nanon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);\r\nif (anon_vma) {\r\natomic_set(&anon_vma->refcount, 1);\r\nanon_vma->degree = 1;\r\nanon_vma->parent = anon_vma;\r\nanon_vma->root = anon_vma;\r\n}\r\nreturn anon_vma;\r\n}\r\nstatic inline void anon_vma_free(struct anon_vma *anon_vma)\r\n{\r\nVM_BUG_ON(atomic_read(&anon_vma->refcount));\r\nmight_sleep();\r\nif (rwsem_is_locked(&anon_vma->root->rwsem)) {\r\nanon_vma_lock_write(anon_vma);\r\nanon_vma_unlock_write(anon_vma);\r\n}\r\nkmem_cache_free(anon_vma_cachep, anon_vma);\r\n}\r\nstatic inline struct anon_vma_chain *anon_vma_chain_alloc(gfp_t gfp)\r\n{\r\nreturn kmem_cache_alloc(anon_vma_chain_cachep, gfp);\r\n}\r\nstatic void anon_vma_chain_free(struct anon_vma_chain *anon_vma_chain)\r\n{\r\nkmem_cache_free(anon_vma_chain_cachep, anon_vma_chain);\r\n}\r\nstatic void anon_vma_chain_link(struct vm_area_struct *vma,\r\nstruct anon_vma_chain *avc,\r\nstruct anon_vma *anon_vma)\r\n{\r\navc->vma = vma;\r\navc->anon_vma = anon_vma;\r\nlist_add(&avc->same_vma, &vma->anon_vma_chain);\r\nanon_vma_interval_tree_insert(avc, &anon_vma->rb_root);\r\n}\r\nint anon_vma_prepare(struct vm_area_struct *vma)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\nstruct anon_vma_chain *avc;\r\nmight_sleep();\r\nif (unlikely(!anon_vma)) {\r\nstruct mm_struct *mm = vma->vm_mm;\r\nstruct anon_vma *allocated;\r\navc = anon_vma_chain_alloc(GFP_KERNEL);\r\nif (!avc)\r\ngoto out_enomem;\r\nanon_vma = find_mergeable_anon_vma(vma);\r\nallocated = NULL;\r\nif (!anon_vma) {\r\nanon_vma = anon_vma_alloc();\r\nif (unlikely(!anon_vma))\r\ngoto out_enomem_free_avc;\r\nallocated = anon_vma;\r\n}\r\nanon_vma_lock_write(anon_vma);\r\nspin_lock(&mm->page_table_lock);\r\nif (likely(!vma->anon_vma)) {\r\nvma->anon_vma = anon_vma;\r\nanon_vma_chain_link(vma, avc, anon_vma);\r\nanon_vma->degree++;\r\nallocated = NULL;\r\navc = NULL;\r\n}\r\nspin_unlock(&mm->page_table_lock);\r\nanon_vma_unlock_write(anon_vma);\r\nif (unlikely(allocated))\r\nput_anon_vma(allocated);\r\nif (unlikely(avc))\r\nanon_vma_chain_free(avc);\r\n}\r\nreturn 0;\r\nout_enomem_free_avc:\r\nanon_vma_chain_free(avc);\r\nout_enomem:\r\nreturn -ENOMEM;\r\n}\r\nstatic inline struct anon_vma *lock_anon_vma_root(struct anon_vma *root, struct anon_vma *anon_vma)\r\n{\r\nstruct anon_vma *new_root = anon_vma->root;\r\nif (new_root != root) {\r\nif (WARN_ON_ONCE(root))\r\nup_write(&root->rwsem);\r\nroot = new_root;\r\ndown_write(&root->rwsem);\r\n}\r\nreturn root;\r\n}\r\nstatic inline void unlock_anon_vma_root(struct anon_vma *root)\r\n{\r\nif (root)\r\nup_write(&root->rwsem);\r\n}\r\nint anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)\r\n{\r\nstruct anon_vma_chain *avc, *pavc;\r\nstruct anon_vma *root = NULL;\r\nlist_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {\r\nstruct anon_vma *anon_vma;\r\navc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);\r\nif (unlikely(!avc)) {\r\nunlock_anon_vma_root(root);\r\nroot = NULL;\r\navc = anon_vma_chain_alloc(GFP_KERNEL);\r\nif (!avc)\r\ngoto enomem_failure;\r\n}\r\nanon_vma = pavc->anon_vma;\r\nroot = lock_anon_vma_root(root, anon_vma);\r\nanon_vma_chain_link(dst, avc, anon_vma);\r\nif (!dst->anon_vma && anon_vma != src->anon_vma &&\r\nanon_vma->degree < 2)\r\ndst->anon_vma = anon_vma;\r\n}\r\nif (dst->anon_vma)\r\ndst->anon_vma->degree++;\r\nunlock_anon_vma_root(root);\r\nreturn 0;\r\nenomem_failure:\r\ndst->anon_vma = NULL;\r\nunlink_anon_vmas(dst);\r\nreturn -ENOMEM;\r\n}\r\nint anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\r\n{\r\nstruct anon_vma_chain *avc;\r\nstruct anon_vma *anon_vma;\r\nint error;\r\nif (!pvma->anon_vma)\r\nreturn 0;\r\nvma->anon_vma = NULL;\r\nerror = anon_vma_clone(vma, pvma);\r\nif (error)\r\nreturn error;\r\nif (vma->anon_vma)\r\nreturn 0;\r\nanon_vma = anon_vma_alloc();\r\nif (!anon_vma)\r\ngoto out_error;\r\navc = anon_vma_chain_alloc(GFP_KERNEL);\r\nif (!avc)\r\ngoto out_error_free_anon_vma;\r\nanon_vma->root = pvma->anon_vma->root;\r\nanon_vma->parent = pvma->anon_vma;\r\nget_anon_vma(anon_vma->root);\r\nvma->anon_vma = anon_vma;\r\nanon_vma_lock_write(anon_vma);\r\nanon_vma_chain_link(vma, avc, anon_vma);\r\nanon_vma->parent->degree++;\r\nanon_vma_unlock_write(anon_vma);\r\nreturn 0;\r\nout_error_free_anon_vma:\r\nput_anon_vma(anon_vma);\r\nout_error:\r\nunlink_anon_vmas(vma);\r\nreturn -ENOMEM;\r\n}\r\nvoid unlink_anon_vmas(struct vm_area_struct *vma)\r\n{\r\nstruct anon_vma_chain *avc, *next;\r\nstruct anon_vma *root = NULL;\r\nlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\r\nstruct anon_vma *anon_vma = avc->anon_vma;\r\nroot = lock_anon_vma_root(root, anon_vma);\r\nanon_vma_interval_tree_remove(avc, &anon_vma->rb_root);\r\nif (RB_EMPTY_ROOT(&anon_vma->rb_root)) {\r\nanon_vma->parent->degree--;\r\ncontinue;\r\n}\r\nlist_del(&avc->same_vma);\r\nanon_vma_chain_free(avc);\r\n}\r\nif (vma->anon_vma)\r\nvma->anon_vma->degree--;\r\nunlock_anon_vma_root(root);\r\nlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\r\nstruct anon_vma *anon_vma = avc->anon_vma;\r\nVM_WARN_ON(anon_vma->degree);\r\nput_anon_vma(anon_vma);\r\nlist_del(&avc->same_vma);\r\nanon_vma_chain_free(avc);\r\n}\r\n}\r\nstatic void anon_vma_ctor(void *data)\r\n{\r\nstruct anon_vma *anon_vma = data;\r\ninit_rwsem(&anon_vma->rwsem);\r\natomic_set(&anon_vma->refcount, 0);\r\nanon_vma->rb_root = RB_ROOT;\r\n}\r\nvoid __init anon_vma_init(void)\r\n{\r\nanon_vma_cachep = kmem_cache_create("anon_vma", sizeof(struct anon_vma),\r\n0, SLAB_DESTROY_BY_RCU|SLAB_PANIC|SLAB_ACCOUNT,\r\nanon_vma_ctor);\r\nanon_vma_chain_cachep = KMEM_CACHE(anon_vma_chain,\r\nSLAB_PANIC|SLAB_ACCOUNT);\r\n}\r\nstruct anon_vma *page_get_anon_vma(struct page *page)\r\n{\r\nstruct anon_vma *anon_vma = NULL;\r\nunsigned long anon_mapping;\r\nrcu_read_lock();\r\nanon_mapping = (unsigned long)READ_ONCE(page->mapping);\r\nif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\r\ngoto out;\r\nif (!page_mapped(page))\r\ngoto out;\r\nanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\r\nif (!atomic_inc_not_zero(&anon_vma->refcount)) {\r\nanon_vma = NULL;\r\ngoto out;\r\n}\r\nif (!page_mapped(page)) {\r\nrcu_read_unlock();\r\nput_anon_vma(anon_vma);\r\nreturn NULL;\r\n}\r\nout:\r\nrcu_read_unlock();\r\nreturn anon_vma;\r\n}\r\nstruct anon_vma *page_lock_anon_vma_read(struct page *page)\r\n{\r\nstruct anon_vma *anon_vma = NULL;\r\nstruct anon_vma *root_anon_vma;\r\nunsigned long anon_mapping;\r\nrcu_read_lock();\r\nanon_mapping = (unsigned long)READ_ONCE(page->mapping);\r\nif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\r\ngoto out;\r\nif (!page_mapped(page))\r\ngoto out;\r\nanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\r\nroot_anon_vma = READ_ONCE(anon_vma->root);\r\nif (down_read_trylock(&root_anon_vma->rwsem)) {\r\nif (!page_mapped(page)) {\r\nup_read(&root_anon_vma->rwsem);\r\nanon_vma = NULL;\r\n}\r\ngoto out;\r\n}\r\nif (!atomic_inc_not_zero(&anon_vma->refcount)) {\r\nanon_vma = NULL;\r\ngoto out;\r\n}\r\nif (!page_mapped(page)) {\r\nrcu_read_unlock();\r\nput_anon_vma(anon_vma);\r\nreturn NULL;\r\n}\r\nrcu_read_unlock();\r\nanon_vma_lock_read(anon_vma);\r\nif (atomic_dec_and_test(&anon_vma->refcount)) {\r\nanon_vma_unlock_read(anon_vma);\r\n__put_anon_vma(anon_vma);\r\nanon_vma = NULL;\r\n}\r\nreturn anon_vma;\r\nout:\r\nrcu_read_unlock();\r\nreturn anon_vma;\r\n}\r\nvoid page_unlock_anon_vma_read(struct anon_vma *anon_vma)\r\n{\r\nanon_vma_unlock_read(anon_vma);\r\n}\r\nvoid try_to_unmap_flush(void)\r\n{\r\nstruct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;\r\nint cpu;\r\nif (!tlb_ubc->flush_required)\r\nreturn;\r\ncpu = get_cpu();\r\nif (cpumask_test_cpu(cpu, &tlb_ubc->cpumask)) {\r\ncount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\r\nlocal_flush_tlb();\r\ntrace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);\r\n}\r\nif (cpumask_any_but(&tlb_ubc->cpumask, cpu) < nr_cpu_ids)\r\nflush_tlb_others(&tlb_ubc->cpumask, NULL, 0, TLB_FLUSH_ALL);\r\ncpumask_clear(&tlb_ubc->cpumask);\r\ntlb_ubc->flush_required = false;\r\ntlb_ubc->writable = false;\r\nput_cpu();\r\n}\r\nvoid try_to_unmap_flush_dirty(void)\r\n{\r\nstruct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;\r\nif (tlb_ubc->writable)\r\ntry_to_unmap_flush();\r\n}\r\nstatic void set_tlb_ubc_flush_pending(struct mm_struct *mm,\r\nstruct page *page, bool writable)\r\n{\r\nstruct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;\r\ncpumask_or(&tlb_ubc->cpumask, &tlb_ubc->cpumask, mm_cpumask(mm));\r\ntlb_ubc->flush_required = true;\r\nif (writable)\r\ntlb_ubc->writable = true;\r\n}\r\nstatic bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)\r\n{\r\nbool should_defer = false;\r\nif (!(flags & TTU_BATCH_FLUSH))\r\nreturn false;\r\nif (cpumask_any_but(mm_cpumask(mm), get_cpu()) < nr_cpu_ids)\r\nshould_defer = true;\r\nput_cpu();\r\nreturn should_defer;\r\n}\r\nstatic void set_tlb_ubc_flush_pending(struct mm_struct *mm,\r\nstruct page *page, bool writable)\r\n{\r\n}\r\nstatic bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)\r\n{\r\nreturn false;\r\n}\r\nunsigned long page_address_in_vma(struct page *page, struct vm_area_struct *vma)\r\n{\r\nunsigned long address;\r\nif (PageAnon(page)) {\r\nstruct anon_vma *page__anon_vma = page_anon_vma(page);\r\nif (!vma->anon_vma || !page__anon_vma ||\r\nvma->anon_vma->root != page__anon_vma->root)\r\nreturn -EFAULT;\r\n} else if (page->mapping) {\r\nif (!vma->vm_file || vma->vm_file->f_mapping != page->mapping)\r\nreturn -EFAULT;\r\n} else\r\nreturn -EFAULT;\r\naddress = __vma_address(page, vma);\r\nif (unlikely(address < vma->vm_start || address >= vma->vm_end))\r\nreturn -EFAULT;\r\nreturn address;\r\n}\r\npmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd = NULL;\r\npmd_t pmde;\r\npgd = pgd_offset(mm, address);\r\nif (!pgd_present(*pgd))\r\ngoto out;\r\npud = pud_offset(pgd, address);\r\nif (!pud_present(*pud))\r\ngoto out;\r\npmd = pmd_offset(pud, address);\r\npmde = *pmd;\r\nbarrier();\r\nif (!pmd_present(pmde) || pmd_trans_huge(pmde))\r\npmd = NULL;\r\nout:\r\nreturn pmd;\r\n}\r\npte_t *__page_check_address(struct page *page, struct mm_struct *mm,\r\nunsigned long address, spinlock_t **ptlp, int sync)\r\n{\r\npmd_t *pmd;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\nif (unlikely(PageHuge(page))) {\r\npte = huge_pte_offset(mm, address);\r\nif (!pte)\r\nreturn NULL;\r\nptl = huge_pte_lockptr(page_hstate(page), mm, pte);\r\ngoto check;\r\n}\r\npmd = mm_find_pmd(mm, address);\r\nif (!pmd)\r\nreturn NULL;\r\npte = pte_offset_map(pmd, address);\r\nif (!sync && !pte_present(*pte)) {\r\npte_unmap(pte);\r\nreturn NULL;\r\n}\r\nptl = pte_lockptr(mm, pmd);\r\ncheck:\r\nspin_lock(ptl);\r\nif (pte_present(*pte) && page_to_pfn(page) == pte_pfn(*pte)) {\r\n*ptlp = ptl;\r\nreturn pte;\r\n}\r\npte_unmap_unlock(pte, ptl);\r\nreturn NULL;\r\n}\r\nint page_mapped_in_vma(struct page *page, struct vm_area_struct *vma)\r\n{\r\nunsigned long address;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\naddress = __vma_address(page, vma);\r\nif (unlikely(address < vma->vm_start || address >= vma->vm_end))\r\nreturn 0;\r\npte = page_check_address(page, vma->vm_mm, address, &ptl, 1);\r\nif (!pte)\r\nreturn 0;\r\npte_unmap_unlock(pte, ptl);\r\nreturn 1;\r\n}\r\nbool page_check_address_transhuge(struct page *page, struct mm_struct *mm,\r\nunsigned long address, pmd_t **pmdp,\r\npte_t **ptep, spinlock_t **ptlp)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\nif (unlikely(PageHuge(page))) {\r\npte = huge_pte_offset(mm, address);\r\nif (!pte)\r\nreturn false;\r\nptl = huge_pte_lockptr(page_hstate(page), mm, pte);\r\npmd = NULL;\r\ngoto check_pte;\r\n}\r\npgd = pgd_offset(mm, address);\r\nif (!pgd_present(*pgd))\r\nreturn false;\r\npud = pud_offset(pgd, address);\r\nif (!pud_present(*pud))\r\nreturn false;\r\npmd = pmd_offset(pud, address);\r\nif (pmd_trans_huge(*pmd)) {\r\nptl = pmd_lock(mm, pmd);\r\nif (!pmd_present(*pmd))\r\ngoto unlock_pmd;\r\nif (unlikely(!pmd_trans_huge(*pmd))) {\r\nspin_unlock(ptl);\r\ngoto map_pte;\r\n}\r\nif (pmd_page(*pmd) != page)\r\ngoto unlock_pmd;\r\npte = NULL;\r\ngoto found;\r\nunlock_pmd:\r\nspin_unlock(ptl);\r\nreturn false;\r\n} else {\r\npmd_t pmde = *pmd;\r\nbarrier();\r\nif (!pmd_present(pmde) || pmd_trans_huge(pmde))\r\nreturn false;\r\n}\r\nmap_pte:\r\npte = pte_offset_map(pmd, address);\r\nif (!pte_present(*pte)) {\r\npte_unmap(pte);\r\nreturn false;\r\n}\r\nptl = pte_lockptr(mm, pmd);\r\ncheck_pte:\r\nspin_lock(ptl);\r\nif (!pte_present(*pte)) {\r\npte_unmap_unlock(pte, ptl);\r\nreturn false;\r\n}\r\nif (pte_pfn(*pte) - page_to_pfn(page) >= hpage_nr_pages(page)) {\r\npte_unmap_unlock(pte, ptl);\r\nreturn false;\r\n}\r\nfound:\r\n*ptep = pte;\r\n*pmdp = pmd;\r\n*ptlp = ptl;\r\nreturn true;\r\n}\r\nstatic int page_referenced_one(struct page *page, struct vm_area_struct *vma,\r\nunsigned long address, void *arg)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nstruct page_referenced_arg *pra = arg;\r\npmd_t *pmd;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\nint referenced = 0;\r\nif (!page_check_address_transhuge(page, mm, address, &pmd, &pte, &ptl))\r\nreturn SWAP_AGAIN;\r\nif (vma->vm_flags & VM_LOCKED) {\r\nif (pte)\r\npte_unmap(pte);\r\nspin_unlock(ptl);\r\npra->vm_flags |= VM_LOCKED;\r\nreturn SWAP_FAIL;\r\n}\r\nif (pte) {\r\nif (ptep_clear_flush_young_notify(vma, address, pte)) {\r\nif (likely(!(vma->vm_flags & VM_SEQ_READ)))\r\nreferenced++;\r\n}\r\npte_unmap(pte);\r\n} else if (IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE)) {\r\nif (pmdp_clear_flush_young_notify(vma, address, pmd))\r\nreferenced++;\r\n} else {\r\nWARN_ON_ONCE(1);\r\n}\r\nspin_unlock(ptl);\r\nif (referenced)\r\nclear_page_idle(page);\r\nif (test_and_clear_page_young(page))\r\nreferenced++;\r\nif (referenced) {\r\npra->referenced++;\r\npra->vm_flags |= vma->vm_flags;\r\n}\r\npra->mapcount--;\r\nif (!pra->mapcount)\r\nreturn SWAP_SUCCESS;\r\nreturn SWAP_AGAIN;\r\n}\r\nstatic bool invalid_page_referenced_vma(struct vm_area_struct *vma, void *arg)\r\n{\r\nstruct page_referenced_arg *pra = arg;\r\nstruct mem_cgroup *memcg = pra->memcg;\r\nif (!mm_match_cgroup(vma->vm_mm, memcg))\r\nreturn true;\r\nreturn false;\r\n}\r\nint page_referenced(struct page *page,\r\nint is_locked,\r\nstruct mem_cgroup *memcg,\r\nunsigned long *vm_flags)\r\n{\r\nint ret;\r\nint we_locked = 0;\r\nstruct page_referenced_arg pra = {\r\n.mapcount = total_mapcount(page),\r\n.memcg = memcg,\r\n};\r\nstruct rmap_walk_control rwc = {\r\n.rmap_one = page_referenced_one,\r\n.arg = (void *)&pra,\r\n.anon_lock = page_lock_anon_vma_read,\r\n};\r\n*vm_flags = 0;\r\nif (!page_mapped(page))\r\nreturn 0;\r\nif (!page_rmapping(page))\r\nreturn 0;\r\nif (!is_locked && (!PageAnon(page) || PageKsm(page))) {\r\nwe_locked = trylock_page(page);\r\nif (!we_locked)\r\nreturn 1;\r\n}\r\nif (memcg) {\r\nrwc.invalid_vma = invalid_page_referenced_vma;\r\n}\r\nret = rmap_walk(page, &rwc);\r\n*vm_flags = pra.vm_flags;\r\nif (we_locked)\r\nunlock_page(page);\r\nreturn pra.referenced;\r\n}\r\nstatic int page_mkclean_one(struct page *page, struct vm_area_struct *vma,\r\nunsigned long address, void *arg)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\nint ret = 0;\r\nint *cleaned = arg;\r\npte = page_check_address(page, mm, address, &ptl, 1);\r\nif (!pte)\r\ngoto out;\r\nif (pte_dirty(*pte) || pte_write(*pte)) {\r\npte_t entry;\r\nflush_cache_page(vma, address, pte_pfn(*pte));\r\nentry = ptep_clear_flush(vma, address, pte);\r\nentry = pte_wrprotect(entry);\r\nentry = pte_mkclean(entry);\r\nset_pte_at(mm, address, pte, entry);\r\nret = 1;\r\n}\r\npte_unmap_unlock(pte, ptl);\r\nif (ret) {\r\nmmu_notifier_invalidate_page(mm, address);\r\n(*cleaned)++;\r\n}\r\nout:\r\nreturn SWAP_AGAIN;\r\n}\r\nstatic bool invalid_mkclean_vma(struct vm_area_struct *vma, void *arg)\r\n{\r\nif (vma->vm_flags & VM_SHARED)\r\nreturn false;\r\nreturn true;\r\n}\r\nint page_mkclean(struct page *page)\r\n{\r\nint cleaned = 0;\r\nstruct address_space *mapping;\r\nstruct rmap_walk_control rwc = {\r\n.arg = (void *)&cleaned,\r\n.rmap_one = page_mkclean_one,\r\n.invalid_vma = invalid_mkclean_vma,\r\n};\r\nBUG_ON(!PageLocked(page));\r\nif (!page_mapped(page))\r\nreturn 0;\r\nmapping = page_mapping(page);\r\nif (!mapping)\r\nreturn 0;\r\nrmap_walk(page, &rwc);\r\nreturn cleaned;\r\n}\r\nvoid page_move_anon_rmap(struct page *page, struct vm_area_struct *vma)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\npage = compound_head(page);\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nVM_BUG_ON_VMA(!anon_vma, vma);\r\nanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\r\nWRITE_ONCE(page->mapping, (struct address_space *) anon_vma);\r\n}\r\nstatic void __page_set_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address, int exclusive)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\nBUG_ON(!anon_vma);\r\nif (PageAnon(page))\r\nreturn;\r\nif (!exclusive)\r\nanon_vma = anon_vma->root;\r\nanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\r\npage->mapping = (struct address_space *) anon_vma;\r\npage->index = linear_page_index(vma, address);\r\n}\r\nstatic void __page_check_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\n#ifdef CONFIG_DEBUG_VM\r\nBUG_ON(page_anon_vma(page)->root != vma->anon_vma->root);\r\nBUG_ON(page_to_pgoff(page) != linear_page_index(vma, address));\r\n#endif\r\n}\r\nvoid page_add_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address, bool compound)\r\n{\r\ndo_page_add_anon_rmap(page, vma, address, compound ? RMAP_COMPOUND : 0);\r\n}\r\nvoid do_page_add_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address, int flags)\r\n{\r\nbool compound = flags & RMAP_COMPOUND;\r\nbool first;\r\nif (compound) {\r\natomic_t *mapcount;\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nVM_BUG_ON_PAGE(!PageTransHuge(page), page);\r\nmapcount = compound_mapcount_ptr(page);\r\nfirst = atomic_inc_and_test(mapcount);\r\n} else {\r\nfirst = atomic_inc_and_test(&page->_mapcount);\r\n}\r\nif (first) {\r\nint nr = compound ? hpage_nr_pages(page) : 1;\r\nif (compound)\r\n__inc_node_page_state(page, NR_ANON_THPS);\r\n__mod_node_page_state(page_pgdat(page), NR_ANON_MAPPED, nr);\r\n}\r\nif (unlikely(PageKsm(page)))\r\nreturn;\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nif (first)\r\n__page_set_anon_rmap(page, vma, address,\r\nflags & RMAP_EXCLUSIVE);\r\nelse\r\n__page_check_anon_rmap(page, vma, address);\r\n}\r\nvoid page_add_new_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address, bool compound)\r\n{\r\nint nr = compound ? hpage_nr_pages(page) : 1;\r\nVM_BUG_ON_VMA(address < vma->vm_start || address >= vma->vm_end, vma);\r\n__SetPageSwapBacked(page);\r\nif (compound) {\r\nVM_BUG_ON_PAGE(!PageTransHuge(page), page);\r\natomic_set(compound_mapcount_ptr(page), 0);\r\n__inc_node_page_state(page, NR_ANON_THPS);\r\n} else {\r\nVM_BUG_ON_PAGE(PageTransCompound(page), page);\r\natomic_set(&page->_mapcount, 0);\r\n}\r\n__mod_node_page_state(page_pgdat(page), NR_ANON_MAPPED, nr);\r\n__page_set_anon_rmap(page, vma, address, 1);\r\n}\r\nvoid page_add_file_rmap(struct page *page, bool compound)\r\n{\r\nint i, nr = 1;\r\nVM_BUG_ON_PAGE(compound && !PageTransHuge(page), page);\r\nlock_page_memcg(page);\r\nif (compound && PageTransHuge(page)) {\r\nfor (i = 0, nr = 0; i < HPAGE_PMD_NR; i++) {\r\nif (atomic_inc_and_test(&page[i]._mapcount))\r\nnr++;\r\n}\r\nif (!atomic_inc_and_test(compound_mapcount_ptr(page)))\r\ngoto out;\r\nVM_BUG_ON_PAGE(!PageSwapBacked(page), page);\r\n__inc_node_page_state(page, NR_SHMEM_PMDMAPPED);\r\n} else {\r\nif (PageTransCompound(page) && page_mapping(page)) {\r\nVM_WARN_ON_ONCE(!PageLocked(page));\r\nSetPageDoubleMap(compound_head(page));\r\nif (PageMlocked(page))\r\nclear_page_mlock(compound_head(page));\r\n}\r\nif (!atomic_inc_and_test(&page->_mapcount))\r\ngoto out;\r\n}\r\n__mod_node_page_state(page_pgdat(page), NR_FILE_MAPPED, nr);\r\nmem_cgroup_inc_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED);\r\nout:\r\nunlock_page_memcg(page);\r\n}\r\nstatic void page_remove_file_rmap(struct page *page, bool compound)\r\n{\r\nint i, nr = 1;\r\nVM_BUG_ON_PAGE(compound && !PageHead(page), page);\r\nlock_page_memcg(page);\r\nif (unlikely(PageHuge(page))) {\r\natomic_dec(compound_mapcount_ptr(page));\r\ngoto out;\r\n}\r\nif (compound && PageTransHuge(page)) {\r\nfor (i = 0, nr = 0; i < HPAGE_PMD_NR; i++) {\r\nif (atomic_add_negative(-1, &page[i]._mapcount))\r\nnr++;\r\n}\r\nif (!atomic_add_negative(-1, compound_mapcount_ptr(page)))\r\ngoto out;\r\nVM_BUG_ON_PAGE(!PageSwapBacked(page), page);\r\n__dec_node_page_state(page, NR_SHMEM_PMDMAPPED);\r\n} else {\r\nif (!atomic_add_negative(-1, &page->_mapcount))\r\ngoto out;\r\n}\r\n__mod_node_page_state(page_pgdat(page), NR_FILE_MAPPED, -nr);\r\nmem_cgroup_dec_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED);\r\nif (unlikely(PageMlocked(page)))\r\nclear_page_mlock(page);\r\nout:\r\nunlock_page_memcg(page);\r\n}\r\nstatic void page_remove_anon_compound_rmap(struct page *page)\r\n{\r\nint i, nr;\r\nif (!atomic_add_negative(-1, compound_mapcount_ptr(page)))\r\nreturn;\r\nif (unlikely(PageHuge(page)))\r\nreturn;\r\nif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE))\r\nreturn;\r\n__dec_node_page_state(page, NR_ANON_THPS);\r\nif (TestClearPageDoubleMap(page)) {\r\nfor (i = 0, nr = 0; i < HPAGE_PMD_NR; i++) {\r\nif (atomic_add_negative(-1, &page[i]._mapcount))\r\nnr++;\r\n}\r\n} else {\r\nnr = HPAGE_PMD_NR;\r\n}\r\nif (unlikely(PageMlocked(page)))\r\nclear_page_mlock(page);\r\nif (nr) {\r\n__mod_node_page_state(page_pgdat(page), NR_ANON_MAPPED, -nr);\r\ndeferred_split_huge_page(page);\r\n}\r\n}\r\nvoid page_remove_rmap(struct page *page, bool compound)\r\n{\r\nif (!PageAnon(page))\r\nreturn page_remove_file_rmap(page, compound);\r\nif (compound)\r\nreturn page_remove_anon_compound_rmap(page);\r\nif (!atomic_add_negative(-1, &page->_mapcount))\r\nreturn;\r\n__dec_node_page_state(page, NR_ANON_MAPPED);\r\nif (unlikely(PageMlocked(page)))\r\nclear_page_mlock(page);\r\nif (PageTransCompound(page))\r\ndeferred_split_huge_page(compound_head(page));\r\n}\r\nstatic int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,\r\nunsigned long address, void *arg)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npte_t *pte;\r\npte_t pteval;\r\nspinlock_t *ptl;\r\nint ret = SWAP_AGAIN;\r\nstruct rmap_private *rp = arg;\r\nenum ttu_flags flags = rp->flags;\r\nif ((flags & TTU_MUNLOCK) && !(vma->vm_flags & VM_LOCKED))\r\ngoto out;\r\nif (flags & TTU_SPLIT_HUGE_PMD) {\r\nsplit_huge_pmd_address(vma, address,\r\nflags & TTU_MIGRATION, page);\r\nif (page_mapcount(page) == 0)\r\ngoto out;\r\n}\r\npte = page_check_address(page, mm, address, &ptl,\r\nPageTransCompound(page));\r\nif (!pte)\r\ngoto out;\r\nif (!(flags & TTU_IGNORE_MLOCK)) {\r\nif (vma->vm_flags & VM_LOCKED) {\r\nif (!PageTransCompound(page)) {\r\nmlock_vma_page(page);\r\n}\r\nret = SWAP_MLOCK;\r\ngoto out_unmap;\r\n}\r\nif (flags & TTU_MUNLOCK)\r\ngoto out_unmap;\r\n}\r\nif (!(flags & TTU_IGNORE_ACCESS)) {\r\nif (ptep_clear_flush_young_notify(vma, address, pte)) {\r\nret = SWAP_FAIL;\r\ngoto out_unmap;\r\n}\r\n}\r\nflush_cache_page(vma, address, page_to_pfn(page));\r\nif (should_defer_flush(mm, flags)) {\r\npteval = ptep_get_and_clear(mm, address, pte);\r\nset_tlb_ubc_flush_pending(mm, page, pte_dirty(pteval));\r\n} else {\r\npteval = ptep_clear_flush(vma, address, pte);\r\n}\r\nif (pte_dirty(pteval))\r\nset_page_dirty(page);\r\nupdate_hiwater_rss(mm);\r\nif (PageHWPoison(page) && !(flags & TTU_IGNORE_HWPOISON)) {\r\nif (PageHuge(page)) {\r\nhugetlb_count_sub(1 << compound_order(page), mm);\r\n} else {\r\ndec_mm_counter(mm, mm_counter(page));\r\n}\r\nset_pte_at(mm, address, pte,\r\nswp_entry_to_pte(make_hwpoison_entry(page)));\r\n} else if (pte_unused(pteval)) {\r\ndec_mm_counter(mm, mm_counter(page));\r\n} else if (IS_ENABLED(CONFIG_MIGRATION) && (flags & TTU_MIGRATION)) {\r\nswp_entry_t entry;\r\npte_t swp_pte;\r\nentry = make_migration_entry(page, pte_write(pteval));\r\nswp_pte = swp_entry_to_pte(entry);\r\nif (pte_soft_dirty(pteval))\r\nswp_pte = pte_swp_mksoft_dirty(swp_pte);\r\nset_pte_at(mm, address, pte, swp_pte);\r\n} else if (PageAnon(page)) {\r\nswp_entry_t entry = { .val = page_private(page) };\r\npte_t swp_pte;\r\nVM_BUG_ON_PAGE(!PageSwapCache(page), page);\r\nif (!PageDirty(page) && (flags & TTU_LZFREE)) {\r\ndec_mm_counter(mm, MM_ANONPAGES);\r\nrp->lazyfreed++;\r\ngoto discard;\r\n}\r\nif (swap_duplicate(entry) < 0) {\r\nset_pte_at(mm, address, pte, pteval);\r\nret = SWAP_FAIL;\r\ngoto out_unmap;\r\n}\r\nif (list_empty(&mm->mmlist)) {\r\nspin_lock(&mmlist_lock);\r\nif (list_empty(&mm->mmlist))\r\nlist_add(&mm->mmlist, &init_mm.mmlist);\r\nspin_unlock(&mmlist_lock);\r\n}\r\ndec_mm_counter(mm, MM_ANONPAGES);\r\ninc_mm_counter(mm, MM_SWAPENTS);\r\nswp_pte = swp_entry_to_pte(entry);\r\nif (pte_soft_dirty(pteval))\r\nswp_pte = pte_swp_mksoft_dirty(swp_pte);\r\nset_pte_at(mm, address, pte, swp_pte);\r\n} else\r\ndec_mm_counter(mm, mm_counter_file(page));\r\ndiscard:\r\npage_remove_rmap(page, PageHuge(page));\r\nput_page(page);\r\nout_unmap:\r\npte_unmap_unlock(pte, ptl);\r\nif (ret != SWAP_FAIL && ret != SWAP_MLOCK && !(flags & TTU_MUNLOCK))\r\nmmu_notifier_invalidate_page(mm, address);\r\nout:\r\nreturn ret;\r\n}\r\nbool is_vma_temporary_stack(struct vm_area_struct *vma)\r\n{\r\nint maybe_stack = vma->vm_flags & (VM_GROWSDOWN | VM_GROWSUP);\r\nif (!maybe_stack)\r\nreturn false;\r\nif ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==\r\nVM_STACK_INCOMPLETE_SETUP)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool invalid_migration_vma(struct vm_area_struct *vma, void *arg)\r\n{\r\nreturn is_vma_temporary_stack(vma);\r\n}\r\nstatic int page_mapcount_is_zero(struct page *page)\r\n{\r\nreturn !page_mapcount(page);\r\n}\r\nint try_to_unmap(struct page *page, enum ttu_flags flags)\r\n{\r\nint ret;\r\nstruct rmap_private rp = {\r\n.flags = flags,\r\n.lazyfreed = 0,\r\n};\r\nstruct rmap_walk_control rwc = {\r\n.rmap_one = try_to_unmap_one,\r\n.arg = &rp,\r\n.done = page_mapcount_is_zero,\r\n.anon_lock = page_lock_anon_vma_read,\r\n};\r\nif ((flags & TTU_MIGRATION) && !PageKsm(page) && PageAnon(page))\r\nrwc.invalid_vma = invalid_migration_vma;\r\nif (flags & TTU_RMAP_LOCKED)\r\nret = rmap_walk_locked(page, &rwc);\r\nelse\r\nret = rmap_walk(page, &rwc);\r\nif (ret != SWAP_MLOCK && !page_mapcount(page)) {\r\nret = SWAP_SUCCESS;\r\nif (rp.lazyfreed && !PageDirty(page))\r\nret = SWAP_LZFREE;\r\n}\r\nreturn ret;\r\n}\r\nstatic int page_not_mapped(struct page *page)\r\n{\r\nreturn !page_mapped(page);\r\n}\r\nint try_to_munlock(struct page *page)\r\n{\r\nint ret;\r\nstruct rmap_private rp = {\r\n.flags = TTU_MUNLOCK,\r\n.lazyfreed = 0,\r\n};\r\nstruct rmap_walk_control rwc = {\r\n.rmap_one = try_to_unmap_one,\r\n.arg = &rp,\r\n.done = page_not_mapped,\r\n.anon_lock = page_lock_anon_vma_read,\r\n};\r\nVM_BUG_ON_PAGE(!PageLocked(page) || PageLRU(page), page);\r\nret = rmap_walk(page, &rwc);\r\nreturn ret;\r\n}\r\nvoid __put_anon_vma(struct anon_vma *anon_vma)\r\n{\r\nstruct anon_vma *root = anon_vma->root;\r\nanon_vma_free(anon_vma);\r\nif (root != anon_vma && atomic_dec_and_test(&root->refcount))\r\nanon_vma_free(root);\r\n}\r\nstatic struct anon_vma *rmap_walk_anon_lock(struct page *page,\r\nstruct rmap_walk_control *rwc)\r\n{\r\nstruct anon_vma *anon_vma;\r\nif (rwc->anon_lock)\r\nreturn rwc->anon_lock(page);\r\nanon_vma = page_anon_vma(page);\r\nif (!anon_vma)\r\nreturn NULL;\r\nanon_vma_lock_read(anon_vma);\r\nreturn anon_vma;\r\n}\r\nstatic int rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc,\r\nbool locked)\r\n{\r\nstruct anon_vma *anon_vma;\r\npgoff_t pgoff;\r\nstruct anon_vma_chain *avc;\r\nint ret = SWAP_AGAIN;\r\nif (locked) {\r\nanon_vma = page_anon_vma(page);\r\nVM_BUG_ON_PAGE(!anon_vma, page);\r\n} else {\r\nanon_vma = rmap_walk_anon_lock(page, rwc);\r\n}\r\nif (!anon_vma)\r\nreturn ret;\r\npgoff = page_to_pgoff(page);\r\nanon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {\r\nstruct vm_area_struct *vma = avc->vma;\r\nunsigned long address = vma_address(page, vma);\r\ncond_resched();\r\nif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))\r\ncontinue;\r\nret = rwc->rmap_one(page, vma, address, rwc->arg);\r\nif (ret != SWAP_AGAIN)\r\nbreak;\r\nif (rwc->done && rwc->done(page))\r\nbreak;\r\n}\r\nif (!locked)\r\nanon_vma_unlock_read(anon_vma);\r\nreturn ret;\r\n}\r\nstatic int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc,\r\nbool locked)\r\n{\r\nstruct address_space *mapping = page_mapping(page);\r\npgoff_t pgoff;\r\nstruct vm_area_struct *vma;\r\nint ret = SWAP_AGAIN;\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nif (!mapping)\r\nreturn ret;\r\npgoff = page_to_pgoff(page);\r\nif (!locked)\r\ni_mmap_lock_read(mapping);\r\nvma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {\r\nunsigned long address = vma_address(page, vma);\r\ncond_resched();\r\nif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))\r\ncontinue;\r\nret = rwc->rmap_one(page, vma, address, rwc->arg);\r\nif (ret != SWAP_AGAIN)\r\ngoto done;\r\nif (rwc->done && rwc->done(page))\r\ngoto done;\r\n}\r\ndone:\r\nif (!locked)\r\ni_mmap_unlock_read(mapping);\r\nreturn ret;\r\n}\r\nint rmap_walk(struct page *page, struct rmap_walk_control *rwc)\r\n{\r\nif (unlikely(PageKsm(page)))\r\nreturn rmap_walk_ksm(page, rwc);\r\nelse if (PageAnon(page))\r\nreturn rmap_walk_anon(page, rwc, false);\r\nelse\r\nreturn rmap_walk_file(page, rwc, false);\r\n}\r\nint rmap_walk_locked(struct page *page, struct rmap_walk_control *rwc)\r\n{\r\nVM_BUG_ON_PAGE(PageKsm(page), page);\r\nif (PageAnon(page))\r\nreturn rmap_walk_anon(page, rwc, true);\r\nelse\r\nreturn rmap_walk_file(page, rwc, true);\r\n}\r\nstatic void __hugepage_set_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address, int exclusive)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\nBUG_ON(!anon_vma);\r\nif (PageAnon(page))\r\nreturn;\r\nif (!exclusive)\r\nanon_vma = anon_vma->root;\r\nanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\r\npage->mapping = (struct address_space *) anon_vma;\r\npage->index = linear_page_index(vma, address);\r\n}\r\nvoid hugepage_add_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\nstruct anon_vma *anon_vma = vma->anon_vma;\r\nint first;\r\nBUG_ON(!PageLocked(page));\r\nBUG_ON(!anon_vma);\r\nfirst = atomic_inc_and_test(compound_mapcount_ptr(page));\r\nif (first)\r\n__hugepage_set_anon_rmap(page, vma, address, 0);\r\n}\r\nvoid hugepage_add_new_anon_rmap(struct page *page,\r\nstruct vm_area_struct *vma, unsigned long address)\r\n{\r\nBUG_ON(address < vma->vm_start || address >= vma->vm_end);\r\natomic_set(compound_mapcount_ptr(page), 0);\r\n__hugepage_set_anon_rmap(page, vma, address, 1);\r\n}
