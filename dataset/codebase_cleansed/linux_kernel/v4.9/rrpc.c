static void rrpc_page_invalidate(struct rrpc *rrpc, struct rrpc_addr *a)\r\n{\r\nstruct rrpc_block *rblk = a->rblk;\r\nunsigned int pg_offset;\r\nlockdep_assert_held(&rrpc->rev_lock);\r\nif (a->addr == ADDR_EMPTY || !rblk)\r\nreturn;\r\nspin_lock(&rblk->lock);\r\ndiv_u64_rem(a->addr, rrpc->dev->sec_per_blk, &pg_offset);\r\nWARN_ON(test_and_set_bit(pg_offset, rblk->invalid_pages));\r\nrblk->nr_invalid_pages++;\r\nspin_unlock(&rblk->lock);\r\nrrpc->rev_trans_map[a->addr - rrpc->poffset].addr = ADDR_EMPTY;\r\n}\r\nstatic void rrpc_invalidate_range(struct rrpc *rrpc, sector_t slba,\r\nunsigned int len)\r\n{\r\nsector_t i;\r\nspin_lock(&rrpc->rev_lock);\r\nfor (i = slba; i < slba + len; i++) {\r\nstruct rrpc_addr *gp = &rrpc->trans_map[i];\r\nrrpc_page_invalidate(rrpc, gp);\r\ngp->rblk = NULL;\r\n}\r\nspin_unlock(&rrpc->rev_lock);\r\n}\r\nstatic struct nvm_rq *rrpc_inflight_laddr_acquire(struct rrpc *rrpc,\r\nsector_t laddr, unsigned int pages)\r\n{\r\nstruct nvm_rq *rqd;\r\nstruct rrpc_inflight_rq *inf;\r\nrqd = mempool_alloc(rrpc->rq_pool, GFP_ATOMIC);\r\nif (!rqd)\r\nreturn ERR_PTR(-ENOMEM);\r\ninf = rrpc_get_inflight_rq(rqd);\r\nif (rrpc_lock_laddr(rrpc, laddr, pages, inf)) {\r\nmempool_free(rqd, rrpc->rq_pool);\r\nreturn NULL;\r\n}\r\nreturn rqd;\r\n}\r\nstatic void rrpc_inflight_laddr_release(struct rrpc *rrpc, struct nvm_rq *rqd)\r\n{\r\nstruct rrpc_inflight_rq *inf = rrpc_get_inflight_rq(rqd);\r\nrrpc_unlock_laddr(rrpc, inf);\r\nmempool_free(rqd, rrpc->rq_pool);\r\n}\r\nstatic void rrpc_discard(struct rrpc *rrpc, struct bio *bio)\r\n{\r\nsector_t slba = bio->bi_iter.bi_sector / NR_PHY_IN_LOG;\r\nsector_t len = bio->bi_iter.bi_size / RRPC_EXPOSED_PAGE_SIZE;\r\nstruct nvm_rq *rqd;\r\nwhile (1) {\r\nrqd = rrpc_inflight_laddr_acquire(rrpc, slba, len);\r\nif (rqd)\r\nbreak;\r\nschedule();\r\n}\r\nif (IS_ERR(rqd)) {\r\npr_err("rrpc: unable to acquire inflight IO\n");\r\nbio_io_error(bio);\r\nreturn;\r\n}\r\nrrpc_invalidate_range(rrpc, slba, len);\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\n}\r\nstatic int block_is_full(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nreturn (rblk->next_page == rrpc->dev->sec_per_blk);\r\n}\r\nstatic u64 block_to_rel_addr(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct nvm_block *blk = rblk->parent;\r\nint lun_blk = blk->id % (rrpc->dev->blks_per_lun * rrpc->nr_luns);\r\nreturn lun_blk * rrpc->dev->sec_per_blk;\r\n}\r\nstatic u64 block_to_addr(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct nvm_block *blk = rblk->parent;\r\nreturn blk->id * rrpc->dev->sec_per_blk;\r\n}\r\nstatic struct ppa_addr linear_to_generic_addr(struct nvm_dev *dev,\r\nstruct ppa_addr r)\r\n{\r\nstruct ppa_addr l;\r\nint secs, pgs, blks, luns;\r\nsector_t ppa = r.ppa;\r\nl.ppa = 0;\r\ndiv_u64_rem(ppa, dev->sec_per_pg, &secs);\r\nl.g.sec = secs;\r\nsector_div(ppa, dev->sec_per_pg);\r\ndiv_u64_rem(ppa, dev->pgs_per_blk, &pgs);\r\nl.g.pg = pgs;\r\nsector_div(ppa, dev->pgs_per_blk);\r\ndiv_u64_rem(ppa, dev->blks_per_lun, &blks);\r\nl.g.blk = blks;\r\nsector_div(ppa, dev->blks_per_lun);\r\ndiv_u64_rem(ppa, dev->luns_per_chnl, &luns);\r\nl.g.lun = luns;\r\nsector_div(ppa, dev->luns_per_chnl);\r\nl.g.ch = ppa;\r\nreturn l;\r\n}\r\nstatic struct ppa_addr rrpc_ppa_to_gaddr(struct nvm_dev *dev, u64 addr)\r\n{\r\nstruct ppa_addr paddr;\r\npaddr.ppa = addr;\r\nreturn linear_to_generic_addr(dev, paddr);\r\n}\r\nstatic void rrpc_set_lun_cur(struct rrpc_lun *rlun, struct rrpc_block *new_rblk,\r\nstruct rrpc_block **cur_rblk)\r\n{\r\nstruct rrpc *rrpc = rlun->rrpc;\r\nif (*cur_rblk) {\r\nspin_lock(&(*cur_rblk)->lock);\r\nWARN_ON(!block_is_full(rrpc, *cur_rblk));\r\nspin_unlock(&(*cur_rblk)->lock);\r\n}\r\n*cur_rblk = new_rblk;\r\n}\r\nstatic struct rrpc_block *rrpc_get_blk(struct rrpc *rrpc, struct rrpc_lun *rlun,\r\nunsigned long flags)\r\n{\r\nstruct nvm_block *blk;\r\nstruct rrpc_block *rblk;\r\nblk = nvm_get_blk(rrpc->dev, rlun->parent, flags);\r\nif (!blk) {\r\npr_err("nvm: rrpc: cannot get new block from media manager\n");\r\nreturn NULL;\r\n}\r\nrblk = rrpc_get_rblk(rlun, blk->id);\r\nblk->priv = rblk;\r\nbitmap_zero(rblk->invalid_pages, rrpc->dev->sec_per_blk);\r\nrblk->next_page = 0;\r\nrblk->nr_invalid_pages = 0;\r\natomic_set(&rblk->data_cmnt_size, 0);\r\nreturn rblk;\r\n}\r\nstatic void rrpc_put_blk(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nnvm_put_blk(rrpc->dev, rblk->parent);\r\n}\r\nstatic void rrpc_put_blks(struct rrpc *rrpc)\r\n{\r\nstruct rrpc_lun *rlun;\r\nint i;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nrlun = &rrpc->luns[i];\r\nif (rlun->cur)\r\nrrpc_put_blk(rrpc, rlun->cur);\r\nif (rlun->gc_cur)\r\nrrpc_put_blk(rrpc, rlun->gc_cur);\r\n}\r\n}\r\nstatic struct rrpc_lun *get_next_lun(struct rrpc *rrpc)\r\n{\r\nint next = atomic_inc_return(&rrpc->next_lun);\r\nreturn &rrpc->luns[next % rrpc->nr_luns];\r\n}\r\nstatic void rrpc_gc_kick(struct rrpc *rrpc)\r\n{\r\nstruct rrpc_lun *rlun;\r\nunsigned int i;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nrlun = &rrpc->luns[i];\r\nqueue_work(rrpc->krqd_wq, &rlun->ws_gc);\r\n}\r\n}\r\nstatic void rrpc_gc_timer(unsigned long data)\r\n{\r\nstruct rrpc *rrpc = (struct rrpc *)data;\r\nrrpc_gc_kick(rrpc);\r\nmod_timer(&rrpc->gc_timer, jiffies + msecs_to_jiffies(10));\r\n}\r\nstatic void rrpc_end_sync_bio(struct bio *bio)\r\n{\r\nstruct completion *waiting = bio->bi_private;\r\nif (bio->bi_error)\r\npr_err("nvm: gc request failed (%u).\n", bio->bi_error);\r\ncomplete(waiting);\r\n}\r\nstatic int rrpc_move_valid_pages(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct request_queue *q = rrpc->dev->q;\r\nstruct rrpc_rev_addr *rev;\r\nstruct nvm_rq *rqd;\r\nstruct bio *bio;\r\nstruct page *page;\r\nint slot;\r\nint nr_sec_per_blk = rrpc->dev->sec_per_blk;\r\nu64 phys_addr;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nif (bitmap_full(rblk->invalid_pages, nr_sec_per_blk))\r\nreturn 0;\r\nbio = bio_alloc(GFP_NOIO, 1);\r\nif (!bio) {\r\npr_err("nvm: could not alloc bio to gc\n");\r\nreturn -ENOMEM;\r\n}\r\npage = mempool_alloc(rrpc->page_pool, GFP_NOIO);\r\nif (!page) {\r\nbio_put(bio);\r\nreturn -ENOMEM;\r\n}\r\nwhile ((slot = find_first_zero_bit(rblk->invalid_pages,\r\nnr_sec_per_blk)) < nr_sec_per_blk) {\r\nphys_addr = rblk->parent->id * nr_sec_per_blk + slot;\r\ntry:\r\nspin_lock(&rrpc->rev_lock);\r\nrev = &rrpc->rev_trans_map[phys_addr - rrpc->poffset];\r\nif (rev->addr == ADDR_EMPTY) {\r\nspin_unlock(&rrpc->rev_lock);\r\ncontinue;\r\n}\r\nrqd = rrpc_inflight_laddr_acquire(rrpc, rev->addr, 1);\r\nif (IS_ERR_OR_NULL(rqd)) {\r\nspin_unlock(&rrpc->rev_lock);\r\nschedule();\r\ngoto try;\r\n}\r\nspin_unlock(&rrpc->rev_lock);\r\nbio->bi_iter.bi_sector = rrpc_get_sector(rev->addr);\r\nbio_set_op_attrs(bio, REQ_OP_READ, 0);\r\nbio->bi_private = &wait;\r\nbio->bi_end_io = rrpc_end_sync_bio;\r\nbio_add_pc_page(q, bio, page, RRPC_EXPOSED_PAGE_SIZE, 0);\r\nif (rrpc_submit_io(rrpc, bio, rqd, NVM_IOTYPE_GC)) {\r\npr_err("rrpc: gc read failed.\n");\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\ngoto finished;\r\n}\r\nwait_for_completion_io(&wait);\r\nif (bio->bi_error) {\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\ngoto finished;\r\n}\r\nbio_reset(bio);\r\nreinit_completion(&wait);\r\nbio->bi_iter.bi_sector = rrpc_get_sector(rev->addr);\r\nbio_set_op_attrs(bio, REQ_OP_WRITE, 0);\r\nbio->bi_private = &wait;\r\nbio->bi_end_io = rrpc_end_sync_bio;\r\nbio_add_pc_page(q, bio, page, RRPC_EXPOSED_PAGE_SIZE, 0);\r\nif (rrpc_submit_io(rrpc, bio, rqd, NVM_IOTYPE_GC)) {\r\npr_err("rrpc: gc write failed.\n");\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\ngoto finished;\r\n}\r\nwait_for_completion_io(&wait);\r\nrrpc_inflight_laddr_release(rrpc, rqd);\r\nif (bio->bi_error)\r\ngoto finished;\r\nbio_reset(bio);\r\n}\r\nfinished:\r\nmempool_free(page, rrpc->page_pool);\r\nbio_put(bio);\r\nif (!bitmap_full(rblk->invalid_pages, nr_sec_per_blk)) {\r\npr_err("nvm: failed to garbage collect block\n");\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic void rrpc_block_gc(struct work_struct *work)\r\n{\r\nstruct rrpc_block_gc *gcb = container_of(work, struct rrpc_block_gc,\r\nws_gc);\r\nstruct rrpc *rrpc = gcb->rrpc;\r\nstruct rrpc_block *rblk = gcb->rblk;\r\nstruct rrpc_lun *rlun = rblk->rlun;\r\nstruct nvm_dev *dev = rrpc->dev;\r\nmempool_free(gcb, rrpc->gcb_pool);\r\npr_debug("nvm: block '%lu' being reclaimed\n", rblk->parent->id);\r\nif (rrpc_move_valid_pages(rrpc, rblk))\r\ngoto put_back;\r\nif (nvm_erase_blk(dev, rblk->parent))\r\ngoto put_back;\r\nrrpc_put_blk(rrpc, rblk);\r\nreturn;\r\nput_back:\r\nspin_lock(&rlun->lock);\r\nlist_add_tail(&rblk->prio, &rlun->prio_list);\r\nspin_unlock(&rlun->lock);\r\n}\r\nstatic struct rrpc_block *rblock_max_invalid(struct rrpc_block *ra,\r\nstruct rrpc_block *rb)\r\n{\r\nif (ra->nr_invalid_pages == rb->nr_invalid_pages)\r\nreturn ra;\r\nreturn (ra->nr_invalid_pages < rb->nr_invalid_pages) ? rb : ra;\r\n}\r\nstatic struct rrpc_block *block_prio_find_max(struct rrpc_lun *rlun)\r\n{\r\nstruct list_head *prio_list = &rlun->prio_list;\r\nstruct rrpc_block *rblock, *max;\r\nBUG_ON(list_empty(prio_list));\r\nmax = list_first_entry(prio_list, struct rrpc_block, prio);\r\nlist_for_each_entry(rblock, prio_list, prio)\r\nmax = rblock_max_invalid(max, rblock);\r\nreturn max;\r\n}\r\nstatic void rrpc_lun_gc(struct work_struct *work)\r\n{\r\nstruct rrpc_lun *rlun = container_of(work, struct rrpc_lun, ws_gc);\r\nstruct rrpc *rrpc = rlun->rrpc;\r\nstruct nvm_lun *lun = rlun->parent;\r\nstruct rrpc_block_gc *gcb;\r\nunsigned int nr_blocks_need;\r\nnr_blocks_need = rrpc->dev->blks_per_lun / GC_LIMIT_INVERSE;\r\nif (nr_blocks_need < rrpc->nr_luns)\r\nnr_blocks_need = rrpc->nr_luns;\r\nspin_lock(&rlun->lock);\r\nwhile (nr_blocks_need > lun->nr_free_blocks &&\r\n!list_empty(&rlun->prio_list)) {\r\nstruct rrpc_block *rblock = block_prio_find_max(rlun);\r\nstruct nvm_block *block = rblock->parent;\r\nif (!rblock->nr_invalid_pages)\r\nbreak;\r\ngcb = mempool_alloc(rrpc->gcb_pool, GFP_ATOMIC);\r\nif (!gcb)\r\nbreak;\r\nlist_del_init(&rblock->prio);\r\nBUG_ON(!block_is_full(rrpc, rblock));\r\npr_debug("rrpc: selected block '%lu' for GC\n", block->id);\r\ngcb->rrpc = rrpc;\r\ngcb->rblk = rblock;\r\nINIT_WORK(&gcb->ws_gc, rrpc_block_gc);\r\nqueue_work(rrpc->kgc_wq, &gcb->ws_gc);\r\nnr_blocks_need--;\r\n}\r\nspin_unlock(&rlun->lock);\r\n}\r\nstatic void rrpc_gc_queue(struct work_struct *work)\r\n{\r\nstruct rrpc_block_gc *gcb = container_of(work, struct rrpc_block_gc,\r\nws_gc);\r\nstruct rrpc *rrpc = gcb->rrpc;\r\nstruct rrpc_block *rblk = gcb->rblk;\r\nstruct rrpc_lun *rlun = rblk->rlun;\r\nspin_lock(&rlun->lock);\r\nlist_add_tail(&rblk->prio, &rlun->prio_list);\r\nspin_unlock(&rlun->lock);\r\nmempool_free(gcb, rrpc->gcb_pool);\r\npr_debug("nvm: block '%lu' is full, allow GC (sched)\n",\r\nrblk->parent->id);\r\n}\r\nstatic struct rrpc_lun *rrpc_get_lun_rr(struct rrpc *rrpc, int is_gc)\r\n{\r\nunsigned int i;\r\nstruct rrpc_lun *rlun, *max_free;\r\nif (!is_gc)\r\nreturn get_next_lun(rrpc);\r\nmax_free = &rrpc->luns[0];\r\nrrpc_for_each_lun(rrpc, rlun, i) {\r\nif (rlun->parent->nr_free_blocks >\r\nmax_free->parent->nr_free_blocks)\r\nmax_free = rlun;\r\n}\r\nreturn max_free;\r\n}\r\nstatic struct rrpc_addr *rrpc_update_map(struct rrpc *rrpc, sector_t laddr,\r\nstruct rrpc_block *rblk, u64 paddr)\r\n{\r\nstruct rrpc_addr *gp;\r\nstruct rrpc_rev_addr *rev;\r\nBUG_ON(laddr >= rrpc->nr_sects);\r\ngp = &rrpc->trans_map[laddr];\r\nspin_lock(&rrpc->rev_lock);\r\nif (gp->rblk)\r\nrrpc_page_invalidate(rrpc, gp);\r\ngp->addr = paddr;\r\ngp->rblk = rblk;\r\nrev = &rrpc->rev_trans_map[gp->addr - rrpc->poffset];\r\nrev->addr = laddr;\r\nspin_unlock(&rrpc->rev_lock);\r\nreturn gp;\r\n}\r\nstatic u64 rrpc_alloc_addr(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nu64 addr = ADDR_EMPTY;\r\nspin_lock(&rblk->lock);\r\nif (block_is_full(rrpc, rblk))\r\ngoto out;\r\naddr = block_to_addr(rrpc, rblk) + rblk->next_page;\r\nrblk->next_page++;\r\nout:\r\nspin_unlock(&rblk->lock);\r\nreturn addr;\r\n}\r\nstatic struct rrpc_addr *rrpc_map_page(struct rrpc *rrpc, sector_t laddr,\r\nint is_gc)\r\n{\r\nstruct rrpc_lun *rlun;\r\nstruct rrpc_block *rblk, **cur_rblk;\r\nstruct nvm_lun *lun;\r\nu64 paddr;\r\nint gc_force = 0;\r\nrlun = rrpc_get_lun_rr(rrpc, is_gc);\r\nlun = rlun->parent;\r\nif (!is_gc && lun->nr_free_blocks < rrpc->nr_luns * 4)\r\nreturn NULL;\r\nspin_lock(&rlun->lock);\r\ncur_rblk = &rlun->cur;\r\nrblk = rlun->cur;\r\nretry:\r\npaddr = rrpc_alloc_addr(rrpc, rblk);\r\nif (paddr != ADDR_EMPTY)\r\ngoto done;\r\nif (!list_empty(&rlun->wblk_list)) {\r\nnew_blk:\r\nrblk = list_first_entry(&rlun->wblk_list, struct rrpc_block,\r\nprio);\r\nrrpc_set_lun_cur(rlun, rblk, cur_rblk);\r\nlist_del(&rblk->prio);\r\ngoto retry;\r\n}\r\nspin_unlock(&rlun->lock);\r\nrblk = rrpc_get_blk(rrpc, rlun, gc_force);\r\nif (rblk) {\r\nspin_lock(&rlun->lock);\r\nlist_add_tail(&rblk->prio, &rlun->wblk_list);\r\ngoto new_blk;\r\n}\r\nif (unlikely(is_gc) && !gc_force) {\r\ncur_rblk = &rlun->gc_cur;\r\nrblk = rlun->gc_cur;\r\ngc_force = 1;\r\nspin_lock(&rlun->lock);\r\ngoto retry;\r\n}\r\npr_err("rrpc: failed to allocate new block\n");\r\nreturn NULL;\r\ndone:\r\nspin_unlock(&rlun->lock);\r\nreturn rrpc_update_map(rrpc, laddr, rblk, paddr);\r\n}\r\nstatic void rrpc_run_gc(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct rrpc_block_gc *gcb;\r\ngcb = mempool_alloc(rrpc->gcb_pool, GFP_ATOMIC);\r\nif (!gcb) {\r\npr_err("rrpc: unable to queue block for gc.");\r\nreturn;\r\n}\r\ngcb->rrpc = rrpc;\r\ngcb->rblk = rblk;\r\nINIT_WORK(&gcb->ws_gc, rrpc_gc_queue);\r\nqueue_work(rrpc->kgc_wq, &gcb->ws_gc);\r\n}\r\nstatic void rrpc_end_io_write(struct rrpc *rrpc, struct rrpc_rq *rrqd,\r\nsector_t laddr, uint8_t npages)\r\n{\r\nstruct rrpc_addr *p;\r\nstruct rrpc_block *rblk;\r\nstruct nvm_lun *lun;\r\nint cmnt_size, i;\r\nfor (i = 0; i < npages; i++) {\r\np = &rrpc->trans_map[laddr + i];\r\nrblk = p->rblk;\r\nlun = rblk->parent->lun;\r\ncmnt_size = atomic_inc_return(&rblk->data_cmnt_size);\r\nif (unlikely(cmnt_size == rrpc->dev->sec_per_blk))\r\nrrpc_run_gc(rrpc, rblk);\r\n}\r\n}\r\nstatic void rrpc_end_io(struct nvm_rq *rqd)\r\n{\r\nstruct rrpc *rrpc = container_of(rqd->ins, struct rrpc, instance);\r\nstruct rrpc_rq *rrqd = nvm_rq_to_pdu(rqd);\r\nuint8_t npages = rqd->nr_ppas;\r\nsector_t laddr = rrpc_get_laddr(rqd->bio) - npages;\r\nif (bio_data_dir(rqd->bio) == WRITE)\r\nrrpc_end_io_write(rrpc, rrqd, laddr, npages);\r\nbio_put(rqd->bio);\r\nif (rrqd->flags & NVM_IOTYPE_GC)\r\nreturn;\r\nrrpc_unlock_rq(rrpc, rqd);\r\nif (npages > 1)\r\nnvm_dev_dma_free(rrpc->dev, rqd->ppa_list, rqd->dma_ppa_list);\r\nmempool_free(rqd, rrpc->rq_pool);\r\n}\r\nstatic int rrpc_read_ppalist_rq(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags, int npages)\r\n{\r\nstruct rrpc_inflight_rq *r = rrpc_get_inflight_rq(rqd);\r\nstruct rrpc_addr *gp;\r\nsector_t laddr = rrpc_get_laddr(bio);\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nint i;\r\nif (!is_gc && rrpc_lock_rq(rrpc, bio, rqd)) {\r\nnvm_dev_dma_free(rrpc->dev, rqd->ppa_list, rqd->dma_ppa_list);\r\nreturn NVM_IO_REQUEUE;\r\n}\r\nfor (i = 0; i < npages; i++) {\r\nBUG_ON(!(laddr + i >= 0 && laddr + i < rrpc->nr_sects));\r\ngp = &rrpc->trans_map[laddr + i];\r\nif (gp->rblk) {\r\nrqd->ppa_list[i] = rrpc_ppa_to_gaddr(rrpc->dev,\r\ngp->addr);\r\n} else {\r\nBUG_ON(is_gc);\r\nrrpc_unlock_laddr(rrpc, r);\r\nnvm_dev_dma_free(rrpc->dev, rqd->ppa_list,\r\nrqd->dma_ppa_list);\r\nreturn NVM_IO_DONE;\r\n}\r\n}\r\nrqd->opcode = NVM_OP_HBREAD;\r\nreturn NVM_IO_OK;\r\n}\r\nstatic int rrpc_read_rq(struct rrpc *rrpc, struct bio *bio, struct nvm_rq *rqd,\r\nunsigned long flags)\r\n{\r\nstruct rrpc_rq *rrqd = nvm_rq_to_pdu(rqd);\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nsector_t laddr = rrpc_get_laddr(bio);\r\nstruct rrpc_addr *gp;\r\nif (!is_gc && rrpc_lock_rq(rrpc, bio, rqd))\r\nreturn NVM_IO_REQUEUE;\r\nBUG_ON(!(laddr >= 0 && laddr < rrpc->nr_sects));\r\ngp = &rrpc->trans_map[laddr];\r\nif (gp->rblk) {\r\nrqd->ppa_addr = rrpc_ppa_to_gaddr(rrpc->dev, gp->addr);\r\n} else {\r\nBUG_ON(is_gc);\r\nrrpc_unlock_rq(rrpc, rqd);\r\nreturn NVM_IO_DONE;\r\n}\r\nrqd->opcode = NVM_OP_HBREAD;\r\nrrqd->addr = gp;\r\nreturn NVM_IO_OK;\r\n}\r\nstatic int rrpc_write_ppalist_rq(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags, int npages)\r\n{\r\nstruct rrpc_inflight_rq *r = rrpc_get_inflight_rq(rqd);\r\nstruct rrpc_addr *p;\r\nsector_t laddr = rrpc_get_laddr(bio);\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nint i;\r\nif (!is_gc && rrpc_lock_rq(rrpc, bio, rqd)) {\r\nnvm_dev_dma_free(rrpc->dev, rqd->ppa_list, rqd->dma_ppa_list);\r\nreturn NVM_IO_REQUEUE;\r\n}\r\nfor (i = 0; i < npages; i++) {\r\np = rrpc_map_page(rrpc, laddr + i, is_gc);\r\nif (!p) {\r\nBUG_ON(is_gc);\r\nrrpc_unlock_laddr(rrpc, r);\r\nnvm_dev_dma_free(rrpc->dev, rqd->ppa_list,\r\nrqd->dma_ppa_list);\r\nrrpc_gc_kick(rrpc);\r\nreturn NVM_IO_REQUEUE;\r\n}\r\nrqd->ppa_list[i] = rrpc_ppa_to_gaddr(rrpc->dev,\r\np->addr);\r\n}\r\nrqd->opcode = NVM_OP_HBWRITE;\r\nreturn NVM_IO_OK;\r\n}\r\nstatic int rrpc_write_rq(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags)\r\n{\r\nstruct rrpc_rq *rrqd = nvm_rq_to_pdu(rqd);\r\nstruct rrpc_addr *p;\r\nint is_gc = flags & NVM_IOTYPE_GC;\r\nsector_t laddr = rrpc_get_laddr(bio);\r\nif (!is_gc && rrpc_lock_rq(rrpc, bio, rqd))\r\nreturn NVM_IO_REQUEUE;\r\np = rrpc_map_page(rrpc, laddr, is_gc);\r\nif (!p) {\r\nBUG_ON(is_gc);\r\nrrpc_unlock_rq(rrpc, rqd);\r\nrrpc_gc_kick(rrpc);\r\nreturn NVM_IO_REQUEUE;\r\n}\r\nrqd->ppa_addr = rrpc_ppa_to_gaddr(rrpc->dev, p->addr);\r\nrqd->opcode = NVM_OP_HBWRITE;\r\nrrqd->addr = p;\r\nreturn NVM_IO_OK;\r\n}\r\nstatic int rrpc_setup_rq(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags, uint8_t npages)\r\n{\r\nif (npages > 1) {\r\nrqd->ppa_list = nvm_dev_dma_alloc(rrpc->dev, GFP_KERNEL,\r\n&rqd->dma_ppa_list);\r\nif (!rqd->ppa_list) {\r\npr_err("rrpc: not able to allocate ppa list\n");\r\nreturn NVM_IO_ERR;\r\n}\r\nif (bio_op(bio) == REQ_OP_WRITE)\r\nreturn rrpc_write_ppalist_rq(rrpc, bio, rqd, flags,\r\nnpages);\r\nreturn rrpc_read_ppalist_rq(rrpc, bio, rqd, flags, npages);\r\n}\r\nif (bio_op(bio) == REQ_OP_WRITE)\r\nreturn rrpc_write_rq(rrpc, bio, rqd, flags);\r\nreturn rrpc_read_rq(rrpc, bio, rqd, flags);\r\n}\r\nstatic int rrpc_submit_io(struct rrpc *rrpc, struct bio *bio,\r\nstruct nvm_rq *rqd, unsigned long flags)\r\n{\r\nint err;\r\nstruct rrpc_rq *rrq = nvm_rq_to_pdu(rqd);\r\nuint8_t nr_pages = rrpc_get_pages(bio);\r\nint bio_size = bio_sectors(bio) << 9;\r\nif (bio_size < rrpc->dev->sec_size)\r\nreturn NVM_IO_ERR;\r\nelse if (bio_size > rrpc->dev->max_rq_size)\r\nreturn NVM_IO_ERR;\r\nerr = rrpc_setup_rq(rrpc, bio, rqd, flags, nr_pages);\r\nif (err)\r\nreturn err;\r\nbio_get(bio);\r\nrqd->bio = bio;\r\nrqd->ins = &rrpc->instance;\r\nrqd->nr_ppas = nr_pages;\r\nrrq->flags = flags;\r\nerr = nvm_submit_io(rrpc->dev, rqd);\r\nif (err) {\r\npr_err("rrpc: I/O submission failed: %d\n", err);\r\nbio_put(bio);\r\nif (!(flags & NVM_IOTYPE_GC)) {\r\nrrpc_unlock_rq(rrpc, rqd);\r\nif (rqd->nr_ppas > 1)\r\nnvm_dev_dma_free(rrpc->dev,\r\nrqd->ppa_list, rqd->dma_ppa_list);\r\n}\r\nreturn NVM_IO_ERR;\r\n}\r\nreturn NVM_IO_OK;\r\n}\r\nstatic blk_qc_t rrpc_make_rq(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct rrpc *rrpc = q->queuedata;\r\nstruct nvm_rq *rqd;\r\nint err;\r\nif (bio_op(bio) == REQ_OP_DISCARD) {\r\nrrpc_discard(rrpc, bio);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nrqd = mempool_alloc(rrpc->rq_pool, GFP_KERNEL);\r\nif (!rqd) {\r\npr_err_ratelimited("rrpc: not able to queue bio.");\r\nbio_io_error(bio);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nmemset(rqd, 0, sizeof(struct nvm_rq));\r\nerr = rrpc_submit_io(rrpc, bio, rqd, NVM_IOTYPE_NONE);\r\nswitch (err) {\r\ncase NVM_IO_OK:\r\nreturn BLK_QC_T_NONE;\r\ncase NVM_IO_ERR:\r\nbio_io_error(bio);\r\nbreak;\r\ncase NVM_IO_DONE:\r\nbio_endio(bio);\r\nbreak;\r\ncase NVM_IO_REQUEUE:\r\nspin_lock(&rrpc->bio_lock);\r\nbio_list_add(&rrpc->requeue_bios, bio);\r\nspin_unlock(&rrpc->bio_lock);\r\nqueue_work(rrpc->kgc_wq, &rrpc->ws_requeue);\r\nbreak;\r\n}\r\nmempool_free(rqd, rrpc->rq_pool);\r\nreturn BLK_QC_T_NONE;\r\n}\r\nstatic void rrpc_requeue(struct work_struct *work)\r\n{\r\nstruct rrpc *rrpc = container_of(work, struct rrpc, ws_requeue);\r\nstruct bio_list bios;\r\nstruct bio *bio;\r\nbio_list_init(&bios);\r\nspin_lock(&rrpc->bio_lock);\r\nbio_list_merge(&bios, &rrpc->requeue_bios);\r\nbio_list_init(&rrpc->requeue_bios);\r\nspin_unlock(&rrpc->bio_lock);\r\nwhile ((bio = bio_list_pop(&bios)))\r\nrrpc_make_rq(rrpc->disk->queue, bio);\r\n}\r\nstatic void rrpc_gc_free(struct rrpc *rrpc)\r\n{\r\nif (rrpc->krqd_wq)\r\ndestroy_workqueue(rrpc->krqd_wq);\r\nif (rrpc->kgc_wq)\r\ndestroy_workqueue(rrpc->kgc_wq);\r\n}\r\nstatic int rrpc_gc_init(struct rrpc *rrpc)\r\n{\r\nrrpc->krqd_wq = alloc_workqueue("rrpc-lun", WQ_MEM_RECLAIM|WQ_UNBOUND,\r\nrrpc->nr_luns);\r\nif (!rrpc->krqd_wq)\r\nreturn -ENOMEM;\r\nrrpc->kgc_wq = alloc_workqueue("rrpc-bg", WQ_MEM_RECLAIM, 1);\r\nif (!rrpc->kgc_wq)\r\nreturn -ENOMEM;\r\nsetup_timer(&rrpc->gc_timer, rrpc_gc_timer, (unsigned long)rrpc);\r\nreturn 0;\r\n}\r\nstatic void rrpc_map_free(struct rrpc *rrpc)\r\n{\r\nvfree(rrpc->rev_trans_map);\r\nvfree(rrpc->trans_map);\r\n}\r\nstatic int rrpc_l2p_update(u64 slba, u32 nlb, __le64 *entries, void *private)\r\n{\r\nstruct rrpc *rrpc = (struct rrpc *)private;\r\nstruct nvm_dev *dev = rrpc->dev;\r\nstruct rrpc_addr *addr = rrpc->trans_map + slba;\r\nstruct rrpc_rev_addr *raddr = rrpc->rev_trans_map;\r\nu64 elba = slba + nlb;\r\nu64 i;\r\nif (unlikely(elba > dev->total_secs)) {\r\npr_err("nvm: L2P data from device is out of bounds!\n");\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < nlb; i++) {\r\nu64 pba = le64_to_cpu(entries[i]);\r\nunsigned int mod;\r\nif (unlikely(pba >= dev->total_secs && pba != U64_MAX)) {\r\npr_err("nvm: L2P data entry is out of bounds!\n");\r\nreturn -EINVAL;\r\n}\r\nif (!pba)\r\ncontinue;\r\ndiv_u64_rem(pba, rrpc->nr_sects, &mod);\r\naddr[i].addr = pba;\r\nraddr[mod].addr = slba + i;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rrpc_map_init(struct rrpc *rrpc)\r\n{\r\nstruct nvm_dev *dev = rrpc->dev;\r\nsector_t i;\r\nint ret;\r\nrrpc->trans_map = vzalloc(sizeof(struct rrpc_addr) * rrpc->nr_sects);\r\nif (!rrpc->trans_map)\r\nreturn -ENOMEM;\r\nrrpc->rev_trans_map = vmalloc(sizeof(struct rrpc_rev_addr)\r\n* rrpc->nr_sects);\r\nif (!rrpc->rev_trans_map)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < rrpc->nr_sects; i++) {\r\nstruct rrpc_addr *p = &rrpc->trans_map[i];\r\nstruct rrpc_rev_addr *r = &rrpc->rev_trans_map[i];\r\np->addr = ADDR_EMPTY;\r\nr->addr = ADDR_EMPTY;\r\n}\r\nif (!dev->ops->get_l2p_tbl)\r\nreturn 0;\r\nret = dev->ops->get_l2p_tbl(dev, rrpc->soffset, rrpc->nr_sects,\r\nrrpc_l2p_update, rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not read L2P table.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rrpc_core_init(struct rrpc *rrpc)\r\n{\r\ndown_write(&rrpc_lock);\r\nif (!rrpc_gcb_cache) {\r\nrrpc_gcb_cache = kmem_cache_create("rrpc_gcb",\r\nsizeof(struct rrpc_block_gc), 0, 0, NULL);\r\nif (!rrpc_gcb_cache) {\r\nup_write(&rrpc_lock);\r\nreturn -ENOMEM;\r\n}\r\nrrpc_rq_cache = kmem_cache_create("rrpc_rq",\r\nsizeof(struct nvm_rq) + sizeof(struct rrpc_rq),\r\n0, 0, NULL);\r\nif (!rrpc_rq_cache) {\r\nkmem_cache_destroy(rrpc_gcb_cache);\r\nup_write(&rrpc_lock);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nup_write(&rrpc_lock);\r\nrrpc->page_pool = mempool_create_page_pool(PAGE_POOL_SIZE, 0);\r\nif (!rrpc->page_pool)\r\nreturn -ENOMEM;\r\nrrpc->gcb_pool = mempool_create_slab_pool(rrpc->dev->nr_luns,\r\nrrpc_gcb_cache);\r\nif (!rrpc->gcb_pool)\r\nreturn -ENOMEM;\r\nrrpc->rq_pool = mempool_create_slab_pool(64, rrpc_rq_cache);\r\nif (!rrpc->rq_pool)\r\nreturn -ENOMEM;\r\nspin_lock_init(&rrpc->inflights.lock);\r\nINIT_LIST_HEAD(&rrpc->inflights.reqs);\r\nreturn 0;\r\n}\r\nstatic void rrpc_core_free(struct rrpc *rrpc)\r\n{\r\nmempool_destroy(rrpc->page_pool);\r\nmempool_destroy(rrpc->gcb_pool);\r\nmempool_destroy(rrpc->rq_pool);\r\n}\r\nstatic void rrpc_luns_free(struct rrpc *rrpc)\r\n{\r\nstruct nvm_dev *dev = rrpc->dev;\r\nstruct nvm_lun *lun;\r\nstruct rrpc_lun *rlun;\r\nint i;\r\nif (!rrpc->luns)\r\nreturn;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nrlun = &rrpc->luns[i];\r\nlun = rlun->parent;\r\nif (!lun)\r\nbreak;\r\ndev->mt->release_lun(dev, lun->id);\r\nvfree(rlun->blocks);\r\n}\r\nkfree(rrpc->luns);\r\n}\r\nstatic int rrpc_luns_init(struct rrpc *rrpc, int lun_begin, int lun_end)\r\n{\r\nstruct nvm_dev *dev = rrpc->dev;\r\nstruct rrpc_lun *rlun;\r\nint i, j, ret = -EINVAL;\r\nif (dev->sec_per_blk > MAX_INVALID_PAGES_STORAGE * BITS_PER_LONG) {\r\npr_err("rrpc: number of pages per block too high.");\r\nreturn -EINVAL;\r\n}\r\nspin_lock_init(&rrpc->rev_lock);\r\nrrpc->luns = kcalloc(rrpc->nr_luns, sizeof(struct rrpc_lun),\r\nGFP_KERNEL);\r\nif (!rrpc->luns)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nint lunid = lun_begin + i;\r\nstruct nvm_lun *lun;\r\nif (dev->mt->reserve_lun(dev, lunid)) {\r\npr_err("rrpc: lun %u is already allocated\n", lunid);\r\ngoto err;\r\n}\r\nlun = dev->mt->get_lun(dev, lunid);\r\nif (!lun)\r\ngoto err;\r\nrlun = &rrpc->luns[i];\r\nrlun->parent = lun;\r\nrlun->blocks = vzalloc(sizeof(struct rrpc_block) *\r\nrrpc->dev->blks_per_lun);\r\nif (!rlun->blocks) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nfor (j = 0; j < rrpc->dev->blks_per_lun; j++) {\r\nstruct rrpc_block *rblk = &rlun->blocks[j];\r\nstruct nvm_block *blk = &lun->blocks[j];\r\nrblk->parent = blk;\r\nrblk->rlun = rlun;\r\nINIT_LIST_HEAD(&rblk->prio);\r\nspin_lock_init(&rblk->lock);\r\n}\r\nrlun->rrpc = rrpc;\r\nINIT_LIST_HEAD(&rlun->prio_list);\r\nINIT_LIST_HEAD(&rlun->wblk_list);\r\nINIT_WORK(&rlun->ws_gc, rrpc_lun_gc);\r\nspin_lock_init(&rlun->lock);\r\n}\r\nreturn 0;\r\nerr:\r\nreturn ret;\r\n}\r\nstatic int rrpc_area_init(struct rrpc *rrpc, sector_t *begin)\r\n{\r\nstruct nvm_dev *dev = rrpc->dev;\r\nstruct nvmm_type *mt = dev->mt;\r\nsector_t size = rrpc->nr_sects * dev->sec_size;\r\nint ret;\r\nsize >>= 9;\r\nret = mt->get_area(dev, begin, size);\r\nif (!ret)\r\n*begin >>= (ilog2(dev->sec_size) - 9);\r\nreturn ret;\r\n}\r\nstatic void rrpc_area_free(struct rrpc *rrpc)\r\n{\r\nstruct nvm_dev *dev = rrpc->dev;\r\nstruct nvmm_type *mt = dev->mt;\r\nsector_t begin = rrpc->soffset << (ilog2(dev->sec_size) - 9);\r\nmt->put_area(dev, begin);\r\n}\r\nstatic void rrpc_free(struct rrpc *rrpc)\r\n{\r\nrrpc_gc_free(rrpc);\r\nrrpc_map_free(rrpc);\r\nrrpc_core_free(rrpc);\r\nrrpc_luns_free(rrpc);\r\nrrpc_area_free(rrpc);\r\nkfree(rrpc);\r\n}\r\nstatic void rrpc_exit(void *private)\r\n{\r\nstruct rrpc *rrpc = private;\r\ndel_timer(&rrpc->gc_timer);\r\nflush_workqueue(rrpc->krqd_wq);\r\nflush_workqueue(rrpc->kgc_wq);\r\nrrpc_free(rrpc);\r\n}\r\nstatic sector_t rrpc_capacity(void *private)\r\n{\r\nstruct rrpc *rrpc = private;\r\nstruct nvm_dev *dev = rrpc->dev;\r\nsector_t reserved, provisioned;\r\nreserved = rrpc->nr_luns * dev->sec_per_blk * 4;\r\nprovisioned = rrpc->nr_sects - reserved;\r\nif (reserved > rrpc->nr_sects) {\r\npr_err("rrpc: not enough space available to expose storage.\n");\r\nreturn 0;\r\n}\r\nsector_div(provisioned, 10);\r\nreturn provisioned * 9 * NR_PHY_IN_LOG;\r\n}\r\nstatic void rrpc_block_map_update(struct rrpc *rrpc, struct rrpc_block *rblk)\r\n{\r\nstruct nvm_dev *dev = rrpc->dev;\r\nint offset;\r\nstruct rrpc_addr *laddr;\r\nu64 bpaddr, paddr, pladdr;\r\nbpaddr = block_to_rel_addr(rrpc, rblk);\r\nfor (offset = 0; offset < dev->sec_per_blk; offset++) {\r\npaddr = bpaddr + offset;\r\npladdr = rrpc->rev_trans_map[paddr].addr;\r\nif (pladdr == ADDR_EMPTY)\r\ncontinue;\r\nladdr = &rrpc->trans_map[pladdr];\r\nif (paddr == laddr->addr) {\r\nladdr->rblk = rblk;\r\n} else {\r\nset_bit(offset, rblk->invalid_pages);\r\nrblk->nr_invalid_pages++;\r\n}\r\n}\r\n}\r\nstatic int rrpc_blocks_init(struct rrpc *rrpc)\r\n{\r\nstruct rrpc_lun *rlun;\r\nstruct rrpc_block *rblk;\r\nint lun_iter, blk_iter;\r\nfor (lun_iter = 0; lun_iter < rrpc->nr_luns; lun_iter++) {\r\nrlun = &rrpc->luns[lun_iter];\r\nfor (blk_iter = 0; blk_iter < rrpc->dev->blks_per_lun;\r\nblk_iter++) {\r\nrblk = &rlun->blocks[blk_iter];\r\nrrpc_block_map_update(rrpc, rblk);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int rrpc_luns_configure(struct rrpc *rrpc)\r\n{\r\nstruct rrpc_lun *rlun;\r\nstruct rrpc_block *rblk;\r\nint i;\r\nfor (i = 0; i < rrpc->nr_luns; i++) {\r\nrlun = &rrpc->luns[i];\r\nrblk = rrpc_get_blk(rrpc, rlun, 0);\r\nif (!rblk)\r\ngoto err;\r\nrrpc_set_lun_cur(rlun, rblk, &rlun->cur);\r\nrblk = rrpc_get_blk(rrpc, rlun, 1);\r\nif (!rblk)\r\ngoto err;\r\nrrpc_set_lun_cur(rlun, rblk, &rlun->gc_cur);\r\n}\r\nreturn 0;\r\nerr:\r\nrrpc_put_blks(rrpc);\r\nreturn -EINVAL;\r\n}\r\nstatic void *rrpc_init(struct nvm_dev *dev, struct gendisk *tdisk,\r\nint lun_begin, int lun_end)\r\n{\r\nstruct request_queue *bqueue = dev->q;\r\nstruct request_queue *tqueue = tdisk->queue;\r\nstruct rrpc *rrpc;\r\nsector_t soffset;\r\nint ret;\r\nif (!(dev->identity.dom & NVM_RSP_L2P)) {\r\npr_err("nvm: rrpc: device does not support l2p (%x)\n",\r\ndev->identity.dom);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nrrpc = kzalloc(sizeof(struct rrpc), GFP_KERNEL);\r\nif (!rrpc)\r\nreturn ERR_PTR(-ENOMEM);\r\nrrpc->instance.tt = &tt_rrpc;\r\nrrpc->dev = dev;\r\nrrpc->disk = tdisk;\r\nbio_list_init(&rrpc->requeue_bios);\r\nspin_lock_init(&rrpc->bio_lock);\r\nINIT_WORK(&rrpc->ws_requeue, rrpc_requeue);\r\nrrpc->nr_luns = lun_end - lun_begin + 1;\r\nrrpc->total_blocks = (unsigned long)dev->blks_per_lun * rrpc->nr_luns;\r\nrrpc->nr_sects = (unsigned long long)dev->sec_per_lun * rrpc->nr_luns;\r\natomic_set(&rrpc->next_lun, -1);\r\nret = rrpc_area_init(rrpc, &soffset);\r\nif (ret < 0) {\r\npr_err("nvm: rrpc: could not initialize area\n");\r\nreturn ERR_PTR(ret);\r\n}\r\nrrpc->soffset = soffset;\r\nret = rrpc_luns_init(rrpc, lun_begin, lun_end);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize luns\n");\r\ngoto err;\r\n}\r\nrrpc->poffset = dev->sec_per_lun * lun_begin;\r\nrrpc->lun_offset = lun_begin;\r\nret = rrpc_core_init(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize core\n");\r\ngoto err;\r\n}\r\nret = rrpc_map_init(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize maps\n");\r\ngoto err;\r\n}\r\nret = rrpc_blocks_init(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize state for blocks\n");\r\ngoto err;\r\n}\r\nret = rrpc_luns_configure(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: not enough blocks available in LUNs.\n");\r\ngoto err;\r\n}\r\nret = rrpc_gc_init(rrpc);\r\nif (ret) {\r\npr_err("nvm: rrpc: could not initialize gc\n");\r\ngoto err;\r\n}\r\nblk_queue_logical_block_size(tqueue, queue_physical_block_size(bqueue));\r\nblk_queue_max_hw_sectors(tqueue, queue_max_hw_sectors(bqueue));\r\npr_info("nvm: rrpc initialized with %u luns and %llu pages.\n",\r\nrrpc->nr_luns, (unsigned long long)rrpc->nr_sects);\r\nmod_timer(&rrpc->gc_timer, jiffies + msecs_to_jiffies(10));\r\nreturn rrpc;\r\nerr:\r\nrrpc_free(rrpc);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic int __init rrpc_module_init(void)\r\n{\r\nreturn nvm_register_tgt_type(&tt_rrpc);\r\n}\r\nstatic void rrpc_module_exit(void)\r\n{\r\nnvm_unregister_tgt_type(&tt_rrpc);\r\n}
