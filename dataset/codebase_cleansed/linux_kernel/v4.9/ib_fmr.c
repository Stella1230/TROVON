struct rds_ib_mr *rds_ib_alloc_fmr(struct rds_ib_device *rds_ibdev, int npages)\r\n{\r\nstruct rds_ib_mr_pool *pool;\r\nstruct rds_ib_mr *ibmr = NULL;\r\nstruct rds_ib_fmr *fmr;\r\nint err = 0;\r\nif (npages <= RDS_MR_8K_MSG_SIZE)\r\npool = rds_ibdev->mr_8k_pool;\r\nelse\r\npool = rds_ibdev->mr_1m_pool;\r\nibmr = rds_ib_try_reuse_ibmr(pool);\r\nif (ibmr)\r\nreturn ibmr;\r\nibmr = kzalloc_node(sizeof(*ibmr), GFP_KERNEL,\r\nrdsibdev_to_node(rds_ibdev));\r\nif (!ibmr) {\r\nerr = -ENOMEM;\r\ngoto out_no_cigar;\r\n}\r\nfmr = &ibmr->u.fmr;\r\nfmr->fmr = ib_alloc_fmr(rds_ibdev->pd,\r\n(IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE |\r\nIB_ACCESS_REMOTE_ATOMIC),\r\n&pool->fmr_attr);\r\nif (IS_ERR(fmr->fmr)) {\r\nerr = PTR_ERR(fmr->fmr);\r\nfmr->fmr = NULL;\r\npr_warn("RDS/IB: %s failed (err=%d)\n", __func__, err);\r\ngoto out_no_cigar;\r\n}\r\nibmr->pool = pool;\r\nif (pool->pool_type == RDS_IB_MR_8K_POOL)\r\nrds_ib_stats_inc(s_ib_rdma_mr_8k_alloc);\r\nelse\r\nrds_ib_stats_inc(s_ib_rdma_mr_1m_alloc);\r\nreturn ibmr;\r\nout_no_cigar:\r\nif (ibmr) {\r\nif (fmr->fmr)\r\nib_dealloc_fmr(fmr->fmr);\r\nkfree(ibmr);\r\n}\r\natomic_dec(&pool->item_count);\r\nreturn ERR_PTR(err);\r\n}\r\nint rds_ib_map_fmr(struct rds_ib_device *rds_ibdev, struct rds_ib_mr *ibmr,\r\nstruct scatterlist *sg, unsigned int nents)\r\n{\r\nstruct ib_device *dev = rds_ibdev->dev;\r\nstruct rds_ib_fmr *fmr = &ibmr->u.fmr;\r\nstruct scatterlist *scat = sg;\r\nu64 io_addr = 0;\r\nu64 *dma_pages;\r\nu32 len;\r\nint page_cnt, sg_dma_len;\r\nint i, j;\r\nint ret;\r\nsg_dma_len = ib_dma_map_sg(dev, sg, nents, DMA_BIDIRECTIONAL);\r\nif (unlikely(!sg_dma_len)) {\r\npr_warn("RDS/IB: %s failed!\n", __func__);\r\nreturn -EBUSY;\r\n}\r\nlen = 0;\r\npage_cnt = 0;\r\nfor (i = 0; i < sg_dma_len; ++i) {\r\nunsigned int dma_len = ib_sg_dma_len(dev, &scat[i]);\r\nu64 dma_addr = ib_sg_dma_address(dev, &scat[i]);\r\nif (dma_addr & ~PAGE_MASK) {\r\nif (i > 0)\r\nreturn -EINVAL;\r\nelse\r\n++page_cnt;\r\n}\r\nif ((dma_addr + dma_len) & ~PAGE_MASK) {\r\nif (i < sg_dma_len - 1)\r\nreturn -EINVAL;\r\nelse\r\n++page_cnt;\r\n}\r\nlen += dma_len;\r\n}\r\npage_cnt += len >> PAGE_SHIFT;\r\nif (page_cnt > ibmr->pool->fmr_attr.max_pages)\r\nreturn -EINVAL;\r\ndma_pages = kmalloc_node(sizeof(u64) * page_cnt, GFP_ATOMIC,\r\nrdsibdev_to_node(rds_ibdev));\r\nif (!dma_pages)\r\nreturn -ENOMEM;\r\npage_cnt = 0;\r\nfor (i = 0; i < sg_dma_len; ++i) {\r\nunsigned int dma_len = ib_sg_dma_len(dev, &scat[i]);\r\nu64 dma_addr = ib_sg_dma_address(dev, &scat[i]);\r\nfor (j = 0; j < dma_len; j += PAGE_SIZE)\r\ndma_pages[page_cnt++] =\r\n(dma_addr & PAGE_MASK) + j;\r\n}\r\nret = ib_map_phys_fmr(fmr->fmr, dma_pages, page_cnt, io_addr);\r\nif (ret)\r\ngoto out;\r\nrds_ib_teardown_mr(ibmr);\r\nibmr->sg = scat;\r\nibmr->sg_len = nents;\r\nibmr->sg_dma_len = sg_dma_len;\r\nibmr->remap_count++;\r\nif (ibmr->pool->pool_type == RDS_IB_MR_8K_POOL)\r\nrds_ib_stats_inc(s_ib_rdma_mr_8k_used);\r\nelse\r\nrds_ib_stats_inc(s_ib_rdma_mr_1m_used);\r\nret = 0;\r\nout:\r\nkfree(dma_pages);\r\nreturn ret;\r\n}\r\nstruct rds_ib_mr *rds_ib_reg_fmr(struct rds_ib_device *rds_ibdev,\r\nstruct scatterlist *sg,\r\nunsigned long nents,\r\nu32 *key)\r\n{\r\nstruct rds_ib_mr *ibmr = NULL;\r\nstruct rds_ib_fmr *fmr;\r\nint ret;\r\nibmr = rds_ib_alloc_fmr(rds_ibdev, nents);\r\nif (IS_ERR(ibmr))\r\nreturn ibmr;\r\nibmr->device = rds_ibdev;\r\nfmr = &ibmr->u.fmr;\r\nret = rds_ib_map_fmr(rds_ibdev, ibmr, sg, nents);\r\nif (ret == 0)\r\n*key = fmr->fmr->rkey;\r\nelse\r\nrds_ib_free_mr(ibmr, 0);\r\nreturn ibmr;\r\n}\r\nvoid rds_ib_unreg_fmr(struct list_head *list, unsigned int *nfreed,\r\nunsigned long *unpinned, unsigned int goal)\r\n{\r\nstruct rds_ib_mr *ibmr, *next;\r\nstruct rds_ib_fmr *fmr;\r\nLIST_HEAD(fmr_list);\r\nint ret = 0;\r\nunsigned int freed = *nfreed;\r\nlist_for_each_entry(ibmr, list, unmap_list) {\r\nfmr = &ibmr->u.fmr;\r\nlist_add(&fmr->fmr->list, &fmr_list);\r\n}\r\nret = ib_unmap_fmr(&fmr_list);\r\nif (ret)\r\npr_warn("RDS/IB: FMR invalidation failed (err=%d)\n", ret);\r\nlist_for_each_entry_safe(ibmr, next, list, unmap_list) {\r\nfmr = &ibmr->u.fmr;\r\n*unpinned += ibmr->sg_len;\r\n__rds_ib_teardown_mr(ibmr);\r\nif (freed < goal ||\r\nibmr->remap_count >= ibmr->pool->fmr_attr.max_maps) {\r\nif (ibmr->pool->pool_type == RDS_IB_MR_8K_POOL)\r\nrds_ib_stats_inc(s_ib_rdma_mr_8k_free);\r\nelse\r\nrds_ib_stats_inc(s_ib_rdma_mr_1m_free);\r\nlist_del(&ibmr->unmap_list);\r\nib_dealloc_fmr(fmr->fmr);\r\nkfree(ibmr);\r\nfreed++;\r\n}\r\n}\r\n*nfreed = freed;\r\n}\r\nvoid rds_ib_free_fmr_list(struct rds_ib_mr *ibmr)\r\n{\r\nstruct rds_ib_mr_pool *pool = ibmr->pool;\r\nif (ibmr->remap_count >= pool->fmr_attr.max_maps)\r\nllist_add(&ibmr->llnode, &pool->drop_list);\r\nelse\r\nllist_add(&ibmr->llnode, &pool->free_list);\r\n}
