static bool iio_buffer_is_active(struct iio_buffer *buf)\r\n{\r\nreturn !list_empty(&buf->buffer_list);\r\n}\r\nstatic size_t iio_buffer_data_available(struct iio_buffer *buf)\r\n{\r\nreturn buf->access->data_available(buf);\r\n}\r\nstatic int iio_buffer_flush_hwfifo(struct iio_dev *indio_dev,\r\nstruct iio_buffer *buf, size_t required)\r\n{\r\nif (!indio_dev->info->hwfifo_flush_to_buffer)\r\nreturn -ENODEV;\r\nreturn indio_dev->info->hwfifo_flush_to_buffer(indio_dev, required);\r\n}\r\nstatic bool iio_buffer_ready(struct iio_dev *indio_dev, struct iio_buffer *buf,\r\nsize_t to_wait, int to_flush)\r\n{\r\nsize_t avail;\r\nint flushed = 0;\r\nif (!indio_dev->info)\r\nreturn true;\r\nif (!iio_buffer_is_active(buf)) {\r\nto_wait = min_t(size_t, to_wait, 1);\r\nto_flush = 0;\r\n}\r\navail = iio_buffer_data_available(buf);\r\nif (avail >= to_wait) {\r\nif (!to_wait && avail < to_flush)\r\niio_buffer_flush_hwfifo(indio_dev, buf,\r\nto_flush - avail);\r\nreturn true;\r\n}\r\nif (to_flush)\r\nflushed = iio_buffer_flush_hwfifo(indio_dev, buf,\r\nto_wait - avail);\r\nif (flushed <= 0)\r\nreturn false;\r\nif (avail + flushed >= to_wait)\r\nreturn true;\r\nreturn false;\r\n}\r\nssize_t iio_buffer_read_first_n_outer(struct file *filp, char __user *buf,\r\nsize_t n, loff_t *f_ps)\r\n{\r\nstruct iio_dev *indio_dev = filp->private_data;\r\nstruct iio_buffer *rb = indio_dev->buffer;\r\nDEFINE_WAIT_FUNC(wait, woken_wake_function);\r\nsize_t datum_size;\r\nsize_t to_wait;\r\nint ret = 0;\r\nif (!indio_dev->info)\r\nreturn -ENODEV;\r\nif (!rb || !rb->access->read_first_n)\r\nreturn -EINVAL;\r\ndatum_size = rb->bytes_per_datum;\r\nif (!datum_size)\r\nreturn 0;\r\nif (filp->f_flags & O_NONBLOCK)\r\nto_wait = 0;\r\nelse\r\nto_wait = min_t(size_t, n / datum_size, rb->watermark);\r\nadd_wait_queue(&rb->pollq, &wait);\r\ndo {\r\nif (!indio_dev->info) {\r\nret = -ENODEV;\r\nbreak;\r\n}\r\nif (!iio_buffer_ready(indio_dev, rb, to_wait, n / datum_size)) {\r\nif (signal_pending(current)) {\r\nret = -ERESTARTSYS;\r\nbreak;\r\n}\r\nwait_woken(&wait, TASK_INTERRUPTIBLE,\r\nMAX_SCHEDULE_TIMEOUT);\r\ncontinue;\r\n}\r\nret = rb->access->read_first_n(rb, n, buf);\r\nif (ret == 0 && (filp->f_flags & O_NONBLOCK))\r\nret = -EAGAIN;\r\n} while (ret == 0);\r\nremove_wait_queue(&rb->pollq, &wait);\r\nreturn ret;\r\n}\r\nunsigned int iio_buffer_poll(struct file *filp,\r\nstruct poll_table_struct *wait)\r\n{\r\nstruct iio_dev *indio_dev = filp->private_data;\r\nstruct iio_buffer *rb = indio_dev->buffer;\r\nif (!indio_dev->info)\r\nreturn 0;\r\npoll_wait(filp, &rb->pollq, wait);\r\nif (iio_buffer_ready(indio_dev, rb, rb->watermark, 0))\r\nreturn POLLIN | POLLRDNORM;\r\nreturn 0;\r\n}\r\nvoid iio_buffer_wakeup_poll(struct iio_dev *indio_dev)\r\n{\r\nif (!indio_dev->buffer)\r\nreturn;\r\nwake_up(&indio_dev->buffer->pollq);\r\n}\r\nvoid iio_buffer_init(struct iio_buffer *buffer)\r\n{\r\nINIT_LIST_HEAD(&buffer->demux_list);\r\nINIT_LIST_HEAD(&buffer->buffer_list);\r\ninit_waitqueue_head(&buffer->pollq);\r\nkref_init(&buffer->ref);\r\nif (!buffer->watermark)\r\nbuffer->watermark = 1;\r\n}\r\nstatic ssize_t iio_show_scan_index(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%u\n", to_iio_dev_attr(attr)->c->scan_index);\r\n}\r\nstatic ssize_t iio_show_fixed_type(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct iio_dev_attr *this_attr = to_iio_dev_attr(attr);\r\nu8 type = this_attr->c->scan_type.endianness;\r\nif (type == IIO_CPU) {\r\n#ifdef __LITTLE_ENDIAN\r\ntype = IIO_LE;\r\n#else\r\ntype = IIO_BE;\r\n#endif\r\n}\r\nif (this_attr->c->scan_type.repeat > 1)\r\nreturn sprintf(buf, "%s:%c%d/%dX%d>>%u\n",\r\niio_endian_prefix[type],\r\nthis_attr->c->scan_type.sign,\r\nthis_attr->c->scan_type.realbits,\r\nthis_attr->c->scan_type.storagebits,\r\nthis_attr->c->scan_type.repeat,\r\nthis_attr->c->scan_type.shift);\r\nelse\r\nreturn sprintf(buf, "%s:%c%d/%d>>%u\n",\r\niio_endian_prefix[type],\r\nthis_attr->c->scan_type.sign,\r\nthis_attr->c->scan_type.realbits,\r\nthis_attr->c->scan_type.storagebits,\r\nthis_attr->c->scan_type.shift);\r\n}\r\nstatic ssize_t iio_scan_el_show(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nint ret;\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nret = !!test_bit(to_iio_dev_attr(attr)->address,\r\nindio_dev->buffer->scan_mask);\r\nreturn sprintf(buf, "%d\n", ret);\r\n}\r\nstatic const unsigned long *iio_scan_mask_match(const unsigned long *av_masks,\r\nunsigned int masklength,\r\nconst unsigned long *mask,\r\nbool strict)\r\n{\r\nif (bitmap_empty(mask, masklength))\r\nreturn NULL;\r\nwhile (*av_masks) {\r\nif (strict) {\r\nif (bitmap_equal(mask, av_masks, masklength))\r\nreturn av_masks;\r\n} else {\r\nif (bitmap_subset(mask, av_masks, masklength))\r\nreturn av_masks;\r\n}\r\nav_masks += BITS_TO_LONGS(masklength);\r\n}\r\nreturn NULL;\r\n}\r\nstatic bool iio_validate_scan_mask(struct iio_dev *indio_dev,\r\nconst unsigned long *mask)\r\n{\r\nif (!indio_dev->setup_ops->validate_scan_mask)\r\nreturn true;\r\nreturn indio_dev->setup_ops->validate_scan_mask(indio_dev, mask);\r\n}\r\nstatic int iio_scan_mask_set(struct iio_dev *indio_dev,\r\nstruct iio_buffer *buffer, int bit)\r\n{\r\nconst unsigned long *mask;\r\nunsigned long *trialmask;\r\ntrialmask = kmalloc(sizeof(*trialmask)*\r\nBITS_TO_LONGS(indio_dev->masklength),\r\nGFP_KERNEL);\r\nif (trialmask == NULL)\r\nreturn -ENOMEM;\r\nif (!indio_dev->masklength) {\r\nWARN(1, "Trying to set scanmask prior to registering buffer\n");\r\ngoto err_invalid_mask;\r\n}\r\nbitmap_copy(trialmask, buffer->scan_mask, indio_dev->masklength);\r\nset_bit(bit, trialmask);\r\nif (!iio_validate_scan_mask(indio_dev, trialmask))\r\ngoto err_invalid_mask;\r\nif (indio_dev->available_scan_masks) {\r\nmask = iio_scan_mask_match(indio_dev->available_scan_masks,\r\nindio_dev->masklength,\r\ntrialmask, false);\r\nif (!mask)\r\ngoto err_invalid_mask;\r\n}\r\nbitmap_copy(buffer->scan_mask, trialmask, indio_dev->masklength);\r\nkfree(trialmask);\r\nreturn 0;\r\nerr_invalid_mask:\r\nkfree(trialmask);\r\nreturn -EINVAL;\r\n}\r\nstatic int iio_scan_mask_clear(struct iio_buffer *buffer, int bit)\r\n{\r\nclear_bit(bit, buffer->scan_mask);\r\nreturn 0;\r\n}\r\nstatic ssize_t iio_scan_el_store(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf,\r\nsize_t len)\r\n{\r\nint ret;\r\nbool state;\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nstruct iio_buffer *buffer = indio_dev->buffer;\r\nstruct iio_dev_attr *this_attr = to_iio_dev_attr(attr);\r\nret = strtobool(buf, &state);\r\nif (ret < 0)\r\nreturn ret;\r\nmutex_lock(&indio_dev->mlock);\r\nif (iio_buffer_is_active(indio_dev->buffer)) {\r\nret = -EBUSY;\r\ngoto error_ret;\r\n}\r\nret = iio_scan_mask_query(indio_dev, buffer, this_attr->address);\r\nif (ret < 0)\r\ngoto error_ret;\r\nif (!state && ret) {\r\nret = iio_scan_mask_clear(buffer, this_attr->address);\r\nif (ret)\r\ngoto error_ret;\r\n} else if (state && !ret) {\r\nret = iio_scan_mask_set(indio_dev, buffer, this_attr->address);\r\nif (ret)\r\ngoto error_ret;\r\n}\r\nerror_ret:\r\nmutex_unlock(&indio_dev->mlock);\r\nreturn ret < 0 ? ret : len;\r\n}\r\nstatic ssize_t iio_scan_el_ts_show(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nreturn sprintf(buf, "%d\n", indio_dev->buffer->scan_timestamp);\r\n}\r\nstatic ssize_t iio_scan_el_ts_store(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf,\r\nsize_t len)\r\n{\r\nint ret;\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nbool state;\r\nret = strtobool(buf, &state);\r\nif (ret < 0)\r\nreturn ret;\r\nmutex_lock(&indio_dev->mlock);\r\nif (iio_buffer_is_active(indio_dev->buffer)) {\r\nret = -EBUSY;\r\ngoto error_ret;\r\n}\r\nindio_dev->buffer->scan_timestamp = state;\r\nerror_ret:\r\nmutex_unlock(&indio_dev->mlock);\r\nreturn ret ? ret : len;\r\n}\r\nstatic int iio_buffer_add_channel_sysfs(struct iio_dev *indio_dev,\r\nconst struct iio_chan_spec *chan)\r\n{\r\nint ret, attrcount = 0;\r\nstruct iio_buffer *buffer = indio_dev->buffer;\r\nret = __iio_add_chan_devattr("index",\r\nchan,\r\n&iio_show_scan_index,\r\nNULL,\r\n0,\r\nIIO_SEPARATE,\r\n&indio_dev->dev,\r\n&buffer->scan_el_dev_attr_list);\r\nif (ret)\r\nreturn ret;\r\nattrcount++;\r\nret = __iio_add_chan_devattr("type",\r\nchan,\r\n&iio_show_fixed_type,\r\nNULL,\r\n0,\r\n0,\r\n&indio_dev->dev,\r\n&buffer->scan_el_dev_attr_list);\r\nif (ret)\r\nreturn ret;\r\nattrcount++;\r\nif (chan->type != IIO_TIMESTAMP)\r\nret = __iio_add_chan_devattr("en",\r\nchan,\r\n&iio_scan_el_show,\r\n&iio_scan_el_store,\r\nchan->scan_index,\r\n0,\r\n&indio_dev->dev,\r\n&buffer->scan_el_dev_attr_list);\r\nelse\r\nret = __iio_add_chan_devattr("en",\r\nchan,\r\n&iio_scan_el_ts_show,\r\n&iio_scan_el_ts_store,\r\nchan->scan_index,\r\n0,\r\n&indio_dev->dev,\r\n&buffer->scan_el_dev_attr_list);\r\nif (ret)\r\nreturn ret;\r\nattrcount++;\r\nret = attrcount;\r\nreturn ret;\r\n}\r\nstatic ssize_t iio_buffer_read_length(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nstruct iio_buffer *buffer = indio_dev->buffer;\r\nreturn sprintf(buf, "%d\n", buffer->length);\r\n}\r\nstatic ssize_t iio_buffer_write_length(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t len)\r\n{\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nstruct iio_buffer *buffer = indio_dev->buffer;\r\nunsigned int val;\r\nint ret;\r\nret = kstrtouint(buf, 10, &val);\r\nif (ret)\r\nreturn ret;\r\nif (val == buffer->length)\r\nreturn len;\r\nmutex_lock(&indio_dev->mlock);\r\nif (iio_buffer_is_active(indio_dev->buffer)) {\r\nret = -EBUSY;\r\n} else {\r\nbuffer->access->set_length(buffer, val);\r\nret = 0;\r\n}\r\nif (ret)\r\ngoto out;\r\nif (buffer->length && buffer->length < buffer->watermark)\r\nbuffer->watermark = buffer->length;\r\nout:\r\nmutex_unlock(&indio_dev->mlock);\r\nreturn ret ? ret : len;\r\n}\r\nstatic ssize_t iio_buffer_show_enable(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nreturn sprintf(buf, "%d\n", iio_buffer_is_active(indio_dev->buffer));\r\n}\r\nstatic unsigned int iio_storage_bytes_for_si(struct iio_dev *indio_dev,\r\nunsigned int scan_index)\r\n{\r\nconst struct iio_chan_spec *ch;\r\nunsigned int bytes;\r\nch = iio_find_channel_from_si(indio_dev, scan_index);\r\nbytes = ch->scan_type.storagebits / 8;\r\nif (ch->scan_type.repeat > 1)\r\nbytes *= ch->scan_type.repeat;\r\nreturn bytes;\r\n}\r\nstatic unsigned int iio_storage_bytes_for_timestamp(struct iio_dev *indio_dev)\r\n{\r\nreturn iio_storage_bytes_for_si(indio_dev,\r\nindio_dev->scan_index_timestamp);\r\n}\r\nstatic int iio_compute_scan_bytes(struct iio_dev *indio_dev,\r\nconst unsigned long *mask, bool timestamp)\r\n{\r\nunsigned bytes = 0;\r\nint length, i;\r\nfor_each_set_bit(i, mask,\r\nindio_dev->masklength) {\r\nlength = iio_storage_bytes_for_si(indio_dev, i);\r\nbytes = ALIGN(bytes, length);\r\nbytes += length;\r\n}\r\nif (timestamp) {\r\nlength = iio_storage_bytes_for_timestamp(indio_dev);\r\nbytes = ALIGN(bytes, length);\r\nbytes += length;\r\n}\r\nreturn bytes;\r\n}\r\nstatic void iio_buffer_activate(struct iio_dev *indio_dev,\r\nstruct iio_buffer *buffer)\r\n{\r\niio_buffer_get(buffer);\r\nlist_add(&buffer->buffer_list, &indio_dev->buffer_list);\r\n}\r\nstatic void iio_buffer_deactivate(struct iio_buffer *buffer)\r\n{\r\nlist_del_init(&buffer->buffer_list);\r\nwake_up_interruptible(&buffer->pollq);\r\niio_buffer_put(buffer);\r\n}\r\nstatic void iio_buffer_deactivate_all(struct iio_dev *indio_dev)\r\n{\r\nstruct iio_buffer *buffer, *_buffer;\r\nlist_for_each_entry_safe(buffer, _buffer,\r\n&indio_dev->buffer_list, buffer_list)\r\niio_buffer_deactivate(buffer);\r\n}\r\nstatic int iio_buffer_enable(struct iio_buffer *buffer,\r\nstruct iio_dev *indio_dev)\r\n{\r\nif (!buffer->access->enable)\r\nreturn 0;\r\nreturn buffer->access->enable(buffer, indio_dev);\r\n}\r\nstatic int iio_buffer_disable(struct iio_buffer *buffer,\r\nstruct iio_dev *indio_dev)\r\n{\r\nif (!buffer->access->disable)\r\nreturn 0;\r\nreturn buffer->access->disable(buffer, indio_dev);\r\n}\r\nstatic void iio_buffer_update_bytes_per_datum(struct iio_dev *indio_dev,\r\nstruct iio_buffer *buffer)\r\n{\r\nunsigned int bytes;\r\nif (!buffer->access->set_bytes_per_datum)\r\nreturn;\r\nbytes = iio_compute_scan_bytes(indio_dev, buffer->scan_mask,\r\nbuffer->scan_timestamp);\r\nbuffer->access->set_bytes_per_datum(buffer, bytes);\r\n}\r\nstatic int iio_buffer_request_update(struct iio_dev *indio_dev,\r\nstruct iio_buffer *buffer)\r\n{\r\nint ret;\r\niio_buffer_update_bytes_per_datum(indio_dev, buffer);\r\nif (buffer->access->request_update) {\r\nret = buffer->access->request_update(buffer);\r\nif (ret) {\r\ndev_dbg(&indio_dev->dev,\r\n"Buffer not started: buffer parameter update failed (%d)\n",\r\nret);\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void iio_free_scan_mask(struct iio_dev *indio_dev,\r\nconst unsigned long *mask)\r\n{\r\nif (!indio_dev->available_scan_masks)\r\nkfree(mask);\r\n}\r\nstatic int iio_verify_update(struct iio_dev *indio_dev,\r\nstruct iio_buffer *insert_buffer, struct iio_buffer *remove_buffer,\r\nstruct iio_device_config *config)\r\n{\r\nunsigned long *compound_mask;\r\nconst unsigned long *scan_mask;\r\nbool strict_scanmask = false;\r\nstruct iio_buffer *buffer;\r\nbool scan_timestamp;\r\nunsigned int modes;\r\nmemset(config, 0, sizeof(*config));\r\nconfig->watermark = ~0;\r\nif (remove_buffer && !insert_buffer &&\r\nlist_is_singular(&indio_dev->buffer_list))\r\nreturn 0;\r\nmodes = indio_dev->modes;\r\nlist_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {\r\nif (buffer == remove_buffer)\r\ncontinue;\r\nmodes &= buffer->access->modes;\r\nconfig->watermark = min(config->watermark, buffer->watermark);\r\n}\r\nif (insert_buffer) {\r\nmodes &= insert_buffer->access->modes;\r\nconfig->watermark = min(config->watermark,\r\ninsert_buffer->watermark);\r\n}\r\nif ((modes & INDIO_BUFFER_TRIGGERED) && indio_dev->trig) {\r\nconfig->mode = INDIO_BUFFER_TRIGGERED;\r\n} else if (modes & INDIO_BUFFER_HARDWARE) {\r\nif (insert_buffer && !list_empty(&indio_dev->buffer_list))\r\nreturn -EINVAL;\r\nconfig->mode = INDIO_BUFFER_HARDWARE;\r\nstrict_scanmask = true;\r\n} else if (modes & INDIO_BUFFER_SOFTWARE) {\r\nconfig->mode = INDIO_BUFFER_SOFTWARE;\r\n} else {\r\nif (indio_dev->modes & INDIO_BUFFER_TRIGGERED)\r\ndev_dbg(&indio_dev->dev, "Buffer not started: no trigger\n");\r\nreturn -EINVAL;\r\n}\r\ncompound_mask = kcalloc(BITS_TO_LONGS(indio_dev->masklength),\r\nsizeof(long), GFP_KERNEL);\r\nif (compound_mask == NULL)\r\nreturn -ENOMEM;\r\nscan_timestamp = false;\r\nlist_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {\r\nif (buffer == remove_buffer)\r\ncontinue;\r\nbitmap_or(compound_mask, compound_mask, buffer->scan_mask,\r\nindio_dev->masklength);\r\nscan_timestamp |= buffer->scan_timestamp;\r\n}\r\nif (insert_buffer) {\r\nbitmap_or(compound_mask, compound_mask,\r\ninsert_buffer->scan_mask, indio_dev->masklength);\r\nscan_timestamp |= insert_buffer->scan_timestamp;\r\n}\r\nif (indio_dev->available_scan_masks) {\r\nscan_mask = iio_scan_mask_match(indio_dev->available_scan_masks,\r\nindio_dev->masklength,\r\ncompound_mask,\r\nstrict_scanmask);\r\nkfree(compound_mask);\r\nif (scan_mask == NULL)\r\nreturn -EINVAL;\r\n} else {\r\nscan_mask = compound_mask;\r\n}\r\nconfig->scan_bytes = iio_compute_scan_bytes(indio_dev,\r\nscan_mask, scan_timestamp);\r\nconfig->scan_mask = scan_mask;\r\nconfig->scan_timestamp = scan_timestamp;\r\nreturn 0;\r\n}\r\nstatic int iio_enable_buffers(struct iio_dev *indio_dev,\r\nstruct iio_device_config *config)\r\n{\r\nstruct iio_buffer *buffer;\r\nint ret;\r\nindio_dev->active_scan_mask = config->scan_mask;\r\nindio_dev->scan_timestamp = config->scan_timestamp;\r\nindio_dev->scan_bytes = config->scan_bytes;\r\niio_update_demux(indio_dev);\r\nif (indio_dev->setup_ops->preenable) {\r\nret = indio_dev->setup_ops->preenable(indio_dev);\r\nif (ret) {\r\ndev_dbg(&indio_dev->dev,\r\n"Buffer not started: buffer preenable failed (%d)\n", ret);\r\ngoto err_undo_config;\r\n}\r\n}\r\nif (indio_dev->info->update_scan_mode) {\r\nret = indio_dev->info\r\n->update_scan_mode(indio_dev,\r\nindio_dev->active_scan_mask);\r\nif (ret < 0) {\r\ndev_dbg(&indio_dev->dev,\r\n"Buffer not started: update scan mode failed (%d)\n",\r\nret);\r\ngoto err_run_postdisable;\r\n}\r\n}\r\nif (indio_dev->info->hwfifo_set_watermark)\r\nindio_dev->info->hwfifo_set_watermark(indio_dev,\r\nconfig->watermark);\r\nlist_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {\r\nret = iio_buffer_enable(buffer, indio_dev);\r\nif (ret)\r\ngoto err_disable_buffers;\r\n}\r\nindio_dev->currentmode = config->mode;\r\nif (indio_dev->setup_ops->postenable) {\r\nret = indio_dev->setup_ops->postenable(indio_dev);\r\nif (ret) {\r\ndev_dbg(&indio_dev->dev,\r\n"Buffer not started: postenable failed (%d)\n", ret);\r\ngoto err_disable_buffers;\r\n}\r\n}\r\nreturn 0;\r\nerr_disable_buffers:\r\nlist_for_each_entry_continue_reverse(buffer, &indio_dev->buffer_list,\r\nbuffer_list)\r\niio_buffer_disable(buffer, indio_dev);\r\nerr_run_postdisable:\r\nindio_dev->currentmode = INDIO_DIRECT_MODE;\r\nif (indio_dev->setup_ops->postdisable)\r\nindio_dev->setup_ops->postdisable(indio_dev);\r\nerr_undo_config:\r\nindio_dev->active_scan_mask = NULL;\r\nreturn ret;\r\n}\r\nstatic int iio_disable_buffers(struct iio_dev *indio_dev)\r\n{\r\nstruct iio_buffer *buffer;\r\nint ret = 0;\r\nint ret2;\r\nif (list_empty(&indio_dev->buffer_list))\r\nreturn 0;\r\nif (indio_dev->setup_ops->predisable) {\r\nret2 = indio_dev->setup_ops->predisable(indio_dev);\r\nif (ret2 && !ret)\r\nret = ret2;\r\n}\r\nlist_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {\r\nret2 = iio_buffer_disable(buffer, indio_dev);\r\nif (ret2 && !ret)\r\nret = ret2;\r\n}\r\nindio_dev->currentmode = INDIO_DIRECT_MODE;\r\nif (indio_dev->setup_ops->postdisable) {\r\nret2 = indio_dev->setup_ops->postdisable(indio_dev);\r\nif (ret2 && !ret)\r\nret = ret2;\r\n}\r\niio_free_scan_mask(indio_dev, indio_dev->active_scan_mask);\r\nindio_dev->active_scan_mask = NULL;\r\nreturn ret;\r\n}\r\nstatic int __iio_update_buffers(struct iio_dev *indio_dev,\r\nstruct iio_buffer *insert_buffer,\r\nstruct iio_buffer *remove_buffer)\r\n{\r\nstruct iio_device_config new_config;\r\nint ret;\r\nret = iio_verify_update(indio_dev, insert_buffer, remove_buffer,\r\n&new_config);\r\nif (ret)\r\nreturn ret;\r\nif (insert_buffer) {\r\nret = iio_buffer_request_update(indio_dev, insert_buffer);\r\nif (ret)\r\ngoto err_free_config;\r\n}\r\nret = iio_disable_buffers(indio_dev);\r\nif (ret)\r\ngoto err_deactivate_all;\r\nif (remove_buffer)\r\niio_buffer_deactivate(remove_buffer);\r\nif (insert_buffer)\r\niio_buffer_activate(indio_dev, insert_buffer);\r\nif (list_empty(&indio_dev->buffer_list))\r\nreturn 0;\r\nret = iio_enable_buffers(indio_dev, &new_config);\r\nif (ret)\r\ngoto err_deactivate_all;\r\nreturn 0;\r\nerr_deactivate_all:\r\niio_buffer_deactivate_all(indio_dev);\r\nerr_free_config:\r\niio_free_scan_mask(indio_dev, new_config.scan_mask);\r\nreturn ret;\r\n}\r\nint iio_update_buffers(struct iio_dev *indio_dev,\r\nstruct iio_buffer *insert_buffer,\r\nstruct iio_buffer *remove_buffer)\r\n{\r\nint ret;\r\nif (insert_buffer == remove_buffer)\r\nreturn 0;\r\nmutex_lock(&indio_dev->info_exist_lock);\r\nmutex_lock(&indio_dev->mlock);\r\nif (insert_buffer && iio_buffer_is_active(insert_buffer))\r\ninsert_buffer = NULL;\r\nif (remove_buffer && !iio_buffer_is_active(remove_buffer))\r\nremove_buffer = NULL;\r\nif (!insert_buffer && !remove_buffer) {\r\nret = 0;\r\ngoto out_unlock;\r\n}\r\nif (indio_dev->info == NULL) {\r\nret = -ENODEV;\r\ngoto out_unlock;\r\n}\r\nret = __iio_update_buffers(indio_dev, insert_buffer, remove_buffer);\r\nout_unlock:\r\nmutex_unlock(&indio_dev->mlock);\r\nmutex_unlock(&indio_dev->info_exist_lock);\r\nreturn ret;\r\n}\r\nvoid iio_disable_all_buffers(struct iio_dev *indio_dev)\r\n{\r\niio_disable_buffers(indio_dev);\r\niio_buffer_deactivate_all(indio_dev);\r\n}\r\nstatic ssize_t iio_buffer_store_enable(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf,\r\nsize_t len)\r\n{\r\nint ret;\r\nbool requested_state;\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nbool inlist;\r\nret = strtobool(buf, &requested_state);\r\nif (ret < 0)\r\nreturn ret;\r\nmutex_lock(&indio_dev->mlock);\r\ninlist = iio_buffer_is_active(indio_dev->buffer);\r\nif (inlist == requested_state)\r\ngoto done;\r\nif (requested_state)\r\nret = __iio_update_buffers(indio_dev,\r\nindio_dev->buffer, NULL);\r\nelse\r\nret = __iio_update_buffers(indio_dev,\r\nNULL, indio_dev->buffer);\r\ndone:\r\nmutex_unlock(&indio_dev->mlock);\r\nreturn (ret < 0) ? ret : len;\r\n}\r\nstatic ssize_t iio_buffer_show_watermark(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nstruct iio_buffer *buffer = indio_dev->buffer;\r\nreturn sprintf(buf, "%u\n", buffer->watermark);\r\n}\r\nstatic ssize_t iio_buffer_store_watermark(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf,\r\nsize_t len)\r\n{\r\nstruct iio_dev *indio_dev = dev_to_iio_dev(dev);\r\nstruct iio_buffer *buffer = indio_dev->buffer;\r\nunsigned int val;\r\nint ret;\r\nret = kstrtouint(buf, 10, &val);\r\nif (ret)\r\nreturn ret;\r\nif (!val)\r\nreturn -EINVAL;\r\nmutex_lock(&indio_dev->mlock);\r\nif (val > buffer->length) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (iio_buffer_is_active(indio_dev->buffer)) {\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\nbuffer->watermark = val;\r\nout:\r\nmutex_unlock(&indio_dev->mlock);\r\nreturn ret ? ret : len;\r\n}\r\nint iio_buffer_alloc_sysfs_and_mask(struct iio_dev *indio_dev)\r\n{\r\nstruct iio_dev_attr *p;\r\nstruct attribute **attr;\r\nstruct iio_buffer *buffer = indio_dev->buffer;\r\nint ret, i, attrn, attrcount, attrcount_orig = 0;\r\nconst struct iio_chan_spec *channels;\r\nchannels = indio_dev->channels;\r\nif (channels) {\r\nint ml = indio_dev->masklength;\r\nfor (i = 0; i < indio_dev->num_channels; i++)\r\nml = max(ml, channels[i].scan_index + 1);\r\nindio_dev->masklength = ml;\r\n}\r\nif (!buffer)\r\nreturn 0;\r\nattrcount = 0;\r\nif (buffer->attrs) {\r\nwhile (buffer->attrs[attrcount] != NULL)\r\nattrcount++;\r\n}\r\nattr = kcalloc(attrcount + ARRAY_SIZE(iio_buffer_attrs) + 1,\r\nsizeof(struct attribute *), GFP_KERNEL);\r\nif (!attr)\r\nreturn -ENOMEM;\r\nmemcpy(attr, iio_buffer_attrs, sizeof(iio_buffer_attrs));\r\nif (!buffer->access->set_length)\r\nattr[0] = &dev_attr_length_ro.attr;\r\nif (buffer->access->flags & INDIO_BUFFER_FLAG_FIXED_WATERMARK)\r\nattr[2] = &dev_attr_watermark_ro.attr;\r\nif (buffer->attrs)\r\nmemcpy(&attr[ARRAY_SIZE(iio_buffer_attrs)], buffer->attrs,\r\nsizeof(struct attribute *) * attrcount);\r\nattr[attrcount + ARRAY_SIZE(iio_buffer_attrs)] = NULL;\r\nbuffer->buffer_group.name = "buffer";\r\nbuffer->buffer_group.attrs = attr;\r\nindio_dev->groups[indio_dev->groupcounter++] = &buffer->buffer_group;\r\nif (buffer->scan_el_attrs != NULL) {\r\nattr = buffer->scan_el_attrs->attrs;\r\nwhile (*attr++ != NULL)\r\nattrcount_orig++;\r\n}\r\nattrcount = attrcount_orig;\r\nINIT_LIST_HEAD(&buffer->scan_el_dev_attr_list);\r\nchannels = indio_dev->channels;\r\nif (channels) {\r\nfor (i = 0; i < indio_dev->num_channels; i++) {\r\nif (channels[i].scan_index < 0)\r\ncontinue;\r\nret = iio_buffer_add_channel_sysfs(indio_dev,\r\n&channels[i]);\r\nif (ret < 0)\r\ngoto error_cleanup_dynamic;\r\nattrcount += ret;\r\nif (channels[i].type == IIO_TIMESTAMP)\r\nindio_dev->scan_index_timestamp =\r\nchannels[i].scan_index;\r\n}\r\nif (indio_dev->masklength && buffer->scan_mask == NULL) {\r\nbuffer->scan_mask = kcalloc(BITS_TO_LONGS(indio_dev->masklength),\r\nsizeof(*buffer->scan_mask),\r\nGFP_KERNEL);\r\nif (buffer->scan_mask == NULL) {\r\nret = -ENOMEM;\r\ngoto error_cleanup_dynamic;\r\n}\r\n}\r\n}\r\nbuffer->scan_el_group.name = iio_scan_elements_group_name;\r\nbuffer->scan_el_group.attrs = kcalloc(attrcount + 1,\r\nsizeof(buffer->scan_el_group.attrs[0]),\r\nGFP_KERNEL);\r\nif (buffer->scan_el_group.attrs == NULL) {\r\nret = -ENOMEM;\r\ngoto error_free_scan_mask;\r\n}\r\nif (buffer->scan_el_attrs)\r\nmemcpy(buffer->scan_el_group.attrs, buffer->scan_el_attrs,\r\nsizeof(buffer->scan_el_group.attrs[0])*attrcount_orig);\r\nattrn = attrcount_orig;\r\nlist_for_each_entry(p, &buffer->scan_el_dev_attr_list, l)\r\nbuffer->scan_el_group.attrs[attrn++] = &p->dev_attr.attr;\r\nindio_dev->groups[indio_dev->groupcounter++] = &buffer->scan_el_group;\r\nreturn 0;\r\nerror_free_scan_mask:\r\nkfree(buffer->scan_mask);\r\nerror_cleanup_dynamic:\r\niio_free_chan_devattr_list(&buffer->scan_el_dev_attr_list);\r\nkfree(indio_dev->buffer->buffer_group.attrs);\r\nreturn ret;\r\n}\r\nvoid iio_buffer_free_sysfs_and_mask(struct iio_dev *indio_dev)\r\n{\r\nif (!indio_dev->buffer)\r\nreturn;\r\nkfree(indio_dev->buffer->scan_mask);\r\nkfree(indio_dev->buffer->buffer_group.attrs);\r\nkfree(indio_dev->buffer->scan_el_group.attrs);\r\niio_free_chan_devattr_list(&indio_dev->buffer->scan_el_dev_attr_list);\r\n}\r\nbool iio_validate_scan_mask_onehot(struct iio_dev *indio_dev,\r\nconst unsigned long *mask)\r\n{\r\nreturn bitmap_weight(mask, indio_dev->masklength) == 1;\r\n}\r\nint iio_scan_mask_query(struct iio_dev *indio_dev,\r\nstruct iio_buffer *buffer, int bit)\r\n{\r\nif (bit > indio_dev->masklength)\r\nreturn -EINVAL;\r\nif (!buffer->scan_mask)\r\nreturn 0;\r\nreturn !!test_bit(bit, buffer->scan_mask);\r\n}\r\nstatic const void *iio_demux(struct iio_buffer *buffer,\r\nconst void *datain)\r\n{\r\nstruct iio_demux_table *t;\r\nif (list_empty(&buffer->demux_list))\r\nreturn datain;\r\nlist_for_each_entry(t, &buffer->demux_list, l)\r\nmemcpy(buffer->demux_bounce + t->to,\r\ndatain + t->from, t->length);\r\nreturn buffer->demux_bounce;\r\n}\r\nstatic int iio_push_to_buffer(struct iio_buffer *buffer, const void *data)\r\n{\r\nconst void *dataout = iio_demux(buffer, data);\r\nint ret;\r\nret = buffer->access->store_to(buffer, dataout);\r\nif (ret)\r\nreturn ret;\r\nwake_up_interruptible_poll(&buffer->pollq, POLLIN | POLLRDNORM);\r\nreturn 0;\r\n}\r\nstatic void iio_buffer_demux_free(struct iio_buffer *buffer)\r\n{\r\nstruct iio_demux_table *p, *q;\r\nlist_for_each_entry_safe(p, q, &buffer->demux_list, l) {\r\nlist_del(&p->l);\r\nkfree(p);\r\n}\r\n}\r\nint iio_push_to_buffers(struct iio_dev *indio_dev, const void *data)\r\n{\r\nint ret;\r\nstruct iio_buffer *buf;\r\nlist_for_each_entry(buf, &indio_dev->buffer_list, buffer_list) {\r\nret = iio_push_to_buffer(buf, data);\r\nif (ret < 0)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int iio_buffer_add_demux(struct iio_buffer *buffer,\r\nstruct iio_demux_table **p, unsigned int in_loc, unsigned int out_loc,\r\nunsigned int length)\r\n{\r\nif (*p && (*p)->from + (*p)->length == in_loc &&\r\n(*p)->to + (*p)->length == out_loc) {\r\n(*p)->length += length;\r\n} else {\r\n*p = kmalloc(sizeof(**p), GFP_KERNEL);\r\nif (*p == NULL)\r\nreturn -ENOMEM;\r\n(*p)->from = in_loc;\r\n(*p)->to = out_loc;\r\n(*p)->length = length;\r\nlist_add_tail(&(*p)->l, &buffer->demux_list);\r\n}\r\nreturn 0;\r\n}\r\nstatic int iio_buffer_update_demux(struct iio_dev *indio_dev,\r\nstruct iio_buffer *buffer)\r\n{\r\nint ret, in_ind = -1, out_ind, length;\r\nunsigned in_loc = 0, out_loc = 0;\r\nstruct iio_demux_table *p = NULL;\r\niio_buffer_demux_free(buffer);\r\nkfree(buffer->demux_bounce);\r\nbuffer->demux_bounce = NULL;\r\nif (bitmap_equal(indio_dev->active_scan_mask,\r\nbuffer->scan_mask,\r\nindio_dev->masklength))\r\nreturn 0;\r\nfor_each_set_bit(out_ind,\r\nbuffer->scan_mask,\r\nindio_dev->masklength) {\r\nin_ind = find_next_bit(indio_dev->active_scan_mask,\r\nindio_dev->masklength,\r\nin_ind + 1);\r\nwhile (in_ind != out_ind) {\r\nin_ind = find_next_bit(indio_dev->active_scan_mask,\r\nindio_dev->masklength,\r\nin_ind + 1);\r\nlength = iio_storage_bytes_for_si(indio_dev, in_ind);\r\nin_loc = roundup(in_loc, length) + length;\r\n}\r\nlength = iio_storage_bytes_for_si(indio_dev, in_ind);\r\nout_loc = roundup(out_loc, length);\r\nin_loc = roundup(in_loc, length);\r\nret = iio_buffer_add_demux(buffer, &p, in_loc, out_loc, length);\r\nif (ret)\r\ngoto error_clear_mux_table;\r\nout_loc += length;\r\nin_loc += length;\r\n}\r\nif (buffer->scan_timestamp) {\r\nlength = iio_storage_bytes_for_timestamp(indio_dev);\r\nout_loc = roundup(out_loc, length);\r\nin_loc = roundup(in_loc, length);\r\nret = iio_buffer_add_demux(buffer, &p, in_loc, out_loc, length);\r\nif (ret)\r\ngoto error_clear_mux_table;\r\nout_loc += length;\r\nin_loc += length;\r\n}\r\nbuffer->demux_bounce = kzalloc(out_loc, GFP_KERNEL);\r\nif (buffer->demux_bounce == NULL) {\r\nret = -ENOMEM;\r\ngoto error_clear_mux_table;\r\n}\r\nreturn 0;\r\nerror_clear_mux_table:\r\niio_buffer_demux_free(buffer);\r\nreturn ret;\r\n}\r\nint iio_update_demux(struct iio_dev *indio_dev)\r\n{\r\nstruct iio_buffer *buffer;\r\nint ret;\r\nlist_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list) {\r\nret = iio_buffer_update_demux(indio_dev, buffer);\r\nif (ret < 0)\r\ngoto error_clear_mux_table;\r\n}\r\nreturn 0;\r\nerror_clear_mux_table:\r\nlist_for_each_entry(buffer, &indio_dev->buffer_list, buffer_list)\r\niio_buffer_demux_free(buffer);\r\nreturn ret;\r\n}\r\nstatic void iio_buffer_release(struct kref *ref)\r\n{\r\nstruct iio_buffer *buffer = container_of(ref, struct iio_buffer, ref);\r\nbuffer->access->release(buffer);\r\n}\r\nstruct iio_buffer *iio_buffer_get(struct iio_buffer *buffer)\r\n{\r\nif (buffer)\r\nkref_get(&buffer->ref);\r\nreturn buffer;\r\n}\r\nvoid iio_buffer_put(struct iio_buffer *buffer)\r\n{\r\nif (buffer)\r\nkref_put(&buffer->ref, iio_buffer_release);\r\n}
