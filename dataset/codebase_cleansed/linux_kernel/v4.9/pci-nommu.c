static int\r\ncheck_addr(char *name, struct device *hwdev, dma_addr_t bus, size_t size)\r\n{\r\nif (hwdev && !dma_capable(hwdev, bus, size)) {\r\nif (*hwdev->dma_mask >= DMA_BIT_MASK(32))\r\nprintk(KERN_ERR\r\n"nommu_%s: overflow %Lx+%zu of device mask %Lx\n",\r\nname, (long long)bus, size,\r\n(long long)*hwdev->dma_mask);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic dma_addr_t nommu_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\ndma_addr_t bus = page_to_phys(page) + offset;\r\nWARN_ON(size == 0);\r\nif (!check_addr("map_single", dev, bus, size))\r\nreturn DMA_ERROR_CODE;\r\nflush_write_buffers();\r\nreturn bus;\r\n}\r\nstatic int nommu_map_sg(struct device *hwdev, struct scatterlist *sg,\r\nint nents, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nstruct scatterlist *s;\r\nint i;\r\nWARN_ON(nents == 0 || sg[0].length == 0);\r\nfor_each_sg(sg, s, nents, i) {\r\nBUG_ON(!sg_page(s));\r\ns->dma_address = sg_phys(s);\r\nif (!check_addr("map_sg", hwdev, s->dma_address, s->length))\r\nreturn 0;\r\ns->dma_length = s->length;\r\n}\r\nflush_write_buffers();\r\nreturn nents;\r\n}\r\nstatic void nommu_sync_single_for_device(struct device *dev,\r\ndma_addr_t addr, size_t size,\r\nenum dma_data_direction dir)\r\n{\r\nflush_write_buffers();\r\n}\r\nstatic void nommu_sync_sg_for_device(struct device *dev,\r\nstruct scatterlist *sg, int nelems,\r\nenum dma_data_direction dir)\r\n{\r\nflush_write_buffers();\r\n}
