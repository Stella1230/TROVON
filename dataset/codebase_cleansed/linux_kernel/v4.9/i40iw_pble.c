void i40iw_destroy_pble_pool(struct i40iw_sc_dev *dev, struct i40iw_hmc_pble_rsrc *pble_rsrc)\r\n{\r\nstruct list_head *clist;\r\nstruct list_head *tlist;\r\nstruct i40iw_chunk *chunk;\r\nstruct i40iw_pble_pool *pinfo = &pble_rsrc->pinfo;\r\nif (pinfo->pool) {\r\nlist_for_each_safe(clist, tlist, &pinfo->clist) {\r\nchunk = list_entry(clist, struct i40iw_chunk, list);\r\nif (chunk->type == I40IW_VMALLOC)\r\ni40iw_free_vmalloc_mem(dev->hw, chunk);\r\nkfree(chunk);\r\n}\r\ngen_pool_destroy(pinfo->pool);\r\n}\r\n}\r\nenum i40iw_status_code i40iw_hmc_init_pble(struct i40iw_sc_dev *dev,\r\nstruct i40iw_hmc_pble_rsrc *pble_rsrc)\r\n{\r\nstruct i40iw_hmc_info *hmc_info;\r\nu32 fpm_idx = 0;\r\nhmc_info = dev->hmc_info;\r\npble_rsrc->fpm_base_addr = hmc_info->hmc_obj[I40IW_HMC_IW_PBLE].base;\r\nif (pble_rsrc->fpm_base_addr & 0xfff)\r\nfpm_idx = (PAGE_SIZE - (pble_rsrc->fpm_base_addr & 0xfff)) >> 3;\r\npble_rsrc->unallocated_pble =\r\nhmc_info->hmc_obj[I40IW_HMC_IW_PBLE].cnt - fpm_idx;\r\npble_rsrc->next_fpm_addr = pble_rsrc->fpm_base_addr + (fpm_idx << 3);\r\npble_rsrc->pinfo.pool_shift = POOL_SHIFT;\r\npble_rsrc->pinfo.pool = gen_pool_create(pble_rsrc->pinfo.pool_shift, -1);\r\nINIT_LIST_HEAD(&pble_rsrc->pinfo.clist);\r\nif (!pble_rsrc->pinfo.pool)\r\ngoto error;\r\nif (add_pble_pool(dev, pble_rsrc))\r\ngoto error;\r\nreturn 0;\r\nerror:i40iw_destroy_pble_pool(dev, pble_rsrc);\r\nreturn I40IW_ERR_NO_MEMORY;\r\n}\r\nstatic inline void get_sd_pd_idx(struct i40iw_hmc_pble_rsrc *pble_rsrc,\r\nstruct sd_pd_idx *idx)\r\n{\r\nidx->sd_idx = (u32)(pble_rsrc->next_fpm_addr) / I40IW_HMC_DIRECT_BP_SIZE;\r\nidx->pd_idx = (u32)(pble_rsrc->next_fpm_addr) / I40IW_HMC_PAGED_BP_SIZE;\r\nidx->rel_pd_idx = (idx->pd_idx % I40IW_HMC_PD_CNT_IN_SD);\r\n}\r\nstatic enum i40iw_status_code add_sd_direct(struct i40iw_sc_dev *dev,\r\nstruct i40iw_hmc_pble_rsrc *pble_rsrc,\r\nstruct i40iw_add_page_info *info)\r\n{\r\nenum i40iw_status_code ret_code = 0;\r\nstruct sd_pd_idx *idx = &info->idx;\r\nstruct i40iw_chunk *chunk = info->chunk;\r\nstruct i40iw_hmc_info *hmc_info = info->hmc_info;\r\nstruct i40iw_hmc_sd_entry *sd_entry = info->sd_entry;\r\nu32 offset = 0;\r\nif (!sd_entry->valid) {\r\nif (dev->is_pf) {\r\nret_code = i40iw_add_sd_table_entry(dev->hw, hmc_info,\r\ninfo->idx.sd_idx,\r\nI40IW_SD_TYPE_DIRECT,\r\nI40IW_HMC_DIRECT_BP_SIZE);\r\nif (ret_code)\r\nreturn ret_code;\r\nchunk->type = I40IW_DMA_COHERENT;\r\n}\r\n}\r\noffset = idx->rel_pd_idx << I40IW_HMC_PAGED_BP_SHIFT;\r\nchunk->size = info->pages << I40IW_HMC_PAGED_BP_SHIFT;\r\nchunk->vaddr = ((u8 *)sd_entry->u.bp.addr.va + offset);\r\nchunk->fpm_addr = pble_rsrc->next_fpm_addr;\r\ni40iw_debug(dev, I40IW_DEBUG_PBLE, "chunk_size[%d] = 0x%x vaddr=%p fpm_addr = %llx\n",\r\nchunk->size, chunk->size, chunk->vaddr, chunk->fpm_addr);\r\nreturn 0;\r\n}\r\nstatic void i40iw_free_vmalloc_mem(struct i40iw_hw *hw, struct i40iw_chunk *chunk)\r\n{\r\nstruct pci_dev *pcidev = (struct pci_dev *)hw->dev_context;\r\nint i;\r\nif (!chunk->pg_cnt)\r\ngoto done;\r\nfor (i = 0; i < chunk->pg_cnt; i++)\r\ndma_unmap_page(&pcidev->dev, chunk->dmaaddrs[i], PAGE_SIZE, DMA_BIDIRECTIONAL);\r\ndone:\r\nkfree(chunk->dmaaddrs);\r\nchunk->dmaaddrs = NULL;\r\nvfree(chunk->vaddr);\r\nchunk->vaddr = NULL;\r\nchunk->type = 0;\r\n}\r\nstatic enum i40iw_status_code i40iw_get_vmalloc_mem(struct i40iw_hw *hw,\r\nstruct i40iw_chunk *chunk,\r\nint pg_cnt)\r\n{\r\nstruct pci_dev *pcidev = (struct pci_dev *)hw->dev_context;\r\nstruct page *page;\r\nu8 *addr;\r\nu32 size;\r\nint i;\r\nchunk->dmaaddrs = kzalloc(pg_cnt << 3, GFP_KERNEL);\r\nif (!chunk->dmaaddrs)\r\nreturn I40IW_ERR_NO_MEMORY;\r\nsize = PAGE_SIZE * pg_cnt;\r\nchunk->vaddr = vmalloc(size);\r\nif (!chunk->vaddr) {\r\nkfree(chunk->dmaaddrs);\r\nchunk->dmaaddrs = NULL;\r\nreturn I40IW_ERR_NO_MEMORY;\r\n}\r\nchunk->size = size;\r\naddr = (u8 *)chunk->vaddr;\r\nfor (i = 0; i < pg_cnt; i++) {\r\npage = vmalloc_to_page((void *)addr);\r\nif (!page)\r\nbreak;\r\nchunk->dmaaddrs[i] = dma_map_page(&pcidev->dev, page, 0,\r\nPAGE_SIZE, DMA_BIDIRECTIONAL);\r\nif (dma_mapping_error(&pcidev->dev, chunk->dmaaddrs[i]))\r\nbreak;\r\naddr += PAGE_SIZE;\r\n}\r\nchunk->pg_cnt = i;\r\nchunk->type = I40IW_VMALLOC;\r\nif (i == pg_cnt)\r\nreturn 0;\r\ni40iw_free_vmalloc_mem(hw, chunk);\r\nreturn I40IW_ERR_NO_MEMORY;\r\n}\r\nstatic inline u32 fpm_to_idx(struct i40iw_hmc_pble_rsrc *pble_rsrc, u64 addr)\r\n{\r\nreturn (addr - (pble_rsrc->fpm_base_addr)) >> 3;\r\n}\r\nstatic enum i40iw_status_code add_bp_pages(struct i40iw_sc_dev *dev,\r\nstruct i40iw_hmc_pble_rsrc *pble_rsrc,\r\nstruct i40iw_add_page_info *info)\r\n{\r\nu8 *addr;\r\nstruct i40iw_dma_mem mem;\r\nstruct i40iw_hmc_pd_entry *pd_entry;\r\nstruct i40iw_hmc_sd_entry *sd_entry = info->sd_entry;\r\nstruct i40iw_hmc_info *hmc_info = info->hmc_info;\r\nstruct i40iw_chunk *chunk = info->chunk;\r\nstruct i40iw_manage_vf_pble_info vf_pble_info;\r\nenum i40iw_status_code status = 0;\r\nu32 rel_pd_idx = info->idx.rel_pd_idx;\r\nu32 pd_idx = info->idx.pd_idx;\r\nu32 i;\r\nstatus = i40iw_get_vmalloc_mem(dev->hw, chunk, info->pages);\r\nif (status)\r\nreturn I40IW_ERR_NO_MEMORY;\r\nstatus = i40iw_add_sd_table_entry(dev->hw, hmc_info,\r\ninfo->idx.sd_idx, I40IW_SD_TYPE_PAGED,\r\nI40IW_HMC_DIRECT_BP_SIZE);\r\nif (status) {\r\ni40iw_free_vmalloc_mem(dev->hw, chunk);\r\nreturn status;\r\n}\r\nif (!dev->is_pf) {\r\nstatus = i40iw_vchnl_vf_add_hmc_objs(dev, I40IW_HMC_IW_PBLE,\r\nfpm_to_idx(pble_rsrc,\r\npble_rsrc->next_fpm_addr),\r\n(info->pages << PBLE_512_SHIFT));\r\nif (status) {\r\ni40iw_pr_err("allocate PBLEs in the PF. Error %i\n", status);\r\ni40iw_free_vmalloc_mem(dev->hw, chunk);\r\nreturn status;\r\n}\r\n}\r\naddr = chunk->vaddr;\r\nfor (i = 0; i < info->pages; i++) {\r\nmem.pa = chunk->dmaaddrs[i];\r\nmem.size = PAGE_SIZE;\r\nmem.va = (void *)(addr);\r\npd_entry = &sd_entry->u.pd_table.pd_entry[rel_pd_idx++];\r\nif (!pd_entry->valid) {\r\nstatus = i40iw_add_pd_table_entry(dev->hw, hmc_info, pd_idx++, &mem);\r\nif (status)\r\ngoto error;\r\naddr += PAGE_SIZE;\r\n} else {\r\ni40iw_pr_err("pd entry is valid expecting to be invalid\n");\r\n}\r\n}\r\nif (!dev->is_pf) {\r\nvf_pble_info.first_pd_index = info->idx.rel_pd_idx;\r\nvf_pble_info.inv_pd_ent = false;\r\nvf_pble_info.pd_entry_cnt = PBLE_PER_PAGE;\r\nvf_pble_info.pd_pl_pba = sd_entry->u.pd_table.pd_page_addr.pa;\r\nvf_pble_info.sd_index = info->idx.sd_idx;\r\nstatus = i40iw_hw_manage_vf_pble_bp(dev->back_dev,\r\n&vf_pble_info, true);\r\nif (status) {\r\ni40iw_pr_err("CQP manage VF PBLE BP failed. %i\n", status);\r\ngoto error;\r\n}\r\n}\r\nchunk->fpm_addr = pble_rsrc->next_fpm_addr;\r\nreturn 0;\r\nerror:\r\ni40iw_free_vmalloc_mem(dev->hw, chunk);\r\nreturn status;\r\n}\r\nstatic enum i40iw_status_code add_pble_pool(struct i40iw_sc_dev *dev,\r\nstruct i40iw_hmc_pble_rsrc *pble_rsrc)\r\n{\r\nstruct i40iw_hmc_sd_entry *sd_entry;\r\nstruct i40iw_hmc_info *hmc_info;\r\nstruct i40iw_chunk *chunk;\r\nstruct i40iw_add_page_info info;\r\nstruct sd_pd_idx *idx = &info.idx;\r\nenum i40iw_status_code ret_code = 0;\r\nenum i40iw_sd_entry_type sd_entry_type;\r\nu64 sd_reg_val = 0;\r\nu32 pages;\r\nif (pble_rsrc->unallocated_pble < PBLE_PER_PAGE)\r\nreturn I40IW_ERR_NO_MEMORY;\r\nif (pble_rsrc->next_fpm_addr & 0xfff) {\r\ni40iw_pr_err("next fpm_addr %llx\n", pble_rsrc->next_fpm_addr);\r\nreturn I40IW_ERR_INVALID_PAGE_DESC_INDEX;\r\n}\r\nchunk = kzalloc(sizeof(*chunk), GFP_KERNEL);\r\nif (!chunk)\r\nreturn I40IW_ERR_NO_MEMORY;\r\nhmc_info = dev->hmc_info;\r\nchunk->fpm_addr = pble_rsrc->next_fpm_addr;\r\nget_sd_pd_idx(pble_rsrc, idx);\r\nsd_entry = &hmc_info->sd_table.sd_entry[idx->sd_idx];\r\npages = (idx->rel_pd_idx) ? (I40IW_HMC_PD_CNT_IN_SD -\r\nidx->rel_pd_idx) : I40IW_HMC_PD_CNT_IN_SD;\r\npages = min(pages, pble_rsrc->unallocated_pble >> PBLE_512_SHIFT);\r\nif (!pages) {\r\nret_code = I40IW_ERR_NO_PBLCHUNKS_AVAILABLE;\r\ngoto error;\r\n}\r\ninfo.chunk = chunk;\r\ninfo.hmc_info = hmc_info;\r\ninfo.pages = pages;\r\ninfo.sd_entry = sd_entry;\r\nif (!sd_entry->valid) {\r\nsd_entry_type = (!idx->rel_pd_idx &&\r\n(pages == I40IW_HMC_PD_CNT_IN_SD) &&\r\ndev->is_pf) ? I40IW_SD_TYPE_DIRECT : I40IW_SD_TYPE_PAGED;\r\n} else {\r\nsd_entry_type = sd_entry->entry_type;\r\n}\r\ni40iw_debug(dev, I40IW_DEBUG_PBLE,\r\n"pages = %d, unallocated_pble[%u] current_fpm_addr = %llx\n",\r\npages, pble_rsrc->unallocated_pble, pble_rsrc->next_fpm_addr);\r\ni40iw_debug(dev, I40IW_DEBUG_PBLE, "sd_entry_type = %d sd_entry valid = %d\n",\r\nsd_entry_type, sd_entry->valid);\r\nif (sd_entry_type == I40IW_SD_TYPE_DIRECT)\r\nret_code = add_sd_direct(dev, pble_rsrc, &info);\r\nif (ret_code)\r\nsd_entry_type = I40IW_SD_TYPE_PAGED;\r\nelse\r\npble_rsrc->stats_direct_sds++;\r\nif (sd_entry_type == I40IW_SD_TYPE_PAGED) {\r\nret_code = add_bp_pages(dev, pble_rsrc, &info);\r\nif (ret_code)\r\ngoto error;\r\nelse\r\npble_rsrc->stats_paged_sds++;\r\n}\r\nif (gen_pool_add_virt(pble_rsrc->pinfo.pool, (unsigned long)chunk->vaddr,\r\n(phys_addr_t)chunk->fpm_addr, chunk->size, -1)) {\r\ni40iw_pr_err("could not allocate memory by gen_pool_addr_virt()\n");\r\nret_code = I40IW_ERR_NO_MEMORY;\r\ngoto error;\r\n}\r\npble_rsrc->next_fpm_addr += chunk->size;\r\ni40iw_debug(dev, I40IW_DEBUG_PBLE, "next_fpm_addr = %llx chunk_size[%u] = 0x%x\n",\r\npble_rsrc->next_fpm_addr, chunk->size, chunk->size);\r\npble_rsrc->unallocated_pble -= (chunk->size >> 3);\r\nlist_add(&chunk->list, &pble_rsrc->pinfo.clist);\r\nsd_reg_val = (sd_entry_type == I40IW_SD_TYPE_PAGED) ?\r\nsd_entry->u.pd_table.pd_page_addr.pa : sd_entry->u.bp.addr.pa;\r\nif (sd_entry->valid)\r\nreturn 0;\r\nif (dev->is_pf) {\r\nret_code = i40iw_hmc_sd_one(dev, hmc_info->hmc_fn_id,\r\nsd_reg_val, idx->sd_idx,\r\nsd_entry->entry_type, true);\r\nif (ret_code) {\r\ni40iw_pr_err("cqp cmd failed for sd (pbles)\n");\r\ngoto error;\r\n}\r\n}\r\nsd_entry->valid = true;\r\nreturn 0;\r\nerror:\r\nkfree(chunk);\r\nreturn ret_code;\r\n}\r\nstatic void free_lvl2(struct i40iw_hmc_pble_rsrc *pble_rsrc,\r\nstruct i40iw_pble_alloc *palloc)\r\n{\r\nu32 i;\r\nstruct gen_pool *pool;\r\nstruct i40iw_pble_level2 *lvl2 = &palloc->level2;\r\nstruct i40iw_pble_info *root = &lvl2->root;\r\nstruct i40iw_pble_info *leaf = lvl2->leaf;\r\npool = pble_rsrc->pinfo.pool;\r\nfor (i = 0; i < lvl2->leaf_cnt; i++, leaf++) {\r\nif (leaf->addr)\r\ngen_pool_free(pool, leaf->addr, (leaf->cnt << 3));\r\nelse\r\nbreak;\r\n}\r\nif (root->addr)\r\ngen_pool_free(pool, root->addr, (root->cnt << 3));\r\nkfree(lvl2->leaf);\r\nlvl2->leaf = NULL;\r\n}\r\nstatic enum i40iw_status_code get_lvl2_pble(struct i40iw_hmc_pble_rsrc *pble_rsrc,\r\nstruct i40iw_pble_alloc *palloc,\r\nstruct gen_pool *pool)\r\n{\r\nu32 lf4k, lflast, total, i;\r\nu32 pblcnt = PBLE_PER_PAGE;\r\nu64 *addr;\r\nstruct i40iw_pble_level2 *lvl2 = &palloc->level2;\r\nstruct i40iw_pble_info *root = &lvl2->root;\r\nstruct i40iw_pble_info *leaf;\r\nlf4k = palloc->total_cnt >> 9;\r\nlflast = palloc->total_cnt % PBLE_PER_PAGE;\r\ntotal = (lflast == 0) ? lf4k : lf4k + 1;\r\nlvl2->leaf_cnt = total;\r\nleaf = kzalloc((sizeof(*leaf) * total), GFP_ATOMIC);\r\nif (!leaf)\r\nreturn I40IW_ERR_NO_MEMORY;\r\nlvl2->leaf = leaf;\r\nroot->addr = gen_pool_alloc(pool, (total << 3));\r\nif (!root->addr) {\r\nkfree(lvl2->leaf);\r\nlvl2->leaf = NULL;\r\nreturn I40IW_ERR_NO_MEMORY;\r\n}\r\nroot->idx = fpm_to_idx(pble_rsrc,\r\n(u64)gen_pool_virt_to_phys(pool, root->addr));\r\nroot->cnt = total;\r\naddr = (u64 *)root->addr;\r\nfor (i = 0; i < total; i++, leaf++) {\r\npblcnt = (lflast && ((i + 1) == total)) ? lflast : PBLE_PER_PAGE;\r\nleaf->addr = gen_pool_alloc(pool, (pblcnt << 3));\r\nif (!leaf->addr)\r\ngoto error;\r\nleaf->idx = fpm_to_idx(pble_rsrc, (u64)gen_pool_virt_to_phys(pool, leaf->addr));\r\nleaf->cnt = pblcnt;\r\n*addr = (u64)leaf->idx;\r\naddr++;\r\n}\r\npalloc->level = I40IW_LEVEL_2;\r\npble_rsrc->stats_lvl2++;\r\nreturn 0;\r\nerror:\r\nfree_lvl2(pble_rsrc, palloc);\r\nreturn I40IW_ERR_NO_MEMORY;\r\n}\r\nstatic enum i40iw_status_code get_lvl1_pble(struct i40iw_sc_dev *dev,\r\nstruct i40iw_hmc_pble_rsrc *pble_rsrc,\r\nstruct i40iw_pble_alloc *palloc)\r\n{\r\nu64 *addr;\r\nstruct gen_pool *pool;\r\nstruct i40iw_pble_info *lvl1 = &palloc->level1;\r\npool = pble_rsrc->pinfo.pool;\r\naddr = (u64 *)gen_pool_alloc(pool, (palloc->total_cnt << 3));\r\nif (!addr)\r\nreturn I40IW_ERR_NO_MEMORY;\r\npalloc->level = I40IW_LEVEL_1;\r\nlvl1->addr = (unsigned long)addr;\r\nlvl1->idx = fpm_to_idx(pble_rsrc, (u64)gen_pool_virt_to_phys(pool,\r\n(unsigned long)addr));\r\nlvl1->cnt = palloc->total_cnt;\r\npble_rsrc->stats_lvl1++;\r\nreturn 0;\r\n}\r\nstatic inline enum i40iw_status_code get_lvl1_lvl2_pble(struct i40iw_sc_dev *dev,\r\nstruct i40iw_hmc_pble_rsrc *pble_rsrc,\r\nstruct i40iw_pble_alloc *palloc,\r\nstruct gen_pool *pool)\r\n{\r\nenum i40iw_status_code status = 0;\r\nstatus = get_lvl1_pble(dev, pble_rsrc, palloc);\r\nif (status && (palloc->total_cnt > PBLE_PER_PAGE))\r\nstatus = get_lvl2_pble(pble_rsrc, palloc, pool);\r\nreturn status;\r\n}\r\nenum i40iw_status_code i40iw_get_pble(struct i40iw_sc_dev *dev,\r\nstruct i40iw_hmc_pble_rsrc *pble_rsrc,\r\nstruct i40iw_pble_alloc *palloc,\r\nu32 pble_cnt)\r\n{\r\nstruct gen_pool *pool;\r\nenum i40iw_status_code status = 0;\r\nu32 max_sds = 0;\r\nint i;\r\npool = pble_rsrc->pinfo.pool;\r\npalloc->total_cnt = pble_cnt;\r\npalloc->level = I40IW_LEVEL_0;\r\nstatus = get_lvl1_lvl2_pble(dev, pble_rsrc, palloc, pool);\r\nif (!status)\r\ngoto exit;\r\nmax_sds = (palloc->total_cnt >> 18) + 1;\r\nfor (i = 0; i < max_sds; i++) {\r\nstatus = add_pble_pool(dev, pble_rsrc);\r\nif (status)\r\nbreak;\r\nstatus = get_lvl1_lvl2_pble(dev, pble_rsrc, palloc, pool);\r\nif (!status)\r\nbreak;\r\n}\r\nexit:\r\nif (!status)\r\npble_rsrc->stats_alloc_ok++;\r\nelse\r\npble_rsrc->stats_alloc_fail++;\r\nreturn status;\r\n}\r\nvoid i40iw_free_pble(struct i40iw_hmc_pble_rsrc *pble_rsrc,\r\nstruct i40iw_pble_alloc *palloc)\r\n{\r\nstruct gen_pool *pool;\r\npool = pble_rsrc->pinfo.pool;\r\nif (palloc->level == I40IW_LEVEL_2)\r\nfree_lvl2(pble_rsrc, palloc);\r\nelse\r\ngen_pool_free(pool, palloc->level1.addr,\r\n(palloc->level1.cnt << 3));\r\npble_rsrc->stats_alloc_freed++;\r\n}
