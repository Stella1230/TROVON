void inet_get_local_port_range(struct net *net, int *low, int *high)\r\n{\r\nunsigned int seq;\r\ndo {\r\nseq = read_seqbegin(&net->ipv4.ip_local_ports.lock);\r\n*low = net->ipv4.ip_local_ports.range[0];\r\n*high = net->ipv4.ip_local_ports.range[1];\r\n} while (read_seqretry(&net->ipv4.ip_local_ports.lock, seq));\r\n}\r\nint inet_csk_bind_conflict(const struct sock *sk,\r\nconst struct inet_bind_bucket *tb, bool relax)\r\n{\r\nstruct sock *sk2;\r\nint reuse = sk->sk_reuse;\r\nint reuseport = sk->sk_reuseport;\r\nkuid_t uid = sock_i_uid((struct sock *)sk);\r\nsk_for_each_bound(sk2, &tb->owners) {\r\nif (sk != sk2 &&\r\n!inet_v6_ipv6only(sk2) &&\r\n(!sk->sk_bound_dev_if ||\r\n!sk2->sk_bound_dev_if ||\r\nsk->sk_bound_dev_if == sk2->sk_bound_dev_if)) {\r\nif ((!reuse || !sk2->sk_reuse ||\r\nsk2->sk_state == TCP_LISTEN) &&\r\n(!reuseport || !sk2->sk_reuseport ||\r\nrcu_access_pointer(sk->sk_reuseport_cb) ||\r\n(sk2->sk_state != TCP_TIME_WAIT &&\r\n!uid_eq(uid, sock_i_uid(sk2))))) {\r\nif (!sk2->sk_rcv_saddr || !sk->sk_rcv_saddr ||\r\nsk2->sk_rcv_saddr == sk->sk_rcv_saddr)\r\nbreak;\r\n}\r\nif (!relax && reuse && sk2->sk_reuse &&\r\nsk2->sk_state != TCP_LISTEN) {\r\nif (!sk2->sk_rcv_saddr || !sk->sk_rcv_saddr ||\r\nsk2->sk_rcv_saddr == sk->sk_rcv_saddr)\r\nbreak;\r\n}\r\n}\r\n}\r\nreturn sk2 != NULL;\r\n}\r\nint inet_csk_get_port(struct sock *sk, unsigned short snum)\r\n{\r\nbool reuse = sk->sk_reuse && sk->sk_state != TCP_LISTEN;\r\nstruct inet_hashinfo *hinfo = sk->sk_prot->h.hashinfo;\r\nint ret = 1, attempts = 5, port = snum;\r\nint smallest_size = -1, smallest_port;\r\nstruct inet_bind_hashbucket *head;\r\nstruct net *net = sock_net(sk);\r\nint i, low, high, attempt_half;\r\nstruct inet_bind_bucket *tb;\r\nkuid_t uid = sock_i_uid(sk);\r\nu32 remaining, offset;\r\nif (port) {\r\nhave_port:\r\nhead = &hinfo->bhash[inet_bhashfn(net, port,\r\nhinfo->bhash_size)];\r\nspin_lock_bh(&head->lock);\r\ninet_bind_bucket_for_each(tb, &head->chain)\r\nif (net_eq(ib_net(tb), net) && tb->port == port)\r\ngoto tb_found;\r\ngoto tb_not_found;\r\n}\r\nagain:\r\nattempt_half = (sk->sk_reuse == SK_CAN_REUSE) ? 1 : 0;\r\nother_half_scan:\r\ninet_get_local_port_range(net, &low, &high);\r\nhigh++;\r\nif (high - low < 4)\r\nattempt_half = 0;\r\nif (attempt_half) {\r\nint half = low + (((high - low) >> 2) << 1);\r\nif (attempt_half == 1)\r\nhigh = half;\r\nelse\r\nlow = half;\r\n}\r\nremaining = high - low;\r\nif (likely(remaining > 1))\r\nremaining &= ~1U;\r\noffset = prandom_u32() % remaining;\r\noffset |= 1U;\r\nsmallest_size = -1;\r\nsmallest_port = low;\r\nother_parity_scan:\r\nport = low + offset;\r\nfor (i = 0; i < remaining; i += 2, port += 2) {\r\nif (unlikely(port >= high))\r\nport -= remaining;\r\nif (inet_is_local_reserved_port(net, port))\r\ncontinue;\r\nhead = &hinfo->bhash[inet_bhashfn(net, port,\r\nhinfo->bhash_size)];\r\nspin_lock_bh(&head->lock);\r\ninet_bind_bucket_for_each(tb, &head->chain)\r\nif (net_eq(ib_net(tb), net) && tb->port == port) {\r\nif (((tb->fastreuse > 0 && reuse) ||\r\n(tb->fastreuseport > 0 &&\r\nsk->sk_reuseport &&\r\n!rcu_access_pointer(sk->sk_reuseport_cb) &&\r\nuid_eq(tb->fastuid, uid))) &&\r\n(tb->num_owners < smallest_size || smallest_size == -1)) {\r\nsmallest_size = tb->num_owners;\r\nsmallest_port = port;\r\n}\r\nif (!inet_csk(sk)->icsk_af_ops->bind_conflict(sk, tb, false))\r\ngoto tb_found;\r\ngoto next_port;\r\n}\r\ngoto tb_not_found;\r\nnext_port:\r\nspin_unlock_bh(&head->lock);\r\ncond_resched();\r\n}\r\nif (smallest_size != -1) {\r\nport = smallest_port;\r\ngoto have_port;\r\n}\r\noffset--;\r\nif (!(offset & 1))\r\ngoto other_parity_scan;\r\nif (attempt_half == 1) {\r\nattempt_half = 2;\r\ngoto other_half_scan;\r\n}\r\nreturn ret;\r\ntb_not_found:\r\ntb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,\r\nnet, head, port);\r\nif (!tb)\r\ngoto fail_unlock;\r\ntb_found:\r\nif (!hlist_empty(&tb->owners)) {\r\nif (sk->sk_reuse == SK_FORCE_REUSE)\r\ngoto success;\r\nif (((tb->fastreuse > 0 && reuse) ||\r\n(tb->fastreuseport > 0 &&\r\n!rcu_access_pointer(sk->sk_reuseport_cb) &&\r\nsk->sk_reuseport && uid_eq(tb->fastuid, uid))) &&\r\nsmallest_size == -1)\r\ngoto success;\r\nif (inet_csk(sk)->icsk_af_ops->bind_conflict(sk, tb, true)) {\r\nif ((reuse ||\r\n(tb->fastreuseport > 0 &&\r\nsk->sk_reuseport &&\r\n!rcu_access_pointer(sk->sk_reuseport_cb) &&\r\nuid_eq(tb->fastuid, uid))) &&\r\nsmallest_size != -1 && --attempts >= 0) {\r\nspin_unlock_bh(&head->lock);\r\ngoto again;\r\n}\r\ngoto fail_unlock;\r\n}\r\nif (!reuse)\r\ntb->fastreuse = 0;\r\nif (!sk->sk_reuseport || !uid_eq(tb->fastuid, uid))\r\ntb->fastreuseport = 0;\r\n} else {\r\ntb->fastreuse = reuse;\r\nif (sk->sk_reuseport) {\r\ntb->fastreuseport = 1;\r\ntb->fastuid = uid;\r\n} else {\r\ntb->fastreuseport = 0;\r\n}\r\n}\r\nsuccess:\r\nif (!inet_csk(sk)->icsk_bind_hash)\r\ninet_bind_hash(sk, tb, port);\r\nWARN_ON(inet_csk(sk)->icsk_bind_hash != tb);\r\nret = 0;\r\nfail_unlock:\r\nspin_unlock_bh(&head->lock);\r\nreturn ret;\r\n}\r\nstatic int inet_csk_wait_for_connect(struct sock *sk, long timeo)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nDEFINE_WAIT(wait);\r\nint err;\r\nfor (;;) {\r\nprepare_to_wait_exclusive(sk_sleep(sk), &wait,\r\nTASK_INTERRUPTIBLE);\r\nrelease_sock(sk);\r\nif (reqsk_queue_empty(&icsk->icsk_accept_queue))\r\ntimeo = schedule_timeout(timeo);\r\nsched_annotate_sleep();\r\nlock_sock(sk);\r\nerr = 0;\r\nif (!reqsk_queue_empty(&icsk->icsk_accept_queue))\r\nbreak;\r\nerr = -EINVAL;\r\nif (sk->sk_state != TCP_LISTEN)\r\nbreak;\r\nerr = sock_intr_errno(timeo);\r\nif (signal_pending(current))\r\nbreak;\r\nerr = -EAGAIN;\r\nif (!timeo)\r\nbreak;\r\n}\r\nfinish_wait(sk_sleep(sk), &wait);\r\nreturn err;\r\n}\r\nstruct sock *inet_csk_accept(struct sock *sk, int flags, int *err)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\r\nstruct request_sock *req;\r\nstruct sock *newsk;\r\nint error;\r\nlock_sock(sk);\r\nerror = -EINVAL;\r\nif (sk->sk_state != TCP_LISTEN)\r\ngoto out_err;\r\nif (reqsk_queue_empty(queue)) {\r\nlong timeo = sock_rcvtimeo(sk, flags & O_NONBLOCK);\r\nerror = -EAGAIN;\r\nif (!timeo)\r\ngoto out_err;\r\nerror = inet_csk_wait_for_connect(sk, timeo);\r\nif (error)\r\ngoto out_err;\r\n}\r\nreq = reqsk_queue_remove(queue, sk);\r\nnewsk = req->sk;\r\nif (sk->sk_protocol == IPPROTO_TCP &&\r\ntcp_rsk(req)->tfo_listener) {\r\nspin_lock_bh(&queue->fastopenq.lock);\r\nif (tcp_rsk(req)->tfo_listener) {\r\nreq->sk = NULL;\r\nreq = NULL;\r\n}\r\nspin_unlock_bh(&queue->fastopenq.lock);\r\n}\r\nout:\r\nrelease_sock(sk);\r\nif (req)\r\nreqsk_put(req);\r\nreturn newsk;\r\nout_err:\r\nnewsk = NULL;\r\nreq = NULL;\r\n*err = error;\r\ngoto out;\r\n}\r\nvoid inet_csk_init_xmit_timers(struct sock *sk,\r\nvoid (*retransmit_handler)(unsigned long),\r\nvoid (*delack_handler)(unsigned long),\r\nvoid (*keepalive_handler)(unsigned long))\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nsetup_timer(&icsk->icsk_retransmit_timer, retransmit_handler,\r\n(unsigned long)sk);\r\nsetup_timer(&icsk->icsk_delack_timer, delack_handler,\r\n(unsigned long)sk);\r\nsetup_timer(&sk->sk_timer, keepalive_handler, (unsigned long)sk);\r\nicsk->icsk_pending = icsk->icsk_ack.pending = 0;\r\n}\r\nvoid inet_csk_clear_xmit_timers(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nicsk->icsk_pending = icsk->icsk_ack.pending = icsk->icsk_ack.blocked = 0;\r\nsk_stop_timer(sk, &icsk->icsk_retransmit_timer);\r\nsk_stop_timer(sk, &icsk->icsk_delack_timer);\r\nsk_stop_timer(sk, &sk->sk_timer);\r\n}\r\nvoid inet_csk_delete_keepalive_timer(struct sock *sk)\r\n{\r\nsk_stop_timer(sk, &sk->sk_timer);\r\n}\r\nvoid inet_csk_reset_keepalive_timer(struct sock *sk, unsigned long len)\r\n{\r\nsk_reset_timer(sk, &sk->sk_timer, jiffies + len);\r\n}\r\nstruct dst_entry *inet_csk_route_req(const struct sock *sk,\r\nstruct flowi4 *fl4,\r\nconst struct request_sock *req)\r\n{\r\nconst struct inet_request_sock *ireq = inet_rsk(req);\r\nstruct net *net = read_pnet(&ireq->ireq_net);\r\nstruct ip_options_rcu *opt = ireq->opt;\r\nstruct rtable *rt;\r\nflowi4_init_output(fl4, ireq->ir_iif, ireq->ir_mark,\r\nRT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\r\nsk->sk_protocol, inet_sk_flowi_flags(sk),\r\n(opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\r\nireq->ir_loc_addr, ireq->ir_rmt_port,\r\nhtons(ireq->ir_num));\r\nsecurity_req_classify_flow(req, flowi4_to_flowi(fl4));\r\nrt = ip_route_output_flow(net, fl4, sk);\r\nif (IS_ERR(rt))\r\ngoto no_route;\r\nif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\r\ngoto route_err;\r\nreturn &rt->dst;\r\nroute_err:\r\nip_rt_put(rt);\r\nno_route:\r\n__IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\r\nreturn NULL;\r\n}\r\nstruct dst_entry *inet_csk_route_child_sock(const struct sock *sk,\r\nstruct sock *newsk,\r\nconst struct request_sock *req)\r\n{\r\nconst struct inet_request_sock *ireq = inet_rsk(req);\r\nstruct net *net = read_pnet(&ireq->ireq_net);\r\nstruct inet_sock *newinet = inet_sk(newsk);\r\nstruct ip_options_rcu *opt;\r\nstruct flowi4 *fl4;\r\nstruct rtable *rt;\r\nfl4 = &newinet->cork.fl.u.ip4;\r\nrcu_read_lock();\r\nopt = rcu_dereference(newinet->inet_opt);\r\nflowi4_init_output(fl4, ireq->ir_iif, ireq->ir_mark,\r\nRT_CONN_FLAGS(sk), RT_SCOPE_UNIVERSE,\r\nsk->sk_protocol, inet_sk_flowi_flags(sk),\r\n(opt && opt->opt.srr) ? opt->opt.faddr : ireq->ir_rmt_addr,\r\nireq->ir_loc_addr, ireq->ir_rmt_port,\r\nhtons(ireq->ir_num));\r\nsecurity_req_classify_flow(req, flowi4_to_flowi(fl4));\r\nrt = ip_route_output_flow(net, fl4, sk);\r\nif (IS_ERR(rt))\r\ngoto no_route;\r\nif (opt && opt->opt.is_strictroute && rt->rt_uses_gateway)\r\ngoto route_err;\r\nrcu_read_unlock();\r\nreturn &rt->dst;\r\nroute_err:\r\nip_rt_put(rt);\r\nno_route:\r\nrcu_read_unlock();\r\n__IP_INC_STATS(net, IPSTATS_MIB_OUTNOROUTES);\r\nreturn NULL;\r\n}\r\nstatic inline void syn_ack_recalc(struct request_sock *req, const int thresh,\r\nconst int max_retries,\r\nconst u8 rskq_defer_accept,\r\nint *expire, int *resend)\r\n{\r\nif (!rskq_defer_accept) {\r\n*expire = req->num_timeout >= thresh;\r\n*resend = 1;\r\nreturn;\r\n}\r\n*expire = req->num_timeout >= thresh &&\r\n(!inet_rsk(req)->acked || req->num_timeout >= max_retries);\r\n*resend = !inet_rsk(req)->acked ||\r\nreq->num_timeout >= rskq_defer_accept - 1;\r\n}\r\nint inet_rtx_syn_ack(const struct sock *parent, struct request_sock *req)\r\n{\r\nint err = req->rsk_ops->rtx_syn_ack(parent, req);\r\nif (!err)\r\nreq->num_retrans++;\r\nreturn err;\r\n}\r\nstatic bool reqsk_queue_unlink(struct request_sock_queue *queue,\r\nstruct request_sock *req)\r\n{\r\nstruct inet_hashinfo *hashinfo = req_to_sk(req)->sk_prot->h.hashinfo;\r\nbool found = false;\r\nif (sk_hashed(req_to_sk(req))) {\r\nspinlock_t *lock = inet_ehash_lockp(hashinfo, req->rsk_hash);\r\nspin_lock(lock);\r\nfound = __sk_nulls_del_node_init_rcu(req_to_sk(req));\r\nspin_unlock(lock);\r\n}\r\nif (timer_pending(&req->rsk_timer) && del_timer_sync(&req->rsk_timer))\r\nreqsk_put(req);\r\nreturn found;\r\n}\r\nvoid inet_csk_reqsk_queue_drop(struct sock *sk, struct request_sock *req)\r\n{\r\nif (reqsk_queue_unlink(&inet_csk(sk)->icsk_accept_queue, req)) {\r\nreqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req);\r\nreqsk_put(req);\r\n}\r\n}\r\nvoid inet_csk_reqsk_queue_drop_and_put(struct sock *sk, struct request_sock *req)\r\n{\r\ninet_csk_reqsk_queue_drop(sk, req);\r\nreqsk_put(req);\r\n}\r\nstatic void reqsk_timer_handler(unsigned long data)\r\n{\r\nstruct request_sock *req = (struct request_sock *)data;\r\nstruct sock *sk_listener = req->rsk_listener;\r\nstruct net *net = sock_net(sk_listener);\r\nstruct inet_connection_sock *icsk = inet_csk(sk_listener);\r\nstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\r\nint qlen, expire = 0, resend = 0;\r\nint max_retries, thresh;\r\nu8 defer_accept;\r\nif (sk_state_load(sk_listener) != TCP_LISTEN)\r\ngoto drop;\r\nmax_retries = icsk->icsk_syn_retries ? : net->ipv4.sysctl_tcp_synack_retries;\r\nthresh = max_retries;\r\nqlen = reqsk_queue_len(queue);\r\nif ((qlen << 1) > max(8U, sk_listener->sk_max_ack_backlog)) {\r\nint young = reqsk_queue_len_young(queue) << 1;\r\nwhile (thresh > 2) {\r\nif (qlen < young)\r\nbreak;\r\nthresh--;\r\nyoung <<= 1;\r\n}\r\n}\r\ndefer_accept = READ_ONCE(queue->rskq_defer_accept);\r\nif (defer_accept)\r\nmax_retries = defer_accept;\r\nsyn_ack_recalc(req, thresh, max_retries, defer_accept,\r\n&expire, &resend);\r\nreq->rsk_ops->syn_ack_timeout(req);\r\nif (!expire &&\r\n(!resend ||\r\n!inet_rtx_syn_ack(sk_listener, req) ||\r\ninet_rsk(req)->acked)) {\r\nunsigned long timeo;\r\nif (req->num_timeout++ == 0)\r\natomic_dec(&queue->young);\r\ntimeo = min(TCP_TIMEOUT_INIT << req->num_timeout, TCP_RTO_MAX);\r\nmod_timer(&req->rsk_timer, jiffies + timeo);\r\nreturn;\r\n}\r\ndrop:\r\ninet_csk_reqsk_queue_drop_and_put(sk_listener, req);\r\n}\r\nstatic void reqsk_queue_hash_req(struct request_sock *req,\r\nunsigned long timeout)\r\n{\r\nreq->num_retrans = 0;\r\nreq->num_timeout = 0;\r\nreq->sk = NULL;\r\nsetup_pinned_timer(&req->rsk_timer, reqsk_timer_handler,\r\n(unsigned long)req);\r\nmod_timer(&req->rsk_timer, jiffies + timeout);\r\ninet_ehash_insert(req_to_sk(req), NULL);\r\nsmp_wmb();\r\natomic_set(&req->rsk_refcnt, 2 + 1);\r\n}\r\nvoid inet_csk_reqsk_queue_hash_add(struct sock *sk, struct request_sock *req,\r\nunsigned long timeout)\r\n{\r\nreqsk_queue_hash_req(req, timeout);\r\ninet_csk_reqsk_queue_added(sk);\r\n}\r\nstruct sock *inet_csk_clone_lock(const struct sock *sk,\r\nconst struct request_sock *req,\r\nconst gfp_t priority)\r\n{\r\nstruct sock *newsk = sk_clone_lock(sk, priority);\r\nif (newsk) {\r\nstruct inet_connection_sock *newicsk = inet_csk(newsk);\r\nnewsk->sk_state = TCP_SYN_RECV;\r\nnewicsk->icsk_bind_hash = NULL;\r\ninet_sk(newsk)->inet_dport = inet_rsk(req)->ir_rmt_port;\r\ninet_sk(newsk)->inet_num = inet_rsk(req)->ir_num;\r\ninet_sk(newsk)->inet_sport = htons(inet_rsk(req)->ir_num);\r\nnewsk->sk_write_space = sk_stream_write_space;\r\nsock_reset_flag(newsk, SOCK_RCU_FREE);\r\nnewsk->sk_mark = inet_rsk(req)->ir_mark;\r\natomic64_set(&newsk->sk_cookie,\r\natomic64_read(&inet_rsk(req)->ir_cookie));\r\nnewicsk->icsk_retransmits = 0;\r\nnewicsk->icsk_backoff = 0;\r\nnewicsk->icsk_probes_out = 0;\r\nmemset(&newicsk->icsk_accept_queue, 0, sizeof(newicsk->icsk_accept_queue));\r\nsecurity_inet_csk_clone(newsk, req);\r\n}\r\nreturn newsk;\r\n}\r\nvoid inet_csk_destroy_sock(struct sock *sk)\r\n{\r\nWARN_ON(sk->sk_state != TCP_CLOSE);\r\nWARN_ON(!sock_flag(sk, SOCK_DEAD));\r\nWARN_ON(!sk_unhashed(sk));\r\nWARN_ON(inet_sk(sk)->inet_num && !inet_csk(sk)->icsk_bind_hash);\r\nsk->sk_prot->destroy(sk);\r\nsk_stream_kill_queues(sk);\r\nxfrm_sk_free_policy(sk);\r\nsk_refcnt_debug_release(sk);\r\nlocal_bh_disable();\r\npercpu_counter_dec(sk->sk_prot->orphan_count);\r\nlocal_bh_enable();\r\nsock_put(sk);\r\n}\r\nvoid inet_csk_prepare_forced_close(struct sock *sk)\r\n__releases(&sk->sk_lock.slock\r\nint inet_csk_listen_start(struct sock *sk, int backlog)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct inet_sock *inet = inet_sk(sk);\r\nint err = -EADDRINUSE;\r\nreqsk_queue_alloc(&icsk->icsk_accept_queue);\r\nsk->sk_max_ack_backlog = backlog;\r\nsk->sk_ack_backlog = 0;\r\ninet_csk_delack_init(sk);\r\nsk_state_store(sk, TCP_LISTEN);\r\nif (!sk->sk_prot->get_port(sk, inet->inet_num)) {\r\ninet->inet_sport = htons(inet->inet_num);\r\nsk_dst_reset(sk);\r\nerr = sk->sk_prot->hash(sk);\r\nif (likely(!err))\r\nreturn 0;\r\n}\r\nsk->sk_state = TCP_CLOSE;\r\nreturn err;\r\n}\r\nstatic void inet_child_forget(struct sock *sk, struct request_sock *req,\r\nstruct sock *child)\r\n{\r\nsk->sk_prot->disconnect(child, O_NONBLOCK);\r\nsock_orphan(child);\r\npercpu_counter_inc(sk->sk_prot->orphan_count);\r\nif (sk->sk_protocol == IPPROTO_TCP && tcp_rsk(req)->tfo_listener) {\r\nBUG_ON(tcp_sk(child)->fastopen_rsk != req);\r\nBUG_ON(sk != req->rsk_listener);\r\ntcp_sk(child)->fastopen_rsk = NULL;\r\n}\r\ninet_csk_destroy_sock(child);\r\nreqsk_put(req);\r\n}\r\nstruct sock *inet_csk_reqsk_queue_add(struct sock *sk,\r\nstruct request_sock *req,\r\nstruct sock *child)\r\n{\r\nstruct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;\r\nspin_lock(&queue->rskq_lock);\r\nif (unlikely(sk->sk_state != TCP_LISTEN)) {\r\ninet_child_forget(sk, req, child);\r\nchild = NULL;\r\n} else {\r\nreq->sk = child;\r\nreq->dl_next = NULL;\r\nif (queue->rskq_accept_head == NULL)\r\nqueue->rskq_accept_head = req;\r\nelse\r\nqueue->rskq_accept_tail->dl_next = req;\r\nqueue->rskq_accept_tail = req;\r\nsk_acceptq_added(sk);\r\n}\r\nspin_unlock(&queue->rskq_lock);\r\nreturn child;\r\n}\r\nstruct sock *inet_csk_complete_hashdance(struct sock *sk, struct sock *child,\r\nstruct request_sock *req, bool own_req)\r\n{\r\nif (own_req) {\r\ninet_csk_reqsk_queue_drop(sk, req);\r\nreqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req);\r\nif (inet_csk_reqsk_queue_add(sk, req, child))\r\nreturn child;\r\n}\r\nbh_unlock_sock(child);\r\nsock_put(child);\r\nreturn NULL;\r\n}\r\nvoid inet_csk_listen_stop(struct sock *sk)\r\n{\r\nstruct inet_connection_sock *icsk = inet_csk(sk);\r\nstruct request_sock_queue *queue = &icsk->icsk_accept_queue;\r\nstruct request_sock *next, *req;\r\nwhile ((req = reqsk_queue_remove(queue, sk)) != NULL) {\r\nstruct sock *child = req->sk;\r\nlocal_bh_disable();\r\nbh_lock_sock(child);\r\nWARN_ON(sock_owned_by_user(child));\r\nsock_hold(child);\r\ninet_child_forget(sk, req, child);\r\nbh_unlock_sock(child);\r\nlocal_bh_enable();\r\nsock_put(child);\r\ncond_resched();\r\n}\r\nif (queue->fastopenq.rskq_rst_head) {\r\nspin_lock_bh(&queue->fastopenq.lock);\r\nreq = queue->fastopenq.rskq_rst_head;\r\nqueue->fastopenq.rskq_rst_head = NULL;\r\nspin_unlock_bh(&queue->fastopenq.lock);\r\nwhile (req != NULL) {\r\nnext = req->dl_next;\r\nreqsk_put(req);\r\nreq = next;\r\n}\r\n}\r\nWARN_ON_ONCE(sk->sk_ack_backlog);\r\n}\r\nvoid inet_csk_addr2sockaddr(struct sock *sk, struct sockaddr *uaddr)\r\n{\r\nstruct sockaddr_in *sin = (struct sockaddr_in *)uaddr;\r\nconst struct inet_sock *inet = inet_sk(sk);\r\nsin->sin_family = AF_INET;\r\nsin->sin_addr.s_addr = inet->inet_daddr;\r\nsin->sin_port = inet->inet_dport;\r\n}\r\nint inet_csk_compat_getsockopt(struct sock *sk, int level, int optname,\r\nchar __user *optval, int __user *optlen)\r\n{\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nif (icsk->icsk_af_ops->compat_getsockopt)\r\nreturn icsk->icsk_af_ops->compat_getsockopt(sk, level, optname,\r\noptval, optlen);\r\nreturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\r\noptval, optlen);\r\n}\r\nint inet_csk_compat_setsockopt(struct sock *sk, int level, int optname,\r\nchar __user *optval, unsigned int optlen)\r\n{\r\nconst struct inet_connection_sock *icsk = inet_csk(sk);\r\nif (icsk->icsk_af_ops->compat_setsockopt)\r\nreturn icsk->icsk_af_ops->compat_setsockopt(sk, level, optname,\r\noptval, optlen);\r\nreturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\r\noptval, optlen);\r\n}\r\nstatic struct dst_entry *inet_csk_rebuild_route(struct sock *sk, struct flowi *fl)\r\n{\r\nconst struct inet_sock *inet = inet_sk(sk);\r\nconst struct ip_options_rcu *inet_opt;\r\n__be32 daddr = inet->inet_daddr;\r\nstruct flowi4 *fl4;\r\nstruct rtable *rt;\r\nrcu_read_lock();\r\ninet_opt = rcu_dereference(inet->inet_opt);\r\nif (inet_opt && inet_opt->opt.srr)\r\ndaddr = inet_opt->opt.faddr;\r\nfl4 = &fl->u.ip4;\r\nrt = ip_route_output_ports(sock_net(sk), fl4, sk, daddr,\r\ninet->inet_saddr, inet->inet_dport,\r\ninet->inet_sport, sk->sk_protocol,\r\nRT_CONN_FLAGS(sk), sk->sk_bound_dev_if);\r\nif (IS_ERR(rt))\r\nrt = NULL;\r\nif (rt)\r\nsk_setup_caps(sk, &rt->dst);\r\nrcu_read_unlock();\r\nreturn &rt->dst;\r\n}\r\nstruct dst_entry *inet_csk_update_pmtu(struct sock *sk, u32 mtu)\r\n{\r\nstruct dst_entry *dst = __sk_dst_check(sk, 0);\r\nstruct inet_sock *inet = inet_sk(sk);\r\nif (!dst) {\r\ndst = inet_csk_rebuild_route(sk, &inet->cork.fl);\r\nif (!dst)\r\ngoto out;\r\n}\r\ndst->ops->update_pmtu(dst, sk, NULL, mtu);\r\ndst = __sk_dst_check(sk, 0);\r\nif (!dst)\r\ndst = inet_csk_rebuild_route(sk, &inet->cork.fl);\r\nout:\r\nreturn dst;\r\n}
