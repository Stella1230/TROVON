static int zero_message_process(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nint rk_digest_size = crypto_ahash_digestsize(tfm);\r\nswitch (rk_digest_size) {\r\ncase SHA1_DIGEST_SIZE:\r\nmemcpy(req->result, sha1_zero_message_hash, rk_digest_size);\r\nbreak;\r\ncase SHA256_DIGEST_SIZE:\r\nmemcpy(req->result, sha256_zero_message_hash, rk_digest_size);\r\nbreak;\r\ncase MD5_DIGEST_SIZE:\r\nmemcpy(req->result, md5_zero_message_hash, rk_digest_size);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void rk_ahash_crypto_complete(struct rk_crypto_info *dev, int err)\r\n{\r\nif (dev->ahash_req->base.complete)\r\ndev->ahash_req->base.complete(&dev->ahash_req->base, err);\r\n}\r\nstatic void rk_ahash_reg_init(struct rk_crypto_info *dev)\r\n{\r\nint reg_status = 0;\r\nreg_status = CRYPTO_READ(dev, RK_CRYPTO_CTRL) |\r\nRK_CRYPTO_HASH_FLUSH | _SBF(0xffff, 16);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_CTRL, reg_status);\r\nreg_status = CRYPTO_READ(dev, RK_CRYPTO_CTRL);\r\nreg_status &= (~RK_CRYPTO_HASH_FLUSH);\r\nreg_status |= _SBF(0xffff, 16);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_CTRL, reg_status);\r\nmemset_io(dev->reg + RK_CRYPTO_HASH_DOUT_0, 0, 32);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_INTENA, RK_CRYPTO_HRDMA_ERR_ENA |\r\nRK_CRYPTO_HRDMA_DONE_ENA);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_INTSTS, RK_CRYPTO_HRDMA_ERR_INT |\r\nRK_CRYPTO_HRDMA_DONE_INT);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_HASH_CTRL, dev->mode |\r\nRK_CRYPTO_HASH_SWAP_DO);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_CONF, RK_CRYPTO_BYTESWAP_HRFIFO |\r\nRK_CRYPTO_BYTESWAP_BRFIFO |\r\nRK_CRYPTO_BYTESWAP_BTFIFO);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_HASH_MSG_LEN, dev->total);\r\n}\r\nstatic int rk_ahash_init(struct ahash_request *req)\r\n{\r\nstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags &\r\nCRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_ahash_init(&rctx->fallback_req);\r\n}\r\nstatic int rk_ahash_update(struct ahash_request *req)\r\n{\r\nstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags &\r\nCRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.nbytes = req->nbytes;\r\nrctx->fallback_req.src = req->src;\r\nreturn crypto_ahash_update(&rctx->fallback_req);\r\n}\r\nstatic int rk_ahash_final(struct ahash_request *req)\r\n{\r\nstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags &\r\nCRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.result = req->result;\r\nreturn crypto_ahash_final(&rctx->fallback_req);\r\n}\r\nstatic int rk_ahash_finup(struct ahash_request *req)\r\n{\r\nstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags &\r\nCRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.nbytes = req->nbytes;\r\nrctx->fallback_req.src = req->src;\r\nrctx->fallback_req.result = req->result;\r\nreturn crypto_ahash_finup(&rctx->fallback_req);\r\n}\r\nstatic int rk_ahash_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags &\r\nCRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_ahash_import(&rctx->fallback_req, in);\r\n}\r\nstatic int rk_ahash_export(struct ahash_request *req, void *out)\r\n{\r\nstruct rk_ahash_rctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct rk_ahash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback_tfm);\r\nrctx->fallback_req.base.flags = req->base.flags &\r\nCRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_ahash_export(&rctx->fallback_req, out);\r\n}\r\nstatic int rk_ahash_digest(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct rk_ahash_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct rk_crypto_info *dev = NULL;\r\nunsigned long flags;\r\nint ret;\r\nif (!req->nbytes)\r\nreturn zero_message_process(req);\r\ndev = tctx->dev;\r\ndev->total = req->nbytes;\r\ndev->left_bytes = req->nbytes;\r\ndev->aligned = 0;\r\ndev->mode = 0;\r\ndev->align_size = 4;\r\ndev->sg_dst = NULL;\r\ndev->sg_src = req->src;\r\ndev->first = req->src;\r\ndev->nents = sg_nents(req->src);\r\nswitch (crypto_ahash_digestsize(tfm)) {\r\ncase SHA1_DIGEST_SIZE:\r\ndev->mode = RK_CRYPTO_HASH_SHA1;\r\nbreak;\r\ncase SHA256_DIGEST_SIZE:\r\ndev->mode = RK_CRYPTO_HASH_SHA256;\r\nbreak;\r\ncase MD5_DIGEST_SIZE:\r\ndev->mode = RK_CRYPTO_HASH_MD5;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nrk_ahash_reg_init(dev);\r\nspin_lock_irqsave(&dev->lock, flags);\r\nret = crypto_enqueue_request(&dev->queue, &req->base);\r\nspin_unlock_irqrestore(&dev->lock, flags);\r\ntasklet_schedule(&dev->crypto_tasklet);\r\nwhile (!CRYPTO_READ(dev, RK_CRYPTO_HASH_STS))\r\nusleep_range(10, 50);\r\nmemcpy_fromio(req->result, dev->reg + RK_CRYPTO_HASH_DOUT_0,\r\ncrypto_ahash_digestsize(tfm));\r\nreturn 0;\r\n}\r\nstatic void crypto_ahash_dma_start(struct rk_crypto_info *dev)\r\n{\r\nCRYPTO_WRITE(dev, RK_CRYPTO_HRDMAS, dev->addr_in);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_HRDMAL, (dev->count + 3) / 4);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_CTRL, RK_CRYPTO_HASH_START |\r\n(RK_CRYPTO_HASH_START << 16));\r\n}\r\nstatic int rk_ahash_set_data_start(struct rk_crypto_info *dev)\r\n{\r\nint err;\r\nerr = dev->load_data(dev, dev->sg_src, NULL);\r\nif (!err)\r\ncrypto_ahash_dma_start(dev);\r\nreturn err;\r\n}\r\nstatic int rk_ahash_start(struct rk_crypto_info *dev)\r\n{\r\nreturn rk_ahash_set_data_start(dev);\r\n}\r\nstatic int rk_ahash_crypto_rx(struct rk_crypto_info *dev)\r\n{\r\nint err = 0;\r\ndev->unload_data(dev);\r\nif (dev->left_bytes) {\r\nif (dev->aligned) {\r\nif (sg_is_last(dev->sg_src)) {\r\ndev_warn(dev->dev, "[%s:%d], Lack of data\n",\r\n__func__, __LINE__);\r\nerr = -ENOMEM;\r\ngoto out_rx;\r\n}\r\ndev->sg_src = sg_next(dev->sg_src);\r\n}\r\nerr = rk_ahash_set_data_start(dev);\r\n} else {\r\ndev->complete(dev, 0);\r\n}\r\nout_rx:\r\nreturn err;\r\n}\r\nstatic int rk_cra_hash_init(struct crypto_tfm *tfm)\r\n{\r\nstruct rk_ahash_ctx *tctx = crypto_tfm_ctx(tfm);\r\nstruct rk_crypto_tmp *algt;\r\nstruct ahash_alg *alg = __crypto_ahash_alg(tfm->__crt_alg);\r\nconst char *alg_name = crypto_tfm_alg_name(tfm);\r\nalgt = container_of(alg, struct rk_crypto_tmp, alg.hash);\r\ntctx->dev = algt->dev;\r\ntctx->dev->addr_vir = (void *)__get_free_page(GFP_KERNEL);\r\nif (!tctx->dev->addr_vir) {\r\ndev_err(tctx->dev->dev, "failed to kmalloc for addr_vir\n");\r\nreturn -ENOMEM;\r\n}\r\ntctx->dev->start = rk_ahash_start;\r\ntctx->dev->update = rk_ahash_crypto_rx;\r\ntctx->dev->complete = rk_ahash_crypto_complete;\r\ntctx->fallback_tfm = crypto_alloc_ahash(alg_name, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(tctx->fallback_tfm)) {\r\ndev_err(tctx->dev->dev, "Could not load fallback driver.\n");\r\nreturn PTR_ERR(tctx->fallback_tfm);\r\n}\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct rk_ahash_rctx) +\r\ncrypto_ahash_reqsize(tctx->fallback_tfm));\r\nreturn tctx->dev->enable_clk(tctx->dev);\r\n}\r\nstatic void rk_cra_hash_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct rk_ahash_ctx *tctx = crypto_tfm_ctx(tfm);\r\nfree_page((unsigned long)tctx->dev->addr_vir);\r\nreturn tctx->dev->disable_clk(tctx->dev);\r\n}
