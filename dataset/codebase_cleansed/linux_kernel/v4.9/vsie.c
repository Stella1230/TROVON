static int set_validity_icpt(struct kvm_s390_sie_block *scb,\r\n__u16 reason_code)\r\n{\r\nscb->ipa = 0x1000;\r\nscb->ipb = ((__u32) reason_code) << 16;\r\nscb->icptcode = ICPT_VALIDITY;\r\nreturn 1;\r\n}\r\nstatic void prefix_unmapped(struct vsie_page *vsie_page)\r\n{\r\natomic_or(PROG_REQUEST, &vsie_page->scb_s.prog20);\r\n}\r\nstatic void prefix_unmapped_sync(struct vsie_page *vsie_page)\r\n{\r\nprefix_unmapped(vsie_page);\r\nif (vsie_page->scb_s.prog0c & PROG_IN_SIE)\r\natomic_or(CPUSTAT_STOP_INT, &vsie_page->scb_s.cpuflags);\r\nwhile (vsie_page->scb_s.prog0c & PROG_IN_SIE)\r\ncpu_relax();\r\n}\r\nstatic void prefix_mapped(struct vsie_page *vsie_page)\r\n{\r\natomic_andnot(PROG_REQUEST, &vsie_page->scb_s.prog20);\r\n}\r\nstatic int prefix_is_mapped(struct vsie_page *vsie_page)\r\n{\r\nreturn !(atomic_read(&vsie_page->scb_s.prog20) & PROG_REQUEST);\r\n}\r\nstatic void update_intervention_requests(struct vsie_page *vsie_page)\r\n{\r\nconst int bits = CPUSTAT_STOP_INT | CPUSTAT_IO_INT | CPUSTAT_EXT_INT;\r\nint cpuflags;\r\ncpuflags = atomic_read(&vsie_page->scb_o->cpuflags);\r\natomic_andnot(bits, &vsie_page->scb_s.cpuflags);\r\natomic_or(cpuflags & bits, &vsie_page->scb_s.cpuflags);\r\n}\r\nstatic int prepare_cpuflags(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nstruct kvm_s390_sie_block *scb_o = vsie_page->scb_o;\r\nint newflags, cpuflags = atomic_read(&scb_o->cpuflags);\r\nif (!(cpuflags & CPUSTAT_ZARCH))\r\nreturn set_validity_icpt(scb_s, 0x0001U);\r\nif (cpuflags & (CPUSTAT_RRF | CPUSTAT_MCDS))\r\nreturn set_validity_icpt(scb_s, 0x0001U);\r\nelse if (cpuflags & (CPUSTAT_SLSV | CPUSTAT_SLSR))\r\nreturn set_validity_icpt(scb_s, 0x0007U);\r\nnewflags = CPUSTAT_ZARCH;\r\nif (cpuflags & CPUSTAT_GED && test_kvm_facility(vcpu->kvm, 8))\r\nnewflags |= CPUSTAT_GED;\r\nif (cpuflags & CPUSTAT_GED2 && test_kvm_facility(vcpu->kvm, 78)) {\r\nif (cpuflags & CPUSTAT_GED)\r\nreturn set_validity_icpt(scb_s, 0x0001U);\r\nnewflags |= CPUSTAT_GED2;\r\n}\r\nif (test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_GPERE))\r\nnewflags |= cpuflags & CPUSTAT_P;\r\nif (test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_GSLS))\r\nnewflags |= cpuflags & CPUSTAT_SM;\r\nif (test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_IBS))\r\nnewflags |= cpuflags & CPUSTAT_IBS;\r\natomic_set(&scb_s->cpuflags, newflags);\r\nreturn 0;\r\n}\r\nstatic int shadow_crycb(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nstruct kvm_s390_sie_block *scb_o = vsie_page->scb_o;\r\nu32 crycb_addr = scb_o->crycbd & 0x7ffffff8U;\r\nunsigned long *b1, *b2;\r\nu8 ecb3_flags;\r\nscb_s->crycbd = 0;\r\nif (!(scb_o->crycbd & vcpu->arch.sie_block->crycbd & CRYCB_FORMAT1))\r\nreturn 0;\r\nif (!test_kvm_facility(vcpu->kvm, 76))\r\nreturn 0;\r\necb3_flags = scb_o->ecb3 & vcpu->arch.sie_block->ecb3 &\r\n(ECB3_AES | ECB3_DEA);\r\nif (!ecb3_flags)\r\nreturn 0;\r\nif ((crycb_addr & PAGE_MASK) != ((crycb_addr + 128) & PAGE_MASK))\r\nreturn set_validity_icpt(scb_s, 0x003CU);\r\nelse if (!crycb_addr)\r\nreturn set_validity_icpt(scb_s, 0x0039U);\r\nif (read_guest_real(vcpu, crycb_addr + 72, &vsie_page->crycb, 56))\r\nreturn set_validity_icpt(scb_s, 0x0035U);\r\nscb_s->ecb3 |= ecb3_flags;\r\nscb_s->crycbd = ((__u32)(__u64) &vsie_page->crycb) | CRYCB_FORMAT1 |\r\nCRYCB_FORMAT2;\r\nb1 = (unsigned long *) vsie_page->crycb.dea_wrapping_key_mask;\r\nb2 = (unsigned long *)\r\nvcpu->kvm->arch.crypto.crycb->dea_wrapping_key_mask;\r\nbitmap_xor(b1, b1, b2, BITS_PER_BYTE * 56);\r\nreturn 0;\r\n}\r\nstatic void prepare_ibc(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nstruct kvm_s390_sie_block *scb_o = vsie_page->scb_o;\r\n__u64 min_ibc = (sclp.ibc >> 16) & 0x0fffU;\r\nscb_s->ibc = 0;\r\nif (vcpu->kvm->arch.model.ibc && (scb_o->ibc & 0x0fffU)) {\r\nscb_s->ibc = scb_o->ibc & 0x0fffU;\r\nif (scb_s->ibc < min_ibc)\r\nscb_s->ibc = min_ibc;\r\nif (scb_s->ibc > vcpu->kvm->arch.model.ibc)\r\nscb_s->ibc = vcpu->kvm->arch.model.ibc;\r\n}\r\n}\r\nstatic void unshadow_scb(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nstruct kvm_s390_sie_block *scb_o = vsie_page->scb_o;\r\nscb_o->icptcode = scb_s->icptcode;\r\nscb_o->icptstatus = scb_s->icptstatus;\r\nscb_o->ipa = scb_s->ipa;\r\nscb_o->ipb = scb_s->ipb;\r\nscb_o->gbea = scb_s->gbea;\r\nscb_o->cputm = scb_s->cputm;\r\nscb_o->ckc = scb_s->ckc;\r\nscb_o->todpr = scb_s->todpr;\r\nscb_o->gpsw = scb_s->gpsw;\r\nscb_o->gg14 = scb_s->gg14;\r\nscb_o->gg15 = scb_s->gg15;\r\nmemcpy(scb_o->gcr, scb_s->gcr, 128);\r\nscb_o->pp = scb_s->pp;\r\nswitch (scb_s->icptcode) {\r\ncase ICPT_PROGI:\r\ncase ICPT_INSTPROGI:\r\ncase ICPT_EXTINT:\r\nmemcpy((void *)((u64)scb_o + 0xc0),\r\n(void *)((u64)scb_s + 0xc0), 0xf0 - 0xc0);\r\nbreak;\r\ncase ICPT_PARTEXEC:\r\nmemcpy((void *)((u64)scb_o + 0xc0),\r\n(void *)((u64)scb_s + 0xc0), 0xd0 - 0xc0);\r\nbreak;\r\n}\r\nif (scb_s->ihcpu != 0xffffU)\r\nscb_o->ihcpu = scb_s->ihcpu;\r\n}\r\nstatic int shadow_scb(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_o = vsie_page->scb_o;\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nbool had_tx = scb_s->ecb & 0x10U;\r\nunsigned long new_mso = 0;\r\nint rc;\r\nscb_s->icptcode = 0;\r\nscb_s->eca = 0;\r\nscb_s->ecb = 0;\r\nscb_s->ecb2 = 0;\r\nscb_s->ecb3 = 0;\r\nscb_s->ecd = 0;\r\nscb_s->fac = 0;\r\nrc = prepare_cpuflags(vcpu, vsie_page);\r\nif (rc)\r\ngoto out;\r\nscb_s->cputm = scb_o->cputm;\r\nscb_s->ckc = scb_o->ckc;\r\nscb_s->todpr = scb_o->todpr;\r\nscb_s->epoch = scb_o->epoch;\r\nscb_s->gpsw = scb_o->gpsw;\r\nscb_s->gg14 = scb_o->gg14;\r\nscb_s->gg15 = scb_o->gg15;\r\nmemcpy(scb_s->gcr, scb_o->gcr, 128);\r\nscb_s->pp = scb_o->pp;\r\nscb_s->gbea = scb_o->gbea;\r\nscb_s->lctl = scb_o->lctl;\r\nscb_s->svcc = scb_o->svcc;\r\nscb_s->ictl = scb_o->ictl;\r\nscb_s->ictl |= ICTL_ISKE | ICTL_SSKE | ICTL_RRBE;\r\nscb_s->icpua = scb_o->icpua;\r\nif (!(atomic_read(&scb_s->cpuflags) & CPUSTAT_SM))\r\nnew_mso = scb_o->mso & 0xfffffffffff00000UL;\r\nif (scb_s->mso != new_mso || scb_s->prefix != scb_o->prefix)\r\nprefix_unmapped(vsie_page);\r\nscb_s->msl = scb_o->msl & 0xfffffffffff00000UL;\r\nscb_s->mso = new_mso;\r\nscb_s->prefix = scb_o->prefix;\r\nif (scb_s->ihcpu != 0xffffU)\r\nscb_s->ihcpu = scb_o->ihcpu;\r\nscb_s->eca |= scb_o->eca & 0x01002000U;\r\nif (test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_ESOP))\r\nscb_s->ecb |= scb_o->ecb & 0x02U;\r\nif (test_kvm_facility(vcpu->kvm, 73)) {\r\nif ((scb_o->ecb & 0x10U) && !had_tx)\r\nprefix_unmapped(vsie_page);\r\nscb_s->ecb |= scb_o->ecb & 0x10U;\r\n}\r\nif (test_kvm_facility(vcpu->kvm, 129)) {\r\nscb_s->eca |= scb_o->eca & 0x00020000U;\r\nscb_s->ecd |= scb_o->ecd & 0x20000000U;\r\n}\r\nif (test_kvm_facility(vcpu->kvm, 64))\r\nscb_s->ecb3 |= scb_o->ecb3 & 0x01U;\r\nif (test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_SIIF))\r\nscb_s->eca |= scb_o->eca & 0x00000001U;\r\nif (test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_IB))\r\nscb_s->eca |= scb_o->eca & 0x40000000U;\r\nif (test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_CEI))\r\nscb_s->eca |= scb_o->eca & 0x80000000U;\r\nprepare_ibc(vcpu, vsie_page);\r\nrc = shadow_crycb(vcpu, vsie_page);\r\nout:\r\nif (rc)\r\nunshadow_scb(vcpu, vsie_page);\r\nreturn rc;\r\n}\r\nvoid kvm_s390_vsie_gmap_notifier(struct gmap *gmap, unsigned long start,\r\nunsigned long end)\r\n{\r\nstruct kvm *kvm = gmap->private;\r\nstruct vsie_page *cur;\r\nunsigned long prefix;\r\nstruct page *page;\r\nint i;\r\nif (!gmap_is_shadow(gmap))\r\nreturn;\r\nif (start >= 1UL << 31)\r\nreturn;\r\nfor (i = 0; i < kvm->arch.vsie.page_count; i++) {\r\npage = READ_ONCE(kvm->arch.vsie.pages[i]);\r\nif (!page)\r\ncontinue;\r\ncur = page_to_virt(page);\r\nif (READ_ONCE(cur->gmap) != gmap)\r\ncontinue;\r\nprefix = cur->scb_s.prefix << GUEST_PREFIX_SHIFT;\r\nprefix += cur->scb_s.mso;\r\nif (prefix <= end && start <= prefix + 2 * PAGE_SIZE - 1)\r\nprefix_unmapped_sync(cur);\r\n}\r\n}\r\nstatic int map_prefix(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nu64 prefix = scb_s->prefix << GUEST_PREFIX_SHIFT;\r\nint rc;\r\nif (prefix_is_mapped(vsie_page))\r\nreturn 0;\r\nprefix_mapped(vsie_page);\r\nprefix += scb_s->mso;\r\nrc = kvm_s390_shadow_fault(vcpu, vsie_page->gmap, prefix);\r\nif (!rc && (scb_s->ecb & 0x10U))\r\nrc = kvm_s390_shadow_fault(vcpu, vsie_page->gmap,\r\nprefix + PAGE_SIZE);\r\nif (rc)\r\nprefix_unmapped(vsie_page);\r\nif (rc > 0 || rc == -EFAULT)\r\nrc = set_validity_icpt(scb_s, 0x0037U);\r\nreturn rc;\r\n}\r\nstatic int pin_guest_page(struct kvm *kvm, gpa_t gpa, hpa_t *hpa)\r\n{\r\nstruct page *page;\r\nhva_t hva;\r\nint rc;\r\nhva = gfn_to_hva(kvm, gpa_to_gfn(gpa));\r\nif (kvm_is_error_hva(hva))\r\nreturn -EINVAL;\r\nrc = get_user_pages_fast(hva, 1, 1, &page);\r\nif (rc < 0)\r\nreturn rc;\r\nelse if (rc != 1)\r\nreturn -ENOMEM;\r\n*hpa = (hpa_t) page_to_virt(page) + (gpa & ~PAGE_MASK);\r\nreturn 0;\r\n}\r\nstatic void unpin_guest_page(struct kvm *kvm, gpa_t gpa, hpa_t hpa)\r\n{\r\nstruct page *page;\r\npage = virt_to_page(hpa);\r\nset_page_dirty_lock(page);\r\nput_page(page);\r\nmark_page_dirty(kvm, gpa_to_gfn(gpa));\r\n}\r\nstatic void unpin_blocks(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_o = vsie_page->scb_o;\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nhpa_t hpa;\r\ngpa_t gpa;\r\nhpa = (u64) scb_s->scaoh << 32 | scb_s->scaol;\r\nif (hpa) {\r\ngpa = scb_o->scaol & ~0xfUL;\r\nif (test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_64BSCAO))\r\ngpa |= (u64) scb_o->scaoh << 32;\r\nunpin_guest_page(vcpu->kvm, gpa, hpa);\r\nscb_s->scaol = 0;\r\nscb_s->scaoh = 0;\r\n}\r\nhpa = scb_s->itdba;\r\nif (hpa) {\r\ngpa = scb_o->itdba & ~0xffUL;\r\nunpin_guest_page(vcpu->kvm, gpa, hpa);\r\nscb_s->itdba = 0;\r\n}\r\nhpa = scb_s->gvrd;\r\nif (hpa) {\r\ngpa = scb_o->gvrd & ~0x1ffUL;\r\nunpin_guest_page(vcpu->kvm, gpa, hpa);\r\nscb_s->gvrd = 0;\r\n}\r\nhpa = scb_s->riccbd;\r\nif (hpa) {\r\ngpa = scb_o->riccbd & ~0x3fUL;\r\nunpin_guest_page(vcpu->kvm, gpa, hpa);\r\nscb_s->riccbd = 0;\r\n}\r\n}\r\nstatic int pin_blocks(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_o = vsie_page->scb_o;\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nhpa_t hpa;\r\ngpa_t gpa;\r\nint rc = 0;\r\ngpa = scb_o->scaol & ~0xfUL;\r\nif (test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_64BSCAO))\r\ngpa |= (u64) scb_o->scaoh << 32;\r\nif (gpa) {\r\nif (!(gpa & ~0x1fffUL))\r\nrc = set_validity_icpt(scb_s, 0x0038U);\r\nelse if ((gpa & ~0x1fffUL) == kvm_s390_get_prefix(vcpu))\r\nrc = set_validity_icpt(scb_s, 0x0011U);\r\nelse if ((gpa & PAGE_MASK) !=\r\n((gpa + sizeof(struct bsca_block) - 1) & PAGE_MASK))\r\nrc = set_validity_icpt(scb_s, 0x003bU);\r\nif (!rc) {\r\nrc = pin_guest_page(vcpu->kvm, gpa, &hpa);\r\nif (rc == -EINVAL)\r\nrc = set_validity_icpt(scb_s, 0x0034U);\r\n}\r\nif (rc)\r\ngoto unpin;\r\nscb_s->scaoh = (u32)((u64)hpa >> 32);\r\nscb_s->scaol = (u32)(u64)hpa;\r\n}\r\ngpa = scb_o->itdba & ~0xffUL;\r\nif (gpa && (scb_s->ecb & 0x10U)) {\r\nif (!(gpa & ~0x1fffU)) {\r\nrc = set_validity_icpt(scb_s, 0x0080U);\r\ngoto unpin;\r\n}\r\nrc = pin_guest_page(vcpu->kvm, gpa, &hpa);\r\nif (rc == -EINVAL)\r\nrc = set_validity_icpt(scb_s, 0x0080U);\r\nif (rc)\r\ngoto unpin;\r\nscb_s->itdba = hpa;\r\n}\r\ngpa = scb_o->gvrd & ~0x1ffUL;\r\nif (gpa && (scb_s->eca & 0x00020000U) &&\r\n!(scb_s->ecd & 0x20000000U)) {\r\nif (!(gpa & ~0x1fffUL)) {\r\nrc = set_validity_icpt(scb_s, 0x1310U);\r\ngoto unpin;\r\n}\r\nrc = pin_guest_page(vcpu->kvm, gpa, &hpa);\r\nif (rc == -EINVAL)\r\nrc = set_validity_icpt(scb_s, 0x1310U);\r\nif (rc)\r\ngoto unpin;\r\nscb_s->gvrd = hpa;\r\n}\r\ngpa = scb_o->riccbd & ~0x3fUL;\r\nif (gpa && (scb_s->ecb3 & 0x01U)) {\r\nif (!(gpa & ~0x1fffUL)) {\r\nrc = set_validity_icpt(scb_s, 0x0043U);\r\ngoto unpin;\r\n}\r\nrc = pin_guest_page(vcpu->kvm, gpa, &hpa);\r\nif (rc == -EINVAL)\r\nrc = set_validity_icpt(scb_s, 0x0043U);\r\nif (rc)\r\ngoto unpin;\r\nscb_s->riccbd = hpa;\r\n}\r\nreturn 0;\r\nunpin:\r\nunpin_blocks(vcpu, vsie_page);\r\nreturn rc;\r\n}\r\nstatic void unpin_scb(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page,\r\ngpa_t gpa)\r\n{\r\nhpa_t hpa = (hpa_t) vsie_page->scb_o;\r\nif (hpa)\r\nunpin_guest_page(vcpu->kvm, gpa, hpa);\r\nvsie_page->scb_o = NULL;\r\n}\r\nstatic int pin_scb(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page,\r\ngpa_t gpa)\r\n{\r\nhpa_t hpa;\r\nint rc;\r\nrc = pin_guest_page(vcpu->kvm, gpa, &hpa);\r\nif (rc == -EINVAL) {\r\nrc = kvm_s390_inject_program_int(vcpu, PGM_ADDRESSING);\r\nif (!rc)\r\nrc = 1;\r\n}\r\nif (!rc)\r\nvsie_page->scb_o = (struct kvm_s390_sie_block *) hpa;\r\nreturn rc;\r\n}\r\nstatic int inject_fault(struct kvm_vcpu *vcpu, __u16 code, __u64 vaddr,\r\nbool write_flag)\r\n{\r\nstruct kvm_s390_pgm_info pgm = {\r\n.code = code,\r\n.trans_exc_code =\r\n(vaddr & 0xfffffffffffff000UL) |\r\n(((unsigned int) !write_flag) + 1) << 10,\r\n.exc_access_id = 0,\r\n.op_access_id = 0,\r\n};\r\nint rc;\r\nif (code == PGM_PROTECTION)\r\npgm.trans_exc_code |= 0x4UL;\r\nrc = kvm_s390_inject_prog_irq(vcpu, &pgm);\r\nreturn rc ? rc : 1;\r\n}\r\nstatic int handle_fault(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nint rc;\r\nif (current->thread.gmap_int_code == PGM_PROTECTION)\r\nreturn inject_fault(vcpu, PGM_PROTECTION,\r\ncurrent->thread.gmap_addr, 1);\r\nrc = kvm_s390_shadow_fault(vcpu, vsie_page->gmap,\r\ncurrent->thread.gmap_addr);\r\nif (rc > 0) {\r\nrc = inject_fault(vcpu, rc,\r\ncurrent->thread.gmap_addr,\r\ncurrent->thread.gmap_write_flag);\r\nif (rc >= 0)\r\nvsie_page->fault_addr = current->thread.gmap_addr;\r\n}\r\nreturn rc;\r\n}\r\nstatic void handle_last_fault(struct kvm_vcpu *vcpu,\r\nstruct vsie_page *vsie_page)\r\n{\r\nif (vsie_page->fault_addr)\r\nkvm_s390_shadow_fault(vcpu, vsie_page->gmap,\r\nvsie_page->fault_addr);\r\nvsie_page->fault_addr = 0;\r\n}\r\nstatic inline void clear_vsie_icpt(struct vsie_page *vsie_page)\r\n{\r\nvsie_page->scb_s.icptcode = 0;\r\n}\r\nstatic void retry_vsie_icpt(struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nint ilen = insn_length(scb_s->ipa >> 8);\r\nif (scb_s->icptstatus & 1) {\r\nilen = (scb_s->icptstatus >> 4) & 0x6;\r\nif (!ilen)\r\nilen = 4;\r\n}\r\nscb_s->gpsw.addr = __rewind_psw(scb_s->gpsw, ilen);\r\nclear_vsie_icpt(vsie_page);\r\n}\r\nstatic int handle_stfle(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\n__u32 fac = vsie_page->scb_o->fac & 0x7ffffff8U;\r\nif (fac && test_kvm_facility(vcpu->kvm, 7)) {\r\nretry_vsie_icpt(vsie_page);\r\nif (read_guest_real(vcpu, fac, &vsie_page->fac,\r\nsizeof(vsie_page->fac)))\r\nreturn set_validity_icpt(scb_s, 0x1090U);\r\nscb_s->fac = (__u32)(__u64) &vsie_page->fac;\r\n}\r\nreturn 0;\r\n}\r\nstatic int do_vsie_run(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nstruct kvm_s390_sie_block *scb_o = vsie_page->scb_o;\r\nint rc;\r\nhandle_last_fault(vcpu, vsie_page);\r\nif (need_resched())\r\nschedule();\r\nif (test_cpu_flag(CIF_MCCK_PENDING))\r\ns390_handle_mcck();\r\nsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\r\nlocal_irq_disable();\r\nguest_enter_irqoff();\r\nlocal_irq_enable();\r\nrc = sie64a(scb_s, vcpu->run->s.regs.gprs);\r\nlocal_irq_disable();\r\nguest_exit_irqoff();\r\nlocal_irq_enable();\r\nvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\r\nif (rc > 0)\r\nrc = 0;\r\nelse if (rc == -EFAULT)\r\nreturn handle_fault(vcpu, vsie_page);\r\nswitch (scb_s->icptcode) {\r\ncase ICPT_INST:\r\nif (scb_s->ipa == 0xb2b0)\r\nrc = handle_stfle(vcpu, vsie_page);\r\nbreak;\r\ncase ICPT_STOP:\r\nif (!(atomic_read(&scb_o->cpuflags) & CPUSTAT_STOP_INT))\r\nclear_vsie_icpt(vsie_page);\r\nbreak;\r\ncase ICPT_VALIDITY:\r\nif ((scb_s->ipa & 0xf000) != 0xf000)\r\nscb_s->ipa += 0x1000;\r\nbreak;\r\n}\r\nreturn rc;\r\n}\r\nstatic void release_gmap_shadow(struct vsie_page *vsie_page)\r\n{\r\nif (vsie_page->gmap)\r\ngmap_put(vsie_page->gmap);\r\nWRITE_ONCE(vsie_page->gmap, NULL);\r\nprefix_unmapped(vsie_page);\r\n}\r\nstatic int acquire_gmap_shadow(struct kvm_vcpu *vcpu,\r\nstruct vsie_page *vsie_page)\r\n{\r\nunsigned long asce;\r\nunion ctlreg0 cr0;\r\nstruct gmap *gmap;\r\nint edat;\r\nasce = vcpu->arch.sie_block->gcr[1];\r\ncr0.val = vcpu->arch.sie_block->gcr[0];\r\nedat = cr0.edat && test_kvm_facility(vcpu->kvm, 8);\r\nedat += edat && test_kvm_facility(vcpu->kvm, 78);\r\nif (vsie_page->gmap && gmap_shadow_valid(vsie_page->gmap, asce, edat))\r\nreturn 0;\r\nrelease_gmap_shadow(vsie_page);\r\ngmap = gmap_shadow(vcpu->arch.gmap, asce, edat);\r\nif (IS_ERR(gmap))\r\nreturn PTR_ERR(gmap);\r\ngmap->private = vcpu->kvm;\r\nWRITE_ONCE(vsie_page->gmap, gmap);\r\nreturn 0;\r\n}\r\nstatic void register_shadow_scb(struct kvm_vcpu *vcpu,\r\nstruct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nWRITE_ONCE(vcpu->arch.vsie_block, &vsie_page->scb_s);\r\natomic_or(CPUSTAT_WAIT, &vcpu->arch.sie_block->cpuflags);\r\npreempt_disable();\r\nscb_s->epoch += vcpu->kvm->arch.epoch;\r\npreempt_enable();\r\n}\r\nstatic void unregister_shadow_scb(struct kvm_vcpu *vcpu)\r\n{\r\natomic_andnot(CPUSTAT_WAIT, &vcpu->arch.sie_block->cpuflags);\r\nWRITE_ONCE(vcpu->arch.vsie_block, NULL);\r\n}\r\nstatic int vsie_run(struct kvm_vcpu *vcpu, struct vsie_page *vsie_page)\r\n{\r\nstruct kvm_s390_sie_block *scb_s = &vsie_page->scb_s;\r\nint rc = 0;\r\nwhile (1) {\r\nrc = acquire_gmap_shadow(vcpu, vsie_page);\r\nif (!rc)\r\nrc = map_prefix(vcpu, vsie_page);\r\nif (!rc) {\r\ngmap_enable(vsie_page->gmap);\r\nupdate_intervention_requests(vsie_page);\r\nrc = do_vsie_run(vcpu, vsie_page);\r\ngmap_enable(vcpu->arch.gmap);\r\n}\r\natomic_andnot(PROG_BLOCK_SIE, &scb_s->prog20);\r\nif (rc == -EAGAIN)\r\nrc = 0;\r\nif (rc || scb_s->icptcode || signal_pending(current) ||\r\nkvm_s390_vcpu_has_irq(vcpu, 0))\r\nbreak;\r\n};\r\nif (rc == -EFAULT) {\r\nscb_s->icptcode = ICPT_PROGI;\r\nscb_s->iprcc = PGM_ADDRESSING;\r\nscb_s->pgmilc = 4;\r\nscb_s->gpsw.addr = __rewind_psw(scb_s->gpsw, 4);\r\n}\r\nreturn rc;\r\n}\r\nstatic struct vsie_page *get_vsie_page(struct kvm *kvm, unsigned long addr)\r\n{\r\nstruct vsie_page *vsie_page;\r\nstruct page *page;\r\nint nr_vcpus;\r\nrcu_read_lock();\r\npage = radix_tree_lookup(&kvm->arch.vsie.addr_to_page, addr >> 9);\r\nrcu_read_unlock();\r\nif (page) {\r\nif (page_ref_inc_return(page) == 2)\r\nreturn page_to_virt(page);\r\npage_ref_dec(page);\r\n}\r\nnr_vcpus = atomic_read(&kvm->online_vcpus);\r\nmutex_lock(&kvm->arch.vsie.mutex);\r\nif (kvm->arch.vsie.page_count < nr_vcpus) {\r\npage = alloc_page(GFP_KERNEL | __GFP_ZERO | GFP_DMA);\r\nif (!page) {\r\nmutex_unlock(&kvm->arch.vsie.mutex);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\npage_ref_inc(page);\r\nkvm->arch.vsie.pages[kvm->arch.vsie.page_count] = page;\r\nkvm->arch.vsie.page_count++;\r\n} else {\r\nwhile (true) {\r\npage = kvm->arch.vsie.pages[kvm->arch.vsie.next];\r\nif (page_ref_inc_return(page) == 2)\r\nbreak;\r\npage_ref_dec(page);\r\nkvm->arch.vsie.next++;\r\nkvm->arch.vsie.next %= nr_vcpus;\r\n}\r\nradix_tree_delete(&kvm->arch.vsie.addr_to_page, page->index >> 9);\r\n}\r\npage->index = addr;\r\nif (radix_tree_insert(&kvm->arch.vsie.addr_to_page, addr >> 9, page)) {\r\npage_ref_dec(page);\r\nmutex_unlock(&kvm->arch.vsie.mutex);\r\nreturn NULL;\r\n}\r\nmutex_unlock(&kvm->arch.vsie.mutex);\r\nvsie_page = page_to_virt(page);\r\nmemset(&vsie_page->scb_s, 0, sizeof(struct kvm_s390_sie_block));\r\nrelease_gmap_shadow(vsie_page);\r\nvsie_page->fault_addr = 0;\r\nvsie_page->scb_s.ihcpu = 0xffffU;\r\nreturn vsie_page;\r\n}\r\nstatic void put_vsie_page(struct kvm *kvm, struct vsie_page *vsie_page)\r\n{\r\nstruct page *page = pfn_to_page(__pa(vsie_page) >> PAGE_SHIFT);\r\npage_ref_dec(page);\r\n}\r\nint kvm_s390_handle_vsie(struct kvm_vcpu *vcpu)\r\n{\r\nstruct vsie_page *vsie_page;\r\nunsigned long scb_addr;\r\nint rc;\r\nvcpu->stat.instruction_sie++;\r\nif (!test_kvm_cpu_feat(vcpu->kvm, KVM_S390_VM_CPU_FEAT_SIEF2))\r\nreturn -EOPNOTSUPP;\r\nif (vcpu->arch.sie_block->gpsw.mask & PSW_MASK_PSTATE)\r\nreturn kvm_s390_inject_program_int(vcpu, PGM_PRIVILEGED_OP);\r\nBUILD_BUG_ON(sizeof(struct vsie_page) != 4096);\r\nscb_addr = kvm_s390_get_base_disp_s(vcpu, NULL);\r\nif (unlikely(scb_addr & 0x1ffUL))\r\nreturn kvm_s390_inject_program_int(vcpu, PGM_SPECIFICATION);\r\nif (signal_pending(current) || kvm_s390_vcpu_has_irq(vcpu, 0))\r\nreturn 0;\r\nvsie_page = get_vsie_page(vcpu->kvm, scb_addr);\r\nif (IS_ERR(vsie_page))\r\nreturn PTR_ERR(vsie_page);\r\nelse if (!vsie_page)\r\nreturn 0;\r\nrc = pin_scb(vcpu, vsie_page, scb_addr);\r\nif (rc)\r\ngoto out_put;\r\nrc = shadow_scb(vcpu, vsie_page);\r\nif (rc)\r\ngoto out_unpin_scb;\r\nrc = pin_blocks(vcpu, vsie_page);\r\nif (rc)\r\ngoto out_unshadow;\r\nregister_shadow_scb(vcpu, vsie_page);\r\nrc = vsie_run(vcpu, vsie_page);\r\nunregister_shadow_scb(vcpu);\r\nunpin_blocks(vcpu, vsie_page);\r\nout_unshadow:\r\nunshadow_scb(vcpu, vsie_page);\r\nout_unpin_scb:\r\nunpin_scb(vcpu, vsie_page, scb_addr);\r\nout_put:\r\nput_vsie_page(vcpu->kvm, vsie_page);\r\nreturn rc < 0 ? rc : 0;\r\n}\r\nvoid kvm_s390_vsie_init(struct kvm *kvm)\r\n{\r\nmutex_init(&kvm->arch.vsie.mutex);\r\nINIT_RADIX_TREE(&kvm->arch.vsie.addr_to_page, GFP_KERNEL);\r\n}\r\nvoid kvm_s390_vsie_destroy(struct kvm *kvm)\r\n{\r\nstruct vsie_page *vsie_page;\r\nstruct page *page;\r\nint i;\r\nmutex_lock(&kvm->arch.vsie.mutex);\r\nfor (i = 0; i < kvm->arch.vsie.page_count; i++) {\r\npage = kvm->arch.vsie.pages[i];\r\nkvm->arch.vsie.pages[i] = NULL;\r\nvsie_page = page_to_virt(page);\r\nrelease_gmap_shadow(vsie_page);\r\nradix_tree_delete(&kvm->arch.vsie.addr_to_page, page->index >> 9);\r\n__free_page(page);\r\n}\r\nkvm->arch.vsie.page_count = 0;\r\nmutex_unlock(&kvm->arch.vsie.mutex);\r\n}\r\nvoid kvm_s390_vsie_kick(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_s390_sie_block *scb = READ_ONCE(vcpu->arch.vsie_block);\r\nif (scb) {\r\natomic_or(PROG_BLOCK_SIE, &scb->prog20);\r\nif (scb->prog0c & PROG_IN_SIE)\r\natomic_or(CPUSTAT_STOP_INT, &scb->cpuflags);\r\n}\r\n}
