static void vhost_disable_cross_endian(struct vhost_virtqueue *vq)\r\n{\r\nvq->user_be = !virtio_legacy_is_little_endian();\r\n}\r\nstatic void vhost_enable_cross_endian_big(struct vhost_virtqueue *vq)\r\n{\r\nvq->user_be = true;\r\n}\r\nstatic void vhost_enable_cross_endian_little(struct vhost_virtqueue *vq)\r\n{\r\nvq->user_be = false;\r\n}\r\nstatic long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)\r\n{\r\nstruct vhost_vring_state s;\r\nif (vq->private_data)\r\nreturn -EBUSY;\r\nif (copy_from_user(&s, argp, sizeof(s)))\r\nreturn -EFAULT;\r\nif (s.num != VHOST_VRING_LITTLE_ENDIAN &&\r\ns.num != VHOST_VRING_BIG_ENDIAN)\r\nreturn -EINVAL;\r\nif (s.num == VHOST_VRING_BIG_ENDIAN)\r\nvhost_enable_cross_endian_big(vq);\r\nelse\r\nvhost_enable_cross_endian_little(vq);\r\nreturn 0;\r\n}\r\nstatic long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,\r\nint __user *argp)\r\n{\r\nstruct vhost_vring_state s = {\r\n.index = idx,\r\n.num = vq->user_be\r\n};\r\nif (copy_to_user(argp, &s, sizeof(s)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic void vhost_init_is_le(struct vhost_virtqueue *vq)\r\n{\r\nvq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1) || !vq->user_be;\r\n}\r\nstatic void vhost_disable_cross_endian(struct vhost_virtqueue *vq)\r\n{\r\n}\r\nstatic long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nstatic long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,\r\nint __user *argp)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nstatic void vhost_init_is_le(struct vhost_virtqueue *vq)\r\n{\r\nif (vhost_has_feature(vq, VIRTIO_F_VERSION_1))\r\nvq->is_le = true;\r\n}\r\nstatic void vhost_reset_is_le(struct vhost_virtqueue *vq)\r\n{\r\nvq->is_le = virtio_legacy_is_little_endian();\r\n}\r\nstatic void vhost_flush_work(struct vhost_work *work)\r\n{\r\nstruct vhost_flush_struct *s;\r\ns = container_of(work, struct vhost_flush_struct, work);\r\ncomplete(&s->wait_event);\r\n}\r\nstatic void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,\r\npoll_table *pt)\r\n{\r\nstruct vhost_poll *poll;\r\npoll = container_of(pt, struct vhost_poll, table);\r\npoll->wqh = wqh;\r\nadd_wait_queue(wqh, &poll->wait);\r\n}\r\nstatic int vhost_poll_wakeup(wait_queue_t *wait, unsigned mode, int sync,\r\nvoid *key)\r\n{\r\nstruct vhost_poll *poll = container_of(wait, struct vhost_poll, wait);\r\nif (!((unsigned long)key & poll->mask))\r\nreturn 0;\r\nvhost_poll_queue(poll);\r\nreturn 0;\r\n}\r\nvoid vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)\r\n{\r\nclear_bit(VHOST_WORK_QUEUED, &work->flags);\r\nwork->fn = fn;\r\ninit_waitqueue_head(&work->done);\r\n}\r\nvoid vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,\r\nunsigned long mask, struct vhost_dev *dev)\r\n{\r\ninit_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);\r\ninit_poll_funcptr(&poll->table, vhost_poll_func);\r\npoll->mask = mask;\r\npoll->dev = dev;\r\npoll->wqh = NULL;\r\nvhost_work_init(&poll->work, fn);\r\n}\r\nint vhost_poll_start(struct vhost_poll *poll, struct file *file)\r\n{\r\nunsigned long mask;\r\nint ret = 0;\r\nif (poll->wqh)\r\nreturn 0;\r\nmask = file->f_op->poll(file, &poll->table);\r\nif (mask)\r\nvhost_poll_wakeup(&poll->wait, 0, 0, (void *)mask);\r\nif (mask & POLLERR) {\r\nif (poll->wqh)\r\nremove_wait_queue(poll->wqh, &poll->wait);\r\nret = -EINVAL;\r\n}\r\nreturn ret;\r\n}\r\nvoid vhost_poll_stop(struct vhost_poll *poll)\r\n{\r\nif (poll->wqh) {\r\nremove_wait_queue(poll->wqh, &poll->wait);\r\npoll->wqh = NULL;\r\n}\r\n}\r\nvoid vhost_work_flush(struct vhost_dev *dev, struct vhost_work *work)\r\n{\r\nstruct vhost_flush_struct flush;\r\nif (dev->worker) {\r\ninit_completion(&flush.wait_event);\r\nvhost_work_init(&flush.work, vhost_flush_work);\r\nvhost_work_queue(dev, &flush.work);\r\nwait_for_completion(&flush.wait_event);\r\n}\r\n}\r\nvoid vhost_poll_flush(struct vhost_poll *poll)\r\n{\r\nvhost_work_flush(poll->dev, &poll->work);\r\n}\r\nvoid vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)\r\n{\r\nif (!dev->worker)\r\nreturn;\r\nif (!test_and_set_bit(VHOST_WORK_QUEUED, &work->flags)) {\r\nsmp_mb();\r\nllist_add(&work->node, &dev->work_list);\r\nwake_up_process(dev->worker);\r\n}\r\n}\r\nbool vhost_has_work(struct vhost_dev *dev)\r\n{\r\nreturn !llist_empty(&dev->work_list);\r\n}\r\nvoid vhost_poll_queue(struct vhost_poll *poll)\r\n{\r\nvhost_work_queue(poll->dev, &poll->work);\r\n}\r\nstatic void vhost_vq_reset(struct vhost_dev *dev,\r\nstruct vhost_virtqueue *vq)\r\n{\r\nvq->num = 1;\r\nvq->desc = NULL;\r\nvq->avail = NULL;\r\nvq->used = NULL;\r\nvq->last_avail_idx = 0;\r\nvq->avail_idx = 0;\r\nvq->last_used_idx = 0;\r\nvq->signalled_used = 0;\r\nvq->signalled_used_valid = false;\r\nvq->used_flags = 0;\r\nvq->log_used = false;\r\nvq->log_addr = -1ull;\r\nvq->private_data = NULL;\r\nvq->acked_features = 0;\r\nvq->log_base = NULL;\r\nvq->error_ctx = NULL;\r\nvq->error = NULL;\r\nvq->kick = NULL;\r\nvq->call_ctx = NULL;\r\nvq->call = NULL;\r\nvq->log_ctx = NULL;\r\nvhost_reset_is_le(vq);\r\nvhost_disable_cross_endian(vq);\r\nvq->busyloop_timeout = 0;\r\nvq->umem = NULL;\r\nvq->iotlb = NULL;\r\n}\r\nstatic int vhost_worker(void *data)\r\n{\r\nstruct vhost_dev *dev = data;\r\nstruct vhost_work *work, *work_next;\r\nstruct llist_node *node;\r\nmm_segment_t oldfs = get_fs();\r\nset_fs(USER_DS);\r\nuse_mm(dev->mm);\r\nfor (;;) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (kthread_should_stop()) {\r\n__set_current_state(TASK_RUNNING);\r\nbreak;\r\n}\r\nnode = llist_del_all(&dev->work_list);\r\nif (!node)\r\nschedule();\r\nnode = llist_reverse_order(node);\r\nsmp_wmb();\r\nllist_for_each_entry_safe(work, work_next, node, node) {\r\nclear_bit(VHOST_WORK_QUEUED, &work->flags);\r\n__set_current_state(TASK_RUNNING);\r\nwork->fn(work);\r\nif (need_resched())\r\nschedule();\r\n}\r\n}\r\nunuse_mm(dev->mm);\r\nset_fs(oldfs);\r\nreturn 0;\r\n}\r\nstatic void vhost_vq_free_iovecs(struct vhost_virtqueue *vq)\r\n{\r\nkfree(vq->indirect);\r\nvq->indirect = NULL;\r\nkfree(vq->log);\r\nvq->log = NULL;\r\nkfree(vq->heads);\r\nvq->heads = NULL;\r\n}\r\nstatic long vhost_dev_alloc_iovecs(struct vhost_dev *dev)\r\n{\r\nstruct vhost_virtqueue *vq;\r\nint i;\r\nfor (i = 0; i < dev->nvqs; ++i) {\r\nvq = dev->vqs[i];\r\nvq->indirect = kmalloc(sizeof *vq->indirect * UIO_MAXIOV,\r\nGFP_KERNEL);\r\nvq->log = kmalloc(sizeof *vq->log * UIO_MAXIOV, GFP_KERNEL);\r\nvq->heads = kmalloc(sizeof *vq->heads * UIO_MAXIOV, GFP_KERNEL);\r\nif (!vq->indirect || !vq->log || !vq->heads)\r\ngoto err_nomem;\r\n}\r\nreturn 0;\r\nerr_nomem:\r\nfor (; i >= 0; --i)\r\nvhost_vq_free_iovecs(dev->vqs[i]);\r\nreturn -ENOMEM;\r\n}\r\nstatic void vhost_dev_free_iovecs(struct vhost_dev *dev)\r\n{\r\nint i;\r\nfor (i = 0; i < dev->nvqs; ++i)\r\nvhost_vq_free_iovecs(dev->vqs[i]);\r\n}\r\nvoid vhost_dev_init(struct vhost_dev *dev,\r\nstruct vhost_virtqueue **vqs, int nvqs)\r\n{\r\nstruct vhost_virtqueue *vq;\r\nint i;\r\ndev->vqs = vqs;\r\ndev->nvqs = nvqs;\r\nmutex_init(&dev->mutex);\r\ndev->log_ctx = NULL;\r\ndev->log_file = NULL;\r\ndev->umem = NULL;\r\ndev->iotlb = NULL;\r\ndev->mm = NULL;\r\ndev->worker = NULL;\r\ninit_llist_head(&dev->work_list);\r\ninit_waitqueue_head(&dev->wait);\r\nINIT_LIST_HEAD(&dev->read_list);\r\nINIT_LIST_HEAD(&dev->pending_list);\r\nspin_lock_init(&dev->iotlb_lock);\r\nfor (i = 0; i < dev->nvqs; ++i) {\r\nvq = dev->vqs[i];\r\nvq->log = NULL;\r\nvq->indirect = NULL;\r\nvq->heads = NULL;\r\nvq->dev = dev;\r\nmutex_init(&vq->mutex);\r\nvhost_vq_reset(dev, vq);\r\nif (vq->handle_kick)\r\nvhost_poll_init(&vq->poll, vq->handle_kick,\r\nPOLLIN, dev);\r\n}\r\n}\r\nlong vhost_dev_check_owner(struct vhost_dev *dev)\r\n{\r\nreturn dev->mm == current->mm ? 0 : -EPERM;\r\n}\r\nstatic void vhost_attach_cgroups_work(struct vhost_work *work)\r\n{\r\nstruct vhost_attach_cgroups_struct *s;\r\ns = container_of(work, struct vhost_attach_cgroups_struct, work);\r\ns->ret = cgroup_attach_task_all(s->owner, current);\r\n}\r\nstatic int vhost_attach_cgroups(struct vhost_dev *dev)\r\n{\r\nstruct vhost_attach_cgroups_struct attach;\r\nattach.owner = current;\r\nvhost_work_init(&attach.work, vhost_attach_cgroups_work);\r\nvhost_work_queue(dev, &attach.work);\r\nvhost_work_flush(dev, &attach.work);\r\nreturn attach.ret;\r\n}\r\nbool vhost_dev_has_owner(struct vhost_dev *dev)\r\n{\r\nreturn dev->mm;\r\n}\r\nlong vhost_dev_set_owner(struct vhost_dev *dev)\r\n{\r\nstruct task_struct *worker;\r\nint err;\r\nif (vhost_dev_has_owner(dev)) {\r\nerr = -EBUSY;\r\ngoto err_mm;\r\n}\r\ndev->mm = get_task_mm(current);\r\nworker = kthread_create(vhost_worker, dev, "vhost-%d", current->pid);\r\nif (IS_ERR(worker)) {\r\nerr = PTR_ERR(worker);\r\ngoto err_worker;\r\n}\r\ndev->worker = worker;\r\nwake_up_process(worker);\r\nerr = vhost_attach_cgroups(dev);\r\nif (err)\r\ngoto err_cgroup;\r\nerr = vhost_dev_alloc_iovecs(dev);\r\nif (err)\r\ngoto err_cgroup;\r\nreturn 0;\r\nerr_cgroup:\r\nkthread_stop(worker);\r\ndev->worker = NULL;\r\nerr_worker:\r\nif (dev->mm)\r\nmmput(dev->mm);\r\ndev->mm = NULL;\r\nerr_mm:\r\nreturn err;\r\n}\r\nstatic void *vhost_kvzalloc(unsigned long size)\r\n{\r\nvoid *n = kzalloc(size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);\r\nif (!n)\r\nn = vzalloc(size);\r\nreturn n;\r\n}\r\nstruct vhost_umem *vhost_dev_reset_owner_prepare(void)\r\n{\r\nreturn vhost_kvzalloc(sizeof(struct vhost_umem));\r\n}\r\nvoid vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_umem *umem)\r\n{\r\nint i;\r\nvhost_dev_cleanup(dev, true);\r\nINIT_LIST_HEAD(&umem->umem_list);\r\ndev->umem = umem;\r\nfor (i = 0; i < dev->nvqs; ++i)\r\ndev->vqs[i]->umem = umem;\r\n}\r\nvoid vhost_dev_stop(struct vhost_dev *dev)\r\n{\r\nint i;\r\nfor (i = 0; i < dev->nvqs; ++i) {\r\nif (dev->vqs[i]->kick && dev->vqs[i]->handle_kick) {\r\nvhost_poll_stop(&dev->vqs[i]->poll);\r\nvhost_poll_flush(&dev->vqs[i]->poll);\r\n}\r\n}\r\n}\r\nstatic void vhost_umem_free(struct vhost_umem *umem,\r\nstruct vhost_umem_node *node)\r\n{\r\nvhost_umem_interval_tree_remove(node, &umem->umem_tree);\r\nlist_del(&node->link);\r\nkfree(node);\r\numem->numem--;\r\n}\r\nstatic void vhost_umem_clean(struct vhost_umem *umem)\r\n{\r\nstruct vhost_umem_node *node, *tmp;\r\nif (!umem)\r\nreturn;\r\nlist_for_each_entry_safe(node, tmp, &umem->umem_list, link)\r\nvhost_umem_free(umem, node);\r\nkvfree(umem);\r\n}\r\nstatic void vhost_clear_msg(struct vhost_dev *dev)\r\n{\r\nstruct vhost_msg_node *node, *n;\r\nspin_lock(&dev->iotlb_lock);\r\nlist_for_each_entry_safe(node, n, &dev->read_list, node) {\r\nlist_del(&node->node);\r\nkfree(node);\r\n}\r\nlist_for_each_entry_safe(node, n, &dev->pending_list, node) {\r\nlist_del(&node->node);\r\nkfree(node);\r\n}\r\nspin_unlock(&dev->iotlb_lock);\r\n}\r\nvoid vhost_dev_cleanup(struct vhost_dev *dev, bool locked)\r\n{\r\nint i;\r\nfor (i = 0; i < dev->nvqs; ++i) {\r\nif (dev->vqs[i]->error_ctx)\r\neventfd_ctx_put(dev->vqs[i]->error_ctx);\r\nif (dev->vqs[i]->error)\r\nfput(dev->vqs[i]->error);\r\nif (dev->vqs[i]->kick)\r\nfput(dev->vqs[i]->kick);\r\nif (dev->vqs[i]->call_ctx)\r\neventfd_ctx_put(dev->vqs[i]->call_ctx);\r\nif (dev->vqs[i]->call)\r\nfput(dev->vqs[i]->call);\r\nvhost_vq_reset(dev, dev->vqs[i]);\r\n}\r\nvhost_dev_free_iovecs(dev);\r\nif (dev->log_ctx)\r\neventfd_ctx_put(dev->log_ctx);\r\ndev->log_ctx = NULL;\r\nif (dev->log_file)\r\nfput(dev->log_file);\r\ndev->log_file = NULL;\r\nvhost_umem_clean(dev->umem);\r\ndev->umem = NULL;\r\nvhost_umem_clean(dev->iotlb);\r\ndev->iotlb = NULL;\r\nvhost_clear_msg(dev);\r\nwake_up_interruptible_poll(&dev->wait, POLLIN | POLLRDNORM);\r\nWARN_ON(!llist_empty(&dev->work_list));\r\nif (dev->worker) {\r\nkthread_stop(dev->worker);\r\ndev->worker = NULL;\r\n}\r\nif (dev->mm)\r\nmmput(dev->mm);\r\ndev->mm = NULL;\r\n}\r\nstatic int log_access_ok(void __user *log_base, u64 addr, unsigned long sz)\r\n{\r\nu64 a = addr / VHOST_PAGE_SIZE / 8;\r\nif (a > ULONG_MAX - (unsigned long)log_base ||\r\na + (unsigned long)log_base > ULONG_MAX)\r\nreturn 0;\r\nreturn access_ok(VERIFY_WRITE, log_base + a,\r\n(sz + VHOST_PAGE_SIZE * 8 - 1) / VHOST_PAGE_SIZE / 8);\r\n}\r\nstatic bool vhost_overflow(u64 uaddr, u64 size)\r\n{\r\nreturn uaddr > ULONG_MAX || size > ULONG_MAX || uaddr > ULONG_MAX - size;\r\n}\r\nstatic int vq_memory_access_ok(void __user *log_base, struct vhost_umem *umem,\r\nint log_all)\r\n{\r\nstruct vhost_umem_node *node;\r\nif (!umem)\r\nreturn 0;\r\nlist_for_each_entry(node, &umem->umem_list, link) {\r\nunsigned long a = node->userspace_addr;\r\nif (vhost_overflow(node->userspace_addr, node->size))\r\nreturn 0;\r\nif (!access_ok(VERIFY_WRITE, (void __user *)a,\r\nnode->size))\r\nreturn 0;\r\nelse if (log_all && !log_access_ok(log_base,\r\nnode->start,\r\nnode->size))\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int memory_access_ok(struct vhost_dev *d, struct vhost_umem *umem,\r\nint log_all)\r\n{\r\nint i;\r\nfor (i = 0; i < d->nvqs; ++i) {\r\nint ok;\r\nbool log;\r\nmutex_lock(&d->vqs[i]->mutex);\r\nlog = log_all || vhost_has_feature(d->vqs[i], VHOST_F_LOG_ALL);\r\nif (d->vqs[i]->private_data)\r\nok = vq_memory_access_ok(d->vqs[i]->log_base,\r\numem, log);\r\nelse\r\nok = 1;\r\nmutex_unlock(&d->vqs[i]->mutex);\r\nif (!ok)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int vhost_copy_to_user(struct vhost_virtqueue *vq, void *to,\r\nconst void *from, unsigned size)\r\n{\r\nint ret;\r\nif (!vq->iotlb)\r\nreturn __copy_to_user(to, from, size);\r\nelse {\r\nstruct iov_iter t;\r\nret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,\r\nARRAY_SIZE(vq->iotlb_iov),\r\nVHOST_ACCESS_WO);\r\nif (ret < 0)\r\ngoto out;\r\niov_iter_init(&t, WRITE, vq->iotlb_iov, ret, size);\r\nret = copy_to_iter(from, size, &t);\r\nif (ret == size)\r\nret = 0;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,\r\nvoid *from, unsigned size)\r\n{\r\nint ret;\r\nif (!vq->iotlb)\r\nreturn __copy_from_user(to, from, size);\r\nelse {\r\nstruct iov_iter f;\r\nret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,\r\nARRAY_SIZE(vq->iotlb_iov),\r\nVHOST_ACCESS_RO);\r\nif (ret < 0) {\r\nvq_err(vq, "IOTLB translation failure: uaddr "\r\n"%p size 0x%llx\n", from,\r\n(unsigned long long) size);\r\ngoto out;\r\n}\r\niov_iter_init(&f, READ, vq->iotlb_iov, ret, size);\r\nret = copy_from_iter(to, size, &f);\r\nif (ret == size)\r\nret = 0;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic void __user *__vhost_get_user(struct vhost_virtqueue *vq,\r\nvoid *addr, unsigned size)\r\n{\r\nint ret;\r\nret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,\r\nARRAY_SIZE(vq->iotlb_iov),\r\nVHOST_ACCESS_RO);\r\nif (ret < 0) {\r\nvq_err(vq, "IOTLB translation failure: uaddr "\r\n"%p size 0x%llx\n", addr,\r\n(unsigned long long) size);\r\nreturn NULL;\r\n}\r\nif (ret != 1 || vq->iotlb_iov[0].iov_len != size) {\r\nvq_err(vq, "Non atomic userspace memory access: uaddr "\r\n"%p size 0x%llx\n", addr,\r\n(unsigned long long) size);\r\nreturn NULL;\r\n}\r\nreturn vq->iotlb_iov[0].iov_base;\r\n}\r\nstatic void vhost_dev_lock_vqs(struct vhost_dev *d)\r\n{\r\nint i = 0;\r\nfor (i = 0; i < d->nvqs; ++i)\r\nmutex_lock(&d->vqs[i]->mutex);\r\n}\r\nstatic void vhost_dev_unlock_vqs(struct vhost_dev *d)\r\n{\r\nint i = 0;\r\nfor (i = 0; i < d->nvqs; ++i)\r\nmutex_unlock(&d->vqs[i]->mutex);\r\n}\r\nstatic int vhost_new_umem_range(struct vhost_umem *umem,\r\nu64 start, u64 size, u64 end,\r\nu64 userspace_addr, int perm)\r\n{\r\nstruct vhost_umem_node *tmp, *node = kmalloc(sizeof(*node), GFP_ATOMIC);\r\nif (!node)\r\nreturn -ENOMEM;\r\nif (umem->numem == max_iotlb_entries) {\r\ntmp = list_first_entry(&umem->umem_list, typeof(*tmp), link);\r\nvhost_umem_free(umem, tmp);\r\n}\r\nnode->start = start;\r\nnode->size = size;\r\nnode->last = end;\r\nnode->userspace_addr = userspace_addr;\r\nnode->perm = perm;\r\nINIT_LIST_HEAD(&node->link);\r\nlist_add_tail(&node->link, &umem->umem_list);\r\nvhost_umem_interval_tree_insert(node, &umem->umem_tree);\r\numem->numem++;\r\nreturn 0;\r\n}\r\nstatic void vhost_del_umem_range(struct vhost_umem *umem,\r\nu64 start, u64 end)\r\n{\r\nstruct vhost_umem_node *node;\r\nwhile ((node = vhost_umem_interval_tree_iter_first(&umem->umem_tree,\r\nstart, end)))\r\nvhost_umem_free(umem, node);\r\n}\r\nstatic void vhost_iotlb_notify_vq(struct vhost_dev *d,\r\nstruct vhost_iotlb_msg *msg)\r\n{\r\nstruct vhost_msg_node *node, *n;\r\nspin_lock(&d->iotlb_lock);\r\nlist_for_each_entry_safe(node, n, &d->pending_list, node) {\r\nstruct vhost_iotlb_msg *vq_msg = &node->msg.iotlb;\r\nif (msg->iova <= vq_msg->iova &&\r\nmsg->iova + msg->size - 1 > vq_msg->iova &&\r\nvq_msg->type == VHOST_IOTLB_MISS) {\r\nvhost_poll_queue(&node->vq->poll);\r\nlist_del(&node->node);\r\nkfree(node);\r\n}\r\n}\r\nspin_unlock(&d->iotlb_lock);\r\n}\r\nstatic int umem_access_ok(u64 uaddr, u64 size, int access)\r\n{\r\nunsigned long a = uaddr;\r\nif (vhost_overflow(uaddr, size))\r\nreturn -EFAULT;\r\nif ((access & VHOST_ACCESS_RO) &&\r\n!access_ok(VERIFY_READ, (void __user *)a, size))\r\nreturn -EFAULT;\r\nif ((access & VHOST_ACCESS_WO) &&\r\n!access_ok(VERIFY_WRITE, (void __user *)a, size))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nint vhost_process_iotlb_msg(struct vhost_dev *dev,\r\nstruct vhost_iotlb_msg *msg)\r\n{\r\nint ret = 0;\r\nvhost_dev_lock_vqs(dev);\r\nswitch (msg->type) {\r\ncase VHOST_IOTLB_UPDATE:\r\nif (!dev->iotlb) {\r\nret = -EFAULT;\r\nbreak;\r\n}\r\nif (umem_access_ok(msg->uaddr, msg->size, msg->perm)) {\r\nret = -EFAULT;\r\nbreak;\r\n}\r\nif (vhost_new_umem_range(dev->iotlb, msg->iova, msg->size,\r\nmsg->iova + msg->size - 1,\r\nmsg->uaddr, msg->perm)) {\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\nvhost_iotlb_notify_vq(dev, msg);\r\nbreak;\r\ncase VHOST_IOTLB_INVALIDATE:\r\nvhost_del_umem_range(dev->iotlb, msg->iova,\r\nmsg->iova + msg->size - 1);\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nvhost_dev_unlock_vqs(dev);\r\nreturn ret;\r\n}\r\nssize_t vhost_chr_write_iter(struct vhost_dev *dev,\r\nstruct iov_iter *from)\r\n{\r\nstruct vhost_msg_node node;\r\nunsigned size = sizeof(struct vhost_msg);\r\nsize_t ret;\r\nint err;\r\nif (iov_iter_count(from) < size)\r\nreturn 0;\r\nret = copy_from_iter(&node.msg, size, from);\r\nif (ret != size)\r\ngoto done;\r\nswitch (node.msg.type) {\r\ncase VHOST_IOTLB_MSG:\r\nerr = vhost_process_iotlb_msg(dev, &node.msg.iotlb);\r\nif (err)\r\nret = err;\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\nbreak;\r\n}\r\ndone:\r\nreturn ret;\r\n}\r\nunsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,\r\npoll_table *wait)\r\n{\r\nunsigned int mask = 0;\r\npoll_wait(file, &dev->wait, wait);\r\nif (!list_empty(&dev->read_list))\r\nmask |= POLLIN | POLLRDNORM;\r\nreturn mask;\r\n}\r\nssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,\r\nint noblock)\r\n{\r\nDEFINE_WAIT(wait);\r\nstruct vhost_msg_node *node;\r\nssize_t ret = 0;\r\nunsigned size = sizeof(struct vhost_msg);\r\nif (iov_iter_count(to) < size)\r\nreturn 0;\r\nwhile (1) {\r\nif (!noblock)\r\nprepare_to_wait(&dev->wait, &wait,\r\nTASK_INTERRUPTIBLE);\r\nnode = vhost_dequeue_msg(dev, &dev->read_list);\r\nif (node)\r\nbreak;\r\nif (noblock) {\r\nret = -EAGAIN;\r\nbreak;\r\n}\r\nif (signal_pending(current)) {\r\nret = -ERESTARTSYS;\r\nbreak;\r\n}\r\nif (!dev->iotlb) {\r\nret = -EBADFD;\r\nbreak;\r\n}\r\nschedule();\r\n}\r\nif (!noblock)\r\nfinish_wait(&dev->wait, &wait);\r\nif (node) {\r\nret = copy_to_iter(&node->msg, size, to);\r\nif (ret != size || node->msg.type != VHOST_IOTLB_MISS) {\r\nkfree(node);\r\nreturn ret;\r\n}\r\nvhost_enqueue_msg(dev, &dev->pending_list, node);\r\n}\r\nreturn ret;\r\n}\r\nstatic int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)\r\n{\r\nstruct vhost_dev *dev = vq->dev;\r\nstruct vhost_msg_node *node;\r\nstruct vhost_iotlb_msg *msg;\r\nnode = vhost_new_msg(vq, VHOST_IOTLB_MISS);\r\nif (!node)\r\nreturn -ENOMEM;\r\nmsg = &node->msg.iotlb;\r\nmsg->type = VHOST_IOTLB_MISS;\r\nmsg->iova = iova;\r\nmsg->perm = access;\r\nvhost_enqueue_msg(dev, &dev->read_list, node);\r\nreturn 0;\r\n}\r\nstatic int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,\r\nstruct vring_desc __user *desc,\r\nstruct vring_avail __user *avail,\r\nstruct vring_used __user *used)\r\n{\r\nsize_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;\r\nreturn access_ok(VERIFY_READ, desc, num * sizeof *desc) &&\r\naccess_ok(VERIFY_READ, avail,\r\nsizeof *avail + num * sizeof *avail->ring + s) &&\r\naccess_ok(VERIFY_WRITE, used,\r\nsizeof *used + num * sizeof *used->ring + s);\r\n}\r\nstatic int iotlb_access_ok(struct vhost_virtqueue *vq,\r\nint access, u64 addr, u64 len)\r\n{\r\nconst struct vhost_umem_node *node;\r\nstruct vhost_umem *umem = vq->iotlb;\r\nu64 s = 0, size;\r\nwhile (len > s) {\r\nnode = vhost_umem_interval_tree_iter_first(&umem->umem_tree,\r\naddr,\r\naddr + len - 1);\r\nif (node == NULL || node->start > addr) {\r\nvhost_iotlb_miss(vq, addr, access);\r\nreturn false;\r\n} else if (!(node->perm & access)) {\r\nreturn false;\r\n}\r\nsize = node->size - addr + node->start;\r\ns += size;\r\naddr += size;\r\n}\r\nreturn true;\r\n}\r\nint vq_iotlb_prefetch(struct vhost_virtqueue *vq)\r\n{\r\nsize_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;\r\nunsigned int num = vq->num;\r\nif (!vq->iotlb)\r\nreturn 1;\r\nreturn iotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq->desc,\r\nnum * sizeof *vq->desc) &&\r\niotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq->avail,\r\nsizeof *vq->avail +\r\nnum * sizeof *vq->avail->ring + s) &&\r\niotlb_access_ok(vq, VHOST_ACCESS_WO, (u64)(uintptr_t)vq->used,\r\nsizeof *vq->used +\r\nnum * sizeof *vq->used->ring + s);\r\n}\r\nint vhost_log_access_ok(struct vhost_dev *dev)\r\n{\r\nreturn memory_access_ok(dev, dev->umem, 1);\r\n}\r\nstatic int vq_log_access_ok(struct vhost_virtqueue *vq,\r\nvoid __user *log_base)\r\n{\r\nsize_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;\r\nreturn vq_memory_access_ok(log_base, vq->umem,\r\nvhost_has_feature(vq, VHOST_F_LOG_ALL)) &&\r\n(!vq->log_used || log_access_ok(log_base, vq->log_addr,\r\nsizeof *vq->used +\r\nvq->num * sizeof *vq->used->ring + s));\r\n}\r\nint vhost_vq_access_ok(struct vhost_virtqueue *vq)\r\n{\r\nif (vq->iotlb) {\r\nreturn 1;\r\n}\r\nreturn vq_access_ok(vq, vq->num, vq->desc, vq->avail, vq->used) &&\r\nvq_log_access_ok(vq, vq->log_base);\r\n}\r\nstatic struct vhost_umem *vhost_umem_alloc(void)\r\n{\r\nstruct vhost_umem *umem = vhost_kvzalloc(sizeof(*umem));\r\nif (!umem)\r\nreturn NULL;\r\numem->umem_tree = RB_ROOT;\r\numem->numem = 0;\r\nINIT_LIST_HEAD(&umem->umem_list);\r\nreturn umem;\r\n}\r\nstatic long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)\r\n{\r\nstruct vhost_memory mem, *newmem;\r\nstruct vhost_memory_region *region;\r\nstruct vhost_umem *newumem, *oldumem;\r\nunsigned long size = offsetof(struct vhost_memory, regions);\r\nint i;\r\nif (copy_from_user(&mem, m, size))\r\nreturn -EFAULT;\r\nif (mem.padding)\r\nreturn -EOPNOTSUPP;\r\nif (mem.nregions > max_mem_regions)\r\nreturn -E2BIG;\r\nnewmem = vhost_kvzalloc(size + mem.nregions * sizeof(*m->regions));\r\nif (!newmem)\r\nreturn -ENOMEM;\r\nmemcpy(newmem, &mem, size);\r\nif (copy_from_user(newmem->regions, m->regions,\r\nmem.nregions * sizeof *m->regions)) {\r\nkvfree(newmem);\r\nreturn -EFAULT;\r\n}\r\nnewumem = vhost_umem_alloc();\r\nif (!newumem) {\r\nkvfree(newmem);\r\nreturn -ENOMEM;\r\n}\r\nfor (region = newmem->regions;\r\nregion < newmem->regions + mem.nregions;\r\nregion++) {\r\nif (vhost_new_umem_range(newumem,\r\nregion->guest_phys_addr,\r\nregion->memory_size,\r\nregion->guest_phys_addr +\r\nregion->memory_size - 1,\r\nregion->userspace_addr,\r\nVHOST_ACCESS_RW))\r\ngoto err;\r\n}\r\nif (!memory_access_ok(d, newumem, 0))\r\ngoto err;\r\noldumem = d->umem;\r\nd->umem = newumem;\r\nfor (i = 0; i < d->nvqs; ++i) {\r\nmutex_lock(&d->vqs[i]->mutex);\r\nd->vqs[i]->umem = newumem;\r\nmutex_unlock(&d->vqs[i]->mutex);\r\n}\r\nkvfree(newmem);\r\nvhost_umem_clean(oldumem);\r\nreturn 0;\r\nerr:\r\nvhost_umem_clean(newumem);\r\nkvfree(newmem);\r\nreturn -EFAULT;\r\n}\r\nlong vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)\r\n{\r\nstruct file *eventfp, *filep = NULL;\r\nbool pollstart = false, pollstop = false;\r\nstruct eventfd_ctx *ctx = NULL;\r\nu32 __user *idxp = argp;\r\nstruct vhost_virtqueue *vq;\r\nstruct vhost_vring_state s;\r\nstruct vhost_vring_file f;\r\nstruct vhost_vring_addr a;\r\nu32 idx;\r\nlong r;\r\nr = get_user(idx, idxp);\r\nif (r < 0)\r\nreturn r;\r\nif (idx >= d->nvqs)\r\nreturn -ENOBUFS;\r\nvq = d->vqs[idx];\r\nmutex_lock(&vq->mutex);\r\nswitch (ioctl) {\r\ncase VHOST_SET_VRING_NUM:\r\nif (vq->private_data) {\r\nr = -EBUSY;\r\nbreak;\r\n}\r\nif (copy_from_user(&s, argp, sizeof s)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nif (!s.num || s.num > 0xffff || (s.num & (s.num - 1))) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nvq->num = s.num;\r\nbreak;\r\ncase VHOST_SET_VRING_BASE:\r\nif (vq->private_data) {\r\nr = -EBUSY;\r\nbreak;\r\n}\r\nif (copy_from_user(&s, argp, sizeof s)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nif (s.num > 0xffff) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nvq->last_avail_idx = s.num;\r\nvq->avail_idx = vq->last_avail_idx;\r\nbreak;\r\ncase VHOST_GET_VRING_BASE:\r\ns.index = idx;\r\ns.num = vq->last_avail_idx;\r\nif (copy_to_user(argp, &s, sizeof s))\r\nr = -EFAULT;\r\nbreak;\r\ncase VHOST_SET_VRING_ADDR:\r\nif (copy_from_user(&a, argp, sizeof a)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nif (a.flags & ~(0x1 << VHOST_VRING_F_LOG)) {\r\nr = -EOPNOTSUPP;\r\nbreak;\r\n}\r\nif ((u64)(unsigned long)a.desc_user_addr != a.desc_user_addr ||\r\n(u64)(unsigned long)a.used_user_addr != a.used_user_addr ||\r\n(u64)(unsigned long)a.avail_user_addr != a.avail_user_addr) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nBUILD_BUG_ON(__alignof__ *vq->avail > VRING_AVAIL_ALIGN_SIZE);\r\nBUILD_BUG_ON(__alignof__ *vq->used > VRING_USED_ALIGN_SIZE);\r\nif ((a.avail_user_addr & (VRING_AVAIL_ALIGN_SIZE - 1)) ||\r\n(a.used_user_addr & (VRING_USED_ALIGN_SIZE - 1)) ||\r\n(a.log_guest_addr & (VRING_USED_ALIGN_SIZE - 1))) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nif (vq->private_data) {\r\nif (!vq_access_ok(vq, vq->num,\r\n(void __user *)(unsigned long)a.desc_user_addr,\r\n(void __user *)(unsigned long)a.avail_user_addr,\r\n(void __user *)(unsigned long)a.used_user_addr)) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nif ((a.flags & (0x1 << VHOST_VRING_F_LOG)) &&\r\n!log_access_ok(vq->log_base, a.log_guest_addr,\r\nsizeof *vq->used +\r\nvq->num * sizeof *vq->used->ring)) {\r\nr = -EINVAL;\r\nbreak;\r\n}\r\n}\r\nvq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));\r\nvq->desc = (void __user *)(unsigned long)a.desc_user_addr;\r\nvq->avail = (void __user *)(unsigned long)a.avail_user_addr;\r\nvq->log_addr = a.log_guest_addr;\r\nvq->used = (void __user *)(unsigned long)a.used_user_addr;\r\nbreak;\r\ncase VHOST_SET_VRING_KICK:\r\nif (copy_from_user(&f, argp, sizeof f)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\neventfp = f.fd == -1 ? NULL : eventfd_fget(f.fd);\r\nif (IS_ERR(eventfp)) {\r\nr = PTR_ERR(eventfp);\r\nbreak;\r\n}\r\nif (eventfp != vq->kick) {\r\npollstop = (filep = vq->kick) != NULL;\r\npollstart = (vq->kick = eventfp) != NULL;\r\n} else\r\nfilep = eventfp;\r\nbreak;\r\ncase VHOST_SET_VRING_CALL:\r\nif (copy_from_user(&f, argp, sizeof f)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\neventfp = f.fd == -1 ? NULL : eventfd_fget(f.fd);\r\nif (IS_ERR(eventfp)) {\r\nr = PTR_ERR(eventfp);\r\nbreak;\r\n}\r\nif (eventfp != vq->call) {\r\nfilep = vq->call;\r\nctx = vq->call_ctx;\r\nvq->call = eventfp;\r\nvq->call_ctx = eventfp ?\r\neventfd_ctx_fileget(eventfp) : NULL;\r\n} else\r\nfilep = eventfp;\r\nbreak;\r\ncase VHOST_SET_VRING_ERR:\r\nif (copy_from_user(&f, argp, sizeof f)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\neventfp = f.fd == -1 ? NULL : eventfd_fget(f.fd);\r\nif (IS_ERR(eventfp)) {\r\nr = PTR_ERR(eventfp);\r\nbreak;\r\n}\r\nif (eventfp != vq->error) {\r\nfilep = vq->error;\r\nvq->error = eventfp;\r\nctx = vq->error_ctx;\r\nvq->error_ctx = eventfp ?\r\neventfd_ctx_fileget(eventfp) : NULL;\r\n} else\r\nfilep = eventfp;\r\nbreak;\r\ncase VHOST_SET_VRING_ENDIAN:\r\nr = vhost_set_vring_endian(vq, argp);\r\nbreak;\r\ncase VHOST_GET_VRING_ENDIAN:\r\nr = vhost_get_vring_endian(vq, idx, argp);\r\nbreak;\r\ncase VHOST_SET_VRING_BUSYLOOP_TIMEOUT:\r\nif (copy_from_user(&s, argp, sizeof(s))) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nvq->busyloop_timeout = s.num;\r\nbreak;\r\ncase VHOST_GET_VRING_BUSYLOOP_TIMEOUT:\r\ns.index = idx;\r\ns.num = vq->busyloop_timeout;\r\nif (copy_to_user(argp, &s, sizeof(s)))\r\nr = -EFAULT;\r\nbreak;\r\ndefault:\r\nr = -ENOIOCTLCMD;\r\n}\r\nif (pollstop && vq->handle_kick)\r\nvhost_poll_stop(&vq->poll);\r\nif (ctx)\r\neventfd_ctx_put(ctx);\r\nif (filep)\r\nfput(filep);\r\nif (pollstart && vq->handle_kick)\r\nr = vhost_poll_start(&vq->poll, vq->kick);\r\nmutex_unlock(&vq->mutex);\r\nif (pollstop && vq->handle_kick)\r\nvhost_poll_flush(&vq->poll);\r\nreturn r;\r\n}\r\nint vhost_init_device_iotlb(struct vhost_dev *d, bool enabled)\r\n{\r\nstruct vhost_umem *niotlb, *oiotlb;\r\nint i;\r\nniotlb = vhost_umem_alloc();\r\nif (!niotlb)\r\nreturn -ENOMEM;\r\noiotlb = d->iotlb;\r\nd->iotlb = niotlb;\r\nfor (i = 0; i < d->nvqs; ++i) {\r\nmutex_lock(&d->vqs[i]->mutex);\r\nd->vqs[i]->iotlb = niotlb;\r\nmutex_unlock(&d->vqs[i]->mutex);\r\n}\r\nvhost_umem_clean(oiotlb);\r\nreturn 0;\r\n}\r\nlong vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)\r\n{\r\nstruct file *eventfp, *filep = NULL;\r\nstruct eventfd_ctx *ctx = NULL;\r\nu64 p;\r\nlong r;\r\nint i, fd;\r\nif (ioctl == VHOST_SET_OWNER) {\r\nr = vhost_dev_set_owner(d);\r\ngoto done;\r\n}\r\nr = vhost_dev_check_owner(d);\r\nif (r)\r\ngoto done;\r\nswitch (ioctl) {\r\ncase VHOST_SET_MEM_TABLE:\r\nr = vhost_set_memory(d, argp);\r\nbreak;\r\ncase VHOST_SET_LOG_BASE:\r\nif (copy_from_user(&p, argp, sizeof p)) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nif ((u64)(unsigned long)p != p) {\r\nr = -EFAULT;\r\nbreak;\r\n}\r\nfor (i = 0; i < d->nvqs; ++i) {\r\nstruct vhost_virtqueue *vq;\r\nvoid __user *base = (void __user *)(unsigned long)p;\r\nvq = d->vqs[i];\r\nmutex_lock(&vq->mutex);\r\nif (vq->private_data && !vq_log_access_ok(vq, base))\r\nr = -EFAULT;\r\nelse\r\nvq->log_base = base;\r\nmutex_unlock(&vq->mutex);\r\n}\r\nbreak;\r\ncase VHOST_SET_LOG_FD:\r\nr = get_user(fd, (int __user *)argp);\r\nif (r < 0)\r\nbreak;\r\neventfp = fd == -1 ? NULL : eventfd_fget(fd);\r\nif (IS_ERR(eventfp)) {\r\nr = PTR_ERR(eventfp);\r\nbreak;\r\n}\r\nif (eventfp != d->log_file) {\r\nfilep = d->log_file;\r\nd->log_file = eventfp;\r\nctx = d->log_ctx;\r\nd->log_ctx = eventfp ?\r\neventfd_ctx_fileget(eventfp) : NULL;\r\n} else\r\nfilep = eventfp;\r\nfor (i = 0; i < d->nvqs; ++i) {\r\nmutex_lock(&d->vqs[i]->mutex);\r\nd->vqs[i]->log_ctx = d->log_ctx;\r\nmutex_unlock(&d->vqs[i]->mutex);\r\n}\r\nif (ctx)\r\neventfd_ctx_put(ctx);\r\nif (filep)\r\nfput(filep);\r\nbreak;\r\ndefault:\r\nr = -ENOIOCTLCMD;\r\nbreak;\r\n}\r\ndone:\r\nreturn r;\r\n}\r\nstatic int set_bit_to_user(int nr, void __user *addr)\r\n{\r\nunsigned long log = (unsigned long)addr;\r\nstruct page *page;\r\nvoid *base;\r\nint bit = nr + (log % PAGE_SIZE) * 8;\r\nint r;\r\nr = get_user_pages_fast(log, 1, 1, &page);\r\nif (r < 0)\r\nreturn r;\r\nBUG_ON(r != 1);\r\nbase = kmap_atomic(page);\r\nset_bit(bit, base);\r\nkunmap_atomic(base);\r\nset_page_dirty_lock(page);\r\nput_page(page);\r\nreturn 0;\r\n}\r\nstatic int log_write(void __user *log_base,\r\nu64 write_address, u64 write_length)\r\n{\r\nu64 write_page = write_address / VHOST_PAGE_SIZE;\r\nint r;\r\nif (!write_length)\r\nreturn 0;\r\nwrite_length += write_address % VHOST_PAGE_SIZE;\r\nfor (;;) {\r\nu64 base = (u64)(unsigned long)log_base;\r\nu64 log = base + write_page / 8;\r\nint bit = write_page % 8;\r\nif ((u64)(unsigned long)log != log)\r\nreturn -EFAULT;\r\nr = set_bit_to_user(bit, (void __user *)(unsigned long)log);\r\nif (r < 0)\r\nreturn r;\r\nif (write_length <= VHOST_PAGE_SIZE)\r\nbreak;\r\nwrite_length -= VHOST_PAGE_SIZE;\r\nwrite_page += 1;\r\n}\r\nreturn r;\r\n}\r\nint vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,\r\nunsigned int log_num, u64 len)\r\n{\r\nint i, r;\r\nsmp_wmb();\r\nfor (i = 0; i < log_num; ++i) {\r\nu64 l = min(log[i].len, len);\r\nr = log_write(vq->log_base, log[i].addr, l);\r\nif (r < 0)\r\nreturn r;\r\nlen -= l;\r\nif (!len) {\r\nif (vq->log_ctx)\r\neventfd_signal(vq->log_ctx, 1);\r\nreturn 0;\r\n}\r\n}\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic int vhost_update_used_flags(struct vhost_virtqueue *vq)\r\n{\r\nvoid __user *used;\r\nif (vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),\r\n&vq->used->flags) < 0)\r\nreturn -EFAULT;\r\nif (unlikely(vq->log_used)) {\r\nsmp_wmb();\r\nused = &vq->used->flags;\r\nlog_write(vq->log_base, vq->log_addr +\r\n(used - (void __user *)vq->used),\r\nsizeof vq->used->flags);\r\nif (vq->log_ctx)\r\neventfd_signal(vq->log_ctx, 1);\r\n}\r\nreturn 0;\r\n}\r\nstatic int vhost_update_avail_event(struct vhost_virtqueue *vq, u16 avail_event)\r\n{\r\nif (vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),\r\nvhost_avail_event(vq)))\r\nreturn -EFAULT;\r\nif (unlikely(vq->log_used)) {\r\nvoid __user *used;\r\nsmp_wmb();\r\nused = vhost_avail_event(vq);\r\nlog_write(vq->log_base, vq->log_addr +\r\n(used - (void __user *)vq->used),\r\nsizeof *vhost_avail_event(vq));\r\nif (vq->log_ctx)\r\neventfd_signal(vq->log_ctx, 1);\r\n}\r\nreturn 0;\r\n}\r\nint vhost_vq_init_access(struct vhost_virtqueue *vq)\r\n{\r\n__virtio16 last_used_idx;\r\nint r;\r\nbool is_le = vq->is_le;\r\nif (!vq->private_data) {\r\nvhost_reset_is_le(vq);\r\nreturn 0;\r\n}\r\nvhost_init_is_le(vq);\r\nr = vhost_update_used_flags(vq);\r\nif (r)\r\ngoto err;\r\nvq->signalled_used_valid = false;\r\nif (!vq->iotlb &&\r\n!access_ok(VERIFY_READ, &vq->used->idx, sizeof vq->used->idx)) {\r\nr = -EFAULT;\r\ngoto err;\r\n}\r\nr = vhost_get_user(vq, last_used_idx, &vq->used->idx);\r\nif (r) {\r\nvq_err(vq, "Can't access used idx at %p\n",\r\n&vq->used->idx);\r\ngoto err;\r\n}\r\nvq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);\r\nreturn 0;\r\nerr:\r\nvq->is_le = is_le;\r\nreturn r;\r\n}\r\nstatic int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,\r\nstruct iovec iov[], int iov_size, int access)\r\n{\r\nconst struct vhost_umem_node *node;\r\nstruct vhost_dev *dev = vq->dev;\r\nstruct vhost_umem *umem = dev->iotlb ? dev->iotlb : dev->umem;\r\nstruct iovec *_iov;\r\nu64 s = 0;\r\nint ret = 0;\r\nwhile ((u64)len > s) {\r\nu64 size;\r\nif (unlikely(ret >= iov_size)) {\r\nret = -ENOBUFS;\r\nbreak;\r\n}\r\nnode = vhost_umem_interval_tree_iter_first(&umem->umem_tree,\r\naddr, addr + len - 1);\r\nif (node == NULL || node->start > addr) {\r\nif (umem != dev->iotlb) {\r\nret = -EFAULT;\r\nbreak;\r\n}\r\nret = -EAGAIN;\r\nbreak;\r\n} else if (!(node->perm & access)) {\r\nret = -EPERM;\r\nbreak;\r\n}\r\n_iov = iov + ret;\r\nsize = node->size - addr + node->start;\r\n_iov->iov_len = min((u64)len - s, size);\r\n_iov->iov_base = (void __user *)(unsigned long)\r\n(node->userspace_addr + addr - node->start);\r\ns += size;\r\naddr += size;\r\n++ret;\r\n}\r\nif (ret == -EAGAIN)\r\nvhost_iotlb_miss(vq, addr, access);\r\nreturn ret;\r\n}\r\nstatic unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)\r\n{\r\nunsigned int next;\r\nif (!(desc->flags & cpu_to_vhost16(vq, VRING_DESC_F_NEXT)))\r\nreturn -1U;\r\nnext = vhost16_to_cpu(vq, desc->next);\r\nread_barrier_depends();\r\nreturn next;\r\n}\r\nstatic int get_indirect(struct vhost_virtqueue *vq,\r\nstruct iovec iov[], unsigned int iov_size,\r\nunsigned int *out_num, unsigned int *in_num,\r\nstruct vhost_log *log, unsigned int *log_num,\r\nstruct vring_desc *indirect)\r\n{\r\nstruct vring_desc desc;\r\nunsigned int i = 0, count, found = 0;\r\nu32 len = vhost32_to_cpu(vq, indirect->len);\r\nstruct iov_iter from;\r\nint ret, access;\r\nif (unlikely(len % sizeof desc)) {\r\nvq_err(vq, "Invalid length in indirect descriptor: "\r\n"len 0x%llx not multiple of 0x%zx\n",\r\n(unsigned long long)len,\r\nsizeof desc);\r\nreturn -EINVAL;\r\n}\r\nret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,\r\nUIO_MAXIOV, VHOST_ACCESS_RO);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -EAGAIN)\r\nvq_err(vq, "Translation failure %d in indirect.\n", ret);\r\nreturn ret;\r\n}\r\niov_iter_init(&from, READ, vq->indirect, ret, len);\r\nread_barrier_depends();\r\ncount = len / sizeof desc;\r\nif (unlikely(count > USHRT_MAX + 1)) {\r\nvq_err(vq, "Indirect buffer length too big: %d\n",\r\nindirect->len);\r\nreturn -E2BIG;\r\n}\r\ndo {\r\nunsigned iov_count = *in_num + *out_num;\r\nif (unlikely(++found > count)) {\r\nvq_err(vq, "Loop detected: last one at %u "\r\n"indirect size %u\n",\r\ni, count);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(copy_from_iter(&desc, sizeof(desc), &from) !=\r\nsizeof(desc))) {\r\nvq_err(vq, "Failed indirect descriptor: idx %d, %zx\n",\r\ni, (size_t)vhost64_to_cpu(vq, indirect->addr) + i * sizeof desc);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT))) {\r\nvq_err(vq, "Nested indirect descriptor: idx %d, %zx\n",\r\ni, (size_t)vhost64_to_cpu(vq, indirect->addr) + i * sizeof desc);\r\nreturn -EINVAL;\r\n}\r\nif (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_WRITE))\r\naccess = VHOST_ACCESS_WO;\r\nelse\r\naccess = VHOST_ACCESS_RO;\r\nret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),\r\nvhost32_to_cpu(vq, desc.len), iov + iov_count,\r\niov_size - iov_count, access);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -EAGAIN)\r\nvq_err(vq, "Translation failure %d indirect idx %d\n",\r\nret, i);\r\nreturn ret;\r\n}\r\nif (access == VHOST_ACCESS_WO) {\r\n*in_num += ret;\r\nif (unlikely(log)) {\r\nlog[*log_num].addr = vhost64_to_cpu(vq, desc.addr);\r\nlog[*log_num].len = vhost32_to_cpu(vq, desc.len);\r\n++*log_num;\r\n}\r\n} else {\r\nif (unlikely(*in_num)) {\r\nvq_err(vq, "Indirect descriptor "\r\n"has out after in: idx %d\n", i);\r\nreturn -EINVAL;\r\n}\r\n*out_num += ret;\r\n}\r\n} while ((i = next_desc(vq, &desc)) != -1);\r\nreturn 0;\r\n}\r\nint vhost_get_vq_desc(struct vhost_virtqueue *vq,\r\nstruct iovec iov[], unsigned int iov_size,\r\nunsigned int *out_num, unsigned int *in_num,\r\nstruct vhost_log *log, unsigned int *log_num)\r\n{\r\nstruct vring_desc desc;\r\nunsigned int i, head, found = 0;\r\nu16 last_avail_idx;\r\n__virtio16 avail_idx;\r\n__virtio16 ring_head;\r\nint ret, access;\r\nlast_avail_idx = vq->last_avail_idx;\r\nif (unlikely(vhost_get_user(vq, avail_idx, &vq->avail->idx))) {\r\nvq_err(vq, "Failed to access avail idx at %p\n",\r\n&vq->avail->idx);\r\nreturn -EFAULT;\r\n}\r\nvq->avail_idx = vhost16_to_cpu(vq, avail_idx);\r\nif (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {\r\nvq_err(vq, "Guest moved used index from %u to %u",\r\nlast_avail_idx, vq->avail_idx);\r\nreturn -EFAULT;\r\n}\r\nif (vq->avail_idx == last_avail_idx)\r\nreturn vq->num;\r\nsmp_rmb();\r\nif (unlikely(vhost_get_user(vq, ring_head,\r\n&vq->avail->ring[last_avail_idx & (vq->num - 1)]))) {\r\nvq_err(vq, "Failed to read head: idx %d address %p\n",\r\nlast_avail_idx,\r\n&vq->avail->ring[last_avail_idx % vq->num]);\r\nreturn -EFAULT;\r\n}\r\nhead = vhost16_to_cpu(vq, ring_head);\r\nif (unlikely(head >= vq->num)) {\r\nvq_err(vq, "Guest says index %u > %u is available",\r\nhead, vq->num);\r\nreturn -EINVAL;\r\n}\r\n*out_num = *in_num = 0;\r\nif (unlikely(log))\r\n*log_num = 0;\r\ni = head;\r\ndo {\r\nunsigned iov_count = *in_num + *out_num;\r\nif (unlikely(i >= vq->num)) {\r\nvq_err(vq, "Desc index is %u > %u, head = %u",\r\ni, vq->num, head);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(++found > vq->num)) {\r\nvq_err(vq, "Loop detected: last one at %u "\r\n"vq size %u head %u\n",\r\ni, vq->num, head);\r\nreturn -EINVAL;\r\n}\r\nret = vhost_copy_from_user(vq, &desc, vq->desc + i,\r\nsizeof desc);\r\nif (unlikely(ret)) {\r\nvq_err(vq, "Failed to get descriptor: idx %d addr %p\n",\r\ni, vq->desc + i);\r\nreturn -EFAULT;\r\n}\r\nif (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {\r\nret = get_indirect(vq, iov, iov_size,\r\nout_num, in_num,\r\nlog, log_num, &desc);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -EAGAIN)\r\nvq_err(vq, "Failure detected "\r\n"in indirect descriptor at idx %d\n", i);\r\nreturn ret;\r\n}\r\ncontinue;\r\n}\r\nif (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_WRITE))\r\naccess = VHOST_ACCESS_WO;\r\nelse\r\naccess = VHOST_ACCESS_RO;\r\nret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),\r\nvhost32_to_cpu(vq, desc.len), iov + iov_count,\r\niov_size - iov_count, access);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -EAGAIN)\r\nvq_err(vq, "Translation failure %d descriptor idx %d\n",\r\nret, i);\r\nreturn ret;\r\n}\r\nif (access == VHOST_ACCESS_WO) {\r\n*in_num += ret;\r\nif (unlikely(log)) {\r\nlog[*log_num].addr = vhost64_to_cpu(vq, desc.addr);\r\nlog[*log_num].len = vhost32_to_cpu(vq, desc.len);\r\n++*log_num;\r\n}\r\n} else {\r\nif (unlikely(*in_num)) {\r\nvq_err(vq, "Descriptor has out after in: "\r\n"idx %d\n", i);\r\nreturn -EINVAL;\r\n}\r\n*out_num += ret;\r\n}\r\n} while ((i = next_desc(vq, &desc)) != -1);\r\nvq->last_avail_idx++;\r\nBUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));\r\nreturn head;\r\n}\r\nvoid vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)\r\n{\r\nvq->last_avail_idx -= n;\r\n}\r\nint vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)\r\n{\r\nstruct vring_used_elem heads = {\r\ncpu_to_vhost32(vq, head),\r\ncpu_to_vhost32(vq, len)\r\n};\r\nreturn vhost_add_used_n(vq, &heads, 1);\r\n}\r\nstatic int __vhost_add_used_n(struct vhost_virtqueue *vq,\r\nstruct vring_used_elem *heads,\r\nunsigned count)\r\n{\r\nstruct vring_used_elem __user *used;\r\nu16 old, new;\r\nint start;\r\nstart = vq->last_used_idx & (vq->num - 1);\r\nused = vq->used->ring + start;\r\nif (count == 1) {\r\nif (vhost_put_user(vq, heads[0].id, &used->id)) {\r\nvq_err(vq, "Failed to write used id");\r\nreturn -EFAULT;\r\n}\r\nif (vhost_put_user(vq, heads[0].len, &used->len)) {\r\nvq_err(vq, "Failed to write used len");\r\nreturn -EFAULT;\r\n}\r\n} else if (vhost_copy_to_user(vq, used, heads, count * sizeof *used)) {\r\nvq_err(vq, "Failed to write used");\r\nreturn -EFAULT;\r\n}\r\nif (unlikely(vq->log_used)) {\r\nsmp_wmb();\r\nlog_write(vq->log_base,\r\nvq->log_addr +\r\n((void __user *)used - (void __user *)vq->used),\r\ncount * sizeof *used);\r\n}\r\nold = vq->last_used_idx;\r\nnew = (vq->last_used_idx += count);\r\nif (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))\r\nvq->signalled_used_valid = false;\r\nreturn 0;\r\n}\r\nint vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,\r\nunsigned count)\r\n{\r\nint start, n, r;\r\nstart = vq->last_used_idx & (vq->num - 1);\r\nn = vq->num - start;\r\nif (n < count) {\r\nr = __vhost_add_used_n(vq, heads, n);\r\nif (r < 0)\r\nreturn r;\r\nheads += n;\r\ncount -= n;\r\n}\r\nr = __vhost_add_used_n(vq, heads, count);\r\nsmp_wmb();\r\nif (vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),\r\n&vq->used->idx)) {\r\nvq_err(vq, "Failed to increment used idx");\r\nreturn -EFAULT;\r\n}\r\nif (unlikely(vq->log_used)) {\r\nlog_write(vq->log_base,\r\nvq->log_addr + offsetof(struct vring_used, idx),\r\nsizeof vq->used->idx);\r\nif (vq->log_ctx)\r\neventfd_signal(vq->log_ctx, 1);\r\n}\r\nreturn r;\r\n}\r\nstatic bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\n__u16 old, new;\r\n__virtio16 event;\r\nbool v;\r\nsmp_mb();\r\nif (vhost_has_feature(vq, VIRTIO_F_NOTIFY_ON_EMPTY) &&\r\nunlikely(vq->avail_idx == vq->last_avail_idx))\r\nreturn true;\r\nif (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {\r\n__virtio16 flags;\r\nif (vhost_get_user(vq, flags, &vq->avail->flags)) {\r\nvq_err(vq, "Failed to get flags");\r\nreturn true;\r\n}\r\nreturn !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));\r\n}\r\nold = vq->signalled_used;\r\nv = vq->signalled_used_valid;\r\nnew = vq->signalled_used = vq->last_used_idx;\r\nvq->signalled_used_valid = true;\r\nif (unlikely(!v))\r\nreturn true;\r\nif (vhost_get_user(vq, event, vhost_used_event(vq))) {\r\nvq_err(vq, "Failed to get used event idx");\r\nreturn true;\r\n}\r\nreturn vring_need_event(vhost16_to_cpu(vq, event), new, old);\r\n}\r\nvoid vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\nif (vq->call_ctx && vhost_notify(dev, vq))\r\neventfd_signal(vq->call_ctx, 1);\r\n}\r\nvoid vhost_add_used_and_signal(struct vhost_dev *dev,\r\nstruct vhost_virtqueue *vq,\r\nunsigned int head, int len)\r\n{\r\nvhost_add_used(vq, head, len);\r\nvhost_signal(dev, vq);\r\n}\r\nvoid vhost_add_used_and_signal_n(struct vhost_dev *dev,\r\nstruct vhost_virtqueue *vq,\r\nstruct vring_used_elem *heads, unsigned count)\r\n{\r\nvhost_add_used_n(vq, heads, count);\r\nvhost_signal(dev, vq);\r\n}\r\nbool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\n__virtio16 avail_idx;\r\nint r;\r\nr = vhost_get_user(vq, avail_idx, &vq->avail->idx);\r\nif (r)\r\nreturn false;\r\nreturn vhost16_to_cpu(vq, avail_idx) == vq->avail_idx;\r\n}\r\nbool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\n__virtio16 avail_idx;\r\nint r;\r\nif (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))\r\nreturn false;\r\nvq->used_flags &= ~VRING_USED_F_NO_NOTIFY;\r\nif (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {\r\nr = vhost_update_used_flags(vq);\r\nif (r) {\r\nvq_err(vq, "Failed to enable notification at %p: %d\n",\r\n&vq->used->flags, r);\r\nreturn false;\r\n}\r\n} else {\r\nr = vhost_update_avail_event(vq, vq->avail_idx);\r\nif (r) {\r\nvq_err(vq, "Failed to update avail event index at %p: %d\n",\r\nvhost_avail_event(vq), r);\r\nreturn false;\r\n}\r\n}\r\nsmp_mb();\r\nr = vhost_get_user(vq, avail_idx, &vq->avail->idx);\r\nif (r) {\r\nvq_err(vq, "Failed to check avail idx at %p: %d\n",\r\n&vq->avail->idx, r);\r\nreturn false;\r\n}\r\nreturn vhost16_to_cpu(vq, avail_idx) != vq->avail_idx;\r\n}\r\nvoid vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)\r\n{\r\nint r;\r\nif (vq->used_flags & VRING_USED_F_NO_NOTIFY)\r\nreturn;\r\nvq->used_flags |= VRING_USED_F_NO_NOTIFY;\r\nif (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {\r\nr = vhost_update_used_flags(vq);\r\nif (r)\r\nvq_err(vq, "Failed to enable notification at %p: %d\n",\r\n&vq->used->flags, r);\r\n}\r\n}\r\nstruct vhost_msg_node *vhost_new_msg(struct vhost_virtqueue *vq, int type)\r\n{\r\nstruct vhost_msg_node *node = kmalloc(sizeof *node, GFP_KERNEL);\r\nif (!node)\r\nreturn NULL;\r\nnode->vq = vq;\r\nnode->msg.type = type;\r\nreturn node;\r\n}\r\nvoid vhost_enqueue_msg(struct vhost_dev *dev, struct list_head *head,\r\nstruct vhost_msg_node *node)\r\n{\r\nspin_lock(&dev->iotlb_lock);\r\nlist_add_tail(&node->node, head);\r\nspin_unlock(&dev->iotlb_lock);\r\nwake_up_interruptible_poll(&dev->wait, POLLIN | POLLRDNORM);\r\n}\r\nstruct vhost_msg_node *vhost_dequeue_msg(struct vhost_dev *dev,\r\nstruct list_head *head)\r\n{\r\nstruct vhost_msg_node *node = NULL;\r\nspin_lock(&dev->iotlb_lock);\r\nif (!list_empty(head)) {\r\nnode = list_first_entry(head, struct vhost_msg_node,\r\nnode);\r\nlist_del(&node->node);\r\n}\r\nspin_unlock(&dev->iotlb_lock);\r\nreturn node;\r\n}\r\nstatic int __init vhost_init(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic void __exit vhost_exit(void)\r\n{\r\n}
