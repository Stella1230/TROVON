static struct skx_dev *get_skx_dev(u8 bus, u8 idx)\r\n{\r\nstruct skx_dev *d;\r\nlist_for_each_entry(d, &skx_edac_list, list) {\r\nif (d->bus[idx] == bus)\r\nreturn d;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int get_all_bus_mappings(void)\r\n{\r\nstruct pci_dev *pdev, *prev;\r\nstruct skx_dev *d;\r\nu32 reg;\r\nint ndev = 0;\r\nprev = NULL;\r\nfor (;;) {\r\npdev = pci_get_device(PCI_VENDOR_ID_INTEL, 0x2016, prev);\r\nif (!pdev)\r\nbreak;\r\nndev++;\r\nd = kzalloc(sizeof(*d), GFP_KERNEL);\r\nif (!d) {\r\npci_dev_put(pdev);\r\nreturn -ENOMEM;\r\n}\r\npci_read_config_dword(pdev, 0xCC, &reg);\r\nd->bus[0] = GET_BITFIELD(reg, 0, 7);\r\nd->bus[1] = GET_BITFIELD(reg, 8, 15);\r\nd->bus[2] = GET_BITFIELD(reg, 16, 23);\r\nd->bus[3] = GET_BITFIELD(reg, 24, 31);\r\nedac_dbg(2, "busses: %x, %x, %x, %x\n",\r\nd->bus[0], d->bus[1], d->bus[2], d->bus[3]);\r\nlist_add_tail(&d->list, &skx_edac_list);\r\nskx_num_sockets++;\r\nprev = pdev;\r\n}\r\nreturn ndev;\r\n}\r\nstatic int get_all_munits(const struct munit *m)\r\n{\r\nstruct pci_dev *pdev, *prev;\r\nstruct skx_dev *d;\r\nu32 reg;\r\nint i = 0, ndev = 0;\r\nprev = NULL;\r\nfor (;;) {\r\npdev = pci_get_device(PCI_VENDOR_ID_INTEL, m->did, prev);\r\nif (!pdev)\r\nbreak;\r\nndev++;\r\nif (m->per_socket == NUM_IMC) {\r\nfor (i = 0; i < NUM_IMC; i++)\r\nif (m->devfn[i] == pdev->devfn)\r\nbreak;\r\nif (i == NUM_IMC)\r\ngoto fail;\r\n}\r\nd = get_skx_dev(pdev->bus->number, m->busidx);\r\nif (!d)\r\ngoto fail;\r\nif (unlikely(pci_enable_device(pdev) < 0)) {\r\nskx_printk(KERN_ERR,\r\n"Couldn't enable %04x:%04x\n", PCI_VENDOR_ID_INTEL, m->did);\r\ngoto fail;\r\n}\r\nswitch (m->mtype) {\r\ncase CHAN0: case CHAN1: case CHAN2:\r\npci_dev_get(pdev);\r\nd->imc[i].chan[m->mtype].cdev = pdev;\r\nbreak;\r\ncase SAD_ALL:\r\npci_dev_get(pdev);\r\nd->sad_all = pdev;\r\nbreak;\r\ncase UTIL_ALL:\r\npci_dev_get(pdev);\r\nd->util_all = pdev;\r\nbreak;\r\ncase SAD:\r\npci_read_config_dword(pdev, 0xB4, &reg);\r\nif (reg != 0) {\r\nif (d->mcroute == 0)\r\nd->mcroute = reg;\r\nelse if (d->mcroute != reg) {\r\nskx_printk(KERN_ERR,\r\n"mcroute mismatch\n");\r\ngoto fail;\r\n}\r\n}\r\nndev--;\r\nbreak;\r\n}\r\nprev = pdev;\r\n}\r\nreturn ndev;\r\nfail:\r\npci_dev_put(pdev);\r\nreturn -ENODEV;\r\n}\r\nstatic u8 get_src_id(struct skx_dev *d)\r\n{\r\nu32 reg;\r\npci_read_config_dword(d->util_all, 0xF0, &reg);\r\nreturn GET_BITFIELD(reg, 12, 14);\r\n}\r\nstatic u8 skx_get_node_id(struct skx_dev *d)\r\n{\r\nu32 reg;\r\npci_read_config_dword(d->util_all, 0xF4, &reg);\r\nreturn GET_BITFIELD(reg, 0, 2);\r\n}\r\nstatic int get_dimm_attr(u32 reg, int lobit, int hibit, int add, int minval,\r\nint maxval, char *name)\r\n{\r\nu32 val = GET_BITFIELD(reg, lobit, hibit);\r\nif (val < minval || val > maxval) {\r\nedac_dbg(2, "bad %s = %d (raw=%x)\n", name, val, reg);\r\nreturn -EINVAL;\r\n}\r\nreturn val + add;\r\n}\r\nstatic int get_width(u32 mtr)\r\n{\r\nswitch (GET_BITFIELD(mtr, 8, 9)) {\r\ncase 0:\r\nreturn DEV_X4;\r\ncase 1:\r\nreturn DEV_X8;\r\ncase 2:\r\nreturn DEV_X16;\r\n}\r\nreturn DEV_UNKNOWN;\r\n}\r\nstatic int skx_get_hi_lo(void)\r\n{\r\nstruct pci_dev *pdev;\r\nu32 reg;\r\npdev = pci_get_device(PCI_VENDOR_ID_INTEL, 0x2034, NULL);\r\nif (!pdev) {\r\nedac_dbg(0, "Can't get tolm/tohm\n");\r\nreturn -ENODEV;\r\n}\r\npci_read_config_dword(pdev, 0xD0, &reg);\r\nskx_tolm = reg;\r\npci_read_config_dword(pdev, 0xD4, &reg);\r\nskx_tohm = reg;\r\npci_read_config_dword(pdev, 0xD8, &reg);\r\nskx_tohm |= (u64)reg << 32;\r\npci_dev_put(pdev);\r\nedac_dbg(2, "tolm=%llx tohm=%llx\n", skx_tolm, skx_tohm);\r\nreturn 0;\r\n}\r\nstatic int get_dimm_info(u32 mtr, u32 amap, struct dimm_info *dimm,\r\nstruct skx_imc *imc, int chan, int dimmno)\r\n{\r\nint banks = 16, ranks, rows, cols, npages;\r\nu64 size;\r\nif (!IS_DIMM_PRESENT(mtr))\r\nreturn 0;\r\nranks = numrank(mtr);\r\nrows = numrow(mtr);\r\ncols = numcol(mtr);\r\nsize = ((1ull << (rows + cols + ranks)) * banks) >> (20 - 3);\r\nnpages = MiB_TO_PAGES(size);\r\nedac_dbg(0, "mc#%d: channel %d, dimm %d, %lld Mb (%d pages) bank: %d, rank: %d, row: %#x, col: %#x\n",\r\nimc->mc, chan, dimmno, size, npages,\r\nbanks, ranks, rows, cols);\r\nimc->chan[chan].dimms[dimmno].close_pg = GET_BITFIELD(mtr, 0, 0);\r\nimc->chan[chan].dimms[dimmno].bank_xor_enable = GET_BITFIELD(mtr, 9, 9);\r\nimc->chan[chan].dimms[dimmno].fine_grain_bank = GET_BITFIELD(amap, 0, 0);\r\nimc->chan[chan].dimms[dimmno].rowbits = rows;\r\nimc->chan[chan].dimms[dimmno].colbits = cols;\r\ndimm->nr_pages = npages;\r\ndimm->grain = 32;\r\ndimm->dtype = get_width(mtr);\r\ndimm->mtype = MEM_DDR4;\r\ndimm->edac_mode = EDAC_SECDED;\r\nsnprintf(dimm->label, sizeof(dimm->label), "CPU_SrcID#%u_MC#%u_Chan#%u_DIMM#%u",\r\nimc->src_id, imc->lmc, chan, dimmno);\r\nreturn 1;\r\n}\r\nstatic bool skx_check_ecc(struct pci_dev *pdev)\r\n{\r\nu32 mtmtr;\r\nSKX_GET_MTMTR(pdev, mtmtr);\r\nreturn !!GET_BITFIELD(mtmtr, 2, 2);\r\n}\r\nstatic int skx_get_dimm_config(struct mem_ctl_info *mci)\r\n{\r\nstruct skx_pvt *pvt = mci->pvt_info;\r\nstruct skx_imc *imc = pvt->imc;\r\nstruct dimm_info *dimm;\r\nint i, j;\r\nu32 mtr, amap;\r\nint ndimms;\r\nfor (i = 0; i < NUM_CHANNELS; i++) {\r\nndimms = 0;\r\npci_read_config_dword(imc->chan[i].cdev, 0x8C, &amap);\r\nfor (j = 0; j < NUM_DIMMS; j++) {\r\ndimm = EDAC_DIMM_PTR(mci->layers, mci->dimms,\r\nmci->n_layers, i, j, 0);\r\npci_read_config_dword(imc->chan[i].cdev,\r\n0x80 + 4*j, &mtr);\r\nndimms += get_dimm_info(mtr, amap, dimm, imc, i, j);\r\n}\r\nif (ndimms && !skx_check_ecc(imc->chan[0].cdev)) {\r\nskx_printk(KERN_ERR, "ECC is disabled on imc %d\n", imc->mc);\r\nreturn -ENODEV;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void skx_unregister_mci(struct skx_imc *imc)\r\n{\r\nstruct mem_ctl_info *mci = imc->mci;\r\nif (!mci)\r\nreturn;\r\nedac_dbg(0, "MC%d: mci = %p\n", imc->mc, mci);\r\nedac_mc_del_mc(mci->pdev);\r\nedac_dbg(1, "%s: free mci struct\n", mci->ctl_name);\r\nkfree(mci->ctl_name);\r\nedac_mc_free(mci);\r\n}\r\nstatic int skx_register_mci(struct skx_imc *imc)\r\n{\r\nstruct mem_ctl_info *mci;\r\nstruct edac_mc_layer layers[2];\r\nstruct pci_dev *pdev = imc->chan[0].cdev;\r\nstruct skx_pvt *pvt;\r\nint rc;\r\nlayers[0].type = EDAC_MC_LAYER_CHANNEL;\r\nlayers[0].size = NUM_CHANNELS;\r\nlayers[0].is_virt_csrow = false;\r\nlayers[1].type = EDAC_MC_LAYER_SLOT;\r\nlayers[1].size = NUM_DIMMS;\r\nlayers[1].is_virt_csrow = true;\r\nmci = edac_mc_alloc(imc->mc, ARRAY_SIZE(layers), layers,\r\nsizeof(struct skx_pvt));\r\nif (unlikely(!mci))\r\nreturn -ENOMEM;\r\nedac_dbg(0, "MC#%d: mci = %p\n", imc->mc, mci);\r\nimc->mci = mci;\r\npvt = mci->pvt_info;\r\npvt->imc = imc;\r\nmci->ctl_name = kasprintf(GFP_KERNEL, "Skylake Socket#%d IMC#%d",\r\nimc->node_id, imc->lmc);\r\nmci->mtype_cap = MEM_FLAG_DDR4;\r\nmci->edac_ctl_cap = EDAC_FLAG_NONE;\r\nmci->edac_cap = EDAC_FLAG_NONE;\r\nmci->mod_name = "skx_edac.c";\r\nmci->dev_name = pci_name(imc->chan[0].cdev);\r\nmci->mod_ver = SKX_REVISION;\r\nmci->ctl_page_to_phys = NULL;\r\nrc = skx_get_dimm_config(mci);\r\nif (rc < 0)\r\ngoto fail;\r\nmci->pdev = &pdev->dev;\r\nif (unlikely(edac_mc_add_mc(mci))) {\r\nedac_dbg(0, "MC: failed edac_mc_add_mc()\n");\r\nrc = -EINVAL;\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nkfree(mci->ctl_name);\r\nedac_mc_free(mci);\r\nimc->mci = NULL;\r\nreturn rc;\r\n}\r\nstatic bool skx_sad_decode(struct decoded_addr *res)\r\n{\r\nstruct skx_dev *d = list_first_entry(&skx_edac_list, typeof(*d), list);\r\nu64 addr = res->addr;\r\nint i, idx, tgt, lchan, shift;\r\nu32 sad, ilv;\r\nu64 limit, prev_limit;\r\nint remote = 0;\r\nif (addr >= skx_tohm || (addr >= skx_tolm && addr < BIT_ULL(32))) {\r\nedac_dbg(0, "Address %llx out of range\n", addr);\r\nreturn false;\r\n}\r\nrestart:\r\nprev_limit = 0;\r\nfor (i = 0; i < SKX_MAX_SAD; i++) {\r\nSKX_GET_SAD(d, i, sad);\r\nlimit = SKX_SAD_LIMIT(sad);\r\nif (SKX_SAD_ENABLE(sad)) {\r\nif (addr >= prev_limit && addr <= limit)\r\ngoto sad_found;\r\n}\r\nprev_limit = limit + 1;\r\n}\r\nedac_dbg(0, "No SAD entry for %llx\n", addr);\r\nreturn false;\r\nsad_found:\r\nSKX_GET_ILV(d, i, ilv);\r\nswitch (SKX_SAD_INTERLEAVE(sad)) {\r\ncase 0:\r\nidx = GET_BITFIELD(addr, 6, 8);\r\nbreak;\r\ncase 1:\r\nidx = GET_BITFIELD(addr, 8, 10);\r\nbreak;\r\ncase 2:\r\nidx = GET_BITFIELD(addr, 12, 14);\r\nbreak;\r\ncase 3:\r\nidx = GET_BITFIELD(addr, 30, 32);\r\nbreak;\r\n}\r\ntgt = GET_BITFIELD(ilv, 4 * idx, 4 * idx + 3);\r\nif (SKX_ILV_REMOTE(tgt)) {\r\nif (remote) {\r\nedac_dbg(0, "Double remote!\n");\r\nreturn false;\r\n}\r\nremote = 1;\r\nlist_for_each_entry(d, &skx_edac_list, list) {\r\nif (d->imc[0].src_id == SKX_ILV_TARGET(tgt))\r\ngoto restart;\r\n}\r\nedac_dbg(0, "Can't find node %d\n", SKX_ILV_TARGET(tgt));\r\nreturn false;\r\n}\r\nif (SKX_SAD_MOD3(sad) == 0)\r\nlchan = SKX_ILV_TARGET(tgt);\r\nelse {\r\nswitch (SKX_SAD_MOD3MODE(sad)) {\r\ncase 0:\r\nshift = 6;\r\nbreak;\r\ncase 1:\r\nshift = 8;\r\nbreak;\r\ncase 2:\r\nshift = 12;\r\nbreak;\r\ndefault:\r\nedac_dbg(0, "illegal mod3mode\n");\r\nreturn false;\r\n}\r\nswitch (SKX_SAD_MOD3ASMOD2(sad)) {\r\ncase 0:\r\nlchan = (addr >> shift) % 3;\r\nbreak;\r\ncase 1:\r\nlchan = (addr >> shift) % 2;\r\nbreak;\r\ncase 2:\r\nlchan = (addr >> shift) % 2;\r\nlchan = (lchan << 1) | ~lchan;\r\nbreak;\r\ncase 3:\r\nlchan = ((addr >> shift) % 2) << 1;\r\nbreak;\r\n}\r\nlchan = (lchan << 1) | (SKX_ILV_TARGET(tgt) & 1);\r\n}\r\nres->dev = d;\r\nres->socket = d->imc[0].src_id;\r\nres->imc = GET_BITFIELD(d->mcroute, lchan * 3, lchan * 3 + 2);\r\nres->channel = GET_BITFIELD(d->mcroute, lchan * 2 + 18, lchan * 2 + 19);\r\nedac_dbg(2, "%llx: socket=%d imc=%d channel=%d\n",\r\nres->addr, res->socket, res->imc, res->channel);\r\nreturn true;\r\n}\r\nstatic u64 skx_do_interleave(u64 addr, int shift, int ways, u64 lowbits)\r\n{\r\naddr >>= shift;\r\naddr /= ways;\r\naddr <<= shift;\r\nreturn addr | (lowbits & ((1ull << shift) - 1));\r\n}\r\nstatic bool skx_tad_decode(struct decoded_addr *res)\r\n{\r\nint i;\r\nu32 base, wayness, chnilvoffset;\r\nint skt_interleave_bit, chn_interleave_bit;\r\nu64 channel_addr;\r\nfor (i = 0; i < SKX_MAX_TAD; i++) {\r\nSKX_GET_TADBASE(res->dev, res->imc, i, base);\r\nSKX_GET_TADWAYNESS(res->dev, res->imc, i, wayness);\r\nif (SKX_TAD_BASE(base) <= res->addr && res->addr <= SKX_TAD_LIMIT(wayness))\r\ngoto tad_found;\r\n}\r\nedac_dbg(0, "No TAD entry for %llx\n", res->addr);\r\nreturn false;\r\ntad_found:\r\nres->sktways = SKX_TAD_SKTWAYS(wayness);\r\nres->chanways = SKX_TAD_CHNWAYS(wayness);\r\nskt_interleave_bit = skx_granularity[SKX_TAD_SKT_GRAN(base)];\r\nchn_interleave_bit = skx_granularity[SKX_TAD_CHN_GRAN(base)];\r\nSKX_GET_TADCHNILVOFFSET(res->dev, res->imc, res->channel, i, chnilvoffset);\r\nchannel_addr = res->addr - SKX_TAD_OFFSET(chnilvoffset);\r\nif (res->chanways == 3 && skt_interleave_bit > chn_interleave_bit) {\r\nchannel_addr = skx_do_interleave(channel_addr, chn_interleave_bit,\r\nres->chanways, channel_addr);\r\nchannel_addr = skx_do_interleave(channel_addr, skt_interleave_bit,\r\nres->sktways, channel_addr);\r\n} else {\r\nchannel_addr = skx_do_interleave(channel_addr, skt_interleave_bit,\r\nres->sktways, res->addr);\r\nchannel_addr = skx_do_interleave(channel_addr, chn_interleave_bit,\r\nres->chanways, res->addr);\r\n}\r\nres->chan_addr = channel_addr;\r\nedac_dbg(2, "%llx: chan_addr=%llx sktways=%d chanways=%d\n",\r\nres->addr, res->chan_addr, res->sktways, res->chanways);\r\nreturn true;\r\n}\r\nstatic bool skx_rir_decode(struct decoded_addr *res)\r\n{\r\nint i, idx, chan_rank;\r\nint shift;\r\nu32 rirway, rirlv;\r\nu64 rank_addr, prev_limit = 0, limit;\r\nif (res->dev->imc[res->imc].chan[res->channel].dimms[0].close_pg)\r\nshift = 6;\r\nelse\r\nshift = 13;\r\nfor (i = 0; i < SKX_MAX_RIR; i++) {\r\nSKX_GET_RIRWAYNESS(res->dev, res->imc, res->channel, i, rirway);\r\nlimit = SKX_RIR_LIMIT(rirway);\r\nif (SKX_RIR_VALID(rirway)) {\r\nif (prev_limit <= res->chan_addr &&\r\nres->chan_addr <= limit)\r\ngoto rir_found;\r\n}\r\nprev_limit = limit;\r\n}\r\nedac_dbg(0, "No RIR entry for %llx\n", res->addr);\r\nreturn false;\r\nrir_found:\r\nrank_addr = res->chan_addr >> shift;\r\nrank_addr /= SKX_RIR_WAYS(rirway);\r\nrank_addr <<= shift;\r\nrank_addr |= res->chan_addr & GENMASK_ULL(shift - 1, 0);\r\nres->rank_address = rank_addr;\r\nidx = (res->chan_addr >> shift) % SKX_RIR_WAYS(rirway);\r\nSKX_GET_RIRILV(res->dev, res->imc, res->channel, idx, i, rirlv);\r\nres->rank_address = rank_addr - SKX_RIR_OFFSET(rirlv);\r\nchan_rank = SKX_RIR_CHAN_RANK(rirlv);\r\nres->channel_rank = chan_rank;\r\nres->dimm = chan_rank / 4;\r\nres->rank = chan_rank % 4;\r\nedac_dbg(2, "%llx: dimm=%d rank=%d chan_rank=%d rank_addr=%llx\n",\r\nres->addr, res->dimm, res->rank,\r\nres->channel_rank, res->rank_address);\r\nreturn true;\r\n}\r\nstatic int skx_bits(u64 addr, int nbits, u8 *bits)\r\n{\r\nint i, res = 0;\r\nfor (i = 0; i < nbits; i++)\r\nres |= ((addr >> bits[i]) & 1) << i;\r\nreturn res;\r\n}\r\nstatic int skx_bank_bits(u64 addr, int b0, int b1, int do_xor, int x0, int x1)\r\n{\r\nint ret = GET_BITFIELD(addr, b0, b0) | (GET_BITFIELD(addr, b1, b1) << 1);\r\nif (do_xor)\r\nret ^= GET_BITFIELD(addr, x0, x0) | (GET_BITFIELD(addr, x1, x1) << 1);\r\nreturn ret;\r\n}\r\nstatic bool skx_mad_decode(struct decoded_addr *r)\r\n{\r\nstruct skx_dimm *dimm = &r->dev->imc[r->imc].chan[r->channel].dimms[r->dimm];\r\nint bg0 = dimm->fine_grain_bank ? 6 : 13;\r\nif (dimm->close_pg) {\r\nr->row = skx_bits(r->rank_address, dimm->rowbits, skx_close_row);\r\nr->column = skx_bits(r->rank_address, dimm->colbits, skx_close_column);\r\nr->column |= 0x400;\r\nr->bank_address = skx_bank_bits(r->rank_address, 8, 9, dimm->bank_xor_enable, 22, 28);\r\nr->bank_group = skx_bank_bits(r->rank_address, 6, 7, dimm->bank_xor_enable, 20, 21);\r\n} else {\r\nr->row = skx_bits(r->rank_address, dimm->rowbits, skx_open_row);\r\nif (dimm->fine_grain_bank)\r\nr->column = skx_bits(r->rank_address, dimm->colbits, skx_open_fine_column);\r\nelse\r\nr->column = skx_bits(r->rank_address, dimm->colbits, skx_open_column);\r\nr->bank_address = skx_bank_bits(r->rank_address, 18, 19, dimm->bank_xor_enable, 22, 23);\r\nr->bank_group = skx_bank_bits(r->rank_address, bg0, 17, dimm->bank_xor_enable, 20, 21);\r\n}\r\nr->row &= (1u << dimm->rowbits) - 1;\r\nedac_dbg(2, "%llx: row=%x col=%x bank_addr=%d bank_group=%d\n",\r\nr->addr, r->row, r->column, r->bank_address,\r\nr->bank_group);\r\nreturn true;\r\n}\r\nstatic bool skx_decode(struct decoded_addr *res)\r\n{\r\nreturn skx_sad_decode(res) && skx_tad_decode(res) &&\r\nskx_rir_decode(res) && skx_mad_decode(res);\r\n}\r\nstatic int debugfs_u64_set(void *data, u64 val)\r\n{\r\nstruct decoded_addr res;\r\nres.addr = val;\r\nskx_decode(&res);\r\nreturn 0;\r\n}\r\nstatic struct dentry *mydebugfs_create(const char *name, umode_t mode,\r\nstruct dentry *parent, u64 *value)\r\n{\r\nreturn debugfs_create_file(name, mode, parent, value, &fops_u64_wo);\r\n}\r\nstatic void setup_skx_debug(void)\r\n{\r\nskx_test = debugfs_create_dir("skx_edac_test", NULL);\r\nmydebugfs_create("addr", S_IWUSR, skx_test, &skx_fake_addr);\r\n}\r\nstatic void teardown_skx_debug(void)\r\n{\r\ndebugfs_remove_recursive(skx_test);\r\n}\r\nstatic void setup_skx_debug(void)\r\n{\r\n}\r\nstatic void teardown_skx_debug(void)\r\n{\r\n}\r\nstatic void skx_mce_output_error(struct mem_ctl_info *mci,\r\nconst struct mce *m,\r\nstruct decoded_addr *res)\r\n{\r\nenum hw_event_mc_err_type tp_event;\r\nchar *type, *optype, msg[256];\r\nbool ripv = GET_BITFIELD(m->mcgstatus, 0, 0);\r\nbool overflow = GET_BITFIELD(m->status, 62, 62);\r\nbool uncorrected_error = GET_BITFIELD(m->status, 61, 61);\r\nbool recoverable;\r\nu32 core_err_cnt = GET_BITFIELD(m->status, 38, 52);\r\nu32 mscod = GET_BITFIELD(m->status, 16, 31);\r\nu32 errcode = GET_BITFIELD(m->status, 0, 15);\r\nu32 optypenum = GET_BITFIELD(m->status, 4, 6);\r\nrecoverable = GET_BITFIELD(m->status, 56, 56);\r\nif (uncorrected_error) {\r\nif (ripv) {\r\ntype = "FATAL";\r\ntp_event = HW_EVENT_ERR_FATAL;\r\n} else {\r\ntype = "NON_FATAL";\r\ntp_event = HW_EVENT_ERR_UNCORRECTED;\r\n}\r\n} else {\r\ntype = "CORRECTED";\r\ntp_event = HW_EVENT_ERR_CORRECTED;\r\n}\r\nif (!((errcode & 0xef80) == 0x80)) {\r\noptype = "Can't parse: it is not a mem";\r\n} else {\r\nswitch (optypenum) {\r\ncase 0:\r\noptype = "generic undef request error";\r\nbreak;\r\ncase 1:\r\noptype = "memory read error";\r\nbreak;\r\ncase 2:\r\noptype = "memory write error";\r\nbreak;\r\ncase 3:\r\noptype = "addr/cmd error";\r\nbreak;\r\ncase 4:\r\noptype = "memory scrubbing error";\r\nbreak;\r\ndefault:\r\noptype = "reserved";\r\nbreak;\r\n}\r\n}\r\nsnprintf(msg, sizeof(msg),\r\n"%s%s err_code:%04x:%04x socket:%d imc:%d rank:%d bg:%d ba:%d row:%x col:%x",\r\noverflow ? " OVERFLOW" : "",\r\n(uncorrected_error && recoverable) ? " recoverable" : "",\r\nmscod, errcode,\r\nres->socket, res->imc, res->rank,\r\nres->bank_group, res->bank_address, res->row, res->column);\r\nedac_dbg(0, "%s\n", msg);\r\nedac_mc_handle_error(tp_event, mci, core_err_cnt,\r\nm->addr >> PAGE_SHIFT, m->addr & ~PAGE_MASK, 0,\r\nres->channel, res->dimm, -1,\r\noptype, msg);\r\n}\r\nstatic int skx_mce_check_error(struct notifier_block *nb, unsigned long val,\r\nvoid *data)\r\n{\r\nstruct mce *mce = (struct mce *)data;\r\nstruct decoded_addr res;\r\nstruct mem_ctl_info *mci;\r\nchar *type;\r\nif (get_edac_report_status() == EDAC_REPORTING_DISABLED)\r\nreturn NOTIFY_DONE;\r\nif ((mce->status & 0xefff) >> 7 != 1 || !(mce->status & MCI_STATUS_ADDRV))\r\nreturn NOTIFY_DONE;\r\nres.addr = mce->addr;\r\nif (!skx_decode(&res))\r\nreturn NOTIFY_DONE;\r\nmci = res.dev->imc[res.imc].mci;\r\nif (mce->mcgstatus & MCG_STATUS_MCIP)\r\ntype = "Exception";\r\nelse\r\ntype = "Event";\r\nskx_mc_printk(mci, KERN_DEBUG, "HANDLING MCE MEMORY ERROR\n");\r\nskx_mc_printk(mci, KERN_DEBUG, "CPU %d: Machine Check %s: %Lx "\r\n"Bank %d: %016Lx\n", mce->extcpu, type,\r\nmce->mcgstatus, mce->bank, mce->status);\r\nskx_mc_printk(mci, KERN_DEBUG, "TSC %llx ", mce->tsc);\r\nskx_mc_printk(mci, KERN_DEBUG, "ADDR %llx ", mce->addr);\r\nskx_mc_printk(mci, KERN_DEBUG, "MISC %llx ", mce->misc);\r\nskx_mc_printk(mci, KERN_DEBUG, "PROCESSOR %u:%x TIME %llu SOCKET "\r\n"%u APIC %x\n", mce->cpuvendor, mce->cpuid,\r\nmce->time, mce->socketid, mce->apicid);\r\nskx_mce_output_error(mci, mce, &res);\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic void skx_remove(void)\r\n{\r\nint i, j;\r\nstruct skx_dev *d, *tmp;\r\nedac_dbg(0, "\n");\r\nlist_for_each_entry_safe(d, tmp, &skx_edac_list, list) {\r\nlist_del(&d->list);\r\nfor (i = 0; i < NUM_IMC; i++) {\r\nskx_unregister_mci(&d->imc[i]);\r\nfor (j = 0; j < NUM_CHANNELS; j++)\r\npci_dev_put(d->imc[i].chan[j].cdev);\r\n}\r\npci_dev_put(d->util_all);\r\npci_dev_put(d->sad_all);\r\nkfree(d);\r\n}\r\n}\r\nint __init skx_init(void)\r\n{\r\nconst struct x86_cpu_id *id;\r\nconst struct munit *m;\r\nint rc = 0, i;\r\nu8 mc = 0, src_id, node_id;\r\nstruct skx_dev *d;\r\nedac_dbg(2, "\n");\r\nid = x86_match_cpu(skx_cpuids);\r\nif (!id)\r\nreturn -ENODEV;\r\nrc = skx_get_hi_lo();\r\nif (rc)\r\nreturn rc;\r\nrc = get_all_bus_mappings();\r\nif (rc < 0)\r\ngoto fail;\r\nif (rc == 0) {\r\nedac_dbg(2, "No memory controllers found\n");\r\nreturn -ENODEV;\r\n}\r\nfor (m = skx_all_munits; m->did; m++) {\r\nrc = get_all_munits(m);\r\nif (rc < 0)\r\ngoto fail;\r\nif (rc != m->per_socket * skx_num_sockets) {\r\nedac_dbg(2, "Expected %d, got %d of %x\n",\r\nm->per_socket * skx_num_sockets, rc, m->did);\r\nrc = -ENODEV;\r\ngoto fail;\r\n}\r\n}\r\nlist_for_each_entry(d, &skx_edac_list, list) {\r\nsrc_id = get_src_id(d);\r\nnode_id = skx_get_node_id(d);\r\nedac_dbg(2, "src_id=%d node_id=%d\n", src_id, node_id);\r\nfor (i = 0; i < NUM_IMC; i++) {\r\nd->imc[i].mc = mc++;\r\nd->imc[i].lmc = i;\r\nd->imc[i].src_id = src_id;\r\nd->imc[i].node_id = node_id;\r\nrc = skx_register_mci(&d->imc[i]);\r\nif (rc < 0)\r\ngoto fail;\r\n}\r\n}\r\nopstate_init();\r\nsetup_skx_debug();\r\nmce_register_decode_chain(&skx_mce_dec);\r\nreturn 0;\r\nfail:\r\nskx_remove();\r\nreturn rc;\r\n}\r\nstatic void __exit skx_exit(void)\r\n{\r\nedac_dbg(2, "\n");\r\nmce_unregister_decode_chain(&skx_mce_dec);\r\nskx_remove();\r\nteardown_skx_debug();\r\n}
