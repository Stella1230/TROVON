static int num_rounds(struct crypto_aes_ctx *ctx)\r\n{\r\nreturn 6 + ctx->key_length / 4;\r\n}\r\nstatic int ce_aes_expandkey(struct crypto_aes_ctx *ctx, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstatic u8 const rcon[] = {\r\n0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80, 0x1b, 0x36,\r\n};\r\nu32 kwords = key_len / sizeof(u32);\r\nstruct aes_block *key_enc, *key_dec;\r\nint i, j;\r\nif (key_len != AES_KEYSIZE_128 &&\r\nkey_len != AES_KEYSIZE_192 &&\r\nkey_len != AES_KEYSIZE_256)\r\nreturn -EINVAL;\r\nmemcpy(ctx->key_enc, in_key, key_len);\r\nctx->key_length = key_len;\r\nkernel_neon_begin();\r\nfor (i = 0; i < sizeof(rcon); i++) {\r\nu32 *rki = ctx->key_enc + (i * kwords);\r\nu32 *rko = rki + kwords;\r\nrko[0] = ror32(ce_aes_sub(rki[kwords - 1]), 8);\r\nrko[0] = rko[0] ^ rki[0] ^ rcon[i];\r\nrko[1] = rko[0] ^ rki[1];\r\nrko[2] = rko[1] ^ rki[2];\r\nrko[3] = rko[2] ^ rki[3];\r\nif (key_len == AES_KEYSIZE_192) {\r\nif (i >= 7)\r\nbreak;\r\nrko[4] = rko[3] ^ rki[4];\r\nrko[5] = rko[4] ^ rki[5];\r\n} else if (key_len == AES_KEYSIZE_256) {\r\nif (i >= 6)\r\nbreak;\r\nrko[4] = ce_aes_sub(rko[3]) ^ rki[4];\r\nrko[5] = rko[4] ^ rki[5];\r\nrko[6] = rko[5] ^ rki[6];\r\nrko[7] = rko[6] ^ rki[7];\r\n}\r\n}\r\nkey_enc = (struct aes_block *)ctx->key_enc;\r\nkey_dec = (struct aes_block *)ctx->key_dec;\r\nj = num_rounds(ctx);\r\nkey_dec[0] = key_enc[j];\r\nfor (i = 1, j--; j > 0; i++, j--)\r\nce_aes_invert(key_dec + i, key_enc + j);\r\nkey_dec[i] = key_enc[0];\r\nkernel_neon_end();\r\nreturn 0;\r\n}\r\nstatic int ce_aes_setkey(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct crypto_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint ret;\r\nret = ce_aes_expandkey(ctx, in_key, key_len);\r\nif (!ret)\r\nreturn 0;\r\ntfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nstatic int xts_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct crypto_aes_xts_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint ret;\r\nret = xts_check_key(tfm, in_key, key_len);\r\nif (ret)\r\nreturn ret;\r\nret = ce_aes_expandkey(&ctx->key1, in_key, key_len / 2);\r\nif (!ret)\r\nret = ce_aes_expandkey(&ctx->key2, &in_key[key_len / 2],\r\nkey_len / 2);\r\nif (!ret)\r\nreturn 0;\r\ntfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nstatic int ecb_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nunsigned int blocks;\r\nint err;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_ecb_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_enc, num_rounds(ctx), blocks);\r\nerr = blkcipher_walk_done(desc, &walk,\r\nwalk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int ecb_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nunsigned int blocks;\r\nint err;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_ecb_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_dec, num_rounds(ctx), blocks);\r\nerr = blkcipher_walk_done(desc, &walk,\r\nwalk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int cbc_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nunsigned int blocks;\r\nint err;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_cbc_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_enc, num_rounds(ctx), blocks,\r\nwalk.iv);\r\nerr = blkcipher_walk_done(desc, &walk,\r\nwalk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nunsigned int blocks;\r\nint err;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_cbc_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_dec, num_rounds(ctx), blocks,\r\nwalk.iv);\r\nerr = blkcipher_walk_done(desc, &walk,\r\nwalk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int ctr_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nstruct blkcipher_walk walk;\r\nint err, blocks;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_enc, num_rounds(ctx), blocks,\r\nwalk.iv);\r\nnbytes -= blocks * AES_BLOCK_SIZE;\r\nif (nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE)\r\nbreak;\r\nerr = blkcipher_walk_done(desc, &walk,\r\nwalk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nif (walk.nbytes % AES_BLOCK_SIZE) {\r\nu8 *tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE;\r\nu8 *tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE;\r\nu8 __aligned(8) tail[AES_BLOCK_SIZE];\r\nblocks = (nbytes <= 8) ? -1 : 1;\r\nce_aes_ctr_encrypt(tail, tsrc, (u8 *)ctx->key_enc,\r\nnum_rounds(ctx), blocks, walk.iv);\r\nmemcpy(tdst, tail, nbytes);\r\nerr = blkcipher_walk_done(desc, &walk, 0);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int xts_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct crypto_aes_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nint err, first, rounds = num_rounds(&ctx->key1);\r\nstruct blkcipher_walk walk;\r\nunsigned int blocks;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\nce_aes_xts_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key1.key_enc, rounds, blocks,\r\nwalk.iv, (u8 *)ctx->key2.key_enc, first);\r\nerr = blkcipher_walk_done(desc, &walk,\r\nwalk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int xts_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct crypto_aes_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nint err, first, rounds = num_rounds(&ctx->key1);\r\nstruct blkcipher_walk walk;\r\nunsigned int blocks;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\nce_aes_xts_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key1.key_dec, rounds, blocks,\r\nwalk.iv, (u8 *)ctx->key2.key_enc, first);\r\nerr = blkcipher_walk_done(desc, &walk,\r\nwalk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int __init aes_init(void)\r\n{\r\nif (!(elf_hwcap2 & HWCAP2_AES))\r\nreturn -ENODEV;\r\nreturn crypto_register_algs(aes_algs, ARRAY_SIZE(aes_algs));\r\n}\r\nstatic void __exit aes_exit(void)\r\n{\r\ncrypto_unregister_algs(aes_algs, ARRAY_SIZE(aes_algs));\r\n}
