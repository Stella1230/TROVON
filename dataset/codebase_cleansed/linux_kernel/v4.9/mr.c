int rvt_driver_mr_init(struct rvt_dev_info *rdi)\r\n{\r\nunsigned int lkey_table_size = rdi->dparms.lkey_table_size;\r\nunsigned lk_tab_size;\r\nint i;\r\nif (!lkey_table_size)\r\nreturn -EINVAL;\r\nspin_lock_init(&rdi->lkey_table.lock);\r\nif (lkey_table_size > RVT_MAX_LKEY_TABLE_BITS) {\r\nrvt_pr_warn(rdi, "lkey bits %u too large, reduced to %u\n",\r\nlkey_table_size, RVT_MAX_LKEY_TABLE_BITS);\r\nrdi->dparms.lkey_table_size = RVT_MAX_LKEY_TABLE_BITS;\r\nlkey_table_size = rdi->dparms.lkey_table_size;\r\n}\r\nrdi->lkey_table.max = 1 << lkey_table_size;\r\nlk_tab_size = rdi->lkey_table.max * sizeof(*rdi->lkey_table.table);\r\nrdi->lkey_table.table = (struct rvt_mregion __rcu **)\r\nvmalloc_node(lk_tab_size, rdi->dparms.node);\r\nif (!rdi->lkey_table.table)\r\nreturn -ENOMEM;\r\nRCU_INIT_POINTER(rdi->dma_mr, NULL);\r\nfor (i = 0; i < rdi->lkey_table.max; i++)\r\nRCU_INIT_POINTER(rdi->lkey_table.table[i], NULL);\r\nreturn 0;\r\n}\r\nvoid rvt_mr_exit(struct rvt_dev_info *rdi)\r\n{\r\nif (rdi->dma_mr)\r\nrvt_pr_err(rdi, "DMA MR not null!\n");\r\nvfree(rdi->lkey_table.table);\r\n}\r\nstatic void rvt_deinit_mregion(struct rvt_mregion *mr)\r\n{\r\nint i = mr->mapsz;\r\nmr->mapsz = 0;\r\nwhile (i)\r\nkfree(mr->map[--i]);\r\n}\r\nstatic int rvt_init_mregion(struct rvt_mregion *mr, struct ib_pd *pd,\r\nint count)\r\n{\r\nint m, i = 0;\r\nstruct rvt_dev_info *dev = ib_to_rvt(pd->device);\r\nmr->mapsz = 0;\r\nm = (count + RVT_SEGSZ - 1) / RVT_SEGSZ;\r\nfor (; i < m; i++) {\r\nmr->map[i] = kzalloc_node(sizeof(*mr->map[0]), GFP_KERNEL,\r\ndev->dparms.node);\r\nif (!mr->map[i]) {\r\nrvt_deinit_mregion(mr);\r\nreturn -ENOMEM;\r\n}\r\nmr->mapsz++;\r\n}\r\ninit_completion(&mr->comp);\r\natomic_set(&mr->refcount, 1);\r\natomic_set(&mr->lkey_invalid, 0);\r\nmr->pd = pd;\r\nmr->max_segs = count;\r\nreturn 0;\r\n}\r\nstatic int rvt_alloc_lkey(struct rvt_mregion *mr, int dma_region)\r\n{\r\nunsigned long flags;\r\nu32 r;\r\nu32 n;\r\nint ret = 0;\r\nstruct rvt_dev_info *dev = ib_to_rvt(mr->pd->device);\r\nstruct rvt_lkey_table *rkt = &dev->lkey_table;\r\nrvt_get_mr(mr);\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nif (dma_region) {\r\nstruct rvt_mregion *tmr;\r\ntmr = rcu_access_pointer(dev->dma_mr);\r\nif (!tmr) {\r\nrcu_assign_pointer(dev->dma_mr, mr);\r\nmr->lkey_published = 1;\r\n} else {\r\nrvt_put_mr(mr);\r\n}\r\ngoto success;\r\n}\r\nr = rkt->next;\r\nn = r;\r\nfor (;;) {\r\nif (!rcu_access_pointer(rkt->table[r]))\r\nbreak;\r\nr = (r + 1) & (rkt->max - 1);\r\nif (r == n)\r\ngoto bail;\r\n}\r\nrkt->next = (r + 1) & (rkt->max - 1);\r\nrkt->gen++;\r\nmr->lkey = (r << (32 - dev->dparms.lkey_table_size)) |\r\n((((1 << (24 - dev->dparms.lkey_table_size)) - 1) & rkt->gen)\r\n<< 8);\r\nif (mr->lkey == 0) {\r\nmr->lkey |= 1 << 8;\r\nrkt->gen++;\r\n}\r\nrcu_assign_pointer(rkt->table[r], mr);\r\nmr->lkey_published = 1;\r\nsuccess:\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nout:\r\nreturn ret;\r\nbail:\r\nrvt_put_mr(mr);\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nstatic void rvt_free_lkey(struct rvt_mregion *mr)\r\n{\r\nunsigned long flags;\r\nu32 lkey = mr->lkey;\r\nu32 r;\r\nstruct rvt_dev_info *dev = ib_to_rvt(mr->pd->device);\r\nstruct rvt_lkey_table *rkt = &dev->lkey_table;\r\nint freed = 0;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nif (!mr->lkey_published)\r\ngoto out;\r\nif (lkey == 0) {\r\nRCU_INIT_POINTER(dev->dma_mr, NULL);\r\n} else {\r\nr = lkey >> (32 - dev->dparms.lkey_table_size);\r\nRCU_INIT_POINTER(rkt->table[r], NULL);\r\n}\r\nmr->lkey_published = 0;\r\nfreed++;\r\nout:\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nif (freed) {\r\nsynchronize_rcu();\r\nrvt_put_mr(mr);\r\n}\r\n}\r\nstatic struct rvt_mr *__rvt_alloc_mr(int count, struct ib_pd *pd)\r\n{\r\nstruct rvt_mr *mr;\r\nint rval = -ENOMEM;\r\nint m;\r\nm = (count + RVT_SEGSZ - 1) / RVT_SEGSZ;\r\nmr = kzalloc(sizeof(*mr) + m * sizeof(mr->mr.map[0]), GFP_KERNEL);\r\nif (!mr)\r\ngoto bail;\r\nrval = rvt_init_mregion(&mr->mr, pd, count);\r\nif (rval)\r\ngoto bail;\r\nrval = rvt_alloc_lkey(&mr->mr, 0);\r\nif (rval)\r\ngoto bail_mregion;\r\nmr->ibmr.lkey = mr->mr.lkey;\r\nmr->ibmr.rkey = mr->mr.lkey;\r\ndone:\r\nreturn mr;\r\nbail_mregion:\r\nrvt_deinit_mregion(&mr->mr);\r\nbail:\r\nkfree(mr);\r\nmr = ERR_PTR(rval);\r\ngoto done;\r\n}\r\nstatic void __rvt_free_mr(struct rvt_mr *mr)\r\n{\r\nrvt_deinit_mregion(&mr->mr);\r\nrvt_free_lkey(&mr->mr);\r\nkfree(mr);\r\n}\r\nstruct ib_mr *rvt_get_dma_mr(struct ib_pd *pd, int acc)\r\n{\r\nstruct rvt_mr *mr;\r\nstruct ib_mr *ret;\r\nint rval;\r\nif (ibpd_to_rvtpd(pd)->user)\r\nreturn ERR_PTR(-EPERM);\r\nmr = kzalloc(sizeof(*mr), GFP_KERNEL);\r\nif (!mr) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nrval = rvt_init_mregion(&mr->mr, pd, 0);\r\nif (rval) {\r\nret = ERR_PTR(rval);\r\ngoto bail;\r\n}\r\nrval = rvt_alloc_lkey(&mr->mr, 1);\r\nif (rval) {\r\nret = ERR_PTR(rval);\r\ngoto bail_mregion;\r\n}\r\nmr->mr.access_flags = acc;\r\nret = &mr->ibmr;\r\ndone:\r\nreturn ret;\r\nbail_mregion:\r\nrvt_deinit_mregion(&mr->mr);\r\nbail:\r\nkfree(mr);\r\ngoto done;\r\n}\r\nstruct ib_mr *rvt_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\r\nu64 virt_addr, int mr_access_flags,\r\nstruct ib_udata *udata)\r\n{\r\nstruct rvt_mr *mr;\r\nstruct ib_umem *umem;\r\nstruct scatterlist *sg;\r\nint n, m, entry;\r\nstruct ib_mr *ret;\r\nif (length == 0)\r\nreturn ERR_PTR(-EINVAL);\r\numem = ib_umem_get(pd->uobject->context, start, length,\r\nmr_access_flags, 0);\r\nif (IS_ERR(umem))\r\nreturn (void *)umem;\r\nn = umem->nmap;\r\nmr = __rvt_alloc_mr(n, pd);\r\nif (IS_ERR(mr)) {\r\nret = (struct ib_mr *)mr;\r\ngoto bail_umem;\r\n}\r\nmr->mr.user_base = start;\r\nmr->mr.iova = virt_addr;\r\nmr->mr.length = length;\r\nmr->mr.offset = ib_umem_offset(umem);\r\nmr->mr.access_flags = mr_access_flags;\r\nmr->umem = umem;\r\nif (is_power_of_2(umem->page_size))\r\nmr->mr.page_shift = ilog2(umem->page_size);\r\nm = 0;\r\nn = 0;\r\nfor_each_sg(umem->sg_head.sgl, sg, umem->nmap, entry) {\r\nvoid *vaddr;\r\nvaddr = page_address(sg_page(sg));\r\nif (!vaddr) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail_inval;\r\n}\r\nmr->mr.map[m]->segs[n].vaddr = vaddr;\r\nmr->mr.map[m]->segs[n].length = umem->page_size;\r\nn++;\r\nif (n == RVT_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nreturn &mr->ibmr;\r\nbail_inval:\r\n__rvt_free_mr(mr);\r\nbail_umem:\r\nib_umem_release(umem);\r\nreturn ret;\r\n}\r\nint rvt_dereg_mr(struct ib_mr *ibmr)\r\n{\r\nstruct rvt_mr *mr = to_imr(ibmr);\r\nstruct rvt_dev_info *rdi = ib_to_rvt(ibmr->pd->device);\r\nint ret = 0;\r\nunsigned long timeout;\r\nrvt_free_lkey(&mr->mr);\r\nrvt_put_mr(&mr->mr);\r\ntimeout = wait_for_completion_timeout(&mr->mr.comp, 5 * HZ);\r\nif (!timeout) {\r\nrvt_pr_err(rdi,\r\n"rvt_dereg_mr timeout mr %p pd %p refcount %u\n",\r\nmr, mr->mr.pd, atomic_read(&mr->mr.refcount));\r\nrvt_get_mr(&mr->mr);\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\nrvt_deinit_mregion(&mr->mr);\r\nif (mr->umem)\r\nib_umem_release(mr->umem);\r\nkfree(mr);\r\nout:\r\nreturn ret;\r\n}\r\nstruct ib_mr *rvt_alloc_mr(struct ib_pd *pd,\r\nenum ib_mr_type mr_type,\r\nu32 max_num_sg)\r\n{\r\nstruct rvt_mr *mr;\r\nif (mr_type != IB_MR_TYPE_MEM_REG)\r\nreturn ERR_PTR(-EINVAL);\r\nmr = __rvt_alloc_mr(max_num_sg, pd);\r\nif (IS_ERR(mr))\r\nreturn (struct ib_mr *)mr;\r\nreturn &mr->ibmr;\r\n}\r\nstatic int rvt_set_page(struct ib_mr *ibmr, u64 addr)\r\n{\r\nstruct rvt_mr *mr = to_imr(ibmr);\r\nu32 ps = 1 << mr->mr.page_shift;\r\nu32 mapped_segs = mr->mr.length >> mr->mr.page_shift;\r\nint m, n;\r\nif (unlikely(mapped_segs == mr->mr.max_segs))\r\nreturn -ENOMEM;\r\nif (mr->mr.length == 0) {\r\nmr->mr.user_base = addr;\r\nmr->mr.iova = addr;\r\n}\r\nm = mapped_segs / RVT_SEGSZ;\r\nn = mapped_segs % RVT_SEGSZ;\r\nmr->mr.map[m]->segs[n].vaddr = (void *)addr;\r\nmr->mr.map[m]->segs[n].length = ps;\r\nmr->mr.length += ps;\r\nreturn 0;\r\n}\r\nint rvt_map_mr_sg(struct ib_mr *ibmr, struct scatterlist *sg,\r\nint sg_nents, unsigned int *sg_offset)\r\n{\r\nstruct rvt_mr *mr = to_imr(ibmr);\r\nmr->mr.length = 0;\r\nmr->mr.page_shift = PAGE_SHIFT;\r\nreturn ib_sg_to_pages(ibmr, sg, sg_nents, sg_offset,\r\nrvt_set_page);\r\n}\r\nint rvt_fast_reg_mr(struct rvt_qp *qp, struct ib_mr *ibmr, u32 key,\r\nint access)\r\n{\r\nstruct rvt_mr *mr = to_imr(ibmr);\r\nif (qp->ibqp.pd != mr->mr.pd)\r\nreturn -EACCES;\r\nif (!mr->mr.lkey || mr->umem)\r\nreturn -EINVAL;\r\nif ((key & 0xFFFFFF00) != (mr->mr.lkey & 0xFFFFFF00))\r\nreturn -EINVAL;\r\nibmr->lkey = key;\r\nibmr->rkey = key;\r\nmr->mr.lkey = key;\r\nmr->mr.access_flags = access;\r\natomic_set(&mr->mr.lkey_invalid, 0);\r\nreturn 0;\r\n}\r\nint rvt_invalidate_rkey(struct rvt_qp *qp, u32 rkey)\r\n{\r\nstruct rvt_dev_info *dev = ib_to_rvt(qp->ibqp.device);\r\nstruct rvt_lkey_table *rkt = &dev->lkey_table;\r\nstruct rvt_mregion *mr;\r\nif (rkey == 0)\r\nreturn -EINVAL;\r\nrcu_read_lock();\r\nmr = rcu_dereference(\r\nrkt->table[(rkey >> (32 - dev->dparms.lkey_table_size))]);\r\nif (unlikely(!mr || mr->lkey != rkey || qp->ibqp.pd != mr->pd))\r\ngoto bail;\r\natomic_set(&mr->lkey_invalid, 1);\r\nrcu_read_unlock();\r\nreturn 0;\r\nbail:\r\nrcu_read_unlock();\r\nreturn -EINVAL;\r\n}\r\nstruct ib_fmr *rvt_alloc_fmr(struct ib_pd *pd, int mr_access_flags,\r\nstruct ib_fmr_attr *fmr_attr)\r\n{\r\nstruct rvt_fmr *fmr;\r\nint m;\r\nstruct ib_fmr *ret;\r\nint rval = -ENOMEM;\r\nm = (fmr_attr->max_pages + RVT_SEGSZ - 1) / RVT_SEGSZ;\r\nfmr = kzalloc(sizeof(*fmr) + m * sizeof(fmr->mr.map[0]), GFP_KERNEL);\r\nif (!fmr)\r\ngoto bail;\r\nrval = rvt_init_mregion(&fmr->mr, pd, fmr_attr->max_pages);\r\nif (rval)\r\ngoto bail;\r\nrval = rvt_alloc_lkey(&fmr->mr, 0);\r\nif (rval)\r\ngoto bail_mregion;\r\nfmr->ibfmr.rkey = fmr->mr.lkey;\r\nfmr->ibfmr.lkey = fmr->mr.lkey;\r\nfmr->mr.access_flags = mr_access_flags;\r\nfmr->mr.max_segs = fmr_attr->max_pages;\r\nfmr->mr.page_shift = fmr_attr->page_shift;\r\nret = &fmr->ibfmr;\r\ndone:\r\nreturn ret;\r\nbail_mregion:\r\nrvt_deinit_mregion(&fmr->mr);\r\nbail:\r\nkfree(fmr);\r\nret = ERR_PTR(rval);\r\ngoto done;\r\n}\r\nint rvt_map_phys_fmr(struct ib_fmr *ibfmr, u64 *page_list,\r\nint list_len, u64 iova)\r\n{\r\nstruct rvt_fmr *fmr = to_ifmr(ibfmr);\r\nstruct rvt_lkey_table *rkt;\r\nunsigned long flags;\r\nint m, n, i;\r\nu32 ps;\r\nstruct rvt_dev_info *rdi = ib_to_rvt(ibfmr->device);\r\ni = atomic_read(&fmr->mr.refcount);\r\nif (i > 2)\r\nreturn -EBUSY;\r\nif (list_len > fmr->mr.max_segs)\r\nreturn -EINVAL;\r\nrkt = &rdi->lkey_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nfmr->mr.user_base = iova;\r\nfmr->mr.iova = iova;\r\nps = 1 << fmr->mr.page_shift;\r\nfmr->mr.length = list_len * ps;\r\nm = 0;\r\nn = 0;\r\nfor (i = 0; i < list_len; i++) {\r\nfmr->mr.map[m]->segs[n].vaddr = (void *)page_list[i];\r\nfmr->mr.map[m]->segs[n].length = ps;\r\nif (++n == RVT_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\nreturn 0;\r\n}\r\nint rvt_unmap_fmr(struct list_head *fmr_list)\r\n{\r\nstruct rvt_fmr *fmr;\r\nstruct rvt_lkey_table *rkt;\r\nunsigned long flags;\r\nstruct rvt_dev_info *rdi;\r\nlist_for_each_entry(fmr, fmr_list, ibfmr.list) {\r\nrdi = ib_to_rvt(fmr->ibfmr.device);\r\nrkt = &rdi->lkey_table;\r\nspin_lock_irqsave(&rkt->lock, flags);\r\nfmr->mr.user_base = 0;\r\nfmr->mr.iova = 0;\r\nfmr->mr.length = 0;\r\nspin_unlock_irqrestore(&rkt->lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nint rvt_dealloc_fmr(struct ib_fmr *ibfmr)\r\n{\r\nstruct rvt_fmr *fmr = to_ifmr(ibfmr);\r\nint ret = 0;\r\nunsigned long timeout;\r\nrvt_free_lkey(&fmr->mr);\r\nrvt_put_mr(&fmr->mr);\r\ntimeout = wait_for_completion_timeout(&fmr->mr.comp, 5 * HZ);\r\nif (!timeout) {\r\nrvt_get_mr(&fmr->mr);\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\nrvt_deinit_mregion(&fmr->mr);\r\nkfree(fmr);\r\nout:\r\nreturn ret;\r\n}\r\nint rvt_lkey_ok(struct rvt_lkey_table *rkt, struct rvt_pd *pd,\r\nstruct rvt_sge *isge, struct ib_sge *sge, int acc)\r\n{\r\nstruct rvt_mregion *mr;\r\nunsigned n, m;\r\nsize_t off;\r\nstruct rvt_dev_info *dev = ib_to_rvt(pd->ibpd.device);\r\nrcu_read_lock();\r\nif (sge->lkey == 0) {\r\nif (pd->user)\r\ngoto bail;\r\nmr = rcu_dereference(dev->dma_mr);\r\nif (!mr)\r\ngoto bail;\r\natomic_inc(&mr->refcount);\r\nrcu_read_unlock();\r\nisge->mr = mr;\r\nisge->vaddr = (void *)sge->addr;\r\nisge->length = sge->length;\r\nisge->sge_length = sge->length;\r\nisge->m = 0;\r\nisge->n = 0;\r\ngoto ok;\r\n}\r\nmr = rcu_dereference(\r\nrkt->table[(sge->lkey >> (32 - dev->dparms.lkey_table_size))]);\r\nif (unlikely(!mr || atomic_read(&mr->lkey_invalid) ||\r\nmr->lkey != sge->lkey || mr->pd != &pd->ibpd))\r\ngoto bail;\r\noff = sge->addr - mr->user_base;\r\nif (unlikely(sge->addr < mr->user_base ||\r\noff + sge->length > mr->length ||\r\n(mr->access_flags & acc) != acc))\r\ngoto bail;\r\natomic_inc(&mr->refcount);\r\nrcu_read_unlock();\r\noff += mr->offset;\r\nif (mr->page_shift) {\r\nsize_t entries_spanned_by_off;\r\nentries_spanned_by_off = off >> mr->page_shift;\r\noff -= (entries_spanned_by_off << mr->page_shift);\r\nm = entries_spanned_by_off / RVT_SEGSZ;\r\nn = entries_spanned_by_off % RVT_SEGSZ;\r\n} else {\r\nm = 0;\r\nn = 0;\r\nwhile (off >= mr->map[m]->segs[n].length) {\r\noff -= mr->map[m]->segs[n].length;\r\nn++;\r\nif (n >= RVT_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\n}\r\nisge->mr = mr;\r\nisge->vaddr = mr->map[m]->segs[n].vaddr + off;\r\nisge->length = mr->map[m]->segs[n].length - off;\r\nisge->sge_length = sge->length;\r\nisge->m = m;\r\nisge->n = n;\r\nok:\r\nreturn 1;\r\nbail:\r\nrcu_read_unlock();\r\nreturn 0;\r\n}\r\nint rvt_rkey_ok(struct rvt_qp *qp, struct rvt_sge *sge,\r\nu32 len, u64 vaddr, u32 rkey, int acc)\r\n{\r\nstruct rvt_dev_info *dev = ib_to_rvt(qp->ibqp.device);\r\nstruct rvt_lkey_table *rkt = &dev->lkey_table;\r\nstruct rvt_mregion *mr;\r\nunsigned n, m;\r\nsize_t off;\r\nrcu_read_lock();\r\nif (rkey == 0) {\r\nstruct rvt_pd *pd = ibpd_to_rvtpd(qp->ibqp.pd);\r\nstruct rvt_dev_info *rdi = ib_to_rvt(pd->ibpd.device);\r\nif (pd->user)\r\ngoto bail;\r\nmr = rcu_dereference(rdi->dma_mr);\r\nif (!mr)\r\ngoto bail;\r\natomic_inc(&mr->refcount);\r\nrcu_read_unlock();\r\nsge->mr = mr;\r\nsge->vaddr = (void *)vaddr;\r\nsge->length = len;\r\nsge->sge_length = len;\r\nsge->m = 0;\r\nsge->n = 0;\r\ngoto ok;\r\n}\r\nmr = rcu_dereference(\r\nrkt->table[(rkey >> (32 - dev->dparms.lkey_table_size))]);\r\nif (unlikely(!mr || atomic_read(&mr->lkey_invalid) ||\r\nmr->lkey != rkey || qp->ibqp.pd != mr->pd))\r\ngoto bail;\r\noff = vaddr - mr->iova;\r\nif (unlikely(vaddr < mr->iova || off + len > mr->length ||\r\n(mr->access_flags & acc) == 0))\r\ngoto bail;\r\natomic_inc(&mr->refcount);\r\nrcu_read_unlock();\r\noff += mr->offset;\r\nif (mr->page_shift) {\r\nsize_t entries_spanned_by_off;\r\nentries_spanned_by_off = off >> mr->page_shift;\r\noff -= (entries_spanned_by_off << mr->page_shift);\r\nm = entries_spanned_by_off / RVT_SEGSZ;\r\nn = entries_spanned_by_off % RVT_SEGSZ;\r\n} else {\r\nm = 0;\r\nn = 0;\r\nwhile (off >= mr->map[m]->segs[n].length) {\r\noff -= mr->map[m]->segs[n].length;\r\nn++;\r\nif (n >= RVT_SEGSZ) {\r\nm++;\r\nn = 0;\r\n}\r\n}\r\n}\r\nsge->mr = mr;\r\nsge->vaddr = mr->map[m]->segs[n].vaddr + off;\r\nsge->length = mr->map[m]->segs[n].length - off;\r\nsge->sge_length = len;\r\nsge->m = m;\r\nsge->n = n;\r\nok:\r\nreturn 1;\r\nbail:\r\nrcu_read_unlock();\r\nreturn 0;\r\n}
