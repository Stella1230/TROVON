__weak void __iomem *ioremap_cache(resource_size_t offset, unsigned long size)\r\n{\r\nreturn ioremap(offset, size);\r\n}\r\nstatic void *arch_memremap_wb(resource_size_t offset, unsigned long size)\r\n{\r\nreturn (__force void *)ioremap_cache(offset, size);\r\n}\r\nstatic void *try_ram_remap(resource_size_t offset, size_t size)\r\n{\r\nunsigned long pfn = PHYS_PFN(offset);\r\nif (pfn_valid(pfn) && !PageHighMem(pfn_to_page(pfn)))\r\nreturn __va(offset);\r\nreturn NULL;\r\n}\r\nvoid *memremap(resource_size_t offset, size_t size, unsigned long flags)\r\n{\r\nint is_ram = region_intersects(offset, size,\r\nIORESOURCE_SYSTEM_RAM, IORES_DESC_NONE);\r\nvoid *addr = NULL;\r\nif (!flags)\r\nreturn NULL;\r\nif (is_ram == REGION_MIXED) {\r\nWARN_ONCE(1, "memremap attempted on mixed range %pa size: %#lx\n",\r\n&offset, (unsigned long) size);\r\nreturn NULL;\r\n}\r\nif (flags & MEMREMAP_WB) {\r\nif (is_ram == REGION_INTERSECTS)\r\naddr = try_ram_remap(offset, size);\r\nif (!addr)\r\naddr = arch_memremap_wb(offset, size);\r\n}\r\nif (!addr && is_ram == REGION_INTERSECTS && flags != MEMREMAP_WB) {\r\nWARN_ONCE(1, "memremap attempted on ram %pa size: %#lx\n",\r\n&offset, (unsigned long) size);\r\nreturn NULL;\r\n}\r\nif (!addr && (flags & MEMREMAP_WT))\r\naddr = ioremap_wt(offset, size);\r\nif (!addr && (flags & MEMREMAP_WC))\r\naddr = ioremap_wc(offset, size);\r\nreturn addr;\r\n}\r\nvoid memunmap(void *addr)\r\n{\r\nif (is_vmalloc_addr(addr))\r\niounmap((void __iomem *) addr);\r\n}\r\nstatic void devm_memremap_release(struct device *dev, void *res)\r\n{\r\nmemunmap(*(void **)res);\r\n}\r\nstatic int devm_memremap_match(struct device *dev, void *res, void *match_data)\r\n{\r\nreturn *(void **)res == match_data;\r\n}\r\nvoid *devm_memremap(struct device *dev, resource_size_t offset,\r\nsize_t size, unsigned long flags)\r\n{\r\nvoid **ptr, *addr;\r\nptr = devres_alloc_node(devm_memremap_release, sizeof(*ptr), GFP_KERNEL,\r\ndev_to_node(dev));\r\nif (!ptr)\r\nreturn ERR_PTR(-ENOMEM);\r\naddr = memremap(offset, size, flags);\r\nif (addr) {\r\n*ptr = addr;\r\ndevres_add(dev, ptr);\r\n} else {\r\ndevres_free(ptr);\r\nreturn ERR_PTR(-ENXIO);\r\n}\r\nreturn addr;\r\n}\r\nvoid devm_memunmap(struct device *dev, void *addr)\r\n{\r\nWARN_ON(devres_release(dev, devm_memremap_release,\r\ndevm_memremap_match, addr));\r\n}\r\nvoid get_zone_device_page(struct page *page)\r\n{\r\npercpu_ref_get(page->pgmap->ref);\r\n}\r\nvoid put_zone_device_page(struct page *page)\r\n{\r\nput_dev_pagemap(page->pgmap);\r\n}\r\nstatic void pgmap_radix_release(struct resource *res)\r\n{\r\nresource_size_t key, align_start, align_size, align_end;\r\nalign_start = res->start & ~(SECTION_SIZE - 1);\r\nalign_size = ALIGN(resource_size(res), SECTION_SIZE);\r\nalign_end = align_start + align_size - 1;\r\nmutex_lock(&pgmap_lock);\r\nfor (key = res->start; key <= res->end; key += SECTION_SIZE)\r\nradix_tree_delete(&pgmap_radix, key >> PA_SECTION_SHIFT);\r\nmutex_unlock(&pgmap_lock);\r\n}\r\nstatic unsigned long pfn_first(struct page_map *page_map)\r\n{\r\nstruct dev_pagemap *pgmap = &page_map->pgmap;\r\nconst struct resource *res = &page_map->res;\r\nstruct vmem_altmap *altmap = pgmap->altmap;\r\nunsigned long pfn;\r\npfn = res->start >> PAGE_SHIFT;\r\nif (altmap)\r\npfn += vmem_altmap_offset(altmap);\r\nreturn pfn;\r\n}\r\nstatic unsigned long pfn_end(struct page_map *page_map)\r\n{\r\nconst struct resource *res = &page_map->res;\r\nreturn (res->start + resource_size(res)) >> PAGE_SHIFT;\r\n}\r\nstatic void devm_memremap_pages_release(struct device *dev, void *data)\r\n{\r\nstruct page_map *page_map = data;\r\nstruct resource *res = &page_map->res;\r\nresource_size_t align_start, align_size;\r\nstruct dev_pagemap *pgmap = &page_map->pgmap;\r\nif (percpu_ref_tryget_live(pgmap->ref)) {\r\ndev_WARN(dev, "%s: page mapping is still live!\n", __func__);\r\npercpu_ref_put(pgmap->ref);\r\n}\r\nalign_start = res->start & ~(SECTION_SIZE - 1);\r\nalign_size = ALIGN(resource_size(res), SECTION_SIZE);\r\narch_remove_memory(align_start, align_size);\r\nuntrack_pfn(NULL, PHYS_PFN(align_start), align_size);\r\npgmap_radix_release(res);\r\ndev_WARN_ONCE(dev, pgmap->altmap && pgmap->altmap->alloc,\r\n"%s: failed to free all reserved pages\n", __func__);\r\n}\r\nstruct dev_pagemap *find_dev_pagemap(resource_size_t phys)\r\n{\r\nstruct page_map *page_map;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\npage_map = radix_tree_lookup(&pgmap_radix, phys >> PA_SECTION_SHIFT);\r\nreturn page_map ? &page_map->pgmap : NULL;\r\n}\r\nvoid *devm_memremap_pages(struct device *dev, struct resource *res,\r\nstruct percpu_ref *ref, struct vmem_altmap *altmap)\r\n{\r\nresource_size_t key, align_start, align_size, align_end;\r\npgprot_t pgprot = PAGE_KERNEL;\r\nstruct dev_pagemap *pgmap;\r\nstruct page_map *page_map;\r\nint error, nid, is_ram;\r\nunsigned long pfn;\r\nalign_start = res->start & ~(SECTION_SIZE - 1);\r\nalign_size = ALIGN(res->start + resource_size(res), SECTION_SIZE)\r\n- align_start;\r\nis_ram = region_intersects(align_start, align_size,\r\nIORESOURCE_SYSTEM_RAM, IORES_DESC_NONE);\r\nif (is_ram == REGION_MIXED) {\r\nWARN_ONCE(1, "%s attempted on mixed region %pr\n",\r\n__func__, res);\r\nreturn ERR_PTR(-ENXIO);\r\n}\r\nif (is_ram == REGION_INTERSECTS)\r\nreturn __va(res->start);\r\nif (!ref)\r\nreturn ERR_PTR(-EINVAL);\r\npage_map = devres_alloc_node(devm_memremap_pages_release,\r\nsizeof(*page_map), GFP_KERNEL, dev_to_node(dev));\r\nif (!page_map)\r\nreturn ERR_PTR(-ENOMEM);\r\npgmap = &page_map->pgmap;\r\nmemcpy(&page_map->res, res, sizeof(*res));\r\npgmap->dev = dev;\r\nif (altmap) {\r\nmemcpy(&page_map->altmap, altmap, sizeof(*altmap));\r\npgmap->altmap = &page_map->altmap;\r\n}\r\npgmap->ref = ref;\r\npgmap->res = &page_map->res;\r\nmutex_lock(&pgmap_lock);\r\nerror = 0;\r\nalign_end = align_start + align_size - 1;\r\nfor (key = align_start; key <= align_end; key += SECTION_SIZE) {\r\nstruct dev_pagemap *dup;\r\nrcu_read_lock();\r\ndup = find_dev_pagemap(key);\r\nrcu_read_unlock();\r\nif (dup) {\r\ndev_err(dev, "%s: %pr collides with mapping for %s\n",\r\n__func__, res, dev_name(dup->dev));\r\nerror = -EBUSY;\r\nbreak;\r\n}\r\nerror = radix_tree_insert(&pgmap_radix, key >> PA_SECTION_SHIFT,\r\npage_map);\r\nif (error) {\r\ndev_err(dev, "%s: failed: %d\n", __func__, error);\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&pgmap_lock);\r\nif (error)\r\ngoto err_radix;\r\nnid = dev_to_node(dev);\r\nif (nid < 0)\r\nnid = numa_mem_id();\r\nerror = track_pfn_remap(NULL, &pgprot, PHYS_PFN(align_start), 0,\r\nalign_size);\r\nif (error)\r\ngoto err_pfn_remap;\r\nerror = arch_add_memory(nid, align_start, align_size, true);\r\nif (error)\r\ngoto err_add_memory;\r\nfor_each_device_pfn(pfn, page_map) {\r\nstruct page *page = pfn_to_page(pfn);\r\nlist_del(&page->lru);\r\npage->pgmap = pgmap;\r\n}\r\ndevres_add(dev, page_map);\r\nreturn __va(res->start);\r\nerr_add_memory:\r\nuntrack_pfn(NULL, PHYS_PFN(align_start), align_size);\r\nerr_pfn_remap:\r\nerr_radix:\r\npgmap_radix_release(res);\r\ndevres_free(page_map);\r\nreturn ERR_PTR(error);\r\n}\r\nunsigned long vmem_altmap_offset(struct vmem_altmap *altmap)\r\n{\r\nreturn altmap->reserve + altmap->free;\r\n}\r\nvoid vmem_altmap_free(struct vmem_altmap *altmap, unsigned long nr_pfns)\r\n{\r\naltmap->alloc -= nr_pfns;\r\n}\r\nstruct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)\r\n{\r\nstruct page *page = (struct page *) memmap_start;\r\nstruct dev_pagemap *pgmap;\r\nrcu_read_lock();\r\npgmap = find_dev_pagemap(__pfn_to_phys(page_to_pfn(page)));\r\nrcu_read_unlock();\r\nreturn pgmap ? pgmap->altmap : NULL;\r\n}
