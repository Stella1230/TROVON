int __meminit vmemmap_create_mapping(unsigned long start,\r\nunsigned long page_size,\r\nunsigned long phys)\r\n{\r\nunsigned long i, flags = _PAGE_PRESENT | _PAGE_ACCESSED |\r\n_PAGE_KERNEL_RW;\r\nBUG_ON(mmu_psize_defs[mmu_vmemmap_psize].enc > 0xf);\r\nflags |= mmu_psize_defs[mmu_vmemmap_psize].enc << 8;\r\nfor (i = 0; i < page_size; i += PAGE_SIZE)\r\nBUG_ON(map_kernel_page(start + i, phys, flags));\r\nreturn 0;\r\n}\r\nvoid vmemmap_remove_mapping(unsigned long start,\r\nunsigned long page_size)\r\n{\r\n}\r\nstatic __ref void *early_alloc_pgtable(unsigned long size)\r\n{\r\nvoid *pt;\r\npt = __va(memblock_alloc_base(size, size, __pa(MAX_DMA_ADDRESS)));\r\nmemset(pt, 0, size);\r\nreturn pt;\r\n}\r\nint map_kernel_page(unsigned long ea, unsigned long pa, unsigned long flags)\r\n{\r\npgd_t *pgdp;\r\npud_t *pudp;\r\npmd_t *pmdp;\r\npte_t *ptep;\r\nBUILD_BUG_ON(TASK_SIZE_USER64 > PGTABLE_RANGE);\r\nif (slab_is_available()) {\r\npgdp = pgd_offset_k(ea);\r\npudp = pud_alloc(&init_mm, pgdp, ea);\r\nif (!pudp)\r\nreturn -ENOMEM;\r\npmdp = pmd_alloc(&init_mm, pudp, ea);\r\nif (!pmdp)\r\nreturn -ENOMEM;\r\nptep = pte_alloc_kernel(pmdp, ea);\r\nif (!ptep)\r\nreturn -ENOMEM;\r\nset_pte_at(&init_mm, ea, ptep, pfn_pte(pa >> PAGE_SHIFT,\r\n__pgprot(flags)));\r\n} else {\r\npgdp = pgd_offset_k(ea);\r\n#ifndef __PAGETABLE_PUD_FOLDED\r\nif (pgd_none(*pgdp)) {\r\npudp = early_alloc_pgtable(PUD_TABLE_SIZE);\r\nBUG_ON(pudp == NULL);\r\npgd_populate(&init_mm, pgdp, pudp);\r\n}\r\n#endif\r\npudp = pud_offset(pgdp, ea);\r\nif (pud_none(*pudp)) {\r\npmdp = early_alloc_pgtable(PMD_TABLE_SIZE);\r\nBUG_ON(pmdp == NULL);\r\npud_populate(&init_mm, pudp, pmdp);\r\n}\r\npmdp = pmd_offset(pudp, ea);\r\nif (!pmd_present(*pmdp)) {\r\nptep = early_alloc_pgtable(PAGE_SIZE);\r\nBUG_ON(ptep == NULL);\r\npmd_populate_kernel(&init_mm, pmdp, ptep);\r\n}\r\nptep = pte_offset_kernel(pmdp, ea);\r\nset_pte_at(&init_mm, ea, ptep, pfn_pte(pa >> PAGE_SHIFT,\r\n__pgprot(flags)));\r\n}\r\nsmp_wmb();\r\nreturn 0;\r\n}
