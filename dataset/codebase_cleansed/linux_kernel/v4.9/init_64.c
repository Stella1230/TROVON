static int __init nonx32_setup(char *str)\r\n{\r\nif (!strcmp(str, "on"))\r\nforce_personality32 &= ~READ_IMPLIES_EXEC;\r\nelse if (!strcmp(str, "off"))\r\nforce_personality32 |= READ_IMPLIES_EXEC;\r\nreturn 1;\r\n}\r\nvoid sync_global_pgds(unsigned long start, unsigned long end, int removed)\r\n{\r\nunsigned long address;\r\nfor (address = start; address <= end; address += PGDIR_SIZE) {\r\nconst pgd_t *pgd_ref = pgd_offset_k(address);\r\nstruct page *page;\r\nif (pgd_none(*pgd_ref) && !removed)\r\ncontinue;\r\nspin_lock(&pgd_lock);\r\nlist_for_each_entry(page, &pgd_list, lru) {\r\npgd_t *pgd;\r\nspinlock_t *pgt_lock;\r\npgd = (pgd_t *)page_address(page) + pgd_index(address);\r\npgt_lock = &pgd_page_get_mm(page)->page_table_lock;\r\nspin_lock(pgt_lock);\r\nif (!pgd_none(*pgd_ref) && !pgd_none(*pgd))\r\nBUG_ON(pgd_page_vaddr(*pgd)\r\n!= pgd_page_vaddr(*pgd_ref));\r\nif (removed) {\r\nif (pgd_none(*pgd_ref) && !pgd_none(*pgd))\r\npgd_clear(pgd);\r\n} else {\r\nif (pgd_none(*pgd))\r\nset_pgd(pgd, *pgd_ref);\r\n}\r\nspin_unlock(pgt_lock);\r\n}\r\nspin_unlock(&pgd_lock);\r\n}\r\n}\r\nstatic __ref void *spp_getpage(void)\r\n{\r\nvoid *ptr;\r\nif (after_bootmem)\r\nptr = (void *) get_zeroed_page(GFP_ATOMIC | __GFP_NOTRACK);\r\nelse\r\nptr = alloc_bootmem_pages(PAGE_SIZE);\r\nif (!ptr || ((unsigned long)ptr & ~PAGE_MASK)) {\r\npanic("set_pte_phys: cannot allocate page data %s\n",\r\nafter_bootmem ? "after bootmem" : "");\r\n}\r\npr_debug("spp_getpage %p\n", ptr);\r\nreturn ptr;\r\n}\r\nstatic pud_t *fill_pud(pgd_t *pgd, unsigned long vaddr)\r\n{\r\nif (pgd_none(*pgd)) {\r\npud_t *pud = (pud_t *)spp_getpage();\r\npgd_populate(&init_mm, pgd, pud);\r\nif (pud != pud_offset(pgd, 0))\r\nprintk(KERN_ERR "PAGETABLE BUG #00! %p <-> %p\n",\r\npud, pud_offset(pgd, 0));\r\n}\r\nreturn pud_offset(pgd, vaddr);\r\n}\r\nstatic pmd_t *fill_pmd(pud_t *pud, unsigned long vaddr)\r\n{\r\nif (pud_none(*pud)) {\r\npmd_t *pmd = (pmd_t *) spp_getpage();\r\npud_populate(&init_mm, pud, pmd);\r\nif (pmd != pmd_offset(pud, 0))\r\nprintk(KERN_ERR "PAGETABLE BUG #01! %p <-> %p\n",\r\npmd, pmd_offset(pud, 0));\r\n}\r\nreturn pmd_offset(pud, vaddr);\r\n}\r\nstatic pte_t *fill_pte(pmd_t *pmd, unsigned long vaddr)\r\n{\r\nif (pmd_none(*pmd)) {\r\npte_t *pte = (pte_t *) spp_getpage();\r\npmd_populate_kernel(&init_mm, pmd, pte);\r\nif (pte != pte_offset_kernel(pmd, 0))\r\nprintk(KERN_ERR "PAGETABLE BUG #02!\n");\r\n}\r\nreturn pte_offset_kernel(pmd, vaddr);\r\n}\r\nvoid set_pte_vaddr_pud(pud_t *pud_page, unsigned long vaddr, pte_t new_pte)\r\n{\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\npud = pud_page + pud_index(vaddr);\r\npmd = fill_pmd(pud, vaddr);\r\npte = fill_pte(pmd, vaddr);\r\nset_pte(pte, new_pte);\r\n__flush_tlb_one(vaddr);\r\n}\r\nvoid set_pte_vaddr(unsigned long vaddr, pte_t pteval)\r\n{\r\npgd_t *pgd;\r\npud_t *pud_page;\r\npr_debug("set_pte_vaddr %lx to %lx\n", vaddr, native_pte_val(pteval));\r\npgd = pgd_offset_k(vaddr);\r\nif (pgd_none(*pgd)) {\r\nprintk(KERN_ERR\r\n"PGD FIXMAP MISSING, it should be setup in head.S!\n");\r\nreturn;\r\n}\r\npud_page = (pud_t*)pgd_page_vaddr(*pgd);\r\nset_pte_vaddr_pud(pud_page, vaddr, pteval);\r\n}\r\npmd_t * __init populate_extra_pmd(unsigned long vaddr)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npgd = pgd_offset_k(vaddr);\r\npud = fill_pud(pgd, vaddr);\r\nreturn fill_pmd(pud, vaddr);\r\n}\r\npte_t * __init populate_extra_pte(unsigned long vaddr)\r\n{\r\npmd_t *pmd;\r\npmd = populate_extra_pmd(vaddr);\r\nreturn fill_pte(pmd, vaddr);\r\n}\r\nstatic void __init __init_extra_mapping(unsigned long phys, unsigned long size,\r\nenum page_cache_mode cache)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npgprot_t prot;\r\npgprot_val(prot) = pgprot_val(PAGE_KERNEL_LARGE) |\r\npgprot_val(pgprot_4k_2_large(cachemode2pgprot(cache)));\r\nBUG_ON((phys & ~PMD_MASK) || (size & ~PMD_MASK));\r\nfor (; size; phys += PMD_SIZE, size -= PMD_SIZE) {\r\npgd = pgd_offset_k((unsigned long)__va(phys));\r\nif (pgd_none(*pgd)) {\r\npud = (pud_t *) spp_getpage();\r\nset_pgd(pgd, __pgd(__pa(pud) | _KERNPG_TABLE |\r\n_PAGE_USER));\r\n}\r\npud = pud_offset(pgd, (unsigned long)__va(phys));\r\nif (pud_none(*pud)) {\r\npmd = (pmd_t *) spp_getpage();\r\nset_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE |\r\n_PAGE_USER));\r\n}\r\npmd = pmd_offset(pud, phys);\r\nBUG_ON(!pmd_none(*pmd));\r\nset_pmd(pmd, __pmd(phys | pgprot_val(prot)));\r\n}\r\n}\r\nvoid __init init_extra_mapping_wb(unsigned long phys, unsigned long size)\r\n{\r\n__init_extra_mapping(phys, size, _PAGE_CACHE_MODE_WB);\r\n}\r\nvoid __init init_extra_mapping_uc(unsigned long phys, unsigned long size)\r\n{\r\n__init_extra_mapping(phys, size, _PAGE_CACHE_MODE_UC);\r\n}\r\nvoid __init cleanup_highmap(void)\r\n{\r\nunsigned long vaddr = __START_KERNEL_map;\r\nunsigned long vaddr_end = __START_KERNEL_map + KERNEL_IMAGE_SIZE;\r\nunsigned long end = roundup((unsigned long)_brk_end, PMD_SIZE) - 1;\r\npmd_t *pmd = level2_kernel_pgt;\r\nif (max_pfn_mapped)\r\nvaddr_end = __START_KERNEL_map + (max_pfn_mapped << PAGE_SHIFT);\r\nfor (; vaddr + PMD_SIZE - 1 < vaddr_end; pmd++, vaddr += PMD_SIZE) {\r\nif (pmd_none(*pmd))\r\ncontinue;\r\nif (vaddr < (unsigned long) _text || vaddr > end)\r\nset_pmd(pmd, __pmd(0));\r\n}\r\n}\r\nstatic unsigned long __meminit\r\nphys_pte_init(pte_t *pte_page, unsigned long paddr, unsigned long paddr_end,\r\npgprot_t prot)\r\n{\r\nunsigned long pages = 0, paddr_next;\r\nunsigned long paddr_last = paddr_end;\r\npte_t *pte;\r\nint i;\r\npte = pte_page + pte_index(paddr);\r\ni = pte_index(paddr);\r\nfor (; i < PTRS_PER_PTE; i++, paddr = paddr_next, pte++) {\r\npaddr_next = (paddr & PAGE_MASK) + PAGE_SIZE;\r\nif (paddr >= paddr_end) {\r\nif (!after_bootmem &&\r\n!e820_any_mapped(paddr & PAGE_MASK, paddr_next,\r\nE820_RAM) &&\r\n!e820_any_mapped(paddr & PAGE_MASK, paddr_next,\r\nE820_RESERVED_KERN))\r\nset_pte(pte, __pte(0));\r\ncontinue;\r\n}\r\nif (!pte_none(*pte)) {\r\nif (!after_bootmem)\r\npages++;\r\ncontinue;\r\n}\r\nif (0)\r\npr_info(" pte=%p addr=%lx pte=%016lx\n", pte, paddr,\r\npfn_pte(paddr >> PAGE_SHIFT, PAGE_KERNEL).pte);\r\npages++;\r\nset_pte(pte, pfn_pte(paddr >> PAGE_SHIFT, prot));\r\npaddr_last = (paddr & PAGE_MASK) + PAGE_SIZE;\r\n}\r\nupdate_page_count(PG_LEVEL_4K, pages);\r\nreturn paddr_last;\r\n}\r\nstatic unsigned long __meminit\r\nphys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,\r\nunsigned long page_size_mask, pgprot_t prot)\r\n{\r\nunsigned long pages = 0, paddr_next;\r\nunsigned long paddr_last = paddr_end;\r\nint i = pmd_index(paddr);\r\nfor (; i < PTRS_PER_PMD; i++, paddr = paddr_next) {\r\npmd_t *pmd = pmd_page + pmd_index(paddr);\r\npte_t *pte;\r\npgprot_t new_prot = prot;\r\npaddr_next = (paddr & PMD_MASK) + PMD_SIZE;\r\nif (paddr >= paddr_end) {\r\nif (!after_bootmem &&\r\n!e820_any_mapped(paddr & PMD_MASK, paddr_next,\r\nE820_RAM) &&\r\n!e820_any_mapped(paddr & PMD_MASK, paddr_next,\r\nE820_RESERVED_KERN))\r\nset_pmd(pmd, __pmd(0));\r\ncontinue;\r\n}\r\nif (!pmd_none(*pmd)) {\r\nif (!pmd_large(*pmd)) {\r\nspin_lock(&init_mm.page_table_lock);\r\npte = (pte_t *)pmd_page_vaddr(*pmd);\r\npaddr_last = phys_pte_init(pte, paddr,\r\npaddr_end, prot);\r\nspin_unlock(&init_mm.page_table_lock);\r\ncontinue;\r\n}\r\nif (page_size_mask & (1 << PG_LEVEL_2M)) {\r\nif (!after_bootmem)\r\npages++;\r\npaddr_last = paddr_next;\r\ncontinue;\r\n}\r\nnew_prot = pte_pgprot(pte_clrhuge(*(pte_t *)pmd));\r\n}\r\nif (page_size_mask & (1<<PG_LEVEL_2M)) {\r\npages++;\r\nspin_lock(&init_mm.page_table_lock);\r\nset_pte((pte_t *)pmd,\r\npfn_pte((paddr & PMD_MASK) >> PAGE_SHIFT,\r\n__pgprot(pgprot_val(prot) | _PAGE_PSE)));\r\nspin_unlock(&init_mm.page_table_lock);\r\npaddr_last = paddr_next;\r\ncontinue;\r\n}\r\npte = alloc_low_page();\r\npaddr_last = phys_pte_init(pte, paddr, paddr_end, new_prot);\r\nspin_lock(&init_mm.page_table_lock);\r\npmd_populate_kernel(&init_mm, pmd, pte);\r\nspin_unlock(&init_mm.page_table_lock);\r\n}\r\nupdate_page_count(PG_LEVEL_2M, pages);\r\nreturn paddr_last;\r\n}\r\nstatic unsigned long __meminit\r\nphys_pud_init(pud_t *pud_page, unsigned long paddr, unsigned long paddr_end,\r\nunsigned long page_size_mask)\r\n{\r\nunsigned long pages = 0, paddr_next;\r\nunsigned long paddr_last = paddr_end;\r\nunsigned long vaddr = (unsigned long)__va(paddr);\r\nint i = pud_index(vaddr);\r\nfor (; i < PTRS_PER_PUD; i++, paddr = paddr_next) {\r\npud_t *pud;\r\npmd_t *pmd;\r\npgprot_t prot = PAGE_KERNEL;\r\nvaddr = (unsigned long)__va(paddr);\r\npud = pud_page + pud_index(vaddr);\r\npaddr_next = (paddr & PUD_MASK) + PUD_SIZE;\r\nif (paddr >= paddr_end) {\r\nif (!after_bootmem &&\r\n!e820_any_mapped(paddr & PUD_MASK, paddr_next,\r\nE820_RAM) &&\r\n!e820_any_mapped(paddr & PUD_MASK, paddr_next,\r\nE820_RESERVED_KERN))\r\nset_pud(pud, __pud(0));\r\ncontinue;\r\n}\r\nif (!pud_none(*pud)) {\r\nif (!pud_large(*pud)) {\r\npmd = pmd_offset(pud, 0);\r\npaddr_last = phys_pmd_init(pmd, paddr,\r\npaddr_end,\r\npage_size_mask,\r\nprot);\r\n__flush_tlb_all();\r\ncontinue;\r\n}\r\nif (page_size_mask & (1 << PG_LEVEL_1G)) {\r\nif (!after_bootmem)\r\npages++;\r\npaddr_last = paddr_next;\r\ncontinue;\r\n}\r\nprot = pte_pgprot(pte_clrhuge(*(pte_t *)pud));\r\n}\r\nif (page_size_mask & (1<<PG_LEVEL_1G)) {\r\npages++;\r\nspin_lock(&init_mm.page_table_lock);\r\nset_pte((pte_t *)pud,\r\npfn_pte((paddr & PUD_MASK) >> PAGE_SHIFT,\r\nPAGE_KERNEL_LARGE));\r\nspin_unlock(&init_mm.page_table_lock);\r\npaddr_last = paddr_next;\r\ncontinue;\r\n}\r\npmd = alloc_low_page();\r\npaddr_last = phys_pmd_init(pmd, paddr, paddr_end,\r\npage_size_mask, prot);\r\nspin_lock(&init_mm.page_table_lock);\r\npud_populate(&init_mm, pud, pmd);\r\nspin_unlock(&init_mm.page_table_lock);\r\n}\r\n__flush_tlb_all();\r\nupdate_page_count(PG_LEVEL_1G, pages);\r\nreturn paddr_last;\r\n}\r\nunsigned long __meminit\r\nkernel_physical_mapping_init(unsigned long paddr_start,\r\nunsigned long paddr_end,\r\nunsigned long page_size_mask)\r\n{\r\nbool pgd_changed = false;\r\nunsigned long vaddr, vaddr_start, vaddr_end, vaddr_next, paddr_last;\r\npaddr_last = paddr_end;\r\nvaddr = (unsigned long)__va(paddr_start);\r\nvaddr_end = (unsigned long)__va(paddr_end);\r\nvaddr_start = vaddr;\r\nfor (; vaddr < vaddr_end; vaddr = vaddr_next) {\r\npgd_t *pgd = pgd_offset_k(vaddr);\r\npud_t *pud;\r\nvaddr_next = (vaddr & PGDIR_MASK) + PGDIR_SIZE;\r\nif (pgd_val(*pgd)) {\r\npud = (pud_t *)pgd_page_vaddr(*pgd);\r\npaddr_last = phys_pud_init(pud, __pa(vaddr),\r\n__pa(vaddr_end),\r\npage_size_mask);\r\ncontinue;\r\n}\r\npud = alloc_low_page();\r\npaddr_last = phys_pud_init(pud, __pa(vaddr), __pa(vaddr_end),\r\npage_size_mask);\r\nspin_lock(&init_mm.page_table_lock);\r\npgd_populate(&init_mm, pgd, pud);\r\nspin_unlock(&init_mm.page_table_lock);\r\npgd_changed = true;\r\n}\r\nif (pgd_changed)\r\nsync_global_pgds(vaddr_start, vaddr_end - 1, 0);\r\n__flush_tlb_all();\r\nreturn paddr_last;\r\n}\r\nvoid __init initmem_init(void)\r\n{\r\nmemblock_set_node(0, (phys_addr_t)ULLONG_MAX, &memblock.memory, 0);\r\n}\r\nvoid __init paging_init(void)\r\n{\r\nsparse_memory_present_with_active_regions(MAX_NUMNODES);\r\nsparse_init();\r\nnode_clear_state(0, N_MEMORY);\r\nif (N_MEMORY != N_NORMAL_MEMORY)\r\nnode_clear_state(0, N_NORMAL_MEMORY);\r\nzone_sizes_init();\r\n}\r\nstatic void update_end_of_memory_vars(u64 start, u64 size)\r\n{\r\nunsigned long end_pfn = PFN_UP(start + size);\r\nif (end_pfn > max_pfn) {\r\nmax_pfn = end_pfn;\r\nmax_low_pfn = end_pfn;\r\nhigh_memory = (void *)__va(max_pfn * PAGE_SIZE - 1) + 1;\r\n}\r\n}\r\nint arch_add_memory(int nid, u64 start, u64 size, bool for_device)\r\n{\r\nstruct pglist_data *pgdat = NODE_DATA(nid);\r\nstruct zone *zone = pgdat->node_zones +\r\nzone_for_memory(nid, start, size, ZONE_NORMAL, for_device);\r\nunsigned long start_pfn = start >> PAGE_SHIFT;\r\nunsigned long nr_pages = size >> PAGE_SHIFT;\r\nint ret;\r\ninit_memory_mapping(start, start + size);\r\nret = __add_pages(nid, zone, start_pfn, nr_pages);\r\nWARN_ON_ONCE(ret);\r\nupdate_end_of_memory_vars(start, size);\r\nreturn ret;\r\n}\r\nstatic void __meminit free_pagetable(struct page *page, int order)\r\n{\r\nunsigned long magic;\r\nunsigned int nr_pages = 1 << order;\r\nstruct vmem_altmap *altmap = to_vmem_altmap((unsigned long) page);\r\nif (altmap) {\r\nvmem_altmap_free(altmap, nr_pages);\r\nreturn;\r\n}\r\nif (PageReserved(page)) {\r\n__ClearPageReserved(page);\r\nmagic = (unsigned long)page->lru.next;\r\nif (magic == SECTION_INFO || magic == MIX_SECTION_INFO) {\r\nwhile (nr_pages--)\r\nput_page_bootmem(page++);\r\n} else\r\nwhile (nr_pages--)\r\nfree_reserved_page(page++);\r\n} else\r\nfree_pages((unsigned long)page_address(page), order);\r\n}\r\nstatic void __meminit free_pte_table(pte_t *pte_start, pmd_t *pmd)\r\n{\r\npte_t *pte;\r\nint i;\r\nfor (i = 0; i < PTRS_PER_PTE; i++) {\r\npte = pte_start + i;\r\nif (!pte_none(*pte))\r\nreturn;\r\n}\r\nfree_pagetable(pmd_page(*pmd), 0);\r\nspin_lock(&init_mm.page_table_lock);\r\npmd_clear(pmd);\r\nspin_unlock(&init_mm.page_table_lock);\r\n}\r\nstatic void __meminit free_pmd_table(pmd_t *pmd_start, pud_t *pud)\r\n{\r\npmd_t *pmd;\r\nint i;\r\nfor (i = 0; i < PTRS_PER_PMD; i++) {\r\npmd = pmd_start + i;\r\nif (!pmd_none(*pmd))\r\nreturn;\r\n}\r\nfree_pagetable(pud_page(*pud), 0);\r\nspin_lock(&init_mm.page_table_lock);\r\npud_clear(pud);\r\nspin_unlock(&init_mm.page_table_lock);\r\n}\r\nstatic void __meminit\r\nremove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,\r\nbool direct)\r\n{\r\nunsigned long next, pages = 0;\r\npte_t *pte;\r\nvoid *page_addr;\r\nphys_addr_t phys_addr;\r\npte = pte_start + pte_index(addr);\r\nfor (; addr < end; addr = next, pte++) {\r\nnext = (addr + PAGE_SIZE) & PAGE_MASK;\r\nif (next > end)\r\nnext = end;\r\nif (!pte_present(*pte))\r\ncontinue;\r\nphys_addr = pte_val(*pte) + (addr & PAGE_MASK);\r\nif (phys_addr < (phys_addr_t)0x40000000)\r\nreturn;\r\nif (PAGE_ALIGNED(addr) && PAGE_ALIGNED(next)) {\r\nif (!direct)\r\nfree_pagetable(pte_page(*pte), 0);\r\nspin_lock(&init_mm.page_table_lock);\r\npte_clear(&init_mm, addr, pte);\r\nspin_unlock(&init_mm.page_table_lock);\r\npages++;\r\n} else {\r\nmemset((void *)addr, PAGE_INUSE, next - addr);\r\npage_addr = page_address(pte_page(*pte));\r\nif (!memchr_inv(page_addr, PAGE_INUSE, PAGE_SIZE)) {\r\nfree_pagetable(pte_page(*pte), 0);\r\nspin_lock(&init_mm.page_table_lock);\r\npte_clear(&init_mm, addr, pte);\r\nspin_unlock(&init_mm.page_table_lock);\r\n}\r\n}\r\n}\r\nflush_tlb_all();\r\nif (direct)\r\nupdate_page_count(PG_LEVEL_4K, -pages);\r\n}\r\nstatic void __meminit\r\nremove_pmd_table(pmd_t *pmd_start, unsigned long addr, unsigned long end,\r\nbool direct)\r\n{\r\nunsigned long next, pages = 0;\r\npte_t *pte_base;\r\npmd_t *pmd;\r\nvoid *page_addr;\r\npmd = pmd_start + pmd_index(addr);\r\nfor (; addr < end; addr = next, pmd++) {\r\nnext = pmd_addr_end(addr, end);\r\nif (!pmd_present(*pmd))\r\ncontinue;\r\nif (pmd_large(*pmd)) {\r\nif (IS_ALIGNED(addr, PMD_SIZE) &&\r\nIS_ALIGNED(next, PMD_SIZE)) {\r\nif (!direct)\r\nfree_pagetable(pmd_page(*pmd),\r\nget_order(PMD_SIZE));\r\nspin_lock(&init_mm.page_table_lock);\r\npmd_clear(pmd);\r\nspin_unlock(&init_mm.page_table_lock);\r\npages++;\r\n} else {\r\nmemset((void *)addr, PAGE_INUSE, next - addr);\r\npage_addr = page_address(pmd_page(*pmd));\r\nif (!memchr_inv(page_addr, PAGE_INUSE,\r\nPMD_SIZE)) {\r\nfree_pagetable(pmd_page(*pmd),\r\nget_order(PMD_SIZE));\r\nspin_lock(&init_mm.page_table_lock);\r\npmd_clear(pmd);\r\nspin_unlock(&init_mm.page_table_lock);\r\n}\r\n}\r\ncontinue;\r\n}\r\npte_base = (pte_t *)pmd_page_vaddr(*pmd);\r\nremove_pte_table(pte_base, addr, next, direct);\r\nfree_pte_table(pte_base, pmd);\r\n}\r\nif (direct)\r\nupdate_page_count(PG_LEVEL_2M, -pages);\r\n}\r\nstatic void __meminit\r\nremove_pud_table(pud_t *pud_start, unsigned long addr, unsigned long end,\r\nbool direct)\r\n{\r\nunsigned long next, pages = 0;\r\npmd_t *pmd_base;\r\npud_t *pud;\r\nvoid *page_addr;\r\npud = pud_start + pud_index(addr);\r\nfor (; addr < end; addr = next, pud++) {\r\nnext = pud_addr_end(addr, end);\r\nif (!pud_present(*pud))\r\ncontinue;\r\nif (pud_large(*pud)) {\r\nif (IS_ALIGNED(addr, PUD_SIZE) &&\r\nIS_ALIGNED(next, PUD_SIZE)) {\r\nif (!direct)\r\nfree_pagetable(pud_page(*pud),\r\nget_order(PUD_SIZE));\r\nspin_lock(&init_mm.page_table_lock);\r\npud_clear(pud);\r\nspin_unlock(&init_mm.page_table_lock);\r\npages++;\r\n} else {\r\nmemset((void *)addr, PAGE_INUSE, next - addr);\r\npage_addr = page_address(pud_page(*pud));\r\nif (!memchr_inv(page_addr, PAGE_INUSE,\r\nPUD_SIZE)) {\r\nfree_pagetable(pud_page(*pud),\r\nget_order(PUD_SIZE));\r\nspin_lock(&init_mm.page_table_lock);\r\npud_clear(pud);\r\nspin_unlock(&init_mm.page_table_lock);\r\n}\r\n}\r\ncontinue;\r\n}\r\npmd_base = (pmd_t *)pud_page_vaddr(*pud);\r\nremove_pmd_table(pmd_base, addr, next, direct);\r\nfree_pmd_table(pmd_base, pud);\r\n}\r\nif (direct)\r\nupdate_page_count(PG_LEVEL_1G, -pages);\r\n}\r\nstatic void __meminit\r\nremove_pagetable(unsigned long start, unsigned long end, bool direct)\r\n{\r\nunsigned long next;\r\nunsigned long addr;\r\npgd_t *pgd;\r\npud_t *pud;\r\nfor (addr = start; addr < end; addr = next) {\r\nnext = pgd_addr_end(addr, end);\r\npgd = pgd_offset_k(addr);\r\nif (!pgd_present(*pgd))\r\ncontinue;\r\npud = (pud_t *)pgd_page_vaddr(*pgd);\r\nremove_pud_table(pud, addr, next, direct);\r\n}\r\nflush_tlb_all();\r\n}\r\nvoid __ref vmemmap_free(unsigned long start, unsigned long end)\r\n{\r\nremove_pagetable(start, end, false);\r\n}\r\nstatic void __meminit\r\nkernel_physical_mapping_remove(unsigned long start, unsigned long end)\r\n{\r\nstart = (unsigned long)__va(start);\r\nend = (unsigned long)__va(end);\r\nremove_pagetable(start, end, true);\r\n}\r\nint __ref arch_remove_memory(u64 start, u64 size)\r\n{\r\nunsigned long start_pfn = start >> PAGE_SHIFT;\r\nunsigned long nr_pages = size >> PAGE_SHIFT;\r\nstruct page *page = pfn_to_page(start_pfn);\r\nstruct vmem_altmap *altmap;\r\nstruct zone *zone;\r\nint ret;\r\naltmap = to_vmem_altmap((unsigned long) page);\r\nif (altmap)\r\npage += vmem_altmap_offset(altmap);\r\nzone = page_zone(page);\r\nret = __remove_pages(zone, start_pfn, nr_pages);\r\nWARN_ON_ONCE(ret);\r\nkernel_physical_mapping_remove(start, start + size);\r\nreturn ret;\r\n}\r\nstatic void __init register_page_bootmem_info(void)\r\n{\r\n#ifdef CONFIG_NUMA\r\nint i;\r\nfor_each_online_node(i)\r\nregister_page_bootmem_info_node(NODE_DATA(i));\r\n#endif\r\n}\r\nvoid __init mem_init(void)\r\n{\r\npci_iommu_alloc();\r\nregister_page_bootmem_info();\r\nfree_all_bootmem();\r\nafter_bootmem = 1;\r\nkclist_add(&kcore_vsyscall, (void *)VSYSCALL_ADDR,\r\nPAGE_SIZE, KCORE_OTHER);\r\nmem_init_print_info(NULL);\r\n}\r\nvoid set_kernel_text_rw(void)\r\n{\r\nunsigned long start = PFN_ALIGN(_text);\r\nunsigned long end = PFN_ALIGN(__stop___ex_table);\r\nif (!kernel_set_to_readonly)\r\nreturn;\r\npr_debug("Set kernel text: %lx - %lx for read write\n",\r\nstart, end);\r\nset_memory_rw(start, (end - start) >> PAGE_SHIFT);\r\n}\r\nvoid set_kernel_text_ro(void)\r\n{\r\nunsigned long start = PFN_ALIGN(_text);\r\nunsigned long end = PFN_ALIGN(__stop___ex_table);\r\nif (!kernel_set_to_readonly)\r\nreturn;\r\npr_debug("Set kernel text: %lx - %lx for read only\n",\r\nstart, end);\r\nset_memory_ro(start, (end - start) >> PAGE_SHIFT);\r\n}\r\nvoid mark_rodata_ro(void)\r\n{\r\nunsigned long start = PFN_ALIGN(_text);\r\nunsigned long rodata_start = PFN_ALIGN(__start_rodata);\r\nunsigned long end = (unsigned long) &__end_rodata_hpage_align;\r\nunsigned long text_end = PFN_ALIGN(&__stop___ex_table);\r\nunsigned long rodata_end = PFN_ALIGN(&__end_rodata);\r\nunsigned long all_end;\r\nprintk(KERN_INFO "Write protecting the kernel read-only data: %luk\n",\r\n(end - start) >> 10);\r\nset_memory_ro(start, (end - start) >> PAGE_SHIFT);\r\nkernel_set_to_readonly = 1;\r\nall_end = roundup((unsigned long)_brk_end, PMD_SIZE);\r\nset_memory_nx(text_end, (all_end - text_end) >> PAGE_SHIFT);\r\nrodata_test();\r\n#ifdef CONFIG_CPA_DEBUG\r\nprintk(KERN_INFO "Testing CPA: undo %lx-%lx\n", start, end);\r\nset_memory_rw(start, (end-start) >> PAGE_SHIFT);\r\nprintk(KERN_INFO "Testing CPA: again\n");\r\nset_memory_ro(start, (end-start) >> PAGE_SHIFT);\r\n#endif\r\nfree_init_pages("unused kernel",\r\n(unsigned long) __va(__pa_symbol(text_end)),\r\n(unsigned long) __va(__pa_symbol(rodata_start)));\r\nfree_init_pages("unused kernel",\r\n(unsigned long) __va(__pa_symbol(rodata_end)),\r\n(unsigned long) __va(__pa_symbol(_sdata)));\r\ndebug_checkwx();\r\n}\r\nint kern_addr_valid(unsigned long addr)\r\n{\r\nunsigned long above = ((long)addr) >> __VIRTUAL_MASK_SHIFT;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nif (above != 0 && above != -1UL)\r\nreturn 0;\r\npgd = pgd_offset_k(addr);\r\nif (pgd_none(*pgd))\r\nreturn 0;\r\npud = pud_offset(pgd, addr);\r\nif (pud_none(*pud))\r\nreturn 0;\r\nif (pud_large(*pud))\r\nreturn pfn_valid(pud_pfn(*pud));\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd))\r\nreturn 0;\r\nif (pmd_large(*pmd))\r\nreturn pfn_valid(pmd_pfn(*pmd));\r\npte = pte_offset_kernel(pmd, addr);\r\nif (pte_none(*pte))\r\nreturn 0;\r\nreturn pfn_valid(pte_pfn(*pte));\r\n}\r\nstatic unsigned long probe_memory_block_size(void)\r\n{\r\nunsigned long bz = MIN_MEMORY_BLOCK_SIZE;\r\nif (is_uv_system() || ((max_pfn << PAGE_SHIFT) >= (64UL << 30)))\r\nbz = 2UL << 30;\r\npr_info("x86/mm: Memory block size: %ldMB\n", bz >> 20);\r\nreturn bz;\r\n}\r\nunsigned long memory_block_size_bytes(void)\r\n{\r\nif (!memory_block_size_probed)\r\nmemory_block_size_probed = probe_memory_block_size();\r\nreturn memory_block_size_probed;\r\n}\r\nstatic int __meminit vmemmap_populate_hugepages(unsigned long start,\r\nunsigned long end, int node, struct vmem_altmap *altmap)\r\n{\r\nunsigned long addr;\r\nunsigned long next;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\nfor (addr = start; addr < end; addr = next) {\r\nnext = pmd_addr_end(addr, end);\r\npgd = vmemmap_pgd_populate(addr, node);\r\nif (!pgd)\r\nreturn -ENOMEM;\r\npud = vmemmap_pud_populate(pgd, addr, node);\r\nif (!pud)\r\nreturn -ENOMEM;\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd)) {\r\nvoid *p;\r\np = __vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);\r\nif (p) {\r\npte_t entry;\r\nentry = pfn_pte(__pa(p) >> PAGE_SHIFT,\r\nPAGE_KERNEL_LARGE);\r\nset_pmd(pmd, __pmd(pte_val(entry)));\r\nif (p_end != p || node_start != node) {\r\nif (p_start)\r\npr_debug(" [%lx-%lx] PMD -> [%p-%p] on node %d\n",\r\naddr_start, addr_end-1, p_start, p_end-1, node_start);\r\naddr_start = addr;\r\nnode_start = node;\r\np_start = p;\r\n}\r\naddr_end = addr + PMD_SIZE;\r\np_end = p + PMD_SIZE;\r\ncontinue;\r\n} else if (altmap)\r\nreturn -ENOMEM;\r\n} else if (pmd_large(*pmd)) {\r\nvmemmap_verify((pte_t *)pmd, node, addr, next);\r\ncontinue;\r\n}\r\npr_warn_once("vmemmap: falling back to regular page backing\n");\r\nif (vmemmap_populate_basepages(addr, next, node))\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nint __meminit vmemmap_populate(unsigned long start, unsigned long end, int node)\r\n{\r\nstruct vmem_altmap *altmap = to_vmem_altmap(start);\r\nint err;\r\nif (boot_cpu_has(X86_FEATURE_PSE))\r\nerr = vmemmap_populate_hugepages(start, end, node, altmap);\r\nelse if (altmap) {\r\npr_err_once("%s: no cpu support for altmap allocations\n",\r\n__func__);\r\nerr = -ENOMEM;\r\n} else\r\nerr = vmemmap_populate_basepages(start, end, node);\r\nif (!err)\r\nsync_global_pgds(start, end - 1, 0);\r\nreturn err;\r\n}\r\nvoid register_page_bootmem_memmap(unsigned long section_nr,\r\nstruct page *start_page, unsigned long size)\r\n{\r\nunsigned long addr = (unsigned long)start_page;\r\nunsigned long end = (unsigned long)(start_page + size);\r\nunsigned long next;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\nunsigned int nr_pages;\r\nstruct page *page;\r\nfor (; addr < end; addr = next) {\r\npte_t *pte = NULL;\r\npgd = pgd_offset_k(addr);\r\nif (pgd_none(*pgd)) {\r\nnext = (addr + PAGE_SIZE) & PAGE_MASK;\r\ncontinue;\r\n}\r\nget_page_bootmem(section_nr, pgd_page(*pgd), MIX_SECTION_INFO);\r\npud = pud_offset(pgd, addr);\r\nif (pud_none(*pud)) {\r\nnext = (addr + PAGE_SIZE) & PAGE_MASK;\r\ncontinue;\r\n}\r\nget_page_bootmem(section_nr, pud_page(*pud), MIX_SECTION_INFO);\r\nif (!boot_cpu_has(X86_FEATURE_PSE)) {\r\nnext = (addr + PAGE_SIZE) & PAGE_MASK;\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd))\r\ncontinue;\r\nget_page_bootmem(section_nr, pmd_page(*pmd),\r\nMIX_SECTION_INFO);\r\npte = pte_offset_kernel(pmd, addr);\r\nif (pte_none(*pte))\r\ncontinue;\r\nget_page_bootmem(section_nr, pte_page(*pte),\r\nSECTION_INFO);\r\n} else {\r\nnext = pmd_addr_end(addr, end);\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd))\r\ncontinue;\r\nnr_pages = 1 << (get_order(PMD_SIZE));\r\npage = pmd_page(*pmd);\r\nwhile (nr_pages--)\r\nget_page_bootmem(section_nr, page++,\r\nSECTION_INFO);\r\n}\r\n}\r\n}\r\nvoid __meminit vmemmap_populate_print_last(void)\r\n{\r\nif (p_start) {\r\npr_debug(" [%lx-%lx] PMD -> [%p-%p] on node %d\n",\r\naddr_start, addr_end-1, p_start, p_end-1, node_start);\r\np_start = NULL;\r\np_end = NULL;\r\nnode_start = 0;\r\n}\r\n}
