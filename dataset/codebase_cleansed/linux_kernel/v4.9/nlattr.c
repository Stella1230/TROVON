static int validate_nla(const struct nlattr *nla, int maxtype,\r\nconst struct nla_policy *policy)\r\n{\r\nconst struct nla_policy *pt;\r\nint minlen = 0, attrlen = nla_len(nla), type = nla_type(nla);\r\nif (type <= 0 || type > maxtype)\r\nreturn 0;\r\npt = &policy[type];\r\nBUG_ON(pt->type > NLA_TYPE_MAX);\r\nswitch (pt->type) {\r\ncase NLA_FLAG:\r\nif (attrlen > 0)\r\nreturn -ERANGE;\r\nbreak;\r\ncase NLA_NUL_STRING:\r\nif (pt->len)\r\nminlen = min_t(int, attrlen, pt->len + 1);\r\nelse\r\nminlen = attrlen;\r\nif (!minlen || memchr(nla_data(nla), '\0', minlen) == NULL)\r\nreturn -EINVAL;\r\ncase NLA_STRING:\r\nif (attrlen < 1)\r\nreturn -ERANGE;\r\nif (pt->len) {\r\nchar *buf = nla_data(nla);\r\nif (buf[attrlen - 1] == '\0')\r\nattrlen--;\r\nif (attrlen > pt->len)\r\nreturn -ERANGE;\r\n}\r\nbreak;\r\ncase NLA_BINARY:\r\nif (pt->len && attrlen > pt->len)\r\nreturn -ERANGE;\r\nbreak;\r\ncase NLA_NESTED_COMPAT:\r\nif (attrlen < pt->len)\r\nreturn -ERANGE;\r\nif (attrlen < NLA_ALIGN(pt->len))\r\nbreak;\r\nif (attrlen < NLA_ALIGN(pt->len) + NLA_HDRLEN)\r\nreturn -ERANGE;\r\nnla = nla_data(nla) + NLA_ALIGN(pt->len);\r\nif (attrlen < NLA_ALIGN(pt->len) + NLA_HDRLEN + nla_len(nla))\r\nreturn -ERANGE;\r\nbreak;\r\ncase NLA_NESTED:\r\nif (attrlen == 0)\r\nbreak;\r\ndefault:\r\nif (pt->len)\r\nminlen = pt->len;\r\nelse if (pt->type != NLA_UNSPEC)\r\nminlen = nla_attr_minlen[pt->type];\r\nif (attrlen < minlen)\r\nreturn -ERANGE;\r\n}\r\nreturn 0;\r\n}\r\nint nla_validate(const struct nlattr *head, int len, int maxtype,\r\nconst struct nla_policy *policy)\r\n{\r\nconst struct nlattr *nla;\r\nint rem, err;\r\nnla_for_each_attr(nla, head, len, rem) {\r\nerr = validate_nla(nla, maxtype, policy);\r\nif (err < 0)\r\ngoto errout;\r\n}\r\nerr = 0;\r\nerrout:\r\nreturn err;\r\n}\r\nint\r\nnla_policy_len(const struct nla_policy *p, int n)\r\n{\r\nint i, len = 0;\r\nfor (i = 0; i < n; i++, p++) {\r\nif (p->len)\r\nlen += nla_total_size(p->len);\r\nelse if (nla_attr_minlen[p->type])\r\nlen += nla_total_size(nla_attr_minlen[p->type]);\r\n}\r\nreturn len;\r\n}\r\nint nla_parse(struct nlattr **tb, int maxtype, const struct nlattr *head,\r\nint len, const struct nla_policy *policy)\r\n{\r\nconst struct nlattr *nla;\r\nint rem, err;\r\nmemset(tb, 0, sizeof(struct nlattr *) * (maxtype + 1));\r\nnla_for_each_attr(nla, head, len, rem) {\r\nu16 type = nla_type(nla);\r\nif (type > 0 && type <= maxtype) {\r\nif (policy) {\r\nerr = validate_nla(nla, maxtype, policy);\r\nif (err < 0)\r\ngoto errout;\r\n}\r\ntb[type] = (struct nlattr *)nla;\r\n}\r\n}\r\nif (unlikely(rem > 0))\r\npr_warn_ratelimited("netlink: %d bytes leftover after parsing attributes in process `%s'.\n",\r\nrem, current->comm);\r\nerr = 0;\r\nerrout:\r\nreturn err;\r\n}\r\nstruct nlattr *nla_find(const struct nlattr *head, int len, int attrtype)\r\n{\r\nconst struct nlattr *nla;\r\nint rem;\r\nnla_for_each_attr(nla, head, len, rem)\r\nif (nla_type(nla) == attrtype)\r\nreturn (struct nlattr *)nla;\r\nreturn NULL;\r\n}\r\nsize_t nla_strlcpy(char *dst, const struct nlattr *nla, size_t dstsize)\r\n{\r\nsize_t srclen = nla_len(nla);\r\nchar *src = nla_data(nla);\r\nif (srclen > 0 && src[srclen - 1] == '\0')\r\nsrclen--;\r\nif (dstsize > 0) {\r\nsize_t len = (srclen >= dstsize) ? dstsize - 1 : srclen;\r\nmemset(dst, 0, dstsize);\r\nmemcpy(dst, src, len);\r\n}\r\nreturn srclen;\r\n}\r\nint nla_memcpy(void *dest, const struct nlattr *src, int count)\r\n{\r\nint minlen = min_t(int, count, nla_len(src));\r\nmemcpy(dest, nla_data(src), minlen);\r\nif (count > minlen)\r\nmemset(dest + minlen, 0, count - minlen);\r\nreturn minlen;\r\n}\r\nint nla_memcmp(const struct nlattr *nla, const void *data,\r\nsize_t size)\r\n{\r\nint d = nla_len(nla) - size;\r\nif (d == 0)\r\nd = memcmp(nla_data(nla), data, size);\r\nreturn d;\r\n}\r\nint nla_strcmp(const struct nlattr *nla, const char *str)\r\n{\r\nint len = strlen(str);\r\nchar *buf = nla_data(nla);\r\nint attrlen = nla_len(nla);\r\nint d;\r\nif (attrlen > 0 && buf[attrlen - 1] == '\0')\r\nattrlen--;\r\nd = attrlen - len;\r\nif (d == 0)\r\nd = memcmp(nla_data(nla), str, len);\r\nreturn d;\r\n}\r\nstruct nlattr *__nla_reserve(struct sk_buff *skb, int attrtype, int attrlen)\r\n{\r\nstruct nlattr *nla;\r\nnla = (struct nlattr *) skb_put(skb, nla_total_size(attrlen));\r\nnla->nla_type = attrtype;\r\nnla->nla_len = nla_attr_size(attrlen);\r\nmemset((unsigned char *) nla + nla->nla_len, 0, nla_padlen(attrlen));\r\nreturn nla;\r\n}\r\nstruct nlattr *__nla_reserve_64bit(struct sk_buff *skb, int attrtype,\r\nint attrlen, int padattr)\r\n{\r\nif (nla_need_padding_for_64bit(skb))\r\nnla_align_64bit(skb, padattr);\r\nreturn __nla_reserve(skb, attrtype, attrlen);\r\n}\r\nvoid *__nla_reserve_nohdr(struct sk_buff *skb, int attrlen)\r\n{\r\nvoid *start;\r\nstart = skb_put(skb, NLA_ALIGN(attrlen));\r\nmemset(start, 0, NLA_ALIGN(attrlen));\r\nreturn start;\r\n}\r\nstruct nlattr *nla_reserve(struct sk_buff *skb, int attrtype, int attrlen)\r\n{\r\nif (unlikely(skb_tailroom(skb) < nla_total_size(attrlen)))\r\nreturn NULL;\r\nreturn __nla_reserve(skb, attrtype, attrlen);\r\n}\r\nstruct nlattr *nla_reserve_64bit(struct sk_buff *skb, int attrtype, int attrlen,\r\nint padattr)\r\n{\r\nsize_t len;\r\nif (nla_need_padding_for_64bit(skb))\r\nlen = nla_total_size_64bit(attrlen);\r\nelse\r\nlen = nla_total_size(attrlen);\r\nif (unlikely(skb_tailroom(skb) < len))\r\nreturn NULL;\r\nreturn __nla_reserve_64bit(skb, attrtype, attrlen, padattr);\r\n}\r\nvoid *nla_reserve_nohdr(struct sk_buff *skb, int attrlen)\r\n{\r\nif (unlikely(skb_tailroom(skb) < NLA_ALIGN(attrlen)))\r\nreturn NULL;\r\nreturn __nla_reserve_nohdr(skb, attrlen);\r\n}\r\nvoid __nla_put(struct sk_buff *skb, int attrtype, int attrlen,\r\nconst void *data)\r\n{\r\nstruct nlattr *nla;\r\nnla = __nla_reserve(skb, attrtype, attrlen);\r\nmemcpy(nla_data(nla), data, attrlen);\r\n}\r\nvoid __nla_put_64bit(struct sk_buff *skb, int attrtype, int attrlen,\r\nconst void *data, int padattr)\r\n{\r\nstruct nlattr *nla;\r\nnla = __nla_reserve_64bit(skb, attrtype, attrlen, padattr);\r\nmemcpy(nla_data(nla), data, attrlen);\r\n}\r\nvoid __nla_put_nohdr(struct sk_buff *skb, int attrlen, const void *data)\r\n{\r\nvoid *start;\r\nstart = __nla_reserve_nohdr(skb, attrlen);\r\nmemcpy(start, data, attrlen);\r\n}\r\nint nla_put(struct sk_buff *skb, int attrtype, int attrlen, const void *data)\r\n{\r\nif (unlikely(skb_tailroom(skb) < nla_total_size(attrlen)))\r\nreturn -EMSGSIZE;\r\n__nla_put(skb, attrtype, attrlen, data);\r\nreturn 0;\r\n}\r\nint nla_put_64bit(struct sk_buff *skb, int attrtype, int attrlen,\r\nconst void *data, int padattr)\r\n{\r\nsize_t len;\r\nif (nla_need_padding_for_64bit(skb))\r\nlen = nla_total_size_64bit(attrlen);\r\nelse\r\nlen = nla_total_size(attrlen);\r\nif (unlikely(skb_tailroom(skb) < len))\r\nreturn -EMSGSIZE;\r\n__nla_put_64bit(skb, attrtype, attrlen, data, padattr);\r\nreturn 0;\r\n}\r\nint nla_put_nohdr(struct sk_buff *skb, int attrlen, const void *data)\r\n{\r\nif (unlikely(skb_tailroom(skb) < NLA_ALIGN(attrlen)))\r\nreturn -EMSGSIZE;\r\n__nla_put_nohdr(skb, attrlen, data);\r\nreturn 0;\r\n}\r\nint nla_append(struct sk_buff *skb, int attrlen, const void *data)\r\n{\r\nif (unlikely(skb_tailroom(skb) < NLA_ALIGN(attrlen)))\r\nreturn -EMSGSIZE;\r\nmemcpy(skb_put(skb, attrlen), data, attrlen);\r\nreturn 0;\r\n}
