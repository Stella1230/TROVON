static inline void blkcipher_map_src(struct blkcipher_walk *walk)\r\n{\r\nwalk->src.virt.addr = scatterwalk_map(&walk->in);\r\n}\r\nstatic inline void blkcipher_map_dst(struct blkcipher_walk *walk)\r\n{\r\nwalk->dst.virt.addr = scatterwalk_map(&walk->out);\r\n}\r\nstatic inline void blkcipher_unmap_src(struct blkcipher_walk *walk)\r\n{\r\nscatterwalk_unmap(walk->src.virt.addr);\r\n}\r\nstatic inline void blkcipher_unmap_dst(struct blkcipher_walk *walk)\r\n{\r\nscatterwalk_unmap(walk->dst.virt.addr);\r\n}\r\nstatic inline u8 *blkcipher_get_spot(u8 *start, unsigned int len)\r\n{\r\nu8 *end_page = (u8 *)(((unsigned long)(start + len - 1)) & PAGE_MASK);\r\nreturn max(start, end_page);\r\n}\r\nstatic inline unsigned int blkcipher_done_slow(struct blkcipher_walk *walk,\r\nunsigned int bsize)\r\n{\r\nu8 *addr;\r\naddr = (u8 *)ALIGN((unsigned long)walk->buffer, walk->alignmask + 1);\r\naddr = blkcipher_get_spot(addr, bsize);\r\nscatterwalk_copychunks(addr, &walk->out, bsize, 1);\r\nreturn bsize;\r\n}\r\nstatic inline unsigned int blkcipher_done_fast(struct blkcipher_walk *walk,\r\nunsigned int n)\r\n{\r\nif (walk->flags & BLKCIPHER_WALK_COPY) {\r\nblkcipher_map_dst(walk);\r\nmemcpy(walk->dst.virt.addr, walk->page, n);\r\nblkcipher_unmap_dst(walk);\r\n} else if (!(walk->flags & BLKCIPHER_WALK_PHYS)) {\r\nif (walk->flags & BLKCIPHER_WALK_DIFF)\r\nblkcipher_unmap_dst(walk);\r\nblkcipher_unmap_src(walk);\r\n}\r\nscatterwalk_advance(&walk->in, n);\r\nscatterwalk_advance(&walk->out, n);\r\nreturn n;\r\n}\r\nint blkcipher_walk_done(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk, int err)\r\n{\r\nunsigned int nbytes = 0;\r\nif (likely(err >= 0)) {\r\nunsigned int n = walk->nbytes - err;\r\nif (likely(!(walk->flags & BLKCIPHER_WALK_SLOW)))\r\nn = blkcipher_done_fast(walk, n);\r\nelse if (WARN_ON(err)) {\r\nerr = -EINVAL;\r\ngoto err;\r\n} else\r\nn = blkcipher_done_slow(walk, n);\r\nnbytes = walk->total - n;\r\nerr = 0;\r\n}\r\nscatterwalk_done(&walk->in, 0, nbytes);\r\nscatterwalk_done(&walk->out, 1, nbytes);\r\nerr:\r\nwalk->total = nbytes;\r\nwalk->nbytes = nbytes;\r\nif (nbytes) {\r\ncrypto_yield(desc->flags);\r\nreturn blkcipher_walk_next(desc, walk);\r\n}\r\nif (walk->iv != desc->info)\r\nmemcpy(desc->info, walk->iv, walk->ivsize);\r\nif (walk->buffer != walk->page)\r\nkfree(walk->buffer);\r\nif (walk->page)\r\nfree_page((unsigned long)walk->page);\r\nreturn err;\r\n}\r\nstatic inline int blkcipher_next_slow(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk,\r\nunsigned int bsize,\r\nunsigned int alignmask)\r\n{\r\nunsigned int n;\r\nunsigned aligned_bsize = ALIGN(bsize, alignmask + 1);\r\nif (walk->buffer)\r\ngoto ok;\r\nwalk->buffer = walk->page;\r\nif (walk->buffer)\r\ngoto ok;\r\nn = aligned_bsize * 3 - (alignmask + 1) +\r\n(alignmask & ~(crypto_tfm_ctx_alignment() - 1));\r\nwalk->buffer = kmalloc(n, GFP_ATOMIC);\r\nif (!walk->buffer)\r\nreturn blkcipher_walk_done(desc, walk, -ENOMEM);\r\nok:\r\nwalk->dst.virt.addr = (u8 *)ALIGN((unsigned long)walk->buffer,\r\nalignmask + 1);\r\nwalk->dst.virt.addr = blkcipher_get_spot(walk->dst.virt.addr, bsize);\r\nwalk->src.virt.addr = blkcipher_get_spot(walk->dst.virt.addr +\r\naligned_bsize, bsize);\r\nscatterwalk_copychunks(walk->src.virt.addr, &walk->in, bsize, 0);\r\nwalk->nbytes = bsize;\r\nwalk->flags |= BLKCIPHER_WALK_SLOW;\r\nreturn 0;\r\n}\r\nstatic inline int blkcipher_next_copy(struct blkcipher_walk *walk)\r\n{\r\nu8 *tmp = walk->page;\r\nblkcipher_map_src(walk);\r\nmemcpy(tmp, walk->src.virt.addr, walk->nbytes);\r\nblkcipher_unmap_src(walk);\r\nwalk->src.virt.addr = tmp;\r\nwalk->dst.virt.addr = tmp;\r\nreturn 0;\r\n}\r\nstatic inline int blkcipher_next_fast(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nunsigned long diff;\r\nwalk->src.phys.page = scatterwalk_page(&walk->in);\r\nwalk->src.phys.offset = offset_in_page(walk->in.offset);\r\nwalk->dst.phys.page = scatterwalk_page(&walk->out);\r\nwalk->dst.phys.offset = offset_in_page(walk->out.offset);\r\nif (walk->flags & BLKCIPHER_WALK_PHYS)\r\nreturn 0;\r\ndiff = walk->src.phys.offset - walk->dst.phys.offset;\r\ndiff |= walk->src.virt.page - walk->dst.virt.page;\r\nblkcipher_map_src(walk);\r\nwalk->dst.virt.addr = walk->src.virt.addr;\r\nif (diff) {\r\nwalk->flags |= BLKCIPHER_WALK_DIFF;\r\nblkcipher_map_dst(walk);\r\n}\r\nreturn 0;\r\n}\r\nstatic int blkcipher_walk_next(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nunsigned int bsize;\r\nunsigned int n;\r\nint err;\r\nn = walk->total;\r\nif (unlikely(n < walk->cipher_blocksize)) {\r\ndesc->flags |= CRYPTO_TFM_RES_BAD_BLOCK_LEN;\r\nreturn blkcipher_walk_done(desc, walk, -EINVAL);\r\n}\r\nbsize = min(walk->walk_blocksize, n);\r\nwalk->flags &= ~(BLKCIPHER_WALK_SLOW | BLKCIPHER_WALK_COPY |\r\nBLKCIPHER_WALK_DIFF);\r\nif (!scatterwalk_aligned(&walk->in, walk->alignmask) ||\r\n!scatterwalk_aligned(&walk->out, walk->alignmask)) {\r\nwalk->flags |= BLKCIPHER_WALK_COPY;\r\nif (!walk->page) {\r\nwalk->page = (void *)__get_free_page(GFP_ATOMIC);\r\nif (!walk->page)\r\nn = 0;\r\n}\r\n}\r\nn = scatterwalk_clamp(&walk->in, n);\r\nn = scatterwalk_clamp(&walk->out, n);\r\nif (unlikely(n < bsize)) {\r\nerr = blkcipher_next_slow(desc, walk, bsize, walk->alignmask);\r\ngoto set_phys_lowmem;\r\n}\r\nwalk->nbytes = n;\r\nif (walk->flags & BLKCIPHER_WALK_COPY) {\r\nerr = blkcipher_next_copy(walk);\r\ngoto set_phys_lowmem;\r\n}\r\nreturn blkcipher_next_fast(desc, walk);\r\nset_phys_lowmem:\r\nif (walk->flags & BLKCIPHER_WALK_PHYS) {\r\nwalk->src.phys.page = virt_to_page(walk->src.virt.addr);\r\nwalk->dst.phys.page = virt_to_page(walk->dst.virt.addr);\r\nwalk->src.phys.offset &= PAGE_SIZE - 1;\r\nwalk->dst.phys.offset &= PAGE_SIZE - 1;\r\n}\r\nreturn err;\r\n}\r\nstatic inline int blkcipher_copy_iv(struct blkcipher_walk *walk)\r\n{\r\nunsigned bs = walk->walk_blocksize;\r\nunsigned aligned_bs = ALIGN(bs, walk->alignmask + 1);\r\nunsigned int size = aligned_bs * 2 +\r\nwalk->ivsize + max(aligned_bs, walk->ivsize) -\r\n(walk->alignmask + 1);\r\nu8 *iv;\r\nsize += walk->alignmask & ~(crypto_tfm_ctx_alignment() - 1);\r\nwalk->buffer = kmalloc(size, GFP_ATOMIC);\r\nif (!walk->buffer)\r\nreturn -ENOMEM;\r\niv = (u8 *)ALIGN((unsigned long)walk->buffer, walk->alignmask + 1);\r\niv = blkcipher_get_spot(iv, bs) + aligned_bs;\r\niv = blkcipher_get_spot(iv, bs) + aligned_bs;\r\niv = blkcipher_get_spot(iv, walk->ivsize);\r\nwalk->iv = memcpy(iv, walk->iv, walk->ivsize);\r\nreturn 0;\r\n}\r\nint blkcipher_walk_virt(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nwalk->flags &= ~BLKCIPHER_WALK_PHYS;\r\nwalk->walk_blocksize = crypto_blkcipher_blocksize(desc->tfm);\r\nwalk->cipher_blocksize = walk->walk_blocksize;\r\nwalk->ivsize = crypto_blkcipher_ivsize(desc->tfm);\r\nwalk->alignmask = crypto_blkcipher_alignmask(desc->tfm);\r\nreturn blkcipher_walk_first(desc, walk);\r\n}\r\nint blkcipher_walk_phys(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nwalk->flags |= BLKCIPHER_WALK_PHYS;\r\nwalk->walk_blocksize = crypto_blkcipher_blocksize(desc->tfm);\r\nwalk->cipher_blocksize = walk->walk_blocksize;\r\nwalk->ivsize = crypto_blkcipher_ivsize(desc->tfm);\r\nwalk->alignmask = crypto_blkcipher_alignmask(desc->tfm);\r\nreturn blkcipher_walk_first(desc, walk);\r\n}\r\nstatic int blkcipher_walk_first(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nif (WARN_ON_ONCE(in_irq()))\r\nreturn -EDEADLK;\r\nwalk->iv = desc->info;\r\nwalk->nbytes = walk->total;\r\nif (unlikely(!walk->total))\r\nreturn 0;\r\nwalk->buffer = NULL;\r\nif (unlikely(((unsigned long)walk->iv & walk->alignmask))) {\r\nint err = blkcipher_copy_iv(walk);\r\nif (err)\r\nreturn err;\r\n}\r\nscatterwalk_start(&walk->in, walk->in.sg);\r\nscatterwalk_start(&walk->out, walk->out.sg);\r\nwalk->page = NULL;\r\nreturn blkcipher_walk_next(desc, walk);\r\n}\r\nint blkcipher_walk_virt_block(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk,\r\nunsigned int blocksize)\r\n{\r\nwalk->flags &= ~BLKCIPHER_WALK_PHYS;\r\nwalk->walk_blocksize = blocksize;\r\nwalk->cipher_blocksize = crypto_blkcipher_blocksize(desc->tfm);\r\nwalk->ivsize = crypto_blkcipher_ivsize(desc->tfm);\r\nwalk->alignmask = crypto_blkcipher_alignmask(desc->tfm);\r\nreturn blkcipher_walk_first(desc, walk);\r\n}\r\nint blkcipher_aead_walk_virt_block(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk,\r\nstruct crypto_aead *tfm,\r\nunsigned int blocksize)\r\n{\r\nwalk->flags &= ~BLKCIPHER_WALK_PHYS;\r\nwalk->walk_blocksize = blocksize;\r\nwalk->cipher_blocksize = crypto_aead_blocksize(tfm);\r\nwalk->ivsize = crypto_aead_ivsize(tfm);\r\nwalk->alignmask = crypto_aead_alignmask(tfm);\r\nreturn blkcipher_walk_first(desc, walk);\r\n}\r\nstatic int setkey_unaligned(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct blkcipher_alg *cipher = &tfm->__crt_alg->cra_blkcipher;\r\nunsigned long alignmask = crypto_tfm_alg_alignmask(tfm);\r\nint ret;\r\nu8 *buffer, *alignbuffer;\r\nunsigned long absize;\r\nabsize = keylen + alignmask;\r\nbuffer = kmalloc(absize, GFP_ATOMIC);\r\nif (!buffer)\r\nreturn -ENOMEM;\r\nalignbuffer = (u8 *)ALIGN((unsigned long)buffer, alignmask + 1);\r\nmemcpy(alignbuffer, key, keylen);\r\nret = cipher->setkey(tfm, alignbuffer, keylen);\r\nmemset(alignbuffer, 0, keylen);\r\nkfree(buffer);\r\nreturn ret;\r\n}\r\nstatic int setkey(struct crypto_tfm *tfm, const u8 *key, unsigned int keylen)\r\n{\r\nstruct blkcipher_alg *cipher = &tfm->__crt_alg->cra_blkcipher;\r\nunsigned long alignmask = crypto_tfm_alg_alignmask(tfm);\r\nif (keylen < cipher->min_keysize || keylen > cipher->max_keysize) {\r\ntfm->crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nif ((unsigned long)key & alignmask)\r\nreturn setkey_unaligned(tfm, key, keylen);\r\nreturn cipher->setkey(tfm, key, keylen);\r\n}\r\nstatic int async_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nreturn setkey(crypto_ablkcipher_tfm(tfm), key, keylen);\r\n}\r\nstatic int async_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm = req->base.tfm;\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\nstruct blkcipher_desc desc = {\r\n.tfm = __crypto_blkcipher_cast(tfm),\r\n.info = req->info,\r\n.flags = req->base.flags,\r\n};\r\nreturn alg->encrypt(&desc, req->dst, req->src, req->nbytes);\r\n}\r\nstatic int async_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_tfm *tfm = req->base.tfm;\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\nstruct blkcipher_desc desc = {\r\n.tfm = __crypto_blkcipher_cast(tfm),\r\n.info = req->info,\r\n.flags = req->base.flags,\r\n};\r\nreturn alg->decrypt(&desc, req->dst, req->src, req->nbytes);\r\n}\r\nstatic unsigned int crypto_blkcipher_ctxsize(struct crypto_alg *alg, u32 type,\r\nu32 mask)\r\n{\r\nstruct blkcipher_alg *cipher = &alg->cra_blkcipher;\r\nunsigned int len = alg->cra_ctxsize;\r\nif ((mask & CRYPTO_ALG_TYPE_MASK) == CRYPTO_ALG_TYPE_MASK &&\r\ncipher->ivsize) {\r\nlen = ALIGN(len, (unsigned long)alg->cra_alignmask + 1);\r\nlen += cipher->ivsize;\r\n}\r\nreturn len;\r\n}\r\nstatic int crypto_init_blkcipher_ops_async(struct crypto_tfm *tfm)\r\n{\r\nstruct ablkcipher_tfm *crt = &tfm->crt_ablkcipher;\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\ncrt->setkey = async_setkey;\r\ncrt->encrypt = async_encrypt;\r\ncrt->decrypt = async_decrypt;\r\ncrt->base = __crypto_ablkcipher_cast(tfm);\r\ncrt->ivsize = alg->ivsize;\r\nreturn 0;\r\n}\r\nstatic int crypto_init_blkcipher_ops_sync(struct crypto_tfm *tfm)\r\n{\r\nstruct blkcipher_tfm *crt = &tfm->crt_blkcipher;\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\nunsigned long align = crypto_tfm_alg_alignmask(tfm) + 1;\r\nunsigned long addr;\r\ncrt->setkey = setkey;\r\ncrt->encrypt = alg->encrypt;\r\ncrt->decrypt = alg->decrypt;\r\naddr = (unsigned long)crypto_tfm_ctx(tfm);\r\naddr = ALIGN(addr, align);\r\naddr += ALIGN(tfm->__crt_alg->cra_ctxsize, align);\r\ncrt->iv = (void *)addr;\r\nreturn 0;\r\n}\r\nstatic int crypto_init_blkcipher_ops(struct crypto_tfm *tfm, u32 type, u32 mask)\r\n{\r\nstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\r\nif (alg->ivsize > PAGE_SIZE / 8)\r\nreturn -EINVAL;\r\nif ((mask & CRYPTO_ALG_TYPE_MASK) == CRYPTO_ALG_TYPE_MASK)\r\nreturn crypto_init_blkcipher_ops_sync(tfm);\r\nelse\r\nreturn crypto_init_blkcipher_ops_async(tfm);\r\n}\r\nstatic int crypto_blkcipher_report(struct sk_buff *skb, struct crypto_alg *alg)\r\n{\r\nstruct crypto_report_blkcipher rblkcipher;\r\nstrncpy(rblkcipher.type, "blkcipher", sizeof(rblkcipher.type));\r\nstrncpy(rblkcipher.geniv, alg->cra_blkcipher.geniv ?: "<default>",\r\nsizeof(rblkcipher.geniv));\r\nrblkcipher.blocksize = alg->cra_blocksize;\r\nrblkcipher.min_keysize = alg->cra_blkcipher.min_keysize;\r\nrblkcipher.max_keysize = alg->cra_blkcipher.max_keysize;\r\nrblkcipher.ivsize = alg->cra_blkcipher.ivsize;\r\nif (nla_put(skb, CRYPTOCFGA_REPORT_BLKCIPHER,\r\nsizeof(struct crypto_report_blkcipher), &rblkcipher))\r\ngoto nla_put_failure;\r\nreturn 0;\r\nnla_put_failure:\r\nreturn -EMSGSIZE;\r\n}\r\nstatic int crypto_blkcipher_report(struct sk_buff *skb, struct crypto_alg *alg)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic void crypto_blkcipher_show(struct seq_file *m, struct crypto_alg *alg)\r\n{\r\nseq_printf(m, "type : blkcipher\n");\r\nseq_printf(m, "blocksize : %u\n", alg->cra_blocksize);\r\nseq_printf(m, "min keysize : %u\n", alg->cra_blkcipher.min_keysize);\r\nseq_printf(m, "max keysize : %u\n", alg->cra_blkcipher.max_keysize);\r\nseq_printf(m, "ivsize : %u\n", alg->cra_blkcipher.ivsize);\r\nseq_printf(m, "geniv : %s\n", alg->cra_blkcipher.geniv ?:\r\n"<default>");\r\n}
