int kexec_should_crash(struct task_struct *p)\r\n{\r\nif (crash_kexec_post_notifiers)\r\nreturn 0;\r\nif (in_interrupt() || !p->pid || is_global_init(p) || panic_on_oops)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nint kexec_crash_loaded(void)\r\n{\r\nreturn !!kexec_crash_image;\r\n}\r\nint sanity_check_segment_list(struct kimage *image)\r\n{\r\nint i;\r\nunsigned long nr_segments = image->nr_segments;\r\nunsigned long total_pages = 0;\r\nfor (i = 0; i < nr_segments; i++) {\r\nunsigned long mstart, mend;\r\nmstart = image->segment[i].mem;\r\nmend = mstart + image->segment[i].memsz;\r\nif (mstart > mend)\r\nreturn -EADDRNOTAVAIL;\r\nif ((mstart & ~PAGE_MASK) || (mend & ~PAGE_MASK))\r\nreturn -EADDRNOTAVAIL;\r\nif (mend >= KEXEC_DESTINATION_MEMORY_LIMIT)\r\nreturn -EADDRNOTAVAIL;\r\n}\r\nfor (i = 0; i < nr_segments; i++) {\r\nunsigned long mstart, mend;\r\nunsigned long j;\r\nmstart = image->segment[i].mem;\r\nmend = mstart + image->segment[i].memsz;\r\nfor (j = 0; j < i; j++) {\r\nunsigned long pstart, pend;\r\npstart = image->segment[j].mem;\r\npend = pstart + image->segment[j].memsz;\r\nif ((mend > pstart) && (mstart < pend))\r\nreturn -EINVAL;\r\n}\r\n}\r\nfor (i = 0; i < nr_segments; i++) {\r\nif (image->segment[i].bufsz > image->segment[i].memsz)\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < nr_segments; i++) {\r\nif (PAGE_COUNT(image->segment[i].memsz) > totalram_pages / 2)\r\nreturn -EINVAL;\r\ntotal_pages += PAGE_COUNT(image->segment[i].memsz);\r\n}\r\nif (total_pages > totalram_pages / 2)\r\nreturn -EINVAL;\r\nif (image->type == KEXEC_TYPE_CRASH) {\r\nfor (i = 0; i < nr_segments; i++) {\r\nunsigned long mstart, mend;\r\nmstart = image->segment[i].mem;\r\nmend = mstart + image->segment[i].memsz - 1;\r\nif ((mstart < phys_to_boot_phys(crashk_res.start)) ||\r\n(mend > phys_to_boot_phys(crashk_res.end)))\r\nreturn -EADDRNOTAVAIL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstruct kimage *do_kimage_alloc_init(void)\r\n{\r\nstruct kimage *image;\r\nimage = kzalloc(sizeof(*image), GFP_KERNEL);\r\nif (!image)\r\nreturn NULL;\r\nimage->head = 0;\r\nimage->entry = &image->head;\r\nimage->last_entry = &image->head;\r\nimage->control_page = ~0;\r\nimage->type = KEXEC_TYPE_DEFAULT;\r\nINIT_LIST_HEAD(&image->control_pages);\r\nINIT_LIST_HEAD(&image->dest_pages);\r\nINIT_LIST_HEAD(&image->unusable_pages);\r\nreturn image;\r\n}\r\nint kimage_is_destination_range(struct kimage *image,\r\nunsigned long start,\r\nunsigned long end)\r\n{\r\nunsigned long i;\r\nfor (i = 0; i < image->nr_segments; i++) {\r\nunsigned long mstart, mend;\r\nmstart = image->segment[i].mem;\r\nmend = mstart + image->segment[i].memsz;\r\nif ((end > mstart) && (start < mend))\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct page *kimage_alloc_pages(gfp_t gfp_mask, unsigned int order)\r\n{\r\nstruct page *pages;\r\npages = alloc_pages(gfp_mask, order);\r\nif (pages) {\r\nunsigned int count, i;\r\npages->mapping = NULL;\r\nset_page_private(pages, order);\r\ncount = 1 << order;\r\nfor (i = 0; i < count; i++)\r\nSetPageReserved(pages + i);\r\n}\r\nreturn pages;\r\n}\r\nstatic void kimage_free_pages(struct page *page)\r\n{\r\nunsigned int order, count, i;\r\norder = page_private(page);\r\ncount = 1 << order;\r\nfor (i = 0; i < count; i++)\r\nClearPageReserved(page + i);\r\n__free_pages(page, order);\r\n}\r\nvoid kimage_free_page_list(struct list_head *list)\r\n{\r\nstruct page *page, *next;\r\nlist_for_each_entry_safe(page, next, list, lru) {\r\nlist_del(&page->lru);\r\nkimage_free_pages(page);\r\n}\r\n}\r\nstatic struct page *kimage_alloc_normal_control_pages(struct kimage *image,\r\nunsigned int order)\r\n{\r\nstruct list_head extra_pages;\r\nstruct page *pages;\r\nunsigned int count;\r\ncount = 1 << order;\r\nINIT_LIST_HEAD(&extra_pages);\r\ndo {\r\nunsigned long pfn, epfn, addr, eaddr;\r\npages = kimage_alloc_pages(KEXEC_CONTROL_MEMORY_GFP, order);\r\nif (!pages)\r\nbreak;\r\npfn = page_to_boot_pfn(pages);\r\nepfn = pfn + count;\r\naddr = pfn << PAGE_SHIFT;\r\neaddr = epfn << PAGE_SHIFT;\r\nif ((epfn >= (KEXEC_CONTROL_MEMORY_LIMIT >> PAGE_SHIFT)) ||\r\nkimage_is_destination_range(image, addr, eaddr)) {\r\nlist_add(&pages->lru, &extra_pages);\r\npages = NULL;\r\n}\r\n} while (!pages);\r\nif (pages) {\r\nlist_add(&pages->lru, &image->control_pages);\r\n}\r\nkimage_free_page_list(&extra_pages);\r\nreturn pages;\r\n}\r\nstatic struct page *kimage_alloc_crash_control_pages(struct kimage *image,\r\nunsigned int order)\r\n{\r\nunsigned long hole_start, hole_end, size;\r\nstruct page *pages;\r\npages = NULL;\r\nsize = (1 << order) << PAGE_SHIFT;\r\nhole_start = (image->control_page + (size - 1)) & ~(size - 1);\r\nhole_end = hole_start + size - 1;\r\nwhile (hole_end <= crashk_res.end) {\r\nunsigned long i;\r\nif (hole_end > KEXEC_CRASH_CONTROL_MEMORY_LIMIT)\r\nbreak;\r\nfor (i = 0; i < image->nr_segments; i++) {\r\nunsigned long mstart, mend;\r\nmstart = image->segment[i].mem;\r\nmend = mstart + image->segment[i].memsz - 1;\r\nif ((hole_end >= mstart) && (hole_start <= mend)) {\r\nhole_start = (mend + (size - 1)) & ~(size - 1);\r\nhole_end = hole_start + size - 1;\r\nbreak;\r\n}\r\n}\r\nif (i == image->nr_segments) {\r\npages = pfn_to_page(hole_start >> PAGE_SHIFT);\r\nimage->control_page = hole_end;\r\nbreak;\r\n}\r\n}\r\nreturn pages;\r\n}\r\nstruct page *kimage_alloc_control_pages(struct kimage *image,\r\nunsigned int order)\r\n{\r\nstruct page *pages = NULL;\r\nswitch (image->type) {\r\ncase KEXEC_TYPE_DEFAULT:\r\npages = kimage_alloc_normal_control_pages(image, order);\r\nbreak;\r\ncase KEXEC_TYPE_CRASH:\r\npages = kimage_alloc_crash_control_pages(image, order);\r\nbreak;\r\n}\r\nreturn pages;\r\n}\r\nstatic int kimage_add_entry(struct kimage *image, kimage_entry_t entry)\r\n{\r\nif (*image->entry != 0)\r\nimage->entry++;\r\nif (image->entry == image->last_entry) {\r\nkimage_entry_t *ind_page;\r\nstruct page *page;\r\npage = kimage_alloc_page(image, GFP_KERNEL, KIMAGE_NO_DEST);\r\nif (!page)\r\nreturn -ENOMEM;\r\nind_page = page_address(page);\r\n*image->entry = virt_to_boot_phys(ind_page) | IND_INDIRECTION;\r\nimage->entry = ind_page;\r\nimage->last_entry = ind_page +\r\n((PAGE_SIZE/sizeof(kimage_entry_t)) - 1);\r\n}\r\n*image->entry = entry;\r\nimage->entry++;\r\n*image->entry = 0;\r\nreturn 0;\r\n}\r\nstatic int kimage_set_destination(struct kimage *image,\r\nunsigned long destination)\r\n{\r\nint result;\r\ndestination &= PAGE_MASK;\r\nresult = kimage_add_entry(image, destination | IND_DESTINATION);\r\nreturn result;\r\n}\r\nstatic int kimage_add_page(struct kimage *image, unsigned long page)\r\n{\r\nint result;\r\npage &= PAGE_MASK;\r\nresult = kimage_add_entry(image, page | IND_SOURCE);\r\nreturn result;\r\n}\r\nstatic void kimage_free_extra_pages(struct kimage *image)\r\n{\r\nkimage_free_page_list(&image->dest_pages);\r\nkimage_free_page_list(&image->unusable_pages);\r\n}\r\nvoid kimage_terminate(struct kimage *image)\r\n{\r\nif (*image->entry != 0)\r\nimage->entry++;\r\n*image->entry = IND_DONE;\r\n}\r\nstatic void kimage_free_entry(kimage_entry_t entry)\r\n{\r\nstruct page *page;\r\npage = boot_pfn_to_page(entry >> PAGE_SHIFT);\r\nkimage_free_pages(page);\r\n}\r\nvoid kimage_free(struct kimage *image)\r\n{\r\nkimage_entry_t *ptr, entry;\r\nkimage_entry_t ind = 0;\r\nif (!image)\r\nreturn;\r\nkimage_free_extra_pages(image);\r\nfor_each_kimage_entry(image, ptr, entry) {\r\nif (entry & IND_INDIRECTION) {\r\nif (ind & IND_INDIRECTION)\r\nkimage_free_entry(ind);\r\nind = entry;\r\n} else if (entry & IND_SOURCE)\r\nkimage_free_entry(entry);\r\n}\r\nif (ind & IND_INDIRECTION)\r\nkimage_free_entry(ind);\r\nmachine_kexec_cleanup(image);\r\nkimage_free_page_list(&image->control_pages);\r\nif (image->file_mode)\r\nkimage_file_post_load_cleanup(image);\r\nkfree(image);\r\n}\r\nstatic kimage_entry_t *kimage_dst_used(struct kimage *image,\r\nunsigned long page)\r\n{\r\nkimage_entry_t *ptr, entry;\r\nunsigned long destination = 0;\r\nfor_each_kimage_entry(image, ptr, entry) {\r\nif (entry & IND_DESTINATION)\r\ndestination = entry & PAGE_MASK;\r\nelse if (entry & IND_SOURCE) {\r\nif (page == destination)\r\nreturn ptr;\r\ndestination += PAGE_SIZE;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct page *kimage_alloc_page(struct kimage *image,\r\ngfp_t gfp_mask,\r\nunsigned long destination)\r\n{\r\nstruct page *page;\r\nunsigned long addr;\r\nlist_for_each_entry(page, &image->dest_pages, lru) {\r\naddr = page_to_boot_pfn(page) << PAGE_SHIFT;\r\nif (addr == destination) {\r\nlist_del(&page->lru);\r\nreturn page;\r\n}\r\n}\r\npage = NULL;\r\nwhile (1) {\r\nkimage_entry_t *old;\r\npage = kimage_alloc_pages(gfp_mask, 0);\r\nif (!page)\r\nreturn NULL;\r\nif (page_to_boot_pfn(page) >\r\n(KEXEC_SOURCE_MEMORY_LIMIT >> PAGE_SHIFT)) {\r\nlist_add(&page->lru, &image->unusable_pages);\r\ncontinue;\r\n}\r\naddr = page_to_boot_pfn(page) << PAGE_SHIFT;\r\nif (addr == destination)\r\nbreak;\r\nif (!kimage_is_destination_range(image, addr,\r\naddr + PAGE_SIZE))\r\nbreak;\r\nold = kimage_dst_used(image, addr);\r\nif (old) {\r\nunsigned long old_addr;\r\nstruct page *old_page;\r\nold_addr = *old & PAGE_MASK;\r\nold_page = boot_pfn_to_page(old_addr >> PAGE_SHIFT);\r\ncopy_highpage(page, old_page);\r\n*old = addr | (*old & ~PAGE_MASK);\r\nif (!(gfp_mask & __GFP_HIGHMEM) &&\r\nPageHighMem(old_page)) {\r\nkimage_free_pages(old_page);\r\ncontinue;\r\n}\r\naddr = old_addr;\r\npage = old_page;\r\nbreak;\r\n}\r\nlist_add(&page->lru, &image->dest_pages);\r\n}\r\nreturn page;\r\n}\r\nstatic int kimage_load_normal_segment(struct kimage *image,\r\nstruct kexec_segment *segment)\r\n{\r\nunsigned long maddr;\r\nsize_t ubytes, mbytes;\r\nint result;\r\nunsigned char __user *buf = NULL;\r\nunsigned char *kbuf = NULL;\r\nresult = 0;\r\nif (image->file_mode)\r\nkbuf = segment->kbuf;\r\nelse\r\nbuf = segment->buf;\r\nubytes = segment->bufsz;\r\nmbytes = segment->memsz;\r\nmaddr = segment->mem;\r\nresult = kimage_set_destination(image, maddr);\r\nif (result < 0)\r\ngoto out;\r\nwhile (mbytes) {\r\nstruct page *page;\r\nchar *ptr;\r\nsize_t uchunk, mchunk;\r\npage = kimage_alloc_page(image, GFP_HIGHUSER, maddr);\r\nif (!page) {\r\nresult = -ENOMEM;\r\ngoto out;\r\n}\r\nresult = kimage_add_page(image, page_to_boot_pfn(page)\r\n<< PAGE_SHIFT);\r\nif (result < 0)\r\ngoto out;\r\nptr = kmap(page);\r\nclear_page(ptr);\r\nptr += maddr & ~PAGE_MASK;\r\nmchunk = min_t(size_t, mbytes,\r\nPAGE_SIZE - (maddr & ~PAGE_MASK));\r\nuchunk = min(ubytes, mchunk);\r\nif (image->file_mode)\r\nmemcpy(ptr, kbuf, uchunk);\r\nelse\r\nresult = copy_from_user(ptr, buf, uchunk);\r\nkunmap(page);\r\nif (result) {\r\nresult = -EFAULT;\r\ngoto out;\r\n}\r\nubytes -= uchunk;\r\nmaddr += mchunk;\r\nif (image->file_mode)\r\nkbuf += mchunk;\r\nelse\r\nbuf += mchunk;\r\nmbytes -= mchunk;\r\n}\r\nout:\r\nreturn result;\r\n}\r\nstatic int kimage_load_crash_segment(struct kimage *image,\r\nstruct kexec_segment *segment)\r\n{\r\nunsigned long maddr;\r\nsize_t ubytes, mbytes;\r\nint result;\r\nunsigned char __user *buf = NULL;\r\nunsigned char *kbuf = NULL;\r\nresult = 0;\r\nif (image->file_mode)\r\nkbuf = segment->kbuf;\r\nelse\r\nbuf = segment->buf;\r\nubytes = segment->bufsz;\r\nmbytes = segment->memsz;\r\nmaddr = segment->mem;\r\nwhile (mbytes) {\r\nstruct page *page;\r\nchar *ptr;\r\nsize_t uchunk, mchunk;\r\npage = boot_pfn_to_page(maddr >> PAGE_SHIFT);\r\nif (!page) {\r\nresult = -ENOMEM;\r\ngoto out;\r\n}\r\nptr = kmap(page);\r\nptr += maddr & ~PAGE_MASK;\r\nmchunk = min_t(size_t, mbytes,\r\nPAGE_SIZE - (maddr & ~PAGE_MASK));\r\nuchunk = min(ubytes, mchunk);\r\nif (mchunk > uchunk) {\r\nmemset(ptr + uchunk, 0, mchunk - uchunk);\r\n}\r\nif (image->file_mode)\r\nmemcpy(ptr, kbuf, uchunk);\r\nelse\r\nresult = copy_from_user(ptr, buf, uchunk);\r\nkexec_flush_icache_page(page);\r\nkunmap(page);\r\nif (result) {\r\nresult = -EFAULT;\r\ngoto out;\r\n}\r\nubytes -= uchunk;\r\nmaddr += mchunk;\r\nif (image->file_mode)\r\nkbuf += mchunk;\r\nelse\r\nbuf += mchunk;\r\nmbytes -= mchunk;\r\n}\r\nout:\r\nreturn result;\r\n}\r\nint kimage_load_segment(struct kimage *image,\r\nstruct kexec_segment *segment)\r\n{\r\nint result = -ENOMEM;\r\nswitch (image->type) {\r\ncase KEXEC_TYPE_DEFAULT:\r\nresult = kimage_load_normal_segment(image, segment);\r\nbreak;\r\ncase KEXEC_TYPE_CRASH:\r\nresult = kimage_load_crash_segment(image, segment);\r\nbreak;\r\n}\r\nreturn result;\r\n}\r\nvoid __crash_kexec(struct pt_regs *regs)\r\n{\r\nif (mutex_trylock(&kexec_mutex)) {\r\nif (kexec_crash_image) {\r\nstruct pt_regs fixed_regs;\r\ncrash_setup_regs(&fixed_regs, regs);\r\ncrash_save_vmcoreinfo();\r\nmachine_crash_shutdown(&fixed_regs);\r\nmachine_kexec(kexec_crash_image);\r\n}\r\nmutex_unlock(&kexec_mutex);\r\n}\r\n}\r\nvoid crash_kexec(struct pt_regs *regs)\r\n{\r\nint old_cpu, this_cpu;\r\nthis_cpu = raw_smp_processor_id();\r\nold_cpu = atomic_cmpxchg(&panic_cpu, PANIC_CPU_INVALID, this_cpu);\r\nif (old_cpu == PANIC_CPU_INVALID) {\r\nprintk_nmi_flush_on_panic();\r\n__crash_kexec(regs);\r\natomic_set(&panic_cpu, PANIC_CPU_INVALID);\r\n}\r\n}\r\nsize_t crash_get_memory_size(void)\r\n{\r\nsize_t size = 0;\r\nmutex_lock(&kexec_mutex);\r\nif (crashk_res.end != crashk_res.start)\r\nsize = resource_size(&crashk_res);\r\nmutex_unlock(&kexec_mutex);\r\nreturn size;\r\n}\r\nvoid __weak crash_free_reserved_phys_range(unsigned long begin,\r\nunsigned long end)\r\n{\r\nunsigned long addr;\r\nfor (addr = begin; addr < end; addr += PAGE_SIZE)\r\nfree_reserved_page(boot_pfn_to_page(addr >> PAGE_SHIFT));\r\n}\r\nint crash_shrink_memory(unsigned long new_size)\r\n{\r\nint ret = 0;\r\nunsigned long start, end;\r\nunsigned long old_size;\r\nstruct resource *ram_res;\r\nmutex_lock(&kexec_mutex);\r\nif (kexec_crash_image) {\r\nret = -ENOENT;\r\ngoto unlock;\r\n}\r\nstart = crashk_res.start;\r\nend = crashk_res.end;\r\nold_size = (end == 0) ? 0 : end - start + 1;\r\nif (new_size >= old_size) {\r\nret = (new_size == old_size) ? 0 : -EINVAL;\r\ngoto unlock;\r\n}\r\nram_res = kzalloc(sizeof(*ram_res), GFP_KERNEL);\r\nif (!ram_res) {\r\nret = -ENOMEM;\r\ngoto unlock;\r\n}\r\nstart = roundup(start, KEXEC_CRASH_MEM_ALIGN);\r\nend = roundup(start + new_size, KEXEC_CRASH_MEM_ALIGN);\r\ncrash_free_reserved_phys_range(end, crashk_res.end);\r\nif ((start == end) && (crashk_res.parent != NULL))\r\nrelease_resource(&crashk_res);\r\nram_res->start = end;\r\nram_res->end = crashk_res.end;\r\nram_res->flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM;\r\nram_res->name = "System RAM";\r\ncrashk_res.end = end - 1;\r\ninsert_resource(&iomem_resource, ram_res);\r\nunlock:\r\nmutex_unlock(&kexec_mutex);\r\nreturn ret;\r\n}\r\nstatic u32 *append_elf_note(u32 *buf, char *name, unsigned type, void *data,\r\nsize_t data_len)\r\n{\r\nstruct elf_note note;\r\nnote.n_namesz = strlen(name) + 1;\r\nnote.n_descsz = data_len;\r\nnote.n_type = type;\r\nmemcpy(buf, &note, sizeof(note));\r\nbuf += (sizeof(note) + 3)/4;\r\nmemcpy(buf, name, note.n_namesz);\r\nbuf += (note.n_namesz + 3)/4;\r\nmemcpy(buf, data, note.n_descsz);\r\nbuf += (note.n_descsz + 3)/4;\r\nreturn buf;\r\n}\r\nstatic void final_note(u32 *buf)\r\n{\r\nstruct elf_note note;\r\nnote.n_namesz = 0;\r\nnote.n_descsz = 0;\r\nnote.n_type = 0;\r\nmemcpy(buf, &note, sizeof(note));\r\n}\r\nvoid crash_save_cpu(struct pt_regs *regs, int cpu)\r\n{\r\nstruct elf_prstatus prstatus;\r\nu32 *buf;\r\nif ((cpu < 0) || (cpu >= nr_cpu_ids))\r\nreturn;\r\nbuf = (u32 *)per_cpu_ptr(crash_notes, cpu);\r\nif (!buf)\r\nreturn;\r\nmemset(&prstatus, 0, sizeof(prstatus));\r\nprstatus.pr_pid = current->pid;\r\nelf_core_copy_kernel_regs(&prstatus.pr_reg, regs);\r\nbuf = append_elf_note(buf, KEXEC_CORE_NOTE_NAME, NT_PRSTATUS,\r\n&prstatus, sizeof(prstatus));\r\nfinal_note(buf);\r\n}\r\nstatic int __init crash_notes_memory_init(void)\r\n{\r\nsize_t size, align;\r\nsize = sizeof(note_buf_t);\r\nalign = min(roundup_pow_of_two(sizeof(note_buf_t)), PAGE_SIZE);\r\nBUILD_BUG_ON(size > PAGE_SIZE);\r\ncrash_notes = __alloc_percpu(size, align);\r\nif (!crash_notes) {\r\npr_warn("Memory allocation for saving cpu register states failed\n");\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init parse_crashkernel_mem(char *cmdline,\r\nunsigned long long system_ram,\r\nunsigned long long *crash_size,\r\nunsigned long long *crash_base)\r\n{\r\nchar *cur = cmdline, *tmp;\r\ndo {\r\nunsigned long long start, end = ULLONG_MAX, size;\r\nstart = memparse(cur, &tmp);\r\nif (cur == tmp) {\r\npr_warn("crashkernel: Memory value expected\n");\r\nreturn -EINVAL;\r\n}\r\ncur = tmp;\r\nif (*cur != '-') {\r\npr_warn("crashkernel: '-' expected\n");\r\nreturn -EINVAL;\r\n}\r\ncur++;\r\nif (*cur != ':') {\r\nend = memparse(cur, &tmp);\r\nif (cur == tmp) {\r\npr_warn("crashkernel: Memory value expected\n");\r\nreturn -EINVAL;\r\n}\r\ncur = tmp;\r\nif (end <= start) {\r\npr_warn("crashkernel: end <= start\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (*cur != ':') {\r\npr_warn("crashkernel: ':' expected\n");\r\nreturn -EINVAL;\r\n}\r\ncur++;\r\nsize = memparse(cur, &tmp);\r\nif (cur == tmp) {\r\npr_warn("Memory value expected\n");\r\nreturn -EINVAL;\r\n}\r\ncur = tmp;\r\nif (size >= system_ram) {\r\npr_warn("crashkernel: invalid size\n");\r\nreturn -EINVAL;\r\n}\r\nif (system_ram >= start && system_ram < end) {\r\n*crash_size = size;\r\nbreak;\r\n}\r\n} while (*cur++ == ',');\r\nif (*crash_size > 0) {\r\nwhile (*cur && *cur != ' ' && *cur != '@')\r\ncur++;\r\nif (*cur == '@') {\r\ncur++;\r\n*crash_base = memparse(cur, &tmp);\r\nif (cur == tmp) {\r\npr_warn("Memory value expected after '@'\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init parse_crashkernel_simple(char *cmdline,\r\nunsigned long long *crash_size,\r\nunsigned long long *crash_base)\r\n{\r\nchar *cur = cmdline;\r\n*crash_size = memparse(cmdline, &cur);\r\nif (cmdline == cur) {\r\npr_warn("crashkernel: memory value expected\n");\r\nreturn -EINVAL;\r\n}\r\nif (*cur == '@')\r\n*crash_base = memparse(cur+1, &cur);\r\nelse if (*cur != ' ' && *cur != '\0') {\r\npr_warn("crashkernel: unrecognized char: %c\n", *cur);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init parse_crashkernel_suffix(char *cmdline,\r\nunsigned long long *crash_size,\r\nconst char *suffix)\r\n{\r\nchar *cur = cmdline;\r\n*crash_size = memparse(cmdline, &cur);\r\nif (cmdline == cur) {\r\npr_warn("crashkernel: memory value expected\n");\r\nreturn -EINVAL;\r\n}\r\nif (strncmp(cur, suffix, strlen(suffix))) {\r\npr_warn("crashkernel: unrecognized char: %c\n", *cur);\r\nreturn -EINVAL;\r\n}\r\ncur += strlen(suffix);\r\nif (*cur != ' ' && *cur != '\0') {\r\npr_warn("crashkernel: unrecognized char: %c\n", *cur);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic __init char *get_last_crashkernel(char *cmdline,\r\nconst char *name,\r\nconst char *suffix)\r\n{\r\nchar *p = cmdline, *ck_cmdline = NULL;\r\np = strstr(p, name);\r\nwhile (p) {\r\nchar *end_p = strchr(p, ' ');\r\nchar *q;\r\nif (!end_p)\r\nend_p = p + strlen(p);\r\nif (!suffix) {\r\nint i;\r\nfor (i = 0; suffix_tbl[i]; i++) {\r\nq = end_p - strlen(suffix_tbl[i]);\r\nif (!strncmp(q, suffix_tbl[i],\r\nstrlen(suffix_tbl[i])))\r\ngoto next;\r\n}\r\nck_cmdline = p;\r\n} else {\r\nq = end_p - strlen(suffix);\r\nif (!strncmp(q, suffix, strlen(suffix)))\r\nck_cmdline = p;\r\n}\r\nnext:\r\np = strstr(p+1, name);\r\n}\r\nif (!ck_cmdline)\r\nreturn NULL;\r\nreturn ck_cmdline;\r\n}\r\nstatic int __init __parse_crashkernel(char *cmdline,\r\nunsigned long long system_ram,\r\nunsigned long long *crash_size,\r\nunsigned long long *crash_base,\r\nconst char *name,\r\nconst char *suffix)\r\n{\r\nchar *first_colon, *first_space;\r\nchar *ck_cmdline;\r\nBUG_ON(!crash_size || !crash_base);\r\n*crash_size = 0;\r\n*crash_base = 0;\r\nck_cmdline = get_last_crashkernel(cmdline, name, suffix);\r\nif (!ck_cmdline)\r\nreturn -EINVAL;\r\nck_cmdline += strlen(name);\r\nif (suffix)\r\nreturn parse_crashkernel_suffix(ck_cmdline, crash_size,\r\nsuffix);\r\nfirst_colon = strchr(ck_cmdline, ':');\r\nfirst_space = strchr(ck_cmdline, ' ');\r\nif (first_colon && (!first_space || first_colon < first_space))\r\nreturn parse_crashkernel_mem(ck_cmdline, system_ram,\r\ncrash_size, crash_base);\r\nreturn parse_crashkernel_simple(ck_cmdline, crash_size, crash_base);\r\n}\r\nint __init parse_crashkernel(char *cmdline,\r\nunsigned long long system_ram,\r\nunsigned long long *crash_size,\r\nunsigned long long *crash_base)\r\n{\r\nreturn __parse_crashkernel(cmdline, system_ram, crash_size, crash_base,\r\n"crashkernel=", NULL);\r\n}\r\nint __init parse_crashkernel_high(char *cmdline,\r\nunsigned long long system_ram,\r\nunsigned long long *crash_size,\r\nunsigned long long *crash_base)\r\n{\r\nreturn __parse_crashkernel(cmdline, system_ram, crash_size, crash_base,\r\n"crashkernel=", suffix_tbl[SUFFIX_HIGH]);\r\n}\r\nint __init parse_crashkernel_low(char *cmdline,\r\nunsigned long long system_ram,\r\nunsigned long long *crash_size,\r\nunsigned long long *crash_base)\r\n{\r\nreturn __parse_crashkernel(cmdline, system_ram, crash_size, crash_base,\r\n"crashkernel=", suffix_tbl[SUFFIX_LOW]);\r\n}\r\nstatic void update_vmcoreinfo_note(void)\r\n{\r\nu32 *buf = vmcoreinfo_note;\r\nif (!vmcoreinfo_size)\r\nreturn;\r\nbuf = append_elf_note(buf, VMCOREINFO_NOTE_NAME, 0, vmcoreinfo_data,\r\nvmcoreinfo_size);\r\nfinal_note(buf);\r\n}\r\nvoid crash_save_vmcoreinfo(void)\r\n{\r\nvmcoreinfo_append_str("CRASHTIME=%ld\n", get_seconds());\r\nupdate_vmcoreinfo_note();\r\n}\r\nvoid vmcoreinfo_append_str(const char *fmt, ...)\r\n{\r\nva_list args;\r\nchar buf[0x50];\r\nsize_t r;\r\nva_start(args, fmt);\r\nr = vscnprintf(buf, sizeof(buf), fmt, args);\r\nva_end(args);\r\nr = min(r, vmcoreinfo_max_size - vmcoreinfo_size);\r\nmemcpy(&vmcoreinfo_data[vmcoreinfo_size], buf, r);\r\nvmcoreinfo_size += r;\r\n}\r\nvoid __weak arch_crash_save_vmcoreinfo(void)\r\n{}\r\nphys_addr_t __weak paddr_vmcoreinfo_note(void)\r\n{\r\nreturn __pa((unsigned long)(char *)&vmcoreinfo_note);\r\n}\r\nstatic int __init crash_save_vmcoreinfo_init(void)\r\n{\r\nVMCOREINFO_OSRELEASE(init_uts_ns.name.release);\r\nVMCOREINFO_PAGESIZE(PAGE_SIZE);\r\nVMCOREINFO_SYMBOL(init_uts_ns);\r\nVMCOREINFO_SYMBOL(node_online_map);\r\n#ifdef CONFIG_MMU\r\nVMCOREINFO_SYMBOL(swapper_pg_dir);\r\n#endif\r\nVMCOREINFO_SYMBOL(_stext);\r\nVMCOREINFO_SYMBOL(vmap_area_list);\r\n#ifndef CONFIG_NEED_MULTIPLE_NODES\r\nVMCOREINFO_SYMBOL(mem_map);\r\nVMCOREINFO_SYMBOL(contig_page_data);\r\n#endif\r\n#ifdef CONFIG_SPARSEMEM\r\nVMCOREINFO_SYMBOL(mem_section);\r\nVMCOREINFO_LENGTH(mem_section, NR_SECTION_ROOTS);\r\nVMCOREINFO_STRUCT_SIZE(mem_section);\r\nVMCOREINFO_OFFSET(mem_section, section_mem_map);\r\n#endif\r\nVMCOREINFO_STRUCT_SIZE(page);\r\nVMCOREINFO_STRUCT_SIZE(pglist_data);\r\nVMCOREINFO_STRUCT_SIZE(zone);\r\nVMCOREINFO_STRUCT_SIZE(free_area);\r\nVMCOREINFO_STRUCT_SIZE(list_head);\r\nVMCOREINFO_SIZE(nodemask_t);\r\nVMCOREINFO_OFFSET(page, flags);\r\nVMCOREINFO_OFFSET(page, _refcount);\r\nVMCOREINFO_OFFSET(page, mapping);\r\nVMCOREINFO_OFFSET(page, lru);\r\nVMCOREINFO_OFFSET(page, _mapcount);\r\nVMCOREINFO_OFFSET(page, private);\r\nVMCOREINFO_OFFSET(page, compound_dtor);\r\nVMCOREINFO_OFFSET(page, compound_order);\r\nVMCOREINFO_OFFSET(page, compound_head);\r\nVMCOREINFO_OFFSET(pglist_data, node_zones);\r\nVMCOREINFO_OFFSET(pglist_data, nr_zones);\r\n#ifdef CONFIG_FLAT_NODE_MEM_MAP\r\nVMCOREINFO_OFFSET(pglist_data, node_mem_map);\r\n#endif\r\nVMCOREINFO_OFFSET(pglist_data, node_start_pfn);\r\nVMCOREINFO_OFFSET(pglist_data, node_spanned_pages);\r\nVMCOREINFO_OFFSET(pglist_data, node_id);\r\nVMCOREINFO_OFFSET(zone, free_area);\r\nVMCOREINFO_OFFSET(zone, vm_stat);\r\nVMCOREINFO_OFFSET(zone, spanned_pages);\r\nVMCOREINFO_OFFSET(free_area, free_list);\r\nVMCOREINFO_OFFSET(list_head, next);\r\nVMCOREINFO_OFFSET(list_head, prev);\r\nVMCOREINFO_OFFSET(vmap_area, va_start);\r\nVMCOREINFO_OFFSET(vmap_area, list);\r\nVMCOREINFO_LENGTH(zone.free_area, MAX_ORDER);\r\nlog_buf_kexec_setup();\r\nVMCOREINFO_LENGTH(free_area.free_list, MIGRATE_TYPES);\r\nVMCOREINFO_NUMBER(NR_FREE_PAGES);\r\nVMCOREINFO_NUMBER(PG_lru);\r\nVMCOREINFO_NUMBER(PG_private);\r\nVMCOREINFO_NUMBER(PG_swapcache);\r\nVMCOREINFO_NUMBER(PG_slab);\r\n#ifdef CONFIG_MEMORY_FAILURE\r\nVMCOREINFO_NUMBER(PG_hwpoison);\r\n#endif\r\nVMCOREINFO_NUMBER(PG_head_mask);\r\nVMCOREINFO_NUMBER(PAGE_BUDDY_MAPCOUNT_VALUE);\r\n#ifdef CONFIG_X86\r\nVMCOREINFO_NUMBER(KERNEL_IMAGE_SIZE);\r\n#endif\r\n#ifdef CONFIG_HUGETLB_PAGE\r\nVMCOREINFO_NUMBER(HUGETLB_PAGE_DTOR);\r\n#endif\r\narch_crash_save_vmcoreinfo();\r\nupdate_vmcoreinfo_note();\r\nreturn 0;\r\n}\r\nint kernel_kexec(void)\r\n{\r\nint error = 0;\r\nif (!mutex_trylock(&kexec_mutex))\r\nreturn -EBUSY;\r\nif (!kexec_image) {\r\nerror = -EINVAL;\r\ngoto Unlock;\r\n}\r\n#ifdef CONFIG_KEXEC_JUMP\r\nif (kexec_image->preserve_context) {\r\nlock_system_sleep();\r\npm_prepare_console();\r\nerror = freeze_processes();\r\nif (error) {\r\nerror = -EBUSY;\r\ngoto Restore_console;\r\n}\r\nsuspend_console();\r\nerror = dpm_suspend_start(PMSG_FREEZE);\r\nif (error)\r\ngoto Resume_console;\r\nerror = dpm_suspend_end(PMSG_FREEZE);\r\nif (error)\r\ngoto Resume_devices;\r\nerror = disable_nonboot_cpus();\r\nif (error)\r\ngoto Enable_cpus;\r\nlocal_irq_disable();\r\nerror = syscore_suspend();\r\nif (error)\r\ngoto Enable_irqs;\r\n} else\r\n#endif\r\n{\r\nkexec_in_progress = true;\r\nkernel_restart_prepare(NULL);\r\nmigrate_to_reboot_cpu();\r\ncpu_hotplug_enable();\r\npr_emerg("Starting new kernel\n");\r\nmachine_shutdown();\r\n}\r\nmachine_kexec(kexec_image);\r\n#ifdef CONFIG_KEXEC_JUMP\r\nif (kexec_image->preserve_context) {\r\nsyscore_resume();\r\nEnable_irqs:\r\nlocal_irq_enable();\r\nEnable_cpus:\r\nenable_nonboot_cpus();\r\ndpm_resume_start(PMSG_RESTORE);\r\nResume_devices:\r\ndpm_resume_end(PMSG_RESTORE);\r\nResume_console:\r\nresume_console();\r\nthaw_processes();\r\nRestore_console:\r\npm_restore_console();\r\nunlock_system_sleep();\r\n}\r\n#endif\r\nUnlock:\r\nmutex_unlock(&kexec_mutex);\r\nreturn error;\r\n}\r\nvoid __weak arch_kexec_protect_crashkres(void)\r\n{}\r\nvoid __weak arch_kexec_unprotect_crashkres(void)\r\n{}
