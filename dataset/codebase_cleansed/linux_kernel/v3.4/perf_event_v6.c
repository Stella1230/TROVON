static inline unsigned long\r\narmv6_pmcr_read(void)\r\n{\r\nu32 val;\r\nasm volatile("mrc p15, 0, %0, c15, c12, 0" : "=r"(val));\r\nreturn val;\r\n}\r\nstatic inline void\r\narmv6_pmcr_write(unsigned long val)\r\n{\r\nasm volatile("mcr p15, 0, %0, c15, c12, 0" : : "r"(val));\r\n}\r\nstatic inline int\r\narmv6_pmcr_has_overflowed(unsigned long pmcr)\r\n{\r\nreturn pmcr & ARMV6_PMCR_OVERFLOWED_MASK;\r\n}\r\nstatic inline int\r\narmv6_pmcr_counter_has_overflowed(unsigned long pmcr,\r\nenum armv6_counters counter)\r\n{\r\nint ret = 0;\r\nif (ARMV6_CYCLE_COUNTER == counter)\r\nret = pmcr & ARMV6_PMCR_CCOUNT_OVERFLOW;\r\nelse if (ARMV6_COUNTER0 == counter)\r\nret = pmcr & ARMV6_PMCR_COUNT0_OVERFLOW;\r\nelse if (ARMV6_COUNTER1 == counter)\r\nret = pmcr & ARMV6_PMCR_COUNT1_OVERFLOW;\r\nelse\r\nWARN_ONCE(1, "invalid counter number (%d)\n", counter);\r\nreturn ret;\r\n}\r\nstatic inline u32\r\narmv6pmu_read_counter(int counter)\r\n{\r\nunsigned long value = 0;\r\nif (ARMV6_CYCLE_COUNTER == counter)\r\nasm volatile("mrc p15, 0, %0, c15, c12, 1" : "=r"(value));\r\nelse if (ARMV6_COUNTER0 == counter)\r\nasm volatile("mrc p15, 0, %0, c15, c12, 2" : "=r"(value));\r\nelse if (ARMV6_COUNTER1 == counter)\r\nasm volatile("mrc p15, 0, %0, c15, c12, 3" : "=r"(value));\r\nelse\r\nWARN_ONCE(1, "invalid counter number (%d)\n", counter);\r\nreturn value;\r\n}\r\nstatic inline void\r\narmv6pmu_write_counter(int counter,\r\nu32 value)\r\n{\r\nif (ARMV6_CYCLE_COUNTER == counter)\r\nasm volatile("mcr p15, 0, %0, c15, c12, 1" : : "r"(value));\r\nelse if (ARMV6_COUNTER0 == counter)\r\nasm volatile("mcr p15, 0, %0, c15, c12, 2" : : "r"(value));\r\nelse if (ARMV6_COUNTER1 == counter)\r\nasm volatile("mcr p15, 0, %0, c15, c12, 3" : : "r"(value));\r\nelse\r\nWARN_ONCE(1, "invalid counter number (%d)\n", counter);\r\n}\r\nstatic void\r\narmv6pmu_enable_event(struct hw_perf_event *hwc,\r\nint idx)\r\n{\r\nunsigned long val, mask, evt, flags;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nif (ARMV6_CYCLE_COUNTER == idx) {\r\nmask = 0;\r\nevt = ARMV6_PMCR_CCOUNT_IEN;\r\n} else if (ARMV6_COUNTER0 == idx) {\r\nmask = ARMV6_PMCR_EVT_COUNT0_MASK;\r\nevt = (hwc->config_base << ARMV6_PMCR_EVT_COUNT0_SHIFT) |\r\nARMV6_PMCR_COUNT0_IEN;\r\n} else if (ARMV6_COUNTER1 == idx) {\r\nmask = ARMV6_PMCR_EVT_COUNT1_MASK;\r\nevt = (hwc->config_base << ARMV6_PMCR_EVT_COUNT1_SHIFT) |\r\nARMV6_PMCR_COUNT1_IEN;\r\n} else {\r\nWARN_ONCE(1, "invalid counter number (%d)\n", idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = armv6_pmcr_read();\r\nval &= ~mask;\r\nval |= evt;\r\narmv6_pmcr_write(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic irqreturn_t\r\narmv6pmu_handle_irq(int irq_num,\r\nvoid *dev)\r\n{\r\nunsigned long pmcr = armv6_pmcr_read();\r\nstruct perf_sample_data data;\r\nstruct pmu_hw_events *cpuc;\r\nstruct pt_regs *regs;\r\nint idx;\r\nif (!armv6_pmcr_has_overflowed(pmcr))\r\nreturn IRQ_NONE;\r\nregs = get_irq_regs();\r\narmv6_pmcr_write(pmcr);\r\nperf_sample_data_init(&data, 0);\r\ncpuc = &__get_cpu_var(cpu_hw_events);\r\nfor (idx = 0; idx < cpu_pmu->num_events; ++idx) {\r\nstruct perf_event *event = cpuc->events[idx];\r\nstruct hw_perf_event *hwc;\r\nif (!event)\r\ncontinue;\r\nif (!armv6_pmcr_counter_has_overflowed(pmcr, idx))\r\ncontinue;\r\nhwc = &event->hw;\r\narmpmu_event_update(event, hwc, idx);\r\ndata.period = event->hw.last_period;\r\nif (!armpmu_event_set_period(event, hwc, idx))\r\ncontinue;\r\nif (perf_event_overflow(event, &data, regs))\r\ncpu_pmu->disable(hwc, idx);\r\n}\r\nirq_work_run();\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void\r\narmv6pmu_start(void)\r\n{\r\nunsigned long flags, val;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = armv6_pmcr_read();\r\nval |= ARMV6_PMCR_ENABLE;\r\narmv6_pmcr_write(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void\r\narmv6pmu_stop(void)\r\n{\r\nunsigned long flags, val;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = armv6_pmcr_read();\r\nval &= ~ARMV6_PMCR_ENABLE;\r\narmv6_pmcr_write(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic int\r\narmv6pmu_get_event_idx(struct pmu_hw_events *cpuc,\r\nstruct hw_perf_event *event)\r\n{\r\nif (ARMV6_PERFCTR_CPU_CYCLES == event->config_base) {\r\nif (test_and_set_bit(ARMV6_CYCLE_COUNTER, cpuc->used_mask))\r\nreturn -EAGAIN;\r\nreturn ARMV6_CYCLE_COUNTER;\r\n} else {\r\nif (!test_and_set_bit(ARMV6_COUNTER1, cpuc->used_mask))\r\nreturn ARMV6_COUNTER1;\r\nif (!test_and_set_bit(ARMV6_COUNTER0, cpuc->used_mask))\r\nreturn ARMV6_COUNTER0;\r\nreturn -EAGAIN;\r\n}\r\n}\r\nstatic void\r\narmv6pmu_disable_event(struct hw_perf_event *hwc,\r\nint idx)\r\n{\r\nunsigned long val, mask, evt, flags;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nif (ARMV6_CYCLE_COUNTER == idx) {\r\nmask = ARMV6_PMCR_CCOUNT_IEN;\r\nevt = 0;\r\n} else if (ARMV6_COUNTER0 == idx) {\r\nmask = ARMV6_PMCR_COUNT0_IEN | ARMV6_PMCR_EVT_COUNT0_MASK;\r\nevt = ARMV6_PERFCTR_NOP << ARMV6_PMCR_EVT_COUNT0_SHIFT;\r\n} else if (ARMV6_COUNTER1 == idx) {\r\nmask = ARMV6_PMCR_COUNT1_IEN | ARMV6_PMCR_EVT_COUNT1_MASK;\r\nevt = ARMV6_PERFCTR_NOP << ARMV6_PMCR_EVT_COUNT1_SHIFT;\r\n} else {\r\nWARN_ONCE(1, "invalid counter number (%d)\n", idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = armv6_pmcr_read();\r\nval &= ~mask;\r\nval |= evt;\r\narmv6_pmcr_write(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic void\r\narmv6mpcore_pmu_disable_event(struct hw_perf_event *hwc,\r\nint idx)\r\n{\r\nunsigned long val, mask, flags, evt = 0;\r\nstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\r\nif (ARMV6_CYCLE_COUNTER == idx) {\r\nmask = ARMV6_PMCR_CCOUNT_IEN;\r\n} else if (ARMV6_COUNTER0 == idx) {\r\nmask = ARMV6_PMCR_COUNT0_IEN;\r\n} else if (ARMV6_COUNTER1 == idx) {\r\nmask = ARMV6_PMCR_COUNT1_IEN;\r\n} else {\r\nWARN_ONCE(1, "invalid counter number (%d)\n", idx);\r\nreturn;\r\n}\r\nraw_spin_lock_irqsave(&events->pmu_lock, flags);\r\nval = armv6_pmcr_read();\r\nval &= ~mask;\r\nval |= evt;\r\narmv6_pmcr_write(val);\r\nraw_spin_unlock_irqrestore(&events->pmu_lock, flags);\r\n}\r\nstatic int armv6_map_event(struct perf_event *event)\r\n{\r\nreturn map_cpu_event(event, &armv6_perf_map,\r\n&armv6_perf_cache_map, 0xFF);\r\n}\r\nstatic struct arm_pmu *__init armv6pmu_init(void)\r\n{\r\nreturn &armv6pmu;\r\n}\r\nstatic int armv6mpcore_map_event(struct perf_event *event)\r\n{\r\nreturn map_cpu_event(event, &armv6mpcore_perf_map,\r\n&armv6mpcore_perf_cache_map, 0xFF);\r\n}\r\nstatic struct arm_pmu *__init armv6mpcore_pmu_init(void)\r\n{\r\nreturn &armv6mpcore_pmu;\r\n}\r\nstatic struct arm_pmu *__init armv6pmu_init(void)\r\n{\r\nreturn NULL;\r\n}\r\nstatic struct arm_pmu *__init armv6mpcore_pmu_init(void)\r\n{\r\nreturn NULL;\r\n}
