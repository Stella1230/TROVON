static int enh_desc_get_tx_status(void *data, struct stmmac_extra_stats *x,\r\nstruct dma_desc *p, void __iomem *ioaddr)\r\n{\r\nint ret = 0;\r\nstruct net_device_stats *stats = (struct net_device_stats *)data;\r\nif (unlikely(p->des01.etx.error_summary)) {\r\nCHIP_DBG(KERN_ERR "GMAC TX error... 0x%08x\n", p->des01.etx);\r\nif (unlikely(p->des01.etx.jabber_timeout)) {\r\nCHIP_DBG(KERN_ERR "\tjabber_timeout error\n");\r\nx->tx_jabber++;\r\n}\r\nif (unlikely(p->des01.etx.frame_flushed)) {\r\nCHIP_DBG(KERN_ERR "\tframe_flushed error\n");\r\nx->tx_frame_flushed++;\r\ndwmac_dma_flush_tx_fifo(ioaddr);\r\n}\r\nif (unlikely(p->des01.etx.loss_carrier)) {\r\nCHIP_DBG(KERN_ERR "\tloss_carrier error\n");\r\nx->tx_losscarrier++;\r\nstats->tx_carrier_errors++;\r\n}\r\nif (unlikely(p->des01.etx.no_carrier)) {\r\nCHIP_DBG(KERN_ERR "\tno_carrier error\n");\r\nx->tx_carrier++;\r\nstats->tx_carrier_errors++;\r\n}\r\nif (unlikely(p->des01.etx.late_collision)) {\r\nCHIP_DBG(KERN_ERR "\tlate_collision error\n");\r\nstats->collisions += p->des01.etx.collision_count;\r\n}\r\nif (unlikely(p->des01.etx.excessive_collisions)) {\r\nCHIP_DBG(KERN_ERR "\texcessive_collisions\n");\r\nstats->collisions += p->des01.etx.collision_count;\r\n}\r\nif (unlikely(p->des01.etx.excessive_deferral)) {\r\nCHIP_DBG(KERN_INFO "\texcessive tx_deferral\n");\r\nx->tx_deferred++;\r\n}\r\nif (unlikely(p->des01.etx.underflow_error)) {\r\nCHIP_DBG(KERN_ERR "\tunderflow error\n");\r\ndwmac_dma_flush_tx_fifo(ioaddr);\r\nx->tx_underflow++;\r\n}\r\nif (unlikely(p->des01.etx.ip_header_error)) {\r\nCHIP_DBG(KERN_ERR "\tTX IP header csum error\n");\r\nx->tx_ip_header_error++;\r\n}\r\nif (unlikely(p->des01.etx.payload_error)) {\r\nCHIP_DBG(KERN_ERR "\tAddr/Payload csum error\n");\r\nx->tx_payload_error++;\r\ndwmac_dma_flush_tx_fifo(ioaddr);\r\n}\r\nret = -1;\r\n}\r\nif (unlikely(p->des01.etx.deferred)) {\r\nCHIP_DBG(KERN_INFO "GMAC TX status: tx deferred\n");\r\nx->tx_deferred++;\r\n}\r\n#ifdef STMMAC_VLAN_TAG_USED\r\nif (p->des01.etx.vlan_frame) {\r\nCHIP_DBG(KERN_INFO "GMAC TX status: VLAN frame\n");\r\nx->tx_vlan++;\r\n}\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int enh_desc_get_tx_len(struct dma_desc *p)\r\n{\r\nreturn p->des01.etx.buffer1_size;\r\n}\r\nstatic int enh_desc_coe_rdes0(int ipc_err, int type, int payload_err)\r\n{\r\nint ret = good_frame;\r\nu32 status = (type << 2 | ipc_err << 1 | payload_err) & 0x7;\r\nif (status == 0x0) {\r\nCHIP_DBG(KERN_INFO "RX Des0 status: IEEE 802.3 Type frame.\n");\r\nret = llc_snap;\r\n} else if (status == 0x4) {\r\nCHIP_DBG(KERN_INFO "RX Des0 status: IPv4/6 No CSUM errorS.\n");\r\nret = good_frame;\r\n} else if (status == 0x5) {\r\nCHIP_DBG(KERN_ERR "RX Des0 status: IPv4/6 Payload Error.\n");\r\nret = csum_none;\r\n} else if (status == 0x6) {\r\nCHIP_DBG(KERN_ERR "RX Des0 status: IPv4/6 Header Error.\n");\r\nret = csum_none;\r\n} else if (status == 0x7) {\r\nCHIP_DBG(KERN_ERR\r\n"RX Des0 status: IPv4/6 Header and Payload Error.\n");\r\nret = csum_none;\r\n} else if (status == 0x1) {\r\nCHIP_DBG(KERN_ERR\r\n"RX Des0 status: IPv4/6 unsupported IP PAYLOAD.\n");\r\nret = discard_frame;\r\n} else if (status == 0x3) {\r\nCHIP_DBG(KERN_ERR "RX Des0 status: No IPv4, IPv6 frame.\n");\r\nret = discard_frame;\r\n}\r\nreturn ret;\r\n}\r\nstatic int enh_desc_get_rx_status(void *data, struct stmmac_extra_stats *x,\r\nstruct dma_desc *p)\r\n{\r\nint ret = good_frame;\r\nstruct net_device_stats *stats = (struct net_device_stats *)data;\r\nif (unlikely(p->des01.erx.error_summary)) {\r\nCHIP_DBG(KERN_ERR "GMAC RX Error Summary 0x%08x\n",\r\np->des01.erx);\r\nif (unlikely(p->des01.erx.descriptor_error)) {\r\nCHIP_DBG(KERN_ERR "\tdescriptor error\n");\r\nx->rx_desc++;\r\nstats->rx_length_errors++;\r\n}\r\nif (unlikely(p->des01.erx.overflow_error)) {\r\nCHIP_DBG(KERN_ERR "\toverflow error\n");\r\nx->rx_gmac_overflow++;\r\n}\r\nif (unlikely(p->des01.erx.ipc_csum_error))\r\nCHIP_DBG(KERN_ERR "\tIPC Csum Error/Giant frame\n");\r\nif (unlikely(p->des01.erx.late_collision)) {\r\nCHIP_DBG(KERN_ERR "\tlate_collision error\n");\r\nstats->collisions++;\r\nstats->collisions++;\r\n}\r\nif (unlikely(p->des01.erx.receive_watchdog)) {\r\nCHIP_DBG(KERN_ERR "\treceive_watchdog error\n");\r\nx->rx_watchdog++;\r\n}\r\nif (unlikely(p->des01.erx.error_gmii)) {\r\nCHIP_DBG(KERN_ERR "\tReceive Error\n");\r\nx->rx_mii++;\r\n}\r\nif (unlikely(p->des01.erx.crc_error)) {\r\nCHIP_DBG(KERN_ERR "\tCRC error\n");\r\nx->rx_crc++;\r\nstats->rx_crc_errors++;\r\n}\r\nret = discard_frame;\r\n}\r\nret = enh_desc_coe_rdes0(p->des01.erx.ipc_csum_error,\r\np->des01.erx.frame_type, p->des01.erx.payload_csum_error);\r\nif (unlikely(p->des01.erx.dribbling)) {\r\nCHIP_DBG(KERN_ERR "GMAC RX: dribbling error\n");\r\nx->dribbling_bit++;\r\n}\r\nif (unlikely(p->des01.erx.sa_filter_fail)) {\r\nCHIP_DBG(KERN_ERR "GMAC RX : Source Address filter fail\n");\r\nx->sa_rx_filter_fail++;\r\nret = discard_frame;\r\n}\r\nif (unlikely(p->des01.erx.da_filter_fail)) {\r\nCHIP_DBG(KERN_ERR "GMAC RX : Dest Address filter fail\n");\r\nx->da_rx_filter_fail++;\r\nret = discard_frame;\r\n}\r\nif (unlikely(p->des01.erx.length_error)) {\r\nCHIP_DBG(KERN_ERR "GMAC RX: length_error error\n");\r\nx->rx_length++;\r\nret = discard_frame;\r\n}\r\n#ifdef STMMAC_VLAN_TAG_USED\r\nif (p->des01.erx.vlan_tag) {\r\nCHIP_DBG(KERN_INFO "GMAC RX: VLAN frame tagged\n");\r\nx->rx_vlan++;\r\n}\r\n#endif\r\nreturn ret;\r\n}\r\nstatic void enh_desc_init_rx_desc(struct dma_desc *p, unsigned int ring_size,\r\nint disable_rx_ic)\r\n{\r\nint i;\r\nfor (i = 0; i < ring_size; i++) {\r\np->des01.erx.own = 1;\r\np->des01.erx.buffer1_size = BUF_SIZE_8KiB - 1;\r\nehn_desc_rx_set_on_ring_chain(p, (i == ring_size - 1));\r\nif (disable_rx_ic)\r\np->des01.erx.disable_ic = 1;\r\np++;\r\n}\r\n}\r\nstatic void enh_desc_init_tx_desc(struct dma_desc *p, unsigned int ring_size)\r\n{\r\nint i;\r\nfor (i = 0; i < ring_size; i++) {\r\np->des01.etx.own = 0;\r\nehn_desc_tx_set_on_ring_chain(p, (i == ring_size - 1));\r\np++;\r\n}\r\n}\r\nstatic int enh_desc_get_tx_owner(struct dma_desc *p)\r\n{\r\nreturn p->des01.etx.own;\r\n}\r\nstatic int enh_desc_get_rx_owner(struct dma_desc *p)\r\n{\r\nreturn p->des01.erx.own;\r\n}\r\nstatic void enh_desc_set_tx_owner(struct dma_desc *p)\r\n{\r\np->des01.etx.own = 1;\r\n}\r\nstatic void enh_desc_set_rx_owner(struct dma_desc *p)\r\n{\r\np->des01.erx.own = 1;\r\n}\r\nstatic int enh_desc_get_tx_ls(struct dma_desc *p)\r\n{\r\nreturn p->des01.etx.last_segment;\r\n}\r\nstatic void enh_desc_release_tx_desc(struct dma_desc *p)\r\n{\r\nint ter = p->des01.etx.end_ring;\r\nmemset(p, 0, offsetof(struct dma_desc, des2));\r\nenh_desc_end_tx_desc(p, ter);\r\n}\r\nstatic void enh_desc_prepare_tx_desc(struct dma_desc *p, int is_fs, int len,\r\nint csum_flag)\r\n{\r\np->des01.etx.first_segment = is_fs;\r\nenh_set_tx_desc_len(p, len);\r\nif (likely(csum_flag))\r\np->des01.etx.checksum_insertion = cic_full;\r\n}\r\nstatic void enh_desc_clear_tx_ic(struct dma_desc *p)\r\n{\r\np->des01.etx.interrupt = 0;\r\n}\r\nstatic void enh_desc_close_tx_desc(struct dma_desc *p)\r\n{\r\np->des01.etx.last_segment = 1;\r\np->des01.etx.interrupt = 1;\r\n}\r\nstatic int enh_desc_get_rx_frame_len(struct dma_desc *p)\r\n{\r\nreturn p->des01.erx.frame_length;\r\n}
