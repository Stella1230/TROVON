static void\r\nnouveau_sgdma_destroy(struct ttm_tt *ttm)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nif (ttm) {\r\nNV_DEBUG(nvbe->dev, "\n");\r\nttm_dma_tt_fini(&nvbe->ttm);\r\nkfree(nvbe);\r\n}\r\n}\r\nstatic int\r\nnv04_sgdma_bind(struct ttm_tt *ttm, struct ttm_mem_reg *mem)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct drm_device *dev = nvbe->dev;\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_gpuobj *gpuobj = dev_priv->gart_info.sg_ctxdma;\r\nunsigned i, j, pte;\r\nNV_DEBUG(dev, "pg=0x%lx\n", mem->start);\r\nnvbe->offset = mem->start << PAGE_SHIFT;\r\npte = (nvbe->offset >> NV_CTXDMA_PAGE_SHIFT) + 2;\r\nfor (i = 0; i < ttm->num_pages; i++) {\r\ndma_addr_t dma_offset = nvbe->ttm.dma_address[i];\r\nuint32_t offset_l = lower_32_bits(dma_offset);\r\nfor (j = 0; j < PAGE_SIZE / NV_CTXDMA_PAGE_SIZE; j++, pte++) {\r\nnv_wo32(gpuobj, (pte * 4) + 0, offset_l | 3);\r\noffset_l += NV_CTXDMA_PAGE_SIZE;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnv04_sgdma_unbind(struct ttm_tt *ttm)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct drm_device *dev = nvbe->dev;\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_gpuobj *gpuobj = dev_priv->gart_info.sg_ctxdma;\r\nunsigned i, j, pte;\r\nNV_DEBUG(dev, "\n");\r\nif (ttm->state != tt_bound)\r\nreturn 0;\r\npte = (nvbe->offset >> NV_CTXDMA_PAGE_SHIFT) + 2;\r\nfor (i = 0; i < ttm->num_pages; i++) {\r\nfor (j = 0; j < PAGE_SIZE / NV_CTXDMA_PAGE_SIZE; j++, pte++)\r\nnv_wo32(gpuobj, (pte * 4) + 0, 0x00000000);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nnv41_sgdma_flush(struct nouveau_sgdma_be *nvbe)\r\n{\r\nstruct drm_device *dev = nvbe->dev;\r\nnv_wr32(dev, 0x100810, 0x00000022);\r\nif (!nv_wait(dev, 0x100810, 0x00000100, 0x00000100))\r\nNV_ERROR(dev, "vm flush timeout: 0x%08x\n",\r\nnv_rd32(dev, 0x100810));\r\nnv_wr32(dev, 0x100810, 0x00000000);\r\n}\r\nstatic int\r\nnv41_sgdma_bind(struct ttm_tt *ttm, struct ttm_mem_reg *mem)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct drm_nouveau_private *dev_priv = nvbe->dev->dev_private;\r\nstruct nouveau_gpuobj *pgt = dev_priv->gart_info.sg_ctxdma;\r\ndma_addr_t *list = nvbe->ttm.dma_address;\r\nu32 pte = mem->start << 2;\r\nu32 cnt = ttm->num_pages;\r\nnvbe->offset = mem->start << PAGE_SHIFT;\r\nwhile (cnt--) {\r\nnv_wo32(pgt, pte, (*list++ >> 7) | 1);\r\npte += 4;\r\n}\r\nnv41_sgdma_flush(nvbe);\r\nreturn 0;\r\n}\r\nstatic int\r\nnv41_sgdma_unbind(struct ttm_tt *ttm)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct drm_nouveau_private *dev_priv = nvbe->dev->dev_private;\r\nstruct nouveau_gpuobj *pgt = dev_priv->gart_info.sg_ctxdma;\r\nu32 pte = (nvbe->offset >> 12) << 2;\r\nu32 cnt = ttm->num_pages;\r\nwhile (cnt--) {\r\nnv_wo32(pgt, pte, 0x00000000);\r\npte += 4;\r\n}\r\nnv41_sgdma_flush(nvbe);\r\nreturn 0;\r\n}\r\nstatic void\r\nnv44_sgdma_flush(struct ttm_tt *ttm)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct drm_device *dev = nvbe->dev;\r\nnv_wr32(dev, 0x100814, (ttm->num_pages - 1) << 12);\r\nnv_wr32(dev, 0x100808, nvbe->offset | 0x20);\r\nif (!nv_wait(dev, 0x100808, 0x00000001, 0x00000001))\r\nNV_ERROR(dev, "gart flush timeout: 0x%08x\n",\r\nnv_rd32(dev, 0x100808));\r\nnv_wr32(dev, 0x100808, 0x00000000);\r\n}\r\nstatic void\r\nnv44_sgdma_fill(struct nouveau_gpuobj *pgt, dma_addr_t *list, u32 base, u32 cnt)\r\n{\r\nstruct drm_nouveau_private *dev_priv = pgt->dev->dev_private;\r\ndma_addr_t dummy = dev_priv->gart_info.dummy.addr;\r\nu32 pte, tmp[4];\r\npte = base >> 2;\r\nbase &= ~0x0000000f;\r\ntmp[0] = nv_ro32(pgt, base + 0x0);\r\ntmp[1] = nv_ro32(pgt, base + 0x4);\r\ntmp[2] = nv_ro32(pgt, base + 0x8);\r\ntmp[3] = nv_ro32(pgt, base + 0xc);\r\nwhile (cnt--) {\r\nu32 addr = list ? (*list++ >> 12) : (dummy >> 12);\r\nswitch (pte++ & 0x3) {\r\ncase 0:\r\ntmp[0] &= ~0x07ffffff;\r\ntmp[0] |= addr;\r\nbreak;\r\ncase 1:\r\ntmp[0] &= ~0xf8000000;\r\ntmp[0] |= addr << 27;\r\ntmp[1] &= ~0x003fffff;\r\ntmp[1] |= addr >> 5;\r\nbreak;\r\ncase 2:\r\ntmp[1] &= ~0xffc00000;\r\ntmp[1] |= addr << 22;\r\ntmp[2] &= ~0x0001ffff;\r\ntmp[2] |= addr >> 10;\r\nbreak;\r\ncase 3:\r\ntmp[2] &= ~0xfffe0000;\r\ntmp[2] |= addr << 17;\r\ntmp[3] &= ~0x00000fff;\r\ntmp[3] |= addr >> 15;\r\nbreak;\r\n}\r\n}\r\ntmp[3] |= 0x40000000;\r\nnv_wo32(pgt, base + 0x0, tmp[0]);\r\nnv_wo32(pgt, base + 0x4, tmp[1]);\r\nnv_wo32(pgt, base + 0x8, tmp[2]);\r\nnv_wo32(pgt, base + 0xc, tmp[3]);\r\n}\r\nstatic int\r\nnv44_sgdma_bind(struct ttm_tt *ttm, struct ttm_mem_reg *mem)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct drm_nouveau_private *dev_priv = nvbe->dev->dev_private;\r\nstruct nouveau_gpuobj *pgt = dev_priv->gart_info.sg_ctxdma;\r\ndma_addr_t *list = nvbe->ttm.dma_address;\r\nu32 pte = mem->start << 2, tmp[4];\r\nu32 cnt = ttm->num_pages;\r\nint i;\r\nnvbe->offset = mem->start << PAGE_SHIFT;\r\nif (pte & 0x0000000c) {\r\nu32 max = 4 - ((pte >> 2) & 0x3);\r\nu32 part = (cnt > max) ? max : cnt;\r\nnv44_sgdma_fill(pgt, list, pte, part);\r\npte += (part << 2);\r\nlist += part;\r\ncnt -= part;\r\n}\r\nwhile (cnt >= 4) {\r\nfor (i = 0; i < 4; i++)\r\ntmp[i] = *list++ >> 12;\r\nnv_wo32(pgt, pte + 0x0, tmp[0] >> 0 | tmp[1] << 27);\r\nnv_wo32(pgt, pte + 0x4, tmp[1] >> 5 | tmp[2] << 22);\r\nnv_wo32(pgt, pte + 0x8, tmp[2] >> 10 | tmp[3] << 17);\r\nnv_wo32(pgt, pte + 0xc, tmp[3] >> 15 | 0x40000000);\r\npte += 0x10;\r\ncnt -= 4;\r\n}\r\nif (cnt)\r\nnv44_sgdma_fill(pgt, list, pte, cnt);\r\nnv44_sgdma_flush(ttm);\r\nreturn 0;\r\n}\r\nstatic int\r\nnv44_sgdma_unbind(struct ttm_tt *ttm)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct drm_nouveau_private *dev_priv = nvbe->dev->dev_private;\r\nstruct nouveau_gpuobj *pgt = dev_priv->gart_info.sg_ctxdma;\r\nu32 pte = (nvbe->offset >> 12) << 2;\r\nu32 cnt = ttm->num_pages;\r\nif (pte & 0x0000000c) {\r\nu32 max = 4 - ((pte >> 2) & 0x3);\r\nu32 part = (cnt > max) ? max : cnt;\r\nnv44_sgdma_fill(pgt, NULL, pte, part);\r\npte += (part << 2);\r\ncnt -= part;\r\n}\r\nwhile (cnt >= 4) {\r\nnv_wo32(pgt, pte + 0x0, 0x00000000);\r\nnv_wo32(pgt, pte + 0x4, 0x00000000);\r\nnv_wo32(pgt, pte + 0x8, 0x00000000);\r\nnv_wo32(pgt, pte + 0xc, 0x00000000);\r\npte += 0x10;\r\ncnt -= 4;\r\n}\r\nif (cnt)\r\nnv44_sgdma_fill(pgt, NULL, pte, cnt);\r\nnv44_sgdma_flush(ttm);\r\nreturn 0;\r\n}\r\nstatic int\r\nnv50_sgdma_bind(struct ttm_tt *ttm, struct ttm_mem_reg *mem)\r\n{\r\nstruct nouveau_sgdma_be *nvbe = (struct nouveau_sgdma_be *)ttm;\r\nstruct nouveau_mem *node = mem->mm_node;\r\nnode->pages = nvbe->ttm.dma_address;\r\nreturn 0;\r\n}\r\nstatic int\r\nnv50_sgdma_unbind(struct ttm_tt *ttm)\r\n{\r\nreturn 0;\r\n}\r\nstruct ttm_tt *\r\nnouveau_sgdma_create_ttm(struct ttm_bo_device *bdev,\r\nunsigned long size, uint32_t page_flags,\r\nstruct page *dummy_read_page)\r\n{\r\nstruct drm_nouveau_private *dev_priv = nouveau_bdev(bdev);\r\nstruct drm_device *dev = dev_priv->dev;\r\nstruct nouveau_sgdma_be *nvbe;\r\nnvbe = kzalloc(sizeof(*nvbe), GFP_KERNEL);\r\nif (!nvbe)\r\nreturn NULL;\r\nnvbe->dev = dev;\r\nnvbe->ttm.ttm.func = dev_priv->gart_info.func;\r\nif (ttm_dma_tt_init(&nvbe->ttm, bdev, size, page_flags, dummy_read_page)) {\r\nkfree(nvbe);\r\nreturn NULL;\r\n}\r\nreturn &nvbe->ttm.ttm;\r\n}\r\nint\r\nnouveau_sgdma_init(struct drm_device *dev)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_gpuobj *gpuobj = NULL;\r\nu32 aper_size, align;\r\nint ret;\r\nif (dev_priv->card_type >= NV_40 && pci_is_pcie(dev->pdev))\r\naper_size = 512 * 1024 * 1024;\r\nelse\r\naper_size = 64 * 1024 * 1024;\r\ndev_priv->gart_info.dummy.page = alloc_page(GFP_DMA32 | GFP_KERNEL);\r\nif (!dev_priv->gart_info.dummy.page)\r\nreturn -ENOMEM;\r\ndev_priv->gart_info.dummy.addr =\r\npci_map_page(dev->pdev, dev_priv->gart_info.dummy.page,\r\n0, PAGE_SIZE, PCI_DMA_BIDIRECTIONAL);\r\nif (pci_dma_mapping_error(dev->pdev, dev_priv->gart_info.dummy.addr)) {\r\nNV_ERROR(dev, "error mapping dummy page\n");\r\n__free_page(dev_priv->gart_info.dummy.page);\r\ndev_priv->gart_info.dummy.page = NULL;\r\nreturn -ENOMEM;\r\n}\r\nif (dev_priv->card_type >= NV_50) {\r\ndev_priv->gart_info.aper_base = 0;\r\ndev_priv->gart_info.aper_size = aper_size;\r\ndev_priv->gart_info.type = NOUVEAU_GART_HW;\r\ndev_priv->gart_info.func = &nv50_sgdma_backend;\r\n} else\r\nif (0 && pci_is_pcie(dev->pdev) &&\r\ndev_priv->chipset > 0x40 && dev_priv->chipset != 0x45) {\r\nif (nv44_graph_class(dev)) {\r\ndev_priv->gart_info.func = &nv44_sgdma_backend;\r\nalign = 512 * 1024;\r\n} else {\r\ndev_priv->gart_info.func = &nv41_sgdma_backend;\r\nalign = 16;\r\n}\r\nret = nouveau_gpuobj_new(dev, NULL, aper_size / 1024, align,\r\nNVOBJ_FLAG_ZERO_ALLOC |\r\nNVOBJ_FLAG_ZERO_FREE, &gpuobj);\r\nif (ret) {\r\nNV_ERROR(dev, "Error creating sgdma object: %d\n", ret);\r\nreturn ret;\r\n}\r\ndev_priv->gart_info.sg_ctxdma = gpuobj;\r\ndev_priv->gart_info.aper_base = 0;\r\ndev_priv->gart_info.aper_size = aper_size;\r\ndev_priv->gart_info.type = NOUVEAU_GART_HW;\r\n} else {\r\nret = nouveau_gpuobj_new(dev, NULL, (aper_size / 1024) + 8, 16,\r\nNVOBJ_FLAG_ZERO_ALLOC |\r\nNVOBJ_FLAG_ZERO_FREE, &gpuobj);\r\nif (ret) {\r\nNV_ERROR(dev, "Error creating sgdma object: %d\n", ret);\r\nreturn ret;\r\n}\r\nnv_wo32(gpuobj, 0, NV_CLASS_DMA_IN_MEMORY |\r\n(1 << 12) |\r\n(0 << 13) |\r\n(0 << 14) |\r\n(2 << 16) );\r\nnv_wo32(gpuobj, 4, aper_size - 1);\r\ndev_priv->gart_info.sg_ctxdma = gpuobj;\r\ndev_priv->gart_info.aper_base = 0;\r\ndev_priv->gart_info.aper_size = aper_size;\r\ndev_priv->gart_info.type = NOUVEAU_GART_PDMA;\r\ndev_priv->gart_info.func = &nv04_sgdma_backend;\r\n}\r\nreturn 0;\r\n}\r\nvoid\r\nnouveau_sgdma_takedown(struct drm_device *dev)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nnouveau_gpuobj_ref(NULL, &dev_priv->gart_info.sg_ctxdma);\r\nif (dev_priv->gart_info.dummy.page) {\r\npci_unmap_page(dev->pdev, dev_priv->gart_info.dummy.addr,\r\nPAGE_SIZE, PCI_DMA_BIDIRECTIONAL);\r\n__free_page(dev_priv->gart_info.dummy.page);\r\ndev_priv->gart_info.dummy.page = NULL;\r\n}\r\n}\r\nuint32_t\r\nnouveau_sgdma_get_physical(struct drm_device *dev, uint32_t offset)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_gpuobj *gpuobj = dev_priv->gart_info.sg_ctxdma;\r\nint pte = (offset >> NV_CTXDMA_PAGE_SHIFT) + 2;\r\nBUG_ON(dev_priv->card_type >= NV_50);\r\nreturn (nv_ro32(gpuobj, 4 * pte) & ~NV_CTXDMA_PAGE_MASK) |\r\n(offset & NV_CTXDMA_PAGE_MASK);\r\n}
