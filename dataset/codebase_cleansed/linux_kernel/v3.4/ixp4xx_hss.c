static inline struct port* dev_to_port(struct net_device *dev)\r\n{\r\nreturn dev_to_hdlc(dev)->priv;\r\n}\r\nstatic inline void memcpy_swab32(u32 *dest, u32 *src, int cnt)\r\n{\r\nint i;\r\nfor (i = 0; i < cnt; i++)\r\ndest[i] = swab32(src[i]);\r\n}\r\nstatic void hss_npe_send(struct port *port, struct msg *msg, const char* what)\r\n{\r\nu32 *val = (u32*)msg;\r\nif (npe_send_message(port->npe, msg, what)) {\r\npr_crit("HSS-%i: unable to send command [%08X:%08X] to %s\n",\r\nport->id, val[0], val[1], npe_name(port->npe));\r\nBUG();\r\n}\r\n}\r\nstatic void hss_config_set_lut(struct port *port)\r\n{\r\nstruct msg msg;\r\nint ch;\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PORT_CONFIG_WRITE;\r\nmsg.hss_port = port->id;\r\nfor (ch = 0; ch < MAX_CHANNELS; ch++) {\r\nmsg.data32 >>= 2;\r\nmsg.data32 |= TDMMAP_HDLC << 30;\r\nif (ch % 16 == 15) {\r\nmsg.index = HSS_CONFIG_TX_LUT + ((ch / 4) & ~3);\r\nhss_npe_send(port, &msg, "HSS_SET_TX_LUT");\r\nmsg.index += HSS_CONFIG_RX_LUT - HSS_CONFIG_TX_LUT;\r\nhss_npe_send(port, &msg, "HSS_SET_RX_LUT");\r\n}\r\n}\r\n}\r\nstatic void hss_config(struct port *port)\r\n{\r\nstruct msg msg;\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PORT_CONFIG_WRITE;\r\nmsg.hss_port = port->id;\r\nmsg.index = HSS_CONFIG_TX_PCR;\r\nmsg.data32 = PCR_FRM_PULSE_DISABLED | PCR_MSB_ENDIAN |\r\nPCR_TX_DATA_ENABLE | PCR_SOF_NO_FBIT;\r\nif (port->clock_type == CLOCK_INT)\r\nmsg.data32 |= PCR_SYNC_CLK_DIR_OUTPUT;\r\nhss_npe_send(port, &msg, "HSS_SET_TX_PCR");\r\nmsg.index = HSS_CONFIG_RX_PCR;\r\nmsg.data32 ^= PCR_TX_DATA_ENABLE | PCR_DCLK_EDGE_RISING;\r\nhss_npe_send(port, &msg, "HSS_SET_RX_PCR");\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PORT_CONFIG_WRITE;\r\nmsg.hss_port = port->id;\r\nmsg.index = HSS_CONFIG_CORE_CR;\r\nmsg.data32 = (port->loopback ? CCR_LOOPBACK : 0) |\r\n(port->id ? CCR_SECOND_HSS : 0);\r\nhss_npe_send(port, &msg, "HSS_SET_CORE_CR");\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PORT_CONFIG_WRITE;\r\nmsg.hss_port = port->id;\r\nmsg.index = HSS_CONFIG_CLOCK_CR;\r\nmsg.data32 = port->clock_reg;\r\nhss_npe_send(port, &msg, "HSS_SET_CLOCK_CR");\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PORT_CONFIG_WRITE;\r\nmsg.hss_port = port->id;\r\nmsg.index = HSS_CONFIG_TX_FCR;\r\nmsg.data16a = FRAME_OFFSET;\r\nmsg.data16b = FRAME_SIZE - 1;\r\nhss_npe_send(port, &msg, "HSS_SET_TX_FCR");\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PORT_CONFIG_WRITE;\r\nmsg.hss_port = port->id;\r\nmsg.index = HSS_CONFIG_RX_FCR;\r\nmsg.data16a = FRAME_OFFSET;\r\nmsg.data16b = FRAME_SIZE - 1;\r\nhss_npe_send(port, &msg, "HSS_SET_RX_FCR");\r\nhss_config_set_lut(port);\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PORT_CONFIG_LOAD;\r\nmsg.hss_port = port->id;\r\nhss_npe_send(port, &msg, "HSS_LOAD_CONFIG");\r\nif (npe_recv_message(port->npe, &msg, "HSS_LOAD_CONFIG") ||\r\nmsg.cmd != PORT_CONFIG_LOAD || msg.data32) {\r\npr_crit("HSS-%i: HSS_LOAD_CONFIG failed\n", port->id);\r\nBUG();\r\n}\r\nnpe_recv_message(port->npe, &msg, "FLUSH_IT");\r\n}\r\nstatic void hss_set_hdlc_cfg(struct port *port)\r\n{\r\nstruct msg msg;\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PKT_PIPE_HDLC_CFG_WRITE;\r\nmsg.hss_port = port->id;\r\nmsg.data8a = port->hdlc_cfg;\r\nmsg.data8b = port->hdlc_cfg | (PKT_EXTRA_FLAGS << 3);\r\nhss_npe_send(port, &msg, "HSS_SET_HDLC_CFG");\r\n}\r\nstatic u32 hss_get_status(struct port *port)\r\n{\r\nstruct msg msg;\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PORT_ERROR_READ;\r\nmsg.hss_port = port->id;\r\nhss_npe_send(port, &msg, "PORT_ERROR_READ");\r\nif (npe_recv_message(port->npe, &msg, "PORT_ERROR_READ")) {\r\npr_crit("HSS-%i: unable to read HSS status\n", port->id);\r\nBUG();\r\n}\r\nreturn msg.data32;\r\n}\r\nstatic void hss_start_hdlc(struct port *port)\r\n{\r\nstruct msg msg;\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PKT_PIPE_FLOW_ENABLE;\r\nmsg.hss_port = port->id;\r\nmsg.data32 = 0;\r\nhss_npe_send(port, &msg, "HSS_ENABLE_PKT_PIPE");\r\n}\r\nstatic void hss_stop_hdlc(struct port *port)\r\n{\r\nstruct msg msg;\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PKT_PIPE_FLOW_DISABLE;\r\nmsg.hss_port = port->id;\r\nhss_npe_send(port, &msg, "HSS_DISABLE_PKT_PIPE");\r\nhss_get_status(port);\r\n}\r\nstatic int hss_load_firmware(struct port *port)\r\n{\r\nstruct msg msg;\r\nint err;\r\nif (port->initialized)\r\nreturn 0;\r\nif (!npe_running(port->npe) &&\r\n(err = npe_load_firmware(port->npe, npe_name(port->npe),\r\nport->dev)))\r\nreturn err;\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = PKT_NUM_PIPES_WRITE;\r\nmsg.hss_port = port->id;\r\nmsg.data8a = PKT_NUM_PIPES;\r\nhss_npe_send(port, &msg, "HSS_SET_PKT_PIPES");\r\nmsg.cmd = PKT_PIPE_FIFO_SIZEW_WRITE;\r\nmsg.data8a = PKT_PIPE_FIFO_SIZEW;\r\nhss_npe_send(port, &msg, "HSS_SET_PKT_FIFO");\r\nmsg.cmd = PKT_PIPE_MODE_WRITE;\r\nmsg.data8a = NPE_PKT_MODE_HDLC;\r\nhss_npe_send(port, &msg, "HSS_SET_PKT_MODE");\r\nmsg.cmd = PKT_PIPE_RX_SIZE_WRITE;\r\nmsg.data16a = HDLC_MAX_MRU;\r\nhss_npe_send(port, &msg, "HSS_SET_PKT_RX_SIZE");\r\nmsg.cmd = PKT_PIPE_IDLE_PATTERN_WRITE;\r\nmsg.data32 = 0x7F7F7F7F;\r\nhss_npe_send(port, &msg, "HSS_SET_PKT_IDLE");\r\nport->initialized = 1;\r\nreturn 0;\r\n}\r\nstatic inline void debug_pkt(struct net_device *dev, const char *func,\r\nu8 *data, int len)\r\n{\r\n#if DEBUG_PKT_BYTES\r\nint i;\r\nprintk(KERN_DEBUG "%s: %s(%i)", dev->name, func, len);\r\nfor (i = 0; i < len; i++) {\r\nif (i >= DEBUG_PKT_BYTES)\r\nbreak;\r\nprintk("%s%02X", !(i % 4) ? " " : "", data[i]);\r\n}\r\nprintk("\n");\r\n#endif\r\n}\r\nstatic inline void debug_desc(u32 phys, struct desc *desc)\r\n{\r\n#if DEBUG_DESC\r\nprintk(KERN_DEBUG "%X: %X %3X %3X %08X %X %X\n",\r\nphys, desc->next, desc->buf_len, desc->pkt_len,\r\ndesc->data, desc->status, desc->error_count);\r\n#endif\r\n}\r\nstatic inline int queue_get_desc(unsigned int queue, struct port *port,\r\nint is_tx)\r\n{\r\nu32 phys, tab_phys, n_desc;\r\nstruct desc *tab;\r\nif (!(phys = qmgr_get_entry(queue)))\r\nreturn -1;\r\nBUG_ON(phys & 0x1F);\r\ntab_phys = is_tx ? tx_desc_phys(port, 0) : rx_desc_phys(port, 0);\r\ntab = is_tx ? tx_desc_ptr(port, 0) : rx_desc_ptr(port, 0);\r\nn_desc = (phys - tab_phys) / sizeof(struct desc);\r\nBUG_ON(n_desc >= (is_tx ? TX_DESCS : RX_DESCS));\r\ndebug_desc(phys, &tab[n_desc]);\r\nBUG_ON(tab[n_desc].next);\r\nreturn n_desc;\r\n}\r\nstatic inline void queue_put_desc(unsigned int queue, u32 phys,\r\nstruct desc *desc)\r\n{\r\ndebug_desc(phys, desc);\r\nBUG_ON(phys & 0x1F);\r\nqmgr_put_entry(queue, phys);\r\n}\r\nstatic inline void dma_unmap_tx(struct port *port, struct desc *desc)\r\n{\r\n#ifdef __ARMEB__\r\ndma_unmap_single(&port->netdev->dev, desc->data,\r\ndesc->buf_len, DMA_TO_DEVICE);\r\n#else\r\ndma_unmap_single(&port->netdev->dev, desc->data & ~3,\r\nALIGN((desc->data & 3) + desc->buf_len, 4),\r\nDMA_TO_DEVICE);\r\n#endif\r\n}\r\nstatic void hss_hdlc_set_carrier(void *pdev, int carrier)\r\n{\r\nstruct net_device *netdev = pdev;\r\nstruct port *port = dev_to_port(netdev);\r\nunsigned long flags;\r\nspin_lock_irqsave(&npe_lock, flags);\r\nport->carrier = carrier;\r\nif (!port->loopback) {\r\nif (carrier)\r\nnetif_carrier_on(netdev);\r\nelse\r\nnetif_carrier_off(netdev);\r\n}\r\nspin_unlock_irqrestore(&npe_lock, flags);\r\n}\r\nstatic void hss_hdlc_rx_irq(void *pdev)\r\n{\r\nstruct net_device *dev = pdev;\r\nstruct port *port = dev_to_port(dev);\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_rx_irq\n", dev->name);\r\n#endif\r\nqmgr_disable_irq(queue_ids[port->id].rx);\r\nnapi_schedule(&port->napi);\r\n}\r\nstatic int hss_hdlc_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct port *port = container_of(napi, struct port, napi);\r\nstruct net_device *dev = port->netdev;\r\nunsigned int rxq = queue_ids[port->id].rx;\r\nunsigned int rxfreeq = queue_ids[port->id].rxfree;\r\nint received = 0;\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_poll\n", dev->name);\r\n#endif\r\nwhile (received < budget) {\r\nstruct sk_buff *skb;\r\nstruct desc *desc;\r\nint n;\r\n#ifdef __ARMEB__\r\nstruct sk_buff *temp;\r\nu32 phys;\r\n#endif\r\nif ((n = queue_get_desc(rxq, port, 0)) < 0) {\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_poll"\r\n" napi_complete\n", dev->name);\r\n#endif\r\nnapi_complete(napi);\r\nqmgr_enable_irq(rxq);\r\nif (!qmgr_stat_empty(rxq) &&\r\nnapi_reschedule(napi)) {\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_poll"\r\n" napi_reschedule succeeded\n",\r\ndev->name);\r\n#endif\r\nqmgr_disable_irq(rxq);\r\ncontinue;\r\n}\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_poll all done\n",\r\ndev->name);\r\n#endif\r\nreturn received;\r\n}\r\ndesc = rx_desc_ptr(port, n);\r\n#if 0\r\nif (desc->error_count)\r\nprintk(KERN_DEBUG "%s: hss_hdlc_poll status 0x%02X"\r\n" errors %u\n", dev->name, desc->status,\r\ndesc->error_count);\r\n#endif\r\nskb = NULL;\r\nswitch (desc->status) {\r\ncase 0:\r\n#ifdef __ARMEB__\r\nif ((skb = netdev_alloc_skb(dev, RX_SIZE)) != NULL) {\r\nphys = dma_map_single(&dev->dev, skb->data,\r\nRX_SIZE,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(&dev->dev, phys)) {\r\ndev_kfree_skb(skb);\r\nskb = NULL;\r\n}\r\n}\r\n#else\r\nskb = netdev_alloc_skb(dev, desc->pkt_len);\r\n#endif\r\nif (!skb)\r\ndev->stats.rx_dropped++;\r\nbreak;\r\ncase ERR_HDLC_ALIGN:\r\ncase ERR_HDLC_ABORT:\r\ndev->stats.rx_frame_errors++;\r\ndev->stats.rx_errors++;\r\nbreak;\r\ncase ERR_HDLC_FCS:\r\ndev->stats.rx_crc_errors++;\r\ndev->stats.rx_errors++;\r\nbreak;\r\ncase ERR_HDLC_TOO_LONG:\r\ndev->stats.rx_length_errors++;\r\ndev->stats.rx_errors++;\r\nbreak;\r\ndefault:\r\nnetdev_err(dev, "hss_hdlc_poll: status 0x%02X errors %u\n",\r\ndesc->status, desc->error_count);\r\ndev->stats.rx_errors++;\r\n}\r\nif (!skb) {\r\ndesc->buf_len = RX_SIZE;\r\ndesc->pkt_len = desc->status = 0;\r\nqueue_put_desc(rxfreeq, rx_desc_phys(port, n), desc);\r\ncontinue;\r\n}\r\n#ifdef __ARMEB__\r\ntemp = skb;\r\nskb = port->rx_buff_tab[n];\r\ndma_unmap_single(&dev->dev, desc->data,\r\nRX_SIZE, DMA_FROM_DEVICE);\r\n#else\r\ndma_sync_single_for_cpu(&dev->dev, desc->data,\r\nRX_SIZE, DMA_FROM_DEVICE);\r\nmemcpy_swab32((u32 *)skb->data, (u32 *)port->rx_buff_tab[n],\r\nALIGN(desc->pkt_len, 4) / 4);\r\n#endif\r\nskb_put(skb, desc->pkt_len);\r\ndebug_pkt(dev, "hss_hdlc_poll", skb->data, skb->len);\r\nskb->protocol = hdlc_type_trans(skb, dev);\r\ndev->stats.rx_packets++;\r\ndev->stats.rx_bytes += skb->len;\r\nnetif_receive_skb(skb);\r\n#ifdef __ARMEB__\r\nport->rx_buff_tab[n] = temp;\r\ndesc->data = phys;\r\n#endif\r\ndesc->buf_len = RX_SIZE;\r\ndesc->pkt_len = 0;\r\nqueue_put_desc(rxfreeq, rx_desc_phys(port, n), desc);\r\nreceived++;\r\n}\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "hss_hdlc_poll: end, not all work done\n");\r\n#endif\r\nreturn received;\r\n}\r\nstatic void hss_hdlc_txdone_irq(void *pdev)\r\n{\r\nstruct net_device *dev = pdev;\r\nstruct port *port = dev_to_port(dev);\r\nint n_desc;\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG DRV_NAME ": hss_hdlc_txdone_irq\n");\r\n#endif\r\nwhile ((n_desc = queue_get_desc(queue_ids[port->id].txdone,\r\nport, 1)) >= 0) {\r\nstruct desc *desc;\r\nint start;\r\ndesc = tx_desc_ptr(port, n_desc);\r\ndev->stats.tx_packets++;\r\ndev->stats.tx_bytes += desc->pkt_len;\r\ndma_unmap_tx(port, desc);\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_txdone_irq free %p\n",\r\ndev->name, port->tx_buff_tab[n_desc]);\r\n#endif\r\nfree_buffer_irq(port->tx_buff_tab[n_desc]);\r\nport->tx_buff_tab[n_desc] = NULL;\r\nstart = qmgr_stat_below_low_watermark(port->plat->txreadyq);\r\nqueue_put_desc(port->plat->txreadyq,\r\ntx_desc_phys(port, n_desc), desc);\r\nif (start) {\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_txdone_irq xmit"\r\n" ready\n", dev->name);\r\n#endif\r\nnetif_wake_queue(dev);\r\n}\r\n}\r\n}\r\nstatic int hss_hdlc_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct port *port = dev_to_port(dev);\r\nunsigned int txreadyq = port->plat->txreadyq;\r\nint len, offset, bytes, n;\r\nvoid *mem;\r\nu32 phys;\r\nstruct desc *desc;\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_xmit\n", dev->name);\r\n#endif\r\nif (unlikely(skb->len > HDLC_MAX_MRU)) {\r\ndev_kfree_skb(skb);\r\ndev->stats.tx_errors++;\r\nreturn NETDEV_TX_OK;\r\n}\r\ndebug_pkt(dev, "hss_hdlc_xmit", skb->data, skb->len);\r\nlen = skb->len;\r\n#ifdef __ARMEB__\r\noffset = 0;\r\nbytes = len;\r\nmem = skb->data;\r\n#else\r\noffset = (int)skb->data & 3;\r\nbytes = ALIGN(offset + len, 4);\r\nif (!(mem = kmalloc(bytes, GFP_ATOMIC))) {\r\ndev_kfree_skb(skb);\r\ndev->stats.tx_dropped++;\r\nreturn NETDEV_TX_OK;\r\n}\r\nmemcpy_swab32(mem, (u32 *)((int)skb->data & ~3), bytes / 4);\r\ndev_kfree_skb(skb);\r\n#endif\r\nphys = dma_map_single(&dev->dev, mem, bytes, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&dev->dev, phys)) {\r\n#ifdef __ARMEB__\r\ndev_kfree_skb(skb);\r\n#else\r\nkfree(mem);\r\n#endif\r\ndev->stats.tx_dropped++;\r\nreturn NETDEV_TX_OK;\r\n}\r\nn = queue_get_desc(txreadyq, port, 1);\r\nBUG_ON(n < 0);\r\ndesc = tx_desc_ptr(port, n);\r\n#ifdef __ARMEB__\r\nport->tx_buff_tab[n] = skb;\r\n#else\r\nport->tx_buff_tab[n] = mem;\r\n#endif\r\ndesc->data = phys + offset;\r\ndesc->buf_len = desc->pkt_len = len;\r\nwmb();\r\nqueue_put_desc(queue_ids[port->id].tx, tx_desc_phys(port, n), desc);\r\nif (qmgr_stat_below_low_watermark(txreadyq)) {\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_xmit queue full\n", dev->name);\r\n#endif\r\nnetif_stop_queue(dev);\r\nif (!qmgr_stat_below_low_watermark(txreadyq)) {\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_xmit ready again\n",\r\ndev->name);\r\n#endif\r\nnetif_wake_queue(dev);\r\n}\r\n}\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: hss_hdlc_xmit end\n", dev->name);\r\n#endif\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int request_hdlc_queues(struct port *port)\r\n{\r\nint err;\r\nerr = qmgr_request_queue(queue_ids[port->id].rxfree, RX_DESCS, 0, 0,\r\n"%s:RX-free", port->netdev->name);\r\nif (err)\r\nreturn err;\r\nerr = qmgr_request_queue(queue_ids[port->id].rx, RX_DESCS, 0, 0,\r\n"%s:RX", port->netdev->name);\r\nif (err)\r\ngoto rel_rxfree;\r\nerr = qmgr_request_queue(queue_ids[port->id].tx, TX_DESCS, 0, 0,\r\n"%s:TX", port->netdev->name);\r\nif (err)\r\ngoto rel_rx;\r\nerr = qmgr_request_queue(port->plat->txreadyq, TX_DESCS, 0, 0,\r\n"%s:TX-ready", port->netdev->name);\r\nif (err)\r\ngoto rel_tx;\r\nerr = qmgr_request_queue(queue_ids[port->id].txdone, TX_DESCS, 0, 0,\r\n"%s:TX-done", port->netdev->name);\r\nif (err)\r\ngoto rel_txready;\r\nreturn 0;\r\nrel_txready:\r\nqmgr_release_queue(port->plat->txreadyq);\r\nrel_tx:\r\nqmgr_release_queue(queue_ids[port->id].tx);\r\nrel_rx:\r\nqmgr_release_queue(queue_ids[port->id].rx);\r\nrel_rxfree:\r\nqmgr_release_queue(queue_ids[port->id].rxfree);\r\nprintk(KERN_DEBUG "%s: unable to request hardware queues\n",\r\nport->netdev->name);\r\nreturn err;\r\n}\r\nstatic void release_hdlc_queues(struct port *port)\r\n{\r\nqmgr_release_queue(queue_ids[port->id].rxfree);\r\nqmgr_release_queue(queue_ids[port->id].rx);\r\nqmgr_release_queue(queue_ids[port->id].txdone);\r\nqmgr_release_queue(queue_ids[port->id].tx);\r\nqmgr_release_queue(port->plat->txreadyq);\r\n}\r\nstatic int init_hdlc_queues(struct port *port)\r\n{\r\nint i;\r\nif (!ports_open)\r\nif (!(dma_pool = dma_pool_create(DRV_NAME, NULL,\r\nPOOL_ALLOC_SIZE, 32, 0)))\r\nreturn -ENOMEM;\r\nif (!(port->desc_tab = dma_pool_alloc(dma_pool, GFP_KERNEL,\r\n&port->desc_tab_phys)))\r\nreturn -ENOMEM;\r\nmemset(port->desc_tab, 0, POOL_ALLOC_SIZE);\r\nmemset(port->rx_buff_tab, 0, sizeof(port->rx_buff_tab));\r\nmemset(port->tx_buff_tab, 0, sizeof(port->tx_buff_tab));\r\nfor (i = 0; i < RX_DESCS; i++) {\r\nstruct desc *desc = rx_desc_ptr(port, i);\r\nbuffer_t *buff;\r\nvoid *data;\r\n#ifdef __ARMEB__\r\nif (!(buff = netdev_alloc_skb(port->netdev, RX_SIZE)))\r\nreturn -ENOMEM;\r\ndata = buff->data;\r\n#else\r\nif (!(buff = kmalloc(RX_SIZE, GFP_KERNEL)))\r\nreturn -ENOMEM;\r\ndata = buff;\r\n#endif\r\ndesc->buf_len = RX_SIZE;\r\ndesc->data = dma_map_single(&port->netdev->dev, data,\r\nRX_SIZE, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(&port->netdev->dev, desc->data)) {\r\nfree_buffer(buff);\r\nreturn -EIO;\r\n}\r\nport->rx_buff_tab[i] = buff;\r\n}\r\nreturn 0;\r\n}\r\nstatic void destroy_hdlc_queues(struct port *port)\r\n{\r\nint i;\r\nif (port->desc_tab) {\r\nfor (i = 0; i < RX_DESCS; i++) {\r\nstruct desc *desc = rx_desc_ptr(port, i);\r\nbuffer_t *buff = port->rx_buff_tab[i];\r\nif (buff) {\r\ndma_unmap_single(&port->netdev->dev,\r\ndesc->data, RX_SIZE,\r\nDMA_FROM_DEVICE);\r\nfree_buffer(buff);\r\n}\r\n}\r\nfor (i = 0; i < TX_DESCS; i++) {\r\nstruct desc *desc = tx_desc_ptr(port, i);\r\nbuffer_t *buff = port->tx_buff_tab[i];\r\nif (buff) {\r\ndma_unmap_tx(port, desc);\r\nfree_buffer(buff);\r\n}\r\n}\r\ndma_pool_free(dma_pool, port->desc_tab, port->desc_tab_phys);\r\nport->desc_tab = NULL;\r\n}\r\nif (!ports_open && dma_pool) {\r\ndma_pool_destroy(dma_pool);\r\ndma_pool = NULL;\r\n}\r\n}\r\nstatic int hss_hdlc_open(struct net_device *dev)\r\n{\r\nstruct port *port = dev_to_port(dev);\r\nunsigned long flags;\r\nint i, err = 0;\r\nif ((err = hdlc_open(dev)))\r\nreturn err;\r\nif ((err = hss_load_firmware(port)))\r\ngoto err_hdlc_close;\r\nif ((err = request_hdlc_queues(port)))\r\ngoto err_hdlc_close;\r\nif ((err = init_hdlc_queues(port)))\r\ngoto err_destroy_queues;\r\nspin_lock_irqsave(&npe_lock, flags);\r\nif (port->plat->open)\r\nif ((err = port->plat->open(port->id, dev,\r\nhss_hdlc_set_carrier)))\r\ngoto err_unlock;\r\nspin_unlock_irqrestore(&npe_lock, flags);\r\nfor (i = 0; i < TX_DESCS; i++)\r\nqueue_put_desc(port->plat->txreadyq,\r\ntx_desc_phys(port, i), tx_desc_ptr(port, i));\r\nfor (i = 0; i < RX_DESCS; i++)\r\nqueue_put_desc(queue_ids[port->id].rxfree,\r\nrx_desc_phys(port, i), rx_desc_ptr(port, i));\r\nnapi_enable(&port->napi);\r\nnetif_start_queue(dev);\r\nqmgr_set_irq(queue_ids[port->id].rx, QUEUE_IRQ_SRC_NOT_EMPTY,\r\nhss_hdlc_rx_irq, dev);\r\nqmgr_set_irq(queue_ids[port->id].txdone, QUEUE_IRQ_SRC_NOT_EMPTY,\r\nhss_hdlc_txdone_irq, dev);\r\nqmgr_enable_irq(queue_ids[port->id].txdone);\r\nports_open++;\r\nhss_set_hdlc_cfg(port);\r\nhss_config(port);\r\nhss_start_hdlc(port);\r\nnapi_schedule(&port->napi);\r\nreturn 0;\r\nerr_unlock:\r\nspin_unlock_irqrestore(&npe_lock, flags);\r\nerr_destroy_queues:\r\ndestroy_hdlc_queues(port);\r\nrelease_hdlc_queues(port);\r\nerr_hdlc_close:\r\nhdlc_close(dev);\r\nreturn err;\r\n}\r\nstatic int hss_hdlc_close(struct net_device *dev)\r\n{\r\nstruct port *port = dev_to_port(dev);\r\nunsigned long flags;\r\nint i, buffs = RX_DESCS;\r\nspin_lock_irqsave(&npe_lock, flags);\r\nports_open--;\r\nqmgr_disable_irq(queue_ids[port->id].rx);\r\nnetif_stop_queue(dev);\r\nnapi_disable(&port->napi);\r\nhss_stop_hdlc(port);\r\nwhile (queue_get_desc(queue_ids[port->id].rxfree, port, 0) >= 0)\r\nbuffs--;\r\nwhile (queue_get_desc(queue_ids[port->id].rx, port, 0) >= 0)\r\nbuffs--;\r\nif (buffs)\r\nnetdev_crit(dev, "unable to drain RX queue, %i buffer(s) left in NPE\n",\r\nbuffs);\r\nbuffs = TX_DESCS;\r\nwhile (queue_get_desc(queue_ids[port->id].tx, port, 1) >= 0)\r\nbuffs--;\r\ni = 0;\r\ndo {\r\nwhile (queue_get_desc(port->plat->txreadyq, port, 1) >= 0)\r\nbuffs--;\r\nif (!buffs)\r\nbreak;\r\n} while (++i < MAX_CLOSE_WAIT);\r\nif (buffs)\r\nnetdev_crit(dev, "unable to drain TX queue, %i buffer(s) left in NPE\n",\r\nbuffs);\r\n#if DEBUG_CLOSE\r\nif (!buffs)\r\nprintk(KERN_DEBUG "Draining TX queues took %i cycles\n", i);\r\n#endif\r\nqmgr_disable_irq(queue_ids[port->id].txdone);\r\nif (port->plat->close)\r\nport->plat->close(port->id, dev);\r\nspin_unlock_irqrestore(&npe_lock, flags);\r\ndestroy_hdlc_queues(port);\r\nrelease_hdlc_queues(port);\r\nhdlc_close(dev);\r\nreturn 0;\r\n}\r\nstatic int hss_hdlc_attach(struct net_device *dev, unsigned short encoding,\r\nunsigned short parity)\r\n{\r\nstruct port *port = dev_to_port(dev);\r\nif (encoding != ENCODING_NRZ)\r\nreturn -EINVAL;\r\nswitch(parity) {\r\ncase PARITY_CRC16_PR1_CCITT:\r\nport->hdlc_cfg = 0;\r\nreturn 0;\r\ncase PARITY_CRC32_PR1_CCITT:\r\nport->hdlc_cfg = PKT_HDLC_CRC_32;\r\nreturn 0;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic u32 check_clock(u32 rate, u32 a, u32 b, u32 c,\r\nu32 *best, u32 *best_diff, u32 *reg)\r\n{\r\nu64 new_rate;\r\nu32 new_diff;\r\nnew_rate = ixp4xx_timer_freq * (u64)(c + 1);\r\ndo_div(new_rate, a * (c + 1) + b + 1);\r\nnew_diff = abs((u32)new_rate - rate);\r\nif (new_diff < *best_diff) {\r\n*best = new_rate;\r\n*best_diff = new_diff;\r\n*reg = (a << 22) | (b << 12) | c;\r\n}\r\nreturn new_diff;\r\n}\r\nstatic void find_best_clock(u32 rate, u32 *best, u32 *reg)\r\n{\r\nu32 a, b, diff = 0xFFFFFFFF;\r\na = ixp4xx_timer_freq / rate;\r\nif (a > 0x3FF) {\r\ncheck_clock(rate, 0x3FF, 1, 1, best, &diff, reg);\r\nreturn;\r\n}\r\nif (a == 0) {\r\na = 1;\r\nrate = ixp4xx_timer_freq;\r\n}\r\nif (rate * a == ixp4xx_timer_freq) {\r\ncheck_clock(rate, a - 1, 1, 1, best, &diff, reg);\r\nreturn;\r\n}\r\nfor (b = 0; b < 0x400; b++) {\r\nu64 c = (b + 1) * (u64)rate;\r\ndo_div(c, ixp4xx_timer_freq - rate * a);\r\nc--;\r\nif (c >= 0xFFF) {\r\nif (b == 0 &&\r\n!check_clock(rate, a - 1, 1, 1, best, &diff, reg))\r\nreturn;\r\ncheck_clock(rate, a, b, 0xFFF, best, &diff, reg);\r\nreturn;\r\n}\r\nif (!check_clock(rate, a, b, c, best, &diff, reg))\r\nreturn;\r\nif (!check_clock(rate, a, b, c + 1, best, &diff, reg))\r\nreturn;\r\n}\r\n}\r\nstatic int hss_hdlc_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\r\n{\r\nconst size_t size = sizeof(sync_serial_settings);\r\nsync_serial_settings new_line;\r\nsync_serial_settings __user *line = ifr->ifr_settings.ifs_ifsu.sync;\r\nstruct port *port = dev_to_port(dev);\r\nunsigned long flags;\r\nint clk;\r\nif (cmd != SIOCWANDEV)\r\nreturn hdlc_ioctl(dev, ifr, cmd);\r\nswitch(ifr->ifr_settings.type) {\r\ncase IF_GET_IFACE:\r\nifr->ifr_settings.type = IF_IFACE_V35;\r\nif (ifr->ifr_settings.size < size) {\r\nifr->ifr_settings.size = size;\r\nreturn -ENOBUFS;\r\n}\r\nmemset(&new_line, 0, sizeof(new_line));\r\nnew_line.clock_type = port->clock_type;\r\nnew_line.clock_rate = port->clock_rate;\r\nnew_line.loopback = port->loopback;\r\nif (copy_to_user(line, &new_line, size))\r\nreturn -EFAULT;\r\nreturn 0;\r\ncase IF_IFACE_SYNC_SERIAL:\r\ncase IF_IFACE_V35:\r\nif(!capable(CAP_NET_ADMIN))\r\nreturn -EPERM;\r\nif (copy_from_user(&new_line, line, size))\r\nreturn -EFAULT;\r\nclk = new_line.clock_type;\r\nif (port->plat->set_clock)\r\nclk = port->plat->set_clock(port->id, clk);\r\nif (clk != CLOCK_EXT && clk != CLOCK_INT)\r\nreturn -EINVAL;\r\nif (new_line.loopback != 0 && new_line.loopback != 1)\r\nreturn -EINVAL;\r\nport->clock_type = clk;\r\nif (clk == CLOCK_INT)\r\nfind_best_clock(new_line.clock_rate, &port->clock_rate,\r\n&port->clock_reg);\r\nelse {\r\nport->clock_rate = 0;\r\nport->clock_reg = CLK42X_SPEED_2048KHZ;\r\n}\r\nport->loopback = new_line.loopback;\r\nspin_lock_irqsave(&npe_lock, flags);\r\nif (dev->flags & IFF_UP)\r\nhss_config(port);\r\nif (port->loopback || port->carrier)\r\nnetif_carrier_on(port->netdev);\r\nelse\r\nnetif_carrier_off(port->netdev);\r\nspin_unlock_irqrestore(&npe_lock, flags);\r\nreturn 0;\r\ndefault:\r\nreturn hdlc_ioctl(dev, ifr, cmd);\r\n}\r\n}\r\nstatic int __devinit hss_init_one(struct platform_device *pdev)\r\n{\r\nstruct port *port;\r\nstruct net_device *dev;\r\nhdlc_device *hdlc;\r\nint err;\r\nif ((port = kzalloc(sizeof(*port), GFP_KERNEL)) == NULL)\r\nreturn -ENOMEM;\r\nif ((port->npe = npe_request(0)) == NULL) {\r\nerr = -ENODEV;\r\ngoto err_free;\r\n}\r\nif ((port->netdev = dev = alloc_hdlcdev(port)) == NULL) {\r\nerr = -ENOMEM;\r\ngoto err_plat;\r\n}\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nhdlc = dev_to_hdlc(dev);\r\nhdlc->attach = hss_hdlc_attach;\r\nhdlc->xmit = hss_hdlc_xmit;\r\ndev->netdev_ops = &hss_hdlc_ops;\r\ndev->tx_queue_len = 100;\r\nport->clock_type = CLOCK_EXT;\r\nport->clock_rate = 0;\r\nport->clock_reg = CLK42X_SPEED_2048KHZ;\r\nport->id = pdev->id;\r\nport->dev = &pdev->dev;\r\nport->plat = pdev->dev.platform_data;\r\nnetif_napi_add(dev, &port->napi, hss_hdlc_poll, NAPI_WEIGHT);\r\nif ((err = register_hdlc_device(dev)))\r\ngoto err_free_netdev;\r\nplatform_set_drvdata(pdev, port);\r\nnetdev_info(dev, "HSS-%i\n", port->id);\r\nreturn 0;\r\nerr_free_netdev:\r\nfree_netdev(dev);\r\nerr_plat:\r\nnpe_release(port->npe);\r\nerr_free:\r\nkfree(port);\r\nreturn err;\r\n}\r\nstatic int __devexit hss_remove_one(struct platform_device *pdev)\r\n{\r\nstruct port *port = platform_get_drvdata(pdev);\r\nunregister_hdlc_device(port->netdev);\r\nfree_netdev(port->netdev);\r\nnpe_release(port->npe);\r\nplatform_set_drvdata(pdev, NULL);\r\nkfree(port);\r\nreturn 0;\r\n}\r\nstatic int __init hss_init_module(void)\r\n{\r\nif ((ixp4xx_read_feature_bits() &\r\n(IXP4XX_FEATURE_HDLC | IXP4XX_FEATURE_HSS)) !=\r\n(IXP4XX_FEATURE_HDLC | IXP4XX_FEATURE_HSS))\r\nreturn -ENODEV;\r\nspin_lock_init(&npe_lock);\r\nreturn platform_driver_register(&ixp4xx_hss_driver);\r\n}\r\nstatic void __exit hss_cleanup_module(void)\r\n{\r\nplatform_driver_unregister(&ixp4xx_hss_driver);\r\n}
