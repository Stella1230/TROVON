struct ipath_user_sdma_queue *\r\nipath_user_sdma_queue_create(struct device *dev, int unit, int port, int sport)\r\n{\r\nstruct ipath_user_sdma_queue *pq =\r\nkmalloc(sizeof(struct ipath_user_sdma_queue), GFP_KERNEL);\r\nif (!pq)\r\ngoto done;\r\npq->counter = 0;\r\npq->sent_counter = 0;\r\nINIT_LIST_HEAD(&pq->sent);\r\nmutex_init(&pq->lock);\r\nsnprintf(pq->pkt_slab_name, sizeof(pq->pkt_slab_name),\r\n"ipath-user-sdma-pkts-%u-%02u.%02u", unit, port, sport);\r\npq->pkt_slab = kmem_cache_create(pq->pkt_slab_name,\r\nsizeof(struct ipath_user_sdma_pkt),\r\n0, 0, NULL);\r\nif (!pq->pkt_slab)\r\ngoto err_kfree;\r\nsnprintf(pq->header_cache_name, sizeof(pq->header_cache_name),\r\n"ipath-user-sdma-headers-%u-%02u.%02u", unit, port, sport);\r\npq->header_cache = dma_pool_create(pq->header_cache_name,\r\ndev,\r\nIPATH_USER_SDMA_EXP_HEADER_LENGTH,\r\n4, 0);\r\nif (!pq->header_cache)\r\ngoto err_slab;\r\npq->dma_pages_root = RB_ROOT;\r\ngoto done;\r\nerr_slab:\r\nkmem_cache_destroy(pq->pkt_slab);\r\nerr_kfree:\r\nkfree(pq);\r\npq = NULL;\r\ndone:\r\nreturn pq;\r\n}\r\nstatic void ipath_user_sdma_init_frag(struct ipath_user_sdma_pkt *pkt,\r\nint i, size_t offset, size_t len,\r\nint put_page, int dma_mapped,\r\nstruct page *page,\r\nvoid *kvaddr, dma_addr_t dma_addr)\r\n{\r\npkt->addr[i].offset = offset;\r\npkt->addr[i].length = len;\r\npkt->addr[i].put_page = put_page;\r\npkt->addr[i].dma_mapped = dma_mapped;\r\npkt->addr[i].page = page;\r\npkt->addr[i].kvaddr = kvaddr;\r\npkt->addr[i].addr = dma_addr;\r\n}\r\nstatic void ipath_user_sdma_init_header(struct ipath_user_sdma_pkt *pkt,\r\nu32 counter, size_t offset,\r\nsize_t len, int dma_mapped,\r\nstruct page *page,\r\nvoid *kvaddr, dma_addr_t dma_addr)\r\n{\r\npkt->naddr = 1;\r\npkt->counter = counter;\r\nipath_user_sdma_init_frag(pkt, 0, offset, len, 0, dma_mapped, page,\r\nkvaddr, dma_addr);\r\n}\r\nstatic int ipath_user_sdma_coalesce(const struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_pkt *pkt,\r\nconst struct iovec *iov,\r\nunsigned long niov) {\r\nint ret = 0;\r\nstruct page *page = alloc_page(GFP_KERNEL);\r\nvoid *mpage_save;\r\nchar *mpage;\r\nint i;\r\nint len = 0;\r\ndma_addr_t dma_addr;\r\nif (!page) {\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\nmpage = kmap(page);\r\nmpage_save = mpage;\r\nfor (i = 0; i < niov; i++) {\r\nint cfur;\r\ncfur = copy_from_user(mpage,\r\niov[i].iov_base, iov[i].iov_len);\r\nif (cfur) {\r\nret = -EFAULT;\r\ngoto free_unmap;\r\n}\r\nmpage += iov[i].iov_len;\r\nlen += iov[i].iov_len;\r\n}\r\ndma_addr = dma_map_page(&dd->pcidev->dev, page, 0, len,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&dd->pcidev->dev, dma_addr)) {\r\nret = -ENOMEM;\r\ngoto free_unmap;\r\n}\r\nipath_user_sdma_init_frag(pkt, 1, 0, len, 0, 1, page, mpage_save,\r\ndma_addr);\r\npkt->naddr = 2;\r\ngoto done;\r\nfree_unmap:\r\nkunmap(page);\r\n__free_page(page);\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int ipath_user_sdma_num_pages(const struct iovec *iov)\r\n{\r\nconst unsigned long addr = (unsigned long) iov->iov_base;\r\nconst unsigned long len = iov->iov_len;\r\nconst unsigned long spage = addr & PAGE_MASK;\r\nconst unsigned long epage = (addr + len - 1) & PAGE_MASK;\r\nreturn 1 + ((epage - spage) >> PAGE_SHIFT);\r\n}\r\nstatic int ipath_user_sdma_page_length(unsigned long addr, unsigned long len)\r\n{\r\nconst unsigned long offset = addr & ~PAGE_MASK;\r\nreturn ((offset + len) > PAGE_SIZE) ? (PAGE_SIZE - offset) : len;\r\n}\r\nstatic void ipath_user_sdma_free_pkt_frag(struct device *dev,\r\nstruct ipath_user_sdma_queue *pq,\r\nstruct ipath_user_sdma_pkt *pkt,\r\nint frag)\r\n{\r\nconst int i = frag;\r\nif (pkt->addr[i].page) {\r\nif (pkt->addr[i].dma_mapped)\r\ndma_unmap_page(dev,\r\npkt->addr[i].addr,\r\npkt->addr[i].length,\r\nDMA_TO_DEVICE);\r\nif (pkt->addr[i].kvaddr)\r\nkunmap(pkt->addr[i].page);\r\nif (pkt->addr[i].put_page)\r\nput_page(pkt->addr[i].page);\r\nelse\r\n__free_page(pkt->addr[i].page);\r\n} else if (pkt->addr[i].kvaddr)\r\ndma_pool_free(pq->header_cache,\r\npkt->addr[i].kvaddr, pkt->addr[i].addr);\r\n}\r\nstatic int ipath_user_sdma_pin_pages(const struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_pkt *pkt,\r\nunsigned long addr, int tlen, int npages)\r\n{\r\nstruct page *pages[2];\r\nint j;\r\nint ret;\r\nret = get_user_pages(current, current->mm, addr,\r\nnpages, 0, 1, pages, NULL);\r\nif (ret != npages) {\r\nint i;\r\nfor (i = 0; i < ret; i++)\r\nput_page(pages[i]);\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\nfor (j = 0; j < npages; j++) {\r\nconst int flen =\r\nipath_user_sdma_page_length(addr, tlen);\r\ndma_addr_t dma_addr =\r\ndma_map_page(&dd->pcidev->dev,\r\npages[j], 0, flen, DMA_TO_DEVICE);\r\nunsigned long fofs = addr & ~PAGE_MASK;\r\nif (dma_mapping_error(&dd->pcidev->dev, dma_addr)) {\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\nipath_user_sdma_init_frag(pkt, pkt->naddr, fofs, flen, 1, 1,\r\npages[j], kmap(pages[j]),\r\ndma_addr);\r\npkt->naddr++;\r\naddr += flen;\r\ntlen -= flen;\r\n}\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int ipath_user_sdma_pin_pkt(const struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_queue *pq,\r\nstruct ipath_user_sdma_pkt *pkt,\r\nconst struct iovec *iov,\r\nunsigned long niov)\r\n{\r\nint ret = 0;\r\nunsigned long idx;\r\nfor (idx = 0; idx < niov; idx++) {\r\nconst int npages = ipath_user_sdma_num_pages(iov + idx);\r\nconst unsigned long addr = (unsigned long) iov[idx].iov_base;\r\nret = ipath_user_sdma_pin_pages(dd, pkt,\r\naddr, iov[idx].iov_len,\r\nnpages);\r\nif (ret < 0)\r\ngoto free_pkt;\r\n}\r\ngoto done;\r\nfree_pkt:\r\nfor (idx = 0; idx < pkt->naddr; idx++)\r\nipath_user_sdma_free_pkt_frag(&dd->pcidev->dev, pq, pkt, idx);\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int ipath_user_sdma_init_payload(const struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_queue *pq,\r\nstruct ipath_user_sdma_pkt *pkt,\r\nconst struct iovec *iov,\r\nunsigned long niov, int npages)\r\n{\r\nint ret = 0;\r\nif (npages >= ARRAY_SIZE(pkt->addr))\r\nret = ipath_user_sdma_coalesce(dd, pkt, iov, niov);\r\nelse\r\nret = ipath_user_sdma_pin_pkt(dd, pq, pkt, iov, niov);\r\nreturn ret;\r\n}\r\nstatic void ipath_user_sdma_free_pkt_list(struct device *dev,\r\nstruct ipath_user_sdma_queue *pq,\r\nstruct list_head *list)\r\n{\r\nstruct ipath_user_sdma_pkt *pkt, *pkt_next;\r\nlist_for_each_entry_safe(pkt, pkt_next, list, list) {\r\nint i;\r\nfor (i = 0; i < pkt->naddr; i++)\r\nipath_user_sdma_free_pkt_frag(dev, pq, pkt, i);\r\nkmem_cache_free(pq->pkt_slab, pkt);\r\n}\r\n}\r\nstatic int ipath_user_sdma_queue_pkts(const struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_queue *pq,\r\nstruct list_head *list,\r\nconst struct iovec *iov,\r\nunsigned long niov,\r\nint maxpkts)\r\n{\r\nunsigned long idx = 0;\r\nint ret = 0;\r\nint npkts = 0;\r\nstruct page *page = NULL;\r\n__le32 *pbc;\r\ndma_addr_t dma_addr;\r\nstruct ipath_user_sdma_pkt *pkt = NULL;\r\nsize_t len;\r\nsize_t nw;\r\nu32 counter = pq->counter;\r\nint dma_mapped = 0;\r\nwhile (idx < niov && npkts < maxpkts) {\r\nconst unsigned long addr = (unsigned long) iov[idx].iov_base;\r\nconst unsigned long idx_save = idx;\r\nunsigned pktnw;\r\nunsigned pktnwc;\r\nint nfrags = 0;\r\nint npages = 0;\r\nint cfur;\r\ndma_mapped = 0;\r\nlen = iov[idx].iov_len;\r\nnw = len >> 2;\r\npage = NULL;\r\npkt = kmem_cache_alloc(pq->pkt_slab, GFP_KERNEL);\r\nif (!pkt) {\r\nret = -ENOMEM;\r\ngoto free_list;\r\n}\r\nif (len < IPATH_USER_SDMA_MIN_HEADER_LENGTH ||\r\nlen > PAGE_SIZE || len & 3 || addr & 3) {\r\nret = -EINVAL;\r\ngoto free_pkt;\r\n}\r\nif (len == IPATH_USER_SDMA_EXP_HEADER_LENGTH)\r\npbc = dma_pool_alloc(pq->header_cache, GFP_KERNEL,\r\n&dma_addr);\r\nelse\r\npbc = NULL;\r\nif (!pbc) {\r\npage = alloc_page(GFP_KERNEL);\r\nif (!page) {\r\nret = -ENOMEM;\r\ngoto free_pkt;\r\n}\r\npbc = kmap(page);\r\n}\r\ncfur = copy_from_user(pbc, iov[idx].iov_base, len);\r\nif (cfur) {\r\nret = -EFAULT;\r\ngoto free_pbc;\r\n}\r\npktnwc = nw - 1;\r\npktnw = le32_to_cpu(*pbc) & IPATH_PBC_LENGTH_MASK;\r\nif (pktnw < pktnwc || pktnw > pktnwc + (PAGE_SIZE >> 2)) {\r\nret = -EINVAL;\r\ngoto free_pbc;\r\n}\r\nidx++;\r\nwhile (pktnwc < pktnw && idx < niov) {\r\nconst size_t slen = iov[idx].iov_len;\r\nconst unsigned long faddr =\r\n(unsigned long) iov[idx].iov_base;\r\nif (slen & 3 || faddr & 3 || !slen ||\r\nslen > PAGE_SIZE) {\r\nret = -EINVAL;\r\ngoto free_pbc;\r\n}\r\nnpages++;\r\nif ((faddr & PAGE_MASK) !=\r\n((faddr + slen - 1) & PAGE_MASK))\r\nnpages++;\r\npktnwc += slen >> 2;\r\nidx++;\r\nnfrags++;\r\n}\r\nif (pktnwc != pktnw) {\r\nret = -EINVAL;\r\ngoto free_pbc;\r\n}\r\nif (page) {\r\ndma_addr = dma_map_page(&dd->pcidev->dev,\r\npage, 0, len, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&dd->pcidev->dev, dma_addr)) {\r\nret = -ENOMEM;\r\ngoto free_pbc;\r\n}\r\ndma_mapped = 1;\r\n}\r\nipath_user_sdma_init_header(pkt, counter, 0, len, dma_mapped,\r\npage, pbc, dma_addr);\r\nif (nfrags) {\r\nret = ipath_user_sdma_init_payload(dd, pq, pkt,\r\niov + idx_save + 1,\r\nnfrags, npages);\r\nif (ret < 0)\r\ngoto free_pbc_dma;\r\n}\r\ncounter++;\r\nnpkts++;\r\nlist_add_tail(&pkt->list, list);\r\n}\r\nret = idx;\r\ngoto done;\r\nfree_pbc_dma:\r\nif (dma_mapped)\r\ndma_unmap_page(&dd->pcidev->dev, dma_addr, len, DMA_TO_DEVICE);\r\nfree_pbc:\r\nif (page) {\r\nkunmap(page);\r\n__free_page(page);\r\n} else\r\ndma_pool_free(pq->header_cache, pbc, dma_addr);\r\nfree_pkt:\r\nkmem_cache_free(pq->pkt_slab, pkt);\r\nfree_list:\r\nipath_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, list);\r\ndone:\r\nreturn ret;\r\n}\r\nstatic void ipath_user_sdma_set_complete_counter(struct ipath_user_sdma_queue *pq,\r\nu32 c)\r\n{\r\npq->sent_counter = c;\r\n}\r\nstatic int ipath_user_sdma_queue_clean(const struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_queue *pq)\r\n{\r\nstruct list_head free_list;\r\nstruct ipath_user_sdma_pkt *pkt;\r\nstruct ipath_user_sdma_pkt *pkt_prev;\r\nint ret = 0;\r\nINIT_LIST_HEAD(&free_list);\r\nlist_for_each_entry_safe(pkt, pkt_prev, &pq->sent, list) {\r\ns64 descd = dd->ipath_sdma_descq_removed - pkt->added;\r\nif (descd < 0)\r\nbreak;\r\nlist_move_tail(&pkt->list, &free_list);\r\nret++;\r\n}\r\nif (!list_empty(&free_list)) {\r\nu32 counter;\r\npkt = list_entry(free_list.prev,\r\nstruct ipath_user_sdma_pkt, list);\r\ncounter = pkt->counter;\r\nipath_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, &free_list);\r\nipath_user_sdma_set_complete_counter(pq, counter);\r\n}\r\nreturn ret;\r\n}\r\nvoid ipath_user_sdma_queue_destroy(struct ipath_user_sdma_queue *pq)\r\n{\r\nif (!pq)\r\nreturn;\r\nkmem_cache_destroy(pq->pkt_slab);\r\ndma_pool_destroy(pq->header_cache);\r\nkfree(pq);\r\n}\r\nstatic int ipath_user_sdma_hwqueue_clean(struct ipath_devdata *dd)\r\n{\r\nint ret;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\nret = ipath_sdma_make_progress(dd);\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\nreturn ret;\r\n}\r\nvoid ipath_user_sdma_queue_drain(struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_queue *pq)\r\n{\r\nint i;\r\nif (!pq)\r\nreturn;\r\nfor (i = 0; i < 100; i++) {\r\nmutex_lock(&pq->lock);\r\nif (list_empty(&pq->sent)) {\r\nmutex_unlock(&pq->lock);\r\nbreak;\r\n}\r\nipath_user_sdma_hwqueue_clean(dd);\r\nipath_user_sdma_queue_clean(dd, pq);\r\nmutex_unlock(&pq->lock);\r\nmsleep(10);\r\n}\r\nif (!list_empty(&pq->sent)) {\r\nstruct list_head free_list;\r\nprintk(KERN_INFO "drain: lists not empty: forcing!\n");\r\nINIT_LIST_HEAD(&free_list);\r\nmutex_lock(&pq->lock);\r\nlist_splice_init(&pq->sent, &free_list);\r\nipath_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, &free_list);\r\nmutex_unlock(&pq->lock);\r\n}\r\n}\r\nstatic inline __le64 ipath_sdma_make_desc0(struct ipath_devdata *dd,\r\nu64 addr, u64 dwlen, u64 dwoffset)\r\n{\r\nreturn cpu_to_le64(\r\n((addr & 0xfffffffcULL) << 32) |\r\n((dd->ipath_sdma_generation & 3ULL) << 30) |\r\n((dwlen & 0x7ffULL) << 16) |\r\n(dwoffset & 0x7ffULL));\r\n}\r\nstatic inline __le64 ipath_sdma_make_first_desc0(__le64 descq)\r\n{\r\nreturn descq | cpu_to_le64(1ULL << 12);\r\n}\r\nstatic inline __le64 ipath_sdma_make_last_desc0(__le64 descq)\r\n{\r\nreturn descq | cpu_to_le64(1ULL << 11 | 1ULL << 13);\r\n}\r\nstatic inline __le64 ipath_sdma_make_desc1(u64 addr)\r\n{\r\nreturn cpu_to_le64(addr >> 32);\r\n}\r\nstatic void ipath_user_sdma_send_frag(struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_pkt *pkt, int idx,\r\nunsigned ofs, u16 tail)\r\n{\r\nconst u64 addr = (u64) pkt->addr[idx].addr +\r\n(u64) pkt->addr[idx].offset;\r\nconst u64 dwlen = (u64) pkt->addr[idx].length / 4;\r\n__le64 *descqp;\r\n__le64 descq0;\r\ndescqp = &dd->ipath_sdma_descq[tail].qw[0];\r\ndescq0 = ipath_sdma_make_desc0(dd, addr, dwlen, ofs);\r\nif (idx == 0)\r\ndescq0 = ipath_sdma_make_first_desc0(descq0);\r\nif (idx == pkt->naddr - 1)\r\ndescq0 = ipath_sdma_make_last_desc0(descq0);\r\ndescqp[0] = descq0;\r\ndescqp[1] = ipath_sdma_make_desc1(addr);\r\n}\r\nstatic int ipath_user_sdma_push_pkts(struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_queue *pq,\r\nstruct list_head *pktlist)\r\n{\r\nint ret = 0;\r\nunsigned long flags;\r\nu16 tail;\r\nif (list_empty(pktlist))\r\nreturn 0;\r\nif (unlikely(!(dd->ipath_flags & IPATH_LINKACTIVE)))\r\nreturn -ECOMM;\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\nif (unlikely(dd->ipath_sdma_status & IPATH_SDMA_ABORT_MASK)) {\r\nret = -ECOMM;\r\ngoto unlock;\r\n}\r\ntail = dd->ipath_sdma_descq_tail;\r\nwhile (!list_empty(pktlist)) {\r\nstruct ipath_user_sdma_pkt *pkt =\r\nlist_entry(pktlist->next, struct ipath_user_sdma_pkt,\r\nlist);\r\nint i;\r\nunsigned ofs = 0;\r\nu16 dtail = tail;\r\nif (pkt->naddr > ipath_sdma_descq_freecnt(dd))\r\ngoto unlock_check_tail;\r\nfor (i = 0; i < pkt->naddr; i++) {\r\nipath_user_sdma_send_frag(dd, pkt, i, ofs, tail);\r\nofs += pkt->addr[i].length >> 2;\r\nif (++tail == dd->ipath_sdma_descq_cnt) {\r\ntail = 0;\r\n++dd->ipath_sdma_generation;\r\n}\r\n}\r\nif ((ofs<<2) > dd->ipath_ibmaxlen) {\r\nipath_dbg("packet size %X > ibmax %X, fail\n",\r\nofs<<2, dd->ipath_ibmaxlen);\r\nret = -EMSGSIZE;\r\ngoto unlock;\r\n}\r\nif (ofs >= IPATH_SMALLBUF_DWORDS) {\r\nfor (i = 0; i < pkt->naddr; i++) {\r\ndd->ipath_sdma_descq[dtail].qw[0] |=\r\ncpu_to_le64(1ULL << 14);\r\nif (++dtail == dd->ipath_sdma_descq_cnt)\r\ndtail = 0;\r\n}\r\n}\r\ndd->ipath_sdma_descq_added += pkt->naddr;\r\npkt->added = dd->ipath_sdma_descq_added;\r\nlist_move_tail(&pkt->list, &pq->sent);\r\nret++;\r\n}\r\nunlock_check_tail:\r\nif (dd->ipath_sdma_descq_tail != tail) {\r\nwmb();\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmatail, tail);\r\ndd->ipath_sdma_descq_tail = tail;\r\n}\r\nunlock:\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\nreturn ret;\r\n}\r\nint ipath_user_sdma_writev(struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_queue *pq,\r\nconst struct iovec *iov,\r\nunsigned long dim)\r\n{\r\nint ret = 0;\r\nstruct list_head list;\r\nint npkts = 0;\r\nINIT_LIST_HEAD(&list);\r\nmutex_lock(&pq->lock);\r\nif (dd->ipath_sdma_descq_added != dd->ipath_sdma_descq_removed) {\r\nipath_user_sdma_hwqueue_clean(dd);\r\nipath_user_sdma_queue_clean(dd, pq);\r\n}\r\nwhile (dim) {\r\nconst int mxp = 8;\r\ndown_write(&current->mm->mmap_sem);\r\nret = ipath_user_sdma_queue_pkts(dd, pq, &list, iov, dim, mxp);\r\nup_write(&current->mm->mmap_sem);\r\nif (ret <= 0)\r\ngoto done_unlock;\r\nelse {\r\ndim -= ret;\r\niov += ret;\r\n}\r\nif (!list_empty(&list)) {\r\nif (ipath_sdma_descq_freecnt(dd) < ret * 4) {\r\nipath_user_sdma_hwqueue_clean(dd);\r\nipath_user_sdma_queue_clean(dd, pq);\r\n}\r\nret = ipath_user_sdma_push_pkts(dd, pq, &list);\r\nif (ret < 0)\r\ngoto done_unlock;\r\nelse {\r\nnpkts += ret;\r\npq->counter += ret;\r\nif (!list_empty(&list))\r\ngoto done_unlock;\r\n}\r\n}\r\n}\r\ndone_unlock:\r\nif (!list_empty(&list))\r\nipath_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, &list);\r\nmutex_unlock(&pq->lock);\r\nreturn (ret < 0) ? ret : npkts;\r\n}\r\nint ipath_user_sdma_make_progress(struct ipath_devdata *dd,\r\nstruct ipath_user_sdma_queue *pq)\r\n{\r\nint ret = 0;\r\nmutex_lock(&pq->lock);\r\nipath_user_sdma_hwqueue_clean(dd);\r\nret = ipath_user_sdma_queue_clean(dd, pq);\r\nmutex_unlock(&pq->lock);\r\nreturn ret;\r\n}\r\nu32 ipath_user_sdma_complete_counter(const struct ipath_user_sdma_queue *pq)\r\n{\r\nreturn pq->sent_counter;\r\n}\r\nu32 ipath_user_sdma_inflight_counter(struct ipath_user_sdma_queue *pq)\r\n{\r\nreturn pq->counter;\r\n}
