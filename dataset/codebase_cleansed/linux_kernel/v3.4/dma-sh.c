static inline unsigned int get_dmte_irq(unsigned int chan)\r\n{\r\nunsigned int irq = 0;\r\nif (chan < ARRAY_SIZE(dmte_irq_map))\r\nirq = dmte_irq_map[chan];\r\n#if defined(CONFIG_SH_DMA_IRQ_MULTI)\r\nif (irq > DMTE6_IRQ)\r\nreturn DMTE6_IRQ;\r\nreturn DMTE0_IRQ;\r\n#else\r\nreturn irq;\r\n#endif\r\n}\r\nstatic inline unsigned int calc_xmit_shift(struct dma_channel *chan)\r\n{\r\nu32 chcr = __raw_readl(dma_base_addr[chan->chan] + CHCR);\r\nint cnt = ((chcr & CHCR_TS_LOW_MASK) >> CHCR_TS_LOW_SHIFT) |\r\n((chcr & CHCR_TS_HIGH_MASK) >> CHCR_TS_HIGH_SHIFT);\r\nreturn ts_shift[cnt];\r\n}\r\nstatic irqreturn_t dma_tei(int irq, void *dev_id)\r\n{\r\nstruct dma_channel *chan = dev_id;\r\nu32 chcr;\r\nchcr = __raw_readl(dma_base_addr[chan->chan] + CHCR);\r\nif (!(chcr & CHCR_TE))\r\nreturn IRQ_NONE;\r\nchcr &= ~(CHCR_IE | CHCR_DE);\r\n__raw_writel(chcr, (dma_base_addr[chan->chan] + CHCR));\r\nwake_up(&chan->wait_queue);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int sh_dmac_request_dma(struct dma_channel *chan)\r\n{\r\nif (unlikely(!(chan->flags & DMA_TEI_CAPABLE)))\r\nreturn 0;\r\nreturn request_irq(get_dmte_irq(chan->chan), dma_tei,\r\n#if defined(CONFIG_SH_DMA_IRQ_MULTI)\r\nIRQF_SHARED,\r\n#else\r\n0,\r\n#endif\r\nchan->dev_id, chan);\r\n}\r\nstatic void sh_dmac_free_dma(struct dma_channel *chan)\r\n{\r\nfree_irq(get_dmte_irq(chan->chan), chan);\r\n}\r\nstatic int\r\nsh_dmac_configure_channel(struct dma_channel *chan, unsigned long chcr)\r\n{\r\nif (!chcr)\r\nchcr = RS_DUAL | CHCR_IE;\r\nif (chcr & CHCR_IE) {\r\nchcr &= ~CHCR_IE;\r\nchan->flags |= DMA_TEI_CAPABLE;\r\n} else {\r\nchan->flags &= ~DMA_TEI_CAPABLE;\r\n}\r\n__raw_writel(chcr, (dma_base_addr[chan->chan] + CHCR));\r\nchan->flags |= DMA_CONFIGURED;\r\nreturn 0;\r\n}\r\nstatic void sh_dmac_enable_dma(struct dma_channel *chan)\r\n{\r\nint irq;\r\nu32 chcr;\r\nchcr = __raw_readl(dma_base_addr[chan->chan] + CHCR);\r\nchcr |= CHCR_DE;\r\nif (chan->flags & DMA_TEI_CAPABLE)\r\nchcr |= CHCR_IE;\r\n__raw_writel(chcr, (dma_base_addr[chan->chan] + CHCR));\r\nif (chan->flags & DMA_TEI_CAPABLE) {\r\nirq = get_dmte_irq(chan->chan);\r\nenable_irq(irq);\r\n}\r\n}\r\nstatic void sh_dmac_disable_dma(struct dma_channel *chan)\r\n{\r\nint irq;\r\nu32 chcr;\r\nif (chan->flags & DMA_TEI_CAPABLE) {\r\nirq = get_dmte_irq(chan->chan);\r\ndisable_irq(irq);\r\n}\r\nchcr = __raw_readl(dma_base_addr[chan->chan] + CHCR);\r\nchcr &= ~(CHCR_DE | CHCR_TE | CHCR_IE);\r\n__raw_writel(chcr, (dma_base_addr[chan->chan] + CHCR));\r\n}\r\nstatic int sh_dmac_xfer_dma(struct dma_channel *chan)\r\n{\r\nif (unlikely(!(chan->flags & DMA_CONFIGURED)))\r\nsh_dmac_configure_channel(chan, 0);\r\nsh_dmac_disable_dma(chan);\r\nif (chan->sar || (mach_is_dreamcast() &&\r\nchan->chan == PVR2_CASCADE_CHAN))\r\n__raw_writel(chan->sar, (dma_base_addr[chan->chan]+SAR));\r\nif (chan->dar || (mach_is_dreamcast() &&\r\nchan->chan == PVR2_CASCADE_CHAN))\r\n__raw_writel(chan->dar, (dma_base_addr[chan->chan] + DAR));\r\n__raw_writel(chan->count >> calc_xmit_shift(chan),\r\n(dma_base_addr[chan->chan] + TCR));\r\nsh_dmac_enable_dma(chan);\r\nreturn 0;\r\n}\r\nstatic int sh_dmac_get_dma_residue(struct dma_channel *chan)\r\n{\r\nif (!(__raw_readl(dma_base_addr[chan->chan] + CHCR) & CHCR_DE))\r\nreturn 0;\r\nreturn __raw_readl(dma_base_addr[chan->chan] + TCR)\r\n<< calc_xmit_shift(chan);\r\n}\r\nstatic inline int dmaor_reset(int no)\r\n{\r\nunsigned long dmaor = dmaor_read_reg(no);\r\ndmaor &= ~(DMAOR_NMIF | DMAOR_AE);\r\ndmaor_write_reg(no, dmaor);\r\ndmaor |= DMAOR_INIT;\r\ndmaor_write_reg(no, dmaor);\r\nif ((dmaor_read_reg(no) & (DMAOR_AE | DMAOR_NMIF))) {\r\nprintk(KERN_ERR "dma-sh: Can't initialize DMAOR.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic irqreturn_t dma_err(int irq, void *dummy)\r\n{\r\n#if defined(CONFIG_SH_DMA_IRQ_MULTI)\r\nint cnt = 0;\r\nswitch (irq) {\r\n#if defined(DMTE6_IRQ) && defined(DMAE1_IRQ)\r\ncase DMTE6_IRQ:\r\ncnt++;\r\n#endif\r\ncase DMTE0_IRQ:\r\nif (dmaor_read_reg(cnt) & (DMAOR_NMIF | DMAOR_AE)) {\r\ndisable_irq(irq);\r\nreturn IRQ_HANDLED;\r\n}\r\ndefault:\r\nreturn IRQ_NONE;\r\n}\r\n#else\r\ndmaor_reset(0);\r\n#if defined(CONFIG_CPU_SUBTYPE_SH7723) || \\r\ndefined(CONFIG_CPU_SUBTYPE_SH7780) || \\r\ndefined(CONFIG_CPU_SUBTYPE_SH7785)\r\ndmaor_reset(1);\r\n#endif\r\ndisable_irq(irq);\r\nreturn IRQ_HANDLED;\r\n#endif\r\n}\r\nstatic unsigned int get_dma_error_irq(int n)\r\n{\r\n#if defined(CONFIG_SH_DMA_IRQ_MULTI)\r\nreturn (n == 0) ? get_dmte_irq(0) : get_dmte_irq(6);\r\n#else\r\nreturn (n == 0) ? DMAE0_IRQ :\r\n#if defined(DMAE1_IRQ)\r\nDMAE1_IRQ;\r\n#else\r\n-1;\r\n#endif\r\n#endif\r\n}\r\nstatic int __init sh_dmac_init(void)\r\n{\r\nstruct dma_info *info = &sh_dmac_info;\r\nint i;\r\n#ifdef CONFIG_CPU_SH4\r\nint n;\r\nfor (n = 0; n < NR_DMAE; n++) {\r\ni = request_irq(get_dma_error_irq(n), dma_err,\r\n#if defined(CONFIG_SH_DMA_IRQ_MULTI)\r\nIRQF_SHARED,\r\n#else\r\n0,\r\n#endif\r\ndmae_name[n], (void *)dmae_name[n]);\r\nif (unlikely(i < 0)) {\r\nprintk(KERN_ERR "%s request_irq fail\n", dmae_name[n]);\r\nreturn i;\r\n}\r\n}\r\n#endif\r\ni = dmaor_reset(0);\r\nif (unlikely(i != 0))\r\nreturn i;\r\n#if defined(CONFIG_CPU_SUBTYPE_SH7723) || \\r\ndefined(CONFIG_CPU_SUBTYPE_SH7780) || \\r\ndefined(CONFIG_CPU_SUBTYPE_SH7785)\r\ni = dmaor_reset(1);\r\nif (unlikely(i != 0))\r\nreturn i;\r\n#endif\r\nreturn register_dmac(info);\r\n}\r\nstatic void __exit sh_dmac_exit(void)\r\n{\r\n#ifdef CONFIG_CPU_SH4\r\nint n;\r\nfor (n = 0; n < NR_DMAE; n++) {\r\nfree_irq(get_dma_error_irq(n), (void *)dmae_name[n]);\r\n}\r\n#endif\r\nunregister_dmac(&sh_dmac_info);\r\n}
