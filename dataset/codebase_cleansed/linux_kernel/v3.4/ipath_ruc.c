void ipath_insert_rnr_queue(struct ipath_qp *qp)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(qp->ibqp.device);\r\nspin_lock(&dev->pending_lock);\r\nif (list_empty(&dev->rnrwait))\r\nlist_add(&qp->timerwait, &dev->rnrwait);\r\nelse {\r\nstruct list_head *l = &dev->rnrwait;\r\nstruct ipath_qp *nqp = list_entry(l->next, struct ipath_qp,\r\ntimerwait);\r\nwhile (qp->s_rnr_timeout >= nqp->s_rnr_timeout) {\r\nqp->s_rnr_timeout -= nqp->s_rnr_timeout;\r\nl = l->next;\r\nif (l->next == &dev->rnrwait) {\r\nnqp = NULL;\r\nbreak;\r\n}\r\nnqp = list_entry(l->next, struct ipath_qp,\r\ntimerwait);\r\n}\r\nif (nqp)\r\nnqp->s_rnr_timeout -= qp->s_rnr_timeout;\r\nlist_add(&qp->timerwait, l);\r\n}\r\nspin_unlock(&dev->pending_lock);\r\n}\r\nint ipath_init_sge(struct ipath_qp *qp, struct ipath_rwqe *wqe,\r\nu32 *lengthp, struct ipath_sge_state *ss)\r\n{\r\nint i, j, ret;\r\nstruct ib_wc wc;\r\n*lengthp = 0;\r\nfor (i = j = 0; i < wqe->num_sge; i++) {\r\nif (wqe->sg_list[i].length == 0)\r\ncontinue;\r\nif (!ipath_lkey_ok(qp, j ? &ss->sg_list[j - 1] : &ss->sge,\r\n&wqe->sg_list[i], IB_ACCESS_LOCAL_WRITE))\r\ngoto bad_lkey;\r\n*lengthp += wqe->sg_list[i].length;\r\nj++;\r\n}\r\nss->num_sge = j;\r\nret = 1;\r\ngoto bail;\r\nbad_lkey:\r\nmemset(&wc, 0, sizeof(wc));\r\nwc.wr_id = wqe->wr_id;\r\nwc.status = IB_WC_LOC_PROT_ERR;\r\nwc.opcode = IB_WC_RECV;\r\nwc.qp = &qp->ibqp;\r\nipath_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_get_rwqe(struct ipath_qp *qp, int wr_id_only)\r\n{\r\nunsigned long flags;\r\nstruct ipath_rq *rq;\r\nstruct ipath_rwq *wq;\r\nstruct ipath_srq *srq;\r\nstruct ipath_rwqe *wqe;\r\nvoid (*handler)(struct ib_event *, void *);\r\nu32 tail;\r\nint ret;\r\nif (qp->ibqp.srq) {\r\nsrq = to_isrq(qp->ibqp.srq);\r\nhandler = srq->ibsrq.event_handler;\r\nrq = &srq->rq;\r\n} else {\r\nsrq = NULL;\r\nhandler = NULL;\r\nrq = &qp->r_rq;\r\n}\r\nspin_lock_irqsave(&rq->lock, flags);\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_RECV_OK)) {\r\nret = 0;\r\ngoto unlock;\r\n}\r\nwq = rq->wq;\r\ntail = wq->tail;\r\nif (tail >= rq->size)\r\ntail = 0;\r\ndo {\r\nif (unlikely(tail == wq->head)) {\r\nret = 0;\r\ngoto unlock;\r\n}\r\nsmp_rmb();\r\nwqe = get_rwqe_ptr(rq, tail);\r\nif (++tail >= rq->size)\r\ntail = 0;\r\nif (wr_id_only)\r\nbreak;\r\nqp->r_sge.sg_list = qp->r_sg_list;\r\n} while (!ipath_init_sge(qp, wqe, &qp->r_len, &qp->r_sge));\r\nqp->r_wr_id = wqe->wr_id;\r\nwq->tail = tail;\r\nret = 1;\r\nset_bit(IPATH_R_WRID_VALID, &qp->r_aflags);\r\nif (handler) {\r\nu32 n;\r\nn = wq->head;\r\nif (n >= rq->size)\r\nn = 0;\r\nif (n < tail)\r\nn += rq->size - tail;\r\nelse\r\nn -= tail;\r\nif (n < srq->limit) {\r\nstruct ib_event ev;\r\nsrq->limit = 0;\r\nspin_unlock_irqrestore(&rq->lock, flags);\r\nev.device = qp->ibqp.device;\r\nev.element.srq = qp->ibqp.srq;\r\nev.event = IB_EVENT_SRQ_LIMIT_REACHED;\r\nhandler(&ev, srq->ibsrq.srq_context);\r\ngoto bail;\r\n}\r\n}\r\nunlock:\r\nspin_unlock_irqrestore(&rq->lock, flags);\r\nbail:\r\nreturn ret;\r\n}\r\nstatic void ipath_ruc_loopback(struct ipath_qp *sqp)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(sqp->ibqp.device);\r\nstruct ipath_qp *qp;\r\nstruct ipath_swqe *wqe;\r\nstruct ipath_sge *sge;\r\nunsigned long flags;\r\nstruct ib_wc wc;\r\nu64 sdata;\r\natomic64_t *maddr;\r\nenum ib_wc_status send_status;\r\nqp = ipath_lookup_qpn(&dev->qp_table, sqp->remote_qpn);\r\nspin_lock_irqsave(&sqp->s_lock, flags);\r\nif ((sqp->s_flags & (IPATH_S_BUSY | IPATH_S_ANY_WAIT)) ||\r\n!(ib_ipath_state_ops[sqp->state] & IPATH_PROCESS_OR_FLUSH_SEND))\r\ngoto unlock;\r\nsqp->s_flags |= IPATH_S_BUSY;\r\nagain:\r\nif (sqp->s_last == sqp->s_head)\r\ngoto clr_busy;\r\nwqe = get_swqe_ptr(sqp, sqp->s_last);\r\nif (!(ib_ipath_state_ops[sqp->state] & IPATH_PROCESS_NEXT_SEND_OK)) {\r\nif (!(ib_ipath_state_ops[sqp->state] & IPATH_FLUSH_SEND))\r\ngoto clr_busy;\r\nsend_status = IB_WC_WR_FLUSH_ERR;\r\ngoto flush_send;\r\n}\r\nif (sqp->s_last == sqp->s_cur) {\r\nif (++sqp->s_cur >= sqp->s_size)\r\nsqp->s_cur = 0;\r\n}\r\nspin_unlock_irqrestore(&sqp->s_lock, flags);\r\nif (!qp || !(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_RECV_OK)) {\r\ndev->n_pkt_drops++;\r\nif (sqp->ibqp.qp_type == IB_QPT_RC)\r\nsend_status = IB_WC_RETRY_EXC_ERR;\r\nelse\r\nsend_status = IB_WC_SUCCESS;\r\ngoto serr;\r\n}\r\nmemset(&wc, 0, sizeof wc);\r\nsend_status = IB_WC_SUCCESS;\r\nsqp->s_sge.sge = wqe->sg_list[0];\r\nsqp->s_sge.sg_list = wqe->sg_list + 1;\r\nsqp->s_sge.num_sge = wqe->wr.num_sge;\r\nsqp->s_len = wqe->length;\r\nswitch (wqe->wr.opcode) {\r\ncase IB_WR_SEND_WITH_IMM:\r\nwc.wc_flags = IB_WC_WITH_IMM;\r\nwc.ex.imm_data = wqe->wr.ex.imm_data;\r\ncase IB_WR_SEND:\r\nif (!ipath_get_rwqe(qp, 0))\r\ngoto rnr_nak;\r\nbreak;\r\ncase IB_WR_RDMA_WRITE_WITH_IMM:\r\nif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_WRITE)))\r\ngoto inv_err;\r\nwc.wc_flags = IB_WC_WITH_IMM;\r\nwc.ex.imm_data = wqe->wr.ex.imm_data;\r\nif (!ipath_get_rwqe(qp, 1))\r\ngoto rnr_nak;\r\ncase IB_WR_RDMA_WRITE:\r\nif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_WRITE)))\r\ngoto inv_err;\r\nif (wqe->length == 0)\r\nbreak;\r\nif (unlikely(!ipath_rkey_ok(qp, &qp->r_sge, wqe->length,\r\nwqe->wr.wr.rdma.remote_addr,\r\nwqe->wr.wr.rdma.rkey,\r\nIB_ACCESS_REMOTE_WRITE)))\r\ngoto acc_err;\r\nbreak;\r\ncase IB_WR_RDMA_READ:\r\nif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_READ)))\r\ngoto inv_err;\r\nif (unlikely(!ipath_rkey_ok(qp, &sqp->s_sge, wqe->length,\r\nwqe->wr.wr.rdma.remote_addr,\r\nwqe->wr.wr.rdma.rkey,\r\nIB_ACCESS_REMOTE_READ)))\r\ngoto acc_err;\r\nqp->r_sge.sge = wqe->sg_list[0];\r\nqp->r_sge.sg_list = wqe->sg_list + 1;\r\nqp->r_sge.num_sge = wqe->wr.num_sge;\r\nbreak;\r\ncase IB_WR_ATOMIC_CMP_AND_SWP:\r\ncase IB_WR_ATOMIC_FETCH_AND_ADD:\r\nif (unlikely(!(qp->qp_access_flags & IB_ACCESS_REMOTE_ATOMIC)))\r\ngoto inv_err;\r\nif (unlikely(!ipath_rkey_ok(qp, &qp->r_sge, sizeof(u64),\r\nwqe->wr.wr.atomic.remote_addr,\r\nwqe->wr.wr.atomic.rkey,\r\nIB_ACCESS_REMOTE_ATOMIC)))\r\ngoto acc_err;\r\nmaddr = (atomic64_t *) qp->r_sge.sge.vaddr;\r\nsdata = wqe->wr.wr.atomic.compare_add;\r\n*(u64 *) sqp->s_sge.sge.vaddr =\r\n(wqe->wr.opcode == IB_WR_ATOMIC_FETCH_AND_ADD) ?\r\n(u64) atomic64_add_return(sdata, maddr) - sdata :\r\n(u64) cmpxchg((u64 *) qp->r_sge.sge.vaddr,\r\nsdata, wqe->wr.wr.atomic.swap);\r\ngoto send_comp;\r\ndefault:\r\nsend_status = IB_WC_LOC_QP_OP_ERR;\r\ngoto serr;\r\n}\r\nsge = &sqp->s_sge.sge;\r\nwhile (sqp->s_len) {\r\nu32 len = sqp->s_len;\r\nif (len > sge->length)\r\nlen = sge->length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nipath_copy_sge(&qp->r_sge, sge->vaddr, len);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (--sqp->s_sge.num_sge)\r\n*sge = *sqp->s_sge.sg_list++;\r\n} else if (sge->length == 0 && sge->mr != NULL) {\r\nif (++sge->n >= IPATH_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\nsqp->s_len -= len;\r\n}\r\nif (!test_and_clear_bit(IPATH_R_WRID_VALID, &qp->r_aflags))\r\ngoto send_comp;\r\nif (wqe->wr.opcode == IB_WR_RDMA_WRITE_WITH_IMM)\r\nwc.opcode = IB_WC_RECV_RDMA_WITH_IMM;\r\nelse\r\nwc.opcode = IB_WC_RECV;\r\nwc.wr_id = qp->r_wr_id;\r\nwc.status = IB_WC_SUCCESS;\r\nwc.byte_len = wqe->length;\r\nwc.qp = &qp->ibqp;\r\nwc.src_qp = qp->remote_qpn;\r\nwc.slid = qp->remote_ah_attr.dlid;\r\nwc.sl = qp->remote_ah_attr.sl;\r\nwc.port_num = 1;\r\nipath_cq_enter(to_icq(qp->ibqp.recv_cq), &wc,\r\nwqe->wr.send_flags & IB_SEND_SOLICITED);\r\nsend_comp:\r\nspin_lock_irqsave(&sqp->s_lock, flags);\r\nflush_send:\r\nsqp->s_rnr_retry = sqp->s_rnr_retry_cnt;\r\nipath_send_complete(sqp, wqe, send_status);\r\ngoto again;\r\nrnr_nak:\r\nif (qp->ibqp.qp_type == IB_QPT_UC)\r\ngoto send_comp;\r\nif (sqp->s_rnr_retry == 0) {\r\nsend_status = IB_WC_RNR_RETRY_EXC_ERR;\r\ngoto serr;\r\n}\r\nif (sqp->s_rnr_retry_cnt < 7)\r\nsqp->s_rnr_retry--;\r\nspin_lock_irqsave(&sqp->s_lock, flags);\r\nif (!(ib_ipath_state_ops[sqp->state] & IPATH_PROCESS_RECV_OK))\r\ngoto clr_busy;\r\nsqp->s_flags |= IPATH_S_WAITING;\r\ndev->n_rnr_naks++;\r\nsqp->s_rnr_timeout = ib_ipath_rnr_table[qp->r_min_rnr_timer];\r\nipath_insert_rnr_queue(sqp);\r\ngoto clr_busy;\r\ninv_err:\r\nsend_status = IB_WC_REM_INV_REQ_ERR;\r\nwc.status = IB_WC_LOC_QP_OP_ERR;\r\ngoto err;\r\nacc_err:\r\nsend_status = IB_WC_REM_ACCESS_ERR;\r\nwc.status = IB_WC_LOC_PROT_ERR;\r\nerr:\r\nipath_rc_error(qp, wc.status);\r\nserr:\r\nspin_lock_irqsave(&sqp->s_lock, flags);\r\nipath_send_complete(sqp, wqe, send_status);\r\nif (sqp->ibqp.qp_type == IB_QPT_RC) {\r\nint lastwqe = ipath_error_qp(sqp, IB_WC_WR_FLUSH_ERR);\r\nsqp->s_flags &= ~IPATH_S_BUSY;\r\nspin_unlock_irqrestore(&sqp->s_lock, flags);\r\nif (lastwqe) {\r\nstruct ib_event ev;\r\nev.device = sqp->ibqp.device;\r\nev.element.qp = &sqp->ibqp;\r\nev.event = IB_EVENT_QP_LAST_WQE_REACHED;\r\nsqp->ibqp.event_handler(&ev, sqp->ibqp.qp_context);\r\n}\r\ngoto done;\r\n}\r\nclr_busy:\r\nsqp->s_flags &= ~IPATH_S_BUSY;\r\nunlock:\r\nspin_unlock_irqrestore(&sqp->s_lock, flags);\r\ndone:\r\nif (qp && atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\nstatic void want_buffer(struct ipath_devdata *dd, struct ipath_qp *qp)\r\n{\r\nif (!(dd->ipath_flags & IPATH_HAS_SEND_DMA) ||\r\nqp->ibqp.qp_type == IB_QPT_SMI) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&dd->ipath_sendctrl_lock, flags);\r\ndd->ipath_sendctrl |= INFINIPATH_S_PIOINTBUFAVAIL;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_sendctrl,\r\ndd->ipath_sendctrl);\r\nipath_read_kreg64(dd, dd->ipath_kregs->kr_scratch);\r\nspin_unlock_irqrestore(&dd->ipath_sendctrl_lock, flags);\r\n}\r\n}\r\nstatic int ipath_no_bufs_available(struct ipath_qp *qp,\r\nstruct ipath_ibdev *dev)\r\n{\r\nunsigned long flags;\r\nint ret = 1;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (ib_ipath_state_ops[qp->state] & IPATH_PROCESS_SEND_OK) {\r\ndev->n_piowait++;\r\nqp->s_flags |= IPATH_S_WAITING;\r\nqp->s_flags &= ~IPATH_S_BUSY;\r\nspin_lock(&dev->pending_lock);\r\nif (list_empty(&qp->piowait))\r\nlist_add_tail(&qp->piowait, &dev->piowait);\r\nspin_unlock(&dev->pending_lock);\r\n} else\r\nret = 0;\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (ret)\r\nwant_buffer(dev->dd, qp);\r\nreturn ret;\r\n}\r\nu32 ipath_make_grh(struct ipath_ibdev *dev, struct ib_grh *hdr,\r\nstruct ib_global_route *grh, u32 hwords, u32 nwords)\r\n{\r\nhdr->version_tclass_flow =\r\ncpu_to_be32((6 << 28) |\r\n(grh->traffic_class << 20) |\r\ngrh->flow_label);\r\nhdr->paylen = cpu_to_be16((hwords - 2 + nwords + SIZE_OF_CRC) << 2);\r\nhdr->next_hdr = 0x1B;\r\nhdr->hop_limit = grh->hop_limit;\r\nhdr->sgid.global.subnet_prefix = dev->gid_prefix;\r\nhdr->sgid.global.interface_id = dev->dd->ipath_guid;\r\nhdr->dgid = grh->dgid;\r\nreturn sizeof(struct ib_grh) / sizeof(u32);\r\n}\r\nvoid ipath_make_ruc_header(struct ipath_ibdev *dev, struct ipath_qp *qp,\r\nstruct ipath_other_headers *ohdr,\r\nu32 bth0, u32 bth2)\r\n{\r\nu16 lrh0;\r\nu32 nwords;\r\nu32 extra_bytes;\r\nextra_bytes = -qp->s_cur_size & 3;\r\nnwords = (qp->s_cur_size + extra_bytes) >> 2;\r\nlrh0 = IPATH_LRH_BTH;\r\nif (unlikely(qp->remote_ah_attr.ah_flags & IB_AH_GRH)) {\r\nqp->s_hdrwords += ipath_make_grh(dev, &qp->s_hdr.u.l.grh,\r\n&qp->remote_ah_attr.grh,\r\nqp->s_hdrwords, nwords);\r\nlrh0 = IPATH_LRH_GRH;\r\n}\r\nlrh0 |= qp->remote_ah_attr.sl << 4;\r\nqp->s_hdr.lrh[0] = cpu_to_be16(lrh0);\r\nqp->s_hdr.lrh[1] = cpu_to_be16(qp->remote_ah_attr.dlid);\r\nqp->s_hdr.lrh[2] = cpu_to_be16(qp->s_hdrwords + nwords + SIZE_OF_CRC);\r\nqp->s_hdr.lrh[3] = cpu_to_be16(dev->dd->ipath_lid |\r\nqp->remote_ah_attr.src_path_bits);\r\nbth0 |= ipath_get_pkey(dev->dd, qp->s_pkey_index);\r\nbth0 |= extra_bytes << 20;\r\nohdr->bth[0] = cpu_to_be32(bth0 | (1 << 22));\r\nohdr->bth[1] = cpu_to_be32(qp->remote_qpn);\r\nohdr->bth[2] = cpu_to_be32(bth2);\r\n}\r\nvoid ipath_do_send(unsigned long data)\r\n{\r\nstruct ipath_qp *qp = (struct ipath_qp *)data;\r\nstruct ipath_ibdev *dev = to_idev(qp->ibqp.device);\r\nint (*make_req)(struct ipath_qp *qp);\r\nunsigned long flags;\r\nif ((qp->ibqp.qp_type == IB_QPT_RC ||\r\nqp->ibqp.qp_type == IB_QPT_UC) &&\r\nqp->remote_ah_attr.dlid == dev->dd->ipath_lid) {\r\nipath_ruc_loopback(qp);\r\ngoto bail;\r\n}\r\nif (qp->ibqp.qp_type == IB_QPT_RC)\r\nmake_req = ipath_make_rc_req;\r\nelse if (qp->ibqp.qp_type == IB_QPT_UC)\r\nmake_req = ipath_make_uc_req;\r\nelse\r\nmake_req = ipath_make_ud_req;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif ((qp->s_flags & (IPATH_S_BUSY | IPATH_S_ANY_WAIT)) ||\r\n!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_OR_FLUSH_SEND)) {\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\ngoto bail;\r\n}\r\nqp->s_flags |= IPATH_S_BUSY;\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nagain:\r\nif (qp->s_hdrwords != 0) {\r\nif (ipath_verbs_send(qp, &qp->s_hdr, qp->s_hdrwords,\r\nqp->s_cur_sge, qp->s_cur_size)) {\r\nif (ipath_no_bufs_available(qp, dev))\r\ngoto bail;\r\n}\r\ndev->n_unicast_xmit++;\r\nqp->s_hdrwords = 0;\r\n}\r\nif (make_req(qp))\r\ngoto again;\r\nbail:;\r\n}\r\nvoid ipath_send_complete(struct ipath_qp *qp, struct ipath_swqe *wqe,\r\nenum ib_wc_status status)\r\n{\r\nu32 old_last, last;\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_OR_FLUSH_SEND))\r\nreturn;\r\nif (!(qp->s_flags & IPATH_S_SIGNAL_REQ_WR) ||\r\n(wqe->wr.send_flags & IB_SEND_SIGNALED) ||\r\nstatus != IB_WC_SUCCESS) {\r\nstruct ib_wc wc;\r\nmemset(&wc, 0, sizeof wc);\r\nwc.wr_id = wqe->wr.wr_id;\r\nwc.status = status;\r\nwc.opcode = ib_ipath_wc_opcode[wqe->wr.opcode];\r\nwc.qp = &qp->ibqp;\r\nif (status == IB_WC_SUCCESS)\r\nwc.byte_len = wqe->length;\r\nipath_cq_enter(to_icq(qp->ibqp.send_cq), &wc,\r\nstatus != IB_WC_SUCCESS);\r\n}\r\nold_last = last = qp->s_last;\r\nif (++last >= qp->s_size)\r\nlast = 0;\r\nqp->s_last = last;\r\nif (qp->s_cur == old_last)\r\nqp->s_cur = last;\r\nif (qp->s_tail == old_last)\r\nqp->s_tail = last;\r\nif (qp->state == IB_QPS_SQD && last == qp->s_cur)\r\nqp->s_draining = 0;\r\n}
