int radeon_gart_table_ram_alloc(struct radeon_device *rdev)\r\n{\r\nvoid *ptr;\r\nptr = pci_alloc_consistent(rdev->pdev, rdev->gart.table_size,\r\n&rdev->gart.table_addr);\r\nif (ptr == NULL) {\r\nreturn -ENOMEM;\r\n}\r\n#ifdef CONFIG_X86\r\nif (rdev->family == CHIP_RS400 || rdev->family == CHIP_RS480 ||\r\nrdev->family == CHIP_RS690 || rdev->family == CHIP_RS740) {\r\nset_memory_uc((unsigned long)ptr,\r\nrdev->gart.table_size >> PAGE_SHIFT);\r\n}\r\n#endif\r\nrdev->gart.ptr = ptr;\r\nmemset((void *)rdev->gart.ptr, 0, rdev->gart.table_size);\r\nreturn 0;\r\n}\r\nvoid radeon_gart_table_ram_free(struct radeon_device *rdev)\r\n{\r\nif (rdev->gart.ptr == NULL) {\r\nreturn;\r\n}\r\n#ifdef CONFIG_X86\r\nif (rdev->family == CHIP_RS400 || rdev->family == CHIP_RS480 ||\r\nrdev->family == CHIP_RS690 || rdev->family == CHIP_RS740) {\r\nset_memory_wb((unsigned long)rdev->gart.ptr,\r\nrdev->gart.table_size >> PAGE_SHIFT);\r\n}\r\n#endif\r\npci_free_consistent(rdev->pdev, rdev->gart.table_size,\r\n(void *)rdev->gart.ptr,\r\nrdev->gart.table_addr);\r\nrdev->gart.ptr = NULL;\r\nrdev->gart.table_addr = 0;\r\n}\r\nint radeon_gart_table_vram_alloc(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (rdev->gart.robj == NULL) {\r\nr = radeon_bo_create(rdev, rdev->gart.table_size,\r\nPAGE_SIZE, true, RADEON_GEM_DOMAIN_VRAM,\r\n&rdev->gart.robj);\r\nif (r) {\r\nreturn r;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint radeon_gart_table_vram_pin(struct radeon_device *rdev)\r\n{\r\nuint64_t gpu_addr;\r\nint r;\r\nr = radeon_bo_reserve(rdev->gart.robj, false);\r\nif (unlikely(r != 0))\r\nreturn r;\r\nr = radeon_bo_pin(rdev->gart.robj,\r\nRADEON_GEM_DOMAIN_VRAM, &gpu_addr);\r\nif (r) {\r\nradeon_bo_unreserve(rdev->gart.robj);\r\nreturn r;\r\n}\r\nr = radeon_bo_kmap(rdev->gart.robj, &rdev->gart.ptr);\r\nif (r)\r\nradeon_bo_unpin(rdev->gart.robj);\r\nradeon_bo_unreserve(rdev->gart.robj);\r\nrdev->gart.table_addr = gpu_addr;\r\nreturn r;\r\n}\r\nvoid radeon_gart_table_vram_unpin(struct radeon_device *rdev)\r\n{\r\nint r;\r\nif (rdev->gart.robj == NULL) {\r\nreturn;\r\n}\r\nr = radeon_bo_reserve(rdev->gart.robj, false);\r\nif (likely(r == 0)) {\r\nradeon_bo_kunmap(rdev->gart.robj);\r\nradeon_bo_unpin(rdev->gart.robj);\r\nradeon_bo_unreserve(rdev->gart.robj);\r\nrdev->gart.ptr = NULL;\r\n}\r\n}\r\nvoid radeon_gart_table_vram_free(struct radeon_device *rdev)\r\n{\r\nif (rdev->gart.robj == NULL) {\r\nreturn;\r\n}\r\nradeon_gart_table_vram_unpin(rdev);\r\nradeon_bo_unref(&rdev->gart.robj);\r\n}\r\nvoid radeon_gart_unbind(struct radeon_device *rdev, unsigned offset,\r\nint pages)\r\n{\r\nunsigned t;\r\nunsigned p;\r\nint i, j;\r\nu64 page_base;\r\nif (!rdev->gart.ready) {\r\nWARN(1, "trying to unbind memory from uninitialized GART !\n");\r\nreturn;\r\n}\r\nt = offset / RADEON_GPU_PAGE_SIZE;\r\np = t / (PAGE_SIZE / RADEON_GPU_PAGE_SIZE);\r\nfor (i = 0; i < pages; i++, p++) {\r\nif (rdev->gart.pages[p]) {\r\nrdev->gart.pages[p] = NULL;\r\nrdev->gart.pages_addr[p] = rdev->dummy_page.addr;\r\npage_base = rdev->gart.pages_addr[p];\r\nfor (j = 0; j < (PAGE_SIZE / RADEON_GPU_PAGE_SIZE); j++, t++) {\r\nif (rdev->gart.ptr) {\r\nradeon_gart_set_page(rdev, t, page_base);\r\n}\r\npage_base += RADEON_GPU_PAGE_SIZE;\r\n}\r\n}\r\n}\r\nmb();\r\nradeon_gart_tlb_flush(rdev);\r\n}\r\nint radeon_gart_bind(struct radeon_device *rdev, unsigned offset,\r\nint pages, struct page **pagelist, dma_addr_t *dma_addr)\r\n{\r\nunsigned t;\r\nunsigned p;\r\nuint64_t page_base;\r\nint i, j;\r\nif (!rdev->gart.ready) {\r\nWARN(1, "trying to bind memory to uninitialized GART !\n");\r\nreturn -EINVAL;\r\n}\r\nt = offset / RADEON_GPU_PAGE_SIZE;\r\np = t / (PAGE_SIZE / RADEON_GPU_PAGE_SIZE);\r\nfor (i = 0; i < pages; i++, p++) {\r\nrdev->gart.pages_addr[p] = dma_addr[i];\r\nrdev->gart.pages[p] = pagelist[i];\r\nif (rdev->gart.ptr) {\r\npage_base = rdev->gart.pages_addr[p];\r\nfor (j = 0; j < (PAGE_SIZE / RADEON_GPU_PAGE_SIZE); j++, t++) {\r\nradeon_gart_set_page(rdev, t, page_base);\r\npage_base += RADEON_GPU_PAGE_SIZE;\r\n}\r\n}\r\n}\r\nmb();\r\nradeon_gart_tlb_flush(rdev);\r\nreturn 0;\r\n}\r\nvoid radeon_gart_restore(struct radeon_device *rdev)\r\n{\r\nint i, j, t;\r\nu64 page_base;\r\nif (!rdev->gart.ptr) {\r\nreturn;\r\n}\r\nfor (i = 0, t = 0; i < rdev->gart.num_cpu_pages; i++) {\r\npage_base = rdev->gart.pages_addr[i];\r\nfor (j = 0; j < (PAGE_SIZE / RADEON_GPU_PAGE_SIZE); j++, t++) {\r\nradeon_gart_set_page(rdev, t, page_base);\r\npage_base += RADEON_GPU_PAGE_SIZE;\r\n}\r\n}\r\nmb();\r\nradeon_gart_tlb_flush(rdev);\r\n}\r\nint radeon_gart_init(struct radeon_device *rdev)\r\n{\r\nint r, i;\r\nif (rdev->gart.pages) {\r\nreturn 0;\r\n}\r\nif (PAGE_SIZE < RADEON_GPU_PAGE_SIZE) {\r\nDRM_ERROR("Page size is smaller than GPU page size!\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_dummy_page_init(rdev);\r\nif (r)\r\nreturn r;\r\nrdev->gart.num_cpu_pages = rdev->mc.gtt_size / PAGE_SIZE;\r\nrdev->gart.num_gpu_pages = rdev->mc.gtt_size / RADEON_GPU_PAGE_SIZE;\r\nDRM_INFO("GART: num cpu pages %u, num gpu pages %u\n",\r\nrdev->gart.num_cpu_pages, rdev->gart.num_gpu_pages);\r\nrdev->gart.pages = kzalloc(sizeof(void *) * rdev->gart.num_cpu_pages,\r\nGFP_KERNEL);\r\nif (rdev->gart.pages == NULL) {\r\nradeon_gart_fini(rdev);\r\nreturn -ENOMEM;\r\n}\r\nrdev->gart.pages_addr = kzalloc(sizeof(dma_addr_t) *\r\nrdev->gart.num_cpu_pages, GFP_KERNEL);\r\nif (rdev->gart.pages_addr == NULL) {\r\nradeon_gart_fini(rdev);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < rdev->gart.num_cpu_pages; i++) {\r\nrdev->gart.pages_addr[i] = rdev->dummy_page.addr;\r\n}\r\nreturn 0;\r\n}\r\nvoid radeon_gart_fini(struct radeon_device *rdev)\r\n{\r\nif (rdev->gart.pages && rdev->gart.pages_addr && rdev->gart.ready) {\r\nradeon_gart_unbind(rdev, 0, rdev->gart.num_cpu_pages);\r\n}\r\nrdev->gart.ready = false;\r\nkfree(rdev->gart.pages);\r\nkfree(rdev->gart.pages_addr);\r\nrdev->gart.pages = NULL;\r\nrdev->gart.pages_addr = NULL;\r\nradeon_dummy_page_fini(rdev);\r\n}\r\nint radeon_vm_manager_init(struct radeon_device *rdev)\r\n{\r\nint r;\r\nrdev->vm_manager.enabled = false;\r\nr = radeon_sa_bo_manager_init(rdev, &rdev->vm_manager.sa_manager,\r\nrdev->vm_manager.max_pfn * 8,\r\nRADEON_GEM_DOMAIN_VRAM);\r\nif (r) {\r\ndev_err(rdev->dev, "failed to allocate vm bo (%dKB)\n",\r\n(rdev->vm_manager.max_pfn * 8) >> 10);\r\nreturn r;\r\n}\r\nr = rdev->vm_manager.funcs->init(rdev);\r\nif (r == 0)\r\nrdev->vm_manager.enabled = true;\r\nreturn r;\r\n}\r\nstatic void radeon_vm_unbind_locked(struct radeon_device *rdev,\r\nstruct radeon_vm *vm)\r\n{\r\nstruct radeon_bo_va *bo_va;\r\nif (vm->id == -1) {\r\nreturn;\r\n}\r\nif (vm->fence) {\r\nradeon_fence_wait(vm->fence, false);\r\nradeon_fence_unref(&vm->fence);\r\n}\r\nrdev->vm_manager.funcs->unbind(rdev, vm);\r\nrdev->vm_manager.use_bitmap &= ~(1 << vm->id);\r\nlist_del_init(&vm->list);\r\nvm->id = -1;\r\nradeon_sa_bo_free(rdev, &vm->sa_bo);\r\nvm->pt = NULL;\r\nlist_for_each_entry(bo_va, &vm->va, vm_list) {\r\nbo_va->valid = false;\r\n}\r\n}\r\nvoid radeon_vm_manager_fini(struct radeon_device *rdev)\r\n{\r\nif (rdev->vm_manager.sa_manager.bo == NULL)\r\nreturn;\r\nradeon_vm_manager_suspend(rdev);\r\nrdev->vm_manager.funcs->fini(rdev);\r\nradeon_sa_bo_manager_fini(rdev, &rdev->vm_manager.sa_manager);\r\nrdev->vm_manager.enabled = false;\r\n}\r\nint radeon_vm_manager_start(struct radeon_device *rdev)\r\n{\r\nif (rdev->vm_manager.sa_manager.bo == NULL) {\r\nreturn -EINVAL;\r\n}\r\nreturn radeon_sa_bo_manager_start(rdev, &rdev->vm_manager.sa_manager);\r\n}\r\nint radeon_vm_manager_suspend(struct radeon_device *rdev)\r\n{\r\nstruct radeon_vm *vm, *tmp;\r\nradeon_mutex_lock(&rdev->cs_mutex);\r\nlist_for_each_entry_safe(vm, tmp, &rdev->vm_manager.lru_vm, list) {\r\nradeon_vm_unbind_locked(rdev, vm);\r\n}\r\nrdev->vm_manager.funcs->fini(rdev);\r\nradeon_mutex_unlock(&rdev->cs_mutex);\r\nreturn radeon_sa_bo_manager_suspend(rdev, &rdev->vm_manager.sa_manager);\r\n}\r\nvoid radeon_vm_unbind(struct radeon_device *rdev, struct radeon_vm *vm)\r\n{\r\nmutex_lock(&vm->mutex);\r\nradeon_vm_unbind_locked(rdev, vm);\r\nmutex_unlock(&vm->mutex);\r\n}\r\nint radeon_vm_bind(struct radeon_device *rdev, struct radeon_vm *vm)\r\n{\r\nstruct radeon_vm *vm_evict;\r\nunsigned i;\r\nint id = -1, r;\r\nif (vm == NULL) {\r\nreturn -EINVAL;\r\n}\r\nif (vm->id != -1) {\r\nlist_del_init(&vm->list);\r\nlist_add_tail(&vm->list, &rdev->vm_manager.lru_vm);\r\nreturn 0;\r\n}\r\nretry:\r\nr = radeon_sa_bo_new(rdev, &rdev->vm_manager.sa_manager, &vm->sa_bo,\r\nRADEON_GPU_PAGE_ALIGN(vm->last_pfn * 8),\r\nRADEON_GPU_PAGE_SIZE);\r\nif (r) {\r\nif (list_empty(&rdev->vm_manager.lru_vm)) {\r\nreturn r;\r\n}\r\nvm_evict = list_first_entry(&rdev->vm_manager.lru_vm, struct radeon_vm, list);\r\nradeon_vm_unbind(rdev, vm_evict);\r\ngoto retry;\r\n}\r\nvm->pt = rdev->vm_manager.sa_manager.cpu_ptr;\r\nvm->pt += (vm->sa_bo.offset >> 3);\r\nvm->pt_gpu_addr = rdev->vm_manager.sa_manager.gpu_addr;\r\nvm->pt_gpu_addr += vm->sa_bo.offset;\r\nmemset(vm->pt, 0, RADEON_GPU_PAGE_ALIGN(vm->last_pfn * 8));\r\nretry_id:\r\nfor (i = 0; i < rdev->vm_manager.nvm; i++) {\r\nif (!(rdev->vm_manager.use_bitmap & (1 << i))) {\r\nid = i;\r\nbreak;\r\n}\r\n}\r\nif (id == -1) {\r\nvm_evict = list_first_entry(&rdev->vm_manager.lru_vm, struct radeon_vm, list);\r\nradeon_vm_unbind(rdev, vm_evict);\r\ngoto retry_id;\r\n}\r\nr = rdev->vm_manager.funcs->bind(rdev, vm, id);\r\nif (r) {\r\nradeon_sa_bo_free(rdev, &vm->sa_bo);\r\nreturn r;\r\n}\r\nrdev->vm_manager.use_bitmap |= 1 << id;\r\nvm->id = id;\r\nlist_add_tail(&vm->list, &rdev->vm_manager.lru_vm);\r\nreturn radeon_vm_bo_update_pte(rdev, vm, rdev->ib_pool.sa_manager.bo,\r\n&rdev->ib_pool.sa_manager.bo->tbo.mem);\r\n}\r\nint radeon_vm_bo_add(struct radeon_device *rdev,\r\nstruct radeon_vm *vm,\r\nstruct radeon_bo *bo,\r\nuint64_t offset,\r\nuint32_t flags)\r\n{\r\nstruct radeon_bo_va *bo_va, *tmp;\r\nstruct list_head *head;\r\nuint64_t size = radeon_bo_size(bo), last_offset = 0;\r\nunsigned last_pfn;\r\nbo_va = kzalloc(sizeof(struct radeon_bo_va), GFP_KERNEL);\r\nif (bo_va == NULL) {\r\nreturn -ENOMEM;\r\n}\r\nbo_va->vm = vm;\r\nbo_va->bo = bo;\r\nbo_va->soffset = offset;\r\nbo_va->eoffset = offset + size;\r\nbo_va->flags = flags;\r\nbo_va->valid = false;\r\nINIT_LIST_HEAD(&bo_va->bo_list);\r\nINIT_LIST_HEAD(&bo_va->vm_list);\r\nif (bo_va->soffset >= bo_va->eoffset) {\r\nkfree(bo_va);\r\nreturn -EINVAL;\r\n}\r\nlast_pfn = bo_va->eoffset / RADEON_GPU_PAGE_SIZE;\r\nif (last_pfn > rdev->vm_manager.max_pfn) {\r\nkfree(bo_va);\r\ndev_err(rdev->dev, "va above limit (0x%08X > 0x%08X)\n",\r\nlast_pfn, rdev->vm_manager.max_pfn);\r\nreturn -EINVAL;\r\n}\r\nmutex_lock(&vm->mutex);\r\nif (last_pfn > vm->last_pfn) {\r\nunsigned align = ((32 << 20) >> 12) - 1;\r\nradeon_mutex_lock(&rdev->cs_mutex);\r\nradeon_vm_unbind_locked(rdev, vm);\r\nradeon_mutex_unlock(&rdev->cs_mutex);\r\nvm->last_pfn = (last_pfn + align) & ~align;\r\n}\r\nhead = &vm->va;\r\nlast_offset = 0;\r\nlist_for_each_entry(tmp, &vm->va, vm_list) {\r\nif (bo_va->soffset >= last_offset && bo_va->eoffset < tmp->soffset) {\r\nbreak;\r\n}\r\nif (bo_va->soffset >= tmp->soffset && bo_va->soffset < tmp->eoffset) {\r\ndev_err(rdev->dev, "bo %p va 0x%08X conflict with (bo %p 0x%08X 0x%08X)\n",\r\nbo, (unsigned)bo_va->soffset, tmp->bo,\r\n(unsigned)tmp->soffset, (unsigned)tmp->eoffset);\r\nkfree(bo_va);\r\nmutex_unlock(&vm->mutex);\r\nreturn -EINVAL;\r\n}\r\nlast_offset = tmp->eoffset;\r\nhead = &tmp->vm_list;\r\n}\r\nlist_add(&bo_va->vm_list, head);\r\nlist_add_tail(&bo_va->bo_list, &bo->va);\r\nmutex_unlock(&vm->mutex);\r\nreturn 0;\r\n}\r\nstatic u64 radeon_vm_get_addr(struct radeon_device *rdev,\r\nstruct ttm_mem_reg *mem,\r\nunsigned pfn)\r\n{\r\nu64 addr = 0;\r\nswitch (mem->mem_type) {\r\ncase TTM_PL_VRAM:\r\naddr = (mem->start << PAGE_SHIFT);\r\naddr += pfn * RADEON_GPU_PAGE_SIZE;\r\naddr += rdev->vm_manager.vram_base_offset;\r\nbreak;\r\ncase TTM_PL_TT:\r\naddr = mem->start << PAGE_SHIFT;\r\naddr += pfn * RADEON_GPU_PAGE_SIZE;\r\naddr = addr >> PAGE_SHIFT;\r\naddr = rdev->gart.pages_addr[addr];\r\naddr += (pfn * RADEON_GPU_PAGE_SIZE) & (~PAGE_MASK);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn addr;\r\n}\r\nint radeon_vm_bo_update_pte(struct radeon_device *rdev,\r\nstruct radeon_vm *vm,\r\nstruct radeon_bo *bo,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nstruct radeon_bo_va *bo_va;\r\nunsigned ngpu_pages, i;\r\nuint64_t addr = 0, pfn;\r\nuint32_t flags;\r\nif (vm->id == -1)\r\nreturn 0;;\r\nbo_va = radeon_bo_va(bo, vm);\r\nif (bo_va == NULL) {\r\ndev_err(rdev->dev, "bo %p not in vm %p\n", bo, vm);\r\nreturn -EINVAL;\r\n}\r\nif (bo_va->valid)\r\nreturn 0;\r\nngpu_pages = radeon_bo_ngpu_pages(bo);\r\nbo_va->flags &= ~RADEON_VM_PAGE_VALID;\r\nbo_va->flags &= ~RADEON_VM_PAGE_SYSTEM;\r\nif (mem) {\r\nif (mem->mem_type != TTM_PL_SYSTEM) {\r\nbo_va->flags |= RADEON_VM_PAGE_VALID;\r\nbo_va->valid = true;\r\n}\r\nif (mem->mem_type == TTM_PL_TT) {\r\nbo_va->flags |= RADEON_VM_PAGE_SYSTEM;\r\n}\r\n}\r\npfn = bo_va->soffset / RADEON_GPU_PAGE_SIZE;\r\nflags = rdev->vm_manager.funcs->page_flags(rdev, bo_va->vm, bo_va->flags);\r\nfor (i = 0, addr = 0; i < ngpu_pages; i++) {\r\nif (mem && bo_va->valid) {\r\naddr = radeon_vm_get_addr(rdev, mem, i);\r\n}\r\nrdev->vm_manager.funcs->set_page(rdev, bo_va->vm, i + pfn, addr, flags);\r\n}\r\nrdev->vm_manager.funcs->tlb_flush(rdev, bo_va->vm);\r\nreturn 0;\r\n}\r\nint radeon_vm_bo_rmv(struct radeon_device *rdev,\r\nstruct radeon_vm *vm,\r\nstruct radeon_bo *bo)\r\n{\r\nstruct radeon_bo_va *bo_va;\r\nbo_va = radeon_bo_va(bo, vm);\r\nif (bo_va == NULL)\r\nreturn 0;\r\nmutex_lock(&vm->mutex);\r\nradeon_mutex_lock(&rdev->cs_mutex);\r\nradeon_vm_bo_update_pte(rdev, vm, bo, NULL);\r\nradeon_mutex_unlock(&rdev->cs_mutex);\r\nlist_del(&bo_va->vm_list);\r\nmutex_unlock(&vm->mutex);\r\nlist_del(&bo_va->bo_list);\r\nkfree(bo_va);\r\nreturn 0;\r\n}\r\nvoid radeon_vm_bo_invalidate(struct radeon_device *rdev,\r\nstruct radeon_bo *bo)\r\n{\r\nstruct radeon_bo_va *bo_va;\r\nBUG_ON(!atomic_read(&bo->tbo.reserved));\r\nlist_for_each_entry(bo_va, &bo->va, bo_list) {\r\nbo_va->valid = false;\r\n}\r\n}\r\nint radeon_vm_init(struct radeon_device *rdev, struct radeon_vm *vm)\r\n{\r\nint r;\r\nvm->id = -1;\r\nvm->fence = NULL;\r\nmutex_init(&vm->mutex);\r\nINIT_LIST_HEAD(&vm->list);\r\nINIT_LIST_HEAD(&vm->va);\r\nvm->last_pfn = 0;\r\nr = radeon_vm_bo_add(rdev, vm, rdev->ib_pool.sa_manager.bo, 0,\r\nRADEON_VM_PAGE_READABLE | RADEON_VM_PAGE_SNOOPED);\r\nreturn r;\r\n}\r\nvoid radeon_vm_fini(struct radeon_device *rdev, struct radeon_vm *vm)\r\n{\r\nstruct radeon_bo_va *bo_va, *tmp;\r\nint r;\r\nmutex_lock(&vm->mutex);\r\nradeon_mutex_lock(&rdev->cs_mutex);\r\nradeon_vm_unbind_locked(rdev, vm);\r\nradeon_mutex_unlock(&rdev->cs_mutex);\r\nr = radeon_bo_reserve(rdev->ib_pool.sa_manager.bo, false);\r\nif (!r) {\r\nbo_va = radeon_bo_va(rdev->ib_pool.sa_manager.bo, vm);\r\nlist_del_init(&bo_va->bo_list);\r\nlist_del_init(&bo_va->vm_list);\r\nradeon_bo_unreserve(rdev->ib_pool.sa_manager.bo);\r\nkfree(bo_va);\r\n}\r\nif (!list_empty(&vm->va)) {\r\ndev_err(rdev->dev, "still active bo inside vm\n");\r\n}\r\nlist_for_each_entry_safe(bo_va, tmp, &vm->va, vm_list) {\r\nlist_del_init(&bo_va->vm_list);\r\nr = radeon_bo_reserve(bo_va->bo, false);\r\nif (!r) {\r\nlist_del_init(&bo_va->bo_list);\r\nradeon_bo_unreserve(bo_va->bo);\r\nkfree(bo_va);\r\n}\r\n}\r\nmutex_unlock(&vm->mutex);\r\n}
