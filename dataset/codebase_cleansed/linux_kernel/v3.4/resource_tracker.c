static inline const char *qp_states_str(enum res_qp_states state)\r\n{\r\nswitch (state) {\r\ncase RES_QP_BUSY: return "RES_QP_BUSY";\r\ncase RES_QP_RESERVED: return "RES_QP_RESERVED";\r\ncase RES_QP_MAPPED: return "RES_QP_MAPPED";\r\ncase RES_QP_HW: return "RES_QP_HW";\r\ndefault: return "Unknown";\r\n}\r\n}\r\nstatic inline const char *mtt_states_str(enum res_mtt_states state)\r\n{\r\nswitch (state) {\r\ncase RES_MTT_BUSY: return "RES_MTT_BUSY";\r\ncase RES_MTT_ALLOCATED: return "RES_MTT_ALLOCATED";\r\ndefault: return "Unknown";\r\n}\r\n}\r\nstatic inline const char *srq_states_str(enum res_srq_states state)\r\n{\r\nswitch (state) {\r\ncase RES_SRQ_BUSY: return "RES_SRQ_BUSY";\r\ncase RES_SRQ_ALLOCATED: return "RES_SRQ_ALLOCATED";\r\ncase RES_SRQ_HW: return "RES_SRQ_HW";\r\ndefault: return "Unknown";\r\n}\r\n}\r\nstatic inline const char *counter_states_str(enum res_counter_states state)\r\n{\r\nswitch (state) {\r\ncase RES_COUNTER_BUSY: return "RES_COUNTER_BUSY";\r\ncase RES_COUNTER_ALLOCATED: return "RES_COUNTER_ALLOCATED";\r\ndefault: return "Unknown";\r\n}\r\n}\r\nstatic const char *ResourceType(enum mlx4_resource rt)\r\n{\r\nswitch (rt) {\r\ncase RES_QP: return "RES_QP";\r\ncase RES_CQ: return "RES_CQ";\r\ncase RES_SRQ: return "RES_SRQ";\r\ncase RES_MPT: return "RES_MPT";\r\ncase RES_MTT: return "RES_MTT";\r\ncase RES_MAC: return "RES_MAC";\r\ncase RES_EQ: return "RES_EQ";\r\ncase RES_COUNTER: return "RES_COUNTER";\r\ndefault: return "Unknown resource type !!!";\r\n};\r\n}\r\nint mlx4_init_resource_tracker(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint i;\r\nint t;\r\npriv->mfunc.master.res_tracker.slave_list =\r\nkzalloc(dev->num_slaves * sizeof(struct slave_list),\r\nGFP_KERNEL);\r\nif (!priv->mfunc.master.res_tracker.slave_list)\r\nreturn -ENOMEM;\r\nfor (i = 0 ; i < dev->num_slaves; i++) {\r\nfor (t = 0; t < MLX4_NUM_OF_RESOURCE_TYPE; ++t)\r\nINIT_LIST_HEAD(&priv->mfunc.master.res_tracker.\r\nslave_list[i].res_list[t]);\r\nmutex_init(&priv->mfunc.master.res_tracker.slave_list[i].mutex);\r\n}\r\nmlx4_dbg(dev, "Started init_resource_tracker: %ld slaves\n",\r\ndev->num_slaves);\r\nfor (i = 0 ; i < MLX4_NUM_OF_RESOURCE_TYPE; i++)\r\nINIT_RADIX_TREE(&priv->mfunc.master.res_tracker.res_tree[i],\r\nGFP_ATOMIC|__GFP_NOWARN);\r\nspin_lock_init(&priv->mfunc.master.res_tracker.lock);\r\nreturn 0 ;\r\n}\r\nvoid mlx4_free_resource_tracker(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint i;\r\nif (priv->mfunc.master.res_tracker.slave_list) {\r\nfor (i = 0 ; i < dev->num_slaves; i++)\r\nmlx4_delete_all_resources_for_slave(dev, i);\r\nkfree(priv->mfunc.master.res_tracker.slave_list);\r\n}\r\n}\r\nstatic void update_ud_gid(struct mlx4_dev *dev,\r\nstruct mlx4_qp_context *qp_ctx, u8 slave)\r\n{\r\nu32 ts = (be32_to_cpu(qp_ctx->flags) >> 16) & 0xff;\r\nif (MLX4_QP_ST_UD == ts)\r\nqp_ctx->pri_path.mgid_index = 0x80 | slave;\r\nmlx4_dbg(dev, "slave %d, new gid index: 0x%x ",\r\nslave, qp_ctx->pri_path.mgid_index);\r\n}\r\nstatic int mpt_mask(struct mlx4_dev *dev)\r\n{\r\nreturn dev->caps.num_mpts - 1;\r\n}\r\nstatic void *find_res(struct mlx4_dev *dev, int res_id,\r\nenum mlx4_resource type)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nreturn radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[type],\r\nres_id);\r\n}\r\nstatic int get_res(struct mlx4_dev *dev, int slave, int res_id,\r\nenum mlx4_resource type,\r\nvoid *res)\r\n{\r\nstruct res_common *r;\r\nint err = 0;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nr = find_res(dev, res_id, type);\r\nif (!r) {\r\nerr = -ENONET;\r\ngoto exit;\r\n}\r\nif (r->state == RES_ANY_BUSY) {\r\nerr = -EBUSY;\r\ngoto exit;\r\n}\r\nif (r->owner != slave) {\r\nerr = -EPERM;\r\ngoto exit;\r\n}\r\nr->from_state = r->state;\r\nr->state = RES_ANY_BUSY;\r\nmlx4_dbg(dev, "res %s id 0x%x to busy\n",\r\nResourceType(type), r->res_id);\r\nif (res)\r\n*((struct res_common **)res) = r;\r\nexit:\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nreturn err;\r\n}\r\nint mlx4_get_slave_from_resource_id(struct mlx4_dev *dev,\r\nenum mlx4_resource type,\r\nint res_id, int *slave)\r\n{\r\nstruct res_common *r;\r\nint err = -ENOENT;\r\nint id = res_id;\r\nif (type == RES_QP)\r\nid &= 0x7fffff;\r\nspin_lock(mlx4_tlock(dev));\r\nr = find_res(dev, id, type);\r\nif (r) {\r\n*slave = r->owner;\r\nerr = 0;\r\n}\r\nspin_unlock(mlx4_tlock(dev));\r\nreturn err;\r\n}\r\nstatic void put_res(struct mlx4_dev *dev, int slave, int res_id,\r\nenum mlx4_resource type)\r\n{\r\nstruct res_common *r;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nr = find_res(dev, res_id, type);\r\nif (r)\r\nr->state = r->from_state;\r\nspin_unlock_irq(mlx4_tlock(dev));\r\n}\r\nstatic struct res_common *alloc_qp_tr(int id)\r\n{\r\nstruct res_qp *ret;\r\nret = kzalloc(sizeof *ret, GFP_KERNEL);\r\nif (!ret)\r\nreturn NULL;\r\nret->com.res_id = id;\r\nret->com.state = RES_QP_RESERVED;\r\nret->local_qpn = id;\r\nINIT_LIST_HEAD(&ret->mcg_list);\r\nspin_lock_init(&ret->mcg_spl);\r\nreturn &ret->com;\r\n}\r\nstatic struct res_common *alloc_mtt_tr(int id, int order)\r\n{\r\nstruct res_mtt *ret;\r\nret = kzalloc(sizeof *ret, GFP_KERNEL);\r\nif (!ret)\r\nreturn NULL;\r\nret->com.res_id = id;\r\nret->order = order;\r\nret->com.state = RES_MTT_ALLOCATED;\r\natomic_set(&ret->ref_count, 0);\r\nreturn &ret->com;\r\n}\r\nstatic struct res_common *alloc_mpt_tr(int id, int key)\r\n{\r\nstruct res_mpt *ret;\r\nret = kzalloc(sizeof *ret, GFP_KERNEL);\r\nif (!ret)\r\nreturn NULL;\r\nret->com.res_id = id;\r\nret->com.state = RES_MPT_RESERVED;\r\nret->key = key;\r\nreturn &ret->com;\r\n}\r\nstatic struct res_common *alloc_eq_tr(int id)\r\n{\r\nstruct res_eq *ret;\r\nret = kzalloc(sizeof *ret, GFP_KERNEL);\r\nif (!ret)\r\nreturn NULL;\r\nret->com.res_id = id;\r\nret->com.state = RES_EQ_RESERVED;\r\nreturn &ret->com;\r\n}\r\nstatic struct res_common *alloc_cq_tr(int id)\r\n{\r\nstruct res_cq *ret;\r\nret = kzalloc(sizeof *ret, GFP_KERNEL);\r\nif (!ret)\r\nreturn NULL;\r\nret->com.res_id = id;\r\nret->com.state = RES_CQ_ALLOCATED;\r\natomic_set(&ret->ref_count, 0);\r\nreturn &ret->com;\r\n}\r\nstatic struct res_common *alloc_srq_tr(int id)\r\n{\r\nstruct res_srq *ret;\r\nret = kzalloc(sizeof *ret, GFP_KERNEL);\r\nif (!ret)\r\nreturn NULL;\r\nret->com.res_id = id;\r\nret->com.state = RES_SRQ_ALLOCATED;\r\natomic_set(&ret->ref_count, 0);\r\nreturn &ret->com;\r\n}\r\nstatic struct res_common *alloc_counter_tr(int id)\r\n{\r\nstruct res_counter *ret;\r\nret = kzalloc(sizeof *ret, GFP_KERNEL);\r\nif (!ret)\r\nreturn NULL;\r\nret->com.res_id = id;\r\nret->com.state = RES_COUNTER_ALLOCATED;\r\nreturn &ret->com;\r\n}\r\nstatic struct res_common *alloc_tr(int id, enum mlx4_resource type, int slave,\r\nint extra)\r\n{\r\nstruct res_common *ret;\r\nswitch (type) {\r\ncase RES_QP:\r\nret = alloc_qp_tr(id);\r\nbreak;\r\ncase RES_MPT:\r\nret = alloc_mpt_tr(id, extra);\r\nbreak;\r\ncase RES_MTT:\r\nret = alloc_mtt_tr(id, extra);\r\nbreak;\r\ncase RES_EQ:\r\nret = alloc_eq_tr(id);\r\nbreak;\r\ncase RES_CQ:\r\nret = alloc_cq_tr(id);\r\nbreak;\r\ncase RES_SRQ:\r\nret = alloc_srq_tr(id);\r\nbreak;\r\ncase RES_MAC:\r\nprintk(KERN_ERR "implementation missing\n");\r\nreturn NULL;\r\ncase RES_COUNTER:\r\nret = alloc_counter_tr(id);\r\nbreak;\r\ndefault:\r\nreturn NULL;\r\n}\r\nif (ret)\r\nret->owner = slave;\r\nreturn ret;\r\n}\r\nstatic int add_res_range(struct mlx4_dev *dev, int slave, int base, int count,\r\nenum mlx4_resource type, int extra)\r\n{\r\nint i;\r\nint err;\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct res_common **res_arr;\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct radix_tree_root *root = &tracker->res_tree[type];\r\nres_arr = kzalloc(count * sizeof *res_arr, GFP_KERNEL);\r\nif (!res_arr)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < count; ++i) {\r\nres_arr[i] = alloc_tr(base + i, type, slave, extra);\r\nif (!res_arr[i]) {\r\nfor (--i; i >= 0; --i)\r\nkfree(res_arr[i]);\r\nkfree(res_arr);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nspin_lock_irq(mlx4_tlock(dev));\r\nfor (i = 0; i < count; ++i) {\r\nif (find_res(dev, base + i, type)) {\r\nerr = -EEXIST;\r\ngoto undo;\r\n}\r\nerr = radix_tree_insert(root, base + i, res_arr[i]);\r\nif (err)\r\ngoto undo;\r\nlist_add_tail(&res_arr[i]->list,\r\n&tracker->slave_list[slave].res_list[type]);\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nkfree(res_arr);\r\nreturn 0;\r\nundo:\r\nfor (--i; i >= base; --i)\r\nradix_tree_delete(&tracker->res_tree[type], i);\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nfor (i = 0; i < count; ++i)\r\nkfree(res_arr[i]);\r\nkfree(res_arr);\r\nreturn err;\r\n}\r\nstatic int remove_qp_ok(struct res_qp *res)\r\n{\r\nif (res->com.state == RES_QP_BUSY)\r\nreturn -EBUSY;\r\nelse if (res->com.state != RES_QP_RESERVED)\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic int remove_mtt_ok(struct res_mtt *res, int order)\r\n{\r\nif (res->com.state == RES_MTT_BUSY ||\r\natomic_read(&res->ref_count)) {\r\nprintk(KERN_DEBUG "%s-%d: state %s, ref_count %d\n",\r\n__func__, __LINE__,\r\nmtt_states_str(res->com.state),\r\natomic_read(&res->ref_count));\r\nreturn -EBUSY;\r\n} else if (res->com.state != RES_MTT_ALLOCATED)\r\nreturn -EPERM;\r\nelse if (res->order != order)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int remove_mpt_ok(struct res_mpt *res)\r\n{\r\nif (res->com.state == RES_MPT_BUSY)\r\nreturn -EBUSY;\r\nelse if (res->com.state != RES_MPT_RESERVED)\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic int remove_eq_ok(struct res_eq *res)\r\n{\r\nif (res->com.state == RES_MPT_BUSY)\r\nreturn -EBUSY;\r\nelse if (res->com.state != RES_MPT_RESERVED)\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic int remove_counter_ok(struct res_counter *res)\r\n{\r\nif (res->com.state == RES_COUNTER_BUSY)\r\nreturn -EBUSY;\r\nelse if (res->com.state != RES_COUNTER_ALLOCATED)\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic int remove_cq_ok(struct res_cq *res)\r\n{\r\nif (res->com.state == RES_CQ_BUSY)\r\nreturn -EBUSY;\r\nelse if (res->com.state != RES_CQ_ALLOCATED)\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic int remove_srq_ok(struct res_srq *res)\r\n{\r\nif (res->com.state == RES_SRQ_BUSY)\r\nreturn -EBUSY;\r\nelse if (res->com.state != RES_SRQ_ALLOCATED)\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic int remove_ok(struct res_common *res, enum mlx4_resource type, int extra)\r\n{\r\nswitch (type) {\r\ncase RES_QP:\r\nreturn remove_qp_ok((struct res_qp *)res);\r\ncase RES_CQ:\r\nreturn remove_cq_ok((struct res_cq *)res);\r\ncase RES_SRQ:\r\nreturn remove_srq_ok((struct res_srq *)res);\r\ncase RES_MPT:\r\nreturn remove_mpt_ok((struct res_mpt *)res);\r\ncase RES_MTT:\r\nreturn remove_mtt_ok((struct res_mtt *)res, extra);\r\ncase RES_MAC:\r\nreturn -ENOSYS;\r\ncase RES_EQ:\r\nreturn remove_eq_ok((struct res_eq *)res);\r\ncase RES_COUNTER:\r\nreturn remove_counter_ok((struct res_counter *)res);\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic int rem_res_range(struct mlx4_dev *dev, int slave, int base, int count,\r\nenum mlx4_resource type, int extra)\r\n{\r\nint i;\r\nint err;\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct res_common *r;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nfor (i = base; i < base + count; ++i) {\r\nr = radix_tree_lookup(&tracker->res_tree[type], i);\r\nif (!r) {\r\nerr = -ENOENT;\r\ngoto out;\r\n}\r\nif (r->owner != slave) {\r\nerr = -EPERM;\r\ngoto out;\r\n}\r\nerr = remove_ok(r, type, extra);\r\nif (err)\r\ngoto out;\r\n}\r\nfor (i = base; i < base + count; ++i) {\r\nr = radix_tree_lookup(&tracker->res_tree[type], i);\r\nradix_tree_delete(&tracker->res_tree[type], i);\r\nlist_del(&r->list);\r\nkfree(r);\r\n}\r\nerr = 0;\r\nout:\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nreturn err;\r\n}\r\nstatic int qp_res_start_move_to(struct mlx4_dev *dev, int slave, int qpn,\r\nenum res_qp_states state, struct res_qp **qp,\r\nint alloc)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct res_qp *r;\r\nint err = 0;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nr = radix_tree_lookup(&tracker->res_tree[RES_QP], qpn);\r\nif (!r)\r\nerr = -ENOENT;\r\nelse if (r->com.owner != slave)\r\nerr = -EPERM;\r\nelse {\r\nswitch (state) {\r\ncase RES_QP_BUSY:\r\nmlx4_dbg(dev, "%s: failed RES_QP, 0x%x\n",\r\n__func__, r->com.res_id);\r\nerr = -EBUSY;\r\nbreak;\r\ncase RES_QP_RESERVED:\r\nif (r->com.state == RES_QP_MAPPED && !alloc)\r\nbreak;\r\nmlx4_dbg(dev, "failed RES_QP, 0x%x\n", r->com.res_id);\r\nerr = -EINVAL;\r\nbreak;\r\ncase RES_QP_MAPPED:\r\nif ((r->com.state == RES_QP_RESERVED && alloc) ||\r\nr->com.state == RES_QP_HW)\r\nbreak;\r\nelse {\r\nmlx4_dbg(dev, "failed RES_QP, 0x%x\n",\r\nr->com.res_id);\r\nerr = -EINVAL;\r\n}\r\nbreak;\r\ncase RES_QP_HW:\r\nif (r->com.state != RES_QP_MAPPED)\r\nerr = -EINVAL;\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\n}\r\nif (!err) {\r\nr->com.from_state = r->com.state;\r\nr->com.to_state = state;\r\nr->com.state = RES_QP_BUSY;\r\nif (qp)\r\n*qp = (struct res_qp *)r;\r\n}\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nreturn err;\r\n}\r\nstatic int mr_res_start_move_to(struct mlx4_dev *dev, int slave, int index,\r\nenum res_mpt_states state, struct res_mpt **mpt)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct res_mpt *r;\r\nint err = 0;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nr = radix_tree_lookup(&tracker->res_tree[RES_MPT], index);\r\nif (!r)\r\nerr = -ENOENT;\r\nelse if (r->com.owner != slave)\r\nerr = -EPERM;\r\nelse {\r\nswitch (state) {\r\ncase RES_MPT_BUSY:\r\nerr = -EINVAL;\r\nbreak;\r\ncase RES_MPT_RESERVED:\r\nif (r->com.state != RES_MPT_MAPPED)\r\nerr = -EINVAL;\r\nbreak;\r\ncase RES_MPT_MAPPED:\r\nif (r->com.state != RES_MPT_RESERVED &&\r\nr->com.state != RES_MPT_HW)\r\nerr = -EINVAL;\r\nbreak;\r\ncase RES_MPT_HW:\r\nif (r->com.state != RES_MPT_MAPPED)\r\nerr = -EINVAL;\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\n}\r\nif (!err) {\r\nr->com.from_state = r->com.state;\r\nr->com.to_state = state;\r\nr->com.state = RES_MPT_BUSY;\r\nif (mpt)\r\n*mpt = (struct res_mpt *)r;\r\n}\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nreturn err;\r\n}\r\nstatic int eq_res_start_move_to(struct mlx4_dev *dev, int slave, int index,\r\nenum res_eq_states state, struct res_eq **eq)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct res_eq *r;\r\nint err = 0;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nr = radix_tree_lookup(&tracker->res_tree[RES_EQ], index);\r\nif (!r)\r\nerr = -ENOENT;\r\nelse if (r->com.owner != slave)\r\nerr = -EPERM;\r\nelse {\r\nswitch (state) {\r\ncase RES_EQ_BUSY:\r\nerr = -EINVAL;\r\nbreak;\r\ncase RES_EQ_RESERVED:\r\nif (r->com.state != RES_EQ_HW)\r\nerr = -EINVAL;\r\nbreak;\r\ncase RES_EQ_HW:\r\nif (r->com.state != RES_EQ_RESERVED)\r\nerr = -EINVAL;\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\n}\r\nif (!err) {\r\nr->com.from_state = r->com.state;\r\nr->com.to_state = state;\r\nr->com.state = RES_EQ_BUSY;\r\nif (eq)\r\n*eq = r;\r\n}\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nreturn err;\r\n}\r\nstatic int cq_res_start_move_to(struct mlx4_dev *dev, int slave, int cqn,\r\nenum res_cq_states state, struct res_cq **cq)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct res_cq *r;\r\nint err;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nr = radix_tree_lookup(&tracker->res_tree[RES_CQ], cqn);\r\nif (!r)\r\nerr = -ENOENT;\r\nelse if (r->com.owner != slave)\r\nerr = -EPERM;\r\nelse {\r\nswitch (state) {\r\ncase RES_CQ_BUSY:\r\nerr = -EBUSY;\r\nbreak;\r\ncase RES_CQ_ALLOCATED:\r\nif (r->com.state != RES_CQ_HW)\r\nerr = -EINVAL;\r\nelse if (atomic_read(&r->ref_count))\r\nerr = -EBUSY;\r\nelse\r\nerr = 0;\r\nbreak;\r\ncase RES_CQ_HW:\r\nif (r->com.state != RES_CQ_ALLOCATED)\r\nerr = -EINVAL;\r\nelse\r\nerr = 0;\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\n}\r\nif (!err) {\r\nr->com.from_state = r->com.state;\r\nr->com.to_state = state;\r\nr->com.state = RES_CQ_BUSY;\r\nif (cq)\r\n*cq = r;\r\n}\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nreturn err;\r\n}\r\nstatic int srq_res_start_move_to(struct mlx4_dev *dev, int slave, int index,\r\nenum res_cq_states state, struct res_srq **srq)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct res_srq *r;\r\nint err = 0;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nr = radix_tree_lookup(&tracker->res_tree[RES_SRQ], index);\r\nif (!r)\r\nerr = -ENOENT;\r\nelse if (r->com.owner != slave)\r\nerr = -EPERM;\r\nelse {\r\nswitch (state) {\r\ncase RES_SRQ_BUSY:\r\nerr = -EINVAL;\r\nbreak;\r\ncase RES_SRQ_ALLOCATED:\r\nif (r->com.state != RES_SRQ_HW)\r\nerr = -EINVAL;\r\nelse if (atomic_read(&r->ref_count))\r\nerr = -EBUSY;\r\nbreak;\r\ncase RES_SRQ_HW:\r\nif (r->com.state != RES_SRQ_ALLOCATED)\r\nerr = -EINVAL;\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\n}\r\nif (!err) {\r\nr->com.from_state = r->com.state;\r\nr->com.to_state = state;\r\nr->com.state = RES_SRQ_BUSY;\r\nif (srq)\r\n*srq = r;\r\n}\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nreturn err;\r\n}\r\nstatic void res_abort_move(struct mlx4_dev *dev, int slave,\r\nenum mlx4_resource type, int id)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct res_common *r;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nr = radix_tree_lookup(&tracker->res_tree[type], id);\r\nif (r && (r->owner == slave))\r\nr->state = r->from_state;\r\nspin_unlock_irq(mlx4_tlock(dev));\r\n}\r\nstatic void res_end_move(struct mlx4_dev *dev, int slave,\r\nenum mlx4_resource type, int id)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct res_common *r;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nr = radix_tree_lookup(&tracker->res_tree[type], id);\r\nif (r && (r->owner == slave))\r\nr->state = r->to_state;\r\nspin_unlock_irq(mlx4_tlock(dev));\r\n}\r\nstatic int valid_reserved(struct mlx4_dev *dev, int slave, int qpn)\r\n{\r\nreturn mlx4_is_qp_reserved(dev, qpn);\r\n}\r\nstatic int qp_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint err;\r\nint count;\r\nint align;\r\nint base;\r\nint qpn;\r\nswitch (op) {\r\ncase RES_OP_RESERVE:\r\ncount = get_param_l(&in_param);\r\nalign = get_param_h(&in_param);\r\nerr = __mlx4_qp_reserve_range(dev, count, align, &base);\r\nif (err)\r\nreturn err;\r\nerr = add_res_range(dev, slave, base, count, RES_QP, 0);\r\nif (err) {\r\n__mlx4_qp_release_range(dev, base, count);\r\nreturn err;\r\n}\r\nset_param_l(out_param, base);\r\nbreak;\r\ncase RES_OP_MAP_ICM:\r\nqpn = get_param_l(&in_param) & 0x7fffff;\r\nif (valid_reserved(dev, slave, qpn)) {\r\nerr = add_res_range(dev, slave, qpn, 1, RES_QP, 0);\r\nif (err)\r\nreturn err;\r\n}\r\nerr = qp_res_start_move_to(dev, slave, qpn, RES_QP_MAPPED,\r\nNULL, 1);\r\nif (err)\r\nreturn err;\r\nif (!valid_reserved(dev, slave, qpn)) {\r\nerr = __mlx4_qp_alloc_icm(dev, qpn);\r\nif (err) {\r\nres_abort_move(dev, slave, RES_QP, qpn);\r\nreturn err;\r\n}\r\n}\r\nres_end_move(dev, slave, RES_QP, qpn);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int mtt_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint err = -EINVAL;\r\nint base;\r\nint order;\r\nif (op != RES_OP_RESERVE_AND_MAP)\r\nreturn err;\r\norder = get_param_l(&in_param);\r\nbase = __mlx4_alloc_mtt_range(dev, order);\r\nif (base == -1)\r\nreturn -ENOMEM;\r\nerr = add_res_range(dev, slave, base, 1, RES_MTT, order);\r\nif (err)\r\n__mlx4_free_mtt_range(dev, base, order);\r\nelse\r\nset_param_l(out_param, base);\r\nreturn err;\r\n}\r\nstatic int mpt_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint err = -EINVAL;\r\nint index;\r\nint id;\r\nstruct res_mpt *mpt;\r\nswitch (op) {\r\ncase RES_OP_RESERVE:\r\nindex = __mlx4_mr_reserve(dev);\r\nif (index == -1)\r\nbreak;\r\nid = index & mpt_mask(dev);\r\nerr = add_res_range(dev, slave, id, 1, RES_MPT, index);\r\nif (err) {\r\n__mlx4_mr_release(dev, index);\r\nbreak;\r\n}\r\nset_param_l(out_param, index);\r\nbreak;\r\ncase RES_OP_MAP_ICM:\r\nindex = get_param_l(&in_param);\r\nid = index & mpt_mask(dev);\r\nerr = mr_res_start_move_to(dev, slave, id,\r\nRES_MPT_MAPPED, &mpt);\r\nif (err)\r\nreturn err;\r\nerr = __mlx4_mr_alloc_icm(dev, mpt->key);\r\nif (err) {\r\nres_abort_move(dev, slave, RES_MPT, id);\r\nreturn err;\r\n}\r\nres_end_move(dev, slave, RES_MPT, id);\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int cq_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint cqn;\r\nint err;\r\nswitch (op) {\r\ncase RES_OP_RESERVE_AND_MAP:\r\nerr = __mlx4_cq_alloc_icm(dev, &cqn);\r\nif (err)\r\nbreak;\r\nerr = add_res_range(dev, slave, cqn, 1, RES_CQ, 0);\r\nif (err) {\r\n__mlx4_cq_free_icm(dev, cqn);\r\nbreak;\r\n}\r\nset_param_l(out_param, cqn);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\n}\r\nreturn err;\r\n}\r\nstatic int srq_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint srqn;\r\nint err;\r\nswitch (op) {\r\ncase RES_OP_RESERVE_AND_MAP:\r\nerr = __mlx4_srq_alloc_icm(dev, &srqn);\r\nif (err)\r\nbreak;\r\nerr = add_res_range(dev, slave, srqn, 1, RES_SRQ, 0);\r\nif (err) {\r\n__mlx4_srq_free_icm(dev, srqn);\r\nbreak;\r\n}\r\nset_param_l(out_param, srqn);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\n}\r\nreturn err;\r\n}\r\nstatic int mac_add_to_slave(struct mlx4_dev *dev, int slave, u64 mac, int port)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct mac_res *res;\r\nres = kzalloc(sizeof *res, GFP_KERNEL);\r\nif (!res)\r\nreturn -ENOMEM;\r\nres->mac = mac;\r\nres->port = (u8) port;\r\nlist_add_tail(&res->list,\r\n&tracker->slave_list[slave].res_list[RES_MAC]);\r\nreturn 0;\r\n}\r\nstatic void mac_del_from_slave(struct mlx4_dev *dev, int slave, u64 mac,\r\nint port)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct list_head *mac_list =\r\n&tracker->slave_list[slave].res_list[RES_MAC];\r\nstruct mac_res *res, *tmp;\r\nlist_for_each_entry_safe(res, tmp, mac_list, list) {\r\nif (res->mac == mac && res->port == (u8) port) {\r\nlist_del(&res->list);\r\nkfree(res);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void rem_slave_macs(struct mlx4_dev *dev, int slave)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct list_head *mac_list =\r\n&tracker->slave_list[slave].res_list[RES_MAC];\r\nstruct mac_res *res, *tmp;\r\nlist_for_each_entry_safe(res, tmp, mac_list, list) {\r\nlist_del(&res->list);\r\n__mlx4_unregister_mac(dev, res->port, res->mac);\r\nkfree(res);\r\n}\r\n}\r\nstatic int mac_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint err = -EINVAL;\r\nint port;\r\nu64 mac;\r\nif (op != RES_OP_RESERVE_AND_MAP)\r\nreturn err;\r\nport = get_param_l(out_param);\r\nmac = in_param;\r\nerr = __mlx4_register_mac(dev, port, mac);\r\nif (err >= 0) {\r\nset_param_l(out_param, err);\r\nerr = 0;\r\n}\r\nif (!err) {\r\nerr = mac_add_to_slave(dev, slave, mac, port);\r\nif (err)\r\n__mlx4_unregister_mac(dev, port, mac);\r\n}\r\nreturn err;\r\n}\r\nstatic int vlan_alloc_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nreturn 0;\r\n}\r\nint mlx4_ALLOC_RES_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint alop = vhcr->op_modifier;\r\nswitch (vhcr->in_modifier) {\r\ncase RES_QP:\r\nerr = qp_alloc_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_MTT:\r\nerr = mtt_alloc_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_MPT:\r\nerr = mpt_alloc_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_CQ:\r\nerr = cq_alloc_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_SRQ:\r\nerr = srq_alloc_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_MAC:\r\nerr = mac_alloc_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_VLAN:\r\nerr = vlan_alloc_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int qp_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param)\r\n{\r\nint err;\r\nint count;\r\nint base;\r\nint qpn;\r\nswitch (op) {\r\ncase RES_OP_RESERVE:\r\nbase = get_param_l(&in_param) & 0x7fffff;\r\ncount = get_param_h(&in_param);\r\nerr = rem_res_range(dev, slave, base, count, RES_QP, 0);\r\nif (err)\r\nbreak;\r\n__mlx4_qp_release_range(dev, base, count);\r\nbreak;\r\ncase RES_OP_MAP_ICM:\r\nqpn = get_param_l(&in_param) & 0x7fffff;\r\nerr = qp_res_start_move_to(dev, slave, qpn, RES_QP_RESERVED,\r\nNULL, 0);\r\nif (err)\r\nreturn err;\r\nif (!valid_reserved(dev, slave, qpn))\r\n__mlx4_qp_free_icm(dev, qpn);\r\nres_end_move(dev, slave, RES_QP, qpn);\r\nif (valid_reserved(dev, slave, qpn))\r\nerr = rem_res_range(dev, slave, qpn, 1, RES_QP, 0);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int mtt_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint err = -EINVAL;\r\nint base;\r\nint order;\r\nif (op != RES_OP_RESERVE_AND_MAP)\r\nreturn err;\r\nbase = get_param_l(&in_param);\r\norder = get_param_h(&in_param);\r\nerr = rem_res_range(dev, slave, base, 1, RES_MTT, order);\r\nif (!err)\r\n__mlx4_free_mtt_range(dev, base, order);\r\nreturn err;\r\n}\r\nstatic int mpt_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param)\r\n{\r\nint err = -EINVAL;\r\nint index;\r\nint id;\r\nstruct res_mpt *mpt;\r\nswitch (op) {\r\ncase RES_OP_RESERVE:\r\nindex = get_param_l(&in_param);\r\nid = index & mpt_mask(dev);\r\nerr = get_res(dev, slave, id, RES_MPT, &mpt);\r\nif (err)\r\nbreak;\r\nindex = mpt->key;\r\nput_res(dev, slave, id, RES_MPT);\r\nerr = rem_res_range(dev, slave, id, 1, RES_MPT, 0);\r\nif (err)\r\nbreak;\r\n__mlx4_mr_release(dev, index);\r\nbreak;\r\ncase RES_OP_MAP_ICM:\r\nindex = get_param_l(&in_param);\r\nid = index & mpt_mask(dev);\r\nerr = mr_res_start_move_to(dev, slave, id,\r\nRES_MPT_RESERVED, &mpt);\r\nif (err)\r\nreturn err;\r\n__mlx4_mr_free_icm(dev, mpt->key);\r\nres_end_move(dev, slave, RES_MPT, id);\r\nreturn err;\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int cq_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint cqn;\r\nint err;\r\nswitch (op) {\r\ncase RES_OP_RESERVE_AND_MAP:\r\ncqn = get_param_l(&in_param);\r\nerr = rem_res_range(dev, slave, cqn, 1, RES_CQ, 0);\r\nif (err)\r\nbreak;\r\n__mlx4_cq_free_icm(dev, cqn);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int srq_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint srqn;\r\nint err;\r\nswitch (op) {\r\ncase RES_OP_RESERVE_AND_MAP:\r\nsrqn = get_param_l(&in_param);\r\nerr = rem_res_range(dev, slave, srqn, 1, RES_SRQ, 0);\r\nif (err)\r\nbreak;\r\n__mlx4_srq_free_icm(dev, srqn);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int mac_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nint port;\r\nint err = 0;\r\nswitch (op) {\r\ncase RES_OP_RESERVE_AND_MAP:\r\nport = get_param_l(out_param);\r\nmac_del_from_slave(dev, slave, in_param, port);\r\n__mlx4_unregister_mac(dev, port, in_param);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int vlan_free_res(struct mlx4_dev *dev, int slave, int op, int cmd,\r\nu64 in_param, u64 *out_param)\r\n{\r\nreturn 0;\r\n}\r\nint mlx4_FREE_RES_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err = -EINVAL;\r\nint alop = vhcr->op_modifier;\r\nswitch (vhcr->in_modifier) {\r\ncase RES_QP:\r\nerr = qp_free_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param);\r\nbreak;\r\ncase RES_MTT:\r\nerr = mtt_free_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_MPT:\r\nerr = mpt_free_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param);\r\nbreak;\r\ncase RES_CQ:\r\nerr = cq_free_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_SRQ:\r\nerr = srq_free_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_MAC:\r\nerr = mac_free_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ncase RES_VLAN:\r\nerr = vlan_free_res(dev, slave, vhcr->op_modifier, alop,\r\nvhcr->in_param, &vhcr->out_param);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic int mr_phys_mpt(struct mlx4_mpt_entry *mpt)\r\n{\r\nreturn (be32_to_cpu(mpt->flags) >> 9) & 1;\r\n}\r\nstatic int mr_get_mtt_addr(struct mlx4_mpt_entry *mpt)\r\n{\r\nreturn (int)be64_to_cpu(mpt->mtt_addr) & 0xfffffff8;\r\n}\r\nstatic int mr_get_mtt_size(struct mlx4_mpt_entry *mpt)\r\n{\r\nreturn be32_to_cpu(mpt->mtt_sz);\r\n}\r\nstatic int qp_get_mtt_addr(struct mlx4_qp_context *qpc)\r\n{\r\nreturn be32_to_cpu(qpc->mtt_base_addr_l) & 0xfffffff8;\r\n}\r\nstatic int srq_get_mtt_addr(struct mlx4_srq_context *srqc)\r\n{\r\nreturn be32_to_cpu(srqc->mtt_base_addr_l) & 0xfffffff8;\r\n}\r\nstatic int qp_get_mtt_size(struct mlx4_qp_context *qpc)\r\n{\r\nint page_shift = (qpc->log_page_size & 0x3f) + 12;\r\nint log_sq_size = (qpc->sq_size_stride >> 3) & 0xf;\r\nint log_sq_sride = qpc->sq_size_stride & 7;\r\nint log_rq_size = (qpc->rq_size_stride >> 3) & 0xf;\r\nint log_rq_stride = qpc->rq_size_stride & 7;\r\nint srq = (be32_to_cpu(qpc->srqn) >> 24) & 1;\r\nint rss = (be32_to_cpu(qpc->flags) >> 13) & 1;\r\nint xrc = (be32_to_cpu(qpc->local_qpn) >> 23) & 1;\r\nint sq_size;\r\nint rq_size;\r\nint total_pages;\r\nint total_mem;\r\nint page_offset = (be32_to_cpu(qpc->params2) >> 6) & 0x3f;\r\nsq_size = 1 << (log_sq_size + log_sq_sride + 4);\r\nrq_size = (srq|rss|xrc) ? 0 : (1 << (log_rq_size + log_rq_stride + 4));\r\ntotal_mem = sq_size + rq_size;\r\ntotal_pages =\r\nroundup_pow_of_two((total_mem + (page_offset << 6)) >>\r\npage_shift);\r\nreturn total_pages;\r\n}\r\nstatic int check_mtt_range(struct mlx4_dev *dev, int slave, int start,\r\nint size, struct res_mtt *mtt)\r\n{\r\nint res_start = mtt->com.res_id;\r\nint res_size = (1 << mtt->order);\r\nif (start < res_start || start + size > res_start + res_size)\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nint mlx4_SW2HW_MPT_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint index = vhcr->in_modifier;\r\nstruct res_mtt *mtt;\r\nstruct res_mpt *mpt;\r\nint mtt_base = mr_get_mtt_addr(inbox->buf) / dev->caps.mtt_entry_sz;\r\nint phys;\r\nint id;\r\nid = index & mpt_mask(dev);\r\nerr = mr_res_start_move_to(dev, slave, id, RES_MPT_HW, &mpt);\r\nif (err)\r\nreturn err;\r\nphys = mr_phys_mpt(inbox->buf);\r\nif (!phys) {\r\nerr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\r\nif (err)\r\ngoto ex_abort;\r\nerr = check_mtt_range(dev, slave, mtt_base,\r\nmr_get_mtt_size(inbox->buf), mtt);\r\nif (err)\r\ngoto ex_put;\r\nmpt->mtt = mtt;\r\n}\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto ex_put;\r\nif (!phys) {\r\natomic_inc(&mtt->ref_count);\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\n}\r\nres_end_move(dev, slave, RES_MPT, id);\r\nreturn 0;\r\nex_put:\r\nif (!phys)\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\nex_abort:\r\nres_abort_move(dev, slave, RES_MPT, id);\r\nreturn err;\r\n}\r\nint mlx4_HW2SW_MPT_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint index = vhcr->in_modifier;\r\nstruct res_mpt *mpt;\r\nint id;\r\nid = index & mpt_mask(dev);\r\nerr = mr_res_start_move_to(dev, slave, id, RES_MPT_MAPPED, &mpt);\r\nif (err)\r\nreturn err;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto ex_abort;\r\nif (mpt->mtt)\r\natomic_dec(&mpt->mtt->ref_count);\r\nres_end_move(dev, slave, RES_MPT, id);\r\nreturn 0;\r\nex_abort:\r\nres_abort_move(dev, slave, RES_MPT, id);\r\nreturn err;\r\n}\r\nint mlx4_QUERY_MPT_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint index = vhcr->in_modifier;\r\nstruct res_mpt *mpt;\r\nint id;\r\nid = index & mpt_mask(dev);\r\nerr = get_res(dev, slave, id, RES_MPT, &mpt);\r\nif (err)\r\nreturn err;\r\nif (mpt->com.from_state != RES_MPT_HW) {\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nout:\r\nput_res(dev, slave, id, RES_MPT);\r\nreturn err;\r\n}\r\nstatic int qp_get_rcqn(struct mlx4_qp_context *qpc)\r\n{\r\nreturn be32_to_cpu(qpc->cqn_recv) & 0xffffff;\r\n}\r\nstatic int qp_get_scqn(struct mlx4_qp_context *qpc)\r\n{\r\nreturn be32_to_cpu(qpc->cqn_send) & 0xffffff;\r\n}\r\nstatic u32 qp_get_srqn(struct mlx4_qp_context *qpc)\r\n{\r\nreturn be32_to_cpu(qpc->srqn) & 0x1ffffff;\r\n}\r\nint mlx4_RST2INIT_QP_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint qpn = vhcr->in_modifier & 0x7fffff;\r\nstruct res_mtt *mtt;\r\nstruct res_qp *qp;\r\nstruct mlx4_qp_context *qpc = inbox->buf + 8;\r\nint mtt_base = qp_get_mtt_addr(qpc) / dev->caps.mtt_entry_sz;\r\nint mtt_size = qp_get_mtt_size(qpc);\r\nstruct res_cq *rcq;\r\nstruct res_cq *scq;\r\nint rcqn = qp_get_rcqn(qpc);\r\nint scqn = qp_get_scqn(qpc);\r\nu32 srqn = qp_get_srqn(qpc) & 0xffffff;\r\nint use_srq = (qp_get_srqn(qpc) >> 24) & 1;\r\nstruct res_srq *srq;\r\nint local_qpn = be32_to_cpu(qpc->local_qpn) & 0xffffff;\r\nerr = qp_res_start_move_to(dev, slave, qpn, RES_QP_HW, &qp, 0);\r\nif (err)\r\nreturn err;\r\nqp->local_qpn = local_qpn;\r\nerr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\r\nif (err)\r\ngoto ex_abort;\r\nerr = check_mtt_range(dev, slave, mtt_base, mtt_size, mtt);\r\nif (err)\r\ngoto ex_put_mtt;\r\nerr = get_res(dev, slave, rcqn, RES_CQ, &rcq);\r\nif (err)\r\ngoto ex_put_mtt;\r\nif (scqn != rcqn) {\r\nerr = get_res(dev, slave, scqn, RES_CQ, &scq);\r\nif (err)\r\ngoto ex_put_rcq;\r\n} else\r\nscq = rcq;\r\nif (use_srq) {\r\nerr = get_res(dev, slave, srqn, RES_SRQ, &srq);\r\nif (err)\r\ngoto ex_put_scq;\r\n}\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto ex_put_srq;\r\natomic_inc(&mtt->ref_count);\r\nqp->mtt = mtt;\r\natomic_inc(&rcq->ref_count);\r\nqp->rcq = rcq;\r\natomic_inc(&scq->ref_count);\r\nqp->scq = scq;\r\nif (scqn != rcqn)\r\nput_res(dev, slave, scqn, RES_CQ);\r\nif (use_srq) {\r\natomic_inc(&srq->ref_count);\r\nput_res(dev, slave, srqn, RES_SRQ);\r\nqp->srq = srq;\r\n}\r\nput_res(dev, slave, rcqn, RES_CQ);\r\nput_res(dev, slave, mtt_base, RES_MTT);\r\nres_end_move(dev, slave, RES_QP, qpn);\r\nreturn 0;\r\nex_put_srq:\r\nif (use_srq)\r\nput_res(dev, slave, srqn, RES_SRQ);\r\nex_put_scq:\r\nif (scqn != rcqn)\r\nput_res(dev, slave, scqn, RES_CQ);\r\nex_put_rcq:\r\nput_res(dev, slave, rcqn, RES_CQ);\r\nex_put_mtt:\r\nput_res(dev, slave, mtt_base, RES_MTT);\r\nex_abort:\r\nres_abort_move(dev, slave, RES_QP, qpn);\r\nreturn err;\r\n}\r\nstatic int eq_get_mtt_addr(struct mlx4_eq_context *eqc)\r\n{\r\nreturn be32_to_cpu(eqc->mtt_base_addr_l) & 0xfffffff8;\r\n}\r\nstatic int eq_get_mtt_size(struct mlx4_eq_context *eqc)\r\n{\r\nint log_eq_size = eqc->log_eq_size & 0x1f;\r\nint page_shift = (eqc->log_page_size & 0x3f) + 12;\r\nif (log_eq_size + 5 < page_shift)\r\nreturn 1;\r\nreturn 1 << (log_eq_size + 5 - page_shift);\r\n}\r\nstatic int cq_get_mtt_addr(struct mlx4_cq_context *cqc)\r\n{\r\nreturn be32_to_cpu(cqc->mtt_base_addr_l) & 0xfffffff8;\r\n}\r\nstatic int cq_get_mtt_size(struct mlx4_cq_context *cqc)\r\n{\r\nint log_cq_size = (be32_to_cpu(cqc->logsize_usrpage) >> 24) & 0x1f;\r\nint page_shift = (cqc->log_page_size & 0x3f) + 12;\r\nif (log_cq_size + 5 < page_shift)\r\nreturn 1;\r\nreturn 1 << (log_cq_size + 5 - page_shift);\r\n}\r\nint mlx4_SW2HW_EQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint eqn = vhcr->in_modifier;\r\nint res_id = (slave << 8) | eqn;\r\nstruct mlx4_eq_context *eqc = inbox->buf;\r\nint mtt_base = eq_get_mtt_addr(eqc) / dev->caps.mtt_entry_sz;\r\nint mtt_size = eq_get_mtt_size(eqc);\r\nstruct res_eq *eq;\r\nstruct res_mtt *mtt;\r\nerr = add_res_range(dev, slave, res_id, 1, RES_EQ, 0);\r\nif (err)\r\nreturn err;\r\nerr = eq_res_start_move_to(dev, slave, res_id, RES_EQ_HW, &eq);\r\nif (err)\r\ngoto out_add;\r\nerr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\r\nif (err)\r\ngoto out_move;\r\nerr = check_mtt_range(dev, slave, mtt_base, mtt_size, mtt);\r\nif (err)\r\ngoto out_put;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto out_put;\r\natomic_inc(&mtt->ref_count);\r\neq->mtt = mtt;\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\nres_end_move(dev, slave, RES_EQ, res_id);\r\nreturn 0;\r\nout_put:\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\nout_move:\r\nres_abort_move(dev, slave, RES_EQ, res_id);\r\nout_add:\r\nrem_res_range(dev, slave, res_id, 1, RES_EQ, 0);\r\nreturn err;\r\n}\r\nstatic int get_containing_mtt(struct mlx4_dev *dev, int slave, int start,\r\nint len, struct res_mtt **res)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct res_mtt *mtt;\r\nint err = -EINVAL;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nlist_for_each_entry(mtt, &tracker->slave_list[slave].res_list[RES_MTT],\r\ncom.list) {\r\nif (!check_mtt_range(dev, slave, start, len, mtt)) {\r\n*res = mtt;\r\nmtt->com.from_state = mtt->com.state;\r\nmtt->com.state = RES_MTT_BUSY;\r\nerr = 0;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nreturn err;\r\n}\r\nint mlx4_WRITE_MTT_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nstruct mlx4_mtt mtt;\r\n__be64 *page_list = inbox->buf;\r\nu64 *pg_list = (u64 *)page_list;\r\nint i;\r\nstruct res_mtt *rmtt = NULL;\r\nint start = be64_to_cpu(page_list[0]);\r\nint npages = vhcr->in_modifier;\r\nint err;\r\nerr = get_containing_mtt(dev, slave, start, npages, &rmtt);\r\nif (err)\r\nreturn err;\r\nmtt.offset = 0;\r\nmtt.order = 0;\r\nmtt.page_shift = 0;\r\nfor (i = 0; i < npages; ++i)\r\npg_list[i + 2] = (be64_to_cpu(page_list[i + 2]) & ~1ULL);\r\nerr = __mlx4_write_mtt(dev, &mtt, be64_to_cpu(page_list[0]), npages,\r\n((u64 *)page_list + 2));\r\nif (rmtt)\r\nput_res(dev, slave, rmtt->com.res_id, RES_MTT);\r\nreturn err;\r\n}\r\nint mlx4_HW2SW_EQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint eqn = vhcr->in_modifier;\r\nint res_id = eqn | (slave << 8);\r\nstruct res_eq *eq;\r\nint err;\r\nerr = eq_res_start_move_to(dev, slave, res_id, RES_EQ_RESERVED, &eq);\r\nif (err)\r\nreturn err;\r\nerr = get_res(dev, slave, eq->mtt->com.res_id, RES_MTT, NULL);\r\nif (err)\r\ngoto ex_abort;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto ex_put;\r\natomic_dec(&eq->mtt->ref_count);\r\nput_res(dev, slave, eq->mtt->com.res_id, RES_MTT);\r\nres_end_move(dev, slave, RES_EQ, res_id);\r\nrem_res_range(dev, slave, res_id, 1, RES_EQ, 0);\r\nreturn 0;\r\nex_put:\r\nput_res(dev, slave, eq->mtt->com.res_id, RES_MTT);\r\nex_abort:\r\nres_abort_move(dev, slave, RES_EQ, res_id);\r\nreturn err;\r\n}\r\nint mlx4_GEN_EQE(struct mlx4_dev *dev, int slave, struct mlx4_eqe *eqe)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_slave_event_eq_info *event_eq;\r\nstruct mlx4_cmd_mailbox *mailbox;\r\nu32 in_modifier = 0;\r\nint err;\r\nint res_id;\r\nstruct res_eq *req;\r\nif (!priv->mfunc.master.slave_state)\r\nreturn -EINVAL;\r\nevent_eq = &priv->mfunc.master.slave_state[slave].event_eq[eqe->type];\r\nif (event_eq->eqn < 0)\r\nreturn 0;\r\nmutex_lock(&priv->mfunc.master.gen_eqe_mutex[slave]);\r\nres_id = (slave << 8) | event_eq->eqn;\r\nerr = get_res(dev, slave, res_id, RES_EQ, &req);\r\nif (err)\r\ngoto unlock;\r\nif (req->com.from_state != RES_EQ_HW) {\r\nerr = -EINVAL;\r\ngoto put;\r\n}\r\nmailbox = mlx4_alloc_cmd_mailbox(dev);\r\nif (IS_ERR(mailbox)) {\r\nerr = PTR_ERR(mailbox);\r\ngoto put;\r\n}\r\nif (eqe->type == MLX4_EVENT_TYPE_CMD) {\r\n++event_eq->token;\r\neqe->event.cmd.token = cpu_to_be16(event_eq->token);\r\n}\r\nmemcpy(mailbox->buf, (u8 *) eqe, 28);\r\nin_modifier = (slave & 0xff) | ((event_eq->eqn & 0xff) << 16);\r\nerr = mlx4_cmd(dev, mailbox->dma, in_modifier, 0,\r\nMLX4_CMD_GEN_EQE, MLX4_CMD_TIME_CLASS_B,\r\nMLX4_CMD_NATIVE);\r\nput_res(dev, slave, res_id, RES_EQ);\r\nmutex_unlock(&priv->mfunc.master.gen_eqe_mutex[slave]);\r\nmlx4_free_cmd_mailbox(dev, mailbox);\r\nreturn err;\r\nput:\r\nput_res(dev, slave, res_id, RES_EQ);\r\nunlock:\r\nmutex_unlock(&priv->mfunc.master.gen_eqe_mutex[slave]);\r\nreturn err;\r\n}\r\nint mlx4_QUERY_EQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint eqn = vhcr->in_modifier;\r\nint res_id = eqn | (slave << 8);\r\nstruct res_eq *eq;\r\nint err;\r\nerr = get_res(dev, slave, res_id, RES_EQ, &eq);\r\nif (err)\r\nreturn err;\r\nif (eq->com.from_state != RES_EQ_HW) {\r\nerr = -EINVAL;\r\ngoto ex_put;\r\n}\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nex_put:\r\nput_res(dev, slave, res_id, RES_EQ);\r\nreturn err;\r\n}\r\nint mlx4_SW2HW_CQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint cqn = vhcr->in_modifier;\r\nstruct mlx4_cq_context *cqc = inbox->buf;\r\nint mtt_base = cq_get_mtt_addr(cqc) / dev->caps.mtt_entry_sz;\r\nstruct res_cq *cq;\r\nstruct res_mtt *mtt;\r\nerr = cq_res_start_move_to(dev, slave, cqn, RES_CQ_HW, &cq);\r\nif (err)\r\nreturn err;\r\nerr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\r\nif (err)\r\ngoto out_move;\r\nerr = check_mtt_range(dev, slave, mtt_base, cq_get_mtt_size(cqc), mtt);\r\nif (err)\r\ngoto out_put;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto out_put;\r\natomic_inc(&mtt->ref_count);\r\ncq->mtt = mtt;\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\nres_end_move(dev, slave, RES_CQ, cqn);\r\nreturn 0;\r\nout_put:\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\nout_move:\r\nres_abort_move(dev, slave, RES_CQ, cqn);\r\nreturn err;\r\n}\r\nint mlx4_HW2SW_CQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint cqn = vhcr->in_modifier;\r\nstruct res_cq *cq;\r\nerr = cq_res_start_move_to(dev, slave, cqn, RES_CQ_ALLOCATED, &cq);\r\nif (err)\r\nreturn err;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto out_move;\r\natomic_dec(&cq->mtt->ref_count);\r\nres_end_move(dev, slave, RES_CQ, cqn);\r\nreturn 0;\r\nout_move:\r\nres_abort_move(dev, slave, RES_CQ, cqn);\r\nreturn err;\r\n}\r\nint mlx4_QUERY_CQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint cqn = vhcr->in_modifier;\r\nstruct res_cq *cq;\r\nint err;\r\nerr = get_res(dev, slave, cqn, RES_CQ, &cq);\r\nif (err)\r\nreturn err;\r\nif (cq->com.from_state != RES_CQ_HW)\r\ngoto ex_put;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nex_put:\r\nput_res(dev, slave, cqn, RES_CQ);\r\nreturn err;\r\n}\r\nstatic int handle_resize(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd,\r\nstruct res_cq *cq)\r\n{\r\nint err;\r\nstruct res_mtt *orig_mtt;\r\nstruct res_mtt *mtt;\r\nstruct mlx4_cq_context *cqc = inbox->buf;\r\nint mtt_base = cq_get_mtt_addr(cqc) / dev->caps.mtt_entry_sz;\r\nerr = get_res(dev, slave, cq->mtt->com.res_id, RES_MTT, &orig_mtt);\r\nif (err)\r\nreturn err;\r\nif (orig_mtt != cq->mtt) {\r\nerr = -EINVAL;\r\ngoto ex_put;\r\n}\r\nerr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\r\nif (err)\r\ngoto ex_put;\r\nerr = check_mtt_range(dev, slave, mtt_base, cq_get_mtt_size(cqc), mtt);\r\nif (err)\r\ngoto ex_put1;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto ex_put1;\r\natomic_dec(&orig_mtt->ref_count);\r\nput_res(dev, slave, orig_mtt->com.res_id, RES_MTT);\r\natomic_inc(&mtt->ref_count);\r\ncq->mtt = mtt;\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\nreturn 0;\r\nex_put1:\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\nex_put:\r\nput_res(dev, slave, orig_mtt->com.res_id, RES_MTT);\r\nreturn err;\r\n}\r\nint mlx4_MODIFY_CQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint cqn = vhcr->in_modifier;\r\nstruct res_cq *cq;\r\nint err;\r\nerr = get_res(dev, slave, cqn, RES_CQ, &cq);\r\nif (err)\r\nreturn err;\r\nif (cq->com.from_state != RES_CQ_HW)\r\ngoto ex_put;\r\nif (vhcr->op_modifier == 0) {\r\nerr = handle_resize(dev, slave, vhcr, inbox, outbox, cmd, cq);\r\ngoto ex_put;\r\n}\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nex_put:\r\nput_res(dev, slave, cqn, RES_CQ);\r\nreturn err;\r\n}\r\nstatic int srq_get_mtt_size(struct mlx4_srq_context *srqc)\r\n{\r\nint log_srq_size = (be32_to_cpu(srqc->state_logsize_srqn) >> 24) & 0xf;\r\nint log_rq_stride = srqc->logstride & 7;\r\nint page_shift = (srqc->log_page_size & 0x3f) + 12;\r\nif (log_srq_size + log_rq_stride + 4 < page_shift)\r\nreturn 1;\r\nreturn 1 << (log_srq_size + log_rq_stride + 4 - page_shift);\r\n}\r\nint mlx4_SW2HW_SRQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint srqn = vhcr->in_modifier;\r\nstruct res_mtt *mtt;\r\nstruct res_srq *srq;\r\nstruct mlx4_srq_context *srqc = inbox->buf;\r\nint mtt_base = srq_get_mtt_addr(srqc) / dev->caps.mtt_entry_sz;\r\nif (srqn != (be32_to_cpu(srqc->state_logsize_srqn) & 0xffffff))\r\nreturn -EINVAL;\r\nerr = srq_res_start_move_to(dev, slave, srqn, RES_SRQ_HW, &srq);\r\nif (err)\r\nreturn err;\r\nerr = get_res(dev, slave, mtt_base, RES_MTT, &mtt);\r\nif (err)\r\ngoto ex_abort;\r\nerr = check_mtt_range(dev, slave, mtt_base, srq_get_mtt_size(srqc),\r\nmtt);\r\nif (err)\r\ngoto ex_put_mtt;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto ex_put_mtt;\r\natomic_inc(&mtt->ref_count);\r\nsrq->mtt = mtt;\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\nres_end_move(dev, slave, RES_SRQ, srqn);\r\nreturn 0;\r\nex_put_mtt:\r\nput_res(dev, slave, mtt->com.res_id, RES_MTT);\r\nex_abort:\r\nres_abort_move(dev, slave, RES_SRQ, srqn);\r\nreturn err;\r\n}\r\nint mlx4_HW2SW_SRQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint srqn = vhcr->in_modifier;\r\nstruct res_srq *srq;\r\nerr = srq_res_start_move_to(dev, slave, srqn, RES_SRQ_ALLOCATED, &srq);\r\nif (err)\r\nreturn err;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto ex_abort;\r\natomic_dec(&srq->mtt->ref_count);\r\nif (srq->cq)\r\natomic_dec(&srq->cq->ref_count);\r\nres_end_move(dev, slave, RES_SRQ, srqn);\r\nreturn 0;\r\nex_abort:\r\nres_abort_move(dev, slave, RES_SRQ, srqn);\r\nreturn err;\r\n}\r\nint mlx4_QUERY_SRQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint srqn = vhcr->in_modifier;\r\nstruct res_srq *srq;\r\nerr = get_res(dev, slave, srqn, RES_SRQ, &srq);\r\nif (err)\r\nreturn err;\r\nif (srq->com.from_state != RES_SRQ_HW) {\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nout:\r\nput_res(dev, slave, srqn, RES_SRQ);\r\nreturn err;\r\n}\r\nint mlx4_ARM_SRQ_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint srqn = vhcr->in_modifier;\r\nstruct res_srq *srq;\r\nerr = get_res(dev, slave, srqn, RES_SRQ, &srq);\r\nif (err)\r\nreturn err;\r\nif (srq->com.from_state != RES_SRQ_HW) {\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nout:\r\nput_res(dev, slave, srqn, RES_SRQ);\r\nreturn err;\r\n}\r\nint mlx4_GEN_QP_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint qpn = vhcr->in_modifier & 0x7fffff;\r\nstruct res_qp *qp;\r\nerr = get_res(dev, slave, qpn, RES_QP, &qp);\r\nif (err)\r\nreturn err;\r\nif (qp->com.from_state != RES_QP_HW) {\r\nerr = -EBUSY;\r\ngoto out;\r\n}\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nout:\r\nput_res(dev, slave, qpn, RES_QP);\r\nreturn err;\r\n}\r\nint mlx4_INIT2RTR_QP_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nstruct mlx4_qp_context *qpc = inbox->buf + 8;\r\nupdate_ud_gid(dev, qpc, (u8)slave);\r\nreturn mlx4_GEN_QP_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\n}\r\nint mlx4_2RST_QP_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint qpn = vhcr->in_modifier & 0x7fffff;\r\nstruct res_qp *qp;\r\nerr = qp_res_start_move_to(dev, slave, qpn, RES_QP_MAPPED, &qp, 0);\r\nif (err)\r\nreturn err;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nif (err)\r\ngoto ex_abort;\r\natomic_dec(&qp->mtt->ref_count);\r\natomic_dec(&qp->rcq->ref_count);\r\natomic_dec(&qp->scq->ref_count);\r\nif (qp->srq)\r\natomic_dec(&qp->srq->ref_count);\r\nres_end_move(dev, slave, RES_QP, qpn);\r\nreturn 0;\r\nex_abort:\r\nres_abort_move(dev, slave, RES_QP, qpn);\r\nreturn err;\r\n}\r\nstatic struct res_gid *find_gid(struct mlx4_dev *dev, int slave,\r\nstruct res_qp *rqp, u8 *gid)\r\n{\r\nstruct res_gid *res;\r\nlist_for_each_entry(res, &rqp->mcg_list, list) {\r\nif (!memcmp(res->gid, gid, 16))\r\nreturn res;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int add_mcg_res(struct mlx4_dev *dev, int slave, struct res_qp *rqp,\r\nu8 *gid, enum mlx4_protocol prot,\r\nenum mlx4_steer_type steer)\r\n{\r\nstruct res_gid *res;\r\nint err;\r\nres = kzalloc(sizeof *res, GFP_KERNEL);\r\nif (!res)\r\nreturn -ENOMEM;\r\nspin_lock_irq(&rqp->mcg_spl);\r\nif (find_gid(dev, slave, rqp, gid)) {\r\nkfree(res);\r\nerr = -EEXIST;\r\n} else {\r\nmemcpy(res->gid, gid, 16);\r\nres->prot = prot;\r\nres->steer = steer;\r\nlist_add_tail(&res->list, &rqp->mcg_list);\r\nerr = 0;\r\n}\r\nspin_unlock_irq(&rqp->mcg_spl);\r\nreturn err;\r\n}\r\nstatic int rem_mcg_res(struct mlx4_dev *dev, int slave, struct res_qp *rqp,\r\nu8 *gid, enum mlx4_protocol prot,\r\nenum mlx4_steer_type steer)\r\n{\r\nstruct res_gid *res;\r\nint err;\r\nspin_lock_irq(&rqp->mcg_spl);\r\nres = find_gid(dev, slave, rqp, gid);\r\nif (!res || res->prot != prot || res->steer != steer)\r\nerr = -EINVAL;\r\nelse {\r\nlist_del(&res->list);\r\nkfree(res);\r\nerr = 0;\r\n}\r\nspin_unlock_irq(&rqp->mcg_spl);\r\nreturn err;\r\n}\r\nint mlx4_QP_ATTACH_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nstruct mlx4_qp qp;\r\nu8 *gid = inbox->buf;\r\nenum mlx4_protocol prot = (vhcr->in_modifier >> 28) & 0x7;\r\nint err, err1;\r\nint qpn;\r\nstruct res_qp *rqp;\r\nint attach = vhcr->op_modifier;\r\nint block_loopback = vhcr->in_modifier >> 31;\r\nu8 steer_type_mask = 2;\r\nenum mlx4_steer_type type = (gid[7] & steer_type_mask) >> 1;\r\nqpn = vhcr->in_modifier & 0xffffff;\r\nerr = get_res(dev, slave, qpn, RES_QP, &rqp);\r\nif (err)\r\nreturn err;\r\nqp.qpn = qpn;\r\nif (attach) {\r\nerr = add_mcg_res(dev, slave, rqp, gid, prot, type);\r\nif (err)\r\ngoto ex_put;\r\nerr = mlx4_qp_attach_common(dev, &qp, gid,\r\nblock_loopback, prot, type);\r\nif (err)\r\ngoto ex_rem;\r\n} else {\r\nerr = rem_mcg_res(dev, slave, rqp, gid, prot, type);\r\nif (err)\r\ngoto ex_put;\r\nerr = mlx4_qp_detach_common(dev, &qp, gid, prot, type);\r\n}\r\nput_res(dev, slave, qpn, RES_QP);\r\nreturn 0;\r\nex_rem:\r\nerr1 = rem_mcg_res(dev, slave, rqp, gid, prot, type);\r\nex_put:\r\nput_res(dev, slave, qpn, RES_QP);\r\nreturn err;\r\n}\r\nint mlx4_QUERY_IF_STAT_wrapper(struct mlx4_dev *dev, int slave,\r\nstruct mlx4_vhcr *vhcr,\r\nstruct mlx4_cmd_mailbox *inbox,\r\nstruct mlx4_cmd_mailbox *outbox,\r\nstruct mlx4_cmd_info *cmd)\r\n{\r\nint err;\r\nint index = vhcr->in_modifier & 0xffff;\r\nerr = get_res(dev, slave, index, RES_COUNTER, NULL);\r\nif (err)\r\nreturn err;\r\nerr = mlx4_DMA_wrapper(dev, slave, vhcr, inbox, outbox, cmd);\r\nput_res(dev, slave, index, RES_COUNTER);\r\nreturn err;\r\n}\r\nstatic void detach_qp(struct mlx4_dev *dev, int slave, struct res_qp *rqp)\r\n{\r\nstruct res_gid *rgid;\r\nstruct res_gid *tmp;\r\nint err;\r\nstruct mlx4_qp qp;\r\nlist_for_each_entry_safe(rgid, tmp, &rqp->mcg_list, list) {\r\nqp.qpn = rqp->local_qpn;\r\nerr = mlx4_qp_detach_common(dev, &qp, rgid->gid, rgid->prot,\r\nrgid->steer);\r\nlist_del(&rgid->list);\r\nkfree(rgid);\r\n}\r\n}\r\nstatic int _move_all_busy(struct mlx4_dev *dev, int slave,\r\nenum mlx4_resource type, int print)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker =\r\n&priv->mfunc.master.res_tracker;\r\nstruct list_head *rlist = &tracker->slave_list[slave].res_list[type];\r\nstruct res_common *r;\r\nstruct res_common *tmp;\r\nint busy;\r\nbusy = 0;\r\nspin_lock_irq(mlx4_tlock(dev));\r\nlist_for_each_entry_safe(r, tmp, rlist, list) {\r\nif (r->owner == slave) {\r\nif (!r->removing) {\r\nif (r->state == RES_ANY_BUSY) {\r\nif (print)\r\nmlx4_dbg(dev,\r\n"%s id 0x%x is busy\n",\r\nResourceType(type),\r\nr->res_id);\r\n++busy;\r\n} else {\r\nr->from_state = r->state;\r\nr->state = RES_ANY_BUSY;\r\nr->removing = 1;\r\n}\r\n}\r\n}\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nreturn busy;\r\n}\r\nstatic int move_all_busy(struct mlx4_dev *dev, int slave,\r\nenum mlx4_resource type)\r\n{\r\nunsigned long begin;\r\nint busy;\r\nbegin = jiffies;\r\ndo {\r\nbusy = _move_all_busy(dev, slave, type, 0);\r\nif (time_after(jiffies, begin + 5 * HZ))\r\nbreak;\r\nif (busy)\r\ncond_resched();\r\n} while (busy);\r\nif (busy)\r\nbusy = _move_all_busy(dev, slave, type, 1);\r\nreturn busy;\r\n}\r\nstatic void rem_slave_qps(struct mlx4_dev *dev, int slave)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct list_head *qp_list =\r\n&tracker->slave_list[slave].res_list[RES_QP];\r\nstruct res_qp *qp;\r\nstruct res_qp *tmp;\r\nint state;\r\nu64 in_param;\r\nint qpn;\r\nint err;\r\nerr = move_all_busy(dev, slave, RES_QP);\r\nif (err)\r\nmlx4_warn(dev, "rem_slave_qps: Could not move all qps to busy"\r\n"for slave %d\n", slave);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nlist_for_each_entry_safe(qp, tmp, qp_list, com.list) {\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nif (qp->com.owner == slave) {\r\nqpn = qp->com.res_id;\r\ndetach_qp(dev, slave, qp);\r\nstate = qp->com.from_state;\r\nwhile (state != 0) {\r\nswitch (state) {\r\ncase RES_QP_RESERVED:\r\nspin_lock_irq(mlx4_tlock(dev));\r\nradix_tree_delete(&tracker->res_tree[RES_QP],\r\nqp->com.res_id);\r\nlist_del(&qp->com.list);\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nkfree(qp);\r\nstate = 0;\r\nbreak;\r\ncase RES_QP_MAPPED:\r\nif (!valid_reserved(dev, slave, qpn))\r\n__mlx4_qp_free_icm(dev, qpn);\r\nstate = RES_QP_RESERVED;\r\nbreak;\r\ncase RES_QP_HW:\r\nin_param = slave;\r\nerr = mlx4_cmd(dev, in_param,\r\nqp->local_qpn, 2,\r\nMLX4_CMD_2RST_QP,\r\nMLX4_CMD_TIME_CLASS_A,\r\nMLX4_CMD_NATIVE);\r\nif (err)\r\nmlx4_dbg(dev, "rem_slave_qps: failed"\r\n" to move slave %d qpn %d to"\r\n" reset\n", slave,\r\nqp->local_qpn);\r\natomic_dec(&qp->rcq->ref_count);\r\natomic_dec(&qp->scq->ref_count);\r\natomic_dec(&qp->mtt->ref_count);\r\nif (qp->srq)\r\natomic_dec(&qp->srq->ref_count);\r\nstate = RES_QP_MAPPED;\r\nbreak;\r\ndefault:\r\nstate = 0;\r\n}\r\n}\r\n}\r\nspin_lock_irq(mlx4_tlock(dev));\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\n}\r\nstatic void rem_slave_srqs(struct mlx4_dev *dev, int slave)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct list_head *srq_list =\r\n&tracker->slave_list[slave].res_list[RES_SRQ];\r\nstruct res_srq *srq;\r\nstruct res_srq *tmp;\r\nint state;\r\nu64 in_param;\r\nLIST_HEAD(tlist);\r\nint srqn;\r\nint err;\r\nerr = move_all_busy(dev, slave, RES_SRQ);\r\nif (err)\r\nmlx4_warn(dev, "rem_slave_srqs: Could not move all srqs to "\r\n"busy for slave %d\n", slave);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nlist_for_each_entry_safe(srq, tmp, srq_list, com.list) {\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nif (srq->com.owner == slave) {\r\nsrqn = srq->com.res_id;\r\nstate = srq->com.from_state;\r\nwhile (state != 0) {\r\nswitch (state) {\r\ncase RES_SRQ_ALLOCATED:\r\n__mlx4_srq_free_icm(dev, srqn);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nradix_tree_delete(&tracker->res_tree[RES_SRQ],\r\nsrqn);\r\nlist_del(&srq->com.list);\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nkfree(srq);\r\nstate = 0;\r\nbreak;\r\ncase RES_SRQ_HW:\r\nin_param = slave;\r\nerr = mlx4_cmd(dev, in_param, srqn, 1,\r\nMLX4_CMD_HW2SW_SRQ,\r\nMLX4_CMD_TIME_CLASS_A,\r\nMLX4_CMD_NATIVE);\r\nif (err)\r\nmlx4_dbg(dev, "rem_slave_srqs: failed"\r\n" to move slave %d srq %d to"\r\n" SW ownership\n",\r\nslave, srqn);\r\natomic_dec(&srq->mtt->ref_count);\r\nif (srq->cq)\r\natomic_dec(&srq->cq->ref_count);\r\nstate = RES_SRQ_ALLOCATED;\r\nbreak;\r\ndefault:\r\nstate = 0;\r\n}\r\n}\r\n}\r\nspin_lock_irq(mlx4_tlock(dev));\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\n}\r\nstatic void rem_slave_cqs(struct mlx4_dev *dev, int slave)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct list_head *cq_list =\r\n&tracker->slave_list[slave].res_list[RES_CQ];\r\nstruct res_cq *cq;\r\nstruct res_cq *tmp;\r\nint state;\r\nu64 in_param;\r\nLIST_HEAD(tlist);\r\nint cqn;\r\nint err;\r\nerr = move_all_busy(dev, slave, RES_CQ);\r\nif (err)\r\nmlx4_warn(dev, "rem_slave_cqs: Could not move all cqs to "\r\n"busy for slave %d\n", slave);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nlist_for_each_entry_safe(cq, tmp, cq_list, com.list) {\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nif (cq->com.owner == slave && !atomic_read(&cq->ref_count)) {\r\ncqn = cq->com.res_id;\r\nstate = cq->com.from_state;\r\nwhile (state != 0) {\r\nswitch (state) {\r\ncase RES_CQ_ALLOCATED:\r\n__mlx4_cq_free_icm(dev, cqn);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nradix_tree_delete(&tracker->res_tree[RES_CQ],\r\ncqn);\r\nlist_del(&cq->com.list);\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nkfree(cq);\r\nstate = 0;\r\nbreak;\r\ncase RES_CQ_HW:\r\nin_param = slave;\r\nerr = mlx4_cmd(dev, in_param, cqn, 1,\r\nMLX4_CMD_HW2SW_CQ,\r\nMLX4_CMD_TIME_CLASS_A,\r\nMLX4_CMD_NATIVE);\r\nif (err)\r\nmlx4_dbg(dev, "rem_slave_cqs: failed"\r\n" to move slave %d cq %d to"\r\n" SW ownership\n",\r\nslave, cqn);\r\natomic_dec(&cq->mtt->ref_count);\r\nstate = RES_CQ_ALLOCATED;\r\nbreak;\r\ndefault:\r\nstate = 0;\r\n}\r\n}\r\n}\r\nspin_lock_irq(mlx4_tlock(dev));\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\n}\r\nstatic void rem_slave_mrs(struct mlx4_dev *dev, int slave)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct list_head *mpt_list =\r\n&tracker->slave_list[slave].res_list[RES_MPT];\r\nstruct res_mpt *mpt;\r\nstruct res_mpt *tmp;\r\nint state;\r\nu64 in_param;\r\nLIST_HEAD(tlist);\r\nint mptn;\r\nint err;\r\nerr = move_all_busy(dev, slave, RES_MPT);\r\nif (err)\r\nmlx4_warn(dev, "rem_slave_mrs: Could not move all mpts to "\r\n"busy for slave %d\n", slave);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nlist_for_each_entry_safe(mpt, tmp, mpt_list, com.list) {\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nif (mpt->com.owner == slave) {\r\nmptn = mpt->com.res_id;\r\nstate = mpt->com.from_state;\r\nwhile (state != 0) {\r\nswitch (state) {\r\ncase RES_MPT_RESERVED:\r\n__mlx4_mr_release(dev, mpt->key);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nradix_tree_delete(&tracker->res_tree[RES_MPT],\r\nmptn);\r\nlist_del(&mpt->com.list);\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nkfree(mpt);\r\nstate = 0;\r\nbreak;\r\ncase RES_MPT_MAPPED:\r\n__mlx4_mr_free_icm(dev, mpt->key);\r\nstate = RES_MPT_RESERVED;\r\nbreak;\r\ncase RES_MPT_HW:\r\nin_param = slave;\r\nerr = mlx4_cmd(dev, in_param, mptn, 0,\r\nMLX4_CMD_HW2SW_MPT,\r\nMLX4_CMD_TIME_CLASS_A,\r\nMLX4_CMD_NATIVE);\r\nif (err)\r\nmlx4_dbg(dev, "rem_slave_mrs: failed"\r\n" to move slave %d mpt %d to"\r\n" SW ownership\n",\r\nslave, mptn);\r\nif (mpt->mtt)\r\natomic_dec(&mpt->mtt->ref_count);\r\nstate = RES_MPT_MAPPED;\r\nbreak;\r\ndefault:\r\nstate = 0;\r\n}\r\n}\r\n}\r\nspin_lock_irq(mlx4_tlock(dev));\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\n}\r\nstatic void rem_slave_mtts(struct mlx4_dev *dev, int slave)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker =\r\n&priv->mfunc.master.res_tracker;\r\nstruct list_head *mtt_list =\r\n&tracker->slave_list[slave].res_list[RES_MTT];\r\nstruct res_mtt *mtt;\r\nstruct res_mtt *tmp;\r\nint state;\r\nLIST_HEAD(tlist);\r\nint base;\r\nint err;\r\nerr = move_all_busy(dev, slave, RES_MTT);\r\nif (err)\r\nmlx4_warn(dev, "rem_slave_mtts: Could not move all mtts to "\r\n"busy for slave %d\n", slave);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nlist_for_each_entry_safe(mtt, tmp, mtt_list, com.list) {\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nif (mtt->com.owner == slave) {\r\nbase = mtt->com.res_id;\r\nstate = mtt->com.from_state;\r\nwhile (state != 0) {\r\nswitch (state) {\r\ncase RES_MTT_ALLOCATED:\r\n__mlx4_free_mtt_range(dev, base,\r\nmtt->order);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nradix_tree_delete(&tracker->res_tree[RES_MTT],\r\nbase);\r\nlist_del(&mtt->com.list);\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nkfree(mtt);\r\nstate = 0;\r\nbreak;\r\ndefault:\r\nstate = 0;\r\n}\r\n}\r\n}\r\nspin_lock_irq(mlx4_tlock(dev));\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\n}\r\nstatic void rem_slave_eqs(struct mlx4_dev *dev, int slave)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_resource_tracker *tracker = &priv->mfunc.master.res_tracker;\r\nstruct list_head *eq_list =\r\n&tracker->slave_list[slave].res_list[RES_EQ];\r\nstruct res_eq *eq;\r\nstruct res_eq *tmp;\r\nint err;\r\nint state;\r\nLIST_HEAD(tlist);\r\nint eqn;\r\nstruct mlx4_cmd_mailbox *mailbox;\r\nerr = move_all_busy(dev, slave, RES_EQ);\r\nif (err)\r\nmlx4_warn(dev, "rem_slave_eqs: Could not move all eqs to "\r\n"busy for slave %d\n", slave);\r\nspin_lock_irq(mlx4_tlock(dev));\r\nlist_for_each_entry_safe(eq, tmp, eq_list, com.list) {\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nif (eq->com.owner == slave) {\r\neqn = eq->com.res_id;\r\nstate = eq->com.from_state;\r\nwhile (state != 0) {\r\nswitch (state) {\r\ncase RES_EQ_RESERVED:\r\nspin_lock_irq(mlx4_tlock(dev));\r\nradix_tree_delete(&tracker->res_tree[RES_EQ],\r\neqn);\r\nlist_del(&eq->com.list);\r\nspin_unlock_irq(mlx4_tlock(dev));\r\nkfree(eq);\r\nstate = 0;\r\nbreak;\r\ncase RES_EQ_HW:\r\nmailbox = mlx4_alloc_cmd_mailbox(dev);\r\nif (IS_ERR(mailbox)) {\r\ncond_resched();\r\ncontinue;\r\n}\r\nerr = mlx4_cmd_box(dev, slave, 0,\r\neqn & 0xff, 0,\r\nMLX4_CMD_HW2SW_EQ,\r\nMLX4_CMD_TIME_CLASS_A,\r\nMLX4_CMD_NATIVE);\r\nmlx4_dbg(dev, "rem_slave_eqs: failed"\r\n" to move slave %d eqs %d to"\r\n" SW ownership\n", slave, eqn);\r\nmlx4_free_cmd_mailbox(dev, mailbox);\r\nif (!err) {\r\natomic_dec(&eq->mtt->ref_count);\r\nstate = RES_EQ_RESERVED;\r\n}\r\nbreak;\r\ndefault:\r\nstate = 0;\r\n}\r\n}\r\n}\r\nspin_lock_irq(mlx4_tlock(dev));\r\n}\r\nspin_unlock_irq(mlx4_tlock(dev));\r\n}\r\nvoid mlx4_delete_all_resources_for_slave(struct mlx4_dev *dev, int slave)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nmutex_lock(&priv->mfunc.master.res_tracker.slave_list[slave].mutex);\r\nrem_slave_macs(dev, slave);\r\nrem_slave_qps(dev, slave);\r\nrem_slave_srqs(dev, slave);\r\nrem_slave_cqs(dev, slave);\r\nrem_slave_mrs(dev, slave);\r\nrem_slave_eqs(dev, slave);\r\nrem_slave_mtts(dev, slave);\r\nmutex_unlock(&priv->mfunc.master.res_tracker.slave_list[slave].mutex);\r\n}
