static inline int offload_activated(struct t3cdev *tdev)\r\n{\r\nconst struct adapter *adapter = tdev2adap(tdev);\r\nreturn test_bit(OFFLOAD_DEVMAP_BIT, &adapter->open_device_map);\r\n}\r\nvoid cxgb3_register_client(struct cxgb3_client *client)\r\n{\r\nstruct t3cdev *tdev;\r\nmutex_lock(&cxgb3_db_lock);\r\nlist_add_tail(&client->client_list, &client_list);\r\nif (client->add) {\r\nlist_for_each_entry(tdev, &ofld_dev_list, ofld_dev_list) {\r\nif (offload_activated(tdev))\r\nclient->add(tdev);\r\n}\r\n}\r\nmutex_unlock(&cxgb3_db_lock);\r\n}\r\nvoid cxgb3_unregister_client(struct cxgb3_client *client)\r\n{\r\nstruct t3cdev *tdev;\r\nmutex_lock(&cxgb3_db_lock);\r\nlist_del(&client->client_list);\r\nif (client->remove) {\r\nlist_for_each_entry(tdev, &ofld_dev_list, ofld_dev_list) {\r\nif (offload_activated(tdev))\r\nclient->remove(tdev);\r\n}\r\n}\r\nmutex_unlock(&cxgb3_db_lock);\r\n}\r\nvoid cxgb3_add_clients(struct t3cdev *tdev)\r\n{\r\nstruct cxgb3_client *client;\r\nmutex_lock(&cxgb3_db_lock);\r\nlist_for_each_entry(client, &client_list, client_list) {\r\nif (client->add)\r\nclient->add(tdev);\r\n}\r\nmutex_unlock(&cxgb3_db_lock);\r\n}\r\nvoid cxgb3_remove_clients(struct t3cdev *tdev)\r\n{\r\nstruct cxgb3_client *client;\r\nmutex_lock(&cxgb3_db_lock);\r\nlist_for_each_entry(client, &client_list, client_list) {\r\nif (client->remove)\r\nclient->remove(tdev);\r\n}\r\nmutex_unlock(&cxgb3_db_lock);\r\n}\r\nvoid cxgb3_event_notify(struct t3cdev *tdev, u32 event, u32 port)\r\n{\r\nstruct cxgb3_client *client;\r\nmutex_lock(&cxgb3_db_lock);\r\nlist_for_each_entry(client, &client_list, client_list) {\r\nif (client->event_handler)\r\nclient->event_handler(tdev, event, port);\r\n}\r\nmutex_unlock(&cxgb3_db_lock);\r\n}\r\nstatic struct net_device *get_iff_from_mac(struct adapter *adapter,\r\nconst unsigned char *mac,\r\nunsigned int vlan)\r\n{\r\nint i;\r\nfor_each_port(adapter, i) {\r\nstruct net_device *dev = adapter->port[i];\r\nif (!memcmp(dev->dev_addr, mac, ETH_ALEN)) {\r\nif (vlan && vlan != VLAN_VID_MASK) {\r\nrcu_read_lock();\r\ndev = __vlan_find_dev_deep(dev, vlan);\r\nrcu_read_unlock();\r\n} else if (netif_is_bond_slave(dev)) {\r\nwhile (dev->master)\r\ndev = dev->master;\r\n}\r\nreturn dev;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic int cxgb_ulp_iscsi_ctl(struct adapter *adapter, unsigned int req,\r\nvoid *data)\r\n{\r\nint i;\r\nint ret = 0;\r\nunsigned int val = 0;\r\nstruct ulp_iscsi_info *uiip = data;\r\nswitch (req) {\r\ncase ULP_ISCSI_GET_PARAMS:\r\nuiip->pdev = adapter->pdev;\r\nuiip->llimit = t3_read_reg(adapter, A_ULPRX_ISCSI_LLIMIT);\r\nuiip->ulimit = t3_read_reg(adapter, A_ULPRX_ISCSI_ULIMIT);\r\nuiip->tagmask = t3_read_reg(adapter, A_ULPRX_ISCSI_TAGMASK);\r\nval = t3_read_reg(adapter, A_ULPRX_ISCSI_PSZ);\r\nfor (i = 0; i < 4; i++, val >>= 8)\r\nuiip->pgsz_factor[i] = val & 0xFF;\r\nval = t3_read_reg(adapter, A_TP_PARA_REG7);\r\nuiip->max_txsz =\r\nuiip->max_rxsz = min((val >> S_PMMAXXFERLEN0)&M_PMMAXXFERLEN0,\r\n(val >> S_PMMAXXFERLEN1)&M_PMMAXXFERLEN1);\r\nval = min(adapter->params.tp.tx_pg_size,\r\nt3_read_reg(adapter, A_PM1_TX_CFG) >> 17);\r\nuiip->max_txsz = min(val, uiip->max_txsz);\r\nval = t3_read_reg(adapter, A_TP_PARA_REG2);\r\nif ((val >> S_MAXRXDATA) != 0x3f60) {\r\nval &= (M_RXCOALESCESIZE << S_RXCOALESCESIZE);\r\nval |= V_MAXRXDATA(0x3f60);\r\nprintk(KERN_INFO\r\n"%s, iscsi set MaxRxData to 16224 (0x%x).\n",\r\nadapter->name, val);\r\nt3_write_reg(adapter, A_TP_PARA_REG2, val);\r\n}\r\nval = min(adapter->params.tp.rx_pg_size,\r\n((t3_read_reg(adapter, A_TP_PARA_REG2)) >>\r\nS_MAXRXDATA) & M_MAXRXDATA);\r\nuiip->max_rxsz = min(val, uiip->max_rxsz);\r\nbreak;\r\ncase ULP_ISCSI_SET_PARAMS:\r\nt3_write_reg(adapter, A_ULPRX_ISCSI_TAGMASK, uiip->tagmask);\r\nfor (i = 0; i < 4; i++)\r\nval |= (uiip->pgsz_factor[i] & 0xF) << (8 * i);\r\nif (val && (val != t3_read_reg(adapter, A_ULPRX_ISCSI_PSZ))) {\r\nprintk(KERN_INFO\r\n"%s, setting iscsi pgsz 0x%x, %u,%u,%u,%u.\n",\r\nadapter->name, val, uiip->pgsz_factor[0],\r\nuiip->pgsz_factor[1], uiip->pgsz_factor[2],\r\nuiip->pgsz_factor[3]);\r\nt3_write_reg(adapter, A_ULPRX_ISCSI_PSZ, val);\r\n}\r\nbreak;\r\ndefault:\r\nret = -EOPNOTSUPP;\r\n}\r\nreturn ret;\r\n}\r\nstatic int cxgb_rdma_ctl(struct adapter *adapter, unsigned int req, void *data)\r\n{\r\nint ret = 0;\r\nswitch (req) {\r\ncase RDMA_GET_PARAMS: {\r\nstruct rdma_info *rdma = data;\r\nstruct pci_dev *pdev = adapter->pdev;\r\nrdma->udbell_physbase = pci_resource_start(pdev, 2);\r\nrdma->udbell_len = pci_resource_len(pdev, 2);\r\nrdma->tpt_base =\r\nt3_read_reg(adapter, A_ULPTX_TPT_LLIMIT);\r\nrdma->tpt_top = t3_read_reg(adapter, A_ULPTX_TPT_ULIMIT);\r\nrdma->pbl_base =\r\nt3_read_reg(adapter, A_ULPTX_PBL_LLIMIT);\r\nrdma->pbl_top = t3_read_reg(adapter, A_ULPTX_PBL_ULIMIT);\r\nrdma->rqt_base = t3_read_reg(adapter, A_ULPRX_RQ_LLIMIT);\r\nrdma->rqt_top = t3_read_reg(adapter, A_ULPRX_RQ_ULIMIT);\r\nrdma->kdb_addr = adapter->regs + A_SG_KDOORBELL;\r\nrdma->pdev = pdev;\r\nbreak;\r\n}\r\ncase RDMA_CQ_OP:{\r\nunsigned long flags;\r\nstruct rdma_cq_op *rdma = data;\r\nspin_lock_irqsave(&adapter->sge.reg_lock, flags);\r\nret = t3_sge_cqcntxt_op(adapter, rdma->id, rdma->op,\r\nrdma->credits);\r\nspin_unlock_irqrestore(&adapter->sge.reg_lock, flags);\r\nbreak;\r\n}\r\ncase RDMA_GET_MEM:{\r\nstruct ch_mem_range *t = data;\r\nstruct mc7 *mem;\r\nif ((t->addr & 7) || (t->len & 7))\r\nreturn -EINVAL;\r\nif (t->mem_id == MEM_CM)\r\nmem = &adapter->cm;\r\nelse if (t->mem_id == MEM_PMRX)\r\nmem = &adapter->pmrx;\r\nelse if (t->mem_id == MEM_PMTX)\r\nmem = &adapter->pmtx;\r\nelse\r\nreturn -EINVAL;\r\nret =\r\nt3_mc7_bd_read(mem, t->addr / 8, t->len / 8,\r\n(u64 *) t->buf);\r\nif (ret)\r\nreturn ret;\r\nbreak;\r\n}\r\ncase RDMA_CQ_SETUP:{\r\nstruct rdma_cq_setup *rdma = data;\r\nspin_lock_irq(&adapter->sge.reg_lock);\r\nret =\r\nt3_sge_init_cqcntxt(adapter, rdma->id,\r\nrdma->base_addr, rdma->size,\r\nASYNC_NOTIF_RSPQ,\r\nrdma->ovfl_mode, rdma->credits,\r\nrdma->credit_thres);\r\nspin_unlock_irq(&adapter->sge.reg_lock);\r\nbreak;\r\n}\r\ncase RDMA_CQ_DISABLE:\r\nspin_lock_irq(&adapter->sge.reg_lock);\r\nret = t3_sge_disable_cqcntxt(adapter, *(unsigned int *)data);\r\nspin_unlock_irq(&adapter->sge.reg_lock);\r\nbreak;\r\ncase RDMA_CTRL_QP_SETUP:{\r\nstruct rdma_ctrlqp_setup *rdma = data;\r\nspin_lock_irq(&adapter->sge.reg_lock);\r\nret = t3_sge_init_ecntxt(adapter, FW_RI_SGEEC_START, 0,\r\nSGE_CNTXT_RDMA,\r\nASYNC_NOTIF_RSPQ,\r\nrdma->base_addr, rdma->size,\r\nFW_RI_TID_START, 1, 0);\r\nspin_unlock_irq(&adapter->sge.reg_lock);\r\nbreak;\r\n}\r\ncase RDMA_GET_MIB: {\r\nspin_lock(&adapter->stats_lock);\r\nt3_tp_get_mib_stats(adapter, (struct tp_mib_stats *)data);\r\nspin_unlock(&adapter->stats_lock);\r\nbreak;\r\n}\r\ndefault:\r\nret = -EOPNOTSUPP;\r\n}\r\nreturn ret;\r\n}\r\nstatic int cxgb_offload_ctl(struct t3cdev *tdev, unsigned int req, void *data)\r\n{\r\nstruct adapter *adapter = tdev2adap(tdev);\r\nstruct tid_range *tid;\r\nstruct mtutab *mtup;\r\nstruct iff_mac *iffmacp;\r\nstruct ddp_params *ddpp;\r\nstruct adap_ports *ports;\r\nstruct ofld_page_info *rx_page_info;\r\nstruct tp_params *tp = &adapter->params.tp;\r\nint i;\r\nswitch (req) {\r\ncase GET_MAX_OUTSTANDING_WR:\r\n*(unsigned int *)data = FW_WR_NUM;\r\nbreak;\r\ncase GET_WR_LEN:\r\n*(unsigned int *)data = WR_FLITS;\r\nbreak;\r\ncase GET_TX_MAX_CHUNK:\r\n*(unsigned int *)data = 1 << 20;\r\nbreak;\r\ncase GET_TID_RANGE:\r\ntid = data;\r\ntid->num = t3_mc5_size(&adapter->mc5) -\r\nadapter->params.mc5.nroutes -\r\nadapter->params.mc5.nfilters - adapter->params.mc5.nservers;\r\ntid->base = 0;\r\nbreak;\r\ncase GET_STID_RANGE:\r\ntid = data;\r\ntid->num = adapter->params.mc5.nservers;\r\ntid->base = t3_mc5_size(&adapter->mc5) - tid->num -\r\nadapter->params.mc5.nfilters - adapter->params.mc5.nroutes;\r\nbreak;\r\ncase GET_L2T_CAPACITY:\r\n*(unsigned int *)data = 2048;\r\nbreak;\r\ncase GET_MTUS:\r\nmtup = data;\r\nmtup->size = NMTUS;\r\nmtup->mtus = adapter->params.mtus;\r\nbreak;\r\ncase GET_IFF_FROM_MAC:\r\niffmacp = data;\r\niffmacp->dev = get_iff_from_mac(adapter, iffmacp->mac_addr,\r\niffmacp->vlan_tag &\r\nVLAN_VID_MASK);\r\nbreak;\r\ncase GET_DDP_PARAMS:\r\nddpp = data;\r\nddpp->llimit = t3_read_reg(adapter, A_ULPRX_TDDP_LLIMIT);\r\nddpp->ulimit = t3_read_reg(adapter, A_ULPRX_TDDP_ULIMIT);\r\nddpp->tag_mask = t3_read_reg(adapter, A_ULPRX_TDDP_TAGMASK);\r\nbreak;\r\ncase GET_PORTS:\r\nports = data;\r\nports->nports = adapter->params.nports;\r\nfor_each_port(adapter, i)\r\nports->lldevs[i] = adapter->port[i];\r\nbreak;\r\ncase ULP_ISCSI_GET_PARAMS:\r\ncase ULP_ISCSI_SET_PARAMS:\r\nif (!offload_running(adapter))\r\nreturn -EAGAIN;\r\nreturn cxgb_ulp_iscsi_ctl(adapter, req, data);\r\ncase RDMA_GET_PARAMS:\r\ncase RDMA_CQ_OP:\r\ncase RDMA_CQ_SETUP:\r\ncase RDMA_CQ_DISABLE:\r\ncase RDMA_CTRL_QP_SETUP:\r\ncase RDMA_GET_MEM:\r\ncase RDMA_GET_MIB:\r\nif (!offload_running(adapter))\r\nreturn -EAGAIN;\r\nreturn cxgb_rdma_ctl(adapter, req, data);\r\ncase GET_RX_PAGE_INFO:\r\nrx_page_info = data;\r\nrx_page_info->page_size = tp->rx_pg_size;\r\nrx_page_info->num = tp->rx_num_pgs;\r\nbreak;\r\ncase GET_ISCSI_IPV4ADDR: {\r\nstruct iscsi_ipv4addr *p = data;\r\nstruct port_info *pi = netdev_priv(p->dev);\r\np->ipv4addr = pi->iscsi_ipv4addr;\r\nbreak;\r\n}\r\ncase GET_EMBEDDED_INFO: {\r\nstruct ch_embedded_info *e = data;\r\nspin_lock(&adapter->stats_lock);\r\nt3_get_fw_version(adapter, &e->fw_vers);\r\nt3_get_tp_version(adapter, &e->tp_vers);\r\nspin_unlock(&adapter->stats_lock);\r\nbreak;\r\n}\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rx_offload_blackhole(struct t3cdev *dev, struct sk_buff **skbs,\r\nint n)\r\n{\r\nwhile (n--)\r\ndev_kfree_skb_any(skbs[n]);\r\nreturn 0;\r\n}\r\nstatic void dummy_neigh_update(struct t3cdev *dev, struct neighbour *neigh)\r\n{\r\n}\r\nvoid cxgb3_set_dummy_ops(struct t3cdev *dev)\r\n{\r\ndev->recv = rx_offload_blackhole;\r\ndev->neigh_update = dummy_neigh_update;\r\n}\r\nvoid *cxgb3_free_atid(struct t3cdev *tdev, int atid)\r\n{\r\nstruct tid_info *t = &(T3C_DATA(tdev))->tid_maps;\r\nunion active_open_entry *p = atid2entry(t, atid);\r\nvoid *ctx = p->t3c_tid.ctx;\r\nspin_lock_bh(&t->atid_lock);\r\np->next = t->afree;\r\nt->afree = p;\r\nt->atids_in_use--;\r\nspin_unlock_bh(&t->atid_lock);\r\nreturn ctx;\r\n}\r\nvoid cxgb3_free_stid(struct t3cdev *tdev, int stid)\r\n{\r\nstruct tid_info *t = &(T3C_DATA(tdev))->tid_maps;\r\nunion listen_entry *p = stid2entry(t, stid);\r\nspin_lock_bh(&t->stid_lock);\r\np->next = t->sfree;\r\nt->sfree = p;\r\nt->stids_in_use--;\r\nspin_unlock_bh(&t->stid_lock);\r\n}\r\nvoid cxgb3_insert_tid(struct t3cdev *tdev, struct cxgb3_client *client,\r\nvoid *ctx, unsigned int tid)\r\n{\r\nstruct tid_info *t = &(T3C_DATA(tdev))->tid_maps;\r\nt->tid_tab[tid].client = client;\r\nt->tid_tab[tid].ctx = ctx;\r\natomic_inc(&t->tids_in_use);\r\n}\r\nstatic inline void mk_tid_release(struct sk_buff *skb, unsigned int tid)\r\n{\r\nstruct cpl_tid_release *req;\r\nskb->priority = CPL_PRIORITY_SETUP;\r\nreq = (struct cpl_tid_release *)__skb_put(skb, sizeof(*req));\r\nreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_TID_RELEASE, tid));\r\n}\r\nstatic void t3_process_tid_release_list(struct work_struct *work)\r\n{\r\nstruct t3c_data *td = container_of(work, struct t3c_data,\r\ntid_release_task);\r\nstruct sk_buff *skb;\r\nstruct t3cdev *tdev = td->dev;\r\nspin_lock_bh(&td->tid_release_lock);\r\nwhile (td->tid_release_list) {\r\nstruct t3c_tid_entry *p = td->tid_release_list;\r\ntd->tid_release_list = p->ctx;\r\nspin_unlock_bh(&td->tid_release_lock);\r\nskb = alloc_skb(sizeof(struct cpl_tid_release),\r\nGFP_KERNEL);\r\nif (!skb)\r\nskb = td->nofail_skb;\r\nif (!skb) {\r\nspin_lock_bh(&td->tid_release_lock);\r\np->ctx = (void *)td->tid_release_list;\r\ntd->tid_release_list = (struct t3c_tid_entry *)p;\r\nbreak;\r\n}\r\nmk_tid_release(skb, p - td->tid_maps.tid_tab);\r\ncxgb3_ofld_send(tdev, skb);\r\np->ctx = NULL;\r\nif (skb == td->nofail_skb)\r\ntd->nofail_skb =\r\nalloc_skb(sizeof(struct cpl_tid_release),\r\nGFP_KERNEL);\r\nspin_lock_bh(&td->tid_release_lock);\r\n}\r\ntd->release_list_incomplete = (td->tid_release_list == NULL) ? 0 : 1;\r\nspin_unlock_bh(&td->tid_release_lock);\r\nif (!td->nofail_skb)\r\ntd->nofail_skb =\r\nalloc_skb(sizeof(struct cpl_tid_release),\r\nGFP_KERNEL);\r\n}\r\nvoid cxgb3_queue_tid_release(struct t3cdev *tdev, unsigned int tid)\r\n{\r\nstruct t3c_data *td = T3C_DATA(tdev);\r\nstruct t3c_tid_entry *p = &td->tid_maps.tid_tab[tid];\r\nspin_lock_bh(&td->tid_release_lock);\r\np->ctx = (void *)td->tid_release_list;\r\np->client = NULL;\r\ntd->tid_release_list = p;\r\nif (!p->ctx || td->release_list_incomplete)\r\nschedule_work(&td->tid_release_task);\r\nspin_unlock_bh(&td->tid_release_lock);\r\n}\r\nvoid cxgb3_remove_tid(struct t3cdev *tdev, void *ctx, unsigned int tid)\r\n{\r\nstruct tid_info *t = &(T3C_DATA(tdev))->tid_maps;\r\nBUG_ON(tid >= t->ntids);\r\nif (tdev->type == T3A)\r\n(void)cmpxchg(&t->tid_tab[tid].ctx, ctx, NULL);\r\nelse {\r\nstruct sk_buff *skb;\r\nskb = alloc_skb(sizeof(struct cpl_tid_release), GFP_ATOMIC);\r\nif (likely(skb)) {\r\nmk_tid_release(skb, tid);\r\ncxgb3_ofld_send(tdev, skb);\r\nt->tid_tab[tid].ctx = NULL;\r\n} else\r\ncxgb3_queue_tid_release(tdev, tid);\r\n}\r\natomic_dec(&t->tids_in_use);\r\n}\r\nint cxgb3_alloc_atid(struct t3cdev *tdev, struct cxgb3_client *client,\r\nvoid *ctx)\r\n{\r\nint atid = -1;\r\nstruct tid_info *t = &(T3C_DATA(tdev))->tid_maps;\r\nspin_lock_bh(&t->atid_lock);\r\nif (t->afree &&\r\nt->atids_in_use + atomic_read(&t->tids_in_use) + MC5_MIN_TIDS <=\r\nt->ntids) {\r\nunion active_open_entry *p = t->afree;\r\natid = (p - t->atid_tab) + t->atid_base;\r\nt->afree = p->next;\r\np->t3c_tid.ctx = ctx;\r\np->t3c_tid.client = client;\r\nt->atids_in_use++;\r\n}\r\nspin_unlock_bh(&t->atid_lock);\r\nreturn atid;\r\n}\r\nint cxgb3_alloc_stid(struct t3cdev *tdev, struct cxgb3_client *client,\r\nvoid *ctx)\r\n{\r\nint stid = -1;\r\nstruct tid_info *t = &(T3C_DATA(tdev))->tid_maps;\r\nspin_lock_bh(&t->stid_lock);\r\nif (t->sfree) {\r\nunion listen_entry *p = t->sfree;\r\nstid = (p - t->stid_tab) + t->stid_base;\r\nt->sfree = p->next;\r\np->t3c_tid.ctx = ctx;\r\np->t3c_tid.client = client;\r\nt->stids_in_use++;\r\n}\r\nspin_unlock_bh(&t->stid_lock);\r\nreturn stid;\r\n}\r\nstruct t3cdev *dev2t3cdev(struct net_device *dev)\r\n{\r\nconst struct port_info *pi = netdev_priv(dev);\r\nreturn (struct t3cdev *)pi->adapter;\r\n}\r\nstatic int do_smt_write_rpl(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nstruct cpl_smt_write_rpl *rpl = cplhdr(skb);\r\nif (rpl->status != CPL_ERR_NONE)\r\nprintk(KERN_ERR\r\n"Unexpected SMT_WRITE_RPL status %u for entry %u\n",\r\nrpl->status, GET_TID(rpl));\r\nreturn CPL_RET_BUF_DONE;\r\n}\r\nstatic int do_l2t_write_rpl(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nstruct cpl_l2t_write_rpl *rpl = cplhdr(skb);\r\nif (rpl->status != CPL_ERR_NONE)\r\nprintk(KERN_ERR\r\n"Unexpected L2T_WRITE_RPL status %u for entry %u\n",\r\nrpl->status, GET_TID(rpl));\r\nreturn CPL_RET_BUF_DONE;\r\n}\r\nstatic int do_rte_write_rpl(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nstruct cpl_rte_write_rpl *rpl = cplhdr(skb);\r\nif (rpl->status != CPL_ERR_NONE)\r\nprintk(KERN_ERR\r\n"Unexpected RTE_WRITE_RPL status %u for entry %u\n",\r\nrpl->status, GET_TID(rpl));\r\nreturn CPL_RET_BUF_DONE;\r\n}\r\nstatic int do_act_open_rpl(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nstruct cpl_act_open_rpl *rpl = cplhdr(skb);\r\nunsigned int atid = G_TID(ntohl(rpl->atid));\r\nstruct t3c_tid_entry *t3c_tid;\r\nt3c_tid = lookup_atid(&(T3C_DATA(dev))->tid_maps, atid);\r\nif (t3c_tid && t3c_tid->ctx && t3c_tid->client &&\r\nt3c_tid->client->handlers &&\r\nt3c_tid->client->handlers[CPL_ACT_OPEN_RPL]) {\r\nreturn t3c_tid->client->handlers[CPL_ACT_OPEN_RPL] (dev, skb,\r\nt3c_tid->\r\nctx);\r\n} else {\r\nprintk(KERN_ERR "%s: received clientless CPL command 0x%x\n",\r\ndev->name, CPL_ACT_OPEN_RPL);\r\nreturn CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;\r\n}\r\n}\r\nstatic int do_stid_rpl(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nunion opcode_tid *p = cplhdr(skb);\r\nunsigned int stid = G_TID(ntohl(p->opcode_tid));\r\nstruct t3c_tid_entry *t3c_tid;\r\nt3c_tid = lookup_stid(&(T3C_DATA(dev))->tid_maps, stid);\r\nif (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&\r\nt3c_tid->client->handlers[p->opcode]) {\r\nreturn t3c_tid->client->handlers[p->opcode] (dev, skb,\r\nt3c_tid->ctx);\r\n} else {\r\nprintk(KERN_ERR "%s: received clientless CPL command 0x%x\n",\r\ndev->name, p->opcode);\r\nreturn CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;\r\n}\r\n}\r\nstatic int do_hwtid_rpl(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nunion opcode_tid *p = cplhdr(skb);\r\nunsigned int hwtid = G_TID(ntohl(p->opcode_tid));\r\nstruct t3c_tid_entry *t3c_tid;\r\nt3c_tid = lookup_tid(&(T3C_DATA(dev))->tid_maps, hwtid);\r\nif (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&\r\nt3c_tid->client->handlers[p->opcode]) {\r\nreturn t3c_tid->client->handlers[p->opcode]\r\n(dev, skb, t3c_tid->ctx);\r\n} else {\r\nprintk(KERN_ERR "%s: received clientless CPL command 0x%x\n",\r\ndev->name, p->opcode);\r\nreturn CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;\r\n}\r\n}\r\nstatic int do_cr(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nstruct cpl_pass_accept_req *req = cplhdr(skb);\r\nunsigned int stid = G_PASS_OPEN_TID(ntohl(req->tos_tid));\r\nstruct tid_info *t = &(T3C_DATA(dev))->tid_maps;\r\nstruct t3c_tid_entry *t3c_tid;\r\nunsigned int tid = GET_TID(req);\r\nif (unlikely(tid >= t->ntids)) {\r\nprintk("%s: passive open TID %u too large\n",\r\ndev->name, tid);\r\nt3_fatal_err(tdev2adap(dev));\r\nreturn CPL_RET_BUF_DONE;\r\n}\r\nt3c_tid = lookup_stid(t, stid);\r\nif (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&\r\nt3c_tid->client->handlers[CPL_PASS_ACCEPT_REQ]) {\r\nreturn t3c_tid->client->handlers[CPL_PASS_ACCEPT_REQ]\r\n(dev, skb, t3c_tid->ctx);\r\n} else {\r\nprintk(KERN_ERR "%s: received clientless CPL command 0x%x\n",\r\ndev->name, CPL_PASS_ACCEPT_REQ);\r\nreturn CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;\r\n}\r\n}\r\nstatic struct sk_buff *cxgb3_get_cpl_reply_skb(struct sk_buff *skb, size_t len,\r\ngfp_t gfp)\r\n{\r\nif (likely(!skb_cloned(skb))) {\r\nBUG_ON(skb->len < len);\r\n__skb_trim(skb, len);\r\nskb_get(skb);\r\n} else {\r\nskb = alloc_skb(len, gfp);\r\nif (skb)\r\n__skb_put(skb, len);\r\n}\r\nreturn skb;\r\n}\r\nstatic int do_abort_req_rss(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nunion opcode_tid *p = cplhdr(skb);\r\nunsigned int hwtid = G_TID(ntohl(p->opcode_tid));\r\nstruct t3c_tid_entry *t3c_tid;\r\nt3c_tid = lookup_tid(&(T3C_DATA(dev))->tid_maps, hwtid);\r\nif (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&\r\nt3c_tid->client->handlers[p->opcode]) {\r\nreturn t3c_tid->client->handlers[p->opcode]\r\n(dev, skb, t3c_tid->ctx);\r\n} else {\r\nstruct cpl_abort_req_rss *req = cplhdr(skb);\r\nstruct cpl_abort_rpl *rpl;\r\nstruct sk_buff *reply_skb;\r\nunsigned int tid = GET_TID(req);\r\nu8 cmd = req->status;\r\nif (req->status == CPL_ERR_RTX_NEG_ADVICE ||\r\nreq->status == CPL_ERR_PERSIST_NEG_ADVICE)\r\ngoto out;\r\nreply_skb = cxgb3_get_cpl_reply_skb(skb,\r\nsizeof(struct\r\ncpl_abort_rpl),\r\nGFP_ATOMIC);\r\nif (!reply_skb) {\r\nprintk("do_abort_req_rss: couldn't get skb!\n");\r\ngoto out;\r\n}\r\nreply_skb->priority = CPL_PRIORITY_DATA;\r\n__skb_put(reply_skb, sizeof(struct cpl_abort_rpl));\r\nrpl = cplhdr(reply_skb);\r\nrpl->wr.wr_hi =\r\nhtonl(V_WR_OP(FW_WROPCODE_OFLD_HOST_ABORT_CON_RPL));\r\nrpl->wr.wr_lo = htonl(V_WR_TID(tid));\r\nOPCODE_TID(rpl) = htonl(MK_OPCODE_TID(CPL_ABORT_RPL, tid));\r\nrpl->cmd = cmd;\r\ncxgb3_ofld_send(dev, reply_skb);\r\nout:\r\nreturn CPL_RET_BUF_DONE;\r\n}\r\n}\r\nstatic int do_act_establish(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nstruct cpl_act_establish *req = cplhdr(skb);\r\nunsigned int atid = G_PASS_OPEN_TID(ntohl(req->tos_tid));\r\nstruct tid_info *t = &(T3C_DATA(dev))->tid_maps;\r\nstruct t3c_tid_entry *t3c_tid;\r\nunsigned int tid = GET_TID(req);\r\nif (unlikely(tid >= t->ntids)) {\r\nprintk("%s: active establish TID %u too large\n",\r\ndev->name, tid);\r\nt3_fatal_err(tdev2adap(dev));\r\nreturn CPL_RET_BUF_DONE;\r\n}\r\nt3c_tid = lookup_atid(t, atid);\r\nif (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&\r\nt3c_tid->client->handlers[CPL_ACT_ESTABLISH]) {\r\nreturn t3c_tid->client->handlers[CPL_ACT_ESTABLISH]\r\n(dev, skb, t3c_tid->ctx);\r\n} else {\r\nprintk(KERN_ERR "%s: received clientless CPL command 0x%x\n",\r\ndev->name, CPL_ACT_ESTABLISH);\r\nreturn CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;\r\n}\r\n}\r\nstatic int do_trace(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nstruct cpl_trace_pkt *p = cplhdr(skb);\r\nskb->protocol = htons(0xffff);\r\nskb->dev = dev->lldev;\r\nskb_pull(skb, sizeof(*p));\r\nskb_reset_mac_header(skb);\r\nnetif_receive_skb(skb);\r\nreturn 0;\r\n}\r\nstatic inline u32 get_hwtid(struct sk_buff *skb)\r\n{\r\nreturn ntohl((__force __be32)skb->priority) >> 8 & 0xfffff;\r\n}\r\nstatic inline u32 get_opcode(struct sk_buff *skb)\r\n{\r\nreturn G_OPCODE(ntohl((__force __be32)skb->csum));\r\n}\r\nstatic int do_term(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nunsigned int hwtid = get_hwtid(skb);\r\nunsigned int opcode = get_opcode(skb);\r\nstruct t3c_tid_entry *t3c_tid;\r\nt3c_tid = lookup_tid(&(T3C_DATA(dev))->tid_maps, hwtid);\r\nif (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&\r\nt3c_tid->client->handlers[opcode]) {\r\nreturn t3c_tid->client->handlers[opcode] (dev, skb,\r\nt3c_tid->ctx);\r\n} else {\r\nprintk(KERN_ERR "%s: received clientless CPL command 0x%x\n",\r\ndev->name, opcode);\r\nreturn CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;\r\n}\r\n}\r\nstatic int nb_callback(struct notifier_block *self, unsigned long event,\r\nvoid *ctx)\r\n{\r\nswitch (event) {\r\ncase (NETEVENT_NEIGH_UPDATE):{\r\ncxgb_neigh_update((struct neighbour *)ctx);\r\nbreak;\r\n}\r\ncase (NETEVENT_REDIRECT):{\r\nstruct netevent_redirect *nr = ctx;\r\ncxgb_redirect(nr->old, nr->new);\r\ncxgb_neigh_update(dst_get_neighbour_noref(nr->new));\r\nbreak;\r\n}\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int do_bad_cpl(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nprintk(KERN_ERR "%s: received bad CPL command 0x%x\n", dev->name,\r\n*skb->data);\r\nreturn CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;\r\n}\r\nvoid t3_register_cpl_handler(unsigned int opcode, cpl_handler_func h)\r\n{\r\nif (opcode < NUM_CPL_CMDS)\r\ncpl_handlers[opcode] = h ? h : do_bad_cpl;\r\nelse\r\nprintk(KERN_ERR "T3C: handler registration for "\r\n"opcode %x failed\n", opcode);\r\n}\r\nstatic int process_rx(struct t3cdev *dev, struct sk_buff **skbs, int n)\r\n{\r\nwhile (n--) {\r\nstruct sk_buff *skb = *skbs++;\r\nunsigned int opcode = get_opcode(skb);\r\nint ret = cpl_handlers[opcode] (dev, skb);\r\n#if VALIDATE_TID\r\nif (ret & CPL_RET_UNKNOWN_TID) {\r\nunion opcode_tid *p = cplhdr(skb);\r\nprintk(KERN_ERR "%s: CPL message (opcode %u) had "\r\n"unknown TID %u\n", dev->name, opcode,\r\nG_TID(ntohl(p->opcode_tid)));\r\n}\r\n#endif\r\nif (ret & CPL_RET_BUF_DONE)\r\nkfree_skb(skb);\r\n}\r\nreturn 0;\r\n}\r\nint cxgb3_ofld_send(struct t3cdev *dev, struct sk_buff *skb)\r\n{\r\nint r;\r\nlocal_bh_disable();\r\nr = dev->send(dev, skb);\r\nlocal_bh_enable();\r\nreturn r;\r\n}\r\nstatic int is_offloading(struct net_device *dev)\r\n{\r\nstruct adapter *adapter;\r\nint i;\r\nread_lock_bh(&adapter_list_lock);\r\nlist_for_each_entry(adapter, &adapter_list, adapter_list) {\r\nfor_each_port(adapter, i) {\r\nif (dev == adapter->port[i]) {\r\nread_unlock_bh(&adapter_list_lock);\r\nreturn 1;\r\n}\r\n}\r\n}\r\nread_unlock_bh(&adapter_list_lock);\r\nreturn 0;\r\n}\r\nstatic void cxgb_neigh_update(struct neighbour *neigh)\r\n{\r\nstruct net_device *dev;\r\nif (!neigh)\r\nreturn;\r\ndev = neigh->dev;\r\nif (dev && (is_offloading(dev))) {\r\nstruct t3cdev *tdev = dev2t3cdev(dev);\r\nBUG_ON(!tdev);\r\nt3_l2t_update(tdev, neigh);\r\n}\r\n}\r\nstatic void set_l2t_ix(struct t3cdev *tdev, u32 tid, struct l2t_entry *e)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cpl_set_tcb_field *req;\r\nskb = alloc_skb(sizeof(*req), GFP_ATOMIC);\r\nif (!skb) {\r\nprintk(KERN_ERR "%s: cannot allocate skb!\n", __func__);\r\nreturn;\r\n}\r\nskb->priority = CPL_PRIORITY_CONTROL;\r\nreq = (struct cpl_set_tcb_field *)skb_put(skb, sizeof(*req));\r\nreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));\r\nOPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, tid));\r\nreq->reply = 0;\r\nreq->cpu_idx = 0;\r\nreq->word = htons(W_TCB_L2T_IX);\r\nreq->mask = cpu_to_be64(V_TCB_L2T_IX(M_TCB_L2T_IX));\r\nreq->val = cpu_to_be64(V_TCB_L2T_IX(e->idx));\r\ntdev->send(tdev, skb);\r\n}\r\nstatic void cxgb_redirect(struct dst_entry *old, struct dst_entry *new)\r\n{\r\nstruct net_device *olddev, *newdev;\r\nstruct neighbour *n;\r\nstruct tid_info *ti;\r\nstruct t3cdev *tdev;\r\nu32 tid;\r\nint update_tcb;\r\nstruct l2t_entry *e;\r\nstruct t3c_tid_entry *te;\r\nn = dst_get_neighbour_noref(old);\r\nif (!n)\r\nreturn;\r\nolddev = n->dev;\r\nn = dst_get_neighbour_noref(new);\r\nif (!n)\r\nreturn;\r\nnewdev = n->dev;\r\nif (!is_offloading(olddev))\r\nreturn;\r\nif (!is_offloading(newdev)) {\r\nprintk(KERN_WARNING "%s: Redirect to non-offload "\r\n"device ignored.\n", __func__);\r\nreturn;\r\n}\r\ntdev = dev2t3cdev(olddev);\r\nBUG_ON(!tdev);\r\nif (tdev != dev2t3cdev(newdev)) {\r\nprintk(KERN_WARNING "%s: Redirect to different "\r\n"offload device ignored.\n", __func__);\r\nreturn;\r\n}\r\ne = t3_l2t_get(tdev, new, newdev);\r\nif (!e) {\r\nprintk(KERN_ERR "%s: couldn't allocate new l2t entry!\n",\r\n__func__);\r\nreturn;\r\n}\r\nti = &(T3C_DATA(tdev))->tid_maps;\r\nfor (tid = 0; tid < ti->ntids; tid++) {\r\nte = lookup_tid(ti, tid);\r\nBUG_ON(!te);\r\nif (te && te->ctx && te->client && te->client->redirect) {\r\nupdate_tcb = te->client->redirect(te->ctx, old, new, e);\r\nif (update_tcb) {\r\nrcu_read_lock();\r\nl2t_hold(L2DATA(tdev), e);\r\nrcu_read_unlock();\r\nset_l2t_ix(tdev, tid, e);\r\n}\r\n}\r\n}\r\nl2t_release(tdev, e);\r\n}\r\nvoid *cxgb_alloc_mem(unsigned long size)\r\n{\r\nvoid *p = kzalloc(size, GFP_KERNEL);\r\nif (!p)\r\np = vzalloc(size);\r\nreturn p;\r\n}\r\nvoid cxgb_free_mem(void *addr)\r\n{\r\nif (is_vmalloc_addr(addr))\r\nvfree(addr);\r\nelse\r\nkfree(addr);\r\n}\r\nstatic int init_tid_tabs(struct tid_info *t, unsigned int ntids,\r\nunsigned int natids, unsigned int nstids,\r\nunsigned int atid_base, unsigned int stid_base)\r\n{\r\nunsigned long size = ntids * sizeof(*t->tid_tab) +\r\nnatids * sizeof(*t->atid_tab) + nstids * sizeof(*t->stid_tab);\r\nt->tid_tab = cxgb_alloc_mem(size);\r\nif (!t->tid_tab)\r\nreturn -ENOMEM;\r\nt->stid_tab = (union listen_entry *)&t->tid_tab[ntids];\r\nt->atid_tab = (union active_open_entry *)&t->stid_tab[nstids];\r\nt->ntids = ntids;\r\nt->nstids = nstids;\r\nt->stid_base = stid_base;\r\nt->sfree = NULL;\r\nt->natids = natids;\r\nt->atid_base = atid_base;\r\nt->afree = NULL;\r\nt->stids_in_use = t->atids_in_use = 0;\r\natomic_set(&t->tids_in_use, 0);\r\nspin_lock_init(&t->stid_lock);\r\nspin_lock_init(&t->atid_lock);\r\nif (nstids) {\r\nwhile (--nstids)\r\nt->stid_tab[nstids - 1].next = &t->stid_tab[nstids];\r\nt->sfree = t->stid_tab;\r\n}\r\nif (natids) {\r\nwhile (--natids)\r\nt->atid_tab[natids - 1].next = &t->atid_tab[natids];\r\nt->afree = t->atid_tab;\r\n}\r\nreturn 0;\r\n}\r\nstatic void free_tid_maps(struct tid_info *t)\r\n{\r\ncxgb_free_mem(t->tid_tab);\r\n}\r\nstatic inline void add_adapter(struct adapter *adap)\r\n{\r\nwrite_lock_bh(&adapter_list_lock);\r\nlist_add_tail(&adap->adapter_list, &adapter_list);\r\nwrite_unlock_bh(&adapter_list_lock);\r\n}\r\nstatic inline void remove_adapter(struct adapter *adap)\r\n{\r\nwrite_lock_bh(&adapter_list_lock);\r\nlist_del(&adap->adapter_list);\r\nwrite_unlock_bh(&adapter_list_lock);\r\n}\r\nint cxgb3_offload_activate(struct adapter *adapter)\r\n{\r\nstruct t3cdev *dev = &adapter->tdev;\r\nint natids, err;\r\nstruct t3c_data *t;\r\nstruct tid_range stid_range, tid_range;\r\nstruct mtutab mtutab;\r\nunsigned int l2t_capacity;\r\nt = kzalloc(sizeof(*t), GFP_KERNEL);\r\nif (!t)\r\nreturn -ENOMEM;\r\nerr = -EOPNOTSUPP;\r\nif (dev->ctl(dev, GET_TX_MAX_CHUNK, &t->tx_max_chunk) < 0 ||\r\ndev->ctl(dev, GET_MAX_OUTSTANDING_WR, &t->max_wrs) < 0 ||\r\ndev->ctl(dev, GET_L2T_CAPACITY, &l2t_capacity) < 0 ||\r\ndev->ctl(dev, GET_MTUS, &mtutab) < 0 ||\r\ndev->ctl(dev, GET_TID_RANGE, &tid_range) < 0 ||\r\ndev->ctl(dev, GET_STID_RANGE, &stid_range) < 0)\r\ngoto out_free;\r\nerr = -ENOMEM;\r\nRCU_INIT_POINTER(dev->l2opt, t3_init_l2t(l2t_capacity));\r\nif (!L2DATA(dev))\r\ngoto out_free;\r\nnatids = min(tid_range.num / 2, MAX_ATIDS);\r\nerr = init_tid_tabs(&t->tid_maps, tid_range.num, natids,\r\nstid_range.num, ATID_BASE, stid_range.base);\r\nif (err)\r\ngoto out_free_l2t;\r\nt->mtus = mtutab.mtus;\r\nt->nmtus = mtutab.size;\r\nINIT_WORK(&t->tid_release_task, t3_process_tid_release_list);\r\nspin_lock_init(&t->tid_release_lock);\r\nINIT_LIST_HEAD(&t->list_node);\r\nt->dev = dev;\r\nT3C_DATA(dev) = t;\r\ndev->recv = process_rx;\r\ndev->neigh_update = t3_l2t_update;\r\nif (list_empty(&adapter_list))\r\nregister_netevent_notifier(&nb);\r\nt->nofail_skb = alloc_skb(sizeof(struct cpl_tid_release), GFP_KERNEL);\r\nt->release_list_incomplete = 0;\r\nadd_adapter(adapter);\r\nreturn 0;\r\nout_free_l2t:\r\nt3_free_l2t(L2DATA(dev));\r\nRCU_INIT_POINTER(dev->l2opt, NULL);\r\nout_free:\r\nkfree(t);\r\nreturn err;\r\n}\r\nstatic void clean_l2_data(struct rcu_head *head)\r\n{\r\nstruct l2t_data *d = container_of(head, struct l2t_data, rcu_head);\r\nt3_free_l2t(d);\r\n}\r\nvoid cxgb3_offload_deactivate(struct adapter *adapter)\r\n{\r\nstruct t3cdev *tdev = &adapter->tdev;\r\nstruct t3c_data *t = T3C_DATA(tdev);\r\nstruct l2t_data *d;\r\nremove_adapter(adapter);\r\nif (list_empty(&adapter_list))\r\nunregister_netevent_notifier(&nb);\r\nfree_tid_maps(&t->tid_maps);\r\nT3C_DATA(tdev) = NULL;\r\nrcu_read_lock();\r\nd = L2DATA(tdev);\r\nrcu_read_unlock();\r\nRCU_INIT_POINTER(tdev->l2opt, NULL);\r\ncall_rcu(&d->rcu_head, clean_l2_data);\r\nif (t->nofail_skb)\r\nkfree_skb(t->nofail_skb);\r\nkfree(t);\r\n}\r\nstatic inline void register_tdev(struct t3cdev *tdev)\r\n{\r\nstatic int unit;\r\nmutex_lock(&cxgb3_db_lock);\r\nsnprintf(tdev->name, sizeof(tdev->name), "ofld_dev%d", unit++);\r\nlist_add_tail(&tdev->ofld_dev_list, &ofld_dev_list);\r\nmutex_unlock(&cxgb3_db_lock);\r\n}\r\nstatic inline void unregister_tdev(struct t3cdev *tdev)\r\n{\r\nmutex_lock(&cxgb3_db_lock);\r\nlist_del(&tdev->ofld_dev_list);\r\nmutex_unlock(&cxgb3_db_lock);\r\n}\r\nstatic inline int adap2type(struct adapter *adapter)\r\n{\r\nint type = 0;\r\nswitch (adapter->params.rev) {\r\ncase T3_REV_A:\r\ntype = T3A;\r\nbreak;\r\ncase T3_REV_B:\r\ncase T3_REV_B2:\r\ntype = T3B;\r\nbreak;\r\ncase T3_REV_C:\r\ntype = T3C;\r\nbreak;\r\n}\r\nreturn type;\r\n}\r\nvoid __devinit cxgb3_adapter_ofld(struct adapter *adapter)\r\n{\r\nstruct t3cdev *tdev = &adapter->tdev;\r\nINIT_LIST_HEAD(&tdev->ofld_dev_list);\r\ncxgb3_set_dummy_ops(tdev);\r\ntdev->send = t3_offload_tx;\r\ntdev->ctl = cxgb_offload_ctl;\r\ntdev->type = adap2type(adapter);\r\nregister_tdev(tdev);\r\n}\r\nvoid __devexit cxgb3_adapter_unofld(struct adapter *adapter)\r\n{\r\nstruct t3cdev *tdev = &adapter->tdev;\r\ntdev->recv = NULL;\r\ntdev->neigh_update = NULL;\r\nunregister_tdev(tdev);\r\n}\r\nvoid __init cxgb3_offload_init(void)\r\n{\r\nint i;\r\nfor (i = 0; i < NUM_CPL_CMDS; ++i)\r\ncpl_handlers[i] = do_bad_cpl;\r\nt3_register_cpl_handler(CPL_SMT_WRITE_RPL, do_smt_write_rpl);\r\nt3_register_cpl_handler(CPL_L2T_WRITE_RPL, do_l2t_write_rpl);\r\nt3_register_cpl_handler(CPL_RTE_WRITE_RPL, do_rte_write_rpl);\r\nt3_register_cpl_handler(CPL_PASS_OPEN_RPL, do_stid_rpl);\r\nt3_register_cpl_handler(CPL_CLOSE_LISTSRV_RPL, do_stid_rpl);\r\nt3_register_cpl_handler(CPL_PASS_ACCEPT_REQ, do_cr);\r\nt3_register_cpl_handler(CPL_PASS_ESTABLISH, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_ABORT_RPL_RSS, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_ABORT_RPL, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_RX_URG_NOTIFY, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_RX_DATA, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_TX_DATA_ACK, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_TX_DMA_ACK, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_ACT_OPEN_RPL, do_act_open_rpl);\r\nt3_register_cpl_handler(CPL_PEER_CLOSE, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_CLOSE_CON_RPL, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_ABORT_REQ_RSS, do_abort_req_rss);\r\nt3_register_cpl_handler(CPL_ACT_ESTABLISH, do_act_establish);\r\nt3_register_cpl_handler(CPL_SET_TCB_RPL, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_GET_TCB_RPL, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_RDMA_TERMINATE, do_term);\r\nt3_register_cpl_handler(CPL_RDMA_EC_STATUS, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_TRACE_PKT, do_trace);\r\nt3_register_cpl_handler(CPL_RX_DATA_DDP, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_RX_DDP_COMPLETE, do_hwtid_rpl);\r\nt3_register_cpl_handler(CPL_ISCSI_HDR, do_hwtid_rpl);\r\n}
