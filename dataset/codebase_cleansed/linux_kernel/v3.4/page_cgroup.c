void __meminit pgdat_page_cgroup_init(struct pglist_data *pgdat)\r\n{\r\npgdat->node_page_cgroup = NULL;\r\n}\r\nstruct page_cgroup *lookup_page_cgroup(struct page *page)\r\n{\r\nunsigned long pfn = page_to_pfn(page);\r\nunsigned long offset;\r\nstruct page_cgroup *base;\r\nbase = NODE_DATA(page_to_nid(page))->node_page_cgroup;\r\n#ifdef CONFIG_DEBUG_VM\r\nif (unlikely(!base))\r\nreturn NULL;\r\n#endif\r\noffset = pfn - NODE_DATA(page_to_nid(page))->node_start_pfn;\r\nreturn base + offset;\r\n}\r\nstatic int __init alloc_node_page_cgroup(int nid)\r\n{\r\nstruct page_cgroup *base;\r\nunsigned long table_size;\r\nunsigned long nr_pages;\r\nnr_pages = NODE_DATA(nid)->node_spanned_pages;\r\nif (!nr_pages)\r\nreturn 0;\r\ntable_size = sizeof(struct page_cgroup) * nr_pages;\r\nbase = __alloc_bootmem_node_nopanic(NODE_DATA(nid),\r\ntable_size, PAGE_SIZE, __pa(MAX_DMA_ADDRESS));\r\nif (!base)\r\nreturn -ENOMEM;\r\nNODE_DATA(nid)->node_page_cgroup = base;\r\ntotal_usage += table_size;\r\nreturn 0;\r\n}\r\nvoid __init page_cgroup_init_flatmem(void)\r\n{\r\nint nid, fail;\r\nif (mem_cgroup_disabled())\r\nreturn;\r\nfor_each_online_node(nid) {\r\nfail = alloc_node_page_cgroup(nid);\r\nif (fail)\r\ngoto fail;\r\n}\r\nprintk(KERN_INFO "allocated %ld bytes of page_cgroup\n", total_usage);\r\nprintk(KERN_INFO "please try 'cgroup_disable=memory' option if you"\r\n" don't want memory cgroups\n");\r\nreturn;\r\nfail:\r\nprintk(KERN_CRIT "allocation of page_cgroup failed.\n");\r\nprintk(KERN_CRIT "please try 'cgroup_disable=memory' boot option\n");\r\npanic("Out of memory");\r\n}\r\nstruct page_cgroup *lookup_page_cgroup(struct page *page)\r\n{\r\nunsigned long pfn = page_to_pfn(page);\r\nstruct mem_section *section = __pfn_to_section(pfn);\r\n#ifdef CONFIG_DEBUG_VM\r\nif (!section->page_cgroup)\r\nreturn NULL;\r\n#endif\r\nreturn section->page_cgroup + pfn;\r\n}\r\nstatic void *__meminit alloc_page_cgroup(size_t size, int nid)\r\n{\r\ngfp_t flags = GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN;\r\nvoid *addr = NULL;\r\naddr = alloc_pages_exact_nid(nid, size, flags);\r\nif (addr) {\r\nkmemleak_alloc(addr, size, 1, flags);\r\nreturn addr;\r\n}\r\nif (node_state(nid, N_HIGH_MEMORY))\r\naddr = vzalloc_node(size, nid);\r\nelse\r\naddr = vzalloc(size);\r\nreturn addr;\r\n}\r\nstatic int __meminit init_section_page_cgroup(unsigned long pfn, int nid)\r\n{\r\nstruct mem_section *section;\r\nstruct page_cgroup *base;\r\nunsigned long table_size;\r\nsection = __pfn_to_section(pfn);\r\nif (section->page_cgroup)\r\nreturn 0;\r\ntable_size = sizeof(struct page_cgroup) * PAGES_PER_SECTION;\r\nbase = alloc_page_cgroup(table_size, nid);\r\nkmemleak_not_leak(base);\r\nif (!base) {\r\nprintk(KERN_ERR "page cgroup allocation failure\n");\r\nreturn -ENOMEM;\r\n}\r\npfn &= PAGE_SECTION_MASK;\r\nsection->page_cgroup = base - pfn;\r\ntotal_usage += table_size;\r\nreturn 0;\r\n}\r\nstatic void free_page_cgroup(void *addr)\r\n{\r\nif (is_vmalloc_addr(addr)) {\r\nvfree(addr);\r\n} else {\r\nstruct page *page = virt_to_page(addr);\r\nsize_t table_size =\r\nsizeof(struct page_cgroup) * PAGES_PER_SECTION;\r\nBUG_ON(PageReserved(page));\r\nfree_pages_exact(addr, table_size);\r\n}\r\n}\r\nvoid __free_page_cgroup(unsigned long pfn)\r\n{\r\nstruct mem_section *ms;\r\nstruct page_cgroup *base;\r\nms = __pfn_to_section(pfn);\r\nif (!ms || !ms->page_cgroup)\r\nreturn;\r\nbase = ms->page_cgroup + pfn;\r\nfree_page_cgroup(base);\r\nms->page_cgroup = NULL;\r\n}\r\nint __meminit online_page_cgroup(unsigned long start_pfn,\r\nunsigned long nr_pages,\r\nint nid)\r\n{\r\nunsigned long start, end, pfn;\r\nint fail = 0;\r\nstart = SECTION_ALIGN_DOWN(start_pfn);\r\nend = SECTION_ALIGN_UP(start_pfn + nr_pages);\r\nif (nid == -1) {\r\nnid = pfn_to_nid(start_pfn);\r\nVM_BUG_ON(!node_state(nid, N_ONLINE));\r\n}\r\nfor (pfn = start; !fail && pfn < end; pfn += PAGES_PER_SECTION) {\r\nif (!pfn_present(pfn))\r\ncontinue;\r\nfail = init_section_page_cgroup(pfn, nid);\r\n}\r\nif (!fail)\r\nreturn 0;\r\nfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION)\r\n__free_page_cgroup(pfn);\r\nreturn -ENOMEM;\r\n}\r\nint __meminit offline_page_cgroup(unsigned long start_pfn,\r\nunsigned long nr_pages, int nid)\r\n{\r\nunsigned long start, end, pfn;\r\nstart = SECTION_ALIGN_DOWN(start_pfn);\r\nend = SECTION_ALIGN_UP(start_pfn + nr_pages);\r\nfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION)\r\n__free_page_cgroup(pfn);\r\nreturn 0;\r\n}\r\nstatic int __meminit page_cgroup_callback(struct notifier_block *self,\r\nunsigned long action, void *arg)\r\n{\r\nstruct memory_notify *mn = arg;\r\nint ret = 0;\r\nswitch (action) {\r\ncase MEM_GOING_ONLINE:\r\nret = online_page_cgroup(mn->start_pfn,\r\nmn->nr_pages, mn->status_change_nid);\r\nbreak;\r\ncase MEM_OFFLINE:\r\noffline_page_cgroup(mn->start_pfn,\r\nmn->nr_pages, mn->status_change_nid);\r\nbreak;\r\ncase MEM_CANCEL_ONLINE:\r\ncase MEM_GOING_OFFLINE:\r\nbreak;\r\ncase MEM_ONLINE:\r\ncase MEM_CANCEL_OFFLINE:\r\nbreak;\r\n}\r\nreturn notifier_from_errno(ret);\r\n}\r\nvoid __init page_cgroup_init(void)\r\n{\r\nunsigned long pfn;\r\nint nid;\r\nif (mem_cgroup_disabled())\r\nreturn;\r\nfor_each_node_state(nid, N_HIGH_MEMORY) {\r\nunsigned long start_pfn, end_pfn;\r\nstart_pfn = node_start_pfn(nid);\r\nend_pfn = node_end_pfn(nid);\r\nfor (pfn = start_pfn;\r\npfn < end_pfn;\r\npfn = ALIGN(pfn + 1, PAGES_PER_SECTION)) {\r\nif (!pfn_valid(pfn))\r\ncontinue;\r\nif (pfn_to_nid(pfn) != nid)\r\ncontinue;\r\nif (init_section_page_cgroup(pfn, nid))\r\ngoto oom;\r\n}\r\n}\r\nhotplug_memory_notifier(page_cgroup_callback, 0);\r\nprintk(KERN_INFO "allocated %ld bytes of page_cgroup\n", total_usage);\r\nprintk(KERN_INFO "please try 'cgroup_disable=memory' option if you "\r\n"don't want memory cgroups\n");\r\nreturn;\r\noom:\r\nprintk(KERN_CRIT "try 'cgroup_disable=memory' boot option\n");\r\npanic("Out of memory");\r\n}\r\nvoid __meminit pgdat_page_cgroup_init(struct pglist_data *pgdat)\r\n{\r\nreturn;\r\n}\r\nstatic int swap_cgroup_prepare(int type)\r\n{\r\nstruct page *page;\r\nstruct swap_cgroup_ctrl *ctrl;\r\nunsigned long idx, max;\r\nctrl = &swap_cgroup_ctrl[type];\r\nfor (idx = 0; idx < ctrl->length; idx++) {\r\npage = alloc_page(GFP_KERNEL | __GFP_ZERO);\r\nif (!page)\r\ngoto not_enough_page;\r\nctrl->map[idx] = page;\r\n}\r\nreturn 0;\r\nnot_enough_page:\r\nmax = idx;\r\nfor (idx = 0; idx < max; idx++)\r\n__free_page(ctrl->map[idx]);\r\nreturn -ENOMEM;\r\n}\r\nstatic struct swap_cgroup *lookup_swap_cgroup(swp_entry_t ent,\r\nstruct swap_cgroup_ctrl **ctrlp)\r\n{\r\npgoff_t offset = swp_offset(ent);\r\nstruct swap_cgroup_ctrl *ctrl;\r\nstruct page *mappage;\r\nstruct swap_cgroup *sc;\r\nctrl = &swap_cgroup_ctrl[swp_type(ent)];\r\nif (ctrlp)\r\n*ctrlp = ctrl;\r\nmappage = ctrl->map[offset / SC_PER_PAGE];\r\nsc = page_address(mappage);\r\nreturn sc + offset % SC_PER_PAGE;\r\n}\r\nunsigned short swap_cgroup_cmpxchg(swp_entry_t ent,\r\nunsigned short old, unsigned short new)\r\n{\r\nstruct swap_cgroup_ctrl *ctrl;\r\nstruct swap_cgroup *sc;\r\nunsigned long flags;\r\nunsigned short retval;\r\nsc = lookup_swap_cgroup(ent, &ctrl);\r\nspin_lock_irqsave(&ctrl->lock, flags);\r\nretval = sc->id;\r\nif (retval == old)\r\nsc->id = new;\r\nelse\r\nretval = 0;\r\nspin_unlock_irqrestore(&ctrl->lock, flags);\r\nreturn retval;\r\n}\r\nunsigned short swap_cgroup_record(swp_entry_t ent, unsigned short id)\r\n{\r\nstruct swap_cgroup_ctrl *ctrl;\r\nstruct swap_cgroup *sc;\r\nunsigned short old;\r\nunsigned long flags;\r\nsc = lookup_swap_cgroup(ent, &ctrl);\r\nspin_lock_irqsave(&ctrl->lock, flags);\r\nold = sc->id;\r\nsc->id = id;\r\nspin_unlock_irqrestore(&ctrl->lock, flags);\r\nreturn old;\r\n}\r\nunsigned short lookup_swap_cgroup_id(swp_entry_t ent)\r\n{\r\nreturn lookup_swap_cgroup(ent, NULL)->id;\r\n}\r\nint swap_cgroup_swapon(int type, unsigned long max_pages)\r\n{\r\nvoid *array;\r\nunsigned long array_size;\r\nunsigned long length;\r\nstruct swap_cgroup_ctrl *ctrl;\r\nif (!do_swap_account)\r\nreturn 0;\r\nlength = DIV_ROUND_UP(max_pages, SC_PER_PAGE);\r\narray_size = length * sizeof(void *);\r\narray = vzalloc(array_size);\r\nif (!array)\r\ngoto nomem;\r\nctrl = &swap_cgroup_ctrl[type];\r\nmutex_lock(&swap_cgroup_mutex);\r\nctrl->length = length;\r\nctrl->map = array;\r\nspin_lock_init(&ctrl->lock);\r\nif (swap_cgroup_prepare(type)) {\r\nctrl->map = NULL;\r\nctrl->length = 0;\r\nmutex_unlock(&swap_cgroup_mutex);\r\nvfree(array);\r\ngoto nomem;\r\n}\r\nmutex_unlock(&swap_cgroup_mutex);\r\nreturn 0;\r\nnomem:\r\nprintk(KERN_INFO "couldn't allocate enough memory for swap_cgroup.\n");\r\nprintk(KERN_INFO\r\n"swap_cgroup can be disabled by swapaccount=0 boot option\n");\r\nreturn -ENOMEM;\r\n}\r\nvoid swap_cgroup_swapoff(int type)\r\n{\r\nstruct page **map;\r\nunsigned long i, length;\r\nstruct swap_cgroup_ctrl *ctrl;\r\nif (!do_swap_account)\r\nreturn;\r\nmutex_lock(&swap_cgroup_mutex);\r\nctrl = &swap_cgroup_ctrl[type];\r\nmap = ctrl->map;\r\nlength = ctrl->length;\r\nctrl->map = NULL;\r\nctrl->length = 0;\r\nmutex_unlock(&swap_cgroup_mutex);\r\nif (map) {\r\nfor (i = 0; i < length; i++) {\r\nstruct page *page = map[i];\r\nif (page)\r\n__free_page(page);\r\n}\r\nvfree(map);\r\n}\r\n}
