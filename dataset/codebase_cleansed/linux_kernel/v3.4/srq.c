static void *get_wqe(struct mlx4_ib_srq *srq, int n)\r\n{\r\nreturn mlx4_buf_offset(&srq->buf, n << srq->msrq.wqe_shift);\r\n}\r\nstatic void mlx4_ib_srq_event(struct mlx4_srq *srq, enum mlx4_event type)\r\n{\r\nstruct ib_event event;\r\nstruct ib_srq *ibsrq = &to_mibsrq(srq)->ibsrq;\r\nif (ibsrq->event_handler) {\r\nevent.device = ibsrq->device;\r\nevent.element.srq = ibsrq;\r\nswitch (type) {\r\ncase MLX4_EVENT_TYPE_SRQ_LIMIT:\r\nevent.event = IB_EVENT_SRQ_LIMIT_REACHED;\r\nbreak;\r\ncase MLX4_EVENT_TYPE_SRQ_CATAS_ERROR:\r\nevent.event = IB_EVENT_SRQ_ERR;\r\nbreak;\r\ndefault:\r\nprintk(KERN_WARNING "mlx4_ib: Unexpected event type %d "\r\n"on SRQ %06x\n", type, srq->srqn);\r\nreturn;\r\n}\r\nibsrq->event_handler(&event, ibsrq->srq_context);\r\n}\r\n}\r\nstruct ib_srq *mlx4_ib_create_srq(struct ib_pd *pd,\r\nstruct ib_srq_init_attr *init_attr,\r\nstruct ib_udata *udata)\r\n{\r\nstruct mlx4_ib_dev *dev = to_mdev(pd->device);\r\nstruct mlx4_ib_srq *srq;\r\nstruct mlx4_wqe_srq_next_seg *next;\r\nstruct mlx4_wqe_data_seg *scatter;\r\nu32 cqn;\r\nu16 xrcdn;\r\nint desc_size;\r\nint buf_size;\r\nint err;\r\nint i;\r\nif (init_attr->attr.max_wr >= dev->dev->caps.max_srq_wqes ||\r\ninit_attr->attr.max_sge > dev->dev->caps.max_srq_sge)\r\nreturn ERR_PTR(-EINVAL);\r\nsrq = kmalloc(sizeof *srq, GFP_KERNEL);\r\nif (!srq)\r\nreturn ERR_PTR(-ENOMEM);\r\nmutex_init(&srq->mutex);\r\nspin_lock_init(&srq->lock);\r\nsrq->msrq.max = roundup_pow_of_two(init_attr->attr.max_wr + 1);\r\nsrq->msrq.max_gs = init_attr->attr.max_sge;\r\ndesc_size = max(32UL,\r\nroundup_pow_of_two(sizeof (struct mlx4_wqe_srq_next_seg) +\r\nsrq->msrq.max_gs *\r\nsizeof (struct mlx4_wqe_data_seg)));\r\nsrq->msrq.wqe_shift = ilog2(desc_size);\r\nbuf_size = srq->msrq.max * desc_size;\r\nif (pd->uobject) {\r\nstruct mlx4_ib_create_srq ucmd;\r\nif (ib_copy_from_udata(&ucmd, udata, sizeof ucmd)) {\r\nerr = -EFAULT;\r\ngoto err_srq;\r\n}\r\nsrq->umem = ib_umem_get(pd->uobject->context, ucmd.buf_addr,\r\nbuf_size, 0, 0);\r\nif (IS_ERR(srq->umem)) {\r\nerr = PTR_ERR(srq->umem);\r\ngoto err_srq;\r\n}\r\nerr = mlx4_mtt_init(dev->dev, ib_umem_page_count(srq->umem),\r\nilog2(srq->umem->page_size), &srq->mtt);\r\nif (err)\r\ngoto err_buf;\r\nerr = mlx4_ib_umem_write_mtt(dev, &srq->mtt, srq->umem);\r\nif (err)\r\ngoto err_mtt;\r\nerr = mlx4_ib_db_map_user(to_mucontext(pd->uobject->context),\r\nucmd.db_addr, &srq->db);\r\nif (err)\r\ngoto err_mtt;\r\n} else {\r\nerr = mlx4_db_alloc(dev->dev, &srq->db, 0);\r\nif (err)\r\ngoto err_srq;\r\n*srq->db.db = 0;\r\nif (mlx4_buf_alloc(dev->dev, buf_size, PAGE_SIZE * 2, &srq->buf)) {\r\nerr = -ENOMEM;\r\ngoto err_db;\r\n}\r\nsrq->head = 0;\r\nsrq->tail = srq->msrq.max - 1;\r\nsrq->wqe_ctr = 0;\r\nfor (i = 0; i < srq->msrq.max; ++i) {\r\nnext = get_wqe(srq, i);\r\nnext->next_wqe_index =\r\ncpu_to_be16((i + 1) & (srq->msrq.max - 1));\r\nfor (scatter = (void *) (next + 1);\r\n(void *) scatter < (void *) next + desc_size;\r\n++scatter)\r\nscatter->lkey = cpu_to_be32(MLX4_INVALID_LKEY);\r\n}\r\nerr = mlx4_mtt_init(dev->dev, srq->buf.npages, srq->buf.page_shift,\r\n&srq->mtt);\r\nif (err)\r\ngoto err_buf;\r\nerr = mlx4_buf_write_mtt(dev->dev, &srq->mtt, &srq->buf);\r\nif (err)\r\ngoto err_mtt;\r\nsrq->wrid = kmalloc(srq->msrq.max * sizeof (u64), GFP_KERNEL);\r\nif (!srq->wrid) {\r\nerr = -ENOMEM;\r\ngoto err_mtt;\r\n}\r\n}\r\ncqn = (init_attr->srq_type == IB_SRQT_XRC) ?\r\nto_mcq(init_attr->ext.xrc.cq)->mcq.cqn : 0;\r\nxrcdn = (init_attr->srq_type == IB_SRQT_XRC) ?\r\nto_mxrcd(init_attr->ext.xrc.xrcd)->xrcdn :\r\n(u16) dev->dev->caps.reserved_xrcds;\r\nerr = mlx4_srq_alloc(dev->dev, to_mpd(pd)->pdn, cqn, xrcdn, &srq->mtt,\r\nsrq->db.dma, &srq->msrq);\r\nif (err)\r\ngoto err_wrid;\r\nsrq->msrq.event = mlx4_ib_srq_event;\r\nsrq->ibsrq.ext.xrc.srq_num = srq->msrq.srqn;\r\nif (pd->uobject)\r\nif (ib_copy_to_udata(udata, &srq->msrq.srqn, sizeof (__u32))) {\r\nerr = -EFAULT;\r\ngoto err_wrid;\r\n}\r\ninit_attr->attr.max_wr = srq->msrq.max - 1;\r\nreturn &srq->ibsrq;\r\nerr_wrid:\r\nif (pd->uobject)\r\nmlx4_ib_db_unmap_user(to_mucontext(pd->uobject->context), &srq->db);\r\nelse\r\nkfree(srq->wrid);\r\nerr_mtt:\r\nmlx4_mtt_cleanup(dev->dev, &srq->mtt);\r\nerr_buf:\r\nif (pd->uobject)\r\nib_umem_release(srq->umem);\r\nelse\r\nmlx4_buf_free(dev->dev, buf_size, &srq->buf);\r\nerr_db:\r\nif (!pd->uobject)\r\nmlx4_db_free(dev->dev, &srq->db);\r\nerr_srq:\r\nkfree(srq);\r\nreturn ERR_PTR(err);\r\n}\r\nint mlx4_ib_modify_srq(struct ib_srq *ibsrq, struct ib_srq_attr *attr,\r\nenum ib_srq_attr_mask attr_mask, struct ib_udata *udata)\r\n{\r\nstruct mlx4_ib_dev *dev = to_mdev(ibsrq->device);\r\nstruct mlx4_ib_srq *srq = to_msrq(ibsrq);\r\nint ret;\r\nif (attr_mask & IB_SRQ_MAX_WR)\r\nreturn -EINVAL;\r\nif (attr_mask & IB_SRQ_LIMIT) {\r\nif (attr->srq_limit >= srq->msrq.max)\r\nreturn -EINVAL;\r\nmutex_lock(&srq->mutex);\r\nret = mlx4_srq_arm(dev->dev, &srq->msrq, attr->srq_limit);\r\nmutex_unlock(&srq->mutex);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint mlx4_ib_query_srq(struct ib_srq *ibsrq, struct ib_srq_attr *srq_attr)\r\n{\r\nstruct mlx4_ib_dev *dev = to_mdev(ibsrq->device);\r\nstruct mlx4_ib_srq *srq = to_msrq(ibsrq);\r\nint ret;\r\nint limit_watermark;\r\nret = mlx4_srq_query(dev->dev, &srq->msrq, &limit_watermark);\r\nif (ret)\r\nreturn ret;\r\nsrq_attr->srq_limit = limit_watermark;\r\nsrq_attr->max_wr = srq->msrq.max - 1;\r\nsrq_attr->max_sge = srq->msrq.max_gs;\r\nreturn 0;\r\n}\r\nint mlx4_ib_destroy_srq(struct ib_srq *srq)\r\n{\r\nstruct mlx4_ib_dev *dev = to_mdev(srq->device);\r\nstruct mlx4_ib_srq *msrq = to_msrq(srq);\r\nmlx4_srq_free(dev->dev, &msrq->msrq);\r\nmlx4_mtt_cleanup(dev->dev, &msrq->mtt);\r\nif (srq->uobject) {\r\nmlx4_ib_db_unmap_user(to_mucontext(srq->uobject->context), &msrq->db);\r\nib_umem_release(msrq->umem);\r\n} else {\r\nkfree(msrq->wrid);\r\nmlx4_buf_free(dev->dev, msrq->msrq.max << msrq->msrq.wqe_shift,\r\n&msrq->buf);\r\nmlx4_db_free(dev->dev, &msrq->db);\r\n}\r\nkfree(msrq);\r\nreturn 0;\r\n}\r\nvoid mlx4_ib_free_srq_wqe(struct mlx4_ib_srq *srq, int wqe_index)\r\n{\r\nstruct mlx4_wqe_srq_next_seg *next;\r\nspin_lock(&srq->lock);\r\nnext = get_wqe(srq, srq->tail);\r\nnext->next_wqe_index = cpu_to_be16(wqe_index);\r\nsrq->tail = wqe_index;\r\nspin_unlock(&srq->lock);\r\n}\r\nint mlx4_ib_post_srq_recv(struct ib_srq *ibsrq, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nstruct mlx4_ib_srq *srq = to_msrq(ibsrq);\r\nstruct mlx4_wqe_srq_next_seg *next;\r\nstruct mlx4_wqe_data_seg *scat;\r\nunsigned long flags;\r\nint err = 0;\r\nint nreq;\r\nint i;\r\nspin_lock_irqsave(&srq->lock, flags);\r\nfor (nreq = 0; wr; ++nreq, wr = wr->next) {\r\nif (unlikely(wr->num_sge > srq->msrq.max_gs)) {\r\nerr = -EINVAL;\r\n*bad_wr = wr;\r\nbreak;\r\n}\r\nif (unlikely(srq->head == srq->tail)) {\r\nerr = -ENOMEM;\r\n*bad_wr = wr;\r\nbreak;\r\n}\r\nsrq->wrid[srq->head] = wr->wr_id;\r\nnext = get_wqe(srq, srq->head);\r\nsrq->head = be16_to_cpu(next->next_wqe_index);\r\nscat = (struct mlx4_wqe_data_seg *) (next + 1);\r\nfor (i = 0; i < wr->num_sge; ++i) {\r\nscat[i].byte_count = cpu_to_be32(wr->sg_list[i].length);\r\nscat[i].lkey = cpu_to_be32(wr->sg_list[i].lkey);\r\nscat[i].addr = cpu_to_be64(wr->sg_list[i].addr);\r\n}\r\nif (i < srq->msrq.max_gs) {\r\nscat[i].byte_count = 0;\r\nscat[i].lkey = cpu_to_be32(MLX4_INVALID_LKEY);\r\nscat[i].addr = 0;\r\n}\r\n}\r\nif (likely(nreq)) {\r\nsrq->wqe_ctr += nreq;\r\nwmb();\r\n*srq->db.db = cpu_to_be32(srq->wqe_ctr);\r\n}\r\nspin_unlock_irqrestore(&srq->lock, flags);\r\nreturn err;\r\n}
