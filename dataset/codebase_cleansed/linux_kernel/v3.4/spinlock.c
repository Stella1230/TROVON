static inline void check_zero(void)\r\n{\r\nif (unlikely(zero_stats)) {\r\nmemset(&spinlock_stats, 0, sizeof(spinlock_stats));\r\nzero_stats = 0;\r\n}\r\n}\r\nstatic inline u64 spin_time_start(void)\r\n{\r\nreturn xen_clocksource_read();\r\n}\r\nstatic void __spin_time_accum(u64 delta, u32 *array)\r\n{\r\nunsigned index = ilog2(delta);\r\ncheck_zero();\r\nif (index < HISTO_BUCKETS)\r\narray[index]++;\r\nelse\r\narray[HISTO_BUCKETS]++;\r\n}\r\nstatic inline void spin_time_accum_spinning(u64 start)\r\n{\r\nu32 delta = xen_clocksource_read() - start;\r\n__spin_time_accum(delta, spinlock_stats.histo_spin_spinning);\r\nspinlock_stats.time_spinning += delta;\r\n}\r\nstatic inline void spin_time_accum_total(u64 start)\r\n{\r\nu32 delta = xen_clocksource_read() - start;\r\n__spin_time_accum(delta, spinlock_stats.histo_spin_total);\r\nspinlock_stats.time_total += delta;\r\n}\r\nstatic inline void spin_time_accum_blocked(u64 start)\r\n{\r\nu32 delta = xen_clocksource_read() - start;\r\n__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);\r\nspinlock_stats.time_blocked += delta;\r\n}\r\nstatic inline u64 spin_time_start(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void spin_time_accum_total(u64 start)\r\n{\r\n}\r\nstatic inline void spin_time_accum_spinning(u64 start)\r\n{\r\n}\r\nstatic inline void spin_time_accum_blocked(u64 start)\r\n{\r\n}\r\nstatic int xen_spin_is_locked(struct arch_spinlock *lock)\r\n{\r\nstruct xen_spinlock *xl = (struct xen_spinlock *)lock;\r\nreturn xl->lock != 0;\r\n}\r\nstatic int xen_spin_is_contended(struct arch_spinlock *lock)\r\n{\r\nstruct xen_spinlock *xl = (struct xen_spinlock *)lock;\r\nreturn xl->spinners != 0;\r\n}\r\nstatic int xen_spin_trylock(struct arch_spinlock *lock)\r\n{\r\nstruct xen_spinlock *xl = (struct xen_spinlock *)lock;\r\nu8 old = 1;\r\nasm("xchgb %b0,%1"\r\n: "+q" (old), "+m" (xl->lock) : : "memory");\r\nreturn old == 0;\r\n}\r\nstatic inline struct xen_spinlock *spinning_lock(struct xen_spinlock *xl)\r\n{\r\nstruct xen_spinlock *prev;\r\nprev = __this_cpu_read(lock_spinners);\r\n__this_cpu_write(lock_spinners, xl);\r\nwmb();\r\ninc_spinners(xl);\r\nreturn prev;\r\n}\r\nstatic inline void unspinning_lock(struct xen_spinlock *xl, struct xen_spinlock *prev)\r\n{\r\ndec_spinners(xl);\r\nwmb();\r\n__this_cpu_write(lock_spinners, prev);\r\n}\r\nstatic noinline int xen_spin_lock_slow(struct arch_spinlock *lock, bool irq_enable)\r\n{\r\nstruct xen_spinlock *xl = (struct xen_spinlock *)lock;\r\nstruct xen_spinlock *prev;\r\nint irq = __this_cpu_read(lock_kicker_irq);\r\nint ret;\r\nu64 start;\r\nif (irq == -1)\r\nreturn 0;\r\nstart = spin_time_start();\r\nprev = spinning_lock(xl);\r\nADD_STATS(taken_slow, 1);\r\nADD_STATS(taken_slow_nested, prev != NULL);\r\ndo {\r\nunsigned long flags;\r\nxen_clear_irq_pending(irq);\r\nret = xen_spin_trylock(lock);\r\nif (ret) {\r\nADD_STATS(taken_slow_pickup, 1);\r\nif (prev != NULL)\r\nxen_set_irq_pending(irq);\r\ngoto out;\r\n}\r\nflags = arch_local_save_flags();\r\nif (irq_enable) {\r\nADD_STATS(taken_slow_irqenable, 1);\r\nraw_local_irq_enable();\r\n}\r\nxen_poll_irq(irq);\r\nraw_local_irq_restore(flags);\r\nADD_STATS(taken_slow_spurious, !xen_test_irq_pending(irq));\r\n} while (!xen_test_irq_pending(irq));\r\nkstat_incr_irqs_this_cpu(irq, irq_to_desc(irq));\r\nout:\r\nunspinning_lock(xl, prev);\r\nspin_time_accum_blocked(start);\r\nreturn ret;\r\n}\r\nstatic inline void __xen_spin_lock(struct arch_spinlock *lock, bool irq_enable)\r\n{\r\nstruct xen_spinlock *xl = (struct xen_spinlock *)lock;\r\nunsigned timeout;\r\nu8 oldval;\r\nu64 start_spin;\r\nADD_STATS(taken, 1);\r\nstart_spin = spin_time_start();\r\ndo {\r\nu64 start_spin_fast = spin_time_start();\r\ntimeout = TIMEOUT;\r\nasm("1: xchgb %1,%0\n"\r\n" testb %1,%1\n"\r\n" jz 3f\n"\r\n"2: rep;nop\n"\r\n" cmpb $0,%0\n"\r\n" je 1b\n"\r\n" dec %2\n"\r\n" jnz 2b\n"\r\n"3:\n"\r\n: "+m" (xl->lock), "=q" (oldval), "+r" (timeout)\r\n: "1" (1)\r\n: "memory");\r\nspin_time_accum_spinning(start_spin_fast);\r\n} while (unlikely(oldval != 0 &&\r\n(TIMEOUT == ~0 || !xen_spin_lock_slow(lock, irq_enable))));\r\nspin_time_accum_total(start_spin);\r\n}\r\nstatic void xen_spin_lock(struct arch_spinlock *lock)\r\n{\r\n__xen_spin_lock(lock, false);\r\n}\r\nstatic void xen_spin_lock_flags(struct arch_spinlock *lock, unsigned long flags)\r\n{\r\n__xen_spin_lock(lock, !raw_irqs_disabled_flags(flags));\r\n}\r\nstatic noinline void xen_spin_unlock_slow(struct xen_spinlock *xl)\r\n{\r\nint cpu;\r\nADD_STATS(released_slow, 1);\r\nfor_each_online_cpu(cpu) {\r\nif (per_cpu(lock_spinners, cpu) == xl) {\r\nADD_STATS(released_slow_kicked, 1);\r\nxen_send_IPI_one(cpu, XEN_SPIN_UNLOCK_VECTOR);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void xen_spin_unlock(struct arch_spinlock *lock)\r\n{\r\nstruct xen_spinlock *xl = (struct xen_spinlock *)lock;\r\nADD_STATS(released, 1);\r\nsmp_wmb();\r\nxl->lock = 0;\r\nmb();\r\nif (unlikely(xl->spinners))\r\nxen_spin_unlock_slow(xl);\r\n}\r\nstatic irqreturn_t dummy_handler(int irq, void *dev_id)\r\n{\r\nBUG();\r\nreturn IRQ_HANDLED;\r\n}\r\nvoid __cpuinit xen_init_lock_cpu(int cpu)\r\n{\r\nint irq;\r\nconst char *name;\r\nname = kasprintf(GFP_KERNEL, "spinlock%d", cpu);\r\nirq = bind_ipi_to_irqhandler(XEN_SPIN_UNLOCK_VECTOR,\r\ncpu,\r\ndummy_handler,\r\nIRQF_DISABLED|IRQF_PERCPU|IRQF_NOBALANCING,\r\nname,\r\nNULL);\r\nif (irq >= 0) {\r\ndisable_irq(irq);\r\nper_cpu(lock_kicker_irq, cpu) = irq;\r\n}\r\nprintk("cpu %d spinlock event irq %d\n", cpu, irq);\r\n}\r\nvoid xen_uninit_lock_cpu(int cpu)\r\n{\r\nunbind_from_irqhandler(per_cpu(lock_kicker_irq, cpu), NULL);\r\n}\r\nvoid __init xen_init_spinlocks(void)\r\n{\r\nBUILD_BUG_ON(sizeof(struct xen_spinlock) > sizeof(arch_spinlock_t));\r\npv_lock_ops.spin_is_locked = xen_spin_is_locked;\r\npv_lock_ops.spin_is_contended = xen_spin_is_contended;\r\npv_lock_ops.spin_lock = xen_spin_lock;\r\npv_lock_ops.spin_lock_flags = xen_spin_lock_flags;\r\npv_lock_ops.spin_trylock = xen_spin_trylock;\r\npv_lock_ops.spin_unlock = xen_spin_unlock;\r\n}\r\nstatic int __init xen_spinlock_debugfs(void)\r\n{\r\nstruct dentry *d_xen = xen_init_debugfs();\r\nif (d_xen == NULL)\r\nreturn -ENOMEM;\r\nd_spin_debug = debugfs_create_dir("spinlocks", d_xen);\r\ndebugfs_create_u8("zero_stats", 0644, d_spin_debug, &zero_stats);\r\ndebugfs_create_u32("timeout", 0644, d_spin_debug, &lock_timeout);\r\ndebugfs_create_u64("taken", 0444, d_spin_debug, &spinlock_stats.taken);\r\ndebugfs_create_u32("taken_slow", 0444, d_spin_debug,\r\n&spinlock_stats.taken_slow);\r\ndebugfs_create_u32("taken_slow_nested", 0444, d_spin_debug,\r\n&spinlock_stats.taken_slow_nested);\r\ndebugfs_create_u32("taken_slow_pickup", 0444, d_spin_debug,\r\n&spinlock_stats.taken_slow_pickup);\r\ndebugfs_create_u32("taken_slow_spurious", 0444, d_spin_debug,\r\n&spinlock_stats.taken_slow_spurious);\r\ndebugfs_create_u32("taken_slow_irqenable", 0444, d_spin_debug,\r\n&spinlock_stats.taken_slow_irqenable);\r\ndebugfs_create_u64("released", 0444, d_spin_debug, &spinlock_stats.released);\r\ndebugfs_create_u32("released_slow", 0444, d_spin_debug,\r\n&spinlock_stats.released_slow);\r\ndebugfs_create_u32("released_slow_kicked", 0444, d_spin_debug,\r\n&spinlock_stats.released_slow_kicked);\r\ndebugfs_create_u64("time_spinning", 0444, d_spin_debug,\r\n&spinlock_stats.time_spinning);\r\ndebugfs_create_u64("time_blocked", 0444, d_spin_debug,\r\n&spinlock_stats.time_blocked);\r\ndebugfs_create_u64("time_total", 0444, d_spin_debug,\r\n&spinlock_stats.time_total);\r\nxen_debugfs_create_u32_array("histo_total", 0444, d_spin_debug,\r\nspinlock_stats.histo_spin_total, HISTO_BUCKETS + 1);\r\nxen_debugfs_create_u32_array("histo_spinning", 0444, d_spin_debug,\r\nspinlock_stats.histo_spin_spinning, HISTO_BUCKETS + 1);\r\nxen_debugfs_create_u32_array("histo_blocked", 0444, d_spin_debug,\r\nspinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);\r\nreturn 0;\r\n}
