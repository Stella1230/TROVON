static void memcpy_disk(void *dest, const void *src, size_t len)\r\n__dm_written_to_disk(src)\r\n{\r\nmemcpy(dest, src, len);\r\n__dm_unbless_for_disk(src);\r\n}\r\nstatic void array_insert(void *base, size_t elt_size, unsigned nr_elts,\r\nunsigned index, void *elt)\r\n__dm_written_to_disk(elt)\r\n{\r\nif (index < nr_elts)\r\nmemmove(base + (elt_size * (index + 1)),\r\nbase + (elt_size * index),\r\n(nr_elts - index) * elt_size);\r\nmemcpy_disk(base + (elt_size * index), elt, elt_size);\r\n}\r\nstatic int bsearch(struct btree_node *n, uint64_t key, int want_hi)\r\n{\r\nint lo = -1, hi = le32_to_cpu(n->header.nr_entries);\r\nwhile (hi - lo > 1) {\r\nint mid = lo + ((hi - lo) / 2);\r\nuint64_t mid_key = le64_to_cpu(n->keys[mid]);\r\nif (mid_key == key)\r\nreturn mid;\r\nif (mid_key < key)\r\nlo = mid;\r\nelse\r\nhi = mid;\r\n}\r\nreturn want_hi ? hi : lo;\r\n}\r\nint lower_bound(struct btree_node *n, uint64_t key)\r\n{\r\nreturn bsearch(n, key, 0);\r\n}\r\nvoid inc_children(struct dm_transaction_manager *tm, struct btree_node *n,\r\nstruct dm_btree_value_type *vt)\r\n{\r\nunsigned i;\r\nuint32_t nr_entries = le32_to_cpu(n->header.nr_entries);\r\nif (le32_to_cpu(n->header.flags) & INTERNAL_NODE)\r\nfor (i = 0; i < nr_entries; i++)\r\ndm_tm_inc(tm, value64(n, i));\r\nelse if (vt->inc)\r\nfor (i = 0; i < nr_entries; i++)\r\nvt->inc(vt->context, value_ptr(n, i));\r\n}\r\nstatic int insert_at(size_t value_size, struct btree_node *node, unsigned index,\r\nuint64_t key, void *value)\r\n__dm_written_to_disk(value)\r\n{\r\nuint32_t nr_entries = le32_to_cpu(node->header.nr_entries);\r\n__le64 key_le = cpu_to_le64(key);\r\nif (index > nr_entries ||\r\nindex >= le32_to_cpu(node->header.max_entries)) {\r\nDMERR("too many entries in btree node for insert");\r\n__dm_unbless_for_disk(value);\r\nreturn -ENOMEM;\r\n}\r\n__dm_bless_for_disk(&key_le);\r\narray_insert(node->keys, sizeof(*node->keys), nr_entries, index, &key_le);\r\narray_insert(value_base(node), value_size, nr_entries, index, value);\r\nnode->header.nr_entries = cpu_to_le32(nr_entries + 1);\r\nreturn 0;\r\n}\r\nstatic uint32_t calc_max_entries(size_t value_size, size_t block_size)\r\n{\r\nuint32_t total, n;\r\nsize_t elt_size = sizeof(uint64_t) + value_size;\r\nblock_size -= sizeof(struct node_header);\r\ntotal = block_size / elt_size;\r\nn = total / 3;\r\nreturn 3 * n;\r\n}\r\nint dm_btree_empty(struct dm_btree_info *info, dm_block_t *root)\r\n{\r\nint r;\r\nstruct dm_block *b;\r\nstruct btree_node *n;\r\nsize_t block_size;\r\nuint32_t max_entries;\r\nr = new_block(info, &b);\r\nif (r < 0)\r\nreturn r;\r\nblock_size = dm_bm_block_size(dm_tm_get_bm(info->tm));\r\nmax_entries = calc_max_entries(info->value_type.size, block_size);\r\nn = dm_block_data(b);\r\nmemset(n, 0, block_size);\r\nn->header.flags = cpu_to_le32(LEAF_NODE);\r\nn->header.nr_entries = cpu_to_le32(0);\r\nn->header.max_entries = cpu_to_le32(max_entries);\r\nn->header.value_size = cpu_to_le32(info->value_type.size);\r\n*root = dm_block_location(b);\r\nreturn unlock_block(info, b);\r\n}\r\nstatic int top_frame(struct del_stack *s, struct frame **f)\r\n{\r\nif (s->top < 0) {\r\nDMERR("btree deletion stack empty");\r\nreturn -EINVAL;\r\n}\r\n*f = s->spine + s->top;\r\nreturn 0;\r\n}\r\nstatic int unprocessed_frames(struct del_stack *s)\r\n{\r\nreturn s->top >= 0;\r\n}\r\nstatic void prefetch_children(struct del_stack *s, struct frame *f)\r\n{\r\nunsigned i;\r\nstruct dm_block_manager *bm = dm_tm_get_bm(s->tm);\r\nfor (i = 0; i < f->nr_children; i++)\r\ndm_bm_prefetch(bm, value64(f->n, i));\r\n}\r\nstatic bool is_internal_level(struct dm_btree_info *info, struct frame *f)\r\n{\r\nreturn f->level < (info->levels - 1);\r\n}\r\nstatic int push_frame(struct del_stack *s, dm_block_t b, unsigned level)\r\n{\r\nint r;\r\nuint32_t ref_count;\r\nif (s->top >= MAX_SPINE_DEPTH - 1) {\r\nDMERR("btree deletion stack out of memory");\r\nreturn -ENOMEM;\r\n}\r\nr = dm_tm_ref(s->tm, b, &ref_count);\r\nif (r)\r\nreturn r;\r\nif (ref_count > 1)\r\ndm_tm_dec(s->tm, b);\r\nelse {\r\nuint32_t flags;\r\nstruct frame *f = s->spine + ++s->top;\r\nr = dm_tm_read_lock(s->tm, b, &btree_node_validator, &f->b);\r\nif (r) {\r\ns->top--;\r\nreturn r;\r\n}\r\nf->n = dm_block_data(f->b);\r\nf->level = level;\r\nf->nr_children = le32_to_cpu(f->n->header.nr_entries);\r\nf->current_child = 0;\r\nflags = le32_to_cpu(f->n->header.flags);\r\nif (flags & INTERNAL_NODE || is_internal_level(s->info, f))\r\nprefetch_children(s, f);\r\n}\r\nreturn 0;\r\n}\r\nstatic void pop_frame(struct del_stack *s)\r\n{\r\nstruct frame *f = s->spine + s->top--;\r\ndm_tm_dec(s->tm, dm_block_location(f->b));\r\ndm_tm_unlock(s->tm, f->b);\r\n}\r\nint dm_btree_del(struct dm_btree_info *info, dm_block_t root)\r\n{\r\nint r;\r\nstruct del_stack *s;\r\ns = kmalloc(sizeof(*s), GFP_KERNEL);\r\nif (!s)\r\nreturn -ENOMEM;\r\ns->info = info;\r\ns->tm = info->tm;\r\ns->top = -1;\r\nr = push_frame(s, root, 0);\r\nif (r)\r\ngoto out;\r\nwhile (unprocessed_frames(s)) {\r\nuint32_t flags;\r\nstruct frame *f;\r\ndm_block_t b;\r\nr = top_frame(s, &f);\r\nif (r)\r\ngoto out;\r\nif (f->current_child >= f->nr_children) {\r\npop_frame(s);\r\ncontinue;\r\n}\r\nflags = le32_to_cpu(f->n->header.flags);\r\nif (flags & INTERNAL_NODE) {\r\nb = value64(f->n, f->current_child);\r\nf->current_child++;\r\nr = push_frame(s, b, f->level);\r\nif (r)\r\ngoto out;\r\n} else if (is_internal_level(info, f)) {\r\nb = value64(f->n, f->current_child);\r\nf->current_child++;\r\nr = push_frame(s, b, f->level + 1);\r\nif (r)\r\ngoto out;\r\n} else {\r\nif (info->value_type.dec) {\r\nunsigned i;\r\nfor (i = 0; i < f->nr_children; i++)\r\ninfo->value_type.dec(info->value_type.context,\r\nvalue_ptr(f->n, i));\r\n}\r\npop_frame(s);\r\n}\r\n}\r\nout:\r\nkfree(s);\r\nreturn r;\r\n}\r\nstatic int btree_lookup_raw(struct ro_spine *s, dm_block_t block, uint64_t key,\r\nint (*search_fn)(struct btree_node *, uint64_t),\r\nuint64_t *result_key, void *v, size_t value_size)\r\n{\r\nint i, r;\r\nuint32_t flags, nr_entries;\r\ndo {\r\nr = ro_step(s, block);\r\nif (r < 0)\r\nreturn r;\r\ni = search_fn(ro_node(s), key);\r\nflags = le32_to_cpu(ro_node(s)->header.flags);\r\nnr_entries = le32_to_cpu(ro_node(s)->header.nr_entries);\r\nif (i < 0 || i >= nr_entries)\r\nreturn -ENODATA;\r\nif (flags & INTERNAL_NODE)\r\nblock = value64(ro_node(s), i);\r\n} while (!(flags & LEAF_NODE));\r\n*result_key = le64_to_cpu(ro_node(s)->keys[i]);\r\nmemcpy(v, value_ptr(ro_node(s), i), value_size);\r\nreturn 0;\r\n}\r\nint dm_btree_lookup(struct dm_btree_info *info, dm_block_t root,\r\nuint64_t *keys, void *value_le)\r\n{\r\nunsigned level, last_level = info->levels - 1;\r\nint r = -ENODATA;\r\nuint64_t rkey;\r\n__le64 internal_value_le;\r\nstruct ro_spine spine;\r\ninit_ro_spine(&spine, info);\r\nfor (level = 0; level < info->levels; level++) {\r\nsize_t size;\r\nvoid *value_p;\r\nif (level == last_level) {\r\nvalue_p = value_le;\r\nsize = info->value_type.size;\r\n} else {\r\nvalue_p = &internal_value_le;\r\nsize = sizeof(uint64_t);\r\n}\r\nr = btree_lookup_raw(&spine, root, keys[level],\r\nlower_bound, &rkey,\r\nvalue_p, size);\r\nif (!r) {\r\nif (rkey != keys[level]) {\r\nexit_ro_spine(&spine);\r\nreturn -ENODATA;\r\n}\r\n} else {\r\nexit_ro_spine(&spine);\r\nreturn r;\r\n}\r\nroot = le64_to_cpu(internal_value_le);\r\n}\r\nexit_ro_spine(&spine);\r\nreturn r;\r\n}\r\nstatic int btree_split_sibling(struct shadow_spine *s, dm_block_t root,\r\nunsigned parent_index, uint64_t key)\r\n{\r\nint r;\r\nsize_t size;\r\nunsigned nr_left, nr_right;\r\nstruct dm_block *left, *right, *parent;\r\nstruct btree_node *ln, *rn, *pn;\r\n__le64 location;\r\nleft = shadow_current(s);\r\nr = new_block(s->info, &right);\r\nif (r < 0)\r\nreturn r;\r\nln = dm_block_data(left);\r\nrn = dm_block_data(right);\r\nnr_left = le32_to_cpu(ln->header.nr_entries) / 2;\r\nnr_right = le32_to_cpu(ln->header.nr_entries) - nr_left;\r\nln->header.nr_entries = cpu_to_le32(nr_left);\r\nrn->header.flags = ln->header.flags;\r\nrn->header.nr_entries = cpu_to_le32(nr_right);\r\nrn->header.max_entries = ln->header.max_entries;\r\nrn->header.value_size = ln->header.value_size;\r\nmemcpy(rn->keys, ln->keys + nr_left, nr_right * sizeof(rn->keys[0]));\r\nsize = le32_to_cpu(ln->header.flags) & INTERNAL_NODE ?\r\nsizeof(uint64_t) : s->info->value_type.size;\r\nmemcpy(value_ptr(rn, 0), value_ptr(ln, nr_left),\r\nsize * nr_right);\r\nparent = shadow_parent(s);\r\npn = dm_block_data(parent);\r\nlocation = cpu_to_le64(dm_block_location(left));\r\n__dm_bless_for_disk(&location);\r\nmemcpy_disk(value_ptr(pn, parent_index),\r\n&location, sizeof(__le64));\r\nlocation = cpu_to_le64(dm_block_location(right));\r\n__dm_bless_for_disk(&location);\r\nr = insert_at(sizeof(__le64), pn, parent_index + 1,\r\nle64_to_cpu(rn->keys[0]), &location);\r\nif (r)\r\nreturn r;\r\nif (key < le64_to_cpu(rn->keys[0])) {\r\nunlock_block(s->info, right);\r\ns->nodes[1] = left;\r\n} else {\r\nunlock_block(s->info, left);\r\ns->nodes[1] = right;\r\n}\r\nreturn 0;\r\n}\r\nstatic int btree_split_beneath(struct shadow_spine *s, uint64_t key)\r\n{\r\nint r;\r\nsize_t size;\r\nunsigned nr_left, nr_right;\r\nstruct dm_block *left, *right, *new_parent;\r\nstruct btree_node *pn, *ln, *rn;\r\n__le64 val;\r\nnew_parent = shadow_current(s);\r\nr = new_block(s->info, &left);\r\nif (r < 0)\r\nreturn r;\r\nr = new_block(s->info, &right);\r\nif (r < 0) {\r\nreturn r;\r\n}\r\npn = dm_block_data(new_parent);\r\nln = dm_block_data(left);\r\nrn = dm_block_data(right);\r\nnr_left = le32_to_cpu(pn->header.nr_entries) / 2;\r\nnr_right = le32_to_cpu(pn->header.nr_entries) - nr_left;\r\nln->header.flags = pn->header.flags;\r\nln->header.nr_entries = cpu_to_le32(nr_left);\r\nln->header.max_entries = pn->header.max_entries;\r\nln->header.value_size = pn->header.value_size;\r\nrn->header.flags = pn->header.flags;\r\nrn->header.nr_entries = cpu_to_le32(nr_right);\r\nrn->header.max_entries = pn->header.max_entries;\r\nrn->header.value_size = pn->header.value_size;\r\nmemcpy(ln->keys, pn->keys, nr_left * sizeof(pn->keys[0]));\r\nmemcpy(rn->keys, pn->keys + nr_left, nr_right * sizeof(pn->keys[0]));\r\nsize = le32_to_cpu(pn->header.flags) & INTERNAL_NODE ?\r\nsizeof(__le64) : s->info->value_type.size;\r\nmemcpy(value_ptr(ln, 0), value_ptr(pn, 0), nr_left * size);\r\nmemcpy(value_ptr(rn, 0), value_ptr(pn, nr_left),\r\nnr_right * size);\r\npn->header.flags = cpu_to_le32(INTERNAL_NODE);\r\npn->header.nr_entries = cpu_to_le32(2);\r\npn->header.max_entries = cpu_to_le32(\r\ncalc_max_entries(sizeof(__le64),\r\ndm_bm_block_size(\r\ndm_tm_get_bm(s->info->tm))));\r\npn->header.value_size = cpu_to_le32(sizeof(__le64));\r\nval = cpu_to_le64(dm_block_location(left));\r\n__dm_bless_for_disk(&val);\r\npn->keys[0] = ln->keys[0];\r\nmemcpy_disk(value_ptr(pn, 0), &val, sizeof(__le64));\r\nval = cpu_to_le64(dm_block_location(right));\r\n__dm_bless_for_disk(&val);\r\npn->keys[1] = rn->keys[0];\r\nmemcpy_disk(value_ptr(pn, 1), &val, sizeof(__le64));\r\nif (s->nodes[0] != new_parent) {\r\nunlock_block(s->info, s->nodes[0]);\r\ns->nodes[0] = new_parent;\r\n}\r\nif (key < le64_to_cpu(rn->keys[0])) {\r\nunlock_block(s->info, right);\r\ns->nodes[1] = left;\r\n} else {\r\nunlock_block(s->info, left);\r\ns->nodes[1] = right;\r\n}\r\ns->count = 2;\r\nreturn 0;\r\n}\r\nstatic int btree_insert_raw(struct shadow_spine *s, dm_block_t root,\r\nstruct dm_btree_value_type *vt,\r\nuint64_t key, unsigned *index)\r\n{\r\nint r, i = *index, top = 1;\r\nstruct btree_node *node;\r\nfor (;;) {\r\nr = shadow_step(s, root, vt);\r\nif (r < 0)\r\nreturn r;\r\nnode = dm_block_data(shadow_current(s));\r\nif (shadow_has_parent(s) && i >= 0) {\r\n__le64 location = cpu_to_le64(dm_block_location(shadow_current(s)));\r\n__dm_bless_for_disk(&location);\r\nmemcpy_disk(value_ptr(dm_block_data(shadow_parent(s)), i),\r\n&location, sizeof(__le64));\r\n}\r\nnode = dm_block_data(shadow_current(s));\r\nif (node->header.nr_entries == node->header.max_entries) {\r\nif (top)\r\nr = btree_split_beneath(s, key);\r\nelse\r\nr = btree_split_sibling(s, root, i, key);\r\nif (r < 0)\r\nreturn r;\r\n}\r\nnode = dm_block_data(shadow_current(s));\r\ni = lower_bound(node, key);\r\nif (le32_to_cpu(node->header.flags) & LEAF_NODE)\r\nbreak;\r\nif (i < 0) {\r\nnode->keys[0] = cpu_to_le64(key);\r\ni = 0;\r\n}\r\nroot = value64(node, i);\r\ntop = 0;\r\n}\r\nif (i < 0 || le64_to_cpu(node->keys[i]) != key)\r\ni++;\r\n*index = i;\r\nreturn 0;\r\n}\r\nstatic int insert(struct dm_btree_info *info, dm_block_t root,\r\nuint64_t *keys, void *value, dm_block_t *new_root,\r\nint *inserted)\r\n__dm_written_to_disk(value)\r\n{\r\nint r, need_insert;\r\nunsigned level, index = -1, last_level = info->levels - 1;\r\ndm_block_t block = root;\r\nstruct shadow_spine spine;\r\nstruct btree_node *n;\r\nstruct dm_btree_value_type le64_type;\r\nle64_type.context = NULL;\r\nle64_type.size = sizeof(__le64);\r\nle64_type.inc = NULL;\r\nle64_type.dec = NULL;\r\nle64_type.equal = NULL;\r\ninit_shadow_spine(&spine, info);\r\nfor (level = 0; level < (info->levels - 1); level++) {\r\nr = btree_insert_raw(&spine, block, &le64_type, keys[level], &index);\r\nif (r < 0)\r\ngoto bad;\r\nn = dm_block_data(shadow_current(&spine));\r\nneed_insert = ((index >= le32_to_cpu(n->header.nr_entries)) ||\r\n(le64_to_cpu(n->keys[index]) != keys[level]));\r\nif (need_insert) {\r\ndm_block_t new_tree;\r\n__le64 new_le;\r\nr = dm_btree_empty(info, &new_tree);\r\nif (r < 0)\r\ngoto bad;\r\nnew_le = cpu_to_le64(new_tree);\r\n__dm_bless_for_disk(&new_le);\r\nr = insert_at(sizeof(uint64_t), n, index,\r\nkeys[level], &new_le);\r\nif (r)\r\ngoto bad;\r\n}\r\nif (level < last_level)\r\nblock = value64(n, index);\r\n}\r\nr = btree_insert_raw(&spine, block, &info->value_type,\r\nkeys[level], &index);\r\nif (r < 0)\r\ngoto bad;\r\nn = dm_block_data(shadow_current(&spine));\r\nneed_insert = ((index >= le32_to_cpu(n->header.nr_entries)) ||\r\n(le64_to_cpu(n->keys[index]) != keys[level]));\r\nif (need_insert) {\r\nif (inserted)\r\n*inserted = 1;\r\nr = insert_at(info->value_type.size, n, index,\r\nkeys[level], value);\r\nif (r)\r\ngoto bad_unblessed;\r\n} else {\r\nif (inserted)\r\n*inserted = 0;\r\nif (info->value_type.dec &&\r\n(!info->value_type.equal ||\r\n!info->value_type.equal(\r\ninfo->value_type.context,\r\nvalue_ptr(n, index),\r\nvalue))) {\r\ninfo->value_type.dec(info->value_type.context,\r\nvalue_ptr(n, index));\r\n}\r\nmemcpy_disk(value_ptr(n, index),\r\nvalue, info->value_type.size);\r\n}\r\n*new_root = shadow_root(&spine);\r\nexit_shadow_spine(&spine);\r\nreturn 0;\r\nbad:\r\n__dm_unbless_for_disk(value);\r\nbad_unblessed:\r\nexit_shadow_spine(&spine);\r\nreturn r;\r\n}\r\nint dm_btree_insert(struct dm_btree_info *info, dm_block_t root,\r\nuint64_t *keys, void *value, dm_block_t *new_root)\r\n__dm_written_to_disk(value)\r\n{\r\nreturn insert(info, root, keys, value, new_root, NULL);\r\n}\r\nint dm_btree_insert_notify(struct dm_btree_info *info, dm_block_t root,\r\nuint64_t *keys, void *value, dm_block_t *new_root,\r\nint *inserted)\r\n__dm_written_to_disk(value)\r\n{\r\nreturn insert(info, root, keys, value, new_root, inserted);\r\n}\r\nstatic int find_key(struct ro_spine *s, dm_block_t block, bool find_highest,\r\nuint64_t *result_key, dm_block_t *next_block)\r\n{\r\nint i, r;\r\nuint32_t flags;\r\ndo {\r\nr = ro_step(s, block);\r\nif (r < 0)\r\nreturn r;\r\nflags = le32_to_cpu(ro_node(s)->header.flags);\r\ni = le32_to_cpu(ro_node(s)->header.nr_entries);\r\nif (!i)\r\nreturn -ENODATA;\r\nelse\r\ni--;\r\nif (find_highest)\r\n*result_key = le64_to_cpu(ro_node(s)->keys[i]);\r\nelse\r\n*result_key = le64_to_cpu(ro_node(s)->keys[0]);\r\nif (next_block || flags & INTERNAL_NODE)\r\nblock = value64(ro_node(s), i);\r\n} while (flags & INTERNAL_NODE);\r\nif (next_block)\r\n*next_block = block;\r\nreturn 0;\r\n}\r\nstatic int dm_btree_find_key(struct dm_btree_info *info, dm_block_t root,\r\nbool find_highest, uint64_t *result_keys)\r\n{\r\nint r = 0, count = 0, level;\r\nstruct ro_spine spine;\r\ninit_ro_spine(&spine, info);\r\nfor (level = 0; level < info->levels; level++) {\r\nr = find_key(&spine, root, find_highest, result_keys + level,\r\nlevel == info->levels - 1 ? NULL : &root);\r\nif (r == -ENODATA) {\r\nr = 0;\r\nbreak;\r\n} else if (r)\r\nbreak;\r\ncount++;\r\n}\r\nexit_ro_spine(&spine);\r\nreturn r ? r : count;\r\n}\r\nint dm_btree_find_highest_key(struct dm_btree_info *info, dm_block_t root,\r\nuint64_t *result_keys)\r\n{\r\nreturn dm_btree_find_key(info, root, true, result_keys);\r\n}\r\nint dm_btree_find_lowest_key(struct dm_btree_info *info, dm_block_t root,\r\nuint64_t *result_keys)\r\n{\r\nreturn dm_btree_find_key(info, root, false, result_keys);\r\n}\r\nstatic int walk_node(struct dm_btree_info *info, dm_block_t block,\r\nint (*fn)(void *context, uint64_t *keys, void *leaf),\r\nvoid *context)\r\n{\r\nint r;\r\nunsigned i, nr;\r\nstruct dm_block *node;\r\nstruct btree_node *n;\r\nuint64_t keys;\r\nr = bn_read_lock(info, block, &node);\r\nif (r)\r\nreturn r;\r\nn = dm_block_data(node);\r\nnr = le32_to_cpu(n->header.nr_entries);\r\nfor (i = 0; i < nr; i++) {\r\nif (le32_to_cpu(n->header.flags) & INTERNAL_NODE) {\r\nr = walk_node(info, value64(n, i), fn, context);\r\nif (r)\r\ngoto out;\r\n} else {\r\nkeys = le64_to_cpu(*key_ptr(n, i));\r\nr = fn(context, &keys, value_ptr(n, i));\r\nif (r)\r\ngoto out;\r\n}\r\n}\r\nout:\r\ndm_tm_unlock(info->tm, node);\r\nreturn r;\r\n}\r\nint dm_btree_walk(struct dm_btree_info *info, dm_block_t root,\r\nint (*fn)(void *context, uint64_t *keys, void *leaf),\r\nvoid *context)\r\n{\r\nBUG_ON(info->levels > 1);\r\nreturn walk_node(info, root, fn, context);\r\n}
