static inline struct throtl_grp *pd_to_tg(struct blkg_policy_data *pd)\r\n{\r\nreturn pd ? container_of(pd, struct throtl_grp, pd) : NULL;\r\n}\r\nstatic inline struct throtl_grp *blkg_to_tg(struct blkcg_gq *blkg)\r\n{\r\nreturn pd_to_tg(blkg_to_pd(blkg, &blkcg_policy_throtl));\r\n}\r\nstatic inline struct blkcg_gq *tg_to_blkg(struct throtl_grp *tg)\r\n{\r\nreturn pd_to_blkg(&tg->pd);\r\n}\r\nstatic inline struct throtl_grp *td_root_tg(struct throtl_data *td)\r\n{\r\nreturn blkg_to_tg(td->queue->root_blkg);\r\n}\r\nstatic struct throtl_grp *sq_to_tg(struct throtl_service_queue *sq)\r\n{\r\nif (sq && sq->parent_sq)\r\nreturn container_of(sq, struct throtl_grp, service_queue);\r\nelse\r\nreturn NULL;\r\n}\r\nstatic struct throtl_data *sq_to_td(struct throtl_service_queue *sq)\r\n{\r\nstruct throtl_grp *tg = sq_to_tg(sq);\r\nif (tg)\r\nreturn tg->td;\r\nelse\r\nreturn container_of(sq, struct throtl_data, service_queue);\r\n}\r\nstatic void tg_stats_init(struct tg_stats_cpu *tg_stats)\r\n{\r\nblkg_rwstat_init(&tg_stats->service_bytes);\r\nblkg_rwstat_init(&tg_stats->serviced);\r\n}\r\nstatic void tg_stats_alloc_fn(struct work_struct *work)\r\n{\r\nstatic struct tg_stats_cpu *stats_cpu;\r\nstruct delayed_work *dwork = to_delayed_work(work);\r\nbool empty = false;\r\nalloc_stats:\r\nif (!stats_cpu) {\r\nint cpu;\r\nstats_cpu = alloc_percpu(struct tg_stats_cpu);\r\nif (!stats_cpu) {\r\nschedule_delayed_work(dwork, msecs_to_jiffies(10));\r\nreturn;\r\n}\r\nfor_each_possible_cpu(cpu)\r\ntg_stats_init(per_cpu_ptr(stats_cpu, cpu));\r\n}\r\nspin_lock_irq(&tg_stats_alloc_lock);\r\nif (!list_empty(&tg_stats_alloc_list)) {\r\nstruct throtl_grp *tg = list_first_entry(&tg_stats_alloc_list,\r\nstruct throtl_grp,\r\nstats_alloc_node);\r\nswap(tg->stats_cpu, stats_cpu);\r\nlist_del_init(&tg->stats_alloc_node);\r\n}\r\nempty = list_empty(&tg_stats_alloc_list);\r\nspin_unlock_irq(&tg_stats_alloc_lock);\r\nif (!empty)\r\ngoto alloc_stats;\r\n}\r\nstatic void throtl_qnode_init(struct throtl_qnode *qn, struct throtl_grp *tg)\r\n{\r\nINIT_LIST_HEAD(&qn->node);\r\nbio_list_init(&qn->bios);\r\nqn->tg = tg;\r\n}\r\nstatic void throtl_qnode_add_bio(struct bio *bio, struct throtl_qnode *qn,\r\nstruct list_head *queued)\r\n{\r\nbio_list_add(&qn->bios, bio);\r\nif (list_empty(&qn->node)) {\r\nlist_add_tail(&qn->node, queued);\r\nblkg_get(tg_to_blkg(qn->tg));\r\n}\r\n}\r\nstatic struct bio *throtl_peek_queued(struct list_head *queued)\r\n{\r\nstruct throtl_qnode *qn = list_first_entry(queued, struct throtl_qnode, node);\r\nstruct bio *bio;\r\nif (list_empty(queued))\r\nreturn NULL;\r\nbio = bio_list_peek(&qn->bios);\r\nWARN_ON_ONCE(!bio);\r\nreturn bio;\r\n}\r\nstatic struct bio *throtl_pop_queued(struct list_head *queued,\r\nstruct throtl_grp **tg_to_put)\r\n{\r\nstruct throtl_qnode *qn = list_first_entry(queued, struct throtl_qnode, node);\r\nstruct bio *bio;\r\nif (list_empty(queued))\r\nreturn NULL;\r\nbio = bio_list_pop(&qn->bios);\r\nWARN_ON_ONCE(!bio);\r\nif (bio_list_empty(&qn->bios)) {\r\nlist_del_init(&qn->node);\r\nif (tg_to_put)\r\n*tg_to_put = qn->tg;\r\nelse\r\nblkg_put(tg_to_blkg(qn->tg));\r\n} else {\r\nlist_move_tail(&qn->node, queued);\r\n}\r\nreturn bio;\r\n}\r\nstatic void throtl_service_queue_init(struct throtl_service_queue *sq,\r\nstruct throtl_service_queue *parent_sq)\r\n{\r\nINIT_LIST_HEAD(&sq->queued[0]);\r\nINIT_LIST_HEAD(&sq->queued[1]);\r\nsq->pending_tree = RB_ROOT;\r\nsq->parent_sq = parent_sq;\r\nsetup_timer(&sq->pending_timer, throtl_pending_timer_fn,\r\n(unsigned long)sq);\r\n}\r\nstatic void throtl_service_queue_exit(struct throtl_service_queue *sq)\r\n{\r\ndel_timer_sync(&sq->pending_timer);\r\n}\r\nstatic void throtl_pd_init(struct blkcg_gq *blkg)\r\n{\r\nstruct throtl_grp *tg = blkg_to_tg(blkg);\r\nstruct throtl_data *td = blkg->q->td;\r\nstruct throtl_service_queue *parent_sq;\r\nunsigned long flags;\r\nint rw;\r\nparent_sq = &td->service_queue;\r\nif (cgroup_on_dfl(blkg->blkcg->css.cgroup) && blkg->parent)\r\nparent_sq = &blkg_to_tg(blkg->parent)->service_queue;\r\nthrotl_service_queue_init(&tg->service_queue, parent_sq);\r\nfor (rw = READ; rw <= WRITE; rw++) {\r\nthrotl_qnode_init(&tg->qnode_on_self[rw], tg);\r\nthrotl_qnode_init(&tg->qnode_on_parent[rw], tg);\r\n}\r\nRB_CLEAR_NODE(&tg->rb_node);\r\ntg->td = td;\r\ntg->bps[READ] = -1;\r\ntg->bps[WRITE] = -1;\r\ntg->iops[READ] = -1;\r\ntg->iops[WRITE] = -1;\r\nspin_lock_irqsave(&tg_stats_alloc_lock, flags);\r\nlist_add(&tg->stats_alloc_node, &tg_stats_alloc_list);\r\nschedule_delayed_work(&tg_stats_alloc_work, 0);\r\nspin_unlock_irqrestore(&tg_stats_alloc_lock, flags);\r\n}\r\nstatic void tg_update_has_rules(struct throtl_grp *tg)\r\n{\r\nstruct throtl_grp *parent_tg = sq_to_tg(tg->service_queue.parent_sq);\r\nint rw;\r\nfor (rw = READ; rw <= WRITE; rw++)\r\ntg->has_rules[rw] = (parent_tg && parent_tg->has_rules[rw]) ||\r\n(tg->bps[rw] != -1 || tg->iops[rw] != -1);\r\n}\r\nstatic void throtl_pd_online(struct blkcg_gq *blkg)\r\n{\r\ntg_update_has_rules(blkg_to_tg(blkg));\r\n}\r\nstatic void throtl_pd_exit(struct blkcg_gq *blkg)\r\n{\r\nstruct throtl_grp *tg = blkg_to_tg(blkg);\r\nunsigned long flags;\r\nspin_lock_irqsave(&tg_stats_alloc_lock, flags);\r\nlist_del_init(&tg->stats_alloc_node);\r\nspin_unlock_irqrestore(&tg_stats_alloc_lock, flags);\r\nfree_percpu(tg->stats_cpu);\r\nthrotl_service_queue_exit(&tg->service_queue);\r\n}\r\nstatic void throtl_pd_reset_stats(struct blkcg_gq *blkg)\r\n{\r\nstruct throtl_grp *tg = blkg_to_tg(blkg);\r\nint cpu;\r\nif (tg->stats_cpu == NULL)\r\nreturn;\r\nfor_each_possible_cpu(cpu) {\r\nstruct tg_stats_cpu *sc = per_cpu_ptr(tg->stats_cpu, cpu);\r\nblkg_rwstat_reset(&sc->service_bytes);\r\nblkg_rwstat_reset(&sc->serviced);\r\n}\r\n}\r\nstatic struct throtl_grp *throtl_lookup_tg(struct throtl_data *td,\r\nstruct blkcg *blkcg)\r\n{\r\nif (blkcg == &blkcg_root)\r\nreturn td_root_tg(td);\r\nreturn blkg_to_tg(blkg_lookup(blkcg, td->queue));\r\n}\r\nstatic struct throtl_grp *throtl_lookup_create_tg(struct throtl_data *td,\r\nstruct blkcg *blkcg)\r\n{\r\nstruct request_queue *q = td->queue;\r\nstruct throtl_grp *tg = NULL;\r\nif (blkcg == &blkcg_root) {\r\ntg = td_root_tg(td);\r\n} else {\r\nstruct blkcg_gq *blkg;\r\nblkg = blkg_lookup_create(blkcg, q);\r\nif (!IS_ERR(blkg))\r\ntg = blkg_to_tg(blkg);\r\nelse if (!blk_queue_dying(q))\r\ntg = td_root_tg(td);\r\n}\r\nreturn tg;\r\n}\r\nstatic struct throtl_grp *\r\nthrotl_rb_first(struct throtl_service_queue *parent_sq)\r\n{\r\nif (!parent_sq->nr_pending)\r\nreturn NULL;\r\nif (!parent_sq->first_pending)\r\nparent_sq->first_pending = rb_first(&parent_sq->pending_tree);\r\nif (parent_sq->first_pending)\r\nreturn rb_entry_tg(parent_sq->first_pending);\r\nreturn NULL;\r\n}\r\nstatic void rb_erase_init(struct rb_node *n, struct rb_root *root)\r\n{\r\nrb_erase(n, root);\r\nRB_CLEAR_NODE(n);\r\n}\r\nstatic void throtl_rb_erase(struct rb_node *n,\r\nstruct throtl_service_queue *parent_sq)\r\n{\r\nif (parent_sq->first_pending == n)\r\nparent_sq->first_pending = NULL;\r\nrb_erase_init(n, &parent_sq->pending_tree);\r\n--parent_sq->nr_pending;\r\n}\r\nstatic void update_min_dispatch_time(struct throtl_service_queue *parent_sq)\r\n{\r\nstruct throtl_grp *tg;\r\ntg = throtl_rb_first(parent_sq);\r\nif (!tg)\r\nreturn;\r\nparent_sq->first_pending_disptime = tg->disptime;\r\n}\r\nstatic void tg_service_queue_add(struct throtl_grp *tg)\r\n{\r\nstruct throtl_service_queue *parent_sq = tg->service_queue.parent_sq;\r\nstruct rb_node **node = &parent_sq->pending_tree.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct throtl_grp *__tg;\r\nunsigned long key = tg->disptime;\r\nint left = 1;\r\nwhile (*node != NULL) {\r\nparent = *node;\r\n__tg = rb_entry_tg(parent);\r\nif (time_before(key, __tg->disptime))\r\nnode = &parent->rb_left;\r\nelse {\r\nnode = &parent->rb_right;\r\nleft = 0;\r\n}\r\n}\r\nif (left)\r\nparent_sq->first_pending = &tg->rb_node;\r\nrb_link_node(&tg->rb_node, parent, node);\r\nrb_insert_color(&tg->rb_node, &parent_sq->pending_tree);\r\n}\r\nstatic void __throtl_enqueue_tg(struct throtl_grp *tg)\r\n{\r\ntg_service_queue_add(tg);\r\ntg->flags |= THROTL_TG_PENDING;\r\ntg->service_queue.parent_sq->nr_pending++;\r\n}\r\nstatic void throtl_enqueue_tg(struct throtl_grp *tg)\r\n{\r\nif (!(tg->flags & THROTL_TG_PENDING))\r\n__throtl_enqueue_tg(tg);\r\n}\r\nstatic void __throtl_dequeue_tg(struct throtl_grp *tg)\r\n{\r\nthrotl_rb_erase(&tg->rb_node, tg->service_queue.parent_sq);\r\ntg->flags &= ~THROTL_TG_PENDING;\r\n}\r\nstatic void throtl_dequeue_tg(struct throtl_grp *tg)\r\n{\r\nif (tg->flags & THROTL_TG_PENDING)\r\n__throtl_dequeue_tg(tg);\r\n}\r\nstatic void throtl_schedule_pending_timer(struct throtl_service_queue *sq,\r\nunsigned long expires)\r\n{\r\nmod_timer(&sq->pending_timer, expires);\r\nthrotl_log(sq, "schedule timer. delay=%lu jiffies=%lu",\r\nexpires - jiffies, jiffies);\r\n}\r\nstatic bool throtl_schedule_next_dispatch(struct throtl_service_queue *sq,\r\nbool force)\r\n{\r\nif (!sq->nr_pending)\r\nreturn true;\r\nupdate_min_dispatch_time(sq);\r\nif (force || time_after(sq->first_pending_disptime, jiffies)) {\r\nthrotl_schedule_pending_timer(sq, sq->first_pending_disptime);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic inline void throtl_start_new_slice_with_credit(struct throtl_grp *tg,\r\nbool rw, unsigned long start)\r\n{\r\ntg->bytes_disp[rw] = 0;\r\ntg->io_disp[rw] = 0;\r\nif (time_after_eq(start, tg->slice_start[rw]))\r\ntg->slice_start[rw] = start;\r\ntg->slice_end[rw] = jiffies + throtl_slice;\r\nthrotl_log(&tg->service_queue,\r\n"[%c] new slice with credit start=%lu end=%lu jiffies=%lu",\r\nrw == READ ? 'R' : 'W', tg->slice_start[rw],\r\ntg->slice_end[rw], jiffies);\r\n}\r\nstatic inline void throtl_start_new_slice(struct throtl_grp *tg, bool rw)\r\n{\r\ntg->bytes_disp[rw] = 0;\r\ntg->io_disp[rw] = 0;\r\ntg->slice_start[rw] = jiffies;\r\ntg->slice_end[rw] = jiffies + throtl_slice;\r\nthrotl_log(&tg->service_queue,\r\n"[%c] new slice start=%lu end=%lu jiffies=%lu",\r\nrw == READ ? 'R' : 'W', tg->slice_start[rw],\r\ntg->slice_end[rw], jiffies);\r\n}\r\nstatic inline void throtl_set_slice_end(struct throtl_grp *tg, bool rw,\r\nunsigned long jiffy_end)\r\n{\r\ntg->slice_end[rw] = roundup(jiffy_end, throtl_slice);\r\n}\r\nstatic inline void throtl_extend_slice(struct throtl_grp *tg, bool rw,\r\nunsigned long jiffy_end)\r\n{\r\ntg->slice_end[rw] = roundup(jiffy_end, throtl_slice);\r\nthrotl_log(&tg->service_queue,\r\n"[%c] extend slice start=%lu end=%lu jiffies=%lu",\r\nrw == READ ? 'R' : 'W', tg->slice_start[rw],\r\ntg->slice_end[rw], jiffies);\r\n}\r\nstatic bool throtl_slice_used(struct throtl_grp *tg, bool rw)\r\n{\r\nif (time_in_range(jiffies, tg->slice_start[rw], tg->slice_end[rw]))\r\nreturn false;\r\nreturn 1;\r\n}\r\nstatic inline void throtl_trim_slice(struct throtl_grp *tg, bool rw)\r\n{\r\nunsigned long nr_slices, time_elapsed, io_trim;\r\nu64 bytes_trim, tmp;\r\nBUG_ON(time_before(tg->slice_end[rw], tg->slice_start[rw]));\r\nif (throtl_slice_used(tg, rw))\r\nreturn;\r\nthrotl_set_slice_end(tg, rw, jiffies + throtl_slice);\r\ntime_elapsed = jiffies - tg->slice_start[rw];\r\nnr_slices = time_elapsed / throtl_slice;\r\nif (!nr_slices)\r\nreturn;\r\ntmp = tg->bps[rw] * throtl_slice * nr_slices;\r\ndo_div(tmp, HZ);\r\nbytes_trim = tmp;\r\nio_trim = (tg->iops[rw] * throtl_slice * nr_slices)/HZ;\r\nif (!bytes_trim && !io_trim)\r\nreturn;\r\nif (tg->bytes_disp[rw] >= bytes_trim)\r\ntg->bytes_disp[rw] -= bytes_trim;\r\nelse\r\ntg->bytes_disp[rw] = 0;\r\nif (tg->io_disp[rw] >= io_trim)\r\ntg->io_disp[rw] -= io_trim;\r\nelse\r\ntg->io_disp[rw] = 0;\r\ntg->slice_start[rw] += nr_slices * throtl_slice;\r\nthrotl_log(&tg->service_queue,\r\n"[%c] trim slice nr=%lu bytes=%llu io=%lu start=%lu end=%lu jiffies=%lu",\r\nrw == READ ? 'R' : 'W', nr_slices, bytes_trim, io_trim,\r\ntg->slice_start[rw], tg->slice_end[rw], jiffies);\r\n}\r\nstatic bool tg_with_in_iops_limit(struct throtl_grp *tg, struct bio *bio,\r\nunsigned long *wait)\r\n{\r\nbool rw = bio_data_dir(bio);\r\nunsigned int io_allowed;\r\nunsigned long jiffy_elapsed, jiffy_wait, jiffy_elapsed_rnd;\r\nu64 tmp;\r\njiffy_elapsed = jiffy_elapsed_rnd = jiffies - tg->slice_start[rw];\r\nif (!jiffy_elapsed)\r\njiffy_elapsed_rnd = throtl_slice;\r\njiffy_elapsed_rnd = roundup(jiffy_elapsed_rnd, throtl_slice);\r\ntmp = (u64)tg->iops[rw] * jiffy_elapsed_rnd;\r\ndo_div(tmp, HZ);\r\nif (tmp > UINT_MAX)\r\nio_allowed = UINT_MAX;\r\nelse\r\nio_allowed = tmp;\r\nif (tg->io_disp[rw] + 1 <= io_allowed) {\r\nif (wait)\r\n*wait = 0;\r\nreturn true;\r\n}\r\njiffy_wait = ((tg->io_disp[rw] + 1) * HZ)/tg->iops[rw] + 1;\r\nif (jiffy_wait > jiffy_elapsed)\r\njiffy_wait = jiffy_wait - jiffy_elapsed;\r\nelse\r\njiffy_wait = 1;\r\nif (wait)\r\n*wait = jiffy_wait;\r\nreturn 0;\r\n}\r\nstatic bool tg_with_in_bps_limit(struct throtl_grp *tg, struct bio *bio,\r\nunsigned long *wait)\r\n{\r\nbool rw = bio_data_dir(bio);\r\nu64 bytes_allowed, extra_bytes, tmp;\r\nunsigned long jiffy_elapsed, jiffy_wait, jiffy_elapsed_rnd;\r\njiffy_elapsed = jiffy_elapsed_rnd = jiffies - tg->slice_start[rw];\r\nif (!jiffy_elapsed)\r\njiffy_elapsed_rnd = throtl_slice;\r\njiffy_elapsed_rnd = roundup(jiffy_elapsed_rnd, throtl_slice);\r\ntmp = tg->bps[rw] * jiffy_elapsed_rnd;\r\ndo_div(tmp, HZ);\r\nbytes_allowed = tmp;\r\nif (tg->bytes_disp[rw] + bio->bi_iter.bi_size <= bytes_allowed) {\r\nif (wait)\r\n*wait = 0;\r\nreturn true;\r\n}\r\nextra_bytes = tg->bytes_disp[rw] + bio->bi_iter.bi_size - bytes_allowed;\r\njiffy_wait = div64_u64(extra_bytes * HZ, tg->bps[rw]);\r\nif (!jiffy_wait)\r\njiffy_wait = 1;\r\njiffy_wait = jiffy_wait + (jiffy_elapsed_rnd - jiffy_elapsed);\r\nif (wait)\r\n*wait = jiffy_wait;\r\nreturn 0;\r\n}\r\nstatic bool tg_may_dispatch(struct throtl_grp *tg, struct bio *bio,\r\nunsigned long *wait)\r\n{\r\nbool rw = bio_data_dir(bio);\r\nunsigned long bps_wait = 0, iops_wait = 0, max_wait = 0;\r\nBUG_ON(tg->service_queue.nr_queued[rw] &&\r\nbio != throtl_peek_queued(&tg->service_queue.queued[rw]));\r\nif (tg->bps[rw] == -1 && tg->iops[rw] == -1) {\r\nif (wait)\r\n*wait = 0;\r\nreturn true;\r\n}\r\nif (throtl_slice_used(tg, rw))\r\nthrotl_start_new_slice(tg, rw);\r\nelse {\r\nif (time_before(tg->slice_end[rw], jiffies + throtl_slice))\r\nthrotl_extend_slice(tg, rw, jiffies + throtl_slice);\r\n}\r\nif (tg_with_in_bps_limit(tg, bio, &bps_wait) &&\r\ntg_with_in_iops_limit(tg, bio, &iops_wait)) {\r\nif (wait)\r\n*wait = 0;\r\nreturn 1;\r\n}\r\nmax_wait = max(bps_wait, iops_wait);\r\nif (wait)\r\n*wait = max_wait;\r\nif (time_before(tg->slice_end[rw], jiffies + max_wait))\r\nthrotl_extend_slice(tg, rw, jiffies + max_wait);\r\nreturn 0;\r\n}\r\nstatic void throtl_update_dispatch_stats(struct blkcg_gq *blkg, u64 bytes,\r\nint rw)\r\n{\r\nstruct throtl_grp *tg = blkg_to_tg(blkg);\r\nstruct tg_stats_cpu *stats_cpu;\r\nunsigned long flags;\r\nif (tg->stats_cpu == NULL)\r\nreturn;\r\nlocal_irq_save(flags);\r\nstats_cpu = this_cpu_ptr(tg->stats_cpu);\r\nblkg_rwstat_add(&stats_cpu->serviced, rw, 1);\r\nblkg_rwstat_add(&stats_cpu->service_bytes, rw, bytes);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void throtl_charge_bio(struct throtl_grp *tg, struct bio *bio)\r\n{\r\nbool rw = bio_data_dir(bio);\r\ntg->bytes_disp[rw] += bio->bi_iter.bi_size;\r\ntg->io_disp[rw]++;\r\nif (!(bio->bi_rw & REQ_THROTTLED)) {\r\nbio->bi_rw |= REQ_THROTTLED;\r\nthrotl_update_dispatch_stats(tg_to_blkg(tg),\r\nbio->bi_iter.bi_size, bio->bi_rw);\r\n}\r\n}\r\nstatic void throtl_add_bio_tg(struct bio *bio, struct throtl_qnode *qn,\r\nstruct throtl_grp *tg)\r\n{\r\nstruct throtl_service_queue *sq = &tg->service_queue;\r\nbool rw = bio_data_dir(bio);\r\nif (!qn)\r\nqn = &tg->qnode_on_self[rw];\r\nif (!sq->nr_queued[rw])\r\ntg->flags |= THROTL_TG_WAS_EMPTY;\r\nthrotl_qnode_add_bio(bio, qn, &sq->queued[rw]);\r\nsq->nr_queued[rw]++;\r\nthrotl_enqueue_tg(tg);\r\n}\r\nstatic void tg_update_disptime(struct throtl_grp *tg)\r\n{\r\nstruct throtl_service_queue *sq = &tg->service_queue;\r\nunsigned long read_wait = -1, write_wait = -1, min_wait = -1, disptime;\r\nstruct bio *bio;\r\nif ((bio = throtl_peek_queued(&sq->queued[READ])))\r\ntg_may_dispatch(tg, bio, &read_wait);\r\nif ((bio = throtl_peek_queued(&sq->queued[WRITE])))\r\ntg_may_dispatch(tg, bio, &write_wait);\r\nmin_wait = min(read_wait, write_wait);\r\ndisptime = jiffies + min_wait;\r\nthrotl_dequeue_tg(tg);\r\ntg->disptime = disptime;\r\nthrotl_enqueue_tg(tg);\r\ntg->flags &= ~THROTL_TG_WAS_EMPTY;\r\n}\r\nstatic void start_parent_slice_with_credit(struct throtl_grp *child_tg,\r\nstruct throtl_grp *parent_tg, bool rw)\r\n{\r\nif (throtl_slice_used(parent_tg, rw)) {\r\nthrotl_start_new_slice_with_credit(parent_tg, rw,\r\nchild_tg->slice_start[rw]);\r\n}\r\n}\r\nstatic void tg_dispatch_one_bio(struct throtl_grp *tg, bool rw)\r\n{\r\nstruct throtl_service_queue *sq = &tg->service_queue;\r\nstruct throtl_service_queue *parent_sq = sq->parent_sq;\r\nstruct throtl_grp *parent_tg = sq_to_tg(parent_sq);\r\nstruct throtl_grp *tg_to_put = NULL;\r\nstruct bio *bio;\r\nbio = throtl_pop_queued(&sq->queued[rw], &tg_to_put);\r\nsq->nr_queued[rw]--;\r\nthrotl_charge_bio(tg, bio);\r\nif (parent_tg) {\r\nthrotl_add_bio_tg(bio, &tg->qnode_on_parent[rw], parent_tg);\r\nstart_parent_slice_with_credit(tg, parent_tg, rw);\r\n} else {\r\nthrotl_qnode_add_bio(bio, &tg->qnode_on_parent[rw],\r\n&parent_sq->queued[rw]);\r\nBUG_ON(tg->td->nr_queued[rw] <= 0);\r\ntg->td->nr_queued[rw]--;\r\n}\r\nthrotl_trim_slice(tg, rw);\r\nif (tg_to_put)\r\nblkg_put(tg_to_blkg(tg_to_put));\r\n}\r\nstatic int throtl_dispatch_tg(struct throtl_grp *tg)\r\n{\r\nstruct throtl_service_queue *sq = &tg->service_queue;\r\nunsigned int nr_reads = 0, nr_writes = 0;\r\nunsigned int max_nr_reads = throtl_grp_quantum*3/4;\r\nunsigned int max_nr_writes = throtl_grp_quantum - max_nr_reads;\r\nstruct bio *bio;\r\nwhile ((bio = throtl_peek_queued(&sq->queued[READ])) &&\r\ntg_may_dispatch(tg, bio, NULL)) {\r\ntg_dispatch_one_bio(tg, bio_data_dir(bio));\r\nnr_reads++;\r\nif (nr_reads >= max_nr_reads)\r\nbreak;\r\n}\r\nwhile ((bio = throtl_peek_queued(&sq->queued[WRITE])) &&\r\ntg_may_dispatch(tg, bio, NULL)) {\r\ntg_dispatch_one_bio(tg, bio_data_dir(bio));\r\nnr_writes++;\r\nif (nr_writes >= max_nr_writes)\r\nbreak;\r\n}\r\nreturn nr_reads + nr_writes;\r\n}\r\nstatic int throtl_select_dispatch(struct throtl_service_queue *parent_sq)\r\n{\r\nunsigned int nr_disp = 0;\r\nwhile (1) {\r\nstruct throtl_grp *tg = throtl_rb_first(parent_sq);\r\nstruct throtl_service_queue *sq = &tg->service_queue;\r\nif (!tg)\r\nbreak;\r\nif (time_before(jiffies, tg->disptime))\r\nbreak;\r\nthrotl_dequeue_tg(tg);\r\nnr_disp += throtl_dispatch_tg(tg);\r\nif (sq->nr_queued[0] || sq->nr_queued[1])\r\ntg_update_disptime(tg);\r\nif (nr_disp >= throtl_quantum)\r\nbreak;\r\n}\r\nreturn nr_disp;\r\n}\r\nstatic void throtl_pending_timer_fn(unsigned long arg)\r\n{\r\nstruct throtl_service_queue *sq = (void *)arg;\r\nstruct throtl_grp *tg = sq_to_tg(sq);\r\nstruct throtl_data *td = sq_to_td(sq);\r\nstruct request_queue *q = td->queue;\r\nstruct throtl_service_queue *parent_sq;\r\nbool dispatched;\r\nint ret;\r\nspin_lock_irq(q->queue_lock);\r\nagain:\r\nparent_sq = sq->parent_sq;\r\ndispatched = false;\r\nwhile (true) {\r\nthrotl_log(sq, "dispatch nr_queued=%u read=%u write=%u",\r\nsq->nr_queued[READ] + sq->nr_queued[WRITE],\r\nsq->nr_queued[READ], sq->nr_queued[WRITE]);\r\nret = throtl_select_dispatch(sq);\r\nif (ret) {\r\nthrotl_log(sq, "bios disp=%u", ret);\r\ndispatched = true;\r\n}\r\nif (throtl_schedule_next_dispatch(sq, false))\r\nbreak;\r\nspin_unlock_irq(q->queue_lock);\r\ncpu_relax();\r\nspin_lock_irq(q->queue_lock);\r\n}\r\nif (!dispatched)\r\ngoto out_unlock;\r\nif (parent_sq) {\r\nif (tg->flags & THROTL_TG_WAS_EMPTY) {\r\ntg_update_disptime(tg);\r\nif (!throtl_schedule_next_dispatch(parent_sq, false)) {\r\nsq = parent_sq;\r\ntg = sq_to_tg(sq);\r\ngoto again;\r\n}\r\n}\r\n} else {\r\nqueue_work(kthrotld_workqueue, &td->dispatch_work);\r\n}\r\nout_unlock:\r\nspin_unlock_irq(q->queue_lock);\r\n}\r\nstatic void blk_throtl_dispatch_work_fn(struct work_struct *work)\r\n{\r\nstruct throtl_data *td = container_of(work, struct throtl_data,\r\ndispatch_work);\r\nstruct throtl_service_queue *td_sq = &td->service_queue;\r\nstruct request_queue *q = td->queue;\r\nstruct bio_list bio_list_on_stack;\r\nstruct bio *bio;\r\nstruct blk_plug plug;\r\nint rw;\r\nbio_list_init(&bio_list_on_stack);\r\nspin_lock_irq(q->queue_lock);\r\nfor (rw = READ; rw <= WRITE; rw++)\r\nwhile ((bio = throtl_pop_queued(&td_sq->queued[rw], NULL)))\r\nbio_list_add(&bio_list_on_stack, bio);\r\nspin_unlock_irq(q->queue_lock);\r\nif (!bio_list_empty(&bio_list_on_stack)) {\r\nblk_start_plug(&plug);\r\nwhile((bio = bio_list_pop(&bio_list_on_stack)))\r\ngeneric_make_request(bio);\r\nblk_finish_plug(&plug);\r\n}\r\n}\r\nstatic u64 tg_prfill_cpu_rwstat(struct seq_file *sf,\r\nstruct blkg_policy_data *pd, int off)\r\n{\r\nstruct throtl_grp *tg = pd_to_tg(pd);\r\nstruct blkg_rwstat rwstat = { }, tmp;\r\nint i, cpu;\r\nif (tg->stats_cpu == NULL)\r\nreturn 0;\r\nfor_each_possible_cpu(cpu) {\r\nstruct tg_stats_cpu *sc = per_cpu_ptr(tg->stats_cpu, cpu);\r\ntmp = blkg_rwstat_read((void *)sc + off);\r\nfor (i = 0; i < BLKG_RWSTAT_NR; i++)\r\nrwstat.cnt[i] += tmp.cnt[i];\r\n}\r\nreturn __blkg_prfill_rwstat(sf, pd, &rwstat);\r\n}\r\nstatic int tg_print_cpu_rwstat(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)), tg_prfill_cpu_rwstat,\r\n&blkcg_policy_throtl, seq_cft(sf)->private, true);\r\nreturn 0;\r\n}\r\nstatic u64 tg_prfill_conf_u64(struct seq_file *sf, struct blkg_policy_data *pd,\r\nint off)\r\n{\r\nstruct throtl_grp *tg = pd_to_tg(pd);\r\nu64 v = *(u64 *)((void *)tg + off);\r\nif (v == -1)\r\nreturn 0;\r\nreturn __blkg_prfill_u64(sf, pd, v);\r\n}\r\nstatic u64 tg_prfill_conf_uint(struct seq_file *sf, struct blkg_policy_data *pd,\r\nint off)\r\n{\r\nstruct throtl_grp *tg = pd_to_tg(pd);\r\nunsigned int v = *(unsigned int *)((void *)tg + off);\r\nif (v == -1)\r\nreturn 0;\r\nreturn __blkg_prfill_u64(sf, pd, v);\r\n}\r\nstatic int tg_print_conf_u64(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)), tg_prfill_conf_u64,\r\n&blkcg_policy_throtl, seq_cft(sf)->private, false);\r\nreturn 0;\r\n}\r\nstatic int tg_print_conf_uint(struct seq_file *sf, void *v)\r\n{\r\nblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)), tg_prfill_conf_uint,\r\n&blkcg_policy_throtl, seq_cft(sf)->private, false);\r\nreturn 0;\r\n}\r\nstatic ssize_t tg_set_conf(struct kernfs_open_file *of,\r\nchar *buf, size_t nbytes, loff_t off, bool is_u64)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(of_css(of));\r\nstruct blkg_conf_ctx ctx;\r\nstruct throtl_grp *tg;\r\nstruct throtl_service_queue *sq;\r\nstruct blkcg_gq *blkg;\r\nstruct cgroup_subsys_state *pos_css;\r\nint ret;\r\nret = blkg_conf_prep(blkcg, &blkcg_policy_throtl, buf, &ctx);\r\nif (ret)\r\nreturn ret;\r\ntg = blkg_to_tg(ctx.blkg);\r\nsq = &tg->service_queue;\r\nif (!ctx.v)\r\nctx.v = -1;\r\nif (is_u64)\r\n*(u64 *)((void *)tg + of_cft(of)->private) = ctx.v;\r\nelse\r\n*(unsigned int *)((void *)tg + of_cft(of)->private) = ctx.v;\r\nthrotl_log(&tg->service_queue,\r\n"limit change rbps=%llu wbps=%llu riops=%u wiops=%u",\r\ntg->bps[READ], tg->bps[WRITE],\r\ntg->iops[READ], tg->iops[WRITE]);\r\nblkg_for_each_descendant_pre(blkg, pos_css, ctx.blkg)\r\ntg_update_has_rules(blkg_to_tg(blkg));\r\nthrotl_start_new_slice(tg, 0);\r\nthrotl_start_new_slice(tg, 1);\r\nif (tg->flags & THROTL_TG_PENDING) {\r\ntg_update_disptime(tg);\r\nthrotl_schedule_next_dispatch(sq->parent_sq, true);\r\n}\r\nblkg_conf_finish(&ctx);\r\nreturn nbytes;\r\n}\r\nstatic ssize_t tg_set_conf_u64(struct kernfs_open_file *of,\r\nchar *buf, size_t nbytes, loff_t off)\r\n{\r\nreturn tg_set_conf(of, buf, nbytes, off, true);\r\n}\r\nstatic ssize_t tg_set_conf_uint(struct kernfs_open_file *of,\r\nchar *buf, size_t nbytes, loff_t off)\r\n{\r\nreturn tg_set_conf(of, buf, nbytes, off, false);\r\n}\r\nstatic void throtl_shutdown_wq(struct request_queue *q)\r\n{\r\nstruct throtl_data *td = q->td;\r\ncancel_work_sync(&td->dispatch_work);\r\n}\r\nbool blk_throtl_bio(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct throtl_data *td = q->td;\r\nstruct throtl_qnode *qn = NULL;\r\nstruct throtl_grp *tg;\r\nstruct throtl_service_queue *sq;\r\nbool rw = bio_data_dir(bio);\r\nstruct blkcg *blkcg;\r\nbool throttled = false;\r\nif (bio->bi_rw & REQ_THROTTLED)\r\ngoto out;\r\nrcu_read_lock();\r\nblkcg = bio_blkcg(bio);\r\ntg = throtl_lookup_tg(td, blkcg);\r\nif (tg) {\r\nif (!tg->has_rules[rw]) {\r\nthrotl_update_dispatch_stats(tg_to_blkg(tg),\r\nbio->bi_iter.bi_size, bio->bi_rw);\r\ngoto out_unlock_rcu;\r\n}\r\n}\r\nspin_lock_irq(q->queue_lock);\r\ntg = throtl_lookup_create_tg(td, blkcg);\r\nif (unlikely(!tg))\r\ngoto out_unlock;\r\nsq = &tg->service_queue;\r\nwhile (true) {\r\nif (sq->nr_queued[rw])\r\nbreak;\r\nif (!tg_may_dispatch(tg, bio, NULL))\r\nbreak;\r\nthrotl_charge_bio(tg, bio);\r\nthrotl_trim_slice(tg, rw);\r\nqn = &tg->qnode_on_parent[rw];\r\nsq = sq->parent_sq;\r\ntg = sq_to_tg(sq);\r\nif (!tg)\r\ngoto out_unlock;\r\n}\r\nthrotl_log(sq, "[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d",\r\nrw == READ ? 'R' : 'W',\r\ntg->bytes_disp[rw], bio->bi_iter.bi_size, tg->bps[rw],\r\ntg->io_disp[rw], tg->iops[rw],\r\nsq->nr_queued[READ], sq->nr_queued[WRITE]);\r\nbio_associate_current(bio);\r\ntg->td->nr_queued[rw]++;\r\nthrotl_add_bio_tg(bio, qn, tg);\r\nthrottled = true;\r\nif (tg->flags & THROTL_TG_WAS_EMPTY) {\r\ntg_update_disptime(tg);\r\nthrotl_schedule_next_dispatch(tg->service_queue.parent_sq, true);\r\n}\r\nout_unlock:\r\nspin_unlock_irq(q->queue_lock);\r\nout_unlock_rcu:\r\nrcu_read_unlock();\r\nout:\r\nif (!throttled)\r\nbio->bi_rw &= ~REQ_THROTTLED;\r\nreturn throttled;\r\n}\r\nstatic void tg_drain_bios(struct throtl_service_queue *parent_sq)\r\n{\r\nstruct throtl_grp *tg;\r\nwhile ((tg = throtl_rb_first(parent_sq))) {\r\nstruct throtl_service_queue *sq = &tg->service_queue;\r\nstruct bio *bio;\r\nthrotl_dequeue_tg(tg);\r\nwhile ((bio = throtl_peek_queued(&sq->queued[READ])))\r\ntg_dispatch_one_bio(tg, bio_data_dir(bio));\r\nwhile ((bio = throtl_peek_queued(&sq->queued[WRITE])))\r\ntg_dispatch_one_bio(tg, bio_data_dir(bio));\r\n}\r\n}\r\nvoid blk_throtl_drain(struct request_queue *q)\r\n__releases(q->queue_lock) __acquires(q->queue_lock)\r\n{\r\nstruct throtl_data *td = q->td;\r\nstruct blkcg_gq *blkg;\r\nstruct cgroup_subsys_state *pos_css;\r\nstruct bio *bio;\r\nint rw;\r\nqueue_lockdep_assert_held(q);\r\nrcu_read_lock();\r\nblkg_for_each_descendant_post(blkg, pos_css, td->queue->root_blkg)\r\ntg_drain_bios(&blkg_to_tg(blkg)->service_queue);\r\ntg_drain_bios(&td->service_queue);\r\nrcu_read_unlock();\r\nspin_unlock_irq(q->queue_lock);\r\nfor (rw = READ; rw <= WRITE; rw++)\r\nwhile ((bio = throtl_pop_queued(&td->service_queue.queued[rw],\r\nNULL)))\r\ngeneric_make_request(bio);\r\nspin_lock_irq(q->queue_lock);\r\n}\r\nint blk_throtl_init(struct request_queue *q)\r\n{\r\nstruct throtl_data *td;\r\nint ret;\r\ntd = kzalloc_node(sizeof(*td), GFP_KERNEL, q->node);\r\nif (!td)\r\nreturn -ENOMEM;\r\nINIT_WORK(&td->dispatch_work, blk_throtl_dispatch_work_fn);\r\nthrotl_service_queue_init(&td->service_queue, NULL);\r\nq->td = td;\r\ntd->queue = q;\r\nret = blkcg_activate_policy(q, &blkcg_policy_throtl);\r\nif (ret)\r\nkfree(td);\r\nreturn ret;\r\n}\r\nvoid blk_throtl_exit(struct request_queue *q)\r\n{\r\nBUG_ON(!q->td);\r\nthrotl_shutdown_wq(q);\r\nblkcg_deactivate_policy(q, &blkcg_policy_throtl);\r\nkfree(q->td);\r\n}\r\nstatic int __init throtl_init(void)\r\n{\r\nkthrotld_workqueue = alloc_workqueue("kthrotld", WQ_MEM_RECLAIM, 0);\r\nif (!kthrotld_workqueue)\r\npanic("Failed to create kthrotld\n");\r\nreturn blkcg_policy_register(&blkcg_policy_throtl);\r\n}
