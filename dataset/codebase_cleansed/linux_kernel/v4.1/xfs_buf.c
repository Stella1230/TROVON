static inline int\r\nxfs_buf_is_vmapped(\r\nstruct xfs_buf *bp)\r\n{\r\nreturn bp->b_addr && bp->b_page_count > 1;\r\n}\r\nstatic inline int\r\nxfs_buf_vmap_len(\r\nstruct xfs_buf *bp)\r\n{\r\nreturn (bp->b_page_count * PAGE_SIZE) - bp->b_offset;\r\n}\r\nvoid\r\nxfs_buf_stale(\r\nstruct xfs_buf *bp)\r\n{\r\nASSERT(xfs_buf_islocked(bp));\r\nbp->b_flags |= XBF_STALE;\r\nbp->b_flags &= ~_XBF_DELWRI_Q;\r\nspin_lock(&bp->b_lock);\r\natomic_set(&bp->b_lru_ref, 0);\r\nif (!(bp->b_state & XFS_BSTATE_DISPOSE) &&\r\n(list_lru_del(&bp->b_target->bt_lru, &bp->b_lru)))\r\natomic_dec(&bp->b_hold);\r\nASSERT(atomic_read(&bp->b_hold) >= 1);\r\nspin_unlock(&bp->b_lock);\r\n}\r\nstatic int\r\nxfs_buf_get_maps(\r\nstruct xfs_buf *bp,\r\nint map_count)\r\n{\r\nASSERT(bp->b_maps == NULL);\r\nbp->b_map_count = map_count;\r\nif (map_count == 1) {\r\nbp->b_maps = &bp->__b_map;\r\nreturn 0;\r\n}\r\nbp->b_maps = kmem_zalloc(map_count * sizeof(struct xfs_buf_map),\r\nKM_NOFS);\r\nif (!bp->b_maps)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void\r\nxfs_buf_free_maps(\r\nstruct xfs_buf *bp)\r\n{\r\nif (bp->b_maps != &bp->__b_map) {\r\nkmem_free(bp->b_maps);\r\nbp->b_maps = NULL;\r\n}\r\n}\r\nstruct xfs_buf *\r\n_xfs_buf_alloc(\r\nstruct xfs_buftarg *target,\r\nstruct xfs_buf_map *map,\r\nint nmaps,\r\nxfs_buf_flags_t flags)\r\n{\r\nstruct xfs_buf *bp;\r\nint error;\r\nint i;\r\nbp = kmem_zone_zalloc(xfs_buf_zone, KM_NOFS);\r\nif (unlikely(!bp))\r\nreturn NULL;\r\nflags &= ~(XBF_UNMAPPED | XBF_TRYLOCK | XBF_ASYNC | XBF_READ_AHEAD);\r\natomic_set(&bp->b_hold, 1);\r\natomic_set(&bp->b_lru_ref, 1);\r\ninit_completion(&bp->b_iowait);\r\nINIT_LIST_HEAD(&bp->b_lru);\r\nINIT_LIST_HEAD(&bp->b_list);\r\nRB_CLEAR_NODE(&bp->b_rbnode);\r\nsema_init(&bp->b_sema, 0);\r\nspin_lock_init(&bp->b_lock);\r\nXB_SET_OWNER(bp);\r\nbp->b_target = target;\r\nbp->b_flags = flags;\r\nerror = xfs_buf_get_maps(bp, nmaps);\r\nif (error) {\r\nkmem_zone_free(xfs_buf_zone, bp);\r\nreturn NULL;\r\n}\r\nbp->b_bn = map[0].bm_bn;\r\nbp->b_length = 0;\r\nfor (i = 0; i < nmaps; i++) {\r\nbp->b_maps[i].bm_bn = map[i].bm_bn;\r\nbp->b_maps[i].bm_len = map[i].bm_len;\r\nbp->b_length += map[i].bm_len;\r\n}\r\nbp->b_io_length = bp->b_length;\r\natomic_set(&bp->b_pin_count, 0);\r\ninit_waitqueue_head(&bp->b_waiters);\r\nXFS_STATS_INC(xb_create);\r\ntrace_xfs_buf_init(bp, _RET_IP_);\r\nreturn bp;\r\n}\r\nSTATIC int\r\n_xfs_buf_get_pages(\r\nxfs_buf_t *bp,\r\nint page_count)\r\n{\r\nif (bp->b_pages == NULL) {\r\nbp->b_page_count = page_count;\r\nif (page_count <= XB_PAGES) {\r\nbp->b_pages = bp->b_page_array;\r\n} else {\r\nbp->b_pages = kmem_alloc(sizeof(struct page *) *\r\npage_count, KM_NOFS);\r\nif (bp->b_pages == NULL)\r\nreturn -ENOMEM;\r\n}\r\nmemset(bp->b_pages, 0, sizeof(struct page *) * page_count);\r\n}\r\nreturn 0;\r\n}\r\nSTATIC void\r\n_xfs_buf_free_pages(\r\nxfs_buf_t *bp)\r\n{\r\nif (bp->b_pages != bp->b_page_array) {\r\nkmem_free(bp->b_pages);\r\nbp->b_pages = NULL;\r\n}\r\n}\r\nvoid\r\nxfs_buf_free(\r\nxfs_buf_t *bp)\r\n{\r\ntrace_xfs_buf_free(bp, _RET_IP_);\r\nASSERT(list_empty(&bp->b_lru));\r\nif (bp->b_flags & _XBF_PAGES) {\r\nuint i;\r\nif (xfs_buf_is_vmapped(bp))\r\nvm_unmap_ram(bp->b_addr - bp->b_offset,\r\nbp->b_page_count);\r\nfor (i = 0; i < bp->b_page_count; i++) {\r\nstruct page *page = bp->b_pages[i];\r\n__free_page(page);\r\n}\r\n} else if (bp->b_flags & _XBF_KMEM)\r\nkmem_free(bp->b_addr);\r\n_xfs_buf_free_pages(bp);\r\nxfs_buf_free_maps(bp);\r\nkmem_zone_free(xfs_buf_zone, bp);\r\n}\r\nSTATIC int\r\nxfs_buf_allocate_memory(\r\nxfs_buf_t *bp,\r\nuint flags)\r\n{\r\nsize_t size;\r\nsize_t nbytes, offset;\r\ngfp_t gfp_mask = xb_to_gfp(flags);\r\nunsigned short page_count, i;\r\nxfs_off_t start, end;\r\nint error;\r\nsize = BBTOB(bp->b_length);\r\nif (size < PAGE_SIZE) {\r\nbp->b_addr = kmem_alloc(size, KM_NOFS);\r\nif (!bp->b_addr) {\r\ngoto use_alloc_page;\r\n}\r\nif (((unsigned long)(bp->b_addr + size - 1) & PAGE_MASK) !=\r\n((unsigned long)bp->b_addr & PAGE_MASK)) {\r\nkmem_free(bp->b_addr);\r\nbp->b_addr = NULL;\r\ngoto use_alloc_page;\r\n}\r\nbp->b_offset = offset_in_page(bp->b_addr);\r\nbp->b_pages = bp->b_page_array;\r\nbp->b_pages[0] = virt_to_page(bp->b_addr);\r\nbp->b_page_count = 1;\r\nbp->b_flags |= _XBF_KMEM;\r\nreturn 0;\r\n}\r\nuse_alloc_page:\r\nstart = BBTOB(bp->b_maps[0].bm_bn) >> PAGE_SHIFT;\r\nend = (BBTOB(bp->b_maps[0].bm_bn + bp->b_length) + PAGE_SIZE - 1)\r\n>> PAGE_SHIFT;\r\npage_count = end - start;\r\nerror = _xfs_buf_get_pages(bp, page_count);\r\nif (unlikely(error))\r\nreturn error;\r\noffset = bp->b_offset;\r\nbp->b_flags |= _XBF_PAGES;\r\nfor (i = 0; i < bp->b_page_count; i++) {\r\nstruct page *page;\r\nuint retries = 0;\r\nretry:\r\npage = alloc_page(gfp_mask);\r\nif (unlikely(page == NULL)) {\r\nif (flags & XBF_READ_AHEAD) {\r\nbp->b_page_count = i;\r\nerror = -ENOMEM;\r\ngoto out_free_pages;\r\n}\r\nif (!(++retries % 100))\r\nxfs_err(NULL,\r\n"possible memory allocation deadlock in %s (mode:0x%x)",\r\n__func__, gfp_mask);\r\nXFS_STATS_INC(xb_page_retries);\r\ncongestion_wait(BLK_RW_ASYNC, HZ/50);\r\ngoto retry;\r\n}\r\nXFS_STATS_INC(xb_page_found);\r\nnbytes = min_t(size_t, size, PAGE_SIZE - offset);\r\nsize -= nbytes;\r\nbp->b_pages[i] = page;\r\noffset = 0;\r\n}\r\nreturn 0;\r\nout_free_pages:\r\nfor (i = 0; i < bp->b_page_count; i++)\r\n__free_page(bp->b_pages[i]);\r\nreturn error;\r\n}\r\nSTATIC int\r\n_xfs_buf_map_pages(\r\nxfs_buf_t *bp,\r\nuint flags)\r\n{\r\nASSERT(bp->b_flags & _XBF_PAGES);\r\nif (bp->b_page_count == 1) {\r\nbp->b_addr = page_address(bp->b_pages[0]) + bp->b_offset;\r\n} else if (flags & XBF_UNMAPPED) {\r\nbp->b_addr = NULL;\r\n} else {\r\nint retried = 0;\r\nunsigned noio_flag;\r\nnoio_flag = memalloc_noio_save();\r\ndo {\r\nbp->b_addr = vm_map_ram(bp->b_pages, bp->b_page_count,\r\n-1, PAGE_KERNEL);\r\nif (bp->b_addr)\r\nbreak;\r\nvm_unmap_aliases();\r\n} while (retried++ <= 1);\r\nmemalloc_noio_restore(noio_flag);\r\nif (!bp->b_addr)\r\nreturn -ENOMEM;\r\nbp->b_addr += bp->b_offset;\r\n}\r\nreturn 0;\r\n}\r\nxfs_buf_t *\r\n_xfs_buf_find(\r\nstruct xfs_buftarg *btp,\r\nstruct xfs_buf_map *map,\r\nint nmaps,\r\nxfs_buf_flags_t flags,\r\nxfs_buf_t *new_bp)\r\n{\r\nsize_t numbytes;\r\nstruct xfs_perag *pag;\r\nstruct rb_node **rbp;\r\nstruct rb_node *parent;\r\nxfs_buf_t *bp;\r\nxfs_daddr_t blkno = map[0].bm_bn;\r\nxfs_daddr_t eofs;\r\nint numblks = 0;\r\nint i;\r\nfor (i = 0; i < nmaps; i++)\r\nnumblks += map[i].bm_len;\r\nnumbytes = BBTOB(numblks);\r\nASSERT(!(numbytes < btp->bt_meta_sectorsize));\r\nASSERT(!(BBTOB(blkno) & (xfs_off_t)btp->bt_meta_sectormask));\r\neofs = XFS_FSB_TO_BB(btp->bt_mount, btp->bt_mount->m_sb.sb_dblocks);\r\nif (blkno < 0 || blkno >= eofs) {\r\nxfs_alert(btp->bt_mount,\r\n"%s: Block out of range: block 0x%llx, EOFS 0x%llx ",\r\n__func__, blkno, eofs);\r\nWARN_ON(1);\r\nreturn NULL;\r\n}\r\npag = xfs_perag_get(btp->bt_mount,\r\nxfs_daddr_to_agno(btp->bt_mount, blkno));\r\nspin_lock(&pag->pag_buf_lock);\r\nrbp = &pag->pag_buf_tree.rb_node;\r\nparent = NULL;\r\nbp = NULL;\r\nwhile (*rbp) {\r\nparent = *rbp;\r\nbp = rb_entry(parent, struct xfs_buf, b_rbnode);\r\nif (blkno < bp->b_bn)\r\nrbp = &(*rbp)->rb_left;\r\nelse if (blkno > bp->b_bn)\r\nrbp = &(*rbp)->rb_right;\r\nelse {\r\nif (bp->b_length != numblks) {\r\nASSERT(bp->b_flags & XBF_STALE);\r\nrbp = &(*rbp)->rb_right;\r\ncontinue;\r\n}\r\natomic_inc(&bp->b_hold);\r\ngoto found;\r\n}\r\n}\r\nif (new_bp) {\r\nrb_link_node(&new_bp->b_rbnode, parent, rbp);\r\nrb_insert_color(&new_bp->b_rbnode, &pag->pag_buf_tree);\r\nnew_bp->b_pag = pag;\r\nspin_unlock(&pag->pag_buf_lock);\r\n} else {\r\nXFS_STATS_INC(xb_miss_locked);\r\nspin_unlock(&pag->pag_buf_lock);\r\nxfs_perag_put(pag);\r\n}\r\nreturn new_bp;\r\nfound:\r\nspin_unlock(&pag->pag_buf_lock);\r\nxfs_perag_put(pag);\r\nif (!xfs_buf_trylock(bp)) {\r\nif (flags & XBF_TRYLOCK) {\r\nxfs_buf_rele(bp);\r\nXFS_STATS_INC(xb_busy_locked);\r\nreturn NULL;\r\n}\r\nxfs_buf_lock(bp);\r\nXFS_STATS_INC(xb_get_locked_waited);\r\n}\r\nif (bp->b_flags & XBF_STALE) {\r\nASSERT((bp->b_flags & _XBF_DELWRI_Q) == 0);\r\nASSERT(bp->b_iodone == NULL);\r\nbp->b_flags &= _XBF_KMEM | _XBF_PAGES;\r\nbp->b_ops = NULL;\r\n}\r\ntrace_xfs_buf_find(bp, flags, _RET_IP_);\r\nXFS_STATS_INC(xb_get_locked);\r\nreturn bp;\r\n}\r\nstruct xfs_buf *\r\nxfs_buf_get_map(\r\nstruct xfs_buftarg *target,\r\nstruct xfs_buf_map *map,\r\nint nmaps,\r\nxfs_buf_flags_t flags)\r\n{\r\nstruct xfs_buf *bp;\r\nstruct xfs_buf *new_bp;\r\nint error = 0;\r\nbp = _xfs_buf_find(target, map, nmaps, flags, NULL);\r\nif (likely(bp))\r\ngoto found;\r\nnew_bp = _xfs_buf_alloc(target, map, nmaps, flags);\r\nif (unlikely(!new_bp))\r\nreturn NULL;\r\nerror = xfs_buf_allocate_memory(new_bp, flags);\r\nif (error) {\r\nxfs_buf_free(new_bp);\r\nreturn NULL;\r\n}\r\nbp = _xfs_buf_find(target, map, nmaps, flags, new_bp);\r\nif (!bp) {\r\nxfs_buf_free(new_bp);\r\nreturn NULL;\r\n}\r\nif (bp != new_bp)\r\nxfs_buf_free(new_bp);\r\nfound:\r\nif (!bp->b_addr) {\r\nerror = _xfs_buf_map_pages(bp, flags);\r\nif (unlikely(error)) {\r\nxfs_warn(target->bt_mount,\r\n"%s: failed to map pagesn", __func__);\r\nxfs_buf_relse(bp);\r\nreturn NULL;\r\n}\r\n}\r\nXFS_STATS_INC(xb_get);\r\ntrace_xfs_buf_get(bp, flags, _RET_IP_);\r\nreturn bp;\r\n}\r\nSTATIC int\r\n_xfs_buf_read(\r\nxfs_buf_t *bp,\r\nxfs_buf_flags_t flags)\r\n{\r\nASSERT(!(flags & XBF_WRITE));\r\nASSERT(bp->b_maps[0].bm_bn != XFS_BUF_DADDR_NULL);\r\nbp->b_flags &= ~(XBF_WRITE | XBF_ASYNC | XBF_READ_AHEAD);\r\nbp->b_flags |= flags & (XBF_READ | XBF_ASYNC | XBF_READ_AHEAD);\r\nif (flags & XBF_ASYNC) {\r\nxfs_buf_submit(bp);\r\nreturn 0;\r\n}\r\nreturn xfs_buf_submit_wait(bp);\r\n}\r\nxfs_buf_t *\r\nxfs_buf_read_map(\r\nstruct xfs_buftarg *target,\r\nstruct xfs_buf_map *map,\r\nint nmaps,\r\nxfs_buf_flags_t flags,\r\nconst struct xfs_buf_ops *ops)\r\n{\r\nstruct xfs_buf *bp;\r\nflags |= XBF_READ;\r\nbp = xfs_buf_get_map(target, map, nmaps, flags);\r\nif (bp) {\r\ntrace_xfs_buf_read(bp, flags, _RET_IP_);\r\nif (!XFS_BUF_ISDONE(bp)) {\r\nXFS_STATS_INC(xb_get_read);\r\nbp->b_ops = ops;\r\n_xfs_buf_read(bp, flags);\r\n} else if (flags & XBF_ASYNC) {\r\nxfs_buf_relse(bp);\r\nreturn NULL;\r\n} else {\r\nbp->b_flags &= ~XBF_READ;\r\n}\r\n}\r\nreturn bp;\r\n}\r\nvoid\r\nxfs_buf_readahead_map(\r\nstruct xfs_buftarg *target,\r\nstruct xfs_buf_map *map,\r\nint nmaps,\r\nconst struct xfs_buf_ops *ops)\r\n{\r\nif (bdi_read_congested(target->bt_bdi))\r\nreturn;\r\nxfs_buf_read_map(target, map, nmaps,\r\nXBF_TRYLOCK|XBF_ASYNC|XBF_READ_AHEAD, ops);\r\n}\r\nint\r\nxfs_buf_read_uncached(\r\nstruct xfs_buftarg *target,\r\nxfs_daddr_t daddr,\r\nsize_t numblks,\r\nint flags,\r\nstruct xfs_buf **bpp,\r\nconst struct xfs_buf_ops *ops)\r\n{\r\nstruct xfs_buf *bp;\r\n*bpp = NULL;\r\nbp = xfs_buf_get_uncached(target, numblks, flags);\r\nif (!bp)\r\nreturn -ENOMEM;\r\nASSERT(bp->b_map_count == 1);\r\nbp->b_bn = XFS_BUF_DADDR_NULL;\r\nbp->b_maps[0].bm_bn = daddr;\r\nbp->b_flags |= XBF_READ;\r\nbp->b_ops = ops;\r\nxfs_buf_submit_wait(bp);\r\nif (bp->b_error) {\r\nint error = bp->b_error;\r\nxfs_buf_relse(bp);\r\nreturn error;\r\n}\r\n*bpp = bp;\r\nreturn 0;\r\n}\r\nvoid\r\nxfs_buf_set_empty(\r\nstruct xfs_buf *bp,\r\nsize_t numblks)\r\n{\r\nif (bp->b_pages)\r\n_xfs_buf_free_pages(bp);\r\nbp->b_pages = NULL;\r\nbp->b_page_count = 0;\r\nbp->b_addr = NULL;\r\nbp->b_length = numblks;\r\nbp->b_io_length = numblks;\r\nASSERT(bp->b_map_count == 1);\r\nbp->b_bn = XFS_BUF_DADDR_NULL;\r\nbp->b_maps[0].bm_bn = XFS_BUF_DADDR_NULL;\r\nbp->b_maps[0].bm_len = bp->b_length;\r\n}\r\nstatic inline struct page *\r\nmem_to_page(\r\nvoid *addr)\r\n{\r\nif ((!is_vmalloc_addr(addr))) {\r\nreturn virt_to_page(addr);\r\n} else {\r\nreturn vmalloc_to_page(addr);\r\n}\r\n}\r\nint\r\nxfs_buf_associate_memory(\r\nxfs_buf_t *bp,\r\nvoid *mem,\r\nsize_t len)\r\n{\r\nint rval;\r\nint i = 0;\r\nunsigned long pageaddr;\r\nunsigned long offset;\r\nsize_t buflen;\r\nint page_count;\r\npageaddr = (unsigned long)mem & PAGE_MASK;\r\noffset = (unsigned long)mem - pageaddr;\r\nbuflen = PAGE_ALIGN(len + offset);\r\npage_count = buflen >> PAGE_SHIFT;\r\nif (bp->b_pages)\r\n_xfs_buf_free_pages(bp);\r\nbp->b_pages = NULL;\r\nbp->b_addr = mem;\r\nrval = _xfs_buf_get_pages(bp, page_count);\r\nif (rval)\r\nreturn rval;\r\nbp->b_offset = offset;\r\nfor (i = 0; i < bp->b_page_count; i++) {\r\nbp->b_pages[i] = mem_to_page((void *)pageaddr);\r\npageaddr += PAGE_SIZE;\r\n}\r\nbp->b_io_length = BTOBB(len);\r\nbp->b_length = BTOBB(buflen);\r\nreturn 0;\r\n}\r\nxfs_buf_t *\r\nxfs_buf_get_uncached(\r\nstruct xfs_buftarg *target,\r\nsize_t numblks,\r\nint flags)\r\n{\r\nunsigned long page_count;\r\nint error, i;\r\nstruct xfs_buf *bp;\r\nDEFINE_SINGLE_BUF_MAP(map, XFS_BUF_DADDR_NULL, numblks);\r\nbp = _xfs_buf_alloc(target, &map, 1, 0);\r\nif (unlikely(bp == NULL))\r\ngoto fail;\r\npage_count = PAGE_ALIGN(numblks << BBSHIFT) >> PAGE_SHIFT;\r\nerror = _xfs_buf_get_pages(bp, page_count);\r\nif (error)\r\ngoto fail_free_buf;\r\nfor (i = 0; i < page_count; i++) {\r\nbp->b_pages[i] = alloc_page(xb_to_gfp(flags));\r\nif (!bp->b_pages[i])\r\ngoto fail_free_mem;\r\n}\r\nbp->b_flags |= _XBF_PAGES;\r\nerror = _xfs_buf_map_pages(bp, 0);\r\nif (unlikely(error)) {\r\nxfs_warn(target->bt_mount,\r\n"%s: failed to map pages", __func__);\r\ngoto fail_free_mem;\r\n}\r\ntrace_xfs_buf_get_uncached(bp, _RET_IP_);\r\nreturn bp;\r\nfail_free_mem:\r\nwhile (--i >= 0)\r\n__free_page(bp->b_pages[i]);\r\n_xfs_buf_free_pages(bp);\r\nfail_free_buf:\r\nxfs_buf_free_maps(bp);\r\nkmem_zone_free(xfs_buf_zone, bp);\r\nfail:\r\nreturn NULL;\r\n}\r\nvoid\r\nxfs_buf_hold(\r\nxfs_buf_t *bp)\r\n{\r\ntrace_xfs_buf_hold(bp, _RET_IP_);\r\natomic_inc(&bp->b_hold);\r\n}\r\nvoid\r\nxfs_buf_rele(\r\nxfs_buf_t *bp)\r\n{\r\nstruct xfs_perag *pag = bp->b_pag;\r\ntrace_xfs_buf_rele(bp, _RET_IP_);\r\nif (!pag) {\r\nASSERT(list_empty(&bp->b_lru));\r\nASSERT(RB_EMPTY_NODE(&bp->b_rbnode));\r\nif (atomic_dec_and_test(&bp->b_hold))\r\nxfs_buf_free(bp);\r\nreturn;\r\n}\r\nASSERT(!RB_EMPTY_NODE(&bp->b_rbnode));\r\nASSERT(atomic_read(&bp->b_hold) > 0);\r\nif (atomic_dec_and_lock(&bp->b_hold, &pag->pag_buf_lock)) {\r\nspin_lock(&bp->b_lock);\r\nif (!(bp->b_flags & XBF_STALE) && atomic_read(&bp->b_lru_ref)) {\r\nif (list_lru_add(&bp->b_target->bt_lru, &bp->b_lru)) {\r\nbp->b_state &= ~XFS_BSTATE_DISPOSE;\r\natomic_inc(&bp->b_hold);\r\n}\r\nspin_unlock(&bp->b_lock);\r\nspin_unlock(&pag->pag_buf_lock);\r\n} else {\r\nif (!(bp->b_state & XFS_BSTATE_DISPOSE)) {\r\nlist_lru_del(&bp->b_target->bt_lru, &bp->b_lru);\r\n} else {\r\nASSERT(list_empty(&bp->b_lru));\r\n}\r\nspin_unlock(&bp->b_lock);\r\nASSERT(!(bp->b_flags & _XBF_DELWRI_Q));\r\nrb_erase(&bp->b_rbnode, &pag->pag_buf_tree);\r\nspin_unlock(&pag->pag_buf_lock);\r\nxfs_perag_put(pag);\r\nxfs_buf_free(bp);\r\n}\r\n}\r\n}\r\nint\r\nxfs_buf_trylock(\r\nstruct xfs_buf *bp)\r\n{\r\nint locked;\r\nlocked = down_trylock(&bp->b_sema) == 0;\r\nif (locked)\r\nXB_SET_OWNER(bp);\r\ntrace_xfs_buf_trylock(bp, _RET_IP_);\r\nreturn locked;\r\n}\r\nvoid\r\nxfs_buf_lock(\r\nstruct xfs_buf *bp)\r\n{\r\ntrace_xfs_buf_lock(bp, _RET_IP_);\r\nif (atomic_read(&bp->b_pin_count) && (bp->b_flags & XBF_STALE))\r\nxfs_log_force(bp->b_target->bt_mount, 0);\r\ndown(&bp->b_sema);\r\nXB_SET_OWNER(bp);\r\ntrace_xfs_buf_lock_done(bp, _RET_IP_);\r\n}\r\nvoid\r\nxfs_buf_unlock(\r\nstruct xfs_buf *bp)\r\n{\r\nXB_CLEAR_OWNER(bp);\r\nup(&bp->b_sema);\r\ntrace_xfs_buf_unlock(bp, _RET_IP_);\r\n}\r\nSTATIC void\r\nxfs_buf_wait_unpin(\r\nxfs_buf_t *bp)\r\n{\r\nDECLARE_WAITQUEUE (wait, current);\r\nif (atomic_read(&bp->b_pin_count) == 0)\r\nreturn;\r\nadd_wait_queue(&bp->b_waiters, &wait);\r\nfor (;;) {\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nif (atomic_read(&bp->b_pin_count) == 0)\r\nbreak;\r\nio_schedule();\r\n}\r\nremove_wait_queue(&bp->b_waiters, &wait);\r\nset_current_state(TASK_RUNNING);\r\n}\r\nvoid\r\nxfs_buf_ioend(\r\nstruct xfs_buf *bp)\r\n{\r\nbool read = bp->b_flags & XBF_READ;\r\ntrace_xfs_buf_iodone(bp, _RET_IP_);\r\nbp->b_flags &= ~(XBF_READ | XBF_WRITE | XBF_READ_AHEAD);\r\nif (!bp->b_error && bp->b_io_error)\r\nxfs_buf_ioerror(bp, bp->b_io_error);\r\nif (read && !bp->b_error && bp->b_ops) {\r\nASSERT(!bp->b_iodone);\r\nbp->b_ops->verify_read(bp);\r\n}\r\nif (!bp->b_error)\r\nbp->b_flags |= XBF_DONE;\r\nif (bp->b_iodone)\r\n(*(bp->b_iodone))(bp);\r\nelse if (bp->b_flags & XBF_ASYNC)\r\nxfs_buf_relse(bp);\r\nelse\r\ncomplete(&bp->b_iowait);\r\n}\r\nstatic void\r\nxfs_buf_ioend_work(\r\nstruct work_struct *work)\r\n{\r\nstruct xfs_buf *bp =\r\ncontainer_of(work, xfs_buf_t, b_ioend_work);\r\nxfs_buf_ioend(bp);\r\n}\r\nvoid\r\nxfs_buf_ioend_async(\r\nstruct xfs_buf *bp)\r\n{\r\nINIT_WORK(&bp->b_ioend_work, xfs_buf_ioend_work);\r\nqueue_work(bp->b_ioend_wq, &bp->b_ioend_work);\r\n}\r\nvoid\r\nxfs_buf_ioerror(\r\nxfs_buf_t *bp,\r\nint error)\r\n{\r\nASSERT(error <= 0 && error >= -1000);\r\nbp->b_error = error;\r\ntrace_xfs_buf_ioerror(bp, error, _RET_IP_);\r\n}\r\nvoid\r\nxfs_buf_ioerror_alert(\r\nstruct xfs_buf *bp,\r\nconst char *func)\r\n{\r\nxfs_alert(bp->b_target->bt_mount,\r\n"metadata I/O error: block 0x%llx (\"%s\") error %d numblks %d",\r\n(__uint64_t)XFS_BUF_ADDR(bp), func, -bp->b_error, bp->b_length);\r\n}\r\nint\r\nxfs_bwrite(\r\nstruct xfs_buf *bp)\r\n{\r\nint error;\r\nASSERT(xfs_buf_islocked(bp));\r\nbp->b_flags |= XBF_WRITE;\r\nbp->b_flags &= ~(XBF_ASYNC | XBF_READ | _XBF_DELWRI_Q |\r\nXBF_WRITE_FAIL | XBF_DONE);\r\nerror = xfs_buf_submit_wait(bp);\r\nif (error) {\r\nxfs_force_shutdown(bp->b_target->bt_mount,\r\nSHUTDOWN_META_IO_ERROR);\r\n}\r\nreturn error;\r\n}\r\nSTATIC void\r\nxfs_buf_bio_end_io(\r\nstruct bio *bio,\r\nint error)\r\n{\r\nxfs_buf_t *bp = (xfs_buf_t *)bio->bi_private;\r\nif (error) {\r\nspin_lock(&bp->b_lock);\r\nif (!bp->b_io_error)\r\nbp->b_io_error = error;\r\nspin_unlock(&bp->b_lock);\r\n}\r\nif (!bp->b_error && xfs_buf_is_vmapped(bp) && (bp->b_flags & XBF_READ))\r\ninvalidate_kernel_vmap_range(bp->b_addr, xfs_buf_vmap_len(bp));\r\nif (atomic_dec_and_test(&bp->b_io_remaining) == 1)\r\nxfs_buf_ioend_async(bp);\r\nbio_put(bio);\r\n}\r\nstatic void\r\nxfs_buf_ioapply_map(\r\nstruct xfs_buf *bp,\r\nint map,\r\nint *buf_offset,\r\nint *count,\r\nint rw)\r\n{\r\nint page_index;\r\nint total_nr_pages = bp->b_page_count;\r\nint nr_pages;\r\nstruct bio *bio;\r\nsector_t sector = bp->b_maps[map].bm_bn;\r\nint size;\r\nint offset;\r\ntotal_nr_pages = bp->b_page_count;\r\npage_index = 0;\r\noffset = *buf_offset;\r\nwhile (offset >= PAGE_SIZE) {\r\npage_index++;\r\noffset -= PAGE_SIZE;\r\n}\r\nsize = min_t(int, BBTOB(bp->b_maps[map].bm_len), *count);\r\n*count -= size;\r\n*buf_offset += size;\r\nnext_chunk:\r\natomic_inc(&bp->b_io_remaining);\r\nnr_pages = BIO_MAX_SECTORS >> (PAGE_SHIFT - BBSHIFT);\r\nif (nr_pages > total_nr_pages)\r\nnr_pages = total_nr_pages;\r\nbio = bio_alloc(GFP_NOIO, nr_pages);\r\nbio->bi_bdev = bp->b_target->bt_bdev;\r\nbio->bi_iter.bi_sector = sector;\r\nbio->bi_end_io = xfs_buf_bio_end_io;\r\nbio->bi_private = bp;\r\nfor (; size && nr_pages; nr_pages--, page_index++) {\r\nint rbytes, nbytes = PAGE_SIZE - offset;\r\nif (nbytes > size)\r\nnbytes = size;\r\nrbytes = bio_add_page(bio, bp->b_pages[page_index], nbytes,\r\noffset);\r\nif (rbytes < nbytes)\r\nbreak;\r\noffset = 0;\r\nsector += BTOBB(nbytes);\r\nsize -= nbytes;\r\ntotal_nr_pages--;\r\n}\r\nif (likely(bio->bi_iter.bi_size)) {\r\nif (xfs_buf_is_vmapped(bp)) {\r\nflush_kernel_vmap_range(bp->b_addr,\r\nxfs_buf_vmap_len(bp));\r\n}\r\nsubmit_bio(rw, bio);\r\nif (size)\r\ngoto next_chunk;\r\n} else {\r\natomic_dec(&bp->b_io_remaining);\r\nxfs_buf_ioerror(bp, -EIO);\r\nbio_put(bio);\r\n}\r\n}\r\nSTATIC void\r\n_xfs_buf_ioapply(\r\nstruct xfs_buf *bp)\r\n{\r\nstruct blk_plug plug;\r\nint rw;\r\nint offset;\r\nint size;\r\nint i;\r\nbp->b_error = 0;\r\nif (!bp->b_ioend_wq)\r\nbp->b_ioend_wq = bp->b_target->bt_mount->m_buf_workqueue;\r\nif (bp->b_flags & XBF_WRITE) {\r\nif (bp->b_flags & XBF_SYNCIO)\r\nrw = WRITE_SYNC;\r\nelse\r\nrw = WRITE;\r\nif (bp->b_flags & XBF_FUA)\r\nrw |= REQ_FUA;\r\nif (bp->b_flags & XBF_FLUSH)\r\nrw |= REQ_FLUSH;\r\nif (bp->b_ops) {\r\nbp->b_ops->verify_write(bp);\r\nif (bp->b_error) {\r\nxfs_force_shutdown(bp->b_target->bt_mount,\r\nSHUTDOWN_CORRUPT_INCORE);\r\nreturn;\r\n}\r\n} else if (bp->b_bn != XFS_BUF_DADDR_NULL) {\r\nstruct xfs_mount *mp = bp->b_target->bt_mount;\r\nif (xfs_sb_version_hascrc(&mp->m_sb)) {\r\nxfs_warn(mp,\r\n"%s: no ops on block 0x%llx/0x%x",\r\n__func__, bp->b_bn, bp->b_length);\r\nxfs_hex_dump(bp->b_addr, 64);\r\ndump_stack();\r\n}\r\n}\r\n} else if (bp->b_flags & XBF_READ_AHEAD) {\r\nrw = READA;\r\n} else {\r\nrw = READ;\r\n}\r\nrw |= REQ_META;\r\noffset = bp->b_offset;\r\nsize = BBTOB(bp->b_io_length);\r\nblk_start_plug(&plug);\r\nfor (i = 0; i < bp->b_map_count; i++) {\r\nxfs_buf_ioapply_map(bp, i, &offset, &size, rw);\r\nif (bp->b_error)\r\nbreak;\r\nif (size <= 0)\r\nbreak;\r\n}\r\nblk_finish_plug(&plug);\r\n}\r\nvoid\r\nxfs_buf_submit(\r\nstruct xfs_buf *bp)\r\n{\r\ntrace_xfs_buf_submit(bp, _RET_IP_);\r\nASSERT(!(bp->b_flags & _XBF_DELWRI_Q));\r\nASSERT(bp->b_flags & XBF_ASYNC);\r\nif (XFS_FORCED_SHUTDOWN(bp->b_target->bt_mount)) {\r\nxfs_buf_ioerror(bp, -EIO);\r\nbp->b_flags &= ~XBF_DONE;\r\nxfs_buf_stale(bp);\r\nxfs_buf_ioend(bp);\r\nreturn;\r\n}\r\nif (bp->b_flags & XBF_WRITE)\r\nxfs_buf_wait_unpin(bp);\r\nbp->b_io_error = 0;\r\nxfs_buf_hold(bp);\r\natomic_set(&bp->b_io_remaining, 1);\r\n_xfs_buf_ioapply(bp);\r\nif (atomic_dec_and_test(&bp->b_io_remaining) == 1) {\r\nif (bp->b_error)\r\nxfs_buf_ioend(bp);\r\nelse\r\nxfs_buf_ioend_async(bp);\r\n}\r\nxfs_buf_rele(bp);\r\n}\r\nint\r\nxfs_buf_submit_wait(\r\nstruct xfs_buf *bp)\r\n{\r\nint error;\r\ntrace_xfs_buf_submit_wait(bp, _RET_IP_);\r\nASSERT(!(bp->b_flags & (_XBF_DELWRI_Q | XBF_ASYNC)));\r\nif (XFS_FORCED_SHUTDOWN(bp->b_target->bt_mount)) {\r\nxfs_buf_ioerror(bp, -EIO);\r\nxfs_buf_stale(bp);\r\nbp->b_flags &= ~XBF_DONE;\r\nreturn -EIO;\r\n}\r\nif (bp->b_flags & XBF_WRITE)\r\nxfs_buf_wait_unpin(bp);\r\nbp->b_io_error = 0;\r\nxfs_buf_hold(bp);\r\natomic_set(&bp->b_io_remaining, 1);\r\n_xfs_buf_ioapply(bp);\r\nif (atomic_dec_and_test(&bp->b_io_remaining) == 1)\r\nxfs_buf_ioend(bp);\r\ntrace_xfs_buf_iowait(bp, _RET_IP_);\r\nwait_for_completion(&bp->b_iowait);\r\ntrace_xfs_buf_iowait_done(bp, _RET_IP_);\r\nerror = bp->b_error;\r\nxfs_buf_rele(bp);\r\nreturn error;\r\n}\r\nxfs_caddr_t\r\nxfs_buf_offset(\r\nxfs_buf_t *bp,\r\nsize_t offset)\r\n{\r\nstruct page *page;\r\nif (bp->b_addr)\r\nreturn bp->b_addr + offset;\r\noffset += bp->b_offset;\r\npage = bp->b_pages[offset >> PAGE_SHIFT];\r\nreturn (xfs_caddr_t)page_address(page) + (offset & (PAGE_SIZE-1));\r\n}\r\nvoid\r\nxfs_buf_iomove(\r\nxfs_buf_t *bp,\r\nsize_t boff,\r\nsize_t bsize,\r\nvoid *data,\r\nxfs_buf_rw_t mode)\r\n{\r\nsize_t bend;\r\nbend = boff + bsize;\r\nwhile (boff < bend) {\r\nstruct page *page;\r\nint page_index, page_offset, csize;\r\npage_index = (boff + bp->b_offset) >> PAGE_SHIFT;\r\npage_offset = (boff + bp->b_offset) & ~PAGE_MASK;\r\npage = bp->b_pages[page_index];\r\ncsize = min_t(size_t, PAGE_SIZE - page_offset,\r\nBBTOB(bp->b_io_length) - boff);\r\nASSERT((csize + page_offset) <= PAGE_SIZE);\r\nswitch (mode) {\r\ncase XBRW_ZERO:\r\nmemset(page_address(page) + page_offset, 0, csize);\r\nbreak;\r\ncase XBRW_READ:\r\nmemcpy(data, page_address(page) + page_offset, csize);\r\nbreak;\r\ncase XBRW_WRITE:\r\nmemcpy(page_address(page) + page_offset, data, csize);\r\n}\r\nboff += csize;\r\ndata += csize;\r\n}\r\n}\r\nstatic enum lru_status\r\nxfs_buftarg_wait_rele(\r\nstruct list_head *item,\r\nstruct list_lru_one *lru,\r\nspinlock_t *lru_lock,\r\nvoid *arg)\r\n{\r\nstruct xfs_buf *bp = container_of(item, struct xfs_buf, b_lru);\r\nstruct list_head *dispose = arg;\r\nif (atomic_read(&bp->b_hold) > 1) {\r\ntrace_xfs_buf_wait_buftarg(bp, _RET_IP_);\r\nreturn LRU_SKIP;\r\n}\r\nif (!spin_trylock(&bp->b_lock))\r\nreturn LRU_SKIP;\r\natomic_set(&bp->b_lru_ref, 0);\r\nbp->b_state |= XFS_BSTATE_DISPOSE;\r\nlist_lru_isolate_move(lru, item, dispose);\r\nspin_unlock(&bp->b_lock);\r\nreturn LRU_REMOVED;\r\n}\r\nvoid\r\nxfs_wait_buftarg(\r\nstruct xfs_buftarg *btp)\r\n{\r\nLIST_HEAD(dispose);\r\nint loop = 0;\r\nwhile (list_lru_count(&btp->bt_lru)) {\r\nlist_lru_walk(&btp->bt_lru, xfs_buftarg_wait_rele,\r\n&dispose, LONG_MAX);\r\nwhile (!list_empty(&dispose)) {\r\nstruct xfs_buf *bp;\r\nbp = list_first_entry(&dispose, struct xfs_buf, b_lru);\r\nlist_del_init(&bp->b_lru);\r\nif (bp->b_flags & XBF_WRITE_FAIL) {\r\nxfs_alert(btp->bt_mount,\r\n"Corruption Alert: Buffer at block 0x%llx had permanent write failures!\n"\r\n"Please run xfs_repair to determine the extent of the problem.",\r\n(long long)bp->b_bn);\r\n}\r\nxfs_buf_rele(bp);\r\n}\r\nif (loop++ != 0)\r\ndelay(100);\r\n}\r\n}\r\nstatic enum lru_status\r\nxfs_buftarg_isolate(\r\nstruct list_head *item,\r\nstruct list_lru_one *lru,\r\nspinlock_t *lru_lock,\r\nvoid *arg)\r\n{\r\nstruct xfs_buf *bp = container_of(item, struct xfs_buf, b_lru);\r\nstruct list_head *dispose = arg;\r\nif (!spin_trylock(&bp->b_lock))\r\nreturn LRU_SKIP;\r\nif (!atomic_add_unless(&bp->b_lru_ref, -1, 0)) {\r\nspin_unlock(&bp->b_lock);\r\nreturn LRU_ROTATE;\r\n}\r\nbp->b_state |= XFS_BSTATE_DISPOSE;\r\nlist_lru_isolate_move(lru, item, dispose);\r\nspin_unlock(&bp->b_lock);\r\nreturn LRU_REMOVED;\r\n}\r\nstatic unsigned long\r\nxfs_buftarg_shrink_scan(\r\nstruct shrinker *shrink,\r\nstruct shrink_control *sc)\r\n{\r\nstruct xfs_buftarg *btp = container_of(shrink,\r\nstruct xfs_buftarg, bt_shrinker);\r\nLIST_HEAD(dispose);\r\nunsigned long freed;\r\nfreed = list_lru_shrink_walk(&btp->bt_lru, sc,\r\nxfs_buftarg_isolate, &dispose);\r\nwhile (!list_empty(&dispose)) {\r\nstruct xfs_buf *bp;\r\nbp = list_first_entry(&dispose, struct xfs_buf, b_lru);\r\nlist_del_init(&bp->b_lru);\r\nxfs_buf_rele(bp);\r\n}\r\nreturn freed;\r\n}\r\nstatic unsigned long\r\nxfs_buftarg_shrink_count(\r\nstruct shrinker *shrink,\r\nstruct shrink_control *sc)\r\n{\r\nstruct xfs_buftarg *btp = container_of(shrink,\r\nstruct xfs_buftarg, bt_shrinker);\r\nreturn list_lru_shrink_count(&btp->bt_lru, sc);\r\n}\r\nvoid\r\nxfs_free_buftarg(\r\nstruct xfs_mount *mp,\r\nstruct xfs_buftarg *btp)\r\n{\r\nunregister_shrinker(&btp->bt_shrinker);\r\nlist_lru_destroy(&btp->bt_lru);\r\nif (mp->m_flags & XFS_MOUNT_BARRIER)\r\nxfs_blkdev_issue_flush(btp);\r\nkmem_free(btp);\r\n}\r\nint\r\nxfs_setsize_buftarg(\r\nxfs_buftarg_t *btp,\r\nunsigned int sectorsize)\r\n{\r\nbtp->bt_meta_sectorsize = sectorsize;\r\nbtp->bt_meta_sectormask = sectorsize - 1;\r\nif (set_blocksize(btp->bt_bdev, sectorsize)) {\r\nchar name[BDEVNAME_SIZE];\r\nbdevname(btp->bt_bdev, name);\r\nxfs_warn(btp->bt_mount,\r\n"Cannot set_blocksize to %u on device %s",\r\nsectorsize, name);\r\nreturn -EINVAL;\r\n}\r\nbtp->bt_logical_sectorsize = bdev_logical_block_size(btp->bt_bdev);\r\nbtp->bt_logical_sectormask = bdev_logical_block_size(btp->bt_bdev) - 1;\r\nreturn 0;\r\n}\r\nSTATIC int\r\nxfs_setsize_buftarg_early(\r\nxfs_buftarg_t *btp,\r\nstruct block_device *bdev)\r\n{\r\nreturn xfs_setsize_buftarg(btp, bdev_logical_block_size(bdev));\r\n}\r\nxfs_buftarg_t *\r\nxfs_alloc_buftarg(\r\nstruct xfs_mount *mp,\r\nstruct block_device *bdev)\r\n{\r\nxfs_buftarg_t *btp;\r\nbtp = kmem_zalloc(sizeof(*btp), KM_SLEEP | KM_NOFS);\r\nbtp->bt_mount = mp;\r\nbtp->bt_dev = bdev->bd_dev;\r\nbtp->bt_bdev = bdev;\r\nbtp->bt_bdi = blk_get_backing_dev_info(bdev);\r\nif (xfs_setsize_buftarg_early(btp, bdev))\r\ngoto error;\r\nif (list_lru_init(&btp->bt_lru))\r\ngoto error;\r\nbtp->bt_shrinker.count_objects = xfs_buftarg_shrink_count;\r\nbtp->bt_shrinker.scan_objects = xfs_buftarg_shrink_scan;\r\nbtp->bt_shrinker.seeks = DEFAULT_SEEKS;\r\nbtp->bt_shrinker.flags = SHRINKER_NUMA_AWARE;\r\nregister_shrinker(&btp->bt_shrinker);\r\nreturn btp;\r\nerror:\r\nkmem_free(btp);\r\nreturn NULL;\r\n}\r\nbool\r\nxfs_buf_delwri_queue(\r\nstruct xfs_buf *bp,\r\nstruct list_head *list)\r\n{\r\nASSERT(xfs_buf_islocked(bp));\r\nASSERT(!(bp->b_flags & XBF_READ));\r\nif (bp->b_flags & _XBF_DELWRI_Q) {\r\ntrace_xfs_buf_delwri_queued(bp, _RET_IP_);\r\nreturn false;\r\n}\r\ntrace_xfs_buf_delwri_queue(bp, _RET_IP_);\r\nbp->b_flags |= _XBF_DELWRI_Q;\r\nif (list_empty(&bp->b_list)) {\r\natomic_inc(&bp->b_hold);\r\nlist_add_tail(&bp->b_list, list);\r\n}\r\nreturn true;\r\n}\r\nstatic int\r\nxfs_buf_cmp(\r\nvoid *priv,\r\nstruct list_head *a,\r\nstruct list_head *b)\r\n{\r\nstruct xfs_buf *ap = container_of(a, struct xfs_buf, b_list);\r\nstruct xfs_buf *bp = container_of(b, struct xfs_buf, b_list);\r\nxfs_daddr_t diff;\r\ndiff = ap->b_maps[0].bm_bn - bp->b_maps[0].bm_bn;\r\nif (diff < 0)\r\nreturn -1;\r\nif (diff > 0)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int\r\n__xfs_buf_delwri_submit(\r\nstruct list_head *buffer_list,\r\nstruct list_head *io_list,\r\nbool wait)\r\n{\r\nstruct blk_plug plug;\r\nstruct xfs_buf *bp, *n;\r\nint pinned = 0;\r\nlist_for_each_entry_safe(bp, n, buffer_list, b_list) {\r\nif (!wait) {\r\nif (xfs_buf_ispinned(bp)) {\r\npinned++;\r\ncontinue;\r\n}\r\nif (!xfs_buf_trylock(bp))\r\ncontinue;\r\n} else {\r\nxfs_buf_lock(bp);\r\n}\r\nif (!(bp->b_flags & _XBF_DELWRI_Q)) {\r\nlist_del_init(&bp->b_list);\r\nxfs_buf_relse(bp);\r\ncontinue;\r\n}\r\nlist_move_tail(&bp->b_list, io_list);\r\ntrace_xfs_buf_delwri_split(bp, _RET_IP_);\r\n}\r\nlist_sort(NULL, io_list, xfs_buf_cmp);\r\nblk_start_plug(&plug);\r\nlist_for_each_entry_safe(bp, n, io_list, b_list) {\r\nbp->b_flags &= ~(_XBF_DELWRI_Q | XBF_ASYNC | XBF_WRITE_FAIL);\r\nbp->b_flags |= XBF_WRITE | XBF_ASYNC;\r\nif (wait)\r\nxfs_buf_hold(bp);\r\nelse\r\nlist_del_init(&bp->b_list);\r\nxfs_buf_submit(bp);\r\n}\r\nblk_finish_plug(&plug);\r\nreturn pinned;\r\n}\r\nint\r\nxfs_buf_delwri_submit_nowait(\r\nstruct list_head *buffer_list)\r\n{\r\nLIST_HEAD (io_list);\r\nreturn __xfs_buf_delwri_submit(buffer_list, &io_list, false);\r\n}\r\nint\r\nxfs_buf_delwri_submit(\r\nstruct list_head *buffer_list)\r\n{\r\nLIST_HEAD (io_list);\r\nint error = 0, error2;\r\nstruct xfs_buf *bp;\r\n__xfs_buf_delwri_submit(buffer_list, &io_list, true);\r\nwhile (!list_empty(&io_list)) {\r\nbp = list_first_entry(&io_list, struct xfs_buf, b_list);\r\nlist_del_init(&bp->b_list);\r\nxfs_buf_lock(bp);\r\nerror2 = bp->b_error;\r\nxfs_buf_relse(bp);\r\nif (!error)\r\nerror = error2;\r\n}\r\nreturn error;\r\n}\r\nint __init\r\nxfs_buf_init(void)\r\n{\r\nxfs_buf_zone = kmem_zone_init_flags(sizeof(xfs_buf_t), "xfs_buf",\r\nKM_ZONE_HWALIGN, NULL);\r\nif (!xfs_buf_zone)\r\ngoto out;\r\nreturn 0;\r\nout:\r\nreturn -ENOMEM;\r\n}\r\nvoid\r\nxfs_buf_terminate(void)\r\n{\r\nkmem_zone_destroy(xfs_buf_zone);\r\n}
