static void rcar_dmac_write(struct rcar_dmac *dmac, u32 reg, u32 data)\r\n{\r\nif (reg == RCAR_DMAOR)\r\nwritew(data, dmac->iomem + reg);\r\nelse\r\nwritel(data, dmac->iomem + reg);\r\n}\r\nstatic u32 rcar_dmac_read(struct rcar_dmac *dmac, u32 reg)\r\n{\r\nif (reg == RCAR_DMAOR)\r\nreturn readw(dmac->iomem + reg);\r\nelse\r\nreturn readl(dmac->iomem + reg);\r\n}\r\nstatic u32 rcar_dmac_chan_read(struct rcar_dmac_chan *chan, u32 reg)\r\n{\r\nif (reg == RCAR_DMARS)\r\nreturn readw(chan->iomem + reg);\r\nelse\r\nreturn readl(chan->iomem + reg);\r\n}\r\nstatic void rcar_dmac_chan_write(struct rcar_dmac_chan *chan, u32 reg, u32 data)\r\n{\r\nif (reg == RCAR_DMARS)\r\nwritew(data, chan->iomem + reg);\r\nelse\r\nwritel(data, chan->iomem + reg);\r\n}\r\nstatic bool rcar_dmac_chan_is_busy(struct rcar_dmac_chan *chan)\r\n{\r\nu32 chcr = rcar_dmac_chan_read(chan, RCAR_DMACHCR);\r\nreturn (chcr & (RCAR_DMACHCR_DE | RCAR_DMACHCR_TE)) == RCAR_DMACHCR_DE;\r\n}\r\nstatic void rcar_dmac_chan_start_xfer(struct rcar_dmac_chan *chan)\r\n{\r\nstruct rcar_dmac_desc *desc = chan->desc.running;\r\nu32 chcr = desc->chcr;\r\nWARN_ON_ONCE(rcar_dmac_chan_is_busy(chan));\r\nif (chan->mid_rid >= 0)\r\nrcar_dmac_chan_write(chan, RCAR_DMARS, chan->mid_rid);\r\nif (desc->hwdescs.use) {\r\nstruct rcar_dmac_xfer_chunk *chunk;\r\ndev_dbg(chan->chan.device->dev,\r\n"chan%u: queue desc %p: %u@%pad\n",\r\nchan->index, desc, desc->nchunks, &desc->hwdescs.dma);\r\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\r\nrcar_dmac_chan_write(chan, RCAR_DMAFIXDPBASE,\r\ndesc->hwdescs.dma >> 32);\r\n#endif\r\nrcar_dmac_chan_write(chan, RCAR_DMADPBASE,\r\n(desc->hwdescs.dma & 0xfffffff0) |\r\nRCAR_DMADPBASE_SEL);\r\nrcar_dmac_chan_write(chan, RCAR_DMACHCRB,\r\nRCAR_DMACHCRB_DCNT(desc->nchunks - 1) |\r\nRCAR_DMACHCRB_DRST);\r\nchunk = list_first_entry(&desc->chunks,\r\nstruct rcar_dmac_xfer_chunk, node);\r\nrcar_dmac_chan_write(chan, RCAR_DMADAR,\r\nchunk->dst_addr & 0xffffffff);\r\nrcar_dmac_chan_write(chan, RCAR_DMADPCR, RCAR_DMADPCR_DIPT(1));\r\nchcr |= RCAR_DMACHCR_RPT_SAR | RCAR_DMACHCR_RPT_DAR\r\n| RCAR_DMACHCR_RPT_TCR | RCAR_DMACHCR_DPB;\r\nif (!desc->cyclic)\r\nchcr |= RCAR_DMACHCR_DPM_ENABLED | RCAR_DMACHCR_IE;\r\nelse if (desc->async_tx.callback)\r\nchcr |= RCAR_DMACHCR_DPM_INFINITE | RCAR_DMACHCR_DSIE;\r\nelse\r\nchcr |= RCAR_DMACHCR_DPM_INFINITE;\r\n} else {\r\nstruct rcar_dmac_xfer_chunk *chunk = desc->running;\r\ndev_dbg(chan->chan.device->dev,\r\n"chan%u: queue chunk %p: %u@%pad -> %pad\n",\r\nchan->index, chunk, chunk->size, &chunk->src_addr,\r\n&chunk->dst_addr);\r\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\r\nrcar_dmac_chan_write(chan, RCAR_DMAFIXSAR,\r\nchunk->src_addr >> 32);\r\nrcar_dmac_chan_write(chan, RCAR_DMAFIXDAR,\r\nchunk->dst_addr >> 32);\r\n#endif\r\nrcar_dmac_chan_write(chan, RCAR_DMASAR,\r\nchunk->src_addr & 0xffffffff);\r\nrcar_dmac_chan_write(chan, RCAR_DMADAR,\r\nchunk->dst_addr & 0xffffffff);\r\nrcar_dmac_chan_write(chan, RCAR_DMATCR,\r\nchunk->size >> desc->xfer_shift);\r\nchcr |= RCAR_DMACHCR_DPM_DISABLED | RCAR_DMACHCR_IE;\r\n}\r\nrcar_dmac_chan_write(chan, RCAR_DMACHCR, chcr | RCAR_DMACHCR_DE);\r\n}\r\nstatic int rcar_dmac_init(struct rcar_dmac *dmac)\r\n{\r\nu16 dmaor;\r\nrcar_dmac_write(dmac, RCAR_DMACHCLR, 0x7fff);\r\nrcar_dmac_write(dmac, RCAR_DMAOR,\r\nRCAR_DMAOR_PRI_FIXED | RCAR_DMAOR_DME);\r\ndmaor = rcar_dmac_read(dmac, RCAR_DMAOR);\r\nif ((dmaor & (RCAR_DMAOR_AE | RCAR_DMAOR_DME)) != RCAR_DMAOR_DME) {\r\ndev_warn(dmac->dev, "DMAOR initialization failed.\n");\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic dma_cookie_t rcar_dmac_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct rcar_dmac_chan *chan = to_rcar_dmac_chan(tx->chan);\r\nstruct rcar_dmac_desc *desc = to_rcar_dmac_desc(tx);\r\nunsigned long flags;\r\ndma_cookie_t cookie;\r\nspin_lock_irqsave(&chan->lock, flags);\r\ncookie = dma_cookie_assign(tx);\r\ndev_dbg(chan->chan.device->dev, "chan%u: submit #%d@%p\n",\r\nchan->index, tx->cookie, desc);\r\nlist_add_tail(&desc->node, &chan->desc.pending);\r\ndesc->running = list_first_entry(&desc->chunks,\r\nstruct rcar_dmac_xfer_chunk, node);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic int rcar_dmac_desc_alloc(struct rcar_dmac_chan *chan, gfp_t gfp)\r\n{\r\nstruct rcar_dmac_desc_page *page;\r\nLIST_HEAD(list);\r\nunsigned int i;\r\npage = (void *)get_zeroed_page(gfp);\r\nif (!page)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < RCAR_DMAC_DESCS_PER_PAGE; ++i) {\r\nstruct rcar_dmac_desc *desc = &page->descs[i];\r\ndma_async_tx_descriptor_init(&desc->async_tx, &chan->chan);\r\ndesc->async_tx.tx_submit = rcar_dmac_tx_submit;\r\nINIT_LIST_HEAD(&desc->chunks);\r\nlist_add_tail(&desc->node, &list);\r\n}\r\nspin_lock_irq(&chan->lock);\r\nlist_splice_tail(&list, &chan->desc.free);\r\nlist_add_tail(&page->node, &chan->desc.pages);\r\nspin_unlock_irq(&chan->lock);\r\nreturn 0;\r\n}\r\nstatic void rcar_dmac_desc_put(struct rcar_dmac_chan *chan,\r\nstruct rcar_dmac_desc *desc)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nlist_splice_tail_init(&desc->chunks, &chan->desc.chunks_free);\r\nlist_add_tail(&desc->node, &chan->desc.free);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic void rcar_dmac_desc_recycle_acked(struct rcar_dmac_chan *chan)\r\n{\r\nstruct rcar_dmac_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\nspin_lock_irq(&chan->lock);\r\nlist_splice_init(&chan->desc.wait, &list);\r\nspin_unlock_irq(&chan->lock);\r\nlist_for_each_entry_safe(desc, _desc, &list, node) {\r\nif (async_tx_test_ack(&desc->async_tx)) {\r\nlist_del(&desc->node);\r\nrcar_dmac_desc_put(chan, desc);\r\n}\r\n}\r\nif (list_empty(&list))\r\nreturn;\r\nspin_lock_irq(&chan->lock);\r\nlist_splice(&list, &chan->desc.wait);\r\nspin_unlock_irq(&chan->lock);\r\n}\r\nstatic struct rcar_dmac_desc *rcar_dmac_desc_get(struct rcar_dmac_chan *chan)\r\n{\r\nstruct rcar_dmac_desc *desc;\r\nint ret;\r\nrcar_dmac_desc_recycle_acked(chan);\r\nspin_lock_irq(&chan->lock);\r\nwhile (list_empty(&chan->desc.free)) {\r\nspin_unlock_irq(&chan->lock);\r\nret = rcar_dmac_desc_alloc(chan, GFP_NOWAIT);\r\nif (ret < 0)\r\nreturn NULL;\r\nspin_lock_irq(&chan->lock);\r\n}\r\ndesc = list_first_entry(&chan->desc.free, struct rcar_dmac_desc, node);\r\nlist_del(&desc->node);\r\nspin_unlock_irq(&chan->lock);\r\nreturn desc;\r\n}\r\nstatic int rcar_dmac_xfer_chunk_alloc(struct rcar_dmac_chan *chan, gfp_t gfp)\r\n{\r\nstruct rcar_dmac_desc_page *page;\r\nLIST_HEAD(list);\r\nunsigned int i;\r\npage = (void *)get_zeroed_page(gfp);\r\nif (!page)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < RCAR_DMAC_XFER_CHUNKS_PER_PAGE; ++i) {\r\nstruct rcar_dmac_xfer_chunk *chunk = &page->chunks[i];\r\nlist_add_tail(&chunk->node, &list);\r\n}\r\nspin_lock_irq(&chan->lock);\r\nlist_splice_tail(&list, &chan->desc.chunks_free);\r\nlist_add_tail(&page->node, &chan->desc.pages);\r\nspin_unlock_irq(&chan->lock);\r\nreturn 0;\r\n}\r\nstatic struct rcar_dmac_xfer_chunk *\r\nrcar_dmac_xfer_chunk_get(struct rcar_dmac_chan *chan)\r\n{\r\nstruct rcar_dmac_xfer_chunk *chunk;\r\nint ret;\r\nspin_lock_irq(&chan->lock);\r\nwhile (list_empty(&chan->desc.chunks_free)) {\r\nspin_unlock_irq(&chan->lock);\r\nret = rcar_dmac_xfer_chunk_alloc(chan, GFP_NOWAIT);\r\nif (ret < 0)\r\nreturn NULL;\r\nspin_lock_irq(&chan->lock);\r\n}\r\nchunk = list_first_entry(&chan->desc.chunks_free,\r\nstruct rcar_dmac_xfer_chunk, node);\r\nlist_del(&chunk->node);\r\nspin_unlock_irq(&chan->lock);\r\nreturn chunk;\r\n}\r\nstatic void rcar_dmac_realloc_hwdesc(struct rcar_dmac_chan *chan,\r\nstruct rcar_dmac_desc *desc, size_t size)\r\n{\r\nsize = PAGE_ALIGN(size);\r\nif (desc->hwdescs.size == size)\r\nreturn;\r\nif (desc->hwdescs.mem) {\r\ndma_free_coherent(chan->chan.device->dev, desc->hwdescs.size,\r\ndesc->hwdescs.mem, desc->hwdescs.dma);\r\ndesc->hwdescs.mem = NULL;\r\ndesc->hwdescs.size = 0;\r\n}\r\nif (!size)\r\nreturn;\r\ndesc->hwdescs.mem = dma_alloc_coherent(chan->chan.device->dev, size,\r\n&desc->hwdescs.dma, GFP_NOWAIT);\r\nif (!desc->hwdescs.mem)\r\nreturn;\r\ndesc->hwdescs.size = size;\r\n}\r\nstatic int rcar_dmac_fill_hwdesc(struct rcar_dmac_chan *chan,\r\nstruct rcar_dmac_desc *desc)\r\n{\r\nstruct rcar_dmac_xfer_chunk *chunk;\r\nstruct rcar_dmac_hw_desc *hwdesc;\r\nrcar_dmac_realloc_hwdesc(chan, desc, desc->nchunks * sizeof(*hwdesc));\r\nhwdesc = desc->hwdescs.mem;\r\nif (!hwdesc)\r\nreturn -ENOMEM;\r\nlist_for_each_entry(chunk, &desc->chunks, node) {\r\nhwdesc->sar = chunk->src_addr;\r\nhwdesc->dar = chunk->dst_addr;\r\nhwdesc->tcr = chunk->size >> desc->xfer_shift;\r\nhwdesc++;\r\n}\r\nreturn 0;\r\n}\r\nstatic void rcar_dmac_chan_halt(struct rcar_dmac_chan *chan)\r\n{\r\nu32 chcr = rcar_dmac_chan_read(chan, RCAR_DMACHCR);\r\nchcr &= ~(RCAR_DMACHCR_DSE | RCAR_DMACHCR_DSIE | RCAR_DMACHCR_IE |\r\nRCAR_DMACHCR_TE | RCAR_DMACHCR_DE);\r\nrcar_dmac_chan_write(chan, RCAR_DMACHCR, chcr);\r\n}\r\nstatic void rcar_dmac_chan_reinit(struct rcar_dmac_chan *chan)\r\n{\r\nstruct rcar_dmac_desc *desc, *_desc;\r\nunsigned long flags;\r\nLIST_HEAD(descs);\r\nspin_lock_irqsave(&chan->lock, flags);\r\nlist_splice_init(&chan->desc.pending, &descs);\r\nlist_splice_init(&chan->desc.active, &descs);\r\nlist_splice_init(&chan->desc.done, &descs);\r\nlist_splice_init(&chan->desc.wait, &descs);\r\nchan->desc.running = NULL;\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &descs, node) {\r\nlist_del(&desc->node);\r\nrcar_dmac_desc_put(chan, desc);\r\n}\r\n}\r\nstatic void rcar_dmac_stop(struct rcar_dmac *dmac)\r\n{\r\nrcar_dmac_write(dmac, RCAR_DMAOR, 0);\r\n}\r\nstatic void rcar_dmac_abort(struct rcar_dmac *dmac)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < dmac->n_channels; ++i) {\r\nstruct rcar_dmac_chan *chan = &dmac->channels[i];\r\nspin_lock(&chan->lock);\r\nrcar_dmac_chan_halt(chan);\r\nspin_unlock(&chan->lock);\r\nrcar_dmac_chan_reinit(chan);\r\n}\r\n}\r\nstatic void rcar_dmac_chan_configure_desc(struct rcar_dmac_chan *chan,\r\nstruct rcar_dmac_desc *desc)\r\n{\r\nstatic const u32 chcr_ts[] = {\r\nRCAR_DMACHCR_TS_1B, RCAR_DMACHCR_TS_2B,\r\nRCAR_DMACHCR_TS_4B, RCAR_DMACHCR_TS_8B,\r\nRCAR_DMACHCR_TS_16B, RCAR_DMACHCR_TS_32B,\r\nRCAR_DMACHCR_TS_64B,\r\n};\r\nunsigned int xfer_size;\r\nu32 chcr;\r\nswitch (desc->direction) {\r\ncase DMA_DEV_TO_MEM:\r\nchcr = RCAR_DMACHCR_DM_INC | RCAR_DMACHCR_SM_FIXED\r\n| RCAR_DMACHCR_RS_DMARS;\r\nxfer_size = chan->src_xfer_size;\r\nbreak;\r\ncase DMA_MEM_TO_DEV:\r\nchcr = RCAR_DMACHCR_DM_FIXED | RCAR_DMACHCR_SM_INC\r\n| RCAR_DMACHCR_RS_DMARS;\r\nxfer_size = chan->dst_xfer_size;\r\nbreak;\r\ncase DMA_MEM_TO_MEM:\r\ndefault:\r\nchcr = RCAR_DMACHCR_DM_INC | RCAR_DMACHCR_SM_INC\r\n| RCAR_DMACHCR_RS_AUTO;\r\nxfer_size = RCAR_DMAC_MEMCPY_XFER_SIZE;\r\nbreak;\r\n}\r\ndesc->xfer_shift = ilog2(xfer_size);\r\ndesc->chcr = chcr | chcr_ts[desc->xfer_shift];\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nrcar_dmac_chan_prep_sg(struct rcar_dmac_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, dma_addr_t dev_addr,\r\nenum dma_transfer_direction dir, unsigned long dma_flags,\r\nbool cyclic)\r\n{\r\nstruct rcar_dmac_xfer_chunk *chunk;\r\nstruct rcar_dmac_desc *desc;\r\nstruct scatterlist *sg;\r\nunsigned int nchunks = 0;\r\nunsigned int max_chunk_size;\r\nunsigned int full_size = 0;\r\nbool highmem = false;\r\nunsigned int i;\r\ndesc = rcar_dmac_desc_get(chan);\r\nif (!desc)\r\nreturn NULL;\r\ndesc->async_tx.flags = dma_flags;\r\ndesc->async_tx.cookie = -EBUSY;\r\ndesc->cyclic = cyclic;\r\ndesc->direction = dir;\r\nrcar_dmac_chan_configure_desc(chan, desc);\r\nmax_chunk_size = (RCAR_DMATCR_MASK + 1) << desc->xfer_shift;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\ndma_addr_t mem_addr = sg_dma_address(sg);\r\nunsigned int len = sg_dma_len(sg);\r\nfull_size += len;\r\nwhile (len) {\r\nunsigned int size = min(len, max_chunk_size);\r\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\r\nif (dev_addr >> 32 != (dev_addr + size - 1) >> 32)\r\nsize = ALIGN(dev_addr, 1ULL << 32) - dev_addr;\r\nif (mem_addr >> 32 != (mem_addr + size - 1) >> 32)\r\nsize = ALIGN(mem_addr, 1ULL << 32) - mem_addr;\r\nif (dev_addr >> 32 || mem_addr >> 32)\r\nhighmem = true;\r\n#endif\r\nchunk = rcar_dmac_xfer_chunk_get(chan);\r\nif (!chunk) {\r\nrcar_dmac_desc_put(chan, desc);\r\nreturn NULL;\r\n}\r\nif (dir == DMA_DEV_TO_MEM) {\r\nchunk->src_addr = dev_addr;\r\nchunk->dst_addr = mem_addr;\r\n} else {\r\nchunk->src_addr = mem_addr;\r\nchunk->dst_addr = dev_addr;\r\n}\r\nchunk->size = size;\r\ndev_dbg(chan->chan.device->dev,\r\n"chan%u: chunk %p/%p sgl %u@%p, %u/%u %pad -> %pad\n",\r\nchan->index, chunk, desc, i, sg, size, len,\r\n&chunk->src_addr, &chunk->dst_addr);\r\nmem_addr += size;\r\nif (dir == DMA_MEM_TO_MEM)\r\ndev_addr += size;\r\nlen -= size;\r\nlist_add_tail(&chunk->node, &desc->chunks);\r\nnchunks++;\r\n}\r\n}\r\ndesc->nchunks = nchunks;\r\ndesc->size = full_size;\r\ndesc->hwdescs.use = !highmem && nchunks > 1;\r\nif (desc->hwdescs.use) {\r\nif (rcar_dmac_fill_hwdesc(chan, desc) < 0)\r\ndesc->hwdescs.use = false;\r\n}\r\nreturn &desc->async_tx;\r\n}\r\nstatic int rcar_dmac_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\r\nint ret;\r\nINIT_LIST_HEAD(&rchan->desc.chunks_free);\r\nINIT_LIST_HEAD(&rchan->desc.pages);\r\nret = rcar_dmac_xfer_chunk_alloc(rchan, GFP_KERNEL);\r\nif (ret < 0)\r\nreturn -ENOMEM;\r\nret = rcar_dmac_desc_alloc(rchan, GFP_KERNEL);\r\nif (ret < 0)\r\nreturn -ENOMEM;\r\nreturn pm_runtime_get_sync(chan->device->dev);\r\n}\r\nstatic void rcar_dmac_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\r\nstruct rcar_dmac *dmac = to_rcar_dmac(chan->device);\r\nstruct rcar_dmac_desc_page *page, *_page;\r\nstruct rcar_dmac_desc *desc;\r\nLIST_HEAD(list);\r\nspin_lock_irq(&rchan->lock);\r\nrcar_dmac_chan_halt(rchan);\r\nspin_unlock_irq(&rchan->lock);\r\nif (rchan->mid_rid >= 0) {\r\nclear_bit(rchan->mid_rid, dmac->modules);\r\nrchan->mid_rid = -EINVAL;\r\n}\r\nlist_splice_init(&rchan->desc.free, &list);\r\nlist_splice_init(&rchan->desc.pending, &list);\r\nlist_splice_init(&rchan->desc.active, &list);\r\nlist_splice_init(&rchan->desc.done, &list);\r\nlist_splice_init(&rchan->desc.wait, &list);\r\nlist_for_each_entry(desc, &list, node)\r\nrcar_dmac_realloc_hwdesc(rchan, desc, 0);\r\nlist_for_each_entry_safe(page, _page, &rchan->desc.pages, node) {\r\nlist_del(&page->node);\r\nfree_page((unsigned long)page);\r\n}\r\npm_runtime_put(chan->device->dev);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nrcar_dmac_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dma_dest,\r\ndma_addr_t dma_src, size_t len, unsigned long flags)\r\n{\r\nstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\r\nstruct scatterlist sgl;\r\nif (!len)\r\nreturn NULL;\r\nsg_init_table(&sgl, 1);\r\nsg_set_page(&sgl, pfn_to_page(PFN_DOWN(dma_src)), len,\r\noffset_in_page(dma_src));\r\nsg_dma_address(&sgl) = dma_src;\r\nsg_dma_len(&sgl) = len;\r\nreturn rcar_dmac_chan_prep_sg(rchan, &sgl, 1, dma_dest,\r\nDMA_MEM_TO_MEM, flags, false);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nrcar_dmac_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction dir,\r\nunsigned long flags, void *context)\r\n{\r\nstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\r\ndma_addr_t dev_addr;\r\nif (rchan->mid_rid < 0 || !sg_len) {\r\ndev_warn(chan->device->dev,\r\n"%s: bad parameter: len=%d, id=%d\n",\r\n__func__, sg_len, rchan->mid_rid);\r\nreturn NULL;\r\n}\r\ndev_addr = dir == DMA_DEV_TO_MEM\r\n? rchan->src_slave_addr : rchan->dst_slave_addr;\r\nreturn rcar_dmac_chan_prep_sg(rchan, sgl, sg_len, dev_addr,\r\ndir, flags, false);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nrcar_dmac_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t buf_addr,\r\nsize_t buf_len, size_t period_len,\r\nenum dma_transfer_direction dir, unsigned long flags)\r\n{\r\nstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\r\nstruct dma_async_tx_descriptor *desc;\r\nstruct scatterlist *sgl;\r\ndma_addr_t dev_addr;\r\nunsigned int sg_len;\r\nunsigned int i;\r\nif (rchan->mid_rid < 0 || buf_len < period_len) {\r\ndev_warn(chan->device->dev,\r\n"%s: bad parameter: buf_len=%zu, period_len=%zu, id=%d\n",\r\n__func__, buf_len, period_len, rchan->mid_rid);\r\nreturn NULL;\r\n}\r\nsg_len = buf_len / period_len;\r\nif (sg_len > RCAR_DMAC_MAX_SG_LEN) {\r\ndev_err(chan->device->dev,\r\n"chan%u: sg length %d exceds limit %d",\r\nrchan->index, sg_len, RCAR_DMAC_MAX_SG_LEN);\r\nreturn NULL;\r\n}\r\nsgl = kcalloc(sg_len, sizeof(*sgl), GFP_NOWAIT);\r\nif (!sgl)\r\nreturn NULL;\r\nsg_init_table(sgl, sg_len);\r\nfor (i = 0; i < sg_len; ++i) {\r\ndma_addr_t src = buf_addr + (period_len * i);\r\nsg_set_page(&sgl[i], pfn_to_page(PFN_DOWN(src)), period_len,\r\noffset_in_page(src));\r\nsg_dma_address(&sgl[i]) = src;\r\nsg_dma_len(&sgl[i]) = period_len;\r\n}\r\ndev_addr = dir == DMA_DEV_TO_MEM\r\n? rchan->src_slave_addr : rchan->dst_slave_addr;\r\ndesc = rcar_dmac_chan_prep_sg(rchan, sgl, sg_len, dev_addr,\r\ndir, flags, true);\r\nkfree(sgl);\r\nreturn desc;\r\n}\r\nstatic int rcar_dmac_device_config(struct dma_chan *chan,\r\nstruct dma_slave_config *cfg)\r\n{\r\nstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\r\nrchan->src_slave_addr = cfg->src_addr;\r\nrchan->dst_slave_addr = cfg->dst_addr;\r\nrchan->src_xfer_size = cfg->src_addr_width;\r\nrchan->dst_xfer_size = cfg->dst_addr_width;\r\nreturn 0;\r\n}\r\nstatic int rcar_dmac_chan_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&rchan->lock, flags);\r\nrcar_dmac_chan_halt(rchan);\r\nspin_unlock_irqrestore(&rchan->lock, flags);\r\nrcar_dmac_chan_reinit(rchan);\r\nreturn 0;\r\n}\r\nstatic unsigned int rcar_dmac_chan_get_residue(struct rcar_dmac_chan *chan,\r\ndma_cookie_t cookie)\r\n{\r\nstruct rcar_dmac_desc *desc = chan->desc.running;\r\nstruct rcar_dmac_xfer_chunk *running = NULL;\r\nstruct rcar_dmac_xfer_chunk *chunk;\r\nunsigned int residue = 0;\r\nunsigned int dptr = 0;\r\nif (!desc)\r\nreturn 0;\r\nif (cookie != desc->async_tx.cookie)\r\nreturn desc->size;\r\nif (desc->hwdescs.use) {\r\ndptr = (rcar_dmac_chan_read(chan, RCAR_DMACHCRB) &\r\nRCAR_DMACHCRB_DPTR_MASK) >> RCAR_DMACHCRB_DPTR_SHIFT;\r\nWARN_ON(dptr >= desc->nchunks);\r\n} else {\r\nrunning = desc->running;\r\n}\r\nlist_for_each_entry_reverse(chunk, &desc->chunks, node) {\r\nif (chunk == running || ++dptr == desc->nchunks)\r\nbreak;\r\nresidue += chunk->size;\r\n}\r\nresidue += rcar_dmac_chan_read(chan, RCAR_DMATCR) << desc->xfer_shift;\r\nreturn residue;\r\n}\r\nstatic enum dma_status rcar_dmac_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\r\nenum dma_status status;\r\nunsigned long flags;\r\nunsigned int residue;\r\nstatus = dma_cookie_status(chan, cookie, txstate);\r\nif (status == DMA_COMPLETE || !txstate)\r\nreturn status;\r\nspin_lock_irqsave(&rchan->lock, flags);\r\nresidue = rcar_dmac_chan_get_residue(rchan, cookie);\r\nspin_unlock_irqrestore(&rchan->lock, flags);\r\ndma_set_residue(txstate, residue);\r\nreturn status;\r\n}\r\nstatic void rcar_dmac_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct rcar_dmac_chan *rchan = to_rcar_dmac_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&rchan->lock, flags);\r\nif (list_empty(&rchan->desc.pending))\r\ngoto done;\r\nlist_splice_tail_init(&rchan->desc.pending, &rchan->desc.active);\r\nif (!rchan->desc.running) {\r\nstruct rcar_dmac_desc *desc;\r\ndesc = list_first_entry(&rchan->desc.active,\r\nstruct rcar_dmac_desc, node);\r\nrchan->desc.running = desc;\r\nrcar_dmac_chan_start_xfer(rchan);\r\n}\r\ndone:\r\nspin_unlock_irqrestore(&rchan->lock, flags);\r\n}\r\nstatic irqreturn_t rcar_dmac_isr_desc_stage_end(struct rcar_dmac_chan *chan)\r\n{\r\nstruct rcar_dmac_desc *desc = chan->desc.running;\r\nunsigned int stage;\r\nif (WARN_ON(!desc || !desc->cyclic)) {\r\nreturn IRQ_NONE;\r\n}\r\nstage = (rcar_dmac_chan_read(chan, RCAR_DMACHCRB) &\r\nRCAR_DMACHCRB_DPTR_MASK) >> RCAR_DMACHCRB_DPTR_SHIFT;\r\nrcar_dmac_chan_write(chan, RCAR_DMADPCR, RCAR_DMADPCR_DIPT(stage));\r\nreturn IRQ_WAKE_THREAD;\r\n}\r\nstatic irqreturn_t rcar_dmac_isr_transfer_end(struct rcar_dmac_chan *chan)\r\n{\r\nstruct rcar_dmac_desc *desc = chan->desc.running;\r\nirqreturn_t ret = IRQ_WAKE_THREAD;\r\nif (WARN_ON_ONCE(!desc)) {\r\nreturn IRQ_NONE;\r\n}\r\nif (!desc->hwdescs.use) {\r\nif (!list_is_last(&desc->running->node, &desc->chunks)) {\r\ndesc->running = list_next_entry(desc->running, node);\r\nif (!desc->cyclic)\r\nret = IRQ_HANDLED;\r\ngoto done;\r\n}\r\nif (desc->cyclic) {\r\ndesc->running =\r\nlist_first_entry(&desc->chunks,\r\nstruct rcar_dmac_xfer_chunk,\r\nnode);\r\ngoto done;\r\n}\r\n}\r\nlist_move_tail(&desc->node, &chan->desc.done);\r\nif (!list_empty(&chan->desc.active))\r\nchan->desc.running = list_first_entry(&chan->desc.active,\r\nstruct rcar_dmac_desc,\r\nnode);\r\nelse\r\nchan->desc.running = NULL;\r\ndone:\r\nif (chan->desc.running)\r\nrcar_dmac_chan_start_xfer(chan);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t rcar_dmac_isr_channel(int irq, void *dev)\r\n{\r\nu32 mask = RCAR_DMACHCR_DSE | RCAR_DMACHCR_TE;\r\nstruct rcar_dmac_chan *chan = dev;\r\nirqreturn_t ret = IRQ_NONE;\r\nu32 chcr;\r\nspin_lock(&chan->lock);\r\nchcr = rcar_dmac_chan_read(chan, RCAR_DMACHCR);\r\nif (chcr & RCAR_DMACHCR_TE)\r\nmask |= RCAR_DMACHCR_DE;\r\nrcar_dmac_chan_write(chan, RCAR_DMACHCR, chcr & ~mask);\r\nif (chcr & RCAR_DMACHCR_DSE)\r\nret |= rcar_dmac_isr_desc_stage_end(chan);\r\nif (chcr & RCAR_DMACHCR_TE)\r\nret |= rcar_dmac_isr_transfer_end(chan);\r\nspin_unlock(&chan->lock);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t rcar_dmac_isr_channel_thread(int irq, void *dev)\r\n{\r\nstruct rcar_dmac_chan *chan = dev;\r\nstruct rcar_dmac_desc *desc;\r\nspin_lock_irq(&chan->lock);\r\nif (chan->desc.running && chan->desc.running->cyclic) {\r\ndma_async_tx_callback callback;\r\nvoid *callback_param;\r\ndesc = chan->desc.running;\r\ncallback = desc->async_tx.callback;\r\ncallback_param = desc->async_tx.callback_param;\r\nif (callback) {\r\nspin_unlock_irq(&chan->lock);\r\ncallback(callback_param);\r\nspin_lock_irq(&chan->lock);\r\n}\r\n}\r\nwhile (!list_empty(&chan->desc.done)) {\r\ndesc = list_first_entry(&chan->desc.done, struct rcar_dmac_desc,\r\nnode);\r\ndma_cookie_complete(&desc->async_tx);\r\nlist_del(&desc->node);\r\nif (desc->async_tx.callback) {\r\nspin_unlock_irq(&chan->lock);\r\ndesc->async_tx.callback(desc->async_tx.callback_param);\r\nspin_lock_irq(&chan->lock);\r\n}\r\nlist_add_tail(&desc->node, &chan->desc.wait);\r\n}\r\nspin_unlock_irq(&chan->lock);\r\nrcar_dmac_desc_recycle_acked(chan);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t rcar_dmac_isr_error(int irq, void *data)\r\n{\r\nstruct rcar_dmac *dmac = data;\r\nif (!(rcar_dmac_read(dmac, RCAR_DMAOR) & RCAR_DMAOR_AE))\r\nreturn IRQ_NONE;\r\nrcar_dmac_stop(dmac);\r\nrcar_dmac_abort(dmac);\r\nrcar_dmac_init(dmac);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic bool rcar_dmac_chan_filter(struct dma_chan *chan, void *arg)\r\n{\r\nstruct rcar_dmac *dmac = to_rcar_dmac(chan->device);\r\nstruct of_phandle_args *dma_spec = arg;\r\nif (chan->device->device_config != rcar_dmac_device_config ||\r\ndma_spec->np != chan->device->dev->of_node)\r\nreturn false;\r\nreturn !test_and_set_bit(dma_spec->args[0], dmac->modules);\r\n}\r\nstatic struct dma_chan *rcar_dmac_of_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct rcar_dmac_chan *rchan;\r\nstruct dma_chan *chan;\r\ndma_cap_mask_t mask;\r\nif (dma_spec->args_count != 1)\r\nreturn NULL;\r\ndma_cap_zero(mask);\r\ndma_cap_set(DMA_SLAVE, mask);\r\nchan = dma_request_channel(mask, rcar_dmac_chan_filter, dma_spec);\r\nif (!chan)\r\nreturn NULL;\r\nrchan = to_rcar_dmac_chan(chan);\r\nrchan->mid_rid = dma_spec->args[0];\r\nreturn chan;\r\n}\r\nstatic int rcar_dmac_sleep_suspend(struct device *dev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int rcar_dmac_sleep_resume(struct device *dev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int rcar_dmac_runtime_suspend(struct device *dev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int rcar_dmac_runtime_resume(struct device *dev)\r\n{\r\nstruct rcar_dmac *dmac = dev_get_drvdata(dev);\r\nreturn rcar_dmac_init(dmac);\r\n}\r\nstatic int rcar_dmac_chan_probe(struct rcar_dmac *dmac,\r\nstruct rcar_dmac_chan *rchan,\r\nunsigned int index)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dmac->dev);\r\nstruct dma_chan *chan = &rchan->chan;\r\nchar pdev_irqname[5];\r\nchar *irqname;\r\nint irq;\r\nint ret;\r\nrchan->index = index;\r\nrchan->iomem = dmac->iomem + RCAR_DMAC_CHAN_OFFSET(index);\r\nrchan->mid_rid = -EINVAL;\r\nspin_lock_init(&rchan->lock);\r\nINIT_LIST_HEAD(&rchan->desc.free);\r\nINIT_LIST_HEAD(&rchan->desc.pending);\r\nINIT_LIST_HEAD(&rchan->desc.active);\r\nINIT_LIST_HEAD(&rchan->desc.done);\r\nINIT_LIST_HEAD(&rchan->desc.wait);\r\nsprintf(pdev_irqname, "ch%u", index);\r\nirq = platform_get_irq_byname(pdev, pdev_irqname);\r\nif (irq < 0) {\r\ndev_err(dmac->dev, "no IRQ specified for channel %u\n", index);\r\nreturn -ENODEV;\r\n}\r\nirqname = devm_kasprintf(dmac->dev, GFP_KERNEL, "%s:%u",\r\ndev_name(dmac->dev), index);\r\nif (!irqname)\r\nreturn -ENOMEM;\r\nret = devm_request_threaded_irq(dmac->dev, irq, rcar_dmac_isr_channel,\r\nrcar_dmac_isr_channel_thread, 0,\r\nirqname, rchan);\r\nif (ret) {\r\ndev_err(dmac->dev, "failed to request IRQ %u (%d)\n", irq, ret);\r\nreturn ret;\r\n}\r\nchan->device = &dmac->engine;\r\ndma_cookie_init(chan);\r\nlist_add_tail(&chan->device_node, &dmac->engine.channels);\r\nreturn 0;\r\n}\r\nstatic int rcar_dmac_parse_of(struct device *dev, struct rcar_dmac *dmac)\r\n{\r\nstruct device_node *np = dev->of_node;\r\nint ret;\r\nret = of_property_read_u32(np, "dma-channels", &dmac->n_channels);\r\nif (ret < 0) {\r\ndev_err(dev, "unable to read dma-channels property\n");\r\nreturn ret;\r\n}\r\nif (dmac->n_channels <= 0 || dmac->n_channels >= 100) {\r\ndev_err(dev, "invalid number of channels %u\n",\r\ndmac->n_channels);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int rcar_dmac_probe(struct platform_device *pdev)\r\n{\r\nconst enum dma_slave_buswidth widths = DMA_SLAVE_BUSWIDTH_1_BYTE |\r\nDMA_SLAVE_BUSWIDTH_2_BYTES | DMA_SLAVE_BUSWIDTH_4_BYTES |\r\nDMA_SLAVE_BUSWIDTH_8_BYTES | DMA_SLAVE_BUSWIDTH_16_BYTES |\r\nDMA_SLAVE_BUSWIDTH_32_BYTES | DMA_SLAVE_BUSWIDTH_64_BYTES;\r\nunsigned int channels_offset = 0;\r\nstruct dma_device *engine;\r\nstruct rcar_dmac *dmac;\r\nstruct resource *mem;\r\nunsigned int i;\r\nchar *irqname;\r\nint irq;\r\nint ret;\r\ndmac = devm_kzalloc(&pdev->dev, sizeof(*dmac), GFP_KERNEL);\r\nif (!dmac)\r\nreturn -ENOMEM;\r\ndmac->dev = &pdev->dev;\r\nplatform_set_drvdata(pdev, dmac);\r\nret = rcar_dmac_parse_of(&pdev->dev, dmac);\r\nif (ret < 0)\r\nreturn ret;\r\nif (pdev->dev.iommu_group) {\r\ndmac->n_channels--;\r\nchannels_offset = 1;\r\n}\r\ndmac->channels = devm_kcalloc(&pdev->dev, dmac->n_channels,\r\nsizeof(*dmac->channels), GFP_KERNEL);\r\nif (!dmac->channels)\r\nreturn -ENOMEM;\r\nmem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ndmac->iomem = devm_ioremap_resource(&pdev->dev, mem);\r\nif (IS_ERR(dmac->iomem))\r\nreturn PTR_ERR(dmac->iomem);\r\nirq = platform_get_irq_byname(pdev, "error");\r\nif (irq < 0) {\r\ndev_err(&pdev->dev, "no error IRQ specified\n");\r\nreturn -ENODEV;\r\n}\r\nirqname = devm_kasprintf(dmac->dev, GFP_KERNEL, "%s:error",\r\ndev_name(dmac->dev));\r\nif (!irqname)\r\nreturn -ENOMEM;\r\nret = devm_request_irq(&pdev->dev, irq, rcar_dmac_isr_error, 0,\r\nirqname, dmac);\r\nif (ret) {\r\ndev_err(&pdev->dev, "failed to request IRQ %u (%d)\n",\r\nirq, ret);\r\nreturn ret;\r\n}\r\npm_runtime_enable(&pdev->dev);\r\nret = pm_runtime_get_sync(&pdev->dev);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev, "runtime PM get sync failed (%d)\n", ret);\r\nreturn ret;\r\n}\r\nret = rcar_dmac_init(dmac);\r\npm_runtime_put(&pdev->dev);\r\nif (ret) {\r\ndev_err(&pdev->dev, "failed to reset device\n");\r\ngoto error;\r\n}\r\nINIT_LIST_HEAD(&dmac->engine.channels);\r\nfor (i = 0; i < dmac->n_channels; ++i) {\r\nret = rcar_dmac_chan_probe(dmac, &dmac->channels[i],\r\ni + channels_offset);\r\nif (ret < 0)\r\ngoto error;\r\n}\r\nret = of_dma_controller_register(pdev->dev.of_node, rcar_dmac_of_xlate,\r\nNULL);\r\nif (ret < 0)\r\ngoto error;\r\nengine = &dmac->engine;\r\ndma_cap_set(DMA_MEMCPY, engine->cap_mask);\r\ndma_cap_set(DMA_SLAVE, engine->cap_mask);\r\nengine->dev = &pdev->dev;\r\nengine->copy_align = ilog2(RCAR_DMAC_MEMCPY_XFER_SIZE);\r\nengine->src_addr_widths = widths;\r\nengine->dst_addr_widths = widths;\r\nengine->directions = BIT(DMA_MEM_TO_DEV) | BIT(DMA_DEV_TO_MEM);\r\nengine->residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\nengine->device_alloc_chan_resources = rcar_dmac_alloc_chan_resources;\r\nengine->device_free_chan_resources = rcar_dmac_free_chan_resources;\r\nengine->device_prep_dma_memcpy = rcar_dmac_prep_dma_memcpy;\r\nengine->device_prep_slave_sg = rcar_dmac_prep_slave_sg;\r\nengine->device_prep_dma_cyclic = rcar_dmac_prep_dma_cyclic;\r\nengine->device_config = rcar_dmac_device_config;\r\nengine->device_terminate_all = rcar_dmac_chan_terminate_all;\r\nengine->device_tx_status = rcar_dmac_tx_status;\r\nengine->device_issue_pending = rcar_dmac_issue_pending;\r\nret = dma_async_device_register(engine);\r\nif (ret < 0)\r\ngoto error;\r\nreturn 0;\r\nerror:\r\nof_dma_controller_free(pdev->dev.of_node);\r\npm_runtime_disable(&pdev->dev);\r\nreturn ret;\r\n}\r\nstatic int rcar_dmac_remove(struct platform_device *pdev)\r\n{\r\nstruct rcar_dmac *dmac = platform_get_drvdata(pdev);\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&dmac->engine);\r\npm_runtime_disable(&pdev->dev);\r\nreturn 0;\r\n}\r\nstatic void rcar_dmac_shutdown(struct platform_device *pdev)\r\n{\r\nstruct rcar_dmac *dmac = platform_get_drvdata(pdev);\r\nrcar_dmac_stop(dmac);\r\n}
