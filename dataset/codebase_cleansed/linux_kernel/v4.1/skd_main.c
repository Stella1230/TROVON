static inline u32 skd_reg_read32(struct skd_device *skdev, u32 offset)\r\n{\r\nu32 val;\r\nif (likely(skdev->dbg_level < 2))\r\nreturn readl(skdev->mem_map[1] + offset);\r\nelse {\r\nbarrier();\r\nval = readl(skdev->mem_map[1] + offset);\r\nbarrier();\r\npr_debug("%s:%s:%d offset %x = %x\n",\r\nskdev->name, __func__, __LINE__, offset, val);\r\nreturn val;\r\n}\r\n}\r\nstatic inline void skd_reg_write32(struct skd_device *skdev, u32 val,\r\nu32 offset)\r\n{\r\nif (likely(skdev->dbg_level < 2)) {\r\nwritel(val, skdev->mem_map[1] + offset);\r\nbarrier();\r\n} else {\r\nbarrier();\r\nwritel(val, skdev->mem_map[1] + offset);\r\nbarrier();\r\npr_debug("%s:%s:%d offset %x = %x\n",\r\nskdev->name, __func__, __LINE__, offset, val);\r\n}\r\n}\r\nstatic inline void skd_reg_write64(struct skd_device *skdev, u64 val,\r\nu32 offset)\r\n{\r\nif (likely(skdev->dbg_level < 2)) {\r\nwriteq(val, skdev->mem_map[1] + offset);\r\nbarrier();\r\n} else {\r\nbarrier();\r\nwriteq(val, skdev->mem_map[1] + offset);\r\nbarrier();\r\npr_debug("%s:%s:%d offset %x = %016llx\n",\r\nskdev->name, __func__, __LINE__, offset, val);\r\n}\r\n}\r\nstatic void skd_fail_all_pending(struct skd_device *skdev)\r\n{\r\nstruct request_queue *q = skdev->queue;\r\nstruct request *req;\r\nfor (;; ) {\r\nreq = blk_peek_request(q);\r\nif (req == NULL)\r\nbreak;\r\nblk_start_request(req);\r\n__blk_end_request_all(req, -EIO);\r\n}\r\n}\r\nstatic void\r\nskd_prep_rw_cdb(struct skd_scsi_request *scsi_req,\r\nint data_dir, unsigned lba,\r\nunsigned count)\r\n{\r\nif (data_dir == READ)\r\nscsi_req->cdb[0] = 0x28;\r\nelse\r\nscsi_req->cdb[0] = 0x2a;\r\nscsi_req->cdb[1] = 0;\r\nscsi_req->cdb[2] = (lba & 0xff000000) >> 24;\r\nscsi_req->cdb[3] = (lba & 0xff0000) >> 16;\r\nscsi_req->cdb[4] = (lba & 0xff00) >> 8;\r\nscsi_req->cdb[5] = (lba & 0xff);\r\nscsi_req->cdb[6] = 0;\r\nscsi_req->cdb[7] = (count & 0xff00) >> 8;\r\nscsi_req->cdb[8] = count & 0xff;\r\nscsi_req->cdb[9] = 0;\r\n}\r\nstatic void\r\nskd_prep_zerosize_flush_cdb(struct skd_scsi_request *scsi_req,\r\nstruct skd_request_context *skreq)\r\n{\r\nskreq->flush_cmd = 1;\r\nscsi_req->cdb[0] = 0x35;\r\nscsi_req->cdb[1] = 0;\r\nscsi_req->cdb[2] = 0;\r\nscsi_req->cdb[3] = 0;\r\nscsi_req->cdb[4] = 0;\r\nscsi_req->cdb[5] = 0;\r\nscsi_req->cdb[6] = 0;\r\nscsi_req->cdb[7] = 0;\r\nscsi_req->cdb[8] = 0;\r\nscsi_req->cdb[9] = 0;\r\n}\r\nstatic void\r\nskd_prep_discard_cdb(struct skd_scsi_request *scsi_req,\r\nstruct skd_request_context *skreq,\r\nstruct page *page,\r\nu32 lba, u32 count)\r\n{\r\nchar *buf;\r\nunsigned long len;\r\nstruct request *req;\r\nbuf = page_address(page);\r\nlen = SKD_DISCARD_CDB_LENGTH;\r\nscsi_req->cdb[0] = UNMAP;\r\nscsi_req->cdb[8] = len;\r\nput_unaligned_be16(6 + 16, &buf[0]);\r\nput_unaligned_be16(16, &buf[2]);\r\nput_unaligned_be64(lba, &buf[8]);\r\nput_unaligned_be32(count, &buf[16]);\r\nreq = skreq->req;\r\nblk_add_request_payload(req, page, len);\r\n}\r\nstatic void skd_request_fn(struct request_queue *q)\r\n{\r\nstruct skd_device *skdev = q->queuedata;\r\nstruct skd_fitmsg_context *skmsg = NULL;\r\nstruct fit_msg_hdr *fmh = NULL;\r\nstruct skd_request_context *skreq;\r\nstruct request *req = NULL;\r\nstruct skd_scsi_request *scsi_req;\r\nstruct page *page;\r\nunsigned long io_flags;\r\nint error;\r\nu32 lba;\r\nu32 count;\r\nint data_dir;\r\nu32 be_lba;\r\nu32 be_count;\r\nu64 be_dmaa;\r\nu64 cmdctxt;\r\nu32 timo_slot;\r\nvoid *cmd_ptr;\r\nint flush, fua;\r\nif (skdev->state != SKD_DRVR_STATE_ONLINE) {\r\nskd_request_fn_not_online(q);\r\nreturn;\r\n}\r\nif (blk_queue_stopped(skdev->queue)) {\r\nif (skdev->skmsg_free_list == NULL ||\r\nskdev->skreq_free_list == NULL ||\r\nskdev->in_flight >= skdev->queue_low_water_mark)\r\nreturn;\r\nqueue_flag_clear(QUEUE_FLAG_STOPPED, skdev->queue);\r\n}\r\nfor (;; ) {\r\nflush = fua = 0;\r\nreq = blk_peek_request(q);\r\nif (req == NULL)\r\nbreak;\r\nlba = (u32)blk_rq_pos(req);\r\ncount = blk_rq_sectors(req);\r\ndata_dir = rq_data_dir(req);\r\nio_flags = req->cmd_flags;\r\nif (io_flags & REQ_FLUSH)\r\nflush++;\r\nif (io_flags & REQ_FUA)\r\nfua++;\r\npr_debug("%s:%s:%d new req=%p lba=%u(0x%x) "\r\n"count=%u(0x%x) dir=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nreq, lba, lba, count, count, data_dir);\r\nif (skdev->in_flight >= skdev->cur_max_queue_depth) {\r\npr_debug("%s:%s:%d qdepth %d, limit %d\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->in_flight, skdev->cur_max_queue_depth);\r\nbreak;\r\n}\r\nskreq = skdev->skreq_free_list;\r\nif (skreq == NULL) {\r\npr_debug("%s:%s:%d Out of req=%p\n",\r\nskdev->name, __func__, __LINE__, q);\r\nbreak;\r\n}\r\nSKD_ASSERT(skreq->state == SKD_REQ_STATE_IDLE);\r\nSKD_ASSERT((skreq->id & SKD_ID_INCR) == 0);\r\nif (skmsg == NULL) {\r\nif (skdev->skmsg_free_list == NULL) {\r\npr_debug("%s:%s:%d Out of msg\n",\r\nskdev->name, __func__, __LINE__);\r\nbreak;\r\n}\r\n}\r\nskreq->flush_cmd = 0;\r\nskreq->n_sg = 0;\r\nskreq->sg_byte_count = 0;\r\nskreq->discard_page = 0;\r\nblk_start_request(req);\r\nskreq->req = req;\r\nskreq->fitmsg_id = 0;\r\nif (skmsg == NULL) {\r\nskmsg = skdev->skmsg_free_list;\r\nif (skmsg == NULL) {\r\npr_debug("%s:%s:%d Out of msg skdev=%p\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev);\r\nbreak;\r\n}\r\nSKD_ASSERT(skmsg->state == SKD_MSG_STATE_IDLE);\r\nSKD_ASSERT((skmsg->id & SKD_ID_INCR) == 0);\r\nskdev->skmsg_free_list = skmsg->next;\r\nskmsg->state = SKD_MSG_STATE_BUSY;\r\nskmsg->id += SKD_ID_INCR;\r\nfmh = (struct fit_msg_hdr *)skmsg->msg_buf;\r\nmemset(fmh, 0, sizeof(*fmh));\r\nfmh->protocol_id = FIT_PROTOCOL_ID_SOFIT;\r\nskmsg->length = sizeof(*fmh);\r\n}\r\nskreq->fitmsg_id = skmsg->id;\r\ncmd_ptr = &skmsg->msg_buf[skmsg->length];\r\nmemset(cmd_ptr, 0, 32);\r\nbe_lba = cpu_to_be32(lba);\r\nbe_count = cpu_to_be32(count);\r\nbe_dmaa = cpu_to_be64((u64)skreq->sksg_dma_address);\r\ncmdctxt = skreq->id + SKD_ID_INCR;\r\nscsi_req = cmd_ptr;\r\nscsi_req->hdr.tag = cmdctxt;\r\nscsi_req->hdr.sg_list_dma_address = be_dmaa;\r\nif (data_dir == READ)\r\nskreq->sg_data_dir = SKD_DATA_DIR_CARD_TO_HOST;\r\nelse\r\nskreq->sg_data_dir = SKD_DATA_DIR_HOST_TO_CARD;\r\nif (io_flags & REQ_DISCARD) {\r\npage = alloc_page(GFP_ATOMIC | __GFP_ZERO);\r\nif (!page) {\r\npr_err("request_fn:Page allocation failed.\n");\r\nskd_end_request(skdev, skreq, -ENOMEM);\r\nbreak;\r\n}\r\nskreq->discard_page = 1;\r\nreq->completion_data = page;\r\nskd_prep_discard_cdb(scsi_req, skreq, page, lba, count);\r\n} else if (flush == SKD_FLUSH_ZERO_SIZE_FIRST) {\r\nskd_prep_zerosize_flush_cdb(scsi_req, skreq);\r\nSKD_ASSERT(skreq->flush_cmd == 1);\r\n} else {\r\nskd_prep_rw_cdb(scsi_req, data_dir, lba, count);\r\n}\r\nif (fua)\r\nscsi_req->cdb[1] |= SKD_FUA_NV;\r\nif (!req->bio)\r\ngoto skip_sg;\r\nerror = skd_preop_sg_list(skdev, skreq);\r\nif (error != 0) {\r\npr_debug("%s:%s:%d error Out\n",\r\nskdev->name, __func__, __LINE__);\r\nskd_end_request(skdev, skreq, error);\r\ncontinue;\r\n}\r\nskip_sg:\r\nscsi_req->hdr.sg_list_len_bytes =\r\ncpu_to_be32(skreq->sg_byte_count);\r\nskdev->skreq_free_list = skreq->next;\r\nskreq->state = SKD_REQ_STATE_BUSY;\r\nskreq->id += SKD_ID_INCR;\r\nskmsg->length += sizeof(struct skd_scsi_request);\r\nfmh->num_protocol_cmds_coalesced++;\r\nskreq->timeout_stamp = skdev->timeout_stamp;\r\ntimo_slot = skreq->timeout_stamp & SKD_TIMEOUT_SLOT_MASK;\r\nskdev->timeout_slot[timo_slot]++;\r\nskdev->in_flight++;\r\npr_debug("%s:%s:%d req=0x%x busy=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskreq->id, skdev->in_flight);\r\nif (skmsg->length >= SKD_N_FITMSG_BYTES ||\r\nfmh->num_protocol_cmds_coalesced >= skd_max_req_per_msg) {\r\nskd_send_fitmsg(skdev, skmsg);\r\nskmsg = NULL;\r\nfmh = NULL;\r\n}\r\n}\r\nif (skmsg != NULL) {\r\nif (skmsg->length > sizeof(struct fit_msg_hdr)) {\r\npr_debug("%s:%s:%d sending msg=%p, len %d\n",\r\nskdev->name, __func__, __LINE__,\r\nskmsg, skmsg->length);\r\nskd_send_fitmsg(skdev, skmsg);\r\n} else {\r\nskmsg->state = SKD_MSG_STATE_IDLE;\r\nskmsg->id += SKD_ID_INCR;\r\nskmsg->next = skdev->skmsg_free_list;\r\nskdev->skmsg_free_list = skmsg;\r\n}\r\nskmsg = NULL;\r\nfmh = NULL;\r\n}\r\nif (req)\r\nblk_stop_queue(skdev->queue);\r\n}\r\nstatic void skd_end_request(struct skd_device *skdev,\r\nstruct skd_request_context *skreq, int error)\r\n{\r\nstruct request *req = skreq->req;\r\nunsigned int io_flags = req->cmd_flags;\r\nif ((io_flags & REQ_DISCARD) &&\r\n(skreq->discard_page == 1)) {\r\npr_debug("%s:%s:%d, free the page!",\r\nskdev->name, __func__, __LINE__);\r\n__free_page(req->completion_data);\r\n}\r\nif (unlikely(error)) {\r\nstruct request *req = skreq->req;\r\nchar *cmd = (rq_data_dir(req) == READ) ? "read" : "write";\r\nu32 lba = (u32)blk_rq_pos(req);\r\nu32 count = blk_rq_sectors(req);\r\npr_err("(%s): Error cmd=%s sect=%u count=%u id=0x%x\n",\r\nskd_name(skdev), cmd, lba, count, skreq->id);\r\n} else\r\npr_debug("%s:%s:%d id=0x%x error=%d\n",\r\nskdev->name, __func__, __LINE__, skreq->id, error);\r\n__blk_end_request_all(skreq->req, error);\r\n}\r\nstatic int skd_preop_sg_list(struct skd_device *skdev,\r\nstruct skd_request_context *skreq)\r\n{\r\nstruct request *req = skreq->req;\r\nint writing = skreq->sg_data_dir == SKD_DATA_DIR_HOST_TO_CARD;\r\nint pci_dir = writing ? PCI_DMA_TODEVICE : PCI_DMA_FROMDEVICE;\r\nstruct scatterlist *sg = &skreq->sg[0];\r\nint n_sg;\r\nint i;\r\nskreq->sg_byte_count = 0;\r\nn_sg = blk_rq_map_sg(skdev->queue, req, sg);\r\nif (n_sg <= 0)\r\nreturn -EINVAL;\r\nn_sg = pci_map_sg(skdev->pdev, sg, n_sg, pci_dir);\r\nif (n_sg <= 0)\r\nreturn -EINVAL;\r\nSKD_ASSERT(n_sg <= skdev->sgs_per_request);\r\nskreq->n_sg = n_sg;\r\nfor (i = 0; i < n_sg; i++) {\r\nstruct fit_sg_descriptor *sgd = &skreq->sksg_list[i];\r\nu32 cnt = sg_dma_len(&sg[i]);\r\nuint64_t dma_addr = sg_dma_address(&sg[i]);\r\nsgd->control = FIT_SGD_CONTROL_NOT_LAST;\r\nsgd->byte_count = cnt;\r\nskreq->sg_byte_count += cnt;\r\nsgd->host_side_addr = dma_addr;\r\nsgd->dev_side_addr = 0;\r\n}\r\nskreq->sksg_list[n_sg - 1].next_desc_ptr = 0LL;\r\nskreq->sksg_list[n_sg - 1].control = FIT_SGD_CONTROL_LAST;\r\nif (unlikely(skdev->dbg_level > 1)) {\r\npr_debug("%s:%s:%d skreq=%x sksg_list=%p sksg_dma=%llx\n",\r\nskdev->name, __func__, __LINE__,\r\nskreq->id, skreq->sksg_list, skreq->sksg_dma_address);\r\nfor (i = 0; i < n_sg; i++) {\r\nstruct fit_sg_descriptor *sgd = &skreq->sksg_list[i];\r\npr_debug("%s:%s:%d sg[%d] count=%u ctrl=0x%x "\r\n"addr=0x%llx next=0x%llx\n",\r\nskdev->name, __func__, __LINE__,\r\ni, sgd->byte_count, sgd->control,\r\nsgd->host_side_addr, sgd->next_desc_ptr);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void skd_postop_sg_list(struct skd_device *skdev,\r\nstruct skd_request_context *skreq)\r\n{\r\nint writing = skreq->sg_data_dir == SKD_DATA_DIR_HOST_TO_CARD;\r\nint pci_dir = writing ? PCI_DMA_TODEVICE : PCI_DMA_FROMDEVICE;\r\nskreq->sksg_list[skreq->n_sg - 1].next_desc_ptr =\r\nskreq->sksg_dma_address +\r\n((skreq->n_sg) * sizeof(struct fit_sg_descriptor));\r\npci_unmap_sg(skdev->pdev, &skreq->sg[0], skreq->n_sg, pci_dir);\r\n}\r\nstatic void skd_request_fn_not_online(struct request_queue *q)\r\n{\r\nstruct skd_device *skdev = q->queuedata;\r\nint error;\r\nSKD_ASSERT(skdev->state != SKD_DRVR_STATE_ONLINE);\r\nskd_log_skdev(skdev, "req_not_online");\r\nswitch (skdev->state) {\r\ncase SKD_DRVR_STATE_PAUSING:\r\ncase SKD_DRVR_STATE_PAUSED:\r\ncase SKD_DRVR_STATE_STARTING:\r\ncase SKD_DRVR_STATE_RESTARTING:\r\ncase SKD_DRVR_STATE_WAIT_BOOT:\r\ncase SKD_DRVR_STATE_BUSY:\r\ncase SKD_DRVR_STATE_BUSY_IMMINENT:\r\ncase SKD_DRVR_STATE_BUSY_ERASE:\r\ncase SKD_DRVR_STATE_DRAINING_TIMEOUT:\r\nreturn;\r\ncase SKD_DRVR_STATE_BUSY_SANITIZE:\r\ncase SKD_DRVR_STATE_STOPPING:\r\ncase SKD_DRVR_STATE_SYNCING:\r\ncase SKD_DRVR_STATE_FAULT:\r\ncase SKD_DRVR_STATE_DISAPPEARED:\r\ndefault:\r\nerror = -EIO;\r\nbreak;\r\n}\r\nskd_fail_all_pending(skdev);\r\n}\r\nstatic void skd_timer_tick(ulong arg)\r\n{\r\nstruct skd_device *skdev = (struct skd_device *)arg;\r\nu32 timo_slot;\r\nu32 overdue_timestamp;\r\nunsigned long reqflags;\r\nu32 state;\r\nif (skdev->state == SKD_DRVR_STATE_FAULT)\r\nreturn;\r\nspin_lock_irqsave(&skdev->lock, reqflags);\r\nstate = SKD_READL(skdev, FIT_STATUS);\r\nstate &= FIT_SR_DRIVE_STATE_MASK;\r\nif (state != skdev->drive_state)\r\nskd_isr_fwstate(skdev);\r\nif (skdev->state != SKD_DRVR_STATE_ONLINE) {\r\nskd_timer_tick_not_online(skdev);\r\ngoto timer_func_out;\r\n}\r\nskdev->timeout_stamp++;\r\ntimo_slot = skdev->timeout_stamp & SKD_TIMEOUT_SLOT_MASK;\r\nif (skdev->timeout_slot[timo_slot] == 0)\r\ngoto timer_func_out;\r\noverdue_timestamp = skdev->timeout_stamp - SKD_N_TIMEOUT_SLOT;\r\npr_debug("%s:%s:%d found %d timeouts, draining busy=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->timeout_slot[timo_slot], skdev->in_flight);\r\npr_err("(%s): Overdue IOs (%d), busy %d\n",\r\nskd_name(skdev), skdev->timeout_slot[timo_slot],\r\nskdev->in_flight);\r\nskdev->timer_countdown = SKD_DRAINING_TIMO;\r\nskdev->state = SKD_DRVR_STATE_DRAINING_TIMEOUT;\r\nskdev->timo_slot = timo_slot;\r\nblk_stop_queue(skdev->queue);\r\ntimer_func_out:\r\nmod_timer(&skdev->timer, (jiffies + HZ));\r\nspin_unlock_irqrestore(&skdev->lock, reqflags);\r\n}\r\nstatic void skd_timer_tick_not_online(struct skd_device *skdev)\r\n{\r\nswitch (skdev->state) {\r\ncase SKD_DRVR_STATE_IDLE:\r\ncase SKD_DRVR_STATE_LOAD:\r\nbreak;\r\ncase SKD_DRVR_STATE_BUSY_SANITIZE:\r\npr_debug("%s:%s:%d drive busy sanitize[%x], driver[%x]\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->drive_state, skdev->state);\r\nif (skdev->timer_countdown > 0) {\r\nskdev->timer_countdown--;\r\nreturn;\r\n}\r\nskd_recover_requests(skdev, 0);\r\nbreak;\r\ncase SKD_DRVR_STATE_BUSY:\r\ncase SKD_DRVR_STATE_BUSY_IMMINENT:\r\ncase SKD_DRVR_STATE_BUSY_ERASE:\r\npr_debug("%s:%s:%d busy[%x], countdown=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->state, skdev->timer_countdown);\r\nif (skdev->timer_countdown > 0) {\r\nskdev->timer_countdown--;\r\nreturn;\r\n}\r\npr_debug("%s:%s:%d busy[%x], timedout=%d, restarting device.",\r\nskdev->name, __func__, __LINE__,\r\nskdev->state, skdev->timer_countdown);\r\nskd_restart_device(skdev);\r\nbreak;\r\ncase SKD_DRVR_STATE_WAIT_BOOT:\r\ncase SKD_DRVR_STATE_STARTING:\r\nif (skdev->timer_countdown > 0) {\r\nskdev->timer_countdown--;\r\nreturn;\r\n}\r\nskdev->state = SKD_DRVR_STATE_FAULT;\r\npr_err("(%s): DriveFault Connect Timeout (%x)\n",\r\nskd_name(skdev), skdev->drive_state);\r\nblk_start_queue(skdev->queue);\r\nskdev->gendisk_on = -1;\r\nwake_up_interruptible(&skdev->waitq);\r\nbreak;\r\ncase SKD_DRVR_STATE_ONLINE:\r\nbreak;\r\ncase SKD_DRVR_STATE_PAUSING:\r\ncase SKD_DRVR_STATE_PAUSED:\r\nbreak;\r\ncase SKD_DRVR_STATE_DRAINING_TIMEOUT:\r\npr_debug("%s:%s:%d "\r\n"draining busy [%d] tick[%d] qdb[%d] tmls[%d]\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->timo_slot,\r\nskdev->timer_countdown,\r\nskdev->in_flight,\r\nskdev->timeout_slot[skdev->timo_slot]);\r\nif (skdev->timeout_slot[skdev->timo_slot] == 0) {\r\npr_debug("%s:%s:%d Slot drained, starting queue.\n",\r\nskdev->name, __func__, __LINE__);\r\nskdev->state = SKD_DRVR_STATE_ONLINE;\r\nblk_start_queue(skdev->queue);\r\nreturn;\r\n}\r\nif (skdev->timer_countdown > 0) {\r\nskdev->timer_countdown--;\r\nreturn;\r\n}\r\nskd_restart_device(skdev);\r\nbreak;\r\ncase SKD_DRVR_STATE_RESTARTING:\r\nif (skdev->timer_countdown > 0) {\r\nskdev->timer_countdown--;\r\nreturn;\r\n}\r\nskdev->state = SKD_DRVR_STATE_FAULT;\r\npr_err("(%s): DriveFault Reconnect Timeout (%x)\n",\r\nskd_name(skdev), skdev->drive_state);\r\nif ((skdev->drive_state == FIT_SR_DRIVE_SOFT_RESET) ||\r\n(skdev->drive_state == FIT_SR_DRIVE_FAULT) ||\r\n(skdev->drive_state == FIT_SR_DRIVE_STATE_MASK))\r\nskd_recover_requests(skdev, 0);\r\nelse {\r\npr_err("(%s): Disable BusMaster (%x)\n",\r\nskd_name(skdev), skdev->drive_state);\r\npci_disable_device(skdev->pdev);\r\nskd_disable_interrupts(skdev);\r\nskd_recover_requests(skdev, 0);\r\n}\r\nblk_start_queue(skdev->queue);\r\nskdev->gendisk_on = -1;\r\nwake_up_interruptible(&skdev->waitq);\r\nbreak;\r\ncase SKD_DRVR_STATE_RESUMING:\r\ncase SKD_DRVR_STATE_STOPPING:\r\ncase SKD_DRVR_STATE_SYNCING:\r\ncase SKD_DRVR_STATE_FAULT:\r\ncase SKD_DRVR_STATE_DISAPPEARED:\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic int skd_start_timer(struct skd_device *skdev)\r\n{\r\nint rc;\r\ninit_timer(&skdev->timer);\r\nsetup_timer(&skdev->timer, skd_timer_tick, (ulong)skdev);\r\nrc = mod_timer(&skdev->timer, (jiffies + HZ));\r\nif (rc)\r\npr_err("%s: failed to start timer %d\n",\r\n__func__, rc);\r\nreturn rc;\r\n}\r\nstatic void skd_kill_timer(struct skd_device *skdev)\r\n{\r\ndel_timer_sync(&skdev->timer);\r\n}\r\nstatic int skd_bdev_ioctl(struct block_device *bdev, fmode_t mode,\r\nuint cmd_in, ulong arg)\r\n{\r\nint rc = 0;\r\nstruct gendisk *disk = bdev->bd_disk;\r\nstruct skd_device *skdev = disk->private_data;\r\nvoid __user *p = (void *)arg;\r\npr_debug("%s:%s:%d %s: CMD[%s] ioctl mode 0x%x, cmd 0x%x arg %0lx\n",\r\nskdev->name, __func__, __LINE__,\r\ndisk->disk_name, current->comm, mode, cmd_in, arg);\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nswitch (cmd_in) {\r\ncase SG_SET_TIMEOUT:\r\ncase SG_GET_TIMEOUT:\r\ncase SG_GET_VERSION_NUM:\r\nrc = scsi_cmd_ioctl(disk->queue, disk, mode, cmd_in, p);\r\nbreak;\r\ncase SG_IO:\r\nrc = skd_ioctl_sg_io(skdev, mode, p);\r\nbreak;\r\ndefault:\r\nrc = -ENOTTY;\r\nbreak;\r\n}\r\npr_debug("%s:%s:%d %s: completion rc %d\n",\r\nskdev->name, __func__, __LINE__, disk->disk_name, rc);\r\nreturn rc;\r\n}\r\nstatic int skd_ioctl_sg_io(struct skd_device *skdev, fmode_t mode,\r\nvoid __user *argp)\r\n{\r\nint rc;\r\nstruct skd_sg_io sksgio;\r\nmemset(&sksgio, 0, sizeof(sksgio));\r\nsksgio.mode = mode;\r\nsksgio.argp = argp;\r\nsksgio.iov = &sksgio.no_iov_iov;\r\nswitch (skdev->state) {\r\ncase SKD_DRVR_STATE_ONLINE:\r\ncase SKD_DRVR_STATE_BUSY_IMMINENT:\r\nbreak;\r\ndefault:\r\npr_debug("%s:%s:%d drive not online\n",\r\nskdev->name, __func__, __LINE__);\r\nrc = -ENXIO;\r\ngoto out;\r\n}\r\nrc = skd_sg_io_get_and_check_args(skdev, &sksgio);\r\nif (rc)\r\ngoto out;\r\nrc = skd_sg_io_obtain_skspcl(skdev, &sksgio);\r\nif (rc)\r\ngoto out;\r\nrc = skd_sg_io_prep_buffering(skdev, &sksgio);\r\nif (rc)\r\ngoto out;\r\nrc = skd_sg_io_copy_buffer(skdev, &sksgio, SG_DXFER_TO_DEV);\r\nif (rc)\r\ngoto out;\r\nrc = skd_sg_io_send_fitmsg(skdev, &sksgio);\r\nif (rc)\r\ngoto out;\r\nrc = skd_sg_io_await(skdev, &sksgio);\r\nif (rc)\r\ngoto out;\r\nrc = skd_sg_io_copy_buffer(skdev, &sksgio, SG_DXFER_FROM_DEV);\r\nif (rc)\r\ngoto out;\r\nrc = skd_sg_io_put_status(skdev, &sksgio);\r\nif (rc)\r\ngoto out;\r\nrc = 0;\r\nout:\r\nskd_sg_io_release_skspcl(skdev, &sksgio);\r\nif (sksgio.iov != NULL && sksgio.iov != &sksgio.no_iov_iov)\r\nkfree(sksgio.iov);\r\nreturn rc;\r\n}\r\nstatic int skd_sg_io_get_and_check_args(struct skd_device *skdev,\r\nstruct skd_sg_io *sksgio)\r\n{\r\nstruct sg_io_hdr *sgp = &sksgio->sg;\r\nint i, acc;\r\nif (!access_ok(VERIFY_WRITE, sksgio->argp, sizeof(sg_io_hdr_t))) {\r\npr_debug("%s:%s:%d access sg failed %p\n",\r\nskdev->name, __func__, __LINE__, sksgio->argp);\r\nreturn -EFAULT;\r\n}\r\nif (__copy_from_user(sgp, sksgio->argp, sizeof(sg_io_hdr_t))) {\r\npr_debug("%s:%s:%d copy_from_user sg failed %p\n",\r\nskdev->name, __func__, __LINE__, sksgio->argp);\r\nreturn -EFAULT;\r\n}\r\nif (sgp->interface_id != SG_INTERFACE_ID_ORIG) {\r\npr_debug("%s:%s:%d interface_id invalid 0x%x\n",\r\nskdev->name, __func__, __LINE__, sgp->interface_id);\r\nreturn -EINVAL;\r\n}\r\nif (sgp->cmd_len > sizeof(sksgio->cdb)) {\r\npr_debug("%s:%s:%d cmd_len invalid %d\n",\r\nskdev->name, __func__, __LINE__, sgp->cmd_len);\r\nreturn -EINVAL;\r\n}\r\nif (sgp->iovec_count > 256) {\r\npr_debug("%s:%s:%d iovec_count invalid %d\n",\r\nskdev->name, __func__, __LINE__, sgp->iovec_count);\r\nreturn -EINVAL;\r\n}\r\nif (sgp->dxfer_len > (PAGE_SIZE * SKD_N_SG_PER_SPECIAL)) {\r\npr_debug("%s:%s:%d dxfer_len invalid %d\n",\r\nskdev->name, __func__, __LINE__, sgp->dxfer_len);\r\nreturn -EINVAL;\r\n}\r\nswitch (sgp->dxfer_direction) {\r\ncase SG_DXFER_NONE:\r\nacc = -1;\r\nbreak;\r\ncase SG_DXFER_TO_DEV:\r\nacc = VERIFY_READ;\r\nbreak;\r\ncase SG_DXFER_FROM_DEV:\r\ncase SG_DXFER_TO_FROM_DEV:\r\nacc = VERIFY_WRITE;\r\nbreak;\r\ndefault:\r\npr_debug("%s:%s:%d dxfer_dir invalid %d\n",\r\nskdev->name, __func__, __LINE__, sgp->dxfer_direction);\r\nreturn -EINVAL;\r\n}\r\nif (copy_from_user(sksgio->cdb, sgp->cmdp, sgp->cmd_len)) {\r\npr_debug("%s:%s:%d copy_from_user cmdp failed %p\n",\r\nskdev->name, __func__, __LINE__, sgp->cmdp);\r\nreturn -EFAULT;\r\n}\r\nif (sgp->mx_sb_len != 0) {\r\nif (!access_ok(VERIFY_WRITE, sgp->sbp, sgp->mx_sb_len)) {\r\npr_debug("%s:%s:%d access sbp failed %p\n",\r\nskdev->name, __func__, __LINE__, sgp->sbp);\r\nreturn -EFAULT;\r\n}\r\n}\r\nif (sgp->iovec_count == 0) {\r\nsksgio->iov[0].iov_base = sgp->dxferp;\r\nsksgio->iov[0].iov_len = sgp->dxfer_len;\r\nsksgio->iovcnt = 1;\r\nsksgio->dxfer_len = sgp->dxfer_len;\r\n} else {\r\nstruct sg_iovec *iov;\r\nuint nbytes = sizeof(*iov) * sgp->iovec_count;\r\nsize_t iov_data_len;\r\niov = kmalloc(nbytes, GFP_KERNEL);\r\nif (iov == NULL) {\r\npr_debug("%s:%s:%d alloc iovec failed %d\n",\r\nskdev->name, __func__, __LINE__,\r\nsgp->iovec_count);\r\nreturn -ENOMEM;\r\n}\r\nsksgio->iov = iov;\r\nsksgio->iovcnt = sgp->iovec_count;\r\nif (copy_from_user(iov, sgp->dxferp, nbytes)) {\r\npr_debug("%s:%s:%d copy_from_user iovec failed %p\n",\r\nskdev->name, __func__, __LINE__, sgp->dxferp);\r\nreturn -EFAULT;\r\n}\r\niov_data_len = 0;\r\nfor (i = 0; i < sgp->iovec_count; i++) {\r\nif (iov_data_len + iov[i].iov_len < iov_data_len)\r\nreturn -EINVAL;\r\niov_data_len += iov[i].iov_len;\r\n}\r\nif (sgp->dxfer_len < iov_data_len) {\r\nsksgio->iovcnt = iov_shorten((struct iovec *)iov,\r\nsgp->iovec_count,\r\nsgp->dxfer_len);\r\nsksgio->dxfer_len = sgp->dxfer_len;\r\n} else\r\nsksgio->dxfer_len = iov_data_len;\r\n}\r\nif (sgp->dxfer_direction != SG_DXFER_NONE) {\r\nstruct sg_iovec *iov = sksgio->iov;\r\nfor (i = 0; i < sksgio->iovcnt; i++, iov++) {\r\nif (!access_ok(acc, iov->iov_base, iov->iov_len)) {\r\npr_debug("%s:%s:%d access data failed %p/%d\n",\r\nskdev->name, __func__, __LINE__,\r\niov->iov_base, (int)iov->iov_len);\r\nreturn -EFAULT;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int skd_sg_io_obtain_skspcl(struct skd_device *skdev,\r\nstruct skd_sg_io *sksgio)\r\n{\r\nstruct skd_special_context *skspcl = NULL;\r\nint rc;\r\nfor (;;) {\r\nulong flags;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\nskspcl = skdev->skspcl_free_list;\r\nif (skspcl != NULL) {\r\nskdev->skspcl_free_list =\r\n(struct skd_special_context *)skspcl->req.next;\r\nskspcl->req.id += SKD_ID_INCR;\r\nskspcl->req.state = SKD_REQ_STATE_SETUP;\r\nskspcl->orphaned = 0;\r\nskspcl->req.n_sg = 0;\r\n}\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nif (skspcl != NULL) {\r\nrc = 0;\r\nbreak;\r\n}\r\npr_debug("%s:%s:%d blocking\n",\r\nskdev->name, __func__, __LINE__);\r\nrc = wait_event_interruptible_timeout(\r\nskdev->waitq,\r\n(skdev->skspcl_free_list != NULL),\r\nmsecs_to_jiffies(sksgio->sg.timeout));\r\npr_debug("%s:%s:%d unblocking, rc=%d\n",\r\nskdev->name, __func__, __LINE__, rc);\r\nif (rc <= 0) {\r\nif (rc == 0)\r\nrc = -ETIMEDOUT;\r\nelse\r\nrc = -EINTR;\r\nbreak;\r\n}\r\n}\r\nsksgio->skspcl = skspcl;\r\nreturn rc;\r\n}\r\nstatic int skd_skreq_prep_buffering(struct skd_device *skdev,\r\nstruct skd_request_context *skreq,\r\nu32 dxfer_len)\r\n{\r\nu32 resid = dxfer_len;\r\nresid += (-resid) & 3;\r\nskreq->sg_byte_count = resid;\r\nskreq->n_sg = 0;\r\nwhile (resid > 0) {\r\nu32 nbytes = PAGE_SIZE;\r\nu32 ix = skreq->n_sg;\r\nstruct scatterlist *sg = &skreq->sg[ix];\r\nstruct fit_sg_descriptor *sksg = &skreq->sksg_list[ix];\r\nstruct page *page;\r\nif (nbytes > resid)\r\nnbytes = resid;\r\npage = alloc_page(GFP_KERNEL);\r\nif (page == NULL)\r\nreturn -ENOMEM;\r\nsg_set_page(sg, page, nbytes, 0);\r\nsksg->control = FIT_SGD_CONTROL_NOT_LAST;\r\nsksg->byte_count = nbytes;\r\nsksg->host_side_addr = sg_phys(sg);\r\nsksg->dev_side_addr = 0;\r\nsksg->next_desc_ptr = skreq->sksg_dma_address +\r\n(ix + 1) * sizeof(*sksg);\r\nskreq->n_sg++;\r\nresid -= nbytes;\r\n}\r\nif (skreq->n_sg > 0) {\r\nu32 ix = skreq->n_sg - 1;\r\nstruct fit_sg_descriptor *sksg = &skreq->sksg_list[ix];\r\nsksg->control = FIT_SGD_CONTROL_LAST;\r\nsksg->next_desc_ptr = 0;\r\n}\r\nif (unlikely(skdev->dbg_level > 1)) {\r\nu32 i;\r\npr_debug("%s:%s:%d skreq=%x sksg_list=%p sksg_dma=%llx\n",\r\nskdev->name, __func__, __LINE__,\r\nskreq->id, skreq->sksg_list, skreq->sksg_dma_address);\r\nfor (i = 0; i < skreq->n_sg; i++) {\r\nstruct fit_sg_descriptor *sgd = &skreq->sksg_list[i];\r\npr_debug("%s:%s:%d sg[%d] count=%u ctrl=0x%x "\r\n"addr=0x%llx next=0x%llx\n",\r\nskdev->name, __func__, __LINE__,\r\ni, sgd->byte_count, sgd->control,\r\nsgd->host_side_addr, sgd->next_desc_ptr);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int skd_sg_io_prep_buffering(struct skd_device *skdev,\r\nstruct skd_sg_io *sksgio)\r\n{\r\nstruct skd_special_context *skspcl = sksgio->skspcl;\r\nstruct skd_request_context *skreq = &skspcl->req;\r\nu32 dxfer_len = sksgio->dxfer_len;\r\nint rc;\r\nrc = skd_skreq_prep_buffering(skdev, skreq, dxfer_len);\r\nreturn rc;\r\n}\r\nstatic int skd_sg_io_copy_buffer(struct skd_device *skdev,\r\nstruct skd_sg_io *sksgio, int dxfer_dir)\r\n{\r\nstruct skd_special_context *skspcl = sksgio->skspcl;\r\nu32 iov_ix = 0;\r\nstruct sg_iovec curiov;\r\nu32 sksg_ix = 0;\r\nu8 *bufp = NULL;\r\nu32 buf_len = 0;\r\nu32 resid = sksgio->dxfer_len;\r\nint rc;\r\ncuriov.iov_len = 0;\r\ncuriov.iov_base = NULL;\r\nif (dxfer_dir != sksgio->sg.dxfer_direction) {\r\nif (dxfer_dir != SG_DXFER_TO_DEV ||\r\nsksgio->sg.dxfer_direction != SG_DXFER_TO_FROM_DEV)\r\nreturn 0;\r\n}\r\nwhile (resid > 0) {\r\nu32 nbytes = PAGE_SIZE;\r\nif (curiov.iov_len == 0) {\r\ncuriov = sksgio->iov[iov_ix++];\r\ncontinue;\r\n}\r\nif (buf_len == 0) {\r\nstruct page *page;\r\npage = sg_page(&skspcl->req.sg[sksg_ix++]);\r\nbufp = page_address(page);\r\nbuf_len = PAGE_SIZE;\r\n}\r\nnbytes = min_t(u32, nbytes, resid);\r\nnbytes = min_t(u32, nbytes, curiov.iov_len);\r\nnbytes = min_t(u32, nbytes, buf_len);\r\nif (dxfer_dir == SG_DXFER_TO_DEV)\r\nrc = __copy_from_user(bufp, curiov.iov_base, nbytes);\r\nelse\r\nrc = __copy_to_user(curiov.iov_base, bufp, nbytes);\r\nif (rc)\r\nreturn -EFAULT;\r\nresid -= nbytes;\r\ncuriov.iov_len -= nbytes;\r\ncuriov.iov_base += nbytes;\r\nbuf_len -= nbytes;\r\n}\r\nreturn 0;\r\n}\r\nstatic int skd_sg_io_send_fitmsg(struct skd_device *skdev,\r\nstruct skd_sg_io *sksgio)\r\n{\r\nstruct skd_special_context *skspcl = sksgio->skspcl;\r\nstruct fit_msg_hdr *fmh = (struct fit_msg_hdr *)skspcl->msg_buf;\r\nstruct skd_scsi_request *scsi_req = (struct skd_scsi_request *)&fmh[1];\r\nmemset(skspcl->msg_buf, 0, SKD_N_SPECIAL_FITMSG_BYTES);\r\nfmh->protocol_id = FIT_PROTOCOL_ID_SOFIT;\r\nfmh->num_protocol_cmds_coalesced = 1;\r\nif (sksgio->sg.dxfer_direction != SG_DXFER_NONE)\r\nscsi_req->hdr.sg_list_dma_address =\r\ncpu_to_be64(skspcl->req.sksg_dma_address);\r\nscsi_req->hdr.tag = skspcl->req.id;\r\nscsi_req->hdr.sg_list_len_bytes =\r\ncpu_to_be32(skspcl->req.sg_byte_count);\r\nmemcpy(scsi_req->cdb, sksgio->cdb, sizeof(scsi_req->cdb));\r\nskspcl->req.state = SKD_REQ_STATE_BUSY;\r\nskd_send_special_fitmsg(skdev, skspcl);\r\nreturn 0;\r\n}\r\nstatic int skd_sg_io_await(struct skd_device *skdev, struct skd_sg_io *sksgio)\r\n{\r\nunsigned long flags;\r\nint rc;\r\nrc = wait_event_interruptible_timeout(skdev->waitq,\r\n(sksgio->skspcl->req.state !=\r\nSKD_REQ_STATE_BUSY),\r\nmsecs_to_jiffies(sksgio->sg.\r\ntimeout));\r\nspin_lock_irqsave(&skdev->lock, flags);\r\nif (sksgio->skspcl->req.state == SKD_REQ_STATE_ABORTED) {\r\npr_debug("%s:%s:%d skspcl %p aborted\n",\r\nskdev->name, __func__, __LINE__, sksgio->skspcl);\r\nsksgio->skspcl->req.completion.status =\r\nSAM_STAT_CHECK_CONDITION;\r\nmemset(&sksgio->skspcl->req.err_info, 0,\r\nsizeof(sksgio->skspcl->req.err_info));\r\nsksgio->skspcl->req.err_info.type = 0x70;\r\nsksgio->skspcl->req.err_info.key = ABORTED_COMMAND;\r\nsksgio->skspcl->req.err_info.code = 0x44;\r\nsksgio->skspcl->req.err_info.qual = 0;\r\nrc = 0;\r\n} else if (sksgio->skspcl->req.state != SKD_REQ_STATE_BUSY)\r\nrc = 0;\r\nelse {\r\nsksgio->skspcl->orphaned = 1;\r\nsksgio->skspcl = NULL;\r\nif (rc == 0) {\r\npr_debug("%s:%s:%d timed out %p (%u ms)\n",\r\nskdev->name, __func__, __LINE__,\r\nsksgio, sksgio->sg.timeout);\r\nrc = -ETIMEDOUT;\r\n} else {\r\npr_debug("%s:%s:%d cntlc %p\n",\r\nskdev->name, __func__, __LINE__, sksgio);\r\nrc = -EINTR;\r\n}\r\n}\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nreturn rc;\r\n}\r\nstatic int skd_sg_io_put_status(struct skd_device *skdev,\r\nstruct skd_sg_io *sksgio)\r\n{\r\nstruct sg_io_hdr *sgp = &sksgio->sg;\r\nstruct skd_special_context *skspcl = sksgio->skspcl;\r\nint resid = 0;\r\nu32 nb = be32_to_cpu(skspcl->req.completion.num_returned_bytes);\r\nsgp->status = skspcl->req.completion.status;\r\nresid = sksgio->dxfer_len - nb;\r\nsgp->masked_status = sgp->status & STATUS_MASK;\r\nsgp->msg_status = 0;\r\nsgp->host_status = 0;\r\nsgp->driver_status = 0;\r\nsgp->resid = resid;\r\nif (sgp->masked_status || sgp->host_status || sgp->driver_status)\r\nsgp->info |= SG_INFO_CHECK;\r\npr_debug("%s:%s:%d status %x masked %x resid 0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nsgp->status, sgp->masked_status, sgp->resid);\r\nif (sgp->masked_status == SAM_STAT_CHECK_CONDITION) {\r\nif (sgp->mx_sb_len > 0) {\r\nstruct fit_comp_error_info *ei = &skspcl->req.err_info;\r\nu32 nbytes = sizeof(*ei);\r\nnbytes = min_t(u32, nbytes, sgp->mx_sb_len);\r\nsgp->sb_len_wr = nbytes;\r\nif (__copy_to_user(sgp->sbp, ei, nbytes)) {\r\npr_debug("%s:%s:%d copy_to_user sense failed %p\n",\r\nskdev->name, __func__, __LINE__,\r\nsgp->sbp);\r\nreturn -EFAULT;\r\n}\r\n}\r\n}\r\nif (__copy_to_user(sksgio->argp, sgp, sizeof(sg_io_hdr_t))) {\r\npr_debug("%s:%s:%d copy_to_user sg failed %p\n",\r\nskdev->name, __func__, __LINE__, sksgio->argp);\r\nreturn -EFAULT;\r\n}\r\nreturn 0;\r\n}\r\nstatic int skd_sg_io_release_skspcl(struct skd_device *skdev,\r\nstruct skd_sg_io *sksgio)\r\n{\r\nstruct skd_special_context *skspcl = sksgio->skspcl;\r\nif (skspcl != NULL) {\r\nulong flags;\r\nsksgio->skspcl = NULL;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\nskd_release_special(skdev, skspcl);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nstatic int skd_format_internal_skspcl(struct skd_device *skdev)\r\n{\r\nstruct skd_special_context *skspcl = &skdev->internal_skspcl;\r\nstruct fit_sg_descriptor *sgd = &skspcl->req.sksg_list[0];\r\nstruct fit_msg_hdr *fmh;\r\nuint64_t dma_address;\r\nstruct skd_scsi_request *scsi;\r\nfmh = (struct fit_msg_hdr *)&skspcl->msg_buf[0];\r\nfmh->protocol_id = FIT_PROTOCOL_ID_SOFIT;\r\nfmh->num_protocol_cmds_coalesced = 1;\r\nscsi = (struct skd_scsi_request *)&skspcl->msg_buf[64];\r\nmemset(scsi, 0, sizeof(*scsi));\r\ndma_address = skspcl->req.sksg_dma_address;\r\nscsi->hdr.sg_list_dma_address = cpu_to_be64(dma_address);\r\nsgd->control = FIT_SGD_CONTROL_LAST;\r\nsgd->byte_count = 0;\r\nsgd->host_side_addr = skspcl->db_dma_address;\r\nsgd->dev_side_addr = 0;\r\nsgd->next_desc_ptr = 0LL;\r\nreturn 1;\r\n}\r\nstatic void skd_send_internal_skspcl(struct skd_device *skdev,\r\nstruct skd_special_context *skspcl,\r\nu8 opcode)\r\n{\r\nstruct fit_sg_descriptor *sgd = &skspcl->req.sksg_list[0];\r\nstruct skd_scsi_request *scsi;\r\nunsigned char *buf = skspcl->data_buf;\r\nint i;\r\nif (skspcl->req.state != SKD_REQ_STATE_IDLE)\r\nreturn;\r\nSKD_ASSERT((skspcl->req.id & SKD_ID_INCR) == 0);\r\nskspcl->req.state = SKD_REQ_STATE_BUSY;\r\nskspcl->req.id += SKD_ID_INCR;\r\nscsi = (struct skd_scsi_request *)&skspcl->msg_buf[64];\r\nscsi->hdr.tag = skspcl->req.id;\r\nmemset(scsi->cdb, 0, sizeof(scsi->cdb));\r\nswitch (opcode) {\r\ncase TEST_UNIT_READY:\r\nscsi->cdb[0] = TEST_UNIT_READY;\r\nsgd->byte_count = 0;\r\nscsi->hdr.sg_list_len_bytes = 0;\r\nbreak;\r\ncase READ_CAPACITY:\r\nscsi->cdb[0] = READ_CAPACITY;\r\nsgd->byte_count = SKD_N_READ_CAP_BYTES;\r\nscsi->hdr.sg_list_len_bytes = cpu_to_be32(sgd->byte_count);\r\nbreak;\r\ncase INQUIRY:\r\nscsi->cdb[0] = INQUIRY;\r\nscsi->cdb[1] = 0x01;\r\nscsi->cdb[2] = 0x80;\r\nscsi->cdb[4] = 0x10;\r\nsgd->byte_count = 16;\r\nscsi->hdr.sg_list_len_bytes = cpu_to_be32(sgd->byte_count);\r\nbreak;\r\ncase SYNCHRONIZE_CACHE:\r\nscsi->cdb[0] = SYNCHRONIZE_CACHE;\r\nsgd->byte_count = 0;\r\nscsi->hdr.sg_list_len_bytes = 0;\r\nbreak;\r\ncase WRITE_BUFFER:\r\nscsi->cdb[0] = WRITE_BUFFER;\r\nscsi->cdb[1] = 0x02;\r\nscsi->cdb[7] = (WR_BUF_SIZE & 0xFF00) >> 8;\r\nscsi->cdb[8] = WR_BUF_SIZE & 0xFF;\r\nsgd->byte_count = WR_BUF_SIZE;\r\nscsi->hdr.sg_list_len_bytes = cpu_to_be32(sgd->byte_count);\r\nfor (i = 0; i < sgd->byte_count; i++)\r\nbuf[i] = i & 0xFF;\r\nbreak;\r\ncase READ_BUFFER:\r\nscsi->cdb[0] = READ_BUFFER;\r\nscsi->cdb[1] = 0x02;\r\nscsi->cdb[7] = (WR_BUF_SIZE & 0xFF00) >> 8;\r\nscsi->cdb[8] = WR_BUF_SIZE & 0xFF;\r\nsgd->byte_count = WR_BUF_SIZE;\r\nscsi->hdr.sg_list_len_bytes = cpu_to_be32(sgd->byte_count);\r\nmemset(skspcl->data_buf, 0, sgd->byte_count);\r\nbreak;\r\ndefault:\r\nSKD_ASSERT("Don't know what to send");\r\nreturn;\r\n}\r\nskd_send_special_fitmsg(skdev, skspcl);\r\n}\r\nstatic void skd_refresh_device_data(struct skd_device *skdev)\r\n{\r\nstruct skd_special_context *skspcl = &skdev->internal_skspcl;\r\nskd_send_internal_skspcl(skdev, skspcl, TEST_UNIT_READY);\r\n}\r\nstatic int skd_chk_read_buf(struct skd_device *skdev,\r\nstruct skd_special_context *skspcl)\r\n{\r\nunsigned char *buf = skspcl->data_buf;\r\nint i;\r\nfor (i = 0; i < WR_BUF_SIZE; i++)\r\nif (buf[i] != (i & 0xFF))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void skd_log_check_status(struct skd_device *skdev, u8 status, u8 key,\r\nu8 code, u8 qual, u8 fruc)\r\n{\r\nif ((status == SAM_STAT_CHECK_CONDITION) && (key == 0x02)\r\n&& (code == 0x04) && (qual == 0x06)) {\r\npr_err("(%s): *** LOST_WRITE_DATA ERROR *** key/asc/"\r\n"ascq/fruc %02x/%02x/%02x/%02x\n",\r\nskd_name(skdev), key, code, qual, fruc);\r\n}\r\n}\r\nstatic void skd_complete_internal(struct skd_device *skdev,\r\nvolatile struct fit_completion_entry_v1\r\n*skcomp,\r\nvolatile struct fit_comp_error_info *skerr,\r\nstruct skd_special_context *skspcl)\r\n{\r\nu8 *buf = skspcl->data_buf;\r\nu8 status;\r\nint i;\r\nstruct skd_scsi_request *scsi =\r\n(struct skd_scsi_request *)&skspcl->msg_buf[64];\r\nSKD_ASSERT(skspcl == &skdev->internal_skspcl);\r\npr_debug("%s:%s:%d complete internal %x\n",\r\nskdev->name, __func__, __LINE__, scsi->cdb[0]);\r\nskspcl->req.completion = *skcomp;\r\nskspcl->req.state = SKD_REQ_STATE_IDLE;\r\nskspcl->req.id += SKD_ID_INCR;\r\nstatus = skspcl->req.completion.status;\r\nskd_log_check_status(skdev, status, skerr->key, skerr->code,\r\nskerr->qual, skerr->fruc);\r\nswitch (scsi->cdb[0]) {\r\ncase TEST_UNIT_READY:\r\nif (status == SAM_STAT_GOOD)\r\nskd_send_internal_skspcl(skdev, skspcl, WRITE_BUFFER);\r\nelse if ((status == SAM_STAT_CHECK_CONDITION) &&\r\n(skerr->key == MEDIUM_ERROR))\r\nskd_send_internal_skspcl(skdev, skspcl, WRITE_BUFFER);\r\nelse {\r\nif (skdev->state == SKD_DRVR_STATE_STOPPING) {\r\npr_debug("%s:%s:%d TUR failed, don't send anymore state 0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->state);\r\nreturn;\r\n}\r\npr_debug("%s:%s:%d **** TUR failed, retry skerr\n",\r\nskdev->name, __func__, __LINE__);\r\nskd_send_internal_skspcl(skdev, skspcl, 0x00);\r\n}\r\nbreak;\r\ncase WRITE_BUFFER:\r\nif (status == SAM_STAT_GOOD)\r\nskd_send_internal_skspcl(skdev, skspcl, READ_BUFFER);\r\nelse {\r\nif (skdev->state == SKD_DRVR_STATE_STOPPING) {\r\npr_debug("%s:%s:%d write buffer failed, don't send anymore state 0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->state);\r\nreturn;\r\n}\r\npr_debug("%s:%s:%d **** write buffer failed, retry skerr\n",\r\nskdev->name, __func__, __LINE__);\r\nskd_send_internal_skspcl(skdev, skspcl, 0x00);\r\n}\r\nbreak;\r\ncase READ_BUFFER:\r\nif (status == SAM_STAT_GOOD) {\r\nif (skd_chk_read_buf(skdev, skspcl) == 0)\r\nskd_send_internal_skspcl(skdev, skspcl,\r\nREAD_CAPACITY);\r\nelse {\r\npr_err(\r\n"(%s):*** W/R Buffer mismatch %d ***\n",\r\nskd_name(skdev), skdev->connect_retries);\r\nif (skdev->connect_retries <\r\nSKD_MAX_CONNECT_RETRIES) {\r\nskdev->connect_retries++;\r\nskd_soft_reset(skdev);\r\n} else {\r\npr_err(\r\n"(%s): W/R Buffer Connect Error\n",\r\nskd_name(skdev));\r\nreturn;\r\n}\r\n}\r\n} else {\r\nif (skdev->state == SKD_DRVR_STATE_STOPPING) {\r\npr_debug("%s:%s:%d "\r\n"read buffer failed, don't send anymore state 0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->state);\r\nreturn;\r\n}\r\npr_debug("%s:%s:%d "\r\n"**** read buffer failed, retry skerr\n",\r\nskdev->name, __func__, __LINE__);\r\nskd_send_internal_skspcl(skdev, skspcl, 0x00);\r\n}\r\nbreak;\r\ncase READ_CAPACITY:\r\nskdev->read_cap_is_valid = 0;\r\nif (status == SAM_STAT_GOOD) {\r\nskdev->read_cap_last_lba =\r\n(buf[0] << 24) | (buf[1] << 16) |\r\n(buf[2] << 8) | buf[3];\r\nskdev->read_cap_blocksize =\r\n(buf[4] << 24) | (buf[5] << 16) |\r\n(buf[6] << 8) | buf[7];\r\npr_debug("%s:%s:%d last lba %d, bs %d\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->read_cap_last_lba,\r\nskdev->read_cap_blocksize);\r\nset_capacity(skdev->disk, skdev->read_cap_last_lba + 1);\r\nskdev->read_cap_is_valid = 1;\r\nskd_send_internal_skspcl(skdev, skspcl, INQUIRY);\r\n} else if ((status == SAM_STAT_CHECK_CONDITION) &&\r\n(skerr->key == MEDIUM_ERROR)) {\r\nskdev->read_cap_last_lba = ~0;\r\nset_capacity(skdev->disk, skdev->read_cap_last_lba + 1);\r\npr_debug("%s:%s:%d "\r\n"**** MEDIUM ERROR caused READCAP to fail, ignore failure and continue to inquiry\n",\r\nskdev->name, __func__, __LINE__);\r\nskd_send_internal_skspcl(skdev, skspcl, INQUIRY);\r\n} else {\r\npr_debug("%s:%s:%d **** READCAP failed, retry TUR\n",\r\nskdev->name, __func__, __LINE__);\r\nskd_send_internal_skspcl(skdev, skspcl,\r\nTEST_UNIT_READY);\r\n}\r\nbreak;\r\ncase INQUIRY:\r\nskdev->inquiry_is_valid = 0;\r\nif (status == SAM_STAT_GOOD) {\r\nskdev->inquiry_is_valid = 1;\r\nfor (i = 0; i < 12; i++)\r\nskdev->inq_serial_num[i] = buf[i + 4];\r\nskdev->inq_serial_num[12] = 0;\r\n}\r\nif (skd_unquiesce_dev(skdev) < 0)\r\npr_debug("%s:%s:%d **** failed, to ONLINE device\n",\r\nskdev->name, __func__, __LINE__);\r\nskdev->connect_retries = 0;\r\nbreak;\r\ncase SYNCHRONIZE_CACHE:\r\nif (status == SAM_STAT_GOOD)\r\nskdev->sync_done = 1;\r\nelse\r\nskdev->sync_done = -1;\r\nwake_up_interruptible(&skdev->waitq);\r\nbreak;\r\ndefault:\r\nSKD_ASSERT("we didn't send this");\r\n}\r\n}\r\nstatic void skd_send_fitmsg(struct skd_device *skdev,\r\nstruct skd_fitmsg_context *skmsg)\r\n{\r\nu64 qcmd;\r\nstruct fit_msg_hdr *fmh;\r\npr_debug("%s:%s:%d dma address 0x%llx, busy=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskmsg->mb_dma_address, skdev->in_flight);\r\npr_debug("%s:%s:%d msg_buf 0x%p, offset %x\n",\r\nskdev->name, __func__, __LINE__,\r\nskmsg->msg_buf, skmsg->offset);\r\nqcmd = skmsg->mb_dma_address;\r\nqcmd |= FIT_QCMD_QID_NORMAL;\r\nfmh = (struct fit_msg_hdr *)skmsg->msg_buf;\r\nskmsg->outstanding = fmh->num_protocol_cmds_coalesced;\r\nif (unlikely(skdev->dbg_level > 1)) {\r\nu8 *bp = (u8 *)skmsg->msg_buf;\r\nint i;\r\nfor (i = 0; i < skmsg->length; i += 8) {\r\npr_debug("%s:%s:%d msg[%2d] %02x %02x %02x %02x "\r\n"%02x %02x %02x %02x\n",\r\nskdev->name, __func__, __LINE__,\r\ni, bp[i + 0], bp[i + 1], bp[i + 2],\r\nbp[i + 3], bp[i + 4], bp[i + 5],\r\nbp[i + 6], bp[i + 7]);\r\nif (i == 0)\r\ni = 64 - 8;\r\n}\r\n}\r\nif (skmsg->length > 256)\r\nqcmd |= FIT_QCMD_MSGSIZE_512;\r\nelse if (skmsg->length > 128)\r\nqcmd |= FIT_QCMD_MSGSIZE_256;\r\nelse if (skmsg->length > 64)\r\nqcmd |= FIT_QCMD_MSGSIZE_128;\r\nelse\r\nqcmd |= FIT_QCMD_MSGSIZE_64;\r\nSKD_WRITEQ(skdev, qcmd, FIT_Q_COMMAND);\r\n}\r\nstatic void skd_send_special_fitmsg(struct skd_device *skdev,\r\nstruct skd_special_context *skspcl)\r\n{\r\nu64 qcmd;\r\nif (unlikely(skdev->dbg_level > 1)) {\r\nu8 *bp = (u8 *)skspcl->msg_buf;\r\nint i;\r\nfor (i = 0; i < SKD_N_SPECIAL_FITMSG_BYTES; i += 8) {\r\npr_debug("%s:%s:%d spcl[%2d] %02x %02x %02x %02x "\r\n"%02x %02x %02x %02x\n",\r\nskdev->name, __func__, __LINE__, i,\r\nbp[i + 0], bp[i + 1], bp[i + 2], bp[i + 3],\r\nbp[i + 4], bp[i + 5], bp[i + 6], bp[i + 7]);\r\nif (i == 0)\r\ni = 64 - 8;\r\n}\r\npr_debug("%s:%s:%d skspcl=%p id=%04x sksg_list=%p sksg_dma=%llx\n",\r\nskdev->name, __func__, __LINE__,\r\nskspcl, skspcl->req.id, skspcl->req.sksg_list,\r\nskspcl->req.sksg_dma_address);\r\nfor (i = 0; i < skspcl->req.n_sg; i++) {\r\nstruct fit_sg_descriptor *sgd =\r\n&skspcl->req.sksg_list[i];\r\npr_debug("%s:%s:%d sg[%d] count=%u ctrl=0x%x "\r\n"addr=0x%llx next=0x%llx\n",\r\nskdev->name, __func__, __LINE__,\r\ni, sgd->byte_count, sgd->control,\r\nsgd->host_side_addr, sgd->next_desc_ptr);\r\n}\r\n}\r\nqcmd = skspcl->mb_dma_address;\r\nqcmd |= FIT_QCMD_QID_NORMAL + FIT_QCMD_MSGSIZE_128;\r\nSKD_WRITEQ(skdev, qcmd, FIT_Q_COMMAND);\r\n}\r\nstatic enum skd_check_status_action\r\nskd_check_status(struct skd_device *skdev,\r\nu8 cmp_status, volatile struct fit_comp_error_info *skerr)\r\n{\r\nint i, n;\r\npr_err("(%s): key/asc/ascq/fruc %02x/%02x/%02x/%02x\n",\r\nskd_name(skdev), skerr->key, skerr->code, skerr->qual,\r\nskerr->fruc);\r\npr_debug("%s:%s:%d stat: t=%02x stat=%02x k=%02x c=%02x q=%02x fruc=%02x\n",\r\nskdev->name, __func__, __LINE__, skerr->type, cmp_status,\r\nskerr->key, skerr->code, skerr->qual, skerr->fruc);\r\nn = sizeof(skd_chkstat_table) / sizeof(skd_chkstat_table[0]);\r\nfor (i = 0; i < n; i++) {\r\nstruct sns_info *sns = &skd_chkstat_table[i];\r\nif (sns->mask & 0x10)\r\nif (skerr->type != sns->type)\r\ncontinue;\r\nif (sns->mask & 0x08)\r\nif (cmp_status != sns->stat)\r\ncontinue;\r\nif (sns->mask & 0x04)\r\nif (skerr->key != sns->key)\r\ncontinue;\r\nif (sns->mask & 0x02)\r\nif (skerr->code != sns->asc)\r\ncontinue;\r\nif (sns->mask & 0x01)\r\nif (skerr->qual != sns->ascq)\r\ncontinue;\r\nif (sns->action == SKD_CHECK_STATUS_REPORT_SMART_ALERT) {\r\npr_err("(%s): SMART Alert: sense key/asc/ascq "\r\n"%02x/%02x/%02x\n",\r\nskd_name(skdev), skerr->key,\r\nskerr->code, skerr->qual);\r\n}\r\nreturn sns->action;\r\n}\r\nif (cmp_status) {\r\npr_debug("%s:%s:%d status check: error\n",\r\nskdev->name, __func__, __LINE__);\r\nreturn SKD_CHECK_STATUS_REPORT_ERROR;\r\n}\r\npr_debug("%s:%s:%d status check good default\n",\r\nskdev->name, __func__, __LINE__);\r\nreturn SKD_CHECK_STATUS_REPORT_GOOD;\r\n}\r\nstatic void skd_resolve_req_exception(struct skd_device *skdev,\r\nstruct skd_request_context *skreq)\r\n{\r\nu8 cmp_status = skreq->completion.status;\r\nswitch (skd_check_status(skdev, cmp_status, &skreq->err_info)) {\r\ncase SKD_CHECK_STATUS_REPORT_GOOD:\r\ncase SKD_CHECK_STATUS_REPORT_SMART_ALERT:\r\nskd_end_request(skdev, skreq, 0);\r\nbreak;\r\ncase SKD_CHECK_STATUS_BUSY_IMMINENT:\r\nskd_log_skreq(skdev, skreq, "retry(busy)");\r\nblk_requeue_request(skdev->queue, skreq->req);\r\npr_info("(%s) drive BUSY imminent\n", skd_name(skdev));\r\nskdev->state = SKD_DRVR_STATE_BUSY_IMMINENT;\r\nskdev->timer_countdown = SKD_TIMER_MINUTES(20);\r\nskd_quiesce_dev(skdev);\r\nbreak;\r\ncase SKD_CHECK_STATUS_REQUEUE_REQUEST:\r\nif ((unsigned long) ++skreq->req->special < SKD_MAX_RETRIES) {\r\nskd_log_skreq(skdev, skreq, "retry");\r\nblk_requeue_request(skdev->queue, skreq->req);\r\nbreak;\r\n}\r\ncase SKD_CHECK_STATUS_REPORT_ERROR:\r\ndefault:\r\nskd_end_request(skdev, skreq, -EIO);\r\nbreak;\r\n}\r\n}\r\nstatic void skd_release_skreq(struct skd_device *skdev,\r\nstruct skd_request_context *skreq)\r\n{\r\nu32 msg_slot;\r\nstruct skd_fitmsg_context *skmsg;\r\nu32 timo_slot;\r\nmsg_slot = skreq->fitmsg_id & SKD_ID_SLOT_MASK;\r\nSKD_ASSERT(msg_slot < skdev->num_fitmsg_context);\r\nskmsg = &skdev->skmsg_table[msg_slot];\r\nif (skmsg->id == skreq->fitmsg_id) {\r\nSKD_ASSERT(skmsg->state == SKD_MSG_STATE_BUSY);\r\nSKD_ASSERT(skmsg->outstanding > 0);\r\nskmsg->outstanding--;\r\nif (skmsg->outstanding == 0) {\r\nskmsg->state = SKD_MSG_STATE_IDLE;\r\nskmsg->id += SKD_ID_INCR;\r\nskmsg->next = skdev->skmsg_free_list;\r\nskdev->skmsg_free_list = skmsg;\r\n}\r\n}\r\nSKD_ASSERT(skdev->in_flight > 0);\r\nskdev->in_flight -= 1;\r\ntimo_slot = skreq->timeout_stamp & SKD_TIMEOUT_SLOT_MASK;\r\nSKD_ASSERT(skdev->timeout_slot[timo_slot] > 0);\r\nskdev->timeout_slot[timo_slot] -= 1;\r\nskreq->req = NULL;\r\nskreq->state = SKD_REQ_STATE_IDLE;\r\nskreq->id += SKD_ID_INCR;\r\nskreq->next = skdev->skreq_free_list;\r\nskdev->skreq_free_list = skreq;\r\n}\r\nstatic void skd_do_inq_page_00(struct skd_device *skdev,\r\nvolatile struct fit_completion_entry_v1 *skcomp,\r\nvolatile struct fit_comp_error_info *skerr,\r\nuint8_t *cdb, uint8_t *buf)\r\n{\r\nuint16_t insert_pt, max_bytes, drive_pages, drive_bytes, new_size;\r\npr_debug("%s:%s:%d skd_do_driver_inquiry: modify supported pages.\n",\r\nskdev->name, __func__, __LINE__);\r\nif (skcomp->status == SAM_STAT_CHECK_CONDITION &&\r\nskerr->key == ILLEGAL_REQUEST && skerr->code == 0x24)\r\nreturn;\r\nmax_bytes = (cdb[3] << 8) | cdb[4];\r\ndrive_pages = (buf[2] << 8) | buf[3];\r\ndrive_bytes = drive_pages + 4;\r\nnew_size = drive_pages + 1;\r\nfor (insert_pt = 4; insert_pt < drive_bytes; insert_pt++) {\r\nif (buf[insert_pt] == DRIVER_INQ_EVPD_PAGE_CODE)\r\nreturn;\r\nelse if (buf[insert_pt] > DRIVER_INQ_EVPD_PAGE_CODE)\r\nbreak;\r\n}\r\nif (insert_pt < max_bytes) {\r\nuint16_t u;\r\nfor (u = new_size + 3; u > insert_pt; u--)\r\nbuf[u] = buf[u - 1];\r\nbuf[insert_pt] = DRIVER_INQ_EVPD_PAGE_CODE;\r\nskcomp->num_returned_bytes =\r\nbe32_to_cpu(skcomp->num_returned_bytes) + 1;\r\nskcomp->num_returned_bytes =\r\nbe32_to_cpu(skcomp->num_returned_bytes);\r\n}\r\nbuf[2] = (uint8_t)((new_size >> 8) & 0xFF);\r\nbuf[3] = (uint8_t)((new_size >> 0) & 0xFF);\r\n}\r\nstatic void skd_get_link_info(struct pci_dev *pdev, u8 *speed, u8 *width)\r\n{\r\nint pcie_reg;\r\nu16 pci_bus_speed;\r\nu8 pci_lanes;\r\npcie_reg = pci_find_capability(pdev, PCI_CAP_ID_EXP);\r\nif (pcie_reg) {\r\nu16 linksta;\r\npci_read_config_word(pdev, pcie_reg + PCI_EXP_LNKSTA, &linksta);\r\npci_bus_speed = linksta & 0xF;\r\npci_lanes = (linksta & 0x3F0) >> 4;\r\n} else {\r\n*speed = STEC_LINK_UNKNOWN;\r\n*width = 0xFF;\r\nreturn;\r\n}\r\nswitch (pci_bus_speed) {\r\ncase 1:\r\n*speed = STEC_LINK_2_5GTS;\r\nbreak;\r\ncase 2:\r\n*speed = STEC_LINK_5GTS;\r\nbreak;\r\ncase 3:\r\n*speed = STEC_LINK_8GTS;\r\nbreak;\r\ndefault:\r\n*speed = STEC_LINK_UNKNOWN;\r\nbreak;\r\n}\r\nif (pci_lanes <= 0x20)\r\n*width = pci_lanes;\r\nelse\r\n*width = 0xFF;\r\n}\r\nstatic void skd_do_inq_page_da(struct skd_device *skdev,\r\nvolatile struct fit_completion_entry_v1 *skcomp,\r\nvolatile struct fit_comp_error_info *skerr,\r\nuint8_t *cdb, uint8_t *buf)\r\n{\r\nstruct pci_dev *pdev = skdev->pdev;\r\nunsigned max_bytes;\r\nstruct driver_inquiry_data inq;\r\nu16 val;\r\npr_debug("%s:%s:%d skd_do_driver_inquiry: return driver page\n",\r\nskdev->name, __func__, __LINE__);\r\nmemset(&inq, 0, sizeof(inq));\r\ninq.page_code = DRIVER_INQ_EVPD_PAGE_CODE;\r\nskd_get_link_info(pdev, &inq.pcie_link_speed, &inq.pcie_link_lanes);\r\ninq.pcie_bus_number = cpu_to_be16(pdev->bus->number);\r\ninq.pcie_device_number = PCI_SLOT(pdev->devfn);\r\ninq.pcie_function_number = PCI_FUNC(pdev->devfn);\r\npci_read_config_word(pdev, PCI_VENDOR_ID, &val);\r\ninq.pcie_vendor_id = cpu_to_be16(val);\r\npci_read_config_word(pdev, PCI_DEVICE_ID, &val);\r\ninq.pcie_device_id = cpu_to_be16(val);\r\npci_read_config_word(pdev, PCI_SUBSYSTEM_VENDOR_ID, &val);\r\ninq.pcie_subsystem_vendor_id = cpu_to_be16(val);\r\npci_read_config_word(pdev, PCI_SUBSYSTEM_ID, &val);\r\ninq.pcie_subsystem_device_id = cpu_to_be16(val);\r\ninq.driver_version_length = sizeof(inq.driver_version);\r\nmemset(&inq.driver_version, ' ', sizeof(inq.driver_version));\r\nmemcpy(inq.driver_version, DRV_VER_COMPL,\r\nmin(sizeof(inq.driver_version), strlen(DRV_VER_COMPL)));\r\ninq.page_length = cpu_to_be16((sizeof(inq) - 4));\r\nskcomp->status = SAM_STAT_GOOD;\r\nmemset((void *)skerr, 0, sizeof(*skerr));\r\nmax_bytes = (cdb[3] << 8) | cdb[4];\r\nmemcpy(buf, &inq, min_t(unsigned, max_bytes, sizeof(inq)));\r\nskcomp->num_returned_bytes =\r\nbe32_to_cpu(min_t(uint16_t, max_bytes, sizeof(inq)));\r\n}\r\nstatic void skd_do_driver_inq(struct skd_device *skdev,\r\nvolatile struct fit_completion_entry_v1 *skcomp,\r\nvolatile struct fit_comp_error_info *skerr,\r\nuint8_t *cdb, uint8_t *buf)\r\n{\r\nif (!buf)\r\nreturn;\r\nelse if (cdb[0] != INQUIRY)\r\nreturn;\r\nelse if ((cdb[1] & 1) == 0)\r\nreturn;\r\nelse if (cdb[2] == 0)\r\nskd_do_inq_page_00(skdev, skcomp, skerr, cdb, buf);\r\nelse if (cdb[2] == DRIVER_INQ_EVPD_PAGE_CODE)\r\nskd_do_inq_page_da(skdev, skcomp, skerr, cdb, buf);\r\n}\r\nstatic unsigned char *skd_sg_1st_page_ptr(struct scatterlist *sg)\r\n{\r\nif (!sg)\r\nreturn NULL;\r\nif (!sg_page(sg))\r\nreturn NULL;\r\nreturn sg_virt(sg);\r\n}\r\nstatic void skd_process_scsi_inq(struct skd_device *skdev,\r\nvolatile struct fit_completion_entry_v1\r\n*skcomp,\r\nvolatile struct fit_comp_error_info *skerr,\r\nstruct skd_special_context *skspcl)\r\n{\r\nuint8_t *buf;\r\nstruct fit_msg_hdr *fmh = (struct fit_msg_hdr *)skspcl->msg_buf;\r\nstruct skd_scsi_request *scsi_req = (struct skd_scsi_request *)&fmh[1];\r\ndma_sync_sg_for_cpu(skdev->class_dev, skspcl->req.sg, skspcl->req.n_sg,\r\nskspcl->req.sg_data_dir);\r\nbuf = skd_sg_1st_page_ptr(skspcl->req.sg);\r\nif (buf)\r\nskd_do_driver_inq(skdev, skcomp, skerr, scsi_req->cdb, buf);\r\n}\r\nstatic int skd_isr_completion_posted(struct skd_device *skdev,\r\nint limit, int *enqueued)\r\n{\r\nvolatile struct fit_completion_entry_v1 *skcmp = NULL;\r\nvolatile struct fit_comp_error_info *skerr;\r\nu16 req_id;\r\nu32 req_slot;\r\nstruct skd_request_context *skreq;\r\nu16 cmp_cntxt = 0;\r\nu8 cmp_status = 0;\r\nu8 cmp_cycle = 0;\r\nu32 cmp_bytes = 0;\r\nint rc = 0;\r\nint processed = 0;\r\nfor (;; ) {\r\nSKD_ASSERT(skdev->skcomp_ix < SKD_N_COMPLETION_ENTRY);\r\nskcmp = &skdev->skcomp_table[skdev->skcomp_ix];\r\ncmp_cycle = skcmp->cycle;\r\ncmp_cntxt = skcmp->tag;\r\ncmp_status = skcmp->status;\r\ncmp_bytes = be32_to_cpu(skcmp->num_returned_bytes);\r\nskerr = &skdev->skerr_table[skdev->skcomp_ix];\r\npr_debug("%s:%s:%d "\r\n"cycle=%d ix=%d got cycle=%d cmdctxt=0x%x stat=%d "\r\n"busy=%d rbytes=0x%x proto=%d\n",\r\nskdev->name, __func__, __LINE__, skdev->skcomp_cycle,\r\nskdev->skcomp_ix, cmp_cycle, cmp_cntxt, cmp_status,\r\nskdev->in_flight, cmp_bytes, skdev->proto_ver);\r\nif (cmp_cycle != skdev->skcomp_cycle) {\r\npr_debug("%s:%s:%d end of completions\n",\r\nskdev->name, __func__, __LINE__);\r\nbreak;\r\n}\r\nskdev->skcomp_ix++;\r\nif (skdev->skcomp_ix >= SKD_N_COMPLETION_ENTRY) {\r\nskdev->skcomp_ix = 0;\r\nskdev->skcomp_cycle++;\r\n}\r\nreq_id = cmp_cntxt;\r\nreq_slot = req_id & SKD_ID_SLOT_AND_TABLE_MASK;\r\nif (req_slot >= skdev->num_req_context) {\r\nskd_complete_other(skdev, skcmp, skerr);\r\ncontinue;\r\n}\r\nskreq = &skdev->skreq_table[req_slot];\r\nif (skreq->id != req_id) {\r\npr_debug("%s:%s:%d mismatch comp_id=0x%x req_id=0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nreq_id, skreq->id);\r\n{\r\nu16 new_id = cmp_cntxt;\r\npr_err("(%s): Completion mismatch "\r\n"comp_id=0x%04x skreq=0x%04x new=0x%04x\n",\r\nskd_name(skdev), req_id,\r\nskreq->id, new_id);\r\ncontinue;\r\n}\r\n}\r\nSKD_ASSERT(skreq->state == SKD_REQ_STATE_BUSY);\r\nif (skreq->state == SKD_REQ_STATE_ABORTED) {\r\npr_debug("%s:%s:%d reclaim req %p id=%04x\n",\r\nskdev->name, __func__, __LINE__,\r\nskreq, skreq->id);\r\nskd_release_skreq(skdev, skreq);\r\ncontinue;\r\n}\r\nskreq->completion = *skcmp;\r\nif (unlikely(cmp_status == SAM_STAT_CHECK_CONDITION)) {\r\nskreq->err_info = *skerr;\r\nskd_log_check_status(skdev, cmp_status, skerr->key,\r\nskerr->code, skerr->qual,\r\nskerr->fruc);\r\n}\r\nif (skreq->n_sg > 0)\r\nskd_postop_sg_list(skdev, skreq);\r\nif (!skreq->req) {\r\npr_debug("%s:%s:%d NULL backptr skdreq %p, "\r\n"req=0x%x req_id=0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nskreq, skreq->id, req_id);\r\n} else {\r\nif (likely(cmp_status == SAM_STAT_GOOD))\r\nskd_end_request(skdev, skreq, 0);\r\nelse\r\nskd_resolve_req_exception(skdev, skreq);\r\n}\r\nskd_release_skreq(skdev, skreq);\r\nif (limit) {\r\nif (++processed >= limit) {\r\nrc = 1;\r\nbreak;\r\n}\r\n}\r\n}\r\nif ((skdev->state == SKD_DRVR_STATE_PAUSING)\r\n&& (skdev->in_flight) == 0) {\r\nskdev->state = SKD_DRVR_STATE_PAUSED;\r\nwake_up_interruptible(&skdev->waitq);\r\n}\r\nreturn rc;\r\n}\r\nstatic void skd_complete_other(struct skd_device *skdev,\r\nvolatile struct fit_completion_entry_v1 *skcomp,\r\nvolatile struct fit_comp_error_info *skerr)\r\n{\r\nu32 req_id = 0;\r\nu32 req_table;\r\nu32 req_slot;\r\nstruct skd_special_context *skspcl;\r\nreq_id = skcomp->tag;\r\nreq_table = req_id & SKD_ID_TABLE_MASK;\r\nreq_slot = req_id & SKD_ID_SLOT_MASK;\r\npr_debug("%s:%s:%d table=0x%x id=0x%x slot=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nreq_table, req_id, req_slot);\r\nswitch (req_table) {\r\ncase SKD_ID_RW_REQUEST:\r\nbreak;\r\ncase SKD_ID_SPECIAL_REQUEST:\r\nif (req_slot < skdev->n_special) {\r\nskspcl = &skdev->skspcl_table[req_slot];\r\nif (skspcl->req.id == req_id &&\r\nskspcl->req.state == SKD_REQ_STATE_BUSY) {\r\nskd_complete_special(skdev,\r\nskcomp, skerr, skspcl);\r\nreturn;\r\n}\r\n}\r\nbreak;\r\ncase SKD_ID_INTERNAL:\r\nif (req_slot == 0) {\r\nskspcl = &skdev->internal_skspcl;\r\nif (skspcl->req.id == req_id &&\r\nskspcl->req.state == SKD_REQ_STATE_BUSY) {\r\nskd_complete_internal(skdev,\r\nskcomp, skerr, skspcl);\r\nreturn;\r\n}\r\n}\r\nbreak;\r\ncase SKD_ID_FIT_MSG:\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void skd_complete_special(struct skd_device *skdev,\r\nvolatile struct fit_completion_entry_v1\r\n*skcomp,\r\nvolatile struct fit_comp_error_info *skerr,\r\nstruct skd_special_context *skspcl)\r\n{\r\npr_debug("%s:%s:%d completing special request %p\n",\r\nskdev->name, __func__, __LINE__, skspcl);\r\nif (skspcl->orphaned) {\r\npr_debug("%s:%s:%d release orphaned %p\n",\r\nskdev->name, __func__, __LINE__, skspcl);\r\nskd_release_special(skdev, skspcl);\r\nreturn;\r\n}\r\nskd_process_scsi_inq(skdev, skcomp, skerr, skspcl);\r\nskspcl->req.state = SKD_REQ_STATE_COMPLETED;\r\nskspcl->req.completion = *skcomp;\r\nskspcl->req.err_info = *skerr;\r\nskd_log_check_status(skdev, skspcl->req.completion.status, skerr->key,\r\nskerr->code, skerr->qual, skerr->fruc);\r\nwake_up_interruptible(&skdev->waitq);\r\n}\r\nstatic void skd_release_special(struct skd_device *skdev,\r\nstruct skd_special_context *skspcl)\r\n{\r\nint i, was_depleted;\r\nfor (i = 0; i < skspcl->req.n_sg; i++) {\r\nstruct page *page = sg_page(&skspcl->req.sg[i]);\r\n__free_page(page);\r\n}\r\nwas_depleted = (skdev->skspcl_free_list == NULL);\r\nskspcl->req.state = SKD_REQ_STATE_IDLE;\r\nskspcl->req.id += SKD_ID_INCR;\r\nskspcl->req.next =\r\n(struct skd_request_context *)skdev->skspcl_free_list;\r\nskdev->skspcl_free_list = (struct skd_special_context *)skspcl;\r\nif (was_depleted) {\r\npr_debug("%s:%s:%d skspcl was depleted\n",\r\nskdev->name, __func__, __LINE__);\r\nwake_up_interruptible(&skdev->waitq);\r\n}\r\n}\r\nstatic void skd_reset_skcomp(struct skd_device *skdev)\r\n{\r\nu32 nbytes;\r\nstruct fit_completion_entry_v1 *skcomp;\r\nnbytes = sizeof(*skcomp) * SKD_N_COMPLETION_ENTRY;\r\nnbytes += sizeof(struct fit_comp_error_info) * SKD_N_COMPLETION_ENTRY;\r\nmemset(skdev->skcomp_table, 0, nbytes);\r\nskdev->skcomp_ix = 0;\r\nskdev->skcomp_cycle = 1;\r\n}\r\nstatic void skd_completion_worker(struct work_struct *work)\r\n{\r\nstruct skd_device *skdev =\r\ncontainer_of(work, struct skd_device, completion_worker);\r\nunsigned long flags;\r\nint flush_enqueued = 0;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\nskd_isr_completion_posted(skdev, 0, &flush_enqueued);\r\nskd_request_fn(skdev->queue);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\n}\r\nirqreturn_t\r\nstatic skd_isr(int irq, void *ptr)\r\n{\r\nstruct skd_device *skdev;\r\nu32 intstat;\r\nu32 ack;\r\nint rc = 0;\r\nint deferred = 0;\r\nint flush_enqueued = 0;\r\nskdev = (struct skd_device *)ptr;\r\nspin_lock(&skdev->lock);\r\nfor (;; ) {\r\nintstat = SKD_READL(skdev, FIT_INT_STATUS_HOST);\r\nack = FIT_INT_DEF_MASK;\r\nack &= intstat;\r\npr_debug("%s:%s:%d intstat=0x%x ack=0x%x\n",\r\nskdev->name, __func__, __LINE__, intstat, ack);\r\nif (ack == 0) {\r\nif (rc == 0)\r\nif (likely (skdev->state\r\n== SKD_DRVR_STATE_ONLINE))\r\ndeferred = 1;\r\nbreak;\r\n}\r\nrc = IRQ_HANDLED;\r\nSKD_WRITEL(skdev, ack, FIT_INT_STATUS_HOST);\r\nif (likely((skdev->state != SKD_DRVR_STATE_LOAD) &&\r\n(skdev->state != SKD_DRVR_STATE_STOPPING))) {\r\nif (intstat & FIT_ISH_COMPLETION_POSTED) {\r\nif (deferred == 0)\r\ndeferred =\r\nskd_isr_completion_posted(skdev,\r\nskd_isr_comp_limit, &flush_enqueued);\r\n}\r\nif (intstat & FIT_ISH_FW_STATE_CHANGE) {\r\nskd_isr_fwstate(skdev);\r\nif (skdev->state == SKD_DRVR_STATE_FAULT ||\r\nskdev->state ==\r\nSKD_DRVR_STATE_DISAPPEARED) {\r\nspin_unlock(&skdev->lock);\r\nreturn rc;\r\n}\r\n}\r\nif (intstat & FIT_ISH_MSG_FROM_DEV)\r\nskd_isr_msg_from_dev(skdev);\r\n}\r\n}\r\nif (unlikely(flush_enqueued))\r\nskd_request_fn(skdev->queue);\r\nif (deferred)\r\nschedule_work(&skdev->completion_worker);\r\nelse if (!flush_enqueued)\r\nskd_request_fn(skdev->queue);\r\nspin_unlock(&skdev->lock);\r\nreturn rc;\r\n}\r\nstatic void skd_drive_fault(struct skd_device *skdev)\r\n{\r\nskdev->state = SKD_DRVR_STATE_FAULT;\r\npr_err("(%s): Drive FAULT\n", skd_name(skdev));\r\n}\r\nstatic void skd_drive_disappeared(struct skd_device *skdev)\r\n{\r\nskdev->state = SKD_DRVR_STATE_DISAPPEARED;\r\npr_err("(%s): Drive DISAPPEARED\n", skd_name(skdev));\r\n}\r\nstatic void skd_isr_fwstate(struct skd_device *skdev)\r\n{\r\nu32 sense;\r\nu32 state;\r\nu32 mtd;\r\nint prev_driver_state = skdev->state;\r\nsense = SKD_READL(skdev, FIT_STATUS);\r\nstate = sense & FIT_SR_DRIVE_STATE_MASK;\r\npr_err("(%s): s1120 state %s(%d)=>%s(%d)\n",\r\nskd_name(skdev),\r\nskd_drive_state_to_str(skdev->drive_state), skdev->drive_state,\r\nskd_drive_state_to_str(state), state);\r\nskdev->drive_state = state;\r\nswitch (skdev->drive_state) {\r\ncase FIT_SR_DRIVE_INIT:\r\nif (skdev->state == SKD_DRVR_STATE_PROTOCOL_MISMATCH) {\r\nskd_disable_interrupts(skdev);\r\nbreak;\r\n}\r\nif (skdev->state == SKD_DRVR_STATE_RESTARTING)\r\nskd_recover_requests(skdev, 0);\r\nif (skdev->state == SKD_DRVR_STATE_WAIT_BOOT) {\r\nskdev->timer_countdown = SKD_STARTING_TIMO;\r\nskdev->state = SKD_DRVR_STATE_STARTING;\r\nskd_soft_reset(skdev);\r\nbreak;\r\n}\r\nmtd = FIT_MXD_CONS(FIT_MTD_FITFW_INIT, 0, 0);\r\nSKD_WRITEL(skdev, mtd, FIT_MSG_TO_DEVICE);\r\nskdev->last_mtd = mtd;\r\nbreak;\r\ncase FIT_SR_DRIVE_ONLINE:\r\nskdev->cur_max_queue_depth = skd_max_queue_depth;\r\nif (skdev->cur_max_queue_depth > skdev->dev_max_queue_depth)\r\nskdev->cur_max_queue_depth = skdev->dev_max_queue_depth;\r\nskdev->queue_low_water_mark =\r\nskdev->cur_max_queue_depth * 2 / 3 + 1;\r\nif (skdev->queue_low_water_mark < 1)\r\nskdev->queue_low_water_mark = 1;\r\npr_info(\r\n"(%s): Queue depth limit=%d dev=%d lowat=%d\n",\r\nskd_name(skdev),\r\nskdev->cur_max_queue_depth,\r\nskdev->dev_max_queue_depth, skdev->queue_low_water_mark);\r\nskd_refresh_device_data(skdev);\r\nbreak;\r\ncase FIT_SR_DRIVE_BUSY:\r\nskdev->state = SKD_DRVR_STATE_BUSY;\r\nskdev->timer_countdown = SKD_BUSY_TIMO;\r\nskd_quiesce_dev(skdev);\r\nbreak;\r\ncase FIT_SR_DRIVE_BUSY_SANITIZE:\r\nskdev->state = SKD_DRVR_STATE_BUSY_SANITIZE;\r\nskdev->timer_countdown = SKD_TIMER_SECONDS(3);\r\nblk_start_queue(skdev->queue);\r\nbreak;\r\ncase FIT_SR_DRIVE_BUSY_ERASE:\r\nskdev->state = SKD_DRVR_STATE_BUSY_ERASE;\r\nskdev->timer_countdown = SKD_BUSY_TIMO;\r\nbreak;\r\ncase FIT_SR_DRIVE_OFFLINE:\r\nskdev->state = SKD_DRVR_STATE_IDLE;\r\nbreak;\r\ncase FIT_SR_DRIVE_SOFT_RESET:\r\nswitch (skdev->state) {\r\ncase SKD_DRVR_STATE_STARTING:\r\ncase SKD_DRVR_STATE_RESTARTING:\r\nbreak;\r\ndefault:\r\nskdev->state = SKD_DRVR_STATE_RESTARTING;\r\nbreak;\r\n}\r\nbreak;\r\ncase FIT_SR_DRIVE_FW_BOOTING:\r\npr_debug("%s:%s:%d ISR FIT_SR_DRIVE_FW_BOOTING %s\n",\r\nskdev->name, __func__, __LINE__, skdev->name);\r\nskdev->state = SKD_DRVR_STATE_WAIT_BOOT;\r\nskdev->timer_countdown = SKD_WAIT_BOOT_TIMO;\r\nbreak;\r\ncase FIT_SR_DRIVE_DEGRADED:\r\ncase FIT_SR_PCIE_LINK_DOWN:\r\ncase FIT_SR_DRIVE_NEED_FW_DOWNLOAD:\r\nbreak;\r\ncase FIT_SR_DRIVE_FAULT:\r\nskd_drive_fault(skdev);\r\nskd_recover_requests(skdev, 0);\r\nblk_start_queue(skdev->queue);\r\nbreak;\r\ncase 0xFF:\r\npr_info("(%s): state=0x%x sense=0x%x\n",\r\nskd_name(skdev), state, sense);\r\nskd_drive_disappeared(skdev);\r\nskd_recover_requests(skdev, 0);\r\nblk_start_queue(skdev->queue);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\npr_err("(%s): Driver state %s(%d)=>%s(%d)\n",\r\nskd_name(skdev),\r\nskd_skdev_state_to_str(prev_driver_state), prev_driver_state,\r\nskd_skdev_state_to_str(skdev->state), skdev->state);\r\n}\r\nstatic void skd_recover_requests(struct skd_device *skdev, int requeue)\r\n{\r\nint i;\r\nfor (i = 0; i < skdev->num_req_context; i++) {\r\nstruct skd_request_context *skreq = &skdev->skreq_table[i];\r\nif (skreq->state == SKD_REQ_STATE_BUSY) {\r\nskd_log_skreq(skdev, skreq, "recover");\r\nSKD_ASSERT((skreq->id & SKD_ID_INCR) != 0);\r\nSKD_ASSERT(skreq->req != NULL);\r\nif (skreq->n_sg > 0)\r\nskd_postop_sg_list(skdev, skreq);\r\nif (requeue &&\r\n(unsigned long) ++skreq->req->special <\r\nSKD_MAX_RETRIES)\r\nblk_requeue_request(skdev->queue, skreq->req);\r\nelse\r\nskd_end_request(skdev, skreq, -EIO);\r\nskreq->req = NULL;\r\nskreq->state = SKD_REQ_STATE_IDLE;\r\nskreq->id += SKD_ID_INCR;\r\n}\r\nif (i > 0)\r\nskreq[-1].next = skreq;\r\nskreq->next = NULL;\r\n}\r\nskdev->skreq_free_list = skdev->skreq_table;\r\nfor (i = 0; i < skdev->num_fitmsg_context; i++) {\r\nstruct skd_fitmsg_context *skmsg = &skdev->skmsg_table[i];\r\nif (skmsg->state == SKD_MSG_STATE_BUSY) {\r\nskd_log_skmsg(skdev, skmsg, "salvaged");\r\nSKD_ASSERT((skmsg->id & SKD_ID_INCR) != 0);\r\nskmsg->state = SKD_MSG_STATE_IDLE;\r\nskmsg->id += SKD_ID_INCR;\r\n}\r\nif (i > 0)\r\nskmsg[-1].next = skmsg;\r\nskmsg->next = NULL;\r\n}\r\nskdev->skmsg_free_list = skdev->skmsg_table;\r\nfor (i = 0; i < skdev->n_special; i++) {\r\nstruct skd_special_context *skspcl = &skdev->skspcl_table[i];\r\nif (skspcl->req.state == SKD_REQ_STATE_BUSY) {\r\nif (skspcl->orphaned) {\r\npr_debug("%s:%s:%d orphaned %p\n",\r\nskdev->name, __func__, __LINE__,\r\nskspcl);\r\nskd_release_special(skdev, skspcl);\r\n} else {\r\npr_debug("%s:%s:%d not orphaned %p\n",\r\nskdev->name, __func__, __LINE__,\r\nskspcl);\r\nskspcl->req.state = SKD_REQ_STATE_ABORTED;\r\n}\r\n}\r\n}\r\nskdev->skspcl_free_list = skdev->skspcl_table;\r\nfor (i = 0; i < SKD_N_TIMEOUT_SLOT; i++)\r\nskdev->timeout_slot[i] = 0;\r\nskdev->in_flight = 0;\r\n}\r\nstatic void skd_isr_msg_from_dev(struct skd_device *skdev)\r\n{\r\nu32 mfd;\r\nu32 mtd;\r\nu32 data;\r\nmfd = SKD_READL(skdev, FIT_MSG_FROM_DEVICE);\r\npr_debug("%s:%s:%d mfd=0x%x last_mtd=0x%x\n",\r\nskdev->name, __func__, __LINE__, mfd, skdev->last_mtd);\r\nif (FIT_MXD_TYPE(mfd) != FIT_MXD_TYPE(skdev->last_mtd))\r\nreturn;\r\nswitch (FIT_MXD_TYPE(mfd)) {\r\ncase FIT_MTD_FITFW_INIT:\r\nskdev->proto_ver = FIT_PROTOCOL_MAJOR_VER(mfd);\r\nif (skdev->proto_ver != FIT_PROTOCOL_VERSION_1) {\r\npr_err("(%s): protocol mismatch\n",\r\nskdev->name);\r\npr_err("(%s): got=%d support=%d\n",\r\nskdev->name, skdev->proto_ver,\r\nFIT_PROTOCOL_VERSION_1);\r\npr_err("(%s): please upgrade driver\n",\r\nskdev->name);\r\nskdev->state = SKD_DRVR_STATE_PROTOCOL_MISMATCH;\r\nskd_soft_reset(skdev);\r\nbreak;\r\n}\r\nmtd = FIT_MXD_CONS(FIT_MTD_GET_CMDQ_DEPTH, 0, 0);\r\nSKD_WRITEL(skdev, mtd, FIT_MSG_TO_DEVICE);\r\nskdev->last_mtd = mtd;\r\nbreak;\r\ncase FIT_MTD_GET_CMDQ_DEPTH:\r\nskdev->dev_max_queue_depth = FIT_MXD_DATA(mfd);\r\nmtd = FIT_MXD_CONS(FIT_MTD_SET_COMPQ_DEPTH, 0,\r\nSKD_N_COMPLETION_ENTRY);\r\nSKD_WRITEL(skdev, mtd, FIT_MSG_TO_DEVICE);\r\nskdev->last_mtd = mtd;\r\nbreak;\r\ncase FIT_MTD_SET_COMPQ_DEPTH:\r\nSKD_WRITEQ(skdev, skdev->cq_dma_address, FIT_MSG_TO_DEVICE_ARG);\r\nmtd = FIT_MXD_CONS(FIT_MTD_SET_COMPQ_ADDR, 0, 0);\r\nSKD_WRITEL(skdev, mtd, FIT_MSG_TO_DEVICE);\r\nskdev->last_mtd = mtd;\r\nbreak;\r\ncase FIT_MTD_SET_COMPQ_ADDR:\r\nskd_reset_skcomp(skdev);\r\nmtd = FIT_MXD_CONS(FIT_MTD_CMD_LOG_HOST_ID, 0, skdev->devno);\r\nSKD_WRITEL(skdev, mtd, FIT_MSG_TO_DEVICE);\r\nskdev->last_mtd = mtd;\r\nbreak;\r\ncase FIT_MTD_CMD_LOG_HOST_ID:\r\nskdev->connect_time_stamp = get_seconds();\r\ndata = skdev->connect_time_stamp & 0xFFFF;\r\nmtd = FIT_MXD_CONS(FIT_MTD_CMD_LOG_TIME_STAMP_LO, 0, data);\r\nSKD_WRITEL(skdev, mtd, FIT_MSG_TO_DEVICE);\r\nskdev->last_mtd = mtd;\r\nbreak;\r\ncase FIT_MTD_CMD_LOG_TIME_STAMP_LO:\r\nskdev->drive_jiffies = FIT_MXD_DATA(mfd);\r\ndata = (skdev->connect_time_stamp >> 16) & 0xFFFF;\r\nmtd = FIT_MXD_CONS(FIT_MTD_CMD_LOG_TIME_STAMP_HI, 0, data);\r\nSKD_WRITEL(skdev, mtd, FIT_MSG_TO_DEVICE);\r\nskdev->last_mtd = mtd;\r\nbreak;\r\ncase FIT_MTD_CMD_LOG_TIME_STAMP_HI:\r\nskdev->drive_jiffies |= (FIT_MXD_DATA(mfd) << 16);\r\nmtd = FIT_MXD_CONS(FIT_MTD_ARM_QUEUE, 0, 0);\r\nSKD_WRITEL(skdev, mtd, FIT_MSG_TO_DEVICE);\r\nskdev->last_mtd = mtd;\r\npr_err("(%s): Time sync driver=0x%x device=0x%x\n",\r\nskd_name(skdev),\r\nskdev->connect_time_stamp, skdev->drive_jiffies);\r\nbreak;\r\ncase FIT_MTD_ARM_QUEUE:\r\nskdev->last_mtd = 0;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nstatic void skd_disable_interrupts(struct skd_device *skdev)\r\n{\r\nu32 sense;\r\nsense = SKD_READL(skdev, FIT_CONTROL);\r\nsense &= ~FIT_CR_ENABLE_INTERRUPTS;\r\nSKD_WRITEL(skdev, sense, FIT_CONTROL);\r\npr_debug("%s:%s:%d sense 0x%x\n",\r\nskdev->name, __func__, __LINE__, sense);\r\nSKD_WRITEL(skdev, ~0, FIT_INT_MASK_HOST);\r\n}\r\nstatic void skd_enable_interrupts(struct skd_device *skdev)\r\n{\r\nu32 val;\r\nval = FIT_ISH_FW_STATE_CHANGE +\r\nFIT_ISH_COMPLETION_POSTED + FIT_ISH_MSG_FROM_DEV;\r\nSKD_WRITEL(skdev, ~val, FIT_INT_MASK_HOST);\r\npr_debug("%s:%s:%d interrupt mask=0x%x\n",\r\nskdev->name, __func__, __LINE__, ~val);\r\nval = SKD_READL(skdev, FIT_CONTROL);\r\nval |= FIT_CR_ENABLE_INTERRUPTS;\r\npr_debug("%s:%s:%d control=0x%x\n",\r\nskdev->name, __func__, __LINE__, val);\r\nSKD_WRITEL(skdev, val, FIT_CONTROL);\r\n}\r\nstatic void skd_soft_reset(struct skd_device *skdev)\r\n{\r\nu32 val;\r\nval = SKD_READL(skdev, FIT_CONTROL);\r\nval |= (FIT_CR_SOFT_RESET);\r\npr_debug("%s:%s:%d control=0x%x\n",\r\nskdev->name, __func__, __LINE__, val);\r\nSKD_WRITEL(skdev, val, FIT_CONTROL);\r\n}\r\nstatic void skd_start_device(struct skd_device *skdev)\r\n{\r\nunsigned long flags;\r\nu32 sense;\r\nu32 state;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\nSKD_WRITEL(skdev, FIT_INT_DEF_MASK, FIT_INT_STATUS_HOST);\r\nsense = SKD_READL(skdev, FIT_STATUS);\r\npr_debug("%s:%s:%d initial status=0x%x\n",\r\nskdev->name, __func__, __LINE__, sense);\r\nstate = sense & FIT_SR_DRIVE_STATE_MASK;\r\nskdev->drive_state = state;\r\nskdev->last_mtd = 0;\r\nskdev->state = SKD_DRVR_STATE_STARTING;\r\nskdev->timer_countdown = SKD_STARTING_TIMO;\r\nskd_enable_interrupts(skdev);\r\nswitch (skdev->drive_state) {\r\ncase FIT_SR_DRIVE_OFFLINE:\r\npr_err("(%s): Drive offline...\n", skd_name(skdev));\r\nbreak;\r\ncase FIT_SR_DRIVE_FW_BOOTING:\r\npr_debug("%s:%s:%d FIT_SR_DRIVE_FW_BOOTING %s\n",\r\nskdev->name, __func__, __LINE__, skdev->name);\r\nskdev->state = SKD_DRVR_STATE_WAIT_BOOT;\r\nskdev->timer_countdown = SKD_WAIT_BOOT_TIMO;\r\nbreak;\r\ncase FIT_SR_DRIVE_BUSY_SANITIZE:\r\npr_info("(%s): Start: BUSY_SANITIZE\n",\r\nskd_name(skdev));\r\nskdev->state = SKD_DRVR_STATE_BUSY_SANITIZE;\r\nskdev->timer_countdown = SKD_STARTED_BUSY_TIMO;\r\nbreak;\r\ncase FIT_SR_DRIVE_BUSY_ERASE:\r\npr_info("(%s): Start: BUSY_ERASE\n", skd_name(skdev));\r\nskdev->state = SKD_DRVR_STATE_BUSY_ERASE;\r\nskdev->timer_countdown = SKD_STARTED_BUSY_TIMO;\r\nbreak;\r\ncase FIT_SR_DRIVE_INIT:\r\ncase FIT_SR_DRIVE_ONLINE:\r\nskd_soft_reset(skdev);\r\nbreak;\r\ncase FIT_SR_DRIVE_BUSY:\r\npr_err("(%s): Drive Busy...\n", skd_name(skdev));\r\nskdev->state = SKD_DRVR_STATE_BUSY;\r\nskdev->timer_countdown = SKD_STARTED_BUSY_TIMO;\r\nbreak;\r\ncase FIT_SR_DRIVE_SOFT_RESET:\r\npr_err("(%s) drive soft reset in prog\n",\r\nskd_name(skdev));\r\nbreak;\r\ncase FIT_SR_DRIVE_FAULT:\r\nskd_drive_fault(skdev);\r\npr_debug("%s:%s:%d starting %s queue\n",\r\nskdev->name, __func__, __LINE__, skdev->name);\r\nblk_start_queue(skdev->queue);\r\nskdev->gendisk_on = -1;\r\nwake_up_interruptible(&skdev->waitq);\r\nbreak;\r\ncase 0xFF:\r\nskd_drive_disappeared(skdev);\r\npr_debug("%s:%s:%d starting %s queue to error-out reqs\n",\r\nskdev->name, __func__, __LINE__, skdev->name);\r\nblk_start_queue(skdev->queue);\r\nskdev->gendisk_on = -1;\r\nwake_up_interruptible(&skdev->waitq);\r\nbreak;\r\ndefault:\r\npr_err("(%s) Start: unknown state %x\n",\r\nskd_name(skdev), skdev->drive_state);\r\nbreak;\r\n}\r\nstate = SKD_READL(skdev, FIT_CONTROL);\r\npr_debug("%s:%s:%d FIT Control Status=0x%x\n",\r\nskdev->name, __func__, __LINE__, state);\r\nstate = SKD_READL(skdev, FIT_INT_STATUS_HOST);\r\npr_debug("%s:%s:%d Intr Status=0x%x\n",\r\nskdev->name, __func__, __LINE__, state);\r\nstate = SKD_READL(skdev, FIT_INT_MASK_HOST);\r\npr_debug("%s:%s:%d Intr Mask=0x%x\n",\r\nskdev->name, __func__, __LINE__, state);\r\nstate = SKD_READL(skdev, FIT_MSG_FROM_DEVICE);\r\npr_debug("%s:%s:%d Msg from Dev=0x%x\n",\r\nskdev->name, __func__, __LINE__, state);\r\nstate = SKD_READL(skdev, FIT_HW_VERSION);\r\npr_debug("%s:%s:%d HW version=0x%x\n",\r\nskdev->name, __func__, __LINE__, state);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\n}\r\nstatic void skd_stop_device(struct skd_device *skdev)\r\n{\r\nunsigned long flags;\r\nstruct skd_special_context *skspcl = &skdev->internal_skspcl;\r\nu32 dev_state;\r\nint i;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\nif (skdev->state != SKD_DRVR_STATE_ONLINE) {\r\npr_err("(%s): skd_stop_device not online no sync\n",\r\nskd_name(skdev));\r\ngoto stop_out;\r\n}\r\nif (skspcl->req.state != SKD_REQ_STATE_IDLE) {\r\npr_err("(%s): skd_stop_device no special\n",\r\nskd_name(skdev));\r\ngoto stop_out;\r\n}\r\nskdev->state = SKD_DRVR_STATE_SYNCING;\r\nskdev->sync_done = 0;\r\nskd_send_internal_skspcl(skdev, skspcl, SYNCHRONIZE_CACHE);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nwait_event_interruptible_timeout(skdev->waitq,\r\n(skdev->sync_done), (10 * HZ));\r\nspin_lock_irqsave(&skdev->lock, flags);\r\nswitch (skdev->sync_done) {\r\ncase 0:\r\npr_err("(%s): skd_stop_device no sync\n",\r\nskd_name(skdev));\r\nbreak;\r\ncase 1:\r\npr_err("(%s): skd_stop_device sync done\n",\r\nskd_name(skdev));\r\nbreak;\r\ndefault:\r\npr_err("(%s): skd_stop_device sync error\n",\r\nskd_name(skdev));\r\n}\r\nstop_out:\r\nskdev->state = SKD_DRVR_STATE_STOPPING;\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nskd_kill_timer(skdev);\r\nspin_lock_irqsave(&skdev->lock, flags);\r\nskd_disable_interrupts(skdev);\r\nSKD_WRITEL(skdev, FIT_INT_DEF_MASK, FIT_INT_STATUS_HOST);\r\nSKD_WRITEL(skdev, FIT_CR_SOFT_RESET, FIT_CONTROL);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nfor (i = 0; i < 10; i++) {\r\ndev_state =\r\nSKD_READL(skdev, FIT_STATUS) & FIT_SR_DRIVE_STATE_MASK;\r\nif (dev_state == FIT_SR_DRIVE_INIT)\r\nbreak;\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nschedule_timeout(msecs_to_jiffies(100));\r\n}\r\nif (dev_state != FIT_SR_DRIVE_INIT)\r\npr_err("(%s): skd_stop_device state error 0x%02x\n",\r\nskd_name(skdev), dev_state);\r\n}\r\nstatic void skd_restart_device(struct skd_device *skdev)\r\n{\r\nu32 state;\r\nSKD_WRITEL(skdev, FIT_INT_DEF_MASK, FIT_INT_STATUS_HOST);\r\nstate = SKD_READL(skdev, FIT_STATUS);\r\npr_debug("%s:%s:%d drive status=0x%x\n",\r\nskdev->name, __func__, __LINE__, state);\r\nstate &= FIT_SR_DRIVE_STATE_MASK;\r\nskdev->drive_state = state;\r\nskdev->last_mtd = 0;\r\nskdev->state = SKD_DRVR_STATE_RESTARTING;\r\nskdev->timer_countdown = SKD_RESTARTING_TIMO;\r\nskd_soft_reset(skdev);\r\n}\r\nstatic int skd_quiesce_dev(struct skd_device *skdev)\r\n{\r\nint rc = 0;\r\nswitch (skdev->state) {\r\ncase SKD_DRVR_STATE_BUSY:\r\ncase SKD_DRVR_STATE_BUSY_IMMINENT:\r\npr_debug("%s:%s:%d stopping %s queue\n",\r\nskdev->name, __func__, __LINE__, skdev->name);\r\nblk_stop_queue(skdev->queue);\r\nbreak;\r\ncase SKD_DRVR_STATE_ONLINE:\r\ncase SKD_DRVR_STATE_STOPPING:\r\ncase SKD_DRVR_STATE_SYNCING:\r\ncase SKD_DRVR_STATE_PAUSING:\r\ncase SKD_DRVR_STATE_PAUSED:\r\ncase SKD_DRVR_STATE_STARTING:\r\ncase SKD_DRVR_STATE_RESTARTING:\r\ncase SKD_DRVR_STATE_RESUMING:\r\ndefault:\r\nrc = -EINVAL;\r\npr_debug("%s:%s:%d state [%d] not implemented\n",\r\nskdev->name, __func__, __LINE__, skdev->state);\r\n}\r\nreturn rc;\r\n}\r\nstatic int skd_unquiesce_dev(struct skd_device *skdev)\r\n{\r\nint prev_driver_state = skdev->state;\r\nskd_log_skdev(skdev, "unquiesce");\r\nif (skdev->state == SKD_DRVR_STATE_ONLINE) {\r\npr_debug("%s:%s:%d **** device already ONLINE\n",\r\nskdev->name, __func__, __LINE__);\r\nreturn 0;\r\n}\r\nif (skdev->drive_state != FIT_SR_DRIVE_ONLINE) {\r\nskdev->state = SKD_DRVR_STATE_BUSY;\r\npr_debug("%s:%s:%d drive BUSY state\n",\r\nskdev->name, __func__, __LINE__);\r\nreturn 0;\r\n}\r\nswitch (skdev->state) {\r\ncase SKD_DRVR_STATE_PAUSED:\r\ncase SKD_DRVR_STATE_BUSY:\r\ncase SKD_DRVR_STATE_BUSY_IMMINENT:\r\ncase SKD_DRVR_STATE_BUSY_ERASE:\r\ncase SKD_DRVR_STATE_STARTING:\r\ncase SKD_DRVR_STATE_RESTARTING:\r\ncase SKD_DRVR_STATE_FAULT:\r\ncase SKD_DRVR_STATE_IDLE:\r\ncase SKD_DRVR_STATE_LOAD:\r\nskdev->state = SKD_DRVR_STATE_ONLINE;\r\npr_err("(%s): Driver state %s(%d)=>%s(%d)\n",\r\nskd_name(skdev),\r\nskd_skdev_state_to_str(prev_driver_state),\r\nprev_driver_state, skd_skdev_state_to_str(skdev->state),\r\nskdev->state);\r\npr_debug("%s:%s:%d **** device ONLINE...starting block queue\n",\r\nskdev->name, __func__, __LINE__);\r\npr_debug("%s:%s:%d starting %s queue\n",\r\nskdev->name, __func__, __LINE__, skdev->name);\r\npr_info("(%s): STEC s1120 ONLINE\n", skd_name(skdev));\r\nblk_start_queue(skdev->queue);\r\nskdev->gendisk_on = 1;\r\nwake_up_interruptible(&skdev->waitq);\r\nbreak;\r\ncase SKD_DRVR_STATE_DISAPPEARED:\r\ndefault:\r\npr_debug("%s:%s:%d **** driver state %d, not implemented \n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->state);\r\nreturn -EBUSY;\r\n}\r\nreturn 0;\r\n}\r\nstatic irqreturn_t skd_reserved_isr(int irq, void *skd_host_data)\r\n{\r\nstruct skd_device *skdev = skd_host_data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\npr_debug("%s:%s:%d MSIX = 0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nSKD_READL(skdev, FIT_INT_STATUS_HOST));\r\npr_err("(%s): MSIX reserved irq %d = 0x%x\n", skd_name(skdev),\r\nirq, SKD_READL(skdev, FIT_INT_STATUS_HOST));\r\nSKD_WRITEL(skdev, FIT_INT_RESERVED_MASK, FIT_INT_STATUS_HOST);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t skd_statec_isr(int irq, void *skd_host_data)\r\n{\r\nstruct skd_device *skdev = skd_host_data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\npr_debug("%s:%s:%d MSIX = 0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nSKD_READL(skdev, FIT_INT_STATUS_HOST));\r\nSKD_WRITEL(skdev, FIT_ISH_FW_STATE_CHANGE, FIT_INT_STATUS_HOST);\r\nskd_isr_fwstate(skdev);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t skd_comp_q(int irq, void *skd_host_data)\r\n{\r\nstruct skd_device *skdev = skd_host_data;\r\nunsigned long flags;\r\nint flush_enqueued = 0;\r\nint deferred;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\npr_debug("%s:%s:%d MSIX = 0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nSKD_READL(skdev, FIT_INT_STATUS_HOST));\r\nSKD_WRITEL(skdev, FIT_ISH_COMPLETION_POSTED, FIT_INT_STATUS_HOST);\r\ndeferred = skd_isr_completion_posted(skdev, skd_isr_comp_limit,\r\n&flush_enqueued);\r\nif (flush_enqueued)\r\nskd_request_fn(skdev->queue);\r\nif (deferred)\r\nschedule_work(&skdev->completion_worker);\r\nelse if (!flush_enqueued)\r\nskd_request_fn(skdev->queue);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t skd_msg_isr(int irq, void *skd_host_data)\r\n{\r\nstruct skd_device *skdev = skd_host_data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\npr_debug("%s:%s:%d MSIX = 0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nSKD_READL(skdev, FIT_INT_STATUS_HOST));\r\nSKD_WRITEL(skdev, FIT_ISH_MSG_FROM_DEV, FIT_INT_STATUS_HOST);\r\nskd_isr_msg_from_dev(skdev);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t skd_qfull_isr(int irq, void *skd_host_data)\r\n{\r\nstruct skd_device *skdev = skd_host_data;\r\nunsigned long flags;\r\nspin_lock_irqsave(&skdev->lock, flags);\r\npr_debug("%s:%s:%d MSIX = 0x%x\n",\r\nskdev->name, __func__, __LINE__,\r\nSKD_READL(skdev, FIT_INT_STATUS_HOST));\r\nSKD_WRITEL(skdev, FIT_INT_QUEUE_FULL, FIT_INT_STATUS_HOST);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void skd_release_msix(struct skd_device *skdev)\r\n{\r\nstruct skd_msix_entry *qentry;\r\nint i;\r\nif (skdev->msix_entries) {\r\nfor (i = 0; i < skdev->msix_count; i++) {\r\nqentry = &skdev->msix_entries[i];\r\nskdev = qentry->rsp;\r\nif (qentry->have_irq)\r\ndevm_free_irq(&skdev->pdev->dev,\r\nqentry->vector, qentry->rsp);\r\n}\r\nkfree(skdev->msix_entries);\r\n}\r\nif (skdev->msix_count)\r\npci_disable_msix(skdev->pdev);\r\nskdev->msix_count = 0;\r\nskdev->msix_entries = NULL;\r\n}\r\nstatic int skd_acquire_msix(struct skd_device *skdev)\r\n{\r\nint i, rc;\r\nstruct pci_dev *pdev = skdev->pdev;\r\nstruct msix_entry *entries;\r\nstruct skd_msix_entry *qentry;\r\nentries = kzalloc(sizeof(struct msix_entry) * SKD_MAX_MSIX_COUNT,\r\nGFP_KERNEL);\r\nif (!entries)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < SKD_MAX_MSIX_COUNT; i++)\r\nentries[i].entry = i;\r\nrc = pci_enable_msix_exact(pdev, entries, SKD_MAX_MSIX_COUNT);\r\nif (rc) {\r\npr_err("(%s): failed to enable MSI-X %d\n",\r\nskd_name(skdev), rc);\r\ngoto msix_out;\r\n}\r\nskdev->msix_count = SKD_MAX_MSIX_COUNT;\r\nskdev->msix_entries = kzalloc(sizeof(struct skd_msix_entry) *\r\nskdev->msix_count, GFP_KERNEL);\r\nif (!skdev->msix_entries) {\r\nrc = -ENOMEM;\r\npr_err("(%s): msix table allocation error\n",\r\nskd_name(skdev));\r\ngoto msix_out;\r\n}\r\nfor (i = 0; i < skdev->msix_count; i++) {\r\nqentry = &skdev->msix_entries[i];\r\nqentry->vector = entries[i].vector;\r\nqentry->entry = entries[i].entry;\r\nqentry->rsp = NULL;\r\nqentry->have_irq = 0;\r\npr_debug("%s:%s:%d %s: <%s> msix (%d) vec %d, entry %x\n",\r\nskdev->name, __func__, __LINE__,\r\npci_name(pdev), skdev->name,\r\ni, qentry->vector, qentry->entry);\r\n}\r\nfor (i = 0; i < skdev->msix_count; i++) {\r\nqentry = &skdev->msix_entries[i];\r\nsnprintf(qentry->isr_name, sizeof(qentry->isr_name),\r\n"%s%d-msix %s", DRV_NAME, skdev->devno,\r\nmsix_entries[i].name);\r\nrc = devm_request_irq(&skdev->pdev->dev, qentry->vector,\r\nmsix_entries[i].handler, 0,\r\nqentry->isr_name, skdev);\r\nif (rc) {\r\npr_err("(%s): Unable to register(%d) MSI-X "\r\n"handler %d: %s\n",\r\nskd_name(skdev), rc, i, qentry->isr_name);\r\ngoto msix_out;\r\n} else {\r\nqentry->have_irq = 1;\r\nqentry->rsp = skdev;\r\n}\r\n}\r\npr_debug("%s:%s:%d %s: <%s> msix %d irq(s) enabled\n",\r\nskdev->name, __func__, __LINE__,\r\npci_name(pdev), skdev->name, skdev->msix_count);\r\nreturn 0;\r\nmsix_out:\r\nif (entries)\r\nkfree(entries);\r\nskd_release_msix(skdev);\r\nreturn rc;\r\n}\r\nstatic int skd_acquire_irq(struct skd_device *skdev)\r\n{\r\nint rc;\r\nstruct pci_dev *pdev;\r\npdev = skdev->pdev;\r\nskdev->msix_count = 0;\r\nRETRY_IRQ_TYPE:\r\nswitch (skdev->irq_type) {\r\ncase SKD_IRQ_MSIX:\r\nrc = skd_acquire_msix(skdev);\r\nif (!rc)\r\npr_info("(%s): MSI-X %d irqs enabled\n",\r\nskd_name(skdev), skdev->msix_count);\r\nelse {\r\npr_err(\r\n"(%s): failed to enable MSI-X, re-trying with MSI %d\n",\r\nskd_name(skdev), rc);\r\nskdev->irq_type = SKD_IRQ_MSI;\r\ngoto RETRY_IRQ_TYPE;\r\n}\r\nbreak;\r\ncase SKD_IRQ_MSI:\r\nsnprintf(skdev->isr_name, sizeof(skdev->isr_name), "%s%d-msi",\r\nDRV_NAME, skdev->devno);\r\nrc = pci_enable_msi_range(pdev, 1, 1);\r\nif (rc > 0) {\r\nrc = devm_request_irq(&pdev->dev, pdev->irq, skd_isr, 0,\r\nskdev->isr_name, skdev);\r\nif (rc) {\r\npci_disable_msi(pdev);\r\npr_err(\r\n"(%s): failed to allocate the MSI interrupt %d\n",\r\nskd_name(skdev), rc);\r\ngoto RETRY_IRQ_LEGACY;\r\n}\r\npr_info("(%s): MSI irq %d enabled\n",\r\nskd_name(skdev), pdev->irq);\r\n} else {\r\nRETRY_IRQ_LEGACY:\r\npr_err(\r\n"(%s): failed to enable MSI, re-trying with LEGACY %d\n",\r\nskd_name(skdev), rc);\r\nskdev->irq_type = SKD_IRQ_LEGACY;\r\ngoto RETRY_IRQ_TYPE;\r\n}\r\nbreak;\r\ncase SKD_IRQ_LEGACY:\r\nsnprintf(skdev->isr_name, sizeof(skdev->isr_name),\r\n"%s%d-legacy", DRV_NAME, skdev->devno);\r\nrc = devm_request_irq(&pdev->dev, pdev->irq, skd_isr,\r\nIRQF_SHARED, skdev->isr_name, skdev);\r\nif (!rc)\r\npr_info("(%s): LEGACY irq %d enabled\n",\r\nskd_name(skdev), pdev->irq);\r\nelse\r\npr_err("(%s): request LEGACY irq error %d\n",\r\nskd_name(skdev), rc);\r\nbreak;\r\ndefault:\r\npr_info("(%s): irq_type %d invalid, re-set to %d\n",\r\nskd_name(skdev), skdev->irq_type, SKD_IRQ_DEFAULT);\r\nskdev->irq_type = SKD_IRQ_LEGACY;\r\ngoto RETRY_IRQ_TYPE;\r\n}\r\nreturn rc;\r\n}\r\nstatic void skd_release_irq(struct skd_device *skdev)\r\n{\r\nswitch (skdev->irq_type) {\r\ncase SKD_IRQ_MSIX:\r\nskd_release_msix(skdev);\r\nbreak;\r\ncase SKD_IRQ_MSI:\r\ndevm_free_irq(&skdev->pdev->dev, skdev->pdev->irq, skdev);\r\npci_disable_msi(skdev->pdev);\r\nbreak;\r\ncase SKD_IRQ_LEGACY:\r\ndevm_free_irq(&skdev->pdev->dev, skdev->pdev->irq, skdev);\r\nbreak;\r\ndefault:\r\npr_err("(%s): wrong irq type %d!",\r\nskd_name(skdev), skdev->irq_type);\r\nbreak;\r\n}\r\n}\r\nstatic int skd_cons_skcomp(struct skd_device *skdev)\r\n{\r\nint rc = 0;\r\nstruct fit_completion_entry_v1 *skcomp;\r\nu32 nbytes;\r\nnbytes = sizeof(*skcomp) * SKD_N_COMPLETION_ENTRY;\r\nnbytes += sizeof(struct fit_comp_error_info) * SKD_N_COMPLETION_ENTRY;\r\npr_debug("%s:%s:%d comp pci_alloc, total bytes %d entries %d\n",\r\nskdev->name, __func__, __LINE__,\r\nnbytes, SKD_N_COMPLETION_ENTRY);\r\nskcomp = pci_zalloc_consistent(skdev->pdev, nbytes,\r\n&skdev->cq_dma_address);\r\nif (skcomp == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nskdev->skcomp_table = skcomp;\r\nskdev->skerr_table = (struct fit_comp_error_info *)((char *)skcomp +\r\nsizeof(*skcomp) *\r\nSKD_N_COMPLETION_ENTRY);\r\nerr_out:\r\nreturn rc;\r\n}\r\nstatic int skd_cons_skmsg(struct skd_device *skdev)\r\n{\r\nint rc = 0;\r\nu32 i;\r\npr_debug("%s:%s:%d skmsg_table kzalloc, struct %lu, count %u total %lu\n",\r\nskdev->name, __func__, __LINE__,\r\nsizeof(struct skd_fitmsg_context),\r\nskdev->num_fitmsg_context,\r\nsizeof(struct skd_fitmsg_context) * skdev->num_fitmsg_context);\r\nskdev->skmsg_table = kzalloc(sizeof(struct skd_fitmsg_context)\r\n*skdev->num_fitmsg_context, GFP_KERNEL);\r\nif (skdev->skmsg_table == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nfor (i = 0; i < skdev->num_fitmsg_context; i++) {\r\nstruct skd_fitmsg_context *skmsg;\r\nskmsg = &skdev->skmsg_table[i];\r\nskmsg->id = i + SKD_ID_FIT_MSG;\r\nskmsg->state = SKD_MSG_STATE_IDLE;\r\nskmsg->msg_buf = pci_alloc_consistent(skdev->pdev,\r\nSKD_N_FITMSG_BYTES + 64,\r\n&skmsg->mb_dma_address);\r\nif (skmsg->msg_buf == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nskmsg->offset = (u32)((u64)skmsg->msg_buf &\r\n(~FIT_QCMD_BASE_ADDRESS_MASK));\r\nskmsg->msg_buf += ~FIT_QCMD_BASE_ADDRESS_MASK;\r\nskmsg->msg_buf = (u8 *)((u64)skmsg->msg_buf &\r\nFIT_QCMD_BASE_ADDRESS_MASK);\r\nskmsg->mb_dma_address += ~FIT_QCMD_BASE_ADDRESS_MASK;\r\nskmsg->mb_dma_address &= FIT_QCMD_BASE_ADDRESS_MASK;\r\nmemset(skmsg->msg_buf, 0, SKD_N_FITMSG_BYTES);\r\nskmsg->next = &skmsg[1];\r\n}\r\nskdev->skmsg_table[i - 1].next = NULL;\r\nskdev->skmsg_free_list = skdev->skmsg_table;\r\nerr_out:\r\nreturn rc;\r\n}\r\nstatic struct fit_sg_descriptor *skd_cons_sg_list(struct skd_device *skdev,\r\nu32 n_sg,\r\ndma_addr_t *ret_dma_addr)\r\n{\r\nstruct fit_sg_descriptor *sg_list;\r\nu32 nbytes;\r\nnbytes = sizeof(*sg_list) * n_sg;\r\nsg_list = pci_alloc_consistent(skdev->pdev, nbytes, ret_dma_addr);\r\nif (sg_list != NULL) {\r\nuint64_t dma_address = *ret_dma_addr;\r\nu32 i;\r\nmemset(sg_list, 0, nbytes);\r\nfor (i = 0; i < n_sg - 1; i++) {\r\nuint64_t ndp_off;\r\nndp_off = (i + 1) * sizeof(struct fit_sg_descriptor);\r\nsg_list[i].next_desc_ptr = dma_address + ndp_off;\r\n}\r\nsg_list[i].next_desc_ptr = 0LL;\r\n}\r\nreturn sg_list;\r\n}\r\nstatic int skd_cons_skreq(struct skd_device *skdev)\r\n{\r\nint rc = 0;\r\nu32 i;\r\npr_debug("%s:%s:%d skreq_table kzalloc, struct %lu, count %u total %lu\n",\r\nskdev->name, __func__, __LINE__,\r\nsizeof(struct skd_request_context),\r\nskdev->num_req_context,\r\nsizeof(struct skd_request_context) * skdev->num_req_context);\r\nskdev->skreq_table = kzalloc(sizeof(struct skd_request_context)\r\n* skdev->num_req_context, GFP_KERNEL);\r\nif (skdev->skreq_table == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\npr_debug("%s:%s:%d alloc sg_table sg_per_req %u scatlist %lu total %lu\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->sgs_per_request, sizeof(struct scatterlist),\r\nskdev->sgs_per_request * sizeof(struct scatterlist));\r\nfor (i = 0; i < skdev->num_req_context; i++) {\r\nstruct skd_request_context *skreq;\r\nskreq = &skdev->skreq_table[i];\r\nskreq->id = i + SKD_ID_RW_REQUEST;\r\nskreq->state = SKD_REQ_STATE_IDLE;\r\nskreq->sg = kzalloc(sizeof(struct scatterlist) *\r\nskdev->sgs_per_request, GFP_KERNEL);\r\nif (skreq->sg == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nsg_init_table(skreq->sg, skdev->sgs_per_request);\r\nskreq->sksg_list = skd_cons_sg_list(skdev,\r\nskdev->sgs_per_request,\r\n&skreq->sksg_dma_address);\r\nif (skreq->sksg_list == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nskreq->next = &skreq[1];\r\n}\r\nskdev->skreq_table[i - 1].next = NULL;\r\nskdev->skreq_free_list = skdev->skreq_table;\r\nerr_out:\r\nreturn rc;\r\n}\r\nstatic int skd_cons_skspcl(struct skd_device *skdev)\r\n{\r\nint rc = 0;\r\nu32 i, nbytes;\r\npr_debug("%s:%s:%d skspcl_table kzalloc, struct %lu, count %u total %lu\n",\r\nskdev->name, __func__, __LINE__,\r\nsizeof(struct skd_special_context),\r\nskdev->n_special,\r\nsizeof(struct skd_special_context) * skdev->n_special);\r\nskdev->skspcl_table = kzalloc(sizeof(struct skd_special_context)\r\n* skdev->n_special, GFP_KERNEL);\r\nif (skdev->skspcl_table == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nfor (i = 0; i < skdev->n_special; i++) {\r\nstruct skd_special_context *skspcl;\r\nskspcl = &skdev->skspcl_table[i];\r\nskspcl->req.id = i + SKD_ID_SPECIAL_REQUEST;\r\nskspcl->req.state = SKD_REQ_STATE_IDLE;\r\nskspcl->req.next = &skspcl[1].req;\r\nnbytes = SKD_N_SPECIAL_FITMSG_BYTES;\r\nskspcl->msg_buf =\r\npci_zalloc_consistent(skdev->pdev, nbytes,\r\n&skspcl->mb_dma_address);\r\nif (skspcl->msg_buf == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nskspcl->req.sg = kzalloc(sizeof(struct scatterlist) *\r\nSKD_N_SG_PER_SPECIAL, GFP_KERNEL);\r\nif (skspcl->req.sg == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nskspcl->req.sksg_list = skd_cons_sg_list(skdev,\r\nSKD_N_SG_PER_SPECIAL,\r\n&skspcl->req.\r\nsksg_dma_address);\r\nif (skspcl->req.sksg_list == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\n}\r\nskdev->skspcl_table[i - 1].req.next = NULL;\r\nskdev->skspcl_free_list = skdev->skspcl_table;\r\nreturn rc;\r\nerr_out:\r\nreturn rc;\r\n}\r\nstatic int skd_cons_sksb(struct skd_device *skdev)\r\n{\r\nint rc = 0;\r\nstruct skd_special_context *skspcl;\r\nu32 nbytes;\r\nskspcl = &skdev->internal_skspcl;\r\nskspcl->req.id = 0 + SKD_ID_INTERNAL;\r\nskspcl->req.state = SKD_REQ_STATE_IDLE;\r\nnbytes = SKD_N_INTERNAL_BYTES;\r\nskspcl->data_buf = pci_zalloc_consistent(skdev->pdev, nbytes,\r\n&skspcl->db_dma_address);\r\nif (skspcl->data_buf == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nnbytes = SKD_N_SPECIAL_FITMSG_BYTES;\r\nskspcl->msg_buf = pci_zalloc_consistent(skdev->pdev, nbytes,\r\n&skspcl->mb_dma_address);\r\nif (skspcl->msg_buf == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nskspcl->req.sksg_list = skd_cons_sg_list(skdev, 1,\r\n&skspcl->req.sksg_dma_address);\r\nif (skspcl->req.sksg_list == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nif (!skd_format_internal_skspcl(skdev)) {\r\nrc = -EINVAL;\r\ngoto err_out;\r\n}\r\nerr_out:\r\nreturn rc;\r\n}\r\nstatic int skd_cons_disk(struct skd_device *skdev)\r\n{\r\nint rc = 0;\r\nstruct gendisk *disk;\r\nstruct request_queue *q;\r\nunsigned long flags;\r\ndisk = alloc_disk(SKD_MINORS_PER_DEVICE);\r\nif (!disk) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nskdev->disk = disk;\r\nsprintf(disk->disk_name, DRV_NAME "%u", skdev->devno);\r\ndisk->major = skdev->major;\r\ndisk->first_minor = skdev->devno * SKD_MINORS_PER_DEVICE;\r\ndisk->fops = &skd_blockdev_ops;\r\ndisk->private_data = skdev;\r\nq = blk_init_queue(skd_request_fn, &skdev->lock);\r\nif (!q) {\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nskdev->queue = q;\r\ndisk->queue = q;\r\nq->queuedata = skdev;\r\nblk_queue_flush(q, REQ_FLUSH | REQ_FUA);\r\nblk_queue_max_segments(q, skdev->sgs_per_request);\r\nblk_queue_max_hw_sectors(q, SKD_N_MAX_SECTORS);\r\nblk_queue_io_opt(q, 8192);\r\nq->limits.discard_granularity = 8192;\r\nq->limits.discard_alignment = 0;\r\nq->limits.max_discard_sectors = UINT_MAX >> 9;\r\nq->limits.discard_zeroes_data = 1;\r\nqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, q);\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_ADD_RANDOM, q);\r\nspin_lock_irqsave(&skdev->lock, flags);\r\npr_debug("%s:%s:%d stopping %s queue\n",\r\nskdev->name, __func__, __LINE__, skdev->name);\r\nblk_stop_queue(skdev->queue);\r\nspin_unlock_irqrestore(&skdev->lock, flags);\r\nerr_out:\r\nreturn rc;\r\n}\r\nstatic struct skd_device *skd_construct(struct pci_dev *pdev)\r\n{\r\nstruct skd_device *skdev;\r\nint blk_major = skd_major;\r\nint rc;\r\nskdev = kzalloc(sizeof(*skdev), GFP_KERNEL);\r\nif (!skdev) {\r\npr_err(PFX "(%s): memory alloc failure\n",\r\npci_name(pdev));\r\nreturn NULL;\r\n}\r\nskdev->state = SKD_DRVR_STATE_LOAD;\r\nskdev->pdev = pdev;\r\nskdev->devno = skd_next_devno++;\r\nskdev->major = blk_major;\r\nskdev->irq_type = skd_isr_type;\r\nsprintf(skdev->name, DRV_NAME "%d", skdev->devno);\r\nskdev->dev_max_queue_depth = 0;\r\nskdev->num_req_context = skd_max_queue_depth;\r\nskdev->num_fitmsg_context = skd_max_queue_depth;\r\nskdev->n_special = skd_max_pass_thru;\r\nskdev->cur_max_queue_depth = 1;\r\nskdev->queue_low_water_mark = 1;\r\nskdev->proto_ver = 99;\r\nskdev->sgs_per_request = skd_sgs_per_request;\r\nskdev->dbg_level = skd_dbg_level;\r\natomic_set(&skdev->device_count, 0);\r\nspin_lock_init(&skdev->lock);\r\nINIT_WORK(&skdev->completion_worker, skd_completion_worker);\r\npr_debug("%s:%s:%d skcomp\n", skdev->name, __func__, __LINE__);\r\nrc = skd_cons_skcomp(skdev);\r\nif (rc < 0)\r\ngoto err_out;\r\npr_debug("%s:%s:%d skmsg\n", skdev->name, __func__, __LINE__);\r\nrc = skd_cons_skmsg(skdev);\r\nif (rc < 0)\r\ngoto err_out;\r\npr_debug("%s:%s:%d skreq\n", skdev->name, __func__, __LINE__);\r\nrc = skd_cons_skreq(skdev);\r\nif (rc < 0)\r\ngoto err_out;\r\npr_debug("%s:%s:%d skspcl\n", skdev->name, __func__, __LINE__);\r\nrc = skd_cons_skspcl(skdev);\r\nif (rc < 0)\r\ngoto err_out;\r\npr_debug("%s:%s:%d sksb\n", skdev->name, __func__, __LINE__);\r\nrc = skd_cons_sksb(skdev);\r\nif (rc < 0)\r\ngoto err_out;\r\npr_debug("%s:%s:%d disk\n", skdev->name, __func__, __LINE__);\r\nrc = skd_cons_disk(skdev);\r\nif (rc < 0)\r\ngoto err_out;\r\npr_debug("%s:%s:%d VICTORY\n", skdev->name, __func__, __LINE__);\r\nreturn skdev;\r\nerr_out:\r\npr_debug("%s:%s:%d construct failed\n",\r\nskdev->name, __func__, __LINE__);\r\nskd_destruct(skdev);\r\nreturn NULL;\r\n}\r\nstatic void skd_free_skcomp(struct skd_device *skdev)\r\n{\r\nif (skdev->skcomp_table != NULL) {\r\nu32 nbytes;\r\nnbytes = sizeof(skdev->skcomp_table[0]) *\r\nSKD_N_COMPLETION_ENTRY;\r\npci_free_consistent(skdev->pdev, nbytes,\r\nskdev->skcomp_table, skdev->cq_dma_address);\r\n}\r\nskdev->skcomp_table = NULL;\r\nskdev->cq_dma_address = 0;\r\n}\r\nstatic void skd_free_skmsg(struct skd_device *skdev)\r\n{\r\nu32 i;\r\nif (skdev->skmsg_table == NULL)\r\nreturn;\r\nfor (i = 0; i < skdev->num_fitmsg_context; i++) {\r\nstruct skd_fitmsg_context *skmsg;\r\nskmsg = &skdev->skmsg_table[i];\r\nif (skmsg->msg_buf != NULL) {\r\nskmsg->msg_buf += skmsg->offset;\r\nskmsg->mb_dma_address += skmsg->offset;\r\npci_free_consistent(skdev->pdev, SKD_N_FITMSG_BYTES,\r\nskmsg->msg_buf,\r\nskmsg->mb_dma_address);\r\n}\r\nskmsg->msg_buf = NULL;\r\nskmsg->mb_dma_address = 0;\r\n}\r\nkfree(skdev->skmsg_table);\r\nskdev->skmsg_table = NULL;\r\n}\r\nstatic void skd_free_sg_list(struct skd_device *skdev,\r\nstruct fit_sg_descriptor *sg_list,\r\nu32 n_sg, dma_addr_t dma_addr)\r\n{\r\nif (sg_list != NULL) {\r\nu32 nbytes;\r\nnbytes = sizeof(*sg_list) * n_sg;\r\npci_free_consistent(skdev->pdev, nbytes, sg_list, dma_addr);\r\n}\r\n}\r\nstatic void skd_free_skreq(struct skd_device *skdev)\r\n{\r\nu32 i;\r\nif (skdev->skreq_table == NULL)\r\nreturn;\r\nfor (i = 0; i < skdev->num_req_context; i++) {\r\nstruct skd_request_context *skreq;\r\nskreq = &skdev->skreq_table[i];\r\nskd_free_sg_list(skdev, skreq->sksg_list,\r\nskdev->sgs_per_request,\r\nskreq->sksg_dma_address);\r\nskreq->sksg_list = NULL;\r\nskreq->sksg_dma_address = 0;\r\nkfree(skreq->sg);\r\n}\r\nkfree(skdev->skreq_table);\r\nskdev->skreq_table = NULL;\r\n}\r\nstatic void skd_free_skspcl(struct skd_device *skdev)\r\n{\r\nu32 i;\r\nu32 nbytes;\r\nif (skdev->skspcl_table == NULL)\r\nreturn;\r\nfor (i = 0; i < skdev->n_special; i++) {\r\nstruct skd_special_context *skspcl;\r\nskspcl = &skdev->skspcl_table[i];\r\nif (skspcl->msg_buf != NULL) {\r\nnbytes = SKD_N_SPECIAL_FITMSG_BYTES;\r\npci_free_consistent(skdev->pdev, nbytes,\r\nskspcl->msg_buf,\r\nskspcl->mb_dma_address);\r\n}\r\nskspcl->msg_buf = NULL;\r\nskspcl->mb_dma_address = 0;\r\nskd_free_sg_list(skdev, skspcl->req.sksg_list,\r\nSKD_N_SG_PER_SPECIAL,\r\nskspcl->req.sksg_dma_address);\r\nskspcl->req.sksg_list = NULL;\r\nskspcl->req.sksg_dma_address = 0;\r\nkfree(skspcl->req.sg);\r\n}\r\nkfree(skdev->skspcl_table);\r\nskdev->skspcl_table = NULL;\r\n}\r\nstatic void skd_free_sksb(struct skd_device *skdev)\r\n{\r\nstruct skd_special_context *skspcl;\r\nu32 nbytes;\r\nskspcl = &skdev->internal_skspcl;\r\nif (skspcl->data_buf != NULL) {\r\nnbytes = SKD_N_INTERNAL_BYTES;\r\npci_free_consistent(skdev->pdev, nbytes,\r\nskspcl->data_buf, skspcl->db_dma_address);\r\n}\r\nskspcl->data_buf = NULL;\r\nskspcl->db_dma_address = 0;\r\nif (skspcl->msg_buf != NULL) {\r\nnbytes = SKD_N_SPECIAL_FITMSG_BYTES;\r\npci_free_consistent(skdev->pdev, nbytes,\r\nskspcl->msg_buf, skspcl->mb_dma_address);\r\n}\r\nskspcl->msg_buf = NULL;\r\nskspcl->mb_dma_address = 0;\r\nskd_free_sg_list(skdev, skspcl->req.sksg_list, 1,\r\nskspcl->req.sksg_dma_address);\r\nskspcl->req.sksg_list = NULL;\r\nskspcl->req.sksg_dma_address = 0;\r\n}\r\nstatic void skd_free_disk(struct skd_device *skdev)\r\n{\r\nstruct gendisk *disk = skdev->disk;\r\nif (disk != NULL) {\r\nstruct request_queue *q = disk->queue;\r\nif (disk->flags & GENHD_FL_UP)\r\ndel_gendisk(disk);\r\nif (q)\r\nblk_cleanup_queue(q);\r\nput_disk(disk);\r\n}\r\nskdev->disk = NULL;\r\n}\r\nstatic void skd_destruct(struct skd_device *skdev)\r\n{\r\nif (skdev == NULL)\r\nreturn;\r\npr_debug("%s:%s:%d disk\n", skdev->name, __func__, __LINE__);\r\nskd_free_disk(skdev);\r\npr_debug("%s:%s:%d sksb\n", skdev->name, __func__, __LINE__);\r\nskd_free_sksb(skdev);\r\npr_debug("%s:%s:%d skspcl\n", skdev->name, __func__, __LINE__);\r\nskd_free_skspcl(skdev);\r\npr_debug("%s:%s:%d skreq\n", skdev->name, __func__, __LINE__);\r\nskd_free_skreq(skdev);\r\npr_debug("%s:%s:%d skmsg\n", skdev->name, __func__, __LINE__);\r\nskd_free_skmsg(skdev);\r\npr_debug("%s:%s:%d skcomp\n", skdev->name, __func__, __LINE__);\r\nskd_free_skcomp(skdev);\r\npr_debug("%s:%s:%d skdev\n", skdev->name, __func__, __LINE__);\r\nkfree(skdev);\r\n}\r\nstatic int skd_bdev_getgeo(struct block_device *bdev, struct hd_geometry *geo)\r\n{\r\nstruct skd_device *skdev;\r\nu64 capacity;\r\nskdev = bdev->bd_disk->private_data;\r\npr_debug("%s:%s:%d %s: CMD[%s] getgeo device\n",\r\nskdev->name, __func__, __LINE__,\r\nbdev->bd_disk->disk_name, current->comm);\r\nif (skdev->read_cap_is_valid) {\r\ncapacity = get_capacity(skdev->disk);\r\ngeo->heads = 64;\r\ngeo->sectors = 255;\r\ngeo->cylinders = (capacity) / (255 * 64);\r\nreturn 0;\r\n}\r\nreturn -EIO;\r\n}\r\nstatic int skd_bdev_attach(struct skd_device *skdev)\r\n{\r\npr_debug("%s:%s:%d add_disk\n", skdev->name, __func__, __LINE__);\r\nadd_disk(skdev->disk);\r\nreturn 0;\r\n}\r\nstatic char *skd_pci_info(struct skd_device *skdev, char *str)\r\n{\r\nint pcie_reg;\r\nstrcpy(str, "PCIe (");\r\npcie_reg = pci_find_capability(skdev->pdev, PCI_CAP_ID_EXP);\r\nif (pcie_reg) {\r\nchar lwstr[6];\r\nuint16_t pcie_lstat, lspeed, lwidth;\r\npcie_reg += 0x12;\r\npci_read_config_word(skdev->pdev, pcie_reg, &pcie_lstat);\r\nlspeed = pcie_lstat & (0xF);\r\nlwidth = (pcie_lstat & 0x3F0) >> 4;\r\nif (lspeed == 1)\r\nstrcat(str, "2.5GT/s ");\r\nelse if (lspeed == 2)\r\nstrcat(str, "5.0GT/s ");\r\nelse\r\nstrcat(str, "<unknown> ");\r\nsnprintf(lwstr, sizeof(lwstr), "%dX)", lwidth);\r\nstrcat(str, lwstr);\r\n}\r\nreturn str;\r\n}\r\nstatic int skd_pci_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nint i;\r\nint rc = 0;\r\nchar pci_str[32];\r\nstruct skd_device *skdev;\r\npr_info("STEC s1120 Driver(%s) version %s-b%s\n",\r\nDRV_NAME, DRV_VERSION, DRV_BUILD_ID);\r\npr_info("(skd?:??:[%s]): vendor=%04X device=%04x\n",\r\npci_name(pdev), pdev->vendor, pdev->device);\r\nrc = pci_enable_device(pdev);\r\nif (rc)\r\nreturn rc;\r\nrc = pci_request_regions(pdev, DRV_NAME);\r\nif (rc)\r\ngoto err_out;\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(64));\r\nif (!rc) {\r\nif (pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64))) {\r\npr_err("(%s): consistent DMA mask error %d\n",\r\npci_name(pdev), rc);\r\n}\r\n} else {\r\n(rc = pci_set_dma_mask(pdev, DMA_BIT_MASK(32)));\r\nif (rc) {\r\npr_err("(%s): DMA mask error %d\n",\r\npci_name(pdev), rc);\r\ngoto err_out_regions;\r\n}\r\n}\r\nif (!skd_major) {\r\nrc = register_blkdev(0, DRV_NAME);\r\nif (rc < 0)\r\ngoto err_out_regions;\r\nBUG_ON(!rc);\r\nskd_major = rc;\r\n}\r\nskdev = skd_construct(pdev);\r\nif (skdev == NULL) {\r\nrc = -ENOMEM;\r\ngoto err_out_regions;\r\n}\r\nskd_pci_info(skdev, pci_str);\r\npr_info("(%s): %s 64bit\n", skd_name(skdev), pci_str);\r\npci_set_master(pdev);\r\nrc = pci_enable_pcie_error_reporting(pdev);\r\nif (rc) {\r\npr_err(\r\n"(%s): bad enable of PCIe error reporting rc=%d\n",\r\nskd_name(skdev), rc);\r\nskdev->pcie_error_reporting_is_enabled = 0;\r\n} else\r\nskdev->pcie_error_reporting_is_enabled = 1;\r\npci_set_drvdata(pdev, skdev);\r\nskdev->disk->driverfs_dev = &pdev->dev;\r\nfor (i = 0; i < SKD_MAX_BARS; i++) {\r\nskdev->mem_phys[i] = pci_resource_start(pdev, i);\r\nskdev->mem_size[i] = (u32)pci_resource_len(pdev, i);\r\nskdev->mem_map[i] = ioremap(skdev->mem_phys[i],\r\nskdev->mem_size[i]);\r\nif (!skdev->mem_map[i]) {\r\npr_err("(%s): Unable to map adapter memory!\n",\r\nskd_name(skdev));\r\nrc = -ENODEV;\r\ngoto err_out_iounmap;\r\n}\r\npr_debug("%s:%s:%d mem_map=%p, phyd=%016llx, size=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->mem_map[i],\r\n(uint64_t)skdev->mem_phys[i], skdev->mem_size[i]);\r\n}\r\nrc = skd_acquire_irq(skdev);\r\nif (rc) {\r\npr_err("(%s): interrupt resource error %d\n",\r\nskd_name(skdev), rc);\r\ngoto err_out_iounmap;\r\n}\r\nrc = skd_start_timer(skdev);\r\nif (rc)\r\ngoto err_out_timer;\r\ninit_waitqueue_head(&skdev->waitq);\r\nskd_start_device(skdev);\r\nrc = wait_event_interruptible_timeout(skdev->waitq,\r\n(skdev->gendisk_on),\r\n(SKD_START_WAIT_SECONDS * HZ));\r\nif (skdev->gendisk_on > 0) {\r\nskd_bdev_attach(skdev);\r\nrc = 0;\r\n} else {\r\npr_err(\r\n"(%s): error: waiting for s1120 timed out %d!\n",\r\nskd_name(skdev), rc);\r\nif (!rc)\r\nrc = -ENXIO;\r\ngoto err_out_timer;\r\n}\r\n#ifdef SKD_VMK_POLL_HANDLER\r\nif (skdev->irq_type == SKD_IRQ_MSIX) {\r\nvmklnx_scsi_register_poll_handler(skdev->scsi_host,\r\nskdev->msix_entries[5].vector,\r\nskd_comp_q, skdev);\r\n} else {\r\nvmklnx_scsi_register_poll_handler(skdev->scsi_host,\r\nskdev->pdev->irq, skd_isr,\r\nskdev);\r\n}\r\n#endif\r\nreturn rc;\r\nerr_out_timer:\r\nskd_stop_device(skdev);\r\nskd_release_irq(skdev);\r\nerr_out_iounmap:\r\nfor (i = 0; i < SKD_MAX_BARS; i++)\r\nif (skdev->mem_map[i])\r\niounmap(skdev->mem_map[i]);\r\nif (skdev->pcie_error_reporting_is_enabled)\r\npci_disable_pcie_error_reporting(pdev);\r\nskd_destruct(skdev);\r\nerr_out_regions:\r\npci_release_regions(pdev);\r\nerr_out:\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\nreturn rc;\r\n}\r\nstatic void skd_pci_remove(struct pci_dev *pdev)\r\n{\r\nint i;\r\nstruct skd_device *skdev;\r\nskdev = pci_get_drvdata(pdev);\r\nif (!skdev) {\r\npr_err("%s: no device data for PCI\n", pci_name(pdev));\r\nreturn;\r\n}\r\nskd_stop_device(skdev);\r\nskd_release_irq(skdev);\r\nfor (i = 0; i < SKD_MAX_BARS; i++)\r\nif (skdev->mem_map[i])\r\niounmap((u32 *)skdev->mem_map[i]);\r\nif (skdev->pcie_error_reporting_is_enabled)\r\npci_disable_pcie_error_reporting(pdev);\r\nskd_destruct(skdev);\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\nreturn;\r\n}\r\nstatic int skd_pci_suspend(struct pci_dev *pdev, pm_message_t state)\r\n{\r\nint i;\r\nstruct skd_device *skdev;\r\nskdev = pci_get_drvdata(pdev);\r\nif (!skdev) {\r\npr_err("%s: no device data for PCI\n", pci_name(pdev));\r\nreturn -EIO;\r\n}\r\nskd_stop_device(skdev);\r\nskd_release_irq(skdev);\r\nfor (i = 0; i < SKD_MAX_BARS; i++)\r\nif (skdev->mem_map[i])\r\niounmap((u32 *)skdev->mem_map[i]);\r\nif (skdev->pcie_error_reporting_is_enabled)\r\npci_disable_pcie_error_reporting(pdev);\r\npci_release_regions(pdev);\r\npci_save_state(pdev);\r\npci_disable_device(pdev);\r\npci_set_power_state(pdev, pci_choose_state(pdev, state));\r\nreturn 0;\r\n}\r\nstatic int skd_pci_resume(struct pci_dev *pdev)\r\n{\r\nint i;\r\nint rc = 0;\r\nstruct skd_device *skdev;\r\nskdev = pci_get_drvdata(pdev);\r\nif (!skdev) {\r\npr_err("%s: no device data for PCI\n", pci_name(pdev));\r\nreturn -1;\r\n}\r\npci_set_power_state(pdev, PCI_D0);\r\npci_enable_wake(pdev, PCI_D0, 0);\r\npci_restore_state(pdev);\r\nrc = pci_enable_device(pdev);\r\nif (rc)\r\nreturn rc;\r\nrc = pci_request_regions(pdev, DRV_NAME);\r\nif (rc)\r\ngoto err_out;\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(64));\r\nif (!rc) {\r\nif (pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64))) {\r\npr_err("(%s): consistent DMA mask error %d\n",\r\npci_name(pdev), rc);\r\n}\r\n} else {\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc) {\r\npr_err("(%s): DMA mask error %d\n",\r\npci_name(pdev), rc);\r\ngoto err_out_regions;\r\n}\r\n}\r\npci_set_master(pdev);\r\nrc = pci_enable_pcie_error_reporting(pdev);\r\nif (rc) {\r\npr_err("(%s): bad enable of PCIe error reporting rc=%d\n",\r\nskdev->name, rc);\r\nskdev->pcie_error_reporting_is_enabled = 0;\r\n} else\r\nskdev->pcie_error_reporting_is_enabled = 1;\r\nfor (i = 0; i < SKD_MAX_BARS; i++) {\r\nskdev->mem_phys[i] = pci_resource_start(pdev, i);\r\nskdev->mem_size[i] = (u32)pci_resource_len(pdev, i);\r\nskdev->mem_map[i] = ioremap(skdev->mem_phys[i],\r\nskdev->mem_size[i]);\r\nif (!skdev->mem_map[i]) {\r\npr_err("(%s): Unable to map adapter memory!\n",\r\nskd_name(skdev));\r\nrc = -ENODEV;\r\ngoto err_out_iounmap;\r\n}\r\npr_debug("%s:%s:%d mem_map=%p, phyd=%016llx, size=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->mem_map[i],\r\n(uint64_t)skdev->mem_phys[i], skdev->mem_size[i]);\r\n}\r\nrc = skd_acquire_irq(skdev);\r\nif (rc) {\r\npr_err("(%s): interrupt resource error %d\n",\r\npci_name(pdev), rc);\r\ngoto err_out_iounmap;\r\n}\r\nrc = skd_start_timer(skdev);\r\nif (rc)\r\ngoto err_out_timer;\r\ninit_waitqueue_head(&skdev->waitq);\r\nskd_start_device(skdev);\r\nreturn rc;\r\nerr_out_timer:\r\nskd_stop_device(skdev);\r\nskd_release_irq(skdev);\r\nerr_out_iounmap:\r\nfor (i = 0; i < SKD_MAX_BARS; i++)\r\nif (skdev->mem_map[i])\r\niounmap(skdev->mem_map[i]);\r\nif (skdev->pcie_error_reporting_is_enabled)\r\npci_disable_pcie_error_reporting(pdev);\r\nerr_out_regions:\r\npci_release_regions(pdev);\r\nerr_out:\r\npci_disable_device(pdev);\r\nreturn rc;\r\n}\r\nstatic void skd_pci_shutdown(struct pci_dev *pdev)\r\n{\r\nstruct skd_device *skdev;\r\npr_err("skd_pci_shutdown called\n");\r\nskdev = pci_get_drvdata(pdev);\r\nif (!skdev) {\r\npr_err("%s: no device data for PCI\n", pci_name(pdev));\r\nreturn;\r\n}\r\npr_err("%s: calling stop\n", skd_name(skdev));\r\nskd_stop_device(skdev);\r\n}\r\nstatic const char *skd_name(struct skd_device *skdev)\r\n{\r\nmemset(skdev->id_str, 0, sizeof(skdev->id_str));\r\nif (skdev->inquiry_is_valid)\r\nsnprintf(skdev->id_str, sizeof(skdev->id_str), "%s:%s:[%s]",\r\nskdev->name, skdev->inq_serial_num,\r\npci_name(skdev->pdev));\r\nelse\r\nsnprintf(skdev->id_str, sizeof(skdev->id_str), "%s:??:[%s]",\r\nskdev->name, pci_name(skdev->pdev));\r\nreturn skdev->id_str;\r\n}\r\nconst char *skd_drive_state_to_str(int state)\r\n{\r\nswitch (state) {\r\ncase FIT_SR_DRIVE_OFFLINE:\r\nreturn "OFFLINE";\r\ncase FIT_SR_DRIVE_INIT:\r\nreturn "INIT";\r\ncase FIT_SR_DRIVE_ONLINE:\r\nreturn "ONLINE";\r\ncase FIT_SR_DRIVE_BUSY:\r\nreturn "BUSY";\r\ncase FIT_SR_DRIVE_FAULT:\r\nreturn "FAULT";\r\ncase FIT_SR_DRIVE_DEGRADED:\r\nreturn "DEGRADED";\r\ncase FIT_SR_PCIE_LINK_DOWN:\r\nreturn "INK_DOWN";\r\ncase FIT_SR_DRIVE_SOFT_RESET:\r\nreturn "SOFT_RESET";\r\ncase FIT_SR_DRIVE_NEED_FW_DOWNLOAD:\r\nreturn "NEED_FW";\r\ncase FIT_SR_DRIVE_INIT_FAULT:\r\nreturn "INIT_FAULT";\r\ncase FIT_SR_DRIVE_BUSY_SANITIZE:\r\nreturn "BUSY_SANITIZE";\r\ncase FIT_SR_DRIVE_BUSY_ERASE:\r\nreturn "BUSY_ERASE";\r\ncase FIT_SR_DRIVE_FW_BOOTING:\r\nreturn "FW_BOOTING";\r\ndefault:\r\nreturn "???";\r\n}\r\n}\r\nconst char *skd_skdev_state_to_str(enum skd_drvr_state state)\r\n{\r\nswitch (state) {\r\ncase SKD_DRVR_STATE_LOAD:\r\nreturn "LOAD";\r\ncase SKD_DRVR_STATE_IDLE:\r\nreturn "IDLE";\r\ncase SKD_DRVR_STATE_BUSY:\r\nreturn "BUSY";\r\ncase SKD_DRVR_STATE_STARTING:\r\nreturn "STARTING";\r\ncase SKD_DRVR_STATE_ONLINE:\r\nreturn "ONLINE";\r\ncase SKD_DRVR_STATE_PAUSING:\r\nreturn "PAUSING";\r\ncase SKD_DRVR_STATE_PAUSED:\r\nreturn "PAUSED";\r\ncase SKD_DRVR_STATE_DRAINING_TIMEOUT:\r\nreturn "DRAINING_TIMEOUT";\r\ncase SKD_DRVR_STATE_RESTARTING:\r\nreturn "RESTARTING";\r\ncase SKD_DRVR_STATE_RESUMING:\r\nreturn "RESUMING";\r\ncase SKD_DRVR_STATE_STOPPING:\r\nreturn "STOPPING";\r\ncase SKD_DRVR_STATE_SYNCING:\r\nreturn "SYNCING";\r\ncase SKD_DRVR_STATE_FAULT:\r\nreturn "FAULT";\r\ncase SKD_DRVR_STATE_DISAPPEARED:\r\nreturn "DISAPPEARED";\r\ncase SKD_DRVR_STATE_BUSY_ERASE:\r\nreturn "BUSY_ERASE";\r\ncase SKD_DRVR_STATE_BUSY_SANITIZE:\r\nreturn "BUSY_SANITIZE";\r\ncase SKD_DRVR_STATE_BUSY_IMMINENT:\r\nreturn "BUSY_IMMINENT";\r\ncase SKD_DRVR_STATE_WAIT_BOOT:\r\nreturn "WAIT_BOOT";\r\ndefault:\r\nreturn "???";\r\n}\r\n}\r\nstatic const char *skd_skmsg_state_to_str(enum skd_fit_msg_state state)\r\n{\r\nswitch (state) {\r\ncase SKD_MSG_STATE_IDLE:\r\nreturn "IDLE";\r\ncase SKD_MSG_STATE_BUSY:\r\nreturn "BUSY";\r\ndefault:\r\nreturn "???";\r\n}\r\n}\r\nstatic const char *skd_skreq_state_to_str(enum skd_req_state state)\r\n{\r\nswitch (state) {\r\ncase SKD_REQ_STATE_IDLE:\r\nreturn "IDLE";\r\ncase SKD_REQ_STATE_SETUP:\r\nreturn "SETUP";\r\ncase SKD_REQ_STATE_BUSY:\r\nreturn "BUSY";\r\ncase SKD_REQ_STATE_COMPLETED:\r\nreturn "COMPLETED";\r\ncase SKD_REQ_STATE_TIMEOUT:\r\nreturn "TIMEOUT";\r\ncase SKD_REQ_STATE_ABORTED:\r\nreturn "ABORTED";\r\ndefault:\r\nreturn "???";\r\n}\r\n}\r\nstatic void skd_log_skdev(struct skd_device *skdev, const char *event)\r\n{\r\npr_debug("%s:%s:%d (%s) skdev=%p event='%s'\n",\r\nskdev->name, __func__, __LINE__, skdev->name, skdev, event);\r\npr_debug("%s:%s:%d drive_state=%s(%d) driver_state=%s(%d)\n",\r\nskdev->name, __func__, __LINE__,\r\nskd_drive_state_to_str(skdev->drive_state), skdev->drive_state,\r\nskd_skdev_state_to_str(skdev->state), skdev->state);\r\npr_debug("%s:%s:%d busy=%d limit=%d dev=%d lowat=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->in_flight, skdev->cur_max_queue_depth,\r\nskdev->dev_max_queue_depth, skdev->queue_low_water_mark);\r\npr_debug("%s:%s:%d timestamp=0x%x cycle=%d cycle_ix=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskdev->timeout_stamp, skdev->skcomp_cycle, skdev->skcomp_ix);\r\n}\r\nstatic void skd_log_skmsg(struct skd_device *skdev,\r\nstruct skd_fitmsg_context *skmsg, const char *event)\r\n{\r\npr_debug("%s:%s:%d (%s) skmsg=%p event='%s'\n",\r\nskdev->name, __func__, __LINE__, skdev->name, skmsg, event);\r\npr_debug("%s:%s:%d state=%s(%d) id=0x%04x length=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskd_skmsg_state_to_str(skmsg->state), skmsg->state,\r\nskmsg->id, skmsg->length);\r\n}\r\nstatic void skd_log_skreq(struct skd_device *skdev,\r\nstruct skd_request_context *skreq, const char *event)\r\n{\r\npr_debug("%s:%s:%d (%s) skreq=%p event='%s'\n",\r\nskdev->name, __func__, __LINE__, skdev->name, skreq, event);\r\npr_debug("%s:%s:%d state=%s(%d) id=0x%04x fitmsg=0x%04x\n",\r\nskdev->name, __func__, __LINE__,\r\nskd_skreq_state_to_str(skreq->state), skreq->state,\r\nskreq->id, skreq->fitmsg_id);\r\npr_debug("%s:%s:%d timo=0x%x sg_dir=%d n_sg=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nskreq->timeout_stamp, skreq->sg_data_dir, skreq->n_sg);\r\nif (skreq->req != NULL) {\r\nstruct request *req = skreq->req;\r\nu32 lba = (u32)blk_rq_pos(req);\r\nu32 count = blk_rq_sectors(req);\r\npr_debug("%s:%s:%d "\r\n"req=%p lba=%u(0x%x) count=%u(0x%x) dir=%d\n",\r\nskdev->name, __func__, __LINE__,\r\nreq, lba, lba, count, count,\r\n(int)rq_data_dir(req));\r\n} else\r\npr_debug("%s:%s:%d req=NULL\n",\r\nskdev->name, __func__, __LINE__);\r\n}\r\nstatic int __init skd_init(void)\r\n{\r\npr_info(PFX " v%s-b%s loaded\n", DRV_VERSION, DRV_BUILD_ID);\r\nswitch (skd_isr_type) {\r\ncase SKD_IRQ_LEGACY:\r\ncase SKD_IRQ_MSI:\r\ncase SKD_IRQ_MSIX:\r\nbreak;\r\ndefault:\r\npr_err(PFX "skd_isr_type %d invalid, re-set to %d\n",\r\nskd_isr_type, SKD_IRQ_DEFAULT);\r\nskd_isr_type = SKD_IRQ_DEFAULT;\r\n}\r\nif (skd_max_queue_depth < 1 ||\r\nskd_max_queue_depth > SKD_MAX_QUEUE_DEPTH) {\r\npr_err(PFX "skd_max_queue_depth %d invalid, re-set to %d\n",\r\nskd_max_queue_depth, SKD_MAX_QUEUE_DEPTH_DEFAULT);\r\nskd_max_queue_depth = SKD_MAX_QUEUE_DEPTH_DEFAULT;\r\n}\r\nif (skd_max_req_per_msg < 1 || skd_max_req_per_msg > 14) {\r\npr_err(PFX "skd_max_req_per_msg %d invalid, re-set to %d\n",\r\nskd_max_req_per_msg, SKD_MAX_REQ_PER_MSG_DEFAULT);\r\nskd_max_req_per_msg = SKD_MAX_REQ_PER_MSG_DEFAULT;\r\n}\r\nif (skd_sgs_per_request < 1 || skd_sgs_per_request > 4096) {\r\npr_err(PFX "skd_sg_per_request %d invalid, re-set to %d\n",\r\nskd_sgs_per_request, SKD_N_SG_PER_REQ_DEFAULT);\r\nskd_sgs_per_request = SKD_N_SG_PER_REQ_DEFAULT;\r\n}\r\nif (skd_dbg_level < 0 || skd_dbg_level > 2) {\r\npr_err(PFX "skd_dbg_level %d invalid, re-set to %d\n",\r\nskd_dbg_level, 0);\r\nskd_dbg_level = 0;\r\n}\r\nif (skd_isr_comp_limit < 0) {\r\npr_err(PFX "skd_isr_comp_limit %d invalid, set to %d\n",\r\nskd_isr_comp_limit, 0);\r\nskd_isr_comp_limit = 0;\r\n}\r\nif (skd_max_pass_thru < 1 || skd_max_pass_thru > 50) {\r\npr_err(PFX "skd_max_pass_thru %d invalid, re-set to %d\n",\r\nskd_max_pass_thru, SKD_N_SPECIAL_CONTEXT);\r\nskd_max_pass_thru = SKD_N_SPECIAL_CONTEXT;\r\n}\r\nreturn pci_register_driver(&skd_driver);\r\n}\r\nstatic void __exit skd_exit(void)\r\n{\r\npr_info(PFX " v%s-b%s unloading\n", DRV_VERSION, DRV_BUILD_ID);\r\npci_unregister_driver(&skd_driver);\r\nif (skd_major)\r\nunregister_blkdev(skd_major, DRV_NAME);\r\n}
