static void rdma_build_arg_xdr(struct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *ctxt,\r\nu32 byte_count)\r\n{\r\nstruct rpcrdma_msg *rmsgp;\r\nstruct page *page;\r\nu32 bc;\r\nint sge_no;\r\npage = ctxt->pages[0];\r\nput_page(rqstp->rq_pages[0]);\r\nrqstp->rq_pages[0] = page;\r\nrqstp->rq_arg.head[0].iov_base = page_address(page);\r\nrqstp->rq_arg.head[0].iov_len =\r\nmin_t(size_t, byte_count, ctxt->sge[0].length);\r\nrqstp->rq_arg.len = byte_count;\r\nrqstp->rq_arg.buflen = byte_count;\r\nbc = byte_count - rqstp->rq_arg.head[0].iov_len;\r\nrqstp->rq_arg.page_len = bc;\r\nrqstp->rq_arg.page_base = 0;\r\nrmsgp = (struct rpcrdma_msg *)rqstp->rq_arg.head[0].iov_base;\r\nif (be32_to_cpu(rmsgp->rm_type) == RDMA_NOMSG)\r\nrqstp->rq_arg.pages = &rqstp->rq_pages[0];\r\nelse\r\nrqstp->rq_arg.pages = &rqstp->rq_pages[1];\r\nsge_no = 1;\r\nwhile (bc && sge_no < ctxt->count) {\r\npage = ctxt->pages[sge_no];\r\nput_page(rqstp->rq_pages[sge_no]);\r\nrqstp->rq_pages[sge_no] = page;\r\nbc -= min_t(u32, bc, ctxt->sge[sge_no].length);\r\nrqstp->rq_arg.buflen += ctxt->sge[sge_no].length;\r\nsge_no++;\r\n}\r\nrqstp->rq_respages = &rqstp->rq_pages[sge_no];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\nbc = sge_no;\r\nwhile (sge_no < ctxt->count) {\r\npage = ctxt->pages[sge_no++];\r\nput_page(page);\r\n}\r\nctxt->count = bc;\r\nrqstp->rq_arg.tail[0].iov_base = NULL;\r\nrqstp->rq_arg.tail[0].iov_len = 0;\r\n}\r\nstatic int rdma_read_max_sge(struct svcxprt_rdma *xprt, int sge_count)\r\n{\r\nif (rdma_node_get_transport(xprt->sc_cm_id->device->node_type) ==\r\nRDMA_TRANSPORT_IWARP)\r\nreturn 1;\r\nelse\r\nreturn min_t(int, sge_count, xprt->sc_max_sge);\r\n}\r\nint rdma_read_chunk_lcl(struct svcxprt_rdma *xprt,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head,\r\nint *page_no,\r\nu32 *page_offset,\r\nu32 rs_handle,\r\nu32 rs_length,\r\nu64 rs_offset,\r\nbool last)\r\n{\r\nstruct ib_send_wr read_wr;\r\nint pages_needed = PAGE_ALIGN(*page_offset + rs_length) >> PAGE_SHIFT;\r\nstruct svc_rdma_op_ctxt *ctxt = svc_rdma_get_context(xprt);\r\nint ret, read, pno;\r\nu32 pg_off = *page_offset;\r\nu32 pg_no = *page_no;\r\nctxt->direction = DMA_FROM_DEVICE;\r\nctxt->read_hdr = head;\r\npages_needed =\r\nmin_t(int, pages_needed, rdma_read_max_sge(xprt, pages_needed));\r\nread = min_t(int, pages_needed << PAGE_SHIFT, rs_length);\r\nfor (pno = 0; pno < pages_needed; pno++) {\r\nint len = min_t(int, rs_length, PAGE_SIZE - pg_off);\r\nhead->arg.pages[pg_no] = rqstp->rq_arg.pages[pg_no];\r\nhead->arg.page_len += len;\r\nhead->arg.len += len;\r\nif (!pg_off)\r\nhead->count++;\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[pg_no+1];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\nctxt->sge[pno].addr =\r\nib_dma_map_page(xprt->sc_cm_id->device,\r\nhead->arg.pages[pg_no], pg_off,\r\nPAGE_SIZE - pg_off,\r\nDMA_FROM_DEVICE);\r\nret = ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nctxt->sge[pno].addr);\r\nif (ret)\r\ngoto err;\r\natomic_inc(&xprt->sc_dma_used);\r\nctxt->sge[pno].lkey = xprt->sc_dma_lkey;\r\nctxt->sge[pno].length = len;\r\nctxt->count++;\r\npg_off += len;\r\nif (pg_off == PAGE_SIZE) {\r\npg_off = 0;\r\npg_no++;\r\n}\r\nrs_length -= len;\r\n}\r\nif (last && rs_length == 0)\r\nset_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\nelse\r\nclear_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\nmemset(&read_wr, 0, sizeof(read_wr));\r\nread_wr.wr_id = (unsigned long)ctxt;\r\nread_wr.opcode = IB_WR_RDMA_READ;\r\nctxt->wr_op = read_wr.opcode;\r\nread_wr.send_flags = IB_SEND_SIGNALED;\r\nread_wr.wr.rdma.rkey = rs_handle;\r\nread_wr.wr.rdma.remote_addr = rs_offset;\r\nread_wr.sg_list = ctxt->sge;\r\nread_wr.num_sge = pages_needed;\r\nret = svc_rdma_send(xprt, &read_wr);\r\nif (ret) {\r\npr_err("svcrdma: Error %d posting RDMA_READ\n", ret);\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\ngoto err;\r\n}\r\n*page_no = pg_no;\r\n*page_offset = pg_off;\r\nret = read;\r\natomic_inc(&rdma_stat_read);\r\nreturn ret;\r\nerr:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn ret;\r\n}\r\nint rdma_read_chunk_frmr(struct svcxprt_rdma *xprt,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head,\r\nint *page_no,\r\nu32 *page_offset,\r\nu32 rs_handle,\r\nu32 rs_length,\r\nu64 rs_offset,\r\nbool last)\r\n{\r\nstruct ib_send_wr read_wr;\r\nstruct ib_send_wr inv_wr;\r\nstruct ib_send_wr fastreg_wr;\r\nu8 key;\r\nint pages_needed = PAGE_ALIGN(*page_offset + rs_length) >> PAGE_SHIFT;\r\nstruct svc_rdma_op_ctxt *ctxt = svc_rdma_get_context(xprt);\r\nstruct svc_rdma_fastreg_mr *frmr = svc_rdma_get_frmr(xprt);\r\nint ret, read, pno;\r\nu32 pg_off = *page_offset;\r\nu32 pg_no = *page_no;\r\nif (IS_ERR(frmr))\r\nreturn -ENOMEM;\r\nctxt->direction = DMA_FROM_DEVICE;\r\nctxt->frmr = frmr;\r\npages_needed = min_t(int, pages_needed, xprt->sc_frmr_pg_list_len);\r\nread = min_t(int, pages_needed << PAGE_SHIFT, rs_length);\r\nfrmr->kva = page_address(rqstp->rq_arg.pages[pg_no]);\r\nfrmr->direction = DMA_FROM_DEVICE;\r\nfrmr->access_flags = (IB_ACCESS_LOCAL_WRITE|IB_ACCESS_REMOTE_WRITE);\r\nfrmr->map_len = pages_needed << PAGE_SHIFT;\r\nfrmr->page_list_len = pages_needed;\r\nfor (pno = 0; pno < pages_needed; pno++) {\r\nint len = min_t(int, rs_length, PAGE_SIZE - pg_off);\r\nhead->arg.pages[pg_no] = rqstp->rq_arg.pages[pg_no];\r\nhead->arg.page_len += len;\r\nhead->arg.len += len;\r\nif (!pg_off)\r\nhead->count++;\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[pg_no+1];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\nfrmr->page_list->page_list[pno] =\r\nib_dma_map_page(xprt->sc_cm_id->device,\r\nhead->arg.pages[pg_no], 0,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\nret = ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nfrmr->page_list->page_list[pno]);\r\nif (ret)\r\ngoto err;\r\natomic_inc(&xprt->sc_dma_used);\r\npg_off += len;\r\nif (pg_off == PAGE_SIZE) {\r\npg_off = 0;\r\npg_no++;\r\n}\r\nrs_length -= len;\r\n}\r\nif (last && rs_length == 0)\r\nset_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\nelse\r\nclear_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\nkey = (u8)(frmr->mr->lkey & 0x000000FF);\r\nib_update_fast_reg_key(frmr->mr, ++key);\r\nctxt->sge[0].addr = (unsigned long)frmr->kva + *page_offset;\r\nctxt->sge[0].lkey = frmr->mr->lkey;\r\nctxt->sge[0].length = read;\r\nctxt->count = 1;\r\nctxt->read_hdr = head;\r\nmemset(&fastreg_wr, 0, sizeof(fastreg_wr));\r\nfastreg_wr.opcode = IB_WR_FAST_REG_MR;\r\nfastreg_wr.send_flags = IB_SEND_SIGNALED;\r\nfastreg_wr.wr.fast_reg.iova_start = (unsigned long)frmr->kva;\r\nfastreg_wr.wr.fast_reg.page_list = frmr->page_list;\r\nfastreg_wr.wr.fast_reg.page_list_len = frmr->page_list_len;\r\nfastreg_wr.wr.fast_reg.page_shift = PAGE_SHIFT;\r\nfastreg_wr.wr.fast_reg.length = frmr->map_len;\r\nfastreg_wr.wr.fast_reg.access_flags = frmr->access_flags;\r\nfastreg_wr.wr.fast_reg.rkey = frmr->mr->lkey;\r\nfastreg_wr.next = &read_wr;\r\nmemset(&read_wr, 0, sizeof(read_wr));\r\nread_wr.send_flags = IB_SEND_SIGNALED;\r\nread_wr.wr.rdma.rkey = rs_handle;\r\nread_wr.wr.rdma.remote_addr = rs_offset;\r\nread_wr.sg_list = ctxt->sge;\r\nread_wr.num_sge = 1;\r\nif (xprt->sc_dev_caps & SVCRDMA_DEVCAP_READ_W_INV) {\r\nread_wr.opcode = IB_WR_RDMA_READ_WITH_INV;\r\nread_wr.wr_id = (unsigned long)ctxt;\r\nread_wr.ex.invalidate_rkey = ctxt->frmr->mr->lkey;\r\n} else {\r\nread_wr.opcode = IB_WR_RDMA_READ;\r\nread_wr.next = &inv_wr;\r\nmemset(&inv_wr, 0, sizeof(inv_wr));\r\ninv_wr.wr_id = (unsigned long)ctxt;\r\ninv_wr.opcode = IB_WR_LOCAL_INV;\r\ninv_wr.send_flags = IB_SEND_SIGNALED | IB_SEND_FENCE;\r\ninv_wr.ex.invalidate_rkey = frmr->mr->lkey;\r\n}\r\nctxt->wr_op = read_wr.opcode;\r\nret = svc_rdma_send(xprt, &fastreg_wr);\r\nif (ret) {\r\npr_err("svcrdma: Error %d posting RDMA_READ\n", ret);\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\ngoto err;\r\n}\r\n*page_no = pg_no;\r\n*page_offset = pg_off;\r\nret = read;\r\natomic_inc(&rdma_stat_read);\r\nreturn ret;\r\nerr:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 0);\r\nsvc_rdma_put_frmr(xprt, frmr);\r\nreturn ret;\r\n}\r\nstatic unsigned int\r\nrdma_rcl_chunk_count(struct rpcrdma_read_chunk *ch)\r\n{\r\nunsigned int count;\r\nfor (count = 0; ch->rc_discrim != xdr_zero; ch++)\r\ncount++;\r\nreturn count;\r\n}\r\nstatic int\r\nrdma_copy_tail(struct svc_rqst *rqstp, struct svc_rdma_op_ctxt *head,\r\nu32 position, u32 byte_count, u32 page_offset, int page_no)\r\n{\r\nchar *srcp, *destp;\r\nint ret;\r\nret = 0;\r\nsrcp = head->arg.head[0].iov_base + position;\r\nbyte_count = head->arg.head[0].iov_len - position;\r\nif (byte_count > PAGE_SIZE) {\r\ndprintk("svcrdma: large tail unsupported\n");\r\nreturn 0;\r\n}\r\nif (page_offset != PAGE_SIZE) {\r\ndestp = page_address(rqstp->rq_arg.pages[page_no]);\r\ndestp += page_offset;\r\nwhile (byte_count--) {\r\n*destp++ = *srcp++;\r\npage_offset++;\r\nif (page_offset == PAGE_SIZE && byte_count)\r\ngoto more;\r\n}\r\ngoto done;\r\n}\r\nmore:\r\npage_no++;\r\ndestp = page_address(rqstp->rq_arg.pages[page_no]);\r\nwhile (byte_count--)\r\n*destp++ = *srcp++;\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[page_no+1];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\ndone:\r\nbyte_count = head->arg.head[0].iov_len - position;\r\nhead->arg.page_len += byte_count;\r\nhead->arg.len += byte_count;\r\nhead->arg.buflen += byte_count;\r\nreturn 1;\r\n}\r\nstatic int rdma_read_chunks(struct svcxprt_rdma *xprt,\r\nstruct rpcrdma_msg *rmsgp,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head)\r\n{\r\nint page_no, ret;\r\nstruct rpcrdma_read_chunk *ch;\r\nu32 handle, page_offset, byte_count;\r\nu32 position;\r\nu64 rs_offset;\r\nbool last;\r\nch = svc_rdma_get_read_chunk(rmsgp);\r\nif (!ch)\r\nreturn 0;\r\nif (rdma_rcl_chunk_count(ch) > RPCSVC_MAXPAGES)\r\nreturn -EINVAL;\r\nhead->arg.head[0] = rqstp->rq_arg.head[0];\r\nhead->arg.tail[0] = rqstp->rq_arg.tail[0];\r\nhead->hdr_count = head->count;\r\nhead->arg.page_base = 0;\r\nhead->arg.page_len = 0;\r\nhead->arg.len = rqstp->rq_arg.len;\r\nhead->arg.buflen = rqstp->rq_arg.buflen;\r\nch = (struct rpcrdma_read_chunk *)&rmsgp->rm_body.rm_chunks[0];\r\nposition = be32_to_cpu(ch->rc_position);\r\nif (position == 0) {\r\nhead->arg.pages = &head->pages[0];\r\npage_offset = head->byte_len;\r\n} else {\r\nhead->arg.pages = &head->pages[head->count];\r\npage_offset = 0;\r\n}\r\nret = 0;\r\npage_no = 0;\r\nfor (; ch->rc_discrim != xdr_zero; ch++) {\r\nif (be32_to_cpu(ch->rc_position) != position)\r\ngoto err;\r\nhandle = be32_to_cpu(ch->rc_target.rs_handle),\r\nbyte_count = be32_to_cpu(ch->rc_target.rs_length);\r\nxdr_decode_hyper((__be32 *)&ch->rc_target.rs_offset,\r\n&rs_offset);\r\nwhile (byte_count > 0) {\r\nlast = (ch + 1)->rc_discrim == xdr_zero;\r\nret = xprt->sc_reader(xprt, rqstp, head,\r\n&page_no, &page_offset,\r\nhandle, byte_count,\r\nrs_offset, last);\r\nif (ret < 0)\r\ngoto err;\r\nbyte_count -= ret;\r\nrs_offset += ret;\r\nhead->arg.buflen += ret;\r\n}\r\n}\r\nif (page_offset & 3) {\r\nu32 pad = 4 - (page_offset & 3);\r\nhead->arg.page_len += pad;\r\nhead->arg.len += pad;\r\nhead->arg.buflen += pad;\r\npage_offset += pad;\r\n}\r\nret = 1;\r\nif (position && position < head->arg.head[0].iov_len)\r\nret = rdma_copy_tail(rqstp, head, position,\r\nbyte_count, page_offset, page_no);\r\nhead->arg.head[0].iov_len = position;\r\nhead->position = position;\r\nerr:\r\nfor (page_no = 0;\r\n&rqstp->rq_pages[page_no] < rqstp->rq_respages; page_no++)\r\nrqstp->rq_pages[page_no] = NULL;\r\nreturn ret;\r\n}\r\nstatic int rdma_read_complete(struct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head)\r\n{\r\nint page_no;\r\nint ret;\r\nfor (page_no = 0; page_no < head->count; page_no++) {\r\nput_page(rqstp->rq_pages[page_no]);\r\nrqstp->rq_pages[page_no] = head->pages[page_no];\r\n}\r\nif (head->position == 0) {\r\nif (head->arg.len <= head->sge[0].length) {\r\nhead->arg.head[0].iov_len = head->arg.len -\r\nhead->byte_len;\r\nhead->arg.page_len = 0;\r\n} else {\r\nhead->arg.head[0].iov_len = head->sge[0].length -\r\nhead->byte_len;\r\nhead->arg.page_len = head->arg.len -\r\nhead->sge[0].length;\r\n}\r\n}\r\nrqstp->rq_arg.pages = &rqstp->rq_pages[head->hdr_count];\r\nrqstp->rq_arg.page_len = head->arg.page_len;\r\nrqstp->rq_arg.page_base = head->arg.page_base;\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[page_no];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\nrqstp->rq_arg.head[0] = head->arg.head[0];\r\nrqstp->rq_arg.tail[0] = head->arg.tail[0];\r\nrqstp->rq_arg.len = head->arg.len;\r\nrqstp->rq_arg.buflen = head->arg.buflen;\r\nsvc_rdma_put_context(head, 0);\r\nrqstp->rq_prot = IPPROTO_MAX;\r\nsvc_xprt_copy_addrs(rqstp, rqstp->rq_xprt);\r\nret = rqstp->rq_arg.head[0].iov_len\r\n+ rqstp->rq_arg.page_len\r\n+ rqstp->rq_arg.tail[0].iov_len;\r\ndprintk("svcrdma: deferred read ret=%d, rq_arg.len=%u, "\r\n"rq_arg.head[0].iov_base=%p, rq_arg.head[0].iov_len=%zu\n",\r\nret, rqstp->rq_arg.len, rqstp->rq_arg.head[0].iov_base,\r\nrqstp->rq_arg.head[0].iov_len);\r\nreturn ret;\r\n}\r\nint svc_rdma_recvfrom(struct svc_rqst *rqstp)\r\n{\r\nstruct svc_xprt *xprt = rqstp->rq_xprt;\r\nstruct svcxprt_rdma *rdma_xprt =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nstruct svc_rdma_op_ctxt *ctxt = NULL;\r\nstruct rpcrdma_msg *rmsgp;\r\nint ret = 0;\r\nint len;\r\ndprintk("svcrdma: rqstp=%p\n", rqstp);\r\nspin_lock_bh(&rdma_xprt->sc_rq_dto_lock);\r\nif (!list_empty(&rdma_xprt->sc_read_complete_q)) {\r\nctxt = list_entry(rdma_xprt->sc_read_complete_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\nspin_unlock_bh(&rdma_xprt->sc_rq_dto_lock);\r\nreturn rdma_read_complete(rqstp, ctxt);\r\n} else if (!list_empty(&rdma_xprt->sc_rq_dto_q)) {\r\nctxt = list_entry(rdma_xprt->sc_rq_dto_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\n} else {\r\natomic_inc(&rdma_stat_rq_starve);\r\nclear_bit(XPT_DATA, &xprt->xpt_flags);\r\nctxt = NULL;\r\n}\r\nspin_unlock_bh(&rdma_xprt->sc_rq_dto_lock);\r\nif (!ctxt) {\r\nif (test_bit(XPT_CLOSE, &xprt->xpt_flags))\r\ngoto close_out;\r\ngoto out;\r\n}\r\ndprintk("svcrdma: processing ctxt=%p on xprt=%p, rqstp=%p, status=%d\n",\r\nctxt, rdma_xprt, rqstp, ctxt->wc_status);\r\natomic_inc(&rdma_stat_recv);\r\nrdma_build_arg_xdr(rqstp, ctxt, ctxt->byte_len);\r\nlen = svc_rdma_xdr_decode_req(&rmsgp, rqstp);\r\nrqstp->rq_xprt_hlen = len;\r\nif (len < 0) {\r\nif (len == -ENOSYS)\r\nsvc_rdma_send_error(rdma_xprt, rmsgp, ERR_VERS);\r\ngoto close_out;\r\n}\r\nret = rdma_read_chunks(rdma_xprt, rmsgp, rqstp, ctxt);\r\nif (ret > 0) {\r\ngoto defer;\r\n} else if (ret < 0) {\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn 0;\r\n}\r\nret = rqstp->rq_arg.head[0].iov_len\r\n+ rqstp->rq_arg.page_len\r\n+ rqstp->rq_arg.tail[0].iov_len;\r\nsvc_rdma_put_context(ctxt, 0);\r\nout:\r\ndprintk("svcrdma: ret=%d, rq_arg.len=%u, "\r\n"rq_arg.head[0].iov_base=%p, rq_arg.head[0].iov_len=%zd\n",\r\nret, rqstp->rq_arg.len,\r\nrqstp->rq_arg.head[0].iov_base,\r\nrqstp->rq_arg.head[0].iov_len);\r\nrqstp->rq_prot = IPPROTO_MAX;\r\nsvc_xprt_copy_addrs(rqstp, xprt);\r\nreturn ret;\r\nclose_out:\r\nif (ctxt)\r\nsvc_rdma_put_context(ctxt, 1);\r\ndprintk("svcrdma: transport %p is closing\n", xprt);\r\nset_bit(XPT_CLOSE, &xprt->xpt_flags);\r\ndefer:\r\nreturn 0;\r\n}
