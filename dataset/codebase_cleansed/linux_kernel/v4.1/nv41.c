static void\r\nnv41_vm_map_sg(struct nvkm_vma *vma, struct nvkm_gpuobj *pgt,\r\nstruct nvkm_mem *mem, u32 pte, u32 cnt, dma_addr_t *list)\r\n{\r\npte = pte * 4;\r\nwhile (cnt) {\r\nu32 page = PAGE_SIZE / NV41_GART_PAGE;\r\nu64 phys = (u64)*list++;\r\nwhile (cnt && page--) {\r\nnv_wo32(pgt, pte, (phys >> 7) | 1);\r\nphys += NV41_GART_PAGE;\r\npte += 4;\r\ncnt -= 1;\r\n}\r\n}\r\n}\r\nstatic void\r\nnv41_vm_unmap(struct nvkm_gpuobj *pgt, u32 pte, u32 cnt)\r\n{\r\npte = pte * 4;\r\nwhile (cnt--) {\r\nnv_wo32(pgt, pte, 0x00000000);\r\npte += 4;\r\n}\r\n}\r\nstatic void\r\nnv41_vm_flush(struct nvkm_vm *vm)\r\n{\r\nstruct nv04_mmu_priv *priv = (void *)vm->mmu;\r\nmutex_lock(&nv_subdev(priv)->mutex);\r\nnv_wr32(priv, 0x100810, 0x00000022);\r\nif (!nv_wait(priv, 0x100810, 0x00000020, 0x00000020)) {\r\nnv_warn(priv, "flush timeout, 0x%08x\n",\r\nnv_rd32(priv, 0x100810));\r\n}\r\nnv_wr32(priv, 0x100810, 0x00000000);\r\nmutex_unlock(&nv_subdev(priv)->mutex);\r\n}\r\nstatic int\r\nnv41_mmu_ctor(struct nvkm_object *parent, struct nvkm_object *engine,\r\nstruct nvkm_oclass *oclass, void *data, u32 size,\r\nstruct nvkm_object **pobject)\r\n{\r\nstruct nvkm_device *device = nv_device(parent);\r\nstruct nv04_mmu_priv *priv;\r\nint ret;\r\nif (pci_find_capability(device->pdev, PCI_CAP_ID_AGP) ||\r\n!nvkm_boolopt(device->cfgopt, "NvPCIE", true)) {\r\nreturn nvkm_object_ctor(parent, engine, &nv04_mmu_oclass,\r\ndata, size, pobject);\r\n}\r\nret = nvkm_mmu_create(parent, engine, oclass, "PCIEGART",\r\n"pciegart", &priv);\r\n*pobject = nv_object(priv);\r\nif (ret)\r\nreturn ret;\r\npriv->base.create = nv04_vm_create;\r\npriv->base.limit = NV41_GART_SIZE;\r\npriv->base.dma_bits = 39;\r\npriv->base.pgt_bits = 32 - 12;\r\npriv->base.spg_shift = 12;\r\npriv->base.lpg_shift = 12;\r\npriv->base.map_sg = nv41_vm_map_sg;\r\npriv->base.unmap = nv41_vm_unmap;\r\npriv->base.flush = nv41_vm_flush;\r\nret = nvkm_vm_create(&priv->base, 0, NV41_GART_SIZE, 0, 4096,\r\n&priv->vm);\r\nif (ret)\r\nreturn ret;\r\nret = nvkm_gpuobj_new(nv_object(priv), NULL,\r\n(NV41_GART_SIZE / NV41_GART_PAGE) * 4, 16,\r\nNVOBJ_FLAG_ZERO_ALLOC,\r\n&priv->vm->pgt[0].obj[0]);\r\npriv->vm->pgt[0].refcount[0] = 1;\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nstatic int\r\nnv41_mmu_init(struct nvkm_object *object)\r\n{\r\nstruct nv04_mmu_priv *priv = (void *)object;\r\nstruct nvkm_gpuobj *dma = priv->vm->pgt[0].obj[0];\r\nint ret;\r\nret = nvkm_mmu_init(&priv->base);\r\nif (ret)\r\nreturn ret;\r\nnv_wr32(priv, 0x100800, dma->addr | 0x00000002);\r\nnv_mask(priv, 0x10008c, 0x00000100, 0x00000100);\r\nnv_wr32(priv, 0x100820, 0x00000000);\r\nreturn 0;\r\n}
