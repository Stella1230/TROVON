static bool global_reclaim(struct scan_control *sc)\r\n{\r\nreturn !sc->target_mem_cgroup;\r\n}\r\nstatic bool global_reclaim(struct scan_control *sc)\r\n{\r\nreturn true;\r\n}\r\nstatic unsigned long zone_reclaimable_pages(struct zone *zone)\r\n{\r\nint nr;\r\nnr = zone_page_state(zone, NR_ACTIVE_FILE) +\r\nzone_page_state(zone, NR_INACTIVE_FILE);\r\nif (get_nr_swap_pages() > 0)\r\nnr += zone_page_state(zone, NR_ACTIVE_ANON) +\r\nzone_page_state(zone, NR_INACTIVE_ANON);\r\nreturn nr;\r\n}\r\nbool zone_reclaimable(struct zone *zone)\r\n{\r\nreturn zone_page_state(zone, NR_PAGES_SCANNED) <\r\nzone_reclaimable_pages(zone) * 6;\r\n}\r\nstatic unsigned long get_lru_size(struct lruvec *lruvec, enum lru_list lru)\r\n{\r\nif (!mem_cgroup_disabled())\r\nreturn mem_cgroup_get_lru_size(lruvec, lru);\r\nreturn zone_page_state(lruvec_zone(lruvec), NR_LRU_BASE + lru);\r\n}\r\nint register_shrinker(struct shrinker *shrinker)\r\n{\r\nsize_t size = sizeof(*shrinker->nr_deferred);\r\nif (nr_node_ids == 1)\r\nshrinker->flags &= ~SHRINKER_NUMA_AWARE;\r\nif (shrinker->flags & SHRINKER_NUMA_AWARE)\r\nsize *= nr_node_ids;\r\nshrinker->nr_deferred = kzalloc(size, GFP_KERNEL);\r\nif (!shrinker->nr_deferred)\r\nreturn -ENOMEM;\r\ndown_write(&shrinker_rwsem);\r\nlist_add_tail(&shrinker->list, &shrinker_list);\r\nup_write(&shrinker_rwsem);\r\nreturn 0;\r\n}\r\nvoid unregister_shrinker(struct shrinker *shrinker)\r\n{\r\ndown_write(&shrinker_rwsem);\r\nlist_del(&shrinker->list);\r\nup_write(&shrinker_rwsem);\r\nkfree(shrinker->nr_deferred);\r\n}\r\nstatic unsigned long do_shrink_slab(struct shrink_control *shrinkctl,\r\nstruct shrinker *shrinker,\r\nunsigned long nr_scanned,\r\nunsigned long nr_eligible)\r\n{\r\nunsigned long freed = 0;\r\nunsigned long long delta;\r\nlong total_scan;\r\nlong freeable;\r\nlong nr;\r\nlong new_nr;\r\nint nid = shrinkctl->nid;\r\nlong batch_size = shrinker->batch ? shrinker->batch\r\n: SHRINK_BATCH;\r\nfreeable = shrinker->count_objects(shrinker, shrinkctl);\r\nif (freeable == 0)\r\nreturn 0;\r\nnr = atomic_long_xchg(&shrinker->nr_deferred[nid], 0);\r\ntotal_scan = nr;\r\ndelta = (4 * nr_scanned) / shrinker->seeks;\r\ndelta *= freeable;\r\ndo_div(delta, nr_eligible + 1);\r\ntotal_scan += delta;\r\nif (total_scan < 0) {\r\npr_err("shrink_slab: %pF negative objects to delete nr=%ld\n",\r\nshrinker->scan_objects, total_scan);\r\ntotal_scan = freeable;\r\n}\r\nif (delta < freeable / 4)\r\ntotal_scan = min(total_scan, freeable / 2);\r\nif (total_scan > freeable * 2)\r\ntotal_scan = freeable * 2;\r\ntrace_mm_shrink_slab_start(shrinker, shrinkctl, nr,\r\nnr_scanned, nr_eligible,\r\nfreeable, delta, total_scan);\r\nwhile (total_scan >= batch_size ||\r\ntotal_scan >= freeable) {\r\nunsigned long ret;\r\nunsigned long nr_to_scan = min(batch_size, total_scan);\r\nshrinkctl->nr_to_scan = nr_to_scan;\r\nret = shrinker->scan_objects(shrinker, shrinkctl);\r\nif (ret == SHRINK_STOP)\r\nbreak;\r\nfreed += ret;\r\ncount_vm_events(SLABS_SCANNED, nr_to_scan);\r\ntotal_scan -= nr_to_scan;\r\ncond_resched();\r\n}\r\nif (total_scan > 0)\r\nnew_nr = atomic_long_add_return(total_scan,\r\n&shrinker->nr_deferred[nid]);\r\nelse\r\nnew_nr = atomic_long_read(&shrinker->nr_deferred[nid]);\r\ntrace_mm_shrink_slab_end(shrinker, nid, freed, nr, new_nr, total_scan);\r\nreturn freed;\r\n}\r\nstatic unsigned long shrink_slab(gfp_t gfp_mask, int nid,\r\nstruct mem_cgroup *memcg,\r\nunsigned long nr_scanned,\r\nunsigned long nr_eligible)\r\n{\r\nstruct shrinker *shrinker;\r\nunsigned long freed = 0;\r\nif (memcg && !memcg_kmem_is_active(memcg))\r\nreturn 0;\r\nif (nr_scanned == 0)\r\nnr_scanned = SWAP_CLUSTER_MAX;\r\nif (!down_read_trylock(&shrinker_rwsem)) {\r\nfreed = 1;\r\ngoto out;\r\n}\r\nlist_for_each_entry(shrinker, &shrinker_list, list) {\r\nstruct shrink_control sc = {\r\n.gfp_mask = gfp_mask,\r\n.nid = nid,\r\n.memcg = memcg,\r\n};\r\nif (memcg && !(shrinker->flags & SHRINKER_MEMCG_AWARE))\r\ncontinue;\r\nif (!(shrinker->flags & SHRINKER_NUMA_AWARE))\r\nsc.nid = 0;\r\nfreed += do_shrink_slab(&sc, shrinker, nr_scanned, nr_eligible);\r\n}\r\nup_read(&shrinker_rwsem);\r\nout:\r\ncond_resched();\r\nreturn freed;\r\n}\r\nvoid drop_slab_node(int nid)\r\n{\r\nunsigned long freed;\r\ndo {\r\nstruct mem_cgroup *memcg = NULL;\r\nfreed = 0;\r\ndo {\r\nfreed += shrink_slab(GFP_KERNEL, nid, memcg,\r\n1000, 1000);\r\n} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);\r\n} while (freed > 10);\r\n}\r\nvoid drop_slab(void)\r\n{\r\nint nid;\r\nfor_each_online_node(nid)\r\ndrop_slab_node(nid);\r\n}\r\nstatic inline int is_page_cache_freeable(struct page *page)\r\n{\r\nreturn page_count(page) - page_has_private(page) == 2;\r\n}\r\nstatic int may_write_to_queue(struct backing_dev_info *bdi,\r\nstruct scan_control *sc)\r\n{\r\nif (current->flags & PF_SWAPWRITE)\r\nreturn 1;\r\nif (!bdi_write_congested(bdi))\r\nreturn 1;\r\nif (bdi == current->backing_dev_info)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void handle_write_error(struct address_space *mapping,\r\nstruct page *page, int error)\r\n{\r\nlock_page(page);\r\nif (page_mapping(page) == mapping)\r\nmapping_set_error(mapping, error);\r\nunlock_page(page);\r\n}\r\nstatic pageout_t pageout(struct page *page, struct address_space *mapping,\r\nstruct scan_control *sc)\r\n{\r\nif (!is_page_cache_freeable(page))\r\nreturn PAGE_KEEP;\r\nif (!mapping) {\r\nif (page_has_private(page)) {\r\nif (try_to_free_buffers(page)) {\r\nClearPageDirty(page);\r\npr_info("%s: orphaned page\n", __func__);\r\nreturn PAGE_CLEAN;\r\n}\r\n}\r\nreturn PAGE_KEEP;\r\n}\r\nif (mapping->a_ops->writepage == NULL)\r\nreturn PAGE_ACTIVATE;\r\nif (!may_write_to_queue(inode_to_bdi(mapping->host), sc))\r\nreturn PAGE_KEEP;\r\nif (clear_page_dirty_for_io(page)) {\r\nint res;\r\nstruct writeback_control wbc = {\r\n.sync_mode = WB_SYNC_NONE,\r\n.nr_to_write = SWAP_CLUSTER_MAX,\r\n.range_start = 0,\r\n.range_end = LLONG_MAX,\r\n.for_reclaim = 1,\r\n};\r\nSetPageReclaim(page);\r\nres = mapping->a_ops->writepage(page, &wbc);\r\nif (res < 0)\r\nhandle_write_error(mapping, page, res);\r\nif (res == AOP_WRITEPAGE_ACTIVATE) {\r\nClearPageReclaim(page);\r\nreturn PAGE_ACTIVATE;\r\n}\r\nif (!PageWriteback(page)) {\r\nClearPageReclaim(page);\r\n}\r\ntrace_mm_vmscan_writepage(page, trace_reclaim_flags(page));\r\ninc_zone_page_state(page, NR_VMSCAN_WRITE);\r\nreturn PAGE_SUCCESS;\r\n}\r\nreturn PAGE_CLEAN;\r\n}\r\nstatic int __remove_mapping(struct address_space *mapping, struct page *page,\r\nbool reclaimed)\r\n{\r\nBUG_ON(!PageLocked(page));\r\nBUG_ON(mapping != page_mapping(page));\r\nspin_lock_irq(&mapping->tree_lock);\r\nif (!page_freeze_refs(page, 2))\r\ngoto cannot_free;\r\nif (unlikely(PageDirty(page))) {\r\npage_unfreeze_refs(page, 2);\r\ngoto cannot_free;\r\n}\r\nif (PageSwapCache(page)) {\r\nswp_entry_t swap = { .val = page_private(page) };\r\nmem_cgroup_swapout(page, swap);\r\n__delete_from_swap_cache(page);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nswapcache_free(swap);\r\n} else {\r\nvoid (*freepage)(struct page *);\r\nvoid *shadow = NULL;\r\nfreepage = mapping->a_ops->freepage;\r\nif (reclaimed && page_is_file_cache(page) &&\r\n!mapping_exiting(mapping))\r\nshadow = workingset_eviction(mapping, page);\r\n__delete_from_page_cache(page, shadow);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nif (freepage != NULL)\r\nfreepage(page);\r\n}\r\nreturn 1;\r\ncannot_free:\r\nspin_unlock_irq(&mapping->tree_lock);\r\nreturn 0;\r\n}\r\nint remove_mapping(struct address_space *mapping, struct page *page)\r\n{\r\nif (__remove_mapping(mapping, page, false)) {\r\npage_unfreeze_refs(page, 1);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid putback_lru_page(struct page *page)\r\n{\r\nbool is_unevictable;\r\nint was_unevictable = PageUnevictable(page);\r\nVM_BUG_ON_PAGE(PageLRU(page), page);\r\nredo:\r\nClearPageUnevictable(page);\r\nif (page_evictable(page)) {\r\nis_unevictable = false;\r\nlru_cache_add(page);\r\n} else {\r\nis_unevictable = true;\r\nadd_page_to_unevictable_list(page);\r\nsmp_mb();\r\n}\r\nif (is_unevictable && page_evictable(page)) {\r\nif (!isolate_lru_page(page)) {\r\nput_page(page);\r\ngoto redo;\r\n}\r\n}\r\nif (was_unevictable && !is_unevictable)\r\ncount_vm_event(UNEVICTABLE_PGRESCUED);\r\nelse if (!was_unevictable && is_unevictable)\r\ncount_vm_event(UNEVICTABLE_PGCULLED);\r\nput_page(page);\r\n}\r\nstatic enum page_references page_check_references(struct page *page,\r\nstruct scan_control *sc)\r\n{\r\nint referenced_ptes, referenced_page;\r\nunsigned long vm_flags;\r\nreferenced_ptes = page_referenced(page, 1, sc->target_mem_cgroup,\r\n&vm_flags);\r\nreferenced_page = TestClearPageReferenced(page);\r\nif (vm_flags & VM_LOCKED)\r\nreturn PAGEREF_RECLAIM;\r\nif (referenced_ptes) {\r\nif (PageSwapBacked(page))\r\nreturn PAGEREF_ACTIVATE;\r\nSetPageReferenced(page);\r\nif (referenced_page || referenced_ptes > 1)\r\nreturn PAGEREF_ACTIVATE;\r\nif (vm_flags & VM_EXEC)\r\nreturn PAGEREF_ACTIVATE;\r\nreturn PAGEREF_KEEP;\r\n}\r\nif (referenced_page && !PageSwapBacked(page))\r\nreturn PAGEREF_RECLAIM_CLEAN;\r\nreturn PAGEREF_RECLAIM;\r\n}\r\nstatic void page_check_dirty_writeback(struct page *page,\r\nbool *dirty, bool *writeback)\r\n{\r\nstruct address_space *mapping;\r\nif (!page_is_file_cache(page)) {\r\n*dirty = false;\r\n*writeback = false;\r\nreturn;\r\n}\r\n*dirty = PageDirty(page);\r\n*writeback = PageWriteback(page);\r\nif (!page_has_private(page))\r\nreturn;\r\nmapping = page_mapping(page);\r\nif (mapping && mapping->a_ops->is_dirty_writeback)\r\nmapping->a_ops->is_dirty_writeback(page, dirty, writeback);\r\n}\r\nstatic unsigned long shrink_page_list(struct list_head *page_list,\r\nstruct zone *zone,\r\nstruct scan_control *sc,\r\nenum ttu_flags ttu_flags,\r\nunsigned long *ret_nr_dirty,\r\nunsigned long *ret_nr_unqueued_dirty,\r\nunsigned long *ret_nr_congested,\r\nunsigned long *ret_nr_writeback,\r\nunsigned long *ret_nr_immediate,\r\nbool force_reclaim)\r\n{\r\nLIST_HEAD(ret_pages);\r\nLIST_HEAD(free_pages);\r\nint pgactivate = 0;\r\nunsigned long nr_unqueued_dirty = 0;\r\nunsigned long nr_dirty = 0;\r\nunsigned long nr_congested = 0;\r\nunsigned long nr_reclaimed = 0;\r\nunsigned long nr_writeback = 0;\r\nunsigned long nr_immediate = 0;\r\ncond_resched();\r\nwhile (!list_empty(page_list)) {\r\nstruct address_space *mapping;\r\nstruct page *page;\r\nint may_enter_fs;\r\nenum page_references references = PAGEREF_RECLAIM_CLEAN;\r\nbool dirty, writeback;\r\ncond_resched();\r\npage = lru_to_page(page_list);\r\nlist_del(&page->lru);\r\nif (!trylock_page(page))\r\ngoto keep;\r\nVM_BUG_ON_PAGE(PageActive(page), page);\r\nVM_BUG_ON_PAGE(page_zone(page) != zone, page);\r\nsc->nr_scanned++;\r\nif (unlikely(!page_evictable(page)))\r\ngoto cull_mlocked;\r\nif (!sc->may_unmap && page_mapped(page))\r\ngoto keep_locked;\r\nif (page_mapped(page) || PageSwapCache(page))\r\nsc->nr_scanned++;\r\nmay_enter_fs = (sc->gfp_mask & __GFP_FS) ||\r\n(PageSwapCache(page) && (sc->gfp_mask & __GFP_IO));\r\npage_check_dirty_writeback(page, &dirty, &writeback);\r\nif (dirty || writeback)\r\nnr_dirty++;\r\nif (dirty && !writeback)\r\nnr_unqueued_dirty++;\r\nmapping = page_mapping(page);\r\nif (((dirty || writeback) && mapping &&\r\nbdi_write_congested(inode_to_bdi(mapping->host))) ||\r\n(writeback && PageReclaim(page)))\r\nnr_congested++;\r\nif (PageWriteback(page)) {\r\nif (current_is_kswapd() &&\r\nPageReclaim(page) &&\r\ntest_bit(ZONE_WRITEBACK, &zone->flags)) {\r\nnr_immediate++;\r\ngoto keep_locked;\r\n} else if (global_reclaim(sc) ||\r\n!PageReclaim(page) || !(sc->gfp_mask & __GFP_IO)) {\r\nSetPageReclaim(page);\r\nnr_writeback++;\r\ngoto keep_locked;\r\n} else {\r\nwait_on_page_writeback(page);\r\n}\r\n}\r\nif (!force_reclaim)\r\nreferences = page_check_references(page, sc);\r\nswitch (references) {\r\ncase PAGEREF_ACTIVATE:\r\ngoto activate_locked;\r\ncase PAGEREF_KEEP:\r\ngoto keep_locked;\r\ncase PAGEREF_RECLAIM:\r\ncase PAGEREF_RECLAIM_CLEAN:\r\n;\r\n}\r\nif (PageAnon(page) && !PageSwapCache(page)) {\r\nif (!(sc->gfp_mask & __GFP_IO))\r\ngoto keep_locked;\r\nif (!add_to_swap(page, page_list))\r\ngoto activate_locked;\r\nmay_enter_fs = 1;\r\nmapping = page_mapping(page);\r\n}\r\nif (page_mapped(page) && mapping) {\r\nswitch (try_to_unmap(page, ttu_flags)) {\r\ncase SWAP_FAIL:\r\ngoto activate_locked;\r\ncase SWAP_AGAIN:\r\ngoto keep_locked;\r\ncase SWAP_MLOCK:\r\ngoto cull_mlocked;\r\ncase SWAP_SUCCESS:\r\n;\r\n}\r\n}\r\nif (PageDirty(page)) {\r\nif (page_is_file_cache(page) &&\r\n(!current_is_kswapd() ||\r\n!test_bit(ZONE_DIRTY, &zone->flags))) {\r\ninc_zone_page_state(page, NR_VMSCAN_IMMEDIATE);\r\nSetPageReclaim(page);\r\ngoto keep_locked;\r\n}\r\nif (references == PAGEREF_RECLAIM_CLEAN)\r\ngoto keep_locked;\r\nif (!may_enter_fs)\r\ngoto keep_locked;\r\nif (!sc->may_writepage)\r\ngoto keep_locked;\r\nswitch (pageout(page, mapping, sc)) {\r\ncase PAGE_KEEP:\r\ngoto keep_locked;\r\ncase PAGE_ACTIVATE:\r\ngoto activate_locked;\r\ncase PAGE_SUCCESS:\r\nif (PageWriteback(page))\r\ngoto keep;\r\nif (PageDirty(page))\r\ngoto keep;\r\nif (!trylock_page(page))\r\ngoto keep;\r\nif (PageDirty(page) || PageWriteback(page))\r\ngoto keep_locked;\r\nmapping = page_mapping(page);\r\ncase PAGE_CLEAN:\r\n;\r\n}\r\n}\r\nif (page_has_private(page)) {\r\nif (!try_to_release_page(page, sc->gfp_mask))\r\ngoto activate_locked;\r\nif (!mapping && page_count(page) == 1) {\r\nunlock_page(page);\r\nif (put_page_testzero(page))\r\ngoto free_it;\r\nelse {\r\nnr_reclaimed++;\r\ncontinue;\r\n}\r\n}\r\n}\r\nif (!mapping || !__remove_mapping(mapping, page, true))\r\ngoto keep_locked;\r\n__clear_page_locked(page);\r\nfree_it:\r\nnr_reclaimed++;\r\nlist_add(&page->lru, &free_pages);\r\ncontinue;\r\ncull_mlocked:\r\nif (PageSwapCache(page))\r\ntry_to_free_swap(page);\r\nunlock_page(page);\r\nputback_lru_page(page);\r\ncontinue;\r\nactivate_locked:\r\nif (PageSwapCache(page) && vm_swap_full())\r\ntry_to_free_swap(page);\r\nVM_BUG_ON_PAGE(PageActive(page), page);\r\nSetPageActive(page);\r\npgactivate++;\r\nkeep_locked:\r\nunlock_page(page);\r\nkeep:\r\nlist_add(&page->lru, &ret_pages);\r\nVM_BUG_ON_PAGE(PageLRU(page) || PageUnevictable(page), page);\r\n}\r\nmem_cgroup_uncharge_list(&free_pages);\r\nfree_hot_cold_page_list(&free_pages, true);\r\nlist_splice(&ret_pages, page_list);\r\ncount_vm_events(PGACTIVATE, pgactivate);\r\n*ret_nr_dirty += nr_dirty;\r\n*ret_nr_congested += nr_congested;\r\n*ret_nr_unqueued_dirty += nr_unqueued_dirty;\r\n*ret_nr_writeback += nr_writeback;\r\n*ret_nr_immediate += nr_immediate;\r\nreturn nr_reclaimed;\r\n}\r\nunsigned long reclaim_clean_pages_from_list(struct zone *zone,\r\nstruct list_head *page_list)\r\n{\r\nstruct scan_control sc = {\r\n.gfp_mask = GFP_KERNEL,\r\n.priority = DEF_PRIORITY,\r\n.may_unmap = 1,\r\n};\r\nunsigned long ret, dummy1, dummy2, dummy3, dummy4, dummy5;\r\nstruct page *page, *next;\r\nLIST_HEAD(clean_pages);\r\nlist_for_each_entry_safe(page, next, page_list, lru) {\r\nif (page_is_file_cache(page) && !PageDirty(page) &&\r\n!isolated_balloon_page(page)) {\r\nClearPageActive(page);\r\nlist_move(&page->lru, &clean_pages);\r\n}\r\n}\r\nret = shrink_page_list(&clean_pages, zone, &sc,\r\nTTU_UNMAP|TTU_IGNORE_ACCESS,\r\n&dummy1, &dummy2, &dummy3, &dummy4, &dummy5, true);\r\nlist_splice(&clean_pages, page_list);\r\nmod_zone_page_state(zone, NR_ISOLATED_FILE, -ret);\r\nreturn ret;\r\n}\r\nint __isolate_lru_page(struct page *page, isolate_mode_t mode)\r\n{\r\nint ret = -EINVAL;\r\nif (!PageLRU(page))\r\nreturn ret;\r\nif (PageUnevictable(page) && !(mode & ISOLATE_UNEVICTABLE))\r\nreturn ret;\r\nret = -EBUSY;\r\nif (mode & (ISOLATE_CLEAN|ISOLATE_ASYNC_MIGRATE)) {\r\nif (PageWriteback(page))\r\nreturn ret;\r\nif (PageDirty(page)) {\r\nstruct address_space *mapping;\r\nif (mode & ISOLATE_CLEAN)\r\nreturn ret;\r\nmapping = page_mapping(page);\r\nif (mapping && !mapping->a_ops->migratepage)\r\nreturn ret;\r\n}\r\n}\r\nif ((mode & ISOLATE_UNMAPPED) && page_mapped(page))\r\nreturn ret;\r\nif (likely(get_page_unless_zero(page))) {\r\nClearPageLRU(page);\r\nret = 0;\r\n}\r\nreturn ret;\r\n}\r\nstatic unsigned long isolate_lru_pages(unsigned long nr_to_scan,\r\nstruct lruvec *lruvec, struct list_head *dst,\r\nunsigned long *nr_scanned, struct scan_control *sc,\r\nisolate_mode_t mode, enum lru_list lru)\r\n{\r\nstruct list_head *src = &lruvec->lists[lru];\r\nunsigned long nr_taken = 0;\r\nunsigned long scan;\r\nfor (scan = 0; scan < nr_to_scan && !list_empty(src); scan++) {\r\nstruct page *page;\r\nint nr_pages;\r\npage = lru_to_page(src);\r\nprefetchw_prev_lru_page(page, src, flags);\r\nVM_BUG_ON_PAGE(!PageLRU(page), page);\r\nswitch (__isolate_lru_page(page, mode)) {\r\ncase 0:\r\nnr_pages = hpage_nr_pages(page);\r\nmem_cgroup_update_lru_size(lruvec, lru, -nr_pages);\r\nlist_move(&page->lru, dst);\r\nnr_taken += nr_pages;\r\nbreak;\r\ncase -EBUSY:\r\nlist_move(&page->lru, src);\r\ncontinue;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\n*nr_scanned = scan;\r\ntrace_mm_vmscan_lru_isolate(sc->order, nr_to_scan, scan,\r\nnr_taken, mode, is_file_lru(lru));\r\nreturn nr_taken;\r\n}\r\nint isolate_lru_page(struct page *page)\r\n{\r\nint ret = -EBUSY;\r\nVM_BUG_ON_PAGE(!page_count(page), page);\r\nif (PageLRU(page)) {\r\nstruct zone *zone = page_zone(page);\r\nstruct lruvec *lruvec;\r\nspin_lock_irq(&zone->lru_lock);\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nif (PageLRU(page)) {\r\nint lru = page_lru(page);\r\nget_page(page);\r\nClearPageLRU(page);\r\ndel_page_from_lru_list(page, lruvec, lru);\r\nret = 0;\r\n}\r\nspin_unlock_irq(&zone->lru_lock);\r\n}\r\nreturn ret;\r\n}\r\nstatic int too_many_isolated(struct zone *zone, int file,\r\nstruct scan_control *sc)\r\n{\r\nunsigned long inactive, isolated;\r\nif (current_is_kswapd())\r\nreturn 0;\r\nif (!global_reclaim(sc))\r\nreturn 0;\r\nif (file) {\r\ninactive = zone_page_state(zone, NR_INACTIVE_FILE);\r\nisolated = zone_page_state(zone, NR_ISOLATED_FILE);\r\n} else {\r\ninactive = zone_page_state(zone, NR_INACTIVE_ANON);\r\nisolated = zone_page_state(zone, NR_ISOLATED_ANON);\r\n}\r\nif ((sc->gfp_mask & GFP_IOFS) == GFP_IOFS)\r\ninactive >>= 3;\r\nreturn isolated > inactive;\r\n}\r\nstatic noinline_for_stack void\r\nputback_inactive_pages(struct lruvec *lruvec, struct list_head *page_list)\r\n{\r\nstruct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nLIST_HEAD(pages_to_free);\r\nwhile (!list_empty(page_list)) {\r\nstruct page *page = lru_to_page(page_list);\r\nint lru;\r\nVM_BUG_ON_PAGE(PageLRU(page), page);\r\nlist_del(&page->lru);\r\nif (unlikely(!page_evictable(page))) {\r\nspin_unlock_irq(&zone->lru_lock);\r\nputback_lru_page(page);\r\nspin_lock_irq(&zone->lru_lock);\r\ncontinue;\r\n}\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nSetPageLRU(page);\r\nlru = page_lru(page);\r\nadd_page_to_lru_list(page, lruvec, lru);\r\nif (is_active_lru(lru)) {\r\nint file = is_file_lru(lru);\r\nint numpages = hpage_nr_pages(page);\r\nreclaim_stat->recent_rotated[file] += numpages;\r\n}\r\nif (put_page_testzero(page)) {\r\n__ClearPageLRU(page);\r\n__ClearPageActive(page);\r\ndel_page_from_lru_list(page, lruvec, lru);\r\nif (unlikely(PageCompound(page))) {\r\nspin_unlock_irq(&zone->lru_lock);\r\nmem_cgroup_uncharge(page);\r\n(*get_compound_page_dtor(page))(page);\r\nspin_lock_irq(&zone->lru_lock);\r\n} else\r\nlist_add(&page->lru, &pages_to_free);\r\n}\r\n}\r\nlist_splice(&pages_to_free, page_list);\r\n}\r\nstatic int current_may_throttle(void)\r\n{\r\nreturn !(current->flags & PF_LESS_THROTTLE) ||\r\ncurrent->backing_dev_info == NULL ||\r\nbdi_write_congested(current->backing_dev_info);\r\n}\r\nstatic noinline_for_stack unsigned long\r\nshrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,\r\nstruct scan_control *sc, enum lru_list lru)\r\n{\r\nLIST_HEAD(page_list);\r\nunsigned long nr_scanned;\r\nunsigned long nr_reclaimed = 0;\r\nunsigned long nr_taken;\r\nunsigned long nr_dirty = 0;\r\nunsigned long nr_congested = 0;\r\nunsigned long nr_unqueued_dirty = 0;\r\nunsigned long nr_writeback = 0;\r\nunsigned long nr_immediate = 0;\r\nisolate_mode_t isolate_mode = 0;\r\nint file = is_file_lru(lru);\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nstruct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;\r\nwhile (unlikely(too_many_isolated(zone, file, sc))) {\r\ncongestion_wait(BLK_RW_ASYNC, HZ/10);\r\nif (fatal_signal_pending(current))\r\nreturn SWAP_CLUSTER_MAX;\r\n}\r\nlru_add_drain();\r\nif (!sc->may_unmap)\r\nisolate_mode |= ISOLATE_UNMAPPED;\r\nif (!sc->may_writepage)\r\nisolate_mode |= ISOLATE_CLEAN;\r\nspin_lock_irq(&zone->lru_lock);\r\nnr_taken = isolate_lru_pages(nr_to_scan, lruvec, &page_list,\r\n&nr_scanned, sc, isolate_mode, lru);\r\n__mod_zone_page_state(zone, NR_LRU_BASE + lru, -nr_taken);\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON + file, nr_taken);\r\nif (global_reclaim(sc)) {\r\n__mod_zone_page_state(zone, NR_PAGES_SCANNED, nr_scanned);\r\nif (current_is_kswapd())\r\n__count_zone_vm_events(PGSCAN_KSWAPD, zone, nr_scanned);\r\nelse\r\n__count_zone_vm_events(PGSCAN_DIRECT, zone, nr_scanned);\r\n}\r\nspin_unlock_irq(&zone->lru_lock);\r\nif (nr_taken == 0)\r\nreturn 0;\r\nnr_reclaimed = shrink_page_list(&page_list, zone, sc, TTU_UNMAP,\r\n&nr_dirty, &nr_unqueued_dirty, &nr_congested,\r\n&nr_writeback, &nr_immediate,\r\nfalse);\r\nspin_lock_irq(&zone->lru_lock);\r\nreclaim_stat->recent_scanned[file] += nr_taken;\r\nif (global_reclaim(sc)) {\r\nif (current_is_kswapd())\r\n__count_zone_vm_events(PGSTEAL_KSWAPD, zone,\r\nnr_reclaimed);\r\nelse\r\n__count_zone_vm_events(PGSTEAL_DIRECT, zone,\r\nnr_reclaimed);\r\n}\r\nputback_inactive_pages(lruvec, &page_list);\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON + file, -nr_taken);\r\nspin_unlock_irq(&zone->lru_lock);\r\nmem_cgroup_uncharge_list(&page_list);\r\nfree_hot_cold_page_list(&page_list, true);\r\nif (nr_writeback && nr_writeback == nr_taken)\r\nset_bit(ZONE_WRITEBACK, &zone->flags);\r\nif (global_reclaim(sc)) {\r\nif (nr_dirty && nr_dirty == nr_congested)\r\nset_bit(ZONE_CONGESTED, &zone->flags);\r\nif (nr_unqueued_dirty == nr_taken)\r\nset_bit(ZONE_DIRTY, &zone->flags);\r\nif (nr_immediate && current_may_throttle())\r\ncongestion_wait(BLK_RW_ASYNC, HZ/10);\r\n}\r\nif (!sc->hibernation_mode && !current_is_kswapd() &&\r\ncurrent_may_throttle())\r\nwait_iff_congested(zone, BLK_RW_ASYNC, HZ/10);\r\ntrace_mm_vmscan_lru_shrink_inactive(zone->zone_pgdat->node_id,\r\nzone_idx(zone),\r\nnr_scanned, nr_reclaimed,\r\nsc->priority,\r\ntrace_shrink_flags(file));\r\nreturn nr_reclaimed;\r\n}\r\nstatic void move_active_pages_to_lru(struct lruvec *lruvec,\r\nstruct list_head *list,\r\nstruct list_head *pages_to_free,\r\nenum lru_list lru)\r\n{\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nunsigned long pgmoved = 0;\r\nstruct page *page;\r\nint nr_pages;\r\nwhile (!list_empty(list)) {\r\npage = lru_to_page(list);\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nVM_BUG_ON_PAGE(PageLRU(page), page);\r\nSetPageLRU(page);\r\nnr_pages = hpage_nr_pages(page);\r\nmem_cgroup_update_lru_size(lruvec, lru, nr_pages);\r\nlist_move(&page->lru, &lruvec->lists[lru]);\r\npgmoved += nr_pages;\r\nif (put_page_testzero(page)) {\r\n__ClearPageLRU(page);\r\n__ClearPageActive(page);\r\ndel_page_from_lru_list(page, lruvec, lru);\r\nif (unlikely(PageCompound(page))) {\r\nspin_unlock_irq(&zone->lru_lock);\r\nmem_cgroup_uncharge(page);\r\n(*get_compound_page_dtor(page))(page);\r\nspin_lock_irq(&zone->lru_lock);\r\n} else\r\nlist_add(&page->lru, pages_to_free);\r\n}\r\n}\r\n__mod_zone_page_state(zone, NR_LRU_BASE + lru, pgmoved);\r\nif (!is_active_lru(lru))\r\n__count_vm_events(PGDEACTIVATE, pgmoved);\r\n}\r\nstatic void shrink_active_list(unsigned long nr_to_scan,\r\nstruct lruvec *lruvec,\r\nstruct scan_control *sc,\r\nenum lru_list lru)\r\n{\r\nunsigned long nr_taken;\r\nunsigned long nr_scanned;\r\nunsigned long vm_flags;\r\nLIST_HEAD(l_hold);\r\nLIST_HEAD(l_active);\r\nLIST_HEAD(l_inactive);\r\nstruct page *page;\r\nstruct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;\r\nunsigned long nr_rotated = 0;\r\nisolate_mode_t isolate_mode = 0;\r\nint file = is_file_lru(lru);\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nlru_add_drain();\r\nif (!sc->may_unmap)\r\nisolate_mode |= ISOLATE_UNMAPPED;\r\nif (!sc->may_writepage)\r\nisolate_mode |= ISOLATE_CLEAN;\r\nspin_lock_irq(&zone->lru_lock);\r\nnr_taken = isolate_lru_pages(nr_to_scan, lruvec, &l_hold,\r\n&nr_scanned, sc, isolate_mode, lru);\r\nif (global_reclaim(sc))\r\n__mod_zone_page_state(zone, NR_PAGES_SCANNED, nr_scanned);\r\nreclaim_stat->recent_scanned[file] += nr_taken;\r\n__count_zone_vm_events(PGREFILL, zone, nr_scanned);\r\n__mod_zone_page_state(zone, NR_LRU_BASE + lru, -nr_taken);\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON + file, nr_taken);\r\nspin_unlock_irq(&zone->lru_lock);\r\nwhile (!list_empty(&l_hold)) {\r\ncond_resched();\r\npage = lru_to_page(&l_hold);\r\nlist_del(&page->lru);\r\nif (unlikely(!page_evictable(page))) {\r\nputback_lru_page(page);\r\ncontinue;\r\n}\r\nif (unlikely(buffer_heads_over_limit)) {\r\nif (page_has_private(page) && trylock_page(page)) {\r\nif (page_has_private(page))\r\ntry_to_release_page(page, 0);\r\nunlock_page(page);\r\n}\r\n}\r\nif (page_referenced(page, 0, sc->target_mem_cgroup,\r\n&vm_flags)) {\r\nnr_rotated += hpage_nr_pages(page);\r\nif ((vm_flags & VM_EXEC) && page_is_file_cache(page)) {\r\nlist_add(&page->lru, &l_active);\r\ncontinue;\r\n}\r\n}\r\nClearPageActive(page);\r\nlist_add(&page->lru, &l_inactive);\r\n}\r\nspin_lock_irq(&zone->lru_lock);\r\nreclaim_stat->recent_rotated[file] += nr_rotated;\r\nmove_active_pages_to_lru(lruvec, &l_active, &l_hold, lru);\r\nmove_active_pages_to_lru(lruvec, &l_inactive, &l_hold, lru - LRU_ACTIVE);\r\n__mod_zone_page_state(zone, NR_ISOLATED_ANON + file, -nr_taken);\r\nspin_unlock_irq(&zone->lru_lock);\r\nmem_cgroup_uncharge_list(&l_hold);\r\nfree_hot_cold_page_list(&l_hold, true);\r\n}\r\nstatic int inactive_anon_is_low_global(struct zone *zone)\r\n{\r\nunsigned long active, inactive;\r\nactive = zone_page_state(zone, NR_ACTIVE_ANON);\r\ninactive = zone_page_state(zone, NR_INACTIVE_ANON);\r\nif (inactive * zone->inactive_ratio < active)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int inactive_anon_is_low(struct lruvec *lruvec)\r\n{\r\nif (!total_swap_pages)\r\nreturn 0;\r\nif (!mem_cgroup_disabled())\r\nreturn mem_cgroup_inactive_anon_is_low(lruvec);\r\nreturn inactive_anon_is_low_global(lruvec_zone(lruvec));\r\n}\r\nstatic inline int inactive_anon_is_low(struct lruvec *lruvec)\r\n{\r\nreturn 0;\r\n}\r\nstatic int inactive_file_is_low(struct lruvec *lruvec)\r\n{\r\nunsigned long inactive;\r\nunsigned long active;\r\ninactive = get_lru_size(lruvec, LRU_INACTIVE_FILE);\r\nactive = get_lru_size(lruvec, LRU_ACTIVE_FILE);\r\nreturn active > inactive;\r\n}\r\nstatic int inactive_list_is_low(struct lruvec *lruvec, enum lru_list lru)\r\n{\r\nif (is_file_lru(lru))\r\nreturn inactive_file_is_low(lruvec);\r\nelse\r\nreturn inactive_anon_is_low(lruvec);\r\n}\r\nstatic unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,\r\nstruct lruvec *lruvec, struct scan_control *sc)\r\n{\r\nif (is_active_lru(lru)) {\r\nif (inactive_list_is_low(lruvec, lru))\r\nshrink_active_list(nr_to_scan, lruvec, sc, lru);\r\nreturn 0;\r\n}\r\nreturn shrink_inactive_list(nr_to_scan, lruvec, sc, lru);\r\n}\r\nstatic void get_scan_count(struct lruvec *lruvec, int swappiness,\r\nstruct scan_control *sc, unsigned long *nr,\r\nunsigned long *lru_pages)\r\n{\r\nstruct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;\r\nu64 fraction[2];\r\nu64 denominator = 0;\r\nstruct zone *zone = lruvec_zone(lruvec);\r\nunsigned long anon_prio, file_prio;\r\nenum scan_balance scan_balance;\r\nunsigned long anon, file;\r\nbool force_scan = false;\r\nunsigned long ap, fp;\r\nenum lru_list lru;\r\nbool some_scanned;\r\nint pass;\r\nif (current_is_kswapd()) {\r\nif (!zone_reclaimable(zone))\r\nforce_scan = true;\r\nif (!mem_cgroup_lruvec_online(lruvec))\r\nforce_scan = true;\r\n}\r\nif (!global_reclaim(sc))\r\nforce_scan = true;\r\nif (!sc->may_swap || (get_nr_swap_pages() <= 0)) {\r\nscan_balance = SCAN_FILE;\r\ngoto out;\r\n}\r\nif (!global_reclaim(sc) && !swappiness) {\r\nscan_balance = SCAN_FILE;\r\ngoto out;\r\n}\r\nif (!sc->priority && swappiness) {\r\nscan_balance = SCAN_EQUAL;\r\ngoto out;\r\n}\r\nif (global_reclaim(sc)) {\r\nunsigned long zonefile;\r\nunsigned long zonefree;\r\nzonefree = zone_page_state(zone, NR_FREE_PAGES);\r\nzonefile = zone_page_state(zone, NR_ACTIVE_FILE) +\r\nzone_page_state(zone, NR_INACTIVE_FILE);\r\nif (unlikely(zonefile + zonefree <= high_wmark_pages(zone))) {\r\nscan_balance = SCAN_ANON;\r\ngoto out;\r\n}\r\n}\r\nif (!inactive_file_is_low(lruvec)) {\r\nscan_balance = SCAN_FILE;\r\ngoto out;\r\n}\r\nscan_balance = SCAN_FRACT;\r\nanon_prio = swappiness;\r\nfile_prio = 200 - anon_prio;\r\nanon = get_lru_size(lruvec, LRU_ACTIVE_ANON) +\r\nget_lru_size(lruvec, LRU_INACTIVE_ANON);\r\nfile = get_lru_size(lruvec, LRU_ACTIVE_FILE) +\r\nget_lru_size(lruvec, LRU_INACTIVE_FILE);\r\nspin_lock_irq(&zone->lru_lock);\r\nif (unlikely(reclaim_stat->recent_scanned[0] > anon / 4)) {\r\nreclaim_stat->recent_scanned[0] /= 2;\r\nreclaim_stat->recent_rotated[0] /= 2;\r\n}\r\nif (unlikely(reclaim_stat->recent_scanned[1] > file / 4)) {\r\nreclaim_stat->recent_scanned[1] /= 2;\r\nreclaim_stat->recent_rotated[1] /= 2;\r\n}\r\nap = anon_prio * (reclaim_stat->recent_scanned[0] + 1);\r\nap /= reclaim_stat->recent_rotated[0] + 1;\r\nfp = file_prio * (reclaim_stat->recent_scanned[1] + 1);\r\nfp /= reclaim_stat->recent_rotated[1] + 1;\r\nspin_unlock_irq(&zone->lru_lock);\r\nfraction[0] = ap;\r\nfraction[1] = fp;\r\ndenominator = ap + fp + 1;\r\nout:\r\nsome_scanned = false;\r\nfor (pass = 0; !some_scanned && pass < 2; pass++) {\r\n*lru_pages = 0;\r\nfor_each_evictable_lru(lru) {\r\nint file = is_file_lru(lru);\r\nunsigned long size;\r\nunsigned long scan;\r\nsize = get_lru_size(lruvec, lru);\r\nscan = size >> sc->priority;\r\nif (!scan && pass && force_scan)\r\nscan = min(size, SWAP_CLUSTER_MAX);\r\nswitch (scan_balance) {\r\ncase SCAN_EQUAL:\r\nbreak;\r\ncase SCAN_FRACT:\r\nscan = div64_u64(scan * fraction[file],\r\ndenominator);\r\nbreak;\r\ncase SCAN_FILE:\r\ncase SCAN_ANON:\r\nif ((scan_balance == SCAN_FILE) != file) {\r\nsize = 0;\r\nscan = 0;\r\n}\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n*lru_pages += size;\r\nnr[lru] = scan;\r\nsome_scanned |= !!scan;\r\n}\r\n}\r\n}\r\nstatic void shrink_lruvec(struct lruvec *lruvec, int swappiness,\r\nstruct scan_control *sc, unsigned long *lru_pages)\r\n{\r\nunsigned long nr[NR_LRU_LISTS];\r\nunsigned long targets[NR_LRU_LISTS];\r\nunsigned long nr_to_scan;\r\nenum lru_list lru;\r\nunsigned long nr_reclaimed = 0;\r\nunsigned long nr_to_reclaim = sc->nr_to_reclaim;\r\nstruct blk_plug plug;\r\nbool scan_adjusted;\r\nget_scan_count(lruvec, swappiness, sc, nr, lru_pages);\r\nmemcpy(targets, nr, sizeof(nr));\r\nscan_adjusted = (global_reclaim(sc) && !current_is_kswapd() &&\r\nsc->priority == DEF_PRIORITY);\r\nblk_start_plug(&plug);\r\nwhile (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||\r\nnr[LRU_INACTIVE_FILE]) {\r\nunsigned long nr_anon, nr_file, percentage;\r\nunsigned long nr_scanned;\r\nfor_each_evictable_lru(lru) {\r\nif (nr[lru]) {\r\nnr_to_scan = min(nr[lru], SWAP_CLUSTER_MAX);\r\nnr[lru] -= nr_to_scan;\r\nnr_reclaimed += shrink_list(lru, nr_to_scan,\r\nlruvec, sc);\r\n}\r\n}\r\nif (nr_reclaimed < nr_to_reclaim || scan_adjusted)\r\ncontinue;\r\nnr_file = nr[LRU_INACTIVE_FILE] + nr[LRU_ACTIVE_FILE];\r\nnr_anon = nr[LRU_INACTIVE_ANON] + nr[LRU_ACTIVE_ANON];\r\nif (!nr_file || !nr_anon)\r\nbreak;\r\nif (nr_file > nr_anon) {\r\nunsigned long scan_target = targets[LRU_INACTIVE_ANON] +\r\ntargets[LRU_ACTIVE_ANON] + 1;\r\nlru = LRU_BASE;\r\npercentage = nr_anon * 100 / scan_target;\r\n} else {\r\nunsigned long scan_target = targets[LRU_INACTIVE_FILE] +\r\ntargets[LRU_ACTIVE_FILE] + 1;\r\nlru = LRU_FILE;\r\npercentage = nr_file * 100 / scan_target;\r\n}\r\nnr[lru] = 0;\r\nnr[lru + LRU_ACTIVE] = 0;\r\nlru = (lru == LRU_FILE) ? LRU_BASE : LRU_FILE;\r\nnr_scanned = targets[lru] - nr[lru];\r\nnr[lru] = targets[lru] * (100 - percentage) / 100;\r\nnr[lru] -= min(nr[lru], nr_scanned);\r\nlru += LRU_ACTIVE;\r\nnr_scanned = targets[lru] - nr[lru];\r\nnr[lru] = targets[lru] * (100 - percentage) / 100;\r\nnr[lru] -= min(nr[lru], nr_scanned);\r\nscan_adjusted = true;\r\n}\r\nblk_finish_plug(&plug);\r\nsc->nr_reclaimed += nr_reclaimed;\r\nif (inactive_anon_is_low(lruvec))\r\nshrink_active_list(SWAP_CLUSTER_MAX, lruvec,\r\nsc, LRU_ACTIVE_ANON);\r\nthrottle_vm_writeout(sc->gfp_mask);\r\n}\r\nstatic bool in_reclaim_compaction(struct scan_control *sc)\r\n{\r\nif (IS_ENABLED(CONFIG_COMPACTION) && sc->order &&\r\n(sc->order > PAGE_ALLOC_COSTLY_ORDER ||\r\nsc->priority < DEF_PRIORITY - 2))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline bool should_continue_reclaim(struct zone *zone,\r\nunsigned long nr_reclaimed,\r\nunsigned long nr_scanned,\r\nstruct scan_control *sc)\r\n{\r\nunsigned long pages_for_compaction;\r\nunsigned long inactive_lru_pages;\r\nif (!in_reclaim_compaction(sc))\r\nreturn false;\r\nif (sc->gfp_mask & __GFP_REPEAT) {\r\nif (!nr_reclaimed && !nr_scanned)\r\nreturn false;\r\n} else {\r\nif (!nr_reclaimed)\r\nreturn false;\r\n}\r\npages_for_compaction = (2UL << sc->order);\r\ninactive_lru_pages = zone_page_state(zone, NR_INACTIVE_FILE);\r\nif (get_nr_swap_pages() > 0)\r\ninactive_lru_pages += zone_page_state(zone, NR_INACTIVE_ANON);\r\nif (sc->nr_reclaimed < pages_for_compaction &&\r\ninactive_lru_pages > pages_for_compaction)\r\nreturn true;\r\nswitch (compaction_suitable(zone, sc->order, 0, 0)) {\r\ncase COMPACT_PARTIAL:\r\ncase COMPACT_CONTINUE:\r\nreturn false;\r\ndefault:\r\nreturn true;\r\n}\r\n}\r\nstatic bool shrink_zone(struct zone *zone, struct scan_control *sc,\r\nbool is_classzone)\r\n{\r\nstruct reclaim_state *reclaim_state = current->reclaim_state;\r\nunsigned long nr_reclaimed, nr_scanned;\r\nbool reclaimable = false;\r\ndo {\r\nstruct mem_cgroup *root = sc->target_mem_cgroup;\r\nstruct mem_cgroup_reclaim_cookie reclaim = {\r\n.zone = zone,\r\n.priority = sc->priority,\r\n};\r\nunsigned long zone_lru_pages = 0;\r\nstruct mem_cgroup *memcg;\r\nnr_reclaimed = sc->nr_reclaimed;\r\nnr_scanned = sc->nr_scanned;\r\nmemcg = mem_cgroup_iter(root, NULL, &reclaim);\r\ndo {\r\nunsigned long lru_pages;\r\nunsigned long scanned;\r\nstruct lruvec *lruvec;\r\nint swappiness;\r\nif (mem_cgroup_low(root, memcg)) {\r\nif (!sc->may_thrash)\r\ncontinue;\r\nmem_cgroup_events(memcg, MEMCG_LOW, 1);\r\n}\r\nlruvec = mem_cgroup_zone_lruvec(zone, memcg);\r\nswappiness = mem_cgroup_swappiness(memcg);\r\nscanned = sc->nr_scanned;\r\nshrink_lruvec(lruvec, swappiness, sc, &lru_pages);\r\nzone_lru_pages += lru_pages;\r\nif (memcg && is_classzone)\r\nshrink_slab(sc->gfp_mask, zone_to_nid(zone),\r\nmemcg, sc->nr_scanned - scanned,\r\nlru_pages);\r\nif (!global_reclaim(sc) &&\r\nsc->nr_reclaimed >= sc->nr_to_reclaim) {\r\nmem_cgroup_iter_break(root, memcg);\r\nbreak;\r\n}\r\n} while ((memcg = mem_cgroup_iter(root, memcg, &reclaim)));\r\nif (global_reclaim(sc) && is_classzone)\r\nshrink_slab(sc->gfp_mask, zone_to_nid(zone), NULL,\r\nsc->nr_scanned - nr_scanned,\r\nzone_lru_pages);\r\nif (reclaim_state) {\r\nsc->nr_reclaimed += reclaim_state->reclaimed_slab;\r\nreclaim_state->reclaimed_slab = 0;\r\n}\r\nvmpressure(sc->gfp_mask, sc->target_mem_cgroup,\r\nsc->nr_scanned - nr_scanned,\r\nsc->nr_reclaimed - nr_reclaimed);\r\nif (sc->nr_reclaimed - nr_reclaimed)\r\nreclaimable = true;\r\n} while (should_continue_reclaim(zone, sc->nr_reclaimed - nr_reclaimed,\r\nsc->nr_scanned - nr_scanned, sc));\r\nreturn reclaimable;\r\n}\r\nstatic inline bool compaction_ready(struct zone *zone, int order)\r\n{\r\nunsigned long balance_gap, watermark;\r\nbool watermark_ok;\r\nbalance_gap = min(low_wmark_pages(zone), DIV_ROUND_UP(\r\nzone->managed_pages, KSWAPD_ZONE_BALANCE_GAP_RATIO));\r\nwatermark = high_wmark_pages(zone) + balance_gap + (2UL << order);\r\nwatermark_ok = zone_watermark_ok_safe(zone, 0, watermark, 0, 0);\r\nif (compaction_deferred(zone, order))\r\nreturn watermark_ok;\r\nif (compaction_suitable(zone, order, 0, 0) == COMPACT_SKIPPED)\r\nreturn false;\r\nreturn watermark_ok;\r\n}\r\nstatic bool shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\r\n{\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\nunsigned long nr_soft_reclaimed;\r\nunsigned long nr_soft_scanned;\r\ngfp_t orig_mask;\r\nenum zone_type requested_highidx = gfp_zone(sc->gfp_mask);\r\nbool reclaimable = false;\r\norig_mask = sc->gfp_mask;\r\nif (buffer_heads_over_limit)\r\nsc->gfp_mask |= __GFP_HIGHMEM;\r\nfor_each_zone_zonelist_nodemask(zone, z, zonelist,\r\nrequested_highidx, sc->nodemask) {\r\nenum zone_type classzone_idx;\r\nif (!populated_zone(zone))\r\ncontinue;\r\nclasszone_idx = requested_highidx;\r\nwhile (!populated_zone(zone->zone_pgdat->node_zones +\r\nclasszone_idx))\r\nclasszone_idx--;\r\nif (global_reclaim(sc)) {\r\nif (!cpuset_zone_allowed(zone,\r\nGFP_KERNEL | __GFP_HARDWALL))\r\ncontinue;\r\nif (sc->priority != DEF_PRIORITY &&\r\n!zone_reclaimable(zone))\r\ncontinue;\r\nif (IS_ENABLED(CONFIG_COMPACTION) &&\r\nsc->order > PAGE_ALLOC_COSTLY_ORDER &&\r\nzonelist_zone_idx(z) <= requested_highidx &&\r\ncompaction_ready(zone, sc->order)) {\r\nsc->compaction_ready = true;\r\ncontinue;\r\n}\r\nnr_soft_scanned = 0;\r\nnr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone,\r\nsc->order, sc->gfp_mask,\r\n&nr_soft_scanned);\r\nsc->nr_reclaimed += nr_soft_reclaimed;\r\nsc->nr_scanned += nr_soft_scanned;\r\nif (nr_soft_reclaimed)\r\nreclaimable = true;\r\n}\r\nif (shrink_zone(zone, sc, zone_idx(zone) == classzone_idx))\r\nreclaimable = true;\r\nif (global_reclaim(sc) &&\r\n!reclaimable && zone_reclaimable(zone))\r\nreclaimable = true;\r\n}\r\nsc->gfp_mask = orig_mask;\r\nreturn reclaimable;\r\n}\r\nstatic unsigned long do_try_to_free_pages(struct zonelist *zonelist,\r\nstruct scan_control *sc)\r\n{\r\nint initial_priority = sc->priority;\r\nunsigned long total_scanned = 0;\r\nunsigned long writeback_threshold;\r\nbool zones_reclaimable;\r\nretry:\r\ndelayacct_freepages_start();\r\nif (global_reclaim(sc))\r\ncount_vm_event(ALLOCSTALL);\r\ndo {\r\nvmpressure_prio(sc->gfp_mask, sc->target_mem_cgroup,\r\nsc->priority);\r\nsc->nr_scanned = 0;\r\nzones_reclaimable = shrink_zones(zonelist, sc);\r\ntotal_scanned += sc->nr_scanned;\r\nif (sc->nr_reclaimed >= sc->nr_to_reclaim)\r\nbreak;\r\nif (sc->compaction_ready)\r\nbreak;\r\nif (sc->priority < DEF_PRIORITY - 2)\r\nsc->may_writepage = 1;\r\nwriteback_threshold = sc->nr_to_reclaim + sc->nr_to_reclaim / 2;\r\nif (total_scanned > writeback_threshold) {\r\nwakeup_flusher_threads(laptop_mode ? 0 : total_scanned,\r\nWB_REASON_TRY_TO_FREE_PAGES);\r\nsc->may_writepage = 1;\r\n}\r\n} while (--sc->priority >= 0);\r\ndelayacct_freepages_end();\r\nif (sc->nr_reclaimed)\r\nreturn sc->nr_reclaimed;\r\nif (sc->compaction_ready)\r\nreturn 1;\r\nif (!sc->may_thrash) {\r\nsc->priority = initial_priority;\r\nsc->may_thrash = 1;\r\ngoto retry;\r\n}\r\nif (zones_reclaimable)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic bool pfmemalloc_watermark_ok(pg_data_t *pgdat)\r\n{\r\nstruct zone *zone;\r\nunsigned long pfmemalloc_reserve = 0;\r\nunsigned long free_pages = 0;\r\nint i;\r\nbool wmark_ok;\r\nfor (i = 0; i <= ZONE_NORMAL; i++) {\r\nzone = &pgdat->node_zones[i];\r\nif (!populated_zone(zone))\r\ncontinue;\r\npfmemalloc_reserve += min_wmark_pages(zone);\r\nfree_pages += zone_page_state(zone, NR_FREE_PAGES);\r\n}\r\nif (!pfmemalloc_reserve)\r\nreturn true;\r\nwmark_ok = free_pages > pfmemalloc_reserve / 2;\r\nif (!wmark_ok && waitqueue_active(&pgdat->kswapd_wait)) {\r\npgdat->classzone_idx = min(pgdat->classzone_idx,\r\n(enum zone_type)ZONE_NORMAL);\r\nwake_up_interruptible(&pgdat->kswapd_wait);\r\n}\r\nreturn wmark_ok;\r\n}\r\nstatic bool throttle_direct_reclaim(gfp_t gfp_mask, struct zonelist *zonelist,\r\nnodemask_t *nodemask)\r\n{\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\npg_data_t *pgdat = NULL;\r\nif (current->flags & PF_KTHREAD)\r\ngoto out;\r\nif (fatal_signal_pending(current))\r\ngoto out;\r\nfor_each_zone_zonelist_nodemask(zone, z, zonelist,\r\ngfp_zone(gfp_mask), nodemask) {\r\nif (zone_idx(zone) > ZONE_NORMAL)\r\ncontinue;\r\npgdat = zone->zone_pgdat;\r\nif (pfmemalloc_watermark_ok(pgdat))\r\ngoto out;\r\nbreak;\r\n}\r\nif (!pgdat)\r\ngoto out;\r\ncount_vm_event(PGSCAN_DIRECT_THROTTLE);\r\nif (!(gfp_mask & __GFP_FS)) {\r\nwait_event_interruptible_timeout(pgdat->pfmemalloc_wait,\r\npfmemalloc_watermark_ok(pgdat), HZ);\r\ngoto check_pending;\r\n}\r\nwait_event_killable(zone->zone_pgdat->pfmemalloc_wait,\r\npfmemalloc_watermark_ok(pgdat));\r\ncheck_pending:\r\nif (fatal_signal_pending(current))\r\nreturn true;\r\nout:\r\nreturn false;\r\n}\r\nunsigned long try_to_free_pages(struct zonelist *zonelist, int order,\r\ngfp_t gfp_mask, nodemask_t *nodemask)\r\n{\r\nunsigned long nr_reclaimed;\r\nstruct scan_control sc = {\r\n.nr_to_reclaim = SWAP_CLUSTER_MAX,\r\n.gfp_mask = (gfp_mask = memalloc_noio_flags(gfp_mask)),\r\n.order = order,\r\n.nodemask = nodemask,\r\n.priority = DEF_PRIORITY,\r\n.may_writepage = !laptop_mode,\r\n.may_unmap = 1,\r\n.may_swap = 1,\r\n};\r\nif (throttle_direct_reclaim(gfp_mask, zonelist, nodemask))\r\nreturn 1;\r\ntrace_mm_vmscan_direct_reclaim_begin(order,\r\nsc.may_writepage,\r\ngfp_mask);\r\nnr_reclaimed = do_try_to_free_pages(zonelist, &sc);\r\ntrace_mm_vmscan_direct_reclaim_end(nr_reclaimed);\r\nreturn nr_reclaimed;\r\n}\r\nunsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *memcg,\r\ngfp_t gfp_mask, bool noswap,\r\nstruct zone *zone,\r\nunsigned long *nr_scanned)\r\n{\r\nstruct scan_control sc = {\r\n.nr_to_reclaim = SWAP_CLUSTER_MAX,\r\n.target_mem_cgroup = memcg,\r\n.may_writepage = !laptop_mode,\r\n.may_unmap = 1,\r\n.may_swap = !noswap,\r\n};\r\nstruct lruvec *lruvec = mem_cgroup_zone_lruvec(zone, memcg);\r\nint swappiness = mem_cgroup_swappiness(memcg);\r\nunsigned long lru_pages;\r\nsc.gfp_mask = (gfp_mask & GFP_RECLAIM_MASK) |\r\n(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK);\r\ntrace_mm_vmscan_memcg_softlimit_reclaim_begin(sc.order,\r\nsc.may_writepage,\r\nsc.gfp_mask);\r\nshrink_lruvec(lruvec, swappiness, &sc, &lru_pages);\r\ntrace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);\r\n*nr_scanned = sc.nr_scanned;\r\nreturn sc.nr_reclaimed;\r\n}\r\nunsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\r\nunsigned long nr_pages,\r\ngfp_t gfp_mask,\r\nbool may_swap)\r\n{\r\nstruct zonelist *zonelist;\r\nunsigned long nr_reclaimed;\r\nint nid;\r\nstruct scan_control sc = {\r\n.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),\r\n.gfp_mask = (gfp_mask & GFP_RECLAIM_MASK) |\r\n(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK),\r\n.target_mem_cgroup = memcg,\r\n.priority = DEF_PRIORITY,\r\n.may_writepage = !laptop_mode,\r\n.may_unmap = 1,\r\n.may_swap = may_swap,\r\n};\r\nnid = mem_cgroup_select_victim_node(memcg);\r\nzonelist = NODE_DATA(nid)->node_zonelists;\r\ntrace_mm_vmscan_memcg_reclaim_begin(0,\r\nsc.may_writepage,\r\nsc.gfp_mask);\r\nnr_reclaimed = do_try_to_free_pages(zonelist, &sc);\r\ntrace_mm_vmscan_memcg_reclaim_end(nr_reclaimed);\r\nreturn nr_reclaimed;\r\n}\r\nstatic void age_active_anon(struct zone *zone, struct scan_control *sc)\r\n{\r\nstruct mem_cgroup *memcg;\r\nif (!total_swap_pages)\r\nreturn;\r\nmemcg = mem_cgroup_iter(NULL, NULL, NULL);\r\ndo {\r\nstruct lruvec *lruvec = mem_cgroup_zone_lruvec(zone, memcg);\r\nif (inactive_anon_is_low(lruvec))\r\nshrink_active_list(SWAP_CLUSTER_MAX, lruvec,\r\nsc, LRU_ACTIVE_ANON);\r\nmemcg = mem_cgroup_iter(NULL, memcg, NULL);\r\n} while (memcg);\r\n}\r\nstatic bool zone_balanced(struct zone *zone, int order,\r\nunsigned long balance_gap, int classzone_idx)\r\n{\r\nif (!zone_watermark_ok_safe(zone, order, high_wmark_pages(zone) +\r\nbalance_gap, classzone_idx, 0))\r\nreturn false;\r\nif (IS_ENABLED(CONFIG_COMPACTION) && order && compaction_suitable(zone,\r\norder, 0, classzone_idx) == COMPACT_SKIPPED)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic bool pgdat_balanced(pg_data_t *pgdat, int order, int classzone_idx)\r\n{\r\nunsigned long managed_pages = 0;\r\nunsigned long balanced_pages = 0;\r\nint i;\r\nfor (i = 0; i <= classzone_idx; i++) {\r\nstruct zone *zone = pgdat->node_zones + i;\r\nif (!populated_zone(zone))\r\ncontinue;\r\nmanaged_pages += zone->managed_pages;\r\nif (!zone_reclaimable(zone)) {\r\nbalanced_pages += zone->managed_pages;\r\ncontinue;\r\n}\r\nif (zone_balanced(zone, order, 0, i))\r\nbalanced_pages += zone->managed_pages;\r\nelse if (!order)\r\nreturn false;\r\n}\r\nif (order)\r\nreturn balanced_pages >= (managed_pages >> 2);\r\nelse\r\nreturn true;\r\n}\r\nstatic bool prepare_kswapd_sleep(pg_data_t *pgdat, int order, long remaining,\r\nint classzone_idx)\r\n{\r\nif (remaining)\r\nreturn false;\r\nif (waitqueue_active(&pgdat->pfmemalloc_wait))\r\nwake_up_all(&pgdat->pfmemalloc_wait);\r\nreturn pgdat_balanced(pgdat, order, classzone_idx);\r\n}\r\nstatic bool kswapd_shrink_zone(struct zone *zone,\r\nint classzone_idx,\r\nstruct scan_control *sc,\r\nunsigned long *nr_attempted)\r\n{\r\nint testorder = sc->order;\r\nunsigned long balance_gap;\r\nbool lowmem_pressure;\r\nsc->nr_to_reclaim = max(SWAP_CLUSTER_MAX, high_wmark_pages(zone));\r\nif (IS_ENABLED(CONFIG_COMPACTION) && sc->order &&\r\ncompaction_suitable(zone, sc->order, 0, classzone_idx)\r\n!= COMPACT_SKIPPED)\r\ntestorder = 0;\r\nbalance_gap = min(low_wmark_pages(zone), DIV_ROUND_UP(\r\nzone->managed_pages, KSWAPD_ZONE_BALANCE_GAP_RATIO));\r\nlowmem_pressure = (buffer_heads_over_limit && is_highmem(zone));\r\nif (!lowmem_pressure && zone_balanced(zone, testorder,\r\nbalance_gap, classzone_idx))\r\nreturn true;\r\nshrink_zone(zone, sc, zone_idx(zone) == classzone_idx);\r\n*nr_attempted += sc->nr_to_reclaim;\r\nclear_bit(ZONE_WRITEBACK, &zone->flags);\r\nif (zone_reclaimable(zone) &&\r\nzone_balanced(zone, testorder, 0, classzone_idx)) {\r\nclear_bit(ZONE_CONGESTED, &zone->flags);\r\nclear_bit(ZONE_DIRTY, &zone->flags);\r\n}\r\nreturn sc->nr_scanned >= sc->nr_to_reclaim;\r\n}\r\nstatic unsigned long balance_pgdat(pg_data_t *pgdat, int order,\r\nint *classzone_idx)\r\n{\r\nint i;\r\nint end_zone = 0;\r\nunsigned long nr_soft_reclaimed;\r\nunsigned long nr_soft_scanned;\r\nstruct scan_control sc = {\r\n.gfp_mask = GFP_KERNEL,\r\n.order = order,\r\n.priority = DEF_PRIORITY,\r\n.may_writepage = !laptop_mode,\r\n.may_unmap = 1,\r\n.may_swap = 1,\r\n};\r\ncount_vm_event(PAGEOUTRUN);\r\ndo {\r\nunsigned long nr_attempted = 0;\r\nbool raise_priority = true;\r\nbool pgdat_needs_compaction = (order > 0);\r\nsc.nr_reclaimed = 0;\r\nfor (i = pgdat->nr_zones - 1; i >= 0; i--) {\r\nstruct zone *zone = pgdat->node_zones + i;\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (sc.priority != DEF_PRIORITY &&\r\n!zone_reclaimable(zone))\r\ncontinue;\r\nage_active_anon(zone, &sc);\r\nif (buffer_heads_over_limit && is_highmem_idx(i)) {\r\nend_zone = i;\r\nbreak;\r\n}\r\nif (!zone_balanced(zone, order, 0, 0)) {\r\nend_zone = i;\r\nbreak;\r\n} else {\r\nclear_bit(ZONE_CONGESTED, &zone->flags);\r\nclear_bit(ZONE_DIRTY, &zone->flags);\r\n}\r\n}\r\nif (i < 0)\r\ngoto out;\r\nfor (i = 0; i <= end_zone; i++) {\r\nstruct zone *zone = pgdat->node_zones + i;\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (pgdat_needs_compaction &&\r\nzone_watermark_ok(zone, order,\r\nlow_wmark_pages(zone),\r\n*classzone_idx, 0))\r\npgdat_needs_compaction = false;\r\n}\r\nif (sc.priority < DEF_PRIORITY - 2)\r\nsc.may_writepage = 1;\r\nfor (i = 0; i <= end_zone; i++) {\r\nstruct zone *zone = pgdat->node_zones + i;\r\nif (!populated_zone(zone))\r\ncontinue;\r\nif (sc.priority != DEF_PRIORITY &&\r\n!zone_reclaimable(zone))\r\ncontinue;\r\nsc.nr_scanned = 0;\r\nnr_soft_scanned = 0;\r\nnr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone,\r\norder, sc.gfp_mask,\r\n&nr_soft_scanned);\r\nsc.nr_reclaimed += nr_soft_reclaimed;\r\nif (kswapd_shrink_zone(zone, end_zone,\r\n&sc, &nr_attempted))\r\nraise_priority = false;\r\n}\r\nif (waitqueue_active(&pgdat->pfmemalloc_wait) &&\r\npfmemalloc_watermark_ok(pgdat))\r\nwake_up_all(&pgdat->pfmemalloc_wait);\r\nif (order && sc.nr_reclaimed >= 2UL << order)\r\norder = sc.order = 0;\r\nif (try_to_freeze() || kthread_should_stop())\r\nbreak;\r\nif (pgdat_needs_compaction && sc.nr_reclaimed > nr_attempted)\r\ncompact_pgdat(pgdat, order);\r\nif (raise_priority || !sc.nr_reclaimed)\r\nsc.priority--;\r\n} while (sc.priority >= 1 &&\r\n!pgdat_balanced(pgdat, order, *classzone_idx));\r\nout:\r\n*classzone_idx = end_zone;\r\nreturn order;\r\n}\r\nstatic void kswapd_try_to_sleep(pg_data_t *pgdat, int order, int classzone_idx)\r\n{\r\nlong remaining = 0;\r\nDEFINE_WAIT(wait);\r\nif (freezing(current) || kthread_should_stop())\r\nreturn;\r\nprepare_to_wait(&pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE);\r\nif (prepare_kswapd_sleep(pgdat, order, remaining, classzone_idx)) {\r\nremaining = schedule_timeout(HZ/10);\r\nfinish_wait(&pgdat->kswapd_wait, &wait);\r\nprepare_to_wait(&pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE);\r\n}\r\nif (prepare_kswapd_sleep(pgdat, order, remaining, classzone_idx)) {\r\ntrace_mm_vmscan_kswapd_sleep(pgdat->node_id);\r\nset_pgdat_percpu_threshold(pgdat, calculate_normal_threshold);\r\nreset_isolation_suitable(pgdat);\r\nif (!kthread_should_stop())\r\nschedule();\r\nset_pgdat_percpu_threshold(pgdat, calculate_pressure_threshold);\r\n} else {\r\nif (remaining)\r\ncount_vm_event(KSWAPD_LOW_WMARK_HIT_QUICKLY);\r\nelse\r\ncount_vm_event(KSWAPD_HIGH_WMARK_HIT_QUICKLY);\r\n}\r\nfinish_wait(&pgdat->kswapd_wait, &wait);\r\n}\r\nstatic int kswapd(void *p)\r\n{\r\nunsigned long order, new_order;\r\nunsigned balanced_order;\r\nint classzone_idx, new_classzone_idx;\r\nint balanced_classzone_idx;\r\npg_data_t *pgdat = (pg_data_t*)p;\r\nstruct task_struct *tsk = current;\r\nstruct reclaim_state reclaim_state = {\r\n.reclaimed_slab = 0,\r\n};\r\nconst struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);\r\nlockdep_set_current_reclaim_state(GFP_KERNEL);\r\nif (!cpumask_empty(cpumask))\r\nset_cpus_allowed_ptr(tsk, cpumask);\r\ncurrent->reclaim_state = &reclaim_state;\r\ntsk->flags |= PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD;\r\nset_freezable();\r\norder = new_order = 0;\r\nbalanced_order = 0;\r\nclasszone_idx = new_classzone_idx = pgdat->nr_zones - 1;\r\nbalanced_classzone_idx = classzone_idx;\r\nfor ( ; ; ) {\r\nbool ret;\r\nif (balanced_classzone_idx >= new_classzone_idx &&\r\nbalanced_order == new_order) {\r\nnew_order = pgdat->kswapd_max_order;\r\nnew_classzone_idx = pgdat->classzone_idx;\r\npgdat->kswapd_max_order = 0;\r\npgdat->classzone_idx = pgdat->nr_zones - 1;\r\n}\r\nif (order < new_order || classzone_idx > new_classzone_idx) {\r\norder = new_order;\r\nclasszone_idx = new_classzone_idx;\r\n} else {\r\nkswapd_try_to_sleep(pgdat, balanced_order,\r\nbalanced_classzone_idx);\r\norder = pgdat->kswapd_max_order;\r\nclasszone_idx = pgdat->classzone_idx;\r\nnew_order = order;\r\nnew_classzone_idx = classzone_idx;\r\npgdat->kswapd_max_order = 0;\r\npgdat->classzone_idx = pgdat->nr_zones - 1;\r\n}\r\nret = try_to_freeze();\r\nif (kthread_should_stop())\r\nbreak;\r\nif (!ret) {\r\ntrace_mm_vmscan_kswapd_wake(pgdat->node_id, order);\r\nbalanced_classzone_idx = classzone_idx;\r\nbalanced_order = balance_pgdat(pgdat, order,\r\n&balanced_classzone_idx);\r\n}\r\n}\r\ntsk->flags &= ~(PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD);\r\ncurrent->reclaim_state = NULL;\r\nlockdep_clear_current_reclaim_state();\r\nreturn 0;\r\n}\r\nvoid wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx)\r\n{\r\npg_data_t *pgdat;\r\nif (!populated_zone(zone))\r\nreturn;\r\nif (!cpuset_zone_allowed(zone, GFP_KERNEL | __GFP_HARDWALL))\r\nreturn;\r\npgdat = zone->zone_pgdat;\r\nif (pgdat->kswapd_max_order < order) {\r\npgdat->kswapd_max_order = order;\r\npgdat->classzone_idx = min(pgdat->classzone_idx, classzone_idx);\r\n}\r\nif (!waitqueue_active(&pgdat->kswapd_wait))\r\nreturn;\r\nif (zone_balanced(zone, order, 0, 0))\r\nreturn;\r\ntrace_mm_vmscan_wakeup_kswapd(pgdat->node_id, zone_idx(zone), order);\r\nwake_up_interruptible(&pgdat->kswapd_wait);\r\n}\r\nunsigned long shrink_all_memory(unsigned long nr_to_reclaim)\r\n{\r\nstruct reclaim_state reclaim_state;\r\nstruct scan_control sc = {\r\n.nr_to_reclaim = nr_to_reclaim,\r\n.gfp_mask = GFP_HIGHUSER_MOVABLE,\r\n.priority = DEF_PRIORITY,\r\n.may_writepage = 1,\r\n.may_unmap = 1,\r\n.may_swap = 1,\r\n.hibernation_mode = 1,\r\n};\r\nstruct zonelist *zonelist = node_zonelist(numa_node_id(), sc.gfp_mask);\r\nstruct task_struct *p = current;\r\nunsigned long nr_reclaimed;\r\np->flags |= PF_MEMALLOC;\r\nlockdep_set_current_reclaim_state(sc.gfp_mask);\r\nreclaim_state.reclaimed_slab = 0;\r\np->reclaim_state = &reclaim_state;\r\nnr_reclaimed = do_try_to_free_pages(zonelist, &sc);\r\np->reclaim_state = NULL;\r\nlockdep_clear_current_reclaim_state();\r\np->flags &= ~PF_MEMALLOC;\r\nreturn nr_reclaimed;\r\n}\r\nstatic int cpu_callback(struct notifier_block *nfb, unsigned long action,\r\nvoid *hcpu)\r\n{\r\nint nid;\r\nif (action == CPU_ONLINE || action == CPU_ONLINE_FROZEN) {\r\nfor_each_node_state(nid, N_MEMORY) {\r\npg_data_t *pgdat = NODE_DATA(nid);\r\nconst struct cpumask *mask;\r\nmask = cpumask_of_node(pgdat->node_id);\r\nif (cpumask_any_and(cpu_online_mask, mask) < nr_cpu_ids)\r\nset_cpus_allowed_ptr(pgdat->kswapd, mask);\r\n}\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nint kswapd_run(int nid)\r\n{\r\npg_data_t *pgdat = NODE_DATA(nid);\r\nint ret = 0;\r\nif (pgdat->kswapd)\r\nreturn 0;\r\npgdat->kswapd = kthread_run(kswapd, pgdat, "kswapd%d", nid);\r\nif (IS_ERR(pgdat->kswapd)) {\r\nBUG_ON(system_state == SYSTEM_BOOTING);\r\npr_err("Failed to start kswapd on node %d\n", nid);\r\nret = PTR_ERR(pgdat->kswapd);\r\npgdat->kswapd = NULL;\r\n}\r\nreturn ret;\r\n}\r\nvoid kswapd_stop(int nid)\r\n{\r\nstruct task_struct *kswapd = NODE_DATA(nid)->kswapd;\r\nif (kswapd) {\r\nkthread_stop(kswapd);\r\nNODE_DATA(nid)->kswapd = NULL;\r\n}\r\n}\r\nstatic int __init kswapd_init(void)\r\n{\r\nint nid;\r\nswap_setup();\r\nfor_each_node_state(nid, N_MEMORY)\r\nkswapd_run(nid);\r\nhotcpu_notifier(cpu_callback, 0);\r\nreturn 0;\r\n}\r\nstatic inline unsigned long zone_unmapped_file_pages(struct zone *zone)\r\n{\r\nunsigned long file_mapped = zone_page_state(zone, NR_FILE_MAPPED);\r\nunsigned long file_lru = zone_page_state(zone, NR_INACTIVE_FILE) +\r\nzone_page_state(zone, NR_ACTIVE_FILE);\r\nreturn (file_lru > file_mapped) ? (file_lru - file_mapped) : 0;\r\n}\r\nstatic long zone_pagecache_reclaimable(struct zone *zone)\r\n{\r\nlong nr_pagecache_reclaimable;\r\nlong delta = 0;\r\nif (zone_reclaim_mode & RECLAIM_SWAP)\r\nnr_pagecache_reclaimable = zone_page_state(zone, NR_FILE_PAGES);\r\nelse\r\nnr_pagecache_reclaimable = zone_unmapped_file_pages(zone);\r\nif (!(zone_reclaim_mode & RECLAIM_WRITE))\r\ndelta += zone_page_state(zone, NR_FILE_DIRTY);\r\nif (unlikely(delta > nr_pagecache_reclaimable))\r\ndelta = nr_pagecache_reclaimable;\r\nreturn nr_pagecache_reclaimable - delta;\r\n}\r\nstatic int __zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)\r\n{\r\nconst unsigned long nr_pages = 1 << order;\r\nstruct task_struct *p = current;\r\nstruct reclaim_state reclaim_state;\r\nstruct scan_control sc = {\r\n.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),\r\n.gfp_mask = (gfp_mask = memalloc_noio_flags(gfp_mask)),\r\n.order = order,\r\n.priority = ZONE_RECLAIM_PRIORITY,\r\n.may_writepage = !!(zone_reclaim_mode & RECLAIM_WRITE),\r\n.may_unmap = !!(zone_reclaim_mode & RECLAIM_SWAP),\r\n.may_swap = 1,\r\n};\r\ncond_resched();\r\np->flags |= PF_MEMALLOC | PF_SWAPWRITE;\r\nlockdep_set_current_reclaim_state(gfp_mask);\r\nreclaim_state.reclaimed_slab = 0;\r\np->reclaim_state = &reclaim_state;\r\nif (zone_pagecache_reclaimable(zone) > zone->min_unmapped_pages) {\r\ndo {\r\nshrink_zone(zone, &sc, true);\r\n} while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);\r\n}\r\np->reclaim_state = NULL;\r\ncurrent->flags &= ~(PF_MEMALLOC | PF_SWAPWRITE);\r\nlockdep_clear_current_reclaim_state();\r\nreturn sc.nr_reclaimed >= nr_pages;\r\n}\r\nint zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)\r\n{\r\nint node_id;\r\nint ret;\r\nif (zone_pagecache_reclaimable(zone) <= zone->min_unmapped_pages &&\r\nzone_page_state(zone, NR_SLAB_RECLAIMABLE) <= zone->min_slab_pages)\r\nreturn ZONE_RECLAIM_FULL;\r\nif (!zone_reclaimable(zone))\r\nreturn ZONE_RECLAIM_FULL;\r\nif (!(gfp_mask & __GFP_WAIT) || (current->flags & PF_MEMALLOC))\r\nreturn ZONE_RECLAIM_NOSCAN;\r\nnode_id = zone_to_nid(zone);\r\nif (node_state(node_id, N_CPU) && node_id != numa_node_id())\r\nreturn ZONE_RECLAIM_NOSCAN;\r\nif (test_and_set_bit(ZONE_RECLAIM_LOCKED, &zone->flags))\r\nreturn ZONE_RECLAIM_NOSCAN;\r\nret = __zone_reclaim(zone, gfp_mask, order);\r\nclear_bit(ZONE_RECLAIM_LOCKED, &zone->flags);\r\nif (!ret)\r\ncount_vm_event(PGSCAN_ZONE_RECLAIM_FAILED);\r\nreturn ret;\r\n}\r\nint page_evictable(struct page *page)\r\n{\r\nreturn !mapping_unevictable(page_mapping(page)) && !PageMlocked(page);\r\n}\r\nvoid check_move_unevictable_pages(struct page **pages, int nr_pages)\r\n{\r\nstruct lruvec *lruvec;\r\nstruct zone *zone = NULL;\r\nint pgscanned = 0;\r\nint pgrescued = 0;\r\nint i;\r\nfor (i = 0; i < nr_pages; i++) {\r\nstruct page *page = pages[i];\r\nstruct zone *pagezone;\r\npgscanned++;\r\npagezone = page_zone(page);\r\nif (pagezone != zone) {\r\nif (zone)\r\nspin_unlock_irq(&zone->lru_lock);\r\nzone = pagezone;\r\nspin_lock_irq(&zone->lru_lock);\r\n}\r\nlruvec = mem_cgroup_page_lruvec(page, zone);\r\nif (!PageLRU(page) || !PageUnevictable(page))\r\ncontinue;\r\nif (page_evictable(page)) {\r\nenum lru_list lru = page_lru_base_type(page);\r\nVM_BUG_ON_PAGE(PageActive(page), page);\r\nClearPageUnevictable(page);\r\ndel_page_from_lru_list(page, lruvec, LRU_UNEVICTABLE);\r\nadd_page_to_lru_list(page, lruvec, lru);\r\npgrescued++;\r\n}\r\n}\r\nif (zone) {\r\n__count_vm_events(UNEVICTABLE_PGRESCUED, pgrescued);\r\n__count_vm_events(UNEVICTABLE_PGSCANNED, pgscanned);\r\nspin_unlock_irq(&zone->lru_lock);\r\n}\r\n}
