static void\r\nxlog_grant_sub_space(\r\nstruct xlog *log,\r\natomic64_t *head,\r\nint bytes)\r\n{\r\nint64_t head_val = atomic64_read(head);\r\nint64_t new, old;\r\ndo {\r\nint cycle, space;\r\nxlog_crack_grant_head_val(head_val, &cycle, &space);\r\nspace -= bytes;\r\nif (space < 0) {\r\nspace += log->l_logsize;\r\ncycle--;\r\n}\r\nold = head_val;\r\nnew = xlog_assign_grant_head_val(cycle, space);\r\nhead_val = atomic64_cmpxchg(head, old, new);\r\n} while (head_val != old);\r\n}\r\nstatic void\r\nxlog_grant_add_space(\r\nstruct xlog *log,\r\natomic64_t *head,\r\nint bytes)\r\n{\r\nint64_t head_val = atomic64_read(head);\r\nint64_t new, old;\r\ndo {\r\nint tmp;\r\nint cycle, space;\r\nxlog_crack_grant_head_val(head_val, &cycle, &space);\r\ntmp = log->l_logsize - space;\r\nif (tmp > bytes)\r\nspace += bytes;\r\nelse {\r\nspace = bytes - tmp;\r\ncycle++;\r\n}\r\nold = head_val;\r\nnew = xlog_assign_grant_head_val(cycle, space);\r\nhead_val = atomic64_cmpxchg(head, old, new);\r\n} while (head_val != old);\r\n}\r\nSTATIC void\r\nxlog_grant_head_init(\r\nstruct xlog_grant_head *head)\r\n{\r\nxlog_assign_grant_head(&head->grant, 1, 0);\r\nINIT_LIST_HEAD(&head->waiters);\r\nspin_lock_init(&head->lock);\r\n}\r\nSTATIC void\r\nxlog_grant_head_wake_all(\r\nstruct xlog_grant_head *head)\r\n{\r\nstruct xlog_ticket *tic;\r\nspin_lock(&head->lock);\r\nlist_for_each_entry(tic, &head->waiters, t_queue)\r\nwake_up_process(tic->t_task);\r\nspin_unlock(&head->lock);\r\n}\r\nstatic inline int\r\nxlog_ticket_reservation(\r\nstruct xlog *log,\r\nstruct xlog_grant_head *head,\r\nstruct xlog_ticket *tic)\r\n{\r\nif (head == &log->l_write_head) {\r\nASSERT(tic->t_flags & XLOG_TIC_PERM_RESERV);\r\nreturn tic->t_unit_res;\r\n} else {\r\nif (tic->t_flags & XLOG_TIC_PERM_RESERV)\r\nreturn tic->t_unit_res * tic->t_cnt;\r\nelse\r\nreturn tic->t_unit_res;\r\n}\r\n}\r\nSTATIC bool\r\nxlog_grant_head_wake(\r\nstruct xlog *log,\r\nstruct xlog_grant_head *head,\r\nint *free_bytes)\r\n{\r\nstruct xlog_ticket *tic;\r\nint need_bytes;\r\nlist_for_each_entry(tic, &head->waiters, t_queue) {\r\nneed_bytes = xlog_ticket_reservation(log, head, tic);\r\nif (*free_bytes < need_bytes)\r\nreturn false;\r\n*free_bytes -= need_bytes;\r\ntrace_xfs_log_grant_wake_up(log, tic);\r\nwake_up_process(tic->t_task);\r\n}\r\nreturn true;\r\n}\r\nSTATIC int\r\nxlog_grant_head_wait(\r\nstruct xlog *log,\r\nstruct xlog_grant_head *head,\r\nstruct xlog_ticket *tic,\r\nint need_bytes) __releases(&head->lock\r\nSTATIC int\r\nxlog_grant_head_check(\r\nstruct xlog *log,\r\nstruct xlog_grant_head *head,\r\nstruct xlog_ticket *tic,\r\nint *need_bytes)\r\n{\r\nint free_bytes;\r\nint error = 0;\r\nASSERT(!(log->l_flags & XLOG_ACTIVE_RECOVERY));\r\n*need_bytes = xlog_ticket_reservation(log, head, tic);\r\nfree_bytes = xlog_space_left(log, &head->grant);\r\nif (!list_empty_careful(&head->waiters)) {\r\nspin_lock(&head->lock);\r\nif (!xlog_grant_head_wake(log, head, &free_bytes) ||\r\nfree_bytes < *need_bytes) {\r\nerror = xlog_grant_head_wait(log, head, tic,\r\n*need_bytes);\r\n}\r\nspin_unlock(&head->lock);\r\n} else if (free_bytes < *need_bytes) {\r\nspin_lock(&head->lock);\r\nerror = xlog_grant_head_wait(log, head, tic, *need_bytes);\r\nspin_unlock(&head->lock);\r\n}\r\nreturn error;\r\n}\r\nstatic void\r\nxlog_tic_reset_res(xlog_ticket_t *tic)\r\n{\r\ntic->t_res_num = 0;\r\ntic->t_res_arr_sum = 0;\r\ntic->t_res_num_ophdrs = 0;\r\n}\r\nstatic void\r\nxlog_tic_add_region(xlog_ticket_t *tic, uint len, uint type)\r\n{\r\nif (tic->t_res_num == XLOG_TIC_LEN_MAX) {\r\ntic->t_res_o_flow += tic->t_res_arr_sum;\r\ntic->t_res_num = 0;\r\ntic->t_res_arr_sum = 0;\r\n}\r\ntic->t_res_arr[tic->t_res_num].r_len = len;\r\ntic->t_res_arr[tic->t_res_num].r_type = type;\r\ntic->t_res_arr_sum += len;\r\ntic->t_res_num++;\r\n}\r\nint\r\nxfs_log_regrant(\r\nstruct xfs_mount *mp,\r\nstruct xlog_ticket *tic)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nint need_bytes;\r\nint error = 0;\r\nif (XLOG_FORCED_SHUTDOWN(log))\r\nreturn -EIO;\r\nXFS_STATS_INC(xs_try_logspace);\r\ntic->t_tid++;\r\nxlog_grant_push_ail(log, tic->t_unit_res);\r\ntic->t_curr_res = tic->t_unit_res;\r\nxlog_tic_reset_res(tic);\r\nif (tic->t_cnt > 0)\r\nreturn 0;\r\ntrace_xfs_log_regrant(log, tic);\r\nerror = xlog_grant_head_check(log, &log->l_write_head, tic,\r\n&need_bytes);\r\nif (error)\r\ngoto out_error;\r\nxlog_grant_add_space(log, &log->l_write_head.grant, need_bytes);\r\ntrace_xfs_log_regrant_exit(log, tic);\r\nxlog_verify_grant_tail(log);\r\nreturn 0;\r\nout_error:\r\ntic->t_curr_res = 0;\r\ntic->t_cnt = 0;\r\nreturn error;\r\n}\r\nint\r\nxfs_log_reserve(\r\nstruct xfs_mount *mp,\r\nint unit_bytes,\r\nint cnt,\r\nstruct xlog_ticket **ticp,\r\n__uint8_t client,\r\nbool permanent,\r\nuint t_type)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nstruct xlog_ticket *tic;\r\nint need_bytes;\r\nint error = 0;\r\nASSERT(client == XFS_TRANSACTION || client == XFS_LOG);\r\nif (XLOG_FORCED_SHUTDOWN(log))\r\nreturn -EIO;\r\nXFS_STATS_INC(xs_try_logspace);\r\nASSERT(*ticp == NULL);\r\ntic = xlog_ticket_alloc(log, unit_bytes, cnt, client, permanent,\r\nKM_SLEEP | KM_MAYFAIL);\r\nif (!tic)\r\nreturn -ENOMEM;\r\ntic->t_trans_type = t_type;\r\n*ticp = tic;\r\nxlog_grant_push_ail(log, tic->t_cnt ? tic->t_unit_res * tic->t_cnt\r\n: tic->t_unit_res);\r\ntrace_xfs_log_reserve(log, tic);\r\nerror = xlog_grant_head_check(log, &log->l_reserve_head, tic,\r\n&need_bytes);\r\nif (error)\r\ngoto out_error;\r\nxlog_grant_add_space(log, &log->l_reserve_head.grant, need_bytes);\r\nxlog_grant_add_space(log, &log->l_write_head.grant, need_bytes);\r\ntrace_xfs_log_reserve_exit(log, tic);\r\nxlog_verify_grant_tail(log);\r\nreturn 0;\r\nout_error:\r\ntic->t_curr_res = 0;\r\ntic->t_cnt = 0;\r\nreturn error;\r\n}\r\nxfs_lsn_t\r\nxfs_log_done(\r\nstruct xfs_mount *mp,\r\nstruct xlog_ticket *ticket,\r\nstruct xlog_in_core **iclog,\r\nuint flags)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nxfs_lsn_t lsn = 0;\r\nif (XLOG_FORCED_SHUTDOWN(log) ||\r\n(((ticket->t_flags & XLOG_TIC_INITED) == 0) &&\r\n(xlog_commit_record(log, ticket, iclog, &lsn)))) {\r\nlsn = (xfs_lsn_t) -1;\r\nif (ticket->t_flags & XLOG_TIC_PERM_RESERV) {\r\nflags |= XFS_LOG_REL_PERM_RESERV;\r\n}\r\n}\r\nif ((ticket->t_flags & XLOG_TIC_PERM_RESERV) == 0 ||\r\n(flags & XFS_LOG_REL_PERM_RESERV)) {\r\ntrace_xfs_log_done_nonperm(log, ticket);\r\nxlog_ungrant_log_space(log, ticket);\r\nxfs_log_ticket_put(ticket);\r\n} else {\r\ntrace_xfs_log_done_perm(log, ticket);\r\nxlog_regrant_reserve_log_space(log, ticket);\r\nticket->t_flags |= XLOG_TIC_INITED;\r\n}\r\nreturn lsn;\r\n}\r\nint\r\nxfs_log_notify(\r\nstruct xfs_mount *mp,\r\nstruct xlog_in_core *iclog,\r\nxfs_log_callback_t *cb)\r\n{\r\nint abortflg;\r\nspin_lock(&iclog->ic_callback_lock);\r\nabortflg = (iclog->ic_state & XLOG_STATE_IOERROR);\r\nif (!abortflg) {\r\nASSERT_ALWAYS((iclog->ic_state == XLOG_STATE_ACTIVE) ||\r\n(iclog->ic_state == XLOG_STATE_WANT_SYNC));\r\ncb->cb_next = NULL;\r\n*(iclog->ic_callback_tail) = cb;\r\niclog->ic_callback_tail = &(cb->cb_next);\r\n}\r\nspin_unlock(&iclog->ic_callback_lock);\r\nreturn abortflg;\r\n}\r\nint\r\nxfs_log_release_iclog(\r\nstruct xfs_mount *mp,\r\nstruct xlog_in_core *iclog)\r\n{\r\nif (xlog_state_release_iclog(mp->m_log, iclog)) {\r\nxfs_force_shutdown(mp, SHUTDOWN_LOG_IO_ERROR);\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nint\r\nxfs_log_mount(\r\nxfs_mount_t *mp,\r\nxfs_buftarg_t *log_target,\r\nxfs_daddr_t blk_offset,\r\nint num_bblks)\r\n{\r\nint error = 0;\r\nint min_logfsbs;\r\nif (!(mp->m_flags & XFS_MOUNT_NORECOVERY)) {\r\nxfs_notice(mp, "Mounting V%d Filesystem",\r\nXFS_SB_VERSION_NUM(&mp->m_sb));\r\n} else {\r\nxfs_notice(mp,\r\n"Mounting V%d filesystem in no-recovery mode. Filesystem will be inconsistent.",\r\nXFS_SB_VERSION_NUM(&mp->m_sb));\r\nASSERT(mp->m_flags & XFS_MOUNT_RDONLY);\r\n}\r\nmp->m_log = xlog_alloc_log(mp, log_target, blk_offset, num_bblks);\r\nif (IS_ERR(mp->m_log)) {\r\nerror = PTR_ERR(mp->m_log);\r\ngoto out;\r\n}\r\nmin_logfsbs = xfs_log_calc_minimum_size(mp);\r\nif (mp->m_sb.sb_logblocks < min_logfsbs) {\r\nxfs_warn(mp,\r\n"Log size %d blocks too small, minimum size is %d blocks",\r\nmp->m_sb.sb_logblocks, min_logfsbs);\r\nerror = -EINVAL;\r\n} else if (mp->m_sb.sb_logblocks > XFS_MAX_LOG_BLOCKS) {\r\nxfs_warn(mp,\r\n"Log size %d blocks too large, maximum size is %lld blocks",\r\nmp->m_sb.sb_logblocks, XFS_MAX_LOG_BLOCKS);\r\nerror = -EINVAL;\r\n} else if (XFS_FSB_TO_B(mp, mp->m_sb.sb_logblocks) > XFS_MAX_LOG_BYTES) {\r\nxfs_warn(mp,\r\n"log size %lld bytes too large, maximum size is %lld bytes",\r\nXFS_FSB_TO_B(mp, mp->m_sb.sb_logblocks),\r\nXFS_MAX_LOG_BYTES);\r\nerror = -EINVAL;\r\n}\r\nif (error) {\r\nif (xfs_sb_version_hascrc(&mp->m_sb)) {\r\nxfs_crit(mp, "AAIEEE! Log failed size checks. Abort!");\r\nASSERT(0);\r\ngoto out_free_log;\r\n}\r\nxfs_crit(mp,\r\n"Log size out of supported range. Continuing onwards, but if log hangs are\n"\r\n"experienced then please report this message in the bug report.");\r\n}\r\nerror = xfs_trans_ail_init(mp);\r\nif (error) {\r\nxfs_warn(mp, "AIL initialisation failed: error %d", error);\r\ngoto out_free_log;\r\n}\r\nmp->m_log->l_ailp = mp->m_ail;\r\nif (!(mp->m_flags & XFS_MOUNT_NORECOVERY)) {\r\nint readonly = (mp->m_flags & XFS_MOUNT_RDONLY);\r\nif (readonly)\r\nmp->m_flags &= ~XFS_MOUNT_RDONLY;\r\nerror = xlog_recover(mp->m_log);\r\nif (readonly)\r\nmp->m_flags |= XFS_MOUNT_RDONLY;\r\nif (error) {\r\nxfs_warn(mp, "log mount/recovery failed: error %d",\r\nerror);\r\ngoto out_destroy_ail;\r\n}\r\n}\r\nerror = xfs_sysfs_init(&mp->m_log->l_kobj, &xfs_log_ktype, &mp->m_kobj,\r\n"log");\r\nif (error)\r\ngoto out_destroy_ail;\r\nmp->m_log->l_flags &= ~XLOG_ACTIVE_RECOVERY;\r\nxlog_cil_init_post_recovery(mp->m_log);\r\nreturn 0;\r\nout_destroy_ail:\r\nxfs_trans_ail_destroy(mp);\r\nout_free_log:\r\nxlog_dealloc_log(mp->m_log);\r\nout:\r\nreturn error;\r\n}\r\nint\r\nxfs_log_mount_finish(xfs_mount_t *mp)\r\n{\r\nint error = 0;\r\nif (!(mp->m_flags & XFS_MOUNT_NORECOVERY)) {\r\nerror = xlog_recover_finish(mp->m_log);\r\nif (!error)\r\nxfs_log_work_queue(mp);\r\n} else {\r\nASSERT(mp->m_flags & XFS_MOUNT_RDONLY);\r\n}\r\nreturn error;\r\n}\r\nint\r\nxfs_log_unmount_write(xfs_mount_t *mp)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nxlog_in_core_t *iclog;\r\n#ifdef DEBUG\r\nxlog_in_core_t *first_iclog;\r\n#endif\r\nxlog_ticket_t *tic = NULL;\r\nxfs_lsn_t lsn;\r\nint error;\r\nif (mp->m_flags & XFS_MOUNT_RDONLY)\r\nreturn 0;\r\nerror = _xfs_log_force(mp, XFS_LOG_SYNC, NULL);\r\nASSERT(error || !(XLOG_FORCED_SHUTDOWN(log)));\r\n#ifdef DEBUG\r\nfirst_iclog = iclog = log->l_iclog;\r\ndo {\r\nif (!(iclog->ic_state & XLOG_STATE_IOERROR)) {\r\nASSERT(iclog->ic_state & XLOG_STATE_ACTIVE);\r\nASSERT(iclog->ic_offset == 0);\r\n}\r\niclog = iclog->ic_next;\r\n} while (iclog != first_iclog);\r\n#endif\r\nif (! (XLOG_FORCED_SHUTDOWN(log))) {\r\nerror = xfs_log_reserve(mp, 600, 1, &tic,\r\nXFS_LOG, 0, XLOG_UNMOUNT_REC_TYPE);\r\nif (!error) {\r\nstruct {\r\n__uint16_t magic;\r\n__uint16_t pad1;\r\n__uint32_t pad2;\r\n} magic = {\r\n.magic = XLOG_UNMOUNT_TYPE,\r\n};\r\nstruct xfs_log_iovec reg = {\r\n.i_addr = &magic,\r\n.i_len = sizeof(magic),\r\n.i_type = XLOG_REG_TYPE_UNMOUNT,\r\n};\r\nstruct xfs_log_vec vec = {\r\n.lv_niovecs = 1,\r\n.lv_iovecp = &reg,\r\n};\r\ntic->t_flags = 0;\r\ntic->t_curr_res -= sizeof(magic);\r\nerror = xlog_write(log, &vec, tic, &lsn,\r\nNULL, XLOG_UNMOUNT_TRANS);\r\n}\r\nif (error)\r\nxfs_alert(mp, "%s: unmount record failed", __func__);\r\nspin_lock(&log->l_icloglock);\r\niclog = log->l_iclog;\r\natomic_inc(&iclog->ic_refcnt);\r\nxlog_state_want_sync(log, iclog);\r\nspin_unlock(&log->l_icloglock);\r\nerror = xlog_state_release_iclog(log, iclog);\r\nspin_lock(&log->l_icloglock);\r\nif (!(iclog->ic_state == XLOG_STATE_ACTIVE ||\r\niclog->ic_state == XLOG_STATE_DIRTY)) {\r\nif (!XLOG_FORCED_SHUTDOWN(log)) {\r\nxlog_wait(&iclog->ic_force_wait,\r\n&log->l_icloglock);\r\n} else {\r\nspin_unlock(&log->l_icloglock);\r\n}\r\n} else {\r\nspin_unlock(&log->l_icloglock);\r\n}\r\nif (tic) {\r\ntrace_xfs_log_umount_write(log, tic);\r\nxlog_ungrant_log_space(log, tic);\r\nxfs_log_ticket_put(tic);\r\n}\r\n} else {\r\nspin_lock(&log->l_icloglock);\r\niclog = log->l_iclog;\r\natomic_inc(&iclog->ic_refcnt);\r\nxlog_state_want_sync(log, iclog);\r\nspin_unlock(&log->l_icloglock);\r\nerror = xlog_state_release_iclog(log, iclog);\r\nspin_lock(&log->l_icloglock);\r\nif ( ! ( iclog->ic_state == XLOG_STATE_ACTIVE\r\n|| iclog->ic_state == XLOG_STATE_DIRTY\r\n|| iclog->ic_state == XLOG_STATE_IOERROR) ) {\r\nxlog_wait(&iclog->ic_force_wait,\r\n&log->l_icloglock);\r\n} else {\r\nspin_unlock(&log->l_icloglock);\r\n}\r\n}\r\nreturn error;\r\n}\r\nvoid\r\nxfs_log_quiesce(\r\nstruct xfs_mount *mp)\r\n{\r\ncancel_delayed_work_sync(&mp->m_log->l_work);\r\nxfs_log_force(mp, XFS_LOG_SYNC);\r\nxfs_ail_push_all_sync(mp->m_ail);\r\nxfs_wait_buftarg(mp->m_ddev_targp);\r\nxfs_buf_lock(mp->m_sb_bp);\r\nxfs_buf_unlock(mp->m_sb_bp);\r\nxfs_log_unmount_write(mp);\r\n}\r\nvoid\r\nxfs_log_unmount(\r\nstruct xfs_mount *mp)\r\n{\r\nxfs_log_quiesce(mp);\r\nxfs_trans_ail_destroy(mp);\r\nxfs_sysfs_del(&mp->m_log->l_kobj);\r\nxlog_dealloc_log(mp->m_log);\r\n}\r\nvoid\r\nxfs_log_item_init(\r\nstruct xfs_mount *mp,\r\nstruct xfs_log_item *item,\r\nint type,\r\nconst struct xfs_item_ops *ops)\r\n{\r\nitem->li_mountp = mp;\r\nitem->li_ailp = mp->m_ail;\r\nitem->li_type = type;\r\nitem->li_ops = ops;\r\nitem->li_lv = NULL;\r\nINIT_LIST_HEAD(&item->li_ail);\r\nINIT_LIST_HEAD(&item->li_cil);\r\n}\r\nvoid\r\nxfs_log_space_wake(\r\nstruct xfs_mount *mp)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nint free_bytes;\r\nif (XLOG_FORCED_SHUTDOWN(log))\r\nreturn;\r\nif (!list_empty_careful(&log->l_write_head.waiters)) {\r\nASSERT(!(log->l_flags & XLOG_ACTIVE_RECOVERY));\r\nspin_lock(&log->l_write_head.lock);\r\nfree_bytes = xlog_space_left(log, &log->l_write_head.grant);\r\nxlog_grant_head_wake(log, &log->l_write_head, &free_bytes);\r\nspin_unlock(&log->l_write_head.lock);\r\n}\r\nif (!list_empty_careful(&log->l_reserve_head.waiters)) {\r\nASSERT(!(log->l_flags & XLOG_ACTIVE_RECOVERY));\r\nspin_lock(&log->l_reserve_head.lock);\r\nfree_bytes = xlog_space_left(log, &log->l_reserve_head.grant);\r\nxlog_grant_head_wake(log, &log->l_reserve_head, &free_bytes);\r\nspin_unlock(&log->l_reserve_head.lock);\r\n}\r\n}\r\nint\r\nxfs_log_need_covered(xfs_mount_t *mp)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nint needed = 0;\r\nif (!xfs_fs_writable(mp, SB_FREEZE_WRITE))\r\nreturn 0;\r\nif (!xlog_cil_empty(log))\r\nreturn 0;\r\nspin_lock(&log->l_icloglock);\r\nswitch (log->l_covered_state) {\r\ncase XLOG_STATE_COVER_DONE:\r\ncase XLOG_STATE_COVER_DONE2:\r\ncase XLOG_STATE_COVER_IDLE:\r\nbreak;\r\ncase XLOG_STATE_COVER_NEED:\r\ncase XLOG_STATE_COVER_NEED2:\r\nif (xfs_ail_min_lsn(log->l_ailp))\r\nbreak;\r\nif (!xlog_iclogs_empty(log))\r\nbreak;\r\nneeded = 1;\r\nif (log->l_covered_state == XLOG_STATE_COVER_NEED)\r\nlog->l_covered_state = XLOG_STATE_COVER_DONE;\r\nelse\r\nlog->l_covered_state = XLOG_STATE_COVER_DONE2;\r\nbreak;\r\ndefault:\r\nneeded = 1;\r\nbreak;\r\n}\r\nspin_unlock(&log->l_icloglock);\r\nreturn needed;\r\n}\r\nxfs_lsn_t\r\nxlog_assign_tail_lsn_locked(\r\nstruct xfs_mount *mp)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nstruct xfs_log_item *lip;\r\nxfs_lsn_t tail_lsn;\r\nassert_spin_locked(&mp->m_ail->xa_lock);\r\nlip = xfs_ail_min(mp->m_ail);\r\nif (lip)\r\ntail_lsn = lip->li_lsn;\r\nelse\r\ntail_lsn = atomic64_read(&log->l_last_sync_lsn);\r\ntrace_xfs_log_assign_tail_lsn(log, tail_lsn);\r\natomic64_set(&log->l_tail_lsn, tail_lsn);\r\nreturn tail_lsn;\r\n}\r\nxfs_lsn_t\r\nxlog_assign_tail_lsn(\r\nstruct xfs_mount *mp)\r\n{\r\nxfs_lsn_t tail_lsn;\r\nspin_lock(&mp->m_ail->xa_lock);\r\ntail_lsn = xlog_assign_tail_lsn_locked(mp);\r\nspin_unlock(&mp->m_ail->xa_lock);\r\nreturn tail_lsn;\r\n}\r\nSTATIC int\r\nxlog_space_left(\r\nstruct xlog *log,\r\natomic64_t *head)\r\n{\r\nint free_bytes;\r\nint tail_bytes;\r\nint tail_cycle;\r\nint head_cycle;\r\nint head_bytes;\r\nxlog_crack_grant_head(head, &head_cycle, &head_bytes);\r\nxlog_crack_atomic_lsn(&log->l_tail_lsn, &tail_cycle, &tail_bytes);\r\ntail_bytes = BBTOB(tail_bytes);\r\nif (tail_cycle == head_cycle && head_bytes >= tail_bytes)\r\nfree_bytes = log->l_logsize - (head_bytes - tail_bytes);\r\nelse if (tail_cycle + 1 < head_cycle)\r\nreturn 0;\r\nelse if (tail_cycle < head_cycle) {\r\nASSERT(tail_cycle == (head_cycle - 1));\r\nfree_bytes = tail_bytes - head_bytes;\r\n} else {\r\nxfs_alert(log->l_mp,\r\n"xlog_space_left: head behind tail\n"\r\n" tail_cycle = %d, tail_bytes = %d\n"\r\n" GH cycle = %d, GH bytes = %d",\r\ntail_cycle, tail_bytes, head_cycle, head_bytes);\r\nASSERT(0);\r\nfree_bytes = log->l_logsize;\r\n}\r\nreturn free_bytes;\r\n}\r\nvoid\r\nxlog_iodone(xfs_buf_t *bp)\r\n{\r\nstruct xlog_in_core *iclog = bp->b_fspriv;\r\nstruct xlog *l = iclog->ic_log;\r\nint aborted = 0;\r\nif (XFS_TEST_ERROR(bp->b_error, l->l_mp,\r\nXFS_ERRTAG_IODONE_IOERR, XFS_RANDOM_IODONE_IOERR)) {\r\nxfs_buf_ioerror_alert(bp, __func__);\r\nxfs_buf_stale(bp);\r\nxfs_force_shutdown(l->l_mp, SHUTDOWN_LOG_IO_ERROR);\r\naborted = XFS_LI_ABORTED;\r\n} else if (iclog->ic_state & XLOG_STATE_IOERROR) {\r\naborted = XFS_LI_ABORTED;\r\n}\r\nASSERT(XFS_BUF_ISASYNC(bp));\r\nxlog_state_done_syncing(iclog, aborted);\r\nxfs_buf_unlock(bp);\r\n}\r\nSTATIC void\r\nxlog_get_iclog_buffer_size(\r\nstruct xfs_mount *mp,\r\nstruct xlog *log)\r\n{\r\nint size;\r\nint xhdrs;\r\nif (mp->m_logbufs <= 0)\r\nlog->l_iclog_bufs = XLOG_MAX_ICLOGS;\r\nelse\r\nlog->l_iclog_bufs = mp->m_logbufs;\r\nif (mp->m_logbsize > 0) {\r\nsize = log->l_iclog_size = mp->m_logbsize;\r\nlog->l_iclog_size_log = 0;\r\nwhile (size != 1) {\r\nlog->l_iclog_size_log++;\r\nsize >>= 1;\r\n}\r\nif (xfs_sb_version_haslogv2(&mp->m_sb)) {\r\nxhdrs = mp->m_logbsize / XLOG_HEADER_CYCLE_SIZE;\r\nif (mp->m_logbsize % XLOG_HEADER_CYCLE_SIZE)\r\nxhdrs++;\r\nlog->l_iclog_hsize = xhdrs << BBSHIFT;\r\nlog->l_iclog_heads = xhdrs;\r\n} else {\r\nASSERT(mp->m_logbsize <= XLOG_BIG_RECORD_BSIZE);\r\nlog->l_iclog_hsize = BBSIZE;\r\nlog->l_iclog_heads = 1;\r\n}\r\ngoto done;\r\n}\r\nlog->l_iclog_size = XLOG_BIG_RECORD_BSIZE;\r\nlog->l_iclog_size_log = XLOG_BIG_RECORD_BSHIFT;\r\nlog->l_iclog_hsize = BBSIZE;\r\nlog->l_iclog_heads = 1;\r\ndone:\r\nif (mp->m_logbufs == 0)\r\nmp->m_logbufs = log->l_iclog_bufs;\r\nif (mp->m_logbsize == 0)\r\nmp->m_logbsize = log->l_iclog_size;\r\n}\r\nvoid\r\nxfs_log_work_queue(\r\nstruct xfs_mount *mp)\r\n{\r\nqueue_delayed_work(mp->m_log_workqueue, &mp->m_log->l_work,\r\nmsecs_to_jiffies(xfs_syncd_centisecs * 10));\r\n}\r\nvoid\r\nxfs_log_worker(\r\nstruct work_struct *work)\r\n{\r\nstruct xlog *log = container_of(to_delayed_work(work),\r\nstruct xlog, l_work);\r\nstruct xfs_mount *mp = log->l_mp;\r\nif (xfs_log_need_covered(mp)) {\r\nxfs_sync_sb(mp, true);\r\n} else\r\nxfs_log_force(mp, 0);\r\nxfs_ail_push_all(mp->m_ail);\r\nxfs_log_work_queue(mp);\r\n}\r\nSTATIC struct xlog *\r\nxlog_alloc_log(\r\nstruct xfs_mount *mp,\r\nstruct xfs_buftarg *log_target,\r\nxfs_daddr_t blk_offset,\r\nint num_bblks)\r\n{\r\nstruct xlog *log;\r\nxlog_rec_header_t *head;\r\nxlog_in_core_t **iclogp;\r\nxlog_in_core_t *iclog, *prev_iclog=NULL;\r\nxfs_buf_t *bp;\r\nint i;\r\nint error = -ENOMEM;\r\nuint log2_size = 0;\r\nlog = kmem_zalloc(sizeof(struct xlog), KM_MAYFAIL);\r\nif (!log) {\r\nxfs_warn(mp, "Log allocation failed: No memory!");\r\ngoto out;\r\n}\r\nlog->l_mp = mp;\r\nlog->l_targ = log_target;\r\nlog->l_logsize = BBTOB(num_bblks);\r\nlog->l_logBBstart = blk_offset;\r\nlog->l_logBBsize = num_bblks;\r\nlog->l_covered_state = XLOG_STATE_COVER_IDLE;\r\nlog->l_flags |= XLOG_ACTIVE_RECOVERY;\r\nINIT_DELAYED_WORK(&log->l_work, xfs_log_worker);\r\nlog->l_prev_block = -1;\r\nxlog_assign_atomic_lsn(&log->l_tail_lsn, 1, 0);\r\nxlog_assign_atomic_lsn(&log->l_last_sync_lsn, 1, 0);\r\nlog->l_curr_cycle = 1;\r\nxlog_grant_head_init(&log->l_reserve_head);\r\nxlog_grant_head_init(&log->l_write_head);\r\nerror = -EFSCORRUPTED;\r\nif (xfs_sb_version_hassector(&mp->m_sb)) {\r\nlog2_size = mp->m_sb.sb_logsectlog;\r\nif (log2_size < BBSHIFT) {\r\nxfs_warn(mp, "Log sector size too small (0x%x < 0x%x)",\r\nlog2_size, BBSHIFT);\r\ngoto out_free_log;\r\n}\r\nlog2_size -= BBSHIFT;\r\nif (log2_size > mp->m_sectbb_log) {\r\nxfs_warn(mp, "Log sector size too large (0x%x > 0x%x)",\r\nlog2_size, mp->m_sectbb_log);\r\ngoto out_free_log;\r\n}\r\nif (log2_size && log->l_logBBstart > 0 &&\r\n!xfs_sb_version_haslogv2(&mp->m_sb)) {\r\nxfs_warn(mp,\r\n"log sector size (0x%x) invalid for configuration.",\r\nlog2_size);\r\ngoto out_free_log;\r\n}\r\n}\r\nlog->l_sectBBsize = 1 << log2_size;\r\nxlog_get_iclog_buffer_size(mp, log);\r\nerror = -ENOMEM;\r\nbp = xfs_buf_alloc(mp->m_logdev_targp, XFS_BUF_DADDR_NULL,\r\nBTOBB(log->l_iclog_size), 0);\r\nif (!bp)\r\ngoto out_free_log;\r\nASSERT(xfs_buf_islocked(bp));\r\nxfs_buf_unlock(bp);\r\nbp->b_ioend_wq = mp->m_log_workqueue;\r\nbp->b_iodone = xlog_iodone;\r\nlog->l_xbuf = bp;\r\nspin_lock_init(&log->l_icloglock);\r\ninit_waitqueue_head(&log->l_flush_wait);\r\niclogp = &log->l_iclog;\r\nASSERT(log->l_iclog_size >= 4096);\r\nfor (i=0; i < log->l_iclog_bufs; i++) {\r\n*iclogp = kmem_zalloc(sizeof(xlog_in_core_t), KM_MAYFAIL);\r\nif (!*iclogp)\r\ngoto out_free_iclog;\r\niclog = *iclogp;\r\niclog->ic_prev = prev_iclog;\r\nprev_iclog = iclog;\r\nbp = xfs_buf_get_uncached(mp->m_logdev_targp,\r\nBTOBB(log->l_iclog_size), 0);\r\nif (!bp)\r\ngoto out_free_iclog;\r\nASSERT(xfs_buf_islocked(bp));\r\nxfs_buf_unlock(bp);\r\nbp->b_ioend_wq = mp->m_log_workqueue;\r\nbp->b_iodone = xlog_iodone;\r\niclog->ic_bp = bp;\r\niclog->ic_data = bp->b_addr;\r\n#ifdef DEBUG\r\nlog->l_iclog_bak[i] = (xfs_caddr_t)&(iclog->ic_header);\r\n#endif\r\nhead = &iclog->ic_header;\r\nmemset(head, 0, sizeof(xlog_rec_header_t));\r\nhead->h_magicno = cpu_to_be32(XLOG_HEADER_MAGIC_NUM);\r\nhead->h_version = cpu_to_be32(\r\nxfs_sb_version_haslogv2(&log->l_mp->m_sb) ? 2 : 1);\r\nhead->h_size = cpu_to_be32(log->l_iclog_size);\r\nhead->h_fmt = cpu_to_be32(XLOG_FMT);\r\nmemcpy(&head->h_fs_uuid, &mp->m_sb.sb_uuid, sizeof(uuid_t));\r\niclog->ic_size = BBTOB(bp->b_length) - log->l_iclog_hsize;\r\niclog->ic_state = XLOG_STATE_ACTIVE;\r\niclog->ic_log = log;\r\natomic_set(&iclog->ic_refcnt, 0);\r\nspin_lock_init(&iclog->ic_callback_lock);\r\niclog->ic_callback_tail = &(iclog->ic_callback);\r\niclog->ic_datap = (char *)iclog->ic_data + log->l_iclog_hsize;\r\ninit_waitqueue_head(&iclog->ic_force_wait);\r\ninit_waitqueue_head(&iclog->ic_write_wait);\r\niclogp = &iclog->ic_next;\r\n}\r\n*iclogp = log->l_iclog;\r\nlog->l_iclog->ic_prev = prev_iclog;\r\nerror = xlog_cil_init(log);\r\nif (error)\r\ngoto out_free_iclog;\r\nreturn log;\r\nout_free_iclog:\r\nfor (iclog = log->l_iclog; iclog; iclog = prev_iclog) {\r\nprev_iclog = iclog->ic_next;\r\nif (iclog->ic_bp)\r\nxfs_buf_free(iclog->ic_bp);\r\nkmem_free(iclog);\r\n}\r\nspinlock_destroy(&log->l_icloglock);\r\nxfs_buf_free(log->l_xbuf);\r\nout_free_log:\r\nkmem_free(log);\r\nout:\r\nreturn ERR_PTR(error);\r\n}\r\nSTATIC int\r\nxlog_commit_record(\r\nstruct xlog *log,\r\nstruct xlog_ticket *ticket,\r\nstruct xlog_in_core **iclog,\r\nxfs_lsn_t *commitlsnp)\r\n{\r\nstruct xfs_mount *mp = log->l_mp;\r\nint error;\r\nstruct xfs_log_iovec reg = {\r\n.i_addr = NULL,\r\n.i_len = 0,\r\n.i_type = XLOG_REG_TYPE_COMMIT,\r\n};\r\nstruct xfs_log_vec vec = {\r\n.lv_niovecs = 1,\r\n.lv_iovecp = &reg,\r\n};\r\nASSERT_ALWAYS(iclog);\r\nerror = xlog_write(log, &vec, ticket, commitlsnp, iclog,\r\nXLOG_COMMIT_TRANS);\r\nif (error)\r\nxfs_force_shutdown(mp, SHUTDOWN_LOG_IO_ERROR);\r\nreturn error;\r\n}\r\nSTATIC void\r\nxlog_grant_push_ail(\r\nstruct xlog *log,\r\nint need_bytes)\r\n{\r\nxfs_lsn_t threshold_lsn = 0;\r\nxfs_lsn_t last_sync_lsn;\r\nint free_blocks;\r\nint free_bytes;\r\nint threshold_block;\r\nint threshold_cycle;\r\nint free_threshold;\r\nASSERT(BTOBB(need_bytes) < log->l_logBBsize);\r\nfree_bytes = xlog_space_left(log, &log->l_reserve_head.grant);\r\nfree_blocks = BTOBBT(free_bytes);\r\nfree_threshold = BTOBB(need_bytes);\r\nfree_threshold = MAX(free_threshold, (log->l_logBBsize >> 2));\r\nfree_threshold = MAX(free_threshold, 256);\r\nif (free_blocks >= free_threshold)\r\nreturn;\r\nxlog_crack_atomic_lsn(&log->l_tail_lsn, &threshold_cycle,\r\n&threshold_block);\r\nthreshold_block += free_threshold;\r\nif (threshold_block >= log->l_logBBsize) {\r\nthreshold_block -= log->l_logBBsize;\r\nthreshold_cycle += 1;\r\n}\r\nthreshold_lsn = xlog_assign_lsn(threshold_cycle,\r\nthreshold_block);\r\nlast_sync_lsn = atomic64_read(&log->l_last_sync_lsn);\r\nif (XFS_LSN_CMP(threshold_lsn, last_sync_lsn) > 0)\r\nthreshold_lsn = last_sync_lsn;\r\nif (!XLOG_FORCED_SHUTDOWN(log))\r\nxfs_ail_push(log->l_ailp, threshold_lsn);\r\n}\r\nSTATIC void\r\nxlog_pack_data(\r\nstruct xlog *log,\r\nstruct xlog_in_core *iclog,\r\nint roundoff)\r\n{\r\nint i, j, k;\r\nint size = iclog->ic_offset + roundoff;\r\n__be32 cycle_lsn;\r\nxfs_caddr_t dp;\r\ncycle_lsn = CYCLE_LSN_DISK(iclog->ic_header.h_lsn);\r\ndp = iclog->ic_datap;\r\nfor (i = 0; i < BTOBB(size); i++) {\r\nif (i >= (XLOG_HEADER_CYCLE_SIZE / BBSIZE))\r\nbreak;\r\niclog->ic_header.h_cycle_data[i] = *(__be32 *)dp;\r\n*(__be32 *)dp = cycle_lsn;\r\ndp += BBSIZE;\r\n}\r\nif (xfs_sb_version_haslogv2(&log->l_mp->m_sb)) {\r\nxlog_in_core_2_t *xhdr = iclog->ic_data;\r\nfor ( ; i < BTOBB(size); i++) {\r\nj = i / (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\r\nk = i % (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\r\nxhdr[j].hic_xheader.xh_cycle_data[k] = *(__be32 *)dp;\r\n*(__be32 *)dp = cycle_lsn;\r\ndp += BBSIZE;\r\n}\r\nfor (i = 1; i < log->l_iclog_heads; i++)\r\nxhdr[i].hic_xheader.xh_cycle = cycle_lsn;\r\n}\r\n}\r\n__le32\r\nxlog_cksum(\r\nstruct xlog *log,\r\nstruct xlog_rec_header *rhead,\r\nchar *dp,\r\nint size)\r\n{\r\n__uint32_t crc;\r\ncrc = xfs_start_cksum((char *)rhead,\r\nsizeof(struct xlog_rec_header),\r\noffsetof(struct xlog_rec_header, h_crc));\r\nif (xfs_sb_version_haslogv2(&log->l_mp->m_sb)) {\r\nunion xlog_in_core2 *xhdr = (union xlog_in_core2 *)rhead;\r\nint i;\r\nfor (i = 1; i < log->l_iclog_heads; i++) {\r\ncrc = crc32c(crc, &xhdr[i].hic_xheader,\r\nsizeof(struct xlog_rec_ext_header));\r\n}\r\n}\r\ncrc = crc32c(crc, dp, size);\r\nreturn xfs_end_cksum(crc);\r\n}\r\nSTATIC int\r\nxlog_bdstrat(\r\nstruct xfs_buf *bp)\r\n{\r\nstruct xlog_in_core *iclog = bp->b_fspriv;\r\nxfs_buf_lock(bp);\r\nif (iclog->ic_state & XLOG_STATE_IOERROR) {\r\nxfs_buf_ioerror(bp, -EIO);\r\nxfs_buf_stale(bp);\r\nxfs_buf_ioend(bp);\r\nreturn 0;\r\n}\r\nxfs_buf_submit(bp);\r\nreturn 0;\r\n}\r\nSTATIC int\r\nxlog_sync(\r\nstruct xlog *log,\r\nstruct xlog_in_core *iclog)\r\n{\r\nxfs_buf_t *bp;\r\nint i;\r\nuint count;\r\nuint count_init;\r\nint roundoff;\r\nint split = 0;\r\nint error;\r\nint v2 = xfs_sb_version_haslogv2(&log->l_mp->m_sb);\r\nint size;\r\nXFS_STATS_INC(xs_log_writes);\r\nASSERT(atomic_read(&iclog->ic_refcnt) == 0);\r\ncount_init = log->l_iclog_hsize + iclog->ic_offset;\r\nif (v2 && log->l_mp->m_sb.sb_logsunit > 1) {\r\ncount = XLOG_LSUNITTOB(log, XLOG_BTOLSUNIT(log, count_init));\r\n} else {\r\ncount = BBTOB(BTOBB(count_init));\r\n}\r\nroundoff = count - count_init;\r\nASSERT(roundoff >= 0);\r\nASSERT((v2 && log->l_mp->m_sb.sb_logsunit > 1 &&\r\nroundoff < log->l_mp->m_sb.sb_logsunit)\r\n||\r\n(log->l_mp->m_sb.sb_logsunit <= 1 &&\r\nroundoff < BBTOB(1)));\r\nxlog_grant_add_space(log, &log->l_reserve_head.grant, roundoff);\r\nxlog_grant_add_space(log, &log->l_write_head.grant, roundoff);\r\nxlog_pack_data(log, iclog, roundoff);\r\nsize = iclog->ic_offset;\r\nif (v2)\r\nsize += roundoff;\r\niclog->ic_header.h_len = cpu_to_be32(size);\r\nbp = iclog->ic_bp;\r\nXFS_BUF_SET_ADDR(bp, BLOCK_LSN(be64_to_cpu(iclog->ic_header.h_lsn)));\r\nXFS_STATS_ADD(xs_log_blocks, BTOBB(count));\r\nif (XFS_BUF_ADDR(bp) + BTOBB(count) > log->l_logBBsize) {\r\nchar *dptr;\r\nsplit = count - (BBTOB(log->l_logBBsize - XFS_BUF_ADDR(bp)));\r\ncount = BBTOB(log->l_logBBsize - XFS_BUF_ADDR(bp));\r\niclog->ic_bwritecnt = 2;\r\ndptr = (char *)&iclog->ic_header + count;\r\nfor (i = 0; i < split; i += BBSIZE) {\r\n__uint32_t cycle = be32_to_cpu(*(__be32 *)dptr);\r\nif (++cycle == XLOG_HEADER_MAGIC_NUM)\r\ncycle++;\r\n*(__be32 *)dptr = cpu_to_be32(cycle);\r\ndptr += BBSIZE;\r\n}\r\n} else {\r\niclog->ic_bwritecnt = 1;\r\n}\r\niclog->ic_header.h_crc = xlog_cksum(log, &iclog->ic_header,\r\niclog->ic_datap, size);\r\nbp->b_io_length = BTOBB(count);\r\nbp->b_fspriv = iclog;\r\nXFS_BUF_ZEROFLAGS(bp);\r\nXFS_BUF_ASYNC(bp);\r\nbp->b_flags |= XBF_SYNCIO;\r\nif (log->l_mp->m_flags & XFS_MOUNT_BARRIER) {\r\nbp->b_flags |= XBF_FUA;\r\nif (log->l_mp->m_logdev_targp != log->l_mp->m_ddev_targp)\r\nxfs_blkdev_issue_flush(log->l_mp->m_ddev_targp);\r\nelse\r\nbp->b_flags |= XBF_FLUSH;\r\n}\r\nASSERT(XFS_BUF_ADDR(bp) <= log->l_logBBsize-1);\r\nASSERT(XFS_BUF_ADDR(bp) + BTOBB(count) <= log->l_logBBsize);\r\nxlog_verify_iclog(log, iclog, count, true);\r\nXFS_BUF_SET_ADDR(bp, XFS_BUF_ADDR(bp) + log->l_logBBstart);\r\nXFS_BUF_WRITE(bp);\r\nerror = xlog_bdstrat(bp);\r\nif (error) {\r\nxfs_buf_ioerror_alert(bp, "xlog_sync");\r\nreturn error;\r\n}\r\nif (split) {\r\nbp = iclog->ic_log->l_xbuf;\r\nXFS_BUF_SET_ADDR(bp, 0);\r\nxfs_buf_associate_memory(bp,\r\n(char *)&iclog->ic_header + count, split);\r\nbp->b_fspriv = iclog;\r\nXFS_BUF_ZEROFLAGS(bp);\r\nXFS_BUF_ASYNC(bp);\r\nbp->b_flags |= XBF_SYNCIO;\r\nif (log->l_mp->m_flags & XFS_MOUNT_BARRIER)\r\nbp->b_flags |= XBF_FUA;\r\nASSERT(XFS_BUF_ADDR(bp) <= log->l_logBBsize-1);\r\nASSERT(XFS_BUF_ADDR(bp) + BTOBB(count) <= log->l_logBBsize);\r\nXFS_BUF_SET_ADDR(bp, XFS_BUF_ADDR(bp) + log->l_logBBstart);\r\nXFS_BUF_WRITE(bp);\r\nerror = xlog_bdstrat(bp);\r\nif (error) {\r\nxfs_buf_ioerror_alert(bp, "xlog_sync (split)");\r\nreturn error;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nSTATIC void\r\nxlog_dealloc_log(\r\nstruct xlog *log)\r\n{\r\nxlog_in_core_t *iclog, *next_iclog;\r\nint i;\r\nxlog_cil_destroy(log);\r\niclog = log->l_iclog;\r\nfor (i = 0; i < log->l_iclog_bufs; i++) {\r\nxfs_buf_lock(iclog->ic_bp);\r\nxfs_buf_unlock(iclog->ic_bp);\r\niclog = iclog->ic_next;\r\n}\r\nxfs_buf_lock(log->l_xbuf);\r\nxfs_buf_unlock(log->l_xbuf);\r\nxfs_buf_set_empty(log->l_xbuf, BTOBB(log->l_iclog_size));\r\nxfs_buf_free(log->l_xbuf);\r\niclog = log->l_iclog;\r\nfor (i = 0; i < log->l_iclog_bufs; i++) {\r\nxfs_buf_free(iclog->ic_bp);\r\nnext_iclog = iclog->ic_next;\r\nkmem_free(iclog);\r\niclog = next_iclog;\r\n}\r\nspinlock_destroy(&log->l_icloglock);\r\nlog->l_mp->m_log = NULL;\r\nkmem_free(log);\r\n}\r\nstatic inline void\r\nxlog_state_finish_copy(\r\nstruct xlog *log,\r\nstruct xlog_in_core *iclog,\r\nint record_cnt,\r\nint copy_bytes)\r\n{\r\nspin_lock(&log->l_icloglock);\r\nbe32_add_cpu(&iclog->ic_header.h_num_logops, record_cnt);\r\niclog->ic_offset += copy_bytes;\r\nspin_unlock(&log->l_icloglock);\r\n}\r\nvoid\r\nxlog_print_tic_res(\r\nstruct xfs_mount *mp,\r\nstruct xlog_ticket *ticket)\r\n{\r\nuint i;\r\nuint ophdr_spc = ticket->t_res_num_ophdrs * (uint)sizeof(xlog_op_header_t);\r\nstatic char *res_type_str[XLOG_REG_TYPE_MAX] = {\r\n"bformat",\r\n"bchunk",\r\n"efi_format",\r\n"efd_format",\r\n"iformat",\r\n"icore",\r\n"iext",\r\n"ibroot",\r\n"ilocal",\r\n"iattr_ext",\r\n"iattr_broot",\r\n"iattr_local",\r\n"qformat",\r\n"dquot",\r\n"quotaoff",\r\n"LR header",\r\n"unmount",\r\n"commit",\r\n"trans header"\r\n};\r\nstatic char *trans_type_str[XFS_TRANS_TYPE_MAX] = {\r\n"SETATTR_NOT_SIZE",\r\n"SETATTR_SIZE",\r\n"INACTIVE",\r\n"CREATE",\r\n"CREATE_TRUNC",\r\n"TRUNCATE_FILE",\r\n"REMOVE",\r\n"LINK",\r\n"RENAME",\r\n"MKDIR",\r\n"RMDIR",\r\n"SYMLINK",\r\n"SET_DMATTRS",\r\n"GROWFS",\r\n"STRAT_WRITE",\r\n"DIOSTRAT",\r\n"WRITE_SYNC",\r\n"WRITEID",\r\n"ADDAFORK",\r\n"ATTRINVAL",\r\n"ATRUNCATE",\r\n"ATTR_SET",\r\n"ATTR_RM",\r\n"ATTR_FLAG",\r\n"CLEAR_AGI_BUCKET",\r\n"QM_SBCHANGE",\r\n"DUMMY1",\r\n"DUMMY2",\r\n"QM_QUOTAOFF",\r\n"QM_DQALLOC",\r\n"QM_SETQLIM",\r\n"QM_DQCLUSTER",\r\n"QM_QINOCREATE",\r\n"QM_QUOTAOFF_END",\r\n"SB_UNIT",\r\n"FSYNC_TS",\r\n"GROWFSRT_ALLOC",\r\n"GROWFSRT_ZERO",\r\n"GROWFSRT_FREE",\r\n"SWAPEXT"\r\n};\r\nxfs_warn(mp,\r\n"xlog_write: reservation summary:\n"\r\n" trans type = %s (%u)\n"\r\n" unit res = %d bytes\n"\r\n" current res = %d bytes\n"\r\n" total reg = %u bytes (o/flow = %u bytes)\n"\r\n" ophdrs = %u (ophdr space = %u bytes)\n"\r\n" ophdr + reg = %u bytes\n"\r\n" num regions = %u",\r\n((ticket->t_trans_type <= 0 ||\r\nticket->t_trans_type > XFS_TRANS_TYPE_MAX) ?\r\n"bad-trans-type" : trans_type_str[ticket->t_trans_type-1]),\r\nticket->t_trans_type,\r\nticket->t_unit_res,\r\nticket->t_curr_res,\r\nticket->t_res_arr_sum, ticket->t_res_o_flow,\r\nticket->t_res_num_ophdrs, ophdr_spc,\r\nticket->t_res_arr_sum +\r\nticket->t_res_o_flow + ophdr_spc,\r\nticket->t_res_num);\r\nfor (i = 0; i < ticket->t_res_num; i++) {\r\nuint r_type = ticket->t_res_arr[i].r_type;\r\nxfs_warn(mp, "region[%u]: %s - %u bytes", i,\r\n((r_type <= 0 || r_type > XLOG_REG_TYPE_MAX) ?\r\n"bad-rtype" : res_type_str[r_type-1]),\r\nticket->t_res_arr[i].r_len);\r\n}\r\nxfs_alert_tag(mp, XFS_PTAG_LOGRES,\r\n"xlog_write: reservation ran out. Need to up reservation");\r\nxfs_force_shutdown(mp, SHUTDOWN_LOG_IO_ERROR);\r\n}\r\nstatic int\r\nxlog_write_calc_vec_length(\r\nstruct xlog_ticket *ticket,\r\nstruct xfs_log_vec *log_vector)\r\n{\r\nstruct xfs_log_vec *lv;\r\nint headers = 0;\r\nint len = 0;\r\nint i;\r\nif (ticket->t_flags & XLOG_TIC_INITED)\r\nheaders++;\r\nfor (lv = log_vector; lv; lv = lv->lv_next) {\r\nif (lv->lv_buf_len == XFS_LOG_VEC_ORDERED)\r\ncontinue;\r\nheaders += lv->lv_niovecs;\r\nfor (i = 0; i < lv->lv_niovecs; i++) {\r\nstruct xfs_log_iovec *vecp = &lv->lv_iovecp[i];\r\nlen += vecp->i_len;\r\nxlog_tic_add_region(ticket, vecp->i_len, vecp->i_type);\r\n}\r\n}\r\nticket->t_res_num_ophdrs += headers;\r\nlen += headers * sizeof(struct xlog_op_header);\r\nreturn len;\r\n}\r\nstatic int\r\nxlog_write_start_rec(\r\nstruct xlog_op_header *ophdr,\r\nstruct xlog_ticket *ticket)\r\n{\r\nif (!(ticket->t_flags & XLOG_TIC_INITED))\r\nreturn 0;\r\nophdr->oh_tid = cpu_to_be32(ticket->t_tid);\r\nophdr->oh_clientid = ticket->t_clientid;\r\nophdr->oh_len = 0;\r\nophdr->oh_flags = XLOG_START_TRANS;\r\nophdr->oh_res2 = 0;\r\nticket->t_flags &= ~XLOG_TIC_INITED;\r\nreturn sizeof(struct xlog_op_header);\r\n}\r\nstatic xlog_op_header_t *\r\nxlog_write_setup_ophdr(\r\nstruct xlog *log,\r\nstruct xlog_op_header *ophdr,\r\nstruct xlog_ticket *ticket,\r\nuint flags)\r\n{\r\nophdr->oh_tid = cpu_to_be32(ticket->t_tid);\r\nophdr->oh_clientid = ticket->t_clientid;\r\nophdr->oh_res2 = 0;\r\nophdr->oh_flags = flags;\r\nswitch (ophdr->oh_clientid) {\r\ncase XFS_TRANSACTION:\r\ncase XFS_VOLUME:\r\ncase XFS_LOG:\r\nbreak;\r\ndefault:\r\nxfs_warn(log->l_mp,\r\n"Bad XFS transaction clientid 0x%x in ticket 0x%p",\r\nophdr->oh_clientid, ticket);\r\nreturn NULL;\r\n}\r\nreturn ophdr;\r\n}\r\nstatic int\r\nxlog_write_setup_copy(\r\nstruct xlog_ticket *ticket,\r\nstruct xlog_op_header *ophdr,\r\nint space_available,\r\nint space_required,\r\nint *copy_off,\r\nint *copy_len,\r\nint *last_was_partial_copy,\r\nint *bytes_consumed)\r\n{\r\nint still_to_copy;\r\nstill_to_copy = space_required - *bytes_consumed;\r\n*copy_off = *bytes_consumed;\r\nif (still_to_copy <= space_available) {\r\n*copy_len = still_to_copy;\r\nophdr->oh_len = cpu_to_be32(*copy_len);\r\nif (*last_was_partial_copy)\r\nophdr->oh_flags |= (XLOG_END_TRANS|XLOG_WAS_CONT_TRANS);\r\n*last_was_partial_copy = 0;\r\n*bytes_consumed = 0;\r\nreturn 0;\r\n}\r\n*copy_len = space_available;\r\nophdr->oh_len = cpu_to_be32(*copy_len);\r\nophdr->oh_flags |= XLOG_CONTINUE_TRANS;\r\nif (*last_was_partial_copy)\r\nophdr->oh_flags |= XLOG_WAS_CONT_TRANS;\r\n*bytes_consumed += *copy_len;\r\n(*last_was_partial_copy)++;\r\nticket->t_curr_res -= sizeof(struct xlog_op_header);\r\nticket->t_res_num_ophdrs++;\r\nreturn sizeof(struct xlog_op_header);\r\n}\r\nstatic int\r\nxlog_write_copy_finish(\r\nstruct xlog *log,\r\nstruct xlog_in_core *iclog,\r\nuint flags,\r\nint *record_cnt,\r\nint *data_cnt,\r\nint *partial_copy,\r\nint *partial_copy_len,\r\nint log_offset,\r\nstruct xlog_in_core **commit_iclog)\r\n{\r\nif (*partial_copy) {\r\nxlog_state_finish_copy(log, iclog, *record_cnt, *data_cnt);\r\n*record_cnt = 0;\r\n*data_cnt = 0;\r\nreturn xlog_state_release_iclog(log, iclog);\r\n}\r\n*partial_copy = 0;\r\n*partial_copy_len = 0;\r\nif (iclog->ic_size - log_offset <= sizeof(xlog_op_header_t)) {\r\nxlog_state_finish_copy(log, iclog, *record_cnt, *data_cnt);\r\n*record_cnt = 0;\r\n*data_cnt = 0;\r\nspin_lock(&log->l_icloglock);\r\nxlog_state_want_sync(log, iclog);\r\nspin_unlock(&log->l_icloglock);\r\nif (!commit_iclog)\r\nreturn xlog_state_release_iclog(log, iclog);\r\nASSERT(flags & XLOG_COMMIT_TRANS);\r\n*commit_iclog = iclog;\r\n}\r\nreturn 0;\r\n}\r\nint\r\nxlog_write(\r\nstruct xlog *log,\r\nstruct xfs_log_vec *log_vector,\r\nstruct xlog_ticket *ticket,\r\nxfs_lsn_t *start_lsn,\r\nstruct xlog_in_core **commit_iclog,\r\nuint flags)\r\n{\r\nstruct xlog_in_core *iclog = NULL;\r\nstruct xfs_log_iovec *vecp;\r\nstruct xfs_log_vec *lv;\r\nint len;\r\nint index;\r\nint partial_copy = 0;\r\nint partial_copy_len = 0;\r\nint contwr = 0;\r\nint record_cnt = 0;\r\nint data_cnt = 0;\r\nint error;\r\n*start_lsn = 0;\r\nlen = xlog_write_calc_vec_length(ticket, log_vector);\r\nif (ticket->t_flags & XLOG_TIC_INITED)\r\nticket->t_curr_res -= sizeof(xlog_op_header_t);\r\nif (flags & (XLOG_COMMIT_TRANS | XLOG_UNMOUNT_TRANS))\r\nticket->t_curr_res -= sizeof(xlog_op_header_t);\r\nif (ticket->t_curr_res < 0)\r\nxlog_print_tic_res(log->l_mp, ticket);\r\nindex = 0;\r\nlv = log_vector;\r\nvecp = lv->lv_iovecp;\r\nwhile (lv && (!lv->lv_niovecs || index < lv->lv_niovecs)) {\r\nvoid *ptr;\r\nint log_offset;\r\nerror = xlog_state_get_iclog_space(log, len, &iclog, ticket,\r\n&contwr, &log_offset);\r\nif (error)\r\nreturn error;\r\nASSERT(log_offset <= iclog->ic_size - 1);\r\nptr = iclog->ic_datap + log_offset;\r\nif (!*start_lsn)\r\n*start_lsn = be64_to_cpu(iclog->ic_header.h_lsn);\r\nwhile (lv && (!lv->lv_niovecs || index < lv->lv_niovecs)) {\r\nstruct xfs_log_iovec *reg;\r\nstruct xlog_op_header *ophdr;\r\nint start_rec_copy;\r\nint copy_len;\r\nint copy_off;\r\nbool ordered = false;\r\nif (lv->lv_buf_len == XFS_LOG_VEC_ORDERED) {\r\nASSERT(lv->lv_niovecs == 0);\r\nordered = true;\r\ngoto next_lv;\r\n}\r\nreg = &vecp[index];\r\nASSERT(reg->i_len % sizeof(__int32_t) == 0);\r\nASSERT((unsigned long)ptr % sizeof(__int32_t) == 0);\r\nstart_rec_copy = xlog_write_start_rec(ptr, ticket);\r\nif (start_rec_copy) {\r\nrecord_cnt++;\r\nxlog_write_adv_cnt(&ptr, &len, &log_offset,\r\nstart_rec_copy);\r\n}\r\nophdr = xlog_write_setup_ophdr(log, ptr, ticket, flags);\r\nif (!ophdr)\r\nreturn -EIO;\r\nxlog_write_adv_cnt(&ptr, &len, &log_offset,\r\nsizeof(struct xlog_op_header));\r\nlen += xlog_write_setup_copy(ticket, ophdr,\r\niclog->ic_size-log_offset,\r\nreg->i_len,\r\n&copy_off, &copy_len,\r\n&partial_copy,\r\n&partial_copy_len);\r\nxlog_verify_dest_ptr(log, ptr);\r\nASSERT(copy_len >= 0);\r\nmemcpy(ptr, reg->i_addr + copy_off, copy_len);\r\nxlog_write_adv_cnt(&ptr, &len, &log_offset, copy_len);\r\ncopy_len += start_rec_copy + sizeof(xlog_op_header_t);\r\nrecord_cnt++;\r\ndata_cnt += contwr ? copy_len : 0;\r\nerror = xlog_write_copy_finish(log, iclog, flags,\r\n&record_cnt, &data_cnt,\r\n&partial_copy,\r\n&partial_copy_len,\r\nlog_offset,\r\ncommit_iclog);\r\nif (error)\r\nreturn error;\r\nif (partial_copy)\r\nbreak;\r\nif (++index == lv->lv_niovecs) {\r\nnext_lv:\r\nlv = lv->lv_next;\r\nindex = 0;\r\nif (lv)\r\nvecp = lv->lv_iovecp;\r\n}\r\nif (record_cnt == 0 && ordered == false) {\r\nif (!lv)\r\nreturn 0;\r\nbreak;\r\n}\r\n}\r\n}\r\nASSERT(len == 0);\r\nxlog_state_finish_copy(log, iclog, record_cnt, data_cnt);\r\nif (!commit_iclog)\r\nreturn xlog_state_release_iclog(log, iclog);\r\nASSERT(flags & XLOG_COMMIT_TRANS);\r\n*commit_iclog = iclog;\r\nreturn 0;\r\n}\r\nSTATIC void\r\nxlog_state_clean_log(\r\nstruct xlog *log)\r\n{\r\nxlog_in_core_t *iclog;\r\nint changed = 0;\r\niclog = log->l_iclog;\r\ndo {\r\nif (iclog->ic_state == XLOG_STATE_DIRTY) {\r\niclog->ic_state = XLOG_STATE_ACTIVE;\r\niclog->ic_offset = 0;\r\nASSERT(iclog->ic_callback == NULL);\r\nif (!changed &&\r\n(be32_to_cpu(iclog->ic_header.h_num_logops) ==\r\nXLOG_COVER_OPS)) {\r\nchanged = 1;\r\n} else {\r\nchanged = 2;\r\n}\r\niclog->ic_header.h_num_logops = 0;\r\nmemset(iclog->ic_header.h_cycle_data, 0,\r\nsizeof(iclog->ic_header.h_cycle_data));\r\niclog->ic_header.h_lsn = 0;\r\n} else if (iclog->ic_state == XLOG_STATE_ACTIVE)\r\n;\r\nelse\r\nbreak;\r\niclog = iclog->ic_next;\r\n} while (iclog != log->l_iclog);\r\nif (changed) {\r\nswitch (log->l_covered_state) {\r\ncase XLOG_STATE_COVER_IDLE:\r\ncase XLOG_STATE_COVER_NEED:\r\ncase XLOG_STATE_COVER_NEED2:\r\nlog->l_covered_state = XLOG_STATE_COVER_NEED;\r\nbreak;\r\ncase XLOG_STATE_COVER_DONE:\r\nif (changed == 1)\r\nlog->l_covered_state = XLOG_STATE_COVER_NEED2;\r\nelse\r\nlog->l_covered_state = XLOG_STATE_COVER_NEED;\r\nbreak;\r\ncase XLOG_STATE_COVER_DONE2:\r\nif (changed == 1)\r\nlog->l_covered_state = XLOG_STATE_COVER_IDLE;\r\nelse\r\nlog->l_covered_state = XLOG_STATE_COVER_NEED;\r\nbreak;\r\ndefault:\r\nASSERT(0);\r\n}\r\n}\r\n}\r\nSTATIC xfs_lsn_t\r\nxlog_get_lowest_lsn(\r\nstruct xlog *log)\r\n{\r\nxlog_in_core_t *lsn_log;\r\nxfs_lsn_t lowest_lsn, lsn;\r\nlsn_log = log->l_iclog;\r\nlowest_lsn = 0;\r\ndo {\r\nif (!(lsn_log->ic_state & (XLOG_STATE_ACTIVE|XLOG_STATE_DIRTY))) {\r\nlsn = be64_to_cpu(lsn_log->ic_header.h_lsn);\r\nif ((lsn && !lowest_lsn) ||\r\n(XFS_LSN_CMP(lsn, lowest_lsn) < 0)) {\r\nlowest_lsn = lsn;\r\n}\r\n}\r\nlsn_log = lsn_log->ic_next;\r\n} while (lsn_log != log->l_iclog);\r\nreturn lowest_lsn;\r\n}\r\nSTATIC void\r\nxlog_state_do_callback(\r\nstruct xlog *log,\r\nint aborted,\r\nstruct xlog_in_core *ciclog)\r\n{\r\nxlog_in_core_t *iclog;\r\nxlog_in_core_t *first_iclog;\r\nxfs_log_callback_t *cb, *cb_next;\r\nint flushcnt = 0;\r\nxfs_lsn_t lowest_lsn;\r\nint ioerrors;\r\nint loopdidcallbacks;\r\nint funcdidcallbacks;\r\nint repeats;\r\nint wake = 0;\r\nspin_lock(&log->l_icloglock);\r\nfirst_iclog = iclog = log->l_iclog;\r\nioerrors = 0;\r\nfuncdidcallbacks = 0;\r\nrepeats = 0;\r\ndo {\r\nfirst_iclog = log->l_iclog;\r\niclog = log->l_iclog;\r\nloopdidcallbacks = 0;\r\nrepeats++;\r\ndo {\r\nif (iclog->ic_state &\r\n(XLOG_STATE_ACTIVE|XLOG_STATE_DIRTY)) {\r\niclog = iclog->ic_next;\r\ncontinue;\r\n}\r\nif (!(iclog->ic_state & XLOG_STATE_IOERROR)) {\r\nif (!(iclog->ic_state &\r\n(XLOG_STATE_DONE_SYNC |\r\nXLOG_STATE_DO_CALLBACK))) {\r\nif (ciclog && (ciclog->ic_state ==\r\nXLOG_STATE_DONE_SYNC)) {\r\nciclog->ic_state = XLOG_STATE_DO_CALLBACK;\r\n}\r\nbreak;\r\n}\r\nlowest_lsn = xlog_get_lowest_lsn(log);\r\nif (lowest_lsn &&\r\nXFS_LSN_CMP(lowest_lsn,\r\nbe64_to_cpu(iclog->ic_header.h_lsn)) < 0) {\r\niclog = iclog->ic_next;\r\ncontinue;\r\n}\r\niclog->ic_state = XLOG_STATE_CALLBACK;\r\nASSERT(XFS_LSN_CMP(atomic64_read(&log->l_last_sync_lsn),\r\nbe64_to_cpu(iclog->ic_header.h_lsn)) <= 0);\r\nif (iclog->ic_callback)\r\natomic64_set(&log->l_last_sync_lsn,\r\nbe64_to_cpu(iclog->ic_header.h_lsn));\r\n} else\r\nioerrors++;\r\nspin_unlock(&log->l_icloglock);\r\nspin_lock(&iclog->ic_callback_lock);\r\ncb = iclog->ic_callback;\r\nwhile (cb) {\r\niclog->ic_callback_tail = &(iclog->ic_callback);\r\niclog->ic_callback = NULL;\r\nspin_unlock(&iclog->ic_callback_lock);\r\nfor (; cb; cb = cb_next) {\r\ncb_next = cb->cb_next;\r\ncb->cb_func(cb->cb_arg, aborted);\r\n}\r\nspin_lock(&iclog->ic_callback_lock);\r\ncb = iclog->ic_callback;\r\n}\r\nloopdidcallbacks++;\r\nfuncdidcallbacks++;\r\nspin_lock(&log->l_icloglock);\r\nASSERT(iclog->ic_callback == NULL);\r\nspin_unlock(&iclog->ic_callback_lock);\r\nif (!(iclog->ic_state & XLOG_STATE_IOERROR))\r\niclog->ic_state = XLOG_STATE_DIRTY;\r\nxlog_state_clean_log(log);\r\nwake_up_all(&iclog->ic_force_wait);\r\niclog = iclog->ic_next;\r\n} while (first_iclog != iclog);\r\nif (repeats > 5000) {\r\nflushcnt += repeats;\r\nrepeats = 0;\r\nxfs_warn(log->l_mp,\r\n"%s: possible infinite loop (%d iterations)",\r\n__func__, flushcnt);\r\n}\r\n} while (!ioerrors && loopdidcallbacks);\r\n#ifdef DEBUG\r\nif (funcdidcallbacks) {\r\nfirst_iclog = iclog = log->l_iclog;\r\ndo {\r\nASSERT(iclog->ic_state != XLOG_STATE_DO_CALLBACK);\r\nif (iclog->ic_state == XLOG_STATE_WANT_SYNC ||\r\niclog->ic_state == XLOG_STATE_SYNCING ||\r\niclog->ic_state == XLOG_STATE_DONE_SYNC ||\r\niclog->ic_state == XLOG_STATE_IOERROR )\r\nbreak;\r\niclog = iclog->ic_next;\r\n} while (first_iclog != iclog);\r\n}\r\n#endif\r\nif (log->l_iclog->ic_state & (XLOG_STATE_ACTIVE|XLOG_STATE_IOERROR))\r\nwake = 1;\r\nspin_unlock(&log->l_icloglock);\r\nif (wake)\r\nwake_up_all(&log->l_flush_wait);\r\n}\r\nSTATIC void\r\nxlog_state_done_syncing(\r\nxlog_in_core_t *iclog,\r\nint aborted)\r\n{\r\nstruct xlog *log = iclog->ic_log;\r\nspin_lock(&log->l_icloglock);\r\nASSERT(iclog->ic_state == XLOG_STATE_SYNCING ||\r\niclog->ic_state == XLOG_STATE_IOERROR);\r\nASSERT(atomic_read(&iclog->ic_refcnt) == 0);\r\nASSERT(iclog->ic_bwritecnt == 1 || iclog->ic_bwritecnt == 2);\r\nif (iclog->ic_state != XLOG_STATE_IOERROR) {\r\nif (--iclog->ic_bwritecnt == 1) {\r\nspin_unlock(&log->l_icloglock);\r\nreturn;\r\n}\r\niclog->ic_state = XLOG_STATE_DONE_SYNC;\r\n}\r\nwake_up_all(&iclog->ic_write_wait);\r\nspin_unlock(&log->l_icloglock);\r\nxlog_state_do_callback(log, aborted, iclog);\r\n}\r\nSTATIC int\r\nxlog_state_get_iclog_space(\r\nstruct xlog *log,\r\nint len,\r\nstruct xlog_in_core **iclogp,\r\nstruct xlog_ticket *ticket,\r\nint *continued_write,\r\nint *logoffsetp)\r\n{\r\nint log_offset;\r\nxlog_rec_header_t *head;\r\nxlog_in_core_t *iclog;\r\nint error;\r\nrestart:\r\nspin_lock(&log->l_icloglock);\r\nif (XLOG_FORCED_SHUTDOWN(log)) {\r\nspin_unlock(&log->l_icloglock);\r\nreturn -EIO;\r\n}\r\niclog = log->l_iclog;\r\nif (iclog->ic_state != XLOG_STATE_ACTIVE) {\r\nXFS_STATS_INC(xs_log_noiclogs);\r\nxlog_wait(&log->l_flush_wait, &log->l_icloglock);\r\ngoto restart;\r\n}\r\nhead = &iclog->ic_header;\r\natomic_inc(&iclog->ic_refcnt);\r\nlog_offset = iclog->ic_offset;\r\nif (log_offset == 0) {\r\nticket->t_curr_res -= log->l_iclog_hsize;\r\nxlog_tic_add_region(ticket,\r\nlog->l_iclog_hsize,\r\nXLOG_REG_TYPE_LRHEADER);\r\nhead->h_cycle = cpu_to_be32(log->l_curr_cycle);\r\nhead->h_lsn = cpu_to_be64(\r\nxlog_assign_lsn(log->l_curr_cycle, log->l_curr_block));\r\nASSERT(log->l_curr_block >= 0);\r\n}\r\nif (iclog->ic_size - iclog->ic_offset < 2*sizeof(xlog_op_header_t)) {\r\nxlog_state_switch_iclogs(log, iclog, iclog->ic_size);\r\nif (!atomic_add_unless(&iclog->ic_refcnt, -1, 1)) {\r\nspin_unlock(&log->l_icloglock);\r\nerror = xlog_state_release_iclog(log, iclog);\r\nif (error)\r\nreturn error;\r\n} else {\r\nspin_unlock(&log->l_icloglock);\r\n}\r\ngoto restart;\r\n}\r\nif (len <= iclog->ic_size - iclog->ic_offset) {\r\n*continued_write = 0;\r\niclog->ic_offset += len;\r\n} else {\r\n*continued_write = 1;\r\nxlog_state_switch_iclogs(log, iclog, iclog->ic_size);\r\n}\r\n*iclogp = iclog;\r\nASSERT(iclog->ic_offset <= iclog->ic_size);\r\nspin_unlock(&log->l_icloglock);\r\n*logoffsetp = log_offset;\r\nreturn 0;\r\n}\r\nSTATIC void\r\nxlog_regrant_reserve_log_space(\r\nstruct xlog *log,\r\nstruct xlog_ticket *ticket)\r\n{\r\ntrace_xfs_log_regrant_reserve_enter(log, ticket);\r\nif (ticket->t_cnt > 0)\r\nticket->t_cnt--;\r\nxlog_grant_sub_space(log, &log->l_reserve_head.grant,\r\nticket->t_curr_res);\r\nxlog_grant_sub_space(log, &log->l_write_head.grant,\r\nticket->t_curr_res);\r\nticket->t_curr_res = ticket->t_unit_res;\r\nxlog_tic_reset_res(ticket);\r\ntrace_xfs_log_regrant_reserve_sub(log, ticket);\r\nif (ticket->t_cnt > 0)\r\nreturn;\r\nxlog_grant_add_space(log, &log->l_reserve_head.grant,\r\nticket->t_unit_res);\r\ntrace_xfs_log_regrant_reserve_exit(log, ticket);\r\nticket->t_curr_res = ticket->t_unit_res;\r\nxlog_tic_reset_res(ticket);\r\n}\r\nSTATIC void\r\nxlog_ungrant_log_space(\r\nstruct xlog *log,\r\nstruct xlog_ticket *ticket)\r\n{\r\nint bytes;\r\nif (ticket->t_cnt > 0)\r\nticket->t_cnt--;\r\ntrace_xfs_log_ungrant_enter(log, ticket);\r\ntrace_xfs_log_ungrant_sub(log, ticket);\r\nbytes = ticket->t_curr_res;\r\nif (ticket->t_cnt > 0) {\r\nASSERT(ticket->t_flags & XLOG_TIC_PERM_RESERV);\r\nbytes += ticket->t_unit_res*ticket->t_cnt;\r\n}\r\nxlog_grant_sub_space(log, &log->l_reserve_head.grant, bytes);\r\nxlog_grant_sub_space(log, &log->l_write_head.grant, bytes);\r\ntrace_xfs_log_ungrant_exit(log, ticket);\r\nxfs_log_space_wake(log->l_mp);\r\n}\r\nSTATIC int\r\nxlog_state_release_iclog(\r\nstruct xlog *log,\r\nstruct xlog_in_core *iclog)\r\n{\r\nint sync = 0;\r\nif (iclog->ic_state & XLOG_STATE_IOERROR)\r\nreturn -EIO;\r\nASSERT(atomic_read(&iclog->ic_refcnt) > 0);\r\nif (!atomic_dec_and_lock(&iclog->ic_refcnt, &log->l_icloglock))\r\nreturn 0;\r\nif (iclog->ic_state & XLOG_STATE_IOERROR) {\r\nspin_unlock(&log->l_icloglock);\r\nreturn -EIO;\r\n}\r\nASSERT(iclog->ic_state == XLOG_STATE_ACTIVE ||\r\niclog->ic_state == XLOG_STATE_WANT_SYNC);\r\nif (iclog->ic_state == XLOG_STATE_WANT_SYNC) {\r\nxfs_lsn_t tail_lsn = xlog_assign_tail_lsn(log->l_mp);\r\nsync++;\r\niclog->ic_state = XLOG_STATE_SYNCING;\r\niclog->ic_header.h_tail_lsn = cpu_to_be64(tail_lsn);\r\nxlog_verify_tail_lsn(log, iclog, tail_lsn);\r\n}\r\nspin_unlock(&log->l_icloglock);\r\nif (sync)\r\nreturn xlog_sync(log, iclog);\r\nreturn 0;\r\n}\r\nSTATIC void\r\nxlog_state_switch_iclogs(\r\nstruct xlog *log,\r\nstruct xlog_in_core *iclog,\r\nint eventual_size)\r\n{\r\nASSERT(iclog->ic_state == XLOG_STATE_ACTIVE);\r\nif (!eventual_size)\r\neventual_size = iclog->ic_offset;\r\niclog->ic_state = XLOG_STATE_WANT_SYNC;\r\niclog->ic_header.h_prev_block = cpu_to_be32(log->l_prev_block);\r\nlog->l_prev_block = log->l_curr_block;\r\nlog->l_prev_cycle = log->l_curr_cycle;\r\nlog->l_curr_block += BTOBB(eventual_size)+BTOBB(log->l_iclog_hsize);\r\nif (xfs_sb_version_haslogv2(&log->l_mp->m_sb) &&\r\nlog->l_mp->m_sb.sb_logsunit > 1) {\r\n__uint32_t sunit_bb = BTOBB(log->l_mp->m_sb.sb_logsunit);\r\nlog->l_curr_block = roundup(log->l_curr_block, sunit_bb);\r\n}\r\nif (log->l_curr_block >= log->l_logBBsize) {\r\nlog->l_curr_cycle++;\r\nif (log->l_curr_cycle == XLOG_HEADER_MAGIC_NUM)\r\nlog->l_curr_cycle++;\r\nlog->l_curr_block -= log->l_logBBsize;\r\nASSERT(log->l_curr_block >= 0);\r\n}\r\nASSERT(iclog == log->l_iclog);\r\nlog->l_iclog = iclog->ic_next;\r\n}\r\nint\r\n_xfs_log_force(\r\nstruct xfs_mount *mp,\r\nuint flags,\r\nint *log_flushed)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nstruct xlog_in_core *iclog;\r\nxfs_lsn_t lsn;\r\nXFS_STATS_INC(xs_log_force);\r\nxlog_cil_force(log);\r\nspin_lock(&log->l_icloglock);\r\niclog = log->l_iclog;\r\nif (iclog->ic_state & XLOG_STATE_IOERROR) {\r\nspin_unlock(&log->l_icloglock);\r\nreturn -EIO;\r\n}\r\nif (iclog->ic_state == XLOG_STATE_ACTIVE ||\r\niclog->ic_state == XLOG_STATE_DIRTY) {\r\nif (iclog->ic_state == XLOG_STATE_DIRTY ||\r\n(atomic_read(&iclog->ic_refcnt) == 0\r\n&& iclog->ic_offset == 0)) {\r\niclog = iclog->ic_prev;\r\nif (iclog->ic_state == XLOG_STATE_ACTIVE ||\r\niclog->ic_state == XLOG_STATE_DIRTY)\r\ngoto no_sleep;\r\nelse\r\ngoto maybe_sleep;\r\n} else {\r\nif (atomic_read(&iclog->ic_refcnt) == 0) {\r\natomic_inc(&iclog->ic_refcnt);\r\nlsn = be64_to_cpu(iclog->ic_header.h_lsn);\r\nxlog_state_switch_iclogs(log, iclog, 0);\r\nspin_unlock(&log->l_icloglock);\r\nif (xlog_state_release_iclog(log, iclog))\r\nreturn -EIO;\r\nif (log_flushed)\r\n*log_flushed = 1;\r\nspin_lock(&log->l_icloglock);\r\nif (be64_to_cpu(iclog->ic_header.h_lsn) == lsn &&\r\niclog->ic_state != XLOG_STATE_DIRTY)\r\ngoto maybe_sleep;\r\nelse\r\ngoto no_sleep;\r\n} else {\r\nxlog_state_switch_iclogs(log, iclog, 0);\r\ngoto maybe_sleep;\r\n}\r\n}\r\n}\r\nmaybe_sleep:\r\nif (flags & XFS_LOG_SYNC) {\r\nif (iclog->ic_state & XLOG_STATE_IOERROR) {\r\nspin_unlock(&log->l_icloglock);\r\nreturn -EIO;\r\n}\r\nXFS_STATS_INC(xs_log_force_sleep);\r\nxlog_wait(&iclog->ic_force_wait, &log->l_icloglock);\r\nif (iclog->ic_state & XLOG_STATE_IOERROR)\r\nreturn -EIO;\r\nif (log_flushed)\r\n*log_flushed = 1;\r\n} else {\r\nno_sleep:\r\nspin_unlock(&log->l_icloglock);\r\n}\r\nreturn 0;\r\n}\r\nvoid\r\nxfs_log_force(\r\nxfs_mount_t *mp,\r\nuint flags)\r\n{\r\nint error;\r\ntrace_xfs_log_force(mp, 0);\r\nerror = _xfs_log_force(mp, flags, NULL);\r\nif (error)\r\nxfs_warn(mp, "%s: error %d returned.", __func__, error);\r\n}\r\nint\r\n_xfs_log_force_lsn(\r\nstruct xfs_mount *mp,\r\nxfs_lsn_t lsn,\r\nuint flags,\r\nint *log_flushed)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nstruct xlog_in_core *iclog;\r\nint already_slept = 0;\r\nASSERT(lsn != 0);\r\nXFS_STATS_INC(xs_log_force);\r\nlsn = xlog_cil_force_lsn(log, lsn);\r\nif (lsn == NULLCOMMITLSN)\r\nreturn 0;\r\ntry_again:\r\nspin_lock(&log->l_icloglock);\r\niclog = log->l_iclog;\r\nif (iclog->ic_state & XLOG_STATE_IOERROR) {\r\nspin_unlock(&log->l_icloglock);\r\nreturn -EIO;\r\n}\r\ndo {\r\nif (be64_to_cpu(iclog->ic_header.h_lsn) != lsn) {\r\niclog = iclog->ic_next;\r\ncontinue;\r\n}\r\nif (iclog->ic_state == XLOG_STATE_DIRTY) {\r\nspin_unlock(&log->l_icloglock);\r\nreturn 0;\r\n}\r\nif (iclog->ic_state == XLOG_STATE_ACTIVE) {\r\nif (!already_slept &&\r\n(iclog->ic_prev->ic_state &\r\n(XLOG_STATE_WANT_SYNC | XLOG_STATE_SYNCING))) {\r\nASSERT(!(iclog->ic_state & XLOG_STATE_IOERROR));\r\nXFS_STATS_INC(xs_log_force_sleep);\r\nxlog_wait(&iclog->ic_prev->ic_write_wait,\r\n&log->l_icloglock);\r\nif (log_flushed)\r\n*log_flushed = 1;\r\nalready_slept = 1;\r\ngoto try_again;\r\n}\r\natomic_inc(&iclog->ic_refcnt);\r\nxlog_state_switch_iclogs(log, iclog, 0);\r\nspin_unlock(&log->l_icloglock);\r\nif (xlog_state_release_iclog(log, iclog))\r\nreturn -EIO;\r\nif (log_flushed)\r\n*log_flushed = 1;\r\nspin_lock(&log->l_icloglock);\r\n}\r\nif ((flags & XFS_LOG_SYNC) &&\r\n!(iclog->ic_state &\r\n(XLOG_STATE_ACTIVE | XLOG_STATE_DIRTY))) {\r\nif (iclog->ic_state & XLOG_STATE_IOERROR) {\r\nspin_unlock(&log->l_icloglock);\r\nreturn -EIO;\r\n}\r\nXFS_STATS_INC(xs_log_force_sleep);\r\nxlog_wait(&iclog->ic_force_wait, &log->l_icloglock);\r\nif (iclog->ic_state & XLOG_STATE_IOERROR)\r\nreturn -EIO;\r\nif (log_flushed)\r\n*log_flushed = 1;\r\n} else {\r\nspin_unlock(&log->l_icloglock);\r\n}\r\nreturn 0;\r\n} while (iclog != log->l_iclog);\r\nspin_unlock(&log->l_icloglock);\r\nreturn 0;\r\n}\r\nvoid\r\nxfs_log_force_lsn(\r\nxfs_mount_t *mp,\r\nxfs_lsn_t lsn,\r\nuint flags)\r\n{\r\nint error;\r\ntrace_xfs_log_force(mp, lsn);\r\nerror = _xfs_log_force_lsn(mp, lsn, flags, NULL);\r\nif (error)\r\nxfs_warn(mp, "%s: error %d returned.", __func__, error);\r\n}\r\nSTATIC void\r\nxlog_state_want_sync(\r\nstruct xlog *log,\r\nstruct xlog_in_core *iclog)\r\n{\r\nassert_spin_locked(&log->l_icloglock);\r\nif (iclog->ic_state == XLOG_STATE_ACTIVE) {\r\nxlog_state_switch_iclogs(log, iclog, 0);\r\n} else {\r\nASSERT(iclog->ic_state &\r\n(XLOG_STATE_WANT_SYNC|XLOG_STATE_IOERROR));\r\n}\r\n}\r\nvoid\r\nxfs_log_ticket_put(\r\nxlog_ticket_t *ticket)\r\n{\r\nASSERT(atomic_read(&ticket->t_ref) > 0);\r\nif (atomic_dec_and_test(&ticket->t_ref))\r\nkmem_zone_free(xfs_log_ticket_zone, ticket);\r\n}\r\nxlog_ticket_t *\r\nxfs_log_ticket_get(\r\nxlog_ticket_t *ticket)\r\n{\r\nASSERT(atomic_read(&ticket->t_ref) > 0);\r\natomic_inc(&ticket->t_ref);\r\nreturn ticket;\r\n}\r\nint\r\nxfs_log_calc_unit_res(\r\nstruct xfs_mount *mp,\r\nint unit_bytes)\r\n{\r\nstruct xlog *log = mp->m_log;\r\nint iclog_space;\r\nuint num_headers;\r\nunit_bytes += sizeof(xlog_op_header_t);\r\nunit_bytes += sizeof(xfs_trans_header_t);\r\nunit_bytes += sizeof(xlog_op_header_t);\r\niclog_space = log->l_iclog_size - log->l_iclog_hsize;\r\nnum_headers = howmany(unit_bytes, iclog_space);\r\nunit_bytes += sizeof(xlog_op_header_t) * num_headers;\r\nwhile (!num_headers ||\r\nhowmany(unit_bytes, iclog_space) > num_headers) {\r\nunit_bytes += sizeof(xlog_op_header_t);\r\nnum_headers++;\r\n}\r\nunit_bytes += log->l_iclog_hsize * num_headers;\r\nunit_bytes += log->l_iclog_hsize;\r\nif (xfs_sb_version_haslogv2(&mp->m_sb) && mp->m_sb.sb_logsunit > 1) {\r\nunit_bytes += 2 * mp->m_sb.sb_logsunit;\r\n} else {\r\nunit_bytes += 2 * BBSIZE;\r\n}\r\nreturn unit_bytes;\r\n}\r\nstruct xlog_ticket *\r\nxlog_ticket_alloc(\r\nstruct xlog *log,\r\nint unit_bytes,\r\nint cnt,\r\nchar client,\r\nbool permanent,\r\nxfs_km_flags_t alloc_flags)\r\n{\r\nstruct xlog_ticket *tic;\r\nint unit_res;\r\ntic = kmem_zone_zalloc(xfs_log_ticket_zone, alloc_flags);\r\nif (!tic)\r\nreturn NULL;\r\nunit_res = xfs_log_calc_unit_res(log->l_mp, unit_bytes);\r\natomic_set(&tic->t_ref, 1);\r\ntic->t_task = current;\r\nINIT_LIST_HEAD(&tic->t_queue);\r\ntic->t_unit_res = unit_res;\r\ntic->t_curr_res = unit_res;\r\ntic->t_cnt = cnt;\r\ntic->t_ocnt = cnt;\r\ntic->t_tid = prandom_u32();\r\ntic->t_clientid = client;\r\ntic->t_flags = XLOG_TIC_INITED;\r\ntic->t_trans_type = 0;\r\nif (permanent)\r\ntic->t_flags |= XLOG_TIC_PERM_RESERV;\r\nxlog_tic_reset_res(tic);\r\nreturn tic;\r\n}\r\nvoid\r\nxlog_verify_dest_ptr(\r\nstruct xlog *log,\r\nchar *ptr)\r\n{\r\nint i;\r\nint good_ptr = 0;\r\nfor (i = 0; i < log->l_iclog_bufs; i++) {\r\nif (ptr >= log->l_iclog_bak[i] &&\r\nptr <= log->l_iclog_bak[i] + log->l_iclog_size)\r\ngood_ptr++;\r\n}\r\nif (!good_ptr)\r\nxfs_emerg(log->l_mp, "%s: invalid ptr", __func__);\r\n}\r\nSTATIC void\r\nxlog_verify_grant_tail(\r\nstruct xlog *log)\r\n{\r\nint tail_cycle, tail_blocks;\r\nint cycle, space;\r\nxlog_crack_grant_head(&log->l_write_head.grant, &cycle, &space);\r\nxlog_crack_atomic_lsn(&log->l_tail_lsn, &tail_cycle, &tail_blocks);\r\nif (tail_cycle != cycle) {\r\nif (cycle - 1 != tail_cycle &&\r\n!(log->l_flags & XLOG_TAIL_WARN)) {\r\nxfs_alert_tag(log->l_mp, XFS_PTAG_LOGRES,\r\n"%s: cycle - 1 != tail_cycle", __func__);\r\nlog->l_flags |= XLOG_TAIL_WARN;\r\n}\r\nif (space > BBTOB(tail_blocks) &&\r\n!(log->l_flags & XLOG_TAIL_WARN)) {\r\nxfs_alert_tag(log->l_mp, XFS_PTAG_LOGRES,\r\n"%s: space > BBTOB(tail_blocks)", __func__);\r\nlog->l_flags |= XLOG_TAIL_WARN;\r\n}\r\n}\r\n}\r\nSTATIC void\r\nxlog_verify_tail_lsn(\r\nstruct xlog *log,\r\nstruct xlog_in_core *iclog,\r\nxfs_lsn_t tail_lsn)\r\n{\r\nint blocks;\r\nif (CYCLE_LSN(tail_lsn) == log->l_prev_cycle) {\r\nblocks =\r\nlog->l_logBBsize - (log->l_prev_block - BLOCK_LSN(tail_lsn));\r\nif (blocks < BTOBB(iclog->ic_offset)+BTOBB(log->l_iclog_hsize))\r\nxfs_emerg(log->l_mp, "%s: ran out of log space", __func__);\r\n} else {\r\nASSERT(CYCLE_LSN(tail_lsn)+1 == log->l_prev_cycle);\r\nif (BLOCK_LSN(tail_lsn) == log->l_prev_block)\r\nxfs_emerg(log->l_mp, "%s: tail wrapped", __func__);\r\nblocks = BLOCK_LSN(tail_lsn) - log->l_prev_block;\r\nif (blocks < BTOBB(iclog->ic_offset) + 1)\r\nxfs_emerg(log->l_mp, "%s: ran out of log space", __func__);\r\n}\r\n}\r\nSTATIC void\r\nxlog_verify_iclog(\r\nstruct xlog *log,\r\nstruct xlog_in_core *iclog,\r\nint count,\r\nbool syncing)\r\n{\r\nxlog_op_header_t *ophead;\r\nxlog_in_core_t *icptr;\r\nxlog_in_core_2_t *xhdr;\r\nxfs_caddr_t ptr;\r\nxfs_caddr_t base_ptr;\r\n__psint_t field_offset;\r\n__uint8_t clientid;\r\nint len, i, j, k, op_len;\r\nint idx;\r\nspin_lock(&log->l_icloglock);\r\nicptr = log->l_iclog;\r\nfor (i = 0; i < log->l_iclog_bufs; i++, icptr = icptr->ic_next)\r\nASSERT(icptr);\r\nif (icptr != log->l_iclog)\r\nxfs_emerg(log->l_mp, "%s: corrupt iclog ring", __func__);\r\nspin_unlock(&log->l_icloglock);\r\nif (iclog->ic_header.h_magicno != cpu_to_be32(XLOG_HEADER_MAGIC_NUM))\r\nxfs_emerg(log->l_mp, "%s: invalid magic num", __func__);\r\nptr = (xfs_caddr_t) &iclog->ic_header;\r\nfor (ptr += BBSIZE; ptr < ((xfs_caddr_t)&iclog->ic_header) + count;\r\nptr += BBSIZE) {\r\nif (*(__be32 *)ptr == cpu_to_be32(XLOG_HEADER_MAGIC_NUM))\r\nxfs_emerg(log->l_mp, "%s: unexpected magic num",\r\n__func__);\r\n}\r\nlen = be32_to_cpu(iclog->ic_header.h_num_logops);\r\nptr = iclog->ic_datap;\r\nbase_ptr = ptr;\r\nophead = (xlog_op_header_t *)ptr;\r\nxhdr = iclog->ic_data;\r\nfor (i = 0; i < len; i++) {\r\nophead = (xlog_op_header_t *)ptr;\r\nfield_offset = (__psint_t)\r\n((xfs_caddr_t)&(ophead->oh_clientid) - base_ptr);\r\nif (!syncing || (field_offset & 0x1ff)) {\r\nclientid = ophead->oh_clientid;\r\n} else {\r\nidx = BTOBBT((xfs_caddr_t)&(ophead->oh_clientid) - iclog->ic_datap);\r\nif (idx >= (XLOG_HEADER_CYCLE_SIZE / BBSIZE)) {\r\nj = idx / (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\r\nk = idx % (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\r\nclientid = xlog_get_client_id(\r\nxhdr[j].hic_xheader.xh_cycle_data[k]);\r\n} else {\r\nclientid = xlog_get_client_id(\r\niclog->ic_header.h_cycle_data[idx]);\r\n}\r\n}\r\nif (clientid != XFS_TRANSACTION && clientid != XFS_LOG)\r\nxfs_warn(log->l_mp,\r\n"%s: invalid clientid %d op 0x%p offset 0x%lx",\r\n__func__, clientid, ophead,\r\n(unsigned long)field_offset);\r\nfield_offset = (__psint_t)\r\n((xfs_caddr_t)&(ophead->oh_len) - base_ptr);\r\nif (!syncing || (field_offset & 0x1ff)) {\r\nop_len = be32_to_cpu(ophead->oh_len);\r\n} else {\r\nidx = BTOBBT((__psint_t)&ophead->oh_len -\r\n(__psint_t)iclog->ic_datap);\r\nif (idx >= (XLOG_HEADER_CYCLE_SIZE / BBSIZE)) {\r\nj = idx / (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\r\nk = idx % (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\r\nop_len = be32_to_cpu(xhdr[j].hic_xheader.xh_cycle_data[k]);\r\n} else {\r\nop_len = be32_to_cpu(iclog->ic_header.h_cycle_data[idx]);\r\n}\r\n}\r\nptr += sizeof(xlog_op_header_t) + op_len;\r\n}\r\n}\r\nSTATIC int\r\nxlog_state_ioerror(\r\nstruct xlog *log)\r\n{\r\nxlog_in_core_t *iclog, *ic;\r\niclog = log->l_iclog;\r\nif (! (iclog->ic_state & XLOG_STATE_IOERROR)) {\r\nic = iclog;\r\ndo {\r\nic->ic_state = XLOG_STATE_IOERROR;\r\nic = ic->ic_next;\r\n} while (ic != iclog);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nint\r\nxfs_log_force_umount(\r\nstruct xfs_mount *mp,\r\nint logerror)\r\n{\r\nstruct xlog *log;\r\nint retval;\r\nlog = mp->m_log;\r\nif (!log ||\r\nlog->l_flags & XLOG_ACTIVE_RECOVERY) {\r\nmp->m_flags |= XFS_MOUNT_FS_SHUTDOWN;\r\nif (mp->m_sb_bp)\r\nXFS_BUF_DONE(mp->m_sb_bp);\r\nreturn 0;\r\n}\r\nif (logerror && log->l_iclog->ic_state & XLOG_STATE_IOERROR) {\r\nASSERT(XLOG_FORCED_SHUTDOWN(log));\r\nreturn 1;\r\n}\r\nif (!logerror)\r\n_xfs_log_force(mp, XFS_LOG_SYNC, NULL);\r\nspin_lock(&log->l_icloglock);\r\nmp->m_flags |= XFS_MOUNT_FS_SHUTDOWN;\r\nif (mp->m_sb_bp)\r\nXFS_BUF_DONE(mp->m_sb_bp);\r\nlog->l_flags |= XLOG_IO_ERROR;\r\nretval = xlog_state_ioerror(log);\r\nspin_unlock(&log->l_icloglock);\r\nxlog_grant_head_wake_all(&log->l_reserve_head);\r\nxlog_grant_head_wake_all(&log->l_write_head);\r\nwake_up_all(&log->l_cilp->xc_commit_wait);\r\nxlog_state_do_callback(log, XFS_LI_ABORTED, NULL);\r\n#ifdef XFSERRORDEBUG\r\n{\r\nxlog_in_core_t *iclog;\r\nspin_lock(&log->l_icloglock);\r\niclog = log->l_iclog;\r\ndo {\r\nASSERT(iclog->ic_callback == 0);\r\niclog = iclog->ic_next;\r\n} while (iclog != log->l_iclog);\r\nspin_unlock(&log->l_icloglock);\r\n}\r\n#endif\r\nreturn retval;\r\n}\r\nSTATIC int\r\nxlog_iclogs_empty(\r\nstruct xlog *log)\r\n{\r\nxlog_in_core_t *iclog;\r\niclog = log->l_iclog;\r\ndo {\r\nif (iclog->ic_header.h_num_logops)\r\nreturn 0;\r\niclog = iclog->ic_next;\r\n} while (iclog != log->l_iclog);\r\nreturn 1;\r\n}
