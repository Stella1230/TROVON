static int param_get_local64(char *buffer, const struct kernel_param *kp)\r\n{\r\nreturn sprintf(buffer, "%lu\n", local64_read((local64_t *)kp->arg));\r\n}\r\nstatic int param_set_local64(const char *val, const struct kernel_param *kp)\r\n{\r\nlocal64_set((local64_t *)kp->arg, 0);\r\nreturn 0;\r\n}\r\nstatic inline bool uv_nmi_action_is(const char *action)\r\n{\r\nreturn (strncmp(uv_nmi_action, action, strlen(action)) == 0);\r\n}\r\nstatic void uv_nmi_setup_mmrs(void)\r\n{\r\nif (uv_read_local_mmr(UVH_NMI_MMRX_SUPPORTED)) {\r\nuv_write_local_mmr(UVH_NMI_MMRX_REQ,\r\n1UL << UVH_NMI_MMRX_REQ_SHIFT);\r\nnmi_mmr = UVH_NMI_MMRX;\r\nnmi_mmr_clear = UVH_NMI_MMRX_CLEAR;\r\nnmi_mmr_pending = 1UL << UVH_NMI_MMRX_SHIFT;\r\npr_info("UV: SMI NMI support: %s\n", UVH_NMI_MMRX_TYPE);\r\n} else {\r\nnmi_mmr = UVH_NMI_MMR;\r\nnmi_mmr_clear = UVH_NMI_MMR_CLEAR;\r\nnmi_mmr_pending = 1UL << UVH_NMI_MMR_SHIFT;\r\npr_info("UV: SMI NMI support: %s\n", UVH_NMI_MMR_TYPE);\r\n}\r\n}\r\nstatic inline int uv_nmi_test_mmr(struct uv_hub_nmi_s *hub_nmi)\r\n{\r\nhub_nmi->nmi_value = uv_read_local_mmr(nmi_mmr);\r\natomic_inc(&hub_nmi->read_mmr_count);\r\nreturn !!(hub_nmi->nmi_value & nmi_mmr_pending);\r\n}\r\nstatic inline void uv_local_mmr_clear_nmi(void)\r\n{\r\nuv_write_local_mmr(nmi_mmr_clear, nmi_mmr_pending);\r\n}\r\nstatic int uv_set_in_nmi(int cpu, struct uv_hub_nmi_s *hub_nmi)\r\n{\r\nint first = atomic_add_unless(&hub_nmi->in_nmi, 1, 1);\r\nif (first) {\r\natomic_set(&hub_nmi->cpu_owner, cpu);\r\nif (atomic_add_unless(&uv_in_nmi, 1, 1))\r\natomic_set(&uv_nmi_cpu, cpu);\r\natomic_inc(&hub_nmi->nmi_count);\r\n}\r\nreturn first;\r\n}\r\nstatic int uv_check_nmi(struct uv_hub_nmi_s *hub_nmi)\r\n{\r\nint cpu = smp_processor_id();\r\nint nmi = 0;\r\nlocal64_inc(&uv_nmi_count);\r\nthis_cpu_inc(uv_cpu_nmi.queries);\r\ndo {\r\nnmi = atomic_read(&hub_nmi->in_nmi);\r\nif (nmi)\r\nbreak;\r\nif (raw_spin_trylock(&hub_nmi->nmi_lock)) {\r\nif (uv_nmi_test_mmr(hub_nmi)) {\r\nuv_set_in_nmi(cpu, hub_nmi);\r\nnmi = 1;\r\nbreak;\r\n}\r\nraw_spin_unlock(&hub_nmi->nmi_lock);\r\n} else {\r\ncpu_relax();\r\nudelay(uv_nmi_slave_delay);\r\nnmi = atomic_read(&hub_nmi->in_nmi);\r\nif (nmi)\r\nbreak;\r\n}\r\nif (!nmi) {\r\nnmi = atomic_read(&uv_in_nmi);\r\nif (nmi)\r\nuv_set_in_nmi(cpu, hub_nmi);\r\n}\r\n} while (0);\r\nif (!nmi)\r\nlocal64_inc(&uv_nmi_misses);\r\nreturn nmi;\r\n}\r\nstatic inline void uv_clear_nmi(int cpu)\r\n{\r\nstruct uv_hub_nmi_s *hub_nmi = uv_hub_nmi;\r\nif (cpu == atomic_read(&hub_nmi->cpu_owner)) {\r\natomic_set(&hub_nmi->cpu_owner, -1);\r\natomic_set(&hub_nmi->in_nmi, 0);\r\nuv_local_mmr_clear_nmi();\r\nraw_spin_unlock(&hub_nmi->nmi_lock);\r\n}\r\n}\r\nstatic void uv_nmi_nr_cpus_ping(void)\r\n{\r\nint cpu;\r\nfor_each_cpu(cpu, uv_nmi_cpu_mask)\r\nuv_cpu_nmi_per(cpu).pinging = 1;\r\napic->send_IPI_mask(uv_nmi_cpu_mask, APIC_DM_NMI);\r\n}\r\nstatic void uv_nmi_cleanup_mask(void)\r\n{\r\nint cpu;\r\nfor_each_cpu(cpu, uv_nmi_cpu_mask) {\r\nuv_cpu_nmi_per(cpu).pinging = 0;\r\nuv_cpu_nmi_per(cpu).state = UV_NMI_STATE_OUT;\r\ncpumask_clear_cpu(cpu, uv_nmi_cpu_mask);\r\n}\r\n}\r\nstatic int uv_nmi_wait_cpus(int first)\r\n{\r\nint i, j, k, n = num_online_cpus();\r\nint last_k = 0, waiting = 0;\r\nif (first) {\r\ncpumask_copy(uv_nmi_cpu_mask, cpu_online_mask);\r\nk = 0;\r\n} else {\r\nk = n - cpumask_weight(uv_nmi_cpu_mask);\r\n}\r\nudelay(uv_nmi_initial_delay);\r\nfor (i = 0; i < uv_nmi_retry_count; i++) {\r\nint loop_delay = uv_nmi_loop_delay;\r\nfor_each_cpu(j, uv_nmi_cpu_mask) {\r\nif (uv_cpu_nmi_per(j).state) {\r\ncpumask_clear_cpu(j, uv_nmi_cpu_mask);\r\nif (++k >= n)\r\nbreak;\r\n}\r\n}\r\nif (k >= n) {\r\nk = n;\r\nbreak;\r\n}\r\nif (last_k != k) {\r\nlast_k = k;\r\nwaiting = 0;\r\n} else if (++waiting > uv_nmi_wait_count)\r\nbreak;\r\nif (waiting && (n - k) == 1 &&\r\ncpumask_test_cpu(0, uv_nmi_cpu_mask))\r\nloop_delay *= 100;\r\nudelay(loop_delay);\r\n}\r\natomic_set(&uv_nmi_cpus_in_nmi, k);\r\nreturn n - k;\r\n}\r\nstatic void uv_nmi_wait(int master)\r\n{\r\nthis_cpu_write(uv_cpu_nmi.state, UV_NMI_STATE_IN);\r\nif (!master)\r\nreturn;\r\ndo {\r\nif (!uv_nmi_wait_cpus(1))\r\nbreak;\r\npr_alert("UV: Sending NMI IPI to %d non-responding CPUs: %*pbl\n",\r\ncpumask_weight(uv_nmi_cpu_mask),\r\ncpumask_pr_args(uv_nmi_cpu_mask));\r\nuv_nmi_nr_cpus_ping();\r\nif (!uv_nmi_wait_cpus(0))\r\nbreak;\r\npr_alert("UV: %d CPUs not in NMI loop: %*pbl\n",\r\ncpumask_weight(uv_nmi_cpu_mask),\r\ncpumask_pr_args(uv_nmi_cpu_mask));\r\n} while (0);\r\npr_alert("UV: %d of %d CPUs in NMI\n",\r\natomic_read(&uv_nmi_cpus_in_nmi), num_online_cpus());\r\n}\r\nstatic void uv_nmi_dump_cpu_ip_hdr(void)\r\n{\r\nprintk(KERN_DEFAULT\r\n"\nUV: %4s %6s %-32s %s (Note: PID 0 not listed)\n",\r\n"CPU", "PID", "COMMAND", "IP");\r\n}\r\nstatic void uv_nmi_dump_cpu_ip(int cpu, struct pt_regs *regs)\r\n{\r\nprintk(KERN_DEFAULT "UV: %4d %6d %-32.32s ",\r\ncpu, current->pid, current->comm);\r\nprintk_address(regs->ip);\r\n}\r\nstatic void uv_nmi_dump_state_cpu(int cpu, struct pt_regs *regs)\r\n{\r\nconst char *dots = " ................................. ";\r\nif (uv_nmi_action_is("ips")) {\r\nif (cpu == 0)\r\nuv_nmi_dump_cpu_ip_hdr();\r\nif (current->pid != 0)\r\nuv_nmi_dump_cpu_ip(cpu, regs);\r\n} else if (uv_nmi_action_is("dump")) {\r\nprintk(KERN_DEFAULT\r\n"UV:%sNMI process trace for CPU %d\n", dots, cpu);\r\nshow_regs(regs);\r\n}\r\nthis_cpu_write(uv_cpu_nmi.state, UV_NMI_STATE_DUMP_DONE);\r\n}\r\nstatic void uv_nmi_trigger_dump(int cpu)\r\n{\r\nint retry = uv_nmi_trigger_delay;\r\nif (uv_cpu_nmi_per(cpu).state != UV_NMI_STATE_IN)\r\nreturn;\r\nuv_cpu_nmi_per(cpu).state = UV_NMI_STATE_DUMP;\r\ndo {\r\ncpu_relax();\r\nudelay(10);\r\nif (uv_cpu_nmi_per(cpu).state\r\n!= UV_NMI_STATE_DUMP)\r\nreturn;\r\n} while (--retry > 0);\r\npr_crit("UV: CPU %d stuck in process dump function\n", cpu);\r\nuv_cpu_nmi_per(cpu).state = UV_NMI_STATE_DUMP_DONE;\r\n}\r\nstatic void uv_nmi_sync_exit(int master)\r\n{\r\natomic_dec(&uv_nmi_cpus_in_nmi);\r\nif (master) {\r\nwhile (atomic_read(&uv_nmi_cpus_in_nmi) > 0)\r\ncpu_relax();\r\natomic_set(&uv_nmi_slave_continue, SLAVE_CLEAR);\r\n} else {\r\nwhile (atomic_read(&uv_nmi_slave_continue))\r\ncpu_relax();\r\n}\r\n}\r\nstatic void uv_nmi_dump_state(int cpu, struct pt_regs *regs, int master)\r\n{\r\nif (master) {\r\nint tcpu;\r\nint ignored = 0;\r\nint saved_console_loglevel = console_loglevel;\r\npr_alert("UV: tracing %s for %d CPUs from CPU %d\n",\r\nuv_nmi_action_is("ips") ? "IPs" : "processes",\r\natomic_read(&uv_nmi_cpus_in_nmi), cpu);\r\nconsole_loglevel = uv_nmi_loglevel;\r\natomic_set(&uv_nmi_slave_continue, SLAVE_EXIT);\r\nfor_each_online_cpu(tcpu) {\r\nif (cpumask_test_cpu(tcpu, uv_nmi_cpu_mask))\r\nignored++;\r\nelse if (tcpu == cpu)\r\nuv_nmi_dump_state_cpu(tcpu, regs);\r\nelse\r\nuv_nmi_trigger_dump(tcpu);\r\n}\r\nif (ignored)\r\nprintk(KERN_DEFAULT "UV: %d CPUs ignored NMI\n",\r\nignored);\r\nconsole_loglevel = saved_console_loglevel;\r\npr_alert("UV: process trace complete\n");\r\n} else {\r\nwhile (!atomic_read(&uv_nmi_slave_continue))\r\ncpu_relax();\r\nwhile (this_cpu_read(uv_cpu_nmi.state) != UV_NMI_STATE_DUMP)\r\ncpu_relax();\r\nuv_nmi_dump_state_cpu(cpu, regs);\r\n}\r\nuv_nmi_sync_exit(master);\r\n}\r\nstatic void uv_nmi_touch_watchdogs(void)\r\n{\r\ntouch_softlockup_watchdog_sync();\r\nclocksource_touch_watchdog();\r\nrcu_cpu_stall_reset();\r\ntouch_nmi_watchdog();\r\n}\r\nstatic void uv_nmi_kdump(int cpu, int master, struct pt_regs *regs)\r\n{\r\nif (master) {\r\npr_emerg("UV: NMI executing crash_kexec on CPU%d\n", cpu);\r\ncrash_kexec(regs);\r\npr_emerg("UV: crash_kexec unexpectedly returned, ");\r\nif (!kexec_crash_image) {\r\npr_cont("crash kernel not loaded\n");\r\natomic_set(&uv_nmi_kexec_failed, 1);\r\nuv_nmi_sync_exit(1);\r\nreturn;\r\n}\r\npr_cont("kexec busy, stalling cpus while waiting\n");\r\n}\r\nwhile (atomic_read(&uv_nmi_kexec_failed) == 0)\r\nmdelay(10);\r\nuv_nmi_sync_exit(0);\r\n}\r\nstatic inline void uv_nmi_kdump(int cpu, int master, struct pt_regs *regs)\r\n{\r\nif (master)\r\npr_err("UV: NMI kdump: KEXEC not supported in this kernel\n");\r\n}\r\nstatic inline int uv_nmi_kdb_reason(void)\r\n{\r\nreturn KDB_REASON_SYSTEM_NMI;\r\n}\r\nstatic inline int uv_nmi_kdb_reason(void)\r\n{\r\nif (uv_nmi_action_is("kgdb"))\r\nreturn 0;\r\npr_err("UV: NMI error: KDB is not enabled in this kernel\n");\r\nreturn -1;\r\n}\r\nstatic void uv_call_kgdb_kdb(int cpu, struct pt_regs *regs, int master)\r\n{\r\nif (master) {\r\nint reason = uv_nmi_kdb_reason();\r\nint ret;\r\nif (reason < 0)\r\nreturn;\r\nret = kgdb_nmicallin(cpu, X86_TRAP_NMI, regs, reason,\r\n&uv_nmi_slave_continue);\r\nif (ret) {\r\npr_alert("KGDB returned error, is kgdboc set?\n");\r\natomic_set(&uv_nmi_slave_continue, SLAVE_EXIT);\r\n}\r\n} else {\r\nint sig;\r\ndo {\r\ncpu_relax();\r\nsig = atomic_read(&uv_nmi_slave_continue);\r\n} while (!sig);\r\nif (sig == SLAVE_CONTINUE)\r\nkgdb_nmicallback(cpu, regs);\r\n}\r\nuv_nmi_sync_exit(master);\r\n}\r\nstatic inline void uv_call_kgdb_kdb(int cpu, struct pt_regs *regs, int master)\r\n{\r\npr_err("UV: NMI error: KGDB is not enabled in this kernel\n");\r\n}\r\nint uv_handle_nmi(unsigned int reason, struct pt_regs *regs)\r\n{\r\nstruct uv_hub_nmi_s *hub_nmi = uv_hub_nmi;\r\nint cpu = smp_processor_id();\r\nint master = 0;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nif (!this_cpu_read(uv_cpu_nmi.pinging) && !uv_check_nmi(hub_nmi)) {\r\nlocal_irq_restore(flags);\r\nreturn NMI_DONE;\r\n}\r\nmaster = (atomic_read(&uv_nmi_cpu) == cpu);\r\nif (uv_nmi_action_is("kdump"))\r\nuv_nmi_kdump(cpu, master, regs);\r\nuv_nmi_wait(master);\r\nif (uv_nmi_action_is("ips") || uv_nmi_action_is("dump"))\r\nuv_nmi_dump_state(cpu, regs, master);\r\nelse if (uv_nmi_action_is("kdb") || uv_nmi_action_is("kgdb"))\r\nuv_call_kgdb_kdb(cpu, regs, master);\r\nthis_cpu_write(uv_cpu_nmi.state, UV_NMI_STATE_OUT);\r\nuv_clear_nmi(cpu);\r\nif (master) {\r\nif (cpumask_weight(uv_nmi_cpu_mask))\r\nuv_nmi_cleanup_mask();\r\natomic_set(&uv_nmi_cpus_in_nmi, -1);\r\natomic_set(&uv_nmi_cpu, -1);\r\natomic_set(&uv_in_nmi, 0);\r\n}\r\nuv_nmi_touch_watchdogs();\r\nlocal_irq_restore(flags);\r\nreturn NMI_HANDLED;\r\n}\r\nstatic int uv_handle_nmi_ping(unsigned int reason, struct pt_regs *regs)\r\n{\r\nint ret;\r\nthis_cpu_inc(uv_cpu_nmi.queries);\r\nif (!this_cpu_read(uv_cpu_nmi.pinging)) {\r\nlocal64_inc(&uv_nmi_ping_misses);\r\nreturn NMI_DONE;\r\n}\r\nthis_cpu_inc(uv_cpu_nmi.pings);\r\nlocal64_inc(&uv_nmi_ping_count);\r\nret = uv_handle_nmi(reason, regs);\r\nthis_cpu_write(uv_cpu_nmi.pinging, 0);\r\nreturn ret;\r\n}\r\nstatic void uv_register_nmi_notifier(void)\r\n{\r\nif (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))\r\npr_warn("UV: NMI handler failed to register\n");\r\nif (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))\r\npr_warn("UV: PING NMI handler failed to register\n");\r\n}\r\nvoid uv_nmi_init(void)\r\n{\r\nunsigned int value;\r\nvalue = apic_read(APIC_LVT1) | APIC_DM_NMI;\r\nvalue &= ~APIC_LVT_MASKED;\r\napic_write(APIC_LVT1, value);\r\n}\r\nvoid uv_nmi_setup(void)\r\n{\r\nint size = sizeof(void *) * (1 << NODES_SHIFT);\r\nint cpu, nid;\r\nuv_nmi_setup_mmrs();\r\nuv_hub_nmi_list = kzalloc(size, GFP_KERNEL);\r\npr_info("UV: NMI hub list @ 0x%p (%d)\n", uv_hub_nmi_list, size);\r\nBUG_ON(!uv_hub_nmi_list);\r\nsize = sizeof(struct uv_hub_nmi_s);\r\nfor_each_present_cpu(cpu) {\r\nnid = cpu_to_node(cpu);\r\nif (uv_hub_nmi_list[nid] == NULL) {\r\nuv_hub_nmi_list[nid] = kzalloc_node(size,\r\nGFP_KERNEL, nid);\r\nBUG_ON(!uv_hub_nmi_list[nid]);\r\nraw_spin_lock_init(&(uv_hub_nmi_list[nid]->nmi_lock));\r\natomic_set(&uv_hub_nmi_list[nid]->cpu_owner, -1);\r\n}\r\nuv_hub_nmi_per(cpu) = uv_hub_nmi_list[nid];\r\n}\r\nBUG_ON(!alloc_cpumask_var(&uv_nmi_cpu_mask, GFP_KERNEL));\r\nuv_register_nmi_notifier();\r\n}
