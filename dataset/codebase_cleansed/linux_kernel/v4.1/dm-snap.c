struct dm_dev *dm_snap_origin(struct dm_snapshot *s)\r\n{\r\nreturn s->origin;\r\n}\r\nstruct dm_dev *dm_snap_cow(struct dm_snapshot *s)\r\n{\r\nreturn s->cow;\r\n}\r\nstatic sector_t chunk_to_sector(struct dm_exception_store *store,\r\nchunk_t chunk)\r\n{\r\nreturn chunk << store->chunk_shift;\r\n}\r\nstatic int bdev_equal(struct block_device *lhs, struct block_device *rhs)\r\n{\r\nreturn lhs == rhs;\r\n}\r\nstatic void init_tracked_chunk(struct bio *bio)\r\n{\r\nstruct dm_snap_tracked_chunk *c = dm_per_bio_data(bio, sizeof(struct dm_snap_tracked_chunk));\r\nINIT_HLIST_NODE(&c->node);\r\n}\r\nstatic bool is_bio_tracked(struct bio *bio)\r\n{\r\nstruct dm_snap_tracked_chunk *c = dm_per_bio_data(bio, sizeof(struct dm_snap_tracked_chunk));\r\nreturn !hlist_unhashed(&c->node);\r\n}\r\nstatic void track_chunk(struct dm_snapshot *s, struct bio *bio, chunk_t chunk)\r\n{\r\nstruct dm_snap_tracked_chunk *c = dm_per_bio_data(bio, sizeof(struct dm_snap_tracked_chunk));\r\nc->chunk = chunk;\r\nspin_lock_irq(&s->tracked_chunk_lock);\r\nhlist_add_head(&c->node,\r\n&s->tracked_chunk_hash[DM_TRACKED_CHUNK_HASH(chunk)]);\r\nspin_unlock_irq(&s->tracked_chunk_lock);\r\n}\r\nstatic void stop_tracking_chunk(struct dm_snapshot *s, struct bio *bio)\r\n{\r\nstruct dm_snap_tracked_chunk *c = dm_per_bio_data(bio, sizeof(struct dm_snap_tracked_chunk));\r\nunsigned long flags;\r\nspin_lock_irqsave(&s->tracked_chunk_lock, flags);\r\nhlist_del(&c->node);\r\nspin_unlock_irqrestore(&s->tracked_chunk_lock, flags);\r\n}\r\nstatic int __chunk_is_tracked(struct dm_snapshot *s, chunk_t chunk)\r\n{\r\nstruct dm_snap_tracked_chunk *c;\r\nint found = 0;\r\nspin_lock_irq(&s->tracked_chunk_lock);\r\nhlist_for_each_entry(c,\r\n&s->tracked_chunk_hash[DM_TRACKED_CHUNK_HASH(chunk)], node) {\r\nif (c->chunk == chunk) {\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irq(&s->tracked_chunk_lock);\r\nreturn found;\r\n}\r\nstatic void __check_for_conflicting_io(struct dm_snapshot *s, chunk_t chunk)\r\n{\r\nwhile (__chunk_is_tracked(s, chunk))\r\nmsleep(1);\r\n}\r\nstatic int init_origin_hash(void)\r\n{\r\nint i;\r\n_origins = kmalloc(ORIGIN_HASH_SIZE * sizeof(struct list_head),\r\nGFP_KERNEL);\r\nif (!_origins) {\r\nDMERR("unable to allocate memory for _origins");\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < ORIGIN_HASH_SIZE; i++)\r\nINIT_LIST_HEAD(_origins + i);\r\n_dm_origins = kmalloc(ORIGIN_HASH_SIZE * sizeof(struct list_head),\r\nGFP_KERNEL);\r\nif (!_dm_origins) {\r\nDMERR("unable to allocate memory for _dm_origins");\r\nkfree(_origins);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < ORIGIN_HASH_SIZE; i++)\r\nINIT_LIST_HEAD(_dm_origins + i);\r\ninit_rwsem(&_origins_lock);\r\nreturn 0;\r\n}\r\nstatic void exit_origin_hash(void)\r\n{\r\nkfree(_origins);\r\nkfree(_dm_origins);\r\n}\r\nstatic unsigned origin_hash(struct block_device *bdev)\r\n{\r\nreturn bdev->bd_dev & ORIGIN_MASK;\r\n}\r\nstatic struct origin *__lookup_origin(struct block_device *origin)\r\n{\r\nstruct list_head *ol;\r\nstruct origin *o;\r\nol = &_origins[origin_hash(origin)];\r\nlist_for_each_entry (o, ol, hash_list)\r\nif (bdev_equal(o->bdev, origin))\r\nreturn o;\r\nreturn NULL;\r\n}\r\nstatic void __insert_origin(struct origin *o)\r\n{\r\nstruct list_head *sl = &_origins[origin_hash(o->bdev)];\r\nlist_add_tail(&o->hash_list, sl);\r\n}\r\nstatic struct dm_origin *__lookup_dm_origin(struct block_device *origin)\r\n{\r\nstruct list_head *ol;\r\nstruct dm_origin *o;\r\nol = &_dm_origins[origin_hash(origin)];\r\nlist_for_each_entry (o, ol, hash_list)\r\nif (bdev_equal(o->dev->bdev, origin))\r\nreturn o;\r\nreturn NULL;\r\n}\r\nstatic void __insert_dm_origin(struct dm_origin *o)\r\n{\r\nstruct list_head *sl = &_dm_origins[origin_hash(o->dev->bdev)];\r\nlist_add_tail(&o->hash_list, sl);\r\n}\r\nstatic void __remove_dm_origin(struct dm_origin *o)\r\n{\r\nlist_del(&o->hash_list);\r\n}\r\nstatic int __find_snapshots_sharing_cow(struct dm_snapshot *snap,\r\nstruct dm_snapshot **snap_src,\r\nstruct dm_snapshot **snap_dest,\r\nstruct dm_snapshot **snap_merge)\r\n{\r\nstruct dm_snapshot *s;\r\nstruct origin *o;\r\nint count = 0;\r\nint active;\r\no = __lookup_origin(snap->origin->bdev);\r\nif (!o)\r\ngoto out;\r\nlist_for_each_entry(s, &o->snapshots, list) {\r\nif (dm_target_is_snapshot_merge(s->ti) && snap_merge)\r\n*snap_merge = s;\r\nif (!bdev_equal(s->cow->bdev, snap->cow->bdev))\r\ncontinue;\r\ndown_read(&s->lock);\r\nactive = s->active;\r\nup_read(&s->lock);\r\nif (active) {\r\nif (snap_src)\r\n*snap_src = s;\r\n} else if (snap_dest)\r\n*snap_dest = s;\r\ncount++;\r\n}\r\nout:\r\nreturn count;\r\n}\r\nstatic int __validate_exception_handover(struct dm_snapshot *snap)\r\n{\r\nstruct dm_snapshot *snap_src = NULL, *snap_dest = NULL;\r\nstruct dm_snapshot *snap_merge = NULL;\r\nif ((__find_snapshots_sharing_cow(snap, &snap_src, &snap_dest,\r\n&snap_merge) == 2) ||\r\nsnap_dest) {\r\nsnap->ti->error = "Snapshot cow pairing for exception "\r\n"table handover failed";\r\nreturn -EINVAL;\r\n}\r\nif (!snap_src)\r\nreturn 0;\r\nif (!dm_target_is_snapshot_merge(snap->ti))\r\nreturn 1;\r\nif (snap_merge) {\r\nsnap->ti->error = "A snapshot is already merging.";\r\nreturn -EINVAL;\r\n}\r\nif (!snap_src->store->type->prepare_merge ||\r\n!snap_src->store->type->commit_merge) {\r\nsnap->ti->error = "Snapshot exception store does not "\r\n"support snapshot-merge.";\r\nreturn -EINVAL;\r\n}\r\nreturn 1;\r\n}\r\nstatic void __insert_snapshot(struct origin *o, struct dm_snapshot *s)\r\n{\r\nstruct dm_snapshot *l;\r\nlist_for_each_entry(l, &o->snapshots, list)\r\nif (l->store->chunk_size < s->store->chunk_size)\r\nbreak;\r\nlist_add_tail(&s->list, &l->list);\r\n}\r\nstatic int register_snapshot(struct dm_snapshot *snap)\r\n{\r\nstruct origin *o, *new_o = NULL;\r\nstruct block_device *bdev = snap->origin->bdev;\r\nint r = 0;\r\nnew_o = kmalloc(sizeof(*new_o), GFP_KERNEL);\r\nif (!new_o)\r\nreturn -ENOMEM;\r\ndown_write(&_origins_lock);\r\nr = __validate_exception_handover(snap);\r\nif (r < 0) {\r\nkfree(new_o);\r\ngoto out;\r\n}\r\no = __lookup_origin(bdev);\r\nif (o)\r\nkfree(new_o);\r\nelse {\r\no = new_o;\r\nINIT_LIST_HEAD(&o->snapshots);\r\no->bdev = bdev;\r\n__insert_origin(o);\r\n}\r\n__insert_snapshot(o, snap);\r\nout:\r\nup_write(&_origins_lock);\r\nreturn r;\r\n}\r\nstatic void reregister_snapshot(struct dm_snapshot *s)\r\n{\r\nstruct block_device *bdev = s->origin->bdev;\r\ndown_write(&_origins_lock);\r\nlist_del(&s->list);\r\n__insert_snapshot(__lookup_origin(bdev), s);\r\nup_write(&_origins_lock);\r\n}\r\nstatic void unregister_snapshot(struct dm_snapshot *s)\r\n{\r\nstruct origin *o;\r\ndown_write(&_origins_lock);\r\no = __lookup_origin(s->origin->bdev);\r\nlist_del(&s->list);\r\nif (o && list_empty(&o->snapshots)) {\r\nlist_del(&o->hash_list);\r\nkfree(o);\r\n}\r\nup_write(&_origins_lock);\r\n}\r\nstatic int dm_exception_table_init(struct dm_exception_table *et,\r\nuint32_t size, unsigned hash_shift)\r\n{\r\nunsigned int i;\r\net->hash_shift = hash_shift;\r\net->hash_mask = size - 1;\r\net->table = dm_vcalloc(size, sizeof(struct list_head));\r\nif (!et->table)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < size; i++)\r\nINIT_LIST_HEAD(et->table + i);\r\nreturn 0;\r\n}\r\nstatic void dm_exception_table_exit(struct dm_exception_table *et,\r\nstruct kmem_cache *mem)\r\n{\r\nstruct list_head *slot;\r\nstruct dm_exception *ex, *next;\r\nint i, size;\r\nsize = et->hash_mask + 1;\r\nfor (i = 0; i < size; i++) {\r\nslot = et->table + i;\r\nlist_for_each_entry_safe (ex, next, slot, hash_list)\r\nkmem_cache_free(mem, ex);\r\n}\r\nvfree(et->table);\r\n}\r\nstatic uint32_t exception_hash(struct dm_exception_table *et, chunk_t chunk)\r\n{\r\nreturn (chunk >> et->hash_shift) & et->hash_mask;\r\n}\r\nstatic void dm_remove_exception(struct dm_exception *e)\r\n{\r\nlist_del(&e->hash_list);\r\n}\r\nstatic struct dm_exception *dm_lookup_exception(struct dm_exception_table *et,\r\nchunk_t chunk)\r\n{\r\nstruct list_head *slot;\r\nstruct dm_exception *e;\r\nslot = &et->table[exception_hash(et, chunk)];\r\nlist_for_each_entry (e, slot, hash_list)\r\nif (chunk >= e->old_chunk &&\r\nchunk <= e->old_chunk + dm_consecutive_chunk_count(e))\r\nreturn e;\r\nreturn NULL;\r\n}\r\nstatic struct dm_exception *alloc_completed_exception(gfp_t gfp)\r\n{\r\nstruct dm_exception *e;\r\ne = kmem_cache_alloc(exception_cache, gfp);\r\nif (!e && gfp == GFP_NOIO)\r\ne = kmem_cache_alloc(exception_cache, GFP_ATOMIC);\r\nreturn e;\r\n}\r\nstatic void free_completed_exception(struct dm_exception *e)\r\n{\r\nkmem_cache_free(exception_cache, e);\r\n}\r\nstatic struct dm_snap_pending_exception *alloc_pending_exception(struct dm_snapshot *s)\r\n{\r\nstruct dm_snap_pending_exception *pe = mempool_alloc(s->pending_pool,\r\nGFP_NOIO);\r\natomic_inc(&s->pending_exceptions_count);\r\npe->snap = s;\r\nreturn pe;\r\n}\r\nstatic void free_pending_exception(struct dm_snap_pending_exception *pe)\r\n{\r\nstruct dm_snapshot *s = pe->snap;\r\nmempool_free(pe, s->pending_pool);\r\nsmp_mb__before_atomic();\r\natomic_dec(&s->pending_exceptions_count);\r\n}\r\nstatic void dm_insert_exception(struct dm_exception_table *eh,\r\nstruct dm_exception *new_e)\r\n{\r\nstruct list_head *l;\r\nstruct dm_exception *e = NULL;\r\nl = &eh->table[exception_hash(eh, new_e->old_chunk)];\r\nif (!eh->hash_shift)\r\ngoto out;\r\nlist_for_each_entry_reverse(e, l, hash_list) {\r\nif (new_e->old_chunk == (e->old_chunk +\r\ndm_consecutive_chunk_count(e) + 1) &&\r\nnew_e->new_chunk == (dm_chunk_number(e->new_chunk) +\r\ndm_consecutive_chunk_count(e) + 1)) {\r\ndm_consecutive_chunk_count_inc(e);\r\nfree_completed_exception(new_e);\r\nreturn;\r\n}\r\nif (new_e->old_chunk == (e->old_chunk - 1) &&\r\nnew_e->new_chunk == (dm_chunk_number(e->new_chunk) - 1)) {\r\ndm_consecutive_chunk_count_inc(e);\r\ne->old_chunk--;\r\ne->new_chunk--;\r\nfree_completed_exception(new_e);\r\nreturn;\r\n}\r\nif (new_e->old_chunk > e->old_chunk)\r\nbreak;\r\n}\r\nout:\r\nlist_add(&new_e->hash_list, e ? &e->hash_list : l);\r\n}\r\nstatic int dm_add_exception(void *context, chunk_t old, chunk_t new)\r\n{\r\nstruct dm_snapshot *s = context;\r\nstruct dm_exception *e;\r\ne = alloc_completed_exception(GFP_KERNEL);\r\nif (!e)\r\nreturn -ENOMEM;\r\ne->old_chunk = old;\r\ne->new_chunk = new;\r\ndm_insert_exception(&s->complete, e);\r\nreturn 0;\r\n}\r\nstatic uint32_t __minimum_chunk_size(struct origin *o)\r\n{\r\nstruct dm_snapshot *snap;\r\nunsigned chunk_size = 0;\r\nif (o)\r\nlist_for_each_entry(snap, &o->snapshots, list)\r\nchunk_size = min_not_zero(chunk_size,\r\nsnap->store->chunk_size);\r\nreturn (uint32_t) chunk_size;\r\n}\r\nstatic int calc_max_buckets(void)\r\n{\r\nunsigned long mem = 2 * 1024 * 1024;\r\nmem /= sizeof(struct list_head);\r\nreturn mem;\r\n}\r\nstatic int init_hash_tables(struct dm_snapshot *s)\r\n{\r\nsector_t hash_size, cow_dev_size, max_buckets;\r\ncow_dev_size = get_dev_size(s->cow->bdev);\r\nmax_buckets = calc_max_buckets();\r\nhash_size = cow_dev_size >> s->store->chunk_shift;\r\nhash_size = min(hash_size, max_buckets);\r\nif (hash_size < 64)\r\nhash_size = 64;\r\nhash_size = rounddown_pow_of_two(hash_size);\r\nif (dm_exception_table_init(&s->complete, hash_size,\r\nDM_CHUNK_CONSECUTIVE_BITS))\r\nreturn -ENOMEM;\r\nhash_size >>= 3;\r\nif (hash_size < 64)\r\nhash_size = 64;\r\nif (dm_exception_table_init(&s->pending, hash_size, 0)) {\r\ndm_exception_table_exit(&s->complete, exception_cache);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void merge_shutdown(struct dm_snapshot *s)\r\n{\r\nclear_bit_unlock(RUNNING_MERGE, &s->state_bits);\r\nsmp_mb__after_atomic();\r\nwake_up_bit(&s->state_bits, RUNNING_MERGE);\r\n}\r\nstatic struct bio *__release_queued_bios_after_merge(struct dm_snapshot *s)\r\n{\r\ns->first_merging_chunk = 0;\r\ns->num_merging_chunks = 0;\r\nreturn bio_list_get(&s->bios_queued_during_merge);\r\n}\r\nstatic int __remove_single_exception_chunk(struct dm_snapshot *s,\r\nchunk_t old_chunk)\r\n{\r\nstruct dm_exception *e;\r\ne = dm_lookup_exception(&s->complete, old_chunk);\r\nif (!e) {\r\nDMERR("Corruption detected: exception for block %llu is "\r\n"on disk but not in memory",\r\n(unsigned long long)old_chunk);\r\nreturn -EINVAL;\r\n}\r\nif (!dm_consecutive_chunk_count(e)) {\r\ndm_remove_exception(e);\r\nfree_completed_exception(e);\r\nreturn 0;\r\n}\r\nif (old_chunk == e->old_chunk) {\r\ne->old_chunk++;\r\ne->new_chunk++;\r\n} else if (old_chunk != e->old_chunk +\r\ndm_consecutive_chunk_count(e)) {\r\nDMERR("Attempt to merge block %llu from the "\r\n"middle of a chunk range [%llu - %llu]",\r\n(unsigned long long)old_chunk,\r\n(unsigned long long)e->old_chunk,\r\n(unsigned long long)\r\ne->old_chunk + dm_consecutive_chunk_count(e));\r\nreturn -EINVAL;\r\n}\r\ndm_consecutive_chunk_count_dec(e);\r\nreturn 0;\r\n}\r\nstatic int remove_single_exception_chunk(struct dm_snapshot *s)\r\n{\r\nstruct bio *b = NULL;\r\nint r;\r\nchunk_t old_chunk = s->first_merging_chunk + s->num_merging_chunks - 1;\r\ndown_write(&s->lock);\r\ndo {\r\nr = __remove_single_exception_chunk(s, old_chunk);\r\nif (r)\r\ngoto out;\r\n} while (old_chunk-- > s->first_merging_chunk);\r\nb = __release_queued_bios_after_merge(s);\r\nout:\r\nup_write(&s->lock);\r\nif (b)\r\nflush_bios(b);\r\nreturn r;\r\n}\r\nstatic uint64_t read_pending_exceptions_done_count(void)\r\n{\r\nuint64_t pending_exceptions_done;\r\nspin_lock(&_pending_exceptions_done_spinlock);\r\npending_exceptions_done = _pending_exceptions_done_count;\r\nspin_unlock(&_pending_exceptions_done_spinlock);\r\nreturn pending_exceptions_done;\r\n}\r\nstatic void increment_pending_exceptions_done_count(void)\r\n{\r\nspin_lock(&_pending_exceptions_done_spinlock);\r\n_pending_exceptions_done_count++;\r\nspin_unlock(&_pending_exceptions_done_spinlock);\r\nwake_up_all(&_pending_exceptions_done);\r\n}\r\nstatic void snapshot_merge_next_chunks(struct dm_snapshot *s)\r\n{\r\nint i, linear_chunks;\r\nchunk_t old_chunk, new_chunk;\r\nstruct dm_io_region src, dest;\r\nsector_t io_size;\r\nuint64_t previous_count;\r\nBUG_ON(!test_bit(RUNNING_MERGE, &s->state_bits));\r\nif (unlikely(test_bit(SHUTDOWN_MERGE, &s->state_bits)))\r\ngoto shut;\r\nif (!s->valid) {\r\nDMERR("Snapshot is invalid: can't merge");\r\ngoto shut;\r\n}\r\nlinear_chunks = s->store->type->prepare_merge(s->store, &old_chunk,\r\n&new_chunk);\r\nif (linear_chunks <= 0) {\r\nif (linear_chunks < 0) {\r\nDMERR("Read error in exception store: "\r\n"shutting down merge");\r\ndown_write(&s->lock);\r\ns->merge_failed = 1;\r\nup_write(&s->lock);\r\n}\r\ngoto shut;\r\n}\r\nold_chunk = old_chunk + 1 - linear_chunks;\r\nnew_chunk = new_chunk + 1 - linear_chunks;\r\nio_size = linear_chunks * s->store->chunk_size;\r\ndest.bdev = s->origin->bdev;\r\ndest.sector = chunk_to_sector(s->store, old_chunk);\r\ndest.count = min(io_size, get_dev_size(dest.bdev) - dest.sector);\r\nsrc.bdev = s->cow->bdev;\r\nsrc.sector = chunk_to_sector(s->store, new_chunk);\r\nsrc.count = dest.count;\r\nprevious_count = read_pending_exceptions_done_count();\r\nwhile (origin_write_extent(s, dest.sector, io_size)) {\r\nwait_event(_pending_exceptions_done,\r\n(read_pending_exceptions_done_count() !=\r\nprevious_count));\r\nprevious_count = read_pending_exceptions_done_count();\r\n}\r\ndown_write(&s->lock);\r\ns->first_merging_chunk = old_chunk;\r\ns->num_merging_chunks = linear_chunks;\r\nup_write(&s->lock);\r\nfor (i = 0; i < linear_chunks; i++)\r\n__check_for_conflicting_io(s, old_chunk + i);\r\ndm_kcopyd_copy(s->kcopyd_client, &src, 1, &dest, 0, merge_callback, s);\r\nreturn;\r\nshut:\r\nmerge_shutdown(s);\r\n}\r\nstatic void merge_callback(int read_err, unsigned long write_err, void *context)\r\n{\r\nstruct dm_snapshot *s = context;\r\nstruct bio *b = NULL;\r\nif (read_err || write_err) {\r\nif (read_err)\r\nDMERR("Read error: shutting down merge.");\r\nelse\r\nDMERR("Write error: shutting down merge.");\r\ngoto shut;\r\n}\r\nif (s->store->type->commit_merge(s->store,\r\ns->num_merging_chunks) < 0) {\r\nDMERR("Write error in exception store: shutting down merge");\r\ngoto shut;\r\n}\r\nif (remove_single_exception_chunk(s) < 0)\r\ngoto shut;\r\nsnapshot_merge_next_chunks(s);\r\nreturn;\r\nshut:\r\ndown_write(&s->lock);\r\ns->merge_failed = 1;\r\nb = __release_queued_bios_after_merge(s);\r\nup_write(&s->lock);\r\nerror_bios(b);\r\nmerge_shutdown(s);\r\n}\r\nstatic void start_merge(struct dm_snapshot *s)\r\n{\r\nif (!test_and_set_bit(RUNNING_MERGE, &s->state_bits))\r\nsnapshot_merge_next_chunks(s);\r\n}\r\nstatic void stop_merge(struct dm_snapshot *s)\r\n{\r\nset_bit(SHUTDOWN_MERGE, &s->state_bits);\r\nwait_on_bit(&s->state_bits, RUNNING_MERGE, TASK_UNINTERRUPTIBLE);\r\nclear_bit(SHUTDOWN_MERGE, &s->state_bits);\r\n}\r\nstatic int snapshot_ctr(struct dm_target *ti, unsigned int argc, char **argv)\r\n{\r\nstruct dm_snapshot *s;\r\nint i;\r\nint r = -EINVAL;\r\nchar *origin_path, *cow_path;\r\nunsigned args_used, num_flush_bios = 1;\r\nfmode_t origin_mode = FMODE_READ;\r\nif (argc != 4) {\r\nti->error = "requires exactly 4 arguments";\r\nr = -EINVAL;\r\ngoto bad;\r\n}\r\nif (dm_target_is_snapshot_merge(ti)) {\r\nnum_flush_bios = 2;\r\norigin_mode = FMODE_WRITE;\r\n}\r\ns = kmalloc(sizeof(*s), GFP_KERNEL);\r\nif (!s) {\r\nti->error = "Cannot allocate private snapshot structure";\r\nr = -ENOMEM;\r\ngoto bad;\r\n}\r\norigin_path = argv[0];\r\nargv++;\r\nargc--;\r\nr = dm_get_device(ti, origin_path, origin_mode, &s->origin);\r\nif (r) {\r\nti->error = "Cannot get origin device";\r\ngoto bad_origin;\r\n}\r\ncow_path = argv[0];\r\nargv++;\r\nargc--;\r\nr = dm_get_device(ti, cow_path, dm_table_get_mode(ti->table), &s->cow);\r\nif (r) {\r\nti->error = "Cannot get COW device";\r\ngoto bad_cow;\r\n}\r\nr = dm_exception_store_create(ti, argc, argv, s, &args_used, &s->store);\r\nif (r) {\r\nti->error = "Couldn't create exception store";\r\nr = -EINVAL;\r\ngoto bad_store;\r\n}\r\nargv += args_used;\r\nargc -= args_used;\r\ns->ti = ti;\r\ns->valid = 1;\r\ns->active = 0;\r\natomic_set(&s->pending_exceptions_count, 0);\r\ns->exception_start_sequence = 0;\r\ns->exception_complete_sequence = 0;\r\nINIT_LIST_HEAD(&s->out_of_order_list);\r\ninit_rwsem(&s->lock);\r\nINIT_LIST_HEAD(&s->list);\r\nspin_lock_init(&s->pe_lock);\r\ns->state_bits = 0;\r\ns->merge_failed = 0;\r\ns->first_merging_chunk = 0;\r\ns->num_merging_chunks = 0;\r\nbio_list_init(&s->bios_queued_during_merge);\r\nif (init_hash_tables(s)) {\r\nti->error = "Unable to allocate hash table space";\r\nr = -ENOMEM;\r\ngoto bad_hash_tables;\r\n}\r\ns->kcopyd_client = dm_kcopyd_client_create(&dm_kcopyd_throttle);\r\nif (IS_ERR(s->kcopyd_client)) {\r\nr = PTR_ERR(s->kcopyd_client);\r\nti->error = "Could not create kcopyd client";\r\ngoto bad_kcopyd;\r\n}\r\ns->pending_pool = mempool_create_slab_pool(MIN_IOS, pending_cache);\r\nif (!s->pending_pool) {\r\nti->error = "Could not allocate mempool for pending exceptions";\r\nr = -ENOMEM;\r\ngoto bad_pending_pool;\r\n}\r\nfor (i = 0; i < DM_TRACKED_CHUNK_HASH_SIZE; i++)\r\nINIT_HLIST_HEAD(&s->tracked_chunk_hash[i]);\r\nspin_lock_init(&s->tracked_chunk_lock);\r\nti->private = s;\r\nti->num_flush_bios = num_flush_bios;\r\nti->per_bio_data_size = sizeof(struct dm_snap_tracked_chunk);\r\nr = register_snapshot(s);\r\nif (r == -ENOMEM) {\r\nti->error = "Snapshot origin struct allocation failed";\r\ngoto bad_load_and_register;\r\n} else if (r < 0) {\r\ngoto bad_load_and_register;\r\n}\r\nif (r > 0) {\r\ns->store->chunk_size = 0;\r\nreturn 0;\r\n}\r\nr = s->store->type->read_metadata(s->store, dm_add_exception,\r\n(void *)s);\r\nif (r < 0) {\r\nti->error = "Failed to read snapshot metadata";\r\ngoto bad_read_metadata;\r\n} else if (r > 0) {\r\ns->valid = 0;\r\nDMWARN("Snapshot is marked invalid.");\r\n}\r\nif (!s->store->chunk_size) {\r\nti->error = "Chunk size not set";\r\ngoto bad_read_metadata;\r\n}\r\nr = dm_set_target_max_io_len(ti, s->store->chunk_size);\r\nif (r)\r\ngoto bad_read_metadata;\r\nreturn 0;\r\nbad_read_metadata:\r\nunregister_snapshot(s);\r\nbad_load_and_register:\r\nmempool_destroy(s->pending_pool);\r\nbad_pending_pool:\r\ndm_kcopyd_client_destroy(s->kcopyd_client);\r\nbad_kcopyd:\r\ndm_exception_table_exit(&s->pending, pending_cache);\r\ndm_exception_table_exit(&s->complete, exception_cache);\r\nbad_hash_tables:\r\ndm_exception_store_destroy(s->store);\r\nbad_store:\r\ndm_put_device(ti, s->cow);\r\nbad_cow:\r\ndm_put_device(ti, s->origin);\r\nbad_origin:\r\nkfree(s);\r\nbad:\r\nreturn r;\r\n}\r\nstatic void __free_exceptions(struct dm_snapshot *s)\r\n{\r\ndm_kcopyd_client_destroy(s->kcopyd_client);\r\ns->kcopyd_client = NULL;\r\ndm_exception_table_exit(&s->pending, pending_cache);\r\ndm_exception_table_exit(&s->complete, exception_cache);\r\n}\r\nstatic void __handover_exceptions(struct dm_snapshot *snap_src,\r\nstruct dm_snapshot *snap_dest)\r\n{\r\nunion {\r\nstruct dm_exception_table table_swap;\r\nstruct dm_exception_store *store_swap;\r\n} u;\r\nu.table_swap = snap_dest->complete;\r\nsnap_dest->complete = snap_src->complete;\r\nsnap_src->complete = u.table_swap;\r\nu.store_swap = snap_dest->store;\r\nsnap_dest->store = snap_src->store;\r\nsnap_src->store = u.store_swap;\r\nsnap_dest->store->snap = snap_dest;\r\nsnap_src->store->snap = snap_src;\r\nsnap_dest->ti->max_io_len = snap_dest->store->chunk_size;\r\nsnap_dest->valid = snap_src->valid;\r\nsnap_src->valid = 0;\r\n}\r\nstatic void snapshot_dtr(struct dm_target *ti)\r\n{\r\n#ifdef CONFIG_DM_DEBUG\r\nint i;\r\n#endif\r\nstruct dm_snapshot *s = ti->private;\r\nstruct dm_snapshot *snap_src = NULL, *snap_dest = NULL;\r\ndown_read(&_origins_lock);\r\n(void) __find_snapshots_sharing_cow(s, &snap_src, &snap_dest, NULL);\r\nif (snap_src && snap_dest && (s == snap_src)) {\r\ndown_write(&snap_dest->lock);\r\nsnap_dest->valid = 0;\r\nup_write(&snap_dest->lock);\r\nDMERR("Cancelling snapshot handover.");\r\n}\r\nup_read(&_origins_lock);\r\nif (dm_target_is_snapshot_merge(ti))\r\nstop_merge(s);\r\nunregister_snapshot(s);\r\nwhile (atomic_read(&s->pending_exceptions_count))\r\nmsleep(1);\r\nsmp_mb();\r\n#ifdef CONFIG_DM_DEBUG\r\nfor (i = 0; i < DM_TRACKED_CHUNK_HASH_SIZE; i++)\r\nBUG_ON(!hlist_empty(&s->tracked_chunk_hash[i]));\r\n#endif\r\n__free_exceptions(s);\r\nmempool_destroy(s->pending_pool);\r\ndm_exception_store_destroy(s->store);\r\ndm_put_device(ti, s->cow);\r\ndm_put_device(ti, s->origin);\r\nkfree(s);\r\n}\r\nstatic void flush_bios(struct bio *bio)\r\n{\r\nstruct bio *n;\r\nwhile (bio) {\r\nn = bio->bi_next;\r\nbio->bi_next = NULL;\r\ngeneric_make_request(bio);\r\nbio = n;\r\n}\r\n}\r\nstatic void retry_origin_bios(struct dm_snapshot *s, struct bio *bio)\r\n{\r\nstruct bio *n;\r\nint r;\r\nwhile (bio) {\r\nn = bio->bi_next;\r\nbio->bi_next = NULL;\r\nr = do_origin(s->origin, bio);\r\nif (r == DM_MAPIO_REMAPPED)\r\ngeneric_make_request(bio);\r\nbio = n;\r\n}\r\n}\r\nstatic void error_bios(struct bio *bio)\r\n{\r\nstruct bio *n;\r\nwhile (bio) {\r\nn = bio->bi_next;\r\nbio->bi_next = NULL;\r\nbio_io_error(bio);\r\nbio = n;\r\n}\r\n}\r\nstatic void __invalidate_snapshot(struct dm_snapshot *s, int err)\r\n{\r\nif (!s->valid)\r\nreturn;\r\nif (err == -EIO)\r\nDMERR("Invalidating snapshot: Error reading/writing.");\r\nelse if (err == -ENOMEM)\r\nDMERR("Invalidating snapshot: Unable to allocate exception.");\r\nif (s->store->type->drop_snapshot)\r\ns->store->type->drop_snapshot(s->store);\r\ns->valid = 0;\r\ndm_table_event(s->ti->table);\r\n}\r\nstatic void pending_complete(struct dm_snap_pending_exception *pe, int success)\r\n{\r\nstruct dm_exception *e;\r\nstruct dm_snapshot *s = pe->snap;\r\nstruct bio *origin_bios = NULL;\r\nstruct bio *snapshot_bios = NULL;\r\nstruct bio *full_bio = NULL;\r\nint error = 0;\r\nif (!success) {\r\ndown_write(&s->lock);\r\n__invalidate_snapshot(s, -EIO);\r\nerror = 1;\r\ngoto out;\r\n}\r\ne = alloc_completed_exception(GFP_NOIO);\r\nif (!e) {\r\ndown_write(&s->lock);\r\n__invalidate_snapshot(s, -ENOMEM);\r\nerror = 1;\r\ngoto out;\r\n}\r\n*e = pe->e;\r\ndown_write(&s->lock);\r\nif (!s->valid) {\r\nfree_completed_exception(e);\r\nerror = 1;\r\ngoto out;\r\n}\r\n__check_for_conflicting_io(s, pe->e.old_chunk);\r\ndm_insert_exception(&s->complete, e);\r\nout:\r\ndm_remove_exception(&pe->e);\r\nsnapshot_bios = bio_list_get(&pe->snapshot_bios);\r\norigin_bios = bio_list_get(&pe->origin_bios);\r\nfull_bio = pe->full_bio;\r\nif (full_bio) {\r\nfull_bio->bi_end_io = pe->full_bio_end_io;\r\nfull_bio->bi_private = pe->full_bio_private;\r\natomic_inc(&full_bio->bi_remaining);\r\n}\r\nincrement_pending_exceptions_done_count();\r\nup_write(&s->lock);\r\nif (error) {\r\nif (full_bio)\r\nbio_io_error(full_bio);\r\nerror_bios(snapshot_bios);\r\n} else {\r\nif (full_bio)\r\nbio_endio(full_bio, 0);\r\nflush_bios(snapshot_bios);\r\n}\r\nretry_origin_bios(s, origin_bios);\r\nfree_pending_exception(pe);\r\n}\r\nstatic void commit_callback(void *context, int success)\r\n{\r\nstruct dm_snap_pending_exception *pe = context;\r\npending_complete(pe, success);\r\n}\r\nstatic void complete_exception(struct dm_snap_pending_exception *pe)\r\n{\r\nstruct dm_snapshot *s = pe->snap;\r\nif (unlikely(pe->copy_error))\r\npending_complete(pe, 0);\r\nelse\r\ns->store->type->commit_exception(s->store, &pe->e,\r\ncommit_callback, pe);\r\n}\r\nstatic void copy_callback(int read_err, unsigned long write_err, void *context)\r\n{\r\nstruct dm_snap_pending_exception *pe = context;\r\nstruct dm_snapshot *s = pe->snap;\r\npe->copy_error = read_err || write_err;\r\nif (pe->exception_sequence == s->exception_complete_sequence) {\r\ns->exception_complete_sequence++;\r\ncomplete_exception(pe);\r\nwhile (!list_empty(&s->out_of_order_list)) {\r\npe = list_entry(s->out_of_order_list.next,\r\nstruct dm_snap_pending_exception, out_of_order_entry);\r\nif (pe->exception_sequence != s->exception_complete_sequence)\r\nbreak;\r\ns->exception_complete_sequence++;\r\nlist_del(&pe->out_of_order_entry);\r\ncomplete_exception(pe);\r\n}\r\n} else {\r\nstruct list_head *lh;\r\nstruct dm_snap_pending_exception *pe2;\r\nlist_for_each_prev(lh, &s->out_of_order_list) {\r\npe2 = list_entry(lh, struct dm_snap_pending_exception, out_of_order_entry);\r\nif (pe2->exception_sequence < pe->exception_sequence)\r\nbreak;\r\n}\r\nlist_add(&pe->out_of_order_entry, lh);\r\n}\r\n}\r\nstatic void start_copy(struct dm_snap_pending_exception *pe)\r\n{\r\nstruct dm_snapshot *s = pe->snap;\r\nstruct dm_io_region src, dest;\r\nstruct block_device *bdev = s->origin->bdev;\r\nsector_t dev_size;\r\ndev_size = get_dev_size(bdev);\r\nsrc.bdev = bdev;\r\nsrc.sector = chunk_to_sector(s->store, pe->e.old_chunk);\r\nsrc.count = min((sector_t)s->store->chunk_size, dev_size - src.sector);\r\ndest.bdev = s->cow->bdev;\r\ndest.sector = chunk_to_sector(s->store, pe->e.new_chunk);\r\ndest.count = src.count;\r\ndm_kcopyd_copy(s->kcopyd_client, &src, 1, &dest, 0, copy_callback, pe);\r\n}\r\nstatic void full_bio_end_io(struct bio *bio, int error)\r\n{\r\nvoid *callback_data = bio->bi_private;\r\ndm_kcopyd_do_callback(callback_data, 0, error ? 1 : 0);\r\n}\r\nstatic void start_full_bio(struct dm_snap_pending_exception *pe,\r\nstruct bio *bio)\r\n{\r\nstruct dm_snapshot *s = pe->snap;\r\nvoid *callback_data;\r\npe->full_bio = bio;\r\npe->full_bio_end_io = bio->bi_end_io;\r\npe->full_bio_private = bio->bi_private;\r\ncallback_data = dm_kcopyd_prepare_callback(s->kcopyd_client,\r\ncopy_callback, pe);\r\nbio->bi_end_io = full_bio_end_io;\r\nbio->bi_private = callback_data;\r\ngeneric_make_request(bio);\r\n}\r\nstatic struct dm_snap_pending_exception *\r\n__lookup_pending_exception(struct dm_snapshot *s, chunk_t chunk)\r\n{\r\nstruct dm_exception *e = dm_lookup_exception(&s->pending, chunk);\r\nif (!e)\r\nreturn NULL;\r\nreturn container_of(e, struct dm_snap_pending_exception, e);\r\n}\r\nstatic struct dm_snap_pending_exception *\r\n__find_pending_exception(struct dm_snapshot *s,\r\nstruct dm_snap_pending_exception *pe, chunk_t chunk)\r\n{\r\nstruct dm_snap_pending_exception *pe2;\r\npe2 = __lookup_pending_exception(s, chunk);\r\nif (pe2) {\r\nfree_pending_exception(pe);\r\nreturn pe2;\r\n}\r\npe->e.old_chunk = chunk;\r\nbio_list_init(&pe->origin_bios);\r\nbio_list_init(&pe->snapshot_bios);\r\npe->started = 0;\r\npe->full_bio = NULL;\r\nif (s->store->type->prepare_exception(s->store, &pe->e)) {\r\nfree_pending_exception(pe);\r\nreturn NULL;\r\n}\r\npe->exception_sequence = s->exception_start_sequence++;\r\ndm_insert_exception(&s->pending, &pe->e);\r\nreturn pe;\r\n}\r\nstatic void remap_exception(struct dm_snapshot *s, struct dm_exception *e,\r\nstruct bio *bio, chunk_t chunk)\r\n{\r\nbio->bi_bdev = s->cow->bdev;\r\nbio->bi_iter.bi_sector =\r\nchunk_to_sector(s->store, dm_chunk_number(e->new_chunk) +\r\n(chunk - e->old_chunk)) +\r\n(bio->bi_iter.bi_sector & s->store->chunk_mask);\r\n}\r\nstatic int snapshot_map(struct dm_target *ti, struct bio *bio)\r\n{\r\nstruct dm_exception *e;\r\nstruct dm_snapshot *s = ti->private;\r\nint r = DM_MAPIO_REMAPPED;\r\nchunk_t chunk;\r\nstruct dm_snap_pending_exception *pe = NULL;\r\ninit_tracked_chunk(bio);\r\nif (bio->bi_rw & REQ_FLUSH) {\r\nbio->bi_bdev = s->cow->bdev;\r\nreturn DM_MAPIO_REMAPPED;\r\n}\r\nchunk = sector_to_chunk(s->store, bio->bi_iter.bi_sector);\r\nif (!s->valid)\r\nreturn -EIO;\r\ndown_write(&s->lock);\r\nif (!s->valid) {\r\nr = -EIO;\r\ngoto out_unlock;\r\n}\r\ne = dm_lookup_exception(&s->complete, chunk);\r\nif (e) {\r\nremap_exception(s, e, bio, chunk);\r\ngoto out_unlock;\r\n}\r\nif (bio_rw(bio) == WRITE) {\r\npe = __lookup_pending_exception(s, chunk);\r\nif (!pe) {\r\nup_write(&s->lock);\r\npe = alloc_pending_exception(s);\r\ndown_write(&s->lock);\r\nif (!s->valid) {\r\nfree_pending_exception(pe);\r\nr = -EIO;\r\ngoto out_unlock;\r\n}\r\ne = dm_lookup_exception(&s->complete, chunk);\r\nif (e) {\r\nfree_pending_exception(pe);\r\nremap_exception(s, e, bio, chunk);\r\ngoto out_unlock;\r\n}\r\npe = __find_pending_exception(s, pe, chunk);\r\nif (!pe) {\r\n__invalidate_snapshot(s, -ENOMEM);\r\nr = -EIO;\r\ngoto out_unlock;\r\n}\r\n}\r\nremap_exception(s, &pe->e, bio, chunk);\r\nr = DM_MAPIO_SUBMITTED;\r\nif (!pe->started &&\r\nbio->bi_iter.bi_size ==\r\n(s->store->chunk_size << SECTOR_SHIFT)) {\r\npe->started = 1;\r\nup_write(&s->lock);\r\nstart_full_bio(pe, bio);\r\ngoto out;\r\n}\r\nbio_list_add(&pe->snapshot_bios, bio);\r\nif (!pe->started) {\r\npe->started = 1;\r\nup_write(&s->lock);\r\nstart_copy(pe);\r\ngoto out;\r\n}\r\n} else {\r\nbio->bi_bdev = s->origin->bdev;\r\ntrack_chunk(s, bio, chunk);\r\n}\r\nout_unlock:\r\nup_write(&s->lock);\r\nout:\r\nreturn r;\r\n}\r\nstatic int snapshot_merge_map(struct dm_target *ti, struct bio *bio)\r\n{\r\nstruct dm_exception *e;\r\nstruct dm_snapshot *s = ti->private;\r\nint r = DM_MAPIO_REMAPPED;\r\nchunk_t chunk;\r\ninit_tracked_chunk(bio);\r\nif (bio->bi_rw & REQ_FLUSH) {\r\nif (!dm_bio_get_target_bio_nr(bio))\r\nbio->bi_bdev = s->origin->bdev;\r\nelse\r\nbio->bi_bdev = s->cow->bdev;\r\nreturn DM_MAPIO_REMAPPED;\r\n}\r\nchunk = sector_to_chunk(s->store, bio->bi_iter.bi_sector);\r\ndown_write(&s->lock);\r\nif (!s->valid)\r\ngoto redirect_to_origin;\r\ne = dm_lookup_exception(&s->complete, chunk);\r\nif (e) {\r\nif (bio_rw(bio) == WRITE &&\r\nchunk >= s->first_merging_chunk &&\r\nchunk < (s->first_merging_chunk +\r\ns->num_merging_chunks)) {\r\nbio->bi_bdev = s->origin->bdev;\r\nbio_list_add(&s->bios_queued_during_merge, bio);\r\nr = DM_MAPIO_SUBMITTED;\r\ngoto out_unlock;\r\n}\r\nremap_exception(s, e, bio, chunk);\r\nif (bio_rw(bio) == WRITE)\r\ntrack_chunk(s, bio, chunk);\r\ngoto out_unlock;\r\n}\r\nredirect_to_origin:\r\nbio->bi_bdev = s->origin->bdev;\r\nif (bio_rw(bio) == WRITE) {\r\nup_write(&s->lock);\r\nreturn do_origin(s->origin, bio);\r\n}\r\nout_unlock:\r\nup_write(&s->lock);\r\nreturn r;\r\n}\r\nstatic int snapshot_end_io(struct dm_target *ti, struct bio *bio, int error)\r\n{\r\nstruct dm_snapshot *s = ti->private;\r\nif (is_bio_tracked(bio))\r\nstop_tracking_chunk(s, bio);\r\nreturn 0;\r\n}\r\nstatic void snapshot_merge_presuspend(struct dm_target *ti)\r\n{\r\nstruct dm_snapshot *s = ti->private;\r\nstop_merge(s);\r\n}\r\nstatic int snapshot_preresume(struct dm_target *ti)\r\n{\r\nint r = 0;\r\nstruct dm_snapshot *s = ti->private;\r\nstruct dm_snapshot *snap_src = NULL, *snap_dest = NULL;\r\ndown_read(&_origins_lock);\r\n(void) __find_snapshots_sharing_cow(s, &snap_src, &snap_dest, NULL);\r\nif (snap_src && snap_dest) {\r\ndown_read(&snap_src->lock);\r\nif (s == snap_src) {\r\nDMERR("Unable to resume snapshot source until "\r\n"handover completes.");\r\nr = -EINVAL;\r\n} else if (!dm_suspended(snap_src->ti)) {\r\nDMERR("Unable to perform snapshot handover until "\r\n"source is suspended.");\r\nr = -EINVAL;\r\n}\r\nup_read(&snap_src->lock);\r\n}\r\nup_read(&_origins_lock);\r\nreturn r;\r\n}\r\nstatic void snapshot_resume(struct dm_target *ti)\r\n{\r\nstruct dm_snapshot *s = ti->private;\r\nstruct dm_snapshot *snap_src = NULL, *snap_dest = NULL, *snap_merging = NULL;\r\nstruct dm_origin *o;\r\nstruct mapped_device *origin_md = NULL;\r\nbool must_restart_merging = false;\r\ndown_read(&_origins_lock);\r\no = __lookup_dm_origin(s->origin->bdev);\r\nif (o)\r\norigin_md = dm_table_get_md(o->ti->table);\r\nif (!origin_md) {\r\n(void) __find_snapshots_sharing_cow(s, NULL, NULL, &snap_merging);\r\nif (snap_merging)\r\norigin_md = dm_table_get_md(snap_merging->ti->table);\r\n}\r\nif (origin_md == dm_table_get_md(ti->table))\r\norigin_md = NULL;\r\nif (origin_md) {\r\nif (dm_hold(origin_md))\r\norigin_md = NULL;\r\n}\r\nup_read(&_origins_lock);\r\nif (origin_md) {\r\ndm_internal_suspend_fast(origin_md);\r\nif (snap_merging && test_bit(RUNNING_MERGE, &snap_merging->state_bits)) {\r\nmust_restart_merging = true;\r\nstop_merge(snap_merging);\r\n}\r\n}\r\ndown_read(&_origins_lock);\r\n(void) __find_snapshots_sharing_cow(s, &snap_src, &snap_dest, NULL);\r\nif (snap_src && snap_dest) {\r\ndown_write(&snap_src->lock);\r\ndown_write_nested(&snap_dest->lock, SINGLE_DEPTH_NESTING);\r\n__handover_exceptions(snap_src, snap_dest);\r\nup_write(&snap_dest->lock);\r\nup_write(&snap_src->lock);\r\n}\r\nup_read(&_origins_lock);\r\nif (origin_md) {\r\nif (must_restart_merging)\r\nstart_merge(snap_merging);\r\ndm_internal_resume_fast(origin_md);\r\ndm_put(origin_md);\r\n}\r\nreregister_snapshot(s);\r\ndown_write(&s->lock);\r\ns->active = 1;\r\nup_write(&s->lock);\r\n}\r\nstatic uint32_t get_origin_minimum_chunksize(struct block_device *bdev)\r\n{\r\nuint32_t min_chunksize;\r\ndown_read(&_origins_lock);\r\nmin_chunksize = __minimum_chunk_size(__lookup_origin(bdev));\r\nup_read(&_origins_lock);\r\nreturn min_chunksize;\r\n}\r\nstatic void snapshot_merge_resume(struct dm_target *ti)\r\n{\r\nstruct dm_snapshot *s = ti->private;\r\nsnapshot_resume(ti);\r\nti->max_io_len = get_origin_minimum_chunksize(s->origin->bdev);\r\nstart_merge(s);\r\n}\r\nstatic void snapshot_status(struct dm_target *ti, status_type_t type,\r\nunsigned status_flags, char *result, unsigned maxlen)\r\n{\r\nunsigned sz = 0;\r\nstruct dm_snapshot *snap = ti->private;\r\nswitch (type) {\r\ncase STATUSTYPE_INFO:\r\ndown_write(&snap->lock);\r\nif (!snap->valid)\r\nDMEMIT("Invalid");\r\nelse if (snap->merge_failed)\r\nDMEMIT("Merge failed");\r\nelse {\r\nif (snap->store->type->usage) {\r\nsector_t total_sectors, sectors_allocated,\r\nmetadata_sectors;\r\nsnap->store->type->usage(snap->store,\r\n&total_sectors,\r\n&sectors_allocated,\r\n&metadata_sectors);\r\nDMEMIT("%llu/%llu %llu",\r\n(unsigned long long)sectors_allocated,\r\n(unsigned long long)total_sectors,\r\n(unsigned long long)metadata_sectors);\r\n}\r\nelse\r\nDMEMIT("Unknown");\r\n}\r\nup_write(&snap->lock);\r\nbreak;\r\ncase STATUSTYPE_TABLE:\r\nDMEMIT("%s %s", snap->origin->name, snap->cow->name);\r\nsnap->store->type->status(snap->store, type, result + sz,\r\nmaxlen - sz);\r\nbreak;\r\n}\r\n}\r\nstatic int snapshot_iterate_devices(struct dm_target *ti,\r\niterate_devices_callout_fn fn, void *data)\r\n{\r\nstruct dm_snapshot *snap = ti->private;\r\nint r;\r\nr = fn(ti, snap->origin, 0, ti->len, data);\r\nif (!r)\r\nr = fn(ti, snap->cow, 0, get_dev_size(snap->cow->bdev), data);\r\nreturn r;\r\n}\r\nstatic int __origin_write(struct list_head *snapshots, sector_t sector,\r\nstruct bio *bio)\r\n{\r\nint r = DM_MAPIO_REMAPPED;\r\nstruct dm_snapshot *snap;\r\nstruct dm_exception *e;\r\nstruct dm_snap_pending_exception *pe;\r\nstruct dm_snap_pending_exception *pe_to_start_now = NULL;\r\nstruct dm_snap_pending_exception *pe_to_start_last = NULL;\r\nchunk_t chunk;\r\nlist_for_each_entry (snap, snapshots, list) {\r\nif (dm_target_is_snapshot_merge(snap->ti))\r\ncontinue;\r\ndown_write(&snap->lock);\r\nif (!snap->valid || !snap->active)\r\ngoto next_snapshot;\r\nif (sector >= dm_table_get_size(snap->ti->table))\r\ngoto next_snapshot;\r\nchunk = sector_to_chunk(snap->store, sector);\r\ne = dm_lookup_exception(&snap->complete, chunk);\r\nif (e)\r\ngoto next_snapshot;\r\npe = __lookup_pending_exception(snap, chunk);\r\nif (!pe) {\r\nup_write(&snap->lock);\r\npe = alloc_pending_exception(snap);\r\ndown_write(&snap->lock);\r\nif (!snap->valid) {\r\nfree_pending_exception(pe);\r\ngoto next_snapshot;\r\n}\r\ne = dm_lookup_exception(&snap->complete, chunk);\r\nif (e) {\r\nfree_pending_exception(pe);\r\ngoto next_snapshot;\r\n}\r\npe = __find_pending_exception(snap, pe, chunk);\r\nif (!pe) {\r\n__invalidate_snapshot(snap, -ENOMEM);\r\ngoto next_snapshot;\r\n}\r\n}\r\nr = DM_MAPIO_SUBMITTED;\r\nif (bio) {\r\nbio_list_add(&pe->origin_bios, bio);\r\nbio = NULL;\r\nif (!pe->started) {\r\npe->started = 1;\r\npe_to_start_last = pe;\r\n}\r\n}\r\nif (!pe->started) {\r\npe->started = 1;\r\npe_to_start_now = pe;\r\n}\r\nnext_snapshot:\r\nup_write(&snap->lock);\r\nif (pe_to_start_now) {\r\nstart_copy(pe_to_start_now);\r\npe_to_start_now = NULL;\r\n}\r\n}\r\nif (pe_to_start_last)\r\nstart_copy(pe_to_start_last);\r\nreturn r;\r\n}\r\nstatic int do_origin(struct dm_dev *origin, struct bio *bio)\r\n{\r\nstruct origin *o;\r\nint r = DM_MAPIO_REMAPPED;\r\ndown_read(&_origins_lock);\r\no = __lookup_origin(origin->bdev);\r\nif (o)\r\nr = __origin_write(&o->snapshots, bio->bi_iter.bi_sector, bio);\r\nup_read(&_origins_lock);\r\nreturn r;\r\n}\r\nstatic int origin_write_extent(struct dm_snapshot *merging_snap,\r\nsector_t sector, unsigned size)\r\n{\r\nint must_wait = 0;\r\nsector_t n;\r\nstruct origin *o;\r\ndown_read(&_origins_lock);\r\no = __lookup_origin(merging_snap->origin->bdev);\r\nfor (n = 0; n < size; n += merging_snap->ti->max_io_len)\r\nif (__origin_write(&o->snapshots, sector + n, NULL) ==\r\nDM_MAPIO_SUBMITTED)\r\nmust_wait = 1;\r\nup_read(&_origins_lock);\r\nreturn must_wait;\r\n}\r\nstatic int origin_ctr(struct dm_target *ti, unsigned int argc, char **argv)\r\n{\r\nint r;\r\nstruct dm_origin *o;\r\nif (argc != 1) {\r\nti->error = "origin: incorrect number of arguments";\r\nreturn -EINVAL;\r\n}\r\no = kmalloc(sizeof(struct dm_origin), GFP_KERNEL);\r\nif (!o) {\r\nti->error = "Cannot allocate private origin structure";\r\nr = -ENOMEM;\r\ngoto bad_alloc;\r\n}\r\nr = dm_get_device(ti, argv[0], dm_table_get_mode(ti->table), &o->dev);\r\nif (r) {\r\nti->error = "Cannot get target device";\r\ngoto bad_open;\r\n}\r\no->ti = ti;\r\nti->private = o;\r\nti->num_flush_bios = 1;\r\nreturn 0;\r\nbad_open:\r\nkfree(o);\r\nbad_alloc:\r\nreturn r;\r\n}\r\nstatic void origin_dtr(struct dm_target *ti)\r\n{\r\nstruct dm_origin *o = ti->private;\r\ndm_put_device(ti, o->dev);\r\nkfree(o);\r\n}\r\nstatic int origin_map(struct dm_target *ti, struct bio *bio)\r\n{\r\nstruct dm_origin *o = ti->private;\r\nunsigned available_sectors;\r\nbio->bi_bdev = o->dev->bdev;\r\nif (unlikely(bio->bi_rw & REQ_FLUSH))\r\nreturn DM_MAPIO_REMAPPED;\r\nif (bio_rw(bio) != WRITE)\r\nreturn DM_MAPIO_REMAPPED;\r\navailable_sectors = o->split_boundary -\r\n((unsigned)bio->bi_iter.bi_sector & (o->split_boundary - 1));\r\nif (bio_sectors(bio) > available_sectors)\r\ndm_accept_partial_bio(bio, available_sectors);\r\nreturn do_origin(o->dev, bio);\r\n}\r\nstatic void origin_resume(struct dm_target *ti)\r\n{\r\nstruct dm_origin *o = ti->private;\r\no->split_boundary = get_origin_minimum_chunksize(o->dev->bdev);\r\ndown_write(&_origins_lock);\r\n__insert_dm_origin(o);\r\nup_write(&_origins_lock);\r\n}\r\nstatic void origin_postsuspend(struct dm_target *ti)\r\n{\r\nstruct dm_origin *o = ti->private;\r\ndown_write(&_origins_lock);\r\n__remove_dm_origin(o);\r\nup_write(&_origins_lock);\r\n}\r\nstatic void origin_status(struct dm_target *ti, status_type_t type,\r\nunsigned status_flags, char *result, unsigned maxlen)\r\n{\r\nstruct dm_origin *o = ti->private;\r\nswitch (type) {\r\ncase STATUSTYPE_INFO:\r\nresult[0] = '\0';\r\nbreak;\r\ncase STATUSTYPE_TABLE:\r\nsnprintf(result, maxlen, "%s", o->dev->name);\r\nbreak;\r\n}\r\n}\r\nstatic int origin_merge(struct dm_target *ti, struct bvec_merge_data *bvm,\r\nstruct bio_vec *biovec, int max_size)\r\n{\r\nstruct dm_origin *o = ti->private;\r\nstruct request_queue *q = bdev_get_queue(o->dev->bdev);\r\nif (!q->merge_bvec_fn)\r\nreturn max_size;\r\nbvm->bi_bdev = o->dev->bdev;\r\nreturn min(max_size, q->merge_bvec_fn(q, bvm, biovec));\r\n}\r\nstatic int origin_iterate_devices(struct dm_target *ti,\r\niterate_devices_callout_fn fn, void *data)\r\n{\r\nstruct dm_origin *o = ti->private;\r\nreturn fn(ti, o->dev, 0, ti->len, data);\r\n}\r\nstatic int __init dm_snapshot_init(void)\r\n{\r\nint r;\r\nr = dm_exception_store_init();\r\nif (r) {\r\nDMERR("Failed to initialize exception stores");\r\nreturn r;\r\n}\r\nr = dm_register_target(&snapshot_target);\r\nif (r < 0) {\r\nDMERR("snapshot target register failed %d", r);\r\ngoto bad_register_snapshot_target;\r\n}\r\nr = dm_register_target(&origin_target);\r\nif (r < 0) {\r\nDMERR("Origin target register failed %d", r);\r\ngoto bad_register_origin_target;\r\n}\r\nr = dm_register_target(&merge_target);\r\nif (r < 0) {\r\nDMERR("Merge target register failed %d", r);\r\ngoto bad_register_merge_target;\r\n}\r\nr = init_origin_hash();\r\nif (r) {\r\nDMERR("init_origin_hash failed.");\r\ngoto bad_origin_hash;\r\n}\r\nexception_cache = KMEM_CACHE(dm_exception, 0);\r\nif (!exception_cache) {\r\nDMERR("Couldn't create exception cache.");\r\nr = -ENOMEM;\r\ngoto bad_exception_cache;\r\n}\r\npending_cache = KMEM_CACHE(dm_snap_pending_exception, 0);\r\nif (!pending_cache) {\r\nDMERR("Couldn't create pending cache.");\r\nr = -ENOMEM;\r\ngoto bad_pending_cache;\r\n}\r\nreturn 0;\r\nbad_pending_cache:\r\nkmem_cache_destroy(exception_cache);\r\nbad_exception_cache:\r\nexit_origin_hash();\r\nbad_origin_hash:\r\ndm_unregister_target(&merge_target);\r\nbad_register_merge_target:\r\ndm_unregister_target(&origin_target);\r\nbad_register_origin_target:\r\ndm_unregister_target(&snapshot_target);\r\nbad_register_snapshot_target:\r\ndm_exception_store_exit();\r\nreturn r;\r\n}\r\nstatic void __exit dm_snapshot_exit(void)\r\n{\r\ndm_unregister_target(&snapshot_target);\r\ndm_unregister_target(&origin_target);\r\ndm_unregister_target(&merge_target);\r\nexit_origin_hash();\r\nkmem_cache_destroy(pending_cache);\r\nkmem_cache_destroy(exception_cache);\r\ndm_exception_store_exit();\r\n}
