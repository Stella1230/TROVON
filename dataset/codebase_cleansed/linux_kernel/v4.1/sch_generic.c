static inline int dev_requeue_skb(struct sk_buff *skb, struct Qdisc *q)\r\n{\r\nq->gso_skb = skb;\r\nq->qstats.requeues++;\r\nq->q.qlen++;\r\n__netif_schedule(q);\r\nreturn 0;\r\n}\r\nstatic void try_bulk_dequeue_skb(struct Qdisc *q,\r\nstruct sk_buff *skb,\r\nconst struct netdev_queue *txq,\r\nint *packets)\r\n{\r\nint bytelimit = qdisc_avail_bulklimit(txq) - skb->len;\r\nwhile (bytelimit > 0) {\r\nstruct sk_buff *nskb = q->dequeue(q);\r\nif (!nskb)\r\nbreak;\r\nbytelimit -= nskb->len;\r\nskb->next = nskb;\r\nskb = nskb;\r\n(*packets)++;\r\n}\r\nskb->next = NULL;\r\n}\r\nstatic struct sk_buff *dequeue_skb(struct Qdisc *q, bool *validate,\r\nint *packets)\r\n{\r\nstruct sk_buff *skb = q->gso_skb;\r\nconst struct netdev_queue *txq = q->dev_queue;\r\n*packets = 1;\r\n*validate = true;\r\nif (unlikely(skb)) {\r\ntxq = skb_get_tx_queue(txq->dev, skb);\r\nif (!netif_xmit_frozen_or_stopped(txq)) {\r\nq->gso_skb = NULL;\r\nq->q.qlen--;\r\n} else\r\nskb = NULL;\r\n*validate = false;\r\n} else {\r\nif (!(q->flags & TCQ_F_ONETXQUEUE) ||\r\n!netif_xmit_frozen_or_stopped(txq)) {\r\nskb = q->dequeue(q);\r\nif (skb && qdisc_may_bulk(q))\r\ntry_bulk_dequeue_skb(q, skb, txq, packets);\r\n}\r\n}\r\nreturn skb;\r\n}\r\nstatic inline int handle_dev_cpu_collision(struct sk_buff *skb,\r\nstruct netdev_queue *dev_queue,\r\nstruct Qdisc *q)\r\n{\r\nint ret;\r\nif (unlikely(dev_queue->xmit_lock_owner == smp_processor_id())) {\r\nkfree_skb_list(skb);\r\nnet_warn_ratelimited("Dead loop on netdevice %s, fix it urgently!\n",\r\ndev_queue->dev->name);\r\nret = qdisc_qlen(q);\r\n} else {\r\n__this_cpu_inc(softnet_data.cpu_collision);\r\nret = dev_requeue_skb(skb, q);\r\n}\r\nreturn ret;\r\n}\r\nint sch_direct_xmit(struct sk_buff *skb, struct Qdisc *q,\r\nstruct net_device *dev, struct netdev_queue *txq,\r\nspinlock_t *root_lock, bool validate)\r\n{\r\nint ret = NETDEV_TX_BUSY;\r\nspin_unlock(root_lock);\r\nif (validate)\r\nskb = validate_xmit_skb_list(skb, dev);\r\nif (skb) {\r\nHARD_TX_LOCK(dev, txq, smp_processor_id());\r\nif (!netif_xmit_frozen_or_stopped(txq))\r\nskb = dev_hard_start_xmit(skb, dev, txq, &ret);\r\nHARD_TX_UNLOCK(dev, txq);\r\n}\r\nspin_lock(root_lock);\r\nif (dev_xmit_complete(ret)) {\r\nret = qdisc_qlen(q);\r\n} else if (ret == NETDEV_TX_LOCKED) {\r\nret = handle_dev_cpu_collision(skb, txq, q);\r\n} else {\r\nif (unlikely(ret != NETDEV_TX_BUSY))\r\nnet_warn_ratelimited("BUG %s code %d qlen %d\n",\r\ndev->name, ret, q->q.qlen);\r\nret = dev_requeue_skb(skb, q);\r\n}\r\nif (ret && netif_xmit_frozen_or_stopped(txq))\r\nret = 0;\r\nreturn ret;\r\n}\r\nstatic inline int qdisc_restart(struct Qdisc *q, int *packets)\r\n{\r\nstruct netdev_queue *txq;\r\nstruct net_device *dev;\r\nspinlock_t *root_lock;\r\nstruct sk_buff *skb;\r\nbool validate;\r\nskb = dequeue_skb(q, &validate, packets);\r\nif (unlikely(!skb))\r\nreturn 0;\r\nroot_lock = qdisc_lock(q);\r\ndev = qdisc_dev(q);\r\ntxq = skb_get_tx_queue(dev, skb);\r\nreturn sch_direct_xmit(skb, q, dev, txq, root_lock, validate);\r\n}\r\nvoid __qdisc_run(struct Qdisc *q)\r\n{\r\nint quota = weight_p;\r\nint packets;\r\nwhile (qdisc_restart(q, &packets)) {\r\nquota -= packets;\r\nif (quota <= 0 || need_resched()) {\r\n__netif_schedule(q);\r\nbreak;\r\n}\r\n}\r\nqdisc_run_end(q);\r\n}\r\nunsigned long dev_trans_start(struct net_device *dev)\r\n{\r\nunsigned long val, res;\r\nunsigned int i;\r\nif (is_vlan_dev(dev))\r\ndev = vlan_dev_real_dev(dev);\r\nres = dev->trans_start;\r\nfor (i = 0; i < dev->num_tx_queues; i++) {\r\nval = netdev_get_tx_queue(dev, i)->trans_start;\r\nif (val && time_after(val, res))\r\nres = val;\r\n}\r\ndev->trans_start = res;\r\nreturn res;\r\n}\r\nstatic void dev_watchdog(unsigned long arg)\r\n{\r\nstruct net_device *dev = (struct net_device *)arg;\r\nnetif_tx_lock(dev);\r\nif (!qdisc_tx_is_noop(dev)) {\r\nif (netif_device_present(dev) &&\r\nnetif_running(dev) &&\r\nnetif_carrier_ok(dev)) {\r\nint some_queue_timedout = 0;\r\nunsigned int i;\r\nunsigned long trans_start;\r\nfor (i = 0; i < dev->num_tx_queues; i++) {\r\nstruct netdev_queue *txq;\r\ntxq = netdev_get_tx_queue(dev, i);\r\ntrans_start = txq->trans_start ? : dev->trans_start;\r\nif (netif_xmit_stopped(txq) &&\r\ntime_after(jiffies, (trans_start +\r\ndev->watchdog_timeo))) {\r\nsome_queue_timedout = 1;\r\ntxq->trans_timeout++;\r\nbreak;\r\n}\r\n}\r\nif (some_queue_timedout) {\r\nWARN_ONCE(1, KERN_INFO "NETDEV WATCHDOG: %s (%s): transmit queue %u timed out\n",\r\ndev->name, netdev_drivername(dev), i);\r\ndev->netdev_ops->ndo_tx_timeout(dev);\r\n}\r\nif (!mod_timer(&dev->watchdog_timer,\r\nround_jiffies(jiffies +\r\ndev->watchdog_timeo)))\r\ndev_hold(dev);\r\n}\r\n}\r\nnetif_tx_unlock(dev);\r\ndev_put(dev);\r\n}\r\nvoid __netdev_watchdog_up(struct net_device *dev)\r\n{\r\nif (dev->netdev_ops->ndo_tx_timeout) {\r\nif (dev->watchdog_timeo <= 0)\r\ndev->watchdog_timeo = 5*HZ;\r\nif (!mod_timer(&dev->watchdog_timer,\r\nround_jiffies(jiffies + dev->watchdog_timeo)))\r\ndev_hold(dev);\r\n}\r\n}\r\nstatic void dev_watchdog_up(struct net_device *dev)\r\n{\r\n__netdev_watchdog_up(dev);\r\n}\r\nstatic void dev_watchdog_down(struct net_device *dev)\r\n{\r\nnetif_tx_lock_bh(dev);\r\nif (del_timer(&dev->watchdog_timer))\r\ndev_put(dev);\r\nnetif_tx_unlock_bh(dev);\r\n}\r\nvoid netif_carrier_on(struct net_device *dev)\r\n{\r\nif (test_and_clear_bit(__LINK_STATE_NOCARRIER, &dev->state)) {\r\nif (dev->reg_state == NETREG_UNINITIALIZED)\r\nreturn;\r\natomic_inc(&dev->carrier_changes);\r\nlinkwatch_fire_event(dev);\r\nif (netif_running(dev))\r\n__netdev_watchdog_up(dev);\r\n}\r\n}\r\nvoid netif_carrier_off(struct net_device *dev)\r\n{\r\nif (!test_and_set_bit(__LINK_STATE_NOCARRIER, &dev->state)) {\r\nif (dev->reg_state == NETREG_UNINITIALIZED)\r\nreturn;\r\natomic_inc(&dev->carrier_changes);\r\nlinkwatch_fire_event(dev);\r\n}\r\n}\r\nstatic int noop_enqueue(struct sk_buff *skb, struct Qdisc *qdisc)\r\n{\r\nkfree_skb(skb);\r\nreturn NET_XMIT_CN;\r\n}\r\nstatic struct sk_buff *noop_dequeue(struct Qdisc *qdisc)\r\n{\r\nreturn NULL;\r\n}\r\nstatic inline struct sk_buff_head *band2list(struct pfifo_fast_priv *priv,\r\nint band)\r\n{\r\nreturn priv->q + band;\r\n}\r\nstatic int pfifo_fast_enqueue(struct sk_buff *skb, struct Qdisc *qdisc)\r\n{\r\nif (skb_queue_len(&qdisc->q) < qdisc_dev(qdisc)->tx_queue_len) {\r\nint band = prio2band[skb->priority & TC_PRIO_MAX];\r\nstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\r\nstruct sk_buff_head *list = band2list(priv, band);\r\npriv->bitmap |= (1 << band);\r\nqdisc->q.qlen++;\r\nreturn __qdisc_enqueue_tail(skb, qdisc, list);\r\n}\r\nreturn qdisc_drop(skb, qdisc);\r\n}\r\nstatic struct sk_buff *pfifo_fast_dequeue(struct Qdisc *qdisc)\r\n{\r\nstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\r\nint band = bitmap2band[priv->bitmap];\r\nif (likely(band >= 0)) {\r\nstruct sk_buff_head *list = band2list(priv, band);\r\nstruct sk_buff *skb = __qdisc_dequeue_head(qdisc, list);\r\nqdisc->q.qlen--;\r\nif (skb_queue_empty(list))\r\npriv->bitmap &= ~(1 << band);\r\nreturn skb;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct sk_buff *pfifo_fast_peek(struct Qdisc *qdisc)\r\n{\r\nstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\r\nint band = bitmap2band[priv->bitmap];\r\nif (band >= 0) {\r\nstruct sk_buff_head *list = band2list(priv, band);\r\nreturn skb_peek(list);\r\n}\r\nreturn NULL;\r\n}\r\nstatic void pfifo_fast_reset(struct Qdisc *qdisc)\r\n{\r\nint prio;\r\nstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\r\nfor (prio = 0; prio < PFIFO_FAST_BANDS; prio++)\r\n__qdisc_reset_queue(qdisc, band2list(priv, prio));\r\npriv->bitmap = 0;\r\nqdisc->qstats.backlog = 0;\r\nqdisc->q.qlen = 0;\r\n}\r\nstatic int pfifo_fast_dump(struct Qdisc *qdisc, struct sk_buff *skb)\r\n{\r\nstruct tc_prio_qopt opt = { .bands = PFIFO_FAST_BANDS };\r\nmemcpy(&opt.priomap, prio2band, TC_PRIO_MAX + 1);\r\nif (nla_put(skb, TCA_OPTIONS, sizeof(opt), &opt))\r\ngoto nla_put_failure;\r\nreturn skb->len;\r\nnla_put_failure:\r\nreturn -1;\r\n}\r\nstatic int pfifo_fast_init(struct Qdisc *qdisc, struct nlattr *opt)\r\n{\r\nint prio;\r\nstruct pfifo_fast_priv *priv = qdisc_priv(qdisc);\r\nfor (prio = 0; prio < PFIFO_FAST_BANDS; prio++)\r\n__skb_queue_head_init(band2list(priv, prio));\r\nqdisc->flags |= TCQ_F_CAN_BYPASS;\r\nreturn 0;\r\n}\r\nstruct Qdisc *qdisc_alloc(struct netdev_queue *dev_queue,\r\nconst struct Qdisc_ops *ops)\r\n{\r\nvoid *p;\r\nstruct Qdisc *sch;\r\nunsigned int size = QDISC_ALIGN(sizeof(*sch)) + ops->priv_size;\r\nint err = -ENOBUFS;\r\nstruct net_device *dev = dev_queue->dev;\r\np = kzalloc_node(size, GFP_KERNEL,\r\nnetdev_queue_numa_node_read(dev_queue));\r\nif (!p)\r\ngoto errout;\r\nsch = (struct Qdisc *) QDISC_ALIGN((unsigned long) p);\r\nif (sch != p) {\r\nkfree(p);\r\np = kzalloc_node(size + QDISC_ALIGNTO - 1, GFP_KERNEL,\r\nnetdev_queue_numa_node_read(dev_queue));\r\nif (!p)\r\ngoto errout;\r\nsch = (struct Qdisc *) QDISC_ALIGN((unsigned long) p);\r\nsch->padded = (char *) sch - (char *) p;\r\n}\r\nINIT_LIST_HEAD(&sch->list);\r\nskb_queue_head_init(&sch->q);\r\nspin_lock_init(&sch->busylock);\r\nlockdep_set_class(&sch->busylock,\r\ndev->qdisc_tx_busylock ?: &qdisc_tx_busylock);\r\nsch->ops = ops;\r\nsch->enqueue = ops->enqueue;\r\nsch->dequeue = ops->dequeue;\r\nsch->dev_queue = dev_queue;\r\ndev_hold(dev);\r\natomic_set(&sch->refcnt, 1);\r\nreturn sch;\r\nerrout:\r\nreturn ERR_PTR(err);\r\n}\r\nstruct Qdisc *qdisc_create_dflt(struct netdev_queue *dev_queue,\r\nconst struct Qdisc_ops *ops,\r\nunsigned int parentid)\r\n{\r\nstruct Qdisc *sch;\r\nif (!try_module_get(ops->owner))\r\ngoto errout;\r\nsch = qdisc_alloc(dev_queue, ops);\r\nif (IS_ERR(sch))\r\ngoto errout;\r\nsch->parent = parentid;\r\nif (!ops->init || ops->init(sch, NULL) == 0)\r\nreturn sch;\r\nqdisc_destroy(sch);\r\nerrout:\r\nreturn NULL;\r\n}\r\nvoid qdisc_reset(struct Qdisc *qdisc)\r\n{\r\nconst struct Qdisc_ops *ops = qdisc->ops;\r\nif (ops->reset)\r\nops->reset(qdisc);\r\nif (qdisc->gso_skb) {\r\nkfree_skb_list(qdisc->gso_skb);\r\nqdisc->gso_skb = NULL;\r\nqdisc->q.qlen = 0;\r\n}\r\n}\r\nstatic void qdisc_rcu_free(struct rcu_head *head)\r\n{\r\nstruct Qdisc *qdisc = container_of(head, struct Qdisc, rcu_head);\r\nif (qdisc_is_percpu_stats(qdisc))\r\nfree_percpu(qdisc->cpu_bstats);\r\nkfree((char *) qdisc - qdisc->padded);\r\n}\r\nvoid qdisc_destroy(struct Qdisc *qdisc)\r\n{\r\nconst struct Qdisc_ops *ops = qdisc->ops;\r\nif (qdisc->flags & TCQ_F_BUILTIN ||\r\n!atomic_dec_and_test(&qdisc->refcnt))\r\nreturn;\r\n#ifdef CONFIG_NET_SCHED\r\nqdisc_list_del(qdisc);\r\nqdisc_put_stab(rtnl_dereference(qdisc->stab));\r\n#endif\r\ngen_kill_estimator(&qdisc->bstats, &qdisc->rate_est);\r\nif (ops->reset)\r\nops->reset(qdisc);\r\nif (ops->destroy)\r\nops->destroy(qdisc);\r\nmodule_put(ops->owner);\r\ndev_put(qdisc_dev(qdisc));\r\nkfree_skb_list(qdisc->gso_skb);\r\ncall_rcu(&qdisc->rcu_head, qdisc_rcu_free);\r\n}\r\nstruct Qdisc *dev_graft_qdisc(struct netdev_queue *dev_queue,\r\nstruct Qdisc *qdisc)\r\n{\r\nstruct Qdisc *oqdisc = dev_queue->qdisc_sleeping;\r\nspinlock_t *root_lock;\r\nroot_lock = qdisc_lock(oqdisc);\r\nspin_lock_bh(root_lock);\r\nif (oqdisc && atomic_read(&oqdisc->refcnt) <= 1)\r\nqdisc_reset(oqdisc);\r\nif (qdisc == NULL)\r\nqdisc = &noop_qdisc;\r\ndev_queue->qdisc_sleeping = qdisc;\r\nrcu_assign_pointer(dev_queue->qdisc, &noop_qdisc);\r\nspin_unlock_bh(root_lock);\r\nreturn oqdisc;\r\n}\r\nstatic void attach_one_default_qdisc(struct net_device *dev,\r\nstruct netdev_queue *dev_queue,\r\nvoid *_unused)\r\n{\r\nstruct Qdisc *qdisc = &noqueue_qdisc;\r\nif (dev->tx_queue_len) {\r\nqdisc = qdisc_create_dflt(dev_queue,\r\ndefault_qdisc_ops, TC_H_ROOT);\r\nif (!qdisc) {\r\nnetdev_info(dev, "activation failed\n");\r\nreturn;\r\n}\r\nif (!netif_is_multiqueue(dev))\r\nqdisc->flags |= TCQ_F_ONETXQUEUE;\r\n}\r\ndev_queue->qdisc_sleeping = qdisc;\r\n}\r\nstatic void attach_default_qdiscs(struct net_device *dev)\r\n{\r\nstruct netdev_queue *txq;\r\nstruct Qdisc *qdisc;\r\ntxq = netdev_get_tx_queue(dev, 0);\r\nif (!netif_is_multiqueue(dev) || dev->tx_queue_len == 0) {\r\nnetdev_for_each_tx_queue(dev, attach_one_default_qdisc, NULL);\r\ndev->qdisc = txq->qdisc_sleeping;\r\natomic_inc(&dev->qdisc->refcnt);\r\n} else {\r\nqdisc = qdisc_create_dflt(txq, &mq_qdisc_ops, TC_H_ROOT);\r\nif (qdisc) {\r\ndev->qdisc = qdisc;\r\nqdisc->ops->attach(qdisc);\r\n}\r\n}\r\n}\r\nstatic void transition_one_qdisc(struct net_device *dev,\r\nstruct netdev_queue *dev_queue,\r\nvoid *_need_watchdog)\r\n{\r\nstruct Qdisc *new_qdisc = dev_queue->qdisc_sleeping;\r\nint *need_watchdog_p = _need_watchdog;\r\nif (!(new_qdisc->flags & TCQ_F_BUILTIN))\r\nclear_bit(__QDISC_STATE_DEACTIVATED, &new_qdisc->state);\r\nrcu_assign_pointer(dev_queue->qdisc, new_qdisc);\r\nif (need_watchdog_p && new_qdisc != &noqueue_qdisc) {\r\ndev_queue->trans_start = 0;\r\n*need_watchdog_p = 1;\r\n}\r\n}\r\nvoid dev_activate(struct net_device *dev)\r\n{\r\nint need_watchdog;\r\nif (dev->qdisc == &noop_qdisc)\r\nattach_default_qdiscs(dev);\r\nif (!netif_carrier_ok(dev))\r\nreturn;\r\nneed_watchdog = 0;\r\nnetdev_for_each_tx_queue(dev, transition_one_qdisc, &need_watchdog);\r\nif (dev_ingress_queue(dev))\r\ntransition_one_qdisc(dev, dev_ingress_queue(dev), NULL);\r\nif (need_watchdog) {\r\ndev->trans_start = jiffies;\r\ndev_watchdog_up(dev);\r\n}\r\n}\r\nstatic void dev_deactivate_queue(struct net_device *dev,\r\nstruct netdev_queue *dev_queue,\r\nvoid *_qdisc_default)\r\n{\r\nstruct Qdisc *qdisc_default = _qdisc_default;\r\nstruct Qdisc *qdisc;\r\nqdisc = rtnl_dereference(dev_queue->qdisc);\r\nif (qdisc) {\r\nspin_lock_bh(qdisc_lock(qdisc));\r\nif (!(qdisc->flags & TCQ_F_BUILTIN))\r\nset_bit(__QDISC_STATE_DEACTIVATED, &qdisc->state);\r\nrcu_assign_pointer(dev_queue->qdisc, qdisc_default);\r\nqdisc_reset(qdisc);\r\nspin_unlock_bh(qdisc_lock(qdisc));\r\n}\r\n}\r\nstatic bool some_qdisc_is_busy(struct net_device *dev)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < dev->num_tx_queues; i++) {\r\nstruct netdev_queue *dev_queue;\r\nspinlock_t *root_lock;\r\nstruct Qdisc *q;\r\nint val;\r\ndev_queue = netdev_get_tx_queue(dev, i);\r\nq = dev_queue->qdisc_sleeping;\r\nroot_lock = qdisc_lock(q);\r\nspin_lock_bh(root_lock);\r\nval = (qdisc_is_running(q) ||\r\ntest_bit(__QDISC_STATE_SCHED, &q->state));\r\nspin_unlock_bh(root_lock);\r\nif (val)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nvoid dev_deactivate_many(struct list_head *head)\r\n{\r\nstruct net_device *dev;\r\nbool sync_needed = false;\r\nlist_for_each_entry(dev, head, close_list) {\r\nnetdev_for_each_tx_queue(dev, dev_deactivate_queue,\r\n&noop_qdisc);\r\nif (dev_ingress_queue(dev))\r\ndev_deactivate_queue(dev, dev_ingress_queue(dev),\r\n&noop_qdisc);\r\ndev_watchdog_down(dev);\r\nsync_needed |= !dev->dismantle;\r\n}\r\nif (sync_needed)\r\nsynchronize_net();\r\nlist_for_each_entry(dev, head, close_list)\r\nwhile (some_qdisc_is_busy(dev))\r\nyield();\r\n}\r\nvoid dev_deactivate(struct net_device *dev)\r\n{\r\nLIST_HEAD(single);\r\nlist_add(&dev->close_list, &single);\r\ndev_deactivate_many(&single);\r\nlist_del(&single);\r\n}\r\nstatic void dev_init_scheduler_queue(struct net_device *dev,\r\nstruct netdev_queue *dev_queue,\r\nvoid *_qdisc)\r\n{\r\nstruct Qdisc *qdisc = _qdisc;\r\nrcu_assign_pointer(dev_queue->qdisc, qdisc);\r\ndev_queue->qdisc_sleeping = qdisc;\r\n}\r\nvoid dev_init_scheduler(struct net_device *dev)\r\n{\r\ndev->qdisc = &noop_qdisc;\r\nnetdev_for_each_tx_queue(dev, dev_init_scheduler_queue, &noop_qdisc);\r\nif (dev_ingress_queue(dev))\r\ndev_init_scheduler_queue(dev, dev_ingress_queue(dev), &noop_qdisc);\r\nsetup_timer(&dev->watchdog_timer, dev_watchdog, (unsigned long)dev);\r\n}\r\nstatic void shutdown_scheduler_queue(struct net_device *dev,\r\nstruct netdev_queue *dev_queue,\r\nvoid *_qdisc_default)\r\n{\r\nstruct Qdisc *qdisc = dev_queue->qdisc_sleeping;\r\nstruct Qdisc *qdisc_default = _qdisc_default;\r\nif (qdisc) {\r\nrcu_assign_pointer(dev_queue->qdisc, qdisc_default);\r\ndev_queue->qdisc_sleeping = qdisc_default;\r\nqdisc_destroy(qdisc);\r\n}\r\n}\r\nvoid dev_shutdown(struct net_device *dev)\r\n{\r\nnetdev_for_each_tx_queue(dev, shutdown_scheduler_queue, &noop_qdisc);\r\nif (dev_ingress_queue(dev))\r\nshutdown_scheduler_queue(dev, dev_ingress_queue(dev), &noop_qdisc);\r\nqdisc_destroy(dev->qdisc);\r\ndev->qdisc = &noop_qdisc;\r\nWARN_ON(timer_pending(&dev->watchdog_timer));\r\n}\r\nvoid psched_ratecfg_precompute(struct psched_ratecfg *r,\r\nconst struct tc_ratespec *conf,\r\nu64 rate64)\r\n{\r\nmemset(r, 0, sizeof(*r));\r\nr->overhead = conf->overhead;\r\nr->rate_bytes_ps = max_t(u64, conf->rate, rate64);\r\nr->linklayer = (conf->linklayer & TC_LINKLAYER_MASK);\r\nr->mult = 1;\r\nif (r->rate_bytes_ps > 0) {\r\nu64 factor = NSEC_PER_SEC;\r\nfor (;;) {\r\nr->mult = div64_u64(factor, r->rate_bytes_ps);\r\nif (r->mult & (1U << 31) || factor & (1ULL << 63))\r\nbreak;\r\nfactor <<= 1;\r\nr->shift++;\r\n}\r\n}\r\n}
