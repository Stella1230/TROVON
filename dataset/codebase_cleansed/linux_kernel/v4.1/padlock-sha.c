static int padlock_sha_init(struct shash_desc *desc)\r\n{\r\nstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\r\nstruct padlock_sha_ctx *ctx = crypto_shash_ctx(desc->tfm);\r\ndctx->fallback.tfm = ctx->fallback;\r\ndctx->fallback.flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_shash_init(&dctx->fallback);\r\n}\r\nstatic int padlock_sha_update(struct shash_desc *desc,\r\nconst u8 *data, unsigned int length)\r\n{\r\nstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\r\ndctx->fallback.flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_shash_update(&dctx->fallback, data, length);\r\n}\r\nstatic int padlock_sha_export(struct shash_desc *desc, void *out)\r\n{\r\nstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\r\nreturn crypto_shash_export(&dctx->fallback, out);\r\n}\r\nstatic int padlock_sha_import(struct shash_desc *desc, const void *in)\r\n{\r\nstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\r\nstruct padlock_sha_ctx *ctx = crypto_shash_ctx(desc->tfm);\r\ndctx->fallback.tfm = ctx->fallback;\r\ndctx->fallback.flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_shash_import(&dctx->fallback, in);\r\n}\r\nstatic inline void padlock_output_block(uint32_t *src,\r\nuint32_t *dst, size_t count)\r\n{\r\nwhile (count--)\r\n*dst++ = swab32(*src++);\r\n}\r\nstatic int padlock_sha1_finup(struct shash_desc *desc, const u8 *in,\r\nunsigned int count, u8 *out)\r\n{\r\nchar buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__\r\n((aligned(STACK_ALIGN)));\r\nchar *result = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\r\nstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\r\nstruct sha1_state state;\r\nunsigned int space;\r\nunsigned int leftover;\r\nint ts_state;\r\nint err;\r\ndctx->fallback.flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nerr = crypto_shash_export(&dctx->fallback, &state);\r\nif (err)\r\ngoto out;\r\nif (state.count + count > ULONG_MAX)\r\nreturn crypto_shash_finup(&dctx->fallback, in, count, out);\r\nleftover = ((state.count - 1) & (SHA1_BLOCK_SIZE - 1)) + 1;\r\nspace = SHA1_BLOCK_SIZE - leftover;\r\nif (space) {\r\nif (count > space) {\r\nerr = crypto_shash_update(&dctx->fallback, in, space) ?:\r\ncrypto_shash_export(&dctx->fallback, &state);\r\nif (err)\r\ngoto out;\r\ncount -= space;\r\nin += space;\r\n} else {\r\nmemcpy(state.buffer + leftover, in, count);\r\nin = state.buffer;\r\ncount += leftover;\r\nstate.count &= ~(SHA1_BLOCK_SIZE - 1);\r\n}\r\n}\r\nmemcpy(result, &state.state, SHA1_DIGEST_SIZE);\r\nts_state = irq_ts_save();\r\nasm volatile (".byte 0xf3,0x0f,0xa6,0xc8"\r\n: \\r\n: "c"((unsigned long)state.count + count), \\r\n"a"((unsigned long)state.count), \\r\n"S"(in), "D"(result));\r\nirq_ts_restore(ts_state);\r\npadlock_output_block((uint32_t *)result, (uint32_t *)out, 5);\r\nout:\r\nreturn err;\r\n}\r\nstatic int padlock_sha1_final(struct shash_desc *desc, u8 *out)\r\n{\r\nu8 buf[4];\r\nreturn padlock_sha1_finup(desc, buf, 0, out);\r\n}\r\nstatic int padlock_sha256_finup(struct shash_desc *desc, const u8 *in,\r\nunsigned int count, u8 *out)\r\n{\r\nchar buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__\r\n((aligned(STACK_ALIGN)));\r\nchar *result = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\r\nstruct padlock_sha_desc *dctx = shash_desc_ctx(desc);\r\nstruct sha256_state state;\r\nunsigned int space;\r\nunsigned int leftover;\r\nint ts_state;\r\nint err;\r\ndctx->fallback.flags = desc->flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nerr = crypto_shash_export(&dctx->fallback, &state);\r\nif (err)\r\ngoto out;\r\nif (state.count + count > ULONG_MAX)\r\nreturn crypto_shash_finup(&dctx->fallback, in, count, out);\r\nleftover = ((state.count - 1) & (SHA256_BLOCK_SIZE - 1)) + 1;\r\nspace = SHA256_BLOCK_SIZE - leftover;\r\nif (space) {\r\nif (count > space) {\r\nerr = crypto_shash_update(&dctx->fallback, in, space) ?:\r\ncrypto_shash_export(&dctx->fallback, &state);\r\nif (err)\r\ngoto out;\r\ncount -= space;\r\nin += space;\r\n} else {\r\nmemcpy(state.buf + leftover, in, count);\r\nin = state.buf;\r\ncount += leftover;\r\nstate.count &= ~(SHA1_BLOCK_SIZE - 1);\r\n}\r\n}\r\nmemcpy(result, &state.state, SHA256_DIGEST_SIZE);\r\nts_state = irq_ts_save();\r\nasm volatile (".byte 0xf3,0x0f,0xa6,0xd0"\r\n: \\r\n: "c"((unsigned long)state.count + count), \\r\n"a"((unsigned long)state.count), \\r\n"S"(in), "D"(result));\r\nirq_ts_restore(ts_state);\r\npadlock_output_block((uint32_t *)result, (uint32_t *)out, 8);\r\nout:\r\nreturn err;\r\n}\r\nstatic int padlock_sha256_final(struct shash_desc *desc, u8 *out)\r\n{\r\nu8 buf[4];\r\nreturn padlock_sha256_finup(desc, buf, 0, out);\r\n}\r\nstatic int padlock_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_shash *hash = __crypto_shash_cast(tfm);\r\nconst char *fallback_driver_name = crypto_tfm_alg_name(tfm);\r\nstruct padlock_sha_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct crypto_shash *fallback_tfm;\r\nint err = -ENOMEM;\r\nfallback_tfm = crypto_alloc_shash(fallback_driver_name, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(fallback_tfm)) {\r\nprintk(KERN_WARNING PFX "Fallback driver '%s' could not be loaded!\n",\r\nfallback_driver_name);\r\nerr = PTR_ERR(fallback_tfm);\r\ngoto out;\r\n}\r\nctx->fallback = fallback_tfm;\r\nhash->descsize += crypto_shash_descsize(fallback_tfm);\r\nreturn 0;\r\nout:\r\nreturn err;\r\n}\r\nstatic void padlock_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct padlock_sha_ctx *ctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_shash(ctx->fallback);\r\n}\r\nstatic int padlock_sha1_init_nano(struct shash_desc *desc)\r\n{\r\nstruct sha1_state *sctx = shash_desc_ctx(desc);\r\n*sctx = (struct sha1_state){\r\n.state = { SHA1_H0, SHA1_H1, SHA1_H2, SHA1_H3, SHA1_H4 },\r\n};\r\nreturn 0;\r\n}\r\nstatic int padlock_sha1_update_nano(struct shash_desc *desc,\r\nconst u8 *data, unsigned int len)\r\n{\r\nstruct sha1_state *sctx = shash_desc_ctx(desc);\r\nunsigned int partial, done;\r\nconst u8 *src;\r\nu8 buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__\r\n((aligned(STACK_ALIGN)));\r\nu8 *dst = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\r\nint ts_state;\r\npartial = sctx->count & 0x3f;\r\nsctx->count += len;\r\ndone = 0;\r\nsrc = data;\r\nmemcpy(dst, (u8 *)(sctx->state), SHA1_DIGEST_SIZE);\r\nif ((partial + len) >= SHA1_BLOCK_SIZE) {\r\nif (partial) {\r\ndone = -partial;\r\nmemcpy(sctx->buffer + partial, data,\r\ndone + SHA1_BLOCK_SIZE);\r\nsrc = sctx->buffer;\r\nts_state = irq_ts_save();\r\nasm volatile (".byte 0xf3,0x0f,0xa6,0xc8"\r\n: "+S"(src), "+D"(dst) \\r\n: "a"((long)-1), "c"((unsigned long)1));\r\nirq_ts_restore(ts_state);\r\ndone += SHA1_BLOCK_SIZE;\r\nsrc = data + done;\r\n}\r\nif (len - done >= SHA1_BLOCK_SIZE) {\r\nts_state = irq_ts_save();\r\nasm volatile (".byte 0xf3,0x0f,0xa6,0xc8"\r\n: "+S"(src), "+D"(dst)\r\n: "a"((long)-1),\r\n"c"((unsigned long)((len - done) / SHA1_BLOCK_SIZE)));\r\nirq_ts_restore(ts_state);\r\ndone += ((len - done) - (len - done) % SHA1_BLOCK_SIZE);\r\nsrc = data + done;\r\n}\r\npartial = 0;\r\n}\r\nmemcpy((u8 *)(sctx->state), dst, SHA1_DIGEST_SIZE);\r\nmemcpy(sctx->buffer + partial, src, len - done);\r\nreturn 0;\r\n}\r\nstatic int padlock_sha1_final_nano(struct shash_desc *desc, u8 *out)\r\n{\r\nstruct sha1_state *state = (struct sha1_state *)shash_desc_ctx(desc);\r\nunsigned int partial, padlen;\r\n__be64 bits;\r\nstatic const u8 padding[64] = { 0x80, };\r\nbits = cpu_to_be64(state->count << 3);\r\npartial = state->count & 0x3f;\r\npadlen = (partial < 56) ? (56 - partial) : ((64+56) - partial);\r\npadlock_sha1_update_nano(desc, padding, padlen);\r\npadlock_sha1_update_nano(desc, (const u8 *)&bits, sizeof(bits));\r\npadlock_output_block((uint32_t *)(state->state), (uint32_t *)out, 5);\r\nreturn 0;\r\n}\r\nstatic int padlock_sha256_init_nano(struct shash_desc *desc)\r\n{\r\nstruct sha256_state *sctx = shash_desc_ctx(desc);\r\n*sctx = (struct sha256_state){\r\n.state = { SHA256_H0, SHA256_H1, SHA256_H2, SHA256_H3, \\r\nSHA256_H4, SHA256_H5, SHA256_H6, SHA256_H7},\r\n};\r\nreturn 0;\r\n}\r\nstatic int padlock_sha256_update_nano(struct shash_desc *desc, const u8 *data,\r\nunsigned int len)\r\n{\r\nstruct sha256_state *sctx = shash_desc_ctx(desc);\r\nunsigned int partial, done;\r\nconst u8 *src;\r\nu8 buf[128 + PADLOCK_ALIGNMENT - STACK_ALIGN] __attribute__\r\n((aligned(STACK_ALIGN)));\r\nu8 *dst = PTR_ALIGN(&buf[0], PADLOCK_ALIGNMENT);\r\nint ts_state;\r\npartial = sctx->count & 0x3f;\r\nsctx->count += len;\r\ndone = 0;\r\nsrc = data;\r\nmemcpy(dst, (u8 *)(sctx->state), SHA256_DIGEST_SIZE);\r\nif ((partial + len) >= SHA256_BLOCK_SIZE) {\r\nif (partial) {\r\ndone = -partial;\r\nmemcpy(sctx->buf + partial, data,\r\ndone + SHA256_BLOCK_SIZE);\r\nsrc = sctx->buf;\r\nts_state = irq_ts_save();\r\nasm volatile (".byte 0xf3,0x0f,0xa6,0xd0"\r\n: "+S"(src), "+D"(dst)\r\n: "a"((long)-1), "c"((unsigned long)1));\r\nirq_ts_restore(ts_state);\r\ndone += SHA256_BLOCK_SIZE;\r\nsrc = data + done;\r\n}\r\nif (len - done >= SHA256_BLOCK_SIZE) {\r\nts_state = irq_ts_save();\r\nasm volatile (".byte 0xf3,0x0f,0xa6,0xd0"\r\n: "+S"(src), "+D"(dst)\r\n: "a"((long)-1),\r\n"c"((unsigned long)((len - done) / 64)));\r\nirq_ts_restore(ts_state);\r\ndone += ((len - done) - (len - done) % 64);\r\nsrc = data + done;\r\n}\r\npartial = 0;\r\n}\r\nmemcpy((u8 *)(sctx->state), dst, SHA256_DIGEST_SIZE);\r\nmemcpy(sctx->buf + partial, src, len - done);\r\nreturn 0;\r\n}\r\nstatic int padlock_sha256_final_nano(struct shash_desc *desc, u8 *out)\r\n{\r\nstruct sha256_state *state =\r\n(struct sha256_state *)shash_desc_ctx(desc);\r\nunsigned int partial, padlen;\r\n__be64 bits;\r\nstatic const u8 padding[64] = { 0x80, };\r\nbits = cpu_to_be64(state->count << 3);\r\npartial = state->count & 0x3f;\r\npadlen = (partial < 56) ? (56 - partial) : ((64+56) - partial);\r\npadlock_sha256_update_nano(desc, padding, padlen);\r\npadlock_sha256_update_nano(desc, (const u8 *)&bits, sizeof(bits));\r\npadlock_output_block((uint32_t *)(state->state), (uint32_t *)out, 8);\r\nreturn 0;\r\n}\r\nstatic int padlock_sha_export_nano(struct shash_desc *desc,\r\nvoid *out)\r\n{\r\nint statesize = crypto_shash_statesize(desc->tfm);\r\nvoid *sctx = shash_desc_ctx(desc);\r\nmemcpy(out, sctx, statesize);\r\nreturn 0;\r\n}\r\nstatic int padlock_sha_import_nano(struct shash_desc *desc,\r\nconst void *in)\r\n{\r\nint statesize = crypto_shash_statesize(desc->tfm);\r\nvoid *sctx = shash_desc_ctx(desc);\r\nmemcpy(sctx, in, statesize);\r\nreturn 0;\r\n}\r\nstatic int __init padlock_init(void)\r\n{\r\nint rc = -ENODEV;\r\nstruct cpuinfo_x86 *c = &cpu_data(0);\r\nstruct shash_alg *sha1;\r\nstruct shash_alg *sha256;\r\nif (!x86_match_cpu(padlock_sha_ids) || !cpu_has_phe_enabled)\r\nreturn -ENODEV;\r\nif (c->x86_model < 0x0f) {\r\nsha1 = &sha1_alg;\r\nsha256 = &sha256_alg;\r\n} else {\r\nsha1 = &sha1_alg_nano;\r\nsha256 = &sha256_alg_nano;\r\n}\r\nrc = crypto_register_shash(sha1);\r\nif (rc)\r\ngoto out;\r\nrc = crypto_register_shash(sha256);\r\nif (rc)\r\ngoto out_unreg1;\r\nprintk(KERN_NOTICE PFX "Using VIA PadLock ACE for SHA1/SHA256 algorithms.\n");\r\nreturn 0;\r\nout_unreg1:\r\ncrypto_unregister_shash(sha1);\r\nout:\r\nprintk(KERN_ERR PFX "VIA PadLock SHA1/SHA256 initialization failed.\n");\r\nreturn rc;\r\n}\r\nstatic void __exit padlock_fini(void)\r\n{\r\nstruct cpuinfo_x86 *c = &cpu_data(0);\r\nif (c->x86_model >= 0x0f) {\r\ncrypto_unregister_shash(&sha1_alg_nano);\r\ncrypto_unregister_shash(&sha256_alg_nano);\r\n} else {\r\ncrypto_unregister_shash(&sha1_alg);\r\ncrypto_unregister_shash(&sha256_alg);\r\n}\r\n}
