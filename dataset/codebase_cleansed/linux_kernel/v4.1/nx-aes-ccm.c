static int ccm_aes_nx_set_key(struct crypto_aead *tfm,\r\nconst u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(&tfm->base);\r\nstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\r\nstruct nx_csbcpb *csbcpb_aead = nx_ctx->csbcpb_aead;\r\nnx_ctx_init(nx_ctx, HCOP_FC_AES);\r\nswitch (key_len) {\r\ncase AES_KEYSIZE_128:\r\nNX_CPB_SET_KEY_SIZE(csbcpb, NX_KS_AES_128);\r\nNX_CPB_SET_KEY_SIZE(csbcpb_aead, NX_KS_AES_128);\r\nnx_ctx->ap = &nx_ctx->props[NX_PROPS_AES_128];\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ncsbcpb->cpb.hdr.mode = NX_MODE_AES_CCM;\r\nmemcpy(csbcpb->cpb.aes_ccm.key, in_key, key_len);\r\ncsbcpb_aead->cpb.hdr.mode = NX_MODE_AES_CCA;\r\nmemcpy(csbcpb_aead->cpb.aes_cca.key, in_key, key_len);\r\nreturn 0;\r\n}\r\nstatic int ccm4309_aes_nx_set_key(struct crypto_aead *tfm,\r\nconst u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(&tfm->base);\r\nif (key_len < 3)\r\nreturn -EINVAL;\r\nkey_len -= 3;\r\nmemcpy(nx_ctx->priv.ccm.nonce, in_key + key_len, 3);\r\nreturn ccm_aes_nx_set_key(tfm, in_key, key_len);\r\n}\r\nstatic int ccm_aes_nx_setauthsize(struct crypto_aead *tfm,\r\nunsigned int authsize)\r\n{\r\nswitch (authsize) {\r\ncase 4:\r\ncase 6:\r\ncase 8:\r\ncase 10:\r\ncase 12:\r\ncase 14:\r\ncase 16:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ncrypto_aead_crt(tfm)->authsize = authsize;\r\nreturn 0;\r\n}\r\nstatic int ccm4309_aes_nx_setauthsize(struct crypto_aead *tfm,\r\nunsigned int authsize)\r\n{\r\nswitch (authsize) {\r\ncase 8:\r\ncase 12:\r\ncase 16:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\ncrypto_aead_crt(tfm)->authsize = authsize;\r\nreturn 0;\r\n}\r\nstatic int set_msg_len(u8 *block, unsigned int msglen, int csize)\r\n{\r\n__be32 data;\r\nmemset(block, 0, csize);\r\nblock += csize;\r\nif (csize >= 4)\r\ncsize = 4;\r\nelse if (msglen > (unsigned int)(1 << (8 * csize)))\r\nreturn -EOVERFLOW;\r\ndata = cpu_to_be32(msglen);\r\nmemcpy(block - csize, (u8 *)&data + 4 - csize, csize);\r\nreturn 0;\r\n}\r\nstatic inline int crypto_ccm_check_iv(const u8 *iv)\r\n{\r\nif (1 > iv[0] || iv[0] > 7)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int generate_b0(u8 *iv, unsigned int assoclen, unsigned int authsize,\r\nunsigned int cryptlen, u8 *b0)\r\n{\r\nunsigned int l, lp, m = authsize;\r\nint rc;\r\nmemcpy(b0, iv, 16);\r\nlp = b0[0];\r\nl = lp + 1;\r\n*b0 |= (8 * ((m - 2) / 2));\r\nif (assoclen)\r\n*b0 |= 64;\r\nrc = set_msg_len(b0 + 16 - l, cryptlen, l);\r\nreturn rc;\r\n}\r\nstatic int generate_pat(u8 *iv,\r\nstruct aead_request *req,\r\nstruct nx_crypto_ctx *nx_ctx,\r\nunsigned int authsize,\r\nunsigned int nbytes,\r\nu8 *out)\r\n{\r\nstruct nx_sg *nx_insg = nx_ctx->in_sg;\r\nstruct nx_sg *nx_outsg = nx_ctx->out_sg;\r\nunsigned int iauth_len = 0;\r\nu8 tmp[16], *b1 = NULL, *b0 = NULL, *result = NULL;\r\nint rc;\r\nunsigned int max_sg_len;\r\nmemset(iv + 15 - iv[0], 0, iv[0] + 1);\r\nif (!req->assoclen) {\r\nb0 = nx_ctx->csbcpb->cpb.aes_ccm.in_pat_or_b0;\r\n} else if (req->assoclen <= 14) {\r\nb0 = nx_ctx->csbcpb->cpb.aes_ccm.in_pat_or_b0;\r\nb1 = nx_ctx->priv.ccm.iauth_tag;\r\niauth_len = req->assoclen;\r\n} else if (req->assoclen <= 65280) {\r\nb0 = nx_ctx->csbcpb_aead->cpb.aes_cca.b0;\r\nb1 = nx_ctx->csbcpb_aead->cpb.aes_cca.b1;\r\niauth_len = 14;\r\n} else {\r\nb0 = nx_ctx->csbcpb_aead->cpb.aes_cca.b0;\r\nb1 = nx_ctx->csbcpb_aead->cpb.aes_cca.b1;\r\niauth_len = 10;\r\n}\r\nrc = generate_b0(iv, req->assoclen, authsize, nbytes, b0);\r\nif (rc)\r\nreturn rc;\r\nif (b1) {\r\nmemset(b1, 0, 16);\r\nif (req->assoclen <= 65280) {\r\n*(u16 *)b1 = (u16)req->assoclen;\r\nscatterwalk_map_and_copy(b1 + 2, req->assoc, 0,\r\niauth_len, SCATTERWALK_FROM_SG);\r\n} else {\r\n*(u16 *)b1 = (u16)(0xfffe);\r\n*(u32 *)&b1[2] = (u32)req->assoclen;\r\nscatterwalk_map_and_copy(b1 + 6, req->assoc, 0,\r\niauth_len, SCATTERWALK_FROM_SG);\r\n}\r\n}\r\nif (!req->assoclen) {\r\nreturn rc;\r\n} else if (req->assoclen <= 14) {\r\nunsigned int len = 16;\r\nnx_insg = nx_build_sg_list(nx_insg, b1, &len, nx_ctx->ap->sglen);\r\nif (len != 16)\r\nreturn -EINVAL;\r\nnx_outsg = nx_build_sg_list(nx_outsg, tmp, &len,\r\nnx_ctx->ap->sglen);\r\nif (len != 16)\r\nreturn -EINVAL;\r\nnx_ctx->op.inlen = (nx_ctx->in_sg - nx_insg) *\r\nsizeof(struct nx_sg);\r\nnx_ctx->op.outlen = (nx_ctx->out_sg - nx_outsg) *\r\nsizeof(struct nx_sg);\r\nNX_CPB_FDM(nx_ctx->csbcpb) |= NX_FDM_ENDE_ENCRYPT;\r\nNX_CPB_FDM(nx_ctx->csbcpb) |= NX_FDM_INTERMEDIATE;\r\nresult = nx_ctx->csbcpb->cpb.aes_ccm.out_pat_or_mac;\r\nrc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\r\nif (rc)\r\nreturn rc;\r\natomic_inc(&(nx_ctx->stats->aes_ops));\r\natomic64_add(req->assoclen, &(nx_ctx->stats->aes_bytes));\r\n} else {\r\nunsigned int processed = 0, to_process;\r\nprocessed += iauth_len;\r\nmax_sg_len = min_t(u64, nx_ctx->ap->sglen,\r\nnx_driver.of.max_sg_len/sizeof(struct nx_sg));\r\nmax_sg_len = min_t(u64, max_sg_len,\r\nnx_ctx->ap->databytelen/NX_PAGE_SIZE);\r\ndo {\r\nto_process = min_t(u32, req->assoclen - processed,\r\nnx_ctx->ap->databytelen);\r\nnx_insg = nx_walk_and_build(nx_ctx->in_sg,\r\nnx_ctx->ap->sglen,\r\nreq->assoc, processed,\r\n&to_process);\r\nif ((to_process + processed) < req->assoclen) {\r\nNX_CPB_FDM(nx_ctx->csbcpb_aead) |=\r\nNX_FDM_INTERMEDIATE;\r\n} else {\r\nNX_CPB_FDM(nx_ctx->csbcpb_aead) &=\r\n~NX_FDM_INTERMEDIATE;\r\n}\r\nnx_ctx->op_aead.inlen = (nx_ctx->in_sg - nx_insg) *\r\nsizeof(struct nx_sg);\r\nresult = nx_ctx->csbcpb_aead->cpb.aes_cca.out_pat_or_b0;\r\nrc = nx_hcall_sync(nx_ctx, &nx_ctx->op_aead,\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\r\nif (rc)\r\nreturn rc;\r\nmemcpy(nx_ctx->csbcpb_aead->cpb.aes_cca.b0,\r\nnx_ctx->csbcpb_aead->cpb.aes_cca.out_pat_or_b0,\r\nAES_BLOCK_SIZE);\r\nNX_CPB_FDM(nx_ctx->csbcpb_aead) |= NX_FDM_CONTINUATION;\r\natomic_inc(&(nx_ctx->stats->aes_ops));\r\natomic64_add(req->assoclen,\r\n&(nx_ctx->stats->aes_bytes));\r\nprocessed += to_process;\r\n} while (processed < req->assoclen);\r\nresult = nx_ctx->csbcpb_aead->cpb.aes_cca.out_pat_or_b0;\r\n}\r\nmemcpy(out, result, AES_BLOCK_SIZE);\r\nreturn rc;\r\n}\r\nstatic int ccm_nx_decrypt(struct aead_request *req,\r\nstruct blkcipher_desc *desc)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\r\nunsigned int nbytes = req->cryptlen;\r\nunsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));\r\nstruct nx_ccm_priv *priv = &nx_ctx->priv.ccm;\r\nunsigned long irq_flags;\r\nunsigned int processed = 0, to_process;\r\nint rc = -1;\r\nspin_lock_irqsave(&nx_ctx->lock, irq_flags);\r\nnbytes -= authsize;\r\nscatterwalk_map_and_copy(priv->oauth_tag,\r\nreq->src, nbytes, authsize,\r\nSCATTERWALK_FROM_SG);\r\nrc = generate_pat(desc->info, req, nx_ctx, authsize, nbytes,\r\ncsbcpb->cpb.aes_ccm.in_pat_or_b0);\r\nif (rc)\r\ngoto out;\r\ndo {\r\nto_process = nbytes - processed;\r\nif ((to_process + processed) < nbytes)\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;\r\nelse\r\nNX_CPB_FDM(csbcpb) &= ~NX_FDM_INTERMEDIATE;\r\nNX_CPB_FDM(nx_ctx->csbcpb) &= ~NX_FDM_ENDE_ENCRYPT;\r\nrc = nx_build_sg_lists(nx_ctx, desc, req->dst, req->src,\r\n&to_process, processed,\r\ncsbcpb->cpb.aes_ccm.iv_or_ctr);\r\nif (rc)\r\ngoto out;\r\nrc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\r\nif (rc)\r\ngoto out;\r\nmemcpy(desc->info, csbcpb->cpb.aes_ccm.out_ctr, AES_BLOCK_SIZE);\r\nmemcpy(csbcpb->cpb.aes_ccm.in_pat_or_b0,\r\ncsbcpb->cpb.aes_ccm.out_pat_or_mac, AES_BLOCK_SIZE);\r\nmemcpy(csbcpb->cpb.aes_ccm.in_s0,\r\ncsbcpb->cpb.aes_ccm.out_s0, AES_BLOCK_SIZE);\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;\r\natomic_inc(&(nx_ctx->stats->aes_ops));\r\natomic64_add(csbcpb->csb.processed_byte_count,\r\n&(nx_ctx->stats->aes_bytes));\r\nprocessed += to_process;\r\n} while (processed < nbytes);\r\nrc = memcmp(csbcpb->cpb.aes_ccm.out_pat_or_mac, priv->oauth_tag,\r\nauthsize) ? -EBADMSG : 0;\r\nout:\r\nspin_unlock_irqrestore(&nx_ctx->lock, irq_flags);\r\nreturn rc;\r\n}\r\nstatic int ccm_nx_encrypt(struct aead_request *req,\r\nstruct blkcipher_desc *desc)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct nx_csbcpb *csbcpb = nx_ctx->csbcpb;\r\nunsigned int nbytes = req->cryptlen;\r\nunsigned int authsize = crypto_aead_authsize(crypto_aead_reqtfm(req));\r\nunsigned long irq_flags;\r\nunsigned int processed = 0, to_process;\r\nint rc = -1;\r\nspin_lock_irqsave(&nx_ctx->lock, irq_flags);\r\nrc = generate_pat(desc->info, req, nx_ctx, authsize, nbytes,\r\ncsbcpb->cpb.aes_ccm.in_pat_or_b0);\r\nif (rc)\r\ngoto out;\r\ndo {\r\nto_process = nbytes - processed;\r\nif ((to_process + processed) < nbytes)\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_INTERMEDIATE;\r\nelse\r\nNX_CPB_FDM(csbcpb) &= ~NX_FDM_INTERMEDIATE;\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_ENDE_ENCRYPT;\r\nrc = nx_build_sg_lists(nx_ctx, desc, req->dst, req->src,\r\n&to_process, processed,\r\ncsbcpb->cpb.aes_ccm.iv_or_ctr);\r\nif (rc)\r\ngoto out;\r\nrc = nx_hcall_sync(nx_ctx, &nx_ctx->op,\r\nreq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP);\r\nif (rc)\r\ngoto out;\r\nmemcpy(desc->info, csbcpb->cpb.aes_ccm.out_ctr, AES_BLOCK_SIZE);\r\nmemcpy(csbcpb->cpb.aes_ccm.in_pat_or_b0,\r\ncsbcpb->cpb.aes_ccm.out_pat_or_mac, AES_BLOCK_SIZE);\r\nmemcpy(csbcpb->cpb.aes_ccm.in_s0,\r\ncsbcpb->cpb.aes_ccm.out_s0, AES_BLOCK_SIZE);\r\nNX_CPB_FDM(csbcpb) |= NX_FDM_CONTINUATION;\r\natomic_inc(&(nx_ctx->stats->aes_ops));\r\natomic64_add(csbcpb->csb.processed_byte_count,\r\n&(nx_ctx->stats->aes_bytes));\r\nprocessed += to_process;\r\n} while (processed < nbytes);\r\nscatterwalk_map_and_copy(csbcpb->cpb.aes_ccm.out_pat_or_mac,\r\nreq->dst, nbytes, authsize,\r\nSCATTERWALK_TO_SG);\r\nout:\r\nspin_unlock_irqrestore(&nx_ctx->lock, irq_flags);\r\nreturn rc;\r\n}\r\nstatic int ccm4309_aes_nx_encrypt(struct aead_request *req)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct blkcipher_desc desc;\r\nu8 *iv = nx_ctx->priv.ccm.iv;\r\niv[0] = 3;\r\nmemcpy(iv + 1, nx_ctx->priv.ccm.nonce, 3);\r\nmemcpy(iv + 4, req->iv, 8);\r\ndesc.info = iv;\r\ndesc.tfm = (struct crypto_blkcipher *)req->base.tfm;\r\nreturn ccm_nx_encrypt(req, &desc);\r\n}\r\nstatic int ccm_aes_nx_encrypt(struct aead_request *req)\r\n{\r\nstruct blkcipher_desc desc;\r\nint rc;\r\ndesc.info = req->iv;\r\ndesc.tfm = (struct crypto_blkcipher *)req->base.tfm;\r\nrc = crypto_ccm_check_iv(desc.info);\r\nif (rc)\r\nreturn rc;\r\nreturn ccm_nx_encrypt(req, &desc);\r\n}\r\nstatic int ccm4309_aes_nx_decrypt(struct aead_request *req)\r\n{\r\nstruct nx_crypto_ctx *nx_ctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct blkcipher_desc desc;\r\nu8 *iv = nx_ctx->priv.ccm.iv;\r\niv[0] = 3;\r\nmemcpy(iv + 1, nx_ctx->priv.ccm.nonce, 3);\r\nmemcpy(iv + 4, req->iv, 8);\r\ndesc.info = iv;\r\ndesc.tfm = (struct crypto_blkcipher *)req->base.tfm;\r\nreturn ccm_nx_decrypt(req, &desc);\r\n}\r\nstatic int ccm_aes_nx_decrypt(struct aead_request *req)\r\n{\r\nstruct blkcipher_desc desc;\r\nint rc;\r\ndesc.info = req->iv;\r\ndesc.tfm = (struct crypto_blkcipher *)req->base.tfm;\r\nrc = crypto_ccm_check_iv(desc.info);\r\nif (rc)\r\nreturn rc;\r\nreturn ccm_nx_decrypt(req, &desc);\r\n}
