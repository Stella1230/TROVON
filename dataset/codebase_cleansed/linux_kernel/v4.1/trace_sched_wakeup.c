static int\r\nfunc_prolog_preempt_disable(struct trace_array *tr,\r\nstruct trace_array_cpu **data,\r\nint *pc)\r\n{\r\nlong disabled;\r\nint cpu;\r\nif (likely(!wakeup_task))\r\nreturn 0;\r\n*pc = preempt_count();\r\npreempt_disable_notrace();\r\ncpu = raw_smp_processor_id();\r\nif (cpu != wakeup_current_cpu)\r\ngoto out_enable;\r\n*data = per_cpu_ptr(tr->trace_buffer.data, cpu);\r\ndisabled = atomic_inc_return(&(*data)->disabled);\r\nif (unlikely(disabled != 1))\r\ngoto out;\r\nreturn 1;\r\nout:\r\natomic_dec(&(*data)->disabled);\r\nout_enable:\r\npreempt_enable_notrace();\r\nreturn 0;\r\n}\r\nstatic void\r\nwakeup_tracer_call(unsigned long ip, unsigned long parent_ip,\r\nstruct ftrace_ops *op, struct pt_regs *pt_regs)\r\n{\r\nstruct trace_array *tr = wakeup_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint pc;\r\nif (!func_prolog_preempt_disable(tr, &data, &pc))\r\nreturn;\r\nlocal_irq_save(flags);\r\ntrace_function(tr, ip, parent_ip, flags, pc);\r\nlocal_irq_restore(flags);\r\natomic_dec(&data->disabled);\r\npreempt_enable_notrace();\r\n}\r\nstatic int register_wakeup_function(struct trace_array *tr, int graph, int set)\r\n{\r\nint ret;\r\nif (function_enabled || (!set && !(trace_flags & TRACE_ITER_FUNCTION)))\r\nreturn 0;\r\nif (graph)\r\nret = register_ftrace_graph(&wakeup_graph_return,\r\n&wakeup_graph_entry);\r\nelse\r\nret = register_ftrace_function(tr->ops);\r\nif (!ret)\r\nfunction_enabled = true;\r\nreturn ret;\r\n}\r\nstatic void unregister_wakeup_function(struct trace_array *tr, int graph)\r\n{\r\nif (!function_enabled)\r\nreturn;\r\nif (graph)\r\nunregister_ftrace_graph();\r\nelse\r\nunregister_ftrace_function(tr->ops);\r\nfunction_enabled = false;\r\n}\r\nstatic void wakeup_function_set(struct trace_array *tr, int set)\r\n{\r\nif (set)\r\nregister_wakeup_function(tr, is_graph(), 1);\r\nelse\r\nunregister_wakeup_function(tr, is_graph());\r\n}\r\nstatic int wakeup_flag_changed(struct trace_array *tr, u32 mask, int set)\r\n{\r\nstruct tracer *tracer = tr->current_trace;\r\nif (mask & TRACE_ITER_FUNCTION)\r\nwakeup_function_set(tr, set);\r\nreturn trace_keep_overwrite(tracer, mask, set);\r\n}\r\nstatic int start_func_tracer(struct trace_array *tr, int graph)\r\n{\r\nint ret;\r\nret = register_wakeup_function(tr, graph, 0);\r\nif (!ret && tracing_is_enabled())\r\ntracer_enabled = 1;\r\nelse\r\ntracer_enabled = 0;\r\nreturn ret;\r\n}\r\nstatic void stop_func_tracer(struct trace_array *tr, int graph)\r\n{\r\ntracer_enabled = 0;\r\nunregister_wakeup_function(tr, graph);\r\n}\r\nstatic int\r\nwakeup_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\r\n{\r\nif (!(bit & TRACE_DISPLAY_GRAPH))\r\nreturn -EINVAL;\r\nif (!(is_graph() ^ set))\r\nreturn 0;\r\nstop_func_tracer(tr, !set);\r\nwakeup_reset(wakeup_trace);\r\ntr->max_latency = 0;\r\nreturn start_func_tracer(tr, set);\r\n}\r\nstatic int wakeup_graph_entry(struct ftrace_graph_ent *trace)\r\n{\r\nstruct trace_array *tr = wakeup_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint pc, ret = 0;\r\nif (!func_prolog_preempt_disable(tr, &data, &pc))\r\nreturn 0;\r\nlocal_save_flags(flags);\r\nret = __trace_graph_entry(tr, trace, flags, pc);\r\natomic_dec(&data->disabled);\r\npreempt_enable_notrace();\r\nreturn ret;\r\n}\r\nstatic void wakeup_graph_return(struct ftrace_graph_ret *trace)\r\n{\r\nstruct trace_array *tr = wakeup_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint pc;\r\nif (!func_prolog_preempt_disable(tr, &data, &pc))\r\nreturn;\r\nlocal_save_flags(flags);\r\n__trace_graph_return(tr, trace, flags, pc);\r\natomic_dec(&data->disabled);\r\npreempt_enable_notrace();\r\nreturn;\r\n}\r\nstatic void wakeup_trace_open(struct trace_iterator *iter)\r\n{\r\nif (is_graph())\r\ngraph_trace_open(iter);\r\n}\r\nstatic void wakeup_trace_close(struct trace_iterator *iter)\r\n{\r\nif (iter->private)\r\ngraph_trace_close(iter);\r\n}\r\nstatic enum print_line_t wakeup_print_line(struct trace_iterator *iter)\r\n{\r\nif (is_graph())\r\nreturn print_graph_function_flags(iter, GRAPH_TRACER_FLAGS);\r\nreturn TRACE_TYPE_UNHANDLED;\r\n}\r\nstatic void wakeup_print_header(struct seq_file *s)\r\n{\r\nif (is_graph())\r\nprint_graph_headers_flags(s, GRAPH_TRACER_FLAGS);\r\nelse\r\ntrace_default_header(s);\r\n}\r\nstatic void\r\n__trace_function(struct trace_array *tr,\r\nunsigned long ip, unsigned long parent_ip,\r\nunsigned long flags, int pc)\r\n{\r\nif (is_graph())\r\ntrace_graph_function(tr, ip, parent_ip, flags, pc);\r\nelse\r\ntrace_function(tr, ip, parent_ip, flags, pc);\r\n}\r\nstatic int\r\nwakeup_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic int wakeup_graph_entry(struct ftrace_graph_ent *trace)\r\n{\r\nreturn -1;\r\n}\r\nstatic enum print_line_t wakeup_print_line(struct trace_iterator *iter)\r\n{\r\nreturn TRACE_TYPE_UNHANDLED;\r\n}\r\nstatic void wakeup_graph_return(struct ftrace_graph_ret *trace) { }\r\nstatic void wakeup_trace_open(struct trace_iterator *iter) { }\r\nstatic void wakeup_trace_close(struct trace_iterator *iter) { }\r\nstatic void wakeup_print_header(struct seq_file *s)\r\n{\r\ntrace_default_header(s);\r\n}\r\nstatic void wakeup_print_header(struct seq_file *s)\r\n{\r\ntrace_latency_header(s);\r\n}\r\nstatic int report_latency(struct trace_array *tr, cycle_t delta)\r\n{\r\nif (tracing_thresh) {\r\nif (delta < tracing_thresh)\r\nreturn 0;\r\n} else {\r\nif (delta <= tr->max_latency)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void\r\nprobe_wakeup_migrate_task(void *ignore, struct task_struct *task, int cpu)\r\n{\r\nif (task != wakeup_task)\r\nreturn;\r\nwakeup_current_cpu = cpu;\r\n}\r\nstatic void\r\ntracing_sched_switch_trace(struct trace_array *tr,\r\nstruct task_struct *prev,\r\nstruct task_struct *next,\r\nunsigned long flags, int pc)\r\n{\r\nstruct ftrace_event_call *call = &event_context_switch;\r\nstruct ring_buffer *buffer = tr->trace_buffer.buffer;\r\nstruct ring_buffer_event *event;\r\nstruct ctx_switch_entry *entry;\r\nevent = trace_buffer_lock_reserve(buffer, TRACE_CTX,\r\nsizeof(*entry), flags, pc);\r\nif (!event)\r\nreturn;\r\nentry = ring_buffer_event_data(event);\r\nentry->prev_pid = prev->pid;\r\nentry->prev_prio = prev->prio;\r\nentry->prev_state = prev->state;\r\nentry->next_pid = next->pid;\r\nentry->next_prio = next->prio;\r\nentry->next_state = next->state;\r\nentry->next_cpu = task_cpu(next);\r\nif (!call_filter_check_discard(call, entry, buffer, event))\r\ntrace_buffer_unlock_commit(buffer, event, flags, pc);\r\n}\r\nstatic void\r\ntracing_sched_wakeup_trace(struct trace_array *tr,\r\nstruct task_struct *wakee,\r\nstruct task_struct *curr,\r\nunsigned long flags, int pc)\r\n{\r\nstruct ftrace_event_call *call = &event_wakeup;\r\nstruct ring_buffer_event *event;\r\nstruct ctx_switch_entry *entry;\r\nstruct ring_buffer *buffer = tr->trace_buffer.buffer;\r\nevent = trace_buffer_lock_reserve(buffer, TRACE_WAKE,\r\nsizeof(*entry), flags, pc);\r\nif (!event)\r\nreturn;\r\nentry = ring_buffer_event_data(event);\r\nentry->prev_pid = curr->pid;\r\nentry->prev_prio = curr->prio;\r\nentry->prev_state = curr->state;\r\nentry->next_pid = wakee->pid;\r\nentry->next_prio = wakee->prio;\r\nentry->next_state = wakee->state;\r\nentry->next_cpu = task_cpu(wakee);\r\nif (!call_filter_check_discard(call, entry, buffer, event))\r\ntrace_buffer_unlock_commit(buffer, event, flags, pc);\r\n}\r\nstatic void notrace\r\nprobe_wakeup_sched_switch(void *ignore,\r\nstruct task_struct *prev, struct task_struct *next)\r\n{\r\nstruct trace_array_cpu *data;\r\ncycle_t T0, T1, delta;\r\nunsigned long flags;\r\nlong disabled;\r\nint cpu;\r\nint pc;\r\ntracing_record_cmdline(prev);\r\nif (unlikely(!tracer_enabled))\r\nreturn;\r\nsmp_rmb();\r\nif (next != wakeup_task)\r\nreturn;\r\npc = preempt_count();\r\ncpu = raw_smp_processor_id();\r\ndisabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->trace_buffer.data, cpu)->disabled);\r\nif (likely(disabled != 1))\r\ngoto out;\r\nlocal_irq_save(flags);\r\narch_spin_lock(&wakeup_lock);\r\nif (unlikely(!tracer_enabled || next != wakeup_task))\r\ngoto out_unlock;\r\ndata = per_cpu_ptr(wakeup_trace->trace_buffer.data, wakeup_cpu);\r\n__trace_function(wakeup_trace, CALLER_ADDR0, CALLER_ADDR1, flags, pc);\r\ntracing_sched_switch_trace(wakeup_trace, prev, next, flags, pc);\r\nT0 = data->preempt_timestamp;\r\nT1 = ftrace_now(cpu);\r\ndelta = T1-T0;\r\nif (!report_latency(wakeup_trace, delta))\r\ngoto out_unlock;\r\nif (likely(!is_tracing_stopped())) {\r\nwakeup_trace->max_latency = delta;\r\nupdate_max_tr(wakeup_trace, wakeup_task, wakeup_cpu);\r\n}\r\nout_unlock:\r\n__wakeup_reset(wakeup_trace);\r\narch_spin_unlock(&wakeup_lock);\r\nlocal_irq_restore(flags);\r\nout:\r\natomic_dec(&per_cpu_ptr(wakeup_trace->trace_buffer.data, cpu)->disabled);\r\n}\r\nstatic void __wakeup_reset(struct trace_array *tr)\r\n{\r\nwakeup_cpu = -1;\r\nwakeup_prio = -1;\r\ntracing_dl = 0;\r\nif (wakeup_task)\r\nput_task_struct(wakeup_task);\r\nwakeup_task = NULL;\r\n}\r\nstatic void wakeup_reset(struct trace_array *tr)\r\n{\r\nunsigned long flags;\r\ntracing_reset_online_cpus(&tr->trace_buffer);\r\nlocal_irq_save(flags);\r\narch_spin_lock(&wakeup_lock);\r\n__wakeup_reset(tr);\r\narch_spin_unlock(&wakeup_lock);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void\r\nprobe_wakeup(void *ignore, struct task_struct *p, int success)\r\n{\r\nstruct trace_array_cpu *data;\r\nint cpu = smp_processor_id();\r\nunsigned long flags;\r\nlong disabled;\r\nint pc;\r\nif (likely(!tracer_enabled))\r\nreturn;\r\ntracing_record_cmdline(p);\r\ntracing_record_cmdline(current);\r\nif (tracing_dl || (wakeup_dl && !dl_task(p)) ||\r\n(wakeup_rt && !dl_task(p) && !rt_task(p)) ||\r\n(!dl_task(p) && (p->prio >= wakeup_prio || p->prio >= current->prio)))\r\nreturn;\r\npc = preempt_count();\r\ndisabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->trace_buffer.data, cpu)->disabled);\r\nif (unlikely(disabled != 1))\r\ngoto out;\r\narch_spin_lock(&wakeup_lock);\r\nif (!tracer_enabled || tracing_dl ||\r\n(!dl_task(p) && p->prio >= wakeup_prio))\r\ngoto out_locked;\r\n__wakeup_reset(wakeup_trace);\r\nwakeup_cpu = task_cpu(p);\r\nwakeup_current_cpu = wakeup_cpu;\r\nwakeup_prio = p->prio;\r\nif (dl_task(p))\r\ntracing_dl = 1;\r\nelse\r\ntracing_dl = 0;\r\nwakeup_task = p;\r\nget_task_struct(wakeup_task);\r\nlocal_save_flags(flags);\r\ndata = per_cpu_ptr(wakeup_trace->trace_buffer.data, wakeup_cpu);\r\ndata->preempt_timestamp = ftrace_now(cpu);\r\ntracing_sched_wakeup_trace(wakeup_trace, p, current, flags, pc);\r\n__trace_function(wakeup_trace, CALLER_ADDR1, CALLER_ADDR2, flags, pc);\r\nout_locked:\r\narch_spin_unlock(&wakeup_lock);\r\nout:\r\natomic_dec(&per_cpu_ptr(wakeup_trace->trace_buffer.data, cpu)->disabled);\r\n}\r\nstatic void start_wakeup_tracer(struct trace_array *tr)\r\n{\r\nint ret;\r\nret = register_trace_sched_wakeup(probe_wakeup, NULL);\r\nif (ret) {\r\npr_info("wakeup trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_wakeup\n");\r\nreturn;\r\n}\r\nret = register_trace_sched_wakeup_new(probe_wakeup, NULL);\r\nif (ret) {\r\npr_info("wakeup trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_wakeup_new\n");\r\ngoto fail_deprobe;\r\n}\r\nret = register_trace_sched_switch(probe_wakeup_sched_switch, NULL);\r\nif (ret) {\r\npr_info("sched trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_switch\n");\r\ngoto fail_deprobe_wake_new;\r\n}\r\nret = register_trace_sched_migrate_task(probe_wakeup_migrate_task, NULL);\r\nif (ret) {\r\npr_info("wakeup trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_migrate_task\n");\r\nreturn;\r\n}\r\nwakeup_reset(tr);\r\nsmp_wmb();\r\nif (start_func_tracer(tr, is_graph()))\r\nprintk(KERN_ERR "failed to start wakeup tracer\n");\r\nreturn;\r\nfail_deprobe_wake_new:\r\nunregister_trace_sched_wakeup_new(probe_wakeup, NULL);\r\nfail_deprobe:\r\nunregister_trace_sched_wakeup(probe_wakeup, NULL);\r\n}\r\nstatic void stop_wakeup_tracer(struct trace_array *tr)\r\n{\r\ntracer_enabled = 0;\r\nstop_func_tracer(tr, is_graph());\r\nunregister_trace_sched_switch(probe_wakeup_sched_switch, NULL);\r\nunregister_trace_sched_wakeup_new(probe_wakeup, NULL);\r\nunregister_trace_sched_wakeup(probe_wakeup, NULL);\r\nunregister_trace_sched_migrate_task(probe_wakeup_migrate_task, NULL);\r\n}\r\nstatic int __wakeup_tracer_init(struct trace_array *tr)\r\n{\r\nsave_flags = trace_flags;\r\nset_tracer_flag(tr, TRACE_ITER_OVERWRITE, 1);\r\nset_tracer_flag(tr, TRACE_ITER_LATENCY_FMT, 1);\r\ntr->max_latency = 0;\r\nwakeup_trace = tr;\r\nftrace_init_array_ops(tr, wakeup_tracer_call);\r\nstart_wakeup_tracer(tr);\r\nwakeup_busy = true;\r\nreturn 0;\r\n}\r\nstatic int wakeup_tracer_init(struct trace_array *tr)\r\n{\r\nif (wakeup_busy)\r\nreturn -EBUSY;\r\nwakeup_dl = 0;\r\nwakeup_rt = 0;\r\nreturn __wakeup_tracer_init(tr);\r\n}\r\nstatic int wakeup_rt_tracer_init(struct trace_array *tr)\r\n{\r\nif (wakeup_busy)\r\nreturn -EBUSY;\r\nwakeup_dl = 0;\r\nwakeup_rt = 1;\r\nreturn __wakeup_tracer_init(tr);\r\n}\r\nstatic int wakeup_dl_tracer_init(struct trace_array *tr)\r\n{\r\nif (wakeup_busy)\r\nreturn -EBUSY;\r\nwakeup_dl = 1;\r\nwakeup_rt = 0;\r\nreturn __wakeup_tracer_init(tr);\r\n}\r\nstatic void wakeup_tracer_reset(struct trace_array *tr)\r\n{\r\nint lat_flag = save_flags & TRACE_ITER_LATENCY_FMT;\r\nint overwrite_flag = save_flags & TRACE_ITER_OVERWRITE;\r\nstop_wakeup_tracer(tr);\r\nwakeup_reset(tr);\r\nset_tracer_flag(tr, TRACE_ITER_LATENCY_FMT, lat_flag);\r\nset_tracer_flag(tr, TRACE_ITER_OVERWRITE, overwrite_flag);\r\nftrace_reset_array_ops(tr);\r\nwakeup_busy = false;\r\n}\r\nstatic void wakeup_tracer_start(struct trace_array *tr)\r\n{\r\nwakeup_reset(tr);\r\ntracer_enabled = 1;\r\n}\r\nstatic void wakeup_tracer_stop(struct trace_array *tr)\r\n{\r\ntracer_enabled = 0;\r\n}\r\n__init static int init_wakeup_tracer(void)\r\n{\r\nint ret;\r\nret = register_tracer(&wakeup_tracer);\r\nif (ret)\r\nreturn ret;\r\nret = register_tracer(&wakeup_rt_tracer);\r\nif (ret)\r\nreturn ret;\r\nret = register_tracer(&wakeup_dl_tracer);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}
