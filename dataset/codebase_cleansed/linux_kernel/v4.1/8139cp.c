static inline void cp_set_rxbufsize (struct cp_private *cp)\r\n{\r\nunsigned int mtu = cp->dev->mtu;\r\nif (mtu > ETH_DATA_LEN)\r\ncp->rx_buf_sz = mtu + ETH_HLEN + 8;\r\nelse\r\ncp->rx_buf_sz = PKT_BUF_SZ;\r\n}\r\nstatic inline void cp_rx_skb (struct cp_private *cp, struct sk_buff *skb,\r\nstruct cp_desc *desc)\r\n{\r\nu32 opts2 = le32_to_cpu(desc->opts2);\r\nskb->protocol = eth_type_trans (skb, cp->dev);\r\ncp->dev->stats.rx_packets++;\r\ncp->dev->stats.rx_bytes += skb->len;\r\nif (opts2 & RxVlanTagged)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), swab16(opts2 & 0xffff));\r\nnapi_gro_receive(&cp->napi, skb);\r\n}\r\nstatic void cp_rx_err_acct (struct cp_private *cp, unsigned rx_tail,\r\nu32 status, u32 len)\r\n{\r\nnetif_dbg(cp, rx_err, cp->dev, "rx err, slot %d status 0x%x len %d\n",\r\nrx_tail, status, len);\r\ncp->dev->stats.rx_errors++;\r\nif (status & RxErrFrame)\r\ncp->dev->stats.rx_frame_errors++;\r\nif (status & RxErrCRC)\r\ncp->dev->stats.rx_crc_errors++;\r\nif ((status & RxErrRunt) || (status & RxErrLong))\r\ncp->dev->stats.rx_length_errors++;\r\nif ((status & (FirstFrag | LastFrag)) != (FirstFrag | LastFrag))\r\ncp->dev->stats.rx_length_errors++;\r\nif (status & RxErrFIFO)\r\ncp->dev->stats.rx_fifo_errors++;\r\n}\r\nstatic inline unsigned int cp_rx_csum_ok (u32 status)\r\n{\r\nunsigned int protocol = (status >> 16) & 0x3;\r\nif (((protocol == RxProtoTCP) && !(status & TCPFail)) ||\r\n((protocol == RxProtoUDP) && !(status & UDPFail)))\r\nreturn 1;\r\nelse\r\nreturn 0;\r\n}\r\nstatic int cp_rx_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct cp_private *cp = container_of(napi, struct cp_private, napi);\r\nstruct net_device *dev = cp->dev;\r\nunsigned int rx_tail = cp->rx_tail;\r\nint rx;\r\nrx_status_loop:\r\nrx = 0;\r\ncpw16(IntrStatus, cp_rx_intr_mask);\r\nwhile (rx < budget) {\r\nu32 status, len;\r\ndma_addr_t mapping, new_mapping;\r\nstruct sk_buff *skb, *new_skb;\r\nstruct cp_desc *desc;\r\nconst unsigned buflen = cp->rx_buf_sz;\r\nskb = cp->rx_skb[rx_tail];\r\nBUG_ON(!skb);\r\ndesc = &cp->rx_ring[rx_tail];\r\nstatus = le32_to_cpu(desc->opts1);\r\nif (status & DescOwn)\r\nbreak;\r\nlen = (status & 0x1fff) - 4;\r\nmapping = le64_to_cpu(desc->addr);\r\nif ((status & (FirstFrag | LastFrag)) != (FirstFrag | LastFrag)) {\r\ncp_rx_err_acct(cp, rx_tail, status, len);\r\ndev->stats.rx_dropped++;\r\ncp->cp_stats.rx_frags++;\r\ngoto rx_next;\r\n}\r\nif (status & (RxError | RxErrFIFO)) {\r\ncp_rx_err_acct(cp, rx_tail, status, len);\r\ngoto rx_next;\r\n}\r\nnetif_dbg(cp, rx_status, dev, "rx slot %d status 0x%x len %d\n",\r\nrx_tail, status, len);\r\nnew_skb = napi_alloc_skb(napi, buflen);\r\nif (!new_skb) {\r\ndev->stats.rx_dropped++;\r\ngoto rx_next;\r\n}\r\nnew_mapping = dma_map_single(&cp->pdev->dev, new_skb->data, buflen,\r\nPCI_DMA_FROMDEVICE);\r\nif (dma_mapping_error(&cp->pdev->dev, new_mapping)) {\r\ndev->stats.rx_dropped++;\r\nkfree_skb(new_skb);\r\ngoto rx_next;\r\n}\r\ndma_unmap_single(&cp->pdev->dev, mapping,\r\nbuflen, PCI_DMA_FROMDEVICE);\r\nif (cp_rx_csum_ok(status))\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nelse\r\nskb_checksum_none_assert(skb);\r\nskb_put(skb, len);\r\ncp->rx_skb[rx_tail] = new_skb;\r\ncp_rx_skb(cp, skb, desc);\r\nrx++;\r\nmapping = new_mapping;\r\nrx_next:\r\ncp->rx_ring[rx_tail].opts2 = 0;\r\ncp->rx_ring[rx_tail].addr = cpu_to_le64(mapping);\r\nif (rx_tail == (CP_RX_RING_SIZE - 1))\r\ndesc->opts1 = cpu_to_le32(DescOwn | RingEnd |\r\ncp->rx_buf_sz);\r\nelse\r\ndesc->opts1 = cpu_to_le32(DescOwn | cp->rx_buf_sz);\r\nrx_tail = NEXT_RX(rx_tail);\r\n}\r\ncp->rx_tail = rx_tail;\r\nif (rx < budget) {\r\nunsigned long flags;\r\nif (cpr16(IntrStatus) & cp_rx_intr_mask)\r\ngoto rx_status_loop;\r\nnapi_gro_flush(napi, false);\r\nspin_lock_irqsave(&cp->lock, flags);\r\n__napi_complete(napi);\r\ncpw16_f(IntrMask, cp_intr_mask);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\n}\r\nreturn rx;\r\n}\r\nstatic irqreturn_t cp_interrupt (int irq, void *dev_instance)\r\n{\r\nstruct net_device *dev = dev_instance;\r\nstruct cp_private *cp;\r\nint handled = 0;\r\nu16 status;\r\nif (unlikely(dev == NULL))\r\nreturn IRQ_NONE;\r\ncp = netdev_priv(dev);\r\nspin_lock(&cp->lock);\r\nstatus = cpr16(IntrStatus);\r\nif (!status || (status == 0xFFFF))\r\ngoto out_unlock;\r\nhandled = 1;\r\nnetif_dbg(cp, intr, dev, "intr, status %04x cmd %02x cpcmd %04x\n",\r\nstatus, cpr8(Cmd), cpr16(CpCmd));\r\ncpw16(IntrStatus, status & ~cp_rx_intr_mask);\r\nif (unlikely(!netif_running(dev))) {\r\ncpw16(IntrMask, 0);\r\ngoto out_unlock;\r\n}\r\nif (status & (RxOK | RxErr | RxEmpty | RxFIFOOvr))\r\nif (napi_schedule_prep(&cp->napi)) {\r\ncpw16_f(IntrMask, cp_norx_intr_mask);\r\n__napi_schedule(&cp->napi);\r\n}\r\nif (status & (TxOK | TxErr | TxEmpty | SWInt))\r\ncp_tx(cp);\r\nif (status & LinkChg)\r\nmii_check_media(&cp->mii_if, netif_msg_link(cp), false);\r\nif (status & PciErr) {\r\nu16 pci_status;\r\npci_read_config_word(cp->pdev, PCI_STATUS, &pci_status);\r\npci_write_config_word(cp->pdev, PCI_STATUS, pci_status);\r\nnetdev_err(dev, "PCI bus error, status=%04x, PCI status=%04x\n",\r\nstatus, pci_status);\r\n}\r\nout_unlock:\r\nspin_unlock(&cp->lock);\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic void cp_poll_controller(struct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nconst int irq = cp->pdev->irq;\r\ndisable_irq(irq);\r\ncp_interrupt(irq, dev);\r\nenable_irq(irq);\r\n}\r\nstatic void cp_tx (struct cp_private *cp)\r\n{\r\nunsigned tx_head = cp->tx_head;\r\nunsigned tx_tail = cp->tx_tail;\r\nunsigned bytes_compl = 0, pkts_compl = 0;\r\nwhile (tx_tail != tx_head) {\r\nstruct cp_desc *txd = cp->tx_ring + tx_tail;\r\nstruct sk_buff *skb;\r\nu32 status;\r\nrmb();\r\nstatus = le32_to_cpu(txd->opts1);\r\nif (status & DescOwn)\r\nbreak;\r\nskb = cp->tx_skb[tx_tail];\r\nBUG_ON(!skb);\r\ndma_unmap_single(&cp->pdev->dev, le64_to_cpu(txd->addr),\r\nle32_to_cpu(txd->opts1) & 0xffff,\r\nPCI_DMA_TODEVICE);\r\nif (status & LastFrag) {\r\nif (status & (TxError | TxFIFOUnder)) {\r\nnetif_dbg(cp, tx_err, cp->dev,\r\n"tx err, status 0x%x\n", status);\r\ncp->dev->stats.tx_errors++;\r\nif (status & TxOWC)\r\ncp->dev->stats.tx_window_errors++;\r\nif (status & TxMaxCol)\r\ncp->dev->stats.tx_aborted_errors++;\r\nif (status & TxLinkFail)\r\ncp->dev->stats.tx_carrier_errors++;\r\nif (status & TxFIFOUnder)\r\ncp->dev->stats.tx_fifo_errors++;\r\n} else {\r\ncp->dev->stats.collisions +=\r\n((status >> TxColCntShift) & TxColCntMask);\r\ncp->dev->stats.tx_packets++;\r\ncp->dev->stats.tx_bytes += skb->len;\r\nnetif_dbg(cp, tx_done, cp->dev,\r\n"tx done, slot %d\n", tx_tail);\r\n}\r\nbytes_compl += skb->len;\r\npkts_compl++;\r\ndev_kfree_skb_irq(skb);\r\n}\r\ncp->tx_skb[tx_tail] = NULL;\r\ntx_tail = NEXT_TX(tx_tail);\r\n}\r\ncp->tx_tail = tx_tail;\r\nnetdev_completed_queue(cp->dev, pkts_compl, bytes_compl);\r\nif (TX_BUFFS_AVAIL(cp) > (MAX_SKB_FRAGS + 1))\r\nnetif_wake_queue(cp->dev);\r\n}\r\nstatic inline u32 cp_tx_vlan_tag(struct sk_buff *skb)\r\n{\r\nreturn skb_vlan_tag_present(skb) ?\r\nTxVlanTag | swab16(skb_vlan_tag_get(skb)) : 0x00;\r\n}\r\nstatic void unwind_tx_frag_mapping(struct cp_private *cp, struct sk_buff *skb,\r\nint first, int entry_last)\r\n{\r\nint frag, index;\r\nstruct cp_desc *txd;\r\nskb_frag_t *this_frag;\r\nfor (frag = 0; frag+first < entry_last; frag++) {\r\nindex = first+frag;\r\ncp->tx_skb[index] = NULL;\r\ntxd = &cp->tx_ring[index];\r\nthis_frag = &skb_shinfo(skb)->frags[frag];\r\ndma_unmap_single(&cp->pdev->dev, le64_to_cpu(txd->addr),\r\nskb_frag_size(this_frag), PCI_DMA_TODEVICE);\r\n}\r\n}\r\nstatic netdev_tx_t cp_start_xmit (struct sk_buff *skb,\r\nstruct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned entry;\r\nu32 eor, flags;\r\nunsigned long intr_flags;\r\n__le32 opts2;\r\nint mss = 0;\r\nspin_lock_irqsave(&cp->lock, intr_flags);\r\nif (TX_BUFFS_AVAIL(cp) <= (skb_shinfo(skb)->nr_frags + 1)) {\r\nnetif_stop_queue(dev);\r\nspin_unlock_irqrestore(&cp->lock, intr_flags);\r\nnetdev_err(dev, "BUG! Tx Ring full when queue awake!\n");\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nentry = cp->tx_head;\r\neor = (entry == (CP_TX_RING_SIZE - 1)) ? RingEnd : 0;\r\nmss = skb_shinfo(skb)->gso_size;\r\nopts2 = cpu_to_le32(cp_tx_vlan_tag(skb));\r\nif (skb_shinfo(skb)->nr_frags == 0) {\r\nstruct cp_desc *txd = &cp->tx_ring[entry];\r\nu32 len;\r\ndma_addr_t mapping;\r\nlen = skb->len;\r\nmapping = dma_map_single(&cp->pdev->dev, skb->data, len, PCI_DMA_TODEVICE);\r\nif (dma_mapping_error(&cp->pdev->dev, mapping))\r\ngoto out_dma_error;\r\ntxd->opts2 = opts2;\r\ntxd->addr = cpu_to_le64(mapping);\r\nwmb();\r\nflags = eor | len | DescOwn | FirstFrag | LastFrag;\r\nif (mss)\r\nflags |= LargeSend | ((mss & MSSMask) << MSSShift);\r\nelse if (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nconst struct iphdr *ip = ip_hdr(skb);\r\nif (ip->protocol == IPPROTO_TCP)\r\nflags |= IPCS | TCPCS;\r\nelse if (ip->protocol == IPPROTO_UDP)\r\nflags |= IPCS | UDPCS;\r\nelse\r\nWARN_ON(1);\r\n}\r\ntxd->opts1 = cpu_to_le32(flags);\r\nwmb();\r\ncp->tx_skb[entry] = skb;\r\nentry = NEXT_TX(entry);\r\n} else {\r\nstruct cp_desc *txd;\r\nu32 first_len, first_eor;\r\ndma_addr_t first_mapping;\r\nint frag, first_entry = entry;\r\nconst struct iphdr *ip = ip_hdr(skb);\r\nfirst_eor = eor;\r\nfirst_len = skb_headlen(skb);\r\nfirst_mapping = dma_map_single(&cp->pdev->dev, skb->data,\r\nfirst_len, PCI_DMA_TODEVICE);\r\nif (dma_mapping_error(&cp->pdev->dev, first_mapping))\r\ngoto out_dma_error;\r\ncp->tx_skb[entry] = skb;\r\nentry = NEXT_TX(entry);\r\nfor (frag = 0; frag < skb_shinfo(skb)->nr_frags; frag++) {\r\nconst skb_frag_t *this_frag = &skb_shinfo(skb)->frags[frag];\r\nu32 len;\r\nu32 ctrl;\r\ndma_addr_t mapping;\r\nlen = skb_frag_size(this_frag);\r\nmapping = dma_map_single(&cp->pdev->dev,\r\nskb_frag_address(this_frag),\r\nlen, PCI_DMA_TODEVICE);\r\nif (dma_mapping_error(&cp->pdev->dev, mapping)) {\r\nunwind_tx_frag_mapping(cp, skb, first_entry, entry);\r\ngoto out_dma_error;\r\n}\r\neor = (entry == (CP_TX_RING_SIZE - 1)) ? RingEnd : 0;\r\nctrl = eor | len | DescOwn;\r\nif (mss)\r\nctrl |= LargeSend |\r\n((mss & MSSMask) << MSSShift);\r\nelse if (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nif (ip->protocol == IPPROTO_TCP)\r\nctrl |= IPCS | TCPCS;\r\nelse if (ip->protocol == IPPROTO_UDP)\r\nctrl |= IPCS | UDPCS;\r\nelse\r\nBUG();\r\n}\r\nif (frag == skb_shinfo(skb)->nr_frags - 1)\r\nctrl |= LastFrag;\r\ntxd = &cp->tx_ring[entry];\r\ntxd->opts2 = opts2;\r\ntxd->addr = cpu_to_le64(mapping);\r\nwmb();\r\ntxd->opts1 = cpu_to_le32(ctrl);\r\nwmb();\r\ncp->tx_skb[entry] = skb;\r\nentry = NEXT_TX(entry);\r\n}\r\ntxd = &cp->tx_ring[first_entry];\r\ntxd->opts2 = opts2;\r\ntxd->addr = cpu_to_le64(first_mapping);\r\nwmb();\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nif (ip->protocol == IPPROTO_TCP)\r\ntxd->opts1 = cpu_to_le32(first_eor | first_len |\r\nFirstFrag | DescOwn |\r\nIPCS | TCPCS);\r\nelse if (ip->protocol == IPPROTO_UDP)\r\ntxd->opts1 = cpu_to_le32(first_eor | first_len |\r\nFirstFrag | DescOwn |\r\nIPCS | UDPCS);\r\nelse\r\nBUG();\r\n} else\r\ntxd->opts1 = cpu_to_le32(first_eor | first_len |\r\nFirstFrag | DescOwn);\r\nwmb();\r\n}\r\ncp->tx_head = entry;\r\nnetdev_sent_queue(dev, skb->len);\r\nnetif_dbg(cp, tx_queued, cp->dev, "tx queued, slot %d, skblen %d\n",\r\nentry, skb->len);\r\nif (TX_BUFFS_AVAIL(cp) <= (MAX_SKB_FRAGS + 1))\r\nnetif_stop_queue(dev);\r\nout_unlock:\r\nspin_unlock_irqrestore(&cp->lock, intr_flags);\r\ncpw8(TxPoll, NormalTxPoll);\r\nreturn NETDEV_TX_OK;\r\nout_dma_error:\r\ndev_kfree_skb_any(skb);\r\ncp->dev->stats.tx_dropped++;\r\ngoto out_unlock;\r\n}\r\nstatic void __cp_set_rx_mode (struct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nu32 mc_filter[2];\r\nint rx_mode;\r\nif (dev->flags & IFF_PROMISC) {\r\nrx_mode =\r\nAcceptBroadcast | AcceptMulticast | AcceptMyPhys |\r\nAcceptAllPhys;\r\nmc_filter[1] = mc_filter[0] = 0xffffffff;\r\n} else if ((netdev_mc_count(dev) > multicast_filter_limit) ||\r\n(dev->flags & IFF_ALLMULTI)) {\r\nrx_mode = AcceptBroadcast | AcceptMulticast | AcceptMyPhys;\r\nmc_filter[1] = mc_filter[0] = 0xffffffff;\r\n} else {\r\nstruct netdev_hw_addr *ha;\r\nrx_mode = AcceptBroadcast | AcceptMyPhys;\r\nmc_filter[1] = mc_filter[0] = 0;\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nint bit_nr = ether_crc(ETH_ALEN, ha->addr) >> 26;\r\nmc_filter[bit_nr >> 5] |= 1 << (bit_nr & 31);\r\nrx_mode |= AcceptMulticast;\r\n}\r\n}\r\ncp->rx_config = cp_rx_config | rx_mode;\r\ncpw32_f(RxConfig, cp->rx_config);\r\ncpw32_f (MAR0 + 0, mc_filter[0]);\r\ncpw32_f (MAR0 + 4, mc_filter[1]);\r\n}\r\nstatic void cp_set_rx_mode (struct net_device *dev)\r\n{\r\nunsigned long flags;\r\nstruct cp_private *cp = netdev_priv(dev);\r\nspin_lock_irqsave (&cp->lock, flags);\r\n__cp_set_rx_mode(dev);\r\nspin_unlock_irqrestore (&cp->lock, flags);\r\n}\r\nstatic void __cp_get_stats(struct cp_private *cp)\r\n{\r\ncp->dev->stats.rx_missed_errors += (cpr32 (RxMissed) & 0xffffff);\r\ncpw32 (RxMissed, 0);\r\n}\r\nstatic struct net_device_stats *cp_get_stats(struct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nif (netif_running(dev) && netif_device_present(dev))\r\n__cp_get_stats(cp);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn &dev->stats;\r\n}\r\nstatic void cp_stop_hw (struct cp_private *cp)\r\n{\r\ncpw16(IntrStatus, ~(cpr16(IntrStatus)));\r\ncpw16_f(IntrMask, 0);\r\ncpw8(Cmd, 0);\r\ncpw16_f(CpCmd, 0);\r\ncpw16_f(IntrStatus, ~(cpr16(IntrStatus)));\r\ncp->rx_tail = 0;\r\ncp->tx_head = cp->tx_tail = 0;\r\nnetdev_reset_queue(cp->dev);\r\n}\r\nstatic void cp_reset_hw (struct cp_private *cp)\r\n{\r\nunsigned work = 1000;\r\ncpw8(Cmd, CmdReset);\r\nwhile (work--) {\r\nif (!(cpr8(Cmd) & CmdReset))\r\nreturn;\r\nschedule_timeout_uninterruptible(10);\r\n}\r\nnetdev_err(cp->dev, "hardware reset timeout\n");\r\n}\r\nstatic inline void cp_start_hw (struct cp_private *cp)\r\n{\r\ndma_addr_t ring_dma;\r\ncpw16(CpCmd, cp->cpcmd);\r\ncpw32_f(HiTxRingAddr, 0);\r\ncpw32_f(HiTxRingAddr + 4, 0);\r\nring_dma = cp->ring_dma;\r\ncpw32_f(RxRingAddr, ring_dma & 0xffffffff);\r\ncpw32_f(RxRingAddr + 4, (ring_dma >> 16) >> 16);\r\nring_dma += sizeof(struct cp_desc) * CP_RX_RING_SIZE;\r\ncpw32_f(TxRingAddr, ring_dma & 0xffffffff);\r\ncpw32_f(TxRingAddr + 4, (ring_dma >> 16) >> 16);\r\ncpw8(Cmd, RxOn | TxOn);\r\nnetdev_reset_queue(cp->dev);\r\n}\r\nstatic void cp_enable_irq(struct cp_private *cp)\r\n{\r\ncpw16_f(IntrMask, cp_intr_mask);\r\n}\r\nstatic void cp_init_hw (struct cp_private *cp)\r\n{\r\nstruct net_device *dev = cp->dev;\r\ncp_reset_hw(cp);\r\ncpw8_f (Cfg9346, Cfg9346_Unlock);\r\ncpw32_f (MAC0 + 0, le32_to_cpu (*(__le32 *) (dev->dev_addr + 0)));\r\ncpw32_f (MAC0 + 4, le32_to_cpu (*(__le32 *) (dev->dev_addr + 4)));\r\ncp_start_hw(cp);\r\ncpw8(TxThresh, 0x06);\r\n__cp_set_rx_mode(dev);\r\ncpw32_f (TxConfig, IFG | (TX_DMA_BURST << TxDMAShift));\r\ncpw8(Config1, cpr8(Config1) | DriverLoaded | PMEnable);\r\ncpw8(Config3, PARMEnable);\r\ncp->wol_enabled = 0;\r\ncpw8(Config5, cpr8(Config5) & PMEStatus);\r\ncpw16(MultiIntr, 0);\r\ncpw8_f(Cfg9346, Cfg9346_Lock);\r\n}\r\nstatic int cp_refill_rx(struct cp_private *cp)\r\n{\r\nstruct net_device *dev = cp->dev;\r\nunsigned i;\r\nfor (i = 0; i < CP_RX_RING_SIZE; i++) {\r\nstruct sk_buff *skb;\r\ndma_addr_t mapping;\r\nskb = netdev_alloc_skb_ip_align(dev, cp->rx_buf_sz);\r\nif (!skb)\r\ngoto err_out;\r\nmapping = dma_map_single(&cp->pdev->dev, skb->data,\r\ncp->rx_buf_sz, PCI_DMA_FROMDEVICE);\r\nif (dma_mapping_error(&cp->pdev->dev, mapping)) {\r\nkfree_skb(skb);\r\ngoto err_out;\r\n}\r\ncp->rx_skb[i] = skb;\r\ncp->rx_ring[i].opts2 = 0;\r\ncp->rx_ring[i].addr = cpu_to_le64(mapping);\r\nif (i == (CP_RX_RING_SIZE - 1))\r\ncp->rx_ring[i].opts1 =\r\ncpu_to_le32(DescOwn | RingEnd | cp->rx_buf_sz);\r\nelse\r\ncp->rx_ring[i].opts1 =\r\ncpu_to_le32(DescOwn | cp->rx_buf_sz);\r\n}\r\nreturn 0;\r\nerr_out:\r\ncp_clean_rings(cp);\r\nreturn -ENOMEM;\r\n}\r\nstatic void cp_init_rings_index (struct cp_private *cp)\r\n{\r\ncp->rx_tail = 0;\r\ncp->tx_head = cp->tx_tail = 0;\r\n}\r\nstatic int cp_init_rings (struct cp_private *cp)\r\n{\r\nmemset(cp->tx_ring, 0, sizeof(struct cp_desc) * CP_TX_RING_SIZE);\r\ncp->tx_ring[CP_TX_RING_SIZE - 1].opts1 = cpu_to_le32(RingEnd);\r\ncp_init_rings_index(cp);\r\nreturn cp_refill_rx (cp);\r\n}\r\nstatic int cp_alloc_rings (struct cp_private *cp)\r\n{\r\nstruct device *d = &cp->pdev->dev;\r\nvoid *mem;\r\nint rc;\r\nmem = dma_alloc_coherent(d, CP_RING_BYTES, &cp->ring_dma, GFP_KERNEL);\r\nif (!mem)\r\nreturn -ENOMEM;\r\ncp->rx_ring = mem;\r\ncp->tx_ring = &cp->rx_ring[CP_RX_RING_SIZE];\r\nrc = cp_init_rings(cp);\r\nif (rc < 0)\r\ndma_free_coherent(d, CP_RING_BYTES, cp->rx_ring, cp->ring_dma);\r\nreturn rc;\r\n}\r\nstatic void cp_clean_rings (struct cp_private *cp)\r\n{\r\nstruct cp_desc *desc;\r\nunsigned i;\r\nfor (i = 0; i < CP_RX_RING_SIZE; i++) {\r\nif (cp->rx_skb[i]) {\r\ndesc = cp->rx_ring + i;\r\ndma_unmap_single(&cp->pdev->dev,le64_to_cpu(desc->addr),\r\ncp->rx_buf_sz, PCI_DMA_FROMDEVICE);\r\ndev_kfree_skb(cp->rx_skb[i]);\r\n}\r\n}\r\nfor (i = 0; i < CP_TX_RING_SIZE; i++) {\r\nif (cp->tx_skb[i]) {\r\nstruct sk_buff *skb = cp->tx_skb[i];\r\ndesc = cp->tx_ring + i;\r\ndma_unmap_single(&cp->pdev->dev,le64_to_cpu(desc->addr),\r\nle32_to_cpu(desc->opts1) & 0xffff,\r\nPCI_DMA_TODEVICE);\r\nif (le32_to_cpu(desc->opts1) & LastFrag)\r\ndev_kfree_skb(skb);\r\ncp->dev->stats.tx_dropped++;\r\n}\r\n}\r\nnetdev_reset_queue(cp->dev);\r\nmemset(cp->rx_ring, 0, sizeof(struct cp_desc) * CP_RX_RING_SIZE);\r\nmemset(cp->tx_ring, 0, sizeof(struct cp_desc) * CP_TX_RING_SIZE);\r\nmemset(cp->rx_skb, 0, sizeof(struct sk_buff *) * CP_RX_RING_SIZE);\r\nmemset(cp->tx_skb, 0, sizeof(struct sk_buff *) * CP_TX_RING_SIZE);\r\n}\r\nstatic void cp_free_rings (struct cp_private *cp)\r\n{\r\ncp_clean_rings(cp);\r\ndma_free_coherent(&cp->pdev->dev, CP_RING_BYTES, cp->rx_ring,\r\ncp->ring_dma);\r\ncp->rx_ring = NULL;\r\ncp->tx_ring = NULL;\r\n}\r\nstatic int cp_open (struct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nconst int irq = cp->pdev->irq;\r\nint rc;\r\nnetif_dbg(cp, ifup, dev, "enabling interface\n");\r\nrc = cp_alloc_rings(cp);\r\nif (rc)\r\nreturn rc;\r\nnapi_enable(&cp->napi);\r\ncp_init_hw(cp);\r\nrc = request_irq(irq, cp_interrupt, IRQF_SHARED, dev->name, dev);\r\nif (rc)\r\ngoto err_out_hw;\r\ncp_enable_irq(cp);\r\nnetif_carrier_off(dev);\r\nmii_check_media(&cp->mii_if, netif_msg_link(cp), true);\r\nnetif_start_queue(dev);\r\nreturn 0;\r\nerr_out_hw:\r\nnapi_disable(&cp->napi);\r\ncp_stop_hw(cp);\r\ncp_free_rings(cp);\r\nreturn rc;\r\n}\r\nstatic int cp_close (struct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nnapi_disable(&cp->napi);\r\nnetif_dbg(cp, ifdown, dev, "disabling interface\n");\r\nspin_lock_irqsave(&cp->lock, flags);\r\nnetif_stop_queue(dev);\r\nnetif_carrier_off(dev);\r\ncp_stop_hw(cp);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nfree_irq(cp->pdev->irq, dev);\r\ncp_free_rings(cp);\r\nreturn 0;\r\n}\r\nstatic void cp_tx_timeout(struct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nint rc;\r\nnetdev_warn(dev, "Transmit timeout, status %2x %4x %4x %4x\n",\r\ncpr8(Cmd), cpr16(CpCmd),\r\ncpr16(IntrStatus), cpr16(IntrMask));\r\nspin_lock_irqsave(&cp->lock, flags);\r\ncp_stop_hw(cp);\r\ncp_clean_rings(cp);\r\nrc = cp_init_rings(cp);\r\ncp_start_hw(cp);\r\ncp_enable_irq(cp);\r\nnetif_wake_queue(dev);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\n}\r\nstatic int cp_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nif (new_mtu < CP_MIN_MTU || new_mtu > CP_MAX_MTU)\r\nreturn -EINVAL;\r\nif (!netif_running(dev)) {\r\ndev->mtu = new_mtu;\r\ncp_set_rxbufsize(cp);\r\nreturn 0;\r\n}\r\ncp_close(dev);\r\ndev->mtu = new_mtu;\r\ncp_set_rxbufsize(cp);\r\nreturn cp_open(dev);\r\n}\r\nstatic int mdio_read(struct net_device *dev, int phy_id, int location)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nreturn location < 8 && mii_2_8139_map[location] ?\r\nreadw(cp->regs + mii_2_8139_map[location]) : 0;\r\n}\r\nstatic void mdio_write(struct net_device *dev, int phy_id, int location,\r\nint value)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nif (location == 0) {\r\ncpw8(Cfg9346, Cfg9346_Unlock);\r\ncpw16(BasicModeCtrl, value);\r\ncpw8(Cfg9346, Cfg9346_Lock);\r\n} else if (location < 8 && mii_2_8139_map[location])\r\ncpw16(mii_2_8139_map[location], value);\r\n}\r\nstatic int netdev_set_wol (struct cp_private *cp,\r\nconst struct ethtool_wolinfo *wol)\r\n{\r\nu8 options;\r\noptions = cpr8 (Config3) & ~(LinkUp | MagicPacket);\r\nif (wol->wolopts) {\r\nif (wol->wolopts & WAKE_PHY) options |= LinkUp;\r\nif (wol->wolopts & WAKE_MAGIC) options |= MagicPacket;\r\n}\r\ncpw8 (Cfg9346, Cfg9346_Unlock);\r\ncpw8 (Config3, options);\r\ncpw8 (Cfg9346, Cfg9346_Lock);\r\noptions = 0;\r\noptions = cpr8 (Config5) & ~(UWF | MWF | BWF);\r\nif (wol->wolopts) {\r\nif (wol->wolopts & WAKE_UCAST) options |= UWF;\r\nif (wol->wolopts & WAKE_BCAST) options |= BWF;\r\nif (wol->wolopts & WAKE_MCAST) options |= MWF;\r\n}\r\ncpw8 (Config5, options);\r\ncp->wol_enabled = (wol->wolopts) ? 1 : 0;\r\nreturn 0;\r\n}\r\nstatic void netdev_get_wol (struct cp_private *cp,\r\nstruct ethtool_wolinfo *wol)\r\n{\r\nu8 options;\r\nwol->wolopts = 0;\r\nwol->supported = WAKE_PHY | WAKE_BCAST | WAKE_MAGIC |\r\nWAKE_MCAST | WAKE_UCAST;\r\nif (!cp->wol_enabled) return;\r\noptions = cpr8 (Config3);\r\nif (options & LinkUp) wol->wolopts |= WAKE_PHY;\r\nif (options & MagicPacket) wol->wolopts |= WAKE_MAGIC;\r\noptions = 0;\r\noptions = cpr8 (Config5);\r\nif (options & UWF) wol->wolopts |= WAKE_UCAST;\r\nif (options & BWF) wol->wolopts |= WAKE_BCAST;\r\nif (options & MWF) wol->wolopts |= WAKE_MCAST;\r\n}\r\nstatic void cp_get_drvinfo (struct net_device *dev, struct ethtool_drvinfo *info)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nstrlcpy(info->driver, DRV_NAME, sizeof(info->driver));\r\nstrlcpy(info->version, DRV_VERSION, sizeof(info->version));\r\nstrlcpy(info->bus_info, pci_name(cp->pdev), sizeof(info->bus_info));\r\n}\r\nstatic void cp_get_ringparam(struct net_device *dev,\r\nstruct ethtool_ringparam *ring)\r\n{\r\nring->rx_max_pending = CP_RX_RING_SIZE;\r\nring->tx_max_pending = CP_TX_RING_SIZE;\r\nring->rx_pending = CP_RX_RING_SIZE;\r\nring->tx_pending = CP_TX_RING_SIZE;\r\n}\r\nstatic int cp_get_regs_len(struct net_device *dev)\r\n{\r\nreturn CP_REGS_SIZE;\r\n}\r\nstatic int cp_get_sset_count (struct net_device *dev, int sset)\r\n{\r\nswitch (sset) {\r\ncase ETH_SS_STATS:\r\nreturn CP_NUM_STATS;\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic int cp_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nint rc;\r\nunsigned long flags;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nrc = mii_ethtool_gset(&cp->mii_if, cmd);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn rc;\r\n}\r\nstatic int cp_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nint rc;\r\nunsigned long flags;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nrc = mii_ethtool_sset(&cp->mii_if, cmd);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn rc;\r\n}\r\nstatic int cp_nway_reset(struct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nreturn mii_nway_restart(&cp->mii_if);\r\n}\r\nstatic u32 cp_get_msglevel(struct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nreturn cp->msg_enable;\r\n}\r\nstatic void cp_set_msglevel(struct net_device *dev, u32 value)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\ncp->msg_enable = value;\r\n}\r\nstatic int cp_set_features(struct net_device *dev, netdev_features_t features)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nif (!((dev->features ^ features) & NETIF_F_RXCSUM))\r\nreturn 0;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nif (features & NETIF_F_RXCSUM)\r\ncp->cpcmd |= RxChkSum;\r\nelse\r\ncp->cpcmd &= ~RxChkSum;\r\nif (features & NETIF_F_HW_VLAN_CTAG_RX)\r\ncp->cpcmd |= RxVlanOn;\r\nelse\r\ncp->cpcmd &= ~RxVlanOn;\r\ncpw16_f(CpCmd, cp->cpcmd);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn 0;\r\n}\r\nstatic void cp_get_regs(struct net_device *dev, struct ethtool_regs *regs,\r\nvoid *p)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nif (regs->len < CP_REGS_SIZE)\r\nreturn ;\r\nregs->version = CP_REGS_VER;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nmemcpy_fromio(p, cp->regs, CP_REGS_SIZE);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\n}\r\nstatic void cp_get_wol (struct net_device *dev, struct ethtool_wolinfo *wol)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nspin_lock_irqsave (&cp->lock, flags);\r\nnetdev_get_wol (cp, wol);\r\nspin_unlock_irqrestore (&cp->lock, flags);\r\n}\r\nstatic int cp_set_wol (struct net_device *dev, struct ethtool_wolinfo *wol)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nint rc;\r\nspin_lock_irqsave (&cp->lock, flags);\r\nrc = netdev_set_wol (cp, wol);\r\nspin_unlock_irqrestore (&cp->lock, flags);\r\nreturn rc;\r\n}\r\nstatic void cp_get_strings (struct net_device *dev, u32 stringset, u8 *buf)\r\n{\r\nswitch (stringset) {\r\ncase ETH_SS_STATS:\r\nmemcpy(buf, &ethtool_stats_keys, sizeof(ethtool_stats_keys));\r\nbreak;\r\ndefault:\r\nBUG();\r\nbreak;\r\n}\r\n}\r\nstatic void cp_get_ethtool_stats (struct net_device *dev,\r\nstruct ethtool_stats *estats, u64 *tmp_stats)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nstruct cp_dma_stats *nic_stats;\r\ndma_addr_t dma;\r\nint i;\r\nnic_stats = dma_alloc_coherent(&cp->pdev->dev, sizeof(*nic_stats),\r\n&dma, GFP_KERNEL);\r\nif (!nic_stats)\r\nreturn;\r\ncpw32(StatsAddr + 4, (u64)dma >> 32);\r\ncpw32(StatsAddr, ((u64)dma & DMA_BIT_MASK(32)) | DumpStats);\r\ncpr32(StatsAddr);\r\nfor (i = 0; i < 1000; i++) {\r\nif ((cpr32(StatsAddr) & DumpStats) == 0)\r\nbreak;\r\nudelay(10);\r\n}\r\ncpw32(StatsAddr, 0);\r\ncpw32(StatsAddr + 4, 0);\r\ncpr32(StatsAddr);\r\ni = 0;\r\ntmp_stats[i++] = le64_to_cpu(nic_stats->tx_ok);\r\ntmp_stats[i++] = le64_to_cpu(nic_stats->rx_ok);\r\ntmp_stats[i++] = le64_to_cpu(nic_stats->tx_err);\r\ntmp_stats[i++] = le32_to_cpu(nic_stats->rx_err);\r\ntmp_stats[i++] = le16_to_cpu(nic_stats->rx_fifo);\r\ntmp_stats[i++] = le16_to_cpu(nic_stats->frame_align);\r\ntmp_stats[i++] = le32_to_cpu(nic_stats->tx_ok_1col);\r\ntmp_stats[i++] = le32_to_cpu(nic_stats->tx_ok_mcol);\r\ntmp_stats[i++] = le64_to_cpu(nic_stats->rx_ok_phys);\r\ntmp_stats[i++] = le64_to_cpu(nic_stats->rx_ok_bcast);\r\ntmp_stats[i++] = le32_to_cpu(nic_stats->rx_ok_mcast);\r\ntmp_stats[i++] = le16_to_cpu(nic_stats->tx_abort);\r\ntmp_stats[i++] = le16_to_cpu(nic_stats->tx_underrun);\r\ntmp_stats[i++] = cp->cp_stats.rx_frags;\r\nBUG_ON(i != CP_NUM_STATS);\r\ndma_free_coherent(&cp->pdev->dev, sizeof(*nic_stats), nic_stats, dma);\r\n}\r\nstatic int cp_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nint rc;\r\nunsigned long flags;\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nrc = generic_mii_ioctl(&cp->mii_if, if_mii(rq), cmd, NULL);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn rc;\r\n}\r\nstatic int cp_set_mac_address(struct net_device *dev, void *p)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nstruct sockaddr *addr = p;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(dev->dev_addr, addr->sa_data, dev->addr_len);\r\nspin_lock_irq(&cp->lock);\r\ncpw8_f(Cfg9346, Cfg9346_Unlock);\r\ncpw32_f(MAC0 + 0, le32_to_cpu (*(__le32 *) (dev->dev_addr + 0)));\r\ncpw32_f(MAC0 + 4, le32_to_cpu (*(__le32 *) (dev->dev_addr + 4)));\r\ncpw8_f(Cfg9346, Cfg9346_Lock);\r\nspin_unlock_irq(&cp->lock);\r\nreturn 0;\r\n}\r\nstatic void eeprom_cmd_start(void __iomem *ee_addr)\r\n{\r\nwriteb (EE_ENB & ~EE_CS, ee_addr);\r\nwriteb (EE_ENB, ee_addr);\r\neeprom_delay ();\r\n}\r\nstatic void eeprom_cmd(void __iomem *ee_addr, int cmd, int cmd_len)\r\n{\r\nint i;\r\nfor (i = cmd_len - 1; i >= 0; i--) {\r\nint dataval = (cmd & (1 << i)) ? EE_DATA_WRITE : 0;\r\nwriteb (EE_ENB | dataval, ee_addr);\r\neeprom_delay ();\r\nwriteb (EE_ENB | dataval | EE_SHIFT_CLK, ee_addr);\r\neeprom_delay ();\r\n}\r\nwriteb (EE_ENB, ee_addr);\r\neeprom_delay ();\r\n}\r\nstatic void eeprom_cmd_end(void __iomem *ee_addr)\r\n{\r\nwriteb(0, ee_addr);\r\neeprom_delay ();\r\n}\r\nstatic void eeprom_extend_cmd(void __iomem *ee_addr, int extend_cmd,\r\nint addr_len)\r\n{\r\nint cmd = (EE_EXTEND_CMD << addr_len) | (extend_cmd << (addr_len - 2));\r\neeprom_cmd_start(ee_addr);\r\neeprom_cmd(ee_addr, cmd, 3 + addr_len);\r\neeprom_cmd_end(ee_addr);\r\n}\r\nstatic u16 read_eeprom (void __iomem *ioaddr, int location, int addr_len)\r\n{\r\nint i;\r\nu16 retval = 0;\r\nvoid __iomem *ee_addr = ioaddr + Cfg9346;\r\nint read_cmd = location | (EE_READ_CMD << addr_len);\r\neeprom_cmd_start(ee_addr);\r\neeprom_cmd(ee_addr, read_cmd, 3 + addr_len);\r\nfor (i = 16; i > 0; i--) {\r\nwriteb (EE_ENB | EE_SHIFT_CLK, ee_addr);\r\neeprom_delay ();\r\nretval =\r\n(retval << 1) | ((readb (ee_addr) & EE_DATA_READ) ? 1 :\r\n0);\r\nwriteb (EE_ENB, ee_addr);\r\neeprom_delay ();\r\n}\r\neeprom_cmd_end(ee_addr);\r\nreturn retval;\r\n}\r\nstatic void write_eeprom(void __iomem *ioaddr, int location, u16 val,\r\nint addr_len)\r\n{\r\nint i;\r\nvoid __iomem *ee_addr = ioaddr + Cfg9346;\r\nint write_cmd = location | (EE_WRITE_CMD << addr_len);\r\neeprom_extend_cmd(ee_addr, EE_EWEN_ADDR, addr_len);\r\neeprom_cmd_start(ee_addr);\r\neeprom_cmd(ee_addr, write_cmd, 3 + addr_len);\r\neeprom_cmd(ee_addr, val, 16);\r\neeprom_cmd_end(ee_addr);\r\neeprom_cmd_start(ee_addr);\r\nfor (i = 0; i < 20000; i++)\r\nif (readb(ee_addr) & EE_DATA_READ)\r\nbreak;\r\neeprom_cmd_end(ee_addr);\r\neeprom_extend_cmd(ee_addr, EE_EWDS_ADDR, addr_len);\r\n}\r\nstatic int cp_get_eeprom_len(struct net_device *dev)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nint size;\r\nspin_lock_irq(&cp->lock);\r\nsize = read_eeprom(cp->regs, 0, 8) == 0x8129 ? 256 : 128;\r\nspin_unlock_irq(&cp->lock);\r\nreturn size;\r\n}\r\nstatic int cp_get_eeprom(struct net_device *dev,\r\nstruct ethtool_eeprom *eeprom, u8 *data)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned int addr_len;\r\nu16 val;\r\nu32 offset = eeprom->offset >> 1;\r\nu32 len = eeprom->len;\r\nu32 i = 0;\r\neeprom->magic = CP_EEPROM_MAGIC;\r\nspin_lock_irq(&cp->lock);\r\naddr_len = read_eeprom(cp->regs, 0, 8) == 0x8129 ? 8 : 6;\r\nif (eeprom->offset & 1) {\r\nval = read_eeprom(cp->regs, offset, addr_len);\r\ndata[i++] = (u8)(val >> 8);\r\noffset++;\r\n}\r\nwhile (i < len - 1) {\r\nval = read_eeprom(cp->regs, offset, addr_len);\r\ndata[i++] = (u8)val;\r\ndata[i++] = (u8)(val >> 8);\r\noffset++;\r\n}\r\nif (i < len) {\r\nval = read_eeprom(cp->regs, offset, addr_len);\r\ndata[i] = (u8)val;\r\n}\r\nspin_unlock_irq(&cp->lock);\r\nreturn 0;\r\n}\r\nstatic int cp_set_eeprom(struct net_device *dev,\r\nstruct ethtool_eeprom *eeprom, u8 *data)\r\n{\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned int addr_len;\r\nu16 val;\r\nu32 offset = eeprom->offset >> 1;\r\nu32 len = eeprom->len;\r\nu32 i = 0;\r\nif (eeprom->magic != CP_EEPROM_MAGIC)\r\nreturn -EINVAL;\r\nspin_lock_irq(&cp->lock);\r\naddr_len = read_eeprom(cp->regs, 0, 8) == 0x8129 ? 8 : 6;\r\nif (eeprom->offset & 1) {\r\nval = read_eeprom(cp->regs, offset, addr_len) & 0xff;\r\nval |= (u16)data[i++] << 8;\r\nwrite_eeprom(cp->regs, offset, val, addr_len);\r\noffset++;\r\n}\r\nwhile (i < len - 1) {\r\nval = (u16)data[i++];\r\nval |= (u16)data[i++] << 8;\r\nwrite_eeprom(cp->regs, offset, val, addr_len);\r\noffset++;\r\n}\r\nif (i < len) {\r\nval = read_eeprom(cp->regs, offset, addr_len) & 0xff00;\r\nval |= (u16)data[i];\r\nwrite_eeprom(cp->regs, offset, val, addr_len);\r\n}\r\nspin_unlock_irq(&cp->lock);\r\nreturn 0;\r\n}\r\nstatic void cp_set_d3_state (struct cp_private *cp)\r\n{\r\npci_enable_wake(cp->pdev, PCI_D0, 1);\r\npci_set_power_state (cp->pdev, PCI_D3hot);\r\n}\r\nstatic int cp_init_one (struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nstruct net_device *dev;\r\nstruct cp_private *cp;\r\nint rc;\r\nvoid __iomem *regs;\r\nresource_size_t pciaddr;\r\nunsigned int addr_len, i, pci_using_dac;\r\npr_info_once("%s", version);\r\nif (pdev->vendor == PCI_VENDOR_ID_REALTEK &&\r\npdev->device == PCI_DEVICE_ID_REALTEK_8139 && pdev->revision < 0x20) {\r\ndev_info(&pdev->dev,\r\n"This (id %04x:%04x rev %02x) is not an 8139C+ compatible chip, use 8139too\n",\r\npdev->vendor, pdev->device, pdev->revision);\r\nreturn -ENODEV;\r\n}\r\ndev = alloc_etherdev(sizeof(struct cp_private));\r\nif (!dev)\r\nreturn -ENOMEM;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\ncp = netdev_priv(dev);\r\ncp->pdev = pdev;\r\ncp->dev = dev;\r\ncp->msg_enable = (debug < 0 ? CP_DEF_MSG_ENABLE : debug);\r\nspin_lock_init (&cp->lock);\r\ncp->mii_if.dev = dev;\r\ncp->mii_if.mdio_read = mdio_read;\r\ncp->mii_if.mdio_write = mdio_write;\r\ncp->mii_if.phy_id = CP_INTERNAL_PHY;\r\ncp->mii_if.phy_id_mask = 0x1f;\r\ncp->mii_if.reg_num_mask = 0x1f;\r\ncp_set_rxbufsize(cp);\r\nrc = pci_enable_device(pdev);\r\nif (rc)\r\ngoto err_out_free;\r\nrc = pci_set_mwi(pdev);\r\nif (rc)\r\ngoto err_out_disable;\r\nrc = pci_request_regions(pdev, DRV_NAME);\r\nif (rc)\r\ngoto err_out_mwi;\r\npciaddr = pci_resource_start(pdev, 1);\r\nif (!pciaddr) {\r\nrc = -EIO;\r\ndev_err(&pdev->dev, "no MMIO resource\n");\r\ngoto err_out_res;\r\n}\r\nif (pci_resource_len(pdev, 1) < CP_REGS_SIZE) {\r\nrc = -EIO;\r\ndev_err(&pdev->dev, "MMIO resource (%llx) too small\n",\r\n(unsigned long long)pci_resource_len(pdev, 1));\r\ngoto err_out_res;\r\n}\r\nif ((sizeof(dma_addr_t) > 4) &&\r\n!pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64)) &&\r\n!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))) {\r\npci_using_dac = 1;\r\n} else {\r\npci_using_dac = 0;\r\nrc = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc) {\r\ndev_err(&pdev->dev,\r\n"No usable DMA configuration, aborting\n");\r\ngoto err_out_res;\r\n}\r\nrc = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (rc) {\r\ndev_err(&pdev->dev,\r\n"No usable consistent DMA configuration, aborting\n");\r\ngoto err_out_res;\r\n}\r\n}\r\ncp->cpcmd = (pci_using_dac ? PCIDAC : 0) |\r\nPCIMulRW | RxChkSum | CpRxOn | CpTxOn;\r\ndev->features |= NETIF_F_RXCSUM;\r\ndev->hw_features |= NETIF_F_RXCSUM;\r\nregs = ioremap(pciaddr, CP_REGS_SIZE);\r\nif (!regs) {\r\nrc = -EIO;\r\ndev_err(&pdev->dev, "Cannot map PCI MMIO (%Lx@%Lx)\n",\r\n(unsigned long long)pci_resource_len(pdev, 1),\r\n(unsigned long long)pciaddr);\r\ngoto err_out_res;\r\n}\r\ncp->regs = regs;\r\ncp_stop_hw(cp);\r\naddr_len = read_eeprom (regs, 0, 8) == 0x8129 ? 8 : 6;\r\nfor (i = 0; i < 3; i++)\r\n((__le16 *) (dev->dev_addr))[i] =\r\ncpu_to_le16(read_eeprom (regs, i + 7, addr_len));\r\ndev->netdev_ops = &cp_netdev_ops;\r\nnetif_napi_add(dev, &cp->napi, cp_rx_poll, 16);\r\ndev->ethtool_ops = &cp_ethtool_ops;\r\ndev->watchdog_timeo = TX_TIMEOUT;\r\ndev->features |= NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;\r\nif (pci_using_dac)\r\ndev->features |= NETIF_F_HIGHDMA;\r\ndev->hw_features |= NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO |\r\nNETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;\r\ndev->vlan_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO |\r\nNETIF_F_HIGHDMA;\r\nrc = register_netdev(dev);\r\nif (rc)\r\ngoto err_out_iomap;\r\nnetdev_info(dev, "RTL-8139C+ at 0x%p, %pM, IRQ %d\n",\r\nregs, dev->dev_addr, pdev->irq);\r\npci_set_drvdata(pdev, dev);\r\npci_set_master(pdev);\r\nif (cp->wol_enabled)\r\ncp_set_d3_state (cp);\r\nreturn 0;\r\nerr_out_iomap:\r\niounmap(regs);\r\nerr_out_res:\r\npci_release_regions(pdev);\r\nerr_out_mwi:\r\npci_clear_mwi(pdev);\r\nerr_out_disable:\r\npci_disable_device(pdev);\r\nerr_out_free:\r\nfree_netdev(dev);\r\nreturn rc;\r\n}\r\nstatic void cp_remove_one (struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunregister_netdev(dev);\r\niounmap(cp->regs);\r\nif (cp->wol_enabled)\r\npci_set_power_state (pdev, PCI_D0);\r\npci_release_regions(pdev);\r\npci_clear_mwi(pdev);\r\npci_disable_device(pdev);\r\nfree_netdev(dev);\r\n}\r\nstatic int cp_suspend (struct pci_dev *pdev, pm_message_t state)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nif (!netif_running(dev))\r\nreturn 0;\r\nnetif_device_detach (dev);\r\nnetif_stop_queue (dev);\r\nspin_lock_irqsave (&cp->lock, flags);\r\ncpw16 (IntrMask, 0);\r\ncpw8 (Cmd, cpr8 (Cmd) & (~RxOn | ~TxOn));\r\nspin_unlock_irqrestore (&cp->lock, flags);\r\npci_save_state(pdev);\r\npci_enable_wake(pdev, pci_choose_state(pdev, state), cp->wol_enabled);\r\npci_set_power_state(pdev, pci_choose_state(pdev, state));\r\nreturn 0;\r\n}\r\nstatic int cp_resume (struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata (pdev);\r\nstruct cp_private *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nif (!netif_running(dev))\r\nreturn 0;\r\nnetif_device_attach (dev);\r\npci_set_power_state(pdev, PCI_D0);\r\npci_restore_state(pdev);\r\npci_enable_wake(pdev, PCI_D0, 0);\r\ncp_init_rings_index (cp);\r\ncp_init_hw (cp);\r\ncp_enable_irq(cp);\r\nnetif_start_queue (dev);\r\nspin_lock_irqsave (&cp->lock, flags);\r\nmii_check_media(&cp->mii_if, netif_msg_link(cp), false);\r\nspin_unlock_irqrestore (&cp->lock, flags);\r\nreturn 0;\r\n}
