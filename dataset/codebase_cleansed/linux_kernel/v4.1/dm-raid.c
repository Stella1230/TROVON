static char *raid10_md_layout_to_format(int layout)\r\n{\r\nif ((layout & 0x10000) && (layout & 0x20000))\r\nreturn "offset";\r\nif ((layout & 0xFF) > 1)\r\nreturn "near";\r\nreturn "far";\r\n}\r\nstatic unsigned raid10_md_layout_to_copies(int layout)\r\n{\r\nif ((layout & 0xFF) > 1)\r\nreturn layout & 0xFF;\r\nreturn (layout >> 8) & 0xFF;\r\n}\r\nstatic int raid10_format_to_md_layout(char *format, unsigned copies)\r\n{\r\nunsigned n = 1, f = 1;\r\nif (!strcmp("near", format))\r\nn = copies;\r\nelse\r\nf = copies;\r\nif (!strcmp("offset", format))\r\nreturn 0x30000 | (f << 8) | n;\r\nif (!strcmp("far", format))\r\nreturn 0x20000 | (f << 8) | n;\r\nreturn (f << 8) | n;\r\n}\r\nstatic struct raid_type *get_raid_type(char *name)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(raid_types); i++)\r\nif (!strcmp(raid_types[i].name, name))\r\nreturn &raid_types[i];\r\nreturn NULL;\r\n}\r\nstatic struct raid_set *context_alloc(struct dm_target *ti, struct raid_type *raid_type, unsigned raid_devs)\r\n{\r\nunsigned i;\r\nstruct raid_set *rs;\r\nif (raid_devs <= raid_type->parity_devs) {\r\nti->error = "Insufficient number of devices";\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nrs = kzalloc(sizeof(*rs) + raid_devs * sizeof(rs->dev[0]), GFP_KERNEL);\r\nif (!rs) {\r\nti->error = "Cannot allocate raid context";\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nmddev_init(&rs->md);\r\nrs->ti = ti;\r\nrs->raid_type = raid_type;\r\nrs->md.raid_disks = raid_devs;\r\nrs->md.level = raid_type->level;\r\nrs->md.new_level = rs->md.level;\r\nrs->md.layout = raid_type->algorithm;\r\nrs->md.new_layout = rs->md.layout;\r\nrs->md.delta_disks = 0;\r\nrs->md.recovery_cp = 0;\r\nfor (i = 0; i < raid_devs; i++)\r\nmd_rdev_init(&rs->dev[i].rdev);\r\nreturn rs;\r\n}\r\nstatic void context_free(struct raid_set *rs)\r\n{\r\nint i;\r\nfor (i = 0; i < rs->md.raid_disks; i++) {\r\nif (rs->dev[i].meta_dev)\r\ndm_put_device(rs->ti, rs->dev[i].meta_dev);\r\nmd_rdev_clear(&rs->dev[i].rdev);\r\nif (rs->dev[i].data_dev)\r\ndm_put_device(rs->ti, rs->dev[i].data_dev);\r\n}\r\nkfree(rs);\r\n}\r\nstatic int dev_parms(struct raid_set *rs, char **argv)\r\n{\r\nint i;\r\nint rebuild = 0;\r\nint metadata_available = 0;\r\nint ret = 0;\r\nfor (i = 0; i < rs->md.raid_disks; i++, argv += 2) {\r\nrs->dev[i].rdev.raid_disk = i;\r\nrs->dev[i].meta_dev = NULL;\r\nrs->dev[i].data_dev = NULL;\r\nrs->dev[i].rdev.data_offset = 0;\r\nrs->dev[i].rdev.mddev = &rs->md;\r\nif (strcmp(argv[0], "-")) {\r\nret = dm_get_device(rs->ti, argv[0],\r\ndm_table_get_mode(rs->ti->table),\r\n&rs->dev[i].meta_dev);\r\nrs->ti->error = "RAID metadata device lookup failure";\r\nif (ret)\r\nreturn ret;\r\nrs->dev[i].rdev.sb_page = alloc_page(GFP_KERNEL);\r\nif (!rs->dev[i].rdev.sb_page)\r\nreturn -ENOMEM;\r\n}\r\nif (!strcmp(argv[1], "-")) {\r\nif (!test_bit(In_sync, &rs->dev[i].rdev.flags) &&\r\n(!rs->dev[i].rdev.recovery_offset)) {\r\nrs->ti->error = "Drive designated for rebuild not specified";\r\nreturn -EINVAL;\r\n}\r\nrs->ti->error = "No data device supplied with metadata device";\r\nif (rs->dev[i].meta_dev)\r\nreturn -EINVAL;\r\ncontinue;\r\n}\r\nret = dm_get_device(rs->ti, argv[1],\r\ndm_table_get_mode(rs->ti->table),\r\n&rs->dev[i].data_dev);\r\nif (ret) {\r\nrs->ti->error = "RAID device lookup failure";\r\nreturn ret;\r\n}\r\nif (rs->dev[i].meta_dev) {\r\nmetadata_available = 1;\r\nrs->dev[i].rdev.meta_bdev = rs->dev[i].meta_dev->bdev;\r\n}\r\nrs->dev[i].rdev.bdev = rs->dev[i].data_dev->bdev;\r\nlist_add(&rs->dev[i].rdev.same_set, &rs->md.disks);\r\nif (!test_bit(In_sync, &rs->dev[i].rdev.flags))\r\nrebuild++;\r\n}\r\nif (metadata_available) {\r\nrs->md.external = 0;\r\nrs->md.persistent = 1;\r\nrs->md.major_version = 2;\r\n} else if (rebuild && !rs->md.recovery_cp) {\r\nDMERR("Unable to rebuild drive while array is not in-sync");\r\nrs->ti->error = "RAID device lookup failure";\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int validate_region_size(struct raid_set *rs, unsigned long region_size)\r\n{\r\nunsigned long min_region_size = rs->ti->len / (1 << 21);\r\nif (!region_size) {\r\nif (min_region_size > (1 << 13)) {\r\nif (min_region_size & (min_region_size - 1))\r\nregion_size = 1 << fls(region_size);\r\nDMINFO("Choosing default region size of %lu sectors",\r\nregion_size);\r\n} else {\r\nDMINFO("Choosing default region size of 4MiB");\r\nregion_size = 1 << 13;\r\n}\r\n} else {\r\nif (region_size > rs->ti->len) {\r\nrs->ti->error = "Supplied region size is too large";\r\nreturn -EINVAL;\r\n}\r\nif (region_size < min_region_size) {\r\nDMERR("Supplied region_size (%lu sectors) below minimum (%lu)",\r\nregion_size, min_region_size);\r\nrs->ti->error = "Supplied region size is too small";\r\nreturn -EINVAL;\r\n}\r\nif (!is_power_of_2(region_size)) {\r\nrs->ti->error = "Region size is not a power of 2";\r\nreturn -EINVAL;\r\n}\r\nif (region_size < rs->md.chunk_sectors) {\r\nrs->ti->error = "Region size is smaller than the chunk size";\r\nreturn -EINVAL;\r\n}\r\n}\r\nrs->md.bitmap_info.chunksize = (region_size << 9);\r\nreturn 0;\r\n}\r\nstatic int validate_raid_redundancy(struct raid_set *rs)\r\n{\r\nunsigned i, rebuild_cnt = 0;\r\nunsigned rebuilds_per_group = 0, copies, d;\r\nunsigned group_size, last_group_start;\r\nfor (i = 0; i < rs->md.raid_disks; i++)\r\nif (!test_bit(In_sync, &rs->dev[i].rdev.flags) ||\r\n!rs->dev[i].rdev.sb_page)\r\nrebuild_cnt++;\r\nswitch (rs->raid_type->level) {\r\ncase 1:\r\nif (rebuild_cnt >= rs->md.raid_disks)\r\ngoto too_many;\r\nbreak;\r\ncase 4:\r\ncase 5:\r\ncase 6:\r\nif (rebuild_cnt > rs->raid_type->parity_devs)\r\ngoto too_many;\r\nbreak;\r\ncase 10:\r\ncopies = raid10_md_layout_to_copies(rs->md.layout);\r\nif (rebuild_cnt < copies)\r\nbreak;\r\nif (!strcmp("near", raid10_md_layout_to_format(rs->md.layout))) {\r\nfor (i = 0; i < rs->md.raid_disks * copies; i++) {\r\nif (!(i % copies))\r\nrebuilds_per_group = 0;\r\nd = i % rs->md.raid_disks;\r\nif ((!rs->dev[d].rdev.sb_page ||\r\n!test_bit(In_sync, &rs->dev[d].rdev.flags)) &&\r\n(++rebuilds_per_group >= copies))\r\ngoto too_many;\r\n}\r\nbreak;\r\n}\r\ngroup_size = (rs->md.raid_disks / copies);\r\nlast_group_start = (rs->md.raid_disks / group_size) - 1;\r\nlast_group_start *= group_size;\r\nfor (i = 0; i < rs->md.raid_disks; i++) {\r\nif (!(i % copies) && !(i > last_group_start))\r\nrebuilds_per_group = 0;\r\nif ((!rs->dev[i].rdev.sb_page ||\r\n!test_bit(In_sync, &rs->dev[i].rdev.flags)) &&\r\n(++rebuilds_per_group >= copies))\r\ngoto too_many;\r\n}\r\nbreak;\r\ndefault:\r\nif (rebuild_cnt)\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\ntoo_many:\r\nreturn -EINVAL;\r\n}\r\nstatic int parse_raid_params(struct raid_set *rs, char **argv,\r\nunsigned num_raid_params)\r\n{\r\nchar *raid10_format = "near";\r\nunsigned raid10_copies = 2;\r\nunsigned i;\r\nunsigned long value, region_size = 0;\r\nsector_t sectors_per_dev = rs->ti->len;\r\nsector_t max_io_len;\r\nchar *key;\r\nif ((kstrtoul(argv[0], 10, &value) < 0)) {\r\nrs->ti->error = "Bad chunk size";\r\nreturn -EINVAL;\r\n} else if (rs->raid_type->level == 1) {\r\nif (value)\r\nDMERR("Ignoring chunk size parameter for RAID 1");\r\nvalue = 0;\r\n} else if (!is_power_of_2(value)) {\r\nrs->ti->error = "Chunk size must be a power of 2";\r\nreturn -EINVAL;\r\n} else if (value < 8) {\r\nrs->ti->error = "Chunk size value is too small";\r\nreturn -EINVAL;\r\n}\r\nrs->md.new_chunk_sectors = rs->md.chunk_sectors = value;\r\nargv++;\r\nnum_raid_params--;\r\nfor (i = 0; i < rs->md.raid_disks; i++) {\r\nset_bit(In_sync, &rs->dev[i].rdev.flags);\r\nrs->dev[i].rdev.recovery_offset = MaxSector;\r\n}\r\nfor (i = 0; i < num_raid_params; i++) {\r\nif (!strcasecmp(argv[i], "nosync")) {\r\nrs->md.recovery_cp = MaxSector;\r\nrs->print_flags |= DMPF_NOSYNC;\r\ncontinue;\r\n}\r\nif (!strcasecmp(argv[i], "sync")) {\r\nrs->md.recovery_cp = 0;\r\nrs->print_flags |= DMPF_SYNC;\r\ncontinue;\r\n}\r\nif ((i + 1) >= num_raid_params) {\r\nrs->ti->error = "Wrong number of raid parameters given";\r\nreturn -EINVAL;\r\n}\r\nkey = argv[i++];\r\nif (!strcasecmp(key, "raid10_format")) {\r\nif (rs->raid_type->level != 10) {\r\nrs->ti->error = "'raid10_format' is an invalid parameter for this RAID type";\r\nreturn -EINVAL;\r\n}\r\nif (strcmp("near", argv[i]) &&\r\nstrcmp("far", argv[i]) &&\r\nstrcmp("offset", argv[i])) {\r\nrs->ti->error = "Invalid 'raid10_format' value given";\r\nreturn -EINVAL;\r\n}\r\nraid10_format = argv[i];\r\nrs->print_flags |= DMPF_RAID10_FORMAT;\r\ncontinue;\r\n}\r\nif (kstrtoul(argv[i], 10, &value) < 0) {\r\nrs->ti->error = "Bad numerical argument given in raid params";\r\nreturn -EINVAL;\r\n}\r\nif (!strcasecmp(key, "rebuild")) {\r\nif (value >= rs->md.raid_disks) {\r\nrs->ti->error = "Invalid rebuild index given";\r\nreturn -EINVAL;\r\n}\r\nclear_bit(In_sync, &rs->dev[value].rdev.flags);\r\nrs->dev[value].rdev.recovery_offset = 0;\r\nrs->print_flags |= DMPF_REBUILD;\r\n} else if (!strcasecmp(key, "write_mostly")) {\r\nif (rs->raid_type->level != 1) {\r\nrs->ti->error = "write_mostly option is only valid for RAID1";\r\nreturn -EINVAL;\r\n}\r\nif (value >= rs->md.raid_disks) {\r\nrs->ti->error = "Invalid write_mostly drive index given";\r\nreturn -EINVAL;\r\n}\r\nset_bit(WriteMostly, &rs->dev[value].rdev.flags);\r\n} else if (!strcasecmp(key, "max_write_behind")) {\r\nif (rs->raid_type->level != 1) {\r\nrs->ti->error = "max_write_behind option is only valid for RAID1";\r\nreturn -EINVAL;\r\n}\r\nrs->print_flags |= DMPF_MAX_WRITE_BEHIND;\r\nvalue /= 2;\r\nif (value > COUNTER_MAX) {\r\nrs->ti->error = "Max write-behind limit out of range";\r\nreturn -EINVAL;\r\n}\r\nrs->md.bitmap_info.max_write_behind = value;\r\n} else if (!strcasecmp(key, "daemon_sleep")) {\r\nrs->print_flags |= DMPF_DAEMON_SLEEP;\r\nif (!value || (value > MAX_SCHEDULE_TIMEOUT)) {\r\nrs->ti->error = "daemon sleep period out of range";\r\nreturn -EINVAL;\r\n}\r\nrs->md.bitmap_info.daemon_sleep = value;\r\n} else if (!strcasecmp(key, "stripe_cache")) {\r\nrs->print_flags |= DMPF_STRIPE_CACHE;\r\nvalue /= 2;\r\nif ((rs->raid_type->level != 5) &&\r\n(rs->raid_type->level != 6)) {\r\nrs->ti->error = "Inappropriate argument: stripe_cache";\r\nreturn -EINVAL;\r\n}\r\nif (raid5_set_cache_size(&rs->md, (int)value)) {\r\nrs->ti->error = "Bad stripe_cache size";\r\nreturn -EINVAL;\r\n}\r\n} else if (!strcasecmp(key, "min_recovery_rate")) {\r\nrs->print_flags |= DMPF_MIN_RECOVERY_RATE;\r\nif (value > INT_MAX) {\r\nrs->ti->error = "min_recovery_rate out of range";\r\nreturn -EINVAL;\r\n}\r\nrs->md.sync_speed_min = (int)value;\r\n} else if (!strcasecmp(key, "max_recovery_rate")) {\r\nrs->print_flags |= DMPF_MAX_RECOVERY_RATE;\r\nif (value > INT_MAX) {\r\nrs->ti->error = "max_recovery_rate out of range";\r\nreturn -EINVAL;\r\n}\r\nrs->md.sync_speed_max = (int)value;\r\n} else if (!strcasecmp(key, "region_size")) {\r\nrs->print_flags |= DMPF_REGION_SIZE;\r\nregion_size = value;\r\n} else if (!strcasecmp(key, "raid10_copies") &&\r\n(rs->raid_type->level == 10)) {\r\nif ((value < 2) || (value > 0xFF)) {\r\nrs->ti->error = "Bad value for 'raid10_copies'";\r\nreturn -EINVAL;\r\n}\r\nrs->print_flags |= DMPF_RAID10_COPIES;\r\nraid10_copies = value;\r\n} else {\r\nDMERR("Unable to parse RAID parameter: %s", key);\r\nrs->ti->error = "Unable to parse RAID parameters";\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (validate_region_size(rs, region_size))\r\nreturn -EINVAL;\r\nif (rs->md.chunk_sectors)\r\nmax_io_len = rs->md.chunk_sectors;\r\nelse\r\nmax_io_len = region_size;\r\nif (dm_set_target_max_io_len(rs->ti, max_io_len))\r\nreturn -EINVAL;\r\nif (rs->raid_type->level == 10) {\r\nif (raid10_copies > rs->md.raid_disks) {\r\nrs->ti->error = "Not enough devices to satisfy specification";\r\nreturn -EINVAL;\r\n}\r\nif (strcmp("near", raid10_format) && (raid10_copies > 2)) {\r\nrs->ti->error = "Too many copies for given RAID10 format.";\r\nreturn -EINVAL;\r\n}\r\nsectors_per_dev = rs->ti->len * raid10_copies;\r\nsector_div(sectors_per_dev, rs->md.raid_disks);\r\nrs->md.layout = raid10_format_to_md_layout(raid10_format,\r\nraid10_copies);\r\nrs->md.new_layout = rs->md.layout;\r\n} else if ((rs->raid_type->level > 1) &&\r\nsector_div(sectors_per_dev,\r\n(rs->md.raid_disks - rs->raid_type->parity_devs))) {\r\nrs->ti->error = "Target length not divisible by number of data devices";\r\nreturn -EINVAL;\r\n}\r\nrs->md.dev_sectors = sectors_per_dev;\r\nrs->md.persistent = 0;\r\nrs->md.external = 1;\r\nreturn 0;\r\n}\r\nstatic void do_table_event(struct work_struct *ws)\r\n{\r\nstruct raid_set *rs = container_of(ws, struct raid_set, md.event_work);\r\ndm_table_event(rs->ti->table);\r\n}\r\nstatic int raid_is_congested(struct dm_target_callbacks *cb, int bits)\r\n{\r\nstruct raid_set *rs = container_of(cb, struct raid_set, callbacks);\r\nreturn mddev_congested(&rs->md, bits);\r\n}\r\nstatic int read_disk_sb(struct md_rdev *rdev, int size)\r\n{\r\nBUG_ON(!rdev->sb_page);\r\nif (rdev->sb_loaded)\r\nreturn 0;\r\nif (!sync_page_io(rdev, 0, size, rdev->sb_page, READ, 1)) {\r\nDMERR("Failed to read superblock of device at position %d",\r\nrdev->raid_disk);\r\nmd_error(rdev->mddev, rdev);\r\nreturn -EINVAL;\r\n}\r\nrdev->sb_loaded = 1;\r\nreturn 0;\r\n}\r\nstatic void super_sync(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nint i;\r\nuint64_t failed_devices;\r\nstruct dm_raid_superblock *sb;\r\nstruct raid_set *rs = container_of(mddev, struct raid_set, md);\r\nsb = page_address(rdev->sb_page);\r\nfailed_devices = le64_to_cpu(sb->failed_devices);\r\nfor (i = 0; i < mddev->raid_disks; i++)\r\nif (!rs->dev[i].data_dev ||\r\ntest_bit(Faulty, &(rs->dev[i].rdev.flags)))\r\nfailed_devices |= (1ULL << i);\r\nmemset(sb + 1, 0, rdev->sb_size - sizeof(*sb));\r\nsb->magic = cpu_to_le32(DM_RAID_MAGIC);\r\nsb->features = cpu_to_le32(0);\r\nsb->num_devices = cpu_to_le32(mddev->raid_disks);\r\nsb->array_position = cpu_to_le32(rdev->raid_disk);\r\nsb->events = cpu_to_le64(mddev->events);\r\nsb->failed_devices = cpu_to_le64(failed_devices);\r\nsb->disk_recovery_offset = cpu_to_le64(rdev->recovery_offset);\r\nsb->array_resync_offset = cpu_to_le64(mddev->recovery_cp);\r\nsb->level = cpu_to_le32(mddev->level);\r\nsb->layout = cpu_to_le32(mddev->layout);\r\nsb->stripe_sectors = cpu_to_le32(mddev->chunk_sectors);\r\n}\r\nstatic int super_load(struct md_rdev *rdev, struct md_rdev *refdev)\r\n{\r\nint ret;\r\nstruct dm_raid_superblock *sb;\r\nstruct dm_raid_superblock *refsb;\r\nuint64_t events_sb, events_refsb;\r\nrdev->sb_start = 0;\r\nrdev->sb_size = bdev_logical_block_size(rdev->meta_bdev);\r\nif (rdev->sb_size < sizeof(*sb) || rdev->sb_size > PAGE_SIZE) {\r\nDMERR("superblock size of a logical block is no longer valid");\r\nreturn -EINVAL;\r\n}\r\nret = read_disk_sb(rdev, rdev->sb_size);\r\nif (ret)\r\nreturn ret;\r\nsb = page_address(rdev->sb_page);\r\nif ((sb->magic != cpu_to_le32(DM_RAID_MAGIC)) ||\r\n(!test_bit(In_sync, &rdev->flags) && !rdev->recovery_offset)) {\r\nsuper_sync(rdev->mddev, rdev);\r\nset_bit(FirstUse, &rdev->flags);\r\nset_bit(MD_CHANGE_DEVS, &rdev->mddev->flags);\r\nreturn refdev ? 0 : 1;\r\n}\r\nif (!refdev)\r\nreturn 1;\r\nevents_sb = le64_to_cpu(sb->events);\r\nrefsb = page_address(refdev->sb_page);\r\nevents_refsb = le64_to_cpu(refsb->events);\r\nreturn (events_sb > events_refsb) ? 1 : 0;\r\n}\r\nstatic int super_init_validation(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nint role;\r\nstruct raid_set *rs = container_of(mddev, struct raid_set, md);\r\nuint64_t events_sb;\r\nuint64_t failed_devices;\r\nstruct dm_raid_superblock *sb;\r\nuint32_t new_devs = 0;\r\nuint32_t rebuilds = 0;\r\nstruct md_rdev *r;\r\nstruct dm_raid_superblock *sb2;\r\nsb = page_address(rdev->sb_page);\r\nevents_sb = le64_to_cpu(sb->events);\r\nfailed_devices = le64_to_cpu(sb->failed_devices);\r\nmddev->events = events_sb ? : 1;\r\nif (le32_to_cpu(sb->level) != mddev->level) {\r\nDMERR("Reshaping arrays not yet supported. (RAID level change)");\r\nreturn -EINVAL;\r\n}\r\nif (le32_to_cpu(sb->layout) != mddev->layout) {\r\nDMERR("Reshaping arrays not yet supported. (RAID layout change)");\r\nDMERR(" 0x%X vs 0x%X", le32_to_cpu(sb->layout), mddev->layout);\r\nDMERR(" Old layout: %s w/ %d copies",\r\nraid10_md_layout_to_format(le32_to_cpu(sb->layout)),\r\nraid10_md_layout_to_copies(le32_to_cpu(sb->layout)));\r\nDMERR(" New layout: %s w/ %d copies",\r\nraid10_md_layout_to_format(mddev->layout),\r\nraid10_md_layout_to_copies(mddev->layout));\r\nreturn -EINVAL;\r\n}\r\nif (le32_to_cpu(sb->stripe_sectors) != mddev->chunk_sectors) {\r\nDMERR("Reshaping arrays not yet supported. (stripe sectors change)");\r\nreturn -EINVAL;\r\n}\r\nif ((rs->raid_type->level != 1) &&\r\n(le32_to_cpu(sb->num_devices) != mddev->raid_disks)) {\r\nDMERR("Reshaping arrays not yet supported. (device count change)");\r\nreturn -EINVAL;\r\n}\r\nif (!(rs->print_flags & (DMPF_SYNC | DMPF_NOSYNC)))\r\nmddev->recovery_cp = le64_to_cpu(sb->array_resync_offset);\r\nrdev_for_each(r, mddev) {\r\nif (!test_bit(In_sync, &r->flags)) {\r\nDMINFO("Device %d specified for rebuild: "\r\n"Clearing superblock", r->raid_disk);\r\nrebuilds++;\r\n} else if (test_bit(FirstUse, &r->flags))\r\nnew_devs++;\r\n}\r\nif (!rebuilds) {\r\nif (new_devs == mddev->raid_disks) {\r\nDMINFO("Superblocks created for new array");\r\nset_bit(MD_ARRAY_FIRST_USE, &mddev->flags);\r\n} else if (new_devs) {\r\nDMERR("New device injected "\r\n"into existing array without 'rebuild' "\r\n"parameter specified");\r\nreturn -EINVAL;\r\n}\r\n} else if (new_devs) {\r\nDMERR("'rebuild' devices cannot be "\r\n"injected into an array with other first-time devices");\r\nreturn -EINVAL;\r\n} else if (mddev->recovery_cp != MaxSector) {\r\nDMERR("'rebuild' specified while array is not in-sync");\r\nreturn -EINVAL;\r\n}\r\nrdev_for_each(r, mddev) {\r\nif (!r->sb_page)\r\ncontinue;\r\nsb2 = page_address(r->sb_page);\r\nsb2->failed_devices = 0;\r\nif (!test_bit(FirstUse, &r->flags) && (r->raid_disk >= 0)) {\r\nrole = le32_to_cpu(sb2->array_position);\r\nif (role != r->raid_disk) {\r\nif (rs->raid_type->level != 1) {\r\nrs->ti->error = "Cannot change device "\r\n"positions in RAID array";\r\nreturn -EINVAL;\r\n}\r\nDMINFO("RAID1 device #%d now at position #%d",\r\nrole, r->raid_disk);\r\n}\r\nif (failed_devices & (1 << role))\r\nset_bit(Faulty, &r->flags);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int super_validate(struct mddev *mddev, struct md_rdev *rdev)\r\n{\r\nstruct dm_raid_superblock *sb = page_address(rdev->sb_page);\r\nif (!mddev->events && super_init_validation(mddev, rdev))\r\nreturn -EINVAL;\r\nmddev->bitmap_info.offset = 4096 >> 9;\r\nrdev->mddev->bitmap_info.default_offset = 4096 >> 9;\r\nif (!test_bit(FirstUse, &rdev->flags)) {\r\nrdev->recovery_offset = le64_to_cpu(sb->disk_recovery_offset);\r\nif (rdev->recovery_offset != MaxSector)\r\nclear_bit(In_sync, &rdev->flags);\r\n}\r\nif (test_bit(Faulty, &rdev->flags)) {\r\nclear_bit(Faulty, &rdev->flags);\r\nclear_bit(In_sync, &rdev->flags);\r\nrdev->saved_raid_disk = rdev->raid_disk;\r\nrdev->recovery_offset = 0;\r\n}\r\nclear_bit(FirstUse, &rdev->flags);\r\nreturn 0;\r\n}\r\nstatic int analyse_superblocks(struct dm_target *ti, struct raid_set *rs)\r\n{\r\nint ret;\r\nstruct raid_dev *dev;\r\nstruct md_rdev *rdev, *tmp, *freshest;\r\nstruct mddev *mddev = &rs->md;\r\nfreshest = NULL;\r\nrdev_for_each_safe(rdev, tmp, mddev) {\r\nif (rs->print_flags & DMPF_SYNC)\r\ncontinue;\r\nif (!rdev->meta_bdev)\r\ncontinue;\r\nret = super_load(rdev, freshest);\r\nswitch (ret) {\r\ncase 1:\r\nfreshest = rdev;\r\nbreak;\r\ncase 0:\r\nbreak;\r\ndefault:\r\ndev = container_of(rdev, struct raid_dev, rdev);\r\nif (dev->meta_dev)\r\ndm_put_device(ti, dev->meta_dev);\r\ndev->meta_dev = NULL;\r\nrdev->meta_bdev = NULL;\r\nif (rdev->sb_page)\r\nput_page(rdev->sb_page);\r\nrdev->sb_page = NULL;\r\nrdev->sb_loaded = 0;\r\nif (dev->data_dev)\r\ndm_put_device(ti, dev->data_dev);\r\ndev->data_dev = NULL;\r\nrdev->bdev = NULL;\r\nlist_del(&rdev->same_set);\r\n}\r\n}\r\nif (!freshest)\r\nreturn 0;\r\nif (validate_raid_redundancy(rs)) {\r\nrs->ti->error = "Insufficient redundancy to activate array";\r\nreturn -EINVAL;\r\n}\r\nti->error = "Unable to assemble array: Invalid superblocks";\r\nif (super_validate(mddev, freshest))\r\nreturn -EINVAL;\r\nrdev_for_each(rdev, mddev)\r\nif ((rdev != freshest) && super_validate(mddev, rdev))\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic void configure_discard_support(struct dm_target *ti, struct raid_set *rs)\r\n{\r\nint i;\r\nbool raid456;\r\nti->discards_supported = false;\r\nraid456 = (rs->md.level == 4 || rs->md.level == 5 || rs->md.level == 6);\r\nfor (i = 0; i < rs->md.raid_disks; i++) {\r\nstruct request_queue *q;\r\nif (!rs->dev[i].rdev.bdev)\r\ncontinue;\r\nq = bdev_get_queue(rs->dev[i].rdev.bdev);\r\nif (!q || !blk_queue_discard(q))\r\nreturn;\r\nif (raid456) {\r\nif (!q->limits.discard_zeroes_data)\r\nreturn;\r\nif (!devices_handle_discard_safely) {\r\nDMERR("raid456 discard support disabled due to discard_zeroes_data uncertainty.");\r\nDMERR("Set dm-raid.devices_handle_discard_safely=Y to override.");\r\nreturn;\r\n}\r\n}\r\n}\r\nti->discards_supported = true;\r\nti->split_discard_bios = !!(rs->md.level == 1 || rs->md.level == 10);\r\nti->num_discard_bios = 1;\r\n}\r\nstatic int raid_ctr(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nint ret;\r\nstruct raid_type *rt;\r\nunsigned long num_raid_params, num_raid_devs;\r\nstruct raid_set *rs = NULL;\r\nif (argc < 2) {\r\nti->error = "Too few arguments";\r\nreturn -EINVAL;\r\n}\r\nrt = get_raid_type(argv[0]);\r\nif (!rt) {\r\nti->error = "Unrecognised raid_type";\r\nreturn -EINVAL;\r\n}\r\nargc--;\r\nargv++;\r\nif (kstrtoul(argv[0], 10, &num_raid_params) < 0) {\r\nti->error = "Cannot understand number of RAID parameters";\r\nreturn -EINVAL;\r\n}\r\nargc--;\r\nargv++;\r\nif (num_raid_params >= argc) {\r\nti->error = "Arguments do not agree with counts given";\r\nreturn -EINVAL;\r\n}\r\nif ((kstrtoul(argv[num_raid_params], 10, &num_raid_devs) < 0) ||\r\n(num_raid_devs >= INT_MAX)) {\r\nti->error = "Cannot understand number of raid devices";\r\nreturn -EINVAL;\r\n}\r\nargc -= num_raid_params + 1;\r\nif (argc != (num_raid_devs * 2)) {\r\nti->error = "Supplied RAID devices does not match the count given";\r\nreturn -EINVAL;\r\n}\r\nrs = context_alloc(ti, rt, (unsigned)num_raid_devs);\r\nif (IS_ERR(rs))\r\nreturn PTR_ERR(rs);\r\nret = parse_raid_params(rs, argv, (unsigned)num_raid_params);\r\nif (ret)\r\ngoto bad;\r\nargv += num_raid_params + 1;\r\nret = dev_parms(rs, argv);\r\nif (ret)\r\ngoto bad;\r\nrs->md.sync_super = super_sync;\r\nret = analyse_superblocks(ti, rs);\r\nif (ret)\r\ngoto bad;\r\nINIT_WORK(&rs->md.event_work, do_table_event);\r\nti->private = rs;\r\nti->num_flush_bios = 1;\r\nconfigure_discard_support(ti, rs);\r\nmutex_lock(&rs->md.reconfig_mutex);\r\nret = md_run(&rs->md);\r\nrs->md.in_sync = 0;\r\nmutex_unlock(&rs->md.reconfig_mutex);\r\nif (ret) {\r\nti->error = "Fail to run raid array";\r\ngoto bad;\r\n}\r\nif (ti->len != rs->md.array_sectors) {\r\nti->error = "Array size does not match requested target length";\r\nret = -EINVAL;\r\ngoto size_mismatch;\r\n}\r\nrs->callbacks.congested_fn = raid_is_congested;\r\ndm_table_add_target_callbacks(ti->table, &rs->callbacks);\r\nmddev_suspend(&rs->md);\r\nreturn 0;\r\nsize_mismatch:\r\nmd_stop(&rs->md);\r\nbad:\r\ncontext_free(rs);\r\nreturn ret;\r\n}\r\nstatic void raid_dtr(struct dm_target *ti)\r\n{\r\nstruct raid_set *rs = ti->private;\r\nlist_del_init(&rs->callbacks.list);\r\nmd_stop(&rs->md);\r\ncontext_free(rs);\r\n}\r\nstatic int raid_map(struct dm_target *ti, struct bio *bio)\r\n{\r\nstruct raid_set *rs = ti->private;\r\nstruct mddev *mddev = &rs->md;\r\nmddev->pers->make_request(mddev, bio);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\nstatic const char *decipher_sync_action(struct mddev *mddev)\r\n{\r\nif (test_bit(MD_RECOVERY_FROZEN, &mddev->recovery))\r\nreturn "frozen";\r\nif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery) ||\r\n(!mddev->ro && test_bit(MD_RECOVERY_NEEDED, &mddev->recovery))) {\r\nif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\r\nreturn "reshape";\r\nif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\r\nif (!test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))\r\nreturn "resync";\r\nelse if (test_bit(MD_RECOVERY_CHECK, &mddev->recovery))\r\nreturn "check";\r\nreturn "repair";\r\n}\r\nif (test_bit(MD_RECOVERY_RECOVER, &mddev->recovery))\r\nreturn "recover";\r\n}\r\nreturn "idle";\r\n}\r\nstatic void raid_status(struct dm_target *ti, status_type_t type,\r\nunsigned status_flags, char *result, unsigned maxlen)\r\n{\r\nstruct raid_set *rs = ti->private;\r\nunsigned raid_param_cnt = 1;\r\nunsigned sz = 0;\r\nint i, array_in_sync = 0;\r\nsector_t sync;\r\nswitch (type) {\r\ncase STATUSTYPE_INFO:\r\nDMEMIT("%s %d ", rs->raid_type->name, rs->md.raid_disks);\r\nif (test_bit(MD_RECOVERY_RUNNING, &rs->md.recovery))\r\nsync = rs->md.curr_resync_completed;\r\nelse\r\nsync = rs->md.recovery_cp;\r\nif (sync >= rs->md.resync_max_sectors) {\r\narray_in_sync = 1;\r\nsync = rs->md.resync_max_sectors;\r\n} else if (test_bit(MD_RECOVERY_REQUESTED, &rs->md.recovery)) {\r\narray_in_sync = 1;\r\n} else {\r\nfor (i = 0; i < rs->md.raid_disks; i++)\r\nif (!test_bit(In_sync, &rs->dev[i].rdev.flags))\r\narray_in_sync = 1;\r\n}\r\nfor (i = 0; i < rs->md.raid_disks; i++) {\r\nif (test_bit(Faulty, &rs->dev[i].rdev.flags))\r\nDMEMIT("D");\r\nelse if (!array_in_sync ||\r\n!test_bit(In_sync, &rs->dev[i].rdev.flags))\r\nDMEMIT("a");\r\nelse\r\nDMEMIT("A");\r\n}\r\nDMEMIT(" %llu/%llu",\r\n(unsigned long long) sync,\r\n(unsigned long long) rs->md.resync_max_sectors);\r\nDMEMIT(" %s", decipher_sync_action(&rs->md));\r\nDMEMIT(" %llu",\r\n(strcmp(rs->md.last_sync_action, "check")) ? 0 :\r\n(unsigned long long)\r\natomic64_read(&rs->md.resync_mismatches));\r\nbreak;\r\ncase STATUSTYPE_TABLE:\r\nfor (i = 0; i < rs->md.raid_disks; i++) {\r\nif ((rs->print_flags & DMPF_REBUILD) &&\r\nrs->dev[i].data_dev &&\r\n!test_bit(In_sync, &rs->dev[i].rdev.flags))\r\nraid_param_cnt += 2;\r\nif (rs->dev[i].data_dev &&\r\ntest_bit(WriteMostly, &rs->dev[i].rdev.flags))\r\nraid_param_cnt += 2;\r\n}\r\nraid_param_cnt += (hweight32(rs->print_flags & ~DMPF_REBUILD) * 2);\r\nif (rs->print_flags & (DMPF_SYNC | DMPF_NOSYNC))\r\nraid_param_cnt--;\r\nDMEMIT("%s %u %u", rs->raid_type->name,\r\nraid_param_cnt, rs->md.chunk_sectors);\r\nif ((rs->print_flags & DMPF_SYNC) &&\r\n(rs->md.recovery_cp == MaxSector))\r\nDMEMIT(" sync");\r\nif (rs->print_flags & DMPF_NOSYNC)\r\nDMEMIT(" nosync");\r\nfor (i = 0; i < rs->md.raid_disks; i++)\r\nif ((rs->print_flags & DMPF_REBUILD) &&\r\nrs->dev[i].data_dev &&\r\n!test_bit(In_sync, &rs->dev[i].rdev.flags))\r\nDMEMIT(" rebuild %u", i);\r\nif (rs->print_flags & DMPF_DAEMON_SLEEP)\r\nDMEMIT(" daemon_sleep %lu",\r\nrs->md.bitmap_info.daemon_sleep);\r\nif (rs->print_flags & DMPF_MIN_RECOVERY_RATE)\r\nDMEMIT(" min_recovery_rate %d", rs->md.sync_speed_min);\r\nif (rs->print_flags & DMPF_MAX_RECOVERY_RATE)\r\nDMEMIT(" max_recovery_rate %d", rs->md.sync_speed_max);\r\nfor (i = 0; i < rs->md.raid_disks; i++)\r\nif (rs->dev[i].data_dev &&\r\ntest_bit(WriteMostly, &rs->dev[i].rdev.flags))\r\nDMEMIT(" write_mostly %u", i);\r\nif (rs->print_flags & DMPF_MAX_WRITE_BEHIND)\r\nDMEMIT(" max_write_behind %lu",\r\nrs->md.bitmap_info.max_write_behind);\r\nif (rs->print_flags & DMPF_STRIPE_CACHE) {\r\nstruct r5conf *conf = rs->md.private;\r\nDMEMIT(" stripe_cache %d",\r\nconf ? conf->max_nr_stripes * 2 : 0);\r\n}\r\nif (rs->print_flags & DMPF_REGION_SIZE)\r\nDMEMIT(" region_size %lu",\r\nrs->md.bitmap_info.chunksize >> 9);\r\nif (rs->print_flags & DMPF_RAID10_COPIES)\r\nDMEMIT(" raid10_copies %u",\r\nraid10_md_layout_to_copies(rs->md.layout));\r\nif (rs->print_flags & DMPF_RAID10_FORMAT)\r\nDMEMIT(" raid10_format %s",\r\nraid10_md_layout_to_format(rs->md.layout));\r\nDMEMIT(" %d", rs->md.raid_disks);\r\nfor (i = 0; i < rs->md.raid_disks; i++) {\r\nif (rs->dev[i].meta_dev)\r\nDMEMIT(" %s", rs->dev[i].meta_dev->name);\r\nelse\r\nDMEMIT(" -");\r\nif (rs->dev[i].data_dev)\r\nDMEMIT(" %s", rs->dev[i].data_dev->name);\r\nelse\r\nDMEMIT(" -");\r\n}\r\n}\r\n}\r\nstatic int raid_message(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nstruct raid_set *rs = ti->private;\r\nstruct mddev *mddev = &rs->md;\r\nif (!strcasecmp(argv[0], "reshape")) {\r\nDMERR("Reshape not supported.");\r\nreturn -EINVAL;\r\n}\r\nif (!mddev->pers || !mddev->pers->sync_request)\r\nreturn -EINVAL;\r\nif (!strcasecmp(argv[0], "frozen"))\r\nset_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\r\nelse\r\nclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\r\nif (!strcasecmp(argv[0], "idle") || !strcasecmp(argv[0], "frozen")) {\r\nif (mddev->sync_thread) {\r\nset_bit(MD_RECOVERY_INTR, &mddev->recovery);\r\nmd_reap_sync_thread(mddev);\r\n}\r\n} else if (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery) ||\r\ntest_bit(MD_RECOVERY_NEEDED, &mddev->recovery))\r\nreturn -EBUSY;\r\nelse if (!strcasecmp(argv[0], "resync"))\r\nset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\r\nelse if (!strcasecmp(argv[0], "recover")) {\r\nset_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\r\nset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\r\n} else {\r\nif (!strcasecmp(argv[0], "check"))\r\nset_bit(MD_RECOVERY_CHECK, &mddev->recovery);\r\nelse if (!!strcasecmp(argv[0], "repair"))\r\nreturn -EINVAL;\r\nset_bit(MD_RECOVERY_REQUESTED, &mddev->recovery);\r\nset_bit(MD_RECOVERY_SYNC, &mddev->recovery);\r\n}\r\nif (mddev->ro == 2) {\r\nmddev->ro = 0;\r\nif (!mddev->suspended)\r\nmd_wakeup_thread(mddev->sync_thread);\r\n}\r\nset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\r\nif (!mddev->suspended)\r\nmd_wakeup_thread(mddev->thread);\r\nreturn 0;\r\n}\r\nstatic int raid_iterate_devices(struct dm_target *ti,\r\niterate_devices_callout_fn fn, void *data)\r\n{\r\nstruct raid_set *rs = ti->private;\r\nunsigned i;\r\nint ret = 0;\r\nfor (i = 0; !ret && i < rs->md.raid_disks; i++)\r\nif (rs->dev[i].data_dev)\r\nret = fn(ti,\r\nrs->dev[i].data_dev,\r\n0,\r\nrs->md.dev_sectors,\r\ndata);\r\nreturn ret;\r\n}\r\nstatic void raid_io_hints(struct dm_target *ti, struct queue_limits *limits)\r\n{\r\nstruct raid_set *rs = ti->private;\r\nunsigned chunk_size = rs->md.chunk_sectors << 9;\r\nstruct r5conf *conf = rs->md.private;\r\nblk_limits_io_min(limits, chunk_size);\r\nblk_limits_io_opt(limits, chunk_size * (conf->raid_disks - conf->max_degraded));\r\n}\r\nstatic void raid_presuspend(struct dm_target *ti)\r\n{\r\nstruct raid_set *rs = ti->private;\r\nmd_stop_writes(&rs->md);\r\n}\r\nstatic void raid_postsuspend(struct dm_target *ti)\r\n{\r\nstruct raid_set *rs = ti->private;\r\nmddev_suspend(&rs->md);\r\n}\r\nstatic void attempt_restore_of_faulty_devices(struct raid_set *rs)\r\n{\r\nint i;\r\nuint64_t failed_devices, cleared_failed_devices = 0;\r\nunsigned long flags;\r\nstruct dm_raid_superblock *sb;\r\nstruct md_rdev *r;\r\nfor (i = 0; i < rs->md.raid_disks; i++) {\r\nr = &rs->dev[i].rdev;\r\nif (test_bit(Faulty, &r->flags) && r->sb_page &&\r\nsync_page_io(r, 0, r->sb_size, r->sb_page, READ, 1)) {\r\nDMINFO("Faulty %s device #%d has readable super block."\r\n" Attempting to revive it.",\r\nrs->raid_type->name, i);\r\nif ((r->raid_disk >= 0) &&\r\n(r->mddev->pers->hot_remove_disk(r->mddev, r) != 0))\r\ncontinue;\r\nr->raid_disk = i;\r\nr->saved_raid_disk = i;\r\nflags = r->flags;\r\nclear_bit(Faulty, &r->flags);\r\nclear_bit(WriteErrorSeen, &r->flags);\r\nclear_bit(In_sync, &r->flags);\r\nif (r->mddev->pers->hot_add_disk(r->mddev, r)) {\r\nr->raid_disk = -1;\r\nr->saved_raid_disk = -1;\r\nr->flags = flags;\r\n} else {\r\nr->recovery_offset = 0;\r\ncleared_failed_devices |= 1 << i;\r\n}\r\n}\r\n}\r\nif (cleared_failed_devices) {\r\nrdev_for_each(r, &rs->md) {\r\nsb = page_address(r->sb_page);\r\nfailed_devices = le64_to_cpu(sb->failed_devices);\r\nfailed_devices &= ~cleared_failed_devices;\r\nsb->failed_devices = cpu_to_le64(failed_devices);\r\n}\r\n}\r\n}\r\nstatic void raid_resume(struct dm_target *ti)\r\n{\r\nstruct raid_set *rs = ti->private;\r\nset_bit(MD_CHANGE_DEVS, &rs->md.flags);\r\nif (!rs->bitmap_loaded) {\r\nbitmap_load(&rs->md);\r\nrs->bitmap_loaded = 1;\r\n} else {\r\nattempt_restore_of_faulty_devices(rs);\r\n}\r\nclear_bit(MD_RECOVERY_FROZEN, &rs->md.recovery);\r\nmddev_resume(&rs->md);\r\n}\r\nstatic int __init dm_raid_init(void)\r\n{\r\nDMINFO("Loading target version %u.%u.%u",\r\nraid_target.version[0],\r\nraid_target.version[1],\r\nraid_target.version[2]);\r\nreturn dm_register_target(&raid_target);\r\n}\r\nstatic void __exit dm_raid_exit(void)\r\n{\r\ndm_unregister_target(&raid_target);\r\n}
