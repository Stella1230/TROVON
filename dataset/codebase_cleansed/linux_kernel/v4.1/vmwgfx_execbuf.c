static void vmw_resource_list_unreserve(struct list_head *list,\r\nbool backoff)\r\n{\r\nstruct vmw_resource_val_node *val;\r\nlist_for_each_entry(val, list, head) {\r\nstruct vmw_resource *res = val->res;\r\nstruct vmw_dma_buffer *new_backup =\r\nbackoff ? NULL : val->new_backup;\r\nif (unlikely(val->staged_bindings)) {\r\nif (!backoff) {\r\nvmw_context_binding_state_transfer\r\n(val->res, val->staged_bindings);\r\n}\r\nkfree(val->staged_bindings);\r\nval->staged_bindings = NULL;\r\n}\r\nvmw_resource_unreserve(res, new_backup,\r\nval->new_backup_offset);\r\nvmw_dmabuf_unreference(&val->new_backup);\r\n}\r\n}\r\nstatic int vmw_resource_val_add(struct vmw_sw_context *sw_context,\r\nstruct vmw_resource *res,\r\nstruct vmw_resource_val_node **p_node)\r\n{\r\nstruct vmw_resource_val_node *node;\r\nstruct drm_hash_item *hash;\r\nint ret;\r\nif (likely(drm_ht_find_item(&sw_context->res_ht, (unsigned long) res,\r\n&hash) == 0)) {\r\nnode = container_of(hash, struct vmw_resource_val_node, hash);\r\nnode->first_usage = false;\r\nif (unlikely(p_node != NULL))\r\n*p_node = node;\r\nreturn 0;\r\n}\r\nnode = kzalloc(sizeof(*node), GFP_KERNEL);\r\nif (unlikely(node == NULL)) {\r\nDRM_ERROR("Failed to allocate a resource validation "\r\n"entry.\n");\r\nreturn -ENOMEM;\r\n}\r\nnode->hash.key = (unsigned long) res;\r\nret = drm_ht_insert_item(&sw_context->res_ht, &node->hash);\r\nif (unlikely(ret != 0)) {\r\nDRM_ERROR("Failed to initialize a resource validation "\r\n"entry.\n");\r\nkfree(node);\r\nreturn ret;\r\n}\r\nlist_add_tail(&node->head, &sw_context->resource_list);\r\nnode->res = vmw_resource_reference(res);\r\nnode->first_usage = true;\r\nif (unlikely(p_node != NULL))\r\n*p_node = node;\r\nreturn 0;\r\n}\r\nstatic int vmw_resource_context_res_add(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nstruct vmw_resource *ctx)\r\n{\r\nstruct list_head *binding_list;\r\nstruct vmw_ctx_binding *entry;\r\nint ret = 0;\r\nstruct vmw_resource *res;\r\nmutex_lock(&dev_priv->binding_mutex);\r\nbinding_list = vmw_context_binding_list(ctx);\r\nlist_for_each_entry(entry, binding_list, ctx_list) {\r\nres = vmw_resource_reference_unless_doomed(entry->bi.res);\r\nif (unlikely(res == NULL))\r\ncontinue;\r\nret = vmw_resource_val_add(sw_context, entry->bi.res, NULL);\r\nvmw_resource_unreference(&res);\r\nif (unlikely(ret != 0))\r\nbreak;\r\n}\r\nmutex_unlock(&dev_priv->binding_mutex);\r\nreturn ret;\r\n}\r\nstatic int vmw_resource_relocation_add(struct list_head *list,\r\nconst struct vmw_resource *res,\r\nunsigned long offset)\r\n{\r\nstruct vmw_resource_relocation *rel;\r\nrel = kmalloc(sizeof(*rel), GFP_KERNEL);\r\nif (unlikely(rel == NULL)) {\r\nDRM_ERROR("Failed to allocate a resource relocation.\n");\r\nreturn -ENOMEM;\r\n}\r\nrel->res = res;\r\nrel->offset = offset;\r\nlist_add_tail(&rel->head, list);\r\nreturn 0;\r\n}\r\nstatic void vmw_resource_relocations_free(struct list_head *list)\r\n{\r\nstruct vmw_resource_relocation *rel, *n;\r\nlist_for_each_entry_safe(rel, n, list, head) {\r\nlist_del(&rel->head);\r\nkfree(rel);\r\n}\r\n}\r\nstatic void vmw_resource_relocations_apply(uint32_t *cb,\r\nstruct list_head *list)\r\n{\r\nstruct vmw_resource_relocation *rel;\r\nlist_for_each_entry(rel, list, head) {\r\nif (likely(rel->res != NULL))\r\ncb[rel->offset] = rel->res->id;\r\nelse\r\ncb[rel->offset] = SVGA_3D_CMD_NOP;\r\n}\r\n}\r\nstatic int vmw_cmd_invalid(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nreturn capable(CAP_SYS_ADMIN) ? : -EINVAL;\r\n}\r\nstatic int vmw_cmd_ok(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nreturn 0;\r\n}\r\nstatic int vmw_bo_to_validate_list(struct vmw_sw_context *sw_context,\r\nstruct ttm_buffer_object *bo,\r\nbool validate_as_mob,\r\nuint32_t *p_val_node)\r\n{\r\nuint32_t val_node;\r\nstruct vmw_validate_buffer *vval_buf;\r\nstruct ttm_validate_buffer *val_buf;\r\nstruct drm_hash_item *hash;\r\nint ret;\r\nif (likely(drm_ht_find_item(&sw_context->res_ht, (unsigned long) bo,\r\n&hash) == 0)) {\r\nvval_buf = container_of(hash, struct vmw_validate_buffer,\r\nhash);\r\nif (unlikely(vval_buf->validate_as_mob != validate_as_mob)) {\r\nDRM_ERROR("Inconsistent buffer usage.\n");\r\nreturn -EINVAL;\r\n}\r\nval_buf = &vval_buf->base;\r\nval_node = vval_buf - sw_context->val_bufs;\r\n} else {\r\nval_node = sw_context->cur_val_buf;\r\nif (unlikely(val_node >= VMWGFX_MAX_VALIDATIONS)) {\r\nDRM_ERROR("Max number of DMA buffers per submission "\r\n"exceeded.\n");\r\nreturn -EINVAL;\r\n}\r\nvval_buf = &sw_context->val_bufs[val_node];\r\nvval_buf->hash.key = (unsigned long) bo;\r\nret = drm_ht_insert_item(&sw_context->res_ht, &vval_buf->hash);\r\nif (unlikely(ret != 0)) {\r\nDRM_ERROR("Failed to initialize a buffer validation "\r\n"entry.\n");\r\nreturn ret;\r\n}\r\n++sw_context->cur_val_buf;\r\nval_buf = &vval_buf->base;\r\nval_buf->bo = ttm_bo_reference(bo);\r\nval_buf->shared = false;\r\nlist_add_tail(&val_buf->head, &sw_context->validate_nodes);\r\nvval_buf->validate_as_mob = validate_as_mob;\r\n}\r\nif (p_val_node)\r\n*p_val_node = val_node;\r\nreturn 0;\r\n}\r\nstatic int vmw_resources_reserve(struct vmw_sw_context *sw_context)\r\n{\r\nstruct vmw_resource_val_node *val;\r\nint ret;\r\nlist_for_each_entry(val, &sw_context->resource_list, head) {\r\nstruct vmw_resource *res = val->res;\r\nret = vmw_resource_reserve(res, val->no_buffer_needed);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (res->backup) {\r\nstruct ttm_buffer_object *bo = &res->backup->base;\r\nret = vmw_bo_to_validate_list\r\n(sw_context, bo,\r\nvmw_resource_needs_backup(res), NULL);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int vmw_resources_validate(struct vmw_sw_context *sw_context)\r\n{\r\nstruct vmw_resource_val_node *val;\r\nint ret;\r\nlist_for_each_entry(val, &sw_context->resource_list, head) {\r\nstruct vmw_resource *res = val->res;\r\nret = vmw_resource_validate(res);\r\nif (unlikely(ret != 0)) {\r\nif (ret != -ERESTARTSYS)\r\nDRM_ERROR("Failed to validate resource.\n");\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_res_reloc_add(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nenum vmw_res_type res_type,\r\nuint32_t *id_loc,\r\nstruct vmw_resource *res,\r\nstruct vmw_resource_val_node **p_val)\r\n{\r\nint ret;\r\nstruct vmw_resource_val_node *node;\r\n*p_val = NULL;\r\nret = vmw_resource_relocation_add(&sw_context->res_relocations,\r\nres,\r\nid_loc - sw_context->buf_start);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_resource_val_add(sw_context, res, &node);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (res_type == vmw_res_context && dev_priv->has_mob &&\r\nnode->first_usage) {\r\nlist_del(&node->head);\r\nlist_add(&node->head, &sw_context->resource_list);\r\nret = vmw_resource_context_res_add(dev_priv, sw_context, res);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nnode->staged_bindings =\r\nkzalloc(sizeof(*node->staged_bindings), GFP_KERNEL);\r\nif (node->staged_bindings == NULL) {\r\nDRM_ERROR("Failed to allocate context binding "\r\n"information.\n");\r\nreturn -ENOMEM;\r\n}\r\nINIT_LIST_HEAD(&node->staged_bindings->list);\r\n}\r\nif (p_val)\r\n*p_val = node;\r\nreturn 0;\r\n}\r\nstatic int\r\nvmw_cmd_res_check(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nenum vmw_res_type res_type,\r\nconst struct vmw_user_resource_conv *converter,\r\nuint32_t *id_loc,\r\nstruct vmw_resource_val_node **p_val)\r\n{\r\nstruct vmw_res_cache_entry *rcache =\r\n&sw_context->res_cache[res_type];\r\nstruct vmw_resource *res;\r\nstruct vmw_resource_val_node *node;\r\nint ret;\r\nif (*id_loc == SVGA3D_INVALID_ID) {\r\nif (p_val)\r\n*p_val = NULL;\r\nif (res_type == vmw_res_context) {\r\nDRM_ERROR("Illegal context invalid id.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nif (likely(rcache->valid && *id_loc == rcache->handle)) {\r\nconst struct vmw_resource *res = rcache->res;\r\nrcache->node->first_usage = false;\r\nif (p_val)\r\n*p_val = rcache->node;\r\nreturn vmw_resource_relocation_add\r\n(&sw_context->res_relocations, res,\r\nid_loc - sw_context->buf_start);\r\n}\r\nret = vmw_user_resource_lookup_handle(dev_priv,\r\nsw_context->fp->tfile,\r\n*id_loc,\r\nconverter,\r\n&res);\r\nif (unlikely(ret != 0)) {\r\nDRM_ERROR("Could not find or use resource 0x%08x.\n",\r\n(unsigned) *id_loc);\r\ndump_stack();\r\nreturn ret;\r\n}\r\nrcache->valid = true;\r\nrcache->res = res;\r\nrcache->handle = *id_loc;\r\nret = vmw_cmd_res_reloc_add(dev_priv, sw_context, res_type, id_loc,\r\nres, &node);\r\nif (unlikely(ret != 0))\r\ngoto out_no_reloc;\r\nrcache->node = node;\r\nif (p_val)\r\n*p_val = node;\r\nvmw_resource_unreference(&res);\r\nreturn 0;\r\nout_no_reloc:\r\nBUG_ON(sw_context->error_resource != NULL);\r\nsw_context->error_resource = res;\r\nreturn ret;\r\n}\r\nstatic int vmw_rebind_contexts(struct vmw_sw_context *sw_context)\r\n{\r\nstruct vmw_resource_val_node *val;\r\nint ret;\r\nlist_for_each_entry(val, &sw_context->resource_list, head) {\r\nif (unlikely(!val->staged_bindings))\r\nbreak;\r\nret = vmw_context_rebind_all(val->res);\r\nif (unlikely(ret != 0)) {\r\nif (ret != -ERESTARTSYS)\r\nDRM_ERROR("Failed to rebind context.\n");\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_cid_check(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_cid_cmd {\r\nSVGA3dCmdHeader header;\r\nuint32_t cid;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_cid_cmd, header);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\r\nuser_context_converter, &cmd->cid, NULL);\r\n}\r\nstatic int vmw_cmd_set_render_target_check(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_sid_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdSetRenderTarget body;\r\n} *cmd;\r\nstruct vmw_resource_val_node *ctx_node;\r\nstruct vmw_resource_val_node *res_node;\r\nint ret;\r\ncmd = container_of(header, struct vmw_sid_cmd, header);\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\r\nuser_context_converter, &cmd->body.cid,\r\n&ctx_node);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.target.sid, &res_node);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (dev_priv->has_mob) {\r\nstruct vmw_ctx_bindinfo bi;\r\nbi.ctx = ctx_node->res;\r\nbi.res = res_node ? res_node->res : NULL;\r\nbi.bt = vmw_ctx_binding_rt;\r\nbi.i1.rt_type = cmd->body.type;\r\nreturn vmw_context_binding_add(ctx_node->staged_bindings, &bi);\r\n}\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_surface_copy_check(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_sid_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdSurfaceCopy body;\r\n} *cmd;\r\nint ret;\r\ncmd = container_of(header, struct vmw_sid_cmd, header);\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.src.sid, NULL);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.dest.sid, NULL);\r\n}\r\nstatic int vmw_cmd_stretch_blt_check(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_sid_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdSurfaceStretchBlt body;\r\n} *cmd;\r\nint ret;\r\ncmd = container_of(header, struct vmw_sid_cmd, header);\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.src.sid, NULL);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.dest.sid, NULL);\r\n}\r\nstatic int vmw_cmd_blt_surf_screen_check(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_sid_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdBlitSurfaceToScreen body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_sid_cmd, header);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.srcImage.sid, NULL);\r\n}\r\nstatic int vmw_cmd_present_check(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_sid_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdPresent body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_sid_cmd, header);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter, &cmd->body.sid,\r\nNULL);\r\n}\r\nstatic int vmw_query_bo_switch_prepare(struct vmw_private *dev_priv,\r\nstruct ttm_buffer_object *new_query_bo,\r\nstruct vmw_sw_context *sw_context)\r\n{\r\nstruct vmw_res_cache_entry *ctx_entry =\r\n&sw_context->res_cache[vmw_res_context];\r\nint ret;\r\nBUG_ON(!ctx_entry->valid);\r\nsw_context->last_query_ctx = ctx_entry->res;\r\nif (unlikely(new_query_bo != sw_context->cur_query_bo)) {\r\nif (unlikely(new_query_bo->num_pages > 4)) {\r\nDRM_ERROR("Query buffer too large.\n");\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(sw_context->cur_query_bo != NULL)) {\r\nsw_context->needs_post_query_barrier = true;\r\nret = vmw_bo_to_validate_list(sw_context,\r\nsw_context->cur_query_bo,\r\ndev_priv->has_mob, NULL);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\nsw_context->cur_query_bo = new_query_bo;\r\nret = vmw_bo_to_validate_list(sw_context,\r\ndev_priv->dummy_query_bo,\r\ndev_priv->has_mob, NULL);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void vmw_query_bo_switch_commit(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context)\r\n{\r\nif (sw_context->needs_post_query_barrier) {\r\nstruct vmw_res_cache_entry *ctx_entry =\r\n&sw_context->res_cache[vmw_res_context];\r\nstruct vmw_resource *ctx;\r\nint ret;\r\nBUG_ON(!ctx_entry->valid);\r\nctx = ctx_entry->res;\r\nret = vmw_fifo_emit_dummy_query(dev_priv, ctx->id);\r\nif (unlikely(ret != 0))\r\nDRM_ERROR("Out of fifo space for dummy query.\n");\r\n}\r\nif (dev_priv->pinned_bo != sw_context->cur_query_bo) {\r\nif (dev_priv->pinned_bo) {\r\nvmw_bo_pin(dev_priv->pinned_bo, false);\r\nttm_bo_unref(&dev_priv->pinned_bo);\r\n}\r\nif (!sw_context->needs_post_query_barrier) {\r\nvmw_bo_pin(sw_context->cur_query_bo, true);\r\nvmw_bo_pin(dev_priv->dummy_query_bo, true);\r\ndev_priv->dummy_query_bo_pinned = true;\r\nBUG_ON(sw_context->last_query_ctx == NULL);\r\ndev_priv->query_cid = sw_context->last_query_ctx->id;\r\ndev_priv->query_cid_valid = true;\r\ndev_priv->pinned_bo =\r\nttm_bo_reference(sw_context->cur_query_bo);\r\n}\r\n}\r\n}\r\nstatic int vmw_translate_mob_ptr(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGAMobId *id,\r\nstruct vmw_dma_buffer **vmw_bo_p)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo = NULL;\r\nstruct ttm_buffer_object *bo;\r\nuint32_t handle = *id;\r\nstruct vmw_relocation *reloc;\r\nint ret;\r\nret = vmw_user_dmabuf_lookup(sw_context->fp->tfile, handle, &vmw_bo);\r\nif (unlikely(ret != 0)) {\r\nDRM_ERROR("Could not find or use MOB buffer.\n");\r\nret = -EINVAL;\r\ngoto out_no_reloc;\r\n}\r\nbo = &vmw_bo->base;\r\nif (unlikely(sw_context->cur_reloc >= VMWGFX_MAX_RELOCATIONS)) {\r\nDRM_ERROR("Max number relocations per submission"\r\n" exceeded\n");\r\nret = -EINVAL;\r\ngoto out_no_reloc;\r\n}\r\nreloc = &sw_context->relocs[sw_context->cur_reloc++];\r\nreloc->mob_loc = id;\r\nreloc->location = NULL;\r\nret = vmw_bo_to_validate_list(sw_context, bo, true, &reloc->index);\r\nif (unlikely(ret != 0))\r\ngoto out_no_reloc;\r\n*vmw_bo_p = vmw_bo;\r\nreturn 0;\r\nout_no_reloc:\r\nvmw_dmabuf_unreference(&vmw_bo);\r\n*vmw_bo_p = NULL;\r\nreturn ret;\r\n}\r\nstatic int vmw_translate_guest_ptr(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGAGuestPtr *ptr,\r\nstruct vmw_dma_buffer **vmw_bo_p)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo = NULL;\r\nstruct ttm_buffer_object *bo;\r\nuint32_t handle = ptr->gmrId;\r\nstruct vmw_relocation *reloc;\r\nint ret;\r\nret = vmw_user_dmabuf_lookup(sw_context->fp->tfile, handle, &vmw_bo);\r\nif (unlikely(ret != 0)) {\r\nDRM_ERROR("Could not find or use GMR region.\n");\r\nret = -EINVAL;\r\ngoto out_no_reloc;\r\n}\r\nbo = &vmw_bo->base;\r\nif (unlikely(sw_context->cur_reloc >= VMWGFX_MAX_RELOCATIONS)) {\r\nDRM_ERROR("Max number relocations per submission"\r\n" exceeded\n");\r\nret = -EINVAL;\r\ngoto out_no_reloc;\r\n}\r\nreloc = &sw_context->relocs[sw_context->cur_reloc++];\r\nreloc->location = ptr;\r\nret = vmw_bo_to_validate_list(sw_context, bo, false, &reloc->index);\r\nif (unlikely(ret != 0))\r\ngoto out_no_reloc;\r\n*vmw_bo_p = vmw_bo;\r\nreturn 0;\r\nout_no_reloc:\r\nvmw_dmabuf_unreference(&vmw_bo);\r\n*vmw_bo_p = NULL;\r\nreturn ret;\r\n}\r\nstatic int vmw_cmd_begin_gb_query(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_begin_gb_query_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdBeginGBQuery q;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_begin_gb_query_cmd,\r\nheader);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\r\nuser_context_converter, &cmd->q.cid,\r\nNULL);\r\n}\r\nstatic int vmw_cmd_begin_query(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_begin_query_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdBeginQuery q;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_begin_query_cmd,\r\nheader);\r\nif (unlikely(dev_priv->has_mob)) {\r\nstruct {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdBeginGBQuery q;\r\n} gb_cmd;\r\nBUG_ON(sizeof(gb_cmd) != sizeof(*cmd));\r\ngb_cmd.header.id = SVGA_3D_CMD_BEGIN_GB_QUERY;\r\ngb_cmd.header.size = cmd->header.size;\r\ngb_cmd.q.cid = cmd->q.cid;\r\ngb_cmd.q.type = cmd->q.type;\r\nmemcpy(cmd, &gb_cmd, sizeof(*cmd));\r\nreturn vmw_cmd_begin_gb_query(dev_priv, sw_context, header);\r\n}\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\r\nuser_context_converter, &cmd->q.cid,\r\nNULL);\r\n}\r\nstatic int vmw_cmd_end_gb_query(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo;\r\nstruct vmw_query_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdEndGBQuery q;\r\n} *cmd;\r\nint ret;\r\ncmd = container_of(header, struct vmw_query_cmd, header);\r\nret = vmw_cmd_cid_check(dev_priv, sw_context, header);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_translate_mob_ptr(dev_priv, sw_context,\r\n&cmd->q.mobid,\r\n&vmw_bo);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_query_bo_switch_prepare(dev_priv, &vmw_bo->base, sw_context);\r\nvmw_dmabuf_unreference(&vmw_bo);\r\nreturn ret;\r\n}\r\nstatic int vmw_cmd_end_query(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo;\r\nstruct vmw_query_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdEndQuery q;\r\n} *cmd;\r\nint ret;\r\ncmd = container_of(header, struct vmw_query_cmd, header);\r\nif (dev_priv->has_mob) {\r\nstruct {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdEndGBQuery q;\r\n} gb_cmd;\r\nBUG_ON(sizeof(gb_cmd) != sizeof(*cmd));\r\ngb_cmd.header.id = SVGA_3D_CMD_END_GB_QUERY;\r\ngb_cmd.header.size = cmd->header.size;\r\ngb_cmd.q.cid = cmd->q.cid;\r\ngb_cmd.q.type = cmd->q.type;\r\ngb_cmd.q.mobid = cmd->q.guestResult.gmrId;\r\ngb_cmd.q.offset = cmd->q.guestResult.offset;\r\nmemcpy(cmd, &gb_cmd, sizeof(*cmd));\r\nreturn vmw_cmd_end_gb_query(dev_priv, sw_context, header);\r\n}\r\nret = vmw_cmd_cid_check(dev_priv, sw_context, header);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_translate_guest_ptr(dev_priv, sw_context,\r\n&cmd->q.guestResult,\r\n&vmw_bo);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_query_bo_switch_prepare(dev_priv, &vmw_bo->base, sw_context);\r\nvmw_dmabuf_unreference(&vmw_bo);\r\nreturn ret;\r\n}\r\nstatic int vmw_cmd_wait_gb_query(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo;\r\nstruct vmw_query_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdWaitForGBQuery q;\r\n} *cmd;\r\nint ret;\r\ncmd = container_of(header, struct vmw_query_cmd, header);\r\nret = vmw_cmd_cid_check(dev_priv, sw_context, header);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_translate_mob_ptr(dev_priv, sw_context,\r\n&cmd->q.mobid,\r\n&vmw_bo);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nvmw_dmabuf_unreference(&vmw_bo);\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_wait_query(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo;\r\nstruct vmw_query_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdWaitForQuery q;\r\n} *cmd;\r\nint ret;\r\ncmd = container_of(header, struct vmw_query_cmd, header);\r\nif (dev_priv->has_mob) {\r\nstruct {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdWaitForGBQuery q;\r\n} gb_cmd;\r\nBUG_ON(sizeof(gb_cmd) != sizeof(*cmd));\r\ngb_cmd.header.id = SVGA_3D_CMD_WAIT_FOR_GB_QUERY;\r\ngb_cmd.header.size = cmd->header.size;\r\ngb_cmd.q.cid = cmd->q.cid;\r\ngb_cmd.q.type = cmd->q.type;\r\ngb_cmd.q.mobid = cmd->q.guestResult.gmrId;\r\ngb_cmd.q.offset = cmd->q.guestResult.offset;\r\nmemcpy(cmd, &gb_cmd, sizeof(*cmd));\r\nreturn vmw_cmd_wait_gb_query(dev_priv, sw_context, header);\r\n}\r\nret = vmw_cmd_cid_check(dev_priv, sw_context, header);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_translate_guest_ptr(dev_priv, sw_context,\r\n&cmd->q.guestResult,\r\n&vmw_bo);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nvmw_dmabuf_unreference(&vmw_bo);\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_dma(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo = NULL;\r\nstruct vmw_surface *srf = NULL;\r\nstruct vmw_dma_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdSurfaceDMA dma;\r\n} *cmd;\r\nint ret;\r\nSVGA3dCmdSurfaceDMASuffix *suffix;\r\nuint32_t bo_size;\r\ncmd = container_of(header, struct vmw_dma_cmd, header);\r\nsuffix = (SVGA3dCmdSurfaceDMASuffix *)((unsigned long) &cmd->dma +\r\nheader->size - sizeof(*suffix));\r\nif (unlikely(suffix->suffixSize != sizeof(*suffix))) {\r\nDRM_ERROR("Invalid DMA suffix size.\n");\r\nreturn -EINVAL;\r\n}\r\nret = vmw_translate_guest_ptr(dev_priv, sw_context,\r\n&cmd->dma.guest.ptr,\r\n&vmw_bo);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nbo_size = vmw_bo->base.num_pages * PAGE_SIZE;\r\nif (unlikely(cmd->dma.guest.ptr.offset > bo_size)) {\r\nDRM_ERROR("Invalid DMA offset.\n");\r\nreturn -EINVAL;\r\n}\r\nbo_size -= cmd->dma.guest.ptr.offset;\r\nif (unlikely(suffix->maximumOffset > bo_size))\r\nsuffix->maximumOffset = bo_size;\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter, &cmd->dma.host.sid,\r\nNULL);\r\nif (unlikely(ret != 0)) {\r\nif (unlikely(ret != -ERESTARTSYS))\r\nDRM_ERROR("could not find surface for DMA.\n");\r\ngoto out_no_surface;\r\n}\r\nsrf = vmw_res_to_srf(sw_context->res_cache[vmw_res_surface].res);\r\nvmw_kms_cursor_snoop(srf, sw_context->fp->tfile, &vmw_bo->base,\r\nheader);\r\nout_no_surface:\r\nvmw_dmabuf_unreference(&vmw_bo);\r\nreturn ret;\r\n}\r\nstatic int vmw_cmd_draw(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_draw_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdDrawPrimitives body;\r\n} *cmd;\r\nSVGA3dVertexDecl *decl = (SVGA3dVertexDecl *)(\r\n(unsigned long)header + sizeof(*cmd));\r\nSVGA3dPrimitiveRange *range;\r\nuint32_t i;\r\nuint32_t maxnum;\r\nint ret;\r\nret = vmw_cmd_cid_check(dev_priv, sw_context, header);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\ncmd = container_of(header, struct vmw_draw_cmd, header);\r\nmaxnum = (header->size - sizeof(cmd->body)) / sizeof(*decl);\r\nif (unlikely(cmd->body.numVertexDecls > maxnum)) {\r\nDRM_ERROR("Illegal number of vertex declarations.\n");\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < cmd->body.numVertexDecls; ++i, ++decl) {\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&decl->array.surfaceId, NULL);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\nmaxnum = (header->size - sizeof(cmd->body) -\r\ncmd->body.numVertexDecls * sizeof(*decl)) / sizeof(*range);\r\nif (unlikely(cmd->body.numRanges > maxnum)) {\r\nDRM_ERROR("Illegal number of index ranges.\n");\r\nreturn -EINVAL;\r\n}\r\nrange = (SVGA3dPrimitiveRange *) decl;\r\nfor (i = 0; i < cmd->body.numRanges; ++i, ++range) {\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&range->indexArray.surfaceId, NULL);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_tex_state(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_tex_state_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdSetTextureState state;\r\n} *cmd;\r\nSVGA3dTextureState *last_state = (SVGA3dTextureState *)\r\n((unsigned long) header + header->size + sizeof(header));\r\nSVGA3dTextureState *cur_state = (SVGA3dTextureState *)\r\n((unsigned long) header + sizeof(struct vmw_tex_state_cmd));\r\nstruct vmw_resource_val_node *ctx_node;\r\nstruct vmw_resource_val_node *res_node;\r\nint ret;\r\ncmd = container_of(header, struct vmw_tex_state_cmd,\r\nheader);\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\r\nuser_context_converter, &cmd->state.cid,\r\n&ctx_node);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nfor (; cur_state < last_state; ++cur_state) {\r\nif (likely(cur_state->name != SVGA3D_TS_BIND_TEXTURE))\r\ncontinue;\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cur_state->value, &res_node);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (dev_priv->has_mob) {\r\nstruct vmw_ctx_bindinfo bi;\r\nbi.ctx = ctx_node->res;\r\nbi.res = res_node ? res_node->res : NULL;\r\nbi.bt = vmw_ctx_binding_tex;\r\nbi.i1.texture_stage = cur_state->stage;\r\nvmw_context_binding_add(ctx_node->staged_bindings,\r\n&bi);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_check_define_gmrfb(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nvoid *buf)\r\n{\r\nstruct vmw_dma_buffer *vmw_bo;\r\nint ret;\r\nstruct {\r\nuint32_t header;\r\nSVGAFifoCmdDefineGMRFB body;\r\n} *cmd = buf;\r\nret = vmw_translate_guest_ptr(dev_priv, sw_context,\r\n&cmd->body.ptr,\r\n&vmw_bo);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nvmw_dmabuf_unreference(&vmw_bo);\r\nreturn ret;\r\n}\r\nstatic int vmw_cmd_switch_backup(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nenum vmw_res_type res_type,\r\nconst struct vmw_user_resource_conv\r\n*converter,\r\nuint32_t *res_id,\r\nuint32_t *buf_id,\r\nunsigned long backup_offset)\r\n{\r\nint ret;\r\nstruct vmw_dma_buffer *dma_buf;\r\nstruct vmw_resource_val_node *val_node;\r\nret = vmw_cmd_res_check(dev_priv, sw_context, res_type,\r\nconverter, res_id, &val_node);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_translate_mob_ptr(dev_priv, sw_context, buf_id, &dma_buf);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (val_node->first_usage)\r\nval_node->no_buffer_needed = true;\r\nvmw_dmabuf_unreference(&val_node->new_backup);\r\nval_node->new_backup = dma_buf;\r\nval_node->new_backup_offset = backup_offset;\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_bind_gb_surface(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_bind_gb_surface_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdBindGBSurface body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_bind_gb_surface_cmd, header);\r\nreturn vmw_cmd_switch_backup(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.sid, &cmd->body.mobid,\r\n0);\r\n}\r\nstatic int vmw_cmd_update_gb_image(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_gb_surface_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdUpdateGBImage body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_gb_surface_cmd, header);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.image.sid, NULL);\r\n}\r\nstatic int vmw_cmd_update_gb_surface(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_gb_surface_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdUpdateGBSurface body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_gb_surface_cmd, header);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.sid, NULL);\r\n}\r\nstatic int vmw_cmd_readback_gb_image(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_gb_surface_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdReadbackGBImage body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_gb_surface_cmd, header);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.image.sid, NULL);\r\n}\r\nstatic int vmw_cmd_readback_gb_surface(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_gb_surface_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdReadbackGBSurface body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_gb_surface_cmd, header);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.sid, NULL);\r\n}\r\nstatic int vmw_cmd_invalidate_gb_image(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_gb_surface_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdInvalidateGBImage body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_gb_surface_cmd, header);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.image.sid, NULL);\r\n}\r\nstatic int vmw_cmd_invalidate_gb_surface(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_gb_surface_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdInvalidateGBSurface body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_gb_surface_cmd, header);\r\nreturn vmw_cmd_res_check(dev_priv, sw_context, vmw_res_surface,\r\nuser_surface_converter,\r\n&cmd->body.sid, NULL);\r\n}\r\nstatic int vmw_cmd_shader_define(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_shader_define_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdDefineShader body;\r\n} *cmd;\r\nint ret;\r\nsize_t size;\r\nstruct vmw_resource_val_node *val;\r\ncmd = container_of(header, struct vmw_shader_define_cmd,\r\nheader);\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\r\nuser_context_converter, &cmd->body.cid,\r\n&val);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (unlikely(!dev_priv->has_mob))\r\nreturn 0;\r\nsize = cmd->header.size - sizeof(cmd->body);\r\nret = vmw_compat_shader_add(dev_priv,\r\nvmw_context_res_man(val->res),\r\ncmd->body.shid, cmd + 1,\r\ncmd->body.type, size,\r\n&sw_context->staged_cmd_res);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nreturn vmw_resource_relocation_add(&sw_context->res_relocations,\r\nNULL, &cmd->header.id -\r\nsw_context->buf_start);\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_shader_destroy(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_shader_destroy_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdDestroyShader body;\r\n} *cmd;\r\nint ret;\r\nstruct vmw_resource_val_node *val;\r\ncmd = container_of(header, struct vmw_shader_destroy_cmd,\r\nheader);\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\r\nuser_context_converter, &cmd->body.cid,\r\n&val);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (unlikely(!dev_priv->has_mob))\r\nreturn 0;\r\nret = vmw_compat_shader_remove(vmw_context_res_man(val->res),\r\ncmd->body.shid,\r\ncmd->body.type,\r\n&sw_context->staged_cmd_res);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nreturn vmw_resource_relocation_add(&sw_context->res_relocations,\r\nNULL, &cmd->header.id -\r\nsw_context->buf_start);\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_set_shader(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_set_shader_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdSetShader body;\r\n} *cmd;\r\nstruct vmw_resource_val_node *ctx_node, *res_node = NULL;\r\nstruct vmw_ctx_bindinfo bi;\r\nstruct vmw_resource *res = NULL;\r\nint ret;\r\ncmd = container_of(header, struct vmw_set_shader_cmd,\r\nheader);\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\r\nuser_context_converter, &cmd->body.cid,\r\n&ctx_node);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (!dev_priv->has_mob)\r\nreturn 0;\r\nif (cmd->body.shid != SVGA3D_INVALID_ID) {\r\nres = vmw_compat_shader_lookup\r\n(vmw_context_res_man(ctx_node->res),\r\ncmd->body.shid,\r\ncmd->body.type);\r\nif (!IS_ERR(res)) {\r\nret = vmw_cmd_res_reloc_add(dev_priv, sw_context,\r\nvmw_res_shader,\r\n&cmd->body.shid, res,\r\n&res_node);\r\nvmw_resource_unreference(&res);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\n}\r\nif (!res_node) {\r\nret = vmw_cmd_res_check(dev_priv, sw_context,\r\nvmw_res_shader,\r\nuser_shader_converter,\r\n&cmd->body.shid, &res_node);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\nbi.ctx = ctx_node->res;\r\nbi.res = res_node ? res_node->res : NULL;\r\nbi.bt = vmw_ctx_binding_shader;\r\nbi.i1.shader_type = cmd->body.type;\r\nreturn vmw_context_binding_add(ctx_node->staged_bindings, &bi);\r\n}\r\nstatic int vmw_cmd_set_shader_const(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_set_shader_const_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdSetShaderConst body;\r\n} *cmd;\r\nint ret;\r\ncmd = container_of(header, struct vmw_set_shader_const_cmd,\r\nheader);\r\nret = vmw_cmd_res_check(dev_priv, sw_context, vmw_res_context,\r\nuser_context_converter, &cmd->body.cid,\r\nNULL);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (dev_priv->has_mob)\r\nheader->id = SVGA_3D_CMD_SET_GB_SHADERCONSTS_INLINE;\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_bind_gb_shader(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nSVGA3dCmdHeader *header)\r\n{\r\nstruct vmw_bind_gb_shader_cmd {\r\nSVGA3dCmdHeader header;\r\nSVGA3dCmdBindGBShader body;\r\n} *cmd;\r\ncmd = container_of(header, struct vmw_bind_gb_shader_cmd,\r\nheader);\r\nreturn vmw_cmd_switch_backup(dev_priv, sw_context, vmw_res_shader,\r\nuser_shader_converter,\r\n&cmd->body.shid, &cmd->body.mobid,\r\ncmd->body.offsetInBytes);\r\n}\r\nstatic int vmw_cmd_check_not_3d(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nvoid *buf, uint32_t *size)\r\n{\r\nuint32_t size_remaining = *size;\r\nuint32_t cmd_id;\r\ncmd_id = le32_to_cpu(((uint32_t *)buf)[0]);\r\nswitch (cmd_id) {\r\ncase SVGA_CMD_UPDATE:\r\n*size = sizeof(uint32_t) + sizeof(SVGAFifoCmdUpdate);\r\nbreak;\r\ncase SVGA_CMD_DEFINE_GMRFB:\r\n*size = sizeof(uint32_t) + sizeof(SVGAFifoCmdDefineGMRFB);\r\nbreak;\r\ncase SVGA_CMD_BLIT_GMRFB_TO_SCREEN:\r\n*size = sizeof(uint32_t) + sizeof(SVGAFifoCmdBlitGMRFBToScreen);\r\nbreak;\r\ncase SVGA_CMD_BLIT_SCREEN_TO_GMRFB:\r\n*size = sizeof(uint32_t) + sizeof(SVGAFifoCmdBlitGMRFBToScreen);\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Unsupported SVGA command: %u.\n", cmd_id);\r\nreturn -EINVAL;\r\n}\r\nif (*size > size_remaining) {\r\nDRM_ERROR("Invalid SVGA command (size mismatch):"\r\n" %u.\n", cmd_id);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(!sw_context->kernel)) {\r\nDRM_ERROR("Kernel only SVGA command: %u.\n", cmd_id);\r\nreturn -EPERM;\r\n}\r\nif (cmd_id == SVGA_CMD_DEFINE_GMRFB)\r\nreturn vmw_cmd_check_define_gmrfb(dev_priv, sw_context, buf);\r\nreturn 0;\r\n}\r\nstatic int vmw_cmd_check(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nvoid *buf, uint32_t *size)\r\n{\r\nuint32_t cmd_id;\r\nuint32_t size_remaining = *size;\r\nSVGA3dCmdHeader *header = (SVGA3dCmdHeader *) buf;\r\nint ret;\r\nconst struct vmw_cmd_entry *entry;\r\nbool gb = dev_priv->capabilities & SVGA_CAP_GBOBJECTS;\r\ncmd_id = le32_to_cpu(((uint32_t *)buf)[0]);\r\nif (unlikely(cmd_id < SVGA_CMD_MAX))\r\nreturn vmw_cmd_check_not_3d(dev_priv, sw_context, buf, size);\r\ncmd_id = le32_to_cpu(header->id);\r\n*size = le32_to_cpu(header->size) + sizeof(SVGA3dCmdHeader);\r\ncmd_id -= SVGA_3D_CMD_BASE;\r\nif (unlikely(*size > size_remaining))\r\ngoto out_invalid;\r\nif (unlikely(cmd_id >= SVGA_3D_CMD_MAX - SVGA_3D_CMD_BASE))\r\ngoto out_invalid;\r\nentry = &vmw_cmd_entries[cmd_id];\r\nif (unlikely(!entry->func))\r\ngoto out_invalid;\r\nif (unlikely(!entry->user_allow && !sw_context->kernel))\r\ngoto out_privileged;\r\nif (unlikely(entry->gb_disable && gb))\r\ngoto out_old;\r\nif (unlikely(entry->gb_enable && !gb))\r\ngoto out_new;\r\nret = entry->func(dev_priv, sw_context, header);\r\nif (unlikely(ret != 0))\r\ngoto out_invalid;\r\nreturn 0;\r\nout_invalid:\r\nDRM_ERROR("Invalid SVGA3D command: %d\n",\r\ncmd_id + SVGA_3D_CMD_BASE);\r\nreturn -EINVAL;\r\nout_privileged:\r\nDRM_ERROR("Privileged SVGA3D command: %d\n",\r\ncmd_id + SVGA_3D_CMD_BASE);\r\nreturn -EPERM;\r\nout_old:\r\nDRM_ERROR("Deprecated (disallowed) SVGA3D command: %d\n",\r\ncmd_id + SVGA_3D_CMD_BASE);\r\nreturn -EINVAL;\r\nout_new:\r\nDRM_ERROR("SVGA3D command: %d not supported by virtual hardware.\n",\r\ncmd_id + SVGA_3D_CMD_BASE);\r\nreturn -EINVAL;\r\n}\r\nstatic int vmw_cmd_check_all(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context,\r\nvoid *buf,\r\nuint32_t size)\r\n{\r\nint32_t cur_size = size;\r\nint ret;\r\nsw_context->buf_start = buf;\r\nwhile (cur_size > 0) {\r\nsize = cur_size;\r\nret = vmw_cmd_check(dev_priv, sw_context, buf, &size);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nbuf = (void *)((unsigned long) buf + size);\r\ncur_size -= size;\r\n}\r\nif (unlikely(cur_size != 0)) {\r\nDRM_ERROR("Command verifier out of sync.\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void vmw_free_relocations(struct vmw_sw_context *sw_context)\r\n{\r\nsw_context->cur_reloc = 0;\r\n}\r\nstatic void vmw_apply_relocations(struct vmw_sw_context *sw_context)\r\n{\r\nuint32_t i;\r\nstruct vmw_relocation *reloc;\r\nstruct ttm_validate_buffer *validate;\r\nstruct ttm_buffer_object *bo;\r\nfor (i = 0; i < sw_context->cur_reloc; ++i) {\r\nreloc = &sw_context->relocs[i];\r\nvalidate = &sw_context->val_bufs[reloc->index].base;\r\nbo = validate->bo;\r\nswitch (bo->mem.mem_type) {\r\ncase TTM_PL_VRAM:\r\nreloc->location->offset += bo->offset;\r\nreloc->location->gmrId = SVGA_GMR_FRAMEBUFFER;\r\nbreak;\r\ncase VMW_PL_GMR:\r\nreloc->location->gmrId = bo->mem.start;\r\nbreak;\r\ncase VMW_PL_MOB:\r\n*reloc->mob_loc = bo->mem.start;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nvmw_free_relocations(sw_context);\r\n}\r\nstatic void vmw_resource_list_unreference(struct list_head *list)\r\n{\r\nstruct vmw_resource_val_node *val, *val_next;\r\nlist_for_each_entry_safe(val, val_next, list, head) {\r\nlist_del_init(&val->head);\r\nvmw_resource_unreference(&val->res);\r\nif (unlikely(val->staged_bindings))\r\nkfree(val->staged_bindings);\r\nkfree(val);\r\n}\r\n}\r\nstatic void vmw_clear_validations(struct vmw_sw_context *sw_context)\r\n{\r\nstruct vmw_validate_buffer *entry, *next;\r\nstruct vmw_resource_val_node *val;\r\nlist_for_each_entry_safe(entry, next, &sw_context->validate_nodes,\r\nbase.head) {\r\nlist_del(&entry->base.head);\r\nttm_bo_unref(&entry->base.bo);\r\n(void) drm_ht_remove_item(&sw_context->res_ht, &entry->hash);\r\nsw_context->cur_val_buf--;\r\n}\r\nBUG_ON(sw_context->cur_val_buf != 0);\r\nlist_for_each_entry(val, &sw_context->resource_list, head)\r\n(void) drm_ht_remove_item(&sw_context->res_ht, &val->hash);\r\n}\r\nstatic int vmw_validate_single_buffer(struct vmw_private *dev_priv,\r\nstruct ttm_buffer_object *bo,\r\nbool validate_as_mob)\r\n{\r\nint ret;\r\nif (bo == dev_priv->pinned_bo ||\r\n(bo == dev_priv->dummy_query_bo &&\r\ndev_priv->dummy_query_bo_pinned))\r\nreturn 0;\r\nif (validate_as_mob)\r\nreturn ttm_bo_validate(bo, &vmw_mob_placement, true, false);\r\nret = ttm_bo_validate(bo, &vmw_vram_gmr_placement, true, false);\r\nif (likely(ret == 0 || ret == -ERESTARTSYS))\r\nreturn ret;\r\nDRM_INFO("Falling through to VRAM.\n");\r\nret = ttm_bo_validate(bo, &vmw_vram_placement, true, false);\r\nreturn ret;\r\n}\r\nstatic int vmw_validate_buffers(struct vmw_private *dev_priv,\r\nstruct vmw_sw_context *sw_context)\r\n{\r\nstruct vmw_validate_buffer *entry;\r\nint ret;\r\nlist_for_each_entry(entry, &sw_context->validate_nodes, base.head) {\r\nret = vmw_validate_single_buffer(dev_priv, entry->base.bo,\r\nentry->validate_as_mob);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int vmw_resize_cmd_bounce(struct vmw_sw_context *sw_context,\r\nuint32_t size)\r\n{\r\nif (likely(sw_context->cmd_bounce_size >= size))\r\nreturn 0;\r\nif (sw_context->cmd_bounce_size == 0)\r\nsw_context->cmd_bounce_size = VMWGFX_CMD_BOUNCE_INIT_SIZE;\r\nwhile (sw_context->cmd_bounce_size < size) {\r\nsw_context->cmd_bounce_size =\r\nPAGE_ALIGN(sw_context->cmd_bounce_size +\r\n(sw_context->cmd_bounce_size >> 1));\r\n}\r\nif (sw_context->cmd_bounce != NULL)\r\nvfree(sw_context->cmd_bounce);\r\nsw_context->cmd_bounce = vmalloc(sw_context->cmd_bounce_size);\r\nif (sw_context->cmd_bounce == NULL) {\r\nDRM_ERROR("Failed to allocate command bounce buffer.\n");\r\nsw_context->cmd_bounce_size = 0;\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nint vmw_execbuf_fence_commands(struct drm_file *file_priv,\r\nstruct vmw_private *dev_priv,\r\nstruct vmw_fence_obj **p_fence,\r\nuint32_t *p_handle)\r\n{\r\nuint32_t sequence;\r\nint ret;\r\nbool synced = false;\r\nBUG_ON(p_handle != NULL && file_priv == NULL);\r\nret = vmw_fifo_send_fence(dev_priv, &sequence);\r\nif (unlikely(ret != 0)) {\r\nDRM_ERROR("Fence submission error. Syncing.\n");\r\nsynced = true;\r\n}\r\nif (p_handle != NULL)\r\nret = vmw_user_fence_create(file_priv, dev_priv->fman,\r\nsequence, p_fence, p_handle);\r\nelse\r\nret = vmw_fence_create(dev_priv->fman, sequence, p_fence);\r\nif (unlikely(ret != 0 && !synced)) {\r\n(void) vmw_fallback_wait(dev_priv, false, false,\r\nsequence, false,\r\nVMW_FENCE_WAIT_TIMEOUT);\r\n*p_fence = NULL;\r\n}\r\nreturn 0;\r\n}\r\nvoid\r\nvmw_execbuf_copy_fence_user(struct vmw_private *dev_priv,\r\nstruct vmw_fpriv *vmw_fp,\r\nint ret,\r\nstruct drm_vmw_fence_rep __user *user_fence_rep,\r\nstruct vmw_fence_obj *fence,\r\nuint32_t fence_handle)\r\n{\r\nstruct drm_vmw_fence_rep fence_rep;\r\nif (user_fence_rep == NULL)\r\nreturn;\r\nmemset(&fence_rep, 0, sizeof(fence_rep));\r\nfence_rep.error = ret;\r\nif (ret == 0) {\r\nBUG_ON(fence == NULL);\r\nfence_rep.handle = fence_handle;\r\nfence_rep.seqno = fence->base.seqno;\r\nvmw_update_seqno(dev_priv, &dev_priv->fifo);\r\nfence_rep.passed_seqno = dev_priv->last_read_seqno;\r\n}\r\nret = copy_to_user(user_fence_rep, &fence_rep,\r\nsizeof(fence_rep));\r\nif (unlikely(ret != 0) && (fence_rep.error == 0)) {\r\nttm_ref_object_base_unref(vmw_fp->tfile,\r\nfence_handle, TTM_REF_USAGE);\r\nDRM_ERROR("Fence copy error. Syncing.\n");\r\n(void) vmw_fence_obj_wait(fence, false, false,\r\nVMW_FENCE_WAIT_TIMEOUT);\r\n}\r\n}\r\nint vmw_execbuf_process(struct drm_file *file_priv,\r\nstruct vmw_private *dev_priv,\r\nvoid __user *user_commands,\r\nvoid *kernel_commands,\r\nuint32_t command_size,\r\nuint64_t throttle_us,\r\nstruct drm_vmw_fence_rep __user *user_fence_rep,\r\nstruct vmw_fence_obj **out_fence)\r\n{\r\nstruct vmw_sw_context *sw_context = &dev_priv->ctx;\r\nstruct vmw_fence_obj *fence = NULL;\r\nstruct vmw_resource *error_resource;\r\nstruct list_head resource_list;\r\nstruct ww_acquire_ctx ticket;\r\nuint32_t handle;\r\nvoid *cmd;\r\nint ret;\r\nret = mutex_lock_interruptible(&dev_priv->cmdbuf_mutex);\r\nif (unlikely(ret != 0))\r\nreturn -ERESTARTSYS;\r\nif (kernel_commands == NULL) {\r\nsw_context->kernel = false;\r\nret = vmw_resize_cmd_bounce(sw_context, command_size);\r\nif (unlikely(ret != 0))\r\ngoto out_unlock;\r\nret = copy_from_user(sw_context->cmd_bounce,\r\nuser_commands, command_size);\r\nif (unlikely(ret != 0)) {\r\nret = -EFAULT;\r\nDRM_ERROR("Failed copying commands.\n");\r\ngoto out_unlock;\r\n}\r\nkernel_commands = sw_context->cmd_bounce;\r\n} else\r\nsw_context->kernel = true;\r\nsw_context->fp = vmw_fpriv(file_priv);\r\nsw_context->cur_reloc = 0;\r\nsw_context->cur_val_buf = 0;\r\nINIT_LIST_HEAD(&sw_context->resource_list);\r\nsw_context->cur_query_bo = dev_priv->pinned_bo;\r\nsw_context->last_query_ctx = NULL;\r\nsw_context->needs_post_query_barrier = false;\r\nmemset(sw_context->res_cache, 0, sizeof(sw_context->res_cache));\r\nINIT_LIST_HEAD(&sw_context->validate_nodes);\r\nINIT_LIST_HEAD(&sw_context->res_relocations);\r\nif (!sw_context->res_ht_initialized) {\r\nret = drm_ht_create(&sw_context->res_ht, VMW_RES_HT_ORDER);\r\nif (unlikely(ret != 0))\r\ngoto out_unlock;\r\nsw_context->res_ht_initialized = true;\r\n}\r\nINIT_LIST_HEAD(&sw_context->staged_cmd_res);\r\nINIT_LIST_HEAD(&resource_list);\r\nret = vmw_cmd_check_all(dev_priv, sw_context, kernel_commands,\r\ncommand_size);\r\nif (unlikely(ret != 0))\r\ngoto out_err_nores;\r\nret = vmw_resources_reserve(sw_context);\r\nif (unlikely(ret != 0))\r\ngoto out_err_nores;\r\nret = ttm_eu_reserve_buffers(&ticket, &sw_context->validate_nodes,\r\ntrue, NULL);\r\nif (unlikely(ret != 0))\r\ngoto out_err;\r\nret = vmw_validate_buffers(dev_priv, sw_context);\r\nif (unlikely(ret != 0))\r\ngoto out_err;\r\nret = vmw_resources_validate(sw_context);\r\nif (unlikely(ret != 0))\r\ngoto out_err;\r\nif (throttle_us) {\r\nret = vmw_wait_lag(dev_priv, &dev_priv->fifo.marker_queue,\r\nthrottle_us);\r\nif (unlikely(ret != 0))\r\ngoto out_err;\r\n}\r\nret = mutex_lock_interruptible(&dev_priv->binding_mutex);\r\nif (unlikely(ret != 0)) {\r\nret = -ERESTARTSYS;\r\ngoto out_err;\r\n}\r\nif (dev_priv->has_mob) {\r\nret = vmw_rebind_contexts(sw_context);\r\nif (unlikely(ret != 0))\r\ngoto out_unlock_binding;\r\n}\r\ncmd = vmw_fifo_reserve(dev_priv, command_size);\r\nif (unlikely(cmd == NULL)) {\r\nDRM_ERROR("Failed reserving fifo space for commands.\n");\r\nret = -ENOMEM;\r\ngoto out_unlock_binding;\r\n}\r\nvmw_apply_relocations(sw_context);\r\nmemcpy(cmd, kernel_commands, command_size);\r\nvmw_resource_relocations_apply(cmd, &sw_context->res_relocations);\r\nvmw_resource_relocations_free(&sw_context->res_relocations);\r\nvmw_fifo_commit(dev_priv, command_size);\r\nvmw_query_bo_switch_commit(dev_priv, sw_context);\r\nret = vmw_execbuf_fence_commands(file_priv, dev_priv,\r\n&fence,\r\n(user_fence_rep) ? &handle : NULL);\r\nif (ret != 0)\r\nDRM_ERROR("Fence submission error. Syncing.\n");\r\nvmw_resource_list_unreserve(&sw_context->resource_list, false);\r\nmutex_unlock(&dev_priv->binding_mutex);\r\nttm_eu_fence_buffer_objects(&ticket, &sw_context->validate_nodes,\r\n(void *) fence);\r\nif (unlikely(dev_priv->pinned_bo != NULL &&\r\n!dev_priv->query_cid_valid))\r\n__vmw_execbuf_release_pinned_bo(dev_priv, fence);\r\nvmw_clear_validations(sw_context);\r\nvmw_execbuf_copy_fence_user(dev_priv, vmw_fpriv(file_priv), ret,\r\nuser_fence_rep, fence, handle);\r\nif (unlikely(out_fence != NULL)) {\r\n*out_fence = fence;\r\nfence = NULL;\r\n} else if (likely(fence != NULL)) {\r\nvmw_fence_obj_unreference(&fence);\r\n}\r\nlist_splice_init(&sw_context->resource_list, &resource_list);\r\nvmw_cmdbuf_res_commit(&sw_context->staged_cmd_res);\r\nmutex_unlock(&dev_priv->cmdbuf_mutex);\r\nvmw_resource_list_unreference(&resource_list);\r\nreturn 0;\r\nout_unlock_binding:\r\nmutex_unlock(&dev_priv->binding_mutex);\r\nout_err:\r\nttm_eu_backoff_reservation(&ticket, &sw_context->validate_nodes);\r\nout_err_nores:\r\nvmw_resource_list_unreserve(&sw_context->resource_list, true);\r\nvmw_resource_relocations_free(&sw_context->res_relocations);\r\nvmw_free_relocations(sw_context);\r\nvmw_clear_validations(sw_context);\r\nif (unlikely(dev_priv->pinned_bo != NULL &&\r\n!dev_priv->query_cid_valid))\r\n__vmw_execbuf_release_pinned_bo(dev_priv, NULL);\r\nout_unlock:\r\nlist_splice_init(&sw_context->resource_list, &resource_list);\r\nerror_resource = sw_context->error_resource;\r\nsw_context->error_resource = NULL;\r\nvmw_cmdbuf_res_revert(&sw_context->staged_cmd_res);\r\nmutex_unlock(&dev_priv->cmdbuf_mutex);\r\nvmw_resource_list_unreference(&resource_list);\r\nif (unlikely(error_resource != NULL))\r\nvmw_resource_unreference(&error_resource);\r\nreturn ret;\r\n}\r\nstatic void vmw_execbuf_unpin_panic(struct vmw_private *dev_priv)\r\n{\r\nDRM_ERROR("Can't unpin query buffer. Trying to recover.\n");\r\n(void) vmw_fallback_wait(dev_priv, false, true, 0, false, 10*HZ);\r\nvmw_bo_pin(dev_priv->pinned_bo, false);\r\nvmw_bo_pin(dev_priv->dummy_query_bo, false);\r\ndev_priv->dummy_query_bo_pinned = false;\r\n}\r\nvoid __vmw_execbuf_release_pinned_bo(struct vmw_private *dev_priv,\r\nstruct vmw_fence_obj *fence)\r\n{\r\nint ret = 0;\r\nstruct list_head validate_list;\r\nstruct ttm_validate_buffer pinned_val, query_val;\r\nstruct vmw_fence_obj *lfence = NULL;\r\nstruct ww_acquire_ctx ticket;\r\nif (dev_priv->pinned_bo == NULL)\r\ngoto out_unlock;\r\nINIT_LIST_HEAD(&validate_list);\r\npinned_val.bo = ttm_bo_reference(dev_priv->pinned_bo);\r\npinned_val.shared = false;\r\nlist_add_tail(&pinned_val.head, &validate_list);\r\nquery_val.bo = ttm_bo_reference(dev_priv->dummy_query_bo);\r\nquery_val.shared = false;\r\nlist_add_tail(&query_val.head, &validate_list);\r\nret = ttm_eu_reserve_buffers(&ticket, &validate_list,\r\nfalse, NULL);\r\nif (unlikely(ret != 0)) {\r\nvmw_execbuf_unpin_panic(dev_priv);\r\ngoto out_no_reserve;\r\n}\r\nif (dev_priv->query_cid_valid) {\r\nBUG_ON(fence != NULL);\r\nret = vmw_fifo_emit_dummy_query(dev_priv, dev_priv->query_cid);\r\nif (unlikely(ret != 0)) {\r\nvmw_execbuf_unpin_panic(dev_priv);\r\ngoto out_no_emit;\r\n}\r\ndev_priv->query_cid_valid = false;\r\n}\r\nvmw_bo_pin(dev_priv->pinned_bo, false);\r\nvmw_bo_pin(dev_priv->dummy_query_bo, false);\r\ndev_priv->dummy_query_bo_pinned = false;\r\nif (fence == NULL) {\r\n(void) vmw_execbuf_fence_commands(NULL, dev_priv, &lfence,\r\nNULL);\r\nfence = lfence;\r\n}\r\nttm_eu_fence_buffer_objects(&ticket, &validate_list, (void *) fence);\r\nif (lfence != NULL)\r\nvmw_fence_obj_unreference(&lfence);\r\nttm_bo_unref(&query_val.bo);\r\nttm_bo_unref(&pinned_val.bo);\r\nttm_bo_unref(&dev_priv->pinned_bo);\r\nout_unlock:\r\nreturn;\r\nout_no_emit:\r\nttm_eu_backoff_reservation(&ticket, &validate_list);\r\nout_no_reserve:\r\nttm_bo_unref(&query_val.bo);\r\nttm_bo_unref(&pinned_val.bo);\r\nttm_bo_unref(&dev_priv->pinned_bo);\r\n}\r\nvoid vmw_execbuf_release_pinned_bo(struct vmw_private *dev_priv)\r\n{\r\nmutex_lock(&dev_priv->cmdbuf_mutex);\r\nif (dev_priv->query_cid_valid)\r\n__vmw_execbuf_release_pinned_bo(dev_priv, NULL);\r\nmutex_unlock(&dev_priv->cmdbuf_mutex);\r\n}\r\nint vmw_execbuf_ioctl(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct vmw_private *dev_priv = vmw_priv(dev);\r\nstruct drm_vmw_execbuf_arg *arg = (struct drm_vmw_execbuf_arg *)data;\r\nint ret;\r\nif (unlikely(arg->version != DRM_VMW_EXECBUF_VERSION)) {\r\nDRM_ERROR("Incorrect execbuf version.\n");\r\nDRM_ERROR("You're running outdated experimental "\r\n"vmwgfx user-space drivers.");\r\nreturn -EINVAL;\r\n}\r\nret = ttm_read_lock(&dev_priv->reservation_sem, true);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = vmw_execbuf_process(file_priv, dev_priv,\r\n(void __user *)(unsigned long)arg->commands,\r\nNULL, arg->command_size, arg->throttle_us,\r\n(void __user *)(unsigned long)arg->fence_rep,\r\nNULL);\r\nttm_read_unlock(&dev_priv->reservation_sem);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nvmw_kms_cursor_post_execbuf(dev_priv);\r\nreturn 0;\r\n}
