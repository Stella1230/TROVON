static int __init setup_slab_nomerge(char *str)\r\n{\r\nslab_nomerge = 1;\r\nreturn 1;\r\n}\r\nunsigned int kmem_cache_size(struct kmem_cache *s)\r\n{\r\nreturn s->object_size;\r\n}\r\nstatic int kmem_cache_sanity_check(const char *name, size_t size)\r\n{\r\nstruct kmem_cache *s = NULL;\r\nif (!name || in_interrupt() || size < sizeof(void *) ||\r\nsize > KMALLOC_MAX_SIZE) {\r\npr_err("kmem_cache_create(%s) integrity check failed\n", name);\r\nreturn -EINVAL;\r\n}\r\nlist_for_each_entry(s, &slab_caches, list) {\r\nchar tmp;\r\nint res;\r\nres = probe_kernel_address(s->name, tmp);\r\nif (res) {\r\npr_err("Slab cache with size %d has lost its name\n",\r\ns->object_size);\r\ncontinue;\r\n}\r\n}\r\nWARN_ON(strchr(name, ' '));\r\nreturn 0;\r\n}\r\nstatic inline int kmem_cache_sanity_check(const char *name, size_t size)\r\n{\r\nreturn 0;\r\n}\r\nvoid slab_init_memcg_params(struct kmem_cache *s)\r\n{\r\ns->memcg_params.is_root_cache = true;\r\nINIT_LIST_HEAD(&s->memcg_params.list);\r\nRCU_INIT_POINTER(s->memcg_params.memcg_caches, NULL);\r\n}\r\nstatic int init_memcg_params(struct kmem_cache *s,\r\nstruct mem_cgroup *memcg, struct kmem_cache *root_cache)\r\n{\r\nstruct memcg_cache_array *arr;\r\nif (memcg) {\r\ns->memcg_params.is_root_cache = false;\r\ns->memcg_params.memcg = memcg;\r\ns->memcg_params.root_cache = root_cache;\r\nreturn 0;\r\n}\r\nslab_init_memcg_params(s);\r\nif (!memcg_nr_cache_ids)\r\nreturn 0;\r\narr = kzalloc(sizeof(struct memcg_cache_array) +\r\nmemcg_nr_cache_ids * sizeof(void *),\r\nGFP_KERNEL);\r\nif (!arr)\r\nreturn -ENOMEM;\r\nRCU_INIT_POINTER(s->memcg_params.memcg_caches, arr);\r\nreturn 0;\r\n}\r\nstatic void destroy_memcg_params(struct kmem_cache *s)\r\n{\r\nif (is_root_cache(s))\r\nkfree(rcu_access_pointer(s->memcg_params.memcg_caches));\r\n}\r\nstatic int update_memcg_params(struct kmem_cache *s, int new_array_size)\r\n{\r\nstruct memcg_cache_array *old, *new;\r\nif (!is_root_cache(s))\r\nreturn 0;\r\nnew = kzalloc(sizeof(struct memcg_cache_array) +\r\nnew_array_size * sizeof(void *), GFP_KERNEL);\r\nif (!new)\r\nreturn -ENOMEM;\r\nold = rcu_dereference_protected(s->memcg_params.memcg_caches,\r\nlockdep_is_held(&slab_mutex));\r\nif (old)\r\nmemcpy(new->entries, old->entries,\r\nmemcg_nr_cache_ids * sizeof(void *));\r\nrcu_assign_pointer(s->memcg_params.memcg_caches, new);\r\nif (old)\r\nkfree_rcu(old, rcu);\r\nreturn 0;\r\n}\r\nint memcg_update_all_caches(int num_memcgs)\r\n{\r\nstruct kmem_cache *s;\r\nint ret = 0;\r\nmutex_lock(&slab_mutex);\r\nlist_for_each_entry(s, &slab_caches, list) {\r\nret = update_memcg_params(s, num_memcgs);\r\nif (ret)\r\nbreak;\r\n}\r\nmutex_unlock(&slab_mutex);\r\nreturn ret;\r\n}\r\nstatic inline int init_memcg_params(struct kmem_cache *s,\r\nstruct mem_cgroup *memcg, struct kmem_cache *root_cache)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void destroy_memcg_params(struct kmem_cache *s)\r\n{\r\n}\r\nint slab_unmergeable(struct kmem_cache *s)\r\n{\r\nif (slab_nomerge || (s->flags & SLAB_NEVER_MERGE))\r\nreturn 1;\r\nif (!is_root_cache(s))\r\nreturn 1;\r\nif (s->ctor)\r\nreturn 1;\r\nif (s->refcount < 0)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstruct kmem_cache *find_mergeable(size_t size, size_t align,\r\nunsigned long flags, const char *name, void (*ctor)(void *))\r\n{\r\nstruct kmem_cache *s;\r\nif (slab_nomerge || (flags & SLAB_NEVER_MERGE))\r\nreturn NULL;\r\nif (ctor)\r\nreturn NULL;\r\nsize = ALIGN(size, sizeof(void *));\r\nalign = calculate_alignment(flags, align, size);\r\nsize = ALIGN(size, align);\r\nflags = kmem_cache_flags(size, flags, name, NULL);\r\nlist_for_each_entry_reverse(s, &slab_caches, list) {\r\nif (slab_unmergeable(s))\r\ncontinue;\r\nif (size > s->size)\r\ncontinue;\r\nif ((flags & SLAB_MERGE_SAME) != (s->flags & SLAB_MERGE_SAME))\r\ncontinue;\r\nif ((s->size & ~(align - 1)) != s->size)\r\ncontinue;\r\nif (s->size - size >= sizeof(void *))\r\ncontinue;\r\nif (IS_ENABLED(CONFIG_SLAB) && align &&\r\n(align > s->align || s->align % align))\r\ncontinue;\r\nreturn s;\r\n}\r\nreturn NULL;\r\n}\r\nunsigned long calculate_alignment(unsigned long flags,\r\nunsigned long align, unsigned long size)\r\n{\r\nif (flags & SLAB_HWCACHE_ALIGN) {\r\nunsigned long ralign = cache_line_size();\r\nwhile (size <= ralign / 2)\r\nralign /= 2;\r\nalign = max(align, ralign);\r\n}\r\nif (align < ARCH_SLAB_MINALIGN)\r\nalign = ARCH_SLAB_MINALIGN;\r\nreturn ALIGN(align, sizeof(void *));\r\n}\r\nstatic struct kmem_cache *\r\ndo_kmem_cache_create(const char *name, size_t object_size, size_t size,\r\nsize_t align, unsigned long flags, void (*ctor)(void *),\r\nstruct mem_cgroup *memcg, struct kmem_cache *root_cache)\r\n{\r\nstruct kmem_cache *s;\r\nint err;\r\nerr = -ENOMEM;\r\ns = kmem_cache_zalloc(kmem_cache, GFP_KERNEL);\r\nif (!s)\r\ngoto out;\r\ns->name = name;\r\ns->object_size = object_size;\r\ns->size = size;\r\ns->align = align;\r\ns->ctor = ctor;\r\nerr = init_memcg_params(s, memcg, root_cache);\r\nif (err)\r\ngoto out_free_cache;\r\nerr = __kmem_cache_create(s, flags);\r\nif (err)\r\ngoto out_free_cache;\r\ns->refcount = 1;\r\nlist_add(&s->list, &slab_caches);\r\nout:\r\nif (err)\r\nreturn ERR_PTR(err);\r\nreturn s;\r\nout_free_cache:\r\ndestroy_memcg_params(s);\r\nkmem_cache_free(kmem_cache, s);\r\ngoto out;\r\n}\r\nstruct kmem_cache *\r\nkmem_cache_create(const char *name, size_t size, size_t align,\r\nunsigned long flags, void (*ctor)(void *))\r\n{\r\nstruct kmem_cache *s;\r\nconst char *cache_name;\r\nint err;\r\nget_online_cpus();\r\nget_online_mems();\r\nmemcg_get_cache_ids();\r\nmutex_lock(&slab_mutex);\r\nerr = kmem_cache_sanity_check(name, size);\r\nif (err) {\r\ns = NULL;\r\ngoto out_unlock;\r\n}\r\nflags &= CACHE_CREATE_MASK;\r\ns = __kmem_cache_alias(name, size, align, flags, ctor);\r\nif (s)\r\ngoto out_unlock;\r\ncache_name = kstrdup_const(name, GFP_KERNEL);\r\nif (!cache_name) {\r\nerr = -ENOMEM;\r\ngoto out_unlock;\r\n}\r\ns = do_kmem_cache_create(cache_name, size, size,\r\ncalculate_alignment(flags, align, size),\r\nflags, ctor, NULL, NULL);\r\nif (IS_ERR(s)) {\r\nerr = PTR_ERR(s);\r\nkfree_const(cache_name);\r\n}\r\nout_unlock:\r\nmutex_unlock(&slab_mutex);\r\nmemcg_put_cache_ids();\r\nput_online_mems();\r\nput_online_cpus();\r\nif (err) {\r\nif (flags & SLAB_PANIC)\r\npanic("kmem_cache_create: Failed to create slab '%s'. Error %d\n",\r\nname, err);\r\nelse {\r\nprintk(KERN_WARNING "kmem_cache_create(%s) failed with error %d",\r\nname, err);\r\ndump_stack();\r\n}\r\nreturn NULL;\r\n}\r\nreturn s;\r\n}\r\nstatic int do_kmem_cache_shutdown(struct kmem_cache *s,\r\nstruct list_head *release, bool *need_rcu_barrier)\r\n{\r\nif (__kmem_cache_shutdown(s) != 0) {\r\nprintk(KERN_ERR "kmem_cache_destroy %s: "\r\n"Slab cache still has objects\n", s->name);\r\ndump_stack();\r\nreturn -EBUSY;\r\n}\r\nif (s->flags & SLAB_DESTROY_BY_RCU)\r\n*need_rcu_barrier = true;\r\n#ifdef CONFIG_MEMCG_KMEM\r\nif (!is_root_cache(s))\r\nlist_del(&s->memcg_params.list);\r\n#endif\r\nlist_move(&s->list, release);\r\nreturn 0;\r\n}\r\nstatic void do_kmem_cache_release(struct list_head *release,\r\nbool need_rcu_barrier)\r\n{\r\nstruct kmem_cache *s, *s2;\r\nif (need_rcu_barrier)\r\nrcu_barrier();\r\nlist_for_each_entry_safe(s, s2, release, list) {\r\n#ifdef SLAB_SUPPORTS_SYSFS\r\nsysfs_slab_remove(s);\r\n#else\r\nslab_kmem_cache_release(s);\r\n#endif\r\n}\r\n}\r\nvoid memcg_create_kmem_cache(struct mem_cgroup *memcg,\r\nstruct kmem_cache *root_cache)\r\n{\r\nstatic char memcg_name_buf[NAME_MAX + 1];\r\nstruct cgroup_subsys_state *css = mem_cgroup_css(memcg);\r\nstruct memcg_cache_array *arr;\r\nstruct kmem_cache *s = NULL;\r\nchar *cache_name;\r\nint idx;\r\nget_online_cpus();\r\nget_online_mems();\r\nmutex_lock(&slab_mutex);\r\nif (!memcg_kmem_is_active(memcg))\r\ngoto out_unlock;\r\nidx = memcg_cache_id(memcg);\r\narr = rcu_dereference_protected(root_cache->memcg_params.memcg_caches,\r\nlockdep_is_held(&slab_mutex));\r\nif (arr->entries[idx])\r\ngoto out_unlock;\r\ncgroup_name(css->cgroup, memcg_name_buf, sizeof(memcg_name_buf));\r\ncache_name = kasprintf(GFP_KERNEL, "%s(%d:%s)", root_cache->name,\r\ncss->id, memcg_name_buf);\r\nif (!cache_name)\r\ngoto out_unlock;\r\ns = do_kmem_cache_create(cache_name, root_cache->object_size,\r\nroot_cache->size, root_cache->align,\r\nroot_cache->flags, root_cache->ctor,\r\nmemcg, root_cache);\r\nif (IS_ERR(s)) {\r\nkfree(cache_name);\r\ngoto out_unlock;\r\n}\r\nlist_add(&s->memcg_params.list, &root_cache->memcg_params.list);\r\nsmp_wmb();\r\narr->entries[idx] = s;\r\nout_unlock:\r\nmutex_unlock(&slab_mutex);\r\nput_online_mems();\r\nput_online_cpus();\r\n}\r\nvoid memcg_deactivate_kmem_caches(struct mem_cgroup *memcg)\r\n{\r\nint idx;\r\nstruct memcg_cache_array *arr;\r\nstruct kmem_cache *s, *c;\r\nidx = memcg_cache_id(memcg);\r\nget_online_cpus();\r\nget_online_mems();\r\nmutex_lock(&slab_mutex);\r\nlist_for_each_entry(s, &slab_caches, list) {\r\nif (!is_root_cache(s))\r\ncontinue;\r\narr = rcu_dereference_protected(s->memcg_params.memcg_caches,\r\nlockdep_is_held(&slab_mutex));\r\nc = arr->entries[idx];\r\nif (!c)\r\ncontinue;\r\n__kmem_cache_shrink(c, true);\r\narr->entries[idx] = NULL;\r\n}\r\nmutex_unlock(&slab_mutex);\r\nput_online_mems();\r\nput_online_cpus();\r\n}\r\nvoid memcg_destroy_kmem_caches(struct mem_cgroup *memcg)\r\n{\r\nLIST_HEAD(release);\r\nbool need_rcu_barrier = false;\r\nstruct kmem_cache *s, *s2;\r\nget_online_cpus();\r\nget_online_mems();\r\nmutex_lock(&slab_mutex);\r\nlist_for_each_entry_safe(s, s2, &slab_caches, list) {\r\nif (is_root_cache(s) || s->memcg_params.memcg != memcg)\r\ncontinue;\r\nBUG_ON(do_kmem_cache_shutdown(s, &release, &need_rcu_barrier));\r\n}\r\nmutex_unlock(&slab_mutex);\r\nput_online_mems();\r\nput_online_cpus();\r\ndo_kmem_cache_release(&release, need_rcu_barrier);\r\n}\r\nvoid slab_kmem_cache_release(struct kmem_cache *s)\r\n{\r\ndestroy_memcg_params(s);\r\nkfree_const(s->name);\r\nkmem_cache_free(kmem_cache, s);\r\n}\r\nvoid kmem_cache_destroy(struct kmem_cache *s)\r\n{\r\nstruct kmem_cache *c, *c2;\r\nLIST_HEAD(release);\r\nbool need_rcu_barrier = false;\r\nbool busy = false;\r\nBUG_ON(!is_root_cache(s));\r\nget_online_cpus();\r\nget_online_mems();\r\nmutex_lock(&slab_mutex);\r\ns->refcount--;\r\nif (s->refcount)\r\ngoto out_unlock;\r\nfor_each_memcg_cache_safe(c, c2, s) {\r\nif (do_kmem_cache_shutdown(c, &release, &need_rcu_barrier))\r\nbusy = true;\r\n}\r\nif (!busy)\r\ndo_kmem_cache_shutdown(s, &release, &need_rcu_barrier);\r\nout_unlock:\r\nmutex_unlock(&slab_mutex);\r\nput_online_mems();\r\nput_online_cpus();\r\ndo_kmem_cache_release(&release, need_rcu_barrier);\r\n}\r\nint kmem_cache_shrink(struct kmem_cache *cachep)\r\n{\r\nint ret;\r\nget_online_cpus();\r\nget_online_mems();\r\nret = __kmem_cache_shrink(cachep, false);\r\nput_online_mems();\r\nput_online_cpus();\r\nreturn ret;\r\n}\r\nint slab_is_available(void)\r\n{\r\nreturn slab_state >= UP;\r\n}\r\nvoid __init create_boot_cache(struct kmem_cache *s, const char *name, size_t size,\r\nunsigned long flags)\r\n{\r\nint err;\r\ns->name = name;\r\ns->size = s->object_size = size;\r\ns->align = calculate_alignment(flags, ARCH_KMALLOC_MINALIGN, size);\r\nslab_init_memcg_params(s);\r\nerr = __kmem_cache_create(s, flags);\r\nif (err)\r\npanic("Creation of kmalloc slab %s size=%zu failed. Reason %d\n",\r\nname, size, err);\r\ns->refcount = -1;\r\n}\r\nstruct kmem_cache *__init create_kmalloc_cache(const char *name, size_t size,\r\nunsigned long flags)\r\n{\r\nstruct kmem_cache *s = kmem_cache_zalloc(kmem_cache, GFP_NOWAIT);\r\nif (!s)\r\npanic("Out of memory when creating slab %s\n", name);\r\ncreate_boot_cache(s, name, size, flags);\r\nlist_add(&s->list, &slab_caches);\r\ns->refcount = 1;\r\nreturn s;\r\n}\r\nstatic inline int size_index_elem(size_t bytes)\r\n{\r\nreturn (bytes - 1) / 8;\r\n}\r\nstruct kmem_cache *kmalloc_slab(size_t size, gfp_t flags)\r\n{\r\nint index;\r\nif (unlikely(size > KMALLOC_MAX_SIZE)) {\r\nWARN_ON_ONCE(!(flags & __GFP_NOWARN));\r\nreturn NULL;\r\n}\r\nif (size <= 192) {\r\nif (!size)\r\nreturn ZERO_SIZE_PTR;\r\nindex = size_index[size_index_elem(size)];\r\n} else\r\nindex = fls(size - 1);\r\n#ifdef CONFIG_ZONE_DMA\r\nif (unlikely((flags & GFP_DMA)))\r\nreturn kmalloc_dma_caches[index];\r\n#endif\r\nreturn kmalloc_caches[index];\r\n}\r\nvoid __init create_kmalloc_caches(unsigned long flags)\r\n{\r\nint i;\r\nBUILD_BUG_ON(KMALLOC_MIN_SIZE > 256 ||\r\n(KMALLOC_MIN_SIZE & (KMALLOC_MIN_SIZE - 1)));\r\nfor (i = 8; i < KMALLOC_MIN_SIZE; i += 8) {\r\nint elem = size_index_elem(i);\r\nif (elem >= ARRAY_SIZE(size_index))\r\nbreak;\r\nsize_index[elem] = KMALLOC_SHIFT_LOW;\r\n}\r\nif (KMALLOC_MIN_SIZE >= 64) {\r\nfor (i = 64 + 8; i <= 96; i += 8)\r\nsize_index[size_index_elem(i)] = 7;\r\n}\r\nif (KMALLOC_MIN_SIZE >= 128) {\r\nfor (i = 128 + 8; i <= 192; i += 8)\r\nsize_index[size_index_elem(i)] = 8;\r\n}\r\nfor (i = KMALLOC_SHIFT_LOW; i <= KMALLOC_SHIFT_HIGH; i++) {\r\nif (!kmalloc_caches[i]) {\r\nkmalloc_caches[i] = create_kmalloc_cache(NULL,\r\n1 << i, flags);\r\n}\r\nif (KMALLOC_MIN_SIZE <= 32 && !kmalloc_caches[1] && i == 6)\r\nkmalloc_caches[1] = create_kmalloc_cache(NULL, 96, flags);\r\nif (KMALLOC_MIN_SIZE <= 64 && !kmalloc_caches[2] && i == 7)\r\nkmalloc_caches[2] = create_kmalloc_cache(NULL, 192, flags);\r\n}\r\nslab_state = UP;\r\nfor (i = 0; i <= KMALLOC_SHIFT_HIGH; i++) {\r\nstruct kmem_cache *s = kmalloc_caches[i];\r\nchar *n;\r\nif (s) {\r\nn = kasprintf(GFP_NOWAIT, "kmalloc-%d", kmalloc_size(i));\r\nBUG_ON(!n);\r\ns->name = n;\r\n}\r\n}\r\n#ifdef CONFIG_ZONE_DMA\r\nfor (i = 0; i <= KMALLOC_SHIFT_HIGH; i++) {\r\nstruct kmem_cache *s = kmalloc_caches[i];\r\nif (s) {\r\nint size = kmalloc_size(i);\r\nchar *n = kasprintf(GFP_NOWAIT,\r\n"dma-kmalloc-%d", size);\r\nBUG_ON(!n);\r\nkmalloc_dma_caches[i] = create_kmalloc_cache(n,\r\nsize, SLAB_CACHE_DMA | flags);\r\n}\r\n}\r\n#endif\r\n}\r\nvoid *kmalloc_order(size_t size, gfp_t flags, unsigned int order)\r\n{\r\nvoid *ret;\r\nstruct page *page;\r\nflags |= __GFP_COMP;\r\npage = alloc_kmem_pages(flags, order);\r\nret = page ? page_address(page) : NULL;\r\nkmemleak_alloc(ret, size, 1, flags);\r\nkasan_kmalloc_large(ret, size);\r\nreturn ret;\r\n}\r\nvoid *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)\r\n{\r\nvoid *ret = kmalloc_order(size, flags, order);\r\ntrace_kmalloc(_RET_IP_, ret, size, PAGE_SIZE << order, flags);\r\nreturn ret;\r\n}\r\nstatic void print_slabinfo_header(struct seq_file *m)\r\n{\r\n#ifdef CONFIG_DEBUG_SLAB\r\nseq_puts(m, "slabinfo - version: 2.1 (statistics)\n");\r\n#else\r\nseq_puts(m, "slabinfo - version: 2.1\n");\r\n#endif\r\nseq_puts(m, "# name <active_objs> <num_objs> <objsize> "\r\n"<objperslab> <pagesperslab>");\r\nseq_puts(m, " : tunables <limit> <batchcount> <sharedfactor>");\r\nseq_puts(m, " : slabdata <active_slabs> <num_slabs> <sharedavail>");\r\n#ifdef CONFIG_DEBUG_SLAB\r\nseq_puts(m, " : globalstat <listallocs> <maxobjs> <grown> <reaped> "\r\n"<error> <maxfreeable> <nodeallocs> <remotefrees> <alienoverflow>");\r\nseq_puts(m, " : cpustat <allochit> <allocmiss> <freehit> <freemiss>");\r\n#endif\r\nseq_putc(m, '\n');\r\n}\r\nvoid *slab_start(struct seq_file *m, loff_t *pos)\r\n{\r\nmutex_lock(&slab_mutex);\r\nreturn seq_list_start(&slab_caches, *pos);\r\n}\r\nvoid *slab_next(struct seq_file *m, void *p, loff_t *pos)\r\n{\r\nreturn seq_list_next(p, &slab_caches, pos);\r\n}\r\nvoid slab_stop(struct seq_file *m, void *p)\r\n{\r\nmutex_unlock(&slab_mutex);\r\n}\r\nstatic void\r\nmemcg_accumulate_slabinfo(struct kmem_cache *s, struct slabinfo *info)\r\n{\r\nstruct kmem_cache *c;\r\nstruct slabinfo sinfo;\r\nif (!is_root_cache(s))\r\nreturn;\r\nfor_each_memcg_cache(c, s) {\r\nmemset(&sinfo, 0, sizeof(sinfo));\r\nget_slabinfo(c, &sinfo);\r\ninfo->active_slabs += sinfo.active_slabs;\r\ninfo->num_slabs += sinfo.num_slabs;\r\ninfo->shared_avail += sinfo.shared_avail;\r\ninfo->active_objs += sinfo.active_objs;\r\ninfo->num_objs += sinfo.num_objs;\r\n}\r\n}\r\nstatic void cache_show(struct kmem_cache *s, struct seq_file *m)\r\n{\r\nstruct slabinfo sinfo;\r\nmemset(&sinfo, 0, sizeof(sinfo));\r\nget_slabinfo(s, &sinfo);\r\nmemcg_accumulate_slabinfo(s, &sinfo);\r\nseq_printf(m, "%-17s %6lu %6lu %6u %4u %4d",\r\ncache_name(s), sinfo.active_objs, sinfo.num_objs, s->size,\r\nsinfo.objects_per_slab, (1 << sinfo.cache_order));\r\nseq_printf(m, " : tunables %4u %4u %4u",\r\nsinfo.limit, sinfo.batchcount, sinfo.shared);\r\nseq_printf(m, " : slabdata %6lu %6lu %6lu",\r\nsinfo.active_slabs, sinfo.num_slabs, sinfo.shared_avail);\r\nslabinfo_show_stats(m, s);\r\nseq_putc(m, '\n');\r\n}\r\nstatic int slab_show(struct seq_file *m, void *p)\r\n{\r\nstruct kmem_cache *s = list_entry(p, struct kmem_cache, list);\r\nif (p == slab_caches.next)\r\nprint_slabinfo_header(m);\r\nif (is_root_cache(s))\r\ncache_show(s, m);\r\nreturn 0;\r\n}\r\nint memcg_slab_show(struct seq_file *m, void *p)\r\n{\r\nstruct kmem_cache *s = list_entry(p, struct kmem_cache, list);\r\nstruct mem_cgroup *memcg = mem_cgroup_from_css(seq_css(m));\r\nif (p == slab_caches.next)\r\nprint_slabinfo_header(m);\r\nif (!is_root_cache(s) && s->memcg_params.memcg == memcg)\r\ncache_show(s, m);\r\nreturn 0;\r\n}\r\nstatic int slabinfo_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &slabinfo_op);\r\n}\r\nstatic int __init slab_proc_init(void)\r\n{\r\nproc_create("slabinfo", SLABINFO_RIGHTS, NULL,\r\n&proc_slabinfo_operations);\r\nreturn 0;\r\n}\r\nstatic __always_inline void *__do_krealloc(const void *p, size_t new_size,\r\ngfp_t flags)\r\n{\r\nvoid *ret;\r\nsize_t ks = 0;\r\nif (p)\r\nks = ksize(p);\r\nif (ks >= new_size) {\r\nkasan_krealloc((void *)p, new_size);\r\nreturn (void *)p;\r\n}\r\nret = kmalloc_track_caller(new_size, flags);\r\nif (ret && p)\r\nmemcpy(ret, p, ks);\r\nreturn ret;\r\n}\r\nvoid *__krealloc(const void *p, size_t new_size, gfp_t flags)\r\n{\r\nif (unlikely(!new_size))\r\nreturn ZERO_SIZE_PTR;\r\nreturn __do_krealloc(p, new_size, flags);\r\n}\r\nvoid *krealloc(const void *p, size_t new_size, gfp_t flags)\r\n{\r\nvoid *ret;\r\nif (unlikely(!new_size)) {\r\nkfree(p);\r\nreturn ZERO_SIZE_PTR;\r\n}\r\nret = __do_krealloc(p, new_size, flags);\r\nif (ret && p != ret)\r\nkfree(p);\r\nreturn ret;\r\n}\r\nvoid kzfree(const void *p)\r\n{\r\nsize_t ks;\r\nvoid *mem = (void *)p;\r\nif (unlikely(ZERO_OR_NULL_PTR(mem)))\r\nreturn;\r\nks = ksize(mem);\r\nmemset(mem, 0, ks);\r\nkfree(mem);\r\n}
