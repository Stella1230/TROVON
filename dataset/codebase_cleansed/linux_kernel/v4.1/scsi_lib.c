static void\r\nscsi_set_blocked(struct scsi_cmnd *cmd, int reason)\r\n{\r\nstruct Scsi_Host *host = cmd->device->host;\r\nstruct scsi_device *device = cmd->device;\r\nstruct scsi_target *starget = scsi_target(device);\r\nswitch (reason) {\r\ncase SCSI_MLQUEUE_HOST_BUSY:\r\natomic_set(&host->host_blocked, host->max_host_blocked);\r\nbreak;\r\ncase SCSI_MLQUEUE_DEVICE_BUSY:\r\ncase SCSI_MLQUEUE_EH_RETRY:\r\natomic_set(&device->device_blocked,\r\ndevice->max_device_blocked);\r\nbreak;\r\ncase SCSI_MLQUEUE_TARGET_BUSY:\r\natomic_set(&starget->target_blocked,\r\nstarget->max_target_blocked);\r\nbreak;\r\n}\r\n}\r\nstatic void scsi_mq_requeue_cmd(struct scsi_cmnd *cmd)\r\n{\r\nstruct scsi_device *sdev = cmd->device;\r\nstruct request_queue *q = cmd->request->q;\r\nblk_mq_requeue_request(cmd->request);\r\nblk_mq_kick_requeue_list(q);\r\nput_device(&sdev->sdev_gendev);\r\n}\r\nstatic void __scsi_queue_insert(struct scsi_cmnd *cmd, int reason, int unbusy)\r\n{\r\nstruct scsi_device *device = cmd->device;\r\nstruct request_queue *q = device->request_queue;\r\nunsigned long flags;\r\nSCSI_LOG_MLQUEUE(1, scmd_printk(KERN_INFO, cmd,\r\n"Inserting command %p into mlqueue\n", cmd));\r\nscsi_set_blocked(cmd, reason);\r\nif (unbusy)\r\nscsi_device_unbusy(device);\r\ncmd->result = 0;\r\nif (q->mq_ops) {\r\nscsi_mq_requeue_cmd(cmd);\r\nreturn;\r\n}\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_requeue_request(q, cmd->request);\r\nkblockd_schedule_work(&device->requeue_work);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\n}\r\nvoid scsi_queue_insert(struct scsi_cmnd *cmd, int reason)\r\n{\r\n__scsi_queue_insert(cmd, reason, 1);\r\n}\r\nint scsi_execute(struct scsi_device *sdev, const unsigned char *cmd,\r\nint data_direction, void *buffer, unsigned bufflen,\r\nunsigned char *sense, int timeout, int retries, u64 flags,\r\nint *resid)\r\n{\r\nstruct request *req;\r\nint write = (data_direction == DMA_TO_DEVICE);\r\nint ret = DRIVER_ERROR << 24;\r\nreq = blk_get_request(sdev->request_queue, write, __GFP_WAIT);\r\nif (IS_ERR(req))\r\nreturn ret;\r\nblk_rq_set_block_pc(req);\r\nif (bufflen && blk_rq_map_kern(sdev->request_queue, req,\r\nbuffer, bufflen, __GFP_WAIT))\r\ngoto out;\r\nreq->cmd_len = COMMAND_SIZE(cmd[0]);\r\nmemcpy(req->cmd, cmd, req->cmd_len);\r\nreq->sense = sense;\r\nreq->sense_len = 0;\r\nreq->retries = retries;\r\nreq->timeout = timeout;\r\nreq->cmd_flags |= flags | REQ_QUIET | REQ_PREEMPT;\r\nblk_execute_rq(req->q, NULL, req, 1);\r\nif (unlikely(req->resid_len > 0 && req->resid_len <= bufflen))\r\nmemset(buffer + (bufflen - req->resid_len), 0, req->resid_len);\r\nif (resid)\r\n*resid = req->resid_len;\r\nret = req->errors;\r\nout:\r\nblk_put_request(req);\r\nreturn ret;\r\n}\r\nint scsi_execute_req_flags(struct scsi_device *sdev, const unsigned char *cmd,\r\nint data_direction, void *buffer, unsigned bufflen,\r\nstruct scsi_sense_hdr *sshdr, int timeout, int retries,\r\nint *resid, u64 flags)\r\n{\r\nchar *sense = NULL;\r\nint result;\r\nif (sshdr) {\r\nsense = kzalloc(SCSI_SENSE_BUFFERSIZE, GFP_NOIO);\r\nif (!sense)\r\nreturn DRIVER_ERROR << 24;\r\n}\r\nresult = scsi_execute(sdev, cmd, data_direction, buffer, bufflen,\r\nsense, timeout, retries, flags, resid);\r\nif (sshdr)\r\nscsi_normalize_sense(sense, SCSI_SENSE_BUFFERSIZE, sshdr);\r\nkfree(sense);\r\nreturn result;\r\n}\r\nstatic void scsi_init_cmd_errh(struct scsi_cmnd *cmd)\r\n{\r\ncmd->serial_number = 0;\r\nscsi_set_resid(cmd, 0);\r\nmemset(cmd->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);\r\nif (cmd->cmd_len == 0)\r\ncmd->cmd_len = scsi_command_size(cmd->cmnd);\r\n}\r\nvoid scsi_device_unbusy(struct scsi_device *sdev)\r\n{\r\nstruct Scsi_Host *shost = sdev->host;\r\nstruct scsi_target *starget = scsi_target(sdev);\r\nunsigned long flags;\r\natomic_dec(&shost->host_busy);\r\nif (starget->can_queue > 0)\r\natomic_dec(&starget->target_busy);\r\nif (unlikely(scsi_host_in_recovery(shost) &&\r\n(shost->host_failed || shost->host_eh_scheduled))) {\r\nspin_lock_irqsave(shost->host_lock, flags);\r\nscsi_eh_wakeup(shost);\r\nspin_unlock_irqrestore(shost->host_lock, flags);\r\n}\r\natomic_dec(&sdev->device_busy);\r\n}\r\nstatic void scsi_kick_queue(struct request_queue *q)\r\n{\r\nif (q->mq_ops)\r\nblk_mq_start_hw_queues(q);\r\nelse\r\nblk_run_queue(q);\r\n}\r\nstatic void scsi_single_lun_run(struct scsi_device *current_sdev)\r\n{\r\nstruct Scsi_Host *shost = current_sdev->host;\r\nstruct scsi_device *sdev, *tmp;\r\nstruct scsi_target *starget = scsi_target(current_sdev);\r\nunsigned long flags;\r\nspin_lock_irqsave(shost->host_lock, flags);\r\nstarget->starget_sdev_user = NULL;\r\nspin_unlock_irqrestore(shost->host_lock, flags);\r\nscsi_kick_queue(current_sdev->request_queue);\r\nspin_lock_irqsave(shost->host_lock, flags);\r\nif (starget->starget_sdev_user)\r\ngoto out;\r\nlist_for_each_entry_safe(sdev, tmp, &starget->devices,\r\nsame_target_siblings) {\r\nif (sdev == current_sdev)\r\ncontinue;\r\nif (scsi_device_get(sdev))\r\ncontinue;\r\nspin_unlock_irqrestore(shost->host_lock, flags);\r\nscsi_kick_queue(sdev->request_queue);\r\nspin_lock_irqsave(shost->host_lock, flags);\r\nscsi_device_put(sdev);\r\n}\r\nout:\r\nspin_unlock_irqrestore(shost->host_lock, flags);\r\n}\r\nstatic inline bool scsi_device_is_busy(struct scsi_device *sdev)\r\n{\r\nif (atomic_read(&sdev->device_busy) >= sdev->queue_depth)\r\nreturn true;\r\nif (atomic_read(&sdev->device_blocked) > 0)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline bool scsi_target_is_busy(struct scsi_target *starget)\r\n{\r\nif (starget->can_queue > 0) {\r\nif (atomic_read(&starget->target_busy) >= starget->can_queue)\r\nreturn true;\r\nif (atomic_read(&starget->target_blocked) > 0)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic inline bool scsi_host_is_busy(struct Scsi_Host *shost)\r\n{\r\nif (shost->can_queue > 0 &&\r\natomic_read(&shost->host_busy) >= shost->can_queue)\r\nreturn true;\r\nif (atomic_read(&shost->host_blocked) > 0)\r\nreturn true;\r\nif (shost->host_self_blocked)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void scsi_starved_list_run(struct Scsi_Host *shost)\r\n{\r\nLIST_HEAD(starved_list);\r\nstruct scsi_device *sdev;\r\nunsigned long flags;\r\nspin_lock_irqsave(shost->host_lock, flags);\r\nlist_splice_init(&shost->starved_list, &starved_list);\r\nwhile (!list_empty(&starved_list)) {\r\nstruct request_queue *slq;\r\nif (scsi_host_is_busy(shost))\r\nbreak;\r\nsdev = list_entry(starved_list.next,\r\nstruct scsi_device, starved_entry);\r\nlist_del_init(&sdev->starved_entry);\r\nif (scsi_target_is_busy(scsi_target(sdev))) {\r\nlist_move_tail(&sdev->starved_entry,\r\n&shost->starved_list);\r\ncontinue;\r\n}\r\nslq = sdev->request_queue;\r\nif (!blk_get_queue(slq))\r\ncontinue;\r\nspin_unlock_irqrestore(shost->host_lock, flags);\r\nscsi_kick_queue(slq);\r\nblk_put_queue(slq);\r\nspin_lock_irqsave(shost->host_lock, flags);\r\n}\r\nlist_splice(&starved_list, &shost->starved_list);\r\nspin_unlock_irqrestore(shost->host_lock, flags);\r\n}\r\nstatic void scsi_run_queue(struct request_queue *q)\r\n{\r\nstruct scsi_device *sdev = q->queuedata;\r\nif (scsi_target(sdev)->single_lun)\r\nscsi_single_lun_run(sdev);\r\nif (!list_empty(&sdev->host->starved_list))\r\nscsi_starved_list_run(sdev->host);\r\nif (q->mq_ops)\r\nblk_mq_start_stopped_hw_queues(q, false);\r\nelse\r\nblk_run_queue(q);\r\n}\r\nvoid scsi_requeue_run_queue(struct work_struct *work)\r\n{\r\nstruct scsi_device *sdev;\r\nstruct request_queue *q;\r\nsdev = container_of(work, struct scsi_device, requeue_work);\r\nq = sdev->request_queue;\r\nscsi_run_queue(q);\r\n}\r\nstatic void scsi_requeue_command(struct request_queue *q, struct scsi_cmnd *cmd)\r\n{\r\nstruct scsi_device *sdev = cmd->device;\r\nstruct request *req = cmd->request;\r\nunsigned long flags;\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_unprep_request(req);\r\nreq->special = NULL;\r\nscsi_put_command(cmd);\r\nblk_requeue_request(q, req);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\nscsi_run_queue(q);\r\nput_device(&sdev->sdev_gendev);\r\n}\r\nvoid scsi_run_host_queues(struct Scsi_Host *shost)\r\n{\r\nstruct scsi_device *sdev;\r\nshost_for_each_device(sdev, shost)\r\nscsi_run_queue(sdev->request_queue);\r\n}\r\nstatic inline unsigned int scsi_sgtable_index(unsigned short nents)\r\n{\r\nunsigned int index;\r\nBUG_ON(nents > SCSI_MAX_SG_SEGMENTS);\r\nif (nents <= 8)\r\nindex = 0;\r\nelse\r\nindex = get_count_order(nents) - 3;\r\nreturn index;\r\n}\r\nstatic void scsi_sg_free(struct scatterlist *sgl, unsigned int nents)\r\n{\r\nstruct scsi_host_sg_pool *sgp;\r\nsgp = scsi_sg_pools + scsi_sgtable_index(nents);\r\nmempool_free(sgl, sgp->pool);\r\n}\r\nstatic struct scatterlist *scsi_sg_alloc(unsigned int nents, gfp_t gfp_mask)\r\n{\r\nstruct scsi_host_sg_pool *sgp;\r\nsgp = scsi_sg_pools + scsi_sgtable_index(nents);\r\nreturn mempool_alloc(sgp->pool, gfp_mask);\r\n}\r\nstatic void scsi_free_sgtable(struct scsi_data_buffer *sdb, bool mq)\r\n{\r\nif (mq && sdb->table.nents <= SCSI_MAX_SG_SEGMENTS)\r\nreturn;\r\n__sg_free_table(&sdb->table, SCSI_MAX_SG_SEGMENTS, mq, scsi_sg_free);\r\n}\r\nstatic int scsi_alloc_sgtable(struct scsi_data_buffer *sdb, int nents, bool mq)\r\n{\r\nstruct scatterlist *first_chunk = NULL;\r\nint ret;\r\nBUG_ON(!nents);\r\nif (mq) {\r\nif (nents <= SCSI_MAX_SG_SEGMENTS) {\r\nsdb->table.nents = nents;\r\nsg_init_table(sdb->table.sgl, sdb->table.nents);\r\nreturn 0;\r\n}\r\nfirst_chunk = sdb->table.sgl;\r\n}\r\nret = __sg_alloc_table(&sdb->table, nents, SCSI_MAX_SG_SEGMENTS,\r\nfirst_chunk, GFP_ATOMIC, scsi_sg_alloc);\r\nif (unlikely(ret))\r\nscsi_free_sgtable(sdb, mq);\r\nreturn ret;\r\n}\r\nstatic void scsi_uninit_cmd(struct scsi_cmnd *cmd)\r\n{\r\nif (cmd->request->cmd_type == REQ_TYPE_FS) {\r\nstruct scsi_driver *drv = scsi_cmd_to_driver(cmd);\r\nif (drv->uninit_command)\r\ndrv->uninit_command(cmd);\r\n}\r\n}\r\nstatic void scsi_mq_free_sgtables(struct scsi_cmnd *cmd)\r\n{\r\nif (cmd->sdb.table.nents)\r\nscsi_free_sgtable(&cmd->sdb, true);\r\nif (cmd->request->next_rq && cmd->request->next_rq->special)\r\nscsi_free_sgtable(cmd->request->next_rq->special, true);\r\nif (scsi_prot_sg_count(cmd))\r\nscsi_free_sgtable(cmd->prot_sdb, true);\r\n}\r\nstatic void scsi_mq_uninit_cmd(struct scsi_cmnd *cmd)\r\n{\r\nstruct scsi_device *sdev = cmd->device;\r\nstruct Scsi_Host *shost = sdev->host;\r\nunsigned long flags;\r\nscsi_mq_free_sgtables(cmd);\r\nscsi_uninit_cmd(cmd);\r\nif (shost->use_cmd_list) {\r\nBUG_ON(list_empty(&cmd->list));\r\nspin_lock_irqsave(&sdev->list_lock, flags);\r\nlist_del_init(&cmd->list);\r\nspin_unlock_irqrestore(&sdev->list_lock, flags);\r\n}\r\n}\r\nstatic void scsi_release_buffers(struct scsi_cmnd *cmd)\r\n{\r\nif (cmd->sdb.table.nents)\r\nscsi_free_sgtable(&cmd->sdb, false);\r\nmemset(&cmd->sdb, 0, sizeof(cmd->sdb));\r\nif (scsi_prot_sg_count(cmd))\r\nscsi_free_sgtable(cmd->prot_sdb, false);\r\n}\r\nstatic void scsi_release_bidi_buffers(struct scsi_cmnd *cmd)\r\n{\r\nstruct scsi_data_buffer *bidi_sdb = cmd->request->next_rq->special;\r\nscsi_free_sgtable(bidi_sdb, false);\r\nkmem_cache_free(scsi_sdb_cache, bidi_sdb);\r\ncmd->request->next_rq->special = NULL;\r\n}\r\nstatic bool scsi_end_request(struct request *req, int error,\r\nunsigned int bytes, unsigned int bidi_bytes)\r\n{\r\nstruct scsi_cmnd *cmd = req->special;\r\nstruct scsi_device *sdev = cmd->device;\r\nstruct request_queue *q = sdev->request_queue;\r\nif (blk_update_request(req, error, bytes))\r\nreturn true;\r\nif (unlikely(bidi_bytes) &&\r\nblk_update_request(req->next_rq, error, bidi_bytes))\r\nreturn true;\r\nif (blk_queue_add_random(q))\r\nadd_disk_randomness(req->rq_disk);\r\nif (req->mq_ctx) {\r\nscsi_mq_uninit_cmd(cmd);\r\n__blk_mq_end_request(req, error);\r\nif (scsi_target(sdev)->single_lun ||\r\n!list_empty(&sdev->host->starved_list))\r\nkblockd_schedule_work(&sdev->requeue_work);\r\nelse\r\nblk_mq_start_stopped_hw_queues(q, true);\r\n} else {\r\nunsigned long flags;\r\nif (bidi_bytes)\r\nscsi_release_bidi_buffers(cmd);\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_finish_request(req, error);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\nscsi_release_buffers(cmd);\r\nscsi_put_command(cmd);\r\nscsi_run_queue(q);\r\n}\r\nput_device(&sdev->sdev_gendev);\r\nreturn false;\r\n}\r\nstatic int __scsi_error_from_host_byte(struct scsi_cmnd *cmd, int result)\r\n{\r\nint error = 0;\r\nswitch(host_byte(result)) {\r\ncase DID_TRANSPORT_FAILFAST:\r\nerror = -ENOLINK;\r\nbreak;\r\ncase DID_TARGET_FAILURE:\r\nset_host_byte(cmd, DID_OK);\r\nerror = -EREMOTEIO;\r\nbreak;\r\ncase DID_NEXUS_FAILURE:\r\nset_host_byte(cmd, DID_OK);\r\nerror = -EBADE;\r\nbreak;\r\ncase DID_ALLOC_FAILURE:\r\nset_host_byte(cmd, DID_OK);\r\nerror = -ENOSPC;\r\nbreak;\r\ncase DID_MEDIUM_ERROR:\r\nset_host_byte(cmd, DID_OK);\r\nerror = -ENODATA;\r\nbreak;\r\ndefault:\r\nerror = -EIO;\r\nbreak;\r\n}\r\nreturn error;\r\n}\r\nvoid scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)\r\n{\r\nint result = cmd->result;\r\nstruct request_queue *q = cmd->device->request_queue;\r\nstruct request *req = cmd->request;\r\nint error = 0;\r\nstruct scsi_sense_hdr sshdr;\r\nbool sense_valid = false;\r\nint sense_deferred = 0, level = 0;\r\nenum {ACTION_FAIL, ACTION_REPREP, ACTION_RETRY,\r\nACTION_DELAYED_RETRY} action;\r\nunsigned long wait_for = (cmd->allowed + 1) * req->timeout;\r\nif (result) {\r\nsense_valid = scsi_command_normalize_sense(cmd, &sshdr);\r\nif (sense_valid)\r\nsense_deferred = scsi_sense_is_deferred(&sshdr);\r\n}\r\nif (req->cmd_type == REQ_TYPE_BLOCK_PC) {\r\nif (result) {\r\nif (sense_valid && req->sense) {\r\nint len = 8 + cmd->sense_buffer[7];\r\nif (len > SCSI_SENSE_BUFFERSIZE)\r\nlen = SCSI_SENSE_BUFFERSIZE;\r\nmemcpy(req->sense, cmd->sense_buffer, len);\r\nreq->sense_len = len;\r\n}\r\nif (!sense_deferred)\r\nerror = __scsi_error_from_host_byte(cmd, result);\r\n}\r\nreq->errors = cmd->result;\r\nreq->resid_len = scsi_get_resid(cmd);\r\nif (scsi_bidi_cmnd(cmd)) {\r\nreq->next_rq->resid_len = scsi_in(cmd)->resid;\r\nif (scsi_end_request(req, 0, blk_rq_bytes(req),\r\nblk_rq_bytes(req->next_rq)))\r\nBUG();\r\nreturn;\r\n}\r\n} else if (blk_rq_bytes(req) == 0 && result && !sense_deferred) {\r\nerror = __scsi_error_from_host_byte(cmd, result);\r\n}\r\nBUG_ON(blk_bidi_rq(req));\r\nSCSI_LOG_HLCOMPLETE(1, scmd_printk(KERN_INFO, cmd,\r\n"%u sectors total, %d bytes done.\n",\r\nblk_rq_sectors(req), good_bytes));\r\nif (sense_valid && (sshdr.sense_key == RECOVERED_ERROR)) {\r\nif ((sshdr.asc == 0x0) && (sshdr.ascq == 0x1d))\r\n;\r\nelse if (!(req->cmd_flags & REQ_QUIET))\r\nscsi_print_sense(cmd);\r\nresult = 0;\r\nerror = 0;\r\n}\r\nif (!scsi_end_request(req, error, good_bytes, 0))\r\nreturn;\r\nif (error && scsi_noretry_cmd(cmd)) {\r\nif (scsi_end_request(req, error, blk_rq_bytes(req), 0))\r\nBUG();\r\nreturn;\r\n}\r\nif (result == 0)\r\ngoto requeue;\r\nerror = __scsi_error_from_host_byte(cmd, result);\r\nif (host_byte(result) == DID_RESET) {\r\naction = ACTION_RETRY;\r\n} else if (sense_valid && !sense_deferred) {\r\nswitch (sshdr.sense_key) {\r\ncase UNIT_ATTENTION:\r\nif (cmd->device->removable) {\r\ncmd->device->changed = 1;\r\naction = ACTION_FAIL;\r\n} else {\r\naction = ACTION_RETRY;\r\n}\r\nbreak;\r\ncase ILLEGAL_REQUEST:\r\nif ((cmd->device->use_10_for_rw &&\r\nsshdr.asc == 0x20 && sshdr.ascq == 0x00) &&\r\n(cmd->cmnd[0] == READ_10 ||\r\ncmd->cmnd[0] == WRITE_10)) {\r\ncmd->device->use_10_for_rw = 0;\r\naction = ACTION_REPREP;\r\n} else if (sshdr.asc == 0x10) {\r\naction = ACTION_FAIL;\r\nerror = -EILSEQ;\r\n} else if (sshdr.asc == 0x20 || sshdr.asc == 0x24) {\r\naction = ACTION_FAIL;\r\nerror = -EREMOTEIO;\r\n} else\r\naction = ACTION_FAIL;\r\nbreak;\r\ncase ABORTED_COMMAND:\r\naction = ACTION_FAIL;\r\nif (sshdr.asc == 0x10)\r\nerror = -EILSEQ;\r\nbreak;\r\ncase NOT_READY:\r\nif (sshdr.asc == 0x04) {\r\nswitch (sshdr.ascq) {\r\ncase 0x01:\r\ncase 0x04:\r\ncase 0x05:\r\ncase 0x06:\r\ncase 0x07:\r\ncase 0x08:\r\ncase 0x09:\r\ncase 0x14:\r\naction = ACTION_DELAYED_RETRY;\r\nbreak;\r\ndefault:\r\naction = ACTION_FAIL;\r\nbreak;\r\n}\r\n} else\r\naction = ACTION_FAIL;\r\nbreak;\r\ncase VOLUME_OVERFLOW:\r\naction = ACTION_FAIL;\r\nbreak;\r\ndefault:\r\naction = ACTION_FAIL;\r\nbreak;\r\n}\r\n} else\r\naction = ACTION_FAIL;\r\nif (action != ACTION_FAIL &&\r\ntime_before(cmd->jiffies_at_alloc + wait_for, jiffies))\r\naction = ACTION_FAIL;\r\nswitch (action) {\r\ncase ACTION_FAIL:\r\nif (!(req->cmd_flags & REQ_QUIET)) {\r\nstatic DEFINE_RATELIMIT_STATE(_rs,\r\nDEFAULT_RATELIMIT_INTERVAL,\r\nDEFAULT_RATELIMIT_BURST);\r\nif (unlikely(scsi_logging_level))\r\nlevel = SCSI_LOG_LEVEL(SCSI_LOG_MLCOMPLETE_SHIFT,\r\nSCSI_LOG_MLCOMPLETE_BITS);\r\nif (!level && __ratelimit(&_rs)) {\r\nscsi_print_result(cmd, NULL, FAILED);\r\nif (driver_byte(result) & DRIVER_SENSE)\r\nscsi_print_sense(cmd);\r\nscsi_print_command(cmd);\r\n}\r\n}\r\nif (!scsi_end_request(req, error, blk_rq_err_bytes(req), 0))\r\nreturn;\r\ncase ACTION_REPREP:\r\nrequeue:\r\nif (q->mq_ops) {\r\ncmd->request->cmd_flags &= ~REQ_DONTPREP;\r\nscsi_mq_uninit_cmd(cmd);\r\nscsi_mq_requeue_cmd(cmd);\r\n} else {\r\nscsi_release_buffers(cmd);\r\nscsi_requeue_command(q, cmd);\r\n}\r\nbreak;\r\ncase ACTION_RETRY:\r\n__scsi_queue_insert(cmd, SCSI_MLQUEUE_EH_RETRY, 0);\r\nbreak;\r\ncase ACTION_DELAYED_RETRY:\r\n__scsi_queue_insert(cmd, SCSI_MLQUEUE_DEVICE_BUSY, 0);\r\nbreak;\r\n}\r\n}\r\nstatic int scsi_init_sgtable(struct request *req, struct scsi_data_buffer *sdb)\r\n{\r\nint count;\r\nif (unlikely(scsi_alloc_sgtable(sdb, req->nr_phys_segments,\r\nreq->mq_ctx != NULL)))\r\nreturn BLKPREP_DEFER;\r\ncount = blk_rq_map_sg(req->q, req, sdb->table.sgl);\r\nBUG_ON(count > sdb->table.nents);\r\nsdb->table.nents = count;\r\nsdb->length = blk_rq_bytes(req);\r\nreturn BLKPREP_OK;\r\n}\r\nint scsi_init_io(struct scsi_cmnd *cmd)\r\n{\r\nstruct scsi_device *sdev = cmd->device;\r\nstruct request *rq = cmd->request;\r\nbool is_mq = (rq->mq_ctx != NULL);\r\nint error;\r\nBUG_ON(!rq->nr_phys_segments);\r\nerror = scsi_init_sgtable(rq, &cmd->sdb);\r\nif (error)\r\ngoto err_exit;\r\nif (blk_bidi_rq(rq)) {\r\nif (!rq->q->mq_ops) {\r\nstruct scsi_data_buffer *bidi_sdb =\r\nkmem_cache_zalloc(scsi_sdb_cache, GFP_ATOMIC);\r\nif (!bidi_sdb) {\r\nerror = BLKPREP_DEFER;\r\ngoto err_exit;\r\n}\r\nrq->next_rq->special = bidi_sdb;\r\n}\r\nerror = scsi_init_sgtable(rq->next_rq, rq->next_rq->special);\r\nif (error)\r\ngoto err_exit;\r\n}\r\nif (blk_integrity_rq(rq)) {\r\nstruct scsi_data_buffer *prot_sdb = cmd->prot_sdb;\r\nint ivecs, count;\r\nif (prot_sdb == NULL) {\r\nWARN_ON_ONCE(1);\r\nerror = BLKPREP_KILL;\r\ngoto err_exit;\r\n}\r\nivecs = blk_rq_count_integrity_sg(rq->q, rq->bio);\r\nif (scsi_alloc_sgtable(prot_sdb, ivecs, is_mq)) {\r\nerror = BLKPREP_DEFER;\r\ngoto err_exit;\r\n}\r\ncount = blk_rq_map_integrity_sg(rq->q, rq->bio,\r\nprot_sdb->table.sgl);\r\nBUG_ON(unlikely(count > ivecs));\r\nBUG_ON(unlikely(count > queue_max_integrity_segments(rq->q)));\r\ncmd->prot_sdb = prot_sdb;\r\ncmd->prot_sdb->table.nents = count;\r\n}\r\nreturn BLKPREP_OK;\r\nerr_exit:\r\nif (is_mq) {\r\nscsi_mq_free_sgtables(cmd);\r\n} else {\r\nscsi_release_buffers(cmd);\r\ncmd->request->special = NULL;\r\nscsi_put_command(cmd);\r\nput_device(&sdev->sdev_gendev);\r\n}\r\nreturn error;\r\n}\r\nstatic struct scsi_cmnd *scsi_get_cmd_from_req(struct scsi_device *sdev,\r\nstruct request *req)\r\n{\r\nstruct scsi_cmnd *cmd;\r\nif (!req->special) {\r\nif (!get_device(&sdev->sdev_gendev))\r\nreturn NULL;\r\ncmd = scsi_get_command(sdev, GFP_ATOMIC);\r\nif (unlikely(!cmd)) {\r\nput_device(&sdev->sdev_gendev);\r\nreturn NULL;\r\n}\r\nreq->special = cmd;\r\n} else {\r\ncmd = req->special;\r\n}\r\ncmd->tag = req->tag;\r\ncmd->request = req;\r\ncmd->cmnd = req->cmd;\r\ncmd->prot_op = SCSI_PROT_NORMAL;\r\nreturn cmd;\r\n}\r\nstatic int scsi_setup_blk_pc_cmnd(struct scsi_device *sdev, struct request *req)\r\n{\r\nstruct scsi_cmnd *cmd = req->special;\r\nif (req->bio) {\r\nint ret = scsi_init_io(cmd);\r\nif (unlikely(ret))\r\nreturn ret;\r\n} else {\r\nBUG_ON(blk_rq_bytes(req));\r\nmemset(&cmd->sdb, 0, sizeof(cmd->sdb));\r\n}\r\ncmd->cmd_len = req->cmd_len;\r\ncmd->transfersize = blk_rq_bytes(req);\r\ncmd->allowed = req->retries;\r\nreturn BLKPREP_OK;\r\n}\r\nstatic int scsi_setup_fs_cmnd(struct scsi_device *sdev, struct request *req)\r\n{\r\nstruct scsi_cmnd *cmd = req->special;\r\nif (unlikely(sdev->scsi_dh_data && sdev->scsi_dh_data->scsi_dh\r\n&& sdev->scsi_dh_data->scsi_dh->prep_fn)) {\r\nint ret = sdev->scsi_dh_data->scsi_dh->prep_fn(sdev, req);\r\nif (ret != BLKPREP_OK)\r\nreturn ret;\r\n}\r\nmemset(cmd->cmnd, 0, BLK_MAX_CDB);\r\nreturn scsi_cmd_to_driver(cmd)->init_command(cmd);\r\n}\r\nstatic int scsi_setup_cmnd(struct scsi_device *sdev, struct request *req)\r\n{\r\nstruct scsi_cmnd *cmd = req->special;\r\nif (!blk_rq_bytes(req))\r\ncmd->sc_data_direction = DMA_NONE;\r\nelse if (rq_data_dir(req) == WRITE)\r\ncmd->sc_data_direction = DMA_TO_DEVICE;\r\nelse\r\ncmd->sc_data_direction = DMA_FROM_DEVICE;\r\nswitch (req->cmd_type) {\r\ncase REQ_TYPE_FS:\r\nreturn scsi_setup_fs_cmnd(sdev, req);\r\ncase REQ_TYPE_BLOCK_PC:\r\nreturn scsi_setup_blk_pc_cmnd(sdev, req);\r\ndefault:\r\nreturn BLKPREP_KILL;\r\n}\r\n}\r\nstatic int\r\nscsi_prep_state_check(struct scsi_device *sdev, struct request *req)\r\n{\r\nint ret = BLKPREP_OK;\r\nif (unlikely(sdev->sdev_state != SDEV_RUNNING)) {\r\nswitch (sdev->sdev_state) {\r\ncase SDEV_OFFLINE:\r\ncase SDEV_TRANSPORT_OFFLINE:\r\nsdev_printk(KERN_ERR, sdev,\r\n"rejecting I/O to offline device\n");\r\nret = BLKPREP_KILL;\r\nbreak;\r\ncase SDEV_DEL:\r\nsdev_printk(KERN_ERR, sdev,\r\n"rejecting I/O to dead device\n");\r\nret = BLKPREP_KILL;\r\nbreak;\r\ncase SDEV_BLOCK:\r\ncase SDEV_CREATED_BLOCK:\r\nret = BLKPREP_DEFER;\r\nbreak;\r\ncase SDEV_QUIESCE:\r\nif (!(req->cmd_flags & REQ_PREEMPT))\r\nret = BLKPREP_DEFER;\r\nbreak;\r\ndefault:\r\nif (!(req->cmd_flags & REQ_PREEMPT))\r\nret = BLKPREP_KILL;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\nscsi_prep_return(struct request_queue *q, struct request *req, int ret)\r\n{\r\nstruct scsi_device *sdev = q->queuedata;\r\nswitch (ret) {\r\ncase BLKPREP_KILL:\r\nreq->errors = DID_NO_CONNECT << 16;\r\nif (req->special) {\r\nstruct scsi_cmnd *cmd = req->special;\r\nscsi_release_buffers(cmd);\r\nscsi_put_command(cmd);\r\nput_device(&sdev->sdev_gendev);\r\nreq->special = NULL;\r\n}\r\nbreak;\r\ncase BLKPREP_DEFER:\r\nif (atomic_read(&sdev->device_busy) == 0)\r\nblk_delay_queue(q, SCSI_QUEUE_DELAY);\r\nbreak;\r\ndefault:\r\nreq->cmd_flags |= REQ_DONTPREP;\r\n}\r\nreturn ret;\r\n}\r\nstatic int scsi_prep_fn(struct request_queue *q, struct request *req)\r\n{\r\nstruct scsi_device *sdev = q->queuedata;\r\nstruct scsi_cmnd *cmd;\r\nint ret;\r\nret = scsi_prep_state_check(sdev, req);\r\nif (ret != BLKPREP_OK)\r\ngoto out;\r\ncmd = scsi_get_cmd_from_req(sdev, req);\r\nif (unlikely(!cmd)) {\r\nret = BLKPREP_DEFER;\r\ngoto out;\r\n}\r\nret = scsi_setup_cmnd(sdev, req);\r\nout:\r\nreturn scsi_prep_return(q, req, ret);\r\n}\r\nstatic void scsi_unprep_fn(struct request_queue *q, struct request *req)\r\n{\r\nscsi_uninit_cmd(req->special);\r\n}\r\nstatic inline int scsi_dev_queue_ready(struct request_queue *q,\r\nstruct scsi_device *sdev)\r\n{\r\nunsigned int busy;\r\nbusy = atomic_inc_return(&sdev->device_busy) - 1;\r\nif (atomic_read(&sdev->device_blocked)) {\r\nif (busy)\r\ngoto out_dec;\r\nif (atomic_dec_return(&sdev->device_blocked) > 0) {\r\nif (!q->mq_ops)\r\nblk_delay_queue(q, SCSI_QUEUE_DELAY);\r\ngoto out_dec;\r\n}\r\nSCSI_LOG_MLQUEUE(3, sdev_printk(KERN_INFO, sdev,\r\n"unblocking device at zero depth\n"));\r\n}\r\nif (busy >= sdev->queue_depth)\r\ngoto out_dec;\r\nreturn 1;\r\nout_dec:\r\natomic_dec(&sdev->device_busy);\r\nreturn 0;\r\n}\r\nstatic inline int scsi_target_queue_ready(struct Scsi_Host *shost,\r\nstruct scsi_device *sdev)\r\n{\r\nstruct scsi_target *starget = scsi_target(sdev);\r\nunsigned int busy;\r\nif (starget->single_lun) {\r\nspin_lock_irq(shost->host_lock);\r\nif (starget->starget_sdev_user &&\r\nstarget->starget_sdev_user != sdev) {\r\nspin_unlock_irq(shost->host_lock);\r\nreturn 0;\r\n}\r\nstarget->starget_sdev_user = sdev;\r\nspin_unlock_irq(shost->host_lock);\r\n}\r\nif (starget->can_queue <= 0)\r\nreturn 1;\r\nbusy = atomic_inc_return(&starget->target_busy) - 1;\r\nif (atomic_read(&starget->target_blocked) > 0) {\r\nif (busy)\r\ngoto starved;\r\nif (atomic_dec_return(&starget->target_blocked) > 0)\r\ngoto out_dec;\r\nSCSI_LOG_MLQUEUE(3, starget_printk(KERN_INFO, starget,\r\n"unblocking target at zero depth\n"));\r\n}\r\nif (busy >= starget->can_queue)\r\ngoto starved;\r\nreturn 1;\r\nstarved:\r\nspin_lock_irq(shost->host_lock);\r\nlist_move_tail(&sdev->starved_entry, &shost->starved_list);\r\nspin_unlock_irq(shost->host_lock);\r\nout_dec:\r\nif (starget->can_queue > 0)\r\natomic_dec(&starget->target_busy);\r\nreturn 0;\r\n}\r\nstatic inline int scsi_host_queue_ready(struct request_queue *q,\r\nstruct Scsi_Host *shost,\r\nstruct scsi_device *sdev)\r\n{\r\nunsigned int busy;\r\nif (scsi_host_in_recovery(shost))\r\nreturn 0;\r\nbusy = atomic_inc_return(&shost->host_busy) - 1;\r\nif (atomic_read(&shost->host_blocked) > 0) {\r\nif (busy)\r\ngoto starved;\r\nif (atomic_dec_return(&shost->host_blocked) > 0)\r\ngoto out_dec;\r\nSCSI_LOG_MLQUEUE(3,\r\nshost_printk(KERN_INFO, shost,\r\n"unblocking host at zero depth\n"));\r\n}\r\nif (shost->can_queue > 0 && busy >= shost->can_queue)\r\ngoto starved;\r\nif (shost->host_self_blocked)\r\ngoto starved;\r\nif (!list_empty(&sdev->starved_entry)) {\r\nspin_lock_irq(shost->host_lock);\r\nif (!list_empty(&sdev->starved_entry))\r\nlist_del_init(&sdev->starved_entry);\r\nspin_unlock_irq(shost->host_lock);\r\n}\r\nreturn 1;\r\nstarved:\r\nspin_lock_irq(shost->host_lock);\r\nif (list_empty(&sdev->starved_entry))\r\nlist_add_tail(&sdev->starved_entry, &shost->starved_list);\r\nspin_unlock_irq(shost->host_lock);\r\nout_dec:\r\natomic_dec(&shost->host_busy);\r\nreturn 0;\r\n}\r\nstatic int scsi_lld_busy(struct request_queue *q)\r\n{\r\nstruct scsi_device *sdev = q->queuedata;\r\nstruct Scsi_Host *shost;\r\nif (blk_queue_dying(q))\r\nreturn 0;\r\nshost = sdev->host;\r\nif (scsi_host_in_recovery(shost) || scsi_device_is_busy(sdev))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void scsi_kill_request(struct request *req, struct request_queue *q)\r\n{\r\nstruct scsi_cmnd *cmd = req->special;\r\nstruct scsi_device *sdev;\r\nstruct scsi_target *starget;\r\nstruct Scsi_Host *shost;\r\nblk_start_request(req);\r\nscmd_printk(KERN_INFO, cmd, "killing request\n");\r\nsdev = cmd->device;\r\nstarget = scsi_target(sdev);\r\nshost = sdev->host;\r\nscsi_init_cmd_errh(cmd);\r\ncmd->result = DID_NO_CONNECT << 16;\r\natomic_inc(&cmd->device->iorequest_cnt);\r\natomic_inc(&sdev->device_busy);\r\natomic_inc(&shost->host_busy);\r\nif (starget->can_queue > 0)\r\natomic_inc(&starget->target_busy);\r\nblk_complete_request(req);\r\n}\r\nstatic void scsi_softirq_done(struct request *rq)\r\n{\r\nstruct scsi_cmnd *cmd = rq->special;\r\nunsigned long wait_for = (cmd->allowed + 1) * rq->timeout;\r\nint disposition;\r\nINIT_LIST_HEAD(&cmd->eh_entry);\r\natomic_inc(&cmd->device->iodone_cnt);\r\nif (cmd->result)\r\natomic_inc(&cmd->device->ioerr_cnt);\r\ndisposition = scsi_decide_disposition(cmd);\r\nif (disposition != SUCCESS &&\r\ntime_before(cmd->jiffies_at_alloc + wait_for, jiffies)) {\r\nsdev_printk(KERN_ERR, cmd->device,\r\n"timing out command, waited %lus\n",\r\nwait_for/HZ);\r\ndisposition = SUCCESS;\r\n}\r\nscsi_log_completion(cmd, disposition);\r\nswitch (disposition) {\r\ncase SUCCESS:\r\nscsi_finish_command(cmd);\r\nbreak;\r\ncase NEEDS_RETRY:\r\nscsi_queue_insert(cmd, SCSI_MLQUEUE_EH_RETRY);\r\nbreak;\r\ncase ADD_TO_MLQUEUE:\r\nscsi_queue_insert(cmd, SCSI_MLQUEUE_DEVICE_BUSY);\r\nbreak;\r\ndefault:\r\nif (!scsi_eh_scmd_add(cmd, 0))\r\nscsi_finish_command(cmd);\r\n}\r\n}\r\nstatic int scsi_dispatch_cmd(struct scsi_cmnd *cmd)\r\n{\r\nstruct Scsi_Host *host = cmd->device->host;\r\nint rtn = 0;\r\natomic_inc(&cmd->device->iorequest_cnt);\r\nif (unlikely(cmd->device->sdev_state == SDEV_DEL)) {\r\ncmd->result = DID_NO_CONNECT << 16;\r\ngoto done;\r\n}\r\nif (unlikely(scsi_device_blocked(cmd->device))) {\r\nSCSI_LOG_MLQUEUE(3, scmd_printk(KERN_INFO, cmd,\r\n"queuecommand : device blocked\n"));\r\nreturn SCSI_MLQUEUE_DEVICE_BUSY;\r\n}\r\nif (cmd->device->lun_in_cdb)\r\ncmd->cmnd[1] = (cmd->cmnd[1] & 0x1f) |\r\n(cmd->device->lun << 5 & 0xe0);\r\nscsi_log_send(cmd);\r\nif (cmd->cmd_len > cmd->device->host->max_cmd_len) {\r\nSCSI_LOG_MLQUEUE(3, scmd_printk(KERN_INFO, cmd,\r\n"queuecommand : command too long. "\r\n"cdb_size=%d host->max_cmd_len=%d\n",\r\ncmd->cmd_len, cmd->device->host->max_cmd_len));\r\ncmd->result = (DID_ABORT << 16);\r\ngoto done;\r\n}\r\nif (unlikely(host->shost_state == SHOST_DEL)) {\r\ncmd->result = (DID_NO_CONNECT << 16);\r\ngoto done;\r\n}\r\ntrace_scsi_dispatch_cmd_start(cmd);\r\nrtn = host->hostt->queuecommand(host, cmd);\r\nif (rtn) {\r\ntrace_scsi_dispatch_cmd_error(cmd, rtn);\r\nif (rtn != SCSI_MLQUEUE_DEVICE_BUSY &&\r\nrtn != SCSI_MLQUEUE_TARGET_BUSY)\r\nrtn = SCSI_MLQUEUE_HOST_BUSY;\r\nSCSI_LOG_MLQUEUE(3, scmd_printk(KERN_INFO, cmd,\r\n"queuecommand : request rejected\n"));\r\n}\r\nreturn rtn;\r\ndone:\r\ncmd->scsi_done(cmd);\r\nreturn 0;\r\n}\r\nstatic void scsi_done(struct scsi_cmnd *cmd)\r\n{\r\ntrace_scsi_dispatch_cmd_done(cmd);\r\nblk_complete_request(cmd->request);\r\n}\r\nstatic void scsi_request_fn(struct request_queue *q)\r\n__releases(q->queue_lock)\r\n__acquires(q->queue_lock)\r\n{\r\nstruct scsi_device *sdev = q->queuedata;\r\nstruct Scsi_Host *shost;\r\nstruct scsi_cmnd *cmd;\r\nstruct request *req;\r\nshost = sdev->host;\r\nfor (;;) {\r\nint rtn;\r\nreq = blk_peek_request(q);\r\nif (!req)\r\nbreak;\r\nif (unlikely(!scsi_device_online(sdev))) {\r\nsdev_printk(KERN_ERR, sdev,\r\n"rejecting I/O to offline device\n");\r\nscsi_kill_request(req, q);\r\ncontinue;\r\n}\r\nif (!scsi_dev_queue_ready(q, sdev))\r\nbreak;\r\nif (!(blk_queue_tagged(q) && !blk_queue_start_tag(q, req)))\r\nblk_start_request(req);\r\nspin_unlock_irq(q->queue_lock);\r\ncmd = req->special;\r\nif (unlikely(cmd == NULL)) {\r\nprintk(KERN_CRIT "impossible request in %s.\n"\r\n"please mail a stack trace to "\r\n"linux-scsi@vger.kernel.org\n",\r\n__func__);\r\nblk_dump_rq_flags(req, "foo");\r\nBUG();\r\n}\r\nif (blk_queue_tagged(q) && !(req->cmd_flags & REQ_QUEUED)) {\r\nspin_lock_irq(shost->host_lock);\r\nif (list_empty(&sdev->starved_entry))\r\nlist_add_tail(&sdev->starved_entry,\r\n&shost->starved_list);\r\nspin_unlock_irq(shost->host_lock);\r\ngoto not_ready;\r\n}\r\nif (!scsi_target_queue_ready(shost, sdev))\r\ngoto not_ready;\r\nif (!scsi_host_queue_ready(q, shost, sdev))\r\ngoto host_not_ready;\r\nif (sdev->simple_tags)\r\ncmd->flags |= SCMD_TAGGED;\r\nelse\r\ncmd->flags &= ~SCMD_TAGGED;\r\nscsi_init_cmd_errh(cmd);\r\ncmd->scsi_done = scsi_done;\r\nrtn = scsi_dispatch_cmd(cmd);\r\nif (rtn) {\r\nscsi_queue_insert(cmd, rtn);\r\nspin_lock_irq(q->queue_lock);\r\ngoto out_delay;\r\n}\r\nspin_lock_irq(q->queue_lock);\r\n}\r\nreturn;\r\nhost_not_ready:\r\nif (scsi_target(sdev)->can_queue > 0)\r\natomic_dec(&scsi_target(sdev)->target_busy);\r\nnot_ready:\r\nspin_lock_irq(q->queue_lock);\r\nblk_requeue_request(q, req);\r\natomic_dec(&sdev->device_busy);\r\nout_delay:\r\nif (!atomic_read(&sdev->device_busy) && !scsi_device_blocked(sdev))\r\nblk_delay_queue(q, SCSI_QUEUE_DELAY);\r\n}\r\nstatic inline int prep_to_mq(int ret)\r\n{\r\nswitch (ret) {\r\ncase BLKPREP_OK:\r\nreturn 0;\r\ncase BLKPREP_DEFER:\r\nreturn BLK_MQ_RQ_QUEUE_BUSY;\r\ndefault:\r\nreturn BLK_MQ_RQ_QUEUE_ERROR;\r\n}\r\n}\r\nstatic int scsi_mq_prep_fn(struct request *req)\r\n{\r\nstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);\r\nstruct scsi_device *sdev = req->q->queuedata;\r\nstruct Scsi_Host *shost = sdev->host;\r\nunsigned char *sense_buf = cmd->sense_buffer;\r\nstruct scatterlist *sg;\r\nmemset(cmd, 0, sizeof(struct scsi_cmnd));\r\nreq->special = cmd;\r\ncmd->request = req;\r\ncmd->device = sdev;\r\ncmd->sense_buffer = sense_buf;\r\ncmd->tag = req->tag;\r\ncmd->cmnd = req->cmd;\r\ncmd->prot_op = SCSI_PROT_NORMAL;\r\nINIT_LIST_HEAD(&cmd->list);\r\nINIT_DELAYED_WORK(&cmd->abort_work, scmd_eh_abort_handler);\r\ncmd->jiffies_at_alloc = jiffies;\r\nif (shost->use_cmd_list) {\r\nspin_lock_irq(&sdev->list_lock);\r\nlist_add_tail(&cmd->list, &sdev->cmd_list);\r\nspin_unlock_irq(&sdev->list_lock);\r\n}\r\nsg = (void *)cmd + sizeof(struct scsi_cmnd) + shost->hostt->cmd_size;\r\ncmd->sdb.table.sgl = sg;\r\nif (scsi_host_get_prot(shost)) {\r\ncmd->prot_sdb = (void *)sg +\r\nmin_t(unsigned int,\r\nshost->sg_tablesize, SCSI_MAX_SG_SEGMENTS) *\r\nsizeof(struct scatterlist);\r\nmemset(cmd->prot_sdb, 0, sizeof(struct scsi_data_buffer));\r\ncmd->prot_sdb->table.sgl =\r\n(struct scatterlist *)(cmd->prot_sdb + 1);\r\n}\r\nif (blk_bidi_rq(req)) {\r\nstruct request *next_rq = req->next_rq;\r\nstruct scsi_data_buffer *bidi_sdb = blk_mq_rq_to_pdu(next_rq);\r\nmemset(bidi_sdb, 0, sizeof(struct scsi_data_buffer));\r\nbidi_sdb->table.sgl =\r\n(struct scatterlist *)(bidi_sdb + 1);\r\nnext_rq->special = bidi_sdb;\r\n}\r\nblk_mq_start_request(req);\r\nreturn scsi_setup_cmnd(sdev, req);\r\n}\r\nstatic void scsi_mq_done(struct scsi_cmnd *cmd)\r\n{\r\ntrace_scsi_dispatch_cmd_done(cmd);\r\nblk_mq_complete_request(cmd->request);\r\n}\r\nstatic int scsi_queue_rq(struct blk_mq_hw_ctx *hctx,\r\nconst struct blk_mq_queue_data *bd)\r\n{\r\nstruct request *req = bd->rq;\r\nstruct request_queue *q = req->q;\r\nstruct scsi_device *sdev = q->queuedata;\r\nstruct Scsi_Host *shost = sdev->host;\r\nstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);\r\nint ret;\r\nint reason;\r\nret = prep_to_mq(scsi_prep_state_check(sdev, req));\r\nif (ret)\r\ngoto out;\r\nret = BLK_MQ_RQ_QUEUE_BUSY;\r\nif (!get_device(&sdev->sdev_gendev))\r\ngoto out;\r\nif (!scsi_dev_queue_ready(q, sdev))\r\ngoto out_put_device;\r\nif (!scsi_target_queue_ready(shost, sdev))\r\ngoto out_dec_device_busy;\r\nif (!scsi_host_queue_ready(q, shost, sdev))\r\ngoto out_dec_target_busy;\r\nif (!(req->cmd_flags & REQ_DONTPREP)) {\r\nret = prep_to_mq(scsi_mq_prep_fn(req));\r\nif (ret)\r\ngoto out_dec_host_busy;\r\nreq->cmd_flags |= REQ_DONTPREP;\r\n} else {\r\nblk_mq_start_request(req);\r\n}\r\nif (sdev->simple_tags)\r\ncmd->flags |= SCMD_TAGGED;\r\nelse\r\ncmd->flags &= ~SCMD_TAGGED;\r\nscsi_init_cmd_errh(cmd);\r\ncmd->scsi_done = scsi_mq_done;\r\nreason = scsi_dispatch_cmd(cmd);\r\nif (reason) {\r\nscsi_set_blocked(cmd, reason);\r\nret = BLK_MQ_RQ_QUEUE_BUSY;\r\ngoto out_dec_host_busy;\r\n}\r\nreturn BLK_MQ_RQ_QUEUE_OK;\r\nout_dec_host_busy:\r\natomic_dec(&shost->host_busy);\r\nout_dec_target_busy:\r\nif (scsi_target(sdev)->can_queue > 0)\r\natomic_dec(&scsi_target(sdev)->target_busy);\r\nout_dec_device_busy:\r\natomic_dec(&sdev->device_busy);\r\nout_put_device:\r\nput_device(&sdev->sdev_gendev);\r\nout:\r\nswitch (ret) {\r\ncase BLK_MQ_RQ_QUEUE_BUSY:\r\nblk_mq_stop_hw_queue(hctx);\r\nif (atomic_read(&sdev->device_busy) == 0 &&\r\n!scsi_device_blocked(sdev))\r\nblk_mq_delay_queue(hctx, SCSI_QUEUE_DELAY);\r\nbreak;\r\ncase BLK_MQ_RQ_QUEUE_ERROR:\r\nif (req->cmd_flags & REQ_DONTPREP)\r\nscsi_mq_uninit_cmd(cmd);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic enum blk_eh_timer_return scsi_timeout(struct request *req,\r\nbool reserved)\r\n{\r\nif (reserved)\r\nreturn BLK_EH_RESET_TIMER;\r\nreturn scsi_times_out(req);\r\n}\r\nstatic int scsi_init_request(void *data, struct request *rq,\r\nunsigned int hctx_idx, unsigned int request_idx,\r\nunsigned int numa_node)\r\n{\r\nstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);\r\ncmd->sense_buffer = kzalloc_node(SCSI_SENSE_BUFFERSIZE, GFP_KERNEL,\r\nnuma_node);\r\nif (!cmd->sense_buffer)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void scsi_exit_request(void *data, struct request *rq,\r\nunsigned int hctx_idx, unsigned int request_idx)\r\n{\r\nstruct scsi_cmnd *cmd = blk_mq_rq_to_pdu(rq);\r\nkfree(cmd->sense_buffer);\r\n}\r\nstatic u64 scsi_calculate_bounce_limit(struct Scsi_Host *shost)\r\n{\r\nstruct device *host_dev;\r\nu64 bounce_limit = 0xffffffff;\r\nif (shost->unchecked_isa_dma)\r\nreturn BLK_BOUNCE_ISA;\r\nif (!PCI_DMA_BUS_IS_PHYS)\r\nreturn BLK_BOUNCE_ANY;\r\nhost_dev = scsi_get_device(shost);\r\nif (host_dev && host_dev->dma_mask)\r\nbounce_limit = (u64)dma_max_pfn(host_dev) << PAGE_SHIFT;\r\nreturn bounce_limit;\r\n}\r\nstatic void __scsi_init_queue(struct Scsi_Host *shost, struct request_queue *q)\r\n{\r\nstruct device *dev = shost->dma_dev;\r\nblk_queue_max_segments(q, min_t(unsigned short, shost->sg_tablesize,\r\nSCSI_MAX_SG_CHAIN_SEGMENTS));\r\nif (scsi_host_prot_dma(shost)) {\r\nshost->sg_prot_tablesize =\r\nmin_not_zero(shost->sg_prot_tablesize,\r\n(unsigned short)SCSI_MAX_PROT_SG_SEGMENTS);\r\nBUG_ON(shost->sg_prot_tablesize < shost->sg_tablesize);\r\nblk_queue_max_integrity_segments(q, shost->sg_prot_tablesize);\r\n}\r\nblk_queue_max_hw_sectors(q, shost->max_sectors);\r\nblk_queue_bounce_limit(q, scsi_calculate_bounce_limit(shost));\r\nblk_queue_segment_boundary(q, shost->dma_boundary);\r\ndma_set_seg_boundary(dev, shost->dma_boundary);\r\nblk_queue_max_segment_size(q, dma_get_max_seg_size(dev));\r\nif (!shost->use_clustering)\r\nq->limits.cluster = 0;\r\nblk_queue_dma_alignment(q, 0x03);\r\n}\r\nstruct request_queue *__scsi_alloc_queue(struct Scsi_Host *shost,\r\nrequest_fn_proc *request_fn)\r\n{\r\nstruct request_queue *q;\r\nq = blk_init_queue(request_fn, NULL);\r\nif (!q)\r\nreturn NULL;\r\n__scsi_init_queue(shost, q);\r\nreturn q;\r\n}\r\nstruct request_queue *scsi_alloc_queue(struct scsi_device *sdev)\r\n{\r\nstruct request_queue *q;\r\nq = __scsi_alloc_queue(sdev->host, scsi_request_fn);\r\nif (!q)\r\nreturn NULL;\r\nblk_queue_prep_rq(q, scsi_prep_fn);\r\nblk_queue_unprep_rq(q, scsi_unprep_fn);\r\nblk_queue_softirq_done(q, scsi_softirq_done);\r\nblk_queue_rq_timed_out(q, scsi_times_out);\r\nblk_queue_lld_busy(q, scsi_lld_busy);\r\nreturn q;\r\n}\r\nstruct request_queue *scsi_mq_alloc_queue(struct scsi_device *sdev)\r\n{\r\nsdev->request_queue = blk_mq_init_queue(&sdev->host->tag_set);\r\nif (IS_ERR(sdev->request_queue))\r\nreturn NULL;\r\nsdev->request_queue->queuedata = sdev;\r\n__scsi_init_queue(sdev->host, sdev->request_queue);\r\nreturn sdev->request_queue;\r\n}\r\nint scsi_mq_setup_tags(struct Scsi_Host *shost)\r\n{\r\nunsigned int cmd_size, sgl_size, tbl_size;\r\ntbl_size = shost->sg_tablesize;\r\nif (tbl_size > SCSI_MAX_SG_SEGMENTS)\r\ntbl_size = SCSI_MAX_SG_SEGMENTS;\r\nsgl_size = tbl_size * sizeof(struct scatterlist);\r\ncmd_size = sizeof(struct scsi_cmnd) + shost->hostt->cmd_size + sgl_size;\r\nif (scsi_host_get_prot(shost))\r\ncmd_size += sizeof(struct scsi_data_buffer) + sgl_size;\r\nmemset(&shost->tag_set, 0, sizeof(shost->tag_set));\r\nshost->tag_set.ops = &scsi_mq_ops;\r\nshost->tag_set.nr_hw_queues = shost->nr_hw_queues ? : 1;\r\nshost->tag_set.queue_depth = shost->can_queue;\r\nshost->tag_set.cmd_size = cmd_size;\r\nshost->tag_set.numa_node = NUMA_NO_NODE;\r\nshost->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;\r\nshost->tag_set.flags |=\r\nBLK_ALLOC_POLICY_TO_MQ_FLAG(shost->hostt->tag_alloc_policy);\r\nshost->tag_set.driver_data = shost;\r\nreturn blk_mq_alloc_tag_set(&shost->tag_set);\r\n}\r\nvoid scsi_mq_destroy_tags(struct Scsi_Host *shost)\r\n{\r\nblk_mq_free_tag_set(&shost->tag_set);\r\n}\r\nvoid scsi_block_requests(struct Scsi_Host *shost)\r\n{\r\nshost->host_self_blocked = 1;\r\n}\r\nvoid scsi_unblock_requests(struct Scsi_Host *shost)\r\n{\r\nshost->host_self_blocked = 0;\r\nscsi_run_host_queues(shost);\r\n}\r\nint __init scsi_init_queue(void)\r\n{\r\nint i;\r\nscsi_sdb_cache = kmem_cache_create("scsi_data_buffer",\r\nsizeof(struct scsi_data_buffer),\r\n0, 0, NULL);\r\nif (!scsi_sdb_cache) {\r\nprintk(KERN_ERR "SCSI: can't init scsi sdb cache\n");\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < SG_MEMPOOL_NR; i++) {\r\nstruct scsi_host_sg_pool *sgp = scsi_sg_pools + i;\r\nint size = sgp->size * sizeof(struct scatterlist);\r\nsgp->slab = kmem_cache_create(sgp->name, size, 0,\r\nSLAB_HWCACHE_ALIGN, NULL);\r\nif (!sgp->slab) {\r\nprintk(KERN_ERR "SCSI: can't init sg slab %s\n",\r\nsgp->name);\r\ngoto cleanup_sdb;\r\n}\r\nsgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE,\r\nsgp->slab);\r\nif (!sgp->pool) {\r\nprintk(KERN_ERR "SCSI: can't init sg mempool %s\n",\r\nsgp->name);\r\ngoto cleanup_sdb;\r\n}\r\n}\r\nreturn 0;\r\ncleanup_sdb:\r\nfor (i = 0; i < SG_MEMPOOL_NR; i++) {\r\nstruct scsi_host_sg_pool *sgp = scsi_sg_pools + i;\r\nif (sgp->pool)\r\nmempool_destroy(sgp->pool);\r\nif (sgp->slab)\r\nkmem_cache_destroy(sgp->slab);\r\n}\r\nkmem_cache_destroy(scsi_sdb_cache);\r\nreturn -ENOMEM;\r\n}\r\nvoid scsi_exit_queue(void)\r\n{\r\nint i;\r\nkmem_cache_destroy(scsi_sdb_cache);\r\nfor (i = 0; i < SG_MEMPOOL_NR; i++) {\r\nstruct scsi_host_sg_pool *sgp = scsi_sg_pools + i;\r\nmempool_destroy(sgp->pool);\r\nkmem_cache_destroy(sgp->slab);\r\n}\r\n}\r\nint\r\nscsi_mode_select(struct scsi_device *sdev, int pf, int sp, int modepage,\r\nunsigned char *buffer, int len, int timeout, int retries,\r\nstruct scsi_mode_data *data, struct scsi_sense_hdr *sshdr)\r\n{\r\nunsigned char cmd[10];\r\nunsigned char *real_buffer;\r\nint ret;\r\nmemset(cmd, 0, sizeof(cmd));\r\ncmd[1] = (pf ? 0x10 : 0) | (sp ? 0x01 : 0);\r\nif (sdev->use_10_for_ms) {\r\nif (len > 65535)\r\nreturn -EINVAL;\r\nreal_buffer = kmalloc(8 + len, GFP_KERNEL);\r\nif (!real_buffer)\r\nreturn -ENOMEM;\r\nmemcpy(real_buffer + 8, buffer, len);\r\nlen += 8;\r\nreal_buffer[0] = 0;\r\nreal_buffer[1] = 0;\r\nreal_buffer[2] = data->medium_type;\r\nreal_buffer[3] = data->device_specific;\r\nreal_buffer[4] = data->longlba ? 0x01 : 0;\r\nreal_buffer[5] = 0;\r\nreal_buffer[6] = data->block_descriptor_length >> 8;\r\nreal_buffer[7] = data->block_descriptor_length;\r\ncmd[0] = MODE_SELECT_10;\r\ncmd[7] = len >> 8;\r\ncmd[8] = len;\r\n} else {\r\nif (len > 255 || data->block_descriptor_length > 255 ||\r\ndata->longlba)\r\nreturn -EINVAL;\r\nreal_buffer = kmalloc(4 + len, GFP_KERNEL);\r\nif (!real_buffer)\r\nreturn -ENOMEM;\r\nmemcpy(real_buffer + 4, buffer, len);\r\nlen += 4;\r\nreal_buffer[0] = 0;\r\nreal_buffer[1] = data->medium_type;\r\nreal_buffer[2] = data->device_specific;\r\nreal_buffer[3] = data->block_descriptor_length;\r\ncmd[0] = MODE_SELECT;\r\ncmd[4] = len;\r\n}\r\nret = scsi_execute_req(sdev, cmd, DMA_TO_DEVICE, real_buffer, len,\r\nsshdr, timeout, retries, NULL);\r\nkfree(real_buffer);\r\nreturn ret;\r\n}\r\nint\r\nscsi_mode_sense(struct scsi_device *sdev, int dbd, int modepage,\r\nunsigned char *buffer, int len, int timeout, int retries,\r\nstruct scsi_mode_data *data, struct scsi_sense_hdr *sshdr)\r\n{\r\nunsigned char cmd[12];\r\nint use_10_for_ms;\r\nint header_length;\r\nint result;\r\nstruct scsi_sense_hdr my_sshdr;\r\nmemset(data, 0, sizeof(*data));\r\nmemset(&cmd[0], 0, 12);\r\ncmd[1] = dbd & 0x18;\r\ncmd[2] = modepage;\r\nif (!sshdr)\r\nsshdr = &my_sshdr;\r\nretry:\r\nuse_10_for_ms = sdev->use_10_for_ms;\r\nif (use_10_for_ms) {\r\nif (len < 8)\r\nlen = 8;\r\ncmd[0] = MODE_SENSE_10;\r\ncmd[8] = len;\r\nheader_length = 8;\r\n} else {\r\nif (len < 4)\r\nlen = 4;\r\ncmd[0] = MODE_SENSE;\r\ncmd[4] = len;\r\nheader_length = 4;\r\n}\r\nmemset(buffer, 0, len);\r\nresult = scsi_execute_req(sdev, cmd, DMA_FROM_DEVICE, buffer, len,\r\nsshdr, timeout, retries, NULL);\r\nif (use_10_for_ms && !scsi_status_is_good(result) &&\r\n(driver_byte(result) & DRIVER_SENSE)) {\r\nif (scsi_sense_valid(sshdr)) {\r\nif ((sshdr->sense_key == ILLEGAL_REQUEST) &&\r\n(sshdr->asc == 0x20) && (sshdr->ascq == 0)) {\r\nsdev->use_10_for_ms = 0;\r\ngoto retry;\r\n}\r\n}\r\n}\r\nif(scsi_status_is_good(result)) {\r\nif (unlikely(buffer[0] == 0x86 && buffer[1] == 0x0b &&\r\n(modepage == 6 || modepage == 8))) {\r\nheader_length = 0;\r\ndata->length = 13;\r\ndata->medium_type = 0;\r\ndata->device_specific = 0;\r\ndata->longlba = 0;\r\ndata->block_descriptor_length = 0;\r\n} else if(use_10_for_ms) {\r\ndata->length = buffer[0]*256 + buffer[1] + 2;\r\ndata->medium_type = buffer[2];\r\ndata->device_specific = buffer[3];\r\ndata->longlba = buffer[4] & 0x01;\r\ndata->block_descriptor_length = buffer[6]*256\r\n+ buffer[7];\r\n} else {\r\ndata->length = buffer[0] + 1;\r\ndata->medium_type = buffer[1];\r\ndata->device_specific = buffer[2];\r\ndata->block_descriptor_length = buffer[3];\r\n}\r\ndata->header_length = header_length;\r\n}\r\nreturn result;\r\n}\r\nint\r\nscsi_test_unit_ready(struct scsi_device *sdev, int timeout, int retries,\r\nstruct scsi_sense_hdr *sshdr_external)\r\n{\r\nchar cmd[] = {\r\nTEST_UNIT_READY, 0, 0, 0, 0, 0,\r\n};\r\nstruct scsi_sense_hdr *sshdr;\r\nint result;\r\nif (!sshdr_external)\r\nsshdr = kzalloc(sizeof(*sshdr), GFP_KERNEL);\r\nelse\r\nsshdr = sshdr_external;\r\ndo {\r\nresult = scsi_execute_req(sdev, cmd, DMA_NONE, NULL, 0, sshdr,\r\ntimeout, retries, NULL);\r\nif (sdev->removable && scsi_sense_valid(sshdr) &&\r\nsshdr->sense_key == UNIT_ATTENTION)\r\nsdev->changed = 1;\r\n} while (scsi_sense_valid(sshdr) &&\r\nsshdr->sense_key == UNIT_ATTENTION && --retries);\r\nif (!sshdr_external)\r\nkfree(sshdr);\r\nreturn result;\r\n}\r\nint\r\nscsi_device_set_state(struct scsi_device *sdev, enum scsi_device_state state)\r\n{\r\nenum scsi_device_state oldstate = sdev->sdev_state;\r\nif (state == oldstate)\r\nreturn 0;\r\nswitch (state) {\r\ncase SDEV_CREATED:\r\nswitch (oldstate) {\r\ncase SDEV_CREATED_BLOCK:\r\nbreak;\r\ndefault:\r\ngoto illegal;\r\n}\r\nbreak;\r\ncase SDEV_RUNNING:\r\nswitch (oldstate) {\r\ncase SDEV_CREATED:\r\ncase SDEV_OFFLINE:\r\ncase SDEV_TRANSPORT_OFFLINE:\r\ncase SDEV_QUIESCE:\r\ncase SDEV_BLOCK:\r\nbreak;\r\ndefault:\r\ngoto illegal;\r\n}\r\nbreak;\r\ncase SDEV_QUIESCE:\r\nswitch (oldstate) {\r\ncase SDEV_RUNNING:\r\ncase SDEV_OFFLINE:\r\ncase SDEV_TRANSPORT_OFFLINE:\r\nbreak;\r\ndefault:\r\ngoto illegal;\r\n}\r\nbreak;\r\ncase SDEV_OFFLINE:\r\ncase SDEV_TRANSPORT_OFFLINE:\r\nswitch (oldstate) {\r\ncase SDEV_CREATED:\r\ncase SDEV_RUNNING:\r\ncase SDEV_QUIESCE:\r\ncase SDEV_BLOCK:\r\nbreak;\r\ndefault:\r\ngoto illegal;\r\n}\r\nbreak;\r\ncase SDEV_BLOCK:\r\nswitch (oldstate) {\r\ncase SDEV_RUNNING:\r\ncase SDEV_CREATED_BLOCK:\r\nbreak;\r\ndefault:\r\ngoto illegal;\r\n}\r\nbreak;\r\ncase SDEV_CREATED_BLOCK:\r\nswitch (oldstate) {\r\ncase SDEV_CREATED:\r\nbreak;\r\ndefault:\r\ngoto illegal;\r\n}\r\nbreak;\r\ncase SDEV_CANCEL:\r\nswitch (oldstate) {\r\ncase SDEV_CREATED:\r\ncase SDEV_RUNNING:\r\ncase SDEV_QUIESCE:\r\ncase SDEV_OFFLINE:\r\ncase SDEV_TRANSPORT_OFFLINE:\r\ncase SDEV_BLOCK:\r\nbreak;\r\ndefault:\r\ngoto illegal;\r\n}\r\nbreak;\r\ncase SDEV_DEL:\r\nswitch (oldstate) {\r\ncase SDEV_CREATED:\r\ncase SDEV_RUNNING:\r\ncase SDEV_OFFLINE:\r\ncase SDEV_TRANSPORT_OFFLINE:\r\ncase SDEV_CANCEL:\r\ncase SDEV_CREATED_BLOCK:\r\nbreak;\r\ndefault:\r\ngoto illegal;\r\n}\r\nbreak;\r\n}\r\nsdev->sdev_state = state;\r\nreturn 0;\r\nillegal:\r\nSCSI_LOG_ERROR_RECOVERY(1,\r\nsdev_printk(KERN_ERR, sdev,\r\n"Illegal state transition %s->%s",\r\nscsi_device_state_name(oldstate),\r\nscsi_device_state_name(state))\r\n);\r\nreturn -EINVAL;\r\n}\r\nstatic void scsi_evt_emit(struct scsi_device *sdev, struct scsi_event *evt)\r\n{\r\nint idx = 0;\r\nchar *envp[3];\r\nswitch (evt->evt_type) {\r\ncase SDEV_EVT_MEDIA_CHANGE:\r\nenvp[idx++] = "SDEV_MEDIA_CHANGE=1";\r\nbreak;\r\ncase SDEV_EVT_INQUIRY_CHANGE_REPORTED:\r\nenvp[idx++] = "SDEV_UA=INQUIRY_DATA_HAS_CHANGED";\r\nbreak;\r\ncase SDEV_EVT_CAPACITY_CHANGE_REPORTED:\r\nenvp[idx++] = "SDEV_UA=CAPACITY_DATA_HAS_CHANGED";\r\nbreak;\r\ncase SDEV_EVT_SOFT_THRESHOLD_REACHED_REPORTED:\r\nenvp[idx++] = "SDEV_UA=THIN_PROVISIONING_SOFT_THRESHOLD_REACHED";\r\nbreak;\r\ncase SDEV_EVT_MODE_PARAMETER_CHANGE_REPORTED:\r\nenvp[idx++] = "SDEV_UA=MODE_PARAMETERS_CHANGED";\r\nbreak;\r\ncase SDEV_EVT_LUN_CHANGE_REPORTED:\r\nenvp[idx++] = "SDEV_UA=REPORTED_LUNS_DATA_HAS_CHANGED";\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nenvp[idx++] = NULL;\r\nkobject_uevent_env(&sdev->sdev_gendev.kobj, KOBJ_CHANGE, envp);\r\n}\r\nvoid scsi_evt_thread(struct work_struct *work)\r\n{\r\nstruct scsi_device *sdev;\r\nenum scsi_device_event evt_type;\r\nLIST_HEAD(event_list);\r\nsdev = container_of(work, struct scsi_device, event_work);\r\nfor (evt_type = SDEV_EVT_FIRST; evt_type <= SDEV_EVT_LAST; evt_type++)\r\nif (test_and_clear_bit(evt_type, sdev->pending_events))\r\nsdev_evt_send_simple(sdev, evt_type, GFP_KERNEL);\r\nwhile (1) {\r\nstruct scsi_event *evt;\r\nstruct list_head *this, *tmp;\r\nunsigned long flags;\r\nspin_lock_irqsave(&sdev->list_lock, flags);\r\nlist_splice_init(&sdev->event_list, &event_list);\r\nspin_unlock_irqrestore(&sdev->list_lock, flags);\r\nif (list_empty(&event_list))\r\nbreak;\r\nlist_for_each_safe(this, tmp, &event_list) {\r\nevt = list_entry(this, struct scsi_event, node);\r\nlist_del(&evt->node);\r\nscsi_evt_emit(sdev, evt);\r\nkfree(evt);\r\n}\r\n}\r\n}\r\nvoid sdev_evt_send(struct scsi_device *sdev, struct scsi_event *evt)\r\n{\r\nunsigned long flags;\r\n#if 0\r\nif (!test_bit(evt->evt_type, sdev->supported_events)) {\r\nkfree(evt);\r\nreturn;\r\n}\r\n#endif\r\nspin_lock_irqsave(&sdev->list_lock, flags);\r\nlist_add_tail(&evt->node, &sdev->event_list);\r\nschedule_work(&sdev->event_work);\r\nspin_unlock_irqrestore(&sdev->list_lock, flags);\r\n}\r\nstruct scsi_event *sdev_evt_alloc(enum scsi_device_event evt_type,\r\ngfp_t gfpflags)\r\n{\r\nstruct scsi_event *evt = kzalloc(sizeof(struct scsi_event), gfpflags);\r\nif (!evt)\r\nreturn NULL;\r\nevt->evt_type = evt_type;\r\nINIT_LIST_HEAD(&evt->node);\r\nswitch (evt_type) {\r\ncase SDEV_EVT_MEDIA_CHANGE:\r\ncase SDEV_EVT_INQUIRY_CHANGE_REPORTED:\r\ncase SDEV_EVT_CAPACITY_CHANGE_REPORTED:\r\ncase SDEV_EVT_SOFT_THRESHOLD_REACHED_REPORTED:\r\ncase SDEV_EVT_MODE_PARAMETER_CHANGE_REPORTED:\r\ncase SDEV_EVT_LUN_CHANGE_REPORTED:\r\ndefault:\r\nbreak;\r\n}\r\nreturn evt;\r\n}\r\nvoid sdev_evt_send_simple(struct scsi_device *sdev,\r\nenum scsi_device_event evt_type, gfp_t gfpflags)\r\n{\r\nstruct scsi_event *evt = sdev_evt_alloc(evt_type, gfpflags);\r\nif (!evt) {\r\nsdev_printk(KERN_ERR, sdev, "event %d eaten due to OOM\n",\r\nevt_type);\r\nreturn;\r\n}\r\nsdev_evt_send(sdev, evt);\r\n}\r\nint\r\nscsi_device_quiesce(struct scsi_device *sdev)\r\n{\r\nint err = scsi_device_set_state(sdev, SDEV_QUIESCE);\r\nif (err)\r\nreturn err;\r\nscsi_run_queue(sdev->request_queue);\r\nwhile (atomic_read(&sdev->device_busy)) {\r\nmsleep_interruptible(200);\r\nscsi_run_queue(sdev->request_queue);\r\n}\r\nreturn 0;\r\n}\r\nvoid scsi_device_resume(struct scsi_device *sdev)\r\n{\r\nif (sdev->sdev_state != SDEV_QUIESCE ||\r\nscsi_device_set_state(sdev, SDEV_RUNNING))\r\nreturn;\r\nscsi_run_queue(sdev->request_queue);\r\n}\r\nstatic void\r\ndevice_quiesce_fn(struct scsi_device *sdev, void *data)\r\n{\r\nscsi_device_quiesce(sdev);\r\n}\r\nvoid\r\nscsi_target_quiesce(struct scsi_target *starget)\r\n{\r\nstarget_for_each_device(starget, NULL, device_quiesce_fn);\r\n}\r\nstatic void\r\ndevice_resume_fn(struct scsi_device *sdev, void *data)\r\n{\r\nscsi_device_resume(sdev);\r\n}\r\nvoid\r\nscsi_target_resume(struct scsi_target *starget)\r\n{\r\nstarget_for_each_device(starget, NULL, device_resume_fn);\r\n}\r\nint\r\nscsi_internal_device_block(struct scsi_device *sdev)\r\n{\r\nstruct request_queue *q = sdev->request_queue;\r\nunsigned long flags;\r\nint err = 0;\r\nerr = scsi_device_set_state(sdev, SDEV_BLOCK);\r\nif (err) {\r\nerr = scsi_device_set_state(sdev, SDEV_CREATED_BLOCK);\r\nif (err)\r\nreturn err;\r\n}\r\nif (q->mq_ops) {\r\nblk_mq_stop_hw_queues(q);\r\n} else {\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_stop_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nint\r\nscsi_internal_device_unblock(struct scsi_device *sdev,\r\nenum scsi_device_state new_state)\r\n{\r\nstruct request_queue *q = sdev->request_queue;\r\nunsigned long flags;\r\nif ((sdev->sdev_state == SDEV_BLOCK) ||\r\n(sdev->sdev_state == SDEV_TRANSPORT_OFFLINE))\r\nsdev->sdev_state = new_state;\r\nelse if (sdev->sdev_state == SDEV_CREATED_BLOCK) {\r\nif (new_state == SDEV_TRANSPORT_OFFLINE ||\r\nnew_state == SDEV_OFFLINE)\r\nsdev->sdev_state = new_state;\r\nelse\r\nsdev->sdev_state = SDEV_CREATED;\r\n} else if (sdev->sdev_state != SDEV_CANCEL &&\r\nsdev->sdev_state != SDEV_OFFLINE)\r\nreturn -EINVAL;\r\nif (q->mq_ops) {\r\nblk_mq_start_stopped_hw_queues(q, false);\r\n} else {\r\nspin_lock_irqsave(q->queue_lock, flags);\r\nblk_start_queue(q);\r\nspin_unlock_irqrestore(q->queue_lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\ndevice_block(struct scsi_device *sdev, void *data)\r\n{\r\nscsi_internal_device_block(sdev);\r\n}\r\nstatic int\r\ntarget_block(struct device *dev, void *data)\r\n{\r\nif (scsi_is_target_device(dev))\r\nstarget_for_each_device(to_scsi_target(dev), NULL,\r\ndevice_block);\r\nreturn 0;\r\n}\r\nvoid\r\nscsi_target_block(struct device *dev)\r\n{\r\nif (scsi_is_target_device(dev))\r\nstarget_for_each_device(to_scsi_target(dev), NULL,\r\ndevice_block);\r\nelse\r\ndevice_for_each_child(dev, NULL, target_block);\r\n}\r\nstatic void\r\ndevice_unblock(struct scsi_device *sdev, void *data)\r\n{\r\nscsi_internal_device_unblock(sdev, *(enum scsi_device_state *)data);\r\n}\r\nstatic int\r\ntarget_unblock(struct device *dev, void *data)\r\n{\r\nif (scsi_is_target_device(dev))\r\nstarget_for_each_device(to_scsi_target(dev), data,\r\ndevice_unblock);\r\nreturn 0;\r\n}\r\nvoid\r\nscsi_target_unblock(struct device *dev, enum scsi_device_state new_state)\r\n{\r\nif (scsi_is_target_device(dev))\r\nstarget_for_each_device(to_scsi_target(dev), &new_state,\r\ndevice_unblock);\r\nelse\r\ndevice_for_each_child(dev, &new_state, target_unblock);\r\n}\r\nvoid *scsi_kmap_atomic_sg(struct scatterlist *sgl, int sg_count,\r\nsize_t *offset, size_t *len)\r\n{\r\nint i;\r\nsize_t sg_len = 0, len_complete = 0;\r\nstruct scatterlist *sg;\r\nstruct page *page;\r\nWARN_ON(!irqs_disabled());\r\nfor_each_sg(sgl, sg, sg_count, i) {\r\nlen_complete = sg_len;\r\nsg_len += sg->length;\r\nif (sg_len > *offset)\r\nbreak;\r\n}\r\nif (unlikely(i == sg_count)) {\r\nprintk(KERN_ERR "%s: Bytes in sg: %zu, requested offset %zu, "\r\n"elements %d\n",\r\n__func__, sg_len, *offset, sg_count);\r\nWARN_ON(1);\r\nreturn NULL;\r\n}\r\n*offset = *offset - len_complete + sg->offset;\r\npage = nth_page(sg_page(sg), (*offset >> PAGE_SHIFT));\r\n*offset &= ~PAGE_MASK;\r\nsg_len = PAGE_SIZE - *offset;\r\nif (*len > sg_len)\r\n*len = sg_len;\r\nreturn kmap_atomic(page);\r\n}\r\nvoid scsi_kunmap_atomic_sg(void *virt)\r\n{\r\nkunmap_atomic(virt);\r\n}\r\nvoid sdev_disable_disk_events(struct scsi_device *sdev)\r\n{\r\natomic_inc(&sdev->disk_events_disable_depth);\r\n}\r\nvoid sdev_enable_disk_events(struct scsi_device *sdev)\r\n{\r\nif (WARN_ON_ONCE(atomic_read(&sdev->disk_events_disable_depth) <= 0))\r\nreturn;\r\natomic_dec(&sdev->disk_events_disable_depth);\r\n}
