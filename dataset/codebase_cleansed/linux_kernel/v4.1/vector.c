void lock_vector_lock(void)\r\n{\r\nraw_spin_lock(&vector_lock);\r\n}\r\nvoid unlock_vector_lock(void)\r\n{\r\nraw_spin_unlock(&vector_lock);\r\n}\r\nstruct irq_cfg *irq_cfg(unsigned int irq)\r\n{\r\nreturn irq_get_chip_data(irq);\r\n}\r\nstruct irq_cfg *irqd_cfg(struct irq_data *irq_data)\r\n{\r\nreturn irq_data->chip_data;\r\n}\r\nstatic struct irq_cfg *alloc_irq_cfg(unsigned int irq, int node)\r\n{\r\nstruct irq_cfg *cfg;\r\ncfg = kzalloc_node(sizeof(*cfg), GFP_KERNEL, node);\r\nif (!cfg)\r\nreturn NULL;\r\nif (!zalloc_cpumask_var_node(&cfg->domain, GFP_KERNEL, node))\r\ngoto out_cfg;\r\nif (!zalloc_cpumask_var_node(&cfg->old_domain, GFP_KERNEL, node))\r\ngoto out_domain;\r\n#ifdef CONFIG_X86_IO_APIC\r\nINIT_LIST_HEAD(&cfg->irq_2_pin);\r\n#endif\r\nreturn cfg;\r\nout_domain:\r\nfree_cpumask_var(cfg->domain);\r\nout_cfg:\r\nkfree(cfg);\r\nreturn NULL;\r\n}\r\nstruct irq_cfg *alloc_irq_and_cfg_at(unsigned int at, int node)\r\n{\r\nint res = irq_alloc_desc_at(at, node);\r\nstruct irq_cfg *cfg;\r\nif (res < 0) {\r\nif (res != -EEXIST)\r\nreturn NULL;\r\ncfg = irq_cfg(at);\r\nif (cfg)\r\nreturn cfg;\r\n}\r\ncfg = alloc_irq_cfg(at, node);\r\nif (cfg)\r\nirq_set_chip_data(at, cfg);\r\nelse\r\nirq_free_desc(at);\r\nreturn cfg;\r\n}\r\nstatic void free_irq_cfg(unsigned int at, struct irq_cfg *cfg)\r\n{\r\nif (!cfg)\r\nreturn;\r\nirq_set_chip_data(at, NULL);\r\nfree_cpumask_var(cfg->domain);\r\nfree_cpumask_var(cfg->old_domain);\r\nkfree(cfg);\r\n}\r\nstatic int\r\n__assign_irq_vector(int irq, struct irq_cfg *cfg, const struct cpumask *mask)\r\n{\r\nstatic int current_vector = FIRST_EXTERNAL_VECTOR + VECTOR_OFFSET_START;\r\nstatic int current_offset = VECTOR_OFFSET_START % 16;\r\nint cpu, err;\r\ncpumask_var_t tmp_mask;\r\nif (cfg->move_in_progress)\r\nreturn -EBUSY;\r\nif (!alloc_cpumask_var(&tmp_mask, GFP_ATOMIC))\r\nreturn -ENOMEM;\r\nerr = -ENOSPC;\r\ncpumask_clear(cfg->old_domain);\r\ncpu = cpumask_first_and(mask, cpu_online_mask);\r\nwhile (cpu < nr_cpu_ids) {\r\nint new_cpu, vector, offset;\r\napic->vector_allocation_domain(cpu, tmp_mask, mask);\r\nif (cpumask_subset(tmp_mask, cfg->domain)) {\r\nerr = 0;\r\nif (cpumask_equal(tmp_mask, cfg->domain))\r\nbreak;\r\ncpumask_andnot(cfg->old_domain, cfg->domain, tmp_mask);\r\ncfg->move_in_progress =\r\ncpumask_intersects(cfg->old_domain, cpu_online_mask);\r\ncpumask_and(cfg->domain, cfg->domain, tmp_mask);\r\nbreak;\r\n}\r\nvector = current_vector;\r\noffset = current_offset;\r\nnext:\r\nvector += 16;\r\nif (vector >= first_system_vector) {\r\noffset = (offset + 1) % 16;\r\nvector = FIRST_EXTERNAL_VECTOR + offset;\r\n}\r\nif (unlikely(current_vector == vector)) {\r\ncpumask_or(cfg->old_domain, cfg->old_domain, tmp_mask);\r\ncpumask_andnot(tmp_mask, mask, cfg->old_domain);\r\ncpu = cpumask_first_and(tmp_mask, cpu_online_mask);\r\ncontinue;\r\n}\r\nif (test_bit(vector, used_vectors))\r\ngoto next;\r\nfor_each_cpu_and(new_cpu, tmp_mask, cpu_online_mask) {\r\nif (per_cpu(vector_irq, new_cpu)[vector] >\r\nVECTOR_UNDEFINED)\r\ngoto next;\r\n}\r\ncurrent_vector = vector;\r\ncurrent_offset = offset;\r\nif (cfg->vector) {\r\ncpumask_copy(cfg->old_domain, cfg->domain);\r\ncfg->move_in_progress =\r\ncpumask_intersects(cfg->old_domain, cpu_online_mask);\r\n}\r\nfor_each_cpu_and(new_cpu, tmp_mask, cpu_online_mask)\r\nper_cpu(vector_irq, new_cpu)[vector] = irq;\r\ncfg->vector = vector;\r\ncpumask_copy(cfg->domain, tmp_mask);\r\nerr = 0;\r\nbreak;\r\n}\r\nfree_cpumask_var(tmp_mask);\r\nreturn err;\r\n}\r\nint assign_irq_vector(int irq, struct irq_cfg *cfg, const struct cpumask *mask)\r\n{\r\nint err;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&vector_lock, flags);\r\nerr = __assign_irq_vector(irq, cfg, mask);\r\nraw_spin_unlock_irqrestore(&vector_lock, flags);\r\nreturn err;\r\n}\r\nvoid clear_irq_vector(int irq, struct irq_cfg *cfg)\r\n{\r\nint cpu, vector;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&vector_lock, flags);\r\nBUG_ON(!cfg->vector);\r\nvector = cfg->vector;\r\nfor_each_cpu_and(cpu, cfg->domain, cpu_online_mask)\r\nper_cpu(vector_irq, cpu)[vector] = VECTOR_UNDEFINED;\r\ncfg->vector = 0;\r\ncpumask_clear(cfg->domain);\r\nif (likely(!cfg->move_in_progress)) {\r\nraw_spin_unlock_irqrestore(&vector_lock, flags);\r\nreturn;\r\n}\r\nfor_each_cpu_and(cpu, cfg->old_domain, cpu_online_mask) {\r\nfor (vector = FIRST_EXTERNAL_VECTOR; vector < NR_VECTORS;\r\nvector++) {\r\nif (per_cpu(vector_irq, cpu)[vector] != irq)\r\ncontinue;\r\nper_cpu(vector_irq, cpu)[vector] = VECTOR_UNDEFINED;\r\nbreak;\r\n}\r\n}\r\ncfg->move_in_progress = 0;\r\nraw_spin_unlock_irqrestore(&vector_lock, flags);\r\n}\r\nint __init arch_probe_nr_irqs(void)\r\n{\r\nint nr;\r\nif (nr_irqs > (NR_VECTORS * nr_cpu_ids))\r\nnr_irqs = NR_VECTORS * nr_cpu_ids;\r\nnr = (gsi_top + nr_legacy_irqs()) + 8 * nr_cpu_ids;\r\n#if defined(CONFIG_PCI_MSI) || defined(CONFIG_HT_IRQ)\r\nif (gsi_top <= NR_IRQS_LEGACY)\r\nnr += 8 * nr_cpu_ids;\r\nelse\r\nnr += gsi_top * 16;\r\n#endif\r\nif (nr < nr_irqs)\r\nnr_irqs = nr;\r\nreturn nr_legacy_irqs();\r\n}\r\nint __init arch_early_irq_init(void)\r\n{\r\nreturn arch_early_ioapic_init();\r\n}\r\nstatic void __setup_vector_irq(int cpu)\r\n{\r\nint irq, vector;\r\nstruct irq_cfg *cfg;\r\nraw_spin_lock(&vector_lock);\r\nfor_each_active_irq(irq) {\r\ncfg = irq_cfg(irq);\r\nif (!cfg)\r\ncontinue;\r\nif (!cpumask_test_cpu(cpu, cfg->domain))\r\ncontinue;\r\nvector = cfg->vector;\r\nper_cpu(vector_irq, cpu)[vector] = irq;\r\n}\r\nfor (vector = 0; vector < NR_VECTORS; ++vector) {\r\nirq = per_cpu(vector_irq, cpu)[vector];\r\nif (irq <= VECTOR_UNDEFINED)\r\ncontinue;\r\ncfg = irq_cfg(irq);\r\nif (!cpumask_test_cpu(cpu, cfg->domain))\r\nper_cpu(vector_irq, cpu)[vector] = VECTOR_UNDEFINED;\r\n}\r\nraw_spin_unlock(&vector_lock);\r\n}\r\nvoid setup_vector_irq(int cpu)\r\n{\r\nint irq;\r\nfor (irq = 0; irq < nr_legacy_irqs(); irq++)\r\nper_cpu(vector_irq, cpu)[IRQ0_VECTOR + irq] = irq;\r\n__setup_vector_irq(cpu);\r\n}\r\nint apic_retrigger_irq(struct irq_data *data)\r\n{\r\nstruct irq_cfg *cfg = irqd_cfg(data);\r\nunsigned long flags;\r\nint cpu;\r\nraw_spin_lock_irqsave(&vector_lock, flags);\r\ncpu = cpumask_first_and(cfg->domain, cpu_online_mask);\r\napic->send_IPI_mask(cpumask_of(cpu), cfg->vector);\r\nraw_spin_unlock_irqrestore(&vector_lock, flags);\r\nreturn 1;\r\n}\r\nvoid apic_ack_edge(struct irq_data *data)\r\n{\r\nirq_complete_move(irqd_cfg(data));\r\nirq_move_irq(data);\r\nack_APIC_irq();\r\n}\r\nint apic_set_affinity(struct irq_data *data, const struct cpumask *mask,\r\nunsigned int *dest_id)\r\n{\r\nstruct irq_cfg *cfg = irqd_cfg(data);\r\nunsigned int irq = data->irq;\r\nint err;\r\nif (!config_enabled(CONFIG_SMP))\r\nreturn -EPERM;\r\nif (!cpumask_intersects(mask, cpu_online_mask))\r\nreturn -EINVAL;\r\nerr = assign_irq_vector(irq, cfg, mask);\r\nif (err)\r\nreturn err;\r\nerr = apic->cpu_mask_to_apicid_and(mask, cfg->domain, dest_id);\r\nif (err) {\r\nif (assign_irq_vector(irq, cfg, data->affinity))\r\npr_err("Failed to recover vector for irq %d\n", irq);\r\nreturn err;\r\n}\r\ncpumask_copy(data->affinity, mask);\r\nreturn 0;\r\n}\r\nvoid send_cleanup_vector(struct irq_cfg *cfg)\r\n{\r\ncpumask_var_t cleanup_mask;\r\nif (unlikely(!alloc_cpumask_var(&cleanup_mask, GFP_ATOMIC))) {\r\nunsigned int i;\r\nfor_each_cpu_and(i, cfg->old_domain, cpu_online_mask)\r\napic->send_IPI_mask(cpumask_of(i),\r\nIRQ_MOVE_CLEANUP_VECTOR);\r\n} else {\r\ncpumask_and(cleanup_mask, cfg->old_domain, cpu_online_mask);\r\napic->send_IPI_mask(cleanup_mask, IRQ_MOVE_CLEANUP_VECTOR);\r\nfree_cpumask_var(cleanup_mask);\r\n}\r\ncfg->move_in_progress = 0;\r\n}\r\nasmlinkage __visible void smp_irq_move_cleanup_interrupt(void)\r\n{\r\nunsigned vector, me;\r\nack_APIC_irq();\r\nirq_enter();\r\nexit_idle();\r\nme = smp_processor_id();\r\nfor (vector = FIRST_EXTERNAL_VECTOR; vector < NR_VECTORS; vector++) {\r\nint irq;\r\nunsigned int irr;\r\nstruct irq_desc *desc;\r\nstruct irq_cfg *cfg;\r\nirq = __this_cpu_read(vector_irq[vector]);\r\nif (irq <= VECTOR_UNDEFINED)\r\ncontinue;\r\ndesc = irq_to_desc(irq);\r\nif (!desc)\r\ncontinue;\r\ncfg = irq_cfg(irq);\r\nif (!cfg)\r\ncontinue;\r\nraw_spin_lock(&desc->lock);\r\nif (cfg->move_in_progress)\r\ngoto unlock;\r\nif (vector == cfg->vector && cpumask_test_cpu(me, cfg->domain))\r\ngoto unlock;\r\nirr = apic_read(APIC_IRR + (vector / 32 * 0x10));\r\nif (irr & (1 << (vector % 32))) {\r\napic->send_IPI_self(IRQ_MOVE_CLEANUP_VECTOR);\r\ngoto unlock;\r\n}\r\n__this_cpu_write(vector_irq[vector], VECTOR_UNDEFINED);\r\nunlock:\r\nraw_spin_unlock(&desc->lock);\r\n}\r\nirq_exit();\r\n}\r\nstatic void __irq_complete_move(struct irq_cfg *cfg, unsigned vector)\r\n{\r\nunsigned me;\r\nif (likely(!cfg->move_in_progress))\r\nreturn;\r\nme = smp_processor_id();\r\nif (vector == cfg->vector && cpumask_test_cpu(me, cfg->domain))\r\nsend_cleanup_vector(cfg);\r\n}\r\nvoid irq_complete_move(struct irq_cfg *cfg)\r\n{\r\n__irq_complete_move(cfg, ~get_irq_regs()->orig_ax);\r\n}\r\nvoid irq_force_complete_move(int irq)\r\n{\r\nstruct irq_cfg *cfg = irq_cfg(irq);\r\nif (!cfg)\r\nreturn;\r\n__irq_complete_move(cfg, cfg->vector);\r\n}\r\nint arch_setup_hwirq(unsigned int irq, int node)\r\n{\r\nstruct irq_cfg *cfg;\r\nunsigned long flags;\r\nint ret;\r\ncfg = alloc_irq_cfg(irq, node);\r\nif (!cfg)\r\nreturn -ENOMEM;\r\nraw_spin_lock_irqsave(&vector_lock, flags);\r\nret = __assign_irq_vector(irq, cfg, apic->target_cpus());\r\nraw_spin_unlock_irqrestore(&vector_lock, flags);\r\nif (!ret)\r\nirq_set_chip_data(irq, cfg);\r\nelse\r\nfree_irq_cfg(irq, cfg);\r\nreturn ret;\r\n}\r\nvoid arch_teardown_hwirq(unsigned int irq)\r\n{\r\nstruct irq_cfg *cfg = irq_cfg(irq);\r\nfree_remapped_irq(irq);\r\nclear_irq_vector(irq, cfg);\r\nfree_irq_cfg(irq, cfg);\r\n}\r\nstatic void __init print_APIC_field(int base)\r\n{\r\nint i;\r\nprintk(KERN_DEBUG);\r\nfor (i = 0; i < 8; i++)\r\npr_cont("%08x", apic_read(base + i*0x10));\r\npr_cont("\n");\r\n}\r\nstatic void __init print_local_APIC(void *dummy)\r\n{\r\nunsigned int i, v, ver, maxlvt;\r\nu64 icr;\r\npr_debug("printing local APIC contents on CPU#%d/%d:\n",\r\nsmp_processor_id(), hard_smp_processor_id());\r\nv = apic_read(APIC_ID);\r\npr_info("... APIC ID: %08x (%01x)\n", v, read_apic_id());\r\nv = apic_read(APIC_LVR);\r\npr_info("... APIC VERSION: %08x\n", v);\r\nver = GET_APIC_VERSION(v);\r\nmaxlvt = lapic_get_maxlvt();\r\nv = apic_read(APIC_TASKPRI);\r\npr_debug("... APIC TASKPRI: %08x (%02x)\n", v, v & APIC_TPRI_MASK);\r\nif (APIC_INTEGRATED(ver)) {\r\nif (!APIC_XAPIC(ver)) {\r\nv = apic_read(APIC_ARBPRI);\r\npr_debug("... APIC ARBPRI: %08x (%02x)\n",\r\nv, v & APIC_ARBPRI_MASK);\r\n}\r\nv = apic_read(APIC_PROCPRI);\r\npr_debug("... APIC PROCPRI: %08x\n", v);\r\n}\r\nif (!APIC_INTEGRATED(ver) || maxlvt == 3) {\r\nv = apic_read(APIC_RRR);\r\npr_debug("... APIC RRR: %08x\n", v);\r\n}\r\nv = apic_read(APIC_LDR);\r\npr_debug("... APIC LDR: %08x\n", v);\r\nif (!x2apic_enabled()) {\r\nv = apic_read(APIC_DFR);\r\npr_debug("... APIC DFR: %08x\n", v);\r\n}\r\nv = apic_read(APIC_SPIV);\r\npr_debug("... APIC SPIV: %08x\n", v);\r\npr_debug("... APIC ISR field:\n");\r\nprint_APIC_field(APIC_ISR);\r\npr_debug("... APIC TMR field:\n");\r\nprint_APIC_field(APIC_TMR);\r\npr_debug("... APIC IRR field:\n");\r\nprint_APIC_field(APIC_IRR);\r\nif (APIC_INTEGRATED(ver)) {\r\nif (maxlvt > 3)\r\napic_write(APIC_ESR, 0);\r\nv = apic_read(APIC_ESR);\r\npr_debug("... APIC ESR: %08x\n", v);\r\n}\r\nicr = apic_icr_read();\r\npr_debug("... APIC ICR: %08x\n", (u32)icr);\r\npr_debug("... APIC ICR2: %08x\n", (u32)(icr >> 32));\r\nv = apic_read(APIC_LVTT);\r\npr_debug("... APIC LVTT: %08x\n", v);\r\nif (maxlvt > 3) {\r\nv = apic_read(APIC_LVTPC);\r\npr_debug("... APIC LVTPC: %08x\n", v);\r\n}\r\nv = apic_read(APIC_LVT0);\r\npr_debug("... APIC LVT0: %08x\n", v);\r\nv = apic_read(APIC_LVT1);\r\npr_debug("... APIC LVT1: %08x\n", v);\r\nif (maxlvt > 2) {\r\nv = apic_read(APIC_LVTERR);\r\npr_debug("... APIC LVTERR: %08x\n", v);\r\n}\r\nv = apic_read(APIC_TMICT);\r\npr_debug("... APIC TMICT: %08x\n", v);\r\nv = apic_read(APIC_TMCCT);\r\npr_debug("... APIC TMCCT: %08x\n", v);\r\nv = apic_read(APIC_TDCR);\r\npr_debug("... APIC TDCR: %08x\n", v);\r\nif (boot_cpu_has(X86_FEATURE_EXTAPIC)) {\r\nv = apic_read(APIC_EFEAT);\r\nmaxlvt = (v >> 16) & 0xff;\r\npr_debug("... APIC EFEAT: %08x\n", v);\r\nv = apic_read(APIC_ECTRL);\r\npr_debug("... APIC ECTRL: %08x\n", v);\r\nfor (i = 0; i < maxlvt; i++) {\r\nv = apic_read(APIC_EILVTn(i));\r\npr_debug("... APIC EILVT%d: %08x\n", i, v);\r\n}\r\n}\r\npr_cont("\n");\r\n}\r\nstatic void __init print_local_APICs(int maxcpu)\r\n{\r\nint cpu;\r\nif (!maxcpu)\r\nreturn;\r\npreempt_disable();\r\nfor_each_online_cpu(cpu) {\r\nif (cpu >= maxcpu)\r\nbreak;\r\nsmp_call_function_single(cpu, print_local_APIC, NULL, 1);\r\n}\r\npreempt_enable();\r\n}\r\nstatic void __init print_PIC(void)\r\n{\r\nunsigned int v;\r\nunsigned long flags;\r\nif (!nr_legacy_irqs())\r\nreturn;\r\npr_debug("\nprinting PIC contents\n");\r\nraw_spin_lock_irqsave(&i8259A_lock, flags);\r\nv = inb(0xa1) << 8 | inb(0x21);\r\npr_debug("... PIC IMR: %04x\n", v);\r\nv = inb(0xa0) << 8 | inb(0x20);\r\npr_debug("... PIC IRR: %04x\n", v);\r\noutb(0x0b, 0xa0);\r\noutb(0x0b, 0x20);\r\nv = inb(0xa0) << 8 | inb(0x20);\r\noutb(0x0a, 0xa0);\r\noutb(0x0a, 0x20);\r\nraw_spin_unlock_irqrestore(&i8259A_lock, flags);\r\npr_debug("... PIC ISR: %04x\n", v);\r\nv = inb(0x4d1) << 8 | inb(0x4d0);\r\npr_debug("... PIC ELCR: %04x\n", v);\r\n}\r\nstatic __init int setup_show_lapic(char *arg)\r\n{\r\nint num = -1;\r\nif (strcmp(arg, "all") == 0) {\r\nshow_lapic = CONFIG_NR_CPUS;\r\n} else {\r\nget_option(&arg, &num);\r\nif (num >= 0)\r\nshow_lapic = num;\r\n}\r\nreturn 1;\r\n}\r\nstatic int __init print_ICs(void)\r\n{\r\nif (apic_verbosity == APIC_QUIET)\r\nreturn 0;\r\nprint_PIC();\r\nif (!cpu_has_apic && !apic_from_smp_config())\r\nreturn 0;\r\nprint_local_APICs(show_lapic);\r\nprint_IO_APICs();\r\nreturn 0;\r\n}
