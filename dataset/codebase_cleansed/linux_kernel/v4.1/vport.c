int ovs_vport_init(void)\r\n{\r\ndev_table = kzalloc(VPORT_HASH_BUCKETS * sizeof(struct hlist_head),\r\nGFP_KERNEL);\r\nif (!dev_table)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid ovs_vport_exit(void)\r\n{\r\nkfree(dev_table);\r\n}\r\nstatic struct hlist_head *hash_bucket(const struct net *net, const char *name)\r\n{\r\nunsigned int hash = jhash(name, strlen(name), (unsigned long) net);\r\nreturn &dev_table[hash & (VPORT_HASH_BUCKETS - 1)];\r\n}\r\nint ovs_vport_ops_register(struct vport_ops *ops)\r\n{\r\nint err = -EEXIST;\r\nstruct vport_ops *o;\r\novs_lock();\r\nlist_for_each_entry(o, &vport_ops_list, list)\r\nif (ops->type == o->type)\r\ngoto errout;\r\nlist_add_tail(&ops->list, &vport_ops_list);\r\nerr = 0;\r\nerrout:\r\novs_unlock();\r\nreturn err;\r\n}\r\nvoid ovs_vport_ops_unregister(struct vport_ops *ops)\r\n{\r\novs_lock();\r\nlist_del(&ops->list);\r\novs_unlock();\r\n}\r\nstruct vport *ovs_vport_locate(const struct net *net, const char *name)\r\n{\r\nstruct hlist_head *bucket = hash_bucket(net, name);\r\nstruct vport *vport;\r\nhlist_for_each_entry_rcu(vport, bucket, hash_node)\r\nif (!strcmp(name, vport->ops->get_name(vport)) &&\r\nnet_eq(ovs_dp_get_net(vport->dp), net))\r\nreturn vport;\r\nreturn NULL;\r\n}\r\nstruct vport *ovs_vport_alloc(int priv_size, const struct vport_ops *ops,\r\nconst struct vport_parms *parms)\r\n{\r\nstruct vport *vport;\r\nsize_t alloc_size;\r\nalloc_size = sizeof(struct vport);\r\nif (priv_size) {\r\nalloc_size = ALIGN(alloc_size, VPORT_ALIGN);\r\nalloc_size += priv_size;\r\n}\r\nvport = kzalloc(alloc_size, GFP_KERNEL);\r\nif (!vport)\r\nreturn ERR_PTR(-ENOMEM);\r\nvport->dp = parms->dp;\r\nvport->port_no = parms->port_no;\r\nvport->ops = ops;\r\nINIT_HLIST_NODE(&vport->dp_hash_node);\r\nif (ovs_vport_set_upcall_portids(vport, parms->upcall_portids)) {\r\nkfree(vport);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nvport->percpu_stats = netdev_alloc_pcpu_stats(struct pcpu_sw_netstats);\r\nif (!vport->percpu_stats) {\r\nkfree(vport);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nreturn vport;\r\n}\r\nvoid ovs_vport_free(struct vport *vport)\r\n{\r\nkfree(rcu_dereference_raw(vport->upcall_portids));\r\nfree_percpu(vport->percpu_stats);\r\nkfree(vport);\r\n}\r\nstatic struct vport_ops *ovs_vport_lookup(const struct vport_parms *parms)\r\n{\r\nstruct vport_ops *ops;\r\nlist_for_each_entry(ops, &vport_ops_list, list)\r\nif (ops->type == parms->type)\r\nreturn ops;\r\nreturn NULL;\r\n}\r\nstruct vport *ovs_vport_add(const struct vport_parms *parms)\r\n{\r\nstruct vport_ops *ops;\r\nstruct vport *vport;\r\nops = ovs_vport_lookup(parms);\r\nif (ops) {\r\nstruct hlist_head *bucket;\r\nif (!try_module_get(ops->owner))\r\nreturn ERR_PTR(-EAFNOSUPPORT);\r\nvport = ops->create(parms);\r\nif (IS_ERR(vport)) {\r\nmodule_put(ops->owner);\r\nreturn vport;\r\n}\r\nbucket = hash_bucket(ovs_dp_get_net(vport->dp),\r\nvport->ops->get_name(vport));\r\nhlist_add_head_rcu(&vport->hash_node, bucket);\r\nreturn vport;\r\n}\r\novs_unlock();\r\nrequest_module("vport-type-%d", parms->type);\r\novs_lock();\r\nif (!ovs_vport_lookup(parms))\r\nreturn ERR_PTR(-EAFNOSUPPORT);\r\nelse\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nint ovs_vport_set_options(struct vport *vport, struct nlattr *options)\r\n{\r\nif (!vport->ops->set_options)\r\nreturn -EOPNOTSUPP;\r\nreturn vport->ops->set_options(vport, options);\r\n}\r\nvoid ovs_vport_del(struct vport *vport)\r\n{\r\nASSERT_OVSL();\r\nhlist_del_rcu(&vport->hash_node);\r\nmodule_put(vport->ops->owner);\r\nvport->ops->destroy(vport);\r\n}\r\nvoid ovs_vport_get_stats(struct vport *vport, struct ovs_vport_stats *stats)\r\n{\r\nint i;\r\nmemset(stats, 0, sizeof(*stats));\r\nstats->rx_errors = atomic_long_read(&vport->err_stats.rx_errors);\r\nstats->tx_errors = atomic_long_read(&vport->err_stats.tx_errors);\r\nstats->tx_dropped = atomic_long_read(&vport->err_stats.tx_dropped);\r\nstats->rx_dropped = atomic_long_read(&vport->err_stats.rx_dropped);\r\nfor_each_possible_cpu(i) {\r\nconst struct pcpu_sw_netstats *percpu_stats;\r\nstruct pcpu_sw_netstats local_stats;\r\nunsigned int start;\r\npercpu_stats = per_cpu_ptr(vport->percpu_stats, i);\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&percpu_stats->syncp);\r\nlocal_stats = *percpu_stats;\r\n} while (u64_stats_fetch_retry_irq(&percpu_stats->syncp, start));\r\nstats->rx_bytes += local_stats.rx_bytes;\r\nstats->rx_packets += local_stats.rx_packets;\r\nstats->tx_bytes += local_stats.tx_bytes;\r\nstats->tx_packets += local_stats.tx_packets;\r\n}\r\n}\r\nint ovs_vport_get_options(const struct vport *vport, struct sk_buff *skb)\r\n{\r\nstruct nlattr *nla;\r\nint err;\r\nif (!vport->ops->get_options)\r\nreturn 0;\r\nnla = nla_nest_start(skb, OVS_VPORT_ATTR_OPTIONS);\r\nif (!nla)\r\nreturn -EMSGSIZE;\r\nerr = vport->ops->get_options(vport, skb);\r\nif (err) {\r\nnla_nest_cancel(skb, nla);\r\nreturn err;\r\n}\r\nnla_nest_end(skb, nla);\r\nreturn 0;\r\n}\r\nint ovs_vport_set_upcall_portids(struct vport *vport, const struct nlattr *ids)\r\n{\r\nstruct vport_portids *old, *vport_portids;\r\nif (!nla_len(ids) || nla_len(ids) % sizeof(u32))\r\nreturn -EINVAL;\r\nold = ovsl_dereference(vport->upcall_portids);\r\nvport_portids = kmalloc(sizeof(*vport_portids) + nla_len(ids),\r\nGFP_KERNEL);\r\nif (!vport_portids)\r\nreturn -ENOMEM;\r\nvport_portids->n_ids = nla_len(ids) / sizeof(u32);\r\nvport_portids->rn_ids = reciprocal_value(vport_portids->n_ids);\r\nnla_memcpy(vport_portids->ids, ids, nla_len(ids));\r\nrcu_assign_pointer(vport->upcall_portids, vport_portids);\r\nif (old)\r\nkfree_rcu(old, rcu);\r\nreturn 0;\r\n}\r\nint ovs_vport_get_upcall_portids(const struct vport *vport,\r\nstruct sk_buff *skb)\r\n{\r\nstruct vport_portids *ids;\r\nids = rcu_dereference_ovsl(vport->upcall_portids);\r\nif (vport->dp->user_features & OVS_DP_F_VPORT_PIDS)\r\nreturn nla_put(skb, OVS_VPORT_ATTR_UPCALL_PID,\r\nids->n_ids * sizeof(u32), (void *)ids->ids);\r\nelse\r\nreturn nla_put_u32(skb, OVS_VPORT_ATTR_UPCALL_PID, ids->ids[0]);\r\n}\r\nu32 ovs_vport_find_upcall_portid(const struct vport *vport, struct sk_buff *skb)\r\n{\r\nstruct vport_portids *ids;\r\nu32 ids_index;\r\nu32 hash;\r\nids = rcu_dereference(vport->upcall_portids);\r\nif (ids->n_ids == 1 && ids->ids[0] == 0)\r\nreturn 0;\r\nhash = skb_get_hash(skb);\r\nids_index = hash - ids->n_ids * reciprocal_divide(hash, ids->rn_ids);\r\nreturn ids->ids[ids_index];\r\n}\r\nvoid ovs_vport_receive(struct vport *vport, struct sk_buff *skb,\r\nconst struct ovs_tunnel_info *tun_info)\r\n{\r\nstruct pcpu_sw_netstats *stats;\r\nstruct sw_flow_key key;\r\nint error;\r\nstats = this_cpu_ptr(vport->percpu_stats);\r\nu64_stats_update_begin(&stats->syncp);\r\nstats->rx_packets++;\r\nstats->rx_bytes += skb->len +\r\n(skb_vlan_tag_present(skb) ? VLAN_HLEN : 0);\r\nu64_stats_update_end(&stats->syncp);\r\nOVS_CB(skb)->input_vport = vport;\r\nOVS_CB(skb)->egress_tun_info = NULL;\r\nerror = ovs_flow_key_extract(tun_info, skb, &key);\r\nif (unlikely(error)) {\r\nkfree_skb(skb);\r\nreturn;\r\n}\r\novs_dp_process_packet(skb, &key);\r\n}\r\nint ovs_vport_send(struct vport *vport, struct sk_buff *skb)\r\n{\r\nint sent = vport->ops->send(vport, skb);\r\nif (likely(sent > 0)) {\r\nstruct pcpu_sw_netstats *stats;\r\nstats = this_cpu_ptr(vport->percpu_stats);\r\nu64_stats_update_begin(&stats->syncp);\r\nstats->tx_packets++;\r\nstats->tx_bytes += sent;\r\nu64_stats_update_end(&stats->syncp);\r\n} else if (sent < 0) {\r\novs_vport_record_error(vport, VPORT_E_TX_ERROR);\r\n} else {\r\novs_vport_record_error(vport, VPORT_E_TX_DROPPED);\r\n}\r\nreturn sent;\r\n}\r\nstatic void ovs_vport_record_error(struct vport *vport,\r\nenum vport_err_type err_type)\r\n{\r\nswitch (err_type) {\r\ncase VPORT_E_RX_DROPPED:\r\natomic_long_inc(&vport->err_stats.rx_dropped);\r\nbreak;\r\ncase VPORT_E_RX_ERROR:\r\natomic_long_inc(&vport->err_stats.rx_errors);\r\nbreak;\r\ncase VPORT_E_TX_DROPPED:\r\natomic_long_inc(&vport->err_stats.tx_dropped);\r\nbreak;\r\ncase VPORT_E_TX_ERROR:\r\natomic_long_inc(&vport->err_stats.tx_errors);\r\nbreak;\r\n}\r\n}\r\nstatic void free_vport_rcu(struct rcu_head *rcu)\r\n{\r\nstruct vport *vport = container_of(rcu, struct vport, rcu);\r\novs_vport_free(vport);\r\n}\r\nvoid ovs_vport_deferred_free(struct vport *vport)\r\n{\r\nif (!vport)\r\nreturn;\r\ncall_rcu(&vport->rcu, free_vport_rcu);\r\n}\r\nint ovs_tunnel_get_egress_info(struct ovs_tunnel_info *egress_tun_info,\r\nstruct net *net,\r\nconst struct ovs_tunnel_info *tun_info,\r\nu8 ipproto,\r\nu32 skb_mark,\r\n__be16 tp_src,\r\n__be16 tp_dst)\r\n{\r\nconst struct ovs_key_ipv4_tunnel *tun_key;\r\nstruct rtable *rt;\r\nstruct flowi4 fl;\r\nif (unlikely(!tun_info))\r\nreturn -EINVAL;\r\ntun_key = &tun_info->tunnel;\r\nrt = ovs_tunnel_route_lookup(net, tun_key, skb_mark, &fl, ipproto);\r\nif (IS_ERR(rt))\r\nreturn PTR_ERR(rt);\r\nip_rt_put(rt);\r\n__ovs_flow_tun_info_init(egress_tun_info,\r\nfl.saddr, tun_key->ipv4_dst,\r\ntun_key->ipv4_tos,\r\ntun_key->ipv4_ttl,\r\ntp_src, tp_dst,\r\ntun_key->tun_id,\r\ntun_key->tun_flags,\r\ntun_info->options,\r\ntun_info->options_len);\r\nreturn 0;\r\n}\r\nint ovs_vport_get_egress_tun_info(struct vport *vport, struct sk_buff *skb,\r\nstruct ovs_tunnel_info *info)\r\n{\r\nif (unlikely(!vport->ops->get_egress_tun_info))\r\nreturn -EINVAL;\r\nreturn vport->ops->get_egress_tun_info(vport, skb, info);\r\n}
