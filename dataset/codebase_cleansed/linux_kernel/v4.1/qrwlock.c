static __always_inline void\r\nrspin_until_writer_unlock(struct qrwlock *lock, u32 cnts)\r\n{\r\nwhile ((cnts & _QW_WMASK) == _QW_LOCKED) {\r\ncpu_relax_lowlatency();\r\ncnts = smp_load_acquire((u32 *)&lock->cnts);\r\n}\r\n}\r\nvoid queue_read_lock_slowpath(struct qrwlock *lock)\r\n{\r\nu32 cnts;\r\nif (unlikely(in_interrupt())) {\r\ncnts = smp_load_acquire((u32 *)&lock->cnts);\r\nrspin_until_writer_unlock(lock, cnts);\r\nreturn;\r\n}\r\natomic_sub(_QR_BIAS, &lock->cnts);\r\narch_spin_lock(&lock->lock);\r\nwhile (atomic_read(&lock->cnts) & _QW_WMASK)\r\ncpu_relax_lowlatency();\r\ncnts = atomic_add_return(_QR_BIAS, &lock->cnts) - _QR_BIAS;\r\nrspin_until_writer_unlock(lock, cnts);\r\narch_spin_unlock(&lock->lock);\r\n}\r\nvoid queue_write_lock_slowpath(struct qrwlock *lock)\r\n{\r\nu32 cnts;\r\narch_spin_lock(&lock->lock);\r\nif (!atomic_read(&lock->cnts) &&\r\n(atomic_cmpxchg(&lock->cnts, 0, _QW_LOCKED) == 0))\r\ngoto unlock;\r\nfor (;;) {\r\ncnts = atomic_read(&lock->cnts);\r\nif (!(cnts & _QW_WMASK) &&\r\n(atomic_cmpxchg(&lock->cnts, cnts,\r\ncnts | _QW_WAITING) == cnts))\r\nbreak;\r\ncpu_relax_lowlatency();\r\n}\r\nfor (;;) {\r\ncnts = atomic_read(&lock->cnts);\r\nif ((cnts == _QW_WAITING) &&\r\n(atomic_cmpxchg(&lock->cnts, _QW_WAITING,\r\n_QW_LOCKED) == _QW_WAITING))\r\nbreak;\r\ncpu_relax_lowlatency();\r\n}\r\nunlock:\r\narch_spin_unlock(&lock->lock);\r\n}
