static unsigned long cancel_userptr(struct drm_i915_gem_object *obj)\r\n{\r\nstruct drm_device *dev = obj->base.dev;\r\nunsigned long end;\r\nmutex_lock(&dev->struct_mutex);\r\nobj->userptr.work = NULL;\r\nif (obj->pages != NULL) {\r\nstruct drm_i915_private *dev_priv = to_i915(dev);\r\nstruct i915_vma *vma, *tmp;\r\nbool was_interruptible;\r\nwas_interruptible = dev_priv->mm.interruptible;\r\ndev_priv->mm.interruptible = false;\r\nlist_for_each_entry_safe(vma, tmp, &obj->vma_list, vma_link) {\r\nint ret = i915_vma_unbind(vma);\r\nWARN_ON(ret && ret != -EIO);\r\n}\r\nWARN_ON(i915_gem_object_put_pages(obj));\r\ndev_priv->mm.interruptible = was_interruptible;\r\n}\r\nend = obj->userptr.ptr + obj->base.size;\r\ndrm_gem_object_unreference(&obj->base);\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn end;\r\n}\r\nstatic void *invalidate_range__linear(struct i915_mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start,\r\nunsigned long end)\r\n{\r\nstruct i915_mmu_object *mo;\r\nunsigned long serial;\r\nrestart:\r\nserial = mn->serial;\r\nlist_for_each_entry(mo, &mn->linear, link) {\r\nstruct drm_i915_gem_object *obj;\r\nif (mo->it.last < start || mo->it.start > end)\r\ncontinue;\r\nobj = mo->obj;\r\nif (!kref_get_unless_zero(&obj->base.refcount))\r\ncontinue;\r\nspin_unlock(&mn->lock);\r\ncancel_userptr(obj);\r\nspin_lock(&mn->lock);\r\nif (serial != mn->serial)\r\ngoto restart;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void i915_gem_userptr_mn_invalidate_range_start(struct mmu_notifier *_mn,\r\nstruct mm_struct *mm,\r\nunsigned long start,\r\nunsigned long end)\r\n{\r\nstruct i915_mmu_notifier *mn = container_of(_mn, struct i915_mmu_notifier, mn);\r\nstruct interval_tree_node *it = NULL;\r\nunsigned long next = start;\r\nunsigned long serial = 0;\r\nend--;\r\nwhile (next < end) {\r\nstruct drm_i915_gem_object *obj = NULL;\r\nspin_lock(&mn->lock);\r\nif (mn->has_linear)\r\nit = invalidate_range__linear(mn, mm, start, end);\r\nelse if (serial == mn->serial)\r\nit = interval_tree_iter_next(it, next, end);\r\nelse\r\nit = interval_tree_iter_first(&mn->objects, start, end);\r\nif (it != NULL) {\r\nobj = container_of(it, struct i915_mmu_object, it)->obj;\r\nif (!kref_get_unless_zero(&obj->base.refcount)) {\r\nspin_unlock(&mn->lock);\r\nserial = 0;\r\ncontinue;\r\n}\r\nserial = mn->serial;\r\n}\r\nspin_unlock(&mn->lock);\r\nif (obj == NULL)\r\nreturn;\r\nnext = cancel_userptr(obj);\r\n}\r\n}\r\nstatic struct i915_mmu_notifier *\r\ni915_mmu_notifier_create(struct mm_struct *mm)\r\n{\r\nstruct i915_mmu_notifier *mn;\r\nint ret;\r\nmn = kmalloc(sizeof(*mn), GFP_KERNEL);\r\nif (mn == NULL)\r\nreturn ERR_PTR(-ENOMEM);\r\nspin_lock_init(&mn->lock);\r\nmn->mn.ops = &i915_gem_userptr_notifier;\r\nmn->objects = RB_ROOT;\r\nmn->serial = 1;\r\nINIT_LIST_HEAD(&mn->linear);\r\nmn->has_linear = false;\r\nret = __mmu_notifier_register(&mn->mn, mm);\r\nif (ret) {\r\nkfree(mn);\r\nreturn ERR_PTR(ret);\r\n}\r\nreturn mn;\r\n}\r\nstatic void __i915_mmu_notifier_update_serial(struct i915_mmu_notifier *mn)\r\n{\r\nif (++mn->serial == 0)\r\nmn->serial = 1;\r\n}\r\nstatic int\r\ni915_mmu_notifier_add(struct drm_device *dev,\r\nstruct i915_mmu_notifier *mn,\r\nstruct i915_mmu_object *mo)\r\n{\r\nstruct interval_tree_node *it;\r\nint ret;\r\nret = i915_mutex_lock_interruptible(dev);\r\nif (ret)\r\nreturn ret;\r\ni915_gem_retire_requests(dev);\r\nspin_lock(&mn->lock);\r\nit = interval_tree_iter_first(&mn->objects,\r\nmo->it.start, mo->it.last);\r\nif (it) {\r\nstruct drm_i915_gem_object *obj;\r\nobj = container_of(it, struct i915_mmu_object, it)->obj;\r\nif (!obj->userptr.workers)\r\nmn->has_linear = mo->is_linear = true;\r\nelse\r\nret = -EAGAIN;\r\n} else\r\ninterval_tree_insert(&mo->it, &mn->objects);\r\nif (ret == 0) {\r\nlist_add(&mo->link, &mn->linear);\r\n__i915_mmu_notifier_update_serial(mn);\r\n}\r\nspin_unlock(&mn->lock);\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn ret;\r\n}\r\nstatic bool i915_mmu_notifier_has_linear(struct i915_mmu_notifier *mn)\r\n{\r\nstruct i915_mmu_object *mo;\r\nlist_for_each_entry(mo, &mn->linear, link)\r\nif (mo->is_linear)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void\r\ni915_mmu_notifier_del(struct i915_mmu_notifier *mn,\r\nstruct i915_mmu_object *mo)\r\n{\r\nspin_lock(&mn->lock);\r\nlist_del(&mo->link);\r\nif (mo->is_linear)\r\nmn->has_linear = i915_mmu_notifier_has_linear(mn);\r\nelse\r\ninterval_tree_remove(&mo->it, &mn->objects);\r\n__i915_mmu_notifier_update_serial(mn);\r\nspin_unlock(&mn->lock);\r\n}\r\nstatic void\r\ni915_gem_userptr_release__mmu_notifier(struct drm_i915_gem_object *obj)\r\n{\r\nstruct i915_mmu_object *mo;\r\nmo = obj->userptr.mmu_object;\r\nif (mo == NULL)\r\nreturn;\r\ni915_mmu_notifier_del(mo->mn, mo);\r\nkfree(mo);\r\nobj->userptr.mmu_object = NULL;\r\n}\r\nstatic struct i915_mmu_notifier *\r\ni915_mmu_notifier_find(struct i915_mm_struct *mm)\r\n{\r\nstruct i915_mmu_notifier *mn = mm->mn;\r\nmn = mm->mn;\r\nif (mn)\r\nreturn mn;\r\ndown_write(&mm->mm->mmap_sem);\r\nmutex_lock(&to_i915(mm->dev)->mm_lock);\r\nif ((mn = mm->mn) == NULL) {\r\nmn = i915_mmu_notifier_create(mm->mm);\r\nif (!IS_ERR(mn))\r\nmm->mn = mn;\r\n}\r\nmutex_unlock(&to_i915(mm->dev)->mm_lock);\r\nup_write(&mm->mm->mmap_sem);\r\nreturn mn;\r\n}\r\nstatic int\r\ni915_gem_userptr_init__mmu_notifier(struct drm_i915_gem_object *obj,\r\nunsigned flags)\r\n{\r\nstruct i915_mmu_notifier *mn;\r\nstruct i915_mmu_object *mo;\r\nint ret;\r\nif (flags & I915_USERPTR_UNSYNCHRONIZED)\r\nreturn capable(CAP_SYS_ADMIN) ? 0 : -EPERM;\r\nif (WARN_ON(obj->userptr.mm == NULL))\r\nreturn -EINVAL;\r\nmn = i915_mmu_notifier_find(obj->userptr.mm);\r\nif (IS_ERR(mn))\r\nreturn PTR_ERR(mn);\r\nmo = kzalloc(sizeof(*mo), GFP_KERNEL);\r\nif (mo == NULL)\r\nreturn -ENOMEM;\r\nmo->mn = mn;\r\nmo->it.start = obj->userptr.ptr;\r\nmo->it.last = mo->it.start + obj->base.size - 1;\r\nmo->obj = obj;\r\nret = i915_mmu_notifier_add(obj->base.dev, mn, mo);\r\nif (ret) {\r\nkfree(mo);\r\nreturn ret;\r\n}\r\nobj->userptr.mmu_object = mo;\r\nreturn 0;\r\n}\r\nstatic void\r\ni915_mmu_notifier_free(struct i915_mmu_notifier *mn,\r\nstruct mm_struct *mm)\r\n{\r\nif (mn == NULL)\r\nreturn;\r\nmmu_notifier_unregister(&mn->mn, mm);\r\nkfree(mn);\r\n}\r\nstatic void\r\ni915_gem_userptr_release__mmu_notifier(struct drm_i915_gem_object *obj)\r\n{\r\n}\r\nstatic int\r\ni915_gem_userptr_init__mmu_notifier(struct drm_i915_gem_object *obj,\r\nunsigned flags)\r\n{\r\nif ((flags & I915_USERPTR_UNSYNCHRONIZED) == 0)\r\nreturn -ENODEV;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic void\r\ni915_mmu_notifier_free(struct i915_mmu_notifier *mn,\r\nstruct mm_struct *mm)\r\n{\r\n}\r\nstatic struct i915_mm_struct *\r\n__i915_mm_struct_find(struct drm_i915_private *dev_priv, struct mm_struct *real)\r\n{\r\nstruct i915_mm_struct *mm;\r\nhash_for_each_possible(dev_priv->mm_structs, mm, node, (unsigned long)real)\r\nif (mm->mm == real)\r\nreturn mm;\r\nreturn NULL;\r\n}\r\nstatic int\r\ni915_gem_userptr_init__mm_struct(struct drm_i915_gem_object *obj)\r\n{\r\nstruct drm_i915_private *dev_priv = to_i915(obj->base.dev);\r\nstruct i915_mm_struct *mm;\r\nint ret = 0;\r\nmutex_lock(&dev_priv->mm_lock);\r\nmm = __i915_mm_struct_find(dev_priv, current->mm);\r\nif (mm == NULL) {\r\nmm = kmalloc(sizeof(*mm), GFP_KERNEL);\r\nif (mm == NULL) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nkref_init(&mm->kref);\r\nmm->dev = obj->base.dev;\r\nmm->mm = current->mm;\r\natomic_inc(&current->mm->mm_count);\r\nmm->mn = NULL;\r\nhash_add(dev_priv->mm_structs,\r\n&mm->node, (unsigned long)mm->mm);\r\n} else\r\nkref_get(&mm->kref);\r\nobj->userptr.mm = mm;\r\nout:\r\nmutex_unlock(&dev_priv->mm_lock);\r\nreturn ret;\r\n}\r\nstatic void\r\n__i915_mm_struct_free__worker(struct work_struct *work)\r\n{\r\nstruct i915_mm_struct *mm = container_of(work, typeof(*mm), work);\r\ni915_mmu_notifier_free(mm->mn, mm->mm);\r\nmmdrop(mm->mm);\r\nkfree(mm);\r\n}\r\nstatic void\r\n__i915_mm_struct_free(struct kref *kref)\r\n{\r\nstruct i915_mm_struct *mm = container_of(kref, typeof(*mm), kref);\r\nhash_del(&mm->node);\r\nmutex_unlock(&to_i915(mm->dev)->mm_lock);\r\nINIT_WORK(&mm->work, __i915_mm_struct_free__worker);\r\nschedule_work(&mm->work);\r\n}\r\nstatic void\r\ni915_gem_userptr_release__mm_struct(struct drm_i915_gem_object *obj)\r\n{\r\nif (obj->userptr.mm == NULL)\r\nreturn;\r\nkref_put_mutex(&obj->userptr.mm->kref,\r\n__i915_mm_struct_free,\r\n&to_i915(obj->base.dev)->mm_lock);\r\nobj->userptr.mm = NULL;\r\n}\r\nstatic int\r\nst_set_pages(struct sg_table **st, struct page **pvec, int num_pages)\r\n{\r\nstruct scatterlist *sg;\r\nint ret, n;\r\n*st = kmalloc(sizeof(**st), GFP_KERNEL);\r\nif (*st == NULL)\r\nreturn -ENOMEM;\r\nif (swiotlb_active()) {\r\nret = sg_alloc_table(*st, num_pages, GFP_KERNEL);\r\nif (ret)\r\ngoto err;\r\nfor_each_sg((*st)->sgl, sg, num_pages, n)\r\nsg_set_page(sg, pvec[n], PAGE_SIZE, 0);\r\n} else {\r\nret = sg_alloc_table_from_pages(*st, pvec, num_pages,\r\n0, num_pages << PAGE_SHIFT,\r\nGFP_KERNEL);\r\nif (ret)\r\ngoto err;\r\n}\r\nreturn 0;\r\nerr:\r\nkfree(*st);\r\n*st = NULL;\r\nreturn ret;\r\n}\r\nstatic void\r\n__i915_gem_userptr_get_pages_worker(struct work_struct *_work)\r\n{\r\nstruct get_pages_work *work = container_of(_work, typeof(*work), work);\r\nstruct drm_i915_gem_object *obj = work->obj;\r\nstruct drm_device *dev = obj->base.dev;\r\nconst int num_pages = obj->base.size >> PAGE_SHIFT;\r\nstruct page **pvec;\r\nint pinned, ret;\r\nret = -ENOMEM;\r\npinned = 0;\r\npvec = kmalloc(num_pages*sizeof(struct page *),\r\nGFP_TEMPORARY | __GFP_NOWARN | __GFP_NORETRY);\r\nif (pvec == NULL)\r\npvec = drm_malloc_ab(num_pages, sizeof(struct page *));\r\nif (pvec != NULL) {\r\nstruct mm_struct *mm = obj->userptr.mm->mm;\r\ndown_read(&mm->mmap_sem);\r\nwhile (pinned < num_pages) {\r\nret = get_user_pages(work->task, mm,\r\nobj->userptr.ptr + pinned * PAGE_SIZE,\r\nnum_pages - pinned,\r\n!obj->userptr.read_only, 0,\r\npvec + pinned, NULL);\r\nif (ret < 0)\r\nbreak;\r\npinned += ret;\r\n}\r\nup_read(&mm->mmap_sem);\r\n}\r\nmutex_lock(&dev->struct_mutex);\r\nif (obj->userptr.work != &work->work) {\r\nret = 0;\r\n} else if (pinned == num_pages) {\r\nret = st_set_pages(&obj->pages, pvec, num_pages);\r\nif (ret == 0) {\r\nlist_add_tail(&obj->global_list, &to_i915(dev)->mm.unbound_list);\r\npinned = 0;\r\n}\r\n}\r\nobj->userptr.work = ERR_PTR(ret);\r\nobj->userptr.workers--;\r\ndrm_gem_object_unreference(&obj->base);\r\nmutex_unlock(&dev->struct_mutex);\r\nrelease_pages(pvec, pinned, 0);\r\ndrm_free_large(pvec);\r\nput_task_struct(work->task);\r\nkfree(work);\r\n}\r\nstatic int\r\ni915_gem_userptr_get_pages(struct drm_i915_gem_object *obj)\r\n{\r\nconst int num_pages = obj->base.size >> PAGE_SHIFT;\r\nstruct page **pvec;\r\nint pinned, ret;\r\npvec = NULL;\r\npinned = 0;\r\nif (obj->userptr.mm->mm == current->mm) {\r\npvec = kmalloc(num_pages*sizeof(struct page *),\r\nGFP_TEMPORARY | __GFP_NOWARN | __GFP_NORETRY);\r\nif (pvec == NULL) {\r\npvec = drm_malloc_ab(num_pages, sizeof(struct page *));\r\nif (pvec == NULL)\r\nreturn -ENOMEM;\r\n}\r\npinned = __get_user_pages_fast(obj->userptr.ptr, num_pages,\r\n!obj->userptr.read_only, pvec);\r\n}\r\nif (pinned < num_pages) {\r\nif (pinned < 0) {\r\nret = pinned;\r\npinned = 0;\r\n} else {\r\nret = -EAGAIN;\r\nif (obj->userptr.work == NULL &&\r\nobj->userptr.workers < I915_GEM_USERPTR_MAX_WORKERS) {\r\nstruct get_pages_work *work;\r\nwork = kmalloc(sizeof(*work), GFP_KERNEL);\r\nif (work != NULL) {\r\nobj->userptr.work = &work->work;\r\nobj->userptr.workers++;\r\nwork->obj = obj;\r\ndrm_gem_object_reference(&obj->base);\r\nwork->task = current;\r\nget_task_struct(work->task);\r\nINIT_WORK(&work->work, __i915_gem_userptr_get_pages_worker);\r\nschedule_work(&work->work);\r\n} else\r\nret = -ENOMEM;\r\n} else {\r\nif (IS_ERR(obj->userptr.work)) {\r\nret = PTR_ERR(obj->userptr.work);\r\nobj->userptr.work = NULL;\r\n}\r\n}\r\n}\r\n} else {\r\nret = st_set_pages(&obj->pages, pvec, num_pages);\r\nif (ret == 0) {\r\nobj->userptr.work = NULL;\r\npinned = 0;\r\n}\r\n}\r\nrelease_pages(pvec, pinned, 0);\r\ndrm_free_large(pvec);\r\nreturn ret;\r\n}\r\nstatic void\r\ni915_gem_userptr_put_pages(struct drm_i915_gem_object *obj)\r\n{\r\nstruct sg_page_iter sg_iter;\r\nBUG_ON(obj->userptr.work != NULL);\r\nif (obj->madv != I915_MADV_WILLNEED)\r\nobj->dirty = 0;\r\nfor_each_sg_page(obj->pages->sgl, &sg_iter, obj->pages->nents, 0) {\r\nstruct page *page = sg_page_iter_page(&sg_iter);\r\nif (obj->dirty)\r\nset_page_dirty(page);\r\nmark_page_accessed(page);\r\npage_cache_release(page);\r\n}\r\nobj->dirty = 0;\r\nsg_free_table(obj->pages);\r\nkfree(obj->pages);\r\n}\r\nstatic void\r\ni915_gem_userptr_release(struct drm_i915_gem_object *obj)\r\n{\r\ni915_gem_userptr_release__mmu_notifier(obj);\r\ni915_gem_userptr_release__mm_struct(obj);\r\n}\r\nstatic int\r\ni915_gem_userptr_dmabuf_export(struct drm_i915_gem_object *obj)\r\n{\r\nif (obj->userptr.mmu_object)\r\nreturn 0;\r\nreturn i915_gem_userptr_init__mmu_notifier(obj, 0);\r\n}\r\nint\r\ni915_gem_userptr_ioctl(struct drm_device *dev, void *data, struct drm_file *file)\r\n{\r\nstruct drm_i915_private *dev_priv = dev->dev_private;\r\nstruct drm_i915_gem_userptr *args = data;\r\nstruct drm_i915_gem_object *obj;\r\nint ret;\r\nu32 handle;\r\nif (args->flags & ~(I915_USERPTR_READ_ONLY |\r\nI915_USERPTR_UNSYNCHRONIZED))\r\nreturn -EINVAL;\r\nif (offset_in_page(args->user_ptr | args->user_size))\r\nreturn -EINVAL;\r\nif (args->user_size > dev_priv->gtt.base.total)\r\nreturn -E2BIG;\r\nif (!access_ok(args->flags & I915_USERPTR_READ_ONLY ? VERIFY_READ : VERIFY_WRITE,\r\n(char __user *)(unsigned long)args->user_ptr, args->user_size))\r\nreturn -EFAULT;\r\nif (args->flags & I915_USERPTR_READ_ONLY) {\r\nreturn -ENODEV;\r\n}\r\nobj = i915_gem_object_alloc(dev);\r\nif (obj == NULL)\r\nreturn -ENOMEM;\r\ndrm_gem_private_object_init(dev, &obj->base, args->user_size);\r\ni915_gem_object_init(obj, &i915_gem_userptr_ops);\r\nobj->cache_level = I915_CACHE_LLC;\r\nobj->base.write_domain = I915_GEM_DOMAIN_CPU;\r\nobj->base.read_domains = I915_GEM_DOMAIN_CPU;\r\nobj->userptr.ptr = args->user_ptr;\r\nobj->userptr.read_only = !!(args->flags & I915_USERPTR_READ_ONLY);\r\nret = i915_gem_userptr_init__mm_struct(obj);\r\nif (ret == 0)\r\nret = i915_gem_userptr_init__mmu_notifier(obj, args->flags);\r\nif (ret == 0)\r\nret = drm_gem_handle_create(file, &obj->base, &handle);\r\ndrm_gem_object_unreference_unlocked(&obj->base);\r\nif (ret)\r\nreturn ret;\r\nargs->handle = handle;\r\nreturn 0;\r\n}\r\nint\r\ni915_gem_init_userptr(struct drm_device *dev)\r\n{\r\nstruct drm_i915_private *dev_priv = to_i915(dev);\r\nmutex_init(&dev_priv->mm_lock);\r\nhash_init(dev_priv->mm_structs);\r\nreturn 0;\r\n}
