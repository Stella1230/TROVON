static bool is_nb_event(struct perf_event *event)\r\n{\r\nreturn event->pmu->type == amd_nb_pmu.type;\r\n}\r\nstatic bool is_l2_event(struct perf_event *event)\r\n{\r\nreturn event->pmu->type == amd_l2_pmu.type;\r\n}\r\nstatic struct amd_uncore *event_to_amd_uncore(struct perf_event *event)\r\n{\r\nif (is_nb_event(event) && amd_uncore_nb)\r\nreturn *per_cpu_ptr(amd_uncore_nb, event->cpu);\r\nelse if (is_l2_event(event) && amd_uncore_l2)\r\nreturn *per_cpu_ptr(amd_uncore_l2, event->cpu);\r\nreturn NULL;\r\n}\r\nstatic void amd_uncore_read(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 prev, new;\r\ns64 delta;\r\nprev = local64_read(&hwc->prev_count);\r\nrdpmcl(hwc->event_base_rdpmc, new);\r\nlocal64_set(&hwc->prev_count, new);\r\ndelta = (new << COUNTER_SHIFT) - (prev << COUNTER_SHIFT);\r\ndelta >>= COUNTER_SHIFT;\r\nlocal64_add(delta, &event->count);\r\n}\r\nstatic void amd_uncore_start(struct perf_event *event, int flags)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (flags & PERF_EF_RELOAD)\r\nwrmsrl(hwc->event_base, (u64)local64_read(&hwc->prev_count));\r\nhwc->state = 0;\r\nwrmsrl(hwc->config_base, (hwc->config | ARCH_PERFMON_EVENTSEL_ENABLE));\r\nperf_event_update_userpage(event);\r\n}\r\nstatic void amd_uncore_stop(struct perf_event *event, int flags)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nwrmsrl(hwc->config_base, hwc->config);\r\nhwc->state |= PERF_HES_STOPPED;\r\nif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\r\namd_uncore_read(event);\r\nhwc->state |= PERF_HES_UPTODATE;\r\n}\r\n}\r\nstatic int amd_uncore_add(struct perf_event *event, int flags)\r\n{\r\nint i;\r\nstruct amd_uncore *uncore = event_to_amd_uncore(event);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (hwc->idx != -1 && uncore->events[hwc->idx] == event)\r\ngoto out;\r\nfor (i = 0; i < uncore->num_counters; i++) {\r\nif (uncore->events[i] == event) {\r\nhwc->idx = i;\r\ngoto out;\r\n}\r\n}\r\nhwc->idx = -1;\r\nfor (i = 0; i < uncore->num_counters; i++) {\r\nif (cmpxchg(&uncore->events[i], NULL, event) == NULL) {\r\nhwc->idx = i;\r\nbreak;\r\n}\r\n}\r\nout:\r\nif (hwc->idx == -1)\r\nreturn -EBUSY;\r\nhwc->config_base = uncore->msr_base + (2 * hwc->idx);\r\nhwc->event_base = uncore->msr_base + 1 + (2 * hwc->idx);\r\nhwc->event_base_rdpmc = uncore->rdpmc_base + hwc->idx;\r\nhwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\r\nif (flags & PERF_EF_START)\r\namd_uncore_start(event, PERF_EF_RELOAD);\r\nreturn 0;\r\n}\r\nstatic void amd_uncore_del(struct perf_event *event, int flags)\r\n{\r\nint i;\r\nstruct amd_uncore *uncore = event_to_amd_uncore(event);\r\nstruct hw_perf_event *hwc = &event->hw;\r\namd_uncore_stop(event, PERF_EF_UPDATE);\r\nfor (i = 0; i < uncore->num_counters; i++) {\r\nif (cmpxchg(&uncore->events[i], event, NULL) == event)\r\nbreak;\r\n}\r\nhwc->idx = -1;\r\n}\r\nstatic int amd_uncore_event_init(struct perf_event *event)\r\n{\r\nstruct amd_uncore *uncore;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (event->attr.type != event->pmu->type)\r\nreturn -ENOENT;\r\nif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\r\nreturn -EINVAL;\r\nif (event->attr.exclude_user || event->attr.exclude_kernel ||\r\nevent->attr.exclude_host || event->attr.exclude_guest)\r\nreturn -EINVAL;\r\nhwc->config = event->attr.config & AMD64_RAW_EVENT_MASK_NB;\r\nhwc->idx = -1;\r\nif (event->cpu < 0)\r\nreturn -EINVAL;\r\nuncore = event_to_amd_uncore(event);\r\nif (!uncore)\r\nreturn -ENODEV;\r\nevent->cpu = uncore->cpu;\r\nreturn 0;\r\n}\r\nstatic ssize_t amd_uncore_attr_show_cpumask(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\ncpumask_t *active_mask;\r\nstruct pmu *pmu = dev_get_drvdata(dev);\r\nif (pmu->type == amd_nb_pmu.type)\r\nactive_mask = &amd_nb_active_mask;\r\nelse if (pmu->type == amd_l2_pmu.type)\r\nactive_mask = &amd_l2_active_mask;\r\nelse\r\nreturn 0;\r\nreturn cpumap_print_to_pagebuf(true, buf, active_mask);\r\n}\r\nstatic struct amd_uncore *amd_uncore_alloc(unsigned int cpu)\r\n{\r\nreturn kzalloc_node(sizeof(struct amd_uncore), GFP_KERNEL,\r\ncpu_to_node(cpu));\r\n}\r\nstatic int amd_uncore_cpu_up_prepare(unsigned int cpu)\r\n{\r\nstruct amd_uncore *uncore_nb = NULL, *uncore_l2;\r\nif (amd_uncore_nb) {\r\nuncore_nb = amd_uncore_alloc(cpu);\r\nif (!uncore_nb)\r\ngoto fail;\r\nuncore_nb->cpu = cpu;\r\nuncore_nb->num_counters = NUM_COUNTERS_NB;\r\nuncore_nb->rdpmc_base = RDPMC_BASE_NB;\r\nuncore_nb->msr_base = MSR_F15H_NB_PERF_CTL;\r\nuncore_nb->active_mask = &amd_nb_active_mask;\r\nuncore_nb->pmu = &amd_nb_pmu;\r\n*per_cpu_ptr(amd_uncore_nb, cpu) = uncore_nb;\r\n}\r\nif (amd_uncore_l2) {\r\nuncore_l2 = amd_uncore_alloc(cpu);\r\nif (!uncore_l2)\r\ngoto fail;\r\nuncore_l2->cpu = cpu;\r\nuncore_l2->num_counters = NUM_COUNTERS_L2;\r\nuncore_l2->rdpmc_base = RDPMC_BASE_L2;\r\nuncore_l2->msr_base = MSR_F16H_L2I_PERF_CTL;\r\nuncore_l2->active_mask = &amd_l2_active_mask;\r\nuncore_l2->pmu = &amd_l2_pmu;\r\n*per_cpu_ptr(amd_uncore_l2, cpu) = uncore_l2;\r\n}\r\nreturn 0;\r\nfail:\r\nkfree(uncore_nb);\r\nreturn -ENOMEM;\r\n}\r\nstatic struct amd_uncore *\r\namd_uncore_find_online_sibling(struct amd_uncore *this,\r\nstruct amd_uncore * __percpu *uncores)\r\n{\r\nunsigned int cpu;\r\nstruct amd_uncore *that;\r\nfor_each_online_cpu(cpu) {\r\nthat = *per_cpu_ptr(uncores, cpu);\r\nif (!that)\r\ncontinue;\r\nif (this == that)\r\ncontinue;\r\nif (this->id == that->id) {\r\nthat->free_when_cpu_online = this;\r\nthis = that;\r\nbreak;\r\n}\r\n}\r\nthis->refcnt++;\r\nreturn this;\r\n}\r\nstatic void amd_uncore_cpu_starting(unsigned int cpu)\r\n{\r\nunsigned int eax, ebx, ecx, edx;\r\nstruct amd_uncore *uncore;\r\nif (amd_uncore_nb) {\r\nuncore = *per_cpu_ptr(amd_uncore_nb, cpu);\r\ncpuid(0x8000001e, &eax, &ebx, &ecx, &edx);\r\nuncore->id = ecx & 0xff;\r\nuncore = amd_uncore_find_online_sibling(uncore, amd_uncore_nb);\r\n*per_cpu_ptr(amd_uncore_nb, cpu) = uncore;\r\n}\r\nif (amd_uncore_l2) {\r\nunsigned int apicid = cpu_data(cpu).apicid;\r\nunsigned int nshared;\r\nuncore = *per_cpu_ptr(amd_uncore_l2, cpu);\r\ncpuid_count(0x8000001d, 2, &eax, &ebx, &ecx, &edx);\r\nnshared = ((eax >> 14) & 0xfff) + 1;\r\nuncore->id = apicid - (apicid % nshared);\r\nuncore = amd_uncore_find_online_sibling(uncore, amd_uncore_l2);\r\n*per_cpu_ptr(amd_uncore_l2, cpu) = uncore;\r\n}\r\n}\r\nstatic void uncore_online(unsigned int cpu,\r\nstruct amd_uncore * __percpu *uncores)\r\n{\r\nstruct amd_uncore *uncore = *per_cpu_ptr(uncores, cpu);\r\nkfree(uncore->free_when_cpu_online);\r\nuncore->free_when_cpu_online = NULL;\r\nif (cpu == uncore->cpu)\r\ncpumask_set_cpu(cpu, uncore->active_mask);\r\n}\r\nstatic void amd_uncore_cpu_online(unsigned int cpu)\r\n{\r\nif (amd_uncore_nb)\r\nuncore_online(cpu, amd_uncore_nb);\r\nif (amd_uncore_l2)\r\nuncore_online(cpu, amd_uncore_l2);\r\n}\r\nstatic void uncore_down_prepare(unsigned int cpu,\r\nstruct amd_uncore * __percpu *uncores)\r\n{\r\nunsigned int i;\r\nstruct amd_uncore *this = *per_cpu_ptr(uncores, cpu);\r\nif (this->cpu != cpu)\r\nreturn;\r\nfor_each_online_cpu(i) {\r\nstruct amd_uncore *that = *per_cpu_ptr(uncores, i);\r\nif (cpu == i)\r\ncontinue;\r\nif (this == that) {\r\nperf_pmu_migrate_context(this->pmu, cpu, i);\r\ncpumask_clear_cpu(cpu, that->active_mask);\r\ncpumask_set_cpu(i, that->active_mask);\r\nthat->cpu = i;\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic void amd_uncore_cpu_down_prepare(unsigned int cpu)\r\n{\r\nif (amd_uncore_nb)\r\nuncore_down_prepare(cpu, amd_uncore_nb);\r\nif (amd_uncore_l2)\r\nuncore_down_prepare(cpu, amd_uncore_l2);\r\n}\r\nstatic void uncore_dead(unsigned int cpu, struct amd_uncore * __percpu *uncores)\r\n{\r\nstruct amd_uncore *uncore = *per_cpu_ptr(uncores, cpu);\r\nif (cpu == uncore->cpu)\r\ncpumask_clear_cpu(cpu, uncore->active_mask);\r\nif (!--uncore->refcnt)\r\nkfree(uncore);\r\n*per_cpu_ptr(uncores, cpu) = NULL;\r\n}\r\nstatic void amd_uncore_cpu_dead(unsigned int cpu)\r\n{\r\nif (amd_uncore_nb)\r\nuncore_dead(cpu, amd_uncore_nb);\r\nif (amd_uncore_l2)\r\nuncore_dead(cpu, amd_uncore_l2);\r\n}\r\nstatic int\r\namd_uncore_cpu_notifier(struct notifier_block *self, unsigned long action,\r\nvoid *hcpu)\r\n{\r\nunsigned int cpu = (long)hcpu;\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_UP_PREPARE:\r\nif (amd_uncore_cpu_up_prepare(cpu))\r\nreturn notifier_from_errno(-ENOMEM);\r\nbreak;\r\ncase CPU_STARTING:\r\namd_uncore_cpu_starting(cpu);\r\nbreak;\r\ncase CPU_ONLINE:\r\namd_uncore_cpu_online(cpu);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\namd_uncore_cpu_down_prepare(cpu);\r\nbreak;\r\ncase CPU_UP_CANCELED:\r\ncase CPU_DEAD:\r\namd_uncore_cpu_dead(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void __init init_cpu_already_online(void *dummy)\r\n{\r\nunsigned int cpu = smp_processor_id();\r\namd_uncore_cpu_starting(cpu);\r\namd_uncore_cpu_online(cpu);\r\n}\r\nstatic void cleanup_cpu_online(void *dummy)\r\n{\r\nunsigned int cpu = smp_processor_id();\r\namd_uncore_cpu_dead(cpu);\r\n}\r\nstatic int __init amd_uncore_init(void)\r\n{\r\nunsigned int cpu, cpu2;\r\nint ret = -ENODEV;\r\nif (boot_cpu_data.x86_vendor != X86_VENDOR_AMD)\r\ngoto fail_nodev;\r\nif (!cpu_has_topoext)\r\ngoto fail_nodev;\r\nif (cpu_has_perfctr_nb) {\r\namd_uncore_nb = alloc_percpu(struct amd_uncore *);\r\nif (!amd_uncore_nb) {\r\nret = -ENOMEM;\r\ngoto fail_nb;\r\n}\r\nret = perf_pmu_register(&amd_nb_pmu, amd_nb_pmu.name, -1);\r\nif (ret)\r\ngoto fail_nb;\r\nprintk(KERN_INFO "perf: AMD NB counters detected\n");\r\nret = 0;\r\n}\r\nif (cpu_has_perfctr_l2) {\r\namd_uncore_l2 = alloc_percpu(struct amd_uncore *);\r\nif (!amd_uncore_l2) {\r\nret = -ENOMEM;\r\ngoto fail_l2;\r\n}\r\nret = perf_pmu_register(&amd_l2_pmu, amd_l2_pmu.name, -1);\r\nif (ret)\r\ngoto fail_l2;\r\nprintk(KERN_INFO "perf: AMD L2I counters detected\n");\r\nret = 0;\r\n}\r\nif (ret)\r\ngoto fail_nodev;\r\ncpu_notifier_register_begin();\r\nfor_each_online_cpu(cpu) {\r\nret = amd_uncore_cpu_up_prepare(cpu);\r\nif (ret)\r\ngoto fail_online;\r\nsmp_call_function_single(cpu, init_cpu_already_online, NULL, 1);\r\n}\r\n__register_cpu_notifier(&amd_uncore_cpu_notifier_block);\r\ncpu_notifier_register_done();\r\nreturn 0;\r\nfail_online:\r\nfor_each_online_cpu(cpu2) {\r\nif (cpu2 == cpu)\r\nbreak;\r\nsmp_call_function_single(cpu, cleanup_cpu_online, NULL, 1);\r\n}\r\ncpu_notifier_register_done();\r\namd_uncore_nb = amd_uncore_l2 = NULL;\r\nif (cpu_has_perfctr_l2)\r\nperf_pmu_unregister(&amd_l2_pmu);\r\nfail_l2:\r\nif (cpu_has_perfctr_nb)\r\nperf_pmu_unregister(&amd_nb_pmu);\r\nif (amd_uncore_l2)\r\nfree_percpu(amd_uncore_l2);\r\nfail_nb:\r\nif (amd_uncore_nb)\r\nfree_percpu(amd_uncore_nb);\r\nfail_nodev:\r\nreturn ret;\r\n}
