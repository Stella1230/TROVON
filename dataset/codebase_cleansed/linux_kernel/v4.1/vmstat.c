static void sum_vm_events(unsigned long *ret)\r\n{\r\nint cpu;\r\nint i;\r\nmemset(ret, 0, NR_VM_EVENT_ITEMS * sizeof(unsigned long));\r\nfor_each_online_cpu(cpu) {\r\nstruct vm_event_state *this = &per_cpu(vm_event_states, cpu);\r\nfor (i = 0; i < NR_VM_EVENT_ITEMS; i++)\r\nret[i] += this->event[i];\r\n}\r\n}\r\nvoid all_vm_events(unsigned long *ret)\r\n{\r\nget_online_cpus();\r\nsum_vm_events(ret);\r\nput_online_cpus();\r\n}\r\nvoid vm_events_fold_cpu(int cpu)\r\n{\r\nstruct vm_event_state *fold_state = &per_cpu(vm_event_states, cpu);\r\nint i;\r\nfor (i = 0; i < NR_VM_EVENT_ITEMS; i++) {\r\ncount_vm_events(i, fold_state->event[i]);\r\nfold_state->event[i] = 0;\r\n}\r\n}\r\nint calculate_pressure_threshold(struct zone *zone)\r\n{\r\nint threshold;\r\nint watermark_distance;\r\nwatermark_distance = low_wmark_pages(zone) - min_wmark_pages(zone);\r\nthreshold = max(1, (int)(watermark_distance / num_online_cpus()));\r\nthreshold = min(125, threshold);\r\nreturn threshold;\r\n}\r\nint calculate_normal_threshold(struct zone *zone)\r\n{\r\nint threshold;\r\nint mem;\r\nmem = zone->managed_pages >> (27 - PAGE_SHIFT);\r\nthreshold = 2 * fls(num_online_cpus()) * (1 + fls(mem));\r\nthreshold = min(125, threshold);\r\nreturn threshold;\r\n}\r\nvoid refresh_zone_stat_thresholds(void)\r\n{\r\nstruct zone *zone;\r\nint cpu;\r\nint threshold;\r\nfor_each_populated_zone(zone) {\r\nunsigned long max_drift, tolerate_drift;\r\nthreshold = calculate_normal_threshold(zone);\r\nfor_each_online_cpu(cpu)\r\nper_cpu_ptr(zone->pageset, cpu)->stat_threshold\r\n= threshold;\r\ntolerate_drift = low_wmark_pages(zone) - min_wmark_pages(zone);\r\nmax_drift = num_online_cpus() * threshold;\r\nif (max_drift > tolerate_drift)\r\nzone->percpu_drift_mark = high_wmark_pages(zone) +\r\nmax_drift;\r\n}\r\n}\r\nvoid set_pgdat_percpu_threshold(pg_data_t *pgdat,\r\nint (*calculate_pressure)(struct zone *))\r\n{\r\nstruct zone *zone;\r\nint cpu;\r\nint threshold;\r\nint i;\r\nfor (i = 0; i < pgdat->nr_zones; i++) {\r\nzone = &pgdat->node_zones[i];\r\nif (!zone->percpu_drift_mark)\r\ncontinue;\r\nthreshold = (*calculate_pressure)(zone);\r\nfor_each_online_cpu(cpu)\r\nper_cpu_ptr(zone->pageset, cpu)->stat_threshold\r\n= threshold;\r\n}\r\n}\r\nvoid __mod_zone_page_state(struct zone *zone, enum zone_stat_item item,\r\nint delta)\r\n{\r\nstruct per_cpu_pageset __percpu *pcp = zone->pageset;\r\ns8 __percpu *p = pcp->vm_stat_diff + item;\r\nlong x;\r\nlong t;\r\nx = delta + __this_cpu_read(*p);\r\nt = __this_cpu_read(pcp->stat_threshold);\r\nif (unlikely(x > t || x < -t)) {\r\nzone_page_state_add(x, zone, item);\r\nx = 0;\r\n}\r\n__this_cpu_write(*p, x);\r\n}\r\nvoid __inc_zone_state(struct zone *zone, enum zone_stat_item item)\r\n{\r\nstruct per_cpu_pageset __percpu *pcp = zone->pageset;\r\ns8 __percpu *p = pcp->vm_stat_diff + item;\r\ns8 v, t;\r\nv = __this_cpu_inc_return(*p);\r\nt = __this_cpu_read(pcp->stat_threshold);\r\nif (unlikely(v > t)) {\r\ns8 overstep = t >> 1;\r\nzone_page_state_add(v + overstep, zone, item);\r\n__this_cpu_write(*p, -overstep);\r\n}\r\n}\r\nvoid __inc_zone_page_state(struct page *page, enum zone_stat_item item)\r\n{\r\n__inc_zone_state(page_zone(page), item);\r\n}\r\nvoid __dec_zone_state(struct zone *zone, enum zone_stat_item item)\r\n{\r\nstruct per_cpu_pageset __percpu *pcp = zone->pageset;\r\ns8 __percpu *p = pcp->vm_stat_diff + item;\r\ns8 v, t;\r\nv = __this_cpu_dec_return(*p);\r\nt = __this_cpu_read(pcp->stat_threshold);\r\nif (unlikely(v < - t)) {\r\ns8 overstep = t >> 1;\r\nzone_page_state_add(v - overstep, zone, item);\r\n__this_cpu_write(*p, overstep);\r\n}\r\n}\r\nvoid __dec_zone_page_state(struct page *page, enum zone_stat_item item)\r\n{\r\n__dec_zone_state(page_zone(page), item);\r\n}\r\nstatic inline void mod_state(struct zone *zone,\r\nenum zone_stat_item item, int delta, int overstep_mode)\r\n{\r\nstruct per_cpu_pageset __percpu *pcp = zone->pageset;\r\ns8 __percpu *p = pcp->vm_stat_diff + item;\r\nlong o, n, t, z;\r\ndo {\r\nz = 0;\r\nt = this_cpu_read(pcp->stat_threshold);\r\no = this_cpu_read(*p);\r\nn = delta + o;\r\nif (n > t || n < -t) {\r\nint os = overstep_mode * (t >> 1) ;\r\nz = n + os;\r\nn = -os;\r\n}\r\n} while (this_cpu_cmpxchg(*p, o, n) != o);\r\nif (z)\r\nzone_page_state_add(z, zone, item);\r\n}\r\nvoid mod_zone_page_state(struct zone *zone, enum zone_stat_item item,\r\nint delta)\r\n{\r\nmod_state(zone, item, delta, 0);\r\n}\r\nvoid inc_zone_state(struct zone *zone, enum zone_stat_item item)\r\n{\r\nmod_state(zone, item, 1, 1);\r\n}\r\nvoid inc_zone_page_state(struct page *page, enum zone_stat_item item)\r\n{\r\nmod_state(page_zone(page), item, 1, 1);\r\n}\r\nvoid dec_zone_page_state(struct page *page, enum zone_stat_item item)\r\n{\r\nmod_state(page_zone(page), item, -1, -1);\r\n}\r\nvoid mod_zone_page_state(struct zone *zone, enum zone_stat_item item,\r\nint delta)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\n__mod_zone_page_state(zone, item, delta);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid inc_zone_state(struct zone *zone, enum zone_stat_item item)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\n__inc_zone_state(zone, item);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid inc_zone_page_state(struct page *page, enum zone_stat_item item)\r\n{\r\nunsigned long flags;\r\nstruct zone *zone;\r\nzone = page_zone(page);\r\nlocal_irq_save(flags);\r\n__inc_zone_state(zone, item);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid dec_zone_page_state(struct page *page, enum zone_stat_item item)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\n__dec_zone_page_state(page, item);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic int fold_diff(int *diff)\r\n{\r\nint i;\r\nint changes = 0;\r\nfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)\r\nif (diff[i]) {\r\natomic_long_add(diff[i], &vm_stat[i]);\r\nchanges++;\r\n}\r\nreturn changes;\r\n}\r\nstatic int refresh_cpu_vm_stats(void)\r\n{\r\nstruct zone *zone;\r\nint i;\r\nint global_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };\r\nint changes = 0;\r\nfor_each_populated_zone(zone) {\r\nstruct per_cpu_pageset __percpu *p = zone->pageset;\r\nfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {\r\nint v;\r\nv = this_cpu_xchg(p->vm_stat_diff[i], 0);\r\nif (v) {\r\natomic_long_add(v, &zone->vm_stat[i]);\r\nglobal_diff[i] += v;\r\n#ifdef CONFIG_NUMA\r\n__this_cpu_write(p->expire, 3);\r\n#endif\r\n}\r\n}\r\ncond_resched();\r\n#ifdef CONFIG_NUMA\r\nif (!__this_cpu_read(p->expire) ||\r\n!__this_cpu_read(p->pcp.count))\r\ncontinue;\r\nif (zone_to_nid(zone) == numa_node_id()) {\r\n__this_cpu_write(p->expire, 0);\r\ncontinue;\r\n}\r\nif (__this_cpu_dec_return(p->expire))\r\ncontinue;\r\nif (__this_cpu_read(p->pcp.count)) {\r\ndrain_zone_pages(zone, this_cpu_ptr(&p->pcp));\r\nchanges++;\r\n}\r\n#endif\r\n}\r\nchanges += fold_diff(global_diff);\r\nreturn changes;\r\n}\r\nvoid cpu_vm_stats_fold(int cpu)\r\n{\r\nstruct zone *zone;\r\nint i;\r\nint global_diff[NR_VM_ZONE_STAT_ITEMS] = { 0, };\r\nfor_each_populated_zone(zone) {\r\nstruct per_cpu_pageset *p;\r\np = per_cpu_ptr(zone->pageset, cpu);\r\nfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)\r\nif (p->vm_stat_diff[i]) {\r\nint v;\r\nv = p->vm_stat_diff[i];\r\np->vm_stat_diff[i] = 0;\r\natomic_long_add(v, &zone->vm_stat[i]);\r\nglobal_diff[i] += v;\r\n}\r\n}\r\nfold_diff(global_diff);\r\n}\r\nvoid drain_zonestat(struct zone *zone, struct per_cpu_pageset *pset)\r\n{\r\nint i;\r\nfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)\r\nif (pset->vm_stat_diff[i]) {\r\nint v = pset->vm_stat_diff[i];\r\npset->vm_stat_diff[i] = 0;\r\natomic_long_add(v, &zone->vm_stat[i]);\r\natomic_long_add(v, &vm_stat[i]);\r\n}\r\n}\r\nvoid zone_statistics(struct zone *preferred_zone, struct zone *z, gfp_t flags)\r\n{\r\nif (z->zone_pgdat == preferred_zone->zone_pgdat) {\r\n__inc_zone_state(z, NUMA_HIT);\r\n} else {\r\n__inc_zone_state(z, NUMA_MISS);\r\n__inc_zone_state(preferred_zone, NUMA_FOREIGN);\r\n}\r\nif (z->node == ((flags & __GFP_OTHER_NODE) ?\r\npreferred_zone->node : numa_node_id()))\r\n__inc_zone_state(z, NUMA_LOCAL);\r\nelse\r\n__inc_zone_state(z, NUMA_OTHER);\r\n}\r\nstatic void fill_contig_page_info(struct zone *zone,\r\nunsigned int suitable_order,\r\nstruct contig_page_info *info)\r\n{\r\nunsigned int order;\r\ninfo->free_pages = 0;\r\ninfo->free_blocks_total = 0;\r\ninfo->free_blocks_suitable = 0;\r\nfor (order = 0; order < MAX_ORDER; order++) {\r\nunsigned long blocks;\r\nblocks = zone->free_area[order].nr_free;\r\ninfo->free_blocks_total += blocks;\r\ninfo->free_pages += blocks << order;\r\nif (order >= suitable_order)\r\ninfo->free_blocks_suitable += blocks <<\r\n(order - suitable_order);\r\n}\r\n}\r\nstatic int __fragmentation_index(unsigned int order, struct contig_page_info *info)\r\n{\r\nunsigned long requested = 1UL << order;\r\nif (!info->free_blocks_total)\r\nreturn 0;\r\nif (info->free_blocks_suitable)\r\nreturn -1000;\r\nreturn 1000 - div_u64( (1000+(div_u64(info->free_pages * 1000ULL, requested))), info->free_blocks_total);\r\n}\r\nint fragmentation_index(struct zone *zone, unsigned int order)\r\n{\r\nstruct contig_page_info info;\r\nfill_contig_page_info(zone, order, &info);\r\nreturn __fragmentation_index(order, &info);\r\n}\r\nstatic void *frag_start(struct seq_file *m, loff_t *pos)\r\n{\r\npg_data_t *pgdat;\r\nloff_t node = *pos;\r\nfor (pgdat = first_online_pgdat();\r\npgdat && node;\r\npgdat = next_online_pgdat(pgdat))\r\n--node;\r\nreturn pgdat;\r\n}\r\nstatic void *frag_next(struct seq_file *m, void *arg, loff_t *pos)\r\n{\r\npg_data_t *pgdat = (pg_data_t *)arg;\r\n(*pos)++;\r\nreturn next_online_pgdat(pgdat);\r\n}\r\nstatic void frag_stop(struct seq_file *m, void *arg)\r\n{\r\n}\r\nstatic void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\r\nvoid (*print)(struct seq_file *m, pg_data_t *, struct zone *))\r\n{\r\nstruct zone *zone;\r\nstruct zone *node_zones = pgdat->node_zones;\r\nunsigned long flags;\r\nfor (zone = node_zones; zone - node_zones < MAX_NR_ZONES; ++zone) {\r\nif (!populated_zone(zone))\r\ncontinue;\r\nspin_lock_irqsave(&zone->lock, flags);\r\nprint(m, pgdat, zone);\r\nspin_unlock_irqrestore(&zone->lock, flags);\r\n}\r\n}\r\nstatic void frag_show_print(struct seq_file *m, pg_data_t *pgdat,\r\nstruct zone *zone)\r\n{\r\nint order;\r\nseq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);\r\nfor (order = 0; order < MAX_ORDER; ++order)\r\nseq_printf(m, "%6lu ", zone->free_area[order].nr_free);\r\nseq_putc(m, '\n');\r\n}\r\nstatic int frag_show(struct seq_file *m, void *arg)\r\n{\r\npg_data_t *pgdat = (pg_data_t *)arg;\r\nwalk_zones_in_node(m, pgdat, frag_show_print);\r\nreturn 0;\r\n}\r\nstatic void pagetypeinfo_showfree_print(struct seq_file *m,\r\npg_data_t *pgdat, struct zone *zone)\r\n{\r\nint order, mtype;\r\nfor (mtype = 0; mtype < MIGRATE_TYPES; mtype++) {\r\nseq_printf(m, "Node %4d, zone %8s, type %12s ",\r\npgdat->node_id,\r\nzone->name,\r\nmigratetype_names[mtype]);\r\nfor (order = 0; order < MAX_ORDER; ++order) {\r\nunsigned long freecount = 0;\r\nstruct free_area *area;\r\nstruct list_head *curr;\r\narea = &(zone->free_area[order]);\r\nlist_for_each(curr, &area->free_list[mtype])\r\nfreecount++;\r\nseq_printf(m, "%6lu ", freecount);\r\n}\r\nseq_putc(m, '\n');\r\n}\r\n}\r\nstatic int pagetypeinfo_showfree(struct seq_file *m, void *arg)\r\n{\r\nint order;\r\npg_data_t *pgdat = (pg_data_t *)arg;\r\nseq_printf(m, "%-43s ", "Free pages count per migrate type at order");\r\nfor (order = 0; order < MAX_ORDER; ++order)\r\nseq_printf(m, "%6d ", order);\r\nseq_putc(m, '\n');\r\nwalk_zones_in_node(m, pgdat, pagetypeinfo_showfree_print);\r\nreturn 0;\r\n}\r\nstatic void pagetypeinfo_showblockcount_print(struct seq_file *m,\r\npg_data_t *pgdat, struct zone *zone)\r\n{\r\nint mtype;\r\nunsigned long pfn;\r\nunsigned long start_pfn = zone->zone_start_pfn;\r\nunsigned long end_pfn = zone_end_pfn(zone);\r\nunsigned long count[MIGRATE_TYPES] = { 0, };\r\nfor (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {\r\nstruct page *page;\r\nif (!pfn_valid(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (!memmap_valid_within(pfn, page, zone))\r\ncontinue;\r\nmtype = get_pageblock_migratetype(page);\r\nif (mtype < MIGRATE_TYPES)\r\ncount[mtype]++;\r\n}\r\nseq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);\r\nfor (mtype = 0; mtype < MIGRATE_TYPES; mtype++)\r\nseq_printf(m, "%12lu ", count[mtype]);\r\nseq_putc(m, '\n');\r\n}\r\nstatic int pagetypeinfo_showblockcount(struct seq_file *m, void *arg)\r\n{\r\nint mtype;\r\npg_data_t *pgdat = (pg_data_t *)arg;\r\nseq_printf(m, "\n%-23s", "Number of blocks type ");\r\nfor (mtype = 0; mtype < MIGRATE_TYPES; mtype++)\r\nseq_printf(m, "%12s ", migratetype_names[mtype]);\r\nseq_putc(m, '\n');\r\nwalk_zones_in_node(m, pgdat, pagetypeinfo_showblockcount_print);\r\nreturn 0;\r\n}\r\nstatic void pagetypeinfo_showmixedcount_print(struct seq_file *m,\r\npg_data_t *pgdat,\r\nstruct zone *zone)\r\n{\r\nstruct page *page;\r\nstruct page_ext *page_ext;\r\nunsigned long pfn = zone->zone_start_pfn, block_end_pfn;\r\nunsigned long end_pfn = pfn + zone->spanned_pages;\r\nunsigned long count[MIGRATE_TYPES] = { 0, };\r\nint pageblock_mt, page_mt;\r\nint i;\r\npfn = zone->zone_start_pfn;\r\nfor (; pfn < end_pfn; ) {\r\nif (!pfn_valid(pfn)) {\r\npfn = ALIGN(pfn + 1, MAX_ORDER_NR_PAGES);\r\ncontinue;\r\n}\r\nblock_end_pfn = ALIGN(pfn + 1, pageblock_nr_pages);\r\nblock_end_pfn = min(block_end_pfn, end_pfn);\r\npage = pfn_to_page(pfn);\r\npageblock_mt = get_pfnblock_migratetype(page, pfn);\r\nfor (; pfn < block_end_pfn; pfn++) {\r\nif (!pfn_valid_within(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (PageBuddy(page)) {\r\npfn += (1UL << page_order(page)) - 1;\r\ncontinue;\r\n}\r\nif (PageReserved(page))\r\ncontinue;\r\npage_ext = lookup_page_ext(page);\r\nif (!test_bit(PAGE_EXT_OWNER, &page_ext->flags))\r\ncontinue;\r\npage_mt = gfpflags_to_migratetype(page_ext->gfp_mask);\r\nif (pageblock_mt != page_mt) {\r\nif (is_migrate_cma(pageblock_mt))\r\ncount[MIGRATE_MOVABLE]++;\r\nelse\r\ncount[pageblock_mt]++;\r\npfn = block_end_pfn;\r\nbreak;\r\n}\r\npfn += (1UL << page_ext->order) - 1;\r\n}\r\n}\r\nseq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);\r\nfor (i = 0; i < MIGRATE_TYPES; i++)\r\nseq_printf(m, "%12lu ", count[i]);\r\nseq_putc(m, '\n');\r\n}\r\nstatic void pagetypeinfo_showmixedcount(struct seq_file *m, pg_data_t *pgdat)\r\n{\r\n#ifdef CONFIG_PAGE_OWNER\r\nint mtype;\r\nif (!page_owner_inited)\r\nreturn;\r\ndrain_all_pages(NULL);\r\nseq_printf(m, "\n%-23s", "Number of mixed blocks ");\r\nfor (mtype = 0; mtype < MIGRATE_TYPES; mtype++)\r\nseq_printf(m, "%12s ", migratetype_names[mtype]);\r\nseq_putc(m, '\n');\r\nwalk_zones_in_node(m, pgdat, pagetypeinfo_showmixedcount_print);\r\n#endif\r\n}\r\nstatic int pagetypeinfo_show(struct seq_file *m, void *arg)\r\n{\r\npg_data_t *pgdat = (pg_data_t *)arg;\r\nif (!node_state(pgdat->node_id, N_MEMORY))\r\nreturn 0;\r\nseq_printf(m, "Page block order: %d\n", pageblock_order);\r\nseq_printf(m, "Pages per block: %lu\n", pageblock_nr_pages);\r\nseq_putc(m, '\n');\r\npagetypeinfo_showfree(m, pgdat);\r\npagetypeinfo_showblockcount(m, pgdat);\r\npagetypeinfo_showmixedcount(m, pgdat);\r\nreturn 0;\r\n}\r\nstatic int fragmentation_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &fragmentation_op);\r\n}\r\nstatic int pagetypeinfo_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &pagetypeinfo_op);\r\n}\r\nstatic void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,\r\nstruct zone *zone)\r\n{\r\nint i;\r\nseq_printf(m, "Node %d, zone %8s", pgdat->node_id, zone->name);\r\nseq_printf(m,\r\n"\n pages free %lu"\r\n"\n min %lu"\r\n"\n low %lu"\r\n"\n high %lu"\r\n"\n scanned %lu"\r\n"\n spanned %lu"\r\n"\n present %lu"\r\n"\n managed %lu",\r\nzone_page_state(zone, NR_FREE_PAGES),\r\nmin_wmark_pages(zone),\r\nlow_wmark_pages(zone),\r\nhigh_wmark_pages(zone),\r\nzone_page_state(zone, NR_PAGES_SCANNED),\r\nzone->spanned_pages,\r\nzone->present_pages,\r\nzone->managed_pages);\r\nfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)\r\nseq_printf(m, "\n %-12s %lu", vmstat_text[i],\r\nzone_page_state(zone, i));\r\nseq_printf(m,\r\n"\n protection: (%ld",\r\nzone->lowmem_reserve[0]);\r\nfor (i = 1; i < ARRAY_SIZE(zone->lowmem_reserve); i++)\r\nseq_printf(m, ", %ld", zone->lowmem_reserve[i]);\r\nseq_printf(m,\r\n")"\r\n"\n pagesets");\r\nfor_each_online_cpu(i) {\r\nstruct per_cpu_pageset *pageset;\r\npageset = per_cpu_ptr(zone->pageset, i);\r\nseq_printf(m,\r\n"\n cpu: %i"\r\n"\n count: %i"\r\n"\n high: %i"\r\n"\n batch: %i",\r\ni,\r\npageset->pcp.count,\r\npageset->pcp.high,\r\npageset->pcp.batch);\r\n#ifdef CONFIG_SMP\r\nseq_printf(m, "\n vm stats threshold: %d",\r\npageset->stat_threshold);\r\n#endif\r\n}\r\nseq_printf(m,\r\n"\n all_unreclaimable: %u"\r\n"\n start_pfn: %lu"\r\n"\n inactive_ratio: %u",\r\n!zone_reclaimable(zone),\r\nzone->zone_start_pfn,\r\nzone->inactive_ratio);\r\nseq_putc(m, '\n');\r\n}\r\nstatic int zoneinfo_show(struct seq_file *m, void *arg)\r\n{\r\npg_data_t *pgdat = (pg_data_t *)arg;\r\nwalk_zones_in_node(m, pgdat, zoneinfo_show_print);\r\nreturn 0;\r\n}\r\nstatic int zoneinfo_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &zoneinfo_op);\r\n}\r\nstatic void *vmstat_start(struct seq_file *m, loff_t *pos)\r\n{\r\nunsigned long *v;\r\nint i, stat_items_size;\r\nif (*pos >= ARRAY_SIZE(vmstat_text))\r\nreturn NULL;\r\nstat_items_size = NR_VM_ZONE_STAT_ITEMS * sizeof(unsigned long) +\r\nNR_VM_WRITEBACK_STAT_ITEMS * sizeof(unsigned long);\r\n#ifdef CONFIG_VM_EVENT_COUNTERS\r\nstat_items_size += sizeof(struct vm_event_state);\r\n#endif\r\nv = kmalloc(stat_items_size, GFP_KERNEL);\r\nm->private = v;\r\nif (!v)\r\nreturn ERR_PTR(-ENOMEM);\r\nfor (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)\r\nv[i] = global_page_state(i);\r\nv += NR_VM_ZONE_STAT_ITEMS;\r\nglobal_dirty_limits(v + NR_DIRTY_BG_THRESHOLD,\r\nv + NR_DIRTY_THRESHOLD);\r\nv += NR_VM_WRITEBACK_STAT_ITEMS;\r\n#ifdef CONFIG_VM_EVENT_COUNTERS\r\nall_vm_events(v);\r\nv[PGPGIN] /= 2;\r\nv[PGPGOUT] /= 2;\r\n#endif\r\nreturn (unsigned long *)m->private + *pos;\r\n}\r\nstatic void *vmstat_next(struct seq_file *m, void *arg, loff_t *pos)\r\n{\r\n(*pos)++;\r\nif (*pos >= ARRAY_SIZE(vmstat_text))\r\nreturn NULL;\r\nreturn (unsigned long *)m->private + *pos;\r\n}\r\nstatic int vmstat_show(struct seq_file *m, void *arg)\r\n{\r\nunsigned long *l = arg;\r\nunsigned long off = l - (unsigned long *)m->private;\r\nseq_printf(m, "%s %lu\n", vmstat_text[off], *l);\r\nreturn 0;\r\n}\r\nstatic void vmstat_stop(struct seq_file *m, void *arg)\r\n{\r\nkfree(m->private);\r\nm->private = NULL;\r\n}\r\nstatic int vmstat_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &vmstat_op);\r\n}\r\nstatic void vmstat_update(struct work_struct *w)\r\n{\r\nif (refresh_cpu_vm_stats())\r\nschedule_delayed_work(this_cpu_ptr(&vmstat_work),\r\nround_jiffies_relative(sysctl_stat_interval));\r\nelse {\r\nint r;\r\nr = cpumask_test_and_set_cpu(smp_processor_id(),\r\ncpu_stat_off);\r\nVM_BUG_ON(r);\r\n}\r\n}\r\nstatic bool need_update(int cpu)\r\n{\r\nstruct zone *zone;\r\nfor_each_populated_zone(zone) {\r\nstruct per_cpu_pageset *p = per_cpu_ptr(zone->pageset, cpu);\r\nBUILD_BUG_ON(sizeof(p->vm_stat_diff[0]) != 1);\r\nif (memchr_inv(p->vm_stat_diff, 0, NR_VM_ZONE_STAT_ITEMS))\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void vmstat_shepherd(struct work_struct *w)\r\n{\r\nint cpu;\r\nget_online_cpus();\r\nfor_each_cpu(cpu, cpu_stat_off)\r\nif (need_update(cpu) &&\r\ncpumask_test_and_clear_cpu(cpu, cpu_stat_off))\r\nschedule_delayed_work_on(cpu,\r\n&per_cpu(vmstat_work, cpu), 0);\r\nput_online_cpus();\r\nschedule_delayed_work(&shepherd,\r\nround_jiffies_relative(sysctl_stat_interval));\r\n}\r\nstatic void __init start_shepherd_timer(void)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu)\r\nINIT_DELAYED_WORK(per_cpu_ptr(&vmstat_work, cpu),\r\nvmstat_update);\r\nif (!alloc_cpumask_var(&cpu_stat_off, GFP_KERNEL))\r\nBUG();\r\ncpumask_copy(cpu_stat_off, cpu_online_mask);\r\nschedule_delayed_work(&shepherd,\r\nround_jiffies_relative(sysctl_stat_interval));\r\n}\r\nstatic void vmstat_cpu_dead(int node)\r\n{\r\nint cpu;\r\nget_online_cpus();\r\nfor_each_online_cpu(cpu)\r\nif (cpu_to_node(cpu) == node)\r\ngoto end;\r\nnode_clear_state(node, N_CPU);\r\nend:\r\nput_online_cpus();\r\n}\r\nstatic int vmstat_cpuup_callback(struct notifier_block *nfb,\r\nunsigned long action,\r\nvoid *hcpu)\r\n{\r\nlong cpu = (long)hcpu;\r\nswitch (action) {\r\ncase CPU_ONLINE:\r\ncase CPU_ONLINE_FROZEN:\r\nrefresh_zone_stat_thresholds();\r\nnode_set_state(cpu_to_node(cpu), N_CPU);\r\ncpumask_set_cpu(cpu, cpu_stat_off);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\ncase CPU_DOWN_PREPARE_FROZEN:\r\ncancel_delayed_work_sync(&per_cpu(vmstat_work, cpu));\r\ncpumask_clear_cpu(cpu, cpu_stat_off);\r\nbreak;\r\ncase CPU_DOWN_FAILED:\r\ncase CPU_DOWN_FAILED_FROZEN:\r\ncpumask_set_cpu(cpu, cpu_stat_off);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\nrefresh_zone_stat_thresholds();\r\nvmstat_cpu_dead(cpu_to_node(cpu));\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic int __init setup_vmstat(void)\r\n{\r\n#ifdef CONFIG_SMP\r\ncpu_notifier_register_begin();\r\n__register_cpu_notifier(&vmstat_notifier);\r\nstart_shepherd_timer();\r\ncpu_notifier_register_done();\r\n#endif\r\n#ifdef CONFIG_PROC_FS\r\nproc_create("buddyinfo", S_IRUGO, NULL, &fragmentation_file_operations);\r\nproc_create("pagetypeinfo", S_IRUGO, NULL, &pagetypeinfo_file_ops);\r\nproc_create("vmstat", S_IRUGO, NULL, &proc_vmstat_file_operations);\r\nproc_create("zoneinfo", S_IRUGO, NULL, &proc_zoneinfo_file_operations);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int unusable_free_index(unsigned int order,\r\nstruct contig_page_info *info)\r\n{\r\nif (info->free_pages == 0)\r\nreturn 1000;\r\nreturn div_u64((info->free_pages - (info->free_blocks_suitable << order)) * 1000ULL, info->free_pages);\r\n}\r\nstatic void unusable_show_print(struct seq_file *m,\r\npg_data_t *pgdat, struct zone *zone)\r\n{\r\nunsigned int order;\r\nint index;\r\nstruct contig_page_info info;\r\nseq_printf(m, "Node %d, zone %8s ",\r\npgdat->node_id,\r\nzone->name);\r\nfor (order = 0; order < MAX_ORDER; ++order) {\r\nfill_contig_page_info(zone, order, &info);\r\nindex = unusable_free_index(order, &info);\r\nseq_printf(m, "%d.%03d ", index / 1000, index % 1000);\r\n}\r\nseq_putc(m, '\n');\r\n}\r\nstatic int unusable_show(struct seq_file *m, void *arg)\r\n{\r\npg_data_t *pgdat = (pg_data_t *)arg;\r\nif (!node_state(pgdat->node_id, N_MEMORY))\r\nreturn 0;\r\nwalk_zones_in_node(m, pgdat, unusable_show_print);\r\nreturn 0;\r\n}\r\nstatic int unusable_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &unusable_op);\r\n}\r\nstatic void extfrag_show_print(struct seq_file *m,\r\npg_data_t *pgdat, struct zone *zone)\r\n{\r\nunsigned int order;\r\nint index;\r\nstruct contig_page_info info;\r\nseq_printf(m, "Node %d, zone %8s ",\r\npgdat->node_id,\r\nzone->name);\r\nfor (order = 0; order < MAX_ORDER; ++order) {\r\nfill_contig_page_info(zone, order, &info);\r\nindex = __fragmentation_index(order, &info);\r\nseq_printf(m, "%d.%03d ", index / 1000, index % 1000);\r\n}\r\nseq_putc(m, '\n');\r\n}\r\nstatic int extfrag_show(struct seq_file *m, void *arg)\r\n{\r\npg_data_t *pgdat = (pg_data_t *)arg;\r\nwalk_zones_in_node(m, pgdat, extfrag_show_print);\r\nreturn 0;\r\n}\r\nstatic int extfrag_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &extfrag_op);\r\n}\r\nstatic int __init extfrag_debug_init(void)\r\n{\r\nstruct dentry *extfrag_debug_root;\r\nextfrag_debug_root = debugfs_create_dir("extfrag", NULL);\r\nif (!extfrag_debug_root)\r\nreturn -ENOMEM;\r\nif (!debugfs_create_file("unusable_index", 0444,\r\nextfrag_debug_root, NULL, &unusable_file_ops))\r\ngoto fail;\r\nif (!debugfs_create_file("extfrag_index", 0444,\r\nextfrag_debug_root, NULL, &extfrag_file_ops))\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\ndebugfs_remove_recursive(extfrag_debug_root);\r\nreturn -ENOMEM;\r\n}
