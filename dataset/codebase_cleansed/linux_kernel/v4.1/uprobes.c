static bool valid_vma(struct vm_area_struct *vma, bool is_register)\r\n{\r\nvm_flags_t flags = VM_HUGETLB | VM_MAYEXEC | VM_MAYSHARE;\r\nif (is_register)\r\nflags |= VM_WRITE;\r\nreturn vma->vm_file && (vma->vm_flags & flags) == VM_MAYEXEC;\r\n}\r\nstatic unsigned long offset_to_vaddr(struct vm_area_struct *vma, loff_t offset)\r\n{\r\nreturn vma->vm_start + offset - ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\r\n}\r\nstatic loff_t vaddr_to_offset(struct vm_area_struct *vma, unsigned long vaddr)\r\n{\r\nreturn ((loff_t)vma->vm_pgoff << PAGE_SHIFT) + (vaddr - vma->vm_start);\r\n}\r\nstatic int __replace_page(struct vm_area_struct *vma, unsigned long addr,\r\nstruct page *page, struct page *kpage)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nspinlock_t *ptl;\r\npte_t *ptep;\r\nint err;\r\nconst unsigned long mmun_start = addr;\r\nconst unsigned long mmun_end = addr + PAGE_SIZE;\r\nstruct mem_cgroup *memcg;\r\nerr = mem_cgroup_try_charge(kpage, vma->vm_mm, GFP_KERNEL, &memcg);\r\nif (err)\r\nreturn err;\r\nlock_page(page);\r\nmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\r\nerr = -EAGAIN;\r\nptep = page_check_address(page, mm, addr, &ptl, 0);\r\nif (!ptep)\r\ngoto unlock;\r\nget_page(kpage);\r\npage_add_new_anon_rmap(kpage, vma, addr);\r\nmem_cgroup_commit_charge(kpage, memcg, false);\r\nlru_cache_add_active_or_unevictable(kpage, vma);\r\nif (!PageAnon(page)) {\r\ndec_mm_counter(mm, MM_FILEPAGES);\r\ninc_mm_counter(mm, MM_ANONPAGES);\r\n}\r\nflush_cache_page(vma, addr, pte_pfn(*ptep));\r\nptep_clear_flush_notify(vma, addr, ptep);\r\nset_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma->vm_page_prot));\r\npage_remove_rmap(page);\r\nif (!page_mapped(page))\r\ntry_to_free_swap(page);\r\npte_unmap_unlock(ptep, ptl);\r\nif (vma->vm_flags & VM_LOCKED)\r\nmunlock_vma_page(page);\r\nput_page(page);\r\nerr = 0;\r\nunlock:\r\nmem_cgroup_cancel_charge(kpage, memcg);\r\nmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\r\nunlock_page(page);\r\nreturn err;\r\n}\r\nbool __weak is_swbp_insn(uprobe_opcode_t *insn)\r\n{\r\nreturn *insn == UPROBE_SWBP_INSN;\r\n}\r\nbool __weak is_trap_insn(uprobe_opcode_t *insn)\r\n{\r\nreturn is_swbp_insn(insn);\r\n}\r\nstatic void copy_from_page(struct page *page, unsigned long vaddr, void *dst, int len)\r\n{\r\nvoid *kaddr = kmap_atomic(page);\r\nmemcpy(dst, kaddr + (vaddr & ~PAGE_MASK), len);\r\nkunmap_atomic(kaddr);\r\n}\r\nstatic void copy_to_page(struct page *page, unsigned long vaddr, const void *src, int len)\r\n{\r\nvoid *kaddr = kmap_atomic(page);\r\nmemcpy(kaddr + (vaddr & ~PAGE_MASK), src, len);\r\nkunmap_atomic(kaddr);\r\n}\r\nstatic int verify_opcode(struct page *page, unsigned long vaddr, uprobe_opcode_t *new_opcode)\r\n{\r\nuprobe_opcode_t old_opcode;\r\nbool is_swbp;\r\ncopy_from_page(page, vaddr, &old_opcode, UPROBE_SWBP_INSN_SIZE);\r\nis_swbp = is_swbp_insn(&old_opcode);\r\nif (is_swbp_insn(new_opcode)) {\r\nif (is_swbp)\r\nreturn 0;\r\n} else {\r\nif (!is_swbp)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nint uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,\r\nuprobe_opcode_t opcode)\r\n{\r\nstruct page *old_page, *new_page;\r\nstruct vm_area_struct *vma;\r\nint ret;\r\nretry:\r\nret = get_user_pages(NULL, mm, vaddr, 1, 0, 1, &old_page, &vma);\r\nif (ret <= 0)\r\nreturn ret;\r\nret = verify_opcode(old_page, vaddr, &opcode);\r\nif (ret <= 0)\r\ngoto put_old;\r\nret = anon_vma_prepare(vma);\r\nif (ret)\r\ngoto put_old;\r\nret = -ENOMEM;\r\nnew_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vaddr);\r\nif (!new_page)\r\ngoto put_old;\r\n__SetPageUptodate(new_page);\r\ncopy_highpage(new_page, old_page);\r\ncopy_to_page(new_page, vaddr, &opcode, UPROBE_SWBP_INSN_SIZE);\r\nret = __replace_page(vma, vaddr, old_page, new_page);\r\npage_cache_release(new_page);\r\nput_old:\r\nput_page(old_page);\r\nif (unlikely(ret == -EAGAIN))\r\ngoto retry;\r\nreturn ret;\r\n}\r\nint __weak set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)\r\n{\r\nreturn uprobe_write_opcode(mm, vaddr, UPROBE_SWBP_INSN);\r\n}\r\nint __weak\r\nset_orig_insn(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)\r\n{\r\nreturn uprobe_write_opcode(mm, vaddr, *(uprobe_opcode_t *)&auprobe->insn);\r\n}\r\nstatic int match_uprobe(struct uprobe *l, struct uprobe *r)\r\n{\r\nif (l->inode < r->inode)\r\nreturn -1;\r\nif (l->inode > r->inode)\r\nreturn 1;\r\nif (l->offset < r->offset)\r\nreturn -1;\r\nif (l->offset > r->offset)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic struct uprobe *__find_uprobe(struct inode *inode, loff_t offset)\r\n{\r\nstruct uprobe u = { .inode = inode, .offset = offset };\r\nstruct rb_node *n = uprobes_tree.rb_node;\r\nstruct uprobe *uprobe;\r\nint match;\r\nwhile (n) {\r\nuprobe = rb_entry(n, struct uprobe, rb_node);\r\nmatch = match_uprobe(&u, uprobe);\r\nif (!match) {\r\natomic_inc(&uprobe->ref);\r\nreturn uprobe;\r\n}\r\nif (match < 0)\r\nn = n->rb_left;\r\nelse\r\nn = n->rb_right;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct uprobe *find_uprobe(struct inode *inode, loff_t offset)\r\n{\r\nstruct uprobe *uprobe;\r\nspin_lock(&uprobes_treelock);\r\nuprobe = __find_uprobe(inode, offset);\r\nspin_unlock(&uprobes_treelock);\r\nreturn uprobe;\r\n}\r\nstatic struct uprobe *__insert_uprobe(struct uprobe *uprobe)\r\n{\r\nstruct rb_node **p = &uprobes_tree.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct uprobe *u;\r\nint match;\r\nwhile (*p) {\r\nparent = *p;\r\nu = rb_entry(parent, struct uprobe, rb_node);\r\nmatch = match_uprobe(uprobe, u);\r\nif (!match) {\r\natomic_inc(&u->ref);\r\nreturn u;\r\n}\r\nif (match < 0)\r\np = &parent->rb_left;\r\nelse\r\np = &parent->rb_right;\r\n}\r\nu = NULL;\r\nrb_link_node(&uprobe->rb_node, parent, p);\r\nrb_insert_color(&uprobe->rb_node, &uprobes_tree);\r\natomic_set(&uprobe->ref, 2);\r\nreturn u;\r\n}\r\nstatic struct uprobe *insert_uprobe(struct uprobe *uprobe)\r\n{\r\nstruct uprobe *u;\r\nspin_lock(&uprobes_treelock);\r\nu = __insert_uprobe(uprobe);\r\nspin_unlock(&uprobes_treelock);\r\nreturn u;\r\n}\r\nstatic void put_uprobe(struct uprobe *uprobe)\r\n{\r\nif (atomic_dec_and_test(&uprobe->ref))\r\nkfree(uprobe);\r\n}\r\nstatic struct uprobe *alloc_uprobe(struct inode *inode, loff_t offset)\r\n{\r\nstruct uprobe *uprobe, *cur_uprobe;\r\nuprobe = kzalloc(sizeof(struct uprobe), GFP_KERNEL);\r\nif (!uprobe)\r\nreturn NULL;\r\nuprobe->inode = igrab(inode);\r\nuprobe->offset = offset;\r\ninit_rwsem(&uprobe->register_rwsem);\r\ninit_rwsem(&uprobe->consumer_rwsem);\r\ncur_uprobe = insert_uprobe(uprobe);\r\nif (cur_uprobe) {\r\nkfree(uprobe);\r\nuprobe = cur_uprobe;\r\niput(inode);\r\n}\r\nreturn uprobe;\r\n}\r\nstatic void consumer_add(struct uprobe *uprobe, struct uprobe_consumer *uc)\r\n{\r\ndown_write(&uprobe->consumer_rwsem);\r\nuc->next = uprobe->consumers;\r\nuprobe->consumers = uc;\r\nup_write(&uprobe->consumer_rwsem);\r\n}\r\nstatic bool consumer_del(struct uprobe *uprobe, struct uprobe_consumer *uc)\r\n{\r\nstruct uprobe_consumer **con;\r\nbool ret = false;\r\ndown_write(&uprobe->consumer_rwsem);\r\nfor (con = &uprobe->consumers; *con; con = &(*con)->next) {\r\nif (*con == uc) {\r\n*con = uc->next;\r\nret = true;\r\nbreak;\r\n}\r\n}\r\nup_write(&uprobe->consumer_rwsem);\r\nreturn ret;\r\n}\r\nstatic int __copy_insn(struct address_space *mapping, struct file *filp,\r\nvoid *insn, int nbytes, loff_t offset)\r\n{\r\nstruct page *page;\r\nif (mapping->a_ops->readpage)\r\npage = read_mapping_page(mapping, offset >> PAGE_CACHE_SHIFT, filp);\r\nelse\r\npage = shmem_read_mapping_page(mapping, offset >> PAGE_CACHE_SHIFT);\r\nif (IS_ERR(page))\r\nreturn PTR_ERR(page);\r\ncopy_from_page(page, offset, insn, nbytes);\r\npage_cache_release(page);\r\nreturn 0;\r\n}\r\nstatic int copy_insn(struct uprobe *uprobe, struct file *filp)\r\n{\r\nstruct address_space *mapping = uprobe->inode->i_mapping;\r\nloff_t offs = uprobe->offset;\r\nvoid *insn = &uprobe->arch.insn;\r\nint size = sizeof(uprobe->arch.insn);\r\nint len, err = -EIO;\r\ndo {\r\nif (offs >= i_size_read(uprobe->inode))\r\nbreak;\r\nlen = min_t(int, size, PAGE_SIZE - (offs & ~PAGE_MASK));\r\nerr = __copy_insn(mapping, filp, insn, len, offs);\r\nif (err)\r\nbreak;\r\ninsn += len;\r\noffs += len;\r\nsize -= len;\r\n} while (size);\r\nreturn err;\r\n}\r\nstatic int prepare_uprobe(struct uprobe *uprobe, struct file *file,\r\nstruct mm_struct *mm, unsigned long vaddr)\r\n{\r\nint ret = 0;\r\nif (test_bit(UPROBE_COPY_INSN, &uprobe->flags))\r\nreturn ret;\r\ndown_write(&uprobe->consumer_rwsem);\r\nif (test_bit(UPROBE_COPY_INSN, &uprobe->flags))\r\ngoto out;\r\nret = copy_insn(uprobe, file);\r\nif (ret)\r\ngoto out;\r\nret = -ENOTSUPP;\r\nif (is_trap_insn((uprobe_opcode_t *)&uprobe->arch.insn))\r\ngoto out;\r\nret = arch_uprobe_analyze_insn(&uprobe->arch, mm, vaddr);\r\nif (ret)\r\ngoto out;\r\nBUG_ON((uprobe->offset & ~PAGE_MASK) +\r\nUPROBE_SWBP_INSN_SIZE > PAGE_SIZE);\r\nsmp_wmb();\r\nset_bit(UPROBE_COPY_INSN, &uprobe->flags);\r\nout:\r\nup_write(&uprobe->consumer_rwsem);\r\nreturn ret;\r\n}\r\nstatic inline bool consumer_filter(struct uprobe_consumer *uc,\r\nenum uprobe_filter_ctx ctx, struct mm_struct *mm)\r\n{\r\nreturn !uc->filter || uc->filter(uc, ctx, mm);\r\n}\r\nstatic bool filter_chain(struct uprobe *uprobe,\r\nenum uprobe_filter_ctx ctx, struct mm_struct *mm)\r\n{\r\nstruct uprobe_consumer *uc;\r\nbool ret = false;\r\ndown_read(&uprobe->consumer_rwsem);\r\nfor (uc = uprobe->consumers; uc; uc = uc->next) {\r\nret = consumer_filter(uc, ctx, mm);\r\nif (ret)\r\nbreak;\r\n}\r\nup_read(&uprobe->consumer_rwsem);\r\nreturn ret;\r\n}\r\nstatic int\r\ninstall_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,\r\nstruct vm_area_struct *vma, unsigned long vaddr)\r\n{\r\nbool first_uprobe;\r\nint ret;\r\nret = prepare_uprobe(uprobe, vma->vm_file, mm, vaddr);\r\nif (ret)\r\nreturn ret;\r\nfirst_uprobe = !test_bit(MMF_HAS_UPROBES, &mm->flags);\r\nif (first_uprobe)\r\nset_bit(MMF_HAS_UPROBES, &mm->flags);\r\nret = set_swbp(&uprobe->arch, mm, vaddr);\r\nif (!ret)\r\nclear_bit(MMF_RECALC_UPROBES, &mm->flags);\r\nelse if (first_uprobe)\r\nclear_bit(MMF_HAS_UPROBES, &mm->flags);\r\nreturn ret;\r\n}\r\nstatic int\r\nremove_breakpoint(struct uprobe *uprobe, struct mm_struct *mm, unsigned long vaddr)\r\n{\r\nset_bit(MMF_RECALC_UPROBES, &mm->flags);\r\nreturn set_orig_insn(&uprobe->arch, mm, vaddr);\r\n}\r\nstatic inline bool uprobe_is_active(struct uprobe *uprobe)\r\n{\r\nreturn !RB_EMPTY_NODE(&uprobe->rb_node);\r\n}\r\nstatic void delete_uprobe(struct uprobe *uprobe)\r\n{\r\nif (WARN_ON(!uprobe_is_active(uprobe)))\r\nreturn;\r\nspin_lock(&uprobes_treelock);\r\nrb_erase(&uprobe->rb_node, &uprobes_tree);\r\nspin_unlock(&uprobes_treelock);\r\nRB_CLEAR_NODE(&uprobe->rb_node);\r\niput(uprobe->inode);\r\nput_uprobe(uprobe);\r\n}\r\nstatic inline struct map_info *free_map_info(struct map_info *info)\r\n{\r\nstruct map_info *next = info->next;\r\nkfree(info);\r\nreturn next;\r\n}\r\nstatic struct map_info *\r\nbuild_map_info(struct address_space *mapping, loff_t offset, bool is_register)\r\n{\r\nunsigned long pgoff = offset >> PAGE_SHIFT;\r\nstruct vm_area_struct *vma;\r\nstruct map_info *curr = NULL;\r\nstruct map_info *prev = NULL;\r\nstruct map_info *info;\r\nint more = 0;\r\nagain:\r\ni_mmap_lock_read(mapping);\r\nvma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {\r\nif (!valid_vma(vma, is_register))\r\ncontinue;\r\nif (!prev && !more) {\r\nprev = kmalloc(sizeof(struct map_info),\r\nGFP_NOWAIT | __GFP_NOMEMALLOC | __GFP_NOWARN);\r\nif (prev)\r\nprev->next = NULL;\r\n}\r\nif (!prev) {\r\nmore++;\r\ncontinue;\r\n}\r\nif (!atomic_inc_not_zero(&vma->vm_mm->mm_users))\r\ncontinue;\r\ninfo = prev;\r\nprev = prev->next;\r\ninfo->next = curr;\r\ncurr = info;\r\ninfo->mm = vma->vm_mm;\r\ninfo->vaddr = offset_to_vaddr(vma, offset);\r\n}\r\ni_mmap_unlock_read(mapping);\r\nif (!more)\r\ngoto out;\r\nprev = curr;\r\nwhile (curr) {\r\nmmput(curr->mm);\r\ncurr = curr->next;\r\n}\r\ndo {\r\ninfo = kmalloc(sizeof(struct map_info), GFP_KERNEL);\r\nif (!info) {\r\ncurr = ERR_PTR(-ENOMEM);\r\ngoto out;\r\n}\r\ninfo->next = prev;\r\nprev = info;\r\n} while (--more);\r\ngoto again;\r\nout:\r\nwhile (prev)\r\nprev = free_map_info(prev);\r\nreturn curr;\r\n}\r\nstatic int\r\nregister_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)\r\n{\r\nbool is_register = !!new;\r\nstruct map_info *info;\r\nint err = 0;\r\npercpu_down_write(&dup_mmap_sem);\r\ninfo = build_map_info(uprobe->inode->i_mapping,\r\nuprobe->offset, is_register);\r\nif (IS_ERR(info)) {\r\nerr = PTR_ERR(info);\r\ngoto out;\r\n}\r\nwhile (info) {\r\nstruct mm_struct *mm = info->mm;\r\nstruct vm_area_struct *vma;\r\nif (err && is_register)\r\ngoto free;\r\ndown_write(&mm->mmap_sem);\r\nvma = find_vma(mm, info->vaddr);\r\nif (!vma || !valid_vma(vma, is_register) ||\r\nfile_inode(vma->vm_file) != uprobe->inode)\r\ngoto unlock;\r\nif (vma->vm_start > info->vaddr ||\r\nvaddr_to_offset(vma, info->vaddr) != uprobe->offset)\r\ngoto unlock;\r\nif (is_register) {\r\nif (consumer_filter(new,\r\nUPROBE_FILTER_REGISTER, mm))\r\nerr = install_breakpoint(uprobe, mm, vma, info->vaddr);\r\n} else if (test_bit(MMF_HAS_UPROBES, &mm->flags)) {\r\nif (!filter_chain(uprobe,\r\nUPROBE_FILTER_UNREGISTER, mm))\r\nerr |= remove_breakpoint(uprobe, mm, info->vaddr);\r\n}\r\nunlock:\r\nup_write(&mm->mmap_sem);\r\nfree:\r\nmmput(mm);\r\ninfo = free_map_info(info);\r\n}\r\nout:\r\npercpu_up_write(&dup_mmap_sem);\r\nreturn err;\r\n}\r\nstatic int __uprobe_register(struct uprobe *uprobe, struct uprobe_consumer *uc)\r\n{\r\nconsumer_add(uprobe, uc);\r\nreturn register_for_each_vma(uprobe, uc);\r\n}\r\nstatic void __uprobe_unregister(struct uprobe *uprobe, struct uprobe_consumer *uc)\r\n{\r\nint err;\r\nif (WARN_ON(!consumer_del(uprobe, uc)))\r\nreturn;\r\nerr = register_for_each_vma(uprobe, NULL);\r\nif (!uprobe->consumers && !err)\r\ndelete_uprobe(uprobe);\r\n}\r\nint uprobe_register(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)\r\n{\r\nstruct uprobe *uprobe;\r\nint ret;\r\nif (!uc->handler && !uc->ret_handler)\r\nreturn -EINVAL;\r\nif (!inode->i_mapping->a_ops->readpage && !shmem_mapping(inode->i_mapping))\r\nreturn -EIO;\r\nif (offset > i_size_read(inode))\r\nreturn -EINVAL;\r\nretry:\r\nuprobe = alloc_uprobe(inode, offset);\r\nif (!uprobe)\r\nreturn -ENOMEM;\r\ndown_write(&uprobe->register_rwsem);\r\nret = -EAGAIN;\r\nif (likely(uprobe_is_active(uprobe))) {\r\nret = __uprobe_register(uprobe, uc);\r\nif (ret)\r\n__uprobe_unregister(uprobe, uc);\r\n}\r\nup_write(&uprobe->register_rwsem);\r\nput_uprobe(uprobe);\r\nif (unlikely(ret == -EAGAIN))\r\ngoto retry;\r\nreturn ret;\r\n}\r\nint uprobe_apply(struct inode *inode, loff_t offset,\r\nstruct uprobe_consumer *uc, bool add)\r\n{\r\nstruct uprobe *uprobe;\r\nstruct uprobe_consumer *con;\r\nint ret = -ENOENT;\r\nuprobe = find_uprobe(inode, offset);\r\nif (WARN_ON(!uprobe))\r\nreturn ret;\r\ndown_write(&uprobe->register_rwsem);\r\nfor (con = uprobe->consumers; con && con != uc ; con = con->next)\r\n;\r\nif (con)\r\nret = register_for_each_vma(uprobe, add ? uc : NULL);\r\nup_write(&uprobe->register_rwsem);\r\nput_uprobe(uprobe);\r\nreturn ret;\r\n}\r\nvoid uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)\r\n{\r\nstruct uprobe *uprobe;\r\nuprobe = find_uprobe(inode, offset);\r\nif (WARN_ON(!uprobe))\r\nreturn;\r\ndown_write(&uprobe->register_rwsem);\r\n__uprobe_unregister(uprobe, uc);\r\nup_write(&uprobe->register_rwsem);\r\nput_uprobe(uprobe);\r\n}\r\nstatic int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)\r\n{\r\nstruct vm_area_struct *vma;\r\nint err = 0;\r\ndown_read(&mm->mmap_sem);\r\nfor (vma = mm->mmap; vma; vma = vma->vm_next) {\r\nunsigned long vaddr;\r\nloff_t offset;\r\nif (!valid_vma(vma, false) ||\r\nfile_inode(vma->vm_file) != uprobe->inode)\r\ncontinue;\r\noffset = (loff_t)vma->vm_pgoff << PAGE_SHIFT;\r\nif (uprobe->offset < offset ||\r\nuprobe->offset >= offset + vma->vm_end - vma->vm_start)\r\ncontinue;\r\nvaddr = offset_to_vaddr(vma, uprobe->offset);\r\nerr |= remove_breakpoint(uprobe, mm, vaddr);\r\n}\r\nup_read(&mm->mmap_sem);\r\nreturn err;\r\n}\r\nstatic struct rb_node *\r\nfind_node_in_range(struct inode *inode, loff_t min, loff_t max)\r\n{\r\nstruct rb_node *n = uprobes_tree.rb_node;\r\nwhile (n) {\r\nstruct uprobe *u = rb_entry(n, struct uprobe, rb_node);\r\nif (inode < u->inode) {\r\nn = n->rb_left;\r\n} else if (inode > u->inode) {\r\nn = n->rb_right;\r\n} else {\r\nif (max < u->offset)\r\nn = n->rb_left;\r\nelse if (min > u->offset)\r\nn = n->rb_right;\r\nelse\r\nbreak;\r\n}\r\n}\r\nreturn n;\r\n}\r\nstatic void build_probe_list(struct inode *inode,\r\nstruct vm_area_struct *vma,\r\nunsigned long start, unsigned long end,\r\nstruct list_head *head)\r\n{\r\nloff_t min, max;\r\nstruct rb_node *n, *t;\r\nstruct uprobe *u;\r\nINIT_LIST_HEAD(head);\r\nmin = vaddr_to_offset(vma, start);\r\nmax = min + (end - start) - 1;\r\nspin_lock(&uprobes_treelock);\r\nn = find_node_in_range(inode, min, max);\r\nif (n) {\r\nfor (t = n; t; t = rb_prev(t)) {\r\nu = rb_entry(t, struct uprobe, rb_node);\r\nif (u->inode != inode || u->offset < min)\r\nbreak;\r\nlist_add(&u->pending_list, head);\r\natomic_inc(&u->ref);\r\n}\r\nfor (t = n; (t = rb_next(t)); ) {\r\nu = rb_entry(t, struct uprobe, rb_node);\r\nif (u->inode != inode || u->offset > max)\r\nbreak;\r\nlist_add(&u->pending_list, head);\r\natomic_inc(&u->ref);\r\n}\r\n}\r\nspin_unlock(&uprobes_treelock);\r\n}\r\nint uprobe_mmap(struct vm_area_struct *vma)\r\n{\r\nstruct list_head tmp_list;\r\nstruct uprobe *uprobe, *u;\r\nstruct inode *inode;\r\nif (no_uprobe_events() || !valid_vma(vma, true))\r\nreturn 0;\r\ninode = file_inode(vma->vm_file);\r\nif (!inode)\r\nreturn 0;\r\nmutex_lock(uprobes_mmap_hash(inode));\r\nbuild_probe_list(inode, vma, vma->vm_start, vma->vm_end, &tmp_list);\r\nlist_for_each_entry_safe(uprobe, u, &tmp_list, pending_list) {\r\nif (!fatal_signal_pending(current) &&\r\nfilter_chain(uprobe, UPROBE_FILTER_MMAP, vma->vm_mm)) {\r\nunsigned long vaddr = offset_to_vaddr(vma, uprobe->offset);\r\ninstall_breakpoint(uprobe, vma->vm_mm, vma, vaddr);\r\n}\r\nput_uprobe(uprobe);\r\n}\r\nmutex_unlock(uprobes_mmap_hash(inode));\r\nreturn 0;\r\n}\r\nstatic bool\r\nvma_has_uprobes(struct vm_area_struct *vma, unsigned long start, unsigned long end)\r\n{\r\nloff_t min, max;\r\nstruct inode *inode;\r\nstruct rb_node *n;\r\ninode = file_inode(vma->vm_file);\r\nmin = vaddr_to_offset(vma, start);\r\nmax = min + (end - start) - 1;\r\nspin_lock(&uprobes_treelock);\r\nn = find_node_in_range(inode, min, max);\r\nspin_unlock(&uprobes_treelock);\r\nreturn !!n;\r\n}\r\nvoid uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end)\r\n{\r\nif (no_uprobe_events() || !valid_vma(vma, false))\r\nreturn;\r\nif (!atomic_read(&vma->vm_mm->mm_users))\r\nreturn;\r\nif (!test_bit(MMF_HAS_UPROBES, &vma->vm_mm->flags) ||\r\ntest_bit(MMF_RECALC_UPROBES, &vma->vm_mm->flags))\r\nreturn;\r\nif (vma_has_uprobes(vma, start, end))\r\nset_bit(MMF_RECALC_UPROBES, &vma->vm_mm->flags);\r\n}\r\nstatic int xol_add_vma(struct mm_struct *mm, struct xol_area *area)\r\n{\r\nint ret = -EALREADY;\r\ndown_write(&mm->mmap_sem);\r\nif (mm->uprobes_state.xol_area)\r\ngoto fail;\r\nif (!area->vaddr) {\r\narea->vaddr = get_unmapped_area(NULL, TASK_SIZE - PAGE_SIZE,\r\nPAGE_SIZE, 0, 0);\r\nif (area->vaddr & ~PAGE_MASK) {\r\nret = area->vaddr;\r\ngoto fail;\r\n}\r\n}\r\nret = install_special_mapping(mm, area->vaddr, PAGE_SIZE,\r\nVM_EXEC|VM_MAYEXEC|VM_DONTCOPY|VM_IO, &area->page);\r\nif (ret)\r\ngoto fail;\r\nsmp_wmb();\r\nmm->uprobes_state.xol_area = area;\r\nfail:\r\nup_write(&mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic struct xol_area *__create_xol_area(unsigned long vaddr)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nuprobe_opcode_t insn = UPROBE_SWBP_INSN;\r\nstruct xol_area *area;\r\narea = kmalloc(sizeof(*area), GFP_KERNEL);\r\nif (unlikely(!area))\r\ngoto out;\r\narea->bitmap = kzalloc(BITS_TO_LONGS(UINSNS_PER_PAGE) * sizeof(long), GFP_KERNEL);\r\nif (!area->bitmap)\r\ngoto free_area;\r\narea->page = alloc_page(GFP_HIGHUSER);\r\nif (!area->page)\r\ngoto free_bitmap;\r\narea->vaddr = vaddr;\r\ninit_waitqueue_head(&area->wq);\r\nset_bit(0, area->bitmap);\r\natomic_set(&area->slot_count, 1);\r\ncopy_to_page(area->page, 0, &insn, UPROBE_SWBP_INSN_SIZE);\r\nif (!xol_add_vma(mm, area))\r\nreturn area;\r\n__free_page(area->page);\r\nfree_bitmap:\r\nkfree(area->bitmap);\r\nfree_area:\r\nkfree(area);\r\nout:\r\nreturn NULL;\r\n}\r\nstatic struct xol_area *get_xol_area(void)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct xol_area *area;\r\nif (!mm->uprobes_state.xol_area)\r\n__create_xol_area(0);\r\narea = mm->uprobes_state.xol_area;\r\nsmp_read_barrier_depends();\r\nreturn area;\r\n}\r\nvoid uprobe_clear_state(struct mm_struct *mm)\r\n{\r\nstruct xol_area *area = mm->uprobes_state.xol_area;\r\nif (!area)\r\nreturn;\r\nput_page(area->page);\r\nkfree(area->bitmap);\r\nkfree(area);\r\n}\r\nvoid uprobe_start_dup_mmap(void)\r\n{\r\npercpu_down_read(&dup_mmap_sem);\r\n}\r\nvoid uprobe_end_dup_mmap(void)\r\n{\r\npercpu_up_read(&dup_mmap_sem);\r\n}\r\nvoid uprobe_dup_mmap(struct mm_struct *oldmm, struct mm_struct *newmm)\r\n{\r\nnewmm->uprobes_state.xol_area = NULL;\r\nif (test_bit(MMF_HAS_UPROBES, &oldmm->flags)) {\r\nset_bit(MMF_HAS_UPROBES, &newmm->flags);\r\nset_bit(MMF_RECALC_UPROBES, &newmm->flags);\r\n}\r\n}\r\nstatic unsigned long xol_take_insn_slot(struct xol_area *area)\r\n{\r\nunsigned long slot_addr;\r\nint slot_nr;\r\ndo {\r\nslot_nr = find_first_zero_bit(area->bitmap, UINSNS_PER_PAGE);\r\nif (slot_nr < UINSNS_PER_PAGE) {\r\nif (!test_and_set_bit(slot_nr, area->bitmap))\r\nbreak;\r\nslot_nr = UINSNS_PER_PAGE;\r\ncontinue;\r\n}\r\nwait_event(area->wq, (atomic_read(&area->slot_count) < UINSNS_PER_PAGE));\r\n} while (slot_nr >= UINSNS_PER_PAGE);\r\nslot_addr = area->vaddr + (slot_nr * UPROBE_XOL_SLOT_BYTES);\r\natomic_inc(&area->slot_count);\r\nreturn slot_addr;\r\n}\r\nstatic unsigned long xol_get_insn_slot(struct uprobe *uprobe)\r\n{\r\nstruct xol_area *area;\r\nunsigned long xol_vaddr;\r\narea = get_xol_area();\r\nif (!area)\r\nreturn 0;\r\nxol_vaddr = xol_take_insn_slot(area);\r\nif (unlikely(!xol_vaddr))\r\nreturn 0;\r\narch_uprobe_copy_ixol(area->page, xol_vaddr,\r\n&uprobe->arch.ixol, sizeof(uprobe->arch.ixol));\r\nreturn xol_vaddr;\r\n}\r\nstatic void xol_free_insn_slot(struct task_struct *tsk)\r\n{\r\nstruct xol_area *area;\r\nunsigned long vma_end;\r\nunsigned long slot_addr;\r\nif (!tsk->mm || !tsk->mm->uprobes_state.xol_area || !tsk->utask)\r\nreturn;\r\nslot_addr = tsk->utask->xol_vaddr;\r\nif (unlikely(!slot_addr))\r\nreturn;\r\narea = tsk->mm->uprobes_state.xol_area;\r\nvma_end = area->vaddr + PAGE_SIZE;\r\nif (area->vaddr <= slot_addr && slot_addr < vma_end) {\r\nunsigned long offset;\r\nint slot_nr;\r\noffset = slot_addr - area->vaddr;\r\nslot_nr = offset / UPROBE_XOL_SLOT_BYTES;\r\nif (slot_nr >= UINSNS_PER_PAGE)\r\nreturn;\r\nclear_bit(slot_nr, area->bitmap);\r\natomic_dec(&area->slot_count);\r\nif (waitqueue_active(&area->wq))\r\nwake_up(&area->wq);\r\ntsk->utask->xol_vaddr = 0;\r\n}\r\n}\r\nvoid __weak arch_uprobe_copy_ixol(struct page *page, unsigned long vaddr,\r\nvoid *src, unsigned long len)\r\n{\r\ncopy_to_page(page, vaddr, src, len);\r\nflush_dcache_page(page);\r\n}\r\nunsigned long __weak uprobe_get_swbp_addr(struct pt_regs *regs)\r\n{\r\nreturn instruction_pointer(regs) - UPROBE_SWBP_INSN_SIZE;\r\n}\r\nunsigned long uprobe_get_trap_addr(struct pt_regs *regs)\r\n{\r\nstruct uprobe_task *utask = current->utask;\r\nif (unlikely(utask && utask->active_uprobe))\r\nreturn utask->vaddr;\r\nreturn instruction_pointer(regs);\r\n}\r\nvoid uprobe_free_utask(struct task_struct *t)\r\n{\r\nstruct uprobe_task *utask = t->utask;\r\nstruct return_instance *ri, *tmp;\r\nif (!utask)\r\nreturn;\r\nif (utask->active_uprobe)\r\nput_uprobe(utask->active_uprobe);\r\nri = utask->return_instances;\r\nwhile (ri) {\r\ntmp = ri;\r\nri = ri->next;\r\nput_uprobe(tmp->uprobe);\r\nkfree(tmp);\r\n}\r\nxol_free_insn_slot(t);\r\nkfree(utask);\r\nt->utask = NULL;\r\n}\r\nstatic struct uprobe_task *get_utask(void)\r\n{\r\nif (!current->utask)\r\ncurrent->utask = kzalloc(sizeof(struct uprobe_task), GFP_KERNEL);\r\nreturn current->utask;\r\n}\r\nstatic int dup_utask(struct task_struct *t, struct uprobe_task *o_utask)\r\n{\r\nstruct uprobe_task *n_utask;\r\nstruct return_instance **p, *o, *n;\r\nn_utask = kzalloc(sizeof(struct uprobe_task), GFP_KERNEL);\r\nif (!n_utask)\r\nreturn -ENOMEM;\r\nt->utask = n_utask;\r\np = &n_utask->return_instances;\r\nfor (o = o_utask->return_instances; o; o = o->next) {\r\nn = kmalloc(sizeof(struct return_instance), GFP_KERNEL);\r\nif (!n)\r\nreturn -ENOMEM;\r\n*n = *o;\r\natomic_inc(&n->uprobe->ref);\r\nn->next = NULL;\r\n*p = n;\r\np = &n->next;\r\nn_utask->depth++;\r\n}\r\nreturn 0;\r\n}\r\nstatic void uprobe_warn(struct task_struct *t, const char *msg)\r\n{\r\npr_warn("uprobe: %s:%d failed to %s\n",\r\ncurrent->comm, current->pid, msg);\r\n}\r\nstatic void dup_xol_work(struct callback_head *work)\r\n{\r\nif (current->flags & PF_EXITING)\r\nreturn;\r\nif (!__create_xol_area(current->utask->dup_xol_addr))\r\nuprobe_warn(current, "dup xol area");\r\n}\r\nvoid uprobe_copy_process(struct task_struct *t, unsigned long flags)\r\n{\r\nstruct uprobe_task *utask = current->utask;\r\nstruct mm_struct *mm = current->mm;\r\nstruct xol_area *area;\r\nt->utask = NULL;\r\nif (!utask || !utask->return_instances)\r\nreturn;\r\nif (mm == t->mm && !(flags & CLONE_VFORK))\r\nreturn;\r\nif (dup_utask(t, utask))\r\nreturn uprobe_warn(t, "dup ret instances");\r\narea = mm->uprobes_state.xol_area;\r\nif (!area)\r\nreturn uprobe_warn(t, "dup xol area");\r\nif (mm == t->mm)\r\nreturn;\r\nt->utask->dup_xol_addr = area->vaddr;\r\ninit_task_work(&t->utask->dup_xol_work, dup_xol_work);\r\ntask_work_add(t, &t->utask->dup_xol_work, true);\r\n}\r\nstatic unsigned long get_trampoline_vaddr(void)\r\n{\r\nstruct xol_area *area;\r\nunsigned long trampoline_vaddr = -1;\r\narea = current->mm->uprobes_state.xol_area;\r\nsmp_read_barrier_depends();\r\nif (area)\r\ntrampoline_vaddr = area->vaddr;\r\nreturn trampoline_vaddr;\r\n}\r\nstatic void prepare_uretprobe(struct uprobe *uprobe, struct pt_regs *regs)\r\n{\r\nstruct return_instance *ri;\r\nstruct uprobe_task *utask;\r\nunsigned long orig_ret_vaddr, trampoline_vaddr;\r\nbool chained = false;\r\nif (!get_xol_area())\r\nreturn;\r\nutask = get_utask();\r\nif (!utask)\r\nreturn;\r\nif (utask->depth >= MAX_URETPROBE_DEPTH) {\r\nprintk_ratelimited(KERN_INFO "uprobe: omit uretprobe due to"\r\n" nestedness limit pid/tgid=%d/%d\n",\r\ncurrent->pid, current->tgid);\r\nreturn;\r\n}\r\nri = kzalloc(sizeof(struct return_instance), GFP_KERNEL);\r\nif (!ri)\r\ngoto fail;\r\ntrampoline_vaddr = get_trampoline_vaddr();\r\norig_ret_vaddr = arch_uretprobe_hijack_return_addr(trampoline_vaddr, regs);\r\nif (orig_ret_vaddr == -1)\r\ngoto fail;\r\nif (orig_ret_vaddr == trampoline_vaddr) {\r\nif (!utask->return_instances) {\r\npr_warn("uprobe: unable to set uretprobe pid/tgid=%d/%d\n",\r\ncurrent->pid, current->tgid);\r\ngoto fail;\r\n}\r\nchained = true;\r\norig_ret_vaddr = utask->return_instances->orig_ret_vaddr;\r\n}\r\natomic_inc(&uprobe->ref);\r\nri->uprobe = uprobe;\r\nri->func = instruction_pointer(regs);\r\nri->orig_ret_vaddr = orig_ret_vaddr;\r\nri->chained = chained;\r\nutask->depth++;\r\nri->next = utask->return_instances;\r\nutask->return_instances = ri;\r\nreturn;\r\nfail:\r\nkfree(ri);\r\n}\r\nstatic int\r\npre_ssout(struct uprobe *uprobe, struct pt_regs *regs, unsigned long bp_vaddr)\r\n{\r\nstruct uprobe_task *utask;\r\nunsigned long xol_vaddr;\r\nint err;\r\nutask = get_utask();\r\nif (!utask)\r\nreturn -ENOMEM;\r\nxol_vaddr = xol_get_insn_slot(uprobe);\r\nif (!xol_vaddr)\r\nreturn -ENOMEM;\r\nutask->xol_vaddr = xol_vaddr;\r\nutask->vaddr = bp_vaddr;\r\nerr = arch_uprobe_pre_xol(&uprobe->arch, regs);\r\nif (unlikely(err)) {\r\nxol_free_insn_slot(current);\r\nreturn err;\r\n}\r\nutask->active_uprobe = uprobe;\r\nutask->state = UTASK_SSTEP;\r\nreturn 0;\r\n}\r\nbool uprobe_deny_signal(void)\r\n{\r\nstruct task_struct *t = current;\r\nstruct uprobe_task *utask = t->utask;\r\nif (likely(!utask || !utask->active_uprobe))\r\nreturn false;\r\nWARN_ON_ONCE(utask->state != UTASK_SSTEP);\r\nif (signal_pending(t)) {\r\nspin_lock_irq(&t->sighand->siglock);\r\nclear_tsk_thread_flag(t, TIF_SIGPENDING);\r\nspin_unlock_irq(&t->sighand->siglock);\r\nif (__fatal_signal_pending(t) || arch_uprobe_xol_was_trapped(t)) {\r\nutask->state = UTASK_SSTEP_TRAPPED;\r\nset_tsk_thread_flag(t, TIF_UPROBE);\r\n}\r\n}\r\nreturn true;\r\n}\r\nstatic void mmf_recalc_uprobes(struct mm_struct *mm)\r\n{\r\nstruct vm_area_struct *vma;\r\nfor (vma = mm->mmap; vma; vma = vma->vm_next) {\r\nif (!valid_vma(vma, false))\r\ncontinue;\r\nif (vma_has_uprobes(vma, vma->vm_start, vma->vm_end))\r\nreturn;\r\n}\r\nclear_bit(MMF_HAS_UPROBES, &mm->flags);\r\n}\r\nstatic int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr)\r\n{\r\nstruct page *page;\r\nuprobe_opcode_t opcode;\r\nint result;\r\npagefault_disable();\r\nresult = __copy_from_user_inatomic(&opcode, (void __user*)vaddr,\r\nsizeof(opcode));\r\npagefault_enable();\r\nif (likely(result == 0))\r\ngoto out;\r\nresult = get_user_pages(NULL, mm, vaddr, 1, 0, 1, &page, NULL);\r\nif (result < 0)\r\nreturn result;\r\ncopy_from_page(page, vaddr, &opcode, UPROBE_SWBP_INSN_SIZE);\r\nput_page(page);\r\nout:\r\nreturn is_trap_insn(&opcode);\r\n}\r\nstatic struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct uprobe *uprobe = NULL;\r\nstruct vm_area_struct *vma;\r\ndown_read(&mm->mmap_sem);\r\nvma = find_vma(mm, bp_vaddr);\r\nif (vma && vma->vm_start <= bp_vaddr) {\r\nif (valid_vma(vma, false)) {\r\nstruct inode *inode = file_inode(vma->vm_file);\r\nloff_t offset = vaddr_to_offset(vma, bp_vaddr);\r\nuprobe = find_uprobe(inode, offset);\r\n}\r\nif (!uprobe)\r\n*is_swbp = is_trap_at_addr(mm, bp_vaddr);\r\n} else {\r\n*is_swbp = -EFAULT;\r\n}\r\nif (!uprobe && test_and_clear_bit(MMF_RECALC_UPROBES, &mm->flags))\r\nmmf_recalc_uprobes(mm);\r\nup_read(&mm->mmap_sem);\r\nreturn uprobe;\r\n}\r\nstatic void handler_chain(struct uprobe *uprobe, struct pt_regs *regs)\r\n{\r\nstruct uprobe_consumer *uc;\r\nint remove = UPROBE_HANDLER_REMOVE;\r\nbool need_prep = false;\r\ndown_read(&uprobe->register_rwsem);\r\nfor (uc = uprobe->consumers; uc; uc = uc->next) {\r\nint rc = 0;\r\nif (uc->handler) {\r\nrc = uc->handler(uc, regs);\r\nWARN(rc & ~UPROBE_HANDLER_MASK,\r\n"bad rc=0x%x from %pf()\n", rc, uc->handler);\r\n}\r\nif (uc->ret_handler)\r\nneed_prep = true;\r\nremove &= rc;\r\n}\r\nif (need_prep && !remove)\r\nprepare_uretprobe(uprobe, regs);\r\nif (remove && uprobe->consumers) {\r\nWARN_ON(!uprobe_is_active(uprobe));\r\nunapply_uprobe(uprobe, current->mm);\r\n}\r\nup_read(&uprobe->register_rwsem);\r\n}\r\nstatic void\r\nhandle_uretprobe_chain(struct return_instance *ri, struct pt_regs *regs)\r\n{\r\nstruct uprobe *uprobe = ri->uprobe;\r\nstruct uprobe_consumer *uc;\r\ndown_read(&uprobe->register_rwsem);\r\nfor (uc = uprobe->consumers; uc; uc = uc->next) {\r\nif (uc->ret_handler)\r\nuc->ret_handler(uc, ri->func, regs);\r\n}\r\nup_read(&uprobe->register_rwsem);\r\n}\r\nstatic bool handle_trampoline(struct pt_regs *regs)\r\n{\r\nstruct uprobe_task *utask;\r\nstruct return_instance *ri, *tmp;\r\nbool chained;\r\nutask = current->utask;\r\nif (!utask)\r\nreturn false;\r\nri = utask->return_instances;\r\nif (!ri)\r\nreturn false;\r\ninstruction_pointer_set(regs, ri->orig_ret_vaddr);\r\nfor (;;) {\r\nhandle_uretprobe_chain(ri, regs);\r\nchained = ri->chained;\r\nput_uprobe(ri->uprobe);\r\ntmp = ri;\r\nri = ri->next;\r\nkfree(tmp);\r\nutask->depth--;\r\nif (!chained)\r\nbreak;\r\nBUG_ON(!ri);\r\n}\r\nutask->return_instances = ri;\r\nreturn true;\r\n}\r\nbool __weak arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs)\r\n{\r\nreturn false;\r\n}\r\nstatic void handle_swbp(struct pt_regs *regs)\r\n{\r\nstruct uprobe *uprobe;\r\nunsigned long bp_vaddr;\r\nint uninitialized_var(is_swbp);\r\nbp_vaddr = uprobe_get_swbp_addr(regs);\r\nif (bp_vaddr == get_trampoline_vaddr()) {\r\nif (handle_trampoline(regs))\r\nreturn;\r\npr_warn("uprobe: unable to handle uretprobe pid/tgid=%d/%d\n",\r\ncurrent->pid, current->tgid);\r\n}\r\nuprobe = find_active_uprobe(bp_vaddr, &is_swbp);\r\nif (!uprobe) {\r\nif (is_swbp > 0) {\r\nsend_sig(SIGTRAP, current, 0);\r\n} else {\r\ninstruction_pointer_set(regs, bp_vaddr);\r\n}\r\nreturn;\r\n}\r\ninstruction_pointer_set(regs, bp_vaddr);\r\nsmp_rmb();\r\nif (unlikely(!test_bit(UPROBE_COPY_INSN, &uprobe->flags)))\r\ngoto out;\r\nif (!get_utask())\r\ngoto out;\r\nif (arch_uprobe_ignore(&uprobe->arch, regs))\r\ngoto out;\r\nhandler_chain(uprobe, regs);\r\nif (arch_uprobe_skip_sstep(&uprobe->arch, regs))\r\ngoto out;\r\nif (!pre_ssout(uprobe, regs, bp_vaddr))\r\nreturn;\r\nout:\r\nput_uprobe(uprobe);\r\n}\r\nstatic void handle_singlestep(struct uprobe_task *utask, struct pt_regs *regs)\r\n{\r\nstruct uprobe *uprobe;\r\nint err = 0;\r\nuprobe = utask->active_uprobe;\r\nif (utask->state == UTASK_SSTEP_ACK)\r\nerr = arch_uprobe_post_xol(&uprobe->arch, regs);\r\nelse if (utask->state == UTASK_SSTEP_TRAPPED)\r\narch_uprobe_abort_xol(&uprobe->arch, regs);\r\nelse\r\nWARN_ON_ONCE(1);\r\nput_uprobe(uprobe);\r\nutask->active_uprobe = NULL;\r\nutask->state = UTASK_RUNNING;\r\nxol_free_insn_slot(current);\r\nspin_lock_irq(&current->sighand->siglock);\r\nrecalc_sigpending();\r\nspin_unlock_irq(&current->sighand->siglock);\r\nif (unlikely(err)) {\r\nuprobe_warn(current, "execute the probed insn, sending SIGILL.");\r\nforce_sig_info(SIGILL, SEND_SIG_FORCED, current);\r\n}\r\n}\r\nvoid uprobe_notify_resume(struct pt_regs *regs)\r\n{\r\nstruct uprobe_task *utask;\r\nclear_thread_flag(TIF_UPROBE);\r\nutask = current->utask;\r\nif (utask && utask->active_uprobe)\r\nhandle_singlestep(utask, regs);\r\nelse\r\nhandle_swbp(regs);\r\n}\r\nint uprobe_pre_sstep_notifier(struct pt_regs *regs)\r\n{\r\nif (!current->mm)\r\nreturn 0;\r\nif (!test_bit(MMF_HAS_UPROBES, &current->mm->flags) &&\r\n(!current->utask || !current->utask->return_instances))\r\nreturn 0;\r\nset_thread_flag(TIF_UPROBE);\r\nreturn 1;\r\n}\r\nint uprobe_post_sstep_notifier(struct pt_regs *regs)\r\n{\r\nstruct uprobe_task *utask = current->utask;\r\nif (!current->mm || !utask || !utask->active_uprobe)\r\nreturn 0;\r\nutask->state = UTASK_SSTEP_ACK;\r\nset_thread_flag(TIF_UPROBE);\r\nreturn 1;\r\n}\r\nstatic int __init init_uprobes(void)\r\n{\r\nint i;\r\nfor (i = 0; i < UPROBES_HASH_SZ; i++)\r\nmutex_init(&uprobes_mmap_mutex[i]);\r\nif (percpu_init_rwsem(&dup_mmap_sem))\r\nreturn -ENOMEM;\r\nreturn register_die_notifier(&uprobe_exception_nb);\r\n}
