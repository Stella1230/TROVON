static inline int is_file_epoll(struct file *f)\r\n{\r\nreturn f->f_op == &eventpoll_fops;\r\n}\r\nstatic inline void ep_set_ffd(struct epoll_filefd *ffd,\r\nstruct file *file, int fd)\r\n{\r\nffd->file = file;\r\nffd->fd = fd;\r\n}\r\nstatic inline int ep_cmp_ffd(struct epoll_filefd *p1,\r\nstruct epoll_filefd *p2)\r\n{\r\nreturn (p1->file > p2->file ? +1:\r\n(p1->file < p2->file ? -1 : p1->fd - p2->fd));\r\n}\r\nstatic inline int ep_is_linked(struct list_head *p)\r\n{\r\nreturn !list_empty(p);\r\n}\r\nstatic inline struct eppoll_entry *ep_pwq_from_wait(wait_queue_t *p)\r\n{\r\nreturn container_of(p, struct eppoll_entry, wait);\r\n}\r\nstatic inline struct epitem *ep_item_from_wait(wait_queue_t *p)\r\n{\r\nreturn container_of(p, struct eppoll_entry, wait)->base;\r\n}\r\nstatic inline struct epitem *ep_item_from_epqueue(poll_table *p)\r\n{\r\nreturn container_of(p, struct ep_pqueue, pt)->epi;\r\n}\r\nstatic inline int ep_op_has_event(int op)\r\n{\r\nreturn op != EPOLL_CTL_DEL;\r\n}\r\nstatic void ep_nested_calls_init(struct nested_calls *ncalls)\r\n{\r\nINIT_LIST_HEAD(&ncalls->tasks_call_list);\r\nspin_lock_init(&ncalls->lock);\r\n}\r\nstatic inline int ep_events_available(struct eventpoll *ep)\r\n{\r\nreturn !list_empty(&ep->rdllist) || ep->ovflist != EP_UNACTIVE_PTR;\r\n}\r\nstatic int ep_call_nested(struct nested_calls *ncalls, int max_nests,\r\nint (*nproc)(void *, void *, int), void *priv,\r\nvoid *cookie, void *ctx)\r\n{\r\nint error, call_nests = 0;\r\nunsigned long flags;\r\nstruct list_head *lsthead = &ncalls->tasks_call_list;\r\nstruct nested_call_node *tncur;\r\nstruct nested_call_node tnode;\r\nspin_lock_irqsave(&ncalls->lock, flags);\r\nlist_for_each_entry(tncur, lsthead, llink) {\r\nif (tncur->ctx == ctx &&\r\n(tncur->cookie == cookie || ++call_nests > max_nests)) {\r\nerror = -1;\r\ngoto out_unlock;\r\n}\r\n}\r\ntnode.ctx = ctx;\r\ntnode.cookie = cookie;\r\nlist_add(&tnode.llink, lsthead);\r\nspin_unlock_irqrestore(&ncalls->lock, flags);\r\nerror = (*nproc)(priv, cookie, call_nests);\r\nspin_lock_irqsave(&ncalls->lock, flags);\r\nlist_del(&tnode.llink);\r\nout_unlock:\r\nspin_unlock_irqrestore(&ncalls->lock, flags);\r\nreturn error;\r\n}\r\nstatic inline void ep_wake_up_nested(wait_queue_head_t *wqueue,\r\nunsigned long events, int subclass)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave_nested(&wqueue->lock, flags, subclass);\r\nwake_up_locked_poll(wqueue, events);\r\nspin_unlock_irqrestore(&wqueue->lock, flags);\r\n}\r\nstatic inline void ep_wake_up_nested(wait_queue_head_t *wqueue,\r\nunsigned long events, int subclass)\r\n{\r\nwake_up_poll(wqueue, events);\r\n}\r\nstatic int ep_poll_wakeup_proc(void *priv, void *cookie, int call_nests)\r\n{\r\nep_wake_up_nested((wait_queue_head_t *) cookie, POLLIN,\r\n1 + call_nests);\r\nreturn 0;\r\n}\r\nstatic void ep_poll_safewake(wait_queue_head_t *wq)\r\n{\r\nint this_cpu = get_cpu();\r\nep_call_nested(&poll_safewake_ncalls, EP_MAX_NESTS,\r\nep_poll_wakeup_proc, NULL, wq, (void *) (long) this_cpu);\r\nput_cpu();\r\n}\r\nstatic void ep_remove_wait_queue(struct eppoll_entry *pwq)\r\n{\r\nwait_queue_head_t *whead;\r\nrcu_read_lock();\r\nwhead = rcu_dereference(pwq->whead);\r\nif (whead)\r\nremove_wait_queue(whead, &pwq->wait);\r\nrcu_read_unlock();\r\n}\r\nstatic void ep_unregister_pollwait(struct eventpoll *ep, struct epitem *epi)\r\n{\r\nstruct list_head *lsthead = &epi->pwqlist;\r\nstruct eppoll_entry *pwq;\r\nwhile (!list_empty(lsthead)) {\r\npwq = list_first_entry(lsthead, struct eppoll_entry, llink);\r\nlist_del(&pwq->llink);\r\nep_remove_wait_queue(pwq);\r\nkmem_cache_free(pwq_cache, pwq);\r\n}\r\n}\r\nstatic inline struct wakeup_source *ep_wakeup_source(struct epitem *epi)\r\n{\r\nreturn rcu_dereference_check(epi->ws, lockdep_is_held(&epi->ep->mtx));\r\n}\r\nstatic inline void ep_pm_stay_awake(struct epitem *epi)\r\n{\r\nstruct wakeup_source *ws = ep_wakeup_source(epi);\r\nif (ws)\r\n__pm_stay_awake(ws);\r\n}\r\nstatic inline bool ep_has_wakeup_source(struct epitem *epi)\r\n{\r\nreturn rcu_access_pointer(epi->ws) ? true : false;\r\n}\r\nstatic inline void ep_pm_stay_awake_rcu(struct epitem *epi)\r\n{\r\nstruct wakeup_source *ws;\r\nrcu_read_lock();\r\nws = rcu_dereference(epi->ws);\r\nif (ws)\r\n__pm_stay_awake(ws);\r\nrcu_read_unlock();\r\n}\r\nstatic int ep_scan_ready_list(struct eventpoll *ep,\r\nint (*sproc)(struct eventpoll *,\r\nstruct list_head *, void *),\r\nvoid *priv, int depth, bool ep_locked)\r\n{\r\nint error, pwake = 0;\r\nunsigned long flags;\r\nstruct epitem *epi, *nepi;\r\nLIST_HEAD(txlist);\r\nif (!ep_locked)\r\nmutex_lock_nested(&ep->mtx, depth);\r\nspin_lock_irqsave(&ep->lock, flags);\r\nlist_splice_init(&ep->rdllist, &txlist);\r\nep->ovflist = NULL;\r\nspin_unlock_irqrestore(&ep->lock, flags);\r\nerror = (*sproc)(ep, &txlist, priv);\r\nspin_lock_irqsave(&ep->lock, flags);\r\nfor (nepi = ep->ovflist; (epi = nepi) != NULL;\r\nnepi = epi->next, epi->next = EP_UNACTIVE_PTR) {\r\nif (!ep_is_linked(&epi->rdllink)) {\r\nlist_add_tail(&epi->rdllink, &ep->rdllist);\r\nep_pm_stay_awake(epi);\r\n}\r\n}\r\nep->ovflist = EP_UNACTIVE_PTR;\r\nlist_splice(&txlist, &ep->rdllist);\r\n__pm_relax(ep->ws);\r\nif (!list_empty(&ep->rdllist)) {\r\nif (waitqueue_active(&ep->wq))\r\nwake_up_locked(&ep->wq);\r\nif (waitqueue_active(&ep->poll_wait))\r\npwake++;\r\n}\r\nspin_unlock_irqrestore(&ep->lock, flags);\r\nif (!ep_locked)\r\nmutex_unlock(&ep->mtx);\r\nif (pwake)\r\nep_poll_safewake(&ep->poll_wait);\r\nreturn error;\r\n}\r\nstatic void epi_rcu_free(struct rcu_head *head)\r\n{\r\nstruct epitem *epi = container_of(head, struct epitem, rcu);\r\nkmem_cache_free(epi_cache, epi);\r\n}\r\nstatic int ep_remove(struct eventpoll *ep, struct epitem *epi)\r\n{\r\nunsigned long flags;\r\nstruct file *file = epi->ffd.file;\r\nep_unregister_pollwait(ep, epi);\r\nspin_lock(&file->f_lock);\r\nlist_del_rcu(&epi->fllink);\r\nspin_unlock(&file->f_lock);\r\nrb_erase(&epi->rbn, &ep->rbr);\r\nspin_lock_irqsave(&ep->lock, flags);\r\nif (ep_is_linked(&epi->rdllink))\r\nlist_del_init(&epi->rdllink);\r\nspin_unlock_irqrestore(&ep->lock, flags);\r\nwakeup_source_unregister(ep_wakeup_source(epi));\r\ncall_rcu(&epi->rcu, epi_rcu_free);\r\natomic_long_dec(&ep->user->epoll_watches);\r\nreturn 0;\r\n}\r\nstatic void ep_free(struct eventpoll *ep)\r\n{\r\nstruct rb_node *rbp;\r\nstruct epitem *epi;\r\nif (waitqueue_active(&ep->poll_wait))\r\nep_poll_safewake(&ep->poll_wait);\r\nmutex_lock(&epmutex);\r\nfor (rbp = rb_first(&ep->rbr); rbp; rbp = rb_next(rbp)) {\r\nepi = rb_entry(rbp, struct epitem, rbn);\r\nep_unregister_pollwait(ep, epi);\r\ncond_resched();\r\n}\r\nmutex_lock(&ep->mtx);\r\nwhile ((rbp = rb_first(&ep->rbr)) != NULL) {\r\nepi = rb_entry(rbp, struct epitem, rbn);\r\nep_remove(ep, epi);\r\ncond_resched();\r\n}\r\nmutex_unlock(&ep->mtx);\r\nmutex_unlock(&epmutex);\r\nmutex_destroy(&ep->mtx);\r\nfree_uid(ep->user);\r\nwakeup_source_unregister(ep->ws);\r\nkfree(ep);\r\n}\r\nstatic int ep_eventpoll_release(struct inode *inode, struct file *file)\r\n{\r\nstruct eventpoll *ep = file->private_data;\r\nif (ep)\r\nep_free(ep);\r\nreturn 0;\r\n}\r\nstatic inline unsigned int ep_item_poll(struct epitem *epi, poll_table *pt)\r\n{\r\npt->_key = epi->event.events;\r\nreturn epi->ffd.file->f_op->poll(epi->ffd.file, pt) & epi->event.events;\r\n}\r\nstatic int ep_read_events_proc(struct eventpoll *ep, struct list_head *head,\r\nvoid *priv)\r\n{\r\nstruct epitem *epi, *tmp;\r\npoll_table pt;\r\ninit_poll_funcptr(&pt, NULL);\r\nlist_for_each_entry_safe(epi, tmp, head, rdllink) {\r\nif (ep_item_poll(epi, &pt))\r\nreturn POLLIN | POLLRDNORM;\r\nelse {\r\n__pm_relax(ep_wakeup_source(epi));\r\nlist_del_init(&epi->rdllink);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int ep_poll_readyevents_proc(void *priv, void *cookie, int call_nests)\r\n{\r\nstruct readyevents_arg *arg = priv;\r\nreturn ep_scan_ready_list(arg->ep, ep_read_events_proc, NULL,\r\ncall_nests + 1, arg->locked);\r\n}\r\nstatic unsigned int ep_eventpoll_poll(struct file *file, poll_table *wait)\r\n{\r\nint pollflags;\r\nstruct eventpoll *ep = file->private_data;\r\nstruct readyevents_arg arg;\r\narg.locked = wait && (wait->_qproc == ep_ptable_queue_proc);\r\narg.ep = ep;\r\npoll_wait(file, &ep->poll_wait, wait);\r\npollflags = ep_call_nested(&poll_readywalk_ncalls, EP_MAX_NESTS,\r\nep_poll_readyevents_proc, &arg, ep, current);\r\nreturn pollflags != -1 ? pollflags : 0;\r\n}\r\nstatic void ep_show_fdinfo(struct seq_file *m, struct file *f)\r\n{\r\nstruct eventpoll *ep = f->private_data;\r\nstruct rb_node *rbp;\r\nmutex_lock(&ep->mtx);\r\nfor (rbp = rb_first(&ep->rbr); rbp; rbp = rb_next(rbp)) {\r\nstruct epitem *epi = rb_entry(rbp, struct epitem, rbn);\r\nseq_printf(m, "tfd: %8d events: %8x data: %16llx\n",\r\nepi->ffd.fd, epi->event.events,\r\n(long long)epi->event.data);\r\nif (seq_has_overflowed(m))\r\nbreak;\r\n}\r\nmutex_unlock(&ep->mtx);\r\n}\r\nvoid eventpoll_release_file(struct file *file)\r\n{\r\nstruct eventpoll *ep;\r\nstruct epitem *epi, *next;\r\nmutex_lock(&epmutex);\r\nlist_for_each_entry_safe(epi, next, &file->f_ep_links, fllink) {\r\nep = epi->ep;\r\nmutex_lock_nested(&ep->mtx, 0);\r\nep_remove(ep, epi);\r\nmutex_unlock(&ep->mtx);\r\n}\r\nmutex_unlock(&epmutex);\r\n}\r\nstatic int ep_alloc(struct eventpoll **pep)\r\n{\r\nint error;\r\nstruct user_struct *user;\r\nstruct eventpoll *ep;\r\nuser = get_current_user();\r\nerror = -ENOMEM;\r\nep = kzalloc(sizeof(*ep), GFP_KERNEL);\r\nif (unlikely(!ep))\r\ngoto free_uid;\r\nspin_lock_init(&ep->lock);\r\nmutex_init(&ep->mtx);\r\ninit_waitqueue_head(&ep->wq);\r\ninit_waitqueue_head(&ep->poll_wait);\r\nINIT_LIST_HEAD(&ep->rdllist);\r\nep->rbr = RB_ROOT;\r\nep->ovflist = EP_UNACTIVE_PTR;\r\nep->user = user;\r\n*pep = ep;\r\nreturn 0;\r\nfree_uid:\r\nfree_uid(user);\r\nreturn error;\r\n}\r\nstatic struct epitem *ep_find(struct eventpoll *ep, struct file *file, int fd)\r\n{\r\nint kcmp;\r\nstruct rb_node *rbp;\r\nstruct epitem *epi, *epir = NULL;\r\nstruct epoll_filefd ffd;\r\nep_set_ffd(&ffd, file, fd);\r\nfor (rbp = ep->rbr.rb_node; rbp; ) {\r\nepi = rb_entry(rbp, struct epitem, rbn);\r\nkcmp = ep_cmp_ffd(&ffd, &epi->ffd);\r\nif (kcmp > 0)\r\nrbp = rbp->rb_right;\r\nelse if (kcmp < 0)\r\nrbp = rbp->rb_left;\r\nelse {\r\nepir = epi;\r\nbreak;\r\n}\r\n}\r\nreturn epir;\r\n}\r\nstatic int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *key)\r\n{\r\nint pwake = 0;\r\nunsigned long flags;\r\nstruct epitem *epi = ep_item_from_wait(wait);\r\nstruct eventpoll *ep = epi->ep;\r\nif ((unsigned long)key & POLLFREE) {\r\nep_pwq_from_wait(wait)->whead = NULL;\r\nlist_del_init(&wait->task_list);\r\n}\r\nspin_lock_irqsave(&ep->lock, flags);\r\nif (!(epi->event.events & ~EP_PRIVATE_BITS))\r\ngoto out_unlock;\r\nif (key && !((unsigned long) key & epi->event.events))\r\ngoto out_unlock;\r\nif (unlikely(ep->ovflist != EP_UNACTIVE_PTR)) {\r\nif (epi->next == EP_UNACTIVE_PTR) {\r\nepi->next = ep->ovflist;\r\nep->ovflist = epi;\r\nif (epi->ws) {\r\n__pm_stay_awake(ep->ws);\r\n}\r\n}\r\ngoto out_unlock;\r\n}\r\nif (!ep_is_linked(&epi->rdllink)) {\r\nlist_add_tail(&epi->rdllink, &ep->rdllist);\r\nep_pm_stay_awake_rcu(epi);\r\n}\r\nif (waitqueue_active(&ep->wq))\r\nwake_up_locked(&ep->wq);\r\nif (waitqueue_active(&ep->poll_wait))\r\npwake++;\r\nout_unlock:\r\nspin_unlock_irqrestore(&ep->lock, flags);\r\nif (pwake)\r\nep_poll_safewake(&ep->poll_wait);\r\nreturn 1;\r\n}\r\nstatic void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,\r\npoll_table *pt)\r\n{\r\nstruct epitem *epi = ep_item_from_epqueue(pt);\r\nstruct eppoll_entry *pwq;\r\nif (epi->nwait >= 0 && (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) {\r\ninit_waitqueue_func_entry(&pwq->wait, ep_poll_callback);\r\npwq->whead = whead;\r\npwq->base = epi;\r\nadd_wait_queue(whead, &pwq->wait);\r\nlist_add_tail(&pwq->llink, &epi->pwqlist);\r\nepi->nwait++;\r\n} else {\r\nepi->nwait = -1;\r\n}\r\n}\r\nstatic void ep_rbtree_insert(struct eventpoll *ep, struct epitem *epi)\r\n{\r\nint kcmp;\r\nstruct rb_node **p = &ep->rbr.rb_node, *parent = NULL;\r\nstruct epitem *epic;\r\nwhile (*p) {\r\nparent = *p;\r\nepic = rb_entry(parent, struct epitem, rbn);\r\nkcmp = ep_cmp_ffd(&epi->ffd, &epic->ffd);\r\nif (kcmp > 0)\r\np = &parent->rb_right;\r\nelse\r\np = &parent->rb_left;\r\n}\r\nrb_link_node(&epi->rbn, parent, p);\r\nrb_insert_color(&epi->rbn, &ep->rbr);\r\n}\r\nstatic int path_count_inc(int nests)\r\n{\r\nif (nests == 0)\r\nreturn 0;\r\nif (++path_count[nests] > path_limits[nests])\r\nreturn -1;\r\nreturn 0;\r\n}\r\nstatic void path_count_init(void)\r\n{\r\nint i;\r\nfor (i = 0; i < PATH_ARR_SIZE; i++)\r\npath_count[i] = 0;\r\n}\r\nstatic int reverse_path_check_proc(void *priv, void *cookie, int call_nests)\r\n{\r\nint error = 0;\r\nstruct file *file = priv;\r\nstruct file *child_file;\r\nstruct epitem *epi;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(epi, &file->f_ep_links, fllink) {\r\nchild_file = epi->ep->file;\r\nif (is_file_epoll(child_file)) {\r\nif (list_empty(&child_file->f_ep_links)) {\r\nif (path_count_inc(call_nests)) {\r\nerror = -1;\r\nbreak;\r\n}\r\n} else {\r\nerror = ep_call_nested(&poll_loop_ncalls,\r\nEP_MAX_NESTS,\r\nreverse_path_check_proc,\r\nchild_file, child_file,\r\ncurrent);\r\n}\r\nif (error != 0)\r\nbreak;\r\n} else {\r\nprintk(KERN_ERR "reverse_path_check_proc: "\r\n"file is not an ep!\n");\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn error;\r\n}\r\nstatic int reverse_path_check(void)\r\n{\r\nint error = 0;\r\nstruct file *current_file;\r\nlist_for_each_entry(current_file, &tfile_check_list, f_tfile_llink) {\r\npath_count_init();\r\nerror = ep_call_nested(&poll_loop_ncalls, EP_MAX_NESTS,\r\nreverse_path_check_proc, current_file,\r\ncurrent_file, current);\r\nif (error)\r\nbreak;\r\n}\r\nreturn error;\r\n}\r\nstatic int ep_create_wakeup_source(struct epitem *epi)\r\n{\r\nconst char *name;\r\nstruct wakeup_source *ws;\r\nif (!epi->ep->ws) {\r\nepi->ep->ws = wakeup_source_register("eventpoll");\r\nif (!epi->ep->ws)\r\nreturn -ENOMEM;\r\n}\r\nname = epi->ffd.file->f_path.dentry->d_name.name;\r\nws = wakeup_source_register(name);\r\nif (!ws)\r\nreturn -ENOMEM;\r\nrcu_assign_pointer(epi->ws, ws);\r\nreturn 0;\r\n}\r\nstatic noinline void ep_destroy_wakeup_source(struct epitem *epi)\r\n{\r\nstruct wakeup_source *ws = ep_wakeup_source(epi);\r\nRCU_INIT_POINTER(epi->ws, NULL);\r\nsynchronize_rcu();\r\nwakeup_source_unregister(ws);\r\n}\r\nstatic int ep_insert(struct eventpoll *ep, struct epoll_event *event,\r\nstruct file *tfile, int fd, int full_check)\r\n{\r\nint error, revents, pwake = 0;\r\nunsigned long flags;\r\nlong user_watches;\r\nstruct epitem *epi;\r\nstruct ep_pqueue epq;\r\nuser_watches = atomic_long_read(&ep->user->epoll_watches);\r\nif (unlikely(user_watches >= max_user_watches))\r\nreturn -ENOSPC;\r\nif (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&epi->rdllink);\r\nINIT_LIST_HEAD(&epi->fllink);\r\nINIT_LIST_HEAD(&epi->pwqlist);\r\nepi->ep = ep;\r\nep_set_ffd(&epi->ffd, tfile, fd);\r\nepi->event = *event;\r\nepi->nwait = 0;\r\nepi->next = EP_UNACTIVE_PTR;\r\nif (epi->event.events & EPOLLWAKEUP) {\r\nerror = ep_create_wakeup_source(epi);\r\nif (error)\r\ngoto error_create_wakeup_source;\r\n} else {\r\nRCU_INIT_POINTER(epi->ws, NULL);\r\n}\r\nepq.epi = epi;\r\ninit_poll_funcptr(&epq.pt, ep_ptable_queue_proc);\r\nrevents = ep_item_poll(epi, &epq.pt);\r\nerror = -ENOMEM;\r\nif (epi->nwait < 0)\r\ngoto error_unregister;\r\nspin_lock(&tfile->f_lock);\r\nlist_add_tail_rcu(&epi->fllink, &tfile->f_ep_links);\r\nspin_unlock(&tfile->f_lock);\r\nep_rbtree_insert(ep, epi);\r\nerror = -EINVAL;\r\nif (full_check && reverse_path_check())\r\ngoto error_remove_epi;\r\nspin_lock_irqsave(&ep->lock, flags);\r\nif ((revents & event->events) && !ep_is_linked(&epi->rdllink)) {\r\nlist_add_tail(&epi->rdllink, &ep->rdllist);\r\nep_pm_stay_awake(epi);\r\nif (waitqueue_active(&ep->wq))\r\nwake_up_locked(&ep->wq);\r\nif (waitqueue_active(&ep->poll_wait))\r\npwake++;\r\n}\r\nspin_unlock_irqrestore(&ep->lock, flags);\r\natomic_long_inc(&ep->user->epoll_watches);\r\nif (pwake)\r\nep_poll_safewake(&ep->poll_wait);\r\nreturn 0;\r\nerror_remove_epi:\r\nspin_lock(&tfile->f_lock);\r\nlist_del_rcu(&epi->fllink);\r\nspin_unlock(&tfile->f_lock);\r\nrb_erase(&epi->rbn, &ep->rbr);\r\nerror_unregister:\r\nep_unregister_pollwait(ep, epi);\r\nspin_lock_irqsave(&ep->lock, flags);\r\nif (ep_is_linked(&epi->rdllink))\r\nlist_del_init(&epi->rdllink);\r\nspin_unlock_irqrestore(&ep->lock, flags);\r\nwakeup_source_unregister(ep_wakeup_source(epi));\r\nerror_create_wakeup_source:\r\nkmem_cache_free(epi_cache, epi);\r\nreturn error;\r\n}\r\nstatic int ep_modify(struct eventpoll *ep, struct epitem *epi, struct epoll_event *event)\r\n{\r\nint pwake = 0;\r\nunsigned int revents;\r\npoll_table pt;\r\ninit_poll_funcptr(&pt, NULL);\r\nepi->event.events = event->events;\r\nepi->event.data = event->data;\r\nif (epi->event.events & EPOLLWAKEUP) {\r\nif (!ep_has_wakeup_source(epi))\r\nep_create_wakeup_source(epi);\r\n} else if (ep_has_wakeup_source(epi)) {\r\nep_destroy_wakeup_source(epi);\r\n}\r\nsmp_mb();\r\nrevents = ep_item_poll(epi, &pt);\r\nif (revents & event->events) {\r\nspin_lock_irq(&ep->lock);\r\nif (!ep_is_linked(&epi->rdllink)) {\r\nlist_add_tail(&epi->rdllink, &ep->rdllist);\r\nep_pm_stay_awake(epi);\r\nif (waitqueue_active(&ep->wq))\r\nwake_up_locked(&ep->wq);\r\nif (waitqueue_active(&ep->poll_wait))\r\npwake++;\r\n}\r\nspin_unlock_irq(&ep->lock);\r\n}\r\nif (pwake)\r\nep_poll_safewake(&ep->poll_wait);\r\nreturn 0;\r\n}\r\nstatic int ep_send_events_proc(struct eventpoll *ep, struct list_head *head,\r\nvoid *priv)\r\n{\r\nstruct ep_send_events_data *esed = priv;\r\nint eventcnt;\r\nunsigned int revents;\r\nstruct epitem *epi;\r\nstruct epoll_event __user *uevent;\r\nstruct wakeup_source *ws;\r\npoll_table pt;\r\ninit_poll_funcptr(&pt, NULL);\r\nfor (eventcnt = 0, uevent = esed->events;\r\n!list_empty(head) && eventcnt < esed->maxevents;) {\r\nepi = list_first_entry(head, struct epitem, rdllink);\r\nws = ep_wakeup_source(epi);\r\nif (ws) {\r\nif (ws->active)\r\n__pm_stay_awake(ep->ws);\r\n__pm_relax(ws);\r\n}\r\nlist_del_init(&epi->rdllink);\r\nrevents = ep_item_poll(epi, &pt);\r\nif (revents) {\r\nif (__put_user(revents, &uevent->events) ||\r\n__put_user(epi->event.data, &uevent->data)) {\r\nlist_add(&epi->rdllink, head);\r\nep_pm_stay_awake(epi);\r\nreturn eventcnt ? eventcnt : -EFAULT;\r\n}\r\neventcnt++;\r\nuevent++;\r\nif (epi->event.events & EPOLLONESHOT)\r\nepi->event.events &= EP_PRIVATE_BITS;\r\nelse if (!(epi->event.events & EPOLLET)) {\r\nlist_add_tail(&epi->rdllink, &ep->rdllist);\r\nep_pm_stay_awake(epi);\r\n}\r\n}\r\n}\r\nreturn eventcnt;\r\n}\r\nstatic int ep_send_events(struct eventpoll *ep,\r\nstruct epoll_event __user *events, int maxevents)\r\n{\r\nstruct ep_send_events_data esed;\r\nesed.maxevents = maxevents;\r\nesed.events = events;\r\nreturn ep_scan_ready_list(ep, ep_send_events_proc, &esed, 0, false);\r\n}\r\nstatic inline struct timespec ep_set_mstimeout(long ms)\r\n{\r\nstruct timespec now, ts = {\r\n.tv_sec = ms / MSEC_PER_SEC,\r\n.tv_nsec = NSEC_PER_MSEC * (ms % MSEC_PER_SEC),\r\n};\r\nktime_get_ts(&now);\r\nreturn timespec_add_safe(now, ts);\r\n}\r\nstatic int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,\r\nint maxevents, long timeout)\r\n{\r\nint res = 0, eavail, timed_out = 0;\r\nunsigned long flags;\r\nlong slack = 0;\r\nwait_queue_t wait;\r\nktime_t expires, *to = NULL;\r\nif (timeout > 0) {\r\nstruct timespec end_time = ep_set_mstimeout(timeout);\r\nslack = select_estimate_accuracy(&end_time);\r\nto = &expires;\r\n*to = timespec_to_ktime(end_time);\r\n} else if (timeout == 0) {\r\ntimed_out = 1;\r\nspin_lock_irqsave(&ep->lock, flags);\r\ngoto check_events;\r\n}\r\nfetch_events:\r\nspin_lock_irqsave(&ep->lock, flags);\r\nif (!ep_events_available(ep)) {\r\ninit_waitqueue_entry(&wait, current);\r\n__add_wait_queue_exclusive(&ep->wq, &wait);\r\nfor (;;) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (ep_events_available(ep) || timed_out)\r\nbreak;\r\nif (signal_pending(current)) {\r\nres = -EINTR;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&ep->lock, flags);\r\nif (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS))\r\ntimed_out = 1;\r\nspin_lock_irqsave(&ep->lock, flags);\r\n}\r\n__remove_wait_queue(&ep->wq, &wait);\r\n__set_current_state(TASK_RUNNING);\r\n}\r\ncheck_events:\r\neavail = ep_events_available(ep);\r\nspin_unlock_irqrestore(&ep->lock, flags);\r\nif (!res && eavail &&\r\n!(res = ep_send_events(ep, events, maxevents)) && !timed_out)\r\ngoto fetch_events;\r\nreturn res;\r\n}\r\nstatic int ep_loop_check_proc(void *priv, void *cookie, int call_nests)\r\n{\r\nint error = 0;\r\nstruct file *file = priv;\r\nstruct eventpoll *ep = file->private_data;\r\nstruct eventpoll *ep_tovisit;\r\nstruct rb_node *rbp;\r\nstruct epitem *epi;\r\nmutex_lock_nested(&ep->mtx, call_nests + 1);\r\nep->visited = 1;\r\nlist_add(&ep->visited_list_link, &visited_list);\r\nfor (rbp = rb_first(&ep->rbr); rbp; rbp = rb_next(rbp)) {\r\nepi = rb_entry(rbp, struct epitem, rbn);\r\nif (unlikely(is_file_epoll(epi->ffd.file))) {\r\nep_tovisit = epi->ffd.file->private_data;\r\nif (ep_tovisit->visited)\r\ncontinue;\r\nerror = ep_call_nested(&poll_loop_ncalls, EP_MAX_NESTS,\r\nep_loop_check_proc, epi->ffd.file,\r\nep_tovisit, current);\r\nif (error != 0)\r\nbreak;\r\n} else {\r\nif (list_empty(&epi->ffd.file->f_tfile_llink))\r\nlist_add(&epi->ffd.file->f_tfile_llink,\r\n&tfile_check_list);\r\n}\r\n}\r\nmutex_unlock(&ep->mtx);\r\nreturn error;\r\n}\r\nstatic int ep_loop_check(struct eventpoll *ep, struct file *file)\r\n{\r\nint ret;\r\nstruct eventpoll *ep_cur, *ep_next;\r\nret = ep_call_nested(&poll_loop_ncalls, EP_MAX_NESTS,\r\nep_loop_check_proc, file, ep, current);\r\nlist_for_each_entry_safe(ep_cur, ep_next, &visited_list,\r\nvisited_list_link) {\r\nep_cur->visited = 0;\r\nlist_del(&ep_cur->visited_list_link);\r\n}\r\nreturn ret;\r\n}\r\nstatic void clear_tfile_check_list(void)\r\n{\r\nstruct file *file;\r\nwhile (!list_empty(&tfile_check_list)) {\r\nfile = list_first_entry(&tfile_check_list, struct file,\r\nf_tfile_llink);\r\nlist_del_init(&file->f_tfile_llink);\r\n}\r\nINIT_LIST_HEAD(&tfile_check_list);\r\n}\r\nstatic int __init eventpoll_init(void)\r\n{\r\nstruct sysinfo si;\r\nsi_meminfo(&si);\r\nmax_user_watches = (((si.totalram - si.totalhigh) / 25) << PAGE_SHIFT) /\r\nEP_ITEM_COST;\r\nBUG_ON(max_user_watches < 0);\r\nep_nested_calls_init(&poll_loop_ncalls);\r\nep_nested_calls_init(&poll_safewake_ncalls);\r\nep_nested_calls_init(&poll_readywalk_ncalls);\r\nBUILD_BUG_ON(sizeof(void *) <= 8 && sizeof(struct epitem) > 128);\r\nepi_cache = kmem_cache_create("eventpoll_epi", sizeof(struct epitem),\r\n0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\r\npwq_cache = kmem_cache_create("eventpoll_pwq",\r\nsizeof(struct eppoll_entry), 0, SLAB_PANIC, NULL);\r\nreturn 0;\r\n}
