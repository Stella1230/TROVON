void ttm_bo_free_old_node(struct ttm_buffer_object *bo)\r\n{\r\nttm_bo_mem_put(bo, &bo->mem);\r\n}\r\nint ttm_bo_move_ttm(struct ttm_buffer_object *bo,\r\nbool evict,\r\nbool no_wait_gpu, struct ttm_mem_reg *new_mem)\r\n{\r\nstruct ttm_tt *ttm = bo->ttm;\r\nstruct ttm_mem_reg *old_mem = &bo->mem;\r\nint ret;\r\nif (old_mem->mem_type != TTM_PL_SYSTEM) {\r\nttm_tt_unbind(ttm);\r\nttm_bo_free_old_node(bo);\r\nttm_flag_masked(&old_mem->placement, TTM_PL_FLAG_SYSTEM,\r\nTTM_PL_MASK_MEM);\r\nold_mem->mem_type = TTM_PL_SYSTEM;\r\n}\r\nret = ttm_tt_set_placement_caching(ttm, new_mem->placement);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nif (new_mem->mem_type != TTM_PL_SYSTEM) {\r\nret = ttm_tt_bind(ttm, new_mem);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\n}\r\n*old_mem = *new_mem;\r\nnew_mem->mm_node = NULL;\r\nreturn 0;\r\n}\r\nint ttm_mem_io_lock(struct ttm_mem_type_manager *man, bool interruptible)\r\n{\r\nif (likely(man->io_reserve_fastpath))\r\nreturn 0;\r\nif (interruptible)\r\nreturn mutex_lock_interruptible(&man->io_reserve_mutex);\r\nmutex_lock(&man->io_reserve_mutex);\r\nreturn 0;\r\n}\r\nvoid ttm_mem_io_unlock(struct ttm_mem_type_manager *man)\r\n{\r\nif (likely(man->io_reserve_fastpath))\r\nreturn;\r\nmutex_unlock(&man->io_reserve_mutex);\r\n}\r\nstatic int ttm_mem_io_evict(struct ttm_mem_type_manager *man)\r\n{\r\nstruct ttm_buffer_object *bo;\r\nif (!man->use_io_reserve_lru || list_empty(&man->io_reserve_lru))\r\nreturn -EAGAIN;\r\nbo = list_first_entry(&man->io_reserve_lru,\r\nstruct ttm_buffer_object,\r\nio_reserve_lru);\r\nlist_del_init(&bo->io_reserve_lru);\r\nttm_bo_unmap_virtual_locked(bo);\r\nreturn 0;\r\n}\r\nint ttm_mem_io_reserve(struct ttm_bo_device *bdev,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nstruct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];\r\nint ret = 0;\r\nif (!bdev->driver->io_mem_reserve)\r\nreturn 0;\r\nif (likely(man->io_reserve_fastpath))\r\nreturn bdev->driver->io_mem_reserve(bdev, mem);\r\nif (bdev->driver->io_mem_reserve &&\r\nmem->bus.io_reserved_count++ == 0) {\r\nretry:\r\nret = bdev->driver->io_mem_reserve(bdev, mem);\r\nif (ret == -EAGAIN) {\r\nret = ttm_mem_io_evict(man);\r\nif (ret == 0)\r\ngoto retry;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nvoid ttm_mem_io_free(struct ttm_bo_device *bdev,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nstruct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];\r\nif (likely(man->io_reserve_fastpath))\r\nreturn;\r\nif (bdev->driver->io_mem_reserve &&\r\n--mem->bus.io_reserved_count == 0 &&\r\nbdev->driver->io_mem_free)\r\nbdev->driver->io_mem_free(bdev, mem);\r\n}\r\nint ttm_mem_io_reserve_vm(struct ttm_buffer_object *bo)\r\n{\r\nstruct ttm_mem_reg *mem = &bo->mem;\r\nint ret;\r\nif (!mem->bus.io_reserved_vm) {\r\nstruct ttm_mem_type_manager *man =\r\n&bo->bdev->man[mem->mem_type];\r\nret = ttm_mem_io_reserve(bo->bdev, mem);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nmem->bus.io_reserved_vm = true;\r\nif (man->use_io_reserve_lru)\r\nlist_add_tail(&bo->io_reserve_lru,\r\n&man->io_reserve_lru);\r\n}\r\nreturn 0;\r\n}\r\nvoid ttm_mem_io_free_vm(struct ttm_buffer_object *bo)\r\n{\r\nstruct ttm_mem_reg *mem = &bo->mem;\r\nif (mem->bus.io_reserved_vm) {\r\nmem->bus.io_reserved_vm = false;\r\nlist_del_init(&bo->io_reserve_lru);\r\nttm_mem_io_free(bo->bdev, mem);\r\n}\r\n}\r\nstatic int ttm_mem_reg_ioremap(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem,\r\nvoid **virtual)\r\n{\r\nstruct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];\r\nint ret;\r\nvoid *addr;\r\n*virtual = NULL;\r\n(void) ttm_mem_io_lock(man, false);\r\nret = ttm_mem_io_reserve(bdev, mem);\r\nttm_mem_io_unlock(man);\r\nif (ret || !mem->bus.is_iomem)\r\nreturn ret;\r\nif (mem->bus.addr) {\r\naddr = mem->bus.addr;\r\n} else {\r\nif (mem->placement & TTM_PL_FLAG_WC)\r\naddr = ioremap_wc(mem->bus.base + mem->bus.offset, mem->bus.size);\r\nelse\r\naddr = ioremap_nocache(mem->bus.base + mem->bus.offset, mem->bus.size);\r\nif (!addr) {\r\n(void) ttm_mem_io_lock(man, false);\r\nttm_mem_io_free(bdev, mem);\r\nttm_mem_io_unlock(man);\r\nreturn -ENOMEM;\r\n}\r\n}\r\n*virtual = addr;\r\nreturn 0;\r\n}\r\nstatic void ttm_mem_reg_iounmap(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem,\r\nvoid *virtual)\r\n{\r\nstruct ttm_mem_type_manager *man;\r\nman = &bdev->man[mem->mem_type];\r\nif (virtual && mem->bus.addr == NULL)\r\niounmap(virtual);\r\n(void) ttm_mem_io_lock(man, false);\r\nttm_mem_io_free(bdev, mem);\r\nttm_mem_io_unlock(man);\r\n}\r\nstatic int ttm_copy_io_page(void *dst, void *src, unsigned long page)\r\n{\r\nuint32_t *dstP =\r\n(uint32_t *) ((unsigned long)dst + (page << PAGE_SHIFT));\r\nuint32_t *srcP =\r\n(uint32_t *) ((unsigned long)src + (page << PAGE_SHIFT));\r\nint i;\r\nfor (i = 0; i < PAGE_SIZE / sizeof(uint32_t); ++i)\r\niowrite32(ioread32(srcP++), dstP++);\r\nreturn 0;\r\n}\r\nstatic int ttm_copy_io_ttm_page(struct ttm_tt *ttm, void *src,\r\nunsigned long page,\r\npgprot_t prot)\r\n{\r\nstruct page *d = ttm->pages[page];\r\nvoid *dst;\r\nif (!d)\r\nreturn -ENOMEM;\r\nsrc = (void *)((unsigned long)src + (page << PAGE_SHIFT));\r\n#ifdef CONFIG_X86\r\ndst = kmap_atomic_prot(d, prot);\r\n#else\r\nif (pgprot_val(prot) != pgprot_val(PAGE_KERNEL))\r\ndst = vmap(&d, 1, 0, prot);\r\nelse\r\ndst = kmap(d);\r\n#endif\r\nif (!dst)\r\nreturn -ENOMEM;\r\nmemcpy_fromio(dst, src, PAGE_SIZE);\r\n#ifdef CONFIG_X86\r\nkunmap_atomic(dst);\r\n#else\r\nif (pgprot_val(prot) != pgprot_val(PAGE_KERNEL))\r\nvunmap(dst);\r\nelse\r\nkunmap(d);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int ttm_copy_ttm_io_page(struct ttm_tt *ttm, void *dst,\r\nunsigned long page,\r\npgprot_t prot)\r\n{\r\nstruct page *s = ttm->pages[page];\r\nvoid *src;\r\nif (!s)\r\nreturn -ENOMEM;\r\ndst = (void *)((unsigned long)dst + (page << PAGE_SHIFT));\r\n#ifdef CONFIG_X86\r\nsrc = kmap_atomic_prot(s, prot);\r\n#else\r\nif (pgprot_val(prot) != pgprot_val(PAGE_KERNEL))\r\nsrc = vmap(&s, 1, 0, prot);\r\nelse\r\nsrc = kmap(s);\r\n#endif\r\nif (!src)\r\nreturn -ENOMEM;\r\nmemcpy_toio(dst, src, PAGE_SIZE);\r\n#ifdef CONFIG_X86\r\nkunmap_atomic(src);\r\n#else\r\nif (pgprot_val(prot) != pgprot_val(PAGE_KERNEL))\r\nvunmap(src);\r\nelse\r\nkunmap(s);\r\n#endif\r\nreturn 0;\r\n}\r\nint ttm_bo_move_memcpy(struct ttm_buffer_object *bo,\r\nbool evict, bool no_wait_gpu,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nstruct ttm_bo_device *bdev = bo->bdev;\r\nstruct ttm_mem_type_manager *man = &bdev->man[new_mem->mem_type];\r\nstruct ttm_tt *ttm = bo->ttm;\r\nstruct ttm_mem_reg *old_mem = &bo->mem;\r\nstruct ttm_mem_reg old_copy = *old_mem;\r\nvoid *old_iomap;\r\nvoid *new_iomap;\r\nint ret;\r\nunsigned long i;\r\nunsigned long page;\r\nunsigned long add = 0;\r\nint dir;\r\nret = ttm_mem_reg_ioremap(bdev, old_mem, &old_iomap);\r\nif (ret)\r\nreturn ret;\r\nret = ttm_mem_reg_ioremap(bdev, new_mem, &new_iomap);\r\nif (ret)\r\ngoto out;\r\nif (old_iomap == NULL && new_iomap == NULL)\r\ngoto out2;\r\nif (old_iomap == NULL &&\r\n(ttm == NULL || (ttm->state == tt_unpopulated &&\r\n!(ttm->page_flags & TTM_PAGE_FLAG_SWAPPED)))) {\r\nmemset_io(new_iomap, 0, new_mem->num_pages*PAGE_SIZE);\r\ngoto out2;\r\n}\r\nif (ttm && ttm->state == tt_unpopulated) {\r\nret = ttm->bdev->driver->ttm_tt_populate(ttm);\r\nif (ret)\r\ngoto out1;\r\n}\r\nadd = 0;\r\ndir = 1;\r\nif ((old_mem->mem_type == new_mem->mem_type) &&\r\n(new_mem->start < old_mem->start + old_mem->size)) {\r\ndir = -1;\r\nadd = new_mem->num_pages - 1;\r\n}\r\nfor (i = 0; i < new_mem->num_pages; ++i) {\r\npage = i * dir + add;\r\nif (old_iomap == NULL) {\r\npgprot_t prot = ttm_io_prot(old_mem->placement,\r\nPAGE_KERNEL);\r\nret = ttm_copy_ttm_io_page(ttm, new_iomap, page,\r\nprot);\r\n} else if (new_iomap == NULL) {\r\npgprot_t prot = ttm_io_prot(new_mem->placement,\r\nPAGE_KERNEL);\r\nret = ttm_copy_io_ttm_page(ttm, old_iomap, page,\r\nprot);\r\n} else\r\nret = ttm_copy_io_page(new_iomap, old_iomap, page);\r\nif (ret)\r\ngoto out1;\r\n}\r\nmb();\r\nout2:\r\nold_copy = *old_mem;\r\n*old_mem = *new_mem;\r\nnew_mem->mm_node = NULL;\r\nif ((man->flags & TTM_MEMTYPE_FLAG_FIXED) && (ttm != NULL)) {\r\nttm_tt_unbind(ttm);\r\nttm_tt_destroy(ttm);\r\nbo->ttm = NULL;\r\n}\r\nout1:\r\nttm_mem_reg_iounmap(bdev, old_mem, new_iomap);\r\nout:\r\nttm_mem_reg_iounmap(bdev, &old_copy, old_iomap);\r\nif (!ret)\r\nttm_bo_mem_put(bo, &old_copy);\r\nreturn ret;\r\n}\r\nstatic void ttm_transfered_destroy(struct ttm_buffer_object *bo)\r\n{\r\nkfree(bo);\r\n}\r\nstatic int ttm_buffer_object_transfer(struct ttm_buffer_object *bo,\r\nstruct ttm_buffer_object **new_obj)\r\n{\r\nstruct ttm_buffer_object *fbo;\r\nint ret;\r\nfbo = kmalloc(sizeof(*fbo), GFP_KERNEL);\r\nif (!fbo)\r\nreturn -ENOMEM;\r\n*fbo = *bo;\r\nINIT_LIST_HEAD(&fbo->ddestroy);\r\nINIT_LIST_HEAD(&fbo->lru);\r\nINIT_LIST_HEAD(&fbo->swap);\r\nINIT_LIST_HEAD(&fbo->io_reserve_lru);\r\ndrm_vma_node_reset(&fbo->vma_node);\r\natomic_set(&fbo->cpu_writers, 0);\r\nkref_init(&fbo->list_kref);\r\nkref_init(&fbo->kref);\r\nfbo->destroy = &ttm_transfered_destroy;\r\nfbo->acc_size = 0;\r\nfbo->resv = &fbo->ttm_resv;\r\nreservation_object_init(fbo->resv);\r\nret = ww_mutex_trylock(&fbo->resv->lock);\r\nWARN_ON(!ret);\r\n*new_obj = fbo;\r\nreturn 0;\r\n}\r\npgprot_t ttm_io_prot(uint32_t caching_flags, pgprot_t tmp)\r\n{\r\nif (caching_flags & TTM_PL_FLAG_CACHED)\r\nreturn tmp;\r\n#if defined(__i386__) || defined(__x86_64__)\r\nif (caching_flags & TTM_PL_FLAG_WC)\r\ntmp = pgprot_writecombine(tmp);\r\nelse if (boot_cpu_data.x86 > 3)\r\ntmp = pgprot_noncached(tmp);\r\n#endif\r\n#if defined(__ia64__) || defined(__arm__) || defined(__powerpc__)\r\nif (caching_flags & TTM_PL_FLAG_WC)\r\ntmp = pgprot_writecombine(tmp);\r\nelse\r\ntmp = pgprot_noncached(tmp);\r\n#endif\r\n#if defined(__sparc__) || defined(__mips__)\r\ntmp = pgprot_noncached(tmp);\r\n#endif\r\nreturn tmp;\r\n}\r\nstatic int ttm_bo_ioremap(struct ttm_buffer_object *bo,\r\nunsigned long offset,\r\nunsigned long size,\r\nstruct ttm_bo_kmap_obj *map)\r\n{\r\nstruct ttm_mem_reg *mem = &bo->mem;\r\nif (bo->mem.bus.addr) {\r\nmap->bo_kmap_type = ttm_bo_map_premapped;\r\nmap->virtual = (void *)(((u8 *)bo->mem.bus.addr) + offset);\r\n} else {\r\nmap->bo_kmap_type = ttm_bo_map_iomap;\r\nif (mem->placement & TTM_PL_FLAG_WC)\r\nmap->virtual = ioremap_wc(bo->mem.bus.base + bo->mem.bus.offset + offset,\r\nsize);\r\nelse\r\nmap->virtual = ioremap_nocache(bo->mem.bus.base + bo->mem.bus.offset + offset,\r\nsize);\r\n}\r\nreturn (!map->virtual) ? -ENOMEM : 0;\r\n}\r\nstatic int ttm_bo_kmap_ttm(struct ttm_buffer_object *bo,\r\nunsigned long start_page,\r\nunsigned long num_pages,\r\nstruct ttm_bo_kmap_obj *map)\r\n{\r\nstruct ttm_mem_reg *mem = &bo->mem; pgprot_t prot;\r\nstruct ttm_tt *ttm = bo->ttm;\r\nint ret;\r\nBUG_ON(!ttm);\r\nif (ttm->state == tt_unpopulated) {\r\nret = ttm->bdev->driver->ttm_tt_populate(ttm);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (num_pages == 1 && (mem->placement & TTM_PL_FLAG_CACHED)) {\r\nmap->bo_kmap_type = ttm_bo_map_kmap;\r\nmap->page = ttm->pages[start_page];\r\nmap->virtual = kmap(map->page);\r\n} else {\r\nprot = ttm_io_prot(mem->placement, PAGE_KERNEL);\r\nmap->bo_kmap_type = ttm_bo_map_vmap;\r\nmap->virtual = vmap(ttm->pages + start_page, num_pages,\r\n0, prot);\r\n}\r\nreturn (!map->virtual) ? -ENOMEM : 0;\r\n}\r\nint ttm_bo_kmap(struct ttm_buffer_object *bo,\r\nunsigned long start_page, unsigned long num_pages,\r\nstruct ttm_bo_kmap_obj *map)\r\n{\r\nstruct ttm_mem_type_manager *man =\r\n&bo->bdev->man[bo->mem.mem_type];\r\nunsigned long offset, size;\r\nint ret;\r\nBUG_ON(!list_empty(&bo->swap));\r\nmap->virtual = NULL;\r\nmap->bo = bo;\r\nif (num_pages > bo->num_pages)\r\nreturn -EINVAL;\r\nif (start_page > bo->num_pages)\r\nreturn -EINVAL;\r\n#if 0\r\nif (num_pages > 1 && !capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\n#endif\r\n(void) ttm_mem_io_lock(man, false);\r\nret = ttm_mem_io_reserve(bo->bdev, &bo->mem);\r\nttm_mem_io_unlock(man);\r\nif (ret)\r\nreturn ret;\r\nif (!bo->mem.bus.is_iomem) {\r\nreturn ttm_bo_kmap_ttm(bo, start_page, num_pages, map);\r\n} else {\r\noffset = start_page << PAGE_SHIFT;\r\nsize = num_pages << PAGE_SHIFT;\r\nreturn ttm_bo_ioremap(bo, offset, size, map);\r\n}\r\n}\r\nvoid ttm_bo_kunmap(struct ttm_bo_kmap_obj *map)\r\n{\r\nstruct ttm_buffer_object *bo = map->bo;\r\nstruct ttm_mem_type_manager *man =\r\n&bo->bdev->man[bo->mem.mem_type];\r\nif (!map->virtual)\r\nreturn;\r\nswitch (map->bo_kmap_type) {\r\ncase ttm_bo_map_iomap:\r\niounmap(map->virtual);\r\nbreak;\r\ncase ttm_bo_map_vmap:\r\nvunmap(map->virtual);\r\nbreak;\r\ncase ttm_bo_map_kmap:\r\nkunmap(map->page);\r\nbreak;\r\ncase ttm_bo_map_premapped:\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n(void) ttm_mem_io_lock(man, false);\r\nttm_mem_io_free(map->bo->bdev, &map->bo->mem);\r\nttm_mem_io_unlock(man);\r\nmap->virtual = NULL;\r\nmap->page = NULL;\r\n}\r\nint ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,\r\nstruct fence *fence,\r\nbool evict,\r\nbool no_wait_gpu,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nstruct ttm_bo_device *bdev = bo->bdev;\r\nstruct ttm_mem_type_manager *man = &bdev->man[new_mem->mem_type];\r\nstruct ttm_mem_reg *old_mem = &bo->mem;\r\nint ret;\r\nstruct ttm_buffer_object *ghost_obj;\r\nreservation_object_add_excl_fence(bo->resv, fence);\r\nif (evict) {\r\nret = ttm_bo_wait(bo, false, false, false);\r\nif (ret)\r\nreturn ret;\r\nif ((man->flags & TTM_MEMTYPE_FLAG_FIXED) &&\r\n(bo->ttm != NULL)) {\r\nttm_tt_unbind(bo->ttm);\r\nttm_tt_destroy(bo->ttm);\r\nbo->ttm = NULL;\r\n}\r\nttm_bo_free_old_node(bo);\r\n} else {\r\nset_bit(TTM_BO_PRIV_FLAG_MOVING, &bo->priv_flags);\r\nret = ttm_buffer_object_transfer(bo, &ghost_obj);\r\nif (ret)\r\nreturn ret;\r\nreservation_object_add_excl_fence(ghost_obj->resv, fence);\r\nif (!(man->flags & TTM_MEMTYPE_FLAG_FIXED))\r\nghost_obj->ttm = NULL;\r\nelse\r\nbo->ttm = NULL;\r\nttm_bo_unreserve(ghost_obj);\r\nttm_bo_unref(&ghost_obj);\r\n}\r\n*old_mem = *new_mem;\r\nnew_mem->mm_node = NULL;\r\nreturn 0;\r\n}
