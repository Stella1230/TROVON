void\r\nnvkm_gpuobj_destroy(struct nvkm_gpuobj *gpuobj)\r\n{\r\nint i;\r\nif (gpuobj->flags & NVOBJ_FLAG_ZERO_FREE) {\r\nfor (i = 0; i < gpuobj->size; i += 4)\r\nnv_wo32(gpuobj, i, 0x00000000);\r\n}\r\nif (gpuobj->node)\r\nnvkm_mm_free(&nv_gpuobj(gpuobj->parent)->heap, &gpuobj->node);\r\nif (gpuobj->heap.block_size)\r\nnvkm_mm_fini(&gpuobj->heap);\r\nnvkm_object_destroy(&gpuobj->object);\r\n}\r\nint\r\nnvkm_gpuobj_create_(struct nvkm_object *parent, struct nvkm_object *engine,\r\nstruct nvkm_oclass *oclass, u32 pclass,\r\nstruct nvkm_object *pargpu, u32 size, u32 align, u32 flags,\r\nint length, void **pobject)\r\n{\r\nstruct nvkm_instmem *imem = nvkm_instmem(parent);\r\nstruct nvkm_bar *bar = nvkm_bar(parent);\r\nstruct nvkm_gpuobj *gpuobj;\r\nstruct nvkm_mm *heap = NULL;\r\nint ret, i;\r\nu64 addr;\r\n*pobject = NULL;\r\nif (pargpu) {\r\nwhile ((pargpu = nv_pclass(pargpu, NV_GPUOBJ_CLASS))) {\r\nif (nv_gpuobj(pargpu)->heap.block_size)\r\nbreak;\r\npargpu = pargpu->parent;\r\n}\r\nif (unlikely(pargpu == NULL)) {\r\nnv_error(parent, "no gpuobj heap\n");\r\nreturn -EINVAL;\r\n}\r\naddr = nv_gpuobj(pargpu)->addr;\r\nheap = &nv_gpuobj(pargpu)->heap;\r\natomic_inc(&parent->refcount);\r\n} else {\r\nret = imem->alloc(imem, parent, size, align, &parent);\r\npargpu = parent;\r\nif (ret)\r\nreturn ret;\r\naddr = nv_memobj(pargpu)->addr;\r\nsize = nv_memobj(pargpu)->size;\r\nif (bar && bar->alloc) {\r\nstruct nvkm_instobj *iobj = (void *)parent;\r\nstruct nvkm_mem **mem = (void *)(iobj + 1);\r\nstruct nvkm_mem *node = *mem;\r\nif (!bar->alloc(bar, parent, node, &pargpu)) {\r\nnvkm_object_ref(NULL, &parent);\r\nparent = pargpu;\r\n}\r\n}\r\n}\r\nret = nvkm_object_create_(parent, engine, oclass, pclass |\r\nNV_GPUOBJ_CLASS, length, pobject);\r\nnvkm_object_ref(NULL, &parent);\r\ngpuobj = *pobject;\r\nif (ret)\r\nreturn ret;\r\ngpuobj->parent = pargpu;\r\ngpuobj->flags = flags;\r\ngpuobj->addr = addr;\r\ngpuobj->size = size;\r\nif (heap) {\r\nret = nvkm_mm_head(heap, 0, 1, size, size, max(align, (u32)1),\r\n&gpuobj->node);\r\nif (ret)\r\nreturn ret;\r\ngpuobj->addr += gpuobj->node->offset;\r\n}\r\nif (gpuobj->flags & NVOBJ_FLAG_HEAP) {\r\nret = nvkm_mm_init(&gpuobj->heap, 0, gpuobj->size, 1);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (flags & NVOBJ_FLAG_ZERO_ALLOC) {\r\nfor (i = 0; i < gpuobj->size; i += 4)\r\nnv_wo32(gpuobj, i, 0x00000000);\r\n}\r\nreturn ret;\r\n}\r\nstatic int\r\n_nvkm_gpuobj_ctor(struct nvkm_object *parent, struct nvkm_object *engine,\r\nstruct nvkm_oclass *oclass, void *data, u32 size,\r\nstruct nvkm_object **pobject)\r\n{\r\nstruct nvkm_gpuobj_class *args = data;\r\nstruct nvkm_gpuobj *object;\r\nint ret;\r\nret = nvkm_gpuobj_create(parent, engine, oclass, 0, args->pargpu,\r\nargs->size, args->align, args->flags,\r\n&object);\r\n*pobject = nv_object(object);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nvoid\r\n_nvkm_gpuobj_dtor(struct nvkm_object *object)\r\n{\r\nnvkm_gpuobj_destroy(nv_gpuobj(object));\r\n}\r\nint\r\n_nvkm_gpuobj_init(struct nvkm_object *object)\r\n{\r\nreturn nvkm_gpuobj_init(nv_gpuobj(object));\r\n}\r\nint\r\n_nvkm_gpuobj_fini(struct nvkm_object *object, bool suspend)\r\n{\r\nreturn nvkm_gpuobj_fini(nv_gpuobj(object), suspend);\r\n}\r\nu32\r\n_nvkm_gpuobj_rd32(struct nvkm_object *object, u64 addr)\r\n{\r\nstruct nvkm_gpuobj *gpuobj = nv_gpuobj(object);\r\nstruct nvkm_ofuncs *pfuncs = nv_ofuncs(gpuobj->parent);\r\nif (gpuobj->node)\r\naddr += gpuobj->node->offset;\r\nreturn pfuncs->rd32(gpuobj->parent, addr);\r\n}\r\nvoid\r\n_nvkm_gpuobj_wr32(struct nvkm_object *object, u64 addr, u32 data)\r\n{\r\nstruct nvkm_gpuobj *gpuobj = nv_gpuobj(object);\r\nstruct nvkm_ofuncs *pfuncs = nv_ofuncs(gpuobj->parent);\r\nif (gpuobj->node)\r\naddr += gpuobj->node->offset;\r\npfuncs->wr32(gpuobj->parent, addr, data);\r\n}\r\nint\r\nnvkm_gpuobj_new(struct nvkm_object *parent, struct nvkm_object *pargpu,\r\nu32 size, u32 align, u32 flags,\r\nstruct nvkm_gpuobj **pgpuobj)\r\n{\r\nstruct nvkm_object *engine = parent;\r\nstruct nvkm_gpuobj_class args = {\r\n.pargpu = pargpu,\r\n.size = size,\r\n.align = align,\r\n.flags = flags,\r\n};\r\nif (!nv_iclass(engine, NV_SUBDEV_CLASS))\r\nengine = &engine->engine->subdev.object;\r\nBUG_ON(engine == NULL);\r\nreturn nvkm_object_ctor(parent, engine, &_nvkm_gpuobj_oclass,\r\n&args, sizeof(args),\r\n(struct nvkm_object **)pgpuobj);\r\n}\r\nint\r\nnvkm_gpuobj_map(struct nvkm_gpuobj *gpuobj, u32 access, struct nvkm_vma *vma)\r\n{\r\nstruct nvkm_bar *bar = nvkm_bar(gpuobj);\r\nint ret = -EINVAL;\r\nif (bar && bar->umap) {\r\nstruct nvkm_instobj *iobj = (void *)\r\nnv_pclass(nv_object(gpuobj), NV_MEMOBJ_CLASS);\r\nstruct nvkm_mem **mem = (void *)(iobj + 1);\r\nret = bar->umap(bar, *mem, access, vma);\r\n}\r\nreturn ret;\r\n}\r\nint\r\nnvkm_gpuobj_map_vm(struct nvkm_gpuobj *gpuobj, struct nvkm_vm *vm,\r\nu32 access, struct nvkm_vma *vma)\r\n{\r\nstruct nvkm_instobj *iobj = (void *)\r\nnv_pclass(nv_object(gpuobj), NV_MEMOBJ_CLASS);\r\nstruct nvkm_mem **mem = (void *)(iobj + 1);\r\nint ret;\r\nret = nvkm_vm_get(vm, gpuobj->size, 12, access, vma);\r\nif (ret)\r\nreturn ret;\r\nnvkm_vm_map(vma, *mem);\r\nreturn 0;\r\n}\r\nvoid\r\nnvkm_gpuobj_unmap(struct nvkm_vma *vma)\r\n{\r\nif (vma->node) {\r\nnvkm_vm_unmap(vma);\r\nnvkm_vm_put(vma);\r\n}\r\n}\r\nstatic void\r\nnvkm_gpudup_dtor(struct nvkm_object *object)\r\n{\r\nstruct nvkm_gpuobj *gpuobj = (void *)object;\r\nnvkm_object_ref(NULL, &gpuobj->parent);\r\nnvkm_object_destroy(&gpuobj->object);\r\n}\r\nint\r\nnvkm_gpuobj_dup(struct nvkm_object *parent, struct nvkm_gpuobj *base,\r\nstruct nvkm_gpuobj **pgpuobj)\r\n{\r\nstruct nvkm_gpuobj *gpuobj;\r\nint ret;\r\nret = nvkm_object_create(parent, &parent->engine->subdev.object,\r\n&nvkm_gpudup_oclass, 0, &gpuobj);\r\n*pgpuobj = gpuobj;\r\nif (ret)\r\nreturn ret;\r\nnvkm_object_ref(nv_object(base), &gpuobj->parent);\r\ngpuobj->addr = base->addr;\r\ngpuobj->size = base->size;\r\nreturn 0;\r\n}
