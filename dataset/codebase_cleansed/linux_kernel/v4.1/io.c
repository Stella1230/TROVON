static unsigned bch_bio_max_sectors(struct bio *bio)\r\n{\r\nstruct request_queue *q = bdev_get_queue(bio->bi_bdev);\r\nstruct bio_vec bv;\r\nstruct bvec_iter iter;\r\nunsigned ret = 0, seg = 0;\r\nif (bio->bi_rw & REQ_DISCARD)\r\nreturn min(bio_sectors(bio), q->limits.max_discard_sectors);\r\nbio_for_each_segment(bv, bio, iter) {\r\nstruct bvec_merge_data bvm = {\r\n.bi_bdev = bio->bi_bdev,\r\n.bi_sector = bio->bi_iter.bi_sector,\r\n.bi_size = ret << 9,\r\n.bi_rw = bio->bi_rw,\r\n};\r\nif (seg == min_t(unsigned, BIO_MAX_PAGES,\r\nqueue_max_segments(q)))\r\nbreak;\r\nif (q->merge_bvec_fn &&\r\nq->merge_bvec_fn(q, &bvm, &bv) < (int) bv.bv_len)\r\nbreak;\r\nseg++;\r\nret += bv.bv_len >> 9;\r\n}\r\nret = min(ret, queue_max_sectors(q));\r\nWARN_ON(!ret);\r\nret = max_t(int, ret, bio_iovec(bio).bv_len >> 9);\r\nreturn ret;\r\n}\r\nstatic void bch_bio_submit_split_done(struct closure *cl)\r\n{\r\nstruct bio_split_hook *s = container_of(cl, struct bio_split_hook, cl);\r\ns->bio->bi_end_io = s->bi_end_io;\r\ns->bio->bi_private = s->bi_private;\r\nbio_endio_nodec(s->bio, 0);\r\nclosure_debug_destroy(&s->cl);\r\nmempool_free(s, s->p->bio_split_hook);\r\n}\r\nstatic void bch_bio_submit_split_endio(struct bio *bio, int error)\r\n{\r\nstruct closure *cl = bio->bi_private;\r\nstruct bio_split_hook *s = container_of(cl, struct bio_split_hook, cl);\r\nif (error)\r\nclear_bit(BIO_UPTODATE, &s->bio->bi_flags);\r\nbio_put(bio);\r\nclosure_put(cl);\r\n}\r\nvoid bch_generic_make_request(struct bio *bio, struct bio_split_pool *p)\r\n{\r\nstruct bio_split_hook *s;\r\nstruct bio *n;\r\nif (!bio_has_data(bio) && !(bio->bi_rw & REQ_DISCARD))\r\ngoto submit;\r\nif (bio_sectors(bio) <= bch_bio_max_sectors(bio))\r\ngoto submit;\r\ns = mempool_alloc(p->bio_split_hook, GFP_NOIO);\r\nclosure_init(&s->cl, NULL);\r\ns->bio = bio;\r\ns->p = p;\r\ns->bi_end_io = bio->bi_end_io;\r\ns->bi_private = bio->bi_private;\r\nbio_get(bio);\r\ndo {\r\nn = bio_next_split(bio, bch_bio_max_sectors(bio),\r\nGFP_NOIO, s->p->bio_split);\r\nn->bi_end_io = bch_bio_submit_split_endio;\r\nn->bi_private = &s->cl;\r\nclosure_get(&s->cl);\r\ngeneric_make_request(n);\r\n} while (n != bio);\r\ncontinue_at(&s->cl, bch_bio_submit_split_done, NULL);\r\nsubmit:\r\ngeneric_make_request(bio);\r\n}\r\nvoid bch_bbio_free(struct bio *bio, struct cache_set *c)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nmempool_free(b, c->bio_meta);\r\n}\r\nstruct bio *bch_bbio_alloc(struct cache_set *c)\r\n{\r\nstruct bbio *b = mempool_alloc(c->bio_meta, GFP_NOIO);\r\nstruct bio *bio = &b->bio;\r\nbio_init(bio);\r\nbio->bi_flags |= BIO_POOL_NONE << BIO_POOL_OFFSET;\r\nbio->bi_max_vecs = bucket_pages(c);\r\nbio->bi_io_vec = bio->bi_inline_vecs;\r\nreturn bio;\r\n}\r\nvoid __bch_submit_bbio(struct bio *bio, struct cache_set *c)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nbio->bi_iter.bi_sector = PTR_OFFSET(&b->key, 0);\r\nbio->bi_bdev = PTR_CACHE(c, &b->key, 0)->bdev;\r\nb->submit_time_us = local_clock_us();\r\nclosure_bio_submit(bio, bio->bi_private, PTR_CACHE(c, &b->key, 0));\r\n}\r\nvoid bch_submit_bbio(struct bio *bio, struct cache_set *c,\r\nstruct bkey *k, unsigned ptr)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nbch_bkey_copy_single_ptr(&b->key, k, ptr);\r\n__bch_submit_bbio(bio, c);\r\n}\r\nvoid bch_count_io_errors(struct cache *ca, int error, const char *m)\r\n{\r\nif (ca->set->error_decay) {\r\nunsigned count = atomic_inc_return(&ca->io_count);\r\nwhile (count > ca->set->error_decay) {\r\nunsigned errors;\r\nunsigned old = count;\r\nunsigned new = count - ca->set->error_decay;\r\ncount = atomic_cmpxchg(&ca->io_count, old, new);\r\nif (count == old) {\r\ncount = new;\r\nerrors = atomic_read(&ca->io_errors);\r\ndo {\r\nold = errors;\r\nnew = ((uint64_t) errors * 127) / 128;\r\nerrors = atomic_cmpxchg(&ca->io_errors,\r\nold, new);\r\n} while (old != errors);\r\n}\r\n}\r\n}\r\nif (error) {\r\nchar buf[BDEVNAME_SIZE];\r\nunsigned errors = atomic_add_return(1 << IO_ERROR_SHIFT,\r\n&ca->io_errors);\r\nerrors >>= IO_ERROR_SHIFT;\r\nif (errors < ca->set->error_limit)\r\npr_err("%s: IO error on %s, recovering",\r\nbdevname(ca->bdev, buf), m);\r\nelse\r\nbch_cache_set_error(ca->set,\r\n"%s: too many IO errors %s",\r\nbdevname(ca->bdev, buf), m);\r\n}\r\n}\r\nvoid bch_bbio_count_io_errors(struct cache_set *c, struct bio *bio,\r\nint error, const char *m)\r\n{\r\nstruct bbio *b = container_of(bio, struct bbio, bio);\r\nstruct cache *ca = PTR_CACHE(c, &b->key, 0);\r\nunsigned threshold = bio->bi_rw & REQ_WRITE\r\n? c->congested_write_threshold_us\r\n: c->congested_read_threshold_us;\r\nif (threshold) {\r\nunsigned t = local_clock_us();\r\nint us = t - b->submit_time_us;\r\nint congested = atomic_read(&c->congested);\r\nif (us > (int) threshold) {\r\nint ms = us / 1024;\r\nc->congested_last_us = t;\r\nms = min(ms, CONGESTED_MAX + congested);\r\natomic_sub(ms, &c->congested);\r\n} else if (congested < 0)\r\natomic_inc(&c->congested);\r\n}\r\nbch_count_io_errors(ca, error, m);\r\n}\r\nvoid bch_bbio_endio(struct cache_set *c, struct bio *bio,\r\nint error, const char *m)\r\n{\r\nstruct closure *cl = bio->bi_private;\r\nbch_bbio_count_io_errors(c, bio, error, m);\r\nbio_put(bio);\r\nclosure_put(cl);\r\n}
