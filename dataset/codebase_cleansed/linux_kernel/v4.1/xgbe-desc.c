static void xgbe_free_ring(struct xgbe_prv_data *pdata,\r\nstruct xgbe_ring *ring)\r\n{\r\nstruct xgbe_ring_data *rdata;\r\nunsigned int i;\r\nif (!ring)\r\nreturn;\r\nif (ring->rdata) {\r\nfor (i = 0; i < ring->rdesc_count; i++) {\r\nrdata = XGBE_GET_DESC_DATA(ring, i);\r\nxgbe_unmap_rdata(pdata, rdata);\r\n}\r\nkfree(ring->rdata);\r\nring->rdata = NULL;\r\n}\r\nif (ring->rx_hdr_pa.pages) {\r\ndma_unmap_page(pdata->dev, ring->rx_hdr_pa.pages_dma,\r\nring->rx_hdr_pa.pages_len, DMA_FROM_DEVICE);\r\nput_page(ring->rx_hdr_pa.pages);\r\nring->rx_hdr_pa.pages = NULL;\r\nring->rx_hdr_pa.pages_len = 0;\r\nring->rx_hdr_pa.pages_offset = 0;\r\nring->rx_hdr_pa.pages_dma = 0;\r\n}\r\nif (ring->rx_buf_pa.pages) {\r\ndma_unmap_page(pdata->dev, ring->rx_buf_pa.pages_dma,\r\nring->rx_buf_pa.pages_len, DMA_FROM_DEVICE);\r\nput_page(ring->rx_buf_pa.pages);\r\nring->rx_buf_pa.pages = NULL;\r\nring->rx_buf_pa.pages_len = 0;\r\nring->rx_buf_pa.pages_offset = 0;\r\nring->rx_buf_pa.pages_dma = 0;\r\n}\r\nif (ring->rdesc) {\r\ndma_free_coherent(pdata->dev,\r\n(sizeof(struct xgbe_ring_desc) *\r\nring->rdesc_count),\r\nring->rdesc, ring->rdesc_dma);\r\nring->rdesc = NULL;\r\n}\r\n}\r\nstatic void xgbe_free_ring_resources(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\nDBGPR("-->xgbe_free_ring_resources\n");\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nxgbe_free_ring(pdata, channel->tx_ring);\r\nxgbe_free_ring(pdata, channel->rx_ring);\r\n}\r\nDBGPR("<--xgbe_free_ring_resources\n");\r\n}\r\nstatic int xgbe_init_ring(struct xgbe_prv_data *pdata,\r\nstruct xgbe_ring *ring, unsigned int rdesc_count)\r\n{\r\nDBGPR("-->xgbe_init_ring\n");\r\nif (!ring)\r\nreturn 0;\r\nring->rdesc_count = rdesc_count;\r\nring->rdesc = dma_alloc_coherent(pdata->dev,\r\n(sizeof(struct xgbe_ring_desc) *\r\nrdesc_count), &ring->rdesc_dma,\r\nGFP_KERNEL);\r\nif (!ring->rdesc)\r\nreturn -ENOMEM;\r\nring->rdata = kcalloc(rdesc_count, sizeof(struct xgbe_ring_data),\r\nGFP_KERNEL);\r\nif (!ring->rdata)\r\nreturn -ENOMEM;\r\nDBGPR(" rdesc=0x%p, rdesc_dma=0x%llx, rdata=0x%p\n",\r\nring->rdesc, ring->rdesc_dma, ring->rdata);\r\nDBGPR("<--xgbe_init_ring\n");\r\nreturn 0;\r\n}\r\nstatic int xgbe_alloc_ring_resources(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_channel *channel;\r\nunsigned int i;\r\nint ret;\r\nDBGPR("-->xgbe_alloc_ring_resources\n");\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nDBGPR(" %s - tx_ring:\n", channel->name);\r\nret = xgbe_init_ring(pdata, channel->tx_ring,\r\npdata->tx_desc_count);\r\nif (ret) {\r\nnetdev_alert(pdata->netdev,\r\n"error initializing Tx ring\n");\r\ngoto err_ring;\r\n}\r\nDBGPR(" %s - rx_ring:\n", channel->name);\r\nret = xgbe_init_ring(pdata, channel->rx_ring,\r\npdata->rx_desc_count);\r\nif (ret) {\r\nnetdev_alert(pdata->netdev,\r\n"error initializing Tx ring\n");\r\ngoto err_ring;\r\n}\r\n}\r\nDBGPR("<--xgbe_alloc_ring_resources\n");\r\nreturn 0;\r\nerr_ring:\r\nxgbe_free_ring_resources(pdata);\r\nreturn ret;\r\n}\r\nstatic int xgbe_alloc_pages(struct xgbe_prv_data *pdata,\r\nstruct xgbe_page_alloc *pa, gfp_t gfp, int order)\r\n{\r\nstruct page *pages = NULL;\r\ndma_addr_t pages_dma;\r\nint ret;\r\ngfp |= __GFP_COLD | __GFP_COMP;\r\nwhile (order >= 0) {\r\npages = alloc_pages(gfp, order);\r\nif (pages)\r\nbreak;\r\norder--;\r\n}\r\nif (!pages)\r\nreturn -ENOMEM;\r\npages_dma = dma_map_page(pdata->dev, pages, 0,\r\nPAGE_SIZE << order, DMA_FROM_DEVICE);\r\nret = dma_mapping_error(pdata->dev, pages_dma);\r\nif (ret) {\r\nput_page(pages);\r\nreturn ret;\r\n}\r\npa->pages = pages;\r\npa->pages_len = PAGE_SIZE << order;\r\npa->pages_offset = 0;\r\npa->pages_dma = pages_dma;\r\nreturn 0;\r\n}\r\nstatic void xgbe_set_buffer_data(struct xgbe_buffer_data *bd,\r\nstruct xgbe_page_alloc *pa,\r\nunsigned int len)\r\n{\r\nget_page(pa->pages);\r\nbd->pa = *pa;\r\nbd->dma = pa->pages_dma + pa->pages_offset;\r\nbd->dma_len = len;\r\npa->pages_offset += len;\r\nif ((pa->pages_offset + len) > pa->pages_len) {\r\nbd->pa_unmap = *pa;\r\npa->pages = NULL;\r\npa->pages_len = 0;\r\npa->pages_offset = 0;\r\npa->pages_dma = 0;\r\n}\r\n}\r\nstatic int xgbe_map_rx_buffer(struct xgbe_prv_data *pdata,\r\nstruct xgbe_ring *ring,\r\nstruct xgbe_ring_data *rdata)\r\n{\r\nint order, ret;\r\nif (!ring->rx_hdr_pa.pages) {\r\nret = xgbe_alloc_pages(pdata, &ring->rx_hdr_pa, GFP_ATOMIC, 0);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (!ring->rx_buf_pa.pages) {\r\norder = max_t(int, PAGE_ALLOC_COSTLY_ORDER - 1, 0);\r\nret = xgbe_alloc_pages(pdata, &ring->rx_buf_pa, GFP_ATOMIC,\r\norder);\r\nif (ret)\r\nreturn ret;\r\n}\r\nxgbe_set_buffer_data(&rdata->rx.hdr, &ring->rx_hdr_pa,\r\nXGBE_SKB_ALLOC_SIZE);\r\nxgbe_set_buffer_data(&rdata->rx.buf, &ring->rx_buf_pa,\r\npdata->rx_buf_size);\r\nreturn 0;\r\n}\r\nstatic void xgbe_wrapper_tx_descriptor_init(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_channel *channel;\r\nstruct xgbe_ring *ring;\r\nstruct xgbe_ring_data *rdata;\r\nstruct xgbe_ring_desc *rdesc;\r\ndma_addr_t rdesc_dma;\r\nunsigned int i, j;\r\nDBGPR("-->xgbe_wrapper_tx_descriptor_init\n");\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nring = channel->tx_ring;\r\nif (!ring)\r\nbreak;\r\nrdesc = ring->rdesc;\r\nrdesc_dma = ring->rdesc_dma;\r\nfor (j = 0; j < ring->rdesc_count; j++) {\r\nrdata = XGBE_GET_DESC_DATA(ring, j);\r\nrdata->rdesc = rdesc;\r\nrdata->rdesc_dma = rdesc_dma;\r\nrdesc++;\r\nrdesc_dma += sizeof(struct xgbe_ring_desc);\r\n}\r\nring->cur = 0;\r\nring->dirty = 0;\r\nmemset(&ring->tx, 0, sizeof(ring->tx));\r\nhw_if->tx_desc_init(channel);\r\n}\r\nDBGPR("<--xgbe_wrapper_tx_descriptor_init\n");\r\n}\r\nstatic void xgbe_wrapper_rx_descriptor_init(struct xgbe_prv_data *pdata)\r\n{\r\nstruct xgbe_hw_if *hw_if = &pdata->hw_if;\r\nstruct xgbe_channel *channel;\r\nstruct xgbe_ring *ring;\r\nstruct xgbe_ring_desc *rdesc;\r\nstruct xgbe_ring_data *rdata;\r\ndma_addr_t rdesc_dma;\r\nunsigned int i, j;\r\nDBGPR("-->xgbe_wrapper_rx_descriptor_init\n");\r\nchannel = pdata->channel;\r\nfor (i = 0; i < pdata->channel_count; i++, channel++) {\r\nring = channel->rx_ring;\r\nif (!ring)\r\nbreak;\r\nrdesc = ring->rdesc;\r\nrdesc_dma = ring->rdesc_dma;\r\nfor (j = 0; j < ring->rdesc_count; j++) {\r\nrdata = XGBE_GET_DESC_DATA(ring, j);\r\nrdata->rdesc = rdesc;\r\nrdata->rdesc_dma = rdesc_dma;\r\nif (xgbe_map_rx_buffer(pdata, ring, rdata))\r\nbreak;\r\nrdesc++;\r\nrdesc_dma += sizeof(struct xgbe_ring_desc);\r\n}\r\nring->cur = 0;\r\nring->dirty = 0;\r\nhw_if->rx_desc_init(channel);\r\n}\r\nDBGPR("<--xgbe_wrapper_rx_descriptor_init\n");\r\n}\r\nstatic void xgbe_unmap_rdata(struct xgbe_prv_data *pdata,\r\nstruct xgbe_ring_data *rdata)\r\n{\r\nif (rdata->skb_dma) {\r\nif (rdata->mapped_as_page) {\r\ndma_unmap_page(pdata->dev, rdata->skb_dma,\r\nrdata->skb_dma_len, DMA_TO_DEVICE);\r\n} else {\r\ndma_unmap_single(pdata->dev, rdata->skb_dma,\r\nrdata->skb_dma_len, DMA_TO_DEVICE);\r\n}\r\nrdata->skb_dma = 0;\r\nrdata->skb_dma_len = 0;\r\n}\r\nif (rdata->skb) {\r\ndev_kfree_skb_any(rdata->skb);\r\nrdata->skb = NULL;\r\n}\r\nif (rdata->rx.hdr.pa.pages)\r\nput_page(rdata->rx.hdr.pa.pages);\r\nif (rdata->rx.hdr.pa_unmap.pages) {\r\ndma_unmap_page(pdata->dev, rdata->rx.hdr.pa_unmap.pages_dma,\r\nrdata->rx.hdr.pa_unmap.pages_len,\r\nDMA_FROM_DEVICE);\r\nput_page(rdata->rx.hdr.pa_unmap.pages);\r\n}\r\nif (rdata->rx.buf.pa.pages)\r\nput_page(rdata->rx.buf.pa.pages);\r\nif (rdata->rx.buf.pa_unmap.pages) {\r\ndma_unmap_page(pdata->dev, rdata->rx.buf.pa_unmap.pages_dma,\r\nrdata->rx.buf.pa_unmap.pages_len,\r\nDMA_FROM_DEVICE);\r\nput_page(rdata->rx.buf.pa_unmap.pages);\r\n}\r\nmemset(&rdata->tx, 0, sizeof(rdata->tx));\r\nmemset(&rdata->rx, 0, sizeof(rdata->rx));\r\nrdata->mapped_as_page = 0;\r\nif (rdata->state_saved) {\r\nrdata->state_saved = 0;\r\nrdata->state.incomplete = 0;\r\nrdata->state.context_next = 0;\r\nrdata->state.skb = NULL;\r\nrdata->state.len = 0;\r\nrdata->state.error = 0;\r\n}\r\n}\r\nstatic int xgbe_map_tx_skb(struct xgbe_channel *channel, struct sk_buff *skb)\r\n{\r\nstruct xgbe_prv_data *pdata = channel->pdata;\r\nstruct xgbe_ring *ring = channel->tx_ring;\r\nstruct xgbe_ring_data *rdata;\r\nstruct xgbe_packet_data *packet;\r\nstruct skb_frag_struct *frag;\r\ndma_addr_t skb_dma;\r\nunsigned int start_index, cur_index;\r\nunsigned int offset, tso, vlan, datalen, len;\r\nunsigned int i;\r\nDBGPR("-->xgbe_map_tx_skb: cur = %d\n", ring->cur);\r\noffset = 0;\r\nstart_index = ring->cur;\r\ncur_index = ring->cur;\r\npacket = &ring->packet_data;\r\npacket->rdesc_count = 0;\r\npacket->length = 0;\r\ntso = XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nTSO_ENABLE);\r\nvlan = XGMAC_GET_BITS(packet->attributes, TX_PACKET_ATTRIBUTES,\r\nVLAN_CTAG);\r\nif ((tso && (packet->mss != ring->tx.cur_mss)) ||\r\n(vlan && (packet->vlan_ctag != ring->tx.cur_vlan_ctag)))\r\ncur_index++;\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index);\r\nif (tso) {\r\nDBGPR(" TSO packet\n");\r\nskb_dma = dma_map_single(pdata->dev, skb->data,\r\npacket->header_len, DMA_TO_DEVICE);\r\nif (dma_mapping_error(pdata->dev, skb_dma)) {\r\nnetdev_alert(pdata->netdev, "dma_map_single failed\n");\r\ngoto err_out;\r\n}\r\nrdata->skb_dma = skb_dma;\r\nrdata->skb_dma_len = packet->header_len;\r\noffset = packet->header_len;\r\npacket->length += packet->header_len;\r\ncur_index++;\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index);\r\n}\r\nfor (datalen = skb_headlen(skb) - offset; datalen; ) {\r\nlen = min_t(unsigned int, datalen, XGBE_TX_MAX_BUF_SIZE);\r\nskb_dma = dma_map_single(pdata->dev, skb->data + offset, len,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(pdata->dev, skb_dma)) {\r\nnetdev_alert(pdata->netdev, "dma_map_single failed\n");\r\ngoto err_out;\r\n}\r\nrdata->skb_dma = skb_dma;\r\nrdata->skb_dma_len = len;\r\nDBGPR(" skb data: index=%u, dma=0x%llx, len=%u\n",\r\ncur_index, skb_dma, len);\r\ndatalen -= len;\r\noffset += len;\r\npacket->length += len;\r\ncur_index++;\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index);\r\n}\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nDBGPR(" mapping frag %u\n", i);\r\nfrag = &skb_shinfo(skb)->frags[i];\r\noffset = 0;\r\nfor (datalen = skb_frag_size(frag); datalen; ) {\r\nlen = min_t(unsigned int, datalen,\r\nXGBE_TX_MAX_BUF_SIZE);\r\nskb_dma = skb_frag_dma_map(pdata->dev, frag, offset,\r\nlen, DMA_TO_DEVICE);\r\nif (dma_mapping_error(pdata->dev, skb_dma)) {\r\nnetdev_alert(pdata->netdev,\r\n"skb_frag_dma_map failed\n");\r\ngoto err_out;\r\n}\r\nrdata->skb_dma = skb_dma;\r\nrdata->skb_dma_len = len;\r\nrdata->mapped_as_page = 1;\r\nDBGPR(" skb data: index=%u, dma=0x%llx, len=%u\n",\r\ncur_index, skb_dma, len);\r\ndatalen -= len;\r\noffset += len;\r\npacket->length += len;\r\ncur_index++;\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index);\r\n}\r\n}\r\nrdata = XGBE_GET_DESC_DATA(ring, cur_index - 1);\r\nrdata->skb = skb;\r\npacket->rdesc_count = cur_index - start_index;\r\nDBGPR("<--xgbe_map_tx_skb: count=%u\n", packet->rdesc_count);\r\nreturn packet->rdesc_count;\r\nerr_out:\r\nwhile (start_index < cur_index) {\r\nrdata = XGBE_GET_DESC_DATA(ring, start_index++);\r\nxgbe_unmap_rdata(pdata, rdata);\r\n}\r\nDBGPR("<--xgbe_map_tx_skb: count=0\n");\r\nreturn 0;\r\n}\r\nvoid xgbe_init_function_ptrs_desc(struct xgbe_desc_if *desc_if)\r\n{\r\nDBGPR("-->xgbe_init_function_ptrs_desc\n");\r\ndesc_if->alloc_ring_resources = xgbe_alloc_ring_resources;\r\ndesc_if->free_ring_resources = xgbe_free_ring_resources;\r\ndesc_if->map_tx_skb = xgbe_map_tx_skb;\r\ndesc_if->map_rx_buffer = xgbe_map_rx_buffer;\r\ndesc_if->unmap_rdata = xgbe_unmap_rdata;\r\ndesc_if->wrapper_tx_desc_init = xgbe_wrapper_tx_descriptor_init;\r\ndesc_if->wrapper_rx_desc_init = xgbe_wrapper_rx_descriptor_init;\r\nDBGPR("<--xgbe_init_function_ptrs_desc\n");\r\n}
