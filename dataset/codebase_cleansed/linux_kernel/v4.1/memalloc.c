void *snd_malloc_pages(size_t size, gfp_t gfp_flags)\r\n{\r\nint pg;\r\nif (WARN_ON(!size))\r\nreturn NULL;\r\nif (WARN_ON(!gfp_flags))\r\nreturn NULL;\r\ngfp_flags |= __GFP_COMP;\r\npg = get_order(size);\r\nreturn (void *) __get_free_pages(gfp_flags, pg);\r\n}\r\nvoid snd_free_pages(void *ptr, size_t size)\r\n{\r\nint pg;\r\nif (ptr == NULL)\r\nreturn;\r\npg = get_order(size);\r\nfree_pages((unsigned long) ptr, pg);\r\n}\r\nstatic void *snd_malloc_dev_pages(struct device *dev, size_t size, dma_addr_t *dma)\r\n{\r\nint pg;\r\ngfp_t gfp_flags;\r\nif (WARN_ON(!dma))\r\nreturn NULL;\r\npg = get_order(size);\r\ngfp_flags = GFP_KERNEL\r\n| __GFP_COMP\r\n| __GFP_NORETRY\r\n| __GFP_NOWARN;\r\nreturn dma_alloc_coherent(dev, PAGE_SIZE << pg, dma, gfp_flags);\r\n}\r\nstatic void snd_free_dev_pages(struct device *dev, size_t size, void *ptr,\r\ndma_addr_t dma)\r\n{\r\nint pg;\r\nif (ptr == NULL)\r\nreturn;\r\npg = get_order(size);\r\ndma_free_coherent(dev, PAGE_SIZE << pg, ptr, dma);\r\n}\r\nstatic void snd_malloc_dev_iram(struct snd_dma_buffer *dmab, size_t size)\r\n{\r\nstruct device *dev = dmab->dev.dev;\r\nstruct gen_pool *pool = NULL;\r\ndmab->area = NULL;\r\ndmab->addr = 0;\r\nif (dev->of_node)\r\npool = of_get_named_gen_pool(dev->of_node, "iram", 0);\r\nif (!pool)\r\nreturn;\r\ndmab->private_data = pool;\r\ndmab->area = gen_pool_dma_alloc(pool, size, &dmab->addr);\r\n}\r\nstatic void snd_free_dev_iram(struct snd_dma_buffer *dmab)\r\n{\r\nstruct gen_pool *pool = dmab->private_data;\r\nif (pool && dmab->area)\r\ngen_pool_free(pool, (unsigned long)dmab->area, dmab->bytes);\r\n}\r\nint snd_dma_alloc_pages(int type, struct device *device, size_t size,\r\nstruct snd_dma_buffer *dmab)\r\n{\r\nif (WARN_ON(!size))\r\nreturn -ENXIO;\r\nif (WARN_ON(!dmab))\r\nreturn -ENXIO;\r\ndmab->dev.type = type;\r\ndmab->dev.dev = device;\r\ndmab->bytes = 0;\r\nswitch (type) {\r\ncase SNDRV_DMA_TYPE_CONTINUOUS:\r\ndmab->area = snd_malloc_pages(size,\r\n(__force gfp_t)(unsigned long)device);\r\ndmab->addr = 0;\r\nbreak;\r\n#ifdef CONFIG_HAS_DMA\r\n#ifdef CONFIG_GENERIC_ALLOCATOR\r\ncase SNDRV_DMA_TYPE_DEV_IRAM:\r\nsnd_malloc_dev_iram(dmab, size);\r\nif (dmab->area)\r\nbreak;\r\ndmab->dev.type = SNDRV_DMA_TYPE_DEV;\r\n#endif\r\ncase SNDRV_DMA_TYPE_DEV:\r\ndmab->area = snd_malloc_dev_pages(device, size, &dmab->addr);\r\nbreak;\r\n#endif\r\n#ifdef CONFIG_SND_DMA_SGBUF\r\ncase SNDRV_DMA_TYPE_DEV_SG:\r\nsnd_malloc_sgbuf_pages(device, size, dmab, NULL);\r\nbreak;\r\n#endif\r\ndefault:\r\npr_err("snd-malloc: invalid device type %d\n", type);\r\ndmab->area = NULL;\r\ndmab->addr = 0;\r\nreturn -ENXIO;\r\n}\r\nif (! dmab->area)\r\nreturn -ENOMEM;\r\ndmab->bytes = size;\r\nreturn 0;\r\n}\r\nint snd_dma_alloc_pages_fallback(int type, struct device *device, size_t size,\r\nstruct snd_dma_buffer *dmab)\r\n{\r\nint err;\r\nwhile ((err = snd_dma_alloc_pages(type, device, size, dmab)) < 0) {\r\nsize_t aligned_size;\r\nif (err != -ENOMEM)\r\nreturn err;\r\nif (size <= PAGE_SIZE)\r\nreturn -ENOMEM;\r\naligned_size = PAGE_SIZE << get_order(size);\r\nif (size != aligned_size)\r\nsize = aligned_size;\r\nelse\r\nsize >>= 1;\r\n}\r\nif (! dmab->area)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid snd_dma_free_pages(struct snd_dma_buffer *dmab)\r\n{\r\nswitch (dmab->dev.type) {\r\ncase SNDRV_DMA_TYPE_CONTINUOUS:\r\nsnd_free_pages(dmab->area, dmab->bytes);\r\nbreak;\r\n#ifdef CONFIG_HAS_DMA\r\n#ifdef CONFIG_GENERIC_ALLOCATOR\r\ncase SNDRV_DMA_TYPE_DEV_IRAM:\r\nsnd_free_dev_iram(dmab);\r\nbreak;\r\n#endif\r\ncase SNDRV_DMA_TYPE_DEV:\r\nsnd_free_dev_pages(dmab->dev.dev, dmab->bytes, dmab->area, dmab->addr);\r\nbreak;\r\n#endif\r\n#ifdef CONFIG_SND_DMA_SGBUF\r\ncase SNDRV_DMA_TYPE_DEV_SG:\r\nsnd_free_sgbuf_pages(dmab);\r\nbreak;\r\n#endif\r\ndefault:\r\npr_err("snd-malloc: invalid device type %d\n", dmab->dev.type);\r\n}\r\n}
