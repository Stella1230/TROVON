static void __init init_irq_default_affinity(void)\r\n{\r\nalloc_cpumask_var(&irq_default_affinity, GFP_NOWAIT);\r\ncpumask_setall(irq_default_affinity);\r\n}\r\nstatic void __init init_irq_default_affinity(void)\r\n{\r\n}\r\nstatic int alloc_masks(struct irq_desc *desc, gfp_t gfp, int node)\r\n{\r\nif (!zalloc_cpumask_var_node(&desc->irq_data.affinity, gfp, node))\r\nreturn -ENOMEM;\r\n#ifdef CONFIG_GENERIC_PENDING_IRQ\r\nif (!zalloc_cpumask_var_node(&desc->pending_mask, gfp, node)) {\r\nfree_cpumask_var(desc->irq_data.affinity);\r\nreturn -ENOMEM;\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void desc_smp_init(struct irq_desc *desc, int node)\r\n{\r\ndesc->irq_data.node = node;\r\ncpumask_copy(desc->irq_data.affinity, irq_default_affinity);\r\n#ifdef CONFIG_GENERIC_PENDING_IRQ\r\ncpumask_clear(desc->pending_mask);\r\n#endif\r\n}\r\nstatic inline int desc_node(struct irq_desc *desc)\r\n{\r\nreturn desc->irq_data.node;\r\n}\r\nstatic inline int\r\nalloc_masks(struct irq_desc *desc, gfp_t gfp, int node) { return 0; }\r\nstatic inline void desc_smp_init(struct irq_desc *desc, int node) { }\r\nstatic inline int desc_node(struct irq_desc *desc) { return 0; }\r\nstatic void desc_set_defaults(unsigned int irq, struct irq_desc *desc, int node,\r\nstruct module *owner)\r\n{\r\nint cpu;\r\ndesc->irq_data.irq = irq;\r\ndesc->irq_data.chip = &no_irq_chip;\r\ndesc->irq_data.chip_data = NULL;\r\ndesc->irq_data.handler_data = NULL;\r\ndesc->irq_data.msi_desc = NULL;\r\nirq_settings_clr_and_set(desc, ~0, _IRQ_DEFAULT_INIT_FLAGS);\r\nirqd_set(&desc->irq_data, IRQD_IRQ_DISABLED);\r\ndesc->handle_irq = handle_bad_irq;\r\ndesc->depth = 1;\r\ndesc->irq_count = 0;\r\ndesc->irqs_unhandled = 0;\r\ndesc->name = NULL;\r\ndesc->owner = owner;\r\nfor_each_possible_cpu(cpu)\r\n*per_cpu_ptr(desc->kstat_irqs, cpu) = 0;\r\ndesc_smp_init(desc, node);\r\n}\r\nstatic void irq_insert_desc(unsigned int irq, struct irq_desc *desc)\r\n{\r\nradix_tree_insert(&irq_desc_tree, irq, desc);\r\n}\r\nstruct irq_desc *irq_to_desc(unsigned int irq)\r\n{\r\nreturn radix_tree_lookup(&irq_desc_tree, irq);\r\n}\r\nstatic void delete_irq_desc(unsigned int irq)\r\n{\r\nradix_tree_delete(&irq_desc_tree, irq);\r\n}\r\nstatic void free_masks(struct irq_desc *desc)\r\n{\r\n#ifdef CONFIG_GENERIC_PENDING_IRQ\r\nfree_cpumask_var(desc->pending_mask);\r\n#endif\r\nfree_cpumask_var(desc->irq_data.affinity);\r\n}\r\nstatic inline void free_masks(struct irq_desc *desc) { }\r\nvoid irq_lock_sparse(void)\r\n{\r\nmutex_lock(&sparse_irq_lock);\r\n}\r\nvoid irq_unlock_sparse(void)\r\n{\r\nmutex_unlock(&sparse_irq_lock);\r\n}\r\nstatic struct irq_desc *alloc_desc(int irq, int node, struct module *owner)\r\n{\r\nstruct irq_desc *desc;\r\ngfp_t gfp = GFP_KERNEL;\r\ndesc = kzalloc_node(sizeof(*desc), gfp, node);\r\nif (!desc)\r\nreturn NULL;\r\ndesc->kstat_irqs = alloc_percpu(unsigned int);\r\nif (!desc->kstat_irqs)\r\ngoto err_desc;\r\nif (alloc_masks(desc, gfp, node))\r\ngoto err_kstat;\r\nraw_spin_lock_init(&desc->lock);\r\nlockdep_set_class(&desc->lock, &irq_desc_lock_class);\r\ndesc_set_defaults(irq, desc, node, owner);\r\nreturn desc;\r\nerr_kstat:\r\nfree_percpu(desc->kstat_irqs);\r\nerr_desc:\r\nkfree(desc);\r\nreturn NULL;\r\n}\r\nstatic void free_desc(unsigned int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nunregister_irq_proc(irq, desc);\r\nmutex_lock(&sparse_irq_lock);\r\ndelete_irq_desc(irq);\r\nmutex_unlock(&sparse_irq_lock);\r\nfree_masks(desc);\r\nfree_percpu(desc->kstat_irqs);\r\nkfree(desc);\r\n}\r\nstatic int alloc_descs(unsigned int start, unsigned int cnt, int node,\r\nstruct module *owner)\r\n{\r\nstruct irq_desc *desc;\r\nint i;\r\nfor (i = 0; i < cnt; i++) {\r\ndesc = alloc_desc(start + i, node, owner);\r\nif (!desc)\r\ngoto err;\r\nmutex_lock(&sparse_irq_lock);\r\nirq_insert_desc(start + i, desc);\r\nmutex_unlock(&sparse_irq_lock);\r\n}\r\nreturn start;\r\nerr:\r\nfor (i--; i >= 0; i--)\r\nfree_desc(start + i);\r\nmutex_lock(&sparse_irq_lock);\r\nbitmap_clear(allocated_irqs, start, cnt);\r\nmutex_unlock(&sparse_irq_lock);\r\nreturn -ENOMEM;\r\n}\r\nstatic int irq_expand_nr_irqs(unsigned int nr)\r\n{\r\nif (nr > IRQ_BITMAP_BITS)\r\nreturn -ENOMEM;\r\nnr_irqs = nr;\r\nreturn 0;\r\n}\r\nint __init early_irq_init(void)\r\n{\r\nint i, initcnt, node = first_online_node;\r\nstruct irq_desc *desc;\r\ninit_irq_default_affinity();\r\ninitcnt = arch_probe_nr_irqs();\r\nprintk(KERN_INFO "NR_IRQS:%d nr_irqs:%d %d\n", NR_IRQS, nr_irqs, initcnt);\r\nif (WARN_ON(nr_irqs > IRQ_BITMAP_BITS))\r\nnr_irqs = IRQ_BITMAP_BITS;\r\nif (WARN_ON(initcnt > IRQ_BITMAP_BITS))\r\ninitcnt = IRQ_BITMAP_BITS;\r\nif (initcnt > nr_irqs)\r\nnr_irqs = initcnt;\r\nfor (i = 0; i < initcnt; i++) {\r\ndesc = alloc_desc(i, node, NULL);\r\nset_bit(i, allocated_irqs);\r\nirq_insert_desc(i, desc);\r\n}\r\nreturn arch_early_irq_init();\r\n}\r\nint __init early_irq_init(void)\r\n{\r\nint count, i, node = first_online_node;\r\nstruct irq_desc *desc;\r\ninit_irq_default_affinity();\r\nprintk(KERN_INFO "NR_IRQS:%d\n", NR_IRQS);\r\ndesc = irq_desc;\r\ncount = ARRAY_SIZE(irq_desc);\r\nfor (i = 0; i < count; i++) {\r\ndesc[i].kstat_irqs = alloc_percpu(unsigned int);\r\nalloc_masks(&desc[i], GFP_KERNEL, node);\r\nraw_spin_lock_init(&desc[i].lock);\r\nlockdep_set_class(&desc[i].lock, &irq_desc_lock_class);\r\ndesc_set_defaults(i, &desc[i], node, NULL);\r\n}\r\nreturn arch_early_irq_init();\r\n}\r\nstruct irq_desc *irq_to_desc(unsigned int irq)\r\n{\r\nreturn (irq < NR_IRQS) ? irq_desc + irq : NULL;\r\n}\r\nstatic void free_desc(unsigned int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&desc->lock, flags);\r\ndesc_set_defaults(irq, desc, desc_node(desc), NULL);\r\nraw_spin_unlock_irqrestore(&desc->lock, flags);\r\n}\r\nstatic inline int alloc_descs(unsigned int start, unsigned int cnt, int node,\r\nstruct module *owner)\r\n{\r\nu32 i;\r\nfor (i = 0; i < cnt; i++) {\r\nstruct irq_desc *desc = irq_to_desc(start + i);\r\ndesc->owner = owner;\r\n}\r\nreturn start;\r\n}\r\nstatic int irq_expand_nr_irqs(unsigned int nr)\r\n{\r\nreturn -ENOMEM;\r\n}\r\nvoid irq_mark_irq(unsigned int irq)\r\n{\r\nmutex_lock(&sparse_irq_lock);\r\nbitmap_set(allocated_irqs, irq, 1);\r\nmutex_unlock(&sparse_irq_lock);\r\n}\r\nvoid irq_init_desc(unsigned int irq)\r\n{\r\nfree_desc(irq);\r\n}\r\nint generic_handle_irq(unsigned int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nif (!desc)\r\nreturn -EINVAL;\r\ngeneric_handle_irq_desc(irq, desc);\r\nreturn 0;\r\n}\r\nint __handle_domain_irq(struct irq_domain *domain, unsigned int hwirq,\r\nbool lookup, struct pt_regs *regs)\r\n{\r\nstruct pt_regs *old_regs = set_irq_regs(regs);\r\nunsigned int irq = hwirq;\r\nint ret = 0;\r\nirq_enter();\r\n#ifdef CONFIG_IRQ_DOMAIN\r\nif (lookup)\r\nirq = irq_find_mapping(domain, hwirq);\r\n#endif\r\nif (unlikely(!irq || irq >= nr_irqs)) {\r\nack_bad_irq(irq);\r\nret = -EINVAL;\r\n} else {\r\ngeneric_handle_irq(irq);\r\n}\r\nirq_exit();\r\nset_irq_regs(old_regs);\r\nreturn ret;\r\n}\r\nvoid irq_free_descs(unsigned int from, unsigned int cnt)\r\n{\r\nint i;\r\nif (from >= nr_irqs || (from + cnt) > nr_irqs)\r\nreturn;\r\nfor (i = 0; i < cnt; i++)\r\nfree_desc(from + i);\r\nmutex_lock(&sparse_irq_lock);\r\nbitmap_clear(allocated_irqs, from, cnt);\r\nmutex_unlock(&sparse_irq_lock);\r\n}\r\nint __ref\r\n__irq_alloc_descs(int irq, unsigned int from, unsigned int cnt, int node,\r\nstruct module *owner)\r\n{\r\nint start, ret;\r\nif (!cnt)\r\nreturn -EINVAL;\r\nif (irq >= 0) {\r\nif (from > irq)\r\nreturn -EINVAL;\r\nfrom = irq;\r\n} else {\r\nfrom = arch_dynirq_lower_bound(from);\r\n}\r\nmutex_lock(&sparse_irq_lock);\r\nstart = bitmap_find_next_zero_area(allocated_irqs, IRQ_BITMAP_BITS,\r\nfrom, cnt, 0);\r\nret = -EEXIST;\r\nif (irq >=0 && start != irq)\r\ngoto err;\r\nif (start + cnt > nr_irqs) {\r\nret = irq_expand_nr_irqs(start + cnt);\r\nif (ret)\r\ngoto err;\r\n}\r\nbitmap_set(allocated_irqs, start, cnt);\r\nmutex_unlock(&sparse_irq_lock);\r\nreturn alloc_descs(start, cnt, node, owner);\r\nerr:\r\nmutex_unlock(&sparse_irq_lock);\r\nreturn ret;\r\n}\r\nunsigned int irq_alloc_hwirqs(int cnt, int node)\r\n{\r\nint i, irq = __irq_alloc_descs(-1, 0, cnt, node, NULL);\r\nif (irq < 0)\r\nreturn 0;\r\nfor (i = irq; cnt > 0; i++, cnt--) {\r\nif (arch_setup_hwirq(i, node))\r\ngoto err;\r\nirq_clear_status_flags(i, _IRQ_NOREQUEST);\r\n}\r\nreturn irq;\r\nerr:\r\nfor (i--; i >= irq; i--) {\r\nirq_set_status_flags(i, _IRQ_NOREQUEST | _IRQ_NOPROBE);\r\narch_teardown_hwirq(i);\r\n}\r\nirq_free_descs(irq, cnt);\r\nreturn 0;\r\n}\r\nvoid irq_free_hwirqs(unsigned int from, int cnt)\r\n{\r\nint i, j;\r\nfor (i = from, j = cnt; j > 0; i++, j--) {\r\nirq_set_status_flags(i, _IRQ_NOREQUEST | _IRQ_NOPROBE);\r\narch_teardown_hwirq(i);\r\n}\r\nirq_free_descs(from, cnt);\r\n}\r\nunsigned int irq_get_next_irq(unsigned int offset)\r\n{\r\nreturn find_next_bit(allocated_irqs, nr_irqs, offset);\r\n}\r\nstruct irq_desc *\r\n__irq_get_desc_lock(unsigned int irq, unsigned long *flags, bool bus,\r\nunsigned int check)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nif (desc) {\r\nif (check & _IRQ_DESC_CHECK) {\r\nif ((check & _IRQ_DESC_PERCPU) &&\r\n!irq_settings_is_per_cpu_devid(desc))\r\nreturn NULL;\r\nif (!(check & _IRQ_DESC_PERCPU) &&\r\nirq_settings_is_per_cpu_devid(desc))\r\nreturn NULL;\r\n}\r\nif (bus)\r\nchip_bus_lock(desc);\r\nraw_spin_lock_irqsave(&desc->lock, *flags);\r\n}\r\nreturn desc;\r\n}\r\nvoid __irq_put_desc_unlock(struct irq_desc *desc, unsigned long flags, bool bus)\r\n{\r\nraw_spin_unlock_irqrestore(&desc->lock, flags);\r\nif (bus)\r\nchip_bus_sync_unlock(desc);\r\n}\r\nint irq_set_percpu_devid(unsigned int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nif (!desc)\r\nreturn -EINVAL;\r\nif (desc->percpu_enabled)\r\nreturn -EINVAL;\r\ndesc->percpu_enabled = kzalloc(sizeof(*desc->percpu_enabled), GFP_KERNEL);\r\nif (!desc->percpu_enabled)\r\nreturn -ENOMEM;\r\nirq_set_percpu_devid_flags(irq);\r\nreturn 0;\r\n}\r\nvoid kstat_incr_irq_this_cpu(unsigned int irq)\r\n{\r\nkstat_incr_irqs_this_cpu(irq, irq_to_desc(irq));\r\n}\r\nunsigned int kstat_irqs_cpu(unsigned int irq, int cpu)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nreturn desc && desc->kstat_irqs ?\r\n*per_cpu_ptr(desc->kstat_irqs, cpu) : 0;\r\n}\r\nunsigned int kstat_irqs(unsigned int irq)\r\n{\r\nstruct irq_desc *desc = irq_to_desc(irq);\r\nint cpu;\r\nint sum = 0;\r\nif (!desc || !desc->kstat_irqs)\r\nreturn 0;\r\nfor_each_possible_cpu(cpu)\r\nsum += *per_cpu_ptr(desc->kstat_irqs, cpu);\r\nreturn sum;\r\n}\r\nunsigned int kstat_irqs_usr(unsigned int irq)\r\n{\r\nint sum;\r\nirq_lock_sparse();\r\nsum = kstat_irqs(irq);\r\nirq_unlock_sparse();\r\nreturn sum;\r\n}
