void evergreen_dma_fence_ring_emit(struct radeon_device *rdev,\r\nstruct radeon_fence *fence)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[fence->ring];\r\nu64 addr = rdev->fence_drv[fence->ring].gpu_addr;\r\nradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_FENCE, 0, 0));\r\nradeon_ring_write(ring, addr & 0xfffffffc);\r\nradeon_ring_write(ring, (upper_32_bits(addr) & 0xff));\r\nradeon_ring_write(ring, fence->seq);\r\nradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_TRAP, 0, 0));\r\nradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_SRBM_WRITE, 0, 0));\r\nradeon_ring_write(ring, (0xf << 16) | (HDP_MEM_COHERENCY_FLUSH_CNTL >> 2));\r\nradeon_ring_write(ring, 1);\r\n}\r\nvoid evergreen_dma_ring_ib_execute(struct radeon_device *rdev,\r\nstruct radeon_ib *ib)\r\n{\r\nstruct radeon_ring *ring = &rdev->ring[ib->ring];\r\nif (rdev->wb.enabled) {\r\nu32 next_rptr = ring->wptr + 4;\r\nwhile ((next_rptr & 7) != 5)\r\nnext_rptr++;\r\nnext_rptr += 3;\r\nradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_WRITE, 0, 1));\r\nradeon_ring_write(ring, ring->next_rptr_gpu_addr & 0xfffffffc);\r\nradeon_ring_write(ring, upper_32_bits(ring->next_rptr_gpu_addr) & 0xff);\r\nradeon_ring_write(ring, next_rptr);\r\n}\r\nwhile ((ring->wptr & 7) != 5)\r\nradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_NOP, 0, 0));\r\nradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_INDIRECT_BUFFER, 0, 0));\r\nradeon_ring_write(ring, (ib->gpu_addr & 0xFFFFFFE0));\r\nradeon_ring_write(ring, (ib->length_dw << 12) | (upper_32_bits(ib->gpu_addr) & 0xFF));\r\n}\r\nstruct radeon_fence *evergreen_copy_dma(struct radeon_device *rdev,\r\nuint64_t src_offset,\r\nuint64_t dst_offset,\r\nunsigned num_gpu_pages,\r\nstruct reservation_object *resv)\r\n{\r\nstruct radeon_fence *fence;\r\nstruct radeon_sync sync;\r\nint ring_index = rdev->asic->copy.dma_ring_index;\r\nstruct radeon_ring *ring = &rdev->ring[ring_index];\r\nu32 size_in_dw, cur_size_in_dw;\r\nint i, num_loops;\r\nint r = 0;\r\nradeon_sync_create(&sync);\r\nsize_in_dw = (num_gpu_pages << RADEON_GPU_PAGE_SHIFT) / 4;\r\nnum_loops = DIV_ROUND_UP(size_in_dw, 0xfffff);\r\nr = radeon_ring_lock(rdev, ring, num_loops * 5 + 11);\r\nif (r) {\r\nDRM_ERROR("radeon: moving bo (%d).\n", r);\r\nradeon_sync_free(rdev, &sync, NULL);\r\nreturn ERR_PTR(r);\r\n}\r\nradeon_sync_resv(rdev, &sync, resv, false);\r\nradeon_sync_rings(rdev, &sync, ring->idx);\r\nfor (i = 0; i < num_loops; i++) {\r\ncur_size_in_dw = size_in_dw;\r\nif (cur_size_in_dw > 0xFFFFF)\r\ncur_size_in_dw = 0xFFFFF;\r\nsize_in_dw -= cur_size_in_dw;\r\nradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_COPY, 0, cur_size_in_dw));\r\nradeon_ring_write(ring, dst_offset & 0xfffffffc);\r\nradeon_ring_write(ring, src_offset & 0xfffffffc);\r\nradeon_ring_write(ring, upper_32_bits(dst_offset) & 0xff);\r\nradeon_ring_write(ring, upper_32_bits(src_offset) & 0xff);\r\nsrc_offset += cur_size_in_dw * 4;\r\ndst_offset += cur_size_in_dw * 4;\r\n}\r\nr = radeon_fence_emit(rdev, &fence, ring->idx);\r\nif (r) {\r\nradeon_ring_unlock_undo(rdev, ring);\r\nradeon_sync_free(rdev, &sync, NULL);\r\nreturn ERR_PTR(r);\r\n}\r\nradeon_ring_unlock_commit(rdev, ring, false);\r\nradeon_sync_free(rdev, &sync, fence);\r\nreturn fence;\r\n}\r\nbool evergreen_dma_is_lockup(struct radeon_device *rdev, struct radeon_ring *ring)\r\n{\r\nu32 reset_mask = evergreen_gpu_check_soft_reset(rdev);\r\nif (!(reset_mask & RADEON_RESET_DMA)) {\r\nradeon_ring_lockup_update(rdev, ring);\r\nreturn false;\r\n}\r\nreturn radeon_ring_test_lockup(rdev, ring);\r\n}
