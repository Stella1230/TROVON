static void nf_conntrack_double_unlock(unsigned int h1, unsigned int h2)\r\n{\r\nh1 %= CONNTRACK_LOCKS;\r\nh2 %= CONNTRACK_LOCKS;\r\nspin_unlock(&nf_conntrack_locks[h1]);\r\nif (h1 != h2)\r\nspin_unlock(&nf_conntrack_locks[h2]);\r\n}\r\nstatic bool nf_conntrack_double_lock(struct net *net, unsigned int h1,\r\nunsigned int h2, unsigned int sequence)\r\n{\r\nh1 %= CONNTRACK_LOCKS;\r\nh2 %= CONNTRACK_LOCKS;\r\nif (h1 <= h2) {\r\nspin_lock(&nf_conntrack_locks[h1]);\r\nif (h1 != h2)\r\nspin_lock_nested(&nf_conntrack_locks[h2],\r\nSINGLE_DEPTH_NESTING);\r\n} else {\r\nspin_lock(&nf_conntrack_locks[h2]);\r\nspin_lock_nested(&nf_conntrack_locks[h1],\r\nSINGLE_DEPTH_NESTING);\r\n}\r\nif (read_seqcount_retry(&net->ct.generation, sequence)) {\r\nnf_conntrack_double_unlock(h1, h2);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void nf_conntrack_all_lock(void)\r\n{\r\nint i;\r\nfor (i = 0; i < CONNTRACK_LOCKS; i++)\r\nspin_lock_nested(&nf_conntrack_locks[i], i);\r\n}\r\nstatic void nf_conntrack_all_unlock(void)\r\n{\r\nint i;\r\nfor (i = 0; i < CONNTRACK_LOCKS; i++)\r\nspin_unlock(&nf_conntrack_locks[i]);\r\n}\r\nstatic u32 hash_conntrack_raw(const struct nf_conntrack_tuple *tuple, u16 zone)\r\n{\r\nunsigned int n;\r\nn = (sizeof(tuple->src) + sizeof(tuple->dst.u3)) / sizeof(u32);\r\nreturn jhash2((u32 *)tuple, n, zone ^ nf_conntrack_hash_rnd ^\r\n(((__force __u16)tuple->dst.u.all << 16) |\r\ntuple->dst.protonum));\r\n}\r\nstatic u32 __hash_bucket(u32 hash, unsigned int size)\r\n{\r\nreturn reciprocal_scale(hash, size);\r\n}\r\nstatic u32 hash_bucket(u32 hash, const struct net *net)\r\n{\r\nreturn __hash_bucket(hash, net->ct.htable_size);\r\n}\r\nstatic u_int32_t __hash_conntrack(const struct nf_conntrack_tuple *tuple,\r\nu16 zone, unsigned int size)\r\n{\r\nreturn __hash_bucket(hash_conntrack_raw(tuple, zone), size);\r\n}\r\nstatic inline u_int32_t hash_conntrack(const struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *tuple)\r\n{\r\nreturn __hash_conntrack(tuple, zone, net->ct.htable_size);\r\n}\r\nbool\r\nnf_ct_get_tuple(const struct sk_buff *skb,\r\nunsigned int nhoff,\r\nunsigned int dataoff,\r\nu_int16_t l3num,\r\nu_int8_t protonum,\r\nstruct nf_conntrack_tuple *tuple,\r\nconst struct nf_conntrack_l3proto *l3proto,\r\nconst struct nf_conntrack_l4proto *l4proto)\r\n{\r\nmemset(tuple, 0, sizeof(*tuple));\r\ntuple->src.l3num = l3num;\r\nif (l3proto->pkt_to_tuple(skb, nhoff, tuple) == 0)\r\nreturn false;\r\ntuple->dst.protonum = protonum;\r\ntuple->dst.dir = IP_CT_DIR_ORIGINAL;\r\nreturn l4proto->pkt_to_tuple(skb, dataoff, tuple);\r\n}\r\nbool nf_ct_get_tuplepr(const struct sk_buff *skb, unsigned int nhoff,\r\nu_int16_t l3num, struct nf_conntrack_tuple *tuple)\r\n{\r\nstruct nf_conntrack_l3proto *l3proto;\r\nstruct nf_conntrack_l4proto *l4proto;\r\nunsigned int protoff;\r\nu_int8_t protonum;\r\nint ret;\r\nrcu_read_lock();\r\nl3proto = __nf_ct_l3proto_find(l3num);\r\nret = l3proto->get_l4proto(skb, nhoff, &protoff, &protonum);\r\nif (ret != NF_ACCEPT) {\r\nrcu_read_unlock();\r\nreturn false;\r\n}\r\nl4proto = __nf_ct_l4proto_find(l3num, protonum);\r\nret = nf_ct_get_tuple(skb, nhoff, protoff, l3num, protonum, tuple,\r\nl3proto, l4proto);\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nbool\r\nnf_ct_invert_tuple(struct nf_conntrack_tuple *inverse,\r\nconst struct nf_conntrack_tuple *orig,\r\nconst struct nf_conntrack_l3proto *l3proto,\r\nconst struct nf_conntrack_l4proto *l4proto)\r\n{\r\nmemset(inverse, 0, sizeof(*inverse));\r\ninverse->src.l3num = orig->src.l3num;\r\nif (l3proto->invert_tuple(inverse, orig) == 0)\r\nreturn false;\r\ninverse->dst.dir = !orig->dst.dir;\r\ninverse->dst.protonum = orig->dst.protonum;\r\nreturn l4proto->invert_tuple(inverse, orig);\r\n}\r\nstatic void\r\nclean_from_lists(struct nf_conn *ct)\r\n{\r\npr_debug("clean_from_lists(%p)\n", ct);\r\nhlist_nulls_del_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode);\r\nhlist_nulls_del_rcu(&ct->tuplehash[IP_CT_DIR_REPLY].hnnode);\r\nnf_ct_remove_expectations(ct);\r\n}\r\nstatic void nf_ct_add_to_dying_list(struct nf_conn *ct)\r\n{\r\nstruct ct_pcpu *pcpu;\r\nct->cpu = smp_processor_id();\r\npcpu = per_cpu_ptr(nf_ct_net(ct)->ct.pcpu_lists, ct->cpu);\r\nspin_lock(&pcpu->lock);\r\nhlist_nulls_add_head(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode,\r\n&pcpu->dying);\r\nspin_unlock(&pcpu->lock);\r\n}\r\nstatic void nf_ct_add_to_unconfirmed_list(struct nf_conn *ct)\r\n{\r\nstruct ct_pcpu *pcpu;\r\nct->cpu = smp_processor_id();\r\npcpu = per_cpu_ptr(nf_ct_net(ct)->ct.pcpu_lists, ct->cpu);\r\nspin_lock(&pcpu->lock);\r\nhlist_nulls_add_head(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode,\r\n&pcpu->unconfirmed);\r\nspin_unlock(&pcpu->lock);\r\n}\r\nstatic void nf_ct_del_from_dying_or_unconfirmed_list(struct nf_conn *ct)\r\n{\r\nstruct ct_pcpu *pcpu;\r\npcpu = per_cpu_ptr(nf_ct_net(ct)->ct.pcpu_lists, ct->cpu);\r\nspin_lock(&pcpu->lock);\r\nBUG_ON(hlist_nulls_unhashed(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode));\r\nhlist_nulls_del_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode);\r\nspin_unlock(&pcpu->lock);\r\n}\r\nstatic void\r\ndestroy_conntrack(struct nf_conntrack *nfct)\r\n{\r\nstruct nf_conn *ct = (struct nf_conn *)nfct;\r\nstruct net *net = nf_ct_net(ct);\r\nstruct nf_conntrack_l4proto *l4proto;\r\npr_debug("destroy_conntrack(%p)\n", ct);\r\nNF_CT_ASSERT(atomic_read(&nfct->use) == 0);\r\nNF_CT_ASSERT(!timer_pending(&ct->timeout));\r\nrcu_read_lock();\r\nl4proto = __nf_ct_l4proto_find(nf_ct_l3num(ct), nf_ct_protonum(ct));\r\nif (l4proto && l4proto->destroy)\r\nl4proto->destroy(ct);\r\nrcu_read_unlock();\r\nlocal_bh_disable();\r\nnf_ct_remove_expectations(ct);\r\nnf_ct_del_from_dying_or_unconfirmed_list(ct);\r\nNF_CT_STAT_INC(net, delete);\r\nlocal_bh_enable();\r\nif (ct->master)\r\nnf_ct_put(ct->master);\r\npr_debug("destroy_conntrack: returning ct=%p to slab\n", ct);\r\nnf_conntrack_free(ct);\r\n}\r\nstatic void nf_ct_delete_from_lists(struct nf_conn *ct)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nunsigned int hash, reply_hash;\r\nu16 zone = nf_ct_zone(ct);\r\nunsigned int sequence;\r\nnf_ct_helper_destroy(ct);\r\nlocal_bh_disable();\r\ndo {\r\nsequence = read_seqcount_begin(&net->ct.generation);\r\nhash = hash_conntrack(net, zone,\r\n&ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple);\r\nreply_hash = hash_conntrack(net, zone,\r\n&ct->tuplehash[IP_CT_DIR_REPLY].tuple);\r\n} while (nf_conntrack_double_lock(net, hash, reply_hash, sequence));\r\nclean_from_lists(ct);\r\nnf_conntrack_double_unlock(hash, reply_hash);\r\nnf_ct_add_to_dying_list(ct);\r\nNF_CT_STAT_INC(net, delete_list);\r\nlocal_bh_enable();\r\n}\r\nbool nf_ct_delete(struct nf_conn *ct, u32 portid, int report)\r\n{\r\nstruct nf_conn_tstamp *tstamp;\r\ntstamp = nf_conn_tstamp_find(ct);\r\nif (tstamp && tstamp->stop == 0)\r\ntstamp->stop = ktime_get_real_ns();\r\nif (nf_ct_is_dying(ct))\r\ngoto delete;\r\nif (nf_conntrack_event_report(IPCT_DESTROY, ct,\r\nportid, report) < 0) {\r\nnf_ct_delete_from_lists(ct);\r\nnf_conntrack_ecache_delayed_work(nf_ct_net(ct));\r\nreturn false;\r\n}\r\nnf_conntrack_ecache_work(nf_ct_net(ct));\r\nset_bit(IPS_DYING_BIT, &ct->status);\r\ndelete:\r\nnf_ct_delete_from_lists(ct);\r\nnf_ct_put(ct);\r\nreturn true;\r\n}\r\nstatic void death_by_timeout(unsigned long ul_conntrack)\r\n{\r\nnf_ct_delete((struct nf_conn *)ul_conntrack, 0, 0);\r\n}\r\nstatic inline bool\r\nnf_ct_key_equal(struct nf_conntrack_tuple_hash *h,\r\nconst struct nf_conntrack_tuple *tuple,\r\nu16 zone)\r\n{\r\nstruct nf_conn *ct = nf_ct_tuplehash_to_ctrack(h);\r\nreturn nf_ct_tuple_equal(tuple, &h->tuple) &&\r\nnf_ct_zone(ct) == zone &&\r\nnf_ct_is_confirmed(ct);\r\n}\r\nstatic struct nf_conntrack_tuple_hash *\r\n____nf_conntrack_find(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *tuple, u32 hash)\r\n{\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct hlist_nulls_node *n;\r\nunsigned int bucket = hash_bucket(hash, net);\r\nlocal_bh_disable();\r\nbegin:\r\nhlist_nulls_for_each_entry_rcu(h, n, &net->ct.hash[bucket], hnnode) {\r\nif (nf_ct_key_equal(h, tuple, zone)) {\r\nNF_CT_STAT_INC(net, found);\r\nlocal_bh_enable();\r\nreturn h;\r\n}\r\nNF_CT_STAT_INC(net, searched);\r\n}\r\nif (get_nulls_value(n) != bucket) {\r\nNF_CT_STAT_INC(net, search_restart);\r\ngoto begin;\r\n}\r\nlocal_bh_enable();\r\nreturn NULL;\r\n}\r\nstatic struct nf_conntrack_tuple_hash *\r\n__nf_conntrack_find_get(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *tuple, u32 hash)\r\n{\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nrcu_read_lock();\r\nbegin:\r\nh = ____nf_conntrack_find(net, zone, tuple, hash);\r\nif (h) {\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (unlikely(nf_ct_is_dying(ct) ||\r\n!atomic_inc_not_zero(&ct->ct_general.use)))\r\nh = NULL;\r\nelse {\r\nif (unlikely(!nf_ct_key_equal(h, tuple, zone))) {\r\nnf_ct_put(ct);\r\ngoto begin;\r\n}\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn h;\r\n}\r\nstruct nf_conntrack_tuple_hash *\r\nnf_conntrack_find_get(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *tuple)\r\n{\r\nreturn __nf_conntrack_find_get(net, zone, tuple,\r\nhash_conntrack_raw(tuple, zone));\r\n}\r\nstatic void __nf_conntrack_hash_insert(struct nf_conn *ct,\r\nunsigned int hash,\r\nunsigned int reply_hash)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nhlist_nulls_add_head_rcu(&ct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode,\r\n&net->ct.hash[hash]);\r\nhlist_nulls_add_head_rcu(&ct->tuplehash[IP_CT_DIR_REPLY].hnnode,\r\n&net->ct.hash[reply_hash]);\r\n}\r\nint\r\nnf_conntrack_hash_check_insert(struct nf_conn *ct)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nunsigned int hash, reply_hash;\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct hlist_nulls_node *n;\r\nu16 zone;\r\nunsigned int sequence;\r\nzone = nf_ct_zone(ct);\r\nlocal_bh_disable();\r\ndo {\r\nsequence = read_seqcount_begin(&net->ct.generation);\r\nhash = hash_conntrack(net, zone,\r\n&ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple);\r\nreply_hash = hash_conntrack(net, zone,\r\n&ct->tuplehash[IP_CT_DIR_REPLY].tuple);\r\n} while (nf_conntrack_double_lock(net, hash, reply_hash, sequence));\r\nhlist_nulls_for_each_entry(h, n, &net->ct.hash[hash], hnnode)\r\nif (nf_ct_tuple_equal(&ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\r\n&h->tuple) &&\r\nzone == nf_ct_zone(nf_ct_tuplehash_to_ctrack(h)))\r\ngoto out;\r\nhlist_nulls_for_each_entry(h, n, &net->ct.hash[reply_hash], hnnode)\r\nif (nf_ct_tuple_equal(&ct->tuplehash[IP_CT_DIR_REPLY].tuple,\r\n&h->tuple) &&\r\nzone == nf_ct_zone(nf_ct_tuplehash_to_ctrack(h)))\r\ngoto out;\r\nadd_timer(&ct->timeout);\r\nsmp_wmb();\r\natomic_set(&ct->ct_general.use, 2);\r\n__nf_conntrack_hash_insert(ct, hash, reply_hash);\r\nnf_conntrack_double_unlock(hash, reply_hash);\r\nNF_CT_STAT_INC(net, insert);\r\nlocal_bh_enable();\r\nreturn 0;\r\nout:\r\nnf_conntrack_double_unlock(hash, reply_hash);\r\nNF_CT_STAT_INC(net, insert_failed);\r\nlocal_bh_enable();\r\nreturn -EEXIST;\r\n}\r\nvoid nf_conntrack_tmpl_insert(struct net *net, struct nf_conn *tmpl)\r\n{\r\nstruct ct_pcpu *pcpu;\r\n__set_bit(IPS_TEMPLATE_BIT, &tmpl->status);\r\n__set_bit(IPS_CONFIRMED_BIT, &tmpl->status);\r\nnf_conntrack_get(&tmpl->ct_general);\r\nlocal_bh_disable();\r\ntmpl->cpu = smp_processor_id();\r\npcpu = per_cpu_ptr(nf_ct_net(tmpl)->ct.pcpu_lists, tmpl->cpu);\r\nspin_lock(&pcpu->lock);\r\nhlist_nulls_add_head_rcu(&tmpl->tuplehash[IP_CT_DIR_ORIGINAL].hnnode,\r\n&pcpu->tmpl);\r\nspin_unlock_bh(&pcpu->lock);\r\n}\r\nint\r\n__nf_conntrack_confirm(struct sk_buff *skb)\r\n{\r\nunsigned int hash, reply_hash;\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nstruct nf_conn_help *help;\r\nstruct nf_conn_tstamp *tstamp;\r\nstruct hlist_nulls_node *n;\r\nenum ip_conntrack_info ctinfo;\r\nstruct net *net;\r\nu16 zone;\r\nunsigned int sequence;\r\nct = nf_ct_get(skb, &ctinfo);\r\nnet = nf_ct_net(ct);\r\nif (CTINFO2DIR(ctinfo) != IP_CT_DIR_ORIGINAL)\r\nreturn NF_ACCEPT;\r\nzone = nf_ct_zone(ct);\r\nlocal_bh_disable();\r\ndo {\r\nsequence = read_seqcount_begin(&net->ct.generation);\r\nhash = *(unsigned long *)&ct->tuplehash[IP_CT_DIR_REPLY].hnnode.pprev;\r\nhash = hash_bucket(hash, net);\r\nreply_hash = hash_conntrack(net, zone,\r\n&ct->tuplehash[IP_CT_DIR_REPLY].tuple);\r\n} while (nf_conntrack_double_lock(net, hash, reply_hash, sequence));\r\nNF_CT_ASSERT(!nf_ct_is_confirmed(ct));\r\npr_debug("Confirming conntrack %p\n", ct);\r\nnf_ct_del_from_dying_or_unconfirmed_list(ct);\r\nif (unlikely(nf_ct_is_dying(ct)))\r\ngoto out;\r\nhlist_nulls_for_each_entry(h, n, &net->ct.hash[hash], hnnode)\r\nif (nf_ct_tuple_equal(&ct->tuplehash[IP_CT_DIR_ORIGINAL].tuple,\r\n&h->tuple) &&\r\nzone == nf_ct_zone(nf_ct_tuplehash_to_ctrack(h)))\r\ngoto out;\r\nhlist_nulls_for_each_entry(h, n, &net->ct.hash[reply_hash], hnnode)\r\nif (nf_ct_tuple_equal(&ct->tuplehash[IP_CT_DIR_REPLY].tuple,\r\n&h->tuple) &&\r\nzone == nf_ct_zone(nf_ct_tuplehash_to_ctrack(h)))\r\ngoto out;\r\nct->timeout.expires += jiffies;\r\nadd_timer(&ct->timeout);\r\natomic_inc(&ct->ct_general.use);\r\nct->status |= IPS_CONFIRMED;\r\ntstamp = nf_conn_tstamp_find(ct);\r\nif (tstamp) {\r\nif (skb->tstamp.tv64 == 0)\r\n__net_timestamp(skb);\r\ntstamp->start = ktime_to_ns(skb->tstamp);\r\n}\r\n__nf_conntrack_hash_insert(ct, hash, reply_hash);\r\nnf_conntrack_double_unlock(hash, reply_hash);\r\nNF_CT_STAT_INC(net, insert);\r\nlocal_bh_enable();\r\nhelp = nfct_help(ct);\r\nif (help && help->helper)\r\nnf_conntrack_event_cache(IPCT_HELPER, ct);\r\nnf_conntrack_event_cache(master_ct(ct) ?\r\nIPCT_RELATED : IPCT_NEW, ct);\r\nreturn NF_ACCEPT;\r\nout:\r\nnf_ct_add_to_dying_list(ct);\r\nnf_conntrack_double_unlock(hash, reply_hash);\r\nNF_CT_STAT_INC(net, insert_failed);\r\nlocal_bh_enable();\r\nreturn NF_DROP;\r\n}\r\nint\r\nnf_conntrack_tuple_taken(const struct nf_conntrack_tuple *tuple,\r\nconst struct nf_conn *ignored_conntrack)\r\n{\r\nstruct net *net = nf_ct_net(ignored_conntrack);\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct hlist_nulls_node *n;\r\nstruct nf_conn *ct;\r\nu16 zone = nf_ct_zone(ignored_conntrack);\r\nunsigned int hash = hash_conntrack(net, zone, tuple);\r\nrcu_read_lock_bh();\r\nhlist_nulls_for_each_entry_rcu(h, n, &net->ct.hash[hash], hnnode) {\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (ct != ignored_conntrack &&\r\nnf_ct_tuple_equal(tuple, &h->tuple) &&\r\nnf_ct_zone(ct) == zone) {\r\nNF_CT_STAT_INC(net, found);\r\nrcu_read_unlock_bh();\r\nreturn 1;\r\n}\r\nNF_CT_STAT_INC(net, searched);\r\n}\r\nrcu_read_unlock_bh();\r\nreturn 0;\r\n}\r\nstatic noinline int early_drop(struct net *net, unsigned int _hash)\r\n{\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct = NULL, *tmp;\r\nstruct hlist_nulls_node *n;\r\nunsigned int i = 0, cnt = 0;\r\nint dropped = 0;\r\nunsigned int hash, sequence;\r\nspinlock_t *lockp;\r\nlocal_bh_disable();\r\nrestart:\r\nsequence = read_seqcount_begin(&net->ct.generation);\r\nhash = hash_bucket(_hash, net);\r\nfor (; i < net->ct.htable_size; i++) {\r\nlockp = &nf_conntrack_locks[hash % CONNTRACK_LOCKS];\r\nspin_lock(lockp);\r\nif (read_seqcount_retry(&net->ct.generation, sequence)) {\r\nspin_unlock(lockp);\r\ngoto restart;\r\n}\r\nhlist_nulls_for_each_entry_rcu(h, n, &net->ct.hash[hash],\r\nhnnode) {\r\ntmp = nf_ct_tuplehash_to_ctrack(h);\r\nif (!test_bit(IPS_ASSURED_BIT, &tmp->status) &&\r\n!nf_ct_is_dying(tmp) &&\r\natomic_inc_not_zero(&tmp->ct_general.use)) {\r\nct = tmp;\r\nbreak;\r\n}\r\ncnt++;\r\n}\r\nhash = (hash + 1) % net->ct.htable_size;\r\nspin_unlock(lockp);\r\nif (ct || cnt >= NF_CT_EVICTION_RANGE)\r\nbreak;\r\n}\r\nlocal_bh_enable();\r\nif (!ct)\r\nreturn dropped;\r\nif (del_timer(&ct->timeout)) {\r\nif (nf_ct_delete(ct, 0, 0)) {\r\ndropped = 1;\r\nNF_CT_STAT_INC_ATOMIC(net, early_drop);\r\n}\r\n}\r\nnf_ct_put(ct);\r\nreturn dropped;\r\n}\r\nvoid init_nf_conntrack_hash_rnd(void)\r\n{\r\nunsigned int rand;\r\ndo {\r\nget_random_bytes(&rand, sizeof(rand));\r\n} while (!rand);\r\ncmpxchg(&nf_conntrack_hash_rnd, 0, rand);\r\n}\r\nstatic struct nf_conn *\r\n__nf_conntrack_alloc(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *orig,\r\nconst struct nf_conntrack_tuple *repl,\r\ngfp_t gfp, u32 hash)\r\n{\r\nstruct nf_conn *ct;\r\nif (unlikely(!nf_conntrack_hash_rnd)) {\r\ninit_nf_conntrack_hash_rnd();\r\nhash = hash_conntrack_raw(orig, zone);\r\n}\r\natomic_inc(&net->ct.count);\r\nif (nf_conntrack_max &&\r\nunlikely(atomic_read(&net->ct.count) > nf_conntrack_max)) {\r\nif (!early_drop(net, hash)) {\r\natomic_dec(&net->ct.count);\r\nnet_warn_ratelimited("nf_conntrack: table full, dropping packet\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\n}\r\nct = kmem_cache_alloc(net->ct.nf_conntrack_cachep, gfp);\r\nif (ct == NULL) {\r\natomic_dec(&net->ct.count);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nspin_lock_init(&ct->lock);\r\nct->tuplehash[IP_CT_DIR_ORIGINAL].tuple = *orig;\r\nct->tuplehash[IP_CT_DIR_ORIGINAL].hnnode.pprev = NULL;\r\nct->tuplehash[IP_CT_DIR_REPLY].tuple = *repl;\r\n*(unsigned long *)(&ct->tuplehash[IP_CT_DIR_REPLY].hnnode.pprev) = hash;\r\nct->status = 0;\r\nsetup_timer(&ct->timeout, death_by_timeout, (unsigned long)ct);\r\nwrite_pnet(&ct->ct_net, net);\r\nmemset(&ct->__nfct_init_offset[0], 0,\r\noffsetof(struct nf_conn, proto) -\r\noffsetof(struct nf_conn, __nfct_init_offset[0]));\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nif (zone) {\r\nstruct nf_conntrack_zone *nf_ct_zone;\r\nnf_ct_zone = nf_ct_ext_add(ct, NF_CT_EXT_ZONE, GFP_ATOMIC);\r\nif (!nf_ct_zone)\r\ngoto out_free;\r\nnf_ct_zone->id = zone;\r\n}\r\n#endif\r\natomic_set(&ct->ct_general.use, 0);\r\nreturn ct;\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nout_free:\r\natomic_dec(&net->ct.count);\r\nkmem_cache_free(net->ct.nf_conntrack_cachep, ct);\r\nreturn ERR_PTR(-ENOMEM);\r\n#endif\r\n}\r\nstruct nf_conn *nf_conntrack_alloc(struct net *net, u16 zone,\r\nconst struct nf_conntrack_tuple *orig,\r\nconst struct nf_conntrack_tuple *repl,\r\ngfp_t gfp)\r\n{\r\nreturn __nf_conntrack_alloc(net, zone, orig, repl, gfp, 0);\r\n}\r\nvoid nf_conntrack_free(struct nf_conn *ct)\r\n{\r\nstruct net *net = nf_ct_net(ct);\r\nNF_CT_ASSERT(atomic_read(&ct->ct_general.use) == 0);\r\nnf_ct_ext_destroy(ct);\r\nnf_ct_ext_free(ct);\r\nkmem_cache_free(net->ct.nf_conntrack_cachep, ct);\r\nsmp_mb__before_atomic();\r\natomic_dec(&net->ct.count);\r\n}\r\nstatic struct nf_conntrack_tuple_hash *\r\ninit_conntrack(struct net *net, struct nf_conn *tmpl,\r\nconst struct nf_conntrack_tuple *tuple,\r\nstruct nf_conntrack_l3proto *l3proto,\r\nstruct nf_conntrack_l4proto *l4proto,\r\nstruct sk_buff *skb,\r\nunsigned int dataoff, u32 hash)\r\n{\r\nstruct nf_conn *ct;\r\nstruct nf_conn_help *help;\r\nstruct nf_conntrack_tuple repl_tuple;\r\nstruct nf_conntrack_ecache *ecache;\r\nstruct nf_conntrack_expect *exp = NULL;\r\nu16 zone = tmpl ? nf_ct_zone(tmpl) : NF_CT_DEFAULT_ZONE;\r\nstruct nf_conn_timeout *timeout_ext;\r\nunsigned int *timeouts;\r\nif (!nf_ct_invert_tuple(&repl_tuple, tuple, l3proto, l4proto)) {\r\npr_debug("Can't invert tuple.\n");\r\nreturn NULL;\r\n}\r\nct = __nf_conntrack_alloc(net, zone, tuple, &repl_tuple, GFP_ATOMIC,\r\nhash);\r\nif (IS_ERR(ct))\r\nreturn (struct nf_conntrack_tuple_hash *)ct;\r\nif (tmpl && nfct_synproxy(tmpl)) {\r\nnfct_seqadj_ext_add(ct);\r\nnfct_synproxy_ext_add(ct);\r\n}\r\ntimeout_ext = tmpl ? nf_ct_timeout_find(tmpl) : NULL;\r\nif (timeout_ext)\r\ntimeouts = NF_CT_TIMEOUT_EXT_DATA(timeout_ext);\r\nelse\r\ntimeouts = l4proto->get_timeouts(net);\r\nif (!l4proto->new(ct, skb, dataoff, timeouts)) {\r\nnf_conntrack_free(ct);\r\npr_debug("init conntrack: can't track with proto module\n");\r\nreturn NULL;\r\n}\r\nif (timeout_ext)\r\nnf_ct_timeout_ext_add(ct, timeout_ext->timeout, GFP_ATOMIC);\r\nnf_ct_acct_ext_add(ct, GFP_ATOMIC);\r\nnf_ct_tstamp_ext_add(ct, GFP_ATOMIC);\r\nnf_ct_labels_ext_add(ct);\r\necache = tmpl ? nf_ct_ecache_find(tmpl) : NULL;\r\nnf_ct_ecache_ext_add(ct, ecache ? ecache->ctmask : 0,\r\necache ? ecache->expmask : 0,\r\nGFP_ATOMIC);\r\nlocal_bh_disable();\r\nif (net->ct.expect_count) {\r\nspin_lock(&nf_conntrack_expect_lock);\r\nexp = nf_ct_find_expectation(net, zone, tuple);\r\nif (exp) {\r\npr_debug("conntrack: expectation arrives ct=%p exp=%p\n",\r\nct, exp);\r\n__set_bit(IPS_EXPECTED_BIT, &ct->status);\r\nct->master = exp->master;\r\nif (exp->helper) {\r\nhelp = nf_ct_helper_ext_add(ct, exp->helper,\r\nGFP_ATOMIC);\r\nif (help)\r\nrcu_assign_pointer(help->helper, exp->helper);\r\n}\r\n#ifdef CONFIG_NF_CONNTRACK_MARK\r\nct->mark = exp->master->mark;\r\n#endif\r\n#ifdef CONFIG_NF_CONNTRACK_SECMARK\r\nct->secmark = exp->master->secmark;\r\n#endif\r\nNF_CT_STAT_INC(net, expect_new);\r\n}\r\nspin_unlock(&nf_conntrack_expect_lock);\r\n}\r\nif (!exp) {\r\n__nf_ct_try_assign_helper(ct, tmpl, GFP_ATOMIC);\r\nNF_CT_STAT_INC(net, new);\r\n}\r\nnf_conntrack_get(&ct->ct_general);\r\nnf_ct_add_to_unconfirmed_list(ct);\r\nlocal_bh_enable();\r\nif (exp) {\r\nif (exp->expectfn)\r\nexp->expectfn(ct, exp);\r\nnf_ct_expect_put(exp);\r\n}\r\nreturn &ct->tuplehash[IP_CT_DIR_ORIGINAL];\r\n}\r\nstatic inline struct nf_conn *\r\nresolve_normal_ct(struct net *net, struct nf_conn *tmpl,\r\nstruct sk_buff *skb,\r\nunsigned int dataoff,\r\nu_int16_t l3num,\r\nu_int8_t protonum,\r\nstruct nf_conntrack_l3proto *l3proto,\r\nstruct nf_conntrack_l4proto *l4proto,\r\nint *set_reply,\r\nenum ip_conntrack_info *ctinfo)\r\n{\r\nstruct nf_conntrack_tuple tuple;\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nu16 zone = tmpl ? nf_ct_zone(tmpl) : NF_CT_DEFAULT_ZONE;\r\nu32 hash;\r\nif (!nf_ct_get_tuple(skb, skb_network_offset(skb),\r\ndataoff, l3num, protonum, &tuple, l3proto,\r\nl4proto)) {\r\npr_debug("resolve_normal_ct: Can't get tuple\n");\r\nreturn NULL;\r\n}\r\nhash = hash_conntrack_raw(&tuple, zone);\r\nh = __nf_conntrack_find_get(net, zone, &tuple, hash);\r\nif (!h) {\r\nh = init_conntrack(net, tmpl, &tuple, l3proto, l4proto,\r\nskb, dataoff, hash);\r\nif (!h)\r\nreturn NULL;\r\nif (IS_ERR(h))\r\nreturn (void *)h;\r\n}\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (NF_CT_DIRECTION(h) == IP_CT_DIR_REPLY) {\r\n*ctinfo = IP_CT_ESTABLISHED_REPLY;\r\n*set_reply = 1;\r\n} else {\r\nif (test_bit(IPS_SEEN_REPLY_BIT, &ct->status)) {\r\npr_debug("nf_conntrack_in: normal packet for %p\n", ct);\r\n*ctinfo = IP_CT_ESTABLISHED;\r\n} else if (test_bit(IPS_EXPECTED_BIT, &ct->status)) {\r\npr_debug("nf_conntrack_in: related packet for %p\n",\r\nct);\r\n*ctinfo = IP_CT_RELATED;\r\n} else {\r\npr_debug("nf_conntrack_in: new packet for %p\n", ct);\r\n*ctinfo = IP_CT_NEW;\r\n}\r\n*set_reply = 0;\r\n}\r\nskb->nfct = &ct->ct_general;\r\nskb->nfctinfo = *ctinfo;\r\nreturn ct;\r\n}\r\nunsigned int\r\nnf_conntrack_in(struct net *net, u_int8_t pf, unsigned int hooknum,\r\nstruct sk_buff *skb)\r\n{\r\nstruct nf_conn *ct, *tmpl = NULL;\r\nenum ip_conntrack_info ctinfo;\r\nstruct nf_conntrack_l3proto *l3proto;\r\nstruct nf_conntrack_l4proto *l4proto;\r\nunsigned int *timeouts;\r\nunsigned int dataoff;\r\nu_int8_t protonum;\r\nint set_reply = 0;\r\nint ret;\r\nif (skb->nfct) {\r\ntmpl = (struct nf_conn *)skb->nfct;\r\nif (!nf_ct_is_template(tmpl)) {\r\nNF_CT_STAT_INC_ATOMIC(net, ignore);\r\nreturn NF_ACCEPT;\r\n}\r\nskb->nfct = NULL;\r\n}\r\nl3proto = __nf_ct_l3proto_find(pf);\r\nret = l3proto->get_l4proto(skb, skb_network_offset(skb),\r\n&dataoff, &protonum);\r\nif (ret <= 0) {\r\npr_debug("not prepared to track yet or error occurred\n");\r\nNF_CT_STAT_INC_ATOMIC(net, error);\r\nNF_CT_STAT_INC_ATOMIC(net, invalid);\r\nret = -ret;\r\ngoto out;\r\n}\r\nl4proto = __nf_ct_l4proto_find(pf, protonum);\r\nif (l4proto->error != NULL) {\r\nret = l4proto->error(net, tmpl, skb, dataoff, &ctinfo,\r\npf, hooknum);\r\nif (ret <= 0) {\r\nNF_CT_STAT_INC_ATOMIC(net, error);\r\nNF_CT_STAT_INC_ATOMIC(net, invalid);\r\nret = -ret;\r\ngoto out;\r\n}\r\nif (skb->nfct)\r\ngoto out;\r\n}\r\nct = resolve_normal_ct(net, tmpl, skb, dataoff, pf, protonum,\r\nl3proto, l4proto, &set_reply, &ctinfo);\r\nif (!ct) {\r\nNF_CT_STAT_INC_ATOMIC(net, invalid);\r\nret = NF_ACCEPT;\r\ngoto out;\r\n}\r\nif (IS_ERR(ct)) {\r\nNF_CT_STAT_INC_ATOMIC(net, drop);\r\nret = NF_DROP;\r\ngoto out;\r\n}\r\nNF_CT_ASSERT(skb->nfct);\r\ntimeouts = nf_ct_timeout_lookup(net, ct, l4proto);\r\nret = l4proto->packet(ct, skb, dataoff, ctinfo, pf, hooknum, timeouts);\r\nif (ret <= 0) {\r\npr_debug("nf_conntrack_in: Can't track with proto module\n");\r\nnf_conntrack_put(skb->nfct);\r\nskb->nfct = NULL;\r\nNF_CT_STAT_INC_ATOMIC(net, invalid);\r\nif (ret == -NF_DROP)\r\nNF_CT_STAT_INC_ATOMIC(net, drop);\r\nret = -ret;\r\ngoto out;\r\n}\r\nif (set_reply && !test_and_set_bit(IPS_SEEN_REPLY_BIT, &ct->status))\r\nnf_conntrack_event_cache(IPCT_REPLY, ct);\r\nout:\r\nif (tmpl) {\r\nif (ret == NF_REPEAT)\r\nskb->nfct = (struct nf_conntrack *)tmpl;\r\nelse\r\nnf_ct_put(tmpl);\r\n}\r\nreturn ret;\r\n}\r\nbool nf_ct_invert_tuplepr(struct nf_conntrack_tuple *inverse,\r\nconst struct nf_conntrack_tuple *orig)\r\n{\r\nbool ret;\r\nrcu_read_lock();\r\nret = nf_ct_invert_tuple(inverse, orig,\r\n__nf_ct_l3proto_find(orig->src.l3num),\r\n__nf_ct_l4proto_find(orig->src.l3num,\r\norig->dst.protonum));\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nvoid nf_conntrack_alter_reply(struct nf_conn *ct,\r\nconst struct nf_conntrack_tuple *newreply)\r\n{\r\nstruct nf_conn_help *help = nfct_help(ct);\r\nNF_CT_ASSERT(!nf_ct_is_confirmed(ct));\r\npr_debug("Altering reply tuple of %p to ", ct);\r\nnf_ct_dump_tuple(newreply);\r\nct->tuplehash[IP_CT_DIR_REPLY].tuple = *newreply;\r\nif (ct->master || (help && !hlist_empty(&help->expectations)))\r\nreturn;\r\nrcu_read_lock();\r\n__nf_ct_try_assign_helper(ct, NULL, GFP_ATOMIC);\r\nrcu_read_unlock();\r\n}\r\nvoid __nf_ct_refresh_acct(struct nf_conn *ct,\r\nenum ip_conntrack_info ctinfo,\r\nconst struct sk_buff *skb,\r\nunsigned long extra_jiffies,\r\nint do_acct)\r\n{\r\nNF_CT_ASSERT(ct->timeout.data == (unsigned long)ct);\r\nNF_CT_ASSERT(skb);\r\nif (test_bit(IPS_FIXED_TIMEOUT_BIT, &ct->status))\r\ngoto acct;\r\nif (!nf_ct_is_confirmed(ct)) {\r\nct->timeout.expires = extra_jiffies;\r\n} else {\r\nunsigned long newtime = jiffies + extra_jiffies;\r\nif (newtime - ct->timeout.expires >= HZ)\r\nmod_timer_pending(&ct->timeout, newtime);\r\n}\r\nacct:\r\nif (do_acct) {\r\nstruct nf_conn_acct *acct;\r\nacct = nf_conn_acct_find(ct);\r\nif (acct) {\r\nstruct nf_conn_counter *counter = acct->counter;\r\natomic64_inc(&counter[CTINFO2DIR(ctinfo)].packets);\r\natomic64_add(skb->len, &counter[CTINFO2DIR(ctinfo)].bytes);\r\n}\r\n}\r\n}\r\nbool __nf_ct_kill_acct(struct nf_conn *ct,\r\nenum ip_conntrack_info ctinfo,\r\nconst struct sk_buff *skb,\r\nint do_acct)\r\n{\r\nif (do_acct) {\r\nstruct nf_conn_acct *acct;\r\nacct = nf_conn_acct_find(ct);\r\nif (acct) {\r\nstruct nf_conn_counter *counter = acct->counter;\r\natomic64_inc(&counter[CTINFO2DIR(ctinfo)].packets);\r\natomic64_add(skb->len - skb_network_offset(skb),\r\n&counter[CTINFO2DIR(ctinfo)].bytes);\r\n}\r\n}\r\nif (del_timer(&ct->timeout)) {\r\nct->timeout.function((unsigned long)ct);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nint nf_ct_port_tuple_to_nlattr(struct sk_buff *skb,\r\nconst struct nf_conntrack_tuple *tuple)\r\n{\r\nif (nla_put_be16(skb, CTA_PROTO_SRC_PORT, tuple->src.u.tcp.port) ||\r\nnla_put_be16(skb, CTA_PROTO_DST_PORT, tuple->dst.u.tcp.port))\r\ngoto nla_put_failure;\r\nreturn 0;\r\nnla_put_failure:\r\nreturn -1;\r\n}\r\nint nf_ct_port_nlattr_to_tuple(struct nlattr *tb[],\r\nstruct nf_conntrack_tuple *t)\r\n{\r\nif (!tb[CTA_PROTO_SRC_PORT] || !tb[CTA_PROTO_DST_PORT])\r\nreturn -EINVAL;\r\nt->src.u.tcp.port = nla_get_be16(tb[CTA_PROTO_SRC_PORT]);\r\nt->dst.u.tcp.port = nla_get_be16(tb[CTA_PROTO_DST_PORT]);\r\nreturn 0;\r\n}\r\nint nf_ct_port_nlattr_tuple_size(void)\r\n{\r\nreturn nla_policy_len(nf_ct_port_nla_policy, CTA_PROTO_MAX + 1);\r\n}\r\nstatic void nf_conntrack_attach(struct sk_buff *nskb, const struct sk_buff *skb)\r\n{\r\nstruct nf_conn *ct;\r\nenum ip_conntrack_info ctinfo;\r\nct = nf_ct_get(skb, &ctinfo);\r\nif (CTINFO2DIR(ctinfo) == IP_CT_DIR_ORIGINAL)\r\nctinfo = IP_CT_RELATED_REPLY;\r\nelse\r\nctinfo = IP_CT_RELATED;\r\nnskb->nfct = &ct->ct_general;\r\nnskb->nfctinfo = ctinfo;\r\nnf_conntrack_get(nskb->nfct);\r\n}\r\nstatic struct nf_conn *\r\nget_next_corpse(struct net *net, int (*iter)(struct nf_conn *i, void *data),\r\nvoid *data, unsigned int *bucket)\r\n{\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nstruct hlist_nulls_node *n;\r\nint cpu;\r\nspinlock_t *lockp;\r\nfor (; *bucket < net->ct.htable_size; (*bucket)++) {\r\nlockp = &nf_conntrack_locks[*bucket % CONNTRACK_LOCKS];\r\nlocal_bh_disable();\r\nspin_lock(lockp);\r\nif (*bucket < net->ct.htable_size) {\r\nhlist_nulls_for_each_entry(h, n, &net->ct.hash[*bucket], hnnode) {\r\nif (NF_CT_DIRECTION(h) != IP_CT_DIR_ORIGINAL)\r\ncontinue;\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (iter(ct, data))\r\ngoto found;\r\n}\r\n}\r\nspin_unlock(lockp);\r\nlocal_bh_enable();\r\n}\r\nfor_each_possible_cpu(cpu) {\r\nstruct ct_pcpu *pcpu = per_cpu_ptr(net->ct.pcpu_lists, cpu);\r\nspin_lock_bh(&pcpu->lock);\r\nhlist_nulls_for_each_entry(h, n, &pcpu->unconfirmed, hnnode) {\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nif (iter(ct, data))\r\nset_bit(IPS_DYING_BIT, &ct->status);\r\n}\r\nspin_unlock_bh(&pcpu->lock);\r\n}\r\nreturn NULL;\r\nfound:\r\natomic_inc(&ct->ct_general.use);\r\nspin_unlock(lockp);\r\nlocal_bh_enable();\r\nreturn ct;\r\n}\r\nvoid nf_ct_iterate_cleanup(struct net *net,\r\nint (*iter)(struct nf_conn *i, void *data),\r\nvoid *data, u32 portid, int report)\r\n{\r\nstruct nf_conn *ct;\r\nunsigned int bucket = 0;\r\nwhile ((ct = get_next_corpse(net, iter, data, &bucket)) != NULL) {\r\nif (del_timer(&ct->timeout))\r\nnf_ct_delete(ct, portid, report);\r\nnf_ct_put(ct);\r\n}\r\n}\r\nstatic int kill_all(struct nf_conn *i, void *data)\r\n{\r\nreturn 1;\r\n}\r\nvoid nf_ct_free_hashtable(void *hash, unsigned int size)\r\n{\r\nif (is_vmalloc_addr(hash))\r\nvfree(hash);\r\nelse\r\nfree_pages((unsigned long)hash,\r\nget_order(sizeof(struct hlist_head) * size));\r\n}\r\nstatic int untrack_refs(void)\r\n{\r\nint cnt = 0, cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct nf_conn *ct = &per_cpu(nf_conntrack_untracked, cpu);\r\ncnt += atomic_read(&ct->ct_general.use) - 1;\r\n}\r\nreturn cnt;\r\n}\r\nvoid nf_conntrack_cleanup_start(void)\r\n{\r\nRCU_INIT_POINTER(ip_ct_attach, NULL);\r\n}\r\nvoid nf_conntrack_cleanup_end(void)\r\n{\r\nRCU_INIT_POINTER(nf_ct_destroy, NULL);\r\nwhile (untrack_refs() > 0)\r\nschedule();\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nnf_ct_extend_unregister(&nf_ct_zone_extend);\r\n#endif\r\nnf_conntrack_proto_fini();\r\nnf_conntrack_seqadj_fini();\r\nnf_conntrack_labels_fini();\r\nnf_conntrack_helper_fini();\r\nnf_conntrack_timeout_fini();\r\nnf_conntrack_ecache_fini();\r\nnf_conntrack_tstamp_fini();\r\nnf_conntrack_acct_fini();\r\nnf_conntrack_expect_fini();\r\n}\r\nvoid nf_conntrack_cleanup_net(struct net *net)\r\n{\r\nLIST_HEAD(single);\r\nlist_add(&net->exit_list, &single);\r\nnf_conntrack_cleanup_net_list(&single);\r\n}\r\nvoid nf_conntrack_cleanup_net_list(struct list_head *net_exit_list)\r\n{\r\nint busy;\r\nstruct net *net;\r\nsynchronize_net();\r\ni_see_dead_people:\r\nbusy = 0;\r\nlist_for_each_entry(net, net_exit_list, exit_list) {\r\nnf_ct_iterate_cleanup(net, kill_all, NULL, 0, 0);\r\nif (atomic_read(&net->ct.count) != 0)\r\nbusy = 1;\r\n}\r\nif (busy) {\r\nschedule();\r\ngoto i_see_dead_people;\r\n}\r\nlist_for_each_entry(net, net_exit_list, exit_list) {\r\nnf_ct_free_hashtable(net->ct.hash, net->ct.htable_size);\r\nnf_conntrack_proto_pernet_fini(net);\r\nnf_conntrack_helper_pernet_fini(net);\r\nnf_conntrack_ecache_pernet_fini(net);\r\nnf_conntrack_tstamp_pernet_fini(net);\r\nnf_conntrack_acct_pernet_fini(net);\r\nnf_conntrack_expect_pernet_fini(net);\r\nkmem_cache_destroy(net->ct.nf_conntrack_cachep);\r\nkfree(net->ct.slabname);\r\nfree_percpu(net->ct.stat);\r\nfree_percpu(net->ct.pcpu_lists);\r\n}\r\n}\r\nvoid *nf_ct_alloc_hashtable(unsigned int *sizep, int nulls)\r\n{\r\nstruct hlist_nulls_head *hash;\r\nunsigned int nr_slots, i;\r\nsize_t sz;\r\nBUILD_BUG_ON(sizeof(struct hlist_nulls_head) != sizeof(struct hlist_head));\r\nnr_slots = *sizep = roundup(*sizep, PAGE_SIZE / sizeof(struct hlist_nulls_head));\r\nsz = nr_slots * sizeof(struct hlist_nulls_head);\r\nhash = (void *)__get_free_pages(GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO,\r\nget_order(sz));\r\nif (!hash) {\r\nprintk(KERN_WARNING "nf_conntrack: falling back to vmalloc.\n");\r\nhash = vzalloc(sz);\r\n}\r\nif (hash && nulls)\r\nfor (i = 0; i < nr_slots; i++)\r\nINIT_HLIST_NULLS_HEAD(&hash[i], i);\r\nreturn hash;\r\n}\r\nint nf_conntrack_set_hashsize(const char *val, struct kernel_param *kp)\r\n{\r\nint i, bucket, rc;\r\nunsigned int hashsize, old_size;\r\nstruct hlist_nulls_head *hash, *old_hash;\r\nstruct nf_conntrack_tuple_hash *h;\r\nstruct nf_conn *ct;\r\nif (current->nsproxy->net_ns != &init_net)\r\nreturn -EOPNOTSUPP;\r\nif (!nf_conntrack_htable_size)\r\nreturn param_set_uint(val, kp);\r\nrc = kstrtouint(val, 0, &hashsize);\r\nif (rc)\r\nreturn rc;\r\nif (!hashsize)\r\nreturn -EINVAL;\r\nhash = nf_ct_alloc_hashtable(&hashsize, 1);\r\nif (!hash)\r\nreturn -ENOMEM;\r\nlocal_bh_disable();\r\nnf_conntrack_all_lock();\r\nwrite_seqcount_begin(&init_net.ct.generation);\r\nfor (i = 0; i < init_net.ct.htable_size; i++) {\r\nwhile (!hlist_nulls_empty(&init_net.ct.hash[i])) {\r\nh = hlist_nulls_entry(init_net.ct.hash[i].first,\r\nstruct nf_conntrack_tuple_hash, hnnode);\r\nct = nf_ct_tuplehash_to_ctrack(h);\r\nhlist_nulls_del_rcu(&h->hnnode);\r\nbucket = __hash_conntrack(&h->tuple, nf_ct_zone(ct),\r\nhashsize);\r\nhlist_nulls_add_head_rcu(&h->hnnode, &hash[bucket]);\r\n}\r\n}\r\nold_size = init_net.ct.htable_size;\r\nold_hash = init_net.ct.hash;\r\ninit_net.ct.htable_size = nf_conntrack_htable_size = hashsize;\r\ninit_net.ct.hash = hash;\r\nwrite_seqcount_end(&init_net.ct.generation);\r\nnf_conntrack_all_unlock();\r\nlocal_bh_enable();\r\nnf_ct_free_hashtable(old_hash, old_size);\r\nreturn 0;\r\n}\r\nvoid nf_ct_untracked_status_or(unsigned long bits)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu)\r\nper_cpu(nf_conntrack_untracked, cpu).status |= bits;\r\n}\r\nint nf_conntrack_init_start(void)\r\n{\r\nint max_factor = 8;\r\nint i, ret, cpu;\r\nfor (i = 0; i < CONNTRACK_LOCKS; i++)\r\nspin_lock_init(&nf_conntrack_locks[i]);\r\nif (!nf_conntrack_htable_size) {\r\nnf_conntrack_htable_size\r\n= (((totalram_pages << PAGE_SHIFT) / 16384)\r\n/ sizeof(struct hlist_head));\r\nif (totalram_pages > (4 * (1024 * 1024 * 1024 / PAGE_SIZE)))\r\nnf_conntrack_htable_size = 65536;\r\nelse if (totalram_pages > (1024 * 1024 * 1024 / PAGE_SIZE))\r\nnf_conntrack_htable_size = 16384;\r\nif (nf_conntrack_htable_size < 32)\r\nnf_conntrack_htable_size = 32;\r\nmax_factor = 4;\r\n}\r\nnf_conntrack_max = max_factor * nf_conntrack_htable_size;\r\nprintk(KERN_INFO "nf_conntrack version %s (%u buckets, %d max)\n",\r\nNF_CONNTRACK_VERSION, nf_conntrack_htable_size,\r\nnf_conntrack_max);\r\nret = nf_conntrack_expect_init();\r\nif (ret < 0)\r\ngoto err_expect;\r\nret = nf_conntrack_acct_init();\r\nif (ret < 0)\r\ngoto err_acct;\r\nret = nf_conntrack_tstamp_init();\r\nif (ret < 0)\r\ngoto err_tstamp;\r\nret = nf_conntrack_ecache_init();\r\nif (ret < 0)\r\ngoto err_ecache;\r\nret = nf_conntrack_timeout_init();\r\nif (ret < 0)\r\ngoto err_timeout;\r\nret = nf_conntrack_helper_init();\r\nif (ret < 0)\r\ngoto err_helper;\r\nret = nf_conntrack_labels_init();\r\nif (ret < 0)\r\ngoto err_labels;\r\nret = nf_conntrack_seqadj_init();\r\nif (ret < 0)\r\ngoto err_seqadj;\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nret = nf_ct_extend_register(&nf_ct_zone_extend);\r\nif (ret < 0)\r\ngoto err_extend;\r\n#endif\r\nret = nf_conntrack_proto_init();\r\nif (ret < 0)\r\ngoto err_proto;\r\nfor_each_possible_cpu(cpu) {\r\nstruct nf_conn *ct = &per_cpu(nf_conntrack_untracked, cpu);\r\nwrite_pnet(&ct->ct_net, &init_net);\r\natomic_set(&ct->ct_general.use, 1);\r\n}\r\nnf_ct_untracked_status_or(IPS_CONFIRMED | IPS_UNTRACKED);\r\nreturn 0;\r\nerr_proto:\r\n#ifdef CONFIG_NF_CONNTRACK_ZONES\r\nnf_ct_extend_unregister(&nf_ct_zone_extend);\r\nerr_extend:\r\n#endif\r\nnf_conntrack_seqadj_fini();\r\nerr_seqadj:\r\nnf_conntrack_labels_fini();\r\nerr_labels:\r\nnf_conntrack_helper_fini();\r\nerr_helper:\r\nnf_conntrack_timeout_fini();\r\nerr_timeout:\r\nnf_conntrack_ecache_fini();\r\nerr_ecache:\r\nnf_conntrack_tstamp_fini();\r\nerr_tstamp:\r\nnf_conntrack_acct_fini();\r\nerr_acct:\r\nnf_conntrack_expect_fini();\r\nerr_expect:\r\nreturn ret;\r\n}\r\nvoid nf_conntrack_init_end(void)\r\n{\r\nRCU_INIT_POINTER(ip_ct_attach, nf_conntrack_attach);\r\nRCU_INIT_POINTER(nf_ct_destroy, destroy_conntrack);\r\n}\r\nint nf_conntrack_init_net(struct net *net)\r\n{\r\nint ret = -ENOMEM;\r\nint cpu;\r\natomic_set(&net->ct.count, 0);\r\nseqcount_init(&net->ct.generation);\r\nnet->ct.pcpu_lists = alloc_percpu(struct ct_pcpu);\r\nif (!net->ct.pcpu_lists)\r\ngoto err_stat;\r\nfor_each_possible_cpu(cpu) {\r\nstruct ct_pcpu *pcpu = per_cpu_ptr(net->ct.pcpu_lists, cpu);\r\nspin_lock_init(&pcpu->lock);\r\nINIT_HLIST_NULLS_HEAD(&pcpu->unconfirmed, UNCONFIRMED_NULLS_VAL);\r\nINIT_HLIST_NULLS_HEAD(&pcpu->dying, DYING_NULLS_VAL);\r\nINIT_HLIST_NULLS_HEAD(&pcpu->tmpl, TEMPLATE_NULLS_VAL);\r\n}\r\nnet->ct.stat = alloc_percpu(struct ip_conntrack_stat);\r\nif (!net->ct.stat)\r\ngoto err_pcpu_lists;\r\nnet->ct.slabname = kasprintf(GFP_KERNEL, "nf_conntrack_%p", net);\r\nif (!net->ct.slabname)\r\ngoto err_slabname;\r\nnet->ct.nf_conntrack_cachep = kmem_cache_create(net->ct.slabname,\r\nsizeof(struct nf_conn), 0,\r\nSLAB_DESTROY_BY_RCU, NULL);\r\nif (!net->ct.nf_conntrack_cachep) {\r\nprintk(KERN_ERR "Unable to create nf_conn slab cache\n");\r\ngoto err_cache;\r\n}\r\nnet->ct.htable_size = nf_conntrack_htable_size;\r\nnet->ct.hash = nf_ct_alloc_hashtable(&net->ct.htable_size, 1);\r\nif (!net->ct.hash) {\r\nprintk(KERN_ERR "Unable to create nf_conntrack_hash\n");\r\ngoto err_hash;\r\n}\r\nret = nf_conntrack_expect_pernet_init(net);\r\nif (ret < 0)\r\ngoto err_expect;\r\nret = nf_conntrack_acct_pernet_init(net);\r\nif (ret < 0)\r\ngoto err_acct;\r\nret = nf_conntrack_tstamp_pernet_init(net);\r\nif (ret < 0)\r\ngoto err_tstamp;\r\nret = nf_conntrack_ecache_pernet_init(net);\r\nif (ret < 0)\r\ngoto err_ecache;\r\nret = nf_conntrack_helper_pernet_init(net);\r\nif (ret < 0)\r\ngoto err_helper;\r\nret = nf_conntrack_proto_pernet_init(net);\r\nif (ret < 0)\r\ngoto err_proto;\r\nreturn 0;\r\nerr_proto:\r\nnf_conntrack_helper_pernet_fini(net);\r\nerr_helper:\r\nnf_conntrack_ecache_pernet_fini(net);\r\nerr_ecache:\r\nnf_conntrack_tstamp_pernet_fini(net);\r\nerr_tstamp:\r\nnf_conntrack_acct_pernet_fini(net);\r\nerr_acct:\r\nnf_conntrack_expect_pernet_fini(net);\r\nerr_expect:\r\nnf_ct_free_hashtable(net->ct.hash, net->ct.htable_size);\r\nerr_hash:\r\nkmem_cache_destroy(net->ct.nf_conntrack_cachep);\r\nerr_cache:\r\nkfree(net->ct.slabname);\r\nerr_slabname:\r\nfree_percpu(net->ct.stat);\r\nerr_pcpu_lists:\r\nfree_percpu(net->ct.pcpu_lists);\r\nerr_stat:\r\nreturn ret;\r\n}
