static void build_data_key(struct dm_thin_device *td,\r\ndm_block_t b, struct dm_cell_key *key)\r\n{\r\nkey->virtual = 0;\r\nkey->dev = dm_thin_dev_id(td);\r\nkey->block_begin = b;\r\nkey->block_end = b + 1ULL;\r\n}\r\nstatic void build_virtual_key(struct dm_thin_device *td, dm_block_t b,\r\nstruct dm_cell_key *key)\r\n{\r\nkey->virtual = 1;\r\nkey->dev = dm_thin_dev_id(td);\r\nkey->block_begin = b;\r\nkey->block_end = b + 1ULL;\r\n}\r\nstatic void throttle_init(struct throttle *t)\r\n{\r\ninit_rwsem(&t->lock);\r\nt->throttle_applied = false;\r\n}\r\nstatic void throttle_work_start(struct throttle *t)\r\n{\r\nt->threshold = jiffies + THROTTLE_THRESHOLD;\r\n}\r\nstatic void throttle_work_update(struct throttle *t)\r\n{\r\nif (!t->throttle_applied && jiffies > t->threshold) {\r\ndown_write(&t->lock);\r\nt->throttle_applied = true;\r\n}\r\n}\r\nstatic void throttle_work_complete(struct throttle *t)\r\n{\r\nif (t->throttle_applied) {\r\nt->throttle_applied = false;\r\nup_write(&t->lock);\r\n}\r\n}\r\nstatic void throttle_lock(struct throttle *t)\r\n{\r\ndown_read(&t->lock);\r\n}\r\nstatic void throttle_unlock(struct throttle *t)\r\n{\r\nup_read(&t->lock);\r\n}\r\nstatic void wake_worker(struct pool *pool)\r\n{\r\nqueue_work(pool->wq, &pool->worker);\r\n}\r\nstatic int bio_detain(struct pool *pool, struct dm_cell_key *key, struct bio *bio,\r\nstruct dm_bio_prison_cell **cell_result)\r\n{\r\nint r;\r\nstruct dm_bio_prison_cell *cell_prealloc;\r\ncell_prealloc = dm_bio_prison_alloc_cell(pool->prison, GFP_NOIO);\r\nr = dm_bio_detain(pool->prison, key, bio, cell_prealloc, cell_result);\r\nif (r)\r\ndm_bio_prison_free_cell(pool->prison, cell_prealloc);\r\nreturn r;\r\n}\r\nstatic void cell_release(struct pool *pool,\r\nstruct dm_bio_prison_cell *cell,\r\nstruct bio_list *bios)\r\n{\r\ndm_cell_release(pool->prison, cell, bios);\r\ndm_bio_prison_free_cell(pool->prison, cell);\r\n}\r\nstatic void cell_visit_release(struct pool *pool,\r\nvoid (*fn)(void *, struct dm_bio_prison_cell *),\r\nvoid *context,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\ndm_cell_visit_release(pool->prison, fn, context, cell);\r\ndm_bio_prison_free_cell(pool->prison, cell);\r\n}\r\nstatic void cell_release_no_holder(struct pool *pool,\r\nstruct dm_bio_prison_cell *cell,\r\nstruct bio_list *bios)\r\n{\r\ndm_cell_release_no_holder(pool->prison, cell, bios);\r\ndm_bio_prison_free_cell(pool->prison, cell);\r\n}\r\nstatic void cell_error_with_code(struct pool *pool,\r\nstruct dm_bio_prison_cell *cell, int error_code)\r\n{\r\ndm_cell_error(pool->prison, cell, error_code);\r\ndm_bio_prison_free_cell(pool->prison, cell);\r\n}\r\nstatic void cell_error(struct pool *pool, struct dm_bio_prison_cell *cell)\r\n{\r\ncell_error_with_code(pool, cell, -EIO);\r\n}\r\nstatic void cell_success(struct pool *pool, struct dm_bio_prison_cell *cell)\r\n{\r\ncell_error_with_code(pool, cell, 0);\r\n}\r\nstatic void cell_requeue(struct pool *pool, struct dm_bio_prison_cell *cell)\r\n{\r\ncell_error_with_code(pool, cell, DM_ENDIO_REQUEUE);\r\n}\r\nstatic void pool_table_init(void)\r\n{\r\nmutex_init(&dm_thin_pool_table.mutex);\r\nINIT_LIST_HEAD(&dm_thin_pool_table.pools);\r\n}\r\nstatic void __pool_table_insert(struct pool *pool)\r\n{\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nlist_add(&pool->list, &dm_thin_pool_table.pools);\r\n}\r\nstatic void __pool_table_remove(struct pool *pool)\r\n{\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nlist_del(&pool->list);\r\n}\r\nstatic struct pool *__pool_table_lookup(struct mapped_device *md)\r\n{\r\nstruct pool *pool = NULL, *tmp;\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nlist_for_each_entry(tmp, &dm_thin_pool_table.pools, list) {\r\nif (tmp->pool_md == md) {\r\npool = tmp;\r\nbreak;\r\n}\r\n}\r\nreturn pool;\r\n}\r\nstatic struct pool *__pool_table_lookup_metadata_dev(struct block_device *md_dev)\r\n{\r\nstruct pool *pool = NULL, *tmp;\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nlist_for_each_entry(tmp, &dm_thin_pool_table.pools, list) {\r\nif (tmp->md_dev == md_dev) {\r\npool = tmp;\r\nbreak;\r\n}\r\n}\r\nreturn pool;\r\n}\r\nstatic void __merge_bio_list(struct bio_list *bios, struct bio_list *master)\r\n{\r\nbio_list_merge(bios, master);\r\nbio_list_init(master);\r\n}\r\nstatic void error_bio_list(struct bio_list *bios, int error)\r\n{\r\nstruct bio *bio;\r\nwhile ((bio = bio_list_pop(bios)))\r\nbio_endio(bio, error);\r\n}\r\nstatic void error_thin_bio_list(struct thin_c *tc, struct bio_list *master, int error)\r\n{\r\nstruct bio_list bios;\r\nunsigned long flags;\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&tc->lock, flags);\r\n__merge_bio_list(&bios, master);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nerror_bio_list(&bios, error);\r\n}\r\nstatic void requeue_deferred_cells(struct thin_c *tc)\r\n{\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nstruct list_head cells;\r\nstruct dm_bio_prison_cell *cell, *tmp;\r\nINIT_LIST_HEAD(&cells);\r\nspin_lock_irqsave(&tc->lock, flags);\r\nlist_splice_init(&tc->deferred_cells, &cells);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nlist_for_each_entry_safe(cell, tmp, &cells, user_list)\r\ncell_requeue(pool, cell);\r\n}\r\nstatic void requeue_io(struct thin_c *tc)\r\n{\r\nstruct bio_list bios;\r\nunsigned long flags;\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&tc->lock, flags);\r\n__merge_bio_list(&bios, &tc->deferred_bio_list);\r\n__merge_bio_list(&bios, &tc->retry_on_resume_list);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nerror_bio_list(&bios, DM_ENDIO_REQUEUE);\r\nrequeue_deferred_cells(tc);\r\n}\r\nstatic void error_retry_list(struct pool *pool)\r\n{\r\nstruct thin_c *tc;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(tc, &pool->active_thins, list)\r\nerror_thin_bio_list(tc, &tc->retry_on_resume_list, -EIO);\r\nrcu_read_unlock();\r\n}\r\nstatic bool block_size_is_power_of_two(struct pool *pool)\r\n{\r\nreturn pool->sectors_per_block_shift >= 0;\r\n}\r\nstatic dm_block_t get_bio_block(struct thin_c *tc, struct bio *bio)\r\n{\r\nstruct pool *pool = tc->pool;\r\nsector_t block_nr = bio->bi_iter.bi_sector;\r\nif (block_size_is_power_of_two(pool))\r\nblock_nr >>= pool->sectors_per_block_shift;\r\nelse\r\n(void) sector_div(block_nr, pool->sectors_per_block);\r\nreturn block_nr;\r\n}\r\nstatic void remap(struct thin_c *tc, struct bio *bio, dm_block_t block)\r\n{\r\nstruct pool *pool = tc->pool;\r\nsector_t bi_sector = bio->bi_iter.bi_sector;\r\nbio->bi_bdev = tc->pool_dev->bdev;\r\nif (block_size_is_power_of_two(pool))\r\nbio->bi_iter.bi_sector =\r\n(block << pool->sectors_per_block_shift) |\r\n(bi_sector & (pool->sectors_per_block - 1));\r\nelse\r\nbio->bi_iter.bi_sector = (block * pool->sectors_per_block) +\r\nsector_div(bi_sector, pool->sectors_per_block);\r\n}\r\nstatic void remap_to_origin(struct thin_c *tc, struct bio *bio)\r\n{\r\nbio->bi_bdev = tc->origin_dev->bdev;\r\n}\r\nstatic int bio_triggers_commit(struct thin_c *tc, struct bio *bio)\r\n{\r\nreturn (bio->bi_rw & (REQ_FLUSH | REQ_FUA)) &&\r\ndm_thin_changed_this_transaction(tc->td);\r\n}\r\nstatic void inc_all_io_entry(struct pool *pool, struct bio *bio)\r\n{\r\nstruct dm_thin_endio_hook *h;\r\nif (bio->bi_rw & REQ_DISCARD)\r\nreturn;\r\nh = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\r\nh->all_io_entry = dm_deferred_entry_inc(pool->all_io_ds);\r\n}\r\nstatic void issue(struct thin_c *tc, struct bio *bio)\r\n{\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nif (!bio_triggers_commit(tc, bio)) {\r\ngeneric_make_request(bio);\r\nreturn;\r\n}\r\nif (dm_thin_aborted_changes(tc->td)) {\r\nbio_io_error(bio);\r\nreturn;\r\n}\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio_list_add(&pool->deferred_flush_bios, bio);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic void remap_to_origin_and_issue(struct thin_c *tc, struct bio *bio)\r\n{\r\nremap_to_origin(tc, bio);\r\nissue(tc, bio);\r\n}\r\nstatic void remap_and_issue(struct thin_c *tc, struct bio *bio,\r\ndm_block_t block)\r\n{\r\nremap(tc, bio, block);\r\nissue(tc, bio);\r\n}\r\nstatic void __complete_mapping_preparation(struct dm_thin_new_mapping *m)\r\n{\r\nstruct pool *pool = m->tc->pool;\r\nif (atomic_dec_and_test(&m->prepare_actions)) {\r\nlist_add_tail(&m->list, &pool->prepared_mappings);\r\nwake_worker(pool);\r\n}\r\n}\r\nstatic void complete_mapping_preparation(struct dm_thin_new_mapping *m)\r\n{\r\nunsigned long flags;\r\nstruct pool *pool = m->tc->pool;\r\nspin_lock_irqsave(&pool->lock, flags);\r\n__complete_mapping_preparation(m);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic void copy_complete(int read_err, unsigned long write_err, void *context)\r\n{\r\nstruct dm_thin_new_mapping *m = context;\r\nm->err = read_err || write_err ? -EIO : 0;\r\ncomplete_mapping_preparation(m);\r\n}\r\nstatic void overwrite_endio(struct bio *bio, int err)\r\n{\r\nstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\r\nstruct dm_thin_new_mapping *m = h->overwrite_mapping;\r\nm->err = err;\r\ncomplete_mapping_preparation(m);\r\n}\r\nstatic void cell_defer_no_holder(struct thin_c *tc, struct dm_bio_prison_cell *cell)\r\n{\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nspin_lock_irqsave(&tc->lock, flags);\r\ncell_release_no_holder(pool, cell, &tc->deferred_bio_list);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nwake_worker(pool);\r\n}\r\nstatic void __inc_remap_and_issue_cell(void *context,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nstruct remap_info *info = context;\r\nstruct bio *bio;\r\nwhile ((bio = bio_list_pop(&cell->bios))) {\r\nif (bio->bi_rw & (REQ_DISCARD | REQ_FLUSH | REQ_FUA))\r\nbio_list_add(&info->defer_bios, bio);\r\nelse {\r\ninc_all_io_entry(info->tc->pool, bio);\r\nbio_list_add(&info->issue_bios, bio);\r\n}\r\n}\r\n}\r\nstatic void inc_remap_and_issue_cell(struct thin_c *tc,\r\nstruct dm_bio_prison_cell *cell,\r\ndm_block_t block)\r\n{\r\nstruct bio *bio;\r\nstruct remap_info info;\r\ninfo.tc = tc;\r\nbio_list_init(&info.defer_bios);\r\nbio_list_init(&info.issue_bios);\r\ncell_visit_release(tc->pool, __inc_remap_and_issue_cell,\r\n&info, cell);\r\nwhile ((bio = bio_list_pop(&info.defer_bios)))\r\nthin_defer_bio(tc, bio);\r\nwhile ((bio = bio_list_pop(&info.issue_bios)))\r\nremap_and_issue(info.tc, bio, block);\r\n}\r\nstatic void process_prepared_mapping_fail(struct dm_thin_new_mapping *m)\r\n{\r\nif (m->bio) {\r\nm->bio->bi_end_io = m->saved_bi_end_io;\r\natomic_inc(&m->bio->bi_remaining);\r\n}\r\ncell_error(m->tc->pool, m->cell);\r\nlist_del(&m->list);\r\nmempool_free(m, m->tc->pool->mapping_pool);\r\n}\r\nstatic void process_prepared_mapping(struct dm_thin_new_mapping *m)\r\n{\r\nstruct thin_c *tc = m->tc;\r\nstruct pool *pool = tc->pool;\r\nstruct bio *bio;\r\nint r;\r\nbio = m->bio;\r\nif (bio) {\r\nbio->bi_end_io = m->saved_bi_end_io;\r\natomic_inc(&bio->bi_remaining);\r\n}\r\nif (m->err) {\r\ncell_error(pool, m->cell);\r\ngoto out;\r\n}\r\nr = dm_thin_insert_block(tc->td, m->virt_block, m->data_block);\r\nif (r) {\r\nmetadata_operation_failed(pool, "dm_thin_insert_block", r);\r\ncell_error(pool, m->cell);\r\ngoto out;\r\n}\r\nif (bio) {\r\ninc_remap_and_issue_cell(tc, m->cell, m->data_block);\r\nbio_endio(bio, 0);\r\n} else {\r\ninc_all_io_entry(tc->pool, m->cell->holder);\r\nremap_and_issue(tc, m->cell->holder, m->data_block);\r\ninc_remap_and_issue_cell(tc, m->cell, m->data_block);\r\n}\r\nout:\r\nlist_del(&m->list);\r\nmempool_free(m, pool->mapping_pool);\r\n}\r\nstatic void process_prepared_discard_fail(struct dm_thin_new_mapping *m)\r\n{\r\nstruct thin_c *tc = m->tc;\r\nbio_io_error(m->bio);\r\ncell_defer_no_holder(tc, m->cell);\r\ncell_defer_no_holder(tc, m->cell2);\r\nmempool_free(m, tc->pool->mapping_pool);\r\n}\r\nstatic void process_prepared_discard_passdown(struct dm_thin_new_mapping *m)\r\n{\r\nstruct thin_c *tc = m->tc;\r\ninc_all_io_entry(tc->pool, m->bio);\r\ncell_defer_no_holder(tc, m->cell);\r\ncell_defer_no_holder(tc, m->cell2);\r\nif (m->pass_discard)\r\nif (m->definitely_not_shared)\r\nremap_and_issue(tc, m->bio, m->data_block);\r\nelse {\r\nbool used = false;\r\nif (dm_pool_block_is_used(tc->pool->pmd, m->data_block, &used) || used)\r\nbio_endio(m->bio, 0);\r\nelse\r\nremap_and_issue(tc, m->bio, m->data_block);\r\n}\r\nelse\r\nbio_endio(m->bio, 0);\r\nmempool_free(m, tc->pool->mapping_pool);\r\n}\r\nstatic void process_prepared_discard(struct dm_thin_new_mapping *m)\r\n{\r\nint r;\r\nstruct thin_c *tc = m->tc;\r\nr = dm_thin_remove_block(tc->td, m->virt_block);\r\nif (r)\r\nDMERR_LIMIT("dm_thin_remove_block() failed");\r\nprocess_prepared_discard_passdown(m);\r\n}\r\nstatic void process_prepared(struct pool *pool, struct list_head *head,\r\nprocess_mapping_fn *fn)\r\n{\r\nunsigned long flags;\r\nstruct list_head maps;\r\nstruct dm_thin_new_mapping *m, *tmp;\r\nINIT_LIST_HEAD(&maps);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nlist_splice_init(head, &maps);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nlist_for_each_entry_safe(m, tmp, &maps, list)\r\n(*fn)(m);\r\n}\r\nstatic int io_overlaps_block(struct pool *pool, struct bio *bio)\r\n{\r\nreturn bio->bi_iter.bi_size ==\r\n(pool->sectors_per_block << SECTOR_SHIFT);\r\n}\r\nstatic int io_overwrites_block(struct pool *pool, struct bio *bio)\r\n{\r\nreturn (bio_data_dir(bio) == WRITE) &&\r\nio_overlaps_block(pool, bio);\r\n}\r\nstatic void save_and_set_endio(struct bio *bio, bio_end_io_t **save,\r\nbio_end_io_t *fn)\r\n{\r\n*save = bio->bi_end_io;\r\nbio->bi_end_io = fn;\r\n}\r\nstatic int ensure_next_mapping(struct pool *pool)\r\n{\r\nif (pool->next_mapping)\r\nreturn 0;\r\npool->next_mapping = mempool_alloc(pool->mapping_pool, GFP_ATOMIC);\r\nreturn pool->next_mapping ? 0 : -ENOMEM;\r\n}\r\nstatic struct dm_thin_new_mapping *get_next_mapping(struct pool *pool)\r\n{\r\nstruct dm_thin_new_mapping *m = pool->next_mapping;\r\nBUG_ON(!pool->next_mapping);\r\nmemset(m, 0, sizeof(struct dm_thin_new_mapping));\r\nINIT_LIST_HEAD(&m->list);\r\nm->bio = NULL;\r\npool->next_mapping = NULL;\r\nreturn m;\r\n}\r\nstatic void ll_zero(struct thin_c *tc, struct dm_thin_new_mapping *m,\r\nsector_t begin, sector_t end)\r\n{\r\nint r;\r\nstruct dm_io_region to;\r\nto.bdev = tc->pool_dev->bdev;\r\nto.sector = begin;\r\nto.count = end - begin;\r\nr = dm_kcopyd_zero(tc->pool->copier, 1, &to, 0, copy_complete, m);\r\nif (r < 0) {\r\nDMERR_LIMIT("dm_kcopyd_zero() failed");\r\ncopy_complete(1, 1, m);\r\n}\r\n}\r\nstatic void remap_and_issue_overwrite(struct thin_c *tc, struct bio *bio,\r\ndm_block_t data_block,\r\nstruct dm_thin_new_mapping *m)\r\n{\r\nstruct pool *pool = tc->pool;\r\nstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\r\nh->overwrite_mapping = m;\r\nm->bio = bio;\r\nsave_and_set_endio(bio, &m->saved_bi_end_io, overwrite_endio);\r\ninc_all_io_entry(pool, bio);\r\nremap_and_issue(tc, bio, data_block);\r\n}\r\nstatic void schedule_copy(struct thin_c *tc, dm_block_t virt_block,\r\nstruct dm_dev *origin, dm_block_t data_origin,\r\ndm_block_t data_dest,\r\nstruct dm_bio_prison_cell *cell, struct bio *bio,\r\nsector_t len)\r\n{\r\nint r;\r\nstruct pool *pool = tc->pool;\r\nstruct dm_thin_new_mapping *m = get_next_mapping(pool);\r\nm->tc = tc;\r\nm->virt_block = virt_block;\r\nm->data_block = data_dest;\r\nm->cell = cell;\r\natomic_set(&m->prepare_actions, 3);\r\nif (!dm_deferred_set_add_work(pool->shared_read_ds, &m->list))\r\ncomplete_mapping_preparation(m);\r\nif (io_overwrites_block(pool, bio))\r\nremap_and_issue_overwrite(tc, bio, data_dest, m);\r\nelse {\r\nstruct dm_io_region from, to;\r\nfrom.bdev = origin->bdev;\r\nfrom.sector = data_origin * pool->sectors_per_block;\r\nfrom.count = len;\r\nto.bdev = tc->pool_dev->bdev;\r\nto.sector = data_dest * pool->sectors_per_block;\r\nto.count = len;\r\nr = dm_kcopyd_copy(pool->copier, &from, 1, &to,\r\n0, copy_complete, m);\r\nif (r < 0) {\r\nDMERR_LIMIT("dm_kcopyd_copy() failed");\r\ncopy_complete(1, 1, m);\r\n}\r\nif (len < pool->sectors_per_block && pool->pf.zero_new_blocks) {\r\natomic_inc(&m->prepare_actions);\r\nll_zero(tc, m,\r\ndata_dest * pool->sectors_per_block + len,\r\n(data_dest + 1) * pool->sectors_per_block);\r\n}\r\n}\r\ncomplete_mapping_preparation(m);\r\n}\r\nstatic void schedule_internal_copy(struct thin_c *tc, dm_block_t virt_block,\r\ndm_block_t data_origin, dm_block_t data_dest,\r\nstruct dm_bio_prison_cell *cell, struct bio *bio)\r\n{\r\nschedule_copy(tc, virt_block, tc->pool_dev,\r\ndata_origin, data_dest, cell, bio,\r\ntc->pool->sectors_per_block);\r\n}\r\nstatic void schedule_zero(struct thin_c *tc, dm_block_t virt_block,\r\ndm_block_t data_block, struct dm_bio_prison_cell *cell,\r\nstruct bio *bio)\r\n{\r\nstruct pool *pool = tc->pool;\r\nstruct dm_thin_new_mapping *m = get_next_mapping(pool);\r\natomic_set(&m->prepare_actions, 1);\r\nm->tc = tc;\r\nm->virt_block = virt_block;\r\nm->data_block = data_block;\r\nm->cell = cell;\r\nif (!pool->pf.zero_new_blocks)\r\nprocess_prepared_mapping(m);\r\nelse if (io_overwrites_block(pool, bio))\r\nremap_and_issue_overwrite(tc, bio, data_block, m);\r\nelse\r\nll_zero(tc, m,\r\ndata_block * pool->sectors_per_block,\r\n(data_block + 1) * pool->sectors_per_block);\r\n}\r\nstatic void schedule_external_copy(struct thin_c *tc, dm_block_t virt_block,\r\ndm_block_t data_dest,\r\nstruct dm_bio_prison_cell *cell, struct bio *bio)\r\n{\r\nstruct pool *pool = tc->pool;\r\nsector_t virt_block_begin = virt_block * pool->sectors_per_block;\r\nsector_t virt_block_end = (virt_block + 1) * pool->sectors_per_block;\r\nif (virt_block_end <= tc->origin_size)\r\nschedule_copy(tc, virt_block, tc->origin_dev,\r\nvirt_block, data_dest, cell, bio,\r\npool->sectors_per_block);\r\nelse if (virt_block_begin < tc->origin_size)\r\nschedule_copy(tc, virt_block, tc->origin_dev,\r\nvirt_block, data_dest, cell, bio,\r\ntc->origin_size - virt_block_begin);\r\nelse\r\nschedule_zero(tc, virt_block, data_dest, cell, bio);\r\n}\r\nstatic void check_for_space(struct pool *pool)\r\n{\r\nint r;\r\ndm_block_t nr_free;\r\nif (get_pool_mode(pool) != PM_OUT_OF_DATA_SPACE)\r\nreturn;\r\nr = dm_pool_get_free_block_count(pool->pmd, &nr_free);\r\nif (r)\r\nreturn;\r\nif (nr_free)\r\nset_pool_mode(pool, PM_WRITE);\r\n}\r\nstatic int commit(struct pool *pool)\r\n{\r\nint r;\r\nif (get_pool_mode(pool) >= PM_READ_ONLY)\r\nreturn -EINVAL;\r\nr = dm_pool_commit_metadata(pool->pmd);\r\nif (r)\r\nmetadata_operation_failed(pool, "dm_pool_commit_metadata", r);\r\nelse\r\ncheck_for_space(pool);\r\nreturn r;\r\n}\r\nstatic void check_low_water_mark(struct pool *pool, dm_block_t free_blocks)\r\n{\r\nunsigned long flags;\r\nif (free_blocks <= pool->low_water_blocks && !pool->low_water_triggered) {\r\nDMWARN("%s: reached low water mark for data device: sending event.",\r\ndm_device_name(pool->pool_md));\r\nspin_lock_irqsave(&pool->lock, flags);\r\npool->low_water_triggered = true;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\ndm_table_event(pool->ti->table);\r\n}\r\n}\r\nstatic int alloc_data_block(struct thin_c *tc, dm_block_t *result)\r\n{\r\nint r;\r\ndm_block_t free_blocks;\r\nstruct pool *pool = tc->pool;\r\nif (WARN_ON(get_pool_mode(pool) != PM_WRITE))\r\nreturn -EINVAL;\r\nr = dm_pool_get_free_block_count(pool->pmd, &free_blocks);\r\nif (r) {\r\nmetadata_operation_failed(pool, "dm_pool_get_free_block_count", r);\r\nreturn r;\r\n}\r\ncheck_low_water_mark(pool, free_blocks);\r\nif (!free_blocks) {\r\nr = commit(pool);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_get_free_block_count(pool->pmd, &free_blocks);\r\nif (r) {\r\nmetadata_operation_failed(pool, "dm_pool_get_free_block_count", r);\r\nreturn r;\r\n}\r\nif (!free_blocks) {\r\nset_pool_mode(pool, PM_OUT_OF_DATA_SPACE);\r\nreturn -ENOSPC;\r\n}\r\n}\r\nr = dm_pool_alloc_data_block(pool->pmd, result);\r\nif (r) {\r\nmetadata_operation_failed(pool, "dm_pool_alloc_data_block", r);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic void retry_on_resume(struct bio *bio)\r\n{\r\nstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\r\nstruct thin_c *tc = h->tc;\r\nunsigned long flags;\r\nspin_lock_irqsave(&tc->lock, flags);\r\nbio_list_add(&tc->retry_on_resume_list, bio);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\n}\r\nstatic int should_error_unserviceable_bio(struct pool *pool)\r\n{\r\nenum pool_mode m = get_pool_mode(pool);\r\nswitch (m) {\r\ncase PM_WRITE:\r\nDMERR_LIMIT("bio unserviceable, yet pool is in PM_WRITE mode");\r\nreturn -EIO;\r\ncase PM_OUT_OF_DATA_SPACE:\r\nreturn pool->pf.error_if_no_space ? -ENOSPC : 0;\r\ncase PM_READ_ONLY:\r\ncase PM_FAIL:\r\nreturn -EIO;\r\ndefault:\r\nDMERR_LIMIT("bio unserviceable, yet pool has an unknown mode");\r\nreturn -EIO;\r\n}\r\n}\r\nstatic void handle_unserviceable_bio(struct pool *pool, struct bio *bio)\r\n{\r\nint error = should_error_unserviceable_bio(pool);\r\nif (error)\r\nbio_endio(bio, error);\r\nelse\r\nretry_on_resume(bio);\r\n}\r\nstatic void retry_bios_on_resume(struct pool *pool, struct dm_bio_prison_cell *cell)\r\n{\r\nstruct bio *bio;\r\nstruct bio_list bios;\r\nint error;\r\nerror = should_error_unserviceable_bio(pool);\r\nif (error) {\r\ncell_error_with_code(pool, cell, error);\r\nreturn;\r\n}\r\nbio_list_init(&bios);\r\ncell_release(pool, cell, &bios);\r\nwhile ((bio = bio_list_pop(&bios)))\r\nretry_on_resume(bio);\r\n}\r\nstatic void process_discard_cell(struct thin_c *tc, struct dm_bio_prison_cell *cell)\r\n{\r\nint r;\r\nstruct bio *bio = cell->holder;\r\nstruct pool *pool = tc->pool;\r\nstruct dm_bio_prison_cell *cell2;\r\nstruct dm_cell_key key2;\r\ndm_block_t block = get_bio_block(tc, bio);\r\nstruct dm_thin_lookup_result lookup_result;\r\nstruct dm_thin_new_mapping *m;\r\nif (tc->requeue_mode) {\r\ncell_requeue(pool, cell);\r\nreturn;\r\n}\r\nr = dm_thin_find_block(tc->td, block, 1, &lookup_result);\r\nswitch (r) {\r\ncase 0:\r\nbuild_data_key(tc->td, lookup_result.block, &key2);\r\nif (bio_detain(tc->pool, &key2, bio, &cell2)) {\r\ncell_defer_no_holder(tc, cell);\r\nbreak;\r\n}\r\nif (io_overlaps_block(pool, bio)) {\r\nm = get_next_mapping(pool);\r\nm->tc = tc;\r\nm->pass_discard = pool->pf.discard_passdown;\r\nm->definitely_not_shared = !lookup_result.shared;\r\nm->virt_block = block;\r\nm->data_block = lookup_result.block;\r\nm->cell = cell;\r\nm->cell2 = cell2;\r\nm->bio = bio;\r\nif (!dm_deferred_set_add_work(pool->all_io_ds, &m->list))\r\npool->process_prepared_discard(m);\r\n} else {\r\ninc_all_io_entry(pool, bio);\r\ncell_defer_no_holder(tc, cell);\r\ncell_defer_no_holder(tc, cell2);\r\nif ((!lookup_result.shared) && pool->pf.discard_passdown)\r\nremap_and_issue(tc, bio, lookup_result.block);\r\nelse\r\nbio_endio(bio, 0);\r\n}\r\nbreak;\r\ncase -ENODATA:\r\ncell_defer_no_holder(tc, cell);\r\nbio_endio(bio, 0);\r\nbreak;\r\ndefault:\r\nDMERR_LIMIT("%s: dm_thin_find_block() failed: error = %d",\r\n__func__, r);\r\ncell_defer_no_holder(tc, cell);\r\nbio_io_error(bio);\r\nbreak;\r\n}\r\n}\r\nstatic void process_discard_bio(struct thin_c *tc, struct bio *bio)\r\n{\r\nstruct dm_bio_prison_cell *cell;\r\nstruct dm_cell_key key;\r\ndm_block_t block = get_bio_block(tc, bio);\r\nbuild_virtual_key(tc->td, block, &key);\r\nif (bio_detain(tc->pool, &key, bio, &cell))\r\nreturn;\r\nprocess_discard_cell(tc, cell);\r\n}\r\nstatic void break_sharing(struct thin_c *tc, struct bio *bio, dm_block_t block,\r\nstruct dm_cell_key *key,\r\nstruct dm_thin_lookup_result *lookup_result,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nint r;\r\ndm_block_t data_block;\r\nstruct pool *pool = tc->pool;\r\nr = alloc_data_block(tc, &data_block);\r\nswitch (r) {\r\ncase 0:\r\nschedule_internal_copy(tc, block, lookup_result->block,\r\ndata_block, cell, bio);\r\nbreak;\r\ncase -ENOSPC:\r\nretry_bios_on_resume(pool, cell);\r\nbreak;\r\ndefault:\r\nDMERR_LIMIT("%s: alloc_data_block() failed: error = %d",\r\n__func__, r);\r\ncell_error(pool, cell);\r\nbreak;\r\n}\r\n}\r\nstatic void __remap_and_issue_shared_cell(void *context,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nstruct remap_info *info = context;\r\nstruct bio *bio;\r\nwhile ((bio = bio_list_pop(&cell->bios))) {\r\nif ((bio_data_dir(bio) == WRITE) ||\r\n(bio->bi_rw & (REQ_DISCARD | REQ_FLUSH | REQ_FUA)))\r\nbio_list_add(&info->defer_bios, bio);\r\nelse {\r\nstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));;\r\nh->shared_read_entry = dm_deferred_entry_inc(info->tc->pool->shared_read_ds);\r\ninc_all_io_entry(info->tc->pool, bio);\r\nbio_list_add(&info->issue_bios, bio);\r\n}\r\n}\r\n}\r\nstatic void remap_and_issue_shared_cell(struct thin_c *tc,\r\nstruct dm_bio_prison_cell *cell,\r\ndm_block_t block)\r\n{\r\nstruct bio *bio;\r\nstruct remap_info info;\r\ninfo.tc = tc;\r\nbio_list_init(&info.defer_bios);\r\nbio_list_init(&info.issue_bios);\r\ncell_visit_release(tc->pool, __remap_and_issue_shared_cell,\r\n&info, cell);\r\nwhile ((bio = bio_list_pop(&info.defer_bios)))\r\nthin_defer_bio(tc, bio);\r\nwhile ((bio = bio_list_pop(&info.issue_bios)))\r\nremap_and_issue(tc, bio, block);\r\n}\r\nstatic void process_shared_bio(struct thin_c *tc, struct bio *bio,\r\ndm_block_t block,\r\nstruct dm_thin_lookup_result *lookup_result,\r\nstruct dm_bio_prison_cell *virt_cell)\r\n{\r\nstruct dm_bio_prison_cell *data_cell;\r\nstruct pool *pool = tc->pool;\r\nstruct dm_cell_key key;\r\nbuild_data_key(tc->td, lookup_result->block, &key);\r\nif (bio_detain(pool, &key, bio, &data_cell)) {\r\ncell_defer_no_holder(tc, virt_cell);\r\nreturn;\r\n}\r\nif (bio_data_dir(bio) == WRITE && bio->bi_iter.bi_size) {\r\nbreak_sharing(tc, bio, block, &key, lookup_result, data_cell);\r\ncell_defer_no_holder(tc, virt_cell);\r\n} else {\r\nstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\r\nh->shared_read_entry = dm_deferred_entry_inc(pool->shared_read_ds);\r\ninc_all_io_entry(pool, bio);\r\nremap_and_issue(tc, bio, lookup_result->block);\r\nremap_and_issue_shared_cell(tc, data_cell, lookup_result->block);\r\nremap_and_issue_shared_cell(tc, virt_cell, lookup_result->block);\r\n}\r\n}\r\nstatic void provision_block(struct thin_c *tc, struct bio *bio, dm_block_t block,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nint r;\r\ndm_block_t data_block;\r\nstruct pool *pool = tc->pool;\r\nif (!bio->bi_iter.bi_size) {\r\ninc_all_io_entry(pool, bio);\r\ncell_defer_no_holder(tc, cell);\r\nremap_and_issue(tc, bio, 0);\r\nreturn;\r\n}\r\nif (bio_data_dir(bio) == READ) {\r\nzero_fill_bio(bio);\r\ncell_defer_no_holder(tc, cell);\r\nbio_endio(bio, 0);\r\nreturn;\r\n}\r\nr = alloc_data_block(tc, &data_block);\r\nswitch (r) {\r\ncase 0:\r\nif (tc->origin_dev)\r\nschedule_external_copy(tc, block, data_block, cell, bio);\r\nelse\r\nschedule_zero(tc, block, data_block, cell, bio);\r\nbreak;\r\ncase -ENOSPC:\r\nretry_bios_on_resume(pool, cell);\r\nbreak;\r\ndefault:\r\nDMERR_LIMIT("%s: alloc_data_block() failed: error = %d",\r\n__func__, r);\r\ncell_error(pool, cell);\r\nbreak;\r\n}\r\n}\r\nstatic void process_cell(struct thin_c *tc, struct dm_bio_prison_cell *cell)\r\n{\r\nint r;\r\nstruct pool *pool = tc->pool;\r\nstruct bio *bio = cell->holder;\r\ndm_block_t block = get_bio_block(tc, bio);\r\nstruct dm_thin_lookup_result lookup_result;\r\nif (tc->requeue_mode) {\r\ncell_requeue(pool, cell);\r\nreturn;\r\n}\r\nr = dm_thin_find_block(tc->td, block, 1, &lookup_result);\r\nswitch (r) {\r\ncase 0:\r\nif (lookup_result.shared)\r\nprocess_shared_bio(tc, bio, block, &lookup_result, cell);\r\nelse {\r\ninc_all_io_entry(pool, bio);\r\nremap_and_issue(tc, bio, lookup_result.block);\r\ninc_remap_and_issue_cell(tc, cell, lookup_result.block);\r\n}\r\nbreak;\r\ncase -ENODATA:\r\nif (bio_data_dir(bio) == READ && tc->origin_dev) {\r\ninc_all_io_entry(pool, bio);\r\ncell_defer_no_holder(tc, cell);\r\nif (bio_end_sector(bio) <= tc->origin_size)\r\nremap_to_origin_and_issue(tc, bio);\r\nelse if (bio->bi_iter.bi_sector < tc->origin_size) {\r\nzero_fill_bio(bio);\r\nbio->bi_iter.bi_size = (tc->origin_size - bio->bi_iter.bi_sector) << SECTOR_SHIFT;\r\nremap_to_origin_and_issue(tc, bio);\r\n} else {\r\nzero_fill_bio(bio);\r\nbio_endio(bio, 0);\r\n}\r\n} else\r\nprovision_block(tc, bio, block, cell);\r\nbreak;\r\ndefault:\r\nDMERR_LIMIT("%s: dm_thin_find_block() failed: error = %d",\r\n__func__, r);\r\ncell_defer_no_holder(tc, cell);\r\nbio_io_error(bio);\r\nbreak;\r\n}\r\n}\r\nstatic void process_bio(struct thin_c *tc, struct bio *bio)\r\n{\r\nstruct pool *pool = tc->pool;\r\ndm_block_t block = get_bio_block(tc, bio);\r\nstruct dm_bio_prison_cell *cell;\r\nstruct dm_cell_key key;\r\nbuild_virtual_key(tc->td, block, &key);\r\nif (bio_detain(pool, &key, bio, &cell))\r\nreturn;\r\nprocess_cell(tc, cell);\r\n}\r\nstatic void __process_bio_read_only(struct thin_c *tc, struct bio *bio,\r\nstruct dm_bio_prison_cell *cell)\r\n{\r\nint r;\r\nint rw = bio_data_dir(bio);\r\ndm_block_t block = get_bio_block(tc, bio);\r\nstruct dm_thin_lookup_result lookup_result;\r\nr = dm_thin_find_block(tc->td, block, 1, &lookup_result);\r\nswitch (r) {\r\ncase 0:\r\nif (lookup_result.shared && (rw == WRITE) && bio->bi_iter.bi_size) {\r\nhandle_unserviceable_bio(tc->pool, bio);\r\nif (cell)\r\ncell_defer_no_holder(tc, cell);\r\n} else {\r\ninc_all_io_entry(tc->pool, bio);\r\nremap_and_issue(tc, bio, lookup_result.block);\r\nif (cell)\r\ninc_remap_and_issue_cell(tc, cell, lookup_result.block);\r\n}\r\nbreak;\r\ncase -ENODATA:\r\nif (cell)\r\ncell_defer_no_holder(tc, cell);\r\nif (rw != READ) {\r\nhandle_unserviceable_bio(tc->pool, bio);\r\nbreak;\r\n}\r\nif (tc->origin_dev) {\r\ninc_all_io_entry(tc->pool, bio);\r\nremap_to_origin_and_issue(tc, bio);\r\nbreak;\r\n}\r\nzero_fill_bio(bio);\r\nbio_endio(bio, 0);\r\nbreak;\r\ndefault:\r\nDMERR_LIMIT("%s: dm_thin_find_block() failed: error = %d",\r\n__func__, r);\r\nif (cell)\r\ncell_defer_no_holder(tc, cell);\r\nbio_io_error(bio);\r\nbreak;\r\n}\r\n}\r\nstatic void process_bio_read_only(struct thin_c *tc, struct bio *bio)\r\n{\r\n__process_bio_read_only(tc, bio, NULL);\r\n}\r\nstatic void process_cell_read_only(struct thin_c *tc, struct dm_bio_prison_cell *cell)\r\n{\r\n__process_bio_read_only(tc, cell->holder, cell);\r\n}\r\nstatic void process_bio_success(struct thin_c *tc, struct bio *bio)\r\n{\r\nbio_endio(bio, 0);\r\n}\r\nstatic void process_bio_fail(struct thin_c *tc, struct bio *bio)\r\n{\r\nbio_io_error(bio);\r\n}\r\nstatic void process_cell_success(struct thin_c *tc, struct dm_bio_prison_cell *cell)\r\n{\r\ncell_success(tc->pool, cell);\r\n}\r\nstatic void process_cell_fail(struct thin_c *tc, struct dm_bio_prison_cell *cell)\r\n{\r\ncell_error(tc->pool, cell);\r\n}\r\nstatic int need_commit_due_to_time(struct pool *pool)\r\n{\r\nreturn !time_in_range(jiffies, pool->last_commit_jiffies,\r\npool->last_commit_jiffies + COMMIT_PERIOD);\r\n}\r\nstatic void __thin_bio_rb_add(struct thin_c *tc, struct bio *bio)\r\n{\r\nstruct rb_node **rbp, *parent;\r\nstruct dm_thin_endio_hook *pbd;\r\nsector_t bi_sector = bio->bi_iter.bi_sector;\r\nrbp = &tc->sort_bio_list.rb_node;\r\nparent = NULL;\r\nwhile (*rbp) {\r\nparent = *rbp;\r\npbd = thin_pbd(parent);\r\nif (bi_sector < thin_bio(pbd)->bi_iter.bi_sector)\r\nrbp = &(*rbp)->rb_left;\r\nelse\r\nrbp = &(*rbp)->rb_right;\r\n}\r\npbd = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\r\nrb_link_node(&pbd->rb_node, parent, rbp);\r\nrb_insert_color(&pbd->rb_node, &tc->sort_bio_list);\r\n}\r\nstatic void __extract_sorted_bios(struct thin_c *tc)\r\n{\r\nstruct rb_node *node;\r\nstruct dm_thin_endio_hook *pbd;\r\nstruct bio *bio;\r\nfor (node = rb_first(&tc->sort_bio_list); node; node = rb_next(node)) {\r\npbd = thin_pbd(node);\r\nbio = thin_bio(pbd);\r\nbio_list_add(&tc->deferred_bio_list, bio);\r\nrb_erase(&pbd->rb_node, &tc->sort_bio_list);\r\n}\r\nWARN_ON(!RB_EMPTY_ROOT(&tc->sort_bio_list));\r\n}\r\nstatic void __sort_thin_deferred_bios(struct thin_c *tc)\r\n{\r\nstruct bio *bio;\r\nstruct bio_list bios;\r\nbio_list_init(&bios);\r\nbio_list_merge(&bios, &tc->deferred_bio_list);\r\nbio_list_init(&tc->deferred_bio_list);\r\nwhile ((bio = bio_list_pop(&bios)))\r\n__thin_bio_rb_add(tc, bio);\r\n__extract_sorted_bios(tc);\r\n}\r\nstatic void process_thin_deferred_bios(struct thin_c *tc)\r\n{\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nstruct bio *bio;\r\nstruct bio_list bios;\r\nstruct blk_plug plug;\r\nunsigned count = 0;\r\nif (tc->requeue_mode) {\r\nerror_thin_bio_list(tc, &tc->deferred_bio_list, DM_ENDIO_REQUEUE);\r\nreturn;\r\n}\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&tc->lock, flags);\r\nif (bio_list_empty(&tc->deferred_bio_list)) {\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nreturn;\r\n}\r\n__sort_thin_deferred_bios(tc);\r\nbio_list_merge(&bios, &tc->deferred_bio_list);\r\nbio_list_init(&tc->deferred_bio_list);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nblk_start_plug(&plug);\r\nwhile ((bio = bio_list_pop(&bios))) {\r\nif (ensure_next_mapping(pool)) {\r\nspin_lock_irqsave(&tc->lock, flags);\r\nbio_list_add(&tc->deferred_bio_list, bio);\r\nbio_list_merge(&tc->deferred_bio_list, &bios);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nbreak;\r\n}\r\nif (bio->bi_rw & REQ_DISCARD)\r\npool->process_discard(tc, bio);\r\nelse\r\npool->process_bio(tc, bio);\r\nif ((count++ & 127) == 0) {\r\nthrottle_work_update(&pool->throttle);\r\ndm_pool_issue_prefetches(pool->pmd);\r\n}\r\n}\r\nblk_finish_plug(&plug);\r\n}\r\nstatic int cmp_cells(const void *lhs, const void *rhs)\r\n{\r\nstruct dm_bio_prison_cell *lhs_cell = *((struct dm_bio_prison_cell **) lhs);\r\nstruct dm_bio_prison_cell *rhs_cell = *((struct dm_bio_prison_cell **) rhs);\r\nBUG_ON(!lhs_cell->holder);\r\nBUG_ON(!rhs_cell->holder);\r\nif (lhs_cell->holder->bi_iter.bi_sector < rhs_cell->holder->bi_iter.bi_sector)\r\nreturn -1;\r\nif (lhs_cell->holder->bi_iter.bi_sector > rhs_cell->holder->bi_iter.bi_sector)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic unsigned sort_cells(struct pool *pool, struct list_head *cells)\r\n{\r\nunsigned count = 0;\r\nstruct dm_bio_prison_cell *cell, *tmp;\r\nlist_for_each_entry_safe(cell, tmp, cells, user_list) {\r\nif (count >= CELL_SORT_ARRAY_SIZE)\r\nbreak;\r\npool->cell_sort_array[count++] = cell;\r\nlist_del(&cell->user_list);\r\n}\r\nsort(pool->cell_sort_array, count, sizeof(cell), cmp_cells, NULL);\r\nreturn count;\r\n}\r\nstatic void process_thin_deferred_cells(struct thin_c *tc)\r\n{\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nstruct list_head cells;\r\nstruct dm_bio_prison_cell *cell;\r\nunsigned i, j, count;\r\nINIT_LIST_HEAD(&cells);\r\nspin_lock_irqsave(&tc->lock, flags);\r\nlist_splice_init(&tc->deferred_cells, &cells);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nif (list_empty(&cells))\r\nreturn;\r\ndo {\r\ncount = sort_cells(tc->pool, &cells);\r\nfor (i = 0; i < count; i++) {\r\ncell = pool->cell_sort_array[i];\r\nBUG_ON(!cell->holder);\r\nif (ensure_next_mapping(pool)) {\r\nfor (j = i; j < count; j++)\r\nlist_add(&pool->cell_sort_array[j]->user_list, &cells);\r\nspin_lock_irqsave(&tc->lock, flags);\r\nlist_splice(&cells, &tc->deferred_cells);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nreturn;\r\n}\r\nif (cell->holder->bi_rw & REQ_DISCARD)\r\npool->process_discard_cell(tc, cell);\r\nelse\r\npool->process_cell(tc, cell);\r\n}\r\n} while (!list_empty(&cells));\r\n}\r\nstatic struct thin_c *get_first_thin(struct pool *pool)\r\n{\r\nstruct thin_c *tc = NULL;\r\nrcu_read_lock();\r\nif (!list_empty(&pool->active_thins)) {\r\ntc = list_entry_rcu(pool->active_thins.next, struct thin_c, list);\r\nthin_get(tc);\r\n}\r\nrcu_read_unlock();\r\nreturn tc;\r\n}\r\nstatic struct thin_c *get_next_thin(struct pool *pool, struct thin_c *tc)\r\n{\r\nstruct thin_c *old_tc = tc;\r\nrcu_read_lock();\r\nlist_for_each_entry_continue_rcu(tc, &pool->active_thins, list) {\r\nthin_get(tc);\r\nthin_put(old_tc);\r\nrcu_read_unlock();\r\nreturn tc;\r\n}\r\nthin_put(old_tc);\r\nrcu_read_unlock();\r\nreturn NULL;\r\n}\r\nstatic void process_deferred_bios(struct pool *pool)\r\n{\r\nunsigned long flags;\r\nstruct bio *bio;\r\nstruct bio_list bios;\r\nstruct thin_c *tc;\r\ntc = get_first_thin(pool);\r\nwhile (tc) {\r\nprocess_thin_deferred_cells(tc);\r\nprocess_thin_deferred_bios(tc);\r\ntc = get_next_thin(pool, tc);\r\n}\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio_list_merge(&bios, &pool->deferred_flush_bios);\r\nbio_list_init(&pool->deferred_flush_bios);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nif (bio_list_empty(&bios) &&\r\n!(dm_pool_changed_this_transaction(pool->pmd) && need_commit_due_to_time(pool)))\r\nreturn;\r\nif (commit(pool)) {\r\nwhile ((bio = bio_list_pop(&bios)))\r\nbio_io_error(bio);\r\nreturn;\r\n}\r\npool->last_commit_jiffies = jiffies;\r\nwhile ((bio = bio_list_pop(&bios)))\r\ngeneric_make_request(bio);\r\n}\r\nstatic void do_worker(struct work_struct *ws)\r\n{\r\nstruct pool *pool = container_of(ws, struct pool, worker);\r\nthrottle_work_start(&pool->throttle);\r\ndm_pool_issue_prefetches(pool->pmd);\r\nthrottle_work_update(&pool->throttle);\r\nprocess_prepared(pool, &pool->prepared_mappings, &pool->process_prepared_mapping);\r\nthrottle_work_update(&pool->throttle);\r\nprocess_prepared(pool, &pool->prepared_discards, &pool->process_prepared_discard);\r\nthrottle_work_update(&pool->throttle);\r\nprocess_deferred_bios(pool);\r\nthrottle_work_complete(&pool->throttle);\r\n}\r\nstatic void do_waker(struct work_struct *ws)\r\n{\r\nstruct pool *pool = container_of(to_delayed_work(ws), struct pool, waker);\r\nwake_worker(pool);\r\nqueue_delayed_work(pool->wq, &pool->waker, COMMIT_PERIOD);\r\n}\r\nstatic void do_no_space_timeout(struct work_struct *ws)\r\n{\r\nstruct pool *pool = container_of(to_delayed_work(ws), struct pool,\r\nno_space_timeout);\r\nif (get_pool_mode(pool) == PM_OUT_OF_DATA_SPACE && !pool->pf.error_if_no_space)\r\nset_pool_mode(pool, PM_READ_ONLY);\r\n}\r\nstatic struct pool_work *to_pool_work(struct work_struct *ws)\r\n{\r\nreturn container_of(ws, struct pool_work, worker);\r\n}\r\nstatic void pool_work_complete(struct pool_work *pw)\r\n{\r\ncomplete(&pw->complete);\r\n}\r\nstatic void pool_work_wait(struct pool_work *pw, struct pool *pool,\r\nvoid (*fn)(struct work_struct *))\r\n{\r\nINIT_WORK_ONSTACK(&pw->worker, fn);\r\ninit_completion(&pw->complete);\r\nqueue_work(pool->wq, &pw->worker);\r\nwait_for_completion(&pw->complete);\r\n}\r\nstatic struct noflush_work *to_noflush(struct work_struct *ws)\r\n{\r\nreturn container_of(to_pool_work(ws), struct noflush_work, pw);\r\n}\r\nstatic void do_noflush_start(struct work_struct *ws)\r\n{\r\nstruct noflush_work *w = to_noflush(ws);\r\nw->tc->requeue_mode = true;\r\nrequeue_io(w->tc);\r\npool_work_complete(&w->pw);\r\n}\r\nstatic void do_noflush_stop(struct work_struct *ws)\r\n{\r\nstruct noflush_work *w = to_noflush(ws);\r\nw->tc->requeue_mode = false;\r\npool_work_complete(&w->pw);\r\n}\r\nstatic void noflush_work(struct thin_c *tc, void (*fn)(struct work_struct *))\r\n{\r\nstruct noflush_work w;\r\nw.tc = tc;\r\npool_work_wait(&w.pw, tc->pool, fn);\r\n}\r\nstatic enum pool_mode get_pool_mode(struct pool *pool)\r\n{\r\nreturn pool->pf.mode;\r\n}\r\nstatic void notify_of_pool_mode_change(struct pool *pool, const char *new_mode)\r\n{\r\ndm_table_event(pool->ti->table);\r\nDMINFO("%s: switching pool to %s mode",\r\ndm_device_name(pool->pool_md), new_mode);\r\n}\r\nstatic void set_pool_mode(struct pool *pool, enum pool_mode new_mode)\r\n{\r\nstruct pool_c *pt = pool->ti->private;\r\nbool needs_check = dm_pool_metadata_needs_check(pool->pmd);\r\nenum pool_mode old_mode = get_pool_mode(pool);\r\nunsigned long no_space_timeout = ACCESS_ONCE(no_space_timeout_secs) * HZ;\r\nif (new_mode == PM_WRITE && needs_check) {\r\nDMERR("%s: unable to switch pool to write mode until repaired.",\r\ndm_device_name(pool->pool_md));\r\nif (old_mode != new_mode)\r\nnew_mode = old_mode;\r\nelse\r\nnew_mode = PM_READ_ONLY;\r\n}\r\nif (old_mode == PM_FAIL)\r\nnew_mode = old_mode;\r\nswitch (new_mode) {\r\ncase PM_FAIL:\r\nif (old_mode != new_mode)\r\nnotify_of_pool_mode_change(pool, "failure");\r\ndm_pool_metadata_read_only(pool->pmd);\r\npool->process_bio = process_bio_fail;\r\npool->process_discard = process_bio_fail;\r\npool->process_cell = process_cell_fail;\r\npool->process_discard_cell = process_cell_fail;\r\npool->process_prepared_mapping = process_prepared_mapping_fail;\r\npool->process_prepared_discard = process_prepared_discard_fail;\r\nerror_retry_list(pool);\r\nbreak;\r\ncase PM_READ_ONLY:\r\nif (old_mode != new_mode)\r\nnotify_of_pool_mode_change(pool, "read-only");\r\ndm_pool_metadata_read_only(pool->pmd);\r\npool->process_bio = process_bio_read_only;\r\npool->process_discard = process_bio_success;\r\npool->process_cell = process_cell_read_only;\r\npool->process_discard_cell = process_cell_success;\r\npool->process_prepared_mapping = process_prepared_mapping_fail;\r\npool->process_prepared_discard = process_prepared_discard_passdown;\r\nerror_retry_list(pool);\r\nbreak;\r\ncase PM_OUT_OF_DATA_SPACE:\r\nif (old_mode != new_mode)\r\nnotify_of_pool_mode_change(pool, "out-of-data-space");\r\npool->process_bio = process_bio_read_only;\r\npool->process_discard = process_discard_bio;\r\npool->process_cell = process_cell_read_only;\r\npool->process_discard_cell = process_discard_cell;\r\npool->process_prepared_mapping = process_prepared_mapping;\r\npool->process_prepared_discard = process_prepared_discard;\r\nif (!pool->pf.error_if_no_space && no_space_timeout)\r\nqueue_delayed_work(pool->wq, &pool->no_space_timeout, no_space_timeout);\r\nbreak;\r\ncase PM_WRITE:\r\nif (old_mode != new_mode)\r\nnotify_of_pool_mode_change(pool, "write");\r\ndm_pool_metadata_read_write(pool->pmd);\r\npool->process_bio = process_bio;\r\npool->process_discard = process_discard_bio;\r\npool->process_cell = process_cell;\r\npool->process_discard_cell = process_discard_cell;\r\npool->process_prepared_mapping = process_prepared_mapping;\r\npool->process_prepared_discard = process_prepared_discard;\r\nbreak;\r\n}\r\npool->pf.mode = new_mode;\r\npt->adjusted_pf.mode = new_mode;\r\n}\r\nstatic void abort_transaction(struct pool *pool)\r\n{\r\nconst char *dev_name = dm_device_name(pool->pool_md);\r\nDMERR_LIMIT("%s: aborting current metadata transaction", dev_name);\r\nif (dm_pool_abort_metadata(pool->pmd)) {\r\nDMERR("%s: failed to abort metadata transaction", dev_name);\r\nset_pool_mode(pool, PM_FAIL);\r\n}\r\nif (dm_pool_metadata_set_needs_check(pool->pmd)) {\r\nDMERR("%s: failed to set 'needs_check' flag in metadata", dev_name);\r\nset_pool_mode(pool, PM_FAIL);\r\n}\r\n}\r\nstatic void metadata_operation_failed(struct pool *pool, const char *op, int r)\r\n{\r\nDMERR_LIMIT("%s: metadata operation '%s' failed: error = %d",\r\ndm_device_name(pool->pool_md), op, r);\r\nabort_transaction(pool);\r\nset_pool_mode(pool, PM_READ_ONLY);\r\n}\r\nstatic void thin_defer_bio(struct thin_c *tc, struct bio *bio)\r\n{\r\nunsigned long flags;\r\nstruct pool *pool = tc->pool;\r\nspin_lock_irqsave(&tc->lock, flags);\r\nbio_list_add(&tc->deferred_bio_list, bio);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nwake_worker(pool);\r\n}\r\nstatic void thin_defer_bio_with_throttle(struct thin_c *tc, struct bio *bio)\r\n{\r\nstruct pool *pool = tc->pool;\r\nthrottle_lock(&pool->throttle);\r\nthin_defer_bio(tc, bio);\r\nthrottle_unlock(&pool->throttle);\r\n}\r\nstatic void thin_defer_cell(struct thin_c *tc, struct dm_bio_prison_cell *cell)\r\n{\r\nunsigned long flags;\r\nstruct pool *pool = tc->pool;\r\nthrottle_lock(&pool->throttle);\r\nspin_lock_irqsave(&tc->lock, flags);\r\nlist_add_tail(&cell->user_list, &tc->deferred_cells);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\nthrottle_unlock(&pool->throttle);\r\nwake_worker(pool);\r\n}\r\nstatic void thin_hook_bio(struct thin_c *tc, struct bio *bio)\r\n{\r\nstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\r\nh->tc = tc;\r\nh->shared_read_entry = NULL;\r\nh->all_io_entry = NULL;\r\nh->overwrite_mapping = NULL;\r\n}\r\nstatic int thin_bio_map(struct dm_target *ti, struct bio *bio)\r\n{\r\nint r;\r\nstruct thin_c *tc = ti->private;\r\ndm_block_t block = get_bio_block(tc, bio);\r\nstruct dm_thin_device *td = tc->td;\r\nstruct dm_thin_lookup_result result;\r\nstruct dm_bio_prison_cell *virt_cell, *data_cell;\r\nstruct dm_cell_key key;\r\nthin_hook_bio(tc, bio);\r\nif (tc->requeue_mode) {\r\nbio_endio(bio, DM_ENDIO_REQUEUE);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\nif (get_pool_mode(tc->pool) == PM_FAIL) {\r\nbio_io_error(bio);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\nif (bio->bi_rw & (REQ_DISCARD | REQ_FLUSH | REQ_FUA)) {\r\nthin_defer_bio_with_throttle(tc, bio);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\nbuild_virtual_key(tc->td, block, &key);\r\nif (bio_detain(tc->pool, &key, bio, &virt_cell))\r\nreturn DM_MAPIO_SUBMITTED;\r\nr = dm_thin_find_block(td, block, 0, &result);\r\nswitch (r) {\r\ncase 0:\r\nif (unlikely(result.shared)) {\r\nthin_defer_cell(tc, virt_cell);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\nbuild_data_key(tc->td, result.block, &key);\r\nif (bio_detain(tc->pool, &key, bio, &data_cell)) {\r\ncell_defer_no_holder(tc, virt_cell);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\ninc_all_io_entry(tc->pool, bio);\r\ncell_defer_no_holder(tc, data_cell);\r\ncell_defer_no_holder(tc, virt_cell);\r\nremap(tc, bio, result.block);\r\nreturn DM_MAPIO_REMAPPED;\r\ncase -ENODATA:\r\ncase -EWOULDBLOCK:\r\nthin_defer_cell(tc, virt_cell);\r\nreturn DM_MAPIO_SUBMITTED;\r\ndefault:\r\nbio_io_error(bio);\r\ncell_defer_no_holder(tc, virt_cell);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\n}\r\nstatic int pool_is_congested(struct dm_target_callbacks *cb, int bdi_bits)\r\n{\r\nstruct pool_c *pt = container_of(cb, struct pool_c, callbacks);\r\nstruct request_queue *q;\r\nif (get_pool_mode(pt->pool) == PM_OUT_OF_DATA_SPACE)\r\nreturn 1;\r\nq = bdev_get_queue(pt->data_dev->bdev);\r\nreturn bdi_congested(&q->backing_dev_info, bdi_bits);\r\n}\r\nstatic void requeue_bios(struct pool *pool)\r\n{\r\nunsigned long flags;\r\nstruct thin_c *tc;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(tc, &pool->active_thins, list) {\r\nspin_lock_irqsave(&tc->lock, flags);\r\nbio_list_merge(&tc->deferred_bio_list, &tc->retry_on_resume_list);\r\nbio_list_init(&tc->retry_on_resume_list);\r\nspin_unlock_irqrestore(&tc->lock, flags);\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic bool data_dev_supports_discard(struct pool_c *pt)\r\n{\r\nstruct request_queue *q = bdev_get_queue(pt->data_dev->bdev);\r\nreturn q && blk_queue_discard(q);\r\n}\r\nstatic bool is_factor(sector_t block_size, uint32_t n)\r\n{\r\nreturn !sector_div(block_size, n);\r\n}\r\nstatic void disable_passdown_if_not_supported(struct pool_c *pt)\r\n{\r\nstruct pool *pool = pt->pool;\r\nstruct block_device *data_bdev = pt->data_dev->bdev;\r\nstruct queue_limits *data_limits = &bdev_get_queue(data_bdev)->limits;\r\nsector_t block_size = pool->sectors_per_block << SECTOR_SHIFT;\r\nconst char *reason = NULL;\r\nchar buf[BDEVNAME_SIZE];\r\nif (!pt->adjusted_pf.discard_passdown)\r\nreturn;\r\nif (!data_dev_supports_discard(pt))\r\nreason = "discard unsupported";\r\nelse if (data_limits->max_discard_sectors < pool->sectors_per_block)\r\nreason = "max discard sectors smaller than a block";\r\nelse if (data_limits->discard_granularity > block_size)\r\nreason = "discard granularity larger than a block";\r\nelse if (!is_factor(block_size, data_limits->discard_granularity))\r\nreason = "discard granularity not a factor of block size";\r\nif (reason) {\r\nDMWARN("Data device (%s) %s: Disabling discard passdown.", bdevname(data_bdev, buf), reason);\r\npt->adjusted_pf.discard_passdown = false;\r\n}\r\n}\r\nstatic int bind_control_target(struct pool *pool, struct dm_target *ti)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nenum pool_mode old_mode = get_pool_mode(pool);\r\nenum pool_mode new_mode = pt->adjusted_pf.mode;\r\npt->adjusted_pf.mode = old_mode;\r\npool->ti = ti;\r\npool->pf = pt->adjusted_pf;\r\npool->low_water_blocks = pt->low_water_blocks;\r\nset_pool_mode(pool, new_mode);\r\nreturn 0;\r\n}\r\nstatic void unbind_control_target(struct pool *pool, struct dm_target *ti)\r\n{\r\nif (pool->ti == ti)\r\npool->ti = NULL;\r\n}\r\nstatic void pool_features_init(struct pool_features *pf)\r\n{\r\npf->mode = PM_WRITE;\r\npf->zero_new_blocks = true;\r\npf->discard_enabled = true;\r\npf->discard_passdown = true;\r\npf->error_if_no_space = false;\r\n}\r\nstatic void __pool_destroy(struct pool *pool)\r\n{\r\n__pool_table_remove(pool);\r\nif (dm_pool_metadata_close(pool->pmd) < 0)\r\nDMWARN("%s: dm_pool_metadata_close() failed.", __func__);\r\ndm_bio_prison_destroy(pool->prison);\r\ndm_kcopyd_client_destroy(pool->copier);\r\nif (pool->wq)\r\ndestroy_workqueue(pool->wq);\r\nif (pool->next_mapping)\r\nmempool_free(pool->next_mapping, pool->mapping_pool);\r\nmempool_destroy(pool->mapping_pool);\r\ndm_deferred_set_destroy(pool->shared_read_ds);\r\ndm_deferred_set_destroy(pool->all_io_ds);\r\nkfree(pool);\r\n}\r\nstatic struct pool *pool_create(struct mapped_device *pool_md,\r\nstruct block_device *metadata_dev,\r\nunsigned long block_size,\r\nint read_only, char **error)\r\n{\r\nint r;\r\nvoid *err_p;\r\nstruct pool *pool;\r\nstruct dm_pool_metadata *pmd;\r\nbool format_device = read_only ? false : true;\r\npmd = dm_pool_metadata_open(metadata_dev, block_size, format_device);\r\nif (IS_ERR(pmd)) {\r\n*error = "Error creating metadata object";\r\nreturn (struct pool *)pmd;\r\n}\r\npool = kmalloc(sizeof(*pool), GFP_KERNEL);\r\nif (!pool) {\r\n*error = "Error allocating memory for pool";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_pool;\r\n}\r\npool->pmd = pmd;\r\npool->sectors_per_block = block_size;\r\nif (block_size & (block_size - 1))\r\npool->sectors_per_block_shift = -1;\r\nelse\r\npool->sectors_per_block_shift = __ffs(block_size);\r\npool->low_water_blocks = 0;\r\npool_features_init(&pool->pf);\r\npool->prison = dm_bio_prison_create();\r\nif (!pool->prison) {\r\n*error = "Error creating pool's bio prison";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_prison;\r\n}\r\npool->copier = dm_kcopyd_client_create(&dm_kcopyd_throttle);\r\nif (IS_ERR(pool->copier)) {\r\nr = PTR_ERR(pool->copier);\r\n*error = "Error creating pool's kcopyd client";\r\nerr_p = ERR_PTR(r);\r\ngoto bad_kcopyd_client;\r\n}\r\npool->wq = alloc_ordered_workqueue("dm-" DM_MSG_PREFIX, WQ_MEM_RECLAIM);\r\nif (!pool->wq) {\r\n*error = "Error creating pool's workqueue";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_wq;\r\n}\r\nthrottle_init(&pool->throttle);\r\nINIT_WORK(&pool->worker, do_worker);\r\nINIT_DELAYED_WORK(&pool->waker, do_waker);\r\nINIT_DELAYED_WORK(&pool->no_space_timeout, do_no_space_timeout);\r\nspin_lock_init(&pool->lock);\r\nbio_list_init(&pool->deferred_flush_bios);\r\nINIT_LIST_HEAD(&pool->prepared_mappings);\r\nINIT_LIST_HEAD(&pool->prepared_discards);\r\nINIT_LIST_HEAD(&pool->active_thins);\r\npool->low_water_triggered = false;\r\npool->suspended = true;\r\npool->shared_read_ds = dm_deferred_set_create();\r\nif (!pool->shared_read_ds) {\r\n*error = "Error creating pool's shared read deferred set";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_shared_read_ds;\r\n}\r\npool->all_io_ds = dm_deferred_set_create();\r\nif (!pool->all_io_ds) {\r\n*error = "Error creating pool's all io deferred set";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_all_io_ds;\r\n}\r\npool->next_mapping = NULL;\r\npool->mapping_pool = mempool_create_slab_pool(MAPPING_POOL_SIZE,\r\n_new_mapping_cache);\r\nif (!pool->mapping_pool) {\r\n*error = "Error creating pool's mapping mempool";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_mapping_pool;\r\n}\r\npool->ref_count = 1;\r\npool->last_commit_jiffies = jiffies;\r\npool->pool_md = pool_md;\r\npool->md_dev = metadata_dev;\r\n__pool_table_insert(pool);\r\nreturn pool;\r\nbad_mapping_pool:\r\ndm_deferred_set_destroy(pool->all_io_ds);\r\nbad_all_io_ds:\r\ndm_deferred_set_destroy(pool->shared_read_ds);\r\nbad_shared_read_ds:\r\ndestroy_workqueue(pool->wq);\r\nbad_wq:\r\ndm_kcopyd_client_destroy(pool->copier);\r\nbad_kcopyd_client:\r\ndm_bio_prison_destroy(pool->prison);\r\nbad_prison:\r\nkfree(pool);\r\nbad_pool:\r\nif (dm_pool_metadata_close(pmd))\r\nDMWARN("%s: dm_pool_metadata_close() failed.", __func__);\r\nreturn err_p;\r\n}\r\nstatic void __pool_inc(struct pool *pool)\r\n{\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\npool->ref_count++;\r\n}\r\nstatic void __pool_dec(struct pool *pool)\r\n{\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nBUG_ON(!pool->ref_count);\r\nif (!--pool->ref_count)\r\n__pool_destroy(pool);\r\n}\r\nstatic struct pool *__pool_find(struct mapped_device *pool_md,\r\nstruct block_device *metadata_dev,\r\nunsigned long block_size, int read_only,\r\nchar **error, int *created)\r\n{\r\nstruct pool *pool = __pool_table_lookup_metadata_dev(metadata_dev);\r\nif (pool) {\r\nif (pool->pool_md != pool_md) {\r\n*error = "metadata device already in use by a pool";\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\n__pool_inc(pool);\r\n} else {\r\npool = __pool_table_lookup(pool_md);\r\nif (pool) {\r\nif (pool->md_dev != metadata_dev) {\r\n*error = "different pool cannot replace a pool";\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\n__pool_inc(pool);\r\n} else {\r\npool = pool_create(pool_md, metadata_dev, block_size, read_only, error);\r\n*created = 1;\r\n}\r\n}\r\nreturn pool;\r\n}\r\nstatic void pool_dtr(struct dm_target *ti)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nmutex_lock(&dm_thin_pool_table.mutex);\r\nunbind_control_target(pt->pool, ti);\r\n__pool_dec(pt->pool);\r\ndm_put_device(ti, pt->metadata_dev);\r\ndm_put_device(ti, pt->data_dev);\r\nkfree(pt);\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\n}\r\nstatic int parse_pool_features(struct dm_arg_set *as, struct pool_features *pf,\r\nstruct dm_target *ti)\r\n{\r\nint r;\r\nunsigned argc;\r\nconst char *arg_name;\r\nstatic struct dm_arg _args[] = {\r\n{0, 4, "Invalid number of pool feature arguments"},\r\n};\r\nif (!as->argc)\r\nreturn 0;\r\nr = dm_read_arg_group(_args, as, &argc, &ti->error);\r\nif (r)\r\nreturn -EINVAL;\r\nwhile (argc && !r) {\r\narg_name = dm_shift_arg(as);\r\nargc--;\r\nif (!strcasecmp(arg_name, "skip_block_zeroing"))\r\npf->zero_new_blocks = false;\r\nelse if (!strcasecmp(arg_name, "ignore_discard"))\r\npf->discard_enabled = false;\r\nelse if (!strcasecmp(arg_name, "no_discard_passdown"))\r\npf->discard_passdown = false;\r\nelse if (!strcasecmp(arg_name, "read_only"))\r\npf->mode = PM_READ_ONLY;\r\nelse if (!strcasecmp(arg_name, "error_if_no_space"))\r\npf->error_if_no_space = true;\r\nelse {\r\nti->error = "Unrecognised pool feature requested";\r\nr = -EINVAL;\r\nbreak;\r\n}\r\n}\r\nreturn r;\r\n}\r\nstatic void metadata_low_callback(void *context)\r\n{\r\nstruct pool *pool = context;\r\nDMWARN("%s: reached low water mark for metadata device: sending event.",\r\ndm_device_name(pool->pool_md));\r\ndm_table_event(pool->ti->table);\r\n}\r\nstatic sector_t get_dev_size(struct block_device *bdev)\r\n{\r\nreturn i_size_read(bdev->bd_inode) >> SECTOR_SHIFT;\r\n}\r\nstatic void warn_if_metadata_device_too_big(struct block_device *bdev)\r\n{\r\nsector_t metadata_dev_size = get_dev_size(bdev);\r\nchar buffer[BDEVNAME_SIZE];\r\nif (metadata_dev_size > THIN_METADATA_MAX_SECTORS_WARNING)\r\nDMWARN("Metadata device %s is larger than %u sectors: excess space will not be used.",\r\nbdevname(bdev, buffer), THIN_METADATA_MAX_SECTORS);\r\n}\r\nstatic sector_t get_metadata_dev_size(struct block_device *bdev)\r\n{\r\nsector_t metadata_dev_size = get_dev_size(bdev);\r\nif (metadata_dev_size > THIN_METADATA_MAX_SECTORS)\r\nmetadata_dev_size = THIN_METADATA_MAX_SECTORS;\r\nreturn metadata_dev_size;\r\n}\r\nstatic dm_block_t get_metadata_dev_size_in_blocks(struct block_device *bdev)\r\n{\r\nsector_t metadata_dev_size = get_metadata_dev_size(bdev);\r\nsector_div(metadata_dev_size, THIN_METADATA_BLOCK_SIZE);\r\nreturn metadata_dev_size;\r\n}\r\nstatic dm_block_t calc_metadata_threshold(struct pool_c *pt)\r\n{\r\ndm_block_t quarter = get_metadata_dev_size_in_blocks(pt->metadata_dev->bdev) / 4;\r\nreturn min((dm_block_t)1024ULL , quarter);\r\n}\r\nstatic int pool_ctr(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nint r, pool_created = 0;\r\nstruct pool_c *pt;\r\nstruct pool *pool;\r\nstruct pool_features pf;\r\nstruct dm_arg_set as;\r\nstruct dm_dev *data_dev;\r\nunsigned long block_size;\r\ndm_block_t low_water_blocks;\r\nstruct dm_dev *metadata_dev;\r\nfmode_t metadata_mode;\r\nmutex_lock(&dm_thin_pool_table.mutex);\r\nif (argc < 4) {\r\nti->error = "Invalid argument count";\r\nr = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nas.argc = argc;\r\nas.argv = argv;\r\npool_features_init(&pf);\r\ndm_consume_args(&as, 4);\r\nr = parse_pool_features(&as, &pf, ti);\r\nif (r)\r\ngoto out_unlock;\r\nmetadata_mode = FMODE_READ | ((pf.mode == PM_READ_ONLY) ? 0 : FMODE_WRITE);\r\nr = dm_get_device(ti, argv[0], metadata_mode, &metadata_dev);\r\nif (r) {\r\nti->error = "Error opening metadata block device";\r\ngoto out_unlock;\r\n}\r\nwarn_if_metadata_device_too_big(metadata_dev->bdev);\r\nr = dm_get_device(ti, argv[1], FMODE_READ | FMODE_WRITE, &data_dev);\r\nif (r) {\r\nti->error = "Error getting data device";\r\ngoto out_metadata;\r\n}\r\nif (kstrtoul(argv[2], 10, &block_size) || !block_size ||\r\nblock_size < DATA_DEV_BLOCK_SIZE_MIN_SECTORS ||\r\nblock_size > DATA_DEV_BLOCK_SIZE_MAX_SECTORS ||\r\nblock_size & (DATA_DEV_BLOCK_SIZE_MIN_SECTORS - 1)) {\r\nti->error = "Invalid block size";\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nif (kstrtoull(argv[3], 10, (unsigned long long *)&low_water_blocks)) {\r\nti->error = "Invalid low water mark";\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\npt = kzalloc(sizeof(*pt), GFP_KERNEL);\r\nif (!pt) {\r\nr = -ENOMEM;\r\ngoto out;\r\n}\r\npool = __pool_find(dm_table_get_md(ti->table), metadata_dev->bdev,\r\nblock_size, pf.mode == PM_READ_ONLY, &ti->error, &pool_created);\r\nif (IS_ERR(pool)) {\r\nr = PTR_ERR(pool);\r\ngoto out_free_pt;\r\n}\r\nif (!pool_created && pf.discard_enabled != pool->pf.discard_enabled) {\r\nti->error = "Discard support cannot be disabled once enabled";\r\nr = -EINVAL;\r\ngoto out_flags_changed;\r\n}\r\npt->pool = pool;\r\npt->ti = ti;\r\npt->metadata_dev = metadata_dev;\r\npt->data_dev = data_dev;\r\npt->low_water_blocks = low_water_blocks;\r\npt->adjusted_pf = pt->requested_pf = pf;\r\nti->num_flush_bios = 1;\r\nti->discard_zeroes_data_unsupported = true;\r\nif (pf.discard_enabled && pf.discard_passdown) {\r\nti->num_discard_bios = 1;\r\nti->discards_supported = true;\r\n}\r\nti->private = pt;\r\nr = dm_pool_register_metadata_threshold(pt->pool->pmd,\r\ncalc_metadata_threshold(pt),\r\nmetadata_low_callback,\r\npool);\r\nif (r)\r\ngoto out_free_pt;\r\npt->callbacks.congested_fn = pool_is_congested;\r\ndm_table_add_target_callbacks(ti->table, &pt->callbacks);\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\nreturn 0;\r\nout_flags_changed:\r\n__pool_dec(pool);\r\nout_free_pt:\r\nkfree(pt);\r\nout:\r\ndm_put_device(ti, data_dev);\r\nout_metadata:\r\ndm_put_device(ti, metadata_dev);\r\nout_unlock:\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\nreturn r;\r\n}\r\nstatic int pool_map(struct dm_target *ti, struct bio *bio)\r\n{\r\nint r;\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio->bi_bdev = pt->data_dev->bdev;\r\nr = DM_MAPIO_REMAPPED;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nreturn r;\r\n}\r\nstatic int maybe_resize_data_dev(struct dm_target *ti, bool *need_commit)\r\n{\r\nint r;\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nsector_t data_size = ti->len;\r\ndm_block_t sb_data_size;\r\n*need_commit = false;\r\n(void) sector_div(data_size, pool->sectors_per_block);\r\nr = dm_pool_get_data_dev_size(pool->pmd, &sb_data_size);\r\nif (r) {\r\nDMERR("%s: failed to retrieve data device size",\r\ndm_device_name(pool->pool_md));\r\nreturn r;\r\n}\r\nif (data_size < sb_data_size) {\r\nDMERR("%s: pool target (%llu blocks) too small: expected %llu",\r\ndm_device_name(pool->pool_md),\r\n(unsigned long long)data_size, sb_data_size);\r\nreturn -EINVAL;\r\n} else if (data_size > sb_data_size) {\r\nif (dm_pool_metadata_needs_check(pool->pmd)) {\r\nDMERR("%s: unable to grow the data device until repaired.",\r\ndm_device_name(pool->pool_md));\r\nreturn 0;\r\n}\r\nif (sb_data_size)\r\nDMINFO("%s: growing the data device from %llu to %llu blocks",\r\ndm_device_name(pool->pool_md),\r\nsb_data_size, (unsigned long long)data_size);\r\nr = dm_pool_resize_data_dev(pool->pmd, data_size);\r\nif (r) {\r\nmetadata_operation_failed(pool, "dm_pool_resize_data_dev", r);\r\nreturn r;\r\n}\r\n*need_commit = true;\r\n}\r\nreturn 0;\r\n}\r\nstatic int maybe_resize_metadata_dev(struct dm_target *ti, bool *need_commit)\r\n{\r\nint r;\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\ndm_block_t metadata_dev_size, sb_metadata_dev_size;\r\n*need_commit = false;\r\nmetadata_dev_size = get_metadata_dev_size_in_blocks(pool->md_dev);\r\nr = dm_pool_get_metadata_dev_size(pool->pmd, &sb_metadata_dev_size);\r\nif (r) {\r\nDMERR("%s: failed to retrieve metadata device size",\r\ndm_device_name(pool->pool_md));\r\nreturn r;\r\n}\r\nif (metadata_dev_size < sb_metadata_dev_size) {\r\nDMERR("%s: metadata device (%llu blocks) too small: expected %llu",\r\ndm_device_name(pool->pool_md),\r\nmetadata_dev_size, sb_metadata_dev_size);\r\nreturn -EINVAL;\r\n} else if (metadata_dev_size > sb_metadata_dev_size) {\r\nif (dm_pool_metadata_needs_check(pool->pmd)) {\r\nDMERR("%s: unable to grow the metadata device until repaired.",\r\ndm_device_name(pool->pool_md));\r\nreturn 0;\r\n}\r\nwarn_if_metadata_device_too_big(pool->md_dev);\r\nDMINFO("%s: growing the metadata device from %llu to %llu blocks",\r\ndm_device_name(pool->pool_md),\r\nsb_metadata_dev_size, metadata_dev_size);\r\nr = dm_pool_resize_metadata_dev(pool->pmd, metadata_dev_size);\r\nif (r) {\r\nmetadata_operation_failed(pool, "dm_pool_resize_metadata_dev", r);\r\nreturn r;\r\n}\r\n*need_commit = true;\r\n}\r\nreturn 0;\r\n}\r\nstatic int pool_preresume(struct dm_target *ti)\r\n{\r\nint r;\r\nbool need_commit1, need_commit2;\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nr = bind_control_target(pool, ti);\r\nif (r)\r\nreturn r;\r\nr = maybe_resize_data_dev(ti, &need_commit1);\r\nif (r)\r\nreturn r;\r\nr = maybe_resize_metadata_dev(ti, &need_commit2);\r\nif (r)\r\nreturn r;\r\nif (need_commit1 || need_commit2)\r\n(void) commit(pool);\r\nreturn 0;\r\n}\r\nstatic void pool_suspend_active_thins(struct pool *pool)\r\n{\r\nstruct thin_c *tc;\r\ntc = get_first_thin(pool);\r\nwhile (tc) {\r\ndm_internal_suspend_noflush(tc->thin_md);\r\ntc = get_next_thin(pool, tc);\r\n}\r\n}\r\nstatic void pool_resume_active_thins(struct pool *pool)\r\n{\r\nstruct thin_c *tc;\r\ntc = get_first_thin(pool);\r\nwhile (tc) {\r\ndm_internal_resume(tc->thin_md);\r\ntc = get_next_thin(pool, tc);\r\n}\r\n}\r\nstatic void pool_resume(struct dm_target *ti)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nunsigned long flags;\r\nrequeue_bios(pool);\r\npool_resume_active_thins(pool);\r\nspin_lock_irqsave(&pool->lock, flags);\r\npool->low_water_triggered = false;\r\npool->suspended = false;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\ndo_waker(&pool->waker.work);\r\n}\r\nstatic void pool_presuspend(struct dm_target *ti)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->lock, flags);\r\npool->suspended = true;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\npool_suspend_active_thins(pool);\r\n}\r\nstatic void pool_presuspend_undo(struct dm_target *ti)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nunsigned long flags;\r\npool_resume_active_thins(pool);\r\nspin_lock_irqsave(&pool->lock, flags);\r\npool->suspended = false;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic void pool_postsuspend(struct dm_target *ti)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\ncancel_delayed_work(&pool->waker);\r\ncancel_delayed_work(&pool->no_space_timeout);\r\nflush_workqueue(pool->wq);\r\n(void) commit(pool);\r\n}\r\nstatic int check_arg_count(unsigned argc, unsigned args_required)\r\n{\r\nif (argc != args_required) {\r\nDMWARN("Message received with %u arguments instead of %u.",\r\nargc, args_required);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int read_dev_id(char *arg, dm_thin_id *dev_id, int warning)\r\n{\r\nif (!kstrtoull(arg, 10, (unsigned long long *)dev_id) &&\r\n*dev_id <= MAX_DEV_ID)\r\nreturn 0;\r\nif (warning)\r\nDMWARN("Message received with invalid device id: %s", arg);\r\nreturn -EINVAL;\r\n}\r\nstatic int process_create_thin_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\ndm_thin_id dev_id;\r\nint r;\r\nr = check_arg_count(argc, 2);\r\nif (r)\r\nreturn r;\r\nr = read_dev_id(argv[1], &dev_id, 1);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_create_thin(pool->pmd, dev_id);\r\nif (r) {\r\nDMWARN("Creation of new thinly-provisioned device with id %s failed.",\r\nargv[1]);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic int process_create_snap_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\ndm_thin_id dev_id;\r\ndm_thin_id origin_dev_id;\r\nint r;\r\nr = check_arg_count(argc, 3);\r\nif (r)\r\nreturn r;\r\nr = read_dev_id(argv[1], &dev_id, 1);\r\nif (r)\r\nreturn r;\r\nr = read_dev_id(argv[2], &origin_dev_id, 1);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_create_snap(pool->pmd, dev_id, origin_dev_id);\r\nif (r) {\r\nDMWARN("Creation of new snapshot %s of device %s failed.",\r\nargv[1], argv[2]);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic int process_delete_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\ndm_thin_id dev_id;\r\nint r;\r\nr = check_arg_count(argc, 2);\r\nif (r)\r\nreturn r;\r\nr = read_dev_id(argv[1], &dev_id, 1);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_delete_thin_device(pool->pmd, dev_id);\r\nif (r)\r\nDMWARN("Deletion of thin device %s failed.", argv[1]);\r\nreturn r;\r\n}\r\nstatic int process_set_transaction_id_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\ndm_thin_id old_id, new_id;\r\nint r;\r\nr = check_arg_count(argc, 3);\r\nif (r)\r\nreturn r;\r\nif (kstrtoull(argv[1], 10, (unsigned long long *)&old_id)) {\r\nDMWARN("set_transaction_id message: Unrecognised id %s.", argv[1]);\r\nreturn -EINVAL;\r\n}\r\nif (kstrtoull(argv[2], 10, (unsigned long long *)&new_id)) {\r\nDMWARN("set_transaction_id message: Unrecognised new id %s.", argv[2]);\r\nreturn -EINVAL;\r\n}\r\nr = dm_pool_set_metadata_transaction_id(pool->pmd, old_id, new_id);\r\nif (r) {\r\nDMWARN("Failed to change transaction id from %s to %s.",\r\nargv[1], argv[2]);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic int process_reserve_metadata_snap_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\nint r;\r\nr = check_arg_count(argc, 1);\r\nif (r)\r\nreturn r;\r\n(void) commit(pool);\r\nr = dm_pool_reserve_metadata_snap(pool->pmd);\r\nif (r)\r\nDMWARN("reserve_metadata_snap message failed.");\r\nreturn r;\r\n}\r\nstatic int process_release_metadata_snap_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\nint r;\r\nr = check_arg_count(argc, 1);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_release_metadata_snap(pool->pmd);\r\nif (r)\r\nDMWARN("release_metadata_snap message failed.");\r\nreturn r;\r\n}\r\nstatic int pool_message(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nint r = -EINVAL;\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nif (get_pool_mode(pool) >= PM_READ_ONLY) {\r\nDMERR("%s: unable to service pool target messages in READ_ONLY or FAIL mode",\r\ndm_device_name(pool->pool_md));\r\nreturn -EINVAL;\r\n}\r\nif (!strcasecmp(argv[0], "create_thin"))\r\nr = process_create_thin_mesg(argc, argv, pool);\r\nelse if (!strcasecmp(argv[0], "create_snap"))\r\nr = process_create_snap_mesg(argc, argv, pool);\r\nelse if (!strcasecmp(argv[0], "delete"))\r\nr = process_delete_mesg(argc, argv, pool);\r\nelse if (!strcasecmp(argv[0], "set_transaction_id"))\r\nr = process_set_transaction_id_mesg(argc, argv, pool);\r\nelse if (!strcasecmp(argv[0], "reserve_metadata_snap"))\r\nr = process_reserve_metadata_snap_mesg(argc, argv, pool);\r\nelse if (!strcasecmp(argv[0], "release_metadata_snap"))\r\nr = process_release_metadata_snap_mesg(argc, argv, pool);\r\nelse\r\nDMWARN("Unrecognised thin pool target message received: %s", argv[0]);\r\nif (!r)\r\n(void) commit(pool);\r\nreturn r;\r\n}\r\nstatic void emit_flags(struct pool_features *pf, char *result,\r\nunsigned sz, unsigned maxlen)\r\n{\r\nunsigned count = !pf->zero_new_blocks + !pf->discard_enabled +\r\n!pf->discard_passdown + (pf->mode == PM_READ_ONLY) +\r\npf->error_if_no_space;\r\nDMEMIT("%u ", count);\r\nif (!pf->zero_new_blocks)\r\nDMEMIT("skip_block_zeroing ");\r\nif (!pf->discard_enabled)\r\nDMEMIT("ignore_discard ");\r\nif (!pf->discard_passdown)\r\nDMEMIT("no_discard_passdown ");\r\nif (pf->mode == PM_READ_ONLY)\r\nDMEMIT("read_only ");\r\nif (pf->error_if_no_space)\r\nDMEMIT("error_if_no_space ");\r\n}\r\nstatic void pool_status(struct dm_target *ti, status_type_t type,\r\nunsigned status_flags, char *result, unsigned maxlen)\r\n{\r\nint r;\r\nunsigned sz = 0;\r\nuint64_t transaction_id;\r\ndm_block_t nr_free_blocks_data;\r\ndm_block_t nr_free_blocks_metadata;\r\ndm_block_t nr_blocks_data;\r\ndm_block_t nr_blocks_metadata;\r\ndm_block_t held_root;\r\nchar buf[BDEVNAME_SIZE];\r\nchar buf2[BDEVNAME_SIZE];\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nswitch (type) {\r\ncase STATUSTYPE_INFO:\r\nif (get_pool_mode(pool) == PM_FAIL) {\r\nDMEMIT("Fail");\r\nbreak;\r\n}\r\nif (!(status_flags & DM_STATUS_NOFLUSH_FLAG) && !dm_suspended(ti))\r\n(void) commit(pool);\r\nr = dm_pool_get_metadata_transaction_id(pool->pmd, &transaction_id);\r\nif (r) {\r\nDMERR("%s: dm_pool_get_metadata_transaction_id returned %d",\r\ndm_device_name(pool->pool_md), r);\r\ngoto err;\r\n}\r\nr = dm_pool_get_free_metadata_block_count(pool->pmd, &nr_free_blocks_metadata);\r\nif (r) {\r\nDMERR("%s: dm_pool_get_free_metadata_block_count returned %d",\r\ndm_device_name(pool->pool_md), r);\r\ngoto err;\r\n}\r\nr = dm_pool_get_metadata_dev_size(pool->pmd, &nr_blocks_metadata);\r\nif (r) {\r\nDMERR("%s: dm_pool_get_metadata_dev_size returned %d",\r\ndm_device_name(pool->pool_md), r);\r\ngoto err;\r\n}\r\nr = dm_pool_get_free_block_count(pool->pmd, &nr_free_blocks_data);\r\nif (r) {\r\nDMERR("%s: dm_pool_get_free_block_count returned %d",\r\ndm_device_name(pool->pool_md), r);\r\ngoto err;\r\n}\r\nr = dm_pool_get_data_dev_size(pool->pmd, &nr_blocks_data);\r\nif (r) {\r\nDMERR("%s: dm_pool_get_data_dev_size returned %d",\r\ndm_device_name(pool->pool_md), r);\r\ngoto err;\r\n}\r\nr = dm_pool_get_metadata_snap(pool->pmd, &held_root);\r\nif (r) {\r\nDMERR("%s: dm_pool_get_metadata_snap returned %d",\r\ndm_device_name(pool->pool_md), r);\r\ngoto err;\r\n}\r\nDMEMIT("%llu %llu/%llu %llu/%llu ",\r\n(unsigned long long)transaction_id,\r\n(unsigned long long)(nr_blocks_metadata - nr_free_blocks_metadata),\r\n(unsigned long long)nr_blocks_metadata,\r\n(unsigned long long)(nr_blocks_data - nr_free_blocks_data),\r\n(unsigned long long)nr_blocks_data);\r\nif (held_root)\r\nDMEMIT("%llu ", held_root);\r\nelse\r\nDMEMIT("- ");\r\nif (pool->pf.mode == PM_OUT_OF_DATA_SPACE)\r\nDMEMIT("out_of_data_space ");\r\nelse if (pool->pf.mode == PM_READ_ONLY)\r\nDMEMIT("ro ");\r\nelse\r\nDMEMIT("rw ");\r\nif (!pool->pf.discard_enabled)\r\nDMEMIT("ignore_discard ");\r\nelse if (pool->pf.discard_passdown)\r\nDMEMIT("discard_passdown ");\r\nelse\r\nDMEMIT("no_discard_passdown ");\r\nif (pool->pf.error_if_no_space)\r\nDMEMIT("error_if_no_space ");\r\nelse\r\nDMEMIT("queue_if_no_space ");\r\nbreak;\r\ncase STATUSTYPE_TABLE:\r\nDMEMIT("%s %s %lu %llu ",\r\nformat_dev_t(buf, pt->metadata_dev->bdev->bd_dev),\r\nformat_dev_t(buf2, pt->data_dev->bdev->bd_dev),\r\n(unsigned long)pool->sectors_per_block,\r\n(unsigned long long)pt->low_water_blocks);\r\nemit_flags(&pt->requested_pf, result, sz, maxlen);\r\nbreak;\r\n}\r\nreturn;\r\nerr:\r\nDMEMIT("Error");\r\n}\r\nstatic int pool_iterate_devices(struct dm_target *ti,\r\niterate_devices_callout_fn fn, void *data)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nreturn fn(ti, pt->data_dev, 0, ti->len, data);\r\n}\r\nstatic int pool_merge(struct dm_target *ti, struct bvec_merge_data *bvm,\r\nstruct bio_vec *biovec, int max_size)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nstruct request_queue *q = bdev_get_queue(pt->data_dev->bdev);\r\nif (!q->merge_bvec_fn)\r\nreturn max_size;\r\nbvm->bi_bdev = pt->data_dev->bdev;\r\nreturn min(max_size, q->merge_bvec_fn(q, bvm, biovec));\r\n}\r\nstatic void set_discard_limits(struct pool_c *pt, struct queue_limits *limits)\r\n{\r\nstruct pool *pool = pt->pool;\r\nstruct queue_limits *data_limits;\r\nlimits->max_discard_sectors = pool->sectors_per_block;\r\nif (pt->adjusted_pf.discard_passdown) {\r\ndata_limits = &bdev_get_queue(pt->data_dev->bdev)->limits;\r\nlimits->discard_granularity = max(data_limits->discard_granularity,\r\npool->sectors_per_block << SECTOR_SHIFT);\r\n} else\r\nlimits->discard_granularity = pool->sectors_per_block << SECTOR_SHIFT;\r\n}\r\nstatic void pool_io_hints(struct dm_target *ti, struct queue_limits *limits)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nsector_t io_opt_sectors = limits->io_opt >> SECTOR_SHIFT;\r\nif (limits->max_sectors < pool->sectors_per_block) {\r\nwhile (!is_factor(pool->sectors_per_block, limits->max_sectors)) {\r\nif ((limits->max_sectors & (limits->max_sectors - 1)) == 0)\r\nlimits->max_sectors--;\r\nlimits->max_sectors = rounddown_pow_of_two(limits->max_sectors);\r\n}\r\n}\r\nif (io_opt_sectors < pool->sectors_per_block ||\r\n!is_factor(io_opt_sectors, pool->sectors_per_block)) {\r\nif (is_factor(pool->sectors_per_block, limits->max_sectors))\r\nblk_limits_io_min(limits, limits->max_sectors << SECTOR_SHIFT);\r\nelse\r\nblk_limits_io_min(limits, pool->sectors_per_block << SECTOR_SHIFT);\r\nblk_limits_io_opt(limits, pool->sectors_per_block << SECTOR_SHIFT);\r\n}\r\nif (!pt->adjusted_pf.discard_enabled) {\r\nlimits->discard_granularity = 0;\r\nreturn;\r\n}\r\ndisable_passdown_if_not_supported(pt);\r\nset_discard_limits(pt, limits);\r\n}\r\nstatic void thin_get(struct thin_c *tc)\r\n{\r\natomic_inc(&tc->refcount);\r\n}\r\nstatic void thin_put(struct thin_c *tc)\r\n{\r\nif (atomic_dec_and_test(&tc->refcount))\r\ncomplete(&tc->can_destroy);\r\n}\r\nstatic void thin_dtr(struct dm_target *ti)\r\n{\r\nstruct thin_c *tc = ti->private;\r\nunsigned long flags;\r\nspin_lock_irqsave(&tc->pool->lock, flags);\r\nlist_del_rcu(&tc->list);\r\nspin_unlock_irqrestore(&tc->pool->lock, flags);\r\nsynchronize_rcu();\r\nthin_put(tc);\r\nwait_for_completion(&tc->can_destroy);\r\nmutex_lock(&dm_thin_pool_table.mutex);\r\n__pool_dec(tc->pool);\r\ndm_pool_close_thin_device(tc->td);\r\ndm_put_device(ti, tc->pool_dev);\r\nif (tc->origin_dev)\r\ndm_put_device(ti, tc->origin_dev);\r\nkfree(tc);\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\n}\r\nstatic int thin_ctr(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nint r;\r\nstruct thin_c *tc;\r\nstruct dm_dev *pool_dev, *origin_dev;\r\nstruct mapped_device *pool_md;\r\nunsigned long flags;\r\nmutex_lock(&dm_thin_pool_table.mutex);\r\nif (argc != 2 && argc != 3) {\r\nti->error = "Invalid argument count";\r\nr = -EINVAL;\r\ngoto out_unlock;\r\n}\r\ntc = ti->private = kzalloc(sizeof(*tc), GFP_KERNEL);\r\nif (!tc) {\r\nti->error = "Out of memory";\r\nr = -ENOMEM;\r\ngoto out_unlock;\r\n}\r\ntc->thin_md = dm_table_get_md(ti->table);\r\nspin_lock_init(&tc->lock);\r\nINIT_LIST_HEAD(&tc->deferred_cells);\r\nbio_list_init(&tc->deferred_bio_list);\r\nbio_list_init(&tc->retry_on_resume_list);\r\ntc->sort_bio_list = RB_ROOT;\r\nif (argc == 3) {\r\nr = dm_get_device(ti, argv[2], FMODE_READ, &origin_dev);\r\nif (r) {\r\nti->error = "Error opening origin device";\r\ngoto bad_origin_dev;\r\n}\r\ntc->origin_dev = origin_dev;\r\n}\r\nr = dm_get_device(ti, argv[0], dm_table_get_mode(ti->table), &pool_dev);\r\nif (r) {\r\nti->error = "Error opening pool device";\r\ngoto bad_pool_dev;\r\n}\r\ntc->pool_dev = pool_dev;\r\nif (read_dev_id(argv[1], (unsigned long long *)&tc->dev_id, 0)) {\r\nti->error = "Invalid device id";\r\nr = -EINVAL;\r\ngoto bad_common;\r\n}\r\npool_md = dm_get_md(tc->pool_dev->bdev->bd_dev);\r\nif (!pool_md) {\r\nti->error = "Couldn't get pool mapped device";\r\nr = -EINVAL;\r\ngoto bad_common;\r\n}\r\ntc->pool = __pool_table_lookup(pool_md);\r\nif (!tc->pool) {\r\nti->error = "Couldn't find pool object";\r\nr = -EINVAL;\r\ngoto bad_pool_lookup;\r\n}\r\n__pool_inc(tc->pool);\r\nif (get_pool_mode(tc->pool) == PM_FAIL) {\r\nti->error = "Couldn't open thin device, Pool is in fail mode";\r\nr = -EINVAL;\r\ngoto bad_pool;\r\n}\r\nr = dm_pool_open_thin_device(tc->pool->pmd, tc->dev_id, &tc->td);\r\nif (r) {\r\nti->error = "Couldn't open thin internal device";\r\ngoto bad_pool;\r\n}\r\nr = dm_set_target_max_io_len(ti, tc->pool->sectors_per_block);\r\nif (r)\r\ngoto bad;\r\nti->num_flush_bios = 1;\r\nti->flush_supported = true;\r\nti->per_bio_data_size = sizeof(struct dm_thin_endio_hook);\r\nti->discard_zeroes_data_unsupported = true;\r\nif (tc->pool->pf.discard_enabled) {\r\nti->discards_supported = true;\r\nti->num_discard_bios = 1;\r\nti->split_discard_bios = true;\r\n}\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\nspin_lock_irqsave(&tc->pool->lock, flags);\r\nif (tc->pool->suspended) {\r\nspin_unlock_irqrestore(&tc->pool->lock, flags);\r\nmutex_lock(&dm_thin_pool_table.mutex);\r\nti->error = "Unable to activate thin device while pool is suspended";\r\nr = -EINVAL;\r\ngoto bad;\r\n}\r\natomic_set(&tc->refcount, 1);\r\ninit_completion(&tc->can_destroy);\r\nlist_add_tail_rcu(&tc->list, &tc->pool->active_thins);\r\nspin_unlock_irqrestore(&tc->pool->lock, flags);\r\nsynchronize_rcu();\r\ndm_put(pool_md);\r\nreturn 0;\r\nbad:\r\ndm_pool_close_thin_device(tc->td);\r\nbad_pool:\r\n__pool_dec(tc->pool);\r\nbad_pool_lookup:\r\ndm_put(pool_md);\r\nbad_common:\r\ndm_put_device(ti, tc->pool_dev);\r\nbad_pool_dev:\r\nif (tc->origin_dev)\r\ndm_put_device(ti, tc->origin_dev);\r\nbad_origin_dev:\r\nkfree(tc);\r\nout_unlock:\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\nreturn r;\r\n}\r\nstatic int thin_map(struct dm_target *ti, struct bio *bio)\r\n{\r\nbio->bi_iter.bi_sector = dm_target_offset(ti, bio->bi_iter.bi_sector);\r\nreturn thin_bio_map(ti, bio);\r\n}\r\nstatic int thin_endio(struct dm_target *ti, struct bio *bio, int err)\r\n{\r\nunsigned long flags;\r\nstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\r\nstruct list_head work;\r\nstruct dm_thin_new_mapping *m, *tmp;\r\nstruct pool *pool = h->tc->pool;\r\nif (h->shared_read_entry) {\r\nINIT_LIST_HEAD(&work);\r\ndm_deferred_entry_dec(h->shared_read_entry, &work);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nlist_for_each_entry_safe(m, tmp, &work, list) {\r\nlist_del(&m->list);\r\n__complete_mapping_preparation(m);\r\n}\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nif (h->all_io_entry) {\r\nINIT_LIST_HEAD(&work);\r\ndm_deferred_entry_dec(h->all_io_entry, &work);\r\nif (!list_empty(&work)) {\r\nspin_lock_irqsave(&pool->lock, flags);\r\nlist_for_each_entry_safe(m, tmp, &work, list)\r\nlist_add_tail(&m->list, &pool->prepared_discards);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nwake_worker(pool);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void thin_presuspend(struct dm_target *ti)\r\n{\r\nstruct thin_c *tc = ti->private;\r\nif (dm_noflush_suspending(ti))\r\nnoflush_work(tc, do_noflush_start);\r\n}\r\nstatic void thin_postsuspend(struct dm_target *ti)\r\n{\r\nstruct thin_c *tc = ti->private;\r\nnoflush_work(tc, do_noflush_stop);\r\n}\r\nstatic int thin_preresume(struct dm_target *ti)\r\n{\r\nstruct thin_c *tc = ti->private;\r\nif (tc->origin_dev)\r\ntc->origin_size = get_dev_size(tc->origin_dev->bdev);\r\nreturn 0;\r\n}\r\nstatic void thin_status(struct dm_target *ti, status_type_t type,\r\nunsigned status_flags, char *result, unsigned maxlen)\r\n{\r\nint r;\r\nssize_t sz = 0;\r\ndm_block_t mapped, highest;\r\nchar buf[BDEVNAME_SIZE];\r\nstruct thin_c *tc = ti->private;\r\nif (get_pool_mode(tc->pool) == PM_FAIL) {\r\nDMEMIT("Fail");\r\nreturn;\r\n}\r\nif (!tc->td)\r\nDMEMIT("-");\r\nelse {\r\nswitch (type) {\r\ncase STATUSTYPE_INFO:\r\nr = dm_thin_get_mapped_count(tc->td, &mapped);\r\nif (r) {\r\nDMERR("dm_thin_get_mapped_count returned %d", r);\r\ngoto err;\r\n}\r\nr = dm_thin_get_highest_mapped_block(tc->td, &highest);\r\nif (r < 0) {\r\nDMERR("dm_thin_get_highest_mapped_block returned %d", r);\r\ngoto err;\r\n}\r\nDMEMIT("%llu ", mapped * tc->pool->sectors_per_block);\r\nif (r)\r\nDMEMIT("%llu", ((highest + 1) *\r\ntc->pool->sectors_per_block) - 1);\r\nelse\r\nDMEMIT("-");\r\nbreak;\r\ncase STATUSTYPE_TABLE:\r\nDMEMIT("%s %lu",\r\nformat_dev_t(buf, tc->pool_dev->bdev->bd_dev),\r\n(unsigned long) tc->dev_id);\r\nif (tc->origin_dev)\r\nDMEMIT(" %s", format_dev_t(buf, tc->origin_dev->bdev->bd_dev));\r\nbreak;\r\n}\r\n}\r\nreturn;\r\nerr:\r\nDMEMIT("Error");\r\n}\r\nstatic int thin_merge(struct dm_target *ti, struct bvec_merge_data *bvm,\r\nstruct bio_vec *biovec, int max_size)\r\n{\r\nstruct thin_c *tc = ti->private;\r\nstruct request_queue *q = bdev_get_queue(tc->pool_dev->bdev);\r\nif (!q->merge_bvec_fn)\r\nreturn max_size;\r\nbvm->bi_bdev = tc->pool_dev->bdev;\r\nbvm->bi_sector = dm_target_offset(ti, bvm->bi_sector);\r\nreturn min(max_size, q->merge_bvec_fn(q, bvm, biovec));\r\n}\r\nstatic int thin_iterate_devices(struct dm_target *ti,\r\niterate_devices_callout_fn fn, void *data)\r\n{\r\nsector_t blocks;\r\nstruct thin_c *tc = ti->private;\r\nstruct pool *pool = tc->pool;\r\nif (!pool->ti)\r\nreturn 0;\r\nblocks = pool->ti->len;\r\n(void) sector_div(blocks, pool->sectors_per_block);\r\nif (blocks)\r\nreturn fn(ti, tc->pool_dev, 0, pool->sectors_per_block * blocks, data);\r\nreturn 0;\r\n}\r\nstatic int __init dm_thin_init(void)\r\n{\r\nint r;\r\npool_table_init();\r\nr = dm_register_target(&thin_target);\r\nif (r)\r\nreturn r;\r\nr = dm_register_target(&pool_target);\r\nif (r)\r\ngoto bad_pool_target;\r\nr = -ENOMEM;\r\n_new_mapping_cache = KMEM_CACHE(dm_thin_new_mapping, 0);\r\nif (!_new_mapping_cache)\r\ngoto bad_new_mapping_cache;\r\nreturn 0;\r\nbad_new_mapping_cache:\r\ndm_unregister_target(&pool_target);\r\nbad_pool_target:\r\ndm_unregister_target(&thin_target);\r\nreturn r;\r\n}\r\nstatic void dm_thin_exit(void)\r\n{\r\ndm_unregister_target(&thin_target);\r\ndm_unregister_target(&pool_target);\r\nkmem_cache_destroy(_new_mapping_cache);\r\n}
