static inline unsigned int gtlb0_get_next_victim(\r\nstruct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nunsigned int victim;\r\nvictim = vcpu_e500->gtlb_nv[0]++;\r\nif (unlikely(vcpu_e500->gtlb_nv[0] >= vcpu_e500->gtlb_params[0].ways))\r\nvcpu_e500->gtlb_nv[0] = 0;\r\nreturn victim;\r\n}\r\nstatic int tlb0_set_base(gva_t addr, int sets, int ways)\r\n{\r\nint set_base;\r\nset_base = (addr >> PAGE_SHIFT) & (sets - 1);\r\nset_base *= ways;\r\nreturn set_base;\r\n}\r\nstatic int gtlb0_set_base(struct kvmppc_vcpu_e500 *vcpu_e500, gva_t addr)\r\n{\r\nreturn tlb0_set_base(addr, vcpu_e500->gtlb_params[0].sets,\r\nvcpu_e500->gtlb_params[0].ways);\r\n}\r\nstatic unsigned int get_tlb_esel(struct kvm_vcpu *vcpu, int tlbsel)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nint esel = get_tlb_esel_bit(vcpu);\r\nif (tlbsel == 0) {\r\nesel &= vcpu_e500->gtlb_params[0].ways - 1;\r\nesel += gtlb0_set_base(vcpu_e500, vcpu->arch.shared->mas2);\r\n} else {\r\nesel &= vcpu_e500->gtlb_params[tlbsel].entries - 1;\r\n}\r\nreturn esel;\r\n}\r\nstatic int kvmppc_e500_tlb_index(struct kvmppc_vcpu_e500 *vcpu_e500,\r\ngva_t eaddr, int tlbsel, unsigned int pid, int as)\r\n{\r\nint size = vcpu_e500->gtlb_params[tlbsel].entries;\r\nunsigned int set_base, offset;\r\nint i;\r\nif (tlbsel == 0) {\r\nset_base = gtlb0_set_base(vcpu_e500, eaddr);\r\nsize = vcpu_e500->gtlb_params[0].ways;\r\n} else {\r\nif (eaddr < vcpu_e500->tlb1_min_eaddr ||\r\neaddr > vcpu_e500->tlb1_max_eaddr)\r\nreturn -1;\r\nset_base = 0;\r\n}\r\noffset = vcpu_e500->gtlb_offset[tlbsel];\r\nfor (i = 0; i < size; i++) {\r\nstruct kvm_book3e_206_tlb_entry *tlbe =\r\n&vcpu_e500->gtlb_arch[offset + set_base + i];\r\nunsigned int tid;\r\nif (eaddr < get_tlb_eaddr(tlbe))\r\ncontinue;\r\nif (eaddr > get_tlb_end(tlbe))\r\ncontinue;\r\ntid = get_tlb_tid(tlbe);\r\nif (tid && (tid != pid))\r\ncontinue;\r\nif (!get_tlb_v(tlbe))\r\ncontinue;\r\nif (get_tlb_ts(tlbe) != as && as != -1)\r\ncontinue;\r\nreturn set_base + i;\r\n}\r\nreturn -1;\r\n}\r\nstatic inline void kvmppc_e500_deliver_tlb_miss(struct kvm_vcpu *vcpu,\r\ngva_t eaddr, int as)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nunsigned int victim, tsized;\r\nint tlbsel;\r\ntlbsel = (vcpu->arch.shared->mas4 >> 28) & 0x1;\r\nvictim = (tlbsel == 0) ? gtlb0_get_next_victim(vcpu_e500) : 0;\r\ntsized = (vcpu->arch.shared->mas4 >> 7) & 0x1f;\r\nvcpu->arch.shared->mas0 = MAS0_TLBSEL(tlbsel) | MAS0_ESEL(victim)\r\n| MAS0_NV(vcpu_e500->gtlb_nv[tlbsel]);\r\nvcpu->arch.shared->mas1 = MAS1_VALID | (as ? MAS1_TS : 0)\r\n| MAS1_TID(get_tlbmiss_tid(vcpu))\r\n| MAS1_TSIZE(tsized);\r\nvcpu->arch.shared->mas2 = (eaddr & MAS2_EPN)\r\n| (vcpu->arch.shared->mas4 & MAS2_ATTRIB_MASK);\r\nvcpu->arch.shared->mas7_3 &= MAS3_U0 | MAS3_U1 | MAS3_U2 | MAS3_U3;\r\nvcpu->arch.shared->mas6 = (vcpu->arch.shared->mas6 & MAS6_SPID1)\r\n| (get_cur_pid(vcpu) << 16)\r\n| (as ? MAS6_SAS : 0);\r\n}\r\nstatic void kvmppc_recalc_tlb1map_range(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nint size = vcpu_e500->gtlb_params[1].entries;\r\nunsigned int offset;\r\ngva_t eaddr;\r\nint i;\r\nvcpu_e500->tlb1_min_eaddr = ~0UL;\r\nvcpu_e500->tlb1_max_eaddr = 0;\r\noffset = vcpu_e500->gtlb_offset[1];\r\nfor (i = 0; i < size; i++) {\r\nstruct kvm_book3e_206_tlb_entry *tlbe =\r\n&vcpu_e500->gtlb_arch[offset + i];\r\nif (!get_tlb_v(tlbe))\r\ncontinue;\r\neaddr = get_tlb_eaddr(tlbe);\r\nvcpu_e500->tlb1_min_eaddr =\r\nmin(vcpu_e500->tlb1_min_eaddr, eaddr);\r\neaddr = get_tlb_end(tlbe);\r\nvcpu_e500->tlb1_max_eaddr =\r\nmax(vcpu_e500->tlb1_max_eaddr, eaddr);\r\n}\r\n}\r\nstatic int kvmppc_need_recalc_tlb1map_range(struct kvmppc_vcpu_e500 *vcpu_e500,\r\nstruct kvm_book3e_206_tlb_entry *gtlbe)\r\n{\r\nunsigned long start, end, size;\r\nsize = get_tlb_bytes(gtlbe);\r\nstart = get_tlb_eaddr(gtlbe) & ~(size - 1);\r\nend = start + size - 1;\r\nreturn vcpu_e500->tlb1_min_eaddr == start ||\r\nvcpu_e500->tlb1_max_eaddr == end;\r\n}\r\nstatic void kvmppc_set_tlb1map_range(struct kvm_vcpu *vcpu,\r\nstruct kvm_book3e_206_tlb_entry *gtlbe)\r\n{\r\nunsigned long start, end, size;\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nif (!get_tlb_v(gtlbe))\r\nreturn;\r\nsize = get_tlb_bytes(gtlbe);\r\nstart = get_tlb_eaddr(gtlbe) & ~(size - 1);\r\nend = start + size - 1;\r\nvcpu_e500->tlb1_min_eaddr = min(vcpu_e500->tlb1_min_eaddr, start);\r\nvcpu_e500->tlb1_max_eaddr = max(vcpu_e500->tlb1_max_eaddr, end);\r\n}\r\nstatic inline int kvmppc_e500_gtlbe_invalidate(\r\nstruct kvmppc_vcpu_e500 *vcpu_e500,\r\nint tlbsel, int esel)\r\n{\r\nstruct kvm_book3e_206_tlb_entry *gtlbe =\r\nget_entry(vcpu_e500, tlbsel, esel);\r\nif (unlikely(get_tlb_iprot(gtlbe)))\r\nreturn -1;\r\nif (tlbsel == 1 && kvmppc_need_recalc_tlb1map_range(vcpu_e500, gtlbe))\r\nkvmppc_recalc_tlb1map_range(vcpu_e500);\r\ngtlbe->mas1 = 0;\r\nreturn 0;\r\n}\r\nint kvmppc_e500_emul_mt_mmucsr0(struct kvmppc_vcpu_e500 *vcpu_e500, ulong value)\r\n{\r\nint esel;\r\nif (value & MMUCSR0_TLB0FI)\r\nfor (esel = 0; esel < vcpu_e500->gtlb_params[0].entries; esel++)\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, 0, esel);\r\nif (value & MMUCSR0_TLB1FI)\r\nfor (esel = 0; esel < vcpu_e500->gtlb_params[1].entries; esel++)\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, 1, esel);\r\nkvmppc_core_flush_tlb(&vcpu_e500->vcpu);\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_e500_emul_tlbivax(struct kvm_vcpu *vcpu, gva_t ea)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nunsigned int ia;\r\nint esel, tlbsel;\r\nia = (ea >> 2) & 0x1;\r\ntlbsel = (ea >> 3) & 0x1;\r\nif (ia) {\r\nfor (esel = 0; esel < vcpu_e500->gtlb_params[tlbsel].entries;\r\nesel++)\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, tlbsel, esel);\r\n} else {\r\nea &= 0xfffff000;\r\nesel = kvmppc_e500_tlb_index(vcpu_e500, ea, tlbsel,\r\nget_cur_pid(vcpu), -1);\r\nif (esel >= 0)\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, tlbsel, esel);\r\n}\r\nkvmppc_core_flush_tlb(&vcpu_e500->vcpu);\r\nreturn EMULATE_DONE;\r\n}\r\nstatic void tlbilx_all(struct kvmppc_vcpu_e500 *vcpu_e500, int tlbsel,\r\nint pid, int type)\r\n{\r\nstruct kvm_book3e_206_tlb_entry *tlbe;\r\nint tid, esel;\r\nfor (esel = 0; esel < vcpu_e500->gtlb_params[tlbsel].entries; esel++) {\r\ntlbe = get_entry(vcpu_e500, tlbsel, esel);\r\ntid = get_tlb_tid(tlbe);\r\nif (type == 0 || tid == pid) {\r\ninval_gtlbe_on_host(vcpu_e500, tlbsel, esel);\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, tlbsel, esel);\r\n}\r\n}\r\n}\r\nstatic void tlbilx_one(struct kvmppc_vcpu_e500 *vcpu_e500, int pid,\r\ngva_t ea)\r\n{\r\nint tlbsel, esel;\r\nfor (tlbsel = 0; tlbsel < 2; tlbsel++) {\r\nesel = kvmppc_e500_tlb_index(vcpu_e500, ea, tlbsel, pid, -1);\r\nif (esel >= 0) {\r\ninval_gtlbe_on_host(vcpu_e500, tlbsel, esel);\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, tlbsel, esel);\r\nbreak;\r\n}\r\n}\r\n}\r\nint kvmppc_e500_emul_tlbilx(struct kvm_vcpu *vcpu, int type, gva_t ea)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nint pid = get_cur_spid(vcpu);\r\nif (type == 0 || type == 1) {\r\ntlbilx_all(vcpu_e500, 0, pid, type);\r\ntlbilx_all(vcpu_e500, 1, pid, type);\r\n} else if (type == 3) {\r\ntlbilx_one(vcpu_e500, pid, ea);\r\n}\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_e500_emul_tlbre(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nint tlbsel, esel;\r\nstruct kvm_book3e_206_tlb_entry *gtlbe;\r\ntlbsel = get_tlb_tlbsel(vcpu);\r\nesel = get_tlb_esel(vcpu, tlbsel);\r\ngtlbe = get_entry(vcpu_e500, tlbsel, esel);\r\nvcpu->arch.shared->mas0 &= ~MAS0_NV(~0);\r\nvcpu->arch.shared->mas0 |= MAS0_NV(vcpu_e500->gtlb_nv[tlbsel]);\r\nvcpu->arch.shared->mas1 = gtlbe->mas1;\r\nvcpu->arch.shared->mas2 = gtlbe->mas2;\r\nvcpu->arch.shared->mas7_3 = gtlbe->mas7_3;\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_e500_emul_tlbsx(struct kvm_vcpu *vcpu, gva_t ea)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nint as = !!get_cur_sas(vcpu);\r\nunsigned int pid = get_cur_spid(vcpu);\r\nint esel, tlbsel;\r\nstruct kvm_book3e_206_tlb_entry *gtlbe = NULL;\r\nfor (tlbsel = 0; tlbsel < 2; tlbsel++) {\r\nesel = kvmppc_e500_tlb_index(vcpu_e500, ea, tlbsel, pid, as);\r\nif (esel >= 0) {\r\ngtlbe = get_entry(vcpu_e500, tlbsel, esel);\r\nbreak;\r\n}\r\n}\r\nif (gtlbe) {\r\nesel &= vcpu_e500->gtlb_params[tlbsel].ways - 1;\r\nvcpu->arch.shared->mas0 = MAS0_TLBSEL(tlbsel) | MAS0_ESEL(esel)\r\n| MAS0_NV(vcpu_e500->gtlb_nv[tlbsel]);\r\nvcpu->arch.shared->mas1 = gtlbe->mas1;\r\nvcpu->arch.shared->mas2 = gtlbe->mas2;\r\nvcpu->arch.shared->mas7_3 = gtlbe->mas7_3;\r\n} else {\r\nint victim;\r\ntlbsel = vcpu->arch.shared->mas4 >> 28 & 0x1;\r\nvictim = (tlbsel == 0) ? gtlb0_get_next_victim(vcpu_e500) : 0;\r\nvcpu->arch.shared->mas0 = MAS0_TLBSEL(tlbsel)\r\n| MAS0_ESEL(victim)\r\n| MAS0_NV(vcpu_e500->gtlb_nv[tlbsel]);\r\nvcpu->arch.shared->mas1 =\r\n(vcpu->arch.shared->mas6 & MAS6_SPID0)\r\n| (vcpu->arch.shared->mas6 & (MAS6_SAS ? MAS1_TS : 0))\r\n| (vcpu->arch.shared->mas4 & MAS4_TSIZED(~0));\r\nvcpu->arch.shared->mas2 &= MAS2_EPN;\r\nvcpu->arch.shared->mas2 |= vcpu->arch.shared->mas4 &\r\nMAS2_ATTRIB_MASK;\r\nvcpu->arch.shared->mas7_3 &= MAS3_U0 | MAS3_U1 |\r\nMAS3_U2 | MAS3_U3;\r\n}\r\nkvmppc_set_exit_type(vcpu, EMULATED_TLBSX_EXITS);\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_e500_emul_tlbwe(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nstruct kvm_book3e_206_tlb_entry *gtlbe;\r\nint tlbsel, esel;\r\nint recal = 0;\r\nint idx;\r\ntlbsel = get_tlb_tlbsel(vcpu);\r\nesel = get_tlb_esel(vcpu, tlbsel);\r\ngtlbe = get_entry(vcpu_e500, tlbsel, esel);\r\nif (get_tlb_v(gtlbe)) {\r\ninval_gtlbe_on_host(vcpu_e500, tlbsel, esel);\r\nif ((tlbsel == 1) &&\r\nkvmppc_need_recalc_tlb1map_range(vcpu_e500, gtlbe))\r\nrecal = 1;\r\n}\r\ngtlbe->mas1 = vcpu->arch.shared->mas1;\r\ngtlbe->mas2 = vcpu->arch.shared->mas2;\r\nif (!(vcpu->arch.shared->msr & MSR_CM))\r\ngtlbe->mas2 &= 0xffffffffUL;\r\ngtlbe->mas7_3 = vcpu->arch.shared->mas7_3;\r\ntrace_kvm_booke206_gtlb_write(vcpu->arch.shared->mas0, gtlbe->mas1,\r\ngtlbe->mas2, gtlbe->mas7_3);\r\nif (tlbsel == 1) {\r\nif (recal)\r\nkvmppc_recalc_tlb1map_range(vcpu_e500);\r\nelse\r\nkvmppc_set_tlb1map_range(vcpu, gtlbe);\r\n}\r\nidx = srcu_read_lock(&vcpu->kvm->srcu);\r\nif (tlbe_is_host_safe(vcpu, gtlbe)) {\r\nu64 eaddr = get_tlb_eaddr(gtlbe);\r\nu64 raddr = get_tlb_raddr(gtlbe);\r\nif (tlbsel == 0) {\r\ngtlbe->mas1 &= ~MAS1_TSIZE(~0);\r\ngtlbe->mas1 |= MAS1_TSIZE(BOOK3E_PAGESZ_4K);\r\n}\r\nkvmppc_mmu_map(vcpu, eaddr, raddr, index_of(tlbsel, esel));\r\n}\r\nsrcu_read_unlock(&vcpu->kvm->srcu, idx);\r\nkvmppc_set_exit_type(vcpu, EMULATED_TLBWE_EXITS);\r\nreturn EMULATE_DONE;\r\n}\r\nstatic int kvmppc_e500_tlb_search(struct kvm_vcpu *vcpu,\r\ngva_t eaddr, unsigned int pid, int as)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nint esel, tlbsel;\r\nfor (tlbsel = 0; tlbsel < 2; tlbsel++) {\r\nesel = kvmppc_e500_tlb_index(vcpu_e500, eaddr, tlbsel, pid, as);\r\nif (esel >= 0)\r\nreturn index_of(tlbsel, esel);\r\n}\r\nreturn -1;\r\n}\r\nint kvmppc_core_vcpu_translate(struct kvm_vcpu *vcpu,\r\nstruct kvm_translation *tr)\r\n{\r\nint index;\r\ngva_t eaddr;\r\nu8 pid;\r\nu8 as;\r\neaddr = tr->linear_address;\r\npid = (tr->linear_address >> 32) & 0xff;\r\nas = (tr->linear_address >> 40) & 0x1;\r\nindex = kvmppc_e500_tlb_search(vcpu, eaddr, pid, as);\r\nif (index < 0) {\r\ntr->valid = 0;\r\nreturn 0;\r\n}\r\ntr->physical_address = kvmppc_mmu_xlate(vcpu, index, eaddr);\r\ntr->valid = 1;\r\nreturn 0;\r\n}\r\nint kvmppc_mmu_itlb_index(struct kvm_vcpu *vcpu, gva_t eaddr)\r\n{\r\nunsigned int as = !!(vcpu->arch.shared->msr & MSR_IS);\r\nreturn kvmppc_e500_tlb_search(vcpu, eaddr, get_cur_pid(vcpu), as);\r\n}\r\nint kvmppc_mmu_dtlb_index(struct kvm_vcpu *vcpu, gva_t eaddr)\r\n{\r\nunsigned int as = !!(vcpu->arch.shared->msr & MSR_DS);\r\nreturn kvmppc_e500_tlb_search(vcpu, eaddr, get_cur_pid(vcpu), as);\r\n}\r\nvoid kvmppc_mmu_itlb_miss(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned int as = !!(vcpu->arch.shared->msr & MSR_IS);\r\nkvmppc_e500_deliver_tlb_miss(vcpu, vcpu->arch.pc, as);\r\n}\r\nvoid kvmppc_mmu_dtlb_miss(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned int as = !!(vcpu->arch.shared->msr & MSR_DS);\r\nkvmppc_e500_deliver_tlb_miss(vcpu, vcpu->arch.fault_dear, as);\r\n}\r\ngpa_t kvmppc_mmu_xlate(struct kvm_vcpu *vcpu, unsigned int index,\r\ngva_t eaddr)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nstruct kvm_book3e_206_tlb_entry *gtlbe;\r\nu64 pgmask;\r\ngtlbe = get_entry(vcpu_e500, tlbsel_of(index), esel_of(index));\r\npgmask = get_tlb_bytes(gtlbe) - 1;\r\nreturn get_tlb_raddr(gtlbe) | (eaddr & pgmask);\r\n}\r\nvoid kvmppc_mmu_destroy_e500(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nstatic void free_gtlb(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nint i;\r\nkvmppc_core_flush_tlb(&vcpu_e500->vcpu);\r\nkfree(vcpu_e500->g2h_tlb1_map);\r\nkfree(vcpu_e500->gtlb_priv[0]);\r\nkfree(vcpu_e500->gtlb_priv[1]);\r\nif (vcpu_e500->shared_tlb_pages) {\r\nvfree((void *)(round_down((uintptr_t)vcpu_e500->gtlb_arch,\r\nPAGE_SIZE)));\r\nfor (i = 0; i < vcpu_e500->num_shared_tlb_pages; i++) {\r\nset_page_dirty_lock(vcpu_e500->shared_tlb_pages[i]);\r\nput_page(vcpu_e500->shared_tlb_pages[i]);\r\n}\r\nvcpu_e500->num_shared_tlb_pages = 0;\r\nkfree(vcpu_e500->shared_tlb_pages);\r\nvcpu_e500->shared_tlb_pages = NULL;\r\n} else {\r\nkfree(vcpu_e500->gtlb_arch);\r\n}\r\nvcpu_e500->gtlb_arch = NULL;\r\n}\r\nvoid kvmppc_get_sregs_e500_tlb(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\r\n{\r\nsregs->u.e.mas0 = vcpu->arch.shared->mas0;\r\nsregs->u.e.mas1 = vcpu->arch.shared->mas1;\r\nsregs->u.e.mas2 = vcpu->arch.shared->mas2;\r\nsregs->u.e.mas7_3 = vcpu->arch.shared->mas7_3;\r\nsregs->u.e.mas4 = vcpu->arch.shared->mas4;\r\nsregs->u.e.mas6 = vcpu->arch.shared->mas6;\r\nsregs->u.e.mmucfg = vcpu->arch.mmucfg;\r\nsregs->u.e.tlbcfg[0] = vcpu->arch.tlbcfg[0];\r\nsregs->u.e.tlbcfg[1] = vcpu->arch.tlbcfg[1];\r\nsregs->u.e.tlbcfg[2] = 0;\r\nsregs->u.e.tlbcfg[3] = 0;\r\n}\r\nint kvmppc_set_sregs_e500_tlb(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\r\n{\r\nif (sregs->u.e.features & KVM_SREGS_E_ARCH206_MMU) {\r\nvcpu->arch.shared->mas0 = sregs->u.e.mas0;\r\nvcpu->arch.shared->mas1 = sregs->u.e.mas1;\r\nvcpu->arch.shared->mas2 = sregs->u.e.mas2;\r\nvcpu->arch.shared->mas7_3 = sregs->u.e.mas7_3;\r\nvcpu->arch.shared->mas4 = sregs->u.e.mas4;\r\nvcpu->arch.shared->mas6 = sregs->u.e.mas6;\r\n}\r\nreturn 0;\r\n}\r\nint kvmppc_get_one_reg_e500_tlb(struct kvm_vcpu *vcpu, u64 id,\r\nunion kvmppc_one_reg *val)\r\n{\r\nint r = 0;\r\nlong int i;\r\nswitch (id) {\r\ncase KVM_REG_PPC_MAS0:\r\n*val = get_reg_val(id, vcpu->arch.shared->mas0);\r\nbreak;\r\ncase KVM_REG_PPC_MAS1:\r\n*val = get_reg_val(id, vcpu->arch.shared->mas1);\r\nbreak;\r\ncase KVM_REG_PPC_MAS2:\r\n*val = get_reg_val(id, vcpu->arch.shared->mas2);\r\nbreak;\r\ncase KVM_REG_PPC_MAS7_3:\r\n*val = get_reg_val(id, vcpu->arch.shared->mas7_3);\r\nbreak;\r\ncase KVM_REG_PPC_MAS4:\r\n*val = get_reg_val(id, vcpu->arch.shared->mas4);\r\nbreak;\r\ncase KVM_REG_PPC_MAS6:\r\n*val = get_reg_val(id, vcpu->arch.shared->mas6);\r\nbreak;\r\ncase KVM_REG_PPC_MMUCFG:\r\n*val = get_reg_val(id, vcpu->arch.mmucfg);\r\nbreak;\r\ncase KVM_REG_PPC_EPTCFG:\r\n*val = get_reg_val(id, vcpu->arch.eptcfg);\r\nbreak;\r\ncase KVM_REG_PPC_TLB0CFG:\r\ncase KVM_REG_PPC_TLB1CFG:\r\ncase KVM_REG_PPC_TLB2CFG:\r\ncase KVM_REG_PPC_TLB3CFG:\r\ni = id - KVM_REG_PPC_TLB0CFG;\r\n*val = get_reg_val(id, vcpu->arch.tlbcfg[i]);\r\nbreak;\r\ncase KVM_REG_PPC_TLB0PS:\r\ncase KVM_REG_PPC_TLB1PS:\r\ncase KVM_REG_PPC_TLB2PS:\r\ncase KVM_REG_PPC_TLB3PS:\r\ni = id - KVM_REG_PPC_TLB0PS;\r\n*val = get_reg_val(id, vcpu->arch.tlbps[i]);\r\nbreak;\r\ndefault:\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nint kvmppc_set_one_reg_e500_tlb(struct kvm_vcpu *vcpu, u64 id,\r\nunion kvmppc_one_reg *val)\r\n{\r\nint r = 0;\r\nlong int i;\r\nswitch (id) {\r\ncase KVM_REG_PPC_MAS0:\r\nvcpu->arch.shared->mas0 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_MAS1:\r\nvcpu->arch.shared->mas1 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_MAS2:\r\nvcpu->arch.shared->mas2 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_MAS7_3:\r\nvcpu->arch.shared->mas7_3 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_MAS4:\r\nvcpu->arch.shared->mas4 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_MAS6:\r\nvcpu->arch.shared->mas6 = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_MMUCFG: {\r\nu32 reg = set_reg_val(id, *val);\r\nif (reg != vcpu->arch.mmucfg)\r\nr = -EINVAL;\r\nbreak;\r\n}\r\ncase KVM_REG_PPC_EPTCFG: {\r\nu32 reg = set_reg_val(id, *val);\r\nif (reg != vcpu->arch.eptcfg)\r\nr = -EINVAL;\r\nbreak;\r\n}\r\ncase KVM_REG_PPC_TLB0CFG:\r\ncase KVM_REG_PPC_TLB1CFG:\r\ncase KVM_REG_PPC_TLB2CFG:\r\ncase KVM_REG_PPC_TLB3CFG: {\r\nu32 reg = set_reg_val(id, *val);\r\ni = id - KVM_REG_PPC_TLB0CFG;\r\nif (reg != vcpu->arch.tlbcfg[i])\r\nr = -EINVAL;\r\nbreak;\r\n}\r\ncase KVM_REG_PPC_TLB0PS:\r\ncase KVM_REG_PPC_TLB1PS:\r\ncase KVM_REG_PPC_TLB2PS:\r\ncase KVM_REG_PPC_TLB3PS: {\r\nu32 reg = set_reg_val(id, *val);\r\ni = id - KVM_REG_PPC_TLB0PS;\r\nif (reg != vcpu->arch.tlbps[i])\r\nr = -EINVAL;\r\nbreak;\r\n}\r\ndefault:\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstatic int vcpu_mmu_geometry_update(struct kvm_vcpu *vcpu,\r\nstruct kvm_book3e_206_tlb_params *params)\r\n{\r\nvcpu->arch.tlbcfg[0] &= ~(TLBnCFG_N_ENTRY | TLBnCFG_ASSOC);\r\nif (params->tlb_sizes[0] <= 2048)\r\nvcpu->arch.tlbcfg[0] |= params->tlb_sizes[0];\r\nvcpu->arch.tlbcfg[0] |= params->tlb_ways[0] << TLBnCFG_ASSOC_SHIFT;\r\nvcpu->arch.tlbcfg[1] &= ~(TLBnCFG_N_ENTRY | TLBnCFG_ASSOC);\r\nvcpu->arch.tlbcfg[1] |= params->tlb_sizes[1];\r\nvcpu->arch.tlbcfg[1] |= params->tlb_ways[1] << TLBnCFG_ASSOC_SHIFT;\r\nreturn 0;\r\n}\r\nint kvm_vcpu_ioctl_config_tlb(struct kvm_vcpu *vcpu,\r\nstruct kvm_config_tlb *cfg)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nstruct kvm_book3e_206_tlb_params params;\r\nchar *virt;\r\nstruct page **pages;\r\nstruct tlbe_priv *privs[2] = {};\r\nu64 *g2h_bitmap = NULL;\r\nsize_t array_len;\r\nu32 sets;\r\nint num_pages, ret, i;\r\nif (cfg->mmu_type != KVM_MMU_FSL_BOOKE_NOHV)\r\nreturn -EINVAL;\r\nif (copy_from_user(&params, (void __user *)(uintptr_t)cfg->params,\r\nsizeof(params)))\r\nreturn -EFAULT;\r\nif (params.tlb_sizes[1] > 64)\r\nreturn -EINVAL;\r\nif (params.tlb_ways[1] != params.tlb_sizes[1])\r\nreturn -EINVAL;\r\nif (params.tlb_sizes[2] != 0 || params.tlb_sizes[3] != 0)\r\nreturn -EINVAL;\r\nif (params.tlb_ways[2] != 0 || params.tlb_ways[3] != 0)\r\nreturn -EINVAL;\r\nif (!is_power_of_2(params.tlb_ways[0]))\r\nreturn -EINVAL;\r\nsets = params.tlb_sizes[0] >> ilog2(params.tlb_ways[0]);\r\nif (!is_power_of_2(sets))\r\nreturn -EINVAL;\r\narray_len = params.tlb_sizes[0] + params.tlb_sizes[1];\r\narray_len *= sizeof(struct kvm_book3e_206_tlb_entry);\r\nif (cfg->array_len < array_len)\r\nreturn -EINVAL;\r\nnum_pages = DIV_ROUND_UP(cfg->array + array_len - 1, PAGE_SIZE) -\r\ncfg->array / PAGE_SIZE;\r\npages = kmalloc(sizeof(struct page *) * num_pages, GFP_KERNEL);\r\nif (!pages)\r\nreturn -ENOMEM;\r\nret = get_user_pages_fast(cfg->array, num_pages, 1, pages);\r\nif (ret < 0)\r\ngoto err_pages;\r\nif (ret != num_pages) {\r\nnum_pages = ret;\r\nret = -EFAULT;\r\ngoto err_put_page;\r\n}\r\nvirt = vmap(pages, num_pages, VM_MAP, PAGE_KERNEL);\r\nif (!virt) {\r\nret = -ENOMEM;\r\ngoto err_put_page;\r\n}\r\nprivs[0] = kzalloc(sizeof(struct tlbe_priv) * params.tlb_sizes[0],\r\nGFP_KERNEL);\r\nprivs[1] = kzalloc(sizeof(struct tlbe_priv) * params.tlb_sizes[1],\r\nGFP_KERNEL);\r\nif (!privs[0] || !privs[1]) {\r\nret = -ENOMEM;\r\ngoto err_privs;\r\n}\r\ng2h_bitmap = kzalloc(sizeof(u64) * params.tlb_sizes[1],\r\nGFP_KERNEL);\r\nif (!g2h_bitmap) {\r\nret = -ENOMEM;\r\ngoto err_privs;\r\n}\r\nfree_gtlb(vcpu_e500);\r\nvcpu_e500->gtlb_priv[0] = privs[0];\r\nvcpu_e500->gtlb_priv[1] = privs[1];\r\nvcpu_e500->g2h_tlb1_map = g2h_bitmap;\r\nvcpu_e500->gtlb_arch = (struct kvm_book3e_206_tlb_entry *)\r\n(virt + (cfg->array & (PAGE_SIZE - 1)));\r\nvcpu_e500->gtlb_params[0].entries = params.tlb_sizes[0];\r\nvcpu_e500->gtlb_params[1].entries = params.tlb_sizes[1];\r\nvcpu_e500->gtlb_offset[0] = 0;\r\nvcpu_e500->gtlb_offset[1] = params.tlb_sizes[0];\r\nvcpu_mmu_geometry_update(vcpu, &params);\r\nvcpu_e500->shared_tlb_pages = pages;\r\nvcpu_e500->num_shared_tlb_pages = num_pages;\r\nvcpu_e500->gtlb_params[0].ways = params.tlb_ways[0];\r\nvcpu_e500->gtlb_params[0].sets = sets;\r\nvcpu_e500->gtlb_params[1].ways = params.tlb_sizes[1];\r\nvcpu_e500->gtlb_params[1].sets = 1;\r\nkvmppc_recalc_tlb1map_range(vcpu_e500);\r\nreturn 0;\r\nerr_privs:\r\nkfree(privs[0]);\r\nkfree(privs[1]);\r\nerr_put_page:\r\nfor (i = 0; i < num_pages; i++)\r\nput_page(pages[i]);\r\nerr_pages:\r\nkfree(pages);\r\nreturn ret;\r\n}\r\nint kvm_vcpu_ioctl_dirty_tlb(struct kvm_vcpu *vcpu,\r\nstruct kvm_dirty_tlb *dirty)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nkvmppc_recalc_tlb1map_range(vcpu_e500);\r\nkvmppc_core_flush_tlb(vcpu);\r\nreturn 0;\r\n}\r\nstatic int vcpu_mmu_init(struct kvm_vcpu *vcpu,\r\nstruct kvmppc_e500_tlb_params *params)\r\n{\r\nvcpu->arch.mmucfg = mfspr(SPRN_MMUCFG) & ~MMUCFG_LPIDSIZE;\r\nvcpu->arch.tlbcfg[0] = mfspr(SPRN_TLB0CFG) &\r\n~(TLBnCFG_N_ENTRY | TLBnCFG_ASSOC);\r\nvcpu->arch.tlbcfg[0] |= params[0].entries;\r\nvcpu->arch.tlbcfg[0] |= params[0].ways << TLBnCFG_ASSOC_SHIFT;\r\nvcpu->arch.tlbcfg[1] = mfspr(SPRN_TLB1CFG) &\r\n~(TLBnCFG_N_ENTRY | TLBnCFG_ASSOC);\r\nvcpu->arch.tlbcfg[1] |= params[1].entries;\r\nvcpu->arch.tlbcfg[1] |= params[1].ways << TLBnCFG_ASSOC_SHIFT;\r\nif (has_feature(vcpu, VCPU_FTR_MMU_V2)) {\r\nvcpu->arch.tlbps[0] = mfspr(SPRN_TLB0PS);\r\nvcpu->arch.tlbps[1] = mfspr(SPRN_TLB1PS);\r\nvcpu->arch.mmucfg &= ~MMUCFG_LRAT;\r\nvcpu->arch.eptcfg = 0;\r\nvcpu->arch.tlbcfg[0] &= ~TLBnCFG_PT;\r\nvcpu->arch.tlbcfg[1] &= ~TLBnCFG_IND;\r\n}\r\nreturn 0;\r\n}\r\nint kvmppc_e500_tlb_init(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nstruct kvm_vcpu *vcpu = &vcpu_e500->vcpu;\r\nint entry_size = sizeof(struct kvm_book3e_206_tlb_entry);\r\nint entries = KVM_E500_TLB0_SIZE + KVM_E500_TLB1_SIZE;\r\nif (e500_mmu_host_init(vcpu_e500))\r\ngoto err;\r\nvcpu_e500->gtlb_params[0].entries = KVM_E500_TLB0_SIZE;\r\nvcpu_e500->gtlb_params[1].entries = KVM_E500_TLB1_SIZE;\r\nvcpu_e500->gtlb_params[0].ways = KVM_E500_TLB0_WAY_NUM;\r\nvcpu_e500->gtlb_params[0].sets =\r\nKVM_E500_TLB0_SIZE / KVM_E500_TLB0_WAY_NUM;\r\nvcpu_e500->gtlb_params[1].ways = KVM_E500_TLB1_SIZE;\r\nvcpu_e500->gtlb_params[1].sets = 1;\r\nvcpu_e500->gtlb_arch = kmalloc(entries * entry_size, GFP_KERNEL);\r\nif (!vcpu_e500->gtlb_arch)\r\nreturn -ENOMEM;\r\nvcpu_e500->gtlb_offset[0] = 0;\r\nvcpu_e500->gtlb_offset[1] = KVM_E500_TLB0_SIZE;\r\nvcpu_e500->gtlb_priv[0] = kzalloc(sizeof(struct tlbe_ref) *\r\nvcpu_e500->gtlb_params[0].entries,\r\nGFP_KERNEL);\r\nif (!vcpu_e500->gtlb_priv[0])\r\ngoto err;\r\nvcpu_e500->gtlb_priv[1] = kzalloc(sizeof(struct tlbe_ref) *\r\nvcpu_e500->gtlb_params[1].entries,\r\nGFP_KERNEL);\r\nif (!vcpu_e500->gtlb_priv[1])\r\ngoto err;\r\nvcpu_e500->g2h_tlb1_map = kzalloc(sizeof(u64) *\r\nvcpu_e500->gtlb_params[1].entries,\r\nGFP_KERNEL);\r\nif (!vcpu_e500->g2h_tlb1_map)\r\ngoto err;\r\nvcpu_mmu_init(vcpu, vcpu_e500->gtlb_params);\r\nkvmppc_recalc_tlb1map_range(vcpu_e500);\r\nreturn 0;\r\nerr:\r\nfree_gtlb(vcpu_e500);\r\nreturn -1;\r\n}\r\nvoid kvmppc_e500_tlb_uninit(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nfree_gtlb(vcpu_e500);\r\ne500_mmu_host_uninit(vcpu_e500);\r\n}
