static inline int tlb1_next(void)\r\n{\r\nstruct paca_struct *paca = get_paca();\r\nstruct tlb_core_data *tcd;\r\nint this, next;\r\ntcd = paca->tcd_ptr;\r\nthis = tcd->esel_next;\r\nnext = this + 1;\r\nif (next >= tcd->esel_max)\r\nnext = tcd->esel_first;\r\ntcd->esel_next = next;\r\nreturn this;\r\n}\r\nstatic inline int tlb1_next(void)\r\n{\r\nint index, ncams;\r\nncams = mfspr(SPRN_TLB1CFG) & TLBnCFG_N_ENTRY;\r\nindex = this_cpu_read(next_tlbcam_idx);\r\nif (unlikely(index == ncams - 1))\r\n__this_cpu_write(next_tlbcam_idx, tlbcam_index);\r\nelse\r\n__this_cpu_inc(next_tlbcam_idx);\r\nreturn index;\r\n}\r\nstatic inline int mmu_get_tsize(int psize)\r\n{\r\nreturn mmu_psize_defs[psize].enc;\r\n}\r\nstatic inline int book3e_tlb_exists(unsigned long ea, unsigned long pid)\r\n{\r\nint found = 0;\r\nmtspr(SPRN_MAS6, pid << 16);\r\nif (mmu_has_feature(MMU_FTR_USE_TLBRSRV)) {\r\nasm volatile(\r\n"li %0,0\n"\r\n"tlbsx. 0,%1\n"\r\n"bne 1f\n"\r\n"li %0,1\n"\r\n"1:\n"\r\n: "=&r"(found) : "r"(ea));\r\n} else {\r\nasm volatile(\r\n"tlbsx 0,%1\n"\r\n"mfspr %0,0x271\n"\r\n"srwi %0,%0,31\n"\r\n: "=&r"(found) : "r"(ea));\r\n}\r\nreturn found;\r\n}\r\nvoid book3e_hugetlb_preload(struct vm_area_struct *vma, unsigned long ea,\r\npte_t pte)\r\n{\r\nunsigned long mas1, mas2;\r\nu64 mas7_3;\r\nunsigned long psize, tsize, shift;\r\nunsigned long flags;\r\nstruct mm_struct *mm;\r\n#ifdef CONFIG_PPC_FSL_BOOK3E\r\nint index;\r\n#endif\r\nif (unlikely(is_kernel_addr(ea)))\r\nreturn;\r\nmm = vma->vm_mm;\r\n#ifdef CONFIG_PPC_MM_SLICES\r\npsize = get_slice_psize(mm, ea);\r\ntsize = mmu_get_tsize(psize);\r\nshift = mmu_psize_defs[psize].shift;\r\n#else\r\npsize = vma_mmu_pagesize(vma);\r\nshift = __ilog2(psize);\r\ntsize = shift - 10;\r\n#endif\r\nlocal_irq_save(flags);\r\nif (unlikely(book3e_tlb_exists(ea, mm->context.id))) {\r\nlocal_irq_restore(flags);\r\nreturn;\r\n}\r\n#ifdef CONFIG_PPC_FSL_BOOK3E\r\nindex = tlb1_next();\r\nmtspr(SPRN_MAS0, MAS0_ESEL(index) | MAS0_TLBSEL(1));\r\n#endif\r\nmas1 = MAS1_VALID | MAS1_TID(mm->context.id) | MAS1_TSIZE(tsize);\r\nmas2 = ea & ~((1UL << shift) - 1);\r\nmas2 |= (pte_val(pte) >> PTE_WIMGE_SHIFT) & MAS2_WIMGE_MASK;\r\nmas7_3 = (u64)pte_pfn(pte) << PAGE_SHIFT;\r\nmas7_3 |= (pte_val(pte) >> PTE_BAP_SHIFT) & MAS3_BAP_MASK;\r\nif (!pte_dirty(pte))\r\nmas7_3 &= ~(MAS3_SW|MAS3_UW);\r\nmtspr(SPRN_MAS1, mas1);\r\nmtspr(SPRN_MAS2, mas2);\r\nif (mmu_has_feature(MMU_FTR_USE_PAIRED_MAS)) {\r\nmtspr(SPRN_MAS7_MAS3, mas7_3);\r\n} else {\r\nif (mmu_has_feature(MMU_FTR_BIG_PHYS))\r\nmtspr(SPRN_MAS7, upper_32_bits(mas7_3));\r\nmtspr(SPRN_MAS3, lower_32_bits(mas7_3));\r\n}\r\nasm volatile ("tlbwe");\r\nlocal_irq_restore(flags);\r\n}\r\nvoid flush_hugetlb_page(struct vm_area_struct *vma, unsigned long vmaddr)\r\n{\r\nstruct hstate *hstate = hstate_file(vma->vm_file);\r\nunsigned long tsize = huge_page_shift(hstate) - 10;\r\n__flush_tlb_page(vma->vm_mm, vmaddr, tsize, 0);\r\n}
