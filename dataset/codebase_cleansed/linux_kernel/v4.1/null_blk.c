static int null_param_store_val(const char *str, int *val, int min, int max)\r\n{\r\nint ret, new_val;\r\nret = kstrtoint(str, 10, &new_val);\r\nif (ret)\r\nreturn -EINVAL;\r\nif (new_val < min || new_val > max)\r\nreturn -EINVAL;\r\n*val = new_val;\r\nreturn 0;\r\n}\r\nstatic int null_set_queue_mode(const char *str, const struct kernel_param *kp)\r\n{\r\nreturn null_param_store_val(str, &queue_mode, NULL_Q_BIO, NULL_Q_MQ);\r\n}\r\nstatic int null_set_irqmode(const char *str, const struct kernel_param *kp)\r\n{\r\nreturn null_param_store_val(str, &irqmode, NULL_IRQ_NONE,\r\nNULL_IRQ_TIMER);\r\n}\r\nstatic void put_tag(struct nullb_queue *nq, unsigned int tag)\r\n{\r\nclear_bit_unlock(tag, nq->tag_map);\r\nif (waitqueue_active(&nq->wait))\r\nwake_up(&nq->wait);\r\n}\r\nstatic unsigned int get_tag(struct nullb_queue *nq)\r\n{\r\nunsigned int tag;\r\ndo {\r\ntag = find_first_zero_bit(nq->tag_map, nq->queue_depth);\r\nif (tag >= nq->queue_depth)\r\nreturn -1U;\r\n} while (test_and_set_bit_lock(tag, nq->tag_map));\r\nreturn tag;\r\n}\r\nstatic void free_cmd(struct nullb_cmd *cmd)\r\n{\r\nput_tag(cmd->nq, cmd->tag);\r\n}\r\nstatic struct nullb_cmd *__alloc_cmd(struct nullb_queue *nq)\r\n{\r\nstruct nullb_cmd *cmd;\r\nunsigned int tag;\r\ntag = get_tag(nq);\r\nif (tag != -1U) {\r\ncmd = &nq->cmds[tag];\r\ncmd->tag = tag;\r\ncmd->nq = nq;\r\nreturn cmd;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct nullb_cmd *alloc_cmd(struct nullb_queue *nq, int can_wait)\r\n{\r\nstruct nullb_cmd *cmd;\r\nDEFINE_WAIT(wait);\r\ncmd = __alloc_cmd(nq);\r\nif (cmd || !can_wait)\r\nreturn cmd;\r\ndo {\r\nprepare_to_wait(&nq->wait, &wait, TASK_UNINTERRUPTIBLE);\r\ncmd = __alloc_cmd(nq);\r\nif (cmd)\r\nbreak;\r\nio_schedule();\r\n} while (1);\r\nfinish_wait(&nq->wait, &wait);\r\nreturn cmd;\r\n}\r\nstatic void end_cmd(struct nullb_cmd *cmd)\r\n{\r\nswitch (queue_mode) {\r\ncase NULL_Q_MQ:\r\nblk_mq_end_request(cmd->rq, 0);\r\nreturn;\r\ncase NULL_Q_RQ:\r\nINIT_LIST_HEAD(&cmd->rq->queuelist);\r\nblk_end_request_all(cmd->rq, 0);\r\nbreak;\r\ncase NULL_Q_BIO:\r\nbio_endio(cmd->bio, 0);\r\nbreak;\r\n}\r\nfree_cmd(cmd);\r\n}\r\nstatic enum hrtimer_restart null_cmd_timer_expired(struct hrtimer *timer)\r\n{\r\nstruct completion_queue *cq;\r\nstruct llist_node *entry;\r\nstruct nullb_cmd *cmd;\r\ncq = &per_cpu(completion_queues, smp_processor_id());\r\nwhile ((entry = llist_del_all(&cq->list)) != NULL) {\r\nentry = llist_reverse_order(entry);\r\ndo {\r\ncmd = container_of(entry, struct nullb_cmd, ll_list);\r\nentry = entry->next;\r\nend_cmd(cmd);\r\n} while (entry);\r\n}\r\nreturn HRTIMER_NORESTART;\r\n}\r\nstatic void null_cmd_end_timer(struct nullb_cmd *cmd)\r\n{\r\nstruct completion_queue *cq = &per_cpu(completion_queues, get_cpu());\r\ncmd->ll_list.next = NULL;\r\nif (llist_add(&cmd->ll_list, &cq->list)) {\r\nktime_t kt = ktime_set(0, completion_nsec);\r\nhrtimer_start(&cq->timer, kt, HRTIMER_MODE_REL);\r\n}\r\nput_cpu();\r\n}\r\nstatic void null_softirq_done_fn(struct request *rq)\r\n{\r\nif (queue_mode == NULL_Q_MQ)\r\nend_cmd(blk_mq_rq_to_pdu(rq));\r\nelse\r\nend_cmd(rq->special);\r\n}\r\nstatic inline void null_handle_cmd(struct nullb_cmd *cmd)\r\n{\r\nswitch (irqmode) {\r\ncase NULL_IRQ_SOFTIRQ:\r\nswitch (queue_mode) {\r\ncase NULL_Q_MQ:\r\nblk_mq_complete_request(cmd->rq);\r\nbreak;\r\ncase NULL_Q_RQ:\r\nblk_complete_request(cmd->rq);\r\nbreak;\r\ncase NULL_Q_BIO:\r\nend_cmd(cmd);\r\nbreak;\r\n}\r\nbreak;\r\ncase NULL_IRQ_NONE:\r\nend_cmd(cmd);\r\nbreak;\r\ncase NULL_IRQ_TIMER:\r\nnull_cmd_end_timer(cmd);\r\nbreak;\r\n}\r\n}\r\nstatic struct nullb_queue *nullb_to_queue(struct nullb *nullb)\r\n{\r\nint index = 0;\r\nif (nullb->nr_queues != 1)\r\nindex = raw_smp_processor_id() / ((nr_cpu_ids + nullb->nr_queues - 1) / nullb->nr_queues);\r\nreturn &nullb->queues[index];\r\n}\r\nstatic void null_queue_bio(struct request_queue *q, struct bio *bio)\r\n{\r\nstruct nullb *nullb = q->queuedata;\r\nstruct nullb_queue *nq = nullb_to_queue(nullb);\r\nstruct nullb_cmd *cmd;\r\ncmd = alloc_cmd(nq, 1);\r\ncmd->bio = bio;\r\nnull_handle_cmd(cmd);\r\n}\r\nstatic int null_rq_prep_fn(struct request_queue *q, struct request *req)\r\n{\r\nstruct nullb *nullb = q->queuedata;\r\nstruct nullb_queue *nq = nullb_to_queue(nullb);\r\nstruct nullb_cmd *cmd;\r\ncmd = alloc_cmd(nq, 0);\r\nif (cmd) {\r\ncmd->rq = req;\r\nreq->special = cmd;\r\nreturn BLKPREP_OK;\r\n}\r\nreturn BLKPREP_DEFER;\r\n}\r\nstatic void null_request_fn(struct request_queue *q)\r\n{\r\nstruct request *rq;\r\nwhile ((rq = blk_fetch_request(q)) != NULL) {\r\nstruct nullb_cmd *cmd = rq->special;\r\nspin_unlock_irq(q->queue_lock);\r\nnull_handle_cmd(cmd);\r\nspin_lock_irq(q->queue_lock);\r\n}\r\n}\r\nstatic int null_queue_rq(struct blk_mq_hw_ctx *hctx,\r\nconst struct blk_mq_queue_data *bd)\r\n{\r\nstruct nullb_cmd *cmd = blk_mq_rq_to_pdu(bd->rq);\r\ncmd->rq = bd->rq;\r\ncmd->nq = hctx->driver_data;\r\nblk_mq_start_request(bd->rq);\r\nnull_handle_cmd(cmd);\r\nreturn BLK_MQ_RQ_QUEUE_OK;\r\n}\r\nstatic void null_init_queue(struct nullb *nullb, struct nullb_queue *nq)\r\n{\r\nBUG_ON(!nullb);\r\nBUG_ON(!nq);\r\ninit_waitqueue_head(&nq->wait);\r\nnq->queue_depth = nullb->queue_depth;\r\n}\r\nstatic int null_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,\r\nunsigned int index)\r\n{\r\nstruct nullb *nullb = data;\r\nstruct nullb_queue *nq = &nullb->queues[index];\r\nhctx->driver_data = nq;\r\nnull_init_queue(nullb, nq);\r\nnullb->nr_queues++;\r\nreturn 0;\r\n}\r\nstatic void null_del_dev(struct nullb *nullb)\r\n{\r\nlist_del_init(&nullb->list);\r\ndel_gendisk(nullb->disk);\r\nblk_cleanup_queue(nullb->q);\r\nif (queue_mode == NULL_Q_MQ)\r\nblk_mq_free_tag_set(&nullb->tag_set);\r\nput_disk(nullb->disk);\r\nkfree(nullb);\r\n}\r\nstatic int null_open(struct block_device *bdev, fmode_t mode)\r\n{\r\nreturn 0;\r\n}\r\nstatic void null_release(struct gendisk *disk, fmode_t mode)\r\n{\r\n}\r\nstatic int setup_commands(struct nullb_queue *nq)\r\n{\r\nstruct nullb_cmd *cmd;\r\nint i, tag_size;\r\nnq->cmds = kzalloc(nq->queue_depth * sizeof(*cmd), GFP_KERNEL);\r\nif (!nq->cmds)\r\nreturn -ENOMEM;\r\ntag_size = ALIGN(nq->queue_depth, BITS_PER_LONG) / BITS_PER_LONG;\r\nnq->tag_map = kzalloc(tag_size * sizeof(unsigned long), GFP_KERNEL);\r\nif (!nq->tag_map) {\r\nkfree(nq->cmds);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < nq->queue_depth; i++) {\r\ncmd = &nq->cmds[i];\r\nINIT_LIST_HEAD(&cmd->list);\r\ncmd->ll_list.next = NULL;\r\ncmd->tag = -1U;\r\n}\r\nreturn 0;\r\n}\r\nstatic void cleanup_queue(struct nullb_queue *nq)\r\n{\r\nkfree(nq->tag_map);\r\nkfree(nq->cmds);\r\n}\r\nstatic void cleanup_queues(struct nullb *nullb)\r\n{\r\nint i;\r\nfor (i = 0; i < nullb->nr_queues; i++)\r\ncleanup_queue(&nullb->queues[i]);\r\nkfree(nullb->queues);\r\n}\r\nstatic int setup_queues(struct nullb *nullb)\r\n{\r\nnullb->queues = kzalloc(submit_queues * sizeof(struct nullb_queue),\r\nGFP_KERNEL);\r\nif (!nullb->queues)\r\nreturn -ENOMEM;\r\nnullb->nr_queues = 0;\r\nnullb->queue_depth = hw_queue_depth;\r\nreturn 0;\r\n}\r\nstatic int init_driver_queues(struct nullb *nullb)\r\n{\r\nstruct nullb_queue *nq;\r\nint i, ret = 0;\r\nfor (i = 0; i < submit_queues; i++) {\r\nnq = &nullb->queues[i];\r\nnull_init_queue(nullb, nq);\r\nret = setup_commands(nq);\r\nif (ret)\r\nreturn ret;\r\nnullb->nr_queues++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int null_add_dev(void)\r\n{\r\nstruct gendisk *disk;\r\nstruct nullb *nullb;\r\nsector_t size;\r\nint rv;\r\nnullb = kzalloc_node(sizeof(*nullb), GFP_KERNEL, home_node);\r\nif (!nullb) {\r\nrv = -ENOMEM;\r\ngoto out;\r\n}\r\nspin_lock_init(&nullb->lock);\r\nif (queue_mode == NULL_Q_MQ && use_per_node_hctx)\r\nsubmit_queues = nr_online_nodes;\r\nrv = setup_queues(nullb);\r\nif (rv)\r\ngoto out_free_nullb;\r\nif (queue_mode == NULL_Q_MQ) {\r\nnullb->tag_set.ops = &null_mq_ops;\r\nnullb->tag_set.nr_hw_queues = submit_queues;\r\nnullb->tag_set.queue_depth = hw_queue_depth;\r\nnullb->tag_set.numa_node = home_node;\r\nnullb->tag_set.cmd_size = sizeof(struct nullb_cmd);\r\nnullb->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;\r\nnullb->tag_set.driver_data = nullb;\r\nrv = blk_mq_alloc_tag_set(&nullb->tag_set);\r\nif (rv)\r\ngoto out_cleanup_queues;\r\nnullb->q = blk_mq_init_queue(&nullb->tag_set);\r\nif (IS_ERR(nullb->q)) {\r\nrv = -ENOMEM;\r\ngoto out_cleanup_tags;\r\n}\r\n} else if (queue_mode == NULL_Q_BIO) {\r\nnullb->q = blk_alloc_queue_node(GFP_KERNEL, home_node);\r\nif (!nullb->q) {\r\nrv = -ENOMEM;\r\ngoto out_cleanup_queues;\r\n}\r\nblk_queue_make_request(nullb->q, null_queue_bio);\r\nrv = init_driver_queues(nullb);\r\nif (rv)\r\ngoto out_cleanup_blk_queue;\r\n} else {\r\nnullb->q = blk_init_queue_node(null_request_fn, &nullb->lock, home_node);\r\nif (!nullb->q) {\r\nrv = -ENOMEM;\r\ngoto out_cleanup_queues;\r\n}\r\nblk_queue_prep_rq(nullb->q, null_rq_prep_fn);\r\nblk_queue_softirq_done(nullb->q, null_softirq_done_fn);\r\nrv = init_driver_queues(nullb);\r\nif (rv)\r\ngoto out_cleanup_blk_queue;\r\n}\r\nnullb->q->queuedata = nullb;\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, nullb->q);\r\nqueue_flag_clear_unlocked(QUEUE_FLAG_ADD_RANDOM, nullb->q);\r\ndisk = nullb->disk = alloc_disk_node(1, home_node);\r\nif (!disk) {\r\nrv = -ENOMEM;\r\ngoto out_cleanup_blk_queue;\r\n}\r\nmutex_lock(&lock);\r\nlist_add_tail(&nullb->list, &nullb_list);\r\nnullb->index = nullb_indexes++;\r\nmutex_unlock(&lock);\r\nblk_queue_logical_block_size(nullb->q, bs);\r\nblk_queue_physical_block_size(nullb->q, bs);\r\nsize = gb * 1024 * 1024 * 1024ULL;\r\nsector_div(size, bs);\r\nset_capacity(disk, size);\r\ndisk->flags |= GENHD_FL_EXT_DEVT | GENHD_FL_SUPPRESS_PARTITION_INFO;\r\ndisk->major = null_major;\r\ndisk->first_minor = nullb->index;\r\ndisk->fops = &null_fops;\r\ndisk->private_data = nullb;\r\ndisk->queue = nullb->q;\r\nsprintf(disk->disk_name, "nullb%d", nullb->index);\r\nadd_disk(disk);\r\nreturn 0;\r\nout_cleanup_blk_queue:\r\nblk_cleanup_queue(nullb->q);\r\nout_cleanup_tags:\r\nif (queue_mode == NULL_Q_MQ)\r\nblk_mq_free_tag_set(&nullb->tag_set);\r\nout_cleanup_queues:\r\ncleanup_queues(nullb);\r\nout_free_nullb:\r\nkfree(nullb);\r\nout:\r\nreturn rv;\r\n}\r\nstatic int __init null_init(void)\r\n{\r\nunsigned int i;\r\nif (bs > PAGE_SIZE) {\r\npr_warn("null_blk: invalid block size\n");\r\npr_warn("null_blk: defaults block size to %lu\n", PAGE_SIZE);\r\nbs = PAGE_SIZE;\r\n}\r\nif (queue_mode == NULL_Q_MQ && use_per_node_hctx) {\r\nif (submit_queues < nr_online_nodes) {\r\npr_warn("null_blk: submit_queues param is set to %u.",\r\nnr_online_nodes);\r\nsubmit_queues = nr_online_nodes;\r\n}\r\n} else if (submit_queues > nr_cpu_ids)\r\nsubmit_queues = nr_cpu_ids;\r\nelse if (!submit_queues)\r\nsubmit_queues = 1;\r\nmutex_init(&lock);\r\nfor_each_possible_cpu(i) {\r\nstruct completion_queue *cq = &per_cpu(completion_queues, i);\r\ninit_llist_head(&cq->list);\r\nif (irqmode != NULL_IRQ_TIMER)\r\ncontinue;\r\nhrtimer_init(&cq->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\ncq->timer.function = null_cmd_timer_expired;\r\n}\r\nnull_major = register_blkdev(0, "nullb");\r\nif (null_major < 0)\r\nreturn null_major;\r\nfor (i = 0; i < nr_devices; i++) {\r\nif (null_add_dev()) {\r\nunregister_blkdev(null_major, "nullb");\r\nreturn -EINVAL;\r\n}\r\n}\r\npr_info("null: module loaded\n");\r\nreturn 0;\r\n}\r\nstatic void __exit null_exit(void)\r\n{\r\nstruct nullb *nullb;\r\nunregister_blkdev(null_major, "nullb");\r\nmutex_lock(&lock);\r\nwhile (!list_empty(&nullb_list)) {\r\nnullb = list_entry(nullb_list.next, struct nullb, list);\r\nnull_del_dev(nullb);\r\n}\r\nmutex_unlock(&lock);\r\n}
