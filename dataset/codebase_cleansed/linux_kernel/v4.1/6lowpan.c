static inline struct lowpan_dev *lowpan_dev(const struct net_device *netdev)\r\n{\r\nreturn netdev_priv(netdev);\r\n}\r\nstatic inline void peer_add(struct lowpan_dev *dev, struct lowpan_peer *peer)\r\n{\r\nlist_add_rcu(&peer->list, &dev->peers);\r\natomic_inc(&dev->peer_count);\r\n}\r\nstatic inline bool peer_del(struct lowpan_dev *dev, struct lowpan_peer *peer)\r\n{\r\nlist_del_rcu(&peer->list);\r\nkfree_rcu(peer, rcu);\r\nmodule_put(THIS_MODULE);\r\nif (atomic_dec_and_test(&dev->peer_count)) {\r\nBT_DBG("last peer");\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic inline struct lowpan_peer *peer_lookup_ba(struct lowpan_dev *dev,\r\nbdaddr_t *ba, __u8 type)\r\n{\r\nstruct lowpan_peer *peer;\r\nBT_DBG("peers %d addr %pMR type %d", atomic_read(&dev->peer_count),\r\nba, type);\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(peer, &dev->peers, list) {\r\nBT_DBG("dst addr %pMR dst type %d",\r\n&peer->chan->dst, peer->chan->dst_type);\r\nif (bacmp(&peer->chan->dst, ba))\r\ncontinue;\r\nif (type == peer->chan->dst_type) {\r\nrcu_read_unlock();\r\nreturn peer;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn NULL;\r\n}\r\nstatic inline struct lowpan_peer *__peer_lookup_chan(struct lowpan_dev *dev,\r\nstruct l2cap_chan *chan)\r\n{\r\nstruct lowpan_peer *peer;\r\nlist_for_each_entry_rcu(peer, &dev->peers, list) {\r\nif (peer->chan == chan)\r\nreturn peer;\r\n}\r\nreturn NULL;\r\n}\r\nstatic inline struct lowpan_peer *__peer_lookup_conn(struct lowpan_dev *dev,\r\nstruct l2cap_conn *conn)\r\n{\r\nstruct lowpan_peer *peer;\r\nlist_for_each_entry_rcu(peer, &dev->peers, list) {\r\nif (peer->chan->conn == conn)\r\nreturn peer;\r\n}\r\nreturn NULL;\r\n}\r\nstatic inline struct lowpan_peer *peer_lookup_dst(struct lowpan_dev *dev,\r\nstruct in6_addr *daddr,\r\nstruct sk_buff *skb)\r\n{\r\nstruct lowpan_peer *peer;\r\nstruct in6_addr *nexthop;\r\nstruct rt6_info *rt = (struct rt6_info *)skb_dst(skb);\r\nint count = atomic_read(&dev->peer_count);\r\nBT_DBG("peers %d addr %pI6c rt %p", count, daddr, rt);\r\nif (count == 1) {\r\nrcu_read_lock();\r\npeer = list_first_or_null_rcu(&dev->peers, struct lowpan_peer,\r\nlist);\r\nrcu_read_unlock();\r\nreturn peer;\r\n}\r\nif (!rt) {\r\nnexthop = &lowpan_cb(skb)->gw;\r\nif (ipv6_addr_any(nexthop))\r\nreturn NULL;\r\n} else {\r\nnexthop = rt6_nexthop(rt);\r\nmemcpy(&lowpan_cb(skb)->gw, nexthop, sizeof(struct in6_addr));\r\n}\r\nBT_DBG("gw %pI6c", nexthop);\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(peer, &dev->peers, list) {\r\nBT_DBG("dst addr %pMR dst type %d ip %pI6c",\r\n&peer->chan->dst, peer->chan->dst_type,\r\n&peer->peer_addr);\r\nif (!ipv6_addr_cmp(&peer->peer_addr, nexthop)) {\r\nrcu_read_unlock();\r\nreturn peer;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn NULL;\r\n}\r\nstatic struct lowpan_peer *lookup_peer(struct l2cap_conn *conn)\r\n{\r\nstruct lowpan_dev *entry;\r\nstruct lowpan_peer *peer = NULL;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(entry, &bt_6lowpan_devices, list) {\r\npeer = __peer_lookup_conn(entry, conn);\r\nif (peer)\r\nbreak;\r\n}\r\nrcu_read_unlock();\r\nreturn peer;\r\n}\r\nstatic struct lowpan_dev *lookup_dev(struct l2cap_conn *conn)\r\n{\r\nstruct lowpan_dev *entry;\r\nstruct lowpan_dev *dev = NULL;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(entry, &bt_6lowpan_devices, list) {\r\nif (conn->hcon->hdev == entry->hdev) {\r\ndev = entry;\r\nbreak;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn dev;\r\n}\r\nstatic int give_skb_to_upper(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct sk_buff *skb_cp;\r\nskb_cp = skb_copy(skb, GFP_ATOMIC);\r\nif (!skb_cp)\r\nreturn NET_RX_DROP;\r\nreturn netif_rx(skb_cp);\r\n}\r\nstatic int iphc_decompress(struct sk_buff *skb, struct net_device *netdev,\r\nstruct l2cap_chan *chan)\r\n{\r\nconst u8 *saddr, *daddr;\r\nu8 iphc0, iphc1;\r\nstruct lowpan_dev *dev;\r\nstruct lowpan_peer *peer;\r\ndev = lowpan_dev(netdev);\r\nrcu_read_lock();\r\npeer = __peer_lookup_chan(dev, chan);\r\nrcu_read_unlock();\r\nif (!peer)\r\nreturn -EINVAL;\r\nsaddr = peer->eui64_addr;\r\ndaddr = dev->netdev->dev_addr;\r\nif (skb->len < 2)\r\nreturn -EINVAL;\r\nif (lowpan_fetch_skb_u8(skb, &iphc0))\r\nreturn -EINVAL;\r\nif (lowpan_fetch_skb_u8(skb, &iphc1))\r\nreturn -EINVAL;\r\nreturn lowpan_header_decompress(skb, netdev,\r\nsaddr, IEEE802154_ADDR_LONG,\r\nEUI64_ADDR_LEN, daddr,\r\nIEEE802154_ADDR_LONG, EUI64_ADDR_LEN,\r\niphc0, iphc1);\r\n}\r\nstatic int recv_pkt(struct sk_buff *skb, struct net_device *dev,\r\nstruct l2cap_chan *chan)\r\n{\r\nstruct sk_buff *local_skb;\r\nint ret;\r\nif (!netif_running(dev))\r\ngoto drop;\r\nif (dev->type != ARPHRD_6LOWPAN)\r\ngoto drop;\r\nskb = skb_share_check(skb, GFP_ATOMIC);\r\nif (!skb)\r\ngoto drop;\r\nif (skb->data[0] == LOWPAN_DISPATCH_IPV6) {\r\nlocal_skb = skb_copy_expand(skb, NET_SKB_PAD - 1,\r\nskb_tailroom(skb), GFP_ATOMIC);\r\nif (!local_skb)\r\ngoto drop;\r\nlocal_skb->protocol = htons(ETH_P_IPV6);\r\nlocal_skb->pkt_type = PACKET_HOST;\r\nskb_reset_network_header(local_skb);\r\nskb_set_transport_header(local_skb, sizeof(struct ipv6hdr));\r\nif (give_skb_to_upper(local_skb, dev) != NET_RX_SUCCESS) {\r\nkfree_skb(local_skb);\r\ngoto drop;\r\n}\r\ndev->stats.rx_bytes += skb->len;\r\ndev->stats.rx_packets++;\r\nconsume_skb(local_skb);\r\nconsume_skb(skb);\r\n} else {\r\nswitch (skb->data[0] & 0xe0) {\r\ncase LOWPAN_DISPATCH_IPHC:\r\nlocal_skb = skb_clone(skb, GFP_ATOMIC);\r\nif (!local_skb)\r\ngoto drop;\r\nret = iphc_decompress(local_skb, dev, chan);\r\nif (ret < 0) {\r\nkfree_skb(local_skb);\r\ngoto drop;\r\n}\r\nlocal_skb->protocol = htons(ETH_P_IPV6);\r\nlocal_skb->pkt_type = PACKET_HOST;\r\nlocal_skb->dev = dev;\r\nif (give_skb_to_upper(local_skb, dev)\r\n!= NET_RX_SUCCESS) {\r\nkfree_skb(local_skb);\r\ngoto drop;\r\n}\r\ndev->stats.rx_bytes += skb->len;\r\ndev->stats.rx_packets++;\r\nconsume_skb(local_skb);\r\nconsume_skb(skb);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nreturn NET_RX_SUCCESS;\r\ndrop:\r\ndev->stats.rx_dropped++;\r\nreturn NET_RX_DROP;\r\n}\r\nstatic int chan_recv_cb(struct l2cap_chan *chan, struct sk_buff *skb)\r\n{\r\nstruct lowpan_dev *dev;\r\nstruct lowpan_peer *peer;\r\nint err;\r\npeer = lookup_peer(chan->conn);\r\nif (!peer)\r\nreturn -ENOENT;\r\ndev = lookup_dev(chan->conn);\r\nif (!dev || !dev->netdev)\r\nreturn -ENOENT;\r\nerr = recv_pkt(skb, dev->netdev, chan);\r\nif (err) {\r\nBT_DBG("recv pkt %d", err);\r\nerr = -EAGAIN;\r\n}\r\nreturn err;\r\n}\r\nstatic u8 get_addr_type_from_eui64(u8 byte)\r\n{\r\nreturn ((byte & 0x02) ? BDADDR_LE_RANDOM : BDADDR_LE_PUBLIC);\r\n}\r\nstatic void copy_to_bdaddr(struct in6_addr *ip6_daddr, bdaddr_t *addr)\r\n{\r\nu8 *eui64 = ip6_daddr->s6_addr + 8;\r\naddr->b[0] = eui64[7];\r\naddr->b[1] = eui64[6];\r\naddr->b[2] = eui64[5];\r\naddr->b[3] = eui64[2];\r\naddr->b[4] = eui64[1];\r\naddr->b[5] = eui64[0];\r\n}\r\nstatic void convert_dest_bdaddr(struct in6_addr *ip6_daddr,\r\nbdaddr_t *addr, u8 *addr_type)\r\n{\r\ncopy_to_bdaddr(ip6_daddr, addr);\r\naddr->b[5] ^= 0x02;\r\n*addr_type = get_addr_type_from_eui64(addr->b[5]);\r\n}\r\nstatic int setup_header(struct sk_buff *skb, struct net_device *netdev,\r\nbdaddr_t *peer_addr, u8 *peer_addr_type)\r\n{\r\nstruct in6_addr ipv6_daddr;\r\nstruct lowpan_dev *dev;\r\nstruct lowpan_peer *peer;\r\nbdaddr_t addr, *any = BDADDR_ANY;\r\nu8 *daddr = any->b;\r\nint err, status = 0;\r\ndev = lowpan_dev(netdev);\r\nmemcpy(&ipv6_daddr, &lowpan_cb(skb)->addr, sizeof(ipv6_daddr));\r\nif (ipv6_addr_is_multicast(&ipv6_daddr)) {\r\nlowpan_cb(skb)->chan = NULL;\r\n} else {\r\nu8 addr_type;\r\nconvert_dest_bdaddr(&ipv6_daddr, &addr, &addr_type);\r\nBT_DBG("dest addr %pMR type %d IP %pI6c", &addr,\r\naddr_type, &ipv6_daddr);\r\npeer = peer_lookup_ba(dev, &addr, addr_type);\r\nif (!peer) {\r\npeer = peer_lookup_dst(dev, &ipv6_daddr, skb);\r\nif (!peer) {\r\nBT_DBG("no such peer %pMR found", &addr);\r\nreturn -ENOENT;\r\n}\r\n}\r\ndaddr = peer->eui64_addr;\r\n*peer_addr = addr;\r\n*peer_addr_type = addr_type;\r\nlowpan_cb(skb)->chan = peer->chan;\r\nstatus = 1;\r\n}\r\nlowpan_header_compress(skb, netdev, ETH_P_IPV6, daddr,\r\ndev->netdev->dev_addr, skb->len);\r\nerr = dev_hard_header(skb, netdev, ETH_P_IPV6, NULL, NULL, 0);\r\nif (err < 0)\r\nreturn err;\r\nreturn status;\r\n}\r\nstatic int header_create(struct sk_buff *skb, struct net_device *netdev,\r\nunsigned short type, const void *_daddr,\r\nconst void *_saddr, unsigned int len)\r\n{\r\nstruct ipv6hdr *hdr;\r\nif (type != ETH_P_IPV6)\r\nreturn -EINVAL;\r\nhdr = ipv6_hdr(skb);\r\nmemcpy(&lowpan_cb(skb)->addr, &hdr->daddr, sizeof(struct in6_addr));\r\nreturn 0;\r\n}\r\nstatic int send_pkt(struct l2cap_chan *chan, struct sk_buff *skb,\r\nstruct net_device *netdev)\r\n{\r\nstruct msghdr msg;\r\nstruct kvec iv;\r\nint err;\r\nchan->data = skb;\r\niv.iov_base = skb->data;\r\niv.iov_len = skb->len;\r\nmemset(&msg, 0, sizeof(msg));\r\niov_iter_kvec(&msg.msg_iter, WRITE | ITER_KVEC, &iv, 1, skb->len);\r\nerr = l2cap_chan_send(chan, &msg, skb->len);\r\nif (err > 0) {\r\nnetdev->stats.tx_bytes += err;\r\nnetdev->stats.tx_packets++;\r\nreturn 0;\r\n}\r\nif (!err)\r\nerr = lowpan_cb(skb)->status;\r\nif (err < 0) {\r\nif (err == -EAGAIN)\r\nnetdev->stats.tx_dropped++;\r\nelse\r\nnetdev->stats.tx_errors++;\r\n}\r\nreturn err;\r\n}\r\nstatic int send_mcast_pkt(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct sk_buff *local_skb;\r\nstruct lowpan_dev *entry;\r\nint err = 0;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(entry, &bt_6lowpan_devices, list) {\r\nstruct lowpan_peer *pentry;\r\nstruct lowpan_dev *dev;\r\nif (entry->netdev != netdev)\r\ncontinue;\r\ndev = lowpan_dev(entry->netdev);\r\nlist_for_each_entry_rcu(pentry, &dev->peers, list) {\r\nint ret;\r\nlocal_skb = skb_clone(skb, GFP_ATOMIC);\r\nBT_DBG("xmit %s to %pMR type %d IP %pI6c chan %p",\r\nnetdev->name,\r\n&pentry->chan->dst, pentry->chan->dst_type,\r\n&pentry->peer_addr, pentry->chan);\r\nret = send_pkt(pentry->chan, local_skb, netdev);\r\nif (ret < 0)\r\nerr = ret;\r\nkfree_skb(local_skb);\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn err;\r\n}\r\nstatic netdev_tx_t bt_xmit(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nint err = 0;\r\nbdaddr_t addr;\r\nu8 addr_type;\r\nskb = skb_unshare(skb, GFP_ATOMIC);\r\nif (!skb)\r\nreturn NET_XMIT_DROP;\r\nerr = setup_header(skb, netdev, &addr, &addr_type);\r\nif (err < 0) {\r\nkfree_skb(skb);\r\nreturn NET_XMIT_DROP;\r\n}\r\nif (err) {\r\nif (lowpan_cb(skb)->chan) {\r\nBT_DBG("xmit %s to %pMR type %d IP %pI6c chan %p",\r\nnetdev->name, &addr, addr_type,\r\n&lowpan_cb(skb)->addr, lowpan_cb(skb)->chan);\r\nerr = send_pkt(lowpan_cb(skb)->chan, skb, netdev);\r\n} else {\r\nerr = -ENOENT;\r\n}\r\n} else {\r\nerr = send_mcast_pkt(skb, netdev);\r\n}\r\ndev_kfree_skb(skb);\r\nif (err)\r\nBT_DBG("ERROR: xmit failed (%d)", err);\r\nreturn err < 0 ? NET_XMIT_DROP : err;\r\n}\r\nstatic void bt_set_lockdep_class_one(struct net_device *dev,\r\nstruct netdev_queue *txq,\r\nvoid *_unused)\r\n{\r\nlockdep_set_class(&txq->_xmit_lock, &bt_netdev_xmit_lock_key);\r\n}\r\nstatic int bt_dev_init(struct net_device *dev)\r\n{\r\nnetdev_for_each_tx_queue(dev, bt_set_lockdep_class_one, NULL);\r\ndev->qdisc_tx_busylock = &bt_tx_busylock;\r\nreturn 0;\r\n}\r\nstatic void netdev_setup(struct net_device *dev)\r\n{\r\ndev->addr_len = EUI64_ADDR_LEN;\r\ndev->type = ARPHRD_6LOWPAN;\r\ndev->hard_header_len = 0;\r\ndev->needed_tailroom = 0;\r\ndev->mtu = IPV6_MIN_MTU;\r\ndev->tx_queue_len = 0;\r\ndev->flags = IFF_RUNNING | IFF_POINTOPOINT |\r\nIFF_MULTICAST;\r\ndev->watchdog_timeo = 0;\r\ndev->netdev_ops = &netdev_ops;\r\ndev->header_ops = &header_ops;\r\ndev->destructor = free_netdev;\r\n}\r\nstatic void set_addr(u8 *eui, u8 *addr, u8 addr_type)\r\n{\r\neui[0] = addr[5];\r\neui[1] = addr[4];\r\neui[2] = addr[3];\r\neui[3] = 0xFF;\r\neui[4] = 0xFE;\r\neui[5] = addr[2];\r\neui[6] = addr[1];\r\neui[7] = addr[0];\r\nif (addr_type == BDADDR_LE_PUBLIC)\r\neui[0] &= ~0x02;\r\nelse\r\neui[0] |= 0x02;\r\nBT_DBG("type %d addr %*phC", addr_type, 8, eui);\r\n}\r\nstatic void set_dev_addr(struct net_device *netdev, bdaddr_t *addr,\r\nu8 addr_type)\r\n{\r\nnetdev->addr_assign_type = NET_ADDR_PERM;\r\nset_addr(netdev->dev_addr, addr->b, addr_type);\r\n}\r\nstatic void ifup(struct net_device *netdev)\r\n{\r\nint err;\r\nrtnl_lock();\r\nerr = dev_open(netdev);\r\nif (err < 0)\r\nBT_INFO("iface %s cannot be opened (%d)", netdev->name, err);\r\nrtnl_unlock();\r\n}\r\nstatic void ifdown(struct net_device *netdev)\r\n{\r\nint err;\r\nrtnl_lock();\r\nerr = dev_close(netdev);\r\nif (err < 0)\r\nBT_INFO("iface %s cannot be closed (%d)", netdev->name, err);\r\nrtnl_unlock();\r\n}\r\nstatic void do_notify_peers(struct work_struct *work)\r\n{\r\nstruct lowpan_dev *dev = container_of(work, struct lowpan_dev,\r\nnotify_peers.work);\r\nnetdev_notify_peers(dev->netdev);\r\n}\r\nstatic bool is_bt_6lowpan(struct hci_conn *hcon)\r\n{\r\nif (hcon->type != LE_LINK)\r\nreturn false;\r\nif (!enable_6lowpan)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic struct l2cap_chan *chan_create(void)\r\n{\r\nstruct l2cap_chan *chan;\r\nchan = l2cap_chan_create();\r\nif (!chan)\r\nreturn NULL;\r\nl2cap_chan_set_defaults(chan);\r\nchan->chan_type = L2CAP_CHAN_CONN_ORIENTED;\r\nchan->mode = L2CAP_MODE_LE_FLOWCTL;\r\nchan->omtu = 65535;\r\nchan->imtu = chan->omtu;\r\nreturn chan;\r\n}\r\nstatic struct l2cap_chan *chan_open(struct l2cap_chan *pchan)\r\n{\r\nstruct l2cap_chan *chan;\r\nchan = chan_create();\r\nif (!chan)\r\nreturn NULL;\r\nchan->remote_mps = chan->omtu;\r\nchan->mps = chan->omtu;\r\nchan->state = BT_CONNECTED;\r\nreturn chan;\r\n}\r\nstatic void set_ip_addr_bits(u8 addr_type, u8 *addr)\r\n{\r\nif (addr_type == BDADDR_LE_PUBLIC)\r\n*addr |= 0x02;\r\nelse\r\n*addr &= ~0x02;\r\n}\r\nstatic struct l2cap_chan *add_peer_chan(struct l2cap_chan *chan,\r\nstruct lowpan_dev *dev)\r\n{\r\nstruct lowpan_peer *peer;\r\npeer = kzalloc(sizeof(*peer), GFP_ATOMIC);\r\nif (!peer)\r\nreturn NULL;\r\npeer->chan = chan;\r\nmemset(&peer->peer_addr, 0, sizeof(struct in6_addr));\r\npeer->peer_addr.s6_addr[0] = 0xFE;\r\npeer->peer_addr.s6_addr[1] = 0x80;\r\nset_addr((u8 *)&peer->peer_addr.s6_addr + 8, chan->dst.b,\r\nchan->dst_type);\r\nmemcpy(&peer->eui64_addr, (u8 *)&peer->peer_addr.s6_addr + 8,\r\nEUI64_ADDR_LEN);\r\nset_ip_addr_bits(chan->dst_type, (u8 *)&peer->peer_addr.s6_addr + 8);\r\nspin_lock(&devices_lock);\r\nINIT_LIST_HEAD(&peer->list);\r\npeer_add(dev, peer);\r\nspin_unlock(&devices_lock);\r\nINIT_DELAYED_WORK(&dev->notify_peers, do_notify_peers);\r\nschedule_delayed_work(&dev->notify_peers, msecs_to_jiffies(100));\r\nreturn peer->chan;\r\n}\r\nstatic int setup_netdev(struct l2cap_chan *chan, struct lowpan_dev **dev)\r\n{\r\nstruct net_device *netdev;\r\nint err = 0;\r\nnetdev = alloc_netdev(sizeof(struct lowpan_dev), IFACE_NAME_TEMPLATE,\r\nNET_NAME_UNKNOWN, netdev_setup);\r\nif (!netdev)\r\nreturn -ENOMEM;\r\nset_dev_addr(netdev, &chan->src, chan->src_type);\r\nnetdev->netdev_ops = &netdev_ops;\r\nSET_NETDEV_DEV(netdev, &chan->conn->hcon->dev);\r\nSET_NETDEV_DEVTYPE(netdev, &bt_type);\r\nerr = register_netdev(netdev);\r\nif (err < 0) {\r\nBT_INFO("register_netdev failed %d", err);\r\nfree_netdev(netdev);\r\ngoto out;\r\n}\r\nBT_DBG("ifindex %d peer bdaddr %pMR type %d my addr %pMR type %d",\r\nnetdev->ifindex, &chan->dst, chan->dst_type,\r\n&chan->src, chan->src_type);\r\nset_bit(__LINK_STATE_PRESENT, &netdev->state);\r\n*dev = netdev_priv(netdev);\r\n(*dev)->netdev = netdev;\r\n(*dev)->hdev = chan->conn->hcon->hdev;\r\nINIT_LIST_HEAD(&(*dev)->peers);\r\nspin_lock(&devices_lock);\r\nINIT_LIST_HEAD(&(*dev)->list);\r\nlist_add_rcu(&(*dev)->list, &bt_6lowpan_devices);\r\nspin_unlock(&devices_lock);\r\nreturn 0;\r\nout:\r\nreturn err;\r\n}\r\nstatic inline void chan_ready_cb(struct l2cap_chan *chan)\r\n{\r\nstruct lowpan_dev *dev;\r\ndev = lookup_dev(chan->conn);\r\nBT_DBG("chan %p conn %p dev %p", chan, chan->conn, dev);\r\nif (!dev) {\r\nif (setup_netdev(chan, &dev) < 0) {\r\nl2cap_chan_del(chan, -ENOENT);\r\nreturn;\r\n}\r\n}\r\nif (!try_module_get(THIS_MODULE))\r\nreturn;\r\nadd_peer_chan(chan, dev);\r\nifup(dev->netdev);\r\n}\r\nstatic inline struct l2cap_chan *chan_new_conn_cb(struct l2cap_chan *pchan)\r\n{\r\nstruct l2cap_chan *chan;\r\nchan = chan_open(pchan);\r\nchan->ops = pchan->ops;\r\nBT_DBG("chan %p pchan %p", chan, pchan);\r\nreturn chan;\r\n}\r\nstatic void delete_netdev(struct work_struct *work)\r\n{\r\nstruct lowpan_dev *entry = container_of(work, struct lowpan_dev,\r\ndelete_netdev);\r\nunregister_netdev(entry->netdev);\r\n}\r\nstatic void chan_close_cb(struct l2cap_chan *chan)\r\n{\r\nstruct lowpan_dev *entry;\r\nstruct lowpan_dev *dev = NULL;\r\nstruct lowpan_peer *peer;\r\nint err = -ENOENT;\r\nbool last = false, removed = true;\r\nBT_DBG("chan %p conn %p", chan, chan->conn);\r\nif (chan->conn && chan->conn->hcon) {\r\nif (!is_bt_6lowpan(chan->conn->hcon))\r\nreturn;\r\nremoved = false;\r\n}\r\nspin_lock(&devices_lock);\r\nlist_for_each_entry_rcu(entry, &bt_6lowpan_devices, list) {\r\ndev = lowpan_dev(entry->netdev);\r\npeer = __peer_lookup_chan(dev, chan);\r\nif (peer) {\r\nlast = peer_del(dev, peer);\r\nerr = 0;\r\nBT_DBG("dev %p removing %speer %p", dev,\r\nlast ? "last " : "1 ", peer);\r\nBT_DBG("chan %p orig refcnt %d", chan,\r\natomic_read(&chan->kref.refcount));\r\nl2cap_chan_put(chan);\r\nbreak;\r\n}\r\n}\r\nif (!err && last && dev && !atomic_read(&dev->peer_count)) {\r\nspin_unlock(&devices_lock);\r\ncancel_delayed_work_sync(&dev->notify_peers);\r\nifdown(dev->netdev);\r\nif (!removed) {\r\nINIT_WORK(&entry->delete_netdev, delete_netdev);\r\nschedule_work(&entry->delete_netdev);\r\n}\r\n} else {\r\nspin_unlock(&devices_lock);\r\n}\r\nreturn;\r\n}\r\nstatic void chan_state_change_cb(struct l2cap_chan *chan, int state, int err)\r\n{\r\nBT_DBG("chan %p conn %p state %s err %d", chan, chan->conn,\r\nstate_to_string(state), err);\r\n}\r\nstatic struct sk_buff *chan_alloc_skb_cb(struct l2cap_chan *chan,\r\nunsigned long hdr_len,\r\nunsigned long len, int nb)\r\n{\r\nreturn bt_skb_alloc(hdr_len + len, GFP_ATOMIC);\r\n}\r\nstatic void chan_suspend_cb(struct l2cap_chan *chan)\r\n{\r\nstruct sk_buff *skb = chan->data;\r\nBT_DBG("chan %p conn %p skb %p", chan, chan->conn, skb);\r\nif (!skb)\r\nreturn;\r\nlowpan_cb(skb)->status = -EAGAIN;\r\n}\r\nstatic void chan_resume_cb(struct l2cap_chan *chan)\r\n{\r\nstruct sk_buff *skb = chan->data;\r\nBT_DBG("chan %p conn %p skb %p", chan, chan->conn, skb);\r\nif (!skb)\r\nreturn;\r\nlowpan_cb(skb)->status = 0;\r\n}\r\nstatic long chan_get_sndtimeo_cb(struct l2cap_chan *chan)\r\n{\r\nreturn L2CAP_CONN_TIMEOUT;\r\n}\r\nstatic inline __u8 bdaddr_type(__u8 type)\r\n{\r\nif (type == ADDR_LE_DEV_PUBLIC)\r\nreturn BDADDR_LE_PUBLIC;\r\nelse\r\nreturn BDADDR_LE_RANDOM;\r\n}\r\nstatic struct l2cap_chan *chan_get(void)\r\n{\r\nstruct l2cap_chan *pchan;\r\npchan = chan_create();\r\nif (!pchan)\r\nreturn NULL;\r\npchan->ops = &bt_6lowpan_chan_ops;\r\nreturn pchan;\r\n}\r\nstatic int bt_6lowpan_connect(bdaddr_t *addr, u8 dst_type)\r\n{\r\nstruct l2cap_chan *pchan;\r\nint err;\r\npchan = chan_get();\r\nif (!pchan)\r\nreturn -EINVAL;\r\nerr = l2cap_chan_connect(pchan, cpu_to_le16(L2CAP_PSM_IPSP), 0,\r\naddr, dst_type);\r\nBT_DBG("chan %p err %d", pchan, err);\r\nif (err < 0)\r\nl2cap_chan_put(pchan);\r\nreturn err;\r\n}\r\nstatic int bt_6lowpan_disconnect(struct l2cap_conn *conn, u8 dst_type)\r\n{\r\nstruct lowpan_peer *peer;\r\nBT_DBG("conn %p dst type %d", conn, dst_type);\r\npeer = lookup_peer(conn);\r\nif (!peer)\r\nreturn -ENOENT;\r\nBT_DBG("peer %p chan %p", peer, peer->chan);\r\nl2cap_chan_close(peer->chan, ENOENT);\r\nreturn 0;\r\n}\r\nstatic struct l2cap_chan *bt_6lowpan_listen(void)\r\n{\r\nbdaddr_t *addr = BDADDR_ANY;\r\nstruct l2cap_chan *pchan;\r\nint err;\r\nif (!enable_6lowpan)\r\nreturn NULL;\r\npchan = chan_get();\r\nif (!pchan)\r\nreturn NULL;\r\npchan->state = BT_LISTEN;\r\npchan->src_type = BDADDR_LE_PUBLIC;\r\natomic_set(&pchan->nesting, L2CAP_NESTING_PARENT);\r\nBT_DBG("chan %p src type %d", pchan, pchan->src_type);\r\nerr = l2cap_add_psm(pchan, addr, cpu_to_le16(L2CAP_PSM_IPSP));\r\nif (err) {\r\nl2cap_chan_put(pchan);\r\nBT_ERR("psm cannot be added err %d", err);\r\nreturn NULL;\r\n}\r\nreturn pchan;\r\n}\r\nstatic int get_l2cap_conn(char *buf, bdaddr_t *addr, u8 *addr_type,\r\nstruct l2cap_conn **conn)\r\n{\r\nstruct hci_conn *hcon;\r\nstruct hci_dev *hdev;\r\nbdaddr_t *src = BDADDR_ANY;\r\nint n;\r\nn = sscanf(buf, "%hhx:%hhx:%hhx:%hhx:%hhx:%hhx %hhu",\r\n&addr->b[5], &addr->b[4], &addr->b[3],\r\n&addr->b[2], &addr->b[1], &addr->b[0],\r\naddr_type);\r\nif (n < 7)\r\nreturn -EINVAL;\r\nhdev = hci_get_route(addr, src);\r\nif (!hdev)\r\nreturn -ENOENT;\r\nhci_dev_lock(hdev);\r\nhcon = hci_conn_hash_lookup_ba(hdev, LE_LINK, addr);\r\nhci_dev_unlock(hdev);\r\nif (!hcon)\r\nreturn -ENOENT;\r\n*conn = (struct l2cap_conn *)hcon->l2cap_data;\r\nBT_DBG("conn %p dst %pMR type %d", *conn, &hcon->dst, hcon->dst_type);\r\nreturn 0;\r\n}\r\nstatic void disconnect_all_peers(void)\r\n{\r\nstruct lowpan_dev *entry;\r\nstruct lowpan_peer *peer, *tmp_peer, *new_peer;\r\nstruct list_head peers;\r\nINIT_LIST_HEAD(&peers);\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(entry, &bt_6lowpan_devices, list) {\r\nlist_for_each_entry_rcu(peer, &entry->peers, list) {\r\nnew_peer = kmalloc(sizeof(*new_peer), GFP_ATOMIC);\r\nif (!new_peer)\r\nbreak;\r\nnew_peer->chan = peer->chan;\r\nINIT_LIST_HEAD(&new_peer->list);\r\nlist_add(&new_peer->list, &peers);\r\n}\r\n}\r\nrcu_read_unlock();\r\nspin_lock(&devices_lock);\r\nlist_for_each_entry_safe(peer, tmp_peer, &peers, list) {\r\nl2cap_chan_close(peer->chan, ENOENT);\r\nlist_del_rcu(&peer->list);\r\nkfree_rcu(peer, rcu);\r\nmodule_put(THIS_MODULE);\r\n}\r\nspin_unlock(&devices_lock);\r\n}\r\nstatic void do_enable_set(struct work_struct *work)\r\n{\r\nstruct set_enable *set_enable = container_of(work,\r\nstruct set_enable, work);\r\nif (!set_enable->flag || enable_6lowpan != set_enable->flag)\r\ndisconnect_all_peers();\r\nenable_6lowpan = set_enable->flag;\r\nif (listen_chan) {\r\nl2cap_chan_close(listen_chan, 0);\r\nl2cap_chan_put(listen_chan);\r\n}\r\nlisten_chan = bt_6lowpan_listen();\r\nkfree(set_enable);\r\n}\r\nstatic int lowpan_enable_set(void *data, u64 val)\r\n{\r\nstruct set_enable *set_enable;\r\nset_enable = kzalloc(sizeof(*set_enable), GFP_KERNEL);\r\nif (!set_enable)\r\nreturn -ENOMEM;\r\nset_enable->flag = !!val;\r\nINIT_WORK(&set_enable->work, do_enable_set);\r\nschedule_work(&set_enable->work);\r\nreturn 0;\r\n}\r\nstatic int lowpan_enable_get(void *data, u64 *val)\r\n{\r\n*val = enable_6lowpan;\r\nreturn 0;\r\n}\r\nstatic ssize_t lowpan_control_write(struct file *fp,\r\nconst char __user *user_buffer,\r\nsize_t count,\r\nloff_t *position)\r\n{\r\nchar buf[32];\r\nsize_t buf_size = min(count, sizeof(buf) - 1);\r\nint ret;\r\nbdaddr_t addr;\r\nu8 addr_type;\r\nstruct l2cap_conn *conn = NULL;\r\nif (copy_from_user(buf, user_buffer, buf_size))\r\nreturn -EFAULT;\r\nbuf[buf_size] = '\0';\r\nif (memcmp(buf, "connect ", 8) == 0) {\r\nret = get_l2cap_conn(&buf[8], &addr, &addr_type, &conn);\r\nif (ret == -EINVAL)\r\nreturn ret;\r\nif (listen_chan) {\r\nl2cap_chan_close(listen_chan, 0);\r\nl2cap_chan_put(listen_chan);\r\nlisten_chan = NULL;\r\n}\r\nif (conn) {\r\nstruct lowpan_peer *peer;\r\nif (!is_bt_6lowpan(conn->hcon))\r\nreturn -EINVAL;\r\npeer = lookup_peer(conn);\r\nif (peer) {\r\nBT_DBG("6LoWPAN connection already exists");\r\nreturn -EALREADY;\r\n}\r\nBT_DBG("conn %p dst %pMR type %d user %d", conn,\r\n&conn->hcon->dst, conn->hcon->dst_type,\r\naddr_type);\r\n}\r\nret = bt_6lowpan_connect(&addr, addr_type);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn count;\r\n}\r\nif (memcmp(buf, "disconnect ", 11) == 0) {\r\nret = get_l2cap_conn(&buf[11], &addr, &addr_type, &conn);\r\nif (ret < 0)\r\nreturn ret;\r\nret = bt_6lowpan_disconnect(conn, addr_type);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn count;\r\n}\r\nreturn count;\r\n}\r\nstatic int lowpan_control_show(struct seq_file *f, void *ptr)\r\n{\r\nstruct lowpan_dev *entry;\r\nstruct lowpan_peer *peer;\r\nspin_lock(&devices_lock);\r\nlist_for_each_entry(entry, &bt_6lowpan_devices, list) {\r\nlist_for_each_entry(peer, &entry->peers, list)\r\nseq_printf(f, "%pMR (type %u)\n",\r\n&peer->chan->dst, peer->chan->dst_type);\r\n}\r\nspin_unlock(&devices_lock);\r\nreturn 0;\r\n}\r\nstatic int lowpan_control_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, lowpan_control_show, inode->i_private);\r\n}\r\nstatic void disconnect_devices(void)\r\n{\r\nstruct lowpan_dev *entry, *tmp, *new_dev;\r\nstruct list_head devices;\r\nINIT_LIST_HEAD(&devices);\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(entry, &bt_6lowpan_devices, list) {\r\nnew_dev = kmalloc(sizeof(*new_dev), GFP_ATOMIC);\r\nif (!new_dev)\r\nbreak;\r\nnew_dev->netdev = entry->netdev;\r\nINIT_LIST_HEAD(&new_dev->list);\r\nlist_add_rcu(&new_dev->list, &devices);\r\n}\r\nrcu_read_unlock();\r\nlist_for_each_entry_safe(entry, tmp, &devices, list) {\r\nifdown(entry->netdev);\r\nBT_DBG("Unregistering netdev %s %p",\r\nentry->netdev->name, entry->netdev);\r\nunregister_netdev(entry->netdev);\r\nkfree(entry);\r\n}\r\n}\r\nstatic int device_event(struct notifier_block *unused,\r\nunsigned long event, void *ptr)\r\n{\r\nstruct net_device *netdev = netdev_notifier_info_to_dev(ptr);\r\nstruct lowpan_dev *entry;\r\nif (netdev->type != ARPHRD_6LOWPAN)\r\nreturn NOTIFY_DONE;\r\nswitch (event) {\r\ncase NETDEV_UNREGISTER:\r\nspin_lock(&devices_lock);\r\nlist_for_each_entry(entry, &bt_6lowpan_devices, list) {\r\nif (entry->netdev == netdev) {\r\nBT_DBG("Unregistered netdev %s %p",\r\nnetdev->name, netdev);\r\nlist_del(&entry->list);\r\nkfree(entry);\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&devices_lock);\r\nbreak;\r\n}\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic int __init bt_6lowpan_init(void)\r\n{\r\nlowpan_enable_debugfs = debugfs_create_file("6lowpan_enable", 0644,\r\nbt_debugfs, NULL,\r\n&lowpan_enable_fops);\r\nlowpan_control_debugfs = debugfs_create_file("6lowpan_control", 0644,\r\nbt_debugfs, NULL,\r\n&lowpan_control_fops);\r\nreturn register_netdevice_notifier(&bt_6lowpan_dev_notifier);\r\n}\r\nstatic void __exit bt_6lowpan_exit(void)\r\n{\r\ndebugfs_remove(lowpan_enable_debugfs);\r\ndebugfs_remove(lowpan_control_debugfs);\r\nif (listen_chan) {\r\nl2cap_chan_close(listen_chan, 0);\r\nl2cap_chan_put(listen_chan);\r\n}\r\ndisconnect_devices();\r\nunregister_netdevice_notifier(&bt_6lowpan_dev_notifier);\r\n}
