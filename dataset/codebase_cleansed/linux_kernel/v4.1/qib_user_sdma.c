static struct qib_user_sdma_rb_node *\r\nqib_user_sdma_rb_search(struct rb_root *root, pid_t pid)\r\n{\r\nstruct qib_user_sdma_rb_node *sdma_rb_node;\r\nstruct rb_node *node = root->rb_node;\r\nwhile (node) {\r\nsdma_rb_node = container_of(node,\r\nstruct qib_user_sdma_rb_node, node);\r\nif (pid < sdma_rb_node->pid)\r\nnode = node->rb_left;\r\nelse if (pid > sdma_rb_node->pid)\r\nnode = node->rb_right;\r\nelse\r\nreturn sdma_rb_node;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int\r\nqib_user_sdma_rb_insert(struct rb_root *root, struct qib_user_sdma_rb_node *new)\r\n{\r\nstruct rb_node **node = &(root->rb_node);\r\nstruct rb_node *parent = NULL;\r\nstruct qib_user_sdma_rb_node *got;\r\nwhile (*node) {\r\ngot = container_of(*node, struct qib_user_sdma_rb_node, node);\r\nparent = *node;\r\nif (new->pid < got->pid)\r\nnode = &((*node)->rb_left);\r\nelse if (new->pid > got->pid)\r\nnode = &((*node)->rb_right);\r\nelse\r\nreturn 0;\r\n}\r\nrb_link_node(&new->node, parent, node);\r\nrb_insert_color(&new->node, root);\r\nreturn 1;\r\n}\r\nstruct qib_user_sdma_queue *\r\nqib_user_sdma_queue_create(struct device *dev, int unit, int ctxt, int sctxt)\r\n{\r\nstruct qib_user_sdma_queue *pq =\r\nkmalloc(sizeof(struct qib_user_sdma_queue), GFP_KERNEL);\r\nstruct qib_user_sdma_rb_node *sdma_rb_node;\r\nif (!pq)\r\ngoto done;\r\npq->counter = 0;\r\npq->sent_counter = 0;\r\npq->num_pending = 0;\r\npq->num_sending = 0;\r\npq->added = 0;\r\npq->sdma_rb_node = NULL;\r\nINIT_LIST_HEAD(&pq->sent);\r\nspin_lock_init(&pq->sent_lock);\r\nmutex_init(&pq->lock);\r\nsnprintf(pq->pkt_slab_name, sizeof(pq->pkt_slab_name),\r\n"qib-user-sdma-pkts-%u-%02u.%02u", unit, ctxt, sctxt);\r\npq->pkt_slab = kmem_cache_create(pq->pkt_slab_name,\r\nsizeof(struct qib_user_sdma_pkt),\r\n0, 0, NULL);\r\nif (!pq->pkt_slab)\r\ngoto err_kfree;\r\nsnprintf(pq->header_cache_name, sizeof(pq->header_cache_name),\r\n"qib-user-sdma-headers-%u-%02u.%02u", unit, ctxt, sctxt);\r\npq->header_cache = dma_pool_create(pq->header_cache_name,\r\ndev,\r\nQIB_USER_SDMA_EXP_HEADER_LENGTH,\r\n4, 0);\r\nif (!pq->header_cache)\r\ngoto err_slab;\r\npq->dma_pages_root = RB_ROOT;\r\nsdma_rb_node = qib_user_sdma_rb_search(&qib_user_sdma_rb_root,\r\ncurrent->pid);\r\nif (sdma_rb_node) {\r\nsdma_rb_node->refcount++;\r\n} else {\r\nint ret;\r\nsdma_rb_node = kmalloc(sizeof(\r\nstruct qib_user_sdma_rb_node), GFP_KERNEL);\r\nif (!sdma_rb_node)\r\ngoto err_rb;\r\nsdma_rb_node->refcount = 1;\r\nsdma_rb_node->pid = current->pid;\r\nret = qib_user_sdma_rb_insert(&qib_user_sdma_rb_root,\r\nsdma_rb_node);\r\nBUG_ON(ret == 0);\r\n}\r\npq->sdma_rb_node = sdma_rb_node;\r\ngoto done;\r\nerr_rb:\r\ndma_pool_destroy(pq->header_cache);\r\nerr_slab:\r\nkmem_cache_destroy(pq->pkt_slab);\r\nerr_kfree:\r\nkfree(pq);\r\npq = NULL;\r\ndone:\r\nreturn pq;\r\n}\r\nstatic void qib_user_sdma_init_frag(struct qib_user_sdma_pkt *pkt,\r\nint i, u16 offset, u16 len,\r\nu16 first_desc, u16 last_desc,\r\nu16 put_page, u16 dma_mapped,\r\nstruct page *page, void *kvaddr,\r\ndma_addr_t dma_addr, u16 dma_length)\r\n{\r\npkt->addr[i].offset = offset;\r\npkt->addr[i].length = len;\r\npkt->addr[i].first_desc = first_desc;\r\npkt->addr[i].last_desc = last_desc;\r\npkt->addr[i].put_page = put_page;\r\npkt->addr[i].dma_mapped = dma_mapped;\r\npkt->addr[i].page = page;\r\npkt->addr[i].kvaddr = kvaddr;\r\npkt->addr[i].addr = dma_addr;\r\npkt->addr[i].dma_length = dma_length;\r\n}\r\nstatic void *qib_user_sdma_alloc_header(struct qib_user_sdma_queue *pq,\r\nsize_t len, dma_addr_t *dma_addr)\r\n{\r\nvoid *hdr;\r\nif (len == QIB_USER_SDMA_EXP_HEADER_LENGTH)\r\nhdr = dma_pool_alloc(pq->header_cache, GFP_KERNEL,\r\ndma_addr);\r\nelse\r\nhdr = NULL;\r\nif (!hdr) {\r\nhdr = kmalloc(len, GFP_KERNEL);\r\nif (!hdr)\r\nreturn NULL;\r\n*dma_addr = 0;\r\n}\r\nreturn hdr;\r\n}\r\nstatic int qib_user_sdma_page_to_frags(const struct qib_devdata *dd,\r\nstruct qib_user_sdma_queue *pq,\r\nstruct qib_user_sdma_pkt *pkt,\r\nstruct page *page, u16 put,\r\nu16 offset, u16 len, void *kvaddr)\r\n{\r\n__le16 *pbc16;\r\nvoid *pbcvaddr;\r\nstruct qib_message_header *hdr;\r\nu16 newlen, pbclen, lastdesc, dma_mapped;\r\nu32 vcto;\r\nunion qib_seqnum seqnum;\r\ndma_addr_t pbcdaddr;\r\ndma_addr_t dma_addr =\r\ndma_map_page(&dd->pcidev->dev,\r\npage, offset, len, DMA_TO_DEVICE);\r\nint ret = 0;\r\nif (dma_mapping_error(&dd->pcidev->dev, dma_addr)) {\r\nif (put) {\r\nput_page(page);\r\n} else {\r\nkunmap(page);\r\n__free_page(page);\r\n}\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\noffset = 0;\r\ndma_mapped = 1;\r\nnext_fragment:\r\nif (pkt->tiddma && len > pkt->tidsm[pkt->tidsmidx].length)\r\nnewlen = pkt->tidsm[pkt->tidsmidx].length;\r\nelse\r\nnewlen = len;\r\nlastdesc = 0;\r\nif ((pkt->payload_size + newlen) >= pkt->frag_size) {\r\nnewlen = pkt->frag_size - pkt->payload_size;\r\nlastdesc = 1;\r\n} else if (pkt->tiddma) {\r\nif (newlen == pkt->tidsm[pkt->tidsmidx].length)\r\nlastdesc = 1;\r\n} else {\r\nif (newlen == pkt->bytes_togo)\r\nlastdesc = 1;\r\n}\r\nqib_user_sdma_init_frag(pkt, pkt->naddr,\r\noffset, newlen,\r\n0, lastdesc,\r\nput, dma_mapped,\r\npage, kvaddr,\r\ndma_addr, len);\r\npkt->bytes_togo -= newlen;\r\npkt->payload_size += newlen;\r\npkt->naddr++;\r\nif (pkt->naddr == pkt->addrlimit) {\r\nret = -EFAULT;\r\ngoto done;\r\n}\r\nif (pkt->bytes_togo == 0) {\r\nif (!pkt->addr[pkt->index].addr) {\r\npkt->addr[pkt->index].addr =\r\ndma_map_single(&dd->pcidev->dev,\r\npkt->addr[pkt->index].kvaddr,\r\npkt->addr[pkt->index].dma_length,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&dd->pcidev->dev,\r\npkt->addr[pkt->index].addr)) {\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\npkt->addr[pkt->index].dma_mapped = 1;\r\n}\r\ngoto done;\r\n}\r\nif (pkt->tiddma) {\r\npkt->tidsm[pkt->tidsmidx].length -= newlen;\r\nif (pkt->tidsm[pkt->tidsmidx].length) {\r\npkt->tidsm[pkt->tidsmidx].offset += newlen;\r\n} else {\r\npkt->tidsmidx++;\r\nif (pkt->tidsmidx == pkt->tidsmcount) {\r\nret = -EFAULT;\r\ngoto done;\r\n}\r\n}\r\n}\r\nif (lastdesc == 0)\r\ngoto done;\r\npbclen = pkt->addr[pkt->index].length;\r\npbcvaddr = qib_user_sdma_alloc_header(pq, pbclen, &pbcdaddr);\r\nif (!pbcvaddr) {\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\npbc16 = (__le16 *)pkt->addr[pkt->index].kvaddr;\r\nmemcpy(pbcvaddr, pbc16, pbclen);\r\nhdr = (struct qib_message_header *)&pbc16[4];\r\npbc16[0] = cpu_to_le16(le16_to_cpu(pbc16[0])-(pkt->bytes_togo>>2));\r\nhdr->lrh[2] = cpu_to_be16(le16_to_cpu(pbc16[0]));\r\nif (pkt->tiddma) {\r\nhdr->iph.pkt_flags =\r\ncpu_to_le16(le16_to_cpu(hdr->iph.pkt_flags)|0x2);\r\nhdr->flags &= ~(0x04|0x20);\r\n} else {\r\nhdr->bth[0] = cpu_to_be32(be32_to_cpu(hdr->bth[0])&0xFFCFFFFF);\r\nhdr->flags &= ~(0x04);\r\n}\r\nvcto = le32_to_cpu(hdr->iph.ver_ctxt_tid_offset);\r\nhdr->iph.chksum = cpu_to_le16(QIB_LRH_BTH +\r\nbe16_to_cpu(hdr->lrh[2]) -\r\n((vcto>>16)&0xFFFF) - (vcto&0xFFFF) -\r\nle16_to_cpu(hdr->iph.pkt_flags));\r\nif (!pkt->addr[pkt->index].addr) {\r\npkt->addr[pkt->index].addr =\r\ndma_map_single(&dd->pcidev->dev,\r\npkt->addr[pkt->index].kvaddr,\r\npkt->addr[pkt->index].dma_length,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&dd->pcidev->dev,\r\npkt->addr[pkt->index].addr)) {\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\npkt->addr[pkt->index].dma_mapped = 1;\r\n}\r\npbc16 = (__le16 *)pbcvaddr;\r\nhdr = (struct qib_message_header *)&pbc16[4];\r\npbc16[0] = cpu_to_le16(le16_to_cpu(pbc16[0])-(pkt->payload_size>>2));\r\nhdr->lrh[2] = cpu_to_be16(le16_to_cpu(pbc16[0]));\r\nif (pkt->tiddma) {\r\nhdr->iph.ver_ctxt_tid_offset = cpu_to_le32(\r\n(le32_to_cpu(hdr->iph.ver_ctxt_tid_offset)&0xFF000000) +\r\n(pkt->tidsm[pkt->tidsmidx].tid<<QLOGIC_IB_I_TID_SHIFT) +\r\n(pkt->tidsm[pkt->tidsmidx].offset>>2));\r\n} else {\r\nhdr->uwords[2] += pkt->payload_size;\r\n}\r\nvcto = le32_to_cpu(hdr->iph.ver_ctxt_tid_offset);\r\nhdr->iph.chksum = cpu_to_le16(QIB_LRH_BTH +\r\nbe16_to_cpu(hdr->lrh[2]) -\r\n((vcto>>16)&0xFFFF) - (vcto&0xFFFF) -\r\nle16_to_cpu(hdr->iph.pkt_flags));\r\nseqnum.val = be32_to_cpu(hdr->bth[2]);\r\nif (pkt->tiddma)\r\nseqnum.seq++;\r\nelse\r\nseqnum.pkt++;\r\nhdr->bth[2] = cpu_to_be32(seqnum.val);\r\nqib_user_sdma_init_frag(pkt, pkt->naddr,\r\n0, pbclen,\r\n1, 0,\r\n0, 0,\r\nNULL, pbcvaddr,\r\npbcdaddr, pbclen);\r\npkt->index = pkt->naddr;\r\npkt->payload_size = 0;\r\npkt->naddr++;\r\nif (pkt->naddr == pkt->addrlimit) {\r\nret = -EFAULT;\r\ngoto done;\r\n}\r\nif (newlen != len) {\r\nif (dma_mapped) {\r\nput = 0;\r\ndma_mapped = 0;\r\npage = NULL;\r\nkvaddr = NULL;\r\n}\r\nlen -= newlen;\r\noffset += newlen;\r\ngoto next_fragment;\r\n}\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int qib_user_sdma_coalesce(const struct qib_devdata *dd,\r\nstruct qib_user_sdma_queue *pq,\r\nstruct qib_user_sdma_pkt *pkt,\r\nconst struct iovec *iov,\r\nunsigned long niov)\r\n{\r\nint ret = 0;\r\nstruct page *page = alloc_page(GFP_KERNEL);\r\nvoid *mpage_save;\r\nchar *mpage;\r\nint i;\r\nint len = 0;\r\nif (!page) {\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\nmpage = kmap(page);\r\nmpage_save = mpage;\r\nfor (i = 0; i < niov; i++) {\r\nint cfur;\r\ncfur = copy_from_user(mpage,\r\niov[i].iov_base, iov[i].iov_len);\r\nif (cfur) {\r\nret = -EFAULT;\r\ngoto free_unmap;\r\n}\r\nmpage += iov[i].iov_len;\r\nlen += iov[i].iov_len;\r\n}\r\nret = qib_user_sdma_page_to_frags(dd, pq, pkt,\r\npage, 0, 0, len, mpage_save);\r\ngoto done;\r\nfree_unmap:\r\nkunmap(page);\r\n__free_page(page);\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int qib_user_sdma_num_pages(const struct iovec *iov)\r\n{\r\nconst unsigned long addr = (unsigned long) iov->iov_base;\r\nconst unsigned long len = iov->iov_len;\r\nconst unsigned long spage = addr & PAGE_MASK;\r\nconst unsigned long epage = (addr + len - 1) & PAGE_MASK;\r\nreturn 1 + ((epage - spage) >> PAGE_SHIFT);\r\n}\r\nstatic void qib_user_sdma_free_pkt_frag(struct device *dev,\r\nstruct qib_user_sdma_queue *pq,\r\nstruct qib_user_sdma_pkt *pkt,\r\nint frag)\r\n{\r\nconst int i = frag;\r\nif (pkt->addr[i].page) {\r\nif (pkt->addr[i].dma_mapped)\r\ndma_unmap_page(dev,\r\npkt->addr[i].addr,\r\npkt->addr[i].dma_length,\r\nDMA_TO_DEVICE);\r\nif (pkt->addr[i].kvaddr)\r\nkunmap(pkt->addr[i].page);\r\nif (pkt->addr[i].put_page)\r\nput_page(pkt->addr[i].page);\r\nelse\r\n__free_page(pkt->addr[i].page);\r\n} else if (pkt->addr[i].kvaddr) {\r\nif (pkt->addr[i].dma_mapped) {\r\ndma_unmap_single(dev,\r\npkt->addr[i].addr,\r\npkt->addr[i].dma_length,\r\nDMA_TO_DEVICE);\r\nkfree(pkt->addr[i].kvaddr);\r\n} else if (pkt->addr[i].addr) {\r\ndma_pool_free(pq->header_cache,\r\npkt->addr[i].kvaddr, pkt->addr[i].addr);\r\n} else {\r\nkfree(pkt->addr[i].kvaddr);\r\n}\r\n}\r\n}\r\nstatic int qib_user_sdma_pin_pages(const struct qib_devdata *dd,\r\nstruct qib_user_sdma_queue *pq,\r\nstruct qib_user_sdma_pkt *pkt,\r\nunsigned long addr, int tlen, int npages)\r\n{\r\nstruct page *pages[8];\r\nint i, j;\r\nint ret = 0;\r\nwhile (npages) {\r\nif (npages > 8)\r\nj = 8;\r\nelse\r\nj = npages;\r\nret = get_user_pages_fast(addr, j, 0, pages);\r\nif (ret != j) {\r\ni = 0;\r\nj = ret;\r\nret = -ENOMEM;\r\ngoto free_pages;\r\n}\r\nfor (i = 0; i < j; i++) {\r\nunsigned long fofs = addr & ~PAGE_MASK;\r\nint flen = ((fofs + tlen) > PAGE_SIZE) ?\r\n(PAGE_SIZE - fofs) : tlen;\r\nret = qib_user_sdma_page_to_frags(dd, pq, pkt,\r\npages[i], 1, fofs, flen, NULL);\r\nif (ret < 0) {\r\ni++;\r\ngoto free_pages;\r\n}\r\naddr += flen;\r\ntlen -= flen;\r\n}\r\nnpages -= j;\r\n}\r\ngoto done;\r\nfree_pages:\r\nwhile (i < j)\r\nput_page(pages[i++]);\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int qib_user_sdma_pin_pkt(const struct qib_devdata *dd,\r\nstruct qib_user_sdma_queue *pq,\r\nstruct qib_user_sdma_pkt *pkt,\r\nconst struct iovec *iov,\r\nunsigned long niov)\r\n{\r\nint ret = 0;\r\nunsigned long idx;\r\nfor (idx = 0; idx < niov; idx++) {\r\nconst int npages = qib_user_sdma_num_pages(iov + idx);\r\nconst unsigned long addr = (unsigned long) iov[idx].iov_base;\r\nret = qib_user_sdma_pin_pages(dd, pq, pkt, addr,\r\niov[idx].iov_len, npages);\r\nif (ret < 0)\r\ngoto free_pkt;\r\n}\r\ngoto done;\r\nfree_pkt:\r\nfor (idx = 1; idx < pkt->naddr; idx++)\r\nqib_user_sdma_free_pkt_frag(&dd->pcidev->dev, pq, pkt, idx);\r\nif (pkt->addr[0].dma_mapped) {\r\ndma_unmap_single(&dd->pcidev->dev,\r\npkt->addr[0].addr,\r\npkt->addr[0].dma_length,\r\nDMA_TO_DEVICE);\r\npkt->addr[0].addr = 0;\r\npkt->addr[0].dma_mapped = 0;\r\n}\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int qib_user_sdma_init_payload(const struct qib_devdata *dd,\r\nstruct qib_user_sdma_queue *pq,\r\nstruct qib_user_sdma_pkt *pkt,\r\nconst struct iovec *iov,\r\nunsigned long niov, int npages)\r\n{\r\nint ret = 0;\r\nif (pkt->frag_size == pkt->bytes_togo &&\r\nnpages >= ARRAY_SIZE(pkt->addr))\r\nret = qib_user_sdma_coalesce(dd, pq, pkt, iov, niov);\r\nelse\r\nret = qib_user_sdma_pin_pkt(dd, pq, pkt, iov, niov);\r\nreturn ret;\r\n}\r\nstatic void qib_user_sdma_free_pkt_list(struct device *dev,\r\nstruct qib_user_sdma_queue *pq,\r\nstruct list_head *list)\r\n{\r\nstruct qib_user_sdma_pkt *pkt, *pkt_next;\r\nlist_for_each_entry_safe(pkt, pkt_next, list, list) {\r\nint i;\r\nfor (i = 0; i < pkt->naddr; i++)\r\nqib_user_sdma_free_pkt_frag(dev, pq, pkt, i);\r\nif (pkt->largepkt)\r\nkfree(pkt);\r\nelse\r\nkmem_cache_free(pq->pkt_slab, pkt);\r\n}\r\nINIT_LIST_HEAD(list);\r\n}\r\nstatic int qib_user_sdma_queue_pkts(const struct qib_devdata *dd,\r\nstruct qib_pportdata *ppd,\r\nstruct qib_user_sdma_queue *pq,\r\nconst struct iovec *iov,\r\nunsigned long niov,\r\nstruct list_head *list,\r\nint *maxpkts, int *ndesc)\r\n{\r\nunsigned long idx = 0;\r\nint ret = 0;\r\nint npkts = 0;\r\n__le32 *pbc;\r\ndma_addr_t dma_addr;\r\nstruct qib_user_sdma_pkt *pkt = NULL;\r\nsize_t len;\r\nsize_t nw;\r\nu32 counter = pq->counter;\r\nu16 frag_size;\r\nwhile (idx < niov && npkts < *maxpkts) {\r\nconst unsigned long addr = (unsigned long) iov[idx].iov_base;\r\nconst unsigned long idx_save = idx;\r\nunsigned pktnw;\r\nunsigned pktnwc;\r\nint nfrags = 0;\r\nint npages = 0;\r\nint bytes_togo = 0;\r\nint tiddma = 0;\r\nint cfur;\r\nlen = iov[idx].iov_len;\r\nnw = len >> 2;\r\nif (len < QIB_USER_SDMA_MIN_HEADER_LENGTH ||\r\nlen > PAGE_SIZE || len & 3 || addr & 3) {\r\nret = -EINVAL;\r\ngoto free_list;\r\n}\r\npbc = qib_user_sdma_alloc_header(pq, len, &dma_addr);\r\nif (!pbc) {\r\nret = -ENOMEM;\r\ngoto free_list;\r\n}\r\ncfur = copy_from_user(pbc, iov[idx].iov_base, len);\r\nif (cfur) {\r\nret = -EFAULT;\r\ngoto free_pbc;\r\n}\r\npktnwc = nw - 1;\r\npktnw = le32_to_cpu(*pbc) & 0xFFFF;\r\nif (pktnw < pktnwc) {\r\nret = -EINVAL;\r\ngoto free_pbc;\r\n}\r\nidx++;\r\nwhile (pktnwc < pktnw && idx < niov) {\r\nconst size_t slen = iov[idx].iov_len;\r\nconst unsigned long faddr =\r\n(unsigned long) iov[idx].iov_base;\r\nif (slen & 3 || faddr & 3 || !slen) {\r\nret = -EINVAL;\r\ngoto free_pbc;\r\n}\r\nnpages += qib_user_sdma_num_pages(&iov[idx]);\r\nbytes_togo += slen;\r\npktnwc += slen >> 2;\r\nidx++;\r\nnfrags++;\r\n}\r\nif (pktnwc != pktnw) {\r\nret = -EINVAL;\r\ngoto free_pbc;\r\n}\r\nfrag_size = ((le32_to_cpu(*pbc))>>16) & 0xFFFF;\r\nif (((frag_size ? frag_size : bytes_togo) + len) >\r\nppd->ibmaxlen) {\r\nret = -EINVAL;\r\ngoto free_pbc;\r\n}\r\nif (frag_size) {\r\nint pktsize, tidsmsize, n;\r\nn = npages*((2*PAGE_SIZE/frag_size)+1);\r\npktsize = sizeof(*pkt) + sizeof(pkt->addr[0])*n;\r\ntiddma = (((le32_to_cpu(pbc[7])>>\r\nQLOGIC_IB_I_TID_SHIFT)&\r\nQLOGIC_IB_I_TID_MASK) !=\r\nQLOGIC_IB_I_TID_MASK);\r\nif (tiddma)\r\ntidsmsize = iov[idx].iov_len;\r\nelse\r\ntidsmsize = 0;\r\npkt = kmalloc(pktsize+tidsmsize, GFP_KERNEL);\r\nif (!pkt) {\r\nret = -ENOMEM;\r\ngoto free_pbc;\r\n}\r\npkt->largepkt = 1;\r\npkt->frag_size = frag_size;\r\npkt->addrlimit = n + ARRAY_SIZE(pkt->addr);\r\nif (tiddma) {\r\nchar *tidsm = (char *)pkt + pktsize;\r\ncfur = copy_from_user(tidsm,\r\niov[idx].iov_base, tidsmsize);\r\nif (cfur) {\r\nret = -EFAULT;\r\ngoto free_pkt;\r\n}\r\npkt->tidsm =\r\n(struct qib_tid_session_member *)tidsm;\r\npkt->tidsmcount = tidsmsize/\r\nsizeof(struct qib_tid_session_member);\r\npkt->tidsmidx = 0;\r\nidx++;\r\n}\r\n*pbc = cpu_to_le32(le32_to_cpu(*pbc) & 0x0000FFFF);\r\n} else {\r\npkt = kmem_cache_alloc(pq->pkt_slab, GFP_KERNEL);\r\nif (!pkt) {\r\nret = -ENOMEM;\r\ngoto free_pbc;\r\n}\r\npkt->largepkt = 0;\r\npkt->frag_size = bytes_togo;\r\npkt->addrlimit = ARRAY_SIZE(pkt->addr);\r\n}\r\npkt->bytes_togo = bytes_togo;\r\npkt->payload_size = 0;\r\npkt->counter = counter;\r\npkt->tiddma = tiddma;\r\nqib_user_sdma_init_frag(pkt, 0,\r\n0, len,\r\n1, 0,\r\n0, 0,\r\nNULL, pbc,\r\ndma_addr, len);\r\npkt->index = 0;\r\npkt->naddr = 1;\r\nif (nfrags) {\r\nret = qib_user_sdma_init_payload(dd, pq, pkt,\r\niov + idx_save + 1,\r\nnfrags, npages);\r\nif (ret < 0)\r\ngoto free_pkt;\r\n} else {\r\npkt->addr[0].last_desc = 1;\r\nif (dma_addr == 0) {\r\ndma_addr = dma_map_single(&dd->pcidev->dev,\r\npbc, len, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&dd->pcidev->dev,\r\ndma_addr)) {\r\nret = -ENOMEM;\r\ngoto free_pkt;\r\n}\r\npkt->addr[0].addr = dma_addr;\r\npkt->addr[0].dma_mapped = 1;\r\n}\r\n}\r\ncounter++;\r\nnpkts++;\r\npkt->pq = pq;\r\npkt->index = 0;\r\n*ndesc += pkt->naddr;\r\nlist_add_tail(&pkt->list, list);\r\n}\r\n*maxpkts = npkts;\r\nret = idx;\r\ngoto done;\r\nfree_pkt:\r\nif (pkt->largepkt)\r\nkfree(pkt);\r\nelse\r\nkmem_cache_free(pq->pkt_slab, pkt);\r\nfree_pbc:\r\nif (dma_addr)\r\ndma_pool_free(pq->header_cache, pbc, dma_addr);\r\nelse\r\nkfree(pbc);\r\nfree_list:\r\nqib_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, list);\r\ndone:\r\nreturn ret;\r\n}\r\nstatic void qib_user_sdma_set_complete_counter(struct qib_user_sdma_queue *pq,\r\nu32 c)\r\n{\r\npq->sent_counter = c;\r\n}\r\nstatic int qib_user_sdma_queue_clean(struct qib_pportdata *ppd,\r\nstruct qib_user_sdma_queue *pq)\r\n{\r\nstruct qib_devdata *dd = ppd->dd;\r\nstruct list_head free_list;\r\nstruct qib_user_sdma_pkt *pkt;\r\nstruct qib_user_sdma_pkt *pkt_prev;\r\nunsigned long flags;\r\nint ret = 0;\r\nif (!pq->num_sending)\r\nreturn 0;\r\nINIT_LIST_HEAD(&free_list);\r\nspin_lock_irqsave(&pq->sent_lock, flags);\r\nlist_for_each_entry_safe(pkt, pkt_prev, &pq->sent, list) {\r\ns64 descd = ppd->sdma_descq_removed - pkt->added;\r\nif (descd < 0)\r\nbreak;\r\nlist_move_tail(&pkt->list, &free_list);\r\nret++;\r\npq->num_sending--;\r\n}\r\nspin_unlock_irqrestore(&pq->sent_lock, flags);\r\nif (!list_empty(&free_list)) {\r\nu32 counter;\r\npkt = list_entry(free_list.prev,\r\nstruct qib_user_sdma_pkt, list);\r\ncounter = pkt->counter;\r\nqib_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, &free_list);\r\nqib_user_sdma_set_complete_counter(pq, counter);\r\n}\r\nreturn ret;\r\n}\r\nvoid qib_user_sdma_queue_destroy(struct qib_user_sdma_queue *pq)\r\n{\r\nif (!pq)\r\nreturn;\r\npq->sdma_rb_node->refcount--;\r\nif (pq->sdma_rb_node->refcount == 0) {\r\nrb_erase(&pq->sdma_rb_node->node, &qib_user_sdma_rb_root);\r\nkfree(pq->sdma_rb_node);\r\n}\r\ndma_pool_destroy(pq->header_cache);\r\nkmem_cache_destroy(pq->pkt_slab);\r\nkfree(pq);\r\n}\r\nstatic int qib_user_sdma_hwqueue_clean(struct qib_pportdata *ppd)\r\n{\r\nint ret;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\nret = qib_sdma_make_progress(ppd);\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\nreturn ret;\r\n}\r\nvoid qib_user_sdma_queue_drain(struct qib_pportdata *ppd,\r\nstruct qib_user_sdma_queue *pq)\r\n{\r\nstruct qib_devdata *dd = ppd->dd;\r\nunsigned long flags;\r\nint i;\r\nif (!pq)\r\nreturn;\r\nfor (i = 0; i < QIB_USER_SDMA_DRAIN_TIMEOUT; i++) {\r\nmutex_lock(&pq->lock);\r\nif (!pq->num_pending && !pq->num_sending) {\r\nmutex_unlock(&pq->lock);\r\nbreak;\r\n}\r\nqib_user_sdma_hwqueue_clean(ppd);\r\nqib_user_sdma_queue_clean(ppd, pq);\r\nmutex_unlock(&pq->lock);\r\nmsleep(20);\r\n}\r\nif (pq->num_pending || pq->num_sending) {\r\nstruct qib_user_sdma_pkt *pkt;\r\nstruct qib_user_sdma_pkt *pkt_prev;\r\nstruct list_head free_list;\r\nmutex_lock(&pq->lock);\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\nif (pq->num_pending) {\r\nlist_for_each_entry_safe(pkt, pkt_prev,\r\n&ppd->sdma_userpending, list) {\r\nif (pkt->pq == pq) {\r\nlist_move_tail(&pkt->list, &pq->sent);\r\npq->num_pending--;\r\npq->num_sending++;\r\n}\r\n}\r\n}\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\nqib_dev_err(dd, "user sdma lists not empty: forcing!\n");\r\nINIT_LIST_HEAD(&free_list);\r\nlist_splice_init(&pq->sent, &free_list);\r\npq->num_sending = 0;\r\nqib_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, &free_list);\r\nmutex_unlock(&pq->lock);\r\n}\r\n}\r\nstatic inline __le64 qib_sdma_make_desc0(u8 gen,\r\nu64 addr, u64 dwlen, u64 dwoffset)\r\n{\r\nreturn cpu_to_le64(\r\n((addr & 0xfffffffcULL) << 32) |\r\n((gen & 3ULL) << 30) |\r\n((dwlen & 0x7ffULL) << 16) |\r\n(dwoffset & 0x7ffULL));\r\n}\r\nstatic inline __le64 qib_sdma_make_first_desc0(__le64 descq)\r\n{\r\nreturn descq | cpu_to_le64(1ULL << 12);\r\n}\r\nstatic inline __le64 qib_sdma_make_last_desc0(__le64 descq)\r\n{\r\nreturn descq | cpu_to_le64(1ULL << 11 | 1ULL << 13);\r\n}\r\nstatic inline __le64 qib_sdma_make_desc1(u64 addr)\r\n{\r\nreturn cpu_to_le64(addr >> 32);\r\n}\r\nstatic void qib_user_sdma_send_frag(struct qib_pportdata *ppd,\r\nstruct qib_user_sdma_pkt *pkt, int idx,\r\nunsigned ofs, u16 tail, u8 gen)\r\n{\r\nconst u64 addr = (u64) pkt->addr[idx].addr +\r\n(u64) pkt->addr[idx].offset;\r\nconst u64 dwlen = (u64) pkt->addr[idx].length / 4;\r\n__le64 *descqp;\r\n__le64 descq0;\r\ndescqp = &ppd->sdma_descq[tail].qw[0];\r\ndescq0 = qib_sdma_make_desc0(gen, addr, dwlen, ofs);\r\nif (pkt->addr[idx].first_desc)\r\ndescq0 = qib_sdma_make_first_desc0(descq0);\r\nif (pkt->addr[idx].last_desc) {\r\ndescq0 = qib_sdma_make_last_desc0(descq0);\r\nif (ppd->sdma_intrequest) {\r\ndescq0 |= cpu_to_le64(1ULL << 15);\r\nppd->sdma_intrequest = 0;\r\n}\r\n}\r\ndescqp[0] = descq0;\r\ndescqp[1] = qib_sdma_make_desc1(addr);\r\n}\r\nvoid qib_user_sdma_send_desc(struct qib_pportdata *ppd,\r\nstruct list_head *pktlist)\r\n{\r\nstruct qib_devdata *dd = ppd->dd;\r\nu16 nfree, nsent;\r\nu16 tail, tail_c;\r\nu8 gen, gen_c;\r\nnfree = qib_sdma_descq_freecnt(ppd);\r\nif (!nfree)\r\nreturn;\r\nretry:\r\nnsent = 0;\r\ntail_c = tail = ppd->sdma_descq_tail;\r\ngen_c = gen = ppd->sdma_generation;\r\nwhile (!list_empty(pktlist)) {\r\nstruct qib_user_sdma_pkt *pkt =\r\nlist_entry(pktlist->next, struct qib_user_sdma_pkt,\r\nlist);\r\nint i, j, c = 0;\r\nunsigned ofs = 0;\r\nu16 dtail = tail;\r\nfor (i = pkt->index; i < pkt->naddr && nfree; i++) {\r\nqib_user_sdma_send_frag(ppd, pkt, i, ofs, tail, gen);\r\nofs += pkt->addr[i].length >> 2;\r\nif (++tail == ppd->sdma_descq_cnt) {\r\ntail = 0;\r\n++gen;\r\nppd->sdma_intrequest = 1;\r\n} else if (tail == (ppd->sdma_descq_cnt>>1)) {\r\nppd->sdma_intrequest = 1;\r\n}\r\nnfree--;\r\nif (pkt->addr[i].last_desc == 0)\r\ncontinue;\r\nif (ofs > dd->piosize2kmax_dwords) {\r\nfor (j = pkt->index; j <= i; j++) {\r\nppd->sdma_descq[dtail].qw[0] |=\r\ncpu_to_le64(1ULL << 14);\r\nif (++dtail == ppd->sdma_descq_cnt)\r\ndtail = 0;\r\n}\r\n}\r\nc += i + 1 - pkt->index;\r\npkt->index = i + 1;\r\ntail_c = dtail = tail;\r\ngen_c = gen;\r\nofs = 0;\r\n}\r\nppd->sdma_descq_added += c;\r\nnsent += c;\r\nif (pkt->index == pkt->naddr) {\r\npkt->added = ppd->sdma_descq_added;\r\npkt->pq->added = pkt->added;\r\npkt->pq->num_pending--;\r\nspin_lock(&pkt->pq->sent_lock);\r\npkt->pq->num_sending++;\r\nlist_move_tail(&pkt->list, &pkt->pq->sent);\r\nspin_unlock(&pkt->pq->sent_lock);\r\n}\r\nif (!nfree || (nsent<<2) > ppd->sdma_descq_cnt)\r\nbreak;\r\n}\r\nif (ppd->sdma_descq_tail != tail_c) {\r\nppd->sdma_generation = gen_c;\r\ndd->f_sdma_update_tail(ppd, tail_c);\r\n}\r\nif (nfree && !list_empty(pktlist))\r\ngoto retry;\r\n}\r\nstatic int qib_user_sdma_push_pkts(struct qib_pportdata *ppd,\r\nstruct qib_user_sdma_queue *pq,\r\nstruct list_head *pktlist, int count)\r\n{\r\nunsigned long flags;\r\nif (unlikely(!(ppd->lflags & QIBL_LINKACTIVE)))\r\nreturn -ECOMM;\r\nif (pq->sdma_rb_node->refcount > 1) {\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\nif (unlikely(!__qib_sdma_running(ppd))) {\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\nreturn -ECOMM;\r\n}\r\npq->num_pending += count;\r\nlist_splice_tail_init(pktlist, &ppd->sdma_userpending);\r\nqib_user_sdma_send_desc(ppd, &ppd->sdma_userpending);\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\nreturn 0;\r\n}\r\npq->num_pending += count;\r\ndo {\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\nif (unlikely(!__qib_sdma_running(ppd))) {\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\nreturn -ECOMM;\r\n}\r\nqib_user_sdma_send_desc(ppd, pktlist);\r\nif (!list_empty(pktlist))\r\nqib_sdma_make_progress(ppd);\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\n} while (!list_empty(pktlist));\r\nreturn 0;\r\n}\r\nint qib_user_sdma_writev(struct qib_ctxtdata *rcd,\r\nstruct qib_user_sdma_queue *pq,\r\nconst struct iovec *iov,\r\nunsigned long dim)\r\n{\r\nstruct qib_devdata *dd = rcd->dd;\r\nstruct qib_pportdata *ppd = rcd->ppd;\r\nint ret = 0;\r\nstruct list_head list;\r\nint npkts = 0;\r\nINIT_LIST_HEAD(&list);\r\nmutex_lock(&pq->lock);\r\nif (!qib_sdma_running(ppd))\r\ngoto done_unlock;\r\nif (pq->added > ppd->sdma_descq_removed)\r\nqib_user_sdma_hwqueue_clean(ppd);\r\nif (pq->num_sending)\r\nqib_user_sdma_queue_clean(ppd, pq);\r\nwhile (dim) {\r\nint mxp = 1;\r\nint ndesc = 0;\r\nret = qib_user_sdma_queue_pkts(dd, ppd, pq,\r\niov, dim, &list, &mxp, &ndesc);\r\nif (ret < 0)\r\ngoto done_unlock;\r\nelse {\r\ndim -= ret;\r\niov += ret;\r\n}\r\nif (!list_empty(&list)) {\r\nif (qib_sdma_descq_freecnt(ppd) < ndesc) {\r\nqib_user_sdma_hwqueue_clean(ppd);\r\nif (pq->num_sending)\r\nqib_user_sdma_queue_clean(ppd, pq);\r\n}\r\nret = qib_user_sdma_push_pkts(ppd, pq, &list, mxp);\r\nif (ret < 0)\r\ngoto done_unlock;\r\nelse {\r\nnpkts += mxp;\r\npq->counter += mxp;\r\n}\r\n}\r\n}\r\ndone_unlock:\r\nif (!list_empty(&list))\r\nqib_user_sdma_free_pkt_list(&dd->pcidev->dev, pq, &list);\r\nmutex_unlock(&pq->lock);\r\nreturn (ret < 0) ? ret : npkts;\r\n}\r\nint qib_user_sdma_make_progress(struct qib_pportdata *ppd,\r\nstruct qib_user_sdma_queue *pq)\r\n{\r\nint ret = 0;\r\nmutex_lock(&pq->lock);\r\nqib_user_sdma_hwqueue_clean(ppd);\r\nret = qib_user_sdma_queue_clean(ppd, pq);\r\nmutex_unlock(&pq->lock);\r\nreturn ret;\r\n}\r\nu32 qib_user_sdma_complete_counter(const struct qib_user_sdma_queue *pq)\r\n{\r\nreturn pq ? pq->sent_counter : 0;\r\n}\r\nu32 qib_user_sdma_inflight_counter(struct qib_user_sdma_queue *pq)\r\n{\r\nreturn pq ? pq->counter : 0;\r\n}
