static inline struct kthread *to_kthread(struct task_struct *k)\r\n{\r\nreturn __to_kthread(k->vfork_done);\r\n}\r\nstatic struct kthread *to_live_kthread(struct task_struct *k)\r\n{\r\nstruct completion *vfork = ACCESS_ONCE(k->vfork_done);\r\nif (likely(vfork))\r\nreturn __to_kthread(vfork);\r\nreturn NULL;\r\n}\r\nbool kthread_should_stop(void)\r\n{\r\nreturn test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags);\r\n}\r\nbool kthread_should_park(void)\r\n{\r\nreturn test_bit(KTHREAD_SHOULD_PARK, &to_kthread(current)->flags);\r\n}\r\nbool kthread_freezable_should_stop(bool *was_frozen)\r\n{\r\nbool frozen = false;\r\nmight_sleep();\r\nif (unlikely(freezing(current)))\r\nfrozen = __refrigerator(true);\r\nif (was_frozen)\r\n*was_frozen = frozen;\r\nreturn kthread_should_stop();\r\n}\r\nvoid *kthread_data(struct task_struct *task)\r\n{\r\nreturn to_kthread(task)->data;\r\n}\r\nvoid *probe_kthread_data(struct task_struct *task)\r\n{\r\nstruct kthread *kthread = to_kthread(task);\r\nvoid *data = NULL;\r\nprobe_kernel_read(&data, &kthread->data, sizeof(data));\r\nreturn data;\r\n}\r\nstatic void __kthread_parkme(struct kthread *self)\r\n{\r\n__set_current_state(TASK_PARKED);\r\nwhile (test_bit(KTHREAD_SHOULD_PARK, &self->flags)) {\r\nif (!test_and_set_bit(KTHREAD_IS_PARKED, &self->flags))\r\ncomplete(&self->parked);\r\nschedule();\r\n__set_current_state(TASK_PARKED);\r\n}\r\nclear_bit(KTHREAD_IS_PARKED, &self->flags);\r\n__set_current_state(TASK_RUNNING);\r\n}\r\nvoid kthread_parkme(void)\r\n{\r\n__kthread_parkme(to_kthread(current));\r\n}\r\nstatic int kthread(void *_create)\r\n{\r\nstruct kthread_create_info *create = _create;\r\nint (*threadfn)(void *data) = create->threadfn;\r\nvoid *data = create->data;\r\nstruct completion *done;\r\nstruct kthread self;\r\nint ret;\r\nself.flags = 0;\r\nself.data = data;\r\ninit_completion(&self.exited);\r\ninit_completion(&self.parked);\r\ncurrent->vfork_done = &self.exited;\r\ndone = xchg(&create->done, NULL);\r\nif (!done) {\r\nkfree(create);\r\ndo_exit(-EINTR);\r\n}\r\n__set_current_state(TASK_UNINTERRUPTIBLE);\r\ncreate->result = current;\r\ncomplete(done);\r\nschedule();\r\nret = -EINTR;\r\nif (!test_bit(KTHREAD_SHOULD_STOP, &self.flags)) {\r\n__kthread_parkme(&self);\r\nret = threadfn(data);\r\n}\r\ndo_exit(ret);\r\n}\r\nint tsk_fork_get_node(struct task_struct *tsk)\r\n{\r\n#ifdef CONFIG_NUMA\r\nif (tsk == kthreadd_task)\r\nreturn tsk->pref_node_fork;\r\n#endif\r\nreturn NUMA_NO_NODE;\r\n}\r\nstatic void create_kthread(struct kthread_create_info *create)\r\n{\r\nint pid;\r\n#ifdef CONFIG_NUMA\r\ncurrent->pref_node_fork = create->node;\r\n#endif\r\npid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);\r\nif (pid < 0) {\r\nstruct completion *done = xchg(&create->done, NULL);\r\nif (!done) {\r\nkfree(create);\r\nreturn;\r\n}\r\ncreate->result = ERR_PTR(pid);\r\ncomplete(done);\r\n}\r\n}\r\nstruct task_struct *kthread_create_on_node(int (*threadfn)(void *data),\r\nvoid *data, int node,\r\nconst char namefmt[],\r\n...)\r\n{\r\nDECLARE_COMPLETION_ONSTACK(done);\r\nstruct task_struct *task;\r\nstruct kthread_create_info *create = kmalloc(sizeof(*create),\r\nGFP_KERNEL);\r\nif (!create)\r\nreturn ERR_PTR(-ENOMEM);\r\ncreate->threadfn = threadfn;\r\ncreate->data = data;\r\ncreate->node = node;\r\ncreate->done = &done;\r\nspin_lock(&kthread_create_lock);\r\nlist_add_tail(&create->list, &kthread_create_list);\r\nspin_unlock(&kthread_create_lock);\r\nwake_up_process(kthreadd_task);\r\nif (unlikely(wait_for_completion_killable(&done))) {\r\nif (xchg(&create->done, NULL))\r\nreturn ERR_PTR(-EINTR);\r\nwait_for_completion(&done);\r\n}\r\ntask = create->result;\r\nif (!IS_ERR(task)) {\r\nstatic const struct sched_param param = { .sched_priority = 0 };\r\nva_list args;\r\nva_start(args, namefmt);\r\nvsnprintf(task->comm, sizeof(task->comm), namefmt, args);\r\nva_end(args);\r\nsched_setscheduler_nocheck(task, SCHED_NORMAL, &param);\r\nset_cpus_allowed_ptr(task, cpu_all_mask);\r\n}\r\nkfree(create);\r\nreturn task;\r\n}\r\nstatic void __kthread_bind(struct task_struct *p, unsigned int cpu, long state)\r\n{\r\nif (!wait_task_inactive(p, state)) {\r\nWARN_ON(1);\r\nreturn;\r\n}\r\ndo_set_cpus_allowed(p, cpumask_of(cpu));\r\np->flags |= PF_NO_SETAFFINITY;\r\n}\r\nvoid kthread_bind(struct task_struct *p, unsigned int cpu)\r\n{\r\n__kthread_bind(p, cpu, TASK_UNINTERRUPTIBLE);\r\n}\r\nstruct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),\r\nvoid *data, unsigned int cpu,\r\nconst char *namefmt)\r\n{\r\nstruct task_struct *p;\r\np = kthread_create_on_node(threadfn, data, cpu_to_node(cpu), namefmt,\r\ncpu);\r\nif (IS_ERR(p))\r\nreturn p;\r\nset_bit(KTHREAD_IS_PER_CPU, &to_kthread(p)->flags);\r\nto_kthread(p)->cpu = cpu;\r\nkthread_park(p);\r\nreturn p;\r\n}\r\nstatic void __kthread_unpark(struct task_struct *k, struct kthread *kthread)\r\n{\r\nclear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);\r\nif (test_and_clear_bit(KTHREAD_IS_PARKED, &kthread->flags)) {\r\nif (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))\r\n__kthread_bind(k, kthread->cpu, TASK_PARKED);\r\nwake_up_state(k, TASK_PARKED);\r\n}\r\n}\r\nvoid kthread_unpark(struct task_struct *k)\r\n{\r\nstruct kthread *kthread = to_live_kthread(k);\r\nif (kthread)\r\n__kthread_unpark(k, kthread);\r\n}\r\nint kthread_park(struct task_struct *k)\r\n{\r\nstruct kthread *kthread = to_live_kthread(k);\r\nint ret = -ENOSYS;\r\nif (kthread) {\r\nif (!test_bit(KTHREAD_IS_PARKED, &kthread->flags)) {\r\nset_bit(KTHREAD_SHOULD_PARK, &kthread->flags);\r\nif (k != current) {\r\nwake_up_process(k);\r\nwait_for_completion(&kthread->parked);\r\n}\r\n}\r\nret = 0;\r\n}\r\nreturn ret;\r\n}\r\nint kthread_stop(struct task_struct *k)\r\n{\r\nstruct kthread *kthread;\r\nint ret;\r\ntrace_sched_kthread_stop(k);\r\nget_task_struct(k);\r\nkthread = to_live_kthread(k);\r\nif (kthread) {\r\nset_bit(KTHREAD_SHOULD_STOP, &kthread->flags);\r\n__kthread_unpark(k, kthread);\r\nwake_up_process(k);\r\nwait_for_completion(&kthread->exited);\r\n}\r\nret = k->exit_code;\r\nput_task_struct(k);\r\ntrace_sched_kthread_stop_ret(ret);\r\nreturn ret;\r\n}\r\nint kthreadd(void *unused)\r\n{\r\nstruct task_struct *tsk = current;\r\nset_task_comm(tsk, "kthreadd");\r\nignore_signals(tsk);\r\nset_cpus_allowed_ptr(tsk, cpu_all_mask);\r\nset_mems_allowed(node_states[N_MEMORY]);\r\ncurrent->flags |= PF_NOFREEZE;\r\nfor (;;) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (list_empty(&kthread_create_list))\r\nschedule();\r\n__set_current_state(TASK_RUNNING);\r\nspin_lock(&kthread_create_lock);\r\nwhile (!list_empty(&kthread_create_list)) {\r\nstruct kthread_create_info *create;\r\ncreate = list_entry(kthread_create_list.next,\r\nstruct kthread_create_info, list);\r\nlist_del_init(&create->list);\r\nspin_unlock(&kthread_create_lock);\r\ncreate_kthread(create);\r\nspin_lock(&kthread_create_lock);\r\n}\r\nspin_unlock(&kthread_create_lock);\r\n}\r\nreturn 0;\r\n}\r\nvoid __init_kthread_worker(struct kthread_worker *worker,\r\nconst char *name,\r\nstruct lock_class_key *key)\r\n{\r\nspin_lock_init(&worker->lock);\r\nlockdep_set_class_and_name(&worker->lock, key, name);\r\nINIT_LIST_HEAD(&worker->work_list);\r\nworker->task = NULL;\r\n}\r\nint kthread_worker_fn(void *worker_ptr)\r\n{\r\nstruct kthread_worker *worker = worker_ptr;\r\nstruct kthread_work *work;\r\nWARN_ON(worker->task);\r\nworker->task = current;\r\nrepeat:\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (kthread_should_stop()) {\r\n__set_current_state(TASK_RUNNING);\r\nspin_lock_irq(&worker->lock);\r\nworker->task = NULL;\r\nspin_unlock_irq(&worker->lock);\r\nreturn 0;\r\n}\r\nwork = NULL;\r\nspin_lock_irq(&worker->lock);\r\nif (!list_empty(&worker->work_list)) {\r\nwork = list_first_entry(&worker->work_list,\r\nstruct kthread_work, node);\r\nlist_del_init(&work->node);\r\n}\r\nworker->current_work = work;\r\nspin_unlock_irq(&worker->lock);\r\nif (work) {\r\n__set_current_state(TASK_RUNNING);\r\nwork->func(work);\r\n} else if (!freezing(current))\r\nschedule();\r\ntry_to_freeze();\r\ngoto repeat;\r\n}\r\nstatic void insert_kthread_work(struct kthread_worker *worker,\r\nstruct kthread_work *work,\r\nstruct list_head *pos)\r\n{\r\nlockdep_assert_held(&worker->lock);\r\nlist_add_tail(&work->node, pos);\r\nwork->worker = worker;\r\nif (!worker->current_work && likely(worker->task))\r\nwake_up_process(worker->task);\r\n}\r\nbool queue_kthread_work(struct kthread_worker *worker,\r\nstruct kthread_work *work)\r\n{\r\nbool ret = false;\r\nunsigned long flags;\r\nspin_lock_irqsave(&worker->lock, flags);\r\nif (list_empty(&work->node)) {\r\ninsert_kthread_work(worker, work, &worker->work_list);\r\nret = true;\r\n}\r\nspin_unlock_irqrestore(&worker->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void kthread_flush_work_fn(struct kthread_work *work)\r\n{\r\nstruct kthread_flush_work *fwork =\r\ncontainer_of(work, struct kthread_flush_work, work);\r\ncomplete(&fwork->done);\r\n}\r\nvoid flush_kthread_work(struct kthread_work *work)\r\n{\r\nstruct kthread_flush_work fwork = {\r\nKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),\r\nCOMPLETION_INITIALIZER_ONSTACK(fwork.done),\r\n};\r\nstruct kthread_worker *worker;\r\nbool noop = false;\r\nretry:\r\nworker = work->worker;\r\nif (!worker)\r\nreturn;\r\nspin_lock_irq(&worker->lock);\r\nif (work->worker != worker) {\r\nspin_unlock_irq(&worker->lock);\r\ngoto retry;\r\n}\r\nif (!list_empty(&work->node))\r\ninsert_kthread_work(worker, &fwork.work, work->node.next);\r\nelse if (worker->current_work == work)\r\ninsert_kthread_work(worker, &fwork.work, worker->work_list.next);\r\nelse\r\nnoop = true;\r\nspin_unlock_irq(&worker->lock);\r\nif (!noop)\r\nwait_for_completion(&fwork.done);\r\n}\r\nvoid flush_kthread_worker(struct kthread_worker *worker)\r\n{\r\nstruct kthread_flush_work fwork = {\r\nKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),\r\nCOMPLETION_INITIALIZER_ONSTACK(fwork.done),\r\n};\r\nqueue_kthread_work(worker, &fwork.work);\r\nwait_for_completion(&fwork.done);\r\n}
