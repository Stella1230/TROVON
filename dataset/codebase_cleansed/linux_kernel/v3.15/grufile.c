static void gru_vma_close(struct vm_area_struct *vma)\r\n{\r\nstruct gru_vma_data *vdata;\r\nstruct gru_thread_state *gts;\r\nstruct list_head *entry, *next;\r\nif (!vma->vm_private_data)\r\nreturn;\r\nvdata = vma->vm_private_data;\r\nvma->vm_private_data = NULL;\r\ngru_dbg(grudev, "vma %p, file %p, vdata %p\n", vma, vma->vm_file,\r\nvdata);\r\nlist_for_each_safe(entry, next, &vdata->vd_head) {\r\ngts =\r\nlist_entry(entry, struct gru_thread_state, ts_next);\r\nlist_del(&gts->ts_next);\r\nmutex_lock(&gts->ts_ctxlock);\r\nif (gts->ts_gru)\r\ngru_unload_context(gts, 0);\r\nmutex_unlock(&gts->ts_ctxlock);\r\ngts_drop(gts);\r\n}\r\nkfree(vdata);\r\nSTAT(vdata_free);\r\n}\r\nstatic int gru_file_mmap(struct file *file, struct vm_area_struct *vma)\r\n{\r\nif ((vma->vm_flags & (VM_SHARED | VM_WRITE)) != (VM_SHARED | VM_WRITE))\r\nreturn -EPERM;\r\nif (vma->vm_start & (GRU_GSEG_PAGESIZE - 1) ||\r\nvma->vm_end & (GRU_GSEG_PAGESIZE - 1))\r\nreturn -EINVAL;\r\nvma->vm_flags |= VM_IO | VM_PFNMAP | VM_LOCKED |\r\nVM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP;\r\nvma->vm_page_prot = PAGE_SHARED;\r\nvma->vm_ops = &gru_vm_ops;\r\nvma->vm_private_data = gru_alloc_vma_data(vma, 0);\r\nif (!vma->vm_private_data)\r\nreturn -ENOMEM;\r\ngru_dbg(grudev, "file %p, vaddr 0x%lx, vma %p, vdata %p\n",\r\nfile, vma->vm_start, vma, vma->vm_private_data);\r\nreturn 0;\r\n}\r\nstatic int gru_create_new_context(unsigned long arg)\r\n{\r\nstruct gru_create_context_req req;\r\nstruct vm_area_struct *vma;\r\nstruct gru_vma_data *vdata;\r\nint ret = -EINVAL;\r\nif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\r\nreturn -EFAULT;\r\nif (req.data_segment_bytes > max_user_dsr_bytes)\r\nreturn -EINVAL;\r\nif (req.control_blocks > max_user_cbrs || !req.maximum_thread_count)\r\nreturn -EINVAL;\r\nif (!(req.options & GRU_OPT_MISS_MASK))\r\nreq.options |= GRU_OPT_MISS_FMM_INTR;\r\ndown_write(&current->mm->mmap_sem);\r\nvma = gru_find_vma(req.gseg);\r\nif (vma) {\r\nvdata = vma->vm_private_data;\r\nvdata->vd_user_options = req.options;\r\nvdata->vd_dsr_au_count =\r\nGRU_DS_BYTES_TO_AU(req.data_segment_bytes);\r\nvdata->vd_cbr_au_count = GRU_CB_COUNT_TO_AU(req.control_blocks);\r\nvdata->vd_tlb_preload_count = req.tlb_preload_count;\r\nret = 0;\r\n}\r\nup_write(&current->mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic long gru_get_config_info(unsigned long arg)\r\n{\r\nstruct gru_config_info info;\r\nint nodesperblade;\r\nif (num_online_nodes() > 1 &&\r\n(uv_node_to_blade_id(1) == uv_node_to_blade_id(0)))\r\nnodesperblade = 2;\r\nelse\r\nnodesperblade = 1;\r\nmemset(&info, 0, sizeof(info));\r\ninfo.cpus = num_online_cpus();\r\ninfo.nodes = num_online_nodes();\r\ninfo.blades = info.nodes / nodesperblade;\r\ninfo.chiplets = GRU_CHIPLETS_PER_BLADE * info.blades;\r\nif (copy_to_user((void __user *)arg, &info, sizeof(info)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic long gru_file_unlocked_ioctl(struct file *file, unsigned int req,\r\nunsigned long arg)\r\n{\r\nint err = -EBADRQC;\r\ngru_dbg(grudev, "file %p, req 0x%x, 0x%lx\n", file, req, arg);\r\nswitch (req) {\r\ncase GRU_CREATE_CONTEXT:\r\nerr = gru_create_new_context(arg);\r\nbreak;\r\ncase GRU_SET_CONTEXT_OPTION:\r\nerr = gru_set_context_option(arg);\r\nbreak;\r\ncase GRU_USER_GET_EXCEPTION_DETAIL:\r\nerr = gru_get_exception_detail(arg);\r\nbreak;\r\ncase GRU_USER_UNLOAD_CONTEXT:\r\nerr = gru_user_unload_context(arg);\r\nbreak;\r\ncase GRU_USER_FLUSH_TLB:\r\nerr = gru_user_flush_tlb(arg);\r\nbreak;\r\ncase GRU_USER_CALL_OS:\r\nerr = gru_handle_user_call_os(arg);\r\nbreak;\r\ncase GRU_GET_GSEG_STATISTICS:\r\nerr = gru_get_gseg_statistics(arg);\r\nbreak;\r\ncase GRU_KTEST:\r\nerr = gru_ktest(arg);\r\nbreak;\r\ncase GRU_GET_CONFIG_INFO:\r\nerr = gru_get_config_info(arg);\r\nbreak;\r\ncase GRU_DUMP_CHIPLET_STATE:\r\nerr = gru_dump_chiplet_request(arg);\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic void gru_init_chiplet(struct gru_state *gru, unsigned long paddr,\r\nvoid *vaddr, int blade_id, int chiplet_id)\r\n{\r\nspin_lock_init(&gru->gs_lock);\r\nspin_lock_init(&gru->gs_asid_lock);\r\ngru->gs_gru_base_paddr = paddr;\r\ngru->gs_gru_base_vaddr = vaddr;\r\ngru->gs_gid = blade_id * GRU_CHIPLETS_PER_BLADE + chiplet_id;\r\ngru->gs_blade = gru_base[blade_id];\r\ngru->gs_blade_id = blade_id;\r\ngru->gs_chiplet_id = chiplet_id;\r\ngru->gs_cbr_map = (GRU_CBR_AU == 64) ? ~0 : (1UL << GRU_CBR_AU) - 1;\r\ngru->gs_dsr_map = (1UL << GRU_DSR_AU) - 1;\r\ngru->gs_asid_limit = MAX_ASID;\r\ngru_tgh_flush_init(gru);\r\nif (gru->gs_gid >= gru_max_gids)\r\ngru_max_gids = gru->gs_gid + 1;\r\ngru_dbg(grudev, "bid %d, gid %d, vaddr %p (0x%lx)\n",\r\nblade_id, gru->gs_gid, gru->gs_gru_base_vaddr,\r\ngru->gs_gru_base_paddr);\r\n}\r\nstatic int gru_init_tables(unsigned long gru_base_paddr, void *gru_base_vaddr)\r\n{\r\nint pnode, nid, bid, chip;\r\nint cbrs, dsrbytes, n;\r\nint order = get_order(sizeof(struct gru_blade_state));\r\nstruct page *page;\r\nstruct gru_state *gru;\r\nunsigned long paddr;\r\nvoid *vaddr;\r\nmax_user_cbrs = GRU_NUM_CB;\r\nmax_user_dsr_bytes = GRU_NUM_DSR_BYTES;\r\nfor_each_possible_blade(bid) {\r\npnode = uv_blade_to_pnode(bid);\r\nnid = uv_blade_to_memory_nid(bid);\r\npage = alloc_pages_node(nid, GFP_KERNEL, order);\r\nif (!page)\r\ngoto fail;\r\ngru_base[bid] = page_address(page);\r\nmemset(gru_base[bid], 0, sizeof(struct gru_blade_state));\r\ngru_base[bid]->bs_lru_gru = &gru_base[bid]->bs_grus[0];\r\nspin_lock_init(&gru_base[bid]->bs_lock);\r\ninit_rwsem(&gru_base[bid]->bs_kgts_sema);\r\ndsrbytes = 0;\r\ncbrs = 0;\r\nfor (gru = gru_base[bid]->bs_grus, chip = 0;\r\nchip < GRU_CHIPLETS_PER_BLADE;\r\nchip++, gru++) {\r\npaddr = gru_chiplet_paddr(gru_base_paddr, pnode, chip);\r\nvaddr = gru_chiplet_vaddr(gru_base_vaddr, pnode, chip);\r\ngru_init_chiplet(gru, paddr, vaddr, bid, chip);\r\nn = hweight64(gru->gs_cbr_map) * GRU_CBR_AU_SIZE;\r\ncbrs = max(cbrs, n);\r\nn = hweight64(gru->gs_dsr_map) * GRU_DSR_AU_BYTES;\r\ndsrbytes = max(dsrbytes, n);\r\n}\r\nmax_user_cbrs = min(max_user_cbrs, cbrs);\r\nmax_user_dsr_bytes = min(max_user_dsr_bytes, dsrbytes);\r\n}\r\nreturn 0;\r\nfail:\r\nfor (bid--; bid >= 0; bid--)\r\nfree_pages((unsigned long)gru_base[bid], order);\r\nreturn -ENOMEM;\r\n}\r\nstatic void gru_free_tables(void)\r\n{\r\nint bid;\r\nint order = get_order(sizeof(struct gru_state) *\r\nGRU_CHIPLETS_PER_BLADE);\r\nfor (bid = 0; bid < GRU_MAX_BLADES; bid++)\r\nfree_pages((unsigned long)gru_base[bid], order);\r\n}\r\nstatic unsigned long gru_chiplet_cpu_to_mmr(int chiplet, int cpu, int *corep)\r\n{\r\nunsigned long mmr = 0;\r\nint core;\r\ncore = uv_cpu_core_number(cpu) + UV_MAX_INT_CORES * uv_cpu_socket_number(cpu);\r\nif (core >= GRU_NUM_TFM || uv_cpu_ht_number(cpu))\r\nreturn 0;\r\nif (chiplet == 0) {\r\nmmr = UVH_GR0_TLB_INT0_CONFIG +\r\ncore * (UVH_GR0_TLB_INT1_CONFIG - UVH_GR0_TLB_INT0_CONFIG);\r\n} else if (chiplet == 1) {\r\nmmr = UVH_GR1_TLB_INT0_CONFIG +\r\ncore * (UVH_GR1_TLB_INT1_CONFIG - UVH_GR1_TLB_INT0_CONFIG);\r\n} else {\r\nBUG();\r\n}\r\n*corep = core;\r\nreturn mmr;\r\n}\r\nstatic void gru_noop(struct irq_data *d)\r\n{\r\n}\r\nstatic int gru_chiplet_setup_tlb_irq(int chiplet, char *irq_name,\r\nirq_handler_t irq_handler, int cpu, int blade)\r\n{\r\nunsigned long mmr;\r\nint irq = IRQ_GRU + chiplet;\r\nint ret, core;\r\nmmr = gru_chiplet_cpu_to_mmr(chiplet, cpu, &core);\r\nif (mmr == 0)\r\nreturn 0;\r\nif (gru_irq_count[chiplet] == 0) {\r\ngru_chip[chiplet].name = irq_name;\r\nret = irq_set_chip(irq, &gru_chip[chiplet]);\r\nif (ret) {\r\nprintk(KERN_ERR "%s: set_irq_chip failed, errno=%d\n",\r\nGRU_DRIVER_ID_STR, -ret);\r\nreturn ret;\r\n}\r\nret = request_irq(irq, irq_handler, 0, irq_name, NULL);\r\nif (ret) {\r\nprintk(KERN_ERR "%s: request_irq failed, errno=%d\n",\r\nGRU_DRIVER_ID_STR, -ret);\r\nreturn ret;\r\n}\r\n}\r\ngru_irq_count[chiplet]++;\r\nreturn 0;\r\n}\r\nstatic void gru_chiplet_teardown_tlb_irq(int chiplet, int cpu, int blade)\r\n{\r\nunsigned long mmr;\r\nint core, irq = IRQ_GRU + chiplet;\r\nif (gru_irq_count[chiplet] == 0)\r\nreturn;\r\nmmr = gru_chiplet_cpu_to_mmr(chiplet, cpu, &core);\r\nif (mmr == 0)\r\nreturn;\r\nif (--gru_irq_count[chiplet] == 0)\r\nfree_irq(irq, NULL);\r\n}\r\nstatic int gru_chiplet_setup_tlb_irq(int chiplet, char *irq_name,\r\nirq_handler_t irq_handler, int cpu, int blade)\r\n{\r\nunsigned long mmr;\r\nint irq, core;\r\nint ret;\r\nmmr = gru_chiplet_cpu_to_mmr(chiplet, cpu, &core);\r\nif (mmr == 0)\r\nreturn 0;\r\nirq = uv_setup_irq(irq_name, cpu, blade, mmr, UV_AFFINITY_CPU);\r\nif (irq < 0) {\r\nprintk(KERN_ERR "%s: uv_setup_irq failed, errno=%d\n",\r\nGRU_DRIVER_ID_STR, -irq);\r\nreturn irq;\r\n}\r\nret = request_irq(irq, irq_handler, 0, irq_name, NULL);\r\nif (ret) {\r\nuv_teardown_irq(irq);\r\nprintk(KERN_ERR "%s: request_irq failed, errno=%d\n",\r\nGRU_DRIVER_ID_STR, -ret);\r\nreturn ret;\r\n}\r\ngru_base[blade]->bs_grus[chiplet].gs_irq[core] = irq;\r\nreturn 0;\r\n}\r\nstatic void gru_chiplet_teardown_tlb_irq(int chiplet, int cpu, int blade)\r\n{\r\nint irq, core;\r\nunsigned long mmr;\r\nmmr = gru_chiplet_cpu_to_mmr(chiplet, cpu, &core);\r\nif (mmr) {\r\nirq = gru_base[blade]->bs_grus[chiplet].gs_irq[core];\r\nif (irq) {\r\nfree_irq(irq, NULL);\r\nuv_teardown_irq(irq);\r\n}\r\n}\r\n}\r\nstatic void gru_teardown_tlb_irqs(void)\r\n{\r\nint blade;\r\nint cpu;\r\nfor_each_online_cpu(cpu) {\r\nblade = uv_cpu_to_blade_id(cpu);\r\ngru_chiplet_teardown_tlb_irq(0, cpu, blade);\r\ngru_chiplet_teardown_tlb_irq(1, cpu, blade);\r\n}\r\nfor_each_possible_blade(blade) {\r\nif (uv_blade_nr_possible_cpus(blade))\r\ncontinue;\r\ngru_chiplet_teardown_tlb_irq(0, 0, blade);\r\ngru_chiplet_teardown_tlb_irq(1, 0, blade);\r\n}\r\n}\r\nstatic int gru_setup_tlb_irqs(void)\r\n{\r\nint blade;\r\nint cpu;\r\nint ret;\r\nfor_each_online_cpu(cpu) {\r\nblade = uv_cpu_to_blade_id(cpu);\r\nret = gru_chiplet_setup_tlb_irq(0, "GRU0_TLB", gru0_intr, cpu, blade);\r\nif (ret != 0)\r\ngoto exit1;\r\nret = gru_chiplet_setup_tlb_irq(1, "GRU1_TLB", gru1_intr, cpu, blade);\r\nif (ret != 0)\r\ngoto exit1;\r\n}\r\nfor_each_possible_blade(blade) {\r\nif (uv_blade_nr_possible_cpus(blade))\r\ncontinue;\r\nret = gru_chiplet_setup_tlb_irq(0, "GRU0_TLB", gru_intr_mblade, 0, blade);\r\nif (ret != 0)\r\ngoto exit1;\r\nret = gru_chiplet_setup_tlb_irq(1, "GRU1_TLB", gru_intr_mblade, 0, blade);\r\nif (ret != 0)\r\ngoto exit1;\r\n}\r\nreturn 0;\r\nexit1:\r\ngru_teardown_tlb_irqs();\r\nreturn ret;\r\n}\r\nstatic int __init gru_init(void)\r\n{\r\nint ret;\r\nif (!is_uv_system() || (is_uvx_hub() && !is_uv2_hub()))\r\nreturn 0;\r\n#if defined CONFIG_IA64\r\ngru_start_paddr = 0xd000000000UL;\r\n#else\r\ngru_start_paddr = uv_read_local_mmr(UVH_RH_GAM_GRU_OVERLAY_CONFIG_MMR) &\r\n0x7fffffffffffUL;\r\n#endif\r\ngru_start_vaddr = __va(gru_start_paddr);\r\ngru_end_paddr = gru_start_paddr + GRU_MAX_BLADES * GRU_SIZE;\r\nprintk(KERN_INFO "GRU space: 0x%lx - 0x%lx\n",\r\ngru_start_paddr, gru_end_paddr);\r\nret = misc_register(&gru_miscdev);\r\nif (ret) {\r\nprintk(KERN_ERR "%s: misc_register failed\n",\r\nGRU_DRIVER_ID_STR);\r\ngoto exit0;\r\n}\r\nret = gru_proc_init();\r\nif (ret) {\r\nprintk(KERN_ERR "%s: proc init failed\n", GRU_DRIVER_ID_STR);\r\ngoto exit1;\r\n}\r\nret = gru_init_tables(gru_start_paddr, gru_start_vaddr);\r\nif (ret) {\r\nprintk(KERN_ERR "%s: init tables failed\n", GRU_DRIVER_ID_STR);\r\ngoto exit2;\r\n}\r\nret = gru_setup_tlb_irqs();\r\nif (ret != 0)\r\ngoto exit3;\r\ngru_kservices_init();\r\nprintk(KERN_INFO "%s: v%s\n", GRU_DRIVER_ID_STR,\r\nGRU_DRIVER_VERSION_STR);\r\nreturn 0;\r\nexit3:\r\ngru_free_tables();\r\nexit2:\r\ngru_proc_exit();\r\nexit1:\r\nmisc_deregister(&gru_miscdev);\r\nexit0:\r\nreturn ret;\r\n}\r\nstatic void __exit gru_exit(void)\r\n{\r\nif (!is_uv_system())\r\nreturn;\r\ngru_teardown_tlb_irqs();\r\ngru_kservices_exit();\r\ngru_free_tables();\r\nmisc_deregister(&gru_miscdev);\r\ngru_proc_exit();\r\n}
