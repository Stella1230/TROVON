static int raid6_have_avx2(void)\r\n{\r\nreturn boot_cpu_has(X86_FEATURE_AVX2) && boot_cpu_has(X86_FEATURE_AVX);\r\n}\r\nstatic void raid6_avx21_gen_syndrome(int disks, size_t bytes, void **ptrs)\r\n{\r\nu8 **dptr = (u8 **)ptrs;\r\nu8 *p, *q;\r\nint d, z, z0;\r\nz0 = disks - 3;\r\np = dptr[z0+1];\r\nq = dptr[z0+2];\r\nkernel_fpu_begin();\r\nasm volatile("vmovdqa %0,%%ymm0" : : "m" (raid6_avx2_constants.x1d[0]));\r\nasm volatile("vpxor %ymm3,%ymm3,%ymm3");\r\nfor (d = 0; d < bytes; d += 32) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d]));\r\nasm volatile("vmovdqa %0,%%ymm2" : : "m" (dptr[z0][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0-1][d]));\r\nasm volatile("vmovdqa %ymm2,%ymm4");\r\nasm volatile("vmovdqa %0,%%ymm6" : : "m" (dptr[z0-1][d]));\r\nfor (z = z0-2; z >= 0; z--) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("vpcmpgtb %ymm4,%ymm3,%ymm5");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm6,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm6,%ymm4,%ymm4");\r\nasm volatile("vmovdqa %0,%%ymm6" : : "m" (dptr[z][d]));\r\n}\r\nasm volatile("vpcmpgtb %ymm4,%ymm3,%ymm5");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm6,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm6,%ymm4,%ymm4");\r\nasm volatile("vmovntdq %%ymm2,%0" : "=m" (p[d]));\r\nasm volatile("vpxor %ymm2,%ymm2,%ymm2");\r\nasm volatile("vmovntdq %%ymm4,%0" : "=m" (q[d]));\r\nasm volatile("vpxor %ymm4,%ymm4,%ymm4");\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nkernel_fpu_end();\r\n}\r\nstatic void raid6_avx22_gen_syndrome(int disks, size_t bytes, void **ptrs)\r\n{\r\nu8 **dptr = (u8 **)ptrs;\r\nu8 *p, *q;\r\nint d, z, z0;\r\nz0 = disks - 3;\r\np = dptr[z0+1];\r\nq = dptr[z0+2];\r\nkernel_fpu_begin();\r\nasm volatile("vmovdqa %0,%%ymm0" : : "m" (raid6_avx2_constants.x1d[0]));\r\nasm volatile("vpxor %ymm1,%ymm1,%ymm1");\r\nfor (d = 0; d < bytes; d += 64) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z0][d+32]));\r\nasm volatile("vmovdqa %0,%%ymm2" : : "m" (dptr[z0][d]));\r\nasm volatile("vmovdqa %0,%%ymm3" : : "m" (dptr[z0][d+32]));\r\nasm volatile("vmovdqa %ymm2,%ymm4");\r\nasm volatile("vmovdqa %ymm3,%ymm6");\r\nfor (z = z0-1; z >= 0; z--) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d+32]));\r\nasm volatile("vpcmpgtb %ymm4,%ymm1,%ymm5");\r\nasm volatile("vpcmpgtb %ymm6,%ymm1,%ymm7");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpaddb %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpand %ymm0,%ymm7,%ymm7");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vmovdqa %0,%%ymm5" : : "m" (dptr[z][d]));\r\nasm volatile("vmovdqa %0,%%ymm7" : : "m" (dptr[z][d+32]));\r\nasm volatile("vpxor %ymm5,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm7,%ymm3,%ymm3");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\n}\r\nasm volatile("vmovntdq %%ymm2,%0" : "=m" (p[d]));\r\nasm volatile("vmovntdq %%ymm3,%0" : "=m" (p[d+32]));\r\nasm volatile("vmovntdq %%ymm4,%0" : "=m" (q[d]));\r\nasm volatile("vmovntdq %%ymm6,%0" : "=m" (q[d+32]));\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nkernel_fpu_end();\r\n}\r\nstatic void raid6_avx24_gen_syndrome(int disks, size_t bytes, void **ptrs)\r\n{\r\nu8 **dptr = (u8 **)ptrs;\r\nu8 *p, *q;\r\nint d, z, z0;\r\nz0 = disks - 3;\r\np = dptr[z0+1];\r\nq = dptr[z0+2];\r\nkernel_fpu_begin();\r\nasm volatile("vmovdqa %0,%%ymm0" : : "m" (raid6_avx2_constants.x1d[0]));\r\nasm volatile("vpxor %ymm1,%ymm1,%ymm1");\r\nasm volatile("vpxor %ymm2,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm3,%ymm3,%ymm3");\r\nasm volatile("vpxor %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpxor %ymm10,%ymm10,%ymm10");\r\nasm volatile("vpxor %ymm11,%ymm11,%ymm11");\r\nasm volatile("vpxor %ymm12,%ymm12,%ymm12");\r\nasm volatile("vpxor %ymm14,%ymm14,%ymm14");\r\nfor (d = 0; d < bytes; d += 128) {\r\nfor (z = z0; z >= 0; z--) {\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d+32]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d+64]));\r\nasm volatile("prefetchnta %0" : : "m" (dptr[z][d+96]));\r\nasm volatile("vpcmpgtb %ymm4,%ymm1,%ymm5");\r\nasm volatile("vpcmpgtb %ymm6,%ymm1,%ymm7");\r\nasm volatile("vpcmpgtb %ymm12,%ymm1,%ymm13");\r\nasm volatile("vpcmpgtb %ymm14,%ymm1,%ymm15");\r\nasm volatile("vpaddb %ymm4,%ymm4,%ymm4");\r\nasm volatile("vpaddb %ymm6,%ymm6,%ymm6");\r\nasm volatile("vpaddb %ymm12,%ymm12,%ymm12");\r\nasm volatile("vpaddb %ymm14,%ymm14,%ymm14");\r\nasm volatile("vpand %ymm0,%ymm5,%ymm5");\r\nasm volatile("vpand %ymm0,%ymm7,%ymm7");\r\nasm volatile("vpand %ymm0,%ymm13,%ymm13");\r\nasm volatile("vpand %ymm0,%ymm15,%ymm15");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vpxor %ymm13,%ymm12,%ymm12");\r\nasm volatile("vpxor %ymm15,%ymm14,%ymm14");\r\nasm volatile("vmovdqa %0,%%ymm5" : : "m" (dptr[z][d]));\r\nasm volatile("vmovdqa %0,%%ymm7" : : "m" (dptr[z][d+32]));\r\nasm volatile("vmovdqa %0,%%ymm13" : : "m" (dptr[z][d+64]));\r\nasm volatile("vmovdqa %0,%%ymm15" : : "m" (dptr[z][d+96]));\r\nasm volatile("vpxor %ymm5,%ymm2,%ymm2");\r\nasm volatile("vpxor %ymm7,%ymm3,%ymm3");\r\nasm volatile("vpxor %ymm13,%ymm10,%ymm10");\r\nasm volatile("vpxor %ymm15,%ymm11,%ymm11");\r\nasm volatile("vpxor %ymm5,%ymm4,%ymm4");\r\nasm volatile("vpxor %ymm7,%ymm6,%ymm6");\r\nasm volatile("vpxor %ymm13,%ymm12,%ymm12");\r\nasm volatile("vpxor %ymm15,%ymm14,%ymm14");\r\n}\r\nasm volatile("vmovntdq %%ymm2,%0" : "=m" (p[d]));\r\nasm volatile("vpxor %ymm2,%ymm2,%ymm2");\r\nasm volatile("vmovntdq %%ymm3,%0" : "=m" (p[d+32]));\r\nasm volatile("vpxor %ymm3,%ymm3,%ymm3");\r\nasm volatile("vmovntdq %%ymm10,%0" : "=m" (p[d+64]));\r\nasm volatile("vpxor %ymm10,%ymm10,%ymm10");\r\nasm volatile("vmovntdq %%ymm11,%0" : "=m" (p[d+96]));\r\nasm volatile("vpxor %ymm11,%ymm11,%ymm11");\r\nasm volatile("vmovntdq %%ymm4,%0" : "=m" (q[d]));\r\nasm volatile("vpxor %ymm4,%ymm4,%ymm4");\r\nasm volatile("vmovntdq %%ymm6,%0" : "=m" (q[d+32]));\r\nasm volatile("vpxor %ymm6,%ymm6,%ymm6");\r\nasm volatile("vmovntdq %%ymm12,%0" : "=m" (q[d+64]));\r\nasm volatile("vpxor %ymm12,%ymm12,%ymm12");\r\nasm volatile("vmovntdq %%ymm14,%0" : "=m" (q[d+96]));\r\nasm volatile("vpxor %ymm14,%ymm14,%ymm14");\r\n}\r\nasm volatile("sfence" : : : "memory");\r\nkernel_fpu_end();\r\n}
