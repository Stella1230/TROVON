static void bio_batch_end_io(struct bio *bio, int err)\r\n{\r\nstruct bio_batch *bb = bio->bi_private;\r\nif (err && (err != -EOPNOTSUPP))\r\nclear_bit(BIO_UPTODATE, &bb->flags);\r\nif (atomic_dec_and_test(&bb->done))\r\ncomplete(bb->wait);\r\nbio_put(bio);\r\n}\r\nint blkdev_issue_discard(struct block_device *bdev, sector_t sector,\r\nsector_t nr_sects, gfp_t gfp_mask, unsigned long flags)\r\n{\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nstruct request_queue *q = bdev_get_queue(bdev);\r\nint type = REQ_WRITE | REQ_DISCARD;\r\nunsigned int max_discard_sectors, granularity;\r\nint alignment;\r\nstruct bio_batch bb;\r\nstruct bio *bio;\r\nint ret = 0;\r\nstruct blk_plug plug;\r\nif (!q)\r\nreturn -ENXIO;\r\nif (!blk_queue_discard(q))\r\nreturn -EOPNOTSUPP;\r\ngranularity = max(q->limits.discard_granularity >> 9, 1U);\r\nalignment = (bdev_discard_alignment(bdev) >> 9) % granularity;\r\nmax_discard_sectors = min(q->limits.max_discard_sectors, UINT_MAX >> 9);\r\nmax_discard_sectors -= max_discard_sectors % granularity;\r\nif (unlikely(!max_discard_sectors)) {\r\nreturn -EOPNOTSUPP;\r\n}\r\nif (flags & BLKDEV_DISCARD_SECURE) {\r\nif (!blk_queue_secdiscard(q))\r\nreturn -EOPNOTSUPP;\r\ntype |= REQ_SECURE;\r\n}\r\natomic_set(&bb.done, 1);\r\nbb.flags = 1 << BIO_UPTODATE;\r\nbb.wait = &wait;\r\nblk_start_plug(&plug);\r\nwhile (nr_sects) {\r\nunsigned int req_sects;\r\nsector_t end_sect, tmp;\r\nbio = bio_alloc(gfp_mask, 1);\r\nif (!bio) {\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\nreq_sects = min_t(sector_t, nr_sects, max_discard_sectors);\r\nend_sect = sector + req_sects;\r\ntmp = end_sect;\r\nif (req_sects < nr_sects &&\r\nsector_div(tmp, granularity) != alignment) {\r\nend_sect = end_sect - alignment;\r\nsector_div(end_sect, granularity);\r\nend_sect = end_sect * granularity + alignment;\r\nreq_sects = end_sect - sector;\r\n}\r\nbio->bi_iter.bi_sector = sector;\r\nbio->bi_end_io = bio_batch_end_io;\r\nbio->bi_bdev = bdev;\r\nbio->bi_private = &bb;\r\nbio->bi_iter.bi_size = req_sects << 9;\r\nnr_sects -= req_sects;\r\nsector = end_sect;\r\natomic_inc(&bb.done);\r\nsubmit_bio(type, bio);\r\ncond_resched();\r\n}\r\nblk_finish_plug(&plug);\r\nif (!atomic_dec_and_test(&bb.done))\r\nwait_for_completion_io(&wait);\r\nif (!test_bit(BIO_UPTODATE, &bb.flags))\r\nret = -EIO;\r\nreturn ret;\r\n}\r\nint blkdev_issue_write_same(struct block_device *bdev, sector_t sector,\r\nsector_t nr_sects, gfp_t gfp_mask,\r\nstruct page *page)\r\n{\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nstruct request_queue *q = bdev_get_queue(bdev);\r\nunsigned int max_write_same_sectors;\r\nstruct bio_batch bb;\r\nstruct bio *bio;\r\nint ret = 0;\r\nif (!q)\r\nreturn -ENXIO;\r\nmax_write_same_sectors = q->limits.max_write_same_sectors;\r\nif (max_write_same_sectors == 0)\r\nreturn -EOPNOTSUPP;\r\natomic_set(&bb.done, 1);\r\nbb.flags = 1 << BIO_UPTODATE;\r\nbb.wait = &wait;\r\nwhile (nr_sects) {\r\nbio = bio_alloc(gfp_mask, 1);\r\nif (!bio) {\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\nbio->bi_iter.bi_sector = sector;\r\nbio->bi_end_io = bio_batch_end_io;\r\nbio->bi_bdev = bdev;\r\nbio->bi_private = &bb;\r\nbio->bi_vcnt = 1;\r\nbio->bi_io_vec->bv_page = page;\r\nbio->bi_io_vec->bv_offset = 0;\r\nbio->bi_io_vec->bv_len = bdev_logical_block_size(bdev);\r\nif (nr_sects > max_write_same_sectors) {\r\nbio->bi_iter.bi_size = max_write_same_sectors << 9;\r\nnr_sects -= max_write_same_sectors;\r\nsector += max_write_same_sectors;\r\n} else {\r\nbio->bi_iter.bi_size = nr_sects << 9;\r\nnr_sects = 0;\r\n}\r\natomic_inc(&bb.done);\r\nsubmit_bio(REQ_WRITE | REQ_WRITE_SAME, bio);\r\n}\r\nif (!atomic_dec_and_test(&bb.done))\r\nwait_for_completion_io(&wait);\r\nif (!test_bit(BIO_UPTODATE, &bb.flags))\r\nret = -ENOTSUPP;\r\nreturn ret;\r\n}\r\nint __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,\r\nsector_t nr_sects, gfp_t gfp_mask)\r\n{\r\nint ret;\r\nstruct bio *bio;\r\nstruct bio_batch bb;\r\nunsigned int sz;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\natomic_set(&bb.done, 1);\r\nbb.flags = 1 << BIO_UPTODATE;\r\nbb.wait = &wait;\r\nret = 0;\r\nwhile (nr_sects != 0) {\r\nbio = bio_alloc(gfp_mask,\r\nmin(nr_sects, (sector_t)BIO_MAX_PAGES));\r\nif (!bio) {\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\nbio->bi_iter.bi_sector = sector;\r\nbio->bi_bdev = bdev;\r\nbio->bi_end_io = bio_batch_end_io;\r\nbio->bi_private = &bb;\r\nwhile (nr_sects != 0) {\r\nsz = min((sector_t) PAGE_SIZE >> 9 , nr_sects);\r\nret = bio_add_page(bio, ZERO_PAGE(0), sz << 9, 0);\r\nnr_sects -= ret >> 9;\r\nsector += ret >> 9;\r\nif (ret < (sz << 9))\r\nbreak;\r\n}\r\nret = 0;\r\natomic_inc(&bb.done);\r\nsubmit_bio(WRITE, bio);\r\n}\r\nif (!atomic_dec_and_test(&bb.done))\r\nwait_for_completion_io(&wait);\r\nif (!test_bit(BIO_UPTODATE, &bb.flags))\r\nret = -EIO;\r\nreturn ret;\r\n}\r\nint blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,\r\nsector_t nr_sects, gfp_t gfp_mask)\r\n{\r\nif (bdev_write_same(bdev)) {\r\nunsigned char bdn[BDEVNAME_SIZE];\r\nif (!blkdev_issue_write_same(bdev, sector, nr_sects, gfp_mask,\r\nZERO_PAGE(0)))\r\nreturn 0;\r\nbdevname(bdev, bdn);\r\npr_err("%s: WRITE SAME failed. Manually zeroing.\n", bdn);\r\n}\r\nreturn __blkdev_issue_zeroout(bdev, sector, nr_sects, gfp_mask);\r\n}
