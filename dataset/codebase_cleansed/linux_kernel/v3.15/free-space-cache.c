static struct inode *__lookup_free_space_inode(struct btrfs_root *root,\r\nstruct btrfs_path *path,\r\nu64 offset)\r\n{\r\nstruct btrfs_key key;\r\nstruct btrfs_key location;\r\nstruct btrfs_disk_key disk_key;\r\nstruct btrfs_free_space_header *header;\r\nstruct extent_buffer *leaf;\r\nstruct inode *inode = NULL;\r\nint ret;\r\nkey.objectid = BTRFS_FREE_SPACE_OBJECTID;\r\nkey.offset = offset;\r\nkey.type = 0;\r\nret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\r\nif (ret < 0)\r\nreturn ERR_PTR(ret);\r\nif (ret > 0) {\r\nbtrfs_release_path(path);\r\nreturn ERR_PTR(-ENOENT);\r\n}\r\nleaf = path->nodes[0];\r\nheader = btrfs_item_ptr(leaf, path->slots[0],\r\nstruct btrfs_free_space_header);\r\nbtrfs_free_space_key(leaf, header, &disk_key);\r\nbtrfs_disk_key_to_cpu(&location, &disk_key);\r\nbtrfs_release_path(path);\r\ninode = btrfs_iget(root->fs_info->sb, &location, root, NULL);\r\nif (!inode)\r\nreturn ERR_PTR(-ENOENT);\r\nif (IS_ERR(inode))\r\nreturn inode;\r\nif (is_bad_inode(inode)) {\r\niput(inode);\r\nreturn ERR_PTR(-ENOENT);\r\n}\r\nmapping_set_gfp_mask(inode->i_mapping,\r\nmapping_gfp_mask(inode->i_mapping) & ~__GFP_FS);\r\nreturn inode;\r\n}\r\nstruct inode *lookup_free_space_inode(struct btrfs_root *root,\r\nstruct btrfs_block_group_cache\r\n*block_group, struct btrfs_path *path)\r\n{\r\nstruct inode *inode = NULL;\r\nu32 flags = BTRFS_INODE_NODATASUM | BTRFS_INODE_NODATACOW;\r\nspin_lock(&block_group->lock);\r\nif (block_group->inode)\r\ninode = igrab(block_group->inode);\r\nspin_unlock(&block_group->lock);\r\nif (inode)\r\nreturn inode;\r\ninode = __lookup_free_space_inode(root, path,\r\nblock_group->key.objectid);\r\nif (IS_ERR(inode))\r\nreturn inode;\r\nspin_lock(&block_group->lock);\r\nif (!((BTRFS_I(inode)->flags & flags) == flags)) {\r\nbtrfs_info(root->fs_info,\r\n"Old style space inode found, converting.");\r\nBTRFS_I(inode)->flags |= BTRFS_INODE_NODATASUM |\r\nBTRFS_INODE_NODATACOW;\r\nblock_group->disk_cache_state = BTRFS_DC_CLEAR;\r\n}\r\nif (!block_group->iref) {\r\nblock_group->inode = igrab(inode);\r\nblock_group->iref = 1;\r\n}\r\nspin_unlock(&block_group->lock);\r\nreturn inode;\r\n}\r\nstatic int __create_free_space_inode(struct btrfs_root *root,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_path *path,\r\nu64 ino, u64 offset)\r\n{\r\nstruct btrfs_key key;\r\nstruct btrfs_disk_key disk_key;\r\nstruct btrfs_free_space_header *header;\r\nstruct btrfs_inode_item *inode_item;\r\nstruct extent_buffer *leaf;\r\nu64 flags = BTRFS_INODE_NOCOMPRESS | BTRFS_INODE_PREALLOC;\r\nint ret;\r\nret = btrfs_insert_empty_inode(trans, root, path, ino);\r\nif (ret)\r\nreturn ret;\r\nif (ino != BTRFS_FREE_INO_OBJECTID)\r\nflags |= BTRFS_INODE_NODATASUM | BTRFS_INODE_NODATACOW;\r\nleaf = path->nodes[0];\r\ninode_item = btrfs_item_ptr(leaf, path->slots[0],\r\nstruct btrfs_inode_item);\r\nbtrfs_item_key(leaf, &disk_key, path->slots[0]);\r\nmemset_extent_buffer(leaf, 0, (unsigned long)inode_item,\r\nsizeof(*inode_item));\r\nbtrfs_set_inode_generation(leaf, inode_item, trans->transid);\r\nbtrfs_set_inode_size(leaf, inode_item, 0);\r\nbtrfs_set_inode_nbytes(leaf, inode_item, 0);\r\nbtrfs_set_inode_uid(leaf, inode_item, 0);\r\nbtrfs_set_inode_gid(leaf, inode_item, 0);\r\nbtrfs_set_inode_mode(leaf, inode_item, S_IFREG | 0600);\r\nbtrfs_set_inode_flags(leaf, inode_item, flags);\r\nbtrfs_set_inode_nlink(leaf, inode_item, 1);\r\nbtrfs_set_inode_transid(leaf, inode_item, trans->transid);\r\nbtrfs_set_inode_block_group(leaf, inode_item, offset);\r\nbtrfs_mark_buffer_dirty(leaf);\r\nbtrfs_release_path(path);\r\nkey.objectid = BTRFS_FREE_SPACE_OBJECTID;\r\nkey.offset = offset;\r\nkey.type = 0;\r\nret = btrfs_insert_empty_item(trans, root, path, &key,\r\nsizeof(struct btrfs_free_space_header));\r\nif (ret < 0) {\r\nbtrfs_release_path(path);\r\nreturn ret;\r\n}\r\nleaf = path->nodes[0];\r\nheader = btrfs_item_ptr(leaf, path->slots[0],\r\nstruct btrfs_free_space_header);\r\nmemset_extent_buffer(leaf, 0, (unsigned long)header, sizeof(*header));\r\nbtrfs_set_free_space_key(leaf, header, &disk_key);\r\nbtrfs_mark_buffer_dirty(leaf);\r\nbtrfs_release_path(path);\r\nreturn 0;\r\n}\r\nint create_free_space_inode(struct btrfs_root *root,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_block_group_cache *block_group,\r\nstruct btrfs_path *path)\r\n{\r\nint ret;\r\nu64 ino;\r\nret = btrfs_find_free_objectid(root, &ino);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn __create_free_space_inode(root, trans, path, ino,\r\nblock_group->key.objectid);\r\n}\r\nint btrfs_check_trunc_cache_free_space(struct btrfs_root *root,\r\nstruct btrfs_block_rsv *rsv)\r\n{\r\nu64 needed_bytes;\r\nint ret;\r\nneeded_bytes = btrfs_calc_trunc_metadata_size(root, 1) +\r\nbtrfs_calc_trans_metadata_size(root, 1);\r\nspin_lock(&rsv->lock);\r\nif (rsv->reserved < needed_bytes)\r\nret = -ENOSPC;\r\nelse\r\nret = 0;\r\nspin_unlock(&rsv->lock);\r\nreturn ret;\r\n}\r\nint btrfs_truncate_free_space_cache(struct btrfs_root *root,\r\nstruct btrfs_trans_handle *trans,\r\nstruct inode *inode)\r\n{\r\nint ret = 0;\r\nbtrfs_i_size_write(inode, 0);\r\ntruncate_pagecache(inode, 0);\r\nret = btrfs_truncate_inode_items(trans, root, inode,\r\n0, BTRFS_EXTENT_DATA_KEY);\r\nif (ret) {\r\nbtrfs_abort_transaction(trans, root, ret);\r\nreturn ret;\r\n}\r\nret = btrfs_update_inode(trans, root, inode);\r\nif (ret)\r\nbtrfs_abort_transaction(trans, root, ret);\r\nreturn ret;\r\n}\r\nstatic int readahead_cache(struct inode *inode)\r\n{\r\nstruct file_ra_state *ra;\r\nunsigned long last_index;\r\nra = kzalloc(sizeof(*ra), GFP_NOFS);\r\nif (!ra)\r\nreturn -ENOMEM;\r\nfile_ra_state_init(ra, inode->i_mapping);\r\nlast_index = (i_size_read(inode) - 1) >> PAGE_CACHE_SHIFT;\r\npage_cache_sync_readahead(inode->i_mapping, ra, NULL, 0, last_index);\r\nkfree(ra);\r\nreturn 0;\r\n}\r\nstatic int io_ctl_init(struct io_ctl *io_ctl, struct inode *inode,\r\nstruct btrfs_root *root)\r\n{\r\nmemset(io_ctl, 0, sizeof(struct io_ctl));\r\nio_ctl->num_pages = (i_size_read(inode) + PAGE_CACHE_SIZE - 1) >>\r\nPAGE_CACHE_SHIFT;\r\nio_ctl->pages = kzalloc(sizeof(struct page *) * io_ctl->num_pages,\r\nGFP_NOFS);\r\nif (!io_ctl->pages)\r\nreturn -ENOMEM;\r\nio_ctl->root = root;\r\nif (btrfs_ino(inode) != BTRFS_FREE_INO_OBJECTID)\r\nio_ctl->check_crcs = 1;\r\nreturn 0;\r\n}\r\nstatic void io_ctl_free(struct io_ctl *io_ctl)\r\n{\r\nkfree(io_ctl->pages);\r\n}\r\nstatic void io_ctl_unmap_page(struct io_ctl *io_ctl)\r\n{\r\nif (io_ctl->cur) {\r\nkunmap(io_ctl->page);\r\nio_ctl->cur = NULL;\r\nio_ctl->orig = NULL;\r\n}\r\n}\r\nstatic void io_ctl_map_page(struct io_ctl *io_ctl, int clear)\r\n{\r\nASSERT(io_ctl->index < io_ctl->num_pages);\r\nio_ctl->page = io_ctl->pages[io_ctl->index++];\r\nio_ctl->cur = kmap(io_ctl->page);\r\nio_ctl->orig = io_ctl->cur;\r\nio_ctl->size = PAGE_CACHE_SIZE;\r\nif (clear)\r\nmemset(io_ctl->cur, 0, PAGE_CACHE_SIZE);\r\n}\r\nstatic void io_ctl_drop_pages(struct io_ctl *io_ctl)\r\n{\r\nint i;\r\nio_ctl_unmap_page(io_ctl);\r\nfor (i = 0; i < io_ctl->num_pages; i++) {\r\nif (io_ctl->pages[i]) {\r\nClearPageChecked(io_ctl->pages[i]);\r\nunlock_page(io_ctl->pages[i]);\r\npage_cache_release(io_ctl->pages[i]);\r\n}\r\n}\r\n}\r\nstatic int io_ctl_prepare_pages(struct io_ctl *io_ctl, struct inode *inode,\r\nint uptodate)\r\n{\r\nstruct page *page;\r\ngfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);\r\nint i;\r\nfor (i = 0; i < io_ctl->num_pages; i++) {\r\npage = find_or_create_page(inode->i_mapping, i, mask);\r\nif (!page) {\r\nio_ctl_drop_pages(io_ctl);\r\nreturn -ENOMEM;\r\n}\r\nio_ctl->pages[i] = page;\r\nif (uptodate && !PageUptodate(page)) {\r\nbtrfs_readpage(NULL, page);\r\nlock_page(page);\r\nif (!PageUptodate(page)) {\r\nbtrfs_err(BTRFS_I(inode)->root->fs_info,\r\n"error reading free space cache");\r\nio_ctl_drop_pages(io_ctl);\r\nreturn -EIO;\r\n}\r\n}\r\n}\r\nfor (i = 0; i < io_ctl->num_pages; i++) {\r\nclear_page_dirty_for_io(io_ctl->pages[i]);\r\nset_page_extent_mapped(io_ctl->pages[i]);\r\n}\r\nreturn 0;\r\n}\r\nstatic void io_ctl_set_generation(struct io_ctl *io_ctl, u64 generation)\r\n{\r\n__le64 *val;\r\nio_ctl_map_page(io_ctl, 1);\r\nif (io_ctl->check_crcs) {\r\nio_ctl->cur += (sizeof(u32) * io_ctl->num_pages);\r\nio_ctl->size -= sizeof(u64) + (sizeof(u32) * io_ctl->num_pages);\r\n} else {\r\nio_ctl->cur += sizeof(u64);\r\nio_ctl->size -= sizeof(u64) * 2;\r\n}\r\nval = io_ctl->cur;\r\n*val = cpu_to_le64(generation);\r\nio_ctl->cur += sizeof(u64);\r\n}\r\nstatic int io_ctl_check_generation(struct io_ctl *io_ctl, u64 generation)\r\n{\r\n__le64 *gen;\r\nif (io_ctl->check_crcs) {\r\nio_ctl->cur += sizeof(u32) * io_ctl->num_pages;\r\nio_ctl->size -= sizeof(u64) +\r\n(sizeof(u32) * io_ctl->num_pages);\r\n} else {\r\nio_ctl->cur += sizeof(u64);\r\nio_ctl->size -= sizeof(u64) * 2;\r\n}\r\ngen = io_ctl->cur;\r\nif (le64_to_cpu(*gen) != generation) {\r\nprintk_ratelimited(KERN_ERR "BTRFS: space cache generation "\r\n"(%Lu) does not match inode (%Lu)\n", *gen,\r\ngeneration);\r\nio_ctl_unmap_page(io_ctl);\r\nreturn -EIO;\r\n}\r\nio_ctl->cur += sizeof(u64);\r\nreturn 0;\r\n}\r\nstatic void io_ctl_set_crc(struct io_ctl *io_ctl, int index)\r\n{\r\nu32 *tmp;\r\nu32 crc = ~(u32)0;\r\nunsigned offset = 0;\r\nif (!io_ctl->check_crcs) {\r\nio_ctl_unmap_page(io_ctl);\r\nreturn;\r\n}\r\nif (index == 0)\r\noffset = sizeof(u32) * io_ctl->num_pages;\r\ncrc = btrfs_csum_data(io_ctl->orig + offset, crc,\r\nPAGE_CACHE_SIZE - offset);\r\nbtrfs_csum_final(crc, (char *)&crc);\r\nio_ctl_unmap_page(io_ctl);\r\ntmp = kmap(io_ctl->pages[0]);\r\ntmp += index;\r\n*tmp = crc;\r\nkunmap(io_ctl->pages[0]);\r\n}\r\nstatic int io_ctl_check_crc(struct io_ctl *io_ctl, int index)\r\n{\r\nu32 *tmp, val;\r\nu32 crc = ~(u32)0;\r\nunsigned offset = 0;\r\nif (!io_ctl->check_crcs) {\r\nio_ctl_map_page(io_ctl, 0);\r\nreturn 0;\r\n}\r\nif (index == 0)\r\noffset = sizeof(u32) * io_ctl->num_pages;\r\ntmp = kmap(io_ctl->pages[0]);\r\ntmp += index;\r\nval = *tmp;\r\nkunmap(io_ctl->pages[0]);\r\nio_ctl_map_page(io_ctl, 0);\r\ncrc = btrfs_csum_data(io_ctl->orig + offset, crc,\r\nPAGE_CACHE_SIZE - offset);\r\nbtrfs_csum_final(crc, (char *)&crc);\r\nif (val != crc) {\r\nprintk_ratelimited(KERN_ERR "BTRFS: csum mismatch on free "\r\n"space cache\n");\r\nio_ctl_unmap_page(io_ctl);\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int io_ctl_add_entry(struct io_ctl *io_ctl, u64 offset, u64 bytes,\r\nvoid *bitmap)\r\n{\r\nstruct btrfs_free_space_entry *entry;\r\nif (!io_ctl->cur)\r\nreturn -ENOSPC;\r\nentry = io_ctl->cur;\r\nentry->offset = cpu_to_le64(offset);\r\nentry->bytes = cpu_to_le64(bytes);\r\nentry->type = (bitmap) ? BTRFS_FREE_SPACE_BITMAP :\r\nBTRFS_FREE_SPACE_EXTENT;\r\nio_ctl->cur += sizeof(struct btrfs_free_space_entry);\r\nio_ctl->size -= sizeof(struct btrfs_free_space_entry);\r\nif (io_ctl->size >= sizeof(struct btrfs_free_space_entry))\r\nreturn 0;\r\nio_ctl_set_crc(io_ctl, io_ctl->index - 1);\r\nif (io_ctl->index >= io_ctl->num_pages)\r\nreturn 0;\r\nio_ctl_map_page(io_ctl, 1);\r\nreturn 0;\r\n}\r\nstatic int io_ctl_add_bitmap(struct io_ctl *io_ctl, void *bitmap)\r\n{\r\nif (!io_ctl->cur)\r\nreturn -ENOSPC;\r\nif (io_ctl->cur != io_ctl->orig) {\r\nio_ctl_set_crc(io_ctl, io_ctl->index - 1);\r\nif (io_ctl->index >= io_ctl->num_pages)\r\nreturn -ENOSPC;\r\nio_ctl_map_page(io_ctl, 0);\r\n}\r\nmemcpy(io_ctl->cur, bitmap, PAGE_CACHE_SIZE);\r\nio_ctl_set_crc(io_ctl, io_ctl->index - 1);\r\nif (io_ctl->index < io_ctl->num_pages)\r\nio_ctl_map_page(io_ctl, 0);\r\nreturn 0;\r\n}\r\nstatic void io_ctl_zero_remaining_pages(struct io_ctl *io_ctl)\r\n{\r\nif (io_ctl->cur != io_ctl->orig)\r\nio_ctl_set_crc(io_ctl, io_ctl->index - 1);\r\nelse\r\nio_ctl_unmap_page(io_ctl);\r\nwhile (io_ctl->index < io_ctl->num_pages) {\r\nio_ctl_map_page(io_ctl, 1);\r\nio_ctl_set_crc(io_ctl, io_ctl->index - 1);\r\n}\r\n}\r\nstatic int io_ctl_read_entry(struct io_ctl *io_ctl,\r\nstruct btrfs_free_space *entry, u8 *type)\r\n{\r\nstruct btrfs_free_space_entry *e;\r\nint ret;\r\nif (!io_ctl->cur) {\r\nret = io_ctl_check_crc(io_ctl, io_ctl->index);\r\nif (ret)\r\nreturn ret;\r\n}\r\ne = io_ctl->cur;\r\nentry->offset = le64_to_cpu(e->offset);\r\nentry->bytes = le64_to_cpu(e->bytes);\r\n*type = e->type;\r\nio_ctl->cur += sizeof(struct btrfs_free_space_entry);\r\nio_ctl->size -= sizeof(struct btrfs_free_space_entry);\r\nif (io_ctl->size >= sizeof(struct btrfs_free_space_entry))\r\nreturn 0;\r\nio_ctl_unmap_page(io_ctl);\r\nreturn 0;\r\n}\r\nstatic int io_ctl_read_bitmap(struct io_ctl *io_ctl,\r\nstruct btrfs_free_space *entry)\r\n{\r\nint ret;\r\nret = io_ctl_check_crc(io_ctl, io_ctl->index);\r\nif (ret)\r\nreturn ret;\r\nmemcpy(entry->bitmap, io_ctl->cur, PAGE_CACHE_SIZE);\r\nio_ctl_unmap_page(io_ctl);\r\nreturn 0;\r\n}\r\nstatic void merge_space_tree(struct btrfs_free_space_ctl *ctl)\r\n{\r\nstruct btrfs_free_space *e, *prev = NULL;\r\nstruct rb_node *n;\r\nagain:\r\nspin_lock(&ctl->tree_lock);\r\nfor (n = rb_first(&ctl->free_space_offset); n; n = rb_next(n)) {\r\ne = rb_entry(n, struct btrfs_free_space, offset_index);\r\nif (!prev)\r\ngoto next;\r\nif (e->bitmap || prev->bitmap)\r\ngoto next;\r\nif (prev->offset + prev->bytes == e->offset) {\r\nunlink_free_space(ctl, prev);\r\nunlink_free_space(ctl, e);\r\nprev->bytes += e->bytes;\r\nkmem_cache_free(btrfs_free_space_cachep, e);\r\nlink_free_space(ctl, prev);\r\nprev = NULL;\r\nspin_unlock(&ctl->tree_lock);\r\ngoto again;\r\n}\r\nnext:\r\nprev = e;\r\n}\r\nspin_unlock(&ctl->tree_lock);\r\n}\r\nstatic int __load_free_space_cache(struct btrfs_root *root, struct inode *inode,\r\nstruct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_path *path, u64 offset)\r\n{\r\nstruct btrfs_free_space_header *header;\r\nstruct extent_buffer *leaf;\r\nstruct io_ctl io_ctl;\r\nstruct btrfs_key key;\r\nstruct btrfs_free_space *e, *n;\r\nstruct list_head bitmaps;\r\nu64 num_entries;\r\nu64 num_bitmaps;\r\nu64 generation;\r\nu8 type;\r\nint ret = 0;\r\nINIT_LIST_HEAD(&bitmaps);\r\nif (!i_size_read(inode))\r\nreturn 0;\r\nkey.objectid = BTRFS_FREE_SPACE_OBJECTID;\r\nkey.offset = offset;\r\nkey.type = 0;\r\nret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\r\nif (ret < 0)\r\nreturn 0;\r\nelse if (ret > 0) {\r\nbtrfs_release_path(path);\r\nreturn 0;\r\n}\r\nret = -1;\r\nleaf = path->nodes[0];\r\nheader = btrfs_item_ptr(leaf, path->slots[0],\r\nstruct btrfs_free_space_header);\r\nnum_entries = btrfs_free_space_entries(leaf, header);\r\nnum_bitmaps = btrfs_free_space_bitmaps(leaf, header);\r\ngeneration = btrfs_free_space_generation(leaf, header);\r\nbtrfs_release_path(path);\r\nif (BTRFS_I(inode)->generation != generation) {\r\nbtrfs_err(root->fs_info,\r\n"free space inode generation (%llu) "\r\n"did not match free space cache generation (%llu)",\r\nBTRFS_I(inode)->generation, generation);\r\nreturn 0;\r\n}\r\nif (!num_entries)\r\nreturn 0;\r\nret = io_ctl_init(&io_ctl, inode, root);\r\nif (ret)\r\nreturn ret;\r\nret = readahead_cache(inode);\r\nif (ret)\r\ngoto out;\r\nret = io_ctl_prepare_pages(&io_ctl, inode, 1);\r\nif (ret)\r\ngoto out;\r\nret = io_ctl_check_crc(&io_ctl, 0);\r\nif (ret)\r\ngoto free_cache;\r\nret = io_ctl_check_generation(&io_ctl, generation);\r\nif (ret)\r\ngoto free_cache;\r\nwhile (num_entries) {\r\ne = kmem_cache_zalloc(btrfs_free_space_cachep,\r\nGFP_NOFS);\r\nif (!e)\r\ngoto free_cache;\r\nret = io_ctl_read_entry(&io_ctl, e, &type);\r\nif (ret) {\r\nkmem_cache_free(btrfs_free_space_cachep, e);\r\ngoto free_cache;\r\n}\r\nif (!e->bytes) {\r\nkmem_cache_free(btrfs_free_space_cachep, e);\r\ngoto free_cache;\r\n}\r\nif (type == BTRFS_FREE_SPACE_EXTENT) {\r\nspin_lock(&ctl->tree_lock);\r\nret = link_free_space(ctl, e);\r\nspin_unlock(&ctl->tree_lock);\r\nif (ret) {\r\nbtrfs_err(root->fs_info,\r\n"Duplicate entries in free space cache, dumping");\r\nkmem_cache_free(btrfs_free_space_cachep, e);\r\ngoto free_cache;\r\n}\r\n} else {\r\nASSERT(num_bitmaps);\r\nnum_bitmaps--;\r\ne->bitmap = kzalloc(PAGE_CACHE_SIZE, GFP_NOFS);\r\nif (!e->bitmap) {\r\nkmem_cache_free(\r\nbtrfs_free_space_cachep, e);\r\ngoto free_cache;\r\n}\r\nspin_lock(&ctl->tree_lock);\r\nret = link_free_space(ctl, e);\r\nctl->total_bitmaps++;\r\nctl->op->recalc_thresholds(ctl);\r\nspin_unlock(&ctl->tree_lock);\r\nif (ret) {\r\nbtrfs_err(root->fs_info,\r\n"Duplicate entries in free space cache, dumping");\r\nkmem_cache_free(btrfs_free_space_cachep, e);\r\ngoto free_cache;\r\n}\r\nlist_add_tail(&e->list, &bitmaps);\r\n}\r\nnum_entries--;\r\n}\r\nio_ctl_unmap_page(&io_ctl);\r\nlist_for_each_entry_safe(e, n, &bitmaps, list) {\r\nlist_del_init(&e->list);\r\nret = io_ctl_read_bitmap(&io_ctl, e);\r\nif (ret)\r\ngoto free_cache;\r\n}\r\nio_ctl_drop_pages(&io_ctl);\r\nmerge_space_tree(ctl);\r\nret = 1;\r\nout:\r\nio_ctl_free(&io_ctl);\r\nreturn ret;\r\nfree_cache:\r\nio_ctl_drop_pages(&io_ctl);\r\n__btrfs_remove_free_space_cache(ctl);\r\ngoto out;\r\n}\r\nint load_free_space_cache(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_block_group_cache *block_group)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_root *root = fs_info->tree_root;\r\nstruct inode *inode;\r\nstruct btrfs_path *path;\r\nint ret = 0;\r\nbool matched;\r\nu64 used = btrfs_block_group_used(&block_group->item);\r\nspin_lock(&block_group->lock);\r\nif (block_group->disk_cache_state != BTRFS_DC_WRITTEN) {\r\nspin_unlock(&block_group->lock);\r\nreturn 0;\r\n}\r\nspin_unlock(&block_group->lock);\r\npath = btrfs_alloc_path();\r\nif (!path)\r\nreturn 0;\r\npath->search_commit_root = 1;\r\npath->skip_locking = 1;\r\ninode = lookup_free_space_inode(root, block_group, path);\r\nif (IS_ERR(inode)) {\r\nbtrfs_free_path(path);\r\nreturn 0;\r\n}\r\nspin_lock(&block_group->lock);\r\nif (block_group->disk_cache_state != BTRFS_DC_WRITTEN) {\r\nspin_unlock(&block_group->lock);\r\nbtrfs_free_path(path);\r\ngoto out;\r\n}\r\nspin_unlock(&block_group->lock);\r\nret = __load_free_space_cache(fs_info->tree_root, inode, ctl,\r\npath, block_group->key.objectid);\r\nbtrfs_free_path(path);\r\nif (ret <= 0)\r\ngoto out;\r\nspin_lock(&ctl->tree_lock);\r\nmatched = (ctl->free_space == (block_group->key.offset - used -\r\nblock_group->bytes_super));\r\nspin_unlock(&ctl->tree_lock);\r\nif (!matched) {\r\n__btrfs_remove_free_space_cache(ctl);\r\nbtrfs_err(fs_info, "block group %llu has wrong amount of free space",\r\nblock_group->key.objectid);\r\nret = -1;\r\n}\r\nout:\r\nif (ret < 0) {\r\nspin_lock(&block_group->lock);\r\nblock_group->disk_cache_state = BTRFS_DC_CLEAR;\r\nspin_unlock(&block_group->lock);\r\nret = 0;\r\nbtrfs_err(fs_info, "failed to load free space cache for block group %llu",\r\nblock_group->key.objectid);\r\n}\r\niput(inode);\r\nreturn ret;\r\n}\r\nstatic int __btrfs_write_out_cache(struct btrfs_root *root, struct inode *inode,\r\nstruct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_block_group_cache *block_group,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_path *path, u64 offset)\r\n{\r\nstruct btrfs_free_space_header *header;\r\nstruct extent_buffer *leaf;\r\nstruct rb_node *node;\r\nstruct list_head *pos, *n;\r\nstruct extent_state *cached_state = NULL;\r\nstruct btrfs_free_cluster *cluster = NULL;\r\nstruct extent_io_tree *unpin = NULL;\r\nstruct io_ctl io_ctl;\r\nstruct list_head bitmap_list;\r\nstruct btrfs_key key;\r\nu64 start, extent_start, extent_end, len;\r\nint entries = 0;\r\nint bitmaps = 0;\r\nint ret;\r\nint err = -1;\r\nINIT_LIST_HEAD(&bitmap_list);\r\nif (!i_size_read(inode))\r\nreturn -1;\r\nret = io_ctl_init(&io_ctl, inode, root);\r\nif (ret)\r\nreturn -1;\r\nif (block_group && !list_empty(&block_group->cluster_list))\r\ncluster = list_entry(block_group->cluster_list.next,\r\nstruct btrfs_free_cluster,\r\nblock_group_list);\r\nio_ctl_prepare_pages(&io_ctl, inode, 0);\r\nlock_extent_bits(&BTRFS_I(inode)->io_tree, 0, i_size_read(inode) - 1,\r\n0, &cached_state);\r\nnode = rb_first(&ctl->free_space_offset);\r\nif (!node && cluster) {\r\nnode = rb_first(&cluster->root);\r\ncluster = NULL;\r\n}\r\nif (io_ctl.check_crcs &&\r\n(io_ctl.num_pages * sizeof(u32)) >= PAGE_CACHE_SIZE)\r\ngoto out_nospc;\r\nio_ctl_set_generation(&io_ctl, trans->transid);\r\nwhile (node) {\r\nstruct btrfs_free_space *e;\r\ne = rb_entry(node, struct btrfs_free_space, offset_index);\r\nentries++;\r\nret = io_ctl_add_entry(&io_ctl, e->offset, e->bytes,\r\ne->bitmap);\r\nif (ret)\r\ngoto out_nospc;\r\nif (e->bitmap) {\r\nlist_add_tail(&e->list, &bitmap_list);\r\nbitmaps++;\r\n}\r\nnode = rb_next(node);\r\nif (!node && cluster) {\r\nnode = rb_first(&cluster->root);\r\ncluster = NULL;\r\n}\r\n}\r\nunpin = root->fs_info->pinned_extents;\r\nif (block_group)\r\nstart = block_group->key.objectid;\r\nwhile (block_group && (start < block_group->key.objectid +\r\nblock_group->key.offset)) {\r\nret = find_first_extent_bit(unpin, start,\r\n&extent_start, &extent_end,\r\nEXTENT_DIRTY, NULL);\r\nif (ret) {\r\nret = 0;\r\nbreak;\r\n}\r\nif (extent_start >= block_group->key.objectid +\r\nblock_group->key.offset)\r\nbreak;\r\nextent_start = max(extent_start, start);\r\nextent_end = min(block_group->key.objectid +\r\nblock_group->key.offset, extent_end + 1);\r\nlen = extent_end - extent_start;\r\nentries++;\r\nret = io_ctl_add_entry(&io_ctl, extent_start, len, NULL);\r\nif (ret)\r\ngoto out_nospc;\r\nstart = extent_end;\r\n}\r\nlist_for_each_safe(pos, n, &bitmap_list) {\r\nstruct btrfs_free_space *entry =\r\nlist_entry(pos, struct btrfs_free_space, list);\r\nret = io_ctl_add_bitmap(&io_ctl, entry->bitmap);\r\nif (ret)\r\ngoto out_nospc;\r\nlist_del_init(&entry->list);\r\n}\r\nio_ctl_zero_remaining_pages(&io_ctl);\r\nret = btrfs_dirty_pages(root, inode, io_ctl.pages, io_ctl.num_pages,\r\n0, i_size_read(inode), &cached_state);\r\nio_ctl_drop_pages(&io_ctl);\r\nunlock_extent_cached(&BTRFS_I(inode)->io_tree, 0,\r\ni_size_read(inode) - 1, &cached_state, GFP_NOFS);\r\nif (ret)\r\ngoto out;\r\nret = btrfs_wait_ordered_range(inode, 0, (u64)-1);\r\nif (ret) {\r\nclear_extent_bit(&BTRFS_I(inode)->io_tree, 0, inode->i_size - 1,\r\nEXTENT_DIRTY | EXTENT_DELALLOC, 0, 0, NULL,\r\nGFP_NOFS);\r\ngoto out;\r\n}\r\nkey.objectid = BTRFS_FREE_SPACE_OBJECTID;\r\nkey.offset = offset;\r\nkey.type = 0;\r\nret = btrfs_search_slot(trans, root, &key, path, 0, 1);\r\nif (ret < 0) {\r\nclear_extent_bit(&BTRFS_I(inode)->io_tree, 0, inode->i_size - 1,\r\nEXTENT_DIRTY | EXTENT_DELALLOC, 0, 0, NULL,\r\nGFP_NOFS);\r\ngoto out;\r\n}\r\nleaf = path->nodes[0];\r\nif (ret > 0) {\r\nstruct btrfs_key found_key;\r\nASSERT(path->slots[0]);\r\npath->slots[0]--;\r\nbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\r\nif (found_key.objectid != BTRFS_FREE_SPACE_OBJECTID ||\r\nfound_key.offset != offset) {\r\nclear_extent_bit(&BTRFS_I(inode)->io_tree, 0,\r\ninode->i_size - 1,\r\nEXTENT_DIRTY | EXTENT_DELALLOC, 0, 0,\r\nNULL, GFP_NOFS);\r\nbtrfs_release_path(path);\r\ngoto out;\r\n}\r\n}\r\nBTRFS_I(inode)->generation = trans->transid;\r\nheader = btrfs_item_ptr(leaf, path->slots[0],\r\nstruct btrfs_free_space_header);\r\nbtrfs_set_free_space_entries(leaf, header, entries);\r\nbtrfs_set_free_space_bitmaps(leaf, header, bitmaps);\r\nbtrfs_set_free_space_generation(leaf, header, trans->transid);\r\nbtrfs_mark_buffer_dirty(leaf);\r\nbtrfs_release_path(path);\r\nerr = 0;\r\nout:\r\nio_ctl_free(&io_ctl);\r\nif (err) {\r\ninvalidate_inode_pages2(inode->i_mapping);\r\nBTRFS_I(inode)->generation = 0;\r\n}\r\nbtrfs_update_inode(trans, root, inode);\r\nreturn err;\r\nout_nospc:\r\nlist_for_each_safe(pos, n, &bitmap_list) {\r\nstruct btrfs_free_space *entry =\r\nlist_entry(pos, struct btrfs_free_space, list);\r\nlist_del_init(&entry->list);\r\n}\r\nio_ctl_drop_pages(&io_ctl);\r\nunlock_extent_cached(&BTRFS_I(inode)->io_tree, 0,\r\ni_size_read(inode) - 1, &cached_state, GFP_NOFS);\r\ngoto out;\r\n}\r\nint btrfs_write_out_cache(struct btrfs_root *root,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_block_group_cache *block_group,\r\nstruct btrfs_path *path)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct inode *inode;\r\nint ret = 0;\r\nroot = root->fs_info->tree_root;\r\nspin_lock(&block_group->lock);\r\nif (block_group->disk_cache_state < BTRFS_DC_SETUP) {\r\nspin_unlock(&block_group->lock);\r\nreturn 0;\r\n}\r\nspin_unlock(&block_group->lock);\r\ninode = lookup_free_space_inode(root, block_group, path);\r\nif (IS_ERR(inode))\r\nreturn 0;\r\nret = __btrfs_write_out_cache(root, inode, ctl, block_group, trans,\r\npath, block_group->key.objectid);\r\nif (ret) {\r\nspin_lock(&block_group->lock);\r\nblock_group->disk_cache_state = BTRFS_DC_ERROR;\r\nspin_unlock(&block_group->lock);\r\nret = 0;\r\n#ifdef DEBUG\r\nbtrfs_err(root->fs_info,\r\n"failed to write free space cache for block group %llu",\r\nblock_group->key.objectid);\r\n#endif\r\n}\r\niput(inode);\r\nreturn ret;\r\n}\r\nstatic inline unsigned long offset_to_bit(u64 bitmap_start, u32 unit,\r\nu64 offset)\r\n{\r\nASSERT(offset >= bitmap_start);\r\noffset -= bitmap_start;\r\nreturn (unsigned long)(div_u64(offset, unit));\r\n}\r\nstatic inline unsigned long bytes_to_bits(u64 bytes, u32 unit)\r\n{\r\nreturn (unsigned long)(div_u64(bytes, unit));\r\n}\r\nstatic inline u64 offset_to_bitmap(struct btrfs_free_space_ctl *ctl,\r\nu64 offset)\r\n{\r\nu64 bitmap_start;\r\nu64 bytes_per_bitmap;\r\nbytes_per_bitmap = BITS_PER_BITMAP * ctl->unit;\r\nbitmap_start = offset - ctl->start;\r\nbitmap_start = div64_u64(bitmap_start, bytes_per_bitmap);\r\nbitmap_start *= bytes_per_bitmap;\r\nbitmap_start += ctl->start;\r\nreturn bitmap_start;\r\n}\r\nstatic int tree_insert_offset(struct rb_root *root, u64 offset,\r\nstruct rb_node *node, int bitmap)\r\n{\r\nstruct rb_node **p = &root->rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct btrfs_free_space *info;\r\nwhile (*p) {\r\nparent = *p;\r\ninfo = rb_entry(parent, struct btrfs_free_space, offset_index);\r\nif (offset < info->offset) {\r\np = &(*p)->rb_left;\r\n} else if (offset > info->offset) {\r\np = &(*p)->rb_right;\r\n} else {\r\nif (bitmap) {\r\nif (info->bitmap) {\r\nWARN_ON_ONCE(1);\r\nreturn -EEXIST;\r\n}\r\np = &(*p)->rb_right;\r\n} else {\r\nif (!info->bitmap) {\r\nWARN_ON_ONCE(1);\r\nreturn -EEXIST;\r\n}\r\np = &(*p)->rb_left;\r\n}\r\n}\r\n}\r\nrb_link_node(node, parent, p);\r\nrb_insert_color(node, root);\r\nreturn 0;\r\n}\r\nstatic struct btrfs_free_space *\r\ntree_search_offset(struct btrfs_free_space_ctl *ctl,\r\nu64 offset, int bitmap_only, int fuzzy)\r\n{\r\nstruct rb_node *n = ctl->free_space_offset.rb_node;\r\nstruct btrfs_free_space *entry, *prev = NULL;\r\nwhile (1) {\r\nif (!n) {\r\nentry = NULL;\r\nbreak;\r\n}\r\nentry = rb_entry(n, struct btrfs_free_space, offset_index);\r\nprev = entry;\r\nif (offset < entry->offset)\r\nn = n->rb_left;\r\nelse if (offset > entry->offset)\r\nn = n->rb_right;\r\nelse\r\nbreak;\r\n}\r\nif (bitmap_only) {\r\nif (!entry)\r\nreturn NULL;\r\nif (entry->bitmap)\r\nreturn entry;\r\nn = rb_next(n);\r\nif (!n)\r\nreturn NULL;\r\nentry = rb_entry(n, struct btrfs_free_space, offset_index);\r\nif (entry->offset != offset)\r\nreturn NULL;\r\nWARN_ON(!entry->bitmap);\r\nreturn entry;\r\n} else if (entry) {\r\nif (entry->bitmap) {\r\nn = rb_prev(&entry->offset_index);\r\nif (n) {\r\nprev = rb_entry(n, struct btrfs_free_space,\r\noffset_index);\r\nif (!prev->bitmap &&\r\nprev->offset + prev->bytes > offset)\r\nentry = prev;\r\n}\r\n}\r\nreturn entry;\r\n}\r\nif (!prev)\r\nreturn NULL;\r\nentry = prev;\r\nif (entry->offset > offset) {\r\nn = rb_prev(&entry->offset_index);\r\nif (n) {\r\nentry = rb_entry(n, struct btrfs_free_space,\r\noffset_index);\r\nASSERT(entry->offset <= offset);\r\n} else {\r\nif (fuzzy)\r\nreturn entry;\r\nelse\r\nreturn NULL;\r\n}\r\n}\r\nif (entry->bitmap) {\r\nn = rb_prev(&entry->offset_index);\r\nif (n) {\r\nprev = rb_entry(n, struct btrfs_free_space,\r\noffset_index);\r\nif (!prev->bitmap &&\r\nprev->offset + prev->bytes > offset)\r\nreturn prev;\r\n}\r\nif (entry->offset + BITS_PER_BITMAP * ctl->unit > offset)\r\nreturn entry;\r\n} else if (entry->offset + entry->bytes > offset)\r\nreturn entry;\r\nif (!fuzzy)\r\nreturn NULL;\r\nwhile (1) {\r\nif (entry->bitmap) {\r\nif (entry->offset + BITS_PER_BITMAP *\r\nctl->unit > offset)\r\nbreak;\r\n} else {\r\nif (entry->offset + entry->bytes > offset)\r\nbreak;\r\n}\r\nn = rb_next(&entry->offset_index);\r\nif (!n)\r\nreturn NULL;\r\nentry = rb_entry(n, struct btrfs_free_space, offset_index);\r\n}\r\nreturn entry;\r\n}\r\nstatic inline void\r\n__unlink_free_space(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info)\r\n{\r\nrb_erase(&info->offset_index, &ctl->free_space_offset);\r\nctl->free_extents--;\r\n}\r\nstatic void unlink_free_space(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info)\r\n{\r\n__unlink_free_space(ctl, info);\r\nctl->free_space -= info->bytes;\r\n}\r\nstatic int link_free_space(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info)\r\n{\r\nint ret = 0;\r\nASSERT(info->bytes || info->bitmap);\r\nret = tree_insert_offset(&ctl->free_space_offset, info->offset,\r\n&info->offset_index, (info->bitmap != NULL));\r\nif (ret)\r\nreturn ret;\r\nctl->free_space += info->bytes;\r\nctl->free_extents++;\r\nreturn ret;\r\n}\r\nstatic void recalculate_thresholds(struct btrfs_free_space_ctl *ctl)\r\n{\r\nstruct btrfs_block_group_cache *block_group = ctl->private;\r\nu64 max_bytes;\r\nu64 bitmap_bytes;\r\nu64 extent_bytes;\r\nu64 size = block_group->key.offset;\r\nu64 bytes_per_bg = BITS_PER_BITMAP * ctl->unit;\r\nint max_bitmaps = div64_u64(size + bytes_per_bg - 1, bytes_per_bg);\r\nmax_bitmaps = max(max_bitmaps, 1);\r\nASSERT(ctl->total_bitmaps <= max_bitmaps);\r\nif (size < 1024 * 1024 * 1024)\r\nmax_bytes = MAX_CACHE_BYTES_PER_GIG;\r\nelse\r\nmax_bytes = MAX_CACHE_BYTES_PER_GIG *\r\ndiv64_u64(size, 1024 * 1024 * 1024);\r\nbitmap_bytes = (ctl->total_bitmaps + 1) * PAGE_CACHE_SIZE;\r\nif (bitmap_bytes >= max_bytes) {\r\nctl->extents_thresh = 0;\r\nreturn;\r\n}\r\nextent_bytes = max_bytes - bitmap_bytes;\r\nextent_bytes = min_t(u64, extent_bytes, div64_u64(max_bytes, 2));\r\nctl->extents_thresh =\r\ndiv64_u64(extent_bytes, (sizeof(struct btrfs_free_space)));\r\n}\r\nstatic inline void __bitmap_clear_bits(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info,\r\nu64 offset, u64 bytes)\r\n{\r\nunsigned long start, count;\r\nstart = offset_to_bit(info->offset, ctl->unit, offset);\r\ncount = bytes_to_bits(bytes, ctl->unit);\r\nASSERT(start + count <= BITS_PER_BITMAP);\r\nbitmap_clear(info->bitmap, start, count);\r\ninfo->bytes -= bytes;\r\n}\r\nstatic void bitmap_clear_bits(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info, u64 offset,\r\nu64 bytes)\r\n{\r\n__bitmap_clear_bits(ctl, info, offset, bytes);\r\nctl->free_space -= bytes;\r\n}\r\nstatic void bitmap_set_bits(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info, u64 offset,\r\nu64 bytes)\r\n{\r\nunsigned long start, count;\r\nstart = offset_to_bit(info->offset, ctl->unit, offset);\r\ncount = bytes_to_bits(bytes, ctl->unit);\r\nASSERT(start + count <= BITS_PER_BITMAP);\r\nbitmap_set(info->bitmap, start, count);\r\ninfo->bytes += bytes;\r\nctl->free_space += bytes;\r\n}\r\nstatic int search_bitmap(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *bitmap_info, u64 *offset,\r\nu64 *bytes)\r\n{\r\nunsigned long found_bits = 0;\r\nunsigned long max_bits = 0;\r\nunsigned long bits, i;\r\nunsigned long next_zero;\r\nunsigned long extent_bits;\r\ni = offset_to_bit(bitmap_info->offset, ctl->unit,\r\nmax_t(u64, *offset, bitmap_info->offset));\r\nbits = bytes_to_bits(*bytes, ctl->unit);\r\nfor_each_set_bit_from(i, bitmap_info->bitmap, BITS_PER_BITMAP) {\r\nnext_zero = find_next_zero_bit(bitmap_info->bitmap,\r\nBITS_PER_BITMAP, i);\r\nextent_bits = next_zero - i;\r\nif (extent_bits >= bits) {\r\nfound_bits = extent_bits;\r\nbreak;\r\n} else if (extent_bits > max_bits) {\r\nmax_bits = extent_bits;\r\n}\r\ni = next_zero;\r\n}\r\nif (found_bits) {\r\n*offset = (u64)(i * ctl->unit) + bitmap_info->offset;\r\n*bytes = (u64)(found_bits) * ctl->unit;\r\nreturn 0;\r\n}\r\n*bytes = (u64)(max_bits) * ctl->unit;\r\nreturn -1;\r\n}\r\nstatic struct btrfs_free_space *\r\nfind_free_space(struct btrfs_free_space_ctl *ctl, u64 *offset, u64 *bytes,\r\nunsigned long align, u64 *max_extent_size)\r\n{\r\nstruct btrfs_free_space *entry;\r\nstruct rb_node *node;\r\nu64 tmp;\r\nu64 align_off;\r\nint ret;\r\nif (!ctl->free_space_offset.rb_node)\r\ngoto out;\r\nentry = tree_search_offset(ctl, offset_to_bitmap(ctl, *offset), 0, 1);\r\nif (!entry)\r\ngoto out;\r\nfor (node = &entry->offset_index; node; node = rb_next(node)) {\r\nentry = rb_entry(node, struct btrfs_free_space, offset_index);\r\nif (entry->bytes < *bytes) {\r\nif (entry->bytes > *max_extent_size)\r\n*max_extent_size = entry->bytes;\r\ncontinue;\r\n}\r\nif (*bytes >= align) {\r\ntmp = entry->offset - ctl->start + align - 1;\r\ndo_div(tmp, align);\r\ntmp = tmp * align + ctl->start;\r\nalign_off = tmp - entry->offset;\r\n} else {\r\nalign_off = 0;\r\ntmp = entry->offset;\r\n}\r\nif (entry->bytes < *bytes + align_off) {\r\nif (entry->bytes > *max_extent_size)\r\n*max_extent_size = entry->bytes;\r\ncontinue;\r\n}\r\nif (entry->bitmap) {\r\nu64 size = *bytes;\r\nret = search_bitmap(ctl, entry, &tmp, &size);\r\nif (!ret) {\r\n*offset = tmp;\r\n*bytes = size;\r\nreturn entry;\r\n} else if (size > *max_extent_size) {\r\n*max_extent_size = size;\r\n}\r\ncontinue;\r\n}\r\n*offset = tmp;\r\n*bytes = entry->bytes - align_off;\r\nreturn entry;\r\n}\r\nout:\r\nreturn NULL;\r\n}\r\nstatic void add_new_bitmap(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info, u64 offset)\r\n{\r\ninfo->offset = offset_to_bitmap(ctl, offset);\r\ninfo->bytes = 0;\r\nINIT_LIST_HEAD(&info->list);\r\nlink_free_space(ctl, info);\r\nctl->total_bitmaps++;\r\nctl->op->recalc_thresholds(ctl);\r\n}\r\nstatic void free_bitmap(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *bitmap_info)\r\n{\r\nunlink_free_space(ctl, bitmap_info);\r\nkfree(bitmap_info->bitmap);\r\nkmem_cache_free(btrfs_free_space_cachep, bitmap_info);\r\nctl->total_bitmaps--;\r\nctl->op->recalc_thresholds(ctl);\r\n}\r\nstatic noinline int remove_from_bitmap(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *bitmap_info,\r\nu64 *offset, u64 *bytes)\r\n{\r\nu64 end;\r\nu64 search_start, search_bytes;\r\nint ret;\r\nagain:\r\nend = bitmap_info->offset + (u64)(BITS_PER_BITMAP * ctl->unit) - 1;\r\nsearch_start = *offset;\r\nsearch_bytes = ctl->unit;\r\nsearch_bytes = min(search_bytes, end - search_start + 1);\r\nret = search_bitmap(ctl, bitmap_info, &search_start, &search_bytes);\r\nif (ret < 0 || search_start != *offset)\r\nreturn -EINVAL;\r\nsearch_bytes = min(search_bytes, *bytes);\r\nsearch_bytes = min(search_bytes, end - search_start + 1);\r\nbitmap_clear_bits(ctl, bitmap_info, search_start, search_bytes);\r\n*offset += search_bytes;\r\n*bytes -= search_bytes;\r\nif (*bytes) {\r\nstruct rb_node *next = rb_next(&bitmap_info->offset_index);\r\nif (!bitmap_info->bytes)\r\nfree_bitmap(ctl, bitmap_info);\r\nif (!next)\r\nreturn -EINVAL;\r\nbitmap_info = rb_entry(next, struct btrfs_free_space,\r\noffset_index);\r\nif (!bitmap_info->bitmap)\r\nreturn -EAGAIN;\r\nsearch_start = *offset;\r\nsearch_bytes = ctl->unit;\r\nret = search_bitmap(ctl, bitmap_info, &search_start,\r\n&search_bytes);\r\nif (ret < 0 || search_start != *offset)\r\nreturn -EAGAIN;\r\ngoto again;\r\n} else if (!bitmap_info->bytes)\r\nfree_bitmap(ctl, bitmap_info);\r\nreturn 0;\r\n}\r\nstatic u64 add_bytes_to_bitmap(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info, u64 offset,\r\nu64 bytes)\r\n{\r\nu64 bytes_to_set = 0;\r\nu64 end;\r\nend = info->offset + (u64)(BITS_PER_BITMAP * ctl->unit);\r\nbytes_to_set = min(end - offset, bytes);\r\nbitmap_set_bits(ctl, info, offset, bytes_to_set);\r\nreturn bytes_to_set;\r\n}\r\nstatic bool use_bitmap(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info)\r\n{\r\nstruct btrfs_block_group_cache *block_group = ctl->private;\r\nif (ctl->free_extents < ctl->extents_thresh) {\r\nif (info->bytes <= block_group->sectorsize * 4) {\r\nif (ctl->free_extents * 2 <= ctl->extents_thresh)\r\nreturn false;\r\n} else {\r\nreturn false;\r\n}\r\n}\r\nif (((BITS_PER_BITMAP * ctl->unit) >> 1) > block_group->key.offset)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic int insert_into_bitmap(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info)\r\n{\r\nstruct btrfs_free_space *bitmap_info;\r\nstruct btrfs_block_group_cache *block_group = NULL;\r\nint added = 0;\r\nu64 bytes, offset, bytes_added;\r\nint ret;\r\nbytes = info->bytes;\r\noffset = info->offset;\r\nif (!ctl->op->use_bitmap(ctl, info))\r\nreturn 0;\r\nif (ctl->op == &free_space_op)\r\nblock_group = ctl->private;\r\nagain:\r\nif (block_group && !list_empty(&block_group->cluster_list)) {\r\nstruct btrfs_free_cluster *cluster;\r\nstruct rb_node *node;\r\nstruct btrfs_free_space *entry;\r\ncluster = list_entry(block_group->cluster_list.next,\r\nstruct btrfs_free_cluster,\r\nblock_group_list);\r\nspin_lock(&cluster->lock);\r\nnode = rb_first(&cluster->root);\r\nif (!node) {\r\nspin_unlock(&cluster->lock);\r\ngoto no_cluster_bitmap;\r\n}\r\nentry = rb_entry(node, struct btrfs_free_space, offset_index);\r\nif (!entry->bitmap) {\r\nspin_unlock(&cluster->lock);\r\ngoto no_cluster_bitmap;\r\n}\r\nif (entry->offset == offset_to_bitmap(ctl, offset)) {\r\nbytes_added = add_bytes_to_bitmap(ctl, entry,\r\noffset, bytes);\r\nbytes -= bytes_added;\r\noffset += bytes_added;\r\n}\r\nspin_unlock(&cluster->lock);\r\nif (!bytes) {\r\nret = 1;\r\ngoto out;\r\n}\r\n}\r\nno_cluster_bitmap:\r\nbitmap_info = tree_search_offset(ctl, offset_to_bitmap(ctl, offset),\r\n1, 0);\r\nif (!bitmap_info) {\r\nASSERT(added == 0);\r\ngoto new_bitmap;\r\n}\r\nbytes_added = add_bytes_to_bitmap(ctl, bitmap_info, offset, bytes);\r\nbytes -= bytes_added;\r\noffset += bytes_added;\r\nadded = 0;\r\nif (!bytes) {\r\nret = 1;\r\ngoto out;\r\n} else\r\ngoto again;\r\nnew_bitmap:\r\nif (info && info->bitmap) {\r\nadd_new_bitmap(ctl, info, offset);\r\nadded = 1;\r\ninfo = NULL;\r\ngoto again;\r\n} else {\r\nspin_unlock(&ctl->tree_lock);\r\nif (!info) {\r\ninfo = kmem_cache_zalloc(btrfs_free_space_cachep,\r\nGFP_NOFS);\r\nif (!info) {\r\nspin_lock(&ctl->tree_lock);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\n}\r\ninfo->bitmap = kzalloc(PAGE_CACHE_SIZE, GFP_NOFS);\r\nspin_lock(&ctl->tree_lock);\r\nif (!info->bitmap) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ngoto again;\r\n}\r\nout:\r\nif (info) {\r\nif (info->bitmap)\r\nkfree(info->bitmap);\r\nkmem_cache_free(btrfs_free_space_cachep, info);\r\n}\r\nreturn ret;\r\n}\r\nstatic bool try_merge_free_space(struct btrfs_free_space_ctl *ctl,\r\nstruct btrfs_free_space *info, bool update_stat)\r\n{\r\nstruct btrfs_free_space *left_info;\r\nstruct btrfs_free_space *right_info;\r\nbool merged = false;\r\nu64 offset = info->offset;\r\nu64 bytes = info->bytes;\r\nright_info = tree_search_offset(ctl, offset + bytes, 0, 0);\r\nif (right_info && rb_prev(&right_info->offset_index))\r\nleft_info = rb_entry(rb_prev(&right_info->offset_index),\r\nstruct btrfs_free_space, offset_index);\r\nelse\r\nleft_info = tree_search_offset(ctl, offset - 1, 0, 0);\r\nif (right_info && !right_info->bitmap) {\r\nif (update_stat)\r\nunlink_free_space(ctl, right_info);\r\nelse\r\n__unlink_free_space(ctl, right_info);\r\ninfo->bytes += right_info->bytes;\r\nkmem_cache_free(btrfs_free_space_cachep, right_info);\r\nmerged = true;\r\n}\r\nif (left_info && !left_info->bitmap &&\r\nleft_info->offset + left_info->bytes == offset) {\r\nif (update_stat)\r\nunlink_free_space(ctl, left_info);\r\nelse\r\n__unlink_free_space(ctl, left_info);\r\ninfo->offset = left_info->offset;\r\ninfo->bytes += left_info->bytes;\r\nkmem_cache_free(btrfs_free_space_cachep, left_info);\r\nmerged = true;\r\n}\r\nreturn merged;\r\n}\r\nint __btrfs_add_free_space(struct btrfs_free_space_ctl *ctl,\r\nu64 offset, u64 bytes)\r\n{\r\nstruct btrfs_free_space *info;\r\nint ret = 0;\r\ninfo = kmem_cache_zalloc(btrfs_free_space_cachep, GFP_NOFS);\r\nif (!info)\r\nreturn -ENOMEM;\r\ninfo->offset = offset;\r\ninfo->bytes = bytes;\r\nspin_lock(&ctl->tree_lock);\r\nif (try_merge_free_space(ctl, info, true))\r\ngoto link;\r\nret = insert_into_bitmap(ctl, info);\r\nif (ret < 0) {\r\ngoto out;\r\n} else if (ret) {\r\nret = 0;\r\ngoto out;\r\n}\r\nlink:\r\nret = link_free_space(ctl, info);\r\nif (ret)\r\nkmem_cache_free(btrfs_free_space_cachep, info);\r\nout:\r\nspin_unlock(&ctl->tree_lock);\r\nif (ret) {\r\nprintk(KERN_CRIT "BTRFS: unable to add free space :%d\n", ret);\r\nASSERT(ret != -EEXIST);\r\n}\r\nreturn ret;\r\n}\r\nint btrfs_remove_free_space(struct btrfs_block_group_cache *block_group,\r\nu64 offset, u64 bytes)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *info;\r\nint ret;\r\nbool re_search = false;\r\nspin_lock(&ctl->tree_lock);\r\nagain:\r\nret = 0;\r\nif (!bytes)\r\ngoto out_lock;\r\ninfo = tree_search_offset(ctl, offset, 0, 0);\r\nif (!info) {\r\ninfo = tree_search_offset(ctl, offset_to_bitmap(ctl, offset),\r\n1, 0);\r\nif (!info) {\r\nWARN_ON(re_search);\r\ngoto out_lock;\r\n}\r\n}\r\nre_search = false;\r\nif (!info->bitmap) {\r\nunlink_free_space(ctl, info);\r\nif (offset == info->offset) {\r\nu64 to_free = min(bytes, info->bytes);\r\ninfo->bytes -= to_free;\r\ninfo->offset += to_free;\r\nif (info->bytes) {\r\nret = link_free_space(ctl, info);\r\nWARN_ON(ret);\r\n} else {\r\nkmem_cache_free(btrfs_free_space_cachep, info);\r\n}\r\noffset += to_free;\r\nbytes -= to_free;\r\ngoto again;\r\n} else {\r\nu64 old_end = info->bytes + info->offset;\r\ninfo->bytes = offset - info->offset;\r\nret = link_free_space(ctl, info);\r\nWARN_ON(ret);\r\nif (ret)\r\ngoto out_lock;\r\nif (old_end < offset + bytes) {\r\nbytes -= old_end - offset;\r\noffset = old_end;\r\ngoto again;\r\n} else if (old_end == offset + bytes) {\r\ngoto out_lock;\r\n}\r\nspin_unlock(&ctl->tree_lock);\r\nret = btrfs_add_free_space(block_group, offset + bytes,\r\nold_end - (offset + bytes));\r\nWARN_ON(ret);\r\ngoto out;\r\n}\r\n}\r\nret = remove_from_bitmap(ctl, info, &offset, &bytes);\r\nif (ret == -EAGAIN) {\r\nre_search = true;\r\ngoto again;\r\n}\r\nout_lock:\r\nspin_unlock(&ctl->tree_lock);\r\nout:\r\nreturn ret;\r\n}\r\nvoid btrfs_dump_free_space(struct btrfs_block_group_cache *block_group,\r\nu64 bytes)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *info;\r\nstruct rb_node *n;\r\nint count = 0;\r\nfor (n = rb_first(&ctl->free_space_offset); n; n = rb_next(n)) {\r\ninfo = rb_entry(n, struct btrfs_free_space, offset_index);\r\nif (info->bytes >= bytes && !block_group->ro)\r\ncount++;\r\nbtrfs_crit(block_group->fs_info,\r\n"entry offset %llu, bytes %llu, bitmap %s",\r\ninfo->offset, info->bytes,\r\n(info->bitmap) ? "yes" : "no");\r\n}\r\nbtrfs_info(block_group->fs_info, "block group has cluster?: %s",\r\nlist_empty(&block_group->cluster_list) ? "no" : "yes");\r\nbtrfs_info(block_group->fs_info,\r\n"%d blocks of free space at or bigger than bytes is", count);\r\n}\r\nvoid btrfs_init_free_space_ctl(struct btrfs_block_group_cache *block_group)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nspin_lock_init(&ctl->tree_lock);\r\nctl->unit = block_group->sectorsize;\r\nctl->start = block_group->key.objectid;\r\nctl->private = block_group;\r\nctl->op = &free_space_op;\r\nctl->extents_thresh = ((1024 * 32) / 2) /\r\nsizeof(struct btrfs_free_space);\r\n}\r\nstatic int\r\n__btrfs_return_cluster_to_free_space(\r\nstruct btrfs_block_group_cache *block_group,\r\nstruct btrfs_free_cluster *cluster)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *entry;\r\nstruct rb_node *node;\r\nspin_lock(&cluster->lock);\r\nif (cluster->block_group != block_group)\r\ngoto out;\r\ncluster->block_group = NULL;\r\ncluster->window_start = 0;\r\nlist_del_init(&cluster->block_group_list);\r\nnode = rb_first(&cluster->root);\r\nwhile (node) {\r\nbool bitmap;\r\nentry = rb_entry(node, struct btrfs_free_space, offset_index);\r\nnode = rb_next(&entry->offset_index);\r\nrb_erase(&entry->offset_index, &cluster->root);\r\nbitmap = (entry->bitmap != NULL);\r\nif (!bitmap)\r\ntry_merge_free_space(ctl, entry, false);\r\ntree_insert_offset(&ctl->free_space_offset,\r\nentry->offset, &entry->offset_index, bitmap);\r\n}\r\ncluster->root = RB_ROOT;\r\nout:\r\nspin_unlock(&cluster->lock);\r\nbtrfs_put_block_group(block_group);\r\nreturn 0;\r\n}\r\nstatic void __btrfs_remove_free_space_cache_locked(\r\nstruct btrfs_free_space_ctl *ctl)\r\n{\r\nstruct btrfs_free_space *info;\r\nstruct rb_node *node;\r\nwhile ((node = rb_last(&ctl->free_space_offset)) != NULL) {\r\ninfo = rb_entry(node, struct btrfs_free_space, offset_index);\r\nif (!info->bitmap) {\r\nunlink_free_space(ctl, info);\r\nkmem_cache_free(btrfs_free_space_cachep, info);\r\n} else {\r\nfree_bitmap(ctl, info);\r\n}\r\nif (need_resched()) {\r\nspin_unlock(&ctl->tree_lock);\r\ncond_resched();\r\nspin_lock(&ctl->tree_lock);\r\n}\r\n}\r\n}\r\nvoid __btrfs_remove_free_space_cache(struct btrfs_free_space_ctl *ctl)\r\n{\r\nspin_lock(&ctl->tree_lock);\r\n__btrfs_remove_free_space_cache_locked(ctl);\r\nspin_unlock(&ctl->tree_lock);\r\n}\r\nvoid btrfs_remove_free_space_cache(struct btrfs_block_group_cache *block_group)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_cluster *cluster;\r\nstruct list_head *head;\r\nspin_lock(&ctl->tree_lock);\r\nwhile ((head = block_group->cluster_list.next) !=\r\n&block_group->cluster_list) {\r\ncluster = list_entry(head, struct btrfs_free_cluster,\r\nblock_group_list);\r\nWARN_ON(cluster->block_group != block_group);\r\n__btrfs_return_cluster_to_free_space(block_group, cluster);\r\nif (need_resched()) {\r\nspin_unlock(&ctl->tree_lock);\r\ncond_resched();\r\nspin_lock(&ctl->tree_lock);\r\n}\r\n}\r\n__btrfs_remove_free_space_cache_locked(ctl);\r\nspin_unlock(&ctl->tree_lock);\r\n}\r\nu64 btrfs_find_space_for_alloc(struct btrfs_block_group_cache *block_group,\r\nu64 offset, u64 bytes, u64 empty_size,\r\nu64 *max_extent_size)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *entry = NULL;\r\nu64 bytes_search = bytes + empty_size;\r\nu64 ret = 0;\r\nu64 align_gap = 0;\r\nu64 align_gap_len = 0;\r\nspin_lock(&ctl->tree_lock);\r\nentry = find_free_space(ctl, &offset, &bytes_search,\r\nblock_group->full_stripe_len, max_extent_size);\r\nif (!entry)\r\ngoto out;\r\nret = offset;\r\nif (entry->bitmap) {\r\nbitmap_clear_bits(ctl, entry, offset, bytes);\r\nif (!entry->bytes)\r\nfree_bitmap(ctl, entry);\r\n} else {\r\nunlink_free_space(ctl, entry);\r\nalign_gap_len = offset - entry->offset;\r\nalign_gap = entry->offset;\r\nentry->offset = offset + bytes;\r\nWARN_ON(entry->bytes < bytes + align_gap_len);\r\nentry->bytes -= bytes + align_gap_len;\r\nif (!entry->bytes)\r\nkmem_cache_free(btrfs_free_space_cachep, entry);\r\nelse\r\nlink_free_space(ctl, entry);\r\n}\r\nout:\r\nspin_unlock(&ctl->tree_lock);\r\nif (align_gap_len)\r\n__btrfs_add_free_space(ctl, align_gap, align_gap_len);\r\nreturn ret;\r\n}\r\nint btrfs_return_cluster_to_free_space(\r\nstruct btrfs_block_group_cache *block_group,\r\nstruct btrfs_free_cluster *cluster)\r\n{\r\nstruct btrfs_free_space_ctl *ctl;\r\nint ret;\r\nspin_lock(&cluster->lock);\r\nif (!block_group) {\r\nblock_group = cluster->block_group;\r\nif (!block_group) {\r\nspin_unlock(&cluster->lock);\r\nreturn 0;\r\n}\r\n} else if (cluster->block_group != block_group) {\r\nspin_unlock(&cluster->lock);\r\nreturn 0;\r\n}\r\natomic_inc(&block_group->count);\r\nspin_unlock(&cluster->lock);\r\nctl = block_group->free_space_ctl;\r\nspin_lock(&ctl->tree_lock);\r\nret = __btrfs_return_cluster_to_free_space(block_group, cluster);\r\nspin_unlock(&ctl->tree_lock);\r\nbtrfs_put_block_group(block_group);\r\nreturn ret;\r\n}\r\nstatic u64 btrfs_alloc_from_bitmap(struct btrfs_block_group_cache *block_group,\r\nstruct btrfs_free_cluster *cluster,\r\nstruct btrfs_free_space *entry,\r\nu64 bytes, u64 min_start,\r\nu64 *max_extent_size)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nint err;\r\nu64 search_start = cluster->window_start;\r\nu64 search_bytes = bytes;\r\nu64 ret = 0;\r\nsearch_start = min_start;\r\nsearch_bytes = bytes;\r\nerr = search_bitmap(ctl, entry, &search_start, &search_bytes);\r\nif (err) {\r\nif (search_bytes > *max_extent_size)\r\n*max_extent_size = search_bytes;\r\nreturn 0;\r\n}\r\nret = search_start;\r\n__bitmap_clear_bits(ctl, entry, ret, bytes);\r\nreturn ret;\r\n}\r\nu64 btrfs_alloc_from_cluster(struct btrfs_block_group_cache *block_group,\r\nstruct btrfs_free_cluster *cluster, u64 bytes,\r\nu64 min_start, u64 *max_extent_size)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *entry = NULL;\r\nstruct rb_node *node;\r\nu64 ret = 0;\r\nspin_lock(&cluster->lock);\r\nif (bytes > cluster->max_size)\r\ngoto out;\r\nif (cluster->block_group != block_group)\r\ngoto out;\r\nnode = rb_first(&cluster->root);\r\nif (!node)\r\ngoto out;\r\nentry = rb_entry(node, struct btrfs_free_space, offset_index);\r\nwhile (1) {\r\nif (entry->bytes < bytes && entry->bytes > *max_extent_size)\r\n*max_extent_size = entry->bytes;\r\nif (entry->bytes < bytes ||\r\n(!entry->bitmap && entry->offset < min_start)) {\r\nnode = rb_next(&entry->offset_index);\r\nif (!node)\r\nbreak;\r\nentry = rb_entry(node, struct btrfs_free_space,\r\noffset_index);\r\ncontinue;\r\n}\r\nif (entry->bitmap) {\r\nret = btrfs_alloc_from_bitmap(block_group,\r\ncluster, entry, bytes,\r\ncluster->window_start,\r\nmax_extent_size);\r\nif (ret == 0) {\r\nnode = rb_next(&entry->offset_index);\r\nif (!node)\r\nbreak;\r\nentry = rb_entry(node, struct btrfs_free_space,\r\noffset_index);\r\ncontinue;\r\n}\r\ncluster->window_start += bytes;\r\n} else {\r\nret = entry->offset;\r\nentry->offset += bytes;\r\nentry->bytes -= bytes;\r\n}\r\nif (entry->bytes == 0)\r\nrb_erase(&entry->offset_index, &cluster->root);\r\nbreak;\r\n}\r\nout:\r\nspin_unlock(&cluster->lock);\r\nif (!ret)\r\nreturn 0;\r\nspin_lock(&ctl->tree_lock);\r\nctl->free_space -= bytes;\r\nif (entry->bytes == 0) {\r\nctl->free_extents--;\r\nif (entry->bitmap) {\r\nkfree(entry->bitmap);\r\nctl->total_bitmaps--;\r\nctl->op->recalc_thresholds(ctl);\r\n}\r\nkmem_cache_free(btrfs_free_space_cachep, entry);\r\n}\r\nspin_unlock(&ctl->tree_lock);\r\nreturn ret;\r\n}\r\nstatic int btrfs_bitmap_cluster(struct btrfs_block_group_cache *block_group,\r\nstruct btrfs_free_space *entry,\r\nstruct btrfs_free_cluster *cluster,\r\nu64 offset, u64 bytes,\r\nu64 cont1_bytes, u64 min_bytes)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nunsigned long next_zero;\r\nunsigned long i;\r\nunsigned long want_bits;\r\nunsigned long min_bits;\r\nunsigned long found_bits;\r\nunsigned long start = 0;\r\nunsigned long total_found = 0;\r\nint ret;\r\ni = offset_to_bit(entry->offset, ctl->unit,\r\nmax_t(u64, offset, entry->offset));\r\nwant_bits = bytes_to_bits(bytes, ctl->unit);\r\nmin_bits = bytes_to_bits(min_bytes, ctl->unit);\r\nagain:\r\nfound_bits = 0;\r\nfor_each_set_bit_from(i, entry->bitmap, BITS_PER_BITMAP) {\r\nnext_zero = find_next_zero_bit(entry->bitmap,\r\nBITS_PER_BITMAP, i);\r\nif (next_zero - i >= min_bits) {\r\nfound_bits = next_zero - i;\r\nbreak;\r\n}\r\ni = next_zero;\r\n}\r\nif (!found_bits)\r\nreturn -ENOSPC;\r\nif (!total_found) {\r\nstart = i;\r\ncluster->max_size = 0;\r\n}\r\ntotal_found += found_bits;\r\nif (cluster->max_size < found_bits * ctl->unit)\r\ncluster->max_size = found_bits * ctl->unit;\r\nif (total_found < want_bits || cluster->max_size < cont1_bytes) {\r\ni = next_zero + 1;\r\ngoto again;\r\n}\r\ncluster->window_start = start * ctl->unit + entry->offset;\r\nrb_erase(&entry->offset_index, &ctl->free_space_offset);\r\nret = tree_insert_offset(&cluster->root, entry->offset,\r\n&entry->offset_index, 1);\r\nASSERT(!ret);\r\ntrace_btrfs_setup_cluster(block_group, cluster,\r\ntotal_found * ctl->unit, 1);\r\nreturn 0;\r\n}\r\nstatic noinline int\r\nsetup_cluster_no_bitmap(struct btrfs_block_group_cache *block_group,\r\nstruct btrfs_free_cluster *cluster,\r\nstruct list_head *bitmaps, u64 offset, u64 bytes,\r\nu64 cont1_bytes, u64 min_bytes)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *first = NULL;\r\nstruct btrfs_free_space *entry = NULL;\r\nstruct btrfs_free_space *last;\r\nstruct rb_node *node;\r\nu64 window_free;\r\nu64 max_extent;\r\nu64 total_size = 0;\r\nentry = tree_search_offset(ctl, offset, 0, 1);\r\nif (!entry)\r\nreturn -ENOSPC;\r\nwhile (entry->bitmap || entry->bytes < min_bytes) {\r\nif (entry->bitmap && list_empty(&entry->list))\r\nlist_add_tail(&entry->list, bitmaps);\r\nnode = rb_next(&entry->offset_index);\r\nif (!node)\r\nreturn -ENOSPC;\r\nentry = rb_entry(node, struct btrfs_free_space, offset_index);\r\n}\r\nwindow_free = entry->bytes;\r\nmax_extent = entry->bytes;\r\nfirst = entry;\r\nlast = entry;\r\nfor (node = rb_next(&entry->offset_index); node;\r\nnode = rb_next(&entry->offset_index)) {\r\nentry = rb_entry(node, struct btrfs_free_space, offset_index);\r\nif (entry->bitmap) {\r\nif (list_empty(&entry->list))\r\nlist_add_tail(&entry->list, bitmaps);\r\ncontinue;\r\n}\r\nif (entry->bytes < min_bytes)\r\ncontinue;\r\nlast = entry;\r\nwindow_free += entry->bytes;\r\nif (entry->bytes > max_extent)\r\nmax_extent = entry->bytes;\r\n}\r\nif (window_free < bytes || max_extent < cont1_bytes)\r\nreturn -ENOSPC;\r\ncluster->window_start = first->offset;\r\nnode = &first->offset_index;\r\ndo {\r\nint ret;\r\nentry = rb_entry(node, struct btrfs_free_space, offset_index);\r\nnode = rb_next(&entry->offset_index);\r\nif (entry->bitmap || entry->bytes < min_bytes)\r\ncontinue;\r\nrb_erase(&entry->offset_index, &ctl->free_space_offset);\r\nret = tree_insert_offset(&cluster->root, entry->offset,\r\n&entry->offset_index, 0);\r\ntotal_size += entry->bytes;\r\nASSERT(!ret);\r\n} while (node && entry != last);\r\ncluster->max_size = max_extent;\r\ntrace_btrfs_setup_cluster(block_group, cluster, total_size, 0);\r\nreturn 0;\r\n}\r\nstatic noinline int\r\nsetup_cluster_bitmap(struct btrfs_block_group_cache *block_group,\r\nstruct btrfs_free_cluster *cluster,\r\nstruct list_head *bitmaps, u64 offset, u64 bytes,\r\nu64 cont1_bytes, u64 min_bytes)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *entry;\r\nint ret = -ENOSPC;\r\nu64 bitmap_offset = offset_to_bitmap(ctl, offset);\r\nif (ctl->total_bitmaps == 0)\r\nreturn -ENOSPC;\r\nentry = list_first_entry(bitmaps, struct btrfs_free_space, list);\r\nif (entry->offset != bitmap_offset) {\r\nentry = tree_search_offset(ctl, bitmap_offset, 1, 0);\r\nif (entry && list_empty(&entry->list))\r\nlist_add(&entry->list, bitmaps);\r\n}\r\nlist_for_each_entry(entry, bitmaps, list) {\r\nif (entry->bytes < bytes)\r\ncontinue;\r\nret = btrfs_bitmap_cluster(block_group, entry, cluster, offset,\r\nbytes, cont1_bytes, min_bytes);\r\nif (!ret)\r\nreturn 0;\r\n}\r\nreturn -ENOSPC;\r\n}\r\nint btrfs_find_space_cluster(struct btrfs_root *root,\r\nstruct btrfs_block_group_cache *block_group,\r\nstruct btrfs_free_cluster *cluster,\r\nu64 offset, u64 bytes, u64 empty_size)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *entry, *tmp;\r\nLIST_HEAD(bitmaps);\r\nu64 min_bytes;\r\nu64 cont1_bytes;\r\nint ret;\r\nif (btrfs_test_opt(root, SSD_SPREAD)) {\r\ncont1_bytes = min_bytes = bytes + empty_size;\r\n} else if (block_group->flags & BTRFS_BLOCK_GROUP_METADATA) {\r\ncont1_bytes = bytes;\r\nmin_bytes = block_group->sectorsize;\r\n} else {\r\ncont1_bytes = max(bytes, (bytes + empty_size) >> 2);\r\nmin_bytes = block_group->sectorsize;\r\n}\r\nspin_lock(&ctl->tree_lock);\r\nif (ctl->free_space < bytes) {\r\nspin_unlock(&ctl->tree_lock);\r\nreturn -ENOSPC;\r\n}\r\nspin_lock(&cluster->lock);\r\nif (cluster->block_group) {\r\nret = 0;\r\ngoto out;\r\n}\r\ntrace_btrfs_find_cluster(block_group, offset, bytes, empty_size,\r\nmin_bytes);\r\nINIT_LIST_HEAD(&bitmaps);\r\nret = setup_cluster_no_bitmap(block_group, cluster, &bitmaps, offset,\r\nbytes + empty_size,\r\ncont1_bytes, min_bytes);\r\nif (ret)\r\nret = setup_cluster_bitmap(block_group, cluster, &bitmaps,\r\noffset, bytes + empty_size,\r\ncont1_bytes, min_bytes);\r\nlist_for_each_entry_safe(entry, tmp, &bitmaps, list)\r\nlist_del_init(&entry->list);\r\nif (!ret) {\r\natomic_inc(&block_group->count);\r\nlist_add_tail(&cluster->block_group_list,\r\n&block_group->cluster_list);\r\ncluster->block_group = block_group;\r\n} else {\r\ntrace_btrfs_failed_cluster_setup(block_group);\r\n}\r\nout:\r\nspin_unlock(&cluster->lock);\r\nspin_unlock(&ctl->tree_lock);\r\nreturn ret;\r\n}\r\nvoid btrfs_init_free_cluster(struct btrfs_free_cluster *cluster)\r\n{\r\nspin_lock_init(&cluster->lock);\r\nspin_lock_init(&cluster->refill_lock);\r\ncluster->root = RB_ROOT;\r\ncluster->max_size = 0;\r\nINIT_LIST_HEAD(&cluster->block_group_list);\r\ncluster->block_group = NULL;\r\n}\r\nstatic int do_trimming(struct btrfs_block_group_cache *block_group,\r\nu64 *total_trimmed, u64 start, u64 bytes,\r\nu64 reserved_start, u64 reserved_bytes)\r\n{\r\nstruct btrfs_space_info *space_info = block_group->space_info;\r\nstruct btrfs_fs_info *fs_info = block_group->fs_info;\r\nint ret;\r\nint update = 0;\r\nu64 trimmed = 0;\r\nspin_lock(&space_info->lock);\r\nspin_lock(&block_group->lock);\r\nif (!block_group->ro) {\r\nblock_group->reserved += reserved_bytes;\r\nspace_info->bytes_reserved += reserved_bytes;\r\nupdate = 1;\r\n}\r\nspin_unlock(&block_group->lock);\r\nspin_unlock(&space_info->lock);\r\nret = btrfs_error_discard_extent(fs_info->extent_root,\r\nstart, bytes, &trimmed);\r\nif (!ret)\r\n*total_trimmed += trimmed;\r\nbtrfs_add_free_space(block_group, reserved_start, reserved_bytes);\r\nif (update) {\r\nspin_lock(&space_info->lock);\r\nspin_lock(&block_group->lock);\r\nif (block_group->ro)\r\nspace_info->bytes_readonly += reserved_bytes;\r\nblock_group->reserved -= reserved_bytes;\r\nspace_info->bytes_reserved -= reserved_bytes;\r\nspin_unlock(&space_info->lock);\r\nspin_unlock(&block_group->lock);\r\n}\r\nreturn ret;\r\n}\r\nstatic int trim_no_bitmap(struct btrfs_block_group_cache *block_group,\r\nu64 *total_trimmed, u64 start, u64 end, u64 minlen)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *entry;\r\nstruct rb_node *node;\r\nint ret = 0;\r\nu64 extent_start;\r\nu64 extent_bytes;\r\nu64 bytes;\r\nwhile (start < end) {\r\nspin_lock(&ctl->tree_lock);\r\nif (ctl->free_space < minlen) {\r\nspin_unlock(&ctl->tree_lock);\r\nbreak;\r\n}\r\nentry = tree_search_offset(ctl, start, 0, 1);\r\nif (!entry) {\r\nspin_unlock(&ctl->tree_lock);\r\nbreak;\r\n}\r\nwhile (entry->bitmap) {\r\nnode = rb_next(&entry->offset_index);\r\nif (!node) {\r\nspin_unlock(&ctl->tree_lock);\r\ngoto out;\r\n}\r\nentry = rb_entry(node, struct btrfs_free_space,\r\noffset_index);\r\n}\r\nif (entry->offset >= end) {\r\nspin_unlock(&ctl->tree_lock);\r\nbreak;\r\n}\r\nextent_start = entry->offset;\r\nextent_bytes = entry->bytes;\r\nstart = max(start, extent_start);\r\nbytes = min(extent_start + extent_bytes, end) - start;\r\nif (bytes < minlen) {\r\nspin_unlock(&ctl->tree_lock);\r\ngoto next;\r\n}\r\nunlink_free_space(ctl, entry);\r\nkmem_cache_free(btrfs_free_space_cachep, entry);\r\nspin_unlock(&ctl->tree_lock);\r\nret = do_trimming(block_group, total_trimmed, start, bytes,\r\nextent_start, extent_bytes);\r\nif (ret)\r\nbreak;\r\nnext:\r\nstart += bytes;\r\nif (fatal_signal_pending(current)) {\r\nret = -ERESTARTSYS;\r\nbreak;\r\n}\r\ncond_resched();\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int trim_bitmaps(struct btrfs_block_group_cache *block_group,\r\nu64 *total_trimmed, u64 start, u64 end, u64 minlen)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\r\nstruct btrfs_free_space *entry;\r\nint ret = 0;\r\nint ret2;\r\nu64 bytes;\r\nu64 offset = offset_to_bitmap(ctl, start);\r\nwhile (offset < end) {\r\nbool next_bitmap = false;\r\nspin_lock(&ctl->tree_lock);\r\nif (ctl->free_space < minlen) {\r\nspin_unlock(&ctl->tree_lock);\r\nbreak;\r\n}\r\nentry = tree_search_offset(ctl, offset, 1, 0);\r\nif (!entry) {\r\nspin_unlock(&ctl->tree_lock);\r\nnext_bitmap = true;\r\ngoto next;\r\n}\r\nbytes = minlen;\r\nret2 = search_bitmap(ctl, entry, &start, &bytes);\r\nif (ret2 || start >= end) {\r\nspin_unlock(&ctl->tree_lock);\r\nnext_bitmap = true;\r\ngoto next;\r\n}\r\nbytes = min(bytes, end - start);\r\nif (bytes < minlen) {\r\nspin_unlock(&ctl->tree_lock);\r\ngoto next;\r\n}\r\nbitmap_clear_bits(ctl, entry, start, bytes);\r\nif (entry->bytes == 0)\r\nfree_bitmap(ctl, entry);\r\nspin_unlock(&ctl->tree_lock);\r\nret = do_trimming(block_group, total_trimmed, start, bytes,\r\nstart, bytes);\r\nif (ret)\r\nbreak;\r\nnext:\r\nif (next_bitmap) {\r\noffset += BITS_PER_BITMAP * ctl->unit;\r\n} else {\r\nstart += bytes;\r\nif (start >= offset + BITS_PER_BITMAP * ctl->unit)\r\noffset += BITS_PER_BITMAP * ctl->unit;\r\n}\r\nif (fatal_signal_pending(current)) {\r\nret = -ERESTARTSYS;\r\nbreak;\r\n}\r\ncond_resched();\r\n}\r\nreturn ret;\r\n}\r\nint btrfs_trim_block_group(struct btrfs_block_group_cache *block_group,\r\nu64 *trimmed, u64 start, u64 end, u64 minlen)\r\n{\r\nint ret;\r\n*trimmed = 0;\r\nret = trim_no_bitmap(block_group, trimmed, start, end, minlen);\r\nif (ret)\r\nreturn ret;\r\nret = trim_bitmaps(block_group, trimmed, start, end, minlen);\r\nreturn ret;\r\n}\r\nu64 btrfs_find_ino_for_alloc(struct btrfs_root *fs_root)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = fs_root->free_ino_ctl;\r\nstruct btrfs_free_space *entry = NULL;\r\nu64 ino = 0;\r\nspin_lock(&ctl->tree_lock);\r\nif (RB_EMPTY_ROOT(&ctl->free_space_offset))\r\ngoto out;\r\nentry = rb_entry(rb_first(&ctl->free_space_offset),\r\nstruct btrfs_free_space, offset_index);\r\nif (!entry->bitmap) {\r\nino = entry->offset;\r\nunlink_free_space(ctl, entry);\r\nentry->offset++;\r\nentry->bytes--;\r\nif (!entry->bytes)\r\nkmem_cache_free(btrfs_free_space_cachep, entry);\r\nelse\r\nlink_free_space(ctl, entry);\r\n} else {\r\nu64 offset = 0;\r\nu64 count = 1;\r\nint ret;\r\nret = search_bitmap(ctl, entry, &offset, &count);\r\nASSERT(!ret);\r\nino = offset;\r\nbitmap_clear_bits(ctl, entry, offset, 1);\r\nif (entry->bytes == 0)\r\nfree_bitmap(ctl, entry);\r\n}\r\nout:\r\nspin_unlock(&ctl->tree_lock);\r\nreturn ino;\r\n}\r\nstruct inode *lookup_free_ino_inode(struct btrfs_root *root,\r\nstruct btrfs_path *path)\r\n{\r\nstruct inode *inode = NULL;\r\nspin_lock(&root->cache_lock);\r\nif (root->cache_inode)\r\ninode = igrab(root->cache_inode);\r\nspin_unlock(&root->cache_lock);\r\nif (inode)\r\nreturn inode;\r\ninode = __lookup_free_space_inode(root, path, 0);\r\nif (IS_ERR(inode))\r\nreturn inode;\r\nspin_lock(&root->cache_lock);\r\nif (!btrfs_fs_closing(root->fs_info))\r\nroot->cache_inode = igrab(inode);\r\nspin_unlock(&root->cache_lock);\r\nreturn inode;\r\n}\r\nint create_free_ino_inode(struct btrfs_root *root,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_path *path)\r\n{\r\nreturn __create_free_space_inode(root, trans, path,\r\nBTRFS_FREE_INO_OBJECTID, 0);\r\n}\r\nint load_free_ino_cache(struct btrfs_fs_info *fs_info, struct btrfs_root *root)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = root->free_ino_ctl;\r\nstruct btrfs_path *path;\r\nstruct inode *inode;\r\nint ret = 0;\r\nu64 root_gen = btrfs_root_generation(&root->root_item);\r\nif (!btrfs_test_opt(root, INODE_MAP_CACHE))\r\nreturn 0;\r\nif (btrfs_fs_closing(fs_info))\r\nreturn 0;\r\npath = btrfs_alloc_path();\r\nif (!path)\r\nreturn 0;\r\ninode = lookup_free_ino_inode(root, path);\r\nif (IS_ERR(inode))\r\ngoto out;\r\nif (root_gen != BTRFS_I(inode)->generation)\r\ngoto out_put;\r\nret = __load_free_space_cache(root, inode, ctl, path, 0);\r\nif (ret < 0)\r\nbtrfs_err(fs_info,\r\n"failed to load free ino cache for root %llu",\r\nroot->root_key.objectid);\r\nout_put:\r\niput(inode);\r\nout:\r\nbtrfs_free_path(path);\r\nreturn ret;\r\n}\r\nint btrfs_write_out_ino_cache(struct btrfs_root *root,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_path *path,\r\nstruct inode *inode)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = root->free_ino_ctl;\r\nint ret;\r\nif (!btrfs_test_opt(root, INODE_MAP_CACHE))\r\nreturn 0;\r\nret = __btrfs_write_out_cache(root, inode, ctl, NULL, trans, path, 0);\r\nif (ret) {\r\nbtrfs_delalloc_release_metadata(inode, inode->i_size);\r\n#ifdef DEBUG\r\nbtrfs_err(root->fs_info,\r\n"failed to write free ino cache for root %llu",\r\nroot->root_key.objectid);\r\n#endif\r\n}\r\nreturn ret;\r\n}\r\nint test_add_free_space_entry(struct btrfs_block_group_cache *cache,\r\nu64 offset, u64 bytes, bool bitmap)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = cache->free_space_ctl;\r\nstruct btrfs_free_space *info = NULL, *bitmap_info;\r\nvoid *map = NULL;\r\nu64 bytes_added;\r\nint ret;\r\nagain:\r\nif (!info) {\r\ninfo = kmem_cache_zalloc(btrfs_free_space_cachep, GFP_NOFS);\r\nif (!info)\r\nreturn -ENOMEM;\r\n}\r\nif (!bitmap) {\r\nspin_lock(&ctl->tree_lock);\r\ninfo->offset = offset;\r\ninfo->bytes = bytes;\r\nret = link_free_space(ctl, info);\r\nspin_unlock(&ctl->tree_lock);\r\nif (ret)\r\nkmem_cache_free(btrfs_free_space_cachep, info);\r\nreturn ret;\r\n}\r\nif (!map) {\r\nmap = kzalloc(PAGE_CACHE_SIZE, GFP_NOFS);\r\nif (!map) {\r\nkmem_cache_free(btrfs_free_space_cachep, info);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nspin_lock(&ctl->tree_lock);\r\nbitmap_info = tree_search_offset(ctl, offset_to_bitmap(ctl, offset),\r\n1, 0);\r\nif (!bitmap_info) {\r\ninfo->bitmap = map;\r\nmap = NULL;\r\nadd_new_bitmap(ctl, info, offset);\r\nbitmap_info = info;\r\n}\r\nbytes_added = add_bytes_to_bitmap(ctl, bitmap_info, offset, bytes);\r\nbytes -= bytes_added;\r\noffset += bytes_added;\r\nspin_unlock(&ctl->tree_lock);\r\nif (bytes)\r\ngoto again;\r\nif (map)\r\nkfree(map);\r\nreturn 0;\r\n}\r\nint test_check_exists(struct btrfs_block_group_cache *cache,\r\nu64 offset, u64 bytes)\r\n{\r\nstruct btrfs_free_space_ctl *ctl = cache->free_space_ctl;\r\nstruct btrfs_free_space *info;\r\nint ret = 0;\r\nspin_lock(&ctl->tree_lock);\r\ninfo = tree_search_offset(ctl, offset, 0, 0);\r\nif (!info) {\r\ninfo = tree_search_offset(ctl, offset_to_bitmap(ctl, offset),\r\n1, 0);\r\nif (!info)\r\ngoto out;\r\n}\r\nhave_info:\r\nif (info->bitmap) {\r\nu64 bit_off, bit_bytes;\r\nstruct rb_node *n;\r\nstruct btrfs_free_space *tmp;\r\nbit_off = offset;\r\nbit_bytes = ctl->unit;\r\nret = search_bitmap(ctl, info, &bit_off, &bit_bytes);\r\nif (!ret) {\r\nif (bit_off == offset) {\r\nret = 1;\r\ngoto out;\r\n} else if (bit_off > offset &&\r\noffset + bytes > bit_off) {\r\nret = 1;\r\ngoto out;\r\n}\r\n}\r\nn = rb_prev(&info->offset_index);\r\nwhile (n) {\r\ntmp = rb_entry(n, struct btrfs_free_space,\r\noffset_index);\r\nif (tmp->offset + tmp->bytes < offset)\r\nbreak;\r\nif (offset + bytes < tmp->offset) {\r\nn = rb_prev(&info->offset_index);\r\ncontinue;\r\n}\r\ninfo = tmp;\r\ngoto have_info;\r\n}\r\nn = rb_next(&info->offset_index);\r\nwhile (n) {\r\ntmp = rb_entry(n, struct btrfs_free_space,\r\noffset_index);\r\nif (offset + bytes < tmp->offset)\r\nbreak;\r\nif (tmp->offset + tmp->bytes < offset) {\r\nn = rb_next(&info->offset_index);\r\ncontinue;\r\n}\r\ninfo = tmp;\r\ngoto have_info;\r\n}\r\ngoto out;\r\n}\r\nif (info->offset == offset) {\r\nret = 1;\r\ngoto out;\r\n}\r\nif (offset > info->offset && offset < info->offset + info->bytes)\r\nret = 1;\r\nout:\r\nspin_unlock(&ctl->tree_lock);\r\nreturn ret;\r\n}
