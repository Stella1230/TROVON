static struct device *chan2dev(struct d40_chan *d40c)\r\n{\r\nreturn &d40c->chan.dev->device;\r\n}\r\nstatic bool chan_is_physical(struct d40_chan *chan)\r\n{\r\nreturn chan->log_num == D40_PHY_CHAN;\r\n}\r\nstatic bool chan_is_logical(struct d40_chan *chan)\r\n{\r\nreturn !chan_is_physical(chan);\r\n}\r\nstatic void __iomem *chan_base(struct d40_chan *chan)\r\n{\r\nreturn chan->base->virtbase + D40_DREG_PCBASE +\r\nchan->phy_chan->num * D40_DREG_PCDELTA;\r\n}\r\nstatic int d40_pool_lli_alloc(struct d40_chan *d40c, struct d40_desc *d40d,\r\nint lli_len)\r\n{\r\nbool is_log = chan_is_logical(d40c);\r\nu32 align;\r\nvoid *base;\r\nif (is_log)\r\nalign = sizeof(struct d40_log_lli);\r\nelse\r\nalign = sizeof(struct d40_phy_lli);\r\nif (lli_len == 1) {\r\nbase = d40d->lli_pool.pre_alloc_lli;\r\nd40d->lli_pool.size = sizeof(d40d->lli_pool.pre_alloc_lli);\r\nd40d->lli_pool.base = NULL;\r\n} else {\r\nd40d->lli_pool.size = lli_len * 2 * align;\r\nbase = kmalloc(d40d->lli_pool.size + align, GFP_NOWAIT);\r\nd40d->lli_pool.base = base;\r\nif (d40d->lli_pool.base == NULL)\r\nreturn -ENOMEM;\r\n}\r\nif (is_log) {\r\nd40d->lli_log.src = PTR_ALIGN(base, align);\r\nd40d->lli_log.dst = d40d->lli_log.src + lli_len;\r\nd40d->lli_pool.dma_addr = 0;\r\n} else {\r\nd40d->lli_phy.src = PTR_ALIGN(base, align);\r\nd40d->lli_phy.dst = d40d->lli_phy.src + lli_len;\r\nd40d->lli_pool.dma_addr = dma_map_single(d40c->base->dev,\r\nd40d->lli_phy.src,\r\nd40d->lli_pool.size,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(d40c->base->dev,\r\nd40d->lli_pool.dma_addr)) {\r\nkfree(d40d->lli_pool.base);\r\nd40d->lli_pool.base = NULL;\r\nd40d->lli_pool.dma_addr = 0;\r\nreturn -ENOMEM;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void d40_pool_lli_free(struct d40_chan *d40c, struct d40_desc *d40d)\r\n{\r\nif (d40d->lli_pool.dma_addr)\r\ndma_unmap_single(d40c->base->dev, d40d->lli_pool.dma_addr,\r\nd40d->lli_pool.size, DMA_TO_DEVICE);\r\nkfree(d40d->lli_pool.base);\r\nd40d->lli_pool.base = NULL;\r\nd40d->lli_pool.size = 0;\r\nd40d->lli_log.src = NULL;\r\nd40d->lli_log.dst = NULL;\r\nd40d->lli_phy.src = NULL;\r\nd40d->lli_phy.dst = NULL;\r\n}\r\nstatic int d40_lcla_alloc_one(struct d40_chan *d40c,\r\nstruct d40_desc *d40d)\r\n{\r\nunsigned long flags;\r\nint i;\r\nint ret = -EINVAL;\r\nspin_lock_irqsave(&d40c->base->lcla_pool.lock, flags);\r\nfor (i = 1 ; i < D40_LCLA_LINK_PER_EVENT_GRP / 2; i++) {\r\nint idx = d40c->phy_chan->num * D40_LCLA_LINK_PER_EVENT_GRP + i;\r\nif (!d40c->base->lcla_pool.alloc_map[idx]) {\r\nd40c->base->lcla_pool.alloc_map[idx] = d40d;\r\nd40d->lcla_alloc++;\r\nret = i;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&d40c->base->lcla_pool.lock, flags);\r\nreturn ret;\r\n}\r\nstatic int d40_lcla_free_all(struct d40_chan *d40c,\r\nstruct d40_desc *d40d)\r\n{\r\nunsigned long flags;\r\nint i;\r\nint ret = -EINVAL;\r\nif (chan_is_physical(d40c))\r\nreturn 0;\r\nspin_lock_irqsave(&d40c->base->lcla_pool.lock, flags);\r\nfor (i = 1 ; i < D40_LCLA_LINK_PER_EVENT_GRP / 2; i++) {\r\nint idx = d40c->phy_chan->num * D40_LCLA_LINK_PER_EVENT_GRP + i;\r\nif (d40c->base->lcla_pool.alloc_map[idx] == d40d) {\r\nd40c->base->lcla_pool.alloc_map[idx] = NULL;\r\nd40d->lcla_alloc--;\r\nif (d40d->lcla_alloc == 0) {\r\nret = 0;\r\nbreak;\r\n}\r\n}\r\n}\r\nspin_unlock_irqrestore(&d40c->base->lcla_pool.lock, flags);\r\nreturn ret;\r\n}\r\nstatic void d40_desc_remove(struct d40_desc *d40d)\r\n{\r\nlist_del(&d40d->node);\r\n}\r\nstatic struct d40_desc *d40_desc_get(struct d40_chan *d40c)\r\n{\r\nstruct d40_desc *desc = NULL;\r\nif (!list_empty(&d40c->client)) {\r\nstruct d40_desc *d;\r\nstruct d40_desc *_d;\r\nlist_for_each_entry_safe(d, _d, &d40c->client, node) {\r\nif (async_tx_test_ack(&d->txd)) {\r\nd40_desc_remove(d);\r\ndesc = d;\r\nmemset(desc, 0, sizeof(*desc));\r\nbreak;\r\n}\r\n}\r\n}\r\nif (!desc)\r\ndesc = kmem_cache_zalloc(d40c->base->desc_slab, GFP_NOWAIT);\r\nif (desc)\r\nINIT_LIST_HEAD(&desc->node);\r\nreturn desc;\r\n}\r\nstatic void d40_desc_free(struct d40_chan *d40c, struct d40_desc *d40d)\r\n{\r\nd40_pool_lli_free(d40c, d40d);\r\nd40_lcla_free_all(d40c, d40d);\r\nkmem_cache_free(d40c->base->desc_slab, d40d);\r\n}\r\nstatic void d40_desc_submit(struct d40_chan *d40c, struct d40_desc *desc)\r\n{\r\nlist_add_tail(&desc->node, &d40c->active);\r\n}\r\nstatic void d40_phy_lli_load(struct d40_chan *chan, struct d40_desc *desc)\r\n{\r\nstruct d40_phy_lli *lli_dst = desc->lli_phy.dst;\r\nstruct d40_phy_lli *lli_src = desc->lli_phy.src;\r\nvoid __iomem *base = chan_base(chan);\r\nwritel(lli_src->reg_cfg, base + D40_CHAN_REG_SSCFG);\r\nwritel(lli_src->reg_elt, base + D40_CHAN_REG_SSELT);\r\nwritel(lli_src->reg_ptr, base + D40_CHAN_REG_SSPTR);\r\nwritel(lli_src->reg_lnk, base + D40_CHAN_REG_SSLNK);\r\nwritel(lli_dst->reg_cfg, base + D40_CHAN_REG_SDCFG);\r\nwritel(lli_dst->reg_elt, base + D40_CHAN_REG_SDELT);\r\nwritel(lli_dst->reg_ptr, base + D40_CHAN_REG_SDPTR);\r\nwritel(lli_dst->reg_lnk, base + D40_CHAN_REG_SDLNK);\r\n}\r\nstatic void d40_desc_done(struct d40_chan *d40c, struct d40_desc *desc)\r\n{\r\nlist_add_tail(&desc->node, &d40c->done);\r\n}\r\nstatic void d40_log_lli_to_lcxa(struct d40_chan *chan, struct d40_desc *desc)\r\n{\r\nstruct d40_lcla_pool *pool = &chan->base->lcla_pool;\r\nstruct d40_log_lli_bidir *lli = &desc->lli_log;\r\nint lli_current = desc->lli_current;\r\nint lli_len = desc->lli_len;\r\nbool cyclic = desc->cyclic;\r\nint curr_lcla = -EINVAL;\r\nint first_lcla = 0;\r\nbool use_esram_lcla = chan->base->plat_data->use_esram_lcla;\r\nbool linkback;\r\nlinkback = cyclic && lli_current == 0;\r\nif (linkback || (lli_len - lli_current > 1)) {\r\nif (!(chan->phy_chan->use_soft_lli &&\r\nchan->dma_cfg.dir == DMA_DEV_TO_MEM))\r\ncurr_lcla = d40_lcla_alloc_one(chan, desc);\r\nfirst_lcla = curr_lcla;\r\n}\r\nif (!linkback || curr_lcla == -EINVAL) {\r\nunsigned int flags = 0;\r\nif (curr_lcla == -EINVAL)\r\nflags |= LLI_TERM_INT;\r\nd40_log_lli_lcpa_write(chan->lcpa,\r\n&lli->dst[lli_current],\r\n&lli->src[lli_current],\r\ncurr_lcla,\r\nflags);\r\nlli_current++;\r\n}\r\nif (curr_lcla < 0)\r\ngoto out;\r\nfor (; lli_current < lli_len; lli_current++) {\r\nunsigned int lcla_offset = chan->phy_chan->num * 1024 +\r\n8 * curr_lcla * 2;\r\nstruct d40_log_lli *lcla = pool->base + lcla_offset;\r\nunsigned int flags = 0;\r\nint next_lcla;\r\nif (lli_current + 1 < lli_len)\r\nnext_lcla = d40_lcla_alloc_one(chan, desc);\r\nelse\r\nnext_lcla = linkback ? first_lcla : -EINVAL;\r\nif (cyclic || next_lcla == -EINVAL)\r\nflags |= LLI_TERM_INT;\r\nif (linkback && curr_lcla == first_lcla) {\r\nd40_log_lli_lcpa_write(chan->lcpa,\r\n&lli->dst[lli_current],\r\n&lli->src[lli_current],\r\nnext_lcla, flags);\r\n}\r\nd40_log_lli_lcla_write(lcla,\r\n&lli->dst[lli_current],\r\n&lli->src[lli_current],\r\nnext_lcla, flags);\r\nif (!use_esram_lcla) {\r\ndma_sync_single_range_for_device(chan->base->dev,\r\npool->dma_addr, lcla_offset,\r\n2 * sizeof(struct d40_log_lli),\r\nDMA_TO_DEVICE);\r\n}\r\ncurr_lcla = next_lcla;\r\nif (curr_lcla == -EINVAL || curr_lcla == first_lcla) {\r\nlli_current++;\r\nbreak;\r\n}\r\n}\r\nout:\r\ndesc->lli_current = lli_current;\r\n}\r\nstatic void d40_desc_load(struct d40_chan *d40c, struct d40_desc *d40d)\r\n{\r\nif (chan_is_physical(d40c)) {\r\nd40_phy_lli_load(d40c, d40d);\r\nd40d->lli_current = d40d->lli_len;\r\n} else\r\nd40_log_lli_to_lcxa(d40c, d40d);\r\n}\r\nstatic struct d40_desc *d40_first_active_get(struct d40_chan *d40c)\r\n{\r\nstruct d40_desc *d;\r\nif (list_empty(&d40c->active))\r\nreturn NULL;\r\nd = list_first_entry(&d40c->active,\r\nstruct d40_desc,\r\nnode);\r\nreturn d;\r\n}\r\nstatic void d40_desc_queue(struct d40_chan *d40c, struct d40_desc *desc)\r\n{\r\nd40_desc_remove(desc);\r\ndesc->is_in_client_list = false;\r\nlist_add_tail(&desc->node, &d40c->pending_queue);\r\n}\r\nstatic struct d40_desc *d40_first_pending(struct d40_chan *d40c)\r\n{\r\nstruct d40_desc *d;\r\nif (list_empty(&d40c->pending_queue))\r\nreturn NULL;\r\nd = list_first_entry(&d40c->pending_queue,\r\nstruct d40_desc,\r\nnode);\r\nreturn d;\r\n}\r\nstatic struct d40_desc *d40_first_queued(struct d40_chan *d40c)\r\n{\r\nstruct d40_desc *d;\r\nif (list_empty(&d40c->queue))\r\nreturn NULL;\r\nd = list_first_entry(&d40c->queue,\r\nstruct d40_desc,\r\nnode);\r\nreturn d;\r\n}\r\nstatic struct d40_desc *d40_first_done(struct d40_chan *d40c)\r\n{\r\nif (list_empty(&d40c->done))\r\nreturn NULL;\r\nreturn list_first_entry(&d40c->done, struct d40_desc, node);\r\n}\r\nstatic int d40_psize_2_burst_size(bool is_log, int psize)\r\n{\r\nif (is_log) {\r\nif (psize == STEDMA40_PSIZE_LOG_1)\r\nreturn 1;\r\n} else {\r\nif (psize == STEDMA40_PSIZE_PHY_1)\r\nreturn 1;\r\n}\r\nreturn 2 << psize;\r\n}\r\nstatic int d40_size_2_dmalen(int size, u32 data_width1, u32 data_width2)\r\n{\r\nint dmalen;\r\nu32 max_w = max(data_width1, data_width2);\r\nu32 min_w = min(data_width1, data_width2);\r\nu32 seg_max = ALIGN(STEDMA40_MAX_SEG_SIZE * min_w, max_w);\r\nif (seg_max > STEDMA40_MAX_SEG_SIZE)\r\nseg_max -= max_w;\r\nif (!IS_ALIGNED(size, max_w))\r\nreturn -EINVAL;\r\nif (size <= seg_max)\r\ndmalen = 1;\r\nelse {\r\ndmalen = size / seg_max;\r\nif (dmalen * seg_max < size)\r\ndmalen++;\r\n}\r\nreturn dmalen;\r\n}\r\nstatic int d40_sg_2_dmalen(struct scatterlist *sgl, int sg_len,\r\nu32 data_width1, u32 data_width2)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nint len = 0;\r\nint ret;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nret = d40_size_2_dmalen(sg_dma_len(sg),\r\ndata_width1, data_width2);\r\nif (ret < 0)\r\nreturn ret;\r\nlen += ret;\r\n}\r\nreturn len;\r\n}\r\nstatic void dma40_backup(void __iomem *baseaddr, u32 *backup,\r\nu32 *regaddr, int num, bool save)\r\n{\r\nint i;\r\nfor (i = 0; i < num; i++) {\r\nvoid __iomem *addr = baseaddr + regaddr[i];\r\nif (save)\r\nbackup[i] = readl_relaxed(addr);\r\nelse\r\nwritel_relaxed(backup[i], addr);\r\n}\r\n}\r\nstatic void d40_save_restore_registers(struct d40_base *base, bool save)\r\n{\r\nint i;\r\nfor (i = 0; i < base->num_phy_chans; i++) {\r\nvoid __iomem *addr;\r\nint idx;\r\nif (base->phy_res[i].reserved)\r\ncontinue;\r\naddr = base->virtbase + D40_DREG_PCBASE + i * D40_DREG_PCDELTA;\r\nidx = i * ARRAY_SIZE(d40_backup_regs_chan);\r\ndma40_backup(addr, &base->reg_val_backup_chan[idx],\r\nd40_backup_regs_chan,\r\nARRAY_SIZE(d40_backup_regs_chan),\r\nsave);\r\n}\r\ndma40_backup(base->virtbase, base->reg_val_backup,\r\nd40_backup_regs, ARRAY_SIZE(d40_backup_regs),\r\nsave);\r\nif (base->gen_dmac.backup)\r\ndma40_backup(base->virtbase, base->reg_val_backup_v4,\r\nbase->gen_dmac.backup,\r\nbase->gen_dmac.backup_size,\r\nsave);\r\n}\r\nstatic void d40_save_restore_registers(struct d40_base *base, bool save)\r\n{\r\n}\r\nstatic int __d40_execute_command_phy(struct d40_chan *d40c,\r\nenum d40_command command)\r\n{\r\nu32 status;\r\nint i;\r\nvoid __iomem *active_reg;\r\nint ret = 0;\r\nunsigned long flags;\r\nu32 wmask;\r\nif (command == D40_DMA_STOP) {\r\nret = __d40_execute_command_phy(d40c, D40_DMA_SUSPEND_REQ);\r\nif (ret)\r\nreturn ret;\r\n}\r\nspin_lock_irqsave(&d40c->base->execmd_lock, flags);\r\nif (d40c->phy_chan->num % 2 == 0)\r\nactive_reg = d40c->base->virtbase + D40_DREG_ACTIVE;\r\nelse\r\nactive_reg = d40c->base->virtbase + D40_DREG_ACTIVO;\r\nif (command == D40_DMA_SUSPEND_REQ) {\r\nstatus = (readl(active_reg) &\r\nD40_CHAN_POS_MASK(d40c->phy_chan->num)) >>\r\nD40_CHAN_POS(d40c->phy_chan->num);\r\nif (status == D40_DMA_SUSPENDED || status == D40_DMA_STOP)\r\ngoto done;\r\n}\r\nwmask = 0xffffffff & ~(D40_CHAN_POS_MASK(d40c->phy_chan->num));\r\nwritel(wmask | (command << D40_CHAN_POS(d40c->phy_chan->num)),\r\nactive_reg);\r\nif (command == D40_DMA_SUSPEND_REQ) {\r\nfor (i = 0 ; i < D40_SUSPEND_MAX_IT; i++) {\r\nstatus = (readl(active_reg) &\r\nD40_CHAN_POS_MASK(d40c->phy_chan->num)) >>\r\nD40_CHAN_POS(d40c->phy_chan->num);\r\ncpu_relax();\r\nudelay(3);\r\nif (status == D40_DMA_STOP ||\r\nstatus == D40_DMA_SUSPENDED)\r\nbreak;\r\n}\r\nif (i == D40_SUSPEND_MAX_IT) {\r\nchan_err(d40c,\r\n"unable to suspend the chl %d (log: %d) status %x\n",\r\nd40c->phy_chan->num, d40c->log_num,\r\nstatus);\r\ndump_stack();\r\nret = -EBUSY;\r\n}\r\n}\r\ndone:\r\nspin_unlock_irqrestore(&d40c->base->execmd_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void d40_term_all(struct d40_chan *d40c)\r\n{\r\nstruct d40_desc *d40d;\r\nstruct d40_desc *_d;\r\nwhile ((d40d = d40_first_done(d40c))) {\r\nd40_desc_remove(d40d);\r\nd40_desc_free(d40c, d40d);\r\n}\r\nwhile ((d40d = d40_first_active_get(d40c))) {\r\nd40_desc_remove(d40d);\r\nd40_desc_free(d40c, d40d);\r\n}\r\nwhile ((d40d = d40_first_queued(d40c))) {\r\nd40_desc_remove(d40d);\r\nd40_desc_free(d40c, d40d);\r\n}\r\nwhile ((d40d = d40_first_pending(d40c))) {\r\nd40_desc_remove(d40d);\r\nd40_desc_free(d40c, d40d);\r\n}\r\nif (!list_empty(&d40c->client))\r\nlist_for_each_entry_safe(d40d, _d, &d40c->client, node) {\r\nd40_desc_remove(d40d);\r\nd40_desc_free(d40c, d40d);\r\n}\r\nif (!list_empty(&d40c->prepare_queue))\r\nlist_for_each_entry_safe(d40d, _d,\r\n&d40c->prepare_queue, node) {\r\nd40_desc_remove(d40d);\r\nd40_desc_free(d40c, d40d);\r\n}\r\nd40c->pending_tx = 0;\r\n}\r\nstatic void __d40_config_set_event(struct d40_chan *d40c,\r\nenum d40_events event_type, u32 event,\r\nint reg)\r\n{\r\nvoid __iomem *addr = chan_base(d40c) + reg;\r\nint tries;\r\nu32 status;\r\nswitch (event_type) {\r\ncase D40_DEACTIVATE_EVENTLINE:\r\nwritel((D40_DEACTIVATE_EVENTLINE << D40_EVENTLINE_POS(event))\r\n| ~D40_EVENTLINE_MASK(event), addr);\r\nbreak;\r\ncase D40_SUSPEND_REQ_EVENTLINE:\r\nstatus = (readl(addr) & D40_EVENTLINE_MASK(event)) >>\r\nD40_EVENTLINE_POS(event);\r\nif (status == D40_DEACTIVATE_EVENTLINE ||\r\nstatus == D40_SUSPEND_REQ_EVENTLINE)\r\nbreak;\r\nwritel((D40_SUSPEND_REQ_EVENTLINE << D40_EVENTLINE_POS(event))\r\n| ~D40_EVENTLINE_MASK(event), addr);\r\nfor (tries = 0 ; tries < D40_SUSPEND_MAX_IT; tries++) {\r\nstatus = (readl(addr) & D40_EVENTLINE_MASK(event)) >>\r\nD40_EVENTLINE_POS(event);\r\ncpu_relax();\r\nudelay(3);\r\nif (status == D40_DEACTIVATE_EVENTLINE)\r\nbreak;\r\n}\r\nif (tries == D40_SUSPEND_MAX_IT) {\r\nchan_err(d40c,\r\n"unable to stop the event_line chl %d (log: %d)"\r\n"status %x\n", d40c->phy_chan->num,\r\nd40c->log_num, status);\r\n}\r\nbreak;\r\ncase D40_ACTIVATE_EVENTLINE:\r\ntries = 100;\r\nwhile (--tries) {\r\nwritel((D40_ACTIVATE_EVENTLINE <<\r\nD40_EVENTLINE_POS(event)) |\r\n~D40_EVENTLINE_MASK(event), addr);\r\nif (readl(addr) & D40_EVENTLINE_MASK(event))\r\nbreak;\r\n}\r\nif (tries != 99)\r\ndev_dbg(chan2dev(d40c),\r\n"[%s] workaround enable S%cLNK (%d tries)\n",\r\n__func__, reg == D40_CHAN_REG_SSLNK ? 'S' : 'D',\r\n100 - tries);\r\nWARN_ON(!tries);\r\nbreak;\r\ncase D40_ROUND_EVENTLINE:\r\nBUG();\r\nbreak;\r\n}\r\n}\r\nstatic void d40_config_set_event(struct d40_chan *d40c,\r\nenum d40_events event_type)\r\n{\r\nu32 event = D40_TYPE_TO_EVENT(d40c->dma_cfg.dev_type);\r\nif ((d40c->dma_cfg.dir == DMA_DEV_TO_MEM) ||\r\n(d40c->dma_cfg.dir == DMA_DEV_TO_DEV))\r\n__d40_config_set_event(d40c, event_type, event,\r\nD40_CHAN_REG_SSLNK);\r\nif (d40c->dma_cfg.dir != DMA_DEV_TO_MEM)\r\n__d40_config_set_event(d40c, event_type, event,\r\nD40_CHAN_REG_SDLNK);\r\n}\r\nstatic u32 d40_chan_has_events(struct d40_chan *d40c)\r\n{\r\nvoid __iomem *chanbase = chan_base(d40c);\r\nu32 val;\r\nval = readl(chanbase + D40_CHAN_REG_SSLNK);\r\nval |= readl(chanbase + D40_CHAN_REG_SDLNK);\r\nreturn val;\r\n}\r\nstatic int\r\n__d40_execute_command_log(struct d40_chan *d40c, enum d40_command command)\r\n{\r\nunsigned long flags;\r\nint ret = 0;\r\nu32 active_status;\r\nvoid __iomem *active_reg;\r\nif (d40c->phy_chan->num % 2 == 0)\r\nactive_reg = d40c->base->virtbase + D40_DREG_ACTIVE;\r\nelse\r\nactive_reg = d40c->base->virtbase + D40_DREG_ACTIVO;\r\nspin_lock_irqsave(&d40c->phy_chan->lock, flags);\r\nswitch (command) {\r\ncase D40_DMA_STOP:\r\ncase D40_DMA_SUSPEND_REQ:\r\nactive_status = (readl(active_reg) &\r\nD40_CHAN_POS_MASK(d40c->phy_chan->num)) >>\r\nD40_CHAN_POS(d40c->phy_chan->num);\r\nif (active_status == D40_DMA_RUN)\r\nd40_config_set_event(d40c, D40_SUSPEND_REQ_EVENTLINE);\r\nelse\r\nd40_config_set_event(d40c, D40_DEACTIVATE_EVENTLINE);\r\nif (!d40_chan_has_events(d40c) && (command == D40_DMA_STOP))\r\nret = __d40_execute_command_phy(d40c, command);\r\nbreak;\r\ncase D40_DMA_RUN:\r\nd40_config_set_event(d40c, D40_ACTIVATE_EVENTLINE);\r\nret = __d40_execute_command_phy(d40c, command);\r\nbreak;\r\ncase D40_DMA_SUSPENDED:\r\nBUG();\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&d40c->phy_chan->lock, flags);\r\nreturn ret;\r\n}\r\nstatic int d40_channel_execute_command(struct d40_chan *d40c,\r\nenum d40_command command)\r\n{\r\nif (chan_is_logical(d40c))\r\nreturn __d40_execute_command_log(d40c, command);\r\nelse\r\nreturn __d40_execute_command_phy(d40c, command);\r\n}\r\nstatic u32 d40_get_prmo(struct d40_chan *d40c)\r\n{\r\nstatic const unsigned int phy_map[] = {\r\n[STEDMA40_PCHAN_BASIC_MODE]\r\n= D40_DREG_PRMO_PCHAN_BASIC,\r\n[STEDMA40_PCHAN_MODULO_MODE]\r\n= D40_DREG_PRMO_PCHAN_MODULO,\r\n[STEDMA40_PCHAN_DOUBLE_DST_MODE]\r\n= D40_DREG_PRMO_PCHAN_DOUBLE_DST,\r\n};\r\nstatic const unsigned int log_map[] = {\r\n[STEDMA40_LCHAN_SRC_PHY_DST_LOG]\r\n= D40_DREG_PRMO_LCHAN_SRC_PHY_DST_LOG,\r\n[STEDMA40_LCHAN_SRC_LOG_DST_PHY]\r\n= D40_DREG_PRMO_LCHAN_SRC_LOG_DST_PHY,\r\n[STEDMA40_LCHAN_SRC_LOG_DST_LOG]\r\n= D40_DREG_PRMO_LCHAN_SRC_LOG_DST_LOG,\r\n};\r\nif (chan_is_physical(d40c))\r\nreturn phy_map[d40c->dma_cfg.mode_opt];\r\nelse\r\nreturn log_map[d40c->dma_cfg.mode_opt];\r\n}\r\nstatic void d40_config_write(struct d40_chan *d40c)\r\n{\r\nu32 addr_base;\r\nu32 var;\r\naddr_base = (d40c->phy_chan->num % 2) * 4;\r\nvar = ((u32)(chan_is_logical(d40c)) + 1) <<\r\nD40_CHAN_POS(d40c->phy_chan->num);\r\nwritel(var, d40c->base->virtbase + D40_DREG_PRMSE + addr_base);\r\nvar = d40_get_prmo(d40c) << D40_CHAN_POS(d40c->phy_chan->num);\r\nwritel(var, d40c->base->virtbase + D40_DREG_PRMOE + addr_base);\r\nif (chan_is_logical(d40c)) {\r\nint lidx = (d40c->phy_chan->num << D40_SREG_ELEM_LOG_LIDX_POS)\r\n& D40_SREG_ELEM_LOG_LIDX_MASK;\r\nvoid __iomem *chanbase = chan_base(d40c);\r\nwritel(d40c->src_def_cfg, chanbase + D40_CHAN_REG_SSCFG);\r\nwritel(d40c->dst_def_cfg, chanbase + D40_CHAN_REG_SDCFG);\r\nwritel(lidx, chanbase + D40_CHAN_REG_SSELT);\r\nwritel(lidx, chanbase + D40_CHAN_REG_SDELT);\r\nwritel(0, chanbase + D40_CHAN_REG_SSLNK);\r\nwritel(0, chanbase + D40_CHAN_REG_SDLNK);\r\n}\r\n}\r\nstatic u32 d40_residue(struct d40_chan *d40c)\r\n{\r\nu32 num_elt;\r\nif (chan_is_logical(d40c))\r\nnum_elt = (readl(&d40c->lcpa->lcsp2) & D40_MEM_LCSP2_ECNT_MASK)\r\n>> D40_MEM_LCSP2_ECNT_POS;\r\nelse {\r\nu32 val = readl(chan_base(d40c) + D40_CHAN_REG_SDELT);\r\nnum_elt = (val & D40_SREG_ELEM_PHY_ECNT_MASK)\r\n>> D40_SREG_ELEM_PHY_ECNT_POS;\r\n}\r\nreturn num_elt * d40c->dma_cfg.dst_info.data_width;\r\n}\r\nstatic bool d40_tx_is_linked(struct d40_chan *d40c)\r\n{\r\nbool is_link;\r\nif (chan_is_logical(d40c))\r\nis_link = readl(&d40c->lcpa->lcsp3) & D40_MEM_LCSP3_DLOS_MASK;\r\nelse\r\nis_link = readl(chan_base(d40c) + D40_CHAN_REG_SDLNK)\r\n& D40_SREG_LNK_PHYS_LNK_MASK;\r\nreturn is_link;\r\n}\r\nstatic int d40_pause(struct d40_chan *d40c)\r\n{\r\nint res = 0;\r\nunsigned long flags;\r\nif (!d40c->busy)\r\nreturn 0;\r\npm_runtime_get_sync(d40c->base->dev);\r\nspin_lock_irqsave(&d40c->lock, flags);\r\nres = d40_channel_execute_command(d40c, D40_DMA_SUSPEND_REQ);\r\npm_runtime_mark_last_busy(d40c->base->dev);\r\npm_runtime_put_autosuspend(d40c->base->dev);\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\nreturn res;\r\n}\r\nstatic int d40_resume(struct d40_chan *d40c)\r\n{\r\nint res = 0;\r\nunsigned long flags;\r\nif (!d40c->busy)\r\nreturn 0;\r\nspin_lock_irqsave(&d40c->lock, flags);\r\npm_runtime_get_sync(d40c->base->dev);\r\nif (d40_residue(d40c) || d40_tx_is_linked(d40c))\r\nres = d40_channel_execute_command(d40c, D40_DMA_RUN);\r\npm_runtime_mark_last_busy(d40c->base->dev);\r\npm_runtime_put_autosuspend(d40c->base->dev);\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\nreturn res;\r\n}\r\nstatic dma_cookie_t d40_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct d40_chan *d40c = container_of(tx->chan,\r\nstruct d40_chan,\r\nchan);\r\nstruct d40_desc *d40d = container_of(tx, struct d40_desc, txd);\r\nunsigned long flags;\r\ndma_cookie_t cookie;\r\nspin_lock_irqsave(&d40c->lock, flags);\r\ncookie = dma_cookie_assign(tx);\r\nd40_desc_queue(d40c, d40d);\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic int d40_start(struct d40_chan *d40c)\r\n{\r\nreturn d40_channel_execute_command(d40c, D40_DMA_RUN);\r\n}\r\nstatic struct d40_desc *d40_queue_start(struct d40_chan *d40c)\r\n{\r\nstruct d40_desc *d40d;\r\nint err;\r\nd40d = d40_first_queued(d40c);\r\nif (d40d != NULL) {\r\nif (!d40c->busy) {\r\nd40c->busy = true;\r\npm_runtime_get_sync(d40c->base->dev);\r\n}\r\nd40_desc_remove(d40d);\r\nd40_desc_submit(d40c, d40d);\r\nd40_desc_load(d40c, d40d);\r\nerr = d40_start(d40c);\r\nif (err)\r\nreturn NULL;\r\n}\r\nreturn d40d;\r\n}\r\nstatic void dma_tc_handle(struct d40_chan *d40c)\r\n{\r\nstruct d40_desc *d40d;\r\nd40d = d40_first_active_get(d40c);\r\nif (d40d == NULL)\r\nreturn;\r\nif (d40d->cyclic) {\r\nif (d40d->lli_current < d40d->lli_len\r\n&& !d40_tx_is_linked(d40c)\r\n&& !d40_residue(d40c)) {\r\nd40_lcla_free_all(d40c, d40d);\r\nd40_desc_load(d40c, d40d);\r\n(void) d40_start(d40c);\r\nif (d40d->lli_current == d40d->lli_len)\r\nd40d->lli_current = 0;\r\n}\r\n} else {\r\nd40_lcla_free_all(d40c, d40d);\r\nif (d40d->lli_current < d40d->lli_len) {\r\nd40_desc_load(d40c, d40d);\r\n(void) d40_start(d40c);\r\nreturn;\r\n}\r\nif (d40_queue_start(d40c) == NULL) {\r\nd40c->busy = false;\r\npm_runtime_mark_last_busy(d40c->base->dev);\r\npm_runtime_put_autosuspend(d40c->base->dev);\r\n}\r\nd40_desc_remove(d40d);\r\nd40_desc_done(d40c, d40d);\r\n}\r\nd40c->pending_tx++;\r\ntasklet_schedule(&d40c->tasklet);\r\n}\r\nstatic void dma_tasklet(unsigned long data)\r\n{\r\nstruct d40_chan *d40c = (struct d40_chan *) data;\r\nstruct d40_desc *d40d;\r\nunsigned long flags;\r\nbool callback_active;\r\ndma_async_tx_callback callback;\r\nvoid *callback_param;\r\nspin_lock_irqsave(&d40c->lock, flags);\r\nd40d = d40_first_done(d40c);\r\nif (d40d == NULL) {\r\nd40d = d40_first_active_get(d40c);\r\nif (d40d == NULL || !d40d->cyclic)\r\ngoto err;\r\n}\r\nif (!d40d->cyclic)\r\ndma_cookie_complete(&d40d->txd);\r\nif (d40c->pending_tx == 0) {\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\nreturn;\r\n}\r\ncallback_active = !!(d40d->txd.flags & DMA_PREP_INTERRUPT);\r\ncallback = d40d->txd.callback;\r\ncallback_param = d40d->txd.callback_param;\r\nif (!d40d->cyclic) {\r\nif (async_tx_test_ack(&d40d->txd)) {\r\nd40_desc_remove(d40d);\r\nd40_desc_free(d40c, d40d);\r\n} else if (!d40d->is_in_client_list) {\r\nd40_desc_remove(d40d);\r\nd40_lcla_free_all(d40c, d40d);\r\nlist_add_tail(&d40d->node, &d40c->client);\r\nd40d->is_in_client_list = true;\r\n}\r\n}\r\nd40c->pending_tx--;\r\nif (d40c->pending_tx)\r\ntasklet_schedule(&d40c->tasklet);\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\nif (callback_active && callback)\r\ncallback(callback_param);\r\nreturn;\r\nerr:\r\nif (d40c->pending_tx > 0)\r\nd40c->pending_tx--;\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\n}\r\nstatic irqreturn_t d40_handle_interrupt(int irq, void *data)\r\n{\r\nint i;\r\nu32 idx;\r\nu32 row;\r\nlong chan = -1;\r\nstruct d40_chan *d40c;\r\nunsigned long flags;\r\nstruct d40_base *base = data;\r\nu32 regs[base->gen_dmac.il_size];\r\nstruct d40_interrupt_lookup *il = base->gen_dmac.il;\r\nu32 il_size = base->gen_dmac.il_size;\r\nspin_lock_irqsave(&base->interrupt_lock, flags);\r\nfor (i = 0; i < il_size; i++)\r\nregs[i] = readl(base->virtbase + il[i].src);\r\nfor (;;) {\r\nchan = find_next_bit((unsigned long *)regs,\r\nBITS_PER_LONG * il_size, chan + 1);\r\nif (chan == BITS_PER_LONG * il_size)\r\nbreak;\r\nrow = chan / BITS_PER_LONG;\r\nidx = chan & (BITS_PER_LONG - 1);\r\nif (il[row].offset == D40_PHY_CHAN)\r\nd40c = base->lookup_phy_chans[idx];\r\nelse\r\nd40c = base->lookup_log_chans[il[row].offset + idx];\r\nif (!d40c) {\r\ncontinue;\r\n}\r\nwritel(BIT(idx), base->virtbase + il[row].clr);\r\nspin_lock(&d40c->lock);\r\nif (!il[row].is_error)\r\ndma_tc_handle(d40c);\r\nelse\r\nd40_err(base->dev, "IRQ chan: %ld offset %d idx %d\n",\r\nchan, il[row].offset, idx);\r\nspin_unlock(&d40c->lock);\r\n}\r\nspin_unlock_irqrestore(&base->interrupt_lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int d40_validate_conf(struct d40_chan *d40c,\r\nstruct stedma40_chan_cfg *conf)\r\n{\r\nint res = 0;\r\nbool is_log = conf->mode == STEDMA40_MODE_LOGICAL;\r\nif (!conf->dir) {\r\nchan_err(d40c, "Invalid direction.\n");\r\nres = -EINVAL;\r\n}\r\nif ((is_log && conf->dev_type > d40c->base->num_log_chans) ||\r\n(!is_log && conf->dev_type > d40c->base->num_phy_chans) ||\r\n(conf->dev_type < 0)) {\r\nchan_err(d40c, "Invalid device type (%d)\n", conf->dev_type);\r\nres = -EINVAL;\r\n}\r\nif (conf->dir == DMA_DEV_TO_DEV) {\r\nchan_err(d40c, "periph to periph not supported\n");\r\nres = -EINVAL;\r\n}\r\nif (d40_psize_2_burst_size(is_log, conf->src_info.psize) *\r\nconf->src_info.data_width !=\r\nd40_psize_2_burst_size(is_log, conf->dst_info.psize) *\r\nconf->dst_info.data_width) {\r\nchan_err(d40c, "src (burst x width) != dst (burst x width)\n");\r\nres = -EINVAL;\r\n}\r\nreturn res;\r\n}\r\nstatic bool d40_alloc_mask_set(struct d40_phy_res *phy,\r\nbool is_src, int log_event_line, bool is_log,\r\nbool *first_user)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&phy->lock, flags);\r\n*first_user = ((phy->allocated_src | phy->allocated_dst)\r\n== D40_ALLOC_FREE);\r\nif (!is_log) {\r\nif (phy->allocated_src == D40_ALLOC_FREE &&\r\nphy->allocated_dst == D40_ALLOC_FREE) {\r\nphy->allocated_dst = D40_ALLOC_PHY;\r\nphy->allocated_src = D40_ALLOC_PHY;\r\ngoto found;\r\n} else\r\ngoto not_found;\r\n}\r\nif (is_src) {\r\nif (phy->allocated_src == D40_ALLOC_PHY)\r\ngoto not_found;\r\nif (phy->allocated_src == D40_ALLOC_FREE)\r\nphy->allocated_src = D40_ALLOC_LOG_FREE;\r\nif (!(phy->allocated_src & BIT(log_event_line))) {\r\nphy->allocated_src |= BIT(log_event_line);\r\ngoto found;\r\n} else\r\ngoto not_found;\r\n} else {\r\nif (phy->allocated_dst == D40_ALLOC_PHY)\r\ngoto not_found;\r\nif (phy->allocated_dst == D40_ALLOC_FREE)\r\nphy->allocated_dst = D40_ALLOC_LOG_FREE;\r\nif (!(phy->allocated_dst & BIT(log_event_line))) {\r\nphy->allocated_dst |= BIT(log_event_line);\r\ngoto found;\r\n} else\r\ngoto not_found;\r\n}\r\nnot_found:\r\nspin_unlock_irqrestore(&phy->lock, flags);\r\nreturn false;\r\nfound:\r\nspin_unlock_irqrestore(&phy->lock, flags);\r\nreturn true;\r\n}\r\nstatic bool d40_alloc_mask_free(struct d40_phy_res *phy, bool is_src,\r\nint log_event_line)\r\n{\r\nunsigned long flags;\r\nbool is_free = false;\r\nspin_lock_irqsave(&phy->lock, flags);\r\nif (!log_event_line) {\r\nphy->allocated_dst = D40_ALLOC_FREE;\r\nphy->allocated_src = D40_ALLOC_FREE;\r\nis_free = true;\r\ngoto out;\r\n}\r\nif (is_src) {\r\nphy->allocated_src &= ~BIT(log_event_line);\r\nif (phy->allocated_src == D40_ALLOC_LOG_FREE)\r\nphy->allocated_src = D40_ALLOC_FREE;\r\n} else {\r\nphy->allocated_dst &= ~BIT(log_event_line);\r\nif (phy->allocated_dst == D40_ALLOC_LOG_FREE)\r\nphy->allocated_dst = D40_ALLOC_FREE;\r\n}\r\nis_free = ((phy->allocated_src | phy->allocated_dst) ==\r\nD40_ALLOC_FREE);\r\nout:\r\nspin_unlock_irqrestore(&phy->lock, flags);\r\nreturn is_free;\r\n}\r\nstatic int d40_allocate_channel(struct d40_chan *d40c, bool *first_phy_user)\r\n{\r\nint dev_type = d40c->dma_cfg.dev_type;\r\nint event_group;\r\nint event_line;\r\nstruct d40_phy_res *phys;\r\nint i;\r\nint j;\r\nint log_num;\r\nint num_phy_chans;\r\nbool is_src;\r\nbool is_log = d40c->dma_cfg.mode == STEDMA40_MODE_LOGICAL;\r\nphys = d40c->base->phy_res;\r\nnum_phy_chans = d40c->base->num_phy_chans;\r\nif (d40c->dma_cfg.dir == DMA_DEV_TO_MEM) {\r\nlog_num = 2 * dev_type;\r\nis_src = true;\r\n} else if (d40c->dma_cfg.dir == DMA_MEM_TO_DEV ||\r\nd40c->dma_cfg.dir == DMA_MEM_TO_MEM) {\r\nlog_num = 2 * dev_type + 1;\r\nis_src = false;\r\n} else\r\nreturn -EINVAL;\r\nevent_group = D40_TYPE_TO_GROUP(dev_type);\r\nevent_line = D40_TYPE_TO_EVENT(dev_type);\r\nif (!is_log) {\r\nif (d40c->dma_cfg.dir == DMA_MEM_TO_MEM) {\r\nif (d40c->dma_cfg.use_fixed_channel) {\r\ni = d40c->dma_cfg.phy_channel;\r\nif (d40_alloc_mask_set(&phys[i], is_src,\r\n0, is_log,\r\nfirst_phy_user))\r\ngoto found_phy;\r\n} else {\r\nfor (i = 0; i < num_phy_chans; i++) {\r\nif (d40_alloc_mask_set(&phys[i], is_src,\r\n0, is_log,\r\nfirst_phy_user))\r\ngoto found_phy;\r\n}\r\n}\r\n} else\r\nfor (j = 0; j < d40c->base->num_phy_chans; j += 8) {\r\nint phy_num = j + event_group * 2;\r\nfor (i = phy_num; i < phy_num + 2; i++) {\r\nif (d40_alloc_mask_set(&phys[i],\r\nis_src,\r\n0,\r\nis_log,\r\nfirst_phy_user))\r\ngoto found_phy;\r\n}\r\n}\r\nreturn -EINVAL;\r\nfound_phy:\r\nd40c->phy_chan = &phys[i];\r\nd40c->log_num = D40_PHY_CHAN;\r\ngoto out;\r\n}\r\nif (dev_type == -1)\r\nreturn -EINVAL;\r\nfor (j = 0; j < d40c->base->num_phy_chans; j += 8) {\r\nint phy_num = j + event_group * 2;\r\nif (d40c->dma_cfg.use_fixed_channel) {\r\ni = d40c->dma_cfg.phy_channel;\r\nif ((i != phy_num) && (i != phy_num + 1)) {\r\ndev_err(chan2dev(d40c),\r\n"invalid fixed phy channel %d\n", i);\r\nreturn -EINVAL;\r\n}\r\nif (d40_alloc_mask_set(&phys[i], is_src, event_line,\r\nis_log, first_phy_user))\r\ngoto found_log;\r\ndev_err(chan2dev(d40c),\r\n"could not allocate fixed phy channel %d\n", i);\r\nreturn -EINVAL;\r\n}\r\nif (is_src) {\r\nfor (i = phy_num; i < phy_num + 2; i++) {\r\nif (d40_alloc_mask_set(&phys[i], is_src,\r\nevent_line, is_log,\r\nfirst_phy_user))\r\ngoto found_log;\r\n}\r\n} else {\r\nfor (i = phy_num + 1; i >= phy_num; i--) {\r\nif (d40_alloc_mask_set(&phys[i], is_src,\r\nevent_line, is_log,\r\nfirst_phy_user))\r\ngoto found_log;\r\n}\r\n}\r\n}\r\nreturn -EINVAL;\r\nfound_log:\r\nd40c->phy_chan = &phys[i];\r\nd40c->log_num = log_num;\r\nout:\r\nif (is_log)\r\nd40c->base->lookup_log_chans[d40c->log_num] = d40c;\r\nelse\r\nd40c->base->lookup_phy_chans[d40c->phy_chan->num] = d40c;\r\nreturn 0;\r\n}\r\nstatic int d40_config_memcpy(struct d40_chan *d40c)\r\n{\r\ndma_cap_mask_t cap = d40c->chan.device->cap_mask;\r\nif (dma_has_cap(DMA_MEMCPY, cap) && !dma_has_cap(DMA_SLAVE, cap)) {\r\nd40c->dma_cfg = dma40_memcpy_conf_log;\r\nd40c->dma_cfg.dev_type = dma40_memcpy_channels[d40c->chan.chan_id];\r\nd40_log_cfg(&d40c->dma_cfg,\r\n&d40c->log_def.lcsp1, &d40c->log_def.lcsp3);\r\n} else if (dma_has_cap(DMA_MEMCPY, cap) &&\r\ndma_has_cap(DMA_SLAVE, cap)) {\r\nd40c->dma_cfg = dma40_memcpy_conf_phy;\r\nd40c->dst_def_cfg |= BIT(D40_SREG_CFG_TIM_POS);\r\nd40c->src_def_cfg |= BIT(D40_SREG_CFG_EIM_POS);\r\nd40c->dst_def_cfg |= BIT(D40_SREG_CFG_EIM_POS);\r\n} else {\r\nchan_err(d40c, "No memcpy\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int d40_free_dma(struct d40_chan *d40c)\r\n{\r\nint res = 0;\r\nu32 event = D40_TYPE_TO_EVENT(d40c->dma_cfg.dev_type);\r\nstruct d40_phy_res *phy = d40c->phy_chan;\r\nbool is_src;\r\nd40_term_all(d40c);\r\nif (phy == NULL) {\r\nchan_err(d40c, "phy == null\n");\r\nreturn -EINVAL;\r\n}\r\nif (phy->allocated_src == D40_ALLOC_FREE &&\r\nphy->allocated_dst == D40_ALLOC_FREE) {\r\nchan_err(d40c, "channel already free\n");\r\nreturn -EINVAL;\r\n}\r\nif (d40c->dma_cfg.dir == DMA_MEM_TO_DEV ||\r\nd40c->dma_cfg.dir == DMA_MEM_TO_MEM)\r\nis_src = false;\r\nelse if (d40c->dma_cfg.dir == DMA_DEV_TO_MEM)\r\nis_src = true;\r\nelse {\r\nchan_err(d40c, "Unknown direction\n");\r\nreturn -EINVAL;\r\n}\r\npm_runtime_get_sync(d40c->base->dev);\r\nres = d40_channel_execute_command(d40c, D40_DMA_STOP);\r\nif (res) {\r\nchan_err(d40c, "stop failed\n");\r\ngoto out;\r\n}\r\nd40_alloc_mask_free(phy, is_src, chan_is_logical(d40c) ? event : 0);\r\nif (chan_is_logical(d40c))\r\nd40c->base->lookup_log_chans[d40c->log_num] = NULL;\r\nelse\r\nd40c->base->lookup_phy_chans[phy->num] = NULL;\r\nif (d40c->busy) {\r\npm_runtime_mark_last_busy(d40c->base->dev);\r\npm_runtime_put_autosuspend(d40c->base->dev);\r\n}\r\nd40c->busy = false;\r\nd40c->phy_chan = NULL;\r\nd40c->configured = false;\r\nout:\r\npm_runtime_mark_last_busy(d40c->base->dev);\r\npm_runtime_put_autosuspend(d40c->base->dev);\r\nreturn res;\r\n}\r\nstatic bool d40_is_paused(struct d40_chan *d40c)\r\n{\r\nvoid __iomem *chanbase = chan_base(d40c);\r\nbool is_paused = false;\r\nunsigned long flags;\r\nvoid __iomem *active_reg;\r\nu32 status;\r\nu32 event = D40_TYPE_TO_EVENT(d40c->dma_cfg.dev_type);\r\nspin_lock_irqsave(&d40c->lock, flags);\r\nif (chan_is_physical(d40c)) {\r\nif (d40c->phy_chan->num % 2 == 0)\r\nactive_reg = d40c->base->virtbase + D40_DREG_ACTIVE;\r\nelse\r\nactive_reg = d40c->base->virtbase + D40_DREG_ACTIVO;\r\nstatus = (readl(active_reg) &\r\nD40_CHAN_POS_MASK(d40c->phy_chan->num)) >>\r\nD40_CHAN_POS(d40c->phy_chan->num);\r\nif (status == D40_DMA_SUSPENDED || status == D40_DMA_STOP)\r\nis_paused = true;\r\ngoto _exit;\r\n}\r\nif (d40c->dma_cfg.dir == DMA_MEM_TO_DEV ||\r\nd40c->dma_cfg.dir == DMA_MEM_TO_MEM) {\r\nstatus = readl(chanbase + D40_CHAN_REG_SDLNK);\r\n} else if (d40c->dma_cfg.dir == DMA_DEV_TO_MEM) {\r\nstatus = readl(chanbase + D40_CHAN_REG_SSLNK);\r\n} else {\r\nchan_err(d40c, "Unknown direction\n");\r\ngoto _exit;\r\n}\r\nstatus = (status & D40_EVENTLINE_MASK(event)) >>\r\nD40_EVENTLINE_POS(event);\r\nif (status != D40_DMA_RUN)\r\nis_paused = true;\r\n_exit:\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\nreturn is_paused;\r\n}\r\nstatic u32 stedma40_residue(struct dma_chan *chan)\r\n{\r\nstruct d40_chan *d40c =\r\ncontainer_of(chan, struct d40_chan, chan);\r\nu32 bytes_left;\r\nunsigned long flags;\r\nspin_lock_irqsave(&d40c->lock, flags);\r\nbytes_left = d40_residue(d40c);\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\nreturn bytes_left;\r\n}\r\nstatic int\r\nd40_prep_sg_log(struct d40_chan *chan, struct d40_desc *desc,\r\nstruct scatterlist *sg_src, struct scatterlist *sg_dst,\r\nunsigned int sg_len, dma_addr_t src_dev_addr,\r\ndma_addr_t dst_dev_addr)\r\n{\r\nstruct stedma40_chan_cfg *cfg = &chan->dma_cfg;\r\nstruct stedma40_half_channel_info *src_info = &cfg->src_info;\r\nstruct stedma40_half_channel_info *dst_info = &cfg->dst_info;\r\nint ret;\r\nret = d40_log_sg_to_lli(sg_src, sg_len,\r\nsrc_dev_addr,\r\ndesc->lli_log.src,\r\nchan->log_def.lcsp1,\r\nsrc_info->data_width,\r\ndst_info->data_width);\r\nret = d40_log_sg_to_lli(sg_dst, sg_len,\r\ndst_dev_addr,\r\ndesc->lli_log.dst,\r\nchan->log_def.lcsp3,\r\ndst_info->data_width,\r\nsrc_info->data_width);\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic int\r\nd40_prep_sg_phy(struct d40_chan *chan, struct d40_desc *desc,\r\nstruct scatterlist *sg_src, struct scatterlist *sg_dst,\r\nunsigned int sg_len, dma_addr_t src_dev_addr,\r\ndma_addr_t dst_dev_addr)\r\n{\r\nstruct stedma40_chan_cfg *cfg = &chan->dma_cfg;\r\nstruct stedma40_half_channel_info *src_info = &cfg->src_info;\r\nstruct stedma40_half_channel_info *dst_info = &cfg->dst_info;\r\nunsigned long flags = 0;\r\nint ret;\r\nif (desc->cyclic)\r\nflags |= LLI_CYCLIC | LLI_TERM_INT;\r\nret = d40_phy_sg_to_lli(sg_src, sg_len, src_dev_addr,\r\ndesc->lli_phy.src,\r\nvirt_to_phys(desc->lli_phy.src),\r\nchan->src_def_cfg,\r\nsrc_info, dst_info, flags);\r\nret = d40_phy_sg_to_lli(sg_dst, sg_len, dst_dev_addr,\r\ndesc->lli_phy.dst,\r\nvirt_to_phys(desc->lli_phy.dst),\r\nchan->dst_def_cfg,\r\ndst_info, src_info, flags);\r\ndma_sync_single_for_device(chan->base->dev, desc->lli_pool.dma_addr,\r\ndesc->lli_pool.size, DMA_TO_DEVICE);\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic struct d40_desc *\r\nd40_prep_desc(struct d40_chan *chan, struct scatterlist *sg,\r\nunsigned int sg_len, unsigned long dma_flags)\r\n{\r\nstruct stedma40_chan_cfg *cfg = &chan->dma_cfg;\r\nstruct d40_desc *desc;\r\nint ret;\r\ndesc = d40_desc_get(chan);\r\nif (!desc)\r\nreturn NULL;\r\ndesc->lli_len = d40_sg_2_dmalen(sg, sg_len, cfg->src_info.data_width,\r\ncfg->dst_info.data_width);\r\nif (desc->lli_len < 0) {\r\nchan_err(chan, "Unaligned size\n");\r\ngoto err;\r\n}\r\nret = d40_pool_lli_alloc(chan, desc, desc->lli_len);\r\nif (ret < 0) {\r\nchan_err(chan, "Could not allocate lli\n");\r\ngoto err;\r\n}\r\ndesc->lli_current = 0;\r\ndesc->txd.flags = dma_flags;\r\ndesc->txd.tx_submit = d40_tx_submit;\r\ndma_async_tx_descriptor_init(&desc->txd, &chan->chan);\r\nreturn desc;\r\nerr:\r\nd40_desc_free(chan, desc);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nd40_prep_sg(struct dma_chan *dchan, struct scatterlist *sg_src,\r\nstruct scatterlist *sg_dst, unsigned int sg_len,\r\nenum dma_transfer_direction direction, unsigned long dma_flags)\r\n{\r\nstruct d40_chan *chan = container_of(dchan, struct d40_chan, chan);\r\ndma_addr_t src_dev_addr = 0;\r\ndma_addr_t dst_dev_addr = 0;\r\nstruct d40_desc *desc;\r\nunsigned long flags;\r\nint ret;\r\nif (!chan->phy_chan) {\r\nchan_err(chan, "Cannot prepare unallocated channel\n");\r\nreturn NULL;\r\n}\r\nspin_lock_irqsave(&chan->lock, flags);\r\ndesc = d40_prep_desc(chan, sg_src, sg_len, dma_flags);\r\nif (desc == NULL)\r\ngoto err;\r\nif (sg_next(&sg_src[sg_len - 1]) == sg_src)\r\ndesc->cyclic = true;\r\nif (direction == DMA_DEV_TO_MEM)\r\nsrc_dev_addr = chan->runtime_addr;\r\nelse if (direction == DMA_MEM_TO_DEV)\r\ndst_dev_addr = chan->runtime_addr;\r\nif (chan_is_logical(chan))\r\nret = d40_prep_sg_log(chan, desc, sg_src, sg_dst,\r\nsg_len, src_dev_addr, dst_dev_addr);\r\nelse\r\nret = d40_prep_sg_phy(chan, desc, sg_src, sg_dst,\r\nsg_len, src_dev_addr, dst_dev_addr);\r\nif (ret) {\r\nchan_err(chan, "Failed to prepare %s sg job: %d\n",\r\nchan_is_logical(chan) ? "log" : "phy", ret);\r\ngoto err;\r\n}\r\nlist_add_tail(&desc->node, &chan->prepare_queue);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn &desc->txd;\r\nerr:\r\nif (desc)\r\nd40_desc_free(chan, desc);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nreturn NULL;\r\n}\r\nbool stedma40_filter(struct dma_chan *chan, void *data)\r\n{\r\nstruct stedma40_chan_cfg *info = data;\r\nstruct d40_chan *d40c =\r\ncontainer_of(chan, struct d40_chan, chan);\r\nint err;\r\nif (data) {\r\nerr = d40_validate_conf(d40c, info);\r\nif (!err)\r\nd40c->dma_cfg = *info;\r\n} else\r\nerr = d40_config_memcpy(d40c);\r\nif (!err)\r\nd40c->configured = true;\r\nreturn err == 0;\r\n}\r\nstatic void __d40_set_prio_rt(struct d40_chan *d40c, int dev_type, bool src)\r\n{\r\nbool realtime = d40c->dma_cfg.realtime;\r\nbool highprio = d40c->dma_cfg.high_priority;\r\nu32 rtreg;\r\nu32 event = D40_TYPE_TO_EVENT(dev_type);\r\nu32 group = D40_TYPE_TO_GROUP(dev_type);\r\nu32 bit = BIT(event);\r\nu32 prioreg;\r\nstruct d40_gen_dmac *dmac = &d40c->base->gen_dmac;\r\nrtreg = realtime ? dmac->realtime_en : dmac->realtime_clear;\r\nif (!src && chan_is_logical(d40c))\r\nhighprio = false;\r\nprioreg = highprio ? dmac->high_prio_en : dmac->high_prio_clear;\r\nif (!src)\r\nbit <<= 16;\r\nwritel(bit, d40c->base->virtbase + prioreg + group * 4);\r\nwritel(bit, d40c->base->virtbase + rtreg + group * 4);\r\n}\r\nstatic void d40_set_prio_realtime(struct d40_chan *d40c)\r\n{\r\nif (d40c->base->rev < 3)\r\nreturn;\r\nif ((d40c->dma_cfg.dir == DMA_DEV_TO_MEM) ||\r\n(d40c->dma_cfg.dir == DMA_DEV_TO_DEV))\r\n__d40_set_prio_rt(d40c, d40c->dma_cfg.dev_type, true);\r\nif ((d40c->dma_cfg.dir == DMA_MEM_TO_DEV) ||\r\n(d40c->dma_cfg.dir == DMA_DEV_TO_DEV))\r\n__d40_set_prio_rt(d40c, d40c->dma_cfg.dev_type, false);\r\n}\r\nstatic struct dma_chan *d40_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct stedma40_chan_cfg cfg;\r\ndma_cap_mask_t cap;\r\nu32 flags;\r\nmemset(&cfg, 0, sizeof(struct stedma40_chan_cfg));\r\ndma_cap_zero(cap);\r\ndma_cap_set(DMA_SLAVE, cap);\r\ncfg.dev_type = dma_spec->args[0];\r\nflags = dma_spec->args[2];\r\nswitch (D40_DT_FLAGS_MODE(flags)) {\r\ncase 0: cfg.mode = STEDMA40_MODE_LOGICAL; break;\r\ncase 1: cfg.mode = STEDMA40_MODE_PHYSICAL; break;\r\n}\r\nswitch (D40_DT_FLAGS_DIR(flags)) {\r\ncase 0:\r\ncfg.dir = DMA_MEM_TO_DEV;\r\ncfg.dst_info.big_endian = D40_DT_FLAGS_BIG_ENDIAN(flags);\r\nbreak;\r\ncase 1:\r\ncfg.dir = DMA_DEV_TO_MEM;\r\ncfg.src_info.big_endian = D40_DT_FLAGS_BIG_ENDIAN(flags);\r\nbreak;\r\n}\r\nif (D40_DT_FLAGS_FIXED_CHAN(flags)) {\r\ncfg.phy_channel = dma_spec->args[1];\r\ncfg.use_fixed_channel = true;\r\n}\r\nif (D40_DT_FLAGS_HIGH_PRIO(flags))\r\ncfg.high_priority = true;\r\nreturn dma_request_channel(cap, stedma40_filter, &cfg);\r\n}\r\nstatic int d40_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nint err;\r\nunsigned long flags;\r\nstruct d40_chan *d40c =\r\ncontainer_of(chan, struct d40_chan, chan);\r\nbool is_free_phy;\r\nspin_lock_irqsave(&d40c->lock, flags);\r\ndma_cookie_init(chan);\r\nif (!d40c->configured) {\r\nerr = d40_config_memcpy(d40c);\r\nif (err) {\r\nchan_err(d40c, "Failed to configure memcpy channel\n");\r\ngoto fail;\r\n}\r\n}\r\nerr = d40_allocate_channel(d40c, &is_free_phy);\r\nif (err) {\r\nchan_err(d40c, "Failed to allocate channel\n");\r\nd40c->configured = false;\r\ngoto fail;\r\n}\r\npm_runtime_get_sync(d40c->base->dev);\r\nd40_set_prio_realtime(d40c);\r\nif (chan_is_logical(d40c)) {\r\nif (d40c->dma_cfg.dir == DMA_DEV_TO_MEM)\r\nd40c->lcpa = d40c->base->lcpa_base +\r\nd40c->dma_cfg.dev_type * D40_LCPA_CHAN_SIZE;\r\nelse\r\nd40c->lcpa = d40c->base->lcpa_base +\r\nd40c->dma_cfg.dev_type *\r\nD40_LCPA_CHAN_SIZE + D40_LCPA_CHAN_DST_DELTA;\r\nd40c->src_def_cfg |= BIT(D40_SREG_CFG_LOG_GIM_POS);\r\nd40c->dst_def_cfg |= BIT(D40_SREG_CFG_LOG_GIM_POS);\r\n}\r\ndev_dbg(chan2dev(d40c), "allocated %s channel (phy %d%s)\n",\r\nchan_is_logical(d40c) ? "logical" : "physical",\r\nd40c->phy_chan->num,\r\nd40c->dma_cfg.use_fixed_channel ? ", fixed" : "");\r\nif (is_free_phy)\r\nd40_config_write(d40c);\r\nfail:\r\npm_runtime_mark_last_busy(d40c->base->dev);\r\npm_runtime_put_autosuspend(d40c->base->dev);\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\nreturn err;\r\n}\r\nstatic void d40_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct d40_chan *d40c =\r\ncontainer_of(chan, struct d40_chan, chan);\r\nint err;\r\nunsigned long flags;\r\nif (d40c->phy_chan == NULL) {\r\nchan_err(d40c, "Cannot free unallocated channel\n");\r\nreturn;\r\n}\r\nspin_lock_irqsave(&d40c->lock, flags);\r\nerr = d40_free_dma(d40c);\r\nif (err)\r\nchan_err(d40c, "Failed to free channel\n");\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *d40_prep_memcpy(struct dma_chan *chan,\r\ndma_addr_t dst,\r\ndma_addr_t src,\r\nsize_t size,\r\nunsigned long dma_flags)\r\n{\r\nstruct scatterlist dst_sg;\r\nstruct scatterlist src_sg;\r\nsg_init_table(&dst_sg, 1);\r\nsg_init_table(&src_sg, 1);\r\nsg_dma_address(&dst_sg) = dst;\r\nsg_dma_address(&src_sg) = src;\r\nsg_dma_len(&dst_sg) = size;\r\nsg_dma_len(&src_sg) = size;\r\nreturn d40_prep_sg(chan, &src_sg, &dst_sg, 1, DMA_NONE, dma_flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nd40_prep_memcpy_sg(struct dma_chan *chan,\r\nstruct scatterlist *dst_sg, unsigned int dst_nents,\r\nstruct scatterlist *src_sg, unsigned int src_nents,\r\nunsigned long dma_flags)\r\n{\r\nif (dst_nents != src_nents)\r\nreturn NULL;\r\nreturn d40_prep_sg(chan, src_sg, dst_sg, src_nents, DMA_NONE, dma_flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nd40_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long dma_flags, void *context)\r\n{\r\nif (!is_slave_direction(direction))\r\nreturn NULL;\r\nreturn d40_prep_sg(chan, sgl, sgl, sg_len, direction, dma_flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\ndma40_prep_dma_cyclic(struct dma_chan *chan, dma_addr_t dma_addr,\r\nsize_t buf_len, size_t period_len,\r\nenum dma_transfer_direction direction, unsigned long flags,\r\nvoid *context)\r\n{\r\nunsigned int periods = buf_len / period_len;\r\nstruct dma_async_tx_descriptor *txd;\r\nstruct scatterlist *sg;\r\nint i;\r\nsg = kcalloc(periods + 1, sizeof(struct scatterlist), GFP_NOWAIT);\r\nif (!sg)\r\nreturn NULL;\r\nfor (i = 0; i < periods; i++) {\r\nsg_dma_address(&sg[i]) = dma_addr;\r\nsg_dma_len(&sg[i]) = period_len;\r\ndma_addr += period_len;\r\n}\r\nsg[periods].offset = 0;\r\nsg_dma_len(&sg[periods]) = 0;\r\nsg[periods].page_link =\r\n((unsigned long)sg | 0x01) & ~0x02;\r\ntxd = d40_prep_sg(chan, sg, sg, periods, direction,\r\nDMA_PREP_INTERRUPT);\r\nkfree(sg);\r\nreturn txd;\r\n}\r\nstatic enum dma_status d40_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\r\nenum dma_status ret;\r\nif (d40c->phy_chan == NULL) {\r\nchan_err(d40c, "Cannot read status of unallocated channel\n");\r\nreturn -EINVAL;\r\n}\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret != DMA_COMPLETE)\r\ndma_set_residue(txstate, stedma40_residue(chan));\r\nif (d40_is_paused(d40c))\r\nret = DMA_PAUSED;\r\nreturn ret;\r\n}\r\nstatic void d40_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\r\nunsigned long flags;\r\nif (d40c->phy_chan == NULL) {\r\nchan_err(d40c, "Channel is not allocated!\n");\r\nreturn;\r\n}\r\nspin_lock_irqsave(&d40c->lock, flags);\r\nlist_splice_tail_init(&d40c->pending_queue, &d40c->queue);\r\nif (!d40c->busy)\r\n(void) d40_queue_start(d40c);\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\n}\r\nstatic void d40_terminate_all(struct dma_chan *chan)\r\n{\r\nunsigned long flags;\r\nstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\r\nint ret;\r\nspin_lock_irqsave(&d40c->lock, flags);\r\npm_runtime_get_sync(d40c->base->dev);\r\nret = d40_channel_execute_command(d40c, D40_DMA_STOP);\r\nif (ret)\r\nchan_err(d40c, "Failed to stop channel\n");\r\nd40_term_all(d40c);\r\npm_runtime_mark_last_busy(d40c->base->dev);\r\npm_runtime_put_autosuspend(d40c->base->dev);\r\nif (d40c->busy) {\r\npm_runtime_mark_last_busy(d40c->base->dev);\r\npm_runtime_put_autosuspend(d40c->base->dev);\r\n}\r\nd40c->busy = false;\r\nspin_unlock_irqrestore(&d40c->lock, flags);\r\n}\r\nstatic int\r\ndma40_config_to_halfchannel(struct d40_chan *d40c,\r\nstruct stedma40_half_channel_info *info,\r\nu32 maxburst)\r\n{\r\nint psize;\r\nif (chan_is_logical(d40c)) {\r\nif (maxburst >= 16)\r\npsize = STEDMA40_PSIZE_LOG_16;\r\nelse if (maxburst >= 8)\r\npsize = STEDMA40_PSIZE_LOG_8;\r\nelse if (maxburst >= 4)\r\npsize = STEDMA40_PSIZE_LOG_4;\r\nelse\r\npsize = STEDMA40_PSIZE_LOG_1;\r\n} else {\r\nif (maxburst >= 16)\r\npsize = STEDMA40_PSIZE_PHY_16;\r\nelse if (maxburst >= 8)\r\npsize = STEDMA40_PSIZE_PHY_8;\r\nelse if (maxburst >= 4)\r\npsize = STEDMA40_PSIZE_PHY_4;\r\nelse\r\npsize = STEDMA40_PSIZE_PHY_1;\r\n}\r\ninfo->psize = psize;\r\ninfo->flow_ctrl = STEDMA40_NO_FLOW_CTRL;\r\nreturn 0;\r\n}\r\nstatic int d40_set_runtime_config(struct dma_chan *chan,\r\nstruct dma_slave_config *config)\r\n{\r\nstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\r\nstruct stedma40_chan_cfg *cfg = &d40c->dma_cfg;\r\nenum dma_slave_buswidth src_addr_width, dst_addr_width;\r\ndma_addr_t config_addr;\r\nu32 src_maxburst, dst_maxburst;\r\nint ret;\r\nsrc_addr_width = config->src_addr_width;\r\nsrc_maxburst = config->src_maxburst;\r\ndst_addr_width = config->dst_addr_width;\r\ndst_maxburst = config->dst_maxburst;\r\nif (config->direction == DMA_DEV_TO_MEM) {\r\nconfig_addr = config->src_addr;\r\nif (cfg->dir != DMA_DEV_TO_MEM)\r\ndev_dbg(d40c->base->dev,\r\n"channel was not configured for peripheral "\r\n"to memory transfer (%d) overriding\n",\r\ncfg->dir);\r\ncfg->dir = DMA_DEV_TO_MEM;\r\nif (dst_addr_width == DMA_SLAVE_BUSWIDTH_UNDEFINED)\r\ndst_addr_width = src_addr_width;\r\nif (dst_maxburst == 0)\r\ndst_maxburst = src_maxburst;\r\n} else if (config->direction == DMA_MEM_TO_DEV) {\r\nconfig_addr = config->dst_addr;\r\nif (cfg->dir != DMA_MEM_TO_DEV)\r\ndev_dbg(d40c->base->dev,\r\n"channel was not configured for memory "\r\n"to peripheral transfer (%d) overriding\n",\r\ncfg->dir);\r\ncfg->dir = DMA_MEM_TO_DEV;\r\nif (src_addr_width == DMA_SLAVE_BUSWIDTH_UNDEFINED)\r\nsrc_addr_width = dst_addr_width;\r\nif (src_maxburst == 0)\r\nsrc_maxburst = dst_maxburst;\r\n} else {\r\ndev_err(d40c->base->dev,\r\n"unrecognized channel direction %d\n",\r\nconfig->direction);\r\nreturn -EINVAL;\r\n}\r\nif (config_addr <= 0) {\r\ndev_err(d40c->base->dev, "no address supplied\n");\r\nreturn -EINVAL;\r\n}\r\nif (src_maxburst * src_addr_width != dst_maxburst * dst_addr_width) {\r\ndev_err(d40c->base->dev,\r\n"src/dst width/maxburst mismatch: %d*%d != %d*%d\n",\r\nsrc_maxburst,\r\nsrc_addr_width,\r\ndst_maxburst,\r\ndst_addr_width);\r\nreturn -EINVAL;\r\n}\r\nif (src_maxburst > 16) {\r\nsrc_maxburst = 16;\r\ndst_maxburst = src_maxburst * src_addr_width / dst_addr_width;\r\n} else if (dst_maxburst > 16) {\r\ndst_maxburst = 16;\r\nsrc_maxburst = dst_maxburst * dst_addr_width / src_addr_width;\r\n}\r\nif (src_addr_width <= DMA_SLAVE_BUSWIDTH_UNDEFINED ||\r\nsrc_addr_width > DMA_SLAVE_BUSWIDTH_8_BYTES ||\r\ndst_addr_width <= DMA_SLAVE_BUSWIDTH_UNDEFINED ||\r\ndst_addr_width > DMA_SLAVE_BUSWIDTH_8_BYTES ||\r\n!is_power_of_2(src_addr_width) ||\r\n!is_power_of_2(dst_addr_width))\r\nreturn -EINVAL;\r\ncfg->src_info.data_width = src_addr_width;\r\ncfg->dst_info.data_width = dst_addr_width;\r\nret = dma40_config_to_halfchannel(d40c, &cfg->src_info,\r\nsrc_maxburst);\r\nif (ret)\r\nreturn ret;\r\nret = dma40_config_to_halfchannel(d40c, &cfg->dst_info,\r\ndst_maxburst);\r\nif (ret)\r\nreturn ret;\r\nif (chan_is_logical(d40c))\r\nd40_log_cfg(cfg, &d40c->log_def.lcsp1, &d40c->log_def.lcsp3);\r\nelse\r\nd40_phy_cfg(cfg, &d40c->src_def_cfg, &d40c->dst_def_cfg);\r\nd40c->runtime_addr = config_addr;\r\nd40c->runtime_direction = config->direction;\r\ndev_dbg(d40c->base->dev,\r\n"configured channel %s for %s, data width %d/%d, "\r\n"maxburst %d/%d elements, LE, no flow control\n",\r\ndma_chan_name(chan),\r\n(config->direction == DMA_DEV_TO_MEM) ? "RX" : "TX",\r\nsrc_addr_width, dst_addr_width,\r\nsrc_maxburst, dst_maxburst);\r\nreturn 0;\r\n}\r\nstatic int d40_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct d40_chan *d40c = container_of(chan, struct d40_chan, chan);\r\nif (d40c->phy_chan == NULL) {\r\nchan_err(d40c, "Channel is not allocated!\n");\r\nreturn -EINVAL;\r\n}\r\nswitch (cmd) {\r\ncase DMA_TERMINATE_ALL:\r\nd40_terminate_all(chan);\r\nreturn 0;\r\ncase DMA_PAUSE:\r\nreturn d40_pause(d40c);\r\ncase DMA_RESUME:\r\nreturn d40_resume(d40c);\r\ncase DMA_SLAVE_CONFIG:\r\nreturn d40_set_runtime_config(chan,\r\n(struct dma_slave_config *) arg);\r\ndefault:\r\nbreak;\r\n}\r\nreturn -ENXIO;\r\n}\r\nstatic void __init d40_chan_init(struct d40_base *base, struct dma_device *dma,\r\nstruct d40_chan *chans, int offset,\r\nint num_chans)\r\n{\r\nint i = 0;\r\nstruct d40_chan *d40c;\r\nINIT_LIST_HEAD(&dma->channels);\r\nfor (i = offset; i < offset + num_chans; i++) {\r\nd40c = &chans[i];\r\nd40c->base = base;\r\nd40c->chan.device = dma;\r\nspin_lock_init(&d40c->lock);\r\nd40c->log_num = D40_PHY_CHAN;\r\nINIT_LIST_HEAD(&d40c->done);\r\nINIT_LIST_HEAD(&d40c->active);\r\nINIT_LIST_HEAD(&d40c->queue);\r\nINIT_LIST_HEAD(&d40c->pending_queue);\r\nINIT_LIST_HEAD(&d40c->client);\r\nINIT_LIST_HEAD(&d40c->prepare_queue);\r\ntasklet_init(&d40c->tasklet, dma_tasklet,\r\n(unsigned long) d40c);\r\nlist_add_tail(&d40c->chan.device_node,\r\n&dma->channels);\r\n}\r\n}\r\nstatic void d40_ops_init(struct d40_base *base, struct dma_device *dev)\r\n{\r\nif (dma_has_cap(DMA_SLAVE, dev->cap_mask))\r\ndev->device_prep_slave_sg = d40_prep_slave_sg;\r\nif (dma_has_cap(DMA_MEMCPY, dev->cap_mask)) {\r\ndev->device_prep_dma_memcpy = d40_prep_memcpy;\r\ndev->copy_align = 2;\r\n}\r\nif (dma_has_cap(DMA_SG, dev->cap_mask))\r\ndev->device_prep_dma_sg = d40_prep_memcpy_sg;\r\nif (dma_has_cap(DMA_CYCLIC, dev->cap_mask))\r\ndev->device_prep_dma_cyclic = dma40_prep_dma_cyclic;\r\ndev->device_alloc_chan_resources = d40_alloc_chan_resources;\r\ndev->device_free_chan_resources = d40_free_chan_resources;\r\ndev->device_issue_pending = d40_issue_pending;\r\ndev->device_tx_status = d40_tx_status;\r\ndev->device_control = d40_control;\r\ndev->dev = base->dev;\r\n}\r\nstatic int __init d40_dmaengine_init(struct d40_base *base,\r\nint num_reserved_chans)\r\n{\r\nint err ;\r\nd40_chan_init(base, &base->dma_slave, base->log_chans,\r\n0, base->num_log_chans);\r\ndma_cap_zero(base->dma_slave.cap_mask);\r\ndma_cap_set(DMA_SLAVE, base->dma_slave.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, base->dma_slave.cap_mask);\r\nd40_ops_init(base, &base->dma_slave);\r\nerr = dma_async_device_register(&base->dma_slave);\r\nif (err) {\r\nd40_err(base->dev, "Failed to register slave channels\n");\r\ngoto failure1;\r\n}\r\nd40_chan_init(base, &base->dma_memcpy, base->log_chans,\r\nbase->num_log_chans, base->num_memcpy_chans);\r\ndma_cap_zero(base->dma_memcpy.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, base->dma_memcpy.cap_mask);\r\ndma_cap_set(DMA_SG, base->dma_memcpy.cap_mask);\r\nd40_ops_init(base, &base->dma_memcpy);\r\nerr = dma_async_device_register(&base->dma_memcpy);\r\nif (err) {\r\nd40_err(base->dev,\r\n"Failed to regsiter memcpy only channels\n");\r\ngoto failure2;\r\n}\r\nd40_chan_init(base, &base->dma_both, base->phy_chans,\r\n0, num_reserved_chans);\r\ndma_cap_zero(base->dma_both.cap_mask);\r\ndma_cap_set(DMA_SLAVE, base->dma_both.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, base->dma_both.cap_mask);\r\ndma_cap_set(DMA_SG, base->dma_both.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, base->dma_slave.cap_mask);\r\nd40_ops_init(base, &base->dma_both);\r\nerr = dma_async_device_register(&base->dma_both);\r\nif (err) {\r\nd40_err(base->dev,\r\n"Failed to register logical and physical capable channels\n");\r\ngoto failure3;\r\n}\r\nreturn 0;\r\nfailure3:\r\ndma_async_device_unregister(&base->dma_memcpy);\r\nfailure2:\r\ndma_async_device_unregister(&base->dma_slave);\r\nfailure1:\r\nreturn err;\r\n}\r\nstatic int dma40_pm_suspend(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct d40_base *base = platform_get_drvdata(pdev);\r\nint ret = 0;\r\nif (base->lcpa_regulator)\r\nret = regulator_disable(base->lcpa_regulator);\r\nreturn ret;\r\n}\r\nstatic int dma40_runtime_suspend(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct d40_base *base = platform_get_drvdata(pdev);\r\nd40_save_restore_registers(base, true);\r\nif (base->rev != 1)\r\nwritel_relaxed(base->gcc_pwr_off_mask,\r\nbase->virtbase + D40_DREG_GCC);\r\nreturn 0;\r\n}\r\nstatic int dma40_runtime_resume(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct d40_base *base = platform_get_drvdata(pdev);\r\nif (base->initialized)\r\nd40_save_restore_registers(base, false);\r\nwritel_relaxed(D40_DREG_GCC_ENABLE_ALL,\r\nbase->virtbase + D40_DREG_GCC);\r\nreturn 0;\r\n}\r\nstatic int dma40_resume(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct d40_base *base = platform_get_drvdata(pdev);\r\nint ret = 0;\r\nif (base->lcpa_regulator)\r\nret = regulator_enable(base->lcpa_regulator);\r\nreturn ret;\r\n}\r\nstatic int __init d40_phy_res_init(struct d40_base *base)\r\n{\r\nint i;\r\nint num_phy_chans_avail = 0;\r\nu32 val[2];\r\nint odd_even_bit = -2;\r\nint gcc = D40_DREG_GCC_ENA;\r\nval[0] = readl(base->virtbase + D40_DREG_PRSME);\r\nval[1] = readl(base->virtbase + D40_DREG_PRSMO);\r\nfor (i = 0; i < base->num_phy_chans; i++) {\r\nbase->phy_res[i].num = i;\r\nodd_even_bit += 2 * ((i % 2) == 0);\r\nif (((val[i % 2] >> odd_even_bit) & 3) == 1) {\r\nbase->phy_res[i].allocated_src = D40_ALLOC_PHY;\r\nbase->phy_res[i].allocated_dst = D40_ALLOC_PHY;\r\nbase->phy_res[i].reserved = true;\r\ngcc |= D40_DREG_GCC_EVTGRP_ENA(D40_PHYS_TO_GROUP(i),\r\nD40_DREG_GCC_SRC);\r\ngcc |= D40_DREG_GCC_EVTGRP_ENA(D40_PHYS_TO_GROUP(i),\r\nD40_DREG_GCC_DST);\r\n} else {\r\nbase->phy_res[i].allocated_src = D40_ALLOC_FREE;\r\nbase->phy_res[i].allocated_dst = D40_ALLOC_FREE;\r\nbase->phy_res[i].reserved = false;\r\nnum_phy_chans_avail++;\r\n}\r\nspin_lock_init(&base->phy_res[i].lock);\r\n}\r\nfor (i = 0; base->plat_data->disabled_channels[i] != -1; i++) {\r\nint chan = base->plat_data->disabled_channels[i];\r\nbase->phy_res[chan].allocated_src = D40_ALLOC_PHY;\r\nbase->phy_res[chan].allocated_dst = D40_ALLOC_PHY;\r\nbase->phy_res[chan].reserved = true;\r\ngcc |= D40_DREG_GCC_EVTGRP_ENA(D40_PHYS_TO_GROUP(chan),\r\nD40_DREG_GCC_SRC);\r\ngcc |= D40_DREG_GCC_EVTGRP_ENA(D40_PHYS_TO_GROUP(chan),\r\nD40_DREG_GCC_DST);\r\nnum_phy_chans_avail--;\r\n}\r\nfor (i = 0; i < base->plat_data->num_of_soft_lli_chans; i++) {\r\nint chan = base->plat_data->soft_lli_chans[i];\r\nbase->phy_res[chan].use_soft_lli = true;\r\n}\r\ndev_info(base->dev, "%d of %d physical DMA channels available\n",\r\nnum_phy_chans_avail, base->num_phy_chans);\r\nval[0] = readl(base->virtbase + D40_DREG_PRTYP);\r\nfor (i = 0; i < base->num_phy_chans; i++) {\r\nif (base->phy_res[i].allocated_src == D40_ALLOC_FREE &&\r\n(val[0] & 0x3) != 1)\r\ndev_info(base->dev,\r\n"[%s] INFO: channel %d is misconfigured (%d)\n",\r\n__func__, i, val[0] & 0x3);\r\nval[0] = val[0] >> 2;\r\n}\r\nwritel(D40_DREG_GCC_ENABLE_ALL, base->virtbase + D40_DREG_GCC);\r\nbase->gcc_pwr_off_mask = gcc;\r\nreturn num_phy_chans_avail;\r\n}\r\nstatic struct d40_base * __init d40_hw_detect_init(struct platform_device *pdev)\r\n{\r\nstruct stedma40_platform_data *plat_data = dev_get_platdata(&pdev->dev);\r\nstruct clk *clk = NULL;\r\nvoid __iomem *virtbase = NULL;\r\nstruct resource *res = NULL;\r\nstruct d40_base *base = NULL;\r\nint num_log_chans = 0;\r\nint num_phy_chans;\r\nint num_memcpy_chans;\r\nint clk_ret = -EINVAL;\r\nint i;\r\nu32 pid;\r\nu32 cid;\r\nu8 rev;\r\nclk = clk_get(&pdev->dev, NULL);\r\nif (IS_ERR(clk)) {\r\nd40_err(&pdev->dev, "No matching clock found\n");\r\ngoto failure;\r\n}\r\nclk_ret = clk_prepare_enable(clk);\r\nif (clk_ret) {\r\nd40_err(&pdev->dev, "Failed to prepare/enable clock\n");\r\ngoto failure;\r\n}\r\nres = platform_get_resource_byname(pdev, IORESOURCE_MEM, "base");\r\nif (!res)\r\ngoto failure;\r\nif (request_mem_region(res->start, resource_size(res),\r\nD40_NAME " I/O base") == NULL)\r\ngoto failure;\r\nvirtbase = ioremap(res->start, resource_size(res));\r\nif (!virtbase)\r\ngoto failure;\r\nfor (pid = 0, i = 0; i < 4; i++)\r\npid |= (readl(virtbase + resource_size(res) - 0x20 + 4 * i)\r\n& 255) << (i * 8);\r\nfor (cid = 0, i = 0; i < 4; i++)\r\ncid |= (readl(virtbase + resource_size(res) - 0x10 + 4 * i)\r\n& 255) << (i * 8);\r\nif (cid != AMBA_CID) {\r\nd40_err(&pdev->dev, "Unknown hardware! No PrimeCell ID\n");\r\ngoto failure;\r\n}\r\nif (AMBA_MANF_BITS(pid) != AMBA_VENDOR_ST) {\r\nd40_err(&pdev->dev, "Unknown designer! Got %x wanted %x\n",\r\nAMBA_MANF_BITS(pid),\r\nAMBA_VENDOR_ST);\r\ngoto failure;\r\n}\r\nrev = AMBA_REV_BITS(pid);\r\nif (rev < 2) {\r\nd40_err(&pdev->dev, "hardware revision: %d is not supported", rev);\r\ngoto failure;\r\n}\r\nif (plat_data->num_of_phy_chans)\r\nnum_phy_chans = plat_data->num_of_phy_chans;\r\nelse\r\nnum_phy_chans = 4 * (readl(virtbase + D40_DREG_ICFG) & 0x7) + 4;\r\nif (plat_data->num_of_memcpy_chans)\r\nnum_memcpy_chans = plat_data->num_of_memcpy_chans;\r\nelse\r\nnum_memcpy_chans = ARRAY_SIZE(dma40_memcpy_channels);\r\nnum_log_chans = num_phy_chans * D40_MAX_LOG_CHAN_PER_PHY;\r\ndev_info(&pdev->dev,\r\n"hardware rev: %d @ %pa with %d physical and %d logical channels\n",\r\nrev, &res->start, num_phy_chans, num_log_chans);\r\nbase = kzalloc(ALIGN(sizeof(struct d40_base), 4) +\r\n(num_phy_chans + num_log_chans + num_memcpy_chans) *\r\nsizeof(struct d40_chan), GFP_KERNEL);\r\nif (base == NULL) {\r\nd40_err(&pdev->dev, "Out of memory\n");\r\ngoto failure;\r\n}\r\nbase->rev = rev;\r\nbase->clk = clk;\r\nbase->num_memcpy_chans = num_memcpy_chans;\r\nbase->num_phy_chans = num_phy_chans;\r\nbase->num_log_chans = num_log_chans;\r\nbase->phy_start = res->start;\r\nbase->phy_size = resource_size(res);\r\nbase->virtbase = virtbase;\r\nbase->plat_data = plat_data;\r\nbase->dev = &pdev->dev;\r\nbase->phy_chans = ((void *)base) + ALIGN(sizeof(struct d40_base), 4);\r\nbase->log_chans = &base->phy_chans[num_phy_chans];\r\nif (base->plat_data->num_of_phy_chans == 14) {\r\nbase->gen_dmac.backup = d40_backup_regs_v4b;\r\nbase->gen_dmac.backup_size = BACKUP_REGS_SZ_V4B;\r\nbase->gen_dmac.interrupt_en = D40_DREG_CPCMIS;\r\nbase->gen_dmac.interrupt_clear = D40_DREG_CPCICR;\r\nbase->gen_dmac.realtime_en = D40_DREG_CRSEG1;\r\nbase->gen_dmac.realtime_clear = D40_DREG_CRCEG1;\r\nbase->gen_dmac.high_prio_en = D40_DREG_CPSEG1;\r\nbase->gen_dmac.high_prio_clear = D40_DREG_CPCEG1;\r\nbase->gen_dmac.il = il_v4b;\r\nbase->gen_dmac.il_size = ARRAY_SIZE(il_v4b);\r\nbase->gen_dmac.init_reg = dma_init_reg_v4b;\r\nbase->gen_dmac.init_reg_size = ARRAY_SIZE(dma_init_reg_v4b);\r\n} else {\r\nif (base->rev >= 3) {\r\nbase->gen_dmac.backup = d40_backup_regs_v4a;\r\nbase->gen_dmac.backup_size = BACKUP_REGS_SZ_V4A;\r\n}\r\nbase->gen_dmac.interrupt_en = D40_DREG_PCMIS;\r\nbase->gen_dmac.interrupt_clear = D40_DREG_PCICR;\r\nbase->gen_dmac.realtime_en = D40_DREG_RSEG1;\r\nbase->gen_dmac.realtime_clear = D40_DREG_RCEG1;\r\nbase->gen_dmac.high_prio_en = D40_DREG_PSEG1;\r\nbase->gen_dmac.high_prio_clear = D40_DREG_PCEG1;\r\nbase->gen_dmac.il = il_v4a;\r\nbase->gen_dmac.il_size = ARRAY_SIZE(il_v4a);\r\nbase->gen_dmac.init_reg = dma_init_reg_v4a;\r\nbase->gen_dmac.init_reg_size = ARRAY_SIZE(dma_init_reg_v4a);\r\n}\r\nbase->phy_res = kzalloc(num_phy_chans * sizeof(struct d40_phy_res),\r\nGFP_KERNEL);\r\nif (!base->phy_res)\r\ngoto failure;\r\nbase->lookup_phy_chans = kzalloc(num_phy_chans *\r\nsizeof(struct d40_chan *),\r\nGFP_KERNEL);\r\nif (!base->lookup_phy_chans)\r\ngoto failure;\r\nbase->lookup_log_chans = kzalloc(num_log_chans *\r\nsizeof(struct d40_chan *),\r\nGFP_KERNEL);\r\nif (!base->lookup_log_chans)\r\ngoto failure;\r\nbase->reg_val_backup_chan = kmalloc(base->num_phy_chans *\r\nsizeof(d40_backup_regs_chan),\r\nGFP_KERNEL);\r\nif (!base->reg_val_backup_chan)\r\ngoto failure;\r\nbase->lcla_pool.alloc_map =\r\nkzalloc(num_phy_chans * sizeof(struct d40_desc *)\r\n* D40_LCLA_LINK_PER_EVENT_GRP, GFP_KERNEL);\r\nif (!base->lcla_pool.alloc_map)\r\ngoto failure;\r\nbase->desc_slab = kmem_cache_create(D40_NAME, sizeof(struct d40_desc),\r\n0, SLAB_HWCACHE_ALIGN,\r\nNULL);\r\nif (base->desc_slab == NULL)\r\ngoto failure;\r\nreturn base;\r\nfailure:\r\nif (!clk_ret)\r\nclk_disable_unprepare(clk);\r\nif (!IS_ERR(clk))\r\nclk_put(clk);\r\nif (virtbase)\r\niounmap(virtbase);\r\nif (res)\r\nrelease_mem_region(res->start,\r\nresource_size(res));\r\nif (virtbase)\r\niounmap(virtbase);\r\nif (base) {\r\nkfree(base->lcla_pool.alloc_map);\r\nkfree(base->reg_val_backup_chan);\r\nkfree(base->lookup_log_chans);\r\nkfree(base->lookup_phy_chans);\r\nkfree(base->phy_res);\r\nkfree(base);\r\n}\r\nreturn NULL;\r\n}\r\nstatic void __init d40_hw_init(struct d40_base *base)\r\n{\r\nint i;\r\nu32 prmseo[2] = {0, 0};\r\nu32 activeo[2] = {0xFFFFFFFF, 0xFFFFFFFF};\r\nu32 pcmis = 0;\r\nu32 pcicr = 0;\r\nstruct d40_reg_val *dma_init_reg = base->gen_dmac.init_reg;\r\nu32 reg_size = base->gen_dmac.init_reg_size;\r\nfor (i = 0; i < reg_size; i++)\r\nwritel(dma_init_reg[i].val,\r\nbase->virtbase + dma_init_reg[i].reg);\r\nfor (i = 0; i < base->num_phy_chans; i++) {\r\nactiveo[i % 2] = activeo[i % 2] << 2;\r\nif (base->phy_res[base->num_phy_chans - i - 1].allocated_src\r\n== D40_ALLOC_PHY) {\r\nactiveo[i % 2] |= 3;\r\ncontinue;\r\n}\r\npcmis = (pcmis << 1) | 1;\r\npcicr = (pcicr << 1) | 1;\r\nprmseo[i % 2] = prmseo[i % 2] << 2;\r\nprmseo[i % 2] |= 1;\r\n}\r\nwritel(prmseo[1], base->virtbase + D40_DREG_PRMSE);\r\nwritel(prmseo[0], base->virtbase + D40_DREG_PRMSO);\r\nwritel(activeo[1], base->virtbase + D40_DREG_ACTIVE);\r\nwritel(activeo[0], base->virtbase + D40_DREG_ACTIVO);\r\nwritel(pcmis, base->virtbase + base->gen_dmac.interrupt_en);\r\nwritel(pcicr, base->virtbase + base->gen_dmac.interrupt_clear);\r\nbase->gen_dmac.init_reg = NULL;\r\nbase->gen_dmac.init_reg_size = 0;\r\n}\r\nstatic int __init d40_lcla_allocate(struct d40_base *base)\r\n{\r\nstruct d40_lcla_pool *pool = &base->lcla_pool;\r\nunsigned long *page_list;\r\nint i, j;\r\nint ret = 0;\r\npage_list = kmalloc(sizeof(unsigned long) * MAX_LCLA_ALLOC_ATTEMPTS,\r\nGFP_KERNEL);\r\nif (!page_list) {\r\nret = -ENOMEM;\r\ngoto failure;\r\n}\r\nbase->lcla_pool.pages = SZ_1K * base->num_phy_chans / PAGE_SIZE;\r\nfor (i = 0; i < MAX_LCLA_ALLOC_ATTEMPTS; i++) {\r\npage_list[i] = __get_free_pages(GFP_KERNEL,\r\nbase->lcla_pool.pages);\r\nif (!page_list[i]) {\r\nd40_err(base->dev, "Failed to allocate %d pages.\n",\r\nbase->lcla_pool.pages);\r\nfor (j = 0; j < i; j++)\r\nfree_pages(page_list[j], base->lcla_pool.pages);\r\ngoto failure;\r\n}\r\nif ((virt_to_phys((void *)page_list[i]) &\r\n(LCLA_ALIGNMENT - 1)) == 0)\r\nbreak;\r\n}\r\nfor (j = 0; j < i; j++)\r\nfree_pages(page_list[j], base->lcla_pool.pages);\r\nif (i < MAX_LCLA_ALLOC_ATTEMPTS) {\r\nbase->lcla_pool.base = (void *)page_list[i];\r\n} else {\r\ndev_warn(base->dev,\r\n"[%s] Failed to get %d pages @ 18 bit align.\n",\r\n__func__, base->lcla_pool.pages);\r\nbase->lcla_pool.base_unaligned = kmalloc(SZ_1K *\r\nbase->num_phy_chans +\r\nLCLA_ALIGNMENT,\r\nGFP_KERNEL);\r\nif (!base->lcla_pool.base_unaligned) {\r\nret = -ENOMEM;\r\ngoto failure;\r\n}\r\nbase->lcla_pool.base = PTR_ALIGN(base->lcla_pool.base_unaligned,\r\nLCLA_ALIGNMENT);\r\n}\r\npool->dma_addr = dma_map_single(base->dev, pool->base,\r\nSZ_1K * base->num_phy_chans,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(base->dev, pool->dma_addr)) {\r\npool->dma_addr = 0;\r\nret = -ENOMEM;\r\ngoto failure;\r\n}\r\nwritel(virt_to_phys(base->lcla_pool.base),\r\nbase->virtbase + D40_DREG_LCLA);\r\nfailure:\r\nkfree(page_list);\r\nreturn ret;\r\n}\r\nstatic int __init d40_of_probe(struct platform_device *pdev,\r\nstruct device_node *np)\r\n{\r\nstruct stedma40_platform_data *pdata;\r\nint num_phy = 0, num_memcpy = 0, num_disabled = 0;\r\nconst __be32 *list;\r\npdata = devm_kzalloc(&pdev->dev,\r\nsizeof(struct stedma40_platform_data),\r\nGFP_KERNEL);\r\nif (!pdata)\r\nreturn -ENOMEM;\r\nof_property_read_u32(np, "dma-channels", &num_phy);\r\nif (num_phy > 0)\r\npdata->num_of_phy_chans = num_phy;\r\nlist = of_get_property(np, "memcpy-channels", &num_memcpy);\r\nnum_memcpy /= sizeof(*list);\r\nif (num_memcpy > D40_MEMCPY_MAX_CHANS || num_memcpy <= 0) {\r\nd40_err(&pdev->dev,\r\n"Invalid number of memcpy channels specified (%d)\n",\r\nnum_memcpy);\r\nreturn -EINVAL;\r\n}\r\npdata->num_of_memcpy_chans = num_memcpy;\r\nof_property_read_u32_array(np, "memcpy-channels",\r\ndma40_memcpy_channels,\r\nnum_memcpy);\r\nlist = of_get_property(np, "disabled-channels", &num_disabled);\r\nnum_disabled /= sizeof(*list);\r\nif (num_disabled >= STEDMA40_MAX_PHYS || num_disabled < 0) {\r\nd40_err(&pdev->dev,\r\n"Invalid number of disabled channels specified (%d)\n",\r\nnum_disabled);\r\nreturn -EINVAL;\r\n}\r\nof_property_read_u32_array(np, "disabled-channels",\r\npdata->disabled_channels,\r\nnum_disabled);\r\npdata->disabled_channels[num_disabled] = -1;\r\npdev->dev.platform_data = pdata;\r\nreturn 0;\r\n}\r\nstatic int __init d40_probe(struct platform_device *pdev)\r\n{\r\nstruct stedma40_platform_data *plat_data = dev_get_platdata(&pdev->dev);\r\nstruct device_node *np = pdev->dev.of_node;\r\nint ret = -ENOENT;\r\nstruct d40_base *base = NULL;\r\nstruct resource *res = NULL;\r\nint num_reserved_chans;\r\nu32 val;\r\nif (!plat_data) {\r\nif (np) {\r\nif(d40_of_probe(pdev, np)) {\r\nret = -ENOMEM;\r\ngoto failure;\r\n}\r\n} else {\r\nd40_err(&pdev->dev, "No pdata or Device Tree provided\n");\r\ngoto failure;\r\n}\r\n}\r\nbase = d40_hw_detect_init(pdev);\r\nif (!base)\r\ngoto failure;\r\nnum_reserved_chans = d40_phy_res_init(base);\r\nplatform_set_drvdata(pdev, base);\r\nspin_lock_init(&base->interrupt_lock);\r\nspin_lock_init(&base->execmd_lock);\r\nres = platform_get_resource_byname(pdev, IORESOURCE_MEM, "lcpa");\r\nif (!res) {\r\nret = -ENOENT;\r\nd40_err(&pdev->dev, "No \"lcpa\" memory resource\n");\r\ngoto failure;\r\n}\r\nbase->lcpa_size = resource_size(res);\r\nbase->phy_lcpa = res->start;\r\nif (request_mem_region(res->start, resource_size(res),\r\nD40_NAME " I/O lcpa") == NULL) {\r\nret = -EBUSY;\r\nd40_err(&pdev->dev, "Failed to request LCPA region %pR\n", res);\r\ngoto failure;\r\n}\r\nval = readl(base->virtbase + D40_DREG_LCPA);\r\nif (res->start != val && val != 0) {\r\ndev_warn(&pdev->dev,\r\n"[%s] Mismatch LCPA dma 0x%x, def %pa\n",\r\n__func__, val, &res->start);\r\n} else\r\nwritel(res->start, base->virtbase + D40_DREG_LCPA);\r\nbase->lcpa_base = ioremap(res->start, resource_size(res));\r\nif (!base->lcpa_base) {\r\nret = -ENOMEM;\r\nd40_err(&pdev->dev, "Failed to ioremap LCPA region\n");\r\ngoto failure;\r\n}\r\nif (base->plat_data->use_esram_lcla) {\r\nres = platform_get_resource_byname(pdev, IORESOURCE_MEM,\r\n"lcla_esram");\r\nif (!res) {\r\nret = -ENOENT;\r\nd40_err(&pdev->dev,\r\n"No \"lcla_esram\" memory resource\n");\r\ngoto failure;\r\n}\r\nbase->lcla_pool.base = ioremap(res->start,\r\nresource_size(res));\r\nif (!base->lcla_pool.base) {\r\nret = -ENOMEM;\r\nd40_err(&pdev->dev, "Failed to ioremap LCLA region\n");\r\ngoto failure;\r\n}\r\nwritel(res->start, base->virtbase + D40_DREG_LCLA);\r\n} else {\r\nret = d40_lcla_allocate(base);\r\nif (ret) {\r\nd40_err(&pdev->dev, "Failed to allocate LCLA area\n");\r\ngoto failure;\r\n}\r\n}\r\nspin_lock_init(&base->lcla_pool.lock);\r\nbase->irq = platform_get_irq(pdev, 0);\r\nret = request_irq(base->irq, d40_handle_interrupt, 0, D40_NAME, base);\r\nif (ret) {\r\nd40_err(&pdev->dev, "No IRQ defined\n");\r\ngoto failure;\r\n}\r\npm_runtime_irq_safe(base->dev);\r\npm_runtime_set_autosuspend_delay(base->dev, DMA40_AUTOSUSPEND_DELAY);\r\npm_runtime_use_autosuspend(base->dev);\r\npm_runtime_enable(base->dev);\r\npm_runtime_resume(base->dev);\r\nif (base->plat_data->use_esram_lcla) {\r\nbase->lcpa_regulator = regulator_get(base->dev, "lcla_esram");\r\nif (IS_ERR(base->lcpa_regulator)) {\r\nd40_err(&pdev->dev, "Failed to get lcpa_regulator\n");\r\nret = PTR_ERR(base->lcpa_regulator);\r\nbase->lcpa_regulator = NULL;\r\ngoto failure;\r\n}\r\nret = regulator_enable(base->lcpa_regulator);\r\nif (ret) {\r\nd40_err(&pdev->dev,\r\n"Failed to enable lcpa_regulator\n");\r\nregulator_put(base->lcpa_regulator);\r\nbase->lcpa_regulator = NULL;\r\ngoto failure;\r\n}\r\n}\r\nbase->initialized = true;\r\nret = d40_dmaengine_init(base, num_reserved_chans);\r\nif (ret)\r\ngoto failure;\r\nbase->dev->dma_parms = &base->dma_parms;\r\nret = dma_set_max_seg_size(base->dev, STEDMA40_MAX_SEG_SIZE);\r\nif (ret) {\r\nd40_err(&pdev->dev, "Failed to set dma max seg size\n");\r\ngoto failure;\r\n}\r\nd40_hw_init(base);\r\nif (np) {\r\nret = of_dma_controller_register(np, d40_xlate, NULL);\r\nif (ret)\r\ndev_err(&pdev->dev,\r\n"could not register of_dma_controller\n");\r\n}\r\ndev_info(base->dev, "initialized\n");\r\nreturn 0;\r\nfailure:\r\nif (base) {\r\nif (base->desc_slab)\r\nkmem_cache_destroy(base->desc_slab);\r\nif (base->virtbase)\r\niounmap(base->virtbase);\r\nif (base->lcla_pool.base && base->plat_data->use_esram_lcla) {\r\niounmap(base->lcla_pool.base);\r\nbase->lcla_pool.base = NULL;\r\n}\r\nif (base->lcla_pool.dma_addr)\r\ndma_unmap_single(base->dev, base->lcla_pool.dma_addr,\r\nSZ_1K * base->num_phy_chans,\r\nDMA_TO_DEVICE);\r\nif (!base->lcla_pool.base_unaligned && base->lcla_pool.base)\r\nfree_pages((unsigned long)base->lcla_pool.base,\r\nbase->lcla_pool.pages);\r\nkfree(base->lcla_pool.base_unaligned);\r\nif (base->phy_lcpa)\r\nrelease_mem_region(base->phy_lcpa,\r\nbase->lcpa_size);\r\nif (base->phy_start)\r\nrelease_mem_region(base->phy_start,\r\nbase->phy_size);\r\nif (base->clk) {\r\nclk_disable_unprepare(base->clk);\r\nclk_put(base->clk);\r\n}\r\nif (base->lcpa_regulator) {\r\nregulator_disable(base->lcpa_regulator);\r\nregulator_put(base->lcpa_regulator);\r\n}\r\nkfree(base->lcla_pool.alloc_map);\r\nkfree(base->lookup_log_chans);\r\nkfree(base->lookup_phy_chans);\r\nkfree(base->phy_res);\r\nkfree(base);\r\n}\r\nd40_err(&pdev->dev, "probe failed\n");\r\nreturn ret;\r\n}\r\nstatic int __init stedma40_init(void)\r\n{\r\nreturn platform_driver_probe(&d40_driver, d40_probe);\r\n}
