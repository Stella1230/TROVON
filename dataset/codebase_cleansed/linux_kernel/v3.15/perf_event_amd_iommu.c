static ssize_t _iommu_event_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nstruct amd_iommu_event_desc *event =\r\ncontainer_of(attr, struct amd_iommu_event_desc, attr);\r\nreturn sprintf(buf, "%s\n", event->event);\r\n}\r\nstatic ssize_t _iommu_cpumask_show(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nint n = cpulist_scnprintf(buf, PAGE_SIZE - 2, &iommu_cpumask);\r\nbuf[n++] = '\n';\r\nbuf[n] = '\0';\r\nreturn n;\r\n}\r\nstatic int get_next_avail_iommu_bnk_cntr(struct perf_amd_iommu *perf_iommu)\r\n{\r\nunsigned long flags;\r\nint shift, bank, cntr, retval;\r\nint max_banks = perf_iommu->max_banks;\r\nint max_cntrs = perf_iommu->max_counters;\r\nraw_spin_lock_irqsave(&perf_iommu->lock, flags);\r\nfor (bank = 0, shift = 0; bank < max_banks; bank++) {\r\nfor (cntr = 0; cntr < max_cntrs; cntr++) {\r\nshift = bank + (bank*3) + cntr;\r\nif (perf_iommu->cntr_assign_mask & (1ULL<<shift)) {\r\ncontinue;\r\n} else {\r\nperf_iommu->cntr_assign_mask |= (1ULL<<shift);\r\nretval = ((u16)((u16)bank<<8) | (u8)(cntr));\r\ngoto out;\r\n}\r\n}\r\n}\r\nretval = -ENOSPC;\r\nout:\r\nraw_spin_unlock_irqrestore(&perf_iommu->lock, flags);\r\nreturn retval;\r\n}\r\nstatic int clear_avail_iommu_bnk_cntr(struct perf_amd_iommu *perf_iommu,\r\nu8 bank, u8 cntr)\r\n{\r\nunsigned long flags;\r\nint max_banks, max_cntrs;\r\nint shift = 0;\r\nmax_banks = perf_iommu->max_banks;\r\nmax_cntrs = perf_iommu->max_counters;\r\nif ((bank > max_banks) || (cntr > max_cntrs))\r\nreturn -EINVAL;\r\nshift = bank + cntr + (bank*3);\r\nraw_spin_lock_irqsave(&perf_iommu->lock, flags);\r\nperf_iommu->cntr_assign_mask &= ~(1ULL<<shift);\r\nraw_spin_unlock_irqrestore(&perf_iommu->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int perf_iommu_event_init(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct perf_amd_iommu *perf_iommu;\r\nu64 config, config1;\r\nif (event->attr.type != event->pmu->type)\r\nreturn -ENOENT;\r\nif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\r\nreturn -EINVAL;\r\nif (event->attr.exclude_user || event->attr.exclude_kernel ||\r\nevent->attr.exclude_host || event->attr.exclude_guest)\r\nreturn -EINVAL;\r\nif (event->cpu < 0)\r\nreturn -EINVAL;\r\nperf_iommu = &__perf_iommu;\r\nif (event->pmu != &perf_iommu->pmu)\r\nreturn -ENOENT;\r\nif (perf_iommu) {\r\nconfig = event->attr.config;\r\nconfig1 = event->attr.config1;\r\n} else {\r\nreturn -EINVAL;\r\n}\r\nperf_iommu->max_banks =\r\namd_iommu_pc_get_max_banks(IOMMU_BASE_DEVID);\r\nperf_iommu->max_counters =\r\namd_iommu_pc_get_max_counters(IOMMU_BASE_DEVID);\r\nif ((perf_iommu->max_banks == 0) || (perf_iommu->max_counters == 0))\r\nreturn -EINVAL;\r\nhwc->config = config;\r\nhwc->extra_reg.config = config1;\r\nreturn 0;\r\n}\r\nstatic void perf_iommu_enable_event(struct perf_event *ev)\r\n{\r\nu8 csource = _GET_CSOURCE(ev);\r\nu16 devid = _GET_DEVID(ev);\r\nu64 reg = 0ULL;\r\nreg = csource;\r\namd_iommu_pc_get_set_reg_val(devid,\r\n_GET_BANK(ev), _GET_CNTR(ev) ,\r\nIOMMU_PC_COUNTER_SRC_REG, &reg, true);\r\nreg = 0ULL | devid | (_GET_DEVID_MASK(ev) << 32);\r\nif (reg)\r\nreg |= (1UL << 31);\r\namd_iommu_pc_get_set_reg_val(devid,\r\n_GET_BANK(ev), _GET_CNTR(ev) ,\r\nIOMMU_PC_DEVID_MATCH_REG, &reg, true);\r\nreg = 0ULL | _GET_PASID(ev) | (_GET_PASID_MASK(ev) << 32);\r\nif (reg)\r\nreg |= (1UL << 31);\r\namd_iommu_pc_get_set_reg_val(devid,\r\n_GET_BANK(ev), _GET_CNTR(ev) ,\r\nIOMMU_PC_PASID_MATCH_REG, &reg, true);\r\nreg = 0ULL | _GET_DOMID(ev) | (_GET_DOMID_MASK(ev) << 32);\r\nif (reg)\r\nreg |= (1UL << 31);\r\namd_iommu_pc_get_set_reg_val(devid,\r\n_GET_BANK(ev), _GET_CNTR(ev) ,\r\nIOMMU_PC_DOMID_MATCH_REG, &reg, true);\r\n}\r\nstatic void perf_iommu_disable_event(struct perf_event *event)\r\n{\r\nu64 reg = 0ULL;\r\namd_iommu_pc_get_set_reg_val(_GET_DEVID(event),\r\n_GET_BANK(event), _GET_CNTR(event),\r\nIOMMU_PC_COUNTER_SRC_REG, &reg, true);\r\n}\r\nstatic void perf_iommu_start(struct perf_event *event, int flags)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\npr_debug("perf: amd_iommu:perf_iommu_start\n");\r\nif (WARN_ON_ONCE(!(hwc->state & PERF_HES_STOPPED)))\r\nreturn;\r\nWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\r\nhwc->state = 0;\r\nif (flags & PERF_EF_RELOAD) {\r\nu64 prev_raw_count = local64_read(&hwc->prev_count);\r\namd_iommu_pc_get_set_reg_val(_GET_DEVID(event),\r\n_GET_BANK(event), _GET_CNTR(event),\r\nIOMMU_PC_COUNTER_REG, &prev_raw_count, true);\r\n}\r\nperf_iommu_enable_event(event);\r\nperf_event_update_userpage(event);\r\n}\r\nstatic void perf_iommu_read(struct perf_event *event)\r\n{\r\nu64 count = 0ULL;\r\nu64 prev_raw_count = 0ULL;\r\nu64 delta = 0ULL;\r\nstruct hw_perf_event *hwc = &event->hw;\r\npr_debug("perf: amd_iommu:perf_iommu_read\n");\r\namd_iommu_pc_get_set_reg_val(_GET_DEVID(event),\r\n_GET_BANK(event), _GET_CNTR(event),\r\nIOMMU_PC_COUNTER_REG, &count, false);\r\ncount &= 0xFFFFFFFFFFFFULL;\r\nprev_raw_count = local64_read(&hwc->prev_count);\r\nif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\r\ncount) != prev_raw_count)\r\nreturn;\r\ndelta = (count << COUNTER_SHIFT) - (prev_raw_count << COUNTER_SHIFT);\r\ndelta >>= COUNTER_SHIFT;\r\nlocal64_add(delta, &event->count);\r\n}\r\nstatic void perf_iommu_stop(struct perf_event *event, int flags)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 config;\r\npr_debug("perf: amd_iommu:perf_iommu_stop\n");\r\nif (hwc->state & PERF_HES_UPTODATE)\r\nreturn;\r\nperf_iommu_disable_event(event);\r\nWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\r\nhwc->state |= PERF_HES_STOPPED;\r\nif (hwc->state & PERF_HES_UPTODATE)\r\nreturn;\r\nconfig = hwc->config;\r\nperf_iommu_read(event);\r\nhwc->state |= PERF_HES_UPTODATE;\r\n}\r\nstatic int perf_iommu_add(struct perf_event *event, int flags)\r\n{\r\nint retval;\r\nstruct perf_amd_iommu *perf_iommu =\r\ncontainer_of(event->pmu, struct perf_amd_iommu, pmu);\r\npr_debug("perf: amd_iommu:perf_iommu_add\n");\r\nevent->hw.state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\r\nretval = get_next_avail_iommu_bnk_cntr(perf_iommu);\r\nif (retval != -ENOSPC)\r\nevent->hw.extra_reg.reg = (u16)retval;\r\nelse\r\nreturn retval;\r\nif (flags & PERF_EF_START)\r\nperf_iommu_start(event, PERF_EF_RELOAD);\r\nreturn 0;\r\n}\r\nstatic void perf_iommu_del(struct perf_event *event, int flags)\r\n{\r\nstruct perf_amd_iommu *perf_iommu =\r\ncontainer_of(event->pmu, struct perf_amd_iommu, pmu);\r\npr_debug("perf: amd_iommu:perf_iommu_del\n");\r\nperf_iommu_stop(event, PERF_EF_UPDATE);\r\nclear_avail_iommu_bnk_cntr(perf_iommu,\r\n_GET_BANK(event),\r\n_GET_CNTR(event));\r\nperf_event_update_userpage(event);\r\n}\r\nstatic __init int _init_events_attrs(struct perf_amd_iommu *perf_iommu)\r\n{\r\nstruct attribute **attrs;\r\nstruct attribute_group *attr_group;\r\nint i = 0, j;\r\nwhile (amd_iommu_v2_event_descs[i].attr.attr.name)\r\ni++;\r\nattr_group = kzalloc(sizeof(struct attribute *)\r\n* (i + 1) + sizeof(*attr_group), GFP_KERNEL);\r\nif (!attr_group)\r\nreturn -ENOMEM;\r\nattrs = (struct attribute **)(attr_group + 1);\r\nfor (j = 0; j < i; j++)\r\nattrs[j] = &amd_iommu_v2_event_descs[j].attr.attr;\r\nattr_group->name = "events";\r\nattr_group->attrs = attrs;\r\nperf_iommu->events_group = attr_group;\r\nreturn 0;\r\n}\r\nstatic __init void amd_iommu_pc_exit(void)\r\n{\r\nif (__perf_iommu.events_group != NULL) {\r\nkfree(__perf_iommu.events_group);\r\n__perf_iommu.events_group = NULL;\r\n}\r\n}\r\nstatic __init int _init_perf_amd_iommu(\r\nstruct perf_amd_iommu *perf_iommu, char *name)\r\n{\r\nint ret;\r\nraw_spin_lock_init(&perf_iommu->lock);\r\nperf_iommu->format_group = &amd_iommu_format_group;\r\ncpumask_set_cpu(0, &iommu_cpumask);\r\nperf_iommu->cpumask_group = &amd_iommu_cpumask_group;\r\nif (_init_events_attrs(perf_iommu) != 0)\r\npr_err("perf: amd_iommu: Only support raw events.\n");\r\nperf_iommu->null_group = NULL;\r\nperf_iommu->pmu.attr_groups = perf_iommu->attr_groups;\r\nret = perf_pmu_register(&perf_iommu->pmu, name, -1);\r\nif (ret) {\r\npr_err("perf: amd_iommu: Failed to initialized.\n");\r\namd_iommu_pc_exit();\r\n} else {\r\npr_info("perf: amd_iommu: Detected. (%d banks, %d counters/bank)\n",\r\namd_iommu_pc_get_max_banks(IOMMU_BASE_DEVID),\r\namd_iommu_pc_get_max_counters(IOMMU_BASE_DEVID));\r\n}\r\nreturn ret;\r\n}\r\nstatic __init int amd_iommu_pc_init(void)\r\n{\r\nif (!amd_iommu_pc_supported())\r\nreturn -ENODEV;\r\n_init_perf_amd_iommu(&__perf_iommu, "amd_iommu");\r\nreturn 0;\r\n}
