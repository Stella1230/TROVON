size_t ubi_calc_fm_size(struct ubi_device *ubi)\r\n{\r\nsize_t size;\r\nsize = sizeof(struct ubi_fm_hdr) + \\r\nsizeof(struct ubi_fm_scan_pool) + \\r\nsizeof(struct ubi_fm_scan_pool) + \\r\n(ubi->peb_count * sizeof(struct ubi_fm_ec)) + \\r\n(sizeof(struct ubi_fm_eba) + \\r\n(ubi->peb_count * sizeof(__be32))) + \\r\nsizeof(struct ubi_fm_volhdr) * UBI_MAX_VOLUMES;\r\nreturn roundup(size, ubi->leb_size);\r\n}\r\nstatic struct ubi_vid_hdr *new_fm_vhdr(struct ubi_device *ubi, int vol_id)\r\n{\r\nstruct ubi_vid_hdr *new;\r\nnew = ubi_zalloc_vid_hdr(ubi, GFP_KERNEL);\r\nif (!new)\r\ngoto out;\r\nnew->vol_type = UBI_VID_DYNAMIC;\r\nnew->vol_id = cpu_to_be32(vol_id);\r\nnew->compat = UBI_COMPAT_DELETE;\r\nout:\r\nreturn new;\r\n}\r\nstatic int add_aeb(struct ubi_attach_info *ai, struct list_head *list,\r\nint pnum, int ec, int scrub)\r\n{\r\nstruct ubi_ainf_peb *aeb;\r\naeb = kmem_cache_alloc(ai->aeb_slab_cache, GFP_KERNEL);\r\nif (!aeb)\r\nreturn -ENOMEM;\r\naeb->pnum = pnum;\r\naeb->ec = ec;\r\naeb->lnum = -1;\r\naeb->scrub = scrub;\r\naeb->copy_flag = aeb->sqnum = 0;\r\nai->ec_sum += aeb->ec;\r\nai->ec_count++;\r\nif (ai->max_ec < aeb->ec)\r\nai->max_ec = aeb->ec;\r\nif (ai->min_ec > aeb->ec)\r\nai->min_ec = aeb->ec;\r\nlist_add_tail(&aeb->u.list, list);\r\nreturn 0;\r\n}\r\nstatic struct ubi_ainf_volume *add_vol(struct ubi_attach_info *ai, int vol_id,\r\nint used_ebs, int data_pad, u8 vol_type,\r\nint last_eb_bytes)\r\n{\r\nstruct ubi_ainf_volume *av;\r\nstruct rb_node **p = &ai->volumes.rb_node, *parent = NULL;\r\nwhile (*p) {\r\nparent = *p;\r\nav = rb_entry(parent, struct ubi_ainf_volume, rb);\r\nif (vol_id > av->vol_id)\r\np = &(*p)->rb_left;\r\nelse if (vol_id > av->vol_id)\r\np = &(*p)->rb_right;\r\n}\r\nav = kmalloc(sizeof(struct ubi_ainf_volume), GFP_KERNEL);\r\nif (!av)\r\ngoto out;\r\nav->highest_lnum = av->leb_count = 0;\r\nav->vol_id = vol_id;\r\nav->used_ebs = used_ebs;\r\nav->data_pad = data_pad;\r\nav->last_data_size = last_eb_bytes;\r\nav->compat = 0;\r\nav->vol_type = vol_type;\r\nav->root = RB_ROOT;\r\ndbg_bld("found volume (ID %i)", vol_id);\r\nrb_link_node(&av->rb, parent, p);\r\nrb_insert_color(&av->rb, &ai->volumes);\r\nout:\r\nreturn av;\r\n}\r\nstatic void assign_aeb_to_av(struct ubi_attach_info *ai,\r\nstruct ubi_ainf_peb *aeb,\r\nstruct ubi_ainf_volume *av)\r\n{\r\nstruct ubi_ainf_peb *tmp_aeb;\r\nstruct rb_node **p = &ai->volumes.rb_node, *parent = NULL;\r\np = &av->root.rb_node;\r\nwhile (*p) {\r\nparent = *p;\r\ntmp_aeb = rb_entry(parent, struct ubi_ainf_peb, u.rb);\r\nif (aeb->lnum != tmp_aeb->lnum) {\r\nif (aeb->lnum < tmp_aeb->lnum)\r\np = &(*p)->rb_left;\r\nelse\r\np = &(*p)->rb_right;\r\ncontinue;\r\n} else\r\nbreak;\r\n}\r\nlist_del(&aeb->u.list);\r\nav->leb_count++;\r\nrb_link_node(&aeb->u.rb, parent, p);\r\nrb_insert_color(&aeb->u.rb, &av->root);\r\n}\r\nstatic int update_vol(struct ubi_device *ubi, struct ubi_attach_info *ai,\r\nstruct ubi_ainf_volume *av, struct ubi_vid_hdr *new_vh,\r\nstruct ubi_ainf_peb *new_aeb)\r\n{\r\nstruct rb_node **p = &av->root.rb_node, *parent = NULL;\r\nstruct ubi_ainf_peb *aeb, *victim;\r\nint cmp_res;\r\nwhile (*p) {\r\nparent = *p;\r\naeb = rb_entry(parent, struct ubi_ainf_peb, u.rb);\r\nif (be32_to_cpu(new_vh->lnum) != aeb->lnum) {\r\nif (be32_to_cpu(new_vh->lnum) < aeb->lnum)\r\np = &(*p)->rb_left;\r\nelse\r\np = &(*p)->rb_right;\r\ncontinue;\r\n}\r\nif (aeb->pnum == new_aeb->pnum) {\r\nubi_assert(aeb->lnum == new_aeb->lnum);\r\nkmem_cache_free(ai->aeb_slab_cache, new_aeb);\r\nreturn 0;\r\n}\r\ncmp_res = ubi_compare_lebs(ubi, aeb, new_aeb->pnum, new_vh);\r\nif (cmp_res < 0)\r\nreturn cmp_res;\r\nif (cmp_res & 1) {\r\nvictim = kmem_cache_alloc(ai->aeb_slab_cache,\r\nGFP_KERNEL);\r\nif (!victim)\r\nreturn -ENOMEM;\r\nvictim->ec = aeb->ec;\r\nvictim->pnum = aeb->pnum;\r\nlist_add_tail(&victim->u.list, &ai->erase);\r\nif (av->highest_lnum == be32_to_cpu(new_vh->lnum))\r\nav->last_data_size = \\r\nbe32_to_cpu(new_vh->data_size);\r\ndbg_bld("vol %i: AEB %i's PEB %i is the newer",\r\nav->vol_id, aeb->lnum, new_aeb->pnum);\r\naeb->ec = new_aeb->ec;\r\naeb->pnum = new_aeb->pnum;\r\naeb->copy_flag = new_vh->copy_flag;\r\naeb->scrub = new_aeb->scrub;\r\nkmem_cache_free(ai->aeb_slab_cache, new_aeb);\r\n} else {\r\ndbg_bld("vol %i: AEB %i's PEB %i is old, dropping it",\r\nav->vol_id, aeb->lnum, new_aeb->pnum);\r\nlist_add_tail(&new_aeb->u.list, &ai->erase);\r\n}\r\nreturn 0;\r\n}\r\nif (av->highest_lnum <= be32_to_cpu(new_vh->lnum)) {\r\nav->highest_lnum = be32_to_cpu(new_vh->lnum);\r\nav->last_data_size = be32_to_cpu(new_vh->data_size);\r\n}\r\nif (av->vol_type == UBI_STATIC_VOLUME)\r\nav->used_ebs = be32_to_cpu(new_vh->used_ebs);\r\nav->leb_count++;\r\nrb_link_node(&new_aeb->u.rb, parent, p);\r\nrb_insert_color(&new_aeb->u.rb, &av->root);\r\nreturn 0;\r\n}\r\nstatic int process_pool_aeb(struct ubi_device *ubi, struct ubi_attach_info *ai,\r\nstruct ubi_vid_hdr *new_vh,\r\nstruct ubi_ainf_peb *new_aeb)\r\n{\r\nstruct ubi_ainf_volume *av, *tmp_av = NULL;\r\nstruct rb_node **p = &ai->volumes.rb_node, *parent = NULL;\r\nint found = 0;\r\nif (be32_to_cpu(new_vh->vol_id) == UBI_FM_SB_VOLUME_ID ||\r\nbe32_to_cpu(new_vh->vol_id) == UBI_FM_DATA_VOLUME_ID) {\r\nkmem_cache_free(ai->aeb_slab_cache, new_aeb);\r\nreturn 0;\r\n}\r\nwhile (*p) {\r\nparent = *p;\r\ntmp_av = rb_entry(parent, struct ubi_ainf_volume, rb);\r\nif (be32_to_cpu(new_vh->vol_id) > tmp_av->vol_id)\r\np = &(*p)->rb_left;\r\nelse if (be32_to_cpu(new_vh->vol_id) < tmp_av->vol_id)\r\np = &(*p)->rb_right;\r\nelse {\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nif (found)\r\nav = tmp_av;\r\nelse {\r\nubi_err("orphaned volume in fastmap pool!");\r\nreturn UBI_BAD_FASTMAP;\r\n}\r\nubi_assert(be32_to_cpu(new_vh->vol_id) == av->vol_id);\r\nreturn update_vol(ubi, ai, av, new_vh, new_aeb);\r\n}\r\nstatic void unmap_peb(struct ubi_attach_info *ai, int pnum)\r\n{\r\nstruct ubi_ainf_volume *av;\r\nstruct rb_node *node, *node2;\r\nstruct ubi_ainf_peb *aeb;\r\nfor (node = rb_first(&ai->volumes); node; node = rb_next(node)) {\r\nav = rb_entry(node, struct ubi_ainf_volume, rb);\r\nfor (node2 = rb_first(&av->root); node2;\r\nnode2 = rb_next(node2)) {\r\naeb = rb_entry(node2, struct ubi_ainf_peb, u.rb);\r\nif (aeb->pnum == pnum) {\r\nrb_erase(&aeb->u.rb, &av->root);\r\nkmem_cache_free(ai->aeb_slab_cache, aeb);\r\nreturn;\r\n}\r\n}\r\n}\r\n}\r\nstatic int scan_pool(struct ubi_device *ubi, struct ubi_attach_info *ai,\r\nint *pebs, int pool_size, unsigned long long *max_sqnum,\r\nstruct list_head *eba_orphans, struct list_head *free)\r\n{\r\nstruct ubi_vid_hdr *vh;\r\nstruct ubi_ec_hdr *ech;\r\nstruct ubi_ainf_peb *new_aeb, *tmp_aeb;\r\nint i, pnum, err, found_orphan, ret = 0;\r\nech = kzalloc(ubi->ec_hdr_alsize, GFP_KERNEL);\r\nif (!ech)\r\nreturn -ENOMEM;\r\nvh = ubi_zalloc_vid_hdr(ubi, GFP_KERNEL);\r\nif (!vh) {\r\nkfree(ech);\r\nreturn -ENOMEM;\r\n}\r\ndbg_bld("scanning fastmap pool: size = %i", pool_size);\r\nfor (i = 0; i < pool_size; i++) {\r\nint scrub = 0;\r\nint image_seq;\r\npnum = be32_to_cpu(pebs[i]);\r\nif (ubi_io_is_bad(ubi, pnum)) {\r\nubi_err("bad PEB in fastmap pool!");\r\nret = UBI_BAD_FASTMAP;\r\ngoto out;\r\n}\r\nerr = ubi_io_read_ec_hdr(ubi, pnum, ech, 0);\r\nif (err && err != UBI_IO_BITFLIPS) {\r\nubi_err("unable to read EC header! PEB:%i err:%i",\r\npnum, err);\r\nret = err > 0 ? UBI_BAD_FASTMAP : err;\r\ngoto out;\r\n} else if (ret == UBI_IO_BITFLIPS)\r\nscrub = 1;\r\nimage_seq = be32_to_cpu(ech->image_seq);\r\nif (image_seq && (image_seq != ubi->image_seq)) {\r\nubi_err("bad image seq: 0x%x, expected: 0x%x",\r\nbe32_to_cpu(ech->image_seq), ubi->image_seq);\r\nret = UBI_BAD_FASTMAP;\r\ngoto out;\r\n}\r\nerr = ubi_io_read_vid_hdr(ubi, pnum, vh, 0);\r\nif (err == UBI_IO_FF || err == UBI_IO_FF_BITFLIPS) {\r\nunsigned long long ec = be64_to_cpu(ech->ec);\r\nunmap_peb(ai, pnum);\r\ndbg_bld("Adding PEB to free: %i", pnum);\r\nif (err == UBI_IO_FF_BITFLIPS)\r\nadd_aeb(ai, free, pnum, ec, 1);\r\nelse\r\nadd_aeb(ai, free, pnum, ec, 0);\r\ncontinue;\r\n} else if (err == 0 || err == UBI_IO_BITFLIPS) {\r\ndbg_bld("Found non empty PEB:%i in pool", pnum);\r\nif (err == UBI_IO_BITFLIPS)\r\nscrub = 1;\r\nfound_orphan = 0;\r\nlist_for_each_entry(tmp_aeb, eba_orphans, u.list) {\r\nif (tmp_aeb->pnum == pnum) {\r\nfound_orphan = 1;\r\nbreak;\r\n}\r\n}\r\nif (found_orphan) {\r\nlist_del(&tmp_aeb->u.list);\r\nkmem_cache_free(ai->aeb_slab_cache, tmp_aeb);\r\n}\r\nnew_aeb = kmem_cache_alloc(ai->aeb_slab_cache,\r\nGFP_KERNEL);\r\nif (!new_aeb) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nnew_aeb->ec = be64_to_cpu(ech->ec);\r\nnew_aeb->pnum = pnum;\r\nnew_aeb->lnum = be32_to_cpu(vh->lnum);\r\nnew_aeb->sqnum = be64_to_cpu(vh->sqnum);\r\nnew_aeb->copy_flag = vh->copy_flag;\r\nnew_aeb->scrub = scrub;\r\nif (*max_sqnum < new_aeb->sqnum)\r\n*max_sqnum = new_aeb->sqnum;\r\nerr = process_pool_aeb(ubi, ai, vh, new_aeb);\r\nif (err) {\r\nret = err > 0 ? UBI_BAD_FASTMAP : err;\r\ngoto out;\r\n}\r\n} else {\r\nubi_err("fastmap pool PEBs contains damaged PEBs!");\r\nret = err > 0 ? UBI_BAD_FASTMAP : err;\r\ngoto out;\r\n}\r\n}\r\nout:\r\nubi_free_vid_hdr(ubi, vh);\r\nkfree(ech);\r\nreturn ret;\r\n}\r\nstatic int count_fastmap_pebs(struct ubi_attach_info *ai)\r\n{\r\nstruct ubi_ainf_peb *aeb;\r\nstruct ubi_ainf_volume *av;\r\nstruct rb_node *rb1, *rb2;\r\nint n = 0;\r\nlist_for_each_entry(aeb, &ai->erase, u.list)\r\nn++;\r\nlist_for_each_entry(aeb, &ai->free, u.list)\r\nn++;\r\nubi_rb_for_each_entry(rb1, av, &ai->volumes, rb)\r\nubi_rb_for_each_entry(rb2, aeb, &av->root, u.rb)\r\nn++;\r\nreturn n;\r\n}\r\nstatic int ubi_attach_fastmap(struct ubi_device *ubi,\r\nstruct ubi_attach_info *ai,\r\nstruct ubi_fastmap_layout *fm)\r\n{\r\nstruct list_head used, eba_orphans, free;\r\nstruct ubi_ainf_volume *av;\r\nstruct ubi_ainf_peb *aeb, *tmp_aeb, *_tmp_aeb;\r\nstruct ubi_ec_hdr *ech;\r\nstruct ubi_fm_sb *fmsb;\r\nstruct ubi_fm_hdr *fmhdr;\r\nstruct ubi_fm_scan_pool *fmpl1, *fmpl2;\r\nstruct ubi_fm_ec *fmec;\r\nstruct ubi_fm_volhdr *fmvhdr;\r\nstruct ubi_fm_eba *fm_eba;\r\nint ret, i, j, pool_size, wl_pool_size;\r\nsize_t fm_pos = 0, fm_size = ubi->fm_size;\r\nunsigned long long max_sqnum = 0;\r\nvoid *fm_raw = ubi->fm_buf;\r\nINIT_LIST_HEAD(&used);\r\nINIT_LIST_HEAD(&free);\r\nINIT_LIST_HEAD(&eba_orphans);\r\nINIT_LIST_HEAD(&ai->corr);\r\nINIT_LIST_HEAD(&ai->free);\r\nINIT_LIST_HEAD(&ai->erase);\r\nINIT_LIST_HEAD(&ai->alien);\r\nai->volumes = RB_ROOT;\r\nai->min_ec = UBI_MAX_ERASECOUNTER;\r\nai->aeb_slab_cache = kmem_cache_create("ubi_ainf_peb_slab",\r\nsizeof(struct ubi_ainf_peb),\r\n0, 0, NULL);\r\nif (!ai->aeb_slab_cache) {\r\nret = -ENOMEM;\r\ngoto fail;\r\n}\r\nfmsb = (struct ubi_fm_sb *)(fm_raw);\r\nai->max_sqnum = fmsb->sqnum;\r\nfm_pos += sizeof(struct ubi_fm_sb);\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nfmhdr = (struct ubi_fm_hdr *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmhdr);\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nif (be32_to_cpu(fmhdr->magic) != UBI_FM_HDR_MAGIC) {\r\nubi_err("bad fastmap header magic: 0x%x, expected: 0x%x",\r\nbe32_to_cpu(fmhdr->magic), UBI_FM_HDR_MAGIC);\r\ngoto fail_bad;\r\n}\r\nfmpl1 = (struct ubi_fm_scan_pool *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmpl1);\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nif (be32_to_cpu(fmpl1->magic) != UBI_FM_POOL_MAGIC) {\r\nubi_err("bad fastmap pool magic: 0x%x, expected: 0x%x",\r\nbe32_to_cpu(fmpl1->magic), UBI_FM_POOL_MAGIC);\r\ngoto fail_bad;\r\n}\r\nfmpl2 = (struct ubi_fm_scan_pool *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmpl2);\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nif (be32_to_cpu(fmpl2->magic) != UBI_FM_POOL_MAGIC) {\r\nubi_err("bad fastmap pool magic: 0x%x, expected: 0x%x",\r\nbe32_to_cpu(fmpl2->magic), UBI_FM_POOL_MAGIC);\r\ngoto fail_bad;\r\n}\r\npool_size = be16_to_cpu(fmpl1->size);\r\nwl_pool_size = be16_to_cpu(fmpl2->size);\r\nfm->max_pool_size = be16_to_cpu(fmpl1->max_size);\r\nfm->max_wl_pool_size = be16_to_cpu(fmpl2->max_size);\r\nif (pool_size > UBI_FM_MAX_POOL_SIZE || pool_size < 0) {\r\nubi_err("bad pool size: %i", pool_size);\r\ngoto fail_bad;\r\n}\r\nif (wl_pool_size > UBI_FM_MAX_POOL_SIZE || wl_pool_size < 0) {\r\nubi_err("bad WL pool size: %i", wl_pool_size);\r\ngoto fail_bad;\r\n}\r\nif (fm->max_pool_size > UBI_FM_MAX_POOL_SIZE ||\r\nfm->max_pool_size < 0) {\r\nubi_err("bad maximal pool size: %i", fm->max_pool_size);\r\ngoto fail_bad;\r\n}\r\nif (fm->max_wl_pool_size > UBI_FM_MAX_POOL_SIZE ||\r\nfm->max_wl_pool_size < 0) {\r\nubi_err("bad maximal WL pool size: %i", fm->max_wl_pool_size);\r\ngoto fail_bad;\r\n}\r\nfor (i = 0; i < be32_to_cpu(fmhdr->free_peb_count); i++) {\r\nfmec = (struct ubi_fm_ec *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmec);\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nadd_aeb(ai, &ai->free, be32_to_cpu(fmec->pnum),\r\nbe32_to_cpu(fmec->ec), 0);\r\n}\r\nfor (i = 0; i < be32_to_cpu(fmhdr->used_peb_count); i++) {\r\nfmec = (struct ubi_fm_ec *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmec);\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nadd_aeb(ai, &used, be32_to_cpu(fmec->pnum),\r\nbe32_to_cpu(fmec->ec), 0);\r\n}\r\nfor (i = 0; i < be32_to_cpu(fmhdr->scrub_peb_count); i++) {\r\nfmec = (struct ubi_fm_ec *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmec);\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nadd_aeb(ai, &used, be32_to_cpu(fmec->pnum),\r\nbe32_to_cpu(fmec->ec), 1);\r\n}\r\nfor (i = 0; i < be32_to_cpu(fmhdr->erase_peb_count); i++) {\r\nfmec = (struct ubi_fm_ec *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmec);\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nadd_aeb(ai, &ai->erase, be32_to_cpu(fmec->pnum),\r\nbe32_to_cpu(fmec->ec), 1);\r\n}\r\nai->mean_ec = div_u64(ai->ec_sum, ai->ec_count);\r\nai->bad_peb_count = be32_to_cpu(fmhdr->bad_peb_count);\r\nfor (i = 0; i < be32_to_cpu(fmhdr->vol_count); i++) {\r\nfmvhdr = (struct ubi_fm_volhdr *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmvhdr);\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nif (be32_to_cpu(fmvhdr->magic) != UBI_FM_VHDR_MAGIC) {\r\nubi_err("bad fastmap vol header magic: 0x%x, " \\r\n"expected: 0x%x",\r\nbe32_to_cpu(fmvhdr->magic), UBI_FM_VHDR_MAGIC);\r\ngoto fail_bad;\r\n}\r\nav = add_vol(ai, be32_to_cpu(fmvhdr->vol_id),\r\nbe32_to_cpu(fmvhdr->used_ebs),\r\nbe32_to_cpu(fmvhdr->data_pad),\r\nfmvhdr->vol_type,\r\nbe32_to_cpu(fmvhdr->last_eb_bytes));\r\nif (!av)\r\ngoto fail_bad;\r\nai->vols_found++;\r\nif (ai->highest_vol_id < be32_to_cpu(fmvhdr->vol_id))\r\nai->highest_vol_id = be32_to_cpu(fmvhdr->vol_id);\r\nfm_eba = (struct ubi_fm_eba *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fm_eba);\r\nfm_pos += (sizeof(__be32) * be32_to_cpu(fm_eba->reserved_pebs));\r\nif (fm_pos >= fm_size)\r\ngoto fail_bad;\r\nif (be32_to_cpu(fm_eba->magic) != UBI_FM_EBA_MAGIC) {\r\nubi_err("bad fastmap EBA header magic: 0x%x, " \\r\n"expected: 0x%x",\r\nbe32_to_cpu(fm_eba->magic), UBI_FM_EBA_MAGIC);\r\ngoto fail_bad;\r\n}\r\nfor (j = 0; j < be32_to_cpu(fm_eba->reserved_pebs); j++) {\r\nint pnum = be32_to_cpu(fm_eba->pnum[j]);\r\nif ((int)be32_to_cpu(fm_eba->pnum[j]) < 0)\r\ncontinue;\r\naeb = NULL;\r\nlist_for_each_entry(tmp_aeb, &used, u.list) {\r\nif (tmp_aeb->pnum == pnum) {\r\naeb = tmp_aeb;\r\nbreak;\r\n}\r\n}\r\nif (!aeb) {\r\naeb = kmem_cache_alloc(ai->aeb_slab_cache,\r\nGFP_KERNEL);\r\nif (!aeb) {\r\nret = -ENOMEM;\r\ngoto fail;\r\n}\r\naeb->lnum = j;\r\naeb->pnum = be32_to_cpu(fm_eba->pnum[j]);\r\naeb->ec = -1;\r\naeb->scrub = aeb->copy_flag = aeb->sqnum = 0;\r\nlist_add_tail(&aeb->u.list, &eba_orphans);\r\ncontinue;\r\n}\r\naeb->lnum = j;\r\nif (av->highest_lnum <= aeb->lnum)\r\nav->highest_lnum = aeb->lnum;\r\nassign_aeb_to_av(ai, aeb, av);\r\ndbg_bld("inserting PEB:%i (LEB %i) to vol %i",\r\naeb->pnum, aeb->lnum, av->vol_id);\r\n}\r\nech = kzalloc(ubi->ec_hdr_alsize, GFP_KERNEL);\r\nif (!ech) {\r\nret = -ENOMEM;\r\ngoto fail;\r\n}\r\nlist_for_each_entry_safe(tmp_aeb, _tmp_aeb, &eba_orphans,\r\nu.list) {\r\nint err;\r\nif (ubi_io_is_bad(ubi, tmp_aeb->pnum)) {\r\nubi_err("bad PEB in fastmap EBA orphan list");\r\nret = UBI_BAD_FASTMAP;\r\nkfree(ech);\r\ngoto fail;\r\n}\r\nerr = ubi_io_read_ec_hdr(ubi, tmp_aeb->pnum, ech, 0);\r\nif (err && err != UBI_IO_BITFLIPS) {\r\nubi_err("unable to read EC header! PEB:%i " \\r\n"err:%i", tmp_aeb->pnum, err);\r\nret = err > 0 ? UBI_BAD_FASTMAP : err;\r\nkfree(ech);\r\ngoto fail;\r\n} else if (err == UBI_IO_BITFLIPS)\r\ntmp_aeb->scrub = 1;\r\ntmp_aeb->ec = be64_to_cpu(ech->ec);\r\nassign_aeb_to_av(ai, tmp_aeb, av);\r\n}\r\nkfree(ech);\r\n}\r\nret = scan_pool(ubi, ai, fmpl1->pebs, pool_size, &max_sqnum,\r\n&eba_orphans, &free);\r\nif (ret)\r\ngoto fail;\r\nret = scan_pool(ubi, ai, fmpl2->pebs, wl_pool_size, &max_sqnum,\r\n&eba_orphans, &free);\r\nif (ret)\r\ngoto fail;\r\nif (max_sqnum > ai->max_sqnum)\r\nai->max_sqnum = max_sqnum;\r\nlist_for_each_entry_safe(tmp_aeb, _tmp_aeb, &free, u.list)\r\nlist_move_tail(&tmp_aeb->u.list, &ai->free);\r\nubi_assert(list_empty(&used));\r\nubi_assert(list_empty(&eba_orphans));\r\nubi_assert(list_empty(&free));\r\nif (WARN_ON(count_fastmap_pebs(ai) != ubi->peb_count -\r\nai->bad_peb_count - fm->used_blocks))\r\ngoto fail_bad;\r\nreturn 0;\r\nfail_bad:\r\nret = UBI_BAD_FASTMAP;\r\nfail:\r\nlist_for_each_entry_safe(tmp_aeb, _tmp_aeb, &used, u.list) {\r\nlist_del(&tmp_aeb->u.list);\r\nkmem_cache_free(ai->aeb_slab_cache, tmp_aeb);\r\n}\r\nlist_for_each_entry_safe(tmp_aeb, _tmp_aeb, &eba_orphans, u.list) {\r\nlist_del(&tmp_aeb->u.list);\r\nkmem_cache_free(ai->aeb_slab_cache, tmp_aeb);\r\n}\r\nlist_for_each_entry_safe(tmp_aeb, _tmp_aeb, &free, u.list) {\r\nlist_del(&tmp_aeb->u.list);\r\nkmem_cache_free(ai->aeb_slab_cache, tmp_aeb);\r\n}\r\nreturn ret;\r\n}\r\nint ubi_scan_fastmap(struct ubi_device *ubi, struct ubi_attach_info *ai,\r\nint fm_anchor)\r\n{\r\nstruct ubi_fm_sb *fmsb, *fmsb2;\r\nstruct ubi_vid_hdr *vh;\r\nstruct ubi_ec_hdr *ech;\r\nstruct ubi_fastmap_layout *fm;\r\nint i, used_blocks, pnum, ret = 0;\r\nsize_t fm_size;\r\n__be32 crc, tmp_crc;\r\nunsigned long long sqnum = 0;\r\nmutex_lock(&ubi->fm_mutex);\r\nmemset(ubi->fm_buf, 0, ubi->fm_size);\r\nfmsb = kmalloc(sizeof(*fmsb), GFP_KERNEL);\r\nif (!fmsb) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nfm = kzalloc(sizeof(*fm), GFP_KERNEL);\r\nif (!fm) {\r\nret = -ENOMEM;\r\nkfree(fmsb);\r\ngoto out;\r\n}\r\nret = ubi_io_read(ubi, fmsb, fm_anchor, ubi->leb_start, sizeof(*fmsb));\r\nif (ret && ret != UBI_IO_BITFLIPS)\r\ngoto free_fm_sb;\r\nelse if (ret == UBI_IO_BITFLIPS)\r\nfm->to_be_tortured[0] = 1;\r\nif (be32_to_cpu(fmsb->magic) != UBI_FM_SB_MAGIC) {\r\nubi_err("bad super block magic: 0x%x, expected: 0x%x",\r\nbe32_to_cpu(fmsb->magic), UBI_FM_SB_MAGIC);\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_fm_sb;\r\n}\r\nif (fmsb->version != UBI_FM_FMT_VERSION) {\r\nubi_err("bad fastmap version: %i, expected: %i",\r\nfmsb->version, UBI_FM_FMT_VERSION);\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_fm_sb;\r\n}\r\nused_blocks = be32_to_cpu(fmsb->used_blocks);\r\nif (used_blocks > UBI_FM_MAX_BLOCKS || used_blocks < 1) {\r\nubi_err("number of fastmap blocks is invalid: %i", used_blocks);\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_fm_sb;\r\n}\r\nfm_size = ubi->leb_size * used_blocks;\r\nif (fm_size != ubi->fm_size) {\r\nubi_err("bad fastmap size: %zi, expected: %zi", fm_size,\r\nubi->fm_size);\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_fm_sb;\r\n}\r\nech = kzalloc(ubi->ec_hdr_alsize, GFP_KERNEL);\r\nif (!ech) {\r\nret = -ENOMEM;\r\ngoto free_fm_sb;\r\n}\r\nvh = ubi_zalloc_vid_hdr(ubi, GFP_KERNEL);\r\nif (!vh) {\r\nret = -ENOMEM;\r\ngoto free_hdr;\r\n}\r\nfor (i = 0; i < used_blocks; i++) {\r\nint image_seq;\r\npnum = be32_to_cpu(fmsb->block_loc[i]);\r\nif (ubi_io_is_bad(ubi, pnum)) {\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_hdr;\r\n}\r\nret = ubi_io_read_ec_hdr(ubi, pnum, ech, 0);\r\nif (ret && ret != UBI_IO_BITFLIPS) {\r\nubi_err("unable to read fastmap block# %i EC (PEB: %i)",\r\ni, pnum);\r\nif (ret > 0)\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_hdr;\r\n} else if (ret == UBI_IO_BITFLIPS)\r\nfm->to_be_tortured[i] = 1;\r\nimage_seq = be32_to_cpu(ech->image_seq);\r\nif (!ubi->image_seq)\r\nubi->image_seq = image_seq;\r\nif (image_seq && (image_seq != ubi->image_seq)) {\r\nubi_err("wrong image seq:%d instead of %d",\r\nbe32_to_cpu(ech->image_seq), ubi->image_seq);\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_hdr;\r\n}\r\nret = ubi_io_read_vid_hdr(ubi, pnum, vh, 0);\r\nif (ret && ret != UBI_IO_BITFLIPS) {\r\nubi_err("unable to read fastmap block# %i (PEB: %i)",\r\ni, pnum);\r\ngoto free_hdr;\r\n}\r\nif (i == 0) {\r\nif (be32_to_cpu(vh->vol_id) != UBI_FM_SB_VOLUME_ID) {\r\nubi_err("bad fastmap anchor vol_id: 0x%x," \\r\n" expected: 0x%x",\r\nbe32_to_cpu(vh->vol_id),\r\nUBI_FM_SB_VOLUME_ID);\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_hdr;\r\n}\r\n} else {\r\nif (be32_to_cpu(vh->vol_id) != UBI_FM_DATA_VOLUME_ID) {\r\nubi_err("bad fastmap data vol_id: 0x%x," \\r\n" expected: 0x%x",\r\nbe32_to_cpu(vh->vol_id),\r\nUBI_FM_DATA_VOLUME_ID);\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_hdr;\r\n}\r\n}\r\nif (sqnum < be64_to_cpu(vh->sqnum))\r\nsqnum = be64_to_cpu(vh->sqnum);\r\nret = ubi_io_read(ubi, ubi->fm_buf + (ubi->leb_size * i), pnum,\r\nubi->leb_start, ubi->leb_size);\r\nif (ret && ret != UBI_IO_BITFLIPS) {\r\nubi_err("unable to read fastmap block# %i (PEB: %i, " \\r\n"err: %i)", i, pnum, ret);\r\ngoto free_hdr;\r\n}\r\n}\r\nkfree(fmsb);\r\nfmsb = NULL;\r\nfmsb2 = (struct ubi_fm_sb *)(ubi->fm_buf);\r\ntmp_crc = be32_to_cpu(fmsb2->data_crc);\r\nfmsb2->data_crc = 0;\r\ncrc = crc32(UBI_CRC32_INIT, ubi->fm_buf, fm_size);\r\nif (crc != tmp_crc) {\r\nubi_err("fastmap data CRC is invalid");\r\nubi_err("CRC should be: 0x%x, calc: 0x%x", tmp_crc, crc);\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_hdr;\r\n}\r\nfmsb2->sqnum = sqnum;\r\nfm->used_blocks = used_blocks;\r\nret = ubi_attach_fastmap(ubi, ai, fm);\r\nif (ret) {\r\nif (ret > 0)\r\nret = UBI_BAD_FASTMAP;\r\ngoto free_hdr;\r\n}\r\nfor (i = 0; i < used_blocks; i++) {\r\nstruct ubi_wl_entry *e;\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e) {\r\nwhile (i--)\r\nkfree(fm->e[i]);\r\nret = -ENOMEM;\r\ngoto free_hdr;\r\n}\r\ne->pnum = be32_to_cpu(fmsb2->block_loc[i]);\r\ne->ec = be32_to_cpu(fmsb2->block_ec[i]);\r\nfm->e[i] = e;\r\n}\r\nubi->fm = fm;\r\nubi->fm_pool.max_size = ubi->fm->max_pool_size;\r\nubi->fm_wl_pool.max_size = ubi->fm->max_wl_pool_size;\r\nubi_msg("attached by fastmap");\r\nubi_msg("fastmap pool size: %d", ubi->fm_pool.max_size);\r\nubi_msg("fastmap WL pool size: %d", ubi->fm_wl_pool.max_size);\r\nubi->fm_disabled = 0;\r\nubi_free_vid_hdr(ubi, vh);\r\nkfree(ech);\r\nout:\r\nmutex_unlock(&ubi->fm_mutex);\r\nif (ret == UBI_BAD_FASTMAP)\r\nubi_err("Attach by fastmap failed, doing a full scan!");\r\nreturn ret;\r\nfree_hdr:\r\nubi_free_vid_hdr(ubi, vh);\r\nkfree(ech);\r\nfree_fm_sb:\r\nkfree(fmsb);\r\nkfree(fm);\r\ngoto out;\r\n}\r\nstatic int ubi_write_fastmap(struct ubi_device *ubi,\r\nstruct ubi_fastmap_layout *new_fm)\r\n{\r\nsize_t fm_pos = 0;\r\nvoid *fm_raw;\r\nstruct ubi_fm_sb *fmsb;\r\nstruct ubi_fm_hdr *fmh;\r\nstruct ubi_fm_scan_pool *fmpl1, *fmpl2;\r\nstruct ubi_fm_ec *fec;\r\nstruct ubi_fm_volhdr *fvh;\r\nstruct ubi_fm_eba *feba;\r\nstruct rb_node *node;\r\nstruct ubi_wl_entry *wl_e;\r\nstruct ubi_volume *vol;\r\nstruct ubi_vid_hdr *avhdr, *dvhdr;\r\nstruct ubi_work *ubi_wrk;\r\nint ret, i, j, free_peb_count, used_peb_count, vol_count;\r\nint scrub_peb_count, erase_peb_count;\r\nfm_raw = ubi->fm_buf;\r\nmemset(ubi->fm_buf, 0, ubi->fm_size);\r\navhdr = new_fm_vhdr(ubi, UBI_FM_SB_VOLUME_ID);\r\nif (!avhdr) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ndvhdr = new_fm_vhdr(ubi, UBI_FM_DATA_VOLUME_ID);\r\nif (!dvhdr) {\r\nret = -ENOMEM;\r\ngoto out_kfree;\r\n}\r\nspin_lock(&ubi->volumes_lock);\r\nspin_lock(&ubi->wl_lock);\r\nfmsb = (struct ubi_fm_sb *)fm_raw;\r\nfm_pos += sizeof(*fmsb);\r\nubi_assert(fm_pos <= ubi->fm_size);\r\nfmh = (struct ubi_fm_hdr *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmh);\r\nubi_assert(fm_pos <= ubi->fm_size);\r\nfmsb->magic = cpu_to_be32(UBI_FM_SB_MAGIC);\r\nfmsb->version = UBI_FM_FMT_VERSION;\r\nfmsb->used_blocks = cpu_to_be32(new_fm->used_blocks);\r\nfmsb->sqnum = 0;\r\nfmh->magic = cpu_to_be32(UBI_FM_HDR_MAGIC);\r\nfree_peb_count = 0;\r\nused_peb_count = 0;\r\nscrub_peb_count = 0;\r\nerase_peb_count = 0;\r\nvol_count = 0;\r\nfmpl1 = (struct ubi_fm_scan_pool *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmpl1);\r\nfmpl1->magic = cpu_to_be32(UBI_FM_POOL_MAGIC);\r\nfmpl1->size = cpu_to_be16(ubi->fm_pool.size);\r\nfmpl1->max_size = cpu_to_be16(ubi->fm_pool.max_size);\r\nfor (i = 0; i < ubi->fm_pool.size; i++)\r\nfmpl1->pebs[i] = cpu_to_be32(ubi->fm_pool.pebs[i]);\r\nfmpl2 = (struct ubi_fm_scan_pool *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fmpl2);\r\nfmpl2->magic = cpu_to_be32(UBI_FM_POOL_MAGIC);\r\nfmpl2->size = cpu_to_be16(ubi->fm_wl_pool.size);\r\nfmpl2->max_size = cpu_to_be16(ubi->fm_wl_pool.max_size);\r\nfor (i = 0; i < ubi->fm_wl_pool.size; i++)\r\nfmpl2->pebs[i] = cpu_to_be32(ubi->fm_wl_pool.pebs[i]);\r\nfor (node = rb_first(&ubi->free); node; node = rb_next(node)) {\r\nwl_e = rb_entry(node, struct ubi_wl_entry, u.rb);\r\nfec = (struct ubi_fm_ec *)(fm_raw + fm_pos);\r\nfec->pnum = cpu_to_be32(wl_e->pnum);\r\nfec->ec = cpu_to_be32(wl_e->ec);\r\nfree_peb_count++;\r\nfm_pos += sizeof(*fec);\r\nubi_assert(fm_pos <= ubi->fm_size);\r\n}\r\nfmh->free_peb_count = cpu_to_be32(free_peb_count);\r\nfor (node = rb_first(&ubi->used); node; node = rb_next(node)) {\r\nwl_e = rb_entry(node, struct ubi_wl_entry, u.rb);\r\nfec = (struct ubi_fm_ec *)(fm_raw + fm_pos);\r\nfec->pnum = cpu_to_be32(wl_e->pnum);\r\nfec->ec = cpu_to_be32(wl_e->ec);\r\nused_peb_count++;\r\nfm_pos += sizeof(*fec);\r\nubi_assert(fm_pos <= ubi->fm_size);\r\n}\r\nfmh->used_peb_count = cpu_to_be32(used_peb_count);\r\nfor (node = rb_first(&ubi->scrub); node; node = rb_next(node)) {\r\nwl_e = rb_entry(node, struct ubi_wl_entry, u.rb);\r\nfec = (struct ubi_fm_ec *)(fm_raw + fm_pos);\r\nfec->pnum = cpu_to_be32(wl_e->pnum);\r\nfec->ec = cpu_to_be32(wl_e->ec);\r\nscrub_peb_count++;\r\nfm_pos += sizeof(*fec);\r\nubi_assert(fm_pos <= ubi->fm_size);\r\n}\r\nfmh->scrub_peb_count = cpu_to_be32(scrub_peb_count);\r\nlist_for_each_entry(ubi_wrk, &ubi->works, list) {\r\nif (ubi_is_erase_work(ubi_wrk)) {\r\nwl_e = ubi_wrk->e;\r\nubi_assert(wl_e);\r\nfec = (struct ubi_fm_ec *)(fm_raw + fm_pos);\r\nfec->pnum = cpu_to_be32(wl_e->pnum);\r\nfec->ec = cpu_to_be32(wl_e->ec);\r\nerase_peb_count++;\r\nfm_pos += sizeof(*fec);\r\nubi_assert(fm_pos <= ubi->fm_size);\r\n}\r\n}\r\nfmh->erase_peb_count = cpu_to_be32(erase_peb_count);\r\nfor (i = 0; i < UBI_MAX_VOLUMES + UBI_INT_VOL_COUNT; i++) {\r\nvol = ubi->volumes[i];\r\nif (!vol)\r\ncontinue;\r\nvol_count++;\r\nfvh = (struct ubi_fm_volhdr *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*fvh);\r\nubi_assert(fm_pos <= ubi->fm_size);\r\nfvh->magic = cpu_to_be32(UBI_FM_VHDR_MAGIC);\r\nfvh->vol_id = cpu_to_be32(vol->vol_id);\r\nfvh->vol_type = vol->vol_type;\r\nfvh->used_ebs = cpu_to_be32(vol->used_ebs);\r\nfvh->data_pad = cpu_to_be32(vol->data_pad);\r\nfvh->last_eb_bytes = cpu_to_be32(vol->last_eb_bytes);\r\nubi_assert(vol->vol_type == UBI_DYNAMIC_VOLUME ||\r\nvol->vol_type == UBI_STATIC_VOLUME);\r\nfeba = (struct ubi_fm_eba *)(fm_raw + fm_pos);\r\nfm_pos += sizeof(*feba) + (sizeof(__be32) * vol->reserved_pebs);\r\nubi_assert(fm_pos <= ubi->fm_size);\r\nfor (j = 0; j < vol->reserved_pebs; j++)\r\nfeba->pnum[j] = cpu_to_be32(vol->eba_tbl[j]);\r\nfeba->reserved_pebs = cpu_to_be32(j);\r\nfeba->magic = cpu_to_be32(UBI_FM_EBA_MAGIC);\r\n}\r\nfmh->vol_count = cpu_to_be32(vol_count);\r\nfmh->bad_peb_count = cpu_to_be32(ubi->bad_peb_count);\r\navhdr->sqnum = cpu_to_be64(ubi_next_sqnum(ubi));\r\navhdr->lnum = 0;\r\nspin_unlock(&ubi->wl_lock);\r\nspin_unlock(&ubi->volumes_lock);\r\ndbg_bld("writing fastmap SB to PEB %i", new_fm->e[0]->pnum);\r\nret = ubi_io_write_vid_hdr(ubi, new_fm->e[0]->pnum, avhdr);\r\nif (ret) {\r\nubi_err("unable to write vid_hdr to fastmap SB!");\r\ngoto out_kfree;\r\n}\r\nfor (i = 0; i < new_fm->used_blocks; i++) {\r\nfmsb->block_loc[i] = cpu_to_be32(new_fm->e[i]->pnum);\r\nfmsb->block_ec[i] = cpu_to_be32(new_fm->e[i]->ec);\r\n}\r\nfmsb->data_crc = 0;\r\nfmsb->data_crc = cpu_to_be32(crc32(UBI_CRC32_INIT, fm_raw,\r\nubi->fm_size));\r\nfor (i = 1; i < new_fm->used_blocks; i++) {\r\ndvhdr->sqnum = cpu_to_be64(ubi_next_sqnum(ubi));\r\ndvhdr->lnum = cpu_to_be32(i);\r\ndbg_bld("writing fastmap data to PEB %i sqnum %llu",\r\nnew_fm->e[i]->pnum, be64_to_cpu(dvhdr->sqnum));\r\nret = ubi_io_write_vid_hdr(ubi, new_fm->e[i]->pnum, dvhdr);\r\nif (ret) {\r\nubi_err("unable to write vid_hdr to PEB %i!",\r\nnew_fm->e[i]->pnum);\r\ngoto out_kfree;\r\n}\r\n}\r\nfor (i = 0; i < new_fm->used_blocks; i++) {\r\nret = ubi_io_write(ubi, fm_raw + (i * ubi->leb_size),\r\nnew_fm->e[i]->pnum, ubi->leb_start, ubi->leb_size);\r\nif (ret) {\r\nubi_err("unable to write fastmap to PEB %i!",\r\nnew_fm->e[i]->pnum);\r\ngoto out_kfree;\r\n}\r\n}\r\nubi_assert(new_fm);\r\nubi->fm = new_fm;\r\ndbg_bld("fastmap written!");\r\nout_kfree:\r\nubi_free_vid_hdr(ubi, avhdr);\r\nubi_free_vid_hdr(ubi, dvhdr);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int erase_block(struct ubi_device *ubi, int pnum)\r\n{\r\nint ret;\r\nstruct ubi_ec_hdr *ec_hdr;\r\nlong long ec;\r\nec_hdr = kzalloc(ubi->ec_hdr_alsize, GFP_KERNEL);\r\nif (!ec_hdr)\r\nreturn -ENOMEM;\r\nret = ubi_io_read_ec_hdr(ubi, pnum, ec_hdr, 0);\r\nif (ret < 0)\r\ngoto out;\r\nelse if (ret && ret != UBI_IO_BITFLIPS) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nret = ubi_io_sync_erase(ubi, pnum, 0);\r\nif (ret < 0)\r\ngoto out;\r\nec = be64_to_cpu(ec_hdr->ec);\r\nec += ret;\r\nif (ec > UBI_MAX_ERASECOUNTER) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nec_hdr->ec = cpu_to_be64(ec);\r\nret = ubi_io_write_ec_hdr(ubi, pnum, ec_hdr);\r\nif (ret < 0)\r\ngoto out;\r\nret = ec;\r\nout:\r\nkfree(ec_hdr);\r\nreturn ret;\r\n}\r\nstatic int invalidate_fastmap(struct ubi_device *ubi,\r\nstruct ubi_fastmap_layout *fm)\r\n{\r\nint ret;\r\nstruct ubi_vid_hdr *vh;\r\nret = erase_block(ubi, fm->e[0]->pnum);\r\nif (ret < 0)\r\nreturn ret;\r\nvh = new_fm_vhdr(ubi, UBI_FM_SB_VOLUME_ID);\r\nif (!vh)\r\nreturn -ENOMEM;\r\nvh->sqnum = cpu_to_be64(ubi_next_sqnum(ubi));\r\nret = ubi_io_write_vid_hdr(ubi, fm->e[0]->pnum, vh);\r\nreturn ret;\r\n}\r\nint ubi_update_fastmap(struct ubi_device *ubi)\r\n{\r\nint ret, i;\r\nstruct ubi_fastmap_layout *new_fm, *old_fm;\r\nstruct ubi_wl_entry *tmp_e;\r\nmutex_lock(&ubi->fm_mutex);\r\nubi_refill_pools(ubi);\r\nif (ubi->ro_mode || ubi->fm_disabled) {\r\nmutex_unlock(&ubi->fm_mutex);\r\nreturn 0;\r\n}\r\nret = ubi_ensure_anchor_pebs(ubi);\r\nif (ret) {\r\nmutex_unlock(&ubi->fm_mutex);\r\nreturn ret;\r\n}\r\nnew_fm = kzalloc(sizeof(*new_fm), GFP_KERNEL);\r\nif (!new_fm) {\r\nmutex_unlock(&ubi->fm_mutex);\r\nreturn -ENOMEM;\r\n}\r\nnew_fm->used_blocks = ubi->fm_size / ubi->leb_size;\r\nfor (i = 0; i < new_fm->used_blocks; i++) {\r\nnew_fm->e[i] = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!new_fm->e[i]) {\r\nwhile (i--)\r\nkfree(new_fm->e[i]);\r\nkfree(new_fm);\r\nmutex_unlock(&ubi->fm_mutex);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nold_fm = ubi->fm;\r\nubi->fm = NULL;\r\nif (new_fm->used_blocks > UBI_FM_MAX_BLOCKS) {\r\nubi_err("fastmap too large");\r\nret = -ENOSPC;\r\ngoto err;\r\n}\r\nfor (i = 1; i < new_fm->used_blocks; i++) {\r\nspin_lock(&ubi->wl_lock);\r\ntmp_e = ubi_wl_get_fm_peb(ubi, 0);\r\nspin_unlock(&ubi->wl_lock);\r\nif (!tmp_e && !old_fm) {\r\nint j;\r\nubi_err("could not get any free erase block");\r\nfor (j = 1; j < i; j++)\r\nubi_wl_put_fm_peb(ubi, new_fm->e[j], j, 0);\r\nret = -ENOSPC;\r\ngoto err;\r\n} else if (!tmp_e && old_fm) {\r\nret = erase_block(ubi, old_fm->e[i]->pnum);\r\nif (ret < 0) {\r\nint j;\r\nfor (j = 1; j < i; j++)\r\nubi_wl_put_fm_peb(ubi, new_fm->e[j],\r\nj, 0);\r\nubi_err("could not erase old fastmap PEB");\r\ngoto err;\r\n}\r\nnew_fm->e[i]->pnum = old_fm->e[i]->pnum;\r\nnew_fm->e[i]->ec = old_fm->e[i]->ec;\r\n} else {\r\nnew_fm->e[i]->pnum = tmp_e->pnum;\r\nnew_fm->e[i]->ec = tmp_e->ec;\r\nif (old_fm)\r\nubi_wl_put_fm_peb(ubi, old_fm->e[i], i,\r\nold_fm->to_be_tortured[i]);\r\n}\r\n}\r\nspin_lock(&ubi->wl_lock);\r\ntmp_e = ubi_wl_get_fm_peb(ubi, 1);\r\nspin_unlock(&ubi->wl_lock);\r\nif (old_fm) {\r\nif (!tmp_e) {\r\nret = erase_block(ubi, old_fm->e[0]->pnum);\r\nif (ret < 0) {\r\nint i;\r\nubi_err("could not erase old anchor PEB");\r\nfor (i = 1; i < new_fm->used_blocks; i++)\r\nubi_wl_put_fm_peb(ubi, new_fm->e[i],\r\ni, 0);\r\ngoto err;\r\n}\r\nnew_fm->e[0]->pnum = old_fm->e[0]->pnum;\r\nnew_fm->e[0]->ec = ret;\r\n} else {\r\nubi_wl_put_fm_peb(ubi, old_fm->e[0], 0,\r\nold_fm->to_be_tortured[0]);\r\nnew_fm->e[0]->pnum = tmp_e->pnum;\r\nnew_fm->e[0]->ec = tmp_e->ec;\r\n}\r\n} else {\r\nif (!tmp_e) {\r\nint i;\r\nubi_err("could not find any anchor PEB");\r\nfor (i = 1; i < new_fm->used_blocks; i++)\r\nubi_wl_put_fm_peb(ubi, new_fm->e[i], i, 0);\r\nret = -ENOSPC;\r\ngoto err;\r\n}\r\nnew_fm->e[0]->pnum = tmp_e->pnum;\r\nnew_fm->e[0]->ec = tmp_e->ec;\r\n}\r\ndown_write(&ubi->work_sem);\r\ndown_write(&ubi->fm_sem);\r\nret = ubi_write_fastmap(ubi, new_fm);\r\nup_write(&ubi->fm_sem);\r\nup_write(&ubi->work_sem);\r\nif (ret)\r\ngoto err;\r\nout_unlock:\r\nmutex_unlock(&ubi->fm_mutex);\r\nkfree(old_fm);\r\nreturn ret;\r\nerr:\r\nkfree(new_fm);\r\nubi_warn("Unable to write new fastmap, err=%i", ret);\r\nret = 0;\r\nif (old_fm) {\r\nret = invalidate_fastmap(ubi, old_fm);\r\nif (ret < 0)\r\nubi_err("Unable to invalidiate current fastmap!");\r\nelse if (ret)\r\nret = 0;\r\n}\r\ngoto out_unlock;\r\n}
