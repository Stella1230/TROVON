int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release)\r\n{\r\natomic_set(&ref->count, 1 + PCPU_COUNT_BIAS);\r\nref->pcpu_count = alloc_percpu(unsigned);\r\nif (!ref->pcpu_count)\r\nreturn -ENOMEM;\r\nref->release = release;\r\nreturn 0;\r\n}\r\nvoid percpu_ref_cancel_init(struct percpu_ref *ref)\r\n{\r\nunsigned __percpu *pcpu_count = ref->pcpu_count;\r\nint cpu;\r\nWARN_ON_ONCE(atomic_read(&ref->count) != 1 + PCPU_COUNT_BIAS);\r\nif (pcpu_count) {\r\nfor_each_possible_cpu(cpu)\r\nWARN_ON_ONCE(*per_cpu_ptr(pcpu_count, cpu));\r\nfree_percpu(ref->pcpu_count);\r\n}\r\n}\r\nstatic void percpu_ref_kill_rcu(struct rcu_head *rcu)\r\n{\r\nstruct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);\r\nunsigned __percpu *pcpu_count = ref->pcpu_count;\r\nunsigned count = 0;\r\nint cpu;\r\npcpu_count = (unsigned __percpu *)\r\n(((unsigned long) pcpu_count) & ~PCPU_STATUS_MASK);\r\nfor_each_possible_cpu(cpu)\r\ncount += *per_cpu_ptr(pcpu_count, cpu);\r\nfree_percpu(pcpu_count);\r\npr_debug("global %i pcpu %i", atomic_read(&ref->count), (int) count);\r\natomic_add((int) count - PCPU_COUNT_BIAS, &ref->count);\r\nWARN_ONCE(atomic_read(&ref->count) <= 0, "percpu ref <= 0 (%i)",\r\natomic_read(&ref->count));\r\nif (ref->confirm_kill)\r\nref->confirm_kill(ref);\r\npercpu_ref_put(ref);\r\n}\r\nvoid percpu_ref_kill_and_confirm(struct percpu_ref *ref,\r\npercpu_ref_func_t *confirm_kill)\r\n{\r\nWARN_ONCE(REF_STATUS(ref->pcpu_count) == PCPU_REF_DEAD,\r\n"percpu_ref_kill() called more than once!\n");\r\nref->pcpu_count = (unsigned __percpu *)\r\n(((unsigned long) ref->pcpu_count)|PCPU_REF_DEAD);\r\nref->confirm_kill = confirm_kill;\r\ncall_rcu_sched(&ref->rcu, percpu_ref_kill_rcu);\r\n}
