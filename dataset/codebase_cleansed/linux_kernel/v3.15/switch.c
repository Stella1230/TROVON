static inline void acquire_spu_lock(struct spu *spu)\r\n{\r\n}\r\nstatic inline void release_spu_lock(struct spu *spu)\r\n{\r\n}\r\nstatic inline int check_spu_isolate(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nu32 isolate_state;\r\nisolate_state = SPU_STATUS_ISOLATED_STATE |\r\nSPU_STATUS_ISOLATED_LOAD_STATUS | SPU_STATUS_ISOLATED_EXIT_STATUS;\r\nreturn (in_be32(&prob->spu_status_R) & isolate_state) ? 1 : 0;\r\n}\r\nstatic inline void disable_interrupts(struct spu_state *csa, struct spu *spu)\r\n{\r\nspin_lock_irq(&spu->register_lock);\r\nif (csa) {\r\ncsa->priv1.int_mask_class0_RW = spu_int_mask_get(spu, 0);\r\ncsa->priv1.int_mask_class1_RW = spu_int_mask_get(spu, 1);\r\ncsa->priv1.int_mask_class2_RW = spu_int_mask_get(spu, 2);\r\n}\r\nspu_int_mask_set(spu, 0, 0ul);\r\nspu_int_mask_set(spu, 1, 0ul);\r\nspu_int_mask_set(spu, 2, 0ul);\r\neieio();\r\nspin_unlock_irq(&spu->register_lock);\r\nset_bit(SPU_CONTEXT_SWITCH_PENDING, &spu->flags);\r\nclear_bit(SPU_CONTEXT_FAULT_PENDING, &spu->flags);\r\nsynchronize_irq(spu->irqs[0]);\r\nsynchronize_irq(spu->irqs[1]);\r\nsynchronize_irq(spu->irqs[2]);\r\n}\r\nstatic inline void set_watchdog_timer(struct spu_state *csa, struct spu *spu)\r\n{\r\n}\r\nstatic inline void inhibit_user_access(struct spu_state *csa, struct spu *spu)\r\n{\r\n}\r\nstatic inline void set_switch_pending(struct spu_state *csa, struct spu *spu)\r\n{\r\n}\r\nstatic inline void save_mfc_cntl(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nswitch (in_be64(&priv2->mfc_control_RW) &\r\nMFC_CNTL_SUSPEND_DMA_STATUS_MASK) {\r\ncase MFC_CNTL_SUSPEND_IN_PROGRESS:\r\nPOLL_WHILE_FALSE((in_be64(&priv2->mfc_control_RW) &\r\nMFC_CNTL_SUSPEND_DMA_STATUS_MASK) ==\r\nMFC_CNTL_SUSPEND_COMPLETE);\r\ncase MFC_CNTL_SUSPEND_COMPLETE:\r\nif (csa)\r\ncsa->priv2.mfc_control_RW =\r\nin_be64(&priv2->mfc_control_RW) |\r\nMFC_CNTL_SUSPEND_DMA_QUEUE;\r\nbreak;\r\ncase MFC_CNTL_NORMAL_DMA_QUEUE_OPERATION:\r\nout_be64(&priv2->mfc_control_RW, MFC_CNTL_SUSPEND_DMA_QUEUE);\r\nPOLL_WHILE_FALSE((in_be64(&priv2->mfc_control_RW) &\r\nMFC_CNTL_SUSPEND_DMA_STATUS_MASK) ==\r\nMFC_CNTL_SUSPEND_COMPLETE);\r\nif (csa)\r\ncsa->priv2.mfc_control_RW =\r\nin_be64(&priv2->mfc_control_RW) &\r\n~MFC_CNTL_SUSPEND_DMA_QUEUE &\r\n~MFC_CNTL_SUSPEND_MASK;\r\nbreak;\r\n}\r\n}\r\nstatic inline void save_spu_runcntl(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\ncsa->prob.spu_runcntl_RW = in_be32(&prob->spu_runcntl_RW);\r\n}\r\nstatic inline void save_mfc_sr1(struct spu_state *csa, struct spu *spu)\r\n{\r\ncsa->priv1.mfc_sr1_RW = spu_mfc_sr1_get(spu);\r\n}\r\nstatic inline void save_spu_status(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nif ((in_be32(&prob->spu_status_R) & SPU_STATUS_RUNNING) == 0) {\r\ncsa->prob.spu_status_R = in_be32(&prob->spu_status_R);\r\n} else {\r\nu32 stopped;\r\nout_be32(&prob->spu_runcntl_RW, SPU_RUNCNTL_STOP);\r\neieio();\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\nstopped =\r\nSPU_STATUS_INVALID_INSTR | SPU_STATUS_SINGLE_STEP |\r\nSPU_STATUS_STOPPED_BY_HALT | SPU_STATUS_STOPPED_BY_STOP;\r\nif ((in_be32(&prob->spu_status_R) & stopped) == 0)\r\ncsa->prob.spu_status_R = SPU_STATUS_RUNNING;\r\nelse\r\ncsa->prob.spu_status_R = in_be32(&prob->spu_status_R);\r\n}\r\n}\r\nstatic inline void save_mfc_stopped_status(struct spu_state *csa,\r\nstruct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nconst u64 mask = MFC_CNTL_DECREMENTER_RUNNING |\r\nMFC_CNTL_DMA_QUEUES_EMPTY;\r\ncsa->priv2.mfc_control_RW &= ~mask;\r\ncsa->priv2.mfc_control_RW |= in_be64(&priv2->mfc_control_RW) & mask;\r\n}\r\nstatic inline void halt_mfc_decr(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->mfc_control_RW,\r\nMFC_CNTL_DECREMENTER_HALTED | MFC_CNTL_SUSPEND_MASK);\r\neieio();\r\n}\r\nstatic inline void save_timebase(struct spu_state *csa, struct spu *spu)\r\n{\r\ncsa->suspend_time = get_cycles();\r\n}\r\nstatic inline void remove_other_spu_access(struct spu_state *csa,\r\nstruct spu *spu)\r\n{\r\n}\r\nstatic inline void do_mfc_mssync(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nout_be64(&prob->spc_mssync_RW, 1UL);\r\nPOLL_WHILE_TRUE(in_be64(&prob->spc_mssync_RW) & MS_SYNC_PENDING);\r\n}\r\nstatic inline void issue_mfc_tlbie(struct spu_state *csa, struct spu *spu)\r\n{\r\nspu_tlb_invalidate(spu);\r\nmb();\r\n}\r\nstatic inline void handle_pending_interrupts(struct spu_state *csa,\r\nstruct spu *spu)\r\n{\r\n}\r\nstatic inline void save_mfc_queues(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nint i;\r\nif ((in_be64(&priv2->mfc_control_RW) & MFC_CNTL_DMA_QUEUES_EMPTY) == 0) {\r\nfor (i = 0; i < 8; i++) {\r\ncsa->priv2.puq[i].mfc_cq_data0_RW =\r\nin_be64(&priv2->puq[i].mfc_cq_data0_RW);\r\ncsa->priv2.puq[i].mfc_cq_data1_RW =\r\nin_be64(&priv2->puq[i].mfc_cq_data1_RW);\r\ncsa->priv2.puq[i].mfc_cq_data2_RW =\r\nin_be64(&priv2->puq[i].mfc_cq_data2_RW);\r\ncsa->priv2.puq[i].mfc_cq_data3_RW =\r\nin_be64(&priv2->puq[i].mfc_cq_data3_RW);\r\n}\r\nfor (i = 0; i < 16; i++) {\r\ncsa->priv2.spuq[i].mfc_cq_data0_RW =\r\nin_be64(&priv2->spuq[i].mfc_cq_data0_RW);\r\ncsa->priv2.spuq[i].mfc_cq_data1_RW =\r\nin_be64(&priv2->spuq[i].mfc_cq_data1_RW);\r\ncsa->priv2.spuq[i].mfc_cq_data2_RW =\r\nin_be64(&priv2->spuq[i].mfc_cq_data2_RW);\r\ncsa->priv2.spuq[i].mfc_cq_data3_RW =\r\nin_be64(&priv2->spuq[i].mfc_cq_data3_RW);\r\n}\r\n}\r\n}\r\nstatic inline void save_ppu_querymask(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\ncsa->prob.dma_querymask_RW = in_be32(&prob->dma_querymask_RW);\r\n}\r\nstatic inline void save_ppu_querytype(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\ncsa->prob.dma_querytype_RW = in_be32(&prob->dma_querytype_RW);\r\n}\r\nstatic inline void save_ppu_tagstatus(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\ncsa->prob.dma_tagstatus_R = in_be32(&prob->dma_tagstatus_R);\r\n}\r\nstatic inline void save_mfc_csr_tsq(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\ncsa->priv2.spu_tag_status_query_RW =\r\nin_be64(&priv2->spu_tag_status_query_RW);\r\n}\r\nstatic inline void save_mfc_csr_cmd(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\ncsa->priv2.spu_cmd_buf1_RW = in_be64(&priv2->spu_cmd_buf1_RW);\r\ncsa->priv2.spu_cmd_buf2_RW = in_be64(&priv2->spu_cmd_buf2_RW);\r\n}\r\nstatic inline void save_mfc_csr_ato(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\ncsa->priv2.spu_atomic_status_RW = in_be64(&priv2->spu_atomic_status_RW);\r\n}\r\nstatic inline void save_mfc_tclass_id(struct spu_state *csa, struct spu *spu)\r\n{\r\ncsa->priv1.mfc_tclass_id_RW = spu_mfc_tclass_id_get(spu);\r\n}\r\nstatic inline void set_mfc_tclass_id(struct spu_state *csa, struct spu *spu)\r\n{\r\nspu_mfc_tclass_id_set(spu, 0x10000000);\r\neieio();\r\n}\r\nstatic inline void purge_mfc_queue(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->mfc_control_RW,\r\nMFC_CNTL_PURGE_DMA_REQUEST |\r\nMFC_CNTL_SUSPEND_MASK);\r\neieio();\r\n}\r\nstatic inline void wait_purge_complete(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nPOLL_WHILE_FALSE((in_be64(&priv2->mfc_control_RW) &\r\nMFC_CNTL_PURGE_DMA_STATUS_MASK) ==\r\nMFC_CNTL_PURGE_DMA_COMPLETE);\r\n}\r\nstatic inline void setup_mfc_sr1(struct spu_state *csa, struct spu *spu)\r\n{\r\nspu_mfc_sr1_set(spu, (MFC_STATE1_MASTER_RUN_CONTROL_MASK |\r\nMFC_STATE1_RELOCATE_MASK |\r\nMFC_STATE1_BUS_TLBIE_MASK));\r\n}\r\nstatic inline void save_spu_npc(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\ncsa->prob.spu_npc_RW = in_be32(&prob->spu_npc_RW);\r\n}\r\nstatic inline void save_spu_privcntl(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\ncsa->priv2.spu_privcntl_RW = in_be64(&priv2->spu_privcntl_RW);\r\n}\r\nstatic inline void reset_spu_privcntl(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->spu_privcntl_RW, 0UL);\r\neieio();\r\n}\r\nstatic inline void save_spu_lslr(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\ncsa->priv2.spu_lslr_RW = in_be64(&priv2->spu_lslr_RW);\r\n}\r\nstatic inline void reset_spu_lslr(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->spu_lslr_RW, LS_ADDR_MASK);\r\neieio();\r\n}\r\nstatic inline void save_spu_cfg(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\ncsa->priv2.spu_cfg_RW = in_be64(&priv2->spu_cfg_RW);\r\n}\r\nstatic inline void save_pm_trace(struct spu_state *csa, struct spu *spu)\r\n{\r\n}\r\nstatic inline void save_mfc_rag(struct spu_state *csa, struct spu *spu)\r\n{\r\ncsa->priv1.resource_allocation_groupID_RW =\r\nspu_resource_allocation_groupID_get(spu);\r\ncsa->priv1.resource_allocation_enable_RW =\r\nspu_resource_allocation_enable_get(spu);\r\n}\r\nstatic inline void save_ppu_mb_stat(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\ncsa->prob.mb_stat_R = in_be32(&prob->mb_stat_R);\r\n}\r\nstatic inline void save_ppu_mb(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\ncsa->prob.pu_mb_R = in_be32(&prob->pu_mb_R);\r\n}\r\nstatic inline void save_ppuint_mb(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\ncsa->priv2.puint_mb_R = in_be64(&priv2->puint_mb_R);\r\n}\r\nstatic inline void save_ch_part1(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nu64 idx, ch_indices[] = { 0UL, 3UL, 4UL, 24UL, 25UL, 27UL };\r\nint i;\r\nout_be64(&priv2->spu_chnlcntptr_RW, 1);\r\ncsa->spu_chnldata_RW[1] = in_be64(&priv2->spu_chnldata_RW);\r\nfor (i = 0; i < ARRAY_SIZE(ch_indices); i++) {\r\nidx = ch_indices[i];\r\nout_be64(&priv2->spu_chnlcntptr_RW, idx);\r\neieio();\r\ncsa->spu_chnldata_RW[idx] = in_be64(&priv2->spu_chnldata_RW);\r\ncsa->spu_chnlcnt_RW[idx] = in_be64(&priv2->spu_chnlcnt_RW);\r\nout_be64(&priv2->spu_chnldata_RW, 0UL);\r\nout_be64(&priv2->spu_chnlcnt_RW, 0UL);\r\neieio();\r\n}\r\n}\r\nstatic inline void save_spu_mb(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nint i;\r\nout_be64(&priv2->spu_chnlcntptr_RW, 29UL);\r\neieio();\r\ncsa->spu_chnlcnt_RW[29] = in_be64(&priv2->spu_chnlcnt_RW);\r\nfor (i = 0; i < 4; i++) {\r\ncsa->spu_mailbox_data[i] = in_be64(&priv2->spu_chnldata_RW);\r\n}\r\nout_be64(&priv2->spu_chnlcnt_RW, 0UL);\r\neieio();\r\n}\r\nstatic inline void save_mfc_cmd(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->spu_chnlcntptr_RW, 21UL);\r\neieio();\r\ncsa->spu_chnlcnt_RW[21] = in_be64(&priv2->spu_chnlcnt_RW);\r\neieio();\r\n}\r\nstatic inline void reset_ch(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nu64 ch_indices[4] = { 21UL, 23UL, 28UL, 30UL };\r\nu64 ch_counts[4] = { 16UL, 1UL, 1UL, 1UL };\r\nu64 idx;\r\nint i;\r\nfor (i = 0; i < 4; i++) {\r\nidx = ch_indices[i];\r\nout_be64(&priv2->spu_chnlcntptr_RW, idx);\r\neieio();\r\nout_be64(&priv2->spu_chnlcnt_RW, ch_counts[i]);\r\neieio();\r\n}\r\n}\r\nstatic inline void resume_mfc_queue(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->mfc_control_RW, MFC_CNTL_RESUME_DMA_QUEUE);\r\n}\r\nstatic inline void setup_mfc_slbs(struct spu_state *csa, struct spu *spu,\r\nunsigned int *code, int code_size)\r\n{\r\nspu_invalidate_slbs(spu);\r\nspu_setup_kernel_slbs(spu, csa->lscsa, code, code_size);\r\n}\r\nstatic inline void set_switch_active(struct spu_state *csa, struct spu *spu)\r\n{\r\nif (test_bit(SPU_CONTEXT_FAULT_PENDING, &spu->flags))\r\ncsa->priv2.mfc_control_RW |= MFC_CNTL_RESTART_DMA_COMMAND;\r\nclear_bit(SPU_CONTEXT_SWITCH_PENDING, &spu->flags);\r\nmb();\r\n}\r\nstatic inline void enable_interrupts(struct spu_state *csa, struct spu *spu)\r\n{\r\nunsigned long class1_mask = CLASS1_ENABLE_SEGMENT_FAULT_INTR |\r\nCLASS1_ENABLE_STORAGE_FAULT_INTR;\r\nspin_lock_irq(&spu->register_lock);\r\nspu_int_stat_clear(spu, 0, CLASS0_INTR_MASK);\r\nspu_int_stat_clear(spu, 1, CLASS1_INTR_MASK);\r\nspu_int_stat_clear(spu, 2, CLASS2_INTR_MASK);\r\nspu_int_mask_set(spu, 0, 0ul);\r\nspu_int_mask_set(spu, 1, class1_mask);\r\nspu_int_mask_set(spu, 2, 0ul);\r\nspin_unlock_irq(&spu->register_lock);\r\n}\r\nstatic inline int send_mfc_dma(struct spu *spu, unsigned long ea,\r\nunsigned int ls_offset, unsigned int size,\r\nunsigned int tag, unsigned int rclass,\r\nunsigned int cmd)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nunion mfc_tag_size_class_cmd command;\r\nunsigned int transfer_size;\r\nvolatile unsigned int status = 0x0;\r\nwhile (size > 0) {\r\ntransfer_size =\r\n(size > MFC_MAX_DMA_SIZE) ? MFC_MAX_DMA_SIZE : size;\r\ncommand.u.mfc_size = transfer_size;\r\ncommand.u.mfc_tag = tag;\r\ncommand.u.mfc_rclassid = rclass;\r\ncommand.u.mfc_cmd = cmd;\r\ndo {\r\nout_be32(&prob->mfc_lsa_W, ls_offset);\r\nout_be64(&prob->mfc_ea_W, ea);\r\nout_be64(&prob->mfc_union_W.all64, command.all64);\r\nstatus =\r\nin_be32(&prob->mfc_union_W.by32.mfc_class_cmd32);\r\nif (unlikely(status & 0x2)) {\r\ncpu_relax();\r\n}\r\n} while (status & 0x3);\r\nsize -= transfer_size;\r\nea += transfer_size;\r\nls_offset += transfer_size;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void save_ls_16kb(struct spu_state *csa, struct spu *spu)\r\n{\r\nunsigned long addr = (unsigned long)&csa->lscsa->ls[0];\r\nunsigned int ls_offset = 0x0;\r\nunsigned int size = 16384;\r\nunsigned int tag = 0;\r\nunsigned int rclass = 0;\r\nunsigned int cmd = MFC_PUT_CMD;\r\nsend_mfc_dma(spu, addr, ls_offset, size, tag, rclass, cmd);\r\n}\r\nstatic inline void set_spu_npc(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nout_be32(&prob->spu_npc_RW, 0);\r\neieio();\r\n}\r\nstatic inline void set_signot1(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nunion {\r\nu64 ull;\r\nu32 ui[2];\r\n} addr64;\r\naddr64.ull = (u64) csa->lscsa;\r\nout_be32(&prob->signal_notify1, addr64.ui[0]);\r\neieio();\r\n}\r\nstatic inline void set_signot2(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nunion {\r\nu64 ull;\r\nu32 ui[2];\r\n} addr64;\r\naddr64.ull = (u64) csa->lscsa;\r\nout_be32(&prob->signal_notify2, addr64.ui[1]);\r\neieio();\r\n}\r\nstatic inline void send_save_code(struct spu_state *csa, struct spu *spu)\r\n{\r\nunsigned long addr = (unsigned long)&spu_save_code[0];\r\nunsigned int ls_offset = 0x0;\r\nunsigned int size = sizeof(spu_save_code);\r\nunsigned int tag = 0;\r\nunsigned int rclass = 0;\r\nunsigned int cmd = MFC_GETFS_CMD;\r\nsend_mfc_dma(spu, addr, ls_offset, size, tag, rclass, cmd);\r\n}\r\nstatic inline void set_ppu_querymask(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nout_be32(&prob->dma_querymask_RW, MFC_TAGID_TO_TAGMASK(0));\r\neieio();\r\n}\r\nstatic inline void wait_tag_complete(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nu32 mask = MFC_TAGID_TO_TAGMASK(0);\r\nunsigned long flags;\r\nPOLL_WHILE_FALSE(in_be32(&prob->dma_tagstatus_R) & mask);\r\nlocal_irq_save(flags);\r\nspu_int_stat_clear(spu, 0, CLASS0_INTR_MASK);\r\nspu_int_stat_clear(spu, 2, CLASS2_INTR_MASK);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic inline void wait_spu_stopped(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nunsigned long flags;\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) & SPU_STATUS_RUNNING);\r\nlocal_irq_save(flags);\r\nspu_int_stat_clear(spu, 0, CLASS0_INTR_MASK);\r\nspu_int_stat_clear(spu, 2, CLASS2_INTR_MASK);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic inline int check_save_status(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nu32 complete;\r\ncomplete = ((SPU_SAVE_COMPLETE << SPU_STOP_STATUS_SHIFT) |\r\nSPU_STATUS_STOPPED_BY_STOP);\r\nreturn (in_be32(&prob->spu_status_R) != complete) ? 1 : 0;\r\n}\r\nstatic inline void terminate_spu_app(struct spu_state *csa, struct spu *spu)\r\n{\r\n}\r\nstatic inline void suspend_mfc_and_halt_decr(struct spu_state *csa,\r\nstruct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->mfc_control_RW, MFC_CNTL_SUSPEND_DMA_QUEUE |\r\nMFC_CNTL_DECREMENTER_HALTED);\r\neieio();\r\n}\r\nstatic inline void wait_suspend_mfc_complete(struct spu_state *csa,\r\nstruct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nPOLL_WHILE_FALSE((in_be64(&priv2->mfc_control_RW) &\r\nMFC_CNTL_SUSPEND_DMA_STATUS_MASK) ==\r\nMFC_CNTL_SUSPEND_COMPLETE);\r\n}\r\nstatic inline int suspend_spe(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nif (in_be32(&prob->spu_status_R) & SPU_STATUS_RUNNING) {\r\nif (in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_ISOLATED_EXIT_STATUS) {\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\n}\r\nif ((in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_ISOLATED_LOAD_STATUS)\r\n|| (in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_ISOLATED_STATE)) {\r\nout_be32(&prob->spu_runcntl_RW, SPU_RUNCNTL_STOP);\r\neieio();\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\nout_be32(&prob->spu_runcntl_RW, 0x2);\r\neieio();\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\n}\r\nif (in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_WAITING_FOR_CHANNEL) {\r\nout_be32(&prob->spu_runcntl_RW, SPU_RUNCNTL_STOP);\r\neieio();\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\n}\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void clear_spu_status(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nif (!(in_be32(&prob->spu_status_R) & SPU_STATUS_RUNNING)) {\r\nif (in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_ISOLATED_EXIT_STATUS) {\r\nspu_mfc_sr1_set(spu,\r\nMFC_STATE1_MASTER_RUN_CONTROL_MASK);\r\neieio();\r\nout_be32(&prob->spu_runcntl_RW, SPU_RUNCNTL_RUNNABLE);\r\neieio();\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\n}\r\nif ((in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_ISOLATED_LOAD_STATUS)\r\n|| (in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_ISOLATED_STATE)) {\r\nspu_mfc_sr1_set(spu,\r\nMFC_STATE1_MASTER_RUN_CONTROL_MASK);\r\neieio();\r\nout_be32(&prob->spu_runcntl_RW, 0x2);\r\neieio();\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\n}\r\n}\r\n}\r\nstatic inline void reset_ch_part1(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nu64 ch_indices[] = { 0UL, 3UL, 4UL, 24UL, 25UL, 27UL };\r\nu64 idx;\r\nint i;\r\nout_be64(&priv2->spu_chnlcntptr_RW, 1);\r\nout_be64(&priv2->spu_chnldata_RW, 0UL);\r\nfor (i = 0; i < ARRAY_SIZE(ch_indices); i++) {\r\nidx = ch_indices[i];\r\nout_be64(&priv2->spu_chnlcntptr_RW, idx);\r\neieio();\r\nout_be64(&priv2->spu_chnldata_RW, 0UL);\r\nout_be64(&priv2->spu_chnlcnt_RW, 0UL);\r\neieio();\r\n}\r\n}\r\nstatic inline void reset_ch_part2(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nu64 ch_indices[5] = { 21UL, 23UL, 28UL, 29UL, 30UL };\r\nu64 ch_counts[5] = { 16UL, 1UL, 1UL, 0UL, 1UL };\r\nu64 idx;\r\nint i;\r\nfor (i = 0; i < 5; i++) {\r\nidx = ch_indices[i];\r\nout_be64(&priv2->spu_chnlcntptr_RW, idx);\r\neieio();\r\nout_be64(&priv2->spu_chnlcnt_RW, ch_counts[i]);\r\neieio();\r\n}\r\n}\r\nstatic inline void setup_spu_status_part1(struct spu_state *csa,\r\nstruct spu *spu)\r\n{\r\nu32 status_P = SPU_STATUS_STOPPED_BY_STOP;\r\nu32 status_I = SPU_STATUS_INVALID_INSTR;\r\nu32 status_H = SPU_STATUS_STOPPED_BY_HALT;\r\nu32 status_S = SPU_STATUS_SINGLE_STEP;\r\nu32 status_S_I = SPU_STATUS_SINGLE_STEP | SPU_STATUS_INVALID_INSTR;\r\nu32 status_S_P = SPU_STATUS_SINGLE_STEP | SPU_STATUS_STOPPED_BY_STOP;\r\nu32 status_P_H = SPU_STATUS_STOPPED_BY_HALT |SPU_STATUS_STOPPED_BY_STOP;\r\nu32 status_P_I = SPU_STATUS_STOPPED_BY_STOP |SPU_STATUS_INVALID_INSTR;\r\nu32 status_code;\r\nstatus_code =\r\n(csa->prob.spu_status_R >> SPU_STOP_STATUS_SHIFT) & 0xFFFF;\r\nif ((csa->prob.spu_status_R & status_P_I) == status_P_I) {\r\ncsa->lscsa->stopped_status.slot[0] = SPU_STOPPED_STATUS_P_I;\r\ncsa->lscsa->stopped_status.slot[1] = status_code;\r\n} else if ((csa->prob.spu_status_R & status_P_H) == status_P_H) {\r\ncsa->lscsa->stopped_status.slot[0] = SPU_STOPPED_STATUS_P_H;\r\ncsa->lscsa->stopped_status.slot[1] = status_code;\r\n} else if ((csa->prob.spu_status_R & status_S_P) == status_S_P) {\r\ncsa->lscsa->stopped_status.slot[0] = SPU_STOPPED_STATUS_S_P;\r\ncsa->lscsa->stopped_status.slot[1] = status_code;\r\n} else if ((csa->prob.spu_status_R & status_S_I) == status_S_I) {\r\ncsa->lscsa->stopped_status.slot[0] = SPU_STOPPED_STATUS_S_I;\r\ncsa->lscsa->stopped_status.slot[1] = status_code;\r\n} else if ((csa->prob.spu_status_R & status_P) == status_P) {\r\ncsa->lscsa->stopped_status.slot[0] = SPU_STOPPED_STATUS_P;\r\ncsa->lscsa->stopped_status.slot[1] = status_code;\r\n} else if ((csa->prob.spu_status_R & status_H) == status_H) {\r\ncsa->lscsa->stopped_status.slot[0] = SPU_STOPPED_STATUS_H;\r\n} else if ((csa->prob.spu_status_R & status_S) == status_S) {\r\ncsa->lscsa->stopped_status.slot[0] = SPU_STOPPED_STATUS_S;\r\n} else if ((csa->prob.spu_status_R & status_I) == status_I) {\r\ncsa->lscsa->stopped_status.slot[0] = SPU_STOPPED_STATUS_I;\r\n}\r\n}\r\nstatic inline void setup_spu_status_part2(struct spu_state *csa,\r\nstruct spu *spu)\r\n{\r\nu32 mask;\r\nmask = SPU_STATUS_INVALID_INSTR |\r\nSPU_STATUS_SINGLE_STEP |\r\nSPU_STATUS_STOPPED_BY_HALT |\r\nSPU_STATUS_STOPPED_BY_STOP | SPU_STATUS_RUNNING;\r\nif (!(csa->prob.spu_status_R & mask)) {\r\ncsa->lscsa->stopped_status.slot[0] = SPU_STOPPED_STATUS_R;\r\n}\r\n}\r\nstatic inline void restore_mfc_rag(struct spu_state *csa, struct spu *spu)\r\n{\r\nspu_resource_allocation_groupID_set(spu,\r\ncsa->priv1.resource_allocation_groupID_RW);\r\nspu_resource_allocation_enable_set(spu,\r\ncsa->priv1.resource_allocation_enable_RW);\r\n}\r\nstatic inline void send_restore_code(struct spu_state *csa, struct spu *spu)\r\n{\r\nunsigned long addr = (unsigned long)&spu_restore_code[0];\r\nunsigned int ls_offset = 0x0;\r\nunsigned int size = sizeof(spu_restore_code);\r\nunsigned int tag = 0;\r\nunsigned int rclass = 0;\r\nunsigned int cmd = MFC_GETFS_CMD;\r\nsend_mfc_dma(spu, addr, ls_offset, size, tag, rclass, cmd);\r\n}\r\nstatic inline void setup_decr(struct spu_state *csa, struct spu *spu)\r\n{\r\nif (csa->priv2.mfc_control_RW & MFC_CNTL_DECREMENTER_RUNNING) {\r\ncycles_t resume_time = get_cycles();\r\ncycles_t delta_time = resume_time - csa->suspend_time;\r\ncsa->lscsa->decr_status.slot[0] = SPU_DECR_STATUS_RUNNING;\r\nif (csa->lscsa->decr.slot[0] < delta_time) {\r\ncsa->lscsa->decr_status.slot[0] |=\r\nSPU_DECR_STATUS_WRAPPED;\r\n}\r\ncsa->lscsa->decr.slot[0] -= delta_time;\r\n} else {\r\ncsa->lscsa->decr_status.slot[0] = 0;\r\n}\r\n}\r\nstatic inline void setup_ppu_mb(struct spu_state *csa, struct spu *spu)\r\n{\r\ncsa->lscsa->ppu_mb.slot[0] = csa->prob.pu_mb_R;\r\n}\r\nstatic inline void setup_ppuint_mb(struct spu_state *csa, struct spu *spu)\r\n{\r\ncsa->lscsa->ppuint_mb.slot[0] = csa->priv2.puint_mb_R;\r\n}\r\nstatic inline int check_restore_status(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nu32 complete;\r\ncomplete = ((SPU_RESTORE_COMPLETE << SPU_STOP_STATUS_SHIFT) |\r\nSPU_STATUS_STOPPED_BY_STOP);\r\nreturn (in_be32(&prob->spu_status_R) != complete) ? 1 : 0;\r\n}\r\nstatic inline void restore_spu_privcntl(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->spu_privcntl_RW, csa->priv2.spu_privcntl_RW);\r\neieio();\r\n}\r\nstatic inline void restore_status_part1(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nu32 mask;\r\nmask = SPU_STATUS_INVALID_INSTR |\r\nSPU_STATUS_SINGLE_STEP |\r\nSPU_STATUS_STOPPED_BY_HALT | SPU_STATUS_STOPPED_BY_STOP;\r\nif (csa->prob.spu_status_R & mask) {\r\nout_be32(&prob->spu_runcntl_RW, SPU_RUNCNTL_RUNNABLE);\r\neieio();\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\n}\r\n}\r\nstatic inline void restore_status_part2(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nu32 mask;\r\nmask = SPU_STATUS_INVALID_INSTR |\r\nSPU_STATUS_SINGLE_STEP |\r\nSPU_STATUS_STOPPED_BY_HALT |\r\nSPU_STATUS_STOPPED_BY_STOP | SPU_STATUS_RUNNING;\r\nif (!(csa->prob.spu_status_R & mask)) {\r\nout_be32(&prob->spu_runcntl_RW, SPU_RUNCNTL_RUNNABLE);\r\neieio();\r\nPOLL_WHILE_FALSE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\nout_be32(&prob->spu_runcntl_RW, SPU_RUNCNTL_STOP);\r\neieio();\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) &\r\nSPU_STATUS_RUNNING);\r\n}\r\n}\r\nstatic inline void restore_ls_16kb(struct spu_state *csa, struct spu *spu)\r\n{\r\nunsigned long addr = (unsigned long)&csa->lscsa->ls[0];\r\nunsigned int ls_offset = 0x0;\r\nunsigned int size = 16384;\r\nunsigned int tag = 0;\r\nunsigned int rclass = 0;\r\nunsigned int cmd = MFC_GET_CMD;\r\nsend_mfc_dma(spu, addr, ls_offset, size, tag, rclass, cmd);\r\n}\r\nstatic inline void suspend_mfc(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->mfc_control_RW, MFC_CNTL_SUSPEND_DMA_QUEUE);\r\neieio();\r\n}\r\nstatic inline void clear_interrupts(struct spu_state *csa, struct spu *spu)\r\n{\r\nspin_lock_irq(&spu->register_lock);\r\nspu_int_mask_set(spu, 0, 0ul);\r\nspu_int_mask_set(spu, 1, 0ul);\r\nspu_int_mask_set(spu, 2, 0ul);\r\nspu_int_stat_clear(spu, 0, CLASS0_INTR_MASK);\r\nspu_int_stat_clear(spu, 1, CLASS1_INTR_MASK);\r\nspu_int_stat_clear(spu, 2, CLASS2_INTR_MASK);\r\nspin_unlock_irq(&spu->register_lock);\r\n}\r\nstatic inline void restore_mfc_queues(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nint i;\r\nif ((csa->priv2.mfc_control_RW & MFC_CNTL_DMA_QUEUES_EMPTY_MASK) == 0) {\r\nfor (i = 0; i < 8; i++) {\r\nout_be64(&priv2->puq[i].mfc_cq_data0_RW,\r\ncsa->priv2.puq[i].mfc_cq_data0_RW);\r\nout_be64(&priv2->puq[i].mfc_cq_data1_RW,\r\ncsa->priv2.puq[i].mfc_cq_data1_RW);\r\nout_be64(&priv2->puq[i].mfc_cq_data2_RW,\r\ncsa->priv2.puq[i].mfc_cq_data2_RW);\r\nout_be64(&priv2->puq[i].mfc_cq_data3_RW,\r\ncsa->priv2.puq[i].mfc_cq_data3_RW);\r\n}\r\nfor (i = 0; i < 16; i++) {\r\nout_be64(&priv2->spuq[i].mfc_cq_data0_RW,\r\ncsa->priv2.spuq[i].mfc_cq_data0_RW);\r\nout_be64(&priv2->spuq[i].mfc_cq_data1_RW,\r\ncsa->priv2.spuq[i].mfc_cq_data1_RW);\r\nout_be64(&priv2->spuq[i].mfc_cq_data2_RW,\r\ncsa->priv2.spuq[i].mfc_cq_data2_RW);\r\nout_be64(&priv2->spuq[i].mfc_cq_data3_RW,\r\ncsa->priv2.spuq[i].mfc_cq_data3_RW);\r\n}\r\n}\r\neieio();\r\n}\r\nstatic inline void restore_ppu_querymask(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nout_be32(&prob->dma_querymask_RW, csa->prob.dma_querymask_RW);\r\neieio();\r\n}\r\nstatic inline void restore_ppu_querytype(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nout_be32(&prob->dma_querytype_RW, csa->prob.dma_querytype_RW);\r\neieio();\r\n}\r\nstatic inline void restore_mfc_csr_tsq(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->spu_tag_status_query_RW,\r\ncsa->priv2.spu_tag_status_query_RW);\r\neieio();\r\n}\r\nstatic inline void restore_mfc_csr_cmd(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->spu_cmd_buf1_RW, csa->priv2.spu_cmd_buf1_RW);\r\nout_be64(&priv2->spu_cmd_buf2_RW, csa->priv2.spu_cmd_buf2_RW);\r\neieio();\r\n}\r\nstatic inline void restore_mfc_csr_ato(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->spu_atomic_status_RW, csa->priv2.spu_atomic_status_RW);\r\n}\r\nstatic inline void restore_mfc_tclass_id(struct spu_state *csa, struct spu *spu)\r\n{\r\nspu_mfc_tclass_id_set(spu, csa->priv1.mfc_tclass_id_RW);\r\neieio();\r\n}\r\nstatic inline void set_llr_event(struct spu_state *csa, struct spu *spu)\r\n{\r\nu64 ch0_cnt, ch0_data;\r\nu64 ch1_data;\r\nch0_cnt = csa->spu_chnlcnt_RW[0];\r\nch0_data = csa->spu_chnldata_RW[0];\r\nch1_data = csa->spu_chnldata_RW[1];\r\ncsa->spu_chnldata_RW[0] |= MFC_LLR_LOST_EVENT;\r\nif ((ch0_cnt == 0) && !(ch0_data & MFC_LLR_LOST_EVENT) &&\r\n(ch1_data & MFC_LLR_LOST_EVENT)) {\r\ncsa->spu_chnlcnt_RW[0] = 1;\r\n}\r\n}\r\nstatic inline void restore_decr_wrapped(struct spu_state *csa, struct spu *spu)\r\n{\r\nif (!(csa->lscsa->decr_status.slot[0] & SPU_DECR_STATUS_WRAPPED))\r\nreturn;\r\nif ((csa->spu_chnlcnt_RW[0] == 0) &&\r\n(csa->spu_chnldata_RW[1] & 0x20) &&\r\n!(csa->spu_chnldata_RW[0] & 0x20))\r\ncsa->spu_chnlcnt_RW[0] = 1;\r\ncsa->spu_chnldata_RW[0] |= 0x20;\r\n}\r\nstatic inline void restore_ch_part1(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nu64 idx, ch_indices[] = { 0UL, 3UL, 4UL, 24UL, 25UL, 27UL };\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(ch_indices); i++) {\r\nidx = ch_indices[i];\r\nout_be64(&priv2->spu_chnlcntptr_RW, idx);\r\neieio();\r\nout_be64(&priv2->spu_chnldata_RW, csa->spu_chnldata_RW[idx]);\r\nout_be64(&priv2->spu_chnlcnt_RW, csa->spu_chnlcnt_RW[idx]);\r\neieio();\r\n}\r\n}\r\nstatic inline void restore_ch_part2(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nu64 ch_indices[3] = { 9UL, 21UL, 23UL };\r\nu64 ch_counts[3] = { 1UL, 16UL, 1UL };\r\nu64 idx;\r\nint i;\r\nch_counts[0] = 1UL;\r\nch_counts[1] = csa->spu_chnlcnt_RW[21];\r\nch_counts[2] = 1UL;\r\nfor (i = 0; i < 3; i++) {\r\nidx = ch_indices[i];\r\nout_be64(&priv2->spu_chnlcntptr_RW, idx);\r\neieio();\r\nout_be64(&priv2->spu_chnlcnt_RW, ch_counts[i]);\r\neieio();\r\n}\r\n}\r\nstatic inline void restore_spu_lslr(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->spu_lslr_RW, csa->priv2.spu_lslr_RW);\r\neieio();\r\n}\r\nstatic inline void restore_spu_cfg(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->spu_cfg_RW, csa->priv2.spu_cfg_RW);\r\neieio();\r\n}\r\nstatic inline void restore_pm_trace(struct spu_state *csa, struct spu *spu)\r\n{\r\n}\r\nstatic inline void restore_spu_npc(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nout_be32(&prob->spu_npc_RW, csa->prob.spu_npc_RW);\r\neieio();\r\n}\r\nstatic inline void restore_spu_mb(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nint i;\r\nout_be64(&priv2->spu_chnlcntptr_RW, 29UL);\r\neieio();\r\nout_be64(&priv2->spu_chnlcnt_RW, csa->spu_chnlcnt_RW[29]);\r\nfor (i = 0; i < 4; i++) {\r\nout_be64(&priv2->spu_chnldata_RW, csa->spu_mailbox_data[i]);\r\n}\r\neieio();\r\n}\r\nstatic inline void check_ppu_mb_stat(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nu32 dummy = 0;\r\nif ((csa->prob.mb_stat_R & 0xFF) == 0) {\r\ndummy = in_be32(&prob->pu_mb_R);\r\neieio();\r\n}\r\n}\r\nstatic inline void check_ppuint_mb_stat(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nu64 dummy = 0UL;\r\nif ((csa->prob.mb_stat_R & 0xFF0000) == 0) {\r\ndummy = in_be64(&priv2->puint_mb_R);\r\neieio();\r\nspu_int_stat_clear(spu, 2, CLASS2_ENABLE_MAILBOX_INTR);\r\neieio();\r\n}\r\n}\r\nstatic inline void restore_mfc_sr1(struct spu_state *csa, struct spu *spu)\r\n{\r\nspu_mfc_sr1_set(spu, csa->priv1.mfc_sr1_RW);\r\neieio();\r\n}\r\nstatic inline void set_int_route(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_context *ctx = spu->ctx;\r\nspu_cpu_affinity_set(spu, ctx->last_ran);\r\n}\r\nstatic inline void restore_other_spu_access(struct spu_state *csa,\r\nstruct spu *spu)\r\n{\r\n}\r\nstatic inline void restore_spu_runcntl(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nif (csa->prob.spu_status_R & SPU_STATUS_RUNNING) {\r\nout_be32(&prob->spu_runcntl_RW, SPU_RUNCNTL_RUNNABLE);\r\neieio();\r\n}\r\n}\r\nstatic inline void restore_mfc_cntl(struct spu_state *csa, struct spu *spu)\r\n{\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be64(&priv2->mfc_control_RW, csa->priv2.mfc_control_RW);\r\neieio();\r\n}\r\nstatic inline void enable_user_access(struct spu_state *csa, struct spu *spu)\r\n{\r\n}\r\nstatic inline void reset_switch_active(struct spu_state *csa, struct spu *spu)\r\n{\r\n}\r\nstatic inline void reenable_interrupts(struct spu_state *csa, struct spu *spu)\r\n{\r\nspin_lock_irq(&spu->register_lock);\r\nspu_int_mask_set(spu, 0, csa->priv1.int_mask_class0_RW);\r\nspu_int_mask_set(spu, 1, csa->priv1.int_mask_class1_RW);\r\nspu_int_mask_set(spu, 2, csa->priv1.int_mask_class2_RW);\r\nspin_unlock_irq(&spu->register_lock);\r\n}\r\nstatic int quiece_spu(struct spu_state *prev, struct spu *spu)\r\n{\r\nif (check_spu_isolate(prev, spu)) {\r\nreturn 2;\r\n}\r\ndisable_interrupts(prev, spu);\r\nset_watchdog_timer(prev, spu);\r\ninhibit_user_access(prev, spu);\r\nif (check_spu_isolate(prev, spu)) {\r\nreturn 6;\r\n}\r\nset_switch_pending(prev, spu);\r\nsave_mfc_cntl(prev, spu);\r\nsave_spu_runcntl(prev, spu);\r\nsave_mfc_sr1(prev, spu);\r\nsave_spu_status(prev, spu);\r\nsave_mfc_stopped_status(prev, spu);\r\nhalt_mfc_decr(prev, spu);\r\nsave_timebase(prev, spu);\r\nremove_other_spu_access(prev, spu);\r\ndo_mfc_mssync(prev, spu);\r\nissue_mfc_tlbie(prev, spu);\r\nhandle_pending_interrupts(prev, spu);\r\nreturn 0;\r\n}\r\nstatic void save_csa(struct spu_state *prev, struct spu *spu)\r\n{\r\nsave_mfc_queues(prev, spu);\r\nsave_ppu_querymask(prev, spu);\r\nsave_ppu_querytype(prev, spu);\r\nsave_ppu_tagstatus(prev, spu);\r\nsave_mfc_csr_tsq(prev, spu);\r\nsave_mfc_csr_cmd(prev, spu);\r\nsave_mfc_csr_ato(prev, spu);\r\nsave_mfc_tclass_id(prev, spu);\r\nset_mfc_tclass_id(prev, spu);\r\nsave_mfc_cmd(prev, spu);\r\npurge_mfc_queue(prev, spu);\r\nwait_purge_complete(prev, spu);\r\nsetup_mfc_sr1(prev, spu);\r\nsave_spu_npc(prev, spu);\r\nsave_spu_privcntl(prev, spu);\r\nreset_spu_privcntl(prev, spu);\r\nsave_spu_lslr(prev, spu);\r\nreset_spu_lslr(prev, spu);\r\nsave_spu_cfg(prev, spu);\r\nsave_pm_trace(prev, spu);\r\nsave_mfc_rag(prev, spu);\r\nsave_ppu_mb_stat(prev, spu);\r\nsave_ppu_mb(prev, spu);\r\nsave_ppuint_mb(prev, spu);\r\nsave_ch_part1(prev, spu);\r\nsave_spu_mb(prev, spu);\r\nreset_ch(prev, spu);\r\n}\r\nstatic void save_lscsa(struct spu_state *prev, struct spu *spu)\r\n{\r\nresume_mfc_queue(prev, spu);\r\nsetup_mfc_slbs(prev, spu, spu_save_code, sizeof(spu_save_code));\r\nset_switch_active(prev, spu);\r\nenable_interrupts(prev, spu);\r\nsave_ls_16kb(prev, spu);\r\nset_spu_npc(prev, spu);\r\nset_signot1(prev, spu);\r\nset_signot2(prev, spu);\r\nsend_save_code(prev, spu);\r\nset_ppu_querymask(prev, spu);\r\nwait_tag_complete(prev, spu);\r\nwait_spu_stopped(prev, spu);\r\n}\r\nstatic void force_spu_isolate_exit(struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nstruct spu_priv2 __iomem *priv2 = spu->priv2;\r\nout_be32(&prob->spu_runcntl_RW, SPU_RUNCNTL_STOP);\r\niobarrier_rw();\r\nPOLL_WHILE_TRUE(in_be32(&prob->spu_status_R) & SPU_STATUS_RUNNING);\r\nspu_mfc_sr1_set(spu, MFC_STATE1_MASTER_RUN_CONTROL_MASK);\r\niobarrier_w();\r\nout_be64(&priv2->spu_privcntl_RW, 4LL);\r\niobarrier_w();\r\nout_be32(&prob->spu_runcntl_RW, 2);\r\niobarrier_rw();\r\nPOLL_WHILE_FALSE((in_be32(&prob->spu_status_R)\r\n& SPU_STATUS_STOPPED_BY_STOP));\r\nout_be64(&priv2->spu_privcntl_RW, SPU_PRIVCNT_LOAD_REQUEST_NORMAL);\r\niobarrier_w();\r\n}\r\nstatic void stop_spu_isolate(struct spu *spu)\r\n{\r\nstruct spu_problem __iomem *prob = spu->problem;\r\nif (in_be32(&prob->spu_status_R) & SPU_STATUS_ISOLATED_STATE) {\r\nforce_spu_isolate_exit(spu);\r\n}\r\n}\r\nstatic void harvest(struct spu_state *prev, struct spu *spu)\r\n{\r\ndisable_interrupts(prev, spu);\r\ninhibit_user_access(prev, spu);\r\nterminate_spu_app(prev, spu);\r\nset_switch_pending(prev, spu);\r\nstop_spu_isolate(spu);\r\nremove_other_spu_access(prev, spu);\r\nsuspend_mfc_and_halt_decr(prev, spu);\r\nwait_suspend_mfc_complete(prev, spu);\r\nif (!suspend_spe(prev, spu))\r\nclear_spu_status(prev, spu);\r\ndo_mfc_mssync(prev, spu);\r\nissue_mfc_tlbie(prev, spu);\r\nhandle_pending_interrupts(prev, spu);\r\npurge_mfc_queue(prev, spu);\r\nwait_purge_complete(prev, spu);\r\nreset_spu_privcntl(prev, spu);\r\nreset_spu_lslr(prev, spu);\r\nsetup_mfc_sr1(prev, spu);\r\nspu_invalidate_slbs(spu);\r\nreset_ch_part1(prev, spu);\r\nreset_ch_part2(prev, spu);\r\nenable_interrupts(prev, spu);\r\nset_switch_active(prev, spu);\r\nset_mfc_tclass_id(prev, spu);\r\nresume_mfc_queue(prev, spu);\r\n}\r\nstatic void restore_lscsa(struct spu_state *next, struct spu *spu)\r\n{\r\nset_watchdog_timer(next, spu);\r\nsetup_spu_status_part1(next, spu);\r\nsetup_spu_status_part2(next, spu);\r\nrestore_mfc_rag(next, spu);\r\nsetup_mfc_slbs(next, spu, spu_restore_code, sizeof(spu_restore_code));\r\nset_spu_npc(next, spu);\r\nset_signot1(next, spu);\r\nset_signot2(next, spu);\r\nsetup_decr(next, spu);\r\nsetup_ppu_mb(next, spu);\r\nsetup_ppuint_mb(next, spu);\r\nsend_restore_code(next, spu);\r\nset_ppu_querymask(next, spu);\r\nwait_tag_complete(next, spu);\r\nwait_spu_stopped(next, spu);\r\n}\r\nstatic void restore_csa(struct spu_state *next, struct spu *spu)\r\n{\r\nrestore_spu_privcntl(next, spu);\r\nrestore_status_part1(next, spu);\r\nrestore_status_part2(next, spu);\r\nrestore_ls_16kb(next, spu);\r\nwait_tag_complete(next, spu);\r\nsuspend_mfc(next, spu);\r\nwait_suspend_mfc_complete(next, spu);\r\nissue_mfc_tlbie(next, spu);\r\nclear_interrupts(next, spu);\r\nrestore_mfc_queues(next, spu);\r\nrestore_ppu_querymask(next, spu);\r\nrestore_ppu_querytype(next, spu);\r\nrestore_mfc_csr_tsq(next, spu);\r\nrestore_mfc_csr_cmd(next, spu);\r\nrestore_mfc_csr_ato(next, spu);\r\nrestore_mfc_tclass_id(next, spu);\r\nset_llr_event(next, spu);\r\nrestore_decr_wrapped(next, spu);\r\nrestore_ch_part1(next, spu);\r\nrestore_ch_part2(next, spu);\r\nrestore_spu_lslr(next, spu);\r\nrestore_spu_cfg(next, spu);\r\nrestore_pm_trace(next, spu);\r\nrestore_spu_npc(next, spu);\r\nrestore_spu_mb(next, spu);\r\ncheck_ppu_mb_stat(next, spu);\r\ncheck_ppuint_mb_stat(next, spu);\r\nspu_invalidate_slbs(spu);\r\nrestore_mfc_sr1(next, spu);\r\nset_int_route(next, spu);\r\nrestore_other_spu_access(next, spu);\r\nrestore_spu_runcntl(next, spu);\r\nrestore_mfc_cntl(next, spu);\r\nenable_user_access(next, spu);\r\nreset_switch_active(next, spu);\r\nreenable_interrupts(next, spu);\r\n}\r\nstatic int __do_spu_save(struct spu_state *prev, struct spu *spu)\r\n{\r\nint rc;\r\nrc = quiece_spu(prev, spu);\r\nswitch (rc) {\r\ndefault:\r\ncase 2:\r\ncase 6:\r\nharvest(prev, spu);\r\nreturn rc;\r\nbreak;\r\ncase 0:\r\nbreak;\r\n}\r\nsave_csa(prev, spu);\r\nsave_lscsa(prev, spu);\r\nreturn check_save_status(prev, spu);\r\n}\r\nstatic int __do_spu_restore(struct spu_state *next, struct spu *spu)\r\n{\r\nint rc;\r\nrestore_lscsa(next, spu);\r\nrc = check_restore_status(next, spu);\r\nswitch (rc) {\r\ndefault:\r\nreturn rc;\r\nbreak;\r\ncase 0:\r\nbreak;\r\n}\r\nrestore_csa(next, spu);\r\nreturn 0;\r\n}\r\nint spu_save(struct spu_state *prev, struct spu *spu)\r\n{\r\nint rc;\r\nacquire_spu_lock(spu);\r\nrc = __do_spu_save(prev, spu);\r\nrelease_spu_lock(spu);\r\nif (rc != 0 && rc != 2 && rc != 6) {\r\npanic("%s failed on SPU[%d], rc=%d.\n",\r\n__func__, spu->number, rc);\r\n}\r\nreturn 0;\r\n}\r\nint spu_restore(struct spu_state *new, struct spu *spu)\r\n{\r\nint rc;\r\nacquire_spu_lock(spu);\r\nharvest(NULL, spu);\r\nspu->slb_replace = 0;\r\nrc = __do_spu_restore(new, spu);\r\nrelease_spu_lock(spu);\r\nif (rc) {\r\npanic("%s failed on SPU[%d] rc=%d.\n",\r\n__func__, spu->number, rc);\r\n}\r\nreturn rc;\r\n}\r\nstatic void init_prob(struct spu_state *csa)\r\n{\r\ncsa->spu_chnlcnt_RW[9] = 1;\r\ncsa->spu_chnlcnt_RW[21] = 16;\r\ncsa->spu_chnlcnt_RW[23] = 1;\r\ncsa->spu_chnlcnt_RW[28] = 1;\r\ncsa->spu_chnlcnt_RW[30] = 1;\r\ncsa->prob.spu_runcntl_RW = SPU_RUNCNTL_STOP;\r\ncsa->prob.mb_stat_R = 0x000400;\r\n}\r\nstatic void init_priv1(struct spu_state *csa)\r\n{\r\ncsa->priv1.mfc_sr1_RW = MFC_STATE1_LOCAL_STORAGE_DECODE_MASK |\r\nMFC_STATE1_MASTER_RUN_CONTROL_MASK |\r\nMFC_STATE1_PROBLEM_STATE_MASK |\r\nMFC_STATE1_RELOCATE_MASK | MFC_STATE1_BUS_TLBIE_MASK;\r\ncsa->priv1.int_mask_class0_RW = CLASS0_ENABLE_DMA_ALIGNMENT_INTR |\r\nCLASS0_ENABLE_INVALID_DMA_COMMAND_INTR |\r\nCLASS0_ENABLE_SPU_ERROR_INTR;\r\ncsa->priv1.int_mask_class1_RW = CLASS1_ENABLE_SEGMENT_FAULT_INTR |\r\nCLASS1_ENABLE_STORAGE_FAULT_INTR;\r\ncsa->priv1.int_mask_class2_RW = CLASS2_ENABLE_SPU_STOP_INTR |\r\nCLASS2_ENABLE_SPU_HALT_INTR |\r\nCLASS2_ENABLE_SPU_DMA_TAG_GROUP_COMPLETE_INTR;\r\n}\r\nstatic void init_priv2(struct spu_state *csa)\r\n{\r\ncsa->priv2.spu_lslr_RW = LS_ADDR_MASK;\r\ncsa->priv2.mfc_control_RW = MFC_CNTL_RESUME_DMA_QUEUE |\r\nMFC_CNTL_NORMAL_DMA_QUEUE_OPERATION |\r\nMFC_CNTL_DMA_QUEUES_EMPTY_MASK;\r\n}\r\nint spu_init_csa(struct spu_state *csa)\r\n{\r\nint rc;\r\nif (!csa)\r\nreturn -EINVAL;\r\nmemset(csa, 0, sizeof(struct spu_state));\r\nrc = spu_alloc_lscsa(csa);\r\nif (rc)\r\nreturn rc;\r\nspin_lock_init(&csa->register_lock);\r\ninit_prob(csa);\r\ninit_priv1(csa);\r\ninit_priv2(csa);\r\nreturn 0;\r\n}\r\nvoid spu_fini_csa(struct spu_state *csa)\r\n{\r\nspu_free_lscsa(csa);\r\n}
