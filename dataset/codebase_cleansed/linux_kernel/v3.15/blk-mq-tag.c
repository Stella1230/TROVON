void blk_mq_wait_for_tags(struct blk_mq_tags *tags)\r\n{\r\nint tag = blk_mq_get_tag(tags, __GFP_WAIT, false);\r\nblk_mq_put_tag(tags, tag);\r\n}\r\nbool blk_mq_has_free_tags(struct blk_mq_tags *tags)\r\n{\r\nreturn !tags ||\r\npercpu_ida_free_tags(&tags->free_tags, nr_cpu_ids) != 0;\r\n}\r\nstatic unsigned int __blk_mq_get_tag(struct blk_mq_tags *tags, gfp_t gfp)\r\n{\r\nint tag;\r\ntag = percpu_ida_alloc(&tags->free_tags, (gfp & __GFP_WAIT) ?\r\nTASK_UNINTERRUPTIBLE : TASK_RUNNING);\r\nif (tag < 0)\r\nreturn BLK_MQ_TAG_FAIL;\r\nreturn tag + tags->nr_reserved_tags;\r\n}\r\nstatic unsigned int __blk_mq_get_reserved_tag(struct blk_mq_tags *tags,\r\ngfp_t gfp)\r\n{\r\nint tag;\r\nif (unlikely(!tags->nr_reserved_tags)) {\r\nWARN_ON_ONCE(1);\r\nreturn BLK_MQ_TAG_FAIL;\r\n}\r\ntag = percpu_ida_alloc(&tags->reserved_tags, (gfp & __GFP_WAIT) ?\r\nTASK_UNINTERRUPTIBLE : TASK_RUNNING);\r\nif (tag < 0)\r\nreturn BLK_MQ_TAG_FAIL;\r\nreturn tag;\r\n}\r\nunsigned int blk_mq_get_tag(struct blk_mq_tags *tags, gfp_t gfp, bool reserved)\r\n{\r\nif (!reserved)\r\nreturn __blk_mq_get_tag(tags, gfp);\r\nreturn __blk_mq_get_reserved_tag(tags, gfp);\r\n}\r\nstatic void __blk_mq_put_tag(struct blk_mq_tags *tags, unsigned int tag)\r\n{\r\nBUG_ON(tag >= tags->nr_tags);\r\npercpu_ida_free(&tags->free_tags, tag - tags->nr_reserved_tags);\r\n}\r\nstatic void __blk_mq_put_reserved_tag(struct blk_mq_tags *tags,\r\nunsigned int tag)\r\n{\r\nBUG_ON(tag >= tags->nr_reserved_tags);\r\npercpu_ida_free(&tags->reserved_tags, tag);\r\n}\r\nvoid blk_mq_put_tag(struct blk_mq_tags *tags, unsigned int tag)\r\n{\r\nif (tag >= tags->nr_reserved_tags)\r\n__blk_mq_put_tag(tags, tag);\r\nelse\r\n__blk_mq_put_reserved_tag(tags, tag);\r\n}\r\nstatic int __blk_mq_tag_iter(unsigned id, void *data)\r\n{\r\nunsigned long *tag_map = data;\r\n__set_bit(id, tag_map);\r\nreturn 0;\r\n}\r\nvoid blk_mq_tag_busy_iter(struct blk_mq_tags *tags,\r\nvoid (*fn)(void *, unsigned long *), void *data)\r\n{\r\nunsigned long *tag_map;\r\nsize_t map_size;\r\nmap_size = ALIGN(tags->nr_tags, BITS_PER_LONG) / BITS_PER_LONG;\r\ntag_map = kzalloc(map_size * sizeof(unsigned long), GFP_ATOMIC);\r\nif (!tag_map)\r\nreturn;\r\npercpu_ida_for_each_free(&tags->free_tags, __blk_mq_tag_iter, tag_map);\r\nif (tags->nr_reserved_tags)\r\npercpu_ida_for_each_free(&tags->reserved_tags, __blk_mq_tag_iter,\r\ntag_map);\r\nfn(data, tag_map);\r\nkfree(tag_map);\r\n}\r\nstruct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,\r\nunsigned int reserved_tags, int node)\r\n{\r\nunsigned int nr_tags, nr_cache;\r\nstruct blk_mq_tags *tags;\r\nint ret;\r\nif (total_tags > BLK_MQ_TAG_MAX) {\r\npr_err("blk-mq: tag depth too large\n");\r\nreturn NULL;\r\n}\r\ntags = kzalloc_node(sizeof(*tags), GFP_KERNEL, node);\r\nif (!tags)\r\nreturn NULL;\r\nnr_tags = total_tags - reserved_tags;\r\nnr_cache = nr_tags / num_possible_cpus();\r\nif (nr_cache < BLK_MQ_TAG_CACHE_MIN)\r\nnr_cache = BLK_MQ_TAG_CACHE_MIN;\r\nelse if (nr_cache > BLK_MQ_TAG_CACHE_MAX)\r\nnr_cache = BLK_MQ_TAG_CACHE_MAX;\r\ntags->nr_tags = total_tags;\r\ntags->nr_reserved_tags = reserved_tags;\r\ntags->nr_max_cache = nr_cache;\r\ntags->nr_batch_move = max(1u, nr_cache / 2);\r\nret = __percpu_ida_init(&tags->free_tags, tags->nr_tags -\r\ntags->nr_reserved_tags,\r\ntags->nr_max_cache,\r\ntags->nr_batch_move);\r\nif (ret)\r\ngoto err_free_tags;\r\nif (reserved_tags) {\r\nret = __percpu_ida_init(&tags->reserved_tags, reserved_tags,\r\n1, 1);\r\nif (ret)\r\ngoto err_reserved_tags;\r\n}\r\nreturn tags;\r\nerr_reserved_tags:\r\npercpu_ida_destroy(&tags->free_tags);\r\nerr_free_tags:\r\nkfree(tags);\r\nreturn NULL;\r\n}\r\nvoid blk_mq_free_tags(struct blk_mq_tags *tags)\r\n{\r\npercpu_ida_destroy(&tags->free_tags);\r\npercpu_ida_destroy(&tags->reserved_tags);\r\nkfree(tags);\r\n}\r\nssize_t blk_mq_tag_sysfs_show(struct blk_mq_tags *tags, char *page)\r\n{\r\nchar *orig_page = page;\r\nunsigned int cpu;\r\nif (!tags)\r\nreturn 0;\r\npage += sprintf(page, "nr_tags=%u, reserved_tags=%u, batch_move=%u,"\r\n" max_cache=%u\n", tags->nr_tags, tags->nr_reserved_tags,\r\ntags->nr_batch_move, tags->nr_max_cache);\r\npage += sprintf(page, "nr_free=%u, nr_reserved=%u\n",\r\npercpu_ida_free_tags(&tags->free_tags, nr_cpu_ids),\r\npercpu_ida_free_tags(&tags->reserved_tags, nr_cpu_ids));\r\nfor_each_possible_cpu(cpu) {\r\npage += sprintf(page, " cpu%02u: nr_free=%u\n", cpu,\r\npercpu_ida_free_tags(&tags->free_tags, cpu));\r\n}\r\nreturn page - orig_page;\r\n}
