static unsigned next_power(unsigned n, unsigned min)\r\n{\r\nreturn roundup_pow_of_two(max(n, min));\r\n}\r\nstatic void iot_init(struct io_tracker *t,\r\nint sequential_threshold, int random_threshold)\r\n{\r\nt->pattern = PATTERN_RANDOM;\r\nt->nr_seq_samples = 0;\r\nt->nr_rand_samples = 0;\r\nt->last_end_oblock = 0;\r\nt->thresholds[PATTERN_RANDOM] = random_threshold;\r\nt->thresholds[PATTERN_SEQUENTIAL] = sequential_threshold;\r\n}\r\nstatic enum io_pattern iot_pattern(struct io_tracker *t)\r\n{\r\nreturn t->pattern;\r\n}\r\nstatic void iot_update_stats(struct io_tracker *t, struct bio *bio)\r\n{\r\nif (bio->bi_iter.bi_sector == from_oblock(t->last_end_oblock) + 1)\r\nt->nr_seq_samples++;\r\nelse {\r\nif (t->nr_seq_samples) {\r\nt->nr_seq_samples = 0;\r\nt->nr_rand_samples = 0;\r\n}\r\nt->nr_rand_samples++;\r\n}\r\nt->last_end_oblock = to_oblock(bio_end_sector(bio) - 1);\r\n}\r\nstatic void iot_check_for_pattern_switch(struct io_tracker *t)\r\n{\r\nswitch (t->pattern) {\r\ncase PATTERN_SEQUENTIAL:\r\nif (t->nr_rand_samples >= t->thresholds[PATTERN_RANDOM]) {\r\nt->pattern = PATTERN_RANDOM;\r\nt->nr_seq_samples = t->nr_rand_samples = 0;\r\n}\r\nbreak;\r\ncase PATTERN_RANDOM:\r\nif (t->nr_seq_samples >= t->thresholds[PATTERN_SEQUENTIAL]) {\r\nt->pattern = PATTERN_SEQUENTIAL;\r\nt->nr_seq_samples = t->nr_rand_samples = 0;\r\n}\r\nbreak;\r\n}\r\n}\r\nstatic void iot_examine_bio(struct io_tracker *t, struct bio *bio)\r\n{\r\niot_update_stats(t, bio);\r\niot_check_for_pattern_switch(t);\r\n}\r\nstatic void queue_init(struct queue *q)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < NR_QUEUE_LEVELS; i++)\r\nINIT_LIST_HEAD(q->qs + i);\r\n}\r\nstatic bool queue_empty(struct queue *q)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < NR_QUEUE_LEVELS; i++)\r\nif (!list_empty(q->qs + i))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic void queue_push(struct queue *q, unsigned level, struct list_head *elt)\r\n{\r\nlist_add_tail(elt, q->qs + level);\r\n}\r\nstatic void queue_remove(struct list_head *elt)\r\n{\r\nlist_del(elt);\r\n}\r\nstatic void queue_shift_down(struct queue *q)\r\n{\r\nunsigned level;\r\nfor (level = 1; level < NR_QUEUE_LEVELS; level++)\r\nlist_splice_init(q->qs + level, q->qs + level - 1);\r\n}\r\nstatic struct list_head *queue_pop(struct queue *q)\r\n{\r\nunsigned level;\r\nstruct list_head *r;\r\nfor (level = 0; level < NR_QUEUE_LEVELS; level++)\r\nif (!list_empty(q->qs + level)) {\r\nr = q->qs[level].next;\r\nlist_del(r);\r\nif (level == 0 && list_empty(q->qs))\r\nqueue_shift_down(q);\r\nreturn r;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct list_head *list_pop(struct list_head *lh)\r\n{\r\nstruct list_head *r = lh->next;\r\nBUG_ON(!r);\r\nlist_del_init(r);\r\nreturn r;\r\n}\r\nstatic int epool_init(struct entry_pool *ep, unsigned nr_entries)\r\n{\r\nunsigned i;\r\nep->entries = vzalloc(sizeof(struct entry) * nr_entries);\r\nif (!ep->entries)\r\nreturn -ENOMEM;\r\nep->entries_end = ep->entries + nr_entries;\r\nINIT_LIST_HEAD(&ep->free);\r\nfor (i = 0; i < nr_entries; i++)\r\nlist_add(&ep->entries[i].list, &ep->free);\r\nep->nr_allocated = 0;\r\nreturn 0;\r\n}\r\nstatic void epool_exit(struct entry_pool *ep)\r\n{\r\nvfree(ep->entries);\r\n}\r\nstatic struct entry *alloc_entry(struct entry_pool *ep)\r\n{\r\nstruct entry *e;\r\nif (list_empty(&ep->free))\r\nreturn NULL;\r\ne = list_entry(list_pop(&ep->free), struct entry, list);\r\nINIT_LIST_HEAD(&e->list);\r\nINIT_HLIST_NODE(&e->hlist);\r\nep->nr_allocated++;\r\nreturn e;\r\n}\r\nstatic struct entry *alloc_particular_entry(struct entry_pool *ep, dm_cblock_t cblock)\r\n{\r\nstruct entry *e = ep->entries + from_cblock(cblock);\r\nlist_del_init(&e->list);\r\nINIT_HLIST_NODE(&e->hlist);\r\nep->nr_allocated++;\r\nreturn e;\r\n}\r\nstatic void free_entry(struct entry_pool *ep, struct entry *e)\r\n{\r\nBUG_ON(!ep->nr_allocated);\r\nep->nr_allocated--;\r\nINIT_HLIST_NODE(&e->hlist);\r\nlist_add(&e->list, &ep->free);\r\n}\r\nstatic struct entry *epool_find(struct entry_pool *ep, dm_cblock_t cblock)\r\n{\r\nstruct entry *e = ep->entries + from_cblock(cblock);\r\nreturn !hlist_unhashed(&e->hlist) ? e : NULL;\r\n}\r\nstatic bool epool_empty(struct entry_pool *ep)\r\n{\r\nreturn list_empty(&ep->free);\r\n}\r\nstatic bool in_pool(struct entry_pool *ep, struct entry *e)\r\n{\r\nreturn e >= ep->entries && e < ep->entries_end;\r\n}\r\nstatic dm_cblock_t infer_cblock(struct entry_pool *ep, struct entry *e)\r\n{\r\nreturn to_cblock(e - ep->entries);\r\n}\r\nstatic void hash_insert(struct mq_policy *mq, struct entry *e)\r\n{\r\nunsigned h = hash_64(from_oblock(e->oblock), mq->hash_bits);\r\nhlist_add_head(&e->hlist, mq->table + h);\r\n}\r\nstatic struct entry *hash_lookup(struct mq_policy *mq, dm_oblock_t oblock)\r\n{\r\nunsigned h = hash_64(from_oblock(oblock), mq->hash_bits);\r\nstruct hlist_head *bucket = mq->table + h;\r\nstruct entry *e;\r\nhlist_for_each_entry(e, bucket, hlist)\r\nif (e->oblock == oblock) {\r\nhlist_del(&e->hlist);\r\nhlist_add_head(&e->hlist, bucket);\r\nreturn e;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void hash_remove(struct entry *e)\r\n{\r\nhlist_del(&e->hlist);\r\n}\r\nstatic bool any_free_cblocks(struct mq_policy *mq)\r\n{\r\nreturn !epool_empty(&mq->cache_pool);\r\n}\r\nstatic bool any_clean_cblocks(struct mq_policy *mq)\r\n{\r\nreturn !queue_empty(&mq->cache_clean);\r\n}\r\nstatic unsigned queue_level(struct entry *e)\r\n{\r\nreturn min((unsigned) ilog2(e->hit_count), NR_QUEUE_LEVELS - 1u);\r\n}\r\nstatic bool in_cache(struct mq_policy *mq, struct entry *e)\r\n{\r\nreturn in_pool(&mq->cache_pool, e);\r\n}\r\nstatic void push(struct mq_policy *mq, struct entry *e)\r\n{\r\ne->tick = mq->tick;\r\nhash_insert(mq, e);\r\nif (in_cache(mq, e))\r\nqueue_push(e->dirty ? &mq->cache_dirty : &mq->cache_clean,\r\nqueue_level(e), &e->list);\r\nelse\r\nqueue_push(&mq->pre_cache, queue_level(e), &e->list);\r\n}\r\nstatic void del(struct mq_policy *mq, struct entry *e)\r\n{\r\nqueue_remove(&e->list);\r\nhash_remove(e);\r\n}\r\nstatic struct entry *pop(struct mq_policy *mq, struct queue *q)\r\n{\r\nstruct entry *e;\r\nstruct list_head *h = queue_pop(q);\r\nif (!h)\r\nreturn NULL;\r\ne = container_of(h, struct entry, list);\r\nhash_remove(e);\r\nreturn e;\r\n}\r\nstatic bool updated_this_tick(struct mq_policy *mq, struct entry *e)\r\n{\r\nreturn mq->tick == e->tick;\r\n}\r\nstatic void check_generation(struct mq_policy *mq)\r\n{\r\nunsigned total = 0, nr = 0, count = 0, level;\r\nstruct list_head *head;\r\nstruct entry *e;\r\nif ((mq->hit_count >= mq->generation_period) && (epool_empty(&mq->cache_pool))) {\r\nmq->hit_count = 0;\r\nmq->generation++;\r\nfor (level = 0; level < NR_QUEUE_LEVELS && count < MAX_TO_AVERAGE; level++) {\r\nhead = mq->cache_clean.qs + level;\r\nlist_for_each_entry(e, head, list) {\r\nnr++;\r\ntotal += e->hit_count;\r\nif (++count >= MAX_TO_AVERAGE)\r\nbreak;\r\n}\r\nhead = mq->cache_dirty.qs + level;\r\nlist_for_each_entry(e, head, list) {\r\nnr++;\r\ntotal += e->hit_count;\r\nif (++count >= MAX_TO_AVERAGE)\r\nbreak;\r\n}\r\n}\r\nmq->promote_threshold = nr ? total / nr : 1;\r\nif (mq->promote_threshold * nr < total)\r\nmq->promote_threshold++;\r\n}\r\n}\r\nstatic void requeue_and_update_tick(struct mq_policy *mq, struct entry *e)\r\n{\r\nif (updated_this_tick(mq, e))\r\nreturn;\r\ne->hit_count++;\r\nmq->hit_count++;\r\ncheck_generation(mq);\r\ne->generation = mq->generation;\r\ndel(mq, e);\r\npush(mq, e);\r\n}\r\nstatic int demote_cblock(struct mq_policy *mq, dm_oblock_t *oblock)\r\n{\r\nstruct entry *demoted = pop(mq, &mq->cache_clean);\r\nif (!demoted)\r\nreturn -ENOSPC;\r\n*oblock = demoted->oblock;\r\nfree_entry(&mq->cache_pool, demoted);\r\nreturn 0;\r\n}\r\nstatic unsigned adjusted_promote_threshold(struct mq_policy *mq,\r\nbool discarded_oblock, int data_dir)\r\n{\r\nif (data_dir == READ)\r\nreturn mq->promote_threshold + mq->read_promote_adjustment;\r\nif (discarded_oblock && (any_free_cblocks(mq) || any_clean_cblocks(mq))) {\r\nreturn mq->discard_promote_adjustment;\r\n}\r\nreturn mq->promote_threshold + mq->write_promote_adjustment;\r\n}\r\nstatic bool should_promote(struct mq_policy *mq, struct entry *e,\r\nbool discarded_oblock, int data_dir)\r\n{\r\nreturn e->hit_count >=\r\nadjusted_promote_threshold(mq, discarded_oblock, data_dir);\r\n}\r\nstatic int cache_entry_found(struct mq_policy *mq,\r\nstruct entry *e,\r\nstruct policy_result *result)\r\n{\r\nrequeue_and_update_tick(mq, e);\r\nif (in_cache(mq, e)) {\r\nresult->op = POLICY_HIT;\r\nresult->cblock = infer_cblock(&mq->cache_pool, e);\r\n}\r\nreturn 0;\r\n}\r\nstatic int pre_cache_to_cache(struct mq_policy *mq, struct entry *e,\r\nstruct policy_result *result)\r\n{\r\nint r;\r\nstruct entry *new_e;\r\nif (epool_empty(&mq->cache_pool)) {\r\nresult->op = POLICY_REPLACE;\r\nr = demote_cblock(mq, &result->old_oblock);\r\nif (r) {\r\nresult->op = POLICY_MISS;\r\nreturn 0;\r\n}\r\n} else\r\nresult->op = POLICY_NEW;\r\nnew_e = alloc_entry(&mq->cache_pool);\r\nBUG_ON(!new_e);\r\nnew_e->oblock = e->oblock;\r\nnew_e->dirty = false;\r\nnew_e->hit_count = e->hit_count;\r\nnew_e->generation = e->generation;\r\nnew_e->tick = e->tick;\r\ndel(mq, e);\r\nfree_entry(&mq->pre_cache_pool, e);\r\npush(mq, new_e);\r\nresult->cblock = infer_cblock(&mq->cache_pool, new_e);\r\nreturn 0;\r\n}\r\nstatic int pre_cache_entry_found(struct mq_policy *mq, struct entry *e,\r\nbool can_migrate, bool discarded_oblock,\r\nint data_dir, struct policy_result *result)\r\n{\r\nint r = 0;\r\nbool updated = updated_this_tick(mq, e);\r\nif ((!discarded_oblock && updated) ||\r\n!should_promote(mq, e, discarded_oblock, data_dir)) {\r\nrequeue_and_update_tick(mq, e);\r\nresult->op = POLICY_MISS;\r\n} else if (!can_migrate)\r\nr = -EWOULDBLOCK;\r\nelse {\r\nrequeue_and_update_tick(mq, e);\r\nr = pre_cache_to_cache(mq, e, result);\r\n}\r\nreturn r;\r\n}\r\nstatic void insert_in_pre_cache(struct mq_policy *mq,\r\ndm_oblock_t oblock)\r\n{\r\nstruct entry *e = alloc_entry(&mq->pre_cache_pool);\r\nif (!e)\r\ne = pop(mq, &mq->pre_cache);\r\nif (unlikely(!e)) {\r\nDMWARN("couldn't pop from pre cache");\r\nreturn;\r\n}\r\ne->dirty = false;\r\ne->oblock = oblock;\r\ne->hit_count = 1;\r\ne->generation = mq->generation;\r\npush(mq, e);\r\n}\r\nstatic void insert_in_cache(struct mq_policy *mq, dm_oblock_t oblock,\r\nstruct policy_result *result)\r\n{\r\nint r;\r\nstruct entry *e;\r\nif (epool_empty(&mq->cache_pool)) {\r\nresult->op = POLICY_REPLACE;\r\nr = demote_cblock(mq, &result->old_oblock);\r\nif (unlikely(r)) {\r\nresult->op = POLICY_MISS;\r\ninsert_in_pre_cache(mq, oblock);\r\nreturn;\r\n}\r\ne = alloc_entry(&mq->cache_pool);\r\nBUG_ON(!e);\r\n} else {\r\ne = alloc_entry(&mq->cache_pool);\r\nresult->op = POLICY_NEW;\r\n}\r\ne->oblock = oblock;\r\ne->dirty = false;\r\ne->hit_count = 1;\r\ne->generation = mq->generation;\r\npush(mq, e);\r\nresult->cblock = infer_cblock(&mq->cache_pool, e);\r\n}\r\nstatic int no_entry_found(struct mq_policy *mq, dm_oblock_t oblock,\r\nbool can_migrate, bool discarded_oblock,\r\nint data_dir, struct policy_result *result)\r\n{\r\nif (adjusted_promote_threshold(mq, discarded_oblock, data_dir) <= 1) {\r\nif (can_migrate)\r\ninsert_in_cache(mq, oblock, result);\r\nelse\r\nreturn -EWOULDBLOCK;\r\n} else {\r\ninsert_in_pre_cache(mq, oblock);\r\nresult->op = POLICY_MISS;\r\n}\r\nreturn 0;\r\n}\r\nstatic int map(struct mq_policy *mq, dm_oblock_t oblock,\r\nbool can_migrate, bool discarded_oblock,\r\nint data_dir, struct policy_result *result)\r\n{\r\nint r = 0;\r\nstruct entry *e = hash_lookup(mq, oblock);\r\nif (e && in_cache(mq, e))\r\nr = cache_entry_found(mq, e, result);\r\nelse if (iot_pattern(&mq->tracker) == PATTERN_SEQUENTIAL)\r\nresult->op = POLICY_MISS;\r\nelse if (e)\r\nr = pre_cache_entry_found(mq, e, can_migrate, discarded_oblock,\r\ndata_dir, result);\r\nelse\r\nr = no_entry_found(mq, oblock, can_migrate, discarded_oblock,\r\ndata_dir, result);\r\nif (r == -EWOULDBLOCK)\r\nresult->op = POLICY_MISS;\r\nreturn r;\r\n}\r\nstatic struct mq_policy *to_mq_policy(struct dm_cache_policy *p)\r\n{\r\nreturn container_of(p, struct mq_policy, policy);\r\n}\r\nstatic void mq_destroy(struct dm_cache_policy *p)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nvfree(mq->table);\r\nepool_exit(&mq->cache_pool);\r\nepool_exit(&mq->pre_cache_pool);\r\nkfree(mq);\r\n}\r\nstatic void copy_tick(struct mq_policy *mq)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&mq->tick_lock, flags);\r\nmq->tick = mq->tick_protected;\r\nspin_unlock_irqrestore(&mq->tick_lock, flags);\r\n}\r\nstatic int mq_map(struct dm_cache_policy *p, dm_oblock_t oblock,\r\nbool can_block, bool can_migrate, bool discarded_oblock,\r\nstruct bio *bio, struct policy_result *result)\r\n{\r\nint r;\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nresult->op = POLICY_MISS;\r\nif (can_block)\r\nmutex_lock(&mq->lock);\r\nelse if (!mutex_trylock(&mq->lock))\r\nreturn -EWOULDBLOCK;\r\ncopy_tick(mq);\r\niot_examine_bio(&mq->tracker, bio);\r\nr = map(mq, oblock, can_migrate, discarded_oblock,\r\nbio_data_dir(bio), result);\r\nmutex_unlock(&mq->lock);\r\nreturn r;\r\n}\r\nstatic int mq_lookup(struct dm_cache_policy *p, dm_oblock_t oblock, dm_cblock_t *cblock)\r\n{\r\nint r;\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nstruct entry *e;\r\nif (!mutex_trylock(&mq->lock))\r\nreturn -EWOULDBLOCK;\r\ne = hash_lookup(mq, oblock);\r\nif (e && in_cache(mq, e)) {\r\n*cblock = infer_cblock(&mq->cache_pool, e);\r\nr = 0;\r\n} else\r\nr = -ENOENT;\r\nmutex_unlock(&mq->lock);\r\nreturn r;\r\n}\r\nstatic void __mq_set_clear_dirty(struct mq_policy *mq, dm_oblock_t oblock, bool set)\r\n{\r\nstruct entry *e;\r\ne = hash_lookup(mq, oblock);\r\nBUG_ON(!e || !in_cache(mq, e));\r\ndel(mq, e);\r\ne->dirty = set;\r\npush(mq, e);\r\n}\r\nstatic void mq_set_dirty(struct dm_cache_policy *p, dm_oblock_t oblock)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nmutex_lock(&mq->lock);\r\n__mq_set_clear_dirty(mq, oblock, true);\r\nmutex_unlock(&mq->lock);\r\n}\r\nstatic void mq_clear_dirty(struct dm_cache_policy *p, dm_oblock_t oblock)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nmutex_lock(&mq->lock);\r\n__mq_set_clear_dirty(mq, oblock, false);\r\nmutex_unlock(&mq->lock);\r\n}\r\nstatic int mq_load_mapping(struct dm_cache_policy *p,\r\ndm_oblock_t oblock, dm_cblock_t cblock,\r\nuint32_t hint, bool hint_valid)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nstruct entry *e;\r\ne = alloc_particular_entry(&mq->cache_pool, cblock);\r\ne->oblock = oblock;\r\ne->dirty = false;\r\ne->hit_count = hint_valid ? hint : 1;\r\ne->generation = mq->generation;\r\npush(mq, e);\r\nreturn 0;\r\n}\r\nstatic int mq_save_hints(struct mq_policy *mq, struct queue *q,\r\npolicy_walk_fn fn, void *context)\r\n{\r\nint r;\r\nunsigned level;\r\nstruct entry *e;\r\nfor (level = 0; level < NR_QUEUE_LEVELS; level++)\r\nlist_for_each_entry(e, q->qs + level, list) {\r\nr = fn(context, infer_cblock(&mq->cache_pool, e),\r\ne->oblock, e->hit_count);\r\nif (r)\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic int mq_walk_mappings(struct dm_cache_policy *p, policy_walk_fn fn,\r\nvoid *context)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nint r = 0;\r\nmutex_lock(&mq->lock);\r\nr = mq_save_hints(mq, &mq->cache_clean, fn, context);\r\nif (!r)\r\nr = mq_save_hints(mq, &mq->cache_dirty, fn, context);\r\nmutex_unlock(&mq->lock);\r\nreturn r;\r\n}\r\nstatic void __remove_mapping(struct mq_policy *mq, dm_oblock_t oblock)\r\n{\r\nstruct entry *e;\r\ne = hash_lookup(mq, oblock);\r\nBUG_ON(!e || !in_cache(mq, e));\r\ndel(mq, e);\r\nfree_entry(&mq->cache_pool, e);\r\n}\r\nstatic void mq_remove_mapping(struct dm_cache_policy *p, dm_oblock_t oblock)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nmutex_lock(&mq->lock);\r\n__remove_mapping(mq, oblock);\r\nmutex_unlock(&mq->lock);\r\n}\r\nstatic int __remove_cblock(struct mq_policy *mq, dm_cblock_t cblock)\r\n{\r\nstruct entry *e = epool_find(&mq->cache_pool, cblock);\r\nif (!e)\r\nreturn -ENODATA;\r\ndel(mq, e);\r\nfree_entry(&mq->cache_pool, e);\r\nreturn 0;\r\n}\r\nstatic int mq_remove_cblock(struct dm_cache_policy *p, dm_cblock_t cblock)\r\n{\r\nint r;\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nmutex_lock(&mq->lock);\r\nr = __remove_cblock(mq, cblock);\r\nmutex_unlock(&mq->lock);\r\nreturn r;\r\n}\r\nstatic int __mq_writeback_work(struct mq_policy *mq, dm_oblock_t *oblock,\r\ndm_cblock_t *cblock)\r\n{\r\nstruct entry *e = pop(mq, &mq->cache_dirty);\r\nif (!e)\r\nreturn -ENODATA;\r\n*oblock = e->oblock;\r\n*cblock = infer_cblock(&mq->cache_pool, e);\r\ne->dirty = false;\r\npush(mq, e);\r\nreturn 0;\r\n}\r\nstatic int mq_writeback_work(struct dm_cache_policy *p, dm_oblock_t *oblock,\r\ndm_cblock_t *cblock)\r\n{\r\nint r;\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nmutex_lock(&mq->lock);\r\nr = __mq_writeback_work(mq, oblock, cblock);\r\nmutex_unlock(&mq->lock);\r\nreturn r;\r\n}\r\nstatic void __force_mapping(struct mq_policy *mq,\r\ndm_oblock_t current_oblock, dm_oblock_t new_oblock)\r\n{\r\nstruct entry *e = hash_lookup(mq, current_oblock);\r\nif (e && in_cache(mq, e)) {\r\ndel(mq, e);\r\ne->oblock = new_oblock;\r\ne->dirty = true;\r\npush(mq, e);\r\n}\r\n}\r\nstatic void mq_force_mapping(struct dm_cache_policy *p,\r\ndm_oblock_t current_oblock, dm_oblock_t new_oblock)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nmutex_lock(&mq->lock);\r\n__force_mapping(mq, current_oblock, new_oblock);\r\nmutex_unlock(&mq->lock);\r\n}\r\nstatic dm_cblock_t mq_residency(struct dm_cache_policy *p)\r\n{\r\ndm_cblock_t r;\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nmutex_lock(&mq->lock);\r\nr = to_cblock(mq->cache_pool.nr_allocated);\r\nmutex_unlock(&mq->lock);\r\nreturn r;\r\n}\r\nstatic void mq_tick(struct dm_cache_policy *p)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nunsigned long flags;\r\nspin_lock_irqsave(&mq->tick_lock, flags);\r\nmq->tick_protected++;\r\nspin_unlock_irqrestore(&mq->tick_lock, flags);\r\n}\r\nstatic int mq_set_config_value(struct dm_cache_policy *p,\r\nconst char *key, const char *value)\r\n{\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nunsigned long tmp;\r\nif (kstrtoul(value, 10, &tmp))\r\nreturn -EINVAL;\r\nif (!strcasecmp(key, "random_threshold")) {\r\nmq->tracker.thresholds[PATTERN_RANDOM] = tmp;\r\n} else if (!strcasecmp(key, "sequential_threshold")) {\r\nmq->tracker.thresholds[PATTERN_SEQUENTIAL] = tmp;\r\n} else if (!strcasecmp(key, "discard_promote_adjustment"))\r\nmq->discard_promote_adjustment = tmp;\r\nelse if (!strcasecmp(key, "read_promote_adjustment"))\r\nmq->read_promote_adjustment = tmp;\r\nelse if (!strcasecmp(key, "write_promote_adjustment"))\r\nmq->write_promote_adjustment = tmp;\r\nelse\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int mq_emit_config_values(struct dm_cache_policy *p, char *result, unsigned maxlen)\r\n{\r\nssize_t sz = 0;\r\nstruct mq_policy *mq = to_mq_policy(p);\r\nDMEMIT("10 random_threshold %u "\r\n"sequential_threshold %u "\r\n"discard_promote_adjustment %u "\r\n"read_promote_adjustment %u "\r\n"write_promote_adjustment %u",\r\nmq->tracker.thresholds[PATTERN_RANDOM],\r\nmq->tracker.thresholds[PATTERN_SEQUENTIAL],\r\nmq->discard_promote_adjustment,\r\nmq->read_promote_adjustment,\r\nmq->write_promote_adjustment);\r\nreturn 0;\r\n}\r\nstatic void init_policy_functions(struct mq_policy *mq)\r\n{\r\nmq->policy.destroy = mq_destroy;\r\nmq->policy.map = mq_map;\r\nmq->policy.lookup = mq_lookup;\r\nmq->policy.set_dirty = mq_set_dirty;\r\nmq->policy.clear_dirty = mq_clear_dirty;\r\nmq->policy.load_mapping = mq_load_mapping;\r\nmq->policy.walk_mappings = mq_walk_mappings;\r\nmq->policy.remove_mapping = mq_remove_mapping;\r\nmq->policy.remove_cblock = mq_remove_cblock;\r\nmq->policy.writeback_work = mq_writeback_work;\r\nmq->policy.force_mapping = mq_force_mapping;\r\nmq->policy.residency = mq_residency;\r\nmq->policy.tick = mq_tick;\r\nmq->policy.emit_config_values = mq_emit_config_values;\r\nmq->policy.set_config_value = mq_set_config_value;\r\n}\r\nstatic struct dm_cache_policy *mq_create(dm_cblock_t cache_size,\r\nsector_t origin_size,\r\nsector_t cache_block_size)\r\n{\r\nstruct mq_policy *mq = kzalloc(sizeof(*mq), GFP_KERNEL);\r\nif (!mq)\r\nreturn NULL;\r\ninit_policy_functions(mq);\r\niot_init(&mq->tracker, SEQUENTIAL_THRESHOLD_DEFAULT, RANDOM_THRESHOLD_DEFAULT);\r\nmq->cache_size = cache_size;\r\nif (epool_init(&mq->pre_cache_pool, from_cblock(cache_size))) {\r\nDMERR("couldn't initialize pool of pre-cache entries");\r\ngoto bad_pre_cache_init;\r\n}\r\nif (epool_init(&mq->cache_pool, from_cblock(cache_size))) {\r\nDMERR("couldn't initialize pool of cache entries");\r\ngoto bad_cache_init;\r\n}\r\nmq->tick_protected = 0;\r\nmq->tick = 0;\r\nmq->hit_count = 0;\r\nmq->generation = 0;\r\nmq->promote_threshold = 0;\r\nmq->discard_promote_adjustment = DEFAULT_DISCARD_PROMOTE_ADJUSTMENT;\r\nmq->read_promote_adjustment = DEFAULT_READ_PROMOTE_ADJUSTMENT;\r\nmq->write_promote_adjustment = DEFAULT_WRITE_PROMOTE_ADJUSTMENT;\r\nmutex_init(&mq->lock);\r\nspin_lock_init(&mq->tick_lock);\r\nqueue_init(&mq->pre_cache);\r\nqueue_init(&mq->cache_clean);\r\nqueue_init(&mq->cache_dirty);\r\nmq->generation_period = max((unsigned) from_cblock(cache_size), 1024U);\r\nmq->nr_buckets = next_power(from_cblock(cache_size) / 2, 16);\r\nmq->hash_bits = ffs(mq->nr_buckets) - 1;\r\nmq->table = vzalloc(sizeof(*mq->table) * mq->nr_buckets);\r\nif (!mq->table)\r\ngoto bad_alloc_table;\r\nreturn &mq->policy;\r\nbad_alloc_table:\r\nepool_exit(&mq->cache_pool);\r\nbad_cache_init:\r\nepool_exit(&mq->pre_cache_pool);\r\nbad_pre_cache_init:\r\nkfree(mq);\r\nreturn NULL;\r\n}\r\nstatic int __init mq_init(void)\r\n{\r\nint r;\r\nmq_entry_cache = kmem_cache_create("dm_mq_policy_cache_entry",\r\nsizeof(struct entry),\r\n__alignof__(struct entry),\r\n0, NULL);\r\nif (!mq_entry_cache)\r\ngoto bad;\r\nr = dm_cache_policy_register(&mq_policy_type);\r\nif (r) {\r\nDMERR("register failed %d", r);\r\ngoto bad_register_mq;\r\n}\r\nr = dm_cache_policy_register(&default_policy_type);\r\nif (!r) {\r\nDMINFO("version %u.%u.%u loaded",\r\nmq_policy_type.version[0],\r\nmq_policy_type.version[1],\r\nmq_policy_type.version[2]);\r\nreturn 0;\r\n}\r\nDMERR("register failed (as default) %d", r);\r\ndm_cache_policy_unregister(&mq_policy_type);\r\nbad_register_mq:\r\nkmem_cache_destroy(mq_entry_cache);\r\nbad:\r\nreturn -ENOMEM;\r\n}\r\nstatic void __exit mq_exit(void)\r\n{\r\ndm_cache_policy_unregister(&mq_policy_type);\r\ndm_cache_policy_unregister(&default_policy_type);\r\nkmem_cache_destroy(mq_entry_cache);\r\n}
