static inline struct sfq_head *sfq_dep_head(struct sfq_sched_data *q, sfq_index val)\r\n{\r\nif (val < SFQ_MAX_FLOWS)\r\nreturn &q->slots[val].dep;\r\nreturn &q->dep[val - SFQ_MAX_FLOWS];\r\n}\r\nstatic inline struct sfq_skb_cb *sfq_skb_cb(const struct sk_buff *skb)\r\n{\r\nqdisc_cb_private_validate(skb, sizeof(struct sfq_skb_cb));\r\nreturn (struct sfq_skb_cb *)qdisc_skb_cb(skb)->data;\r\n}\r\nstatic unsigned int sfq_hash(const struct sfq_sched_data *q,\r\nconst struct sk_buff *skb)\r\n{\r\nconst struct flow_keys *keys = &sfq_skb_cb(skb)->keys;\r\nunsigned int hash;\r\nhash = jhash_3words((__force u32)keys->dst,\r\n(__force u32)keys->src ^ keys->ip_proto,\r\n(__force u32)keys->ports, q->perturbation);\r\nreturn hash & (q->divisor - 1);\r\n}\r\nstatic unsigned int sfq_classify(struct sk_buff *skb, struct Qdisc *sch,\r\nint *qerr)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nstruct tcf_result res;\r\nint result;\r\nif (TC_H_MAJ(skb->priority) == sch->handle &&\r\nTC_H_MIN(skb->priority) > 0 &&\r\nTC_H_MIN(skb->priority) <= q->divisor)\r\nreturn TC_H_MIN(skb->priority);\r\nif (!q->filter_list) {\r\nskb_flow_dissect(skb, &sfq_skb_cb(skb)->keys);\r\nreturn sfq_hash(q, skb) + 1;\r\n}\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\r\nresult = tc_classify(skb, q->filter_list, &res);\r\nif (result >= 0) {\r\n#ifdef CONFIG_NET_CLS_ACT\r\nswitch (result) {\r\ncase TC_ACT_STOLEN:\r\ncase TC_ACT_QUEUED:\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\r\ncase TC_ACT_SHOT:\r\nreturn 0;\r\n}\r\n#endif\r\nif (TC_H_MIN(res.classid) <= q->divisor)\r\nreturn TC_H_MIN(res.classid);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void sfq_link(struct sfq_sched_data *q, sfq_index x)\r\n{\r\nsfq_index p, n;\r\nstruct sfq_slot *slot = &q->slots[x];\r\nint qlen = slot->qlen;\r\np = qlen + SFQ_MAX_FLOWS;\r\nn = q->dep[qlen].next;\r\nslot->dep.next = n;\r\nslot->dep.prev = p;\r\nq->dep[qlen].next = x;\r\nsfq_dep_head(q, n)->prev = x;\r\n}\r\nstatic inline void sfq_dec(struct sfq_sched_data *q, sfq_index x)\r\n{\r\nsfq_index p, n;\r\nint d;\r\nsfq_unlink(q, x, n, p);\r\nd = q->slots[x].qlen--;\r\nif (n == p && q->cur_depth == d)\r\nq->cur_depth--;\r\nsfq_link(q, x);\r\n}\r\nstatic inline void sfq_inc(struct sfq_sched_data *q, sfq_index x)\r\n{\r\nsfq_index p, n;\r\nint d;\r\nsfq_unlink(q, x, n, p);\r\nd = ++q->slots[x].qlen;\r\nif (q->cur_depth < d)\r\nq->cur_depth = d;\r\nsfq_link(q, x);\r\n}\r\nstatic inline struct sk_buff *slot_dequeue_tail(struct sfq_slot *slot)\r\n{\r\nstruct sk_buff *skb = slot->skblist_prev;\r\nslot->skblist_prev = skb->prev;\r\nskb->prev->next = (struct sk_buff *)slot;\r\nskb->next = skb->prev = NULL;\r\nreturn skb;\r\n}\r\nstatic inline struct sk_buff *slot_dequeue_head(struct sfq_slot *slot)\r\n{\r\nstruct sk_buff *skb = slot->skblist_next;\r\nslot->skblist_next = skb->next;\r\nskb->next->prev = (struct sk_buff *)slot;\r\nskb->next = skb->prev = NULL;\r\nreturn skb;\r\n}\r\nstatic inline void slot_queue_init(struct sfq_slot *slot)\r\n{\r\nmemset(slot, 0, sizeof(*slot));\r\nslot->skblist_prev = slot->skblist_next = (struct sk_buff *)slot;\r\n}\r\nstatic inline void slot_queue_add(struct sfq_slot *slot, struct sk_buff *skb)\r\n{\r\nskb->prev = slot->skblist_prev;\r\nskb->next = (struct sk_buff *)slot;\r\nslot->skblist_prev->next = skb;\r\nslot->skblist_prev = skb;\r\n}\r\nstatic unsigned int sfq_drop(struct Qdisc *sch)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nsfq_index x, d = q->cur_depth;\r\nstruct sk_buff *skb;\r\nunsigned int len;\r\nstruct sfq_slot *slot;\r\nif (d > 1) {\r\nx = q->dep[d].next;\r\nslot = &q->slots[x];\r\ndrop:\r\nskb = q->headdrop ? slot_dequeue_head(slot) : slot_dequeue_tail(slot);\r\nlen = qdisc_pkt_len(skb);\r\nslot->backlog -= len;\r\nsfq_dec(q, x);\r\nkfree_skb(skb);\r\nsch->q.qlen--;\r\nsch->qstats.drops++;\r\nsch->qstats.backlog -= len;\r\nreturn len;\r\n}\r\nif (d == 1) {\r\nx = q->tail->next;\r\nslot = &q->slots[x];\r\nq->tail->next = slot->next;\r\nq->ht[slot->hash] = SFQ_EMPTY_SLOT;\r\ngoto drop;\r\n}\r\nreturn 0;\r\n}\r\nstatic int sfq_prob_mark(const struct sfq_sched_data *q)\r\n{\r\nreturn q->flags & TC_RED_ECN;\r\n}\r\nstatic int sfq_hard_mark(const struct sfq_sched_data *q)\r\n{\r\nreturn (q->flags & (TC_RED_ECN | TC_RED_HARDDROP)) == TC_RED_ECN;\r\n}\r\nstatic int sfq_headdrop(const struct sfq_sched_data *q)\r\n{\r\nreturn q->headdrop;\r\n}\r\nstatic int\r\nsfq_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nunsigned int hash;\r\nsfq_index x, qlen;\r\nstruct sfq_slot *slot;\r\nint uninitialized_var(ret);\r\nstruct sk_buff *head;\r\nint delta;\r\nhash = sfq_classify(skb, sch, &ret);\r\nif (hash == 0) {\r\nif (ret & __NET_XMIT_BYPASS)\r\nsch->qstats.drops++;\r\nkfree_skb(skb);\r\nreturn ret;\r\n}\r\nhash--;\r\nx = q->ht[hash];\r\nslot = &q->slots[x];\r\nif (x == SFQ_EMPTY_SLOT) {\r\nx = q->dep[0].next;\r\nif (x >= SFQ_MAX_FLOWS)\r\nreturn qdisc_drop(skb, sch);\r\nq->ht[hash] = x;\r\nslot = &q->slots[x];\r\nslot->hash = hash;\r\nslot->backlog = 0;\r\nred_set_vars(&slot->vars);\r\ngoto enqueue;\r\n}\r\nif (q->red_parms) {\r\nslot->vars.qavg = red_calc_qavg_no_idle_time(q->red_parms,\r\n&slot->vars,\r\nslot->backlog);\r\nswitch (red_action(q->red_parms,\r\n&slot->vars,\r\nslot->vars.qavg)) {\r\ncase RED_DONT_MARK:\r\nbreak;\r\ncase RED_PROB_MARK:\r\nsch->qstats.overlimits++;\r\nif (sfq_prob_mark(q)) {\r\nif (sfq_headdrop(q) &&\r\nINET_ECN_set_ce(slot->skblist_next)) {\r\nq->stats.prob_mark_head++;\r\nbreak;\r\n}\r\nif (INET_ECN_set_ce(skb)) {\r\nq->stats.prob_mark++;\r\nbreak;\r\n}\r\n}\r\nq->stats.prob_drop++;\r\ngoto congestion_drop;\r\ncase RED_HARD_MARK:\r\nsch->qstats.overlimits++;\r\nif (sfq_hard_mark(q)) {\r\nif (sfq_headdrop(q) &&\r\nINET_ECN_set_ce(slot->skblist_next)) {\r\nq->stats.forced_mark_head++;\r\nbreak;\r\n}\r\nif (INET_ECN_set_ce(skb)) {\r\nq->stats.forced_mark++;\r\nbreak;\r\n}\r\n}\r\nq->stats.forced_drop++;\r\ngoto congestion_drop;\r\n}\r\n}\r\nif (slot->qlen >= q->maxdepth) {\r\ncongestion_drop:\r\nif (!sfq_headdrop(q))\r\nreturn qdisc_drop(skb, sch);\r\nhead = slot_dequeue_head(slot);\r\ndelta = qdisc_pkt_len(head) - qdisc_pkt_len(skb);\r\nsch->qstats.backlog -= delta;\r\nslot->backlog -= delta;\r\nqdisc_drop(head, sch);\r\nslot_queue_add(slot, skb);\r\nreturn NET_XMIT_CN;\r\n}\r\nenqueue:\r\nsch->qstats.backlog += qdisc_pkt_len(skb);\r\nslot->backlog += qdisc_pkt_len(skb);\r\nslot_queue_add(slot, skb);\r\nsfq_inc(q, x);\r\nif (slot->qlen == 1) {\r\nif (q->tail == NULL) {\r\nslot->next = x;\r\n} else {\r\nslot->next = q->tail->next;\r\nq->tail->next = x;\r\n}\r\nq->tail = slot;\r\nslot->allot = q->scaled_quantum;\r\n}\r\nif (++sch->q.qlen <= q->limit)\r\nreturn NET_XMIT_SUCCESS;\r\nqlen = slot->qlen;\r\nsfq_drop(sch);\r\nif (qlen != slot->qlen)\r\nreturn NET_XMIT_CN;\r\nqdisc_tree_decrease_qlen(sch, 1);\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic struct sk_buff *\r\nsfq_dequeue(struct Qdisc *sch)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nsfq_index a, next_a;\r\nstruct sfq_slot *slot;\r\nif (q->tail == NULL)\r\nreturn NULL;\r\nnext_slot:\r\na = q->tail->next;\r\nslot = &q->slots[a];\r\nif (slot->allot <= 0) {\r\nq->tail = slot;\r\nslot->allot += q->scaled_quantum;\r\ngoto next_slot;\r\n}\r\nskb = slot_dequeue_head(slot);\r\nsfq_dec(q, a);\r\nqdisc_bstats_update(sch, skb);\r\nsch->q.qlen--;\r\nsch->qstats.backlog -= qdisc_pkt_len(skb);\r\nslot->backlog -= qdisc_pkt_len(skb);\r\nif (slot->qlen == 0) {\r\nq->ht[slot->hash] = SFQ_EMPTY_SLOT;\r\nnext_a = slot->next;\r\nif (a == next_a) {\r\nq->tail = NULL;\r\nreturn skb;\r\n}\r\nq->tail->next = next_a;\r\n} else {\r\nslot->allot -= SFQ_ALLOT_SIZE(qdisc_pkt_len(skb));\r\n}\r\nreturn skb;\r\n}\r\nstatic void\r\nsfq_reset(struct Qdisc *sch)\r\n{\r\nstruct sk_buff *skb;\r\nwhile ((skb = sfq_dequeue(sch)) != NULL)\r\nkfree_skb(skb);\r\n}\r\nstatic void sfq_rehash(struct Qdisc *sch)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nint i;\r\nstruct sfq_slot *slot;\r\nstruct sk_buff_head list;\r\nint dropped = 0;\r\n__skb_queue_head_init(&list);\r\nfor (i = 0; i < q->maxflows; i++) {\r\nslot = &q->slots[i];\r\nif (!slot->qlen)\r\ncontinue;\r\nwhile (slot->qlen) {\r\nskb = slot_dequeue_head(slot);\r\nsfq_dec(q, i);\r\n__skb_queue_tail(&list, skb);\r\n}\r\nslot->backlog = 0;\r\nred_set_vars(&slot->vars);\r\nq->ht[slot->hash] = SFQ_EMPTY_SLOT;\r\n}\r\nq->tail = NULL;\r\nwhile ((skb = __skb_dequeue(&list)) != NULL) {\r\nunsigned int hash = sfq_hash(q, skb);\r\nsfq_index x = q->ht[hash];\r\nslot = &q->slots[x];\r\nif (x == SFQ_EMPTY_SLOT) {\r\nx = q->dep[0].next;\r\nif (x >= SFQ_MAX_FLOWS) {\r\ndrop: sch->qstats.backlog -= qdisc_pkt_len(skb);\r\nkfree_skb(skb);\r\ndropped++;\r\ncontinue;\r\n}\r\nq->ht[hash] = x;\r\nslot = &q->slots[x];\r\nslot->hash = hash;\r\n}\r\nif (slot->qlen >= q->maxdepth)\r\ngoto drop;\r\nslot_queue_add(slot, skb);\r\nif (q->red_parms)\r\nslot->vars.qavg = red_calc_qavg(q->red_parms,\r\n&slot->vars,\r\nslot->backlog);\r\nslot->backlog += qdisc_pkt_len(skb);\r\nsfq_inc(q, x);\r\nif (slot->qlen == 1) {\r\nif (q->tail == NULL) {\r\nslot->next = x;\r\n} else {\r\nslot->next = q->tail->next;\r\nq->tail->next = x;\r\n}\r\nq->tail = slot;\r\nslot->allot = q->scaled_quantum;\r\n}\r\n}\r\nsch->q.qlen -= dropped;\r\nqdisc_tree_decrease_qlen(sch, dropped);\r\n}\r\nstatic void sfq_perturbation(unsigned long arg)\r\n{\r\nstruct Qdisc *sch = (struct Qdisc *)arg;\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nspinlock_t *root_lock = qdisc_lock(qdisc_root_sleeping(sch));\r\nspin_lock(root_lock);\r\nq->perturbation = prandom_u32();\r\nif (!q->filter_list && q->tail)\r\nsfq_rehash(sch);\r\nspin_unlock(root_lock);\r\nif (q->perturb_period)\r\nmod_timer(&q->perturb_timer, jiffies + q->perturb_period);\r\n}\r\nstatic int sfq_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nstruct tc_sfq_qopt *ctl = nla_data(opt);\r\nstruct tc_sfq_qopt_v1 *ctl_v1 = NULL;\r\nunsigned int qlen;\r\nstruct red_parms *p = NULL;\r\nif (opt->nla_len < nla_attr_size(sizeof(*ctl)))\r\nreturn -EINVAL;\r\nif (opt->nla_len >= nla_attr_size(sizeof(*ctl_v1)))\r\nctl_v1 = nla_data(opt);\r\nif (ctl->divisor &&\r\n(!is_power_of_2(ctl->divisor) || ctl->divisor > 65536))\r\nreturn -EINVAL;\r\nif (ctl_v1 && ctl_v1->qth_min) {\r\np = kmalloc(sizeof(*p), GFP_KERNEL);\r\nif (!p)\r\nreturn -ENOMEM;\r\n}\r\nsch_tree_lock(sch);\r\nif (ctl->quantum) {\r\nq->quantum = ctl->quantum;\r\nq->scaled_quantum = SFQ_ALLOT_SIZE(q->quantum);\r\n}\r\nq->perturb_period = ctl->perturb_period * HZ;\r\nif (ctl->flows)\r\nq->maxflows = min_t(u32, ctl->flows, SFQ_MAX_FLOWS);\r\nif (ctl->divisor) {\r\nq->divisor = ctl->divisor;\r\nq->maxflows = min_t(u32, q->maxflows, q->divisor);\r\n}\r\nif (ctl_v1) {\r\nif (ctl_v1->depth)\r\nq->maxdepth = min_t(u32, ctl_v1->depth, SFQ_MAX_DEPTH);\r\nif (p) {\r\nswap(q->red_parms, p);\r\nred_set_parms(q->red_parms,\r\nctl_v1->qth_min, ctl_v1->qth_max,\r\nctl_v1->Wlog,\r\nctl_v1->Plog, ctl_v1->Scell_log,\r\nNULL,\r\nctl_v1->max_P);\r\n}\r\nq->flags = ctl_v1->flags;\r\nq->headdrop = ctl_v1->headdrop;\r\n}\r\nif (ctl->limit) {\r\nq->limit = min_t(u32, ctl->limit, q->maxdepth * q->maxflows);\r\nq->maxflows = min_t(u32, q->maxflows, q->limit);\r\n}\r\nqlen = sch->q.qlen;\r\nwhile (sch->q.qlen > q->limit)\r\nsfq_drop(sch);\r\nqdisc_tree_decrease_qlen(sch, qlen - sch->q.qlen);\r\ndel_timer(&q->perturb_timer);\r\nif (q->perturb_period) {\r\nmod_timer(&q->perturb_timer, jiffies + q->perturb_period);\r\nq->perturbation = prandom_u32();\r\n}\r\nsch_tree_unlock(sch);\r\nkfree(p);\r\nreturn 0;\r\n}\r\nstatic void *sfq_alloc(size_t sz)\r\n{\r\nvoid *ptr = kmalloc(sz, GFP_KERNEL | __GFP_NOWARN);\r\nif (!ptr)\r\nptr = vmalloc(sz);\r\nreturn ptr;\r\n}\r\nstatic void sfq_free(void *addr)\r\n{\r\nif (addr) {\r\nif (is_vmalloc_addr(addr))\r\nvfree(addr);\r\nelse\r\nkfree(addr);\r\n}\r\n}\r\nstatic void sfq_destroy(struct Qdisc *sch)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\ntcf_destroy_chain(&q->filter_list);\r\nq->perturb_period = 0;\r\ndel_timer_sync(&q->perturb_timer);\r\nsfq_free(q->ht);\r\nsfq_free(q->slots);\r\nkfree(q->red_parms);\r\n}\r\nstatic int sfq_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nint i;\r\nq->perturb_timer.function = sfq_perturbation;\r\nq->perturb_timer.data = (unsigned long)sch;\r\ninit_timer_deferrable(&q->perturb_timer);\r\nfor (i = 0; i < SFQ_MAX_DEPTH + 1; i++) {\r\nq->dep[i].next = i + SFQ_MAX_FLOWS;\r\nq->dep[i].prev = i + SFQ_MAX_FLOWS;\r\n}\r\nq->limit = SFQ_MAX_DEPTH;\r\nq->maxdepth = SFQ_MAX_DEPTH;\r\nq->cur_depth = 0;\r\nq->tail = NULL;\r\nq->divisor = SFQ_DEFAULT_HASH_DIVISOR;\r\nq->maxflows = SFQ_DEFAULT_FLOWS;\r\nq->quantum = psched_mtu(qdisc_dev(sch));\r\nq->scaled_quantum = SFQ_ALLOT_SIZE(q->quantum);\r\nq->perturb_period = 0;\r\nq->perturbation = prandom_u32();\r\nif (opt) {\r\nint err = sfq_change(sch, opt);\r\nif (err)\r\nreturn err;\r\n}\r\nq->ht = sfq_alloc(sizeof(q->ht[0]) * q->divisor);\r\nq->slots = sfq_alloc(sizeof(q->slots[0]) * q->maxflows);\r\nif (!q->ht || !q->slots) {\r\nsfq_destroy(sch);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < q->divisor; i++)\r\nq->ht[i] = SFQ_EMPTY_SLOT;\r\nfor (i = 0; i < q->maxflows; i++) {\r\nslot_queue_init(&q->slots[i]);\r\nsfq_link(q, i);\r\n}\r\nif (q->limit >= 1)\r\nsch->flags |= TCQ_F_CAN_BYPASS;\r\nelse\r\nsch->flags &= ~TCQ_F_CAN_BYPASS;\r\nreturn 0;\r\n}\r\nstatic int sfq_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nunsigned char *b = skb_tail_pointer(skb);\r\nstruct tc_sfq_qopt_v1 opt;\r\nstruct red_parms *p = q->red_parms;\r\nmemset(&opt, 0, sizeof(opt));\r\nopt.v0.quantum = q->quantum;\r\nopt.v0.perturb_period = q->perturb_period / HZ;\r\nopt.v0.limit = q->limit;\r\nopt.v0.divisor = q->divisor;\r\nopt.v0.flows = q->maxflows;\r\nopt.depth = q->maxdepth;\r\nopt.headdrop = q->headdrop;\r\nif (p) {\r\nopt.qth_min = p->qth_min >> p->Wlog;\r\nopt.qth_max = p->qth_max >> p->Wlog;\r\nopt.Wlog = p->Wlog;\r\nopt.Plog = p->Plog;\r\nopt.Scell_log = p->Scell_log;\r\nopt.max_P = p->max_P;\r\n}\r\nmemcpy(&opt.stats, &q->stats, sizeof(opt.stats));\r\nopt.flags = q->flags;\r\nif (nla_put(skb, TCA_OPTIONS, sizeof(opt), &opt))\r\ngoto nla_put_failure;\r\nreturn skb->len;\r\nnla_put_failure:\r\nnlmsg_trim(skb, b);\r\nreturn -1;\r\n}\r\nstatic struct Qdisc *sfq_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nreturn NULL;\r\n}\r\nstatic unsigned long sfq_get(struct Qdisc *sch, u32 classid)\r\n{\r\nreturn 0;\r\n}\r\nstatic unsigned long sfq_bind(struct Qdisc *sch, unsigned long parent,\r\nu32 classid)\r\n{\r\nsch->flags &= ~TCQ_F_CAN_BYPASS;\r\nreturn 0;\r\n}\r\nstatic void sfq_put(struct Qdisc *q, unsigned long cl)\r\n{\r\n}\r\nstatic struct tcf_proto **sfq_find_tcf(struct Qdisc *sch, unsigned long cl)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nif (cl)\r\nreturn NULL;\r\nreturn &q->filter_list;\r\n}\r\nstatic int sfq_dump_class(struct Qdisc *sch, unsigned long cl,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\ntcm->tcm_handle |= TC_H_MIN(cl);\r\nreturn 0;\r\n}\r\nstatic int sfq_dump_class_stats(struct Qdisc *sch, unsigned long cl,\r\nstruct gnet_dump *d)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nsfq_index idx = q->ht[cl - 1];\r\nstruct gnet_stats_queue qs = { 0 };\r\nstruct tc_sfq_xstats xstats = { 0 };\r\nif (idx != SFQ_EMPTY_SLOT) {\r\nconst struct sfq_slot *slot = &q->slots[idx];\r\nxstats.allot = slot->allot << SFQ_ALLOT_SHIFT;\r\nqs.qlen = slot->qlen;\r\nqs.backlog = slot->backlog;\r\n}\r\nif (gnet_stats_copy_queue(d, &qs) < 0)\r\nreturn -1;\r\nreturn gnet_stats_copy_app(d, &xstats, sizeof(xstats));\r\n}\r\nstatic void sfq_walk(struct Qdisc *sch, struct qdisc_walker *arg)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nunsigned int i;\r\nif (arg->stop)\r\nreturn;\r\nfor (i = 0; i < q->divisor; i++) {\r\nif (q->ht[i] == SFQ_EMPTY_SLOT ||\r\narg->count < arg->skip) {\r\narg->count++;\r\ncontinue;\r\n}\r\nif (arg->fn(sch, i + 1, arg) < 0) {\r\narg->stop = 1;\r\nbreak;\r\n}\r\narg->count++;\r\n}\r\n}\r\nstatic int __init sfq_module_init(void)\r\n{\r\nreturn register_qdisc(&sfq_qdisc_ops);\r\n}\r\nstatic void __exit sfq_module_exit(void)\r\n{\r\nunregister_qdisc(&sfq_qdisc_ops);\r\n}
