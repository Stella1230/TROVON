void ipath_cq_enter(struct ipath_cq *cq, struct ib_wc *entry, int solicited)\r\n{\r\nstruct ipath_cq_wc *wc;\r\nunsigned long flags;\r\nu32 head;\r\nu32 next;\r\nspin_lock_irqsave(&cq->lock, flags);\r\nwc = cq->queue;\r\nhead = wc->head;\r\nif (head >= (unsigned) cq->ibcq.cqe) {\r\nhead = cq->ibcq.cqe;\r\nnext = 0;\r\n} else\r\nnext = head + 1;\r\nif (unlikely(next == wc->tail)) {\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\nif (cq->ibcq.event_handler) {\r\nstruct ib_event ev;\r\nev.device = cq->ibcq.device;\r\nev.element.cq = &cq->ibcq;\r\nev.event = IB_EVENT_CQ_ERR;\r\ncq->ibcq.event_handler(&ev, cq->ibcq.cq_context);\r\n}\r\nreturn;\r\n}\r\nif (cq->ip) {\r\nwc->uqueue[head].wr_id = entry->wr_id;\r\nwc->uqueue[head].status = entry->status;\r\nwc->uqueue[head].opcode = entry->opcode;\r\nwc->uqueue[head].vendor_err = entry->vendor_err;\r\nwc->uqueue[head].byte_len = entry->byte_len;\r\nwc->uqueue[head].ex.imm_data = (__u32 __force) entry->ex.imm_data;\r\nwc->uqueue[head].qp_num = entry->qp->qp_num;\r\nwc->uqueue[head].src_qp = entry->src_qp;\r\nwc->uqueue[head].wc_flags = entry->wc_flags;\r\nwc->uqueue[head].pkey_index = entry->pkey_index;\r\nwc->uqueue[head].slid = entry->slid;\r\nwc->uqueue[head].sl = entry->sl;\r\nwc->uqueue[head].dlid_path_bits = entry->dlid_path_bits;\r\nwc->uqueue[head].port_num = entry->port_num;\r\nsmp_wmb();\r\n} else\r\nwc->kqueue[head] = *entry;\r\nwc->head = next;\r\nif (cq->notify == IB_CQ_NEXT_COMP ||\r\n(cq->notify == IB_CQ_SOLICITED && solicited)) {\r\ncq->notify = IB_CQ_NONE;\r\ncq->triggered++;\r\ntasklet_hi_schedule(&cq->comptask);\r\n}\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\nif (entry->status != IB_WC_SUCCESS)\r\nto_idev(cq->ibcq.device)->n_wqe_errs++;\r\n}\r\nint ipath_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *entry)\r\n{\r\nstruct ipath_cq *cq = to_icq(ibcq);\r\nstruct ipath_cq_wc *wc;\r\nunsigned long flags;\r\nint npolled;\r\nu32 tail;\r\nif (cq->ip) {\r\nnpolled = -EINVAL;\r\ngoto bail;\r\n}\r\nspin_lock_irqsave(&cq->lock, flags);\r\nwc = cq->queue;\r\ntail = wc->tail;\r\nif (tail > (u32) cq->ibcq.cqe)\r\ntail = (u32) cq->ibcq.cqe;\r\nfor (npolled = 0; npolled < num_entries; ++npolled, ++entry) {\r\nif (tail == wc->head)\r\nbreak;\r\n*entry = wc->kqueue[tail];\r\nif (tail >= cq->ibcq.cqe)\r\ntail = 0;\r\nelse\r\ntail++;\r\n}\r\nwc->tail = tail;\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\nbail:\r\nreturn npolled;\r\n}\r\nstatic void send_complete(unsigned long data)\r\n{\r\nstruct ipath_cq *cq = (struct ipath_cq *)data;\r\nfor (;;) {\r\nu8 triggered = cq->triggered;\r\ncq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);\r\nif (cq->triggered == triggered)\r\nreturn;\r\n}\r\n}\r\nstruct ib_cq *ipath_create_cq(struct ib_device *ibdev, int entries, int comp_vector,\r\nstruct ib_ucontext *context,\r\nstruct ib_udata *udata)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibdev);\r\nstruct ipath_cq *cq;\r\nstruct ipath_cq_wc *wc;\r\nstruct ib_cq *ret;\r\nu32 sz;\r\nif (entries < 1 || entries > ib_ipath_max_cqes) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto done;\r\n}\r\ncq = kmalloc(sizeof(*cq), GFP_KERNEL);\r\nif (!cq) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto done;\r\n}\r\nsz = sizeof(*wc);\r\nif (udata && udata->outlen >= sizeof(__u64))\r\nsz += sizeof(struct ib_uverbs_wc) * (entries + 1);\r\nelse\r\nsz += sizeof(struct ib_wc) * (entries + 1);\r\nwc = vmalloc_user(sz);\r\nif (!wc) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_cq;\r\n}\r\nif (udata && udata->outlen >= sizeof(__u64)) {\r\nint err;\r\ncq->ip = ipath_create_mmap_info(dev, sz, context, wc);\r\nif (!cq->ip) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_wc;\r\n}\r\nerr = ib_copy_to_udata(udata, &cq->ip->offset,\r\nsizeof(cq->ip->offset));\r\nif (err) {\r\nret = ERR_PTR(err);\r\ngoto bail_ip;\r\n}\r\n} else\r\ncq->ip = NULL;\r\nspin_lock(&dev->n_cqs_lock);\r\nif (dev->n_cqs_allocated == ib_ipath_max_cqs) {\r\nspin_unlock(&dev->n_cqs_lock);\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_ip;\r\n}\r\ndev->n_cqs_allocated++;\r\nspin_unlock(&dev->n_cqs_lock);\r\nif (cq->ip) {\r\nspin_lock_irq(&dev->pending_lock);\r\nlist_add(&cq->ip->pending_mmaps, &dev->pending_mmaps);\r\nspin_unlock_irq(&dev->pending_lock);\r\n}\r\ncq->ibcq.cqe = entries;\r\ncq->notify = IB_CQ_NONE;\r\ncq->triggered = 0;\r\nspin_lock_init(&cq->lock);\r\ntasklet_init(&cq->comptask, send_complete, (unsigned long)cq);\r\nwc->head = 0;\r\nwc->tail = 0;\r\ncq->queue = wc;\r\nret = &cq->ibcq;\r\ngoto done;\r\nbail_ip:\r\nkfree(cq->ip);\r\nbail_wc:\r\nvfree(wc);\r\nbail_cq:\r\nkfree(cq);\r\ndone:\r\nreturn ret;\r\n}\r\nint ipath_destroy_cq(struct ib_cq *ibcq)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibcq->device);\r\nstruct ipath_cq *cq = to_icq(ibcq);\r\ntasklet_kill(&cq->comptask);\r\nspin_lock(&dev->n_cqs_lock);\r\ndev->n_cqs_allocated--;\r\nspin_unlock(&dev->n_cqs_lock);\r\nif (cq->ip)\r\nkref_put(&cq->ip->ref, ipath_release_mmap_info);\r\nelse\r\nvfree(cq->queue);\r\nkfree(cq);\r\nreturn 0;\r\n}\r\nint ipath_req_notify_cq(struct ib_cq *ibcq, enum ib_cq_notify_flags notify_flags)\r\n{\r\nstruct ipath_cq *cq = to_icq(ibcq);\r\nunsigned long flags;\r\nint ret = 0;\r\nspin_lock_irqsave(&cq->lock, flags);\r\nif (cq->notify != IB_CQ_NEXT_COMP)\r\ncq->notify = notify_flags & IB_CQ_SOLICITED_MASK;\r\nif ((notify_flags & IB_CQ_REPORT_MISSED_EVENTS) &&\r\ncq->queue->head != cq->queue->tail)\r\nret = 1;\r\nspin_unlock_irqrestore(&cq->lock, flags);\r\nreturn ret;\r\n}\r\nint ipath_resize_cq(struct ib_cq *ibcq, int cqe, struct ib_udata *udata)\r\n{\r\nstruct ipath_cq *cq = to_icq(ibcq);\r\nstruct ipath_cq_wc *old_wc;\r\nstruct ipath_cq_wc *wc;\r\nu32 head, tail, n;\r\nint ret;\r\nu32 sz;\r\nif (cqe < 1 || cqe > ib_ipath_max_cqes) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nsz = sizeof(*wc);\r\nif (udata && udata->outlen >= sizeof(__u64))\r\nsz += sizeof(struct ib_uverbs_wc) * (cqe + 1);\r\nelse\r\nsz += sizeof(struct ib_wc) * (cqe + 1);\r\nwc = vmalloc_user(sz);\r\nif (!wc) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nif (udata && udata->outlen >= sizeof(__u64)) {\r\n__u64 offset = 0;\r\nret = ib_copy_to_udata(udata, &offset, sizeof(offset));\r\nif (ret)\r\ngoto bail_free;\r\n}\r\nspin_lock_irq(&cq->lock);\r\nold_wc = cq->queue;\r\nhead = old_wc->head;\r\nif (head > (u32) cq->ibcq.cqe)\r\nhead = (u32) cq->ibcq.cqe;\r\ntail = old_wc->tail;\r\nif (tail > (u32) cq->ibcq.cqe)\r\ntail = (u32) cq->ibcq.cqe;\r\nif (head < tail)\r\nn = cq->ibcq.cqe + 1 + head - tail;\r\nelse\r\nn = head - tail;\r\nif (unlikely((u32)cqe < n)) {\r\nret = -EINVAL;\r\ngoto bail_unlock;\r\n}\r\nfor (n = 0; tail != head; n++) {\r\nif (cq->ip)\r\nwc->uqueue[n] = old_wc->uqueue[tail];\r\nelse\r\nwc->kqueue[n] = old_wc->kqueue[tail];\r\nif (tail == (u32) cq->ibcq.cqe)\r\ntail = 0;\r\nelse\r\ntail++;\r\n}\r\ncq->ibcq.cqe = cqe;\r\nwc->head = n;\r\nwc->tail = 0;\r\ncq->queue = wc;\r\nspin_unlock_irq(&cq->lock);\r\nvfree(old_wc);\r\nif (cq->ip) {\r\nstruct ipath_ibdev *dev = to_idev(ibcq->device);\r\nstruct ipath_mmap_info *ip = cq->ip;\r\nipath_update_mmap_info(dev, ip, sz, wc);\r\nif (udata && udata->outlen >= sizeof(__u64)) {\r\nret = ib_copy_to_udata(udata, &ip->offset,\r\nsizeof(ip->offset));\r\nif (ret)\r\ngoto bail;\r\n}\r\nspin_lock_irq(&dev->pending_lock);\r\nif (list_empty(&ip->pending_mmaps))\r\nlist_add(&ip->pending_mmaps, &dev->pending_mmaps);\r\nspin_unlock_irq(&dev->pending_lock);\r\n}\r\nret = 0;\r\ngoto bail;\r\nbail_unlock:\r\nspin_unlock_irq(&cq->lock);\r\nbail_free:\r\nvfree(wc);\r\nbail:\r\nreturn ret;\r\n}
