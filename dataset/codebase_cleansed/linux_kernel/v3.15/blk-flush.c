static unsigned int blk_flush_policy(unsigned int fflags, struct request *rq)\r\n{\r\nunsigned int policy = 0;\r\nif (blk_rq_sectors(rq))\r\npolicy |= REQ_FSEQ_DATA;\r\nif (fflags & REQ_FLUSH) {\r\nif (rq->cmd_flags & REQ_FLUSH)\r\npolicy |= REQ_FSEQ_PREFLUSH;\r\nif (!(fflags & REQ_FUA) && (rq->cmd_flags & REQ_FUA))\r\npolicy |= REQ_FSEQ_POSTFLUSH;\r\n}\r\nreturn policy;\r\n}\r\nstatic unsigned int blk_flush_cur_seq(struct request *rq)\r\n{\r\nreturn 1 << ffz(rq->flush.seq);\r\n}\r\nstatic void blk_flush_restore_request(struct request *rq)\r\n{\r\nrq->bio = rq->biotail;\r\nrq->cmd_flags &= ~REQ_FLUSH_SEQ;\r\nrq->end_io = rq->flush.saved_end_io;\r\nblk_clear_rq_complete(rq);\r\n}\r\nstatic void mq_flush_run(struct work_struct *work)\r\n{\r\nstruct request *rq;\r\nrq = container_of(work, struct request, mq_flush_work);\r\nmemset(&rq->csd, 0, sizeof(rq->csd));\r\nblk_mq_insert_request(rq, false, true, false);\r\n}\r\nstatic bool blk_flush_queue_rq(struct request *rq, bool add_front)\r\n{\r\nif (rq->q->mq_ops) {\r\nINIT_WORK(&rq->mq_flush_work, mq_flush_run);\r\nkblockd_schedule_work(rq->q, &rq->mq_flush_work);\r\nreturn false;\r\n} else {\r\nif (add_front)\r\nlist_add(&rq->queuelist, &rq->q->queue_head);\r\nelse\r\nlist_add_tail(&rq->queuelist, &rq->q->queue_head);\r\nreturn true;\r\n}\r\n}\r\nstatic bool blk_flush_complete_seq(struct request *rq, unsigned int seq,\r\nint error)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct list_head *pending = &q->flush_queue[q->flush_pending_idx];\r\nbool queued = false, kicked;\r\nBUG_ON(rq->flush.seq & seq);\r\nrq->flush.seq |= seq;\r\nif (likely(!error))\r\nseq = blk_flush_cur_seq(rq);\r\nelse\r\nseq = REQ_FSEQ_DONE;\r\nswitch (seq) {\r\ncase REQ_FSEQ_PREFLUSH:\r\ncase REQ_FSEQ_POSTFLUSH:\r\nif (list_empty(pending))\r\nq->flush_pending_since = jiffies;\r\nlist_move_tail(&rq->flush.list, pending);\r\nbreak;\r\ncase REQ_FSEQ_DATA:\r\nlist_move_tail(&rq->flush.list, &q->flush_data_in_flight);\r\nqueued = blk_flush_queue_rq(rq, true);\r\nbreak;\r\ncase REQ_FSEQ_DONE:\r\nBUG_ON(!list_empty(&rq->queuelist));\r\nlist_del_init(&rq->flush.list);\r\nblk_flush_restore_request(rq);\r\nif (q->mq_ops)\r\nblk_mq_end_io(rq, error);\r\nelse\r\n__blk_end_request_all(rq, error);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nkicked = blk_kick_flush(q);\r\nreturn kicked | queued;\r\n}\r\nstatic void flush_end_io(struct request *flush_rq, int error)\r\n{\r\nstruct request_queue *q = flush_rq->q;\r\nstruct list_head *running;\r\nbool queued = false;\r\nstruct request *rq, *n;\r\nunsigned long flags = 0;\r\nif (q->mq_ops)\r\nspin_lock_irqsave(&q->mq_flush_lock, flags);\r\nrunning = &q->flush_queue[q->flush_running_idx];\r\nBUG_ON(q->flush_pending_idx == q->flush_running_idx);\r\nq->flush_running_idx ^= 1;\r\nif (!q->mq_ops)\r\nelv_completed_request(q, flush_rq);\r\nlist_for_each_entry_safe(rq, n, running, flush.list) {\r\nunsigned int seq = blk_flush_cur_seq(rq);\r\nBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\r\nqueued |= blk_flush_complete_seq(rq, seq, error);\r\n}\r\nif (queued || q->flush_queue_delayed) {\r\nWARN_ON(q->mq_ops);\r\nblk_run_queue_async(q);\r\n}\r\nq->flush_queue_delayed = 0;\r\nif (q->mq_ops)\r\nspin_unlock_irqrestore(&q->mq_flush_lock, flags);\r\n}\r\nstatic bool blk_kick_flush(struct request_queue *q)\r\n{\r\nstruct list_head *pending = &q->flush_queue[q->flush_pending_idx];\r\nstruct request *first_rq =\r\nlist_first_entry(pending, struct request, flush.list);\r\nif (q->flush_pending_idx != q->flush_running_idx || list_empty(pending))\r\nreturn false;\r\nif (!list_empty(&q->flush_data_in_flight) &&\r\ntime_before(jiffies,\r\nq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\r\nreturn false;\r\nq->flush_pending_idx ^= 1;\r\nif (q->mq_ops) {\r\nstruct blk_mq_ctx *ctx = first_rq->mq_ctx;\r\nstruct blk_mq_hw_ctx *hctx = q->mq_ops->map_queue(q, ctx->cpu);\r\nblk_mq_rq_init(hctx, q->flush_rq);\r\nq->flush_rq->mq_ctx = ctx;\r\nq->flush_rq->tag = first_rq->tag;\r\n} else {\r\nblk_rq_init(q, q->flush_rq);\r\n}\r\nq->flush_rq->cmd_type = REQ_TYPE_FS;\r\nq->flush_rq->cmd_flags = WRITE_FLUSH | REQ_FLUSH_SEQ;\r\nq->flush_rq->rq_disk = first_rq->rq_disk;\r\nq->flush_rq->end_io = flush_end_io;\r\nreturn blk_flush_queue_rq(q->flush_rq, false);\r\n}\r\nstatic void flush_data_end_io(struct request *rq, int error)\r\n{\r\nstruct request_queue *q = rq->q;\r\nif (blk_flush_complete_seq(rq, REQ_FSEQ_DATA, error))\r\nblk_run_queue_async(q);\r\n}\r\nstatic void mq_flush_data_end_io(struct request *rq, int error)\r\n{\r\nstruct request_queue *q = rq->q;\r\nstruct blk_mq_hw_ctx *hctx;\r\nstruct blk_mq_ctx *ctx;\r\nunsigned long flags;\r\nctx = rq->mq_ctx;\r\nhctx = q->mq_ops->map_queue(q, ctx->cpu);\r\nspin_lock_irqsave(&q->mq_flush_lock, flags);\r\nif (blk_flush_complete_seq(rq, REQ_FSEQ_DATA, error))\r\nblk_mq_run_hw_queue(hctx, true);\r\nspin_unlock_irqrestore(&q->mq_flush_lock, flags);\r\n}\r\nvoid blk_insert_flush(struct request *rq)\r\n{\r\nstruct request_queue *q = rq->q;\r\nunsigned int fflags = q->flush_flags;\r\nunsigned int policy = blk_flush_policy(fflags, rq);\r\nrq->cmd_flags &= ~REQ_FLUSH;\r\nif (!(fflags & REQ_FUA))\r\nrq->cmd_flags &= ~REQ_FUA;\r\nif (!policy) {\r\nif (q->mq_ops)\r\nblk_mq_end_io(rq, 0);\r\nelse\r\n__blk_end_bidi_request(rq, 0, 0, 0);\r\nreturn;\r\n}\r\nBUG_ON(rq->bio != rq->biotail);\r\nif ((policy & REQ_FSEQ_DATA) &&\r\n!(policy & (REQ_FSEQ_PREFLUSH | REQ_FSEQ_POSTFLUSH))) {\r\nif (q->mq_ops) {\r\nblk_mq_insert_request(rq, false, false, true);\r\n} else\r\nlist_add_tail(&rq->queuelist, &q->queue_head);\r\nreturn;\r\n}\r\nmemset(&rq->flush, 0, sizeof(rq->flush));\r\nINIT_LIST_HEAD(&rq->flush.list);\r\nrq->cmd_flags |= REQ_FLUSH_SEQ;\r\nrq->flush.saved_end_io = rq->end_io;\r\nif (q->mq_ops) {\r\nrq->end_io = mq_flush_data_end_io;\r\nspin_lock_irq(&q->mq_flush_lock);\r\nblk_flush_complete_seq(rq, REQ_FSEQ_ACTIONS & ~policy, 0);\r\nspin_unlock_irq(&q->mq_flush_lock);\r\nreturn;\r\n}\r\nrq->end_io = flush_data_end_io;\r\nblk_flush_complete_seq(rq, REQ_FSEQ_ACTIONS & ~policy, 0);\r\n}\r\nvoid blk_abort_flushes(struct request_queue *q)\r\n{\r\nstruct request *rq, *n;\r\nint i;\r\nlist_for_each_entry_safe(rq, n, &q->flush_data_in_flight, flush.list) {\r\nlist_del_init(&rq->flush.list);\r\nblk_flush_restore_request(rq);\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(q->flush_queue); i++) {\r\nlist_for_each_entry_safe(rq, n, &q->flush_queue[i],\r\nflush.list) {\r\nlist_del_init(&rq->flush.list);\r\nblk_flush_restore_request(rq);\r\nlist_add_tail(&rq->queuelist, &q->queue_head);\r\n}\r\n}\r\n}\r\nint blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,\r\nsector_t *error_sector)\r\n{\r\nstruct request_queue *q;\r\nstruct bio *bio;\r\nint ret = 0;\r\nif (bdev->bd_disk == NULL)\r\nreturn -ENXIO;\r\nq = bdev_get_queue(bdev);\r\nif (!q)\r\nreturn -ENXIO;\r\nif (!q->make_request_fn)\r\nreturn -ENXIO;\r\nbio = bio_alloc(gfp_mask, 0);\r\nbio->bi_bdev = bdev;\r\nret = submit_bio_wait(WRITE_FLUSH, bio);\r\nif (error_sector)\r\n*error_sector = bio->bi_iter.bi_sector;\r\nbio_put(bio);\r\nreturn ret;\r\n}\r\nvoid blk_mq_init_flush(struct request_queue *q)\r\n{\r\nspin_lock_init(&q->mq_flush_lock);\r\n}
