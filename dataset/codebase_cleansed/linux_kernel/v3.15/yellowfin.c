static int yellowfin_init_one(struct pci_dev *pdev,\r\nconst struct pci_device_id *ent)\r\n{\r\nstruct net_device *dev;\r\nstruct yellowfin_private *np;\r\nint irq;\r\nint chip_idx = ent->driver_data;\r\nstatic int find_cnt;\r\nvoid __iomem *ioaddr;\r\nint i, option = find_cnt < MAX_UNITS ? options[find_cnt] : 0;\r\nint drv_flags = pci_id_tbl[chip_idx].drv_flags;\r\nvoid *ring_space;\r\ndma_addr_t ring_dma;\r\n#ifdef USE_IO_OPS\r\nint bar = 0;\r\n#else\r\nint bar = 1;\r\n#endif\r\n#ifndef MODULE\r\nstatic int printed_version;\r\nif (!printed_version++)\r\nprintk(version);\r\n#endif\r\ni = pci_enable_device(pdev);\r\nif (i) return i;\r\ndev = alloc_etherdev(sizeof(*np));\r\nif (!dev)\r\nreturn -ENOMEM;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nnp = netdev_priv(dev);\r\nif (pci_request_regions(pdev, DRV_NAME))\r\ngoto err_out_free_netdev;\r\npci_set_master (pdev);\r\nioaddr = pci_iomap(pdev, bar, YELLOWFIN_SIZE);\r\nif (!ioaddr)\r\ngoto err_out_free_res;\r\nirq = pdev->irq;\r\nif (drv_flags & DontUseEeprom)\r\nfor (i = 0; i < 6; i++)\r\ndev->dev_addr[i] = ioread8(ioaddr + StnAddr + i);\r\nelse {\r\nint ee_offset = (read_eeprom(ioaddr, 6) == 0xff ? 0x100 : 0);\r\nfor (i = 0; i < 6; i++)\r\ndev->dev_addr[i] = read_eeprom(ioaddr, ee_offset + i);\r\n}\r\niowrite32(0x80000000, ioaddr + DMACtrl);\r\npci_set_drvdata(pdev, dev);\r\nspin_lock_init(&np->lock);\r\nnp->pci_dev = pdev;\r\nnp->chip_id = chip_idx;\r\nnp->drv_flags = drv_flags;\r\nnp->base = ioaddr;\r\nring_space = pci_alloc_consistent(pdev, TX_TOTAL_SIZE, &ring_dma);\r\nif (!ring_space)\r\ngoto err_out_cleardev;\r\nnp->tx_ring = ring_space;\r\nnp->tx_ring_dma = ring_dma;\r\nring_space = pci_alloc_consistent(pdev, RX_TOTAL_SIZE, &ring_dma);\r\nif (!ring_space)\r\ngoto err_out_unmap_tx;\r\nnp->rx_ring = ring_space;\r\nnp->rx_ring_dma = ring_dma;\r\nring_space = pci_alloc_consistent(pdev, STATUS_TOTAL_SIZE, &ring_dma);\r\nif (!ring_space)\r\ngoto err_out_unmap_rx;\r\nnp->tx_status = ring_space;\r\nnp->tx_status_dma = ring_dma;\r\nif (dev->mem_start)\r\noption = dev->mem_start;\r\nif (option > 0) {\r\nif (option & 0x200)\r\nnp->full_duplex = 1;\r\nnp->default_port = option & 15;\r\nif (np->default_port)\r\nnp->medialock = 1;\r\n}\r\nif (find_cnt < MAX_UNITS && full_duplex[find_cnt] > 0)\r\nnp->full_duplex = 1;\r\nif (np->full_duplex)\r\nnp->duplex_lock = 1;\r\ndev->netdev_ops = &netdev_ops;\r\nSET_ETHTOOL_OPS(dev, &ethtool_ops);\r\ndev->watchdog_timeo = TX_TIMEOUT;\r\nif (mtu)\r\ndev->mtu = mtu;\r\ni = register_netdev(dev);\r\nif (i)\r\ngoto err_out_unmap_status;\r\nnetdev_info(dev, "%s type %8x at %p, %pM, IRQ %d\n",\r\npci_id_tbl[chip_idx].name,\r\nioread32(ioaddr + ChipRev), ioaddr,\r\ndev->dev_addr, irq);\r\nif (np->drv_flags & HasMII) {\r\nint phy, phy_idx = 0;\r\nfor (phy = 0; phy < 32 && phy_idx < MII_CNT; phy++) {\r\nint mii_status = mdio_read(ioaddr, phy, 1);\r\nif (mii_status != 0xffff && mii_status != 0x0000) {\r\nnp->phys[phy_idx++] = phy;\r\nnp->advertising = mdio_read(ioaddr, phy, 4);\r\nnetdev_info(dev, "MII PHY found at address %d, status 0x%04x advertising %04x\n",\r\nphy, mii_status, np->advertising);\r\n}\r\n}\r\nnp->mii_cnt = phy_idx;\r\n}\r\nfind_cnt++;\r\nreturn 0;\r\nerr_out_unmap_status:\r\npci_free_consistent(pdev, STATUS_TOTAL_SIZE, np->tx_status,\r\nnp->tx_status_dma);\r\nerr_out_unmap_rx:\r\npci_free_consistent(pdev, RX_TOTAL_SIZE, np->rx_ring, np->rx_ring_dma);\r\nerr_out_unmap_tx:\r\npci_free_consistent(pdev, TX_TOTAL_SIZE, np->tx_ring, np->tx_ring_dma);\r\nerr_out_cleardev:\r\npci_iounmap(pdev, ioaddr);\r\nerr_out_free_res:\r\npci_release_regions(pdev);\r\nerr_out_free_netdev:\r\nfree_netdev (dev);\r\nreturn -ENODEV;\r\n}\r\nstatic int read_eeprom(void __iomem *ioaddr, int location)\r\n{\r\nint bogus_cnt = 10000;\r\niowrite8(location, ioaddr + EEAddr);\r\niowrite8(0x30 | ((location >> 8) & 7), ioaddr + EECtrl);\r\nwhile ((ioread8(ioaddr + EEStatus) & 0x80) && --bogus_cnt > 0)\r\n;\r\nreturn ioread8(ioaddr + EERead);\r\n}\r\nstatic int mdio_read(void __iomem *ioaddr, int phy_id, int location)\r\n{\r\nint i;\r\niowrite16((phy_id<<8) + location, ioaddr + MII_Addr);\r\niowrite16(1, ioaddr + MII_Cmd);\r\nfor (i = 10000; i >= 0; i--)\r\nif ((ioread16(ioaddr + MII_Status) & 1) == 0)\r\nbreak;\r\nreturn ioread16(ioaddr + MII_Rd_Data);\r\n}\r\nstatic void mdio_write(void __iomem *ioaddr, int phy_id, int location, int value)\r\n{\r\nint i;\r\niowrite16((phy_id<<8) + location, ioaddr + MII_Addr);\r\niowrite16(value, ioaddr + MII_Wr_Data);\r\nfor (i = 10000; i >= 0; i--)\r\nif ((ioread16(ioaddr + MII_Status) & 1) == 0)\r\nbreak;\r\n}\r\nstatic int yellowfin_open(struct net_device *dev)\r\n{\r\nstruct yellowfin_private *yp = netdev_priv(dev);\r\nconst int irq = yp->pci_dev->irq;\r\nvoid __iomem *ioaddr = yp->base;\r\nint i, rc;\r\niowrite32(0x80000000, ioaddr + DMACtrl);\r\nrc = request_irq(irq, yellowfin_interrupt, IRQF_SHARED, dev->name, dev);\r\nif (rc)\r\nreturn rc;\r\nrc = yellowfin_init_ring(dev);\r\nif (rc < 0)\r\ngoto err_free_irq;\r\niowrite32(yp->rx_ring_dma, ioaddr + RxPtr);\r\niowrite32(yp->tx_ring_dma, ioaddr + TxPtr);\r\nfor (i = 0; i < 6; i++)\r\niowrite8(dev->dev_addr[i], ioaddr + StnAddr + i);\r\niowrite32(0x00800080, ioaddr + TxIntrSel);\r\niowrite32(0x00800080, ioaddr + TxBranchSel);\r\niowrite32(0x00400040, ioaddr + TxWaitSel);\r\niowrite32(0x00400040, ioaddr + RxIntrSel);\r\niowrite32(0x00400040, ioaddr + RxBranchSel);\r\niowrite32(0x00400040, ioaddr + RxWaitSel);\r\niowrite32(dma_ctrl, ioaddr + DMACtrl);\r\niowrite16(fifo_cfg, ioaddr + FIFOcfg);\r\niowrite32(0x0030FFFF, ioaddr + FlowCtrl);\r\nyp->tx_threshold = 32;\r\niowrite32(yp->tx_threshold, ioaddr + TxThreshold);\r\nif (dev->if_port == 0)\r\ndev->if_port = yp->default_port;\r\nnetif_start_queue(dev);\r\nif (yp->drv_flags & IsGigabit) {\r\nyp->full_duplex = 1;\r\niowrite16(0x01CF, ioaddr + Cnfg);\r\n} else {\r\niowrite16(0x0018, ioaddr + FrameGap0);\r\niowrite16(0x1018, ioaddr + FrameGap1);\r\niowrite16(0x101C | (yp->full_duplex ? 2 : 0), ioaddr + Cnfg);\r\n}\r\nset_rx_mode(dev);\r\niowrite16(0x81ff, ioaddr + IntrEnb);\r\niowrite16(0x0000, ioaddr + EventStatus);\r\niowrite32(0x80008000, ioaddr + RxCtrl);\r\niowrite32(0x80008000, ioaddr + TxCtrl);\r\nif (yellowfin_debug > 2) {\r\nnetdev_printk(KERN_DEBUG, dev, "Done %s()\n", __func__);\r\n}\r\ninit_timer(&yp->timer);\r\nyp->timer.expires = jiffies + 3*HZ;\r\nyp->timer.data = (unsigned long)dev;\r\nyp->timer.function = yellowfin_timer;\r\nadd_timer(&yp->timer);\r\nout:\r\nreturn rc;\r\nerr_free_irq:\r\nfree_irq(irq, dev);\r\ngoto out;\r\n}\r\nstatic void yellowfin_timer(unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *)data;\r\nstruct yellowfin_private *yp = netdev_priv(dev);\r\nvoid __iomem *ioaddr = yp->base;\r\nint next_tick = 60*HZ;\r\nif (yellowfin_debug > 3) {\r\nnetdev_printk(KERN_DEBUG, dev, "Yellowfin timer tick, status %08x\n",\r\nioread16(ioaddr + IntrStatus));\r\n}\r\nif (yp->mii_cnt) {\r\nint bmsr = mdio_read(ioaddr, yp->phys[0], MII_BMSR);\r\nint lpa = mdio_read(ioaddr, yp->phys[0], MII_LPA);\r\nint negotiated = lpa & yp->advertising;\r\nif (yellowfin_debug > 1)\r\nnetdev_printk(KERN_DEBUG, dev, "MII #%d status register is %04x, link partner capability %04x\n",\r\nyp->phys[0], bmsr, lpa);\r\nyp->full_duplex = mii_duplex(yp->duplex_lock, negotiated);\r\niowrite16(0x101C | (yp->full_duplex ? 2 : 0), ioaddr + Cnfg);\r\nif (bmsr & BMSR_LSTATUS)\r\nnext_tick = 60*HZ;\r\nelse\r\nnext_tick = 3*HZ;\r\n}\r\nyp->timer.expires = jiffies + next_tick;\r\nadd_timer(&yp->timer);\r\n}\r\nstatic void yellowfin_tx_timeout(struct net_device *dev)\r\n{\r\nstruct yellowfin_private *yp = netdev_priv(dev);\r\nvoid __iomem *ioaddr = yp->base;\r\nnetdev_warn(dev, "Yellowfin transmit timed out at %d/%d Tx status %04x, Rx status %04x, resetting...\n",\r\nyp->cur_tx, yp->dirty_tx,\r\nioread32(ioaddr + TxStatus),\r\nioread32(ioaddr + RxStatus));\r\nif (yellowfin_debug) {\r\nint i;\r\npr_warning(" Rx ring %p: ", yp->rx_ring);\r\nfor (i = 0; i < RX_RING_SIZE; i++)\r\npr_cont(" %08x", yp->rx_ring[i].result_status);\r\npr_cont("\n");\r\npr_warning(" Tx ring %p: ", yp->tx_ring);\r\nfor (i = 0; i < TX_RING_SIZE; i++)\r\npr_cont(" %04x /%08x",\r\nyp->tx_status[i].tx_errs,\r\nyp->tx_ring[i].result_status);\r\npr_cont("\n");\r\n}\r\ndev->if_port = 0;\r\niowrite32(0x10001000, yp->base + TxCtrl);\r\nif (yp->cur_tx - yp->dirty_tx < TX_QUEUE_SIZE)\r\nnetif_wake_queue (dev);\r\ndev->trans_start = jiffies;\r\ndev->stats.tx_errors++;\r\n}\r\nstatic int yellowfin_init_ring(struct net_device *dev)\r\n{\r\nstruct yellowfin_private *yp = netdev_priv(dev);\r\nint i, j;\r\nyp->tx_full = 0;\r\nyp->cur_rx = yp->cur_tx = 0;\r\nyp->dirty_tx = 0;\r\nyp->rx_buf_sz = (dev->mtu <= 1500 ? PKT_BUF_SZ : dev->mtu + 32);\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nyp->rx_ring[i].dbdma_cmd =\r\ncpu_to_le32(CMD_RX_BUF | INTR_ALWAYS | yp->rx_buf_sz);\r\nyp->rx_ring[i].branch_addr = cpu_to_le32(yp->rx_ring_dma +\r\n((i+1)%RX_RING_SIZE)*sizeof(struct yellowfin_desc));\r\n}\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nstruct sk_buff *skb = netdev_alloc_skb(dev, yp->rx_buf_sz + 2);\r\nyp->rx_skbuff[i] = skb;\r\nif (skb == NULL)\r\nbreak;\r\nskb_reserve(skb, 2);\r\nyp->rx_ring[i].addr = cpu_to_le32(pci_map_single(yp->pci_dev,\r\nskb->data, yp->rx_buf_sz, PCI_DMA_FROMDEVICE));\r\n}\r\nif (i != RX_RING_SIZE) {\r\nfor (j = 0; j < i; j++)\r\ndev_kfree_skb(yp->rx_skbuff[j]);\r\nreturn -ENOMEM;\r\n}\r\nyp->rx_ring[i-1].dbdma_cmd = cpu_to_le32(CMD_STOP);\r\nyp->dirty_rx = (unsigned int)(i - RX_RING_SIZE);\r\n#define NO_TXSTATS\r\n#ifdef NO_TXSTATS\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nyp->tx_skbuff[i] = NULL;\r\nyp->tx_ring[i].dbdma_cmd = cpu_to_le32(CMD_STOP);\r\nyp->tx_ring[i].branch_addr = cpu_to_le32(yp->tx_ring_dma +\r\n((i+1)%TX_RING_SIZE)*sizeof(struct yellowfin_desc));\r\n}\r\nyp->tx_ring[--i].dbdma_cmd = cpu_to_le32(CMD_STOP | BRANCH_ALWAYS);\r\n#else\r\n{\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nj = 2*i;\r\nyp->tx_skbuff[i] = 0;\r\nyp->tx_ring[j].dbdma_cmd = cpu_to_le32(CMD_STOP);\r\nyp->tx_ring[j].branch_addr = cpu_to_le32(yp->tx_ring_dma +\r\n(j+1)*sizeof(struct yellowfin_desc));\r\nj++;\r\nif (yp->flags & FullTxStatus) {\r\nyp->tx_ring[j].dbdma_cmd =\r\ncpu_to_le32(CMD_TXSTATUS | sizeof(*yp->tx_status));\r\nyp->tx_ring[j].request_cnt = sizeof(*yp->tx_status);\r\nyp->tx_ring[j].addr = cpu_to_le32(yp->tx_status_dma +\r\ni*sizeof(struct tx_status_words));\r\n} else {\r\nyp->tx_ring[j].dbdma_cmd =\r\ncpu_to_le32(CMD_TXSTATUS | INTR_ALWAYS | 2);\r\nyp->tx_ring[j].request_cnt = 2;\r\nyp->tx_ring[j].addr = cpu_to_le32(yp->tx_status_dma +\r\ni*sizeof(struct tx_status_words) +\r\n&(yp->tx_status[0].tx_errs) -\r\n&(yp->tx_status[0]));\r\n}\r\nyp->tx_ring[j].branch_addr = cpu_to_le32(yp->tx_ring_dma +\r\n((j+1)%(2*TX_RING_SIZE))*sizeof(struct yellowfin_desc));\r\n}\r\nyp->tx_ring[++j].dbdma_cmd |= cpu_to_le32(BRANCH_ALWAYS | INTR_ALWAYS);\r\n}\r\n#endif\r\nyp->tx_tail_desc = &yp->tx_status[0];\r\nreturn 0;\r\n}\r\nstatic netdev_tx_t yellowfin_start_xmit(struct sk_buff *skb,\r\nstruct net_device *dev)\r\n{\r\nstruct yellowfin_private *yp = netdev_priv(dev);\r\nunsigned entry;\r\nint len = skb->len;\r\nnetif_stop_queue (dev);\r\nentry = yp->cur_tx % TX_RING_SIZE;\r\nif (gx_fix) {\r\nint cacheline_end = ((unsigned long)skb->data + skb->len) % 32;\r\nif (cacheline_end > 24 || cacheline_end == 0) {\r\nlen = skb->len + 32 - cacheline_end + 1;\r\nif (skb_padto(skb, len)) {\r\nyp->tx_skbuff[entry] = NULL;\r\nnetif_wake_queue(dev);\r\nreturn NETDEV_TX_OK;\r\n}\r\n}\r\n}\r\nyp->tx_skbuff[entry] = skb;\r\n#ifdef NO_TXSTATS\r\nyp->tx_ring[entry].addr = cpu_to_le32(pci_map_single(yp->pci_dev,\r\nskb->data, len, PCI_DMA_TODEVICE));\r\nyp->tx_ring[entry].result_status = 0;\r\nif (entry >= TX_RING_SIZE-1) {\r\nyp->tx_ring[0].dbdma_cmd = cpu_to_le32(CMD_STOP);\r\nyp->tx_ring[TX_RING_SIZE-1].dbdma_cmd =\r\ncpu_to_le32(CMD_TX_PKT|BRANCH_ALWAYS | len);\r\n} else {\r\nyp->tx_ring[entry+1].dbdma_cmd = cpu_to_le32(CMD_STOP);\r\nyp->tx_ring[entry].dbdma_cmd =\r\ncpu_to_le32(CMD_TX_PKT | BRANCH_IFTRUE | len);\r\n}\r\nyp->cur_tx++;\r\n#else\r\nyp->tx_ring[entry<<1].request_cnt = len;\r\nyp->tx_ring[entry<<1].addr = cpu_to_le32(pci_map_single(yp->pci_dev,\r\nskb->data, len, PCI_DMA_TODEVICE));\r\nyp->cur_tx++;\r\n{\r\nunsigned next_entry = yp->cur_tx % TX_RING_SIZE;\r\nyp->tx_ring[next_entry<<1].dbdma_cmd = cpu_to_le32(CMD_STOP);\r\n}\r\nyp->tx_ring[entry<<1].dbdma_cmd =\r\ncpu_to_le32( ((entry % 6) == 0 ? CMD_TX_PKT|INTR_ALWAYS|BRANCH_IFTRUE :\r\nCMD_TX_PKT | BRANCH_IFTRUE) | len);\r\n#endif\r\niowrite32(0x10001000, yp->base + TxCtrl);\r\nif (yp->cur_tx - yp->dirty_tx < TX_QUEUE_SIZE)\r\nnetif_start_queue (dev);\r\nelse\r\nyp->tx_full = 1;\r\nif (yellowfin_debug > 4) {\r\nnetdev_printk(KERN_DEBUG, dev, "Yellowfin transmit frame #%d queued in slot %d\n",\r\nyp->cur_tx, entry);\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic irqreturn_t yellowfin_interrupt(int irq, void *dev_instance)\r\n{\r\nstruct net_device *dev = dev_instance;\r\nstruct yellowfin_private *yp;\r\nvoid __iomem *ioaddr;\r\nint boguscnt = max_interrupt_work;\r\nunsigned int handled = 0;\r\nyp = netdev_priv(dev);\r\nioaddr = yp->base;\r\nspin_lock (&yp->lock);\r\ndo {\r\nu16 intr_status = ioread16(ioaddr + IntrClear);\r\nif (yellowfin_debug > 4)\r\nnetdev_printk(KERN_DEBUG, dev, "Yellowfin interrupt, status %04x\n",\r\nintr_status);\r\nif (intr_status == 0)\r\nbreak;\r\nhandled = 1;\r\nif (intr_status & (IntrRxDone | IntrEarlyRx)) {\r\nyellowfin_rx(dev);\r\niowrite32(0x10001000, ioaddr + RxCtrl);\r\n}\r\n#ifdef NO_TXSTATS\r\nfor (; yp->cur_tx - yp->dirty_tx > 0; yp->dirty_tx++) {\r\nint entry = yp->dirty_tx % TX_RING_SIZE;\r\nstruct sk_buff *skb;\r\nif (yp->tx_ring[entry].result_status == 0)\r\nbreak;\r\nskb = yp->tx_skbuff[entry];\r\ndev->stats.tx_packets++;\r\ndev->stats.tx_bytes += skb->len;\r\npci_unmap_single(yp->pci_dev, le32_to_cpu(yp->tx_ring[entry].addr),\r\nskb->len, PCI_DMA_TODEVICE);\r\ndev_kfree_skb_irq(skb);\r\nyp->tx_skbuff[entry] = NULL;\r\n}\r\nif (yp->tx_full &&\r\nyp->cur_tx - yp->dirty_tx < TX_QUEUE_SIZE - 4) {\r\nyp->tx_full = 0;\r\nnetif_wake_queue(dev);\r\n}\r\n#else\r\nif ((intr_status & IntrTxDone) || (yp->tx_tail_desc->tx_errs)) {\r\nunsigned dirty_tx = yp->dirty_tx;\r\nfor (dirty_tx = yp->dirty_tx; yp->cur_tx - dirty_tx > 0;\r\ndirty_tx++) {\r\nint entry = dirty_tx % TX_RING_SIZE;\r\nu16 tx_errs = yp->tx_status[entry].tx_errs;\r\nstruct sk_buff *skb;\r\n#ifndef final_version\r\nif (yellowfin_debug > 5)\r\nnetdev_printk(KERN_DEBUG, dev, "Tx queue %d check, Tx status %04x %04x %04x %04x\n",\r\nentry,\r\nyp->tx_status[entry].tx_cnt,\r\nyp->tx_status[entry].tx_errs,\r\nyp->tx_status[entry].total_tx_cnt,\r\nyp->tx_status[entry].paused);\r\n#endif\r\nif (tx_errs == 0)\r\nbreak;\r\nskb = yp->tx_skbuff[entry];\r\nif (tx_errs & 0xF810) {\r\n#ifndef final_version\r\nif (yellowfin_debug > 1)\r\nnetdev_printk(KERN_DEBUG, dev, "Transmit error, Tx status %04x\n",\r\ntx_errs);\r\n#endif\r\ndev->stats.tx_errors++;\r\nif (tx_errs & 0xF800) dev->stats.tx_aborted_errors++;\r\nif (tx_errs & 0x0800) dev->stats.tx_carrier_errors++;\r\nif (tx_errs & 0x2000) dev->stats.tx_window_errors++;\r\nif (tx_errs & 0x8000) dev->stats.tx_fifo_errors++;\r\n} else {\r\n#ifndef final_version\r\nif (yellowfin_debug > 4)\r\nnetdev_printk(KERN_DEBUG, dev, "Normal transmit, Tx status %04x\n",\r\ntx_errs);\r\n#endif\r\ndev->stats.tx_bytes += skb->len;\r\ndev->stats.collisions += tx_errs & 15;\r\ndev->stats.tx_packets++;\r\n}\r\npci_unmap_single(yp->pci_dev,\r\nyp->tx_ring[entry<<1].addr, skb->len,\r\nPCI_DMA_TODEVICE);\r\ndev_kfree_skb_irq(skb);\r\nyp->tx_skbuff[entry] = 0;\r\nyp->tx_status[entry].tx_errs = 0;\r\n}\r\n#ifndef final_version\r\nif (yp->cur_tx - dirty_tx > TX_RING_SIZE) {\r\nnetdev_err(dev, "Out-of-sync dirty pointer, %d vs. %d, full=%d\n",\r\ndirty_tx, yp->cur_tx, yp->tx_full);\r\ndirty_tx += TX_RING_SIZE;\r\n}\r\n#endif\r\nif (yp->tx_full &&\r\nyp->cur_tx - dirty_tx < TX_QUEUE_SIZE - 2) {\r\nyp->tx_full = 0;\r\nnetif_wake_queue(dev);\r\n}\r\nyp->dirty_tx = dirty_tx;\r\nyp->tx_tail_desc = &yp->tx_status[dirty_tx % TX_RING_SIZE];\r\n}\r\n#endif\r\nif (intr_status & 0x2ee)\r\nyellowfin_error(dev, intr_status);\r\nif (--boguscnt < 0) {\r\nnetdev_warn(dev, "Too much work at interrupt, status=%#04x\n",\r\nintr_status);\r\nbreak;\r\n}\r\n} while (1);\r\nif (yellowfin_debug > 3)\r\nnetdev_printk(KERN_DEBUG, dev, "exiting interrupt, status=%#04x\n",\r\nioread16(ioaddr + IntrStatus));\r\nspin_unlock (&yp->lock);\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic int yellowfin_rx(struct net_device *dev)\r\n{\r\nstruct yellowfin_private *yp = netdev_priv(dev);\r\nint entry = yp->cur_rx % RX_RING_SIZE;\r\nint boguscnt = yp->dirty_rx + RX_RING_SIZE - yp->cur_rx;\r\nif (yellowfin_debug > 4) {\r\nprintk(KERN_DEBUG " In yellowfin_rx(), entry %d status %08x\n",\r\nentry, yp->rx_ring[entry].result_status);\r\nprintk(KERN_DEBUG " #%d desc. %08x %08x %08x\n",\r\nentry, yp->rx_ring[entry].dbdma_cmd, yp->rx_ring[entry].addr,\r\nyp->rx_ring[entry].result_status);\r\n}\r\nwhile (1) {\r\nstruct yellowfin_desc *desc = &yp->rx_ring[entry];\r\nstruct sk_buff *rx_skb = yp->rx_skbuff[entry];\r\ns16 frame_status;\r\nu16 desc_status;\r\nint data_size, yf_size;\r\nu8 *buf_addr;\r\nif(!desc->result_status)\r\nbreak;\r\npci_dma_sync_single_for_cpu(yp->pci_dev, le32_to_cpu(desc->addr),\r\nyp->rx_buf_sz, PCI_DMA_FROMDEVICE);\r\ndesc_status = le32_to_cpu(desc->result_status) >> 16;\r\nbuf_addr = rx_skb->data;\r\ndata_size = (le32_to_cpu(desc->dbdma_cmd) -\r\nle32_to_cpu(desc->result_status)) & 0xffff;\r\nframe_status = get_unaligned_le16(&(buf_addr[data_size - 2]));\r\nif (yellowfin_debug > 4)\r\nprintk(KERN_DEBUG " %s() status was %04x\n",\r\n__func__, frame_status);\r\nif (--boguscnt < 0)\r\nbreak;\r\nyf_size = sizeof(struct yellowfin_desc);\r\nif ( ! (desc_status & RX_EOP)) {\r\nif (data_size != 0)\r\nnetdev_warn(dev, "Oversized Ethernet frame spanned multiple buffers, status %04x, data_size %d!\n",\r\ndesc_status, data_size);\r\ndev->stats.rx_length_errors++;\r\n} else if ((yp->drv_flags & IsGigabit) && (frame_status & 0x0038)) {\r\nif (yellowfin_debug > 3)\r\nprintk(KERN_DEBUG " %s() Rx error was %04x\n",\r\n__func__, frame_status);\r\ndev->stats.rx_errors++;\r\nif (frame_status & 0x0060) dev->stats.rx_length_errors++;\r\nif (frame_status & 0x0008) dev->stats.rx_frame_errors++;\r\nif (frame_status & 0x0010) dev->stats.rx_crc_errors++;\r\nif (frame_status < 0) dev->stats.rx_dropped++;\r\n} else if ( !(yp->drv_flags & IsGigabit) &&\r\n((buf_addr[data_size-1] & 0x85) || buf_addr[data_size-2] & 0xC0)) {\r\nu8 status1 = buf_addr[data_size-2];\r\nu8 status2 = buf_addr[data_size-1];\r\ndev->stats.rx_errors++;\r\nif (status1 & 0xC0) dev->stats.rx_length_errors++;\r\nif (status2 & 0x03) dev->stats.rx_frame_errors++;\r\nif (status2 & 0x04) dev->stats.rx_crc_errors++;\r\nif (status2 & 0x80) dev->stats.rx_dropped++;\r\n#ifdef YF_PROTOTYPE\r\n} else if ((yp->flags & HasMACAddrBug) &&\r\n!ether_addr_equal(le32_to_cpu(yp->rx_ring_dma +\r\nentry * yf_size),\r\ndev->dev_addr) &&\r\n!ether_addr_equal(le32_to_cpu(yp->rx_ring_dma +\r\nentry * yf_size),\r\n"\377\377\377\377\377\377")) {\r\nif (bogus_rx++ == 0)\r\nnetdev_warn(dev, "Bad frame to %pM\n",\r\nbuf_addr);\r\n#endif\r\n} else {\r\nstruct sk_buff *skb;\r\nint pkt_len = data_size -\r\n(yp->chip_id ? 7 : 8 + buf_addr[data_size - 8]);\r\n#ifndef final_version\r\nif (yellowfin_debug > 4)\r\nprintk(KERN_DEBUG " %s() normal Rx pkt length %d of %d, bogus_cnt %d\n",\r\n__func__, pkt_len, data_size, boguscnt);\r\n#endif\r\nif (pkt_len > rx_copybreak) {\r\nskb_put(skb = rx_skb, pkt_len);\r\npci_unmap_single(yp->pci_dev,\r\nle32_to_cpu(yp->rx_ring[entry].addr),\r\nyp->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE);\r\nyp->rx_skbuff[entry] = NULL;\r\n} else {\r\nskb = netdev_alloc_skb(dev, pkt_len + 2);\r\nif (skb == NULL)\r\nbreak;\r\nskb_reserve(skb, 2);\r\nskb_copy_to_linear_data(skb, rx_skb->data, pkt_len);\r\nskb_put(skb, pkt_len);\r\npci_dma_sync_single_for_device(yp->pci_dev,\r\nle32_to_cpu(desc->addr),\r\nyp->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE);\r\n}\r\nskb->protocol = eth_type_trans(skb, dev);\r\nnetif_rx(skb);\r\ndev->stats.rx_packets++;\r\ndev->stats.rx_bytes += pkt_len;\r\n}\r\nentry = (++yp->cur_rx) % RX_RING_SIZE;\r\n}\r\nfor (; yp->cur_rx - yp->dirty_rx > 0; yp->dirty_rx++) {\r\nentry = yp->dirty_rx % RX_RING_SIZE;\r\nif (yp->rx_skbuff[entry] == NULL) {\r\nstruct sk_buff *skb = netdev_alloc_skb(dev, yp->rx_buf_sz + 2);\r\nif (skb == NULL)\r\nbreak;\r\nyp->rx_skbuff[entry] = skb;\r\nskb_reserve(skb, 2);\r\nyp->rx_ring[entry].addr = cpu_to_le32(pci_map_single(yp->pci_dev,\r\nskb->data, yp->rx_buf_sz, PCI_DMA_FROMDEVICE));\r\n}\r\nyp->rx_ring[entry].dbdma_cmd = cpu_to_le32(CMD_STOP);\r\nyp->rx_ring[entry].result_status = 0;\r\nif (entry != 0)\r\nyp->rx_ring[entry - 1].dbdma_cmd =\r\ncpu_to_le32(CMD_RX_BUF | INTR_ALWAYS | yp->rx_buf_sz);\r\nelse\r\nyp->rx_ring[RX_RING_SIZE - 1].dbdma_cmd =\r\ncpu_to_le32(CMD_RX_BUF | INTR_ALWAYS | BRANCH_ALWAYS\r\n| yp->rx_buf_sz);\r\n}\r\nreturn 0;\r\n}\r\nstatic void yellowfin_error(struct net_device *dev, int intr_status)\r\n{\r\nnetdev_err(dev, "Something Wicked happened! %04x\n", intr_status);\r\nif (intr_status & (IntrTxPCIErr | IntrTxPCIFault))\r\ndev->stats.tx_errors++;\r\nif (intr_status & (IntrRxPCIErr | IntrRxPCIFault))\r\ndev->stats.rx_errors++;\r\n}\r\nstatic int yellowfin_close(struct net_device *dev)\r\n{\r\nstruct yellowfin_private *yp = netdev_priv(dev);\r\nvoid __iomem *ioaddr = yp->base;\r\nint i;\r\nnetif_stop_queue (dev);\r\nif (yellowfin_debug > 1) {\r\nnetdev_printk(KERN_DEBUG, dev, "Shutting down ethercard, status was Tx %04x Rx %04x Int %02x\n",\r\nioread16(ioaddr + TxStatus),\r\nioread16(ioaddr + RxStatus),\r\nioread16(ioaddr + IntrStatus));\r\nnetdev_printk(KERN_DEBUG, dev, "Queue pointers were Tx %d / %d, Rx %d / %d\n",\r\nyp->cur_tx, yp->dirty_tx,\r\nyp->cur_rx, yp->dirty_rx);\r\n}\r\niowrite16(0x0000, ioaddr + IntrEnb);\r\niowrite32(0x80000000, ioaddr + RxCtrl);\r\niowrite32(0x80000000, ioaddr + TxCtrl);\r\ndel_timer(&yp->timer);\r\n#if defined(__i386__)\r\nif (yellowfin_debug > 2) {\r\nprintk(KERN_DEBUG " Tx ring at %08llx:\n",\r\n(unsigned long long)yp->tx_ring_dma);\r\nfor (i = 0; i < TX_RING_SIZE*2; i++)\r\nprintk(KERN_DEBUG " %c #%d desc. %08x %08x %08x %08x\n",\r\nioread32(ioaddr + TxPtr) == (long)&yp->tx_ring[i] ? '>' : ' ',\r\ni, yp->tx_ring[i].dbdma_cmd, yp->tx_ring[i].addr,\r\nyp->tx_ring[i].branch_addr, yp->tx_ring[i].result_status);\r\nprintk(KERN_DEBUG " Tx status %p:\n", yp->tx_status);\r\nfor (i = 0; i < TX_RING_SIZE; i++)\r\nprintk(KERN_DEBUG " #%d status %04x %04x %04x %04x\n",\r\ni, yp->tx_status[i].tx_cnt, yp->tx_status[i].tx_errs,\r\nyp->tx_status[i].total_tx_cnt, yp->tx_status[i].paused);\r\nprintk(KERN_DEBUG " Rx ring %08llx:\n",\r\n(unsigned long long)yp->rx_ring_dma);\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nprintk(KERN_DEBUG " %c #%d desc. %08x %08x %08x\n",\r\nioread32(ioaddr + RxPtr) == (long)&yp->rx_ring[i] ? '>' : ' ',\r\ni, yp->rx_ring[i].dbdma_cmd, yp->rx_ring[i].addr,\r\nyp->rx_ring[i].result_status);\r\nif (yellowfin_debug > 6) {\r\nif (get_unaligned((u8*)yp->rx_ring[i].addr) != 0x69) {\r\nint j;\r\nprintk(KERN_DEBUG);\r\nfor (j = 0; j < 0x50; j++)\r\npr_cont(" %04x",\r\nget_unaligned(((u16*)yp->rx_ring[i].addr) + j));\r\npr_cont("\n");\r\n}\r\n}\r\n}\r\n}\r\n#endif\r\nfree_irq(yp->pci_dev->irq, dev);\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nyp->rx_ring[i].dbdma_cmd = cpu_to_le32(CMD_STOP);\r\nyp->rx_ring[i].addr = cpu_to_le32(0xBADF00D0);\r\nif (yp->rx_skbuff[i]) {\r\ndev_kfree_skb(yp->rx_skbuff[i]);\r\n}\r\nyp->rx_skbuff[i] = NULL;\r\n}\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nif (yp->tx_skbuff[i])\r\ndev_kfree_skb(yp->tx_skbuff[i]);\r\nyp->tx_skbuff[i] = NULL;\r\n}\r\n#ifdef YF_PROTOTYPE\r\nif (yellowfin_debug > 0) {\r\nnetdev_printk(KERN_DEBUG, dev, "Received %d frames that we should not have\n",\r\nbogus_rx);\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void set_rx_mode(struct net_device *dev)\r\n{\r\nstruct yellowfin_private *yp = netdev_priv(dev);\r\nvoid __iomem *ioaddr = yp->base;\r\nu16 cfg_value = ioread16(ioaddr + Cnfg);\r\niowrite16(cfg_value & ~0x1000, ioaddr + Cnfg);\r\nif (dev->flags & IFF_PROMISC) {\r\niowrite16(0x000F, ioaddr + AddrMode);\r\n} else if ((netdev_mc_count(dev) > 64) ||\r\n(dev->flags & IFF_ALLMULTI)) {\r\niowrite16(0x000B, ioaddr + AddrMode);\r\n} else if (!netdev_mc_empty(dev)) {\r\nstruct netdev_hw_addr *ha;\r\nu16 hash_table[4];\r\nint i;\r\nmemset(hash_table, 0, sizeof(hash_table));\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nunsigned int bit;\r\nif (yp->drv_flags & HasMulticastBug) {\r\nbit = (ether_crc_le(3, ha->addr) >> 3) & 0x3f;\r\nhash_table[bit >> 4] |= (1 << bit);\r\nbit = (ether_crc_le(4, ha->addr) >> 3) & 0x3f;\r\nhash_table[bit >> 4] |= (1 << bit);\r\nbit = (ether_crc_le(5, ha->addr) >> 3) & 0x3f;\r\nhash_table[bit >> 4] |= (1 << bit);\r\n}\r\nbit = (ether_crc_le(6, ha->addr) >> 3) & 0x3f;\r\nhash_table[bit >> 4] |= (1 << bit);\r\n}\r\nfor (i = 0; i < 4; i++)\r\niowrite16(hash_table[i], ioaddr + HashTbl + i*2);\r\niowrite16(0x0003, ioaddr + AddrMode);\r\n} else {\r\niowrite16(0x0001, ioaddr + AddrMode);\r\n}\r\niowrite16(cfg_value | 0x1000, ioaddr + Cnfg);\r\n}\r\nstatic void yellowfin_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\r\n{\r\nstruct yellowfin_private *np = netdev_priv(dev);\r\nstrlcpy(info->driver, DRV_NAME, sizeof(info->driver));\r\nstrlcpy(info->version, DRV_VERSION, sizeof(info->version));\r\nstrlcpy(info->bus_info, pci_name(np->pci_dev), sizeof(info->bus_info));\r\n}\r\nstatic int netdev_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct yellowfin_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->base;\r\nstruct mii_ioctl_data *data = if_mii(rq);\r\nswitch(cmd) {\r\ncase SIOCGMIIPHY:\r\ndata->phy_id = np->phys[0] & 0x1f;\r\ncase SIOCGMIIREG:\r\ndata->val_out = mdio_read(ioaddr, data->phy_id & 0x1f, data->reg_num & 0x1f);\r\nreturn 0;\r\ncase SIOCSMIIREG:\r\nif (data->phy_id == np->phys[0]) {\r\nu16 value = data->val_in;\r\nswitch (data->reg_num) {\r\ncase 0:\r\nnp->medialock = (value & 0x9000) ? 0 : 1;\r\nif (np->medialock)\r\nnp->full_duplex = (value & 0x0100) ? 1 : 0;\r\nbreak;\r\ncase 4: np->advertising = value; break;\r\n}\r\n}\r\nmdio_write(ioaddr, data->phy_id & 0x1f, data->reg_num & 0x1f, data->val_in);\r\nreturn 0;\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void yellowfin_remove_one(struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct yellowfin_private *np;\r\nBUG_ON(!dev);\r\nnp = netdev_priv(dev);\r\npci_free_consistent(pdev, STATUS_TOTAL_SIZE, np->tx_status,\r\nnp->tx_status_dma);\r\npci_free_consistent(pdev, RX_TOTAL_SIZE, np->rx_ring, np->rx_ring_dma);\r\npci_free_consistent(pdev, TX_TOTAL_SIZE, np->tx_ring, np->tx_ring_dma);\r\nunregister_netdev (dev);\r\npci_iounmap(pdev, np->base);\r\npci_release_regions (pdev);\r\nfree_netdev (dev);\r\n}\r\nstatic int __init yellowfin_init (void)\r\n{\r\n#ifdef MODULE\r\nprintk(version);\r\n#endif\r\nreturn pci_register_driver(&yellowfin_driver);\r\n}\r\nstatic void __exit yellowfin_cleanup (void)\r\n{\r\npci_unregister_driver (&yellowfin_driver);\r\n}
