int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)\r\n{\r\nreturn !!(v->arch.pending_exceptions) ||\r\nv->requests;\r\n}\r\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)\r\n{\r\nreturn 1;\r\n}\r\nint kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)\r\n{\r\nint r;\r\nWARN_ON(irqs_disabled());\r\nhard_irq_disable();\r\nwhile (true) {\r\nif (need_resched()) {\r\nlocal_irq_enable();\r\ncond_resched();\r\nhard_irq_disable();\r\ncontinue;\r\n}\r\nif (signal_pending(current)) {\r\nkvmppc_account_exit(vcpu, SIGNAL_EXITS);\r\nvcpu->run->exit_reason = KVM_EXIT_INTR;\r\nr = -EINTR;\r\nbreak;\r\n}\r\nvcpu->mode = IN_GUEST_MODE;\r\nsmp_mb();\r\nif (vcpu->requests) {\r\nlocal_irq_enable();\r\ntrace_kvm_check_requests(vcpu);\r\nr = kvmppc_core_check_requests(vcpu);\r\nhard_irq_disable();\r\nif (r > 0)\r\ncontinue;\r\nbreak;\r\n}\r\nif (kvmppc_core_prepare_to_enter(vcpu)) {\r\ncontinue;\r\n}\r\nkvm_guest_enter();\r\nreturn 1;\r\n}\r\nlocal_irq_enable();\r\nreturn r;\r\n}\r\nint kvmppc_kvm_pv(struct kvm_vcpu *vcpu)\r\n{\r\nint nr = kvmppc_get_gpr(vcpu, 11);\r\nint r;\r\nunsigned long __maybe_unused param1 = kvmppc_get_gpr(vcpu, 3);\r\nunsigned long __maybe_unused param2 = kvmppc_get_gpr(vcpu, 4);\r\nunsigned long __maybe_unused param3 = kvmppc_get_gpr(vcpu, 5);\r\nunsigned long __maybe_unused param4 = kvmppc_get_gpr(vcpu, 6);\r\nunsigned long r2 = 0;\r\nif (!(vcpu->arch.shared->msr & MSR_SF)) {\r\nparam1 &= 0xffffffff;\r\nparam2 &= 0xffffffff;\r\nparam3 &= 0xffffffff;\r\nparam4 &= 0xffffffff;\r\n}\r\nswitch (nr) {\r\ncase KVM_HCALL_TOKEN(KVM_HC_PPC_MAP_MAGIC_PAGE):\r\n{\r\nvcpu->arch.magic_page_pa = param1;\r\nvcpu->arch.magic_page_ea = param2;\r\nr2 = KVM_MAGIC_FEAT_SR | KVM_MAGIC_FEAT_MAS0_TO_SPRG7;\r\nr = EV_SUCCESS;\r\nbreak;\r\n}\r\ncase KVM_HCALL_TOKEN(KVM_HC_FEATURES):\r\nr = EV_SUCCESS;\r\n#if defined(CONFIG_PPC_BOOK3S) || defined(CONFIG_KVM_E500V2)\r\nr2 |= (1 << KVM_FEATURE_MAGIC_PAGE);\r\n#endif\r\nbreak;\r\ncase EV_HCALL_TOKEN(EV_IDLE):\r\nr = EV_SUCCESS;\r\nkvm_vcpu_block(vcpu);\r\nclear_bit(KVM_REQ_UNHALT, &vcpu->requests);\r\nbreak;\r\ndefault:\r\nr = EV_UNIMPLEMENTED;\r\nbreak;\r\n}\r\nkvmppc_set_gpr(vcpu, 4, r2);\r\nreturn r;\r\n}\r\nint kvmppc_sanity_check(struct kvm_vcpu *vcpu)\r\n{\r\nint r = false;\r\nif (!vcpu->arch.pvr)\r\ngoto out;\r\nif ((vcpu->arch.cpu_type != KVM_CPU_3S_64) && vcpu->arch.papr_enabled)\r\ngoto out;\r\nif (!vcpu->arch.papr_enabled && is_kvmppc_hv_enabled(vcpu->kvm))\r\ngoto out;\r\n#ifdef CONFIG_KVM_BOOKE_HV\r\nif (!cpu_has_feature(CPU_FTR_EMB_HV))\r\ngoto out;\r\n#endif\r\nr = true;\r\nout:\r\nvcpu->arch.sane = r;\r\nreturn r ? 0 : -EINVAL;\r\n}\r\nint kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er;\r\nint r;\r\ner = kvmppc_emulate_instruction(run, vcpu);\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nr = RESUME_GUEST_NV;\r\nbreak;\r\ncase EMULATE_DO_MMIO:\r\nrun->exit_reason = KVM_EXIT_MMIO;\r\nr = RESUME_HOST_NV;\r\nbreak;\r\ncase EMULATE_FAIL:\r\nprintk(KERN_EMERG "%s: emulation failed (%08x)\n", __func__,\r\nkvmppc_get_last_inst(vcpu));\r\nr = RESUME_HOST;\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\nr = RESUME_GUEST;\r\n}\r\nreturn r;\r\n}\r\nint kvm_arch_hardware_enable(void *garbage)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_hardware_disable(void *garbage)\r\n{\r\n}\r\nint kvm_arch_hardware_setup(void)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_hardware_unsetup(void)\r\n{\r\n}\r\nvoid kvm_arch_check_processor_compat(void *rtn)\r\n{\r\n*(int *)rtn = kvmppc_core_check_processor_compat();\r\n}\r\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type)\r\n{\r\nstruct kvmppc_ops *kvm_ops = NULL;\r\nif (type == 0) {\r\nif (kvmppc_hv_ops)\r\nkvm_ops = kvmppc_hv_ops;\r\nelse\r\nkvm_ops = kvmppc_pr_ops;\r\nif (!kvm_ops)\r\ngoto err_out;\r\n} else if (type == KVM_VM_PPC_HV) {\r\nif (!kvmppc_hv_ops)\r\ngoto err_out;\r\nkvm_ops = kvmppc_hv_ops;\r\n} else if (type == KVM_VM_PPC_PR) {\r\nif (!kvmppc_pr_ops)\r\ngoto err_out;\r\nkvm_ops = kvmppc_pr_ops;\r\n} else\r\ngoto err_out;\r\nif (kvm_ops->owner && !try_module_get(kvm_ops->owner))\r\nreturn -ENOENT;\r\nkvm->arch.kvm_ops = kvm_ops;\r\nreturn kvmppc_core_init_vm(kvm);\r\nerr_out:\r\nreturn -EINVAL;\r\n}\r\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\r\n{\r\nunsigned int i;\r\nstruct kvm_vcpu *vcpu;\r\nkvm_for_each_vcpu(i, vcpu, kvm)\r\nkvm_arch_vcpu_free(vcpu);\r\nmutex_lock(&kvm->lock);\r\nfor (i = 0; i < atomic_read(&kvm->online_vcpus); i++)\r\nkvm->vcpus[i] = NULL;\r\natomic_set(&kvm->online_vcpus, 0);\r\nkvmppc_core_destroy_vm(kvm);\r\nmutex_unlock(&kvm->lock);\r\nmodule_put(kvm->arch.kvm_ops->owner);\r\n}\r\nvoid kvm_arch_sync_events(struct kvm *kvm)\r\n{\r\n}\r\nint kvm_dev_ioctl_check_extension(long ext)\r\n{\r\nint r;\r\nint hv_enabled = kvmppc_hv_ops ? 1 : 0;\r\nswitch (ext) {\r\n#ifdef CONFIG_BOOKE\r\ncase KVM_CAP_PPC_BOOKE_SREGS:\r\ncase KVM_CAP_PPC_BOOKE_WATCHDOG:\r\ncase KVM_CAP_PPC_EPR:\r\n#else\r\ncase KVM_CAP_PPC_SEGSTATE:\r\ncase KVM_CAP_PPC_HIOR:\r\ncase KVM_CAP_PPC_PAPR:\r\n#endif\r\ncase KVM_CAP_PPC_UNSET_IRQ:\r\ncase KVM_CAP_PPC_IRQ_LEVEL:\r\ncase KVM_CAP_ENABLE_CAP:\r\ncase KVM_CAP_ONE_REG:\r\ncase KVM_CAP_IOEVENTFD:\r\ncase KVM_CAP_DEVICE_CTRL:\r\nr = 1;\r\nbreak;\r\ncase KVM_CAP_PPC_PAIRED_SINGLES:\r\ncase KVM_CAP_PPC_OSI:\r\ncase KVM_CAP_PPC_GET_PVINFO:\r\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\r\ncase KVM_CAP_SW_TLB:\r\n#endif\r\nr = !hv_enabled;\r\nbreak;\r\n#ifdef CONFIG_KVM_MMIO\r\ncase KVM_CAP_COALESCED_MMIO:\r\nr = KVM_COALESCED_MMIO_PAGE_OFFSET;\r\nbreak;\r\n#endif\r\n#ifdef CONFIG_KVM_MPIC\r\ncase KVM_CAP_IRQ_MPIC:\r\nr = 1;\r\nbreak;\r\n#endif\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\ncase KVM_CAP_SPAPR_TCE:\r\ncase KVM_CAP_PPC_ALLOC_HTAB:\r\ncase KVM_CAP_PPC_RTAS:\r\n#ifdef CONFIG_KVM_XICS\r\ncase KVM_CAP_IRQ_XICS:\r\n#endif\r\nr = 1;\r\nbreak;\r\n#endif\r\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\r\ncase KVM_CAP_PPC_SMT:\r\nif (hv_enabled)\r\nr = threads_per_core;\r\nelse\r\nr = 0;\r\nbreak;\r\ncase KVM_CAP_PPC_RMA:\r\nr = hv_enabled;\r\nif (r && cpu_has_feature(CPU_FTR_ARCH_201))\r\nr = 2;\r\nbreak;\r\n#endif\r\ncase KVM_CAP_SYNC_MMU:\r\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\r\nif (hv_enabled)\r\nr = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;\r\nelse\r\nr = 0;\r\n#elif defined(KVM_ARCH_WANT_MMU_NOTIFIER)\r\nr = 1;\r\n#else\r\nr = 0;\r\n#endif\r\nbreak;\r\n#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE\r\ncase KVM_CAP_PPC_HTAB_FD:\r\nr = hv_enabled;\r\nbreak;\r\n#endif\r\ncase KVM_CAP_NR_VCPUS:\r\nif (hv_enabled)\r\nr = num_present_cpus();\r\nelse\r\nr = num_online_cpus();\r\nbreak;\r\ncase KVM_CAP_MAX_VCPUS:\r\nr = KVM_MAX_VCPUS;\r\nbreak;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\ncase KVM_CAP_PPC_GET_SMMU_INFO:\r\nr = 1;\r\nbreak;\r\n#endif\r\ndefault:\r\nr = 0;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nlong kvm_arch_dev_ioctl(struct file *filp,\r\nunsigned int ioctl, unsigned long arg)\r\n{\r\nreturn -EINVAL;\r\n}\r\nvoid kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,\r\nstruct kvm_memory_slot *dont)\r\n{\r\nkvmppc_core_free_memslot(kvm, free, dont);\r\n}\r\nint kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,\r\nunsigned long npages)\r\n{\r\nreturn kvmppc_core_create_memslot(kvm, slot, npages);\r\n}\r\nvoid kvm_arch_memslots_updated(struct kvm *kvm)\r\n{\r\n}\r\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot,\r\nstruct kvm_userspace_memory_region *mem,\r\nenum kvm_mr_change change)\r\n{\r\nreturn kvmppc_core_prepare_memory_region(kvm, memslot, mem);\r\n}\r\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\r\nstruct kvm_userspace_memory_region *mem,\r\nconst struct kvm_memory_slot *old,\r\nenum kvm_mr_change change)\r\n{\r\nkvmppc_core_commit_memory_region(kvm, mem, old);\r\n}\r\nvoid kvm_arch_flush_shadow_all(struct kvm *kvm)\r\n{\r\n}\r\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\r\nstruct kvm_memory_slot *slot)\r\n{\r\nkvmppc_core_flush_memslot(kvm, slot);\r\n}\r\nstruct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nvcpu = kvmppc_core_vcpu_create(kvm, id);\r\nif (!IS_ERR(vcpu)) {\r\nvcpu->arch.wqp = &vcpu->wq;\r\nkvmppc_create_vcpu_debugfs(vcpu, id);\r\n}\r\nreturn vcpu;\r\n}\r\nint kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)\r\n{\r\nhrtimer_cancel(&vcpu->arch.dec_timer);\r\ntasklet_kill(&vcpu->arch.tasklet);\r\nkvmppc_remove_vcpu_debugfs(vcpu);\r\nswitch (vcpu->arch.irq_type) {\r\ncase KVMPPC_IRQ_MPIC:\r\nkvmppc_mpic_disconnect_vcpu(vcpu->arch.mpic, vcpu);\r\nbreak;\r\ncase KVMPPC_IRQ_XICS:\r\nkvmppc_xics_free_icp(vcpu);\r\nbreak;\r\n}\r\nkvmppc_core_vcpu_free(vcpu);\r\n}\r\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nkvm_arch_vcpu_free(vcpu);\r\n}\r\nint kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvmppc_core_pending_dec(vcpu);\r\n}\r\nenum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nvcpu = container_of(timer, struct kvm_vcpu, arch.dec_timer);\r\ntasklet_schedule(&vcpu->arch.tasklet);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nint kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)\r\n{\r\nint ret;\r\nhrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);\r\ntasklet_init(&vcpu->arch.tasklet, kvmppc_decrementer_func, (ulong)vcpu);\r\nvcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;\r\nvcpu->arch.dec_expires = ~(u64)0;\r\n#ifdef CONFIG_KVM_EXIT_TIMING\r\nmutex_init(&vcpu->arch.exit_timing_lock);\r\n#endif\r\nret = kvmppc_subarch_vcpu_init(vcpu);\r\nreturn ret;\r\n}\r\nvoid kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_mmu_destroy(vcpu);\r\nkvmppc_subarch_vcpu_uninit(vcpu);\r\n}\r\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\n#ifdef CONFIG_BOOKE\r\nmtspr(SPRN_VRSAVE, vcpu->arch.vrsave);\r\n#endif\r\nkvmppc_core_vcpu_load(vcpu, cpu);\r\n}\r\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_core_vcpu_put(vcpu);\r\n#ifdef CONFIG_BOOKE\r\nvcpu->arch.vrsave = mfspr(SPRN_VRSAVE);\r\n#endif\r\n}\r\nstatic void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,\r\nstruct kvm_run *run)\r\n{\r\nkvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, run->dcr.data);\r\n}\r\nstatic void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,\r\nstruct kvm_run *run)\r\n{\r\nu64 uninitialized_var(gpr);\r\nif (run->mmio.len > sizeof(gpr)) {\r\nprintk(KERN_ERR "bad MMIO length: %d\n", run->mmio.len);\r\nreturn;\r\n}\r\nif (vcpu->arch.mmio_is_bigendian) {\r\nswitch (run->mmio.len) {\r\ncase 8: gpr = *(u64 *)run->mmio.data; break;\r\ncase 4: gpr = *(u32 *)run->mmio.data; break;\r\ncase 2: gpr = *(u16 *)run->mmio.data; break;\r\ncase 1: gpr = *(u8 *)run->mmio.data; break;\r\n}\r\n} else {\r\nswitch (run->mmio.len) {\r\ncase 4: gpr = ld_le32((u32 *)run->mmio.data); break;\r\ncase 2: gpr = ld_le16((u16 *)run->mmio.data); break;\r\ncase 1: gpr = *(u8 *)run->mmio.data; break;\r\n}\r\n}\r\nif (vcpu->arch.mmio_sign_extend) {\r\nswitch (run->mmio.len) {\r\n#ifdef CONFIG_PPC64\r\ncase 4:\r\ngpr = (s64)(s32)gpr;\r\nbreak;\r\n#endif\r\ncase 2:\r\ngpr = (s64)(s16)gpr;\r\nbreak;\r\ncase 1:\r\ngpr = (s64)(s8)gpr;\r\nbreak;\r\n}\r\n}\r\nkvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);\r\nswitch (vcpu->arch.io_gpr & KVM_MMIO_REG_EXT_MASK) {\r\ncase KVM_MMIO_REG_GPR:\r\nkvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);\r\nbreak;\r\ncase KVM_MMIO_REG_FPR:\r\nVCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;\r\nbreak;\r\n#ifdef CONFIG_PPC_BOOK3S\r\ncase KVM_MMIO_REG_QPR:\r\nvcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;\r\nbreak;\r\ncase KVM_MMIO_REG_FQPR:\r\nVCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;\r\nvcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;\r\nbreak;\r\n#endif\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nint kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int rt, unsigned int bytes,\r\nint is_default_endian)\r\n{\r\nint idx, ret;\r\nint is_bigendian;\r\nif (kvmppc_need_byteswap(vcpu)) {\r\nis_bigendian = !is_default_endian;\r\n} else {\r\nis_bigendian = is_default_endian;\r\n}\r\nif (bytes > sizeof(run->mmio.data)) {\r\nprintk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\n}\r\nrun->mmio.phys_addr = vcpu->arch.paddr_accessed;\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 0;\r\nvcpu->arch.io_gpr = rt;\r\nvcpu->arch.mmio_is_bigendian = is_bigendian;\r\nvcpu->mmio_needed = 1;\r\nvcpu->mmio_is_write = 0;\r\nvcpu->arch.mmio_sign_extend = 0;\r\nidx = srcu_read_lock(&vcpu->kvm->srcu);\r\nret = kvm_io_bus_read(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,\r\nbytes, &run->mmio.data);\r\nsrcu_read_unlock(&vcpu->kvm->srcu, idx);\r\nif (!ret) {\r\nkvmppc_complete_mmio_load(vcpu, run);\r\nvcpu->mmio_needed = 0;\r\nreturn EMULATE_DONE;\r\n}\r\nreturn EMULATE_DO_MMIO;\r\n}\r\nint kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int rt, unsigned int bytes,\r\nint is_default_endian)\r\n{\r\nint r;\r\nvcpu->arch.mmio_sign_extend = 1;\r\nr = kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian);\r\nreturn r;\r\n}\r\nint kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nu64 val, unsigned int bytes, int is_default_endian)\r\n{\r\nvoid *data = run->mmio.data;\r\nint idx, ret;\r\nint is_bigendian;\r\nif (kvmppc_need_byteswap(vcpu)) {\r\nis_bigendian = !is_default_endian;\r\n} else {\r\nis_bigendian = is_default_endian;\r\n}\r\nif (bytes > sizeof(run->mmio.data)) {\r\nprintk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\n}\r\nrun->mmio.phys_addr = vcpu->arch.paddr_accessed;\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 1;\r\nvcpu->mmio_needed = 1;\r\nvcpu->mmio_is_write = 1;\r\nif (is_bigendian) {\r\nswitch (bytes) {\r\ncase 8: *(u64 *)data = val; break;\r\ncase 4: *(u32 *)data = val; break;\r\ncase 2: *(u16 *)data = val; break;\r\ncase 1: *(u8 *)data = val; break;\r\n}\r\n} else {\r\nswitch (bytes) {\r\ncase 4: st_le32(data, val); break;\r\ncase 2: st_le16(data, val); break;\r\ncase 1: *(u8 *)data = val; break;\r\n}\r\n}\r\nidx = srcu_read_lock(&vcpu->kvm->srcu);\r\nret = kvm_io_bus_write(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,\r\nbytes, &run->mmio.data);\r\nsrcu_read_unlock(&vcpu->kvm->srcu, idx);\r\nif (!ret) {\r\nvcpu->mmio_needed = 0;\r\nreturn EMULATE_DONE;\r\n}\r\nreturn EMULATE_DO_MMIO;\r\n}\r\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nint r;\r\nsigset_t sigsaved;\r\nif (vcpu->sigset_active)\r\nsigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);\r\nif (vcpu->mmio_needed) {\r\nif (!vcpu->mmio_is_write)\r\nkvmppc_complete_mmio_load(vcpu, run);\r\nvcpu->mmio_needed = 0;\r\n} else if (vcpu->arch.dcr_needed) {\r\nif (!vcpu->arch.dcr_is_write)\r\nkvmppc_complete_dcr_load(vcpu, run);\r\nvcpu->arch.dcr_needed = 0;\r\n} else if (vcpu->arch.osi_needed) {\r\nu64 *gprs = run->osi.gprs;\r\nint i;\r\nfor (i = 0; i < 32; i++)\r\nkvmppc_set_gpr(vcpu, i, gprs[i]);\r\nvcpu->arch.osi_needed = 0;\r\n} else if (vcpu->arch.hcall_needed) {\r\nint i;\r\nkvmppc_set_gpr(vcpu, 3, run->papr_hcall.ret);\r\nfor (i = 0; i < 9; ++i)\r\nkvmppc_set_gpr(vcpu, 4 + i, run->papr_hcall.args[i]);\r\nvcpu->arch.hcall_needed = 0;\r\n#ifdef CONFIG_BOOKE\r\n} else if (vcpu->arch.epr_needed) {\r\nkvmppc_set_epr(vcpu, run->epr.epr);\r\nvcpu->arch.epr_needed = 0;\r\n#endif\r\n}\r\nr = kvmppc_vcpu_run(run, vcpu);\r\nif (vcpu->sigset_active)\r\nsigprocmask(SIG_SETMASK, &sigsaved, NULL);\r\nreturn r;\r\n}\r\nint kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)\r\n{\r\nif (irq->irq == KVM_INTERRUPT_UNSET) {\r\nkvmppc_core_dequeue_external(vcpu);\r\nreturn 0;\r\n}\r\nkvmppc_core_queue_external(vcpu, irq);\r\nkvm_vcpu_kick(vcpu);\r\nreturn 0;\r\n}\r\nstatic int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,\r\nstruct kvm_enable_cap *cap)\r\n{\r\nint r;\r\nif (cap->flags)\r\nreturn -EINVAL;\r\nswitch (cap->cap) {\r\ncase KVM_CAP_PPC_OSI:\r\nr = 0;\r\nvcpu->arch.osi_enabled = true;\r\nbreak;\r\ncase KVM_CAP_PPC_PAPR:\r\nr = 0;\r\nvcpu->arch.papr_enabled = true;\r\nbreak;\r\ncase KVM_CAP_PPC_EPR:\r\nr = 0;\r\nif (cap->args[0])\r\nvcpu->arch.epr_flags |= KVMPPC_EPR_USER;\r\nelse\r\nvcpu->arch.epr_flags &= ~KVMPPC_EPR_USER;\r\nbreak;\r\n#ifdef CONFIG_BOOKE\r\ncase KVM_CAP_PPC_BOOKE_WATCHDOG:\r\nr = 0;\r\nvcpu->arch.watchdog_enabled = true;\r\nbreak;\r\n#endif\r\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\r\ncase KVM_CAP_SW_TLB: {\r\nstruct kvm_config_tlb cfg;\r\nvoid __user *user_ptr = (void __user *)(uintptr_t)cap->args[0];\r\nr = -EFAULT;\r\nif (copy_from_user(&cfg, user_ptr, sizeof(cfg)))\r\nbreak;\r\nr = kvm_vcpu_ioctl_config_tlb(vcpu, &cfg);\r\nbreak;\r\n}\r\n#endif\r\n#ifdef CONFIG_KVM_MPIC\r\ncase KVM_CAP_IRQ_MPIC: {\r\nstruct fd f;\r\nstruct kvm_device *dev;\r\nr = -EBADF;\r\nf = fdget(cap->args[0]);\r\nif (!f.file)\r\nbreak;\r\nr = -EPERM;\r\ndev = kvm_device_from_filp(f.file);\r\nif (dev)\r\nr = kvmppc_mpic_connect_vcpu(dev, vcpu, cap->args[1]);\r\nfdput(f);\r\nbreak;\r\n}\r\n#endif\r\n#ifdef CONFIG_KVM_XICS\r\ncase KVM_CAP_IRQ_XICS: {\r\nstruct fd f;\r\nstruct kvm_device *dev;\r\nr = -EBADF;\r\nf = fdget(cap->args[0]);\r\nif (!f.file)\r\nbreak;\r\nr = -EPERM;\r\ndev = kvm_device_from_filp(f.file);\r\nif (dev)\r\nr = kvmppc_xics_connect_vcpu(dev, vcpu, cap->args[1]);\r\nfdput(f);\r\nbreak;\r\n}\r\n#endif\r\ndefault:\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nif (!r)\r\nr = kvmppc_sanity_check(vcpu);\r\nreturn r;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\r\nstruct kvm_mp_state *mp_state)\r\n{\r\nreturn -EINVAL;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\r\nstruct kvm_mp_state *mp_state)\r\n{\r\nreturn -EINVAL;\r\n}\r\nlong kvm_arch_vcpu_ioctl(struct file *filp,\r\nunsigned int ioctl, unsigned long arg)\r\n{\r\nstruct kvm_vcpu *vcpu = filp->private_data;\r\nvoid __user *argp = (void __user *)arg;\r\nlong r;\r\nswitch (ioctl) {\r\ncase KVM_INTERRUPT: {\r\nstruct kvm_interrupt irq;\r\nr = -EFAULT;\r\nif (copy_from_user(&irq, argp, sizeof(irq)))\r\ngoto out;\r\nr = kvm_vcpu_ioctl_interrupt(vcpu, &irq);\r\ngoto out;\r\n}\r\ncase KVM_ENABLE_CAP:\r\n{\r\nstruct kvm_enable_cap cap;\r\nr = -EFAULT;\r\nif (copy_from_user(&cap, argp, sizeof(cap)))\r\ngoto out;\r\nr = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);\r\nbreak;\r\n}\r\ncase KVM_SET_ONE_REG:\r\ncase KVM_GET_ONE_REG:\r\n{\r\nstruct kvm_one_reg reg;\r\nr = -EFAULT;\r\nif (copy_from_user(&reg, argp, sizeof(reg)))\r\ngoto out;\r\nif (ioctl == KVM_SET_ONE_REG)\r\nr = kvm_vcpu_ioctl_set_one_reg(vcpu, &reg);\r\nelse\r\nr = kvm_vcpu_ioctl_get_one_reg(vcpu, &reg);\r\nbreak;\r\n}\r\n#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)\r\ncase KVM_DIRTY_TLB: {\r\nstruct kvm_dirty_tlb dirty;\r\nr = -EFAULT;\r\nif (copy_from_user(&dirty, argp, sizeof(dirty)))\r\ngoto out;\r\nr = kvm_vcpu_ioctl_dirty_tlb(vcpu, &dirty);\r\nbreak;\r\n}\r\n#endif\r\ndefault:\r\nr = -EINVAL;\r\n}\r\nout:\r\nreturn r;\r\n}\r\nint kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)\r\n{\r\nreturn VM_FAULT_SIGBUS;\r\n}\r\nstatic int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)\r\n{\r\nu32 inst_nop = 0x60000000;\r\n#ifdef CONFIG_KVM_BOOKE_HV\r\nu32 inst_sc1 = 0x44000022;\r\npvinfo->hcall[0] = inst_sc1;\r\npvinfo->hcall[1] = inst_nop;\r\npvinfo->hcall[2] = inst_nop;\r\npvinfo->hcall[3] = inst_nop;\r\n#else\r\nu32 inst_lis = 0x3c000000;\r\nu32 inst_ori = 0x60000000;\r\nu32 inst_sc = 0x44000002;\r\nu32 inst_imm_mask = 0xffff;\r\npvinfo->hcall[0] = inst_lis | ((KVM_SC_MAGIC_R0 >> 16) & inst_imm_mask);\r\npvinfo->hcall[1] = inst_ori | (KVM_SC_MAGIC_R0 & inst_imm_mask);\r\npvinfo->hcall[2] = inst_sc;\r\npvinfo->hcall[3] = inst_nop;\r\n#endif\r\npvinfo->flags = KVM_PPC_PVINFO_FLAGS_EV_IDLE;\r\nreturn 0;\r\n}\r\nint kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,\r\nbool line_status)\r\n{\r\nif (!irqchip_in_kernel(kvm))\r\nreturn -ENXIO;\r\nirq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,\r\nirq_event->irq, irq_event->level,\r\nline_status);\r\nreturn 0;\r\n}\r\nlong kvm_arch_vm_ioctl(struct file *filp,\r\nunsigned int ioctl, unsigned long arg)\r\n{\r\nstruct kvm *kvm __maybe_unused = filp->private_data;\r\nvoid __user *argp = (void __user *)arg;\r\nlong r;\r\nswitch (ioctl) {\r\ncase KVM_PPC_GET_PVINFO: {\r\nstruct kvm_ppc_pvinfo pvinfo;\r\nmemset(&pvinfo, 0, sizeof(pvinfo));\r\nr = kvm_vm_ioctl_get_pvinfo(&pvinfo);\r\nif (copy_to_user(argp, &pvinfo, sizeof(pvinfo))) {\r\nr = -EFAULT;\r\ngoto out;\r\n}\r\nbreak;\r\n}\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\ncase KVM_CREATE_SPAPR_TCE: {\r\nstruct kvm_create_spapr_tce create_tce;\r\nr = -EFAULT;\r\nif (copy_from_user(&create_tce, argp, sizeof(create_tce)))\r\ngoto out;\r\nr = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce);\r\ngoto out;\r\n}\r\ncase KVM_PPC_GET_SMMU_INFO: {\r\nstruct kvm_ppc_smmu_info info;\r\nstruct kvm *kvm = filp->private_data;\r\nmemset(&info, 0, sizeof(info));\r\nr = kvm->arch.kvm_ops->get_smmu_info(kvm, &info);\r\nif (r >= 0 && copy_to_user(argp, &info, sizeof(info)))\r\nr = -EFAULT;\r\nbreak;\r\n}\r\ncase KVM_PPC_RTAS_DEFINE_TOKEN: {\r\nstruct kvm *kvm = filp->private_data;\r\nr = kvm_vm_ioctl_rtas_define_token(kvm, argp);\r\nbreak;\r\n}\r\ndefault: {\r\nstruct kvm *kvm = filp->private_data;\r\nr = kvm->arch.kvm_ops->arch_vm_ioctl(filp, ioctl, arg);\r\n}\r\n#else\r\ndefault:\r\nr = -ENOTTY;\r\n#endif\r\n}\r\nout:\r\nreturn r;\r\n}\r\nlong kvmppc_alloc_lpid(void)\r\n{\r\nlong lpid;\r\ndo {\r\nlpid = find_first_zero_bit(lpid_inuse, KVMPPC_NR_LPIDS);\r\nif (lpid >= nr_lpids) {\r\npr_err("%s: No LPIDs free\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\n} while (test_and_set_bit(lpid, lpid_inuse));\r\nreturn lpid;\r\n}\r\nvoid kvmppc_claim_lpid(long lpid)\r\n{\r\nset_bit(lpid, lpid_inuse);\r\n}\r\nvoid kvmppc_free_lpid(long lpid)\r\n{\r\nclear_bit(lpid, lpid_inuse);\r\n}\r\nvoid kvmppc_init_lpid(unsigned long nr_lpids_param)\r\n{\r\nnr_lpids = min_t(unsigned long, KVMPPC_NR_LPIDS, nr_lpids_param);\r\nmemset(lpid_inuse, 0, sizeof(lpid_inuse));\r\n}\r\nint kvm_arch_init(void *opaque)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_exit(void)\r\n{\r\n}
