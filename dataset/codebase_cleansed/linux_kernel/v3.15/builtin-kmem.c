static int init_cpunode_map(void)\r\n{\r\nFILE *fp;\r\nint i, err = -1;\r\nfp = fopen("/sys/devices/system/cpu/kernel_max", "r");\r\nif (!fp) {\r\nmax_cpu_num = 4096;\r\nreturn 0;\r\n}\r\nif (fscanf(fp, "%d", &max_cpu_num) < 1) {\r\npr_err("Failed to read 'kernel_max' from sysfs");\r\ngoto out_close;\r\n}\r\nmax_cpu_num++;\r\ncpunode_map = calloc(max_cpu_num, sizeof(int));\r\nif (!cpunode_map) {\r\npr_err("%s: calloc failed\n", __func__);\r\ngoto out_close;\r\n}\r\nfor (i = 0; i < max_cpu_num; i++)\r\ncpunode_map[i] = -1;\r\nerr = 0;\r\nout_close:\r\nfclose(fp);\r\nreturn err;\r\n}\r\nstatic int setup_cpunode_map(void)\r\n{\r\nstruct dirent *dent1, *dent2;\r\nDIR *dir1, *dir2;\r\nunsigned int cpu, mem;\r\nchar buf[PATH_MAX];\r\nif (init_cpunode_map())\r\nreturn -1;\r\ndir1 = opendir(PATH_SYS_NODE);\r\nif (!dir1)\r\nreturn 0;\r\nwhile ((dent1 = readdir(dir1)) != NULL) {\r\nif (dent1->d_type != DT_DIR ||\r\nsscanf(dent1->d_name, "node%u", &mem) < 1)\r\ncontinue;\r\nsnprintf(buf, PATH_MAX, "%s/%s", PATH_SYS_NODE, dent1->d_name);\r\ndir2 = opendir(buf);\r\nif (!dir2)\r\ncontinue;\r\nwhile ((dent2 = readdir(dir2)) != NULL) {\r\nif (dent2->d_type != DT_LNK ||\r\nsscanf(dent2->d_name, "cpu%u", &cpu) < 1)\r\ncontinue;\r\ncpunode_map[cpu] = mem;\r\n}\r\nclosedir(dir2);\r\n}\r\nclosedir(dir1);\r\nreturn 0;\r\n}\r\nstatic int insert_alloc_stat(unsigned long call_site, unsigned long ptr,\r\nint bytes_req, int bytes_alloc, int cpu)\r\n{\r\nstruct rb_node **node = &root_alloc_stat.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct alloc_stat *data = NULL;\r\nwhile (*node) {\r\nparent = *node;\r\ndata = rb_entry(*node, struct alloc_stat, node);\r\nif (ptr > data->ptr)\r\nnode = &(*node)->rb_right;\r\nelse if (ptr < data->ptr)\r\nnode = &(*node)->rb_left;\r\nelse\r\nbreak;\r\n}\r\nif (data && data->ptr == ptr) {\r\ndata->hit++;\r\ndata->bytes_req += bytes_req;\r\ndata->bytes_alloc += bytes_alloc;\r\n} else {\r\ndata = malloc(sizeof(*data));\r\nif (!data) {\r\npr_err("%s: malloc failed\n", __func__);\r\nreturn -1;\r\n}\r\ndata->ptr = ptr;\r\ndata->pingpong = 0;\r\ndata->hit = 1;\r\ndata->bytes_req = bytes_req;\r\ndata->bytes_alloc = bytes_alloc;\r\nrb_link_node(&data->node, parent, node);\r\nrb_insert_color(&data->node, &root_alloc_stat);\r\n}\r\ndata->call_site = call_site;\r\ndata->alloc_cpu = cpu;\r\nreturn 0;\r\n}\r\nstatic int insert_caller_stat(unsigned long call_site,\r\nint bytes_req, int bytes_alloc)\r\n{\r\nstruct rb_node **node = &root_caller_stat.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct alloc_stat *data = NULL;\r\nwhile (*node) {\r\nparent = *node;\r\ndata = rb_entry(*node, struct alloc_stat, node);\r\nif (call_site > data->call_site)\r\nnode = &(*node)->rb_right;\r\nelse if (call_site < data->call_site)\r\nnode = &(*node)->rb_left;\r\nelse\r\nbreak;\r\n}\r\nif (data && data->call_site == call_site) {\r\ndata->hit++;\r\ndata->bytes_req += bytes_req;\r\ndata->bytes_alloc += bytes_alloc;\r\n} else {\r\ndata = malloc(sizeof(*data));\r\nif (!data) {\r\npr_err("%s: malloc failed\n", __func__);\r\nreturn -1;\r\n}\r\ndata->call_site = call_site;\r\ndata->pingpong = 0;\r\ndata->hit = 1;\r\ndata->bytes_req = bytes_req;\r\ndata->bytes_alloc = bytes_alloc;\r\nrb_link_node(&data->node, parent, node);\r\nrb_insert_color(&data->node, &root_caller_stat);\r\n}\r\nreturn 0;\r\n}\r\nstatic int perf_evsel__process_alloc_event(struct perf_evsel *evsel,\r\nstruct perf_sample *sample)\r\n{\r\nunsigned long ptr = perf_evsel__intval(evsel, sample, "ptr"),\r\ncall_site = perf_evsel__intval(evsel, sample, "call_site");\r\nint bytes_req = perf_evsel__intval(evsel, sample, "bytes_req"),\r\nbytes_alloc = perf_evsel__intval(evsel, sample, "bytes_alloc");\r\nif (insert_alloc_stat(call_site, ptr, bytes_req, bytes_alloc, sample->cpu) ||\r\ninsert_caller_stat(call_site, bytes_req, bytes_alloc))\r\nreturn -1;\r\ntotal_requested += bytes_req;\r\ntotal_allocated += bytes_alloc;\r\nnr_allocs++;\r\nreturn 0;\r\n}\r\nstatic int perf_evsel__process_alloc_node_event(struct perf_evsel *evsel,\r\nstruct perf_sample *sample)\r\n{\r\nint ret = perf_evsel__process_alloc_event(evsel, sample);\r\nif (!ret) {\r\nint node1 = cpunode_map[sample->cpu],\r\nnode2 = perf_evsel__intval(evsel, sample, "node");\r\nif (node1 != node2)\r\nnr_cross_allocs++;\r\n}\r\nreturn ret;\r\n}\r\nstatic struct alloc_stat *search_alloc_stat(unsigned long ptr,\r\nunsigned long call_site,\r\nstruct rb_root *root,\r\nsort_fn_t sort_fn)\r\n{\r\nstruct rb_node *node = root->rb_node;\r\nstruct alloc_stat key = { .ptr = ptr, .call_site = call_site };\r\nwhile (node) {\r\nstruct alloc_stat *data;\r\nint cmp;\r\ndata = rb_entry(node, struct alloc_stat, node);\r\ncmp = sort_fn(&key, data);\r\nif (cmp < 0)\r\nnode = node->rb_left;\r\nelse if (cmp > 0)\r\nnode = node->rb_right;\r\nelse\r\nreturn data;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int perf_evsel__process_free_event(struct perf_evsel *evsel,\r\nstruct perf_sample *sample)\r\n{\r\nunsigned long ptr = perf_evsel__intval(evsel, sample, "ptr");\r\nstruct alloc_stat *s_alloc, *s_caller;\r\ns_alloc = search_alloc_stat(ptr, 0, &root_alloc_stat, ptr_cmp);\r\nif (!s_alloc)\r\nreturn 0;\r\nif ((short)sample->cpu != s_alloc->alloc_cpu) {\r\ns_alloc->pingpong++;\r\ns_caller = search_alloc_stat(0, s_alloc->call_site,\r\n&root_caller_stat, callsite_cmp);\r\nif (!s_caller)\r\nreturn -1;\r\ns_caller->pingpong++;\r\n}\r\ns_alloc->alloc_cpu = -1;\r\nreturn 0;\r\n}\r\nstatic int process_sample_event(struct perf_tool *tool __maybe_unused,\r\nunion perf_event *event,\r\nstruct perf_sample *sample,\r\nstruct perf_evsel *evsel,\r\nstruct machine *machine)\r\n{\r\nstruct thread *thread = machine__findnew_thread(machine, sample->pid,\r\nsample->pid);\r\nif (thread == NULL) {\r\npr_debug("problem processing %d event, skipping it.\n",\r\nevent->header.type);\r\nreturn -1;\r\n}\r\ndump_printf(" ... thread: %s:%d\n", thread__comm_str(thread), thread->tid);\r\nif (evsel->handler != NULL) {\r\ntracepoint_handler f = evsel->handler;\r\nreturn f(evsel, sample);\r\n}\r\nreturn 0;\r\n}\r\nstatic double fragmentation(unsigned long n_req, unsigned long n_alloc)\r\n{\r\nif (n_alloc == 0)\r\nreturn 0.0;\r\nelse\r\nreturn 100.0 - (100.0 * n_req / n_alloc);\r\n}\r\nstatic void __print_result(struct rb_root *root, struct perf_session *session,\r\nint n_lines, int is_caller)\r\n{\r\nstruct rb_node *next;\r\nstruct machine *machine = &session->machines.host;\r\nprintf("%.102s\n", graph_dotted_line);\r\nprintf(" %-34s |", is_caller ? "Callsite": "Alloc Ptr");\r\nprintf(" Total_alloc/Per | Total_req/Per | Hit | Ping-pong | Frag\n");\r\nprintf("%.102s\n", graph_dotted_line);\r\nnext = rb_first(root);\r\nwhile (next && n_lines--) {\r\nstruct alloc_stat *data = rb_entry(next, struct alloc_stat,\r\nnode);\r\nstruct symbol *sym = NULL;\r\nstruct map *map;\r\nchar buf[BUFSIZ];\r\nu64 addr;\r\nif (is_caller) {\r\naddr = data->call_site;\r\nif (!raw_ip)\r\nsym = machine__find_kernel_function(machine, addr, &map, NULL);\r\n} else\r\naddr = data->ptr;\r\nif (sym != NULL)\r\nsnprintf(buf, sizeof(buf), "%s+%" PRIx64 "", sym->name,\r\naddr - map->unmap_ip(map, sym->start));\r\nelse\r\nsnprintf(buf, sizeof(buf), "%#" PRIx64 "", addr);\r\nprintf(" %-34s |", buf);\r\nprintf(" %9llu/%-5lu | %9llu/%-5lu | %8lu | %8lu | %6.3f%%\n",\r\n(unsigned long long)data->bytes_alloc,\r\n(unsigned long)data->bytes_alloc / data->hit,\r\n(unsigned long long)data->bytes_req,\r\n(unsigned long)data->bytes_req / data->hit,\r\n(unsigned long)data->hit,\r\n(unsigned long)data->pingpong,\r\nfragmentation(data->bytes_req, data->bytes_alloc));\r\nnext = rb_next(next);\r\n}\r\nif (n_lines == -1)\r\nprintf(" ... | ... | ... | ... | ... | ... \n");\r\nprintf("%.102s\n", graph_dotted_line);\r\n}\r\nstatic void print_summary(void)\r\n{\r\nprintf("\nSUMMARY\n=======\n");\r\nprintf("Total bytes requested: %lu\n", total_requested);\r\nprintf("Total bytes allocated: %lu\n", total_allocated);\r\nprintf("Total bytes wasted on internal fragmentation: %lu\n",\r\ntotal_allocated - total_requested);\r\nprintf("Internal fragmentation: %f%%\n",\r\nfragmentation(total_requested, total_allocated));\r\nprintf("Cross CPU allocations: %lu/%lu\n", nr_cross_allocs, nr_allocs);\r\n}\r\nstatic void print_result(struct perf_session *session)\r\n{\r\nif (caller_flag)\r\n__print_result(&root_caller_sorted, session, caller_lines, 1);\r\nif (alloc_flag)\r\n__print_result(&root_alloc_sorted, session, alloc_lines, 0);\r\nprint_summary();\r\n}\r\nstatic void sort_insert(struct rb_root *root, struct alloc_stat *data,\r\nstruct list_head *sort_list)\r\n{\r\nstruct rb_node **new = &(root->rb_node);\r\nstruct rb_node *parent = NULL;\r\nstruct sort_dimension *sort;\r\nwhile (*new) {\r\nstruct alloc_stat *this;\r\nint cmp = 0;\r\nthis = rb_entry(*new, struct alloc_stat, node);\r\nparent = *new;\r\nlist_for_each_entry(sort, sort_list, list) {\r\ncmp = sort->cmp(data, this);\r\nif (cmp)\r\nbreak;\r\n}\r\nif (cmp > 0)\r\nnew = &((*new)->rb_left);\r\nelse\r\nnew = &((*new)->rb_right);\r\n}\r\nrb_link_node(&data->node, parent, new);\r\nrb_insert_color(&data->node, root);\r\n}\r\nstatic void __sort_result(struct rb_root *root, struct rb_root *root_sorted,\r\nstruct list_head *sort_list)\r\n{\r\nstruct rb_node *node;\r\nstruct alloc_stat *data;\r\nfor (;;) {\r\nnode = rb_first(root);\r\nif (!node)\r\nbreak;\r\nrb_erase(node, root);\r\ndata = rb_entry(node, struct alloc_stat, node);\r\nsort_insert(root_sorted, data, sort_list);\r\n}\r\n}\r\nstatic void sort_result(void)\r\n{\r\n__sort_result(&root_alloc_stat, &root_alloc_sorted, &alloc_sort);\r\n__sort_result(&root_caller_stat, &root_caller_sorted, &caller_sort);\r\n}\r\nstatic int __cmd_kmem(void)\r\n{\r\nint err = -EINVAL;\r\nstruct perf_session *session;\r\nconst struct perf_evsel_str_handler kmem_tracepoints[] = {\r\n{ "kmem:kmalloc", perf_evsel__process_alloc_event, },\r\n{ "kmem:kmem_cache_alloc", perf_evsel__process_alloc_event, },\r\n{ "kmem:kmalloc_node", perf_evsel__process_alloc_node_event, },\r\n{ "kmem:kmem_cache_alloc_node", perf_evsel__process_alloc_node_event, },\r\n{ "kmem:kfree", perf_evsel__process_free_event, },\r\n{ "kmem:kmem_cache_free", perf_evsel__process_free_event, },\r\n};\r\nstruct perf_data_file file = {\r\n.path = input_name,\r\n.mode = PERF_DATA_MODE_READ,\r\n};\r\nsession = perf_session__new(&file, false, &perf_kmem);\r\nif (session == NULL)\r\nreturn -ENOMEM;\r\nif (perf_session__create_kernel_maps(session) < 0)\r\ngoto out_delete;\r\nif (!perf_session__has_traces(session, "kmem record"))\r\ngoto out_delete;\r\nif (perf_session__set_tracepoints_handlers(session, kmem_tracepoints)) {\r\npr_err("Initializing perf session tracepoint handlers failed\n");\r\nreturn -1;\r\n}\r\nsetup_pager();\r\nerr = perf_session__process_events(session, &perf_kmem);\r\nif (err != 0)\r\ngoto out_delete;\r\nsort_result();\r\nprint_result(session);\r\nout_delete:\r\nperf_session__delete(session);\r\nreturn err;\r\n}\r\nstatic int ptr_cmp(struct alloc_stat *l, struct alloc_stat *r)\r\n{\r\nif (l->ptr < r->ptr)\r\nreturn -1;\r\nelse if (l->ptr > r->ptr)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int callsite_cmp(struct alloc_stat *l, struct alloc_stat *r)\r\n{\r\nif (l->call_site < r->call_site)\r\nreturn -1;\r\nelse if (l->call_site > r->call_site)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int hit_cmp(struct alloc_stat *l, struct alloc_stat *r)\r\n{\r\nif (l->hit < r->hit)\r\nreturn -1;\r\nelse if (l->hit > r->hit)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int bytes_cmp(struct alloc_stat *l, struct alloc_stat *r)\r\n{\r\nif (l->bytes_alloc < r->bytes_alloc)\r\nreturn -1;\r\nelse if (l->bytes_alloc > r->bytes_alloc)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int frag_cmp(struct alloc_stat *l, struct alloc_stat *r)\r\n{\r\ndouble x, y;\r\nx = fragmentation(l->bytes_req, l->bytes_alloc);\r\ny = fragmentation(r->bytes_req, r->bytes_alloc);\r\nif (x < y)\r\nreturn -1;\r\nelse if (x > y)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int pingpong_cmp(struct alloc_stat *l, struct alloc_stat *r)\r\n{\r\nif (l->pingpong < r->pingpong)\r\nreturn -1;\r\nelse if (l->pingpong > r->pingpong)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int sort_dimension__add(const char *tok, struct list_head *list)\r\n{\r\nstruct sort_dimension *sort;\r\nint i;\r\nfor (i = 0; i < NUM_AVAIL_SORTS; i++) {\r\nif (!strcmp(avail_sorts[i]->name, tok)) {\r\nsort = memdup(avail_sorts[i], sizeof(*avail_sorts[i]));\r\nif (!sort) {\r\npr_err("%s: memdup failed\n", __func__);\r\nreturn -1;\r\n}\r\nlist_add_tail(&sort->list, list);\r\nreturn 0;\r\n}\r\n}\r\nreturn -1;\r\n}\r\nstatic int setup_sorting(struct list_head *sort_list, const char *arg)\r\n{\r\nchar *tok;\r\nchar *str = strdup(arg);\r\nif (!str) {\r\npr_err("%s: strdup failed\n", __func__);\r\nreturn -1;\r\n}\r\nwhile (true) {\r\ntok = strsep(&str, ",");\r\nif (!tok)\r\nbreak;\r\nif (sort_dimension__add(tok, sort_list) < 0) {\r\nerror("Unknown --sort key: '%s'", tok);\r\nfree(str);\r\nreturn -1;\r\n}\r\n}\r\nfree(str);\r\nreturn 0;\r\n}\r\nstatic int parse_sort_opt(const struct option *opt __maybe_unused,\r\nconst char *arg, int unset __maybe_unused)\r\n{\r\nif (!arg)\r\nreturn -1;\r\nif (caller_flag > alloc_flag)\r\nreturn setup_sorting(&caller_sort, arg);\r\nelse\r\nreturn setup_sorting(&alloc_sort, arg);\r\nreturn 0;\r\n}\r\nstatic int parse_caller_opt(const struct option *opt __maybe_unused,\r\nconst char *arg __maybe_unused,\r\nint unset __maybe_unused)\r\n{\r\ncaller_flag = (alloc_flag + 1);\r\nreturn 0;\r\n}\r\nstatic int parse_alloc_opt(const struct option *opt __maybe_unused,\r\nconst char *arg __maybe_unused,\r\nint unset __maybe_unused)\r\n{\r\nalloc_flag = (caller_flag + 1);\r\nreturn 0;\r\n}\r\nstatic int parse_line_opt(const struct option *opt __maybe_unused,\r\nconst char *arg, int unset __maybe_unused)\r\n{\r\nint lines;\r\nif (!arg)\r\nreturn -1;\r\nlines = strtoul(arg, NULL, 10);\r\nif (caller_flag > alloc_flag)\r\ncaller_lines = lines;\r\nelse\r\nalloc_lines = lines;\r\nreturn 0;\r\n}\r\nstatic int __cmd_record(int argc, const char **argv)\r\n{\r\nconst char * const record_args[] = {\r\n"record", "-a", "-R", "-c", "1",\r\n"-e", "kmem:kmalloc",\r\n"-e", "kmem:kmalloc_node",\r\n"-e", "kmem:kfree",\r\n"-e", "kmem:kmem_cache_alloc",\r\n"-e", "kmem:kmem_cache_alloc_node",\r\n"-e", "kmem:kmem_cache_free",\r\n};\r\nunsigned int rec_argc, i, j;\r\nconst char **rec_argv;\r\nrec_argc = ARRAY_SIZE(record_args) + argc - 1;\r\nrec_argv = calloc(rec_argc + 1, sizeof(char *));\r\nif (rec_argv == NULL)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < ARRAY_SIZE(record_args); i++)\r\nrec_argv[i] = strdup(record_args[i]);\r\nfor (j = 1; j < (unsigned int)argc; j++, i++)\r\nrec_argv[i] = argv[j];\r\nreturn cmd_record(i, rec_argv, NULL);\r\n}\r\nint cmd_kmem(int argc, const char **argv, const char *prefix __maybe_unused)\r\n{\r\nconst char * const default_sort_order = "frag,hit,bytes";\r\nconst struct option kmem_options[] = {\r\nOPT_STRING('i', "input", &input_name, "file", "input file name"),\r\nOPT_CALLBACK_NOOPT(0, "caller", NULL, NULL,\r\n"show per-callsite statistics", parse_caller_opt),\r\nOPT_CALLBACK_NOOPT(0, "alloc", NULL, NULL,\r\n"show per-allocation statistics", parse_alloc_opt),\r\nOPT_CALLBACK('s', "sort", NULL, "key[,key2...]",\r\n"sort by keys: ptr, call_site, bytes, hit, pingpong, frag",\r\nparse_sort_opt),\r\nOPT_CALLBACK('l', "line", NULL, "num", "show n lines", parse_line_opt),\r\nOPT_BOOLEAN(0, "raw-ip", &raw_ip, "show raw ip instead of symbol"),\r\nOPT_END()\r\n};\r\nconst char * const kmem_usage[] = {\r\n"perf kmem [<options>] {record|stat}",\r\nNULL\r\n};\r\nargc = parse_options(argc, argv, kmem_options, kmem_usage, 0);\r\nif (!argc)\r\nusage_with_options(kmem_usage, kmem_options);\r\nsymbol__init();\r\nif (!strncmp(argv[0], "rec", 3)) {\r\nreturn __cmd_record(argc, argv);\r\n} else if (!strcmp(argv[0], "stat")) {\r\nif (setup_cpunode_map())\r\nreturn -1;\r\nif (list_empty(&caller_sort))\r\nsetup_sorting(&caller_sort, default_sort_order);\r\nif (list_empty(&alloc_sort))\r\nsetup_sorting(&alloc_sort, default_sort_order);\r\nreturn __cmd_kmem();\r\n} else\r\nusage_with_options(kmem_usage, kmem_options);\r\nreturn 0;\r\n}
