static int __init setcoherentio(char *str)\r\n{\r\ncoherentio = 1;\r\npr_info("Hardware DMA cache coherency (command line)\n");\r\nreturn 0;\r\n}\r\nstatic int __init setnocoherentio(char *str)\r\n{\r\ncoherentio = 0;\r\npr_info("Software DMA cache coherency (command line)\n");\r\nreturn 0;\r\n}\r\nstatic inline struct page *dma_addr_to_page(struct device *dev,\r\ndma_addr_t dma_addr)\r\n{\r\nreturn pfn_to_page(\r\nplat_dma_addr_to_phys(dev, dma_addr) >> PAGE_SHIFT);\r\n}\r\nstatic inline int cpu_needs_post_dma_flush(struct device *dev)\r\n{\r\nreturn !plat_device_is_coherent(dev) &&\r\n(boot_cpu_type() == CPU_R10000 ||\r\nboot_cpu_type() == CPU_R12000 ||\r\nboot_cpu_type() == CPU_BMIPS5000);\r\n}\r\nstatic gfp_t massage_gfp_flags(const struct device *dev, gfp_t gfp)\r\n{\r\ngfp_t dma_flag;\r\ngfp &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);\r\n#ifdef CONFIG_ISA\r\nif (dev == NULL)\r\ndma_flag = __GFP_DMA;\r\nelse\r\n#endif\r\n#if defined(CONFIG_ZONE_DMA32) && defined(CONFIG_ZONE_DMA)\r\nif (dev->coherent_dma_mask < DMA_BIT_MASK(32))\r\ndma_flag = __GFP_DMA;\r\nelse if (dev->coherent_dma_mask < DMA_BIT_MASK(64))\r\ndma_flag = __GFP_DMA32;\r\nelse\r\n#endif\r\n#if defined(CONFIG_ZONE_DMA32) && !defined(CONFIG_ZONE_DMA)\r\nif (dev->coherent_dma_mask < DMA_BIT_MASK(64))\r\ndma_flag = __GFP_DMA32;\r\nelse\r\n#endif\r\n#if defined(CONFIG_ZONE_DMA) && !defined(CONFIG_ZONE_DMA32)\r\nif (dev->coherent_dma_mask < DMA_BIT_MASK(64))\r\ndma_flag = __GFP_DMA;\r\nelse\r\n#endif\r\ndma_flag = 0;\r\ngfp |= __GFP_NORETRY;\r\nreturn gfp | dma_flag;\r\n}\r\nvoid *dma_alloc_noncoherent(struct device *dev, size_t size,\r\ndma_addr_t * dma_handle, gfp_t gfp)\r\n{\r\nvoid *ret;\r\ngfp = massage_gfp_flags(dev, gfp);\r\nret = (void *) __get_free_pages(gfp, get_order(size));\r\nif (ret != NULL) {\r\nmemset(ret, 0, size);\r\n*dma_handle = plat_map_dma_mem(dev, ret, size);\r\n}\r\nreturn ret;\r\n}\r\nstatic void *mips_dma_alloc_coherent(struct device *dev, size_t size,\r\ndma_addr_t * dma_handle, gfp_t gfp, struct dma_attrs *attrs)\r\n{\r\nvoid *ret;\r\nif (dma_alloc_from_coherent(dev, size, dma_handle, &ret))\r\nreturn ret;\r\ngfp = massage_gfp_flags(dev, gfp);\r\nret = (void *) __get_free_pages(gfp, get_order(size));\r\nif (ret) {\r\nmemset(ret, 0, size);\r\n*dma_handle = plat_map_dma_mem(dev, ret, size);\r\nif (!plat_device_is_coherent(dev)) {\r\ndma_cache_wback_inv((unsigned long) ret, size);\r\nif (!hw_coherentio)\r\nret = UNCAC_ADDR(ret);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nvoid dma_free_noncoherent(struct device *dev, size_t size, void *vaddr,\r\ndma_addr_t dma_handle)\r\n{\r\nplat_unmap_dma_mem(dev, dma_handle, size, DMA_BIDIRECTIONAL);\r\nfree_pages((unsigned long) vaddr, get_order(size));\r\n}\r\nstatic void mips_dma_free_coherent(struct device *dev, size_t size, void *vaddr,\r\ndma_addr_t dma_handle, struct dma_attrs *attrs)\r\n{\r\nunsigned long addr = (unsigned long) vaddr;\r\nint order = get_order(size);\r\nif (dma_release_from_coherent(dev, order, vaddr))\r\nreturn;\r\nplat_unmap_dma_mem(dev, dma_handle, size, DMA_BIDIRECTIONAL);\r\nif (!plat_device_is_coherent(dev) && !hw_coherentio)\r\naddr = CAC_ADDR(addr);\r\nfree_pages(addr, get_order(size));\r\n}\r\nstatic inline void __dma_sync_virtual(void *addr, size_t size,\r\nenum dma_data_direction direction)\r\n{\r\nswitch (direction) {\r\ncase DMA_TO_DEVICE:\r\ndma_cache_wback((unsigned long)addr, size);\r\nbreak;\r\ncase DMA_FROM_DEVICE:\r\ndma_cache_inv((unsigned long)addr, size);\r\nbreak;\r\ncase DMA_BIDIRECTIONAL:\r\ndma_cache_wback_inv((unsigned long)addr, size);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic inline void __dma_sync(struct page *page,\r\nunsigned long offset, size_t size, enum dma_data_direction direction)\r\n{\r\nsize_t left = size;\r\ndo {\r\nsize_t len = left;\r\nif (PageHighMem(page)) {\r\nvoid *addr;\r\nif (offset + len > PAGE_SIZE) {\r\nif (offset >= PAGE_SIZE) {\r\npage += offset >> PAGE_SHIFT;\r\noffset &= ~PAGE_MASK;\r\n}\r\nlen = PAGE_SIZE - offset;\r\n}\r\naddr = kmap_atomic(page);\r\n__dma_sync_virtual(addr + offset, len, direction);\r\nkunmap_atomic(addr);\r\n} else\r\n__dma_sync_virtual(page_address(page) + offset,\r\nsize, direction);\r\noffset = 0;\r\npage++;\r\nleft -= len;\r\n} while (left);\r\n}\r\nstatic void mips_dma_unmap_page(struct device *dev, dma_addr_t dma_addr,\r\nsize_t size, enum dma_data_direction direction, struct dma_attrs *attrs)\r\n{\r\nif (cpu_needs_post_dma_flush(dev))\r\n__dma_sync(dma_addr_to_page(dev, dma_addr),\r\ndma_addr & ~PAGE_MASK, size, direction);\r\nplat_unmap_dma_mem(dev, dma_addr, size, direction);\r\n}\r\nstatic int mips_dma_map_sg(struct device *dev, struct scatterlist *sg,\r\nint nents, enum dma_data_direction direction, struct dma_attrs *attrs)\r\n{\r\nint i;\r\nfor (i = 0; i < nents; i++, sg++) {\r\nif (!plat_device_is_coherent(dev))\r\n__dma_sync(sg_page(sg), sg->offset, sg->length,\r\ndirection);\r\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\r\nsg->dma_length = sg->length;\r\n#endif\r\nsg->dma_address = plat_map_dma_mem_page(dev, sg_page(sg)) +\r\nsg->offset;\r\n}\r\nreturn nents;\r\n}\r\nstatic dma_addr_t mips_dma_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size, enum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nif (!plat_device_is_coherent(dev))\r\n__dma_sync(page, offset, size, direction);\r\nreturn plat_map_dma_mem_page(dev, page) + offset;\r\n}\r\nstatic void mips_dma_unmap_sg(struct device *dev, struct scatterlist *sg,\r\nint nhwentries, enum dma_data_direction direction,\r\nstruct dma_attrs *attrs)\r\n{\r\nint i;\r\nfor (i = 0; i < nhwentries; i++, sg++) {\r\nif (!plat_device_is_coherent(dev) &&\r\ndirection != DMA_TO_DEVICE)\r\n__dma_sync(sg_page(sg), sg->offset, sg->length,\r\ndirection);\r\nplat_unmap_dma_mem(dev, sg->dma_address, sg->length, direction);\r\n}\r\n}\r\nstatic void mips_dma_sync_single_for_cpu(struct device *dev,\r\ndma_addr_t dma_handle, size_t size, enum dma_data_direction direction)\r\n{\r\nif (cpu_needs_post_dma_flush(dev))\r\n__dma_sync(dma_addr_to_page(dev, dma_handle),\r\ndma_handle & ~PAGE_MASK, size, direction);\r\n}\r\nstatic void mips_dma_sync_single_for_device(struct device *dev,\r\ndma_addr_t dma_handle, size_t size, enum dma_data_direction direction)\r\n{\r\nif (!plat_device_is_coherent(dev))\r\n__dma_sync(dma_addr_to_page(dev, dma_handle),\r\ndma_handle & ~PAGE_MASK, size, direction);\r\n}\r\nstatic void mips_dma_sync_sg_for_cpu(struct device *dev,\r\nstruct scatterlist *sg, int nelems, enum dma_data_direction direction)\r\n{\r\nint i;\r\nif (cpu_needs_post_dma_flush(dev))\r\nfor (i = 0; i < nelems; i++, sg++)\r\n__dma_sync(sg_page(sg), sg->offset, sg->length,\r\ndirection);\r\n}\r\nstatic void mips_dma_sync_sg_for_device(struct device *dev,\r\nstruct scatterlist *sg, int nelems, enum dma_data_direction direction)\r\n{\r\nint i;\r\nif (!plat_device_is_coherent(dev))\r\nfor (i = 0; i < nelems; i++, sg++)\r\n__dma_sync(sg_page(sg), sg->offset, sg->length,\r\ndirection);\r\n}\r\nint mips_dma_mapping_error(struct device *dev, dma_addr_t dma_addr)\r\n{\r\nreturn 0;\r\n}\r\nint mips_dma_supported(struct device *dev, u64 mask)\r\n{\r\nreturn plat_dma_supported(dev, mask);\r\n}\r\nvoid dma_cache_sync(struct device *dev, void *vaddr, size_t size,\r\nenum dma_data_direction direction)\r\n{\r\nBUG_ON(direction == DMA_NONE);\r\nif (!plat_device_is_coherent(dev))\r\n__dma_sync_virtual(vaddr, size, direction);\r\n}\r\nstatic int __init mips_dma_init(void)\r\n{\r\ndma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);\r\nreturn 0;\r\n}
