static int cls_bpf_classify(struct sk_buff *skb, const struct tcf_proto *tp,\r\nstruct tcf_result *res)\r\n{\r\nstruct cls_bpf_head *head = tp->root;\r\nstruct cls_bpf_prog *prog;\r\nint ret;\r\nlist_for_each_entry(prog, &head->plist, link) {\r\nint filter_res = SK_RUN_FILTER(prog->filter, skb);\r\nif (filter_res == 0)\r\ncontinue;\r\n*res = prog->res;\r\nif (filter_res != -1)\r\nres->classid = filter_res;\r\nret = tcf_exts_exec(skb, &prog->exts, res);\r\nif (ret < 0)\r\ncontinue;\r\nreturn ret;\r\n}\r\nreturn -1;\r\n}\r\nstatic int cls_bpf_init(struct tcf_proto *tp)\r\n{\r\nstruct cls_bpf_head *head;\r\nhead = kzalloc(sizeof(*head), GFP_KERNEL);\r\nif (head == NULL)\r\nreturn -ENOBUFS;\r\nINIT_LIST_HEAD(&head->plist);\r\ntp->root = head;\r\nreturn 0;\r\n}\r\nstatic void cls_bpf_delete_prog(struct tcf_proto *tp, struct cls_bpf_prog *prog)\r\n{\r\ntcf_unbind_filter(tp, &prog->res);\r\ntcf_exts_destroy(tp, &prog->exts);\r\nsk_unattached_filter_destroy(prog->filter);\r\nkfree(prog->bpf_ops);\r\nkfree(prog);\r\n}\r\nstatic int cls_bpf_delete(struct tcf_proto *tp, unsigned long arg)\r\n{\r\nstruct cls_bpf_head *head = tp->root;\r\nstruct cls_bpf_prog *prog, *todel = (struct cls_bpf_prog *) arg;\r\nlist_for_each_entry(prog, &head->plist, link) {\r\nif (prog == todel) {\r\ntcf_tree_lock(tp);\r\nlist_del(&prog->link);\r\ntcf_tree_unlock(tp);\r\ncls_bpf_delete_prog(tp, prog);\r\nreturn 0;\r\n}\r\n}\r\nreturn -ENOENT;\r\n}\r\nstatic void cls_bpf_destroy(struct tcf_proto *tp)\r\n{\r\nstruct cls_bpf_head *head = tp->root;\r\nstruct cls_bpf_prog *prog, *tmp;\r\nlist_for_each_entry_safe(prog, tmp, &head->plist, link) {\r\nlist_del(&prog->link);\r\ncls_bpf_delete_prog(tp, prog);\r\n}\r\nkfree(head);\r\n}\r\nstatic unsigned long cls_bpf_get(struct tcf_proto *tp, u32 handle)\r\n{\r\nstruct cls_bpf_head *head = tp->root;\r\nstruct cls_bpf_prog *prog;\r\nunsigned long ret = 0UL;\r\nif (head == NULL)\r\nreturn 0UL;\r\nlist_for_each_entry(prog, &head->plist, link) {\r\nif (prog->handle == handle) {\r\nret = (unsigned long) prog;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void cls_bpf_put(struct tcf_proto *tp, unsigned long f)\r\n{\r\n}\r\nstatic int cls_bpf_modify_existing(struct net *net, struct tcf_proto *tp,\r\nstruct cls_bpf_prog *prog,\r\nunsigned long base, struct nlattr **tb,\r\nstruct nlattr *est)\r\n{\r\nstruct sock_filter *bpf_ops, *bpf_old;\r\nstruct tcf_exts exts;\r\nstruct sock_fprog tmp;\r\nstruct sk_filter *fp, *fp_old;\r\nu16 bpf_size, bpf_len;\r\nu32 classid;\r\nint ret;\r\nif (!tb[TCA_BPF_OPS_LEN] || !tb[TCA_BPF_OPS] || !tb[TCA_BPF_CLASSID])\r\nreturn -EINVAL;\r\ntcf_exts_init(&exts, TCA_BPF_ACT, TCA_BPF_POLICE);\r\nret = tcf_exts_validate(net, tp, tb, est, &exts);\r\nif (ret < 0)\r\nreturn ret;\r\nclassid = nla_get_u32(tb[TCA_BPF_CLASSID]);\r\nbpf_len = nla_get_u16(tb[TCA_BPF_OPS_LEN]);\r\nif (bpf_len > BPF_MAXINSNS || bpf_len == 0) {\r\nret = -EINVAL;\r\ngoto errout;\r\n}\r\nbpf_size = bpf_len * sizeof(*bpf_ops);\r\nbpf_ops = kzalloc(bpf_size, GFP_KERNEL);\r\nif (bpf_ops == NULL) {\r\nret = -ENOMEM;\r\ngoto errout;\r\n}\r\nmemcpy(bpf_ops, nla_data(tb[TCA_BPF_OPS]), bpf_size);\r\ntmp.len = bpf_len;\r\ntmp.filter = (struct sock_filter __user *) bpf_ops;\r\nret = sk_unattached_filter_create(&fp, &tmp);\r\nif (ret)\r\ngoto errout_free;\r\ntcf_tree_lock(tp);\r\nfp_old = prog->filter;\r\nbpf_old = prog->bpf_ops;\r\nprog->bpf_len = bpf_len;\r\nprog->bpf_ops = bpf_ops;\r\nprog->filter = fp;\r\nprog->res.classid = classid;\r\ntcf_tree_unlock(tp);\r\ntcf_bind_filter(tp, &prog->res, base);\r\ntcf_exts_change(tp, &prog->exts, &exts);\r\nif (fp_old)\r\nsk_unattached_filter_destroy(fp_old);\r\nif (bpf_old)\r\nkfree(bpf_old);\r\nreturn 0;\r\nerrout_free:\r\nkfree(bpf_ops);\r\nerrout:\r\ntcf_exts_destroy(tp, &exts);\r\nreturn ret;\r\n}\r\nstatic u32 cls_bpf_grab_new_handle(struct tcf_proto *tp,\r\nstruct cls_bpf_head *head)\r\n{\r\nunsigned int i = 0x80000000;\r\ndo {\r\nif (++head->hgen == 0x7FFFFFFF)\r\nhead->hgen = 1;\r\n} while (--i > 0 && cls_bpf_get(tp, head->hgen));\r\nif (i == 0)\r\npr_err("Insufficient number of handles\n");\r\nreturn i;\r\n}\r\nstatic int cls_bpf_change(struct net *net, struct sk_buff *in_skb,\r\nstruct tcf_proto *tp, unsigned long base,\r\nu32 handle, struct nlattr **tca,\r\nunsigned long *arg)\r\n{\r\nstruct cls_bpf_head *head = tp->root;\r\nstruct cls_bpf_prog *prog = (struct cls_bpf_prog *) *arg;\r\nstruct nlattr *tb[TCA_BPF_MAX + 1];\r\nint ret;\r\nif (tca[TCA_OPTIONS] == NULL)\r\nreturn -EINVAL;\r\nret = nla_parse_nested(tb, TCA_BPF_MAX, tca[TCA_OPTIONS], bpf_policy);\r\nif (ret < 0)\r\nreturn ret;\r\nif (prog != NULL) {\r\nif (handle && prog->handle != handle)\r\nreturn -EINVAL;\r\nreturn cls_bpf_modify_existing(net, tp, prog, base, tb,\r\ntca[TCA_RATE]);\r\n}\r\nprog = kzalloc(sizeof(*prog), GFP_KERNEL);\r\nif (prog == NULL)\r\nreturn -ENOBUFS;\r\ntcf_exts_init(&prog->exts, TCA_BPF_ACT, TCA_BPF_POLICE);\r\nif (handle == 0)\r\nprog->handle = cls_bpf_grab_new_handle(tp, head);\r\nelse\r\nprog->handle = handle;\r\nif (prog->handle == 0) {\r\nret = -EINVAL;\r\ngoto errout;\r\n}\r\nret = cls_bpf_modify_existing(net, tp, prog, base, tb, tca[TCA_RATE]);\r\nif (ret < 0)\r\ngoto errout;\r\ntcf_tree_lock(tp);\r\nlist_add(&prog->link, &head->plist);\r\ntcf_tree_unlock(tp);\r\n*arg = (unsigned long) prog;\r\nreturn 0;\r\nerrout:\r\nif (*arg == 0UL && prog)\r\nkfree(prog);\r\nreturn ret;\r\n}\r\nstatic int cls_bpf_dump(struct net *net, struct tcf_proto *tp, unsigned long fh,\r\nstruct sk_buff *skb, struct tcmsg *tm)\r\n{\r\nstruct cls_bpf_prog *prog = (struct cls_bpf_prog *) fh;\r\nstruct nlattr *nest, *nla;\r\nif (prog == NULL)\r\nreturn skb->len;\r\ntm->tcm_handle = prog->handle;\r\nnest = nla_nest_start(skb, TCA_OPTIONS);\r\nif (nest == NULL)\r\ngoto nla_put_failure;\r\nif (nla_put_u32(skb, TCA_BPF_CLASSID, prog->res.classid))\r\ngoto nla_put_failure;\r\nif (nla_put_u16(skb, TCA_BPF_OPS_LEN, prog->bpf_len))\r\ngoto nla_put_failure;\r\nnla = nla_reserve(skb, TCA_BPF_OPS, prog->bpf_len *\r\nsizeof(struct sock_filter));\r\nif (nla == NULL)\r\ngoto nla_put_failure;\r\nmemcpy(nla_data(nla), prog->bpf_ops, nla_len(nla));\r\nif (tcf_exts_dump(skb, &prog->exts) < 0)\r\ngoto nla_put_failure;\r\nnla_nest_end(skb, nest);\r\nif (tcf_exts_dump_stats(skb, &prog->exts) < 0)\r\ngoto nla_put_failure;\r\nreturn skb->len;\r\nnla_put_failure:\r\nnla_nest_cancel(skb, nest);\r\nreturn -1;\r\n}\r\nstatic void cls_bpf_walk(struct tcf_proto *tp, struct tcf_walker *arg)\r\n{\r\nstruct cls_bpf_head *head = tp->root;\r\nstruct cls_bpf_prog *prog;\r\nlist_for_each_entry(prog, &head->plist, link) {\r\nif (arg->count < arg->skip)\r\ngoto skip;\r\nif (arg->fn(tp, (unsigned long) prog, arg) < 0) {\r\narg->stop = 1;\r\nbreak;\r\n}\r\nskip:\r\narg->count++;\r\n}\r\n}\r\nstatic int __init cls_bpf_init_mod(void)\r\n{\r\nreturn register_tcf_proto_ops(&cls_bpf_ops);\r\n}\r\nstatic void __exit cls_bpf_exit_mod(void)\r\n{\r\nunregister_tcf_proto_ops(&cls_bpf_ops);\r\n}
