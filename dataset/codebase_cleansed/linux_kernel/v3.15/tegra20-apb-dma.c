static inline void tdma_write(struct tegra_dma *tdma, u32 reg, u32 val)\r\n{\r\nwritel(val, tdma->base_addr + reg);\r\n}\r\nstatic inline u32 tdma_read(struct tegra_dma *tdma, u32 reg)\r\n{\r\nreturn readl(tdma->base_addr + reg);\r\n}\r\nstatic inline void tdc_write(struct tegra_dma_channel *tdc,\r\nu32 reg, u32 val)\r\n{\r\nwritel(val, tdc->tdma->base_addr + tdc->chan_base_offset + reg);\r\n}\r\nstatic inline u32 tdc_read(struct tegra_dma_channel *tdc, u32 reg)\r\n{\r\nreturn readl(tdc->tdma->base_addr + tdc->chan_base_offset + reg);\r\n}\r\nstatic inline struct tegra_dma_channel *to_tegra_dma_chan(struct dma_chan *dc)\r\n{\r\nreturn container_of(dc, struct tegra_dma_channel, dma_chan);\r\n}\r\nstatic inline struct tegra_dma_desc *txd_to_tegra_dma_desc(\r\nstruct dma_async_tx_descriptor *td)\r\n{\r\nreturn container_of(td, struct tegra_dma_desc, txd);\r\n}\r\nstatic inline struct device *tdc2dev(struct tegra_dma_channel *tdc)\r\n{\r\nreturn &tdc->dma_chan.dev->device;\r\n}\r\nstatic struct tegra_dma_desc *tegra_dma_desc_get(\r\nstruct tegra_dma_channel *tdc)\r\n{\r\nstruct tegra_dma_desc *dma_desc;\r\nunsigned long flags;\r\nspin_lock_irqsave(&tdc->lock, flags);\r\nlist_for_each_entry(dma_desc, &tdc->free_dma_desc, node) {\r\nif (async_tx_test_ack(&dma_desc->txd)) {\r\nlist_del(&dma_desc->node);\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\ndma_desc->txd.flags = 0;\r\nreturn dma_desc;\r\n}\r\n}\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\ndma_desc = kzalloc(sizeof(*dma_desc), GFP_ATOMIC);\r\nif (!dma_desc) {\r\ndev_err(tdc2dev(tdc), "dma_desc alloc failed\n");\r\nreturn NULL;\r\n}\r\ndma_async_tx_descriptor_init(&dma_desc->txd, &tdc->dma_chan);\r\ndma_desc->txd.tx_submit = tegra_dma_tx_submit;\r\ndma_desc->txd.flags = 0;\r\nreturn dma_desc;\r\n}\r\nstatic void tegra_dma_desc_put(struct tegra_dma_channel *tdc,\r\nstruct tegra_dma_desc *dma_desc)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&tdc->lock, flags);\r\nif (!list_empty(&dma_desc->tx_list))\r\nlist_splice_init(&dma_desc->tx_list, &tdc->free_sg_req);\r\nlist_add_tail(&dma_desc->node, &tdc->free_dma_desc);\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\n}\r\nstatic struct tegra_dma_sg_req *tegra_dma_sg_req_get(\r\nstruct tegra_dma_channel *tdc)\r\n{\r\nstruct tegra_dma_sg_req *sg_req = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&tdc->lock, flags);\r\nif (!list_empty(&tdc->free_sg_req)) {\r\nsg_req = list_first_entry(&tdc->free_sg_req,\r\ntypeof(*sg_req), node);\r\nlist_del(&sg_req->node);\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nreturn sg_req;\r\n}\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nsg_req = kzalloc(sizeof(struct tegra_dma_sg_req), GFP_ATOMIC);\r\nif (!sg_req)\r\ndev_err(tdc2dev(tdc), "sg_req alloc failed\n");\r\nreturn sg_req;\r\n}\r\nstatic int tegra_dma_slave_config(struct dma_chan *dc,\r\nstruct dma_slave_config *sconfig)\r\n{\r\nstruct tegra_dma_channel *tdc = to_tegra_dma_chan(dc);\r\nif (!list_empty(&tdc->pending_sg_req)) {\r\ndev_err(tdc2dev(tdc), "Configuration not allowed\n");\r\nreturn -EBUSY;\r\n}\r\nmemcpy(&tdc->dma_sconfig, sconfig, sizeof(*sconfig));\r\nif (!tdc->slave_id)\r\ntdc->slave_id = sconfig->slave_id;\r\ntdc->config_init = true;\r\nreturn 0;\r\n}\r\nstatic void tegra_dma_global_pause(struct tegra_dma_channel *tdc,\r\nbool wait_for_burst_complete)\r\n{\r\nstruct tegra_dma *tdma = tdc->tdma;\r\nspin_lock(&tdma->global_lock);\r\ntdma_write(tdma, TEGRA_APBDMA_GENERAL, 0);\r\nif (wait_for_burst_complete)\r\nudelay(TEGRA_APBDMA_BURST_COMPLETE_TIME);\r\n}\r\nstatic void tegra_dma_global_resume(struct tegra_dma_channel *tdc)\r\n{\r\nstruct tegra_dma *tdma = tdc->tdma;\r\ntdma_write(tdma, TEGRA_APBDMA_GENERAL, TEGRA_APBDMA_GENERAL_ENABLE);\r\nspin_unlock(&tdma->global_lock);\r\n}\r\nstatic void tegra_dma_pause(struct tegra_dma_channel *tdc,\r\nbool wait_for_burst_complete)\r\n{\r\nstruct tegra_dma *tdma = tdc->tdma;\r\nif (tdma->chip_data->support_channel_pause) {\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_CSRE,\r\nTEGRA_APBDMA_CHAN_CSRE_PAUSE);\r\nif (wait_for_burst_complete)\r\nudelay(TEGRA_APBDMA_BURST_COMPLETE_TIME);\r\n} else {\r\ntegra_dma_global_pause(tdc, wait_for_burst_complete);\r\n}\r\n}\r\nstatic void tegra_dma_resume(struct tegra_dma_channel *tdc)\r\n{\r\nstruct tegra_dma *tdma = tdc->tdma;\r\nif (tdma->chip_data->support_channel_pause) {\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_CSRE, 0);\r\n} else {\r\ntegra_dma_global_resume(tdc);\r\n}\r\n}\r\nstatic void tegra_dma_stop(struct tegra_dma_channel *tdc)\r\n{\r\nu32 csr;\r\nu32 status;\r\ncsr = tdc_read(tdc, TEGRA_APBDMA_CHAN_CSR);\r\ncsr &= ~TEGRA_APBDMA_CSR_IE_EOC;\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_CSR, csr);\r\ncsr &= ~TEGRA_APBDMA_CSR_ENB;\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_CSR, csr);\r\nstatus = tdc_read(tdc, TEGRA_APBDMA_CHAN_STATUS);\r\nif (status & TEGRA_APBDMA_STATUS_ISE_EOC) {\r\ndev_dbg(tdc2dev(tdc), "%s():clearing interrupt\n", __func__);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_STATUS, status);\r\n}\r\ntdc->busy = false;\r\n}\r\nstatic void tegra_dma_start(struct tegra_dma_channel *tdc,\r\nstruct tegra_dma_sg_req *sg_req)\r\n{\r\nstruct tegra_dma_channel_regs *ch_regs = &sg_req->ch_regs;\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_CSR, ch_regs->csr);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_APBSEQ, ch_regs->apb_seq);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_APBPTR, ch_regs->apb_ptr);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_AHBSEQ, ch_regs->ahb_seq);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_AHBPTR, ch_regs->ahb_ptr);\r\nif (tdc->tdma->chip_data->support_separate_wcount_reg)\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_WCOUNT, ch_regs->wcount);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_CSR,\r\nch_regs->csr | TEGRA_APBDMA_CSR_ENB);\r\n}\r\nstatic void tegra_dma_configure_for_next(struct tegra_dma_channel *tdc,\r\nstruct tegra_dma_sg_req *nsg_req)\r\n{\r\nunsigned long status;\r\ntegra_dma_pause(tdc, false);\r\nstatus = tdc_read(tdc, TEGRA_APBDMA_CHAN_STATUS);\r\nif (status & TEGRA_APBDMA_STATUS_ISE_EOC) {\r\ndev_err(tdc2dev(tdc),\r\n"Skipping new configuration as interrupt is pending\n");\r\ntegra_dma_resume(tdc);\r\nreturn;\r\n}\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_APBPTR, nsg_req->ch_regs.apb_ptr);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_AHBPTR, nsg_req->ch_regs.ahb_ptr);\r\nif (tdc->tdma->chip_data->support_separate_wcount_reg)\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_WCOUNT,\r\nnsg_req->ch_regs.wcount);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_CSR,\r\nnsg_req->ch_regs.csr | TEGRA_APBDMA_CSR_ENB);\r\nnsg_req->configured = true;\r\ntegra_dma_resume(tdc);\r\n}\r\nstatic void tdc_start_head_req(struct tegra_dma_channel *tdc)\r\n{\r\nstruct tegra_dma_sg_req *sg_req;\r\nif (list_empty(&tdc->pending_sg_req))\r\nreturn;\r\nsg_req = list_first_entry(&tdc->pending_sg_req,\r\ntypeof(*sg_req), node);\r\ntegra_dma_start(tdc, sg_req);\r\nsg_req->configured = true;\r\ntdc->busy = true;\r\n}\r\nstatic void tdc_configure_next_head_desc(struct tegra_dma_channel *tdc)\r\n{\r\nstruct tegra_dma_sg_req *hsgreq;\r\nstruct tegra_dma_sg_req *hnsgreq;\r\nif (list_empty(&tdc->pending_sg_req))\r\nreturn;\r\nhsgreq = list_first_entry(&tdc->pending_sg_req, typeof(*hsgreq), node);\r\nif (!list_is_last(&hsgreq->node, &tdc->pending_sg_req)) {\r\nhnsgreq = list_first_entry(&hsgreq->node,\r\ntypeof(*hnsgreq), node);\r\ntegra_dma_configure_for_next(tdc, hnsgreq);\r\n}\r\n}\r\nstatic inline int get_current_xferred_count(struct tegra_dma_channel *tdc,\r\nstruct tegra_dma_sg_req *sg_req, unsigned long status)\r\n{\r\nreturn sg_req->req_len - (status & TEGRA_APBDMA_STATUS_COUNT_MASK) - 4;\r\n}\r\nstatic void tegra_dma_abort_all(struct tegra_dma_channel *tdc)\r\n{\r\nstruct tegra_dma_sg_req *sgreq;\r\nstruct tegra_dma_desc *dma_desc;\r\nwhile (!list_empty(&tdc->pending_sg_req)) {\r\nsgreq = list_first_entry(&tdc->pending_sg_req,\r\ntypeof(*sgreq), node);\r\nlist_move_tail(&sgreq->node, &tdc->free_sg_req);\r\nif (sgreq->last_sg) {\r\ndma_desc = sgreq->dma_desc;\r\ndma_desc->dma_status = DMA_ERROR;\r\nlist_add_tail(&dma_desc->node, &tdc->free_dma_desc);\r\nif (!dma_desc->cb_count)\r\nlist_add_tail(&dma_desc->cb_node,\r\n&tdc->cb_desc);\r\ndma_desc->cb_count++;\r\n}\r\n}\r\ntdc->isr_handler = NULL;\r\n}\r\nstatic bool handle_continuous_head_request(struct tegra_dma_channel *tdc,\r\nstruct tegra_dma_sg_req *last_sg_req, bool to_terminate)\r\n{\r\nstruct tegra_dma_sg_req *hsgreq = NULL;\r\nif (list_empty(&tdc->pending_sg_req)) {\r\ndev_err(tdc2dev(tdc), "Dma is running without req\n");\r\ntegra_dma_stop(tdc);\r\nreturn false;\r\n}\r\nhsgreq = list_first_entry(&tdc->pending_sg_req, typeof(*hsgreq), node);\r\nif (!hsgreq->configured) {\r\ntegra_dma_stop(tdc);\r\ndev_err(tdc2dev(tdc), "Error in dma transfer, aborting dma\n");\r\ntegra_dma_abort_all(tdc);\r\nreturn false;\r\n}\r\nif (!to_terminate)\r\ntdc_configure_next_head_desc(tdc);\r\nreturn true;\r\n}\r\nstatic void handle_once_dma_done(struct tegra_dma_channel *tdc,\r\nbool to_terminate)\r\n{\r\nstruct tegra_dma_sg_req *sgreq;\r\nstruct tegra_dma_desc *dma_desc;\r\ntdc->busy = false;\r\nsgreq = list_first_entry(&tdc->pending_sg_req, typeof(*sgreq), node);\r\ndma_desc = sgreq->dma_desc;\r\ndma_desc->bytes_transferred += sgreq->req_len;\r\nlist_del(&sgreq->node);\r\nif (sgreq->last_sg) {\r\ndma_desc->dma_status = DMA_COMPLETE;\r\ndma_cookie_complete(&dma_desc->txd);\r\nif (!dma_desc->cb_count)\r\nlist_add_tail(&dma_desc->cb_node, &tdc->cb_desc);\r\ndma_desc->cb_count++;\r\nlist_add_tail(&dma_desc->node, &tdc->free_dma_desc);\r\n}\r\nlist_add_tail(&sgreq->node, &tdc->free_sg_req);\r\nif (to_terminate || list_empty(&tdc->pending_sg_req))\r\nreturn;\r\ntdc_start_head_req(tdc);\r\nreturn;\r\n}\r\nstatic void handle_cont_sngl_cycle_dma_done(struct tegra_dma_channel *tdc,\r\nbool to_terminate)\r\n{\r\nstruct tegra_dma_sg_req *sgreq;\r\nstruct tegra_dma_desc *dma_desc;\r\nbool st;\r\nsgreq = list_first_entry(&tdc->pending_sg_req, typeof(*sgreq), node);\r\ndma_desc = sgreq->dma_desc;\r\ndma_desc->bytes_transferred += sgreq->req_len;\r\nif (!dma_desc->cb_count)\r\nlist_add_tail(&dma_desc->cb_node, &tdc->cb_desc);\r\ndma_desc->cb_count++;\r\nif (!list_is_last(&sgreq->node, &tdc->pending_sg_req)) {\r\nlist_move_tail(&sgreq->node, &tdc->pending_sg_req);\r\nsgreq->configured = false;\r\nst = handle_continuous_head_request(tdc, sgreq, to_terminate);\r\nif (!st)\r\ndma_desc->dma_status = DMA_ERROR;\r\n}\r\nreturn;\r\n}\r\nstatic void tegra_dma_tasklet(unsigned long data)\r\n{\r\nstruct tegra_dma_channel *tdc = (struct tegra_dma_channel *)data;\r\ndma_async_tx_callback callback = NULL;\r\nvoid *callback_param = NULL;\r\nstruct tegra_dma_desc *dma_desc;\r\nunsigned long flags;\r\nint cb_count;\r\nspin_lock_irqsave(&tdc->lock, flags);\r\nwhile (!list_empty(&tdc->cb_desc)) {\r\ndma_desc = list_first_entry(&tdc->cb_desc,\r\ntypeof(*dma_desc), cb_node);\r\nlist_del(&dma_desc->cb_node);\r\ncallback = dma_desc->txd.callback;\r\ncallback_param = dma_desc->txd.callback_param;\r\ncb_count = dma_desc->cb_count;\r\ndma_desc->cb_count = 0;\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nwhile (cb_count-- && callback)\r\ncallback(callback_param);\r\nspin_lock_irqsave(&tdc->lock, flags);\r\n}\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\n}\r\nstatic irqreturn_t tegra_dma_isr(int irq, void *dev_id)\r\n{\r\nstruct tegra_dma_channel *tdc = dev_id;\r\nunsigned long status;\r\nunsigned long flags;\r\nspin_lock_irqsave(&tdc->lock, flags);\r\nstatus = tdc_read(tdc, TEGRA_APBDMA_CHAN_STATUS);\r\nif (status & TEGRA_APBDMA_STATUS_ISE_EOC) {\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_STATUS, status);\r\ntdc->isr_handler(tdc, false);\r\ntasklet_schedule(&tdc->tasklet);\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\ndev_info(tdc2dev(tdc),\r\n"Interrupt already served status 0x%08lx\n", status);\r\nreturn IRQ_NONE;\r\n}\r\nstatic dma_cookie_t tegra_dma_tx_submit(struct dma_async_tx_descriptor *txd)\r\n{\r\nstruct tegra_dma_desc *dma_desc = txd_to_tegra_dma_desc(txd);\r\nstruct tegra_dma_channel *tdc = to_tegra_dma_chan(txd->chan);\r\nunsigned long flags;\r\ndma_cookie_t cookie;\r\nspin_lock_irqsave(&tdc->lock, flags);\r\ndma_desc->dma_status = DMA_IN_PROGRESS;\r\ncookie = dma_cookie_assign(&dma_desc->txd);\r\nlist_splice_tail_init(&dma_desc->tx_list, &tdc->pending_sg_req);\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic void tegra_dma_issue_pending(struct dma_chan *dc)\r\n{\r\nstruct tegra_dma_channel *tdc = to_tegra_dma_chan(dc);\r\nunsigned long flags;\r\nspin_lock_irqsave(&tdc->lock, flags);\r\nif (list_empty(&tdc->pending_sg_req)) {\r\ndev_err(tdc2dev(tdc), "No DMA request\n");\r\ngoto end;\r\n}\r\nif (!tdc->busy) {\r\ntdc_start_head_req(tdc);\r\nif (tdc->cyclic) {\r\nudelay(TEGRA_APBDMA_BURST_COMPLETE_TIME);\r\ntdc_configure_next_head_desc(tdc);\r\n}\r\n}\r\nend:\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nreturn;\r\n}\r\nstatic void tegra_dma_terminate_all(struct dma_chan *dc)\r\n{\r\nstruct tegra_dma_channel *tdc = to_tegra_dma_chan(dc);\r\nstruct tegra_dma_sg_req *sgreq;\r\nstruct tegra_dma_desc *dma_desc;\r\nunsigned long flags;\r\nunsigned long status;\r\nunsigned long wcount;\r\nbool was_busy;\r\nspin_lock_irqsave(&tdc->lock, flags);\r\nif (list_empty(&tdc->pending_sg_req)) {\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nreturn;\r\n}\r\nif (!tdc->busy)\r\ngoto skip_dma_stop;\r\ntegra_dma_pause(tdc, true);\r\nstatus = tdc_read(tdc, TEGRA_APBDMA_CHAN_STATUS);\r\nif (status & TEGRA_APBDMA_STATUS_ISE_EOC) {\r\ndev_dbg(tdc2dev(tdc), "%s():handling isr\n", __func__);\r\ntdc->isr_handler(tdc, true);\r\nstatus = tdc_read(tdc, TEGRA_APBDMA_CHAN_STATUS);\r\n}\r\nif (tdc->tdma->chip_data->support_separate_wcount_reg)\r\nwcount = tdc_read(tdc, TEGRA_APBDMA_CHAN_WORD_TRANSFER);\r\nelse\r\nwcount = status;\r\nwas_busy = tdc->busy;\r\ntegra_dma_stop(tdc);\r\nif (!list_empty(&tdc->pending_sg_req) && was_busy) {\r\nsgreq = list_first_entry(&tdc->pending_sg_req,\r\ntypeof(*sgreq), node);\r\nsgreq->dma_desc->bytes_transferred +=\r\nget_current_xferred_count(tdc, sgreq, wcount);\r\n}\r\ntegra_dma_resume(tdc);\r\nskip_dma_stop:\r\ntegra_dma_abort_all(tdc);\r\nwhile (!list_empty(&tdc->cb_desc)) {\r\ndma_desc = list_first_entry(&tdc->cb_desc,\r\ntypeof(*dma_desc), cb_node);\r\nlist_del(&dma_desc->cb_node);\r\ndma_desc->cb_count = 0;\r\n}\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\n}\r\nstatic enum dma_status tegra_dma_tx_status(struct dma_chan *dc,\r\ndma_cookie_t cookie, struct dma_tx_state *txstate)\r\n{\r\nstruct tegra_dma_channel *tdc = to_tegra_dma_chan(dc);\r\nstruct tegra_dma_desc *dma_desc;\r\nstruct tegra_dma_sg_req *sg_req;\r\nenum dma_status ret;\r\nunsigned long flags;\r\nunsigned int residual;\r\nret = dma_cookie_status(dc, cookie, txstate);\r\nif (ret == DMA_COMPLETE)\r\nreturn ret;\r\nspin_lock_irqsave(&tdc->lock, flags);\r\nlist_for_each_entry(dma_desc, &tdc->free_dma_desc, node) {\r\nif (dma_desc->txd.cookie == cookie) {\r\nresidual = dma_desc->bytes_requested -\r\n(dma_desc->bytes_transferred %\r\ndma_desc->bytes_requested);\r\ndma_set_residue(txstate, residual);\r\nret = dma_desc->dma_status;\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nreturn ret;\r\n}\r\n}\r\nlist_for_each_entry(sg_req, &tdc->pending_sg_req, node) {\r\ndma_desc = sg_req->dma_desc;\r\nif (dma_desc->txd.cookie == cookie) {\r\nresidual = dma_desc->bytes_requested -\r\n(dma_desc->bytes_transferred %\r\ndma_desc->bytes_requested);\r\ndma_set_residue(txstate, residual);\r\nret = dma_desc->dma_status;\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nreturn ret;\r\n}\r\n}\r\ndev_dbg(tdc2dev(tdc), "cookie %d does not found\n", cookie);\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nreturn ret;\r\n}\r\nstatic int tegra_dma_device_control(struct dma_chan *dc, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nswitch (cmd) {\r\ncase DMA_SLAVE_CONFIG:\r\nreturn tegra_dma_slave_config(dc,\r\n(struct dma_slave_config *)arg);\r\ncase DMA_TERMINATE_ALL:\r\ntegra_dma_terminate_all(dc);\r\nreturn 0;\r\ndefault:\r\nbreak;\r\n}\r\nreturn -ENXIO;\r\n}\r\nstatic inline int get_bus_width(struct tegra_dma_channel *tdc,\r\nenum dma_slave_buswidth slave_bw)\r\n{\r\nswitch (slave_bw) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nreturn TEGRA_APBDMA_APBSEQ_BUS_WIDTH_8;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nreturn TEGRA_APBDMA_APBSEQ_BUS_WIDTH_16;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nreturn TEGRA_APBDMA_APBSEQ_BUS_WIDTH_32;\r\ncase DMA_SLAVE_BUSWIDTH_8_BYTES:\r\nreturn TEGRA_APBDMA_APBSEQ_BUS_WIDTH_64;\r\ndefault:\r\ndev_warn(tdc2dev(tdc),\r\n"slave bw is not supported, using 32bits\n");\r\nreturn TEGRA_APBDMA_APBSEQ_BUS_WIDTH_32;\r\n}\r\n}\r\nstatic inline int get_burst_size(struct tegra_dma_channel *tdc,\r\nu32 burst_size, enum dma_slave_buswidth slave_bw, int len)\r\n{\r\nint burst_byte;\r\nint burst_ahb_width;\r\nburst_byte = burst_size * slave_bw;\r\nburst_ahb_width = burst_byte / 4;\r\nif (!burst_ahb_width) {\r\nif (len & 0xF)\r\nreturn TEGRA_APBDMA_AHBSEQ_BURST_1;\r\nelse if ((len >> 4) & 0x1)\r\nreturn TEGRA_APBDMA_AHBSEQ_BURST_4;\r\nelse\r\nreturn TEGRA_APBDMA_AHBSEQ_BURST_8;\r\n}\r\nif (burst_ahb_width < 4)\r\nreturn TEGRA_APBDMA_AHBSEQ_BURST_1;\r\nelse if (burst_ahb_width < 8)\r\nreturn TEGRA_APBDMA_AHBSEQ_BURST_4;\r\nelse\r\nreturn TEGRA_APBDMA_AHBSEQ_BURST_8;\r\n}\r\nstatic int get_transfer_param(struct tegra_dma_channel *tdc,\r\nenum dma_transfer_direction direction, unsigned long *apb_addr,\r\nunsigned long *apb_seq, unsigned long *csr, unsigned int *burst_size,\r\nenum dma_slave_buswidth *slave_bw)\r\n{\r\nswitch (direction) {\r\ncase DMA_MEM_TO_DEV:\r\n*apb_addr = tdc->dma_sconfig.dst_addr;\r\n*apb_seq = get_bus_width(tdc, tdc->dma_sconfig.dst_addr_width);\r\n*burst_size = tdc->dma_sconfig.dst_maxburst;\r\n*slave_bw = tdc->dma_sconfig.dst_addr_width;\r\n*csr = TEGRA_APBDMA_CSR_DIR;\r\nreturn 0;\r\ncase DMA_DEV_TO_MEM:\r\n*apb_addr = tdc->dma_sconfig.src_addr;\r\n*apb_seq = get_bus_width(tdc, tdc->dma_sconfig.src_addr_width);\r\n*burst_size = tdc->dma_sconfig.src_maxburst;\r\n*slave_bw = tdc->dma_sconfig.src_addr_width;\r\n*csr = 0;\r\nreturn 0;\r\ndefault:\r\ndev_err(tdc2dev(tdc), "Dma direction is not supported\n");\r\nreturn -EINVAL;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic void tegra_dma_prep_wcount(struct tegra_dma_channel *tdc,\r\nstruct tegra_dma_channel_regs *ch_regs, u32 len)\r\n{\r\nu32 len_field = (len - 4) & 0xFFFC;\r\nif (tdc->tdma->chip_data->support_separate_wcount_reg)\r\nch_regs->wcount = len_field;\r\nelse\r\nch_regs->csr |= len_field;\r\n}\r\nstatic struct dma_async_tx_descriptor *tegra_dma_prep_slave_sg(\r\nstruct dma_chan *dc, struct scatterlist *sgl, unsigned int sg_len,\r\nenum dma_transfer_direction direction, unsigned long flags,\r\nvoid *context)\r\n{\r\nstruct tegra_dma_channel *tdc = to_tegra_dma_chan(dc);\r\nstruct tegra_dma_desc *dma_desc;\r\nunsigned int i;\r\nstruct scatterlist *sg;\r\nunsigned long csr, ahb_seq, apb_ptr, apb_seq;\r\nstruct list_head req_list;\r\nstruct tegra_dma_sg_req *sg_req = NULL;\r\nu32 burst_size;\r\nenum dma_slave_buswidth slave_bw;\r\nint ret;\r\nif (!tdc->config_init) {\r\ndev_err(tdc2dev(tdc), "dma channel is not configured\n");\r\nreturn NULL;\r\n}\r\nif (sg_len < 1) {\r\ndev_err(tdc2dev(tdc), "Invalid segment length %d\n", sg_len);\r\nreturn NULL;\r\n}\r\nret = get_transfer_param(tdc, direction, &apb_ptr, &apb_seq, &csr,\r\n&burst_size, &slave_bw);\r\nif (ret < 0)\r\nreturn NULL;\r\nINIT_LIST_HEAD(&req_list);\r\nahb_seq = TEGRA_APBDMA_AHBSEQ_INTR_ENB;\r\nahb_seq |= TEGRA_APBDMA_AHBSEQ_WRAP_NONE <<\r\nTEGRA_APBDMA_AHBSEQ_WRAP_SHIFT;\r\nahb_seq |= TEGRA_APBDMA_AHBSEQ_BUS_WIDTH_32;\r\ncsr |= TEGRA_APBDMA_CSR_ONCE | TEGRA_APBDMA_CSR_FLOW;\r\ncsr |= tdc->slave_id << TEGRA_APBDMA_CSR_REQ_SEL_SHIFT;\r\nif (flags & DMA_PREP_INTERRUPT)\r\ncsr |= TEGRA_APBDMA_CSR_IE_EOC;\r\napb_seq |= TEGRA_APBDMA_APBSEQ_WRAP_WORD_1;\r\ndma_desc = tegra_dma_desc_get(tdc);\r\nif (!dma_desc) {\r\ndev_err(tdc2dev(tdc), "Dma descriptors not available\n");\r\nreturn NULL;\r\n}\r\nINIT_LIST_HEAD(&dma_desc->tx_list);\r\nINIT_LIST_HEAD(&dma_desc->cb_node);\r\ndma_desc->cb_count = 0;\r\ndma_desc->bytes_requested = 0;\r\ndma_desc->bytes_transferred = 0;\r\ndma_desc->dma_status = DMA_IN_PROGRESS;\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nu32 len, mem;\r\nmem = sg_dma_address(sg);\r\nlen = sg_dma_len(sg);\r\nif ((len & 3) || (mem & 3) ||\r\n(len > tdc->tdma->chip_data->max_dma_count)) {\r\ndev_err(tdc2dev(tdc),\r\n"Dma length/memory address is not supported\n");\r\ntegra_dma_desc_put(tdc, dma_desc);\r\nreturn NULL;\r\n}\r\nsg_req = tegra_dma_sg_req_get(tdc);\r\nif (!sg_req) {\r\ndev_err(tdc2dev(tdc), "Dma sg-req not available\n");\r\ntegra_dma_desc_put(tdc, dma_desc);\r\nreturn NULL;\r\n}\r\nahb_seq |= get_burst_size(tdc, burst_size, slave_bw, len);\r\ndma_desc->bytes_requested += len;\r\nsg_req->ch_regs.apb_ptr = apb_ptr;\r\nsg_req->ch_regs.ahb_ptr = mem;\r\nsg_req->ch_regs.csr = csr;\r\ntegra_dma_prep_wcount(tdc, &sg_req->ch_regs, len);\r\nsg_req->ch_regs.apb_seq = apb_seq;\r\nsg_req->ch_regs.ahb_seq = ahb_seq;\r\nsg_req->configured = false;\r\nsg_req->last_sg = false;\r\nsg_req->dma_desc = dma_desc;\r\nsg_req->req_len = len;\r\nlist_add_tail(&sg_req->node, &dma_desc->tx_list);\r\n}\r\nsg_req->last_sg = true;\r\nif (flags & DMA_CTRL_ACK)\r\ndma_desc->txd.flags = DMA_CTRL_ACK;\r\nif (!tdc->isr_handler) {\r\ntdc->isr_handler = handle_once_dma_done;\r\ntdc->cyclic = false;\r\n} else {\r\nif (tdc->cyclic) {\r\ndev_err(tdc2dev(tdc), "DMA configured in cyclic mode\n");\r\ntegra_dma_desc_put(tdc, dma_desc);\r\nreturn NULL;\r\n}\r\n}\r\nreturn &dma_desc->txd;\r\n}\r\nstatic struct dma_async_tx_descriptor *tegra_dma_prep_dma_cyclic(\r\nstruct dma_chan *dc, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction direction,\r\nunsigned long flags, void *context)\r\n{\r\nstruct tegra_dma_channel *tdc = to_tegra_dma_chan(dc);\r\nstruct tegra_dma_desc *dma_desc = NULL;\r\nstruct tegra_dma_sg_req *sg_req = NULL;\r\nunsigned long csr, ahb_seq, apb_ptr, apb_seq;\r\nint len;\r\nsize_t remain_len;\r\ndma_addr_t mem = buf_addr;\r\nu32 burst_size;\r\nenum dma_slave_buswidth slave_bw;\r\nint ret;\r\nif (!buf_len || !period_len) {\r\ndev_err(tdc2dev(tdc), "Invalid buffer/period len\n");\r\nreturn NULL;\r\n}\r\nif (!tdc->config_init) {\r\ndev_err(tdc2dev(tdc), "DMA slave is not configured\n");\r\nreturn NULL;\r\n}\r\nif (tdc->busy) {\r\ndev_err(tdc2dev(tdc), "Request not allowed when dma running\n");\r\nreturn NULL;\r\n}\r\nif (buf_len % period_len) {\r\ndev_err(tdc2dev(tdc), "buf_len is not multiple of period_len\n");\r\nreturn NULL;\r\n}\r\nlen = period_len;\r\nif ((len & 3) || (buf_addr & 3) ||\r\n(len > tdc->tdma->chip_data->max_dma_count)) {\r\ndev_err(tdc2dev(tdc), "Req len/mem address is not correct\n");\r\nreturn NULL;\r\n}\r\nret = get_transfer_param(tdc, direction, &apb_ptr, &apb_seq, &csr,\r\n&burst_size, &slave_bw);\r\nif (ret < 0)\r\nreturn NULL;\r\nahb_seq = TEGRA_APBDMA_AHBSEQ_INTR_ENB;\r\nahb_seq |= TEGRA_APBDMA_AHBSEQ_WRAP_NONE <<\r\nTEGRA_APBDMA_AHBSEQ_WRAP_SHIFT;\r\nahb_seq |= TEGRA_APBDMA_AHBSEQ_BUS_WIDTH_32;\r\ncsr |= TEGRA_APBDMA_CSR_FLOW;\r\nif (flags & DMA_PREP_INTERRUPT)\r\ncsr |= TEGRA_APBDMA_CSR_IE_EOC;\r\ncsr |= tdc->slave_id << TEGRA_APBDMA_CSR_REQ_SEL_SHIFT;\r\napb_seq |= TEGRA_APBDMA_APBSEQ_WRAP_WORD_1;\r\ndma_desc = tegra_dma_desc_get(tdc);\r\nif (!dma_desc) {\r\ndev_err(tdc2dev(tdc), "not enough descriptors available\n");\r\nreturn NULL;\r\n}\r\nINIT_LIST_HEAD(&dma_desc->tx_list);\r\nINIT_LIST_HEAD(&dma_desc->cb_node);\r\ndma_desc->cb_count = 0;\r\ndma_desc->bytes_transferred = 0;\r\ndma_desc->bytes_requested = buf_len;\r\nremain_len = buf_len;\r\nwhile (remain_len) {\r\nsg_req = tegra_dma_sg_req_get(tdc);\r\nif (!sg_req) {\r\ndev_err(tdc2dev(tdc), "Dma sg-req not available\n");\r\ntegra_dma_desc_put(tdc, dma_desc);\r\nreturn NULL;\r\n}\r\nahb_seq |= get_burst_size(tdc, burst_size, slave_bw, len);\r\nsg_req->ch_regs.apb_ptr = apb_ptr;\r\nsg_req->ch_regs.ahb_ptr = mem;\r\nsg_req->ch_regs.csr = csr;\r\ntegra_dma_prep_wcount(tdc, &sg_req->ch_regs, len);\r\nsg_req->ch_regs.apb_seq = apb_seq;\r\nsg_req->ch_regs.ahb_seq = ahb_seq;\r\nsg_req->configured = false;\r\nsg_req->half_done = false;\r\nsg_req->last_sg = false;\r\nsg_req->dma_desc = dma_desc;\r\nsg_req->req_len = len;\r\nlist_add_tail(&sg_req->node, &dma_desc->tx_list);\r\nremain_len -= len;\r\nmem += len;\r\n}\r\nsg_req->last_sg = true;\r\nif (flags & DMA_CTRL_ACK)\r\ndma_desc->txd.flags = DMA_CTRL_ACK;\r\nif (!tdc->isr_handler) {\r\ntdc->isr_handler = handle_cont_sngl_cycle_dma_done;\r\ntdc->cyclic = true;\r\n} else {\r\nif (!tdc->cyclic) {\r\ndev_err(tdc2dev(tdc), "DMA configuration conflict\n");\r\ntegra_dma_desc_put(tdc, dma_desc);\r\nreturn NULL;\r\n}\r\n}\r\nreturn &dma_desc->txd;\r\n}\r\nstatic int tegra_dma_alloc_chan_resources(struct dma_chan *dc)\r\n{\r\nstruct tegra_dma_channel *tdc = to_tegra_dma_chan(dc);\r\nstruct tegra_dma *tdma = tdc->tdma;\r\nint ret;\r\ndma_cookie_init(&tdc->dma_chan);\r\ntdc->config_init = false;\r\nret = clk_prepare_enable(tdma->dma_clk);\r\nif (ret < 0)\r\ndev_err(tdc2dev(tdc), "clk_prepare_enable failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nstatic void tegra_dma_free_chan_resources(struct dma_chan *dc)\r\n{\r\nstruct tegra_dma_channel *tdc = to_tegra_dma_chan(dc);\r\nstruct tegra_dma *tdma = tdc->tdma;\r\nstruct tegra_dma_desc *dma_desc;\r\nstruct tegra_dma_sg_req *sg_req;\r\nstruct list_head dma_desc_list;\r\nstruct list_head sg_req_list;\r\nunsigned long flags;\r\nINIT_LIST_HEAD(&dma_desc_list);\r\nINIT_LIST_HEAD(&sg_req_list);\r\ndev_dbg(tdc2dev(tdc), "Freeing channel %d\n", tdc->id);\r\nif (tdc->busy)\r\ntegra_dma_terminate_all(dc);\r\nspin_lock_irqsave(&tdc->lock, flags);\r\nlist_splice_init(&tdc->pending_sg_req, &sg_req_list);\r\nlist_splice_init(&tdc->free_sg_req, &sg_req_list);\r\nlist_splice_init(&tdc->free_dma_desc, &dma_desc_list);\r\nINIT_LIST_HEAD(&tdc->cb_desc);\r\ntdc->config_init = false;\r\ntdc->isr_handler = NULL;\r\nspin_unlock_irqrestore(&tdc->lock, flags);\r\nwhile (!list_empty(&dma_desc_list)) {\r\ndma_desc = list_first_entry(&dma_desc_list,\r\ntypeof(*dma_desc), node);\r\nlist_del(&dma_desc->node);\r\nkfree(dma_desc);\r\n}\r\nwhile (!list_empty(&sg_req_list)) {\r\nsg_req = list_first_entry(&sg_req_list, typeof(*sg_req), node);\r\nlist_del(&sg_req->node);\r\nkfree(sg_req);\r\n}\r\nclk_disable_unprepare(tdma->dma_clk);\r\ntdc->slave_id = 0;\r\n}\r\nstatic struct dma_chan *tegra_dma_of_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct tegra_dma *tdma = ofdma->of_dma_data;\r\nstruct dma_chan *chan;\r\nstruct tegra_dma_channel *tdc;\r\nchan = dma_get_any_slave_channel(&tdma->dma_dev);\r\nif (!chan)\r\nreturn NULL;\r\ntdc = to_tegra_dma_chan(chan);\r\ntdc->slave_id = dma_spec->args[0];\r\nreturn chan;\r\n}\r\nstatic int tegra_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *res;\r\nstruct tegra_dma *tdma;\r\nint ret;\r\nint i;\r\nconst struct tegra_dma_chip_data *cdata = NULL;\r\nconst struct of_device_id *match;\r\nmatch = of_match_device(tegra_dma_of_match, &pdev->dev);\r\nif (!match) {\r\ndev_err(&pdev->dev, "Error: No device match found\n");\r\nreturn -ENODEV;\r\n}\r\ncdata = match->data;\r\ntdma = devm_kzalloc(&pdev->dev, sizeof(*tdma) + cdata->nr_channels *\r\nsizeof(struct tegra_dma_channel), GFP_KERNEL);\r\nif (!tdma) {\r\ndev_err(&pdev->dev, "Error: memory allocation failed\n");\r\nreturn -ENOMEM;\r\n}\r\ntdma->dev = &pdev->dev;\r\ntdma->chip_data = cdata;\r\nplatform_set_drvdata(pdev, tdma);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ntdma->base_addr = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(tdma->base_addr))\r\nreturn PTR_ERR(tdma->base_addr);\r\ntdma->dma_clk = devm_clk_get(&pdev->dev, NULL);\r\nif (IS_ERR(tdma->dma_clk)) {\r\ndev_err(&pdev->dev, "Error: Missing controller clock\n");\r\nreturn PTR_ERR(tdma->dma_clk);\r\n}\r\ntdma->rst = devm_reset_control_get(&pdev->dev, "dma");\r\nif (IS_ERR(tdma->rst)) {\r\ndev_err(&pdev->dev, "Error: Missing reset\n");\r\nreturn PTR_ERR(tdma->rst);\r\n}\r\nspin_lock_init(&tdma->global_lock);\r\npm_runtime_enable(&pdev->dev);\r\nif (!pm_runtime_enabled(&pdev->dev)) {\r\nret = tegra_dma_runtime_resume(&pdev->dev);\r\nif (ret) {\r\ndev_err(&pdev->dev, "dma_runtime_resume failed %d\n",\r\nret);\r\ngoto err_pm_disable;\r\n}\r\n}\r\nret = clk_prepare_enable(tdma->dma_clk);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev, "clk_prepare_enable failed: %d\n", ret);\r\ngoto err_pm_disable;\r\n}\r\nreset_control_assert(tdma->rst);\r\nudelay(2);\r\nreset_control_deassert(tdma->rst);\r\ntdma_write(tdma, TEGRA_APBDMA_GENERAL, TEGRA_APBDMA_GENERAL_ENABLE);\r\ntdma_write(tdma, TEGRA_APBDMA_CONTROL, 0);\r\ntdma_write(tdma, TEGRA_APBDMA_IRQ_MASK_SET, 0xFFFFFFFFul);\r\nclk_disable_unprepare(tdma->dma_clk);\r\nINIT_LIST_HEAD(&tdma->dma_dev.channels);\r\nfor (i = 0; i < cdata->nr_channels; i++) {\r\nstruct tegra_dma_channel *tdc = &tdma->channels[i];\r\ntdc->chan_base_offset = TEGRA_APBDMA_CHANNEL_BASE_ADD_OFFSET +\r\ni * cdata->channel_reg_size;\r\nres = platform_get_resource(pdev, IORESOURCE_IRQ, i);\r\nif (!res) {\r\nret = -EINVAL;\r\ndev_err(&pdev->dev, "No irq resource for chan %d\n", i);\r\ngoto err_irq;\r\n}\r\ntdc->irq = res->start;\r\nsnprintf(tdc->name, sizeof(tdc->name), "apbdma.%d", i);\r\nret = devm_request_irq(&pdev->dev, tdc->irq,\r\ntegra_dma_isr, 0, tdc->name, tdc);\r\nif (ret) {\r\ndev_err(&pdev->dev,\r\n"request_irq failed with err %d channel %d\n",\r\nret, i);\r\ngoto err_irq;\r\n}\r\ntdc->dma_chan.device = &tdma->dma_dev;\r\ndma_cookie_init(&tdc->dma_chan);\r\nlist_add_tail(&tdc->dma_chan.device_node,\r\n&tdma->dma_dev.channels);\r\ntdc->tdma = tdma;\r\ntdc->id = i;\r\ntasklet_init(&tdc->tasklet, tegra_dma_tasklet,\r\n(unsigned long)tdc);\r\nspin_lock_init(&tdc->lock);\r\nINIT_LIST_HEAD(&tdc->pending_sg_req);\r\nINIT_LIST_HEAD(&tdc->free_sg_req);\r\nINIT_LIST_HEAD(&tdc->free_dma_desc);\r\nINIT_LIST_HEAD(&tdc->cb_desc);\r\n}\r\ndma_cap_set(DMA_SLAVE, tdma->dma_dev.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, tdma->dma_dev.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, tdma->dma_dev.cap_mask);\r\ntdma->dma_dev.dev = &pdev->dev;\r\ntdma->dma_dev.device_alloc_chan_resources =\r\ntegra_dma_alloc_chan_resources;\r\ntdma->dma_dev.device_free_chan_resources =\r\ntegra_dma_free_chan_resources;\r\ntdma->dma_dev.device_prep_slave_sg = tegra_dma_prep_slave_sg;\r\ntdma->dma_dev.device_prep_dma_cyclic = tegra_dma_prep_dma_cyclic;\r\ntdma->dma_dev.device_control = tegra_dma_device_control;\r\ntdma->dma_dev.device_tx_status = tegra_dma_tx_status;\r\ntdma->dma_dev.device_issue_pending = tegra_dma_issue_pending;\r\nret = dma_async_device_register(&tdma->dma_dev);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev,\r\n"Tegra20 APB DMA driver registration failed %d\n", ret);\r\ngoto err_irq;\r\n}\r\nret = of_dma_controller_register(pdev->dev.of_node,\r\ntegra_dma_of_xlate, tdma);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev,\r\n"Tegra20 APB DMA OF registration failed %d\n", ret);\r\ngoto err_unregister_dma_dev;\r\n}\r\ndev_info(&pdev->dev, "Tegra20 APB DMA driver register %d channels\n",\r\ncdata->nr_channels);\r\nreturn 0;\r\nerr_unregister_dma_dev:\r\ndma_async_device_unregister(&tdma->dma_dev);\r\nerr_irq:\r\nwhile (--i >= 0) {\r\nstruct tegra_dma_channel *tdc = &tdma->channels[i];\r\ntasklet_kill(&tdc->tasklet);\r\n}\r\nerr_pm_disable:\r\npm_runtime_disable(&pdev->dev);\r\nif (!pm_runtime_status_suspended(&pdev->dev))\r\ntegra_dma_runtime_suspend(&pdev->dev);\r\nreturn ret;\r\n}\r\nstatic int tegra_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct tegra_dma *tdma = platform_get_drvdata(pdev);\r\nint i;\r\nstruct tegra_dma_channel *tdc;\r\ndma_async_device_unregister(&tdma->dma_dev);\r\nfor (i = 0; i < tdma->chip_data->nr_channels; ++i) {\r\ntdc = &tdma->channels[i];\r\ntasklet_kill(&tdc->tasklet);\r\n}\r\npm_runtime_disable(&pdev->dev);\r\nif (!pm_runtime_status_suspended(&pdev->dev))\r\ntegra_dma_runtime_suspend(&pdev->dev);\r\nreturn 0;\r\n}\r\nstatic int tegra_dma_runtime_suspend(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct tegra_dma *tdma = platform_get_drvdata(pdev);\r\nclk_disable_unprepare(tdma->dma_clk);\r\nreturn 0;\r\n}\r\nstatic int tegra_dma_runtime_resume(struct device *dev)\r\n{\r\nstruct platform_device *pdev = to_platform_device(dev);\r\nstruct tegra_dma *tdma = platform_get_drvdata(pdev);\r\nint ret;\r\nret = clk_prepare_enable(tdma->dma_clk);\r\nif (ret < 0) {\r\ndev_err(dev, "clk_enable failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int tegra_dma_pm_suspend(struct device *dev)\r\n{\r\nstruct tegra_dma *tdma = dev_get_drvdata(dev);\r\nint i;\r\nint ret;\r\nret = tegra_dma_runtime_resume(dev);\r\nif (ret < 0)\r\nreturn ret;\r\ntdma->reg_gen = tdma_read(tdma, TEGRA_APBDMA_GENERAL);\r\nfor (i = 0; i < tdma->chip_data->nr_channels; i++) {\r\nstruct tegra_dma_channel *tdc = &tdma->channels[i];\r\nstruct tegra_dma_channel_regs *ch_reg = &tdc->channel_reg;\r\nch_reg->csr = tdc_read(tdc, TEGRA_APBDMA_CHAN_CSR);\r\nch_reg->ahb_ptr = tdc_read(tdc, TEGRA_APBDMA_CHAN_AHBPTR);\r\nch_reg->apb_ptr = tdc_read(tdc, TEGRA_APBDMA_CHAN_APBPTR);\r\nch_reg->ahb_seq = tdc_read(tdc, TEGRA_APBDMA_CHAN_AHBSEQ);\r\nch_reg->apb_seq = tdc_read(tdc, TEGRA_APBDMA_CHAN_APBSEQ);\r\n}\r\ntegra_dma_runtime_suspend(dev);\r\nreturn 0;\r\n}\r\nstatic int tegra_dma_pm_resume(struct device *dev)\r\n{\r\nstruct tegra_dma *tdma = dev_get_drvdata(dev);\r\nint i;\r\nint ret;\r\nret = tegra_dma_runtime_resume(dev);\r\nif (ret < 0)\r\nreturn ret;\r\ntdma_write(tdma, TEGRA_APBDMA_GENERAL, tdma->reg_gen);\r\ntdma_write(tdma, TEGRA_APBDMA_CONTROL, 0);\r\ntdma_write(tdma, TEGRA_APBDMA_IRQ_MASK_SET, 0xFFFFFFFFul);\r\nfor (i = 0; i < tdma->chip_data->nr_channels; i++) {\r\nstruct tegra_dma_channel *tdc = &tdma->channels[i];\r\nstruct tegra_dma_channel_regs *ch_reg = &tdc->channel_reg;\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_APBSEQ, ch_reg->apb_seq);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_APBPTR, ch_reg->apb_ptr);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_AHBSEQ, ch_reg->ahb_seq);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_AHBPTR, ch_reg->ahb_ptr);\r\ntdc_write(tdc, TEGRA_APBDMA_CHAN_CSR,\r\n(ch_reg->csr & ~TEGRA_APBDMA_CSR_ENB));\r\n}\r\ntegra_dma_runtime_suspend(dev);\r\nreturn 0;\r\n}
