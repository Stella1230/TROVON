static void c2_print_macaddr(struct net_device *netdev)\r\n{\r\npr_debug("%s: MAC %pM, IRQ %u\n", netdev->name, netdev->dev_addr, netdev->irq);\r\n}\r\nstatic void c2_set_rxbufsize(struct c2_port *c2_port)\r\n{\r\nstruct net_device *netdev = c2_port->netdev;\r\nif (netdev->mtu > RX_BUF_SIZE)\r\nc2_port->rx_buf_size =\r\nnetdev->mtu + ETH_HLEN + sizeof(struct c2_rxp_hdr) +\r\nNET_IP_ALIGN;\r\nelse\r\nc2_port->rx_buf_size = sizeof(struct c2_rxp_hdr) + RX_BUF_SIZE;\r\n}\r\nstatic int c2_tx_ring_alloc(struct c2_ring *tx_ring, void *vaddr,\r\ndma_addr_t base, void __iomem * mmio_txp_ring)\r\n{\r\nstruct c2_tx_desc *tx_desc;\r\nstruct c2_txp_desc __iomem *txp_desc;\r\nstruct c2_element *elem;\r\nint i;\r\ntx_ring->start = kmalloc(sizeof(*elem) * tx_ring->count, GFP_KERNEL);\r\nif (!tx_ring->start)\r\nreturn -ENOMEM;\r\nelem = tx_ring->start;\r\ntx_desc = vaddr;\r\ntxp_desc = mmio_txp_ring;\r\nfor (i = 0; i < tx_ring->count; i++, elem++, tx_desc++, txp_desc++) {\r\ntx_desc->len = 0;\r\ntx_desc->status = 0;\r\n__raw_writeq((__force u64) cpu_to_be64(0x1122334455667788ULL),\r\n(void __iomem *) txp_desc + C2_TXP_ADDR);\r\n__raw_writew(0, (void __iomem *) txp_desc + C2_TXP_LEN);\r\n__raw_writew((__force u16) cpu_to_be16(TXP_HTXD_UNINIT),\r\n(void __iomem *) txp_desc + C2_TXP_FLAGS);\r\nelem->skb = NULL;\r\nelem->ht_desc = tx_desc;\r\nelem->hw_desc = txp_desc;\r\nif (i == tx_ring->count - 1) {\r\nelem->next = tx_ring->start;\r\ntx_desc->next_offset = base;\r\n} else {\r\nelem->next = elem + 1;\r\ntx_desc->next_offset =\r\nbase + (i + 1) * sizeof(*tx_desc);\r\n}\r\n}\r\ntx_ring->to_use = tx_ring->to_clean = tx_ring->start;\r\nreturn 0;\r\n}\r\nstatic int c2_rx_ring_alloc(struct c2_ring *rx_ring, void *vaddr,\r\ndma_addr_t base, void __iomem * mmio_rxp_ring)\r\n{\r\nstruct c2_rx_desc *rx_desc;\r\nstruct c2_rxp_desc __iomem *rxp_desc;\r\nstruct c2_element *elem;\r\nint i;\r\nrx_ring->start = kmalloc(sizeof(*elem) * rx_ring->count, GFP_KERNEL);\r\nif (!rx_ring->start)\r\nreturn -ENOMEM;\r\nelem = rx_ring->start;\r\nrx_desc = vaddr;\r\nrxp_desc = mmio_rxp_ring;\r\nfor (i = 0; i < rx_ring->count; i++, elem++, rx_desc++, rxp_desc++) {\r\nrx_desc->len = 0;\r\nrx_desc->status = 0;\r\n__raw_writew((__force u16) cpu_to_be16(RXP_HRXD_OK),\r\n(void __iomem *) rxp_desc + C2_RXP_STATUS);\r\n__raw_writew(0, (void __iomem *) rxp_desc + C2_RXP_COUNT);\r\n__raw_writew(0, (void __iomem *) rxp_desc + C2_RXP_LEN);\r\n__raw_writeq((__force u64) cpu_to_be64(0x99aabbccddeeffULL),\r\n(void __iomem *) rxp_desc + C2_RXP_ADDR);\r\n__raw_writew((__force u16) cpu_to_be16(RXP_HRXD_UNINIT),\r\n(void __iomem *) rxp_desc + C2_RXP_FLAGS);\r\nelem->skb = NULL;\r\nelem->ht_desc = rx_desc;\r\nelem->hw_desc = rxp_desc;\r\nif (i == rx_ring->count - 1) {\r\nelem->next = rx_ring->start;\r\nrx_desc->next_offset = base;\r\n} else {\r\nelem->next = elem + 1;\r\nrx_desc->next_offset =\r\nbase + (i + 1) * sizeof(*rx_desc);\r\n}\r\n}\r\nrx_ring->to_use = rx_ring->to_clean = rx_ring->start;\r\nreturn 0;\r\n}\r\nstatic inline int c2_rx_alloc(struct c2_port *c2_port, struct c2_element *elem)\r\n{\r\nstruct c2_dev *c2dev = c2_port->c2dev;\r\nstruct c2_rx_desc *rx_desc = elem->ht_desc;\r\nstruct sk_buff *skb;\r\ndma_addr_t mapaddr;\r\nu32 maplen;\r\nstruct c2_rxp_hdr *rxp_hdr;\r\nskb = dev_alloc_skb(c2_port->rx_buf_size);\r\nif (unlikely(!skb)) {\r\npr_debug("%s: out of memory for receive\n",\r\nc2_port->netdev->name);\r\nreturn -ENOMEM;\r\n}\r\nmemset(skb->data, 0, sizeof(*rxp_hdr));\r\nskb->dev = c2_port->netdev;\r\nmaplen = c2_port->rx_buf_size;\r\nmapaddr =\r\npci_map_single(c2dev->pcidev, skb->data, maplen,\r\nPCI_DMA_FROMDEVICE);\r\nrxp_hdr = (struct c2_rxp_hdr *) skb->data;\r\nrxp_hdr->flags = RXP_HRXD_READY;\r\n__raw_writew(0, elem->hw_desc + C2_RXP_STATUS);\r\n__raw_writew((__force u16) cpu_to_be16((u16) maplen - sizeof(*rxp_hdr)),\r\nelem->hw_desc + C2_RXP_LEN);\r\n__raw_writeq((__force u64) cpu_to_be64(mapaddr), elem->hw_desc + C2_RXP_ADDR);\r\n__raw_writew((__force u16) cpu_to_be16(RXP_HRXD_READY),\r\nelem->hw_desc + C2_RXP_FLAGS);\r\nelem->skb = skb;\r\nelem->mapaddr = mapaddr;\r\nelem->maplen = maplen;\r\nrx_desc->len = maplen;\r\nreturn 0;\r\n}\r\nstatic int c2_rx_fill(struct c2_port *c2_port)\r\n{\r\nstruct c2_ring *rx_ring = &c2_port->rx_ring;\r\nstruct c2_element *elem;\r\nint ret = 0;\r\nelem = rx_ring->start;\r\ndo {\r\nif (c2_rx_alloc(c2_port, elem)) {\r\nret = 1;\r\nbreak;\r\n}\r\n} while ((elem = elem->next) != rx_ring->start);\r\nrx_ring->to_clean = rx_ring->start;\r\nreturn ret;\r\n}\r\nstatic void c2_rx_clean(struct c2_port *c2_port)\r\n{\r\nstruct c2_dev *c2dev = c2_port->c2dev;\r\nstruct c2_ring *rx_ring = &c2_port->rx_ring;\r\nstruct c2_element *elem;\r\nstruct c2_rx_desc *rx_desc;\r\nelem = rx_ring->start;\r\ndo {\r\nrx_desc = elem->ht_desc;\r\nrx_desc->len = 0;\r\n__raw_writew(0, elem->hw_desc + C2_RXP_STATUS);\r\n__raw_writew(0, elem->hw_desc + C2_RXP_COUNT);\r\n__raw_writew(0, elem->hw_desc + C2_RXP_LEN);\r\n__raw_writeq((__force u64) cpu_to_be64(0x99aabbccddeeffULL),\r\nelem->hw_desc + C2_RXP_ADDR);\r\n__raw_writew((__force u16) cpu_to_be16(RXP_HRXD_UNINIT),\r\nelem->hw_desc + C2_RXP_FLAGS);\r\nif (elem->skb) {\r\npci_unmap_single(c2dev->pcidev, elem->mapaddr,\r\nelem->maplen, PCI_DMA_FROMDEVICE);\r\ndev_kfree_skb(elem->skb);\r\nelem->skb = NULL;\r\n}\r\n} while ((elem = elem->next) != rx_ring->start);\r\n}\r\nstatic inline int c2_tx_free(struct c2_dev *c2dev, struct c2_element *elem)\r\n{\r\nstruct c2_tx_desc *tx_desc = elem->ht_desc;\r\ntx_desc->len = 0;\r\npci_unmap_single(c2dev->pcidev, elem->mapaddr, elem->maplen,\r\nPCI_DMA_TODEVICE);\r\nif (elem->skb) {\r\ndev_kfree_skb_any(elem->skb);\r\nelem->skb = NULL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void c2_tx_clean(struct c2_port *c2_port)\r\n{\r\nstruct c2_ring *tx_ring = &c2_port->tx_ring;\r\nstruct c2_element *elem;\r\nstruct c2_txp_desc txp_htxd;\r\nint retry;\r\nunsigned long flags;\r\nspin_lock_irqsave(&c2_port->tx_lock, flags);\r\nelem = tx_ring->start;\r\ndo {\r\nretry = 0;\r\ndo {\r\ntxp_htxd.flags =\r\nreadw(elem->hw_desc + C2_TXP_FLAGS);\r\nif (txp_htxd.flags == TXP_HTXD_READY) {\r\nretry = 1;\r\n__raw_writew(0,\r\nelem->hw_desc + C2_TXP_LEN);\r\n__raw_writeq(0,\r\nelem->hw_desc + C2_TXP_ADDR);\r\n__raw_writew((__force u16) cpu_to_be16(TXP_HTXD_DONE),\r\nelem->hw_desc + C2_TXP_FLAGS);\r\nc2_port->netdev->stats.tx_dropped++;\r\nbreak;\r\n} else {\r\n__raw_writew(0,\r\nelem->hw_desc + C2_TXP_LEN);\r\n__raw_writeq((__force u64) cpu_to_be64(0x1122334455667788ULL),\r\nelem->hw_desc + C2_TXP_ADDR);\r\n__raw_writew((__force u16) cpu_to_be16(TXP_HTXD_UNINIT),\r\nelem->hw_desc + C2_TXP_FLAGS);\r\n}\r\nc2_tx_free(c2_port->c2dev, elem);\r\n} while ((elem = elem->next) != tx_ring->start);\r\n} while (retry);\r\nc2_port->tx_avail = c2_port->tx_ring.count - 1;\r\nc2_port->c2dev->cur_tx = tx_ring->to_use - tx_ring->start;\r\nif (c2_port->tx_avail > MAX_SKB_FRAGS + 1)\r\nnetif_wake_queue(c2_port->netdev);\r\nspin_unlock_irqrestore(&c2_port->tx_lock, flags);\r\n}\r\nstatic void c2_tx_interrupt(struct net_device *netdev)\r\n{\r\nstruct c2_port *c2_port = netdev_priv(netdev);\r\nstruct c2_dev *c2dev = c2_port->c2dev;\r\nstruct c2_ring *tx_ring = &c2_port->tx_ring;\r\nstruct c2_element *elem;\r\nstruct c2_txp_desc txp_htxd;\r\nspin_lock(&c2_port->tx_lock);\r\nfor (elem = tx_ring->to_clean; elem != tx_ring->to_use;\r\nelem = elem->next) {\r\ntxp_htxd.flags =\r\nbe16_to_cpu((__force __be16) readw(elem->hw_desc + C2_TXP_FLAGS));\r\nif (txp_htxd.flags != TXP_HTXD_DONE)\r\nbreak;\r\nif (netif_msg_tx_done(c2_port)) {\r\ntxp_htxd.len =\r\nbe16_to_cpu((__force __be16) readw(elem->hw_desc + C2_TXP_LEN));\r\npr_debug("%s: tx done slot %3Zu status 0x%x len "\r\n"%5u bytes\n",\r\nnetdev->name, elem - tx_ring->start,\r\ntxp_htxd.flags, txp_htxd.len);\r\n}\r\nc2_tx_free(c2dev, elem);\r\n++(c2_port->tx_avail);\r\n}\r\ntx_ring->to_clean = elem;\r\nif (netif_queue_stopped(netdev)\r\n&& c2_port->tx_avail > MAX_SKB_FRAGS + 1)\r\nnetif_wake_queue(netdev);\r\nspin_unlock(&c2_port->tx_lock);\r\n}\r\nstatic void c2_rx_error(struct c2_port *c2_port, struct c2_element *elem)\r\n{\r\nstruct c2_rx_desc *rx_desc = elem->ht_desc;\r\nstruct c2_rxp_hdr *rxp_hdr = (struct c2_rxp_hdr *) elem->skb->data;\r\nif (rxp_hdr->status != RXP_HRXD_OK ||\r\nrxp_hdr->len > (rx_desc->len - sizeof(*rxp_hdr))) {\r\npr_debug("BAD RXP_HRXD\n");\r\npr_debug(" rx_desc : %p\n", rx_desc);\r\npr_debug(" index : %Zu\n",\r\nelem - c2_port->rx_ring.start);\r\npr_debug(" len : %u\n", rx_desc->len);\r\npr_debug(" rxp_hdr : %p [PA %p]\n", rxp_hdr,\r\n(void *) __pa((unsigned long) rxp_hdr));\r\npr_debug(" flags : 0x%x\n", rxp_hdr->flags);\r\npr_debug(" status: 0x%x\n", rxp_hdr->status);\r\npr_debug(" len : %u\n", rxp_hdr->len);\r\npr_debug(" rsvd : 0x%x\n", rxp_hdr->rsvd);\r\n}\r\nelem->skb->data = elem->skb->head;\r\nskb_reset_tail_pointer(elem->skb);\r\nmemset(elem->skb->data, 0, sizeof(*rxp_hdr));\r\n__raw_writew(0, elem->hw_desc + C2_RXP_STATUS);\r\n__raw_writew(0, elem->hw_desc + C2_RXP_COUNT);\r\n__raw_writew((__force u16) cpu_to_be16((u16) elem->maplen - sizeof(*rxp_hdr)),\r\nelem->hw_desc + C2_RXP_LEN);\r\n__raw_writeq((__force u64) cpu_to_be64(elem->mapaddr),\r\nelem->hw_desc + C2_RXP_ADDR);\r\n__raw_writew((__force u16) cpu_to_be16(RXP_HRXD_READY),\r\nelem->hw_desc + C2_RXP_FLAGS);\r\npr_debug("packet dropped\n");\r\nc2_port->netdev->stats.rx_dropped++;\r\n}\r\nstatic void c2_rx_interrupt(struct net_device *netdev)\r\n{\r\nstruct c2_port *c2_port = netdev_priv(netdev);\r\nstruct c2_dev *c2dev = c2_port->c2dev;\r\nstruct c2_ring *rx_ring = &c2_port->rx_ring;\r\nstruct c2_element *elem;\r\nstruct c2_rx_desc *rx_desc;\r\nstruct c2_rxp_hdr *rxp_hdr;\r\nstruct sk_buff *skb;\r\ndma_addr_t mapaddr;\r\nu32 maplen, buflen;\r\nunsigned long flags;\r\nspin_lock_irqsave(&c2dev->lock, flags);\r\nrx_ring->to_clean = rx_ring->start + c2dev->cur_rx;\r\nfor (elem = rx_ring->to_clean; elem->next != rx_ring->to_clean;\r\nelem = elem->next) {\r\nrx_desc = elem->ht_desc;\r\nmapaddr = elem->mapaddr;\r\nmaplen = elem->maplen;\r\nskb = elem->skb;\r\nrxp_hdr = (struct c2_rxp_hdr *) skb->data;\r\nif (rxp_hdr->flags != RXP_HRXD_DONE)\r\nbreak;\r\nbuflen = rxp_hdr->len;\r\nif (rxp_hdr->status != RXP_HRXD_OK ||\r\nbuflen > (rx_desc->len - sizeof(*rxp_hdr))) {\r\nc2_rx_error(c2_port, elem);\r\ncontinue;\r\n}\r\nif (c2_rx_alloc(c2_port, elem)) {\r\nc2_rx_error(c2_port, elem);\r\ncontinue;\r\n}\r\npci_unmap_single(c2dev->pcidev, mapaddr, maplen,\r\nPCI_DMA_FROMDEVICE);\r\nprefetch(skb->data);\r\nskb->data += sizeof(*rxp_hdr);\r\nskb_set_tail_pointer(skb, buflen);\r\nskb->len = buflen;\r\nskb->protocol = eth_type_trans(skb, netdev);\r\nnetif_rx(skb);\r\nnetdev->stats.rx_packets++;\r\nnetdev->stats.rx_bytes += buflen;\r\n}\r\nrx_ring->to_clean = elem;\r\nc2dev->cur_rx = elem - rx_ring->start;\r\nC2_SET_CUR_RX(c2dev, c2dev->cur_rx);\r\nspin_unlock_irqrestore(&c2dev->lock, flags);\r\n}\r\nstatic irqreturn_t c2_interrupt(int irq, void *dev_id)\r\n{\r\nunsigned int netisr0, dmaisr;\r\nint handled = 0;\r\nstruct c2_dev *c2dev = (struct c2_dev *) dev_id;\r\nnetisr0 = readl(c2dev->regs + C2_NISR0);\r\nif (netisr0) {\r\nc2_rx_interrupt(c2dev->netdev);\r\nc2_tx_interrupt(c2dev->netdev);\r\nwritel(netisr0, c2dev->regs + C2_NISR0);\r\nhandled++;\r\n}\r\ndmaisr = readl(c2dev->regs + C2_DISR);\r\nif (dmaisr) {\r\nwritel(dmaisr, c2dev->regs + C2_DISR);\r\nc2_rnic_interrupt(c2dev);\r\nhandled++;\r\n}\r\nif (handled) {\r\nreturn IRQ_HANDLED;\r\n} else {\r\nreturn IRQ_NONE;\r\n}\r\n}\r\nstatic int c2_up(struct net_device *netdev)\r\n{\r\nstruct c2_port *c2_port = netdev_priv(netdev);\r\nstruct c2_dev *c2dev = c2_port->c2dev;\r\nstruct c2_element *elem;\r\nstruct c2_rxp_hdr *rxp_hdr;\r\nstruct in_device *in_dev;\r\nsize_t rx_size, tx_size;\r\nint ret, i;\r\nunsigned int netimr0;\r\nif (netif_msg_ifup(c2_port))\r\npr_debug("%s: enabling interface\n", netdev->name);\r\nc2_set_rxbufsize(c2_port);\r\nrx_size = c2_port->rx_ring.count * sizeof(struct c2_rx_desc);\r\ntx_size = c2_port->tx_ring.count * sizeof(struct c2_tx_desc);\r\nc2_port->mem_size = tx_size + rx_size;\r\nc2_port->mem = pci_alloc_consistent(c2dev->pcidev, c2_port->mem_size,\r\n&c2_port->dma);\r\nif (c2_port->mem == NULL) {\r\npr_debug("Unable to allocate memory for "\r\n"host descriptor rings\n");\r\nreturn -ENOMEM;\r\n}\r\nmemset(c2_port->mem, 0, c2_port->mem_size);\r\nif ((ret =\r\nc2_rx_ring_alloc(&c2_port->rx_ring, c2_port->mem, c2_port->dma,\r\nc2dev->mmio_rxp_ring))) {\r\npr_debug("Unable to create RX ring\n");\r\ngoto bail0;\r\n}\r\nif (c2_rx_fill(c2_port)) {\r\npr_debug("Unable to fill RX ring\n");\r\ngoto bail1;\r\n}\r\nif ((ret = c2_tx_ring_alloc(&c2_port->tx_ring, c2_port->mem + rx_size,\r\nc2_port->dma + rx_size,\r\nc2dev->mmio_txp_ring))) {\r\npr_debug("Unable to create TX ring\n");\r\ngoto bail1;\r\n}\r\nc2_port->tx_avail = c2_port->tx_ring.count - 1;\r\nc2_port->tx_ring.to_use = c2_port->tx_ring.to_clean =\r\nc2_port->tx_ring.start + c2dev->cur_tx;\r\nBUG_ON(c2_port->tx_ring.to_use != c2_port->tx_ring.to_clean);\r\nc2_reset(c2_port);\r\nfor (i = 0, elem = c2_port->rx_ring.start; i < c2_port->rx_ring.count;\r\ni++, elem++) {\r\nrxp_hdr = (struct c2_rxp_hdr *) elem->skb->data;\r\nrxp_hdr->flags = 0;\r\n__raw_writew((__force u16) cpu_to_be16(RXP_HRXD_READY),\r\nelem->hw_desc + C2_RXP_FLAGS);\r\n}\r\nnetif_start_queue(netdev);\r\nwritel(0, c2dev->regs + C2_IDIS);\r\nnetimr0 = readl(c2dev->regs + C2_NIMR0);\r\nnetimr0 &= ~(C2_PCI_HTX_INT | C2_PCI_HRX_INT);\r\nwritel(netimr0, c2dev->regs + C2_NIMR0);\r\nin_dev = in_dev_get(netdev);\r\nIN_DEV_CONF_SET(in_dev, ARP_IGNORE, 1);\r\nin_dev_put(in_dev);\r\nreturn 0;\r\nbail1:\r\nc2_rx_clean(c2_port);\r\nkfree(c2_port->rx_ring.start);\r\nbail0:\r\npci_free_consistent(c2dev->pcidev, c2_port->mem_size, c2_port->mem,\r\nc2_port->dma);\r\nreturn ret;\r\n}\r\nstatic int c2_down(struct net_device *netdev)\r\n{\r\nstruct c2_port *c2_port = netdev_priv(netdev);\r\nstruct c2_dev *c2dev = c2_port->c2dev;\r\nif (netif_msg_ifdown(c2_port))\r\npr_debug("%s: disabling interface\n",\r\nnetdev->name);\r\nc2_tx_interrupt(netdev);\r\nnetif_stop_queue(netdev);\r\nwritel(1, c2dev->regs + C2_IDIS);\r\nwritel(0, c2dev->regs + C2_NIMR0);\r\nc2_reset(c2_port);\r\nc2_tx_clean(c2_port);\r\nc2_rx_clean(c2_port);\r\nkfree(c2_port->rx_ring.start);\r\nkfree(c2_port->tx_ring.start);\r\npci_free_consistent(c2dev->pcidev, c2_port->mem_size, c2_port->mem,\r\nc2_port->dma);\r\nreturn 0;\r\n}\r\nstatic void c2_reset(struct c2_port *c2_port)\r\n{\r\nstruct c2_dev *c2dev = c2_port->c2dev;\r\nunsigned int cur_rx = c2dev->cur_rx;\r\nC2_SET_CUR_RX(c2dev, cur_rx | C2_PCI_HRX_QUI);\r\nssleep(2);\r\ncur_rx = C2_GET_CUR_RX(c2dev);\r\nif (cur_rx & C2_PCI_HRX_QUI)\r\npr_debug("c2_reset: failed to quiesce the hardware!\n");\r\ncur_rx &= ~C2_PCI_HRX_QUI;\r\nc2dev->cur_rx = cur_rx;\r\npr_debug("Current RX: %u\n", c2dev->cur_rx);\r\n}\r\nstatic int c2_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct c2_port *c2_port = netdev_priv(netdev);\r\nstruct c2_dev *c2dev = c2_port->c2dev;\r\nstruct c2_ring *tx_ring = &c2_port->tx_ring;\r\nstruct c2_element *elem;\r\ndma_addr_t mapaddr;\r\nu32 maplen;\r\nunsigned long flags;\r\nunsigned int i;\r\nspin_lock_irqsave(&c2_port->tx_lock, flags);\r\nif (unlikely(c2_port->tx_avail < (skb_shinfo(skb)->nr_frags + 1))) {\r\nnetif_stop_queue(netdev);\r\nspin_unlock_irqrestore(&c2_port->tx_lock, flags);\r\npr_debug("%s: Tx ring full when queue awake!\n",\r\nnetdev->name);\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nmaplen = skb_headlen(skb);\r\nmapaddr =\r\npci_map_single(c2dev->pcidev, skb->data, maplen, PCI_DMA_TODEVICE);\r\nelem = tx_ring->to_use;\r\nelem->skb = skb;\r\nelem->mapaddr = mapaddr;\r\nelem->maplen = maplen;\r\n__raw_writeq((__force u64) cpu_to_be64(mapaddr),\r\nelem->hw_desc + C2_TXP_ADDR);\r\n__raw_writew((__force u16) cpu_to_be16(maplen),\r\nelem->hw_desc + C2_TXP_LEN);\r\n__raw_writew((__force u16) cpu_to_be16(TXP_HTXD_READY),\r\nelem->hw_desc + C2_TXP_FLAGS);\r\nnetdev->stats.tx_packets++;\r\nnetdev->stats.tx_bytes += maplen;\r\nif (skb_shinfo(skb)->nr_frags) {\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\nmaplen = skb_frag_size(frag);\r\nmapaddr = skb_frag_dma_map(&c2dev->pcidev->dev, frag,\r\n0, maplen, DMA_TO_DEVICE);\r\nelem = elem->next;\r\nelem->skb = NULL;\r\nelem->mapaddr = mapaddr;\r\nelem->maplen = maplen;\r\n__raw_writeq((__force u64) cpu_to_be64(mapaddr),\r\nelem->hw_desc + C2_TXP_ADDR);\r\n__raw_writew((__force u16) cpu_to_be16(maplen),\r\nelem->hw_desc + C2_TXP_LEN);\r\n__raw_writew((__force u16) cpu_to_be16(TXP_HTXD_READY),\r\nelem->hw_desc + C2_TXP_FLAGS);\r\nnetdev->stats.tx_packets++;\r\nnetdev->stats.tx_bytes += maplen;\r\n}\r\n}\r\ntx_ring->to_use = elem->next;\r\nc2_port->tx_avail -= (skb_shinfo(skb)->nr_frags + 1);\r\nif (c2_port->tx_avail <= MAX_SKB_FRAGS + 1) {\r\nnetif_stop_queue(netdev);\r\nif (netif_msg_tx_queued(c2_port))\r\npr_debug("%s: transmit queue full\n",\r\nnetdev->name);\r\n}\r\nspin_unlock_irqrestore(&c2_port->tx_lock, flags);\r\nnetdev->trans_start = jiffies;\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void c2_tx_timeout(struct net_device *netdev)\r\n{\r\nstruct c2_port *c2_port = netdev_priv(netdev);\r\nif (netif_msg_timer(c2_port))\r\npr_debug("%s: tx timeout\n", netdev->name);\r\nc2_tx_clean(c2_port);\r\n}\r\nstatic int c2_change_mtu(struct net_device *netdev, int new_mtu)\r\n{\r\nint ret = 0;\r\nif (new_mtu < ETH_ZLEN || new_mtu > ETH_JUMBO_MTU)\r\nreturn -EINVAL;\r\nnetdev->mtu = new_mtu;\r\nif (netif_running(netdev)) {\r\nc2_down(netdev);\r\nc2_up(netdev);\r\n}\r\nreturn ret;\r\n}\r\nstatic struct net_device *c2_devinit(struct c2_dev *c2dev,\r\nvoid __iomem * mmio_addr)\r\n{\r\nstruct c2_port *c2_port = NULL;\r\nstruct net_device *netdev = alloc_etherdev(sizeof(*c2_port));\r\nif (!netdev) {\r\npr_debug("c2_port etherdev alloc failed");\r\nreturn NULL;\r\n}\r\nSET_NETDEV_DEV(netdev, &c2dev->pcidev->dev);\r\nnetdev->netdev_ops = &c2_netdev;\r\nnetdev->watchdog_timeo = C2_TX_TIMEOUT;\r\nnetdev->irq = c2dev->pcidev->irq;\r\nc2_port = netdev_priv(netdev);\r\nc2_port->netdev = netdev;\r\nc2_port->c2dev = c2dev;\r\nc2_port->msg_enable = netif_msg_init(debug, default_msg);\r\nc2_port->tx_ring.count = C2_NUM_TX_DESC;\r\nc2_port->rx_ring.count = C2_NUM_RX_DESC;\r\nspin_lock_init(&c2_port->tx_lock);\r\nmemcpy_fromio(netdev->dev_addr, mmio_addr + C2_REGS_ENADDR, 6);\r\nif (!is_valid_ether_addr(netdev->dev_addr)) {\r\npr_debug("Invalid MAC Address\n");\r\nc2_print_macaddr(netdev);\r\nfree_netdev(netdev);\r\nreturn NULL;\r\n}\r\nc2dev->netdev = netdev;\r\nreturn netdev;\r\n}\r\nstatic int c2_probe(struct pci_dev *pcidev, const struct pci_device_id *ent)\r\n{\r\nint ret = 0, i;\r\nunsigned long reg0_start, reg0_flags, reg0_len;\r\nunsigned long reg2_start, reg2_flags, reg2_len;\r\nunsigned long reg4_start, reg4_flags, reg4_len;\r\nunsigned kva_map_size;\r\nstruct net_device *netdev = NULL;\r\nstruct c2_dev *c2dev = NULL;\r\nvoid __iomem *mmio_regs = NULL;\r\nprintk(KERN_INFO PFX "AMSO1100 Gigabit Ethernet driver v%s loaded\n",\r\nDRV_VERSION);\r\nret = pci_enable_device(pcidev);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "%s: Unable to enable PCI device\n",\r\npci_name(pcidev));\r\ngoto bail0;\r\n}\r\nreg0_start = pci_resource_start(pcidev, BAR_0);\r\nreg0_len = pci_resource_len(pcidev, BAR_0);\r\nreg0_flags = pci_resource_flags(pcidev, BAR_0);\r\nreg2_start = pci_resource_start(pcidev, BAR_2);\r\nreg2_len = pci_resource_len(pcidev, BAR_2);\r\nreg2_flags = pci_resource_flags(pcidev, BAR_2);\r\nreg4_start = pci_resource_start(pcidev, BAR_4);\r\nreg4_len = pci_resource_len(pcidev, BAR_4);\r\nreg4_flags = pci_resource_flags(pcidev, BAR_4);\r\npr_debug("BAR0 size = 0x%lX bytes\n", reg0_len);\r\npr_debug("BAR2 size = 0x%lX bytes\n", reg2_len);\r\npr_debug("BAR4 size = 0x%lX bytes\n", reg4_len);\r\nif (!(reg0_flags & IORESOURCE_MEM) ||\r\n!(reg2_flags & IORESOURCE_MEM) || !(reg4_flags & IORESOURCE_MEM)) {\r\nprintk(KERN_ERR PFX "PCI regions not an MMIO resource\n");\r\nret = -ENODEV;\r\ngoto bail1;\r\n}\r\nif ((reg0_len < C2_REG0_SIZE) ||\r\n(reg2_len < C2_REG2_SIZE) || (reg4_len < C2_REG4_SIZE)) {\r\nprintk(KERN_ERR PFX "Invalid PCI region sizes\n");\r\nret = -ENODEV;\r\ngoto bail1;\r\n}\r\nret = pci_request_regions(pcidev, DRV_NAME);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "%s: Unable to request regions\n",\r\npci_name(pcidev));\r\ngoto bail1;\r\n}\r\nif ((sizeof(dma_addr_t) > 4)) {\r\nret = pci_set_dma_mask(pcidev, DMA_BIT_MASK(64));\r\nif (ret < 0) {\r\nprintk(KERN_ERR PFX "64b DMA configuration failed\n");\r\ngoto bail2;\r\n}\r\n} else {\r\nret = pci_set_dma_mask(pcidev, DMA_BIT_MASK(32));\r\nif (ret < 0) {\r\nprintk(KERN_ERR PFX "32b DMA configuration failed\n");\r\ngoto bail2;\r\n}\r\n}\r\npci_set_master(pcidev);\r\nmmio_regs = ioremap_nocache(reg4_start + C2_PCI_REGS_OFFSET,\r\nsizeof(struct c2_adapter_pci_regs));\r\nif (!mmio_regs) {\r\nprintk(KERN_ERR PFX\r\n"Unable to remap adapter PCI registers in BAR4\n");\r\nret = -EIO;\r\ngoto bail2;\r\n}\r\nfor (i = 0; i < sizeof(c2_magic); i++) {\r\nif (c2_magic[i] != readb(mmio_regs + C2_REGS_MAGIC + i)) {\r\nprintk(KERN_ERR PFX "Downlevel Firmware boot loader "\r\n"[%d/%Zd: got 0x%x, exp 0x%x]. Use the cc_flash "\r\n"utility to update your boot loader\n",\r\ni + 1, sizeof(c2_magic),\r\nreadb(mmio_regs + C2_REGS_MAGIC + i),\r\nc2_magic[i]);\r\nprintk(KERN_ERR PFX "Adapter not claimed\n");\r\niounmap(mmio_regs);\r\nret = -EIO;\r\ngoto bail2;\r\n}\r\n}\r\nif (be32_to_cpu((__force __be32) readl(mmio_regs + C2_REGS_VERS)) != C2_VERSION) {\r\nprintk(KERN_ERR PFX "Version mismatch "\r\n"[fw=%u, c2=%u], Adapter not claimed\n",\r\nbe32_to_cpu((__force __be32) readl(mmio_regs + C2_REGS_VERS)),\r\nC2_VERSION);\r\nret = -EINVAL;\r\niounmap(mmio_regs);\r\ngoto bail2;\r\n}\r\nif (be32_to_cpu((__force __be32) readl(mmio_regs + C2_REGS_IVN)) != C2_IVN) {\r\nprintk(KERN_ERR PFX "Downlevel FIrmware level. You should be using "\r\n"the OpenIB device support kit. "\r\n"[fw=0x%x, c2=0x%x], Adapter not claimed\n",\r\nbe32_to_cpu((__force __be32) readl(mmio_regs + C2_REGS_IVN)),\r\nC2_IVN);\r\nret = -EINVAL;\r\niounmap(mmio_regs);\r\ngoto bail2;\r\n}\r\nc2dev = (struct c2_dev *) ib_alloc_device(sizeof(*c2dev));\r\nif (!c2dev) {\r\nprintk(KERN_ERR PFX "%s: Unable to alloc hardware struct\n",\r\npci_name(pcidev));\r\nret = -ENOMEM;\r\niounmap(mmio_regs);\r\ngoto bail2;\r\n}\r\nmemset(c2dev, 0, sizeof(*c2dev));\r\nspin_lock_init(&c2dev->lock);\r\nc2dev->pcidev = pcidev;\r\nc2dev->cur_tx = 0;\r\nc2dev->cur_rx =\r\n(be32_to_cpu((__force __be32) readl(mmio_regs + C2_REGS_HRX_CUR)) -\r\n0xffffc000) / sizeof(struct c2_rxp_desc);\r\nret = request_irq(pcidev->irq, c2_interrupt, IRQF_SHARED, DRV_NAME, c2dev);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "%s: requested IRQ %u is busy\n",\r\npci_name(pcidev), pcidev->irq);\r\niounmap(mmio_regs);\r\ngoto bail3;\r\n}\r\npci_set_drvdata(pcidev, c2dev);\r\nif ((netdev = c2_devinit(c2dev, mmio_regs)) == NULL) {\r\nret = -ENOMEM;\r\niounmap(mmio_regs);\r\ngoto bail4;\r\n}\r\nkva_map_size = be32_to_cpu((__force __be32) readl(mmio_regs + C2_REGS_PCI_WINSIZE));\r\niounmap(mmio_regs);\r\nret = register_netdev(netdev);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "Unable to register netdev, ret = %d\n",\r\nret);\r\ngoto bail5;\r\n}\r\nnetif_stop_queue(netdev);\r\nc2dev->mmio_rxp_ring = ioremap_nocache(reg4_start + C2_RXP_HRXDQ_OFFSET,\r\nC2_RXP_HRXDQ_SIZE);\r\nif (!c2dev->mmio_rxp_ring) {\r\nprintk(KERN_ERR PFX "Unable to remap MMIO HRXDQ region\n");\r\nret = -EIO;\r\ngoto bail6;\r\n}\r\nc2dev->mmio_txp_ring = ioremap_nocache(reg4_start + C2_TXP_HTXDQ_OFFSET,\r\nC2_TXP_HTXDQ_SIZE);\r\nif (!c2dev->mmio_txp_ring) {\r\nprintk(KERN_ERR PFX "Unable to remap MMIO HTXDQ region\n");\r\nret = -EIO;\r\ngoto bail7;\r\n}\r\nC2_SET_CUR_RX(c2dev, c2dev->cur_rx);\r\nc2dev->regs = ioremap_nocache(reg0_start, reg0_len);\r\nif (!c2dev->regs) {\r\nprintk(KERN_ERR PFX "Unable to remap BAR0\n");\r\nret = -EIO;\r\ngoto bail8;\r\n}\r\nc2dev->pa = reg4_start + C2_PCI_REGS_OFFSET;\r\nc2dev->kva = ioremap_nocache(reg4_start + C2_PCI_REGS_OFFSET,\r\nkva_map_size);\r\nif (!c2dev->kva) {\r\nprintk(KERN_ERR PFX "Unable to remap BAR4\n");\r\nret = -EIO;\r\ngoto bail9;\r\n}\r\nc2_print_macaddr(netdev);\r\nret = c2_rnic_init(c2dev);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "c2_rnic_init failed: %d\n", ret);\r\ngoto bail10;\r\n}\r\nret = c2_register_device(c2dev);\r\nif (ret)\r\ngoto bail10;\r\nreturn 0;\r\nbail10:\r\niounmap(c2dev->kva);\r\nbail9:\r\niounmap(c2dev->regs);\r\nbail8:\r\niounmap(c2dev->mmio_txp_ring);\r\nbail7:\r\niounmap(c2dev->mmio_rxp_ring);\r\nbail6:\r\nunregister_netdev(netdev);\r\nbail5:\r\nfree_netdev(netdev);\r\nbail4:\r\nfree_irq(pcidev->irq, c2dev);\r\nbail3:\r\nib_dealloc_device(&c2dev->ibdev);\r\nbail2:\r\npci_release_regions(pcidev);\r\nbail1:\r\npci_disable_device(pcidev);\r\nbail0:\r\nreturn ret;\r\n}\r\nstatic void c2_remove(struct pci_dev *pcidev)\r\n{\r\nstruct c2_dev *c2dev = pci_get_drvdata(pcidev);\r\nstruct net_device *netdev = c2dev->netdev;\r\nc2_unregister_device(c2dev);\r\nc2_rnic_term(c2dev);\r\nunregister_netdev(netdev);\r\nfree_netdev(netdev);\r\nfree_irq(pcidev->irq, c2dev);\r\niounmap(c2dev->kva);\r\niounmap(c2dev->regs);\r\niounmap(c2dev->mmio_txp_ring);\r\niounmap(c2dev->mmio_rxp_ring);\r\nib_dealloc_device(&c2dev->ibdev);\r\npci_release_regions(pcidev);\r\npci_disable_device(pcidev);\r\npci_set_drvdata(pcidev, NULL);\r\n}
