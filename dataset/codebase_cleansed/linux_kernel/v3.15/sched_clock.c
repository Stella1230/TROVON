static u64 notrace jiffy_sched_clock_read(void)\r\n{\r\nreturn (u64)(jiffies - INITIAL_JIFFIES);\r\n}\r\nstatic u64 notrace read_sched_clock_32_wrapper(void)\r\n{\r\nreturn read_sched_clock_32();\r\n}\r\nstatic inline u64 notrace cyc_to_ns(u64 cyc, u32 mult, u32 shift)\r\n{\r\nreturn (cyc * mult) >> shift;\r\n}\r\nunsigned long long notrace sched_clock(void)\r\n{\r\nu64 epoch_ns;\r\nu64 epoch_cyc;\r\nu64 cyc;\r\nunsigned long seq;\r\nif (cd.suspended)\r\nreturn cd.epoch_ns;\r\ndo {\r\nseq = raw_read_seqcount_begin(&cd.seq);\r\nepoch_cyc = cd.epoch_cyc;\r\nepoch_ns = cd.epoch_ns;\r\n} while (read_seqcount_retry(&cd.seq, seq));\r\ncyc = read_sched_clock();\r\ncyc = (cyc - epoch_cyc) & sched_clock_mask;\r\nreturn epoch_ns + cyc_to_ns(cyc, cd.mult, cd.shift);\r\n}\r\nstatic void notrace update_sched_clock(void)\r\n{\r\nunsigned long flags;\r\nu64 cyc;\r\nu64 ns;\r\ncyc = read_sched_clock();\r\nns = cd.epoch_ns +\r\ncyc_to_ns((cyc - cd.epoch_cyc) & sched_clock_mask,\r\ncd.mult, cd.shift);\r\nraw_local_irq_save(flags);\r\nraw_write_seqcount_begin(&cd.seq);\r\ncd.epoch_ns = ns;\r\ncd.epoch_cyc = cyc;\r\nraw_write_seqcount_end(&cd.seq);\r\nraw_local_irq_restore(flags);\r\n}\r\nstatic enum hrtimer_restart sched_clock_poll(struct hrtimer *hrt)\r\n{\r\nupdate_sched_clock();\r\nhrtimer_forward_now(hrt, cd.wrap_kt);\r\nreturn HRTIMER_RESTART;\r\n}\r\nvoid __init sched_clock_register(u64 (*read)(void), int bits,\r\nunsigned long rate)\r\n{\r\nu64 res, wrap, new_mask, new_epoch, cyc, ns;\r\nu32 new_mult, new_shift;\r\nktime_t new_wrap_kt;\r\nunsigned long r;\r\nchar r_unit;\r\nif (cd.rate > rate)\r\nreturn;\r\nWARN_ON(!irqs_disabled());\r\nclocks_calc_mult_shift(&new_mult, &new_shift, rate, NSEC_PER_SEC, 3600);\r\nnew_mask = CLOCKSOURCE_MASK(bits);\r\nwrap = clocks_calc_max_nsecs(new_mult, new_shift, 0, new_mask);\r\nnew_wrap_kt = ns_to_ktime(wrap - (wrap >> 3));\r\nnew_epoch = read();\r\ncyc = read_sched_clock();\r\nns = cd.epoch_ns + cyc_to_ns((cyc - cd.epoch_cyc) & sched_clock_mask,\r\ncd.mult, cd.shift);\r\nraw_write_seqcount_begin(&cd.seq);\r\nread_sched_clock = read;\r\nsched_clock_mask = new_mask;\r\ncd.rate = rate;\r\ncd.wrap_kt = new_wrap_kt;\r\ncd.mult = new_mult;\r\ncd.shift = new_shift;\r\ncd.epoch_cyc = new_epoch;\r\ncd.epoch_ns = ns;\r\nraw_write_seqcount_end(&cd.seq);\r\nr = rate;\r\nif (r >= 4000000) {\r\nr /= 1000000;\r\nr_unit = 'M';\r\n} else if (r >= 1000) {\r\nr /= 1000;\r\nr_unit = 'k';\r\n} else\r\nr_unit = ' ';\r\nres = cyc_to_ns(1ULL, new_mult, new_shift);\r\npr_info("sched_clock: %u bits at %lu%cHz, resolution %lluns, wraps every %lluns\n",\r\nbits, r, r_unit, res, wrap);\r\nif (irqtime > 0 || (irqtime == -1 && rate >= 1000000))\r\nenable_sched_clock_irqtime();\r\npr_debug("Registered %pF as sched_clock source\n", read);\r\n}\r\nvoid __init setup_sched_clock(u32 (*read)(void), int bits, unsigned long rate)\r\n{\r\nread_sched_clock_32 = read;\r\nsched_clock_register(read_sched_clock_32_wrapper, bits, rate);\r\n}\r\nvoid __init sched_clock_postinit(void)\r\n{\r\nif (read_sched_clock == jiffy_sched_clock_read)\r\nsched_clock_register(jiffy_sched_clock_read, BITS_PER_LONG, HZ);\r\nupdate_sched_clock();\r\nhrtimer_init(&sched_clock_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nsched_clock_timer.function = sched_clock_poll;\r\nhrtimer_start(&sched_clock_timer, cd.wrap_kt, HRTIMER_MODE_REL);\r\n}\r\nstatic int sched_clock_suspend(void)\r\n{\r\nsched_clock_poll(&sched_clock_timer);\r\ncd.suspended = true;\r\nreturn 0;\r\n}\r\nstatic void sched_clock_resume(void)\r\n{\r\ncd.epoch_cyc = read_sched_clock();\r\ncd.suspended = false;\r\n}\r\nstatic int __init sched_clock_syscore_init(void)\r\n{\r\nregister_syscore_ops(&sched_clock_ops);\r\nreturn 0;\r\n}
