static int ccp_aes_xts_complete(struct crypto_async_request *async_req, int ret)\r\n{\r\nstruct ablkcipher_request *req = ablkcipher_request_cast(async_req);\r\nstruct ccp_aes_req_ctx *rctx = ablkcipher_request_ctx(req);\r\nif (ret)\r\nreturn ret;\r\nmemcpy(req->info, rctx->iv, AES_BLOCK_SIZE);\r\nreturn 0;\r\n}\r\nstatic int ccp_aes_xts_setkey(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int key_len)\r\n{\r\nstruct ccp_ctx *ctx = crypto_tfm_ctx(crypto_ablkcipher_tfm(tfm));\r\nswitch (key_len) {\r\ncase AES_KEYSIZE_128 * 2:\r\nmemcpy(ctx->u.aes.key, key, key_len);\r\nbreak;\r\n}\r\nctx->u.aes.key_len = key_len / 2;\r\nsg_init_one(&ctx->u.aes.key_sg, ctx->u.aes.key, key_len);\r\nreturn crypto_ablkcipher_setkey(ctx->u.aes.tfm_ablkcipher, key,\r\nkey_len);\r\n}\r\nstatic int ccp_aes_xts_crypt(struct ablkcipher_request *req,\r\nunsigned int encrypt)\r\n{\r\nstruct crypto_tfm *tfm =\r\ncrypto_ablkcipher_tfm(crypto_ablkcipher_reqtfm(req));\r\nstruct ccp_ctx *ctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct ccp_aes_req_ctx *rctx = ablkcipher_request_ctx(req);\r\nunsigned int unit;\r\nint ret;\r\nif (!ctx->u.aes.key_len)\r\nreturn -EINVAL;\r\nif (req->nbytes & (AES_BLOCK_SIZE - 1))\r\nreturn -EINVAL;\r\nif (!req->info)\r\nreturn -EINVAL;\r\nfor (unit = 0; unit < ARRAY_SIZE(unit_size_map); unit++)\r\nif (!(req->nbytes & (unit_size_map[unit].size - 1)))\r\nbreak;\r\nif ((unit_size_map[unit].value == CCP_XTS_AES_UNIT_SIZE__LAST) ||\r\n(ctx->u.aes.key_len != AES_KEYSIZE_128)) {\r\nablkcipher_request_set_tfm(req, ctx->u.aes.tfm_ablkcipher);\r\nret = (encrypt) ? crypto_ablkcipher_encrypt(req) :\r\ncrypto_ablkcipher_decrypt(req);\r\nablkcipher_request_set_tfm(req, __crypto_ablkcipher_cast(tfm));\r\nreturn ret;\r\n}\r\nmemcpy(rctx->iv, req->info, AES_BLOCK_SIZE);\r\nsg_init_one(&rctx->iv_sg, rctx->iv, AES_BLOCK_SIZE);\r\nmemset(&rctx->cmd, 0, sizeof(rctx->cmd));\r\nINIT_LIST_HEAD(&rctx->cmd.entry);\r\nrctx->cmd.engine = CCP_ENGINE_XTS_AES_128;\r\nrctx->cmd.u.xts.action = (encrypt) ? CCP_AES_ACTION_ENCRYPT\r\n: CCP_AES_ACTION_DECRYPT;\r\nrctx->cmd.u.xts.unit_size = unit_size_map[unit].value;\r\nrctx->cmd.u.xts.key = &ctx->u.aes.key_sg;\r\nrctx->cmd.u.xts.key_len = ctx->u.aes.key_len;\r\nrctx->cmd.u.xts.iv = &rctx->iv_sg;\r\nrctx->cmd.u.xts.iv_len = AES_BLOCK_SIZE;\r\nrctx->cmd.u.xts.src = req->src;\r\nrctx->cmd.u.xts.src_len = req->nbytes;\r\nrctx->cmd.u.xts.dst = req->dst;\r\nret = ccp_crypto_enqueue_request(&req->base, &rctx->cmd);\r\nreturn ret;\r\n}\r\nstatic int ccp_aes_xts_encrypt(struct ablkcipher_request *req)\r\n{\r\nreturn ccp_aes_xts_crypt(req, 1);\r\n}\r\nstatic int ccp_aes_xts_decrypt(struct ablkcipher_request *req)\r\n{\r\nreturn ccp_aes_xts_crypt(req, 0);\r\n}\r\nstatic int ccp_aes_xts_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct ccp_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct crypto_ablkcipher *fallback_tfm;\r\nctx->complete = ccp_aes_xts_complete;\r\nctx->u.aes.key_len = 0;\r\nfallback_tfm = crypto_alloc_ablkcipher(tfm->__crt_alg->cra_name, 0,\r\nCRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(fallback_tfm)) {\r\npr_warn("could not load fallback driver %s\n",\r\ntfm->__crt_alg->cra_name);\r\nreturn PTR_ERR(fallback_tfm);\r\n}\r\nctx->u.aes.tfm_ablkcipher = fallback_tfm;\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct ccp_aes_req_ctx) +\r\nfallback_tfm->base.crt_ablkcipher.reqsize;\r\nreturn 0;\r\n}\r\nstatic void ccp_aes_xts_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct ccp_ctx *ctx = crypto_tfm_ctx(tfm);\r\nif (ctx->u.aes.tfm_ablkcipher)\r\ncrypto_free_ablkcipher(ctx->u.aes.tfm_ablkcipher);\r\nctx->u.aes.tfm_ablkcipher = NULL;\r\n}\r\nstatic int ccp_register_aes_xts_alg(struct list_head *head,\r\nconst struct ccp_aes_xts_def *def)\r\n{\r\nstruct ccp_crypto_ablkcipher_alg *ccp_alg;\r\nstruct crypto_alg *alg;\r\nint ret;\r\nccp_alg = kzalloc(sizeof(*ccp_alg), GFP_KERNEL);\r\nif (!ccp_alg)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&ccp_alg->entry);\r\nalg = &ccp_alg->alg;\r\nsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s", def->name);\r\nsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ndef->drv_name);\r\nalg->cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_KERN_DRIVER_ONLY |\r\nCRYPTO_ALG_NEED_FALLBACK;\r\nalg->cra_blocksize = AES_BLOCK_SIZE;\r\nalg->cra_ctxsize = sizeof(struct ccp_ctx);\r\nalg->cra_priority = CCP_CRA_PRIORITY;\r\nalg->cra_type = &crypto_ablkcipher_type;\r\nalg->cra_ablkcipher.setkey = ccp_aes_xts_setkey;\r\nalg->cra_ablkcipher.encrypt = ccp_aes_xts_encrypt;\r\nalg->cra_ablkcipher.decrypt = ccp_aes_xts_decrypt;\r\nalg->cra_ablkcipher.min_keysize = AES_MIN_KEY_SIZE * 2;\r\nalg->cra_ablkcipher.max_keysize = AES_MAX_KEY_SIZE * 2;\r\nalg->cra_ablkcipher.ivsize = AES_BLOCK_SIZE;\r\nalg->cra_init = ccp_aes_xts_cra_init;\r\nalg->cra_exit = ccp_aes_xts_cra_exit;\r\nalg->cra_module = THIS_MODULE;\r\nret = crypto_register_alg(alg);\r\nif (ret) {\r\npr_err("%s ablkcipher algorithm registration error (%d)\n",\r\nalg->cra_name, ret);\r\nkfree(ccp_alg);\r\nreturn ret;\r\n}\r\nlist_add(&ccp_alg->entry, head);\r\nreturn 0;\r\n}\r\nint ccp_register_aes_xts_algs(struct list_head *head)\r\n{\r\nint i, ret;\r\nfor (i = 0; i < ARRAY_SIZE(aes_xts_algs); i++) {\r\nret = ccp_register_aes_xts_alg(head, &aes_xts_algs[i]);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}
