uint32_t kvm_mips_get_kernel_asid(struct kvm_vcpu *vcpu)\r\n{\r\nreturn vcpu->arch.guest_kernel_asid[smp_processor_id()] & ASID_MASK;\r\n}\r\nuint32_t kvm_mips_get_user_asid(struct kvm_vcpu *vcpu)\r\n{\r\nreturn vcpu->arch.guest_user_asid[smp_processor_id()] & ASID_MASK;\r\n}\r\ninline uint32_t kvm_mips_get_commpage_asid (struct kvm_vcpu *vcpu)\r\n{\r\nreturn vcpu->kvm->arch.commpage_tlb;\r\n}\r\nvoid kvm_mips_dump_host_tlbs(void)\r\n{\r\nunsigned long old_entryhi;\r\nunsigned long old_pagemask;\r\nstruct kvm_mips_tlb tlb;\r\nunsigned long flags;\r\nint i;\r\nlocal_irq_save(flags);\r\nold_entryhi = read_c0_entryhi();\r\nold_pagemask = read_c0_pagemask();\r\nprintk("HOST TLBs:\n");\r\nprintk("ASID: %#lx\n", read_c0_entryhi() & ASID_MASK);\r\nfor (i = 0; i < current_cpu_data.tlbsize; i++) {\r\nwrite_c0_index(i);\r\nmtc0_tlbw_hazard();\r\ntlb_read();\r\ntlbw_use_hazard();\r\ntlb.tlb_hi = read_c0_entryhi();\r\ntlb.tlb_lo0 = read_c0_entrylo0();\r\ntlb.tlb_lo1 = read_c0_entrylo1();\r\ntlb.tlb_mask = read_c0_pagemask();\r\nprintk("TLB%c%3d Hi 0x%08lx ",\r\n(tlb.tlb_lo0 | tlb.tlb_lo1) & MIPS3_PG_V ? ' ' : '*',\r\ni, tlb.tlb_hi);\r\nprintk("Lo0=0x%09" PRIx64 " %c%c attr %lx ",\r\n(uint64_t) mips3_tlbpfn_to_paddr(tlb.tlb_lo0),\r\n(tlb.tlb_lo0 & MIPS3_PG_D) ? 'D' : ' ',\r\n(tlb.tlb_lo0 & MIPS3_PG_G) ? 'G' : ' ',\r\n(tlb.tlb_lo0 >> 3) & 7);\r\nprintk("Lo1=0x%09" PRIx64 " %c%c attr %lx sz=%lx\n",\r\n(uint64_t) mips3_tlbpfn_to_paddr(tlb.tlb_lo1),\r\n(tlb.tlb_lo1 & MIPS3_PG_D) ? 'D' : ' ',\r\n(tlb.tlb_lo1 & MIPS3_PG_G) ? 'G' : ' ',\r\n(tlb.tlb_lo1 >> 3) & 7, tlb.tlb_mask);\r\n}\r\nwrite_c0_entryhi(old_entryhi);\r\nwrite_c0_pagemask(old_pagemask);\r\nmtc0_tlbw_hazard();\r\nlocal_irq_restore(flags);\r\n}\r\nvoid kvm_mips_dump_guest_tlbs(struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nstruct kvm_mips_tlb tlb;\r\nint i;\r\nprintk("Guest TLBs:\n");\r\nprintk("Guest EntryHi: %#lx\n", kvm_read_c0_guest_entryhi(cop0));\r\nfor (i = 0; i < KVM_MIPS_GUEST_TLB_SIZE; i++) {\r\ntlb = vcpu->arch.guest_tlb[i];\r\nprintk("TLB%c%3d Hi 0x%08lx ",\r\n(tlb.tlb_lo0 | tlb.tlb_lo1) & MIPS3_PG_V ? ' ' : '*',\r\ni, tlb.tlb_hi);\r\nprintk("Lo0=0x%09" PRIx64 " %c%c attr %lx ",\r\n(uint64_t) mips3_tlbpfn_to_paddr(tlb.tlb_lo0),\r\n(tlb.tlb_lo0 & MIPS3_PG_D) ? 'D' : ' ',\r\n(tlb.tlb_lo0 & MIPS3_PG_G) ? 'G' : ' ',\r\n(tlb.tlb_lo0 >> 3) & 7);\r\nprintk("Lo1=0x%09" PRIx64 " %c%c attr %lx sz=%lx\n",\r\n(uint64_t) mips3_tlbpfn_to_paddr(tlb.tlb_lo1),\r\n(tlb.tlb_lo1 & MIPS3_PG_D) ? 'D' : ' ',\r\n(tlb.tlb_lo1 & MIPS3_PG_G) ? 'G' : ' ',\r\n(tlb.tlb_lo1 >> 3) & 7, tlb.tlb_mask);\r\n}\r\n}\r\nstatic int kvm_mips_map_page(struct kvm *kvm, gfn_t gfn)\r\n{\r\nint srcu_idx, err = 0;\r\npfn_t pfn;\r\nif (kvm->arch.guest_pmap[gfn] != KVM_INVALID_PAGE)\r\nreturn 0;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\npfn = kvm_mips_gfn_to_pfn(kvm, gfn);\r\nif (kvm_mips_is_error_pfn(pfn)) {\r\nkvm_err("Couldn't get pfn for gfn %#" PRIx64 "!\n", gfn);\r\nerr = -EFAULT;\r\ngoto out;\r\n}\r\nkvm->arch.guest_pmap[gfn] = pfn;\r\nout:\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\nreturn err;\r\n}\r\nunsigned long kvm_mips_translate_guest_kseg0_to_hpa(struct kvm_vcpu *vcpu,\r\nunsigned long gva)\r\n{\r\ngfn_t gfn;\r\nuint32_t offset = gva & ~PAGE_MASK;\r\nstruct kvm *kvm = vcpu->kvm;\r\nif (KVM_GUEST_KSEGX(gva) != KVM_GUEST_KSEG0) {\r\nkvm_err("%s/%p: Invalid gva: %#lx\n", __func__,\r\n__builtin_return_address(0), gva);\r\nreturn KVM_INVALID_PAGE;\r\n}\r\ngfn = (KVM_GUEST_CPHYSADDR(gva) >> PAGE_SHIFT);\r\nif (gfn >= kvm->arch.guest_pmap_npages) {\r\nkvm_err("%s: Invalid gfn: %#llx, GVA: %#lx\n", __func__, gfn,\r\ngva);\r\nreturn KVM_INVALID_PAGE;\r\n}\r\nif (kvm_mips_map_page(vcpu->kvm, gfn) < 0)\r\nreturn KVM_INVALID_ADDR;\r\nreturn (kvm->arch.guest_pmap[gfn] << PAGE_SHIFT) + offset;\r\n}\r\nint\r\nkvm_mips_host_tlb_write(struct kvm_vcpu *vcpu, unsigned long entryhi,\r\nunsigned long entrylo0, unsigned long entrylo1, int flush_dcache_mask)\r\n{\r\nunsigned long flags;\r\nunsigned long old_entryhi;\r\nvolatile int idx;\r\nlocal_irq_save(flags);\r\nold_entryhi = read_c0_entryhi();\r\nwrite_c0_entryhi(entryhi);\r\nmtc0_tlbw_hazard();\r\ntlb_probe();\r\ntlb_probe_hazard();\r\nidx = read_c0_index();\r\nif (idx > current_cpu_data.tlbsize) {\r\nkvm_err("%s: Invalid Index: %d\n", __func__, idx);\r\nkvm_mips_dump_host_tlbs();\r\nreturn -1;\r\n}\r\nif (idx < 0) {\r\nidx = read_c0_random() % current_cpu_data.tlbsize;\r\nwrite_c0_index(idx);\r\nmtc0_tlbw_hazard();\r\n}\r\nwrite_c0_entrylo0(entrylo0);\r\nwrite_c0_entrylo1(entrylo1);\r\nmtc0_tlbw_hazard();\r\ntlb_write_indexed();\r\ntlbw_use_hazard();\r\n#ifdef DEBUG\r\nif (debug) {\r\nkvm_debug("@ %#lx idx: %2d [entryhi(R): %#lx] "\r\n"entrylo0(R): 0x%08lx, entrylo1(R): 0x%08lx\n",\r\nvcpu->arch.pc, idx, read_c0_entryhi(),\r\nread_c0_entrylo0(), read_c0_entrylo1());\r\n}\r\n#endif\r\nif (flush_dcache_mask) {\r\nif (entrylo0 & MIPS3_PG_V) {\r\n++vcpu->stat.flush_dcache_exits;\r\nflush_data_cache_page((entryhi & VPN2_MASK) & ~flush_dcache_mask);\r\n}\r\nif (entrylo1 & MIPS3_PG_V) {\r\n++vcpu->stat.flush_dcache_exits;\r\nflush_data_cache_page(((entryhi & VPN2_MASK) & ~flush_dcache_mask) |\r\n(0x1 << PAGE_SHIFT));\r\n}\r\n}\r\nwrite_c0_entryhi(old_entryhi);\r\nmtc0_tlbw_hazard();\r\ntlbw_use_hazard();\r\nlocal_irq_restore(flags);\r\nreturn 0;\r\n}\r\nint kvm_mips_handle_kseg0_tlb_fault(unsigned long badvaddr,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\ngfn_t gfn;\r\npfn_t pfn0, pfn1;\r\nunsigned long vaddr = 0;\r\nunsigned long entryhi = 0, entrylo0 = 0, entrylo1 = 0;\r\nint even;\r\nstruct kvm *kvm = vcpu->kvm;\r\nconst int flush_dcache_mask = 0;\r\nif (KVM_GUEST_KSEGX(badvaddr) != KVM_GUEST_KSEG0) {\r\nkvm_err("%s: Invalid BadVaddr: %#lx\n", __func__, badvaddr);\r\nkvm_mips_dump_host_tlbs();\r\nreturn -1;\r\n}\r\ngfn = (KVM_GUEST_CPHYSADDR(badvaddr) >> PAGE_SHIFT);\r\nif (gfn >= kvm->arch.guest_pmap_npages) {\r\nkvm_err("%s: Invalid gfn: %#llx, BadVaddr: %#lx\n", __func__,\r\ngfn, badvaddr);\r\nkvm_mips_dump_host_tlbs();\r\nreturn -1;\r\n}\r\neven = !(gfn & 0x1);\r\nvaddr = badvaddr & (PAGE_MASK << 1);\r\nif (kvm_mips_map_page(vcpu->kvm, gfn) < 0)\r\nreturn -1;\r\nif (kvm_mips_map_page(vcpu->kvm, gfn ^ 0x1) < 0)\r\nreturn -1;\r\nif (even) {\r\npfn0 = kvm->arch.guest_pmap[gfn];\r\npfn1 = kvm->arch.guest_pmap[gfn ^ 0x1];\r\n} else {\r\npfn0 = kvm->arch.guest_pmap[gfn ^ 0x1];\r\npfn1 = kvm->arch.guest_pmap[gfn];\r\n}\r\nentryhi = (vaddr | kvm_mips_get_kernel_asid(vcpu));\r\nentrylo0 = mips3_paddr_to_tlbpfn(pfn0 << PAGE_SHIFT) | (0x3 << 3) | (1 << 2) |\r\n(0x1 << 1);\r\nentrylo1 = mips3_paddr_to_tlbpfn(pfn1 << PAGE_SHIFT) | (0x3 << 3) | (1 << 2) |\r\n(0x1 << 1);\r\nreturn kvm_mips_host_tlb_write(vcpu, entryhi, entrylo0, entrylo1,\r\nflush_dcache_mask);\r\n}\r\nint kvm_mips_handle_commpage_tlb_fault(unsigned long badvaddr,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\npfn_t pfn0, pfn1;\r\nunsigned long flags, old_entryhi = 0, vaddr = 0;\r\nunsigned long entrylo0 = 0, entrylo1 = 0;\r\npfn0 = CPHYSADDR(vcpu->arch.kseg0_commpage) >> PAGE_SHIFT;\r\npfn1 = 0;\r\nentrylo0 = mips3_paddr_to_tlbpfn(pfn0 << PAGE_SHIFT) | (0x3 << 3) | (1 << 2) |\r\n(0x1 << 1);\r\nentrylo1 = 0;\r\nlocal_irq_save(flags);\r\nold_entryhi = read_c0_entryhi();\r\nvaddr = badvaddr & (PAGE_MASK << 1);\r\nwrite_c0_entryhi(vaddr | kvm_mips_get_kernel_asid(vcpu));\r\nmtc0_tlbw_hazard();\r\nwrite_c0_entrylo0(entrylo0);\r\nmtc0_tlbw_hazard();\r\nwrite_c0_entrylo1(entrylo1);\r\nmtc0_tlbw_hazard();\r\nwrite_c0_index(kvm_mips_get_commpage_asid(vcpu));\r\nmtc0_tlbw_hazard();\r\ntlb_write_indexed();\r\nmtc0_tlbw_hazard();\r\ntlbw_use_hazard();\r\n#ifdef DEBUG\r\nkvm_debug ("@ %#lx idx: %2d [entryhi(R): %#lx] entrylo0 (R): 0x%08lx, entrylo1(R): 0x%08lx\n",\r\nvcpu->arch.pc, read_c0_index(), read_c0_entryhi(),\r\nread_c0_entrylo0(), read_c0_entrylo1());\r\n#endif\r\nwrite_c0_entryhi(old_entryhi);\r\nmtc0_tlbw_hazard();\r\ntlbw_use_hazard();\r\nlocal_irq_restore(flags);\r\nreturn 0;\r\n}\r\nint\r\nkvm_mips_handle_mapped_seg_tlb_fault(struct kvm_vcpu *vcpu,\r\nstruct kvm_mips_tlb *tlb, unsigned long *hpa0, unsigned long *hpa1)\r\n{\r\nunsigned long entryhi = 0, entrylo0 = 0, entrylo1 = 0;\r\nstruct kvm *kvm = vcpu->kvm;\r\npfn_t pfn0, pfn1;\r\nif ((tlb->tlb_hi & VPN2_MASK) == 0) {\r\npfn0 = 0;\r\npfn1 = 0;\r\n} else {\r\nif (kvm_mips_map_page(kvm, mips3_tlbpfn_to_paddr(tlb->tlb_lo0) >> PAGE_SHIFT) < 0)\r\nreturn -1;\r\nif (kvm_mips_map_page(kvm, mips3_tlbpfn_to_paddr(tlb->tlb_lo1) >> PAGE_SHIFT) < 0)\r\nreturn -1;\r\npfn0 = kvm->arch.guest_pmap[mips3_tlbpfn_to_paddr(tlb->tlb_lo0) >> PAGE_SHIFT];\r\npfn1 = kvm->arch.guest_pmap[mips3_tlbpfn_to_paddr(tlb->tlb_lo1) >> PAGE_SHIFT];\r\n}\r\nif (hpa0)\r\n*hpa0 = pfn0 << PAGE_SHIFT;\r\nif (hpa1)\r\n*hpa1 = pfn1 << PAGE_SHIFT;\r\nentryhi = (tlb->tlb_hi & VPN2_MASK) | (KVM_GUEST_KERNEL_MODE(vcpu) ?\r\nkvm_mips_get_kernel_asid(vcpu) : kvm_mips_get_user_asid(vcpu));\r\nentrylo0 = mips3_paddr_to_tlbpfn(pfn0 << PAGE_SHIFT) | (0x3 << 3) |\r\n(tlb->tlb_lo0 & MIPS3_PG_D) | (tlb->tlb_lo0 & MIPS3_PG_V);\r\nentrylo1 = mips3_paddr_to_tlbpfn(pfn1 << PAGE_SHIFT) | (0x3 << 3) |\r\n(tlb->tlb_lo1 & MIPS3_PG_D) | (tlb->tlb_lo1 & MIPS3_PG_V);\r\n#ifdef DEBUG\r\nkvm_debug("@ %#lx tlb_lo0: 0x%08lx tlb_lo1: 0x%08lx\n", vcpu->arch.pc,\r\ntlb->tlb_lo0, tlb->tlb_lo1);\r\n#endif\r\nreturn kvm_mips_host_tlb_write(vcpu, entryhi, entrylo0, entrylo1,\r\ntlb->tlb_mask);\r\n}\r\nint kvm_mips_guest_tlb_lookup(struct kvm_vcpu *vcpu, unsigned long entryhi)\r\n{\r\nint i;\r\nint index = -1;\r\nstruct kvm_mips_tlb *tlb = vcpu->arch.guest_tlb;\r\nfor (i = 0; i < KVM_MIPS_GUEST_TLB_SIZE; i++) {\r\nif (((TLB_VPN2(tlb[i]) & ~tlb[i].tlb_mask) == ((entryhi & VPN2_MASK) & ~tlb[i].tlb_mask)) &&\r\n(TLB_IS_GLOBAL(tlb[i]) || (TLB_ASID(tlb[i]) == (entryhi & ASID_MASK)))) {\r\nindex = i;\r\nbreak;\r\n}\r\n}\r\n#ifdef DEBUG\r\nkvm_debug("%s: entryhi: %#lx, index: %d lo0: %#lx, lo1: %#lx\n",\r\n__func__, entryhi, index, tlb[i].tlb_lo0, tlb[i].tlb_lo1);\r\n#endif\r\nreturn index;\r\n}\r\nint kvm_mips_host_tlb_lookup(struct kvm_vcpu *vcpu, unsigned long vaddr)\r\n{\r\nunsigned long old_entryhi, flags;\r\nvolatile int idx;\r\nlocal_irq_save(flags);\r\nold_entryhi = read_c0_entryhi();\r\nif (KVM_GUEST_KERNEL_MODE(vcpu))\r\nwrite_c0_entryhi((vaddr & VPN2_MASK) | kvm_mips_get_kernel_asid(vcpu));\r\nelse {\r\nwrite_c0_entryhi((vaddr & VPN2_MASK) | kvm_mips_get_user_asid(vcpu));\r\n}\r\nmtc0_tlbw_hazard();\r\ntlb_probe();\r\ntlb_probe_hazard();\r\nidx = read_c0_index();\r\nwrite_c0_entryhi(old_entryhi);\r\nmtc0_tlbw_hazard();\r\ntlbw_use_hazard();\r\nlocal_irq_restore(flags);\r\n#ifdef DEBUG\r\nkvm_debug("Host TLB lookup, %#lx, idx: %2d\n", vaddr, idx);\r\n#endif\r\nreturn idx;\r\n}\r\nint kvm_mips_host_tlb_inv(struct kvm_vcpu *vcpu, unsigned long va)\r\n{\r\nint idx;\r\nunsigned long flags, old_entryhi;\r\nlocal_irq_save(flags);\r\nold_entryhi = read_c0_entryhi();\r\nwrite_c0_entryhi((va & VPN2_MASK) | kvm_mips_get_user_asid(vcpu));\r\nmtc0_tlbw_hazard();\r\ntlb_probe();\r\ntlb_probe_hazard();\r\nidx = read_c0_index();\r\nif (idx >= current_cpu_data.tlbsize)\r\nBUG();\r\nif (idx > 0) {\r\nwrite_c0_entryhi(UNIQUE_ENTRYHI(idx));\r\nmtc0_tlbw_hazard();\r\nwrite_c0_entrylo0(0);\r\nmtc0_tlbw_hazard();\r\nwrite_c0_entrylo1(0);\r\nmtc0_tlbw_hazard();\r\ntlb_write_indexed();\r\nmtc0_tlbw_hazard();\r\n}\r\nwrite_c0_entryhi(old_entryhi);\r\nmtc0_tlbw_hazard();\r\ntlbw_use_hazard();\r\nlocal_irq_restore(flags);\r\n#ifdef DEBUG\r\nif (idx > 0) {\r\nkvm_debug("%s: Invalidated entryhi %#lx @ idx %d\n", __func__,\r\n(va & VPN2_MASK) | (vcpu->arch.asid_map[va & ASID_MASK] & ASID_MASK), idx);\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nint kvm_mips_host_tlb_inv_index(struct kvm_vcpu *vcpu, int index)\r\n{\r\nunsigned long flags, old_entryhi;\r\nif (index >= current_cpu_data.tlbsize)\r\nBUG();\r\nlocal_irq_save(flags);\r\nold_entryhi = read_c0_entryhi();\r\nwrite_c0_entryhi(UNIQUE_ENTRYHI(index));\r\nmtc0_tlbw_hazard();\r\nwrite_c0_index(index);\r\nmtc0_tlbw_hazard();\r\nwrite_c0_entrylo0(0);\r\nmtc0_tlbw_hazard();\r\nwrite_c0_entrylo1(0);\r\nmtc0_tlbw_hazard();\r\ntlb_write_indexed();\r\nmtc0_tlbw_hazard();\r\ntlbw_use_hazard();\r\nwrite_c0_entryhi(old_entryhi);\r\nmtc0_tlbw_hazard();\r\ntlbw_use_hazard();\r\nlocal_irq_restore(flags);\r\nreturn 0;\r\n}\r\nvoid kvm_mips_flush_host_tlb(int skip_kseg0)\r\n{\r\nunsigned long flags;\r\nunsigned long old_entryhi, entryhi;\r\nunsigned long old_pagemask;\r\nint entry = 0;\r\nint maxentry = current_cpu_data.tlbsize;\r\nlocal_irq_save(flags);\r\nold_entryhi = read_c0_entryhi();\r\nold_pagemask = read_c0_pagemask();\r\nfor (entry = 0; entry < maxentry; entry++) {\r\nwrite_c0_index(entry);\r\nmtc0_tlbw_hazard();\r\nif (skip_kseg0) {\r\ntlb_read();\r\ntlbw_use_hazard();\r\nentryhi = read_c0_entryhi();\r\nif (KVM_GUEST_KSEGX(entryhi) == KVM_GUEST_KSEG0) {\r\ncontinue;\r\n}\r\n}\r\nwrite_c0_entryhi(UNIQUE_ENTRYHI(entry));\r\nmtc0_tlbw_hazard();\r\nwrite_c0_entrylo0(0);\r\nmtc0_tlbw_hazard();\r\nwrite_c0_entrylo1(0);\r\nmtc0_tlbw_hazard();\r\ntlb_write_indexed();\r\nmtc0_tlbw_hazard();\r\n}\r\ntlbw_use_hazard();\r\nwrite_c0_entryhi(old_entryhi);\r\nwrite_c0_pagemask(old_pagemask);\r\nmtc0_tlbw_hazard();\r\ntlbw_use_hazard();\r\nlocal_irq_restore(flags);\r\n}\r\nvoid\r\nkvm_get_new_mmu_context(struct mm_struct *mm, unsigned long cpu,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nunsigned long asid = asid_cache(cpu);\r\nif (!((asid += ASID_INC) & ASID_MASK)) {\r\nif (cpu_has_vtag_icache) {\r\nflush_icache_all();\r\n}\r\nkvm_local_flush_tlb_all();\r\nif (!asid)\r\nasid = ASID_FIRST_VERSION;\r\n}\r\ncpu_context(cpu, mm) = asid_cache(cpu) = asid;\r\n}\r\nvoid kvm_local_flush_tlb_all(void)\r\n{\r\nunsigned long flags;\r\nunsigned long old_ctx;\r\nint entry = 0;\r\nlocal_irq_save(flags);\r\nold_ctx = read_c0_entryhi();\r\nwrite_c0_entrylo0(0);\r\nwrite_c0_entrylo1(0);\r\nwhile (entry < current_cpu_data.tlbsize) {\r\nwrite_c0_entryhi(UNIQUE_ENTRYHI(entry));\r\nwrite_c0_index(entry);\r\nmtc0_tlbw_hazard();\r\ntlb_write_indexed();\r\nentry++;\r\n}\r\ntlbw_use_hazard();\r\nwrite_c0_entryhi(old_ctx);\r\nmtc0_tlbw_hazard();\r\nlocal_irq_restore(flags);\r\n}\r\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nunsigned long flags;\r\nint newasid = 0;\r\n#ifdef DEBUG\r\nkvm_debug("%s: vcpu %p, cpu: %d\n", __func__, vcpu, cpu);\r\n#endif\r\nlocal_irq_save(flags);\r\nif (((vcpu->arch.\r\nguest_kernel_asid[cpu] ^ asid_cache(cpu)) & ASID_VERSION_MASK)) {\r\nkvm_get_new_mmu_context(&vcpu->arch.guest_kernel_mm, cpu, vcpu);\r\nvcpu->arch.guest_kernel_asid[cpu] =\r\nvcpu->arch.guest_kernel_mm.context.asid[cpu];\r\nkvm_get_new_mmu_context(&vcpu->arch.guest_user_mm, cpu, vcpu);\r\nvcpu->arch.guest_user_asid[cpu] =\r\nvcpu->arch.guest_user_mm.context.asid[cpu];\r\nnewasid++;\r\nkvm_info("[%d]: cpu_context: %#lx\n", cpu,\r\ncpu_context(cpu, current->mm));\r\nkvm_info("[%d]: Allocated new ASID for Guest Kernel: %#x\n",\r\ncpu, vcpu->arch.guest_kernel_asid[cpu]);\r\nkvm_info("[%d]: Allocated new ASID for Guest User: %#x\n", cpu,\r\nvcpu->arch.guest_user_asid[cpu]);\r\n}\r\nif (vcpu->arch.last_sched_cpu != cpu) {\r\nkvm_info("[%d->%d]KVM VCPU[%d] switch\n",\r\nvcpu->arch.last_sched_cpu, cpu, vcpu->vcpu_id);\r\n}\r\nif (!newasid) {\r\nif (current->flags & PF_VCPU) {\r\nwrite_c0_entryhi(vcpu->arch.\r\npreempt_entryhi & ASID_MASK);\r\nehb();\r\n}\r\n} else {\r\nif (current->flags & PF_VCPU) {\r\nif (KVM_GUEST_KERNEL_MODE(vcpu))\r\nwrite_c0_entryhi(vcpu->arch.\r\nguest_kernel_asid[cpu] &\r\nASID_MASK);\r\nelse\r\nwrite_c0_entryhi(vcpu->arch.\r\nguest_user_asid[cpu] &\r\nASID_MASK);\r\nehb();\r\n}\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long flags;\r\nuint32_t cpu;\r\nlocal_irq_save(flags);\r\ncpu = smp_processor_id();\r\nvcpu->arch.preempt_entryhi = read_c0_entryhi();\r\nvcpu->arch.last_sched_cpu = cpu;\r\nif (((cpu_context(cpu, current->mm) ^ asid_cache(cpu)) &\r\nASID_VERSION_MASK)) {\r\nkvm_debug("%s: Dropping MMU Context: %#lx\n", __func__,\r\ncpu_context(cpu, current->mm));\r\ndrop_mmu_context(current->mm, cpu);\r\n}\r\nwrite_c0_entryhi(cpu_asid(cpu, current->mm));\r\nehb();\r\nlocal_irq_restore(flags);\r\n}\r\nuint32_t kvm_get_inst(uint32_t *opc, struct kvm_vcpu *vcpu)\r\n{\r\nstruct mips_coproc *cop0 = vcpu->arch.cop0;\r\nunsigned long paddr, flags;\r\nuint32_t inst;\r\nint index;\r\nif (KVM_GUEST_KSEGX((unsigned long) opc) < KVM_GUEST_KSEG0 ||\r\nKVM_GUEST_KSEGX((unsigned long) opc) == KVM_GUEST_KSEG23) {\r\nlocal_irq_save(flags);\r\nindex = kvm_mips_host_tlb_lookup(vcpu, (unsigned long) opc);\r\nif (index >= 0) {\r\ninst = *(opc);\r\n} else {\r\nindex =\r\nkvm_mips_guest_tlb_lookup(vcpu,\r\n((unsigned long) opc & VPN2_MASK)\r\n|\r\n(kvm_read_c0_guest_entryhi\r\n(cop0) & ASID_MASK));\r\nif (index < 0) {\r\nkvm_err\r\n("%s: get_user_failed for %p, vcpu: %p, ASID: %#lx\n",\r\n__func__, opc, vcpu, read_c0_entryhi());\r\nkvm_mips_dump_host_tlbs();\r\nlocal_irq_restore(flags);\r\nreturn KVM_INVALID_INST;\r\n}\r\nkvm_mips_handle_mapped_seg_tlb_fault(vcpu,\r\n&vcpu->arch.\r\nguest_tlb[index],\r\nNULL, NULL);\r\ninst = *(opc);\r\n}\r\nlocal_irq_restore(flags);\r\n} else if (KVM_GUEST_KSEGX(opc) == KVM_GUEST_KSEG0) {\r\npaddr =\r\nkvm_mips_translate_guest_kseg0_to_hpa(vcpu,\r\n(unsigned long) opc);\r\ninst = *(uint32_t *) CKSEG0ADDR(paddr);\r\n} else {\r\nkvm_err("%s: illegal address: %p\n", __func__, opc);\r\nreturn KVM_INVALID_INST;\r\n}\r\nreturn inst;\r\n}
