struct dm_io_client *dm_io_client_create(void)\r\n{\r\nstruct dm_io_client *client;\r\nunsigned min_ios = dm_get_reserved_bio_based_ios();\r\nclient = kmalloc(sizeof(*client), GFP_KERNEL);\r\nif (!client)\r\nreturn ERR_PTR(-ENOMEM);\r\nclient->pool = mempool_create_slab_pool(min_ios, _dm_io_cache);\r\nif (!client->pool)\r\ngoto bad;\r\nclient->bios = bioset_create(min_ios, 0);\r\nif (!client->bios)\r\ngoto bad;\r\nreturn client;\r\nbad:\r\nif (client->pool)\r\nmempool_destroy(client->pool);\r\nkfree(client);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nvoid dm_io_client_destroy(struct dm_io_client *client)\r\n{\r\nmempool_destroy(client->pool);\r\nbioset_free(client->bios);\r\nkfree(client);\r\n}\r\nstatic void store_io_and_region_in_bio(struct bio *bio, struct io *io,\r\nunsigned region)\r\n{\r\nif (unlikely(!IS_ALIGNED((unsigned long)io, DM_IO_MAX_REGIONS))) {\r\nDMCRIT("Unaligned struct io pointer %p", io);\r\nBUG();\r\n}\r\nbio->bi_private = (void *)((unsigned long)io | region);\r\n}\r\nstatic void retrieve_io_and_region_from_bio(struct bio *bio, struct io **io,\r\nunsigned *region)\r\n{\r\nunsigned long val = (unsigned long)bio->bi_private;\r\n*io = (void *)(val & -(unsigned long)DM_IO_MAX_REGIONS);\r\n*region = val & (DM_IO_MAX_REGIONS - 1);\r\n}\r\nstatic void dec_count(struct io *io, unsigned int region, int error)\r\n{\r\nif (error)\r\nset_bit(region, &io->error_bits);\r\nif (atomic_dec_and_test(&io->count)) {\r\nif (io->vma_invalidate_size)\r\ninvalidate_kernel_vmap_range(io->vma_invalidate_address,\r\nio->vma_invalidate_size);\r\nif (io->sleeper)\r\nwake_up_process(io->sleeper);\r\nelse {\r\nunsigned long r = io->error_bits;\r\nio_notify_fn fn = io->callback;\r\nvoid *context = io->context;\r\nmempool_free(io, io->client->pool);\r\nfn(r, context);\r\n}\r\n}\r\n}\r\nstatic void endio(struct bio *bio, int error)\r\n{\r\nstruct io *io;\r\nunsigned region;\r\nif (error && bio_data_dir(bio) == READ)\r\nzero_fill_bio(bio);\r\nretrieve_io_and_region_from_bio(bio, &io, &region);\r\nbio_put(bio);\r\ndec_count(io, region, error);\r\n}\r\nstatic void list_get_page(struct dpages *dp,\r\nstruct page **p, unsigned long *len, unsigned *offset)\r\n{\r\nunsigned o = dp->context_u;\r\nstruct page_list *pl = (struct page_list *) dp->context_ptr;\r\n*p = pl->page;\r\n*len = PAGE_SIZE - o;\r\n*offset = o;\r\n}\r\nstatic void list_next_page(struct dpages *dp)\r\n{\r\nstruct page_list *pl = (struct page_list *) dp->context_ptr;\r\ndp->context_ptr = pl->next;\r\ndp->context_u = 0;\r\n}\r\nstatic void list_dp_init(struct dpages *dp, struct page_list *pl, unsigned offset)\r\n{\r\ndp->get_page = list_get_page;\r\ndp->next_page = list_next_page;\r\ndp->context_u = offset;\r\ndp->context_ptr = pl;\r\n}\r\nstatic void bio_get_page(struct dpages *dp, struct page **p,\r\nunsigned long *len, unsigned *offset)\r\n{\r\nstruct bio_vec *bvec = dp->context_ptr;\r\n*p = bvec->bv_page;\r\n*len = bvec->bv_len - dp->context_u;\r\n*offset = bvec->bv_offset + dp->context_u;\r\n}\r\nstatic void bio_next_page(struct dpages *dp)\r\n{\r\nstruct bio_vec *bvec = dp->context_ptr;\r\ndp->context_ptr = bvec + 1;\r\ndp->context_u = 0;\r\n}\r\nstatic void bio_dp_init(struct dpages *dp, struct bio *bio)\r\n{\r\ndp->get_page = bio_get_page;\r\ndp->next_page = bio_next_page;\r\ndp->context_ptr = __bvec_iter_bvec(bio->bi_io_vec, bio->bi_iter);\r\ndp->context_u = bio->bi_iter.bi_bvec_done;\r\n}\r\nstatic void vm_get_page(struct dpages *dp,\r\nstruct page **p, unsigned long *len, unsigned *offset)\r\n{\r\n*p = vmalloc_to_page(dp->context_ptr);\r\n*offset = dp->context_u;\r\n*len = PAGE_SIZE - dp->context_u;\r\n}\r\nstatic void vm_next_page(struct dpages *dp)\r\n{\r\ndp->context_ptr += PAGE_SIZE - dp->context_u;\r\ndp->context_u = 0;\r\n}\r\nstatic void vm_dp_init(struct dpages *dp, void *data)\r\n{\r\ndp->get_page = vm_get_page;\r\ndp->next_page = vm_next_page;\r\ndp->context_u = ((unsigned long) data) & (PAGE_SIZE - 1);\r\ndp->context_ptr = data;\r\n}\r\nstatic void km_get_page(struct dpages *dp, struct page **p, unsigned long *len,\r\nunsigned *offset)\r\n{\r\n*p = virt_to_page(dp->context_ptr);\r\n*offset = dp->context_u;\r\n*len = PAGE_SIZE - dp->context_u;\r\n}\r\nstatic void km_next_page(struct dpages *dp)\r\n{\r\ndp->context_ptr += PAGE_SIZE - dp->context_u;\r\ndp->context_u = 0;\r\n}\r\nstatic void km_dp_init(struct dpages *dp, void *data)\r\n{\r\ndp->get_page = km_get_page;\r\ndp->next_page = km_next_page;\r\ndp->context_u = ((unsigned long) data) & (PAGE_SIZE - 1);\r\ndp->context_ptr = data;\r\n}\r\nstatic void do_region(int rw, unsigned region, struct dm_io_region *where,\r\nstruct dpages *dp, struct io *io)\r\n{\r\nstruct bio *bio;\r\nstruct page *page;\r\nunsigned long len;\r\nunsigned offset;\r\nunsigned num_bvecs;\r\nsector_t remaining = where->count;\r\nstruct request_queue *q = bdev_get_queue(where->bdev);\r\nunsigned short logical_block_size = queue_logical_block_size(q);\r\nsector_t num_sectors;\r\ndo {\r\nif ((rw & REQ_DISCARD) || (rw & REQ_WRITE_SAME))\r\nnum_bvecs = 1;\r\nelse\r\nnum_bvecs = min_t(int, bio_get_nr_vecs(where->bdev),\r\ndm_sector_div_up(remaining, (PAGE_SIZE >> SECTOR_SHIFT)));\r\nbio = bio_alloc_bioset(GFP_NOIO, num_bvecs, io->client->bios);\r\nbio->bi_iter.bi_sector = where->sector + (where->count - remaining);\r\nbio->bi_bdev = where->bdev;\r\nbio->bi_end_io = endio;\r\nstore_io_and_region_in_bio(bio, io, region);\r\nif (rw & REQ_DISCARD) {\r\nnum_sectors = min_t(sector_t, q->limits.max_discard_sectors, remaining);\r\nbio->bi_iter.bi_size = num_sectors << SECTOR_SHIFT;\r\nremaining -= num_sectors;\r\n} else if (rw & REQ_WRITE_SAME) {\r\ndp->get_page(dp, &page, &len, &offset);\r\nbio_add_page(bio, page, logical_block_size, offset);\r\nnum_sectors = min_t(sector_t, q->limits.max_write_same_sectors, remaining);\r\nbio->bi_iter.bi_size = num_sectors << SECTOR_SHIFT;\r\noffset = 0;\r\nremaining -= num_sectors;\r\ndp->next_page(dp);\r\n} else while (remaining) {\r\ndp->get_page(dp, &page, &len, &offset);\r\nlen = min(len, to_bytes(remaining));\r\nif (!bio_add_page(bio, page, len, offset))\r\nbreak;\r\noffset = 0;\r\nremaining -= to_sector(len);\r\ndp->next_page(dp);\r\n}\r\natomic_inc(&io->count);\r\nsubmit_bio(rw, bio);\r\n} while (remaining);\r\n}\r\nstatic void dispatch_io(int rw, unsigned int num_regions,\r\nstruct dm_io_region *where, struct dpages *dp,\r\nstruct io *io, int sync)\r\n{\r\nint i;\r\nstruct dpages old_pages = *dp;\r\nBUG_ON(num_regions > DM_IO_MAX_REGIONS);\r\nif (sync)\r\nrw |= REQ_SYNC;\r\nfor (i = 0; i < num_regions; i++) {\r\n*dp = old_pages;\r\nif (where[i].count || (rw & REQ_FLUSH))\r\ndo_region(rw, i, where + i, dp, io);\r\n}\r\ndec_count(io, 0, 0);\r\n}\r\nstatic int sync_io(struct dm_io_client *client, unsigned int num_regions,\r\nstruct dm_io_region *where, int rw, struct dpages *dp,\r\nunsigned long *error_bits)\r\n{\r\nvolatile char io_[sizeof(struct io) + __alignof__(struct io) - 1];\r\nstruct io *io = (struct io *)PTR_ALIGN(&io_, __alignof__(struct io));\r\nif (num_regions > 1 && (rw & RW_MASK) != WRITE) {\r\nWARN_ON(1);\r\nreturn -EIO;\r\n}\r\nio->error_bits = 0;\r\natomic_set(&io->count, 1);\r\nio->sleeper = current;\r\nio->client = client;\r\nio->vma_invalidate_address = dp->vma_invalidate_address;\r\nio->vma_invalidate_size = dp->vma_invalidate_size;\r\ndispatch_io(rw, num_regions, where, dp, io, 1);\r\nwhile (1) {\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nif (!atomic_read(&io->count))\r\nbreak;\r\nio_schedule();\r\n}\r\nset_current_state(TASK_RUNNING);\r\nif (error_bits)\r\n*error_bits = io->error_bits;\r\nreturn io->error_bits ? -EIO : 0;\r\n}\r\nstatic int async_io(struct dm_io_client *client, unsigned int num_regions,\r\nstruct dm_io_region *where, int rw, struct dpages *dp,\r\nio_notify_fn fn, void *context)\r\n{\r\nstruct io *io;\r\nif (num_regions > 1 && (rw & RW_MASK) != WRITE) {\r\nWARN_ON(1);\r\nfn(1, context);\r\nreturn -EIO;\r\n}\r\nio = mempool_alloc(client->pool, GFP_NOIO);\r\nio->error_bits = 0;\r\natomic_set(&io->count, 1);\r\nio->sleeper = NULL;\r\nio->client = client;\r\nio->callback = fn;\r\nio->context = context;\r\nio->vma_invalidate_address = dp->vma_invalidate_address;\r\nio->vma_invalidate_size = dp->vma_invalidate_size;\r\ndispatch_io(rw, num_regions, where, dp, io, 0);\r\nreturn 0;\r\n}\r\nstatic int dp_init(struct dm_io_request *io_req, struct dpages *dp,\r\nunsigned long size)\r\n{\r\ndp->vma_invalidate_address = NULL;\r\ndp->vma_invalidate_size = 0;\r\nswitch (io_req->mem.type) {\r\ncase DM_IO_PAGE_LIST:\r\nlist_dp_init(dp, io_req->mem.ptr.pl, io_req->mem.offset);\r\nbreak;\r\ncase DM_IO_BIO:\r\nbio_dp_init(dp, io_req->mem.ptr.bio);\r\nbreak;\r\ncase DM_IO_VMA:\r\nflush_kernel_vmap_range(io_req->mem.ptr.vma, size);\r\nif ((io_req->bi_rw & RW_MASK) == READ) {\r\ndp->vma_invalidate_address = io_req->mem.ptr.vma;\r\ndp->vma_invalidate_size = size;\r\n}\r\nvm_dp_init(dp, io_req->mem.ptr.vma);\r\nbreak;\r\ncase DM_IO_KMEM:\r\nkm_dp_init(dp, io_req->mem.ptr.addr);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nint dm_io(struct dm_io_request *io_req, unsigned num_regions,\r\nstruct dm_io_region *where, unsigned long *sync_error_bits)\r\n{\r\nint r;\r\nstruct dpages dp;\r\nr = dp_init(io_req, &dp, (unsigned long)where->count << SECTOR_SHIFT);\r\nif (r)\r\nreturn r;\r\nif (!io_req->notify.fn)\r\nreturn sync_io(io_req->client, num_regions, where,\r\nio_req->bi_rw, &dp, sync_error_bits);\r\nreturn async_io(io_req->client, num_regions, where, io_req->bi_rw,\r\n&dp, io_req->notify.fn, io_req->notify.context);\r\n}\r\nint __init dm_io_init(void)\r\n{\r\n_dm_io_cache = KMEM_CACHE(io, 0);\r\nif (!_dm_io_cache)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid dm_io_exit(void)\r\n{\r\nkmem_cache_destroy(_dm_io_cache);\r\n_dm_io_cache = NULL;\r\n}
