static pgd_t *spgd_addr(struct lg_cpu *cpu, u32 i, unsigned long vaddr)\r\n{\r\nunsigned int index = pgd_index(vaddr);\r\n#ifndef CONFIG_X86_PAE\r\nif (index >= SWITCHER_PGD_INDEX) {\r\nkill_guest(cpu, "attempt to access switcher pages");\r\nindex = 0;\r\n}\r\n#endif\r\nreturn &cpu->lg->pgdirs[i].pgdir[index];\r\n}\r\nstatic pmd_t *spmd_addr(struct lg_cpu *cpu, pgd_t spgd, unsigned long vaddr)\r\n{\r\nunsigned int index = pmd_index(vaddr);\r\npmd_t *page;\r\nif (pgd_index(vaddr) == SWITCHER_PGD_INDEX &&\r\nindex >= SWITCHER_PMD_INDEX) {\r\nkill_guest(cpu, "attempt to access switcher pages");\r\nindex = 0;\r\n}\r\nBUG_ON(!(pgd_flags(spgd) & _PAGE_PRESENT));\r\npage = __va(pgd_pfn(spgd) << PAGE_SHIFT);\r\nreturn &page[index];\r\n}\r\nstatic pte_t *spte_addr(struct lg_cpu *cpu, pgd_t spgd, unsigned long vaddr)\r\n{\r\n#ifdef CONFIG_X86_PAE\r\npmd_t *pmd = spmd_addr(cpu, spgd, vaddr);\r\npte_t *page = __va(pmd_pfn(*pmd) << PAGE_SHIFT);\r\nBUG_ON(!(pmd_flags(*pmd) & _PAGE_PRESENT));\r\n#else\r\npte_t *page = __va(pgd_pfn(spgd) << PAGE_SHIFT);\r\nBUG_ON(!(pgd_flags(spgd) & _PAGE_PRESENT));\r\n#endif\r\nreturn &page[pte_index(vaddr)];\r\n}\r\nstatic unsigned long gpgd_addr(struct lg_cpu *cpu, unsigned long vaddr)\r\n{\r\nunsigned int index = vaddr >> (PGDIR_SHIFT);\r\nreturn cpu->lg->pgdirs[cpu->cpu_pgd].gpgdir + index * sizeof(pgd_t);\r\n}\r\nstatic unsigned long gpmd_addr(pgd_t gpgd, unsigned long vaddr)\r\n{\r\nunsigned long gpage = pgd_pfn(gpgd) << PAGE_SHIFT;\r\nBUG_ON(!(pgd_flags(gpgd) & _PAGE_PRESENT));\r\nreturn gpage + pmd_index(vaddr) * sizeof(pmd_t);\r\n}\r\nstatic unsigned long gpte_addr(struct lg_cpu *cpu,\r\npmd_t gpmd, unsigned long vaddr)\r\n{\r\nunsigned long gpage = pmd_pfn(gpmd) << PAGE_SHIFT;\r\nBUG_ON(!(pmd_flags(gpmd) & _PAGE_PRESENT));\r\nreturn gpage + pte_index(vaddr) * sizeof(pte_t);\r\n}\r\nstatic unsigned long gpte_addr(struct lg_cpu *cpu,\r\npgd_t gpgd, unsigned long vaddr)\r\n{\r\nunsigned long gpage = pgd_pfn(gpgd) << PAGE_SHIFT;\r\nBUG_ON(!(pgd_flags(gpgd) & _PAGE_PRESENT));\r\nreturn gpage + pte_index(vaddr) * sizeof(pte_t);\r\n}\r\nstatic unsigned long get_pfn(unsigned long virtpfn, int write)\r\n{\r\nstruct page *page;\r\nif (get_user_pages_fast(virtpfn << PAGE_SHIFT, 1, write, &page) == 1)\r\nreturn page_to_pfn(page);\r\nreturn -1UL;\r\n}\r\nstatic pte_t gpte_to_spte(struct lg_cpu *cpu, pte_t gpte, int write)\r\n{\r\nunsigned long pfn, base, flags;\r\nflags = (pte_flags(gpte) & ~_PAGE_GLOBAL);\r\nbase = (unsigned long)cpu->lg->mem_base / PAGE_SIZE;\r\npfn = get_pfn(base + pte_pfn(gpte), write);\r\nif (pfn == -1UL) {\r\nkill_guest(cpu, "failed to get page %lu", pte_pfn(gpte));\r\nflags = 0;\r\n}\r\nreturn pfn_pte(pfn, __pgprot(flags));\r\n}\r\nstatic void release_pte(pte_t pte)\r\n{\r\nif (pte_flags(pte) & _PAGE_PRESENT)\r\nput_page(pte_page(pte));\r\n}\r\nstatic void check_gpte(struct lg_cpu *cpu, pte_t gpte)\r\n{\r\nif ((pte_flags(gpte) & _PAGE_PSE) ||\r\npte_pfn(gpte) >= cpu->lg->pfn_limit)\r\nkill_guest(cpu, "bad page table entry");\r\n}\r\nstatic void check_gpgd(struct lg_cpu *cpu, pgd_t gpgd)\r\n{\r\nif ((pgd_flags(gpgd) & ~CHECK_GPGD_MASK) ||\r\n(pgd_pfn(gpgd) >= cpu->lg->pfn_limit))\r\nkill_guest(cpu, "bad page directory entry");\r\n}\r\nstatic void check_gpmd(struct lg_cpu *cpu, pmd_t gpmd)\r\n{\r\nif ((pmd_flags(gpmd) & ~_PAGE_TABLE) ||\r\n(pmd_pfn(gpmd) >= cpu->lg->pfn_limit))\r\nkill_guest(cpu, "bad page middle directory entry");\r\n}\r\nbool demand_page(struct lg_cpu *cpu, unsigned long vaddr, int errcode)\r\n{\r\npgd_t gpgd;\r\npgd_t *spgd;\r\nunsigned long gpte_ptr;\r\npte_t gpte;\r\npte_t *spte;\r\n#ifdef CONFIG_X86_PAE\r\npmd_t *spmd;\r\npmd_t gpmd;\r\n#endif\r\nif (unlikely(cpu->linear_pages)) {\r\ngpgd = __pgd(CHECK_GPGD_MASK);\r\n} else {\r\ngpgd = lgread(cpu, gpgd_addr(cpu, vaddr), pgd_t);\r\nif (!(pgd_flags(gpgd) & _PAGE_PRESENT))\r\nreturn false;\r\n}\r\nspgd = spgd_addr(cpu, cpu->cpu_pgd, vaddr);\r\nif (!(pgd_flags(*spgd) & _PAGE_PRESENT)) {\r\nunsigned long ptepage = get_zeroed_page(GFP_KERNEL);\r\nif (!ptepage) {\r\nkill_guest(cpu, "out of memory allocating pte page");\r\nreturn false;\r\n}\r\ncheck_gpgd(cpu, gpgd);\r\nset_pgd(spgd, __pgd(__pa(ptepage) | pgd_flags(gpgd)));\r\n}\r\n#ifdef CONFIG_X86_PAE\r\nif (unlikely(cpu->linear_pages)) {\r\ngpmd = __pmd(_PAGE_TABLE);\r\n} else {\r\ngpmd = lgread(cpu, gpmd_addr(gpgd, vaddr), pmd_t);\r\nif (!(pmd_flags(gpmd) & _PAGE_PRESENT))\r\nreturn false;\r\n}\r\nspmd = spmd_addr(cpu, *spgd, vaddr);\r\nif (!(pmd_flags(*spmd) & _PAGE_PRESENT)) {\r\nunsigned long ptepage = get_zeroed_page(GFP_KERNEL);\r\nif (!ptepage) {\r\nkill_guest(cpu, "out of memory allocating pte page");\r\nreturn false;\r\n}\r\ncheck_gpmd(cpu, gpmd);\r\nset_pmd(spmd, __pmd(__pa(ptepage) | pmd_flags(gpmd)));\r\n}\r\ngpte_ptr = gpte_addr(cpu, gpmd, vaddr);\r\n#else\r\ngpte_ptr = gpte_addr(cpu, gpgd, vaddr);\r\n#endif\r\nif (unlikely(cpu->linear_pages)) {\r\ngpte = __pte((vaddr & PAGE_MASK) | _PAGE_RW | _PAGE_PRESENT);\r\n} else {\r\ngpte = lgread(cpu, gpte_ptr, pte_t);\r\n}\r\nif (!(pte_flags(gpte) & _PAGE_PRESENT))\r\nreturn false;\r\nif ((errcode & 2) && !(pte_flags(gpte) & _PAGE_RW))\r\nreturn false;\r\nif ((errcode & 4) && !(pte_flags(gpte) & _PAGE_USER))\r\nreturn false;\r\ncheck_gpte(cpu, gpte);\r\ngpte = pte_mkyoung(gpte);\r\nif (errcode & 2)\r\ngpte = pte_mkdirty(gpte);\r\nspte = spte_addr(cpu, *spgd, vaddr);\r\nrelease_pte(*spte);\r\nif (pte_dirty(gpte))\r\n*spte = gpte_to_spte(cpu, gpte, 1);\r\nelse\r\nset_pte(spte, gpte_to_spte(cpu, pte_wrprotect(gpte), 0));\r\nif (likely(!cpu->linear_pages))\r\nlgwrite(cpu, gpte_ptr, pte_t, gpte);\r\nreturn true;\r\n}\r\nstatic bool page_writable(struct lg_cpu *cpu, unsigned long vaddr)\r\n{\r\npgd_t *spgd;\r\nunsigned long flags;\r\n#ifdef CONFIG_X86_PAE\r\npmd_t *spmd;\r\n#endif\r\nspgd = spgd_addr(cpu, cpu->cpu_pgd, vaddr);\r\nif (!(pgd_flags(*spgd) & _PAGE_PRESENT))\r\nreturn false;\r\n#ifdef CONFIG_X86_PAE\r\nspmd = spmd_addr(cpu, *spgd, vaddr);\r\nif (!(pmd_flags(*spmd) & _PAGE_PRESENT))\r\nreturn false;\r\n#endif\r\nflags = pte_flags(*(spte_addr(cpu, *spgd, vaddr)));\r\nreturn (flags & (_PAGE_PRESENT|_PAGE_RW)) == (_PAGE_PRESENT|_PAGE_RW);\r\n}\r\nvoid pin_page(struct lg_cpu *cpu, unsigned long vaddr)\r\n{\r\nif (!page_writable(cpu, vaddr) && !demand_page(cpu, vaddr, 2))\r\nkill_guest(cpu, "bad stack page %#lx", vaddr);\r\n}\r\nstatic void release_pmd(pmd_t *spmd)\r\n{\r\nif (pmd_flags(*spmd) & _PAGE_PRESENT) {\r\nunsigned int i;\r\npte_t *ptepage = __va(pmd_pfn(*spmd) << PAGE_SHIFT);\r\nfor (i = 0; i < PTRS_PER_PTE; i++)\r\nrelease_pte(ptepage[i]);\r\nfree_page((long)ptepage);\r\nset_pmd(spmd, __pmd(0));\r\n}\r\n}\r\nstatic void release_pgd(pgd_t *spgd)\r\n{\r\nif (pgd_flags(*spgd) & _PAGE_PRESENT) {\r\nunsigned int i;\r\npmd_t *pmdpage = __va(pgd_pfn(*spgd) << PAGE_SHIFT);\r\nfor (i = 0; i < PTRS_PER_PMD; i++)\r\nrelease_pmd(&pmdpage[i]);\r\nfree_page((long)pmdpage);\r\nset_pgd(spgd, __pgd(0));\r\n}\r\n}\r\nstatic void release_pgd(pgd_t *spgd)\r\n{\r\nif (pgd_flags(*spgd) & _PAGE_PRESENT) {\r\nunsigned int i;\r\npte_t *ptepage = __va(pgd_pfn(*spgd) << PAGE_SHIFT);\r\nfor (i = 0; i < PTRS_PER_PTE; i++)\r\nrelease_pte(ptepage[i]);\r\nfree_page((long)ptepage);\r\n*spgd = __pgd(0);\r\n}\r\n}\r\nstatic void flush_user_mappings(struct lguest *lg, int idx)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < pgd_index(lg->kernel_address); i++)\r\nrelease_pgd(lg->pgdirs[idx].pgdir + i);\r\n}\r\nvoid guest_pagetable_flush_user(struct lg_cpu *cpu)\r\n{\r\nflush_user_mappings(cpu->lg, cpu->cpu_pgd);\r\n}\r\nunsigned long guest_pa(struct lg_cpu *cpu, unsigned long vaddr)\r\n{\r\npgd_t gpgd;\r\npte_t gpte;\r\n#ifdef CONFIG_X86_PAE\r\npmd_t gpmd;\r\n#endif\r\nif (unlikely(cpu->linear_pages))\r\nreturn vaddr;\r\ngpgd = lgread(cpu, gpgd_addr(cpu, vaddr), pgd_t);\r\nif (!(pgd_flags(gpgd) & _PAGE_PRESENT)) {\r\nkill_guest(cpu, "Bad address %#lx", vaddr);\r\nreturn -1UL;\r\n}\r\n#ifdef CONFIG_X86_PAE\r\ngpmd = lgread(cpu, gpmd_addr(gpgd, vaddr), pmd_t);\r\nif (!(pmd_flags(gpmd) & _PAGE_PRESENT))\r\nkill_guest(cpu, "Bad address %#lx", vaddr);\r\ngpte = lgread(cpu, gpte_addr(cpu, gpmd, vaddr), pte_t);\r\n#else\r\ngpte = lgread(cpu, gpte_addr(cpu, gpgd, vaddr), pte_t);\r\n#endif\r\nif (!(pte_flags(gpte) & _PAGE_PRESENT))\r\nkill_guest(cpu, "Bad address %#lx", vaddr);\r\nreturn pte_pfn(gpte) * PAGE_SIZE | (vaddr & ~PAGE_MASK);\r\n}\r\nstatic unsigned int find_pgdir(struct lguest *lg, unsigned long pgtable)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < ARRAY_SIZE(lg->pgdirs); i++)\r\nif (lg->pgdirs[i].pgdir && lg->pgdirs[i].gpgdir == pgtable)\r\nbreak;\r\nreturn i;\r\n}\r\nstatic unsigned int new_pgdir(struct lg_cpu *cpu,\r\nunsigned long gpgdir,\r\nint *blank_pgdir)\r\n{\r\nunsigned int next;\r\n#ifdef CONFIG_X86_PAE\r\npmd_t *pmd_table;\r\n#endif\r\nnext = random32() % ARRAY_SIZE(cpu->lg->pgdirs);\r\nif (!cpu->lg->pgdirs[next].pgdir) {\r\ncpu->lg->pgdirs[next].pgdir =\r\n(pgd_t *)get_zeroed_page(GFP_KERNEL);\r\nif (!cpu->lg->pgdirs[next].pgdir)\r\nnext = cpu->cpu_pgd;\r\nelse {\r\n#ifdef CONFIG_X86_PAE\r\npmd_table = (pmd_t *)get_zeroed_page(GFP_KERNEL);\r\nif (!pmd_table) {\r\nfree_page((long)cpu->lg->pgdirs[next].pgdir);\r\nset_pgd(cpu->lg->pgdirs[next].pgdir, __pgd(0));\r\nnext = cpu->cpu_pgd;\r\n} else {\r\nset_pgd(cpu->lg->pgdirs[next].pgdir +\r\nSWITCHER_PGD_INDEX,\r\n__pgd(__pa(pmd_table) | _PAGE_PRESENT));\r\n*blank_pgdir = 1;\r\n}\r\n#else\r\n*blank_pgdir = 1;\r\n#endif\r\n}\r\n}\r\ncpu->lg->pgdirs[next].gpgdir = gpgdir;\r\nflush_user_mappings(cpu->lg, next);\r\nreturn next;\r\n}\r\nstatic void release_all_pagetables(struct lguest *lg)\r\n{\r\nunsigned int i, j;\r\nfor (i = 0; i < ARRAY_SIZE(lg->pgdirs); i++)\r\nif (lg->pgdirs[i].pgdir) {\r\n#ifdef CONFIG_X86_PAE\r\npgd_t *spgd;\r\npmd_t *pmdpage;\r\nunsigned int k;\r\nspgd = lg->pgdirs[i].pgdir + SWITCHER_PGD_INDEX;\r\npmdpage = __va(pgd_pfn(*spgd) << PAGE_SHIFT);\r\nfor (k = 0; k < SWITCHER_PMD_INDEX; k++)\r\nrelease_pmd(&pmdpage[k]);\r\n#endif\r\nfor (j = 0; j < SWITCHER_PGD_INDEX; j++)\r\nrelease_pgd(lg->pgdirs[i].pgdir + j);\r\n}\r\n}\r\nvoid guest_pagetable_clear_all(struct lg_cpu *cpu)\r\n{\r\nrelease_all_pagetables(cpu->lg);\r\npin_stack_pages(cpu);\r\n}\r\nvoid guest_new_pagetable(struct lg_cpu *cpu, unsigned long pgtable)\r\n{\r\nint newpgdir, repin = 0;\r\nif (unlikely(cpu->linear_pages)) {\r\nrelease_all_pagetables(cpu->lg);\r\ncpu->linear_pages = false;\r\nnewpgdir = ARRAY_SIZE(cpu->lg->pgdirs);\r\n} else {\r\nnewpgdir = find_pgdir(cpu->lg, pgtable);\r\n}\r\nif (newpgdir == ARRAY_SIZE(cpu->lg->pgdirs))\r\nnewpgdir = new_pgdir(cpu, pgtable, &repin);\r\ncpu->cpu_pgd = newpgdir;\r\nif (repin)\r\npin_stack_pages(cpu);\r\n}\r\nstatic void do_set_pte(struct lg_cpu *cpu, int idx,\r\nunsigned long vaddr, pte_t gpte)\r\n{\r\npgd_t *spgd = spgd_addr(cpu, idx, vaddr);\r\n#ifdef CONFIG_X86_PAE\r\npmd_t *spmd;\r\n#endif\r\nif (pgd_flags(*spgd) & _PAGE_PRESENT) {\r\n#ifdef CONFIG_X86_PAE\r\nspmd = spmd_addr(cpu, *spgd, vaddr);\r\nif (pmd_flags(*spmd) & _PAGE_PRESENT) {\r\n#endif\r\npte_t *spte = spte_addr(cpu, *spgd, vaddr);\r\nrelease_pte(*spte);\r\nif (pte_flags(gpte) & (_PAGE_DIRTY | _PAGE_ACCESSED)) {\r\ncheck_gpte(cpu, gpte);\r\nset_pte(spte,\r\ngpte_to_spte(cpu, gpte,\r\npte_flags(gpte) & _PAGE_DIRTY));\r\n} else {\r\nset_pte(spte, __pte(0));\r\n}\r\n#ifdef CONFIG_X86_PAE\r\n}\r\n#endif\r\n}\r\n}\r\nvoid guest_set_pte(struct lg_cpu *cpu,\r\nunsigned long gpgdir, unsigned long vaddr, pte_t gpte)\r\n{\r\nif (vaddr >= cpu->lg->kernel_address) {\r\nunsigned int i;\r\nfor (i = 0; i < ARRAY_SIZE(cpu->lg->pgdirs); i++)\r\nif (cpu->lg->pgdirs[i].pgdir)\r\ndo_set_pte(cpu, i, vaddr, gpte);\r\n} else {\r\nint pgdir = find_pgdir(cpu->lg, gpgdir);\r\nif (pgdir != ARRAY_SIZE(cpu->lg->pgdirs))\r\ndo_set_pte(cpu, pgdir, vaddr, gpte);\r\n}\r\n}\r\nvoid guest_set_pgd(struct lguest *lg, unsigned long gpgdir, u32 idx)\r\n{\r\nint pgdir;\r\nif (idx >= SWITCHER_PGD_INDEX)\r\nreturn;\r\npgdir = find_pgdir(lg, gpgdir);\r\nif (pgdir < ARRAY_SIZE(lg->pgdirs))\r\nrelease_pgd(lg->pgdirs[pgdir].pgdir + idx);\r\n}\r\nvoid guest_set_pmd(struct lguest *lg, unsigned long pmdp, u32 idx)\r\n{\r\nguest_pagetable_clear_all(&lg->cpus[0]);\r\n}\r\nint init_guest_pagetable(struct lguest *lg)\r\n{\r\nstruct lg_cpu *cpu = &lg->cpus[0];\r\nint allocated = 0;\r\ncpu->cpu_pgd = new_pgdir(cpu, 0, &allocated);\r\nif (!allocated)\r\nreturn -ENOMEM;\r\ncpu->linear_pages = true;\r\nreturn 0;\r\n}\r\nvoid page_table_guest_data_init(struct lg_cpu *cpu)\r\n{\r\nif (get_user(cpu->lg->kernel_address,\r\n&cpu->lg->lguest_data->kernel_address)\r\n|| put_user(RESERVE_MEM * 1024 * 1024,\r\n&cpu->lg->lguest_data->reserve_mem)) {\r\nkill_guest(cpu, "bad guest page %p", cpu->lg->lguest_data);\r\nreturn;\r\n}\r\n#ifdef CONFIG_X86_PAE\r\nif (pgd_index(cpu->lg->kernel_address) == SWITCHER_PGD_INDEX &&\r\npmd_index(cpu->lg->kernel_address) == SWITCHER_PMD_INDEX)\r\n#else\r\nif (pgd_index(cpu->lg->kernel_address) >= SWITCHER_PGD_INDEX)\r\n#endif\r\nkill_guest(cpu, "bad kernel address %#lx",\r\ncpu->lg->kernel_address);\r\n}\r\nvoid free_guest_pagetable(struct lguest *lg)\r\n{\r\nunsigned int i;\r\nrelease_all_pagetables(lg);\r\nfor (i = 0; i < ARRAY_SIZE(lg->pgdirs); i++)\r\nfree_page((long)lg->pgdirs[i].pgdir);\r\n}\r\nvoid map_switcher_in_guest(struct lg_cpu *cpu, struct lguest_pages *pages)\r\n{\r\npte_t *switcher_pte_page = __this_cpu_read(switcher_pte_pages);\r\npte_t regs_pte;\r\n#ifdef CONFIG_X86_PAE\r\npmd_t switcher_pmd;\r\npmd_t *pmd_table;\r\nswitcher_pmd = pfn_pmd(__pa(switcher_pte_page) >> PAGE_SHIFT,\r\nPAGE_KERNEL_EXEC);\r\npmd_table = __va(pgd_pfn(cpu->lg->\r\npgdirs[cpu->cpu_pgd].pgdir[SWITCHER_PGD_INDEX])\r\n<< PAGE_SHIFT);\r\nset_pmd(&pmd_table[SWITCHER_PMD_INDEX], switcher_pmd);\r\n#else\r\npgd_t switcher_pgd;\r\nswitcher_pgd = __pgd(__pa(switcher_pte_page) | __PAGE_KERNEL_EXEC);\r\ncpu->lg->pgdirs[cpu->cpu_pgd].pgdir[SWITCHER_PGD_INDEX] = switcher_pgd;\r\n#endif\r\nregs_pte = pfn_pte(__pa(cpu->regs_page) >> PAGE_SHIFT, PAGE_KERNEL);\r\nset_pte(&switcher_pte_page[pte_index((unsigned long)pages)], regs_pte);\r\n}\r\nstatic void free_switcher_pte_pages(void)\r\n{\r\nunsigned int i;\r\nfor_each_possible_cpu(i)\r\nfree_page((long)switcher_pte_page(i));\r\n}\r\nstatic __init void populate_switcher_pte_page(unsigned int cpu,\r\nstruct page *switcher_page[],\r\nunsigned int pages)\r\n{\r\nunsigned int i;\r\npte_t *pte = switcher_pte_page(cpu);\r\nfor (i = 0; i < pages; i++) {\r\nset_pte(&pte[i], mk_pte(switcher_page[i],\r\n__pgprot(_PAGE_PRESENT|_PAGE_ACCESSED)));\r\n}\r\ni = pages + cpu*2;\r\nset_pte(&pte[i], pfn_pte(page_to_pfn(switcher_page[i]),\r\n__pgprot(_PAGE_PRESENT|_PAGE_ACCESSED|_PAGE_RW)));\r\nset_pte(&pte[i+1], pfn_pte(page_to_pfn(switcher_page[i+1]),\r\n__pgprot(_PAGE_PRESENT|_PAGE_ACCESSED)));\r\n}\r\n__init int init_pagetables(struct page **switcher_page, unsigned int pages)\r\n{\r\nunsigned int i;\r\nfor_each_possible_cpu(i) {\r\nswitcher_pte_page(i) = (pte_t *)get_zeroed_page(GFP_KERNEL);\r\nif (!switcher_pte_page(i)) {\r\nfree_switcher_pte_pages();\r\nreturn -ENOMEM;\r\n}\r\npopulate_switcher_pte_page(i, switcher_page, pages);\r\n}\r\nreturn 0;\r\n}\r\nvoid free_pagetables(void)\r\n{\r\nfree_switcher_pte_pages();\r\n}
