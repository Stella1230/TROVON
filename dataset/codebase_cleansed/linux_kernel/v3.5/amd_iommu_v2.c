static u16 device_id(struct pci_dev *pdev)\r\n{\r\nu16 devid;\r\ndevid = pdev->bus->number;\r\ndevid = (devid << 8) | pdev->devfn;\r\nreturn devid;\r\n}\r\nstatic struct device_state *get_device_state(u16 devid)\r\n{\r\nstruct device_state *dev_state;\r\nunsigned long flags;\r\nspin_lock_irqsave(&state_lock, flags);\r\ndev_state = state_table[devid];\r\nif (dev_state != NULL)\r\natomic_inc(&dev_state->count);\r\nspin_unlock_irqrestore(&state_lock, flags);\r\nreturn dev_state;\r\n}\r\nstatic void free_device_state(struct device_state *dev_state)\r\n{\r\niommu_detach_device(dev_state->domain, &dev_state->pdev->dev);\r\niommu_domain_free(dev_state->domain);\r\nkfree(dev_state);\r\n}\r\nstatic void put_device_state(struct device_state *dev_state)\r\n{\r\nif (atomic_dec_and_test(&dev_state->count))\r\nwake_up(&dev_state->wq);\r\n}\r\nstatic void put_device_state_wait(struct device_state *dev_state)\r\n{\r\nDEFINE_WAIT(wait);\r\nprepare_to_wait(&dev_state->wq, &wait, TASK_UNINTERRUPTIBLE);\r\nif (!atomic_dec_and_test(&dev_state->count))\r\nschedule();\r\nfinish_wait(&dev_state->wq, &wait);\r\nfree_device_state(dev_state);\r\n}\r\nstatic void link_pasid_state(struct pasid_state *pasid_state)\r\n{\r\nspin_lock(&ps_lock);\r\nlist_add_tail(&pasid_state->list, &pasid_state_list);\r\nspin_unlock(&ps_lock);\r\n}\r\nstatic void __unlink_pasid_state(struct pasid_state *pasid_state)\r\n{\r\nlist_del(&pasid_state->list);\r\n}\r\nstatic void unlink_pasid_state(struct pasid_state *pasid_state)\r\n{\r\nspin_lock(&ps_lock);\r\n__unlink_pasid_state(pasid_state);\r\nspin_unlock(&ps_lock);\r\n}\r\nstatic struct pasid_state **__get_pasid_state_ptr(struct device_state *dev_state,\r\nint pasid, bool alloc)\r\n{\r\nstruct pasid_state **root, **ptr;\r\nint level, index;\r\nlevel = dev_state->pasid_levels;\r\nroot = dev_state->states;\r\nwhile (true) {\r\nindex = (pasid >> (9 * level)) & 0x1ff;\r\nptr = &root[index];\r\nif (level == 0)\r\nbreak;\r\nif (*ptr == NULL) {\r\nif (!alloc)\r\nreturn NULL;\r\n*ptr = (void *)get_zeroed_page(GFP_ATOMIC);\r\nif (*ptr == NULL)\r\nreturn NULL;\r\n}\r\nroot = (struct pasid_state **)*ptr;\r\nlevel -= 1;\r\n}\r\nreturn ptr;\r\n}\r\nstatic int set_pasid_state(struct device_state *dev_state,\r\nstruct pasid_state *pasid_state,\r\nint pasid)\r\n{\r\nstruct pasid_state **ptr;\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(&dev_state->lock, flags);\r\nptr = __get_pasid_state_ptr(dev_state, pasid, true);\r\nret = -ENOMEM;\r\nif (ptr == NULL)\r\ngoto out_unlock;\r\nret = -ENOMEM;\r\nif (*ptr != NULL)\r\ngoto out_unlock;\r\n*ptr = pasid_state;\r\nret = 0;\r\nout_unlock:\r\nspin_unlock_irqrestore(&dev_state->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void clear_pasid_state(struct device_state *dev_state, int pasid)\r\n{\r\nstruct pasid_state **ptr;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev_state->lock, flags);\r\nptr = __get_pasid_state_ptr(dev_state, pasid, true);\r\nif (ptr == NULL)\r\ngoto out_unlock;\r\n*ptr = NULL;\r\nout_unlock:\r\nspin_unlock_irqrestore(&dev_state->lock, flags);\r\n}\r\nstatic struct pasid_state *get_pasid_state(struct device_state *dev_state,\r\nint pasid)\r\n{\r\nstruct pasid_state **ptr, *ret = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev_state->lock, flags);\r\nptr = __get_pasid_state_ptr(dev_state, pasid, false);\r\nif (ptr == NULL)\r\ngoto out_unlock;\r\nret = *ptr;\r\nif (ret)\r\natomic_inc(&ret->count);\r\nout_unlock:\r\nspin_unlock_irqrestore(&dev_state->lock, flags);\r\nreturn ret;\r\n}\r\nstatic void free_pasid_state(struct pasid_state *pasid_state)\r\n{\r\nkfree(pasid_state);\r\n}\r\nstatic void put_pasid_state(struct pasid_state *pasid_state)\r\n{\r\nif (atomic_dec_and_test(&pasid_state->count)) {\r\nput_device_state(pasid_state->device_state);\r\nwake_up(&pasid_state->wq);\r\n}\r\n}\r\nstatic void put_pasid_state_wait(struct pasid_state *pasid_state)\r\n{\r\nDEFINE_WAIT(wait);\r\nprepare_to_wait(&pasid_state->wq, &wait, TASK_UNINTERRUPTIBLE);\r\nif (atomic_dec_and_test(&pasid_state->count))\r\nput_device_state(pasid_state->device_state);\r\nelse\r\nschedule();\r\nfinish_wait(&pasid_state->wq, &wait);\r\nmmput(pasid_state->mm);\r\nfree_pasid_state(pasid_state);\r\n}\r\nstatic void __unbind_pasid(struct pasid_state *pasid_state)\r\n{\r\nstruct iommu_domain *domain;\r\ndomain = pasid_state->device_state->domain;\r\namd_iommu_domain_clear_gcr3(domain, pasid_state->pasid);\r\nclear_pasid_state(pasid_state->device_state, pasid_state->pasid);\r\nflush_workqueue(iommu_wq);\r\nmmu_notifier_unregister(&pasid_state->mn, pasid_state->mm);\r\nput_pasid_state(pasid_state);\r\n}\r\nstatic void unbind_pasid(struct device_state *dev_state, int pasid)\r\n{\r\nstruct pasid_state *pasid_state;\r\npasid_state = get_pasid_state(dev_state, pasid);\r\nif (pasid_state == NULL)\r\nreturn;\r\nunlink_pasid_state(pasid_state);\r\n__unbind_pasid(pasid_state);\r\nput_pasid_state_wait(pasid_state);\r\n}\r\nstatic void free_pasid_states_level1(struct pasid_state **tbl)\r\n{\r\nint i;\r\nfor (i = 0; i < 512; ++i) {\r\nif (tbl[i] == NULL)\r\ncontinue;\r\nfree_page((unsigned long)tbl[i]);\r\n}\r\n}\r\nstatic void free_pasid_states_level2(struct pasid_state **tbl)\r\n{\r\nstruct pasid_state **ptr;\r\nint i;\r\nfor (i = 0; i < 512; ++i) {\r\nif (tbl[i] == NULL)\r\ncontinue;\r\nptr = (struct pasid_state **)tbl[i];\r\nfree_pasid_states_level1(ptr);\r\n}\r\n}\r\nstatic void free_pasid_states(struct device_state *dev_state)\r\n{\r\nstruct pasid_state *pasid_state;\r\nint i;\r\nfor (i = 0; i < dev_state->max_pasids; ++i) {\r\npasid_state = get_pasid_state(dev_state, i);\r\nif (pasid_state == NULL)\r\ncontinue;\r\nput_pasid_state(pasid_state);\r\nunbind_pasid(dev_state, i);\r\n}\r\nif (dev_state->pasid_levels == 2)\r\nfree_pasid_states_level2(dev_state->states);\r\nelse if (dev_state->pasid_levels == 1)\r\nfree_pasid_states_level1(dev_state->states);\r\nelse if (dev_state->pasid_levels != 0)\r\nBUG();\r\nfree_page((unsigned long)dev_state->states);\r\n}\r\nstatic struct pasid_state *mn_to_state(struct mmu_notifier *mn)\r\n{\r\nreturn container_of(mn, struct pasid_state, mn);\r\n}\r\nstatic void __mn_flush_page(struct mmu_notifier *mn,\r\nunsigned long address)\r\n{\r\nstruct pasid_state *pasid_state;\r\nstruct device_state *dev_state;\r\npasid_state = mn_to_state(mn);\r\ndev_state = pasid_state->device_state;\r\namd_iommu_flush_page(dev_state->domain, pasid_state->pasid, address);\r\n}\r\nstatic int mn_clear_flush_young(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long address)\r\n{\r\n__mn_flush_page(mn, address);\r\nreturn 0;\r\n}\r\nstatic void mn_change_pte(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long address,\r\npte_t pte)\r\n{\r\n__mn_flush_page(mn, address);\r\n}\r\nstatic void mn_invalidate_page(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long address)\r\n{\r\n__mn_flush_page(mn, address);\r\n}\r\nstatic void mn_invalidate_range_start(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start, unsigned long end)\r\n{\r\nstruct pasid_state *pasid_state;\r\nstruct device_state *dev_state;\r\npasid_state = mn_to_state(mn);\r\ndev_state = pasid_state->device_state;\r\namd_iommu_domain_set_gcr3(dev_state->domain, pasid_state->pasid,\r\n__pa(empty_page_table));\r\n}\r\nstatic void mn_invalidate_range_end(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start, unsigned long end)\r\n{\r\nstruct pasid_state *pasid_state;\r\nstruct device_state *dev_state;\r\npasid_state = mn_to_state(mn);\r\ndev_state = pasid_state->device_state;\r\namd_iommu_domain_set_gcr3(dev_state->domain, pasid_state->pasid,\r\n__pa(pasid_state->mm->pgd));\r\n}\r\nstatic void set_pri_tag_status(struct pasid_state *pasid_state,\r\nu16 tag, int status)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&pasid_state->lock, flags);\r\npasid_state->pri[tag].status = status;\r\nspin_unlock_irqrestore(&pasid_state->lock, flags);\r\n}\r\nstatic void finish_pri_tag(struct device_state *dev_state,\r\nstruct pasid_state *pasid_state,\r\nu16 tag)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&pasid_state->lock, flags);\r\nif (atomic_dec_and_test(&pasid_state->pri[tag].inflight) &&\r\npasid_state->pri[tag].finish) {\r\namd_iommu_complete_ppr(dev_state->pdev, pasid_state->pasid,\r\npasid_state->pri[tag].status, tag);\r\npasid_state->pri[tag].finish = false;\r\npasid_state->pri[tag].status = PPR_SUCCESS;\r\n}\r\nspin_unlock_irqrestore(&pasid_state->lock, flags);\r\n}\r\nstatic void do_fault(struct work_struct *work)\r\n{\r\nstruct fault *fault = container_of(work, struct fault, work);\r\nint npages, write;\r\nstruct page *page;\r\nwrite = !!(fault->flags & PPR_FAULT_WRITE);\r\nnpages = get_user_pages(fault->state->task, fault->state->mm,\r\nfault->address, 1, write, 0, &page, NULL);\r\nif (npages == 1) {\r\nput_page(page);\r\n} else if (fault->dev_state->inv_ppr_cb) {\r\nint status;\r\nstatus = fault->dev_state->inv_ppr_cb(fault->dev_state->pdev,\r\nfault->pasid,\r\nfault->address,\r\nfault->flags);\r\nswitch (status) {\r\ncase AMD_IOMMU_INV_PRI_RSP_SUCCESS:\r\nset_pri_tag_status(fault->state, fault->tag, PPR_SUCCESS);\r\nbreak;\r\ncase AMD_IOMMU_INV_PRI_RSP_INVALID:\r\nset_pri_tag_status(fault->state, fault->tag, PPR_INVALID);\r\nbreak;\r\ncase AMD_IOMMU_INV_PRI_RSP_FAIL:\r\nset_pri_tag_status(fault->state, fault->tag, PPR_FAILURE);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n} else {\r\nset_pri_tag_status(fault->state, fault->tag, PPR_INVALID);\r\n}\r\nfinish_pri_tag(fault->dev_state, fault->state, fault->tag);\r\nput_pasid_state(fault->state);\r\nkfree(fault);\r\n}\r\nstatic int ppr_notifier(struct notifier_block *nb, unsigned long e, void *data)\r\n{\r\nstruct amd_iommu_fault *iommu_fault;\r\nstruct pasid_state *pasid_state;\r\nstruct device_state *dev_state;\r\nunsigned long flags;\r\nstruct fault *fault;\r\nbool finish;\r\nu16 tag;\r\nint ret;\r\niommu_fault = data;\r\ntag = iommu_fault->tag & 0x1ff;\r\nfinish = (iommu_fault->tag >> 9) & 1;\r\nret = NOTIFY_DONE;\r\ndev_state = get_device_state(iommu_fault->device_id);\r\nif (dev_state == NULL)\r\ngoto out;\r\npasid_state = get_pasid_state(dev_state, iommu_fault->pasid);\r\nif (pasid_state == NULL) {\r\namd_iommu_complete_ppr(dev_state->pdev, iommu_fault->pasid,\r\nPPR_INVALID, tag);\r\ngoto out_drop_state;\r\n}\r\nspin_lock_irqsave(&pasid_state->lock, flags);\r\natomic_inc(&pasid_state->pri[tag].inflight);\r\nif (finish)\r\npasid_state->pri[tag].finish = true;\r\nspin_unlock_irqrestore(&pasid_state->lock, flags);\r\nfault = kzalloc(sizeof(*fault), GFP_ATOMIC);\r\nif (fault == NULL) {\r\nfinish_pri_tag(dev_state, pasid_state, tag);\r\ngoto out_drop_state;\r\n}\r\nfault->dev_state = dev_state;\r\nfault->address = iommu_fault->address;\r\nfault->state = pasid_state;\r\nfault->tag = tag;\r\nfault->finish = finish;\r\nfault->flags = iommu_fault->flags;\r\nINIT_WORK(&fault->work, do_fault);\r\nqueue_work(iommu_wq, &fault->work);\r\nret = NOTIFY_OK;\r\nout_drop_state:\r\nput_device_state(dev_state);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int task_exit(struct notifier_block *nb, unsigned long e, void *data)\r\n{\r\nstruct pasid_state *pasid_state;\r\nstruct task_struct *task;\r\ntask = data;\r\nagain:\r\nspin_lock(&ps_lock);\r\nlist_for_each_entry(pasid_state, &pasid_state_list, list) {\r\nstruct device_state *dev_state;\r\nint pasid;\r\nif (pasid_state->task != task)\r\ncontinue;\r\nspin_unlock(&ps_lock);\r\ndev_state = pasid_state->device_state;\r\npasid = pasid_state->pasid;\r\nif (pasid_state->device_state->inv_ctx_cb)\r\ndev_state->inv_ctx_cb(dev_state->pdev, pasid);\r\nunbind_pasid(dev_state, pasid);\r\ngoto again;\r\n}\r\nspin_unlock(&ps_lock);\r\nreturn NOTIFY_OK;\r\n}\r\nint amd_iommu_bind_pasid(struct pci_dev *pdev, int pasid,\r\nstruct task_struct *task)\r\n{\r\nstruct pasid_state *pasid_state;\r\nstruct device_state *dev_state;\r\nu16 devid;\r\nint ret;\r\nmight_sleep();\r\nif (!amd_iommu_v2_supported())\r\nreturn -ENODEV;\r\ndevid = device_id(pdev);\r\ndev_state = get_device_state(devid);\r\nif (dev_state == NULL)\r\nreturn -EINVAL;\r\nret = -EINVAL;\r\nif (pasid < 0 || pasid >= dev_state->max_pasids)\r\ngoto out;\r\nret = -ENOMEM;\r\npasid_state = kzalloc(sizeof(*pasid_state), GFP_KERNEL);\r\nif (pasid_state == NULL)\r\ngoto out;\r\natomic_set(&pasid_state->count, 1);\r\ninit_waitqueue_head(&pasid_state->wq);\r\npasid_state->task = task;\r\npasid_state->mm = get_task_mm(task);\r\npasid_state->device_state = dev_state;\r\npasid_state->pasid = pasid;\r\npasid_state->mn.ops = &iommu_mn;\r\nif (pasid_state->mm == NULL)\r\ngoto out_free;\r\nmmu_notifier_register(&pasid_state->mn, pasid_state->mm);\r\nret = set_pasid_state(dev_state, pasid_state, pasid);\r\nif (ret)\r\ngoto out_unregister;\r\nret = amd_iommu_domain_set_gcr3(dev_state->domain, pasid,\r\n__pa(pasid_state->mm->pgd));\r\nif (ret)\r\ngoto out_clear_state;\r\nlink_pasid_state(pasid_state);\r\nreturn 0;\r\nout_clear_state:\r\nclear_pasid_state(dev_state, pasid);\r\nout_unregister:\r\nmmu_notifier_unregister(&pasid_state->mn, pasid_state->mm);\r\nout_free:\r\nfree_pasid_state(pasid_state);\r\nout:\r\nput_device_state(dev_state);\r\nreturn ret;\r\n}\r\nvoid amd_iommu_unbind_pasid(struct pci_dev *pdev, int pasid)\r\n{\r\nstruct device_state *dev_state;\r\nu16 devid;\r\nmight_sleep();\r\nif (!amd_iommu_v2_supported())\r\nreturn;\r\ndevid = device_id(pdev);\r\ndev_state = get_device_state(devid);\r\nif (dev_state == NULL)\r\nreturn;\r\nif (pasid < 0 || pasid >= dev_state->max_pasids)\r\ngoto out;\r\nunbind_pasid(dev_state, pasid);\r\nout:\r\nput_device_state(dev_state);\r\n}\r\nint amd_iommu_init_device(struct pci_dev *pdev, int pasids)\r\n{\r\nstruct device_state *dev_state;\r\nunsigned long flags;\r\nint ret, tmp;\r\nu16 devid;\r\nmight_sleep();\r\nif (!amd_iommu_v2_supported())\r\nreturn -ENODEV;\r\nif (pasids <= 0 || pasids > (PASID_MASK + 1))\r\nreturn -EINVAL;\r\ndevid = device_id(pdev);\r\ndev_state = kzalloc(sizeof(*dev_state), GFP_KERNEL);\r\nif (dev_state == NULL)\r\nreturn -ENOMEM;\r\nspin_lock_init(&dev_state->lock);\r\ninit_waitqueue_head(&dev_state->wq);\r\ndev_state->pdev = pdev;\r\ntmp = pasids;\r\nfor (dev_state->pasid_levels = 0; (tmp - 1) & ~0x1ff; tmp >>= 9)\r\ndev_state->pasid_levels += 1;\r\natomic_set(&dev_state->count, 1);\r\ndev_state->max_pasids = pasids;\r\nret = -ENOMEM;\r\ndev_state->states = (void *)get_zeroed_page(GFP_KERNEL);\r\nif (dev_state->states == NULL)\r\ngoto out_free_dev_state;\r\ndev_state->domain = iommu_domain_alloc(&pci_bus_type);\r\nif (dev_state->domain == NULL)\r\ngoto out_free_states;\r\namd_iommu_domain_direct_map(dev_state->domain);\r\nret = amd_iommu_domain_enable_v2(dev_state->domain, pasids);\r\nif (ret)\r\ngoto out_free_domain;\r\nret = iommu_attach_device(dev_state->domain, &pdev->dev);\r\nif (ret != 0)\r\ngoto out_free_domain;\r\nspin_lock_irqsave(&state_lock, flags);\r\nif (state_table[devid] != NULL) {\r\nspin_unlock_irqrestore(&state_lock, flags);\r\nret = -EBUSY;\r\ngoto out_free_domain;\r\n}\r\nstate_table[devid] = dev_state;\r\nspin_unlock_irqrestore(&state_lock, flags);\r\nreturn 0;\r\nout_free_domain:\r\niommu_domain_free(dev_state->domain);\r\nout_free_states:\r\nfree_page((unsigned long)dev_state->states);\r\nout_free_dev_state:\r\nkfree(dev_state);\r\nreturn ret;\r\n}\r\nvoid amd_iommu_free_device(struct pci_dev *pdev)\r\n{\r\nstruct device_state *dev_state;\r\nunsigned long flags;\r\nu16 devid;\r\nif (!amd_iommu_v2_supported())\r\nreturn;\r\ndevid = device_id(pdev);\r\nspin_lock_irqsave(&state_lock, flags);\r\ndev_state = state_table[devid];\r\nif (dev_state == NULL) {\r\nspin_unlock_irqrestore(&state_lock, flags);\r\nreturn;\r\n}\r\nstate_table[devid] = NULL;\r\nspin_unlock_irqrestore(&state_lock, flags);\r\nfree_pasid_states(dev_state);\r\nput_device_state_wait(dev_state);\r\n}\r\nint amd_iommu_set_invalid_ppr_cb(struct pci_dev *pdev,\r\namd_iommu_invalid_ppr_cb cb)\r\n{\r\nstruct device_state *dev_state;\r\nunsigned long flags;\r\nu16 devid;\r\nint ret;\r\nif (!amd_iommu_v2_supported())\r\nreturn -ENODEV;\r\ndevid = device_id(pdev);\r\nspin_lock_irqsave(&state_lock, flags);\r\nret = -EINVAL;\r\ndev_state = state_table[devid];\r\nif (dev_state == NULL)\r\ngoto out_unlock;\r\ndev_state->inv_ppr_cb = cb;\r\nret = 0;\r\nout_unlock:\r\nspin_unlock_irqrestore(&state_lock, flags);\r\nreturn ret;\r\n}\r\nint amd_iommu_set_invalidate_ctx_cb(struct pci_dev *pdev,\r\namd_iommu_invalidate_ctx cb)\r\n{\r\nstruct device_state *dev_state;\r\nunsigned long flags;\r\nu16 devid;\r\nint ret;\r\nif (!amd_iommu_v2_supported())\r\nreturn -ENODEV;\r\ndevid = device_id(pdev);\r\nspin_lock_irqsave(&state_lock, flags);\r\nret = -EINVAL;\r\ndev_state = state_table[devid];\r\nif (dev_state == NULL)\r\ngoto out_unlock;\r\ndev_state->inv_ctx_cb = cb;\r\nret = 0;\r\nout_unlock:\r\nspin_unlock_irqrestore(&state_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int __init amd_iommu_v2_init(void)\r\n{\r\nsize_t state_table_size;\r\nint ret;\r\npr_info("AMD IOMMUv2 driver by Joerg Roedel <joerg.roedel@amd.com>\n");\r\nif (!amd_iommu_v2_supported()) {\r\npr_info("AMD IOMMUv2 functionality not available on this sytem\n");\r\nreturn 0;\r\n}\r\nspin_lock_init(&state_lock);\r\nstate_table_size = MAX_DEVICES * sizeof(struct device_state *);\r\nstate_table = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(state_table_size));\r\nif (state_table == NULL)\r\nreturn -ENOMEM;\r\nret = -ENOMEM;\r\niommu_wq = create_workqueue("amd_iommu_v2");\r\nif (iommu_wq == NULL)\r\ngoto out_free;\r\nret = -ENOMEM;\r\nempty_page_table = (u64 *)get_zeroed_page(GFP_KERNEL);\r\nif (empty_page_table == NULL)\r\ngoto out_destroy_wq;\r\namd_iommu_register_ppr_notifier(&ppr_nb);\r\nprofile_event_register(PROFILE_TASK_EXIT, &profile_nb);\r\nreturn 0;\r\nout_destroy_wq:\r\ndestroy_workqueue(iommu_wq);\r\nout_free:\r\nfree_pages((unsigned long)state_table, get_order(state_table_size));\r\nreturn ret;\r\n}\r\nstatic void __exit amd_iommu_v2_exit(void)\r\n{\r\nstruct device_state *dev_state;\r\nsize_t state_table_size;\r\nint i;\r\nif (!amd_iommu_v2_supported())\r\nreturn;\r\nprofile_event_unregister(PROFILE_TASK_EXIT, &profile_nb);\r\namd_iommu_unregister_ppr_notifier(&ppr_nb);\r\nflush_workqueue(iommu_wq);\r\nfor (i = 0; i < MAX_DEVICES; ++i) {\r\ndev_state = get_device_state(i);\r\nif (dev_state == NULL)\r\ncontinue;\r\nWARN_ON_ONCE(1);\r\nput_device_state(dev_state);\r\namd_iommu_free_device(dev_state->pdev);\r\n}\r\ndestroy_workqueue(iommu_wq);\r\nstate_table_size = MAX_DEVICES * sizeof(struct device_state *);\r\nfree_pages((unsigned long)state_table, get_order(state_table_size));\r\nfree_page((unsigned long)empty_page_table);\r\n}
