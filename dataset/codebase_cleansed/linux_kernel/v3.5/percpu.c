static bool pcpu_addr_in_first_chunk(void *addr)\r\n{\r\nvoid *first_start = pcpu_first_chunk->base_addr;\r\nreturn addr >= first_start && addr < first_start + pcpu_unit_size;\r\n}\r\nstatic bool pcpu_addr_in_reserved_chunk(void *addr)\r\n{\r\nvoid *first_start = pcpu_first_chunk->base_addr;\r\nreturn addr >= first_start &&\r\naddr < first_start + pcpu_reserved_chunk_limit;\r\n}\r\nstatic int __pcpu_size_to_slot(int size)\r\n{\r\nint highbit = fls(size);\r\nreturn max(highbit - PCPU_SLOT_BASE_SHIFT + 2, 1);\r\n}\r\nstatic int pcpu_size_to_slot(int size)\r\n{\r\nif (size == pcpu_unit_size)\r\nreturn pcpu_nr_slots - 1;\r\nreturn __pcpu_size_to_slot(size);\r\n}\r\nstatic int pcpu_chunk_slot(const struct pcpu_chunk *chunk)\r\n{\r\nif (chunk->free_size < sizeof(int) || chunk->contig_hint < sizeof(int))\r\nreturn 0;\r\nreturn pcpu_size_to_slot(chunk->free_size);\r\n}\r\nstatic void pcpu_set_page_chunk(struct page *page, struct pcpu_chunk *pcpu)\r\n{\r\npage->index = (unsigned long)pcpu;\r\n}\r\nstatic struct pcpu_chunk *pcpu_get_page_chunk(struct page *page)\r\n{\r\nreturn (struct pcpu_chunk *)page->index;\r\n}\r\nstatic int __maybe_unused pcpu_page_idx(unsigned int cpu, int page_idx)\r\n{\r\nreturn pcpu_unit_map[cpu] * pcpu_unit_pages + page_idx;\r\n}\r\nstatic unsigned long pcpu_chunk_addr(struct pcpu_chunk *chunk,\r\nunsigned int cpu, int page_idx)\r\n{\r\nreturn (unsigned long)chunk->base_addr + pcpu_unit_offsets[cpu] +\r\n(page_idx << PAGE_SHIFT);\r\n}\r\nstatic void __maybe_unused pcpu_next_unpop(struct pcpu_chunk *chunk,\r\nint *rs, int *re, int end)\r\n{\r\n*rs = find_next_zero_bit(chunk->populated, end, *rs);\r\n*re = find_next_bit(chunk->populated, end, *rs + 1);\r\n}\r\nstatic void __maybe_unused pcpu_next_pop(struct pcpu_chunk *chunk,\r\nint *rs, int *re, int end)\r\n{\r\n*rs = find_next_bit(chunk->populated, end, *rs);\r\n*re = find_next_zero_bit(chunk->populated, end, *rs + 1);\r\n}\r\nstatic void *pcpu_mem_zalloc(size_t size)\r\n{\r\nif (WARN_ON_ONCE(!slab_is_available()))\r\nreturn NULL;\r\nif (size <= PAGE_SIZE)\r\nreturn kzalloc(size, GFP_KERNEL);\r\nelse\r\nreturn vzalloc(size);\r\n}\r\nstatic void pcpu_mem_free(void *ptr, size_t size)\r\n{\r\nif (size <= PAGE_SIZE)\r\nkfree(ptr);\r\nelse\r\nvfree(ptr);\r\n}\r\nstatic void pcpu_chunk_relocate(struct pcpu_chunk *chunk, int oslot)\r\n{\r\nint nslot = pcpu_chunk_slot(chunk);\r\nif (chunk != pcpu_reserved_chunk && oslot != nslot) {\r\nif (oslot < nslot)\r\nlist_move(&chunk->list, &pcpu_slot[nslot]);\r\nelse\r\nlist_move_tail(&chunk->list, &pcpu_slot[nslot]);\r\n}\r\n}\r\nstatic int pcpu_need_to_extend(struct pcpu_chunk *chunk)\r\n{\r\nint new_alloc;\r\nif (chunk->map_alloc >= chunk->map_used + 2)\r\nreturn 0;\r\nnew_alloc = PCPU_DFL_MAP_ALLOC;\r\nwhile (new_alloc < chunk->map_used + 2)\r\nnew_alloc *= 2;\r\nreturn new_alloc;\r\n}\r\nstatic int pcpu_extend_area_map(struct pcpu_chunk *chunk, int new_alloc)\r\n{\r\nint *old = NULL, *new = NULL;\r\nsize_t old_size = 0, new_size = new_alloc * sizeof(new[0]);\r\nunsigned long flags;\r\nnew = pcpu_mem_zalloc(new_size);\r\nif (!new)\r\nreturn -ENOMEM;\r\nspin_lock_irqsave(&pcpu_lock, flags);\r\nif (new_alloc <= chunk->map_alloc)\r\ngoto out_unlock;\r\nold_size = chunk->map_alloc * sizeof(chunk->map[0]);\r\nold = chunk->map;\r\nmemcpy(new, old, old_size);\r\nchunk->map_alloc = new_alloc;\r\nchunk->map = new;\r\nnew = NULL;\r\nout_unlock:\r\nspin_unlock_irqrestore(&pcpu_lock, flags);\r\npcpu_mem_free(old, old_size);\r\npcpu_mem_free(new, new_size);\r\nreturn 0;\r\n}\r\nstatic void pcpu_split_block(struct pcpu_chunk *chunk, int i,\r\nint head, int tail)\r\n{\r\nint nr_extra = !!head + !!tail;\r\nBUG_ON(chunk->map_alloc < chunk->map_used + nr_extra);\r\nmemmove(&chunk->map[i + nr_extra], &chunk->map[i],\r\nsizeof(chunk->map[0]) * (chunk->map_used - i));\r\nchunk->map_used += nr_extra;\r\nif (head) {\r\nchunk->map[i + 1] = chunk->map[i] - head;\r\nchunk->map[i++] = head;\r\n}\r\nif (tail) {\r\nchunk->map[i++] -= tail;\r\nchunk->map[i] = tail;\r\n}\r\n}\r\nstatic int pcpu_alloc_area(struct pcpu_chunk *chunk, int size, int align)\r\n{\r\nint oslot = pcpu_chunk_slot(chunk);\r\nint max_contig = 0;\r\nint i, off;\r\nfor (i = 0, off = 0; i < chunk->map_used; off += abs(chunk->map[i++])) {\r\nbool is_last = i + 1 == chunk->map_used;\r\nint head, tail;\r\nhead = ALIGN(off, align) - off;\r\nBUG_ON(i == 0 && head != 0);\r\nif (chunk->map[i] < 0)\r\ncontinue;\r\nif (chunk->map[i] < head + size) {\r\nmax_contig = max(chunk->map[i], max_contig);\r\ncontinue;\r\n}\r\nif (head && (head < sizeof(int) || chunk->map[i - 1] > 0)) {\r\nif (chunk->map[i - 1] > 0)\r\nchunk->map[i - 1] += head;\r\nelse {\r\nchunk->map[i - 1] -= head;\r\nchunk->free_size -= head;\r\n}\r\nchunk->map[i] -= head;\r\noff += head;\r\nhead = 0;\r\n}\r\ntail = chunk->map[i] - head - size;\r\nif (tail < sizeof(int))\r\ntail = 0;\r\nif (head || tail) {\r\npcpu_split_block(chunk, i, head, tail);\r\nif (head) {\r\ni++;\r\noff += head;\r\nmax_contig = max(chunk->map[i - 1], max_contig);\r\n}\r\nif (tail)\r\nmax_contig = max(chunk->map[i + 1], max_contig);\r\n}\r\nif (is_last)\r\nchunk->contig_hint = max_contig;\r\nelse\r\nchunk->contig_hint = max(chunk->contig_hint,\r\nmax_contig);\r\nchunk->free_size -= chunk->map[i];\r\nchunk->map[i] = -chunk->map[i];\r\npcpu_chunk_relocate(chunk, oslot);\r\nreturn off;\r\n}\r\nchunk->contig_hint = max_contig;\r\npcpu_chunk_relocate(chunk, oslot);\r\nreturn -1;\r\n}\r\nstatic void pcpu_free_area(struct pcpu_chunk *chunk, int freeme)\r\n{\r\nint oslot = pcpu_chunk_slot(chunk);\r\nint i, off;\r\nfor (i = 0, off = 0; i < chunk->map_used; off += abs(chunk->map[i++]))\r\nif (off == freeme)\r\nbreak;\r\nBUG_ON(off != freeme);\r\nBUG_ON(chunk->map[i] > 0);\r\nchunk->map[i] = -chunk->map[i];\r\nchunk->free_size += chunk->map[i];\r\nif (i > 0 && chunk->map[i - 1] >= 0) {\r\nchunk->map[i - 1] += chunk->map[i];\r\nchunk->map_used--;\r\nmemmove(&chunk->map[i], &chunk->map[i + 1],\r\n(chunk->map_used - i) * sizeof(chunk->map[0]));\r\ni--;\r\n}\r\nif (i + 1 < chunk->map_used && chunk->map[i + 1] >= 0) {\r\nchunk->map[i] += chunk->map[i + 1];\r\nchunk->map_used--;\r\nmemmove(&chunk->map[i + 1], &chunk->map[i + 2],\r\n(chunk->map_used - (i + 1)) * sizeof(chunk->map[0]));\r\n}\r\nchunk->contig_hint = max(chunk->map[i], chunk->contig_hint);\r\npcpu_chunk_relocate(chunk, oslot);\r\n}\r\nstatic struct pcpu_chunk *pcpu_alloc_chunk(void)\r\n{\r\nstruct pcpu_chunk *chunk;\r\nchunk = pcpu_mem_zalloc(pcpu_chunk_struct_size);\r\nif (!chunk)\r\nreturn NULL;\r\nchunk->map = pcpu_mem_zalloc(PCPU_DFL_MAP_ALLOC *\r\nsizeof(chunk->map[0]));\r\nif (!chunk->map) {\r\nkfree(chunk);\r\nreturn NULL;\r\n}\r\nchunk->map_alloc = PCPU_DFL_MAP_ALLOC;\r\nchunk->map[chunk->map_used++] = pcpu_unit_size;\r\nINIT_LIST_HEAD(&chunk->list);\r\nchunk->free_size = pcpu_unit_size;\r\nchunk->contig_hint = pcpu_unit_size;\r\nreturn chunk;\r\n}\r\nstatic void pcpu_free_chunk(struct pcpu_chunk *chunk)\r\n{\r\nif (!chunk)\r\nreturn;\r\npcpu_mem_free(chunk->map, chunk->map_alloc * sizeof(chunk->map[0]));\r\nkfree(chunk);\r\n}\r\nstatic struct pcpu_chunk *pcpu_chunk_addr_search(void *addr)\r\n{\r\nif (pcpu_addr_in_first_chunk(addr)) {\r\nif (pcpu_addr_in_reserved_chunk(addr))\r\nreturn pcpu_reserved_chunk;\r\nreturn pcpu_first_chunk;\r\n}\r\naddr += pcpu_unit_offsets[raw_smp_processor_id()];\r\nreturn pcpu_get_page_chunk(pcpu_addr_to_page(addr));\r\n}\r\nstatic void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved)\r\n{\r\nstatic int warn_limit = 10;\r\nstruct pcpu_chunk *chunk;\r\nconst char *err;\r\nint slot, off, new_alloc;\r\nunsigned long flags;\r\nvoid __percpu *ptr;\r\nif (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE)) {\r\nWARN(true, "illegal size (%zu) or align (%zu) for "\r\n"percpu allocation\n", size, align);\r\nreturn NULL;\r\n}\r\nmutex_lock(&pcpu_alloc_mutex);\r\nspin_lock_irqsave(&pcpu_lock, flags);\r\nif (reserved && pcpu_reserved_chunk) {\r\nchunk = pcpu_reserved_chunk;\r\nif (size > chunk->contig_hint) {\r\nerr = "alloc from reserved chunk failed";\r\ngoto fail_unlock;\r\n}\r\nwhile ((new_alloc = pcpu_need_to_extend(chunk))) {\r\nspin_unlock_irqrestore(&pcpu_lock, flags);\r\nif (pcpu_extend_area_map(chunk, new_alloc) < 0) {\r\nerr = "failed to extend area map of reserved chunk";\r\ngoto fail_unlock_mutex;\r\n}\r\nspin_lock_irqsave(&pcpu_lock, flags);\r\n}\r\noff = pcpu_alloc_area(chunk, size, align);\r\nif (off >= 0)\r\ngoto area_found;\r\nerr = "alloc from reserved chunk failed";\r\ngoto fail_unlock;\r\n}\r\nrestart:\r\nfor (slot = pcpu_size_to_slot(size); slot < pcpu_nr_slots; slot++) {\r\nlist_for_each_entry(chunk, &pcpu_slot[slot], list) {\r\nif (size > chunk->contig_hint)\r\ncontinue;\r\nnew_alloc = pcpu_need_to_extend(chunk);\r\nif (new_alloc) {\r\nspin_unlock_irqrestore(&pcpu_lock, flags);\r\nif (pcpu_extend_area_map(chunk,\r\nnew_alloc) < 0) {\r\nerr = "failed to extend area map";\r\ngoto fail_unlock_mutex;\r\n}\r\nspin_lock_irqsave(&pcpu_lock, flags);\r\ngoto restart;\r\n}\r\noff = pcpu_alloc_area(chunk, size, align);\r\nif (off >= 0)\r\ngoto area_found;\r\n}\r\n}\r\nspin_unlock_irqrestore(&pcpu_lock, flags);\r\nchunk = pcpu_create_chunk();\r\nif (!chunk) {\r\nerr = "failed to allocate new chunk";\r\ngoto fail_unlock_mutex;\r\n}\r\nspin_lock_irqsave(&pcpu_lock, flags);\r\npcpu_chunk_relocate(chunk, -1);\r\ngoto restart;\r\narea_found:\r\nspin_unlock_irqrestore(&pcpu_lock, flags);\r\nif (pcpu_populate_chunk(chunk, off, size)) {\r\nspin_lock_irqsave(&pcpu_lock, flags);\r\npcpu_free_area(chunk, off);\r\nerr = "failed to populate";\r\ngoto fail_unlock;\r\n}\r\nmutex_unlock(&pcpu_alloc_mutex);\r\nptr = __addr_to_pcpu_ptr(chunk->base_addr + off);\r\nkmemleak_alloc_percpu(ptr, size);\r\nreturn ptr;\r\nfail_unlock:\r\nspin_unlock_irqrestore(&pcpu_lock, flags);\r\nfail_unlock_mutex:\r\nmutex_unlock(&pcpu_alloc_mutex);\r\nif (warn_limit) {\r\npr_warning("PERCPU: allocation failed, size=%zu align=%zu, "\r\n"%s\n", size, align, err);\r\ndump_stack();\r\nif (!--warn_limit)\r\npr_info("PERCPU: limit reached, disable warning\n");\r\n}\r\nreturn NULL;\r\n}\r\nvoid __percpu *__alloc_percpu(size_t size, size_t align)\r\n{\r\nreturn pcpu_alloc(size, align, false);\r\n}\r\nvoid __percpu *__alloc_reserved_percpu(size_t size, size_t align)\r\n{\r\nreturn pcpu_alloc(size, align, true);\r\n}\r\nstatic void pcpu_reclaim(struct work_struct *work)\r\n{\r\nLIST_HEAD(todo);\r\nstruct list_head *head = &pcpu_slot[pcpu_nr_slots - 1];\r\nstruct pcpu_chunk *chunk, *next;\r\nmutex_lock(&pcpu_alloc_mutex);\r\nspin_lock_irq(&pcpu_lock);\r\nlist_for_each_entry_safe(chunk, next, head, list) {\r\nWARN_ON(chunk->immutable);\r\nif (chunk == list_first_entry(head, struct pcpu_chunk, list))\r\ncontinue;\r\nlist_move(&chunk->list, &todo);\r\n}\r\nspin_unlock_irq(&pcpu_lock);\r\nlist_for_each_entry_safe(chunk, next, &todo, list) {\r\npcpu_depopulate_chunk(chunk, 0, pcpu_unit_size);\r\npcpu_destroy_chunk(chunk);\r\n}\r\nmutex_unlock(&pcpu_alloc_mutex);\r\n}\r\nvoid free_percpu(void __percpu *ptr)\r\n{\r\nvoid *addr;\r\nstruct pcpu_chunk *chunk;\r\nunsigned long flags;\r\nint off;\r\nif (!ptr)\r\nreturn;\r\nkmemleak_free_percpu(ptr);\r\naddr = __pcpu_ptr_to_addr(ptr);\r\nspin_lock_irqsave(&pcpu_lock, flags);\r\nchunk = pcpu_chunk_addr_search(addr);\r\noff = addr - chunk->base_addr;\r\npcpu_free_area(chunk, off);\r\nif (chunk->free_size == pcpu_unit_size) {\r\nstruct pcpu_chunk *pos;\r\nlist_for_each_entry(pos, &pcpu_slot[pcpu_nr_slots - 1], list)\r\nif (pos != chunk) {\r\nschedule_work(&pcpu_reclaim_work);\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&pcpu_lock, flags);\r\n}\r\nbool is_kernel_percpu_address(unsigned long addr)\r\n{\r\n#ifdef CONFIG_SMP\r\nconst size_t static_size = __per_cpu_end - __per_cpu_start;\r\nvoid __percpu *base = __addr_to_pcpu_ptr(pcpu_base_addr);\r\nunsigned int cpu;\r\nfor_each_possible_cpu(cpu) {\r\nvoid *start = per_cpu_ptr(base, cpu);\r\nif ((void *)addr >= start && (void *)addr < start + static_size)\r\nreturn true;\r\n}\r\n#endif\r\nreturn false;\r\n}\r\nphys_addr_t per_cpu_ptr_to_phys(void *addr)\r\n{\r\nvoid __percpu *base = __addr_to_pcpu_ptr(pcpu_base_addr);\r\nbool in_first_chunk = false;\r\nunsigned long first_low, first_high;\r\nunsigned int cpu;\r\nfirst_low = pcpu_chunk_addr(pcpu_first_chunk, pcpu_low_unit_cpu, 0);\r\nfirst_high = pcpu_chunk_addr(pcpu_first_chunk, pcpu_high_unit_cpu,\r\npcpu_unit_pages);\r\nif ((unsigned long)addr >= first_low &&\r\n(unsigned long)addr < first_high) {\r\nfor_each_possible_cpu(cpu) {\r\nvoid *start = per_cpu_ptr(base, cpu);\r\nif (addr >= start && addr < start + pcpu_unit_size) {\r\nin_first_chunk = true;\r\nbreak;\r\n}\r\n}\r\n}\r\nif (in_first_chunk) {\r\nif (!is_vmalloc_addr(addr))\r\nreturn __pa(addr);\r\nelse\r\nreturn page_to_phys(vmalloc_to_page(addr)) +\r\noffset_in_page(addr);\r\n} else\r\nreturn page_to_phys(pcpu_addr_to_page(addr)) +\r\noffset_in_page(addr);\r\n}\r\nstruct pcpu_alloc_info * __init pcpu_alloc_alloc_info(int nr_groups,\r\nint nr_units)\r\n{\r\nstruct pcpu_alloc_info *ai;\r\nsize_t base_size, ai_size;\r\nvoid *ptr;\r\nint unit;\r\nbase_size = ALIGN(sizeof(*ai) + nr_groups * sizeof(ai->groups[0]),\r\n__alignof__(ai->groups[0].cpu_map[0]));\r\nai_size = base_size + nr_units * sizeof(ai->groups[0].cpu_map[0]);\r\nptr = alloc_bootmem_nopanic(PFN_ALIGN(ai_size));\r\nif (!ptr)\r\nreturn NULL;\r\nai = ptr;\r\nptr += base_size;\r\nai->groups[0].cpu_map = ptr;\r\nfor (unit = 0; unit < nr_units; unit++)\r\nai->groups[0].cpu_map[unit] = NR_CPUS;\r\nai->nr_groups = nr_groups;\r\nai->__ai_size = PFN_ALIGN(ai_size);\r\nreturn ai;\r\n}\r\nvoid __init pcpu_free_alloc_info(struct pcpu_alloc_info *ai)\r\n{\r\nfree_bootmem(__pa(ai), ai->__ai_size);\r\n}\r\nstatic void pcpu_dump_alloc_info(const char *lvl,\r\nconst struct pcpu_alloc_info *ai)\r\n{\r\nint group_width = 1, cpu_width = 1, width;\r\nchar empty_str[] = "--------";\r\nint alloc = 0, alloc_end = 0;\r\nint group, v;\r\nint upa, apl;\r\nv = ai->nr_groups;\r\nwhile (v /= 10)\r\ngroup_width++;\r\nv = num_possible_cpus();\r\nwhile (v /= 10)\r\ncpu_width++;\r\nempty_str[min_t(int, cpu_width, sizeof(empty_str) - 1)] = '\0';\r\nupa = ai->alloc_size / ai->unit_size;\r\nwidth = upa * (cpu_width + 1) + group_width + 3;\r\napl = rounddown_pow_of_two(max(60 / width, 1));\r\nprintk("%spcpu-alloc: s%zu r%zu d%zu u%zu alloc=%zu*%zu",\r\nlvl, ai->static_size, ai->reserved_size, ai->dyn_size,\r\nai->unit_size, ai->alloc_size / ai->atom_size, ai->atom_size);\r\nfor (group = 0; group < ai->nr_groups; group++) {\r\nconst struct pcpu_group_info *gi = &ai->groups[group];\r\nint unit = 0, unit_end = 0;\r\nBUG_ON(gi->nr_units % upa);\r\nfor (alloc_end += gi->nr_units / upa;\r\nalloc < alloc_end; alloc++) {\r\nif (!(alloc % apl)) {\r\nprintk(KERN_CONT "\n");\r\nprintk("%spcpu-alloc: ", lvl);\r\n}\r\nprintk(KERN_CONT "[%0*d] ", group_width, group);\r\nfor (unit_end += upa; unit < unit_end; unit++)\r\nif (gi->cpu_map[unit] != NR_CPUS)\r\nprintk(KERN_CONT "%0*d ", cpu_width,\r\ngi->cpu_map[unit]);\r\nelse\r\nprintk(KERN_CONT "%s ", empty_str);\r\n}\r\n}\r\nprintk(KERN_CONT "\n");\r\n}\r\nint __init pcpu_setup_first_chunk(const struct pcpu_alloc_info *ai,\r\nvoid *base_addr)\r\n{\r\nstatic char cpus_buf[4096] __initdata;\r\nstatic int smap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;\r\nstatic int dmap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;\r\nsize_t dyn_size = ai->dyn_size;\r\nsize_t size_sum = ai->static_size + ai->reserved_size + dyn_size;\r\nstruct pcpu_chunk *schunk, *dchunk = NULL;\r\nunsigned long *group_offsets;\r\nsize_t *group_sizes;\r\nunsigned long *unit_off;\r\nunsigned int cpu;\r\nint *unit_map;\r\nint group, unit, i;\r\ncpumask_scnprintf(cpus_buf, sizeof(cpus_buf), cpu_possible_mask);\r\n#define PCPU_SETUP_BUG_ON(cond) do { \\r\nif (unlikely(cond)) { \\r\npr_emerg("PERCPU: failed to initialize, %s", #cond); \\r\npr_emerg("PERCPU: cpu_possible_mask=%s\n", cpus_buf); \\r\npcpu_dump_alloc_info(KERN_EMERG, ai); \\r\nBUG(); \\r\n} \\r\n} while (0)\r\nPCPU_SETUP_BUG_ON(ai->nr_groups <= 0);\r\n#ifdef CONFIG_SMP\r\nPCPU_SETUP_BUG_ON(!ai->static_size);\r\nPCPU_SETUP_BUG_ON((unsigned long)__per_cpu_start & ~PAGE_MASK);\r\n#endif\r\nPCPU_SETUP_BUG_ON(!base_addr);\r\nPCPU_SETUP_BUG_ON((unsigned long)base_addr & ~PAGE_MASK);\r\nPCPU_SETUP_BUG_ON(ai->unit_size < size_sum);\r\nPCPU_SETUP_BUG_ON(ai->unit_size & ~PAGE_MASK);\r\nPCPU_SETUP_BUG_ON(ai->unit_size < PCPU_MIN_UNIT_SIZE);\r\nPCPU_SETUP_BUG_ON(ai->dyn_size < PERCPU_DYNAMIC_EARLY_SIZE);\r\nPCPU_SETUP_BUG_ON(pcpu_verify_alloc_info(ai) < 0);\r\ngroup_offsets = alloc_bootmem(ai->nr_groups * sizeof(group_offsets[0]));\r\ngroup_sizes = alloc_bootmem(ai->nr_groups * sizeof(group_sizes[0]));\r\nunit_map = alloc_bootmem(nr_cpu_ids * sizeof(unit_map[0]));\r\nunit_off = alloc_bootmem(nr_cpu_ids * sizeof(unit_off[0]));\r\nfor (cpu = 0; cpu < nr_cpu_ids; cpu++)\r\nunit_map[cpu] = UINT_MAX;\r\npcpu_low_unit_cpu = NR_CPUS;\r\npcpu_high_unit_cpu = NR_CPUS;\r\nfor (group = 0, unit = 0; group < ai->nr_groups; group++, unit += i) {\r\nconst struct pcpu_group_info *gi = &ai->groups[group];\r\ngroup_offsets[group] = gi->base_offset;\r\ngroup_sizes[group] = gi->nr_units * ai->unit_size;\r\nfor (i = 0; i < gi->nr_units; i++) {\r\ncpu = gi->cpu_map[i];\r\nif (cpu == NR_CPUS)\r\ncontinue;\r\nPCPU_SETUP_BUG_ON(cpu > nr_cpu_ids);\r\nPCPU_SETUP_BUG_ON(!cpu_possible(cpu));\r\nPCPU_SETUP_BUG_ON(unit_map[cpu] != UINT_MAX);\r\nunit_map[cpu] = unit + i;\r\nunit_off[cpu] = gi->base_offset + i * ai->unit_size;\r\nif (pcpu_low_unit_cpu == NR_CPUS ||\r\nunit_off[cpu] < unit_off[pcpu_low_unit_cpu])\r\npcpu_low_unit_cpu = cpu;\r\nif (pcpu_high_unit_cpu == NR_CPUS ||\r\nunit_off[cpu] > unit_off[pcpu_high_unit_cpu])\r\npcpu_high_unit_cpu = cpu;\r\n}\r\n}\r\npcpu_nr_units = unit;\r\nfor_each_possible_cpu(cpu)\r\nPCPU_SETUP_BUG_ON(unit_map[cpu] == UINT_MAX);\r\n#undef PCPU_SETUP_BUG_ON\r\npcpu_dump_alloc_info(KERN_DEBUG, ai);\r\npcpu_nr_groups = ai->nr_groups;\r\npcpu_group_offsets = group_offsets;\r\npcpu_group_sizes = group_sizes;\r\npcpu_unit_map = unit_map;\r\npcpu_unit_offsets = unit_off;\r\npcpu_unit_pages = ai->unit_size >> PAGE_SHIFT;\r\npcpu_unit_size = pcpu_unit_pages << PAGE_SHIFT;\r\npcpu_atom_size = ai->atom_size;\r\npcpu_chunk_struct_size = sizeof(struct pcpu_chunk) +\r\nBITS_TO_LONGS(pcpu_unit_pages) * sizeof(unsigned long);\r\npcpu_nr_slots = __pcpu_size_to_slot(pcpu_unit_size) + 2;\r\npcpu_slot = alloc_bootmem(pcpu_nr_slots * sizeof(pcpu_slot[0]));\r\nfor (i = 0; i < pcpu_nr_slots; i++)\r\nINIT_LIST_HEAD(&pcpu_slot[i]);\r\nschunk = alloc_bootmem(pcpu_chunk_struct_size);\r\nINIT_LIST_HEAD(&schunk->list);\r\nschunk->base_addr = base_addr;\r\nschunk->map = smap;\r\nschunk->map_alloc = ARRAY_SIZE(smap);\r\nschunk->immutable = true;\r\nbitmap_fill(schunk->populated, pcpu_unit_pages);\r\nif (ai->reserved_size) {\r\nschunk->free_size = ai->reserved_size;\r\npcpu_reserved_chunk = schunk;\r\npcpu_reserved_chunk_limit = ai->static_size + ai->reserved_size;\r\n} else {\r\nschunk->free_size = dyn_size;\r\ndyn_size = 0;\r\n}\r\nschunk->contig_hint = schunk->free_size;\r\nschunk->map[schunk->map_used++] = -ai->static_size;\r\nif (schunk->free_size)\r\nschunk->map[schunk->map_used++] = schunk->free_size;\r\nif (dyn_size) {\r\ndchunk = alloc_bootmem(pcpu_chunk_struct_size);\r\nINIT_LIST_HEAD(&dchunk->list);\r\ndchunk->base_addr = base_addr;\r\ndchunk->map = dmap;\r\ndchunk->map_alloc = ARRAY_SIZE(dmap);\r\ndchunk->immutable = true;\r\nbitmap_fill(dchunk->populated, pcpu_unit_pages);\r\ndchunk->contig_hint = dchunk->free_size = dyn_size;\r\ndchunk->map[dchunk->map_used++] = -pcpu_reserved_chunk_limit;\r\ndchunk->map[dchunk->map_used++] = dchunk->free_size;\r\n}\r\npcpu_first_chunk = dchunk ?: schunk;\r\npcpu_chunk_relocate(pcpu_first_chunk, -1);\r\npcpu_base_addr = base_addr;\r\nreturn 0;\r\n}\r\nstatic int __init percpu_alloc_setup(char *str)\r\n{\r\nif (0)\r\n;\r\n#ifdef CONFIG_NEED_PER_CPU_EMBED_FIRST_CHUNK\r\nelse if (!strcmp(str, "embed"))\r\npcpu_chosen_fc = PCPU_FC_EMBED;\r\n#endif\r\n#ifdef CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK\r\nelse if (!strcmp(str, "page"))\r\npcpu_chosen_fc = PCPU_FC_PAGE;\r\n#endif\r\nelse\r\npr_warning("PERCPU: unknown allocator %s specified\n", str);\r\nreturn 0;\r\n}\r\nstatic struct pcpu_alloc_info * __init pcpu_build_alloc_info(\r\nsize_t reserved_size, size_t dyn_size,\r\nsize_t atom_size,\r\npcpu_fc_cpu_distance_fn_t cpu_distance_fn)\r\n{\r\nstatic int group_map[NR_CPUS] __initdata;\r\nstatic int group_cnt[NR_CPUS] __initdata;\r\nconst size_t static_size = __per_cpu_end - __per_cpu_start;\r\nint nr_groups = 1, nr_units = 0;\r\nsize_t size_sum, min_unit_size, alloc_size;\r\nint upa, max_upa, uninitialized_var(best_upa);\r\nint last_allocs, group, unit;\r\nunsigned int cpu, tcpu;\r\nstruct pcpu_alloc_info *ai;\r\nunsigned int *cpu_map;\r\nmemset(group_map, 0, sizeof(group_map));\r\nmemset(group_cnt, 0, sizeof(group_cnt));\r\nsize_sum = PFN_ALIGN(static_size + reserved_size +\r\nmax_t(size_t, dyn_size, PERCPU_DYNAMIC_EARLY_SIZE));\r\ndyn_size = size_sum - static_size - reserved_size;\r\nmin_unit_size = max_t(size_t, size_sum, PCPU_MIN_UNIT_SIZE);\r\nalloc_size = roundup(min_unit_size, atom_size);\r\nupa = alloc_size / min_unit_size;\r\nwhile (alloc_size % upa || ((alloc_size / upa) & ~PAGE_MASK))\r\nupa--;\r\nmax_upa = upa;\r\nfor_each_possible_cpu(cpu) {\r\ngroup = 0;\r\nnext_group:\r\nfor_each_possible_cpu(tcpu) {\r\nif (cpu == tcpu)\r\nbreak;\r\nif (group_map[tcpu] == group && cpu_distance_fn &&\r\n(cpu_distance_fn(cpu, tcpu) > LOCAL_DISTANCE ||\r\ncpu_distance_fn(tcpu, cpu) > LOCAL_DISTANCE)) {\r\ngroup++;\r\nnr_groups = max(nr_groups, group + 1);\r\ngoto next_group;\r\n}\r\n}\r\ngroup_map[cpu] = group;\r\ngroup_cnt[group]++;\r\n}\r\nlast_allocs = INT_MAX;\r\nfor (upa = max_upa; upa; upa--) {\r\nint allocs = 0, wasted = 0;\r\nif (alloc_size % upa || ((alloc_size / upa) & ~PAGE_MASK))\r\ncontinue;\r\nfor (group = 0; group < nr_groups; group++) {\r\nint this_allocs = DIV_ROUND_UP(group_cnt[group], upa);\r\nallocs += this_allocs;\r\nwasted += this_allocs * upa - group_cnt[group];\r\n}\r\nif (wasted > num_possible_cpus() / 3)\r\ncontinue;\r\nif (allocs > last_allocs)\r\nbreak;\r\nlast_allocs = allocs;\r\nbest_upa = upa;\r\n}\r\nupa = best_upa;\r\nfor (group = 0; group < nr_groups; group++)\r\nnr_units += roundup(group_cnt[group], upa);\r\nai = pcpu_alloc_alloc_info(nr_groups, nr_units);\r\nif (!ai)\r\nreturn ERR_PTR(-ENOMEM);\r\ncpu_map = ai->groups[0].cpu_map;\r\nfor (group = 0; group < nr_groups; group++) {\r\nai->groups[group].cpu_map = cpu_map;\r\ncpu_map += roundup(group_cnt[group], upa);\r\n}\r\nai->static_size = static_size;\r\nai->reserved_size = reserved_size;\r\nai->dyn_size = dyn_size;\r\nai->unit_size = alloc_size / upa;\r\nai->atom_size = atom_size;\r\nai->alloc_size = alloc_size;\r\nfor (group = 0, unit = 0; group_cnt[group]; group++) {\r\nstruct pcpu_group_info *gi = &ai->groups[group];\r\ngi->base_offset = unit * ai->unit_size;\r\nfor_each_possible_cpu(cpu)\r\nif (group_map[cpu] == group)\r\ngi->cpu_map[gi->nr_units++] = cpu;\r\ngi->nr_units = roundup(gi->nr_units, upa);\r\nunit += gi->nr_units;\r\n}\r\nBUG_ON(unit != nr_units);\r\nreturn ai;\r\n}\r\nint __init pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,\r\nsize_t atom_size,\r\npcpu_fc_cpu_distance_fn_t cpu_distance_fn,\r\npcpu_fc_alloc_fn_t alloc_fn,\r\npcpu_fc_free_fn_t free_fn)\r\n{\r\nvoid *base = (void *)ULONG_MAX;\r\nvoid **areas = NULL;\r\nstruct pcpu_alloc_info *ai;\r\nsize_t size_sum, areas_size, max_distance;\r\nint group, i, rc;\r\nai = pcpu_build_alloc_info(reserved_size, dyn_size, atom_size,\r\ncpu_distance_fn);\r\nif (IS_ERR(ai))\r\nreturn PTR_ERR(ai);\r\nsize_sum = ai->static_size + ai->reserved_size + ai->dyn_size;\r\nareas_size = PFN_ALIGN(ai->nr_groups * sizeof(void *));\r\nareas = alloc_bootmem_nopanic(areas_size);\r\nif (!areas) {\r\nrc = -ENOMEM;\r\ngoto out_free;\r\n}\r\nfor (group = 0; group < ai->nr_groups; group++) {\r\nstruct pcpu_group_info *gi = &ai->groups[group];\r\nunsigned int cpu = NR_CPUS;\r\nvoid *ptr;\r\nfor (i = 0; i < gi->nr_units && cpu == NR_CPUS; i++)\r\ncpu = gi->cpu_map[i];\r\nBUG_ON(cpu == NR_CPUS);\r\nptr = alloc_fn(cpu, gi->nr_units * ai->unit_size, atom_size);\r\nif (!ptr) {\r\nrc = -ENOMEM;\r\ngoto out_free_areas;\r\n}\r\nkmemleak_free(ptr);\r\nareas[group] = ptr;\r\nbase = min(ptr, base);\r\n}\r\nfor (group = 0; group < ai->nr_groups; group++) {\r\nstruct pcpu_group_info *gi = &ai->groups[group];\r\nvoid *ptr = areas[group];\r\nfor (i = 0; i < gi->nr_units; i++, ptr += ai->unit_size) {\r\nif (gi->cpu_map[i] == NR_CPUS) {\r\nfree_fn(ptr, ai->unit_size);\r\ncontinue;\r\n}\r\nmemcpy(ptr, __per_cpu_load, ai->static_size);\r\nfree_fn(ptr + size_sum, ai->unit_size - size_sum);\r\n}\r\n}\r\nmax_distance = 0;\r\nfor (group = 0; group < ai->nr_groups; group++) {\r\nai->groups[group].base_offset = areas[group] - base;\r\nmax_distance = max_t(size_t, max_distance,\r\nai->groups[group].base_offset);\r\n}\r\nmax_distance += ai->unit_size;\r\nif (max_distance > (VMALLOC_END - VMALLOC_START) * 3 / 4) {\r\npr_warning("PERCPU: max_distance=0x%zx too large for vmalloc "\r\n"space 0x%lx\n", max_distance,\r\n(unsigned long)(VMALLOC_END - VMALLOC_START));\r\n#ifdef CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK\r\nrc = -EINVAL;\r\ngoto out_free;\r\n#endif\r\n}\r\npr_info("PERCPU: Embedded %zu pages/cpu @%p s%zu r%zu d%zu u%zu\n",\r\nPFN_DOWN(size_sum), base, ai->static_size, ai->reserved_size,\r\nai->dyn_size, ai->unit_size);\r\nrc = pcpu_setup_first_chunk(ai, base);\r\ngoto out_free;\r\nout_free_areas:\r\nfor (group = 0; group < ai->nr_groups; group++)\r\nfree_fn(areas[group],\r\nai->groups[group].nr_units * ai->unit_size);\r\nout_free:\r\npcpu_free_alloc_info(ai);\r\nif (areas)\r\nfree_bootmem(__pa(areas), areas_size);\r\nreturn rc;\r\n}\r\nint __init pcpu_page_first_chunk(size_t reserved_size,\r\npcpu_fc_alloc_fn_t alloc_fn,\r\npcpu_fc_free_fn_t free_fn,\r\npcpu_fc_populate_pte_fn_t populate_pte_fn)\r\n{\r\nstatic struct vm_struct vm;\r\nstruct pcpu_alloc_info *ai;\r\nchar psize_str[16];\r\nint unit_pages;\r\nsize_t pages_size;\r\nstruct page **pages;\r\nint unit, i, j, rc;\r\nsnprintf(psize_str, sizeof(psize_str), "%luK", PAGE_SIZE >> 10);\r\nai = pcpu_build_alloc_info(reserved_size, 0, PAGE_SIZE, NULL);\r\nif (IS_ERR(ai))\r\nreturn PTR_ERR(ai);\r\nBUG_ON(ai->nr_groups != 1);\r\nBUG_ON(ai->groups[0].nr_units != num_possible_cpus());\r\nunit_pages = ai->unit_size >> PAGE_SHIFT;\r\npages_size = PFN_ALIGN(unit_pages * num_possible_cpus() *\r\nsizeof(pages[0]));\r\npages = alloc_bootmem(pages_size);\r\nj = 0;\r\nfor (unit = 0; unit < num_possible_cpus(); unit++)\r\nfor (i = 0; i < unit_pages; i++) {\r\nunsigned int cpu = ai->groups[0].cpu_map[unit];\r\nvoid *ptr;\r\nptr = alloc_fn(cpu, PAGE_SIZE, PAGE_SIZE);\r\nif (!ptr) {\r\npr_warning("PERCPU: failed to allocate %s page "\r\n"for cpu%u\n", psize_str, cpu);\r\ngoto enomem;\r\n}\r\nkmemleak_free(ptr);\r\npages[j++] = virt_to_page(ptr);\r\n}\r\nvm.flags = VM_ALLOC;\r\nvm.size = num_possible_cpus() * ai->unit_size;\r\nvm_area_register_early(&vm, PAGE_SIZE);\r\nfor (unit = 0; unit < num_possible_cpus(); unit++) {\r\nunsigned long unit_addr =\r\n(unsigned long)vm.addr + unit * ai->unit_size;\r\nfor (i = 0; i < unit_pages; i++)\r\npopulate_pte_fn(unit_addr + (i << PAGE_SHIFT));\r\nrc = __pcpu_map_pages(unit_addr, &pages[unit * unit_pages],\r\nunit_pages);\r\nif (rc < 0)\r\npanic("failed to map percpu area, err=%d\n", rc);\r\nmemcpy((void *)unit_addr, __per_cpu_load, ai->static_size);\r\n}\r\npr_info("PERCPU: %d %s pages/cpu @%p s%zu r%zu d%zu\n",\r\nunit_pages, psize_str, vm.addr, ai->static_size,\r\nai->reserved_size, ai->dyn_size);\r\nrc = pcpu_setup_first_chunk(ai, vm.addr);\r\ngoto out_free_ar;\r\nenomem:\r\nwhile (--j >= 0)\r\nfree_fn(page_address(pages[j]), PAGE_SIZE);\r\nrc = -ENOMEM;\r\nout_free_ar:\r\nfree_bootmem(__pa(pages), pages_size);\r\npcpu_free_alloc_info(ai);\r\nreturn rc;\r\n}\r\nstatic void * __init pcpu_dfl_fc_alloc(unsigned int cpu, size_t size,\r\nsize_t align)\r\n{\r\nreturn __alloc_bootmem_nopanic(size, align, __pa(MAX_DMA_ADDRESS));\r\n}\r\nstatic void __init pcpu_dfl_fc_free(void *ptr, size_t size)\r\n{\r\nfree_bootmem(__pa(ptr), size);\r\n}\r\nvoid __init setup_per_cpu_areas(void)\r\n{\r\nunsigned long delta;\r\nunsigned int cpu;\r\nint rc;\r\nrc = pcpu_embed_first_chunk(PERCPU_MODULE_RESERVE,\r\nPERCPU_DYNAMIC_RESERVE, PAGE_SIZE, NULL,\r\npcpu_dfl_fc_alloc, pcpu_dfl_fc_free);\r\nif (rc < 0)\r\npanic("Failed to initialize percpu areas.");\r\ndelta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;\r\nfor_each_possible_cpu(cpu)\r\n__per_cpu_offset[cpu] = delta + pcpu_unit_offsets[cpu];\r\n}\r\nvoid __init setup_per_cpu_areas(void)\r\n{\r\nconst size_t unit_size =\r\nroundup_pow_of_two(max_t(size_t, PCPU_MIN_UNIT_SIZE,\r\nPERCPU_DYNAMIC_RESERVE));\r\nstruct pcpu_alloc_info *ai;\r\nvoid *fc;\r\nai = pcpu_alloc_alloc_info(1, 1);\r\nfc = __alloc_bootmem(unit_size, PAGE_SIZE, __pa(MAX_DMA_ADDRESS));\r\nif (!ai || !fc)\r\npanic("Failed to allocate memory for percpu areas.");\r\nkmemleak_free(fc);\r\nai->dyn_size = unit_size;\r\nai->unit_size = unit_size;\r\nai->atom_size = unit_size;\r\nai->alloc_size = unit_size;\r\nai->groups[0].nr_units = 1;\r\nai->groups[0].cpu_map[0] = 0;\r\nif (pcpu_setup_first_chunk(ai, fc) < 0)\r\npanic("Failed to initialize percpu areas.");\r\n}\r\nvoid __init percpu_init_late(void)\r\n{\r\nstruct pcpu_chunk *target_chunks[] =\r\n{ pcpu_first_chunk, pcpu_reserved_chunk, NULL };\r\nstruct pcpu_chunk *chunk;\r\nunsigned long flags;\r\nint i;\r\nfor (i = 0; (chunk = target_chunks[i]); i++) {\r\nint *map;\r\nconst size_t size = PERCPU_DYNAMIC_EARLY_SLOTS * sizeof(map[0]);\r\nBUILD_BUG_ON(size > PAGE_SIZE);\r\nmap = pcpu_mem_zalloc(size);\r\nBUG_ON(!map);\r\nspin_lock_irqsave(&pcpu_lock, flags);\r\nmemcpy(map, chunk->map, size);\r\nchunk->map = map;\r\nspin_unlock_irqrestore(&pcpu_lock, flags);\r\n}\r\n}
