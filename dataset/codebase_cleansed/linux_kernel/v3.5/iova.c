void\r\ninit_iova_domain(struct iova_domain *iovad, unsigned long pfn_32bit)\r\n{\r\nspin_lock_init(&iovad->iova_rbtree_lock);\r\niovad->rbroot = RB_ROOT;\r\niovad->cached32_node = NULL;\r\niovad->dma_32bit_pfn = pfn_32bit;\r\n}\r\nstatic struct rb_node *\r\n__get_cached_rbnode(struct iova_domain *iovad, unsigned long *limit_pfn)\r\n{\r\nif ((*limit_pfn != iovad->dma_32bit_pfn) ||\r\n(iovad->cached32_node == NULL))\r\nreturn rb_last(&iovad->rbroot);\r\nelse {\r\nstruct rb_node *prev_node = rb_prev(iovad->cached32_node);\r\nstruct iova *curr_iova =\r\ncontainer_of(iovad->cached32_node, struct iova, node);\r\n*limit_pfn = curr_iova->pfn_lo - 1;\r\nreturn prev_node;\r\n}\r\n}\r\nstatic void\r\n__cached_rbnode_insert_update(struct iova_domain *iovad,\r\nunsigned long limit_pfn, struct iova *new)\r\n{\r\nif (limit_pfn != iovad->dma_32bit_pfn)\r\nreturn;\r\niovad->cached32_node = &new->node;\r\n}\r\nstatic void\r\n__cached_rbnode_delete_update(struct iova_domain *iovad, struct iova *free)\r\n{\r\nstruct iova *cached_iova;\r\nstruct rb_node *curr;\r\nif (!iovad->cached32_node)\r\nreturn;\r\ncurr = iovad->cached32_node;\r\ncached_iova = container_of(curr, struct iova, node);\r\nif (free->pfn_lo >= cached_iova->pfn_lo) {\r\nstruct rb_node *node = rb_next(&free->node);\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nif (node && iova->pfn_lo < iovad->dma_32bit_pfn)\r\niovad->cached32_node = node;\r\nelse\r\niovad->cached32_node = NULL;\r\n}\r\n}\r\nstatic int\r\niova_get_pad_size(int size, unsigned int limit_pfn)\r\n{\r\nunsigned int pad_size = 0;\r\nunsigned int order = ilog2(size);\r\nif (order)\r\npad_size = (limit_pfn + 1) % (1 << order);\r\nreturn pad_size;\r\n}\r\nstatic int __alloc_and_insert_iova_range(struct iova_domain *iovad,\r\nunsigned long size, unsigned long limit_pfn,\r\nstruct iova *new, bool size_aligned)\r\n{\r\nstruct rb_node *prev, *curr = NULL;\r\nunsigned long flags;\r\nunsigned long saved_pfn;\r\nunsigned int pad_size = 0;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nsaved_pfn = limit_pfn;\r\ncurr = __get_cached_rbnode(iovad, &limit_pfn);\r\nprev = curr;\r\nwhile (curr) {\r\nstruct iova *curr_iova = container_of(curr, struct iova, node);\r\nif (limit_pfn < curr_iova->pfn_lo)\r\ngoto move_left;\r\nelse if (limit_pfn < curr_iova->pfn_hi)\r\ngoto adjust_limit_pfn;\r\nelse {\r\nif (size_aligned)\r\npad_size = iova_get_pad_size(size, limit_pfn);\r\nif ((curr_iova->pfn_hi + size + pad_size) <= limit_pfn)\r\nbreak;\r\n}\r\nadjust_limit_pfn:\r\nlimit_pfn = curr_iova->pfn_lo - 1;\r\nmove_left:\r\nprev = curr;\r\ncurr = rb_prev(curr);\r\n}\r\nif (!curr) {\r\nif (size_aligned)\r\npad_size = iova_get_pad_size(size, limit_pfn);\r\nif ((IOVA_START_PFN + size + pad_size) > limit_pfn) {\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nnew->pfn_lo = limit_pfn - (size + pad_size) + 1;\r\nnew->pfn_hi = new->pfn_lo + size - 1;\r\n{\r\nstruct rb_node **entry, *parent = NULL;\r\nif (prev)\r\nentry = &prev;\r\nelse\r\nentry = &iovad->rbroot.rb_node;\r\nwhile (*entry) {\r\nstruct iova *this = container_of(*entry,\r\nstruct iova, node);\r\nparent = *entry;\r\nif (new->pfn_lo < this->pfn_lo)\r\nentry = &((*entry)->rb_left);\r\nelse if (new->pfn_lo > this->pfn_lo)\r\nentry = &((*entry)->rb_right);\r\nelse\r\nBUG();\r\n}\r\nrb_link_node(&new->node, parent, entry);\r\nrb_insert_color(&new->node, &iovad->rbroot);\r\n}\r\n__cached_rbnode_insert_update(iovad, saved_pfn, new);\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn 0;\r\n}\r\nstatic void\r\niova_insert_rbtree(struct rb_root *root, struct iova *iova)\r\n{\r\nstruct rb_node **new = &(root->rb_node), *parent = NULL;\r\nwhile (*new) {\r\nstruct iova *this = container_of(*new, struct iova, node);\r\nparent = *new;\r\nif (iova->pfn_lo < this->pfn_lo)\r\nnew = &((*new)->rb_left);\r\nelse if (iova->pfn_lo > this->pfn_lo)\r\nnew = &((*new)->rb_right);\r\nelse\r\nBUG();\r\n}\r\nrb_link_node(&iova->node, parent, new);\r\nrb_insert_color(&iova->node, root);\r\n}\r\nstruct iova *\r\nalloc_iova(struct iova_domain *iovad, unsigned long size,\r\nunsigned long limit_pfn,\r\nbool size_aligned)\r\n{\r\nstruct iova *new_iova;\r\nint ret;\r\nnew_iova = alloc_iova_mem();\r\nif (!new_iova)\r\nreturn NULL;\r\nif (size_aligned)\r\nsize = __roundup_pow_of_two(size);\r\nret = __alloc_and_insert_iova_range(iovad, size, limit_pfn,\r\nnew_iova, size_aligned);\r\nif (ret) {\r\nfree_iova_mem(new_iova);\r\nreturn NULL;\r\n}\r\nreturn new_iova;\r\n}\r\nstruct iova *find_iova(struct iova_domain *iovad, unsigned long pfn)\r\n{\r\nunsigned long flags;\r\nstruct rb_node *node;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nnode = iovad->rbroot.rb_node;\r\nwhile (node) {\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nif ((pfn >= iova->pfn_lo) && (pfn <= iova->pfn_hi)) {\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn iova;\r\n}\r\nif (pfn < iova->pfn_lo)\r\nnode = node->rb_left;\r\nelse if (pfn > iova->pfn_lo)\r\nnode = node->rb_right;\r\n}\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn NULL;\r\n}\r\nvoid\r\n__free_iova(struct iova_domain *iovad, struct iova *iova)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\n__cached_rbnode_delete_update(iovad, iova);\r\nrb_erase(&iova->node, &iovad->rbroot);\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nfree_iova_mem(iova);\r\n}\r\nvoid\r\nfree_iova(struct iova_domain *iovad, unsigned long pfn)\r\n{\r\nstruct iova *iova = find_iova(iovad, pfn);\r\nif (iova)\r\n__free_iova(iovad, iova);\r\n}\r\nvoid put_iova_domain(struct iova_domain *iovad)\r\n{\r\nstruct rb_node *node;\r\nunsigned long flags;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nnode = rb_first(&iovad->rbroot);\r\nwhile (node) {\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nrb_erase(node, &iovad->rbroot);\r\nfree_iova_mem(iova);\r\nnode = rb_first(&iovad->rbroot);\r\n}\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\n}\r\nstatic int\r\n__is_range_overlap(struct rb_node *node,\r\nunsigned long pfn_lo, unsigned long pfn_hi)\r\n{\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nif ((pfn_lo <= iova->pfn_hi) && (pfn_hi >= iova->pfn_lo))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic struct iova *\r\n__insert_new_range(struct iova_domain *iovad,\r\nunsigned long pfn_lo, unsigned long pfn_hi)\r\n{\r\nstruct iova *iova;\r\niova = alloc_iova_mem();\r\nif (!iova)\r\nreturn iova;\r\niova->pfn_hi = pfn_hi;\r\niova->pfn_lo = pfn_lo;\r\niova_insert_rbtree(&iovad->rbroot, iova);\r\nreturn iova;\r\n}\r\nstatic void\r\n__adjust_overlap_range(struct iova *iova,\r\nunsigned long *pfn_lo, unsigned long *pfn_hi)\r\n{\r\nif (*pfn_lo < iova->pfn_lo)\r\niova->pfn_lo = *pfn_lo;\r\nif (*pfn_hi > iova->pfn_hi)\r\n*pfn_lo = iova->pfn_hi + 1;\r\n}\r\nstruct iova *\r\nreserve_iova(struct iova_domain *iovad,\r\nunsigned long pfn_lo, unsigned long pfn_hi)\r\n{\r\nstruct rb_node *node;\r\nunsigned long flags;\r\nstruct iova *iova;\r\nunsigned int overlap = 0;\r\nspin_lock_irqsave(&iovad->iova_rbtree_lock, flags);\r\nfor (node = rb_first(&iovad->rbroot); node; node = rb_next(node)) {\r\nif (__is_range_overlap(node, pfn_lo, pfn_hi)) {\r\niova = container_of(node, struct iova, node);\r\n__adjust_overlap_range(iova, &pfn_lo, &pfn_hi);\r\nif ((pfn_lo >= iova->pfn_lo) &&\r\n(pfn_hi <= iova->pfn_hi))\r\ngoto finish;\r\noverlap = 1;\r\n} else if (overlap)\r\nbreak;\r\n}\r\niova = __insert_new_range(iovad, pfn_lo, pfn_hi);\r\nfinish:\r\nspin_unlock_irqrestore(&iovad->iova_rbtree_lock, flags);\r\nreturn iova;\r\n}\r\nvoid\r\ncopy_reserved_iova(struct iova_domain *from, struct iova_domain *to)\r\n{\r\nunsigned long flags;\r\nstruct rb_node *node;\r\nspin_lock_irqsave(&from->iova_rbtree_lock, flags);\r\nfor (node = rb_first(&from->rbroot); node; node = rb_next(node)) {\r\nstruct iova *iova = container_of(node, struct iova, node);\r\nstruct iova *new_iova;\r\nnew_iova = reserve_iova(to, iova->pfn_lo, iova->pfn_hi);\r\nif (!new_iova)\r\nprintk(KERN_ERR "Reserve iova range %lx@%lx failed\n",\r\niova->pfn_lo, iova->pfn_lo);\r\n}\r\nspin_unlock_irqrestore(&from->iova_rbtree_lock, flags);\r\n}
