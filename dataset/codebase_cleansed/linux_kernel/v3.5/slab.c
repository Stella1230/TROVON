static __always_inline int index_of(const size_t size)\r\n{\r\nextern void __bad_size(void);\r\nif (__builtin_constant_p(size)) {\r\nint i = 0;\r\n#define CACHE(x) \\r\nif (size <=x) \\r\nreturn i; \\r\nelse \\r\ni++;\r\n#include <linux/kmalloc_sizes.h>\r\n#undef CACHE\r\n__bad_size();\r\n} else\r\n__bad_size();\r\nreturn 0;\r\n}\r\nstatic void kmem_list3_init(struct kmem_list3 *parent)\r\n{\r\nINIT_LIST_HEAD(&parent->slabs_full);\r\nINIT_LIST_HEAD(&parent->slabs_partial);\r\nINIT_LIST_HEAD(&parent->slabs_free);\r\nparent->shared = NULL;\r\nparent->alien = NULL;\r\nparent->colour_next = 0;\r\nspin_lock_init(&parent->list_lock);\r\nparent->free_objects = 0;\r\nparent->free_touched = 0;\r\n}\r\nstatic int obj_offset(struct kmem_cache *cachep)\r\n{\r\nreturn cachep->obj_offset;\r\n}\r\nstatic int obj_size(struct kmem_cache *cachep)\r\n{\r\nreturn cachep->obj_size;\r\n}\r\nstatic unsigned long long *dbg_redzone1(struct kmem_cache *cachep, void *objp)\r\n{\r\nBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\r\nreturn (unsigned long long*) (objp + obj_offset(cachep) -\r\nsizeof(unsigned long long));\r\n}\r\nstatic unsigned long long *dbg_redzone2(struct kmem_cache *cachep, void *objp)\r\n{\r\nBUG_ON(!(cachep->flags & SLAB_RED_ZONE));\r\nif (cachep->flags & SLAB_STORE_USER)\r\nreturn (unsigned long long *)(objp + cachep->buffer_size -\r\nsizeof(unsigned long long) -\r\nREDZONE_ALIGN);\r\nreturn (unsigned long long *) (objp + cachep->buffer_size -\r\nsizeof(unsigned long long));\r\n}\r\nstatic void **dbg_userword(struct kmem_cache *cachep, void *objp)\r\n{\r\nBUG_ON(!(cachep->flags & SLAB_STORE_USER));\r\nreturn (void **)(objp + cachep->buffer_size - BYTES_PER_WORD);\r\n}\r\nsize_t slab_buffer_size(struct kmem_cache *cachep)\r\n{\r\nreturn cachep->buffer_size;\r\n}\r\nstatic inline void page_set_cache(struct page *page, struct kmem_cache *cache)\r\n{\r\npage->lru.next = (struct list_head *)cache;\r\n}\r\nstatic inline struct kmem_cache *page_get_cache(struct page *page)\r\n{\r\npage = compound_head(page);\r\nBUG_ON(!PageSlab(page));\r\nreturn (struct kmem_cache *)page->lru.next;\r\n}\r\nstatic inline void page_set_slab(struct page *page, struct slab *slab)\r\n{\r\npage->lru.prev = (struct list_head *)slab;\r\n}\r\nstatic inline struct slab *page_get_slab(struct page *page)\r\n{\r\nBUG_ON(!PageSlab(page));\r\nreturn (struct slab *)page->lru.prev;\r\n}\r\nstatic inline struct kmem_cache *virt_to_cache(const void *obj)\r\n{\r\nstruct page *page = virt_to_head_page(obj);\r\nreturn page_get_cache(page);\r\n}\r\nstatic inline struct slab *virt_to_slab(const void *obj)\r\n{\r\nstruct page *page = virt_to_head_page(obj);\r\nreturn page_get_slab(page);\r\n}\r\nstatic inline void *index_to_obj(struct kmem_cache *cache, struct slab *slab,\r\nunsigned int idx)\r\n{\r\nreturn slab->s_mem + cache->buffer_size * idx;\r\n}\r\nstatic inline unsigned int obj_to_index(const struct kmem_cache *cache,\r\nconst struct slab *slab, void *obj)\r\n{\r\nu32 offset = (obj - slab->s_mem);\r\nreturn reciprocal_divide(offset, cache->reciprocal_buffer_size);\r\n}\r\nint slab_is_available(void)\r\n{\r\nreturn g_cpucache_up >= EARLY;\r\n}\r\nstatic void slab_set_lock_classes(struct kmem_cache *cachep,\r\nstruct lock_class_key *l3_key, struct lock_class_key *alc_key,\r\nint q)\r\n{\r\nstruct array_cache **alc;\r\nstruct kmem_list3 *l3;\r\nint r;\r\nl3 = cachep->nodelists[q];\r\nif (!l3)\r\nreturn;\r\nlockdep_set_class(&l3->list_lock, l3_key);\r\nalc = l3->alien;\r\nif (!alc || (unsigned long)alc == BAD_ALIEN_MAGIC)\r\nreturn;\r\nfor_each_node(r) {\r\nif (alc[r])\r\nlockdep_set_class(&alc[r]->lock, alc_key);\r\n}\r\n}\r\nstatic void slab_set_debugobj_lock_classes_node(struct kmem_cache *cachep, int node)\r\n{\r\nslab_set_lock_classes(cachep, &debugobj_l3_key, &debugobj_alc_key, node);\r\n}\r\nstatic void slab_set_debugobj_lock_classes(struct kmem_cache *cachep)\r\n{\r\nint node;\r\nfor_each_online_node(node)\r\nslab_set_debugobj_lock_classes_node(cachep, node);\r\n}\r\nstatic void init_node_lock_keys(int q)\r\n{\r\nstruct cache_sizes *s = malloc_sizes;\r\nif (g_cpucache_up < LATE)\r\nreturn;\r\nfor (s = malloc_sizes; s->cs_size != ULONG_MAX; s++) {\r\nstruct kmem_list3 *l3;\r\nl3 = s->cs_cachep->nodelists[q];\r\nif (!l3 || OFF_SLAB(s->cs_cachep))\r\ncontinue;\r\nslab_set_lock_classes(s->cs_cachep, &on_slab_l3_key,\r\n&on_slab_alc_key, q);\r\n}\r\n}\r\nstatic inline void init_lock_keys(void)\r\n{\r\nint node;\r\nfor_each_node(node)\r\ninit_node_lock_keys(node);\r\n}\r\nstatic void init_node_lock_keys(int q)\r\n{\r\n}\r\nstatic inline void init_lock_keys(void)\r\n{\r\n}\r\nstatic void slab_set_debugobj_lock_classes_node(struct kmem_cache *cachep, int node)\r\n{\r\n}\r\nstatic void slab_set_debugobj_lock_classes(struct kmem_cache *cachep)\r\n{\r\n}\r\nstatic inline struct array_cache *cpu_cache_get(struct kmem_cache *cachep)\r\n{\r\nreturn cachep->array[smp_processor_id()];\r\n}\r\nstatic inline struct kmem_cache *__find_general_cachep(size_t size,\r\ngfp_t gfpflags)\r\n{\r\nstruct cache_sizes *csizep = malloc_sizes;\r\n#if DEBUG\r\nBUG_ON(malloc_sizes[INDEX_AC].cs_cachep == NULL);\r\n#endif\r\nif (!size)\r\nreturn ZERO_SIZE_PTR;\r\nwhile (size > csizep->cs_size)\r\ncsizep++;\r\n#ifdef CONFIG_ZONE_DMA\r\nif (unlikely(gfpflags & GFP_DMA))\r\nreturn csizep->cs_dmacachep;\r\n#endif\r\nreturn csizep->cs_cachep;\r\n}\r\nstatic struct kmem_cache *kmem_find_general_cachep(size_t size, gfp_t gfpflags)\r\n{\r\nreturn __find_general_cachep(size, gfpflags);\r\n}\r\nstatic size_t slab_mgmt_size(size_t nr_objs, size_t align)\r\n{\r\nreturn ALIGN(sizeof(struct slab)+nr_objs*sizeof(kmem_bufctl_t), align);\r\n}\r\nstatic void cache_estimate(unsigned long gfporder, size_t buffer_size,\r\nsize_t align, int flags, size_t *left_over,\r\nunsigned int *num)\r\n{\r\nint nr_objs;\r\nsize_t mgmt_size;\r\nsize_t slab_size = PAGE_SIZE << gfporder;\r\nif (flags & CFLGS_OFF_SLAB) {\r\nmgmt_size = 0;\r\nnr_objs = slab_size / buffer_size;\r\nif (nr_objs > SLAB_LIMIT)\r\nnr_objs = SLAB_LIMIT;\r\n} else {\r\nnr_objs = (slab_size - sizeof(struct slab)) /\r\n(buffer_size + sizeof(kmem_bufctl_t));\r\nif (slab_mgmt_size(nr_objs, align) + nr_objs*buffer_size\r\n> slab_size)\r\nnr_objs--;\r\nif (nr_objs > SLAB_LIMIT)\r\nnr_objs = SLAB_LIMIT;\r\nmgmt_size = slab_mgmt_size(nr_objs, align);\r\n}\r\n*num = nr_objs;\r\n*left_over = slab_size - nr_objs*buffer_size - mgmt_size;\r\n}\r\nstatic void __slab_error(const char *function, struct kmem_cache *cachep,\r\nchar *msg)\r\n{\r\nprintk(KERN_ERR "slab error in %s(): cache `%s': %s\n",\r\nfunction, cachep->name, msg);\r\ndump_stack();\r\n}\r\nstatic int __init noaliencache_setup(char *s)\r\n{\r\nuse_alien_caches = 0;\r\nreturn 1;\r\n}\r\nstatic int __init slab_max_order_setup(char *str)\r\n{\r\nget_option(&str, &slab_max_order);\r\nslab_max_order = slab_max_order < 0 ? 0 :\r\nmin(slab_max_order, MAX_ORDER - 1);\r\nslab_max_order_set = true;\r\nreturn 1;\r\n}\r\nstatic void init_reap_node(int cpu)\r\n{\r\nint node;\r\nnode = next_node(cpu_to_mem(cpu), node_online_map);\r\nif (node == MAX_NUMNODES)\r\nnode = first_node(node_online_map);\r\nper_cpu(slab_reap_node, cpu) = node;\r\n}\r\nstatic void next_reap_node(void)\r\n{\r\nint node = __this_cpu_read(slab_reap_node);\r\nnode = next_node(node, node_online_map);\r\nif (unlikely(node >= MAX_NUMNODES))\r\nnode = first_node(node_online_map);\r\n__this_cpu_write(slab_reap_node, node);\r\n}\r\nstatic void __cpuinit start_cpu_timer(int cpu)\r\n{\r\nstruct delayed_work *reap_work = &per_cpu(slab_reap_work, cpu);\r\nif (keventd_up() && reap_work->work.func == NULL) {\r\ninit_reap_node(cpu);\r\nINIT_DELAYED_WORK_DEFERRABLE(reap_work, cache_reap);\r\nschedule_delayed_work_on(cpu, reap_work,\r\n__round_jiffies_relative(HZ, cpu));\r\n}\r\n}\r\nstatic struct array_cache *alloc_arraycache(int node, int entries,\r\nint batchcount, gfp_t gfp)\r\n{\r\nint memsize = sizeof(void *) * entries + sizeof(struct array_cache);\r\nstruct array_cache *nc = NULL;\r\nnc = kmalloc_node(memsize, gfp, node);\r\nkmemleak_no_scan(nc);\r\nif (nc) {\r\nnc->avail = 0;\r\nnc->limit = entries;\r\nnc->batchcount = batchcount;\r\nnc->touched = 0;\r\nspin_lock_init(&nc->lock);\r\n}\r\nreturn nc;\r\n}\r\nstatic int transfer_objects(struct array_cache *to,\r\nstruct array_cache *from, unsigned int max)\r\n{\r\nint nr = min3(from->avail, max, to->limit - to->avail);\r\nif (!nr)\r\nreturn 0;\r\nmemcpy(to->entry + to->avail, from->entry + from->avail -nr,\r\nsizeof(void *) *nr);\r\nfrom->avail -= nr;\r\nto->avail += nr;\r\nreturn nr;\r\n}\r\nstatic inline struct array_cache **alloc_alien_cache(int node, int limit, gfp_t gfp)\r\n{\r\nreturn (struct array_cache **)BAD_ALIEN_MAGIC;\r\n}\r\nstatic inline void free_alien_cache(struct array_cache **ac_ptr)\r\n{\r\n}\r\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void *alternate_node_alloc(struct kmem_cache *cachep,\r\ngfp_t flags)\r\n{\r\nreturn NULL;\r\n}\r\nstatic inline void *____cache_alloc_node(struct kmem_cache *cachep,\r\ngfp_t flags, int nodeid)\r\n{\r\nreturn NULL;\r\n}\r\nstatic struct array_cache **alloc_alien_cache(int node, int limit, gfp_t gfp)\r\n{\r\nstruct array_cache **ac_ptr;\r\nint memsize = sizeof(void *) * nr_node_ids;\r\nint i;\r\nif (limit > 1)\r\nlimit = 12;\r\nac_ptr = kzalloc_node(memsize, gfp, node);\r\nif (ac_ptr) {\r\nfor_each_node(i) {\r\nif (i == node || !node_online(i))\r\ncontinue;\r\nac_ptr[i] = alloc_arraycache(node, limit, 0xbaadf00d, gfp);\r\nif (!ac_ptr[i]) {\r\nfor (i--; i >= 0; i--)\r\nkfree(ac_ptr[i]);\r\nkfree(ac_ptr);\r\nreturn NULL;\r\n}\r\n}\r\n}\r\nreturn ac_ptr;\r\n}\r\nstatic void free_alien_cache(struct array_cache **ac_ptr)\r\n{\r\nint i;\r\nif (!ac_ptr)\r\nreturn;\r\nfor_each_node(i)\r\nkfree(ac_ptr[i]);\r\nkfree(ac_ptr);\r\n}\r\nstatic void __drain_alien_cache(struct kmem_cache *cachep,\r\nstruct array_cache *ac, int node)\r\n{\r\nstruct kmem_list3 *rl3 = cachep->nodelists[node];\r\nif (ac->avail) {\r\nspin_lock(&rl3->list_lock);\r\nif (rl3->shared)\r\ntransfer_objects(rl3->shared, ac, ac->limit);\r\nfree_block(cachep, ac->entry, ac->avail, node);\r\nac->avail = 0;\r\nspin_unlock(&rl3->list_lock);\r\n}\r\n}\r\nstatic void reap_alien(struct kmem_cache *cachep, struct kmem_list3 *l3)\r\n{\r\nint node = __this_cpu_read(slab_reap_node);\r\nif (l3->alien) {\r\nstruct array_cache *ac = l3->alien[node];\r\nif (ac && ac->avail && spin_trylock_irq(&ac->lock)) {\r\n__drain_alien_cache(cachep, ac, node);\r\nspin_unlock_irq(&ac->lock);\r\n}\r\n}\r\n}\r\nstatic void drain_alien_cache(struct kmem_cache *cachep,\r\nstruct array_cache **alien)\r\n{\r\nint i = 0;\r\nstruct array_cache *ac;\r\nunsigned long flags;\r\nfor_each_online_node(i) {\r\nac = alien[i];\r\nif (ac) {\r\nspin_lock_irqsave(&ac->lock, flags);\r\n__drain_alien_cache(cachep, ac, i);\r\nspin_unlock_irqrestore(&ac->lock, flags);\r\n}\r\n}\r\n}\r\nstatic inline int cache_free_alien(struct kmem_cache *cachep, void *objp)\r\n{\r\nstruct slab *slabp = virt_to_slab(objp);\r\nint nodeid = slabp->nodeid;\r\nstruct kmem_list3 *l3;\r\nstruct array_cache *alien = NULL;\r\nint node;\r\nnode = numa_mem_id();\r\nif (likely(slabp->nodeid == node))\r\nreturn 0;\r\nl3 = cachep->nodelists[node];\r\nSTATS_INC_NODEFREES(cachep);\r\nif (l3->alien && l3->alien[nodeid]) {\r\nalien = l3->alien[nodeid];\r\nspin_lock(&alien->lock);\r\nif (unlikely(alien->avail == alien->limit)) {\r\nSTATS_INC_ACOVERFLOW(cachep);\r\n__drain_alien_cache(cachep, alien, nodeid);\r\n}\r\nalien->entry[alien->avail++] = objp;\r\nspin_unlock(&alien->lock);\r\n} else {\r\nspin_lock(&(cachep->nodelists[nodeid])->list_lock);\r\nfree_block(cachep, &objp, 1, nodeid);\r\nspin_unlock(&(cachep->nodelists[nodeid])->list_lock);\r\n}\r\nreturn 1;\r\n}\r\nstatic int init_cache_nodelists_node(int node)\r\n{\r\nstruct kmem_cache *cachep;\r\nstruct kmem_list3 *l3;\r\nconst int memsize = sizeof(struct kmem_list3);\r\nlist_for_each_entry(cachep, &cache_chain, next) {\r\nif (!cachep->nodelists[node]) {\r\nl3 = kmalloc_node(memsize, GFP_KERNEL, node);\r\nif (!l3)\r\nreturn -ENOMEM;\r\nkmem_list3_init(l3);\r\nl3->next_reap = jiffies + REAPTIMEOUT_LIST3 +\r\n((unsigned long)cachep) % REAPTIMEOUT_LIST3;\r\ncachep->nodelists[node] = l3;\r\n}\r\nspin_lock_irq(&cachep->nodelists[node]->list_lock);\r\ncachep->nodelists[node]->free_limit =\r\n(1 + nr_cpus_node(node)) *\r\ncachep->batchcount + cachep->num;\r\nspin_unlock_irq(&cachep->nodelists[node]->list_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic void __cpuinit cpuup_canceled(long cpu)\r\n{\r\nstruct kmem_cache *cachep;\r\nstruct kmem_list3 *l3 = NULL;\r\nint node = cpu_to_mem(cpu);\r\nconst struct cpumask *mask = cpumask_of_node(node);\r\nlist_for_each_entry(cachep, &cache_chain, next) {\r\nstruct array_cache *nc;\r\nstruct array_cache *shared;\r\nstruct array_cache **alien;\r\nnc = cachep->array[cpu];\r\ncachep->array[cpu] = NULL;\r\nl3 = cachep->nodelists[node];\r\nif (!l3)\r\ngoto free_array_cache;\r\nspin_lock_irq(&l3->list_lock);\r\nl3->free_limit -= cachep->batchcount;\r\nif (nc)\r\nfree_block(cachep, nc->entry, nc->avail, node);\r\nif (!cpumask_empty(mask)) {\r\nspin_unlock_irq(&l3->list_lock);\r\ngoto free_array_cache;\r\n}\r\nshared = l3->shared;\r\nif (shared) {\r\nfree_block(cachep, shared->entry,\r\nshared->avail, node);\r\nl3->shared = NULL;\r\n}\r\nalien = l3->alien;\r\nl3->alien = NULL;\r\nspin_unlock_irq(&l3->list_lock);\r\nkfree(shared);\r\nif (alien) {\r\ndrain_alien_cache(cachep, alien);\r\nfree_alien_cache(alien);\r\n}\r\nfree_array_cache:\r\nkfree(nc);\r\n}\r\nlist_for_each_entry(cachep, &cache_chain, next) {\r\nl3 = cachep->nodelists[node];\r\nif (!l3)\r\ncontinue;\r\ndrain_freelist(cachep, l3, l3->free_objects);\r\n}\r\n}\r\nstatic int __cpuinit cpuup_prepare(long cpu)\r\n{\r\nstruct kmem_cache *cachep;\r\nstruct kmem_list3 *l3 = NULL;\r\nint node = cpu_to_mem(cpu);\r\nint err;\r\nerr = init_cache_nodelists_node(node);\r\nif (err < 0)\r\ngoto bad;\r\nlist_for_each_entry(cachep, &cache_chain, next) {\r\nstruct array_cache *nc;\r\nstruct array_cache *shared = NULL;\r\nstruct array_cache **alien = NULL;\r\nnc = alloc_arraycache(node, cachep->limit,\r\ncachep->batchcount, GFP_KERNEL);\r\nif (!nc)\r\ngoto bad;\r\nif (cachep->shared) {\r\nshared = alloc_arraycache(node,\r\ncachep->shared * cachep->batchcount,\r\n0xbaadf00d, GFP_KERNEL);\r\nif (!shared) {\r\nkfree(nc);\r\ngoto bad;\r\n}\r\n}\r\nif (use_alien_caches) {\r\nalien = alloc_alien_cache(node, cachep->limit, GFP_KERNEL);\r\nif (!alien) {\r\nkfree(shared);\r\nkfree(nc);\r\ngoto bad;\r\n}\r\n}\r\ncachep->array[cpu] = nc;\r\nl3 = cachep->nodelists[node];\r\nBUG_ON(!l3);\r\nspin_lock_irq(&l3->list_lock);\r\nif (!l3->shared) {\r\nl3->shared = shared;\r\nshared = NULL;\r\n}\r\n#ifdef CONFIG_NUMA\r\nif (!l3->alien) {\r\nl3->alien = alien;\r\nalien = NULL;\r\n}\r\n#endif\r\nspin_unlock_irq(&l3->list_lock);\r\nkfree(shared);\r\nfree_alien_cache(alien);\r\nif (cachep->flags & SLAB_DEBUG_OBJECTS)\r\nslab_set_debugobj_lock_classes_node(cachep, node);\r\n}\r\ninit_node_lock_keys(node);\r\nreturn 0;\r\nbad:\r\ncpuup_canceled(cpu);\r\nreturn -ENOMEM;\r\n}\r\nstatic int __cpuinit cpuup_callback(struct notifier_block *nfb,\r\nunsigned long action, void *hcpu)\r\n{\r\nlong cpu = (long)hcpu;\r\nint err = 0;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_UP_PREPARE_FROZEN:\r\nmutex_lock(&cache_chain_mutex);\r\nerr = cpuup_prepare(cpu);\r\nmutex_unlock(&cache_chain_mutex);\r\nbreak;\r\ncase CPU_ONLINE:\r\ncase CPU_ONLINE_FROZEN:\r\nstart_cpu_timer(cpu);\r\nbreak;\r\n#ifdef CONFIG_HOTPLUG_CPU\r\ncase CPU_DOWN_PREPARE:\r\ncase CPU_DOWN_PREPARE_FROZEN:\r\ncancel_delayed_work_sync(&per_cpu(slab_reap_work, cpu));\r\nper_cpu(slab_reap_work, cpu).work.func = NULL;\r\nbreak;\r\ncase CPU_DOWN_FAILED:\r\ncase CPU_DOWN_FAILED_FROZEN:\r\nstart_cpu_timer(cpu);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\n#endif\r\ncase CPU_UP_CANCELED:\r\ncase CPU_UP_CANCELED_FROZEN:\r\nmutex_lock(&cache_chain_mutex);\r\ncpuup_canceled(cpu);\r\nmutex_unlock(&cache_chain_mutex);\r\nbreak;\r\n}\r\nreturn notifier_from_errno(err);\r\n}\r\nstatic int __meminit drain_cache_nodelists_node(int node)\r\n{\r\nstruct kmem_cache *cachep;\r\nint ret = 0;\r\nlist_for_each_entry(cachep, &cache_chain, next) {\r\nstruct kmem_list3 *l3;\r\nl3 = cachep->nodelists[node];\r\nif (!l3)\r\ncontinue;\r\ndrain_freelist(cachep, l3, l3->free_objects);\r\nif (!list_empty(&l3->slabs_full) ||\r\n!list_empty(&l3->slabs_partial)) {\r\nret = -EBUSY;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int __meminit slab_memory_callback(struct notifier_block *self,\r\nunsigned long action, void *arg)\r\n{\r\nstruct memory_notify *mnb = arg;\r\nint ret = 0;\r\nint nid;\r\nnid = mnb->status_change_nid;\r\nif (nid < 0)\r\ngoto out;\r\nswitch (action) {\r\ncase MEM_GOING_ONLINE:\r\nmutex_lock(&cache_chain_mutex);\r\nret = init_cache_nodelists_node(nid);\r\nmutex_unlock(&cache_chain_mutex);\r\nbreak;\r\ncase MEM_GOING_OFFLINE:\r\nmutex_lock(&cache_chain_mutex);\r\nret = drain_cache_nodelists_node(nid);\r\nmutex_unlock(&cache_chain_mutex);\r\nbreak;\r\ncase MEM_ONLINE:\r\ncase MEM_OFFLINE:\r\ncase MEM_CANCEL_ONLINE:\r\ncase MEM_CANCEL_OFFLINE:\r\nbreak;\r\n}\r\nout:\r\nreturn notifier_from_errno(ret);\r\n}\r\nstatic void __init init_list(struct kmem_cache *cachep, struct kmem_list3 *list,\r\nint nodeid)\r\n{\r\nstruct kmem_list3 *ptr;\r\nptr = kmalloc_node(sizeof(struct kmem_list3), GFP_NOWAIT, nodeid);\r\nBUG_ON(!ptr);\r\nmemcpy(ptr, list, sizeof(struct kmem_list3));\r\nspin_lock_init(&ptr->list_lock);\r\nMAKE_ALL_LISTS(cachep, ptr, nodeid);\r\ncachep->nodelists[nodeid] = ptr;\r\n}\r\nstatic void __init set_up_list3s(struct kmem_cache *cachep, int index)\r\n{\r\nint node;\r\nfor_each_online_node(node) {\r\ncachep->nodelists[node] = &initkmem_list3[index + node];\r\ncachep->nodelists[node]->next_reap = jiffies +\r\nREAPTIMEOUT_LIST3 +\r\n((unsigned long)cachep) % REAPTIMEOUT_LIST3;\r\n}\r\n}\r\nvoid __init kmem_cache_init(void)\r\n{\r\nsize_t left_over;\r\nstruct cache_sizes *sizes;\r\nstruct cache_names *names;\r\nint i;\r\nint order;\r\nint node;\r\nif (num_possible_nodes() == 1)\r\nuse_alien_caches = 0;\r\nfor (i = 0; i < NUM_INIT_LISTS; i++) {\r\nkmem_list3_init(&initkmem_list3[i]);\r\nif (i < MAX_NUMNODES)\r\ncache_cache.nodelists[i] = NULL;\r\n}\r\nset_up_list3s(&cache_cache, CACHE_CACHE);\r\nif (!slab_max_order_set && totalram_pages > (32 << 20) >> PAGE_SHIFT)\r\nslab_max_order = SLAB_MAX_ORDER_HI;\r\nnode = numa_mem_id();\r\nINIT_LIST_HEAD(&cache_chain);\r\nlist_add(&cache_cache.next, &cache_chain);\r\ncache_cache.colour_off = cache_line_size();\r\ncache_cache.array[smp_processor_id()] = &initarray_cache.cache;\r\ncache_cache.nodelists[node] = &initkmem_list3[CACHE_CACHE + node];\r\ncache_cache.buffer_size = offsetof(struct kmem_cache, array[nr_cpu_ids]) +\r\nnr_node_ids * sizeof(struct kmem_list3 *);\r\n#if DEBUG\r\ncache_cache.obj_size = cache_cache.buffer_size;\r\n#endif\r\ncache_cache.buffer_size = ALIGN(cache_cache.buffer_size,\r\ncache_line_size());\r\ncache_cache.reciprocal_buffer_size =\r\nreciprocal_value(cache_cache.buffer_size);\r\nfor (order = 0; order < MAX_ORDER; order++) {\r\ncache_estimate(order, cache_cache.buffer_size,\r\ncache_line_size(), 0, &left_over, &cache_cache.num);\r\nif (cache_cache.num)\r\nbreak;\r\n}\r\nBUG_ON(!cache_cache.num);\r\ncache_cache.gfporder = order;\r\ncache_cache.colour = left_over / cache_cache.colour_off;\r\ncache_cache.slab_size = ALIGN(cache_cache.num * sizeof(kmem_bufctl_t) +\r\nsizeof(struct slab), cache_line_size());\r\nsizes = malloc_sizes;\r\nnames = cache_names;\r\nsizes[INDEX_AC].cs_cachep = kmem_cache_create(names[INDEX_AC].name,\r\nsizes[INDEX_AC].cs_size,\r\nARCH_KMALLOC_MINALIGN,\r\nARCH_KMALLOC_FLAGS|SLAB_PANIC,\r\nNULL);\r\nif (INDEX_AC != INDEX_L3) {\r\nsizes[INDEX_L3].cs_cachep =\r\nkmem_cache_create(names[INDEX_L3].name,\r\nsizes[INDEX_L3].cs_size,\r\nARCH_KMALLOC_MINALIGN,\r\nARCH_KMALLOC_FLAGS|SLAB_PANIC,\r\nNULL);\r\n}\r\nslab_early_init = 0;\r\nwhile (sizes->cs_size != ULONG_MAX) {\r\nif (!sizes->cs_cachep) {\r\nsizes->cs_cachep = kmem_cache_create(names->name,\r\nsizes->cs_size,\r\nARCH_KMALLOC_MINALIGN,\r\nARCH_KMALLOC_FLAGS|SLAB_PANIC,\r\nNULL);\r\n}\r\n#ifdef CONFIG_ZONE_DMA\r\nsizes->cs_dmacachep = kmem_cache_create(\r\nnames->name_dma,\r\nsizes->cs_size,\r\nARCH_KMALLOC_MINALIGN,\r\nARCH_KMALLOC_FLAGS|SLAB_CACHE_DMA|\r\nSLAB_PANIC,\r\nNULL);\r\n#endif\r\nsizes++;\r\nnames++;\r\n}\r\n{\r\nstruct array_cache *ptr;\r\nptr = kmalloc(sizeof(struct arraycache_init), GFP_NOWAIT);\r\nBUG_ON(cpu_cache_get(&cache_cache) != &initarray_cache.cache);\r\nmemcpy(ptr, cpu_cache_get(&cache_cache),\r\nsizeof(struct arraycache_init));\r\nspin_lock_init(&ptr->lock);\r\ncache_cache.array[smp_processor_id()] = ptr;\r\nptr = kmalloc(sizeof(struct arraycache_init), GFP_NOWAIT);\r\nBUG_ON(cpu_cache_get(malloc_sizes[INDEX_AC].cs_cachep)\r\n!= &initarray_generic.cache);\r\nmemcpy(ptr, cpu_cache_get(malloc_sizes[INDEX_AC].cs_cachep),\r\nsizeof(struct arraycache_init));\r\nspin_lock_init(&ptr->lock);\r\nmalloc_sizes[INDEX_AC].cs_cachep->array[smp_processor_id()] =\r\nptr;\r\n}\r\n{\r\nint nid;\r\nfor_each_online_node(nid) {\r\ninit_list(&cache_cache, &initkmem_list3[CACHE_CACHE + nid], nid);\r\ninit_list(malloc_sizes[INDEX_AC].cs_cachep,\r\n&initkmem_list3[SIZE_AC + nid], nid);\r\nif (INDEX_AC != INDEX_L3) {\r\ninit_list(malloc_sizes[INDEX_L3].cs_cachep,\r\n&initkmem_list3[SIZE_L3 + nid], nid);\r\n}\r\n}\r\n}\r\ng_cpucache_up = EARLY;\r\n}\r\nvoid __init kmem_cache_init_late(void)\r\n{\r\nstruct kmem_cache *cachep;\r\ng_cpucache_up = LATE;\r\ninit_lock_keys();\r\nmutex_lock(&cache_chain_mutex);\r\nlist_for_each_entry(cachep, &cache_chain, next)\r\nif (enable_cpucache(cachep, GFP_NOWAIT))\r\nBUG();\r\nmutex_unlock(&cache_chain_mutex);\r\ng_cpucache_up = FULL;\r\nregister_cpu_notifier(&cpucache_notifier);\r\n#ifdef CONFIG_NUMA\r\nhotplug_memory_notifier(slab_memory_callback, SLAB_CALLBACK_PRI);\r\n#endif\r\n}\r\nstatic int __init cpucache_init(void)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nstart_cpu_timer(cpu);\r\nreturn 0;\r\n}\r\nstatic noinline void\r\nslab_out_of_memory(struct kmem_cache *cachep, gfp_t gfpflags, int nodeid)\r\n{\r\nstruct kmem_list3 *l3;\r\nstruct slab *slabp;\r\nunsigned long flags;\r\nint node;\r\nprintk(KERN_WARNING\r\n"SLAB: Unable to allocate memory on node %d (gfp=0x%x)\n",\r\nnodeid, gfpflags);\r\nprintk(KERN_WARNING " cache: %s, object size: %d, order: %d\n",\r\ncachep->name, cachep->buffer_size, cachep->gfporder);\r\nfor_each_online_node(node) {\r\nunsigned long active_objs = 0, num_objs = 0, free_objects = 0;\r\nunsigned long active_slabs = 0, num_slabs = 0;\r\nl3 = cachep->nodelists[node];\r\nif (!l3)\r\ncontinue;\r\nspin_lock_irqsave(&l3->list_lock, flags);\r\nlist_for_each_entry(slabp, &l3->slabs_full, list) {\r\nactive_objs += cachep->num;\r\nactive_slabs++;\r\n}\r\nlist_for_each_entry(slabp, &l3->slabs_partial, list) {\r\nactive_objs += slabp->inuse;\r\nactive_slabs++;\r\n}\r\nlist_for_each_entry(slabp, &l3->slabs_free, list)\r\nnum_slabs++;\r\nfree_objects += l3->free_objects;\r\nspin_unlock_irqrestore(&l3->list_lock, flags);\r\nnum_slabs += active_slabs;\r\nnum_objs = num_slabs * cachep->num;\r\nprintk(KERN_WARNING\r\n" node %d: slabs: %ld/%ld, objs: %ld/%ld, free: %ld\n",\r\nnode, active_slabs, num_slabs, active_objs, num_objs,\r\nfree_objects);\r\n}\r\n}\r\nstatic void *kmem_getpages(struct kmem_cache *cachep, gfp_t flags, int nodeid)\r\n{\r\nstruct page *page;\r\nint nr_pages;\r\nint i;\r\n#ifndef CONFIG_MMU\r\nflags |= __GFP_COMP;\r\n#endif\r\nflags |= cachep->gfpflags;\r\nif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\r\nflags |= __GFP_RECLAIMABLE;\r\npage = alloc_pages_exact_node(nodeid, flags | __GFP_NOTRACK, cachep->gfporder);\r\nif (!page) {\r\nif (!(flags & __GFP_NOWARN) && printk_ratelimit())\r\nslab_out_of_memory(cachep, flags, nodeid);\r\nreturn NULL;\r\n}\r\nnr_pages = (1 << cachep->gfporder);\r\nif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\r\nadd_zone_page_state(page_zone(page),\r\nNR_SLAB_RECLAIMABLE, nr_pages);\r\nelse\r\nadd_zone_page_state(page_zone(page),\r\nNR_SLAB_UNRECLAIMABLE, nr_pages);\r\nfor (i = 0; i < nr_pages; i++)\r\n__SetPageSlab(page + i);\r\nif (kmemcheck_enabled && !(cachep->flags & SLAB_NOTRACK)) {\r\nkmemcheck_alloc_shadow(page, cachep->gfporder, flags, nodeid);\r\nif (cachep->ctor)\r\nkmemcheck_mark_uninitialized_pages(page, nr_pages);\r\nelse\r\nkmemcheck_mark_unallocated_pages(page, nr_pages);\r\n}\r\nreturn page_address(page);\r\n}\r\nstatic void kmem_freepages(struct kmem_cache *cachep, void *addr)\r\n{\r\nunsigned long i = (1 << cachep->gfporder);\r\nstruct page *page = virt_to_page(addr);\r\nconst unsigned long nr_freed = i;\r\nkmemcheck_free_shadow(page, cachep->gfporder);\r\nif (cachep->flags & SLAB_RECLAIM_ACCOUNT)\r\nsub_zone_page_state(page_zone(page),\r\nNR_SLAB_RECLAIMABLE, nr_freed);\r\nelse\r\nsub_zone_page_state(page_zone(page),\r\nNR_SLAB_UNRECLAIMABLE, nr_freed);\r\nwhile (i--) {\r\nBUG_ON(!PageSlab(page));\r\n__ClearPageSlab(page);\r\npage++;\r\n}\r\nif (current->reclaim_state)\r\ncurrent->reclaim_state->reclaimed_slab += nr_freed;\r\nfree_pages((unsigned long)addr, cachep->gfporder);\r\n}\r\nstatic void kmem_rcu_free(struct rcu_head *head)\r\n{\r\nstruct slab_rcu *slab_rcu = (struct slab_rcu *)head;\r\nstruct kmem_cache *cachep = slab_rcu->cachep;\r\nkmem_freepages(cachep, slab_rcu->addr);\r\nif (OFF_SLAB(cachep))\r\nkmem_cache_free(cachep->slabp_cache, slab_rcu);\r\n}\r\nstatic void store_stackinfo(struct kmem_cache *cachep, unsigned long *addr,\r\nunsigned long caller)\r\n{\r\nint size = obj_size(cachep);\r\naddr = (unsigned long *)&((char *)addr)[obj_offset(cachep)];\r\nif (size < 5 * sizeof(unsigned long))\r\nreturn;\r\n*addr++ = 0x12345678;\r\n*addr++ = caller;\r\n*addr++ = smp_processor_id();\r\nsize -= 3 * sizeof(unsigned long);\r\n{\r\nunsigned long *sptr = &caller;\r\nunsigned long svalue;\r\nwhile (!kstack_end(sptr)) {\r\nsvalue = *sptr++;\r\nif (kernel_text_address(svalue)) {\r\n*addr++ = svalue;\r\nsize -= sizeof(unsigned long);\r\nif (size <= sizeof(unsigned long))\r\nbreak;\r\n}\r\n}\r\n}\r\n*addr++ = 0x87654321;\r\n}\r\nstatic void poison_obj(struct kmem_cache *cachep, void *addr, unsigned char val)\r\n{\r\nint size = obj_size(cachep);\r\naddr = &((char *)addr)[obj_offset(cachep)];\r\nmemset(addr, val, size);\r\n*(unsigned char *)(addr + size - 1) = POISON_END;\r\n}\r\nstatic void dump_line(char *data, int offset, int limit)\r\n{\r\nint i;\r\nunsigned char error = 0;\r\nint bad_count = 0;\r\nprintk(KERN_ERR "%03x: ", offset);\r\nfor (i = 0; i < limit; i++) {\r\nif (data[offset + i] != POISON_FREE) {\r\nerror = data[offset + i];\r\nbad_count++;\r\n}\r\n}\r\nprint_hex_dump(KERN_CONT, "", 0, 16, 1,\r\n&data[offset], limit, 1);\r\nif (bad_count == 1) {\r\nerror ^= POISON_FREE;\r\nif (!(error & (error - 1))) {\r\nprintk(KERN_ERR "Single bit error detected. Probably "\r\n"bad RAM.\n");\r\n#ifdef CONFIG_X86\r\nprintk(KERN_ERR "Run memtest86+ or a similar memory "\r\n"test tool.\n");\r\n#else\r\nprintk(KERN_ERR "Run a memory test tool.\n");\r\n#endif\r\n}\r\n}\r\n}\r\nstatic void print_objinfo(struct kmem_cache *cachep, void *objp, int lines)\r\n{\r\nint i, size;\r\nchar *realobj;\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nprintk(KERN_ERR "Redzone: 0x%llx/0x%llx.\n",\r\n*dbg_redzone1(cachep, objp),\r\n*dbg_redzone2(cachep, objp));\r\n}\r\nif (cachep->flags & SLAB_STORE_USER) {\r\nprintk(KERN_ERR "Last user: [<%p>]",\r\n*dbg_userword(cachep, objp));\r\nprint_symbol("(%s)",\r\n(unsigned long)*dbg_userword(cachep, objp));\r\nprintk("\n");\r\n}\r\nrealobj = (char *)objp + obj_offset(cachep);\r\nsize = obj_size(cachep);\r\nfor (i = 0; i < size && lines; i += 16, lines--) {\r\nint limit;\r\nlimit = 16;\r\nif (i + limit > size)\r\nlimit = size - i;\r\ndump_line(realobj, i, limit);\r\n}\r\n}\r\nstatic void check_poison_obj(struct kmem_cache *cachep, void *objp)\r\n{\r\nchar *realobj;\r\nint size, i;\r\nint lines = 0;\r\nrealobj = (char *)objp + obj_offset(cachep);\r\nsize = obj_size(cachep);\r\nfor (i = 0; i < size; i++) {\r\nchar exp = POISON_FREE;\r\nif (i == size - 1)\r\nexp = POISON_END;\r\nif (realobj[i] != exp) {\r\nint limit;\r\nif (lines == 0) {\r\nprintk(KERN_ERR\r\n"Slab corruption (%s): %s start=%p, len=%d\n",\r\nprint_tainted(), cachep->name, realobj, size);\r\nprint_objinfo(cachep, objp, 0);\r\n}\r\ni = (i / 16) * 16;\r\nlimit = 16;\r\nif (i + limit > size)\r\nlimit = size - i;\r\ndump_line(realobj, i, limit);\r\ni += 16;\r\nlines++;\r\nif (lines > 5)\r\nbreak;\r\n}\r\n}\r\nif (lines != 0) {\r\nstruct slab *slabp = virt_to_slab(objp);\r\nunsigned int objnr;\r\nobjnr = obj_to_index(cachep, slabp, objp);\r\nif (objnr) {\r\nobjp = index_to_obj(cachep, slabp, objnr - 1);\r\nrealobj = (char *)objp + obj_offset(cachep);\r\nprintk(KERN_ERR "Prev obj: start=%p, len=%d\n",\r\nrealobj, size);\r\nprint_objinfo(cachep, objp, 2);\r\n}\r\nif (objnr + 1 < cachep->num) {\r\nobjp = index_to_obj(cachep, slabp, objnr + 1);\r\nrealobj = (char *)objp + obj_offset(cachep);\r\nprintk(KERN_ERR "Next obj: start=%p, len=%d\n",\r\nrealobj, size);\r\nprint_objinfo(cachep, objp, 2);\r\n}\r\n}\r\n}\r\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep, struct slab *slabp)\r\n{\r\nint i;\r\nfor (i = 0; i < cachep->num; i++) {\r\nvoid *objp = index_to_obj(cachep, slabp, i);\r\nif (cachep->flags & SLAB_POISON) {\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nif (cachep->buffer_size % PAGE_SIZE == 0 &&\r\nOFF_SLAB(cachep))\r\nkernel_map_pages(virt_to_page(objp),\r\ncachep->buffer_size / PAGE_SIZE, 1);\r\nelse\r\ncheck_poison_obj(cachep, objp);\r\n#else\r\ncheck_poison_obj(cachep, objp);\r\n#endif\r\n}\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\r\nslab_error(cachep, "start of a freed object "\r\n"was overwritten");\r\nif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\r\nslab_error(cachep, "end of a freed object "\r\n"was overwritten");\r\n}\r\n}\r\n}\r\nstatic void slab_destroy_debugcheck(struct kmem_cache *cachep, struct slab *slabp)\r\n{\r\n}\r\nstatic void slab_destroy(struct kmem_cache *cachep, struct slab *slabp)\r\n{\r\nvoid *addr = slabp->s_mem - slabp->colouroff;\r\nslab_destroy_debugcheck(cachep, slabp);\r\nif (unlikely(cachep->flags & SLAB_DESTROY_BY_RCU)) {\r\nstruct slab_rcu *slab_rcu;\r\nslab_rcu = (struct slab_rcu *)slabp;\r\nslab_rcu->cachep = cachep;\r\nslab_rcu->addr = addr;\r\ncall_rcu(&slab_rcu->head, kmem_rcu_free);\r\n} else {\r\nkmem_freepages(cachep, addr);\r\nif (OFF_SLAB(cachep))\r\nkmem_cache_free(cachep->slabp_cache, slabp);\r\n}\r\n}\r\nstatic void __kmem_cache_destroy(struct kmem_cache *cachep)\r\n{\r\nint i;\r\nstruct kmem_list3 *l3;\r\nfor_each_online_cpu(i)\r\nkfree(cachep->array[i]);\r\nfor_each_online_node(i) {\r\nl3 = cachep->nodelists[i];\r\nif (l3) {\r\nkfree(l3->shared);\r\nfree_alien_cache(l3->alien);\r\nkfree(l3);\r\n}\r\n}\r\nkmem_cache_free(&cache_cache, cachep);\r\n}\r\nstatic size_t calculate_slab_order(struct kmem_cache *cachep,\r\nsize_t size, size_t align, unsigned long flags)\r\n{\r\nunsigned long offslab_limit;\r\nsize_t left_over = 0;\r\nint gfporder;\r\nfor (gfporder = 0; gfporder <= KMALLOC_MAX_ORDER; gfporder++) {\r\nunsigned int num;\r\nsize_t remainder;\r\ncache_estimate(gfporder, size, align, flags, &remainder, &num);\r\nif (!num)\r\ncontinue;\r\nif (flags & CFLGS_OFF_SLAB) {\r\noffslab_limit = size - sizeof(struct slab);\r\noffslab_limit /= sizeof(kmem_bufctl_t);\r\nif (num > offslab_limit)\r\nbreak;\r\n}\r\ncachep->num = num;\r\ncachep->gfporder = gfporder;\r\nleft_over = remainder;\r\nif (flags & SLAB_RECLAIM_ACCOUNT)\r\nbreak;\r\nif (gfporder >= slab_max_order)\r\nbreak;\r\nif (left_over * 8 <= (PAGE_SIZE << gfporder))\r\nbreak;\r\n}\r\nreturn left_over;\r\n}\r\nstatic int __init_refok setup_cpu_cache(struct kmem_cache *cachep, gfp_t gfp)\r\n{\r\nif (g_cpucache_up == FULL)\r\nreturn enable_cpucache(cachep, gfp);\r\nif (g_cpucache_up == NONE) {\r\ncachep->array[smp_processor_id()] = &initarray_generic.cache;\r\nset_up_list3s(cachep, SIZE_AC);\r\nif (INDEX_AC == INDEX_L3)\r\ng_cpucache_up = PARTIAL_L3;\r\nelse\r\ng_cpucache_up = PARTIAL_AC;\r\n} else {\r\ncachep->array[smp_processor_id()] =\r\nkmalloc(sizeof(struct arraycache_init), gfp);\r\nif (g_cpucache_up == PARTIAL_AC) {\r\nset_up_list3s(cachep, SIZE_L3);\r\ng_cpucache_up = PARTIAL_L3;\r\n} else {\r\nint node;\r\nfor_each_online_node(node) {\r\ncachep->nodelists[node] =\r\nkmalloc_node(sizeof(struct kmem_list3),\r\ngfp, node);\r\nBUG_ON(!cachep->nodelists[node]);\r\nkmem_list3_init(cachep->nodelists[node]);\r\n}\r\n}\r\n}\r\ncachep->nodelists[numa_mem_id()]->next_reap =\r\njiffies + REAPTIMEOUT_LIST3 +\r\n((unsigned long)cachep) % REAPTIMEOUT_LIST3;\r\ncpu_cache_get(cachep)->avail = 0;\r\ncpu_cache_get(cachep)->limit = BOOT_CPUCACHE_ENTRIES;\r\ncpu_cache_get(cachep)->batchcount = 1;\r\ncpu_cache_get(cachep)->touched = 0;\r\ncachep->batchcount = 1;\r\ncachep->limit = BOOT_CPUCACHE_ENTRIES;\r\nreturn 0;\r\n}\r\nstruct kmem_cache *\r\nkmem_cache_create (const char *name, size_t size, size_t align,\r\nunsigned long flags, void (*ctor)(void *))\r\n{\r\nsize_t left_over, slab_size, ralign;\r\nstruct kmem_cache *cachep = NULL, *pc;\r\ngfp_t gfp;\r\nif (!name || in_interrupt() || (size < BYTES_PER_WORD) ||\r\nsize > KMALLOC_MAX_SIZE) {\r\nprintk(KERN_ERR "%s: Early error in slab %s\n", __func__,\r\nname);\r\nBUG();\r\n}\r\nif (slab_is_available()) {\r\nget_online_cpus();\r\nmutex_lock(&cache_chain_mutex);\r\n}\r\nlist_for_each_entry(pc, &cache_chain, next) {\r\nchar tmp;\r\nint res;\r\nres = probe_kernel_address(pc->name, tmp);\r\nif (res) {\r\nprintk(KERN_ERR\r\n"SLAB: cache with size %d has lost its name\n",\r\npc->buffer_size);\r\ncontinue;\r\n}\r\nif (!strcmp(pc->name, name)) {\r\nprintk(KERN_ERR\r\n"kmem_cache_create: duplicate cache %s\n", name);\r\ndump_stack();\r\ngoto oops;\r\n}\r\n}\r\n#if DEBUG\r\nWARN_ON(strchr(name, ' '));\r\n#if FORCED_DEBUG\r\nif (size < 4096 || fls(size - 1) == fls(size-1 + REDZONE_ALIGN +\r\n2 * sizeof(unsigned long long)))\r\nflags |= SLAB_RED_ZONE | SLAB_STORE_USER;\r\nif (!(flags & SLAB_DESTROY_BY_RCU))\r\nflags |= SLAB_POISON;\r\n#endif\r\nif (flags & SLAB_DESTROY_BY_RCU)\r\nBUG_ON(flags & SLAB_POISON);\r\n#endif\r\nBUG_ON(flags & ~CREATE_MASK);\r\nif (size & (BYTES_PER_WORD - 1)) {\r\nsize += (BYTES_PER_WORD - 1);\r\nsize &= ~(BYTES_PER_WORD - 1);\r\n}\r\nif (flags & SLAB_HWCACHE_ALIGN) {\r\nralign = cache_line_size();\r\nwhile (size <= ralign / 2)\r\nralign /= 2;\r\n} else {\r\nralign = BYTES_PER_WORD;\r\n}\r\nif (flags & SLAB_STORE_USER)\r\nralign = BYTES_PER_WORD;\r\nif (flags & SLAB_RED_ZONE) {\r\nralign = REDZONE_ALIGN;\r\nsize += REDZONE_ALIGN - 1;\r\nsize &= ~(REDZONE_ALIGN - 1);\r\n}\r\nif (ralign < ARCH_SLAB_MINALIGN) {\r\nralign = ARCH_SLAB_MINALIGN;\r\n}\r\nif (ralign < align) {\r\nralign = align;\r\n}\r\nif (ralign > __alignof__(unsigned long long))\r\nflags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\r\nalign = ralign;\r\nif (slab_is_available())\r\ngfp = GFP_KERNEL;\r\nelse\r\ngfp = GFP_NOWAIT;\r\ncachep = kmem_cache_zalloc(&cache_cache, gfp);\r\nif (!cachep)\r\ngoto oops;\r\ncachep->nodelists = (struct kmem_list3 **)&cachep->array[nr_cpu_ids];\r\n#if DEBUG\r\ncachep->obj_size = size;\r\nif (flags & SLAB_RED_ZONE) {\r\ncachep->obj_offset += sizeof(unsigned long long);\r\nsize += 2 * sizeof(unsigned long long);\r\n}\r\nif (flags & SLAB_STORE_USER) {\r\nif (flags & SLAB_RED_ZONE)\r\nsize += REDZONE_ALIGN;\r\nelse\r\nsize += BYTES_PER_WORD;\r\n}\r\n#if FORCED_DEBUG && defined(CONFIG_DEBUG_PAGEALLOC)\r\nif (size >= malloc_sizes[INDEX_L3 + 1].cs_size\r\n&& cachep->obj_size > cache_line_size() && ALIGN(size, align) < PAGE_SIZE) {\r\ncachep->obj_offset += PAGE_SIZE - ALIGN(size, align);\r\nsize = PAGE_SIZE;\r\n}\r\n#endif\r\n#endif\r\nif ((size >= (PAGE_SIZE >> 3)) && !slab_early_init &&\r\n!(flags & SLAB_NOLEAKTRACE))\r\nflags |= CFLGS_OFF_SLAB;\r\nsize = ALIGN(size, align);\r\nleft_over = calculate_slab_order(cachep, size, align, flags);\r\nif (!cachep->num) {\r\nprintk(KERN_ERR\r\n"kmem_cache_create: couldn't create cache %s.\n", name);\r\nkmem_cache_free(&cache_cache, cachep);\r\ncachep = NULL;\r\ngoto oops;\r\n}\r\nslab_size = ALIGN(cachep->num * sizeof(kmem_bufctl_t)\r\n+ sizeof(struct slab), align);\r\nif (flags & CFLGS_OFF_SLAB && left_over >= slab_size) {\r\nflags &= ~CFLGS_OFF_SLAB;\r\nleft_over -= slab_size;\r\n}\r\nif (flags & CFLGS_OFF_SLAB) {\r\nslab_size =\r\ncachep->num * sizeof(kmem_bufctl_t) + sizeof(struct slab);\r\n#ifdef CONFIG_PAGE_POISONING\r\nif (size % PAGE_SIZE == 0 && flags & SLAB_POISON)\r\nflags &= ~(SLAB_RED_ZONE | SLAB_STORE_USER);\r\n#endif\r\n}\r\ncachep->colour_off = cache_line_size();\r\nif (cachep->colour_off < align)\r\ncachep->colour_off = align;\r\ncachep->colour = left_over / cachep->colour_off;\r\ncachep->slab_size = slab_size;\r\ncachep->flags = flags;\r\ncachep->gfpflags = 0;\r\nif (CONFIG_ZONE_DMA_FLAG && (flags & SLAB_CACHE_DMA))\r\ncachep->gfpflags |= GFP_DMA;\r\ncachep->buffer_size = size;\r\ncachep->reciprocal_buffer_size = reciprocal_value(size);\r\nif (flags & CFLGS_OFF_SLAB) {\r\ncachep->slabp_cache = kmem_find_general_cachep(slab_size, 0u);\r\nBUG_ON(ZERO_OR_NULL_PTR(cachep->slabp_cache));\r\n}\r\ncachep->ctor = ctor;\r\ncachep->name = name;\r\nif (setup_cpu_cache(cachep, gfp)) {\r\n__kmem_cache_destroy(cachep);\r\ncachep = NULL;\r\ngoto oops;\r\n}\r\nif (flags & SLAB_DEBUG_OBJECTS) {\r\nWARN_ON_ONCE(flags & SLAB_DESTROY_BY_RCU);\r\nslab_set_debugobj_lock_classes(cachep);\r\n}\r\nlist_add(&cachep->next, &cache_chain);\r\noops:\r\nif (!cachep && (flags & SLAB_PANIC))\r\npanic("kmem_cache_create(): failed to create slab `%s'\n",\r\nname);\r\nif (slab_is_available()) {\r\nmutex_unlock(&cache_chain_mutex);\r\nput_online_cpus();\r\n}\r\nreturn cachep;\r\n}\r\nstatic void check_irq_off(void)\r\n{\r\nBUG_ON(!irqs_disabled());\r\n}\r\nstatic void check_irq_on(void)\r\n{\r\nBUG_ON(irqs_disabled());\r\n}\r\nstatic void check_spinlock_acquired(struct kmem_cache *cachep)\r\n{\r\n#ifdef CONFIG_SMP\r\ncheck_irq_off();\r\nassert_spin_locked(&cachep->nodelists[numa_mem_id()]->list_lock);\r\n#endif\r\n}\r\nstatic void check_spinlock_acquired_node(struct kmem_cache *cachep, int node)\r\n{\r\n#ifdef CONFIG_SMP\r\ncheck_irq_off();\r\nassert_spin_locked(&cachep->nodelists[node]->list_lock);\r\n#endif\r\n}\r\nstatic void do_drain(void *arg)\r\n{\r\nstruct kmem_cache *cachep = arg;\r\nstruct array_cache *ac;\r\nint node = numa_mem_id();\r\ncheck_irq_off();\r\nac = cpu_cache_get(cachep);\r\nspin_lock(&cachep->nodelists[node]->list_lock);\r\nfree_block(cachep, ac->entry, ac->avail, node);\r\nspin_unlock(&cachep->nodelists[node]->list_lock);\r\nac->avail = 0;\r\n}\r\nstatic void drain_cpu_caches(struct kmem_cache *cachep)\r\n{\r\nstruct kmem_list3 *l3;\r\nint node;\r\non_each_cpu(do_drain, cachep, 1);\r\ncheck_irq_on();\r\nfor_each_online_node(node) {\r\nl3 = cachep->nodelists[node];\r\nif (l3 && l3->alien)\r\ndrain_alien_cache(cachep, l3->alien);\r\n}\r\nfor_each_online_node(node) {\r\nl3 = cachep->nodelists[node];\r\nif (l3)\r\ndrain_array(cachep, l3, l3->shared, 1, node);\r\n}\r\n}\r\nstatic int drain_freelist(struct kmem_cache *cache,\r\nstruct kmem_list3 *l3, int tofree)\r\n{\r\nstruct list_head *p;\r\nint nr_freed;\r\nstruct slab *slabp;\r\nnr_freed = 0;\r\nwhile (nr_freed < tofree && !list_empty(&l3->slabs_free)) {\r\nspin_lock_irq(&l3->list_lock);\r\np = l3->slabs_free.prev;\r\nif (p == &l3->slabs_free) {\r\nspin_unlock_irq(&l3->list_lock);\r\ngoto out;\r\n}\r\nslabp = list_entry(p, struct slab, list);\r\n#if DEBUG\r\nBUG_ON(slabp->inuse);\r\n#endif\r\nlist_del(&slabp->list);\r\nl3->free_objects -= cache->num;\r\nspin_unlock_irq(&l3->list_lock);\r\nslab_destroy(cache, slabp);\r\nnr_freed++;\r\n}\r\nout:\r\nreturn nr_freed;\r\n}\r\nstatic int __cache_shrink(struct kmem_cache *cachep)\r\n{\r\nint ret = 0, i = 0;\r\nstruct kmem_list3 *l3;\r\ndrain_cpu_caches(cachep);\r\ncheck_irq_on();\r\nfor_each_online_node(i) {\r\nl3 = cachep->nodelists[i];\r\nif (!l3)\r\ncontinue;\r\ndrain_freelist(cachep, l3, l3->free_objects);\r\nret += !list_empty(&l3->slabs_full) ||\r\n!list_empty(&l3->slabs_partial);\r\n}\r\nreturn (ret ? 1 : 0);\r\n}\r\nint kmem_cache_shrink(struct kmem_cache *cachep)\r\n{\r\nint ret;\r\nBUG_ON(!cachep || in_interrupt());\r\nget_online_cpus();\r\nmutex_lock(&cache_chain_mutex);\r\nret = __cache_shrink(cachep);\r\nmutex_unlock(&cache_chain_mutex);\r\nput_online_cpus();\r\nreturn ret;\r\n}\r\nvoid kmem_cache_destroy(struct kmem_cache *cachep)\r\n{\r\nBUG_ON(!cachep || in_interrupt());\r\nget_online_cpus();\r\nmutex_lock(&cache_chain_mutex);\r\nlist_del(&cachep->next);\r\nif (__cache_shrink(cachep)) {\r\nslab_error(cachep, "Can't free all objects");\r\nlist_add(&cachep->next, &cache_chain);\r\nmutex_unlock(&cache_chain_mutex);\r\nput_online_cpus();\r\nreturn;\r\n}\r\nif (unlikely(cachep->flags & SLAB_DESTROY_BY_RCU))\r\nrcu_barrier();\r\n__kmem_cache_destroy(cachep);\r\nmutex_unlock(&cache_chain_mutex);\r\nput_online_cpus();\r\n}\r\nstatic struct slab *alloc_slabmgmt(struct kmem_cache *cachep, void *objp,\r\nint colour_off, gfp_t local_flags,\r\nint nodeid)\r\n{\r\nstruct slab *slabp;\r\nif (OFF_SLAB(cachep)) {\r\nslabp = kmem_cache_alloc_node(cachep->slabp_cache,\r\nlocal_flags, nodeid);\r\nkmemleak_scan_area(&slabp->list, sizeof(struct list_head),\r\nlocal_flags);\r\nif (!slabp)\r\nreturn NULL;\r\n} else {\r\nslabp = objp + colour_off;\r\ncolour_off += cachep->slab_size;\r\n}\r\nslabp->inuse = 0;\r\nslabp->colouroff = colour_off;\r\nslabp->s_mem = objp + colour_off;\r\nslabp->nodeid = nodeid;\r\nslabp->free = 0;\r\nreturn slabp;\r\n}\r\nstatic inline kmem_bufctl_t *slab_bufctl(struct slab *slabp)\r\n{\r\nreturn (kmem_bufctl_t *) (slabp + 1);\r\n}\r\nstatic void cache_init_objs(struct kmem_cache *cachep,\r\nstruct slab *slabp)\r\n{\r\nint i;\r\nfor (i = 0; i < cachep->num; i++) {\r\nvoid *objp = index_to_obj(cachep, slabp, i);\r\n#if DEBUG\r\nif (cachep->flags & SLAB_POISON)\r\npoison_obj(cachep, objp, POISON_FREE);\r\nif (cachep->flags & SLAB_STORE_USER)\r\n*dbg_userword(cachep, objp) = NULL;\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\n*dbg_redzone1(cachep, objp) = RED_INACTIVE;\r\n*dbg_redzone2(cachep, objp) = RED_INACTIVE;\r\n}\r\nif (cachep->ctor && !(cachep->flags & SLAB_POISON))\r\ncachep->ctor(objp + obj_offset(cachep));\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nif (*dbg_redzone2(cachep, objp) != RED_INACTIVE)\r\nslab_error(cachep, "constructor overwrote the"\r\n" end of an object");\r\nif (*dbg_redzone1(cachep, objp) != RED_INACTIVE)\r\nslab_error(cachep, "constructor overwrote the"\r\n" start of an object");\r\n}\r\nif ((cachep->buffer_size % PAGE_SIZE) == 0 &&\r\nOFF_SLAB(cachep) && cachep->flags & SLAB_POISON)\r\nkernel_map_pages(virt_to_page(objp),\r\ncachep->buffer_size / PAGE_SIZE, 0);\r\n#else\r\nif (cachep->ctor)\r\ncachep->ctor(objp);\r\n#endif\r\nslab_bufctl(slabp)[i] = i + 1;\r\n}\r\nslab_bufctl(slabp)[i - 1] = BUFCTL_END;\r\n}\r\nstatic void kmem_flagcheck(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nif (CONFIG_ZONE_DMA_FLAG) {\r\nif (flags & GFP_DMA)\r\nBUG_ON(!(cachep->gfpflags & GFP_DMA));\r\nelse\r\nBUG_ON(cachep->gfpflags & GFP_DMA);\r\n}\r\n}\r\nstatic void *slab_get_obj(struct kmem_cache *cachep, struct slab *slabp,\r\nint nodeid)\r\n{\r\nvoid *objp = index_to_obj(cachep, slabp, slabp->free);\r\nkmem_bufctl_t next;\r\nslabp->inuse++;\r\nnext = slab_bufctl(slabp)[slabp->free];\r\n#if DEBUG\r\nslab_bufctl(slabp)[slabp->free] = BUFCTL_FREE;\r\nWARN_ON(slabp->nodeid != nodeid);\r\n#endif\r\nslabp->free = next;\r\nreturn objp;\r\n}\r\nstatic void slab_put_obj(struct kmem_cache *cachep, struct slab *slabp,\r\nvoid *objp, int nodeid)\r\n{\r\nunsigned int objnr = obj_to_index(cachep, slabp, objp);\r\n#if DEBUG\r\nWARN_ON(slabp->nodeid != nodeid);\r\nif (slab_bufctl(slabp)[objnr] + 1 <= SLAB_LIMIT + 1) {\r\nprintk(KERN_ERR "slab: double free detected in cache "\r\n"'%s', objp %p\n", cachep->name, objp);\r\nBUG();\r\n}\r\n#endif\r\nslab_bufctl(slabp)[objnr] = slabp->free;\r\nslabp->free = objnr;\r\nslabp->inuse--;\r\n}\r\nstatic void slab_map_pages(struct kmem_cache *cache, struct slab *slab,\r\nvoid *addr)\r\n{\r\nint nr_pages;\r\nstruct page *page;\r\npage = virt_to_page(addr);\r\nnr_pages = 1;\r\nif (likely(!PageCompound(page)))\r\nnr_pages <<= cache->gfporder;\r\ndo {\r\npage_set_cache(page, cache);\r\npage_set_slab(page, slab);\r\npage++;\r\n} while (--nr_pages);\r\n}\r\nstatic int cache_grow(struct kmem_cache *cachep,\r\ngfp_t flags, int nodeid, void *objp)\r\n{\r\nstruct slab *slabp;\r\nsize_t offset;\r\ngfp_t local_flags;\r\nstruct kmem_list3 *l3;\r\nBUG_ON(flags & GFP_SLAB_BUG_MASK);\r\nlocal_flags = flags & (GFP_CONSTRAINT_MASK|GFP_RECLAIM_MASK);\r\ncheck_irq_off();\r\nl3 = cachep->nodelists[nodeid];\r\nspin_lock(&l3->list_lock);\r\noffset = l3->colour_next;\r\nl3->colour_next++;\r\nif (l3->colour_next >= cachep->colour)\r\nl3->colour_next = 0;\r\nspin_unlock(&l3->list_lock);\r\noffset *= cachep->colour_off;\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_enable();\r\nkmem_flagcheck(cachep, flags);\r\nif (!objp)\r\nobjp = kmem_getpages(cachep, local_flags, nodeid);\r\nif (!objp)\r\ngoto failed;\r\nslabp = alloc_slabmgmt(cachep, objp, offset,\r\nlocal_flags & ~GFP_CONSTRAINT_MASK, nodeid);\r\nif (!slabp)\r\ngoto opps1;\r\nslab_map_pages(cachep, slabp, objp);\r\ncache_init_objs(cachep, slabp);\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_disable();\r\ncheck_irq_off();\r\nspin_lock(&l3->list_lock);\r\nlist_add_tail(&slabp->list, &(l3->slabs_free));\r\nSTATS_INC_GROWN(cachep);\r\nl3->free_objects += cachep->num;\r\nspin_unlock(&l3->list_lock);\r\nreturn 1;\r\nopps1:\r\nkmem_freepages(cachep, objp);\r\nfailed:\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_disable();\r\nreturn 0;\r\n}\r\nstatic void kfree_debugcheck(const void *objp)\r\n{\r\nif (!virt_addr_valid(objp)) {\r\nprintk(KERN_ERR "kfree_debugcheck: out of range ptr %lxh.\n",\r\n(unsigned long)objp);\r\nBUG();\r\n}\r\n}\r\nstatic inline void verify_redzone_free(struct kmem_cache *cache, void *obj)\r\n{\r\nunsigned long long redzone1, redzone2;\r\nredzone1 = *dbg_redzone1(cache, obj);\r\nredzone2 = *dbg_redzone2(cache, obj);\r\nif (redzone1 == RED_ACTIVE && redzone2 == RED_ACTIVE)\r\nreturn;\r\nif (redzone1 == RED_INACTIVE && redzone2 == RED_INACTIVE)\r\nslab_error(cache, "double free detected");\r\nelse\r\nslab_error(cache, "memory outside object was overwritten");\r\nprintk(KERN_ERR "%p: redzone 1:0x%llx, redzone 2:0x%llx.\n",\r\nobj, redzone1, redzone2);\r\n}\r\nstatic void *cache_free_debugcheck(struct kmem_cache *cachep, void *objp,\r\nvoid *caller)\r\n{\r\nstruct page *page;\r\nunsigned int objnr;\r\nstruct slab *slabp;\r\nBUG_ON(virt_to_cache(objp) != cachep);\r\nobjp -= obj_offset(cachep);\r\nkfree_debugcheck(objp);\r\npage = virt_to_head_page(objp);\r\nslabp = page_get_slab(page);\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nverify_redzone_free(cachep, objp);\r\n*dbg_redzone1(cachep, objp) = RED_INACTIVE;\r\n*dbg_redzone2(cachep, objp) = RED_INACTIVE;\r\n}\r\nif (cachep->flags & SLAB_STORE_USER)\r\n*dbg_userword(cachep, objp) = caller;\r\nobjnr = obj_to_index(cachep, slabp, objp);\r\nBUG_ON(objnr >= cachep->num);\r\nBUG_ON(objp != index_to_obj(cachep, slabp, objnr));\r\n#ifdef CONFIG_DEBUG_SLAB_LEAK\r\nslab_bufctl(slabp)[objnr] = BUFCTL_FREE;\r\n#endif\r\nif (cachep->flags & SLAB_POISON) {\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nif ((cachep->buffer_size % PAGE_SIZE)==0 && OFF_SLAB(cachep)) {\r\nstore_stackinfo(cachep, objp, (unsigned long)caller);\r\nkernel_map_pages(virt_to_page(objp),\r\ncachep->buffer_size / PAGE_SIZE, 0);\r\n} else {\r\npoison_obj(cachep, objp, POISON_FREE);\r\n}\r\n#else\r\npoison_obj(cachep, objp, POISON_FREE);\r\n#endif\r\n}\r\nreturn objp;\r\n}\r\nstatic void check_slabp(struct kmem_cache *cachep, struct slab *slabp)\r\n{\r\nkmem_bufctl_t i;\r\nint entries = 0;\r\nfor (i = slabp->free; i != BUFCTL_END; i = slab_bufctl(slabp)[i]) {\r\nentries++;\r\nif (entries > cachep->num || i >= cachep->num)\r\ngoto bad;\r\n}\r\nif (entries != cachep->num - slabp->inuse) {\r\nbad:\r\nprintk(KERN_ERR "slab: Internal list corruption detected in "\r\n"cache '%s'(%d), slabp %p(%d). Tainted(%s). Hexdump:\n",\r\ncachep->name, cachep->num, slabp, slabp->inuse,\r\nprint_tainted());\r\nprint_hex_dump(KERN_ERR, "", DUMP_PREFIX_OFFSET, 16, 1, slabp,\r\nsizeof(*slabp) + cachep->num * sizeof(kmem_bufctl_t),\r\n1);\r\nBUG();\r\n}\r\n}\r\nstatic void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nint batchcount;\r\nstruct kmem_list3 *l3;\r\nstruct array_cache *ac;\r\nint node;\r\nretry:\r\ncheck_irq_off();\r\nnode = numa_mem_id();\r\nac = cpu_cache_get(cachep);\r\nbatchcount = ac->batchcount;\r\nif (!ac->touched && batchcount > BATCHREFILL_LIMIT) {\r\nbatchcount = BATCHREFILL_LIMIT;\r\n}\r\nl3 = cachep->nodelists[node];\r\nBUG_ON(ac->avail > 0 || !l3);\r\nspin_lock(&l3->list_lock);\r\nif (l3->shared && transfer_objects(ac, l3->shared, batchcount)) {\r\nl3->shared->touched = 1;\r\ngoto alloc_done;\r\n}\r\nwhile (batchcount > 0) {\r\nstruct list_head *entry;\r\nstruct slab *slabp;\r\nentry = l3->slabs_partial.next;\r\nif (entry == &l3->slabs_partial) {\r\nl3->free_touched = 1;\r\nentry = l3->slabs_free.next;\r\nif (entry == &l3->slabs_free)\r\ngoto must_grow;\r\n}\r\nslabp = list_entry(entry, struct slab, list);\r\ncheck_slabp(cachep, slabp);\r\ncheck_spinlock_acquired(cachep);\r\nBUG_ON(slabp->inuse >= cachep->num);\r\nwhile (slabp->inuse < cachep->num && batchcount--) {\r\nSTATS_INC_ALLOCED(cachep);\r\nSTATS_INC_ACTIVE(cachep);\r\nSTATS_SET_HIGH(cachep);\r\nac->entry[ac->avail++] = slab_get_obj(cachep, slabp,\r\nnode);\r\n}\r\ncheck_slabp(cachep, slabp);\r\nlist_del(&slabp->list);\r\nif (slabp->free == BUFCTL_END)\r\nlist_add(&slabp->list, &l3->slabs_full);\r\nelse\r\nlist_add(&slabp->list, &l3->slabs_partial);\r\n}\r\nmust_grow:\r\nl3->free_objects -= ac->avail;\r\nalloc_done:\r\nspin_unlock(&l3->list_lock);\r\nif (unlikely(!ac->avail)) {\r\nint x;\r\nx = cache_grow(cachep, flags | GFP_THISNODE, node, NULL);\r\nac = cpu_cache_get(cachep);\r\nif (!x && ac->avail == 0)\r\nreturn NULL;\r\nif (!ac->avail)\r\ngoto retry;\r\n}\r\nac->touched = 1;\r\nreturn ac->entry[--ac->avail];\r\n}\r\nstatic inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,\r\ngfp_t flags)\r\n{\r\nmight_sleep_if(flags & __GFP_WAIT);\r\n#if DEBUG\r\nkmem_flagcheck(cachep, flags);\r\n#endif\r\n}\r\nstatic void *cache_alloc_debugcheck_after(struct kmem_cache *cachep,\r\ngfp_t flags, void *objp, void *caller)\r\n{\r\nif (!objp)\r\nreturn objp;\r\nif (cachep->flags & SLAB_POISON) {\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nif ((cachep->buffer_size % PAGE_SIZE) == 0 && OFF_SLAB(cachep))\r\nkernel_map_pages(virt_to_page(objp),\r\ncachep->buffer_size / PAGE_SIZE, 1);\r\nelse\r\ncheck_poison_obj(cachep, objp);\r\n#else\r\ncheck_poison_obj(cachep, objp);\r\n#endif\r\npoison_obj(cachep, objp, POISON_INUSE);\r\n}\r\nif (cachep->flags & SLAB_STORE_USER)\r\n*dbg_userword(cachep, objp) = caller;\r\nif (cachep->flags & SLAB_RED_ZONE) {\r\nif (*dbg_redzone1(cachep, objp) != RED_INACTIVE ||\r\n*dbg_redzone2(cachep, objp) != RED_INACTIVE) {\r\nslab_error(cachep, "double free, or memory outside"\r\n" object was overwritten");\r\nprintk(KERN_ERR\r\n"%p: redzone 1:0x%llx, redzone 2:0x%llx\n",\r\nobjp, *dbg_redzone1(cachep, objp),\r\n*dbg_redzone2(cachep, objp));\r\n}\r\n*dbg_redzone1(cachep, objp) = RED_ACTIVE;\r\n*dbg_redzone2(cachep, objp) = RED_ACTIVE;\r\n}\r\n#ifdef CONFIG_DEBUG_SLAB_LEAK\r\n{\r\nstruct slab *slabp;\r\nunsigned objnr;\r\nslabp = page_get_slab(virt_to_head_page(objp));\r\nobjnr = (unsigned)(objp - slabp->s_mem) / cachep->buffer_size;\r\nslab_bufctl(slabp)[objnr] = BUFCTL_ACTIVE;\r\n}\r\n#endif\r\nobjp += obj_offset(cachep);\r\nif (cachep->ctor && cachep->flags & SLAB_POISON)\r\ncachep->ctor(objp);\r\nif (ARCH_SLAB_MINALIGN &&\r\n((unsigned long)objp & (ARCH_SLAB_MINALIGN-1))) {\r\nprintk(KERN_ERR "0x%p: not aligned to ARCH_SLAB_MINALIGN=%d\n",\r\nobjp, (int)ARCH_SLAB_MINALIGN);\r\n}\r\nreturn objp;\r\n}\r\nstatic bool slab_should_failslab(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nif (cachep == &cache_cache)\r\nreturn false;\r\nreturn should_failslab(obj_size(cachep), flags, cachep->flags);\r\n}\r\nstatic inline void *____cache_alloc(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nvoid *objp;\r\nstruct array_cache *ac;\r\ncheck_irq_off();\r\nac = cpu_cache_get(cachep);\r\nif (likely(ac->avail)) {\r\nSTATS_INC_ALLOCHIT(cachep);\r\nac->touched = 1;\r\nobjp = ac->entry[--ac->avail];\r\n} else {\r\nSTATS_INC_ALLOCMISS(cachep);\r\nobjp = cache_alloc_refill(cachep, flags);\r\nac = cpu_cache_get(cachep);\r\n}\r\nif (objp)\r\nkmemleak_erase(&ac->entry[ac->avail]);\r\nreturn objp;\r\n}\r\nstatic void *alternate_node_alloc(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nint nid_alloc, nid_here;\r\nif (in_interrupt() || (flags & __GFP_THISNODE))\r\nreturn NULL;\r\nnid_alloc = nid_here = numa_mem_id();\r\nif (cpuset_do_slab_mem_spread() && (cachep->flags & SLAB_MEM_SPREAD))\r\nnid_alloc = cpuset_slab_spread_node();\r\nelse if (current->mempolicy)\r\nnid_alloc = slab_node(current->mempolicy);\r\nif (nid_alloc != nid_here)\r\nreturn ____cache_alloc_node(cachep, flags, nid_alloc);\r\nreturn NULL;\r\n}\r\nstatic void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)\r\n{\r\nstruct zonelist *zonelist;\r\ngfp_t local_flags;\r\nstruct zoneref *z;\r\nstruct zone *zone;\r\nenum zone_type high_zoneidx = gfp_zone(flags);\r\nvoid *obj = NULL;\r\nint nid;\r\nunsigned int cpuset_mems_cookie;\r\nif (flags & __GFP_THISNODE)\r\nreturn NULL;\r\nlocal_flags = flags & (GFP_CONSTRAINT_MASK|GFP_RECLAIM_MASK);\r\nretry_cpuset:\r\ncpuset_mems_cookie = get_mems_allowed();\r\nzonelist = node_zonelist(slab_node(current->mempolicy), flags);\r\nretry:\r\nfor_each_zone_zonelist(zone, z, zonelist, high_zoneidx) {\r\nnid = zone_to_nid(zone);\r\nif (cpuset_zone_allowed_hardwall(zone, flags) &&\r\ncache->nodelists[nid] &&\r\ncache->nodelists[nid]->free_objects) {\r\nobj = ____cache_alloc_node(cache,\r\nflags | GFP_THISNODE, nid);\r\nif (obj)\r\nbreak;\r\n}\r\n}\r\nif (!obj) {\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_enable();\r\nkmem_flagcheck(cache, flags);\r\nobj = kmem_getpages(cache, local_flags, numa_mem_id());\r\nif (local_flags & __GFP_WAIT)\r\nlocal_irq_disable();\r\nif (obj) {\r\nnid = page_to_nid(virt_to_page(obj));\r\nif (cache_grow(cache, flags, nid, obj)) {\r\nobj = ____cache_alloc_node(cache,\r\nflags | GFP_THISNODE, nid);\r\nif (!obj)\r\ngoto retry;\r\n} else {\r\nobj = NULL;\r\n}\r\n}\r\n}\r\nif (unlikely(!put_mems_allowed(cpuset_mems_cookie) && !obj))\r\ngoto retry_cpuset;\r\nreturn obj;\r\n}\r\nstatic void *____cache_alloc_node(struct kmem_cache *cachep, gfp_t flags,\r\nint nodeid)\r\n{\r\nstruct list_head *entry;\r\nstruct slab *slabp;\r\nstruct kmem_list3 *l3;\r\nvoid *obj;\r\nint x;\r\nl3 = cachep->nodelists[nodeid];\r\nBUG_ON(!l3);\r\nretry:\r\ncheck_irq_off();\r\nspin_lock(&l3->list_lock);\r\nentry = l3->slabs_partial.next;\r\nif (entry == &l3->slabs_partial) {\r\nl3->free_touched = 1;\r\nentry = l3->slabs_free.next;\r\nif (entry == &l3->slabs_free)\r\ngoto must_grow;\r\n}\r\nslabp = list_entry(entry, struct slab, list);\r\ncheck_spinlock_acquired_node(cachep, nodeid);\r\ncheck_slabp(cachep, slabp);\r\nSTATS_INC_NODEALLOCS(cachep);\r\nSTATS_INC_ACTIVE(cachep);\r\nSTATS_SET_HIGH(cachep);\r\nBUG_ON(slabp->inuse == cachep->num);\r\nobj = slab_get_obj(cachep, slabp, nodeid);\r\ncheck_slabp(cachep, slabp);\r\nl3->free_objects--;\r\nlist_del(&slabp->list);\r\nif (slabp->free == BUFCTL_END)\r\nlist_add(&slabp->list, &l3->slabs_full);\r\nelse\r\nlist_add(&slabp->list, &l3->slabs_partial);\r\nspin_unlock(&l3->list_lock);\r\ngoto done;\r\nmust_grow:\r\nspin_unlock(&l3->list_lock);\r\nx = cache_grow(cachep, flags | GFP_THISNODE, nodeid, NULL);\r\nif (x)\r\ngoto retry;\r\nreturn fallback_alloc(cachep, flags);\r\ndone:\r\nreturn obj;\r\n}\r\nstatic __always_inline void *\r\n__cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid,\r\nvoid *caller)\r\n{\r\nunsigned long save_flags;\r\nvoid *ptr;\r\nint slab_node = numa_mem_id();\r\nflags &= gfp_allowed_mask;\r\nlockdep_trace_alloc(flags);\r\nif (slab_should_failslab(cachep, flags))\r\nreturn NULL;\r\ncache_alloc_debugcheck_before(cachep, flags);\r\nlocal_irq_save(save_flags);\r\nif (nodeid == NUMA_NO_NODE)\r\nnodeid = slab_node;\r\nif (unlikely(!cachep->nodelists[nodeid])) {\r\nptr = fallback_alloc(cachep, flags);\r\ngoto out;\r\n}\r\nif (nodeid == slab_node) {\r\nptr = ____cache_alloc(cachep, flags);\r\nif (ptr)\r\ngoto out;\r\n}\r\nptr = ____cache_alloc_node(cachep, flags, nodeid);\r\nout:\r\nlocal_irq_restore(save_flags);\r\nptr = cache_alloc_debugcheck_after(cachep, flags, ptr, caller);\r\nkmemleak_alloc_recursive(ptr, obj_size(cachep), 1, cachep->flags,\r\nflags);\r\nif (likely(ptr))\r\nkmemcheck_slab_alloc(cachep, flags, ptr, obj_size(cachep));\r\nif (unlikely((flags & __GFP_ZERO) && ptr))\r\nmemset(ptr, 0, obj_size(cachep));\r\nreturn ptr;\r\n}\r\nstatic __always_inline void *\r\n__do_cache_alloc(struct kmem_cache *cache, gfp_t flags)\r\n{\r\nvoid *objp;\r\nif (unlikely(current->flags & (PF_SPREAD_SLAB | PF_MEMPOLICY))) {\r\nobjp = alternate_node_alloc(cache, flags);\r\nif (objp)\r\ngoto out;\r\n}\r\nobjp = ____cache_alloc(cache, flags);\r\nif (!objp)\r\nobjp = ____cache_alloc_node(cache, flags, numa_mem_id());\r\nout:\r\nreturn objp;\r\n}\r\nstatic __always_inline void *\r\n__do_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nreturn ____cache_alloc(cachep, flags);\r\n}\r\nstatic __always_inline void *\r\n__cache_alloc(struct kmem_cache *cachep, gfp_t flags, void *caller)\r\n{\r\nunsigned long save_flags;\r\nvoid *objp;\r\nflags &= gfp_allowed_mask;\r\nlockdep_trace_alloc(flags);\r\nif (slab_should_failslab(cachep, flags))\r\nreturn NULL;\r\ncache_alloc_debugcheck_before(cachep, flags);\r\nlocal_irq_save(save_flags);\r\nobjp = __do_cache_alloc(cachep, flags);\r\nlocal_irq_restore(save_flags);\r\nobjp = cache_alloc_debugcheck_after(cachep, flags, objp, caller);\r\nkmemleak_alloc_recursive(objp, obj_size(cachep), 1, cachep->flags,\r\nflags);\r\nprefetchw(objp);\r\nif (likely(objp))\r\nkmemcheck_slab_alloc(cachep, flags, objp, obj_size(cachep));\r\nif (unlikely((flags & __GFP_ZERO) && objp))\r\nmemset(objp, 0, obj_size(cachep));\r\nreturn objp;\r\n}\r\nstatic void free_block(struct kmem_cache *cachep, void **objpp, int nr_objects,\r\nint node)\r\n{\r\nint i;\r\nstruct kmem_list3 *l3;\r\nfor (i = 0; i < nr_objects; i++) {\r\nvoid *objp = objpp[i];\r\nstruct slab *slabp;\r\nslabp = virt_to_slab(objp);\r\nl3 = cachep->nodelists[node];\r\nlist_del(&slabp->list);\r\ncheck_spinlock_acquired_node(cachep, node);\r\ncheck_slabp(cachep, slabp);\r\nslab_put_obj(cachep, slabp, objp, node);\r\nSTATS_DEC_ACTIVE(cachep);\r\nl3->free_objects++;\r\ncheck_slabp(cachep, slabp);\r\nif (slabp->inuse == 0) {\r\nif (l3->free_objects > l3->free_limit) {\r\nl3->free_objects -= cachep->num;\r\nslab_destroy(cachep, slabp);\r\n} else {\r\nlist_add(&slabp->list, &l3->slabs_free);\r\n}\r\n} else {\r\nlist_add_tail(&slabp->list, &l3->slabs_partial);\r\n}\r\n}\r\n}\r\nstatic void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac)\r\n{\r\nint batchcount;\r\nstruct kmem_list3 *l3;\r\nint node = numa_mem_id();\r\nbatchcount = ac->batchcount;\r\n#if DEBUG\r\nBUG_ON(!batchcount || batchcount > ac->avail);\r\n#endif\r\ncheck_irq_off();\r\nl3 = cachep->nodelists[node];\r\nspin_lock(&l3->list_lock);\r\nif (l3->shared) {\r\nstruct array_cache *shared_array = l3->shared;\r\nint max = shared_array->limit - shared_array->avail;\r\nif (max) {\r\nif (batchcount > max)\r\nbatchcount = max;\r\nmemcpy(&(shared_array->entry[shared_array->avail]),\r\nac->entry, sizeof(void *) * batchcount);\r\nshared_array->avail += batchcount;\r\ngoto free_done;\r\n}\r\n}\r\nfree_block(cachep, ac->entry, batchcount, node);\r\nfree_done:\r\n#if STATS\r\n{\r\nint i = 0;\r\nstruct list_head *p;\r\np = l3->slabs_free.next;\r\nwhile (p != &(l3->slabs_free)) {\r\nstruct slab *slabp;\r\nslabp = list_entry(p, struct slab, list);\r\nBUG_ON(slabp->inuse);\r\ni++;\r\np = p->next;\r\n}\r\nSTATS_SET_FREEABLE(cachep, i);\r\n}\r\n#endif\r\nspin_unlock(&l3->list_lock);\r\nac->avail -= batchcount;\r\nmemmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);\r\n}\r\nstatic inline void __cache_free(struct kmem_cache *cachep, void *objp,\r\nvoid *caller)\r\n{\r\nstruct array_cache *ac = cpu_cache_get(cachep);\r\ncheck_irq_off();\r\nkmemleak_free_recursive(objp, cachep->flags);\r\nobjp = cache_free_debugcheck(cachep, objp, caller);\r\nkmemcheck_slab_free(cachep, objp, obj_size(cachep));\r\nif (nr_online_nodes > 1 && cache_free_alien(cachep, objp))\r\nreturn;\r\nif (likely(ac->avail < ac->limit)) {\r\nSTATS_INC_FREEHIT(cachep);\r\n} else {\r\nSTATS_INC_FREEMISS(cachep);\r\ncache_flusharray(cachep, ac);\r\n}\r\nac->entry[ac->avail++] = objp;\r\n}\r\nvoid *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nvoid *ret = __cache_alloc(cachep, flags, __builtin_return_address(0));\r\ntrace_kmem_cache_alloc(_RET_IP_, ret,\r\nobj_size(cachep), cachep->buffer_size, flags);\r\nreturn ret;\r\n}\r\nvoid *\r\nkmem_cache_alloc_trace(size_t size, struct kmem_cache *cachep, gfp_t flags)\r\n{\r\nvoid *ret;\r\nret = __cache_alloc(cachep, flags, __builtin_return_address(0));\r\ntrace_kmalloc(_RET_IP_, ret,\r\nsize, slab_buffer_size(cachep), flags);\r\nreturn ret;\r\n}\r\nvoid *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)\r\n{\r\nvoid *ret = __cache_alloc_node(cachep, flags, nodeid,\r\n__builtin_return_address(0));\r\ntrace_kmem_cache_alloc_node(_RET_IP_, ret,\r\nobj_size(cachep), cachep->buffer_size,\r\nflags, nodeid);\r\nreturn ret;\r\n}\r\nvoid *kmem_cache_alloc_node_trace(size_t size,\r\nstruct kmem_cache *cachep,\r\ngfp_t flags,\r\nint nodeid)\r\n{\r\nvoid *ret;\r\nret = __cache_alloc_node(cachep, flags, nodeid,\r\n__builtin_return_address(0));\r\ntrace_kmalloc_node(_RET_IP_, ret,\r\nsize, slab_buffer_size(cachep),\r\nflags, nodeid);\r\nreturn ret;\r\n}\r\nstatic __always_inline void *\r\n__do_kmalloc_node(size_t size, gfp_t flags, int node, void *caller)\r\n{\r\nstruct kmem_cache *cachep;\r\ncachep = kmem_find_general_cachep(size, flags);\r\nif (unlikely(ZERO_OR_NULL_PTR(cachep)))\r\nreturn cachep;\r\nreturn kmem_cache_alloc_node_trace(size, cachep, flags, node);\r\n}\r\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node)\r\n{\r\nreturn __do_kmalloc_node(size, flags, node,\r\n__builtin_return_address(0));\r\n}\r\nvoid *__kmalloc_node_track_caller(size_t size, gfp_t flags,\r\nint node, unsigned long caller)\r\n{\r\nreturn __do_kmalloc_node(size, flags, node, (void *)caller);\r\n}\r\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node)\r\n{\r\nreturn __do_kmalloc_node(size, flags, node, NULL);\r\n}\r\nstatic __always_inline void *__do_kmalloc(size_t size, gfp_t flags,\r\nvoid *caller)\r\n{\r\nstruct kmem_cache *cachep;\r\nvoid *ret;\r\ncachep = __find_general_cachep(size, flags);\r\nif (unlikely(ZERO_OR_NULL_PTR(cachep)))\r\nreturn cachep;\r\nret = __cache_alloc(cachep, flags, caller);\r\ntrace_kmalloc((unsigned long) caller, ret,\r\nsize, cachep->buffer_size, flags);\r\nreturn ret;\r\n}\r\nvoid *__kmalloc(size_t size, gfp_t flags)\r\n{\r\nreturn __do_kmalloc(size, flags, __builtin_return_address(0));\r\n}\r\nvoid *__kmalloc_track_caller(size_t size, gfp_t flags, unsigned long caller)\r\n{\r\nreturn __do_kmalloc(size, flags, (void *)caller);\r\n}\r\nvoid *__kmalloc(size_t size, gfp_t flags)\r\n{\r\nreturn __do_kmalloc(size, flags, NULL);\r\n}\r\nvoid kmem_cache_free(struct kmem_cache *cachep, void *objp)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\ndebug_check_no_locks_freed(objp, obj_size(cachep));\r\nif (!(cachep->flags & SLAB_DEBUG_OBJECTS))\r\ndebug_check_no_obj_freed(objp, obj_size(cachep));\r\n__cache_free(cachep, objp, __builtin_return_address(0));\r\nlocal_irq_restore(flags);\r\ntrace_kmem_cache_free(_RET_IP_, objp);\r\n}\r\nvoid kfree(const void *objp)\r\n{\r\nstruct kmem_cache *c;\r\nunsigned long flags;\r\ntrace_kfree(_RET_IP_, objp);\r\nif (unlikely(ZERO_OR_NULL_PTR(objp)))\r\nreturn;\r\nlocal_irq_save(flags);\r\nkfree_debugcheck(objp);\r\nc = virt_to_cache(objp);\r\ndebug_check_no_locks_freed(objp, obj_size(c));\r\ndebug_check_no_obj_freed(objp, obj_size(c));\r\n__cache_free(c, (void *)objp, __builtin_return_address(0));\r\nlocal_irq_restore(flags);\r\n}\r\nunsigned int kmem_cache_size(struct kmem_cache *cachep)\r\n{\r\nreturn obj_size(cachep);\r\n}\r\nstatic int alloc_kmemlist(struct kmem_cache *cachep, gfp_t gfp)\r\n{\r\nint node;\r\nstruct kmem_list3 *l3;\r\nstruct array_cache *new_shared;\r\nstruct array_cache **new_alien = NULL;\r\nfor_each_online_node(node) {\r\nif (use_alien_caches) {\r\nnew_alien = alloc_alien_cache(node, cachep->limit, gfp);\r\nif (!new_alien)\r\ngoto fail;\r\n}\r\nnew_shared = NULL;\r\nif (cachep->shared) {\r\nnew_shared = alloc_arraycache(node,\r\ncachep->shared*cachep->batchcount,\r\n0xbaadf00d, gfp);\r\nif (!new_shared) {\r\nfree_alien_cache(new_alien);\r\ngoto fail;\r\n}\r\n}\r\nl3 = cachep->nodelists[node];\r\nif (l3) {\r\nstruct array_cache *shared = l3->shared;\r\nspin_lock_irq(&l3->list_lock);\r\nif (shared)\r\nfree_block(cachep, shared->entry,\r\nshared->avail, node);\r\nl3->shared = new_shared;\r\nif (!l3->alien) {\r\nl3->alien = new_alien;\r\nnew_alien = NULL;\r\n}\r\nl3->free_limit = (1 + nr_cpus_node(node)) *\r\ncachep->batchcount + cachep->num;\r\nspin_unlock_irq(&l3->list_lock);\r\nkfree(shared);\r\nfree_alien_cache(new_alien);\r\ncontinue;\r\n}\r\nl3 = kmalloc_node(sizeof(struct kmem_list3), gfp, node);\r\nif (!l3) {\r\nfree_alien_cache(new_alien);\r\nkfree(new_shared);\r\ngoto fail;\r\n}\r\nkmem_list3_init(l3);\r\nl3->next_reap = jiffies + REAPTIMEOUT_LIST3 +\r\n((unsigned long)cachep) % REAPTIMEOUT_LIST3;\r\nl3->shared = new_shared;\r\nl3->alien = new_alien;\r\nl3->free_limit = (1 + nr_cpus_node(node)) *\r\ncachep->batchcount + cachep->num;\r\ncachep->nodelists[node] = l3;\r\n}\r\nreturn 0;\r\nfail:\r\nif (!cachep->next.next) {\r\nnode--;\r\nwhile (node >= 0) {\r\nif (cachep->nodelists[node]) {\r\nl3 = cachep->nodelists[node];\r\nkfree(l3->shared);\r\nfree_alien_cache(l3->alien);\r\nkfree(l3);\r\ncachep->nodelists[node] = NULL;\r\n}\r\nnode--;\r\n}\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic void do_ccupdate_local(void *info)\r\n{\r\nstruct ccupdate_struct *new = info;\r\nstruct array_cache *old;\r\ncheck_irq_off();\r\nold = cpu_cache_get(new->cachep);\r\nnew->cachep->array[smp_processor_id()] = new->new[smp_processor_id()];\r\nnew->new[smp_processor_id()] = old;\r\n}\r\nstatic int do_tune_cpucache(struct kmem_cache *cachep, int limit,\r\nint batchcount, int shared, gfp_t gfp)\r\n{\r\nstruct ccupdate_struct *new;\r\nint i;\r\nnew = kzalloc(sizeof(*new) + nr_cpu_ids * sizeof(struct array_cache *),\r\ngfp);\r\nif (!new)\r\nreturn -ENOMEM;\r\nfor_each_online_cpu(i) {\r\nnew->new[i] = alloc_arraycache(cpu_to_mem(i), limit,\r\nbatchcount, gfp);\r\nif (!new->new[i]) {\r\nfor (i--; i >= 0; i--)\r\nkfree(new->new[i]);\r\nkfree(new);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nnew->cachep = cachep;\r\non_each_cpu(do_ccupdate_local, (void *)new, 1);\r\ncheck_irq_on();\r\ncachep->batchcount = batchcount;\r\ncachep->limit = limit;\r\ncachep->shared = shared;\r\nfor_each_online_cpu(i) {\r\nstruct array_cache *ccold = new->new[i];\r\nif (!ccold)\r\ncontinue;\r\nspin_lock_irq(&cachep->nodelists[cpu_to_mem(i)]->list_lock);\r\nfree_block(cachep, ccold->entry, ccold->avail, cpu_to_mem(i));\r\nspin_unlock_irq(&cachep->nodelists[cpu_to_mem(i)]->list_lock);\r\nkfree(ccold);\r\n}\r\nkfree(new);\r\nreturn alloc_kmemlist(cachep, gfp);\r\n}\r\nstatic int enable_cpucache(struct kmem_cache *cachep, gfp_t gfp)\r\n{\r\nint err;\r\nint limit, shared;\r\nif (cachep->buffer_size > 131072)\r\nlimit = 1;\r\nelse if (cachep->buffer_size > PAGE_SIZE)\r\nlimit = 8;\r\nelse if (cachep->buffer_size > 1024)\r\nlimit = 24;\r\nelse if (cachep->buffer_size > 256)\r\nlimit = 54;\r\nelse\r\nlimit = 120;\r\nshared = 0;\r\nif (cachep->buffer_size <= PAGE_SIZE && num_possible_cpus() > 1)\r\nshared = 8;\r\n#if DEBUG\r\nif (limit > 32)\r\nlimit = 32;\r\n#endif\r\nerr = do_tune_cpucache(cachep, limit, (limit + 1) / 2, shared, gfp);\r\nif (err)\r\nprintk(KERN_ERR "enable_cpucache failed for %s, error %d.\n",\r\ncachep->name, -err);\r\nreturn err;\r\n}\r\nstatic void drain_array(struct kmem_cache *cachep, struct kmem_list3 *l3,\r\nstruct array_cache *ac, int force, int node)\r\n{\r\nint tofree;\r\nif (!ac || !ac->avail)\r\nreturn;\r\nif (ac->touched && !force) {\r\nac->touched = 0;\r\n} else {\r\nspin_lock_irq(&l3->list_lock);\r\nif (ac->avail) {\r\ntofree = force ? ac->avail : (ac->limit + 4) / 5;\r\nif (tofree > ac->avail)\r\ntofree = (ac->avail + 1) / 2;\r\nfree_block(cachep, ac->entry, tofree, node);\r\nac->avail -= tofree;\r\nmemmove(ac->entry, &(ac->entry[tofree]),\r\nsizeof(void *) * ac->avail);\r\n}\r\nspin_unlock_irq(&l3->list_lock);\r\n}\r\n}\r\nstatic void cache_reap(struct work_struct *w)\r\n{\r\nstruct kmem_cache *searchp;\r\nstruct kmem_list3 *l3;\r\nint node = numa_mem_id();\r\nstruct delayed_work *work = to_delayed_work(w);\r\nif (!mutex_trylock(&cache_chain_mutex))\r\ngoto out;\r\nlist_for_each_entry(searchp, &cache_chain, next) {\r\ncheck_irq_on();\r\nl3 = searchp->nodelists[node];\r\nreap_alien(searchp, l3);\r\ndrain_array(searchp, l3, cpu_cache_get(searchp), 0, node);\r\nif (time_after(l3->next_reap, jiffies))\r\ngoto next;\r\nl3->next_reap = jiffies + REAPTIMEOUT_LIST3;\r\ndrain_array(searchp, l3, l3->shared, 0, node);\r\nif (l3->free_touched)\r\nl3->free_touched = 0;\r\nelse {\r\nint freed;\r\nfreed = drain_freelist(searchp, l3, (l3->free_limit +\r\n5 * searchp->num - 1) / (5 * searchp->num));\r\nSTATS_ADD_REAPED(searchp, freed);\r\n}\r\nnext:\r\ncond_resched();\r\n}\r\ncheck_irq_on();\r\nmutex_unlock(&cache_chain_mutex);\r\nnext_reap_node();\r\nout:\r\nschedule_delayed_work(work, round_jiffies_relative(REAPTIMEOUT_CPUC));\r\n}\r\nstatic void print_slabinfo_header(struct seq_file *m)\r\n{\r\n#if STATS\r\nseq_puts(m, "slabinfo - version: 2.1 (statistics)\n");\r\n#else\r\nseq_puts(m, "slabinfo - version: 2.1\n");\r\n#endif\r\nseq_puts(m, "# name <active_objs> <num_objs> <objsize> "\r\n"<objperslab> <pagesperslab>");\r\nseq_puts(m, " : tunables <limit> <batchcount> <sharedfactor>");\r\nseq_puts(m, " : slabdata <active_slabs> <num_slabs> <sharedavail>");\r\n#if STATS\r\nseq_puts(m, " : globalstat <listallocs> <maxobjs> <grown> <reaped> "\r\n"<error> <maxfreeable> <nodeallocs> <remotefrees> <alienoverflow>");\r\nseq_puts(m, " : cpustat <allochit> <allocmiss> <freehit> <freemiss>");\r\n#endif\r\nseq_putc(m, '\n');\r\n}\r\nstatic void *s_start(struct seq_file *m, loff_t *pos)\r\n{\r\nloff_t n = *pos;\r\nmutex_lock(&cache_chain_mutex);\r\nif (!n)\r\nprint_slabinfo_header(m);\r\nreturn seq_list_start(&cache_chain, *pos);\r\n}\r\nstatic void *s_next(struct seq_file *m, void *p, loff_t *pos)\r\n{\r\nreturn seq_list_next(p, &cache_chain, pos);\r\n}\r\nstatic void s_stop(struct seq_file *m, void *p)\r\n{\r\nmutex_unlock(&cache_chain_mutex);\r\n}\r\nstatic int s_show(struct seq_file *m, void *p)\r\n{\r\nstruct kmem_cache *cachep = list_entry(p, struct kmem_cache, next);\r\nstruct slab *slabp;\r\nunsigned long active_objs;\r\nunsigned long num_objs;\r\nunsigned long active_slabs = 0;\r\nunsigned long num_slabs, free_objects = 0, shared_avail = 0;\r\nconst char *name;\r\nchar *error = NULL;\r\nint node;\r\nstruct kmem_list3 *l3;\r\nactive_objs = 0;\r\nnum_slabs = 0;\r\nfor_each_online_node(node) {\r\nl3 = cachep->nodelists[node];\r\nif (!l3)\r\ncontinue;\r\ncheck_irq_on();\r\nspin_lock_irq(&l3->list_lock);\r\nlist_for_each_entry(slabp, &l3->slabs_full, list) {\r\nif (slabp->inuse != cachep->num && !error)\r\nerror = "slabs_full accounting error";\r\nactive_objs += cachep->num;\r\nactive_slabs++;\r\n}\r\nlist_for_each_entry(slabp, &l3->slabs_partial, list) {\r\nif (slabp->inuse == cachep->num && !error)\r\nerror = "slabs_partial inuse accounting error";\r\nif (!slabp->inuse && !error)\r\nerror = "slabs_partial/inuse accounting error";\r\nactive_objs += slabp->inuse;\r\nactive_slabs++;\r\n}\r\nlist_for_each_entry(slabp, &l3->slabs_free, list) {\r\nif (slabp->inuse && !error)\r\nerror = "slabs_free/inuse accounting error";\r\nnum_slabs++;\r\n}\r\nfree_objects += l3->free_objects;\r\nif (l3->shared)\r\nshared_avail += l3->shared->avail;\r\nspin_unlock_irq(&l3->list_lock);\r\n}\r\nnum_slabs += active_slabs;\r\nnum_objs = num_slabs * cachep->num;\r\nif (num_objs - active_objs != free_objects && !error)\r\nerror = "free_objects accounting error";\r\nname = cachep->name;\r\nif (error)\r\nprintk(KERN_ERR "slab: cache %s error: %s\n", name, error);\r\nseq_printf(m, "%-17s %6lu %6lu %6u %4u %4d",\r\nname, active_objs, num_objs, cachep->buffer_size,\r\ncachep->num, (1 << cachep->gfporder));\r\nseq_printf(m, " : tunables %4u %4u %4u",\r\ncachep->limit, cachep->batchcount, cachep->shared);\r\nseq_printf(m, " : slabdata %6lu %6lu %6lu",\r\nactive_slabs, num_slabs, shared_avail);\r\n#if STATS\r\n{\r\nunsigned long high = cachep->high_mark;\r\nunsigned long allocs = cachep->num_allocations;\r\nunsigned long grown = cachep->grown;\r\nunsigned long reaped = cachep->reaped;\r\nunsigned long errors = cachep->errors;\r\nunsigned long max_freeable = cachep->max_freeable;\r\nunsigned long node_allocs = cachep->node_allocs;\r\nunsigned long node_frees = cachep->node_frees;\r\nunsigned long overflows = cachep->node_overflow;\r\nseq_printf(m, " : globalstat %7lu %6lu %5lu %4lu "\r\n"%4lu %4lu %4lu %4lu %4lu",\r\nallocs, high, grown,\r\nreaped, errors, max_freeable, node_allocs,\r\nnode_frees, overflows);\r\n}\r\n{\r\nunsigned long allochit = atomic_read(&cachep->allochit);\r\nunsigned long allocmiss = atomic_read(&cachep->allocmiss);\r\nunsigned long freehit = atomic_read(&cachep->freehit);\r\nunsigned long freemiss = atomic_read(&cachep->freemiss);\r\nseq_printf(m, " : cpustat %6lu %6lu %6lu %6lu",\r\nallochit, allocmiss, freehit, freemiss);\r\n}\r\n#endif\r\nseq_putc(m, '\n');\r\nreturn 0;\r\n}\r\nstatic ssize_t slabinfo_write(struct file *file, const char __user *buffer,\r\nsize_t count, loff_t *ppos)\r\n{\r\nchar kbuf[MAX_SLABINFO_WRITE + 1], *tmp;\r\nint limit, batchcount, shared, res;\r\nstruct kmem_cache *cachep;\r\nif (count > MAX_SLABINFO_WRITE)\r\nreturn -EINVAL;\r\nif (copy_from_user(&kbuf, buffer, count))\r\nreturn -EFAULT;\r\nkbuf[MAX_SLABINFO_WRITE] = '\0';\r\ntmp = strchr(kbuf, ' ');\r\nif (!tmp)\r\nreturn -EINVAL;\r\n*tmp = '\0';\r\ntmp++;\r\nif (sscanf(tmp, " %d %d %d", &limit, &batchcount, &shared) != 3)\r\nreturn -EINVAL;\r\nmutex_lock(&cache_chain_mutex);\r\nres = -EINVAL;\r\nlist_for_each_entry(cachep, &cache_chain, next) {\r\nif (!strcmp(cachep->name, kbuf)) {\r\nif (limit < 1 || batchcount < 1 ||\r\nbatchcount > limit || shared < 0) {\r\nres = 0;\r\n} else {\r\nres = do_tune_cpucache(cachep, limit,\r\nbatchcount, shared,\r\nGFP_KERNEL);\r\n}\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&cache_chain_mutex);\r\nif (res >= 0)\r\nres = count;\r\nreturn res;\r\n}\r\nstatic int slabinfo_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open(file, &slabinfo_op);\r\n}\r\nstatic void *leaks_start(struct seq_file *m, loff_t *pos)\r\n{\r\nmutex_lock(&cache_chain_mutex);\r\nreturn seq_list_start(&cache_chain, *pos);\r\n}\r\nstatic inline int add_caller(unsigned long *n, unsigned long v)\r\n{\r\nunsigned long *p;\r\nint l;\r\nif (!v)\r\nreturn 1;\r\nl = n[1];\r\np = n + 2;\r\nwhile (l) {\r\nint i = l/2;\r\nunsigned long *q = p + 2 * i;\r\nif (*q == v) {\r\nq[1]++;\r\nreturn 1;\r\n}\r\nif (*q > v) {\r\nl = i;\r\n} else {\r\np = q + 2;\r\nl -= i + 1;\r\n}\r\n}\r\nif (++n[1] == n[0])\r\nreturn 0;\r\nmemmove(p + 2, p, n[1] * 2 * sizeof(unsigned long) - ((void *)p - (void *)n));\r\np[0] = v;\r\np[1] = 1;\r\nreturn 1;\r\n}\r\nstatic void handle_slab(unsigned long *n, struct kmem_cache *c, struct slab *s)\r\n{\r\nvoid *p;\r\nint i;\r\nif (n[0] == n[1])\r\nreturn;\r\nfor (i = 0, p = s->s_mem; i < c->num; i++, p += c->buffer_size) {\r\nif (slab_bufctl(s)[i] != BUFCTL_ACTIVE)\r\ncontinue;\r\nif (!add_caller(n, (unsigned long)*dbg_userword(c, p)))\r\nreturn;\r\n}\r\n}\r\nstatic void show_symbol(struct seq_file *m, unsigned long address)\r\n{\r\n#ifdef CONFIG_KALLSYMS\r\nunsigned long offset, size;\r\nchar modname[MODULE_NAME_LEN], name[KSYM_NAME_LEN];\r\nif (lookup_symbol_attrs(address, &size, &offset, modname, name) == 0) {\r\nseq_printf(m, "%s+%#lx/%#lx", name, offset, size);\r\nif (modname[0])\r\nseq_printf(m, " [%s]", modname);\r\nreturn;\r\n}\r\n#endif\r\nseq_printf(m, "%p", (void *)address);\r\n}\r\nstatic int leaks_show(struct seq_file *m, void *p)\r\n{\r\nstruct kmem_cache *cachep = list_entry(p, struct kmem_cache, next);\r\nstruct slab *slabp;\r\nstruct kmem_list3 *l3;\r\nconst char *name;\r\nunsigned long *n = m->private;\r\nint node;\r\nint i;\r\nif (!(cachep->flags & SLAB_STORE_USER))\r\nreturn 0;\r\nif (!(cachep->flags & SLAB_RED_ZONE))\r\nreturn 0;\r\nn[1] = 0;\r\nfor_each_online_node(node) {\r\nl3 = cachep->nodelists[node];\r\nif (!l3)\r\ncontinue;\r\ncheck_irq_on();\r\nspin_lock_irq(&l3->list_lock);\r\nlist_for_each_entry(slabp, &l3->slabs_full, list)\r\nhandle_slab(n, cachep, slabp);\r\nlist_for_each_entry(slabp, &l3->slabs_partial, list)\r\nhandle_slab(n, cachep, slabp);\r\nspin_unlock_irq(&l3->list_lock);\r\n}\r\nname = cachep->name;\r\nif (n[0] == n[1]) {\r\nmutex_unlock(&cache_chain_mutex);\r\nm->private = kzalloc(n[0] * 4 * sizeof(unsigned long), GFP_KERNEL);\r\nif (!m->private) {\r\nm->private = n;\r\nmutex_lock(&cache_chain_mutex);\r\nreturn -ENOMEM;\r\n}\r\n*(unsigned long *)m->private = n[0] * 2;\r\nkfree(n);\r\nmutex_lock(&cache_chain_mutex);\r\nm->count = m->size;\r\nreturn 0;\r\n}\r\nfor (i = 0; i < n[1]; i++) {\r\nseq_printf(m, "%s: %lu ", name, n[2*i+3]);\r\nshow_symbol(m, n[2*i+2]);\r\nseq_putc(m, '\n');\r\n}\r\nreturn 0;\r\n}\r\nstatic int slabstats_open(struct inode *inode, struct file *file)\r\n{\r\nunsigned long *n = kzalloc(PAGE_SIZE, GFP_KERNEL);\r\nint ret = -ENOMEM;\r\nif (n) {\r\nret = seq_open(file, &slabstats_op);\r\nif (!ret) {\r\nstruct seq_file *m = file->private_data;\r\n*n = PAGE_SIZE / (2 * sizeof(unsigned long));\r\nm->private = n;\r\nn = NULL;\r\n}\r\nkfree(n);\r\n}\r\nreturn ret;\r\n}\r\nstatic int __init slab_proc_init(void)\r\n{\r\nproc_create("slabinfo",S_IWUSR|S_IRUSR,NULL,&proc_slabinfo_operations);\r\n#ifdef CONFIG_DEBUG_SLAB_LEAK\r\nproc_create("slab_allocators", 0, NULL, &proc_slabstats_operations);\r\n#endif\r\nreturn 0;\r\n}\r\nsize_t ksize(const void *objp)\r\n{\r\nBUG_ON(!objp);\r\nif (unlikely(objp == ZERO_SIZE_PTR))\r\nreturn 0;\r\nreturn obj_size(virt_to_cache(objp));\r\n}
