static int do_adjust_pte(struct vm_area_struct *vma, unsigned long address,\r\nunsigned long pfn, pte_t *ptep)\r\n{\r\npte_t entry = *ptep;\r\nint ret;\r\nret = pte_present(entry);\r\nif (ret && (pte_val(entry) & L_PTE_MT_MASK) != shared_pte_mask) {\r\nflush_cache_page(vma, address, pfn);\r\nouter_flush_range((pfn << PAGE_SHIFT),\r\n(pfn << PAGE_SHIFT) + PAGE_SIZE);\r\npte_val(entry) &= ~L_PTE_MT_MASK;\r\npte_val(entry) |= shared_pte_mask;\r\nset_pte_at(vma->vm_mm, address, ptep, entry);\r\nflush_tlb_page(vma, address);\r\n}\r\nreturn ret;\r\n}\r\nstatic inline void do_pte_lock(spinlock_t *ptl)\r\n{\r\nspin_lock_nested(ptl, SINGLE_DEPTH_NESTING);\r\n}\r\nstatic inline void do_pte_unlock(spinlock_t *ptl)\r\n{\r\nspin_unlock(ptl);\r\n}\r\nstatic inline void do_pte_lock(spinlock_t *ptl) {}\r\nstatic inline void do_pte_unlock(spinlock_t *ptl) {}\r\nstatic int adjust_pte(struct vm_area_struct *vma, unsigned long address,\r\nunsigned long pfn)\r\n{\r\nspinlock_t *ptl;\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nint ret;\r\npgd = pgd_offset(vma->vm_mm, address);\r\nif (pgd_none_or_clear_bad(pgd))\r\nreturn 0;\r\npud = pud_offset(pgd, address);\r\nif (pud_none_or_clear_bad(pud))\r\nreturn 0;\r\npmd = pmd_offset(pud, address);\r\nif (pmd_none_or_clear_bad(pmd))\r\nreturn 0;\r\nptl = pte_lockptr(vma->vm_mm, pmd);\r\npte = pte_offset_map(pmd, address);\r\ndo_pte_lock(ptl);\r\nret = do_adjust_pte(vma, address, pfn, pte);\r\ndo_pte_unlock(ptl);\r\npte_unmap(pte);\r\nreturn ret;\r\n}\r\nstatic void\r\nmake_coherent(struct address_space *mapping, struct vm_area_struct *vma,\r\nunsigned long addr, pte_t *ptep, unsigned long pfn)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nstruct vm_area_struct *mpnt;\r\nstruct prio_tree_iter iter;\r\nunsigned long offset;\r\npgoff_t pgoff;\r\nint aliases = 0;\r\npgoff = vma->vm_pgoff + ((addr - vma->vm_start) >> PAGE_SHIFT);\r\nflush_dcache_mmap_lock(mapping);\r\nvma_prio_tree_foreach(mpnt, &iter, &mapping->i_mmap, pgoff, pgoff) {\r\nif (mpnt->vm_mm != mm || mpnt == vma)\r\ncontinue;\r\nif (!(mpnt->vm_flags & VM_MAYSHARE))\r\ncontinue;\r\noffset = (pgoff - mpnt->vm_pgoff) << PAGE_SHIFT;\r\naliases += adjust_pte(mpnt, mpnt->vm_start + offset, pfn);\r\n}\r\nflush_dcache_mmap_unlock(mapping);\r\nif (aliases)\r\ndo_adjust_pte(vma, addr, pfn, ptep);\r\n}\r\nvoid update_mmu_cache(struct vm_area_struct *vma, unsigned long addr,\r\npte_t *ptep)\r\n{\r\nunsigned long pfn = pte_pfn(*ptep);\r\nstruct address_space *mapping;\r\nstruct page *page;\r\nif (!pfn_valid(pfn))\r\nreturn;\r\npage = pfn_to_page(pfn);\r\nif (page == ZERO_PAGE(0))\r\nreturn;\r\nmapping = page_mapping(page);\r\nif (!test_and_set_bit(PG_dcache_clean, &page->flags))\r\n__flush_dcache_page(mapping, page);\r\nif (mapping) {\r\nif (cache_is_vivt())\r\nmake_coherent(mapping, vma, addr, ptep, pfn);\r\nelse if (vma->vm_flags & VM_EXEC)\r\n__flush_icache_all();\r\n}\r\n}\r\nstatic int __init check_writebuffer(unsigned long *p1, unsigned long *p2)\r\n{\r\nregister unsigned long zero = 0, one = 1, val;\r\nlocal_irq_disable();\r\nmb();\r\n*p1 = one;\r\nmb();\r\n*p2 = zero;\r\nmb();\r\nval = *p1;\r\nmb();\r\nlocal_irq_enable();\r\nreturn val != zero;\r\n}\r\nvoid __init check_writebuffer_bugs(void)\r\n{\r\nstruct page *page;\r\nconst char *reason;\r\nunsigned long v = 1;\r\nprintk(KERN_INFO "CPU: Testing write buffer coherency: ");\r\npage = alloc_page(GFP_KERNEL);\r\nif (page) {\r\nunsigned long *p1, *p2;\r\npgprot_t prot = __pgprot_modify(PAGE_KERNEL,\r\nL_PTE_MT_MASK, L_PTE_MT_BUFFERABLE);\r\np1 = vmap(&page, 1, VM_IOREMAP, prot);\r\np2 = vmap(&page, 1, VM_IOREMAP, prot);\r\nif (p1 && p2) {\r\nv = check_writebuffer(p1, p2);\r\nreason = "enabling work-around";\r\n} else {\r\nreason = "unable to map memory\n";\r\n}\r\nvunmap(p1);\r\nvunmap(p2);\r\nput_page(page);\r\n} else {\r\nreason = "unable to grab page\n";\r\n}\r\nif (v) {\r\nprintk("failed, %s\n", reason);\r\nshared_pte_mask = L_PTE_MT_UNCACHED;\r\n} else {\r\nprintk("ok\n");\r\n}\r\n}
