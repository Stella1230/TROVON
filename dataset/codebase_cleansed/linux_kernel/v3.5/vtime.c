static inline __u64 get_vtimer(void)\r\n{\r\n__u64 timer;\r\nasm volatile("STPT %0" : "=m" (timer));\r\nreturn timer;\r\n}\r\nstatic inline void set_vtimer(__u64 expires)\r\n{\r\n__u64 timer;\r\nasm volatile (" STPT %0\n"\r\n" SPT %1"\r\n: "=m" (timer) : "m" (expires) );\r\nS390_lowcore.system_timer += S390_lowcore.last_update_timer - timer;\r\nS390_lowcore.last_update_timer = expires;\r\n}\r\nstatic void do_account_vtime(struct task_struct *tsk, int hardirq_offset)\r\n{\r\nstruct thread_info *ti = task_thread_info(tsk);\r\n__u64 timer, clock, user, system, steal;\r\ntimer = S390_lowcore.last_update_timer;\r\nclock = S390_lowcore.last_update_clock;\r\nasm volatile (" STPT %0\n"\r\n" STCK %1"\r\n: "=m" (S390_lowcore.last_update_timer),\r\n"=m" (S390_lowcore.last_update_clock) );\r\nS390_lowcore.system_timer += timer - S390_lowcore.last_update_timer;\r\nS390_lowcore.steal_timer += S390_lowcore.last_update_clock - clock;\r\nuser = S390_lowcore.user_timer - ti->user_timer;\r\nS390_lowcore.steal_timer -= user;\r\nti->user_timer = S390_lowcore.user_timer;\r\naccount_user_time(tsk, user, user);\r\nsystem = S390_lowcore.system_timer - ti->system_timer;\r\nS390_lowcore.steal_timer -= system;\r\nti->system_timer = S390_lowcore.system_timer;\r\naccount_system_time(tsk, hardirq_offset, system, system);\r\nsteal = S390_lowcore.steal_timer;\r\nif ((s64) steal > 0) {\r\nS390_lowcore.steal_timer = 0;\r\naccount_steal_time(steal);\r\n}\r\n}\r\nvoid account_vtime(struct task_struct *prev, struct task_struct *next)\r\n{\r\nstruct thread_info *ti;\r\ndo_account_vtime(prev, 0);\r\nti = task_thread_info(prev);\r\nti->user_timer = S390_lowcore.user_timer;\r\nti->system_timer = S390_lowcore.system_timer;\r\nti = task_thread_info(next);\r\nS390_lowcore.user_timer = ti->user_timer;\r\nS390_lowcore.system_timer = ti->system_timer;\r\n}\r\nvoid account_process_tick(struct task_struct *tsk, int user_tick)\r\n{\r\ndo_account_vtime(tsk, HARDIRQ_OFFSET);\r\n}\r\nvoid account_system_vtime(struct task_struct *tsk)\r\n{\r\nstruct thread_info *ti = task_thread_info(tsk);\r\n__u64 timer, system;\r\ntimer = S390_lowcore.last_update_timer;\r\nS390_lowcore.last_update_timer = get_vtimer();\r\nS390_lowcore.system_timer += timer - S390_lowcore.last_update_timer;\r\nsystem = S390_lowcore.system_timer - ti->system_timer;\r\nS390_lowcore.steal_timer -= system;\r\nti->system_timer = S390_lowcore.system_timer;\r\naccount_system_time(tsk, 0, system, system);\r\n}\r\nvoid __kprobes vtime_stop_cpu(void)\r\n{\r\nstruct s390_idle_data *idle = &__get_cpu_var(s390_idle);\r\nstruct vtimer_queue *vq = &__get_cpu_var(virt_cpu_timer);\r\nunsigned long long idle_time;\r\nunsigned long psw_mask;\r\ntrace_hardirqs_on();\r\nstop_critical_timings();\r\npsw_mask = psw_kernel_bits | PSW_MASK_WAIT | PSW_MASK_DAT |\r\nPSW_MASK_IO | PSW_MASK_EXT | PSW_MASK_MCHECK;\r\nidle->nohz_delay = 0;\r\npsw_idle(idle, vq, psw_mask, !list_empty(&vq->list));\r\nstart_critical_timings();\r\nidle->sequence++;\r\nsmp_wmb();\r\nidle_time = idle->idle_exit - idle->idle_enter;\r\nidle->idle_time += idle_time;\r\nidle->idle_enter = idle->idle_exit = 0ULL;\r\nidle->idle_count++;\r\naccount_idle_time(idle_time);\r\nsmp_wmb();\r\nidle->sequence++;\r\n}\r\ncputime64_t s390_get_idle_time(int cpu)\r\n{\r\nstruct s390_idle_data *idle = &per_cpu(s390_idle, cpu);\r\nunsigned long long now, idle_enter, idle_exit;\r\nunsigned int sequence;\r\ndo {\r\nnow = get_clock();\r\nsequence = ACCESS_ONCE(idle->sequence);\r\nidle_enter = ACCESS_ONCE(idle->idle_enter);\r\nidle_exit = ACCESS_ONCE(idle->idle_exit);\r\n} while ((sequence & 1) || (idle->sequence != sequence));\r\nreturn idle_enter ? ((idle_exit ? : now) - idle_enter) : 0;\r\n}\r\nstatic void list_add_sorted(struct vtimer_list *timer, struct list_head *head)\r\n{\r\nstruct vtimer_list *event;\r\nlist_for_each_entry(event, head, entry) {\r\nif (event->expires > timer->expires) {\r\nlist_add_tail(&timer->entry, &event->entry);\r\nreturn;\r\n}\r\n}\r\nlist_add_tail(&timer->entry, head);\r\n}\r\nstatic void do_callbacks(struct list_head *cb_list)\r\n{\r\nstruct vtimer_queue *vq;\r\nstruct vtimer_list *event, *tmp;\r\nif (list_empty(cb_list))\r\nreturn;\r\nvq = &__get_cpu_var(virt_cpu_timer);\r\nlist_for_each_entry_safe(event, tmp, cb_list, entry) {\r\nlist_del_init(&event->entry);\r\n(event->function)(event->data);\r\nif (event->interval) {\r\nevent->expires = event->interval + vq->elapsed;\r\nspin_lock(&vq->lock);\r\nlist_add_sorted(event, &vq->list);\r\nspin_unlock(&vq->lock);\r\n}\r\n}\r\n}\r\nstatic void do_cpu_timer_interrupt(struct ext_code ext_code,\r\nunsigned int param32, unsigned long param64)\r\n{\r\nstruct vtimer_queue *vq;\r\nstruct vtimer_list *event, *tmp;\r\nstruct list_head cb_list;\r\n__u64 elapsed, next;\r\nkstat_cpu(smp_processor_id()).irqs[EXTINT_TMR]++;\r\nINIT_LIST_HEAD(&cb_list);\r\nvq = &__get_cpu_var(virt_cpu_timer);\r\nspin_lock(&vq->lock);\r\nelapsed = vq->elapsed + (vq->timer - S390_lowcore.async_enter_timer);\r\nBUG_ON((s64) elapsed < 0);\r\nvq->elapsed = 0;\r\nlist_for_each_entry_safe(event, tmp, &vq->list, entry) {\r\nif (event->expires < elapsed)\r\nlist_move_tail(&event->entry, &cb_list);\r\nelse\r\nevent->expires -= elapsed;\r\n}\r\nspin_unlock(&vq->lock);\r\ndo_callbacks(&cb_list);\r\nnext = VTIMER_MAX_SLICE;\r\nspin_lock(&vq->lock);\r\nif (!list_empty(&vq->list)) {\r\nevent = list_first_entry(&vq->list, struct vtimer_list, entry);\r\nnext = event->expires;\r\n}\r\nspin_unlock(&vq->lock);\r\nelapsed = S390_lowcore.async_enter_timer - get_vtimer();\r\nset_vtimer(next - elapsed);\r\nvq->timer = next - elapsed;\r\nvq->elapsed = elapsed;\r\n}\r\nvoid init_virt_timer(struct vtimer_list *timer)\r\n{\r\ntimer->function = NULL;\r\nINIT_LIST_HEAD(&timer->entry);\r\n}\r\nstatic inline int vtimer_pending(struct vtimer_list *timer)\r\n{\r\nreturn (!list_empty(&timer->entry));\r\n}\r\nstatic void internal_add_vtimer(struct vtimer_list *timer)\r\n{\r\nstruct vtimer_queue *vq;\r\nunsigned long flags;\r\n__u64 left, expires;\r\nvq = &per_cpu(virt_cpu_timer, timer->cpu);\r\nspin_lock_irqsave(&vq->lock, flags);\r\nBUG_ON(timer->cpu != smp_processor_id());\r\nif (list_empty(&vq->list)) {\r\nlist_add(&timer->entry, &vq->list);\r\nset_vtimer(timer->expires);\r\nvq->timer = timer->expires;\r\nvq->elapsed = 0;\r\n} else {\r\nexpires = timer->expires;\r\nleft = get_vtimer();\r\nif (likely((s64) expires < (s64) left)) {\r\nset_vtimer(expires);\r\nvq->elapsed += vq->timer - left;\r\nvq->timer = expires;\r\n} else {\r\nvq->elapsed += vq->timer - left;\r\nvq->timer = left;\r\n}\r\ntimer->expires += vq->elapsed;\r\nlist_add_sorted(timer, &vq->list);\r\n}\r\nspin_unlock_irqrestore(&vq->lock, flags);\r\nput_cpu();\r\n}\r\nstatic inline void prepare_vtimer(struct vtimer_list *timer)\r\n{\r\nBUG_ON(!timer->function);\r\nBUG_ON(!timer->expires || timer->expires > VTIMER_MAX_SLICE);\r\nBUG_ON(vtimer_pending(timer));\r\ntimer->cpu = get_cpu();\r\n}\r\nvoid add_virt_timer(void *new)\r\n{\r\nstruct vtimer_list *timer;\r\ntimer = (struct vtimer_list *)new;\r\nprepare_vtimer(timer);\r\ntimer->interval = 0;\r\ninternal_add_vtimer(timer);\r\n}\r\nvoid add_virt_timer_periodic(void *new)\r\n{\r\nstruct vtimer_list *timer;\r\ntimer = (struct vtimer_list *)new;\r\nprepare_vtimer(timer);\r\ntimer->interval = timer->expires;\r\ninternal_add_vtimer(timer);\r\n}\r\nstatic int __mod_vtimer(struct vtimer_list *timer, __u64 expires, int periodic)\r\n{\r\nstruct vtimer_queue *vq;\r\nunsigned long flags;\r\nint cpu;\r\nBUG_ON(!timer->function);\r\nBUG_ON(!expires || expires > VTIMER_MAX_SLICE);\r\nif (timer->expires == expires && vtimer_pending(timer))\r\nreturn 1;\r\ncpu = get_cpu();\r\nvq = &per_cpu(virt_cpu_timer, cpu);\r\nspin_lock_irqsave(&vq->lock, flags);\r\nif (!vtimer_pending(timer)) {\r\nspin_unlock_irqrestore(&vq->lock, flags);\r\nif (periodic)\r\ntimer->interval = expires;\r\nelse\r\ntimer->interval = 0;\r\ntimer->expires = expires;\r\ntimer->cpu = cpu;\r\ninternal_add_vtimer(timer);\r\nreturn 0;\r\n}\r\nBUG_ON(timer->cpu != cpu);\r\nlist_del_init(&timer->entry);\r\ntimer->expires = expires;\r\nif (periodic)\r\ntimer->interval = expires;\r\nspin_unlock_irqrestore(&vq->lock, flags);\r\ninternal_add_vtimer(timer);\r\nreturn 1;\r\n}\r\nint mod_virt_timer(struct vtimer_list *timer, __u64 expires)\r\n{\r\nreturn __mod_vtimer(timer, expires, 0);\r\n}\r\nint mod_virt_timer_periodic(struct vtimer_list *timer, __u64 expires)\r\n{\r\nreturn __mod_vtimer(timer, expires, 1);\r\n}\r\nint del_virt_timer(struct vtimer_list *timer)\r\n{\r\nunsigned long flags;\r\nstruct vtimer_queue *vq;\r\nif (!vtimer_pending(timer))\r\nreturn 0;\r\nvq = &per_cpu(virt_cpu_timer, timer->cpu);\r\nspin_lock_irqsave(&vq->lock, flags);\r\nlist_del_init(&timer->entry);\r\nspin_unlock_irqrestore(&vq->lock, flags);\r\nreturn 1;\r\n}\r\nvoid init_cpu_vtimer(void)\r\n{\r\nstruct vtimer_queue *vq;\r\nvq = &__get_cpu_var(virt_cpu_timer);\r\nINIT_LIST_HEAD(&vq->list);\r\nspin_lock_init(&vq->lock);\r\n__ctl_set_bit(0,10);\r\nset_vtimer(0x7fffffffffffffffULL);\r\n}\r\nstatic int __cpuinit s390_nohz_notify(struct notifier_block *self,\r\nunsigned long action, void *hcpu)\r\n{\r\nstruct s390_idle_data *idle;\r\nlong cpu = (long) hcpu;\r\nidle = &per_cpu(s390_idle, cpu);\r\nswitch (action) {\r\ncase CPU_DYING:\r\ncase CPU_DYING_FROZEN:\r\nidle->nohz_delay = 0;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nvoid __init vtime_init(void)\r\n{\r\nif (register_external_interrupt(0x1005, do_cpu_timer_interrupt))\r\npanic("Couldn't request external interrupt 0x1005");\r\ninit_cpu_vtimer();\r\ncpu_notifier(s390_nohz_notify, 0);\r\n}
