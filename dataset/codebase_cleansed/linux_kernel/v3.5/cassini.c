static inline void cas_lock_tx(struct cas *cp)\r\n{\r\nint i;\r\nfor (i = 0; i < N_TX_RINGS; i++)\r\nspin_lock(&cp->tx_lock[i]);\r\n}\r\nstatic inline void cas_lock_all(struct cas *cp)\r\n{\r\nspin_lock_irq(&cp->lock);\r\ncas_lock_tx(cp);\r\n}\r\nstatic inline void cas_unlock_tx(struct cas *cp)\r\n{\r\nint i;\r\nfor (i = N_TX_RINGS; i > 0; i--)\r\nspin_unlock(&cp->tx_lock[i - 1]);\r\n}\r\nstatic inline void cas_unlock_all(struct cas *cp)\r\n{\r\ncas_unlock_tx(cp);\r\nspin_unlock_irq(&cp->lock);\r\n}\r\nstatic void cas_disable_irq(struct cas *cp, const int ring)\r\n{\r\nif (ring == 0) {\r\nwritel(0xFFFFFFFF, cp->regs + REG_INTR_MASK);\r\nreturn;\r\n}\r\nif (cp->cas_flags & CAS_FLAG_REG_PLUS) {\r\nswitch (ring) {\r\n#if defined (USE_PCI_INTB) || defined(USE_PCI_INTC) || defined(USE_PCI_INTD)\r\n#ifdef USE_PCI_INTB\r\ncase 1:\r\n#endif\r\n#ifdef USE_PCI_INTC\r\ncase 2:\r\n#endif\r\n#ifdef USE_PCI_INTD\r\ncase 3:\r\n#endif\r\nwritel(INTRN_MASK_CLEAR_ALL | INTRN_MASK_RX_EN,\r\ncp->regs + REG_PLUS_INTRN_MASK(ring));\r\nbreak;\r\n#endif\r\ndefault:\r\nwritel(INTRN_MASK_CLEAR_ALL, cp->regs +\r\nREG_PLUS_INTRN_MASK(ring));\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic inline void cas_mask_intr(struct cas *cp)\r\n{\r\nint i;\r\nfor (i = 0; i < N_RX_COMP_RINGS; i++)\r\ncas_disable_irq(cp, i);\r\n}\r\nstatic void cas_enable_irq(struct cas *cp, const int ring)\r\n{\r\nif (ring == 0) {\r\nwritel(INTR_TX_DONE, cp->regs + REG_INTR_MASK);\r\nreturn;\r\n}\r\nif (cp->cas_flags & CAS_FLAG_REG_PLUS) {\r\nswitch (ring) {\r\n#if defined (USE_PCI_INTB) || defined(USE_PCI_INTC) || defined(USE_PCI_INTD)\r\n#ifdef USE_PCI_INTB\r\ncase 1:\r\n#endif\r\n#ifdef USE_PCI_INTC\r\ncase 2:\r\n#endif\r\n#ifdef USE_PCI_INTD\r\ncase 3:\r\n#endif\r\nwritel(INTRN_MASK_RX_EN, cp->regs +\r\nREG_PLUS_INTRN_MASK(ring));\r\nbreak;\r\n#endif\r\ndefault:\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic inline void cas_unmask_intr(struct cas *cp)\r\n{\r\nint i;\r\nfor (i = 0; i < N_RX_COMP_RINGS; i++)\r\ncas_enable_irq(cp, i);\r\n}\r\nstatic inline void cas_entropy_gather(struct cas *cp)\r\n{\r\n#ifdef USE_ENTROPY_DEV\r\nif ((cp->cas_flags & CAS_FLAG_ENTROPY_DEV) == 0)\r\nreturn;\r\nbatch_entropy_store(readl(cp->regs + REG_ENTROPY_IV),\r\nreadl(cp->regs + REG_ENTROPY_IV),\r\nsizeof(uint64_t)*8);\r\n#endif\r\n}\r\nstatic inline void cas_entropy_reset(struct cas *cp)\r\n{\r\n#ifdef USE_ENTROPY_DEV\r\nif ((cp->cas_flags & CAS_FLAG_ENTROPY_DEV) == 0)\r\nreturn;\r\nwritel(BIM_LOCAL_DEV_PAD | BIM_LOCAL_DEV_PROM | BIM_LOCAL_DEV_EXT,\r\ncp->regs + REG_BIM_LOCAL_DEV_EN);\r\nwriteb(ENTROPY_RESET_STC_MODE, cp->regs + REG_ENTROPY_RESET);\r\nwriteb(0x55, cp->regs + REG_ENTROPY_RAND_REG);\r\nif (readb(cp->regs + REG_ENTROPY_RAND_REG) == 0)\r\ncp->cas_flags &= ~CAS_FLAG_ENTROPY_DEV;\r\n#endif\r\n}\r\nstatic u16 cas_phy_read(struct cas *cp, int reg)\r\n{\r\nu32 cmd;\r\nint limit = STOP_TRIES_PHY;\r\ncmd = MIF_FRAME_ST | MIF_FRAME_OP_READ;\r\ncmd |= CAS_BASE(MIF_FRAME_PHY_ADDR, cp->phy_addr);\r\ncmd |= CAS_BASE(MIF_FRAME_REG_ADDR, reg);\r\ncmd |= MIF_FRAME_TURN_AROUND_MSB;\r\nwritel(cmd, cp->regs + REG_MIF_FRAME);\r\nwhile (limit-- > 0) {\r\nudelay(10);\r\ncmd = readl(cp->regs + REG_MIF_FRAME);\r\nif (cmd & MIF_FRAME_TURN_AROUND_LSB)\r\nreturn cmd & MIF_FRAME_DATA_MASK;\r\n}\r\nreturn 0xFFFF;\r\n}\r\nstatic int cas_phy_write(struct cas *cp, int reg, u16 val)\r\n{\r\nint limit = STOP_TRIES_PHY;\r\nu32 cmd;\r\ncmd = MIF_FRAME_ST | MIF_FRAME_OP_WRITE;\r\ncmd |= CAS_BASE(MIF_FRAME_PHY_ADDR, cp->phy_addr);\r\ncmd |= CAS_BASE(MIF_FRAME_REG_ADDR, reg);\r\ncmd |= MIF_FRAME_TURN_AROUND_MSB;\r\ncmd |= val & MIF_FRAME_DATA_MASK;\r\nwritel(cmd, cp->regs + REG_MIF_FRAME);\r\nwhile (limit-- > 0) {\r\nudelay(10);\r\ncmd = readl(cp->regs + REG_MIF_FRAME);\r\nif (cmd & MIF_FRAME_TURN_AROUND_LSB)\r\nreturn 0;\r\n}\r\nreturn -1;\r\n}\r\nstatic void cas_phy_powerup(struct cas *cp)\r\n{\r\nu16 ctl = cas_phy_read(cp, MII_BMCR);\r\nif ((ctl & BMCR_PDOWN) == 0)\r\nreturn;\r\nctl &= ~BMCR_PDOWN;\r\ncas_phy_write(cp, MII_BMCR, ctl);\r\n}\r\nstatic void cas_phy_powerdown(struct cas *cp)\r\n{\r\nu16 ctl = cas_phy_read(cp, MII_BMCR);\r\nif (ctl & BMCR_PDOWN)\r\nreturn;\r\nctl |= BMCR_PDOWN;\r\ncas_phy_write(cp, MII_BMCR, ctl);\r\n}\r\nstatic int cas_page_free(struct cas *cp, cas_page_t *page)\r\n{\r\npci_unmap_page(cp->pdev, page->dma_addr, cp->page_size,\r\nPCI_DMA_FROMDEVICE);\r\n__free_pages(page->buffer, cp->page_order);\r\nkfree(page);\r\nreturn 0;\r\n}\r\nstatic cas_page_t *cas_page_alloc(struct cas *cp, const gfp_t flags)\r\n{\r\ncas_page_t *page;\r\npage = kmalloc(sizeof(cas_page_t), flags);\r\nif (!page)\r\nreturn NULL;\r\nINIT_LIST_HEAD(&page->list);\r\nRX_USED_SET(page, 0);\r\npage->buffer = alloc_pages(flags, cp->page_order);\r\nif (!page->buffer)\r\ngoto page_err;\r\npage->dma_addr = pci_map_page(cp->pdev, page->buffer, 0,\r\ncp->page_size, PCI_DMA_FROMDEVICE);\r\nreturn page;\r\npage_err:\r\nkfree(page);\r\nreturn NULL;\r\n}\r\nstatic void cas_spare_init(struct cas *cp)\r\n{\r\nspin_lock(&cp->rx_inuse_lock);\r\nINIT_LIST_HEAD(&cp->rx_inuse_list);\r\nspin_unlock(&cp->rx_inuse_lock);\r\nspin_lock(&cp->rx_spare_lock);\r\nINIT_LIST_HEAD(&cp->rx_spare_list);\r\ncp->rx_spares_needed = RX_SPARE_COUNT;\r\nspin_unlock(&cp->rx_spare_lock);\r\n}\r\nstatic void cas_spare_free(struct cas *cp)\r\n{\r\nstruct list_head list, *elem, *tmp;\r\nINIT_LIST_HEAD(&list);\r\nspin_lock(&cp->rx_spare_lock);\r\nlist_splice_init(&cp->rx_spare_list, &list);\r\nspin_unlock(&cp->rx_spare_lock);\r\nlist_for_each_safe(elem, tmp, &list) {\r\ncas_page_free(cp, list_entry(elem, cas_page_t, list));\r\n}\r\nINIT_LIST_HEAD(&list);\r\n#if 1\r\nspin_lock(&cp->rx_inuse_lock);\r\nlist_splice_init(&cp->rx_inuse_list, &list);\r\nspin_unlock(&cp->rx_inuse_lock);\r\n#else\r\nspin_lock(&cp->rx_spare_lock);\r\nlist_splice_init(&cp->rx_inuse_list, &list);\r\nspin_unlock(&cp->rx_spare_lock);\r\n#endif\r\nlist_for_each_safe(elem, tmp, &list) {\r\ncas_page_free(cp, list_entry(elem, cas_page_t, list));\r\n}\r\n}\r\nstatic void cas_spare_recover(struct cas *cp, const gfp_t flags)\r\n{\r\nstruct list_head list, *elem, *tmp;\r\nint needed, i;\r\nINIT_LIST_HEAD(&list);\r\nspin_lock(&cp->rx_inuse_lock);\r\nlist_splice_init(&cp->rx_inuse_list, &list);\r\nspin_unlock(&cp->rx_inuse_lock);\r\nlist_for_each_safe(elem, tmp, &list) {\r\ncas_page_t *page = list_entry(elem, cas_page_t, list);\r\nif (page_count(page->buffer) > 1)\r\ncontinue;\r\nlist_del(elem);\r\nspin_lock(&cp->rx_spare_lock);\r\nif (cp->rx_spares_needed > 0) {\r\nlist_add(elem, &cp->rx_spare_list);\r\ncp->rx_spares_needed--;\r\nspin_unlock(&cp->rx_spare_lock);\r\n} else {\r\nspin_unlock(&cp->rx_spare_lock);\r\ncas_page_free(cp, page);\r\n}\r\n}\r\nif (!list_empty(&list)) {\r\nspin_lock(&cp->rx_inuse_lock);\r\nlist_splice(&list, &cp->rx_inuse_list);\r\nspin_unlock(&cp->rx_inuse_lock);\r\n}\r\nspin_lock(&cp->rx_spare_lock);\r\nneeded = cp->rx_spares_needed;\r\nspin_unlock(&cp->rx_spare_lock);\r\nif (!needed)\r\nreturn;\r\nINIT_LIST_HEAD(&list);\r\ni = 0;\r\nwhile (i < needed) {\r\ncas_page_t *spare = cas_page_alloc(cp, flags);\r\nif (!spare)\r\nbreak;\r\nlist_add(&spare->list, &list);\r\ni++;\r\n}\r\nspin_lock(&cp->rx_spare_lock);\r\nlist_splice(&list, &cp->rx_spare_list);\r\ncp->rx_spares_needed -= i;\r\nspin_unlock(&cp->rx_spare_lock);\r\n}\r\nstatic cas_page_t *cas_page_dequeue(struct cas *cp)\r\n{\r\nstruct list_head *entry;\r\nint recover;\r\nspin_lock(&cp->rx_spare_lock);\r\nif (list_empty(&cp->rx_spare_list)) {\r\nspin_unlock(&cp->rx_spare_lock);\r\ncas_spare_recover(cp, GFP_ATOMIC);\r\nspin_lock(&cp->rx_spare_lock);\r\nif (list_empty(&cp->rx_spare_list)) {\r\nnetif_err(cp, rx_err, cp->dev,\r\n"no spare buffers available\n");\r\nspin_unlock(&cp->rx_spare_lock);\r\nreturn NULL;\r\n}\r\n}\r\nentry = cp->rx_spare_list.next;\r\nlist_del(entry);\r\nrecover = ++cp->rx_spares_needed;\r\nspin_unlock(&cp->rx_spare_lock);\r\nif ((recover & (RX_SPARE_RECOVER_VAL - 1)) == 0) {\r\n#if 1\r\natomic_inc(&cp->reset_task_pending);\r\natomic_inc(&cp->reset_task_pending_spare);\r\nschedule_work(&cp->reset_task);\r\n#else\r\natomic_set(&cp->reset_task_pending, CAS_RESET_SPARE);\r\nschedule_work(&cp->reset_task);\r\n#endif\r\n}\r\nreturn list_entry(entry, cas_page_t, list);\r\n}\r\nstatic void cas_mif_poll(struct cas *cp, const int enable)\r\n{\r\nu32 cfg;\r\ncfg = readl(cp->regs + REG_MIF_CFG);\r\ncfg &= (MIF_CFG_MDIO_0 | MIF_CFG_MDIO_1);\r\nif (cp->phy_type & CAS_PHY_MII_MDIO1)\r\ncfg |= MIF_CFG_PHY_SELECT;\r\nif (enable) {\r\ncfg |= MIF_CFG_POLL_EN;\r\ncfg |= CAS_BASE(MIF_CFG_POLL_REG, MII_BMSR);\r\ncfg |= CAS_BASE(MIF_CFG_POLL_PHY, cp->phy_addr);\r\n}\r\nwritel((enable) ? ~(BMSR_LSTATUS | BMSR_ANEGCOMPLETE) : 0xFFFF,\r\ncp->regs + REG_MIF_MASK);\r\nwritel(cfg, cp->regs + REG_MIF_CFG);\r\n}\r\nstatic void cas_begin_auto_negotiation(struct cas *cp, struct ethtool_cmd *ep)\r\n{\r\nu16 ctl;\r\n#if 1\r\nint lcntl;\r\nint changed = 0;\r\nint oldstate = cp->lstate;\r\nint link_was_not_down = !(oldstate == link_down);\r\n#endif\r\nif (!ep)\r\ngoto start_aneg;\r\nlcntl = cp->link_cntl;\r\nif (ep->autoneg == AUTONEG_ENABLE)\r\ncp->link_cntl = BMCR_ANENABLE;\r\nelse {\r\nu32 speed = ethtool_cmd_speed(ep);\r\ncp->link_cntl = 0;\r\nif (speed == SPEED_100)\r\ncp->link_cntl |= BMCR_SPEED100;\r\nelse if (speed == SPEED_1000)\r\ncp->link_cntl |= CAS_BMCR_SPEED1000;\r\nif (ep->duplex == DUPLEX_FULL)\r\ncp->link_cntl |= BMCR_FULLDPLX;\r\n}\r\n#if 1\r\nchanged = (lcntl != cp->link_cntl);\r\n#endif\r\nstart_aneg:\r\nif (cp->lstate == link_up) {\r\nnetdev_info(cp->dev, "PCS link down\n");\r\n} else {\r\nif (changed) {\r\nnetdev_info(cp->dev, "link configuration changed\n");\r\n}\r\n}\r\ncp->lstate = link_down;\r\ncp->link_transition = LINK_TRANSITION_LINK_DOWN;\r\nif (!cp->hw_running)\r\nreturn;\r\n#if 1\r\nif (oldstate == link_up)\r\nnetif_carrier_off(cp->dev);\r\nif (changed && link_was_not_down) {\r\natomic_inc(&cp->reset_task_pending);\r\natomic_inc(&cp->reset_task_pending_all);\r\nschedule_work(&cp->reset_task);\r\ncp->timer_ticks = 0;\r\nmod_timer(&cp->link_timer, jiffies + CAS_LINK_TIMEOUT);\r\nreturn;\r\n}\r\n#endif\r\nif (cp->phy_type & CAS_PHY_SERDES) {\r\nu32 val = readl(cp->regs + REG_PCS_MII_CTRL);\r\nif (cp->link_cntl & BMCR_ANENABLE) {\r\nval |= (PCS_MII_RESTART_AUTONEG | PCS_MII_AUTONEG_EN);\r\ncp->lstate = link_aneg;\r\n} else {\r\nif (cp->link_cntl & BMCR_FULLDPLX)\r\nval |= PCS_MII_CTRL_DUPLEX;\r\nval &= ~PCS_MII_AUTONEG_EN;\r\ncp->lstate = link_force_ok;\r\n}\r\ncp->link_transition = LINK_TRANSITION_LINK_CONFIG;\r\nwritel(val, cp->regs + REG_PCS_MII_CTRL);\r\n} else {\r\ncas_mif_poll(cp, 0);\r\nctl = cas_phy_read(cp, MII_BMCR);\r\nctl &= ~(BMCR_FULLDPLX | BMCR_SPEED100 |\r\nCAS_BMCR_SPEED1000 | BMCR_ANENABLE);\r\nctl |= cp->link_cntl;\r\nif (ctl & BMCR_ANENABLE) {\r\nctl |= BMCR_ANRESTART;\r\ncp->lstate = link_aneg;\r\n} else {\r\ncp->lstate = link_force_ok;\r\n}\r\ncp->link_transition = LINK_TRANSITION_LINK_CONFIG;\r\ncas_phy_write(cp, MII_BMCR, ctl);\r\ncas_mif_poll(cp, 1);\r\n}\r\ncp->timer_ticks = 0;\r\nmod_timer(&cp->link_timer, jiffies + CAS_LINK_TIMEOUT);\r\n}\r\nstatic int cas_reset_mii_phy(struct cas *cp)\r\n{\r\nint limit = STOP_TRIES_PHY;\r\nu16 val;\r\ncas_phy_write(cp, MII_BMCR, BMCR_RESET);\r\nudelay(100);\r\nwhile (--limit) {\r\nval = cas_phy_read(cp, MII_BMCR);\r\nif ((val & BMCR_RESET) == 0)\r\nbreak;\r\nudelay(10);\r\n}\r\nreturn limit <= 0;\r\n}\r\nstatic int cas_saturn_firmware_init(struct cas *cp)\r\n{\r\nconst struct firmware *fw;\r\nconst char fw_name[] = "sun/cassini.bin";\r\nint err;\r\nif (PHY_NS_DP83065 != cp->phy_id)\r\nreturn 0;\r\nerr = request_firmware(&fw, fw_name, &cp->pdev->dev);\r\nif (err) {\r\npr_err("Failed to load firmware \"%s\"\n",\r\nfw_name);\r\nreturn err;\r\n}\r\nif (fw->size < 2) {\r\npr_err("bogus length %zu in \"%s\"\n",\r\nfw->size, fw_name);\r\nerr = -EINVAL;\r\ngoto out;\r\n}\r\ncp->fw_load_addr= fw->data[1] << 8 | fw->data[0];\r\ncp->fw_size = fw->size - 2;\r\ncp->fw_data = vmalloc(cp->fw_size);\r\nif (!cp->fw_data) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nmemcpy(cp->fw_data, &fw->data[2], cp->fw_size);\r\nout:\r\nrelease_firmware(fw);\r\nreturn err;\r\n}\r\nstatic void cas_saturn_firmware_load(struct cas *cp)\r\n{\r\nint i;\r\ncas_phy_powerdown(cp);\r\ncas_phy_write(cp, DP83065_MII_MEM, 0x0);\r\ncas_phy_write(cp, DP83065_MII_REGE, 0x8ff9);\r\ncas_phy_write(cp, DP83065_MII_REGD, 0xbd);\r\ncas_phy_write(cp, DP83065_MII_REGE, 0x8ffa);\r\ncas_phy_write(cp, DP83065_MII_REGD, 0x82);\r\ncas_phy_write(cp, DP83065_MII_REGE, 0x8ffb);\r\ncas_phy_write(cp, DP83065_MII_REGD, 0x0);\r\ncas_phy_write(cp, DP83065_MII_REGE, 0x8ffc);\r\ncas_phy_write(cp, DP83065_MII_REGD, 0x39);\r\ncas_phy_write(cp, DP83065_MII_MEM, 0x1);\r\ncas_phy_write(cp, DP83065_MII_REGE, cp->fw_load_addr);\r\nfor (i = 0; i < cp->fw_size; i++)\r\ncas_phy_write(cp, DP83065_MII_REGD, cp->fw_data[i]);\r\ncas_phy_write(cp, DP83065_MII_REGE, 0x8ff8);\r\ncas_phy_write(cp, DP83065_MII_REGD, 0x1);\r\n}\r\nstatic void cas_phy_init(struct cas *cp)\r\n{\r\nu16 val;\r\nif (CAS_PHY_MII(cp->phy_type)) {\r\nwritel(PCS_DATAPATH_MODE_MII,\r\ncp->regs + REG_PCS_DATAPATH_MODE);\r\ncas_mif_poll(cp, 0);\r\ncas_reset_mii_phy(cp);\r\nif (PHY_LUCENT_B0 == cp->phy_id) {\r\ncas_phy_write(cp, LUCENT_MII_REG, 0x8000);\r\ncas_phy_write(cp, MII_BMCR, 0x00f1);\r\ncas_phy_write(cp, LUCENT_MII_REG, 0x0);\r\n} else if (PHY_BROADCOM_B0 == (cp->phy_id & 0xFFFFFFFC)) {\r\ncas_phy_write(cp, BROADCOM_MII_REG8, 0x0C20);\r\ncas_phy_write(cp, BROADCOM_MII_REG7, 0x0012);\r\ncas_phy_write(cp, BROADCOM_MII_REG5, 0x1804);\r\ncas_phy_write(cp, BROADCOM_MII_REG7, 0x0013);\r\ncas_phy_write(cp, BROADCOM_MII_REG5, 0x1204);\r\ncas_phy_write(cp, BROADCOM_MII_REG7, 0x8006);\r\ncas_phy_write(cp, BROADCOM_MII_REG5, 0x0132);\r\ncas_phy_write(cp, BROADCOM_MII_REG7, 0x8006);\r\ncas_phy_write(cp, BROADCOM_MII_REG5, 0x0232);\r\ncas_phy_write(cp, BROADCOM_MII_REG7, 0x201F);\r\ncas_phy_write(cp, BROADCOM_MII_REG5, 0x0A20);\r\n} else if (PHY_BROADCOM_5411 == cp->phy_id) {\r\nval = cas_phy_read(cp, BROADCOM_MII_REG4);\r\nval = cas_phy_read(cp, BROADCOM_MII_REG4);\r\nif (val & 0x0080) {\r\ncas_phy_write(cp, BROADCOM_MII_REG4,\r\nval & ~0x0080);\r\n}\r\n} else if (cp->cas_flags & CAS_FLAG_SATURN) {\r\nwritel((cp->phy_type & CAS_PHY_MII_MDIO0) ?\r\nSATURN_PCFG_FSI : 0x0,\r\ncp->regs + REG_SATURN_PCFG);\r\nif (PHY_NS_DP83065 == cp->phy_id) {\r\ncas_saturn_firmware_load(cp);\r\n}\r\ncas_phy_powerup(cp);\r\n}\r\nval = cas_phy_read(cp, MII_BMCR);\r\nval &= ~BMCR_ANENABLE;\r\ncas_phy_write(cp, MII_BMCR, val);\r\nudelay(10);\r\ncas_phy_write(cp, MII_ADVERTISE,\r\ncas_phy_read(cp, MII_ADVERTISE) |\r\n(ADVERTISE_10HALF | ADVERTISE_10FULL |\r\nADVERTISE_100HALF | ADVERTISE_100FULL |\r\nCAS_ADVERTISE_PAUSE |\r\nCAS_ADVERTISE_ASYM_PAUSE));\r\nif (cp->cas_flags & CAS_FLAG_1000MB_CAP) {\r\nval = cas_phy_read(cp, CAS_MII_1000_CTRL);\r\nval &= ~CAS_ADVERTISE_1000HALF;\r\nval |= CAS_ADVERTISE_1000FULL;\r\ncas_phy_write(cp, CAS_MII_1000_CTRL, val);\r\n}\r\n} else {\r\nu32 val;\r\nint limit;\r\nwritel(PCS_DATAPATH_MODE_SERDES,\r\ncp->regs + REG_PCS_DATAPATH_MODE);\r\nif (cp->cas_flags & CAS_FLAG_SATURN)\r\nwritel(0, cp->regs + REG_SATURN_PCFG);\r\nval = readl(cp->regs + REG_PCS_MII_CTRL);\r\nval |= PCS_MII_RESET;\r\nwritel(val, cp->regs + REG_PCS_MII_CTRL);\r\nlimit = STOP_TRIES;\r\nwhile (--limit > 0) {\r\nudelay(10);\r\nif ((readl(cp->regs + REG_PCS_MII_CTRL) &\r\nPCS_MII_RESET) == 0)\r\nbreak;\r\n}\r\nif (limit <= 0)\r\nnetdev_warn(cp->dev, "PCS reset bit would not clear [%08x]\n",\r\nreadl(cp->regs + REG_PCS_STATE_MACHINE));\r\nwritel(0x0, cp->regs + REG_PCS_CFG);\r\nval = readl(cp->regs + REG_PCS_MII_ADVERT);\r\nval &= ~PCS_MII_ADVERT_HD;\r\nval |= (PCS_MII_ADVERT_FD | PCS_MII_ADVERT_SYM_PAUSE |\r\nPCS_MII_ADVERT_ASYM_PAUSE);\r\nwritel(val, cp->regs + REG_PCS_MII_ADVERT);\r\nwritel(PCS_CFG_EN, cp->regs + REG_PCS_CFG);\r\nwritel(PCS_SERDES_CTRL_SYNCD_EN,\r\ncp->regs + REG_PCS_SERDES_CTRL);\r\n}\r\n}\r\nstatic int cas_pcs_link_check(struct cas *cp)\r\n{\r\nu32 stat, state_machine;\r\nint retval = 0;\r\nstat = readl(cp->regs + REG_PCS_MII_STATUS);\r\nif ((stat & PCS_MII_STATUS_LINK_STATUS) == 0)\r\nstat = readl(cp->regs + REG_PCS_MII_STATUS);\r\nif ((stat & (PCS_MII_STATUS_AUTONEG_COMP |\r\nPCS_MII_STATUS_REMOTE_FAULT)) ==\r\n(PCS_MII_STATUS_AUTONEG_COMP | PCS_MII_STATUS_REMOTE_FAULT))\r\nnetif_info(cp, link, cp->dev, "PCS RemoteFault\n");\r\nstate_machine = readl(cp->regs + REG_PCS_STATE_MACHINE);\r\nif ((state_machine & PCS_SM_LINK_STATE_MASK) != SM_LINK_STATE_UP) {\r\nstat &= ~PCS_MII_STATUS_LINK_STATUS;\r\n} else if (state_machine & PCS_SM_WORD_SYNC_STATE_MASK) {\r\nstat |= PCS_MII_STATUS_LINK_STATUS;\r\n}\r\nif (stat & PCS_MII_STATUS_LINK_STATUS) {\r\nif (cp->lstate != link_up) {\r\nif (cp->opened) {\r\ncp->lstate = link_up;\r\ncp->link_transition = LINK_TRANSITION_LINK_UP;\r\ncas_set_link_modes(cp);\r\nnetif_carrier_on(cp->dev);\r\n}\r\n}\r\n} else if (cp->lstate == link_up) {\r\ncp->lstate = link_down;\r\nif (link_transition_timeout != 0 &&\r\ncp->link_transition != LINK_TRANSITION_REQUESTED_RESET &&\r\n!cp->link_transition_jiffies_valid) {\r\nretval = 1;\r\ncp->link_transition = LINK_TRANSITION_REQUESTED_RESET;\r\ncp->link_transition_jiffies = jiffies;\r\ncp->link_transition_jiffies_valid = 1;\r\n} else {\r\ncp->link_transition = LINK_TRANSITION_ON_FAILURE;\r\n}\r\nnetif_carrier_off(cp->dev);\r\nif (cp->opened)\r\nnetif_info(cp, link, cp->dev, "PCS link down\n");\r\nif ((cp->cas_flags & CAS_FLAG_REG_PLUS) == 0) {\r\nstat = readl(cp->regs + REG_PCS_SERDES_STATE);\r\nif (stat == 0x03)\r\nreturn 1;\r\n}\r\n} else if (cp->lstate == link_down) {\r\nif (link_transition_timeout != 0 &&\r\ncp->link_transition != LINK_TRANSITION_REQUESTED_RESET &&\r\n!cp->link_transition_jiffies_valid) {\r\nretval = 1;\r\ncp->link_transition = LINK_TRANSITION_REQUESTED_RESET;\r\ncp->link_transition_jiffies = jiffies;\r\ncp->link_transition_jiffies_valid = 1;\r\n} else {\r\ncp->link_transition = LINK_TRANSITION_STILL_FAILED;\r\n}\r\n}\r\nreturn retval;\r\n}\r\nstatic int cas_pcs_interrupt(struct net_device *dev,\r\nstruct cas *cp, u32 status)\r\n{\r\nu32 stat = readl(cp->regs + REG_PCS_INTR_STATUS);\r\nif ((stat & PCS_INTR_STATUS_LINK_CHANGE) == 0)\r\nreturn 0;\r\nreturn cas_pcs_link_check(cp);\r\n}\r\nstatic int cas_txmac_interrupt(struct net_device *dev,\r\nstruct cas *cp, u32 status)\r\n{\r\nu32 txmac_stat = readl(cp->regs + REG_MAC_TX_STATUS);\r\nif (!txmac_stat)\r\nreturn 0;\r\nnetif_printk(cp, intr, KERN_DEBUG, cp->dev,\r\n"txmac interrupt, txmac_stat: 0x%x\n", txmac_stat);\r\nif ((txmac_stat & MAC_TX_DEFER_TIMER) &&\r\n!(txmac_stat & ~MAC_TX_DEFER_TIMER))\r\nreturn 0;\r\nspin_lock(&cp->stat_lock[0]);\r\nif (txmac_stat & MAC_TX_UNDERRUN) {\r\nnetdev_err(dev, "TX MAC xmit underrun\n");\r\ncp->net_stats[0].tx_fifo_errors++;\r\n}\r\nif (txmac_stat & MAC_TX_MAX_PACKET_ERR) {\r\nnetdev_err(dev, "TX MAC max packet size error\n");\r\ncp->net_stats[0].tx_errors++;\r\n}\r\nif (txmac_stat & MAC_TX_COLL_NORMAL)\r\ncp->net_stats[0].collisions += 0x10000;\r\nif (txmac_stat & MAC_TX_COLL_EXCESS) {\r\ncp->net_stats[0].tx_aborted_errors += 0x10000;\r\ncp->net_stats[0].collisions += 0x10000;\r\n}\r\nif (txmac_stat & MAC_TX_COLL_LATE) {\r\ncp->net_stats[0].tx_aborted_errors += 0x10000;\r\ncp->net_stats[0].collisions += 0x10000;\r\n}\r\nspin_unlock(&cp->stat_lock[0]);\r\nreturn 0;\r\n}\r\nstatic void cas_load_firmware(struct cas *cp, cas_hp_inst_t *firmware)\r\n{\r\ncas_hp_inst_t *inst;\r\nu32 val;\r\nint i;\r\ni = 0;\r\nwhile ((inst = firmware) && inst->note) {\r\nwritel(i, cp->regs + REG_HP_INSTR_RAM_ADDR);\r\nval = CAS_BASE(HP_INSTR_RAM_HI_VAL, inst->val);\r\nval |= CAS_BASE(HP_INSTR_RAM_HI_MASK, inst->mask);\r\nwritel(val, cp->regs + REG_HP_INSTR_RAM_DATA_HI);\r\nval = CAS_BASE(HP_INSTR_RAM_MID_OUTARG, inst->outarg >> 10);\r\nval |= CAS_BASE(HP_INSTR_RAM_MID_OUTOP, inst->outop);\r\nval |= CAS_BASE(HP_INSTR_RAM_MID_FNEXT, inst->fnext);\r\nval |= CAS_BASE(HP_INSTR_RAM_MID_FOFF, inst->foff);\r\nval |= CAS_BASE(HP_INSTR_RAM_MID_SNEXT, inst->snext);\r\nval |= CAS_BASE(HP_INSTR_RAM_MID_SOFF, inst->soff);\r\nval |= CAS_BASE(HP_INSTR_RAM_MID_OP, inst->op);\r\nwritel(val, cp->regs + REG_HP_INSTR_RAM_DATA_MID);\r\nval = CAS_BASE(HP_INSTR_RAM_LOW_OUTMASK, inst->outmask);\r\nval |= CAS_BASE(HP_INSTR_RAM_LOW_OUTSHIFT, inst->outshift);\r\nval |= CAS_BASE(HP_INSTR_RAM_LOW_OUTEN, inst->outenab);\r\nval |= CAS_BASE(HP_INSTR_RAM_LOW_OUTARG, inst->outarg);\r\nwritel(val, cp->regs + REG_HP_INSTR_RAM_DATA_LOW);\r\n++firmware;\r\n++i;\r\n}\r\n}\r\nstatic void cas_init_rx_dma(struct cas *cp)\r\n{\r\nu64 desc_dma = cp->block_dvma;\r\nu32 val;\r\nint i, size;\r\nval = CAS_BASE(RX_CFG_SWIVEL, RX_SWIVEL_OFF_VAL);\r\nval |= CAS_BASE(RX_CFG_DESC_RING, RX_DESC_RINGN_INDEX(0));\r\nval |= CAS_BASE(RX_CFG_COMP_RING, RX_COMP_RINGN_INDEX(0));\r\nif ((N_RX_DESC_RINGS > 1) &&\r\n(cp->cas_flags & CAS_FLAG_REG_PLUS))\r\nval |= CAS_BASE(RX_CFG_DESC_RING1, RX_DESC_RINGN_INDEX(1));\r\nwritel(val, cp->regs + REG_RX_CFG);\r\nval = (unsigned long) cp->init_rxds[0] -\r\n(unsigned long) cp->init_block;\r\nwritel((desc_dma + val) >> 32, cp->regs + REG_RX_DB_HI);\r\nwritel((desc_dma + val) & 0xffffffff, cp->regs + REG_RX_DB_LOW);\r\nwritel(RX_DESC_RINGN_SIZE(0) - 4, cp->regs + REG_RX_KICK);\r\nif (cp->cas_flags & CAS_FLAG_REG_PLUS) {\r\nval = (unsigned long) cp->init_rxds[1] -\r\n(unsigned long) cp->init_block;\r\nwritel((desc_dma + val) >> 32, cp->regs + REG_PLUS_RX_DB1_HI);\r\nwritel((desc_dma + val) & 0xffffffff, cp->regs +\r\nREG_PLUS_RX_DB1_LOW);\r\nwritel(RX_DESC_RINGN_SIZE(1) - 4, cp->regs +\r\nREG_PLUS_RX_KICK1);\r\n}\r\nval = (unsigned long) cp->init_rxcs[0] -\r\n(unsigned long) cp->init_block;\r\nwritel((desc_dma + val) >> 32, cp->regs + REG_RX_CB_HI);\r\nwritel((desc_dma + val) & 0xffffffff, cp->regs + REG_RX_CB_LOW);\r\nif (cp->cas_flags & CAS_FLAG_REG_PLUS) {\r\nfor (i = 1; i < MAX_RX_COMP_RINGS; i++) {\r\nval = (unsigned long) cp->init_rxcs[i] -\r\n(unsigned long) cp->init_block;\r\nwritel((desc_dma + val) >> 32, cp->regs +\r\nREG_PLUS_RX_CBN_HI(i));\r\nwritel((desc_dma + val) & 0xffffffff, cp->regs +\r\nREG_PLUS_RX_CBN_LOW(i));\r\n}\r\n}\r\nreadl(cp->regs + REG_INTR_STATUS_ALIAS);\r\nwritel(INTR_RX_DONE | INTR_RX_BUF_UNAVAIL, cp->regs + REG_ALIAS_CLEAR);\r\nif (cp->cas_flags & CAS_FLAG_REG_PLUS) {\r\nfor (i = 1; i < N_RX_COMP_RINGS; i++)\r\nreadl(cp->regs + REG_PLUS_INTRN_STATUS_ALIAS(i));\r\nif (N_RX_COMP_RINGS > 1)\r\nwritel(INTR_RX_DONE_ALT | INTR_RX_BUF_UNAVAIL_1,\r\ncp->regs + REG_PLUS_ALIASN_CLEAR(1));\r\nfor (i = 2; i < N_RX_COMP_RINGS; i++)\r\nwritel(INTR_RX_DONE_ALT,\r\ncp->regs + REG_PLUS_ALIASN_CLEAR(i));\r\n}\r\nval = CAS_BASE(RX_PAUSE_THRESH_OFF,\r\ncp->rx_pause_off / RX_PAUSE_THRESH_QUANTUM);\r\nval |= CAS_BASE(RX_PAUSE_THRESH_ON,\r\ncp->rx_pause_on / RX_PAUSE_THRESH_QUANTUM);\r\nwritel(val, cp->regs + REG_RX_PAUSE_THRESH);\r\nfor (i = 0; i < 64; i++) {\r\nwritel(i, cp->regs + REG_RX_TABLE_ADDR);\r\nwritel(0x0, cp->regs + REG_RX_TABLE_DATA_LOW);\r\nwritel(0x0, cp->regs + REG_RX_TABLE_DATA_MID);\r\nwritel(0x0, cp->regs + REG_RX_TABLE_DATA_HI);\r\n}\r\nwritel(0x0, cp->regs + REG_RX_CTRL_FIFO_ADDR);\r\nwritel(0x0, cp->regs + REG_RX_IPP_FIFO_ADDR);\r\n#ifdef USE_RX_BLANK\r\nval = CAS_BASE(RX_BLANK_INTR_TIME, RX_BLANK_INTR_TIME_VAL);\r\nval |= CAS_BASE(RX_BLANK_INTR_PKT, RX_BLANK_INTR_PKT_VAL);\r\nwritel(val, cp->regs + REG_RX_BLANK);\r\n#else\r\nwritel(0x0, cp->regs + REG_RX_BLANK);\r\n#endif\r\nval = CAS_BASE(RX_AE_THRESH_COMP, RX_AE_COMP_VAL);\r\nwritel(val, cp->regs + REG_RX_AE_THRESH);\r\nif (cp->cas_flags & CAS_FLAG_REG_PLUS) {\r\nval = CAS_BASE(RX_AE1_THRESH_FREE, RX_AE_FREEN_VAL(1));\r\nwritel(val, cp->regs + REG_PLUS_RX_AE1_THRESH);\r\n}\r\nwritel(0x0, cp->regs + REG_RX_RED);\r\nval = 0;\r\nif (cp->page_size == 0x1000)\r\nval = 0x1;\r\nelse if (cp->page_size == 0x2000)\r\nval = 0x2;\r\nelse if (cp->page_size == 0x4000)\r\nval = 0x3;\r\nsize = cp->dev->mtu + 64;\r\nif (size > cp->page_size)\r\nsize = cp->page_size;\r\nif (size <= 0x400)\r\ni = 0x0;\r\nelse if (size <= 0x800)\r\ni = 0x1;\r\nelse if (size <= 0x1000)\r\ni = 0x2;\r\nelse\r\ni = 0x3;\r\ncp->mtu_stride = 1 << (i + 10);\r\nval = CAS_BASE(RX_PAGE_SIZE, val);\r\nval |= CAS_BASE(RX_PAGE_SIZE_MTU_STRIDE, i);\r\nval |= CAS_BASE(RX_PAGE_SIZE_MTU_COUNT, cp->page_size >> (i + 10));\r\nval |= CAS_BASE(RX_PAGE_SIZE_MTU_OFF, 0x1);\r\nwritel(val, cp->regs + REG_RX_PAGE_SIZE);\r\nif (CAS_HP_FIRMWARE == cas_prog_null)\r\nreturn;\r\nval = CAS_BASE(HP_CFG_NUM_CPU, CAS_NCPUS > 63 ? 0 : CAS_NCPUS);\r\nval |= HP_CFG_PARSE_EN | HP_CFG_SYN_INC_MASK;\r\nval |= CAS_BASE(HP_CFG_TCP_THRESH, HP_TCP_THRESH_VAL);\r\nwritel(val, cp->regs + REG_HP_CFG);\r\n}\r\nstatic inline void cas_rxc_init(struct cas_rx_comp *rxc)\r\n{\r\nmemset(rxc, 0, sizeof(*rxc));\r\nrxc->word4 = cpu_to_le64(RX_COMP4_ZERO);\r\n}\r\nstatic inline cas_page_t *cas_page_spare(struct cas *cp, const int index)\r\n{\r\ncas_page_t *page = cp->rx_pages[1][index];\r\ncas_page_t *new;\r\nif (page_count(page->buffer) == 1)\r\nreturn page;\r\nnew = cas_page_dequeue(cp);\r\nif (new) {\r\nspin_lock(&cp->rx_inuse_lock);\r\nlist_add(&page->list, &cp->rx_inuse_list);\r\nspin_unlock(&cp->rx_inuse_lock);\r\n}\r\nreturn new;\r\n}\r\nstatic cas_page_t *cas_page_swap(struct cas *cp, const int ring,\r\nconst int index)\r\n{\r\ncas_page_t **page0 = cp->rx_pages[0];\r\ncas_page_t **page1 = cp->rx_pages[1];\r\nif (page_count(page0[index]->buffer) > 1) {\r\ncas_page_t *new = cas_page_spare(cp, index);\r\nif (new) {\r\npage1[index] = page0[index];\r\npage0[index] = new;\r\n}\r\n}\r\nRX_USED_SET(page0[index], 0);\r\nreturn page0[index];\r\n}\r\nstatic void cas_clean_rxds(struct cas *cp)\r\n{\r\nstruct cas_rx_desc *rxd = cp->init_rxds[0];\r\nint i, size;\r\nfor (i = 0; i < N_RX_FLOWS; i++) {\r\nstruct sk_buff *skb;\r\nwhile ((skb = __skb_dequeue(&cp->rx_flows[i]))) {\r\ncas_skb_release(skb);\r\n}\r\n}\r\nsize = RX_DESC_RINGN_SIZE(0);\r\nfor (i = 0; i < size; i++) {\r\ncas_page_t *page = cas_page_swap(cp, 0, i);\r\nrxd[i].buffer = cpu_to_le64(page->dma_addr);\r\nrxd[i].index = cpu_to_le64(CAS_BASE(RX_INDEX_NUM, i) |\r\nCAS_BASE(RX_INDEX_RING, 0));\r\n}\r\ncp->rx_old[0] = RX_DESC_RINGN_SIZE(0) - 4;\r\ncp->rx_last[0] = 0;\r\ncp->cas_flags &= ~CAS_FLAG_RXD_POST(0);\r\n}\r\nstatic void cas_clean_rxcs(struct cas *cp)\r\n{\r\nint i, j;\r\nmemset(cp->rx_cur, 0, sizeof(*cp->rx_cur)*N_RX_COMP_RINGS);\r\nmemset(cp->rx_new, 0, sizeof(*cp->rx_new)*N_RX_COMP_RINGS);\r\nfor (i = 0; i < N_RX_COMP_RINGS; i++) {\r\nstruct cas_rx_comp *rxc = cp->init_rxcs[i];\r\nfor (j = 0; j < RX_COMP_RINGN_SIZE(i); j++) {\r\ncas_rxc_init(rxc + j);\r\n}\r\n}\r\n}\r\nstatic int cas_rxmac_interrupt(struct net_device *dev, struct cas *cp,\r\nu32 status)\r\n{\r\nu32 stat = readl(cp->regs + REG_MAC_RX_STATUS);\r\nif (!stat)\r\nreturn 0;\r\nnetif_dbg(cp, intr, cp->dev, "rxmac interrupt, stat: 0x%x\n", stat);\r\nspin_lock(&cp->stat_lock[0]);\r\nif (stat & MAC_RX_ALIGN_ERR)\r\ncp->net_stats[0].rx_frame_errors += 0x10000;\r\nif (stat & MAC_RX_CRC_ERR)\r\ncp->net_stats[0].rx_crc_errors += 0x10000;\r\nif (stat & MAC_RX_LEN_ERR)\r\ncp->net_stats[0].rx_length_errors += 0x10000;\r\nif (stat & MAC_RX_OVERFLOW) {\r\ncp->net_stats[0].rx_over_errors++;\r\ncp->net_stats[0].rx_fifo_errors++;\r\n}\r\nspin_unlock(&cp->stat_lock[0]);\r\nreturn 0;\r\n}\r\nstatic int cas_mac_interrupt(struct net_device *dev, struct cas *cp,\r\nu32 status)\r\n{\r\nu32 stat = readl(cp->regs + REG_MAC_CTRL_STATUS);\r\nif (!stat)\r\nreturn 0;\r\nnetif_printk(cp, intr, KERN_DEBUG, cp->dev,\r\n"mac interrupt, stat: 0x%x\n", stat);\r\nif (stat & MAC_CTRL_PAUSE_STATE)\r\ncp->pause_entered++;\r\nif (stat & MAC_CTRL_PAUSE_RECEIVED)\r\ncp->pause_last_time_recvd = (stat >> 16);\r\nreturn 0;\r\n}\r\nstatic inline int cas_mdio_link_not_up(struct cas *cp)\r\n{\r\nu16 val;\r\nswitch (cp->lstate) {\r\ncase link_force_ret:\r\nnetif_info(cp, link, cp->dev, "Autoneg failed again, keeping forced mode\n");\r\ncas_phy_write(cp, MII_BMCR, cp->link_fcntl);\r\ncp->timer_ticks = 5;\r\ncp->lstate = link_force_ok;\r\ncp->link_transition = LINK_TRANSITION_LINK_CONFIG;\r\nbreak;\r\ncase link_aneg:\r\nval = cas_phy_read(cp, MII_BMCR);\r\nval &= ~(BMCR_ANRESTART | BMCR_ANENABLE);\r\nval |= BMCR_FULLDPLX;\r\nval |= (cp->cas_flags & CAS_FLAG_1000MB_CAP) ?\r\nCAS_BMCR_SPEED1000 : BMCR_SPEED100;\r\ncas_phy_write(cp, MII_BMCR, val);\r\ncp->timer_ticks = 5;\r\ncp->lstate = link_force_try;\r\ncp->link_transition = LINK_TRANSITION_LINK_CONFIG;\r\nbreak;\r\ncase link_force_try:\r\nval = cas_phy_read(cp, MII_BMCR);\r\ncp->timer_ticks = 5;\r\nif (val & CAS_BMCR_SPEED1000) {\r\nval &= ~CAS_BMCR_SPEED1000;\r\nval |= (BMCR_SPEED100 | BMCR_FULLDPLX);\r\ncas_phy_write(cp, MII_BMCR, val);\r\nbreak;\r\n}\r\nif (val & BMCR_SPEED100) {\r\nif (val & BMCR_FULLDPLX)\r\nval &= ~BMCR_FULLDPLX;\r\nelse {\r\nval &= ~BMCR_SPEED100;\r\n}\r\ncas_phy_write(cp, MII_BMCR, val);\r\nbreak;\r\n}\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int cas_mii_link_check(struct cas *cp, const u16 bmsr)\r\n{\r\nint restart;\r\nif (bmsr & BMSR_LSTATUS) {\r\nif ((cp->lstate == link_force_try) &&\r\n(cp->link_cntl & BMCR_ANENABLE)) {\r\ncp->lstate = link_force_ret;\r\ncp->link_transition = LINK_TRANSITION_LINK_CONFIG;\r\ncas_mif_poll(cp, 0);\r\ncp->link_fcntl = cas_phy_read(cp, MII_BMCR);\r\ncp->timer_ticks = 5;\r\nif (cp->opened)\r\nnetif_info(cp, link, cp->dev,\r\n"Got link after fallback, retrying autoneg once...\n");\r\ncas_phy_write(cp, MII_BMCR,\r\ncp->link_fcntl | BMCR_ANENABLE |\r\nBMCR_ANRESTART);\r\ncas_mif_poll(cp, 1);\r\n} else if (cp->lstate != link_up) {\r\ncp->lstate = link_up;\r\ncp->link_transition = LINK_TRANSITION_LINK_UP;\r\nif (cp->opened) {\r\ncas_set_link_modes(cp);\r\nnetif_carrier_on(cp->dev);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nrestart = 0;\r\nif (cp->lstate == link_up) {\r\ncp->lstate = link_down;\r\ncp->link_transition = LINK_TRANSITION_LINK_DOWN;\r\nnetif_carrier_off(cp->dev);\r\nif (cp->opened)\r\nnetif_info(cp, link, cp->dev, "Link down\n");\r\nrestart = 1;\r\n} else if (++cp->timer_ticks > 10)\r\ncas_mdio_link_not_up(cp);\r\nreturn restart;\r\n}\r\nstatic int cas_mif_interrupt(struct net_device *dev, struct cas *cp,\r\nu32 status)\r\n{\r\nu32 stat = readl(cp->regs + REG_MIF_STATUS);\r\nu16 bmsr;\r\nif (CAS_VAL(MIF_STATUS_POLL_STATUS, stat) == 0)\r\nreturn 0;\r\nbmsr = CAS_VAL(MIF_STATUS_POLL_DATA, stat);\r\nreturn cas_mii_link_check(cp, bmsr);\r\n}\r\nstatic int cas_pci_interrupt(struct net_device *dev, struct cas *cp,\r\nu32 status)\r\n{\r\nu32 stat = readl(cp->regs + REG_PCI_ERR_STATUS);\r\nif (!stat)\r\nreturn 0;\r\nnetdev_err(dev, "PCI error [%04x:%04x]",\r\nstat, readl(cp->regs + REG_BIM_DIAG));\r\nif ((stat & PCI_ERR_BADACK) &&\r\n((cp->cas_flags & CAS_FLAG_REG_PLUS) == 0))\r\npr_cont(" <No ACK64# during ABS64 cycle>");\r\nif (stat & PCI_ERR_DTRTO)\r\npr_cont(" <Delayed transaction timeout>");\r\nif (stat & PCI_ERR_OTHER)\r\npr_cont(" <other>");\r\nif (stat & PCI_ERR_BIM_DMA_WRITE)\r\npr_cont(" <BIM DMA 0 write req>");\r\nif (stat & PCI_ERR_BIM_DMA_READ)\r\npr_cont(" <BIM DMA 0 read req>");\r\npr_cont("\n");\r\nif (stat & PCI_ERR_OTHER) {\r\nu16 cfg;\r\npci_read_config_word(cp->pdev, PCI_STATUS, &cfg);\r\nnetdev_err(dev, "Read PCI cfg space status [%04x]\n", cfg);\r\nif (cfg & PCI_STATUS_PARITY)\r\nnetdev_err(dev, "PCI parity error detected\n");\r\nif (cfg & PCI_STATUS_SIG_TARGET_ABORT)\r\nnetdev_err(dev, "PCI target abort\n");\r\nif (cfg & PCI_STATUS_REC_TARGET_ABORT)\r\nnetdev_err(dev, "PCI master acks target abort\n");\r\nif (cfg & PCI_STATUS_REC_MASTER_ABORT)\r\nnetdev_err(dev, "PCI master abort\n");\r\nif (cfg & PCI_STATUS_SIG_SYSTEM_ERROR)\r\nnetdev_err(dev, "PCI system error SERR#\n");\r\nif (cfg & PCI_STATUS_DETECTED_PARITY)\r\nnetdev_err(dev, "PCI parity error\n");\r\ncfg &= (PCI_STATUS_PARITY |\r\nPCI_STATUS_SIG_TARGET_ABORT |\r\nPCI_STATUS_REC_TARGET_ABORT |\r\nPCI_STATUS_REC_MASTER_ABORT |\r\nPCI_STATUS_SIG_SYSTEM_ERROR |\r\nPCI_STATUS_DETECTED_PARITY);\r\npci_write_config_word(cp->pdev, PCI_STATUS, cfg);\r\n}\r\nreturn 1;\r\n}\r\nstatic int cas_abnormal_irq(struct net_device *dev, struct cas *cp,\r\nu32 status)\r\n{\r\nif (status & INTR_RX_TAG_ERROR) {\r\nnetif_printk(cp, rx_err, KERN_DEBUG, cp->dev,\r\n"corrupt rx tag framing\n");\r\nspin_lock(&cp->stat_lock[0]);\r\ncp->net_stats[0].rx_errors++;\r\nspin_unlock(&cp->stat_lock[0]);\r\ngoto do_reset;\r\n}\r\nif (status & INTR_RX_LEN_MISMATCH) {\r\nnetif_printk(cp, rx_err, KERN_DEBUG, cp->dev,\r\n"length mismatch for rx frame\n");\r\nspin_lock(&cp->stat_lock[0]);\r\ncp->net_stats[0].rx_errors++;\r\nspin_unlock(&cp->stat_lock[0]);\r\ngoto do_reset;\r\n}\r\nif (status & INTR_PCS_STATUS) {\r\nif (cas_pcs_interrupt(dev, cp, status))\r\ngoto do_reset;\r\n}\r\nif (status & INTR_TX_MAC_STATUS) {\r\nif (cas_txmac_interrupt(dev, cp, status))\r\ngoto do_reset;\r\n}\r\nif (status & INTR_RX_MAC_STATUS) {\r\nif (cas_rxmac_interrupt(dev, cp, status))\r\ngoto do_reset;\r\n}\r\nif (status & INTR_MAC_CTRL_STATUS) {\r\nif (cas_mac_interrupt(dev, cp, status))\r\ngoto do_reset;\r\n}\r\nif (status & INTR_MIF_STATUS) {\r\nif (cas_mif_interrupt(dev, cp, status))\r\ngoto do_reset;\r\n}\r\nif (status & INTR_PCI_ERROR_STATUS) {\r\nif (cas_pci_interrupt(dev, cp, status))\r\ngoto do_reset;\r\n}\r\nreturn 0;\r\ndo_reset:\r\n#if 1\r\natomic_inc(&cp->reset_task_pending);\r\natomic_inc(&cp->reset_task_pending_all);\r\nnetdev_err(dev, "reset called in cas_abnormal_irq [0x%x]\n", status);\r\nschedule_work(&cp->reset_task);\r\n#else\r\natomic_set(&cp->reset_task_pending, CAS_RESET_ALL);\r\nnetdev_err(dev, "reset called in cas_abnormal_irq\n");\r\nschedule_work(&cp->reset_task);\r\n#endif\r\nreturn 1;\r\n}\r\nstatic inline int cas_calc_tabort(struct cas *cp, const unsigned long addr,\r\nconst int len)\r\n{\r\nunsigned long off = addr + len;\r\nif (CAS_TABORT(cp) == 1)\r\nreturn 0;\r\nif ((CAS_ROUND_PAGE(off) - off) > TX_TARGET_ABORT_LEN)\r\nreturn 0;\r\nreturn TX_TARGET_ABORT_LEN;\r\n}\r\nstatic inline void cas_tx_ringN(struct cas *cp, int ring, int limit)\r\n{\r\nstruct cas_tx_desc *txds;\r\nstruct sk_buff **skbs;\r\nstruct net_device *dev = cp->dev;\r\nint entry, count;\r\nspin_lock(&cp->tx_lock[ring]);\r\ntxds = cp->init_txds[ring];\r\nskbs = cp->tx_skbs[ring];\r\nentry = cp->tx_old[ring];\r\ncount = TX_BUFF_COUNT(ring, entry, limit);\r\nwhile (entry != limit) {\r\nstruct sk_buff *skb = skbs[entry];\r\ndma_addr_t daddr;\r\nu32 dlen;\r\nint frag;\r\nif (!skb) {\r\nentry = TX_DESC_NEXT(ring, entry);\r\ncontinue;\r\n}\r\ncount -= skb_shinfo(skb)->nr_frags +\r\n+ cp->tx_tiny_use[ring][entry].nbufs + 1;\r\nif (count < 0)\r\nbreak;\r\nnetif_printk(cp, tx_done, KERN_DEBUG, cp->dev,\r\n"tx[%d] done, slot %d\n", ring, entry);\r\nskbs[entry] = NULL;\r\ncp->tx_tiny_use[ring][entry].nbufs = 0;\r\nfor (frag = 0; frag <= skb_shinfo(skb)->nr_frags; frag++) {\r\nstruct cas_tx_desc *txd = txds + entry;\r\ndaddr = le64_to_cpu(txd->buffer);\r\ndlen = CAS_VAL(TX_DESC_BUFLEN,\r\nle64_to_cpu(txd->control));\r\npci_unmap_page(cp->pdev, daddr, dlen,\r\nPCI_DMA_TODEVICE);\r\nentry = TX_DESC_NEXT(ring, entry);\r\nif (cp->tx_tiny_use[ring][entry].used) {\r\ncp->tx_tiny_use[ring][entry].used = 0;\r\nentry = TX_DESC_NEXT(ring, entry);\r\n}\r\n}\r\nspin_lock(&cp->stat_lock[ring]);\r\ncp->net_stats[ring].tx_packets++;\r\ncp->net_stats[ring].tx_bytes += skb->len;\r\nspin_unlock(&cp->stat_lock[ring]);\r\ndev_kfree_skb_irq(skb);\r\n}\r\ncp->tx_old[ring] = entry;\r\nif (netif_queue_stopped(dev) &&\r\n(TX_BUFFS_AVAIL(cp, ring) > CAS_TABORT(cp)*(MAX_SKB_FRAGS + 1)))\r\nnetif_wake_queue(dev);\r\nspin_unlock(&cp->tx_lock[ring]);\r\n}\r\nstatic void cas_tx(struct net_device *dev, struct cas *cp,\r\nu32 status)\r\n{\r\nint limit, ring;\r\n#ifdef USE_TX_COMPWB\r\nu64 compwb = le64_to_cpu(cp->init_block->tx_compwb);\r\n#endif\r\nnetif_printk(cp, intr, KERN_DEBUG, cp->dev,\r\n"tx interrupt, status: 0x%x, %llx\n",\r\nstatus, (unsigned long long)compwb);\r\nfor (ring = 0; ring < N_TX_RINGS; ring++) {\r\n#ifdef USE_TX_COMPWB\r\nlimit = (CAS_VAL(TX_COMPWB_MSB, compwb) << 8) |\r\nCAS_VAL(TX_COMPWB_LSB, compwb);\r\ncompwb = TX_COMPWB_NEXT(compwb);\r\n#else\r\nlimit = readl(cp->regs + REG_TX_COMPN(ring));\r\n#endif\r\nif (cp->tx_old[ring] != limit)\r\ncas_tx_ringN(cp, ring, limit);\r\n}\r\n}\r\nstatic int cas_rx_process_pkt(struct cas *cp, struct cas_rx_comp *rxc,\r\nint entry, const u64 *words,\r\nstruct sk_buff **skbref)\r\n{\r\nint dlen, hlen, len, i, alloclen;\r\nint off, swivel = RX_SWIVEL_OFF_VAL;\r\nstruct cas_page *page;\r\nstruct sk_buff *skb;\r\nvoid *addr, *crcaddr;\r\n__sum16 csum;\r\nchar *p;\r\nhlen = CAS_VAL(RX_COMP2_HDR_SIZE, words[1]);\r\ndlen = CAS_VAL(RX_COMP1_DATA_SIZE, words[0]);\r\nlen = hlen + dlen;\r\nif (RX_COPY_ALWAYS || (words[2] & RX_COMP3_SMALL_PKT))\r\nalloclen = len;\r\nelse\r\nalloclen = max(hlen, RX_COPY_MIN);\r\nskb = netdev_alloc_skb(cp->dev, alloclen + swivel + cp->crc_size);\r\nif (skb == NULL)\r\nreturn -1;\r\n*skbref = skb;\r\nskb_reserve(skb, swivel);\r\np = skb->data;\r\naddr = crcaddr = NULL;\r\nif (hlen) {\r\ni = CAS_VAL(RX_COMP2_HDR_INDEX, words[1]);\r\npage = cp->rx_pages[CAS_VAL(RX_INDEX_RING, i)][CAS_VAL(RX_INDEX_NUM, i)];\r\noff = CAS_VAL(RX_COMP2_HDR_OFF, words[1]) * 0x100 +\r\nswivel;\r\ni = hlen;\r\nif (!dlen)\r\ni += cp->crc_size;\r\npci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr + off, i,\r\nPCI_DMA_FROMDEVICE);\r\naddr = cas_page_map(page->buffer);\r\nmemcpy(p, addr + off, i);\r\npci_dma_sync_single_for_device(cp->pdev, page->dma_addr + off, i,\r\nPCI_DMA_FROMDEVICE);\r\ncas_page_unmap(addr);\r\nRX_USED_ADD(page, 0x100);\r\np += hlen;\r\nswivel = 0;\r\n}\r\nif (alloclen < (hlen + dlen)) {\r\nskb_frag_t *frag = skb_shinfo(skb)->frags;\r\ni = CAS_VAL(RX_COMP1_DATA_INDEX, words[0]);\r\npage = cp->rx_pages[CAS_VAL(RX_INDEX_RING, i)][CAS_VAL(RX_INDEX_NUM, i)];\r\noff = CAS_VAL(RX_COMP1_DATA_OFF, words[0]) + swivel;\r\nhlen = min(cp->page_size - off, dlen);\r\nif (hlen < 0) {\r\nnetif_printk(cp, rx_err, KERN_DEBUG, cp->dev,\r\n"rx page overflow: %d\n", hlen);\r\ndev_kfree_skb_irq(skb);\r\nreturn -1;\r\n}\r\ni = hlen;\r\nif (i == dlen)\r\ni += cp->crc_size;\r\npci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr + off, i,\r\nPCI_DMA_FROMDEVICE);\r\nswivel = 0;\r\nif (p == (char *) skb->data) {\r\naddr = cas_page_map(page->buffer);\r\nmemcpy(p, addr + off, RX_COPY_MIN);\r\npci_dma_sync_single_for_device(cp->pdev, page->dma_addr + off, i,\r\nPCI_DMA_FROMDEVICE);\r\ncas_page_unmap(addr);\r\noff += RX_COPY_MIN;\r\nswivel = RX_COPY_MIN;\r\nRX_USED_ADD(page, cp->mtu_stride);\r\n} else {\r\nRX_USED_ADD(page, hlen);\r\n}\r\nskb_put(skb, alloclen);\r\nskb_shinfo(skb)->nr_frags++;\r\nskb->data_len += hlen - swivel;\r\nskb->truesize += hlen - swivel;\r\nskb->len += hlen - swivel;\r\n__skb_frag_set_page(frag, page->buffer);\r\n__skb_frag_ref(frag);\r\nfrag->page_offset = off;\r\nskb_frag_size_set(frag, hlen - swivel);\r\nif ((words[0] & RX_COMP1_SPLIT_PKT) && ((dlen -= hlen) > 0)) {\r\nhlen = dlen;\r\noff = 0;\r\ni = CAS_VAL(RX_COMP2_NEXT_INDEX, words[1]);\r\npage = cp->rx_pages[CAS_VAL(RX_INDEX_RING, i)][CAS_VAL(RX_INDEX_NUM, i)];\r\npci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr,\r\nhlen + cp->crc_size,\r\nPCI_DMA_FROMDEVICE);\r\npci_dma_sync_single_for_device(cp->pdev, page->dma_addr,\r\nhlen + cp->crc_size,\r\nPCI_DMA_FROMDEVICE);\r\nskb_shinfo(skb)->nr_frags++;\r\nskb->data_len += hlen;\r\nskb->len += hlen;\r\nfrag++;\r\n__skb_frag_set_page(frag, page->buffer);\r\n__skb_frag_ref(frag);\r\nfrag->page_offset = 0;\r\nskb_frag_size_set(frag, hlen);\r\nRX_USED_ADD(page, hlen + cp->crc_size);\r\n}\r\nif (cp->crc_size) {\r\naddr = cas_page_map(page->buffer);\r\ncrcaddr = addr + off + hlen;\r\n}\r\n} else {\r\nif (!dlen)\r\ngoto end_copy_pkt;\r\ni = CAS_VAL(RX_COMP1_DATA_INDEX, words[0]);\r\npage = cp->rx_pages[CAS_VAL(RX_INDEX_RING, i)][CAS_VAL(RX_INDEX_NUM, i)];\r\noff = CAS_VAL(RX_COMP1_DATA_OFF, words[0]) + swivel;\r\nhlen = min(cp->page_size - off, dlen);\r\nif (hlen < 0) {\r\nnetif_printk(cp, rx_err, KERN_DEBUG, cp->dev,\r\n"rx page overflow: %d\n", hlen);\r\ndev_kfree_skb_irq(skb);\r\nreturn -1;\r\n}\r\ni = hlen;\r\nif (i == dlen)\r\ni += cp->crc_size;\r\npci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr + off, i,\r\nPCI_DMA_FROMDEVICE);\r\naddr = cas_page_map(page->buffer);\r\nmemcpy(p, addr + off, i);\r\npci_dma_sync_single_for_device(cp->pdev, page->dma_addr + off, i,\r\nPCI_DMA_FROMDEVICE);\r\ncas_page_unmap(addr);\r\nif (p == (char *) skb->data)\r\nRX_USED_ADD(page, cp->mtu_stride);\r\nelse\r\nRX_USED_ADD(page, i);\r\nif ((words[0] & RX_COMP1_SPLIT_PKT) && ((dlen -= hlen) > 0)) {\r\np += hlen;\r\ni = CAS_VAL(RX_COMP2_NEXT_INDEX, words[1]);\r\npage = cp->rx_pages[CAS_VAL(RX_INDEX_RING, i)][CAS_VAL(RX_INDEX_NUM, i)];\r\npci_dma_sync_single_for_cpu(cp->pdev, page->dma_addr,\r\ndlen + cp->crc_size,\r\nPCI_DMA_FROMDEVICE);\r\naddr = cas_page_map(page->buffer);\r\nmemcpy(p, addr, dlen + cp->crc_size);\r\npci_dma_sync_single_for_device(cp->pdev, page->dma_addr,\r\ndlen + cp->crc_size,\r\nPCI_DMA_FROMDEVICE);\r\ncas_page_unmap(addr);\r\nRX_USED_ADD(page, dlen + cp->crc_size);\r\n}\r\nend_copy_pkt:\r\nif (cp->crc_size) {\r\naddr = NULL;\r\ncrcaddr = skb->data + alloclen;\r\n}\r\nskb_put(skb, alloclen);\r\n}\r\ncsum = (__force __sum16)htons(CAS_VAL(RX_COMP4_TCP_CSUM, words[3]));\r\nif (cp->crc_size) {\r\ncsum = csum_fold(csum_partial(crcaddr, cp->crc_size,\r\ncsum_unfold(csum)));\r\nif (addr)\r\ncas_page_unmap(addr);\r\n}\r\nskb->protocol = eth_type_trans(skb, cp->dev);\r\nif (skb->protocol == htons(ETH_P_IP)) {\r\nskb->csum = csum_unfold(~csum);\r\nskb->ip_summed = CHECKSUM_COMPLETE;\r\n} else\r\nskb_checksum_none_assert(skb);\r\nreturn len;\r\n}\r\nstatic inline void cas_rx_flow_pkt(struct cas *cp, const u64 *words,\r\nstruct sk_buff *skb)\r\n{\r\nint flowid = CAS_VAL(RX_COMP3_FLOWID, words[2]) & (N_RX_FLOWS - 1);\r\nstruct sk_buff_head *flow = &cp->rx_flows[flowid];\r\n__skb_queue_tail(flow, skb);\r\nif (words[0] & RX_COMP1_RELEASE_FLOW) {\r\nwhile ((skb = __skb_dequeue(flow))) {\r\ncas_skb_release(skb);\r\n}\r\n}\r\n}\r\nstatic void cas_post_page(struct cas *cp, const int ring, const int index)\r\n{\r\ncas_page_t *new;\r\nint entry;\r\nentry = cp->rx_old[ring];\r\nnew = cas_page_swap(cp, ring, index);\r\ncp->init_rxds[ring][entry].buffer = cpu_to_le64(new->dma_addr);\r\ncp->init_rxds[ring][entry].index =\r\ncpu_to_le64(CAS_BASE(RX_INDEX_NUM, index) |\r\nCAS_BASE(RX_INDEX_RING, ring));\r\nentry = RX_DESC_ENTRY(ring, entry + 1);\r\ncp->rx_old[ring] = entry;\r\nif (entry % 4)\r\nreturn;\r\nif (ring == 0)\r\nwritel(entry, cp->regs + REG_RX_KICK);\r\nelse if ((N_RX_DESC_RINGS > 1) &&\r\n(cp->cas_flags & CAS_FLAG_REG_PLUS))\r\nwritel(entry, cp->regs + REG_PLUS_RX_KICK1);\r\n}\r\nstatic int cas_post_rxds_ringN(struct cas *cp, int ring, int num)\r\n{\r\nunsigned int entry, last, count, released;\r\nint cluster;\r\ncas_page_t **page = cp->rx_pages[ring];\r\nentry = cp->rx_old[ring];\r\nnetif_printk(cp, intr, KERN_DEBUG, cp->dev,\r\n"rxd[%d] interrupt, done: %d\n", ring, entry);\r\ncluster = -1;\r\ncount = entry & 0x3;\r\nlast = RX_DESC_ENTRY(ring, num ? entry + num - 4: entry - 4);\r\nreleased = 0;\r\nwhile (entry != last) {\r\nif (page_count(page[entry]->buffer) > 1) {\r\ncas_page_t *new = cas_page_dequeue(cp);\r\nif (!new) {\r\ncp->cas_flags |= CAS_FLAG_RXD_POST(ring);\r\nif (!timer_pending(&cp->link_timer))\r\nmod_timer(&cp->link_timer, jiffies +\r\nCAS_LINK_FAST_TIMEOUT);\r\ncp->rx_old[ring] = entry;\r\ncp->rx_last[ring] = num ? num - released : 0;\r\nreturn -ENOMEM;\r\n}\r\nspin_lock(&cp->rx_inuse_lock);\r\nlist_add(&page[entry]->list, &cp->rx_inuse_list);\r\nspin_unlock(&cp->rx_inuse_lock);\r\ncp->init_rxds[ring][entry].buffer =\r\ncpu_to_le64(new->dma_addr);\r\npage[entry] = new;\r\n}\r\nif (++count == 4) {\r\ncluster = entry;\r\ncount = 0;\r\n}\r\nreleased++;\r\nentry = RX_DESC_ENTRY(ring, entry + 1);\r\n}\r\ncp->rx_old[ring] = entry;\r\nif (cluster < 0)\r\nreturn 0;\r\nif (ring == 0)\r\nwritel(cluster, cp->regs + REG_RX_KICK);\r\nelse if ((N_RX_DESC_RINGS > 1) &&\r\n(cp->cas_flags & CAS_FLAG_REG_PLUS))\r\nwritel(cluster, cp->regs + REG_PLUS_RX_KICK1);\r\nreturn 0;\r\n}\r\nstatic int cas_rx_ringN(struct cas *cp, int ring, int budget)\r\n{\r\nstruct cas_rx_comp *rxcs = cp->init_rxcs[ring];\r\nint entry, drops;\r\nint npackets = 0;\r\nnetif_printk(cp, intr, KERN_DEBUG, cp->dev,\r\n"rx[%d] interrupt, done: %d/%d\n",\r\nring,\r\nreadl(cp->regs + REG_RX_COMP_HEAD), cp->rx_new[ring]);\r\nentry = cp->rx_new[ring];\r\ndrops = 0;\r\nwhile (1) {\r\nstruct cas_rx_comp *rxc = rxcs + entry;\r\nstruct sk_buff *uninitialized_var(skb);\r\nint type, len;\r\nu64 words[4];\r\nint i, dring;\r\nwords[0] = le64_to_cpu(rxc->word1);\r\nwords[1] = le64_to_cpu(rxc->word2);\r\nwords[2] = le64_to_cpu(rxc->word3);\r\nwords[3] = le64_to_cpu(rxc->word4);\r\ntype = CAS_VAL(RX_COMP1_TYPE, words[0]);\r\nif (type == 0)\r\nbreak;\r\nif (words[3] & RX_COMP4_ZERO) {\r\nbreak;\r\n}\r\nif (words[3] & (RX_COMP4_LEN_MISMATCH | RX_COMP4_BAD)) {\r\nspin_lock(&cp->stat_lock[ring]);\r\ncp->net_stats[ring].rx_errors++;\r\nif (words[3] & RX_COMP4_LEN_MISMATCH)\r\ncp->net_stats[ring].rx_length_errors++;\r\nif (words[3] & RX_COMP4_BAD)\r\ncp->net_stats[ring].rx_crc_errors++;\r\nspin_unlock(&cp->stat_lock[ring]);\r\ndrop_it:\r\nspin_lock(&cp->stat_lock[ring]);\r\n++cp->net_stats[ring].rx_dropped;\r\nspin_unlock(&cp->stat_lock[ring]);\r\ngoto next;\r\n}\r\nlen = cas_rx_process_pkt(cp, rxc, entry, words, &skb);\r\nif (len < 0) {\r\n++drops;\r\ngoto drop_it;\r\n}\r\nif (RX_DONT_BATCH || (type == 0x2)) {\r\ncas_skb_release(skb);\r\n} else {\r\ncas_rx_flow_pkt(cp, words, skb);\r\n}\r\nspin_lock(&cp->stat_lock[ring]);\r\ncp->net_stats[ring].rx_packets++;\r\ncp->net_stats[ring].rx_bytes += len;\r\nspin_unlock(&cp->stat_lock[ring]);\r\nnext:\r\nnpackets++;\r\nif (words[0] & RX_COMP1_RELEASE_HDR) {\r\ni = CAS_VAL(RX_COMP2_HDR_INDEX, words[1]);\r\ndring = CAS_VAL(RX_INDEX_RING, i);\r\ni = CAS_VAL(RX_INDEX_NUM, i);\r\ncas_post_page(cp, dring, i);\r\n}\r\nif (words[0] & RX_COMP1_RELEASE_DATA) {\r\ni = CAS_VAL(RX_COMP1_DATA_INDEX, words[0]);\r\ndring = CAS_VAL(RX_INDEX_RING, i);\r\ni = CAS_VAL(RX_INDEX_NUM, i);\r\ncas_post_page(cp, dring, i);\r\n}\r\nif (words[0] & RX_COMP1_RELEASE_NEXT) {\r\ni = CAS_VAL(RX_COMP2_NEXT_INDEX, words[1]);\r\ndring = CAS_VAL(RX_INDEX_RING, i);\r\ni = CAS_VAL(RX_INDEX_NUM, i);\r\ncas_post_page(cp, dring, i);\r\n}\r\nentry = RX_COMP_ENTRY(ring, entry + 1 +\r\nCAS_VAL(RX_COMP1_SKIP, words[0]));\r\n#ifdef USE_NAPI\r\nif (budget && (npackets >= budget))\r\nbreak;\r\n#endif\r\n}\r\ncp->rx_new[ring] = entry;\r\nif (drops)\r\nnetdev_info(cp->dev, "Memory squeeze, deferring packet\n");\r\nreturn npackets;\r\n}\r\nstatic void cas_post_rxcs_ringN(struct net_device *dev,\r\nstruct cas *cp, int ring)\r\n{\r\nstruct cas_rx_comp *rxc = cp->init_rxcs[ring];\r\nint last, entry;\r\nlast = cp->rx_cur[ring];\r\nentry = cp->rx_new[ring];\r\nnetif_printk(cp, intr, KERN_DEBUG, dev,\r\n"rxc[%d] interrupt, done: %d/%d\n",\r\nring, readl(cp->regs + REG_RX_COMP_HEAD), entry);\r\nwhile (last != entry) {\r\ncas_rxc_init(rxc + last);\r\nlast = RX_COMP_ENTRY(ring, last + 1);\r\n}\r\ncp->rx_cur[ring] = last;\r\nif (ring == 0)\r\nwritel(last, cp->regs + REG_RX_COMP_TAIL);\r\nelse if (cp->cas_flags & CAS_FLAG_REG_PLUS)\r\nwritel(last, cp->regs + REG_PLUS_RX_COMPN_TAIL(ring));\r\n}\r\nstatic inline void cas_handle_irqN(struct net_device *dev,\r\nstruct cas *cp, const u32 status,\r\nconst int ring)\r\n{\r\nif (status & (INTR_RX_COMP_FULL_ALT | INTR_RX_COMP_AF_ALT))\r\ncas_post_rxcs_ringN(dev, cp, ring);\r\n}\r\nstatic irqreturn_t cas_interruptN(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev = dev_id;\r\nstruct cas *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nint ring = (irq == cp->pci_irq_INTC) ? 2 : 3;\r\nu32 status = readl(cp->regs + REG_PLUS_INTRN_STATUS(ring));\r\nif (status == 0)\r\nreturn IRQ_NONE;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nif (status & INTR_RX_DONE_ALT) {\r\n#ifdef USE_NAPI\r\ncas_mask_intr(cp);\r\nnapi_schedule(&cp->napi);\r\n#else\r\ncas_rx_ringN(cp, ring, 0);\r\n#endif\r\nstatus &= ~INTR_RX_DONE_ALT;\r\n}\r\nif (status)\r\ncas_handle_irqN(dev, cp, status, ring);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic inline void cas_handle_irq1(struct cas *cp, const u32 status)\r\n{\r\nif (status & INTR_RX_BUF_UNAVAIL_1) {\r\ncas_post_rxds_ringN(cp, 1, 0);\r\nspin_lock(&cp->stat_lock[1]);\r\ncp->net_stats[1].rx_dropped++;\r\nspin_unlock(&cp->stat_lock[1]);\r\n}\r\nif (status & INTR_RX_BUF_AE_1)\r\ncas_post_rxds_ringN(cp, 1, RX_DESC_RINGN_SIZE(1) -\r\nRX_AE_FREEN_VAL(1));\r\nif (status & (INTR_RX_COMP_AF | INTR_RX_COMP_FULL))\r\ncas_post_rxcs_ringN(cp, 1);\r\n}\r\nstatic irqreturn_t cas_interrupt1(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev = dev_id;\r\nstruct cas *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nu32 status = readl(cp->regs + REG_PLUS_INTRN_STATUS(1));\r\nif (status == 0)\r\nreturn IRQ_NONE;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nif (status & INTR_RX_DONE_ALT) {\r\n#ifdef USE_NAPI\r\ncas_mask_intr(cp);\r\nnapi_schedule(&cp->napi);\r\n#else\r\ncas_rx_ringN(cp, 1, 0);\r\n#endif\r\nstatus &= ~INTR_RX_DONE_ALT;\r\n}\r\nif (status)\r\ncas_handle_irq1(cp, status);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic inline void cas_handle_irq(struct net_device *dev,\r\nstruct cas *cp, const u32 status)\r\n{\r\nif (status & INTR_ERROR_MASK)\r\ncas_abnormal_irq(dev, cp, status);\r\nif (status & INTR_RX_BUF_UNAVAIL) {\r\ncas_post_rxds_ringN(cp, 0, 0);\r\nspin_lock(&cp->stat_lock[0]);\r\ncp->net_stats[0].rx_dropped++;\r\nspin_unlock(&cp->stat_lock[0]);\r\n} else if (status & INTR_RX_BUF_AE) {\r\ncas_post_rxds_ringN(cp, 0, RX_DESC_RINGN_SIZE(0) -\r\nRX_AE_FREEN_VAL(0));\r\n}\r\nif (status & (INTR_RX_COMP_AF | INTR_RX_COMP_FULL))\r\ncas_post_rxcs_ringN(dev, cp, 0);\r\n}\r\nstatic irqreturn_t cas_interrupt(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev = dev_id;\r\nstruct cas *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nu32 status = readl(cp->regs + REG_INTR_STATUS);\r\nif (status == 0)\r\nreturn IRQ_NONE;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nif (status & (INTR_TX_ALL | INTR_TX_INTME)) {\r\ncas_tx(dev, cp, status);\r\nstatus &= ~(INTR_TX_ALL | INTR_TX_INTME);\r\n}\r\nif (status & INTR_RX_DONE) {\r\n#ifdef USE_NAPI\r\ncas_mask_intr(cp);\r\nnapi_schedule(&cp->napi);\r\n#else\r\ncas_rx_ringN(cp, 0, 0);\r\n#endif\r\nstatus &= ~INTR_RX_DONE;\r\n}\r\nif (status)\r\ncas_handle_irq(dev, cp, status);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int cas_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct cas *cp = container_of(napi, struct cas, napi);\r\nstruct net_device *dev = cp->dev;\r\nint i, enable_intr, credits;\r\nu32 status = readl(cp->regs + REG_INTR_STATUS);\r\nunsigned long flags;\r\nspin_lock_irqsave(&cp->lock, flags);\r\ncas_tx(dev, cp, status);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nenable_intr = 1;\r\ncredits = 0;\r\nfor (i = 0; i < N_RX_COMP_RINGS; i++) {\r\nint j;\r\nfor (j = 0; j < N_RX_COMP_RINGS; j++) {\r\ncredits += cas_rx_ringN(cp, j, budget / N_RX_COMP_RINGS);\r\nif (credits >= budget) {\r\nenable_intr = 0;\r\ngoto rx_comp;\r\n}\r\n}\r\n}\r\nrx_comp:\r\nspin_lock_irqsave(&cp->lock, flags);\r\nif (status)\r\ncas_handle_irq(dev, cp, status);\r\n#ifdef USE_PCI_INTB\r\nif (N_RX_COMP_RINGS > 1) {\r\nstatus = readl(cp->regs + REG_PLUS_INTRN_STATUS(1));\r\nif (status)\r\ncas_handle_irq1(dev, cp, status);\r\n}\r\n#endif\r\n#ifdef USE_PCI_INTC\r\nif (N_RX_COMP_RINGS > 2) {\r\nstatus = readl(cp->regs + REG_PLUS_INTRN_STATUS(2));\r\nif (status)\r\ncas_handle_irqN(dev, cp, status, 2);\r\n}\r\n#endif\r\n#ifdef USE_PCI_INTD\r\nif (N_RX_COMP_RINGS > 3) {\r\nstatus = readl(cp->regs + REG_PLUS_INTRN_STATUS(3));\r\nif (status)\r\ncas_handle_irqN(dev, cp, status, 3);\r\n}\r\n#endif\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nif (enable_intr) {\r\nnapi_complete(napi);\r\ncas_unmask_intr(cp);\r\n}\r\nreturn credits;\r\n}\r\nstatic void cas_netpoll(struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\ncas_disable_irq(cp, 0);\r\ncas_interrupt(cp->pdev->irq, dev);\r\ncas_enable_irq(cp, 0);\r\n#ifdef USE_PCI_INTB\r\nif (N_RX_COMP_RINGS > 1) {\r\n}\r\n#endif\r\n#ifdef USE_PCI_INTC\r\nif (N_RX_COMP_RINGS > 2) {\r\n}\r\n#endif\r\n#ifdef USE_PCI_INTD\r\nif (N_RX_COMP_RINGS > 3) {\r\n}\r\n#endif\r\n}\r\nstatic void cas_tx_timeout(struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nnetdev_err(dev, "transmit timed out, resetting\n");\r\nif (!cp->hw_running) {\r\nnetdev_err(dev, "hrm.. hw not running!\n");\r\nreturn;\r\n}\r\nnetdev_err(dev, "MIF_STATE[%08x]\n",\r\nreadl(cp->regs + REG_MIF_STATE_MACHINE));\r\nnetdev_err(dev, "MAC_STATE[%08x]\n",\r\nreadl(cp->regs + REG_MAC_STATE_MACHINE));\r\nnetdev_err(dev, "TX_STATE[%08x:%08x:%08x] FIFO[%08x:%08x:%08x] SM1[%08x] SM2[%08x]\n",\r\nreadl(cp->regs + REG_TX_CFG),\r\nreadl(cp->regs + REG_MAC_TX_STATUS),\r\nreadl(cp->regs + REG_MAC_TX_CFG),\r\nreadl(cp->regs + REG_TX_FIFO_PKT_CNT),\r\nreadl(cp->regs + REG_TX_FIFO_WRITE_PTR),\r\nreadl(cp->regs + REG_TX_FIFO_READ_PTR),\r\nreadl(cp->regs + REG_TX_SM_1),\r\nreadl(cp->regs + REG_TX_SM_2));\r\nnetdev_err(dev, "RX_STATE[%08x:%08x:%08x]\n",\r\nreadl(cp->regs + REG_RX_CFG),\r\nreadl(cp->regs + REG_MAC_RX_STATUS),\r\nreadl(cp->regs + REG_MAC_RX_CFG));\r\nnetdev_err(dev, "HP_STATE[%08x:%08x:%08x:%08x]\n",\r\nreadl(cp->regs + REG_HP_STATE_MACHINE),\r\nreadl(cp->regs + REG_HP_STATUS0),\r\nreadl(cp->regs + REG_HP_STATUS1),\r\nreadl(cp->regs + REG_HP_STATUS2));\r\n#if 1\r\natomic_inc(&cp->reset_task_pending);\r\natomic_inc(&cp->reset_task_pending_all);\r\nschedule_work(&cp->reset_task);\r\n#else\r\natomic_set(&cp->reset_task_pending, CAS_RESET_ALL);\r\nschedule_work(&cp->reset_task);\r\n#endif\r\n}\r\nstatic inline int cas_intme(int ring, int entry)\r\n{\r\nif (!(entry & ((TX_DESC_RINGN_SIZE(ring) >> 1) - 1)))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void cas_write_txd(struct cas *cp, int ring, int entry,\r\ndma_addr_t mapping, int len, u64 ctrl, int last)\r\n{\r\nstruct cas_tx_desc *txd = cp->init_txds[ring] + entry;\r\nctrl |= CAS_BASE(TX_DESC_BUFLEN, len);\r\nif (cas_intme(ring, entry))\r\nctrl |= TX_DESC_INTME;\r\nif (last)\r\nctrl |= TX_DESC_EOF;\r\ntxd->control = cpu_to_le64(ctrl);\r\ntxd->buffer = cpu_to_le64(mapping);\r\n}\r\nstatic inline void *tx_tiny_buf(struct cas *cp, const int ring,\r\nconst int entry)\r\n{\r\nreturn cp->tx_tiny_bufs[ring] + TX_TINY_BUF_LEN*entry;\r\n}\r\nstatic inline dma_addr_t tx_tiny_map(struct cas *cp, const int ring,\r\nconst int entry, const int tentry)\r\n{\r\ncp->tx_tiny_use[ring][tentry].nbufs++;\r\ncp->tx_tiny_use[ring][entry].used = 1;\r\nreturn cp->tx_tiny_dvma[ring] + TX_TINY_BUF_LEN*entry;\r\n}\r\nstatic inline int cas_xmit_tx_ringN(struct cas *cp, int ring,\r\nstruct sk_buff *skb)\r\n{\r\nstruct net_device *dev = cp->dev;\r\nint entry, nr_frags, frag, tabort, tentry;\r\ndma_addr_t mapping;\r\nunsigned long flags;\r\nu64 ctrl;\r\nu32 len;\r\nspin_lock_irqsave(&cp->tx_lock[ring], flags);\r\nif (TX_BUFFS_AVAIL(cp, ring) <=\r\nCAS_TABORT(cp)*(skb_shinfo(skb)->nr_frags + 1)) {\r\nnetif_stop_queue(dev);\r\nspin_unlock_irqrestore(&cp->tx_lock[ring], flags);\r\nnetdev_err(dev, "BUG! Tx Ring full when queue awake!\n");\r\nreturn 1;\r\n}\r\nctrl = 0;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nconst u64 csum_start_off = skb_checksum_start_offset(skb);\r\nconst u64 csum_stuff_off = csum_start_off + skb->csum_offset;\r\nctrl = TX_DESC_CSUM_EN |\r\nCAS_BASE(TX_DESC_CSUM_START, csum_start_off) |\r\nCAS_BASE(TX_DESC_CSUM_STUFF, csum_stuff_off);\r\n}\r\nentry = cp->tx_new[ring];\r\ncp->tx_skbs[ring][entry] = skb;\r\nnr_frags = skb_shinfo(skb)->nr_frags;\r\nlen = skb_headlen(skb);\r\nmapping = pci_map_page(cp->pdev, virt_to_page(skb->data),\r\noffset_in_page(skb->data), len,\r\nPCI_DMA_TODEVICE);\r\ntentry = entry;\r\ntabort = cas_calc_tabort(cp, (unsigned long) skb->data, len);\r\nif (unlikely(tabort)) {\r\ncas_write_txd(cp, ring, entry, mapping, len - tabort,\r\nctrl | TX_DESC_SOF, 0);\r\nentry = TX_DESC_NEXT(ring, entry);\r\nskb_copy_from_linear_data_offset(skb, len - tabort,\r\ntx_tiny_buf(cp, ring, entry), tabort);\r\nmapping = tx_tiny_map(cp, ring, entry, tentry);\r\ncas_write_txd(cp, ring, entry, mapping, tabort, ctrl,\r\n(nr_frags == 0));\r\n} else {\r\ncas_write_txd(cp, ring, entry, mapping, len, ctrl |\r\nTX_DESC_SOF, (nr_frags == 0));\r\n}\r\nentry = TX_DESC_NEXT(ring, entry);\r\nfor (frag = 0; frag < nr_frags; frag++) {\r\nconst skb_frag_t *fragp = &skb_shinfo(skb)->frags[frag];\r\nlen = skb_frag_size(fragp);\r\nmapping = skb_frag_dma_map(&cp->pdev->dev, fragp, 0, len,\r\nDMA_TO_DEVICE);\r\ntabort = cas_calc_tabort(cp, fragp->page_offset, len);\r\nif (unlikely(tabort)) {\r\nvoid *addr;\r\ncas_write_txd(cp, ring, entry, mapping, len - tabort,\r\nctrl, 0);\r\nentry = TX_DESC_NEXT(ring, entry);\r\naddr = cas_page_map(skb_frag_page(fragp));\r\nmemcpy(tx_tiny_buf(cp, ring, entry),\r\naddr + fragp->page_offset + len - tabort,\r\ntabort);\r\ncas_page_unmap(addr);\r\nmapping = tx_tiny_map(cp, ring, entry, tentry);\r\nlen = tabort;\r\n}\r\ncas_write_txd(cp, ring, entry, mapping, len, ctrl,\r\n(frag + 1 == nr_frags));\r\nentry = TX_DESC_NEXT(ring, entry);\r\n}\r\ncp->tx_new[ring] = entry;\r\nif (TX_BUFFS_AVAIL(cp, ring) <= CAS_TABORT(cp)*(MAX_SKB_FRAGS + 1))\r\nnetif_stop_queue(dev);\r\nnetif_printk(cp, tx_queued, KERN_DEBUG, dev,\r\n"tx[%d] queued, slot %d, skblen %d, avail %d\n",\r\nring, entry, skb->len, TX_BUFFS_AVAIL(cp, ring));\r\nwritel(entry, cp->regs + REG_TX_KICKN(ring));\r\nspin_unlock_irqrestore(&cp->tx_lock[ring], flags);\r\nreturn 0;\r\n}\r\nstatic netdev_tx_t cas_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nstatic int ring;\r\nif (skb_padto(skb, cp->min_frame_size))\r\nreturn NETDEV_TX_OK;\r\nif (cas_xmit_tx_ringN(cp, ring++ & N_TX_RINGS_MASK, skb))\r\nreturn NETDEV_TX_BUSY;\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void cas_init_tx_dma(struct cas *cp)\r\n{\r\nu64 desc_dma = cp->block_dvma;\r\nunsigned long off;\r\nu32 val;\r\nint i;\r\n#ifdef USE_TX_COMPWB\r\noff = offsetof(struct cas_init_block, tx_compwb);\r\nwritel((desc_dma + off) >> 32, cp->regs + REG_TX_COMPWB_DB_HI);\r\nwritel((desc_dma + off) & 0xffffffff, cp->regs + REG_TX_COMPWB_DB_LOW);\r\n#endif\r\nval = TX_CFG_COMPWB_Q1 | TX_CFG_COMPWB_Q2 |\r\nTX_CFG_COMPWB_Q3 | TX_CFG_COMPWB_Q4 |\r\nTX_CFG_DMA_RDPIPE_DIS | TX_CFG_PACED_MODE |\r\nTX_CFG_INTR_COMPWB_DIS;\r\nfor (i = 0; i < MAX_TX_RINGS; i++) {\r\noff = (unsigned long) cp->init_txds[i] -\r\n(unsigned long) cp->init_block;\r\nval |= CAS_TX_RINGN_BASE(i);\r\nwritel((desc_dma + off) >> 32, cp->regs + REG_TX_DBN_HI(i));\r\nwritel((desc_dma + off) & 0xffffffff, cp->regs +\r\nREG_TX_DBN_LOW(i));\r\n}\r\nwritel(val, cp->regs + REG_TX_CFG);\r\n#ifdef USE_QOS\r\nwritel(0x800, cp->regs + REG_TX_MAXBURST_0);\r\nwritel(0x1600, cp->regs + REG_TX_MAXBURST_1);\r\nwritel(0x2400, cp->regs + REG_TX_MAXBURST_2);\r\nwritel(0x4800, cp->regs + REG_TX_MAXBURST_3);\r\n#else\r\nwritel(0x800, cp->regs + REG_TX_MAXBURST_0);\r\nwritel(0x800, cp->regs + REG_TX_MAXBURST_1);\r\nwritel(0x800, cp->regs + REG_TX_MAXBURST_2);\r\nwritel(0x800, cp->regs + REG_TX_MAXBURST_3);\r\n#endif\r\n}\r\nstatic inline void cas_init_dma(struct cas *cp)\r\n{\r\ncas_init_tx_dma(cp);\r\ncas_init_rx_dma(cp);\r\n}\r\nstatic void cas_process_mc_list(struct cas *cp)\r\n{\r\nu16 hash_table[16];\r\nu32 crc;\r\nstruct netdev_hw_addr *ha;\r\nint i = 1;\r\nmemset(hash_table, 0, sizeof(hash_table));\r\nnetdev_for_each_mc_addr(ha, cp->dev) {\r\nif (i <= CAS_MC_EXACT_MATCH_SIZE) {\r\nwritel((ha->addr[4] << 8) | ha->addr[5],\r\ncp->regs + REG_MAC_ADDRN(i*3 + 0));\r\nwritel((ha->addr[2] << 8) | ha->addr[3],\r\ncp->regs + REG_MAC_ADDRN(i*3 + 1));\r\nwritel((ha->addr[0] << 8) | ha->addr[1],\r\ncp->regs + REG_MAC_ADDRN(i*3 + 2));\r\ni++;\r\n}\r\nelse {\r\ncrc = ether_crc_le(ETH_ALEN, ha->addr);\r\ncrc >>= 24;\r\nhash_table[crc >> 4] |= 1 << (15 - (crc & 0xf));\r\n}\r\n}\r\nfor (i = 0; i < 16; i++)\r\nwritel(hash_table[i], cp->regs + REG_MAC_HASH_TABLEN(i));\r\n}\r\nstatic u32 cas_setup_multicast(struct cas *cp)\r\n{\r\nu32 rxcfg = 0;\r\nint i;\r\nif (cp->dev->flags & IFF_PROMISC) {\r\nrxcfg |= MAC_RX_CFG_PROMISC_EN;\r\n} else if (cp->dev->flags & IFF_ALLMULTI) {\r\nfor (i=0; i < 16; i++)\r\nwritel(0xFFFF, cp->regs + REG_MAC_HASH_TABLEN(i));\r\nrxcfg |= MAC_RX_CFG_HASH_FILTER_EN;\r\n} else {\r\ncas_process_mc_list(cp);\r\nrxcfg |= MAC_RX_CFG_HASH_FILTER_EN;\r\n}\r\nreturn rxcfg;\r\n}\r\nstatic void cas_clear_mac_err(struct cas *cp)\r\n{\r\nwritel(0, cp->regs + REG_MAC_COLL_NORMAL);\r\nwritel(0, cp->regs + REG_MAC_COLL_FIRST);\r\nwritel(0, cp->regs + REG_MAC_COLL_EXCESS);\r\nwritel(0, cp->regs + REG_MAC_COLL_LATE);\r\nwritel(0, cp->regs + REG_MAC_TIMER_DEFER);\r\nwritel(0, cp->regs + REG_MAC_ATTEMPTS_PEAK);\r\nwritel(0, cp->regs + REG_MAC_RECV_FRAME);\r\nwritel(0, cp->regs + REG_MAC_LEN_ERR);\r\nwritel(0, cp->regs + REG_MAC_ALIGN_ERR);\r\nwritel(0, cp->regs + REG_MAC_FCS_ERR);\r\nwritel(0, cp->regs + REG_MAC_RX_CODE_ERR);\r\n}\r\nstatic void cas_mac_reset(struct cas *cp)\r\n{\r\nint i;\r\nwritel(0x1, cp->regs + REG_MAC_TX_RESET);\r\nwritel(0x1, cp->regs + REG_MAC_RX_RESET);\r\ni = STOP_TRIES;\r\nwhile (i-- > 0) {\r\nif (readl(cp->regs + REG_MAC_TX_RESET) == 0)\r\nbreak;\r\nudelay(10);\r\n}\r\ni = STOP_TRIES;\r\nwhile (i-- > 0) {\r\nif (readl(cp->regs + REG_MAC_RX_RESET) == 0)\r\nbreak;\r\nudelay(10);\r\n}\r\nif (readl(cp->regs + REG_MAC_TX_RESET) |\r\nreadl(cp->regs + REG_MAC_RX_RESET))\r\nnetdev_err(cp->dev, "mac tx[%d]/rx[%d] reset failed [%08x]\n",\r\nreadl(cp->regs + REG_MAC_TX_RESET),\r\nreadl(cp->regs + REG_MAC_RX_RESET),\r\nreadl(cp->regs + REG_MAC_STATE_MACHINE));\r\n}\r\nstatic void cas_init_mac(struct cas *cp)\r\n{\r\nunsigned char *e = &cp->dev->dev_addr[0];\r\nint i;\r\ncas_mac_reset(cp);\r\nwritel(CAWR_RR_DIS, cp->regs + REG_CAWR);\r\n#if !defined(CONFIG_SPARC64) && !defined(CONFIG_ALPHA)\r\nif ((cp->cas_flags & CAS_FLAG_TARGET_ABORT) == 0)\r\nwritel(INF_BURST_EN, cp->regs + REG_INF_BURST);\r\n#endif\r\nwritel(0x1BF0, cp->regs + REG_MAC_SEND_PAUSE);\r\nwritel(0x00, cp->regs + REG_MAC_IPG0);\r\nwritel(0x08, cp->regs + REG_MAC_IPG1);\r\nwritel(0x04, cp->regs + REG_MAC_IPG2);\r\nwritel(0x40, cp->regs + REG_MAC_SLOT_TIME);\r\nwritel(ETH_ZLEN + 4, cp->regs + REG_MAC_FRAMESIZE_MIN);\r\nwritel(CAS_BASE(MAC_FRAMESIZE_MAX_BURST, 0x2000) |\r\nCAS_BASE(MAC_FRAMESIZE_MAX_FRAME,\r\n(CAS_MAX_MTU + ETH_HLEN + 4 + 4)),\r\ncp->regs + REG_MAC_FRAMESIZE_MAX);\r\nif ((cp->cas_flags & CAS_FLAG_SATURN) && cp->crc_size)\r\nwritel(0x41, cp->regs + REG_MAC_PA_SIZE);\r\nelse\r\nwritel(0x07, cp->regs + REG_MAC_PA_SIZE);\r\nwritel(0x04, cp->regs + REG_MAC_JAM_SIZE);\r\nwritel(0x10, cp->regs + REG_MAC_ATTEMPT_LIMIT);\r\nwritel(0x8808, cp->regs + REG_MAC_CTRL_TYPE);\r\nwritel((e[5] | (e[4] << 8)) & 0x3ff, cp->regs + REG_MAC_RANDOM_SEED);\r\nwritel(0, cp->regs + REG_MAC_ADDR_FILTER0);\r\nwritel(0, cp->regs + REG_MAC_ADDR_FILTER1);\r\nwritel(0, cp->regs + REG_MAC_ADDR_FILTER2);\r\nwritel(0, cp->regs + REG_MAC_ADDR_FILTER2_1_MASK);\r\nwritel(0, cp->regs + REG_MAC_ADDR_FILTER0_MASK);\r\nfor (i = 0; i < 45; i++)\r\nwritel(0x0, cp->regs + REG_MAC_ADDRN(i));\r\nwritel((e[4] << 8) | e[5], cp->regs + REG_MAC_ADDRN(0));\r\nwritel((e[2] << 8) | e[3], cp->regs + REG_MAC_ADDRN(1));\r\nwritel((e[0] << 8) | e[1], cp->regs + REG_MAC_ADDRN(2));\r\nwritel(0x0001, cp->regs + REG_MAC_ADDRN(42));\r\nwritel(0xc200, cp->regs + REG_MAC_ADDRN(43));\r\nwritel(0x0180, cp->regs + REG_MAC_ADDRN(44));\r\ncp->mac_rx_cfg = cas_setup_multicast(cp);\r\nspin_lock(&cp->stat_lock[N_TX_RINGS]);\r\ncas_clear_mac_err(cp);\r\nspin_unlock(&cp->stat_lock[N_TX_RINGS]);\r\nwritel(MAC_TX_FRAME_XMIT, cp->regs + REG_MAC_TX_MASK);\r\nwritel(MAC_RX_FRAME_RECV, cp->regs + REG_MAC_RX_MASK);\r\nwritel(0xffffffff, cp->regs + REG_MAC_CTRL_MASK);\r\n}\r\nstatic void cas_init_pause_thresholds(struct cas *cp)\r\n{\r\nif (cp->rx_fifo_size <= (2 * 1024)) {\r\ncp->rx_pause_off = cp->rx_pause_on = cp->rx_fifo_size;\r\n} else {\r\nint max_frame = (cp->dev->mtu + ETH_HLEN + 4 + 4 + 64) & ~63;\r\nif (max_frame * 3 > cp->rx_fifo_size) {\r\ncp->rx_pause_off = 7104;\r\ncp->rx_pause_on = 960;\r\n} else {\r\nint off = (cp->rx_fifo_size - (max_frame * 2));\r\nint on = off - max_frame;\r\ncp->rx_pause_off = off;\r\ncp->rx_pause_on = on;\r\n}\r\n}\r\n}\r\nstatic int cas_vpd_match(const void __iomem *p, const char *str)\r\n{\r\nint len = strlen(str) + 1;\r\nint i;\r\nfor (i = 0; i < len; i++) {\r\nif (readb(p + i) != str[i])\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int cas_get_vpd_info(struct cas *cp, unsigned char *dev_addr,\r\nconst int offset)\r\n{\r\nvoid __iomem *p = cp->regs + REG_EXPANSION_ROM_RUN_START;\r\nvoid __iomem *base, *kstart;\r\nint i, len;\r\nint found = 0;\r\n#define VPD_FOUND_MAC 0x01\r\n#define VPD_FOUND_PHY 0x02\r\nint phy_type = CAS_PHY_MII_MDIO0;\r\nint mac_off = 0;\r\n#if defined(CONFIG_SPARC)\r\nconst unsigned char *addr;\r\n#endif\r\nwritel(BIM_LOCAL_DEV_PROM | BIM_LOCAL_DEV_PAD,\r\ncp->regs + REG_BIM_LOCAL_DEV_EN);\r\nif (readb(p) != 0x55 || readb(p + 1) != 0xaa)\r\ngoto use_random_mac_addr;\r\nbase = NULL;\r\nfor (i = 2; i < EXPANSION_ROM_SIZE; i++) {\r\nif ((readb(p + i + 0) == 0x50) &&\r\n(readb(p + i + 1) == 0x43) &&\r\n(readb(p + i + 2) == 0x49) &&\r\n(readb(p + i + 3) == 0x52)) {\r\nbase = p + (readb(p + i + 8) |\r\n(readb(p + i + 9) << 8));\r\nbreak;\r\n}\r\n}\r\nif (!base || (readb(base) != 0x82))\r\ngoto use_random_mac_addr;\r\ni = (readb(base + 1) | (readb(base + 2) << 8)) + 3;\r\nwhile (i < EXPANSION_ROM_SIZE) {\r\nif (readb(base + i) != 0x90)\r\ngoto use_random_mac_addr;\r\nlen = readb(base + i + 1) | (readb(base + i + 2) << 8);\r\nkstart = base + i + 3;\r\np = kstart;\r\nwhile ((p - kstart) < len) {\r\nint klen = readb(p + 2);\r\nint j;\r\nchar type;\r\np += 3;\r\nif (readb(p) != 'I')\r\ngoto next;\r\ntype = readb(p + 3);\r\nif (type == 'B') {\r\nif ((klen == 29) && readb(p + 4) == 6 &&\r\ncas_vpd_match(p + 5,\r\n"local-mac-address")) {\r\nif (mac_off++ > offset)\r\ngoto next;\r\nfor (j = 0; j < 6; j++)\r\ndev_addr[j] =\r\nreadb(p + 23 + j);\r\ngoto found_mac;\r\n}\r\n}\r\nif (type != 'S')\r\ngoto next;\r\n#ifdef USE_ENTROPY_DEV\r\nif ((klen == 24) &&\r\ncas_vpd_match(p + 5, "entropy-dev") &&\r\ncas_vpd_match(p + 17, "vms110")) {\r\ncp->cas_flags |= CAS_FLAG_ENTROPY_DEV;\r\ngoto next;\r\n}\r\n#endif\r\nif (found & VPD_FOUND_PHY)\r\ngoto next;\r\nif ((klen == 18) && readb(p + 4) == 4 &&\r\ncas_vpd_match(p + 5, "phy-type")) {\r\nif (cas_vpd_match(p + 14, "pcs")) {\r\nphy_type = CAS_PHY_SERDES;\r\ngoto found_phy;\r\n}\r\n}\r\nif ((klen == 23) && readb(p + 4) == 4 &&\r\ncas_vpd_match(p + 5, "phy-interface")) {\r\nif (cas_vpd_match(p + 19, "pcs")) {\r\nphy_type = CAS_PHY_SERDES;\r\ngoto found_phy;\r\n}\r\n}\r\nfound_mac:\r\nfound |= VPD_FOUND_MAC;\r\ngoto next;\r\nfound_phy:\r\nfound |= VPD_FOUND_PHY;\r\nnext:\r\np += klen;\r\n}\r\ni += len + 3;\r\n}\r\nuse_random_mac_addr:\r\nif (found & VPD_FOUND_MAC)\r\ngoto done;\r\n#if defined(CONFIG_SPARC)\r\naddr = of_get_property(cp->of_node, "local-mac-address", NULL);\r\nif (addr != NULL) {\r\nmemcpy(dev_addr, addr, 6);\r\ngoto done;\r\n}\r\n#endif\r\npr_info("MAC address not found in ROM VPD\n");\r\ndev_addr[0] = 0x08;\r\ndev_addr[1] = 0x00;\r\ndev_addr[2] = 0x20;\r\nget_random_bytes(dev_addr + 3, 3);\r\ndone:\r\nwritel(0, cp->regs + REG_BIM_LOCAL_DEV_EN);\r\nreturn phy_type;\r\n}\r\nstatic void cas_check_pci_invariants(struct cas *cp)\r\n{\r\nstruct pci_dev *pdev = cp->pdev;\r\ncp->cas_flags = 0;\r\nif ((pdev->vendor == PCI_VENDOR_ID_SUN) &&\r\n(pdev->device == PCI_DEVICE_ID_SUN_CASSINI)) {\r\nif (pdev->revision >= CAS_ID_REVPLUS)\r\ncp->cas_flags |= CAS_FLAG_REG_PLUS;\r\nif (pdev->revision < CAS_ID_REVPLUS02u)\r\ncp->cas_flags |= CAS_FLAG_TARGET_ABORT;\r\nif (pdev->revision < CAS_ID_REV2)\r\ncp->cas_flags |= CAS_FLAG_NO_HW_CSUM;\r\n} else {\r\ncp->cas_flags |= CAS_FLAG_REG_PLUS;\r\nif ((pdev->vendor == PCI_VENDOR_ID_NS) &&\r\n(pdev->device == PCI_DEVICE_ID_NS_SATURN))\r\ncp->cas_flags |= CAS_FLAG_SATURN;\r\n}\r\n}\r\nstatic int cas_check_invariants(struct cas *cp)\r\n{\r\nstruct pci_dev *pdev = cp->pdev;\r\nu32 cfg;\r\nint i;\r\ncp->page_order = 0;\r\n#ifdef USE_PAGE_ORDER\r\nif (PAGE_SHIFT < CAS_JUMBO_PAGE_SHIFT) {\r\nstruct page *page = alloc_pages(GFP_ATOMIC,\r\nCAS_JUMBO_PAGE_SHIFT -\r\nPAGE_SHIFT);\r\nif (page) {\r\n__free_pages(page, CAS_JUMBO_PAGE_SHIFT - PAGE_SHIFT);\r\ncp->page_order = CAS_JUMBO_PAGE_SHIFT - PAGE_SHIFT;\r\n} else {\r\nprintk("MTU limited to %d bytes\n", CAS_MAX_MTU);\r\n}\r\n}\r\n#endif\r\ncp->page_size = (PAGE_SIZE << cp->page_order);\r\ncp->tx_fifo_size = readl(cp->regs + REG_TX_FIFO_SIZE) * 64;\r\ncp->rx_fifo_size = RX_FIFO_SIZE;\r\ncp->phy_type = cas_get_vpd_info(cp, cp->dev->dev_addr,\r\nPCI_SLOT(pdev->devfn));\r\nif (cp->phy_type & CAS_PHY_SERDES) {\r\ncp->cas_flags |= CAS_FLAG_1000MB_CAP;\r\nreturn 0;\r\n}\r\ncfg = readl(cp->regs + REG_MIF_CFG);\r\nif (cfg & MIF_CFG_MDIO_1) {\r\ncp->phy_type = CAS_PHY_MII_MDIO1;\r\n} else if (cfg & MIF_CFG_MDIO_0) {\r\ncp->phy_type = CAS_PHY_MII_MDIO0;\r\n}\r\ncas_mif_poll(cp, 0);\r\nwritel(PCS_DATAPATH_MODE_MII, cp->regs + REG_PCS_DATAPATH_MODE);\r\nfor (i = 0; i < 32; i++) {\r\nu32 phy_id;\r\nint j;\r\nfor (j = 0; j < 3; j++) {\r\ncp->phy_addr = i;\r\nphy_id = cas_phy_read(cp, MII_PHYSID1) << 16;\r\nphy_id |= cas_phy_read(cp, MII_PHYSID2);\r\nif (phy_id && (phy_id != 0xFFFFFFFF)) {\r\ncp->phy_id = phy_id;\r\ngoto done;\r\n}\r\n}\r\n}\r\npr_err("MII phy did not respond [%08x]\n",\r\nreadl(cp->regs + REG_MIF_STATE_MACHINE));\r\nreturn -1;\r\ndone:\r\ncfg = cas_phy_read(cp, MII_BMSR);\r\nif ((cfg & CAS_BMSR_1000_EXTEND) &&\r\ncas_phy_read(cp, CAS_MII_1000_EXTEND))\r\ncp->cas_flags |= CAS_FLAG_1000MB_CAP;\r\nreturn 0;\r\n}\r\nstatic inline void cas_start_dma(struct cas *cp)\r\n{\r\nint i;\r\nu32 val;\r\nint txfailed = 0;\r\nval = readl(cp->regs + REG_TX_CFG) | TX_CFG_DMA_EN;\r\nwritel(val, cp->regs + REG_TX_CFG);\r\nval = readl(cp->regs + REG_RX_CFG) | RX_CFG_DMA_EN;\r\nwritel(val, cp->regs + REG_RX_CFG);\r\nval = readl(cp->regs + REG_MAC_TX_CFG) | MAC_TX_CFG_EN;\r\nwritel(val, cp->regs + REG_MAC_TX_CFG);\r\nval = readl(cp->regs + REG_MAC_RX_CFG) | MAC_RX_CFG_EN;\r\nwritel(val, cp->regs + REG_MAC_RX_CFG);\r\ni = STOP_TRIES;\r\nwhile (i-- > 0) {\r\nval = readl(cp->regs + REG_MAC_TX_CFG);\r\nif ((val & MAC_TX_CFG_EN))\r\nbreak;\r\nudelay(10);\r\n}\r\nif (i < 0) txfailed = 1;\r\ni = STOP_TRIES;\r\nwhile (i-- > 0) {\r\nval = readl(cp->regs + REG_MAC_RX_CFG);\r\nif ((val & MAC_RX_CFG_EN)) {\r\nif (txfailed) {\r\nnetdev_err(cp->dev,\r\n"enabling mac failed [tx:%08x:%08x]\n",\r\nreadl(cp->regs + REG_MIF_STATE_MACHINE),\r\nreadl(cp->regs + REG_MAC_STATE_MACHINE));\r\n}\r\ngoto enable_rx_done;\r\n}\r\nudelay(10);\r\n}\r\nnetdev_err(cp->dev, "enabling mac failed [%s:%08x:%08x]\n",\r\n(txfailed ? "tx,rx" : "rx"),\r\nreadl(cp->regs + REG_MIF_STATE_MACHINE),\r\nreadl(cp->regs + REG_MAC_STATE_MACHINE));\r\nenable_rx_done:\r\ncas_unmask_intr(cp);\r\nwritel(RX_DESC_RINGN_SIZE(0) - 4, cp->regs + REG_RX_KICK);\r\nwritel(0, cp->regs + REG_RX_COMP_TAIL);\r\nif (cp->cas_flags & CAS_FLAG_REG_PLUS) {\r\nif (N_RX_DESC_RINGS > 1)\r\nwritel(RX_DESC_RINGN_SIZE(1) - 4,\r\ncp->regs + REG_PLUS_RX_KICK1);\r\nfor (i = 1; i < N_RX_COMP_RINGS; i++)\r\nwritel(0, cp->regs + REG_PLUS_RX_COMPN_TAIL(i));\r\n}\r\n}\r\nstatic void cas_read_pcs_link_mode(struct cas *cp, int *fd, int *spd,\r\nint *pause)\r\n{\r\nu32 val = readl(cp->regs + REG_PCS_MII_LPA);\r\n*fd = (val & PCS_MII_LPA_FD) ? 1 : 0;\r\n*pause = (val & PCS_MII_LPA_SYM_PAUSE) ? 0x01 : 0x00;\r\nif (val & PCS_MII_LPA_ASYM_PAUSE)\r\n*pause |= 0x10;\r\n*spd = 1000;\r\n}\r\nstatic void cas_read_mii_link_mode(struct cas *cp, int *fd, int *spd,\r\nint *pause)\r\n{\r\nu32 val;\r\n*fd = 0;\r\n*spd = 10;\r\n*pause = 0;\r\nval = cas_phy_read(cp, MII_LPA);\r\nif (val & CAS_LPA_PAUSE)\r\n*pause = 0x01;\r\nif (val & CAS_LPA_ASYM_PAUSE)\r\n*pause |= 0x10;\r\nif (val & LPA_DUPLEX)\r\n*fd = 1;\r\nif (val & LPA_100)\r\n*spd = 100;\r\nif (cp->cas_flags & CAS_FLAG_1000MB_CAP) {\r\nval = cas_phy_read(cp, CAS_MII_1000_STATUS);\r\nif (val & (CAS_LPA_1000FULL | CAS_LPA_1000HALF))\r\n*spd = 1000;\r\nif (val & CAS_LPA_1000FULL)\r\n*fd = 1;\r\n}\r\n}\r\nstatic void cas_set_link_modes(struct cas *cp)\r\n{\r\nu32 val;\r\nint full_duplex, speed, pause;\r\nfull_duplex = 0;\r\nspeed = 10;\r\npause = 0;\r\nif (CAS_PHY_MII(cp->phy_type)) {\r\ncas_mif_poll(cp, 0);\r\nval = cas_phy_read(cp, MII_BMCR);\r\nif (val & BMCR_ANENABLE) {\r\ncas_read_mii_link_mode(cp, &full_duplex, &speed,\r\n&pause);\r\n} else {\r\nif (val & BMCR_FULLDPLX)\r\nfull_duplex = 1;\r\nif (val & BMCR_SPEED100)\r\nspeed = 100;\r\nelse if (val & CAS_BMCR_SPEED1000)\r\nspeed = (cp->cas_flags & CAS_FLAG_1000MB_CAP) ?\r\n1000 : 100;\r\n}\r\ncas_mif_poll(cp, 1);\r\n} else {\r\nval = readl(cp->regs + REG_PCS_MII_CTRL);\r\ncas_read_pcs_link_mode(cp, &full_duplex, &speed, &pause);\r\nif ((val & PCS_MII_AUTONEG_EN) == 0) {\r\nif (val & PCS_MII_CTRL_DUPLEX)\r\nfull_duplex = 1;\r\n}\r\n}\r\nnetif_info(cp, link, cp->dev, "Link up at %d Mbps, %s-duplex\n",\r\nspeed, full_duplex ? "full" : "half");\r\nval = MAC_XIF_TX_MII_OUTPUT_EN | MAC_XIF_LINK_LED;\r\nif (CAS_PHY_MII(cp->phy_type)) {\r\nval |= MAC_XIF_MII_BUFFER_OUTPUT_EN;\r\nif (!full_duplex)\r\nval |= MAC_XIF_DISABLE_ECHO;\r\n}\r\nif (full_duplex)\r\nval |= MAC_XIF_FDPLX_LED;\r\nif (speed == 1000)\r\nval |= MAC_XIF_GMII_MODE;\r\nwritel(val, cp->regs + REG_MAC_XIF_CFG);\r\nval = MAC_TX_CFG_IPG_EN;\r\nif (full_duplex) {\r\nval |= MAC_TX_CFG_IGNORE_CARRIER;\r\nval |= MAC_TX_CFG_IGNORE_COLL;\r\n} else {\r\n#ifndef USE_CSMA_CD_PROTO\r\nval |= MAC_TX_CFG_NEVER_GIVE_UP_EN;\r\nval |= MAC_TX_CFG_NEVER_GIVE_UP_LIM;\r\n#endif\r\n}\r\nif ((speed == 1000) && !full_duplex) {\r\nwritel(val | MAC_TX_CFG_CARRIER_EXTEND,\r\ncp->regs + REG_MAC_TX_CFG);\r\nval = readl(cp->regs + REG_MAC_RX_CFG);\r\nval &= ~MAC_RX_CFG_STRIP_FCS;\r\nwritel(val | MAC_RX_CFG_CARRIER_EXTEND,\r\ncp->regs + REG_MAC_RX_CFG);\r\nwritel(0x200, cp->regs + REG_MAC_SLOT_TIME);\r\ncp->crc_size = 4;\r\ncp->min_frame_size = CAS_1000MB_MIN_FRAME;\r\n} else {\r\nwritel(val, cp->regs + REG_MAC_TX_CFG);\r\nval = readl(cp->regs + REG_MAC_RX_CFG);\r\nif (full_duplex) {\r\nval |= MAC_RX_CFG_STRIP_FCS;\r\ncp->crc_size = 0;\r\ncp->min_frame_size = CAS_MIN_MTU;\r\n} else {\r\nval &= ~MAC_RX_CFG_STRIP_FCS;\r\ncp->crc_size = 4;\r\ncp->min_frame_size = CAS_MIN_FRAME;\r\n}\r\nwritel(val & ~MAC_RX_CFG_CARRIER_EXTEND,\r\ncp->regs + REG_MAC_RX_CFG);\r\nwritel(0x40, cp->regs + REG_MAC_SLOT_TIME);\r\n}\r\nif (netif_msg_link(cp)) {\r\nif (pause & 0x01) {\r\nnetdev_info(cp->dev, "Pause is enabled (rxfifo: %d off: %d on: %d)\n",\r\ncp->rx_fifo_size,\r\ncp->rx_pause_off,\r\ncp->rx_pause_on);\r\n} else if (pause & 0x10) {\r\nnetdev_info(cp->dev, "TX pause enabled\n");\r\n} else {\r\nnetdev_info(cp->dev, "Pause is disabled\n");\r\n}\r\n}\r\nval = readl(cp->regs + REG_MAC_CTRL_CFG);\r\nval &= ~(MAC_CTRL_CFG_SEND_PAUSE_EN | MAC_CTRL_CFG_RECV_PAUSE_EN);\r\nif (pause) {\r\nval |= MAC_CTRL_CFG_SEND_PAUSE_EN;\r\nif (pause & 0x01) {\r\nval |= MAC_CTRL_CFG_RECV_PAUSE_EN;\r\n}\r\n}\r\nwritel(val, cp->regs + REG_MAC_CTRL_CFG);\r\ncas_start_dma(cp);\r\n}\r\nstatic void cas_init_hw(struct cas *cp, int restart_link)\r\n{\r\nif (restart_link)\r\ncas_phy_init(cp);\r\ncas_init_pause_thresholds(cp);\r\ncas_init_mac(cp);\r\ncas_init_dma(cp);\r\nif (restart_link) {\r\ncp->timer_ticks = 0;\r\ncas_begin_auto_negotiation(cp, NULL);\r\n} else if (cp->lstate == link_up) {\r\ncas_set_link_modes(cp);\r\nnetif_carrier_on(cp->dev);\r\n}\r\n}\r\nstatic void cas_hard_reset(struct cas *cp)\r\n{\r\nwritel(BIM_LOCAL_DEV_SOFT_0, cp->regs + REG_BIM_LOCAL_DEV_EN);\r\nudelay(20);\r\npci_restore_state(cp->pdev);\r\n}\r\nstatic void cas_global_reset(struct cas *cp, int blkflag)\r\n{\r\nint limit;\r\nif (blkflag && !CAS_PHY_MII(cp->phy_type)) {\r\nwritel((SW_RESET_TX | SW_RESET_RX | SW_RESET_BLOCK_PCS_SLINK),\r\ncp->regs + REG_SW_RESET);\r\n} else {\r\nwritel(SW_RESET_TX | SW_RESET_RX, cp->regs + REG_SW_RESET);\r\n}\r\nmdelay(3);\r\nlimit = STOP_TRIES;\r\nwhile (limit-- > 0) {\r\nu32 val = readl(cp->regs + REG_SW_RESET);\r\nif ((val & (SW_RESET_TX | SW_RESET_RX)) == 0)\r\ngoto done;\r\nudelay(10);\r\n}\r\nnetdev_err(cp->dev, "sw reset failed\n");\r\ndone:\r\nwritel(BIM_CFG_DPAR_INTR_ENABLE | BIM_CFG_RMA_INTR_ENABLE |\r\nBIM_CFG_RTA_INTR_ENABLE, cp->regs + REG_BIM_CFG);\r\nwritel(0xFFFFFFFFU & ~(PCI_ERR_BADACK | PCI_ERR_DTRTO |\r\nPCI_ERR_OTHER | PCI_ERR_BIM_DMA_WRITE |\r\nPCI_ERR_BIM_DMA_READ), cp->regs +\r\nREG_PCI_ERR_STATUS_MASK);\r\nwritel(PCS_DATAPATH_MODE_MII, cp->regs + REG_PCS_DATAPATH_MODE);\r\n}\r\nstatic void cas_reset(struct cas *cp, int blkflag)\r\n{\r\nu32 val;\r\ncas_mask_intr(cp);\r\ncas_global_reset(cp, blkflag);\r\ncas_mac_reset(cp);\r\ncas_entropy_reset(cp);\r\nval = readl(cp->regs + REG_TX_CFG);\r\nval &= ~TX_CFG_DMA_EN;\r\nwritel(val, cp->regs + REG_TX_CFG);\r\nval = readl(cp->regs + REG_RX_CFG);\r\nval &= ~RX_CFG_DMA_EN;\r\nwritel(val, cp->regs + REG_RX_CFG);\r\nif ((cp->cas_flags & CAS_FLAG_TARGET_ABORT) ||\r\n(CAS_HP_ALT_FIRMWARE == cas_prog_null)) {\r\ncas_load_firmware(cp, CAS_HP_FIRMWARE);\r\n} else {\r\ncas_load_firmware(cp, CAS_HP_ALT_FIRMWARE);\r\n}\r\nspin_lock(&cp->stat_lock[N_TX_RINGS]);\r\ncas_clear_mac_err(cp);\r\nspin_unlock(&cp->stat_lock[N_TX_RINGS]);\r\n}\r\nstatic void cas_shutdown(struct cas *cp)\r\n{\r\nunsigned long flags;\r\ncp->hw_running = 0;\r\ndel_timer_sync(&cp->link_timer);\r\n#if 0\r\nwhile (atomic_read(&cp->reset_task_pending_mtu) ||\r\natomic_read(&cp->reset_task_pending_spare) ||\r\natomic_read(&cp->reset_task_pending_all))\r\nschedule();\r\n#else\r\nwhile (atomic_read(&cp->reset_task_pending))\r\nschedule();\r\n#endif\r\ncas_lock_all_save(cp, flags);\r\ncas_reset(cp, 0);\r\nif (cp->cas_flags & CAS_FLAG_SATURN)\r\ncas_phy_powerdown(cp);\r\ncas_unlock_all_restore(cp, flags);\r\n}\r\nstatic int cas_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nif (new_mtu < CAS_MIN_MTU || new_mtu > CAS_MAX_MTU)\r\nreturn -EINVAL;\r\ndev->mtu = new_mtu;\r\nif (!netif_running(dev) || !netif_device_present(dev))\r\nreturn 0;\r\n#if 1\r\natomic_inc(&cp->reset_task_pending);\r\nif ((cp->phy_type & CAS_PHY_SERDES)) {\r\natomic_inc(&cp->reset_task_pending_all);\r\n} else {\r\natomic_inc(&cp->reset_task_pending_mtu);\r\n}\r\nschedule_work(&cp->reset_task);\r\n#else\r\natomic_set(&cp->reset_task_pending, (cp->phy_type & CAS_PHY_SERDES) ?\r\nCAS_RESET_ALL : CAS_RESET_MTU);\r\npr_err("reset called in cas_change_mtu\n");\r\nschedule_work(&cp->reset_task);\r\n#endif\r\nflush_work_sync(&cp->reset_task);\r\nreturn 0;\r\n}\r\nstatic void cas_clean_txd(struct cas *cp, int ring)\r\n{\r\nstruct cas_tx_desc *txd = cp->init_txds[ring];\r\nstruct sk_buff *skb, **skbs = cp->tx_skbs[ring];\r\nu64 daddr, dlen;\r\nint i, size;\r\nsize = TX_DESC_RINGN_SIZE(ring);\r\nfor (i = 0; i < size; i++) {\r\nint frag;\r\nif (skbs[i] == NULL)\r\ncontinue;\r\nskb = skbs[i];\r\nskbs[i] = NULL;\r\nfor (frag = 0; frag <= skb_shinfo(skb)->nr_frags; frag++) {\r\nint ent = i & (size - 1);\r\ndaddr = le64_to_cpu(txd[ent].buffer);\r\ndlen = CAS_VAL(TX_DESC_BUFLEN,\r\nle64_to_cpu(txd[ent].control));\r\npci_unmap_page(cp->pdev, daddr, dlen,\r\nPCI_DMA_TODEVICE);\r\nif (frag != skb_shinfo(skb)->nr_frags) {\r\ni++;\r\nent = i & (size - 1);\r\nif (cp->tx_tiny_use[ring][ent].used)\r\ni++;\r\n}\r\n}\r\ndev_kfree_skb_any(skb);\r\n}\r\nmemset(cp->tx_tiny_use[ring], 0, size*sizeof(*cp->tx_tiny_use[ring]));\r\n}\r\nstatic inline void cas_free_rx_desc(struct cas *cp, int ring)\r\n{\r\ncas_page_t **page = cp->rx_pages[ring];\r\nint i, size;\r\nsize = RX_DESC_RINGN_SIZE(ring);\r\nfor (i = 0; i < size; i++) {\r\nif (page[i]) {\r\ncas_page_free(cp, page[i]);\r\npage[i] = NULL;\r\n}\r\n}\r\n}\r\nstatic void cas_free_rxds(struct cas *cp)\r\n{\r\nint i;\r\nfor (i = 0; i < N_RX_DESC_RINGS; i++)\r\ncas_free_rx_desc(cp, i);\r\n}\r\nstatic void cas_clean_rings(struct cas *cp)\r\n{\r\nint i;\r\nmemset(cp->tx_old, 0, sizeof(*cp->tx_old)*N_TX_RINGS);\r\nmemset(cp->tx_new, 0, sizeof(*cp->tx_new)*N_TX_RINGS);\r\nfor (i = 0; i < N_TX_RINGS; i++)\r\ncas_clean_txd(cp, i);\r\nmemset(cp->init_block, 0, sizeof(struct cas_init_block));\r\ncas_clean_rxds(cp);\r\ncas_clean_rxcs(cp);\r\n}\r\nstatic inline int cas_alloc_rx_desc(struct cas *cp, int ring)\r\n{\r\ncas_page_t **page = cp->rx_pages[ring];\r\nint size, i = 0;\r\nsize = RX_DESC_RINGN_SIZE(ring);\r\nfor (i = 0; i < size; i++) {\r\nif ((page[i] = cas_page_alloc(cp, GFP_KERNEL)) == NULL)\r\nreturn -1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int cas_alloc_rxds(struct cas *cp)\r\n{\r\nint i;\r\nfor (i = 0; i < N_RX_DESC_RINGS; i++) {\r\nif (cas_alloc_rx_desc(cp, i) < 0) {\r\ncas_free_rxds(cp);\r\nreturn -1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void cas_reset_task(struct work_struct *work)\r\n{\r\nstruct cas *cp = container_of(work, struct cas, reset_task);\r\n#if 0\r\nint pending = atomic_read(&cp->reset_task_pending);\r\n#else\r\nint pending_all = atomic_read(&cp->reset_task_pending_all);\r\nint pending_spare = atomic_read(&cp->reset_task_pending_spare);\r\nint pending_mtu = atomic_read(&cp->reset_task_pending_mtu);\r\nif (pending_all == 0 && pending_spare == 0 && pending_mtu == 0) {\r\natomic_dec(&cp->reset_task_pending);\r\nreturn;\r\n}\r\n#endif\r\nif (cp->hw_running) {\r\nunsigned long flags;\r\nnetif_device_detach(cp->dev);\r\ncas_lock_all_save(cp, flags);\r\nif (cp->opened) {\r\ncas_spare_recover(cp, GFP_ATOMIC);\r\n}\r\n#if 1\r\nif (!pending_all && !pending_mtu)\r\ngoto done;\r\n#else\r\nif (pending == CAS_RESET_SPARE)\r\ngoto done;\r\n#endif\r\n#if 1\r\ncas_reset(cp, !(pending_all > 0));\r\nif (cp->opened)\r\ncas_clean_rings(cp);\r\ncas_init_hw(cp, (pending_all > 0));\r\n#else\r\ncas_reset(cp, !(pending == CAS_RESET_ALL));\r\nif (cp->opened)\r\ncas_clean_rings(cp);\r\ncas_init_hw(cp, pending == CAS_RESET_ALL);\r\n#endif\r\ndone:\r\ncas_unlock_all_restore(cp, flags);\r\nnetif_device_attach(cp->dev);\r\n}\r\n#if 1\r\natomic_sub(pending_all, &cp->reset_task_pending_all);\r\natomic_sub(pending_spare, &cp->reset_task_pending_spare);\r\natomic_sub(pending_mtu, &cp->reset_task_pending_mtu);\r\natomic_dec(&cp->reset_task_pending);\r\n#else\r\natomic_set(&cp->reset_task_pending, 0);\r\n#endif\r\n}\r\nstatic void cas_link_timer(unsigned long data)\r\n{\r\nstruct cas *cp = (struct cas *) data;\r\nint mask, pending = 0, reset = 0;\r\nunsigned long flags;\r\nif (link_transition_timeout != 0 &&\r\ncp->link_transition_jiffies_valid &&\r\n((jiffies - cp->link_transition_jiffies) >\r\n(link_transition_timeout))) {\r\ncp->link_transition_jiffies_valid = 0;\r\n}\r\nif (!cp->hw_running)\r\nreturn;\r\nspin_lock_irqsave(&cp->lock, flags);\r\ncas_lock_tx(cp);\r\ncas_entropy_gather(cp);\r\n#if 1\r\nif (atomic_read(&cp->reset_task_pending_all) ||\r\natomic_read(&cp->reset_task_pending_spare) ||\r\natomic_read(&cp->reset_task_pending_mtu))\r\ngoto done;\r\n#else\r\nif (atomic_read(&cp->reset_task_pending))\r\ngoto done;\r\n#endif\r\nif ((mask = (cp->cas_flags & CAS_FLAG_RXD_POST_MASK))) {\r\nint i, rmask;\r\nfor (i = 0; i < MAX_RX_DESC_RINGS; i++) {\r\nrmask = CAS_FLAG_RXD_POST(i);\r\nif ((mask & rmask) == 0)\r\ncontinue;\r\nif (cas_post_rxds_ringN(cp, i, cp->rx_last[i]) < 0) {\r\npending = 1;\r\ncontinue;\r\n}\r\ncp->cas_flags &= ~rmask;\r\n}\r\n}\r\nif (CAS_PHY_MII(cp->phy_type)) {\r\nu16 bmsr;\r\ncas_mif_poll(cp, 0);\r\nbmsr = cas_phy_read(cp, MII_BMSR);\r\nbmsr = cas_phy_read(cp, MII_BMSR);\r\ncas_mif_poll(cp, 1);\r\nreadl(cp->regs + REG_MIF_STATUS);\r\nreset = cas_mii_link_check(cp, bmsr);\r\n} else {\r\nreset = cas_pcs_link_check(cp);\r\n}\r\nif (reset)\r\ngoto done;\r\nif ((readl(cp->regs + REG_MAC_TX_STATUS) & MAC_TX_FRAME_XMIT) == 0) {\r\nu32 val = readl(cp->regs + REG_MAC_STATE_MACHINE);\r\nu32 wptr, rptr;\r\nint tlm = CAS_VAL(MAC_SM_TLM, val);\r\nif (((tlm == 0x5) || (tlm == 0x3)) &&\r\n(CAS_VAL(MAC_SM_ENCAP_SM, val) == 0)) {\r\nnetif_printk(cp, tx_err, KERN_DEBUG, cp->dev,\r\n"tx err: MAC_STATE[%08x]\n", val);\r\nreset = 1;\r\ngoto done;\r\n}\r\nval = readl(cp->regs + REG_TX_FIFO_PKT_CNT);\r\nwptr = readl(cp->regs + REG_TX_FIFO_WRITE_PTR);\r\nrptr = readl(cp->regs + REG_TX_FIFO_READ_PTR);\r\nif ((val == 0) && (wptr != rptr)) {\r\nnetif_printk(cp, tx_err, KERN_DEBUG, cp->dev,\r\n"tx err: TX_FIFO[%08x:%08x:%08x]\n",\r\nval, wptr, rptr);\r\nreset = 1;\r\n}\r\nif (reset)\r\ncas_hard_reset(cp);\r\n}\r\ndone:\r\nif (reset) {\r\n#if 1\r\natomic_inc(&cp->reset_task_pending);\r\natomic_inc(&cp->reset_task_pending_all);\r\nschedule_work(&cp->reset_task);\r\n#else\r\natomic_set(&cp->reset_task_pending, CAS_RESET_ALL);\r\npr_err("reset called in cas_link_timer\n");\r\nschedule_work(&cp->reset_task);\r\n#endif\r\n}\r\nif (!pending)\r\nmod_timer(&cp->link_timer, jiffies + CAS_LINK_TIMEOUT);\r\ncas_unlock_tx(cp);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\n}\r\nstatic void cas_tx_tiny_free(struct cas *cp)\r\n{\r\nstruct pci_dev *pdev = cp->pdev;\r\nint i;\r\nfor (i = 0; i < N_TX_RINGS; i++) {\r\nif (!cp->tx_tiny_bufs[i])\r\ncontinue;\r\npci_free_consistent(pdev, TX_TINY_BUF_BLOCK,\r\ncp->tx_tiny_bufs[i],\r\ncp->tx_tiny_dvma[i]);\r\ncp->tx_tiny_bufs[i] = NULL;\r\n}\r\n}\r\nstatic int cas_tx_tiny_alloc(struct cas *cp)\r\n{\r\nstruct pci_dev *pdev = cp->pdev;\r\nint i;\r\nfor (i = 0; i < N_TX_RINGS; i++) {\r\ncp->tx_tiny_bufs[i] =\r\npci_alloc_consistent(pdev, TX_TINY_BUF_BLOCK,\r\n&cp->tx_tiny_dvma[i]);\r\nif (!cp->tx_tiny_bufs[i]) {\r\ncas_tx_tiny_free(cp);\r\nreturn -1;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int cas_open(struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nint hw_was_up, err;\r\nunsigned long flags;\r\nmutex_lock(&cp->pm_mutex);\r\nhw_was_up = cp->hw_running;\r\nif (!cp->hw_running) {\r\ncas_lock_all_save(cp, flags);\r\ncas_reset(cp, 0);\r\ncp->hw_running = 1;\r\ncas_unlock_all_restore(cp, flags);\r\n}\r\nerr = -ENOMEM;\r\nif (cas_tx_tiny_alloc(cp) < 0)\r\ngoto err_unlock;\r\nif (cas_alloc_rxds(cp) < 0)\r\ngoto err_tx_tiny;\r\ncas_spare_init(cp);\r\ncas_spare_recover(cp, GFP_KERNEL);\r\nif (request_irq(cp->pdev->irq, cas_interrupt,\r\nIRQF_SHARED, dev->name, (void *) dev)) {\r\nnetdev_err(cp->dev, "failed to request irq !\n");\r\nerr = -EAGAIN;\r\ngoto err_spare;\r\n}\r\n#ifdef USE_NAPI\r\nnapi_enable(&cp->napi);\r\n#endif\r\ncas_lock_all_save(cp, flags);\r\ncas_clean_rings(cp);\r\ncas_init_hw(cp, !hw_was_up);\r\ncp->opened = 1;\r\ncas_unlock_all_restore(cp, flags);\r\nnetif_start_queue(dev);\r\nmutex_unlock(&cp->pm_mutex);\r\nreturn 0;\r\nerr_spare:\r\ncas_spare_free(cp);\r\ncas_free_rxds(cp);\r\nerr_tx_tiny:\r\ncas_tx_tiny_free(cp);\r\nerr_unlock:\r\nmutex_unlock(&cp->pm_mutex);\r\nreturn err;\r\n}\r\nstatic int cas_close(struct net_device *dev)\r\n{\r\nunsigned long flags;\r\nstruct cas *cp = netdev_priv(dev);\r\n#ifdef USE_NAPI\r\nnapi_disable(&cp->napi);\r\n#endif\r\nmutex_lock(&cp->pm_mutex);\r\nnetif_stop_queue(dev);\r\ncas_lock_all_save(cp, flags);\r\ncp->opened = 0;\r\ncas_reset(cp, 0);\r\ncas_phy_init(cp);\r\ncas_begin_auto_negotiation(cp, NULL);\r\ncas_clean_rings(cp);\r\ncas_unlock_all_restore(cp, flags);\r\nfree_irq(cp->pdev->irq, (void *) dev);\r\ncas_spare_free(cp);\r\ncas_free_rxds(cp);\r\ncas_tx_tiny_free(cp);\r\nmutex_unlock(&cp->pm_mutex);\r\nreturn 0;\r\n}\r\nstatic void cas_read_regs(struct cas *cp, u8 *ptr, int len)\r\n{\r\nu8 *p;\r\nint i;\r\nunsigned long flags;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nfor (i = 0, p = ptr; i < len ; i ++, p += sizeof(u32)) {\r\nu16 hval;\r\nu32 val;\r\nif (ethtool_register_table[i].offsets < 0) {\r\nhval = cas_phy_read(cp,\r\n-ethtool_register_table[i].offsets);\r\nval = hval;\r\n} else {\r\nval= readl(cp->regs+ethtool_register_table[i].offsets);\r\n}\r\nmemcpy(p, (u8 *)&val, sizeof(u32));\r\n}\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\n}\r\nstatic struct net_device_stats *cas_get_stats(struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nstruct net_device_stats *stats = cp->net_stats;\r\nunsigned long flags;\r\nint i;\r\nunsigned long tmp;\r\nif (!cp->hw_running)\r\nreturn stats + N_TX_RINGS;\r\nspin_lock_irqsave(&cp->stat_lock[N_TX_RINGS], flags);\r\nstats[N_TX_RINGS].rx_crc_errors +=\r\nreadl(cp->regs + REG_MAC_FCS_ERR) & 0xffff;\r\nstats[N_TX_RINGS].rx_frame_errors +=\r\nreadl(cp->regs + REG_MAC_ALIGN_ERR) &0xffff;\r\nstats[N_TX_RINGS].rx_length_errors +=\r\nreadl(cp->regs + REG_MAC_LEN_ERR) & 0xffff;\r\n#if 1\r\ntmp = (readl(cp->regs + REG_MAC_COLL_EXCESS) & 0xffff) +\r\n(readl(cp->regs + REG_MAC_COLL_LATE) & 0xffff);\r\nstats[N_TX_RINGS].tx_aborted_errors += tmp;\r\nstats[N_TX_RINGS].collisions +=\r\ntmp + (readl(cp->regs + REG_MAC_COLL_NORMAL) & 0xffff);\r\n#else\r\nstats[N_TX_RINGS].tx_aborted_errors +=\r\nreadl(cp->regs + REG_MAC_COLL_EXCESS);\r\nstats[N_TX_RINGS].collisions += readl(cp->regs + REG_MAC_COLL_EXCESS) +\r\nreadl(cp->regs + REG_MAC_COLL_LATE);\r\n#endif\r\ncas_clear_mac_err(cp);\r\nspin_lock(&cp->stat_lock[0]);\r\nstats[N_TX_RINGS].collisions += stats[0].collisions;\r\nstats[N_TX_RINGS].rx_over_errors += stats[0].rx_over_errors;\r\nstats[N_TX_RINGS].rx_frame_errors += stats[0].rx_frame_errors;\r\nstats[N_TX_RINGS].rx_fifo_errors += stats[0].rx_fifo_errors;\r\nstats[N_TX_RINGS].tx_aborted_errors += stats[0].tx_aborted_errors;\r\nstats[N_TX_RINGS].tx_fifo_errors += stats[0].tx_fifo_errors;\r\nspin_unlock(&cp->stat_lock[0]);\r\nfor (i = 0; i < N_TX_RINGS; i++) {\r\nspin_lock(&cp->stat_lock[i]);\r\nstats[N_TX_RINGS].rx_length_errors +=\r\nstats[i].rx_length_errors;\r\nstats[N_TX_RINGS].rx_crc_errors += stats[i].rx_crc_errors;\r\nstats[N_TX_RINGS].rx_packets += stats[i].rx_packets;\r\nstats[N_TX_RINGS].tx_packets += stats[i].tx_packets;\r\nstats[N_TX_RINGS].rx_bytes += stats[i].rx_bytes;\r\nstats[N_TX_RINGS].tx_bytes += stats[i].tx_bytes;\r\nstats[N_TX_RINGS].rx_errors += stats[i].rx_errors;\r\nstats[N_TX_RINGS].tx_errors += stats[i].tx_errors;\r\nstats[N_TX_RINGS].rx_dropped += stats[i].rx_dropped;\r\nstats[N_TX_RINGS].tx_dropped += stats[i].tx_dropped;\r\nmemset(stats + i, 0, sizeof(struct net_device_stats));\r\nspin_unlock(&cp->stat_lock[i]);\r\n}\r\nspin_unlock_irqrestore(&cp->stat_lock[N_TX_RINGS], flags);\r\nreturn stats + N_TX_RINGS;\r\n}\r\nstatic void cas_set_multicast(struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nu32 rxcfg, rxcfg_new;\r\nunsigned long flags;\r\nint limit = STOP_TRIES;\r\nif (!cp->hw_running)\r\nreturn;\r\nspin_lock_irqsave(&cp->lock, flags);\r\nrxcfg = readl(cp->regs + REG_MAC_RX_CFG);\r\nwritel(rxcfg & ~MAC_RX_CFG_EN, cp->regs + REG_MAC_RX_CFG);\r\nwhile (readl(cp->regs + REG_MAC_RX_CFG) & MAC_RX_CFG_EN) {\r\nif (!limit--)\r\nbreak;\r\nudelay(10);\r\n}\r\nlimit = STOP_TRIES;\r\nrxcfg &= ~(MAC_RX_CFG_PROMISC_EN | MAC_RX_CFG_HASH_FILTER_EN);\r\nwritel(rxcfg & ~MAC_RX_CFG_EN, cp->regs + REG_MAC_RX_CFG);\r\nwhile (readl(cp->regs + REG_MAC_RX_CFG) & MAC_RX_CFG_HASH_FILTER_EN) {\r\nif (!limit--)\r\nbreak;\r\nudelay(10);\r\n}\r\ncp->mac_rx_cfg = rxcfg_new = cas_setup_multicast(cp);\r\nrxcfg |= rxcfg_new;\r\nwritel(rxcfg, cp->regs + REG_MAC_RX_CFG);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\n}\r\nstatic void cas_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nstrlcpy(info->driver, DRV_MODULE_NAME, sizeof(info->driver));\r\nstrlcpy(info->version, DRV_MODULE_VERSION, sizeof(info->version));\r\nstrlcpy(info->bus_info, pci_name(cp->pdev), sizeof(info->bus_info));\r\ninfo->regdump_len = cp->casreg_len < CAS_MAX_REGS ?\r\ncp->casreg_len : CAS_MAX_REGS;\r\ninfo->n_stats = CAS_NUM_STAT_KEYS;\r\n}\r\nstatic int cas_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nu16 bmcr;\r\nint full_duplex, speed, pause;\r\nunsigned long flags;\r\nenum link_state linkstate = link_up;\r\ncmd->advertising = 0;\r\ncmd->supported = SUPPORTED_Autoneg;\r\nif (cp->cas_flags & CAS_FLAG_1000MB_CAP) {\r\ncmd->supported |= SUPPORTED_1000baseT_Full;\r\ncmd->advertising |= ADVERTISED_1000baseT_Full;\r\n}\r\nspin_lock_irqsave(&cp->lock, flags);\r\nbmcr = 0;\r\nlinkstate = cp->lstate;\r\nif (CAS_PHY_MII(cp->phy_type)) {\r\ncmd->port = PORT_MII;\r\ncmd->transceiver = (cp->cas_flags & CAS_FLAG_SATURN) ?\r\nXCVR_INTERNAL : XCVR_EXTERNAL;\r\ncmd->phy_address = cp->phy_addr;\r\ncmd->advertising |= ADVERTISED_TP | ADVERTISED_MII |\r\nADVERTISED_10baseT_Half |\r\nADVERTISED_10baseT_Full |\r\nADVERTISED_100baseT_Half |\r\nADVERTISED_100baseT_Full;\r\ncmd->supported |=\r\n(SUPPORTED_10baseT_Half |\r\nSUPPORTED_10baseT_Full |\r\nSUPPORTED_100baseT_Half |\r\nSUPPORTED_100baseT_Full |\r\nSUPPORTED_TP | SUPPORTED_MII);\r\nif (cp->hw_running) {\r\ncas_mif_poll(cp, 0);\r\nbmcr = cas_phy_read(cp, MII_BMCR);\r\ncas_read_mii_link_mode(cp, &full_duplex,\r\n&speed, &pause);\r\ncas_mif_poll(cp, 1);\r\n}\r\n} else {\r\ncmd->port = PORT_FIBRE;\r\ncmd->transceiver = XCVR_INTERNAL;\r\ncmd->phy_address = 0;\r\ncmd->supported |= SUPPORTED_FIBRE;\r\ncmd->advertising |= ADVERTISED_FIBRE;\r\nif (cp->hw_running) {\r\nbmcr = readl(cp->regs + REG_PCS_MII_CTRL);\r\ncas_read_pcs_link_mode(cp, &full_duplex,\r\n&speed, &pause);\r\n}\r\n}\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nif (bmcr & BMCR_ANENABLE) {\r\ncmd->advertising |= ADVERTISED_Autoneg;\r\ncmd->autoneg = AUTONEG_ENABLE;\r\nethtool_cmd_speed_set(cmd, ((speed == 10) ?\r\nSPEED_10 :\r\n((speed == 1000) ?\r\nSPEED_1000 : SPEED_100)));\r\ncmd->duplex = full_duplex ? DUPLEX_FULL : DUPLEX_HALF;\r\n} else {\r\ncmd->autoneg = AUTONEG_DISABLE;\r\nethtool_cmd_speed_set(cmd, ((bmcr & CAS_BMCR_SPEED1000) ?\r\nSPEED_1000 :\r\n((bmcr & BMCR_SPEED100) ?\r\nSPEED_100 : SPEED_10)));\r\ncmd->duplex =\r\n(bmcr & BMCR_FULLDPLX) ?\r\nDUPLEX_FULL : DUPLEX_HALF;\r\n}\r\nif (linkstate != link_up) {\r\nif (cp->link_cntl & BMCR_ANENABLE) {\r\nethtool_cmd_speed_set(cmd, 0);\r\ncmd->duplex = 0xff;\r\n} else {\r\nethtool_cmd_speed_set(cmd, SPEED_10);\r\nif (cp->link_cntl & BMCR_SPEED100) {\r\nethtool_cmd_speed_set(cmd, SPEED_100);\r\n} else if (cp->link_cntl & CAS_BMCR_SPEED1000) {\r\nethtool_cmd_speed_set(cmd, SPEED_1000);\r\n}\r\ncmd->duplex = (cp->link_cntl & BMCR_FULLDPLX)?\r\nDUPLEX_FULL : DUPLEX_HALF;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int cas_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nu32 speed = ethtool_cmd_speed(cmd);\r\nif (cmd->autoneg != AUTONEG_ENABLE &&\r\ncmd->autoneg != AUTONEG_DISABLE)\r\nreturn -EINVAL;\r\nif (cmd->autoneg == AUTONEG_DISABLE &&\r\n((speed != SPEED_1000 &&\r\nspeed != SPEED_100 &&\r\nspeed != SPEED_10) ||\r\n(cmd->duplex != DUPLEX_HALF &&\r\ncmd->duplex != DUPLEX_FULL)))\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&cp->lock, flags);\r\ncas_begin_auto_negotiation(cp, cmd);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int cas_nway_reset(struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nif ((cp->link_cntl & BMCR_ANENABLE) == 0)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&cp->lock, flags);\r\ncas_begin_auto_negotiation(cp, NULL);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nreturn 0;\r\n}\r\nstatic u32 cas_get_link(struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nreturn cp->lstate == link_up;\r\n}\r\nstatic u32 cas_get_msglevel(struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nreturn cp->msg_enable;\r\n}\r\nstatic void cas_set_msglevel(struct net_device *dev, u32 value)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\ncp->msg_enable = value;\r\n}\r\nstatic int cas_get_regs_len(struct net_device *dev)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nreturn cp->casreg_len < CAS_MAX_REGS ? cp->casreg_len: CAS_MAX_REGS;\r\n}\r\nstatic void cas_get_regs(struct net_device *dev, struct ethtool_regs *regs,\r\nvoid *p)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nregs->version = 0;\r\ncas_read_regs(cp, p, regs->len / sizeof(u32));\r\n}\r\nstatic int cas_get_sset_count(struct net_device *dev, int sset)\r\n{\r\nswitch (sset) {\r\ncase ETH_SS_STATS:\r\nreturn CAS_NUM_STAT_KEYS;\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void cas_get_strings(struct net_device *dev, u32 stringset, u8 *data)\r\n{\r\nmemcpy(data, &ethtool_cassini_statnames,\r\nCAS_NUM_STAT_KEYS * ETH_GSTRING_LEN);\r\n}\r\nstatic void cas_get_ethtool_stats(struct net_device *dev,\r\nstruct ethtool_stats *estats, u64 *data)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nstruct net_device_stats *stats = cas_get_stats(cp->dev);\r\nint i = 0;\r\ndata[i++] = stats->collisions;\r\ndata[i++] = stats->rx_bytes;\r\ndata[i++] = stats->rx_crc_errors;\r\ndata[i++] = stats->rx_dropped;\r\ndata[i++] = stats->rx_errors;\r\ndata[i++] = stats->rx_fifo_errors;\r\ndata[i++] = stats->rx_frame_errors;\r\ndata[i++] = stats->rx_length_errors;\r\ndata[i++] = stats->rx_over_errors;\r\ndata[i++] = stats->rx_packets;\r\ndata[i++] = stats->tx_aborted_errors;\r\ndata[i++] = stats->tx_bytes;\r\ndata[i++] = stats->tx_dropped;\r\ndata[i++] = stats->tx_errors;\r\ndata[i++] = stats->tx_fifo_errors;\r\ndata[i++] = stats->tx_packets;\r\nBUG_ON(i != CAS_NUM_STAT_KEYS);\r\n}\r\nstatic int cas_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\r\n{\r\nstruct cas *cp = netdev_priv(dev);\r\nstruct mii_ioctl_data *data = if_mii(ifr);\r\nunsigned long flags;\r\nint rc = -EOPNOTSUPP;\r\nmutex_lock(&cp->pm_mutex);\r\nswitch (cmd) {\r\ncase SIOCGMIIPHY:\r\ndata->phy_id = cp->phy_addr;\r\ncase SIOCGMIIREG:\r\nspin_lock_irqsave(&cp->lock, flags);\r\ncas_mif_poll(cp, 0);\r\ndata->val_out = cas_phy_read(cp, data->reg_num & 0x1f);\r\ncas_mif_poll(cp, 1);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nrc = 0;\r\nbreak;\r\ncase SIOCSMIIREG:\r\nspin_lock_irqsave(&cp->lock, flags);\r\ncas_mif_poll(cp, 0);\r\nrc = cas_phy_write(cp, data->reg_num & 0x1f, data->val_in);\r\ncas_mif_poll(cp, 1);\r\nspin_unlock_irqrestore(&cp->lock, flags);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nmutex_unlock(&cp->pm_mutex);\r\nreturn rc;\r\n}\r\nstatic void __devinit cas_program_bridge(struct pci_dev *cas_pdev)\r\n{\r\nstruct pci_dev *pdev = cas_pdev->bus->self;\r\nu32 val;\r\nif (!pdev)\r\nreturn;\r\nif (pdev->vendor != 0x8086 || pdev->device != 0x537c)\r\nreturn;\r\npci_read_config_dword(pdev, 0x40, &val);\r\nval &= ~0x00040000;\r\npci_write_config_dword(pdev, 0x40, val);\r\npci_write_config_word(pdev, 0x50, (5 << 10) | 0x3ff);\r\npci_write_config_word(pdev, 0x52,\r\n(0x7 << 13) |\r\n(0x7 << 10) |\r\n(0x7 << 7) |\r\n(0x7 << 4) |\r\n(0xf << 0));\r\npci_write_config_byte(pdev, PCI_CACHE_LINE_SIZE, 0x08);\r\npci_write_config_byte(pdev, PCI_LATENCY_TIMER, 0xff);\r\n}\r\nstatic int __devinit cas_init_one(struct pci_dev *pdev,\r\nconst struct pci_device_id *ent)\r\n{\r\nstatic int cas_version_printed = 0;\r\nunsigned long casreg_len;\r\nstruct net_device *dev;\r\nstruct cas *cp;\r\nint i, err, pci_using_dac;\r\nu16 pci_cmd;\r\nu8 orig_cacheline_size = 0, cas_cacheline_size = 0;\r\nif (cas_version_printed++ == 0)\r\npr_info("%s", version);\r\nerr = pci_enable_device(pdev);\r\nif (err) {\r\ndev_err(&pdev->dev, "Cannot enable PCI device, aborting\n");\r\nreturn err;\r\n}\r\nif (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) {\r\ndev_err(&pdev->dev, "Cannot find proper PCI device "\r\n"base address, aborting\n");\r\nerr = -ENODEV;\r\ngoto err_out_disable_pdev;\r\n}\r\ndev = alloc_etherdev(sizeof(*cp));\r\nif (!dev) {\r\nerr = -ENOMEM;\r\ngoto err_out_disable_pdev;\r\n}\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nerr = pci_request_regions(pdev, dev->name);\r\nif (err) {\r\ndev_err(&pdev->dev, "Cannot obtain PCI resources, aborting\n");\r\ngoto err_out_free_netdev;\r\n}\r\npci_set_master(pdev);\r\npci_read_config_word(pdev, PCI_COMMAND, &pci_cmd);\r\npci_cmd &= ~PCI_COMMAND_SERR;\r\npci_cmd |= PCI_COMMAND_PARITY;\r\npci_write_config_word(pdev, PCI_COMMAND, pci_cmd);\r\nif (pci_try_set_mwi(pdev))\r\npr_warning("Could not enable MWI for %s\n", pci_name(pdev));\r\ncas_program_bridge(pdev);\r\n#if 1\r\npci_read_config_byte(pdev, PCI_CACHE_LINE_SIZE,\r\n&orig_cacheline_size);\r\nif (orig_cacheline_size < CAS_PREF_CACHELINE_SIZE) {\r\ncas_cacheline_size =\r\n(CAS_PREF_CACHELINE_SIZE < SMP_CACHE_BYTES) ?\r\nCAS_PREF_CACHELINE_SIZE : SMP_CACHE_BYTES;\r\nif (pci_write_config_byte(pdev,\r\nPCI_CACHE_LINE_SIZE,\r\ncas_cacheline_size)) {\r\ndev_err(&pdev->dev, "Could not set PCI cache "\r\n"line size\n");\r\ngoto err_write_cacheline;\r\n}\r\n}\r\n#endif\r\nif (!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))) {\r\npci_using_dac = 1;\r\nerr = pci_set_consistent_dma_mask(pdev,\r\nDMA_BIT_MASK(64));\r\nif (err < 0) {\r\ndev_err(&pdev->dev, "Unable to obtain 64-bit DMA "\r\n"for consistent allocations\n");\r\ngoto err_out_free_res;\r\n}\r\n} else {\r\nerr = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (err) {\r\ndev_err(&pdev->dev, "No usable DMA configuration, "\r\n"aborting\n");\r\ngoto err_out_free_res;\r\n}\r\npci_using_dac = 0;\r\n}\r\ncasreg_len = pci_resource_len(pdev, 0);\r\ncp = netdev_priv(dev);\r\ncp->pdev = pdev;\r\n#if 1\r\ncp->orig_cacheline_size = cas_cacheline_size ? orig_cacheline_size: 0;\r\n#endif\r\ncp->dev = dev;\r\ncp->msg_enable = (cassini_debug < 0) ? CAS_DEF_MSG_ENABLE :\r\ncassini_debug;\r\n#if defined(CONFIG_SPARC)\r\ncp->of_node = pci_device_to_OF_node(pdev);\r\n#endif\r\ncp->link_transition = LINK_TRANSITION_UNKNOWN;\r\ncp->link_transition_jiffies_valid = 0;\r\nspin_lock_init(&cp->lock);\r\nspin_lock_init(&cp->rx_inuse_lock);\r\nspin_lock_init(&cp->rx_spare_lock);\r\nfor (i = 0; i < N_TX_RINGS; i++) {\r\nspin_lock_init(&cp->stat_lock[i]);\r\nspin_lock_init(&cp->tx_lock[i]);\r\n}\r\nspin_lock_init(&cp->stat_lock[N_TX_RINGS]);\r\nmutex_init(&cp->pm_mutex);\r\ninit_timer(&cp->link_timer);\r\ncp->link_timer.function = cas_link_timer;\r\ncp->link_timer.data = (unsigned long) cp;\r\n#if 1\r\natomic_set(&cp->reset_task_pending, 0);\r\natomic_set(&cp->reset_task_pending_all, 0);\r\natomic_set(&cp->reset_task_pending_spare, 0);\r\natomic_set(&cp->reset_task_pending_mtu, 0);\r\n#endif\r\nINIT_WORK(&cp->reset_task, cas_reset_task);\r\nif (link_mode >= 0 && link_mode < 6)\r\ncp->link_cntl = link_modes[link_mode];\r\nelse\r\ncp->link_cntl = BMCR_ANENABLE;\r\ncp->lstate = link_down;\r\ncp->link_transition = LINK_TRANSITION_LINK_DOWN;\r\nnetif_carrier_off(cp->dev);\r\ncp->timer_ticks = 0;\r\ncp->regs = pci_iomap(pdev, 0, casreg_len);\r\nif (!cp->regs) {\r\ndev_err(&pdev->dev, "Cannot map device registers, aborting\n");\r\ngoto err_out_free_res;\r\n}\r\ncp->casreg_len = casreg_len;\r\npci_save_state(pdev);\r\ncas_check_pci_invariants(cp);\r\ncas_hard_reset(cp);\r\ncas_reset(cp, 0);\r\nif (cas_check_invariants(cp))\r\ngoto err_out_iounmap;\r\nif (cp->cas_flags & CAS_FLAG_SATURN)\r\nif (cas_saturn_firmware_init(cp))\r\ngoto err_out_iounmap;\r\ncp->init_block = (struct cas_init_block *)\r\npci_alloc_consistent(pdev, sizeof(struct cas_init_block),\r\n&cp->block_dvma);\r\nif (!cp->init_block) {\r\ndev_err(&pdev->dev, "Cannot allocate init block, aborting\n");\r\ngoto err_out_iounmap;\r\n}\r\nfor (i = 0; i < N_TX_RINGS; i++)\r\ncp->init_txds[i] = cp->init_block->txds[i];\r\nfor (i = 0; i < N_RX_DESC_RINGS; i++)\r\ncp->init_rxds[i] = cp->init_block->rxds[i];\r\nfor (i = 0; i < N_RX_COMP_RINGS; i++)\r\ncp->init_rxcs[i] = cp->init_block->rxcs[i];\r\nfor (i = 0; i < N_RX_FLOWS; i++)\r\nskb_queue_head_init(&cp->rx_flows[i]);\r\ndev->netdev_ops = &cas_netdev_ops;\r\ndev->ethtool_ops = &cas_ethtool_ops;\r\ndev->watchdog_timeo = CAS_TX_TIMEOUT;\r\n#ifdef USE_NAPI\r\nnetif_napi_add(dev, &cp->napi, cas_poll, 64);\r\n#endif\r\ndev->irq = pdev->irq;\r\ndev->dma = 0;\r\nif ((cp->cas_flags & CAS_FLAG_NO_HW_CSUM) == 0)\r\ndev->features |= NETIF_F_HW_CSUM | NETIF_F_SG;\r\nif (pci_using_dac)\r\ndev->features |= NETIF_F_HIGHDMA;\r\nif (register_netdev(dev)) {\r\ndev_err(&pdev->dev, "Cannot register net device, aborting\n");\r\ngoto err_out_free_consistent;\r\n}\r\ni = readl(cp->regs + REG_BIM_CFG);\r\nnetdev_info(dev, "Sun Cassini%s (%sbit/%sMHz PCI/%s) Ethernet[%d] %pM\n",\r\n(cp->cas_flags & CAS_FLAG_REG_PLUS) ? "+" : "",\r\n(i & BIM_CFG_32BIT) ? "32" : "64",\r\n(i & BIM_CFG_66MHZ) ? "66" : "33",\r\n(cp->phy_type == CAS_PHY_SERDES) ? "Fi" : "Cu", pdev->irq,\r\ndev->dev_addr);\r\npci_set_drvdata(pdev, dev);\r\ncp->hw_running = 1;\r\ncas_entropy_reset(cp);\r\ncas_phy_init(cp);\r\ncas_begin_auto_negotiation(cp, NULL);\r\nreturn 0;\r\nerr_out_free_consistent:\r\npci_free_consistent(pdev, sizeof(struct cas_init_block),\r\ncp->init_block, cp->block_dvma);\r\nerr_out_iounmap:\r\nmutex_lock(&cp->pm_mutex);\r\nif (cp->hw_running)\r\ncas_shutdown(cp);\r\nmutex_unlock(&cp->pm_mutex);\r\npci_iounmap(pdev, cp->regs);\r\nerr_out_free_res:\r\npci_release_regions(pdev);\r\nerr_write_cacheline:\r\npci_write_config_byte(pdev, PCI_CACHE_LINE_SIZE, orig_cacheline_size);\r\nerr_out_free_netdev:\r\nfree_netdev(dev);\r\nerr_out_disable_pdev:\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\nreturn -ENODEV;\r\n}\r\nstatic void __devexit cas_remove_one(struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct cas *cp;\r\nif (!dev)\r\nreturn;\r\ncp = netdev_priv(dev);\r\nunregister_netdev(dev);\r\nif (cp->fw_data)\r\nvfree(cp->fw_data);\r\nmutex_lock(&cp->pm_mutex);\r\ncancel_work_sync(&cp->reset_task);\r\nif (cp->hw_running)\r\ncas_shutdown(cp);\r\nmutex_unlock(&cp->pm_mutex);\r\n#if 1\r\nif (cp->orig_cacheline_size) {\r\npci_write_config_byte(pdev, PCI_CACHE_LINE_SIZE,\r\ncp->orig_cacheline_size);\r\n}\r\n#endif\r\npci_free_consistent(pdev, sizeof(struct cas_init_block),\r\ncp->init_block, cp->block_dvma);\r\npci_iounmap(pdev, cp->regs);\r\nfree_netdev(dev);\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\n}\r\nstatic int cas_suspend(struct pci_dev *pdev, pm_message_t state)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct cas *cp = netdev_priv(dev);\r\nunsigned long flags;\r\nmutex_lock(&cp->pm_mutex);\r\nif (cp->opened) {\r\nnetif_device_detach(dev);\r\ncas_lock_all_save(cp, flags);\r\ncas_reset(cp, 0);\r\ncas_clean_rings(cp);\r\ncas_unlock_all_restore(cp, flags);\r\n}\r\nif (cp->hw_running)\r\ncas_shutdown(cp);\r\nmutex_unlock(&cp->pm_mutex);\r\nreturn 0;\r\n}\r\nstatic int cas_resume(struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nstruct cas *cp = netdev_priv(dev);\r\nnetdev_info(dev, "resuming\n");\r\nmutex_lock(&cp->pm_mutex);\r\ncas_hard_reset(cp);\r\nif (cp->opened) {\r\nunsigned long flags;\r\ncas_lock_all_save(cp, flags);\r\ncas_reset(cp, 0);\r\ncp->hw_running = 1;\r\ncas_clean_rings(cp);\r\ncas_init_hw(cp, 1);\r\ncas_unlock_all_restore(cp, flags);\r\nnetif_device_attach(dev);\r\n}\r\nmutex_unlock(&cp->pm_mutex);\r\nreturn 0;\r\n}\r\nstatic int __init cas_init(void)\r\n{\r\nif (linkdown_timeout > 0)\r\nlink_transition_timeout = linkdown_timeout * HZ;\r\nelse\r\nlink_transition_timeout = 0;\r\nreturn pci_register_driver(&cas_driver);\r\n}\r\nstatic void __exit cas_cleanup(void)\r\n{\r\npci_unregister_driver(&cas_driver);\r\n}
