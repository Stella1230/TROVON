static dma_addr_t xen_phys_to_bus(phys_addr_t paddr)\r\n{\r\nreturn phys_to_machine(XPADDR(paddr)).maddr;\r\n}\r\nstatic phys_addr_t xen_bus_to_phys(dma_addr_t baddr)\r\n{\r\nreturn machine_to_phys(XMADDR(baddr)).paddr;\r\n}\r\nstatic dma_addr_t xen_virt_to_bus(void *address)\r\n{\r\nreturn xen_phys_to_bus(virt_to_phys(address));\r\n}\r\nstatic int check_pages_physically_contiguous(unsigned long pfn,\r\nunsigned int offset,\r\nsize_t length)\r\n{\r\nunsigned long next_mfn;\r\nint i;\r\nint nr_pages;\r\nnext_mfn = pfn_to_mfn(pfn);\r\nnr_pages = (offset + length + PAGE_SIZE-1) >> PAGE_SHIFT;\r\nfor (i = 1; i < nr_pages; i++) {\r\nif (pfn_to_mfn(++pfn) != ++next_mfn)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int range_straddles_page_boundary(phys_addr_t p, size_t size)\r\n{\r\nunsigned long pfn = PFN_DOWN(p);\r\nunsigned int offset = p & ~PAGE_MASK;\r\nif (offset + size <= PAGE_SIZE)\r\nreturn 0;\r\nif (check_pages_physically_contiguous(pfn, offset, size))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int is_xen_swiotlb_buffer(dma_addr_t dma_addr)\r\n{\r\nunsigned long mfn = PFN_DOWN(dma_addr);\r\nunsigned long pfn = mfn_to_local_pfn(mfn);\r\nphys_addr_t paddr;\r\nif (pfn_valid(pfn)) {\r\npaddr = PFN_PHYS(pfn);\r\nreturn paddr >= virt_to_phys(xen_io_tlb_start) &&\r\npaddr < virt_to_phys(xen_io_tlb_end);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nxen_swiotlb_fixup(void *buf, size_t size, unsigned long nslabs)\r\n{\r\nint i, rc;\r\nint dma_bits;\r\ndma_bits = get_order(IO_TLB_SEGSIZE << IO_TLB_SHIFT) + PAGE_SHIFT;\r\ni = 0;\r\ndo {\r\nint slabs = min(nslabs - i, (unsigned long)IO_TLB_SEGSIZE);\r\ndo {\r\nrc = xen_create_contiguous_region(\r\n(unsigned long)buf + (i << IO_TLB_SHIFT),\r\nget_order(slabs << IO_TLB_SHIFT),\r\ndma_bits);\r\n} while (rc && dma_bits++ < max_dma_bits);\r\nif (rc)\r\nreturn rc;\r\ni += slabs;\r\n} while (i < nslabs);\r\nreturn 0;\r\n}\r\nvoid __init xen_swiotlb_init(int verbose)\r\n{\r\nunsigned long bytes;\r\nint rc = -ENOMEM;\r\nunsigned long nr_tbl;\r\nchar *m = NULL;\r\nunsigned int repeat = 3;\r\nnr_tbl = swiotlb_nr_tbl();\r\nif (nr_tbl)\r\nxen_io_tlb_nslabs = nr_tbl;\r\nelse {\r\nxen_io_tlb_nslabs = (64 * 1024 * 1024 >> IO_TLB_SHIFT);\r\nxen_io_tlb_nslabs = ALIGN(xen_io_tlb_nslabs, IO_TLB_SEGSIZE);\r\n}\r\nretry:\r\nbytes = xen_io_tlb_nslabs << IO_TLB_SHIFT;\r\nxen_io_tlb_start = alloc_bootmem_pages(PAGE_ALIGN(bytes));\r\nif (!xen_io_tlb_start) {\r\nm = "Cannot allocate Xen-SWIOTLB buffer!\n";\r\ngoto error;\r\n}\r\nxen_io_tlb_end = xen_io_tlb_start + bytes;\r\nrc = xen_swiotlb_fixup(xen_io_tlb_start,\r\nbytes,\r\nxen_io_tlb_nslabs);\r\nif (rc) {\r\nfree_bootmem(__pa(xen_io_tlb_start), PAGE_ALIGN(bytes));\r\nm = "Failed to get contiguous memory for DMA from Xen!\n"\\r\n"You either: don't have the permissions, do not have"\\r\n" enough free memory under 4GB, or the hypervisor memory"\\r\n"is too fragmented!";\r\ngoto error;\r\n}\r\nstart_dma_addr = xen_virt_to_bus(xen_io_tlb_start);\r\nswiotlb_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs, verbose);\r\nreturn;\r\nerror:\r\nif (repeat--) {\r\nxen_io_tlb_nslabs = max(1024UL,\r\n(xen_io_tlb_nslabs >> 1));\r\nprintk(KERN_INFO "Xen-SWIOTLB: Lowering to %luMB\n",\r\n(xen_io_tlb_nslabs << IO_TLB_SHIFT) >> 20);\r\ngoto retry;\r\n}\r\nxen_raw_printk("%s (rc:%d)", m, rc);\r\npanic("%s (rc:%d)", m, rc);\r\n}\r\nvoid *\r\nxen_swiotlb_alloc_coherent(struct device *hwdev, size_t size,\r\ndma_addr_t *dma_handle, gfp_t flags,\r\nstruct dma_attrs *attrs)\r\n{\r\nvoid *ret;\r\nint order = get_order(size);\r\nu64 dma_mask = DMA_BIT_MASK(32);\r\nunsigned long vstart;\r\nphys_addr_t phys;\r\ndma_addr_t dev_addr;\r\nflags &= ~(__GFP_DMA | __GFP_HIGHMEM);\r\nif (dma_alloc_from_coherent(hwdev, size, dma_handle, &ret))\r\nreturn ret;\r\nvstart = __get_free_pages(flags, order);\r\nret = (void *)vstart;\r\nif (!ret)\r\nreturn ret;\r\nif (hwdev && hwdev->coherent_dma_mask)\r\ndma_mask = hwdev->coherent_dma_mask;\r\nphys = virt_to_phys(ret);\r\ndev_addr = xen_phys_to_bus(phys);\r\nif (((dev_addr + size - 1 <= dma_mask)) &&\r\n!range_straddles_page_boundary(phys, size))\r\n*dma_handle = dev_addr;\r\nelse {\r\nif (xen_create_contiguous_region(vstart, order,\r\nfls64(dma_mask)) != 0) {\r\nfree_pages(vstart, order);\r\nreturn NULL;\r\n}\r\n*dma_handle = virt_to_machine(ret).maddr;\r\n}\r\nmemset(ret, 0, size);\r\nreturn ret;\r\n}\r\nvoid\r\nxen_swiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,\r\ndma_addr_t dev_addr, struct dma_attrs *attrs)\r\n{\r\nint order = get_order(size);\r\nphys_addr_t phys;\r\nu64 dma_mask = DMA_BIT_MASK(32);\r\nif (dma_release_from_coherent(hwdev, order, vaddr))\r\nreturn;\r\nif (hwdev && hwdev->coherent_dma_mask)\r\ndma_mask = hwdev->coherent_dma_mask;\r\nphys = virt_to_phys(vaddr);\r\nif (((dev_addr + size - 1 > dma_mask)) ||\r\nrange_straddles_page_boundary(phys, size))\r\nxen_destroy_contiguous_region((unsigned long)vaddr, order);\r\nfree_pages((unsigned long)vaddr, order);\r\n}\r\ndma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nphys_addr_t phys = page_to_phys(page) + offset;\r\ndma_addr_t dev_addr = xen_phys_to_bus(phys);\r\nvoid *map;\r\nBUG_ON(dir == DMA_NONE);\r\nif (dma_capable(dev, dev_addr, size) &&\r\n!range_straddles_page_boundary(phys, size) && !swiotlb_force)\r\nreturn dev_addr;\r\nmap = swiotlb_tbl_map_single(dev, start_dma_addr, phys, size, dir);\r\nif (!map)\r\nreturn DMA_ERROR_CODE;\r\ndev_addr = xen_virt_to_bus(map);\r\nif (!dma_capable(dev, dev_addr, size)) {\r\nswiotlb_tbl_unmap_single(dev, map, size, dir);\r\ndev_addr = 0;\r\n}\r\nreturn dev_addr;\r\n}\r\nstatic void xen_unmap_single(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nphys_addr_t paddr = xen_bus_to_phys(dev_addr);\r\nBUG_ON(dir == DMA_NONE);\r\nif (is_xen_swiotlb_buffer(dev_addr)) {\r\nswiotlb_tbl_unmap_single(hwdev, phys_to_virt(paddr), size, dir);\r\nreturn;\r\n}\r\nif (dir != DMA_FROM_DEVICE)\r\nreturn;\r\ndma_mark_clean(phys_to_virt(paddr), size);\r\n}\r\nvoid xen_swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nxen_unmap_single(hwdev, dev_addr, size, dir);\r\n}\r\nstatic void\r\nxen_swiotlb_sync_single(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nphys_addr_t paddr = xen_bus_to_phys(dev_addr);\r\nBUG_ON(dir == DMA_NONE);\r\nif (is_xen_swiotlb_buffer(dev_addr)) {\r\nswiotlb_tbl_sync_single(hwdev, phys_to_virt(paddr), size, dir,\r\ntarget);\r\nreturn;\r\n}\r\nif (dir != DMA_FROM_DEVICE)\r\nreturn;\r\ndma_mark_clean(phys_to_virt(paddr), size);\r\n}\r\nvoid\r\nxen_swiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nxen_swiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_CPU);\r\n}\r\nvoid\r\nxen_swiotlb_sync_single_for_device(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nxen_swiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_DEVICE);\r\n}\r\nint\r\nxen_swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nBUG_ON(dir == DMA_NONE);\r\nfor_each_sg(sgl, sg, nelems, i) {\r\nphys_addr_t paddr = sg_phys(sg);\r\ndma_addr_t dev_addr = xen_phys_to_bus(paddr);\r\nif (swiotlb_force ||\r\n!dma_capable(hwdev, dev_addr, sg->length) ||\r\nrange_straddles_page_boundary(paddr, sg->length)) {\r\nvoid *map = swiotlb_tbl_map_single(hwdev,\r\nstart_dma_addr,\r\nsg_phys(sg),\r\nsg->length, dir);\r\nif (!map) {\r\nxen_swiotlb_unmap_sg_attrs(hwdev, sgl, i, dir,\r\nattrs);\r\nsgl[0].dma_length = 0;\r\nreturn DMA_ERROR_CODE;\r\n}\r\nsg->dma_address = xen_virt_to_bus(map);\r\n} else\r\nsg->dma_address = dev_addr;\r\nsg->dma_length = sg->length;\r\n}\r\nreturn nelems;\r\n}\r\nint\r\nxen_swiotlb_map_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,\r\nenum dma_data_direction dir)\r\n{\r\nreturn xen_swiotlb_map_sg_attrs(hwdev, sgl, nelems, dir, NULL);\r\n}\r\nvoid\r\nxen_swiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nBUG_ON(dir == DMA_NONE);\r\nfor_each_sg(sgl, sg, nelems, i)\r\nxen_unmap_single(hwdev, sg->dma_address, sg->dma_length, dir);\r\n}\r\nvoid\r\nxen_swiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,\r\nenum dma_data_direction dir)\r\n{\r\nreturn xen_swiotlb_unmap_sg_attrs(hwdev, sgl, nelems, dir, NULL);\r\n}\r\nstatic void\r\nxen_swiotlb_sync_sg(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(sgl, sg, nelems, i)\r\nxen_swiotlb_sync_single(hwdev, sg->dma_address,\r\nsg->dma_length, dir, target);\r\n}\r\nvoid\r\nxen_swiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,\r\nint nelems, enum dma_data_direction dir)\r\n{\r\nxen_swiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_CPU);\r\n}\r\nvoid\r\nxen_swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,\r\nint nelems, enum dma_data_direction dir)\r\n{\r\nxen_swiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_DEVICE);\r\n}\r\nint\r\nxen_swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)\r\n{\r\nreturn !dma_addr;\r\n}\r\nint\r\nxen_swiotlb_dma_supported(struct device *hwdev, u64 mask)\r\n{\r\nreturn xen_virt_to_bus(xen_io_tlb_end - 1) <= mask;\r\n}
