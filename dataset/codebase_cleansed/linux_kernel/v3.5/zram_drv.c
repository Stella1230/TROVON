static void zram_stat_inc(u32 *v)\r\n{\r\n*v = *v + 1;\r\n}\r\nstatic void zram_stat_dec(u32 *v)\r\n{\r\n*v = *v - 1;\r\n}\r\nstatic void zram_stat64_add(struct zram *zram, u64 *v, u64 inc)\r\n{\r\nspin_lock(&zram->stat64_lock);\r\n*v = *v + inc;\r\nspin_unlock(&zram->stat64_lock);\r\n}\r\nstatic void zram_stat64_sub(struct zram *zram, u64 *v, u64 dec)\r\n{\r\nspin_lock(&zram->stat64_lock);\r\n*v = *v - dec;\r\nspin_unlock(&zram->stat64_lock);\r\n}\r\nstatic void zram_stat64_inc(struct zram *zram, u64 *v)\r\n{\r\nzram_stat64_add(zram, v, 1);\r\n}\r\nstatic int zram_test_flag(struct zram *zram, u32 index,\r\nenum zram_pageflags flag)\r\n{\r\nreturn zram->table[index].flags & BIT(flag);\r\n}\r\nstatic void zram_set_flag(struct zram *zram, u32 index,\r\nenum zram_pageflags flag)\r\n{\r\nzram->table[index].flags |= BIT(flag);\r\n}\r\nstatic void zram_clear_flag(struct zram *zram, u32 index,\r\nenum zram_pageflags flag)\r\n{\r\nzram->table[index].flags &= ~BIT(flag);\r\n}\r\nstatic int page_zero_filled(void *ptr)\r\n{\r\nunsigned int pos;\r\nunsigned long *page;\r\npage = (unsigned long *)ptr;\r\nfor (pos = 0; pos != PAGE_SIZE / sizeof(*page); pos++) {\r\nif (page[pos])\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void zram_set_disksize(struct zram *zram, size_t totalram_bytes)\r\n{\r\nif (!zram->disksize) {\r\npr_info(\r\n"disk size not provided. You can use disksize_kb module "\r\n"param to specify size.\nUsing default: (%u%% of RAM).\n",\r\ndefault_disksize_perc_ram\r\n);\r\nzram->disksize = default_disksize_perc_ram *\r\n(totalram_bytes / 100);\r\n}\r\nif (zram->disksize > 2 * (totalram_bytes)) {\r\npr_info(\r\n"There is little point creating a zram of greater than "\r\n"twice the size of memory since we expect a 2:1 compression "\r\n"ratio. Note that zram uses about 0.1%% of the size of "\r\n"the disk when not in use so a huge zram is "\r\n"wasteful.\n"\r\n"\tMemory Size: %zu kB\n"\r\n"\tSize you selected: %llu kB\n"\r\n"Continuing anyway ...\n",\r\ntotalram_bytes >> 10, zram->disksize\r\n);\r\n}\r\nzram->disksize &= PAGE_MASK;\r\n}\r\nstatic void zram_free_page(struct zram *zram, size_t index)\r\n{\r\nvoid *handle = zram->table[index].handle;\r\nif (unlikely(!handle)) {\r\nif (zram_test_flag(zram, index, ZRAM_ZERO)) {\r\nzram_clear_flag(zram, index, ZRAM_ZERO);\r\nzram_stat_dec(&zram->stats.pages_zero);\r\n}\r\nreturn;\r\n}\r\nif (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED))) {\r\n__free_page(handle);\r\nzram_clear_flag(zram, index, ZRAM_UNCOMPRESSED);\r\nzram_stat_dec(&zram->stats.pages_expand);\r\ngoto out;\r\n}\r\nzs_free(zram->mem_pool, handle);\r\nif (zram->table[index].size <= PAGE_SIZE / 2)\r\nzram_stat_dec(&zram->stats.good_compress);\r\nout:\r\nzram_stat64_sub(zram, &zram->stats.compr_size,\r\nzram->table[index].size);\r\nzram_stat_dec(&zram->stats.pages_stored);\r\nzram->table[index].handle = NULL;\r\nzram->table[index].size = 0;\r\n}\r\nstatic void handle_zero_page(struct bio_vec *bvec)\r\n{\r\nstruct page *page = bvec->bv_page;\r\nvoid *user_mem;\r\nuser_mem = kmap_atomic(page);\r\nmemset(user_mem + bvec->bv_offset, 0, bvec->bv_len);\r\nkunmap_atomic(user_mem);\r\nflush_dcache_page(page);\r\n}\r\nstatic void handle_uncompressed_page(struct zram *zram, struct bio_vec *bvec,\r\nu32 index, int offset)\r\n{\r\nstruct page *page = bvec->bv_page;\r\nunsigned char *user_mem, *cmem;\r\nuser_mem = kmap_atomic(page);\r\ncmem = kmap_atomic(zram->table[index].handle);\r\nmemcpy(user_mem + bvec->bv_offset, cmem + offset, bvec->bv_len);\r\nkunmap_atomic(cmem);\r\nkunmap_atomic(user_mem);\r\nflush_dcache_page(page);\r\n}\r\nstatic inline int is_partial_io(struct bio_vec *bvec)\r\n{\r\nreturn bvec->bv_len != PAGE_SIZE;\r\n}\r\nstatic int zram_bvec_read(struct zram *zram, struct bio_vec *bvec,\r\nu32 index, int offset, struct bio *bio)\r\n{\r\nint ret;\r\nsize_t clen;\r\nstruct page *page;\r\nstruct zobj_header *zheader;\r\nunsigned char *user_mem, *cmem, *uncmem = NULL;\r\npage = bvec->bv_page;\r\nif (zram_test_flag(zram, index, ZRAM_ZERO)) {\r\nhandle_zero_page(bvec);\r\nreturn 0;\r\n}\r\nif (unlikely(!zram->table[index].handle)) {\r\npr_debug("Read before write: sector=%lu, size=%u",\r\n(ulong)(bio->bi_sector), bio->bi_size);\r\nhandle_zero_page(bvec);\r\nreturn 0;\r\n}\r\nif (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED))) {\r\nhandle_uncompressed_page(zram, bvec, index, offset);\r\nreturn 0;\r\n}\r\nif (is_partial_io(bvec)) {\r\nuncmem = kmalloc(PAGE_SIZE, GFP_KERNEL);\r\nif (!uncmem) {\r\npr_info("Error allocating temp memory!\n");\r\nreturn -ENOMEM;\r\n}\r\n}\r\nuser_mem = kmap_atomic(page);\r\nif (!is_partial_io(bvec))\r\nuncmem = user_mem;\r\nclen = PAGE_SIZE;\r\ncmem = zs_map_object(zram->mem_pool, zram->table[index].handle);\r\nret = lzo1x_decompress_safe(cmem + sizeof(*zheader),\r\nzram->table[index].size,\r\nuncmem, &clen);\r\nif (is_partial_io(bvec)) {\r\nmemcpy(user_mem + bvec->bv_offset, uncmem + offset,\r\nbvec->bv_len);\r\nkfree(uncmem);\r\n}\r\nzs_unmap_object(zram->mem_pool, zram->table[index].handle);\r\nkunmap_atomic(user_mem);\r\nif (unlikely(ret != LZO_E_OK)) {\r\npr_err("Decompression failed! err=%d, page=%u\n", ret, index);\r\nzram_stat64_inc(zram, &zram->stats.failed_reads);\r\nreturn ret;\r\n}\r\nflush_dcache_page(page);\r\nreturn 0;\r\n}\r\nstatic int zram_read_before_write(struct zram *zram, char *mem, u32 index)\r\n{\r\nint ret;\r\nsize_t clen = PAGE_SIZE;\r\nstruct zobj_header *zheader;\r\nunsigned char *cmem;\r\nif (zram_test_flag(zram, index, ZRAM_ZERO) ||\r\n!zram->table[index].handle) {\r\nmemset(mem, 0, PAGE_SIZE);\r\nreturn 0;\r\n}\r\ncmem = zs_map_object(zram->mem_pool, zram->table[index].handle);\r\nif (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED))) {\r\nmemcpy(mem, cmem, PAGE_SIZE);\r\nkunmap_atomic(cmem);\r\nreturn 0;\r\n}\r\nret = lzo1x_decompress_safe(cmem + sizeof(*zheader),\r\nzram->table[index].size,\r\nmem, &clen);\r\nzs_unmap_object(zram->mem_pool, zram->table[index].handle);\r\nif (unlikely(ret != LZO_E_OK)) {\r\npr_err("Decompression failed! err=%d, page=%u\n", ret, index);\r\nzram_stat64_inc(zram, &zram->stats.failed_reads);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic int zram_bvec_write(struct zram *zram, struct bio_vec *bvec, u32 index,\r\nint offset)\r\n{\r\nint ret;\r\nu32 store_offset;\r\nsize_t clen;\r\nvoid *handle;\r\nstruct zobj_header *zheader;\r\nstruct page *page, *page_store;\r\nunsigned char *user_mem, *cmem, *src, *uncmem = NULL;\r\npage = bvec->bv_page;\r\nsrc = zram->compress_buffer;\r\nif (is_partial_io(bvec)) {\r\nuncmem = kmalloc(PAGE_SIZE, GFP_KERNEL);\r\nif (!uncmem) {\r\npr_info("Error allocating temp memory!\n");\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = zram_read_before_write(zram, uncmem, index);\r\nif (ret) {\r\nkfree(uncmem);\r\ngoto out;\r\n}\r\n}\r\nif (zram->table[index].handle ||\r\nzram_test_flag(zram, index, ZRAM_ZERO))\r\nzram_free_page(zram, index);\r\nuser_mem = kmap_atomic(page);\r\nif (is_partial_io(bvec))\r\nmemcpy(uncmem + offset, user_mem + bvec->bv_offset,\r\nbvec->bv_len);\r\nelse\r\nuncmem = user_mem;\r\nif (page_zero_filled(uncmem)) {\r\nkunmap_atomic(user_mem);\r\nif (is_partial_io(bvec))\r\nkfree(uncmem);\r\nzram_stat_inc(&zram->stats.pages_zero);\r\nzram_set_flag(zram, index, ZRAM_ZERO);\r\nret = 0;\r\ngoto out;\r\n}\r\nret = lzo1x_1_compress(uncmem, PAGE_SIZE, src, &clen,\r\nzram->compress_workmem);\r\nkunmap_atomic(user_mem);\r\nif (is_partial_io(bvec))\r\nkfree(uncmem);\r\nif (unlikely(ret != LZO_E_OK)) {\r\npr_err("Compression failed! err=%d\n", ret);\r\ngoto out;\r\n}\r\nif (unlikely(clen > max_zpage_size)) {\r\nclen = PAGE_SIZE;\r\npage_store = alloc_page(GFP_NOIO | __GFP_HIGHMEM);\r\nif (unlikely(!page_store)) {\r\npr_info("Error allocating memory for "\r\n"incompressible page: %u\n", index);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nstore_offset = 0;\r\nzram_set_flag(zram, index, ZRAM_UNCOMPRESSED);\r\nzram_stat_inc(&zram->stats.pages_expand);\r\nhandle = page_store;\r\nsrc = kmap_atomic(page);\r\ncmem = kmap_atomic(page_store);\r\ngoto memstore;\r\n}\r\nhandle = zs_malloc(zram->mem_pool, clen + sizeof(*zheader));\r\nif (!handle) {\r\npr_info("Error allocating memory for compressed "\r\n"page: %u, size=%zu\n", index, clen);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ncmem = zs_map_object(zram->mem_pool, handle);\r\nmemstore:\r\n#if 0\r\nif (!zram_test_flag(zram, index, ZRAM_UNCOMPRESSED)) {\r\nzheader = (struct zobj_header *)cmem;\r\nzheader->table_idx = index;\r\ncmem += sizeof(*zheader);\r\n}\r\n#endif\r\nmemcpy(cmem, src, clen);\r\nif (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED))) {\r\nkunmap_atomic(cmem);\r\nkunmap_atomic(src);\r\n} else {\r\nzs_unmap_object(zram->mem_pool, handle);\r\n}\r\nzram->table[index].handle = handle;\r\nzram->table[index].size = clen;\r\nzram_stat64_add(zram, &zram->stats.compr_size, clen);\r\nzram_stat_inc(&zram->stats.pages_stored);\r\nif (clen <= PAGE_SIZE / 2)\r\nzram_stat_inc(&zram->stats.good_compress);\r\nreturn 0;\r\nout:\r\nif (ret)\r\nzram_stat64_inc(zram, &zram->stats.failed_writes);\r\nreturn ret;\r\n}\r\nstatic int zram_bvec_rw(struct zram *zram, struct bio_vec *bvec, u32 index,\r\nint offset, struct bio *bio, int rw)\r\n{\r\nint ret;\r\nif (rw == READ) {\r\ndown_read(&zram->lock);\r\nret = zram_bvec_read(zram, bvec, index, offset, bio);\r\nup_read(&zram->lock);\r\n} else {\r\ndown_write(&zram->lock);\r\nret = zram_bvec_write(zram, bvec, index, offset);\r\nup_write(&zram->lock);\r\n}\r\nreturn ret;\r\n}\r\nstatic void update_position(u32 *index, int *offset, struct bio_vec *bvec)\r\n{\r\nif (*offset + bvec->bv_len >= PAGE_SIZE)\r\n(*index)++;\r\n*offset = (*offset + bvec->bv_len) % PAGE_SIZE;\r\n}\r\nstatic void __zram_make_request(struct zram *zram, struct bio *bio, int rw)\r\n{\r\nint i, offset;\r\nu32 index;\r\nstruct bio_vec *bvec;\r\nswitch (rw) {\r\ncase READ:\r\nzram_stat64_inc(zram, &zram->stats.num_reads);\r\nbreak;\r\ncase WRITE:\r\nzram_stat64_inc(zram, &zram->stats.num_writes);\r\nbreak;\r\n}\r\nindex = bio->bi_sector >> SECTORS_PER_PAGE_SHIFT;\r\noffset = (bio->bi_sector & (SECTORS_PER_PAGE - 1)) << SECTOR_SHIFT;\r\nbio_for_each_segment(bvec, bio, i) {\r\nint max_transfer_size = PAGE_SIZE - offset;\r\nif (bvec->bv_len > max_transfer_size) {\r\nstruct bio_vec bv;\r\nbv.bv_page = bvec->bv_page;\r\nbv.bv_len = max_transfer_size;\r\nbv.bv_offset = bvec->bv_offset;\r\nif (zram_bvec_rw(zram, &bv, index, offset, bio, rw) < 0)\r\ngoto out;\r\nbv.bv_len = bvec->bv_len - max_transfer_size;\r\nbv.bv_offset += max_transfer_size;\r\nif (zram_bvec_rw(zram, &bv, index+1, 0, bio, rw) < 0)\r\ngoto out;\r\n} else\r\nif (zram_bvec_rw(zram, bvec, index, offset, bio, rw)\r\n< 0)\r\ngoto out;\r\nupdate_position(&index, &offset, bvec);\r\n}\r\nset_bit(BIO_UPTODATE, &bio->bi_flags);\r\nbio_endio(bio, 0);\r\nreturn;\r\nout:\r\nbio_io_error(bio);\r\n}\r\nstatic inline int valid_io_request(struct zram *zram, struct bio *bio)\r\n{\r\nif (unlikely(\r\n(bio->bi_sector >= (zram->disksize >> SECTOR_SHIFT)) ||\r\n(bio->bi_sector & (ZRAM_SECTOR_PER_LOGICAL_BLOCK - 1)) ||\r\n(bio->bi_size & (ZRAM_LOGICAL_BLOCK_SIZE - 1)))) {\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void zram_make_request(struct request_queue *queue, struct bio *bio)\r\n{\r\nstruct zram *zram = queue->queuedata;\r\nif (unlikely(!zram->init_done) && zram_init_device(zram))\r\ngoto error;\r\ndown_read(&zram->init_lock);\r\nif (unlikely(!zram->init_done))\r\ngoto error_unlock;\r\nif (!valid_io_request(zram, bio)) {\r\nzram_stat64_inc(zram, &zram->stats.invalid_io);\r\ngoto error_unlock;\r\n}\r\n__zram_make_request(zram, bio, bio_data_dir(bio));\r\nup_read(&zram->init_lock);\r\nreturn;\r\nerror_unlock:\r\nup_read(&zram->init_lock);\r\nerror:\r\nbio_io_error(bio);\r\n}\r\nvoid __zram_reset_device(struct zram *zram)\r\n{\r\nsize_t index;\r\nzram->init_done = 0;\r\nkfree(zram->compress_workmem);\r\nfree_pages((unsigned long)zram->compress_buffer, 1);\r\nzram->compress_workmem = NULL;\r\nzram->compress_buffer = NULL;\r\nfor (index = 0; index < zram->disksize >> PAGE_SHIFT; index++) {\r\nvoid *handle = zram->table[index].handle;\r\nif (!handle)\r\ncontinue;\r\nif (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED)))\r\n__free_page(handle);\r\nelse\r\nzs_free(zram->mem_pool, handle);\r\n}\r\nvfree(zram->table);\r\nzram->table = NULL;\r\nzs_destroy_pool(zram->mem_pool);\r\nzram->mem_pool = NULL;\r\nmemset(&zram->stats, 0, sizeof(zram->stats));\r\nzram->disksize = 0;\r\n}\r\nvoid zram_reset_device(struct zram *zram)\r\n{\r\ndown_write(&zram->init_lock);\r\n__zram_reset_device(zram);\r\nup_write(&zram->init_lock);\r\n}\r\nint zram_init_device(struct zram *zram)\r\n{\r\nint ret;\r\nsize_t num_pages;\r\ndown_write(&zram->init_lock);\r\nif (zram->init_done) {\r\nup_write(&zram->init_lock);\r\nreturn 0;\r\n}\r\nzram_set_disksize(zram, totalram_pages << PAGE_SHIFT);\r\nzram->compress_workmem = kzalloc(LZO1X_MEM_COMPRESS, GFP_KERNEL);\r\nif (!zram->compress_workmem) {\r\npr_err("Error allocating compressor working memory!\n");\r\nret = -ENOMEM;\r\ngoto fail_no_table;\r\n}\r\nzram->compress_buffer =\r\n(void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO, 1);\r\nif (!zram->compress_buffer) {\r\npr_err("Error allocating compressor buffer space\n");\r\nret = -ENOMEM;\r\ngoto fail_no_table;\r\n}\r\nnum_pages = zram->disksize >> PAGE_SHIFT;\r\nzram->table = vzalloc(num_pages * sizeof(*zram->table));\r\nif (!zram->table) {\r\npr_err("Error allocating zram address table\n");\r\nret = -ENOMEM;\r\ngoto fail_no_table;\r\n}\r\nset_capacity(zram->disk, zram->disksize >> SECTOR_SHIFT);\r\nqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, zram->disk->queue);\r\nzram->mem_pool = zs_create_pool("zram", GFP_NOIO | __GFP_HIGHMEM);\r\nif (!zram->mem_pool) {\r\npr_err("Error creating memory pool\n");\r\nret = -ENOMEM;\r\ngoto fail;\r\n}\r\nzram->init_done = 1;\r\nup_write(&zram->init_lock);\r\npr_debug("Initialization done!\n");\r\nreturn 0;\r\nfail_no_table:\r\nzram->disksize = 0;\r\nfail:\r\n__zram_reset_device(zram);\r\nup_write(&zram->init_lock);\r\npr_err("Initialization failed: err=%d\n", ret);\r\nreturn ret;\r\n}\r\nstatic void zram_slot_free_notify(struct block_device *bdev,\r\nunsigned long index)\r\n{\r\nstruct zram *zram;\r\nzram = bdev->bd_disk->private_data;\r\nzram_free_page(zram, index);\r\nzram_stat64_inc(zram, &zram->stats.notify_free);\r\n}\r\nstatic int create_device(struct zram *zram, int device_id)\r\n{\r\nint ret = 0;\r\ninit_rwsem(&zram->lock);\r\ninit_rwsem(&zram->init_lock);\r\nspin_lock_init(&zram->stat64_lock);\r\nzram->queue = blk_alloc_queue(GFP_KERNEL);\r\nif (!zram->queue) {\r\npr_err("Error allocating disk queue for device %d\n",\r\ndevice_id);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nblk_queue_make_request(zram->queue, zram_make_request);\r\nzram->queue->queuedata = zram;\r\nzram->disk = alloc_disk(1);\r\nif (!zram->disk) {\r\nblk_cleanup_queue(zram->queue);\r\npr_warning("Error allocating disk structure for device %d\n",\r\ndevice_id);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nzram->disk->major = zram_major;\r\nzram->disk->first_minor = device_id;\r\nzram->disk->fops = &zram_devops;\r\nzram->disk->queue = zram->queue;\r\nzram->disk->private_data = zram;\r\nsnprintf(zram->disk->disk_name, 16, "zram%d", device_id);\r\nset_capacity(zram->disk, 0);\r\nblk_queue_physical_block_size(zram->disk->queue, PAGE_SIZE);\r\nblk_queue_logical_block_size(zram->disk->queue,\r\nZRAM_LOGICAL_BLOCK_SIZE);\r\nblk_queue_io_min(zram->disk->queue, PAGE_SIZE);\r\nblk_queue_io_opt(zram->disk->queue, PAGE_SIZE);\r\nadd_disk(zram->disk);\r\nret = sysfs_create_group(&disk_to_dev(zram->disk)->kobj,\r\n&zram_disk_attr_group);\r\nif (ret < 0) {\r\npr_warning("Error creating sysfs group");\r\ngoto out;\r\n}\r\nzram->init_done = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic void destroy_device(struct zram *zram)\r\n{\r\nsysfs_remove_group(&disk_to_dev(zram->disk)->kobj,\r\n&zram_disk_attr_group);\r\nif (zram->disk) {\r\ndel_gendisk(zram->disk);\r\nput_disk(zram->disk);\r\n}\r\nif (zram->queue)\r\nblk_cleanup_queue(zram->queue);\r\n}\r\nunsigned int zram_get_num_devices(void)\r\n{\r\nreturn num_devices;\r\n}\r\nstatic int __init zram_init(void)\r\n{\r\nint ret, dev_id;\r\nif (num_devices > max_num_devices) {\r\npr_warning("Invalid value for num_devices: %u\n",\r\nnum_devices);\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nzram_major = register_blkdev(0, "zram");\r\nif (zram_major <= 0) {\r\npr_warning("Unable to get major number\n");\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\nif (!num_devices) {\r\npr_info("num_devices not specified. Using default: 1\n");\r\nnum_devices = 1;\r\n}\r\npr_info("Creating %u devices ...\n", num_devices);\r\nzram_devices = kzalloc(num_devices * sizeof(struct zram), GFP_KERNEL);\r\nif (!zram_devices) {\r\nret = -ENOMEM;\r\ngoto unregister;\r\n}\r\nfor (dev_id = 0; dev_id < num_devices; dev_id++) {\r\nret = create_device(&zram_devices[dev_id], dev_id);\r\nif (ret)\r\ngoto free_devices;\r\n}\r\nreturn 0;\r\nfree_devices:\r\nwhile (dev_id)\r\ndestroy_device(&zram_devices[--dev_id]);\r\nkfree(zram_devices);\r\nunregister:\r\nunregister_blkdev(zram_major, "zram");\r\nout:\r\nreturn ret;\r\n}\r\nstatic void __exit zram_exit(void)\r\n{\r\nint i;\r\nstruct zram *zram;\r\nfor (i = 0; i < num_devices; i++) {\r\nzram = &zram_devices[i];\r\ndestroy_device(zram);\r\nif (zram->init_done)\r\nzram_reset_device(zram);\r\n}\r\nunregister_blkdev(zram_major, "zram");\r\nkfree(zram_devices);\r\npr_debug("Cleanup done!\n");\r\n}
