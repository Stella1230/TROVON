int kthread_should_stop(void)\r\n{\r\nreturn to_kthread(current)->should_stop;\r\n}\r\nbool kthread_freezable_should_stop(bool *was_frozen)\r\n{\r\nbool frozen = false;\r\nmight_sleep();\r\nif (unlikely(freezing(current)))\r\nfrozen = __refrigerator(true);\r\nif (was_frozen)\r\n*was_frozen = frozen;\r\nreturn kthread_should_stop();\r\n}\r\nvoid *kthread_data(struct task_struct *task)\r\n{\r\nreturn to_kthread(task)->data;\r\n}\r\nstatic int kthread(void *_create)\r\n{\r\nstruct kthread_create_info *create = _create;\r\nint (*threadfn)(void *data) = create->threadfn;\r\nvoid *data = create->data;\r\nstruct kthread self;\r\nint ret;\r\nself.should_stop = 0;\r\nself.data = data;\r\ninit_completion(&self.exited);\r\ncurrent->vfork_done = &self.exited;\r\n__set_current_state(TASK_UNINTERRUPTIBLE);\r\ncreate->result = current;\r\ncomplete(&create->done);\r\nschedule();\r\nret = -EINTR;\r\nif (!self.should_stop)\r\nret = threadfn(data);\r\ndo_exit(ret);\r\n}\r\nint tsk_fork_get_node(struct task_struct *tsk)\r\n{\r\n#ifdef CONFIG_NUMA\r\nif (tsk == kthreadd_task)\r\nreturn tsk->pref_node_fork;\r\n#endif\r\nreturn numa_node_id();\r\n}\r\nstatic void create_kthread(struct kthread_create_info *create)\r\n{\r\nint pid;\r\n#ifdef CONFIG_NUMA\r\ncurrent->pref_node_fork = create->node;\r\n#endif\r\npid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);\r\nif (pid < 0) {\r\ncreate->result = ERR_PTR(pid);\r\ncomplete(&create->done);\r\n}\r\n}\r\nstruct task_struct *kthread_create_on_node(int (*threadfn)(void *data),\r\nvoid *data,\r\nint node,\r\nconst char namefmt[],\r\n...)\r\n{\r\nstruct kthread_create_info create;\r\ncreate.threadfn = threadfn;\r\ncreate.data = data;\r\ncreate.node = node;\r\ninit_completion(&create.done);\r\nspin_lock(&kthread_create_lock);\r\nlist_add_tail(&create.list, &kthread_create_list);\r\nspin_unlock(&kthread_create_lock);\r\nwake_up_process(kthreadd_task);\r\nwait_for_completion(&create.done);\r\nif (!IS_ERR(create.result)) {\r\nstatic const struct sched_param param = { .sched_priority = 0 };\r\nva_list args;\r\nva_start(args, namefmt);\r\nvsnprintf(create.result->comm, sizeof(create.result->comm),\r\nnamefmt, args);\r\nva_end(args);\r\nsched_setscheduler_nocheck(create.result, SCHED_NORMAL, &param);\r\nset_cpus_allowed_ptr(create.result, cpu_all_mask);\r\n}\r\nreturn create.result;\r\n}\r\nvoid kthread_bind(struct task_struct *p, unsigned int cpu)\r\n{\r\nif (!wait_task_inactive(p, TASK_UNINTERRUPTIBLE)) {\r\nWARN_ON(1);\r\nreturn;\r\n}\r\ndo_set_cpus_allowed(p, cpumask_of(cpu));\r\np->flags |= PF_THREAD_BOUND;\r\n}\r\nint kthread_stop(struct task_struct *k)\r\n{\r\nstruct kthread *kthread;\r\nint ret;\r\ntrace_sched_kthread_stop(k);\r\nget_task_struct(k);\r\nkthread = to_kthread(k);\r\nbarrier();\r\nif (k->vfork_done != NULL) {\r\nkthread->should_stop = 1;\r\nwake_up_process(k);\r\nwait_for_completion(&kthread->exited);\r\n}\r\nret = k->exit_code;\r\nput_task_struct(k);\r\ntrace_sched_kthread_stop_ret(ret);\r\nreturn ret;\r\n}\r\nint kthreadd(void *unused)\r\n{\r\nstruct task_struct *tsk = current;\r\nset_task_comm(tsk, "kthreadd");\r\nignore_signals(tsk);\r\nset_cpus_allowed_ptr(tsk, cpu_all_mask);\r\nset_mems_allowed(node_states[N_HIGH_MEMORY]);\r\ncurrent->flags |= PF_NOFREEZE;\r\nfor (;;) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (list_empty(&kthread_create_list))\r\nschedule();\r\n__set_current_state(TASK_RUNNING);\r\nspin_lock(&kthread_create_lock);\r\nwhile (!list_empty(&kthread_create_list)) {\r\nstruct kthread_create_info *create;\r\ncreate = list_entry(kthread_create_list.next,\r\nstruct kthread_create_info, list);\r\nlist_del_init(&create->list);\r\nspin_unlock(&kthread_create_lock);\r\ncreate_kthread(create);\r\nspin_lock(&kthread_create_lock);\r\n}\r\nspin_unlock(&kthread_create_lock);\r\n}\r\nreturn 0;\r\n}\r\nvoid __init_kthread_worker(struct kthread_worker *worker,\r\nconst char *name,\r\nstruct lock_class_key *key)\r\n{\r\nspin_lock_init(&worker->lock);\r\nlockdep_set_class_and_name(&worker->lock, key, name);\r\nINIT_LIST_HEAD(&worker->work_list);\r\nworker->task = NULL;\r\n}\r\nint kthread_worker_fn(void *worker_ptr)\r\n{\r\nstruct kthread_worker *worker = worker_ptr;\r\nstruct kthread_work *work;\r\nWARN_ON(worker->task);\r\nworker->task = current;\r\nrepeat:\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (kthread_should_stop()) {\r\n__set_current_state(TASK_RUNNING);\r\nspin_lock_irq(&worker->lock);\r\nworker->task = NULL;\r\nspin_unlock_irq(&worker->lock);\r\nreturn 0;\r\n}\r\nwork = NULL;\r\nspin_lock_irq(&worker->lock);\r\nif (!list_empty(&worker->work_list)) {\r\nwork = list_first_entry(&worker->work_list,\r\nstruct kthread_work, node);\r\nlist_del_init(&work->node);\r\n}\r\nspin_unlock_irq(&worker->lock);\r\nif (work) {\r\n__set_current_state(TASK_RUNNING);\r\nwork->func(work);\r\nsmp_wmb();\r\nwork->done_seq = work->queue_seq;\r\nsmp_mb();\r\nif (atomic_read(&work->flushing))\r\nwake_up_all(&work->done);\r\n} else if (!freezing(current))\r\nschedule();\r\ntry_to_freeze();\r\ngoto repeat;\r\n}\r\nbool queue_kthread_work(struct kthread_worker *worker,\r\nstruct kthread_work *work)\r\n{\r\nbool ret = false;\r\nunsigned long flags;\r\nspin_lock_irqsave(&worker->lock, flags);\r\nif (list_empty(&work->node)) {\r\nlist_add_tail(&work->node, &worker->work_list);\r\nwork->queue_seq++;\r\nif (likely(worker->task))\r\nwake_up_process(worker->task);\r\nret = true;\r\n}\r\nspin_unlock_irqrestore(&worker->lock, flags);\r\nreturn ret;\r\n}\r\nvoid flush_kthread_work(struct kthread_work *work)\r\n{\r\nint seq = work->queue_seq;\r\natomic_inc(&work->flushing);\r\nsmp_mb__after_atomic_inc();\r\nwait_event(work->done, seq - work->done_seq <= 0);\r\natomic_dec(&work->flushing);\r\nsmp_mb__after_atomic_dec();\r\n}\r\nstatic void kthread_flush_work_fn(struct kthread_work *work)\r\n{\r\nstruct kthread_flush_work *fwork =\r\ncontainer_of(work, struct kthread_flush_work, work);\r\ncomplete(&fwork->done);\r\n}\r\nvoid flush_kthread_worker(struct kthread_worker *worker)\r\n{\r\nstruct kthread_flush_work fwork = {\r\nKTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),\r\nCOMPLETION_INITIALIZER_ONSTACK(fwork.done),\r\n};\r\nqueue_kthread_work(worker, &fwork.work);\r\nwait_for_completion(&fwork.done);\r\n}
