static struct sa11x0_dma_chan *to_sa11x0_dma_chan(struct dma_chan *chan)\r\n{\r\nreturn container_of(chan, struct sa11x0_dma_chan, chan);\r\n}\r\nstatic struct sa11x0_dma_dev *to_sa11x0_dma(struct dma_device *dmadev)\r\n{\r\nreturn container_of(dmadev, struct sa11x0_dma_dev, slave);\r\n}\r\nstatic struct sa11x0_dma_desc *to_sa11x0_dma_tx(struct dma_async_tx_descriptor *tx)\r\n{\r\nreturn container_of(tx, struct sa11x0_dma_desc, tx);\r\n}\r\nstatic struct sa11x0_dma_desc *sa11x0_dma_next_desc(struct sa11x0_dma_chan *c)\r\n{\r\nif (list_empty(&c->desc_issued))\r\nreturn NULL;\r\nreturn list_first_entry(&c->desc_issued, struct sa11x0_dma_desc, node);\r\n}\r\nstatic void sa11x0_dma_start_desc(struct sa11x0_dma_phy *p, struct sa11x0_dma_desc *txd)\r\n{\r\nlist_del(&txd->node);\r\np->txd_load = txd;\r\np->sg_load = 0;\r\ndev_vdbg(p->dev->slave.dev, "pchan %u: txd %p[%x]: starting: DDAR:%x\n",\r\np->num, txd, txd->tx.cookie, txd->ddar);\r\n}\r\nstatic void noinline sa11x0_dma_start_sg(struct sa11x0_dma_phy *p,\r\nstruct sa11x0_dma_chan *c)\r\n{\r\nstruct sa11x0_dma_desc *txd = p->txd_load;\r\nstruct sa11x0_dma_sg *sg;\r\nvoid __iomem *base = p->base;\r\nunsigned dbsx, dbtx;\r\nu32 dcsr;\r\nif (!txd)\r\nreturn;\r\ndcsr = readl_relaxed(base + DMA_DCSR_R);\r\nif ((dcsr & (DCSR_STRTA | DCSR_STRTB)) == (DCSR_STRTA | DCSR_STRTB))\r\nreturn;\r\nif (p->sg_load == txd->sglen) {\r\nstruct sa11x0_dma_desc *txn = sa11x0_dma_next_desc(c);\r\nif (txn && txn->ddar == txd->ddar) {\r\ntxd = txn;\r\nsa11x0_dma_start_desc(p, txn);\r\n} else {\r\np->txd_load = NULL;\r\nreturn;\r\n}\r\n}\r\nsg = &txd->sg[p->sg_load++];\r\nif (((dcsr & (DCSR_BIU | DCSR_STRTB)) == (DCSR_BIU | DCSR_STRTB)) ||\r\n((dcsr & (DCSR_BIU | DCSR_STRTA)) == 0)) {\r\ndbsx = DMA_DBSA;\r\ndbtx = DMA_DBTA;\r\ndcsr = DCSR_STRTA | DCSR_IE | DCSR_RUN;\r\n} else {\r\ndbsx = DMA_DBSB;\r\ndbtx = DMA_DBTB;\r\ndcsr = DCSR_STRTB | DCSR_IE | DCSR_RUN;\r\n}\r\nwritel_relaxed(sg->addr, base + dbsx);\r\nwritel_relaxed(sg->len, base + dbtx);\r\nwritel(dcsr, base + DMA_DCSR_S);\r\ndev_dbg(p->dev->slave.dev, "pchan %u: load: DCSR:%02x DBS%c:%08x DBT%c:%08x\n",\r\np->num, dcsr,\r\n'A' + (dbsx == DMA_DBSB), sg->addr,\r\n'A' + (dbtx == DMA_DBTB), sg->len);\r\n}\r\nstatic void noinline sa11x0_dma_complete(struct sa11x0_dma_phy *p,\r\nstruct sa11x0_dma_chan *c)\r\n{\r\nstruct sa11x0_dma_desc *txd = p->txd_done;\r\nif (++p->sg_done == txd->sglen) {\r\nstruct sa11x0_dma_dev *d = p->dev;\r\ndev_vdbg(d->slave.dev, "pchan %u: txd %p[%x]: completed\n",\r\np->num, p->txd_done, p->txd_done->tx.cookie);\r\nc->lc = txd->tx.cookie;\r\nspin_lock(&d->lock);\r\nlist_add_tail(&txd->node, &d->desc_complete);\r\nspin_unlock(&d->lock);\r\np->sg_done = 0;\r\np->txd_done = p->txd_load;\r\ntasklet_schedule(&d->task);\r\n}\r\nsa11x0_dma_start_sg(p, c);\r\n}\r\nstatic irqreturn_t sa11x0_dma_irq(int irq, void *dev_id)\r\n{\r\nstruct sa11x0_dma_phy *p = dev_id;\r\nstruct sa11x0_dma_dev *d = p->dev;\r\nstruct sa11x0_dma_chan *c;\r\nu32 dcsr;\r\ndcsr = readl_relaxed(p->base + DMA_DCSR_R);\r\nif (!(dcsr & (DCSR_ERROR | DCSR_DONEA | DCSR_DONEB)))\r\nreturn IRQ_NONE;\r\nwritel_relaxed(dcsr & (DCSR_ERROR | DCSR_DONEA | DCSR_DONEB),\r\np->base + DMA_DCSR_C);\r\ndev_dbg(d->slave.dev, "pchan %u: irq: DCSR:%02x\n", p->num, dcsr);\r\nif (dcsr & DCSR_ERROR) {\r\ndev_err(d->slave.dev, "pchan %u: error. DCSR:%02x DDAR:%08x DBSA:%08x DBTA:%08x DBSB:%08x DBTB:%08x\n",\r\np->num, dcsr,\r\nreadl_relaxed(p->base + DMA_DDAR),\r\nreadl_relaxed(p->base + DMA_DBSA),\r\nreadl_relaxed(p->base + DMA_DBTA),\r\nreadl_relaxed(p->base + DMA_DBSB),\r\nreadl_relaxed(p->base + DMA_DBTB));\r\n}\r\nc = p->vchan;\r\nif (c) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->lock, flags);\r\nif (c->phy == p) {\r\nif (dcsr & DCSR_DONEA)\r\nsa11x0_dma_complete(p, c);\r\nif (dcsr & DCSR_DONEB)\r\nsa11x0_dma_complete(p, c);\r\n}\r\nspin_unlock_irqrestore(&c->lock, flags);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void sa11x0_dma_start_txd(struct sa11x0_dma_chan *c)\r\n{\r\nstruct sa11x0_dma_desc *txd = sa11x0_dma_next_desc(c);\r\nif (txd) {\r\nstruct sa11x0_dma_phy *p = c->phy;\r\nsa11x0_dma_start_desc(p, txd);\r\np->txd_done = txd;\r\np->sg_done = 0;\r\nWARN_ON(readl_relaxed(p->base + DMA_DCSR_R) &\r\n(DCSR_STRTA | DCSR_STRTB));\r\nwritel_relaxed(DCSR_RUN | DCSR_STRTA | DCSR_STRTB,\r\np->base + DMA_DCSR_C);\r\nwritel_relaxed(txd->ddar, p->base + DMA_DDAR);\r\nsa11x0_dma_start_sg(p, c);\r\nsa11x0_dma_start_sg(p, c);\r\n}\r\n}\r\nstatic void sa11x0_dma_tasklet(unsigned long arg)\r\n{\r\nstruct sa11x0_dma_dev *d = (struct sa11x0_dma_dev *)arg;\r\nstruct sa11x0_dma_phy *p;\r\nstruct sa11x0_dma_chan *c;\r\nstruct sa11x0_dma_desc *txd, *txn;\r\nLIST_HEAD(head);\r\nunsigned pch, pch_alloc = 0;\r\ndev_dbg(d->slave.dev, "tasklet enter\n");\r\nspin_lock_irq(&d->lock);\r\nlist_splice_init(&d->desc_complete, &head);\r\nspin_unlock_irq(&d->lock);\r\nlist_for_each_entry(txd, &head, node) {\r\nc = to_sa11x0_dma_chan(txd->tx.chan);\r\ndev_dbg(d->slave.dev, "vchan %p: txd %p[%x] completed\n",\r\nc, txd, txd->tx.cookie);\r\nspin_lock_irq(&c->lock);\r\np = c->phy;\r\nif (p) {\r\nif (!p->txd_done)\r\nsa11x0_dma_start_txd(c);\r\nif (!p->txd_done) {\r\ndev_dbg(d->slave.dev, "pchan %u: free\n", p->num);\r\nc->phy = NULL;\r\np->vchan = NULL;\r\n}\r\n}\r\nspin_unlock_irq(&c->lock);\r\n}\r\nspin_lock_irq(&d->lock);\r\nfor (pch = 0; pch < NR_PHY_CHAN; pch++) {\r\np = &d->phy[pch];\r\nif (p->vchan == NULL && !list_empty(&d->chan_pending)) {\r\nc = list_first_entry(&d->chan_pending,\r\nstruct sa11x0_dma_chan, node);\r\nlist_del_init(&c->node);\r\npch_alloc |= 1 << pch;\r\np->vchan = c;\r\ndev_dbg(d->slave.dev, "pchan %u: alloc vchan %p\n", pch, c);\r\n}\r\n}\r\nspin_unlock_irq(&d->lock);\r\nfor (pch = 0; pch < NR_PHY_CHAN; pch++) {\r\nif (pch_alloc & (1 << pch)) {\r\np = &d->phy[pch];\r\nc = p->vchan;\r\nspin_lock_irq(&c->lock);\r\nc->phy = p;\r\nsa11x0_dma_start_txd(c);\r\nspin_unlock_irq(&c->lock);\r\n}\r\n}\r\nlist_for_each_entry_safe(txd, txn, &head, node) {\r\ndma_async_tx_callback callback = txd->tx.callback;\r\nvoid *callback_param = txd->tx.callback_param;\r\ndev_dbg(d->slave.dev, "txd %p[%x]: callback and free\n",\r\ntxd, txd->tx.cookie);\r\nkfree(txd);\r\nif (callback)\r\ncallback(callback_param);\r\n}\r\ndev_dbg(d->slave.dev, "tasklet exit\n");\r\n}\r\nstatic void sa11x0_dma_desc_free(struct sa11x0_dma_dev *d, struct list_head *head)\r\n{\r\nstruct sa11x0_dma_desc *txd, *txn;\r\nlist_for_each_entry_safe(txd, txn, head, node) {\r\ndev_dbg(d->slave.dev, "txd %p: freeing\n", txd);\r\nkfree(txd);\r\n}\r\n}\r\nstatic int sa11x0_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nreturn 0;\r\n}\r\nstatic void sa11x0_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct sa11x0_dma_chan *c = to_sa11x0_dma_chan(chan);\r\nstruct sa11x0_dma_dev *d = to_sa11x0_dma(chan->device);\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&c->lock, flags);\r\nspin_lock(&d->lock);\r\nlist_del_init(&c->node);\r\nspin_unlock(&d->lock);\r\nlist_splice_tail_init(&c->desc_submitted, &head);\r\nlist_splice_tail_init(&c->desc_issued, &head);\r\nspin_unlock_irqrestore(&c->lock, flags);\r\nsa11x0_dma_desc_free(d, &head);\r\n}\r\nstatic dma_addr_t sa11x0_dma_pos(struct sa11x0_dma_phy *p)\r\n{\r\nunsigned reg;\r\nu32 dcsr;\r\ndcsr = readl_relaxed(p->base + DMA_DCSR_R);\r\nif ((dcsr & (DCSR_BIU | DCSR_STRTA)) == DCSR_STRTA ||\r\n(dcsr & (DCSR_BIU | DCSR_STRTB)) == DCSR_BIU)\r\nreg = DMA_DBSA;\r\nelse\r\nreg = DMA_DBSB;\r\nreturn readl_relaxed(p->base + reg);\r\n}\r\nstatic enum dma_status sa11x0_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie, struct dma_tx_state *state)\r\n{\r\nstruct sa11x0_dma_chan *c = to_sa11x0_dma_chan(chan);\r\nstruct sa11x0_dma_dev *d = to_sa11x0_dma(chan->device);\r\nstruct sa11x0_dma_phy *p;\r\nstruct sa11x0_dma_desc *txd;\r\ndma_cookie_t last_used, last_complete;\r\nunsigned long flags;\r\nenum dma_status ret;\r\nsize_t bytes = 0;\r\nlast_used = c->chan.cookie;\r\nlast_complete = c->lc;\r\nret = dma_async_is_complete(cookie, last_complete, last_used);\r\nif (ret == DMA_SUCCESS) {\r\ndma_set_tx_state(state, last_complete, last_used, 0);\r\nreturn ret;\r\n}\r\nspin_lock_irqsave(&c->lock, flags);\r\np = c->phy;\r\nret = c->status;\r\nif (p) {\r\ndma_addr_t addr = sa11x0_dma_pos(p);\r\ndev_vdbg(d->slave.dev, "tx_status: addr:%x\n", addr);\r\ntxd = p->txd_done;\r\nif (txd) {\r\nunsigned i;\r\nfor (i = 0; i < txd->sglen; i++) {\r\ndev_vdbg(d->slave.dev, "tx_status: [%u] %x+%x\n",\r\ni, txd->sg[i].addr, txd->sg[i].len);\r\nif (addr >= txd->sg[i].addr &&\r\naddr < txd->sg[i].addr + txd->sg[i].len) {\r\nunsigned len;\r\nlen = txd->sg[i].len -\r\n(addr - txd->sg[i].addr);\r\ndev_vdbg(d->slave.dev, "tx_status: [%u] +%x\n",\r\ni, len);\r\nbytes += len;\r\ni++;\r\nbreak;\r\n}\r\n}\r\nfor (; i < txd->sglen; i++) {\r\ndev_vdbg(d->slave.dev, "tx_status: [%u] %x+%x ++\n",\r\ni, txd->sg[i].addr, txd->sg[i].len);\r\nbytes += txd->sg[i].len;\r\n}\r\n}\r\nif (txd != p->txd_load && p->txd_load)\r\nbytes += p->txd_load->size;\r\n}\r\nlist_for_each_entry(txd, &c->desc_issued, node) {\r\nbytes += txd->size;\r\n}\r\nspin_unlock_irqrestore(&c->lock, flags);\r\ndma_set_tx_state(state, last_complete, last_used, bytes);\r\ndev_vdbg(d->slave.dev, "tx_status: bytes 0x%zx\n", bytes);\r\nreturn ret;\r\n}\r\nstatic void sa11x0_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct sa11x0_dma_chan *c = to_sa11x0_dma_chan(chan);\r\nstruct sa11x0_dma_dev *d = to_sa11x0_dma(chan->device);\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->lock, flags);\r\nlist_splice_tail_init(&c->desc_submitted, &c->desc_issued);\r\nif (!list_empty(&c->desc_issued)) {\r\nspin_lock(&d->lock);\r\nif (!c->phy && list_empty(&c->node)) {\r\nlist_add_tail(&c->node, &d->chan_pending);\r\ntasklet_schedule(&d->task);\r\ndev_dbg(d->slave.dev, "vchan %p: issued\n", c);\r\n}\r\nspin_unlock(&d->lock);\r\n} else\r\ndev_dbg(d->slave.dev, "vchan %p: nothing to issue\n", c);\r\nspin_unlock_irqrestore(&c->lock, flags);\r\n}\r\nstatic dma_cookie_t sa11x0_dma_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct sa11x0_dma_chan *c = to_sa11x0_dma_chan(tx->chan);\r\nstruct sa11x0_dma_desc *txd = to_sa11x0_dma_tx(tx);\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->lock, flags);\r\nc->chan.cookie += 1;\r\nif (c->chan.cookie < 0)\r\nc->chan.cookie = 1;\r\ntxd->tx.cookie = c->chan.cookie;\r\nlist_add_tail(&txd->node, &c->desc_submitted);\r\nspin_unlock_irqrestore(&c->lock, flags);\r\ndev_dbg(tx->chan->device->dev, "vchan %p: txd %p[%x]: submitted\n",\r\nc, txd, txd->tx.cookie);\r\nreturn txd->tx.cookie;\r\n}\r\nstatic struct dma_async_tx_descriptor *sa11x0_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sg, unsigned int sglen,\r\nenum dma_transfer_direction dir, unsigned long flags, void *context)\r\n{\r\nstruct sa11x0_dma_chan *c = to_sa11x0_dma_chan(chan);\r\nstruct sa11x0_dma_desc *txd;\r\nstruct scatterlist *sgent;\r\nunsigned i, j = sglen;\r\nsize_t size = 0;\r\nif (dir != (c->ddar & DDAR_RW ? DMA_DEV_TO_MEM : DMA_MEM_TO_DEV)) {\r\ndev_err(chan->device->dev, "vchan %p: bad DMA direction: DDAR:%08x dir:%u\n",\r\nc, c->ddar, dir);\r\nreturn NULL;\r\n}\r\nif (sglen == 0)\r\nreturn NULL;\r\nfor_each_sg(sg, sgent, sglen, i) {\r\ndma_addr_t addr = sg_dma_address(sgent);\r\nunsigned int len = sg_dma_len(sgent);\r\nif (len > DMA_MAX_SIZE)\r\nj += DIV_ROUND_UP(len, DMA_MAX_SIZE & ~DMA_ALIGN) - 1;\r\nif (addr & DMA_ALIGN) {\r\ndev_dbg(chan->device->dev, "vchan %p: bad buffer alignment: %08x\n",\r\nc, addr);\r\nreturn NULL;\r\n}\r\n}\r\ntxd = kzalloc(sizeof(*txd) + j * sizeof(txd->sg[0]), GFP_ATOMIC);\r\nif (!txd) {\r\ndev_dbg(chan->device->dev, "vchan %p: kzalloc failed\n", c);\r\nreturn NULL;\r\n}\r\nj = 0;\r\nfor_each_sg(sg, sgent, sglen, i) {\r\ndma_addr_t addr = sg_dma_address(sgent);\r\nunsigned len = sg_dma_len(sgent);\r\nsize += len;\r\ndo {\r\nunsigned tlen = len;\r\nif (tlen > DMA_MAX_SIZE) {\r\nunsigned mult = DIV_ROUND_UP(tlen,\r\nDMA_MAX_SIZE & ~DMA_ALIGN);\r\ntlen = (tlen / mult) & ~DMA_ALIGN;\r\n}\r\ntxd->sg[j].addr = addr;\r\ntxd->sg[j].len = tlen;\r\naddr += tlen;\r\nlen -= tlen;\r\nj++;\r\n} while (len);\r\n}\r\ndma_async_tx_descriptor_init(&txd->tx, &c->chan);\r\ntxd->tx.flags = flags;\r\ntxd->tx.tx_submit = sa11x0_dma_tx_submit;\r\ntxd->ddar = c->ddar;\r\ntxd->size = size;\r\ntxd->sglen = j;\r\ndev_dbg(chan->device->dev, "vchan %p: txd %p: size %u nr %u\n",\r\nc, txd, txd->size, txd->sglen);\r\nreturn &txd->tx;\r\n}\r\nstatic int sa11x0_dma_slave_config(struct sa11x0_dma_chan *c, struct dma_slave_config *cfg)\r\n{\r\nu32 ddar = c->ddar & ((0xf << 4) | DDAR_RW);\r\ndma_addr_t addr;\r\nenum dma_slave_buswidth width;\r\nu32 maxburst;\r\nif (ddar & DDAR_RW) {\r\naddr = cfg->src_addr;\r\nwidth = cfg->src_addr_width;\r\nmaxburst = cfg->src_maxburst;\r\n} else {\r\naddr = cfg->dst_addr;\r\nwidth = cfg->dst_addr_width;\r\nmaxburst = cfg->dst_maxburst;\r\n}\r\nif ((width != DMA_SLAVE_BUSWIDTH_1_BYTE &&\r\nwidth != DMA_SLAVE_BUSWIDTH_2_BYTES) ||\r\n(maxburst != 4 && maxburst != 8))\r\nreturn -EINVAL;\r\nif (width == DMA_SLAVE_BUSWIDTH_2_BYTES)\r\nddar |= DDAR_DW;\r\nif (maxburst == 8)\r\nddar |= DDAR_BS;\r\ndev_dbg(c->chan.device->dev, "vchan %p: dma_slave_config addr %x width %u burst %u\n",\r\nc, addr, width, maxburst);\r\nc->ddar = ddar | (addr & 0xf0000000) | (addr & 0x003ffffc) << 6;\r\nreturn 0;\r\n}\r\nstatic int sa11x0_dma_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct sa11x0_dma_chan *c = to_sa11x0_dma_chan(chan);\r\nstruct sa11x0_dma_dev *d = to_sa11x0_dma(chan->device);\r\nstruct sa11x0_dma_phy *p;\r\nLIST_HEAD(head);\r\nunsigned long flags;\r\nint ret;\r\nswitch (cmd) {\r\ncase DMA_SLAVE_CONFIG:\r\nreturn sa11x0_dma_slave_config(c, (struct dma_slave_config *)arg);\r\ncase DMA_TERMINATE_ALL:\r\ndev_dbg(d->slave.dev, "vchan %p: terminate all\n", c);\r\nspin_lock_irqsave(&c->lock, flags);\r\nlist_splice_tail_init(&c->desc_submitted, &head);\r\nlist_splice_tail_init(&c->desc_issued, &head);\r\np = c->phy;\r\nif (p) {\r\nstruct sa11x0_dma_desc *txd, *txn;\r\ndev_dbg(d->slave.dev, "pchan %u: terminating\n", p->num);\r\nwritel(DCSR_RUN | DCSR_IE |\r\nDCSR_STRTA | DCSR_DONEA |\r\nDCSR_STRTB | DCSR_DONEB,\r\np->base + DMA_DCSR_C);\r\nlist_for_each_entry_safe(txd, txn, &d->desc_complete, node)\r\nif (txd->tx.chan == &c->chan)\r\nlist_move(&txd->node, &head);\r\nif (p->txd_load) {\r\nif (p->txd_load != p->txd_done)\r\nlist_add_tail(&p->txd_load->node, &head);\r\np->txd_load = NULL;\r\n}\r\nif (p->txd_done) {\r\nlist_add_tail(&p->txd_done->node, &head);\r\np->txd_done = NULL;\r\n}\r\nc->phy = NULL;\r\nspin_lock(&d->lock);\r\np->vchan = NULL;\r\nspin_unlock(&d->lock);\r\ntasklet_schedule(&d->task);\r\n}\r\nspin_unlock_irqrestore(&c->lock, flags);\r\nsa11x0_dma_desc_free(d, &head);\r\nret = 0;\r\nbreak;\r\ncase DMA_PAUSE:\r\ndev_dbg(d->slave.dev, "vchan %p: pause\n", c);\r\nspin_lock_irqsave(&c->lock, flags);\r\nif (c->status == DMA_IN_PROGRESS) {\r\nc->status = DMA_PAUSED;\r\np = c->phy;\r\nif (p) {\r\nwritel(DCSR_RUN | DCSR_IE, p->base + DMA_DCSR_C);\r\n} else {\r\nspin_lock(&d->lock);\r\nlist_del_init(&c->node);\r\nspin_unlock(&d->lock);\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->lock, flags);\r\nret = 0;\r\nbreak;\r\ncase DMA_RESUME:\r\ndev_dbg(d->slave.dev, "vchan %p: resume\n", c);\r\nspin_lock_irqsave(&c->lock, flags);\r\nif (c->status == DMA_PAUSED) {\r\nc->status = DMA_IN_PROGRESS;\r\np = c->phy;\r\nif (p) {\r\nwritel(DCSR_RUN | DCSR_IE, p->base + DMA_DCSR_S);\r\n} else if (!list_empty(&c->desc_issued)) {\r\nspin_lock(&d->lock);\r\nlist_add_tail(&c->node, &d->chan_pending);\r\nspin_unlock(&d->lock);\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->lock, flags);\r\nret = 0;\r\nbreak;\r\ndefault:\r\nret = -ENXIO;\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int __devinit sa11x0_dma_init_dmadev(struct dma_device *dmadev,\r\nstruct device *dev)\r\n{\r\nunsigned i;\r\ndmadev->chancnt = ARRAY_SIZE(chan_desc);\r\nINIT_LIST_HEAD(&dmadev->channels);\r\ndmadev->dev = dev;\r\ndmadev->device_alloc_chan_resources = sa11x0_dma_alloc_chan_resources;\r\ndmadev->device_free_chan_resources = sa11x0_dma_free_chan_resources;\r\ndmadev->device_control = sa11x0_dma_control;\r\ndmadev->device_tx_status = sa11x0_dma_tx_status;\r\ndmadev->device_issue_pending = sa11x0_dma_issue_pending;\r\nfor (i = 0; i < dmadev->chancnt; i++) {\r\nstruct sa11x0_dma_chan *c;\r\nc = kzalloc(sizeof(*c), GFP_KERNEL);\r\nif (!c) {\r\ndev_err(dev, "no memory for channel %u\n", i);\r\nreturn -ENOMEM;\r\n}\r\nc->chan.device = dmadev;\r\nc->status = DMA_IN_PROGRESS;\r\nc->ddar = chan_desc[i].ddar;\r\nc->name = chan_desc[i].name;\r\nspin_lock_init(&c->lock);\r\nINIT_LIST_HEAD(&c->desc_submitted);\r\nINIT_LIST_HEAD(&c->desc_issued);\r\nINIT_LIST_HEAD(&c->node);\r\nlist_add_tail(&c->chan.device_node, &dmadev->channels);\r\n}\r\nreturn dma_async_device_register(dmadev);\r\n}\r\nstatic int sa11x0_dma_request_irq(struct platform_device *pdev, int nr,\r\nvoid *data)\r\n{\r\nint irq = platform_get_irq(pdev, nr);\r\nif (irq <= 0)\r\nreturn -ENXIO;\r\nreturn request_irq(irq, sa11x0_dma_irq, 0, dev_name(&pdev->dev), data);\r\n}\r\nstatic void sa11x0_dma_free_irq(struct platform_device *pdev, int nr,\r\nvoid *data)\r\n{\r\nint irq = platform_get_irq(pdev, nr);\r\nif (irq > 0)\r\nfree_irq(irq, data);\r\n}\r\nstatic void sa11x0_dma_free_channels(struct dma_device *dmadev)\r\n{\r\nstruct sa11x0_dma_chan *c, *cn;\r\nlist_for_each_entry_safe(c, cn, &dmadev->channels, chan.device_node) {\r\nlist_del(&c->chan.device_node);\r\nkfree(c);\r\n}\r\n}\r\nstatic int __devinit sa11x0_dma_probe(struct platform_device *pdev)\r\n{\r\nstruct sa11x0_dma_dev *d;\r\nstruct resource *res;\r\nunsigned i;\r\nint ret;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res)\r\nreturn -ENXIO;\r\nd = kzalloc(sizeof(*d), GFP_KERNEL);\r\nif (!d) {\r\nret = -ENOMEM;\r\ngoto err_alloc;\r\n}\r\nspin_lock_init(&d->lock);\r\nINIT_LIST_HEAD(&d->chan_pending);\r\nINIT_LIST_HEAD(&d->desc_complete);\r\nd->base = ioremap(res->start, resource_size(res));\r\nif (!d->base) {\r\nret = -ENOMEM;\r\ngoto err_ioremap;\r\n}\r\ntasklet_init(&d->task, sa11x0_dma_tasklet, (unsigned long)d);\r\nfor (i = 0; i < NR_PHY_CHAN; i++) {\r\nstruct sa11x0_dma_phy *p = &d->phy[i];\r\np->dev = d;\r\np->num = i;\r\np->base = d->base + i * DMA_SIZE;\r\nwritel_relaxed(DCSR_RUN | DCSR_IE | DCSR_ERROR |\r\nDCSR_DONEA | DCSR_STRTA | DCSR_DONEB | DCSR_STRTB,\r\np->base + DMA_DCSR_C);\r\nwritel_relaxed(0, p->base + DMA_DDAR);\r\nret = sa11x0_dma_request_irq(pdev, i, p);\r\nif (ret) {\r\nwhile (i) {\r\ni--;\r\nsa11x0_dma_free_irq(pdev, i, &d->phy[i]);\r\n}\r\ngoto err_irq;\r\n}\r\n}\r\ndma_cap_set(DMA_SLAVE, d->slave.cap_mask);\r\nd->slave.device_prep_slave_sg = sa11x0_dma_prep_slave_sg;\r\nret = sa11x0_dma_init_dmadev(&d->slave, &pdev->dev);\r\nif (ret) {\r\ndev_warn(d->slave.dev, "failed to register slave async device: %d\n",\r\nret);\r\ngoto err_slave_reg;\r\n}\r\nplatform_set_drvdata(pdev, d);\r\nreturn 0;\r\nerr_slave_reg:\r\nsa11x0_dma_free_channels(&d->slave);\r\nfor (i = 0; i < NR_PHY_CHAN; i++)\r\nsa11x0_dma_free_irq(pdev, i, &d->phy[i]);\r\nerr_irq:\r\ntasklet_kill(&d->task);\r\niounmap(d->base);\r\nerr_ioremap:\r\nkfree(d);\r\nerr_alloc:\r\nreturn ret;\r\n}\r\nstatic int __devexit sa11x0_dma_remove(struct platform_device *pdev)\r\n{\r\nstruct sa11x0_dma_dev *d = platform_get_drvdata(pdev);\r\nunsigned pch;\r\ndma_async_device_unregister(&d->slave);\r\nsa11x0_dma_free_channels(&d->slave);\r\nfor (pch = 0; pch < NR_PHY_CHAN; pch++)\r\nsa11x0_dma_free_irq(pdev, pch, &d->phy[pch]);\r\ntasklet_kill(&d->task);\r\niounmap(d->base);\r\nkfree(d);\r\nreturn 0;\r\n}\r\nstatic int sa11x0_dma_suspend(struct device *dev)\r\n{\r\nstruct sa11x0_dma_dev *d = dev_get_drvdata(dev);\r\nunsigned pch;\r\nfor (pch = 0; pch < NR_PHY_CHAN; pch++) {\r\nstruct sa11x0_dma_phy *p = &d->phy[pch];\r\nu32 dcsr, saved_dcsr;\r\ndcsr = saved_dcsr = readl_relaxed(p->base + DMA_DCSR_R);\r\nif (dcsr & DCSR_RUN) {\r\nwritel(DCSR_RUN | DCSR_IE, p->base + DMA_DCSR_C);\r\ndcsr = readl_relaxed(p->base + DMA_DCSR_R);\r\n}\r\nsaved_dcsr &= DCSR_RUN | DCSR_IE;\r\nif (dcsr & DCSR_BIU) {\r\np->dbs[0] = readl_relaxed(p->base + DMA_DBSB);\r\np->dbt[0] = readl_relaxed(p->base + DMA_DBTB);\r\np->dbs[1] = readl_relaxed(p->base + DMA_DBSA);\r\np->dbt[1] = readl_relaxed(p->base + DMA_DBTA);\r\nsaved_dcsr |= (dcsr & DCSR_STRTA ? DCSR_STRTB : 0) |\r\n(dcsr & DCSR_STRTB ? DCSR_STRTA : 0);\r\n} else {\r\np->dbs[0] = readl_relaxed(p->base + DMA_DBSA);\r\np->dbt[0] = readl_relaxed(p->base + DMA_DBTA);\r\np->dbs[1] = readl_relaxed(p->base + DMA_DBSB);\r\np->dbt[1] = readl_relaxed(p->base + DMA_DBTB);\r\nsaved_dcsr |= dcsr & (DCSR_STRTA | DCSR_STRTB);\r\n}\r\np->dcsr = saved_dcsr;\r\nwritel(DCSR_STRTA | DCSR_STRTB, p->base + DMA_DCSR_C);\r\n}\r\nreturn 0;\r\n}\r\nstatic int sa11x0_dma_resume(struct device *dev)\r\n{\r\nstruct sa11x0_dma_dev *d = dev_get_drvdata(dev);\r\nunsigned pch;\r\nfor (pch = 0; pch < NR_PHY_CHAN; pch++) {\r\nstruct sa11x0_dma_phy *p = &d->phy[pch];\r\nstruct sa11x0_dma_desc *txd = NULL;\r\nu32 dcsr = readl_relaxed(p->base + DMA_DCSR_R);\r\nWARN_ON(dcsr & (DCSR_BIU | DCSR_STRTA | DCSR_STRTB | DCSR_RUN));\r\nif (p->txd_done)\r\ntxd = p->txd_done;\r\nelse if (p->txd_load)\r\ntxd = p->txd_load;\r\nif (!txd)\r\ncontinue;\r\nwritel_relaxed(txd->ddar, p->base + DMA_DDAR);\r\nwritel_relaxed(p->dbs[0], p->base + DMA_DBSA);\r\nwritel_relaxed(p->dbt[0], p->base + DMA_DBTA);\r\nwritel_relaxed(p->dbs[1], p->base + DMA_DBSB);\r\nwritel_relaxed(p->dbt[1], p->base + DMA_DBTB);\r\nwritel_relaxed(p->dcsr, p->base + DMA_DCSR_S);\r\n}\r\nreturn 0;\r\n}\r\nbool sa11x0_dma_filter_fn(struct dma_chan *chan, void *param)\r\n{\r\nif (chan->device->dev->driver == &sa11x0_dma_driver.driver) {\r\nstruct sa11x0_dma_chan *c = to_sa11x0_dma_chan(chan);\r\nconst char *p = param;\r\nreturn !strcmp(c->name, p);\r\n}\r\nreturn false;\r\n}\r\nstatic int __init sa11x0_dma_init(void)\r\n{\r\nreturn platform_driver_register(&sa11x0_dma_driver);\r\n}\r\nstatic void __exit sa11x0_dma_exit(void)\r\n{\r\nplatform_driver_unregister(&sa11x0_dma_driver);\r\n}
