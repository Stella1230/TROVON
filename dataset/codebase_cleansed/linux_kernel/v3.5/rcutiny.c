static void rcu_idle_enter_common(long long oldval)\r\n{\r\nif (rcu_dynticks_nesting) {\r\nRCU_TRACE(trace_rcu_dyntick("--=",\r\noldval, rcu_dynticks_nesting));\r\nreturn;\r\n}\r\nRCU_TRACE(trace_rcu_dyntick("Start", oldval, rcu_dynticks_nesting));\r\nif (!is_idle_task(current)) {\r\nstruct task_struct *idle = idle_task(smp_processor_id());\r\nRCU_TRACE(trace_rcu_dyntick("Error on entry: not idle task",\r\noldval, rcu_dynticks_nesting));\r\nftrace_dump(DUMP_ALL);\r\nWARN_ONCE(1, "Current pid: %d comm: %s / Idle pid: %d comm: %s",\r\ncurrent->pid, current->comm,\r\nidle->pid, idle->comm);\r\n}\r\nrcu_sched_qs(0);\r\n}\r\nvoid rcu_idle_enter(void)\r\n{\r\nunsigned long flags;\r\nlong long oldval;\r\nlocal_irq_save(flags);\r\noldval = rcu_dynticks_nesting;\r\nWARN_ON_ONCE((rcu_dynticks_nesting & DYNTICK_TASK_NEST_MASK) == 0);\r\nif ((rcu_dynticks_nesting & DYNTICK_TASK_NEST_MASK) ==\r\nDYNTICK_TASK_NEST_VALUE)\r\nrcu_dynticks_nesting = 0;\r\nelse\r\nrcu_dynticks_nesting -= DYNTICK_TASK_NEST_VALUE;\r\nrcu_idle_enter_common(oldval);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid rcu_irq_exit(void)\r\n{\r\nunsigned long flags;\r\nlong long oldval;\r\nlocal_irq_save(flags);\r\noldval = rcu_dynticks_nesting;\r\nrcu_dynticks_nesting--;\r\nWARN_ON_ONCE(rcu_dynticks_nesting < 0);\r\nrcu_idle_enter_common(oldval);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void rcu_idle_exit_common(long long oldval)\r\n{\r\nif (oldval) {\r\nRCU_TRACE(trace_rcu_dyntick("++=",\r\noldval, rcu_dynticks_nesting));\r\nreturn;\r\n}\r\nRCU_TRACE(trace_rcu_dyntick("End", oldval, rcu_dynticks_nesting));\r\nif (!is_idle_task(current)) {\r\nstruct task_struct *idle = idle_task(smp_processor_id());\r\nRCU_TRACE(trace_rcu_dyntick("Error on exit: not idle task",\r\noldval, rcu_dynticks_nesting));\r\nftrace_dump(DUMP_ALL);\r\nWARN_ONCE(1, "Current pid: %d comm: %s / Idle pid: %d comm: %s",\r\ncurrent->pid, current->comm,\r\nidle->pid, idle->comm);\r\n}\r\n}\r\nvoid rcu_idle_exit(void)\r\n{\r\nunsigned long flags;\r\nlong long oldval;\r\nlocal_irq_save(flags);\r\noldval = rcu_dynticks_nesting;\r\nWARN_ON_ONCE(rcu_dynticks_nesting < 0);\r\nif (rcu_dynticks_nesting & DYNTICK_TASK_NEST_MASK)\r\nrcu_dynticks_nesting += DYNTICK_TASK_NEST_VALUE;\r\nelse\r\nrcu_dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;\r\nrcu_idle_exit_common(oldval);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid rcu_irq_enter(void)\r\n{\r\nunsigned long flags;\r\nlong long oldval;\r\nlocal_irq_save(flags);\r\noldval = rcu_dynticks_nesting;\r\nrcu_dynticks_nesting++;\r\nWARN_ON_ONCE(rcu_dynticks_nesting == 0);\r\nrcu_idle_exit_common(oldval);\r\nlocal_irq_restore(flags);\r\n}\r\nint rcu_is_cpu_idle(void)\r\n{\r\nreturn !rcu_dynticks_nesting;\r\n}\r\nint rcu_is_cpu_rrupt_from_idle(void)\r\n{\r\nreturn rcu_dynticks_nesting <= 0;\r\n}\r\nstatic int rcu_qsctr_help(struct rcu_ctrlblk *rcp)\r\n{\r\nif (rcp->rcucblist != NULL &&\r\nrcp->donetail != rcp->curtail) {\r\nrcp->donetail = rcp->curtail;\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid rcu_sched_qs(int cpu)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nif (rcu_qsctr_help(&rcu_sched_ctrlblk) +\r\nrcu_qsctr_help(&rcu_bh_ctrlblk))\r\ninvoke_rcu_callbacks();\r\nlocal_irq_restore(flags);\r\n}\r\nvoid rcu_bh_qs(int cpu)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nif (rcu_qsctr_help(&rcu_bh_ctrlblk))\r\ninvoke_rcu_callbacks();\r\nlocal_irq_restore(flags);\r\n}\r\nvoid rcu_check_callbacks(int cpu, int user)\r\n{\r\nif (user || rcu_is_cpu_rrupt_from_idle())\r\nrcu_sched_qs(cpu);\r\nelse if (!in_softirq())\r\nrcu_bh_qs(cpu);\r\nrcu_preempt_check_callbacks();\r\n}\r\nstatic void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)\r\n{\r\nchar *rn = NULL;\r\nstruct rcu_head *next, *list;\r\nunsigned long flags;\r\nRCU_TRACE(int cb_count = 0);\r\nif (&rcp->rcucblist == rcp->donetail) {\r\nRCU_TRACE(trace_rcu_batch_start(rcp->name, 0, 0, -1));\r\nRCU_TRACE(trace_rcu_batch_end(rcp->name, 0,\r\nACCESS_ONCE(rcp->rcucblist),\r\nneed_resched(),\r\nis_idle_task(current),\r\nrcu_is_callbacks_kthread()));\r\nreturn;\r\n}\r\nlocal_irq_save(flags);\r\nRCU_TRACE(trace_rcu_batch_start(rcp->name, 0, rcp->qlen, -1));\r\nlist = rcp->rcucblist;\r\nrcp->rcucblist = *rcp->donetail;\r\n*rcp->donetail = NULL;\r\nif (rcp->curtail == rcp->donetail)\r\nrcp->curtail = &rcp->rcucblist;\r\nrcu_preempt_remove_callbacks(rcp);\r\nrcp->donetail = &rcp->rcucblist;\r\nlocal_irq_restore(flags);\r\nRCU_TRACE(rn = rcp->name);\r\nwhile (list) {\r\nnext = list->next;\r\nprefetch(next);\r\ndebug_rcu_head_unqueue(list);\r\nlocal_bh_disable();\r\n__rcu_reclaim(rn, list);\r\nlocal_bh_enable();\r\nlist = next;\r\nRCU_TRACE(cb_count++);\r\n}\r\nRCU_TRACE(rcu_trace_sub_qlen(rcp, cb_count));\r\nRCU_TRACE(trace_rcu_batch_end(rcp->name, cb_count, 0, need_resched(),\r\nis_idle_task(current),\r\nrcu_is_callbacks_kthread()));\r\n}\r\nstatic void rcu_process_callbacks(struct softirq_action *unused)\r\n{\r\n__rcu_process_callbacks(&rcu_sched_ctrlblk);\r\n__rcu_process_callbacks(&rcu_bh_ctrlblk);\r\nrcu_preempt_process_callbacks();\r\n}\r\nvoid synchronize_sched(void)\r\n{\r\nrcu_lockdep_assert(!lock_is_held(&rcu_bh_lock_map) &&\r\n!lock_is_held(&rcu_lock_map) &&\r\n!lock_is_held(&rcu_sched_lock_map),\r\n"Illegal synchronize_sched() in RCU read-side critical section");\r\ncond_resched();\r\n}\r\nstatic void __call_rcu(struct rcu_head *head,\r\nvoid (*func)(struct rcu_head *rcu),\r\nstruct rcu_ctrlblk *rcp)\r\n{\r\nunsigned long flags;\r\ndebug_rcu_head_queue(head);\r\nhead->func = func;\r\nhead->next = NULL;\r\nlocal_irq_save(flags);\r\n*rcp->curtail = head;\r\nrcp->curtail = &head->next;\r\nRCU_TRACE(rcp->qlen++);\r\nlocal_irq_restore(flags);\r\n}\r\nvoid call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))\r\n{\r\n__call_rcu(head, func, &rcu_sched_ctrlblk);\r\n}\r\nvoid call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))\r\n{\r\n__call_rcu(head, func, &rcu_bh_ctrlblk);\r\n}
