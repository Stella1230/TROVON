static void chclr_write(struct sh_dmae_chan *sh_dc, u32 data)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_dc);\r\n__raw_writel(data, shdev->chan_reg +\r\nshdev->pdata->channel[sh_dc->id].chclr_offset);\r\n}\r\nstatic void sh_dmae_writel(struct sh_dmae_chan *sh_dc, u32 data, u32 reg)\r\n{\r\n__raw_writel(data, sh_dc->base + reg / sizeof(u32));\r\n}\r\nstatic u32 sh_dmae_readl(struct sh_dmae_chan *sh_dc, u32 reg)\r\n{\r\nreturn __raw_readl(sh_dc->base + reg / sizeof(u32));\r\n}\r\nstatic u16 dmaor_read(struct sh_dmae_device *shdev)\r\n{\r\nu32 __iomem *addr = shdev->chan_reg + DMAOR / sizeof(u32);\r\nif (shdev->pdata->dmaor_is_32bit)\r\nreturn __raw_readl(addr);\r\nelse\r\nreturn __raw_readw(addr);\r\n}\r\nstatic void dmaor_write(struct sh_dmae_device *shdev, u16 data)\r\n{\r\nu32 __iomem *addr = shdev->chan_reg + DMAOR / sizeof(u32);\r\nif (shdev->pdata->dmaor_is_32bit)\r\n__raw_writel(data, addr);\r\nelse\r\n__raw_writew(data, addr);\r\n}\r\nstatic void chcr_write(struct sh_dmae_chan *sh_dc, u32 data)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_dc);\r\n__raw_writel(data, sh_dc->base + shdev->chcr_offset / sizeof(u32));\r\n}\r\nstatic u32 chcr_read(struct sh_dmae_chan *sh_dc)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_dc);\r\nreturn __raw_readl(sh_dc->base + shdev->chcr_offset / sizeof(u32));\r\n}\r\nstatic void sh_dmae_ctl_stop(struct sh_dmae_device *shdev)\r\n{\r\nunsigned short dmaor;\r\nunsigned long flags;\r\nspin_lock_irqsave(&sh_dmae_lock, flags);\r\ndmaor = dmaor_read(shdev);\r\ndmaor_write(shdev, dmaor & ~(DMAOR_NMIF | DMAOR_AE | DMAOR_DME));\r\nspin_unlock_irqrestore(&sh_dmae_lock, flags);\r\n}\r\nstatic int sh_dmae_rst(struct sh_dmae_device *shdev)\r\n{\r\nunsigned short dmaor;\r\nunsigned long flags;\r\nspin_lock_irqsave(&sh_dmae_lock, flags);\r\ndmaor = dmaor_read(shdev) & ~(DMAOR_NMIF | DMAOR_AE | DMAOR_DME);\r\nif (shdev->pdata->chclr_present) {\r\nint i;\r\nfor (i = 0; i < shdev->pdata->channel_num; i++) {\r\nstruct sh_dmae_chan *sh_chan = shdev->chan[i];\r\nif (sh_chan)\r\nchclr_write(sh_chan, 0);\r\n}\r\n}\r\ndmaor_write(shdev, dmaor | shdev->pdata->dmaor_init);\r\ndmaor = dmaor_read(shdev);\r\nspin_unlock_irqrestore(&sh_dmae_lock, flags);\r\nif (dmaor & (DMAOR_AE | DMAOR_NMIF)) {\r\ndev_warn(shdev->common.dev, "Can't initialize DMAOR.\n");\r\nreturn -EIO;\r\n}\r\nif (shdev->pdata->dmaor_init & ~dmaor)\r\ndev_warn(shdev->common.dev,\r\n"DMAOR=0x%x hasn't latched the initial value 0x%x.\n",\r\ndmaor, shdev->pdata->dmaor_init);\r\nreturn 0;\r\n}\r\nstatic bool dmae_is_busy(struct sh_dmae_chan *sh_chan)\r\n{\r\nu32 chcr = chcr_read(sh_chan);\r\nif ((chcr & (CHCR_DE | CHCR_TE)) == CHCR_DE)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic unsigned int calc_xmit_shift(struct sh_dmae_chan *sh_chan, u32 chcr)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nstruct sh_dmae_pdata *pdata = shdev->pdata;\r\nint cnt = ((chcr & pdata->ts_low_mask) >> pdata->ts_low_shift) |\r\n((chcr & pdata->ts_high_mask) >> pdata->ts_high_shift);\r\nif (cnt >= pdata->ts_shift_num)\r\ncnt = 0;\r\nreturn pdata->ts_shift[cnt];\r\n}\r\nstatic u32 log2size_to_chcr(struct sh_dmae_chan *sh_chan, int l2size)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nstruct sh_dmae_pdata *pdata = shdev->pdata;\r\nint i;\r\nfor (i = 0; i < pdata->ts_shift_num; i++)\r\nif (pdata->ts_shift[i] == l2size)\r\nbreak;\r\nif (i == pdata->ts_shift_num)\r\ni = 0;\r\nreturn ((i << pdata->ts_low_shift) & pdata->ts_low_mask) |\r\n((i << pdata->ts_high_shift) & pdata->ts_high_mask);\r\n}\r\nstatic void dmae_set_reg(struct sh_dmae_chan *sh_chan, struct sh_dmae_regs *hw)\r\n{\r\nsh_dmae_writel(sh_chan, hw->sar, SAR);\r\nsh_dmae_writel(sh_chan, hw->dar, DAR);\r\nsh_dmae_writel(sh_chan, hw->tcr >> sh_chan->xmit_shift, TCR);\r\n}\r\nstatic void dmae_start(struct sh_dmae_chan *sh_chan)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nu32 chcr = chcr_read(sh_chan);\r\nif (shdev->pdata->needs_tend_set)\r\nsh_dmae_writel(sh_chan, 0xFFFFFFFF, TEND);\r\nchcr |= CHCR_DE | shdev->chcr_ie_bit;\r\nchcr_write(sh_chan, chcr & ~CHCR_TE);\r\n}\r\nstatic void dmae_halt(struct sh_dmae_chan *sh_chan)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nu32 chcr = chcr_read(sh_chan);\r\nchcr &= ~(CHCR_DE | CHCR_TE | shdev->chcr_ie_bit);\r\nchcr_write(sh_chan, chcr);\r\n}\r\nstatic void dmae_init(struct sh_dmae_chan *sh_chan)\r\n{\r\nu32 chcr = DM_INC | SM_INC | 0x400 | log2size_to_chcr(sh_chan,\r\nLOG2_DEFAULT_XFER_SIZE);\r\nsh_chan->xmit_shift = calc_xmit_shift(sh_chan, chcr);\r\nchcr_write(sh_chan, chcr);\r\n}\r\nstatic int dmae_set_chcr(struct sh_dmae_chan *sh_chan, u32 val)\r\n{\r\nif (dmae_is_busy(sh_chan))\r\nreturn -EBUSY;\r\nsh_chan->xmit_shift = calc_xmit_shift(sh_chan, val);\r\nchcr_write(sh_chan, val);\r\nreturn 0;\r\n}\r\nstatic int dmae_set_dmars(struct sh_dmae_chan *sh_chan, u16 val)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nstruct sh_dmae_pdata *pdata = shdev->pdata;\r\nconst struct sh_dmae_channel *chan_pdata = &pdata->channel[sh_chan->id];\r\nu16 __iomem *addr = shdev->dmars;\r\nunsigned int shift = chan_pdata->dmars_bit;\r\nif (dmae_is_busy(sh_chan))\r\nreturn -EBUSY;\r\nif (pdata->no_dmars)\r\nreturn 0;\r\nif (!addr)\r\naddr = (u16 __iomem *)shdev->chan_reg;\r\naddr += chan_pdata->dmars / sizeof(u16);\r\n__raw_writew((__raw_readw(addr) & (0xff00 >> shift)) | (val << shift),\r\naddr);\r\nreturn 0;\r\n}\r\nstatic dma_cookie_t sh_dmae_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct sh_desc *desc = tx_to_sh_desc(tx), *chunk, *last = desc, *c;\r\nstruct sh_dmae_chan *sh_chan = to_sh_chan(tx->chan);\r\nstruct sh_dmae_slave *param = tx->chan->private;\r\ndma_async_tx_callback callback = tx->callback;\r\ndma_cookie_t cookie;\r\nbool power_up;\r\nspin_lock_irq(&sh_chan->desc_lock);\r\nif (list_empty(&sh_chan->ld_queue))\r\npower_up = true;\r\nelse\r\npower_up = false;\r\ncookie = dma_cookie_assign(tx);\r\nlist_for_each_entry_safe(chunk, c, desc->node.prev, node) {\r\nif (chunk != desc && (chunk->mark == DESC_IDLE ||\r\nchunk->async_tx.cookie > 0 ||\r\nchunk->async_tx.cookie == -EBUSY ||\r\n&chunk->node == &sh_chan->ld_free))\r\nbreak;\r\nchunk->mark = DESC_SUBMITTED;\r\nchunk->async_tx.callback = NULL;\r\nchunk->cookie = cookie;\r\nlist_move_tail(&chunk->node, &sh_chan->ld_queue);\r\nlast = chunk;\r\n}\r\nlast->async_tx.callback = callback;\r\nlast->async_tx.callback_param = tx->callback_param;\r\ndev_dbg(sh_chan->dev, "submit #%d@%p on %d: %x[%d] -> %x\n",\r\ntx->cookie, &last->async_tx, sh_chan->id,\r\ndesc->hw.sar, desc->hw.tcr, desc->hw.dar);\r\nif (power_up) {\r\nsh_chan->pm_state = DMAE_PM_BUSY;\r\npm_runtime_get(sh_chan->dev);\r\nspin_unlock_irq(&sh_chan->desc_lock);\r\npm_runtime_barrier(sh_chan->dev);\r\nspin_lock_irq(&sh_chan->desc_lock);\r\nif (sh_chan->pm_state != DMAE_PM_ESTABLISHED) {\r\ndev_dbg(sh_chan->dev, "Bring up channel %d\n",\r\nsh_chan->id);\r\nif (param) {\r\nconst struct sh_dmae_slave_config *cfg =\r\nparam->config;\r\ndmae_set_dmars(sh_chan, cfg->mid_rid);\r\ndmae_set_chcr(sh_chan, cfg->chcr);\r\n} else {\r\ndmae_init(sh_chan);\r\n}\r\nif (sh_chan->pm_state == DMAE_PM_PENDING)\r\nsh_chan_xfer_ld_queue(sh_chan);\r\nsh_chan->pm_state = DMAE_PM_ESTABLISHED;\r\n}\r\n} else {\r\nsh_chan->pm_state = DMAE_PM_PENDING;\r\n}\r\nspin_unlock_irq(&sh_chan->desc_lock);\r\nreturn cookie;\r\n}\r\nstatic struct sh_desc *sh_dmae_get_desc(struct sh_dmae_chan *sh_chan)\r\n{\r\nstruct sh_desc *desc;\r\nlist_for_each_entry(desc, &sh_chan->ld_free, node)\r\nif (desc->mark != DESC_PREPARED) {\r\nBUG_ON(desc->mark != DESC_IDLE);\r\nlist_del(&desc->node);\r\nreturn desc;\r\n}\r\nreturn NULL;\r\n}\r\nstatic const struct sh_dmae_slave_config *sh_dmae_find_slave(\r\nstruct sh_dmae_chan *sh_chan, struct sh_dmae_slave *param)\r\n{\r\nstruct sh_dmae_device *shdev = to_sh_dev(sh_chan);\r\nstruct sh_dmae_pdata *pdata = shdev->pdata;\r\nint i;\r\nif (param->slave_id >= SH_DMA_SLAVE_NUMBER)\r\nreturn NULL;\r\nfor (i = 0; i < pdata->slave_num; i++)\r\nif (pdata->slave[i].slave_id == param->slave_id)\r\nreturn pdata->slave + i;\r\nreturn NULL;\r\n}\r\nstatic int sh_dmae_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct sh_dmae_chan *sh_chan = to_sh_chan(chan);\r\nstruct sh_desc *desc;\r\nstruct sh_dmae_slave *param = chan->private;\r\nint ret;\r\nif (param) {\r\nconst struct sh_dmae_slave_config *cfg;\r\ncfg = sh_dmae_find_slave(sh_chan, param);\r\nif (!cfg) {\r\nret = -EINVAL;\r\ngoto efindslave;\r\n}\r\nif (test_and_set_bit(param->slave_id, sh_dmae_slave_used)) {\r\nret = -EBUSY;\r\ngoto etestused;\r\n}\r\nparam->config = cfg;\r\n}\r\nwhile (sh_chan->descs_allocated < NR_DESCS_PER_CHANNEL) {\r\ndesc = kzalloc(sizeof(struct sh_desc), GFP_KERNEL);\r\nif (!desc)\r\nbreak;\r\ndma_async_tx_descriptor_init(&desc->async_tx,\r\n&sh_chan->common);\r\ndesc->async_tx.tx_submit = sh_dmae_tx_submit;\r\ndesc->mark = DESC_IDLE;\r\nlist_add(&desc->node, &sh_chan->ld_free);\r\nsh_chan->descs_allocated++;\r\n}\r\nif (!sh_chan->descs_allocated) {\r\nret = -ENOMEM;\r\ngoto edescalloc;\r\n}\r\nreturn sh_chan->descs_allocated;\r\nedescalloc:\r\nif (param)\r\nclear_bit(param->slave_id, sh_dmae_slave_used);\r\netestused:\r\nefindslave:\r\nchan->private = NULL;\r\nreturn ret;\r\n}\r\nstatic void sh_dmae_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct sh_dmae_chan *sh_chan = to_sh_chan(chan);\r\nstruct sh_desc *desc, *_desc;\r\nLIST_HEAD(list);\r\nspin_lock_irq(&sh_chan->desc_lock);\r\ndmae_halt(sh_chan);\r\nspin_unlock_irq(&sh_chan->desc_lock);\r\nif (!list_empty(&sh_chan->ld_queue))\r\nsh_dmae_chan_ld_cleanup(sh_chan, true);\r\nif (chan->private) {\r\nstruct sh_dmae_slave *param = chan->private;\r\nclear_bit(param->slave_id, sh_dmae_slave_used);\r\nchan->private = NULL;\r\n}\r\nspin_lock_irq(&sh_chan->desc_lock);\r\nlist_splice_init(&sh_chan->ld_free, &list);\r\nsh_chan->descs_allocated = 0;\r\nspin_unlock_irq(&sh_chan->desc_lock);\r\nlist_for_each_entry_safe(desc, _desc, &list, node)\r\nkfree(desc);\r\n}\r\nstatic struct sh_desc *sh_dmae_add_desc(struct sh_dmae_chan *sh_chan,\r\nunsigned long flags, dma_addr_t *dest, dma_addr_t *src, size_t *len,\r\nstruct sh_desc **first, enum dma_transfer_direction direction)\r\n{\r\nstruct sh_desc *new;\r\nsize_t copy_size;\r\nif (!*len)\r\nreturn NULL;\r\nnew = sh_dmae_get_desc(sh_chan);\r\nif (!new) {\r\ndev_err(sh_chan->dev, "No free link descriptor available\n");\r\nreturn NULL;\r\n}\r\ncopy_size = min(*len, (size_t)SH_DMA_TCR_MAX + 1);\r\nnew->hw.sar = *src;\r\nnew->hw.dar = *dest;\r\nnew->hw.tcr = copy_size;\r\nif (!*first) {\r\nnew->async_tx.cookie = -EBUSY;\r\n*first = new;\r\n} else {\r\nnew->async_tx.cookie = -EINVAL;\r\n}\r\ndev_dbg(sh_chan->dev,\r\n"chaining (%u/%u)@%x -> %x with %p, cookie %d, shift %d\n",\r\ncopy_size, *len, *src, *dest, &new->async_tx,\r\nnew->async_tx.cookie, sh_chan->xmit_shift);\r\nnew->mark = DESC_PREPARED;\r\nnew->async_tx.flags = flags;\r\nnew->direction = direction;\r\n*len -= copy_size;\r\nif (direction == DMA_MEM_TO_MEM || direction == DMA_MEM_TO_DEV)\r\n*src += copy_size;\r\nif (direction == DMA_MEM_TO_MEM || direction == DMA_DEV_TO_MEM)\r\n*dest += copy_size;\r\nreturn new;\r\n}\r\nstatic struct dma_async_tx_descriptor *sh_dmae_prep_sg(struct sh_dmae_chan *sh_chan,\r\nstruct scatterlist *sgl, unsigned int sg_len, dma_addr_t *addr,\r\nenum dma_transfer_direction direction, unsigned long flags)\r\n{\r\nstruct scatterlist *sg;\r\nstruct sh_desc *first = NULL, *new = NULL ;\r\nLIST_HEAD(tx_list);\r\nint chunks = 0;\r\nunsigned long irq_flags;\r\nint i;\r\nif (!sg_len)\r\nreturn NULL;\r\nfor_each_sg(sgl, sg, sg_len, i)\r\nchunks += (sg_dma_len(sg) + SH_DMA_TCR_MAX) /\r\n(SH_DMA_TCR_MAX + 1);\r\nspin_lock_irqsave(&sh_chan->desc_lock, irq_flags);\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\ndma_addr_t sg_addr = sg_dma_address(sg);\r\nsize_t len = sg_dma_len(sg);\r\nif (!len)\r\ngoto err_get_desc;\r\ndo {\r\ndev_dbg(sh_chan->dev, "Add SG #%d@%p[%d], dma %llx\n",\r\ni, sg, len, (unsigned long long)sg_addr);\r\nif (direction == DMA_DEV_TO_MEM)\r\nnew = sh_dmae_add_desc(sh_chan, flags,\r\n&sg_addr, addr, &len, &first,\r\ndirection);\r\nelse\r\nnew = sh_dmae_add_desc(sh_chan, flags,\r\naddr, &sg_addr, &len, &first,\r\ndirection);\r\nif (!new)\r\ngoto err_get_desc;\r\nnew->chunks = chunks--;\r\nlist_add_tail(&new->node, &tx_list);\r\n} while (len);\r\n}\r\nif (new != first)\r\nnew->async_tx.cookie = -ENOSPC;\r\nlist_splice_tail(&tx_list, &sh_chan->ld_free);\r\nspin_unlock_irqrestore(&sh_chan->desc_lock, irq_flags);\r\nreturn &first->async_tx;\r\nerr_get_desc:\r\nlist_for_each_entry(new, &tx_list, node)\r\nnew->mark = DESC_IDLE;\r\nlist_splice(&tx_list, &sh_chan->ld_free);\r\nspin_unlock_irqrestore(&sh_chan->desc_lock, irq_flags);\r\nreturn NULL;\r\n}\r\nstatic struct dma_async_tx_descriptor *sh_dmae_prep_memcpy(\r\nstruct dma_chan *chan, dma_addr_t dma_dest, dma_addr_t dma_src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct sh_dmae_chan *sh_chan;\r\nstruct scatterlist sg;\r\nif (!chan || !len)\r\nreturn NULL;\r\nsh_chan = to_sh_chan(chan);\r\nsg_init_table(&sg, 1);\r\nsg_set_page(&sg, pfn_to_page(PFN_DOWN(dma_src)), len,\r\noffset_in_page(dma_src));\r\nsg_dma_address(&sg) = dma_src;\r\nsg_dma_len(&sg) = len;\r\nreturn sh_dmae_prep_sg(sh_chan, &sg, 1, &dma_dest, DMA_MEM_TO_MEM,\r\nflags);\r\n}\r\nstatic struct dma_async_tx_descriptor *sh_dmae_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl, unsigned int sg_len,\r\nenum dma_transfer_direction direction, unsigned long flags,\r\nvoid *context)\r\n{\r\nstruct sh_dmae_slave *param;\r\nstruct sh_dmae_chan *sh_chan;\r\ndma_addr_t slave_addr;\r\nif (!chan)\r\nreturn NULL;\r\nsh_chan = to_sh_chan(chan);\r\nparam = chan->private;\r\nif (!param || !sg_len) {\r\ndev_warn(sh_chan->dev, "%s: bad parameter: %p, %d, %d\n",\r\n__func__, param, sg_len, param ? param->slave_id : -1);\r\nreturn NULL;\r\n}\r\nslave_addr = param->config->addr;\r\nreturn sh_dmae_prep_sg(sh_chan, sgl, sg_len, &slave_addr,\r\ndirection, flags);\r\n}\r\nstatic int sh_dmae_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct sh_dmae_chan *sh_chan = to_sh_chan(chan);\r\nunsigned long flags;\r\nif (cmd != DMA_TERMINATE_ALL)\r\nreturn -ENXIO;\r\nif (!chan)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&sh_chan->desc_lock, flags);\r\ndmae_halt(sh_chan);\r\nif (!list_empty(&sh_chan->ld_queue)) {\r\nstruct sh_desc *desc = list_entry(sh_chan->ld_queue.next,\r\nstruct sh_desc, node);\r\ndesc->partial = (desc->hw.tcr - sh_dmae_readl(sh_chan, TCR)) <<\r\nsh_chan->xmit_shift;\r\n}\r\nspin_unlock_irqrestore(&sh_chan->desc_lock, flags);\r\nsh_dmae_chan_ld_cleanup(sh_chan, true);\r\nreturn 0;\r\n}\r\nstatic dma_async_tx_callback __ld_cleanup(struct sh_dmae_chan *sh_chan, bool all)\r\n{\r\nstruct sh_desc *desc, *_desc;\r\nbool head_acked = false;\r\ndma_cookie_t cookie = 0;\r\ndma_async_tx_callback callback = NULL;\r\nvoid *param = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&sh_chan->desc_lock, flags);\r\nlist_for_each_entry_safe(desc, _desc, &sh_chan->ld_queue, node) {\r\nstruct dma_async_tx_descriptor *tx = &desc->async_tx;\r\nBUG_ON(tx->cookie > 0 && tx->cookie != desc->cookie);\r\nBUG_ON(desc->mark != DESC_SUBMITTED &&\r\ndesc->mark != DESC_COMPLETED &&\r\ndesc->mark != DESC_WAITING);\r\nif (!all && desc->mark == DESC_SUBMITTED &&\r\ndesc->cookie != cookie)\r\nbreak;\r\nif (tx->cookie > 0)\r\ncookie = tx->cookie;\r\nif (desc->mark == DESC_COMPLETED && desc->chunks == 1) {\r\nif (sh_chan->common.completed_cookie != desc->cookie - 1)\r\ndev_dbg(sh_chan->dev,\r\n"Completing cookie %d, expected %d\n",\r\ndesc->cookie,\r\nsh_chan->common.completed_cookie + 1);\r\nsh_chan->common.completed_cookie = desc->cookie;\r\n}\r\nif (desc->mark == DESC_COMPLETED && tx->callback) {\r\ndesc->mark = DESC_WAITING;\r\ncallback = tx->callback;\r\nparam = tx->callback_param;\r\ndev_dbg(sh_chan->dev, "descriptor #%d@%p on %d callback\n",\r\ntx->cookie, tx, sh_chan->id);\r\nBUG_ON(desc->chunks != 1);\r\nbreak;\r\n}\r\nif (tx->cookie > 0 || tx->cookie == -EBUSY) {\r\nif (desc->mark == DESC_COMPLETED) {\r\nBUG_ON(tx->cookie < 0);\r\ndesc->mark = DESC_WAITING;\r\n}\r\nhead_acked = async_tx_test_ack(tx);\r\n} else {\r\nswitch (desc->mark) {\r\ncase DESC_COMPLETED:\r\ndesc->mark = DESC_WAITING;\r\ncase DESC_WAITING:\r\nif (head_acked)\r\nasync_tx_ack(&desc->async_tx);\r\n}\r\n}\r\ndev_dbg(sh_chan->dev, "descriptor %p #%d completed.\n",\r\ntx, tx->cookie);\r\nif (((desc->mark == DESC_COMPLETED ||\r\ndesc->mark == DESC_WAITING) &&\r\nasync_tx_test_ack(&desc->async_tx)) || all) {\r\ndesc->mark = DESC_IDLE;\r\nlist_move(&desc->node, &sh_chan->ld_free);\r\nif (list_empty(&sh_chan->ld_queue)) {\r\ndev_dbg(sh_chan->dev, "Bring down channel %d\n", sh_chan->id);\r\npm_runtime_put(sh_chan->dev);\r\n}\r\n}\r\n}\r\nif (all && !callback)\r\nsh_chan->common.completed_cookie = sh_chan->common.cookie;\r\nspin_unlock_irqrestore(&sh_chan->desc_lock, flags);\r\nif (callback)\r\ncallback(param);\r\nreturn callback;\r\n}\r\nstatic void sh_dmae_chan_ld_cleanup(struct sh_dmae_chan *sh_chan, bool all)\r\n{\r\nwhile (__ld_cleanup(sh_chan, all))\r\n;\r\n}\r\nstatic void sh_chan_xfer_ld_queue(struct sh_dmae_chan *sh_chan)\r\n{\r\nstruct sh_desc *desc;\r\nif (dmae_is_busy(sh_chan))\r\nreturn;\r\nlist_for_each_entry(desc, &sh_chan->ld_queue, node)\r\nif (desc->mark == DESC_SUBMITTED) {\r\ndev_dbg(sh_chan->dev, "Queue #%d to %d: %u@%x -> %x\n",\r\ndesc->async_tx.cookie, sh_chan->id,\r\ndesc->hw.tcr, desc->hw.sar, desc->hw.dar);\r\ndmae_set_reg(sh_chan, &desc->hw);\r\ndmae_start(sh_chan);\r\nbreak;\r\n}\r\n}\r\nstatic void sh_dmae_memcpy_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct sh_dmae_chan *sh_chan = to_sh_chan(chan);\r\nspin_lock_irq(&sh_chan->desc_lock);\r\nif (sh_chan->pm_state == DMAE_PM_ESTABLISHED)\r\nsh_chan_xfer_ld_queue(sh_chan);\r\nelse\r\nsh_chan->pm_state = DMAE_PM_PENDING;\r\nspin_unlock_irq(&sh_chan->desc_lock);\r\n}\r\nstatic enum dma_status sh_dmae_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct sh_dmae_chan *sh_chan = to_sh_chan(chan);\r\nenum dma_status status;\r\nunsigned long flags;\r\nsh_dmae_chan_ld_cleanup(sh_chan, false);\r\nspin_lock_irqsave(&sh_chan->desc_lock, flags);\r\nstatus = dma_cookie_status(chan, cookie, txstate);\r\nif (status != DMA_SUCCESS) {\r\nstruct sh_desc *desc;\r\nstatus = DMA_ERROR;\r\nlist_for_each_entry(desc, &sh_chan->ld_queue, node)\r\nif (desc->cookie == cookie) {\r\nstatus = DMA_IN_PROGRESS;\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&sh_chan->desc_lock, flags);\r\nreturn status;\r\n}\r\nstatic irqreturn_t sh_dmae_interrupt(int irq, void *data)\r\n{\r\nirqreturn_t ret = IRQ_NONE;\r\nstruct sh_dmae_chan *sh_chan = data;\r\nu32 chcr;\r\nspin_lock(&sh_chan->desc_lock);\r\nchcr = chcr_read(sh_chan);\r\nif (chcr & CHCR_TE) {\r\ndmae_halt(sh_chan);\r\nret = IRQ_HANDLED;\r\ntasklet_schedule(&sh_chan->tasklet);\r\n}\r\nspin_unlock(&sh_chan->desc_lock);\r\nreturn ret;\r\n}\r\nstatic bool sh_dmae_reset(struct sh_dmae_device *shdev)\r\n{\r\nunsigned int handled = 0;\r\nint i;\r\nsh_dmae_ctl_stop(shdev);\r\nfor (i = 0; i < SH_DMAC_MAX_CHANNELS; i++) {\r\nstruct sh_dmae_chan *sh_chan = shdev->chan[i];\r\nstruct sh_desc *desc;\r\nLIST_HEAD(dl);\r\nif (!sh_chan)\r\ncontinue;\r\nspin_lock(&sh_chan->desc_lock);\r\ndmae_halt(sh_chan);\r\nlist_splice_init(&sh_chan->ld_queue, &dl);\r\nif (!list_empty(&dl)) {\r\ndev_dbg(sh_chan->dev, "Bring down channel %d\n", sh_chan->id);\r\npm_runtime_put(sh_chan->dev);\r\n}\r\nsh_chan->pm_state = DMAE_PM_ESTABLISHED;\r\nspin_unlock(&sh_chan->desc_lock);\r\nlist_for_each_entry(desc, &dl, node) {\r\nstruct dma_async_tx_descriptor *tx = &desc->async_tx;\r\ndesc->mark = DESC_IDLE;\r\nif (tx->callback)\r\ntx->callback(tx->callback_param);\r\n}\r\nspin_lock(&sh_chan->desc_lock);\r\nlist_splice(&dl, &sh_chan->ld_free);\r\nspin_unlock(&sh_chan->desc_lock);\r\nhandled++;\r\n}\r\nsh_dmae_rst(shdev);\r\nreturn !!handled;\r\n}\r\nstatic irqreturn_t sh_dmae_err(int irq, void *data)\r\n{\r\nstruct sh_dmae_device *shdev = data;\r\nif (!(dmaor_read(shdev) & DMAOR_AE))\r\nreturn IRQ_NONE;\r\nsh_dmae_reset(data);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void dmae_do_tasklet(unsigned long data)\r\n{\r\nstruct sh_dmae_chan *sh_chan = (struct sh_dmae_chan *)data;\r\nstruct sh_desc *desc;\r\nu32 sar_buf = sh_dmae_readl(sh_chan, SAR);\r\nu32 dar_buf = sh_dmae_readl(sh_chan, DAR);\r\nspin_lock_irq(&sh_chan->desc_lock);\r\nlist_for_each_entry(desc, &sh_chan->ld_queue, node) {\r\nif (desc->mark == DESC_SUBMITTED &&\r\n((desc->direction == DMA_DEV_TO_MEM &&\r\n(desc->hw.dar + desc->hw.tcr) == dar_buf) ||\r\n(desc->hw.sar + desc->hw.tcr) == sar_buf)) {\r\ndev_dbg(sh_chan->dev, "done #%d@%p dst %u\n",\r\ndesc->async_tx.cookie, &desc->async_tx,\r\ndesc->hw.dar);\r\ndesc->mark = DESC_COMPLETED;\r\nbreak;\r\n}\r\n}\r\nsh_chan_xfer_ld_queue(sh_chan);\r\nspin_unlock_irq(&sh_chan->desc_lock);\r\nsh_dmae_chan_ld_cleanup(sh_chan, false);\r\n}\r\nstatic bool sh_dmae_nmi_notify(struct sh_dmae_device *shdev)\r\n{\r\nif ((dmaor_read(shdev) & DMAOR_NMIF) == 0)\r\nreturn false;\r\nreturn sh_dmae_reset(shdev);\r\n}\r\nstatic int sh_dmae_nmi_handler(struct notifier_block *self,\r\nunsigned long cmd, void *data)\r\n{\r\nstruct sh_dmae_device *shdev;\r\nint ret = NOTIFY_DONE;\r\nbool triggered;\r\nif (!in_nmi())\r\nreturn NOTIFY_DONE;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(shdev, &sh_dmae_devices, node) {\r\ntriggered = sh_dmae_nmi_notify(shdev);\r\nif (triggered == true)\r\nret = NOTIFY_OK;\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nstatic int __devinit sh_dmae_chan_probe(struct sh_dmae_device *shdev, int id,\r\nint irq, unsigned long flags)\r\n{\r\nint err;\r\nconst struct sh_dmae_channel *chan_pdata = &shdev->pdata->channel[id];\r\nstruct platform_device *pdev = to_platform_device(shdev->common.dev);\r\nstruct sh_dmae_chan *new_sh_chan;\r\nnew_sh_chan = kzalloc(sizeof(struct sh_dmae_chan), GFP_KERNEL);\r\nif (!new_sh_chan) {\r\ndev_err(shdev->common.dev,\r\n"No free memory for allocating dma channels!\n");\r\nreturn -ENOMEM;\r\n}\r\nnew_sh_chan->pm_state = DMAE_PM_ESTABLISHED;\r\nnew_sh_chan->common.device = &shdev->common;\r\ndma_cookie_init(&new_sh_chan->common);\r\nnew_sh_chan->dev = shdev->common.dev;\r\nnew_sh_chan->id = id;\r\nnew_sh_chan->irq = irq;\r\nnew_sh_chan->base = shdev->chan_reg + chan_pdata->offset / sizeof(u32);\r\ntasklet_init(&new_sh_chan->tasklet, dmae_do_tasklet,\r\n(unsigned long)new_sh_chan);\r\nspin_lock_init(&new_sh_chan->desc_lock);\r\nINIT_LIST_HEAD(&new_sh_chan->ld_queue);\r\nINIT_LIST_HEAD(&new_sh_chan->ld_free);\r\nlist_add_tail(&new_sh_chan->common.device_node,\r\n&shdev->common.channels);\r\nshdev->common.chancnt++;\r\nif (pdev->id >= 0)\r\nsnprintf(new_sh_chan->dev_id, sizeof(new_sh_chan->dev_id),\r\n"sh-dmae%d.%d", pdev->id, new_sh_chan->id);\r\nelse\r\nsnprintf(new_sh_chan->dev_id, sizeof(new_sh_chan->dev_id),\r\n"sh-dma%d", new_sh_chan->id);\r\nerr = request_irq(irq, &sh_dmae_interrupt, flags,\r\nnew_sh_chan->dev_id, new_sh_chan);\r\nif (err) {\r\ndev_err(shdev->common.dev, "DMA channel %d request_irq error "\r\n"with return %d\n", id, err);\r\ngoto err_no_irq;\r\n}\r\nshdev->chan[id] = new_sh_chan;\r\nreturn 0;\r\nerr_no_irq:\r\nlist_del(&new_sh_chan->common.device_node);\r\nkfree(new_sh_chan);\r\nreturn err;\r\n}\r\nstatic void sh_dmae_chan_remove(struct sh_dmae_device *shdev)\r\n{\r\nint i;\r\nfor (i = shdev->common.chancnt - 1 ; i >= 0 ; i--) {\r\nif (shdev->chan[i]) {\r\nstruct sh_dmae_chan *sh_chan = shdev->chan[i];\r\nfree_irq(sh_chan->irq, sh_chan);\r\nlist_del(&sh_chan->common.device_node);\r\nkfree(sh_chan);\r\nshdev->chan[i] = NULL;\r\n}\r\n}\r\nshdev->common.chancnt = 0;\r\n}\r\nstatic int __init sh_dmae_probe(struct platform_device *pdev)\r\n{\r\nstruct sh_dmae_pdata *pdata = pdev->dev.platform_data;\r\nunsigned long irqflags = IRQF_DISABLED,\r\nchan_flag[SH_DMAC_MAX_CHANNELS] = {};\r\nint errirq, chan_irq[SH_DMAC_MAX_CHANNELS];\r\nint err, i, irq_cnt = 0, irqres = 0, irq_cap = 0;\r\nstruct sh_dmae_device *shdev;\r\nstruct resource *chan, *dmars, *errirq_res, *chanirq_res;\r\nif (!pdata || !pdata->channel_num)\r\nreturn -ENODEV;\r\nchan = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ndmars = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nerrirq_res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (!chan || !errirq_res)\r\nreturn -ENODEV;\r\nif (!request_mem_region(chan->start, resource_size(chan), pdev->name)) {\r\ndev_err(&pdev->dev, "DMAC register region already claimed\n");\r\nreturn -EBUSY;\r\n}\r\nif (dmars && !request_mem_region(dmars->start, resource_size(dmars), pdev->name)) {\r\ndev_err(&pdev->dev, "DMAC DMARS region already claimed\n");\r\nerr = -EBUSY;\r\ngoto ermrdmars;\r\n}\r\nerr = -ENOMEM;\r\nshdev = kzalloc(sizeof(struct sh_dmae_device), GFP_KERNEL);\r\nif (!shdev) {\r\ndev_err(&pdev->dev, "Not enough memory\n");\r\ngoto ealloc;\r\n}\r\nshdev->chan_reg = ioremap(chan->start, resource_size(chan));\r\nif (!shdev->chan_reg)\r\ngoto emapchan;\r\nif (dmars) {\r\nshdev->dmars = ioremap(dmars->start, resource_size(dmars));\r\nif (!shdev->dmars)\r\ngoto emapdmars;\r\n}\r\nshdev->pdata = pdata;\r\nif (pdata->chcr_offset)\r\nshdev->chcr_offset = pdata->chcr_offset;\r\nelse\r\nshdev->chcr_offset = CHCR;\r\nif (pdata->chcr_ie_bit)\r\nshdev->chcr_ie_bit = pdata->chcr_ie_bit;\r\nelse\r\nshdev->chcr_ie_bit = CHCR_IE;\r\nplatform_set_drvdata(pdev, shdev);\r\nshdev->common.dev = &pdev->dev;\r\npm_runtime_enable(&pdev->dev);\r\npm_runtime_get_sync(&pdev->dev);\r\nspin_lock_irq(&sh_dmae_lock);\r\nlist_add_tail_rcu(&shdev->node, &sh_dmae_devices);\r\nspin_unlock_irq(&sh_dmae_lock);\r\nerr = sh_dmae_rst(shdev);\r\nif (err)\r\ngoto rst_err;\r\nINIT_LIST_HEAD(&shdev->common.channels);\r\nif (!pdata->slave_only)\r\ndma_cap_set(DMA_MEMCPY, shdev->common.cap_mask);\r\nif (pdata->slave && pdata->slave_num)\r\ndma_cap_set(DMA_SLAVE, shdev->common.cap_mask);\r\nshdev->common.device_alloc_chan_resources\r\n= sh_dmae_alloc_chan_resources;\r\nshdev->common.device_free_chan_resources = sh_dmae_free_chan_resources;\r\nshdev->common.device_prep_dma_memcpy = sh_dmae_prep_memcpy;\r\nshdev->common.device_tx_status = sh_dmae_tx_status;\r\nshdev->common.device_issue_pending = sh_dmae_memcpy_issue_pending;\r\nshdev->common.device_prep_slave_sg = sh_dmae_prep_slave_sg;\r\nshdev->common.device_control = sh_dmae_control;\r\nshdev->common.copy_align = LOG2_DEFAULT_XFER_SIZE;\r\n#if defined(CONFIG_CPU_SH4) || defined(CONFIG_ARCH_SHMOBILE)\r\nchanirq_res = platform_get_resource(pdev, IORESOURCE_IRQ, 1);\r\nif (!chanirq_res)\r\nchanirq_res = errirq_res;\r\nelse\r\nirqres++;\r\nif (chanirq_res == errirq_res ||\r\n(errirq_res->flags & IORESOURCE_BITS) == IORESOURCE_IRQ_SHAREABLE)\r\nirqflags = IRQF_SHARED;\r\nerrirq = errirq_res->start;\r\nerr = request_irq(errirq, sh_dmae_err, irqflags,\r\n"DMAC Address Error", shdev);\r\nif (err) {\r\ndev_err(&pdev->dev,\r\n"DMA failed requesting irq #%d, error %d\n",\r\nerrirq, err);\r\ngoto eirq_err;\r\n}\r\n#else\r\nchanirq_res = errirq_res;\r\n#endif\r\nif (chanirq_res->start == chanirq_res->end &&\r\n!platform_get_resource(pdev, IORESOURCE_IRQ, 1)) {\r\nfor (; irq_cnt < pdata->channel_num; irq_cnt++) {\r\nif (irq_cnt < SH_DMAC_MAX_CHANNELS) {\r\nchan_irq[irq_cnt] = chanirq_res->start;\r\nchan_flag[irq_cnt] = IRQF_SHARED;\r\n} else {\r\nirq_cap = 1;\r\nbreak;\r\n}\r\n}\r\n} else {\r\ndo {\r\nfor (i = chanirq_res->start; i <= chanirq_res->end; i++) {\r\nif (irq_cnt >= SH_DMAC_MAX_CHANNELS) {\r\nirq_cap = 1;\r\nbreak;\r\n}\r\nif ((errirq_res->flags & IORESOURCE_BITS) ==\r\nIORESOURCE_IRQ_SHAREABLE)\r\nchan_flag[irq_cnt] = IRQF_SHARED;\r\nelse\r\nchan_flag[irq_cnt] = IRQF_DISABLED;\r\ndev_dbg(&pdev->dev,\r\n"Found IRQ %d for channel %d\n",\r\ni, irq_cnt);\r\nchan_irq[irq_cnt++] = i;\r\n}\r\nif (irq_cnt >= SH_DMAC_MAX_CHANNELS)\r\nbreak;\r\nchanirq_res = platform_get_resource(pdev,\r\nIORESOURCE_IRQ, ++irqres);\r\n} while (irq_cnt < pdata->channel_num && chanirq_res);\r\n}\r\nfor (i = 0; i < irq_cnt; i++) {\r\nerr = sh_dmae_chan_probe(shdev, i, chan_irq[i], chan_flag[i]);\r\nif (err)\r\ngoto chan_probe_err;\r\n}\r\nif (irq_cap)\r\ndev_notice(&pdev->dev, "Attempting to register %d DMA "\r\n"channels when a maximum of %d are supported.\n",\r\npdata->channel_num, SH_DMAC_MAX_CHANNELS);\r\npm_runtime_put(&pdev->dev);\r\ndma_async_device_register(&shdev->common);\r\nreturn err;\r\nchan_probe_err:\r\nsh_dmae_chan_remove(shdev);\r\n#if defined(CONFIG_CPU_SH4) || defined(CONFIG_ARCH_SHMOBILE)\r\nfree_irq(errirq, shdev);\r\neirq_err:\r\n#endif\r\nrst_err:\r\nspin_lock_irq(&sh_dmae_lock);\r\nlist_del_rcu(&shdev->node);\r\nspin_unlock_irq(&sh_dmae_lock);\r\npm_runtime_put(&pdev->dev);\r\npm_runtime_disable(&pdev->dev);\r\nif (dmars)\r\niounmap(shdev->dmars);\r\nplatform_set_drvdata(pdev, NULL);\r\nemapdmars:\r\niounmap(shdev->chan_reg);\r\nsynchronize_rcu();\r\nemapchan:\r\nkfree(shdev);\r\nealloc:\r\nif (dmars)\r\nrelease_mem_region(dmars->start, resource_size(dmars));\r\nermrdmars:\r\nrelease_mem_region(chan->start, resource_size(chan));\r\nreturn err;\r\n}\r\nstatic int __exit sh_dmae_remove(struct platform_device *pdev)\r\n{\r\nstruct sh_dmae_device *shdev = platform_get_drvdata(pdev);\r\nstruct resource *res;\r\nint errirq = platform_get_irq(pdev, 0);\r\ndma_async_device_unregister(&shdev->common);\r\nif (errirq > 0)\r\nfree_irq(errirq, shdev);\r\nspin_lock_irq(&sh_dmae_lock);\r\nlist_del_rcu(&shdev->node);\r\nspin_unlock_irq(&sh_dmae_lock);\r\nsh_dmae_chan_remove(shdev);\r\npm_runtime_disable(&pdev->dev);\r\nif (shdev->dmars)\r\niounmap(shdev->dmars);\r\niounmap(shdev->chan_reg);\r\nplatform_set_drvdata(pdev, NULL);\r\nsynchronize_rcu();\r\nkfree(shdev);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (res)\r\nrelease_mem_region(res->start, resource_size(res));\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nif (res)\r\nrelease_mem_region(res->start, resource_size(res));\r\nreturn 0;\r\n}\r\nstatic void sh_dmae_shutdown(struct platform_device *pdev)\r\n{\r\nstruct sh_dmae_device *shdev = platform_get_drvdata(pdev);\r\nsh_dmae_ctl_stop(shdev);\r\n}\r\nstatic int sh_dmae_runtime_suspend(struct device *dev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int sh_dmae_runtime_resume(struct device *dev)\r\n{\r\nstruct sh_dmae_device *shdev = dev_get_drvdata(dev);\r\nreturn sh_dmae_rst(shdev);\r\n}\r\nstatic int sh_dmae_suspend(struct device *dev)\r\n{\r\nreturn 0;\r\n}\r\nstatic int sh_dmae_resume(struct device *dev)\r\n{\r\nstruct sh_dmae_device *shdev = dev_get_drvdata(dev);\r\nint i, ret;\r\nret = sh_dmae_rst(shdev);\r\nif (ret < 0)\r\ndev_err(dev, "Failed to reset!\n");\r\nfor (i = 0; i < shdev->pdata->channel_num; i++) {\r\nstruct sh_dmae_chan *sh_chan = shdev->chan[i];\r\nstruct sh_dmae_slave *param = sh_chan->common.private;\r\nif (!sh_chan->descs_allocated)\r\ncontinue;\r\nif (param) {\r\nconst struct sh_dmae_slave_config *cfg = param->config;\r\ndmae_set_dmars(sh_chan, cfg->mid_rid);\r\ndmae_set_chcr(sh_chan, cfg->chcr);\r\n} else {\r\ndmae_init(sh_chan);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init sh_dmae_init(void)\r\n{\r\nint err = register_die_notifier(&sh_dmae_nmi_notifier);\r\nif (err)\r\nreturn err;\r\nreturn platform_driver_probe(&sh_dmae_driver, sh_dmae_probe);\r\n}\r\nstatic void __exit sh_dmae_exit(void)\r\n{\r\nplatform_driver_unregister(&sh_dmae_driver);\r\nunregister_die_notifier(&sh_dmae_nmi_notifier);\r\n}
