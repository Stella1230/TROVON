static inline void comp_event_callback(struct ehca_cq *cq)\r\n{\r\nif (!cq->ib_cq.comp_handler)\r\nreturn;\r\nspin_lock(&cq->cb_lock);\r\ncq->ib_cq.comp_handler(&cq->ib_cq, cq->ib_cq.cq_context);\r\nspin_unlock(&cq->cb_lock);\r\nreturn;\r\n}\r\nstatic void print_error_data(struct ehca_shca *shca, void *data,\r\nu64 *rblock, int length)\r\n{\r\nu64 type = EHCA_BMASK_GET(ERROR_DATA_TYPE, rblock[2]);\r\nu64 resource = rblock[1];\r\nswitch (type) {\r\ncase 0x1:\r\n{\r\nstruct ehca_qp *qp = (struct ehca_qp *)data;\r\nif (rblock[6] == 0)\r\nreturn;\r\nehca_err(&shca->ib_device,\r\n"QP 0x%x (resource=%llx) has errors.",\r\nqp->ib_qp.qp_num, resource);\r\nbreak;\r\n}\r\ncase 0x4:\r\n{\r\nstruct ehca_cq *cq = (struct ehca_cq *)data;\r\nehca_err(&shca->ib_device,\r\n"CQ 0x%x (resource=%llx) has errors.",\r\ncq->cq_number, resource);\r\nbreak;\r\n}\r\ndefault:\r\nehca_err(&shca->ib_device,\r\n"Unknown error type: %llx on %s.",\r\ntype, shca->ib_device.name);\r\nbreak;\r\n}\r\nehca_err(&shca->ib_device, "Error data is available: %llx.", resource);\r\nehca_err(&shca->ib_device, "EHCA ----- error data begin "\r\n"---------------------------------------------------");\r\nehca_dmp(rblock, length, "resource=%llx", resource);\r\nehca_err(&shca->ib_device, "EHCA ----- error data end "\r\n"----------------------------------------------------");\r\nreturn;\r\n}\r\nint ehca_error_data(struct ehca_shca *shca, void *data,\r\nu64 resource)\r\n{\r\nunsigned long ret;\r\nu64 *rblock;\r\nunsigned long block_count;\r\nrblock = ehca_alloc_fw_ctrlblock(GFP_ATOMIC);\r\nif (!rblock) {\r\nehca_err(&shca->ib_device, "Cannot allocate rblock memory.");\r\nret = -ENOMEM;\r\ngoto error_data1;\r\n}\r\nret = hipz_h_error_data(shca->ipz_hca_handle,\r\nresource,\r\nrblock,\r\n&block_count);\r\nif (ret == H_R_STATE)\r\nehca_err(&shca->ib_device,\r\n"No error data is available: %llx.", resource);\r\nelse if (ret == H_SUCCESS) {\r\nint length;\r\nlength = EHCA_BMASK_GET(ERROR_DATA_LENGTH, rblock[0]);\r\nif (length > EHCA_PAGESIZE)\r\nlength = EHCA_PAGESIZE;\r\nprint_error_data(shca, data, rblock, length);\r\n} else\r\nehca_err(&shca->ib_device,\r\n"Error data could not be fetched: %llx", resource);\r\nehca_free_fw_ctrlblock(rblock);\r\nerror_data1:\r\nreturn ret;\r\n}\r\nstatic void dispatch_qp_event(struct ehca_shca *shca, struct ehca_qp *qp,\r\nenum ib_event_type event_type)\r\n{\r\nstruct ib_event event;\r\nif (event_type == IB_EVENT_PATH_MIG && !qp->mig_armed)\r\nreturn;\r\nevent.device = &shca->ib_device;\r\nevent.event = event_type;\r\nif (qp->ext_type == EQPT_SRQ) {\r\nif (!qp->ib_srq.event_handler)\r\nreturn;\r\nevent.element.srq = &qp->ib_srq;\r\nqp->ib_srq.event_handler(&event, qp->ib_srq.srq_context);\r\n} else {\r\nif (!qp->ib_qp.event_handler)\r\nreturn;\r\nevent.element.qp = &qp->ib_qp;\r\nqp->ib_qp.event_handler(&event, qp->ib_qp.qp_context);\r\n}\r\n}\r\nstatic void qp_event_callback(struct ehca_shca *shca, u64 eqe,\r\nenum ib_event_type event_type, int fatal)\r\n{\r\nstruct ehca_qp *qp;\r\nu32 token = EHCA_BMASK_GET(EQE_QP_TOKEN, eqe);\r\nread_lock(&ehca_qp_idr_lock);\r\nqp = idr_find(&ehca_qp_idr, token);\r\nif (qp)\r\natomic_inc(&qp->nr_events);\r\nread_unlock(&ehca_qp_idr_lock);\r\nif (!qp)\r\nreturn;\r\nif (fatal)\r\nehca_error_data(shca, qp, qp->ipz_qp_handle.handle);\r\ndispatch_qp_event(shca, qp, fatal && qp->ext_type == EQPT_SRQ ?\r\nIB_EVENT_SRQ_ERR : event_type);\r\nif (fatal && qp->ext_type == EQPT_SRQBASE)\r\ndispatch_qp_event(shca, qp, IB_EVENT_QP_LAST_WQE_REACHED);\r\nif (atomic_dec_and_test(&qp->nr_events))\r\nwake_up(&qp->wait_completion);\r\nreturn;\r\n}\r\nstatic void cq_event_callback(struct ehca_shca *shca,\r\nu64 eqe)\r\n{\r\nstruct ehca_cq *cq;\r\nu32 token = EHCA_BMASK_GET(EQE_CQ_TOKEN, eqe);\r\nread_lock(&ehca_cq_idr_lock);\r\ncq = idr_find(&ehca_cq_idr, token);\r\nif (cq)\r\natomic_inc(&cq->nr_events);\r\nread_unlock(&ehca_cq_idr_lock);\r\nif (!cq)\r\nreturn;\r\nehca_error_data(shca, cq, cq->ipz_cq_handle.handle);\r\nif (atomic_dec_and_test(&cq->nr_events))\r\nwake_up(&cq->wait_completion);\r\nreturn;\r\n}\r\nstatic void parse_identifier(struct ehca_shca *shca, u64 eqe)\r\n{\r\nu8 identifier = EHCA_BMASK_GET(EQE_EE_IDENTIFIER, eqe);\r\nswitch (identifier) {\r\ncase 0x02:\r\nqp_event_callback(shca, eqe, IB_EVENT_PATH_MIG, 0);\r\nbreak;\r\ncase 0x03:\r\nqp_event_callback(shca, eqe, IB_EVENT_COMM_EST, 0);\r\nbreak;\r\ncase 0x04:\r\nqp_event_callback(shca, eqe, IB_EVENT_SQ_DRAINED, 0);\r\nbreak;\r\ncase 0x05:\r\ncase 0x06:\r\nqp_event_callback(shca, eqe, IB_EVENT_QP_FATAL, 1);\r\nbreak;\r\ncase 0x07:\r\ncase 0x08:\r\ncq_event_callback(shca, eqe);\r\nbreak;\r\ncase 0x09:\r\nehca_err(&shca->ib_device, "MRMWPTE error.");\r\nbreak;\r\ncase 0x0A:\r\nehca_err(&shca->ib_device, "Port event.");\r\nbreak;\r\ncase 0x0B:\r\nehca_err(&shca->ib_device, "MR access error.");\r\nbreak;\r\ncase 0x0C:\r\nehca_err(&shca->ib_device, "EQ error.");\r\nbreak;\r\ncase 0x0D:\r\nehca_err(&shca->ib_device, "P/Q_Key mismatch.");\r\nbreak;\r\ncase 0x10:\r\nehca_err(&shca->ib_device, "Sampling complete.");\r\nbreak;\r\ncase 0x11:\r\nehca_err(&shca->ib_device, "Unaffiliated access error.");\r\nbreak;\r\ncase 0x12:\r\nehca_err(&shca->ib_device, "Path migrating.");\r\nbreak;\r\ncase 0x13:\r\nehca_err(&shca->ib_device, "Interface trace stopped.");\r\nbreak;\r\ncase 0x14:\r\nehca_info(&shca->ib_device, "First error capture available");\r\nbreak;\r\ncase 0x15:\r\nqp_event_callback(shca, eqe, IB_EVENT_SRQ_LIMIT_REACHED, 0);\r\nbreak;\r\ndefault:\r\nehca_err(&shca->ib_device, "Unknown identifier: %x on %s.",\r\nidentifier, shca->ib_device.name);\r\nbreak;\r\n}\r\nreturn;\r\n}\r\nstatic void dispatch_port_event(struct ehca_shca *shca, int port_num,\r\nenum ib_event_type type, const char *msg)\r\n{\r\nstruct ib_event event;\r\nehca_info(&shca->ib_device, "port %d %s.", port_num, msg);\r\nevent.device = &shca->ib_device;\r\nevent.event = type;\r\nevent.element.port_num = port_num;\r\nib_dispatch_event(&event);\r\n}\r\nstatic void notify_port_conf_change(struct ehca_shca *shca, int port_num)\r\n{\r\nstruct ehca_sma_attr new_attr;\r\nstruct ehca_sma_attr *old_attr = &shca->sport[port_num - 1].saved_attr;\r\nehca_query_sma_attr(shca, port_num, &new_attr);\r\nif (new_attr.sm_sl != old_attr->sm_sl ||\r\nnew_attr.sm_lid != old_attr->sm_lid)\r\ndispatch_port_event(shca, port_num, IB_EVENT_SM_CHANGE,\r\n"SM changed");\r\nif (new_attr.lid != old_attr->lid ||\r\nnew_attr.lmc != old_attr->lmc)\r\ndispatch_port_event(shca, port_num, IB_EVENT_LID_CHANGE,\r\n"LID changed");\r\nif (new_attr.pkey_tbl_len != old_attr->pkey_tbl_len ||\r\nmemcmp(new_attr.pkeys, old_attr->pkeys,\r\nsizeof(u16) * new_attr.pkey_tbl_len))\r\ndispatch_port_event(shca, port_num, IB_EVENT_PKEY_CHANGE,\r\n"P_Key changed");\r\n*old_attr = new_attr;\r\n}\r\nstatic int replay_modify_qp(struct ehca_sport *sport)\r\n{\r\nint aqp1_destroyed;\r\nunsigned long flags;\r\nspin_lock_irqsave(&sport->mod_sqp_lock, flags);\r\naqp1_destroyed = !sport->ibqp_sqp[IB_QPT_GSI];\r\nif (sport->ibqp_sqp[IB_QPT_SMI])\r\nehca_recover_sqp(sport->ibqp_sqp[IB_QPT_SMI]);\r\nif (!aqp1_destroyed)\r\nehca_recover_sqp(sport->ibqp_sqp[IB_QPT_GSI]);\r\nspin_unlock_irqrestore(&sport->mod_sqp_lock, flags);\r\nreturn aqp1_destroyed;\r\n}\r\nstatic void parse_ec(struct ehca_shca *shca, u64 eqe)\r\n{\r\nu8 ec = EHCA_BMASK_GET(NEQE_EVENT_CODE, eqe);\r\nu8 port = EHCA_BMASK_GET(NEQE_PORT_NUMBER, eqe);\r\nu8 spec_event;\r\nstruct ehca_sport *sport = &shca->sport[port - 1];\r\nswitch (ec) {\r\ncase 0x30:\r\nif (EHCA_BMASK_GET(NEQE_PORT_AVAILABILITY, eqe)) {\r\nif (ehca_nr_ports < 0)\r\nif (replay_modify_qp(sport))\r\nbreak;\r\nsport->port_state = IB_PORT_ACTIVE;\r\ndispatch_port_event(shca, port, IB_EVENT_PORT_ACTIVE,\r\n"is active");\r\nehca_query_sma_attr(shca, port, &sport->saved_attr);\r\n} else {\r\nsport->port_state = IB_PORT_DOWN;\r\ndispatch_port_event(shca, port, IB_EVENT_PORT_ERR,\r\n"is inactive");\r\n}\r\nbreak;\r\ncase 0x31:\r\nif (EHCA_BMASK_GET(NEQE_DISRUPTIVE, eqe)) {\r\nehca_warn(&shca->ib_device, "disruptive port "\r\n"%d configuration change", port);\r\nsport->port_state = IB_PORT_DOWN;\r\ndispatch_port_event(shca, port, IB_EVENT_PORT_ERR,\r\n"is inactive");\r\nsport->port_state = IB_PORT_ACTIVE;\r\ndispatch_port_event(shca, port, IB_EVENT_PORT_ACTIVE,\r\n"is active");\r\nehca_query_sma_attr(shca, port,\r\n&sport->saved_attr);\r\n} else\r\nnotify_port_conf_change(shca, port);\r\nbreak;\r\ncase 0x32:\r\nehca_err(&shca->ib_device, "Adapter malfunction.");\r\nbreak;\r\ncase 0x33:\r\nehca_err(&shca->ib_device, "Traced stopped.");\r\nbreak;\r\ncase 0x34:\r\nspec_event = EHCA_BMASK_GET(NEQE_SPECIFIC_EVENT, eqe);\r\nif (spec_event == 0x80)\r\ndispatch_port_event(shca, port,\r\nIB_EVENT_CLIENT_REREGISTER,\r\n"client reregister req.");\r\nelse\r\nehca_warn(&shca->ib_device, "Unknown util async "\r\n"event %x on port %x", spec_event, port);\r\nbreak;\r\ndefault:\r\nehca_err(&shca->ib_device, "Unknown event code: %x on %s.",\r\nec, shca->ib_device.name);\r\nbreak;\r\n}\r\nreturn;\r\n}\r\nstatic inline void reset_eq_pending(struct ehca_cq *cq)\r\n{\r\nu64 CQx_EP;\r\nstruct h_galpa gal = cq->galpas.kernel;\r\nhipz_galpa_store_cq(gal, cqx_ep, 0x0);\r\nCQx_EP = hipz_galpa_load(gal, CQTEMM_OFFSET(cqx_ep));\r\nreturn;\r\n}\r\nirqreturn_t ehca_interrupt_neq(int irq, void *dev_id)\r\n{\r\nstruct ehca_shca *shca = (struct ehca_shca*)dev_id;\r\ntasklet_hi_schedule(&shca->neq.interrupt_task);\r\nreturn IRQ_HANDLED;\r\n}\r\nvoid ehca_tasklet_neq(unsigned long data)\r\n{\r\nstruct ehca_shca *shca = (struct ehca_shca*)data;\r\nstruct ehca_eqe *eqe;\r\nu64 ret;\r\neqe = ehca_poll_eq(shca, &shca->neq);\r\nwhile (eqe) {\r\nif (!EHCA_BMASK_GET(NEQE_COMPLETION_EVENT, eqe->entry))\r\nparse_ec(shca, eqe->entry);\r\neqe = ehca_poll_eq(shca, &shca->neq);\r\n}\r\nret = hipz_h_reset_event(shca->ipz_hca_handle,\r\nshca->neq.ipz_eq_handle, 0xFFFFFFFFFFFFFFFFL);\r\nif (ret != H_SUCCESS)\r\nehca_err(&shca->ib_device, "Can't clear notification events.");\r\nreturn;\r\n}\r\nirqreturn_t ehca_interrupt_eq(int irq, void *dev_id)\r\n{\r\nstruct ehca_shca *shca = (struct ehca_shca*)dev_id;\r\ntasklet_hi_schedule(&shca->eq.interrupt_task);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic inline void process_eqe(struct ehca_shca *shca, struct ehca_eqe *eqe)\r\n{\r\nu64 eqe_value;\r\nu32 token;\r\nstruct ehca_cq *cq;\r\neqe_value = eqe->entry;\r\nehca_dbg(&shca->ib_device, "eqe_value=%llx", eqe_value);\r\nif (EHCA_BMASK_GET(EQE_COMPLETION_EVENT, eqe_value)) {\r\nehca_dbg(&shca->ib_device, "Got completion event");\r\ntoken = EHCA_BMASK_GET(EQE_CQ_TOKEN, eqe_value);\r\nread_lock(&ehca_cq_idr_lock);\r\ncq = idr_find(&ehca_cq_idr, token);\r\nif (cq)\r\natomic_inc(&cq->nr_events);\r\nread_unlock(&ehca_cq_idr_lock);\r\nif (cq == NULL) {\r\nehca_err(&shca->ib_device,\r\n"Invalid eqe for non-existing cq token=%x",\r\ntoken);\r\nreturn;\r\n}\r\nreset_eq_pending(cq);\r\nif (ehca_scaling_code)\r\nqueue_comp_task(cq);\r\nelse {\r\ncomp_event_callback(cq);\r\nif (atomic_dec_and_test(&cq->nr_events))\r\nwake_up(&cq->wait_completion);\r\n}\r\n} else {\r\nehca_dbg(&shca->ib_device, "Got non completion event");\r\nparse_identifier(shca, eqe_value);\r\n}\r\n}\r\nvoid ehca_process_eq(struct ehca_shca *shca, int is_irq)\r\n{\r\nstruct ehca_eq *eq = &shca->eq;\r\nstruct ehca_eqe_cache_entry *eqe_cache = eq->eqe_cache;\r\nu64 eqe_value, ret;\r\nint eqe_cnt, i;\r\nint eq_empty = 0;\r\nspin_lock(&eq->irq_spinlock);\r\nif (is_irq) {\r\nconst int max_query_cnt = 100;\r\nint query_cnt = 0;\r\nint int_state = 1;\r\ndo {\r\nint_state = hipz_h_query_int_state(\r\nshca->ipz_hca_handle, eq->ist);\r\nquery_cnt++;\r\niosync();\r\n} while (int_state && query_cnt < max_query_cnt);\r\nif (unlikely((query_cnt == max_query_cnt)))\r\nehca_dbg(&shca->ib_device, "int_state=%x query_cnt=%x",\r\nint_state, query_cnt);\r\n}\r\neqe_cnt = 0;\r\ndo {\r\nu32 token;\r\neqe_cache[eqe_cnt].eqe = ehca_poll_eq(shca, eq);\r\nif (!eqe_cache[eqe_cnt].eqe)\r\nbreak;\r\neqe_value = eqe_cache[eqe_cnt].eqe->entry;\r\nif (EHCA_BMASK_GET(EQE_COMPLETION_EVENT, eqe_value)) {\r\ntoken = EHCA_BMASK_GET(EQE_CQ_TOKEN, eqe_value);\r\nread_lock(&ehca_cq_idr_lock);\r\neqe_cache[eqe_cnt].cq = idr_find(&ehca_cq_idr, token);\r\nif (eqe_cache[eqe_cnt].cq)\r\natomic_inc(&eqe_cache[eqe_cnt].cq->nr_events);\r\nread_unlock(&ehca_cq_idr_lock);\r\nif (!eqe_cache[eqe_cnt].cq) {\r\nehca_err(&shca->ib_device,\r\n"Invalid eqe for non-existing cq "\r\n"token=%x", token);\r\ncontinue;\r\n}\r\n} else\r\neqe_cache[eqe_cnt].cq = NULL;\r\neqe_cnt++;\r\n} while (eqe_cnt < EHCA_EQE_CACHE_SIZE);\r\nif (!eqe_cnt) {\r\nif (is_irq)\r\nehca_dbg(&shca->ib_device,\r\n"No eqe found for irq event");\r\ngoto unlock_irq_spinlock;\r\n} else if (!is_irq) {\r\nret = hipz_h_eoi(eq->ist);\r\nif (ret != H_SUCCESS)\r\nehca_err(&shca->ib_device,\r\n"bad return code EOI -rc = %lld\n", ret);\r\nehca_dbg(&shca->ib_device, "deadman found %x eqe", eqe_cnt);\r\n}\r\nif (unlikely(eqe_cnt == EHCA_EQE_CACHE_SIZE))\r\nehca_dbg(&shca->ib_device, "too many eqes for one irq event");\r\nfor (i = 0; i < eqe_cnt; i++) {\r\nif (eq->eqe_cache[i].cq)\r\nreset_eq_pending(eq->eqe_cache[i].cq);\r\n}\r\nspin_lock(&eq->spinlock);\r\neq_empty = (!ipz_eqit_eq_peek_valid(&shca->eq.ipz_queue));\r\nspin_unlock(&eq->spinlock);\r\nfor (i = 0; i < eqe_cnt; i++)\r\nif (eq->eqe_cache[i].cq) {\r\nif (ehca_scaling_code)\r\nqueue_comp_task(eq->eqe_cache[i].cq);\r\nelse {\r\nstruct ehca_cq *cq = eq->eqe_cache[i].cq;\r\ncomp_event_callback(cq);\r\nif (atomic_dec_and_test(&cq->nr_events))\r\nwake_up(&cq->wait_completion);\r\n}\r\n} else {\r\nehca_dbg(&shca->ib_device, "Got non completion event");\r\nparse_identifier(shca, eq->eqe_cache[i].eqe->entry);\r\n}\r\nif (eq_empty)\r\ngoto unlock_irq_spinlock;\r\ndo {\r\nstruct ehca_eqe *eqe;\r\neqe = ehca_poll_eq(shca, &shca->eq);\r\nif (!eqe)\r\nbreak;\r\nprocess_eqe(shca, eqe);\r\n} while (1);\r\nunlock_irq_spinlock:\r\nspin_unlock(&eq->irq_spinlock);\r\n}\r\nvoid ehca_tasklet_eq(unsigned long data)\r\n{\r\nehca_process_eq((struct ehca_shca*)data, 1);\r\n}\r\nstatic inline int find_next_online_cpu(struct ehca_comp_pool *pool)\r\n{\r\nint cpu;\r\nunsigned long flags;\r\nWARN_ON_ONCE(!in_interrupt());\r\nif (ehca_debug_level >= 3)\r\nehca_dmp(cpu_online_mask, cpumask_size(), "");\r\nspin_lock_irqsave(&pool->last_cpu_lock, flags);\r\ncpu = cpumask_next(pool->last_cpu, cpu_online_mask);\r\nif (cpu >= nr_cpu_ids)\r\ncpu = cpumask_first(cpu_online_mask);\r\npool->last_cpu = cpu;\r\nspin_unlock_irqrestore(&pool->last_cpu_lock, flags);\r\nreturn cpu;\r\n}\r\nstatic void __queue_comp_task(struct ehca_cq *__cq,\r\nstruct ehca_cpu_comp_task *cct)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&cct->task_lock, flags);\r\nspin_lock(&__cq->task_lock);\r\nif (__cq->nr_callbacks == 0) {\r\n__cq->nr_callbacks++;\r\nlist_add_tail(&__cq->entry, &cct->cq_list);\r\ncct->cq_jobs++;\r\nwake_up(&cct->wait_queue);\r\n} else\r\n__cq->nr_callbacks++;\r\nspin_unlock(&__cq->task_lock);\r\nspin_unlock_irqrestore(&cct->task_lock, flags);\r\n}\r\nstatic void queue_comp_task(struct ehca_cq *__cq)\r\n{\r\nint cpu_id;\r\nstruct ehca_cpu_comp_task *cct;\r\nint cq_jobs;\r\nunsigned long flags;\r\ncpu_id = find_next_online_cpu(pool);\r\nBUG_ON(!cpu_online(cpu_id));\r\ncct = per_cpu_ptr(pool->cpu_comp_tasks, cpu_id);\r\nBUG_ON(!cct);\r\nspin_lock_irqsave(&cct->task_lock, flags);\r\ncq_jobs = cct->cq_jobs;\r\nspin_unlock_irqrestore(&cct->task_lock, flags);\r\nif (cq_jobs > 0) {\r\ncpu_id = find_next_online_cpu(pool);\r\ncct = per_cpu_ptr(pool->cpu_comp_tasks, cpu_id);\r\nBUG_ON(!cct);\r\n}\r\n__queue_comp_task(__cq, cct);\r\n}\r\nstatic void run_comp_task(struct ehca_cpu_comp_task *cct)\r\n{\r\nstruct ehca_cq *cq;\r\nunsigned long flags;\r\nspin_lock_irqsave(&cct->task_lock, flags);\r\nwhile (!list_empty(&cct->cq_list)) {\r\ncq = list_entry(cct->cq_list.next, struct ehca_cq, entry);\r\nspin_unlock_irqrestore(&cct->task_lock, flags);\r\ncomp_event_callback(cq);\r\nif (atomic_dec_and_test(&cq->nr_events))\r\nwake_up(&cq->wait_completion);\r\nspin_lock_irqsave(&cct->task_lock, flags);\r\nspin_lock(&cq->task_lock);\r\ncq->nr_callbacks--;\r\nif (!cq->nr_callbacks) {\r\nlist_del_init(cct->cq_list.next);\r\ncct->cq_jobs--;\r\n}\r\nspin_unlock(&cq->task_lock);\r\n}\r\nspin_unlock_irqrestore(&cct->task_lock, flags);\r\n}\r\nstatic int comp_task(void *__cct)\r\n{\r\nstruct ehca_cpu_comp_task *cct = __cct;\r\nint cql_empty;\r\nDECLARE_WAITQUEUE(wait, current);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nwhile (!kthread_should_stop()) {\r\nadd_wait_queue(&cct->wait_queue, &wait);\r\nspin_lock_irq(&cct->task_lock);\r\ncql_empty = list_empty(&cct->cq_list);\r\nspin_unlock_irq(&cct->task_lock);\r\nif (cql_empty)\r\nschedule();\r\nelse\r\n__set_current_state(TASK_RUNNING);\r\nremove_wait_queue(&cct->wait_queue, &wait);\r\nspin_lock_irq(&cct->task_lock);\r\ncql_empty = list_empty(&cct->cq_list);\r\nspin_unlock_irq(&cct->task_lock);\r\nif (!cql_empty)\r\nrun_comp_task(__cct);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\n}\r\n__set_current_state(TASK_RUNNING);\r\nreturn 0;\r\n}\r\nstatic struct task_struct *create_comp_task(struct ehca_comp_pool *pool,\r\nint cpu)\r\n{\r\nstruct ehca_cpu_comp_task *cct;\r\ncct = per_cpu_ptr(pool->cpu_comp_tasks, cpu);\r\nspin_lock_init(&cct->task_lock);\r\nINIT_LIST_HEAD(&cct->cq_list);\r\ninit_waitqueue_head(&cct->wait_queue);\r\ncct->task = kthread_create_on_node(comp_task, cct, cpu_to_node(cpu),\r\n"ehca_comp/%d", cpu);\r\nreturn cct->task;\r\n}\r\nstatic void destroy_comp_task(struct ehca_comp_pool *pool,\r\nint cpu)\r\n{\r\nstruct ehca_cpu_comp_task *cct;\r\nstruct task_struct *task;\r\nunsigned long flags_cct;\r\ncct = per_cpu_ptr(pool->cpu_comp_tasks, cpu);\r\nspin_lock_irqsave(&cct->task_lock, flags_cct);\r\ntask = cct->task;\r\ncct->task = NULL;\r\ncct->cq_jobs = 0;\r\nspin_unlock_irqrestore(&cct->task_lock, flags_cct);\r\nif (task)\r\nkthread_stop(task);\r\n}\r\nstatic void __cpuinit take_over_work(struct ehca_comp_pool *pool, int cpu)\r\n{\r\nstruct ehca_cpu_comp_task *cct = per_cpu_ptr(pool->cpu_comp_tasks, cpu);\r\nLIST_HEAD(list);\r\nstruct ehca_cq *cq;\r\nunsigned long flags_cct;\r\nspin_lock_irqsave(&cct->task_lock, flags_cct);\r\nlist_splice_init(&cct->cq_list, &list);\r\nwhile (!list_empty(&list)) {\r\ncq = list_entry(cct->cq_list.next, struct ehca_cq, entry);\r\nlist_del(&cq->entry);\r\n__queue_comp_task(cq, this_cpu_ptr(pool->cpu_comp_tasks));\r\n}\r\nspin_unlock_irqrestore(&cct->task_lock, flags_cct);\r\n}\r\nstatic int __cpuinit comp_pool_callback(struct notifier_block *nfb,\r\nunsigned long action,\r\nvoid *hcpu)\r\n{\r\nunsigned int cpu = (unsigned long)hcpu;\r\nstruct ehca_cpu_comp_task *cct;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_UP_PREPARE_FROZEN:\r\nehca_gen_dbg("CPU: %x (CPU_PREPARE)", cpu);\r\nif (!create_comp_task(pool, cpu)) {\r\nehca_gen_err("Can't create comp_task for cpu: %x", cpu);\r\nreturn notifier_from_errno(-ENOMEM);\r\n}\r\nbreak;\r\ncase CPU_UP_CANCELED:\r\ncase CPU_UP_CANCELED_FROZEN:\r\nehca_gen_dbg("CPU: %x (CPU_CANCELED)", cpu);\r\ncct = per_cpu_ptr(pool->cpu_comp_tasks, cpu);\r\nkthread_bind(cct->task, cpumask_any(cpu_online_mask));\r\ndestroy_comp_task(pool, cpu);\r\nbreak;\r\ncase CPU_ONLINE:\r\ncase CPU_ONLINE_FROZEN:\r\nehca_gen_dbg("CPU: %x (CPU_ONLINE)", cpu);\r\ncct = per_cpu_ptr(pool->cpu_comp_tasks, cpu);\r\nkthread_bind(cct->task, cpu);\r\nwake_up_process(cct->task);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\ncase CPU_DOWN_PREPARE_FROZEN:\r\nehca_gen_dbg("CPU: %x (CPU_DOWN_PREPARE)", cpu);\r\nbreak;\r\ncase CPU_DOWN_FAILED:\r\ncase CPU_DOWN_FAILED_FROZEN:\r\nehca_gen_dbg("CPU: %x (CPU_DOWN_FAILED)", cpu);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\nehca_gen_dbg("CPU: %x (CPU_DEAD)", cpu);\r\ndestroy_comp_task(pool, cpu);\r\ntake_over_work(pool, cpu);\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nint ehca_create_comp_pool(void)\r\n{\r\nint cpu;\r\nstruct task_struct *task;\r\nif (!ehca_scaling_code)\r\nreturn 0;\r\npool = kzalloc(sizeof(struct ehca_comp_pool), GFP_KERNEL);\r\nif (pool == NULL)\r\nreturn -ENOMEM;\r\nspin_lock_init(&pool->last_cpu_lock);\r\npool->last_cpu = cpumask_any(cpu_online_mask);\r\npool->cpu_comp_tasks = alloc_percpu(struct ehca_cpu_comp_task);\r\nif (pool->cpu_comp_tasks == NULL) {\r\nkfree(pool);\r\nreturn -EINVAL;\r\n}\r\nfor_each_online_cpu(cpu) {\r\ntask = create_comp_task(pool, cpu);\r\nif (task) {\r\nkthread_bind(task, cpu);\r\nwake_up_process(task);\r\n}\r\n}\r\nregister_hotcpu_notifier(&comp_pool_callback_nb);\r\nprintk(KERN_INFO "eHCA scaling code enabled\n");\r\nreturn 0;\r\n}\r\nvoid ehca_destroy_comp_pool(void)\r\n{\r\nint i;\r\nif (!ehca_scaling_code)\r\nreturn;\r\nunregister_hotcpu_notifier(&comp_pool_callback_nb);\r\nfor_each_online_cpu(i)\r\ndestroy_comp_task(pool, i);\r\nfree_percpu(pool->cpu_comp_tasks);\r\nkfree(pool);\r\n}
