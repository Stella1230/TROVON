void kvmppc_mmu_invalidate_pte(struct kvm_vcpu *vcpu, struct hpte_cache *pte)\r\n{\r\nppc_md.hpte_invalidate(pte->slot, pte->host_va,\r\nMMU_PAGE_4K, MMU_SEGSIZE_256M,\r\nfalse);\r\n}\r\nstatic u16 kvmppc_sid_hash(struct kvm_vcpu *vcpu, u64 gvsid)\r\n{\r\nreturn (u16)(((gvsid >> (SID_MAP_BITS * 7)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 6)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 5)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 4)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 3)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 2)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 1)) & SID_MAP_MASK) ^\r\n((gvsid >> (SID_MAP_BITS * 0)) & SID_MAP_MASK));\r\n}\r\nstatic struct kvmppc_sid_map *find_sid_vsid(struct kvm_vcpu *vcpu, u64 gvsid)\r\n{\r\nstruct kvmppc_sid_map *map;\r\nu16 sid_map_mask;\r\nif (vcpu->arch.shared->msr & MSR_PR)\r\ngvsid |= VSID_PR;\r\nsid_map_mask = kvmppc_sid_hash(vcpu, gvsid);\r\nmap = &to_book3s(vcpu)->sid_map[sid_map_mask];\r\nif (map->valid && (map->guest_vsid == gvsid)) {\r\ntrace_kvm_book3s_slb_found(gvsid, map->host_vsid);\r\nreturn map;\r\n}\r\nmap = &to_book3s(vcpu)->sid_map[SID_MAP_MASK - sid_map_mask];\r\nif (map->valid && (map->guest_vsid == gvsid)) {\r\ntrace_kvm_book3s_slb_found(gvsid, map->host_vsid);\r\nreturn map;\r\n}\r\ntrace_kvm_book3s_slb_fail(sid_map_mask, gvsid);\r\nreturn NULL;\r\n}\r\nint kvmppc_mmu_map_page(struct kvm_vcpu *vcpu, struct kvmppc_pte *orig_pte)\r\n{\r\npfn_t hpaddr;\r\nulong hash, hpteg, va;\r\nu64 vsid;\r\nint ret;\r\nint rflags = 0x192;\r\nint vflags = 0;\r\nint attempt = 0;\r\nstruct kvmppc_sid_map *map;\r\nint r = 0;\r\nhpaddr = kvmppc_gfn_to_pfn(vcpu, orig_pte->raddr >> PAGE_SHIFT);\r\nif (is_error_pfn(hpaddr)) {\r\nprintk(KERN_INFO "Couldn't get guest page for gfn %lx!\n", orig_pte->eaddr);\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nhpaddr <<= PAGE_SHIFT;\r\nhpaddr |= orig_pte->raddr & (~0xfffULL & ~PAGE_MASK);\r\nvcpu->arch.mmu.esid_to_vsid(vcpu, orig_pte->eaddr >> SID_SHIFT, &vsid);\r\nmap = find_sid_vsid(vcpu, vsid);\r\nif (!map) {\r\nret = kvmppc_mmu_map_segment(vcpu, orig_pte->eaddr);\r\nWARN_ON(ret < 0);\r\nmap = find_sid_vsid(vcpu, vsid);\r\n}\r\nif (!map) {\r\nprintk(KERN_ERR "KVM: Segment map for 0x%llx (0x%lx) failed\n",\r\nvsid, orig_pte->eaddr);\r\nWARN_ON(true);\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nvsid = map->host_vsid;\r\nva = hpt_va(orig_pte->eaddr, vsid, MMU_SEGSIZE_256M);\r\nif (!orig_pte->may_write)\r\nrflags |= HPTE_R_PP;\r\nelse\r\nmark_page_dirty(vcpu->kvm, orig_pte->raddr >> PAGE_SHIFT);\r\nif (!orig_pte->may_execute)\r\nrflags |= HPTE_R_N;\r\nhash = hpt_hash(va, PTE_SIZE, MMU_SEGSIZE_256M);\r\nmap_again:\r\nhpteg = ((hash & htab_hash_mask) * HPTES_PER_GROUP);\r\nif (attempt > 1)\r\nif (ppc_md.hpte_remove(hpteg) < 0) {\r\nr = -1;\r\ngoto out;\r\n}\r\nret = ppc_md.hpte_insert(hpteg, va, hpaddr, rflags, vflags, MMU_PAGE_4K, MMU_SEGSIZE_256M);\r\nif (ret < 0) {\r\nhash = ~hash;\r\nvflags ^= HPTE_V_SECONDARY;\r\nattempt++;\r\ngoto map_again;\r\n} else {\r\nstruct hpte_cache *pte = kvmppc_mmu_hpte_cache_next(vcpu);\r\ntrace_kvm_book3s_64_mmu_map(rflags, hpteg, va, hpaddr, orig_pte);\r\nif ((ret & _PTEIDX_SECONDARY) && !(vflags & HPTE_V_SECONDARY)) {\r\nhash = ~hash;\r\nhpteg = ((hash & htab_hash_mask) * HPTES_PER_GROUP);\r\n}\r\npte->slot = hpteg + (ret & 7);\r\npte->host_va = va;\r\npte->pte = *orig_pte;\r\npte->pfn = hpaddr >> PAGE_SHIFT;\r\nkvmppc_mmu_hpte_cache_map(vcpu, pte);\r\n}\r\nout:\r\nreturn r;\r\n}\r\nstatic struct kvmppc_sid_map *create_sid_map(struct kvm_vcpu *vcpu, u64 gvsid)\r\n{\r\nstruct kvmppc_sid_map *map;\r\nstruct kvmppc_vcpu_book3s *vcpu_book3s = to_book3s(vcpu);\r\nu16 sid_map_mask;\r\nstatic int backwards_map = 0;\r\nif (vcpu->arch.shared->msr & MSR_PR)\r\ngvsid |= VSID_PR;\r\nsid_map_mask = kvmppc_sid_hash(vcpu, gvsid);\r\nif (backwards_map)\r\nsid_map_mask = SID_MAP_MASK - sid_map_mask;\r\nmap = &to_book3s(vcpu)->sid_map[sid_map_mask];\r\nbackwards_map = !backwards_map;\r\nif (vcpu_book3s->proto_vsid_next == vcpu_book3s->proto_vsid_max) {\r\nvcpu_book3s->proto_vsid_next = vcpu_book3s->proto_vsid_first;\r\nmemset(vcpu_book3s->sid_map, 0,\r\nsizeof(struct kvmppc_sid_map) * SID_MAP_NUM);\r\nkvmppc_mmu_pte_flush(vcpu, 0, 0);\r\nkvmppc_mmu_flush_segments(vcpu);\r\n}\r\nmap->host_vsid = vsid_scramble(vcpu_book3s->proto_vsid_next++, 256M);\r\nmap->guest_vsid = gvsid;\r\nmap->valid = true;\r\ntrace_kvm_book3s_slb_map(sid_map_mask, gvsid, map->host_vsid);\r\nreturn map;\r\n}\r\nstatic int kvmppc_mmu_next_segment(struct kvm_vcpu *vcpu, ulong esid)\r\n{\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nint i;\r\nint max_slb_size = 64;\r\nint found_inval = -1;\r\nint r;\r\nif (!svcpu->slb_max)\r\nsvcpu->slb_max = 1;\r\nfor (i = 1; i < svcpu->slb_max; i++) {\r\nif (!(svcpu->slb[i].esid & SLB_ESID_V))\r\nfound_inval = i;\r\nelse if ((svcpu->slb[i].esid & ESID_MASK) == esid) {\r\nr = i;\r\ngoto out;\r\n}\r\n}\r\nif (found_inval > 0) {\r\nr = found_inval;\r\ngoto out;\r\n}\r\nif (mmu_slb_size < 64)\r\nmax_slb_size = mmu_slb_size;\r\nif ((svcpu->slb_max) == max_slb_size)\r\nkvmppc_mmu_flush_segments(vcpu);\r\nr = svcpu->slb_max;\r\nsvcpu->slb_max++;\r\nout:\r\nsvcpu_put(svcpu);\r\nreturn r;\r\n}\r\nint kvmppc_mmu_map_segment(struct kvm_vcpu *vcpu, ulong eaddr)\r\n{\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nu64 esid = eaddr >> SID_SHIFT;\r\nu64 slb_esid = (eaddr & ESID_MASK) | SLB_ESID_V;\r\nu64 slb_vsid = SLB_VSID_USER;\r\nu64 gvsid;\r\nint slb_index;\r\nstruct kvmppc_sid_map *map;\r\nint r = 0;\r\nslb_index = kvmppc_mmu_next_segment(vcpu, eaddr & ESID_MASK);\r\nif (vcpu->arch.mmu.esid_to_vsid(vcpu, esid, &gvsid)) {\r\nsvcpu->slb[slb_index].esid = 0;\r\nr = -ENOENT;\r\ngoto out;\r\n}\r\nmap = find_sid_vsid(vcpu, gvsid);\r\nif (!map)\r\nmap = create_sid_map(vcpu, gvsid);\r\nmap->guest_esid = esid;\r\nslb_vsid |= (map->host_vsid << 12);\r\nslb_vsid &= ~SLB_VSID_KP;\r\nslb_esid |= slb_index;\r\nsvcpu->slb[slb_index].esid = slb_esid;\r\nsvcpu->slb[slb_index].vsid = slb_vsid;\r\ntrace_kvm_book3s_slbmte(slb_vsid, slb_esid);\r\nout:\r\nsvcpu_put(svcpu);\r\nreturn r;\r\n}\r\nvoid kvmppc_mmu_flush_segments(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_book3s_shadow_vcpu *svcpu = svcpu_get(vcpu);\r\nsvcpu->slb_max = 1;\r\nsvcpu->slb[0].esid = 0;\r\nsvcpu_put(svcpu);\r\n}\r\nvoid kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_mmu_hpte_destroy(vcpu);\r\n__destroy_context(to_book3s(vcpu)->context_id[0]);\r\n}\r\nint kvmppc_mmu_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nint err;\r\nerr = __init_new_context();\r\nif (err < 0)\r\nreturn -1;\r\nvcpu3s->context_id[0] = err;\r\nvcpu3s->proto_vsid_max = ((vcpu3s->context_id[0] + 1)\r\n<< USER_ESID_BITS) - 1;\r\nvcpu3s->proto_vsid_first = vcpu3s->context_id[0] << USER_ESID_BITS;\r\nvcpu3s->proto_vsid_next = vcpu3s->proto_vsid_first;\r\nkvmppc_mmu_hpte_init(vcpu);\r\nreturn 0;\r\n}
