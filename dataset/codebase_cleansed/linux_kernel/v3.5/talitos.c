static void to_talitos_ptr(struct talitos_ptr *talitos_ptr, dma_addr_t dma_addr)\r\n{\r\ntalitos_ptr->ptr = cpu_to_be32(lower_32_bits(dma_addr));\r\ntalitos_ptr->eptr = upper_32_bits(dma_addr);\r\n}\r\nstatic void map_single_talitos_ptr(struct device *dev,\r\nstruct talitos_ptr *talitos_ptr,\r\nunsigned short len, void *data,\r\nunsigned char extent,\r\nenum dma_data_direction dir)\r\n{\r\ndma_addr_t dma_addr = dma_map_single(dev, data, len, dir);\r\ntalitos_ptr->len = cpu_to_be16(len);\r\nto_talitos_ptr(talitos_ptr, dma_addr);\r\ntalitos_ptr->j_extent = extent;\r\n}\r\nstatic void unmap_single_talitos_ptr(struct device *dev,\r\nstruct talitos_ptr *talitos_ptr,\r\nenum dma_data_direction dir)\r\n{\r\ndma_unmap_single(dev, be32_to_cpu(talitos_ptr->ptr),\r\nbe16_to_cpu(talitos_ptr->len), dir);\r\n}\r\nstatic int reset_channel(struct device *dev, int ch)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nunsigned int timeout = TALITOS_TIMEOUT;\r\nsetbits32(priv->chan[ch].reg + TALITOS_CCCR, TALITOS_CCCR_RESET);\r\nwhile ((in_be32(priv->chan[ch].reg + TALITOS_CCCR) & TALITOS_CCCR_RESET)\r\n&& --timeout)\r\ncpu_relax();\r\nif (timeout == 0) {\r\ndev_err(dev, "failed to reset channel %d\n", ch);\r\nreturn -EIO;\r\n}\r\nsetbits32(priv->chan[ch].reg + TALITOS_CCCR_LO, TALITOS_CCCR_LO_EAE |\r\nTALITOS_CCCR_LO_CDWE | TALITOS_CCCR_LO_CDIE);\r\nif (priv->features & TALITOS_FTR_HW_AUTH_CHECK)\r\nsetbits32(priv->chan[ch].reg + TALITOS_CCCR_LO,\r\nTALITOS_CCCR_LO_IWSE);\r\nreturn 0;\r\n}\r\nstatic int reset_device(struct device *dev)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nunsigned int timeout = TALITOS_TIMEOUT;\r\nu32 mcr = TALITOS_MCR_SWR;\r\nsetbits32(priv->reg + TALITOS_MCR, mcr);\r\nwhile ((in_be32(priv->reg + TALITOS_MCR) & TALITOS_MCR_SWR)\r\n&& --timeout)\r\ncpu_relax();\r\nif (priv->irq[1]) {\r\nmcr = TALITOS_MCR_RCA1 | TALITOS_MCR_RCA3;\r\nsetbits32(priv->reg + TALITOS_MCR, mcr);\r\n}\r\nif (timeout == 0) {\r\ndev_err(dev, "failed to reset device\n");\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int init_device(struct device *dev)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nint ch, err;\r\nerr = reset_device(dev);\r\nif (err)\r\nreturn err;\r\nerr = reset_device(dev);\r\nif (err)\r\nreturn err;\r\nfor (ch = 0; ch < priv->num_channels; ch++) {\r\nerr = reset_channel(dev, ch);\r\nif (err)\r\nreturn err;\r\n}\r\nsetbits32(priv->reg + TALITOS_IMR, TALITOS_IMR_INIT);\r\nsetbits32(priv->reg + TALITOS_IMR_LO, TALITOS_IMR_LO_INIT);\r\nif (priv->features & TALITOS_FTR_HW_AUTH_CHECK)\r\nsetbits32(priv->reg + TALITOS_MDEUICR_LO,\r\nTALITOS_MDEUICR_LO_ICE);\r\nreturn 0;\r\n}\r\nstatic int talitos_submit(struct device *dev, int ch, struct talitos_desc *desc,\r\nvoid (*callback)(struct device *dev,\r\nstruct talitos_desc *desc,\r\nvoid *context, int error),\r\nvoid *context)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nstruct talitos_request *request;\r\nunsigned long flags;\r\nint head;\r\nspin_lock_irqsave(&priv->chan[ch].head_lock, flags);\r\nif (!atomic_inc_not_zero(&priv->chan[ch].submit_count)) {\r\nspin_unlock_irqrestore(&priv->chan[ch].head_lock, flags);\r\nreturn -EAGAIN;\r\n}\r\nhead = priv->chan[ch].head;\r\nrequest = &priv->chan[ch].fifo[head];\r\nrequest->dma_desc = dma_map_single(dev, desc, sizeof(*desc),\r\nDMA_BIDIRECTIONAL);\r\nrequest->callback = callback;\r\nrequest->context = context;\r\npriv->chan[ch].head = (priv->chan[ch].head + 1) & (priv->fifo_len - 1);\r\nsmp_wmb();\r\nrequest->desc = desc;\r\nwmb();\r\nout_be32(priv->chan[ch].reg + TALITOS_FF,\r\nupper_32_bits(request->dma_desc));\r\nout_be32(priv->chan[ch].reg + TALITOS_FF_LO,\r\nlower_32_bits(request->dma_desc));\r\nspin_unlock_irqrestore(&priv->chan[ch].head_lock, flags);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic void flush_channel(struct device *dev, int ch, int error, int reset_ch)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nstruct talitos_request *request, saved_req;\r\nunsigned long flags;\r\nint tail, status;\r\nspin_lock_irqsave(&priv->chan[ch].tail_lock, flags);\r\ntail = priv->chan[ch].tail;\r\nwhile (priv->chan[ch].fifo[tail].desc) {\r\nrequest = &priv->chan[ch].fifo[tail];\r\nrmb();\r\nif ((request->desc->hdr & DESC_HDR_DONE) == DESC_HDR_DONE)\r\nstatus = 0;\r\nelse\r\nif (!error)\r\nbreak;\r\nelse\r\nstatus = error;\r\ndma_unmap_single(dev, request->dma_desc,\r\nsizeof(struct talitos_desc),\r\nDMA_BIDIRECTIONAL);\r\nsaved_req.desc = request->desc;\r\nsaved_req.callback = request->callback;\r\nsaved_req.context = request->context;\r\nsmp_wmb();\r\nrequest->desc = NULL;\r\npriv->chan[ch].tail = (tail + 1) & (priv->fifo_len - 1);\r\nspin_unlock_irqrestore(&priv->chan[ch].tail_lock, flags);\r\natomic_dec(&priv->chan[ch].submit_count);\r\nsaved_req.callback(dev, saved_req.desc, saved_req.context,\r\nstatus);\r\nif (error && !reset_ch && status == error)\r\nreturn;\r\nspin_lock_irqsave(&priv->chan[ch].tail_lock, flags);\r\ntail = priv->chan[ch].tail;\r\n}\r\nspin_unlock_irqrestore(&priv->chan[ch].tail_lock, flags);\r\n}\r\nstatic u32 current_desc_hdr(struct device *dev, int ch)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nint tail = priv->chan[ch].tail;\r\ndma_addr_t cur_desc;\r\ncur_desc = in_be32(priv->chan[ch].reg + TALITOS_CDPR_LO);\r\nwhile (priv->chan[ch].fifo[tail].dma_desc != cur_desc) {\r\ntail = (tail + 1) & (priv->fifo_len - 1);\r\nif (tail == priv->chan[ch].tail) {\r\ndev_err(dev, "couldn't locate current descriptor\n");\r\nreturn 0;\r\n}\r\n}\r\nreturn priv->chan[ch].fifo[tail].desc->hdr;\r\n}\r\nstatic void report_eu_error(struct device *dev, int ch, u32 desc_hdr)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nint i;\r\nif (!desc_hdr)\r\ndesc_hdr = in_be32(priv->chan[ch].reg + TALITOS_DESCBUF);\r\nswitch (desc_hdr & DESC_HDR_SEL0_MASK) {\r\ncase DESC_HDR_SEL0_AFEU:\r\ndev_err(dev, "AFEUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_AFEUISR),\r\nin_be32(priv->reg + TALITOS_AFEUISR_LO));\r\nbreak;\r\ncase DESC_HDR_SEL0_DEU:\r\ndev_err(dev, "DEUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_DEUISR),\r\nin_be32(priv->reg + TALITOS_DEUISR_LO));\r\nbreak;\r\ncase DESC_HDR_SEL0_MDEUA:\r\ncase DESC_HDR_SEL0_MDEUB:\r\ndev_err(dev, "MDEUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_MDEUISR),\r\nin_be32(priv->reg + TALITOS_MDEUISR_LO));\r\nbreak;\r\ncase DESC_HDR_SEL0_RNG:\r\ndev_err(dev, "RNGUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_RNGUISR),\r\nin_be32(priv->reg + TALITOS_RNGUISR_LO));\r\nbreak;\r\ncase DESC_HDR_SEL0_PKEU:\r\ndev_err(dev, "PKEUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_PKEUISR),\r\nin_be32(priv->reg + TALITOS_PKEUISR_LO));\r\nbreak;\r\ncase DESC_HDR_SEL0_AESU:\r\ndev_err(dev, "AESUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_AESUISR),\r\nin_be32(priv->reg + TALITOS_AESUISR_LO));\r\nbreak;\r\ncase DESC_HDR_SEL0_CRCU:\r\ndev_err(dev, "CRCUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_CRCUISR),\r\nin_be32(priv->reg + TALITOS_CRCUISR_LO));\r\nbreak;\r\ncase DESC_HDR_SEL0_KEU:\r\ndev_err(dev, "KEUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_KEUISR),\r\nin_be32(priv->reg + TALITOS_KEUISR_LO));\r\nbreak;\r\n}\r\nswitch (desc_hdr & DESC_HDR_SEL1_MASK) {\r\ncase DESC_HDR_SEL1_MDEUA:\r\ncase DESC_HDR_SEL1_MDEUB:\r\ndev_err(dev, "MDEUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_MDEUISR),\r\nin_be32(priv->reg + TALITOS_MDEUISR_LO));\r\nbreak;\r\ncase DESC_HDR_SEL1_CRCU:\r\ndev_err(dev, "CRCUISR 0x%08x_%08x\n",\r\nin_be32(priv->reg + TALITOS_CRCUISR),\r\nin_be32(priv->reg + TALITOS_CRCUISR_LO));\r\nbreak;\r\n}\r\nfor (i = 0; i < 8; i++)\r\ndev_err(dev, "DESCBUF 0x%08x_%08x\n",\r\nin_be32(priv->chan[ch].reg + TALITOS_DESCBUF + 8*i),\r\nin_be32(priv->chan[ch].reg + TALITOS_DESCBUF_LO + 8*i));\r\n}\r\nstatic void talitos_error(struct device *dev, u32 isr, u32 isr_lo)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nunsigned int timeout = TALITOS_TIMEOUT;\r\nint ch, error, reset_dev = 0, reset_ch = 0;\r\nu32 v, v_lo;\r\nfor (ch = 0; ch < priv->num_channels; ch++) {\r\nif (!(isr & (1 << (ch * 2 + 1))))\r\ncontinue;\r\nerror = -EINVAL;\r\nv = in_be32(priv->chan[ch].reg + TALITOS_CCPSR);\r\nv_lo = in_be32(priv->chan[ch].reg + TALITOS_CCPSR_LO);\r\nif (v_lo & TALITOS_CCPSR_LO_DOF) {\r\ndev_err(dev, "double fetch fifo overflow error\n");\r\nerror = -EAGAIN;\r\nreset_ch = 1;\r\n}\r\nif (v_lo & TALITOS_CCPSR_LO_SOF) {\r\ndev_err(dev, "single fetch fifo overflow error\n");\r\nerror = -EAGAIN;\r\n}\r\nif (v_lo & TALITOS_CCPSR_LO_MDTE)\r\ndev_err(dev, "master data transfer error\n");\r\nif (v_lo & TALITOS_CCPSR_LO_SGDLZ)\r\ndev_err(dev, "s/g data length zero error\n");\r\nif (v_lo & TALITOS_CCPSR_LO_FPZ)\r\ndev_err(dev, "fetch pointer zero error\n");\r\nif (v_lo & TALITOS_CCPSR_LO_IDH)\r\ndev_err(dev, "illegal descriptor header error\n");\r\nif (v_lo & TALITOS_CCPSR_LO_IEU)\r\ndev_err(dev, "invalid execution unit error\n");\r\nif (v_lo & TALITOS_CCPSR_LO_EU)\r\nreport_eu_error(dev, ch, current_desc_hdr(dev, ch));\r\nif (v_lo & TALITOS_CCPSR_LO_GB)\r\ndev_err(dev, "gather boundary error\n");\r\nif (v_lo & TALITOS_CCPSR_LO_GRL)\r\ndev_err(dev, "gather return/length error\n");\r\nif (v_lo & TALITOS_CCPSR_LO_SB)\r\ndev_err(dev, "scatter boundary error\n");\r\nif (v_lo & TALITOS_CCPSR_LO_SRL)\r\ndev_err(dev, "scatter return/length error\n");\r\nflush_channel(dev, ch, error, reset_ch);\r\nif (reset_ch) {\r\nreset_channel(dev, ch);\r\n} else {\r\nsetbits32(priv->chan[ch].reg + TALITOS_CCCR,\r\nTALITOS_CCCR_CONT);\r\nsetbits32(priv->chan[ch].reg + TALITOS_CCCR_LO, 0);\r\nwhile ((in_be32(priv->chan[ch].reg + TALITOS_CCCR) &\r\nTALITOS_CCCR_CONT) && --timeout)\r\ncpu_relax();\r\nif (timeout == 0) {\r\ndev_err(dev, "failed to restart channel %d\n",\r\nch);\r\nreset_dev = 1;\r\n}\r\n}\r\n}\r\nif (reset_dev || isr & ~TALITOS_ISR_4CHERR || isr_lo) {\r\ndev_err(dev, "done overflow, internal time out, or rngu error: "\r\n"ISR 0x%08x_%08x\n", isr, isr_lo);\r\nfor (ch = 0; ch < priv->num_channels; ch++)\r\nflush_channel(dev, ch, -EIO, 1);\r\ninit_device(dev);\r\n}\r\n}\r\nstatic int talitos_rng_data_present(struct hwrng *rng, int wait)\r\n{\r\nstruct device *dev = (struct device *)rng->priv;\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nu32 ofl;\r\nint i;\r\nfor (i = 0; i < 20; i++) {\r\nofl = in_be32(priv->reg + TALITOS_RNGUSR_LO) &\r\nTALITOS_RNGUSR_LO_OFL;\r\nif (ofl || !wait)\r\nbreak;\r\nudelay(10);\r\n}\r\nreturn !!ofl;\r\n}\r\nstatic int talitos_rng_data_read(struct hwrng *rng, u32 *data)\r\n{\r\nstruct device *dev = (struct device *)rng->priv;\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\n*data = in_be32(priv->reg + TALITOS_RNGU_FIFO);\r\n*data = in_be32(priv->reg + TALITOS_RNGU_FIFO_LO);\r\nreturn sizeof(u32);\r\n}\r\nstatic int talitos_rng_init(struct hwrng *rng)\r\n{\r\nstruct device *dev = (struct device *)rng->priv;\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nunsigned int timeout = TALITOS_TIMEOUT;\r\nsetbits32(priv->reg + TALITOS_RNGURCR_LO, TALITOS_RNGURCR_LO_SR);\r\nwhile (!(in_be32(priv->reg + TALITOS_RNGUSR_LO) & TALITOS_RNGUSR_LO_RD)\r\n&& --timeout)\r\ncpu_relax();\r\nif (timeout == 0) {\r\ndev_err(dev, "failed to reset rng hw\n");\r\nreturn -ENODEV;\r\n}\r\nsetbits32(priv->reg + TALITOS_RNGUDSR_LO, 0);\r\nreturn 0;\r\n}\r\nstatic int talitos_register_rng(struct device *dev)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\npriv->rng.name = dev_driver_string(dev),\r\npriv->rng.init = talitos_rng_init,\r\npriv->rng.data_present = talitos_rng_data_present,\r\npriv->rng.data_read = talitos_rng_data_read,\r\npriv->rng.priv = (unsigned long)dev;\r\nreturn hwrng_register(&priv->rng);\r\n}\r\nstatic void talitos_unregister_rng(struct device *dev)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nhwrng_unregister(&priv->rng);\r\n}\r\nstatic int aead_setauthsize(struct crypto_aead *authenc,\r\nunsigned int authsize)\r\n{\r\nstruct talitos_ctx *ctx = crypto_aead_ctx(authenc);\r\nctx->authsize = authsize;\r\nreturn 0;\r\n}\r\nstatic int aead_setkey(struct crypto_aead *authenc,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct talitos_ctx *ctx = crypto_aead_ctx(authenc);\r\nstruct rtattr *rta = (void *)key;\r\nstruct crypto_authenc_key_param *param;\r\nunsigned int authkeylen;\r\nunsigned int enckeylen;\r\nif (!RTA_OK(rta, keylen))\r\ngoto badkey;\r\nif (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)\r\ngoto badkey;\r\nif (RTA_PAYLOAD(rta) < sizeof(*param))\r\ngoto badkey;\r\nparam = RTA_DATA(rta);\r\nenckeylen = be32_to_cpu(param->enckeylen);\r\nkey += RTA_ALIGN(rta->rta_len);\r\nkeylen -= RTA_ALIGN(rta->rta_len);\r\nif (keylen < enckeylen)\r\ngoto badkey;\r\nauthkeylen = keylen - enckeylen;\r\nif (keylen > TALITOS_MAX_KEY_SIZE)\r\ngoto badkey;\r\nmemcpy(&ctx->key, key, keylen);\r\nctx->keylen = keylen;\r\nctx->enckeylen = enckeylen;\r\nctx->authkeylen = authkeylen;\r\nreturn 0;\r\nbadkey:\r\ncrypto_aead_set_flags(authenc, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nstatic int talitos_map_sg(struct device *dev, struct scatterlist *sg,\r\nunsigned int nents, enum dma_data_direction dir,\r\nint chained)\r\n{\r\nif (unlikely(chained))\r\nwhile (sg) {\r\ndma_map_sg(dev, sg, 1, dir);\r\nsg = scatterwalk_sg_next(sg);\r\n}\r\nelse\r\ndma_map_sg(dev, sg, nents, dir);\r\nreturn nents;\r\n}\r\nstatic void talitos_unmap_sg_chain(struct device *dev, struct scatterlist *sg,\r\nenum dma_data_direction dir)\r\n{\r\nwhile (sg) {\r\ndma_unmap_sg(dev, sg, 1, dir);\r\nsg = scatterwalk_sg_next(sg);\r\n}\r\n}\r\nstatic void talitos_sg_unmap(struct device *dev,\r\nstruct talitos_edesc *edesc,\r\nstruct scatterlist *src,\r\nstruct scatterlist *dst)\r\n{\r\nunsigned int src_nents = edesc->src_nents ? : 1;\r\nunsigned int dst_nents = edesc->dst_nents ? : 1;\r\nif (src != dst) {\r\nif (edesc->src_is_chained)\r\ntalitos_unmap_sg_chain(dev, src, DMA_TO_DEVICE);\r\nelse\r\ndma_unmap_sg(dev, src, src_nents, DMA_TO_DEVICE);\r\nif (dst) {\r\nif (edesc->dst_is_chained)\r\ntalitos_unmap_sg_chain(dev, dst,\r\nDMA_FROM_DEVICE);\r\nelse\r\ndma_unmap_sg(dev, dst, dst_nents,\r\nDMA_FROM_DEVICE);\r\n}\r\n} else\r\nif (edesc->src_is_chained)\r\ntalitos_unmap_sg_chain(dev, src, DMA_BIDIRECTIONAL);\r\nelse\r\ndma_unmap_sg(dev, src, src_nents, DMA_BIDIRECTIONAL);\r\n}\r\nstatic void ipsec_esp_unmap(struct device *dev,\r\nstruct talitos_edesc *edesc,\r\nstruct aead_request *areq)\r\n{\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[6], DMA_FROM_DEVICE);\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[3], DMA_TO_DEVICE);\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[2], DMA_TO_DEVICE);\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[0], DMA_TO_DEVICE);\r\ndma_unmap_sg(dev, areq->assoc, 1, DMA_TO_DEVICE);\r\ntalitos_sg_unmap(dev, edesc, areq->src, areq->dst);\r\nif (edesc->dma_len)\r\ndma_unmap_single(dev, edesc->dma_link_tbl, edesc->dma_len,\r\nDMA_BIDIRECTIONAL);\r\n}\r\nstatic void ipsec_esp_encrypt_done(struct device *dev,\r\nstruct talitos_desc *desc, void *context,\r\nint err)\r\n{\r\nstruct aead_request *areq = context;\r\nstruct crypto_aead *authenc = crypto_aead_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_aead_ctx(authenc);\r\nstruct talitos_edesc *edesc;\r\nstruct scatterlist *sg;\r\nvoid *icvdata;\r\nedesc = container_of(desc, struct talitos_edesc, desc);\r\nipsec_esp_unmap(dev, edesc, areq);\r\nif (edesc->dma_len) {\r\nicvdata = &edesc->link_tbl[edesc->src_nents +\r\nedesc->dst_nents + 2];\r\nsg = sg_last(areq->dst, edesc->dst_nents);\r\nmemcpy((char *)sg_virt(sg) + sg->length - ctx->authsize,\r\nicvdata, ctx->authsize);\r\n}\r\nkfree(edesc);\r\naead_request_complete(areq, err);\r\n}\r\nstatic void ipsec_esp_decrypt_swauth_done(struct device *dev,\r\nstruct talitos_desc *desc,\r\nvoid *context, int err)\r\n{\r\nstruct aead_request *req = context;\r\nstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\r\nstruct talitos_ctx *ctx = crypto_aead_ctx(authenc);\r\nstruct talitos_edesc *edesc;\r\nstruct scatterlist *sg;\r\nvoid *icvdata;\r\nedesc = container_of(desc, struct talitos_edesc, desc);\r\nipsec_esp_unmap(dev, edesc, req);\r\nif (!err) {\r\nif (edesc->dma_len)\r\nicvdata = &edesc->link_tbl[edesc->src_nents +\r\nedesc->dst_nents + 2];\r\nelse\r\nicvdata = &edesc->link_tbl[0];\r\nsg = sg_last(req->dst, edesc->dst_nents ? : 1);\r\nerr = memcmp(icvdata, (char *)sg_virt(sg) + sg->length -\r\nctx->authsize, ctx->authsize) ? -EBADMSG : 0;\r\n}\r\nkfree(edesc);\r\naead_request_complete(req, err);\r\n}\r\nstatic void ipsec_esp_decrypt_hwauth_done(struct device *dev,\r\nstruct talitos_desc *desc,\r\nvoid *context, int err)\r\n{\r\nstruct aead_request *req = context;\r\nstruct talitos_edesc *edesc;\r\nedesc = container_of(desc, struct talitos_edesc, desc);\r\nipsec_esp_unmap(dev, edesc, req);\r\nif (!err && ((desc->hdr_lo & DESC_HDR_LO_ICCR1_MASK) !=\r\nDESC_HDR_LO_ICCR1_PASS))\r\nerr = -EBADMSG;\r\nkfree(edesc);\r\naead_request_complete(req, err);\r\n}\r\nstatic int sg_to_link_tbl(struct scatterlist *sg, int sg_count,\r\nint cryptlen, struct talitos_ptr *link_tbl_ptr)\r\n{\r\nint n_sg = sg_count;\r\nwhile (n_sg--) {\r\nto_talitos_ptr(link_tbl_ptr, sg_dma_address(sg));\r\nlink_tbl_ptr->len = cpu_to_be16(sg_dma_len(sg));\r\nlink_tbl_ptr->j_extent = 0;\r\nlink_tbl_ptr++;\r\ncryptlen -= sg_dma_len(sg);\r\nsg = scatterwalk_sg_next(sg);\r\n}\r\nlink_tbl_ptr--;\r\nwhile (be16_to_cpu(link_tbl_ptr->len) <= (-cryptlen)) {\r\ncryptlen += be16_to_cpu(link_tbl_ptr->len);\r\nlink_tbl_ptr->len = 0;\r\nsg_count--;\r\nlink_tbl_ptr--;\r\n}\r\nlink_tbl_ptr->len = cpu_to_be16(be16_to_cpu(link_tbl_ptr->len)\r\n+ cryptlen);\r\nlink_tbl_ptr->j_extent = DESC_PTR_LNKTBL_RETURN;\r\nreturn sg_count;\r\n}\r\nstatic int ipsec_esp(struct talitos_edesc *edesc, struct aead_request *areq,\r\nu8 *giv, u64 seq,\r\nvoid (*callback) (struct device *dev,\r\nstruct talitos_desc *desc,\r\nvoid *context, int error))\r\n{\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_aead_ctx(aead);\r\nstruct device *dev = ctx->dev;\r\nstruct talitos_desc *desc = &edesc->desc;\r\nunsigned int cryptlen = areq->cryptlen;\r\nunsigned int authsize = ctx->authsize;\r\nunsigned int ivsize = crypto_aead_ivsize(aead);\r\nint sg_count, ret;\r\nint sg_link_tbl_len;\r\nmap_single_talitos_ptr(dev, &desc->ptr[0], ctx->authkeylen, &ctx->key,\r\n0, DMA_TO_DEVICE);\r\nmap_single_talitos_ptr(dev, &desc->ptr[1], areq->assoclen + ivsize,\r\nsg_virt(areq->assoc), 0, DMA_TO_DEVICE);\r\nmap_single_talitos_ptr(dev, &desc->ptr[2], ivsize, giv ?: areq->iv, 0,\r\nDMA_TO_DEVICE);\r\nmap_single_talitos_ptr(dev, &desc->ptr[3], ctx->enckeylen,\r\n(char *)&ctx->key + ctx->authkeylen, 0,\r\nDMA_TO_DEVICE);\r\ndesc->ptr[4].len = cpu_to_be16(cryptlen);\r\ndesc->ptr[4].j_extent = authsize;\r\nsg_count = talitos_map_sg(dev, areq->src, edesc->src_nents ? : 1,\r\n(areq->src == areq->dst) ? DMA_BIDIRECTIONAL\r\n: DMA_TO_DEVICE,\r\nedesc->src_is_chained);\r\nif (sg_count == 1) {\r\nto_talitos_ptr(&desc->ptr[4], sg_dma_address(areq->src));\r\n} else {\r\nsg_link_tbl_len = cryptlen;\r\nif (edesc->desc.hdr & DESC_HDR_MODE1_MDEU_CICV)\r\nsg_link_tbl_len = cryptlen + authsize;\r\nsg_count = sg_to_link_tbl(areq->src, sg_count, sg_link_tbl_len,\r\n&edesc->link_tbl[0]);\r\nif (sg_count > 1) {\r\ndesc->ptr[4].j_extent |= DESC_PTR_LNKTBL_JUMP;\r\nto_talitos_ptr(&desc->ptr[4], edesc->dma_link_tbl);\r\ndma_sync_single_for_device(dev, edesc->dma_link_tbl,\r\nedesc->dma_len,\r\nDMA_BIDIRECTIONAL);\r\n} else {\r\nto_talitos_ptr(&desc->ptr[4],\r\nsg_dma_address(areq->src));\r\n}\r\n}\r\ndesc->ptr[5].len = cpu_to_be16(cryptlen);\r\ndesc->ptr[5].j_extent = authsize;\r\nif (areq->src != areq->dst)\r\nsg_count = talitos_map_sg(dev, areq->dst,\r\nedesc->dst_nents ? : 1,\r\nDMA_FROM_DEVICE,\r\nedesc->dst_is_chained);\r\nif (sg_count == 1) {\r\nto_talitos_ptr(&desc->ptr[5], sg_dma_address(areq->dst));\r\n} else {\r\nstruct talitos_ptr *link_tbl_ptr =\r\n&edesc->link_tbl[edesc->src_nents + 1];\r\nto_talitos_ptr(&desc->ptr[5], edesc->dma_link_tbl +\r\n(edesc->src_nents + 1) *\r\nsizeof(struct talitos_ptr));\r\nsg_count = sg_to_link_tbl(areq->dst, sg_count, cryptlen,\r\nlink_tbl_ptr);\r\nlink_tbl_ptr += sg_count - 1;\r\nlink_tbl_ptr->j_extent = 0;\r\nsg_count++;\r\nlink_tbl_ptr++;\r\nlink_tbl_ptr->j_extent = DESC_PTR_LNKTBL_RETURN;\r\nlink_tbl_ptr->len = cpu_to_be16(authsize);\r\nto_talitos_ptr(link_tbl_ptr, edesc->dma_link_tbl +\r\n(edesc->src_nents + edesc->dst_nents + 2) *\r\nsizeof(struct talitos_ptr));\r\ndesc->ptr[5].j_extent |= DESC_PTR_LNKTBL_JUMP;\r\ndma_sync_single_for_device(ctx->dev, edesc->dma_link_tbl,\r\nedesc->dma_len, DMA_BIDIRECTIONAL);\r\n}\r\nmap_single_talitos_ptr(dev, &desc->ptr[6], ivsize, ctx->iv, 0,\r\nDMA_FROM_DEVICE);\r\nret = talitos_submit(dev, ctx->ch, desc, callback, areq);\r\nif (ret != -EINPROGRESS) {\r\nipsec_esp_unmap(dev, edesc, areq);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\n}\r\nstatic int sg_count(struct scatterlist *sg_list, int nbytes, int *chained)\r\n{\r\nstruct scatterlist *sg = sg_list;\r\nint sg_nents = 0;\r\n*chained = 0;\r\nwhile (nbytes > 0) {\r\nsg_nents++;\r\nnbytes -= sg->length;\r\nif (!sg_is_last(sg) && (sg + 1)->length == 0)\r\n*chained = 1;\r\nsg = scatterwalk_sg_next(sg);\r\n}\r\nreturn sg_nents;\r\n}\r\nstatic size_t sg_copy_end_to_buffer(struct scatterlist *sgl, unsigned int nents,\r\nvoid *buf, size_t buflen, unsigned int skip)\r\n{\r\nunsigned int offset = 0;\r\nunsigned int boffset = 0;\r\nstruct sg_mapping_iter miter;\r\nunsigned long flags;\r\nunsigned int sg_flags = SG_MITER_ATOMIC;\r\nsize_t total_buffer = buflen + skip;\r\nsg_flags |= SG_MITER_FROM_SG;\r\nsg_miter_start(&miter, sgl, nents, sg_flags);\r\nlocal_irq_save(flags);\r\nwhile (sg_miter_next(&miter) && offset < total_buffer) {\r\nunsigned int len;\r\nunsigned int ignore;\r\nif ((offset + miter.length) > skip) {\r\nif (offset < skip) {\r\nignore = skip - offset;\r\nlen = miter.length - ignore;\r\nif (boffset + len > buflen)\r\nlen = buflen - boffset;\r\nmemcpy(buf + boffset, miter.addr + ignore, len);\r\n} else {\r\nlen = miter.length;\r\nif (boffset + len > buflen)\r\nlen = buflen - boffset;\r\nmemcpy(buf + boffset, miter.addr, len);\r\n}\r\nboffset += len;\r\n}\r\noffset += miter.length;\r\n}\r\nsg_miter_stop(&miter);\r\nlocal_irq_restore(flags);\r\nreturn boffset;\r\n}\r\nstatic struct talitos_edesc *talitos_edesc_alloc(struct device *dev,\r\nstruct scatterlist *src,\r\nstruct scatterlist *dst,\r\nint hash_result,\r\nunsigned int cryptlen,\r\nunsigned int authsize,\r\nint icv_stashing,\r\nu32 cryptoflags)\r\n{\r\nstruct talitos_edesc *edesc;\r\nint src_nents, dst_nents, alloc_len, dma_len;\r\nint src_chained, dst_chained = 0;\r\ngfp_t flags = cryptoflags & CRYPTO_TFM_REQ_MAY_SLEEP ? GFP_KERNEL :\r\nGFP_ATOMIC;\r\nif (cryptlen + authsize > TALITOS_MAX_DATA_LEN) {\r\ndev_err(dev, "length exceeds h/w max limit\n");\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nsrc_nents = sg_count(src, cryptlen + authsize, &src_chained);\r\nsrc_nents = (src_nents == 1) ? 0 : src_nents;\r\nif (hash_result) {\r\ndst_nents = 0;\r\n} else {\r\nif (dst == src) {\r\ndst_nents = src_nents;\r\n} else {\r\ndst_nents = sg_count(dst, cryptlen + authsize,\r\n&dst_chained);\r\ndst_nents = (dst_nents == 1) ? 0 : dst_nents;\r\n}\r\n}\r\nalloc_len = sizeof(struct talitos_edesc);\r\nif (src_nents || dst_nents) {\r\ndma_len = (src_nents + dst_nents + 2) *\r\nsizeof(struct talitos_ptr) + authsize;\r\nalloc_len += dma_len;\r\n} else {\r\ndma_len = 0;\r\nalloc_len += icv_stashing ? authsize : 0;\r\n}\r\nedesc = kmalloc(alloc_len, GFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(dev, "could not allocate edescriptor\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nedesc->src_nents = src_nents;\r\nedesc->dst_nents = dst_nents;\r\nedesc->src_is_chained = src_chained;\r\nedesc->dst_is_chained = dst_chained;\r\nedesc->dma_len = dma_len;\r\nif (dma_len)\r\nedesc->dma_link_tbl = dma_map_single(dev, &edesc->link_tbl[0],\r\nedesc->dma_len,\r\nDMA_BIDIRECTIONAL);\r\nreturn edesc;\r\n}\r\nstatic struct talitos_edesc *aead_edesc_alloc(struct aead_request *areq,\r\nint icv_stashing)\r\n{\r\nstruct crypto_aead *authenc = crypto_aead_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_aead_ctx(authenc);\r\nreturn talitos_edesc_alloc(ctx->dev, areq->src, areq->dst, 0,\r\nareq->cryptlen, ctx->authsize, icv_stashing,\r\nareq->base.flags);\r\n}\r\nstatic int aead_encrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\r\nstruct talitos_ctx *ctx = crypto_aead_ctx(authenc);\r\nstruct talitos_edesc *edesc;\r\nedesc = aead_edesc_alloc(req, 0);\r\nif (IS_ERR(edesc))\r\nreturn PTR_ERR(edesc);\r\nedesc->desc.hdr = ctx->desc_hdr_template | DESC_HDR_MODE0_ENCRYPT;\r\nreturn ipsec_esp(edesc, req, NULL, 0, ipsec_esp_encrypt_done);\r\n}\r\nstatic int aead_decrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *authenc = crypto_aead_reqtfm(req);\r\nstruct talitos_ctx *ctx = crypto_aead_ctx(authenc);\r\nunsigned int authsize = ctx->authsize;\r\nstruct talitos_private *priv = dev_get_drvdata(ctx->dev);\r\nstruct talitos_edesc *edesc;\r\nstruct scatterlist *sg;\r\nvoid *icvdata;\r\nreq->cryptlen -= authsize;\r\nedesc = aead_edesc_alloc(req, 1);\r\nif (IS_ERR(edesc))\r\nreturn PTR_ERR(edesc);\r\nif ((priv->features & TALITOS_FTR_HW_AUTH_CHECK) &&\r\n((!edesc->src_nents && !edesc->dst_nents) ||\r\npriv->features & TALITOS_FTR_SRC_LINK_TBL_LEN_INCLUDES_EXTENT)) {\r\nedesc->desc.hdr = ctx->desc_hdr_template |\r\nDESC_HDR_DIR_INBOUND |\r\nDESC_HDR_MODE1_MDEU_CICV;\r\nedesc->desc.hdr_lo = 0;\r\nreturn ipsec_esp(edesc, req, NULL, 0,\r\nipsec_esp_decrypt_hwauth_done);\r\n}\r\nedesc->desc.hdr = ctx->desc_hdr_template | DESC_HDR_DIR_INBOUND;\r\nif (edesc->dma_len)\r\nicvdata = &edesc->link_tbl[edesc->src_nents +\r\nedesc->dst_nents + 2];\r\nelse\r\nicvdata = &edesc->link_tbl[0];\r\nsg = sg_last(req->src, edesc->src_nents ? : 1);\r\nmemcpy(icvdata, (char *)sg_virt(sg) + sg->length - ctx->authsize,\r\nctx->authsize);\r\nreturn ipsec_esp(edesc, req, NULL, 0, ipsec_esp_decrypt_swauth_done);\r\n}\r\nstatic int aead_givencrypt(struct aead_givcrypt_request *req)\r\n{\r\nstruct aead_request *areq = &req->areq;\r\nstruct crypto_aead *authenc = crypto_aead_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_aead_ctx(authenc);\r\nstruct talitos_edesc *edesc;\r\nedesc = aead_edesc_alloc(areq, 0);\r\nif (IS_ERR(edesc))\r\nreturn PTR_ERR(edesc);\r\nedesc->desc.hdr = ctx->desc_hdr_template | DESC_HDR_MODE0_ENCRYPT;\r\nmemcpy(req->giv, ctx->iv, crypto_aead_ivsize(authenc));\r\n*(__be64 *)req->giv ^= cpu_to_be64(req->seq);\r\nreturn ipsec_esp(edesc, areq, req->giv, req->seq,\r\nipsec_esp_encrypt_done);\r\n}\r\nstatic int ablkcipher_setkey(struct crypto_ablkcipher *cipher,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct talitos_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nmemcpy(&ctx->key, key, keylen);\r\nctx->keylen = keylen;\r\nreturn 0;\r\n}\r\nstatic void common_nonsnoop_unmap(struct device *dev,\r\nstruct talitos_edesc *edesc,\r\nstruct ablkcipher_request *areq)\r\n{\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[5], DMA_FROM_DEVICE);\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[2], DMA_TO_DEVICE);\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[1], DMA_TO_DEVICE);\r\ntalitos_sg_unmap(dev, edesc, areq->src, areq->dst);\r\nif (edesc->dma_len)\r\ndma_unmap_single(dev, edesc->dma_link_tbl, edesc->dma_len,\r\nDMA_BIDIRECTIONAL);\r\n}\r\nstatic void ablkcipher_done(struct device *dev,\r\nstruct talitos_desc *desc, void *context,\r\nint err)\r\n{\r\nstruct ablkcipher_request *areq = context;\r\nstruct talitos_edesc *edesc;\r\nedesc = container_of(desc, struct talitos_edesc, desc);\r\ncommon_nonsnoop_unmap(dev, edesc, areq);\r\nkfree(edesc);\r\nareq->base.complete(&areq->base, err);\r\n}\r\nstatic int common_nonsnoop(struct talitos_edesc *edesc,\r\nstruct ablkcipher_request *areq,\r\nvoid (*callback) (struct device *dev,\r\nstruct talitos_desc *desc,\r\nvoid *context, int error))\r\n{\r\nstruct crypto_ablkcipher *cipher = crypto_ablkcipher_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct device *dev = ctx->dev;\r\nstruct talitos_desc *desc = &edesc->desc;\r\nunsigned int cryptlen = areq->nbytes;\r\nunsigned int ivsize;\r\nint sg_count, ret;\r\ndesc->ptr[0].len = 0;\r\nto_talitos_ptr(&desc->ptr[0], 0);\r\ndesc->ptr[0].j_extent = 0;\r\nivsize = crypto_ablkcipher_ivsize(cipher);\r\nmap_single_talitos_ptr(dev, &desc->ptr[1], ivsize, areq->info, 0,\r\nDMA_TO_DEVICE);\r\nmap_single_talitos_ptr(dev, &desc->ptr[2], ctx->keylen,\r\n(char *)&ctx->key, 0, DMA_TO_DEVICE);\r\ndesc->ptr[3].len = cpu_to_be16(cryptlen);\r\ndesc->ptr[3].j_extent = 0;\r\nsg_count = talitos_map_sg(dev, areq->src, edesc->src_nents ? : 1,\r\n(areq->src == areq->dst) ? DMA_BIDIRECTIONAL\r\n: DMA_TO_DEVICE,\r\nedesc->src_is_chained);\r\nif (sg_count == 1) {\r\nto_talitos_ptr(&desc->ptr[3], sg_dma_address(areq->src));\r\n} else {\r\nsg_count = sg_to_link_tbl(areq->src, sg_count, cryptlen,\r\n&edesc->link_tbl[0]);\r\nif (sg_count > 1) {\r\nto_talitos_ptr(&desc->ptr[3], edesc->dma_link_tbl);\r\ndesc->ptr[3].j_extent |= DESC_PTR_LNKTBL_JUMP;\r\ndma_sync_single_for_device(dev, edesc->dma_link_tbl,\r\nedesc->dma_len,\r\nDMA_BIDIRECTIONAL);\r\n} else {\r\nto_talitos_ptr(&desc->ptr[3],\r\nsg_dma_address(areq->src));\r\n}\r\n}\r\ndesc->ptr[4].len = cpu_to_be16(cryptlen);\r\ndesc->ptr[4].j_extent = 0;\r\nif (areq->src != areq->dst)\r\nsg_count = talitos_map_sg(dev, areq->dst,\r\nedesc->dst_nents ? : 1,\r\nDMA_FROM_DEVICE,\r\nedesc->dst_is_chained);\r\nif (sg_count == 1) {\r\nto_talitos_ptr(&desc->ptr[4], sg_dma_address(areq->dst));\r\n} else {\r\nstruct talitos_ptr *link_tbl_ptr =\r\n&edesc->link_tbl[edesc->src_nents + 1];\r\nto_talitos_ptr(&desc->ptr[4], edesc->dma_link_tbl +\r\n(edesc->src_nents + 1) *\r\nsizeof(struct talitos_ptr));\r\ndesc->ptr[4].j_extent |= DESC_PTR_LNKTBL_JUMP;\r\nsg_count = sg_to_link_tbl(areq->dst, sg_count, cryptlen,\r\nlink_tbl_ptr);\r\ndma_sync_single_for_device(ctx->dev, edesc->dma_link_tbl,\r\nedesc->dma_len, DMA_BIDIRECTIONAL);\r\n}\r\nmap_single_talitos_ptr(dev, &desc->ptr[5], ivsize, ctx->iv, 0,\r\nDMA_FROM_DEVICE);\r\ndesc->ptr[6].len = 0;\r\nto_talitos_ptr(&desc->ptr[6], 0);\r\ndesc->ptr[6].j_extent = 0;\r\nret = talitos_submit(dev, ctx->ch, desc, callback, areq);\r\nif (ret != -EINPROGRESS) {\r\ncommon_nonsnoop_unmap(dev, edesc, areq);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\n}\r\nstatic struct talitos_edesc *ablkcipher_edesc_alloc(struct ablkcipher_request *\r\nareq)\r\n{\r\nstruct crypto_ablkcipher *cipher = crypto_ablkcipher_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nreturn talitos_edesc_alloc(ctx->dev, areq->src, areq->dst, 0,\r\nareq->nbytes, 0, 0, areq->base.flags);\r\n}\r\nstatic int ablkcipher_encrypt(struct ablkcipher_request *areq)\r\n{\r\nstruct crypto_ablkcipher *cipher = crypto_ablkcipher_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct talitos_edesc *edesc;\r\nedesc = ablkcipher_edesc_alloc(areq);\r\nif (IS_ERR(edesc))\r\nreturn PTR_ERR(edesc);\r\nedesc->desc.hdr = ctx->desc_hdr_template | DESC_HDR_MODE0_ENCRYPT;\r\nreturn common_nonsnoop(edesc, areq, ablkcipher_done);\r\n}\r\nstatic int ablkcipher_decrypt(struct ablkcipher_request *areq)\r\n{\r\nstruct crypto_ablkcipher *cipher = crypto_ablkcipher_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct talitos_edesc *edesc;\r\nedesc = ablkcipher_edesc_alloc(areq);\r\nif (IS_ERR(edesc))\r\nreturn PTR_ERR(edesc);\r\nedesc->desc.hdr = ctx->desc_hdr_template | DESC_HDR_DIR_INBOUND;\r\nreturn common_nonsnoop(edesc, areq, ablkcipher_done);\r\n}\r\nstatic void common_nonsnoop_hash_unmap(struct device *dev,\r\nstruct talitos_edesc *edesc,\r\nstruct ahash_request *areq)\r\n{\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[5], DMA_FROM_DEVICE);\r\nif (edesc->desc.ptr[1].len)\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[1],\r\nDMA_TO_DEVICE);\r\nif (edesc->desc.ptr[2].len)\r\nunmap_single_talitos_ptr(dev, &edesc->desc.ptr[2],\r\nDMA_TO_DEVICE);\r\ntalitos_sg_unmap(dev, edesc, req_ctx->psrc, NULL);\r\nif (edesc->dma_len)\r\ndma_unmap_single(dev, edesc->dma_link_tbl, edesc->dma_len,\r\nDMA_BIDIRECTIONAL);\r\n}\r\nstatic void ahash_done(struct device *dev,\r\nstruct talitos_desc *desc, void *context,\r\nint err)\r\n{\r\nstruct ahash_request *areq = context;\r\nstruct talitos_edesc *edesc =\r\ncontainer_of(desc, struct talitos_edesc, desc);\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nif (!req_ctx->last && req_ctx->to_hash_later) {\r\nmemcpy(req_ctx->buf, req_ctx->bufnext, req_ctx->to_hash_later);\r\nreq_ctx->nbuf = req_ctx->to_hash_later;\r\n}\r\ncommon_nonsnoop_hash_unmap(dev, edesc, areq);\r\nkfree(edesc);\r\nareq->base.complete(&areq->base, err);\r\n}\r\nstatic int common_nonsnoop_hash(struct talitos_edesc *edesc,\r\nstruct ahash_request *areq, unsigned int length,\r\nvoid (*callback) (struct device *dev,\r\nstruct talitos_desc *desc,\r\nvoid *context, int error))\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_ahash_ctx(tfm);\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nstruct device *dev = ctx->dev;\r\nstruct talitos_desc *desc = &edesc->desc;\r\nint sg_count, ret;\r\ndesc->ptr[0] = zero_entry;\r\nif (!req_ctx->first || req_ctx->swinit) {\r\nmap_single_talitos_ptr(dev, &desc->ptr[1],\r\nreq_ctx->hw_context_size,\r\n(char *)req_ctx->hw_context, 0,\r\nDMA_TO_DEVICE);\r\nreq_ctx->swinit = 0;\r\n} else {\r\ndesc->ptr[1] = zero_entry;\r\nreq_ctx->first = 0;\r\n}\r\nif (ctx->keylen)\r\nmap_single_talitos_ptr(dev, &desc->ptr[2], ctx->keylen,\r\n(char *)&ctx->key, 0, DMA_TO_DEVICE);\r\nelse\r\ndesc->ptr[2] = zero_entry;\r\ndesc->ptr[3].len = cpu_to_be16(length);\r\ndesc->ptr[3].j_extent = 0;\r\nsg_count = talitos_map_sg(dev, req_ctx->psrc,\r\nedesc->src_nents ? : 1,\r\nDMA_TO_DEVICE,\r\nedesc->src_is_chained);\r\nif (sg_count == 1) {\r\nto_talitos_ptr(&desc->ptr[3], sg_dma_address(req_ctx->psrc));\r\n} else {\r\nsg_count = sg_to_link_tbl(req_ctx->psrc, sg_count, length,\r\n&edesc->link_tbl[0]);\r\nif (sg_count > 1) {\r\ndesc->ptr[3].j_extent |= DESC_PTR_LNKTBL_JUMP;\r\nto_talitos_ptr(&desc->ptr[3], edesc->dma_link_tbl);\r\ndma_sync_single_for_device(ctx->dev,\r\nedesc->dma_link_tbl,\r\nedesc->dma_len,\r\nDMA_BIDIRECTIONAL);\r\n} else {\r\nto_talitos_ptr(&desc->ptr[3],\r\nsg_dma_address(req_ctx->psrc));\r\n}\r\n}\r\ndesc->ptr[4] = zero_entry;\r\nif (req_ctx->last)\r\nmap_single_talitos_ptr(dev, &desc->ptr[5],\r\ncrypto_ahash_digestsize(tfm),\r\nareq->result, 0, DMA_FROM_DEVICE);\r\nelse\r\nmap_single_talitos_ptr(dev, &desc->ptr[5],\r\nreq_ctx->hw_context_size,\r\nreq_ctx->hw_context, 0, DMA_FROM_DEVICE);\r\ndesc->ptr[6] = zero_entry;\r\nret = talitos_submit(dev, ctx->ch, desc, callback, areq);\r\nif (ret != -EINPROGRESS) {\r\ncommon_nonsnoop_hash_unmap(dev, edesc, areq);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\n}\r\nstatic struct talitos_edesc *ahash_edesc_alloc(struct ahash_request *areq,\r\nunsigned int nbytes)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_ahash_ctx(tfm);\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nreturn talitos_edesc_alloc(ctx->dev, req_ctx->psrc, NULL, 1,\r\nnbytes, 0, 0, areq->base.flags);\r\n}\r\nstatic int ahash_init(struct ahash_request *areq)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nreq_ctx->nbuf = 0;\r\nreq_ctx->first = 1;\r\nreq_ctx->swinit = 0;\r\nreq_ctx->hw_context_size =\r\n(crypto_ahash_digestsize(tfm) <= SHA256_DIGEST_SIZE)\r\n? TALITOS_MDEU_CONTEXT_SIZE_MD5_SHA1_SHA256\r\n: TALITOS_MDEU_CONTEXT_SIZE_SHA384_SHA512;\r\nreturn 0;\r\n}\r\nstatic int ahash_init_sha224_swinit(struct ahash_request *areq)\r\n{\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nahash_init(areq);\r\nreq_ctx->swinit = 1;\r\nreq_ctx->hw_context[0] = SHA224_H0;\r\nreq_ctx->hw_context[1] = SHA224_H1;\r\nreq_ctx->hw_context[2] = SHA224_H2;\r\nreq_ctx->hw_context[3] = SHA224_H3;\r\nreq_ctx->hw_context[4] = SHA224_H4;\r\nreq_ctx->hw_context[5] = SHA224_H5;\r\nreq_ctx->hw_context[6] = SHA224_H6;\r\nreq_ctx->hw_context[7] = SHA224_H7;\r\nreq_ctx->hw_context[8] = 0;\r\nreq_ctx->hw_context[9] = 0;\r\nreturn 0;\r\n}\r\nstatic int ahash_process_req(struct ahash_request *areq, unsigned int nbytes)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);\r\nstruct talitos_ctx *ctx = crypto_ahash_ctx(tfm);\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nstruct talitos_edesc *edesc;\r\nunsigned int blocksize =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nunsigned int nbytes_to_hash;\r\nunsigned int to_hash_later;\r\nunsigned int nsg;\r\nint chained;\r\nif (!req_ctx->last && (nbytes + req_ctx->nbuf <= blocksize)) {\r\nsg_copy_to_buffer(areq->src,\r\nsg_count(areq->src, nbytes, &chained),\r\nreq_ctx->buf + req_ctx->nbuf, nbytes);\r\nreq_ctx->nbuf += nbytes;\r\nreturn 0;\r\n}\r\nnbytes_to_hash = nbytes + req_ctx->nbuf;\r\nto_hash_later = nbytes_to_hash & (blocksize - 1);\r\nif (req_ctx->last)\r\nto_hash_later = 0;\r\nelse if (to_hash_later)\r\nnbytes_to_hash -= to_hash_later;\r\nelse {\r\nnbytes_to_hash -= blocksize;\r\nto_hash_later = blocksize;\r\n}\r\nif (req_ctx->nbuf) {\r\nnsg = (req_ctx->nbuf < nbytes_to_hash) ? 2 : 1;\r\nsg_init_table(req_ctx->bufsl, nsg);\r\nsg_set_buf(req_ctx->bufsl, req_ctx->buf, req_ctx->nbuf);\r\nif (nsg > 1)\r\nscatterwalk_sg_chain(req_ctx->bufsl, 2, areq->src);\r\nreq_ctx->psrc = req_ctx->bufsl;\r\n} else\r\nreq_ctx->psrc = areq->src;\r\nif (to_hash_later) {\r\nint nents = sg_count(areq->src, nbytes, &chained);\r\nsg_copy_end_to_buffer(areq->src, nents,\r\nreq_ctx->bufnext,\r\nto_hash_later,\r\nnbytes - to_hash_later);\r\n}\r\nreq_ctx->to_hash_later = to_hash_later;\r\nedesc = ahash_edesc_alloc(areq, nbytes_to_hash);\r\nif (IS_ERR(edesc))\r\nreturn PTR_ERR(edesc);\r\nedesc->desc.hdr = ctx->desc_hdr_template;\r\nif (req_ctx->last)\r\nedesc->desc.hdr |= DESC_HDR_MODE0_MDEU_PAD;\r\nelse\r\nedesc->desc.hdr |= DESC_HDR_MODE0_MDEU_CONT;\r\nif (req_ctx->first && !req_ctx->swinit)\r\nedesc->desc.hdr |= DESC_HDR_MODE0_MDEU_INIT;\r\nif (ctx->keylen && (req_ctx->first || req_ctx->last))\r\nedesc->desc.hdr |= DESC_HDR_MODE0_MDEU_HMAC;\r\nreturn common_nonsnoop_hash(edesc, areq, nbytes_to_hash,\r\nahash_done);\r\n}\r\nstatic int ahash_update(struct ahash_request *areq)\r\n{\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nreq_ctx->last = 0;\r\nreturn ahash_process_req(areq, areq->nbytes);\r\n}\r\nstatic int ahash_final(struct ahash_request *areq)\r\n{\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nreq_ctx->last = 1;\r\nreturn ahash_process_req(areq, 0);\r\n}\r\nstatic int ahash_finup(struct ahash_request *areq)\r\n{\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nreq_ctx->last = 1;\r\nreturn ahash_process_req(areq, areq->nbytes);\r\n}\r\nstatic int ahash_digest(struct ahash_request *areq)\r\n{\r\nstruct talitos_ahash_req_ctx *req_ctx = ahash_request_ctx(areq);\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(areq);\r\nahash->init(areq);\r\nreq_ctx->last = 1;\r\nreturn ahash_process_req(areq, areq->nbytes);\r\n}\r\nstatic void keyhash_complete(struct crypto_async_request *req, int err)\r\n{\r\nstruct keyhash_result *res = req->data;\r\nif (err == -EINPROGRESS)\r\nreturn;\r\nres->err = err;\r\ncomplete(&res->completion);\r\n}\r\nstatic int keyhash(struct crypto_ahash *tfm, const u8 *key, unsigned int keylen,\r\nu8 *hash)\r\n{\r\nstruct talitos_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));\r\nstruct scatterlist sg[1];\r\nstruct ahash_request *req;\r\nstruct keyhash_result hresult;\r\nint ret;\r\ninit_completion(&hresult.completion);\r\nreq = ahash_request_alloc(tfm, GFP_KERNEL);\r\nif (!req)\r\nreturn -ENOMEM;\r\nctx->keylen = 0;\r\nahash_request_set_callback(req, CRYPTO_TFM_REQ_MAY_BACKLOG,\r\nkeyhash_complete, &hresult);\r\nsg_init_one(&sg[0], key, keylen);\r\nahash_request_set_crypt(req, sg, hash, keylen);\r\nret = crypto_ahash_digest(req);\r\nswitch (ret) {\r\ncase 0:\r\nbreak;\r\ncase -EINPROGRESS:\r\ncase -EBUSY:\r\nret = wait_for_completion_interruptible(\r\n&hresult.completion);\r\nif (!ret)\r\nret = hresult.err;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nahash_request_free(req);\r\nreturn ret;\r\n}\r\nstatic int ahash_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct talitos_ctx *ctx = crypto_tfm_ctx(crypto_ahash_tfm(tfm));\r\nunsigned int blocksize =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nunsigned int digestsize = crypto_ahash_digestsize(tfm);\r\nunsigned int keysize = keylen;\r\nu8 hash[SHA512_DIGEST_SIZE];\r\nint ret;\r\nif (keylen <= blocksize)\r\nmemcpy(ctx->key, key, keysize);\r\nelse {\r\nret = keyhash(tfm, key, keylen, hash);\r\nif (ret) {\r\ncrypto_ahash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nkeysize = digestsize;\r\nmemcpy(ctx->key, hash, digestsize);\r\n}\r\nctx->keylen = keysize;\r\nreturn 0;\r\n}\r\nstatic int talitos_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct talitos_crypto_alg *talitos_alg;\r\nstruct talitos_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct talitos_private *priv;\r\nif ((alg->cra_flags & CRYPTO_ALG_TYPE_MASK) == CRYPTO_ALG_TYPE_AHASH)\r\ntalitos_alg = container_of(__crypto_ahash_alg(alg),\r\nstruct talitos_crypto_alg,\r\nalgt.alg.hash);\r\nelse\r\ntalitos_alg = container_of(alg, struct talitos_crypto_alg,\r\nalgt.alg.crypto);\r\nctx->dev = talitos_alg->dev;\r\npriv = dev_get_drvdata(ctx->dev);\r\nctx->ch = atomic_inc_return(&priv->last_chan) &\r\n(priv->num_channels - 1);\r\nctx->desc_hdr_template = talitos_alg->algt.desc_hdr_template;\r\nctx->desc_hdr_template |= DESC_HDR_DONE_NOTIFY;\r\nreturn 0;\r\n}\r\nstatic int talitos_cra_init_aead(struct crypto_tfm *tfm)\r\n{\r\nstruct talitos_ctx *ctx = crypto_tfm_ctx(tfm);\r\ntalitos_cra_init(tfm);\r\nget_random_bytes(ctx->iv, TALITOS_MAX_IV_LENGTH);\r\nreturn 0;\r\n}\r\nstatic int talitos_cra_init_ahash(struct crypto_tfm *tfm)\r\n{\r\nstruct talitos_ctx *ctx = crypto_tfm_ctx(tfm);\r\ntalitos_cra_init(tfm);\r\nctx->keylen = 0;\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct talitos_ahash_req_ctx));\r\nreturn 0;\r\n}\r\nstatic int hw_supports(struct device *dev, __be32 desc_hdr_template)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nint ret;\r\nret = (1 << DESC_TYPE(desc_hdr_template) & priv->desc_types) &&\r\n(1 << PRIMARY_EU(desc_hdr_template) & priv->exec_units);\r\nif (SECONDARY_EU(desc_hdr_template))\r\nret = ret && (1 << SECONDARY_EU(desc_hdr_template)\r\n& priv->exec_units);\r\nreturn ret;\r\n}\r\nstatic int talitos_remove(struct platform_device *ofdev)\r\n{\r\nstruct device *dev = &ofdev->dev;\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nstruct talitos_crypto_alg *t_alg, *n;\r\nint i;\r\nlist_for_each_entry_safe(t_alg, n, &priv->alg_list, entry) {\r\nswitch (t_alg->algt.type) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\ncrypto_unregister_alg(&t_alg->algt.alg.crypto);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\ncrypto_unregister_ahash(&t_alg->algt.alg.hash);\r\nbreak;\r\n}\r\nlist_del(&t_alg->entry);\r\nkfree(t_alg);\r\n}\r\nif (hw_supports(dev, DESC_HDR_SEL0_RNG))\r\ntalitos_unregister_rng(dev);\r\nfor (i = 0; i < priv->num_channels; i++)\r\nkfree(priv->chan[i].fifo);\r\nkfree(priv->chan);\r\nfor (i = 0; i < 2; i++)\r\nif (priv->irq[i]) {\r\nfree_irq(priv->irq[i], dev);\r\nirq_dispose_mapping(priv->irq[i]);\r\n}\r\ntasklet_kill(&priv->done_task[0]);\r\nif (priv->irq[1])\r\ntasklet_kill(&priv->done_task[1]);\r\niounmap(priv->reg);\r\ndev_set_drvdata(dev, NULL);\r\nkfree(priv);\r\nreturn 0;\r\n}\r\nstatic struct talitos_crypto_alg *talitos_alg_alloc(struct device *dev,\r\nstruct talitos_alg_template\r\n*template)\r\n{\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nstruct talitos_crypto_alg *t_alg;\r\nstruct crypto_alg *alg;\r\nt_alg = kzalloc(sizeof(struct talitos_crypto_alg), GFP_KERNEL);\r\nif (!t_alg)\r\nreturn ERR_PTR(-ENOMEM);\r\nt_alg->algt = *template;\r\nswitch (t_alg->algt.type) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\nalg = &t_alg->algt.alg.crypto;\r\nalg->cra_init = talitos_cra_init;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\nalg = &t_alg->algt.alg.crypto;\r\nalg->cra_init = talitos_cra_init_aead;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\nalg = &t_alg->algt.alg.hash.halg.base;\r\nalg->cra_init = talitos_cra_init_ahash;\r\nif (!(priv->features & TALITOS_FTR_HMAC_OK) &&\r\n!strncmp(alg->cra_name, "hmac", 4)) {\r\nkfree(t_alg);\r\nreturn ERR_PTR(-ENOTSUPP);\r\n}\r\nif (!(priv->features & TALITOS_FTR_SHA224_HWINIT) &&\r\n(!strcmp(alg->cra_name, "sha224") ||\r\n!strcmp(alg->cra_name, "hmac(sha224)"))) {\r\nt_alg->algt.alg.hash.init = ahash_init_sha224_swinit;\r\nt_alg->algt.desc_hdr_template =\r\nDESC_HDR_TYPE_COMMON_NONSNOOP_NO_AFEU |\r\nDESC_HDR_SEL0_MDEUA |\r\nDESC_HDR_MODE0_MDEU_SHA256;\r\n}\r\nbreak;\r\ndefault:\r\ndev_err(dev, "unknown algorithm type %d\n", t_alg->algt.type);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nalg->cra_module = THIS_MODULE;\r\nalg->cra_priority = TALITOS_CRA_PRIORITY;\r\nalg->cra_alignmask = 0;\r\nalg->cra_ctxsize = sizeof(struct talitos_ctx);\r\nalg->cra_flags |= CRYPTO_ALG_KERN_DRIVER_ONLY;\r\nt_alg->dev = dev;\r\nreturn t_alg;\r\n}\r\nstatic int talitos_probe_irq(struct platform_device *ofdev)\r\n{\r\nstruct device *dev = &ofdev->dev;\r\nstruct device_node *np = ofdev->dev.of_node;\r\nstruct talitos_private *priv = dev_get_drvdata(dev);\r\nint err;\r\npriv->irq[0] = irq_of_parse_and_map(np, 0);\r\nif (!priv->irq[0]) {\r\ndev_err(dev, "failed to map irq\n");\r\nreturn -EINVAL;\r\n}\r\npriv->irq[1] = irq_of_parse_and_map(np, 1);\r\nif (!priv->irq[1]) {\r\nerr = request_irq(priv->irq[0], talitos_interrupt_4ch, 0,\r\ndev_driver_string(dev), dev);\r\ngoto primary_out;\r\n}\r\nerr = request_irq(priv->irq[0], talitos_interrupt_ch0_2, 0,\r\ndev_driver_string(dev), dev);\r\nif (err)\r\ngoto primary_out;\r\nerr = request_irq(priv->irq[1], talitos_interrupt_ch1_3, 0,\r\ndev_driver_string(dev), dev);\r\nif (err) {\r\ndev_err(dev, "failed to request secondary irq\n");\r\nirq_dispose_mapping(priv->irq[1]);\r\npriv->irq[1] = 0;\r\n}\r\nreturn err;\r\nprimary_out:\r\nif (err) {\r\ndev_err(dev, "failed to request primary irq\n");\r\nirq_dispose_mapping(priv->irq[0]);\r\npriv->irq[0] = 0;\r\n}\r\nreturn err;\r\n}\r\nstatic int talitos_probe(struct platform_device *ofdev)\r\n{\r\nstruct device *dev = &ofdev->dev;\r\nstruct device_node *np = ofdev->dev.of_node;\r\nstruct talitos_private *priv;\r\nconst unsigned int *prop;\r\nint i, err;\r\npriv = kzalloc(sizeof(struct talitos_private), GFP_KERNEL);\r\nif (!priv)\r\nreturn -ENOMEM;\r\ndev_set_drvdata(dev, priv);\r\npriv->ofdev = ofdev;\r\nspin_lock_init(&priv->reg_lock);\r\nerr = talitos_probe_irq(ofdev);\r\nif (err)\r\ngoto err_out;\r\nif (!priv->irq[1]) {\r\ntasklet_init(&priv->done_task[0], talitos_done_4ch,\r\n(unsigned long)dev);\r\n} else {\r\ntasklet_init(&priv->done_task[0], talitos_done_ch0_2,\r\n(unsigned long)dev);\r\ntasklet_init(&priv->done_task[1], talitos_done_ch1_3,\r\n(unsigned long)dev);\r\n}\r\nINIT_LIST_HEAD(&priv->alg_list);\r\npriv->reg = of_iomap(np, 0);\r\nif (!priv->reg) {\r\ndev_err(dev, "failed to of_iomap\n");\r\nerr = -ENOMEM;\r\ngoto err_out;\r\n}\r\nprop = of_get_property(np, "fsl,num-channels", NULL);\r\nif (prop)\r\npriv->num_channels = *prop;\r\nprop = of_get_property(np, "fsl,channel-fifo-len", NULL);\r\nif (prop)\r\npriv->chfifo_len = *prop;\r\nprop = of_get_property(np, "fsl,exec-units-mask", NULL);\r\nif (prop)\r\npriv->exec_units = *prop;\r\nprop = of_get_property(np, "fsl,descriptor-types-mask", NULL);\r\nif (prop)\r\npriv->desc_types = *prop;\r\nif (!is_power_of_2(priv->num_channels) || !priv->chfifo_len ||\r\n!priv->exec_units || !priv->desc_types) {\r\ndev_err(dev, "invalid property data in device tree node\n");\r\nerr = -EINVAL;\r\ngoto err_out;\r\n}\r\nif (of_device_is_compatible(np, "fsl,sec3.0"))\r\npriv->features |= TALITOS_FTR_SRC_LINK_TBL_LEN_INCLUDES_EXTENT;\r\nif (of_device_is_compatible(np, "fsl,sec2.1"))\r\npriv->features |= TALITOS_FTR_HW_AUTH_CHECK |\r\nTALITOS_FTR_SHA224_HWINIT |\r\nTALITOS_FTR_HMAC_OK;\r\npriv->chan = kzalloc(sizeof(struct talitos_channel) *\r\npriv->num_channels, GFP_KERNEL);\r\nif (!priv->chan) {\r\ndev_err(dev, "failed to allocate channel management space\n");\r\nerr = -ENOMEM;\r\ngoto err_out;\r\n}\r\nfor (i = 0; i < priv->num_channels; i++) {\r\npriv->chan[i].reg = priv->reg + TALITOS_CH_STRIDE * (i + 1);\r\nif (!priv->irq[1] || !(i & 1))\r\npriv->chan[i].reg += TALITOS_CH_BASE_OFFSET;\r\n}\r\nfor (i = 0; i < priv->num_channels; i++) {\r\nspin_lock_init(&priv->chan[i].head_lock);\r\nspin_lock_init(&priv->chan[i].tail_lock);\r\n}\r\npriv->fifo_len = roundup_pow_of_two(priv->chfifo_len);\r\nfor (i = 0; i < priv->num_channels; i++) {\r\npriv->chan[i].fifo = kzalloc(sizeof(struct talitos_request) *\r\npriv->fifo_len, GFP_KERNEL);\r\nif (!priv->chan[i].fifo) {\r\ndev_err(dev, "failed to allocate request fifo %d\n", i);\r\nerr = -ENOMEM;\r\ngoto err_out;\r\n}\r\n}\r\nfor (i = 0; i < priv->num_channels; i++)\r\natomic_set(&priv->chan[i].submit_count,\r\n-(priv->chfifo_len - 1));\r\ndma_set_mask(dev, DMA_BIT_MASK(36));\r\nerr = init_device(dev);\r\nif (err) {\r\ndev_err(dev, "failed to initialize device\n");\r\ngoto err_out;\r\n}\r\nif (hw_supports(dev, DESC_HDR_SEL0_RNG)) {\r\nerr = talitos_register_rng(dev);\r\nif (err) {\r\ndev_err(dev, "failed to register hwrng: %d\n", err);\r\ngoto err_out;\r\n} else\r\ndev_info(dev, "hwrng\n");\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(driver_algs); i++) {\r\nif (hw_supports(dev, driver_algs[i].desc_hdr_template)) {\r\nstruct talitos_crypto_alg *t_alg;\r\nchar *name = NULL;\r\nt_alg = talitos_alg_alloc(dev, &driver_algs[i]);\r\nif (IS_ERR(t_alg)) {\r\nerr = PTR_ERR(t_alg);\r\nif (err == -ENOTSUPP)\r\ncontinue;\r\ngoto err_out;\r\n}\r\nswitch (t_alg->algt.type) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\nerr = crypto_register_alg(\r\n&t_alg->algt.alg.crypto);\r\nname = t_alg->algt.alg.crypto.cra_driver_name;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\nerr = crypto_register_ahash(\r\n&t_alg->algt.alg.hash);\r\nname =\r\nt_alg->algt.alg.hash.halg.base.cra_driver_name;\r\nbreak;\r\n}\r\nif (err) {\r\ndev_err(dev, "%s alg registration failed\n",\r\nname);\r\nkfree(t_alg);\r\n} else\r\nlist_add_tail(&t_alg->entry, &priv->alg_list);\r\n}\r\n}\r\nif (!list_empty(&priv->alg_list))\r\ndev_info(dev, "%s algorithms registered in /proc/crypto\n",\r\n(char *)of_get_property(np, "compatible", NULL));\r\nreturn 0;\r\nerr_out:\r\ntalitos_remove(ofdev);\r\nreturn err;\r\n}
