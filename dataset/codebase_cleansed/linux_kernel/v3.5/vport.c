int ovs_vport_init(void)\r\n{\r\ndev_table = kzalloc(VPORT_HASH_BUCKETS * sizeof(struct hlist_head),\r\nGFP_KERNEL);\r\nif (!dev_table)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid ovs_vport_exit(void)\r\n{\r\nkfree(dev_table);\r\n}\r\nstatic struct hlist_head *hash_bucket(const char *name)\r\n{\r\nunsigned int hash = full_name_hash(name, strlen(name));\r\nreturn &dev_table[hash & (VPORT_HASH_BUCKETS - 1)];\r\n}\r\nstruct vport *ovs_vport_locate(const char *name)\r\n{\r\nstruct hlist_head *bucket = hash_bucket(name);\r\nstruct vport *vport;\r\nstruct hlist_node *node;\r\nhlist_for_each_entry_rcu(vport, node, bucket, hash_node)\r\nif (!strcmp(name, vport->ops->get_name(vport)))\r\nreturn vport;\r\nreturn NULL;\r\n}\r\nstruct vport *ovs_vport_alloc(int priv_size, const struct vport_ops *ops,\r\nconst struct vport_parms *parms)\r\n{\r\nstruct vport *vport;\r\nsize_t alloc_size;\r\nalloc_size = sizeof(struct vport);\r\nif (priv_size) {\r\nalloc_size = ALIGN(alloc_size, VPORT_ALIGN);\r\nalloc_size += priv_size;\r\n}\r\nvport = kzalloc(alloc_size, GFP_KERNEL);\r\nif (!vport)\r\nreturn ERR_PTR(-ENOMEM);\r\nvport->dp = parms->dp;\r\nvport->port_no = parms->port_no;\r\nvport->upcall_pid = parms->upcall_pid;\r\nvport->ops = ops;\r\nvport->percpu_stats = alloc_percpu(struct vport_percpu_stats);\r\nif (!vport->percpu_stats) {\r\nkfree(vport);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nspin_lock_init(&vport->stats_lock);\r\nreturn vport;\r\n}\r\nvoid ovs_vport_free(struct vport *vport)\r\n{\r\nfree_percpu(vport->percpu_stats);\r\nkfree(vport);\r\n}\r\nstruct vport *ovs_vport_add(const struct vport_parms *parms)\r\n{\r\nstruct vport *vport;\r\nint err = 0;\r\nint i;\r\nASSERT_RTNL();\r\nfor (i = 0; i < ARRAY_SIZE(vport_ops_list); i++) {\r\nif (vport_ops_list[i]->type == parms->type) {\r\nvport = vport_ops_list[i]->create(parms);\r\nif (IS_ERR(vport)) {\r\nerr = PTR_ERR(vport);\r\ngoto out;\r\n}\r\nhlist_add_head_rcu(&vport->hash_node,\r\nhash_bucket(vport->ops->get_name(vport)));\r\nreturn vport;\r\n}\r\n}\r\nerr = -EAFNOSUPPORT;\r\nout:\r\nreturn ERR_PTR(err);\r\n}\r\nint ovs_vport_set_options(struct vport *vport, struct nlattr *options)\r\n{\r\nASSERT_RTNL();\r\nif (!vport->ops->set_options)\r\nreturn -EOPNOTSUPP;\r\nreturn vport->ops->set_options(vport, options);\r\n}\r\nvoid ovs_vport_del(struct vport *vport)\r\n{\r\nASSERT_RTNL();\r\nhlist_del_rcu(&vport->hash_node);\r\nvport->ops->destroy(vport);\r\n}\r\nvoid ovs_vport_get_stats(struct vport *vport, struct ovs_vport_stats *stats)\r\n{\r\nint i;\r\nmemset(stats, 0, sizeof(*stats));\r\nspin_lock_bh(&vport->stats_lock);\r\nstats->rx_errors = vport->err_stats.rx_errors;\r\nstats->tx_errors = vport->err_stats.tx_errors;\r\nstats->tx_dropped = vport->err_stats.tx_dropped;\r\nstats->rx_dropped = vport->err_stats.rx_dropped;\r\nspin_unlock_bh(&vport->stats_lock);\r\nfor_each_possible_cpu(i) {\r\nconst struct vport_percpu_stats *percpu_stats;\r\nstruct vport_percpu_stats local_stats;\r\nunsigned int start;\r\npercpu_stats = per_cpu_ptr(vport->percpu_stats, i);\r\ndo {\r\nstart = u64_stats_fetch_begin_bh(&percpu_stats->sync);\r\nlocal_stats = *percpu_stats;\r\n} while (u64_stats_fetch_retry_bh(&percpu_stats->sync, start));\r\nstats->rx_bytes += local_stats.rx_bytes;\r\nstats->rx_packets += local_stats.rx_packets;\r\nstats->tx_bytes += local_stats.tx_bytes;\r\nstats->tx_packets += local_stats.tx_packets;\r\n}\r\n}\r\nint ovs_vport_get_options(const struct vport *vport, struct sk_buff *skb)\r\n{\r\nstruct nlattr *nla;\r\nnla = nla_nest_start(skb, OVS_VPORT_ATTR_OPTIONS);\r\nif (!nla)\r\nreturn -EMSGSIZE;\r\nif (vport->ops->get_options) {\r\nint err = vport->ops->get_options(vport, skb);\r\nif (err) {\r\nnla_nest_cancel(skb, nla);\r\nreturn err;\r\n}\r\n}\r\nnla_nest_end(skb, nla);\r\nreturn 0;\r\n}\r\nvoid ovs_vport_receive(struct vport *vport, struct sk_buff *skb)\r\n{\r\nstruct vport_percpu_stats *stats;\r\nstats = per_cpu_ptr(vport->percpu_stats, smp_processor_id());\r\nu64_stats_update_begin(&stats->sync);\r\nstats->rx_packets++;\r\nstats->rx_bytes += skb->len;\r\nu64_stats_update_end(&stats->sync);\r\novs_dp_process_received_packet(vport, skb);\r\n}\r\nint ovs_vport_send(struct vport *vport, struct sk_buff *skb)\r\n{\r\nint sent = vport->ops->send(vport, skb);\r\nif (likely(sent)) {\r\nstruct vport_percpu_stats *stats;\r\nstats = per_cpu_ptr(vport->percpu_stats, smp_processor_id());\r\nu64_stats_update_begin(&stats->sync);\r\nstats->tx_packets++;\r\nstats->tx_bytes += sent;\r\nu64_stats_update_end(&stats->sync);\r\n}\r\nreturn sent;\r\n}\r\nvoid ovs_vport_record_error(struct vport *vport, enum vport_err_type err_type)\r\n{\r\nspin_lock(&vport->stats_lock);\r\nswitch (err_type) {\r\ncase VPORT_E_RX_DROPPED:\r\nvport->err_stats.rx_dropped++;\r\nbreak;\r\ncase VPORT_E_RX_ERROR:\r\nvport->err_stats.rx_errors++;\r\nbreak;\r\ncase VPORT_E_TX_DROPPED:\r\nvport->err_stats.tx_dropped++;\r\nbreak;\r\ncase VPORT_E_TX_ERROR:\r\nvport->err_stats.tx_errors++;\r\nbreak;\r\n};\r\nspin_unlock(&vport->stats_lock);\r\n}
