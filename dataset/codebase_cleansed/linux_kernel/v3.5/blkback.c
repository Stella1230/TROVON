static inline int vaddr_pagenr(struct pending_req *req, int seg)\r\n{\r\nreturn (req - blkbk->pending_reqs) *\r\nBLKIF_MAX_SEGMENTS_PER_REQUEST + seg;\r\n}\r\nstatic inline unsigned long vaddr(struct pending_req *req, int seg)\r\n{\r\nunsigned long pfn = page_to_pfn(blkbk->pending_page(req, seg));\r\nreturn (unsigned long)pfn_to_kaddr(pfn);\r\n}\r\nstatic struct pending_req *alloc_req(void)\r\n{\r\nstruct pending_req *req = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&blkbk->pending_free_lock, flags);\r\nif (!list_empty(&blkbk->pending_free)) {\r\nreq = list_entry(blkbk->pending_free.next, struct pending_req,\r\nfree_list);\r\nlist_del(&req->free_list);\r\n}\r\nspin_unlock_irqrestore(&blkbk->pending_free_lock, flags);\r\nreturn req;\r\n}\r\nstatic void free_req(struct pending_req *req)\r\n{\r\nunsigned long flags;\r\nint was_empty;\r\nspin_lock_irqsave(&blkbk->pending_free_lock, flags);\r\nwas_empty = list_empty(&blkbk->pending_free);\r\nlist_add(&req->free_list, &blkbk->pending_free);\r\nspin_unlock_irqrestore(&blkbk->pending_free_lock, flags);\r\nif (was_empty)\r\nwake_up(&blkbk->pending_free_wq);\r\n}\r\nstatic int xen_vbd_translate(struct phys_req *req, struct xen_blkif *blkif,\r\nint operation)\r\n{\r\nstruct xen_vbd *vbd = &blkif->vbd;\r\nint rc = -EACCES;\r\nif ((operation != READ) && vbd->readonly)\r\ngoto out;\r\nif (likely(req->nr_sects)) {\r\nblkif_sector_t end = req->sector_number + req->nr_sects;\r\nif (unlikely(end < req->sector_number))\r\ngoto out;\r\nif (unlikely(end > vbd_sz(vbd)))\r\ngoto out;\r\n}\r\nreq->dev = vbd->pdevice;\r\nreq->bdev = vbd->bdev;\r\nrc = 0;\r\nout:\r\nreturn rc;\r\n}\r\nstatic void xen_vbd_resize(struct xen_blkif *blkif)\r\n{\r\nstruct xen_vbd *vbd = &blkif->vbd;\r\nstruct xenbus_transaction xbt;\r\nint err;\r\nstruct xenbus_device *dev = xen_blkbk_xenbus(blkif->be);\r\nunsigned long long new_size = vbd_sz(vbd);\r\npr_info(DRV_PFX "VBD Resize: Domid: %d, Device: (%d, %d)\n",\r\nblkif->domid, MAJOR(vbd->pdevice), MINOR(vbd->pdevice));\r\npr_info(DRV_PFX "VBD Resize: new size %llu\n", new_size);\r\nvbd->size = new_size;\r\nagain:\r\nerr = xenbus_transaction_start(&xbt);\r\nif (err) {\r\npr_warn(DRV_PFX "Error starting transaction");\r\nreturn;\r\n}\r\nerr = xenbus_printf(xbt, dev->nodename, "sectors", "%llu",\r\n(unsigned long long)vbd_sz(vbd));\r\nif (err) {\r\npr_warn(DRV_PFX "Error writing new size");\r\ngoto abort;\r\n}\r\nerr = xenbus_printf(xbt, dev->nodename, "state", "%d", dev->state);\r\nif (err) {\r\npr_warn(DRV_PFX "Error writing the state");\r\ngoto abort;\r\n}\r\nerr = xenbus_transaction_end(xbt, 0);\r\nif (err == -EAGAIN)\r\ngoto again;\r\nif (err)\r\npr_warn(DRV_PFX "Error ending transaction");\r\nreturn;\r\nabort:\r\nxenbus_transaction_end(xbt, 1);\r\n}\r\nstatic void blkif_notify_work(struct xen_blkif *blkif)\r\n{\r\nblkif->waiting_reqs = 1;\r\nwake_up(&blkif->wq);\r\n}\r\nirqreturn_t xen_blkif_be_int(int irq, void *dev_id)\r\n{\r\nblkif_notify_work(dev_id);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void print_stats(struct xen_blkif *blkif)\r\n{\r\npr_info("xen-blkback (%s): oo %3d | rd %4d | wr %4d | f %4d"\r\n" | ds %4d\n",\r\ncurrent->comm, blkif->st_oo_req,\r\nblkif->st_rd_req, blkif->st_wr_req,\r\nblkif->st_f_req, blkif->st_ds_req);\r\nblkif->st_print = jiffies + msecs_to_jiffies(10 * 1000);\r\nblkif->st_rd_req = 0;\r\nblkif->st_wr_req = 0;\r\nblkif->st_oo_req = 0;\r\nblkif->st_ds_req = 0;\r\n}\r\nint xen_blkif_schedule(void *arg)\r\n{\r\nstruct xen_blkif *blkif = arg;\r\nstruct xen_vbd *vbd = &blkif->vbd;\r\nxen_blkif_get(blkif);\r\nwhile (!kthread_should_stop()) {\r\nif (try_to_freeze())\r\ncontinue;\r\nif (unlikely(vbd->size != vbd_sz(vbd)))\r\nxen_vbd_resize(blkif);\r\nwait_event_interruptible(\r\nblkif->wq,\r\nblkif->waiting_reqs || kthread_should_stop());\r\nwait_event_interruptible(\r\nblkbk->pending_free_wq,\r\n!list_empty(&blkbk->pending_free) ||\r\nkthread_should_stop());\r\nblkif->waiting_reqs = 0;\r\nsmp_mb();\r\nif (do_block_io_op(blkif))\r\nblkif->waiting_reqs = 1;\r\nif (log_stats && time_after(jiffies, blkif->st_print))\r\nprint_stats(blkif);\r\n}\r\nif (log_stats)\r\nprint_stats(blkif);\r\nblkif->xenblkd = NULL;\r\nxen_blkif_put(blkif);\r\nreturn 0;\r\n}\r\nstatic void xen_blkbk_unmap(struct pending_req *req)\r\n{\r\nstruct gnttab_unmap_grant_ref unmap[BLKIF_MAX_SEGMENTS_PER_REQUEST];\r\nstruct page *pages[BLKIF_MAX_SEGMENTS_PER_REQUEST];\r\nunsigned int i, invcount = 0;\r\ngrant_handle_t handle;\r\nint ret;\r\nfor (i = 0; i < req->nr_pages; i++) {\r\nhandle = pending_handle(req, i);\r\nif (handle == BLKBACK_INVALID_HANDLE)\r\ncontinue;\r\ngnttab_set_unmap_op(&unmap[invcount], vaddr(req, i),\r\nGNTMAP_host_map, handle);\r\npending_handle(req, i) = BLKBACK_INVALID_HANDLE;\r\npages[invcount] = virt_to_page(vaddr(req, i));\r\ninvcount++;\r\n}\r\nret = gnttab_unmap_refs(unmap, pages, invcount, false);\r\nBUG_ON(ret);\r\n}\r\nstatic int xen_blkbk_map(struct blkif_request *req,\r\nstruct pending_req *pending_req,\r\nstruct seg_buf seg[])\r\n{\r\nstruct gnttab_map_grant_ref map[BLKIF_MAX_SEGMENTS_PER_REQUEST];\r\nint i;\r\nint nseg = req->u.rw.nr_segments;\r\nint ret = 0;\r\nfor (i = 0; i < nseg; i++) {\r\nuint32_t flags;\r\nflags = GNTMAP_host_map;\r\nif (pending_req->operation != BLKIF_OP_READ)\r\nflags |= GNTMAP_readonly;\r\ngnttab_set_map_op(&map[i], vaddr(pending_req, i), flags,\r\nreq->u.rw.seg[i].gref,\r\npending_req->blkif->domid);\r\n}\r\nret = gnttab_map_refs(map, NULL, &blkbk->pending_page(pending_req, 0), nseg);\r\nBUG_ON(ret);\r\nfor (i = 0; i < nseg; i++) {\r\nif (unlikely(map[i].status != 0)) {\r\npr_debug(DRV_PFX "invalid buffer -- could not remap it\n");\r\nmap[i].handle = BLKBACK_INVALID_HANDLE;\r\nret |= 1;\r\n}\r\npending_handle(pending_req, i) = map[i].handle;\r\nif (ret)\r\ncontinue;\r\nseg[i].buf = map[i].dev_bus_addr |\r\n(req->u.rw.seg[i].first_sect << 9);\r\n}\r\nreturn ret;\r\n}\r\nstatic int dispatch_discard_io(struct xen_blkif *blkif,\r\nstruct blkif_request *req)\r\n{\r\nint err = 0;\r\nint status = BLKIF_RSP_OKAY;\r\nstruct block_device *bdev = blkif->vbd.bdev;\r\nunsigned long secure;\r\nblkif->st_ds_req++;\r\nxen_blkif_get(blkif);\r\nsecure = (blkif->vbd.discard_secure &&\r\n(req->u.discard.flag & BLKIF_DISCARD_SECURE)) ?\r\nBLKDEV_DISCARD_SECURE : 0;\r\nerr = blkdev_issue_discard(bdev, req->u.discard.sector_number,\r\nreq->u.discard.nr_sectors,\r\nGFP_KERNEL, secure);\r\nif (err == -EOPNOTSUPP) {\r\npr_debug(DRV_PFX "discard op failed, not supported\n");\r\nstatus = BLKIF_RSP_EOPNOTSUPP;\r\n} else if (err)\r\nstatus = BLKIF_RSP_ERROR;\r\nmake_response(blkif, req->u.discard.id, req->operation, status);\r\nxen_blkif_put(blkif);\r\nreturn err;\r\n}\r\nstatic void xen_blk_drain_io(struct xen_blkif *blkif)\r\n{\r\natomic_set(&blkif->drain, 1);\r\ndo {\r\nif (atomic_read(&blkif->refcnt) <= 2)\r\nbreak;\r\nwait_for_completion_interruptible_timeout(\r\n&blkif->drain_complete, HZ);\r\nif (!atomic_read(&blkif->drain))\r\nbreak;\r\n} while (!kthread_should_stop());\r\natomic_set(&blkif->drain, 0);\r\n}\r\nstatic void __end_block_io_op(struct pending_req *pending_req, int error)\r\n{\r\nif ((pending_req->operation == BLKIF_OP_FLUSH_DISKCACHE) &&\r\n(error == -EOPNOTSUPP)) {\r\npr_debug(DRV_PFX "flush diskcache op failed, not supported\n");\r\nxen_blkbk_flush_diskcache(XBT_NIL, pending_req->blkif->be, 0);\r\npending_req->status = BLKIF_RSP_EOPNOTSUPP;\r\n} else if ((pending_req->operation == BLKIF_OP_WRITE_BARRIER) &&\r\n(error == -EOPNOTSUPP)) {\r\npr_debug(DRV_PFX "write barrier op failed, not supported\n");\r\nxen_blkbk_barrier(XBT_NIL, pending_req->blkif->be, 0);\r\npending_req->status = BLKIF_RSP_EOPNOTSUPP;\r\n} else if (error) {\r\npr_debug(DRV_PFX "Buffer not up-to-date at end of operation,"\r\n" error=%d\n", error);\r\npending_req->status = BLKIF_RSP_ERROR;\r\n}\r\nif (atomic_dec_and_test(&pending_req->pendcnt)) {\r\nxen_blkbk_unmap(pending_req);\r\nmake_response(pending_req->blkif, pending_req->id,\r\npending_req->operation, pending_req->status);\r\nxen_blkif_put(pending_req->blkif);\r\nif (atomic_read(&pending_req->blkif->refcnt) <= 2) {\r\nif (atomic_read(&pending_req->blkif->drain))\r\ncomplete(&pending_req->blkif->drain_complete);\r\n}\r\nfree_req(pending_req);\r\n}\r\n}\r\nstatic void end_block_io_op(struct bio *bio, int error)\r\n{\r\n__end_block_io_op(bio->bi_private, error);\r\nbio_put(bio);\r\n}\r\nstatic int\r\n__do_block_io_op(struct xen_blkif *blkif)\r\n{\r\nunion blkif_back_rings *blk_rings = &blkif->blk_rings;\r\nstruct blkif_request req;\r\nstruct pending_req *pending_req;\r\nRING_IDX rc, rp;\r\nint more_to_do = 0;\r\nrc = blk_rings->common.req_cons;\r\nrp = blk_rings->common.sring->req_prod;\r\nrmb();\r\nwhile (rc != rp) {\r\nif (RING_REQUEST_CONS_OVERFLOW(&blk_rings->common, rc))\r\nbreak;\r\nif (kthread_should_stop()) {\r\nmore_to_do = 1;\r\nbreak;\r\n}\r\npending_req = alloc_req();\r\nif (NULL == pending_req) {\r\nblkif->st_oo_req++;\r\nmore_to_do = 1;\r\nbreak;\r\n}\r\nswitch (blkif->blk_protocol) {\r\ncase BLKIF_PROTOCOL_NATIVE:\r\nmemcpy(&req, RING_GET_REQUEST(&blk_rings->native, rc), sizeof(req));\r\nbreak;\r\ncase BLKIF_PROTOCOL_X86_32:\r\nblkif_get_x86_32_req(&req, RING_GET_REQUEST(&blk_rings->x86_32, rc));\r\nbreak;\r\ncase BLKIF_PROTOCOL_X86_64:\r\nblkif_get_x86_64_req(&req, RING_GET_REQUEST(&blk_rings->x86_64, rc));\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nblk_rings->common.req_cons = ++rc;\r\nbarrier();\r\nif (unlikely(req.operation == BLKIF_OP_DISCARD)) {\r\nfree_req(pending_req);\r\nif (dispatch_discard_io(blkif, &req))\r\nbreak;\r\n} else if (dispatch_rw_block_io(blkif, &req, pending_req))\r\nbreak;\r\ncond_resched();\r\n}\r\nreturn more_to_do;\r\n}\r\nstatic int\r\ndo_block_io_op(struct xen_blkif *blkif)\r\n{\r\nunion blkif_back_rings *blk_rings = &blkif->blk_rings;\r\nint more_to_do;\r\ndo {\r\nmore_to_do = __do_block_io_op(blkif);\r\nif (more_to_do)\r\nbreak;\r\nRING_FINAL_CHECK_FOR_REQUESTS(&blk_rings->common, more_to_do);\r\n} while (more_to_do);\r\nreturn more_to_do;\r\n}\r\nstatic int dispatch_rw_block_io(struct xen_blkif *blkif,\r\nstruct blkif_request *req,\r\nstruct pending_req *pending_req)\r\n{\r\nstruct phys_req preq;\r\nstruct seg_buf seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];\r\nunsigned int nseg;\r\nstruct bio *bio = NULL;\r\nstruct bio *biolist[BLKIF_MAX_SEGMENTS_PER_REQUEST];\r\nint i, nbio = 0;\r\nint operation;\r\nstruct blk_plug plug;\r\nbool drain = false;\r\nswitch (req->operation) {\r\ncase BLKIF_OP_READ:\r\nblkif->st_rd_req++;\r\noperation = READ;\r\nbreak;\r\ncase BLKIF_OP_WRITE:\r\nblkif->st_wr_req++;\r\noperation = WRITE_ODIRECT;\r\nbreak;\r\ncase BLKIF_OP_WRITE_BARRIER:\r\ndrain = true;\r\ncase BLKIF_OP_FLUSH_DISKCACHE:\r\nblkif->st_f_req++;\r\noperation = WRITE_FLUSH;\r\nbreak;\r\ndefault:\r\noperation = 0;\r\ngoto fail_response;\r\nbreak;\r\n}\r\nnseg = req->u.rw.nr_segments;\r\nif (unlikely(nseg == 0 && operation != WRITE_FLUSH) ||\r\nunlikely(nseg > BLKIF_MAX_SEGMENTS_PER_REQUEST)) {\r\npr_debug(DRV_PFX "Bad number of segments in request (%d)\n",\r\nnseg);\r\ngoto fail_response;\r\n}\r\npreq.dev = req->u.rw.handle;\r\npreq.sector_number = req->u.rw.sector_number;\r\npreq.nr_sects = 0;\r\npending_req->blkif = blkif;\r\npending_req->id = req->u.rw.id;\r\npending_req->operation = req->operation;\r\npending_req->status = BLKIF_RSP_OKAY;\r\npending_req->nr_pages = nseg;\r\nfor (i = 0; i < nseg; i++) {\r\nseg[i].nsec = req->u.rw.seg[i].last_sect -\r\nreq->u.rw.seg[i].first_sect + 1;\r\nif ((req->u.rw.seg[i].last_sect >= (PAGE_SIZE >> 9)) ||\r\n(req->u.rw.seg[i].last_sect < req->u.rw.seg[i].first_sect))\r\ngoto fail_response;\r\npreq.nr_sects += seg[i].nsec;\r\n}\r\nif (xen_vbd_translate(&preq, blkif, operation) != 0) {\r\npr_debug(DRV_PFX "access denied: %s of [%llu,%llu] on dev=%04x\n",\r\noperation == READ ? "read" : "write",\r\npreq.sector_number,\r\npreq.sector_number + preq.nr_sects, preq.dev);\r\ngoto fail_response;\r\n}\r\nfor (i = 0; i < nseg; i++) {\r\nif (((int)preq.sector_number|(int)seg[i].nsec) &\r\n((bdev_logical_block_size(preq.bdev) >> 9) - 1)) {\r\npr_debug(DRV_PFX "Misaligned I/O request from domain %d",\r\nblkif->domid);\r\ngoto fail_response;\r\n}\r\n}\r\nif (drain)\r\nxen_blk_drain_io(pending_req->blkif);\r\nif (xen_blkbk_map(req, pending_req, seg))\r\ngoto fail_flush;\r\nxen_blkif_get(blkif);\r\nfor (i = 0; i < nseg; i++) {\r\nwhile ((bio == NULL) ||\r\n(bio_add_page(bio,\r\nblkbk->pending_page(pending_req, i),\r\nseg[i].nsec << 9,\r\nseg[i].buf & ~PAGE_MASK) == 0)) {\r\nbio = bio_alloc(GFP_KERNEL, nseg-i);\r\nif (unlikely(bio == NULL))\r\ngoto fail_put_bio;\r\nbiolist[nbio++] = bio;\r\nbio->bi_bdev = preq.bdev;\r\nbio->bi_private = pending_req;\r\nbio->bi_end_io = end_block_io_op;\r\nbio->bi_sector = preq.sector_number;\r\n}\r\npreq.sector_number += seg[i].nsec;\r\n}\r\nif (!bio) {\r\nBUG_ON(operation != WRITE_FLUSH);\r\nbio = bio_alloc(GFP_KERNEL, 0);\r\nif (unlikely(bio == NULL))\r\ngoto fail_put_bio;\r\nbiolist[nbio++] = bio;\r\nbio->bi_bdev = preq.bdev;\r\nbio->bi_private = pending_req;\r\nbio->bi_end_io = end_block_io_op;\r\n}\r\natomic_set(&pending_req->pendcnt, nbio);\r\nblk_start_plug(&plug);\r\nfor (i = 0; i < nbio; i++)\r\nsubmit_bio(operation, biolist[i]);\r\nblk_finish_plug(&plug);\r\nif (operation == READ)\r\nblkif->st_rd_sect += preq.nr_sects;\r\nelse if (operation & WRITE)\r\nblkif->st_wr_sect += preq.nr_sects;\r\nreturn 0;\r\nfail_flush:\r\nxen_blkbk_unmap(pending_req);\r\nfail_response:\r\nmake_response(blkif, req->u.rw.id, req->operation, BLKIF_RSP_ERROR);\r\nfree_req(pending_req);\r\nmsleep(1);\r\nreturn -EIO;\r\nfail_put_bio:\r\nfor (i = 0; i < nbio; i++)\r\nbio_put(biolist[i]);\r\n__end_block_io_op(pending_req, -EINVAL);\r\nmsleep(1);\r\nreturn -EIO;\r\n}\r\nstatic void make_response(struct xen_blkif *blkif, u64 id,\r\nunsigned short op, int st)\r\n{\r\nstruct blkif_response resp;\r\nunsigned long flags;\r\nunion blkif_back_rings *blk_rings = &blkif->blk_rings;\r\nint notify;\r\nresp.id = id;\r\nresp.operation = op;\r\nresp.status = st;\r\nspin_lock_irqsave(&blkif->blk_ring_lock, flags);\r\nswitch (blkif->blk_protocol) {\r\ncase BLKIF_PROTOCOL_NATIVE:\r\nmemcpy(RING_GET_RESPONSE(&blk_rings->native, blk_rings->native.rsp_prod_pvt),\r\n&resp, sizeof(resp));\r\nbreak;\r\ncase BLKIF_PROTOCOL_X86_32:\r\nmemcpy(RING_GET_RESPONSE(&blk_rings->x86_32, blk_rings->x86_32.rsp_prod_pvt),\r\n&resp, sizeof(resp));\r\nbreak;\r\ncase BLKIF_PROTOCOL_X86_64:\r\nmemcpy(RING_GET_RESPONSE(&blk_rings->x86_64, blk_rings->x86_64.rsp_prod_pvt),\r\n&resp, sizeof(resp));\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nblk_rings->common.rsp_prod_pvt++;\r\nRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&blk_rings->common, notify);\r\nspin_unlock_irqrestore(&blkif->blk_ring_lock, flags);\r\nif (notify)\r\nnotify_remote_via_irq(blkif->irq);\r\n}\r\nstatic int __init xen_blkif_init(void)\r\n{\r\nint i, mmap_pages;\r\nint rc = 0;\r\nif (!xen_domain())\r\nreturn -ENODEV;\r\nblkbk = kzalloc(sizeof(struct xen_blkbk), GFP_KERNEL);\r\nif (!blkbk) {\r\npr_alert(DRV_PFX "%s: out of memory!\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nmmap_pages = xen_blkif_reqs * BLKIF_MAX_SEGMENTS_PER_REQUEST;\r\nblkbk->pending_reqs = kzalloc(sizeof(blkbk->pending_reqs[0]) *\r\nxen_blkif_reqs, GFP_KERNEL);\r\nblkbk->pending_grant_handles = kmalloc(sizeof(blkbk->pending_grant_handles[0]) *\r\nmmap_pages, GFP_KERNEL);\r\nblkbk->pending_pages = kzalloc(sizeof(blkbk->pending_pages[0]) *\r\nmmap_pages, GFP_KERNEL);\r\nif (!blkbk->pending_reqs || !blkbk->pending_grant_handles ||\r\n!blkbk->pending_pages) {\r\nrc = -ENOMEM;\r\ngoto out_of_memory;\r\n}\r\nfor (i = 0; i < mmap_pages; i++) {\r\nblkbk->pending_grant_handles[i] = BLKBACK_INVALID_HANDLE;\r\nblkbk->pending_pages[i] = alloc_page(GFP_KERNEL);\r\nif (blkbk->pending_pages[i] == NULL) {\r\nrc = -ENOMEM;\r\ngoto out_of_memory;\r\n}\r\n}\r\nrc = xen_blkif_interface_init();\r\nif (rc)\r\ngoto failed_init;\r\nINIT_LIST_HEAD(&blkbk->pending_free);\r\nspin_lock_init(&blkbk->pending_free_lock);\r\ninit_waitqueue_head(&blkbk->pending_free_wq);\r\nfor (i = 0; i < xen_blkif_reqs; i++)\r\nlist_add_tail(&blkbk->pending_reqs[i].free_list,\r\n&blkbk->pending_free);\r\nrc = xen_blkif_xenbus_init();\r\nif (rc)\r\ngoto failed_init;\r\nreturn 0;\r\nout_of_memory:\r\npr_alert(DRV_PFX "%s: out of memory\n", __func__);\r\nfailed_init:\r\nkfree(blkbk->pending_reqs);\r\nkfree(blkbk->pending_grant_handles);\r\nif (blkbk->pending_pages) {\r\nfor (i = 0; i < mmap_pages; i++) {\r\nif (blkbk->pending_pages[i])\r\n__free_page(blkbk->pending_pages[i]);\r\n}\r\nkfree(blkbk->pending_pages);\r\n}\r\nkfree(blkbk);\r\nblkbk = NULL;\r\nreturn rc;\r\n}
