static void vl15_watchdog_enq(struct ipath_devdata *dd)\r\n{\r\nif (atomic_inc_return(&dd->ipath_sdma_vl15_count) == 1) {\r\nunsigned long interval = (HZ + 19) / 20;\r\ndd->ipath_sdma_vl15_timer.expires = jiffies + interval;\r\nadd_timer(&dd->ipath_sdma_vl15_timer);\r\n}\r\n}\r\nstatic void vl15_watchdog_deq(struct ipath_devdata *dd)\r\n{\r\nif (atomic_dec_return(&dd->ipath_sdma_vl15_count) != 0) {\r\nunsigned long interval = (HZ + 19) / 20;\r\nmod_timer(&dd->ipath_sdma_vl15_timer, jiffies + interval);\r\n} else {\r\ndel_timer(&dd->ipath_sdma_vl15_timer);\r\n}\r\n}\r\nstatic void vl15_watchdog_timeout(unsigned long opaque)\r\n{\r\nstruct ipath_devdata *dd = (struct ipath_devdata *)opaque;\r\nif (atomic_read(&dd->ipath_sdma_vl15_count) != 0) {\r\nipath_dbg("vl15 watchdog timeout - clearing\n");\r\nipath_cancel_sends(dd, 1);\r\nipath_hol_down(dd);\r\n} else {\r\nipath_dbg("vl15 watchdog timeout - "\r\n"condition already cleared\n");\r\n}\r\n}\r\nstatic void unmap_desc(struct ipath_devdata *dd, unsigned head)\r\n{\r\n__le64 *descqp = &dd->ipath_sdma_descq[head].qw[0];\r\nu64 desc[2];\r\ndma_addr_t addr;\r\nsize_t len;\r\ndesc[0] = le64_to_cpu(descqp[0]);\r\ndesc[1] = le64_to_cpu(descqp[1]);\r\naddr = (desc[1] << 32) | (desc[0] >> 32);\r\nlen = (desc[0] >> 14) & (0x7ffULL << 2);\r\ndma_unmap_single(&dd->pcidev->dev, addr, len, DMA_TO_DEVICE);\r\n}\r\nint ipath_sdma_make_progress(struct ipath_devdata *dd)\r\n{\r\nstruct list_head *lp = NULL;\r\nstruct ipath_sdma_txreq *txp = NULL;\r\nu16 dmahead;\r\nu16 start_idx = 0;\r\nint progress = 0;\r\nif (!list_empty(&dd->ipath_sdma_activelist)) {\r\nlp = dd->ipath_sdma_activelist.next;\r\ntxp = list_entry(lp, struct ipath_sdma_txreq, list);\r\nstart_idx = txp->start_idx;\r\n}\r\ndmahead = (u16)ipath_read_kreg32(dd, dd->ipath_kregs->kr_senddmahead);\r\nif (dmahead >= dd->ipath_sdma_descq_cnt)\r\ngoto done;\r\nwhile (dd->ipath_sdma_descq_head != dmahead) {\r\nif (txp && txp->flags & IPATH_SDMA_TXREQ_F_FREEDESC &&\r\ndd->ipath_sdma_descq_head == start_idx) {\r\nunmap_desc(dd, dd->ipath_sdma_descq_head);\r\nstart_idx++;\r\nif (start_idx == dd->ipath_sdma_descq_cnt)\r\nstart_idx = 0;\r\n}\r\ndd->ipath_sdma_descq_removed++;\r\nif (++dd->ipath_sdma_descq_head == dd->ipath_sdma_descq_cnt)\r\ndd->ipath_sdma_descq_head = 0;\r\nif (txp && txp->next_descq_idx == dd->ipath_sdma_descq_head) {\r\nif (txp->flags & IPATH_SDMA_TXREQ_F_VL15)\r\nvl15_watchdog_deq(dd);\r\nlist_move_tail(lp, &dd->ipath_sdma_notifylist);\r\nif (!list_empty(&dd->ipath_sdma_activelist)) {\r\nlp = dd->ipath_sdma_activelist.next;\r\ntxp = list_entry(lp, struct ipath_sdma_txreq,\r\nlist);\r\nstart_idx = txp->start_idx;\r\n} else {\r\nlp = NULL;\r\ntxp = NULL;\r\n}\r\n}\r\nprogress = 1;\r\n}\r\nif (progress)\r\ntasklet_hi_schedule(&dd->ipath_sdma_notify_task);\r\ndone:\r\nreturn progress;\r\n}\r\nstatic void ipath_sdma_notify(struct ipath_devdata *dd, struct list_head *list)\r\n{\r\nstruct ipath_sdma_txreq *txp, *txp_next;\r\nlist_for_each_entry_safe(txp, txp_next, list, list) {\r\nlist_del_init(&txp->list);\r\nif (txp->callback)\r\n(*txp->callback)(txp->callback_cookie,\r\ntxp->callback_status);\r\n}\r\n}\r\nstatic void sdma_notify_taskbody(struct ipath_devdata *dd)\r\n{\r\nunsigned long flags;\r\nstruct list_head list;\r\nINIT_LIST_HEAD(&list);\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\nlist_splice_init(&dd->ipath_sdma_notifylist, &list);\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\nipath_sdma_notify(dd, &list);\r\nipath_ib_piobufavail(dd->verbs_dev);\r\n}\r\nstatic void sdma_notify_task(unsigned long opaque)\r\n{\r\nstruct ipath_devdata *dd = (struct ipath_devdata *)opaque;\r\nif (!test_bit(IPATH_SDMA_SHUTDOWN, &dd->ipath_sdma_status))\r\nsdma_notify_taskbody(dd);\r\n}\r\nstatic void dump_sdma_state(struct ipath_devdata *dd)\r\n{\r\nunsigned long reg;\r\nreg = ipath_read_kreg64(dd, dd->ipath_kregs->kr_senddmastatus);\r\nipath_cdbg(VERBOSE, "kr_senddmastatus: 0x%016lx\n", reg);\r\nreg = ipath_read_kreg64(dd, dd->ipath_kregs->kr_sendctrl);\r\nipath_cdbg(VERBOSE, "kr_sendctrl: 0x%016lx\n", reg);\r\nreg = ipath_read_kreg64(dd, dd->ipath_kregs->kr_senddmabufmask0);\r\nipath_cdbg(VERBOSE, "kr_senddmabufmask0: 0x%016lx\n", reg);\r\nreg = ipath_read_kreg64(dd, dd->ipath_kregs->kr_senddmabufmask1);\r\nipath_cdbg(VERBOSE, "kr_senddmabufmask1: 0x%016lx\n", reg);\r\nreg = ipath_read_kreg64(dd, dd->ipath_kregs->kr_senddmabufmask2);\r\nipath_cdbg(VERBOSE, "kr_senddmabufmask2: 0x%016lx\n", reg);\r\nreg = ipath_read_kreg64(dd, dd->ipath_kregs->kr_senddmatail);\r\nipath_cdbg(VERBOSE, "kr_senddmatail: 0x%016lx\n", reg);\r\nreg = ipath_read_kreg64(dd, dd->ipath_kregs->kr_senddmahead);\r\nipath_cdbg(VERBOSE, "kr_senddmahead: 0x%016lx\n", reg);\r\n}\r\nstatic void sdma_abort_task(unsigned long opaque)\r\n{\r\nstruct ipath_devdata *dd = (struct ipath_devdata *) opaque;\r\nu64 status;\r\nunsigned long flags;\r\nif (test_bit(IPATH_SDMA_SHUTDOWN, &dd->ipath_sdma_status))\r\nreturn;\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\nstatus = dd->ipath_sdma_status & IPATH_SDMA_ABORT_MASK;\r\nif (status == IPATH_SDMA_ABORT_NONE)\r\ngoto unlock;\r\nif (status == IPATH_SDMA_ABORT_DISARMED) {\r\nif (jiffies < dd->ipath_sdma_abort_intr_timeout)\r\ngoto resched_noprint;\r\nipath_dbg("give up waiting for SDMADISABLED intr\n");\r\n__set_bit(IPATH_SDMA_DISABLED, &dd->ipath_sdma_status);\r\nstatus = IPATH_SDMA_ABORT_ABORTED;\r\n}\r\nif (status == IPATH_SDMA_ABORT_ABORTED) {\r\nstruct ipath_sdma_txreq *txp, *txpnext;\r\nu64 hwstatus;\r\nint notify = 0;\r\nhwstatus = ipath_read_kreg64(dd,\r\ndd->ipath_kregs->kr_senddmastatus);\r\nif ((hwstatus & (IPATH_SDMA_STATUS_SCORE_BOARD_DRAIN_IN_PROG |\r\nIPATH_SDMA_STATUS_ABORT_IN_PROG |\r\nIPATH_SDMA_STATUS_INTERNAL_SDMA_ENABLE)) ||\r\n!(hwstatus & IPATH_SDMA_STATUS_SCB_EMPTY)) {\r\nif (dd->ipath_sdma_reset_wait > 0) {\r\n--dd->ipath_sdma_reset_wait;\r\ngoto resched;\r\n}\r\nipath_cdbg(VERBOSE, "gave up waiting for quiescent "\r\n"status after SDMA reset, continuing\n");\r\ndump_sdma_state(dd);\r\n}\r\nlist_for_each_entry_safe(txp, txpnext,\r\n&dd->ipath_sdma_activelist, list) {\r\ntxp->callback_status = IPATH_SDMA_TXREQ_S_ABORTED;\r\nif (txp->flags & IPATH_SDMA_TXREQ_F_VL15)\r\nvl15_watchdog_deq(dd);\r\nlist_move_tail(&txp->list, &dd->ipath_sdma_notifylist);\r\nnotify = 1;\r\n}\r\nif (notify)\r\ntasklet_hi_schedule(&dd->ipath_sdma_notify_task);\r\ndd->ipath_sdma_descq_tail = 0;\r\ndd->ipath_sdma_descq_head = 0;\r\ndd->ipath_sdma_head_dma[0] = 0;\r\ndd->ipath_sdma_generation = 0;\r\ndd->ipath_sdma_descq_removed = dd->ipath_sdma_descq_added;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmalengen,\r\n(u64) dd->ipath_sdma_descq_cnt | (1ULL << 18));\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\nspin_lock_irqsave(&dd->ipath_sendctrl_lock, flags);\r\ndd->ipath_sendctrl &= ~INFINIPATH_S_SDMAENABLE;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_sendctrl,\r\ndd->ipath_sendctrl);\r\nipath_read_kreg64(dd, dd->ipath_kregs->kr_scratch);\r\nspin_unlock_irqrestore(&dd->ipath_sendctrl_lock, flags);\r\ndd->ipath_sdma_abort_jiffies = 0;\r\nif (dd->ipath_flags & IPATH_LINKACTIVE)\r\nipath_restart_sdma(dd);\r\ngoto done;\r\n}\r\nresched:\r\nif (jiffies > dd->ipath_sdma_abort_jiffies) {\r\nipath_dbg("looping with status 0x%08lx\n",\r\ndd->ipath_sdma_status);\r\ndd->ipath_sdma_abort_jiffies = jiffies + 5 * HZ;\r\n}\r\nresched_noprint:\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\nif (!test_bit(IPATH_SDMA_SHUTDOWN, &dd->ipath_sdma_status))\r\ntasklet_hi_schedule(&dd->ipath_sdma_abort_task);\r\nreturn;\r\nunlock:\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\ndone:\r\nreturn;\r\n}\r\nvoid ipath_sdma_intr(struct ipath_devdata *dd)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\n(void) ipath_sdma_make_progress(dd);\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\n}\r\nstatic int alloc_sdma(struct ipath_devdata *dd)\r\n{\r\nint ret = 0;\r\ndd->ipath_sdma_descq = dma_alloc_coherent(&dd->pcidev->dev,\r\nSDMA_DESCQ_SZ, &dd->ipath_sdma_descq_phys, GFP_KERNEL);\r\nif (!dd->ipath_sdma_descq) {\r\nipath_dev_err(dd, "failed to allocate SendDMA descriptor "\r\n"FIFO memory\n");\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\ndd->ipath_sdma_descq_cnt =\r\nSDMA_DESCQ_SZ / sizeof(struct ipath_sdma_desc);\r\ndd->ipath_sdma_head_dma = dma_alloc_coherent(&dd->pcidev->dev,\r\nPAGE_SIZE, &dd->ipath_sdma_head_phys, GFP_KERNEL);\r\nif (!dd->ipath_sdma_head_dma) {\r\nipath_dev_err(dd, "failed to allocate SendDMA head memory\n");\r\nret = -ENOMEM;\r\ngoto cleanup_descq;\r\n}\r\ndd->ipath_sdma_head_dma[0] = 0;\r\ninit_timer(&dd->ipath_sdma_vl15_timer);\r\ndd->ipath_sdma_vl15_timer.function = vl15_watchdog_timeout;\r\ndd->ipath_sdma_vl15_timer.data = (unsigned long)dd;\r\natomic_set(&dd->ipath_sdma_vl15_count, 0);\r\ngoto done;\r\ncleanup_descq:\r\ndma_free_coherent(&dd->pcidev->dev, SDMA_DESCQ_SZ,\r\n(void *)dd->ipath_sdma_descq, dd->ipath_sdma_descq_phys);\r\ndd->ipath_sdma_descq = NULL;\r\ndd->ipath_sdma_descq_phys = 0;\r\ndone:\r\nreturn ret;\r\n}\r\nint setup_sdma(struct ipath_devdata *dd)\r\n{\r\nint ret = 0;\r\nunsigned i, n;\r\nu64 tmp64;\r\nu64 senddmabufmask[3] = { 0 };\r\nunsigned long flags;\r\nret = alloc_sdma(dd);\r\nif (ret)\r\ngoto done;\r\nif (!dd->ipath_sdma_descq) {\r\nipath_dev_err(dd, "SendDMA memory not allocated\n");\r\ngoto done;\r\n}\r\ndd->ipath_sdma_status = IPATH_SDMA_ABORT_ABORTED;\r\ndd->ipath_sdma_abort_jiffies = 0;\r\ndd->ipath_sdma_generation = 0;\r\ndd->ipath_sdma_descq_tail = 0;\r\ndd->ipath_sdma_descq_head = 0;\r\ndd->ipath_sdma_descq_removed = 0;\r\ndd->ipath_sdma_descq_added = 0;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmabase,\r\ndd->ipath_sdma_descq_phys);\r\ntmp64 = dd->ipath_sdma_descq_cnt;\r\ntmp64 |= 1<<18;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmalengen, tmp64);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmatail,\r\ndd->ipath_sdma_descq_tail);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmaheadaddr,\r\ndd->ipath_sdma_head_phys);\r\nn = dd->ipath_piobcnt2k + dd->ipath_piobcnt4k;\r\ni = dd->ipath_lastport_piobuf + dd->ipath_pioreserved;\r\nipath_chg_pioavailkernel(dd, i, n - i , 0);\r\nfor (; i < n; ++i) {\r\nunsigned word = i / 64;\r\nunsigned bit = i & 63;\r\nBUG_ON(word >= 3);\r\nsenddmabufmask[word] |= 1ULL << bit;\r\n}\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmabufmask0,\r\nsenddmabufmask[0]);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmabufmask1,\r\nsenddmabufmask[1]);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmabufmask2,\r\nsenddmabufmask[2]);\r\nINIT_LIST_HEAD(&dd->ipath_sdma_activelist);\r\nINIT_LIST_HEAD(&dd->ipath_sdma_notifylist);\r\ntasklet_init(&dd->ipath_sdma_notify_task, sdma_notify_task,\r\n(unsigned long) dd);\r\ntasklet_init(&dd->ipath_sdma_abort_task, sdma_abort_task,\r\n(unsigned long) dd);\r\nspin_lock_irqsave(&dd->ipath_sendctrl_lock, flags);\r\ndd->ipath_sendctrl |= INFINIPATH_S_SDMAINTENABLE;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_sendctrl, dd->ipath_sendctrl);\r\nipath_read_kreg64(dd, dd->ipath_kregs->kr_scratch);\r\n__set_bit(IPATH_SDMA_RUNNING, &dd->ipath_sdma_status);\r\nspin_unlock_irqrestore(&dd->ipath_sendctrl_lock, flags);\r\ndone:\r\nreturn ret;\r\n}\r\nvoid teardown_sdma(struct ipath_devdata *dd)\r\n{\r\nstruct ipath_sdma_txreq *txp, *txpnext;\r\nunsigned long flags;\r\ndma_addr_t sdma_head_phys = 0;\r\ndma_addr_t sdma_descq_phys = 0;\r\nvoid *sdma_descq = NULL;\r\nvoid *sdma_head_dma = NULL;\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\n__clear_bit(IPATH_SDMA_RUNNING, &dd->ipath_sdma_status);\r\n__set_bit(IPATH_SDMA_ABORTING, &dd->ipath_sdma_status);\r\n__set_bit(IPATH_SDMA_SHUTDOWN, &dd->ipath_sdma_status);\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\ntasklet_kill(&dd->ipath_sdma_abort_task);\r\ntasklet_kill(&dd->ipath_sdma_notify_task);\r\nspin_lock_irqsave(&dd->ipath_sendctrl_lock, flags);\r\ndd->ipath_sendctrl &= ~INFINIPATH_S_SDMAENABLE;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_sendctrl,\r\ndd->ipath_sendctrl);\r\nipath_read_kreg64(dd, dd->ipath_kregs->kr_scratch);\r\nspin_unlock_irqrestore(&dd->ipath_sendctrl_lock, flags);\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\nlist_for_each_entry_safe(txp, txpnext, &dd->ipath_sdma_activelist,\r\nlist) {\r\ntxp->callback_status = IPATH_SDMA_TXREQ_S_SHUTDOWN;\r\nif (txp->flags & IPATH_SDMA_TXREQ_F_VL15)\r\nvl15_watchdog_deq(dd);\r\nlist_move_tail(&txp->list, &dd->ipath_sdma_notifylist);\r\n}\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\nsdma_notify_taskbody(dd);\r\ndel_timer_sync(&dd->ipath_sdma_vl15_timer);\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\ndd->ipath_sdma_abort_jiffies = 0;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmabase, 0);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmalengen, 0);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmatail, 0);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmaheadaddr, 0);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmabufmask0, 0);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmabufmask1, 0);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmabufmask2, 0);\r\nif (dd->ipath_sdma_head_dma) {\r\nsdma_head_dma = (void *) dd->ipath_sdma_head_dma;\r\nsdma_head_phys = dd->ipath_sdma_head_phys;\r\ndd->ipath_sdma_head_dma = NULL;\r\ndd->ipath_sdma_head_phys = 0;\r\n}\r\nif (dd->ipath_sdma_descq) {\r\nsdma_descq = dd->ipath_sdma_descq;\r\nsdma_descq_phys = dd->ipath_sdma_descq_phys;\r\ndd->ipath_sdma_descq = NULL;\r\ndd->ipath_sdma_descq_phys = 0;\r\n}\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\nif (sdma_head_dma)\r\ndma_free_coherent(&dd->pcidev->dev, PAGE_SIZE,\r\nsdma_head_dma, sdma_head_phys);\r\nif (sdma_descq)\r\ndma_free_coherent(&dd->pcidev->dev, SDMA_DESCQ_SZ,\r\nsdma_descq, sdma_descq_phys);\r\n}\r\nvoid ipath_restart_sdma(struct ipath_devdata *dd)\r\n{\r\nunsigned long flags;\r\nint needed = 1;\r\nif (!(dd->ipath_flags & IPATH_HAS_SEND_DMA))\r\ngoto bail;\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\nif (!test_bit(IPATH_SDMA_RUNNING, &dd->ipath_sdma_status)\r\n|| test_bit(IPATH_SDMA_SHUTDOWN, &dd->ipath_sdma_status))\r\nneeded = 0;\r\nelse {\r\n__clear_bit(IPATH_SDMA_DISABLED, &dd->ipath_sdma_status);\r\n__clear_bit(IPATH_SDMA_DISARMED, &dd->ipath_sdma_status);\r\n__clear_bit(IPATH_SDMA_ABORTING, &dd->ipath_sdma_status);\r\n}\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\nif (!needed) {\r\nipath_dbg("invalid attempt to restart SDMA, status 0x%08lx\n",\r\ndd->ipath_sdma_status);\r\ngoto bail;\r\n}\r\nspin_lock_irqsave(&dd->ipath_sendctrl_lock, flags);\r\ndd->ipath_sendctrl &= ~INFINIPATH_S_SDMAENABLE;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_sendctrl, dd->ipath_sendctrl);\r\nipath_read_kreg64(dd, dd->ipath_kregs->kr_scratch);\r\ndd->ipath_sendctrl |= INFINIPATH_S_SDMAENABLE;\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_sendctrl, dd->ipath_sendctrl);\r\nipath_read_kreg64(dd, dd->ipath_kregs->kr_scratch);\r\nspin_unlock_irqrestore(&dd->ipath_sendctrl_lock, flags);\r\nipath_ib_piobufavail(dd->verbs_dev);\r\nbail:\r\nreturn;\r\n}\r\nstatic inline void make_sdma_desc(struct ipath_devdata *dd,\r\nu64 *sdmadesc, u64 addr, u64 dwlen, u64 dwoffset)\r\n{\r\nWARN_ON(addr & 3);\r\nsdmadesc[1] = addr >> 32;\r\nsdmadesc[0] = (addr & 0xfffffffcULL) << 32;\r\nsdmadesc[0] |= (dd->ipath_sdma_generation & 3ULL) << 30;\r\nsdmadesc[0] |= (dwlen & 0x7ffULL) << 16;\r\nsdmadesc[0] |= dwoffset & 0x7ffULL;\r\n}\r\nint ipath_sdma_verbs_send(struct ipath_devdata *dd,\r\nstruct ipath_sge_state *ss, u32 dwords,\r\nstruct ipath_verbs_txreq *tx)\r\n{\r\nunsigned long flags;\r\nstruct ipath_sge *sge;\r\nint ret = 0;\r\nu16 tail;\r\n__le64 *descqp;\r\nu64 sdmadesc[2];\r\nu32 dwoffset;\r\ndma_addr_t addr;\r\nif ((tx->map_len + (dwords<<2)) > dd->ipath_ibmaxlen) {\r\nipath_dbg("packet size %X > ibmax %X, fail\n",\r\ntx->map_len + (dwords<<2), dd->ipath_ibmaxlen);\r\nret = -EMSGSIZE;\r\ngoto fail;\r\n}\r\nspin_lock_irqsave(&dd->ipath_sdma_lock, flags);\r\nretry:\r\nif (unlikely(test_bit(IPATH_SDMA_ABORTING, &dd->ipath_sdma_status))) {\r\nret = -EBUSY;\r\ngoto unlock;\r\n}\r\nif (tx->txreq.sg_count > ipath_sdma_descq_freecnt(dd)) {\r\nif (ipath_sdma_make_progress(dd))\r\ngoto retry;\r\nret = -ENOBUFS;\r\ngoto unlock;\r\n}\r\naddr = dma_map_single(&dd->pcidev->dev, tx->txreq.map_addr,\r\ntx->map_len, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&dd->pcidev->dev, addr))\r\ngoto ioerr;\r\ndwoffset = tx->map_len >> 2;\r\nmake_sdma_desc(dd, sdmadesc, (u64) addr, dwoffset, 0);\r\nsdmadesc[0] |= 1ULL << 12;\r\nif (tx->txreq.flags & IPATH_SDMA_TXREQ_F_USELARGEBUF)\r\nsdmadesc[0] |= 1ULL << 14;\r\ntail = dd->ipath_sdma_descq_tail;\r\ndescqp = &dd->ipath_sdma_descq[tail].qw[0];\r\n*descqp++ = cpu_to_le64(sdmadesc[0]);\r\n*descqp++ = cpu_to_le64(sdmadesc[1]);\r\nif (tx->txreq.flags & IPATH_SDMA_TXREQ_F_FREEDESC)\r\ntx->txreq.start_idx = tail;\r\nif (++tail == dd->ipath_sdma_descq_cnt) {\r\ntail = 0;\r\ndescqp = &dd->ipath_sdma_descq[0].qw[0];\r\n++dd->ipath_sdma_generation;\r\n}\r\nsge = &ss->sge;\r\nwhile (dwords) {\r\nu32 dw;\r\nu32 len;\r\nlen = dwords << 2;\r\nif (len > sge->length)\r\nlen = sge->length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\ndw = (len + 3) >> 2;\r\naddr = dma_map_single(&dd->pcidev->dev, sge->vaddr, dw << 2,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(&dd->pcidev->dev, addr))\r\ngoto unmap;\r\nmake_sdma_desc(dd, sdmadesc, (u64) addr, dw, dwoffset);\r\nif (tx->txreq.flags & IPATH_SDMA_TXREQ_F_USELARGEBUF)\r\nsdmadesc[0] |= 1ULL << 14;\r\n*descqp++ = cpu_to_le64(sdmadesc[0]);\r\n*descqp++ = cpu_to_le64(sdmadesc[1]);\r\nif (++tail == dd->ipath_sdma_descq_cnt) {\r\ntail = 0;\r\ndescqp = &dd->ipath_sdma_descq[0].qw[0];\r\n++dd->ipath_sdma_generation;\r\n}\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr != NULL) {\r\nif (++sge->n >= IPATH_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\ndwoffset += dw;\r\ndwords -= dw;\r\n}\r\nif (!tail)\r\ndescqp = &dd->ipath_sdma_descq[dd->ipath_sdma_descq_cnt].qw[0];\r\ndescqp -= 2;\r\ndescqp[0] |= cpu_to_le64(1ULL << 11);\r\nif (tx->txreq.flags & IPATH_SDMA_TXREQ_F_INTREQ) {\r\ndescqp[0] |= cpu_to_le64(1ULL << 15);\r\n}\r\nwmb();\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_senddmatail, tail);\r\ntx->txreq.next_descq_idx = tail;\r\ntx->txreq.callback_status = IPATH_SDMA_TXREQ_S_OK;\r\ndd->ipath_sdma_descq_tail = tail;\r\ndd->ipath_sdma_descq_added += tx->txreq.sg_count;\r\nlist_add_tail(&tx->txreq.list, &dd->ipath_sdma_activelist);\r\nif (tx->txreq.flags & IPATH_SDMA_TXREQ_F_VL15)\r\nvl15_watchdog_enq(dd);\r\ngoto unlock;\r\nunmap:\r\nwhile (tail != dd->ipath_sdma_descq_tail) {\r\nif (!tail)\r\ntail = dd->ipath_sdma_descq_cnt - 1;\r\nelse\r\ntail--;\r\nunmap_desc(dd, tail);\r\n}\r\nioerr:\r\nret = -EIO;\r\nunlock:\r\nspin_unlock_irqrestore(&dd->ipath_sdma_lock, flags);\r\nfail:\r\nreturn ret;\r\n}
