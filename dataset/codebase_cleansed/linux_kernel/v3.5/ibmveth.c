static inline u32 ibmveth_rxq_flags(struct ibmveth_adapter *adapter)\r\n{\r\nreturn adapter->rx_queue.queue_addr[adapter->rx_queue.index].flags_off;\r\n}\r\nstatic inline int ibmveth_rxq_toggle(struct ibmveth_adapter *adapter)\r\n{\r\nreturn (ibmveth_rxq_flags(adapter) & IBMVETH_RXQ_TOGGLE) >>\r\nIBMVETH_RXQ_TOGGLE_SHIFT;\r\n}\r\nstatic inline int ibmveth_rxq_pending_buffer(struct ibmveth_adapter *adapter)\r\n{\r\nreturn ibmveth_rxq_toggle(adapter) == adapter->rx_queue.toggle;\r\n}\r\nstatic inline int ibmveth_rxq_buffer_valid(struct ibmveth_adapter *adapter)\r\n{\r\nreturn ibmveth_rxq_flags(adapter) & IBMVETH_RXQ_VALID;\r\n}\r\nstatic inline int ibmveth_rxq_frame_offset(struct ibmveth_adapter *adapter)\r\n{\r\nreturn ibmveth_rxq_flags(adapter) & IBMVETH_RXQ_OFF_MASK;\r\n}\r\nstatic inline int ibmveth_rxq_frame_length(struct ibmveth_adapter *adapter)\r\n{\r\nreturn adapter->rx_queue.queue_addr[adapter->rx_queue.index].length;\r\n}\r\nstatic inline int ibmveth_rxq_csum_good(struct ibmveth_adapter *adapter)\r\n{\r\nreturn ibmveth_rxq_flags(adapter) & IBMVETH_RXQ_CSUM_GOOD;\r\n}\r\nstatic void ibmveth_init_buffer_pool(struct ibmveth_buff_pool *pool,\r\nu32 pool_index, u32 pool_size,\r\nu32 buff_size, u32 pool_active)\r\n{\r\npool->size = pool_size;\r\npool->index = pool_index;\r\npool->buff_size = buff_size;\r\npool->threshold = pool_size * 7 / 8;\r\npool->active = pool_active;\r\n}\r\nstatic int ibmveth_alloc_buffer_pool(struct ibmveth_buff_pool *pool)\r\n{\r\nint i;\r\npool->free_map = kmalloc(sizeof(u16) * pool->size, GFP_KERNEL);\r\nif (!pool->free_map)\r\nreturn -1;\r\npool->dma_addr = kmalloc(sizeof(dma_addr_t) * pool->size, GFP_KERNEL);\r\nif (!pool->dma_addr) {\r\nkfree(pool->free_map);\r\npool->free_map = NULL;\r\nreturn -1;\r\n}\r\npool->skbuff = kcalloc(pool->size, sizeof(void *), GFP_KERNEL);\r\nif (!pool->skbuff) {\r\nkfree(pool->dma_addr);\r\npool->dma_addr = NULL;\r\nkfree(pool->free_map);\r\npool->free_map = NULL;\r\nreturn -1;\r\n}\r\nmemset(pool->dma_addr, 0, sizeof(dma_addr_t) * pool->size);\r\nfor (i = 0; i < pool->size; ++i)\r\npool->free_map[i] = i;\r\natomic_set(&pool->available, 0);\r\npool->producer_index = 0;\r\npool->consumer_index = 0;\r\nreturn 0;\r\n}\r\nstatic inline void ibmveth_flush_buffer(void *addr, unsigned long length)\r\n{\r\nunsigned long offset;\r\nfor (offset = 0; offset < length; offset += SMP_CACHE_BYTES)\r\nasm("dcbfl %0,%1" :: "b" (addr), "r" (offset));\r\n}\r\nstatic void ibmveth_replenish_buffer_pool(struct ibmveth_adapter *adapter,\r\nstruct ibmveth_buff_pool *pool)\r\n{\r\nu32 i;\r\nu32 count = pool->size - atomic_read(&pool->available);\r\nu32 buffers_added = 0;\r\nstruct sk_buff *skb;\r\nunsigned int free_index, index;\r\nu64 correlator;\r\nunsigned long lpar_rc;\r\ndma_addr_t dma_addr;\r\nmb();\r\nfor (i = 0; i < count; ++i) {\r\nunion ibmveth_buf_desc desc;\r\nskb = netdev_alloc_skb(adapter->netdev, pool->buff_size);\r\nif (!skb) {\r\nnetdev_dbg(adapter->netdev,\r\n"replenish: unable to allocate skb\n");\r\nadapter->replenish_no_mem++;\r\nbreak;\r\n}\r\nfree_index = pool->consumer_index;\r\npool->consumer_index++;\r\nif (pool->consumer_index >= pool->size)\r\npool->consumer_index = 0;\r\nindex = pool->free_map[free_index];\r\nBUG_ON(index == IBM_VETH_INVALID_MAP);\r\nBUG_ON(pool->skbuff[index] != NULL);\r\ndma_addr = dma_map_single(&adapter->vdev->dev, skb->data,\r\npool->buff_size, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(&adapter->vdev->dev, dma_addr))\r\ngoto failure;\r\npool->free_map[free_index] = IBM_VETH_INVALID_MAP;\r\npool->dma_addr[index] = dma_addr;\r\npool->skbuff[index] = skb;\r\ncorrelator = ((u64)pool->index << 32) | index;\r\n*(u64 *)skb->data = correlator;\r\ndesc.fields.flags_len = IBMVETH_BUF_VALID | pool->buff_size;\r\ndesc.fields.address = dma_addr;\r\nif (rx_flush) {\r\nunsigned int len = min(pool->buff_size,\r\nadapter->netdev->mtu +\r\nIBMVETH_BUFF_OH);\r\nibmveth_flush_buffer(skb->data, len);\r\n}\r\nlpar_rc = h_add_logical_lan_buffer(adapter->vdev->unit_address,\r\ndesc.desc);\r\nif (lpar_rc != H_SUCCESS) {\r\ngoto failure;\r\n} else {\r\nbuffers_added++;\r\nadapter->replenish_add_buff_success++;\r\n}\r\n}\r\nmb();\r\natomic_add(buffers_added, &(pool->available));\r\nreturn;\r\nfailure:\r\npool->free_map[free_index] = index;\r\npool->skbuff[index] = NULL;\r\nif (pool->consumer_index == 0)\r\npool->consumer_index = pool->size - 1;\r\nelse\r\npool->consumer_index--;\r\nif (!dma_mapping_error(&adapter->vdev->dev, dma_addr))\r\ndma_unmap_single(&adapter->vdev->dev,\r\npool->dma_addr[index], pool->buff_size,\r\nDMA_FROM_DEVICE);\r\ndev_kfree_skb_any(skb);\r\nadapter->replenish_add_buff_failure++;\r\nmb();\r\natomic_add(buffers_added, &(pool->available));\r\n}\r\nstatic void ibmveth_replenish_task(struct ibmveth_adapter *adapter)\r\n{\r\nint i;\r\nadapter->replenish_task_cycles++;\r\nfor (i = (IBMVETH_NUM_BUFF_POOLS - 1); i >= 0; i--) {\r\nstruct ibmveth_buff_pool *pool = &adapter->rx_buff_pool[i];\r\nif (pool->active &&\r\n(atomic_read(&pool->available) < pool->threshold))\r\nibmveth_replenish_buffer_pool(adapter, pool);\r\n}\r\nadapter->rx_no_buffer = *(u64 *)(((char*)adapter->buffer_list_addr) +\r\n4096 - 8);\r\n}\r\nstatic void ibmveth_free_buffer_pool(struct ibmveth_adapter *adapter,\r\nstruct ibmveth_buff_pool *pool)\r\n{\r\nint i;\r\nkfree(pool->free_map);\r\npool->free_map = NULL;\r\nif (pool->skbuff && pool->dma_addr) {\r\nfor (i = 0; i < pool->size; ++i) {\r\nstruct sk_buff *skb = pool->skbuff[i];\r\nif (skb) {\r\ndma_unmap_single(&adapter->vdev->dev,\r\npool->dma_addr[i],\r\npool->buff_size,\r\nDMA_FROM_DEVICE);\r\ndev_kfree_skb_any(skb);\r\npool->skbuff[i] = NULL;\r\n}\r\n}\r\n}\r\nif (pool->dma_addr) {\r\nkfree(pool->dma_addr);\r\npool->dma_addr = NULL;\r\n}\r\nif (pool->skbuff) {\r\nkfree(pool->skbuff);\r\npool->skbuff = NULL;\r\n}\r\n}\r\nstatic void ibmveth_remove_buffer_from_pool(struct ibmveth_adapter *adapter,\r\nu64 correlator)\r\n{\r\nunsigned int pool = correlator >> 32;\r\nunsigned int index = correlator & 0xffffffffUL;\r\nunsigned int free_index;\r\nstruct sk_buff *skb;\r\nBUG_ON(pool >= IBMVETH_NUM_BUFF_POOLS);\r\nBUG_ON(index >= adapter->rx_buff_pool[pool].size);\r\nskb = adapter->rx_buff_pool[pool].skbuff[index];\r\nBUG_ON(skb == NULL);\r\nadapter->rx_buff_pool[pool].skbuff[index] = NULL;\r\ndma_unmap_single(&adapter->vdev->dev,\r\nadapter->rx_buff_pool[pool].dma_addr[index],\r\nadapter->rx_buff_pool[pool].buff_size,\r\nDMA_FROM_DEVICE);\r\nfree_index = adapter->rx_buff_pool[pool].producer_index;\r\nadapter->rx_buff_pool[pool].producer_index++;\r\nif (adapter->rx_buff_pool[pool].producer_index >=\r\nadapter->rx_buff_pool[pool].size)\r\nadapter->rx_buff_pool[pool].producer_index = 0;\r\nadapter->rx_buff_pool[pool].free_map[free_index] = index;\r\nmb();\r\natomic_dec(&(adapter->rx_buff_pool[pool].available));\r\n}\r\nstatic inline struct sk_buff *ibmveth_rxq_get_buffer(struct ibmveth_adapter *adapter)\r\n{\r\nu64 correlator = adapter->rx_queue.queue_addr[adapter->rx_queue.index].correlator;\r\nunsigned int pool = correlator >> 32;\r\nunsigned int index = correlator & 0xffffffffUL;\r\nBUG_ON(pool >= IBMVETH_NUM_BUFF_POOLS);\r\nBUG_ON(index >= adapter->rx_buff_pool[pool].size);\r\nreturn adapter->rx_buff_pool[pool].skbuff[index];\r\n}\r\nstatic int ibmveth_rxq_recycle_buffer(struct ibmveth_adapter *adapter)\r\n{\r\nu32 q_index = adapter->rx_queue.index;\r\nu64 correlator = adapter->rx_queue.queue_addr[q_index].correlator;\r\nunsigned int pool = correlator >> 32;\r\nunsigned int index = correlator & 0xffffffffUL;\r\nunion ibmveth_buf_desc desc;\r\nunsigned long lpar_rc;\r\nint ret = 1;\r\nBUG_ON(pool >= IBMVETH_NUM_BUFF_POOLS);\r\nBUG_ON(index >= adapter->rx_buff_pool[pool].size);\r\nif (!adapter->rx_buff_pool[pool].active) {\r\nibmveth_rxq_harvest_buffer(adapter);\r\nibmveth_free_buffer_pool(adapter, &adapter->rx_buff_pool[pool]);\r\ngoto out;\r\n}\r\ndesc.fields.flags_len = IBMVETH_BUF_VALID |\r\nadapter->rx_buff_pool[pool].buff_size;\r\ndesc.fields.address = adapter->rx_buff_pool[pool].dma_addr[index];\r\nlpar_rc = h_add_logical_lan_buffer(adapter->vdev->unit_address, desc.desc);\r\nif (lpar_rc != H_SUCCESS) {\r\nnetdev_dbg(adapter->netdev, "h_add_logical_lan_buffer failed "\r\n"during recycle rc=%ld", lpar_rc);\r\nibmveth_remove_buffer_from_pool(adapter, adapter->rx_queue.queue_addr[adapter->rx_queue.index].correlator);\r\nret = 0;\r\n}\r\nif (++adapter->rx_queue.index == adapter->rx_queue.num_slots) {\r\nadapter->rx_queue.index = 0;\r\nadapter->rx_queue.toggle = !adapter->rx_queue.toggle;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic void ibmveth_rxq_harvest_buffer(struct ibmveth_adapter *adapter)\r\n{\r\nibmveth_remove_buffer_from_pool(adapter, adapter->rx_queue.queue_addr[adapter->rx_queue.index].correlator);\r\nif (++adapter->rx_queue.index == adapter->rx_queue.num_slots) {\r\nadapter->rx_queue.index = 0;\r\nadapter->rx_queue.toggle = !adapter->rx_queue.toggle;\r\n}\r\n}\r\nstatic void ibmveth_cleanup(struct ibmveth_adapter *adapter)\r\n{\r\nint i;\r\nstruct device *dev = &adapter->vdev->dev;\r\nif (adapter->buffer_list_addr != NULL) {\r\nif (!dma_mapping_error(dev, adapter->buffer_list_dma)) {\r\ndma_unmap_single(dev, adapter->buffer_list_dma, 4096,\r\nDMA_BIDIRECTIONAL);\r\nadapter->buffer_list_dma = DMA_ERROR_CODE;\r\n}\r\nfree_page((unsigned long)adapter->buffer_list_addr);\r\nadapter->buffer_list_addr = NULL;\r\n}\r\nif (adapter->filter_list_addr != NULL) {\r\nif (!dma_mapping_error(dev, adapter->filter_list_dma)) {\r\ndma_unmap_single(dev, adapter->filter_list_dma, 4096,\r\nDMA_BIDIRECTIONAL);\r\nadapter->filter_list_dma = DMA_ERROR_CODE;\r\n}\r\nfree_page((unsigned long)adapter->filter_list_addr);\r\nadapter->filter_list_addr = NULL;\r\n}\r\nif (adapter->rx_queue.queue_addr != NULL) {\r\nif (!dma_mapping_error(dev, adapter->rx_queue.queue_dma)) {\r\ndma_unmap_single(dev,\r\nadapter->rx_queue.queue_dma,\r\nadapter->rx_queue.queue_len,\r\nDMA_BIDIRECTIONAL);\r\nadapter->rx_queue.queue_dma = DMA_ERROR_CODE;\r\n}\r\nkfree(adapter->rx_queue.queue_addr);\r\nadapter->rx_queue.queue_addr = NULL;\r\n}\r\nfor (i = 0; i < IBMVETH_NUM_BUFF_POOLS; i++)\r\nif (adapter->rx_buff_pool[i].active)\r\nibmveth_free_buffer_pool(adapter,\r\n&adapter->rx_buff_pool[i]);\r\nif (adapter->bounce_buffer != NULL) {\r\nif (!dma_mapping_error(dev, adapter->bounce_buffer_dma)) {\r\ndma_unmap_single(&adapter->vdev->dev,\r\nadapter->bounce_buffer_dma,\r\nadapter->netdev->mtu + IBMVETH_BUFF_OH,\r\nDMA_BIDIRECTIONAL);\r\nadapter->bounce_buffer_dma = DMA_ERROR_CODE;\r\n}\r\nkfree(adapter->bounce_buffer);\r\nadapter->bounce_buffer = NULL;\r\n}\r\n}\r\nstatic int ibmveth_register_logical_lan(struct ibmveth_adapter *adapter,\r\nunion ibmveth_buf_desc rxq_desc, u64 mac_address)\r\n{\r\nint rc, try_again = 1;\r\nretry:\r\nrc = h_register_logical_lan(adapter->vdev->unit_address,\r\nadapter->buffer_list_dma, rxq_desc.desc,\r\nadapter->filter_list_dma, mac_address);\r\nif (rc != H_SUCCESS && try_again) {\r\ndo {\r\nrc = h_free_logical_lan(adapter->vdev->unit_address);\r\n} while (H_IS_LONG_BUSY(rc) || (rc == H_BUSY));\r\ntry_again = 0;\r\ngoto retry;\r\n}\r\nreturn rc;\r\n}\r\nstatic int ibmveth_open(struct net_device *netdev)\r\n{\r\nstruct ibmveth_adapter *adapter = netdev_priv(netdev);\r\nu64 mac_address = 0;\r\nint rxq_entries = 1;\r\nunsigned long lpar_rc;\r\nint rc;\r\nunion ibmveth_buf_desc rxq_desc;\r\nint i;\r\nstruct device *dev;\r\nnetdev_dbg(netdev, "open starting\n");\r\nnapi_enable(&adapter->napi);\r\nfor(i = 0; i < IBMVETH_NUM_BUFF_POOLS; i++)\r\nrxq_entries += adapter->rx_buff_pool[i].size;\r\nadapter->buffer_list_addr = (void*) get_zeroed_page(GFP_KERNEL);\r\nadapter->filter_list_addr = (void*) get_zeroed_page(GFP_KERNEL);\r\nif (!adapter->buffer_list_addr || !adapter->filter_list_addr) {\r\nnetdev_err(netdev, "unable to allocate filter or buffer list "\r\n"pages\n");\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nadapter->rx_queue.queue_len = sizeof(struct ibmveth_rx_q_entry) *\r\nrxq_entries;\r\nadapter->rx_queue.queue_addr = kmalloc(adapter->rx_queue.queue_len,\r\nGFP_KERNEL);\r\nif (!adapter->rx_queue.queue_addr) {\r\nnetdev_err(netdev, "unable to allocate rx queue pages\n");\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\ndev = &adapter->vdev->dev;\r\nadapter->buffer_list_dma = dma_map_single(dev,\r\nadapter->buffer_list_addr, 4096, DMA_BIDIRECTIONAL);\r\nadapter->filter_list_dma = dma_map_single(dev,\r\nadapter->filter_list_addr, 4096, DMA_BIDIRECTIONAL);\r\nadapter->rx_queue.queue_dma = dma_map_single(dev,\r\nadapter->rx_queue.queue_addr,\r\nadapter->rx_queue.queue_len, DMA_BIDIRECTIONAL);\r\nif ((dma_mapping_error(dev, adapter->buffer_list_dma)) ||\r\n(dma_mapping_error(dev, adapter->filter_list_dma)) ||\r\n(dma_mapping_error(dev, adapter->rx_queue.queue_dma))) {\r\nnetdev_err(netdev, "unable to map filter or buffer list "\r\n"pages\n");\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\nadapter->rx_queue.index = 0;\r\nadapter->rx_queue.num_slots = rxq_entries;\r\nadapter->rx_queue.toggle = 1;\r\nmemcpy(&mac_address, netdev->dev_addr, netdev->addr_len);\r\nmac_address = mac_address >> 16;\r\nrxq_desc.fields.flags_len = IBMVETH_BUF_VALID |\r\nadapter->rx_queue.queue_len;\r\nrxq_desc.fields.address = adapter->rx_queue.queue_dma;\r\nnetdev_dbg(netdev, "buffer list @ 0x%p\n", adapter->buffer_list_addr);\r\nnetdev_dbg(netdev, "filter list @ 0x%p\n", adapter->filter_list_addr);\r\nnetdev_dbg(netdev, "receive q @ 0x%p\n", adapter->rx_queue.queue_addr);\r\nh_vio_signal(adapter->vdev->unit_address, VIO_IRQ_DISABLE);\r\nlpar_rc = ibmveth_register_logical_lan(adapter, rxq_desc, mac_address);\r\nif (lpar_rc != H_SUCCESS) {\r\nnetdev_err(netdev, "h_register_logical_lan failed with %ld\n",\r\nlpar_rc);\r\nnetdev_err(netdev, "buffer TCE:0x%llx filter TCE:0x%llx rxq "\r\n"desc:0x%llx MAC:0x%llx\n",\r\nadapter->buffer_list_dma,\r\nadapter->filter_list_dma,\r\nrxq_desc.desc,\r\nmac_address);\r\nrc = -ENONET;\r\ngoto err_out;\r\n}\r\nfor (i = 0; i < IBMVETH_NUM_BUFF_POOLS; i++) {\r\nif (!adapter->rx_buff_pool[i].active)\r\ncontinue;\r\nif (ibmveth_alloc_buffer_pool(&adapter->rx_buff_pool[i])) {\r\nnetdev_err(netdev, "unable to alloc pool\n");\r\nadapter->rx_buff_pool[i].active = 0;\r\nrc = -ENOMEM;\r\ngoto err_out;\r\n}\r\n}\r\nnetdev_dbg(netdev, "registering irq 0x%x\n", netdev->irq);\r\nrc = request_irq(netdev->irq, ibmveth_interrupt, 0, netdev->name,\r\nnetdev);\r\nif (rc != 0) {\r\nnetdev_err(netdev, "unable to request irq 0x%x, rc %d\n",\r\nnetdev->irq, rc);\r\ndo {\r\nlpar_rc = h_free_logical_lan(adapter->vdev->unit_address);\r\n} while (H_IS_LONG_BUSY(lpar_rc) || (lpar_rc == H_BUSY));\r\ngoto err_out;\r\n}\r\nadapter->bounce_buffer =\r\nkmalloc(netdev->mtu + IBMVETH_BUFF_OH, GFP_KERNEL);\r\nif (!adapter->bounce_buffer) {\r\nnetdev_err(netdev, "unable to allocate bounce buffer\n");\r\nrc = -ENOMEM;\r\ngoto err_out_free_irq;\r\n}\r\nadapter->bounce_buffer_dma =\r\ndma_map_single(&adapter->vdev->dev, adapter->bounce_buffer,\r\nnetdev->mtu + IBMVETH_BUFF_OH, DMA_BIDIRECTIONAL);\r\nif (dma_mapping_error(dev, adapter->bounce_buffer_dma)) {\r\nnetdev_err(netdev, "unable to map bounce buffer\n");\r\nrc = -ENOMEM;\r\ngoto err_out_free_irq;\r\n}\r\nnetdev_dbg(netdev, "initial replenish cycle\n");\r\nibmveth_interrupt(netdev->irq, netdev);\r\nnetif_start_queue(netdev);\r\nnetdev_dbg(netdev, "open complete\n");\r\nreturn 0;\r\nerr_out_free_irq:\r\nfree_irq(netdev->irq, netdev);\r\nerr_out:\r\nibmveth_cleanup(adapter);\r\nnapi_disable(&adapter->napi);\r\nreturn rc;\r\n}\r\nstatic int ibmveth_close(struct net_device *netdev)\r\n{\r\nstruct ibmveth_adapter *adapter = netdev_priv(netdev);\r\nlong lpar_rc;\r\nnetdev_dbg(netdev, "close starting\n");\r\nnapi_disable(&adapter->napi);\r\nif (!adapter->pool_config)\r\nnetif_stop_queue(netdev);\r\nh_vio_signal(adapter->vdev->unit_address, VIO_IRQ_DISABLE);\r\ndo {\r\nlpar_rc = h_free_logical_lan(adapter->vdev->unit_address);\r\n} while (H_IS_LONG_BUSY(lpar_rc) || (lpar_rc == H_BUSY));\r\nif (lpar_rc != H_SUCCESS) {\r\nnetdev_err(netdev, "h_free_logical_lan failed with %lx, "\r\n"continuing with close\n", lpar_rc);\r\n}\r\nfree_irq(netdev->irq, netdev);\r\nadapter->rx_no_buffer = *(u64 *)(((char *)adapter->buffer_list_addr) +\r\n4096 - 8);\r\nibmveth_cleanup(adapter);\r\nnetdev_dbg(netdev, "close complete\n");\r\nreturn 0;\r\n}\r\nstatic int netdev_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\ncmd->supported = (SUPPORTED_1000baseT_Full | SUPPORTED_Autoneg |\r\nSUPPORTED_FIBRE);\r\ncmd->advertising = (ADVERTISED_1000baseT_Full | ADVERTISED_Autoneg |\r\nADVERTISED_FIBRE);\r\nethtool_cmd_speed_set(cmd, SPEED_1000);\r\ncmd->duplex = DUPLEX_FULL;\r\ncmd->port = PORT_FIBRE;\r\ncmd->phy_address = 0;\r\ncmd->transceiver = XCVR_INTERNAL;\r\ncmd->autoneg = AUTONEG_ENABLE;\r\ncmd->maxtxpkt = 0;\r\ncmd->maxrxpkt = 1;\r\nreturn 0;\r\n}\r\nstatic void netdev_get_drvinfo(struct net_device *dev,\r\nstruct ethtool_drvinfo *info)\r\n{\r\nstrncpy(info->driver, ibmveth_driver_name, sizeof(info->driver) - 1);\r\nstrncpy(info->version, ibmveth_driver_version,\r\nsizeof(info->version) - 1);\r\n}\r\nstatic netdev_features_t ibmveth_fix_features(struct net_device *dev,\r\nnetdev_features_t features)\r\n{\r\nif (!(features & NETIF_F_RXCSUM))\r\nfeatures &= ~NETIF_F_ALL_CSUM;\r\nreturn features;\r\n}\r\nstatic int ibmveth_set_csum_offload(struct net_device *dev, u32 data)\r\n{\r\nstruct ibmveth_adapter *adapter = netdev_priv(dev);\r\nunsigned long set_attr, clr_attr, ret_attr;\r\nunsigned long set_attr6, clr_attr6;\r\nlong ret, ret4, ret6;\r\nint rc1 = 0, rc2 = 0;\r\nint restart = 0;\r\nif (netif_running(dev)) {\r\nrestart = 1;\r\nadapter->pool_config = 1;\r\nibmveth_close(dev);\r\nadapter->pool_config = 0;\r\n}\r\nset_attr = 0;\r\nclr_attr = 0;\r\nset_attr6 = 0;\r\nclr_attr6 = 0;\r\nif (data) {\r\nset_attr = IBMVETH_ILLAN_IPV4_TCP_CSUM;\r\nset_attr6 = IBMVETH_ILLAN_IPV6_TCP_CSUM;\r\n} else {\r\nclr_attr = IBMVETH_ILLAN_IPV4_TCP_CSUM;\r\nclr_attr6 = IBMVETH_ILLAN_IPV6_TCP_CSUM;\r\n}\r\nret = h_illan_attributes(adapter->vdev->unit_address, 0, 0, &ret_attr);\r\nif (ret == H_SUCCESS && !(ret_attr & IBMVETH_ILLAN_ACTIVE_TRUNK) &&\r\n!(ret_attr & IBMVETH_ILLAN_TRUNK_PRI_MASK) &&\r\n(ret_attr & IBMVETH_ILLAN_PADDED_PKT_CSUM)) {\r\nret4 = h_illan_attributes(adapter->vdev->unit_address, clr_attr,\r\nset_attr, &ret_attr);\r\nif (ret4 != H_SUCCESS) {\r\nnetdev_err(dev, "unable to change IPv4 checksum "\r\n"offload settings. %d rc=%ld\n",\r\ndata, ret4);\r\nh_illan_attributes(adapter->vdev->unit_address,\r\nset_attr, clr_attr, &ret_attr);\r\nif (data == 1)\r\ndev->features &= ~NETIF_F_IP_CSUM;\r\n} else {\r\nadapter->fw_ipv4_csum_support = data;\r\n}\r\nret6 = h_illan_attributes(adapter->vdev->unit_address,\r\nclr_attr6, set_attr6, &ret_attr);\r\nif (ret6 != H_SUCCESS) {\r\nnetdev_err(dev, "unable to change IPv6 checksum "\r\n"offload settings. %d rc=%ld\n",\r\ndata, ret6);\r\nh_illan_attributes(adapter->vdev->unit_address,\r\nset_attr6, clr_attr6, &ret_attr);\r\nif (data == 1)\r\ndev->features &= ~NETIF_F_IPV6_CSUM;\r\n} else\r\nadapter->fw_ipv6_csum_support = data;\r\nif (ret4 == H_SUCCESS || ret6 == H_SUCCESS)\r\nadapter->rx_csum = data;\r\nelse\r\nrc1 = -EIO;\r\n} else {\r\nrc1 = -EIO;\r\nnetdev_err(dev, "unable to change checksum offload settings."\r\n" %d rc=%ld ret_attr=%lx\n", data, ret,\r\nret_attr);\r\n}\r\nif (restart)\r\nrc2 = ibmveth_open(dev);\r\nreturn rc1 ? rc1 : rc2;\r\n}\r\nstatic int ibmveth_set_features(struct net_device *dev,\r\nnetdev_features_t features)\r\n{\r\nstruct ibmveth_adapter *adapter = netdev_priv(dev);\r\nint rx_csum = !!(features & NETIF_F_RXCSUM);\r\nint rc;\r\nif (rx_csum == adapter->rx_csum)\r\nreturn 0;\r\nrc = ibmveth_set_csum_offload(dev, rx_csum);\r\nif (rc && !adapter->rx_csum)\r\ndev->features = features & ~(NETIF_F_ALL_CSUM | NETIF_F_RXCSUM);\r\nreturn rc;\r\n}\r\nstatic void ibmveth_get_strings(struct net_device *dev, u32 stringset, u8 *data)\r\n{\r\nint i;\r\nif (stringset != ETH_SS_STATS)\r\nreturn;\r\nfor (i = 0; i < ARRAY_SIZE(ibmveth_stats); i++, data += ETH_GSTRING_LEN)\r\nmemcpy(data, ibmveth_stats[i].name, ETH_GSTRING_LEN);\r\n}\r\nstatic int ibmveth_get_sset_count(struct net_device *dev, int sset)\r\n{\r\nswitch (sset) {\r\ncase ETH_SS_STATS:\r\nreturn ARRAY_SIZE(ibmveth_stats);\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void ibmveth_get_ethtool_stats(struct net_device *dev,\r\nstruct ethtool_stats *stats, u64 *data)\r\n{\r\nint i;\r\nstruct ibmveth_adapter *adapter = netdev_priv(dev);\r\nfor (i = 0; i < ARRAY_SIZE(ibmveth_stats); i++)\r\ndata[i] = IBMVETH_GET_STAT(adapter, ibmveth_stats[i].offset);\r\n}\r\nstatic int ibmveth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\r\n{\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic int ibmveth_send(struct ibmveth_adapter *adapter,\r\nunion ibmveth_buf_desc *descs)\r\n{\r\nunsigned long correlator;\r\nunsigned int retry_count;\r\nunsigned long ret;\r\nretry_count = 1024;\r\ncorrelator = 0;\r\ndo {\r\nret = h_send_logical_lan(adapter->vdev->unit_address,\r\ndescs[0].desc, descs[1].desc,\r\ndescs[2].desc, descs[3].desc,\r\ndescs[4].desc, descs[5].desc,\r\ncorrelator, &correlator);\r\n} while ((ret == H_BUSY) && (retry_count--));\r\nif (ret != H_SUCCESS && ret != H_DROPPED) {\r\nnetdev_err(adapter->netdev, "tx: h_send_logical_lan failed "\r\n"with rc=%ld\n", ret);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic netdev_tx_t ibmveth_start_xmit(struct sk_buff *skb,\r\nstruct net_device *netdev)\r\n{\r\nstruct ibmveth_adapter *adapter = netdev_priv(netdev);\r\nunsigned int desc_flags;\r\nunion ibmveth_buf_desc descs[6];\r\nint last, i;\r\nint force_bounce = 0;\r\ndma_addr_t dma_addr;\r\nif (skb_shinfo(skb)->nr_frags > 5 && __skb_linearize(skb)) {\r\nnetdev->stats.tx_dropped++;\r\ngoto out;\r\n}\r\nif (skb->ip_summed == CHECKSUM_PARTIAL &&\r\n((skb->protocol == htons(ETH_P_IP) &&\r\nip_hdr(skb)->protocol != IPPROTO_TCP) ||\r\n(skb->protocol == htons(ETH_P_IPV6) &&\r\nipv6_hdr(skb)->nexthdr != IPPROTO_TCP)) &&\r\nskb_checksum_help(skb)) {\r\nnetdev_err(netdev, "tx: failed to checksum packet\n");\r\nnetdev->stats.tx_dropped++;\r\ngoto out;\r\n}\r\ndesc_flags = IBMVETH_BUF_VALID;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nunsigned char *buf = skb_transport_header(skb) +\r\nskb->csum_offset;\r\ndesc_flags |= (IBMVETH_BUF_NO_CSUM | IBMVETH_BUF_CSUM_GOOD);\r\nbuf[0] = 0;\r\nbuf[1] = 0;\r\n}\r\nretry_bounce:\r\nmemset(descs, 0, sizeof(descs));\r\nif (force_bounce || (!skb_is_nonlinear(skb) &&\r\n(skb->len < tx_copybreak))) {\r\nskb_copy_from_linear_data(skb, adapter->bounce_buffer,\r\nskb->len);\r\ndescs[0].fields.flags_len = desc_flags | skb->len;\r\ndescs[0].fields.address = adapter->bounce_buffer_dma;\r\nif (ibmveth_send(adapter, descs)) {\r\nadapter->tx_send_failed++;\r\nnetdev->stats.tx_dropped++;\r\n} else {\r\nnetdev->stats.tx_packets++;\r\nnetdev->stats.tx_bytes += skb->len;\r\n}\r\ngoto out;\r\n}\r\ndma_addr = dma_map_single(&adapter->vdev->dev, skb->data,\r\nskb_headlen(skb), DMA_TO_DEVICE);\r\nif (dma_mapping_error(&adapter->vdev->dev, dma_addr))\r\ngoto map_failed;\r\ndescs[0].fields.flags_len = desc_flags | skb_headlen(skb);\r\ndescs[0].fields.address = dma_addr;\r\nfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\r\nconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\r\ndma_addr = skb_frag_dma_map(&adapter->vdev->dev, frag, 0,\r\nskb_frag_size(frag), DMA_TO_DEVICE);\r\nif (dma_mapping_error(&adapter->vdev->dev, dma_addr))\r\ngoto map_failed_frags;\r\ndescs[i+1].fields.flags_len = desc_flags | skb_frag_size(frag);\r\ndescs[i+1].fields.address = dma_addr;\r\n}\r\nif (ibmveth_send(adapter, descs)) {\r\nadapter->tx_send_failed++;\r\nnetdev->stats.tx_dropped++;\r\n} else {\r\nnetdev->stats.tx_packets++;\r\nnetdev->stats.tx_bytes += skb->len;\r\n}\r\ndma_unmap_single(&adapter->vdev->dev,\r\ndescs[0].fields.address,\r\ndescs[0].fields.flags_len & IBMVETH_BUF_LEN_MASK,\r\nDMA_TO_DEVICE);\r\nfor (i = 1; i < skb_shinfo(skb)->nr_frags + 1; i++)\r\ndma_unmap_page(&adapter->vdev->dev, descs[i].fields.address,\r\ndescs[i].fields.flags_len & IBMVETH_BUF_LEN_MASK,\r\nDMA_TO_DEVICE);\r\nout:\r\ndev_kfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\nmap_failed_frags:\r\nlast = i+1;\r\nfor (i = 0; i < last; i++)\r\ndma_unmap_page(&adapter->vdev->dev, descs[i].fields.address,\r\ndescs[i].fields.flags_len & IBMVETH_BUF_LEN_MASK,\r\nDMA_TO_DEVICE);\r\nmap_failed:\r\nif (!firmware_has_feature(FW_FEATURE_CMO))\r\nnetdev_err(netdev, "tx: unable to map xmit buffer\n");\r\nadapter->tx_map_failed++;\r\nskb_linearize(skb);\r\nforce_bounce = 1;\r\ngoto retry_bounce;\r\n}\r\nstatic int ibmveth_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct ibmveth_adapter *adapter =\r\ncontainer_of(napi, struct ibmveth_adapter, napi);\r\nstruct net_device *netdev = adapter->netdev;\r\nint frames_processed = 0;\r\nunsigned long lpar_rc;\r\nrestart_poll:\r\ndo {\r\nif (!ibmveth_rxq_pending_buffer(adapter))\r\nbreak;\r\nsmp_rmb();\r\nif (!ibmveth_rxq_buffer_valid(adapter)) {\r\nwmb();\r\nadapter->rx_invalid_buffer++;\r\nnetdev_dbg(netdev, "recycling invalid buffer\n");\r\nibmveth_rxq_recycle_buffer(adapter);\r\n} else {\r\nstruct sk_buff *skb, *new_skb;\r\nint length = ibmveth_rxq_frame_length(adapter);\r\nint offset = ibmveth_rxq_frame_offset(adapter);\r\nint csum_good = ibmveth_rxq_csum_good(adapter);\r\nskb = ibmveth_rxq_get_buffer(adapter);\r\nnew_skb = NULL;\r\nif (length < rx_copybreak)\r\nnew_skb = netdev_alloc_skb(netdev, length);\r\nif (new_skb) {\r\nskb_copy_to_linear_data(new_skb,\r\nskb->data + offset,\r\nlength);\r\nif (rx_flush)\r\nibmveth_flush_buffer(skb->data,\r\nlength + offset);\r\nif (!ibmveth_rxq_recycle_buffer(adapter))\r\nkfree_skb(skb);\r\nskb = new_skb;\r\n} else {\r\nibmveth_rxq_harvest_buffer(adapter);\r\nskb_reserve(skb, offset);\r\n}\r\nskb_put(skb, length);\r\nskb->protocol = eth_type_trans(skb, netdev);\r\nif (csum_good)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nnetif_receive_skb(skb);\r\nnetdev->stats.rx_packets++;\r\nnetdev->stats.rx_bytes += length;\r\nframes_processed++;\r\n}\r\n} while (frames_processed < budget);\r\nibmveth_replenish_task(adapter);\r\nif (frames_processed < budget) {\r\nlpar_rc = h_vio_signal(adapter->vdev->unit_address,\r\nVIO_IRQ_ENABLE);\r\nBUG_ON(lpar_rc != H_SUCCESS);\r\nnapi_complete(napi);\r\nif (ibmveth_rxq_pending_buffer(adapter) &&\r\nnapi_reschedule(napi)) {\r\nlpar_rc = h_vio_signal(adapter->vdev->unit_address,\r\nVIO_IRQ_DISABLE);\r\ngoto restart_poll;\r\n}\r\n}\r\nreturn frames_processed;\r\n}\r\nstatic irqreturn_t ibmveth_interrupt(int irq, void *dev_instance)\r\n{\r\nstruct net_device *netdev = dev_instance;\r\nstruct ibmveth_adapter *adapter = netdev_priv(netdev);\r\nunsigned long lpar_rc;\r\nif (napi_schedule_prep(&adapter->napi)) {\r\nlpar_rc = h_vio_signal(adapter->vdev->unit_address,\r\nVIO_IRQ_DISABLE);\r\nBUG_ON(lpar_rc != H_SUCCESS);\r\n__napi_schedule(&adapter->napi);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void ibmveth_set_multicast_list(struct net_device *netdev)\r\n{\r\nstruct ibmveth_adapter *adapter = netdev_priv(netdev);\r\nunsigned long lpar_rc;\r\nif ((netdev->flags & IFF_PROMISC) ||\r\n(netdev_mc_count(netdev) > adapter->mcastFilterSize)) {\r\nlpar_rc = h_multicast_ctrl(adapter->vdev->unit_address,\r\nIbmVethMcastEnableRecv |\r\nIbmVethMcastDisableFiltering,\r\n0);\r\nif (lpar_rc != H_SUCCESS) {\r\nnetdev_err(netdev, "h_multicast_ctrl rc=%ld when "\r\n"entering promisc mode\n", lpar_rc);\r\n}\r\n} else {\r\nstruct netdev_hw_addr *ha;\r\nlpar_rc = h_multicast_ctrl(adapter->vdev->unit_address,\r\nIbmVethMcastEnableRecv |\r\nIbmVethMcastDisableFiltering |\r\nIbmVethMcastClearFilterTable,\r\n0);\r\nif (lpar_rc != H_SUCCESS) {\r\nnetdev_err(netdev, "h_multicast_ctrl rc=%ld when "\r\n"attempting to clear filter table\n",\r\nlpar_rc);\r\n}\r\nnetdev_for_each_mc_addr(ha, netdev) {\r\nunsigned long mcast_addr = 0;\r\nmemcpy(((char *)&mcast_addr)+2, ha->addr, 6);\r\nlpar_rc = h_multicast_ctrl(adapter->vdev->unit_address,\r\nIbmVethMcastAddFilter,\r\nmcast_addr);\r\nif (lpar_rc != H_SUCCESS) {\r\nnetdev_err(netdev, "h_multicast_ctrl rc=%ld "\r\n"when adding an entry to the filter "\r\n"table\n", lpar_rc);\r\n}\r\n}\r\nlpar_rc = h_multicast_ctrl(adapter->vdev->unit_address,\r\nIbmVethMcastEnableFiltering,\r\n0);\r\nif (lpar_rc != H_SUCCESS) {\r\nnetdev_err(netdev, "h_multicast_ctrl rc=%ld when "\r\n"enabling filtering\n", lpar_rc);\r\n}\r\n}\r\n}\r\nstatic int ibmveth_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct ibmveth_adapter *adapter = netdev_priv(dev);\r\nstruct vio_dev *viodev = adapter->vdev;\r\nint new_mtu_oh = new_mtu + IBMVETH_BUFF_OH;\r\nint i, rc;\r\nint need_restart = 0;\r\nif (new_mtu < IBMVETH_MIN_MTU)\r\nreturn -EINVAL;\r\nfor (i = 0; i < IBMVETH_NUM_BUFF_POOLS; i++)\r\nif (new_mtu_oh < adapter->rx_buff_pool[i].buff_size)\r\nbreak;\r\nif (i == IBMVETH_NUM_BUFF_POOLS)\r\nreturn -EINVAL;\r\nif (netif_running(adapter->netdev)) {\r\nneed_restart = 1;\r\nadapter->pool_config = 1;\r\nibmveth_close(adapter->netdev);\r\nadapter->pool_config = 0;\r\n}\r\nfor (i = 0; i < IBMVETH_NUM_BUFF_POOLS; i++) {\r\nadapter->rx_buff_pool[i].active = 1;\r\nif (new_mtu_oh < adapter->rx_buff_pool[i].buff_size) {\r\ndev->mtu = new_mtu;\r\nvio_cmo_set_dev_desired(viodev,\r\nibmveth_get_desired_dma\r\n(viodev));\r\nif (need_restart) {\r\nreturn ibmveth_open(adapter->netdev);\r\n}\r\nreturn 0;\r\n}\r\n}\r\nif (need_restart && (rc = ibmveth_open(adapter->netdev)))\r\nreturn rc;\r\nreturn -EINVAL;\r\n}\r\nstatic void ibmveth_poll_controller(struct net_device *dev)\r\n{\r\nibmveth_replenish_task(netdev_priv(dev));\r\nibmveth_interrupt(dev->irq, dev);\r\n}\r\nstatic unsigned long ibmveth_get_desired_dma(struct vio_dev *vdev)\r\n{\r\nstruct net_device *netdev = dev_get_drvdata(&vdev->dev);\r\nstruct ibmveth_adapter *adapter;\r\nunsigned long ret;\r\nint i;\r\nint rxqentries = 1;\r\nif (netdev == NULL)\r\nreturn IOMMU_PAGE_ALIGN(IBMVETH_IO_ENTITLEMENT_DEFAULT);\r\nadapter = netdev_priv(netdev);\r\nret = IBMVETH_BUFF_LIST_SIZE + IBMVETH_FILT_LIST_SIZE;\r\nret += IOMMU_PAGE_ALIGN(netdev->mtu);\r\nfor (i = 0; i < IBMVETH_NUM_BUFF_POOLS; i++) {\r\nif (adapter->rx_buff_pool[i].active)\r\nret +=\r\nadapter->rx_buff_pool[i].size *\r\nIOMMU_PAGE_ALIGN(adapter->rx_buff_pool[i].\r\nbuff_size);\r\nrxqentries += adapter->rx_buff_pool[i].size;\r\n}\r\nret += IOMMU_PAGE_ALIGN(rxqentries * sizeof(struct ibmveth_rx_q_entry));\r\nreturn ret;\r\n}\r\nstatic int __devinit ibmveth_probe(struct vio_dev *dev,\r\nconst struct vio_device_id *id)\r\n{\r\nint rc, i;\r\nstruct net_device *netdev;\r\nstruct ibmveth_adapter *adapter;\r\nunsigned char *mac_addr_p;\r\nunsigned int *mcastFilterSize_p;\r\ndev_dbg(&dev->dev, "entering ibmveth_probe for UA 0x%x\n",\r\ndev->unit_address);\r\nmac_addr_p = (unsigned char *)vio_get_attribute(dev, VETH_MAC_ADDR,\r\nNULL);\r\nif (!mac_addr_p) {\r\ndev_err(&dev->dev, "Can't find VETH_MAC_ADDR attribute\n");\r\nreturn -EINVAL;\r\n}\r\nmcastFilterSize_p = (unsigned int *)vio_get_attribute(dev,\r\nVETH_MCAST_FILTER_SIZE, NULL);\r\nif (!mcastFilterSize_p) {\r\ndev_err(&dev->dev, "Can't find VETH_MCAST_FILTER_SIZE "\r\n"attribute\n");\r\nreturn -EINVAL;\r\n}\r\nnetdev = alloc_etherdev(sizeof(struct ibmveth_adapter));\r\nif (!netdev)\r\nreturn -ENOMEM;\r\nadapter = netdev_priv(netdev);\r\ndev_set_drvdata(&dev->dev, netdev);\r\nadapter->vdev = dev;\r\nadapter->netdev = netdev;\r\nadapter->mcastFilterSize = *mcastFilterSize_p;\r\nadapter->pool_config = 0;\r\nnetif_napi_add(netdev, &adapter->napi, ibmveth_poll, 16);\r\nif ((*mac_addr_p & 0x3) != 0x02)\r\nmac_addr_p += 2;\r\nadapter->mac_addr = 0;\r\nmemcpy(&adapter->mac_addr, mac_addr_p, 6);\r\nnetdev->irq = dev->irq;\r\nnetdev->netdev_ops = &ibmveth_netdev_ops;\r\nnetdev->ethtool_ops = &netdev_ethtool_ops;\r\nSET_NETDEV_DEV(netdev, &dev->dev);\r\nnetdev->hw_features = NETIF_F_SG | NETIF_F_RXCSUM |\r\nNETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\r\nnetdev->features |= netdev->hw_features;\r\nmemcpy(netdev->dev_addr, &adapter->mac_addr, netdev->addr_len);\r\nfor (i = 0; i < IBMVETH_NUM_BUFF_POOLS; i++) {\r\nstruct kobject *kobj = &adapter->rx_buff_pool[i].kobj;\r\nint error;\r\nibmveth_init_buffer_pool(&adapter->rx_buff_pool[i], i,\r\npool_count[i], pool_size[i],\r\npool_active[i]);\r\nerror = kobject_init_and_add(kobj, &ktype_veth_pool,\r\n&dev->dev.kobj, "pool%d", i);\r\nif (!error)\r\nkobject_uevent(kobj, KOBJ_ADD);\r\n}\r\nnetdev_dbg(netdev, "adapter @ 0x%p\n", adapter);\r\nadapter->buffer_list_dma = DMA_ERROR_CODE;\r\nadapter->filter_list_dma = DMA_ERROR_CODE;\r\nadapter->rx_queue.queue_dma = DMA_ERROR_CODE;\r\nnetdev_dbg(netdev, "registering netdev...\n");\r\nibmveth_set_features(netdev, netdev->features);\r\nrc = register_netdev(netdev);\r\nif (rc) {\r\nnetdev_dbg(netdev, "failed to register netdev rc=%d\n", rc);\r\nfree_netdev(netdev);\r\nreturn rc;\r\n}\r\nnetdev_dbg(netdev, "registered\n");\r\nreturn 0;\r\n}\r\nstatic int __devexit ibmveth_remove(struct vio_dev *dev)\r\n{\r\nstruct net_device *netdev = dev_get_drvdata(&dev->dev);\r\nstruct ibmveth_adapter *adapter = netdev_priv(netdev);\r\nint i;\r\nfor (i = 0; i < IBMVETH_NUM_BUFF_POOLS; i++)\r\nkobject_put(&adapter->rx_buff_pool[i].kobj);\r\nunregister_netdev(netdev);\r\nfree_netdev(netdev);\r\ndev_set_drvdata(&dev->dev, NULL);\r\nreturn 0;\r\n}\r\nstatic ssize_t veth_pool_show(struct kobject *kobj,\r\nstruct attribute *attr, char *buf)\r\n{\r\nstruct ibmveth_buff_pool *pool = container_of(kobj,\r\nstruct ibmveth_buff_pool,\r\nkobj);\r\nif (attr == &veth_active_attr)\r\nreturn sprintf(buf, "%d\n", pool->active);\r\nelse if (attr == &veth_num_attr)\r\nreturn sprintf(buf, "%d\n", pool->size);\r\nelse if (attr == &veth_size_attr)\r\nreturn sprintf(buf, "%d\n", pool->buff_size);\r\nreturn 0;\r\n}\r\nstatic ssize_t veth_pool_store(struct kobject *kobj, struct attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct ibmveth_buff_pool *pool = container_of(kobj,\r\nstruct ibmveth_buff_pool,\r\nkobj);\r\nstruct net_device *netdev = dev_get_drvdata(\r\ncontainer_of(kobj->parent, struct device, kobj));\r\nstruct ibmveth_adapter *adapter = netdev_priv(netdev);\r\nlong value = simple_strtol(buf, NULL, 10);\r\nlong rc;\r\nif (attr == &veth_active_attr) {\r\nif (value && !pool->active) {\r\nif (netif_running(netdev)) {\r\nif (ibmveth_alloc_buffer_pool(pool)) {\r\nnetdev_err(netdev,\r\n"unable to alloc pool\n");\r\nreturn -ENOMEM;\r\n}\r\npool->active = 1;\r\nadapter->pool_config = 1;\r\nibmveth_close(netdev);\r\nadapter->pool_config = 0;\r\nif ((rc = ibmveth_open(netdev)))\r\nreturn rc;\r\n} else {\r\npool->active = 1;\r\n}\r\n} else if (!value && pool->active) {\r\nint mtu = netdev->mtu + IBMVETH_BUFF_OH;\r\nint i;\r\nfor (i = 0; i < IBMVETH_NUM_BUFF_POOLS; i++) {\r\nif (pool == &adapter->rx_buff_pool[i])\r\ncontinue;\r\nif (!adapter->rx_buff_pool[i].active)\r\ncontinue;\r\nif (mtu <= adapter->rx_buff_pool[i].buff_size)\r\nbreak;\r\n}\r\nif (i == IBMVETH_NUM_BUFF_POOLS) {\r\nnetdev_err(netdev, "no active pool >= MTU\n");\r\nreturn -EPERM;\r\n}\r\nif (netif_running(netdev)) {\r\nadapter->pool_config = 1;\r\nibmveth_close(netdev);\r\npool->active = 0;\r\nadapter->pool_config = 0;\r\nif ((rc = ibmveth_open(netdev)))\r\nreturn rc;\r\n}\r\npool->active = 0;\r\n}\r\n} else if (attr == &veth_num_attr) {\r\nif (value <= 0 || value > IBMVETH_MAX_POOL_COUNT) {\r\nreturn -EINVAL;\r\n} else {\r\nif (netif_running(netdev)) {\r\nadapter->pool_config = 1;\r\nibmveth_close(netdev);\r\nadapter->pool_config = 0;\r\npool->size = value;\r\nif ((rc = ibmveth_open(netdev)))\r\nreturn rc;\r\n} else {\r\npool->size = value;\r\n}\r\n}\r\n} else if (attr == &veth_size_attr) {\r\nif (value <= IBMVETH_BUFF_OH || value > IBMVETH_MAX_BUF_SIZE) {\r\nreturn -EINVAL;\r\n} else {\r\nif (netif_running(netdev)) {\r\nadapter->pool_config = 1;\r\nibmveth_close(netdev);\r\nadapter->pool_config = 0;\r\npool->buff_size = value;\r\nif ((rc = ibmveth_open(netdev)))\r\nreturn rc;\r\n} else {\r\npool->buff_size = value;\r\n}\r\n}\r\n}\r\nibmveth_interrupt(netdev->irq, netdev);\r\nreturn count;\r\n}\r\nstatic int ibmveth_resume(struct device *dev)\r\n{\r\nstruct net_device *netdev = dev_get_drvdata(dev);\r\nibmveth_interrupt(netdev->irq, netdev);\r\nreturn 0;\r\n}\r\nstatic int __init ibmveth_module_init(void)\r\n{\r\nprintk(KERN_DEBUG "%s: %s %s\n", ibmveth_driver_name,\r\nibmveth_driver_string, ibmveth_driver_version);\r\nreturn vio_register_driver(&ibmveth_driver);\r\n}\r\nstatic void __exit ibmveth_module_exit(void)\r\n{\r\nvio_unregister_driver(&ibmveth_driver);\r\n}
