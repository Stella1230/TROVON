static inline int\r\npreempt_trace(void)\r\n{\r\nreturn ((trace_type & TRACER_PREEMPT_OFF) && preempt_count());\r\n}\r\nstatic inline int\r\nirq_trace(void)\r\n{\r\nreturn ((trace_type & TRACER_IRQS_OFF) &&\r\nirqs_disabled());\r\n}\r\nstatic int func_prolog_dec(struct trace_array *tr,\r\nstruct trace_array_cpu **data,\r\nunsigned long *flags)\r\n{\r\nlong disabled;\r\nint cpu;\r\ncpu = raw_smp_processor_id();\r\nif (likely(!per_cpu(tracing_cpu, cpu)))\r\nreturn 0;\r\nlocal_save_flags(*flags);\r\nif (!irqs_disabled_flags(*flags))\r\nreturn 0;\r\n*data = tr->data[cpu];\r\ndisabled = atomic_inc_return(&(*data)->disabled);\r\nif (likely(disabled == 1))\r\nreturn 1;\r\natomic_dec(&(*data)->disabled);\r\nreturn 0;\r\n}\r\nstatic void\r\nirqsoff_tracer_call(unsigned long ip, unsigned long parent_ip)\r\n{\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nif (!func_prolog_dec(tr, &data, &flags))\r\nreturn;\r\ntrace_function(tr, ip, parent_ip, flags, preempt_count());\r\natomic_dec(&data->disabled);\r\n}\r\nstatic int irqsoff_set_flag(u32 old_flags, u32 bit, int set)\r\n{\r\nint cpu;\r\nif (!(bit & TRACE_DISPLAY_GRAPH))\r\nreturn -EINVAL;\r\nif (!(is_graph() ^ set))\r\nreturn 0;\r\nstop_irqsoff_tracer(irqsoff_trace, !set);\r\nfor_each_possible_cpu(cpu)\r\nper_cpu(tracing_cpu, cpu) = 0;\r\ntracing_max_latency = 0;\r\ntracing_reset_online_cpus(irqsoff_trace);\r\nreturn start_irqsoff_tracer(irqsoff_trace, set);\r\n}\r\nstatic int irqsoff_graph_entry(struct ftrace_graph_ent *trace)\r\n{\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint ret;\r\nint pc;\r\nif (!func_prolog_dec(tr, &data, &flags))\r\nreturn 0;\r\npc = preempt_count();\r\nret = __trace_graph_entry(tr, trace, flags, pc);\r\natomic_dec(&data->disabled);\r\nreturn ret;\r\n}\r\nstatic void irqsoff_graph_return(struct ftrace_graph_ret *trace)\r\n{\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint pc;\r\nif (!func_prolog_dec(tr, &data, &flags))\r\nreturn;\r\npc = preempt_count();\r\n__trace_graph_return(tr, trace, flags, pc);\r\natomic_dec(&data->disabled);\r\n}\r\nstatic void irqsoff_trace_open(struct trace_iterator *iter)\r\n{\r\nif (is_graph())\r\ngraph_trace_open(iter);\r\n}\r\nstatic void irqsoff_trace_close(struct trace_iterator *iter)\r\n{\r\nif (iter->private)\r\ngraph_trace_close(iter);\r\n}\r\nstatic enum print_line_t irqsoff_print_line(struct trace_iterator *iter)\r\n{\r\nif (is_graph())\r\nreturn print_graph_function_flags(iter, GRAPH_TRACER_FLAGS);\r\nreturn TRACE_TYPE_UNHANDLED;\r\n}\r\nstatic void irqsoff_print_header(struct seq_file *s)\r\n{\r\nif (is_graph())\r\nprint_graph_headers_flags(s, GRAPH_TRACER_FLAGS);\r\nelse\r\ntrace_default_header(s);\r\n}\r\nstatic void\r\n__trace_function(struct trace_array *tr,\r\nunsigned long ip, unsigned long parent_ip,\r\nunsigned long flags, int pc)\r\n{\r\nif (is_graph())\r\ntrace_graph_function(tr, ip, parent_ip, flags, pc);\r\nelse\r\ntrace_function(tr, ip, parent_ip, flags, pc);\r\n}\r\nstatic int irqsoff_set_flag(u32 old_flags, u32 bit, int set)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic int irqsoff_graph_entry(struct ftrace_graph_ent *trace)\r\n{\r\nreturn -1;\r\n}\r\nstatic enum print_line_t irqsoff_print_line(struct trace_iterator *iter)\r\n{\r\nreturn TRACE_TYPE_UNHANDLED;\r\n}\r\nstatic void irqsoff_graph_return(struct ftrace_graph_ret *trace) { }\r\nstatic void irqsoff_trace_open(struct trace_iterator *iter) { }\r\nstatic void irqsoff_trace_close(struct trace_iterator *iter) { }\r\nstatic void irqsoff_print_header(struct seq_file *s)\r\n{\r\ntrace_default_header(s);\r\n}\r\nstatic void irqsoff_print_header(struct seq_file *s)\r\n{\r\ntrace_latency_header(s);\r\n}\r\nstatic int report_latency(cycle_t delta)\r\n{\r\nif (tracing_thresh) {\r\nif (delta < tracing_thresh)\r\nreturn 0;\r\n} else {\r\nif (delta <= tracing_max_latency)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void\r\ncheck_critical_timing(struct trace_array *tr,\r\nstruct trace_array_cpu *data,\r\nunsigned long parent_ip,\r\nint cpu)\r\n{\r\ncycle_t T0, T1, delta;\r\nunsigned long flags;\r\nint pc;\r\nT0 = data->preempt_timestamp;\r\nT1 = ftrace_now(cpu);\r\ndelta = T1-T0;\r\nlocal_save_flags(flags);\r\npc = preempt_count();\r\nif (!report_latency(delta))\r\ngoto out;\r\nraw_spin_lock_irqsave(&max_trace_lock, flags);\r\nif (!report_latency(delta))\r\ngoto out_unlock;\r\n__trace_function(tr, CALLER_ADDR0, parent_ip, flags, pc);\r\n__trace_stack(tr, flags, 5, pc);\r\nif (data->critical_sequence != max_sequence)\r\ngoto out_unlock;\r\ndata->critical_end = parent_ip;\r\nif (likely(!is_tracing_stopped())) {\r\ntracing_max_latency = delta;\r\nupdate_max_tr_single(tr, current, cpu);\r\n}\r\nmax_sequence++;\r\nout_unlock:\r\nraw_spin_unlock_irqrestore(&max_trace_lock, flags);\r\nout:\r\ndata->critical_sequence = max_sequence;\r\ndata->preempt_timestamp = ftrace_now(cpu);\r\n__trace_function(tr, CALLER_ADDR0, parent_ip, flags, pc);\r\n}\r\nstatic inline void\r\nstart_critical_timing(unsigned long ip, unsigned long parent_ip)\r\n{\r\nint cpu;\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nif (likely(!tracer_enabled))\r\nreturn;\r\ncpu = raw_smp_processor_id();\r\nif (per_cpu(tracing_cpu, cpu))\r\nreturn;\r\ndata = tr->data[cpu];\r\nif (unlikely(!data) || atomic_read(&data->disabled))\r\nreturn;\r\natomic_inc(&data->disabled);\r\ndata->critical_sequence = max_sequence;\r\ndata->preempt_timestamp = ftrace_now(cpu);\r\ndata->critical_start = parent_ip ? : ip;\r\nlocal_save_flags(flags);\r\n__trace_function(tr, ip, parent_ip, flags, preempt_count());\r\nper_cpu(tracing_cpu, cpu) = 1;\r\natomic_dec(&data->disabled);\r\n}\r\nstatic inline void\r\nstop_critical_timing(unsigned long ip, unsigned long parent_ip)\r\n{\r\nint cpu;\r\nstruct trace_array *tr = irqsoff_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\ncpu = raw_smp_processor_id();\r\nif (unlikely(per_cpu(tracing_cpu, cpu)))\r\nper_cpu(tracing_cpu, cpu) = 0;\r\nelse\r\nreturn;\r\nif (!tracer_enabled)\r\nreturn;\r\ndata = tr->data[cpu];\r\nif (unlikely(!data) ||\r\n!data->critical_start || atomic_read(&data->disabled))\r\nreturn;\r\natomic_inc(&data->disabled);\r\nlocal_save_flags(flags);\r\n__trace_function(tr, ip, parent_ip, flags, preempt_count());\r\ncheck_critical_timing(tr, data, parent_ip ? : ip, cpu);\r\ndata->critical_start = 0;\r\natomic_dec(&data->disabled);\r\n}\r\nvoid start_critical_timings(void)\r\n{\r\nif (preempt_trace() || irq_trace())\r\nstart_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\r\n}\r\nvoid stop_critical_timings(void)\r\n{\r\nif (preempt_trace() || irq_trace())\r\nstop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\r\n}\r\nvoid time_hardirqs_on(unsigned long a0, unsigned long a1)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstop_critical_timing(a0, a1);\r\n}\r\nvoid time_hardirqs_off(unsigned long a0, unsigned long a1)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstart_critical_timing(a0, a1);\r\n}\r\nvoid trace_softirqs_on(unsigned long ip)\r\n{\r\n}\r\nvoid trace_softirqs_off(unsigned long ip)\r\n{\r\n}\r\ninline void print_irqtrace_events(struct task_struct *curr)\r\n{\r\n}\r\nvoid trace_hardirqs_on(void)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\r\n}\r\nvoid trace_hardirqs_off(void)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstart_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\r\n}\r\nvoid trace_hardirqs_on_caller(unsigned long caller_addr)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstop_critical_timing(CALLER_ADDR0, caller_addr);\r\n}\r\nvoid trace_hardirqs_off_caller(unsigned long caller_addr)\r\n{\r\nif (!preempt_trace() && irq_trace())\r\nstart_critical_timing(CALLER_ADDR0, caller_addr);\r\n}\r\nvoid trace_preempt_on(unsigned long a0, unsigned long a1)\r\n{\r\nif (preempt_trace() && !irq_trace())\r\nstop_critical_timing(a0, a1);\r\n}\r\nvoid trace_preempt_off(unsigned long a0, unsigned long a1)\r\n{\r\nif (preempt_trace() && !irq_trace())\r\nstart_critical_timing(a0, a1);\r\n}\r\nstatic int start_irqsoff_tracer(struct trace_array *tr, int graph)\r\n{\r\nint ret = 0;\r\nif (!graph)\r\nret = register_ftrace_function(&trace_ops);\r\nelse\r\nret = register_ftrace_graph(&irqsoff_graph_return,\r\n&irqsoff_graph_entry);\r\nif (!ret && tracing_is_enabled())\r\ntracer_enabled = 1;\r\nelse\r\ntracer_enabled = 0;\r\nreturn ret;\r\n}\r\nstatic void stop_irqsoff_tracer(struct trace_array *tr, int graph)\r\n{\r\ntracer_enabled = 0;\r\nif (!graph)\r\nunregister_ftrace_function(&trace_ops);\r\nelse\r\nunregister_ftrace_graph();\r\n}\r\nstatic void __irqsoff_tracer_init(struct trace_array *tr)\r\n{\r\nsave_lat_flag = trace_flags & TRACE_ITER_LATENCY_FMT;\r\ntrace_flags |= TRACE_ITER_LATENCY_FMT;\r\ntracing_max_latency = 0;\r\nirqsoff_trace = tr;\r\nsmp_wmb();\r\ntracing_reset_online_cpus(tr);\r\nif (start_irqsoff_tracer(tr, is_graph()))\r\nprintk(KERN_ERR "failed to start irqsoff tracer\n");\r\n}\r\nstatic void irqsoff_tracer_reset(struct trace_array *tr)\r\n{\r\nstop_irqsoff_tracer(tr, is_graph());\r\nif (!save_lat_flag)\r\ntrace_flags &= ~TRACE_ITER_LATENCY_FMT;\r\n}\r\nstatic void irqsoff_tracer_start(struct trace_array *tr)\r\n{\r\ntracer_enabled = 1;\r\n}\r\nstatic void irqsoff_tracer_stop(struct trace_array *tr)\r\n{\r\ntracer_enabled = 0;\r\n}\r\nstatic int irqsoff_tracer_init(struct trace_array *tr)\r\n{\r\ntrace_type = TRACER_IRQS_OFF;\r\n__irqsoff_tracer_init(tr);\r\nreturn 0;\r\n}\r\nstatic int preemptoff_tracer_init(struct trace_array *tr)\r\n{\r\ntrace_type = TRACER_PREEMPT_OFF;\r\n__irqsoff_tracer_init(tr);\r\nreturn 0;\r\n}\r\nstatic int preemptirqsoff_tracer_init(struct trace_array *tr)\r\n{\r\ntrace_type = TRACER_IRQS_OFF | TRACER_PREEMPT_OFF;\r\n__irqsoff_tracer_init(tr);\r\nreturn 0;\r\n}\r\n__init static int init_irqsoff_tracer(void)\r\n{\r\nregister_irqsoff(irqsoff_tracer);\r\nregister_preemptoff(preemptoff_tracer);\r\nregister_preemptirqsoff(preemptirqsoff_tracer);\r\nreturn 0;\r\n}
