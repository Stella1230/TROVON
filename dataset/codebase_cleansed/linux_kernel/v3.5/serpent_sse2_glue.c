static inline bool serpent_fpu_begin(bool fpu_enabled, unsigned int nbytes)\r\n{\r\nif (fpu_enabled)\r\nreturn true;\r\nif (nbytes < SERPENT_BLOCK_SIZE * SERPENT_PARALLEL_BLOCKS)\r\nreturn false;\r\nkernel_fpu_begin();\r\nreturn true;\r\n}\r\nstatic inline void serpent_fpu_end(bool fpu_enabled)\r\n{\r\nif (fpu_enabled)\r\nkernel_fpu_end();\r\n}\r\nstatic int ecb_crypt(struct blkcipher_desc *desc, struct blkcipher_walk *walk,\r\nbool enc)\r\n{\r\nbool fpu_enabled = false;\r\nstruct serpent_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nunsigned int nbytes;\r\nint err;\r\nerr = blkcipher_walk_virt(desc, walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nwhile ((nbytes = walk->nbytes)) {\r\nu8 *wsrc = walk->src.virt.addr;\r\nu8 *wdst = walk->dst.virt.addr;\r\nfpu_enabled = serpent_fpu_begin(fpu_enabled, nbytes);\r\nif (nbytes >= bsize * SERPENT_PARALLEL_BLOCKS) {\r\ndo {\r\nif (enc)\r\nserpent_enc_blk_xway(ctx, wdst, wsrc);\r\nelse\r\nserpent_dec_blk_xway(ctx, wdst, wsrc);\r\nwsrc += bsize * SERPENT_PARALLEL_BLOCKS;\r\nwdst += bsize * SERPENT_PARALLEL_BLOCKS;\r\nnbytes -= bsize * SERPENT_PARALLEL_BLOCKS;\r\n} while (nbytes >= bsize * SERPENT_PARALLEL_BLOCKS);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\ndo {\r\nif (enc)\r\n__serpent_encrypt(ctx, wdst, wsrc);\r\nelse\r\n__serpent_decrypt(ctx, wdst, wsrc);\r\nwsrc += bsize;\r\nwdst += bsize;\r\nnbytes -= bsize;\r\n} while (nbytes >= bsize);\r\ndone:\r\nerr = blkcipher_walk_done(desc, walk, nbytes);\r\n}\r\nserpent_fpu_end(fpu_enabled);\r\nreturn err;\r\n}\r\nstatic int ecb_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct blkcipher_walk walk;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ecb_crypt(desc, &walk, true);\r\n}\r\nstatic int ecb_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct blkcipher_walk walk;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nreturn ecb_crypt(desc, &walk, false);\r\n}\r\nstatic unsigned int __cbc_encrypt(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct serpent_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nunsigned int nbytes = walk->nbytes;\r\nu128 *src = (u128 *)walk->src.virt.addr;\r\nu128 *dst = (u128 *)walk->dst.virt.addr;\r\nu128 *iv = (u128 *)walk->iv;\r\ndo {\r\nu128_xor(dst, src, iv);\r\n__serpent_encrypt(ctx, (u8 *)dst, (u8 *)dst);\r\niv = dst;\r\nsrc += 1;\r\ndst += 1;\r\nnbytes -= bsize;\r\n} while (nbytes >= bsize);\r\nu128_xor((u128 *)walk->iv, (u128 *)walk->iv, iv);\r\nreturn nbytes;\r\n}\r\nstatic int cbc_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\nwhile ((nbytes = walk.nbytes)) {\r\nnbytes = __cbc_encrypt(desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nreturn err;\r\n}\r\nstatic unsigned int __cbc_decrypt(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct serpent_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nunsigned int nbytes = walk->nbytes;\r\nu128 *src = (u128 *)walk->src.virt.addr;\r\nu128 *dst = (u128 *)walk->dst.virt.addr;\r\nu128 ivs[SERPENT_PARALLEL_BLOCKS - 1];\r\nu128 last_iv;\r\nint i;\r\nsrc += nbytes / bsize - 1;\r\ndst += nbytes / bsize - 1;\r\nlast_iv = *src;\r\nif (nbytes >= bsize * SERPENT_PARALLEL_BLOCKS) {\r\ndo {\r\nnbytes -= bsize * (SERPENT_PARALLEL_BLOCKS - 1);\r\nsrc -= SERPENT_PARALLEL_BLOCKS - 1;\r\ndst -= SERPENT_PARALLEL_BLOCKS - 1;\r\nfor (i = 0; i < SERPENT_PARALLEL_BLOCKS - 1; i++)\r\nivs[i] = src[i];\r\nserpent_dec_blk_xway(ctx, (u8 *)dst, (u8 *)src);\r\nfor (i = 0; i < SERPENT_PARALLEL_BLOCKS - 1; i++)\r\nu128_xor(dst + (i + 1), dst + (i + 1), ivs + i);\r\nnbytes -= bsize;\r\nif (nbytes < bsize)\r\ngoto done;\r\nu128_xor(dst, dst, src - 1);\r\nsrc -= 1;\r\ndst -= 1;\r\n} while (nbytes >= bsize * SERPENT_PARALLEL_BLOCKS);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\nfor (;;) {\r\n__serpent_decrypt(ctx, (u8 *)dst, (u8 *)src);\r\nnbytes -= bsize;\r\nif (nbytes < bsize)\r\nbreak;\r\nu128_xor(dst, dst, src - 1);\r\nsrc -= 1;\r\ndst -= 1;\r\n}\r\ndone:\r\nu128_xor(dst, dst, (u128 *)walk->iv);\r\n*(u128 *)walk->iv = last_iv;\r\nreturn nbytes;\r\n}\r\nstatic int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nbool fpu_enabled = false;\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt(desc, &walk);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nwhile ((nbytes = walk.nbytes)) {\r\nfpu_enabled = serpent_fpu_begin(fpu_enabled, nbytes);\r\nnbytes = __cbc_decrypt(desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nserpent_fpu_end(fpu_enabled);\r\nreturn err;\r\n}\r\nstatic inline void u128_to_be128(be128 *dst, const u128 *src)\r\n{\r\ndst->a = cpu_to_be64(src->a);\r\ndst->b = cpu_to_be64(src->b);\r\n}\r\nstatic inline void be128_to_u128(u128 *dst, const be128 *src)\r\n{\r\ndst->a = be64_to_cpu(src->a);\r\ndst->b = be64_to_cpu(src->b);\r\n}\r\nstatic inline void u128_inc(u128 *i)\r\n{\r\ni->b++;\r\nif (!i->b)\r\ni->a++;\r\n}\r\nstatic void ctr_crypt_final(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct serpent_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nu8 *ctrblk = walk->iv;\r\nu8 keystream[SERPENT_BLOCK_SIZE];\r\nu8 *src = walk->src.virt.addr;\r\nu8 *dst = walk->dst.virt.addr;\r\nunsigned int nbytes = walk->nbytes;\r\n__serpent_encrypt(ctx, keystream, ctrblk);\r\ncrypto_xor(keystream, src, nbytes);\r\nmemcpy(dst, keystream, nbytes);\r\ncrypto_inc(ctrblk, SERPENT_BLOCK_SIZE);\r\n}\r\nstatic unsigned int __ctr_crypt(struct blkcipher_desc *desc,\r\nstruct blkcipher_walk *walk)\r\n{\r\nstruct serpent_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nunsigned int nbytes = walk->nbytes;\r\nu128 *src = (u128 *)walk->src.virt.addr;\r\nu128 *dst = (u128 *)walk->dst.virt.addr;\r\nu128 ctrblk;\r\nbe128 ctrblocks[SERPENT_PARALLEL_BLOCKS];\r\nint i;\r\nbe128_to_u128(&ctrblk, (be128 *)walk->iv);\r\nif (nbytes >= bsize * SERPENT_PARALLEL_BLOCKS) {\r\ndo {\r\nfor (i = 0; i < SERPENT_PARALLEL_BLOCKS; i++) {\r\nif (dst != src)\r\ndst[i] = src[i];\r\nu128_to_be128(&ctrblocks[i], &ctrblk);\r\nu128_inc(&ctrblk);\r\n}\r\nserpent_enc_blk_xway_xor(ctx, (u8 *)dst,\r\n(u8 *)ctrblocks);\r\nsrc += SERPENT_PARALLEL_BLOCKS;\r\ndst += SERPENT_PARALLEL_BLOCKS;\r\nnbytes -= bsize * SERPENT_PARALLEL_BLOCKS;\r\n} while (nbytes >= bsize * SERPENT_PARALLEL_BLOCKS);\r\nif (nbytes < bsize)\r\ngoto done;\r\n}\r\ndo {\r\nif (dst != src)\r\n*dst = *src;\r\nu128_to_be128(&ctrblocks[0], &ctrblk);\r\nu128_inc(&ctrblk);\r\n__serpent_encrypt(ctx, (u8 *)ctrblocks, (u8 *)ctrblocks);\r\nu128_xor(dst, dst, (u128 *)ctrblocks);\r\nsrc += 1;\r\ndst += 1;\r\nnbytes -= bsize;\r\n} while (nbytes >= bsize);\r\ndone:\r\nu128_to_be128((be128 *)walk->iv, &ctrblk);\r\nreturn nbytes;\r\n}\r\nstatic int ctr_crypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nbool fpu_enabled = false;\r\nstruct blkcipher_walk walk;\r\nint err;\r\nblkcipher_walk_init(&walk, dst, src, nbytes);\r\nerr = blkcipher_walk_virt_block(desc, &walk, SERPENT_BLOCK_SIZE);\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nwhile ((nbytes = walk.nbytes) >= SERPENT_BLOCK_SIZE) {\r\nfpu_enabled = serpent_fpu_begin(fpu_enabled, nbytes);\r\nnbytes = __ctr_crypt(desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, nbytes);\r\n}\r\nserpent_fpu_end(fpu_enabled);\r\nif (walk.nbytes) {\r\nctr_crypt_final(desc, &walk);\r\nerr = blkcipher_walk_done(desc, &walk, 0);\r\n}\r\nreturn err;\r\n}\r\nstatic void encrypt_callback(void *priv, u8 *srcdst, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nstruct crypt_priv *ctx = priv;\r\nint i;\r\nctx->fpu_enabled = serpent_fpu_begin(ctx->fpu_enabled, nbytes);\r\nif (nbytes == bsize * SERPENT_PARALLEL_BLOCKS) {\r\nserpent_enc_blk_xway(ctx->ctx, srcdst, srcdst);\r\nreturn;\r\n}\r\nfor (i = 0; i < nbytes / bsize; i++, srcdst += bsize)\r\n__serpent_encrypt(ctx->ctx, srcdst, srcdst);\r\n}\r\nstatic void decrypt_callback(void *priv, u8 *srcdst, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nstruct crypt_priv *ctx = priv;\r\nint i;\r\nctx->fpu_enabled = serpent_fpu_begin(ctx->fpu_enabled, nbytes);\r\nif (nbytes == bsize * SERPENT_PARALLEL_BLOCKS) {\r\nserpent_dec_blk_xway(ctx->ctx, srcdst, srcdst);\r\nreturn;\r\n}\r\nfor (i = 0; i < nbytes / bsize; i++, srcdst += bsize)\r\n__serpent_decrypt(ctx->ctx, srcdst, srcdst);\r\n}\r\nstatic int lrw_serpent_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint err;\r\nerr = __serpent_setkey(&ctx->serpent_ctx, key, keylen -\r\nSERPENT_BLOCK_SIZE);\r\nif (err)\r\nreturn err;\r\nreturn lrw_init_table(&ctx->lrw_table, key + keylen -\r\nSERPENT_BLOCK_SIZE);\r\n}\r\nstatic int lrw_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->serpent_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int lrw_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->serpent_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic void lrw_exit_tfm(struct crypto_tfm *tfm)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nlrw_free_table(&ctx->lrw_table);\r\n}\r\nstatic int xts_serpent_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_tfm_ctx(tfm);\r\nu32 *flags = &tfm->crt_flags;\r\nint err;\r\nif (keylen % 2) {\r\n*flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nerr = __serpent_setkey(&ctx->crypt_ctx, key, keylen / 2);\r\nif (err)\r\nreturn err;\r\nreturn __serpent_setkey(&ctx->tweak_ctx, key + keylen / 2, keylen / 2);\r\n}\r\nstatic int xts_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->crypt_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = &ctx->tweak_ctx,\r\n.tweak_fn = XTS_TWEAK_CAST(__serpent_encrypt),\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int xts_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->crypt_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = &ctx->tweak_ctx,\r\n.tweak_fn = XTS_TWEAK_CAST(__serpent_encrypt),\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int ablk_set_key(struct crypto_ablkcipher *tfm, const u8 *key,\r\nunsigned int key_len)\r\n{\r\nstruct async_serpent_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct crypto_ablkcipher *child = &ctx->cryptd_tfm->base;\r\nint err;\r\ncrypto_ablkcipher_clear_flags(child, CRYPTO_TFM_REQ_MASK);\r\ncrypto_ablkcipher_set_flags(child, crypto_ablkcipher_get_flags(tfm)\r\n& CRYPTO_TFM_REQ_MASK);\r\nerr = crypto_ablkcipher_setkey(child, key, key_len);\r\ncrypto_ablkcipher_set_flags(tfm, crypto_ablkcipher_get_flags(child)\r\n& CRYPTO_TFM_RES_MASK);\r\nreturn err;\r\n}\r\nstatic int __ablk_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nstruct async_serpent_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nstruct blkcipher_desc desc;\r\ndesc.tfm = cryptd_ablkcipher_child(ctx->cryptd_tfm);\r\ndesc.info = req->info;\r\ndesc.flags = 0;\r\nreturn crypto_blkcipher_crt(desc.tfm)->encrypt(\r\n&desc, req->dst, req->src, req->nbytes);\r\n}\r\nstatic int ablk_encrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nstruct async_serpent_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nif (!irq_fpu_usable()) {\r\nstruct ablkcipher_request *cryptd_req =\r\nablkcipher_request_ctx(req);\r\nmemcpy(cryptd_req, req, sizeof(*req));\r\nablkcipher_request_set_tfm(cryptd_req, &ctx->cryptd_tfm->base);\r\nreturn crypto_ablkcipher_encrypt(cryptd_req);\r\n} else {\r\nreturn __ablk_encrypt(req);\r\n}\r\n}\r\nstatic int ablk_decrypt(struct ablkcipher_request *req)\r\n{\r\nstruct crypto_ablkcipher *tfm = crypto_ablkcipher_reqtfm(req);\r\nstruct async_serpent_ctx *ctx = crypto_ablkcipher_ctx(tfm);\r\nif (!irq_fpu_usable()) {\r\nstruct ablkcipher_request *cryptd_req =\r\nablkcipher_request_ctx(req);\r\nmemcpy(cryptd_req, req, sizeof(*req));\r\nablkcipher_request_set_tfm(cryptd_req, &ctx->cryptd_tfm->base);\r\nreturn crypto_ablkcipher_decrypt(cryptd_req);\r\n} else {\r\nstruct blkcipher_desc desc;\r\ndesc.tfm = cryptd_ablkcipher_child(ctx->cryptd_tfm);\r\ndesc.info = req->info;\r\ndesc.flags = 0;\r\nreturn crypto_blkcipher_crt(desc.tfm)->decrypt(\r\n&desc, req->dst, req->src, req->nbytes);\r\n}\r\n}\r\nstatic void ablk_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct async_serpent_ctx *ctx = crypto_tfm_ctx(tfm);\r\ncryptd_free_ablkcipher(ctx->cryptd_tfm);\r\n}\r\nstatic int ablk_init(struct crypto_tfm *tfm)\r\n{\r\nstruct async_serpent_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct cryptd_ablkcipher *cryptd_tfm;\r\nchar drv_name[CRYPTO_MAX_ALG_NAME];\r\nsnprintf(drv_name, sizeof(drv_name), "__driver-%s",\r\ncrypto_tfm_alg_driver_name(tfm));\r\ncryptd_tfm = cryptd_alloc_ablkcipher(drv_name, 0, 0);\r\nif (IS_ERR(cryptd_tfm))\r\nreturn PTR_ERR(cryptd_tfm);\r\nctx->cryptd_tfm = cryptd_tfm;\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct ablkcipher_request) +\r\ncrypto_ablkcipher_reqsize(&cryptd_tfm->base);\r\nreturn 0;\r\n}\r\nstatic int __init serpent_sse2_init(void)\r\n{\r\nif (!cpu_has_xmm2) {\r\nprintk(KERN_INFO "SSE2 instructions are not detected.\n");\r\nreturn -ENODEV;\r\n}\r\nreturn crypto_register_algs(serpent_algs, ARRAY_SIZE(serpent_algs));\r\n}\r\nstatic void __exit serpent_sse2_exit(void)\r\n{\r\ncrypto_unregister_algs(serpent_algs, ARRAY_SIZE(serpent_algs));\r\n}
