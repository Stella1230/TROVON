__weak int hw_breakpoint_weight(struct perf_event *bp)\r\n{\r\nreturn 1;\r\n}\r\nstatic inline enum bp_type_idx find_slot_idx(struct perf_event *bp)\r\n{\r\nif (bp->attr.bp_type & HW_BREAKPOINT_RW)\r\nreturn TYPE_DATA;\r\nreturn TYPE_INST;\r\n}\r\nstatic unsigned int max_task_bp_pinned(int cpu, enum bp_type_idx type)\r\n{\r\nint i;\r\nunsigned int *tsk_pinned = per_cpu(nr_task_bp_pinned[type], cpu);\r\nfor (i = nr_slots[type] - 1; i >= 0; i--) {\r\nif (tsk_pinned[i] > 0)\r\nreturn i + 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int task_bp_pinned(struct perf_event *bp, enum bp_type_idx type)\r\n{\r\nstruct task_struct *tsk = bp->hw.bp_target;\r\nstruct perf_event *iter;\r\nint count = 0;\r\nlist_for_each_entry(iter, &bp_task_head, hw.bp_list) {\r\nif (iter->hw.bp_target == tsk && find_slot_idx(iter) == type)\r\ncount += hw_breakpoint_weight(iter);\r\n}\r\nreturn count;\r\n}\r\nstatic void\r\nfetch_bp_busy_slots(struct bp_busy_slots *slots, struct perf_event *bp,\r\nenum bp_type_idx type)\r\n{\r\nint cpu = bp->cpu;\r\nstruct task_struct *tsk = bp->hw.bp_target;\r\nif (cpu >= 0) {\r\nslots->pinned = per_cpu(nr_cpu_bp_pinned[type], cpu);\r\nif (!tsk)\r\nslots->pinned += max_task_bp_pinned(cpu, type);\r\nelse\r\nslots->pinned += task_bp_pinned(bp, type);\r\nslots->flexible = per_cpu(nr_bp_flexible[type], cpu);\r\nreturn;\r\n}\r\nfor_each_online_cpu(cpu) {\r\nunsigned int nr;\r\nnr = per_cpu(nr_cpu_bp_pinned[type], cpu);\r\nif (!tsk)\r\nnr += max_task_bp_pinned(cpu, type);\r\nelse\r\nnr += task_bp_pinned(bp, type);\r\nif (nr > slots->pinned)\r\nslots->pinned = nr;\r\nnr = per_cpu(nr_bp_flexible[type], cpu);\r\nif (nr > slots->flexible)\r\nslots->flexible = nr;\r\n}\r\n}\r\nstatic void\r\nfetch_this_slot(struct bp_busy_slots *slots, int weight)\r\n{\r\nslots->pinned += weight;\r\n}\r\nstatic void toggle_bp_task_slot(struct perf_event *bp, int cpu, bool enable,\r\nenum bp_type_idx type, int weight)\r\n{\r\nunsigned int *tsk_pinned;\r\nint old_count = 0;\r\nint old_idx = 0;\r\nint idx = 0;\r\nold_count = task_bp_pinned(bp, type);\r\nold_idx = old_count - 1;\r\nidx = old_idx + weight;\r\ntsk_pinned = per_cpu(nr_task_bp_pinned[type], cpu);\r\nif (enable) {\r\ntsk_pinned[idx]++;\r\nif (old_count > 0)\r\ntsk_pinned[old_idx]--;\r\n} else {\r\ntsk_pinned[idx]--;\r\nif (old_count > 0)\r\ntsk_pinned[old_idx]++;\r\n}\r\n}\r\nstatic void\r\ntoggle_bp_slot(struct perf_event *bp, bool enable, enum bp_type_idx type,\r\nint weight)\r\n{\r\nint cpu = bp->cpu;\r\nstruct task_struct *tsk = bp->hw.bp_target;\r\nif (!tsk) {\r\nif (enable)\r\nper_cpu(nr_cpu_bp_pinned[type], bp->cpu) += weight;\r\nelse\r\nper_cpu(nr_cpu_bp_pinned[type], bp->cpu) -= weight;\r\nreturn;\r\n}\r\nif (!enable)\r\nlist_del(&bp->hw.bp_list);\r\nif (cpu >= 0) {\r\ntoggle_bp_task_slot(bp, cpu, enable, type, weight);\r\n} else {\r\nfor_each_online_cpu(cpu)\r\ntoggle_bp_task_slot(bp, cpu, enable, type, weight);\r\n}\r\nif (enable)\r\nlist_add_tail(&bp->hw.bp_list, &bp_task_head);\r\n}\r\n__weak void arch_unregister_hw_breakpoint(struct perf_event *bp)\r\n{\r\n}\r\nstatic int __reserve_bp_slot(struct perf_event *bp)\r\n{\r\nstruct bp_busy_slots slots = {0};\r\nenum bp_type_idx type;\r\nint weight;\r\nif (!constraints_initialized)\r\nreturn -ENOMEM;\r\nif (bp->attr.bp_type == HW_BREAKPOINT_EMPTY ||\r\nbp->attr.bp_type == HW_BREAKPOINT_INVALID)\r\nreturn -EINVAL;\r\ntype = find_slot_idx(bp);\r\nweight = hw_breakpoint_weight(bp);\r\nfetch_bp_busy_slots(&slots, bp, type);\r\nfetch_this_slot(&slots, weight);\r\nif (slots.pinned + (!!slots.flexible) > nr_slots[type])\r\nreturn -ENOSPC;\r\ntoggle_bp_slot(bp, true, type, weight);\r\nreturn 0;\r\n}\r\nint reserve_bp_slot(struct perf_event *bp)\r\n{\r\nint ret;\r\nmutex_lock(&nr_bp_mutex);\r\nret = __reserve_bp_slot(bp);\r\nmutex_unlock(&nr_bp_mutex);\r\nreturn ret;\r\n}\r\nstatic void __release_bp_slot(struct perf_event *bp)\r\n{\r\nenum bp_type_idx type;\r\nint weight;\r\ntype = find_slot_idx(bp);\r\nweight = hw_breakpoint_weight(bp);\r\ntoggle_bp_slot(bp, false, type, weight);\r\n}\r\nvoid release_bp_slot(struct perf_event *bp)\r\n{\r\nmutex_lock(&nr_bp_mutex);\r\narch_unregister_hw_breakpoint(bp);\r\n__release_bp_slot(bp);\r\nmutex_unlock(&nr_bp_mutex);\r\n}\r\nint dbg_reserve_bp_slot(struct perf_event *bp)\r\n{\r\nif (mutex_is_locked(&nr_bp_mutex))\r\nreturn -1;\r\nreturn __reserve_bp_slot(bp);\r\n}\r\nint dbg_release_bp_slot(struct perf_event *bp)\r\n{\r\nif (mutex_is_locked(&nr_bp_mutex))\r\nreturn -1;\r\n__release_bp_slot(bp);\r\nreturn 0;\r\n}\r\nstatic int validate_hw_breakpoint(struct perf_event *bp)\r\n{\r\nint ret;\r\nret = arch_validate_hwbkpt_settings(bp);\r\nif (ret)\r\nreturn ret;\r\nif (arch_check_bp_in_kernelspace(bp)) {\r\nif (bp->attr.exclude_kernel)\r\nreturn -EINVAL;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\n}\r\nreturn 0;\r\n}\r\nint register_perf_hw_breakpoint(struct perf_event *bp)\r\n{\r\nint ret;\r\nret = reserve_bp_slot(bp);\r\nif (ret)\r\nreturn ret;\r\nret = validate_hw_breakpoint(bp);\r\nif (ret)\r\nrelease_bp_slot(bp);\r\nreturn ret;\r\n}\r\nstruct perf_event *\r\nregister_user_hw_breakpoint(struct perf_event_attr *attr,\r\nperf_overflow_handler_t triggered,\r\nvoid *context,\r\nstruct task_struct *tsk)\r\n{\r\nreturn perf_event_create_kernel_counter(attr, -1, tsk, triggered,\r\ncontext);\r\n}\r\nint modify_user_hw_breakpoint(struct perf_event *bp, struct perf_event_attr *attr)\r\n{\r\nu64 old_addr = bp->attr.bp_addr;\r\nu64 old_len = bp->attr.bp_len;\r\nint old_type = bp->attr.bp_type;\r\nint err = 0;\r\nperf_event_disable(bp);\r\nbp->attr.bp_addr = attr->bp_addr;\r\nbp->attr.bp_type = attr->bp_type;\r\nbp->attr.bp_len = attr->bp_len;\r\nif (attr->disabled)\r\ngoto end;\r\nerr = validate_hw_breakpoint(bp);\r\nif (!err)\r\nperf_event_enable(bp);\r\nif (err) {\r\nbp->attr.bp_addr = old_addr;\r\nbp->attr.bp_type = old_type;\r\nbp->attr.bp_len = old_len;\r\nif (!bp->attr.disabled)\r\nperf_event_enable(bp);\r\nreturn err;\r\n}\r\nend:\r\nbp->attr.disabled = attr->disabled;\r\nreturn 0;\r\n}\r\nvoid unregister_hw_breakpoint(struct perf_event *bp)\r\n{\r\nif (!bp)\r\nreturn;\r\nperf_event_release_kernel(bp);\r\n}\r\nstruct perf_event * __percpu *\r\nregister_wide_hw_breakpoint(struct perf_event_attr *attr,\r\nperf_overflow_handler_t triggered,\r\nvoid *context)\r\n{\r\nstruct perf_event * __percpu *cpu_events, **pevent, *bp;\r\nlong err;\r\nint cpu;\r\ncpu_events = alloc_percpu(typeof(*cpu_events));\r\nif (!cpu_events)\r\nreturn (void __percpu __force *)ERR_PTR(-ENOMEM);\r\nget_online_cpus();\r\nfor_each_online_cpu(cpu) {\r\npevent = per_cpu_ptr(cpu_events, cpu);\r\nbp = perf_event_create_kernel_counter(attr, cpu, NULL,\r\ntriggered, context);\r\n*pevent = bp;\r\nif (IS_ERR(bp)) {\r\nerr = PTR_ERR(bp);\r\ngoto fail;\r\n}\r\n}\r\nput_online_cpus();\r\nreturn cpu_events;\r\nfail:\r\nfor_each_online_cpu(cpu) {\r\npevent = per_cpu_ptr(cpu_events, cpu);\r\nif (IS_ERR(*pevent))\r\nbreak;\r\nunregister_hw_breakpoint(*pevent);\r\n}\r\nput_online_cpus();\r\nfree_percpu(cpu_events);\r\nreturn (void __percpu __force *)ERR_PTR(err);\r\n}\r\nvoid unregister_wide_hw_breakpoint(struct perf_event * __percpu *cpu_events)\r\n{\r\nint cpu;\r\nstruct perf_event **pevent;\r\nfor_each_possible_cpu(cpu) {\r\npevent = per_cpu_ptr(cpu_events, cpu);\r\nunregister_hw_breakpoint(*pevent);\r\n}\r\nfree_percpu(cpu_events);\r\n}\r\nstatic void bp_perf_event_destroy(struct perf_event *event)\r\n{\r\nrelease_bp_slot(event);\r\n}\r\nstatic int hw_breakpoint_event_init(struct perf_event *bp)\r\n{\r\nint err;\r\nif (bp->attr.type != PERF_TYPE_BREAKPOINT)\r\nreturn -ENOENT;\r\nif (has_branch_stack(bp))\r\nreturn -EOPNOTSUPP;\r\nerr = register_perf_hw_breakpoint(bp);\r\nif (err)\r\nreturn err;\r\nbp->destroy = bp_perf_event_destroy;\r\nreturn 0;\r\n}\r\nstatic int hw_breakpoint_add(struct perf_event *bp, int flags)\r\n{\r\nif (!(flags & PERF_EF_START))\r\nbp->hw.state = PERF_HES_STOPPED;\r\nreturn arch_install_hw_breakpoint(bp);\r\n}\r\nstatic void hw_breakpoint_del(struct perf_event *bp, int flags)\r\n{\r\narch_uninstall_hw_breakpoint(bp);\r\n}\r\nstatic void hw_breakpoint_start(struct perf_event *bp, int flags)\r\n{\r\nbp->hw.state = 0;\r\n}\r\nstatic void hw_breakpoint_stop(struct perf_event *bp, int flags)\r\n{\r\nbp->hw.state = PERF_HES_STOPPED;\r\n}\r\nstatic int hw_breakpoint_event_idx(struct perf_event *bp)\r\n{\r\nreturn 0;\r\n}\r\nint __init init_hw_breakpoint(void)\r\n{\r\nunsigned int **task_bp_pinned;\r\nint cpu, err_cpu;\r\nint i;\r\nfor (i = 0; i < TYPE_MAX; i++)\r\nnr_slots[i] = hw_breakpoint_slots(i);\r\nfor_each_possible_cpu(cpu) {\r\nfor (i = 0; i < TYPE_MAX; i++) {\r\ntask_bp_pinned = &per_cpu(nr_task_bp_pinned[i], cpu);\r\n*task_bp_pinned = kzalloc(sizeof(int) * nr_slots[i],\r\nGFP_KERNEL);\r\nif (!*task_bp_pinned)\r\ngoto err_alloc;\r\n}\r\n}\r\nconstraints_initialized = 1;\r\nperf_pmu_register(&perf_breakpoint, "breakpoint", PERF_TYPE_BREAKPOINT);\r\nreturn register_die_notifier(&hw_breakpoint_exceptions_nb);\r\nerr_alloc:\r\nfor_each_possible_cpu(err_cpu) {\r\nfor (i = 0; i < TYPE_MAX; i++)\r\nkfree(per_cpu(nr_task_bp_pinned[i], cpu));\r\nif (err_cpu == cpu)\r\nbreak;\r\n}\r\nreturn -ENOMEM;\r\n}
