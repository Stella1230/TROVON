void __ioat2_issue_pending(struct ioat2_dma_chan *ioat)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nioat->dmacount += ioat2_ring_pending(ioat);\r\nioat->issued = ioat->head;\r\nwritew(ioat->dmacount, chan->reg_base + IOAT_CHAN_DMACOUNT_OFFSET);\r\ndev_dbg(to_dev(chan),\r\n"%s: head: %#x tail: %#x issued: %#x count: %#x\n",\r\n__func__, ioat->head, ioat->tail, ioat->issued, ioat->dmacount);\r\n}\r\nvoid ioat2_issue_pending(struct dma_chan *c)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nif (ioat2_ring_pending(ioat)) {\r\nspin_lock_bh(&ioat->prep_lock);\r\n__ioat2_issue_pending(ioat);\r\nspin_unlock_bh(&ioat->prep_lock);\r\n}\r\n}\r\nstatic void ioat2_update_pending(struct ioat2_dma_chan *ioat)\r\n{\r\nif (ioat2_ring_pending(ioat) > ioat_pending_level)\r\n__ioat2_issue_pending(ioat);\r\n}\r\nstatic void __ioat2_start_null_desc(struct ioat2_dma_chan *ioat)\r\n{\r\nstruct ioat_ring_ent *desc;\r\nstruct ioat_dma_descriptor *hw;\r\nif (ioat2_ring_space(ioat) < 1) {\r\ndev_err(to_dev(&ioat->base),\r\n"Unable to start null desc - ring full\n");\r\nreturn;\r\n}\r\ndev_dbg(to_dev(&ioat->base), "%s: head: %#x tail: %#x issued: %#x\n",\r\n__func__, ioat->head, ioat->tail, ioat->issued);\r\ndesc = ioat2_get_ring_ent(ioat, ioat->head);\r\nhw = desc->hw;\r\nhw->ctl = 0;\r\nhw->ctl_f.null = 1;\r\nhw->ctl_f.int_en = 1;\r\nhw->ctl_f.compl_write = 1;\r\nhw->size = NULL_DESC_BUFFER_SIZE;\r\nhw->src_addr = 0;\r\nhw->dst_addr = 0;\r\nasync_tx_ack(&desc->txd);\r\nioat2_set_chainaddr(ioat, desc->txd.phys);\r\ndump_desc_dbg(ioat, desc);\r\nwmb();\r\nioat->head += 1;\r\n__ioat2_issue_pending(ioat);\r\n}\r\nstatic void ioat2_start_null_desc(struct ioat2_dma_chan *ioat)\r\n{\r\nspin_lock_bh(&ioat->prep_lock);\r\n__ioat2_start_null_desc(ioat);\r\nspin_unlock_bh(&ioat->prep_lock);\r\n}\r\nstatic void __cleanup(struct ioat2_dma_chan *ioat, dma_addr_t phys_complete)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct ioat_ring_ent *desc;\r\nbool seen_current = false;\r\nu16 active;\r\nint idx = ioat->tail, i;\r\ndev_dbg(to_dev(chan), "%s: head: %#x tail: %#x issued: %#x\n",\r\n__func__, ioat->head, ioat->tail, ioat->issued);\r\nactive = ioat2_ring_active(ioat);\r\nfor (i = 0; i < active && !seen_current; i++) {\r\nsmp_read_barrier_depends();\r\nprefetch(ioat2_get_ring_ent(ioat, idx + i + 1));\r\ndesc = ioat2_get_ring_ent(ioat, idx + i);\r\ntx = &desc->txd;\r\ndump_desc_dbg(ioat, desc);\r\nif (tx->cookie) {\r\nioat_dma_unmap(chan, tx->flags, desc->len, desc->hw);\r\ndma_cookie_complete(tx);\r\nif (tx->callback) {\r\ntx->callback(tx->callback_param);\r\ntx->callback = NULL;\r\n}\r\n}\r\nif (tx->phys == phys_complete)\r\nseen_current = true;\r\n}\r\nsmp_mb();\r\nioat->tail = idx + i;\r\nBUG_ON(active && !seen_current);\r\nchan->last_completion = phys_complete;\r\nif (active - i == 0) {\r\ndev_dbg(to_dev(chan), "%s: cancel completion timeout\n",\r\n__func__);\r\nclear_bit(IOAT_COMPLETION_PENDING, &chan->state);\r\nmod_timer(&chan->timer, jiffies + IDLE_TIMEOUT);\r\n}\r\n}\r\nstatic void ioat2_cleanup(struct ioat2_dma_chan *ioat)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\ndma_addr_t phys_complete;\r\nspin_lock_bh(&chan->cleanup_lock);\r\nif (ioat_cleanup_preamble(chan, &phys_complete))\r\n__cleanup(ioat, phys_complete);\r\nspin_unlock_bh(&chan->cleanup_lock);\r\n}\r\nvoid ioat2_cleanup_event(unsigned long data)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan((void *) data);\r\nioat2_cleanup(ioat);\r\nwritew(IOAT_CHANCTRL_RUN, ioat->base.reg_base + IOAT_CHANCTRL_OFFSET);\r\n}\r\nvoid __ioat2_restart_chan(struct ioat2_dma_chan *ioat)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nioat->issued = ioat->tail;\r\nioat->dmacount = 0;\r\nset_bit(IOAT_COMPLETION_PENDING, &chan->state);\r\nmod_timer(&chan->timer, jiffies + COMPLETION_TIMEOUT);\r\ndev_dbg(to_dev(chan),\r\n"%s: head: %#x tail: %#x issued: %#x count: %#x\n",\r\n__func__, ioat->head, ioat->tail, ioat->issued, ioat->dmacount);\r\nif (ioat2_ring_pending(ioat)) {\r\nstruct ioat_ring_ent *desc;\r\ndesc = ioat2_get_ring_ent(ioat, ioat->tail);\r\nioat2_set_chainaddr(ioat, desc->txd.phys);\r\n__ioat2_issue_pending(ioat);\r\n} else\r\n__ioat2_start_null_desc(ioat);\r\n}\r\nint ioat2_quiesce(struct ioat_chan_common *chan, unsigned long tmo)\r\n{\r\nunsigned long end = jiffies + tmo;\r\nint err = 0;\r\nu32 status;\r\nstatus = ioat_chansts(chan);\r\nif (is_ioat_active(status) || is_ioat_idle(status))\r\nioat_suspend(chan);\r\nwhile (is_ioat_active(status) || is_ioat_idle(status)) {\r\nif (tmo && time_after(jiffies, end)) {\r\nerr = -ETIMEDOUT;\r\nbreak;\r\n}\r\nstatus = ioat_chansts(chan);\r\ncpu_relax();\r\n}\r\nreturn err;\r\n}\r\nint ioat2_reset_sync(struct ioat_chan_common *chan, unsigned long tmo)\r\n{\r\nunsigned long end = jiffies + tmo;\r\nint err = 0;\r\nioat_reset(chan);\r\nwhile (ioat_reset_pending(chan)) {\r\nif (end && time_after(jiffies, end)) {\r\nerr = -ETIMEDOUT;\r\nbreak;\r\n}\r\ncpu_relax();\r\n}\r\nreturn err;\r\n}\r\nstatic void ioat2_restart_channel(struct ioat2_dma_chan *ioat)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\ndma_addr_t phys_complete;\r\nioat2_quiesce(chan, 0);\r\nif (ioat_cleanup_preamble(chan, &phys_complete))\r\n__cleanup(ioat, phys_complete);\r\n__ioat2_restart_chan(ioat);\r\n}\r\nvoid ioat2_timer_event(unsigned long data)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan((void *) data);\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nif (test_bit(IOAT_COMPLETION_PENDING, &chan->state)) {\r\ndma_addr_t phys_complete;\r\nu64 status;\r\nstatus = ioat_chansts(chan);\r\nif (is_ioat_halted(status)) {\r\nu32 chanerr;\r\nchanerr = readl(chan->reg_base + IOAT_CHANERR_OFFSET);\r\ndev_err(to_dev(chan), "%s: Channel halted (%x)\n",\r\n__func__, chanerr);\r\nif (test_bit(IOAT_RUN, &chan->state))\r\nBUG_ON(is_ioat_bug(chanerr));\r\nelse\r\nreturn;\r\n}\r\nspin_lock_bh(&chan->cleanup_lock);\r\nif (ioat_cleanup_preamble(chan, &phys_complete)) {\r\n__cleanup(ioat, phys_complete);\r\n} else if (test_bit(IOAT_COMPLETION_ACK, &chan->state)) {\r\nspin_lock_bh(&ioat->prep_lock);\r\nioat2_restart_channel(ioat);\r\nspin_unlock_bh(&ioat->prep_lock);\r\n} else {\r\nset_bit(IOAT_COMPLETION_ACK, &chan->state);\r\nmod_timer(&chan->timer, jiffies + COMPLETION_TIMEOUT);\r\n}\r\nspin_unlock_bh(&chan->cleanup_lock);\r\n} else {\r\nu16 active;\r\nspin_lock_bh(&chan->cleanup_lock);\r\nspin_lock_bh(&ioat->prep_lock);\r\nactive = ioat2_ring_active(ioat);\r\nif (active == 0 && ioat->alloc_order > ioat_get_alloc_order())\r\nreshape_ring(ioat, ioat->alloc_order-1);\r\nspin_unlock_bh(&ioat->prep_lock);\r\nspin_unlock_bh(&chan->cleanup_lock);\r\nif (ioat->alloc_order > ioat_get_alloc_order())\r\nmod_timer(&chan->timer, jiffies + IDLE_TIMEOUT);\r\n}\r\n}\r\nstatic int ioat2_reset_hw(struct ioat_chan_common *chan)\r\n{\r\nu32 chanerr;\r\nioat2_quiesce(chan, msecs_to_jiffies(100));\r\nchanerr = readl(chan->reg_base + IOAT_CHANERR_OFFSET);\r\nwritel(chanerr, chan->reg_base + IOAT_CHANERR_OFFSET);\r\nreturn ioat2_reset_sync(chan, msecs_to_jiffies(200));\r\n}\r\nint ioat2_enumerate_channels(struct ioatdma_device *device)\r\n{\r\nstruct ioat2_dma_chan *ioat;\r\nstruct device *dev = &device->pdev->dev;\r\nstruct dma_device *dma = &device->common;\r\nu8 xfercap_log;\r\nint i;\r\nINIT_LIST_HEAD(&dma->channels);\r\ndma->chancnt = readb(device->reg_base + IOAT_CHANCNT_OFFSET);\r\ndma->chancnt &= 0x1f;\r\nif (dma->chancnt > ARRAY_SIZE(device->idx)) {\r\ndev_warn(dev, "(%d) exceeds max supported channels (%zu)\n",\r\ndma->chancnt, ARRAY_SIZE(device->idx));\r\ndma->chancnt = ARRAY_SIZE(device->idx);\r\n}\r\nxfercap_log = readb(device->reg_base + IOAT_XFERCAP_OFFSET);\r\nxfercap_log &= 0x1f;\r\nif (xfercap_log == 0)\r\nreturn 0;\r\ndev_dbg(dev, "%s: xfercap = %d\n", __func__, 1 << xfercap_log);\r\n#ifdef CONFIG_I7300_IDLE_IOAT_CHANNEL\r\nif (i7300_idle_platform_probe(NULL, NULL, 1) == 0)\r\ndma->chancnt--;\r\n#endif\r\nfor (i = 0; i < dma->chancnt; i++) {\r\nioat = devm_kzalloc(dev, sizeof(*ioat), GFP_KERNEL);\r\nif (!ioat)\r\nbreak;\r\nioat_init_channel(device, &ioat->base, i);\r\nioat->xfercap_log = xfercap_log;\r\nspin_lock_init(&ioat->prep_lock);\r\nif (device->reset_hw(&ioat->base)) {\r\ni = 0;\r\nbreak;\r\n}\r\n}\r\ndma->chancnt = i;\r\nreturn i;\r\n}\r\nstatic dma_cookie_t ioat2_tx_submit_unlock(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct dma_chan *c = tx->chan;\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nstruct ioat_chan_common *chan = &ioat->base;\r\ndma_cookie_t cookie;\r\ncookie = dma_cookie_assign(tx);\r\ndev_dbg(to_dev(&ioat->base), "%s: cookie: %d\n", __func__, cookie);\r\nif (!test_and_set_bit(IOAT_COMPLETION_PENDING, &chan->state))\r\nmod_timer(&chan->timer, jiffies + COMPLETION_TIMEOUT);\r\nwmb();\r\nioat->head += ioat->produce;\r\nioat2_update_pending(ioat);\r\nspin_unlock_bh(&ioat->prep_lock);\r\nreturn cookie;\r\n}\r\nstatic struct ioat_ring_ent *ioat2_alloc_ring_ent(struct dma_chan *chan, gfp_t flags)\r\n{\r\nstruct ioat_dma_descriptor *hw;\r\nstruct ioat_ring_ent *desc;\r\nstruct ioatdma_device *dma;\r\ndma_addr_t phys;\r\ndma = to_ioatdma_device(chan->device);\r\nhw = pci_pool_alloc(dma->dma_pool, flags, &phys);\r\nif (!hw)\r\nreturn NULL;\r\nmemset(hw, 0, sizeof(*hw));\r\ndesc = kmem_cache_alloc(ioat2_cache, flags);\r\nif (!desc) {\r\npci_pool_free(dma->dma_pool, hw, phys);\r\nreturn NULL;\r\n}\r\nmemset(desc, 0, sizeof(*desc));\r\ndma_async_tx_descriptor_init(&desc->txd, chan);\r\ndesc->txd.tx_submit = ioat2_tx_submit_unlock;\r\ndesc->hw = hw;\r\ndesc->txd.phys = phys;\r\nreturn desc;\r\n}\r\nstatic void ioat2_free_ring_ent(struct ioat_ring_ent *desc, struct dma_chan *chan)\r\n{\r\nstruct ioatdma_device *dma;\r\ndma = to_ioatdma_device(chan->device);\r\npci_pool_free(dma->dma_pool, desc->hw, desc->txd.phys);\r\nkmem_cache_free(ioat2_cache, desc);\r\n}\r\nstatic struct ioat_ring_ent **ioat2_alloc_ring(struct dma_chan *c, int order, gfp_t flags)\r\n{\r\nstruct ioat_ring_ent **ring;\r\nint descs = 1 << order;\r\nint i;\r\nif (order > ioat_get_max_alloc_order())\r\nreturn NULL;\r\nring = kcalloc(descs, sizeof(*ring), flags);\r\nif (!ring)\r\nreturn NULL;\r\nfor (i = 0; i < descs; i++) {\r\nring[i] = ioat2_alloc_ring_ent(c, flags);\r\nif (!ring[i]) {\r\nwhile (i--)\r\nioat2_free_ring_ent(ring[i], c);\r\nkfree(ring);\r\nreturn NULL;\r\n}\r\nset_desc_id(ring[i], i);\r\n}\r\nfor (i = 0; i < descs-1; i++) {\r\nstruct ioat_ring_ent *next = ring[i+1];\r\nstruct ioat_dma_descriptor *hw = ring[i]->hw;\r\nhw->next = next->txd.phys;\r\n}\r\nring[i]->hw->next = ring[0]->txd.phys;\r\nreturn ring;\r\n}\r\nint ioat2_alloc_chan_resources(struct dma_chan *c)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nstruct ioat_ring_ent **ring;\r\nu64 status;\r\nint order;\r\nint i = 0;\r\nif (ioat->ring)\r\nreturn 1 << ioat->alloc_order;\r\nwritew(IOAT_CHANCTRL_RUN, chan->reg_base + IOAT_CHANCTRL_OFFSET);\r\nchan->completion = pci_pool_alloc(chan->device->completion_pool,\r\nGFP_KERNEL, &chan->completion_dma);\r\nif (!chan->completion)\r\nreturn -ENOMEM;\r\nmemset(chan->completion, 0, sizeof(*chan->completion));\r\nwritel(((u64) chan->completion_dma) & 0x00000000FFFFFFFF,\r\nchan->reg_base + IOAT_CHANCMP_OFFSET_LOW);\r\nwritel(((u64) chan->completion_dma) >> 32,\r\nchan->reg_base + IOAT_CHANCMP_OFFSET_HIGH);\r\norder = ioat_get_alloc_order();\r\nring = ioat2_alloc_ring(c, order, GFP_KERNEL);\r\nif (!ring)\r\nreturn -ENOMEM;\r\nspin_lock_bh(&chan->cleanup_lock);\r\nspin_lock_bh(&ioat->prep_lock);\r\nioat->ring = ring;\r\nioat->head = 0;\r\nioat->issued = 0;\r\nioat->tail = 0;\r\nioat->alloc_order = order;\r\nspin_unlock_bh(&ioat->prep_lock);\r\nspin_unlock_bh(&chan->cleanup_lock);\r\ntasklet_enable(&chan->cleanup_task);\r\nioat2_start_null_desc(ioat);\r\ndo {\r\nudelay(1);\r\nstatus = ioat_chansts(chan);\r\n} while (i++ < 20 && !is_ioat_active(status) && !is_ioat_idle(status));\r\nif (is_ioat_active(status) || is_ioat_idle(status)) {\r\nset_bit(IOAT_RUN, &chan->state);\r\nreturn 1 << ioat->alloc_order;\r\n} else {\r\nu32 chanerr = readl(chan->reg_base + IOAT_CHANERR_OFFSET);\r\ndev_WARN(to_dev(chan),\r\n"failed to start channel chanerr: %#x\n", chanerr);\r\nioat2_free_chan_resources(c);\r\nreturn -EFAULT;\r\n}\r\n}\r\nbool reshape_ring(struct ioat2_dma_chan *ioat, int order)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nstruct dma_chan *c = &chan->common;\r\nconst u32 curr_size = ioat2_ring_size(ioat);\r\nconst u16 active = ioat2_ring_active(ioat);\r\nconst u32 new_size = 1 << order;\r\nstruct ioat_ring_ent **ring;\r\nu16 i;\r\nif (order > ioat_get_max_alloc_order())\r\nreturn false;\r\nif (active == curr_size)\r\nreturn false;\r\nif (active >= new_size)\r\nreturn false;\r\nring = kcalloc(new_size, sizeof(*ring), GFP_NOWAIT);\r\nif (!ring)\r\nreturn false;\r\nif (new_size > curr_size) {\r\nfor (i = 0; i < curr_size; i++) {\r\nu16 curr_idx = (ioat->tail+i) & (curr_size-1);\r\nu16 new_idx = (ioat->tail+i) & (new_size-1);\r\nring[new_idx] = ioat->ring[curr_idx];\r\nset_desc_id(ring[new_idx], new_idx);\r\n}\r\nfor (i = curr_size; i < new_size; i++) {\r\nu16 new_idx = (ioat->tail+i) & (new_size-1);\r\nring[new_idx] = ioat2_alloc_ring_ent(c, GFP_NOWAIT);\r\nif (!ring[new_idx]) {\r\nwhile (i--) {\r\nu16 new_idx = (ioat->tail+i) & (new_size-1);\r\nioat2_free_ring_ent(ring[new_idx], c);\r\n}\r\nkfree(ring);\r\nreturn false;\r\n}\r\nset_desc_id(ring[new_idx], new_idx);\r\n}\r\nfor (i = curr_size-1; i < new_size; i++) {\r\nu16 new_idx = (ioat->tail+i) & (new_size-1);\r\nstruct ioat_ring_ent *next = ring[(new_idx+1) & (new_size-1)];\r\nstruct ioat_dma_descriptor *hw = ring[new_idx]->hw;\r\nhw->next = next->txd.phys;\r\n}\r\n} else {\r\nstruct ioat_dma_descriptor *hw;\r\nstruct ioat_ring_ent *next;\r\nfor (i = 0; i < new_size; i++) {\r\nu16 curr_idx = (ioat->tail+i) & (curr_size-1);\r\nu16 new_idx = (ioat->tail+i) & (new_size-1);\r\nring[new_idx] = ioat->ring[curr_idx];\r\nset_desc_id(ring[new_idx], new_idx);\r\n}\r\nfor (i = new_size; i < curr_size; i++) {\r\nstruct ioat_ring_ent *ent;\r\nent = ioat2_get_ring_ent(ioat, ioat->tail+i);\r\nioat2_free_ring_ent(ent, c);\r\n}\r\nhw = ring[(ioat->tail+new_size-1) & (new_size-1)]->hw;\r\nnext = ring[(ioat->tail+new_size) & (new_size-1)];\r\nhw->next = next->txd.phys;\r\n}\r\ndev_dbg(to_dev(chan), "%s: allocated %d descriptors\n",\r\n__func__, new_size);\r\nkfree(ioat->ring);\r\nioat->ring = ring;\r\nioat->alloc_order = order;\r\nreturn true;\r\n}\r\nint ioat2_check_space_lock(struct ioat2_dma_chan *ioat, int num_descs)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nbool retry;\r\nretry:\r\nspin_lock_bh(&ioat->prep_lock);\r\nif (likely(ioat2_ring_space(ioat) > num_descs)) {\r\ndev_dbg(to_dev(chan), "%s: num_descs: %d (%x:%x:%x)\n",\r\n__func__, num_descs, ioat->head, ioat->tail, ioat->issued);\r\nioat->produce = num_descs;\r\nreturn 0;\r\n}\r\nretry = test_and_set_bit(IOAT_RESHAPE_PENDING, &chan->state);\r\nspin_unlock_bh(&ioat->prep_lock);\r\nif (retry)\r\ngoto retry;\r\nspin_lock_bh(&chan->cleanup_lock);\r\nspin_lock_bh(&ioat->prep_lock);\r\nretry = reshape_ring(ioat, ioat->alloc_order + 1);\r\nclear_bit(IOAT_RESHAPE_PENDING, &chan->state);\r\nspin_unlock_bh(&ioat->prep_lock);\r\nspin_unlock_bh(&chan->cleanup_lock);\r\nif (retry)\r\ngoto retry;\r\nif (printk_ratelimit())\r\ndev_dbg(to_dev(chan), "%s: ring full! num_descs: %d (%x:%x:%x)\n",\r\n__func__, num_descs, ioat->head, ioat->tail, ioat->issued);\r\nif (jiffies > chan->timer.expires && timer_pending(&chan->timer)) {\r\nstruct ioatdma_device *device = chan->device;\r\nmod_timer(&chan->timer, jiffies + COMPLETION_TIMEOUT);\r\ndevice->timer_fn((unsigned long) &chan->common);\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstruct dma_async_tx_descriptor *\r\nioat2_dma_prep_memcpy_lock(struct dma_chan *c, dma_addr_t dma_dest,\r\ndma_addr_t dma_src, size_t len, unsigned long flags)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nstruct ioat_dma_descriptor *hw;\r\nstruct ioat_ring_ent *desc;\r\ndma_addr_t dst = dma_dest;\r\ndma_addr_t src = dma_src;\r\nsize_t total_len = len;\r\nint num_descs, idx, i;\r\nnum_descs = ioat2_xferlen_to_descs(ioat, len);\r\nif (likely(num_descs) && ioat2_check_space_lock(ioat, num_descs) == 0)\r\nidx = ioat->head;\r\nelse\r\nreturn NULL;\r\ni = 0;\r\ndo {\r\nsize_t copy = min_t(size_t, len, 1 << ioat->xfercap_log);\r\ndesc = ioat2_get_ring_ent(ioat, idx + i);\r\nhw = desc->hw;\r\nhw->size = copy;\r\nhw->ctl = 0;\r\nhw->src_addr = src;\r\nhw->dst_addr = dst;\r\nlen -= copy;\r\ndst += copy;\r\nsrc += copy;\r\ndump_desc_dbg(ioat, desc);\r\n} while (++i < num_descs);\r\ndesc->txd.flags = flags;\r\ndesc->len = total_len;\r\nhw->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);\r\nhw->ctl_f.fence = !!(flags & DMA_PREP_FENCE);\r\nhw->ctl_f.compl_write = 1;\r\ndump_desc_dbg(ioat, desc);\r\nreturn &desc->txd;\r\n}\r\nvoid ioat2_free_chan_resources(struct dma_chan *c)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nstruct ioatdma_device *device = chan->device;\r\nstruct ioat_ring_ent *desc;\r\nconst u16 total_descs = 1 << ioat->alloc_order;\r\nint descs;\r\nint i;\r\nif (!ioat->ring)\r\nreturn;\r\ntasklet_disable(&chan->cleanup_task);\r\ndel_timer_sync(&chan->timer);\r\ndevice->cleanup_fn((unsigned long) c);\r\ndevice->reset_hw(chan);\r\nclear_bit(IOAT_RUN, &chan->state);\r\nspin_lock_bh(&chan->cleanup_lock);\r\nspin_lock_bh(&ioat->prep_lock);\r\ndescs = ioat2_ring_space(ioat);\r\ndev_dbg(to_dev(chan), "freeing %d idle descriptors\n", descs);\r\nfor (i = 0; i < descs; i++) {\r\ndesc = ioat2_get_ring_ent(ioat, ioat->head + i);\r\nioat2_free_ring_ent(desc, c);\r\n}\r\nif (descs < total_descs)\r\ndev_err(to_dev(chan), "Freeing %d in use descriptors!\n",\r\ntotal_descs - descs);\r\nfor (i = 0; i < total_descs - descs; i++) {\r\ndesc = ioat2_get_ring_ent(ioat, ioat->tail + i);\r\ndump_desc_dbg(ioat, desc);\r\nioat2_free_ring_ent(desc, c);\r\n}\r\nkfree(ioat->ring);\r\nioat->ring = NULL;\r\nioat->alloc_order = 0;\r\npci_pool_free(device->completion_pool, chan->completion,\r\nchan->completion_dma);\r\nspin_unlock_bh(&ioat->prep_lock);\r\nspin_unlock_bh(&chan->cleanup_lock);\r\nchan->last_completion = 0;\r\nchan->completion_dma = 0;\r\nioat->dmacount = 0;\r\n}\r\nstatic ssize_t ring_size_show(struct dma_chan *c, char *page)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nreturn sprintf(page, "%d\n", (1 << ioat->alloc_order) & ~1);\r\n}\r\nstatic ssize_t ring_active_show(struct dma_chan *c, char *page)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nreturn sprintf(page, "%d\n", ioat2_ring_active(ioat));\r\n}\r\nint __devinit ioat2_dma_probe(struct ioatdma_device *device, int dca)\r\n{\r\nstruct pci_dev *pdev = device->pdev;\r\nstruct dma_device *dma;\r\nstruct dma_chan *c;\r\nstruct ioat_chan_common *chan;\r\nint err;\r\ndevice->enumerate_channels = ioat2_enumerate_channels;\r\ndevice->reset_hw = ioat2_reset_hw;\r\ndevice->cleanup_fn = ioat2_cleanup_event;\r\ndevice->timer_fn = ioat2_timer_event;\r\ndevice->self_test = ioat_dma_self_test;\r\ndma = &device->common;\r\ndma->device_prep_dma_memcpy = ioat2_dma_prep_memcpy_lock;\r\ndma->device_issue_pending = ioat2_issue_pending;\r\ndma->device_alloc_chan_resources = ioat2_alloc_chan_resources;\r\ndma->device_free_chan_resources = ioat2_free_chan_resources;\r\ndma->device_tx_status = ioat_dma_tx_status;\r\nerr = ioat_probe(device);\r\nif (err)\r\nreturn err;\r\nioat_set_tcp_copy_break(2048);\r\nlist_for_each_entry(c, &dma->channels, device_node) {\r\nchan = to_chan_common(c);\r\nwritel(IOAT_DCACTRL_CMPL_WRITE_ENABLE | IOAT_DMA_DCA_ANY_CPU,\r\nchan->reg_base + IOAT_DCACTRL_OFFSET);\r\n}\r\nerr = ioat_register(device);\r\nif (err)\r\nreturn err;\r\nioat_kobject_add(device, &ioat2_ktype);\r\nif (dca)\r\ndevice->dca = ioat2_dca_init(pdev, device->reg_base);\r\nreturn err;\r\n}
