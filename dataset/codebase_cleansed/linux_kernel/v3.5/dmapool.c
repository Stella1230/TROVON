static ssize_t\r\nshow_pools(struct device *dev, struct device_attribute *attr, char *buf)\r\n{\r\nunsigned temp;\r\nunsigned size;\r\nchar *next;\r\nstruct dma_page *page;\r\nstruct dma_pool *pool;\r\nnext = buf;\r\nsize = PAGE_SIZE;\r\ntemp = scnprintf(next, size, "poolinfo - 0.1\n");\r\nsize -= temp;\r\nnext += temp;\r\nmutex_lock(&pools_lock);\r\nlist_for_each_entry(pool, &dev->dma_pools, pools) {\r\nunsigned pages = 0;\r\nunsigned blocks = 0;\r\nspin_lock_irq(&pool->lock);\r\nlist_for_each_entry(page, &pool->page_list, page_list) {\r\npages++;\r\nblocks += page->in_use;\r\n}\r\nspin_unlock_irq(&pool->lock);\r\ntemp = scnprintf(next, size, "%-16s %4u %4Zu %4Zu %2u\n",\r\npool->name, blocks,\r\npages * (pool->allocation / pool->size),\r\npool->size, pages);\r\nsize -= temp;\r\nnext += temp;\r\n}\r\nmutex_unlock(&pools_lock);\r\nreturn PAGE_SIZE - size;\r\n}\r\nstruct dma_pool *dma_pool_create(const char *name, struct device *dev,\r\nsize_t size, size_t align, size_t boundary)\r\n{\r\nstruct dma_pool *retval;\r\nsize_t allocation;\r\nif (align == 0) {\r\nalign = 1;\r\n} else if (align & (align - 1)) {\r\nreturn NULL;\r\n}\r\nif (size == 0) {\r\nreturn NULL;\r\n} else if (size < 4) {\r\nsize = 4;\r\n}\r\nif ((size % align) != 0)\r\nsize = ALIGN(size, align);\r\nallocation = max_t(size_t, size, PAGE_SIZE);\r\nif (!boundary) {\r\nboundary = allocation;\r\n} else if ((boundary < size) || (boundary & (boundary - 1))) {\r\nreturn NULL;\r\n}\r\nretval = kmalloc_node(sizeof(*retval), GFP_KERNEL, dev_to_node(dev));\r\nif (!retval)\r\nreturn retval;\r\nstrlcpy(retval->name, name, sizeof(retval->name));\r\nretval->dev = dev;\r\nINIT_LIST_HEAD(&retval->page_list);\r\nspin_lock_init(&retval->lock);\r\nretval->size = size;\r\nretval->boundary = boundary;\r\nretval->allocation = allocation;\r\ninit_waitqueue_head(&retval->waitq);\r\nif (dev) {\r\nint ret;\r\nmutex_lock(&pools_lock);\r\nif (list_empty(&dev->dma_pools))\r\nret = device_create_file(dev, &dev_attr_pools);\r\nelse\r\nret = 0;\r\nif (!ret)\r\nlist_add(&retval->pools, &dev->dma_pools);\r\nelse {\r\nkfree(retval);\r\nretval = NULL;\r\n}\r\nmutex_unlock(&pools_lock);\r\n} else\r\nINIT_LIST_HEAD(&retval->pools);\r\nreturn retval;\r\n}\r\nstatic void pool_initialise_page(struct dma_pool *pool, struct dma_page *page)\r\n{\r\nunsigned int offset = 0;\r\nunsigned int next_boundary = pool->boundary;\r\ndo {\r\nunsigned int next = offset + pool->size;\r\nif (unlikely((next + pool->size) >= next_boundary)) {\r\nnext = next_boundary;\r\nnext_boundary += pool->boundary;\r\n}\r\n*(int *)(page->vaddr + offset) = next;\r\noffset = next;\r\n} while (offset < pool->allocation);\r\n}\r\nstatic struct dma_page *pool_alloc_page(struct dma_pool *pool, gfp_t mem_flags)\r\n{\r\nstruct dma_page *page;\r\npage = kmalloc(sizeof(*page), mem_flags);\r\nif (!page)\r\nreturn NULL;\r\npage->vaddr = dma_alloc_coherent(pool->dev, pool->allocation,\r\n&page->dma, mem_flags);\r\nif (page->vaddr) {\r\n#ifdef DMAPOOL_DEBUG\r\nmemset(page->vaddr, POOL_POISON_FREED, pool->allocation);\r\n#endif\r\npool_initialise_page(pool, page);\r\nlist_add(&page->page_list, &pool->page_list);\r\npage->in_use = 0;\r\npage->offset = 0;\r\n} else {\r\nkfree(page);\r\npage = NULL;\r\n}\r\nreturn page;\r\n}\r\nstatic inline int is_page_busy(struct dma_page *page)\r\n{\r\nreturn page->in_use != 0;\r\n}\r\nstatic void pool_free_page(struct dma_pool *pool, struct dma_page *page)\r\n{\r\ndma_addr_t dma = page->dma;\r\n#ifdef DMAPOOL_DEBUG\r\nmemset(page->vaddr, POOL_POISON_FREED, pool->allocation);\r\n#endif\r\ndma_free_coherent(pool->dev, pool->allocation, page->vaddr, dma);\r\nlist_del(&page->page_list);\r\nkfree(page);\r\n}\r\nvoid dma_pool_destroy(struct dma_pool *pool)\r\n{\r\nmutex_lock(&pools_lock);\r\nlist_del(&pool->pools);\r\nif (pool->dev && list_empty(&pool->dev->dma_pools))\r\ndevice_remove_file(pool->dev, &dev_attr_pools);\r\nmutex_unlock(&pools_lock);\r\nwhile (!list_empty(&pool->page_list)) {\r\nstruct dma_page *page;\r\npage = list_entry(pool->page_list.next,\r\nstruct dma_page, page_list);\r\nif (is_page_busy(page)) {\r\nif (pool->dev)\r\ndev_err(pool->dev,\r\n"dma_pool_destroy %s, %p busy\n",\r\npool->name, page->vaddr);\r\nelse\r\nprintk(KERN_ERR\r\n"dma_pool_destroy %s, %p busy\n",\r\npool->name, page->vaddr);\r\nlist_del(&page->page_list);\r\nkfree(page);\r\n} else\r\npool_free_page(pool, page);\r\n}\r\nkfree(pool);\r\n}\r\nvoid *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,\r\ndma_addr_t *handle)\r\n{\r\nunsigned long flags;\r\nstruct dma_page *page;\r\nsize_t offset;\r\nvoid *retval;\r\nmight_sleep_if(mem_flags & __GFP_WAIT);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nrestart:\r\nlist_for_each_entry(page, &pool->page_list, page_list) {\r\nif (page->offset < pool->allocation)\r\ngoto ready;\r\n}\r\npage = pool_alloc_page(pool, GFP_ATOMIC);\r\nif (!page) {\r\nif (mem_flags & __GFP_WAIT) {\r\nDECLARE_WAITQUEUE(wait, current);\r\n__set_current_state(TASK_UNINTERRUPTIBLE);\r\n__add_wait_queue(&pool->waitq, &wait);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nschedule_timeout(POOL_TIMEOUT_JIFFIES);\r\nspin_lock_irqsave(&pool->lock, flags);\r\n__remove_wait_queue(&pool->waitq, &wait);\r\ngoto restart;\r\n}\r\nretval = NULL;\r\ngoto done;\r\n}\r\nready:\r\npage->in_use++;\r\noffset = page->offset;\r\npage->offset = *(int *)(page->vaddr + offset);\r\nretval = offset + page->vaddr;\r\n*handle = offset + page->dma;\r\n#ifdef DMAPOOL_DEBUG\r\nmemset(retval, POOL_POISON_ALLOCATED, pool->size);\r\n#endif\r\ndone:\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nreturn retval;\r\n}\r\nstatic struct dma_page *pool_find_page(struct dma_pool *pool, dma_addr_t dma)\r\n{\r\nstruct dma_page *page;\r\nlist_for_each_entry(page, &pool->page_list, page_list) {\r\nif (dma < page->dma)\r\ncontinue;\r\nif (dma < (page->dma + pool->allocation))\r\nreturn page;\r\n}\r\nreturn NULL;\r\n}\r\nvoid dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)\r\n{\r\nstruct dma_page *page;\r\nunsigned long flags;\r\nunsigned int offset;\r\nspin_lock_irqsave(&pool->lock, flags);\r\npage = pool_find_page(pool, dma);\r\nif (!page) {\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nif (pool->dev)\r\ndev_err(pool->dev,\r\n"dma_pool_free %s, %p/%lx (bad dma)\n",\r\npool->name, vaddr, (unsigned long)dma);\r\nelse\r\nprintk(KERN_ERR "dma_pool_free %s, %p/%lx (bad dma)\n",\r\npool->name, vaddr, (unsigned long)dma);\r\nreturn;\r\n}\r\noffset = vaddr - page->vaddr;\r\n#ifdef DMAPOOL_DEBUG\r\nif ((dma - page->dma) != offset) {\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nif (pool->dev)\r\ndev_err(pool->dev,\r\n"dma_pool_free %s, %p (bad vaddr)/%Lx\n",\r\npool->name, vaddr, (unsigned long long)dma);\r\nelse\r\nprintk(KERN_ERR\r\n"dma_pool_free %s, %p (bad vaddr)/%Lx\n",\r\npool->name, vaddr, (unsigned long long)dma);\r\nreturn;\r\n}\r\n{\r\nunsigned int chain = page->offset;\r\nwhile (chain < pool->allocation) {\r\nif (chain != offset) {\r\nchain = *(int *)(page->vaddr + chain);\r\ncontinue;\r\n}\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nif (pool->dev)\r\ndev_err(pool->dev, "dma_pool_free %s, dma %Lx "\r\n"already free\n", pool->name,\r\n(unsigned long long)dma);\r\nelse\r\nprintk(KERN_ERR "dma_pool_free %s, dma %Lx "\r\n"already free\n", pool->name,\r\n(unsigned long long)dma);\r\nreturn;\r\n}\r\n}\r\nmemset(vaddr, POOL_POISON_FREED, pool->size);\r\n#endif\r\npage->in_use--;\r\n*(int *)vaddr = page->offset;\r\npage->offset = offset;\r\nif (waitqueue_active(&pool->waitq))\r\nwake_up_locked(&pool->waitq);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic void dmam_pool_release(struct device *dev, void *res)\r\n{\r\nstruct dma_pool *pool = *(struct dma_pool **)res;\r\ndma_pool_destroy(pool);\r\n}\r\nstatic int dmam_pool_match(struct device *dev, void *res, void *match_data)\r\n{\r\nreturn *(struct dma_pool **)res == match_data;\r\n}\r\nstruct dma_pool *dmam_pool_create(const char *name, struct device *dev,\r\nsize_t size, size_t align, size_t allocation)\r\n{\r\nstruct dma_pool **ptr, *pool;\r\nptr = devres_alloc(dmam_pool_release, sizeof(*ptr), GFP_KERNEL);\r\nif (!ptr)\r\nreturn NULL;\r\npool = *ptr = dma_pool_create(name, dev, size, align, allocation);\r\nif (pool)\r\ndevres_add(dev, ptr);\r\nelse\r\ndevres_free(ptr);\r\nreturn pool;\r\n}\r\nvoid dmam_pool_destroy(struct dma_pool *pool)\r\n{\r\nstruct device *dev = pool->dev;\r\nWARN_ON(devres_destroy(dev, dmam_pool_release, dmam_pool_match, pool));\r\ndma_pool_destroy(pool);\r\n}
