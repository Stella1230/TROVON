static void octeon_mgmt_set_rx_irq(struct octeon_mgmt *p, int enable)\r\n{\r\nint port = p->port;\r\nunion cvmx_mixx_intena mix_intena;\r\nunsigned long flags;\r\nspin_lock_irqsave(&p->lock, flags);\r\nmix_intena.u64 = cvmx_read_csr(CVMX_MIXX_INTENA(port));\r\nmix_intena.s.ithena = enable ? 1 : 0;\r\ncvmx_write_csr(CVMX_MIXX_INTENA(port), mix_intena.u64);\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\nstatic void octeon_mgmt_set_tx_irq(struct octeon_mgmt *p, int enable)\r\n{\r\nint port = p->port;\r\nunion cvmx_mixx_intena mix_intena;\r\nunsigned long flags;\r\nspin_lock_irqsave(&p->lock, flags);\r\nmix_intena.u64 = cvmx_read_csr(CVMX_MIXX_INTENA(port));\r\nmix_intena.s.othena = enable ? 1 : 0;\r\ncvmx_write_csr(CVMX_MIXX_INTENA(port), mix_intena.u64);\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\nstatic inline void octeon_mgmt_enable_rx_irq(struct octeon_mgmt *p)\r\n{\r\nocteon_mgmt_set_rx_irq(p, 1);\r\n}\r\nstatic inline void octeon_mgmt_disable_rx_irq(struct octeon_mgmt *p)\r\n{\r\nocteon_mgmt_set_rx_irq(p, 0);\r\n}\r\nstatic inline void octeon_mgmt_enable_tx_irq(struct octeon_mgmt *p)\r\n{\r\nocteon_mgmt_set_tx_irq(p, 1);\r\n}\r\nstatic inline void octeon_mgmt_disable_tx_irq(struct octeon_mgmt *p)\r\n{\r\nocteon_mgmt_set_tx_irq(p, 0);\r\n}\r\nstatic unsigned int ring_max_fill(unsigned int ring_size)\r\n{\r\nreturn ring_size - 8;\r\n}\r\nstatic unsigned int ring_size_to_bytes(unsigned int ring_size)\r\n{\r\nreturn ring_size * sizeof(union mgmt_port_ring_entry);\r\n}\r\nstatic void octeon_mgmt_rx_fill_ring(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint port = p->port;\r\nwhile (p->rx_current_fill < ring_max_fill(OCTEON_MGMT_RX_RING_SIZE)) {\r\nunsigned int size;\r\nunion mgmt_port_ring_entry re;\r\nstruct sk_buff *skb;\r\nsize = netdev->mtu + OCTEON_MGMT_RX_HEADROOM + 8 + NET_IP_ALIGN;\r\nskb = netdev_alloc_skb(netdev, size);\r\nif (!skb)\r\nbreak;\r\nskb_reserve(skb, NET_IP_ALIGN);\r\n__skb_queue_tail(&p->rx_list, skb);\r\nre.d64 = 0;\r\nre.s.len = size;\r\nre.s.addr = dma_map_single(p->dev, skb->data,\r\nsize,\r\nDMA_FROM_DEVICE);\r\np->rx_ring[p->rx_next_fill] = re.d64;\r\ndma_sync_single_for_device(p->dev, p->rx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\np->rx_next_fill =\r\n(p->rx_next_fill + 1) % OCTEON_MGMT_RX_RING_SIZE;\r\np->rx_current_fill++;\r\ncvmx_write_csr(CVMX_MIXX_IRING2(port), 1);\r\n}\r\n}\r\nstatic void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)\r\n{\r\nint port = p->port;\r\nunion cvmx_mixx_orcnt mix_orcnt;\r\nunion mgmt_port_ring_entry re;\r\nstruct sk_buff *skb;\r\nint cleaned = 0;\r\nunsigned long flags;\r\nmix_orcnt.u64 = cvmx_read_csr(CVMX_MIXX_ORCNT(port));\r\nwhile (mix_orcnt.s.orcnt) {\r\nspin_lock_irqsave(&p->tx_list.lock, flags);\r\nmix_orcnt.u64 = cvmx_read_csr(CVMX_MIXX_ORCNT(port));\r\nif (mix_orcnt.s.orcnt == 0) {\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\nbreak;\r\n}\r\ndma_sync_single_for_cpu(p->dev, p->tx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nre.d64 = p->tx_ring[p->tx_next_clean];\r\np->tx_next_clean =\r\n(p->tx_next_clean + 1) % OCTEON_MGMT_TX_RING_SIZE;\r\nskb = __skb_dequeue(&p->tx_list);\r\nmix_orcnt.u64 = 0;\r\nmix_orcnt.s.orcnt = 1;\r\ncvmx_write_csr(CVMX_MIXX_ORCNT(port), mix_orcnt.u64);\r\np->tx_current_fill--;\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\ndma_unmap_single(p->dev, re.s.addr, re.s.len,\r\nDMA_TO_DEVICE);\r\ndev_kfree_skb_any(skb);\r\ncleaned++;\r\nmix_orcnt.u64 = cvmx_read_csr(CVMX_MIXX_ORCNT(port));\r\n}\r\nif (cleaned && netif_queue_stopped(p->netdev))\r\nnetif_wake_queue(p->netdev);\r\n}\r\nstatic void octeon_mgmt_clean_tx_tasklet(unsigned long arg)\r\n{\r\nstruct octeon_mgmt *p = (struct octeon_mgmt *)arg;\r\nocteon_mgmt_clean_tx_buffers(p);\r\nocteon_mgmt_enable_tx_irq(p);\r\n}\r\nstatic void octeon_mgmt_update_rx_stats(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint port = p->port;\r\nunsigned long flags;\r\nu64 drop, bad;\r\ndrop = cvmx_read_csr(CVMX_AGL_GMX_RXX_STATS_PKTS_DRP(port));\r\nbad = cvmx_read_csr(CVMX_AGL_GMX_RXX_STATS_PKTS_BAD(port));\r\nif (drop || bad) {\r\nspin_lock_irqsave(&p->lock, flags);\r\nnetdev->stats.rx_errors += bad;\r\nnetdev->stats.rx_dropped += drop;\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\n}\r\nstatic void octeon_mgmt_update_tx_stats(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint port = p->port;\r\nunsigned long flags;\r\nunion cvmx_agl_gmx_txx_stat0 s0;\r\nunion cvmx_agl_gmx_txx_stat1 s1;\r\ns0.u64 = cvmx_read_csr(CVMX_AGL_GMX_TXX_STAT0(port));\r\ns1.u64 = cvmx_read_csr(CVMX_AGL_GMX_TXX_STAT1(port));\r\nif (s0.s.xsdef || s0.s.xscol || s1.s.scol || s1.s.mcol) {\r\nspin_lock_irqsave(&p->lock, flags);\r\nnetdev->stats.tx_errors += s0.s.xsdef + s0.s.xscol;\r\nnetdev->stats.collisions += s1.s.scol + s1.s.mcol;\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\n}\r\nstatic u64 octeon_mgmt_dequeue_rx_buffer(struct octeon_mgmt *p,\r\nstruct sk_buff **pskb)\r\n{\r\nunion mgmt_port_ring_entry re;\r\ndma_sync_single_for_cpu(p->dev, p->rx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nre.d64 = p->rx_ring[p->rx_next];\r\np->rx_next = (p->rx_next + 1) % OCTEON_MGMT_RX_RING_SIZE;\r\np->rx_current_fill--;\r\n*pskb = __skb_dequeue(&p->rx_list);\r\ndma_unmap_single(p->dev, re.s.addr,\r\nETH_FRAME_LEN + OCTEON_MGMT_RX_HEADROOM,\r\nDMA_FROM_DEVICE);\r\nreturn re.d64;\r\n}\r\nstatic int octeon_mgmt_receive_one(struct octeon_mgmt *p)\r\n{\r\nint port = p->port;\r\nstruct net_device *netdev = p->netdev;\r\nunion cvmx_mixx_ircnt mix_ircnt;\r\nunion mgmt_port_ring_entry re;\r\nstruct sk_buff *skb;\r\nstruct sk_buff *skb2;\r\nstruct sk_buff *skb_new;\r\nunion mgmt_port_ring_entry re2;\r\nint rc = 1;\r\nre.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb);\r\nif (likely(re.s.code == RING_ENTRY_CODE_DONE)) {\r\nskb_put(skb, re.s.len);\r\ngood:\r\nskb->protocol = eth_type_trans(skb, netdev);\r\nnetdev->stats.rx_packets++;\r\nnetdev->stats.rx_bytes += skb->len;\r\nnetif_receive_skb(skb);\r\nrc = 0;\r\n} else if (re.s.code == RING_ENTRY_CODE_MORE) {\r\nskb_put(skb, re.s.len);\r\ndo {\r\nre2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);\r\nif (re2.s.code != RING_ENTRY_CODE_MORE\r\n&& re2.s.code != RING_ENTRY_CODE_DONE)\r\ngoto split_error;\r\nskb_put(skb2, re2.s.len);\r\nskb_new = skb_copy_expand(skb, 0, skb2->len,\r\nGFP_ATOMIC);\r\nif (!skb_new)\r\ngoto split_error;\r\nif (skb_copy_bits(skb2, 0, skb_tail_pointer(skb_new),\r\nskb2->len))\r\ngoto split_error;\r\nskb_put(skb_new, skb2->len);\r\ndev_kfree_skb_any(skb);\r\ndev_kfree_skb_any(skb2);\r\nskb = skb_new;\r\n} while (re2.s.code == RING_ENTRY_CODE_MORE);\r\ngoto good;\r\n} else {\r\ndev_kfree_skb_any(skb);\r\n}\r\ngoto done;\r\nsplit_error:\r\ndev_kfree_skb_any(skb);\r\ndev_kfree_skb_any(skb2);\r\nwhile (re2.s.code == RING_ENTRY_CODE_MORE) {\r\nre2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);\r\ndev_kfree_skb_any(skb2);\r\n}\r\nnetdev->stats.rx_errors++;\r\ndone:\r\nmix_ircnt.u64 = 0;\r\nmix_ircnt.s.ircnt = 1;\r\ncvmx_write_csr(CVMX_MIXX_IRCNT(port), mix_ircnt.u64);\r\nreturn rc;\r\n}\r\nstatic int octeon_mgmt_receive_packets(struct octeon_mgmt *p, int budget)\r\n{\r\nint port = p->port;\r\nunsigned int work_done = 0;\r\nunion cvmx_mixx_ircnt mix_ircnt;\r\nint rc;\r\nmix_ircnt.u64 = cvmx_read_csr(CVMX_MIXX_IRCNT(port));\r\nwhile (work_done < budget && mix_ircnt.s.ircnt) {\r\nrc = octeon_mgmt_receive_one(p);\r\nif (!rc)\r\nwork_done++;\r\nmix_ircnt.u64 = cvmx_read_csr(CVMX_MIXX_IRCNT(port));\r\n}\r\nocteon_mgmt_rx_fill_ring(p->netdev);\r\nreturn work_done;\r\n}\r\nstatic int octeon_mgmt_napi_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct octeon_mgmt *p = container_of(napi, struct octeon_mgmt, napi);\r\nstruct net_device *netdev = p->netdev;\r\nunsigned int work_done = 0;\r\nwork_done = octeon_mgmt_receive_packets(p, budget);\r\nif (work_done < budget) {\r\nnapi_complete(napi);\r\nocteon_mgmt_enable_rx_irq(p);\r\n}\r\nocteon_mgmt_update_rx_stats(netdev);\r\nreturn work_done;\r\n}\r\nstatic void octeon_mgmt_reset_hw(struct octeon_mgmt *p)\r\n{\r\nunion cvmx_mixx_ctl mix_ctl;\r\nunion cvmx_mixx_bist mix_bist;\r\nunion cvmx_agl_gmx_bist agl_gmx_bist;\r\nmix_ctl.u64 = 0;\r\ncvmx_write_csr(CVMX_MIXX_CTL(p->port), mix_ctl.u64);\r\ndo {\r\nmix_ctl.u64 = cvmx_read_csr(CVMX_MIXX_CTL(p->port));\r\n} while (mix_ctl.s.busy);\r\nmix_ctl.s.reset = 1;\r\ncvmx_write_csr(CVMX_MIXX_CTL(p->port), mix_ctl.u64);\r\ncvmx_read_csr(CVMX_MIXX_CTL(p->port));\r\ncvmx_wait(64);\r\nmix_bist.u64 = cvmx_read_csr(CVMX_MIXX_BIST(p->port));\r\nif (mix_bist.u64)\r\ndev_warn(p->dev, "MIX failed BIST (0x%016llx)\n",\r\n(unsigned long long)mix_bist.u64);\r\nagl_gmx_bist.u64 = cvmx_read_csr(CVMX_AGL_GMX_BIST);\r\nif (agl_gmx_bist.u64)\r\ndev_warn(p->dev, "AGL failed BIST (0x%016llx)\n",\r\n(unsigned long long)agl_gmx_bist.u64);\r\n}\r\nstatic void octeon_mgmt_cam_state_add(struct octeon_mgmt_cam_state *cs,\r\nunsigned char *addr)\r\n{\r\nint i;\r\nfor (i = 0; i < 6; i++)\r\ncs->cam[i] |= (u64)addr[i] << (8 * (cs->cam_index));\r\ncs->cam_mask |= (1ULL << cs->cam_index);\r\ncs->cam_index++;\r\n}\r\nstatic void octeon_mgmt_set_rx_filtering(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint port = p->port;\r\nunion cvmx_agl_gmx_rxx_adr_ctl adr_ctl;\r\nunion cvmx_agl_gmx_prtx_cfg agl_gmx_prtx;\r\nunsigned long flags;\r\nunsigned int prev_packet_enable;\r\nunsigned int cam_mode = 1;\r\nunsigned int multicast_mode = 1;\r\nstruct octeon_mgmt_cam_state cam_state;\r\nstruct netdev_hw_addr *ha;\r\nint available_cam_entries;\r\nmemset(&cam_state, 0, sizeof(cam_state));\r\nif ((netdev->flags & IFF_PROMISC) || netdev->uc.count > 7) {\r\ncam_mode = 0;\r\navailable_cam_entries = 8;\r\n} else {\r\navailable_cam_entries = 7 - netdev->uc.count;\r\n}\r\nif (netdev->flags & IFF_MULTICAST) {\r\nif (cam_mode == 0 || (netdev->flags & IFF_ALLMULTI) ||\r\nnetdev_mc_count(netdev) > available_cam_entries)\r\nmulticast_mode = 2;\r\nelse\r\nmulticast_mode = 0;\r\n}\r\nif (cam_mode == 1) {\r\nocteon_mgmt_cam_state_add(&cam_state, netdev->dev_addr);\r\nnetdev_for_each_uc_addr(ha, netdev)\r\nocteon_mgmt_cam_state_add(&cam_state, ha->addr);\r\n}\r\nif (multicast_mode == 0) {\r\nnetdev_for_each_mc_addr(ha, netdev)\r\nocteon_mgmt_cam_state_add(&cam_state, ha->addr);\r\n}\r\nspin_lock_irqsave(&p->lock, flags);\r\nagl_gmx_prtx.u64 = cvmx_read_csr(CVMX_AGL_GMX_PRTX_CFG(port));\r\nprev_packet_enable = agl_gmx_prtx.s.en;\r\nagl_gmx_prtx.s.en = 0;\r\ncvmx_write_csr(CVMX_AGL_GMX_PRTX_CFG(port), agl_gmx_prtx.u64);\r\nadr_ctl.u64 = 0;\r\nadr_ctl.s.cam_mode = cam_mode;\r\nadr_ctl.s.mcst = multicast_mode;\r\nadr_ctl.s.bcst = 1;\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CTL(port), adr_ctl.u64);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM0(port), cam_state.cam[0]);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM1(port), cam_state.cam[1]);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM2(port), cam_state.cam[2]);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM3(port), cam_state.cam[3]);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM4(port), cam_state.cam[4]);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM5(port), cam_state.cam[5]);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_ADR_CAM_EN(port), cam_state.cam_mask);\r\nagl_gmx_prtx.s.en = prev_packet_enable;\r\ncvmx_write_csr(CVMX_AGL_GMX_PRTX_CFG(port), agl_gmx_prtx.u64);\r\nspin_unlock_irqrestore(&p->lock, flags);\r\n}\r\nstatic int octeon_mgmt_set_mac_address(struct net_device *netdev, void *addr)\r\n{\r\nstruct sockaddr *sa = addr;\r\nif (!is_valid_ether_addr(sa->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(netdev->dev_addr, sa->sa_data, ETH_ALEN);\r\nocteon_mgmt_set_rx_filtering(netdev);\r\nreturn 0;\r\n}\r\nstatic int octeon_mgmt_change_mtu(struct net_device *netdev, int new_mtu)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint port = p->port;\r\nint size_without_fcs = new_mtu + OCTEON_MGMT_RX_HEADROOM;\r\nif (size_without_fcs < 64 || size_without_fcs > 16383) {\r\ndev_warn(p->dev, "MTU must be between %d and %d.\n",\r\n64 - OCTEON_MGMT_RX_HEADROOM,\r\n16383 - OCTEON_MGMT_RX_HEADROOM);\r\nreturn -EINVAL;\r\n}\r\nnetdev->mtu = new_mtu;\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_FRM_MAX(port), size_without_fcs);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_JABBER(port),\r\n(size_without_fcs + 7) & 0xfff8);\r\nreturn 0;\r\n}\r\nstatic irqreturn_t octeon_mgmt_interrupt(int cpl, void *dev_id)\r\n{\r\nstruct net_device *netdev = dev_id;\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint port = p->port;\r\nunion cvmx_mixx_isr mixx_isr;\r\nmixx_isr.u64 = cvmx_read_csr(CVMX_MIXX_ISR(port));\r\ncvmx_write_csr(CVMX_MIXX_ISR(port), mixx_isr.u64);\r\ncvmx_read_csr(CVMX_MIXX_ISR(port));\r\nif (mixx_isr.s.irthresh) {\r\nocteon_mgmt_disable_rx_irq(p);\r\nnapi_schedule(&p->napi);\r\n}\r\nif (mixx_isr.s.orthresh) {\r\nocteon_mgmt_disable_tx_irq(p);\r\ntasklet_schedule(&p->tx_clean_tasklet);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int octeon_mgmt_ioctl(struct net_device *netdev,\r\nstruct ifreq *rq, int cmd)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nif (!netif_running(netdev))\r\nreturn -EINVAL;\r\nif (!p->phydev)\r\nreturn -EINVAL;\r\nreturn phy_mii_ioctl(p->phydev, rq, cmd);\r\n}\r\nstatic void octeon_mgmt_adjust_link(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint port = p->port;\r\nunion cvmx_agl_gmx_prtx_cfg prtx_cfg;\r\nunsigned long flags;\r\nint link_changed = 0;\r\nspin_lock_irqsave(&p->lock, flags);\r\nif (p->phydev->link) {\r\nif (!p->last_link)\r\nlink_changed = 1;\r\nif (p->last_duplex != p->phydev->duplex) {\r\np->last_duplex = p->phydev->duplex;\r\nprtx_cfg.u64 =\r\ncvmx_read_csr(CVMX_AGL_GMX_PRTX_CFG(port));\r\nprtx_cfg.s.duplex = p->phydev->duplex;\r\ncvmx_write_csr(CVMX_AGL_GMX_PRTX_CFG(port),\r\nprtx_cfg.u64);\r\n}\r\n} else {\r\nif (p->last_link)\r\nlink_changed = -1;\r\n}\r\np->last_link = p->phydev->link;\r\nspin_unlock_irqrestore(&p->lock, flags);\r\nif (link_changed != 0) {\r\nif (link_changed > 0) {\r\nnetif_carrier_on(netdev);\r\npr_info("%s: Link is up - %d/%s\n", netdev->name,\r\np->phydev->speed,\r\nDUPLEX_FULL == p->phydev->duplex ?\r\n"Full" : "Half");\r\n} else {\r\nnetif_carrier_off(netdev);\r\npr_info("%s: Link is down\n", netdev->name);\r\n}\r\n}\r\n}\r\nstatic int octeon_mgmt_init_phy(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nchar phy_id[MII_BUS_ID_SIZE + 3];\r\nif (octeon_is_simulation()) {\r\nnetif_carrier_on(netdev);\r\nreturn 0;\r\n}\r\nsnprintf(phy_id, sizeof(phy_id), PHY_ID_FMT, "mdio-octeon-0", p->port);\r\np->phydev = phy_connect(netdev, phy_id, octeon_mgmt_adjust_link, 0,\r\nPHY_INTERFACE_MODE_MII);\r\nif (IS_ERR(p->phydev)) {\r\np->phydev = NULL;\r\nreturn -1;\r\n}\r\nphy_start_aneg(p->phydev);\r\nreturn 0;\r\n}\r\nstatic int octeon_mgmt_open(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint port = p->port;\r\nunion cvmx_mixx_ctl mix_ctl;\r\nunion cvmx_agl_gmx_inf_mode agl_gmx_inf_mode;\r\nunion cvmx_mixx_oring1 oring1;\r\nunion cvmx_mixx_iring1 iring1;\r\nunion cvmx_agl_gmx_prtx_cfg prtx_cfg;\r\nunion cvmx_agl_gmx_rxx_frm_ctl rxx_frm_ctl;\r\nunion cvmx_mixx_irhwm mix_irhwm;\r\nunion cvmx_mixx_orhwm mix_orhwm;\r\nunion cvmx_mixx_intena mix_intena;\r\nstruct sockaddr sa;\r\np->tx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nGFP_KERNEL);\r\nif (!p->tx_ring)\r\nreturn -ENOMEM;\r\np->tx_ring_handle =\r\ndma_map_single(p->dev, p->tx_ring,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\np->tx_next = 0;\r\np->tx_next_clean = 0;\r\np->tx_current_fill = 0;\r\np->rx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nGFP_KERNEL);\r\nif (!p->rx_ring)\r\ngoto err_nomem;\r\np->rx_ring_handle =\r\ndma_map_single(p->dev, p->rx_ring,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\np->rx_next = 0;\r\np->rx_next_fill = 0;\r\np->rx_current_fill = 0;\r\nocteon_mgmt_reset_hw(p);\r\nmix_ctl.u64 = cvmx_read_csr(CVMX_MIXX_CTL(port));\r\nif (mix_ctl.s.reset) {\r\nmix_ctl.s.reset = 0;\r\ncvmx_write_csr(CVMX_MIXX_CTL(port), mix_ctl.u64);\r\ndo {\r\nmix_ctl.u64 = cvmx_read_csr(CVMX_MIXX_CTL(port));\r\n} while (mix_ctl.s.reset);\r\n}\r\nagl_gmx_inf_mode.u64 = 0;\r\nagl_gmx_inf_mode.s.en = 1;\r\ncvmx_write_csr(CVMX_AGL_GMX_INF_MODE, agl_gmx_inf_mode.u64);\r\noring1.u64 = 0;\r\noring1.s.obase = p->tx_ring_handle >> 3;\r\noring1.s.osize = OCTEON_MGMT_TX_RING_SIZE;\r\ncvmx_write_csr(CVMX_MIXX_ORING1(port), oring1.u64);\r\niring1.u64 = 0;\r\niring1.s.ibase = p->rx_ring_handle >> 3;\r\niring1.s.isize = OCTEON_MGMT_RX_RING_SIZE;\r\ncvmx_write_csr(CVMX_MIXX_IRING1(port), iring1.u64);\r\nprtx_cfg.u64 = cvmx_read_csr(CVMX_AGL_GMX_PRTX_CFG(port));\r\nprtx_cfg.s.en = 0;\r\ncvmx_write_csr(CVMX_AGL_GMX_PRTX_CFG(port), prtx_cfg.u64);\r\nmemcpy(sa.sa_data, netdev->dev_addr, ETH_ALEN);\r\nocteon_mgmt_set_mac_address(netdev, &sa);\r\nocteon_mgmt_change_mtu(netdev, netdev->mtu);\r\nmix_ctl.u64 = 0;\r\nmix_ctl.s.crc_strip = 1;\r\nmix_ctl.s.en = 1;\r\nmix_ctl.s.nbtarb = 0;\r\nmix_ctl.s.mrq_hwm = 1;\r\ncvmx_write_csr(CVMX_MIXX_CTL(port), mix_ctl.u64);\r\nif (OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X)\r\n|| OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X)) {\r\nunion cvmx_agl_gmx_drv_ctl drv_ctl;\r\ndrv_ctl.u64 = cvmx_read_csr(CVMX_AGL_GMX_DRV_CTL);\r\nif (port) {\r\ndrv_ctl.s.byp_en1 = 1;\r\ndrv_ctl.s.nctl1 = 6;\r\ndrv_ctl.s.pctl1 = 6;\r\n} else {\r\ndrv_ctl.s.byp_en = 1;\r\ndrv_ctl.s.nctl = 6;\r\ndrv_ctl.s.pctl = 6;\r\n}\r\ncvmx_write_csr(CVMX_AGL_GMX_DRV_CTL, drv_ctl.u64);\r\n}\r\nocteon_mgmt_rx_fill_ring(netdev);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_STATS_CTL(port), 1);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_STATS_PKTS_DRP(port), 0);\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_STATS_PKTS_BAD(port), 0);\r\ncvmx_write_csr(CVMX_AGL_GMX_TXX_STATS_CTL(port), 1);\r\ncvmx_write_csr(CVMX_AGL_GMX_TXX_STAT0(port), 0);\r\ncvmx_write_csr(CVMX_AGL_GMX_TXX_STAT1(port), 0);\r\ncvmx_write_csr(CVMX_MIXX_ISR(port), cvmx_read_csr(CVMX_MIXX_ISR(port)));\r\nif (request_irq(p->irq, octeon_mgmt_interrupt, 0, netdev->name,\r\nnetdev)) {\r\ndev_err(p->dev, "request_irq(%d) failed.\n", p->irq);\r\ngoto err_noirq;\r\n}\r\nmix_irhwm.u64 = 0;\r\nmix_irhwm.s.irhwm = 0;\r\ncvmx_write_csr(CVMX_MIXX_IRHWM(port), mix_irhwm.u64);\r\nmix_orhwm.u64 = 0;\r\nmix_orhwm.s.orhwm = 1;\r\ncvmx_write_csr(CVMX_MIXX_ORHWM(port), mix_orhwm.u64);\r\nmix_intena.u64 = 0;\r\nmix_intena.s.ithena = 1;\r\nmix_intena.s.othena = 1;\r\ncvmx_write_csr(CVMX_MIXX_INTENA(port), mix_intena.u64);\r\nrxx_frm_ctl.u64 = 0;\r\nrxx_frm_ctl.s.pre_align = 1;\r\nrxx_frm_ctl.s.pad_len = 1;\r\nrxx_frm_ctl.s.vlan_len = 1;\r\nrxx_frm_ctl.s.pre_free = 1;\r\nrxx_frm_ctl.s.ctl_smac = 0;\r\nrxx_frm_ctl.s.ctl_mcst = 1;\r\nrxx_frm_ctl.s.ctl_bck = 1;\r\nrxx_frm_ctl.s.ctl_drp = 1;\r\nrxx_frm_ctl.s.pre_strp = 1;\r\nrxx_frm_ctl.s.pre_chk = 1;\r\ncvmx_write_csr(CVMX_AGL_GMX_RXX_FRM_CTL(port), rxx_frm_ctl.u64);\r\nagl_gmx_inf_mode.u64 = 0;\r\nagl_gmx_inf_mode.s.en = 1;\r\ncvmx_write_csr(CVMX_AGL_GMX_INF_MODE, agl_gmx_inf_mode.u64);\r\nprtx_cfg.u64 = cvmx_read_csr(CVMX_AGL_GMX_PRTX_CFG(port));\r\nprtx_cfg.s.tx_en = 1;\r\nprtx_cfg.s.rx_en = 1;\r\nprtx_cfg.s.en = 1;\r\np->last_duplex = 1;\r\nprtx_cfg.s.duplex = p->last_duplex;\r\ncvmx_write_csr(CVMX_AGL_GMX_PRTX_CFG(port), prtx_cfg.u64);\r\np->last_link = 0;\r\nnetif_carrier_off(netdev);\r\nif (octeon_mgmt_init_phy(netdev)) {\r\ndev_err(p->dev, "Cannot initialize PHY.\n");\r\ngoto err_noirq;\r\n}\r\nnetif_wake_queue(netdev);\r\nnapi_enable(&p->napi);\r\nreturn 0;\r\nerr_noirq:\r\nocteon_mgmt_reset_hw(p);\r\ndma_unmap_single(p->dev, p->rx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nkfree(p->rx_ring);\r\nerr_nomem:\r\ndma_unmap_single(p->dev, p->tx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nkfree(p->tx_ring);\r\nreturn -ENOMEM;\r\n}\r\nstatic int octeon_mgmt_stop(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nnapi_disable(&p->napi);\r\nnetif_stop_queue(netdev);\r\nif (p->phydev)\r\nphy_disconnect(p->phydev);\r\nnetif_carrier_off(netdev);\r\nocteon_mgmt_reset_hw(p);\r\nfree_irq(p->irq, netdev);\r\nskb_queue_purge(&p->tx_list);\r\nskb_queue_purge(&p->rx_list);\r\ndma_unmap_single(p->dev, p->rx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nkfree(p->rx_ring);\r\ndma_unmap_single(p->dev, p->tx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nkfree(p->tx_ring);\r\nreturn 0;\r\n}\r\nstatic int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nint port = p->port;\r\nunion mgmt_port_ring_entry re;\r\nunsigned long flags;\r\nint rv = NETDEV_TX_BUSY;\r\nre.d64 = 0;\r\nre.s.len = skb->len;\r\nre.s.addr = dma_map_single(p->dev, skb->data,\r\nskb->len,\r\nDMA_TO_DEVICE);\r\nspin_lock_irqsave(&p->tx_list.lock, flags);\r\nif (unlikely(p->tx_current_fill >= ring_max_fill(OCTEON_MGMT_TX_RING_SIZE) - 1)) {\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\nnetif_stop_queue(netdev);\r\nspin_lock_irqsave(&p->tx_list.lock, flags);\r\n}\r\nif (unlikely(p->tx_current_fill >=\r\nring_max_fill(OCTEON_MGMT_TX_RING_SIZE))) {\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\ndma_unmap_single(p->dev, re.s.addr, re.s.len,\r\nDMA_TO_DEVICE);\r\ngoto out;\r\n}\r\n__skb_queue_tail(&p->tx_list, skb);\r\np->tx_ring[p->tx_next] = re.d64;\r\np->tx_next = (p->tx_next + 1) % OCTEON_MGMT_TX_RING_SIZE;\r\np->tx_current_fill++;\r\nspin_unlock_irqrestore(&p->tx_list.lock, flags);\r\ndma_sync_single_for_device(p->dev, p->tx_ring_handle,\r\nring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),\r\nDMA_BIDIRECTIONAL);\r\nnetdev->stats.tx_packets++;\r\nnetdev->stats.tx_bytes += skb->len;\r\ncvmx_write_csr(CVMX_MIXX_ORING2(port), 1);\r\nrv = NETDEV_TX_OK;\r\nout:\r\nocteon_mgmt_update_tx_stats(netdev);\r\nreturn rv;\r\n}\r\nstatic void octeon_mgmt_poll_controller(struct net_device *netdev)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nocteon_mgmt_receive_packets(p, 16);\r\nocteon_mgmt_update_rx_stats(netdev);\r\n}\r\nstatic void octeon_mgmt_get_drvinfo(struct net_device *netdev,\r\nstruct ethtool_drvinfo *info)\r\n{\r\nstrncpy(info->driver, DRV_NAME, sizeof(info->driver));\r\nstrncpy(info->version, DRV_VERSION, sizeof(info->version));\r\nstrncpy(info->fw_version, "N/A", sizeof(info->fw_version));\r\nstrncpy(info->bus_info, "N/A", sizeof(info->bus_info));\r\ninfo->n_stats = 0;\r\ninfo->testinfo_len = 0;\r\ninfo->regdump_len = 0;\r\ninfo->eedump_len = 0;\r\n}\r\nstatic int octeon_mgmt_get_settings(struct net_device *netdev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nif (p->phydev)\r\nreturn phy_ethtool_gset(p->phydev, cmd);\r\nreturn -EINVAL;\r\n}\r\nstatic int octeon_mgmt_set_settings(struct net_device *netdev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct octeon_mgmt *p = netdev_priv(netdev);\r\nif (!capable(CAP_NET_ADMIN))\r\nreturn -EPERM;\r\nif (p->phydev)\r\nreturn phy_ethtool_sset(p->phydev, cmd);\r\nreturn -EINVAL;\r\n}\r\nstatic int __devinit octeon_mgmt_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *res_irq;\r\nstruct net_device *netdev;\r\nstruct octeon_mgmt *p;\r\nint i;\r\nnetdev = alloc_etherdev(sizeof(struct octeon_mgmt));\r\nif (netdev == NULL)\r\nreturn -ENOMEM;\r\ndev_set_drvdata(&pdev->dev, netdev);\r\np = netdev_priv(netdev);\r\nnetif_napi_add(netdev, &p->napi, octeon_mgmt_napi_poll,\r\nOCTEON_MGMT_NAPI_WEIGHT);\r\np->netdev = netdev;\r\np->dev = &pdev->dev;\r\np->port = pdev->id;\r\nsnprintf(netdev->name, IFNAMSIZ, "mgmt%d", p->port);\r\nres_irq = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (!res_irq)\r\ngoto err;\r\np->irq = res_irq->start;\r\nspin_lock_init(&p->lock);\r\nskb_queue_head_init(&p->tx_list);\r\nskb_queue_head_init(&p->rx_list);\r\ntasklet_init(&p->tx_clean_tasklet,\r\nocteon_mgmt_clean_tx_tasklet, (unsigned long)p);\r\nnetdev->priv_flags |= IFF_UNICAST_FLT;\r\nnetdev->netdev_ops = &octeon_mgmt_ops;\r\nnetdev->ethtool_ops = &octeon_mgmt_ethtool_ops;\r\nfor (i = 0; i < 6; i++)\r\nnetdev->dev_addr[i] = octeon_bootinfo->mac_addr_base[i];\r\nnetdev->dev_addr[5] += p->port;\r\nif (p->port >= octeon_bootinfo->mac_addr_count)\r\ndev_err(&pdev->dev,\r\n"Error %s: Using MAC outside of the assigned range: %pM\n",\r\nnetdev->name, netdev->dev_addr);\r\nif (register_netdev(netdev))\r\ngoto err;\r\ndev_info(&pdev->dev, "Version " DRV_VERSION "\n");\r\nreturn 0;\r\nerr:\r\nfree_netdev(netdev);\r\nreturn -ENOENT;\r\n}\r\nstatic int __devexit octeon_mgmt_remove(struct platform_device *pdev)\r\n{\r\nstruct net_device *netdev = dev_get_drvdata(&pdev->dev);\r\nunregister_netdev(netdev);\r\nfree_netdev(netdev);\r\nreturn 0;\r\n}\r\nstatic int __init octeon_mgmt_mod_init(void)\r\n{\r\nocteon_mdiobus_force_mod_depencency();\r\nreturn platform_driver_register(&octeon_mgmt_driver);\r\n}\r\nstatic void __exit octeon_mgmt_mod_exit(void)\r\n{\r\nplatform_driver_unregister(&octeon_mgmt_driver);\r\n}
