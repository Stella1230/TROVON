static inline\r\nstruct sirfsoc_dma_chan *dma_chan_to_sirfsoc_dma_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct sirfsoc_dma_chan, chan);\r\n}\r\nstatic inline struct sirfsoc_dma *dma_chan_to_sirfsoc_dma(struct dma_chan *c)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(c);\r\nreturn container_of(schan, struct sirfsoc_dma, channels[c->chan_id]);\r\n}\r\nstatic void sirfsoc_dma_execute(struct sirfsoc_dma_chan *schan)\r\n{\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(&schan->chan);\r\nint cid = schan->chan.chan_id;\r\nstruct sirfsoc_dma_desc *sdesc = NULL;\r\nsdesc = list_first_entry(&schan->queued, struct sirfsoc_dma_desc,\r\nnode);\r\nlist_move_tail(&schan->queued, &schan->active);\r\nwritel_relaxed(sdesc->width, sdma->base + SIRFSOC_DMA_WIDTH_0 +\r\ncid * 4);\r\nwritel_relaxed(cid | (schan->mode << SIRFSOC_DMA_MODE_CTRL_BIT) |\r\n(sdesc->dir << SIRFSOC_DMA_DIR_CTRL_BIT),\r\nsdma->base + cid * 0x10 + SIRFSOC_DMA_CH_CTRL);\r\nwritel_relaxed(sdesc->xlen, sdma->base + cid * 0x10 +\r\nSIRFSOC_DMA_CH_XLEN);\r\nwritel_relaxed(sdesc->ylen, sdma->base + cid * 0x10 +\r\nSIRFSOC_DMA_CH_YLEN);\r\nwritel_relaxed(readl_relaxed(sdma->base + SIRFSOC_DMA_INT_EN) |\r\n(1 << cid), sdma->base + SIRFSOC_DMA_INT_EN);\r\nwritel(sdesc->addr >> 2, sdma->base + cid * 0x10 + SIRFSOC_DMA_CH_ADDR);\r\nif (sdesc->cyclic) {\r\nwritel((1 << cid) | 1 << (cid + 16) |\r\nreadl_relaxed(sdma->base + SIRFSOC_DMA_CH_LOOP_CTRL),\r\nsdma->base + SIRFSOC_DMA_CH_LOOP_CTRL);\r\nschan->happened_cyclic = schan->completed_cyclic = 0;\r\n}\r\n}\r\nstatic irqreturn_t sirfsoc_dma_irq(int irq, void *data)\r\n{\r\nstruct sirfsoc_dma *sdma = data;\r\nstruct sirfsoc_dma_chan *schan;\r\nstruct sirfsoc_dma_desc *sdesc = NULL;\r\nu32 is;\r\nint ch;\r\nis = readl(sdma->base + SIRFSOC_DMA_CH_INT);\r\nwhile ((ch = fls(is) - 1) >= 0) {\r\nis &= ~(1 << ch);\r\nwritel_relaxed(1 << ch, sdma->base + SIRFSOC_DMA_CH_INT);\r\nschan = &sdma->channels[ch];\r\nspin_lock(&schan->lock);\r\nsdesc = list_first_entry(&schan->active, struct sirfsoc_dma_desc,\r\nnode);\r\nif (!sdesc->cyclic) {\r\nlist_splice_tail_init(&schan->active, &schan->completed);\r\nif (!list_empty(&schan->queued))\r\nsirfsoc_dma_execute(schan);\r\n} else\r\nschan->happened_cyclic++;\r\nspin_unlock(&schan->lock);\r\n}\r\ntasklet_schedule(&sdma->tasklet);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void sirfsoc_dma_process_completed(struct sirfsoc_dma *sdma)\r\n{\r\ndma_cookie_t last_cookie = 0;\r\nstruct sirfsoc_dma_chan *schan;\r\nstruct sirfsoc_dma_desc *sdesc;\r\nstruct dma_async_tx_descriptor *desc;\r\nunsigned long flags;\r\nunsigned long happened_cyclic;\r\nLIST_HEAD(list);\r\nint i;\r\nfor (i = 0; i < sdma->dma.chancnt; i++) {\r\nschan = &sdma->channels[i];\r\nspin_lock_irqsave(&schan->lock, flags);\r\nif (!list_empty(&schan->completed)) {\r\nlist_splice_tail_init(&schan->completed, &list);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nlist_for_each_entry(sdesc, &list, node) {\r\ndesc = &sdesc->desc;\r\nif (desc->callback)\r\ndesc->callback(desc->callback_param);\r\nlast_cookie = desc->cookie;\r\ndma_run_dependencies(desc);\r\n}\r\nspin_lock_irqsave(&schan->lock, flags);\r\nlist_splice_tail_init(&list, &schan->free);\r\nschan->chan.completed_cookie = last_cookie;\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\n} else {\r\nsdesc = list_first_entry(&schan->active, struct sirfsoc_dma_desc,\r\nnode);\r\nif (!sdesc || (sdesc && !sdesc->cyclic)) {\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\ncontinue;\r\n}\r\nhappened_cyclic = schan->happened_cyclic;\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\ndesc = &sdesc->desc;\r\nwhile (happened_cyclic != schan->completed_cyclic) {\r\nif (desc->callback)\r\ndesc->callback(desc->callback_param);\r\nschan->completed_cyclic++;\r\n}\r\n}\r\n}\r\n}\r\nstatic void sirfsoc_dma_tasklet(unsigned long data)\r\n{\r\nstruct sirfsoc_dma *sdma = (void *)data;\r\nsirfsoc_dma_process_completed(sdma);\r\n}\r\nstatic dma_cookie_t sirfsoc_dma_tx_submit(struct dma_async_tx_descriptor *txd)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(txd->chan);\r\nstruct sirfsoc_dma_desc *sdesc;\r\nunsigned long flags;\r\ndma_cookie_t cookie;\r\nsdesc = container_of(txd, struct sirfsoc_dma_desc, desc);\r\nspin_lock_irqsave(&schan->lock, flags);\r\nlist_move_tail(&sdesc->node, &schan->queued);\r\ncookie = dma_cookie_assign(txd);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn cookie;\r\n}\r\nstatic int sirfsoc_dma_slave_config(struct sirfsoc_dma_chan *schan,\r\nstruct dma_slave_config *config)\r\n{\r\nunsigned long flags;\r\nif ((config->src_addr_width != DMA_SLAVE_BUSWIDTH_4_BYTES) ||\r\n(config->dst_addr_width != DMA_SLAVE_BUSWIDTH_4_BYTES))\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nschan->mode = (config->src_maxburst == 4 ? 1 : 0);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_terminate_all(struct sirfsoc_dma_chan *schan)\r\n{\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(&schan->chan);\r\nint cid = schan->chan.chan_id;\r\nunsigned long flags;\r\nwritel_relaxed(readl_relaxed(sdma->base + SIRFSOC_DMA_INT_EN) &\r\n~(1 << cid), sdma->base + SIRFSOC_DMA_INT_EN);\r\nwritel_relaxed(1 << cid, sdma->base + SIRFSOC_DMA_CH_VALID);\r\nwritel_relaxed(readl_relaxed(sdma->base + SIRFSOC_DMA_CH_LOOP_CTRL)\r\n& ~((1 << cid) | 1 << (cid + 16)),\r\nsdma->base + SIRFSOC_DMA_CH_LOOP_CTRL);\r\nspin_lock_irqsave(&schan->lock, flags);\r\nlist_splice_tail_init(&schan->active, &schan->free);\r\nlist_splice_tail_init(&schan->queued, &schan->free);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int sirfsoc_dma_control(struct dma_chan *chan, enum dma_ctrl_cmd cmd,\r\nunsigned long arg)\r\n{\r\nstruct dma_slave_config *config;\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nswitch (cmd) {\r\ncase DMA_TERMINATE_ALL:\r\nreturn sirfsoc_dma_terminate_all(schan);\r\ncase DMA_SLAVE_CONFIG:\r\nconfig = (struct dma_slave_config *)arg;\r\nreturn sirfsoc_dma_slave_config(schan, config);\r\ndefault:\r\nbreak;\r\n}\r\nreturn -ENOSYS;\r\n}\r\nstatic int sirfsoc_dma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(chan);\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma_desc *sdesc;\r\nunsigned long flags;\r\nLIST_HEAD(descs);\r\nint i;\r\nfor (i = 0; i < SIRFSOC_DMA_DESCRIPTORS; i++) {\r\nsdesc = kzalloc(sizeof(*sdesc), GFP_KERNEL);\r\nif (!sdesc) {\r\ndev_notice(sdma->dma.dev, "Memory allocation error. "\r\n"Allocated only %u descriptors\n", i);\r\nbreak;\r\n}\r\ndma_async_tx_descriptor_init(&sdesc->desc, chan);\r\nsdesc->desc.flags = DMA_CTRL_ACK;\r\nsdesc->desc.tx_submit = sirfsoc_dma_tx_submit;\r\nlist_add_tail(&sdesc->node, &descs);\r\n}\r\nif (i == 0)\r\nreturn -ENOMEM;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nlist_splice_tail_init(&descs, &schan->free);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn i;\r\n}\r\nstatic void sirfsoc_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma_desc *sdesc, *tmp;\r\nunsigned long flags;\r\nLIST_HEAD(descs);\r\nspin_lock_irqsave(&schan->lock, flags);\r\nBUG_ON(!list_empty(&schan->prepared));\r\nBUG_ON(!list_empty(&schan->queued));\r\nBUG_ON(!list_empty(&schan->active));\r\nBUG_ON(!list_empty(&schan->completed));\r\nlist_splice_tail_init(&schan->free, &descs);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nlist_for_each_entry_safe(sdesc, tmp, &descs, node)\r\nkfree(sdesc);\r\n}\r\nstatic void sirfsoc_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nif (list_empty(&schan->active) && !list_empty(&schan->queued))\r\nsirfsoc_dma_execute(schan);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\n}\r\nstatic enum dma_status\r\nsirfsoc_dma_tx_status(struct dma_chan *chan, dma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nunsigned long flags;\r\nenum dma_status ret;\r\nspin_lock_irqsave(&schan->lock, flags);\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nspin_unlock_irqrestore(&schan->lock, flags);\r\nreturn ret;\r\n}\r\nstatic struct dma_async_tx_descriptor *sirfsoc_dma_prep_interleaved(\r\nstruct dma_chan *chan, struct dma_interleaved_template *xt,\r\nunsigned long flags)\r\n{\r\nstruct sirfsoc_dma *sdma = dma_chan_to_sirfsoc_dma(chan);\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma_desc *sdesc = NULL;\r\nunsigned long iflags;\r\nint ret;\r\nif ((xt->dir != DMA_MEM_TO_DEV) || (xt->dir != DMA_DEV_TO_MEM)) {\r\nret = -EINVAL;\r\ngoto err_dir;\r\n}\r\nspin_lock_irqsave(&schan->lock, iflags);\r\nif (!list_empty(&schan->free)) {\r\nsdesc = list_first_entry(&schan->free, struct sirfsoc_dma_desc,\r\nnode);\r\nlist_del(&sdesc->node);\r\n}\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nif (!sdesc) {\r\nsirfsoc_dma_process_completed(sdma);\r\nret = 0;\r\ngoto no_desc;\r\n}\r\nspin_lock_irqsave(&schan->lock, iflags);\r\nif ((xt->frame_size == 1) && (xt->numf > 0)) {\r\nsdesc->cyclic = 0;\r\nsdesc->xlen = xt->sgl[0].size / SIRFSOC_DMA_WORD_LEN;\r\nsdesc->width = (xt->sgl[0].size + xt->sgl[0].icg) /\r\nSIRFSOC_DMA_WORD_LEN;\r\nsdesc->ylen = xt->numf - 1;\r\nif (xt->dir == DMA_MEM_TO_DEV) {\r\nsdesc->addr = xt->src_start;\r\nsdesc->dir = 1;\r\n} else {\r\nsdesc->addr = xt->dst_start;\r\nsdesc->dir = 0;\r\n}\r\nlist_add_tail(&sdesc->node, &schan->prepared);\r\n} else {\r\npr_err("sirfsoc DMA Invalid xfer\n");\r\nret = -EINVAL;\r\ngoto err_xfer;\r\n}\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nreturn &sdesc->desc;\r\nerr_xfer:\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nno_desc:\r\nerr_dir:\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nsirfsoc_dma_prep_cyclic(struct dma_chan *chan, dma_addr_t addr,\r\nsize_t buf_len, size_t period_len,\r\nenum dma_transfer_direction direction, void *context)\r\n{\r\nstruct sirfsoc_dma_chan *schan = dma_chan_to_sirfsoc_dma_chan(chan);\r\nstruct sirfsoc_dma_desc *sdesc = NULL;\r\nunsigned long iflags;\r\nif (buf_len != 2 * period_len)\r\nreturn ERR_PTR(-EINVAL);\r\nspin_lock_irqsave(&schan->lock, iflags);\r\nif (!list_empty(&schan->free)) {\r\nsdesc = list_first_entry(&schan->free, struct sirfsoc_dma_desc,\r\nnode);\r\nlist_del(&sdesc->node);\r\n}\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nif (!sdesc)\r\nreturn 0;\r\nspin_lock_irqsave(&schan->lock, iflags);\r\nsdesc->addr = addr;\r\nsdesc->cyclic = 1;\r\nsdesc->xlen = 0;\r\nsdesc->ylen = buf_len / SIRFSOC_DMA_WORD_LEN - 1;\r\nsdesc->width = 1;\r\nlist_add_tail(&sdesc->node, &schan->prepared);\r\nspin_unlock_irqrestore(&schan->lock, iflags);\r\nreturn &sdesc->desc;\r\n}\r\nbool sirfsoc_dma_filter_id(struct dma_chan *chan, void *chan_id)\r\n{\r\nunsigned int ch_nr = (unsigned int) chan_id;\r\nif (ch_nr == chan->chan_id +\r\nchan->device->dev_id * SIRFSOC_DMA_CHANNELS)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic int __devinit sirfsoc_dma_probe(struct platform_device *op)\r\n{\r\nstruct device_node *dn = op->dev.of_node;\r\nstruct device *dev = &op->dev;\r\nstruct dma_device *dma;\r\nstruct sirfsoc_dma *sdma;\r\nstruct sirfsoc_dma_chan *schan;\r\nstruct resource res;\r\nulong regs_start, regs_size;\r\nu32 id;\r\nint ret, i;\r\nsdma = devm_kzalloc(dev, sizeof(*sdma), GFP_KERNEL);\r\nif (!sdma) {\r\ndev_err(dev, "Memory exhausted!\n");\r\nreturn -ENOMEM;\r\n}\r\nif (of_property_read_u32(dn, "cell-index", &id)) {\r\ndev_err(dev, "Fail to get DMAC index\n");\r\nret = -ENODEV;\r\ngoto free_mem;\r\n}\r\nsdma->irq = irq_of_parse_and_map(dn, 0);\r\nif (sdma->irq == NO_IRQ) {\r\ndev_err(dev, "Error mapping IRQ!\n");\r\nret = -EINVAL;\r\ngoto free_mem;\r\n}\r\nret = of_address_to_resource(dn, 0, &res);\r\nif (ret) {\r\ndev_err(dev, "Error parsing memory region!\n");\r\ngoto free_mem;\r\n}\r\nregs_start = res.start;\r\nregs_size = resource_size(&res);\r\nsdma->base = devm_ioremap(dev, regs_start, regs_size);\r\nif (!sdma->base) {\r\ndev_err(dev, "Error mapping memory region!\n");\r\nret = -ENOMEM;\r\ngoto irq_dispose;\r\n}\r\nret = devm_request_irq(dev, sdma->irq, &sirfsoc_dma_irq, 0, DRV_NAME,\r\nsdma);\r\nif (ret) {\r\ndev_err(dev, "Error requesting IRQ!\n");\r\nret = -EINVAL;\r\ngoto unmap_mem;\r\n}\r\ndma = &sdma->dma;\r\ndma->dev = dev;\r\ndma->chancnt = SIRFSOC_DMA_CHANNELS;\r\ndma->device_alloc_chan_resources = sirfsoc_dma_alloc_chan_resources;\r\ndma->device_free_chan_resources = sirfsoc_dma_free_chan_resources;\r\ndma->device_issue_pending = sirfsoc_dma_issue_pending;\r\ndma->device_control = sirfsoc_dma_control;\r\ndma->device_tx_status = sirfsoc_dma_tx_status;\r\ndma->device_prep_interleaved_dma = sirfsoc_dma_prep_interleaved;\r\ndma->device_prep_dma_cyclic = sirfsoc_dma_prep_cyclic;\r\nINIT_LIST_HEAD(&dma->channels);\r\ndma_cap_set(DMA_SLAVE, dma->cap_mask);\r\ndma_cap_set(DMA_CYCLIC, dma->cap_mask);\r\ndma_cap_set(DMA_INTERLEAVE, dma->cap_mask);\r\ndma_cap_set(DMA_PRIVATE, dma->cap_mask);\r\nfor (i = 0; i < dma->chancnt; i++) {\r\nschan = &sdma->channels[i];\r\nschan->chan.device = dma;\r\ndma_cookie_init(&schan->chan);\r\nINIT_LIST_HEAD(&schan->free);\r\nINIT_LIST_HEAD(&schan->prepared);\r\nINIT_LIST_HEAD(&schan->queued);\r\nINIT_LIST_HEAD(&schan->active);\r\nINIT_LIST_HEAD(&schan->completed);\r\nspin_lock_init(&schan->lock);\r\nlist_add_tail(&schan->chan.device_node, &dma->channels);\r\n}\r\ntasklet_init(&sdma->tasklet, sirfsoc_dma_tasklet, (unsigned long)sdma);\r\ndev_set_drvdata(dev, sdma);\r\nret = dma_async_device_register(dma);\r\nif (ret)\r\ngoto free_irq;\r\ndev_info(dev, "initialized SIRFSOC DMAC driver\n");\r\nreturn 0;\r\nfree_irq:\r\ndevm_free_irq(dev, sdma->irq, sdma);\r\nirq_dispose:\r\nirq_dispose_mapping(sdma->irq);\r\nunmap_mem:\r\niounmap(sdma->base);\r\nfree_mem:\r\ndevm_kfree(dev, sdma);\r\nreturn ret;\r\n}\r\nstatic int __devexit sirfsoc_dma_remove(struct platform_device *op)\r\n{\r\nstruct device *dev = &op->dev;\r\nstruct sirfsoc_dma *sdma = dev_get_drvdata(dev);\r\ndma_async_device_unregister(&sdma->dma);\r\ndevm_free_irq(dev, sdma->irq, sdma);\r\nirq_dispose_mapping(sdma->irq);\r\niounmap(sdma->base);\r\ndevm_kfree(dev, sdma);\r\nreturn 0;\r\n}
