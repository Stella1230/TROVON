static void __cpuinit early_init_intel(struct cpuinfo_x86 *c)\r\n{\r\nu64 misc_enable;\r\nif (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {\r\nrdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);\r\nif (misc_enable & MSR_IA32_MISC_ENABLE_LIMIT_CPUID) {\r\nmisc_enable &= ~MSR_IA32_MISC_ENABLE_LIMIT_CPUID;\r\nwrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);\r\nc->cpuid_level = cpuid_eax(0);\r\nget_cpu_cap(c);\r\n}\r\n}\r\nif ((c->x86 == 0xf && c->x86_model >= 0x03) ||\r\n(c->x86 == 0x6 && c->x86_model >= 0x0e))\r\nset_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);\r\nif (c->x86 >= 6 && !cpu_has(c, X86_FEATURE_IA64)) {\r\nunsigned lower_word;\r\nwrmsr(MSR_IA32_UCODE_REV, 0, 0);\r\nsync_core();\r\nrdmsr(MSR_IA32_UCODE_REV, lower_word, c->microcode);\r\n}\r\nif (c->x86 == 6 && c->x86_model == 0x1c && c->x86_mask <= 2 &&\r\nc->microcode < 0x20e) {\r\nprintk(KERN_WARNING "Atom PSE erratum detected, BIOS microcode update recommended\n");\r\nclear_cpu_cap(c, X86_FEATURE_PSE);\r\n}\r\n#ifdef CONFIG_X86_64\r\nset_cpu_cap(c, X86_FEATURE_SYSENTER32);\r\n#else\r\nif (c->x86 == 15 && c->x86_cache_alignment == 64)\r\nc->x86_cache_alignment = 128;\r\n#endif\r\nif (c->x86 == 0xF && c->x86_model == 0x3\r\n&& (c->x86_mask == 0x3 || c->x86_mask == 0x4))\r\nc->x86_phys_bits = 36;\r\nif (c->x86_power & (1 << 8)) {\r\nset_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);\r\nset_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);\r\nif (!check_tsc_unstable())\r\nsched_clock_stable = 1;\r\n}\r\nif (c->x86 == 6 && c->x86_model < 15)\r\nclear_cpu_cap(c, X86_FEATURE_PAT);\r\n#ifdef CONFIG_KMEMCHECK\r\nif (c->x86 == 15) {\r\nrdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);\r\nif (misc_enable & MSR_IA32_MISC_ENABLE_FAST_STRING) {\r\nprintk(KERN_INFO "kmemcheck: Disabling fast string operations\n");\r\nmisc_enable &= ~MSR_IA32_MISC_ENABLE_FAST_STRING;\r\nwrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);\r\n}\r\n}\r\n#endif\r\nif (c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xd)) {\r\nrdmsrl(MSR_IA32_MISC_ENABLE, misc_enable);\r\nif (!(misc_enable & MSR_IA32_MISC_ENABLE_FAST_STRING)) {\r\nprintk(KERN_INFO "Disabled fast string operations\n");\r\nsetup_clear_cpu_cap(X86_FEATURE_REP_GOOD);\r\nsetup_clear_cpu_cap(X86_FEATURE_ERMS);\r\n}\r\n}\r\n}\r\nint __cpuinit ppro_with_ram_bug(void)\r\n{\r\nif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&\r\nboot_cpu_data.x86 == 6 &&\r\nboot_cpu_data.x86_model == 1 &&\r\nboot_cpu_data.x86_mask < 8) {\r\nprintk(KERN_INFO "Pentium Pro with Errata#50 detected. Taking evasive action.\n");\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __cpuinit trap_init_f00f_bug(void)\r\n{\r\n__set_fixmap(FIX_F00F_IDT, __pa(&idt_table), PAGE_KERNEL_RO);\r\nidt_descr.address = fix_to_virt(FIX_F00F_IDT);\r\nload_idt(&idt_descr);\r\n}\r\nstatic void __cpuinit intel_smp_check(struct cpuinfo_x86 *c)\r\n{\r\nif (!c->cpu_index)\r\nreturn;\r\nif (c->x86 == 5 &&\r\nc->x86_mask >= 1 && c->x86_mask <= 4 &&\r\nc->x86_model <= 3) {\r\nWARN_ONCE(1, "WARNING: SMP operation may be unreliable"\r\n"with B stepping processors.\n");\r\n}\r\n}\r\nstatic void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)\r\n{\r\nunsigned long lo, hi;\r\n#ifdef CONFIG_X86_F00F_BUG\r\nc->f00f_bug = 0;\r\nif (!paravirt_enabled() && c->x86 == 5) {\r\nstatic int f00f_workaround_enabled;\r\nc->f00f_bug = 1;\r\nif (!f00f_workaround_enabled) {\r\ntrap_init_f00f_bug();\r\nprintk(KERN_NOTICE "Intel Pentium with F0 0F bug - workaround enabled.\n");\r\nf00f_workaround_enabled = 1;\r\n}\r\n}\r\n#endif\r\nif ((c->x86<<8 | c->x86_model<<4 | c->x86_mask) < 0x633)\r\nclear_cpu_cap(c, X86_FEATURE_SEP);\r\nif ((c->x86 == 15) && (c->x86_model == 1) && (c->x86_mask == 1)) {\r\nrdmsr(MSR_IA32_MISC_ENABLE, lo, hi);\r\nif ((lo & MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE) == 0) {\r\nprintk (KERN_INFO "CPU: C0 stepping P4 Xeon detected.\n");\r\nprintk (KERN_INFO "CPU: Disabling hardware prefetching (Errata 037)\n");\r\nlo |= MSR_IA32_MISC_ENABLE_PREFETCH_DISABLE;\r\nwrmsr(MSR_IA32_MISC_ENABLE, lo, hi);\r\n}\r\n}\r\nif (cpu_has_apic && (c->x86<<8 | c->x86_model<<4) == 0x520 &&\r\n(c->x86_mask < 0x6 || c->x86_mask == 0xb))\r\nset_cpu_cap(c, X86_FEATURE_11AP);\r\n#ifdef CONFIG_X86_INTEL_USERCOPY\r\nswitch (c->x86) {\r\ncase 4:\r\nbreak;\r\ncase 5:\r\nbreak;\r\ncase 6:\r\nmovsl_mask.mask = 7;\r\nbreak;\r\ncase 15:\r\nmovsl_mask.mask = 7;\r\nbreak;\r\n}\r\n#endif\r\n#ifdef CONFIG_X86_NUMAQ\r\nnumaq_tsc_disable();\r\n#endif\r\nintel_smp_check(c);\r\n}\r\nstatic void __cpuinit intel_workarounds(struct cpuinfo_x86 *c)\r\n{\r\n}\r\nstatic void __cpuinit srat_detect_node(struct cpuinfo_x86 *c)\r\n{\r\n#ifdef CONFIG_NUMA\r\nunsigned node;\r\nint cpu = smp_processor_id();\r\nnode = numa_cpu_node(cpu);\r\nif (node == NUMA_NO_NODE || !node_online(node)) {\r\nnode = cpu_to_node(cpu);\r\n}\r\nnuma_set_node(cpu, node);\r\n#endif\r\n}\r\nstatic int __cpuinit intel_num_cpu_cores(struct cpuinfo_x86 *c)\r\n{\r\nunsigned int eax, ebx, ecx, edx;\r\nif (c->cpuid_level < 4)\r\nreturn 1;\r\ncpuid_count(4, 0, &eax, &ebx, &ecx, &edx);\r\nif (eax & 0x1f)\r\nreturn (eax >> 26) + 1;\r\nelse\r\nreturn 1;\r\n}\r\nstatic void __cpuinit detect_vmx_virtcap(struct cpuinfo_x86 *c)\r\n{\r\n#define X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW 0x00200000\r\n#define X86_VMX_FEATURE_PROC_CTLS_VNMI 0x00400000\r\n#define X86_VMX_FEATURE_PROC_CTLS_2ND_CTLS 0x80000000\r\n#define X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC 0x00000001\r\n#define X86_VMX_FEATURE_PROC_CTLS2_EPT 0x00000002\r\n#define X86_VMX_FEATURE_PROC_CTLS2_VPID 0x00000020\r\nu32 vmx_msr_low, vmx_msr_high, msr_ctl, msr_ctl2;\r\nclear_cpu_cap(c, X86_FEATURE_TPR_SHADOW);\r\nclear_cpu_cap(c, X86_FEATURE_VNMI);\r\nclear_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);\r\nclear_cpu_cap(c, X86_FEATURE_EPT);\r\nclear_cpu_cap(c, X86_FEATURE_VPID);\r\nrdmsr(MSR_IA32_VMX_PROCBASED_CTLS, vmx_msr_low, vmx_msr_high);\r\nmsr_ctl = vmx_msr_high | vmx_msr_low;\r\nif (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW)\r\nset_cpu_cap(c, X86_FEATURE_TPR_SHADOW);\r\nif (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_VNMI)\r\nset_cpu_cap(c, X86_FEATURE_VNMI);\r\nif (msr_ctl & X86_VMX_FEATURE_PROC_CTLS_2ND_CTLS) {\r\nrdmsr(MSR_IA32_VMX_PROCBASED_CTLS2,\r\nvmx_msr_low, vmx_msr_high);\r\nmsr_ctl2 = vmx_msr_high | vmx_msr_low;\r\nif ((msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VIRT_APIC) &&\r\n(msr_ctl & X86_VMX_FEATURE_PROC_CTLS_TPR_SHADOW))\r\nset_cpu_cap(c, X86_FEATURE_FLEXPRIORITY);\r\nif (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_EPT)\r\nset_cpu_cap(c, X86_FEATURE_EPT);\r\nif (msr_ctl2 & X86_VMX_FEATURE_PROC_CTLS2_VPID)\r\nset_cpu_cap(c, X86_FEATURE_VPID);\r\n}\r\n}\r\nstatic void __cpuinit init_intel(struct cpuinfo_x86 *c)\r\n{\r\nunsigned int l2 = 0;\r\nearly_init_intel(c);\r\nintel_workarounds(c);\r\ndetect_extended_topology(c);\r\nl2 = init_intel_cacheinfo(c);\r\nif (c->cpuid_level > 9) {\r\nunsigned eax = cpuid_eax(10);\r\nif ((eax & 0xff) && (((eax>>8) & 0xff) > 1))\r\nset_cpu_cap(c, X86_FEATURE_ARCH_PERFMON);\r\n}\r\nif (cpu_has_xmm2)\r\nset_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);\r\nif (cpu_has_ds) {\r\nunsigned int l1;\r\nrdmsr(MSR_IA32_MISC_ENABLE, l1, l2);\r\nif (!(l1 & (1<<11)))\r\nset_cpu_cap(c, X86_FEATURE_BTS);\r\nif (!(l1 & (1<<12)))\r\nset_cpu_cap(c, X86_FEATURE_PEBS);\r\n}\r\nif (c->x86 == 6 && c->x86_model == 29 && cpu_has_clflush)\r\nset_cpu_cap(c, X86_FEATURE_CLFLUSH_MONITOR);\r\n#ifdef CONFIG_X86_64\r\nif (c->x86 == 15)\r\nc->x86_cache_alignment = c->x86_clflush_size * 2;\r\nif (c->x86 == 6)\r\nset_cpu_cap(c, X86_FEATURE_REP_GOOD);\r\n#else\r\nif (c->x86 == 6) {\r\nchar *p = NULL;\r\nswitch (c->x86_model) {\r\ncase 5:\r\nif (l2 == 0)\r\np = "Celeron (Covington)";\r\nelse if (l2 == 256)\r\np = "Mobile Pentium II (Dixon)";\r\nbreak;\r\ncase 6:\r\nif (l2 == 128)\r\np = "Celeron (Mendocino)";\r\nelse if (c->x86_mask == 0 || c->x86_mask == 5)\r\np = "Celeron-A";\r\nbreak;\r\ncase 8:\r\nif (l2 == 128)\r\np = "Celeron (Coppermine)";\r\nbreak;\r\n}\r\nif (p)\r\nstrcpy(c->x86_model_id, p);\r\n}\r\nif (c->x86 == 15)\r\nset_cpu_cap(c, X86_FEATURE_P4);\r\nif (c->x86 == 6)\r\nset_cpu_cap(c, X86_FEATURE_P3);\r\n#endif\r\nif (!cpu_has(c, X86_FEATURE_XTOPOLOGY)) {\r\nc->x86_max_cores = intel_num_cpu_cores(c);\r\n#ifdef CONFIG_X86_32\r\ndetect_ht(c);\r\n#endif\r\n}\r\nsrat_detect_node(c);\r\nif (cpu_has(c, X86_FEATURE_VMX))\r\ndetect_vmx_virtcap(c);\r\nif (cpu_has(c, X86_FEATURE_EPB)) {\r\nu64 epb;\r\nrdmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);\r\nif ((epb & 0xF) == ENERGY_PERF_BIAS_PERFORMANCE) {\r\nprintk_once(KERN_WARNING "ENERGY_PERF_BIAS:"\r\n" Set to 'normal', was 'performance'\n"\r\n"ENERGY_PERF_BIAS: View and update with"\r\n" x86_energy_perf_policy(8)\n");\r\nepb = (epb & ~0xF) | ENERGY_PERF_BIAS_NORMAL;\r\nwrmsrl(MSR_IA32_ENERGY_PERF_BIAS, epb);\r\n}\r\n}\r\n}\r\nstatic unsigned int __cpuinit intel_size_cache(struct cpuinfo_x86 *c, unsigned int size)\r\n{\r\nif ((c->x86 == 6) && (c->x86_model == 11) && (size == 0))\r\nsize = 256;\r\nreturn size;\r\n}
