static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *filp,\r\nunsigned long addr,\r\nunsigned long len,\r\nunsigned long pgoff,\r\nunsigned long flags)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct * vma;\r\nunsigned long task_size = TASK_SIZE;\r\nunsigned long start_addr;\r\nif (test_thread_flag(TIF_32BIT))\r\ntask_size = STACK_TOP32;\r\nif (unlikely(len >= VA_EXCLUDE_START))\r\nreturn -ENOMEM;\r\nif (len > mm->cached_hole_size) {\r\nstart_addr = addr = mm->free_area_cache;\r\n} else {\r\nstart_addr = addr = TASK_UNMAPPED_BASE;\r\nmm->cached_hole_size = 0;\r\n}\r\ntask_size -= len;\r\nfull_search:\r\naddr = ALIGN(addr, HPAGE_SIZE);\r\nfor (vma = find_vma(mm, addr); ; vma = vma->vm_next) {\r\nif (addr < VA_EXCLUDE_START &&\r\n(addr + len) >= VA_EXCLUDE_START) {\r\naddr = VA_EXCLUDE_END;\r\nvma = find_vma(mm, VA_EXCLUDE_END);\r\n}\r\nif (unlikely(task_size < addr)) {\r\nif (start_addr != TASK_UNMAPPED_BASE) {\r\nstart_addr = addr = TASK_UNMAPPED_BASE;\r\nmm->cached_hole_size = 0;\r\ngoto full_search;\r\n}\r\nreturn -ENOMEM;\r\n}\r\nif (likely(!vma || addr + len <= vma->vm_start)) {\r\nmm->free_area_cache = addr + len;\r\nreturn addr;\r\n}\r\nif (addr + mm->cached_hole_size < vma->vm_start)\r\nmm->cached_hole_size = vma->vm_start - addr;\r\naddr = ALIGN(vma->vm_end, HPAGE_SIZE);\r\n}\r\n}\r\nstatic unsigned long\r\nhugetlb_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,\r\nconst unsigned long len,\r\nconst unsigned long pgoff,\r\nconst unsigned long flags)\r\n{\r\nstruct vm_area_struct *vma;\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long addr = addr0;\r\nBUG_ON(!test_thread_flag(TIF_32BIT));\r\nif (len <= mm->cached_hole_size) {\r\nmm->cached_hole_size = 0;\r\nmm->free_area_cache = mm->mmap_base;\r\n}\r\naddr = mm->free_area_cache & HPAGE_MASK;\r\nif (likely(addr > len)) {\r\nvma = find_vma(mm, addr-len);\r\nif (!vma || addr <= vma->vm_start) {\r\nreturn (mm->free_area_cache = addr-len);\r\n}\r\n}\r\nif (unlikely(mm->mmap_base < len))\r\ngoto bottomup;\r\naddr = (mm->mmap_base-len) & HPAGE_MASK;\r\ndo {\r\nvma = find_vma(mm, addr);\r\nif (likely(!vma || addr+len <= vma->vm_start)) {\r\nreturn (mm->free_area_cache = addr);\r\n}\r\nif (addr + mm->cached_hole_size < vma->vm_start)\r\nmm->cached_hole_size = vma->vm_start - addr;\r\naddr = (vma->vm_start-len) & HPAGE_MASK;\r\n} while (likely(len < vma->vm_start));\r\nbottomup:\r\nmm->cached_hole_size = ~0UL;\r\nmm->free_area_cache = TASK_UNMAPPED_BASE;\r\naddr = arch_get_unmapped_area(filp, addr0, len, pgoff, flags);\r\nmm->free_area_cache = mm->mmap_base;\r\nmm->cached_hole_size = ~0UL;\r\nreturn addr;\r\n}\r\nunsigned long\r\nhugetlb_get_unmapped_area(struct file *file, unsigned long addr,\r\nunsigned long len, unsigned long pgoff, unsigned long flags)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nunsigned long task_size = TASK_SIZE;\r\nif (test_thread_flag(TIF_32BIT))\r\ntask_size = STACK_TOP32;\r\nif (len & ~HPAGE_MASK)\r\nreturn -EINVAL;\r\nif (len > task_size)\r\nreturn -ENOMEM;\r\nif (flags & MAP_FIXED) {\r\nif (prepare_hugepage_range(file, addr, len))\r\nreturn -EINVAL;\r\nreturn addr;\r\n}\r\nif (addr) {\r\naddr = ALIGN(addr, HPAGE_SIZE);\r\nvma = find_vma(mm, addr);\r\nif (task_size - len >= addr &&\r\n(!vma || addr + len <= vma->vm_start))\r\nreturn addr;\r\n}\r\nif (mm->get_unmapped_area == arch_get_unmapped_area)\r\nreturn hugetlb_get_unmapped_area_bottomup(file, addr, len,\r\npgoff, flags);\r\nelse\r\nreturn hugetlb_get_unmapped_area_topdown(file, addr, len,\r\npgoff, flags);\r\n}\r\npte_t *huge_pte_alloc(struct mm_struct *mm,\r\nunsigned long addr, unsigned long sz)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte = NULL;\r\naddr &= HPAGE_MASK;\r\npgd = pgd_offset(mm, addr);\r\npud = pud_alloc(mm, pgd, addr);\r\nif (pud) {\r\npmd = pmd_alloc(mm, pud, addr);\r\nif (pmd)\r\npte = pte_alloc_map(mm, NULL, pmd, addr);\r\n}\r\nreturn pte;\r\n}\r\npte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte = NULL;\r\naddr &= HPAGE_MASK;\r\npgd = pgd_offset(mm, addr);\r\nif (!pgd_none(*pgd)) {\r\npud = pud_offset(pgd, addr);\r\nif (!pud_none(*pud)) {\r\npmd = pmd_offset(pud, addr);\r\nif (!pmd_none(*pmd))\r\npte = pte_offset_map(pmd, addr);\r\n}\r\n}\r\nreturn pte;\r\n}\r\nint huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)\r\n{\r\nreturn 0;\r\n}\r\nvoid set_huge_pte_at(struct mm_struct *mm, unsigned long addr,\r\npte_t *ptep, pte_t entry)\r\n{\r\nint i;\r\nif (!pte_present(*ptep) && pte_present(entry))\r\nmm->context.huge_pte_count++;\r\naddr &= HPAGE_MASK;\r\nfor (i = 0; i < (1 << HUGETLB_PAGE_ORDER); i++) {\r\nset_pte_at(mm, addr, ptep, entry);\r\nptep++;\r\naddr += PAGE_SIZE;\r\npte_val(entry) += PAGE_SIZE;\r\n}\r\n}\r\npte_t huge_ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,\r\npte_t *ptep)\r\n{\r\npte_t entry;\r\nint i;\r\nentry = *ptep;\r\nif (pte_present(entry))\r\nmm->context.huge_pte_count--;\r\naddr &= HPAGE_MASK;\r\nfor (i = 0; i < (1 << HUGETLB_PAGE_ORDER); i++) {\r\npte_clear(mm, addr, ptep);\r\naddr += PAGE_SIZE;\r\nptep++;\r\n}\r\nreturn entry;\r\n}\r\nstruct page *follow_huge_addr(struct mm_struct *mm,\r\nunsigned long address, int write)\r\n{\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nint pmd_huge(pmd_t pmd)\r\n{\r\nreturn 0;\r\n}\r\nint pud_huge(pud_t pud)\r\n{\r\nreturn 0;\r\n}\r\nstruct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,\r\npmd_t *pmd, int write)\r\n{\r\nreturn NULL;\r\n}\r\nstatic void context_reload(void *__data)\r\n{\r\nstruct mm_struct *mm = __data;\r\nif (mm == current->mm)\r\nload_secondary_context(mm);\r\n}\r\nvoid hugetlb_prefault_arch_hook(struct mm_struct *mm)\r\n{\r\nstruct tsb_config *tp = &mm->context.tsb_block[MM_TSB_HUGE];\r\nif (likely(tp->tsb != NULL))\r\nreturn;\r\ntsb_grow(mm, MM_TSB_HUGE, 0);\r\ntsb_context_switch(mm);\r\nsmp_tsb_sync(mm);\r\nif (tlb_type == cheetah_plus) {\r\nunsigned long ctx;\r\nspin_lock(&ctx_alloc_lock);\r\nctx = mm->context.sparc64_ctx_val;\r\nctx &= ~CTX_PGSZ_MASK;\r\nctx |= CTX_PGSZ_BASE << CTX_PGSZ0_SHIFT;\r\nctx |= CTX_PGSZ_HUGE << CTX_PGSZ1_SHIFT;\r\nif (ctx != mm->context.sparc64_ctx_val) {\r\ndo_flush_tlb_mm(mm);\r\nmm->context.sparc64_ctx_val = ctx;\r\non_each_cpu(context_reload, mm, 0);\r\n}\r\nspin_unlock(&ctx_alloc_lock);\r\n}\r\n}
