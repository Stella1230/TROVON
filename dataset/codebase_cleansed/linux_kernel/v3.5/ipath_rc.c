static u32 restart_sge(struct ipath_sge_state *ss, struct ipath_swqe *wqe,\r\nu32 psn, u32 pmtu)\r\n{\r\nu32 len;\r\nlen = ((psn - wqe->psn) & IPATH_PSN_MASK) * pmtu;\r\nss->sge = wqe->sg_list[0];\r\nss->sg_list = wqe->sg_list + 1;\r\nss->num_sge = wqe->wr.num_sge;\r\nipath_skip_sge(ss, len);\r\nreturn wqe->length - len;\r\n}\r\nstatic void ipath_init_restart(struct ipath_qp *qp, struct ipath_swqe *wqe)\r\n{\r\nstruct ipath_ibdev *dev;\r\nqp->s_len = restart_sge(&qp->s_sge, wqe, qp->s_psn,\r\nib_mtu_enum_to_int(qp->path_mtu));\r\ndev = to_idev(qp->ibqp.device);\r\nspin_lock(&dev->pending_lock);\r\nif (list_empty(&qp->timerwait))\r\nlist_add_tail(&qp->timerwait,\r\n&dev->pending[dev->pending_index]);\r\nspin_unlock(&dev->pending_lock);\r\n}\r\nstatic int ipath_make_rc_ack(struct ipath_ibdev *dev, struct ipath_qp *qp,\r\nstruct ipath_other_headers *ohdr, u32 pmtu)\r\n{\r\nstruct ipath_ack_entry *e;\r\nu32 hwords;\r\nu32 len;\r\nu32 bth0;\r\nu32 bth2;\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_RECV_OK))\r\ngoto bail;\r\nhwords = 5;\r\nswitch (qp->s_ack_state) {\r\ncase OP(RDMA_READ_RESPONSE_LAST):\r\ncase OP(RDMA_READ_RESPONSE_ONLY):\r\ncase OP(ATOMIC_ACKNOWLEDGE):\r\nif (++qp->s_tail_ack_queue > IPATH_MAX_RDMA_ATOMIC)\r\nqp->s_tail_ack_queue = 0;\r\ncase OP(SEND_ONLY):\r\ncase OP(ACKNOWLEDGE):\r\nif (qp->r_head_ack_queue == qp->s_tail_ack_queue) {\r\nif (qp->s_flags & IPATH_S_ACK_PENDING)\r\ngoto normal;\r\nqp->s_ack_state = OP(ACKNOWLEDGE);\r\ngoto bail;\r\n}\r\ne = &qp->s_ack_queue[qp->s_tail_ack_queue];\r\nif (e->opcode == OP(RDMA_READ_REQUEST)) {\r\nqp->s_ack_rdma_sge = e->rdma_sge;\r\nqp->s_cur_sge = &qp->s_ack_rdma_sge;\r\nlen = e->rdma_sge.sge.sge_length;\r\nif (len > pmtu) {\r\nlen = pmtu;\r\nqp->s_ack_state = OP(RDMA_READ_RESPONSE_FIRST);\r\n} else {\r\nqp->s_ack_state = OP(RDMA_READ_RESPONSE_ONLY);\r\ne->sent = 1;\r\n}\r\nohdr->u.aeth = ipath_compute_aeth(qp);\r\nhwords++;\r\nqp->s_ack_rdma_psn = e->psn;\r\nbth2 = qp->s_ack_rdma_psn++ & IPATH_PSN_MASK;\r\n} else {\r\nqp->s_cur_sge = NULL;\r\nlen = 0;\r\nqp->s_ack_state = OP(ATOMIC_ACKNOWLEDGE);\r\nohdr->u.at.aeth = ipath_compute_aeth(qp);\r\nohdr->u.at.atomic_ack_eth[0] =\r\ncpu_to_be32(e->atomic_data >> 32);\r\nohdr->u.at.atomic_ack_eth[1] =\r\ncpu_to_be32(e->atomic_data);\r\nhwords += sizeof(ohdr->u.at) / sizeof(u32);\r\nbth2 = e->psn;\r\ne->sent = 1;\r\n}\r\nbth0 = qp->s_ack_state << 24;\r\nbreak;\r\ncase OP(RDMA_READ_RESPONSE_FIRST):\r\nqp->s_ack_state = OP(RDMA_READ_RESPONSE_MIDDLE);\r\ncase OP(RDMA_READ_RESPONSE_MIDDLE):\r\nlen = qp->s_ack_rdma_sge.sge.sge_length;\r\nif (len > pmtu)\r\nlen = pmtu;\r\nelse {\r\nohdr->u.aeth = ipath_compute_aeth(qp);\r\nhwords++;\r\nqp->s_ack_state = OP(RDMA_READ_RESPONSE_LAST);\r\nqp->s_ack_queue[qp->s_tail_ack_queue].sent = 1;\r\n}\r\nbth0 = qp->s_ack_state << 24;\r\nbth2 = qp->s_ack_rdma_psn++ & IPATH_PSN_MASK;\r\nbreak;\r\ndefault:\r\nnormal:\r\nqp->s_ack_state = OP(SEND_ONLY);\r\nqp->s_flags &= ~IPATH_S_ACK_PENDING;\r\nqp->s_cur_sge = NULL;\r\nif (qp->s_nak_state)\r\nohdr->u.aeth =\r\ncpu_to_be32((qp->r_msn & IPATH_MSN_MASK) |\r\n(qp->s_nak_state <<\r\nIPATH_AETH_CREDIT_SHIFT));\r\nelse\r\nohdr->u.aeth = ipath_compute_aeth(qp);\r\nhwords++;\r\nlen = 0;\r\nbth0 = OP(ACKNOWLEDGE) << 24;\r\nbth2 = qp->s_ack_psn & IPATH_PSN_MASK;\r\n}\r\nqp->s_hdrwords = hwords;\r\nqp->s_cur_size = len;\r\nipath_make_ruc_header(dev, qp, ohdr, bth0, bth2);\r\nreturn 1;\r\nbail:\r\nreturn 0;\r\n}\r\nint ipath_make_rc_req(struct ipath_qp *qp)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct ipath_other_headers *ohdr;\r\nstruct ipath_sge_state *ss;\r\nstruct ipath_swqe *wqe;\r\nu32 hwords;\r\nu32 len;\r\nu32 bth0;\r\nu32 bth2;\r\nu32 pmtu = ib_mtu_enum_to_int(qp->path_mtu);\r\nchar newreq;\r\nunsigned long flags;\r\nint ret = 0;\r\nohdr = &qp->s_hdr.u.oth;\r\nif (qp->remote_ah_attr.ah_flags & IB_AH_GRH)\r\nohdr = &qp->s_hdr.u.l.oth;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif ((qp->r_head_ack_queue != qp->s_tail_ack_queue ||\r\n(qp->s_flags & IPATH_S_ACK_PENDING) ||\r\nqp->s_ack_state != OP(ACKNOWLEDGE)) &&\r\nipath_make_rc_ack(dev, qp, ohdr, pmtu))\r\ngoto done;\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_SEND_OK)) {\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_FLUSH_SEND))\r\ngoto bail;\r\nif (qp->s_last == qp->s_head)\r\ngoto bail;\r\nif (atomic_read(&qp->s_dma_busy)) {\r\nqp->s_flags |= IPATH_S_WAIT_DMA;\r\ngoto bail;\r\n}\r\nwqe = get_swqe_ptr(qp, qp->s_last);\r\nipath_send_complete(qp, wqe, IB_WC_WR_FLUSH_ERR);\r\ngoto done;\r\n}\r\nif (qp->s_rnr_timeout) {\r\nqp->s_flags |= IPATH_S_WAITING;\r\ngoto bail;\r\n}\r\nhwords = 5;\r\nbth0 = 1 << 22;\r\nwqe = get_swqe_ptr(qp, qp->s_cur);\r\nswitch (qp->s_state) {\r\ndefault:\r\nif (!(ib_ipath_state_ops[qp->state] &\r\nIPATH_PROCESS_NEXT_SEND_OK))\r\ngoto bail;\r\nnewreq = 0;\r\nif (qp->s_cur == qp->s_tail) {\r\nif (qp->s_tail == qp->s_head)\r\ngoto bail;\r\nif ((wqe->wr.send_flags & IB_SEND_FENCE) &&\r\nqp->s_num_rd_atomic) {\r\nqp->s_flags |= IPATH_S_FENCE_PENDING;\r\ngoto bail;\r\n}\r\nwqe->psn = qp->s_next_psn;\r\nnewreq = 1;\r\n}\r\nlen = wqe->length;\r\nss = &qp->s_sge;\r\nbth2 = 0;\r\nswitch (wqe->wr.opcode) {\r\ncase IB_WR_SEND:\r\ncase IB_WR_SEND_WITH_IMM:\r\nif (qp->s_lsn != (u32) -1 &&\r\nipath_cmp24(wqe->ssn, qp->s_lsn + 1) > 0) {\r\nqp->s_flags |= IPATH_S_WAIT_SSN_CREDIT;\r\ngoto bail;\r\n}\r\nwqe->lpsn = wqe->psn;\r\nif (len > pmtu) {\r\nwqe->lpsn += (len - 1) / pmtu;\r\nqp->s_state = OP(SEND_FIRST);\r\nlen = pmtu;\r\nbreak;\r\n}\r\nif (wqe->wr.opcode == IB_WR_SEND)\r\nqp->s_state = OP(SEND_ONLY);\r\nelse {\r\nqp->s_state = OP(SEND_ONLY_WITH_IMMEDIATE);\r\nohdr->u.imm_data = wqe->wr.ex.imm_data;\r\nhwords += 1;\r\n}\r\nif (wqe->wr.send_flags & IB_SEND_SOLICITED)\r\nbth0 |= 1 << 23;\r\nbth2 = 1 << 31;\r\nif (++qp->s_cur == qp->s_size)\r\nqp->s_cur = 0;\r\nbreak;\r\ncase IB_WR_RDMA_WRITE:\r\nif (newreq && qp->s_lsn != (u32) -1)\r\nqp->s_lsn++;\r\ncase IB_WR_RDMA_WRITE_WITH_IMM:\r\nif (qp->s_lsn != (u32) -1 &&\r\nipath_cmp24(wqe->ssn, qp->s_lsn + 1) > 0) {\r\nqp->s_flags |= IPATH_S_WAIT_SSN_CREDIT;\r\ngoto bail;\r\n}\r\nohdr->u.rc.reth.vaddr =\r\ncpu_to_be64(wqe->wr.wr.rdma.remote_addr);\r\nohdr->u.rc.reth.rkey =\r\ncpu_to_be32(wqe->wr.wr.rdma.rkey);\r\nohdr->u.rc.reth.length = cpu_to_be32(len);\r\nhwords += sizeof(struct ib_reth) / sizeof(u32);\r\nwqe->lpsn = wqe->psn;\r\nif (len > pmtu) {\r\nwqe->lpsn += (len - 1) / pmtu;\r\nqp->s_state = OP(RDMA_WRITE_FIRST);\r\nlen = pmtu;\r\nbreak;\r\n}\r\nif (wqe->wr.opcode == IB_WR_RDMA_WRITE)\r\nqp->s_state = OP(RDMA_WRITE_ONLY);\r\nelse {\r\nqp->s_state =\r\nOP(RDMA_WRITE_ONLY_WITH_IMMEDIATE);\r\nohdr->u.rc.imm_data = wqe->wr.ex.imm_data;\r\nhwords += 1;\r\nif (wqe->wr.send_flags & IB_SEND_SOLICITED)\r\nbth0 |= 1 << 23;\r\n}\r\nbth2 = 1 << 31;\r\nif (++qp->s_cur == qp->s_size)\r\nqp->s_cur = 0;\r\nbreak;\r\ncase IB_WR_RDMA_READ:\r\nif (newreq) {\r\nif (qp->s_num_rd_atomic >=\r\nqp->s_max_rd_atomic) {\r\nqp->s_flags |= IPATH_S_RDMAR_PENDING;\r\ngoto bail;\r\n}\r\nqp->s_num_rd_atomic++;\r\nif (qp->s_lsn != (u32) -1)\r\nqp->s_lsn++;\r\nif (len > pmtu)\r\nqp->s_next_psn += (len - 1) / pmtu;\r\nwqe->lpsn = qp->s_next_psn++;\r\n}\r\nohdr->u.rc.reth.vaddr =\r\ncpu_to_be64(wqe->wr.wr.rdma.remote_addr);\r\nohdr->u.rc.reth.rkey =\r\ncpu_to_be32(wqe->wr.wr.rdma.rkey);\r\nohdr->u.rc.reth.length = cpu_to_be32(len);\r\nqp->s_state = OP(RDMA_READ_REQUEST);\r\nhwords += sizeof(ohdr->u.rc.reth) / sizeof(u32);\r\nss = NULL;\r\nlen = 0;\r\nif (++qp->s_cur == qp->s_size)\r\nqp->s_cur = 0;\r\nbreak;\r\ncase IB_WR_ATOMIC_CMP_AND_SWP:\r\ncase IB_WR_ATOMIC_FETCH_AND_ADD:\r\nif (newreq) {\r\nif (qp->s_num_rd_atomic >=\r\nqp->s_max_rd_atomic) {\r\nqp->s_flags |= IPATH_S_RDMAR_PENDING;\r\ngoto bail;\r\n}\r\nqp->s_num_rd_atomic++;\r\nif (qp->s_lsn != (u32) -1)\r\nqp->s_lsn++;\r\nwqe->lpsn = wqe->psn;\r\n}\r\nif (wqe->wr.opcode == IB_WR_ATOMIC_CMP_AND_SWP) {\r\nqp->s_state = OP(COMPARE_SWAP);\r\nohdr->u.atomic_eth.swap_data = cpu_to_be64(\r\nwqe->wr.wr.atomic.swap);\r\nohdr->u.atomic_eth.compare_data = cpu_to_be64(\r\nwqe->wr.wr.atomic.compare_add);\r\n} else {\r\nqp->s_state = OP(FETCH_ADD);\r\nohdr->u.atomic_eth.swap_data = cpu_to_be64(\r\nwqe->wr.wr.atomic.compare_add);\r\nohdr->u.atomic_eth.compare_data = 0;\r\n}\r\nohdr->u.atomic_eth.vaddr[0] = cpu_to_be32(\r\nwqe->wr.wr.atomic.remote_addr >> 32);\r\nohdr->u.atomic_eth.vaddr[1] = cpu_to_be32(\r\nwqe->wr.wr.atomic.remote_addr);\r\nohdr->u.atomic_eth.rkey = cpu_to_be32(\r\nwqe->wr.wr.atomic.rkey);\r\nhwords += sizeof(struct ib_atomic_eth) / sizeof(u32);\r\nss = NULL;\r\nlen = 0;\r\nif (++qp->s_cur == qp->s_size)\r\nqp->s_cur = 0;\r\nbreak;\r\ndefault:\r\ngoto bail;\r\n}\r\nqp->s_sge.sge = wqe->sg_list[0];\r\nqp->s_sge.sg_list = wqe->sg_list + 1;\r\nqp->s_sge.num_sge = wqe->wr.num_sge;\r\nqp->s_len = wqe->length;\r\nif (newreq) {\r\nqp->s_tail++;\r\nif (qp->s_tail >= qp->s_size)\r\nqp->s_tail = 0;\r\n}\r\nbth2 |= qp->s_psn & IPATH_PSN_MASK;\r\nif (wqe->wr.opcode == IB_WR_RDMA_READ)\r\nqp->s_psn = wqe->lpsn + 1;\r\nelse {\r\nqp->s_psn++;\r\nif (ipath_cmp24(qp->s_psn, qp->s_next_psn) > 0)\r\nqp->s_next_psn = qp->s_psn;\r\n}\r\nspin_lock(&dev->pending_lock);\r\nif (list_empty(&qp->timerwait))\r\nlist_add_tail(&qp->timerwait,\r\n&dev->pending[dev->pending_index]);\r\nspin_unlock(&dev->pending_lock);\r\nbreak;\r\ncase OP(RDMA_READ_RESPONSE_FIRST):\r\nipath_init_restart(qp, wqe);\r\ncase OP(SEND_FIRST):\r\nqp->s_state = OP(SEND_MIDDLE);\r\ncase OP(SEND_MIDDLE):\r\nbth2 = qp->s_psn++ & IPATH_PSN_MASK;\r\nif (ipath_cmp24(qp->s_psn, qp->s_next_psn) > 0)\r\nqp->s_next_psn = qp->s_psn;\r\nss = &qp->s_sge;\r\nlen = qp->s_len;\r\nif (len > pmtu) {\r\nlen = pmtu;\r\nbreak;\r\n}\r\nif (wqe->wr.opcode == IB_WR_SEND)\r\nqp->s_state = OP(SEND_LAST);\r\nelse {\r\nqp->s_state = OP(SEND_LAST_WITH_IMMEDIATE);\r\nohdr->u.imm_data = wqe->wr.ex.imm_data;\r\nhwords += 1;\r\n}\r\nif (wqe->wr.send_flags & IB_SEND_SOLICITED)\r\nbth0 |= 1 << 23;\r\nbth2 |= 1 << 31;\r\nqp->s_cur++;\r\nif (qp->s_cur >= qp->s_size)\r\nqp->s_cur = 0;\r\nbreak;\r\ncase OP(RDMA_READ_RESPONSE_LAST):\r\nipath_init_restart(qp, wqe);\r\ncase OP(RDMA_WRITE_FIRST):\r\nqp->s_state = OP(RDMA_WRITE_MIDDLE);\r\ncase OP(RDMA_WRITE_MIDDLE):\r\nbth2 = qp->s_psn++ & IPATH_PSN_MASK;\r\nif (ipath_cmp24(qp->s_psn, qp->s_next_psn) > 0)\r\nqp->s_next_psn = qp->s_psn;\r\nss = &qp->s_sge;\r\nlen = qp->s_len;\r\nif (len > pmtu) {\r\nlen = pmtu;\r\nbreak;\r\n}\r\nif (wqe->wr.opcode == IB_WR_RDMA_WRITE)\r\nqp->s_state = OP(RDMA_WRITE_LAST);\r\nelse {\r\nqp->s_state = OP(RDMA_WRITE_LAST_WITH_IMMEDIATE);\r\nohdr->u.imm_data = wqe->wr.ex.imm_data;\r\nhwords += 1;\r\nif (wqe->wr.send_flags & IB_SEND_SOLICITED)\r\nbth0 |= 1 << 23;\r\n}\r\nbth2 |= 1 << 31;\r\nqp->s_cur++;\r\nif (qp->s_cur >= qp->s_size)\r\nqp->s_cur = 0;\r\nbreak;\r\ncase OP(RDMA_READ_RESPONSE_MIDDLE):\r\nipath_init_restart(qp, wqe);\r\nlen = ((qp->s_psn - wqe->psn) & IPATH_PSN_MASK) * pmtu;\r\nohdr->u.rc.reth.vaddr =\r\ncpu_to_be64(wqe->wr.wr.rdma.remote_addr + len);\r\nohdr->u.rc.reth.rkey =\r\ncpu_to_be32(wqe->wr.wr.rdma.rkey);\r\nohdr->u.rc.reth.length = cpu_to_be32(qp->s_len);\r\nqp->s_state = OP(RDMA_READ_REQUEST);\r\nhwords += sizeof(ohdr->u.rc.reth) / sizeof(u32);\r\nbth2 = qp->s_psn & IPATH_PSN_MASK;\r\nqp->s_psn = wqe->lpsn + 1;\r\nss = NULL;\r\nlen = 0;\r\nqp->s_cur++;\r\nif (qp->s_cur == qp->s_size)\r\nqp->s_cur = 0;\r\nbreak;\r\n}\r\nif (ipath_cmp24(qp->s_psn, qp->s_last_psn + IPATH_PSN_CREDIT - 1) >= 0)\r\nbth2 |= 1 << 31;\r\nqp->s_len -= len;\r\nqp->s_hdrwords = hwords;\r\nqp->s_cur_sge = ss;\r\nqp->s_cur_size = len;\r\nipath_make_ruc_header(dev, qp, ohdr, bth0 | (qp->s_state << 24), bth2);\r\ndone:\r\nret = 1;\r\ngoto unlock;\r\nbail:\r\nqp->s_flags &= ~IPATH_S_BUSY;\r\nunlock:\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void send_rc_ack(struct ipath_qp *qp)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct ipath_devdata *dd;\r\nu16 lrh0;\r\nu32 bth0;\r\nu32 hwords;\r\nu32 __iomem *piobuf;\r\nstruct ipath_ib_header hdr;\r\nstruct ipath_other_headers *ohdr;\r\nunsigned long flags;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (qp->r_head_ack_queue != qp->s_tail_ack_queue ||\r\n(qp->s_flags & IPATH_S_ACK_PENDING) ||\r\nqp->s_ack_state != OP(ACKNOWLEDGE))\r\ngoto queue_ack;\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\ndd = dev->dd;\r\nif (!(dd->ipath_flags & IPATH_LINKACTIVE))\r\ngoto done;\r\npiobuf = ipath_getpiobuf(dd, 0, NULL);\r\nif (!piobuf) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\ngoto queue_ack;\r\n}\r\nohdr = &hdr.u.oth;\r\nlrh0 = IPATH_LRH_BTH;\r\nhwords = 6;\r\nif (unlikely(qp->remote_ah_attr.ah_flags & IB_AH_GRH)) {\r\nhwords += ipath_make_grh(dev, &hdr.u.l.grh,\r\n&qp->remote_ah_attr.grh,\r\nhwords, 0);\r\nohdr = &hdr.u.l.oth;\r\nlrh0 = IPATH_LRH_GRH;\r\n}\r\nbth0 = ipath_get_pkey(dd, qp->s_pkey_index) |\r\n(OP(ACKNOWLEDGE) << 24) | (1 << 22);\r\nif (qp->r_nak_state)\r\nohdr->u.aeth = cpu_to_be32((qp->r_msn & IPATH_MSN_MASK) |\r\n(qp->r_nak_state <<\r\nIPATH_AETH_CREDIT_SHIFT));\r\nelse\r\nohdr->u.aeth = ipath_compute_aeth(qp);\r\nlrh0 |= qp->remote_ah_attr.sl << 4;\r\nhdr.lrh[0] = cpu_to_be16(lrh0);\r\nhdr.lrh[1] = cpu_to_be16(qp->remote_ah_attr.dlid);\r\nhdr.lrh[2] = cpu_to_be16(hwords + SIZE_OF_CRC);\r\nhdr.lrh[3] = cpu_to_be16(dd->ipath_lid |\r\nqp->remote_ah_attr.src_path_bits);\r\nohdr->bth[0] = cpu_to_be32(bth0);\r\nohdr->bth[1] = cpu_to_be32(qp->remote_qpn);\r\nohdr->bth[2] = cpu_to_be32(qp->r_ack_psn & IPATH_PSN_MASK);\r\nwriteq(hwords + 1, piobuf);\r\nif (dd->ipath_flags & IPATH_PIO_FLUSH_WC) {\r\nu32 *hdrp = (u32 *) &hdr;\r\nipath_flush_wc();\r\n__iowrite32_copy(piobuf + 2, hdrp, hwords - 1);\r\nipath_flush_wc();\r\n__raw_writel(hdrp[hwords - 1], piobuf + hwords + 1);\r\n} else\r\n__iowrite32_copy(piobuf + 2, (u32 *) &hdr, hwords);\r\nipath_flush_wc();\r\ndev->n_unicast_xmit++;\r\ngoto done;\r\nqueue_ack:\r\nif (ib_ipath_state_ops[qp->state] & IPATH_PROCESS_RECV_OK) {\r\ndev->n_rc_qacks++;\r\nqp->s_flags |= IPATH_S_ACK_PENDING;\r\nqp->s_nak_state = qp->r_nak_state;\r\nqp->s_ack_psn = qp->r_ack_psn;\r\nipath_schedule_send(qp);\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\ndone:\r\nreturn;\r\n}\r\nstatic void reset_psn(struct ipath_qp *qp, u32 psn)\r\n{\r\nu32 n = qp->s_last;\r\nstruct ipath_swqe *wqe = get_swqe_ptr(qp, n);\r\nu32 opcode;\r\nqp->s_cur = n;\r\nif (ipath_cmp24(psn, wqe->psn) <= 0) {\r\nqp->s_state = OP(SEND_LAST);\r\ngoto done;\r\n}\r\nopcode = wqe->wr.opcode;\r\nfor (;;) {\r\nint diff;\r\nif (++n == qp->s_size)\r\nn = 0;\r\nif (n == qp->s_tail)\r\nbreak;\r\nwqe = get_swqe_ptr(qp, n);\r\ndiff = ipath_cmp24(psn, wqe->psn);\r\nif (diff < 0)\r\nbreak;\r\nqp->s_cur = n;\r\nif (diff == 0) {\r\nqp->s_state = OP(SEND_LAST);\r\ngoto done;\r\n}\r\nopcode = wqe->wr.opcode;\r\n}\r\nswitch (opcode) {\r\ncase IB_WR_SEND:\r\ncase IB_WR_SEND_WITH_IMM:\r\nqp->s_state = OP(RDMA_READ_RESPONSE_FIRST);\r\nbreak;\r\ncase IB_WR_RDMA_WRITE:\r\ncase IB_WR_RDMA_WRITE_WITH_IMM:\r\nqp->s_state = OP(RDMA_READ_RESPONSE_LAST);\r\nbreak;\r\ncase IB_WR_RDMA_READ:\r\nqp->s_state = OP(RDMA_READ_RESPONSE_MIDDLE);\r\nbreak;\r\ndefault:\r\nqp->s_state = OP(SEND_LAST);\r\n}\r\ndone:\r\nqp->s_psn = psn;\r\n}\r\nvoid ipath_restart_rc(struct ipath_qp *qp, u32 psn)\r\n{\r\nstruct ipath_swqe *wqe = get_swqe_ptr(qp, qp->s_last);\r\nstruct ipath_ibdev *dev;\r\nif (qp->s_retry == 0) {\r\nipath_send_complete(qp, wqe, IB_WC_RETRY_EXC_ERR);\r\nipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);\r\ngoto bail;\r\n}\r\nqp->s_retry--;\r\ndev = to_idev(qp->ibqp.device);\r\nspin_lock(&dev->pending_lock);\r\nif (!list_empty(&qp->timerwait))\r\nlist_del_init(&qp->timerwait);\r\nif (!list_empty(&qp->piowait))\r\nlist_del_init(&qp->piowait);\r\nspin_unlock(&dev->pending_lock);\r\nif (wqe->wr.opcode == IB_WR_RDMA_READ)\r\ndev->n_rc_resends++;\r\nelse\r\ndev->n_rc_resends += (qp->s_psn - psn) & IPATH_PSN_MASK;\r\nreset_psn(qp, psn);\r\nipath_schedule_send(qp);\r\nbail:\r\nreturn;\r\n}\r\nstatic inline void update_last_psn(struct ipath_qp *qp, u32 psn)\r\n{\r\nqp->s_last_psn = psn;\r\n}\r\nstatic int do_rc_ack(struct ipath_qp *qp, u32 aeth, u32 psn, int opcode,\r\nu64 val)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct ib_wc wc;\r\nenum ib_wc_status status;\r\nstruct ipath_swqe *wqe;\r\nint ret = 0;\r\nu32 ack_psn;\r\nint diff;\r\nspin_lock(&dev->pending_lock);\r\nif (!list_empty(&qp->timerwait))\r\nlist_del_init(&qp->timerwait);\r\nspin_unlock(&dev->pending_lock);\r\nack_psn = psn;\r\nif (aeth >> 29)\r\nack_psn--;\r\nwqe = get_swqe_ptr(qp, qp->s_last);\r\nwhile ((diff = ipath_cmp24(ack_psn, wqe->lpsn)) >= 0) {\r\nif (wqe->wr.opcode == IB_WR_RDMA_READ &&\r\nopcode == OP(RDMA_READ_RESPONSE_ONLY) &&\r\ndiff == 0) {\r\nret = 1;\r\ngoto bail;\r\n}\r\nif ((wqe->wr.opcode == IB_WR_RDMA_READ &&\r\n(opcode != OP(RDMA_READ_RESPONSE_LAST) || diff != 0)) ||\r\n((wqe->wr.opcode == IB_WR_ATOMIC_CMP_AND_SWP ||\r\nwqe->wr.opcode == IB_WR_ATOMIC_FETCH_AND_ADD) &&\r\n(opcode != OP(ATOMIC_ACKNOWLEDGE) || diff != 0))) {\r\nupdate_last_psn(qp, wqe->psn - 1);\r\nipath_restart_rc(qp, wqe->psn);\r\ngoto bail;\r\n}\r\nif (wqe->wr.opcode == IB_WR_ATOMIC_CMP_AND_SWP ||\r\nwqe->wr.opcode == IB_WR_ATOMIC_FETCH_AND_ADD)\r\n*(u64 *) wqe->sg_list[0].vaddr = val;\r\nif (qp->s_num_rd_atomic &&\r\n(wqe->wr.opcode == IB_WR_RDMA_READ ||\r\nwqe->wr.opcode == IB_WR_ATOMIC_CMP_AND_SWP ||\r\nwqe->wr.opcode == IB_WR_ATOMIC_FETCH_AND_ADD)) {\r\nqp->s_num_rd_atomic--;\r\nif (((qp->s_flags & IPATH_S_FENCE_PENDING) &&\r\n!qp->s_num_rd_atomic) ||\r\nqp->s_flags & IPATH_S_RDMAR_PENDING)\r\nipath_schedule_send(qp);\r\n}\r\nif (!(qp->s_flags & IPATH_S_SIGNAL_REQ_WR) ||\r\n(wqe->wr.send_flags & IB_SEND_SIGNALED)) {\r\nmemset(&wc, 0, sizeof wc);\r\nwc.wr_id = wqe->wr.wr_id;\r\nwc.status = IB_WC_SUCCESS;\r\nwc.opcode = ib_ipath_wc_opcode[wqe->wr.opcode];\r\nwc.byte_len = wqe->length;\r\nwc.qp = &qp->ibqp;\r\nwc.src_qp = qp->remote_qpn;\r\nwc.slid = qp->remote_ah_attr.dlid;\r\nwc.sl = qp->remote_ah_attr.sl;\r\nipath_cq_enter(to_icq(qp->ibqp.send_cq), &wc, 0);\r\n}\r\nqp->s_retry = qp->s_retry_cnt;\r\nif (qp->s_last == qp->s_cur) {\r\nif (++qp->s_cur >= qp->s_size)\r\nqp->s_cur = 0;\r\nqp->s_last = qp->s_cur;\r\nif (qp->s_last == qp->s_tail)\r\nbreak;\r\nwqe = get_swqe_ptr(qp, qp->s_cur);\r\nqp->s_state = OP(SEND_LAST);\r\nqp->s_psn = wqe->psn;\r\n} else {\r\nif (++qp->s_last >= qp->s_size)\r\nqp->s_last = 0;\r\nif (qp->state == IB_QPS_SQD && qp->s_last == qp->s_cur)\r\nqp->s_draining = 0;\r\nif (qp->s_last == qp->s_tail)\r\nbreak;\r\nwqe = get_swqe_ptr(qp, qp->s_last);\r\n}\r\n}\r\nswitch (aeth >> 29) {\r\ncase 0:\r\ndev->n_rc_acks++;\r\nif (qp->s_last != qp->s_tail) {\r\nspin_lock(&dev->pending_lock);\r\nif (list_empty(&qp->timerwait))\r\nlist_add_tail(&qp->timerwait,\r\n&dev->pending[dev->pending_index]);\r\nspin_unlock(&dev->pending_lock);\r\nif (ipath_cmp24(qp->s_psn, psn) <= 0) {\r\nreset_psn(qp, psn + 1);\r\nipath_schedule_send(qp);\r\n}\r\n} else if (ipath_cmp24(qp->s_psn, psn) <= 0) {\r\nqp->s_state = OP(SEND_LAST);\r\nqp->s_psn = psn + 1;\r\n}\r\nipath_get_credit(qp, aeth);\r\nqp->s_rnr_retry = qp->s_rnr_retry_cnt;\r\nqp->s_retry = qp->s_retry_cnt;\r\nupdate_last_psn(qp, psn);\r\nret = 1;\r\ngoto bail;\r\ncase 1:\r\ndev->n_rnr_naks++;\r\nif (qp->s_last == qp->s_tail)\r\ngoto bail;\r\nif (qp->s_rnr_retry == 0) {\r\nstatus = IB_WC_RNR_RETRY_EXC_ERR;\r\ngoto class_b;\r\n}\r\nif (qp->s_rnr_retry_cnt < 7)\r\nqp->s_rnr_retry--;\r\nupdate_last_psn(qp, psn - 1);\r\nif (wqe->wr.opcode == IB_WR_RDMA_READ)\r\ndev->n_rc_resends++;\r\nelse\r\ndev->n_rc_resends +=\r\n(qp->s_psn - psn) & IPATH_PSN_MASK;\r\nreset_psn(qp, psn);\r\nqp->s_rnr_timeout =\r\nib_ipath_rnr_table[(aeth >> IPATH_AETH_CREDIT_SHIFT) &\r\nIPATH_AETH_CREDIT_MASK];\r\nipath_insert_rnr_queue(qp);\r\nipath_schedule_send(qp);\r\ngoto bail;\r\ncase 3:\r\nif (qp->s_last == qp->s_tail)\r\ngoto bail;\r\nupdate_last_psn(qp, psn - 1);\r\nswitch ((aeth >> IPATH_AETH_CREDIT_SHIFT) &\r\nIPATH_AETH_CREDIT_MASK) {\r\ncase 0:\r\ndev->n_seq_naks++;\r\nipath_restart_rc(qp, psn);\r\nbreak;\r\ncase 1:\r\nstatus = IB_WC_REM_INV_REQ_ERR;\r\ndev->n_other_naks++;\r\ngoto class_b;\r\ncase 2:\r\nstatus = IB_WC_REM_ACCESS_ERR;\r\ndev->n_other_naks++;\r\ngoto class_b;\r\ncase 3:\r\nstatus = IB_WC_REM_OP_ERR;\r\ndev->n_other_naks++;\r\nclass_b:\r\nipath_send_complete(qp, wqe, status);\r\nipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);\r\nbreak;\r\ndefault:\r\ngoto reserved;\r\n}\r\nqp->s_rnr_retry = qp->s_rnr_retry_cnt;\r\ngoto bail;\r\ndefault:\r\nreserved:\r\ngoto bail;\r\n}\r\nbail:\r\nreturn ret;\r\n}\r\nstatic inline void ipath_rc_rcv_resp(struct ipath_ibdev *dev,\r\nstruct ipath_other_headers *ohdr,\r\nvoid *data, u32 tlen,\r\nstruct ipath_qp *qp,\r\nu32 opcode,\r\nu32 psn, u32 hdrsize, u32 pmtu,\r\nint header_in_data)\r\n{\r\nstruct ipath_swqe *wqe;\r\nenum ib_wc_status status;\r\nunsigned long flags;\r\nint diff;\r\nu32 pad;\r\nu32 aeth;\r\nu64 val;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_RECV_OK))\r\ngoto ack_done;\r\nif (ipath_cmp24(psn, qp->s_next_psn) >= 0)\r\ngoto ack_done;\r\ndiff = ipath_cmp24(psn, qp->s_last_psn);\r\nif (unlikely(diff <= 0)) {\r\nif (diff == 0 && opcode == OP(ACKNOWLEDGE)) {\r\nif (!header_in_data)\r\naeth = be32_to_cpu(ohdr->u.aeth);\r\nelse {\r\naeth = be32_to_cpu(((__be32 *) data)[0]);\r\ndata += sizeof(__be32);\r\n}\r\nif ((aeth >> 29) == 0)\r\nipath_get_credit(qp, aeth);\r\n}\r\ngoto ack_done;\r\n}\r\nif (unlikely(qp->s_last == qp->s_tail))\r\ngoto ack_done;\r\nwqe = get_swqe_ptr(qp, qp->s_last);\r\nstatus = IB_WC_SUCCESS;\r\nswitch (opcode) {\r\ncase OP(ACKNOWLEDGE):\r\ncase OP(ATOMIC_ACKNOWLEDGE):\r\ncase OP(RDMA_READ_RESPONSE_FIRST):\r\nif (!header_in_data)\r\naeth = be32_to_cpu(ohdr->u.aeth);\r\nelse {\r\naeth = be32_to_cpu(((__be32 *) data)[0]);\r\ndata += sizeof(__be32);\r\n}\r\nif (opcode == OP(ATOMIC_ACKNOWLEDGE)) {\r\nif (!header_in_data) {\r\n__be32 *p = ohdr->u.at.atomic_ack_eth;\r\nval = ((u64) be32_to_cpu(p[0]) << 32) |\r\nbe32_to_cpu(p[1]);\r\n} else\r\nval = be64_to_cpu(((__be64 *) data)[0]);\r\n} else\r\nval = 0;\r\nif (!do_rc_ack(qp, aeth, psn, opcode, val) ||\r\nopcode != OP(RDMA_READ_RESPONSE_FIRST))\r\ngoto ack_done;\r\nhdrsize += 4;\r\nwqe = get_swqe_ptr(qp, qp->s_last);\r\nif (unlikely(wqe->wr.opcode != IB_WR_RDMA_READ))\r\ngoto ack_op_err;\r\nqp->r_flags &= ~IPATH_R_RDMAR_SEQ;\r\nqp->s_rdma_read_len = restart_sge(&qp->s_rdma_read_sge,\r\nwqe, psn, pmtu);\r\ngoto read_middle;\r\ncase OP(RDMA_READ_RESPONSE_MIDDLE):\r\nif (unlikely(ipath_cmp24(psn, qp->s_last_psn + 1))) {\r\ndev->n_rdma_seq++;\r\nif (qp->r_flags & IPATH_R_RDMAR_SEQ)\r\ngoto ack_done;\r\nqp->r_flags |= IPATH_R_RDMAR_SEQ;\r\nipath_restart_rc(qp, qp->s_last_psn + 1);\r\ngoto ack_done;\r\n}\r\nif (unlikely(wqe->wr.opcode != IB_WR_RDMA_READ))\r\ngoto ack_op_err;\r\nread_middle:\r\nif (unlikely(tlen != (hdrsize + pmtu + 4)))\r\ngoto ack_len_err;\r\nif (unlikely(pmtu >= qp->s_rdma_read_len))\r\ngoto ack_len_err;\r\nspin_lock(&dev->pending_lock);\r\nif (qp->s_rnr_timeout == 0 && !list_empty(&qp->timerwait))\r\nlist_move_tail(&qp->timerwait,\r\n&dev->pending[dev->pending_index]);\r\nspin_unlock(&dev->pending_lock);\r\nif (opcode == OP(RDMA_READ_RESPONSE_MIDDLE))\r\nqp->s_retry = qp->s_retry_cnt;\r\nqp->s_rdma_read_len -= pmtu;\r\nupdate_last_psn(qp, psn);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nipath_copy_sge(&qp->s_rdma_read_sge, data, pmtu);\r\ngoto bail;\r\ncase OP(RDMA_READ_RESPONSE_ONLY):\r\nif (!header_in_data)\r\naeth = be32_to_cpu(ohdr->u.aeth);\r\nelse\r\naeth = be32_to_cpu(((__be32 *) data)[0]);\r\nif (!do_rc_ack(qp, aeth, psn, opcode, 0))\r\ngoto ack_done;\r\npad = (be32_to_cpu(ohdr->bth[0]) >> 20) & 3;\r\nif (unlikely(tlen < (hdrsize + pad + 8)))\r\ngoto ack_len_err;\r\nwqe = get_swqe_ptr(qp, qp->s_last);\r\nqp->s_rdma_read_len = restart_sge(&qp->s_rdma_read_sge,\r\nwqe, psn, pmtu);\r\ngoto read_last;\r\ncase OP(RDMA_READ_RESPONSE_LAST):\r\nif (unlikely(ipath_cmp24(psn, qp->s_last_psn + 1))) {\r\ndev->n_rdma_seq++;\r\nif (qp->r_flags & IPATH_R_RDMAR_SEQ)\r\ngoto ack_done;\r\nqp->r_flags |= IPATH_R_RDMAR_SEQ;\r\nipath_restart_rc(qp, qp->s_last_psn + 1);\r\ngoto ack_done;\r\n}\r\nif (unlikely(wqe->wr.opcode != IB_WR_RDMA_READ))\r\ngoto ack_op_err;\r\npad = (be32_to_cpu(ohdr->bth[0]) >> 20) & 3;\r\nif (unlikely(tlen <= (hdrsize + pad + 8)))\r\ngoto ack_len_err;\r\nread_last:\r\ntlen -= hdrsize + pad + 8;\r\nif (unlikely(tlen != qp->s_rdma_read_len))\r\ngoto ack_len_err;\r\nif (!header_in_data)\r\naeth = be32_to_cpu(ohdr->u.aeth);\r\nelse {\r\naeth = be32_to_cpu(((__be32 *) data)[0]);\r\ndata += sizeof(__be32);\r\n}\r\nipath_copy_sge(&qp->s_rdma_read_sge, data, tlen);\r\n(void) do_rc_ack(qp, aeth, psn,\r\nOP(RDMA_READ_RESPONSE_LAST), 0);\r\ngoto ack_done;\r\n}\r\nack_op_err:\r\nstatus = IB_WC_LOC_QP_OP_ERR;\r\ngoto ack_err;\r\nack_len_err:\r\nstatus = IB_WC_LOC_LEN_ERR;\r\nack_err:\r\nipath_send_complete(qp, wqe, status);\r\nipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);\r\nack_done:\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nbail:\r\nreturn;\r\n}\r\nstatic inline int ipath_rc_rcv_error(struct ipath_ibdev *dev,\r\nstruct ipath_other_headers *ohdr,\r\nvoid *data,\r\nstruct ipath_qp *qp,\r\nu32 opcode,\r\nu32 psn,\r\nint diff,\r\nint header_in_data)\r\n{\r\nstruct ipath_ack_entry *e;\r\nu8 i, prev;\r\nint old_req;\r\nunsigned long flags;\r\nif (diff > 0) {\r\nif (!qp->r_nak_state) {\r\nqp->r_nak_state = IB_NAK_PSN_ERROR;\r\nqp->r_ack_psn = qp->r_psn;\r\ngoto send_ack;\r\n}\r\ngoto done;\r\n}\r\npsn &= IPATH_PSN_MASK;\r\ne = NULL;\r\nold_req = 1;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_RECV_OK))\r\ngoto unlock_done;\r\nfor (i = qp->r_head_ack_queue; ; i = prev) {\r\nif (i == qp->s_tail_ack_queue)\r\nold_req = 0;\r\nif (i)\r\nprev = i - 1;\r\nelse\r\nprev = IPATH_MAX_RDMA_ATOMIC;\r\nif (prev == qp->r_head_ack_queue) {\r\ne = NULL;\r\nbreak;\r\n}\r\ne = &qp->s_ack_queue[prev];\r\nif (!e->opcode) {\r\ne = NULL;\r\nbreak;\r\n}\r\nif (ipath_cmp24(psn, e->psn) >= 0) {\r\nif (prev == qp->s_tail_ack_queue)\r\nold_req = 0;\r\nbreak;\r\n}\r\n}\r\nswitch (opcode) {\r\ncase OP(RDMA_READ_REQUEST): {\r\nstruct ib_reth *reth;\r\nu32 offset;\r\nu32 len;\r\nif (!e || e->opcode != OP(RDMA_READ_REQUEST) || old_req)\r\ngoto unlock_done;\r\nif (!header_in_data)\r\nreth = &ohdr->u.rc.reth;\r\nelse {\r\nreth = (struct ib_reth *)data;\r\ndata += sizeof(*reth);\r\n}\r\noffset = ((psn - e->psn) & IPATH_PSN_MASK) *\r\nib_mtu_enum_to_int(qp->path_mtu);\r\nlen = be32_to_cpu(reth->length);\r\nif (unlikely(offset + len > e->rdma_sge.sge.sge_length))\r\ngoto unlock_done;\r\nif (len != 0) {\r\nu32 rkey = be32_to_cpu(reth->rkey);\r\nu64 vaddr = be64_to_cpu(reth->vaddr);\r\nint ok;\r\nok = ipath_rkey_ok(qp, &e->rdma_sge,\r\nlen, vaddr, rkey,\r\nIB_ACCESS_REMOTE_READ);\r\nif (unlikely(!ok))\r\ngoto unlock_done;\r\n} else {\r\ne->rdma_sge.sg_list = NULL;\r\ne->rdma_sge.num_sge = 0;\r\ne->rdma_sge.sge.mr = NULL;\r\ne->rdma_sge.sge.vaddr = NULL;\r\ne->rdma_sge.sge.length = 0;\r\ne->rdma_sge.sge.sge_length = 0;\r\n}\r\ne->psn = psn;\r\nqp->s_ack_state = OP(ACKNOWLEDGE);\r\nqp->s_tail_ack_queue = prev;\r\nbreak;\r\n}\r\ncase OP(COMPARE_SWAP):\r\ncase OP(FETCH_ADD): {\r\nif (!e || e->opcode != (u8) opcode || old_req)\r\ngoto unlock_done;\r\nqp->s_ack_state = OP(ACKNOWLEDGE);\r\nqp->s_tail_ack_queue = prev;\r\nbreak;\r\n}\r\ndefault:\r\nif (old_req)\r\ngoto unlock_done;\r\nif (i == qp->r_head_ack_queue) {\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nqp->r_nak_state = 0;\r\nqp->r_ack_psn = qp->r_psn - 1;\r\ngoto send_ack;\r\n}\r\nif (qp->r_head_ack_queue == qp->s_tail_ack_queue &&\r\n!(qp->s_flags & IPATH_S_ACK_PENDING) &&\r\nqp->s_ack_state == OP(ACKNOWLEDGE)) {\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nqp->r_nak_state = 0;\r\nqp->r_ack_psn = qp->s_ack_queue[i].psn - 1;\r\ngoto send_ack;\r\n}\r\nqp->s_ack_state = OP(ACKNOWLEDGE);\r\nqp->s_tail_ack_queue = i;\r\nbreak;\r\n}\r\nqp->r_nak_state = 0;\r\nipath_schedule_send(qp);\r\nunlock_done:\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\ndone:\r\nreturn 1;\r\nsend_ack:\r\nreturn 0;\r\n}\r\nvoid ipath_rc_error(struct ipath_qp *qp, enum ib_wc_status err)\r\n{\r\nunsigned long flags;\r\nint lastwqe;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nlastwqe = ipath_error_qp(qp, err);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (lastwqe) {\r\nstruct ib_event ev;\r\nev.device = qp->ibqp.device;\r\nev.element.qp = &qp->ibqp;\r\nev.event = IB_EVENT_QP_LAST_WQE_REACHED;\r\nqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\r\n}\r\n}\r\nstatic inline void ipath_update_ack_queue(struct ipath_qp *qp, unsigned n)\r\n{\r\nunsigned next;\r\nnext = n + 1;\r\nif (next > IPATH_MAX_RDMA_ATOMIC)\r\nnext = 0;\r\nif (n == qp->s_tail_ack_queue) {\r\nqp->s_tail_ack_queue = next;\r\nqp->s_ack_state = OP(ACKNOWLEDGE);\r\n}\r\n}\r\nvoid ipath_rc_rcv(struct ipath_ibdev *dev, struct ipath_ib_header *hdr,\r\nint has_grh, void *data, u32 tlen, struct ipath_qp *qp)\r\n{\r\nstruct ipath_other_headers *ohdr;\r\nu32 opcode;\r\nu32 hdrsize;\r\nu32 psn;\r\nu32 pad;\r\nstruct ib_wc wc;\r\nu32 pmtu = ib_mtu_enum_to_int(qp->path_mtu);\r\nint diff;\r\nstruct ib_reth *reth;\r\nint header_in_data;\r\nunsigned long flags;\r\nif (unlikely(be16_to_cpu(hdr->lrh[3]) != qp->remote_ah_attr.dlid))\r\ngoto done;\r\nif (!has_grh) {\r\nohdr = &hdr->u.oth;\r\nhdrsize = 8 + 12;\r\npsn = be32_to_cpu(ohdr->bth[2]);\r\nheader_in_data = 0;\r\n} else {\r\nohdr = &hdr->u.l.oth;\r\nhdrsize = 8 + 40 + 12;\r\nheader_in_data = dev->dd->ipath_rcvhdrentsize == 16;\r\nif (header_in_data) {\r\npsn = be32_to_cpu(((__be32 *) data)[0]);\r\ndata += sizeof(__be32);\r\n} else\r\npsn = be32_to_cpu(ohdr->bth[2]);\r\n}\r\nopcode = be32_to_cpu(ohdr->bth[0]) >> 24;\r\nif (opcode >= OP(RDMA_READ_RESPONSE_FIRST) &&\r\nopcode <= OP(ATOMIC_ACKNOWLEDGE)) {\r\nipath_rc_rcv_resp(dev, ohdr, data, tlen, qp, opcode, psn,\r\nhdrsize, pmtu, header_in_data);\r\ngoto done;\r\n}\r\ndiff = ipath_cmp24(psn, qp->r_psn);\r\nif (unlikely(diff)) {\r\nif (ipath_rc_rcv_error(dev, ohdr, data, qp, opcode,\r\npsn, diff, header_in_data))\r\ngoto done;\r\ngoto send_ack;\r\n}\r\nswitch (qp->r_state) {\r\ncase OP(SEND_FIRST):\r\ncase OP(SEND_MIDDLE):\r\nif (opcode == OP(SEND_MIDDLE) ||\r\nopcode == OP(SEND_LAST) ||\r\nopcode == OP(SEND_LAST_WITH_IMMEDIATE))\r\nbreak;\r\ngoto nack_inv;\r\ncase OP(RDMA_WRITE_FIRST):\r\ncase OP(RDMA_WRITE_MIDDLE):\r\nif (opcode == OP(RDMA_WRITE_MIDDLE) ||\r\nopcode == OP(RDMA_WRITE_LAST) ||\r\nopcode == OP(RDMA_WRITE_LAST_WITH_IMMEDIATE))\r\nbreak;\r\ngoto nack_inv;\r\ndefault:\r\nif (opcode == OP(SEND_MIDDLE) ||\r\nopcode == OP(SEND_LAST) ||\r\nopcode == OP(SEND_LAST_WITH_IMMEDIATE) ||\r\nopcode == OP(RDMA_WRITE_MIDDLE) ||\r\nopcode == OP(RDMA_WRITE_LAST) ||\r\nopcode == OP(RDMA_WRITE_LAST_WITH_IMMEDIATE))\r\ngoto nack_inv;\r\nbreak;\r\n}\r\nmemset(&wc, 0, sizeof wc);\r\nswitch (opcode) {\r\ncase OP(SEND_FIRST):\r\nif (!ipath_get_rwqe(qp, 0))\r\ngoto rnr_nak;\r\nqp->r_rcv_len = 0;\r\ncase OP(SEND_MIDDLE):\r\ncase OP(RDMA_WRITE_MIDDLE):\r\nsend_middle:\r\nif (unlikely(tlen != (hdrsize + pmtu + 4)))\r\ngoto nack_inv;\r\nqp->r_rcv_len += pmtu;\r\nif (unlikely(qp->r_rcv_len > qp->r_len))\r\ngoto nack_inv;\r\nipath_copy_sge(&qp->r_sge, data, pmtu);\r\nbreak;\r\ncase OP(RDMA_WRITE_LAST_WITH_IMMEDIATE):\r\nif (!ipath_get_rwqe(qp, 1))\r\ngoto rnr_nak;\r\ngoto send_last_imm;\r\ncase OP(SEND_ONLY):\r\ncase OP(SEND_ONLY_WITH_IMMEDIATE):\r\nif (!ipath_get_rwqe(qp, 0))\r\ngoto rnr_nak;\r\nqp->r_rcv_len = 0;\r\nif (opcode == OP(SEND_ONLY))\r\ngoto send_last;\r\ncase OP(SEND_LAST_WITH_IMMEDIATE):\r\nsend_last_imm:\r\nif (header_in_data) {\r\nwc.ex.imm_data = *(__be32 *) data;\r\ndata += sizeof(__be32);\r\n} else {\r\nwc.ex.imm_data = ohdr->u.imm_data;\r\n}\r\nhdrsize += 4;\r\nwc.wc_flags = IB_WC_WITH_IMM;\r\ncase OP(SEND_LAST):\r\ncase OP(RDMA_WRITE_LAST):\r\nsend_last:\r\npad = (be32_to_cpu(ohdr->bth[0]) >> 20) & 3;\r\nif (unlikely(tlen < (hdrsize + pad + 4)))\r\ngoto nack_inv;\r\ntlen -= (hdrsize + pad + 4);\r\nwc.byte_len = tlen + qp->r_rcv_len;\r\nif (unlikely(wc.byte_len > qp->r_len))\r\ngoto nack_inv;\r\nipath_copy_sge(&qp->r_sge, data, tlen);\r\nqp->r_msn++;\r\nif (!test_and_clear_bit(IPATH_R_WRID_VALID, &qp->r_aflags))\r\nbreak;\r\nwc.wr_id = qp->r_wr_id;\r\nwc.status = IB_WC_SUCCESS;\r\nif (opcode == OP(RDMA_WRITE_LAST_WITH_IMMEDIATE) ||\r\nopcode == OP(RDMA_WRITE_ONLY_WITH_IMMEDIATE))\r\nwc.opcode = IB_WC_RECV_RDMA_WITH_IMM;\r\nelse\r\nwc.opcode = IB_WC_RECV;\r\nwc.qp = &qp->ibqp;\r\nwc.src_qp = qp->remote_qpn;\r\nwc.slid = qp->remote_ah_attr.dlid;\r\nwc.sl = qp->remote_ah_attr.sl;\r\nipath_cq_enter(to_icq(qp->ibqp.recv_cq), &wc,\r\n(ohdr->bth[0] &\r\ncpu_to_be32(1 << 23)) != 0);\r\nbreak;\r\ncase OP(RDMA_WRITE_FIRST):\r\ncase OP(RDMA_WRITE_ONLY):\r\ncase OP(RDMA_WRITE_ONLY_WITH_IMMEDIATE):\r\nif (unlikely(!(qp->qp_access_flags &\r\nIB_ACCESS_REMOTE_WRITE)))\r\ngoto nack_inv;\r\nif (!header_in_data)\r\nreth = &ohdr->u.rc.reth;\r\nelse {\r\nreth = (struct ib_reth *)data;\r\ndata += sizeof(*reth);\r\n}\r\nhdrsize += sizeof(*reth);\r\nqp->r_len = be32_to_cpu(reth->length);\r\nqp->r_rcv_len = 0;\r\nif (qp->r_len != 0) {\r\nu32 rkey = be32_to_cpu(reth->rkey);\r\nu64 vaddr = be64_to_cpu(reth->vaddr);\r\nint ok;\r\nok = ipath_rkey_ok(qp, &qp->r_sge,\r\nqp->r_len, vaddr, rkey,\r\nIB_ACCESS_REMOTE_WRITE);\r\nif (unlikely(!ok))\r\ngoto nack_acc;\r\n} else {\r\nqp->r_sge.sg_list = NULL;\r\nqp->r_sge.sge.mr = NULL;\r\nqp->r_sge.sge.vaddr = NULL;\r\nqp->r_sge.sge.length = 0;\r\nqp->r_sge.sge.sge_length = 0;\r\n}\r\nif (opcode == OP(RDMA_WRITE_FIRST))\r\ngoto send_middle;\r\nelse if (opcode == OP(RDMA_WRITE_ONLY))\r\ngoto send_last;\r\nif (!ipath_get_rwqe(qp, 1))\r\ngoto rnr_nak;\r\ngoto send_last_imm;\r\ncase OP(RDMA_READ_REQUEST): {\r\nstruct ipath_ack_entry *e;\r\nu32 len;\r\nu8 next;\r\nif (unlikely(!(qp->qp_access_flags &\r\nIB_ACCESS_REMOTE_READ)))\r\ngoto nack_inv;\r\nnext = qp->r_head_ack_queue + 1;\r\nif (next > IPATH_MAX_RDMA_ATOMIC)\r\nnext = 0;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_RECV_OK))\r\ngoto unlock;\r\nif (unlikely(next == qp->s_tail_ack_queue)) {\r\nif (!qp->s_ack_queue[next].sent)\r\ngoto nack_inv_unlck;\r\nipath_update_ack_queue(qp, next);\r\n}\r\ne = &qp->s_ack_queue[qp->r_head_ack_queue];\r\nif (!header_in_data)\r\nreth = &ohdr->u.rc.reth;\r\nelse {\r\nreth = (struct ib_reth *)data;\r\ndata += sizeof(*reth);\r\n}\r\nlen = be32_to_cpu(reth->length);\r\nif (len) {\r\nu32 rkey = be32_to_cpu(reth->rkey);\r\nu64 vaddr = be64_to_cpu(reth->vaddr);\r\nint ok;\r\nok = ipath_rkey_ok(qp, &e->rdma_sge, len, vaddr,\r\nrkey, IB_ACCESS_REMOTE_READ);\r\nif (unlikely(!ok))\r\ngoto nack_acc_unlck;\r\nif (len > pmtu)\r\nqp->r_psn += (len - 1) / pmtu;\r\n} else {\r\ne->rdma_sge.sg_list = NULL;\r\ne->rdma_sge.num_sge = 0;\r\ne->rdma_sge.sge.mr = NULL;\r\ne->rdma_sge.sge.vaddr = NULL;\r\ne->rdma_sge.sge.length = 0;\r\ne->rdma_sge.sge.sge_length = 0;\r\n}\r\ne->opcode = opcode;\r\ne->sent = 0;\r\ne->psn = psn;\r\nqp->r_msn++;\r\nqp->r_psn++;\r\nqp->r_state = opcode;\r\nqp->r_nak_state = 0;\r\nqp->r_head_ack_queue = next;\r\nipath_schedule_send(qp);\r\ngoto unlock;\r\n}\r\ncase OP(COMPARE_SWAP):\r\ncase OP(FETCH_ADD): {\r\nstruct ib_atomic_eth *ateth;\r\nstruct ipath_ack_entry *e;\r\nu64 vaddr;\r\natomic64_t *maddr;\r\nu64 sdata;\r\nu32 rkey;\r\nu8 next;\r\nif (unlikely(!(qp->qp_access_flags &\r\nIB_ACCESS_REMOTE_ATOMIC)))\r\ngoto nack_inv;\r\nnext = qp->r_head_ack_queue + 1;\r\nif (next > IPATH_MAX_RDMA_ATOMIC)\r\nnext = 0;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_RECV_OK))\r\ngoto unlock;\r\nif (unlikely(next == qp->s_tail_ack_queue)) {\r\nif (!qp->s_ack_queue[next].sent)\r\ngoto nack_inv_unlck;\r\nipath_update_ack_queue(qp, next);\r\n}\r\nif (!header_in_data)\r\nateth = &ohdr->u.atomic_eth;\r\nelse\r\nateth = (struct ib_atomic_eth *)data;\r\nvaddr = ((u64) be32_to_cpu(ateth->vaddr[0]) << 32) |\r\nbe32_to_cpu(ateth->vaddr[1]);\r\nif (unlikely(vaddr & (sizeof(u64) - 1)))\r\ngoto nack_inv_unlck;\r\nrkey = be32_to_cpu(ateth->rkey);\r\nif (unlikely(!ipath_rkey_ok(qp, &qp->r_sge,\r\nsizeof(u64), vaddr, rkey,\r\nIB_ACCESS_REMOTE_ATOMIC)))\r\ngoto nack_acc_unlck;\r\nmaddr = (atomic64_t *) qp->r_sge.sge.vaddr;\r\nsdata = be64_to_cpu(ateth->swap_data);\r\ne = &qp->s_ack_queue[qp->r_head_ack_queue];\r\ne->atomic_data = (opcode == OP(FETCH_ADD)) ?\r\n(u64) atomic64_add_return(sdata, maddr) - sdata :\r\n(u64) cmpxchg((u64 *) qp->r_sge.sge.vaddr,\r\nbe64_to_cpu(ateth->compare_data),\r\nsdata);\r\ne->opcode = opcode;\r\ne->sent = 0;\r\ne->psn = psn & IPATH_PSN_MASK;\r\nqp->r_msn++;\r\nqp->r_psn++;\r\nqp->r_state = opcode;\r\nqp->r_nak_state = 0;\r\nqp->r_head_ack_queue = next;\r\nipath_schedule_send(qp);\r\ngoto unlock;\r\n}\r\ndefault:\r\ngoto nack_inv;\r\n}\r\nqp->r_psn++;\r\nqp->r_state = opcode;\r\nqp->r_ack_psn = psn;\r\nqp->r_nak_state = 0;\r\nif (psn & (1 << 31))\r\ngoto send_ack;\r\ngoto done;\r\nrnr_nak:\r\nqp->r_nak_state = IB_RNR_NAK | qp->r_min_rnr_timer;\r\nqp->r_ack_psn = qp->r_psn;\r\ngoto send_ack;\r\nnack_inv_unlck:\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nnack_inv:\r\nipath_rc_error(qp, IB_WC_LOC_QP_OP_ERR);\r\nqp->r_nak_state = IB_NAK_INVALID_REQUEST;\r\nqp->r_ack_psn = qp->r_psn;\r\ngoto send_ack;\r\nnack_acc_unlck:\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nnack_acc:\r\nipath_rc_error(qp, IB_WC_LOC_PROT_ERR);\r\nqp->r_nak_state = IB_NAK_REMOTE_ACCESS_ERROR;\r\nqp->r_ack_psn = qp->r_psn;\r\nsend_ack:\r\nsend_rc_ack(qp);\r\ngoto done;\r\nunlock:\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\ndone:\r\nreturn;\r\n}
