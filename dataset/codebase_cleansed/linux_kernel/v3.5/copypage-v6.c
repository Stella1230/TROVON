static void v6_copy_user_highpage_nonaliasing(struct page *to,\r\nstruct page *from, unsigned long vaddr, struct vm_area_struct *vma)\r\n{\r\nvoid *kto, *kfrom;\r\nkfrom = kmap_atomic(from);\r\nkto = kmap_atomic(to);\r\ncopy_page(kto, kfrom);\r\nkunmap_atomic(kto);\r\nkunmap_atomic(kfrom);\r\n}\r\nstatic void v6_clear_user_highpage_nonaliasing(struct page *page, unsigned long vaddr)\r\n{\r\nvoid *kaddr = kmap_atomic(page);\r\nclear_page(kaddr);\r\nkunmap_atomic(kaddr);\r\n}\r\nstatic void discard_old_kernel_data(void *kto)\r\n{\r\n__asm__("mcrr p15, 0, %1, %0, c6 @ 0xec401f06"\r\n:\r\n: "r" (kto),\r\n"r" ((unsigned long)kto + PAGE_SIZE - L1_CACHE_BYTES)\r\n: "cc");\r\n}\r\nstatic void v6_copy_user_highpage_aliasing(struct page *to,\r\nstruct page *from, unsigned long vaddr, struct vm_area_struct *vma)\r\n{\r\nunsigned int offset = CACHE_COLOUR(vaddr);\r\nunsigned long kfrom, kto;\r\nif (!test_and_set_bit(PG_dcache_clean, &from->flags))\r\n__flush_dcache_page(page_mapping(from), from);\r\ndiscard_old_kernel_data(page_address(to));\r\nraw_spin_lock(&v6_lock);\r\nkfrom = COPYPAGE_V6_FROM + (offset << PAGE_SHIFT);\r\nkto = COPYPAGE_V6_TO + (offset << PAGE_SHIFT);\r\nset_top_pte(kfrom, mk_pte(from, PAGE_KERNEL));\r\nset_top_pte(kto, mk_pte(to, PAGE_KERNEL));\r\ncopy_page((void *)kto, (void *)kfrom);\r\nraw_spin_unlock(&v6_lock);\r\n}\r\nstatic void v6_clear_user_highpage_aliasing(struct page *page, unsigned long vaddr)\r\n{\r\nunsigned long to = COPYPAGE_V6_TO + (CACHE_COLOUR(vaddr) << PAGE_SHIFT);\r\ndiscard_old_kernel_data(page_address(page));\r\nraw_spin_lock(&v6_lock);\r\nset_top_pte(to, mk_pte(page, PAGE_KERNEL));\r\nclear_page((void *)to);\r\nraw_spin_unlock(&v6_lock);\r\n}\r\nstatic int __init v6_userpage_init(void)\r\n{\r\nif (cache_is_vipt_aliasing()) {\r\ncpu_user.cpu_clear_user_highpage = v6_clear_user_highpage_aliasing;\r\ncpu_user.cpu_copy_user_highpage = v6_copy_user_highpage_aliasing;\r\n}\r\nreturn 0;\r\n}
