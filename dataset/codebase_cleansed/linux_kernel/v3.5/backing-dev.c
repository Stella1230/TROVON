void bdi_lock_two(struct bdi_writeback *wb1, struct bdi_writeback *wb2)\r\n{\r\nif (wb1 < wb2) {\r\nspin_lock(&wb1->list_lock);\r\nspin_lock_nested(&wb2->list_lock, 1);\r\n} else {\r\nspin_lock(&wb2->list_lock);\r\nspin_lock_nested(&wb1->list_lock, 1);\r\n}\r\n}\r\nstatic void bdi_debug_init(void)\r\n{\r\nbdi_debug_root = debugfs_create_dir("bdi", NULL);\r\n}\r\nstatic int bdi_debug_stats_show(struct seq_file *m, void *v)\r\n{\r\nstruct backing_dev_info *bdi = m->private;\r\nstruct bdi_writeback *wb = &bdi->wb;\r\nunsigned long background_thresh;\r\nunsigned long dirty_thresh;\r\nunsigned long bdi_thresh;\r\nunsigned long nr_dirty, nr_io, nr_more_io;\r\nstruct inode *inode;\r\nnr_dirty = nr_io = nr_more_io = 0;\r\nspin_lock(&wb->list_lock);\r\nlist_for_each_entry(inode, &wb->b_dirty, i_wb_list)\r\nnr_dirty++;\r\nlist_for_each_entry(inode, &wb->b_io, i_wb_list)\r\nnr_io++;\r\nlist_for_each_entry(inode, &wb->b_more_io, i_wb_list)\r\nnr_more_io++;\r\nspin_unlock(&wb->list_lock);\r\nglobal_dirty_limits(&background_thresh, &dirty_thresh);\r\nbdi_thresh = bdi_dirty_limit(bdi, dirty_thresh);\r\n#define K(x) ((x) << (PAGE_SHIFT - 10))\r\nseq_printf(m,\r\n"BdiWriteback: %10lu kB\n"\r\n"BdiReclaimable: %10lu kB\n"\r\n"BdiDirtyThresh: %10lu kB\n"\r\n"DirtyThresh: %10lu kB\n"\r\n"BackgroundThresh: %10lu kB\n"\r\n"BdiDirtied: %10lu kB\n"\r\n"BdiWritten: %10lu kB\n"\r\n"BdiWriteBandwidth: %10lu kBps\n"\r\n"b_dirty: %10lu\n"\r\n"b_io: %10lu\n"\r\n"b_more_io: %10lu\n"\r\n"bdi_list: %10u\n"\r\n"state: %10lx\n",\r\n(unsigned long) K(bdi_stat(bdi, BDI_WRITEBACK)),\r\n(unsigned long) K(bdi_stat(bdi, BDI_RECLAIMABLE)),\r\nK(bdi_thresh),\r\nK(dirty_thresh),\r\nK(background_thresh),\r\n(unsigned long) K(bdi_stat(bdi, BDI_DIRTIED)),\r\n(unsigned long) K(bdi_stat(bdi, BDI_WRITTEN)),\r\n(unsigned long) K(bdi->write_bandwidth),\r\nnr_dirty,\r\nnr_io,\r\nnr_more_io,\r\n!list_empty(&bdi->bdi_list), bdi->state);\r\n#undef K\r\nreturn 0;\r\n}\r\nstatic int bdi_debug_stats_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, bdi_debug_stats_show, inode->i_private);\r\n}\r\nstatic void bdi_debug_register(struct backing_dev_info *bdi, const char *name)\r\n{\r\nbdi->debug_dir = debugfs_create_dir(name, bdi_debug_root);\r\nbdi->debug_stats = debugfs_create_file("stats", 0444, bdi->debug_dir,\r\nbdi, &bdi_debug_stats_fops);\r\n}\r\nstatic void bdi_debug_unregister(struct backing_dev_info *bdi)\r\n{\r\ndebugfs_remove(bdi->debug_stats);\r\ndebugfs_remove(bdi->debug_dir);\r\n}\r\nstatic inline void bdi_debug_init(void)\r\n{\r\n}\r\nstatic inline void bdi_debug_register(struct backing_dev_info *bdi,\r\nconst char *name)\r\n{\r\n}\r\nstatic inline void bdi_debug_unregister(struct backing_dev_info *bdi)\r\n{\r\n}\r\nstatic ssize_t read_ahead_kb_store(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct backing_dev_info *bdi = dev_get_drvdata(dev);\r\nchar *end;\r\nunsigned long read_ahead_kb;\r\nssize_t ret = -EINVAL;\r\nread_ahead_kb = simple_strtoul(buf, &end, 10);\r\nif (*buf && (end[0] == '\0' || (end[0] == '\n' && end[1] == '\0'))) {\r\nbdi->ra_pages = read_ahead_kb >> (PAGE_SHIFT - 10);\r\nret = count;\r\n}\r\nreturn ret;\r\n}\r\nstatic ssize_t min_ratio_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t count)\r\n{\r\nstruct backing_dev_info *bdi = dev_get_drvdata(dev);\r\nchar *end;\r\nunsigned int ratio;\r\nssize_t ret = -EINVAL;\r\nratio = simple_strtoul(buf, &end, 10);\r\nif (*buf && (end[0] == '\0' || (end[0] == '\n' && end[1] == '\0'))) {\r\nret = bdi_set_min_ratio(bdi, ratio);\r\nif (!ret)\r\nret = count;\r\n}\r\nreturn ret;\r\n}\r\nstatic ssize_t max_ratio_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buf, size_t count)\r\n{\r\nstruct backing_dev_info *bdi = dev_get_drvdata(dev);\r\nchar *end;\r\nunsigned int ratio;\r\nssize_t ret = -EINVAL;\r\nratio = simple_strtoul(buf, &end, 10);\r\nif (*buf && (end[0] == '\0' || (end[0] == '\n' && end[1] == '\0'))) {\r\nret = bdi_set_max_ratio(bdi, ratio);\r\nif (!ret)\r\nret = count;\r\n}\r\nreturn ret;\r\n}\r\nstatic __init int bdi_class_init(void)\r\n{\r\nbdi_class = class_create(THIS_MODULE, "bdi");\r\nif (IS_ERR(bdi_class))\r\nreturn PTR_ERR(bdi_class);\r\nbdi_class->dev_attrs = bdi_dev_attrs;\r\nbdi_debug_init();\r\nreturn 0;\r\n}\r\nstatic int __init default_bdi_init(void)\r\n{\r\nint err;\r\nsync_supers_tsk = kthread_run(bdi_sync_supers, NULL, "sync_supers");\r\nBUG_ON(IS_ERR(sync_supers_tsk));\r\nsetup_timer(&sync_supers_timer, sync_supers_timer_fn, 0);\r\nbdi_arm_supers_timer();\r\nerr = bdi_init(&default_backing_dev_info);\r\nif (!err)\r\nbdi_register(&default_backing_dev_info, NULL, "default");\r\nerr = bdi_init(&noop_backing_dev_info);\r\nreturn err;\r\n}\r\nint bdi_has_dirty_io(struct backing_dev_info *bdi)\r\n{\r\nreturn wb_has_dirty_io(&bdi->wb);\r\n}\r\nstatic int bdi_sync_supers(void *unused)\r\n{\r\nset_user_nice(current, 0);\r\nwhile (!kthread_should_stop()) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nschedule();\r\nsync_supers();\r\n}\r\nreturn 0;\r\n}\r\nvoid bdi_arm_supers_timer(void)\r\n{\r\nunsigned long next;\r\nif (!dirty_writeback_interval)\r\nreturn;\r\nnext = msecs_to_jiffies(dirty_writeback_interval * 10) + jiffies;\r\nmod_timer(&sync_supers_timer, round_jiffies_up(next));\r\n}\r\nstatic void sync_supers_timer_fn(unsigned long unused)\r\n{\r\nwake_up_process(sync_supers_tsk);\r\nbdi_arm_supers_timer();\r\n}\r\nstatic void wakeup_timer_fn(unsigned long data)\r\n{\r\nstruct backing_dev_info *bdi = (struct backing_dev_info *)data;\r\nspin_lock_bh(&bdi->wb_lock);\r\nif (bdi->wb.task) {\r\ntrace_writeback_wake_thread(bdi);\r\nwake_up_process(bdi->wb.task);\r\n} else if (bdi->dev) {\r\ntrace_writeback_wake_forker_thread(bdi);\r\nwake_up_process(default_backing_dev_info.wb.task);\r\n}\r\nspin_unlock_bh(&bdi->wb_lock);\r\n}\r\nvoid bdi_wakeup_thread_delayed(struct backing_dev_info *bdi)\r\n{\r\nunsigned long timeout;\r\ntimeout = msecs_to_jiffies(dirty_writeback_interval * 10);\r\nmod_timer(&bdi->wb.wakeup_timer, jiffies + timeout);\r\n}\r\nstatic unsigned long bdi_longest_inactive(void)\r\n{\r\nunsigned long interval;\r\ninterval = msecs_to_jiffies(dirty_writeback_interval * 10);\r\nreturn max(5UL * 60 * HZ, interval);\r\n}\r\nstatic void bdi_clear_pending(struct backing_dev_info *bdi)\r\n{\r\nclear_bit(BDI_pending, &bdi->state);\r\nsmp_mb__after_clear_bit();\r\nwake_up_bit(&bdi->state, BDI_pending);\r\n}\r\nstatic int bdi_forker_thread(void *ptr)\r\n{\r\nstruct bdi_writeback *me = ptr;\r\ncurrent->flags |= PF_SWAPWRITE;\r\nset_freezable();\r\nset_user_nice(current, 0);\r\nfor (;;) {\r\nstruct task_struct *task = NULL;\r\nstruct backing_dev_info *bdi;\r\nenum {\r\nNO_ACTION,\r\nFORK_THREAD,\r\nKILL_THREAD,\r\n} action = NO_ACTION;\r\nif (wb_has_dirty_io(me) || !list_empty(&me->bdi->work_list)) {\r\ndel_timer(&me->wakeup_timer);\r\nwb_do_writeback(me, 0);\r\n}\r\nspin_lock_bh(&bdi_lock);\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nlist_for_each_entry(bdi, &bdi_list, bdi_list) {\r\nbool have_dirty_io;\r\nif (!bdi_cap_writeback_dirty(bdi) ||\r\nbdi_cap_flush_forker(bdi))\r\ncontinue;\r\nWARN(!test_bit(BDI_registered, &bdi->state),\r\n"bdi %p/%s is not registered!\n", bdi, bdi->name);\r\nhave_dirty_io = !list_empty(&bdi->work_list) ||\r\nwb_has_dirty_io(&bdi->wb);\r\nif (!bdi->wb.task && have_dirty_io) {\r\nset_bit(BDI_pending, &bdi->state);\r\naction = FORK_THREAD;\r\nbreak;\r\n}\r\nspin_lock(&bdi->wb_lock);\r\nif (bdi->wb.task && !have_dirty_io &&\r\ntime_after(jiffies, bdi->wb.last_active +\r\nbdi_longest_inactive())) {\r\ntask = bdi->wb.task;\r\nbdi->wb.task = NULL;\r\nspin_unlock(&bdi->wb_lock);\r\nset_bit(BDI_pending, &bdi->state);\r\naction = KILL_THREAD;\r\nbreak;\r\n}\r\nspin_unlock(&bdi->wb_lock);\r\n}\r\nspin_unlock_bh(&bdi_lock);\r\nif (!list_empty(&me->bdi->work_list))\r\n__set_current_state(TASK_RUNNING);\r\nswitch (action) {\r\ncase FORK_THREAD:\r\n__set_current_state(TASK_RUNNING);\r\ntask = kthread_create(bdi_writeback_thread, &bdi->wb,\r\n"flush-%s", dev_name(bdi->dev));\r\nif (IS_ERR(task)) {\r\nwriteback_inodes_wb(&bdi->wb, 1024,\r\nWB_REASON_FORKER_THREAD);\r\n} else {\r\nspin_lock_bh(&bdi->wb_lock);\r\nbdi->wb.task = task;\r\nspin_unlock_bh(&bdi->wb_lock);\r\nwake_up_process(task);\r\n}\r\nbdi_clear_pending(bdi);\r\nbreak;\r\ncase KILL_THREAD:\r\n__set_current_state(TASK_RUNNING);\r\nkthread_stop(task);\r\nbdi_clear_pending(bdi);\r\nbreak;\r\ncase NO_ACTION:\r\nif (!wb_has_dirty_io(me) || !dirty_writeback_interval)\r\nschedule_timeout(bdi_longest_inactive());\r\nelse\r\nschedule_timeout(msecs_to_jiffies(dirty_writeback_interval * 10));\r\ntry_to_freeze();\r\nbreak;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void bdi_remove_from_list(struct backing_dev_info *bdi)\r\n{\r\nspin_lock_bh(&bdi_lock);\r\nlist_del_rcu(&bdi->bdi_list);\r\nspin_unlock_bh(&bdi_lock);\r\nsynchronize_rcu_expedited();\r\n}\r\nint bdi_register(struct backing_dev_info *bdi, struct device *parent,\r\nconst char *fmt, ...)\r\n{\r\nva_list args;\r\nstruct device *dev;\r\nif (bdi->dev)\r\nreturn 0;\r\nva_start(args, fmt);\r\ndev = device_create_vargs(bdi_class, parent, MKDEV(0, 0), bdi, fmt, args);\r\nva_end(args);\r\nif (IS_ERR(dev))\r\nreturn PTR_ERR(dev);\r\nbdi->dev = dev;\r\nif (bdi_cap_flush_forker(bdi)) {\r\nstruct bdi_writeback *wb = &bdi->wb;\r\nwb->task = kthread_run(bdi_forker_thread, wb, "bdi-%s",\r\ndev_name(dev));\r\nif (IS_ERR(wb->task))\r\nreturn PTR_ERR(wb->task);\r\n}\r\nbdi_debug_register(bdi, dev_name(dev));\r\nset_bit(BDI_registered, &bdi->state);\r\nspin_lock_bh(&bdi_lock);\r\nlist_add_tail_rcu(&bdi->bdi_list, &bdi_list);\r\nspin_unlock_bh(&bdi_lock);\r\ntrace_writeback_bdi_register(bdi);\r\nreturn 0;\r\n}\r\nint bdi_register_dev(struct backing_dev_info *bdi, dev_t dev)\r\n{\r\nreturn bdi_register(bdi, NULL, "%u:%u", MAJOR(dev), MINOR(dev));\r\n}\r\nstatic void bdi_wb_shutdown(struct backing_dev_info *bdi)\r\n{\r\nstruct task_struct *task;\r\nif (!bdi_cap_writeback_dirty(bdi))\r\nreturn;\r\nbdi_remove_from_list(bdi);\r\nwait_on_bit(&bdi->state, BDI_pending, bdi_sched_wait,\r\nTASK_UNINTERRUPTIBLE);\r\nspin_lock_bh(&bdi->wb_lock);\r\ntask = bdi->wb.task;\r\nbdi->wb.task = NULL;\r\nspin_unlock_bh(&bdi->wb_lock);\r\nif (task)\r\nkthread_stop(task);\r\n}\r\nstatic void bdi_prune_sb(struct backing_dev_info *bdi)\r\n{\r\nstruct super_block *sb;\r\nspin_lock(&sb_lock);\r\nlist_for_each_entry(sb, &super_blocks, s_list) {\r\nif (sb->s_bdi == bdi)\r\nsb->s_bdi = &default_backing_dev_info;\r\n}\r\nspin_unlock(&sb_lock);\r\n}\r\nvoid bdi_unregister(struct backing_dev_info *bdi)\r\n{\r\nstruct device *dev = bdi->dev;\r\nif (dev) {\r\nbdi_set_min_ratio(bdi, 0);\r\ntrace_writeback_bdi_unregister(bdi);\r\nbdi_prune_sb(bdi);\r\ndel_timer_sync(&bdi->wb.wakeup_timer);\r\nif (!bdi_cap_flush_forker(bdi))\r\nbdi_wb_shutdown(bdi);\r\nbdi_debug_unregister(bdi);\r\nspin_lock_bh(&bdi->wb_lock);\r\nbdi->dev = NULL;\r\nspin_unlock_bh(&bdi->wb_lock);\r\ndevice_unregister(dev);\r\n}\r\n}\r\nstatic void bdi_wb_init(struct bdi_writeback *wb, struct backing_dev_info *bdi)\r\n{\r\nmemset(wb, 0, sizeof(*wb));\r\nwb->bdi = bdi;\r\nwb->last_old_flush = jiffies;\r\nINIT_LIST_HEAD(&wb->b_dirty);\r\nINIT_LIST_HEAD(&wb->b_io);\r\nINIT_LIST_HEAD(&wb->b_more_io);\r\nspin_lock_init(&wb->list_lock);\r\nsetup_timer(&wb->wakeup_timer, wakeup_timer_fn, (unsigned long)bdi);\r\n}\r\nint bdi_init(struct backing_dev_info *bdi)\r\n{\r\nint i, err;\r\nbdi->dev = NULL;\r\nbdi->min_ratio = 0;\r\nbdi->max_ratio = 100;\r\nbdi->max_prop_frac = PROP_FRAC_BASE;\r\nspin_lock_init(&bdi->wb_lock);\r\nINIT_LIST_HEAD(&bdi->bdi_list);\r\nINIT_LIST_HEAD(&bdi->work_list);\r\nbdi_wb_init(&bdi->wb, bdi);\r\nfor (i = 0; i < NR_BDI_STAT_ITEMS; i++) {\r\nerr = percpu_counter_init(&bdi->bdi_stat[i], 0);\r\nif (err)\r\ngoto err;\r\n}\r\nbdi->dirty_exceeded = 0;\r\nbdi->bw_time_stamp = jiffies;\r\nbdi->written_stamp = 0;\r\nbdi->balanced_dirty_ratelimit = INIT_BW;\r\nbdi->dirty_ratelimit = INIT_BW;\r\nbdi->write_bandwidth = INIT_BW;\r\nbdi->avg_write_bandwidth = INIT_BW;\r\nerr = prop_local_init_percpu(&bdi->completions);\r\nif (err) {\r\nerr:\r\nwhile (i--)\r\npercpu_counter_destroy(&bdi->bdi_stat[i]);\r\n}\r\nreturn err;\r\n}\r\nvoid bdi_destroy(struct backing_dev_info *bdi)\r\n{\r\nint i;\r\nif (bdi_has_dirty_io(bdi)) {\r\nstruct bdi_writeback *dst = &default_backing_dev_info.wb;\r\nbdi_lock_two(&bdi->wb, dst);\r\nlist_splice(&bdi->wb.b_dirty, &dst->b_dirty);\r\nlist_splice(&bdi->wb.b_io, &dst->b_io);\r\nlist_splice(&bdi->wb.b_more_io, &dst->b_more_io);\r\nspin_unlock(&bdi->wb.list_lock);\r\nspin_unlock(&dst->list_lock);\r\n}\r\nbdi_unregister(bdi);\r\ndel_timer_sync(&bdi->wb.wakeup_timer);\r\nfor (i = 0; i < NR_BDI_STAT_ITEMS; i++)\r\npercpu_counter_destroy(&bdi->bdi_stat[i]);\r\nprop_local_destroy_percpu(&bdi->completions);\r\n}\r\nint bdi_setup_and_register(struct backing_dev_info *bdi, char *name,\r\nunsigned int cap)\r\n{\r\nchar tmp[32];\r\nint err;\r\nbdi->name = name;\r\nbdi->capabilities = cap;\r\nerr = bdi_init(bdi);\r\nif (err)\r\nreturn err;\r\nsprintf(tmp, "%.28s%s", name, "-%d");\r\nerr = bdi_register(bdi, NULL, tmp, atomic_long_inc_return(&bdi_seq));\r\nif (err) {\r\nbdi_destroy(bdi);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nvoid clear_bdi_congested(struct backing_dev_info *bdi, int sync)\r\n{\r\nenum bdi_state bit;\r\nwait_queue_head_t *wqh = &congestion_wqh[sync];\r\nbit = sync ? BDI_sync_congested : BDI_async_congested;\r\nif (test_and_clear_bit(bit, &bdi->state))\r\natomic_dec(&nr_bdi_congested[sync]);\r\nsmp_mb__after_clear_bit();\r\nif (waitqueue_active(wqh))\r\nwake_up(wqh);\r\n}\r\nvoid set_bdi_congested(struct backing_dev_info *bdi, int sync)\r\n{\r\nenum bdi_state bit;\r\nbit = sync ? BDI_sync_congested : BDI_async_congested;\r\nif (!test_and_set_bit(bit, &bdi->state))\r\natomic_inc(&nr_bdi_congested[sync]);\r\n}\r\nlong congestion_wait(int sync, long timeout)\r\n{\r\nlong ret;\r\nunsigned long start = jiffies;\r\nDEFINE_WAIT(wait);\r\nwait_queue_head_t *wqh = &congestion_wqh[sync];\r\nprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);\r\nret = io_schedule_timeout(timeout);\r\nfinish_wait(wqh, &wait);\r\ntrace_writeback_congestion_wait(jiffies_to_usecs(timeout),\r\njiffies_to_usecs(jiffies - start));\r\nreturn ret;\r\n}\r\nlong wait_iff_congested(struct zone *zone, int sync, long timeout)\r\n{\r\nlong ret;\r\nunsigned long start = jiffies;\r\nDEFINE_WAIT(wait);\r\nwait_queue_head_t *wqh = &congestion_wqh[sync];\r\nif (atomic_read(&nr_bdi_congested[sync]) == 0 ||\r\n!zone_is_reclaim_congested(zone)) {\r\ncond_resched();\r\nret = timeout - (jiffies - start);\r\nif (ret < 0)\r\nret = 0;\r\ngoto out;\r\n}\r\nprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);\r\nret = io_schedule_timeout(timeout);\r\nfinish_wait(wqh, &wait);\r\nout:\r\ntrace_writeback_wait_iff_congested(jiffies_to_usecs(timeout),\r\njiffies_to_usecs(jiffies - start));\r\nreturn ret;\r\n}
