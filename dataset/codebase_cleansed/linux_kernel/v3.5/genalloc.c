static int set_bits_ll(unsigned long *addr, unsigned long mask_to_set)\r\n{\r\nunsigned long val, nval;\r\nnval = *addr;\r\ndo {\r\nval = nval;\r\nif (val & mask_to_set)\r\nreturn -EBUSY;\r\ncpu_relax();\r\n} while ((nval = cmpxchg(addr, val, val | mask_to_set)) != val);\r\nreturn 0;\r\n}\r\nstatic int clear_bits_ll(unsigned long *addr, unsigned long mask_to_clear)\r\n{\r\nunsigned long val, nval;\r\nnval = *addr;\r\ndo {\r\nval = nval;\r\nif ((val & mask_to_clear) != mask_to_clear)\r\nreturn -EBUSY;\r\ncpu_relax();\r\n} while ((nval = cmpxchg(addr, val, val & ~mask_to_clear)) != val);\r\nreturn 0;\r\n}\r\nstatic int bitmap_set_ll(unsigned long *map, int start, int nr)\r\n{\r\nunsigned long *p = map + BIT_WORD(start);\r\nconst int size = start + nr;\r\nint bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);\r\nunsigned long mask_to_set = BITMAP_FIRST_WORD_MASK(start);\r\nwhile (nr - bits_to_set >= 0) {\r\nif (set_bits_ll(p, mask_to_set))\r\nreturn nr;\r\nnr -= bits_to_set;\r\nbits_to_set = BITS_PER_LONG;\r\nmask_to_set = ~0UL;\r\np++;\r\n}\r\nif (nr) {\r\nmask_to_set &= BITMAP_LAST_WORD_MASK(size);\r\nif (set_bits_ll(p, mask_to_set))\r\nreturn nr;\r\n}\r\nreturn 0;\r\n}\r\nstatic int bitmap_clear_ll(unsigned long *map, int start, int nr)\r\n{\r\nunsigned long *p = map + BIT_WORD(start);\r\nconst int size = start + nr;\r\nint bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);\r\nunsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);\r\nwhile (nr - bits_to_clear >= 0) {\r\nif (clear_bits_ll(p, mask_to_clear))\r\nreturn nr;\r\nnr -= bits_to_clear;\r\nbits_to_clear = BITS_PER_LONG;\r\nmask_to_clear = ~0UL;\r\np++;\r\n}\r\nif (nr) {\r\nmask_to_clear &= BITMAP_LAST_WORD_MASK(size);\r\nif (clear_bits_ll(p, mask_to_clear))\r\nreturn nr;\r\n}\r\nreturn 0;\r\n}\r\nstruct gen_pool *gen_pool_create(int min_alloc_order, int nid)\r\n{\r\nstruct gen_pool *pool;\r\npool = kmalloc_node(sizeof(struct gen_pool), GFP_KERNEL, nid);\r\nif (pool != NULL) {\r\nspin_lock_init(&pool->lock);\r\nINIT_LIST_HEAD(&pool->chunks);\r\npool->min_alloc_order = min_alloc_order;\r\n}\r\nreturn pool;\r\n}\r\nint gen_pool_add_virt(struct gen_pool *pool, unsigned long virt, phys_addr_t phys,\r\nsize_t size, int nid)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nint nbits = size >> pool->min_alloc_order;\r\nint nbytes = sizeof(struct gen_pool_chunk) +\r\n(nbits + BITS_PER_BYTE - 1) / BITS_PER_BYTE;\r\nchunk = kmalloc_node(nbytes, GFP_KERNEL | __GFP_ZERO, nid);\r\nif (unlikely(chunk == NULL))\r\nreturn -ENOMEM;\r\nchunk->phys_addr = phys;\r\nchunk->start_addr = virt;\r\nchunk->end_addr = virt + size;\r\natomic_set(&chunk->avail, size);\r\nspin_lock(&pool->lock);\r\nlist_add_rcu(&chunk->next_chunk, &pool->chunks);\r\nspin_unlock(&pool->lock);\r\nreturn 0;\r\n}\r\nphys_addr_t gen_pool_virt_to_phys(struct gen_pool *pool, unsigned long addr)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nphys_addr_t paddr = -1;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {\r\nif (addr >= chunk->start_addr && addr < chunk->end_addr) {\r\npaddr = chunk->phys_addr + (addr - chunk->start_addr);\r\nbreak;\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn paddr;\r\n}\r\nvoid gen_pool_destroy(struct gen_pool *pool)\r\n{\r\nstruct list_head *_chunk, *_next_chunk;\r\nstruct gen_pool_chunk *chunk;\r\nint order = pool->min_alloc_order;\r\nint bit, end_bit;\r\nlist_for_each_safe(_chunk, _next_chunk, &pool->chunks) {\r\nchunk = list_entry(_chunk, struct gen_pool_chunk, next_chunk);\r\nlist_del(&chunk->next_chunk);\r\nend_bit = (chunk->end_addr - chunk->start_addr) >> order;\r\nbit = find_next_bit(chunk->bits, end_bit, 0);\r\nBUG_ON(bit < end_bit);\r\nkfree(chunk);\r\n}\r\nkfree(pool);\r\nreturn;\r\n}\r\nunsigned long gen_pool_alloc(struct gen_pool *pool, size_t size)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nunsigned long addr = 0;\r\nint order = pool->min_alloc_order;\r\nint nbits, start_bit = 0, end_bit, remain;\r\n#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG\r\nBUG_ON(in_nmi());\r\n#endif\r\nif (size == 0)\r\nreturn 0;\r\nnbits = (size + (1UL << order) - 1) >> order;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {\r\nif (size > atomic_read(&chunk->avail))\r\ncontinue;\r\nend_bit = (chunk->end_addr - chunk->start_addr) >> order;\r\nretry:\r\nstart_bit = bitmap_find_next_zero_area(chunk->bits, end_bit,\r\nstart_bit, nbits, 0);\r\nif (start_bit >= end_bit)\r\ncontinue;\r\nremain = bitmap_set_ll(chunk->bits, start_bit, nbits);\r\nif (remain) {\r\nremain = bitmap_clear_ll(chunk->bits, start_bit,\r\nnbits - remain);\r\nBUG_ON(remain);\r\ngoto retry;\r\n}\r\naddr = chunk->start_addr + ((unsigned long)start_bit << order);\r\nsize = nbits << order;\r\natomic_sub(size, &chunk->avail);\r\nbreak;\r\n}\r\nrcu_read_unlock();\r\nreturn addr;\r\n}\r\nvoid gen_pool_free(struct gen_pool *pool, unsigned long addr, size_t size)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nint order = pool->min_alloc_order;\r\nint start_bit, nbits, remain;\r\n#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG\r\nBUG_ON(in_nmi());\r\n#endif\r\nnbits = (size + (1UL << order) - 1) >> order;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk) {\r\nif (addr >= chunk->start_addr && addr < chunk->end_addr) {\r\nBUG_ON(addr + size > chunk->end_addr);\r\nstart_bit = (addr - chunk->start_addr) >> order;\r\nremain = bitmap_clear_ll(chunk->bits, start_bit, nbits);\r\nBUG_ON(remain);\r\nsize = nbits << order;\r\natomic_add(size, &chunk->avail);\r\nrcu_read_unlock();\r\nreturn;\r\n}\r\n}\r\nrcu_read_unlock();\r\nBUG();\r\n}\r\nvoid gen_pool_for_each_chunk(struct gen_pool *pool,\r\nvoid (*func)(struct gen_pool *pool, struct gen_pool_chunk *chunk, void *data),\r\nvoid *data)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &(pool)->chunks, next_chunk)\r\nfunc(pool, chunk, data);\r\nrcu_read_unlock();\r\n}\r\nsize_t gen_pool_avail(struct gen_pool *pool)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nsize_t avail = 0;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk)\r\navail += atomic_read(&chunk->avail);\r\nrcu_read_unlock();\r\nreturn avail;\r\n}\r\nsize_t gen_pool_size(struct gen_pool *pool)\r\n{\r\nstruct gen_pool_chunk *chunk;\r\nsize_t size = 0;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(chunk, &pool->chunks, next_chunk)\r\nsize += chunk->end_addr - chunk->start_addr;\r\nrcu_read_unlock();\r\nreturn size;\r\n}
