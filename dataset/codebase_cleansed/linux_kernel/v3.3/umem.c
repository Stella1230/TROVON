static void __ib_umem_release(struct ib_device *dev, struct ib_umem *umem, int dirty)\r\n{\r\nstruct ib_umem_chunk *chunk, *tmp;\r\nint i;\r\nlist_for_each_entry_safe(chunk, tmp, &umem->chunk_list, list) {\r\nib_dma_unmap_sg(dev, chunk->page_list,\r\nchunk->nents, DMA_BIDIRECTIONAL);\r\nfor (i = 0; i < chunk->nents; ++i) {\r\nstruct page *page = sg_page(&chunk->page_list[i]);\r\nif (umem->writable && dirty)\r\nset_page_dirty_lock(page);\r\nput_page(page);\r\n}\r\nkfree(chunk);\r\n}\r\n}\r\nstruct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,\r\nsize_t size, int access, int dmasync)\r\n{\r\nstruct ib_umem *umem;\r\nstruct page **page_list;\r\nstruct vm_area_struct **vma_list;\r\nstruct ib_umem_chunk *chunk;\r\nunsigned long locked;\r\nunsigned long lock_limit;\r\nunsigned long cur_base;\r\nunsigned long npages;\r\nint ret;\r\nint off;\r\nint i;\r\nDEFINE_DMA_ATTRS(attrs);\r\nif (dmasync)\r\ndma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);\r\nif (!can_do_mlock())\r\nreturn ERR_PTR(-EPERM);\r\numem = kmalloc(sizeof *umem, GFP_KERNEL);\r\nif (!umem)\r\nreturn ERR_PTR(-ENOMEM);\r\numem->context = context;\r\numem->length = size;\r\numem->offset = addr & ~PAGE_MASK;\r\numem->page_size = PAGE_SIZE;\r\numem->writable = !!(access & ~IB_ACCESS_REMOTE_READ);\r\numem->hugetlb = 1;\r\nINIT_LIST_HEAD(&umem->chunk_list);\r\npage_list = (struct page **) __get_free_page(GFP_KERNEL);\r\nif (!page_list) {\r\nkfree(umem);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nvma_list = (struct vm_area_struct **) __get_free_page(GFP_KERNEL);\r\nif (!vma_list)\r\numem->hugetlb = 0;\r\nnpages = PAGE_ALIGN(size + umem->offset) >> PAGE_SHIFT;\r\ndown_write(&current->mm->mmap_sem);\r\nlocked = npages + current->mm->pinned_vm;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\r\nif ((locked > lock_limit) && !capable(CAP_IPC_LOCK)) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ncur_base = addr & PAGE_MASK;\r\nret = 0;\r\nwhile (npages) {\r\nret = get_user_pages(current, current->mm, cur_base,\r\nmin_t(unsigned long, npages,\r\nPAGE_SIZE / sizeof (struct page *)),\r\n1, !umem->writable, page_list, vma_list);\r\nif (ret < 0)\r\ngoto out;\r\ncur_base += ret * PAGE_SIZE;\r\nnpages -= ret;\r\noff = 0;\r\nwhile (ret) {\r\nchunk = kmalloc(sizeof *chunk + sizeof (struct scatterlist) *\r\nmin_t(int, ret, IB_UMEM_MAX_PAGE_CHUNK),\r\nGFP_KERNEL);\r\nif (!chunk) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nchunk->nents = min_t(int, ret, IB_UMEM_MAX_PAGE_CHUNK);\r\nsg_init_table(chunk->page_list, chunk->nents);\r\nfor (i = 0; i < chunk->nents; ++i) {\r\nif (vma_list &&\r\n!is_vm_hugetlb_page(vma_list[i + off]))\r\numem->hugetlb = 0;\r\nsg_set_page(&chunk->page_list[i], page_list[i + off], PAGE_SIZE, 0);\r\n}\r\nchunk->nmap = ib_dma_map_sg_attrs(context->device,\r\n&chunk->page_list[0],\r\nchunk->nents,\r\nDMA_BIDIRECTIONAL,\r\n&attrs);\r\nif (chunk->nmap <= 0) {\r\nfor (i = 0; i < chunk->nents; ++i)\r\nput_page(sg_page(&chunk->page_list[i]));\r\nkfree(chunk);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret -= chunk->nents;\r\noff += chunk->nents;\r\nlist_add_tail(&chunk->list, &umem->chunk_list);\r\n}\r\nret = 0;\r\n}\r\nout:\r\nif (ret < 0) {\r\n__ib_umem_release(context->device, umem, 0);\r\nkfree(umem);\r\n} else\r\ncurrent->mm->pinned_vm = locked;\r\nup_write(&current->mm->mmap_sem);\r\nif (vma_list)\r\nfree_page((unsigned long) vma_list);\r\nfree_page((unsigned long) page_list);\r\nreturn ret < 0 ? ERR_PTR(ret) : umem;\r\n}\r\nstatic void ib_umem_account(struct work_struct *work)\r\n{\r\nstruct ib_umem *umem = container_of(work, struct ib_umem, work);\r\ndown_write(&umem->mm->mmap_sem);\r\numem->mm->pinned_vm -= umem->diff;\r\nup_write(&umem->mm->mmap_sem);\r\nmmput(umem->mm);\r\nkfree(umem);\r\n}\r\nvoid ib_umem_release(struct ib_umem *umem)\r\n{\r\nstruct ib_ucontext *context = umem->context;\r\nstruct mm_struct *mm;\r\nunsigned long diff;\r\n__ib_umem_release(umem->context->device, umem, 1);\r\nmm = get_task_mm(current);\r\nif (!mm) {\r\nkfree(umem);\r\nreturn;\r\n}\r\ndiff = PAGE_ALIGN(umem->length + umem->offset) >> PAGE_SHIFT;\r\nif (context->closing) {\r\nif (!down_write_trylock(&mm->mmap_sem)) {\r\nINIT_WORK(&umem->work, ib_umem_account);\r\numem->mm = mm;\r\numem->diff = diff;\r\nqueue_work(ib_wq, &umem->work);\r\nreturn;\r\n}\r\n} else\r\ndown_write(&mm->mmap_sem);\r\ncurrent->mm->locked_vm -= diff;\r\nup_write(&mm->mmap_sem);\r\nmmput(mm);\r\nkfree(umem);\r\n}\r\nint ib_umem_page_count(struct ib_umem *umem)\r\n{\r\nstruct ib_umem_chunk *chunk;\r\nint shift;\r\nint i;\r\nint n;\r\nshift = ilog2(umem->page_size);\r\nn = 0;\r\nlist_for_each_entry(chunk, &umem->chunk_list, list)\r\nfor (i = 0; i < chunk->nmap; ++i)\r\nn += sg_dma_len(&chunk->page_list[i]) >> shift;\r\nreturn n;\r\n}
