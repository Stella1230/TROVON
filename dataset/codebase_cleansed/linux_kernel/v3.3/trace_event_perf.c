static int perf_trace_event_perm(struct ftrace_event_call *tp_event,\r\nstruct perf_event *p_event)\r\n{\r\nif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\r\nreturn 0;\r\nif (p_event->attach_state == PERF_ATTACH_TASK) {\r\nif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\r\nreturn 0;\r\n}\r\nif (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nreturn 0;\r\n}\r\nstatic int perf_trace_event_init(struct ftrace_event_call *tp_event,\r\nstruct perf_event *p_event)\r\n{\r\nstruct hlist_head __percpu *list;\r\nint ret;\r\nint cpu;\r\nret = perf_trace_event_perm(tp_event, p_event);\r\nif (ret)\r\nreturn ret;\r\np_event->tp_event = tp_event;\r\nif (tp_event->perf_refcount++ > 0)\r\nreturn 0;\r\nret = -ENOMEM;\r\nlist = alloc_percpu(struct hlist_head);\r\nif (!list)\r\ngoto fail;\r\nfor_each_possible_cpu(cpu)\r\nINIT_HLIST_HEAD(per_cpu_ptr(list, cpu));\r\ntp_event->perf_events = list;\r\nif (!total_ref_count) {\r\nchar __percpu *buf;\r\nint i;\r\nfor (i = 0; i < PERF_NR_CONTEXTS; i++) {\r\nbuf = (char __percpu *)alloc_percpu(perf_trace_t);\r\nif (!buf)\r\ngoto fail;\r\nperf_trace_buf[i] = buf;\r\n}\r\n}\r\nret = tp_event->class->reg(tp_event, TRACE_REG_PERF_REGISTER);\r\nif (ret)\r\ngoto fail;\r\ntotal_ref_count++;\r\nreturn 0;\r\nfail:\r\nif (!total_ref_count) {\r\nint i;\r\nfor (i = 0; i < PERF_NR_CONTEXTS; i++) {\r\nfree_percpu(perf_trace_buf[i]);\r\nperf_trace_buf[i] = NULL;\r\n}\r\n}\r\nif (!--tp_event->perf_refcount) {\r\nfree_percpu(tp_event->perf_events);\r\ntp_event->perf_events = NULL;\r\n}\r\nreturn ret;\r\n}\r\nint perf_trace_init(struct perf_event *p_event)\r\n{\r\nstruct ftrace_event_call *tp_event;\r\nint event_id = p_event->attr.config;\r\nint ret = -EINVAL;\r\nmutex_lock(&event_mutex);\r\nlist_for_each_entry(tp_event, &ftrace_events, list) {\r\nif (tp_event->event.type == event_id &&\r\ntp_event->class && tp_event->class->reg &&\r\ntry_module_get(tp_event->mod)) {\r\nret = perf_trace_event_init(tp_event, p_event);\r\nif (ret)\r\nmodule_put(tp_event->mod);\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&event_mutex);\r\nreturn ret;\r\n}\r\nint perf_trace_add(struct perf_event *p_event, int flags)\r\n{\r\nstruct ftrace_event_call *tp_event = p_event->tp_event;\r\nstruct hlist_head __percpu *pcpu_list;\r\nstruct hlist_head *list;\r\npcpu_list = tp_event->perf_events;\r\nif (WARN_ON_ONCE(!pcpu_list))\r\nreturn -EINVAL;\r\nif (!(flags & PERF_EF_START))\r\np_event->hw.state = PERF_HES_STOPPED;\r\nlist = this_cpu_ptr(pcpu_list);\r\nhlist_add_head_rcu(&p_event->hlist_entry, list);\r\nreturn 0;\r\n}\r\nvoid perf_trace_del(struct perf_event *p_event, int flags)\r\n{\r\nhlist_del_rcu(&p_event->hlist_entry);\r\n}\r\nvoid perf_trace_destroy(struct perf_event *p_event)\r\n{\r\nstruct ftrace_event_call *tp_event = p_event->tp_event;\r\nint i;\r\nmutex_lock(&event_mutex);\r\nif (--tp_event->perf_refcount > 0)\r\ngoto out;\r\ntp_event->class->reg(tp_event, TRACE_REG_PERF_UNREGISTER);\r\ntracepoint_synchronize_unregister();\r\nfree_percpu(tp_event->perf_events);\r\ntp_event->perf_events = NULL;\r\nif (!--total_ref_count) {\r\nfor (i = 0; i < PERF_NR_CONTEXTS; i++) {\r\nfree_percpu(perf_trace_buf[i]);\r\nperf_trace_buf[i] = NULL;\r\n}\r\n}\r\nout:\r\nmodule_put(tp_event->mod);\r\nmutex_unlock(&event_mutex);\r\n}\r\n__kprobes void *perf_trace_buf_prepare(int size, unsigned short type,\r\nstruct pt_regs *regs, int *rctxp)\r\n{\r\nstruct trace_entry *entry;\r\nunsigned long flags;\r\nchar *raw_data;\r\nint pc;\r\nBUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(unsigned long));\r\npc = preempt_count();\r\n*rctxp = perf_swevent_get_recursion_context();\r\nif (*rctxp < 0)\r\nreturn NULL;\r\nraw_data = this_cpu_ptr(perf_trace_buf[*rctxp]);\r\nmemset(&raw_data[size - sizeof(u64)], 0, sizeof(u64));\r\nentry = (struct trace_entry *)raw_data;\r\nlocal_save_flags(flags);\r\ntracing_generic_entry_update(entry, flags, pc);\r\nentry->type = type;\r\nreturn raw_data;\r\n}
