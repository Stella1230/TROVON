static inline struct nouveau_fence *\r\nnouveau_fence(void *sync_obj)\r\n{\r\nreturn (struct nouveau_fence *)sync_obj;\r\n}\r\nstatic void\r\nnouveau_fence_del(struct kref *ref)\r\n{\r\nstruct nouveau_fence *fence =\r\ncontainer_of(ref, struct nouveau_fence, refcount);\r\nnouveau_channel_ref(NULL, &fence->channel);\r\nkfree(fence);\r\n}\r\nvoid\r\nnouveau_fence_update(struct nouveau_channel *chan)\r\n{\r\nstruct drm_device *dev = chan->dev;\r\nstruct nouveau_fence *tmp, *fence;\r\nuint32_t sequence;\r\nspin_lock(&chan->fence.lock);\r\nif (likely(!list_empty(&chan->fence.pending))) {\r\nif (USE_REFCNT(dev))\r\nsequence = nvchan_rd32(chan, 0x48);\r\nelse\r\nsequence = atomic_read(&chan->fence.last_sequence_irq);\r\nif (chan->fence.sequence_ack == sequence)\r\ngoto out;\r\nchan->fence.sequence_ack = sequence;\r\n}\r\nlist_for_each_entry_safe(fence, tmp, &chan->fence.pending, entry) {\r\nsequence = fence->sequence;\r\nfence->signalled = true;\r\nlist_del(&fence->entry);\r\nif (unlikely(fence->work))\r\nfence->work(fence->priv, true);\r\nkref_put(&fence->refcount, nouveau_fence_del);\r\nif (sequence == chan->fence.sequence_ack)\r\nbreak;\r\n}\r\nout:\r\nspin_unlock(&chan->fence.lock);\r\n}\r\nint\r\nnouveau_fence_new(struct nouveau_channel *chan, struct nouveau_fence **pfence,\r\nbool emit)\r\n{\r\nstruct nouveau_fence *fence;\r\nint ret = 0;\r\nfence = kzalloc(sizeof(*fence), GFP_KERNEL);\r\nif (!fence)\r\nreturn -ENOMEM;\r\nkref_init(&fence->refcount);\r\nnouveau_channel_ref(chan, &fence->channel);\r\nif (emit)\r\nret = nouveau_fence_emit(fence);\r\nif (ret)\r\nnouveau_fence_unref(&fence);\r\n*pfence = fence;\r\nreturn ret;\r\n}\r\nstruct nouveau_channel *\r\nnouveau_fence_channel(struct nouveau_fence *fence)\r\n{\r\nreturn fence ? nouveau_channel_get_unlocked(fence->channel) : NULL;\r\n}\r\nint\r\nnouveau_fence_emit(struct nouveau_fence *fence)\r\n{\r\nstruct nouveau_channel *chan = fence->channel;\r\nstruct drm_device *dev = chan->dev;\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nint ret;\r\nret = RING_SPACE(chan, 2);\r\nif (ret)\r\nreturn ret;\r\nif (unlikely(chan->fence.sequence == chan->fence.sequence_ack - 1)) {\r\nnouveau_fence_update(chan);\r\nBUG_ON(chan->fence.sequence ==\r\nchan->fence.sequence_ack - 1);\r\n}\r\nfence->sequence = ++chan->fence.sequence;\r\nkref_get(&fence->refcount);\r\nspin_lock(&chan->fence.lock);\r\nlist_add_tail(&fence->entry, &chan->fence.pending);\r\nspin_unlock(&chan->fence.lock);\r\nif (USE_REFCNT(dev)) {\r\nif (dev_priv->card_type < NV_C0)\r\nBEGIN_RING(chan, NvSubSw, 0x0050, 1);\r\nelse\r\nBEGIN_NVC0(chan, 2, NvSubM2MF, 0x0050, 1);\r\n} else {\r\nBEGIN_RING(chan, NvSubSw, 0x0150, 1);\r\n}\r\nOUT_RING (chan, fence->sequence);\r\nFIRE_RING(chan);\r\nreturn 0;\r\n}\r\nvoid\r\nnouveau_fence_work(struct nouveau_fence *fence,\r\nvoid (*work)(void *priv, bool signalled),\r\nvoid *priv)\r\n{\r\nBUG_ON(fence->work);\r\nspin_lock(&fence->channel->fence.lock);\r\nif (fence->signalled) {\r\nwork(priv, true);\r\n} else {\r\nfence->work = work;\r\nfence->priv = priv;\r\n}\r\nspin_unlock(&fence->channel->fence.lock);\r\n}\r\nvoid\r\n__nouveau_fence_unref(void **sync_obj)\r\n{\r\nstruct nouveau_fence *fence = nouveau_fence(*sync_obj);\r\nif (fence)\r\nkref_put(&fence->refcount, nouveau_fence_del);\r\n*sync_obj = NULL;\r\n}\r\nvoid *\r\n__nouveau_fence_ref(void *sync_obj)\r\n{\r\nstruct nouveau_fence *fence = nouveau_fence(sync_obj);\r\nkref_get(&fence->refcount);\r\nreturn sync_obj;\r\n}\r\nbool\r\n__nouveau_fence_signalled(void *sync_obj, void *sync_arg)\r\n{\r\nstruct nouveau_fence *fence = nouveau_fence(sync_obj);\r\nstruct nouveau_channel *chan = fence->channel;\r\nif (fence->signalled)\r\nreturn true;\r\nnouveau_fence_update(chan);\r\nreturn fence->signalled;\r\n}\r\nint\r\n__nouveau_fence_wait(void *sync_obj, void *sync_arg, bool lazy, bool intr)\r\n{\r\nunsigned long timeout = jiffies + (3 * DRM_HZ);\r\nunsigned long sleep_time = NSEC_PER_MSEC / 1000;\r\nktime_t t;\r\nint ret = 0;\r\nwhile (1) {\r\nif (__nouveau_fence_signalled(sync_obj, sync_arg))\r\nbreak;\r\nif (time_after_eq(jiffies, timeout)) {\r\nret = -EBUSY;\r\nbreak;\r\n}\r\n__set_current_state(intr ? TASK_INTERRUPTIBLE\r\n: TASK_UNINTERRUPTIBLE);\r\nif (lazy) {\r\nt = ktime_set(0, sleep_time);\r\nschedule_hrtimeout(&t, HRTIMER_MODE_REL);\r\nsleep_time *= 2;\r\nif (sleep_time > NSEC_PER_MSEC)\r\nsleep_time = NSEC_PER_MSEC;\r\n}\r\nif (intr && signal_pending(current)) {\r\nret = -ERESTARTSYS;\r\nbreak;\r\n}\r\n}\r\n__set_current_state(TASK_RUNNING);\r\nreturn ret;\r\n}\r\nstatic struct nouveau_semaphore *\r\nsemaphore_alloc(struct drm_device *dev)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_semaphore *sema;\r\nint size = (dev_priv->chipset < 0x84) ? 4 : 16;\r\nint ret, i;\r\nif (!USE_SEMA(dev))\r\nreturn NULL;\r\nsema = kmalloc(sizeof(*sema), GFP_KERNEL);\r\nif (!sema)\r\ngoto fail;\r\nret = drm_mm_pre_get(&dev_priv->fence.heap);\r\nif (ret)\r\ngoto fail;\r\nspin_lock(&dev_priv->fence.lock);\r\nsema->mem = drm_mm_search_free(&dev_priv->fence.heap, size, 0, 0);\r\nif (sema->mem)\r\nsema->mem = drm_mm_get_block_atomic(sema->mem, size, 0);\r\nspin_unlock(&dev_priv->fence.lock);\r\nif (!sema->mem)\r\ngoto fail;\r\nkref_init(&sema->ref);\r\nsema->dev = dev;\r\nfor (i = sema->mem->start; i < sema->mem->start + size; i += 4)\r\nnouveau_bo_wr32(dev_priv->fence.bo, i / 4, 0);\r\nreturn sema;\r\nfail:\r\nkfree(sema);\r\nreturn NULL;\r\n}\r\nstatic void\r\nsemaphore_free(struct kref *ref)\r\n{\r\nstruct nouveau_semaphore *sema =\r\ncontainer_of(ref, struct nouveau_semaphore, ref);\r\nstruct drm_nouveau_private *dev_priv = sema->dev->dev_private;\r\nspin_lock(&dev_priv->fence.lock);\r\ndrm_mm_put_block(sema->mem);\r\nspin_unlock(&dev_priv->fence.lock);\r\nkfree(sema);\r\n}\r\nstatic void\r\nsemaphore_work(void *priv, bool signalled)\r\n{\r\nstruct nouveau_semaphore *sema = priv;\r\nstruct drm_nouveau_private *dev_priv = sema->dev->dev_private;\r\nif (unlikely(!signalled))\r\nnouveau_bo_wr32(dev_priv->fence.bo, sema->mem->start / 4, 1);\r\nkref_put(&sema->ref, semaphore_free);\r\n}\r\nstatic int\r\nsemaphore_acquire(struct nouveau_channel *chan, struct nouveau_semaphore *sema)\r\n{\r\nstruct drm_nouveau_private *dev_priv = chan->dev->dev_private;\r\nstruct nouveau_fence *fence = NULL;\r\nu64 offset = chan->fence.vma.offset + sema->mem->start;\r\nint ret;\r\nif (dev_priv->chipset < 0x84) {\r\nret = RING_SPACE(chan, 4);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_RING(chan, NvSubSw, NV_SW_DMA_SEMAPHORE, 3);\r\nOUT_RING (chan, NvSema);\r\nOUT_RING (chan, offset);\r\nOUT_RING (chan, 1);\r\n} else\r\nif (dev_priv->chipset < 0xc0) {\r\nret = RING_SPACE(chan, 7);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_RING(chan, NvSubSw, NV_SW_DMA_SEMAPHORE, 1);\r\nOUT_RING (chan, chan->vram_handle);\r\nBEGIN_RING(chan, NvSubSw, 0x0010, 4);\r\nOUT_RING (chan, upper_32_bits(offset));\r\nOUT_RING (chan, lower_32_bits(offset));\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 1);\r\n} else {\r\nret = RING_SPACE(chan, 5);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NVC0(chan, 2, NvSubM2MF, 0x0010, 4);\r\nOUT_RING (chan, upper_32_bits(offset));\r\nOUT_RING (chan, lower_32_bits(offset));\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 0x1001);\r\n}\r\nret = nouveau_fence_new(chan, &fence, true);\r\nif (ret)\r\nreturn ret;\r\nkref_get(&sema->ref);\r\nnouveau_fence_work(fence, semaphore_work, sema);\r\nnouveau_fence_unref(&fence);\r\nreturn 0;\r\n}\r\nstatic int\r\nsemaphore_release(struct nouveau_channel *chan, struct nouveau_semaphore *sema)\r\n{\r\nstruct drm_nouveau_private *dev_priv = chan->dev->dev_private;\r\nstruct nouveau_fence *fence = NULL;\r\nu64 offset = chan->fence.vma.offset + sema->mem->start;\r\nint ret;\r\nif (dev_priv->chipset < 0x84) {\r\nret = RING_SPACE(chan, 5);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_RING(chan, NvSubSw, NV_SW_DMA_SEMAPHORE, 2);\r\nOUT_RING (chan, NvSema);\r\nOUT_RING (chan, offset);\r\nBEGIN_RING(chan, NvSubSw, NV_SW_SEMAPHORE_RELEASE, 1);\r\nOUT_RING (chan, 1);\r\n} else\r\nif (dev_priv->chipset < 0xc0) {\r\nret = RING_SPACE(chan, 7);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_RING(chan, NvSubSw, NV_SW_DMA_SEMAPHORE, 1);\r\nOUT_RING (chan, chan->vram_handle);\r\nBEGIN_RING(chan, NvSubSw, 0x0010, 4);\r\nOUT_RING (chan, upper_32_bits(offset));\r\nOUT_RING (chan, lower_32_bits(offset));\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 2);\r\n} else {\r\nret = RING_SPACE(chan, 5);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_NVC0(chan, 2, NvSubM2MF, 0x0010, 4);\r\nOUT_RING (chan, upper_32_bits(offset));\r\nOUT_RING (chan, lower_32_bits(offset));\r\nOUT_RING (chan, 1);\r\nOUT_RING (chan, 0x1002);\r\n}\r\nret = nouveau_fence_new(chan, &fence, true);\r\nif (ret)\r\nreturn ret;\r\nkref_get(&sema->ref);\r\nnouveau_fence_work(fence, semaphore_work, sema);\r\nnouveau_fence_unref(&fence);\r\nreturn 0;\r\n}\r\nint\r\nnouveau_fence_sync(struct nouveau_fence *fence,\r\nstruct nouveau_channel *wchan)\r\n{\r\nstruct nouveau_channel *chan = nouveau_fence_channel(fence);\r\nstruct drm_device *dev = wchan->dev;\r\nstruct nouveau_semaphore *sema;\r\nint ret = 0;\r\nif (likely(!chan || chan == wchan ||\r\nnouveau_fence_signalled(fence)))\r\ngoto out;\r\nsema = semaphore_alloc(dev);\r\nif (!sema) {\r\nret = nouveau_fence_wait(fence, true, false);\r\ngoto out;\r\n}\r\nif (!mutex_trylock(&chan->mutex)) {\r\nret = nouveau_fence_wait(fence, true, false);\r\ngoto out_unref;\r\n}\r\nret = semaphore_acquire(wchan, sema);\r\nif (ret)\r\ngoto out_unlock;\r\nret = semaphore_release(chan, sema);\r\nout_unlock:\r\nmutex_unlock(&chan->mutex);\r\nout_unref:\r\nkref_put(&sema->ref, semaphore_free);\r\nout:\r\nif (chan)\r\nnouveau_channel_put_unlocked(&chan);\r\nreturn ret;\r\n}\r\nint\r\n__nouveau_fence_flush(void *sync_obj, void *sync_arg)\r\n{\r\nreturn 0;\r\n}\r\nint\r\nnouveau_fence_channel_init(struct nouveau_channel *chan)\r\n{\r\nstruct drm_device *dev = chan->dev;\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_gpuobj *obj = NULL;\r\nint ret;\r\nif (dev_priv->card_type < NV_C0) {\r\nret = nouveau_gpuobj_gr_new(chan, NvSw, NV_SW);\r\nif (ret)\r\nreturn ret;\r\nret = RING_SPACE(chan, 2);\r\nif (ret)\r\nreturn ret;\r\nBEGIN_RING(chan, NvSubSw, 0, 1);\r\nOUT_RING (chan, NvSw);\r\nFIRE_RING (chan);\r\n}\r\nif (USE_SEMA(dev) && dev_priv->chipset < 0x84) {\r\nstruct ttm_mem_reg *mem = &dev_priv->fence.bo->bo.mem;\r\nret = nouveau_gpuobj_dma_new(chan, NV_CLASS_DMA_FROM_MEMORY,\r\nmem->start << PAGE_SHIFT,\r\nmem->size, NV_MEM_ACCESS_RW,\r\nNV_MEM_TARGET_VRAM, &obj);\r\nif (ret)\r\nreturn ret;\r\nret = nouveau_ramht_insert(chan, NvSema, obj);\r\nnouveau_gpuobj_ref(NULL, &obj);\r\nif (ret)\r\nreturn ret;\r\n} else\r\nif (USE_SEMA(dev)) {\r\nret = nouveau_bo_vma_add(dev_priv->fence.bo, chan->vm,\r\n&chan->fence.vma);\r\nif (ret)\r\nreturn ret;\r\n}\r\natomic_set(&chan->fence.last_sequence_irq, 0);\r\nreturn 0;\r\n}\r\nvoid\r\nnouveau_fence_channel_fini(struct nouveau_channel *chan)\r\n{\r\nstruct drm_nouveau_private *dev_priv = chan->dev->dev_private;\r\nstruct nouveau_fence *tmp, *fence;\r\nspin_lock(&chan->fence.lock);\r\nlist_for_each_entry_safe(fence, tmp, &chan->fence.pending, entry) {\r\nfence->signalled = true;\r\nlist_del(&fence->entry);\r\nif (unlikely(fence->work))\r\nfence->work(fence->priv, false);\r\nkref_put(&fence->refcount, nouveau_fence_del);\r\n}\r\nspin_unlock(&chan->fence.lock);\r\nnouveau_bo_vma_del(dev_priv->fence.bo, &chan->fence.vma);\r\n}\r\nint\r\nnouveau_fence_init(struct drm_device *dev)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nint size = (dev_priv->chipset < 0x84) ? 4096 : 16384;\r\nint ret;\r\nif (USE_SEMA(dev)) {\r\nret = nouveau_bo_new(dev, size, 0, TTM_PL_FLAG_VRAM,\r\n0, 0, &dev_priv->fence.bo);\r\nif (ret)\r\nreturn ret;\r\nret = nouveau_bo_pin(dev_priv->fence.bo, TTM_PL_FLAG_VRAM);\r\nif (ret)\r\ngoto fail;\r\nret = nouveau_bo_map(dev_priv->fence.bo);\r\nif (ret)\r\ngoto fail;\r\nret = drm_mm_init(&dev_priv->fence.heap, 0,\r\ndev_priv->fence.bo->bo.mem.size);\r\nif (ret)\r\ngoto fail;\r\nspin_lock_init(&dev_priv->fence.lock);\r\n}\r\nreturn 0;\r\nfail:\r\nnouveau_bo_unmap(dev_priv->fence.bo);\r\nnouveau_bo_ref(NULL, &dev_priv->fence.bo);\r\nreturn ret;\r\n}\r\nvoid\r\nnouveau_fence_fini(struct drm_device *dev)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nif (USE_SEMA(dev)) {\r\ndrm_mm_takedown(&dev_priv->fence.heap);\r\nnouveau_bo_unmap(dev_priv->fence.bo);\r\nnouveau_bo_unpin(dev_priv->fence.bo);\r\nnouveau_bo_ref(NULL, &dev_priv->fence.bo);\r\n}\r\n}
