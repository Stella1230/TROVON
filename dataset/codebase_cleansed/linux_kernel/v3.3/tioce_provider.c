static void inline\r\ntioce_mmr_war_pre(struct tioce_kernel *kern, void __iomem *mmr_addr)\r\n{\r\nu64 mmr_base;\r\nu64 mmr_offset;\r\nif (kern->ce_common->ce_rev != TIOCE_REV_A)\r\nreturn;\r\nmmr_base = kern->ce_common->ce_pcibus.bs_base;\r\nmmr_offset = (unsigned long)mmr_addr - mmr_base;\r\nif (mmr_offset < 0x45000) {\r\nu64 mmr_war_offset;\r\nif (mmr_offset == 0 || mmr_offset == 0x80)\r\nmmr_war_offset = 0xc0;\r\nelse if (mmr_offset == 0x148 || mmr_offset == 0x200)\r\nmmr_war_offset = 0x28;\r\nelse\r\nmmr_war_offset = 0x158;\r\nreadq_relaxed((void __iomem *)(mmr_base + mmr_war_offset));\r\n}\r\n}\r\nstatic void inline\r\ntioce_mmr_war_post(struct tioce_kernel *kern, void __iomem *mmr_addr)\r\n{\r\nu64 mmr_base;\r\nu64 mmr_offset;\r\nif (kern->ce_common->ce_rev != TIOCE_REV_A)\r\nreturn;\r\nmmr_base = kern->ce_common->ce_pcibus.bs_base;\r\nmmr_offset = (unsigned long)mmr_addr - mmr_base;\r\nif (mmr_offset < 0x45000) {\r\nif (mmr_offset == 0x100)\r\nreadq_relaxed((void __iomem *)(mmr_base + 0x38));\r\nreadq_relaxed((void __iomem *)(mmr_base + 0xb050));\r\n}\r\n}\r\nstatic u64\r\ntioce_dma_d64(unsigned long ct_addr, int dma_flags)\r\n{\r\nu64 bus_addr;\r\nbus_addr = ct_addr | (1UL << 63);\r\nif (dma_flags & SN_DMA_MSI)\r\nbus_addr |= (1UL << 61);\r\nreturn bus_addr;\r\n}\r\nstatic inline void\r\npcidev_to_tioce(struct pci_dev *pdev, struct tioce __iomem **base,\r\nstruct tioce_kernel **kernel, int *port)\r\n{\r\nstruct pcidev_info *pcidev_info;\r\nstruct tioce_common *ce_common;\r\nstruct tioce_kernel *ce_kernel;\r\npcidev_info = SN_PCIDEV_INFO(pdev);\r\nce_common = (struct tioce_common *)pcidev_info->pdi_pcibus_info;\r\nce_kernel = (struct tioce_kernel *)ce_common->ce_kernel_private;\r\nif (base)\r\n*base = (struct tioce __iomem *)ce_common->ce_pcibus.bs_base;\r\nif (kernel)\r\n*kernel = ce_kernel;\r\nif (port)\r\n*port =\r\n(pdev->bus->number < ce_kernel->ce_port1_secondary) ? 0 : 1;\r\n}\r\nstatic u64\r\ntioce_alloc_map(struct tioce_kernel *ce_kern, int type, int port,\r\nu64 ct_addr, int len, int dma_flags)\r\n{\r\nint i;\r\nint j;\r\nint first;\r\nint last;\r\nint entries;\r\nint nates;\r\nu64 pagesize;\r\nint msi_capable, msi_wanted;\r\nu64 *ate_shadow;\r\nu64 __iomem *ate_reg;\r\nu64 addr;\r\nstruct tioce __iomem *ce_mmr;\r\nu64 bus_base;\r\nstruct tioce_dmamap *map;\r\nce_mmr = (struct tioce __iomem *)ce_kern->ce_common->ce_pcibus.bs_base;\r\nswitch (type) {\r\ncase TIOCE_ATE_M32:\r\nfirst = 64;\r\nentries = TIOCE_NUM_M3240_ATES - 64;\r\nate_shadow = ce_kern->ce_ate3240_shadow;\r\nate_reg = ce_mmr->ce_ure_ate3240;\r\npagesize = ce_kern->ce_ate3240_pagesize;\r\nbus_base = TIOCE_M32_MIN;\r\nmsi_capable = 1;\r\nbreak;\r\ncase TIOCE_ATE_M40:\r\nfirst = 0;\r\nentries = TIOCE_NUM_M40_ATES;\r\nate_shadow = ce_kern->ce_ate40_shadow;\r\nate_reg = ce_mmr->ce_ure_ate40;\r\npagesize = MB(64);\r\nbus_base = TIOCE_M40_MIN;\r\nmsi_capable = 0;\r\nbreak;\r\ncase TIOCE_ATE_M40S:\r\nfirst = port * 32;\r\nentries = 32;\r\nate_shadow = ce_kern->ce_ate3240_shadow;\r\nate_reg = ce_mmr->ce_ure_ate3240;\r\npagesize = GB(16);\r\nbus_base = TIOCE_M40S_MIN;\r\nmsi_capable = 0;\r\nbreak;\r\ndefault:\r\nreturn 0;\r\n}\r\nmsi_wanted = dma_flags & SN_DMA_MSI;\r\nif (msi_wanted && !msi_capable)\r\nreturn 0;\r\nnates = ATE_NPAGES(ct_addr, len, pagesize);\r\nif (nates > entries)\r\nreturn 0;\r\nlast = first + entries - nates;\r\nfor (i = first; i <= last; i++) {\r\nif (ATE_VALID(ate_shadow[i]))\r\ncontinue;\r\nfor (j = i; j < i + nates; j++)\r\nif (ATE_VALID(ate_shadow[j]))\r\nbreak;\r\nif (j >= i + nates)\r\nbreak;\r\n}\r\nif (i > last)\r\nreturn 0;\r\nmap = kzalloc(sizeof(struct tioce_dmamap), GFP_ATOMIC);\r\nif (!map)\r\nreturn 0;\r\naddr = ct_addr;\r\nfor (j = 0; j < nates; j++) {\r\nu64 ate;\r\nate = ATE_MAKE(addr, pagesize, msi_wanted);\r\nate_shadow[i + j] = ate;\r\ntioce_mmr_storei(ce_kern, &ate_reg[i + j], ate);\r\naddr += pagesize;\r\n}\r\nmap->refcnt = 1;\r\nmap->nbytes = nates * pagesize;\r\nmap->ct_start = ct_addr & ~ATE_PAGEMASK(pagesize);\r\nmap->pci_start = bus_base + (i * pagesize);\r\nmap->ate_hw = &ate_reg[i];\r\nmap->ate_shadow = &ate_shadow[i];\r\nmap->ate_count = nates;\r\nlist_add(&map->ce_dmamap_list, &ce_kern->ce_dmamap_list);\r\nreturn (map->pci_start + (ct_addr - map->ct_start));\r\n}\r\nstatic u64\r\ntioce_dma_d32(struct pci_dev *pdev, u64 ct_addr, int dma_flags)\r\n{\r\nint dma_ok;\r\nint port;\r\nstruct tioce __iomem *ce_mmr;\r\nstruct tioce_kernel *ce_kern;\r\nu64 ct_upper;\r\nu64 ct_lower;\r\ndma_addr_t bus_addr;\r\nif (dma_flags & SN_DMA_MSI)\r\nreturn 0;\r\nct_upper = ct_addr & ~0x3fffffffUL;\r\nct_lower = ct_addr & 0x3fffffffUL;\r\npcidev_to_tioce(pdev, &ce_mmr, &ce_kern, &port);\r\nif (ce_kern->ce_port[port].dirmap_refcnt == 0) {\r\nu64 tmp;\r\nce_kern->ce_port[port].dirmap_shadow = ct_upper;\r\ntioce_mmr_storei(ce_kern, &ce_mmr->ce_ure_dir_map[port],\r\nct_upper);\r\ntmp = ce_mmr->ce_ure_dir_map[port];\r\ndma_ok = 1;\r\n} else\r\ndma_ok = (ce_kern->ce_port[port].dirmap_shadow == ct_upper);\r\nif (dma_ok) {\r\nce_kern->ce_port[port].dirmap_refcnt++;\r\nbus_addr = TIOCE_D32_MIN + ct_lower;\r\n} else\r\nbus_addr = 0;\r\nreturn bus_addr;\r\n}\r\nstatic u64\r\ntioce_dma_barrier(u64 bus_addr, int on)\r\n{\r\nu64 barrier_bit;\r\nif (TIOCE_M40_ADDR(bus_addr) || TIOCE_M40S_ADDR(bus_addr))\r\nreturn bus_addr;\r\nif (TIOCE_D64_ADDR(bus_addr))\r\nbarrier_bit = (1UL << 62);\r\nelse\r\nbarrier_bit = (1UL << 30);\r\nreturn (on) ? (bus_addr | barrier_bit) : (bus_addr & ~barrier_bit);\r\n}\r\nvoid\r\ntioce_dma_unmap(struct pci_dev *pdev, dma_addr_t bus_addr, int dir)\r\n{\r\nint i;\r\nint port;\r\nstruct tioce_kernel *ce_kern;\r\nstruct tioce __iomem *ce_mmr;\r\nunsigned long flags;\r\nbus_addr = tioce_dma_barrier(bus_addr, 0);\r\npcidev_to_tioce(pdev, &ce_mmr, &ce_kern, &port);\r\nif (TIOCE_D64_ADDR(bus_addr))\r\nreturn;\r\nspin_lock_irqsave(&ce_kern->ce_lock, flags);\r\nif (TIOCE_D32_ADDR(bus_addr)) {\r\nif (--ce_kern->ce_port[port].dirmap_refcnt == 0) {\r\nce_kern->ce_port[port].dirmap_shadow = 0;\r\ntioce_mmr_storei(ce_kern, &ce_mmr->ce_ure_dir_map[port],\r\n0);\r\n}\r\n} else {\r\nstruct tioce_dmamap *map;\r\nlist_for_each_entry(map, &ce_kern->ce_dmamap_list,\r\nce_dmamap_list) {\r\nu64 last;\r\nlast = map->pci_start + map->nbytes - 1;\r\nif (bus_addr >= map->pci_start && bus_addr <= last)\r\nbreak;\r\n}\r\nif (&map->ce_dmamap_list == &ce_kern->ce_dmamap_list) {\r\nprintk(KERN_WARNING\r\n"%s: %s - no map found for bus_addr 0x%llx\n",\r\n__func__, pci_name(pdev), bus_addr);\r\n} else if (--map->refcnt == 0) {\r\nfor (i = 0; i < map->ate_count; i++) {\r\nmap->ate_shadow[i] = 0;\r\ntioce_mmr_storei(ce_kern, &map->ate_hw[i], 0);\r\n}\r\nlist_del(&map->ce_dmamap_list);\r\nkfree(map);\r\n}\r\n}\r\nspin_unlock_irqrestore(&ce_kern->ce_lock, flags);\r\n}\r\nstatic u64\r\ntioce_do_dma_map(struct pci_dev *pdev, u64 paddr, size_t byte_count,\r\nint barrier, int dma_flags)\r\n{\r\nunsigned long flags;\r\nu64 ct_addr;\r\nu64 mapaddr = 0;\r\nstruct tioce_kernel *ce_kern;\r\nstruct tioce_dmamap *map;\r\nint port;\r\nu64 dma_mask;\r\ndma_mask = (barrier) ? pdev->dev.coherent_dma_mask : pdev->dma_mask;\r\nif (dma_mask < 0x7fffffffUL)\r\nreturn 0;\r\nif (SN_DMA_ADDRTYPE(dma_flags) == SN_DMA_ADDR_PHYS)\r\nct_addr = PHYS_TO_TIODMA(paddr);\r\nelse\r\nct_addr = paddr;\r\nif (dma_mask == ~0UL) {\r\nmapaddr = tioce_dma_d64(ct_addr, dma_flags);\r\nif (mapaddr)\r\ngoto dma_map_done;\r\n}\r\npcidev_to_tioce(pdev, NULL, &ce_kern, &port);\r\nspin_lock_irqsave(&ce_kern->ce_lock, flags);\r\nlist_for_each_entry(map, &ce_kern->ce_dmamap_list, ce_dmamap_list) {\r\nu64 last;\r\nlast = map->ct_start + map->nbytes - 1;\r\nif (ct_addr >= map->ct_start &&\r\nct_addr + byte_count - 1 <= last &&\r\nmap->pci_start <= dma_mask) {\r\nmap->refcnt++;\r\nmapaddr = map->pci_start + (ct_addr - map->ct_start);\r\nbreak;\r\n}\r\n}\r\nif (!mapaddr && !barrier && dma_mask >= 0xffffffffffUL) {\r\nif (byte_count > MB(64)) {\r\nmapaddr = tioce_alloc_map(ce_kern, TIOCE_ATE_M40S,\r\nport, ct_addr, byte_count,\r\ndma_flags);\r\nif (!mapaddr)\r\nmapaddr =\r\ntioce_alloc_map(ce_kern, TIOCE_ATE_M40, -1,\r\nct_addr, byte_count,\r\ndma_flags);\r\n} else {\r\nmapaddr = tioce_alloc_map(ce_kern, TIOCE_ATE_M40, -1,\r\nct_addr, byte_count,\r\ndma_flags);\r\nif (!mapaddr)\r\nmapaddr =\r\ntioce_alloc_map(ce_kern, TIOCE_ATE_M40S,\r\nport, ct_addr, byte_count,\r\ndma_flags);\r\n}\r\n}\r\nif (!mapaddr && dma_mask >= 0xffffffffUL)\r\nmapaddr = tioce_dma_d32(pdev, ct_addr, dma_flags);\r\nif (!mapaddr)\r\nmapaddr =\r\ntioce_alloc_map(ce_kern, TIOCE_ATE_M32, -1, ct_addr,\r\nbyte_count, dma_flags);\r\nspin_unlock_irqrestore(&ce_kern->ce_lock, flags);\r\ndma_map_done:\r\nif (mapaddr && barrier)\r\nmapaddr = tioce_dma_barrier(mapaddr, 1);\r\nreturn mapaddr;\r\n}\r\nstatic u64\r\ntioce_dma(struct pci_dev *pdev, unsigned long paddr, size_t byte_count, int dma_flags)\r\n{\r\nreturn tioce_do_dma_map(pdev, paddr, byte_count, 0, dma_flags);\r\n}\r\nstatic u64\r\ntioce_dma_consistent(struct pci_dev *pdev, unsigned long paddr, size_t byte_count, int dma_flags)\r\n{\r\nreturn tioce_do_dma_map(pdev, paddr, byte_count, 1, dma_flags);\r\n}\r\nstatic irqreturn_t\r\ntioce_error_intr_handler(int irq, void *arg)\r\n{\r\nstruct tioce_common *soft = arg;\r\nstruct ia64_sal_retval ret_stuff;\r\nret_stuff.status = 0;\r\nret_stuff.v0 = 0;\r\nSAL_CALL_NOLOCK(ret_stuff, (u64) SN_SAL_IOIF_ERROR_INTERRUPT,\r\nsoft->ce_pcibus.bs_persist_segment,\r\nsoft->ce_pcibus.bs_persist_busnum, 0, 0, 0, 0, 0);\r\nif (ret_stuff.v0)\r\npanic("tioce_error_intr_handler: Fatal TIOCE error");\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void\r\ntioce_reserve_m32(struct tioce_kernel *ce_kern, u64 base, u64 limit)\r\n{\r\nint ate_index, last_ate, ps;\r\nstruct tioce __iomem *ce_mmr;\r\nce_mmr = (struct tioce __iomem *)ce_kern->ce_common->ce_pcibus.bs_base;\r\nps = ce_kern->ce_ate3240_pagesize;\r\nate_index = ATE_PAGE(base, ps);\r\nlast_ate = ate_index + ATE_NPAGES(base, limit-base+1, ps) - 1;\r\nif (ate_index < 64)\r\nate_index = 64;\r\nif (last_ate >= TIOCE_NUM_M3240_ATES)\r\nlast_ate = TIOCE_NUM_M3240_ATES - 1;\r\nwhile (ate_index <= last_ate) {\r\nu64 ate;\r\nate = ATE_MAKE(0xdeadbeef, ps, 0);\r\nce_kern->ce_ate3240_shadow[ate_index] = ate;\r\ntioce_mmr_storei(ce_kern, &ce_mmr->ce_ure_ate3240[ate_index],\r\nate);\r\nate_index++;\r\n}\r\n}\r\nstatic struct tioce_kernel *\r\ntioce_kern_init(struct tioce_common *tioce_common)\r\n{\r\nint i;\r\nint ps;\r\nint dev;\r\nu32 tmp;\r\nunsigned int seg, bus;\r\nstruct tioce __iomem *tioce_mmr;\r\nstruct tioce_kernel *tioce_kern;\r\ntioce_kern = kzalloc(sizeof(struct tioce_kernel), GFP_KERNEL);\r\nif (!tioce_kern) {\r\nreturn NULL;\r\n}\r\ntioce_kern->ce_common = tioce_common;\r\nspin_lock_init(&tioce_kern->ce_lock);\r\nINIT_LIST_HEAD(&tioce_kern->ce_dmamap_list);\r\ntioce_common->ce_kernel_private = (u64) tioce_kern;\r\nseg = tioce_common->ce_pcibus.bs_persist_segment;\r\nbus = tioce_common->ce_pcibus.bs_persist_busnum;\r\nraw_pci_read(seg, bus, PCI_DEVFN(2, 0), PCI_SECONDARY_BUS, 1,&tmp);\r\ntioce_kern->ce_port1_secondary = (u8) tmp;\r\ntioce_mmr = (struct tioce __iomem *)tioce_common->ce_pcibus.bs_base;\r\ntioce_mmr_clri(tioce_kern, &tioce_mmr->ce_ure_page_map,\r\nCE_URE_PAGESIZE_MASK);\r\ntioce_mmr_seti(tioce_kern, &tioce_mmr->ce_ure_page_map,\r\nCE_URE_256K_PAGESIZE);\r\nps = tioce_kern->ce_ate3240_pagesize = KB(256);\r\nfor (i = 0; i < TIOCE_NUM_M40_ATES; i++) {\r\ntioce_kern->ce_ate40_shadow[i] = 0;\r\ntioce_mmr_storei(tioce_kern, &tioce_mmr->ce_ure_ate40[i], 0);\r\n}\r\nfor (i = 0; i < TIOCE_NUM_M3240_ATES; i++) {\r\ntioce_kern->ce_ate3240_shadow[i] = 0;\r\ntioce_mmr_storei(tioce_kern, &tioce_mmr->ce_ure_ate3240[i], 0);\r\n}\r\nfor (dev = 1; dev <= 2; dev++) {\r\nu64 base, limit;\r\nraw_pci_read(seg, bus, PCI_DEVFN(dev, 0),\r\nPCI_MEMORY_BASE, 2, &tmp);\r\nbase = (u64)tmp << 16;\r\nraw_pci_read(seg, bus, PCI_DEVFN(dev, 0),\r\nPCI_MEMORY_LIMIT, 2, &tmp);\r\nlimit = (u64)tmp << 16;\r\nlimit |= 0xfffffUL;\r\nif (base < limit)\r\ntioce_reserve_m32(tioce_kern, base, limit);\r\nraw_pci_read(seg, bus, PCI_DEVFN(dev, 0),\r\nPCI_PREF_MEMORY_BASE, 2, &tmp);\r\nbase = ((u64)tmp & PCI_PREF_RANGE_MASK) << 16;\r\nraw_pci_read(seg, bus, PCI_DEVFN(dev, 0),\r\nPCI_PREF_BASE_UPPER32, 4, &tmp);\r\nbase |= (u64)tmp << 32;\r\nraw_pci_read(seg, bus, PCI_DEVFN(dev, 0),\r\nPCI_PREF_MEMORY_LIMIT, 2, &tmp);\r\nlimit = ((u64)tmp & PCI_PREF_RANGE_MASK) << 16;\r\nlimit |= 0xfffffUL;\r\nraw_pci_read(seg, bus, PCI_DEVFN(dev, 0),\r\nPCI_PREF_LIMIT_UPPER32, 4, &tmp);\r\nlimit |= (u64)tmp << 32;\r\nif ((base < limit) && TIOCE_M32_ADDR(base))\r\ntioce_reserve_m32(tioce_kern, base, limit);\r\n}\r\nreturn tioce_kern;\r\n}\r\nstatic void\r\ntioce_force_interrupt(struct sn_irq_info *sn_irq_info)\r\n{\r\nstruct pcidev_info *pcidev_info;\r\nstruct tioce_common *ce_common;\r\nstruct tioce_kernel *ce_kern;\r\nstruct tioce __iomem *ce_mmr;\r\nu64 force_int_val;\r\nif (!sn_irq_info->irq_bridge)\r\nreturn;\r\nif (sn_irq_info->irq_bridge_type != PCIIO_ASIC_TYPE_TIOCE)\r\nreturn;\r\npcidev_info = (struct pcidev_info *)sn_irq_info->irq_pciioinfo;\r\nif (!pcidev_info)\r\nreturn;\r\nce_common = (struct tioce_common *)pcidev_info->pdi_pcibus_info;\r\nce_mmr = (struct tioce __iomem *)ce_common->ce_pcibus.bs_base;\r\nce_kern = (struct tioce_kernel *)ce_common->ce_kernel_private;\r\nif (ce_common->ce_rev == TIOCE_REV_A) {\r\nu64 int_bit_mask = (1ULL << sn_irq_info->irq_int_bit);\r\nu64 status;\r\ntioce_mmr_load(ce_kern, &ce_mmr->ce_adm_int_status, &status);\r\nif (status & int_bit_mask) {\r\nu64 force_irq = (1 << 8) | sn_irq_info->irq_irq;\r\nu64 ctalk = sn_irq_info->irq_xtalkaddr;\r\nu64 nasid, offset;\r\nnasid = (ctalk & CTALK_NASID_MASK) >> CTALK_NASID_SHFT;\r\noffset = (ctalk & CTALK_NODE_OFFSET);\r\nHUB_S(TIO_IOSPACE_ADDR(nasid, offset), force_irq);\r\n}\r\nreturn;\r\n}\r\nswitch (sn_irq_info->irq_int_bit) {\r\ncase CE_ADM_INT_PCIE_PORT1_DEV_A_SHFT:\r\nforce_int_val = 1UL << CE_ADM_FORCE_INT_PCIE_PORT1_DEV_A_SHFT;\r\nbreak;\r\ncase CE_ADM_INT_PCIE_PORT1_DEV_B_SHFT:\r\nforce_int_val = 1UL << CE_ADM_FORCE_INT_PCIE_PORT1_DEV_B_SHFT;\r\nbreak;\r\ncase CE_ADM_INT_PCIE_PORT1_DEV_C_SHFT:\r\nforce_int_val = 1UL << CE_ADM_FORCE_INT_PCIE_PORT1_DEV_C_SHFT;\r\nbreak;\r\ncase CE_ADM_INT_PCIE_PORT1_DEV_D_SHFT:\r\nforce_int_val = 1UL << CE_ADM_FORCE_INT_PCIE_PORT1_DEV_D_SHFT;\r\nbreak;\r\ncase CE_ADM_INT_PCIE_PORT2_DEV_A_SHFT:\r\nforce_int_val = 1UL << CE_ADM_FORCE_INT_PCIE_PORT2_DEV_A_SHFT;\r\nbreak;\r\ncase CE_ADM_INT_PCIE_PORT2_DEV_B_SHFT:\r\nforce_int_val = 1UL << CE_ADM_FORCE_INT_PCIE_PORT2_DEV_B_SHFT;\r\nbreak;\r\ncase CE_ADM_INT_PCIE_PORT2_DEV_C_SHFT:\r\nforce_int_val = 1UL << CE_ADM_FORCE_INT_PCIE_PORT2_DEV_C_SHFT;\r\nbreak;\r\ncase CE_ADM_INT_PCIE_PORT2_DEV_D_SHFT:\r\nforce_int_val = 1UL << CE_ADM_FORCE_INT_PCIE_PORT2_DEV_D_SHFT;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\ntioce_mmr_storei(ce_kern, &ce_mmr->ce_adm_force_int, force_int_val);\r\n}\r\nstatic void\r\ntioce_target_interrupt(struct sn_irq_info *sn_irq_info)\r\n{\r\nstruct pcidev_info *pcidev_info;\r\nstruct tioce_common *ce_common;\r\nstruct tioce_kernel *ce_kern;\r\nstruct tioce __iomem *ce_mmr;\r\nint bit;\r\nu64 vector;\r\npcidev_info = (struct pcidev_info *)sn_irq_info->irq_pciioinfo;\r\nif (!pcidev_info)\r\nreturn;\r\nce_common = (struct tioce_common *)pcidev_info->pdi_pcibus_info;\r\nce_mmr = (struct tioce __iomem *)ce_common->ce_pcibus.bs_base;\r\nce_kern = (struct tioce_kernel *)ce_common->ce_kernel_private;\r\nbit = sn_irq_info->irq_int_bit;\r\ntioce_mmr_seti(ce_kern, &ce_mmr->ce_adm_int_mask, (1UL << bit));\r\nvector = (u64)sn_irq_info->irq_irq << INTR_VECTOR_SHFT;\r\nvector |= sn_irq_info->irq_xtalkaddr;\r\ntioce_mmr_storei(ce_kern, &ce_mmr->ce_adm_int_dest[bit], vector);\r\ntioce_mmr_clri(ce_kern, &ce_mmr->ce_adm_int_mask, (1UL << bit));\r\ntioce_force_interrupt(sn_irq_info);\r\n}\r\nstatic void *\r\ntioce_bus_fixup(struct pcibus_bussoft *prom_bussoft, struct pci_controller *controller)\r\n{\r\nstruct tioce_common *tioce_common;\r\nstruct tioce_kernel *tioce_kern;\r\nstruct tioce __iomem *tioce_mmr;\r\ntioce_common = kzalloc(sizeof(struct tioce_common), GFP_KERNEL);\r\nif (!tioce_common)\r\nreturn NULL;\r\nmemcpy(tioce_common, prom_bussoft, sizeof(struct tioce_common));\r\ntioce_common->ce_pcibus.bs_base = (unsigned long)\r\nioremap(REGION_OFFSET(tioce_common->ce_pcibus.bs_base),\r\nsizeof(struct tioce_common));\r\ntioce_kern = tioce_kern_init(tioce_common);\r\nif (tioce_kern == NULL) {\r\nkfree(tioce_common);\r\nreturn NULL;\r\n}\r\ntioce_mmr = (struct tioce __iomem *)tioce_common->ce_pcibus.bs_base;\r\ntioce_mmr_seti(tioce_kern, &tioce_mmr->ce_adm_int_status_alias, ~0ULL);\r\ntioce_mmr_seti(tioce_kern, &tioce_mmr->ce_adm_error_summary_alias,\r\n~0ULL);\r\ntioce_mmr_seti(tioce_kern, &tioce_mmr->ce_dre_comp_err_addr, 0ULL);\r\nif (request_irq(SGI_PCIASIC_ERROR,\r\ntioce_error_intr_handler,\r\nIRQF_SHARED, "TIOCE error", (void *)tioce_common))\r\nprintk(KERN_WARNING\r\n"%s: Unable to get irq %d. "\r\n"Error interrupts won't be routed for "\r\n"TIOCE bus %04x:%02x\n",\r\n__func__, SGI_PCIASIC_ERROR,\r\ntioce_common->ce_pcibus.bs_persist_segment,\r\ntioce_common->ce_pcibus.bs_persist_busnum);\r\nsn_set_err_irq_affinity(SGI_PCIASIC_ERROR);\r\nreturn tioce_common;\r\n}\r\nint\r\ntioce_init_provider(void)\r\n{\r\nsn_pci_provider[PCIIO_ASIC_TYPE_TIOCE] = &tioce_pci_interfaces;\r\nreturn 0;\r\n}
