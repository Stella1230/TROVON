static unsigned int steal_context_smp(unsigned int id)\r\n{\r\nstruct mm_struct *mm;\r\nunsigned int cpu, max, i;\r\nmax = last_context - first_context;\r\nwhile (max--) {\r\nmm = context_mm[id];\r\nif (mm->context.active) {\r\nid++;\r\nif (id > last_context)\r\nid = first_context;\r\ncontinue;\r\n}\r\npr_hardcont(" | steal %d from 0x%p", id, mm);\r\nmm->context.id = MMU_NO_CONTEXT;\r\nfor_each_cpu(cpu, mm_cpumask(mm)) {\r\nfor (i = cpu_first_thread_sibling(cpu);\r\ni <= cpu_last_thread_sibling(cpu); i++)\r\n__set_bit(id, stale_map[i]);\r\ncpu = i - 1;\r\n}\r\nreturn id;\r\n}\r\nraw_spin_unlock(&context_lock);\r\ncpu_relax();\r\nraw_spin_lock(&context_lock);\r\nreturn MMU_NO_CONTEXT;\r\n}\r\nstatic unsigned int steal_context_up(unsigned int id)\r\n{\r\nstruct mm_struct *mm;\r\nint cpu = smp_processor_id();\r\nmm = context_mm[id];\r\npr_hardcont(" | steal %d from 0x%p", id, mm);\r\nlocal_flush_tlb_mm(mm);\r\nmm->context.id = MMU_NO_CONTEXT;\r\n__clear_bit(id, stale_map[cpu]);\r\nreturn id;\r\n}\r\nstatic void context_check_map(void)\r\n{\r\nunsigned int id, nrf, nact;\r\nnrf = nact = 0;\r\nfor (id = first_context; id <= last_context; id++) {\r\nint used = test_bit(id, context_map);\r\nif (!used)\r\nnrf++;\r\nif (used != (context_mm[id] != NULL))\r\npr_err("MMU: Context %d is %s and MM is %p !\n",\r\nid, used ? "used" : "free", context_mm[id]);\r\nif (context_mm[id] != NULL)\r\nnact += context_mm[id]->context.active;\r\n}\r\nif (nrf != nr_free_contexts) {\r\npr_err("MMU: Free context count out of sync ! (%d vs %d)\n",\r\nnr_free_contexts, nrf);\r\nnr_free_contexts = nrf;\r\n}\r\nif (nact > num_online_cpus())\r\npr_err("MMU: More active contexts than CPUs ! (%d vs %d)\n",\r\nnact, num_online_cpus());\r\nif (first_context > 0 && !test_bit(0, context_map))\r\npr_err("MMU: Context 0 has been freed !!!\n");\r\n}\r\nstatic void context_check_map(void) { }\r\nvoid switch_mmu_context(struct mm_struct *prev, struct mm_struct *next)\r\n{\r\nunsigned int i, id, cpu = smp_processor_id();\r\nunsigned long *map;\r\nraw_spin_lock(&context_lock);\r\npr_hard("[%d] activating context for mm @%p, active=%d, id=%d",\r\ncpu, next, next->context.active, next->context.id);\r\n#ifdef CONFIG_SMP\r\nnext->context.active++;\r\nif (prev) {\r\npr_hardcont(" (old=0x%p a=%d)", prev, prev->context.active);\r\nWARN_ON(prev->context.active < 1);\r\nprev->context.active--;\r\n}\r\nagain:\r\n#endif\r\nid = next->context.id;\r\nif (likely(id != MMU_NO_CONTEXT)) {\r\n#ifdef DEBUG_MAP_CONSISTENCY\r\nif (context_mm[id] != next)\r\npr_err("MMU: mm 0x%p has id %d but context_mm[%d] says 0x%p\n",\r\nnext, id, id, context_mm[id]);\r\n#endif\r\ngoto ctxt_ok;\r\n}\r\nid = next_context;\r\nif (id > last_context)\r\nid = first_context;\r\nmap = context_map;\r\nif (nr_free_contexts == 0) {\r\n#ifdef CONFIG_SMP\r\nif (num_online_cpus() > 1) {\r\nid = steal_context_smp(id);\r\nif (id == MMU_NO_CONTEXT)\r\ngoto again;\r\ngoto stolen;\r\n}\r\n#endif\r\nid = steal_context_up(id);\r\ngoto stolen;\r\n}\r\nnr_free_contexts--;\r\nwhile (__test_and_set_bit(id, map)) {\r\nid = find_next_zero_bit(map, last_context+1, id);\r\nif (id > last_context)\r\nid = first_context;\r\n}\r\nstolen:\r\nnext_context = id + 1;\r\ncontext_mm[id] = next;\r\nnext->context.id = id;\r\npr_hardcont(" | new id=%d,nrf=%d", id, nr_free_contexts);\r\ncontext_check_map();\r\nctxt_ok:\r\nif (test_bit(id, stale_map[cpu])) {\r\npr_hardcont(" | stale flush %d [%d..%d]",\r\nid, cpu_first_thread_sibling(cpu),\r\ncpu_last_thread_sibling(cpu));\r\nlocal_flush_tlb_mm(next);\r\nfor (i = cpu_first_thread_sibling(cpu);\r\ni <= cpu_last_thread_sibling(cpu); i++) {\r\n__clear_bit(id, stale_map[i]);\r\n}\r\n}\r\npr_hardcont(" -> %d\n", id);\r\nset_context(id, next->pgd);\r\nraw_spin_unlock(&context_lock);\r\n}\r\nint init_new_context(struct task_struct *t, struct mm_struct *mm)\r\n{\r\npr_hard("initing context for mm @%p\n", mm);\r\nmm->context.id = MMU_NO_CONTEXT;\r\nmm->context.active = 0;\r\n#ifdef CONFIG_PPC_MM_SLICES\r\nif (slice_mm_new_context(mm))\r\nslice_set_user_psize(mm, mmu_virtual_psize);\r\n#endif\r\nreturn 0;\r\n}\r\nvoid destroy_context(struct mm_struct *mm)\r\n{\r\nunsigned long flags;\r\nunsigned int id;\r\nif (mm->context.id == MMU_NO_CONTEXT)\r\nreturn;\r\nWARN_ON(mm->context.active != 0);\r\nraw_spin_lock_irqsave(&context_lock, flags);\r\nid = mm->context.id;\r\nif (id != MMU_NO_CONTEXT) {\r\n__clear_bit(id, context_map);\r\nmm->context.id = MMU_NO_CONTEXT;\r\n#ifdef DEBUG_MAP_CONSISTENCY\r\nmm->context.active = 0;\r\n#endif\r\ncontext_mm[id] = NULL;\r\nnr_free_contexts++;\r\n}\r\nraw_spin_unlock_irqrestore(&context_lock, flags);\r\n}\r\nstatic int __cpuinit mmu_context_cpu_notify(struct notifier_block *self,\r\nunsigned long action, void *hcpu)\r\n{\r\nunsigned int cpu = (unsigned int)(long)hcpu;\r\n#ifdef CONFIG_HOTPLUG_CPU\r\nstruct task_struct *p;\r\n#endif\r\nif (cpu == boot_cpuid)\r\nreturn NOTIFY_OK;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_UP_PREPARE_FROZEN:\r\npr_devel("MMU: Allocating stale context map for CPU %d\n", cpu);\r\nstale_map[cpu] = kzalloc(CTX_MAP_SIZE, GFP_KERNEL);\r\nbreak;\r\n#ifdef CONFIG_HOTPLUG_CPU\r\ncase CPU_UP_CANCELED:\r\ncase CPU_UP_CANCELED_FROZEN:\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\npr_devel("MMU: Freeing stale context map for CPU %d\n", cpu);\r\nkfree(stale_map[cpu]);\r\nstale_map[cpu] = NULL;\r\nread_lock(&tasklist_lock);\r\nfor_each_process(p) {\r\nif (p->mm)\r\ncpumask_clear_cpu(cpu, mm_cpumask(p->mm));\r\n}\r\nread_unlock(&tasklist_lock);\r\nbreak;\r\n#endif\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nvoid __init mmu_context_init(void)\r\n{\r\ninit_mm.context.active = NR_CPUS;\r\nif (mmu_has_feature(MMU_FTR_TYPE_8xx)) {\r\nfirst_context = 0;\r\nlast_context = 15;\r\n} else if (mmu_has_feature(MMU_FTR_TYPE_47x)) {\r\nfirst_context = 1;\r\nlast_context = 65535;\r\n} else\r\n#ifdef CONFIG_PPC_BOOK3E_MMU\r\nif (mmu_has_feature(MMU_FTR_TYPE_3E)) {\r\nu32 mmucfg = mfspr(SPRN_MMUCFG);\r\nu32 pid_bits = (mmucfg & MMUCFG_PIDSIZE_MASK)\r\n>> MMUCFG_PIDSIZE_SHIFT;\r\nfirst_context = 1;\r\nlast_context = (1UL << (pid_bits + 1)) - 1;\r\n} else\r\n#endif\r\n{\r\nfirst_context = 1;\r\nlast_context = 255;\r\n}\r\n#ifdef DEBUG_CLAMP_LAST_CONTEXT\r\nlast_context = DEBUG_CLAMP_LAST_CONTEXT;\r\n#endif\r\ncontext_map = alloc_bootmem(CTX_MAP_SIZE);\r\ncontext_mm = alloc_bootmem(sizeof(void *) * (last_context + 1));\r\n#ifndef CONFIG_SMP\r\nstale_map[0] = alloc_bootmem(CTX_MAP_SIZE);\r\n#else\r\nstale_map[boot_cpuid] = alloc_bootmem(CTX_MAP_SIZE);\r\nregister_cpu_notifier(&mmu_context_cpu_nb);\r\n#endif\r\nprintk(KERN_INFO\r\n"MMU: Allocated %zu bytes of context maps for %d contexts\n",\r\n2 * CTX_MAP_SIZE + (sizeof(void *) * (last_context + 1)),\r\nlast_context - first_context + 1);\r\ncontext_map[0] = (1 << first_context) - 1;\r\nnext_context = first_context;\r\nnr_free_contexts = last_context - first_context + 1;\r\n}
