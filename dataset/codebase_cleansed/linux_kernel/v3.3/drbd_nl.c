int drbd_khelper(struct drbd_conf *mdev, char *cmd)\r\n{\r\nchar *envp[] = { "HOME=/",\r\n"TERM=linux",\r\n"PATH=/sbin:/usr/sbin:/bin:/usr/bin",\r\nNULL,\r\nNULL,\r\nNULL };\r\nchar mb[12], af[20], ad[60], *afs;\r\nchar *argv[] = {usermode_helper, cmd, mb, NULL };\r\nint ret;\r\nsnprintf(mb, 12, "minor-%d", mdev_to_minor(mdev));\r\nif (get_net_conf(mdev)) {\r\nswitch (((struct sockaddr *)mdev->net_conf->peer_addr)->sa_family) {\r\ncase AF_INET6:\r\nafs = "ipv6";\r\nsnprintf(ad, 60, "DRBD_PEER_ADDRESS=%pI6",\r\n&((struct sockaddr_in6 *)mdev->net_conf->peer_addr)->sin6_addr);\r\nbreak;\r\ncase AF_INET:\r\nafs = "ipv4";\r\nsnprintf(ad, 60, "DRBD_PEER_ADDRESS=%pI4",\r\n&((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\r\nbreak;\r\ndefault:\r\nafs = "ssocks";\r\nsnprintf(ad, 60, "DRBD_PEER_ADDRESS=%pI4",\r\n&((struct sockaddr_in *)mdev->net_conf->peer_addr)->sin_addr);\r\n}\r\nsnprintf(af, 20, "DRBD_PEER_AF=%s", afs);\r\nenvp[3]=af;\r\nenvp[4]=ad;\r\nput_net_conf(mdev);\r\n}\r\ndrbd_md_sync(mdev);\r\ndev_info(DEV, "helper command: %s %s %s\n", usermode_helper, cmd, mb);\r\ndrbd_bcast_ev_helper(mdev, cmd);\r\nret = call_usermodehelper(usermode_helper, argv, envp, 1);\r\nif (ret)\r\ndev_warn(DEV, "helper command: %s %s %s exit code %u (0x%x)\n",\r\nusermode_helper, cmd, mb,\r\n(ret >> 8) & 0xff, ret);\r\nelse\r\ndev_info(DEV, "helper command: %s %s %s exit code %u (0x%x)\n",\r\nusermode_helper, cmd, mb,\r\n(ret >> 8) & 0xff, ret);\r\nif (ret < 0)\r\nret = 0;\r\nreturn ret;\r\n}\r\nenum drbd_disk_state drbd_try_outdate_peer(struct drbd_conf *mdev)\r\n{\r\nchar *ex_to_string;\r\nint r;\r\nenum drbd_disk_state nps;\r\nenum drbd_fencing_p fp;\r\nD_ASSERT(mdev->state.pdsk == D_UNKNOWN);\r\nif (get_ldev_if_state(mdev, D_CONSISTENT)) {\r\nfp = mdev->ldev->dc.fencing;\r\nput_ldev(mdev);\r\n} else {\r\ndev_warn(DEV, "Not fencing peer, I'm not even Consistent myself.\n");\r\nnps = mdev->state.pdsk;\r\ngoto out;\r\n}\r\nr = drbd_khelper(mdev, "fence-peer");\r\nswitch ((r>>8) & 0xff) {\r\ncase 3:\r\nex_to_string = "peer is inconsistent or worse";\r\nnps = D_INCONSISTENT;\r\nbreak;\r\ncase 4:\r\nex_to_string = "peer was fenced";\r\nnps = D_OUTDATED;\r\nbreak;\r\ncase 5:\r\nif (mdev->state.disk == D_UP_TO_DATE) {\r\nex_to_string = "peer is unreachable, assumed to be dead";\r\nnps = D_OUTDATED;\r\n} else {\r\nex_to_string = "peer unreachable, doing nothing since disk != UpToDate";\r\nnps = mdev->state.pdsk;\r\n}\r\nbreak;\r\ncase 6:\r\nex_to_string = "peer is active";\r\ndev_warn(DEV, "Peer is primary, outdating myself.\n");\r\nnps = D_UNKNOWN;\r\n_drbd_request_state(mdev, NS(disk, D_OUTDATED), CS_WAIT_COMPLETE);\r\nbreak;\r\ncase 7:\r\nif (fp != FP_STONITH)\r\ndev_err(DEV, "fence-peer() = 7 && fencing != Stonith !!!\n");\r\nex_to_string = "peer was stonithed";\r\nnps = D_OUTDATED;\r\nbreak;\r\ndefault:\r\nnps = D_UNKNOWN;\r\ndev_err(DEV, "fence-peer helper broken, returned %d\n", (r>>8)&0xff);\r\nreturn nps;\r\n}\r\ndev_info(DEV, "fence-peer helper returned %d (%s)\n",\r\n(r>>8) & 0xff, ex_to_string);\r\nout:\r\nif (mdev->state.susp_fen && nps >= D_UNKNOWN) {\r\n_drbd_request_state(mdev, NS(susp_fen, 0), CS_VERBOSE);\r\n}\r\nreturn nps;\r\n}\r\nstatic int _try_outdate_peer_async(void *data)\r\n{\r\nstruct drbd_conf *mdev = (struct drbd_conf *)data;\r\nenum drbd_disk_state nps;\r\nunion drbd_state ns;\r\nnps = drbd_try_outdate_peer(mdev);\r\nspin_lock_irq(&mdev->req_lock);\r\nns = mdev->state;\r\nif (ns.conn < C_WF_REPORT_PARAMS) {\r\nns.pdsk = nps;\r\n_drbd_set_state(mdev, ns, CS_VERBOSE, NULL);\r\n}\r\nspin_unlock_irq(&mdev->req_lock);\r\nreturn 0;\r\n}\r\nvoid drbd_try_outdate_peer_async(struct drbd_conf *mdev)\r\n{\r\nstruct task_struct *opa;\r\nopa = kthread_run(_try_outdate_peer_async, mdev, "drbd%d_a_helper", mdev_to_minor(mdev));\r\nif (IS_ERR(opa))\r\ndev_err(DEV, "out of mem, failed to invoke fence-peer helper\n");\r\n}\r\nenum drbd_state_rv\r\ndrbd_set_role(struct drbd_conf *mdev, enum drbd_role new_role, int force)\r\n{\r\nconst int max_tries = 4;\r\nenum drbd_state_rv rv = SS_UNKNOWN_ERROR;\r\nint try = 0;\r\nint forced = 0;\r\nunion drbd_state mask, val;\r\nenum drbd_disk_state nps;\r\nif (new_role == R_PRIMARY)\r\nrequest_ping(mdev);\r\nmutex_lock(&mdev->state_mutex);\r\nmask.i = 0; mask.role = R_MASK;\r\nval.i = 0; val.role = new_role;\r\nwhile (try++ < max_tries) {\r\nrv = _drbd_request_state(mdev, mask, val, CS_WAIT_COMPLETE);\r\nif (rv == SS_CW_FAILED_BY_PEER && mask.pdsk != 0) {\r\nval.pdsk = 0;\r\nmask.pdsk = 0;\r\ncontinue;\r\n}\r\nif (rv == SS_NO_UP_TO_DATE_DISK && force &&\r\n(mdev->state.disk < D_UP_TO_DATE &&\r\nmdev->state.disk >= D_INCONSISTENT)) {\r\nmask.disk = D_MASK;\r\nval.disk = D_UP_TO_DATE;\r\nforced = 1;\r\ncontinue;\r\n}\r\nif (rv == SS_NO_UP_TO_DATE_DISK &&\r\nmdev->state.disk == D_CONSISTENT && mask.pdsk == 0) {\r\nD_ASSERT(mdev->state.pdsk == D_UNKNOWN);\r\nnps = drbd_try_outdate_peer(mdev);\r\nif (nps == D_OUTDATED || nps == D_INCONSISTENT) {\r\nval.disk = D_UP_TO_DATE;\r\nmask.disk = D_MASK;\r\n}\r\nval.pdsk = nps;\r\nmask.pdsk = D_MASK;\r\ncontinue;\r\n}\r\nif (rv == SS_NOTHING_TO_DO)\r\ngoto fail;\r\nif (rv == SS_PRIMARY_NOP && mask.pdsk == 0) {\r\nnps = drbd_try_outdate_peer(mdev);\r\nif (force && nps > D_OUTDATED) {\r\ndev_warn(DEV, "Forced into split brain situation!\n");\r\nnps = D_OUTDATED;\r\n}\r\nmask.pdsk = D_MASK;\r\nval.pdsk = nps;\r\ncontinue;\r\n}\r\nif (rv == SS_TWO_PRIMARIES) {\r\nschedule_timeout_interruptible((mdev->net_conf->ping_timeo+1)*HZ/10);\r\nif (try < max_tries)\r\ntry = max_tries - 1;\r\ncontinue;\r\n}\r\nif (rv < SS_SUCCESS) {\r\nrv = _drbd_request_state(mdev, mask, val,\r\nCS_VERBOSE + CS_WAIT_COMPLETE);\r\nif (rv < SS_SUCCESS)\r\ngoto fail;\r\n}\r\nbreak;\r\n}\r\nif (rv < SS_SUCCESS)\r\ngoto fail;\r\nif (forced)\r\ndev_warn(DEV, "Forced to consider local data as UpToDate!\n");\r\nwait_event(mdev->misc_wait, atomic_read(&mdev->ap_pending_cnt) == 0);\r\nif (new_role == R_SECONDARY) {\r\nset_disk_ro(mdev->vdisk, true);\r\nif (get_ldev(mdev)) {\r\nmdev->ldev->md.uuid[UI_CURRENT] &= ~(u64)1;\r\nput_ldev(mdev);\r\n}\r\n} else {\r\nif (get_net_conf(mdev)) {\r\nmdev->net_conf->want_lose = 0;\r\nput_net_conf(mdev);\r\n}\r\nset_disk_ro(mdev->vdisk, false);\r\nif (get_ldev(mdev)) {\r\nif (((mdev->state.conn < C_CONNECTED ||\r\nmdev->state.pdsk <= D_FAILED)\r\n&& mdev->ldev->md.uuid[UI_BITMAP] == 0) || forced)\r\ndrbd_uuid_new_current(mdev);\r\nmdev->ldev->md.uuid[UI_CURRENT] |= (u64)1;\r\nput_ldev(mdev);\r\n}\r\n}\r\nif (mdev->state.conn >= C_WF_REPORT_PARAMS) {\r\nif (forced)\r\ndrbd_send_uuids(mdev);\r\ndrbd_send_state(mdev);\r\n}\r\ndrbd_md_sync(mdev);\r\nkobject_uevent(&disk_to_dev(mdev->vdisk)->kobj, KOBJ_CHANGE);\r\nfail:\r\nmutex_unlock(&mdev->state_mutex);\r\nreturn rv;\r\n}\r\nstatic struct drbd_conf *ensure_mdev(int minor, int create)\r\n{\r\nstruct drbd_conf *mdev;\r\nif (minor >= minor_count)\r\nreturn NULL;\r\nmdev = minor_to_mdev(minor);\r\nif (!mdev && create) {\r\nstruct gendisk *disk = NULL;\r\nmdev = drbd_new_device(minor);\r\nspin_lock_irq(&drbd_pp_lock);\r\nif (minor_table[minor] == NULL) {\r\nminor_table[minor] = mdev;\r\ndisk = mdev->vdisk;\r\nmdev = NULL;\r\n}\r\nspin_unlock_irq(&drbd_pp_lock);\r\nif (disk)\r\nadd_disk(disk);\r\nelse\r\ndrbd_free_mdev(mdev);\r\nmdev = minor_to_mdev(minor);\r\n}\r\nreturn mdev;\r\n}\r\nstatic int drbd_nl_primary(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nstruct primary primary_args;\r\nmemset(&primary_args, 0, sizeof(struct primary));\r\nif (!primary_from_tags(mdev, nlp->tag_list, &primary_args)) {\r\nreply->ret_code = ERR_MANDATORY_TAG;\r\nreturn 0;\r\n}\r\nreply->ret_code =\r\ndrbd_set_role(mdev, R_PRIMARY, primary_args.primary_force);\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_secondary(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nreply->ret_code = drbd_set_role(mdev, R_SECONDARY, 0);\r\nreturn 0;\r\n}\r\nstatic void drbd_md_set_sector_offsets(struct drbd_conf *mdev,\r\nstruct drbd_backing_dev *bdev)\r\n{\r\nsector_t md_size_sect = 0;\r\nswitch (bdev->dc.meta_dev_idx) {\r\ndefault:\r\nbdev->md.md_size_sect = MD_RESERVED_SECT;\r\nbdev->md.md_offset = drbd_md_ss__(mdev, bdev);\r\nbdev->md.al_offset = MD_AL_OFFSET;\r\nbdev->md.bm_offset = MD_BM_OFFSET;\r\nbreak;\r\ncase DRBD_MD_INDEX_FLEX_EXT:\r\nbdev->md.md_size_sect = drbd_get_capacity(bdev->md_bdev);\r\nbdev->md.md_offset = 0;\r\nbdev->md.al_offset = MD_AL_OFFSET;\r\nbdev->md.bm_offset = MD_BM_OFFSET;\r\nbreak;\r\ncase DRBD_MD_INDEX_INTERNAL:\r\ncase DRBD_MD_INDEX_FLEX_INT:\r\nbdev->md.md_offset = drbd_md_ss__(mdev, bdev);\r\nbdev->md.al_offset = -MD_AL_MAX_SIZE;\r\nmd_size_sect = drbd_get_capacity(bdev->backing_bdev);\r\nmd_size_sect = ALIGN(md_size_sect, BM_SECT_PER_EXT);\r\nmd_size_sect = BM_SECT_TO_EXT(md_size_sect);\r\nmd_size_sect = ALIGN(md_size_sect, 8);\r\nmd_size_sect += MD_BM_OFFSET;\r\nbdev->md.md_size_sect = md_size_sect;\r\nbdev->md.bm_offset = -md_size_sect + MD_AL_OFFSET;\r\nbreak;\r\n}\r\n}\r\nchar *ppsize(char *buf, unsigned long long size)\r\n{\r\nstatic char units[] = { 'K', 'M', 'G', 'T', 'P', 'E' };\r\nint base = 0;\r\nwhile (size >= 10000 && base < sizeof(units)-1) {\r\nsize = (size >> 10) + !!(size & (1<<9));\r\nbase++;\r\n}\r\nsprintf(buf, "%u %cB", (unsigned)size, units[base]);\r\nreturn buf;\r\n}\r\nvoid drbd_suspend_io(struct drbd_conf *mdev)\r\n{\r\nset_bit(SUSPEND_IO, &mdev->flags);\r\nif (is_susp(mdev->state))\r\nreturn;\r\nwait_event(mdev->misc_wait, !atomic_read(&mdev->ap_bio_cnt));\r\n}\r\nvoid drbd_resume_io(struct drbd_conf *mdev)\r\n{\r\nclear_bit(SUSPEND_IO, &mdev->flags);\r\nwake_up(&mdev->misc_wait);\r\n}\r\nenum determine_dev_size drbd_determine_dev_size(struct drbd_conf *mdev, enum dds_flags flags) __must_hold(local)\r\n{\r\nsector_t prev_first_sect, prev_size;\r\nsector_t la_size;\r\nsector_t size;\r\nchar ppb[10];\r\nint md_moved, la_size_changed;\r\nenum determine_dev_size rv = unchanged;\r\ndrbd_suspend_io(mdev);\r\nwait_event(mdev->al_wait, lc_try_lock(mdev->act_log));\r\nprev_first_sect = drbd_md_first_sector(mdev->ldev);\r\nprev_size = mdev->ldev->md.md_size_sect;\r\nla_size = mdev->ldev->md.la_size_sect;\r\ndrbd_md_set_sector_offsets(mdev, mdev->ldev);\r\nsize = drbd_new_dev_size(mdev, mdev->ldev, flags & DDSF_FORCED);\r\nif (drbd_get_capacity(mdev->this_bdev) != size ||\r\ndrbd_bm_capacity(mdev) != size) {\r\nint err;\r\nerr = drbd_bm_resize(mdev, size, !(flags & DDSF_NO_RESYNC));\r\nif (unlikely(err)) {\r\nsize = drbd_bm_capacity(mdev)>>1;\r\nif (size == 0) {\r\ndev_err(DEV, "OUT OF MEMORY! "\r\n"Could not allocate bitmap!\n");\r\n} else {\r\ndev_err(DEV, "BM resizing failed. "\r\n"Leaving size unchanged at size = %lu KB\n",\r\n(unsigned long)size);\r\n}\r\nrv = dev_size_error;\r\n}\r\ndrbd_set_my_capacity(mdev, size);\r\nmdev->ldev->md.la_size_sect = size;\r\ndev_info(DEV, "size = %s (%llu KB)\n", ppsize(ppb, size>>1),\r\n(unsigned long long)size>>1);\r\n}\r\nif (rv == dev_size_error)\r\ngoto out;\r\nla_size_changed = (la_size != mdev->ldev->md.la_size_sect);\r\nmd_moved = prev_first_sect != drbd_md_first_sector(mdev->ldev)\r\n|| prev_size != mdev->ldev->md.md_size_sect;\r\nif (la_size_changed || md_moved) {\r\nint err;\r\ndrbd_al_shrink(mdev);\r\ndev_info(DEV, "Writing the whole bitmap, %s\n",\r\nla_size_changed && md_moved ? "size changed and md moved" :\r\nla_size_changed ? "size changed" : "md moved");\r\nerr = drbd_bitmap_io(mdev, &drbd_bm_write,\r\n"size changed", BM_LOCKED_MASK);\r\nif (err) {\r\nrv = dev_size_error;\r\ngoto out;\r\n}\r\ndrbd_md_mark_dirty(mdev);\r\n}\r\nif (size > la_size)\r\nrv = grew;\r\nif (size < la_size)\r\nrv = shrunk;\r\nout:\r\nlc_unlock(mdev->act_log);\r\nwake_up(&mdev->al_wait);\r\ndrbd_resume_io(mdev);\r\nreturn rv;\r\n}\r\nsector_t\r\ndrbd_new_dev_size(struct drbd_conf *mdev, struct drbd_backing_dev *bdev, int assume_peer_has_space)\r\n{\r\nsector_t p_size = mdev->p_size;\r\nsector_t la_size = bdev->md.la_size_sect;\r\nsector_t m_size;\r\nsector_t u_size = bdev->dc.disk_size;\r\nsector_t size = 0;\r\nm_size = drbd_get_max_capacity(bdev);\r\nif (mdev->state.conn < C_CONNECTED && assume_peer_has_space) {\r\ndev_warn(DEV, "Resize while not connected was forced by the user!\n");\r\np_size = m_size;\r\n}\r\nif (p_size && m_size) {\r\nsize = min_t(sector_t, p_size, m_size);\r\n} else {\r\nif (la_size) {\r\nsize = la_size;\r\nif (m_size && m_size < size)\r\nsize = m_size;\r\nif (p_size && p_size < size)\r\nsize = p_size;\r\n} else {\r\nif (m_size)\r\nsize = m_size;\r\nif (p_size)\r\nsize = p_size;\r\n}\r\n}\r\nif (size == 0)\r\ndev_err(DEV, "Both nodes diskless!\n");\r\nif (u_size) {\r\nif (u_size > size)\r\ndev_err(DEV, "Requested disk size is too big (%lu > %lu)\n",\r\n(unsigned long)u_size>>1, (unsigned long)size>>1);\r\nelse\r\nsize = u_size;\r\n}\r\nreturn size;\r\n}\r\nstatic int drbd_check_al_size(struct drbd_conf *mdev)\r\n{\r\nstruct lru_cache *n, *t;\r\nstruct lc_element *e;\r\nunsigned int in_use;\r\nint i;\r\nERR_IF(mdev->sync_conf.al_extents < 7)\r\nmdev->sync_conf.al_extents = 127;\r\nif (mdev->act_log &&\r\nmdev->act_log->nr_elements == mdev->sync_conf.al_extents)\r\nreturn 0;\r\nin_use = 0;\r\nt = mdev->act_log;\r\nn = lc_create("act_log", drbd_al_ext_cache,\r\nmdev->sync_conf.al_extents, sizeof(struct lc_element), 0);\r\nif (n == NULL) {\r\ndev_err(DEV, "Cannot allocate act_log lru!\n");\r\nreturn -ENOMEM;\r\n}\r\nspin_lock_irq(&mdev->al_lock);\r\nif (t) {\r\nfor (i = 0; i < t->nr_elements; i++) {\r\ne = lc_element_by_index(t, i);\r\nif (e->refcnt)\r\ndev_err(DEV, "refcnt(%d)==%d\n",\r\ne->lc_number, e->refcnt);\r\nin_use += e->refcnt;\r\n}\r\n}\r\nif (!in_use)\r\nmdev->act_log = n;\r\nspin_unlock_irq(&mdev->al_lock);\r\nif (in_use) {\r\ndev_err(DEV, "Activity log still in use!\n");\r\nlc_destroy(n);\r\nreturn -EBUSY;\r\n} else {\r\nif (t)\r\nlc_destroy(t);\r\n}\r\ndrbd_md_mark_dirty(mdev);\r\nreturn 0;\r\n}\r\nstatic void drbd_setup_queue_param(struct drbd_conf *mdev, unsigned int max_bio_size)\r\n{\r\nstruct request_queue * const q = mdev->rq_queue;\r\nint max_hw_sectors = max_bio_size >> 9;\r\nint max_segments = 0;\r\nif (get_ldev_if_state(mdev, D_ATTACHING)) {\r\nstruct request_queue * const b = mdev->ldev->backing_bdev->bd_disk->queue;\r\nmax_hw_sectors = min(queue_max_hw_sectors(b), max_bio_size >> 9);\r\nmax_segments = mdev->ldev->dc.max_bio_bvecs;\r\nput_ldev(mdev);\r\n}\r\nblk_queue_logical_block_size(q, 512);\r\nblk_queue_max_hw_sectors(q, max_hw_sectors);\r\nblk_queue_max_segments(q, max_segments ? max_segments : BLK_MAX_SEGMENTS);\r\nblk_queue_segment_boundary(q, PAGE_CACHE_SIZE-1);\r\nif (get_ldev_if_state(mdev, D_ATTACHING)) {\r\nstruct request_queue * const b = mdev->ldev->backing_bdev->bd_disk->queue;\r\nblk_queue_stack_limits(q, b);\r\nif (q->backing_dev_info.ra_pages != b->backing_dev_info.ra_pages) {\r\ndev_info(DEV, "Adjusting my ra_pages to backing device's (%lu -> %lu)\n",\r\nq->backing_dev_info.ra_pages,\r\nb->backing_dev_info.ra_pages);\r\nq->backing_dev_info.ra_pages = b->backing_dev_info.ra_pages;\r\n}\r\nput_ldev(mdev);\r\n}\r\n}\r\nvoid drbd_reconsider_max_bio_size(struct drbd_conf *mdev)\r\n{\r\nint now, new, local, peer;\r\nnow = queue_max_hw_sectors(mdev->rq_queue) << 9;\r\nlocal = mdev->local_max_bio_size;\r\npeer = mdev->peer_max_bio_size;\r\nif (get_ldev_if_state(mdev, D_ATTACHING)) {\r\nlocal = queue_max_hw_sectors(mdev->ldev->backing_bdev->bd_disk->queue) << 9;\r\nmdev->local_max_bio_size = local;\r\nput_ldev(mdev);\r\n}\r\nif (mdev->state.conn >= C_CONNECTED) {\r\nif (mdev->agreed_pro_version < 94)\r\npeer = mdev->peer_max_bio_size;\r\nelse if (mdev->agreed_pro_version == 94)\r\npeer = DRBD_MAX_SIZE_H80_PACKET;\r\nelse\r\npeer = DRBD_MAX_BIO_SIZE;\r\n}\r\nnew = min_t(int, local, peer);\r\nif (mdev->state.role == R_PRIMARY && new < now)\r\ndev_err(DEV, "ASSERT FAILED new < now; (%d < %d)\n", new, now);\r\nif (new != now)\r\ndev_info(DEV, "max BIO size = %u\n", new);\r\ndrbd_setup_queue_param(mdev, new);\r\n}\r\nstatic void drbd_reconfig_start(struct drbd_conf *mdev)\r\n{\r\nwait_event(mdev->state_wait, !test_and_set_bit(CONFIG_PENDING, &mdev->flags));\r\nwait_event(mdev->state_wait, !test_bit(DEVICE_DYING, &mdev->flags));\r\ndrbd_thread_start(&mdev->worker);\r\ndrbd_flush_workqueue(mdev);\r\n}\r\nstatic void drbd_reconfig_done(struct drbd_conf *mdev)\r\n{\r\nspin_lock_irq(&mdev->req_lock);\r\nif (mdev->state.disk == D_DISKLESS &&\r\nmdev->state.conn == C_STANDALONE &&\r\nmdev->state.role == R_SECONDARY) {\r\nset_bit(DEVICE_DYING, &mdev->flags);\r\ndrbd_thread_stop_nowait(&mdev->worker);\r\n} else\r\nclear_bit(CONFIG_PENDING, &mdev->flags);\r\nspin_unlock_irq(&mdev->req_lock);\r\nwake_up(&mdev->state_wait);\r\n}\r\nstatic void drbd_suspend_al(struct drbd_conf *mdev)\r\n{\r\nint s = 0;\r\nif (lc_try_lock(mdev->act_log)) {\r\ndrbd_al_shrink(mdev);\r\nlc_unlock(mdev->act_log);\r\n} else {\r\ndev_warn(DEV, "Failed to lock al in drbd_suspend_al()\n");\r\nreturn;\r\n}\r\nspin_lock_irq(&mdev->req_lock);\r\nif (mdev->state.conn < C_CONNECTED)\r\ns = !test_and_set_bit(AL_SUSPENDED, &mdev->flags);\r\nspin_unlock_irq(&mdev->req_lock);\r\nif (s)\r\ndev_info(DEV, "Suspended AL updates\n");\r\n}\r\nstatic int drbd_nl_disk_conf(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nenum drbd_ret_code retcode;\r\nenum determine_dev_size dd;\r\nsector_t max_possible_sectors;\r\nsector_t min_md_device_sectors;\r\nstruct drbd_backing_dev *nbc = NULL;\r\nstruct block_device *bdev;\r\nstruct lru_cache *resync_lru = NULL;\r\nunion drbd_state ns, os;\r\nenum drbd_state_rv rv;\r\nint cp_discovered = 0;\r\nint logical_block_size;\r\ndrbd_reconfig_start(mdev);\r\nif (mdev->state.disk > D_DISKLESS) {\r\nretcode = ERR_DISK_CONFIGURED;\r\ngoto fail;\r\n}\r\nwait_event(mdev->misc_wait, !atomic_read(&mdev->local_cnt));\r\nnbc = kzalloc(sizeof(struct drbd_backing_dev), GFP_KERNEL);\r\nif (!nbc) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\nnbc->dc.disk_size = DRBD_DISK_SIZE_SECT_DEF;\r\nnbc->dc.on_io_error = DRBD_ON_IO_ERROR_DEF;\r\nnbc->dc.fencing = DRBD_FENCING_DEF;\r\nnbc->dc.max_bio_bvecs = DRBD_MAX_BIO_BVECS_DEF;\r\nif (!disk_conf_from_tags(mdev, nlp->tag_list, &nbc->dc)) {\r\nretcode = ERR_MANDATORY_TAG;\r\ngoto fail;\r\n}\r\nif (nbc->dc.meta_dev_idx < DRBD_MD_INDEX_FLEX_INT) {\r\nretcode = ERR_MD_IDX_INVALID;\r\ngoto fail;\r\n}\r\nif (get_net_conf(mdev)) {\r\nint prot = mdev->net_conf->wire_protocol;\r\nput_net_conf(mdev);\r\nif (nbc->dc.fencing == FP_STONITH && prot == DRBD_PROT_A) {\r\nretcode = ERR_STONITH_AND_PROT_A;\r\ngoto fail;\r\n}\r\n}\r\nbdev = blkdev_get_by_path(nbc->dc.backing_dev,\r\nFMODE_READ | FMODE_WRITE | FMODE_EXCL, mdev);\r\nif (IS_ERR(bdev)) {\r\ndev_err(DEV, "open(\"%s\") failed with %ld\n", nbc->dc.backing_dev,\r\nPTR_ERR(bdev));\r\nretcode = ERR_OPEN_DISK;\r\ngoto fail;\r\n}\r\nnbc->backing_bdev = bdev;\r\nbdev = blkdev_get_by_path(nbc->dc.meta_dev,\r\nFMODE_READ | FMODE_WRITE | FMODE_EXCL,\r\n(nbc->dc.meta_dev_idx < 0) ?\r\n(void *)mdev : (void *)drbd_m_holder);\r\nif (IS_ERR(bdev)) {\r\ndev_err(DEV, "open(\"%s\") failed with %ld\n", nbc->dc.meta_dev,\r\nPTR_ERR(bdev));\r\nretcode = ERR_OPEN_MD_DISK;\r\ngoto fail;\r\n}\r\nnbc->md_bdev = bdev;\r\nif ((nbc->backing_bdev == nbc->md_bdev) !=\r\n(nbc->dc.meta_dev_idx == DRBD_MD_INDEX_INTERNAL ||\r\nnbc->dc.meta_dev_idx == DRBD_MD_INDEX_FLEX_INT)) {\r\nretcode = ERR_MD_IDX_INVALID;\r\ngoto fail;\r\n}\r\nresync_lru = lc_create("resync", drbd_bm_ext_cache,\r\n61, sizeof(struct bm_extent),\r\noffsetof(struct bm_extent, lce));\r\nif (!resync_lru) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\ndrbd_md_set_sector_offsets(mdev, nbc);\r\nif (drbd_get_max_capacity(nbc) < nbc->dc.disk_size) {\r\ndev_err(DEV, "max capacity %llu smaller than disk size %llu\n",\r\n(unsigned long long) drbd_get_max_capacity(nbc),\r\n(unsigned long long) nbc->dc.disk_size);\r\nretcode = ERR_DISK_TO_SMALL;\r\ngoto fail;\r\n}\r\nif (nbc->dc.meta_dev_idx < 0) {\r\nmax_possible_sectors = DRBD_MAX_SECTORS_FLEX;\r\nmin_md_device_sectors = (2<<10);\r\n} else {\r\nmax_possible_sectors = DRBD_MAX_SECTORS;\r\nmin_md_device_sectors = MD_RESERVED_SECT * (nbc->dc.meta_dev_idx + 1);\r\n}\r\nif (drbd_get_capacity(nbc->md_bdev) < min_md_device_sectors) {\r\nretcode = ERR_MD_DISK_TO_SMALL;\r\ndev_warn(DEV, "refusing attach: md-device too small, "\r\n"at least %llu sectors needed for this meta-disk type\n",\r\n(unsigned long long) min_md_device_sectors);\r\ngoto fail;\r\n}\r\nif (drbd_get_max_capacity(nbc) <\r\ndrbd_get_capacity(mdev->this_bdev)) {\r\nretcode = ERR_DISK_TO_SMALL;\r\ngoto fail;\r\n}\r\nnbc->known_size = drbd_get_capacity(nbc->backing_bdev);\r\nif (nbc->known_size > max_possible_sectors) {\r\ndev_warn(DEV, "==> truncating very big lower level device "\r\n"to currently maximum possible %llu sectors <==\n",\r\n(unsigned long long) max_possible_sectors);\r\nif (nbc->dc.meta_dev_idx >= 0)\r\ndev_warn(DEV, "==>> using internal or flexible "\r\n"meta data may help <<==\n");\r\n}\r\ndrbd_suspend_io(mdev);\r\nwait_event(mdev->misc_wait, !atomic_read(&mdev->ap_pending_cnt) || is_susp(mdev->state));\r\ndrbd_flush_workqueue(mdev);\r\nrv = _drbd_request_state(mdev, NS(disk, D_ATTACHING), CS_VERBOSE);\r\nretcode = rv;\r\ndrbd_resume_io(mdev);\r\nif (rv < SS_SUCCESS)\r\ngoto fail;\r\nif (!get_ldev_if_state(mdev, D_ATTACHING))\r\ngoto force_diskless;\r\ndrbd_md_set_sector_offsets(mdev, nbc);\r\nlogical_block_size = bdev_logical_block_size(nbc->md_bdev);\r\nif (logical_block_size == 0)\r\nlogical_block_size = MD_SECTOR_SIZE;\r\nif (logical_block_size != MD_SECTOR_SIZE) {\r\nif (!mdev->md_io_tmpp) {\r\nstruct page *page = alloc_page(GFP_NOIO);\r\nif (!page)\r\ngoto force_diskless_dec;\r\ndev_warn(DEV, "Meta data's bdev logical_block_size = %d != %d\n",\r\nlogical_block_size, MD_SECTOR_SIZE);\r\ndev_warn(DEV, "Workaround engaged (has performance impact).\n");\r\nmdev->md_io_tmpp = page;\r\n}\r\n}\r\nif (!mdev->bitmap) {\r\nif (drbd_bm_init(mdev)) {\r\nretcode = ERR_NOMEM;\r\ngoto force_diskless_dec;\r\n}\r\n}\r\nretcode = drbd_md_read(mdev, nbc);\r\nif (retcode != NO_ERROR)\r\ngoto force_diskless_dec;\r\nif (mdev->state.conn < C_CONNECTED &&\r\nmdev->state.role == R_PRIMARY &&\r\n(mdev->ed_uuid & ~((u64)1)) != (nbc->md.uuid[UI_CURRENT] & ~((u64)1))) {\r\ndev_err(DEV, "Can only attach to data with current UUID=%016llX\n",\r\n(unsigned long long)mdev->ed_uuid);\r\nretcode = ERR_DATA_NOT_CURRENT;\r\ngoto force_diskless_dec;\r\n}\r\nif (drbd_check_al_size(mdev)) {\r\nretcode = ERR_NOMEM;\r\ngoto force_diskless_dec;\r\n}\r\nif (drbd_md_test_flag(nbc, MDF_CONSISTENT) &&\r\ndrbd_new_dev_size(mdev, nbc, 0) < nbc->md.la_size_sect) {\r\ndev_warn(DEV, "refusing to truncate a consistent device\n");\r\nretcode = ERR_DISK_TO_SMALL;\r\ngoto force_diskless_dec;\r\n}\r\nif (!drbd_al_read_log(mdev, nbc)) {\r\nretcode = ERR_IO_MD_DISK;\r\ngoto force_diskless_dec;\r\n}\r\nif (nbc->dc.no_md_flush)\r\nset_bit(MD_NO_FUA, &mdev->flags);\r\nelse\r\nclear_bit(MD_NO_FUA, &mdev->flags);\r\nD_ASSERT(mdev->ldev == NULL);\r\nmdev->ldev = nbc;\r\nmdev->resync = resync_lru;\r\nnbc = NULL;\r\nresync_lru = NULL;\r\nmdev->write_ordering = WO_bdev_flush;\r\ndrbd_bump_write_ordering(mdev, WO_bdev_flush);\r\nif (drbd_md_test_flag(mdev->ldev, MDF_CRASHED_PRIMARY))\r\nset_bit(CRASHED_PRIMARY, &mdev->flags);\r\nelse\r\nclear_bit(CRASHED_PRIMARY, &mdev->flags);\r\nif (drbd_md_test_flag(mdev->ldev, MDF_PRIMARY_IND) &&\r\n!(mdev->state.role == R_PRIMARY && mdev->state.susp_nod)) {\r\nset_bit(CRASHED_PRIMARY, &mdev->flags);\r\ncp_discovered = 1;\r\n}\r\nmdev->send_cnt = 0;\r\nmdev->recv_cnt = 0;\r\nmdev->read_cnt = 0;\r\nmdev->writ_cnt = 0;\r\ndrbd_reconsider_max_bio_size(mdev);\r\nclear_bit(USE_DEGR_WFC_T, &mdev->flags);\r\nif (mdev->state.role != R_PRIMARY &&\r\ndrbd_md_test_flag(mdev->ldev, MDF_PRIMARY_IND) &&\r\n!drbd_md_test_flag(mdev->ldev, MDF_CONNECTED_IND))\r\nset_bit(USE_DEGR_WFC_T, &mdev->flags);\r\ndd = drbd_determine_dev_size(mdev, 0);\r\nif (dd == dev_size_error) {\r\nretcode = ERR_NOMEM_BITMAP;\r\ngoto force_diskless_dec;\r\n} else if (dd == grew)\r\nset_bit(RESYNC_AFTER_NEG, &mdev->flags);\r\nif (drbd_md_test_flag(mdev->ldev, MDF_FULL_SYNC)) {\r\ndev_info(DEV, "Assuming that all blocks are out of sync "\r\n"(aka FullSync)\n");\r\nif (drbd_bitmap_io(mdev, &drbd_bmio_set_n_write,\r\n"set_n_write from attaching", BM_LOCKED_MASK)) {\r\nretcode = ERR_IO_MD_DISK;\r\ngoto force_diskless_dec;\r\n}\r\n} else {\r\nif (drbd_bitmap_io(mdev, &drbd_bm_read,\r\n"read from attaching", BM_LOCKED_MASK) < 0) {\r\nretcode = ERR_IO_MD_DISK;\r\ngoto force_diskless_dec;\r\n}\r\n}\r\nif (cp_discovered) {\r\ndrbd_al_apply_to_bm(mdev);\r\nif (drbd_bitmap_io(mdev, &drbd_bm_write,\r\n"crashed primary apply AL", BM_LOCKED_MASK)) {\r\nretcode = ERR_IO_MD_DISK;\r\ngoto force_diskless_dec;\r\n}\r\n}\r\nif (_drbd_bm_total_weight(mdev) == drbd_bm_bits(mdev))\r\ndrbd_suspend_al(mdev);\r\nspin_lock_irq(&mdev->req_lock);\r\nos = mdev->state;\r\nns.i = os.i;\r\nif (drbd_md_test_flag(mdev->ldev, MDF_CONSISTENT)) {\r\nif (drbd_md_test_flag(mdev->ldev, MDF_WAS_UP_TO_DATE))\r\nns.disk = D_CONSISTENT;\r\nelse\r\nns.disk = D_OUTDATED;\r\n} else {\r\nns.disk = D_INCONSISTENT;\r\n}\r\nif (drbd_md_test_flag(mdev->ldev, MDF_PEER_OUT_DATED))\r\nns.pdsk = D_OUTDATED;\r\nif ( ns.disk == D_CONSISTENT &&\r\n(ns.pdsk == D_OUTDATED || mdev->ldev->dc.fencing == FP_DONT_CARE))\r\nns.disk = D_UP_TO_DATE;\r\nif (mdev->state.conn == C_CONNECTED) {\r\nmdev->new_state_tmp.i = ns.i;\r\nns.i = os.i;\r\nns.disk = D_NEGOTIATING;\r\nkfree(mdev->p_uuid);\r\nmdev->p_uuid = NULL;\r\n}\r\nrv = _drbd_set_state(mdev, ns, CS_VERBOSE, NULL);\r\nns = mdev->state;\r\nspin_unlock_irq(&mdev->req_lock);\r\nif (rv < SS_SUCCESS)\r\ngoto force_diskless_dec;\r\nif (mdev->state.role == R_PRIMARY)\r\nmdev->ldev->md.uuid[UI_CURRENT] |= (u64)1;\r\nelse\r\nmdev->ldev->md.uuid[UI_CURRENT] &= ~(u64)1;\r\ndrbd_md_mark_dirty(mdev);\r\ndrbd_md_sync(mdev);\r\nkobject_uevent(&disk_to_dev(mdev->vdisk)->kobj, KOBJ_CHANGE);\r\nput_ldev(mdev);\r\nreply->ret_code = retcode;\r\ndrbd_reconfig_done(mdev);\r\nreturn 0;\r\nforce_diskless_dec:\r\nput_ldev(mdev);\r\nforce_diskless:\r\ndrbd_force_state(mdev, NS(disk, D_FAILED));\r\ndrbd_md_sync(mdev);\r\nfail:\r\nif (nbc) {\r\nif (nbc->backing_bdev)\r\nblkdev_put(nbc->backing_bdev,\r\nFMODE_READ | FMODE_WRITE | FMODE_EXCL);\r\nif (nbc->md_bdev)\r\nblkdev_put(nbc->md_bdev,\r\nFMODE_READ | FMODE_WRITE | FMODE_EXCL);\r\nkfree(nbc);\r\n}\r\nlc_destroy(resync_lru);\r\nreply->ret_code = retcode;\r\ndrbd_reconfig_done(mdev);\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_detach(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nenum drbd_ret_code retcode;\r\nint ret;\r\ndrbd_suspend_io(mdev);\r\nretcode = drbd_request_state(mdev, NS(disk, D_FAILED));\r\nret = wait_event_interruptible(mdev->misc_wait,\r\nmdev->state.disk != D_FAILED);\r\ndrbd_resume_io(mdev);\r\nif ((int)retcode == (int)SS_IS_DISKLESS)\r\nretcode = SS_NOTHING_TO_DO;\r\nif (ret)\r\nretcode = ERR_INTR;\r\nreply->ret_code = retcode;\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_net_conf(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nint i, ns;\r\nenum drbd_ret_code retcode;\r\nstruct net_conf *new_conf = NULL;\r\nstruct crypto_hash *tfm = NULL;\r\nstruct crypto_hash *integrity_w_tfm = NULL;\r\nstruct crypto_hash *integrity_r_tfm = NULL;\r\nstruct hlist_head *new_tl_hash = NULL;\r\nstruct hlist_head *new_ee_hash = NULL;\r\nstruct drbd_conf *odev;\r\nchar hmac_name[CRYPTO_MAX_ALG_NAME];\r\nvoid *int_dig_out = NULL;\r\nvoid *int_dig_in = NULL;\r\nvoid *int_dig_vv = NULL;\r\nstruct sockaddr *new_my_addr, *new_peer_addr, *taken_addr;\r\ndrbd_reconfig_start(mdev);\r\nif (mdev->state.conn > C_STANDALONE) {\r\nretcode = ERR_NET_CONFIGURED;\r\ngoto fail;\r\n}\r\nnew_conf = kzalloc(sizeof(struct net_conf), GFP_KERNEL);\r\nif (!new_conf) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\nnew_conf->timeout = DRBD_TIMEOUT_DEF;\r\nnew_conf->try_connect_int = DRBD_CONNECT_INT_DEF;\r\nnew_conf->ping_int = DRBD_PING_INT_DEF;\r\nnew_conf->max_epoch_size = DRBD_MAX_EPOCH_SIZE_DEF;\r\nnew_conf->max_buffers = DRBD_MAX_BUFFERS_DEF;\r\nnew_conf->unplug_watermark = DRBD_UNPLUG_WATERMARK_DEF;\r\nnew_conf->sndbuf_size = DRBD_SNDBUF_SIZE_DEF;\r\nnew_conf->rcvbuf_size = DRBD_RCVBUF_SIZE_DEF;\r\nnew_conf->ko_count = DRBD_KO_COUNT_DEF;\r\nnew_conf->after_sb_0p = DRBD_AFTER_SB_0P_DEF;\r\nnew_conf->after_sb_1p = DRBD_AFTER_SB_1P_DEF;\r\nnew_conf->after_sb_2p = DRBD_AFTER_SB_2P_DEF;\r\nnew_conf->want_lose = 0;\r\nnew_conf->two_primaries = 0;\r\nnew_conf->wire_protocol = DRBD_PROT_C;\r\nnew_conf->ping_timeo = DRBD_PING_TIMEO_DEF;\r\nnew_conf->rr_conflict = DRBD_RR_CONFLICT_DEF;\r\nnew_conf->on_congestion = DRBD_ON_CONGESTION_DEF;\r\nnew_conf->cong_extents = DRBD_CONG_EXTENTS_DEF;\r\nif (!net_conf_from_tags(mdev, nlp->tag_list, new_conf)) {\r\nretcode = ERR_MANDATORY_TAG;\r\ngoto fail;\r\n}\r\nif (new_conf->two_primaries\r\n&& (new_conf->wire_protocol != DRBD_PROT_C)) {\r\nretcode = ERR_NOT_PROTO_C;\r\ngoto fail;\r\n}\r\nif (get_ldev(mdev)) {\r\nenum drbd_fencing_p fp = mdev->ldev->dc.fencing;\r\nput_ldev(mdev);\r\nif (new_conf->wire_protocol == DRBD_PROT_A && fp == FP_STONITH) {\r\nretcode = ERR_STONITH_AND_PROT_A;\r\ngoto fail;\r\n}\r\n}\r\nif (new_conf->on_congestion != OC_BLOCK && new_conf->wire_protocol != DRBD_PROT_A) {\r\nretcode = ERR_CONG_NOT_PROTO_A;\r\ngoto fail;\r\n}\r\nif (mdev->state.role == R_PRIMARY && new_conf->want_lose) {\r\nretcode = ERR_DISCARD;\r\ngoto fail;\r\n}\r\nretcode = NO_ERROR;\r\nnew_my_addr = (struct sockaddr *)&new_conf->my_addr;\r\nnew_peer_addr = (struct sockaddr *)&new_conf->peer_addr;\r\nfor (i = 0; i < minor_count; i++) {\r\nodev = minor_to_mdev(i);\r\nif (!odev || odev == mdev)\r\ncontinue;\r\nif (get_net_conf(odev)) {\r\ntaken_addr = (struct sockaddr *)&odev->net_conf->my_addr;\r\nif (new_conf->my_addr_len == odev->net_conf->my_addr_len &&\r\n!memcmp(new_my_addr, taken_addr, new_conf->my_addr_len))\r\nretcode = ERR_LOCAL_ADDR;\r\ntaken_addr = (struct sockaddr *)&odev->net_conf->peer_addr;\r\nif (new_conf->peer_addr_len == odev->net_conf->peer_addr_len &&\r\n!memcmp(new_peer_addr, taken_addr, new_conf->peer_addr_len))\r\nretcode = ERR_PEER_ADDR;\r\nput_net_conf(odev);\r\nif (retcode != NO_ERROR)\r\ngoto fail;\r\n}\r\n}\r\nif (new_conf->cram_hmac_alg[0] != 0) {\r\nsnprintf(hmac_name, CRYPTO_MAX_ALG_NAME, "hmac(%s)",\r\nnew_conf->cram_hmac_alg);\r\ntfm = crypto_alloc_hash(hmac_name, 0, CRYPTO_ALG_ASYNC);\r\nif (IS_ERR(tfm)) {\r\ntfm = NULL;\r\nretcode = ERR_AUTH_ALG;\r\ngoto fail;\r\n}\r\nif (!drbd_crypto_is_hash(crypto_hash_tfm(tfm))) {\r\nretcode = ERR_AUTH_ALG_ND;\r\ngoto fail;\r\n}\r\n}\r\nif (new_conf->integrity_alg[0]) {\r\nintegrity_w_tfm = crypto_alloc_hash(new_conf->integrity_alg, 0, CRYPTO_ALG_ASYNC);\r\nif (IS_ERR(integrity_w_tfm)) {\r\nintegrity_w_tfm = NULL;\r\nretcode=ERR_INTEGRITY_ALG;\r\ngoto fail;\r\n}\r\nif (!drbd_crypto_is_hash(crypto_hash_tfm(integrity_w_tfm))) {\r\nretcode=ERR_INTEGRITY_ALG_ND;\r\ngoto fail;\r\n}\r\nintegrity_r_tfm = crypto_alloc_hash(new_conf->integrity_alg, 0, CRYPTO_ALG_ASYNC);\r\nif (IS_ERR(integrity_r_tfm)) {\r\nintegrity_r_tfm = NULL;\r\nretcode=ERR_INTEGRITY_ALG;\r\ngoto fail;\r\n}\r\n}\r\nns = new_conf->max_epoch_size/8;\r\nif (mdev->tl_hash_s != ns) {\r\nnew_tl_hash = kzalloc(ns*sizeof(void *), GFP_KERNEL);\r\nif (!new_tl_hash) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\n}\r\nns = new_conf->max_buffers/8;\r\nif (new_conf->two_primaries && (mdev->ee_hash_s != ns)) {\r\nnew_ee_hash = kzalloc(ns*sizeof(void *), GFP_KERNEL);\r\nif (!new_ee_hash) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\n}\r\n((char *)new_conf->shared_secret)[SHARED_SECRET_MAX-1] = 0;\r\nif (integrity_w_tfm) {\r\ni = crypto_hash_digestsize(integrity_w_tfm);\r\nint_dig_out = kmalloc(i, GFP_KERNEL);\r\nif (!int_dig_out) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\nint_dig_in = kmalloc(i, GFP_KERNEL);\r\nif (!int_dig_in) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\nint_dig_vv = kmalloc(i, GFP_KERNEL);\r\nif (!int_dig_vv) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\n}\r\nif (!mdev->bitmap) {\r\nif(drbd_bm_init(mdev)) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\n}\r\ndrbd_flush_workqueue(mdev);\r\nspin_lock_irq(&mdev->req_lock);\r\nif (mdev->net_conf != NULL) {\r\nretcode = ERR_NET_CONFIGURED;\r\nspin_unlock_irq(&mdev->req_lock);\r\ngoto fail;\r\n}\r\nmdev->net_conf = new_conf;\r\nmdev->send_cnt = 0;\r\nmdev->recv_cnt = 0;\r\nif (new_tl_hash) {\r\nkfree(mdev->tl_hash);\r\nmdev->tl_hash_s = mdev->net_conf->max_epoch_size/8;\r\nmdev->tl_hash = new_tl_hash;\r\n}\r\nif (new_ee_hash) {\r\nkfree(mdev->ee_hash);\r\nmdev->ee_hash_s = mdev->net_conf->max_buffers/8;\r\nmdev->ee_hash = new_ee_hash;\r\n}\r\ncrypto_free_hash(mdev->cram_hmac_tfm);\r\nmdev->cram_hmac_tfm = tfm;\r\ncrypto_free_hash(mdev->integrity_w_tfm);\r\nmdev->integrity_w_tfm = integrity_w_tfm;\r\ncrypto_free_hash(mdev->integrity_r_tfm);\r\nmdev->integrity_r_tfm = integrity_r_tfm;\r\nkfree(mdev->int_dig_out);\r\nkfree(mdev->int_dig_in);\r\nkfree(mdev->int_dig_vv);\r\nmdev->int_dig_out=int_dig_out;\r\nmdev->int_dig_in=int_dig_in;\r\nmdev->int_dig_vv=int_dig_vv;\r\nretcode = _drbd_set_state(_NS(mdev, conn, C_UNCONNECTED), CS_VERBOSE, NULL);\r\nspin_unlock_irq(&mdev->req_lock);\r\nkobject_uevent(&disk_to_dev(mdev->vdisk)->kobj, KOBJ_CHANGE);\r\nreply->ret_code = retcode;\r\ndrbd_reconfig_done(mdev);\r\nreturn 0;\r\nfail:\r\nkfree(int_dig_out);\r\nkfree(int_dig_in);\r\nkfree(int_dig_vv);\r\ncrypto_free_hash(tfm);\r\ncrypto_free_hash(integrity_w_tfm);\r\ncrypto_free_hash(integrity_r_tfm);\r\nkfree(new_tl_hash);\r\nkfree(new_ee_hash);\r\nkfree(new_conf);\r\nreply->ret_code = retcode;\r\ndrbd_reconfig_done(mdev);\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_disconnect(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nint retcode;\r\nstruct disconnect dc;\r\nmemset(&dc, 0, sizeof(struct disconnect));\r\nif (!disconnect_from_tags(mdev, nlp->tag_list, &dc)) {\r\nretcode = ERR_MANDATORY_TAG;\r\ngoto fail;\r\n}\r\nif (dc.force) {\r\nspin_lock_irq(&mdev->req_lock);\r\nif (mdev->state.conn >= C_WF_CONNECTION)\r\n_drbd_set_state(_NS(mdev, conn, C_DISCONNECTING), CS_HARD, NULL);\r\nspin_unlock_irq(&mdev->req_lock);\r\ngoto done;\r\n}\r\nretcode = _drbd_request_state(mdev, NS(conn, C_DISCONNECTING), CS_ORDERED);\r\nif (retcode == SS_NOTHING_TO_DO)\r\ngoto done;\r\nelse if (retcode == SS_ALREADY_STANDALONE)\r\ngoto done;\r\nelse if (retcode == SS_PRIMARY_NOP) {\r\nretcode = drbd_request_state(mdev, NS2(conn, C_DISCONNECTING,\r\npdsk, D_OUTDATED));\r\n} else if (retcode == SS_CW_FAILED_BY_PEER) {\r\nretcode = _drbd_request_state(mdev, NS2(conn, C_DISCONNECTING,\r\ndisk, D_OUTDATED),\r\nCS_ORDERED);\r\nif (retcode == SS_IS_DISKLESS || retcode == SS_LOWER_THAN_OUTDATED) {\r\ndrbd_force_state(mdev, NS(conn, C_DISCONNECTING));\r\nretcode = SS_SUCCESS;\r\n}\r\n}\r\nif (retcode < SS_SUCCESS)\r\ngoto fail;\r\nif (wait_event_interruptible(mdev->state_wait,\r\nmdev->state.conn != C_DISCONNECTING)) {\r\nretcode = ERR_INTR;\r\ngoto fail;\r\n}\r\ndone:\r\nretcode = NO_ERROR;\r\nfail:\r\ndrbd_md_sync(mdev);\r\nreply->ret_code = retcode;\r\nreturn 0;\r\n}\r\nvoid resync_after_online_grow(struct drbd_conf *mdev)\r\n{\r\nint iass;\r\ndev_info(DEV, "Resync of new storage after online grow\n");\r\nif (mdev->state.role != mdev->state.peer)\r\niass = (mdev->state.role == R_PRIMARY);\r\nelse\r\niass = test_bit(DISCARD_CONCURRENT, &mdev->flags);\r\nif (iass)\r\ndrbd_start_resync(mdev, C_SYNC_SOURCE);\r\nelse\r\n_drbd_request_state(mdev, NS(conn, C_WF_SYNC_UUID), CS_VERBOSE + CS_SERIALIZE);\r\n}\r\nstatic int drbd_nl_resize(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nstruct resize rs;\r\nint retcode = NO_ERROR;\r\nenum determine_dev_size dd;\r\nenum dds_flags ddsf;\r\nmemset(&rs, 0, sizeof(struct resize));\r\nif (!resize_from_tags(mdev, nlp->tag_list, &rs)) {\r\nretcode = ERR_MANDATORY_TAG;\r\ngoto fail;\r\n}\r\nif (mdev->state.conn > C_CONNECTED) {\r\nretcode = ERR_RESIZE_RESYNC;\r\ngoto fail;\r\n}\r\nif (mdev->state.role == R_SECONDARY &&\r\nmdev->state.peer == R_SECONDARY) {\r\nretcode = ERR_NO_PRIMARY;\r\ngoto fail;\r\n}\r\nif (!get_ldev(mdev)) {\r\nretcode = ERR_NO_DISK;\r\ngoto fail;\r\n}\r\nif (rs.no_resync && mdev->agreed_pro_version < 93) {\r\nretcode = ERR_NEED_APV_93;\r\ngoto fail;\r\n}\r\nif (mdev->ldev->known_size != drbd_get_capacity(mdev->ldev->backing_bdev))\r\nmdev->ldev->known_size = drbd_get_capacity(mdev->ldev->backing_bdev);\r\nmdev->ldev->dc.disk_size = (sector_t)rs.resize_size;\r\nddsf = (rs.resize_force ? DDSF_FORCED : 0) | (rs.no_resync ? DDSF_NO_RESYNC : 0);\r\ndd = drbd_determine_dev_size(mdev, ddsf);\r\ndrbd_md_sync(mdev);\r\nput_ldev(mdev);\r\nif (dd == dev_size_error) {\r\nretcode = ERR_NOMEM_BITMAP;\r\ngoto fail;\r\n}\r\nif (mdev->state.conn == C_CONNECTED) {\r\nif (dd == grew)\r\nset_bit(RESIZE_PENDING, &mdev->flags);\r\ndrbd_send_uuids(mdev);\r\ndrbd_send_sizes(mdev, 1, ddsf);\r\n}\r\nfail:\r\nreply->ret_code = retcode;\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_syncer_conf(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nint retcode = NO_ERROR;\r\nint err;\r\nint ovr;\r\nint rsr;\r\nstruct crypto_hash *verify_tfm = NULL;\r\nstruct crypto_hash *csums_tfm = NULL;\r\nstruct syncer_conf sc;\r\ncpumask_var_t new_cpu_mask;\r\nint *rs_plan_s = NULL;\r\nint fifo_size;\r\nif (!zalloc_cpumask_var(&new_cpu_mask, GFP_KERNEL)) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\nif (nlp->flags & DRBD_NL_SET_DEFAULTS) {\r\nmemset(&sc, 0, sizeof(struct syncer_conf));\r\nsc.rate = DRBD_RATE_DEF;\r\nsc.after = DRBD_AFTER_DEF;\r\nsc.al_extents = DRBD_AL_EXTENTS_DEF;\r\nsc.on_no_data = DRBD_ON_NO_DATA_DEF;\r\nsc.c_plan_ahead = DRBD_C_PLAN_AHEAD_DEF;\r\nsc.c_delay_target = DRBD_C_DELAY_TARGET_DEF;\r\nsc.c_fill_target = DRBD_C_FILL_TARGET_DEF;\r\nsc.c_max_rate = DRBD_C_MAX_RATE_DEF;\r\nsc.c_min_rate = DRBD_C_MIN_RATE_DEF;\r\n} else\r\nmemcpy(&sc, &mdev->sync_conf, sizeof(struct syncer_conf));\r\nif (!syncer_conf_from_tags(mdev, nlp->tag_list, &sc)) {\r\nretcode = ERR_MANDATORY_TAG;\r\ngoto fail;\r\n}\r\nrsr = ( mdev->state.conn == C_SYNC_SOURCE ||\r\nmdev->state.conn == C_SYNC_TARGET ||\r\nmdev->state.conn == C_PAUSED_SYNC_S ||\r\nmdev->state.conn == C_PAUSED_SYNC_T );\r\nif (rsr && strcmp(sc.csums_alg, mdev->sync_conf.csums_alg)) {\r\nretcode = ERR_CSUMS_RESYNC_RUNNING;\r\ngoto fail;\r\n}\r\nif (!rsr && sc.csums_alg[0]) {\r\ncsums_tfm = crypto_alloc_hash(sc.csums_alg, 0, CRYPTO_ALG_ASYNC);\r\nif (IS_ERR(csums_tfm)) {\r\ncsums_tfm = NULL;\r\nretcode = ERR_CSUMS_ALG;\r\ngoto fail;\r\n}\r\nif (!drbd_crypto_is_hash(crypto_hash_tfm(csums_tfm))) {\r\nretcode = ERR_CSUMS_ALG_ND;\r\ngoto fail;\r\n}\r\n}\r\novr = (mdev->state.conn == C_VERIFY_S || mdev->state.conn == C_VERIFY_T);\r\nif (ovr) {\r\nif (strcmp(sc.verify_alg, mdev->sync_conf.verify_alg)) {\r\nretcode = ERR_VERIFY_RUNNING;\r\ngoto fail;\r\n}\r\n}\r\nif (!ovr && sc.verify_alg[0]) {\r\nverify_tfm = crypto_alloc_hash(sc.verify_alg, 0, CRYPTO_ALG_ASYNC);\r\nif (IS_ERR(verify_tfm)) {\r\nverify_tfm = NULL;\r\nretcode = ERR_VERIFY_ALG;\r\ngoto fail;\r\n}\r\nif (!drbd_crypto_is_hash(crypto_hash_tfm(verify_tfm))) {\r\nretcode = ERR_VERIFY_ALG_ND;\r\ngoto fail;\r\n}\r\n}\r\nif (nr_cpu_ids > 1 && sc.cpu_mask[0] != 0) {\r\nerr = bitmap_parse(sc.cpu_mask, 32,\r\ncpumask_bits(new_cpu_mask), nr_cpu_ids);\r\nif (err) {\r\ndev_warn(DEV, "bitmap_parse() failed with %d\n", err);\r\nretcode = ERR_CPU_MASK_PARSE;\r\ngoto fail;\r\n}\r\n}\r\nERR_IF (sc.rate < 1) sc.rate = 1;\r\nERR_IF (sc.al_extents < 7) sc.al_extents = 127;\r\n#define AL_MAX ((MD_AL_MAX_SIZE-1) * AL_EXTENTS_PT)\r\nif (sc.al_extents > AL_MAX) {\r\ndev_err(DEV, "sc.al_extents > %d\n", AL_MAX);\r\nsc.al_extents = AL_MAX;\r\n}\r\n#undef AL_MAX\r\nif (sc.after >= 0)\r\nensure_mdev(sc.after, 1);\r\nretcode = drbd_alter_sa(mdev, sc.after);\r\nif (retcode != NO_ERROR)\r\ngoto fail;\r\nfifo_size = (sc.c_plan_ahead * 10 * SLEEP_TIME) / HZ;\r\nif (fifo_size != mdev->rs_plan_s.size && fifo_size > 0) {\r\nrs_plan_s = kzalloc(sizeof(int) * fifo_size, GFP_KERNEL);\r\nif (!rs_plan_s) {\r\ndev_err(DEV, "kmalloc of fifo_buffer failed");\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\n}\r\nspin_lock(&mdev->peer_seq_lock);\r\nmdev->sync_conf = sc;\r\nif (!rsr) {\r\ncrypto_free_hash(mdev->csums_tfm);\r\nmdev->csums_tfm = csums_tfm;\r\ncsums_tfm = NULL;\r\n}\r\nif (!ovr) {\r\ncrypto_free_hash(mdev->verify_tfm);\r\nmdev->verify_tfm = verify_tfm;\r\nverify_tfm = NULL;\r\n}\r\nif (fifo_size != mdev->rs_plan_s.size) {\r\nkfree(mdev->rs_plan_s.values);\r\nmdev->rs_plan_s.values = rs_plan_s;\r\nmdev->rs_plan_s.size = fifo_size;\r\nmdev->rs_planed = 0;\r\nrs_plan_s = NULL;\r\n}\r\nspin_unlock(&mdev->peer_seq_lock);\r\nif (get_ldev(mdev)) {\r\nwait_event(mdev->al_wait, lc_try_lock(mdev->act_log));\r\ndrbd_al_shrink(mdev);\r\nerr = drbd_check_al_size(mdev);\r\nlc_unlock(mdev->act_log);\r\nwake_up(&mdev->al_wait);\r\nput_ldev(mdev);\r\ndrbd_md_sync(mdev);\r\nif (err) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\n}\r\nif (mdev->state.conn >= C_CONNECTED)\r\ndrbd_send_sync_param(mdev, &sc);\r\nif (!cpumask_equal(mdev->cpu_mask, new_cpu_mask)) {\r\ncpumask_copy(mdev->cpu_mask, new_cpu_mask);\r\ndrbd_calc_cpu_mask(mdev);\r\nmdev->receiver.reset_cpu_mask = 1;\r\nmdev->asender.reset_cpu_mask = 1;\r\nmdev->worker.reset_cpu_mask = 1;\r\n}\r\nkobject_uevent(&disk_to_dev(mdev->vdisk)->kobj, KOBJ_CHANGE);\r\nfail:\r\nkfree(rs_plan_s);\r\nfree_cpumask_var(new_cpu_mask);\r\ncrypto_free_hash(csums_tfm);\r\ncrypto_free_hash(verify_tfm);\r\nreply->ret_code = retcode;\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_invalidate(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nint retcode;\r\nwait_event(mdev->misc_wait, !test_bit(BITMAP_IO, &mdev->flags));\r\nretcode = _drbd_request_state(mdev, NS(conn, C_STARTING_SYNC_T), CS_ORDERED);\r\nif (retcode < SS_SUCCESS && retcode != SS_NEED_CONNECTION)\r\nretcode = drbd_request_state(mdev, NS(conn, C_STARTING_SYNC_T));\r\nwhile (retcode == SS_NEED_CONNECTION) {\r\nspin_lock_irq(&mdev->req_lock);\r\nif (mdev->state.conn < C_CONNECTED)\r\nretcode = _drbd_set_state(_NS(mdev, disk, D_INCONSISTENT), CS_VERBOSE, NULL);\r\nspin_unlock_irq(&mdev->req_lock);\r\nif (retcode != SS_NEED_CONNECTION)\r\nbreak;\r\nretcode = drbd_request_state(mdev, NS(conn, C_STARTING_SYNC_T));\r\n}\r\nreply->ret_code = retcode;\r\nreturn 0;\r\n}\r\nstatic int drbd_bmio_set_susp_al(struct drbd_conf *mdev)\r\n{\r\nint rv;\r\nrv = drbd_bmio_set_n_write(mdev);\r\ndrbd_suspend_al(mdev);\r\nreturn rv;\r\n}\r\nstatic int drbd_nl_invalidate_peer(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nint retcode;\r\nwait_event(mdev->misc_wait, !test_bit(BITMAP_IO, &mdev->flags));\r\nretcode = _drbd_request_state(mdev, NS(conn, C_STARTING_SYNC_S), CS_ORDERED);\r\nif (retcode < SS_SUCCESS) {\r\nif (retcode == SS_NEED_CONNECTION && mdev->state.role == R_PRIMARY) {\r\nretcode = drbd_request_state(mdev, NS(pdsk, D_INCONSISTENT));\r\nif (retcode >= SS_SUCCESS) {\r\nif (drbd_bitmap_io(mdev, &drbd_bmio_set_susp_al,\r\n"set_n_write from invalidate_peer",\r\nBM_LOCKED_SET_ALLOWED))\r\nretcode = ERR_IO_MD_DISK;\r\n}\r\n} else\r\nretcode = drbd_request_state(mdev, NS(conn, C_STARTING_SYNC_S));\r\n}\r\nreply->ret_code = retcode;\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_pause_sync(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nint retcode = NO_ERROR;\r\nif (drbd_request_state(mdev, NS(user_isp, 1)) == SS_NOTHING_TO_DO)\r\nretcode = ERR_PAUSE_IS_SET;\r\nreply->ret_code = retcode;\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_resume_sync(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nint retcode = NO_ERROR;\r\nunion drbd_state s;\r\nif (drbd_request_state(mdev, NS(user_isp, 0)) == SS_NOTHING_TO_DO) {\r\ns = mdev->state;\r\nif (s.conn == C_PAUSED_SYNC_S || s.conn == C_PAUSED_SYNC_T) {\r\nretcode = s.aftr_isp ? ERR_PIC_AFTER_DEP :\r\ns.peer_isp ? ERR_PIC_PEER_DEP : ERR_PAUSE_IS_CLEAR;\r\n} else {\r\nretcode = ERR_PAUSE_IS_CLEAR;\r\n}\r\n}\r\nreply->ret_code = retcode;\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_suspend_io(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nreply->ret_code = drbd_request_state(mdev, NS(susp, 1));\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_resume_io(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nif (test_bit(NEW_CUR_UUID, &mdev->flags)) {\r\ndrbd_uuid_new_current(mdev);\r\nclear_bit(NEW_CUR_UUID, &mdev->flags);\r\n}\r\ndrbd_suspend_io(mdev);\r\nreply->ret_code = drbd_request_state(mdev, NS3(susp, 0, susp_nod, 0, susp_fen, 0));\r\nif (reply->ret_code == SS_SUCCESS) {\r\nif (mdev->state.conn < C_CONNECTED)\r\ntl_clear(mdev);\r\nif (mdev->state.disk == D_DISKLESS || mdev->state.disk == D_FAILED)\r\ntl_restart(mdev, fail_frozen_disk_io);\r\n}\r\ndrbd_resume_io(mdev);\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_outdate(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nreply->ret_code = drbd_request_state(mdev, NS(disk, D_OUTDATED));\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_get_config(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nunsigned short *tl;\r\ntl = reply->tag_list;\r\nif (get_ldev(mdev)) {\r\ntl = disk_conf_to_tags(mdev, &mdev->ldev->dc, tl);\r\nput_ldev(mdev);\r\n}\r\nif (get_net_conf(mdev)) {\r\ntl = net_conf_to_tags(mdev, mdev->net_conf, tl);\r\nput_net_conf(mdev);\r\n}\r\ntl = syncer_conf_to_tags(mdev, &mdev->sync_conf, tl);\r\nput_unaligned(TT_END, tl++);\r\nreturn (int)((char *)tl - (char *)reply->tag_list);\r\n}\r\nstatic int drbd_nl_get_state(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nunsigned short *tl = reply->tag_list;\r\nunion drbd_state s = mdev->state;\r\nunsigned long rs_left;\r\nunsigned int res;\r\ntl = get_state_to_tags(mdev, (struct get_state *)&s, tl);\r\nif (s.conn >= C_SYNC_SOURCE && s.conn <= C_PAUSED_SYNC_T) {\r\nif (get_ldev(mdev)) {\r\ndrbd_get_syncer_progress(mdev, &rs_left, &res);\r\ntl = tl_add_int(tl, T_sync_progress, &res);\r\nput_ldev(mdev);\r\n}\r\n}\r\nput_unaligned(TT_END, tl++);\r\nreturn (int)((char *)tl - (char *)reply->tag_list);\r\n}\r\nstatic int drbd_nl_get_uuids(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nunsigned short *tl;\r\ntl = reply->tag_list;\r\nif (get_ldev(mdev)) {\r\ntl = tl_add_blob(tl, T_uuids, mdev->ldev->md.uuid, UI_SIZE*sizeof(u64));\r\ntl = tl_add_int(tl, T_uuids_flags, &mdev->ldev->md.flags);\r\nput_ldev(mdev);\r\n}\r\nput_unaligned(TT_END, tl++);\r\nreturn (int)((char *)tl - (char *)reply->tag_list);\r\n}\r\nstatic int drbd_nl_get_timeout_flag(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nunsigned short *tl;\r\nchar rv;\r\ntl = reply->tag_list;\r\nrv = mdev->state.pdsk == D_OUTDATED ? UT_PEER_OUTDATED :\r\ntest_bit(USE_DEGR_WFC_T, &mdev->flags) ? UT_DEGRADED : UT_DEFAULT;\r\ntl = tl_add_blob(tl, T_use_degraded, &rv, sizeof(rv));\r\nput_unaligned(TT_END, tl++);\r\nreturn (int)((char *)tl - (char *)reply->tag_list);\r\n}\r\nstatic int drbd_nl_start_ov(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nstruct start_ov args =\r\n{ .start_sector = mdev->ov_start_sector };\r\nif (!start_ov_from_tags(mdev, nlp->tag_list, &args)) {\r\nreply->ret_code = ERR_MANDATORY_TAG;\r\nreturn 0;\r\n}\r\nwait_event(mdev->misc_wait, !test_bit(BITMAP_IO, &mdev->flags));\r\nmdev->ov_start_sector = args.start_sector & ~BM_SECT_PER_BIT;\r\nreply->ret_code = drbd_request_state(mdev,NS(conn,C_VERIFY_S));\r\nreturn 0;\r\n}\r\nstatic int drbd_nl_new_c_uuid(struct drbd_conf *mdev, struct drbd_nl_cfg_req *nlp,\r\nstruct drbd_nl_cfg_reply *reply)\r\n{\r\nint retcode = NO_ERROR;\r\nint skip_initial_sync = 0;\r\nint err;\r\nstruct new_c_uuid args;\r\nmemset(&args, 0, sizeof(struct new_c_uuid));\r\nif (!new_c_uuid_from_tags(mdev, nlp->tag_list, &args)) {\r\nreply->ret_code = ERR_MANDATORY_TAG;\r\nreturn 0;\r\n}\r\nmutex_lock(&mdev->state_mutex);\r\nif (!get_ldev(mdev)) {\r\nretcode = ERR_NO_DISK;\r\ngoto out;\r\n}\r\nif (mdev->state.conn == C_CONNECTED && mdev->agreed_pro_version >= 90 &&\r\nmdev->ldev->md.uuid[UI_CURRENT] == UUID_JUST_CREATED && args.clear_bm) {\r\ndev_info(DEV, "Preparing to skip initial sync\n");\r\nskip_initial_sync = 1;\r\n} else if (mdev->state.conn != C_STANDALONE) {\r\nretcode = ERR_CONNECTED;\r\ngoto out_dec;\r\n}\r\ndrbd_uuid_set(mdev, UI_BITMAP, 0);\r\ndrbd_uuid_new_current(mdev);\r\nif (args.clear_bm) {\r\nerr = drbd_bitmap_io(mdev, &drbd_bmio_clear_n_write,\r\n"clear_n_write from new_c_uuid", BM_LOCKED_MASK);\r\nif (err) {\r\ndev_err(DEV, "Writing bitmap failed with %d\n",err);\r\nretcode = ERR_IO_MD_DISK;\r\n}\r\nif (skip_initial_sync) {\r\ndrbd_send_uuids_skip_initial_sync(mdev);\r\n_drbd_uuid_set(mdev, UI_BITMAP, 0);\r\ndrbd_print_uuids(mdev, "cleared bitmap UUID");\r\nspin_lock_irq(&mdev->req_lock);\r\n_drbd_set_state(_NS2(mdev, disk, D_UP_TO_DATE, pdsk, D_UP_TO_DATE),\r\nCS_VERBOSE, NULL);\r\nspin_unlock_irq(&mdev->req_lock);\r\n}\r\n}\r\ndrbd_md_sync(mdev);\r\nout_dec:\r\nput_ldev(mdev);\r\nout:\r\nmutex_unlock(&mdev->state_mutex);\r\nreply->ret_code = retcode;\r\nreturn 0;\r\n}\r\nstatic void drbd_connector_callback(struct cn_msg *req, struct netlink_skb_parms *nsp)\r\n{\r\nstruct drbd_nl_cfg_req *nlp = (struct drbd_nl_cfg_req *)req->data;\r\nstruct cn_handler_struct *cm;\r\nstruct cn_msg *cn_reply;\r\nstruct drbd_nl_cfg_reply *reply;\r\nstruct drbd_conf *mdev;\r\nint retcode, rr;\r\nint reply_size = sizeof(struct cn_msg)\r\n+ sizeof(struct drbd_nl_cfg_reply)\r\n+ sizeof(short int);\r\nif (!try_module_get(THIS_MODULE)) {\r\nprintk(KERN_ERR "drbd: try_module_get() failed!\n");\r\nreturn;\r\n}\r\nif (!cap_raised(current_cap(), CAP_SYS_ADMIN)) {\r\nretcode = ERR_PERM;\r\ngoto fail;\r\n}\r\nmdev = ensure_mdev(nlp->drbd_minor,\r\n(nlp->flags & DRBD_NL_CREATE_DEVICE));\r\nif (!mdev) {\r\nretcode = ERR_MINOR_INVALID;\r\ngoto fail;\r\n}\r\nif (nlp->packet_type >= P_nl_after_last_packet ||\r\nnlp->packet_type == P_return_code_only) {\r\nretcode = ERR_PACKET_NR;\r\ngoto fail;\r\n}\r\ncm = cnd_table + nlp->packet_type;\r\nif (cm->function == NULL) {\r\nretcode = ERR_PACKET_NR;\r\ngoto fail;\r\n}\r\nreply_size += cm->reply_body_size;\r\ncn_reply = kzalloc(reply_size, GFP_KERNEL);\r\nif (!cn_reply) {\r\nretcode = ERR_NOMEM;\r\ngoto fail;\r\n}\r\nreply = (struct drbd_nl_cfg_reply *) cn_reply->data;\r\nreply->packet_type =\r\ncm->reply_body_size ? nlp->packet_type : P_return_code_only;\r\nreply->minor = nlp->drbd_minor;\r\nreply->ret_code = NO_ERROR;\r\nrr = cm->function(mdev, nlp, reply);\r\ncn_reply->id = req->id;\r\ncn_reply->seq = req->seq;\r\ncn_reply->ack = req->ack + 1;\r\ncn_reply->len = sizeof(struct drbd_nl_cfg_reply) + rr;\r\ncn_reply->flags = 0;\r\nrr = cn_netlink_send(cn_reply, CN_IDX_DRBD, GFP_KERNEL);\r\nif (rr && rr != -ESRCH)\r\nprintk(KERN_INFO "drbd: cn_netlink_send()=%d\n", rr);\r\nkfree(cn_reply);\r\nmodule_put(THIS_MODULE);\r\nreturn;\r\nfail:\r\ndrbd_nl_send_reply(req, retcode);\r\nmodule_put(THIS_MODULE);\r\n}\r\nstatic unsigned short *\r\n__tl_add_blob(unsigned short *tl, enum drbd_tags tag, const void *data,\r\nunsigned short len, int nul_terminated)\r\n{\r\nunsigned short l = tag_descriptions[tag_number(tag)].max_len;\r\nlen = (len < l) ? len : l;\r\nput_unaligned(tag, tl++);\r\nput_unaligned(len, tl++);\r\nmemcpy(tl, data, len);\r\ntl = (unsigned short*)((char*)tl + len);\r\nif (nul_terminated)\r\n*((char*)tl - 1) = 0;\r\nreturn tl;\r\n}\r\nstatic unsigned short *\r\ntl_add_blob(unsigned short *tl, enum drbd_tags tag, const void *data, int len)\r\n{\r\nreturn __tl_add_blob(tl, tag, data, len, 0);\r\n}\r\nstatic unsigned short *\r\ntl_add_str(unsigned short *tl, enum drbd_tags tag, const char *str)\r\n{\r\nreturn __tl_add_blob(tl, tag, str, strlen(str)+1, 0);\r\n}\r\nstatic unsigned short *\r\ntl_add_int(unsigned short *tl, enum drbd_tags tag, const void *val)\r\n{\r\nput_unaligned(tag, tl++);\r\nswitch(tag_type(tag)) {\r\ncase TT_INTEGER:\r\nput_unaligned(sizeof(int), tl++);\r\nput_unaligned(*(int *)val, (int *)tl);\r\ntl = (unsigned short*)((char*)tl+sizeof(int));\r\nbreak;\r\ncase TT_INT64:\r\nput_unaligned(sizeof(u64), tl++);\r\nput_unaligned(*(u64 *)val, (u64 *)tl);\r\ntl = (unsigned short*)((char*)tl+sizeof(u64));\r\nbreak;\r\ndefault:\r\n;\r\n}\r\nreturn tl;\r\n}\r\nvoid drbd_bcast_state(struct drbd_conf *mdev, union drbd_state state)\r\n{\r\nchar buffer[sizeof(struct cn_msg)+\r\nsizeof(struct drbd_nl_cfg_reply)+\r\nsizeof(struct get_state_tag_len_struct)+\r\nsizeof(short int)];\r\nstruct cn_msg *cn_reply = (struct cn_msg *) buffer;\r\nstruct drbd_nl_cfg_reply *reply =\r\n(struct drbd_nl_cfg_reply *)cn_reply->data;\r\nunsigned short *tl = reply->tag_list;\r\ntl = get_state_to_tags(mdev, (struct get_state *)&state, tl);\r\nput_unaligned(TT_END, tl++);\r\ncn_reply->id.idx = CN_IDX_DRBD;\r\ncn_reply->id.val = CN_VAL_DRBD;\r\ncn_reply->seq = atomic_add_return(1, &drbd_nl_seq);\r\ncn_reply->ack = 0;\r\ncn_reply->len = sizeof(struct drbd_nl_cfg_reply) +\r\n(int)((char *)tl - (char *)reply->tag_list);\r\ncn_reply->flags = 0;\r\nreply->packet_type = P_get_state;\r\nreply->minor = mdev_to_minor(mdev);\r\nreply->ret_code = NO_ERROR;\r\ncn_netlink_send(cn_reply, CN_IDX_DRBD, GFP_NOIO);\r\n}\r\nvoid drbd_bcast_ev_helper(struct drbd_conf *mdev, char *helper_name)\r\n{\r\nchar buffer[sizeof(struct cn_msg)+\r\nsizeof(struct drbd_nl_cfg_reply)+\r\nsizeof(struct call_helper_tag_len_struct)+\r\nsizeof(short int)];\r\nstruct cn_msg *cn_reply = (struct cn_msg *) buffer;\r\nstruct drbd_nl_cfg_reply *reply =\r\n(struct drbd_nl_cfg_reply *)cn_reply->data;\r\nunsigned short *tl = reply->tag_list;\r\ntl = tl_add_str(tl, T_helper, helper_name);\r\nput_unaligned(TT_END, tl++);\r\ncn_reply->id.idx = CN_IDX_DRBD;\r\ncn_reply->id.val = CN_VAL_DRBD;\r\ncn_reply->seq = atomic_add_return(1, &drbd_nl_seq);\r\ncn_reply->ack = 0;\r\ncn_reply->len = sizeof(struct drbd_nl_cfg_reply) +\r\n(int)((char *)tl - (char *)reply->tag_list);\r\ncn_reply->flags = 0;\r\nreply->packet_type = P_call_helper;\r\nreply->minor = mdev_to_minor(mdev);\r\nreply->ret_code = NO_ERROR;\r\ncn_netlink_send(cn_reply, CN_IDX_DRBD, GFP_NOIO);\r\n}\r\nvoid drbd_bcast_ee(struct drbd_conf *mdev,\r\nconst char *reason, const int dgs,\r\nconst char* seen_hash, const char* calc_hash,\r\nconst struct drbd_epoch_entry* e)\r\n{\r\nstruct cn_msg *cn_reply;\r\nstruct drbd_nl_cfg_reply *reply;\r\nunsigned short *tl;\r\nstruct page *page;\r\nunsigned len;\r\nif (!e)\r\nreturn;\r\nif (!reason || !reason[0])\r\nreturn;\r\ncn_reply = kzalloc(\r\nsizeof(struct cn_msg)+\r\nsizeof(struct drbd_nl_cfg_reply)+\r\nsizeof(struct dump_ee_tag_len_struct)+\r\nsizeof(short int),\r\nGFP_NOIO);\r\nif (!cn_reply) {\r\ndev_err(DEV, "could not kmalloc buffer for drbd_bcast_ee, sector %llu, size %u\n",\r\n(unsigned long long)e->sector, e->size);\r\nreturn;\r\n}\r\nreply = (struct drbd_nl_cfg_reply*)cn_reply->data;\r\ntl = reply->tag_list;\r\ntl = tl_add_str(tl, T_dump_ee_reason, reason);\r\ntl = tl_add_blob(tl, T_seen_digest, seen_hash, dgs);\r\ntl = tl_add_blob(tl, T_calc_digest, calc_hash, dgs);\r\ntl = tl_add_int(tl, T_ee_sector, &e->sector);\r\ntl = tl_add_int(tl, T_ee_block_id, &e->block_id);\r\nlen = min_t(unsigned, e->size, 32 << 10);\r\nput_unaligned(T_ee_data, tl++);\r\nput_unaligned(len, tl++);\r\npage = e->pages;\r\npage_chain_for_each(page) {\r\nvoid *d = kmap_atomic(page, KM_USER0);\r\nunsigned l = min_t(unsigned, len, PAGE_SIZE);\r\nmemcpy(tl, d, l);\r\nkunmap_atomic(d, KM_USER0);\r\ntl = (unsigned short*)((char*)tl + l);\r\nlen -= l;\r\nif (len == 0)\r\nbreak;\r\n}\r\nput_unaligned(TT_END, tl++);\r\ncn_reply->id.idx = CN_IDX_DRBD;\r\ncn_reply->id.val = CN_VAL_DRBD;\r\ncn_reply->seq = atomic_add_return(1,&drbd_nl_seq);\r\ncn_reply->ack = 0;\r\ncn_reply->len = sizeof(struct drbd_nl_cfg_reply) +\r\n(int)((char*)tl - (char*)reply->tag_list);\r\ncn_reply->flags = 0;\r\nreply->packet_type = P_dump_ee;\r\nreply->minor = mdev_to_minor(mdev);\r\nreply->ret_code = NO_ERROR;\r\ncn_netlink_send(cn_reply, CN_IDX_DRBD, GFP_NOIO);\r\nkfree(cn_reply);\r\n}\r\nvoid drbd_bcast_sync_progress(struct drbd_conf *mdev)\r\n{\r\nchar buffer[sizeof(struct cn_msg)+\r\nsizeof(struct drbd_nl_cfg_reply)+\r\nsizeof(struct sync_progress_tag_len_struct)+\r\nsizeof(short int)];\r\nstruct cn_msg *cn_reply = (struct cn_msg *) buffer;\r\nstruct drbd_nl_cfg_reply *reply =\r\n(struct drbd_nl_cfg_reply *)cn_reply->data;\r\nunsigned short *tl = reply->tag_list;\r\nunsigned long rs_left;\r\nunsigned int res;\r\nif (!get_ldev(mdev))\r\nreturn;\r\ndrbd_get_syncer_progress(mdev, &rs_left, &res);\r\nput_ldev(mdev);\r\ntl = tl_add_int(tl, T_sync_progress, &res);\r\nput_unaligned(TT_END, tl++);\r\ncn_reply->id.idx = CN_IDX_DRBD;\r\ncn_reply->id.val = CN_VAL_DRBD;\r\ncn_reply->seq = atomic_add_return(1, &drbd_nl_seq);\r\ncn_reply->ack = 0;\r\ncn_reply->len = sizeof(struct drbd_nl_cfg_reply) +\r\n(int)((char *)tl - (char *)reply->tag_list);\r\ncn_reply->flags = 0;\r\nreply->packet_type = P_sync_progress;\r\nreply->minor = mdev_to_minor(mdev);\r\nreply->ret_code = NO_ERROR;\r\ncn_netlink_send(cn_reply, CN_IDX_DRBD, GFP_NOIO);\r\n}\r\nint __init drbd_nl_init(void)\r\n{\r\nstatic struct cb_id cn_id_drbd;\r\nint err, try=10;\r\ncn_id_drbd.val = CN_VAL_DRBD;\r\ndo {\r\ncn_id_drbd.idx = cn_idx;\r\nerr = cn_add_callback(&cn_id_drbd, "cn_drbd", &drbd_connector_callback);\r\nif (!err)\r\nbreak;\r\ncn_idx = (cn_idx + CN_IDX_STEP);\r\n} while (try--);\r\nif (err) {\r\nprintk(KERN_ERR "drbd: cn_drbd failed to register\n");\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nvoid drbd_nl_cleanup(void)\r\n{\r\nstatic struct cb_id cn_id_drbd;\r\ncn_id_drbd.idx = cn_idx;\r\ncn_id_drbd.val = CN_VAL_DRBD;\r\ncn_del_callback(&cn_id_drbd);\r\n}\r\nvoid drbd_nl_send_reply(struct cn_msg *req, int ret_code)\r\n{\r\nchar buffer[sizeof(struct cn_msg)+sizeof(struct drbd_nl_cfg_reply)];\r\nstruct cn_msg *cn_reply = (struct cn_msg *) buffer;\r\nstruct drbd_nl_cfg_reply *reply =\r\n(struct drbd_nl_cfg_reply *)cn_reply->data;\r\nint rr;\r\nmemset(buffer, 0, sizeof(buffer));\r\ncn_reply->id = req->id;\r\ncn_reply->seq = req->seq;\r\ncn_reply->ack = req->ack + 1;\r\ncn_reply->len = sizeof(struct drbd_nl_cfg_reply);\r\ncn_reply->flags = 0;\r\nreply->packet_type = P_return_code_only;\r\nreply->minor = ((struct drbd_nl_cfg_req *)req->data)->drbd_minor;\r\nreply->ret_code = ret_code;\r\nrr = cn_netlink_send(cn_reply, CN_IDX_DRBD, GFP_NOIO);\r\nif (rr && rr != -ESRCH)\r\nprintk(KERN_INFO "drbd: cn_netlink_send()=%d\n", rr);\r\n}
