static inline u32 arch_spin_read_noalloc(void *lock)\r\n{\r\nreturn atomic_cmpxchg((atomic_t *)lock, -1, -1);\r\n}\r\nvoid arch_spin_lock_slow(arch_spinlock_t *lock, u32 my_ticket)\r\n{\r\nif (unlikely(my_ticket & __ARCH_SPIN_NEXT_OVERFLOW)) {\r\n__insn_fetchand4(&lock->lock, ~__ARCH_SPIN_NEXT_OVERFLOW);\r\nmy_ticket &= ~__ARCH_SPIN_NEXT_OVERFLOW;\r\n}\r\nfor (;;) {\r\nu32 val = arch_spin_read_noalloc(lock);\r\nu32 delta = my_ticket - arch_spin_current(val);\r\nif (delta == 0)\r\nreturn;\r\nrelax((128 / CYCLES_PER_RELAX_LOOP) * delta);\r\n}\r\n}\r\nint arch_spin_trylock(arch_spinlock_t *lock)\r\n{\r\nu32 val = arch_spin_read_noalloc(lock);\r\nif (unlikely(arch_spin_current(val) != arch_spin_next(val)))\r\nreturn 0;\r\nreturn cmpxchg(&lock->lock, val, (val + 1) & ~__ARCH_SPIN_NEXT_OVERFLOW)\r\n== val;\r\n}\r\nvoid arch_spin_unlock_wait(arch_spinlock_t *lock)\r\n{\r\nu32 iterations = 0;\r\nwhile (arch_spin_is_locked(lock))\r\ndelay_backoff(iterations++);\r\n}\r\nvoid __read_lock_failed(arch_rwlock_t *rw)\r\n{\r\nu32 val;\r\nint iterations = 0;\r\ndo {\r\ndelay_backoff(iterations++);\r\nval = __insn_fetchaddgez4(&rw->lock, 1);\r\n} while (unlikely(arch_write_val_locked(val)));\r\n}\r\nvoid __write_lock_failed(arch_rwlock_t *rw, u32 val)\r\n{\r\nint iterations = 0;\r\ndo {\r\nif (!arch_write_val_locked(val))\r\nval = __insn_fetchand4(&rw->lock, ~__WRITE_LOCK_BIT);\r\ndelay_backoff(iterations++);\r\nval = __insn_fetchor4(&rw->lock, __WRITE_LOCK_BIT);\r\n} while (val != 0);\r\n}
