static inline int local_sid_setup_one(struct id *entry)\r\n{\r\nunsigned long sid;\r\nint ret = -1;\r\nsid = ++(__get_cpu_var(pcpu_last_used_sid));\r\nif (sid < NUM_TIDS) {\r\n__get_cpu_var(pcpu_sids).entry[sid] = entry;\r\nentry->val = sid;\r\nentry->pentry = &__get_cpu_var(pcpu_sids).entry[sid];\r\nret = sid;\r\n}\r\nWARN_ON(sid > NUM_TIDS);\r\nreturn ret;\r\n}\r\nstatic inline int local_sid_lookup(struct id *entry)\r\n{\r\nif (entry && entry->val != 0 &&\r\n__get_cpu_var(pcpu_sids).entry[entry->val] == entry &&\r\nentry->pentry == &__get_cpu_var(pcpu_sids).entry[entry->val])\r\nreturn entry->val;\r\nreturn -1;\r\n}\r\nstatic inline void local_sid_destroy_all(void)\r\n{\r\npreempt_disable();\r\n__get_cpu_var(pcpu_last_used_sid) = 0;\r\nmemset(&__get_cpu_var(pcpu_sids), 0, sizeof(__get_cpu_var(pcpu_sids)));\r\npreempt_enable();\r\n}\r\nstatic void *kvmppc_e500_id_table_alloc(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nvcpu_e500->idt = kzalloc(sizeof(struct vcpu_id_table), GFP_KERNEL);\r\nreturn vcpu_e500->idt;\r\n}\r\nstatic void kvmppc_e500_id_table_free(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nkfree(vcpu_e500->idt);\r\n}\r\nstatic void kvmppc_e500_id_table_reset_all(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nmemset(vcpu_e500->idt, 0, sizeof(struct vcpu_id_table));\r\nkvmppc_e500_recalc_shadow_pid(vcpu_e500);\r\n}\r\nstatic inline void kvmppc_e500_id_table_reset_one(\r\nstruct kvmppc_vcpu_e500 *vcpu_e500,\r\nint as, int pid, int pr)\r\n{\r\nstruct vcpu_id_table *idt = vcpu_e500->idt;\r\nBUG_ON(as >= 2);\r\nBUG_ON(pid >= NUM_TIDS);\r\nBUG_ON(pr >= 2);\r\nidt->id[as][pid][pr].val = 0;\r\nidt->id[as][pid][pr].pentry = NULL;\r\nkvmppc_e500_recalc_shadow_pid(vcpu_e500);\r\n}\r\nstatic unsigned int kvmppc_e500_get_sid(struct kvmppc_vcpu_e500 *vcpu_e500,\r\nunsigned int as, unsigned int gid,\r\nunsigned int pr, int avoid_recursion)\r\n{\r\nstruct vcpu_id_table *idt = vcpu_e500->idt;\r\nint sid;\r\nBUG_ON(as >= 2);\r\nBUG_ON(gid >= NUM_TIDS);\r\nBUG_ON(pr >= 2);\r\nsid = local_sid_lookup(&idt->id[as][gid][pr]);\r\nwhile (sid <= 0) {\r\nsid = local_sid_setup_one(&idt->id[as][gid][pr]);\r\nif (sid <= 0) {\r\n_tlbil_all();\r\nlocal_sid_destroy_all();\r\n}\r\nif (!avoid_recursion)\r\nkvmppc_e500_recalc_shadow_pid(vcpu_e500);\r\n}\r\nreturn sid;\r\n}\r\nvoid kvmppc_e500_recalc_shadow_pid(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\npreempt_disable();\r\nvcpu_e500->vcpu.arch.shadow_pid = kvmppc_e500_get_sid(vcpu_e500,\r\nget_cur_as(&vcpu_e500->vcpu),\r\nget_cur_pid(&vcpu_e500->vcpu),\r\nget_cur_pr(&vcpu_e500->vcpu), 1);\r\nvcpu_e500->vcpu.arch.shadow_pid1 = kvmppc_e500_get_sid(vcpu_e500,\r\nget_cur_as(&vcpu_e500->vcpu), 0,\r\nget_cur_pr(&vcpu_e500->vcpu), 1);\r\npreempt_enable();\r\n}\r\nvoid kvmppc_dump_tlbs(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nstruct tlbe *tlbe;\r\nint i, tlbsel;\r\nprintk("| %8s | %8s | %8s | %8s | %8s |\n",\r\n"nr", "mas1", "mas2", "mas3", "mas7");\r\nfor (tlbsel = 0; tlbsel < 2; tlbsel++) {\r\nprintk("Guest TLB%d:\n", tlbsel);\r\nfor (i = 0; i < vcpu_e500->gtlb_size[tlbsel]; i++) {\r\ntlbe = &vcpu_e500->gtlb_arch[tlbsel][i];\r\nif (tlbe->mas1 & MAS1_VALID)\r\nprintk(" G[%d][%3d] | %08X | %08X | %08X | %08X |\n",\r\ntlbsel, i, tlbe->mas1, tlbe->mas2,\r\ntlbe->mas3, tlbe->mas7);\r\n}\r\n}\r\n}\r\nstatic inline unsigned int tlb0_get_next_victim(\r\nstruct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nunsigned int victim;\r\nvictim = vcpu_e500->gtlb_nv[0]++;\r\nif (unlikely(vcpu_e500->gtlb_nv[0] >= KVM_E500_TLB0_WAY_NUM))\r\nvcpu_e500->gtlb_nv[0] = 0;\r\nreturn victim;\r\n}\r\nstatic inline unsigned int tlb1_max_shadow_size(void)\r\n{\r\nreturn tlb1_entry_num - tlbcam_index - 1;\r\n}\r\nstatic inline int tlbe_is_writable(struct tlbe *tlbe)\r\n{\r\nreturn tlbe->mas3 & (MAS3_SW|MAS3_UW);\r\n}\r\nstatic inline u32 e500_shadow_mas3_attrib(u32 mas3, int usermode)\r\n{\r\nmas3 &= MAS3_ATTRIB_MASK;\r\nif (!usermode) {\r\nmas3 &= ~E500_TLB_USER_PERM_MASK;\r\nmas3 |= (mas3 & E500_TLB_SUPER_PERM_MASK) << 1;\r\n}\r\nreturn mas3 | E500_TLB_SUPER_PERM_MASK;\r\n}\r\nstatic inline u32 e500_shadow_mas2_attrib(u32 mas2, int usermode)\r\n{\r\n#ifdef CONFIG_SMP\r\nreturn (mas2 & MAS2_ATTRIB_MASK) | MAS2_M;\r\n#else\r\nreturn mas2 & MAS2_ATTRIB_MASK;\r\n#endif\r\n}\r\nstatic inline void __write_host_tlbe(struct tlbe *stlbe, uint32_t mas0)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nmtspr(SPRN_MAS0, mas0);\r\nmtspr(SPRN_MAS1, stlbe->mas1);\r\nmtspr(SPRN_MAS2, stlbe->mas2);\r\nmtspr(SPRN_MAS3, stlbe->mas3);\r\nmtspr(SPRN_MAS7, stlbe->mas7);\r\nasm volatile("isync; tlbwe" : : : "memory");\r\nlocal_irq_restore(flags);\r\n}\r\nstatic inline void write_host_tlbe(struct kvmppc_vcpu_e500 *vcpu_e500,\r\nint tlbsel, int esel, struct tlbe *stlbe)\r\n{\r\nif (tlbsel == 0) {\r\n__write_host_tlbe(stlbe,\r\nMAS0_TLBSEL(0) |\r\nMAS0_ESEL(esel & (KVM_E500_TLB0_WAY_NUM - 1)));\r\n} else {\r\n__write_host_tlbe(stlbe,\r\nMAS0_TLBSEL(1) |\r\nMAS0_ESEL(to_htlb1_esel(esel)));\r\n}\r\ntrace_kvm_stlb_write(index_of(tlbsel, esel), stlbe->mas1, stlbe->mas2,\r\nstlbe->mas3, stlbe->mas7);\r\n}\r\nvoid kvmppc_map_magic(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nstruct tlbe magic;\r\nulong shared_page = ((ulong)vcpu->arch.shared) & PAGE_MASK;\r\nunsigned int stid;\r\npfn_t pfn;\r\npfn = (pfn_t)virt_to_phys((void *)shared_page) >> PAGE_SHIFT;\r\nget_page(pfn_to_page(pfn));\r\npreempt_disable();\r\nstid = kvmppc_e500_get_sid(vcpu_e500, 0, 0, 0, 0);\r\nmagic.mas1 = MAS1_VALID | MAS1_TS | MAS1_TID(stid) |\r\nMAS1_TSIZE(BOOK3E_PAGESZ_4K);\r\nmagic.mas2 = vcpu->arch.magic_page_ea | MAS2_M;\r\nmagic.mas3 = (pfn << PAGE_SHIFT) |\r\nMAS3_SW | MAS3_SR | MAS3_UW | MAS3_UR;\r\nmagic.mas7 = pfn >> (32 - PAGE_SHIFT);\r\n__write_host_tlbe(&magic, MAS0_TLBSEL(1) | MAS0_ESEL(tlbcam_index));\r\npreempt_enable();\r\n}\r\nvoid kvmppc_e500_tlb_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nkvmppc_e500_recalc_shadow_pid(vcpu_e500);\r\n}\r\nvoid kvmppc_e500_tlb_put(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nstatic void kvmppc_e500_stlbe_invalidate(struct kvmppc_vcpu_e500 *vcpu_e500,\r\nint tlbsel, int esel)\r\n{\r\nstruct tlbe *gtlbe = &vcpu_e500->gtlb_arch[tlbsel][esel];\r\nstruct vcpu_id_table *idt = vcpu_e500->idt;\r\nunsigned int pr, tid, ts, pid;\r\nu32 val, eaddr;\r\nunsigned long flags;\r\nts = get_tlb_ts(gtlbe);\r\ntid = get_tlb_tid(gtlbe);\r\npreempt_disable();\r\nfor (pr = 0; pr < 2; pr++) {\r\nif (tlbsel == 1 ||\r\n(pid = local_sid_lookup(&idt->id[ts][tid][pr])) <= 0) {\r\nkvmppc_e500_id_table_reset_one(vcpu_e500, ts, tid, pr);\r\ncontinue;\r\n}\r\nval = (pid << MAS6_SPID_SHIFT) | MAS6_SAS;\r\neaddr = get_tlb_eaddr(gtlbe);\r\nlocal_irq_save(flags);\r\nmtspr(SPRN_MAS6, val);\r\nasm volatile("tlbsx 0, %[eaddr]" : : [eaddr] "r" (eaddr));\r\nval = mfspr(SPRN_MAS1);\r\nif (val & MAS1_VALID) {\r\nmtspr(SPRN_MAS1, val & ~MAS1_VALID);\r\nasm volatile("tlbwe");\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\npreempt_enable();\r\n}\r\nstatic int kvmppc_e500_tlb_index(struct kvmppc_vcpu_e500 *vcpu_e500,\r\ngva_t eaddr, int tlbsel, unsigned int pid, int as)\r\n{\r\nint size = vcpu_e500->gtlb_size[tlbsel];\r\nint set_base;\r\nint i;\r\nif (tlbsel == 0) {\r\nint mask = size / KVM_E500_TLB0_WAY_NUM - 1;\r\nset_base = (eaddr >> PAGE_SHIFT) & mask;\r\nset_base *= KVM_E500_TLB0_WAY_NUM;\r\nsize = KVM_E500_TLB0_WAY_NUM;\r\n} else {\r\nset_base = 0;\r\n}\r\nfor (i = 0; i < size; i++) {\r\nstruct tlbe *tlbe = &vcpu_e500->gtlb_arch[tlbsel][set_base + i];\r\nunsigned int tid;\r\nif (eaddr < get_tlb_eaddr(tlbe))\r\ncontinue;\r\nif (eaddr > get_tlb_end(tlbe))\r\ncontinue;\r\ntid = get_tlb_tid(tlbe);\r\nif (tid && (tid != pid))\r\ncontinue;\r\nif (!get_tlb_v(tlbe))\r\ncontinue;\r\nif (get_tlb_ts(tlbe) != as && as != -1)\r\ncontinue;\r\nreturn set_base + i;\r\n}\r\nreturn -1;\r\n}\r\nstatic inline void kvmppc_e500_priv_setup(struct tlbe_priv *priv,\r\nstruct tlbe *gtlbe,\r\npfn_t pfn)\r\n{\r\npriv->pfn = pfn;\r\npriv->flags = E500_TLB_VALID;\r\nif (tlbe_is_writable(gtlbe))\r\npriv->flags |= E500_TLB_DIRTY;\r\n}\r\nstatic inline void kvmppc_e500_priv_release(struct tlbe_priv *priv)\r\n{\r\nif (priv->flags & E500_TLB_VALID) {\r\nif (priv->flags & E500_TLB_DIRTY)\r\nkvm_release_pfn_dirty(priv->pfn);\r\nelse\r\nkvm_release_pfn_clean(priv->pfn);\r\npriv->flags = 0;\r\n}\r\n}\r\nstatic inline void kvmppc_e500_deliver_tlb_miss(struct kvm_vcpu *vcpu,\r\nunsigned int eaddr, int as)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nunsigned int victim, pidsel, tsized;\r\nint tlbsel;\r\ntlbsel = (vcpu_e500->mas4 >> 28) & 0x1;\r\nvictim = (tlbsel == 0) ? tlb0_get_next_victim(vcpu_e500) : 0;\r\npidsel = (vcpu_e500->mas4 >> 16) & 0xf;\r\ntsized = (vcpu_e500->mas4 >> 7) & 0x1f;\r\nvcpu_e500->mas0 = MAS0_TLBSEL(tlbsel) | MAS0_ESEL(victim)\r\n| MAS0_NV(vcpu_e500->gtlb_nv[tlbsel]);\r\nvcpu_e500->mas1 = MAS1_VALID | (as ? MAS1_TS : 0)\r\n| MAS1_TID(vcpu_e500->pid[pidsel])\r\n| MAS1_TSIZE(tsized);\r\nvcpu_e500->mas2 = (eaddr & MAS2_EPN)\r\n| (vcpu_e500->mas4 & MAS2_ATTRIB_MASK);\r\nvcpu_e500->mas3 &= MAS3_U0 | MAS3_U1 | MAS3_U2 | MAS3_U3;\r\nvcpu_e500->mas6 = (vcpu_e500->mas6 & MAS6_SPID1)\r\n| (get_cur_pid(vcpu) << 16)\r\n| (as ? MAS6_SAS : 0);\r\nvcpu_e500->mas7 = 0;\r\n}\r\nstatic inline void kvmppc_e500_setup_stlbe(struct kvmppc_vcpu_e500 *vcpu_e500,\r\nstruct tlbe *gtlbe, int tsize,\r\nstruct tlbe_priv *priv,\r\nu64 gvaddr, struct tlbe *stlbe)\r\n{\r\npfn_t pfn = priv->pfn;\r\nunsigned int stid;\r\nstid = kvmppc_e500_get_sid(vcpu_e500, get_tlb_ts(gtlbe),\r\nget_tlb_tid(gtlbe),\r\nget_cur_pr(&vcpu_e500->vcpu), 0);\r\nstlbe->mas1 = MAS1_TSIZE(tsize)\r\n| MAS1_TID(stid) | MAS1_TS | MAS1_VALID;\r\nstlbe->mas2 = (gvaddr & MAS2_EPN)\r\n| e500_shadow_mas2_attrib(gtlbe->mas2,\r\nvcpu_e500->vcpu.arch.shared->msr & MSR_PR);\r\nstlbe->mas3 = ((pfn << PAGE_SHIFT) & MAS3_RPN)\r\n| e500_shadow_mas3_attrib(gtlbe->mas3,\r\nvcpu_e500->vcpu.arch.shared->msr & MSR_PR);\r\nstlbe->mas7 = (pfn >> (32 - PAGE_SHIFT)) & MAS7_RPN;\r\n}\r\nstatic inline void kvmppc_e500_shadow_map(struct kvmppc_vcpu_e500 *vcpu_e500,\r\nu64 gvaddr, gfn_t gfn, struct tlbe *gtlbe, int tlbsel, int esel,\r\nstruct tlbe *stlbe)\r\n{\r\nstruct kvm_memory_slot *slot;\r\nunsigned long pfn, hva;\r\nint pfnmap = 0;\r\nint tsize = BOOK3E_PAGESZ_4K;\r\nstruct tlbe_priv *priv;\r\nslot = gfn_to_memslot(vcpu_e500->vcpu.kvm, gfn);\r\nhva = gfn_to_hva_memslot(slot, gfn);\r\nif (tlbsel == 1) {\r\nstruct vm_area_struct *vma;\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma(current->mm, hva);\r\nif (vma && hva >= vma->vm_start &&\r\n(vma->vm_flags & VM_PFNMAP)) {\r\nunsigned long start, end;\r\nunsigned long slot_start, slot_end;\r\npfnmap = 1;\r\nstart = vma->vm_pgoff;\r\nend = start +\r\n((vma->vm_end - vma->vm_start) >> PAGE_SHIFT);\r\npfn = start + ((hva - vma->vm_start) >> PAGE_SHIFT);\r\nslot_start = pfn - (gfn - slot->base_gfn);\r\nslot_end = slot_start + slot->npages;\r\nif (start < slot_start)\r\nstart = slot_start;\r\nif (end > slot_end)\r\nend = slot_end;\r\ntsize = (gtlbe->mas1 & MAS1_TSIZE_MASK) >>\r\nMAS1_TSIZE_SHIFT;\r\ntsize = max(BOOK3E_PAGESZ_4K, tsize & ~1);\r\nfor (; tsize > BOOK3E_PAGESZ_4K; tsize -= 2) {\r\nunsigned long gfn_start, gfn_end, tsize_pages;\r\ntsize_pages = 1 << (tsize - 2);\r\ngfn_start = gfn & ~(tsize_pages - 1);\r\ngfn_end = gfn_start + tsize_pages;\r\nif (gfn_start + pfn - gfn < start)\r\ncontinue;\r\nif (gfn_end + pfn - gfn > end)\r\ncontinue;\r\nif ((gfn & (tsize_pages - 1)) !=\r\n(pfn & (tsize_pages - 1)))\r\ncontinue;\r\ngvaddr &= ~((tsize_pages << PAGE_SHIFT) - 1);\r\npfn &= ~(tsize_pages - 1);\r\nbreak;\r\n}\r\n}\r\nup_read(&current->mm->mmap_sem);\r\n}\r\nif (likely(!pfnmap)) {\r\npfn = gfn_to_pfn_memslot(vcpu_e500->vcpu.kvm, slot, gfn);\r\nif (is_error_pfn(pfn)) {\r\nprintk(KERN_ERR "Couldn't get real page for gfn %lx!\n",\r\n(long)gfn);\r\nkvm_release_pfn_clean(pfn);\r\nreturn;\r\n}\r\n}\r\npriv = &vcpu_e500->gtlb_priv[tlbsel][esel];\r\nkvmppc_e500_priv_release(priv);\r\nkvmppc_e500_priv_setup(priv, gtlbe, pfn);\r\nkvmppc_e500_setup_stlbe(vcpu_e500, gtlbe, tsize, priv, gvaddr, stlbe);\r\n}\r\nstatic int kvmppc_e500_tlb0_map(struct kvmppc_vcpu_e500 *vcpu_e500,\r\nint esel, struct tlbe *stlbe)\r\n{\r\nstruct tlbe *gtlbe;\r\ngtlbe = &vcpu_e500->gtlb_arch[0][esel];\r\nkvmppc_e500_shadow_map(vcpu_e500, get_tlb_eaddr(gtlbe),\r\nget_tlb_raddr(gtlbe) >> PAGE_SHIFT,\r\ngtlbe, 0, esel, stlbe);\r\nreturn esel;\r\n}\r\nstatic int kvmppc_e500_tlb1_map(struct kvmppc_vcpu_e500 *vcpu_e500,\r\nu64 gvaddr, gfn_t gfn, struct tlbe *gtlbe, struct tlbe *stlbe)\r\n{\r\nunsigned int victim;\r\nvictim = vcpu_e500->gtlb_nv[1]++;\r\nif (unlikely(vcpu_e500->gtlb_nv[1] >= tlb1_max_shadow_size()))\r\nvcpu_e500->gtlb_nv[1] = 0;\r\nkvmppc_e500_shadow_map(vcpu_e500, gvaddr, gfn, gtlbe, 1, victim, stlbe);\r\nreturn victim;\r\n}\r\nvoid kvmppc_mmu_msr_notify(struct kvm_vcpu *vcpu, u32 old_msr)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nkvmppc_e500_recalc_shadow_pid(vcpu_e500);\r\n}\r\nstatic inline int kvmppc_e500_gtlbe_invalidate(\r\nstruct kvmppc_vcpu_e500 *vcpu_e500,\r\nint tlbsel, int esel)\r\n{\r\nstruct tlbe *gtlbe = &vcpu_e500->gtlb_arch[tlbsel][esel];\r\nif (unlikely(get_tlb_iprot(gtlbe)))\r\nreturn -1;\r\ngtlbe->mas1 = 0;\r\nreturn 0;\r\n}\r\nint kvmppc_e500_emul_mt_mmucsr0(struct kvmppc_vcpu_e500 *vcpu_e500, ulong value)\r\n{\r\nint esel;\r\nif (value & MMUCSR0_TLB0FI)\r\nfor (esel = 0; esel < vcpu_e500->gtlb_size[0]; esel++)\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, 0, esel);\r\nif (value & MMUCSR0_TLB1FI)\r\nfor (esel = 0; esel < vcpu_e500->gtlb_size[1]; esel++)\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, 1, esel);\r\nkvmppc_e500_id_table_reset_all(vcpu_e500);\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_e500_emul_tlbivax(struct kvm_vcpu *vcpu, int ra, int rb)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nunsigned int ia;\r\nint esel, tlbsel;\r\ngva_t ea;\r\nea = ((ra) ? kvmppc_get_gpr(vcpu, ra) : 0) + kvmppc_get_gpr(vcpu, rb);\r\nia = (ea >> 2) & 0x1;\r\ntlbsel = (ea >> 3) & 0x1;\r\nif (ia) {\r\nfor (esel = 0; esel < vcpu_e500->gtlb_size[tlbsel]; esel++)\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, tlbsel, esel);\r\n} else {\r\nea &= 0xfffff000;\r\nesel = kvmppc_e500_tlb_index(vcpu_e500, ea, tlbsel,\r\nget_cur_pid(vcpu), -1);\r\nif (esel >= 0)\r\nkvmppc_e500_gtlbe_invalidate(vcpu_e500, tlbsel, esel);\r\n}\r\nkvmppc_e500_id_table_reset_all(vcpu_e500);\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_e500_emul_tlbre(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nint tlbsel, esel;\r\nstruct tlbe *gtlbe;\r\ntlbsel = get_tlb_tlbsel(vcpu_e500);\r\nesel = get_tlb_esel(vcpu_e500, tlbsel);\r\ngtlbe = &vcpu_e500->gtlb_arch[tlbsel][esel];\r\nvcpu_e500->mas0 &= ~MAS0_NV(~0);\r\nvcpu_e500->mas0 |= MAS0_NV(vcpu_e500->gtlb_nv[tlbsel]);\r\nvcpu_e500->mas1 = gtlbe->mas1;\r\nvcpu_e500->mas2 = gtlbe->mas2;\r\nvcpu_e500->mas3 = gtlbe->mas3;\r\nvcpu_e500->mas7 = gtlbe->mas7;\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_e500_emul_tlbsx(struct kvm_vcpu *vcpu, int rb)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nint as = !!get_cur_sas(vcpu_e500);\r\nunsigned int pid = get_cur_spid(vcpu_e500);\r\nint esel, tlbsel;\r\nstruct tlbe *gtlbe = NULL;\r\ngva_t ea;\r\nea = kvmppc_get_gpr(vcpu, rb);\r\nfor (tlbsel = 0; tlbsel < 2; tlbsel++) {\r\nesel = kvmppc_e500_tlb_index(vcpu_e500, ea, tlbsel, pid, as);\r\nif (esel >= 0) {\r\ngtlbe = &vcpu_e500->gtlb_arch[tlbsel][esel];\r\nbreak;\r\n}\r\n}\r\nif (gtlbe) {\r\nvcpu_e500->mas0 = MAS0_TLBSEL(tlbsel) | MAS0_ESEL(esel)\r\n| MAS0_NV(vcpu_e500->gtlb_nv[tlbsel]);\r\nvcpu_e500->mas1 = gtlbe->mas1;\r\nvcpu_e500->mas2 = gtlbe->mas2;\r\nvcpu_e500->mas3 = gtlbe->mas3;\r\nvcpu_e500->mas7 = gtlbe->mas7;\r\n} else {\r\nint victim;\r\ntlbsel = vcpu_e500->mas4 >> 28 & 0x1;\r\nvictim = (tlbsel == 0) ? tlb0_get_next_victim(vcpu_e500) : 0;\r\nvcpu_e500->mas0 = MAS0_TLBSEL(tlbsel) | MAS0_ESEL(victim)\r\n| MAS0_NV(vcpu_e500->gtlb_nv[tlbsel]);\r\nvcpu_e500->mas1 = (vcpu_e500->mas6 & MAS6_SPID0)\r\n| (vcpu_e500->mas6 & (MAS6_SAS ? MAS1_TS : 0))\r\n| (vcpu_e500->mas4 & MAS4_TSIZED(~0));\r\nvcpu_e500->mas2 &= MAS2_EPN;\r\nvcpu_e500->mas2 |= vcpu_e500->mas4 & MAS2_ATTRIB_MASK;\r\nvcpu_e500->mas3 &= MAS3_U0 | MAS3_U1 | MAS3_U2 | MAS3_U3;\r\nvcpu_e500->mas7 = 0;\r\n}\r\nkvmppc_set_exit_type(vcpu, EMULATED_TLBSX_EXITS);\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_e500_emul_tlbwe(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nstruct tlbe *gtlbe;\r\nint tlbsel, esel;\r\ntlbsel = get_tlb_tlbsel(vcpu_e500);\r\nesel = get_tlb_esel(vcpu_e500, tlbsel);\r\ngtlbe = &vcpu_e500->gtlb_arch[tlbsel][esel];\r\nif (get_tlb_v(gtlbe))\r\nkvmppc_e500_stlbe_invalidate(vcpu_e500, tlbsel, esel);\r\ngtlbe->mas1 = vcpu_e500->mas1;\r\ngtlbe->mas2 = vcpu_e500->mas2;\r\ngtlbe->mas3 = vcpu_e500->mas3;\r\ngtlbe->mas7 = vcpu_e500->mas7;\r\ntrace_kvm_gtlb_write(vcpu_e500->mas0, gtlbe->mas1, gtlbe->mas2,\r\ngtlbe->mas3, gtlbe->mas7);\r\nif (tlbe_is_host_safe(vcpu, gtlbe)) {\r\nstruct tlbe stlbe;\r\nint stlbsel, sesel;\r\nu64 eaddr;\r\nu64 raddr;\r\npreempt_disable();\r\nswitch (tlbsel) {\r\ncase 0:\r\ngtlbe->mas1 &= ~MAS1_TSIZE(~0);\r\ngtlbe->mas1 |= MAS1_TSIZE(BOOK3E_PAGESZ_4K);\r\nstlbsel = 0;\r\nsesel = kvmppc_e500_tlb0_map(vcpu_e500, esel, &stlbe);\r\nbreak;\r\ncase 1:\r\neaddr = get_tlb_eaddr(gtlbe);\r\nraddr = get_tlb_raddr(gtlbe);\r\nstlbsel = 1;\r\nsesel = kvmppc_e500_tlb1_map(vcpu_e500, eaddr,\r\nraddr >> PAGE_SHIFT, gtlbe, &stlbe);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nwrite_host_tlbe(vcpu_e500, stlbsel, sesel, &stlbe);\r\npreempt_enable();\r\n}\r\nkvmppc_set_exit_type(vcpu, EMULATED_TLBWE_EXITS);\r\nreturn EMULATE_DONE;\r\n}\r\nint kvmppc_mmu_itlb_index(struct kvm_vcpu *vcpu, gva_t eaddr)\r\n{\r\nunsigned int as = !!(vcpu->arch.shared->msr & MSR_IS);\r\nreturn kvmppc_e500_tlb_search(vcpu, eaddr, get_cur_pid(vcpu), as);\r\n}\r\nint kvmppc_mmu_dtlb_index(struct kvm_vcpu *vcpu, gva_t eaddr)\r\n{\r\nunsigned int as = !!(vcpu->arch.shared->msr & MSR_DS);\r\nreturn kvmppc_e500_tlb_search(vcpu, eaddr, get_cur_pid(vcpu), as);\r\n}\r\nvoid kvmppc_mmu_itlb_miss(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned int as = !!(vcpu->arch.shared->msr & MSR_IS);\r\nkvmppc_e500_deliver_tlb_miss(vcpu, vcpu->arch.pc, as);\r\n}\r\nvoid kvmppc_mmu_dtlb_miss(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned int as = !!(vcpu->arch.shared->msr & MSR_DS);\r\nkvmppc_e500_deliver_tlb_miss(vcpu, vcpu->arch.fault_dear, as);\r\n}\r\ngpa_t kvmppc_mmu_xlate(struct kvm_vcpu *vcpu, unsigned int index,\r\ngva_t eaddr)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nstruct tlbe *gtlbe =\r\n&vcpu_e500->gtlb_arch[tlbsel_of(index)][esel_of(index)];\r\nu64 pgmask = get_tlb_bytes(gtlbe) - 1;\r\nreturn get_tlb_raddr(gtlbe) | (eaddr & pgmask);\r\n}\r\nvoid kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nvoid kvmppc_mmu_map(struct kvm_vcpu *vcpu, u64 eaddr, gpa_t gpaddr,\r\nunsigned int index)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nstruct tlbe_priv *priv;\r\nstruct tlbe *gtlbe, stlbe;\r\nint tlbsel = tlbsel_of(index);\r\nint esel = esel_of(index);\r\nint stlbsel, sesel;\r\ngtlbe = &vcpu_e500->gtlb_arch[tlbsel][esel];\r\npreempt_disable();\r\nswitch (tlbsel) {\r\ncase 0:\r\nstlbsel = 0;\r\nsesel = esel;\r\npriv = &vcpu_e500->gtlb_priv[stlbsel][sesel];\r\nkvmppc_e500_setup_stlbe(vcpu_e500, gtlbe, BOOK3E_PAGESZ_4K,\r\npriv, eaddr, &stlbe);\r\nbreak;\r\ncase 1: {\r\ngfn_t gfn = gpaddr >> PAGE_SHIFT;\r\nstlbsel = 1;\r\nsesel = kvmppc_e500_tlb1_map(vcpu_e500, eaddr, gfn,\r\ngtlbe, &stlbe);\r\nbreak;\r\n}\r\ndefault:\r\nBUG();\r\nbreak;\r\n}\r\nwrite_host_tlbe(vcpu_e500, stlbsel, sesel, &stlbe);\r\npreempt_enable();\r\n}\r\nint kvmppc_e500_tlb_search(struct kvm_vcpu *vcpu,\r\ngva_t eaddr, unsigned int pid, int as)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nint esel, tlbsel;\r\nfor (tlbsel = 0; tlbsel < 2; tlbsel++) {\r\nesel = kvmppc_e500_tlb_index(vcpu_e500, eaddr, tlbsel, pid, as);\r\nif (esel >= 0)\r\nreturn index_of(tlbsel, esel);\r\n}\r\nreturn -1;\r\n}\r\nvoid kvmppc_set_pid(struct kvm_vcpu *vcpu, u32 pid)\r\n{\r\nstruct kvmppc_vcpu_e500 *vcpu_e500 = to_e500(vcpu);\r\nif (vcpu->arch.pid != pid) {\r\nvcpu_e500->pid[0] = vcpu->arch.pid = pid;\r\nkvmppc_e500_recalc_shadow_pid(vcpu_e500);\r\n}\r\n}\r\nvoid kvmppc_e500_tlb_setup(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nstruct tlbe *tlbe;\r\ntlbe = &vcpu_e500->gtlb_arch[1][0];\r\ntlbe->mas1 = MAS1_VALID | MAS1_TSIZE(BOOK3E_PAGESZ_256M);\r\ntlbe->mas2 = 0;\r\ntlbe->mas3 = E500_TLB_SUPER_PERM_MASK;\r\ntlbe->mas7 = 0;\r\ntlbe = &vcpu_e500->gtlb_arch[1][1];\r\ntlbe->mas1 = MAS1_VALID | MAS1_TSIZE(BOOK3E_PAGESZ_4K);\r\ntlbe->mas2 = (0xe0004500 & 0xFFFFF000) | MAS2_I | MAS2_G;\r\ntlbe->mas3 = (0xe0004500 & 0xFFFFF000) | E500_TLB_SUPER_PERM_MASK;\r\ntlbe->mas7 = 0;\r\n}\r\nint kvmppc_e500_tlb_init(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\ntlb1_entry_num = mfspr(SPRN_TLB1CFG) & 0xFFF;\r\nvcpu_e500->gtlb_size[0] = KVM_E500_TLB0_SIZE;\r\nvcpu_e500->gtlb_arch[0] =\r\nkzalloc(sizeof(struct tlbe) * KVM_E500_TLB0_SIZE, GFP_KERNEL);\r\nif (vcpu_e500->gtlb_arch[0] == NULL)\r\ngoto err_out;\r\nvcpu_e500->gtlb_size[1] = KVM_E500_TLB1_SIZE;\r\nvcpu_e500->gtlb_arch[1] =\r\nkzalloc(sizeof(struct tlbe) * KVM_E500_TLB1_SIZE, GFP_KERNEL);\r\nif (vcpu_e500->gtlb_arch[1] == NULL)\r\ngoto err_out_guest0;\r\nvcpu_e500->gtlb_priv[0] = (struct tlbe_priv *)\r\nkzalloc(sizeof(struct tlbe_priv) * KVM_E500_TLB0_SIZE, GFP_KERNEL);\r\nif (vcpu_e500->gtlb_priv[0] == NULL)\r\ngoto err_out_guest1;\r\nvcpu_e500->gtlb_priv[1] = (struct tlbe_priv *)\r\nkzalloc(sizeof(struct tlbe_priv) * KVM_E500_TLB1_SIZE, GFP_KERNEL);\r\nif (vcpu_e500->gtlb_priv[1] == NULL)\r\ngoto err_out_priv0;\r\nif (kvmppc_e500_id_table_alloc(vcpu_e500) == NULL)\r\ngoto err_out_priv1;\r\nvcpu_e500->tlb0cfg = mfspr(SPRN_TLB0CFG) & ~0xfffUL;\r\nvcpu_e500->tlb0cfg |= vcpu_e500->gtlb_size[0];\r\nvcpu_e500->tlb1cfg = mfspr(SPRN_TLB1CFG) & ~0xfffUL;\r\nvcpu_e500->tlb1cfg |= vcpu_e500->gtlb_size[1];\r\nreturn 0;\r\nerr_out_priv1:\r\nkfree(vcpu_e500->gtlb_priv[1]);\r\nerr_out_priv0:\r\nkfree(vcpu_e500->gtlb_priv[0]);\r\nerr_out_guest1:\r\nkfree(vcpu_e500->gtlb_arch[1]);\r\nerr_out_guest0:\r\nkfree(vcpu_e500->gtlb_arch[0]);\r\nerr_out:\r\nreturn -1;\r\n}\r\nvoid kvmppc_e500_tlb_uninit(struct kvmppc_vcpu_e500 *vcpu_e500)\r\n{\r\nint stlbsel, i;\r\nfor (stlbsel = 0; stlbsel < 2; stlbsel++)\r\nfor (i = 0; i < vcpu_e500->gtlb_size[stlbsel]; i++) {\r\nstruct tlbe_priv *priv =\r\n&vcpu_e500->gtlb_priv[stlbsel][i];\r\nkvmppc_e500_priv_release(priv);\r\n}\r\nkvmppc_e500_id_table_free(vcpu_e500);\r\nkfree(vcpu_e500->gtlb_arch[1]);\r\nkfree(vcpu_e500->gtlb_arch[0]);\r\n}
