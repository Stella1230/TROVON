void *dma_alloc_coherent(struct device *dev, size_t size,\r\ndma_addr_t *dma_handle, gfp_t gfp)\r\n{\r\nvoid *ret;\r\ngfp &= ~(__GFP_DMA | __GFP_HIGHMEM);\r\nif (dev == NULL || (*dev->dma_mask < 0xffffffff))\r\ngfp |= GFP_DMA;\r\nret = (void *)__get_free_pages(gfp, get_order(size));\r\nif (ret != NULL) {\r\nmemset(ret, 0, size);\r\n*dma_handle = virt_to_phys(ret);\r\n}\r\nreturn ret;\r\n}\r\nvoid dma_free_coherent(struct device *dev, size_t size,\r\nvoid *vaddr, dma_addr_t dma_handle)\r\n{\r\nfree_pages((unsigned long)vaddr, get_order(size));\r\n}\r\nvoid dma_sync_single_for_device(struct device *dev, dma_addr_t handle,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nswitch (dir) {\r\ncase DMA_TO_DEVICE:\r\nflush_dcache_range(handle, size);\r\nbreak;\r\ncase DMA_FROM_DEVICE:\r\nbreak;\r\ndefault:\r\nif (printk_ratelimit())\r\nprintk("dma_sync_single_for_device: unsupported dir %u\n", dir);\r\nbreak;\r\n}\r\n}\r\ndma_addr_t dma_map_single(struct device *dev, void *addr, size_t size,\r\nenum dma_data_direction dir)\r\n{\r\ndma_addr_t handle = virt_to_phys(addr);\r\nflush_dcache_range(handle, size);\r\nreturn handle;\r\n}\r\ndma_addr_t dma_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir)\r\n{\r\ndma_addr_t handle = page_to_phys(page) + offset;\r\ndma_sync_single_for_device(dev, handle, size, dir);\r\nreturn handle;\r\n}
