static void rds_iw_frag_drop_page(struct rds_page_frag *frag)\r\n{\r\nrdsdebug("frag %p page %p\n", frag, frag->f_page);\r\n__free_page(frag->f_page);\r\nfrag->f_page = NULL;\r\n}\r\nstatic void rds_iw_frag_free(struct rds_page_frag *frag)\r\n{\r\nrdsdebug("frag %p page %p\n", frag, frag->f_page);\r\nBUG_ON(frag->f_page);\r\nkmem_cache_free(rds_iw_frag_slab, frag);\r\n}\r\nstatic void rds_iw_recv_unmap_page(struct rds_iw_connection *ic,\r\nstruct rds_iw_recv_work *recv)\r\n{\r\nstruct rds_page_frag *frag = recv->r_frag;\r\nrdsdebug("recv %p frag %p page %p\n", recv, frag, frag->f_page);\r\nif (frag->f_mapped)\r\nib_dma_unmap_page(ic->i_cm_id->device,\r\nfrag->f_mapped,\r\nRDS_FRAG_SIZE, DMA_FROM_DEVICE);\r\nfrag->f_mapped = 0;\r\n}\r\nvoid rds_iw_recv_init_ring(struct rds_iw_connection *ic)\r\n{\r\nstruct rds_iw_recv_work *recv;\r\nu32 i;\r\nfor (i = 0, recv = ic->i_recvs; i < ic->i_recv_ring.w_nr; i++, recv++) {\r\nstruct ib_sge *sge;\r\nrecv->r_iwinc = NULL;\r\nrecv->r_frag = NULL;\r\nrecv->r_wr.next = NULL;\r\nrecv->r_wr.wr_id = i;\r\nrecv->r_wr.sg_list = recv->r_sge;\r\nrecv->r_wr.num_sge = RDS_IW_RECV_SGE;\r\nsge = rds_iw_data_sge(ic, recv->r_sge);\r\nsge->addr = 0;\r\nsge->length = RDS_FRAG_SIZE;\r\nsge->lkey = 0;\r\nsge = rds_iw_header_sge(ic, recv->r_sge);\r\nsge->addr = ic->i_recv_hdrs_dma + (i * sizeof(struct rds_header));\r\nsge->length = sizeof(struct rds_header);\r\nsge->lkey = 0;\r\n}\r\n}\r\nstatic void rds_iw_recv_clear_one(struct rds_iw_connection *ic,\r\nstruct rds_iw_recv_work *recv)\r\n{\r\nif (recv->r_iwinc) {\r\nrds_inc_put(&recv->r_iwinc->ii_inc);\r\nrecv->r_iwinc = NULL;\r\n}\r\nif (recv->r_frag) {\r\nrds_iw_recv_unmap_page(ic, recv);\r\nif (recv->r_frag->f_page)\r\nrds_iw_frag_drop_page(recv->r_frag);\r\nrds_iw_frag_free(recv->r_frag);\r\nrecv->r_frag = NULL;\r\n}\r\n}\r\nvoid rds_iw_recv_clear_ring(struct rds_iw_connection *ic)\r\n{\r\nu32 i;\r\nfor (i = 0; i < ic->i_recv_ring.w_nr; i++)\r\nrds_iw_recv_clear_one(ic, &ic->i_recvs[i]);\r\nif (ic->i_frag.f_page)\r\nrds_iw_frag_drop_page(&ic->i_frag);\r\n}\r\nstatic int rds_iw_recv_refill_one(struct rds_connection *conn,\r\nstruct rds_iw_recv_work *recv,\r\ngfp_t kptr_gfp, gfp_t page_gfp)\r\n{\r\nstruct rds_iw_connection *ic = conn->c_transport_data;\r\ndma_addr_t dma_addr;\r\nstruct ib_sge *sge;\r\nint ret = -ENOMEM;\r\nif (!recv->r_iwinc) {\r\nif (!atomic_add_unless(&rds_iw_allocation, 1, rds_iw_sysctl_max_recv_allocation)) {\r\nrds_iw_stats_inc(s_iw_rx_alloc_limit);\r\ngoto out;\r\n}\r\nrecv->r_iwinc = kmem_cache_alloc(rds_iw_incoming_slab,\r\nkptr_gfp);\r\nif (!recv->r_iwinc) {\r\natomic_dec(&rds_iw_allocation);\r\ngoto out;\r\n}\r\nINIT_LIST_HEAD(&recv->r_iwinc->ii_frags);\r\nrds_inc_init(&recv->r_iwinc->ii_inc, conn, conn->c_faddr);\r\n}\r\nif (!recv->r_frag) {\r\nrecv->r_frag = kmem_cache_alloc(rds_iw_frag_slab, kptr_gfp);\r\nif (!recv->r_frag)\r\ngoto out;\r\nINIT_LIST_HEAD(&recv->r_frag->f_item);\r\nrecv->r_frag->f_page = NULL;\r\n}\r\nif (!ic->i_frag.f_page) {\r\nic->i_frag.f_page = alloc_page(page_gfp);\r\nif (!ic->i_frag.f_page)\r\ngoto out;\r\nic->i_frag.f_offset = 0;\r\n}\r\ndma_addr = ib_dma_map_page(ic->i_cm_id->device,\r\nic->i_frag.f_page,\r\nic->i_frag.f_offset,\r\nRDS_FRAG_SIZE,\r\nDMA_FROM_DEVICE);\r\nif (ib_dma_mapping_error(ic->i_cm_id->device, dma_addr))\r\ngoto out;\r\nrecv->r_frag->f_page = ic->i_frag.f_page;\r\nrecv->r_frag->f_offset = ic->i_frag.f_offset;\r\nrecv->r_frag->f_mapped = dma_addr;\r\nsge = rds_iw_data_sge(ic, recv->r_sge);\r\nsge->addr = dma_addr;\r\nsge->length = RDS_FRAG_SIZE;\r\nsge = rds_iw_header_sge(ic, recv->r_sge);\r\nsge->addr = ic->i_recv_hdrs_dma + (recv - ic->i_recvs) * sizeof(struct rds_header);\r\nsge->length = sizeof(struct rds_header);\r\nget_page(recv->r_frag->f_page);\r\nif (ic->i_frag.f_offset < RDS_PAGE_LAST_OFF) {\r\nic->i_frag.f_offset += RDS_FRAG_SIZE;\r\n} else {\r\nput_page(ic->i_frag.f_page);\r\nic->i_frag.f_page = NULL;\r\nic->i_frag.f_offset = 0;\r\n}\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nint rds_iw_recv_refill(struct rds_connection *conn, gfp_t kptr_gfp,\r\ngfp_t page_gfp, int prefill)\r\n{\r\nstruct rds_iw_connection *ic = conn->c_transport_data;\r\nstruct rds_iw_recv_work *recv;\r\nstruct ib_recv_wr *failed_wr;\r\nunsigned int posted = 0;\r\nint ret = 0;\r\nu32 pos;\r\nwhile ((prefill || rds_conn_up(conn)) &&\r\nrds_iw_ring_alloc(&ic->i_recv_ring, 1, &pos)) {\r\nif (pos >= ic->i_recv_ring.w_nr) {\r\nprintk(KERN_NOTICE "Argh - ring alloc returned pos=%u\n",\r\npos);\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nrecv = &ic->i_recvs[pos];\r\nret = rds_iw_recv_refill_one(conn, recv, kptr_gfp, page_gfp);\r\nif (ret) {\r\nret = -1;\r\nbreak;\r\n}\r\nret = ib_post_recv(ic->i_cm_id->qp, &recv->r_wr, &failed_wr);\r\nrdsdebug("recv %p iwinc %p page %p addr %lu ret %d\n", recv,\r\nrecv->r_iwinc, recv->r_frag->f_page,\r\n(long) recv->r_frag->f_mapped, ret);\r\nif (ret) {\r\nrds_iw_conn_error(conn, "recv post on "\r\n"%pI4 returned %d, disconnecting and "\r\n"reconnecting\n", &conn->c_faddr,\r\nret);\r\nret = -1;\r\nbreak;\r\n}\r\nposted++;\r\n}\r\nif (ic->i_flowctl && posted)\r\nrds_iw_advertise_credits(conn, posted);\r\nif (ret)\r\nrds_iw_ring_unalloc(&ic->i_recv_ring, 1);\r\nreturn ret;\r\n}\r\nstatic void rds_iw_inc_purge(struct rds_incoming *inc)\r\n{\r\nstruct rds_iw_incoming *iwinc;\r\nstruct rds_page_frag *frag;\r\nstruct rds_page_frag *pos;\r\niwinc = container_of(inc, struct rds_iw_incoming, ii_inc);\r\nrdsdebug("purging iwinc %p inc %p\n", iwinc, inc);\r\nlist_for_each_entry_safe(frag, pos, &iwinc->ii_frags, f_item) {\r\nlist_del_init(&frag->f_item);\r\nrds_iw_frag_drop_page(frag);\r\nrds_iw_frag_free(frag);\r\n}\r\n}\r\nvoid rds_iw_inc_free(struct rds_incoming *inc)\r\n{\r\nstruct rds_iw_incoming *iwinc;\r\niwinc = container_of(inc, struct rds_iw_incoming, ii_inc);\r\nrds_iw_inc_purge(inc);\r\nrdsdebug("freeing iwinc %p inc %p\n", iwinc, inc);\r\nBUG_ON(!list_empty(&iwinc->ii_frags));\r\nkmem_cache_free(rds_iw_incoming_slab, iwinc);\r\natomic_dec(&rds_iw_allocation);\r\nBUG_ON(atomic_read(&rds_iw_allocation) < 0);\r\n}\r\nint rds_iw_inc_copy_to_user(struct rds_incoming *inc, struct iovec *first_iov,\r\nsize_t size)\r\n{\r\nstruct rds_iw_incoming *iwinc;\r\nstruct rds_page_frag *frag;\r\nstruct iovec *iov = first_iov;\r\nunsigned long to_copy;\r\nunsigned long frag_off = 0;\r\nunsigned long iov_off = 0;\r\nint copied = 0;\r\nint ret;\r\nu32 len;\r\niwinc = container_of(inc, struct rds_iw_incoming, ii_inc);\r\nfrag = list_entry(iwinc->ii_frags.next, struct rds_page_frag, f_item);\r\nlen = be32_to_cpu(inc->i_hdr.h_len);\r\nwhile (copied < size && copied < len) {\r\nif (frag_off == RDS_FRAG_SIZE) {\r\nfrag = list_entry(frag->f_item.next,\r\nstruct rds_page_frag, f_item);\r\nfrag_off = 0;\r\n}\r\nwhile (iov_off == iov->iov_len) {\r\niov_off = 0;\r\niov++;\r\n}\r\nto_copy = min(iov->iov_len - iov_off, RDS_FRAG_SIZE - frag_off);\r\nto_copy = min_t(size_t, to_copy, size - copied);\r\nto_copy = min_t(unsigned long, to_copy, len - copied);\r\nrdsdebug("%lu bytes to user [%p, %zu] + %lu from frag "\r\n"[%p, %lu] + %lu\n",\r\nto_copy, iov->iov_base, iov->iov_len, iov_off,\r\nfrag->f_page, frag->f_offset, frag_off);\r\nret = rds_page_copy_to_user(frag->f_page,\r\nfrag->f_offset + frag_off,\r\niov->iov_base + iov_off,\r\nto_copy);\r\nif (ret) {\r\ncopied = ret;\r\nbreak;\r\n}\r\niov_off += to_copy;\r\nfrag_off += to_copy;\r\ncopied += to_copy;\r\n}\r\nreturn copied;\r\n}\r\nvoid rds_iw_recv_init_ack(struct rds_iw_connection *ic)\r\n{\r\nstruct ib_send_wr *wr = &ic->i_ack_wr;\r\nstruct ib_sge *sge = &ic->i_ack_sge;\r\nsge->addr = ic->i_ack_dma;\r\nsge->length = sizeof(struct rds_header);\r\nsge->lkey = rds_iw_local_dma_lkey(ic);\r\nwr->sg_list = sge;\r\nwr->num_sge = 1;\r\nwr->opcode = IB_WR_SEND;\r\nwr->wr_id = RDS_IW_ACK_WR_ID;\r\nwr->send_flags = IB_SEND_SIGNALED | IB_SEND_SOLICITED;\r\n}\r\nstatic void rds_iw_set_ack(struct rds_iw_connection *ic, u64 seq,\r\nint ack_required)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&ic->i_ack_lock, flags);\r\nic->i_ack_next = seq;\r\nif (ack_required)\r\nset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nspin_unlock_irqrestore(&ic->i_ack_lock, flags);\r\n}\r\nstatic u64 rds_iw_get_ack(struct rds_iw_connection *ic)\r\n{\r\nunsigned long flags;\r\nu64 seq;\r\nclear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nspin_lock_irqsave(&ic->i_ack_lock, flags);\r\nseq = ic->i_ack_next;\r\nspin_unlock_irqrestore(&ic->i_ack_lock, flags);\r\nreturn seq;\r\n}\r\nstatic void rds_iw_set_ack(struct rds_iw_connection *ic, u64 seq,\r\nint ack_required)\r\n{\r\natomic64_set(&ic->i_ack_next, seq);\r\nif (ack_required) {\r\nsmp_mb__before_clear_bit();\r\nset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\n}\r\n}\r\nstatic u64 rds_iw_get_ack(struct rds_iw_connection *ic)\r\n{\r\nclear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nsmp_mb__after_clear_bit();\r\nreturn atomic64_read(&ic->i_ack_next);\r\n}\r\nstatic void rds_iw_send_ack(struct rds_iw_connection *ic, unsigned int adv_credits)\r\n{\r\nstruct rds_header *hdr = ic->i_ack;\r\nstruct ib_send_wr *failed_wr;\r\nu64 seq;\r\nint ret;\r\nseq = rds_iw_get_ack(ic);\r\nrdsdebug("send_ack: ic %p ack %llu\n", ic, (unsigned long long) seq);\r\nrds_message_populate_header(hdr, 0, 0, 0);\r\nhdr->h_ack = cpu_to_be64(seq);\r\nhdr->h_credit = adv_credits;\r\nrds_message_make_checksum(hdr);\r\nic->i_ack_queued = jiffies;\r\nret = ib_post_send(ic->i_cm_id->qp, &ic->i_ack_wr, &failed_wr);\r\nif (unlikely(ret)) {\r\nclear_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags);\r\nset_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nrds_iw_stats_inc(s_iw_ack_send_failure);\r\nrds_iw_conn_error(ic->conn, "sending ack failed\n");\r\n} else\r\nrds_iw_stats_inc(s_iw_ack_sent);\r\n}\r\nvoid rds_iw_attempt_ack(struct rds_iw_connection *ic)\r\n{\r\nunsigned int adv_credits;\r\nif (!test_bit(IB_ACK_REQUESTED, &ic->i_ack_flags))\r\nreturn;\r\nif (test_and_set_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags)) {\r\nrds_iw_stats_inc(s_iw_ack_send_delayed);\r\nreturn;\r\n}\r\nif (!rds_iw_send_grab_credits(ic, 1, &adv_credits, 0, RDS_MAX_ADV_CREDIT)) {\r\nrds_iw_stats_inc(s_iw_tx_throttle);\r\nclear_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags);\r\nreturn;\r\n}\r\nclear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags);\r\nrds_iw_send_ack(ic, adv_credits);\r\n}\r\nvoid rds_iw_ack_send_complete(struct rds_iw_connection *ic)\r\n{\r\nclear_bit(IB_ACK_IN_FLIGHT, &ic->i_ack_flags);\r\nrds_iw_attempt_ack(ic);\r\n}\r\nu64 rds_iw_piggyb_ack(struct rds_iw_connection *ic)\r\n{\r\nif (test_and_clear_bit(IB_ACK_REQUESTED, &ic->i_ack_flags))\r\nrds_iw_stats_inc(s_iw_ack_send_piggybacked);\r\nreturn rds_iw_get_ack(ic);\r\n}\r\nstatic void rds_iw_cong_recv(struct rds_connection *conn,\r\nstruct rds_iw_incoming *iwinc)\r\n{\r\nstruct rds_cong_map *map;\r\nunsigned int map_off;\r\nunsigned int map_page;\r\nstruct rds_page_frag *frag;\r\nunsigned long frag_off;\r\nunsigned long to_copy;\r\nunsigned long copied;\r\nuint64_t uncongested = 0;\r\nvoid *addr;\r\nif (be32_to_cpu(iwinc->ii_inc.i_hdr.h_len) != RDS_CONG_MAP_BYTES)\r\nreturn;\r\nmap = conn->c_fcong;\r\nmap_page = 0;\r\nmap_off = 0;\r\nfrag = list_entry(iwinc->ii_frags.next, struct rds_page_frag, f_item);\r\nfrag_off = 0;\r\ncopied = 0;\r\nwhile (copied < RDS_CONG_MAP_BYTES) {\r\nuint64_t *src, *dst;\r\nunsigned int k;\r\nto_copy = min(RDS_FRAG_SIZE - frag_off, PAGE_SIZE - map_off);\r\nBUG_ON(to_copy & 7);\r\naddr = kmap_atomic(frag->f_page, KM_SOFTIRQ0);\r\nsrc = addr + frag_off;\r\ndst = (void *)map->m_page_addrs[map_page] + map_off;\r\nfor (k = 0; k < to_copy; k += 8) {\r\nuncongested |= ~(*src) & *dst;\r\n*dst++ = *src++;\r\n}\r\nkunmap_atomic(addr, KM_SOFTIRQ0);\r\ncopied += to_copy;\r\nmap_off += to_copy;\r\nif (map_off == PAGE_SIZE) {\r\nmap_off = 0;\r\nmap_page++;\r\n}\r\nfrag_off += to_copy;\r\nif (frag_off == RDS_FRAG_SIZE) {\r\nfrag = list_entry(frag->f_item.next,\r\nstruct rds_page_frag, f_item);\r\nfrag_off = 0;\r\n}\r\n}\r\nuncongested = le64_to_cpu(uncongested);\r\nrds_cong_map_updated(map, uncongested);\r\n}\r\nstatic void rds_iw_process_recv(struct rds_connection *conn,\r\nstruct rds_iw_recv_work *recv, u32 byte_len,\r\nstruct rds_iw_ack_state *state)\r\n{\r\nstruct rds_iw_connection *ic = conn->c_transport_data;\r\nstruct rds_iw_incoming *iwinc = ic->i_iwinc;\r\nstruct rds_header *ihdr, *hdr;\r\nrdsdebug("ic %p iwinc %p recv %p byte len %u\n", ic, iwinc, recv,\r\nbyte_len);\r\nif (byte_len < sizeof(struct rds_header)) {\r\nrds_iw_conn_error(conn, "incoming message "\r\n"from %pI4 didn't inclue a "\r\n"header, disconnecting and "\r\n"reconnecting\n",\r\n&conn->c_faddr);\r\nreturn;\r\n}\r\nbyte_len -= sizeof(struct rds_header);\r\nihdr = &ic->i_recv_hdrs[recv - ic->i_recvs];\r\nif (!rds_message_verify_checksum(ihdr)) {\r\nrds_iw_conn_error(conn, "incoming message "\r\n"from %pI4 has corrupted header - "\r\n"forcing a reconnect\n",\r\n&conn->c_faddr);\r\nrds_stats_inc(s_recv_drop_bad_checksum);\r\nreturn;\r\n}\r\nstate->ack_recv = be64_to_cpu(ihdr->h_ack);\r\nstate->ack_recv_valid = 1;\r\nif (ihdr->h_credit)\r\nrds_iw_send_add_credits(conn, ihdr->h_credit);\r\nif (ihdr->h_sport == 0 && ihdr->h_dport == 0 && byte_len == 0) {\r\nrds_iw_stats_inc(s_iw_ack_received);\r\nrds_iw_frag_drop_page(recv->r_frag);\r\nreturn;\r\n}\r\nif (!iwinc) {\r\niwinc = recv->r_iwinc;\r\nrecv->r_iwinc = NULL;\r\nic->i_iwinc = iwinc;\r\nhdr = &iwinc->ii_inc.i_hdr;\r\nmemcpy(hdr, ihdr, sizeof(*hdr));\r\nic->i_recv_data_rem = be32_to_cpu(hdr->h_len);\r\nrdsdebug("ic %p iwinc %p rem %u flag 0x%x\n", ic, iwinc,\r\nic->i_recv_data_rem, hdr->h_flags);\r\n} else {\r\nhdr = &iwinc->ii_inc.i_hdr;\r\nif (hdr->h_sequence != ihdr->h_sequence ||\r\nhdr->h_len != ihdr->h_len ||\r\nhdr->h_sport != ihdr->h_sport ||\r\nhdr->h_dport != ihdr->h_dport) {\r\nrds_iw_conn_error(conn,\r\n"fragment header mismatch; forcing reconnect\n");\r\nreturn;\r\n}\r\n}\r\nlist_add_tail(&recv->r_frag->f_item, &iwinc->ii_frags);\r\nrecv->r_frag = NULL;\r\nif (ic->i_recv_data_rem > RDS_FRAG_SIZE)\r\nic->i_recv_data_rem -= RDS_FRAG_SIZE;\r\nelse {\r\nic->i_recv_data_rem = 0;\r\nic->i_iwinc = NULL;\r\nif (iwinc->ii_inc.i_hdr.h_flags == RDS_FLAG_CONG_BITMAP)\r\nrds_iw_cong_recv(conn, iwinc);\r\nelse {\r\nrds_recv_incoming(conn, conn->c_faddr, conn->c_laddr,\r\n&iwinc->ii_inc, GFP_ATOMIC,\r\nKM_SOFTIRQ0);\r\nstate->ack_next = be64_to_cpu(hdr->h_sequence);\r\nstate->ack_next_valid = 1;\r\n}\r\nif (hdr->h_flags & RDS_FLAG_ACK_REQUIRED) {\r\nrds_stats_inc(s_recv_ack_required);\r\nstate->ack_required = 1;\r\n}\r\nrds_inc_put(&iwinc->ii_inc);\r\n}\r\n}\r\nvoid rds_iw_recv_cq_comp_handler(struct ib_cq *cq, void *context)\r\n{\r\nstruct rds_connection *conn = context;\r\nstruct rds_iw_connection *ic = conn->c_transport_data;\r\nrdsdebug("conn %p cq %p\n", conn, cq);\r\nrds_iw_stats_inc(s_iw_rx_cq_call);\r\ntasklet_schedule(&ic->i_recv_tasklet);\r\n}\r\nstatic inline void rds_poll_cq(struct rds_iw_connection *ic,\r\nstruct rds_iw_ack_state *state)\r\n{\r\nstruct rds_connection *conn = ic->conn;\r\nstruct ib_wc wc;\r\nstruct rds_iw_recv_work *recv;\r\nwhile (ib_poll_cq(ic->i_recv_cq, 1, &wc) > 0) {\r\nrdsdebug("wc wr_id 0x%llx status %u byte_len %u imm_data %u\n",\r\n(unsigned long long)wc.wr_id, wc.status, wc.byte_len,\r\nbe32_to_cpu(wc.ex.imm_data));\r\nrds_iw_stats_inc(s_iw_rx_cq_event);\r\nrecv = &ic->i_recvs[rds_iw_ring_oldest(&ic->i_recv_ring)];\r\nrds_iw_recv_unmap_page(ic, recv);\r\nif (rds_conn_up(conn) || rds_conn_connecting(conn)) {\r\nif (wc.status == IB_WC_SUCCESS) {\r\nrds_iw_process_recv(conn, recv, wc.byte_len, state);\r\n} else {\r\nrds_iw_conn_error(conn, "recv completion on "\r\n"%pI4 had status %u, disconnecting and "\r\n"reconnecting\n", &conn->c_faddr,\r\nwc.status);\r\n}\r\n}\r\nrds_iw_ring_free(&ic->i_recv_ring, 1);\r\n}\r\n}\r\nvoid rds_iw_recv_tasklet_fn(unsigned long data)\r\n{\r\nstruct rds_iw_connection *ic = (struct rds_iw_connection *) data;\r\nstruct rds_connection *conn = ic->conn;\r\nstruct rds_iw_ack_state state = { 0, };\r\nrds_poll_cq(ic, &state);\r\nib_req_notify_cq(ic->i_recv_cq, IB_CQ_SOLICITED);\r\nrds_poll_cq(ic, &state);\r\nif (state.ack_next_valid)\r\nrds_iw_set_ack(ic, state.ack_next, state.ack_required);\r\nif (state.ack_recv_valid && state.ack_recv > ic->i_ack_recv) {\r\nrds_send_drop_acked(conn, state.ack_recv, NULL);\r\nic->i_ack_recv = state.ack_recv;\r\n}\r\nif (rds_conn_up(conn))\r\nrds_iw_attempt_ack(ic);\r\nif (rds_iw_ring_empty(&ic->i_recv_ring))\r\nrds_iw_stats_inc(s_iw_rx_ring_empty);\r\nif (rds_iw_ring_low(&ic->i_recv_ring))\r\nqueue_delayed_work(rds_wq, &conn->c_recv_w, 0);\r\n}\r\nint rds_iw_recv(struct rds_connection *conn)\r\n{\r\nstruct rds_iw_connection *ic = conn->c_transport_data;\r\nint ret = 0;\r\nrdsdebug("conn %p\n", conn);\r\nmutex_lock(&ic->i_recv_mutex);\r\nif (rds_iw_recv_refill(conn, GFP_KERNEL, GFP_HIGHUSER, 0))\r\nret = -ENOMEM;\r\nelse\r\nrds_iw_stats_inc(s_iw_rx_refill_from_thread);\r\nmutex_unlock(&ic->i_recv_mutex);\r\nif (rds_conn_up(conn))\r\nrds_iw_attempt_ack(ic);\r\nreturn ret;\r\n}\r\nint rds_iw_recv_init(void)\r\n{\r\nstruct sysinfo si;\r\nint ret = -ENOMEM;\r\nsi_meminfo(&si);\r\nrds_iw_sysctl_max_recv_allocation = si.totalram / 3 * PAGE_SIZE / RDS_FRAG_SIZE;\r\nrds_iw_incoming_slab = kmem_cache_create("rds_iw_incoming",\r\nsizeof(struct rds_iw_incoming),\r\n0, 0, NULL);\r\nif (!rds_iw_incoming_slab)\r\ngoto out;\r\nrds_iw_frag_slab = kmem_cache_create("rds_iw_frag",\r\nsizeof(struct rds_page_frag),\r\n0, 0, NULL);\r\nif (!rds_iw_frag_slab)\r\nkmem_cache_destroy(rds_iw_incoming_slab);\r\nelse\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nvoid rds_iw_recv_exit(void)\r\n{\r\nkmem_cache_destroy(rds_iw_incoming_slab);\r\nkmem_cache_destroy(rds_iw_frag_slab);\r\n}
